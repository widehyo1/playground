```sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find html/ -name '*.html' \
find sample/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```
```sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.jsonl"

date
# find html/ -name '*.html' \
find sample/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs ".")
      echo "{\"url\":\"$url\",\"title\":\"$title\",\"content\":$md}"
  ' > "$out"
date
```


jq -r '[.url, .title, .content] | @csv' bulk_data_xargs_v2.jsonl > 


~/gitclone/playground/content_base/cli/pipeline/geeknews $ !psql
psql -h localhost -p 15432 -U postgres -f bulk_insert.sql
Password for user postgres:
COPY 99
~/gitclone/playground/content_base/cli/pipeline/geeknews $ psql -h localhost -p 15432 -U postgres
Password for user postgres:
psql (17.5 (Ubuntu 17.5-1.pgdg22.04+1), server 17.4 (Debian 17.4-1.pgdg120+2))
Type "help" for help.

postgres=# \dt
               List of relations
 Schema |       Name        | Type  |  Owner
--------+-------------------+-------+----------
 public | geeknews          | table | postgres
 public | geeknews_parallel | table | postgres
 public | geeknews_xargs    | table | postgres
(3 rows)

postgres=# select count(1) from geeknews_xargs;
 count
-------
    99
(1 row)

postgres=# select * from geeknews_xargs limit 3;
               url                |                     title                     |                                 content
----------------------------------+-----------------------------------------------+--------------------------------------------------------------------------
 https://news.hada.io/topic?id=87 | Git Commit Messages Guide [한글]              | []{#topic_contents}                                                     +
                                  |                                               |
 https://news.hada.io/topic?id=4  | 당신네 회사 개발팀의 핵심가치는 무엇인가요 ?  | []{#topic_contents}                                                     +
                                  |                                               |                                                                         +
                                  |                                               | 회사 개발팀이 어떤 가치를 중요하게 여기는 지 보여줘서, 개발자들이 회사를+
                                  |                                               | 선택할때 중요한 정보를 미리취득가능.\                                   +
                                  |                                               | 좋은 회사를 찾는 개발자와 자신들의 문화와 잘 어울리는 이를 찾는 회사,   +
                                  |                                               | 양쪽의 니즈가 서로 맞아서 생긴 서비스                                   +
                                  |                                               |
 https://news.hada.io/topic?id=35 | VS code로 WSL 환경에서 코딩하기               | []{#topic_contents}                                                     +
                                  |                                               |                                                                         +
                                  |                                               | vscode, remote development, linux, windows,사실 wsl이 아니어도          +
                                  |                                               | 상관없음이 밝혀져                                                       +
                                  |                                               |                                                                         +
                                  |                                               | []{#contents22 .comment_contents}                                       +
                                  |                                               |                                                                         +
                                  |                                               | wsl hell!                                                               +
                                  |                                               |
(3 rows)

postgres=#
\q

~/gitclone/playground/content_base/cli/pipeline/geeknews $ jq -Rs "." sitemap.xml | jq -r "[.] | @csv"
"<sitemapindex xmlns=""http://www.sitemaps.org/schemas/sitemap/0.9"">
<sitemap>
<loc>https://news.hada.io/sitemap/sitemap.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2019.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2020.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2021.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2022.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2023.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2024.xml</loc>
</sitemap>
<sitemap>
<loc>https://news.hada.io/sitemap/2025.xml</loc>
</sitemap>
</sitemapindex>
"


~ $ jq -Rn --arg url "asdf" --arg title "qwer" --arg content "zxcv" '[ $url, $title, $content ] | @csv'
"\"asdf\",\"qwer\",\"zxcv\""



```sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find html/ -name '*.html' \
find sample/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```


---

1차 실험
```bash
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```


```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_htmlq_xargs_v2.sh
Thu Aug 28 22:21:57 KST 2025
Thu Aug 28 22:38:41 KST 2025

real    16m44.062s
user    98m2.638s
sys     28m33.128s
```


2차 실험

```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat parse_htmlq_xargs_v2.sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```

~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_htmlq_xargs_v2.sh
Thu Aug 28 23:38:54 KST 2025
^[[C

Fri Aug 29 00:15:03 KST 2025

real    36m9.306s
user    205m54.874s
sys     70m22.585s


3차 실험

```bash
#!/bin/bash
# parse_lynx_xargs.sh

out="bulk_data_lynx.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"

      # 필요한 DOM 추출
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")

      # 타이틀 추출
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")

      # 본문만 추출 → 텍스트 변환 → CSV escape
      content=$(echo "$parsed" | tail -n +2 | \
        LANG=ko_KR.UTF-8 lynx -stdin -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 | \
        jq -Rs "." | jq -r "[.] | @csv")

      echo "\"$url\",\"$title\",$content"
  ' > "$out"
date
```


~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_lynx_xargs.sh
Fri Aug 29 01:27:46 KST 2025

Fri Aug 29 01:53:35 KST 2025

real    25m48.326s
user    178m16.809s
sys     25m4.673s

---


~/gitclone/playground/content_base/cli/pipeline/geeknews $ csvlook bulk_data_xargs_v2.csv
Your file is not "utf-8-sig" encoded. Please specify the correct encoding with the --encoding flag. Use the -v flag to see the complete error.
~/gitclone/playground/content_base/cli/pipeline/geeknews $ wc bulk_data_xargs_v2.csv
 1619856  8775160 86490618 bulk_data_xargs_v2.csv

```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ wc bulk_data_xargs_v2.csv
 1619856  8775160 86490618 bulk_data_xargs_v2.csv
~/gitclone/playground/content_base/cli/pipeline/geeknews $ csvlook bulk_data_xargs_v2.csv
Your file is not "utf-8-sig" encoded. Please specify the correct encoding with the --encoding flag. Use the -v flag to see the complete error.

~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash bulk_insert.sh
Password for user postgres:
psql:bulk_insert.sql:1: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 5739: ""

~/gitclone/playground/content_base/cli/pipeline/geeknews $ sed -n '5700,5800p' bulk_data_xargs_v2.csv


  5645  "https://news.hada.io/topic?id=8991","Valve, Proton 8.0-1 릴리즈","[]{#topic_contents}
  5646
  5647  Forspoken, Dead Space, Nioh 2, 원피스 해적무쌍 4, 삼국지14 등을 지원
  5648  "
  5649  "https://news.hada.io/topic?id=12931","WhisperSpeech – Whisper를 역으로 구축한 오픈 소스 음성합성 시스템","[]{#topic_contents}
  5650
  ...
  5768      매핑하는 것으로 제한적으로 수행 가능.
  5769    - IPA 접근 방식은 음성 품질과 음색의 변화를 더 잘 학습할 수 있게 할 수
  5770      있음.
  5771
  5772  - **Piper를 사용한 맞춤형 목소리 훈련에 대한 관찰**
  5773  "https://news.hada.io/topic?id=10199","GPT-4의 비결정성은 Sparse MoE에 의해 발생","[]{#topic_contents}
  5774
  5775    - Piper를 사용하여 맞춤형 목소리를 훈련하는 비디오를 보고, 데이터셋에
  5776
```


---

인코딩 문제 발견

~/gitclone/playground/content_base/cli/pipeline/geeknews $ file -i html/*.html | awk '{ print $3 }' | sort | uniq -c
     14 charset=binary
  21994 charset=utf-8
~/gitclone/playground/content_base/cli/pipeline/geeknews $ file -i html/*.html | grep 'charset=binary'
html/10247.html: application/octet-stream; charset=binary
html/10576.html: application/octet-stream; charset=binary
html/13382.html: application/octet-stream; charset=binary
html/13775.html: application/octet-stream; charset=binary
html/14606.html: application/octet-stream; charset=binary
html/15403.html: application/octet-stream; charset=binary
html/16388.html: application/octet-stream; charset=binary
html/20452.html: application/octet-stream; charset=binary
html/210.html:   application/octet-stream; charset=binary
html/4018.html:  application/octet-stream; charset=binary
html/7044.html:  application/octet-stream; charset=binary
html/753.html:   application/octet-stream; charset=binary
html/9123.html:  application/octet-stream; charset=binary
html/9810.html:  application/octet-stream; charset=binary



html/10247.html: application/octet-stream; charset=binary
html/10576.html: application/octet-stream; charset=binary
html/13382.html: application/octet-stream; charset=binary
html/13775.html: application/octet-stream; charset=binary
html/14606.html: application/octet-stream; charset=binary
html/15403.html: application/octet-stream; charset=binary
html/16388.html: application/octet-stream; charset=binary
html/20452.html: application/octet-stream; charset=binary
html/210.html:   application/octet-stream; charset=binary
html/4018.html:  application/octet-stream; charset=binary
html/7044.html:  application/octet-stream; charset=binary
html/753.html:   application/octet-stream; charset=binary
html/9123.html:  application/octet-stream; charset=binary
html/9810.html:  application/octet-stream; charset=binary



html/10247.html
html/10576.html
html/13382.html
html/13775.html
html/14606.html
html/15403.html
html/16388.html
html/20452.html
html/210.html
html/4018.html
html/7044.html
html/753.html
html/9123.html
html/9810.html



 $ grep -P '[^\x00-\x7F]' html/210.html

grep -P '[^\x00-\x7F]' html/10247.html
grep -P '[^\x00-\x7F]' html/10576.html
grep -P '[^\x00-\x7F]' html/13382.html
grep -P '[^\x00-\x7F]' html/13775.html
grep -P '[^\x00-\x7F]' html/14606.html
grep -P '[^\x00-\x7F]' html/15403.html
grep -P '[^\x00-\x7F]' html/16388.html
grep -P '[^\x00-\x7F]' html/20452.html
grep -P '[^\x00-\x7F]' html/210.html
grep -P '[^\x00-\x7F]' html/4018.html
grep -P '[^\x00-\x7F]' html/7044.html
grep -P '[^\x00-\x7F]' html/753.html
grep -P '[^\x00-\x7F]' html/9123.html
grep -P '[^\x00-\x7F]' html/9810.html


~/gitclone/playground/content_base/cli/pipeline/geeknews $ file -i 210.html
210.html: application/octet-stream; charset=binary
~/gitclone/playground/content_base/cli/pipeline/geeknews $ file -i 210_work.html
210_work.html: text/html; charset=utf-8



html/10247.html
html/10576.html
html/13382.html
html/13775.html
html/14606.html
html/15403.html
html/16388.html
html/20452.html
html/210.html
html/4018.html
html/7044.html
html/753.html
html/9123.html
html/9810.html



mv html/10247.html html/10247.html.err
mv html/10576.html html/10576.html.err
mv html/13382.html html/13382.html.err
mv html/13775.html html/13775.html.err
mv html/14606.html html/14606.html.err
mv html/15403.html html/15403.html.err
mv html/16388.html html/16388.html.err
mv html/20452.html html/20452.html.err
mv html/210.html html/210.html.err
mv html/4018.html html/4018.html.err
mv html/7044.html html/7044.html.err
mv html/753.html html/753.html.err
mv html/9123.html html/9123.html.err
mv html/9810.html html/9810.html.err


~/gitclone/playground/content_base/cli/pipeline/geeknews $ mv html/10247.html html/10247.html.err
mv html/10576.html html/10576.html.err
mv html/13382.html html/13382.html.err
mv html/13775.html html/13775.html.err
mv html/14606.html html/14606.html.err
mv html/15403.html html/15403.html.err
mv html/16388.html html/16388.html.err
mv html/20452.html html/20452.html.err
mv html/210.html html/210.html.err
mv html/4018.html html/4018.html.err
mv html/7044.html html/7044.html.err
mv html/753.html html/753.html.err
mv html/9123.html html/9123.html.err
mv html/9810.html html/9810.html.err
~/gitclone/playground/content_base/cli/pipeline/geeknews $
exit

~/gitclone/playground/content_base/cli/pipeline/geeknews $ file -i html/*.html | awk '{ print $3 }' | sort | uniq -c
  21994 charset=utf-8


---


2차 실험

```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat parse_htmlq_xargs_v2.sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```

~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_htmlq_xargs_v2.sh
Thu Aug 28 23:38:54 KST 2025
Fri Aug 29 00:15:03 KST 2025

real    36m9.306s
user    205m54.874s
sys     70m22.585s



 $ htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f html/3172.html | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv" | csvlook

~/gitclone/playground/content_base/cli/pipeline/geeknews $ strace -c bash -c 'htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f html/3172.html | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv"'

% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- ----------------
 95.47    0.139130       23188         6         1 wait4
  0.68    0.000985          20        49        10 newfstatat
  0.60    0.000877          26        33        13 openat
  0.45    0.000655          26        25           mmap
  0.43    0.000631          17        36         8 close
  0.36    0.000521          19        27           rt_sigprocmask
  0.34    0.000491          98         5           clone
  0.20    0.000290          18        16           rt_sigaction
  0.18    0.000268          26        10         1 ioctl
  0.14    0.000205          51         4           pread64
  0.14    0.000201          40         5           mprotect
  0.13    0.000193          32         6           read
  0.09    0.000138          19         7           geteuid
  0.09    0.000130          18         7           getgid
  0.09    0.000129          18         7         2 access
  0.09    0.000125          17         7           getuid
  0.09    0.000124          17         7           getegid
  0.05    0.000080          20         4           pipe2
  0.05    0.000078          26         3           getppid
  0.05    0.000069          17         4           brk
  0.05    0.000066          22         3           getpid
  0.03    0.000037          18         2           prlimit64
  0.02    0.000036          36         1           set_tid_address
  0.02    0.000036          36         1           set_robust_list
  0.02    0.000036          36         1           rseq
  0.02    0.000035          17         2         1 arch_prctl
  0.02    0.000029          29         1           munmap
  0.02    0.000025          25         1           rt_sigreturn
  0.02    0.000023          23         1           sysinfo
  0.01    0.000020          20         1           getpgrp
  0.01    0.000019          19         1         1 getpeername
  0.01    0.000019          19         1           uname
  0.01    0.000017          17         1           getrandom
  0.01    0.000016          16         1           futex
  0.00    0.000000           0         1           execve
------ ----------- ----------- --------- --------- ----------------
100.00    0.145734         507       287        37 total
~/gitclone/playground/content_base/cli/pipeline/geeknews $ strace -c bash -c 'htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f html/3172.html | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv"'




```sh
f="html/3172.html"

# 전체 시간
echo "[전체 파이프라인]"
time bash -c '
  htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$0" |
  tail -n +2 |
  pandoc -f html -t markdown |
  jq -Rs "." |
  jq -r "[.] | @csv"
' "$f"

echo
echo "[htmlq]"
time htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f" > /dev/null

echo "[pandoc]"
htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents"  -f "$f" | tail -n +2 > temp.html
time pandoc -f html -t markdown < temp.html > /dev/null

echo "[jq]"
pandoc pandoc -f html -t markdown > temp.md
time jq -Rs "." < temp.md | jq -r '[.] | @csv' > /dev/null
```
```bash
#!/bin/bash
f="html/3172.html"

# 전체 시간
echo "[전체 파이프라인]"
time bash -c '
  htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$0" |
  tail -n +2 |
  pandoc -f html -t markdown |
  jq -Rs "." |
  jq -r "[.] | @csv"
' "$f"

echo
echo "[htmlq]"
time htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f" > /dev/null

echo "[pandoc]"
htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents"  -f "$f" | tail -n +2 > temp.html
time pandoc -f html -t markdown < temp.html > /dev/null

echo "[jq]"
pandoc -f html -t markdown < temp.html > temp.md
time jq -Rs "." < temp.md | jq -r '[.] | @csv' > /dev/null
```

```
[전체 파이프라인]
real    0m0.093s
user    0m0.101s
sys     0m0.045s

[htmlq]

real    0m0.003s
user    0m0.003s
sys     0m0.000s
[pandoc]

real    0m0.070s
user    0m0.031s
sys     0m0.018s
[jq]

real    0m0.030s
user    0m0.051s
sys     0m0.008s

```


https://lynx.invisible-island.net/current/index.html

18118  2025-08-29 00:48:32 mv lynx-cur.tar.gz ~
18119  2025-08-29 00:48:33 cd
18120  2025-08-29 00:48:33 ll
18121  2025-08-29 00:48:44 mv lynx-cur.tar.gz gitclone/playground/utils/lynx/
18122  2025-08-29 00:48:46 cd gitclone/playground/utils/lynx/
18123  2025-08-29 00:48:47 ll
18124  2025-08-29 00:48:50 tarx lynx-cur.tar.gz
18125  2025-08-29 00:48:51 ll
18126  2025-08-29 00:48:52 cd lynx2.9.2/
18127  2025-08-29 00:48:53 ll
18128  2025-08-29 00:49:06 cd src/
18129  2025-08-29 00:49:06 ll
18130  2025-08-29 00:49:15 cd ..
18131  2025-08-29 00:49:16 ll
18132  2025-08-29 00:49:25 cat README
18133  2025-08-29 00:49:35 cat INSTALLATION
18134  2025-08-29 00:51:18 ./configure
18135  2025-08-29 00:51:53 make -j$(proc -n)
18136  2025-08-29 00:52:30 lynx
18137  2025-08-29 00:52:36 sudo make install

https://ftp.gnu.org/pub/gnu/gzip/
 $ pip install html2text

```bash
#!/bin/bash
# htmlq로 content만 추출
htmlq 'span#topic_contents, span.comment_contents' -f html/3172.html > temp.html

echo "[pandoc]"
# pandoc
time pandoc -f html -t markdown < temp.html > /dev/null

echo "[html2text]"
# html2text
time html2text < temp.html > /dev/null

echo "[lynx]"
# lynx
time lynx -dump -nolist -width=1000 temp.html > /dev/null
```

[pandoc]

real    0m0.083s
user    0m0.018s
sys     0m0.035s
[html2text]

real    0m0.043s
user    0m0.038s
sys     0m0.004s
[lynx]

real    0m0.017s
user    0m0.012s
sys     0m0.004s


LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 temp.html


```bash
#!/bin/bash
# parse_lynx_xargs.sh

out="bulk_data_lynx.csv"

date
find sample/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"

      # 필요한 DOM 추출
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")

      # 타이틀 추출
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")

      # 본문만 추출 → 텍스트 변환 → CSV escape
      content=$(echo "$parsed" | tail -n +2 | \
        LANG=ko_KR.UTF-8 lynx -stdin -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 | \
        jq -Rs "." | jq -r "[.] | @csv")

      echo "\"$url\",\"$title\",$content"
  ' > "$out"
date
```

~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_lynx_xargs.sh
Fri Aug 29 01:04:43 KST 2025
Fri Aug 29 01:04:45 KST 2025

real    0m2.409s
user    0m16.657s
sys     0m2.075s
~/gitclone/playground/content_base/cli/pipeline/geeknews $ head bulk_data_lynx.csv
"https://news.hada.io/topic?id=4","당신네 회사 개발팀의 핵심가치는 무엇인가요 ? ","   회사 개발팀이 어떤 가치를 중요하게 여기는 지 보여줘서, 개발자들이 회사를 선택할때 중요한 정보를 미리취득가능.
   좋은 회사를 찾는 개발자와 자신들의 문화와 잘 어울리는 이를 찾는 회사, 양쪽의 니즈가 서로 맞아서 생긴 서비스
"
"https://news.hada.io/topic?id=87","Git Commit Messages Guide [한글]","
"
"https://news.hada.io/topic?id=50","'타다' 금지법 나오나","   김경진 의원은 유난히 타다를 집요하게 공격하네요...
   국토부는 면허 비용을 내게 하고 차량 공유 서비스를 제도권에 편입시키려는 거 같은데, 이런 비용이 자금력 있는 곳들만 들어오게 하는 진입장벽이 되지는 않을지.
"
"https://news.hada.io/topic?id=33","Dropbox 파일 직접 전송 가능한 Transfer 서비스 공개","   기존의 링크 공유가 아닌 파일을 직접 보내는 방식. 수신확인 기능 및 통계도 지원. 요금제에 따라 크기 차등 지원
"
~/gitclone/playground/content_base/cli/pipeline/geeknews $ head bulk_data_lynx.csv | csvlook
| https://news.hada.io/topic?id=4  | 당신네 회사 개발팀의 핵심가치는 무엇인가요 ?            |    회사 개발팀이 어떤 가치를 중요하게 여기는 지 보여줘서, 개발자들이 회사를 선택할때 중요한 정보를 미리취득가능.
   좋은 회사를 찾는 개발자와 자신들의 문화와 잘 어울리는 이를 찾는 회사, 양쪽의 니즈가 서로 맞아서 생긴 서비스
 |
| -------------------------------- | ------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ |
| https://news.hada.io/topic?id=87 | Git Commit Messages Guide [한글]       |                                                                                                                                      |
| https://news.hada.io/topic?id=50 | '타다' 금지법 나오나                         |    김경진 의원은 유난히 타다를 집요하게 공격하네요...
   국토부는 면허 비용을 내게 하고 차량 공유 서비스를 제도권에 편입시키려는 거 같은데, 이런 비용이 자금력 있는 곳들만 들어오게 하는 진입장벽이 되지는 않을지.
        |
| https://news.hada.io/topic?id=33 | Dropbox 파일 직접 전송 가능한 Transfer 서비스 공개 |    기존의 링크 공유가 아닌 파일을 직접 보내는 방식. 수신확인 기능 및 통계도 지원. 요금제에 따라 크기 차등 지원
                    


