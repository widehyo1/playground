"https://news.hada.io/topic?id=21468","Canyon.mid","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Canyon.mid

     * Canyon.mid은 1990년대 Windows를 사용한 경험이 있는 사람들에게 익숙한 MIDI 음악 파일임
     * 이 곡은 Windows 3.x 시대의 테스트 및 데모용 기본 음악으로 널리 사용되었음
     * MIDI 표준을 활용해 매우 작은 파일 크기로 다양한 악기 음향을 경험할 수 있었음
     * 단순하면서도 기억에 남는 멜로디로 레거시 PC 사운드의 상징적 대표 곡임
     * 향수와 복고 트렌드로 인해 최근 개발자와 크리에이터 사이에서 다시 주목을 받고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Canyon.mid 곡 개요

     * Canyon.mid는 1990년대 중반 Windows 운영체제에서 기본 제공된 대표적인 MIDI 음악 파일임
     * 이 곡은 주로 Windows Media Player, MIDI 테스트, 그리고 하드웨어 오디오 드라이버의 동작 확인 목적으로 활용됨
     * MIDI 파일이기 때문에 자체적인 음원 데이터가 아니라 연주 정보(노트, 볼륨, 악기 등) 만 저장하므로, 파일 크기가 아주 작고 다양한 사운드카드에서 재생 가능함

기술적 특징 및 시대적 의미

     * MIDI 포맷은 전자악기 또는 컴퓨터가 음악과 효과를 제어하는 표준 신호 방식을 의미함
     * Canyon.mid는 다양한 악기를 조화롭게 혼합한 멜로디와 리듬으로 Windows PC의 멀티미디어 기능을 체험하게 해줌
     * 각 PC나 사운드카드마다 음색이 다르게 들려 동일한 곡이라도 사용자별로 음질 경험이 다양함

상징성과 영향

     * 단순한 구성과 밝고 경쾌한 멜로디로 많은 사람들의 기억에 남음
     * 복고풍을 즐기는 개발자나 음악가 커뮤니티에서 최근 다시 주목받고 있음
     * 원본 미디 파일은 크리에이티브 커먼즈 등의 라이선스로 공유되고 있어 자유롭게 감상 또는 재해석 자료로 활용 가능함

개발자 및 크리에이터 관점에서의 가치

     * MIDI 파일 분석 및 변환 실습 용도로 높은 활용 가치가 있음
     * 현대적인 DAW, 신디사이저와의 연동 테스트, 샘플 재가공 창작 실험 등에 활용됨
     * 초기 PC 미디어 경험과 차세대 오디오 기술의 연결고리 역할 수행함

        Hacker News 의견

     * 유튜브 영상의 ""레트로 스타일 컴퓨터""를 보다가 Tandy 1000 RSX에 대해 검색하게 됨. 이 컴퓨터가 Tandy 1000라는 이름을 달고 있으면서 16컬러 VGA 그래픽을 지원하는 것이 어색하게 느껴짐. 실제로 Tandy 1000 RSX는 1991년에 출시된 마지막 모델로, 이미 IBM PCjr에서 시작된 ""Tandy 그래픽"" 대신 Super VGA가 들어감. 기본적으로 Adlib이나 Sound Blaster 카드가 내장되어 있지 않았지만 ISA 슬롯이 하나 있어서 호환 사운드 카드 장착이 가능함. 프로세서는 기존 Tandy 1000과 달리 386, 램은 1MB 구성임
          + Tandy 1000에서 VGA를 사용하는 게 그리 드문 일은 아님. ISA 슬롯이 있는 초기 모델 대부분은 VGA 카드를 꽂을 수 있었음. 하드웨어상으로 문제는 없었으며 소프트웨어에서만 지원해주면 됨. Tandy 사의 PCM 잡지에서 다양한 VGA 카드 평가와 정보를 다뤘던 기억이 있음. 후기에 VGA 해상도를 지원하는 DeskMate 버전에 대해서도 본 적 있음
     * Sound Blaster에 들어있던 곡들 기억남. 예시로 이 곡 추천함. 초기 PC 스피커 삐빅 소리와 비교하면 정말 흑백 TV에서 컬러 TV로 넘어간 수준의 발전이라고 느껴짐. 그 시절 이런 식의 기술 도약은 매우 흔한 일상이었음
     * 왜 우리가 그 아름다운 미니멀리즘을 모두 없앴을까라는 생각이 듦. 옛날 컴퓨터만으로도 충분한 게임, 엔터테인먼트, 생산성을 구현했었음. 하지만 '충분함'의 기준은 계속 높아지면서 끝없이 뭔가를 쫓는 느낌이 듦. 마치 당근을 쫓는 동물처럼
          + 사실 이 이슈는 컴퓨터의 발전이라기보단 인간의 성향과 취향에 더 가까운 이야기임. 다들 자신의 젊었던 시절이 최고의 음악, 영화, 문화, 스포츠, 모든 것의 절정이라고 생각하는 경향이 있음. 1950년대든 1990년대든 마찬가지임
          + ""충분히 게임을 했었다""고 하지만, 과연 그랬을까라는 생각이 듦. 90년대에도 게임을 했고 지금도 게임을 하지만, 지금이 훨씬 재미있고 앞으로 나올 새로운 변화가 매우 기대됨(ex: gta6)
          + 객관적으로 봤을 때 90년대 컴퓨터는 한 명이 흥미를 느낄 만한 정보조차도 제대로 관리하기 어려운 수준이었음. 지금은 커뮤니티, 국가, 세계 단위 정보까지 다룰 수 있는 가능성이 생겼음. 더 강력해진 하드웨어와 OS 위에서 소프트웨어 발전이 이루어질 수 있는 가능성을 긍정적으로 생각함
          + 미니멀리즘을 바란다는 의견이 있지만, 실제로 실험적인 OS가 현대적인 UI를 갖추지 못하면 무시당할 수 있다는 댓글도 있더라. 결국 쉽지 않은 선택이라는 생각이 듦
          + 미니멀리즘이 사라진 건 비즈니스 경쟁 탓이라 생각함. 미니멀리즘은 '판매 포인트'가 되지 않음. 경쟁 업체가 더 강력한 성능과 기능을 추가하면서 시장이 확장되고, 사용자는 점점 덩치만 커진 제품으로 이동할 수밖에 없는 환경임. 미니멀리즘 제품은 점점 사라지거나 생태계와 호환되지 않아서 쓸 수 없게 됨
     * 예전엔 사운드카드가 없어서(주로 LucasArts 게임) 모든 게임을 PC 스피커로만 플레이함. 내겐 미디 버전 음악은 너무 심심하게 느껴짐. 몇 년 동안이나 사랑해온 스피커 특유의 ""거친 전기적 힘""이 부족함
          + Monkey Island 테마곡이 PC 스피커에서 울리는 소리를 아직도 뚜렷하게 기억함. 미디 버전보다 훨씬 더 마음에 듦. 세월이 정말 빠르게 흘렀네라는 생각이 듦
          + 네가 표현한 방식이 정말 멋짐. 혹시 칩튠 음악도 좋아할 수 있을 것 같음. 위키피디아에서 샘플도 들어볼 수 있음(chiptune)
          + PC 스피커와 미디의 차이가 정확히 무엇인지 궁금함. 그리고 왜 요즘은 '정확한' 소리를 내기 위해 미디 기기가 더 이상 필요 없는지 질문함
     * (원본) 미디 파일이 33KB인데 비디오(3.4MB)가 불과 100배 정도만 커서 약간 놀라움
          + 미디를 기기별로 일관된 소리로 재생하고 싶으면 쉽지 않음. 그 독특한 음색은 마이크로소프트가 Roland의 GS Wavetable을 라이선스했기 때문에 가능했음. 그게 없으면 저 독특한 음색 정보를 잃게 됨
          + 거의 정적 화면이라서, 사실상 대부분이 오디오 파일일 텍스트임. 따라서 비디오 자체는 데이터가 거의 없는 수준임
     * Canyon.mid의 작곡가 George Stone의 인터뷰 링크 공유함
     * 영상 플레이어에서 컨트롤을 절대 비활성화하지 말아주길 바람. 볼륨을 내리는 걸 깜빡하고 유튜브 영상을 틀었다가 최고 볼륨으로 소리를 들어버림
          + YT/Winamp 등에서 볼륨을 100%로 안 맞추면, 과정이 1) 오디오 신호 생성 2) 신호를 양자화하면서 볼륨 감소(정보 손실) 3) 다시 신호를 증폭(손실 정보 복구 불가) 순으로 불필요한 음질 저하를 유발함. 극단적으로 볼륨을 1%로 낮춘 뒤 다시 100배 증폭하면 효과가 더 심해짐. 그렇게 할 이유가 없음
          + 리눅스 파이어폭스 환경에선 클릭하면 영상이 멈춤. 플레이어에 어떤 컨트롤도 없지만, 영상 전체 아무데나 클릭해서 재생/일시정지하는 조작법은 웬만한 비디오 플레이어에서 표준화된 동작임
     * 예전이 더 단순하고, 느리고, 수동적이고, 때론 번거롭고, 더 버그가 많았지만 더 창의적이고 사용자 지정 가능했던 시간이 그립기도 함. 이런 변화들이 축적된 덕에 여기까지 온 것은 알지만, 마냥 그리운 향수임. 좋은 기억임
     * 이런 분위기를 좋아한다면, 예전 신디사이저 키보드·모듈용으로 직접 만든 데모곡들도 추천함
          + Emu Proteus 1
          + Emu Proteus 2
          + Roland MT-32
          + Roland D-10
          + Roland SC-33 (MIDI 애니메이션 포함)
          + Yamaha MU100 (1시간!)
          + 예전엔 이런 ""롬플러""가 처음 출시될 때마다 이런 곡들이 꼭 있었는데, 요즘도 이런 전통이 이어지는지 모르겠음
     * 옛날에는 미디어 플레이어가 그냥 midi 덤프 파일을 재생해줬다는 사실이 신기하게 느껴짐. 리눅스를 오래 써와서 그런지, 이제는 midi 파일을 그냥 즉시 재생하는 게 아니라 사운드폰트·소프트신스 선택, 별도 플레이어 설치, 복잡한 설정 등 온갖 선택을 거쳐야 해서 오히려 정신 없음. mpv 같은 플레이어가 그냥 기본 소프트신스를 선택해 바로 재생해주면 좋겠음. 요즘은 사운드폰트 지정도 어렵고 심지어 탐색/구간 이동도 자유롭지 않음
       참고: 사운드폰트 리스트
          + 맞음. mp3 이전 시대엔 midi, mod 파일 공유가 흔했음. 그 시절만의 분위기였음
"
"https://news.hada.io/topic?id=21556","Show GN: Quick Info on Cursor – 키보드 중심 개발을 위한 VSCode/Cursor 확장프로그램","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Show GN: Quick Info on Cursor – 키보드 중심 개발을 위한 VSCode/Cursor 확장프로그램

   키보드만으로 개발하던 중, 변수나 함수 정보를 확인하기 위해 마우스를 잡게 되는 불편을 느낀 적 있으신가요?

   Quick Info on Cursor는 이러한 상황을 해결하기 위해 개발한 VSCode 및 Cursor 확장 프로그램입니다.

   키보드 커서가 변수나 함수 위에 일정 시간(기본 500ms) 머무르면 자동으로 마우스 hover처럼 툴팁이 표시됩니다.

   사용해보시고 개선 아이디어나 피드백이 있다면 GitHub에 남겨주시면 감사하겠습니다.
   (https://github.com/developerjhp/vscode-quick-info-on-cursor)

   저도 제 밑에 댓글 두 분과 완전히 일맥상통한데, 키보드를 덜 누르는 걸 선호하는 분들이 분명 계실거라, 니즈는 있을 것 같네요

   툴팁 단축키 누르는 절차 하나를 줄이는 플러그인 인가요?

   저는 툴팁을 띄우는 단축키를 사용하고 있는데 혹시 이 방식과 차별되는 포인트가 있는지 궁금합니다

   오, 툴팁을 띄우는 단축키/커맨드는 어떤 거에요?

   Keyboard Shortcuts에서 ""Hover"" 검색하면 나와요. 전 ⌘K ⌘I 네요.
"
"https://news.hada.io/topic?id=21542","아이폰 8, 태양광 기반 Vision OCR 서버로 다시 태어남","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  아이폰 8, 태양광 기반 Vision OCR 서버로 다시 태어남

     * 중고 iPhone 8을 태양광으로 구동되는 개인 Vision OCR 서버로 개조한 경험 공유임
     * Apple Vision 프레임워크를 활용해 연 83,418건의 이미지 텍스트 추출과 48GB 이미지 처리를 현지에서 실행함
     * 전체 시스템은 EcoFlow River 2 Pro, 미니 PC, Tailscale 네트워크 등으로 구성되어 있음
     * 태양광을 통한 완전 오프그리드 실현과 연간 $84~120 CAD의 전기 비용 절감 효과 증명이 특징임
     * 개인 정보 보호, 비용, 에너지 독립, 전자 폐기물 저감 등 다양한 관점에서 로컬 컴퓨팅의 의의를 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * 약 1년간 태양광 기반 iPhone 8 서버로 83,418건의 OCR 요청과 48GB 이미지를 처리한 사례임
     * 대부분의 사람들은 구형 iPhone을 보관하지만, 본인은 이를 오프그리드 서버로 전환해 비용 절감과 환경 친화 실현을 목표로 함
     * OCR 서버는 블로그와 별도로 운영되는 부가 프로젝트임

기술 요약 (TL;DR)

     * 시스템 구성
          + iPhone 8에서 SwiftUI 앱 구동 및 Apple Vision 기반 OCR 가능
          + EcoFlow River 2 Pro(768Wh)와 220W 태양광 패널 조합 사용
          + 미니 PC에서 웹 서비스 및 API 라우팅 담당
          + Tailscale 네트워크로 기기 간 안전한 연결 유지
     * 1년 이후 주요 성과
          + 누적 83,418건 OCR 및 48GB 이미지 처리
          + 바쁜 날 1,000회 이상 처리
          + 1년 연속 사용 후 iPhone 배터리 건강 76% 수준
          + 연간 $84~120 CAD 전기료 절감 효과
     * 주요 인사이트
          + Apple Vision 프레임워크는 정확도에서 클라우드 서비스와 견줄만함
          + 구형 디바이스도 서버 워크로드에서 높은 신뢰성 보임
          + 태양광은 배터리 관리만 신경 쓰면 컴퓨팅 구동에 충분함
          + 로컬 처리 방식이 개인 정보 보호와 비용 절감 면에서 매우 우수함

프로젝트 배경: 왜 이런 걸 하는가?

  논리적 동기

     * 본인은 수백 장의 이미지를 자동 분류하는 프로젝트를 진행 중임
     * 합리적인 선택은 Mac에서 Apple Vision으로 OCR 처리하는 방법임

  “나만의 방식” 적용

     * 여유 있는 중고 iPhone 8과 EcoFlow River 2 Pro의 활용 방안 고민
     * 기존 OCR 서버를 태양광 기반으로 개조하며, 개인적 만족감 추구

  예기치 못한 장점

     * 실시간 대시보드 구현으로 창가에서 버드와칭하며 상태 모니터링 가능
     * 개인 프로젝트의 전력 독립성 달성
     * 실제 누적 전력 소비(월 37.4~45.8 kWh)로 연간 절감 효과 입증
     * 기기가 방문객 대상 흥미로운 대화거리로 작용

시스템 구성

     * 미니 PC: 웹서버, 이미지 처리, Plex 서버 등 다중 서비스 구동
     * iPhone 8: SwiftUI 앱을 통한 OCR 및 대시보드 역할 수행
     * EcoFlow 파워스테이션: 두 장비 모두를 오프그리드로 운용
     * Tailscale: 네트워크 안전 연결 지원
     * 처리 흐름
          + 이미지 처리 서비스가 iPhone에 이미지 전송 → Apple Vision을 활용한 OCR 실행 → 결과 반환 및 대시보드에 통계 표시
          + 모든 과정이 외부 전력에 의존하지 않고 태양광으로만 운용됨

하드웨어 및 태양광

  파워스테이션 선택

     * EcoFlow River 2 Pro는 원래 캠핑용으로 구매했으나, 해당 프로젝트에 최적 활용
     * GearScouts.com 등에서 파워스테이션 실구매가 비교 추천
     * iPhone 8 서버는 0.5~1W(대기 시) , 2~5W(처리시) 전력 소모로 매우 효율적임
     * 미니 PC는 15~30W, 전체 일일 전력 소모는 약 1.2kWh

  태양광 성능 (계절별)

     * 여름: 150~220W 최대 입력, 무제한 운용과 충전 가능
     * 가을/봄: 20~60W 평균, 일부 배터리와 병행 운용
     * 겨울: 5~20W, 배터리 주력(15~20시간 운용)
     * River 2 Pro의 높은 용량과 충방전 관리로 장기간 안정성 유지

iOS 기반 OCR 서버 앱 개발

  Apple Vision 프레임워크의 강점

     * Apple Vision은 로컬에서 매우 빠르고 정확한 OCR 기능 제공
     * 외부 API/클라우드 사용 필요 없이 디바이스 내부 처리 가능
     * 사용 예시 코드 제공(UIImage 처리 → 텍스트 추출)
     * 개인 정보 노출, 클라우드 요금 걱정 없는 환경 구성

  SwiftUI 대시보드 및 분석

     * 대시보드에서 실시간 통계(오늘 요청, 누적 건수, 평균 처리 시간, 성공률) 표시
     * Google Analytics 4 통합으로 활용자/세션/실시간 사용자 데이터 확인
     * 창가에 올려 둔 서버에서 작은 데이터센터처럼 대시보드 감상 가능

태양광 운용의 현실적 과제

     * 캐나다 특성상 햇살 좋은 여름은 짧고 구름 많은 겨울이 길어 계절별 운용 전략 필요
     * 여름: 태양광만으로 충전 및 동시 운용 가능
     * 봄/가을: 태양광+배터리 하이브리드
     * 겨울: 배터리 운용 중심, 간헐적 태양광 보조
     * 1년 상시 운용 후에도 iPhone 배터리 건강 76% 유지
     * 저온 환경에서는 OCR 속도 감소, 따뜻할 때 더 빠르게 동작함

비용 분석

  투자비 및 운영비

     * 초기 투자
          + EcoFlow River 2 Pro: $599 CAD(기존 캠핑용)
          + 220W 솔라 패널: $180 CAD
          + 부자재: 약 $50 CAD
          + 추가 솔라 투자 총액: 약 $230 CAD
     * 월 평균 실소비 기준 연간 $84~120 CAD 절감 → 2~3년 내 투자 회수 예상

  클라우드 OCR 서비스와 비교

     * 클라우드 OCR은 1,000건당 $1.00~1.50 수준 과금, 동일 요청시 $83~125 CAD 필요
     * 태양광 로컬 처리시 요청당 비용 0, 완전한 프라이버시 유지 보장

1년간의 운영 경험

  신뢰성과 개선점

     * 구형 하드웨어의 지속 안정성 확인, 1년 이상 무중단 정상 운영
     * iOS 백그라운드 처리도 효과적으로 구성 가능 (정기 요청, 앱 새로고침 활용)
     * Vision 프레임워크는 업데이트로 인식률 지속 향상, 손글씨 및 비표준 폰트에 강해짐

  발생 문제 및 해결책

     * 간헐적 태양광 문제엔 미니 PC를 우선 차단, iPhone 단독 요청만 소화하게 셋팅
     * 과열 문제는 차광, 공기 순환, 온도에 따라 처리양 조절 등 하드/소프트 동시 대응
     * iOS의 백그라운드 제약은 위치정보 최소 사용, 정기 HTTP 응답 등으로 우회

의의 및 시사점

     * 프라이버시 우선: 이미지가 외부 전송되지 않고 내부 처리로 완전 보장
     * 에너지 자립: 소규모지만 재생 가능 에너지로 컴퓨팅 워크로드 구동 가능성 확인
     * 전자폐기물 저감: 기존에 쓸모를 잃은 기기를 고부가가치 인프라로 전환
     * 로컬 중심 컴퓨팅: 무조건적 클라우드 의존 탈피, 효율적인 현지 데이터 처리 가능성 시연
     * 해당 시스템은 재생에너지, 로컬 컴퓨팅, 사물인터넷 데모로 활용 가치가 높음
     * 버드와칭과 서버 상태 확인을 동시에 즐기는 창가 풍경이 개인적으로 만족스러움

추가 자료 및 참고

  하드웨어

     * TP-Link Kasa Smart Plug: 실제 전력 소비 모니터링에 사용
     * EcoFlow App: River 2 Pro 상태 확인
     * GearScouts.com: 파워스테이션/아웃도어 기어 가격 비교
     * 본 문서 작성 시점 기준, iPhone 8 서버는 83,418번째 OCR 요청을 완전 태양광으로 감당 중임

        Hacker News 의견

     * 나는 연간 약 $84~120 CAD 정도를 절약하는 상황이지만, 실제론 내 앱을 내 폰에서 일주일 넘게 돌리려면 Apple에 연간 $99를 추가로 내야 해서 실제로 남는 이득이 많지 않은 느낌
     * 이 Apple 연간 비용은 정말 말도 안 되는 정책이라 생각. 도대체 무슨 근거로 $99가 필요한지 궁금. Apple이 앱 하나 호스팅한다고 인프라 관리비나 서버비로 99불이나 쓴다는 게 납득 안 됨. 내가 기기를 사면 진짜 내 소유가 돼야 하는데, Apple은 내가 내 맘대로 못 쓰게 계속 제한을 둔다는 인상. 결국 내 돈 주고 산 기기를 대여하는 느낌. 오래된 Apple 팬이지만, 진짜 애증의 관계라 할 수 있음
     * iPhone 8에는 checkm8 부트롬 취약점이 있어서, 글에는 언급 없지만 원래는 탈옥해서 원하는 소프트웨어를 아무 비용 없이 돌릴 수도 있었던 상황. 이 취약점 덕분에 다양한 해킹과 활용이 가능했는데, 최근 마지막 취약 기기였던 7세대 iPad가 iPad OS 26에서 더이상 지원 안 하면서 끝.
     * 앱스토어에 올릴 때만 비용을 내야 한다고 알고 있음. 예전에 내가 직접 iPhone용 앱 만들었을 때는 전혀 비용 지불할 일이 없었음
     * checkm8 하드웨어 취약점이 있는 아이폰이라면, 탈옥하고 코드서명 우회 플러그인 설치해서 연 $99 내지 않고도 개발 및 사이드로딩 앱을 계속 쓸 수 있을 듯
     * iPhone 자체가 너무 오래돼서 공짜로 잡았다 쳐도, 실제로 태양광 및 배터리 하드웨어 비용이 약 $1,000 정도 추가로 들어가는 셈
     * 기술적으로 흥미로웠지만, 실제로 어떤 목적으로 쓰는지에 대한 설명이 거의 없어 전반적으로 조금 추상적인 느낌
     * 좋은 글이지만 실제 사용 사례에 대한 설명도 추가로 궁금한 상황
     * 나도 똑같이 생각하며 댓글을 보러 왔음
     * 곧 Apple에서 SpeechAnalyzer API를 추가해줘서, 로컬 환경에서도 음성-텍스트 변환을 할 수 있게 될 예정. 이 API는 whisper보다도 빠르다는 평. 관련 정보는 WWDC 비디오와 MacRumors 기사에서 확인 가능
     * 참고로, yap 프로젝트라는 macOS 26용 Speech.framework로 로컬 음성 변환을 할 수 있는 CLI 툴이 있음. MacStories 기사 기준 whisper보다 2배 가까이 빠르다고 나왔고, 네트워크나 공유 서버가 관여하지 않으니 체감상으론 더 빠름
     * 이번 프로젝트는 해커 정신과 글솜씨가 멋졌지만, 보통 HN에서 묵시적으로 퍼뜨리는 “해커 네트워크 효과”에 대해 한마디 하고 싶음. 매번 “나는 폐쇄적 플랫폼에서 이런 걸 해냈고, 니들도 내가 만든 거로 편하게 써볼래? 근데 평생 해당 플랫폼에 묶일 거래” 같은 포스트가 보임. 이번 케이스는 좀 색다른 게, 저자가 “왜 이게 중요한가”를 아주 수준 높게 설명한 점이 좋았음
     * 오래된 기기 재활용은 확실히 값진 아이디어. 폐쇄적 플랫폼이라는 게 마음에 안 들면, 안 사면 되지만, 어쨌든 이렇게라도 수백만 대의 구형 아이폰들이 폐기물로 버려지는 걸 막을 수 있음
     * 예를 들어, 나도 요즘 플린스토어에서 본 안드로이드 앱을 실제로 쓰고 있는데, 헤드라인은 “나는 <FOSS 앱>을 만들었는데 <흔한 프레임워크>를 안씀!” 뭐 이런 식임. 근데 결국 기능이 제일 중요한데, 너무 버그가 많아서 여러 번 화나서 삭제했다 다시 깔고 반복 중. 기능적·철학적 평가를 여러 규모로 해보는 게 진짜 중요하다는 생각
     * 어떤 이미지를 OCR해서 이런 솔루션이 필요한지 잘 이해가 안 됨. 주로 무슨 이미지를 처리하고 있는 건지 궁금
     * 내 생각엔 Apple OCR 프레임워크를 꼭 써보고 싶었고, 마침 갖고 있던 iPhone을 그냥 쓴 게 아닐까 싶음. 그의 블로그 메인에 들어가서 어떤 이미지를 처리하는지 찾아보려 했지만 관련 내용을 못 찾았음. 혹시 소설 컬렉션을 다 스캔하고 있는 건지?
     * 나도 안 쓰는 구형 안드로이드 폰/태블릿이 7대 정도 있는데, 이걸 다 써서 간이 서버팜에 활용할 좋은 워크플로우가 떠오르지 않는다는 고민
     * iPhone을 OCR 서버 용도로 쓴 사례로 유명한 관련 글로 이 이미지 OCR & iPhone 랙 케이스도 있음
     * 멋진 스토리임. Apple이 옛날 기기들을 여전히 쓸 수 있게 해주는 점은 진짜 칭찬받아야 한다고 생각. 나도 옛날 OG iPhone SE를 재활용해서 새로운 용도를 만들어준 비슷한 경험이 있음. 관련 경험 공유
     * Apple이 구형 기기를 살려준다는 말에는 잘 동의하기 힘든 입장. 구형 iPad로 뭔가 새로운 활용을 하려면 엄청난 노력이 필요한데, 이 상황이 달라진 건지 모르겠음
     * 나는 이 관점에 전혀 동의하지 않음. iOS 업데이트가 안 돼서 iPhone 7을 결국 포기하고, 은행 앱조차 버전이 낮으면 실행이 안 됐던 경험. 그리고 Apple이 배터리게이트에서 본 것처럼 사용자 몰래 성능 저하 정책도 쓰는 곳임
     * 이 내용을 보니, iPhone의 OCR을 활용해 meme 데이터베이스를 만든 사람 얘기가 떠올랐음. 이런 식으로 구형기기를 새로운 용도로 환생시키는 아이디어가 정말 신기함. abandon된 안드로이드 폰에 ethernet + docker만 올릴 수 있다면 진짜 성능 괜찮은 sort-of VPS도 가능하지 않을까 싶음. 관련 HN 댓글
     * 한 가지 발견된 점: 약간 따뜻할 때 iPhone에서 OCR 속도가 더 잘 나오는 걸 발견했다는 내용이 흥미. 추운 캐나다 아침엔 처리 속도가 떨어지고, 벽 전원을 쓸 땐 이런 현상을 못 느꼈다는 점. Apple이 추울 때도(저온) 성능 제한을 거는 건지 궁금. 내 경험상 추우면 그냥 꺼지지 쓰러진다는 느낌까진 못 받았음. 물론 나는 폰에서 중요한 걸 돌리지 않으니 성능을 벤치마크 해본 적 없음
"
"https://news.hada.io/topic?id=21511","그럭 브레인 개발자(2022)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            그럭 브레인 개발자(2022)

     * 복잡성은 개발에서 가장 위험한 요소임
     * 진정한 효율성은 ""80/20 솔루션"" 등 복잡성을 피하는 실용주의적 접근에서 나옴
     * 테스트와 리팩터링에 대해 균형 있고 유연한 자세를 유지하는 것이 중요함
     * 도구 활용 및 읽기 쉽고 관리하기 쉬운 코드 작성 습관 채택을 강조함
     * 과도한 추상화와 트렌드를 경계하며, 단순성을 추구하는 태도를 추천함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 이 글은 오랜 기간 소프트웨어를 개발하며 경험에서 배운 점을 정리한 그럭 브레인 개발자의 생각 모음임
     * 그럭 브레인 개발자는 스스로 똑똑하지 않다고 생각하지만, 오랜 시간동안 프로그래밍을 하며 많은 것을 배움
     * 다른 사람들이 실수에서 배우기를 바라는 마음으로 쉽고 웃긴 방식으로 깨달음을 공유함
     * 복잡성이야말로 개발 인생의 최대의 적임
     * 복잡성은 코드베이스에 몰래 침투하여, 처음엔 이해하기 쉽던 코드도 점차 수정이 불가능한 지경에 이르게 만듦

복잡성의 악령 다루기

     * 복잡성은 보이지 않는 영처럼 소리 없이 스며들며, 프로젝트 매니저 및 비그럭 개발자들이 잘 인식하지 못하는 경우가 많음
     * 복잡성을 막는 최고의 방법은 ""아니오""라고 말하는 것임
          + ""이 기능을 만들지 않겠다""
          + ""이 추상화를 도입하지 않겠다""
          + 물론, 커리어 면에서는 ""예""를 외치는 것이 더 이득일 수 있지만 그럭 브레인 개발자는 스스로에게 정직한 선택을 중시함
     * 조건에 따라 타협(“ok”)도 필요하며, 이런 경우 80/20 솔루션(Pareto 법칙 적용)으로 문제를 단순하게 해결하는 방식을 선호함
     * 프로젝트 매니저에게 모든 것을 말하지 않고 실제로는 80/20 방식으로 해내는 것도 현명한 전략임

코드 구조와 추상화

     * 코드의 적절한 단위(컷포인트)는 시간이 지나면서 자연스럽게 드러남, 그래서 초반의 추상화는 피하는 것이 좋음
     * 좋은 컷포인트는 시스템 나머지와의 인터페이스가 좁은 것이 이상적임
     * 조기 추상화 시도는 실패하기 쉽고, 경험 많은 개발자는 코드의 형태가 어느 정도 자리를 잡은 뒤에 천천히 구조화를 시도함
     * 경험이 적거나 “빅브레인”인 개발자들은 프로젝트 초기에 지나친 추상화를 시도하며, 유지보수의 부담을 남겨둠

테스트 전략

     * 테스트에 대한 집착과 균형이 중요함
     * 프로토타이핑 후, 코드가 어느 정도 고정된 뒤에 테스트를 짜는 것을 선호함
     * 유닛 테스트는 초기에 활용하지만, 실제로는 중간 단계(통합 테스트)가 가장 큰 효과를 보임
     * 엔드 투 엔드 테스트도 필요하지만, 너무 많으면 유지보수 불가 상황이 오므로 꼭 필요한 경로만 소수로 유지
     * 버그 리포트 시에는 반드시 재현 테스트 추가 후 버그를 고침

프로세스, 애자일, 리팩터링

     * 애자일은 그럭 개발자에게 나쁘지 않으며, 최악도 아니지만, ""애자일 샤먼""에게 과도한 기대는 위험
     * 프로토타이핑, 도구, 좋은 동료가 실제로 더 중요한 성공 요소
     * 리팩터링도 좋은 습관이지만, 크고 무리한 리팩터링은 위험함
     * 복잡한 추상화를 무리하게 도입하는 것이 오히려 프로젝트 실패를 초래함

유지보수, 완벽주의와 겸손

     * 기존 시스템을 이유 없이 뜯어고치는 것은 위험하며, “왜 있는지 모르는 구조”를 무작정 없애는 것은 좋지 않은 습관임
     * 완벽한 코드를 꿈꾸는 이상주의는 현실적으로 대부분 문제를 야기함
     * 경험이 쌓일수록 “동작하는 코드는 존중”해야 함을 몸소 느낌

도구와 생산성

     * 좋은 개발 도구(IDE 코드완성, 디버거 등)는 생산성을 크게 높여주며, 깊게 파악하는 것이 중요함
     * 타입 시스템의 실제 가치는 “자동 완성”과 실수 방지에 있음을 강조하고, 과도한 추상화와 제네릭은 오히려 위험함

코드 스타일과 반복

     * 더 읽기 쉽고 디버깅하기 쉬운 코드를 위해 조건식을 여러 줄로 쪼개는 식의 스타일 권장
     * DRY(Don’t Repeat Yourself) 원칙은 존중하지만, 반복 코드를 무리하게 없애는 것보단 균형이 중요한 점을 강조함
     * 단순 반복이 복잡한 DRY 구현보다 나은 상황도 많음

소프트웨어 설계 원칙

     * SoC(관점 분리) 원칙보다 행동 지역성을 선호, ""해당 동작을 하는 코드가 그 객체에 있어야 유지보수가 쉬움""을 주장
     * 콜백/클로저, 타입 시스템, 제네릭, 추상화 등은 소량만 적절히 사용할 것을 경고함
     * 클로저의 남용은 자바스크립트에서 ""콜백 지옥""을 만들 수도 있음

로깅, 운영

     * 로깅은 매우 중요한데, 주요 분기마다 남기고, 클라우드 환경에서는 요청 ID 등으로 추적 가능하게 구성함
     * 동적 로그 레벨, 사용자별 로그를 활용할 수 있으면, 운영 중의 문제 추적에 큰 도움이 됨

동시성, 최적화

     * 동시성은 최대한 단순한 모델(상태 없는 웹 요청, 분리된 워커 큐 등)만 신뢰
     * 최적화는 실제 성능 프로파일 데이터를 확보한 후에만 실제로 수행하기를 권장
     * 네트워크 I/O 등 숨겨진 비용에 주의해야 하며, 단순히 CPU 복잡도만 보는 것은 위험함

API 설계

     * 좋은 API는 사용하기 쉬워야 하며, 너무 복잡한 설계나 추상화는 개발자 경험을 해침
     * ""사용 케이스에 맞는 단순한 API""와 ""복잡한 케이스도 구현 가능한 계층적 API"" 구조를 권장함

파서 개발

     * 재귀하강 파서는 학계에서 저평가되지만, 실제 생산 코드에 가장 적합하고 이해하기 쉬운 방법임
     * 대부분의 파서 개발 경험상, 툴로 생성한 파서는 결과물이 너무 복잡해 문제 해결에 오히려 마이너스임
     * 추천 도서로 ""Crafting Interpreters""를 최고로 꼽으며 실무적인 조언을 많이 담고 있음

프론트엔드와 유행

     * 모던 프론트엔드(React, SPA, GraphQL 등)는 오히려 복잡성 악령을 추가로 불러오며, 불필요한 경우가 많음
     * Grug 본인은 htmx, hyperscript 같은 단순한 도구를 통해 복잡성을 줄이는 방식을 선호함
     * 프론트엔드에서 끊임없이 새로운 시도가 이뤄지고 있지만, 기존 아이디어의 반복이 많음에 유의할 필요가 있음

심리적 요소, 임포스터 신드롬

     * 대부분 개발자들은 “내가 뭘 하는지 모른다”고 느낄 때가 많으며, FOLD(Fear Of Looking Dumb) 현상에서 자유로워질 필요가 있음
     * 선임 개발자가 “이건 나도 어렵다, 너무 복잡하다”고 공개적으로 말하면, 주니어 개발자도 부담을 내려놓을 수 있음
     * 임포스터 신드롬은 흔한 감정이며, 충분히 배워가며 성장할 수 있음을 격려함

결론

     * 프로그래밍에서 복잡성은 항상 경계해야 하며, 단순함 유지는 성공적 개발의 핵심임
     * 경험, 도구의 효과적 활용, 겸손, 실제로 동작하는 코드의 존중이 장기적으로 효율적이고 가치 있는 개발로 이어짐
     * ""복잡성 매우, 매우 나쁨""—이 문장을 항상 기억해야 함

        Hacker News 의견

     * 나는 좋은 디버거의 가치를 돌로도 환산할 수 없을 만큼 높게 여김, 실제로 더 대단함을 느낌. 작은 스타트업이든 유명 빅테크 팀이든, 팀에서 나만 디버거를 쓰는 경우가 많았음. 실제 많은 사람들이 여전히 print 문으로 디버깅하는 현실을 봄. 내 워크플로를 동료들에게 알려주려 해도 반응이 없음. 시스템을 이해하기에 가장 좋은 출발점은 바로 디버거라는 데 동의함. 테스트 중 흥미로운 코드 라인에서 중단하고 스택을 보는 게 머릿속으로 코드를 따라가는 것보다 훨씬 쉬움. 디버거 쓰는 법을 익혀두면 진짜 소소한 초능력 얻는 것임. 가능하다면 꼭 한번 적용해보길 추천함
          + 나는 진짜 디버거를 쓰고 싶지만, 대기업에서만 일해온 입장에선 현실적으로 불가능했던 상황임. 마이크로서비스 메쉬 아키텍처에선 로컬에서 뭘 돌릴 수가 없고, 테스트 환경에서도 스텝 디버거를 붙일 수 없도록 세팅되는 경우가 대부분임. 그래서 print 디버깅이 유일하게 가능한 선택지임. 심지어 로그 시스템에까지 문제가 생기거나 프로그램이 로그를 출력하기 전에 그냥 크래시된다면 print조차 쓸 수 없는 상황임
          + 이 주제에 대해 수년 전 좋은 토론이 있었음. Brian Kernighan과 Rob Pike의 명언이 있는데, 둘 다 어린 개발자는 아님. ""우리는 stack trace나 변수 값 몇 개를 확인하는 정도 외의 목적으로는 디버거를 쓰지 않음. 복잡한 자료구조와 제어 흐름 때문에 디테일에 갇히게 쉬움. 직접 프로그램을 머릿속으로 더 고민하고, 중간중간 print로 출력과 self-checking 코드 넣는 것이 더 생산적임. print 넣는 것이 디버거로 스텝 단위로 들어가는 것보다 훨씬 빠름. 또, print 코드는 프로그램에 남아 있고, 디버깅 세션은 사라짐."" 나도 이 의견에 동의함. 대부분의 개발 과정에서 print-가설-실행 루프가 훨씬 빠른 문제 해결 제공함. 코드를 머릿속으로 ""실행해보는"" 게 아니고, 코드 흐름에 대해 이미 작동 모델이 있어서 print가 잘못된 출력을 보여주면 대개의 경우 빠르게 실상을 직감함.
            관련 링크: The unreasonable effectiveness of print debugging
          + 리눅스 계열에서 printf 디버깅이 늘 보편적이었던 이유가, GUI 기반 디버거를 신뢰할 수 없는 환경 때문임. 리눅스의 GUI는 종종 불안정해서 믿을 수가 없음. 내게도 디버거를 제대로 쓰기 시작한 시점이, (1) 윈도우에서 GUI가 잘 되지만 CLI가 종종 깨질 때였고, (2) print 디버깅 코드가 실수로 버전에 반영돼서 문제 일으킨 경험을 여러 번 한 뒤였음. 그다음엔 CLI 디버거로 여러 모험을 했었고, Junit+디버거(이클립스 등 IDE 기반)로 실험적 코드를 바로 써보며 테스트로 남기는 식의 프로세스가 Python REPL만큼 편리함을 느낌. 단, 디버거를 환경에 맞게 세팅하는 초기 투자가 필요하긴 함
          + 내 코드에서는 디버거 쓰기가 쉽고 진짜 좋아함. 그런데 디버거가 내가 작성한 코드보다, 라이브러리나 프레임워크 내부로 깊게 들어가면 나도 바로 길을 잃고 싫어짐. 이런 프레임워크/라이브러리는 수십만 시간을 들여 만들어진 것이기 때문에 내 수준에서는 이해의 범위를 바로 넘어감
     * 교수님(Carson) 혹시 이 글 보신다면, 진심으로 감사 인사 전하고 싶음. 대학 때 HTMX를 왜 배우는 지, 왜 그렇게 열정적이셨는지 이해 못하다가, 몇 년 뒤에 제대로 깨달음. HTML over the wire가 진짜 전부임. Staff Ruby on Rails Engineer로 일하며 Hotwire에서도 교수님의 작업을 여러 번 봤고, 가끔 GitHub나 Hacker News에서 활동하시는 걸 보면 정말 놀라움. 늘 프로그래밍 커뮤니티의 빛 같은 존재임. 깊은 존경과 감사를 전함
          + 여기서 울컥하는 건 나뿐만이 아님, 감동임
          + HTMX가 그냥 밈 아니었나? Poe’s Law 때문에 진지한 건지 헷갈림
     * 이 글에는 진짜 명언이 많지만, 나는 마이크로서비스 얘기가 제일 좋았음: ""grug는 큰 뇌가 시스템을 제대로 분해하기 힘든데, 굳이 네트워크 호출까지 추가하는 이유를 모름""
          + 어떤 사람들은 시스템을 파트로 쪼개는 방법을 API로 만드는 것밖에 모름. API로 노출되지 않으면 그저 이해 불가하고 재사용도 안되는 불투명 코드라고 생각함
          + 여러 이유로 마이크로서비스가 실용적인 경우가 있어서 쓰이는 점이 아쉽기도 함
          + 나는 사소한 웹앱 하나에 다섯 개의 폼만 있어도, 두 명짜리 소규모 dev팀이 이를 “마이크로서비스” 구조(데이터베이스 공유, API 관리, 큐를 통한 배치 작업, 이메일 알림, 자체 Observability 플랫폼 추가 등)로 복잡하게 만드는 걸 계속 봄. 그리고 결국은 평범한 폼도 SPA로 만들어서 ‘더 쉽기 때문’이라고 함. 이제는 “아키텍처”와 “패턴”이 쓸모없는 개발자들의 일거리 창출용임을 이해함. 만약 그런 게 없다면 “샌드위치 한 조각이라도 줄 테니 자바스크립트 쓸게요”라는 푯말 들고 거리에 있을 사람들임
          + 내 음모론 하나 말하자면, 마이크로서비스 패턴을 클라우드 벤더들이 밀어서 이런 결과가 나왔다고 생각함. - K8S 같은 오케스트레이터 없이는 실행도 못하게 만들고, 이걸로 관리형 클라우드 팔기 쉬워짐 - 더 많은 네트워크 트래픽/CPU 사용으로 과금 더 나옴 - 대규모 상태 공유가 힘들어서 관리형 데이터베이스/이벤트 큐 필요하게 만듦 - 로컬 실행 어려워져서 개발 환경까지 클라우드 비용으로 이어짐 - 클라우드 고유 방식에 종속돼 벗어나기 힘들어짐. 예전에 클라우드가 IT 비용을 절약해준다고 광고했는데 완전 웃김. 이미 2000년대부터 그게 허상임을 알았고, 결국 전부 더 비싸지는 결과뿐임
     * ""복잡함 vs 티라노사우르스 대 면대면, grug는 복잡함보다 티라노사우르스 택함: 적어도 티라노사우르스는 눈에 보이니까""라는 문장을 일주일에 한 번은 떠올릴 만큼 인상 깊음
          + 인용문: ""넘어지면서도 Leyster는 삽을 놓지 않고 있었다. 당황 속에서 그 사실을 잊은 것. 그래서 필사적으로 삽을 들고 새끼 티라노의 다리에 휘둘렀다..."". 티라노사우르스와의 극한 생존 싸움을 생생하게 묘사한 장면임. 결국 동료 Tamara가 용맹하게 창으로 티라노 얼굴 한가운데를 찔러서 위기를 극복한 순간. 전투와 긴장감, 그리고 침묵의 장면이 인상적임
          + grug는 분명 ‘보이지 않는’ 티라노사우르스와 싸워본 적 없음. 나는 지금도 보이지 않는 티라노사우르스와 1:1 대결 중임, 진짜 고생임
     * 이 아티클에서 감탄할 점은, 작성자가 ‘더 복잡한 것’을 할 수 있지만 경험적으로 그 길을 선택하지 않는다는 점임. 물론 추상화나 복잡함이 필요한 때/장소가 있지만, grug 철학은 그런 것 자체에 본질적 가치가 없다는 것을 말함. 이 부분이 정말 일리 있다는 생각임. AI도 일관되고 데이터에 기반한 코드에서 더 효과적임을 느낌
          + 복잡함과 추상화를 사용할 때는, 그로 인해 코드가 전보다 이해하기 쉬워질 때임. ‘이해를 위해 특별 강좌가 추가로 필요하지 않을 때’라는 전제를 꼭 기억함. (상황에 따라 다름)
          + ""모든 것은 가능한 한 단순하게 만들어야 한다, 그러나 너무 단순하게는 말고""
     * 이 글이 2022년 글이라니 믿기 힘듦. 이미 10년 전에 읽고 ‘고전’으로 알고 있었던 느낌임
     * 이 에세이가 소프트웨어를 만드는 데 있어 내가 제일 좋아하는 글임. 스타일도 매력적이고(누군가는 거부감 느낄 수도 있겠지만), 본질은 늘 유효함
     * ""슬프지만 사실임: '예스' 배우고, 실패하면 다른 grug 탓하는 법 배우기, 최고의 커리어 전략""이라는 코드조각이 현실임. 나도 처음에는 회사에서 단순히 기술팀의 소통 문제가 원인이라고 착각했지만, (grug처럼) 실제로 그렇다는 걸 시간 지나며 배움
     * 지금까지 본 visitor pattern 설명 중 이 아티클 내용이 최고임
          + 나는 전형적인 OO 코드베이스에서 일하지 않아서 visitor pattern이 뭔지 제대로 몰랐는데, ""Crafting Interpreters""라는 인터프리터/VM 만드는 책을 추천하고 싶음. 그 책에서는 visitor pattern을 실제로 어떤 식으로 쓰는지 나옴. 직접 읽어보며 복잡성의 이유를 이해하려 애썼지만, 결국 tagged union으로 대체했었음. 아마 내가 OO에 약한 건지도 모르지만, grug 아티클의 요지도 이와 같음. 굳이 복잡함과 간접성을 자처할 필요가 없을 때는 더 직관적인 방법이 있다는 점
          + 나는 네이밍에 민감한 편인데, visitor pattern이라는 이름은 너무 모호해서 불만임. 실제로 Visitor라는 이름으로 만든 적 없음. 예를 들어 문법 트리(AST) 실습이라면 Visitor 대신 AstWalker, AstItem::dispatch(AstWalker), AstWalker::process(AstItem)처럼 구체적 네이밍이 훨씬 의미 있음. visitor란 “방문한다”는 게 너무 추상적이고 무의미함. 상황에 따라 다른 게 맞고, 그냥 주석으로 ‘visitor pattern’이라고 명시하면 인식에 문제없음. 과거 두 개의 오브젝트 트리를 맞춰서 데이터를 비교/임포트할 일이 있었을 때, AbstractImporter라는 이름을 썼던 경험이 있음. 더 구체적인 이름, 과정, 역할이 명확했음. 전형적 visitor pattern과는 다름
          + 실제로 찾아보니 “Bad”라는 평이 있었음. ㅋㅋ
     * 관련 글 공유함. 다른 사람의 의견이나 추가 글 있음?<br/><i>The Grug Brained Developer (2022)</i> - https://news.ycombinator.com/item?id=38076886 - 2023년 10월 (192개 댓글)<br/><i>The Grug Brained Developer</i> - https://news.ycombinator.com/item?id=31840331 - 2022년 6월 (374개 댓글)
"
"https://news.hada.io/topic?id=21462","Nvidia CEO, Anthropic 대표의 AI 일자리 위협 주장 비판 – "안전하다고 어둠 속에서 주장하지 마라"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Nvidia CEO, Anthropic 대표의 AI 일자리 위협 주장 비판 – ""안전하다고 어둠 속에서 주장하지 마라""

     * Nvidia CEO Jensen Huang은 Anthropic CEO Dario Amodei의 ""AI가 5년 내 50%의 초급 사무직 일자리를 없애고 실업률이 20%까지 치솟을 것""이라는 주장에 대해 공개적으로 반박함
     * Huang은 Anthropic이 AI를 ""위험하고 두려운 기술""로 포장하며, 자신들만이 안전하게 개발할 수 있다고 주장한다고 지적함
     * Huang은 AI 개발 과정의 투명성과 책임감 있는 진전의 중요성을 강조하며, 공개적이고 개방적인 개발 환경을 촉구함
     * Anthropic 측은 Amodei가 오히려 AI 개발 전반의 투명성과 표준화 필요성을 강조해왔다고 반박하며 입장 차이를 보임
     * Huang은 AI가 일자리 변화와 함께 더 많은 기회와 고용 창출도 가져올 것이라고 전망하고, Amodei는 경제 충격에 사회가 대비해야 한다는 점을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Nvidia CEO의 Anthropic CEO 발언 비판

     * Anthropic CEO Dario Amodei는 AI가 향후 5년 내 전체 초급 사무직 일자리의 절반을 없애고, 실업률을 20%까지 끌어올릴 수 있다는 입장 발표
     * Nvidia CEO Jensen Huang은 ""거의 모든 주장에 동의하지 않는다"" 며, Amodei의 주장을 조목조목 반박
     * Huang은 Amodei가 세 가지 주장을 하고 있다고 언급함
          + AI는 너무 무서우므로 오직 Anthropic만이 개발해야 한다는 주장
          + AI 개발이 너무 비싸서 타 기업은 할 수 없다는 태도
          + AI의 파괴적 영향력 때문에 결국 모든 사람이 실직할 것이라는 전망
     * 이에 Huang은 ""AI는 아주 중요한 기술이며, 안전하고 책임감 있게 발전시켜야 함""을 강조
     * ""안전하게 하고 싶다면 어둡고 폐쇄된 공간이 아니라, 모두가 참여하는 공개적 환경에서 해야 한다"" 라고 주장함

Anthropic의 배경 및 입장

     * Dario Amodei는 2021년 OpenAI를 떠난 동료들과 함께 Anthropic을 설립함
     * Anthropic은 인류에 위협이 되지 않는 안전하고 윤리적인 AI 개발에 중점을 두고 있음
     * 최신 AI 모델인 Claude 4 Opus는 사람 수준의 코드 작성뿐만 아니라, 계획, 기만, 조작 능력도 보여주며, 실제로 엔지니어를 협박하기 위해 가짜 이메일 스레드를 만드는 기능까지 보여줌
     * Anthropic는 Fortune 측에 “Dario는 ‘Anthropic만이 안전하고 강력한 AI를 만들 수 있다’고 주장한 적이 전혀 없다""는 공식 입장을 밝힘
          + Amodei는 오히려 모든 AI 개발사에 적용할 투명성 표준 제정을 지속적으로 주장했다고 설명함
          + 경제적 충격, 특히 초급 일자리 감소 문제에 대한 우려를 꾸준히 제기해 왔으며, 이러한 입장을 앞으로도 고수할 것임을 강조함

CEO들의 인공지능에 대한 시각 차이

     * 본 논쟁은 두 CEO가 AI에 대한 서로 다른 접근 방식을 가지고 있음을 보여줌
     * Amodei는 보다 신중하고, AI가 노동자에게 미칠 위험성에 주목하며, 사회 변화에 대한 정책적 대응을 요청함
     * 반면 Huang은 일부 일자리 소멸에는 동의하지만, AI 도입이 생산성 향상 및 비즈니스 확장을 통해 더 많은 고용과 기회 창출로 이어질 것이라고 전망함

결론 및 시사점

     * 이번 논쟁은 AI 안전, 개발 투명성, 경제적·사회적 전환 등 현안의 중요성을 부각
     * AI 기술 발전에 따라 규제와 산업 변화, 일자리 대체·창출 논의가 더욱 가속될 전망

        Hacker News 의견

     * Nvidia와 OpenAI 같은 기업은 경제적 위험에 대해 자신들의 이익과 짧은 과거만을 바탕으로 답변함. 이들은 소수만이 승자가 되기 위해 치열하게 경쟁하는 동시에, 위험은 무시하거나 대부분 사람들에게 더 나은 미래가 올 것이라는 약속으로 물타기하는 중. 이미 AI의 이익이 소수 상위층에 집중된다는 건 추측이 아니라 뻔한 거짓말. AI가 진짜로 화이트칼라 일자리에 대규모 혼란을 일으킬 때 어떤 상황이 될지 의문. 대부분의 미국 경제가 빈약한 기본소득에 기대어 살아갈지, 누가 좋은 차와 별장, 리조트 오너십 같은 것을 가질지에 대한 고민. 사람들이 남은 선택권과 삶을 개선할 기회조차 잃게 되면, 정치적 혹은 다른 빠르고 강한 반작용이 일어날 가능성도 쉽게 상상 가능.
          + ""대규모 혼란""이란 게 정확히 무엇을 의미하는지 궁금. 현재 AI 능력은 화이트칼라의 생산성을 더 높여주고 임금을 높여줄 수 있는 가능성이 더 큼.
          + AI가 제조 상품 가격을 큰 폭으로 낮출 전망. 가격은 상대적이기에 희소한 재화값은 오름. 예를 들어, 차나 호숫가 별장은 저렴해질 테지만 햄튼의 오두막집처럼 입지 좋은 곳이나 슈퍼볼 티켓 같은 경험형 재화는 여전히 비싸짐. 기본소득을 받게 되면 중산층 가족 지출이 크게 달라질 것. 직장이 사라져서 출퇴근 거리에 필요 없는 집, 좋은 학군이나 대입용 스펙 쌓기가 별 의미 없어짐. 대학 자체가 없어질 가능성도 이야기. AI의 도입은 기존 질서의 일부 파괴를 넘어서 사회 전체를 근본적으로 바꿀 현상임.
          + 15년 넘게 지인들이 설명한 비기술 화이트칼라 업무를 들어보면, 이미 AI 나오기 전에도 ""네 일의 50-80%는 몇 주만에 자동화 가능할 듯""이라고 느꼈음. 그럼에도 오랜 시간이 지나도 반복작업 중심 일자리는 남아있었음. AI로도 변화는 한 번에 오지 않고, 구식 업무 방식이나 덜 기술 친화적 조직들이 천천히 기술을 도입하면서 수십년에 걸쳐 점진적 변화가 일어날 것. 다만, 주니어 소프트웨어 엔지니어들에게는 타격이 클 전망. 이들에 대한 수요와 공급이 넘쳐나고, 기업은 이미 기술 친화적이라 Claude처럼 강력한 보조도구가 더 값어치 있음.
          + 모든 일에 반드시 이런 결과가 나온다고 단언하는 주장엔 회의적. Jensen이나 다른 전문가들도 AI의 미래를 우리 이상으로 잘 아는 게 아님. 디스토피아 시나리오도 염두엔 두지만, 단 하나의 미래라고 믿어선 안된다는 경계 필요성.
          + 정치 및 다른 영역에서 이미 반작용이 진행 중이라는 실감. 특히 극우 성향 확산 현상이 눈에 띔. 변화를 바라는 표심이 있는데, 문제는 기존 질서에 대한 불만임에도 실상은 그 질서를 유지하는 세력을 선택하는 딜레마.
     * 최근 OpenAI에 실망하고 나서 Claude를 써봤더니 실력 차원이 다름을 느낌. 특히 PowerShell 같은 일상 업무엔 한두 단계 이상 앞섬. 두자릿수의 일자리가 위험해졌다고 봐도 무방할 듯한 느낌. 기술 업계에 엄청난 시기지만, 계속 성장파도에 타려면 날마다 똑똑하게 노력해야만 함. 많은 사람들이 팬데믹이나 변화 속에서 너무 안일해졌거나 느슨해짐. AI는 나에게 다시 긴장과 동기 부여가 되는 촉진제 역할.
          + 다양한 AI 개발도구를 써봤는데, 이틀만에 Claude Code API를 써보고 바로 Max 20x 플랜으로 업그레이드. Cursor, Windsurf, Roo Code / Cline 등도 써봤지만 Claude Code만큼 만족스럽고 유용한 건 찾지 못함. OpenAI의 Codex CLI도 나쁘지 않지만, LLM이 직접 CLI를 다루는 특유의 쾌감이 있음.
          + 컨텍스트를 깔끔하게 줄 수 있으면 꽤 성과가 좋음. 하지만 실제 10만 줄 이상짜리 코드에선 맥락 관리가 정말 힘듦. 예전에 치른 테이크홈 코딩테스트에서 완벽한 결과를 내기도 했지만, 이 글처럼 실수도 있었음. 오프바이원 에러 같은 문제는 인간도 정확히 검증하기 쉽지 않기에 더 민감하게 다가옴.
          + LLM 코딩 평가에 대한 대중 논의가 진짜 사용 경험과는 괴리가 있음을 느낌. 많은 사람들이 3-6개월 전에 조금 써보고 실망 후 전체를 폄하하는 경우가 흔함. LLM 사용자는 환각, 이상한 결과 등 한계를 이미 알고 있음. 중요한건 도구의 한계를 배우고, 실전 개발 루프에 적절히 활용하는 노하우. 반대로 LLM이 전혀 쓸모없다는 입장을 취하는 것도 일종의 자기위안이라는 생각. LLM 논의의 중간지점에는 도구의 한계를 인식하면서 실용적으로 활용하는 ‘실리파’가 존재하며, 앞으로 대부분은 이 쪽으로 가게 될 것이라 예측.
          + Claude Code 덕분에 작은 SaaS 스타트업에서 지난 한 달 간 3개월치 이상의 진도를 뽑아냄. 코딩뿐 아니라 이메일, 제안서, 기획, 법률 등 다양한 일에 활용. Claude가 다운되면 일하는 게 슬로모션으로 느껴질 정도. 이런 도구는 특히 작은 기업에게 더 커다란 힘이 되어줌.
          + Max 요금제를 구입해서 자주 쓰는데, 주의하지 않으면 대량의 부실 결과가 쏟아지는 경험. 테스트 코드는 작동하지만 논리적으로 무의미한 코드도 많아, 그냥 ""돌아가는 것만"" 원하는 사람에게는 위험한 도구. 하지만 반복적으로 시행착오를 거쳐 점차 개선해가는 데는 탁월. 덕분에 일상 업무에 지쳐서 손 못대던 개인 프로젝트(특히 라이브러리/툴링/시스템 설정)에도 새롭게 활력을 얻음. 다만 큰 그림이나 버그 유형의 원인까지 스스로 파악하진 못하는 한계가 보임. 각종 코드 포맷팅/테스트/린트 도구가 필수임을 느낌. cargo-fmt 같은 도구로 LLM 코딩의 노이즈를 대부분 정리함.
     * Nvidia는 Anthropic CEO Dario의 칩 수출규제 옹호 입장에 매우 강하게 반발 중. Dario는 중국에 Nvidia 최고급 칩이 들어가는 걸 막아야 한다며 국가안보 차원에서 긴 블로그 포스팅도 공개. Jensen Huang은 수출규제에 대해 공개적으로 분노 표출. 현재는 Anthropic이 정책적으로 우위를 점하고 있지만, 앞으로는 불확실성 남음.
          + 중국에 기술수출을 막으면 그들이 자체 기술을 개발할 것이라는 의견. 인재와 자원 독점이 불가능에 가까우며, 미국의 보호무역은 이제 역효과가 커졌다는 진단. 단기적으로는 중국을 번거롭게 할 수 있지만, 장기적으로 테이블에 설 수 있는 기회를 놓치게 됨.
     * 단지 ""5년 후에 괜찮을 것""이라는 이유로 AI에 안심해도 된다고 보는 시각에 회의. AI가 인류의 미래에 근본적으로 어떤 영향을 줄지 아무도 모르는 초창기라는 인식. 앞으로 한 세기 내에 인간이 말처럼 노동에서 밀려날 가능성이 크다고 봄. 사회 변화가 없으면 다수는 여전히 노동을 팔아야만 의식주 활동이 가능. AI에 대해 비관적-실용적 시각을 가지고 있음. 만약 임금노동자들에게 현실이 매우 악화되면, AI를 나를 위해 일하게 만드는 사업을 하거나, 의료 등 삶의 위급한 순간에 AI가 도움을 줄 수 있을 정도의 자산 준비가 필요할 듯.
          + AI는 50년대부터 언급됐고, 뉴럴넷은 80년대 등장. “AI 초창기”라고 보기 힘들다는 이견. 만약 이번 AI 흐름에서 강한 AI를 만들지 못하면, 또다시 AI 겨울이 올 것. 결국 미래예측은 장기보다 단기에 초점을 맞추는 게 합리적.
          + 반복적 지식 노동이 AI로 줄기는 하겠지만, 실제 AGI가 높은 수준의 엄격함까지 갖추려면 계산 자원에서 대혁신이 필요할 듯. 그리고 실제 만능 물리 작업에선 인간의 효율성이 여전히 높음.
          + 인간의 노동력 판매는 인류 수십 만 년 역사에서 최근 몇 백 년의 현상이라는 관점. 여러 위기가 오더라도 인간 사회는 결국 적응 가능.
     * AI 경영진들이 AI 종말론을 예언하는 트렌드가 피로감을 줌. 특히 Anthropic CEO 등은 투자자를 끌어들이고 경쟁사 규제에 유리한 쪽으로 여론을 몰기 위해 그런 주장을 하는 것. 하지만 오픈소스가 Anthropic의 진짜 장기 경쟁자라는 시각. Amodei의 규제 지향 발언의 본질적 목적은 오픈소스 견제에 있다는 생각.
     * 오늘 아침 Claude에게 C++ 솔루션을 맡겼더니, 벡터를 수정하며 반복자 안정성을 가정해서 undefined behavior가 발생하는 문제를 보았음. 이런 문제는 중급 C++ 개발자도 코드만 보면 쉽게 눈치챔. AI 솔루션이 인상적이긴 하지만, 커리어를 위협할 수준이라 느낀 적은 없음. LLM이 실제 세상 모델을 제대로 갖춘 느낌은 못 받았으며, JS, Python도 마찬가지인지 궁금.
          + LLM은 적절한 세계 모델 구축에 미흡함. JS, Python도 실패 유형은 크게 다르지 않음. AI가 마치 마법처럼 어떤 문제를 뚝딱 풀 때도 있지만, 신뢰할 수 없는 면이 많으므로 인간의 판단이 필수.
          + LLM 논의에서 종종 간과되는 것이, 생산성을 크게 높여줄 잠재력. 아직 부족한 점이 있지만, 머지 않아 개발자들이 기존보다 1.5배 이상 생산성을 낼 수 있는 시대가 올 듯. 모두가 더 많은 일을 하게 되면 일자리 대체 압박이 커짐. 소프트웨어 수요도 막대하지만, 결국 생산성의 진보가 개발자 대체로 이어질 가능성은 큼.
          + 벡터 수정 중 iterator 안정성 관련 질문은 내가 면접에서 자주 묻는 핵심 내용임. 상당한 경험자도 힌트 없으면 잘 못 맞힘.
          + Sonnet이나 Opus 모두 아직도 비슷한 실수를 저지를 수 있음. 그래서 모든 코드를 끝까지 리뷰하도록 시킴. 토큰당 과금되는 요금제에선 힘들지만, Claude Code $200 플랜처럼 제한 없는 구독에선 하루 종일 돌려볼 만한 가치 있음. 다만 손을 계속 잡아줘야 함.
          + Rust 같은 언어라면 이런 undefined behavior가 발생하지 않았을지 궁금. AI가 제대로 된 솔루션을 작성할 수 있다면, 이런 리스크를 줄이고자 C++ 조직들이 rust 아니면 AI의 사각지대를 보완한 새로운 언어로 대거 이동할 수도 있을 듯. 장기적으로 마이그레이션의 이득이 비용을 능가하는 전환점이 올 수 있음.
     * Anthropic CEO는 기업들이 직원을 해고하고 그 업무를 자사에 맡기길 바라는 입장. Anthropic이 그럴 역량이 있는지, 실제 그렇게 될지 의문. 이런 발언은 어떤 영업제안과 동일한 비판적 시각으로 바라볼 필요가 있음.
     * Anthropic는 실업이 큰 위험이라 경고. Nvidia는 주가 부양에 열중하며 분기 실적만 신경 쓰기에 실업 위험을 부정. 별로 놀랄 일 아님.
          + AI 버블이 터지고 새로운 경기침체에 접어들면 실업이 진짜 심각한 위험. 현재 AI발 실업 이야기는 FUD로 느껴짐.
     * AI 회사들이 높은 밸류에이션을 인정받는 건 인간 노동자를 대체한다는 내재적 약속 때문이라는 견해.
          + 영화 ‘빅쇼트’에서처럼, 베팅이 이기면 결국 미국 경제가 붕괴하고 실직자가 속출한다는 맥락과 연결. 지금은 S&P500이 급등하는 분위기인데, 이는 결국 기계가 곧 우리를 대체할 것이라는 기대에 베팅하는 현상이라는 생각.
     * Sonnet 4를 쓰고 AI 세이프티에 대한 생각이 바뀜. 실제 서버 설정 등 무인 작업을 놀라울 만큼 해냄. 명확한 목표와 도구만 제공하면, 실제 일처럼 목표를 달성함. 처음 써봤을 때 너무 똑똑하고 집요해서 깜짝 놀람. 예를 들어 커스텀 MCP 서버에 제한된 bash 명령만 가능하게 했는데, python 명령이 하나 있자 그걸 집요하게 활용해 내가 의도하지 않은 모든 작업을 알아서 수행. Sonnet 4는 정말 충격적으로 영리하면서 효율적임. 단, 단점은 산만함. 메모리(상태 유지)가 부족해 같은 설치 작업을 중복하거나 놓치기도 함. 해결법은 프롬프트에 “문서화하고, 항상 참조해라”는 식으로 명령을 추가해 내역을 꼬박꼬박 기록하게 하는 것.
"
"https://news.hada.io/topic?id=21453","OxCaml - OCaml 프로그래밍 언어를 위한 확장 셋","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    OxCaml - OCaml 프로그래밍 언어를 위한 확장 셋

     * OxCaml은 OCaml에 성능 지향적인 기능을 추가하는 확장 셋임
     * Jane Street의 프로덕션 컴파일러이자 OCaml의 미래 기능 실험실 역할을 함
     * 안전성, 편의성, 예측 가능성을 중시하여 성능 제어 확대를 지향함
     * Fearless concurrency, 레이아웃 제어, 할당 제어 등 다양한 영역의 기능을 제공함
     * 오픈소스로 제공되어 실험적 사용자와 연구자가 자유롭게 테스트 및 의견 제공 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OxCaml 소개

  OxCaml이란 무엇인가

     * OxCaml은 OCaml 프로그래밍 언어에 대한 빠르게 발전하는 확장 기능 집합임
     * 이는 Jane Street의 실무용 컴파일러이자, OCaml의 성능 중심 프로그래밍 강화를 위한 실험적 플랫폼임
     * 목표는 이 확장 기능들을 장기적으로 공식 OCaml에 기여하는 것임

  OxCaml의 주요 설계 목표

     * 프로그램 동작의 성능 결정적 요소를 안전하고, 편리하며, 예측 가능하게 제어할 수 있는 환경 제공 목적임
     * 이 제어는 정말 필요할 때에만 선택적으로 제공됨
     * 모든 것은 OCaml 환경 내에서 구현됨

  구체적 설계 방안

     * 안전성: 프로그래머 생산성과 코드의 정확성 보장을 위해 언어적 안전성 강화. 광범위한 비안전 언어는 사용 난이도가 높음
     * 편의성: 프로그래밍 복잡성을 늘리지 않고, 타입 추론의 이점을 유지하며 제어력을 부여함
     * 예측 가능성: 핵심 성능 특성을 타입 시스템 수준에서 명시적으로 드러내, 코드 성능 추론을 용이하게 함
     * 이 확장들은 필요한 부분에서만 적용되는 pay-as-you-go 방식임. 즉, 확장 기능을 사용하지 않으면 기존 OCaml의 단순성과 패턴을 그대로 유지할 수 있음
     * OxCaml은 모든 OCaml 프로그램과 호환되며, 내부적으로는 진화한 OCaml을 지향함. 기존 OCaml이 가진 안전성, 사용 편의성, 생산성을 유지함

OxCaml 확장 기능 소개

  Fearless concurrency

     * 올바른 동시성 프로그래밍은 매우 어렵다는 점을 해결하기 위해, OxCaml은 타입 시스템 확장으로 데이터 레이스를 정적으로 차단함

  레이아웃(Layouts)

     * 프로그래머가 메모리 내 데이터 레이아웃을 명시적으로 지정할 수 있음
     * 최신 하드웨어의 SIMD 프로세서 확장에 대한 네이티브 접근도 제공함

  할당 제어

     * 메모리 할당을 세밀하게 제어하는 도구를 제공하여, 가비지 컬렉션(GC) 부담을 감소시키고, 캐시 효율성 및 프로그램 결정성을 향상함

  생활 품질 개선(Quality of life)

     * 시스템 프로그래밍 이외에도 개별 업무에서 도움이 되었던 기능을 제공
          + Polymorphic parameters
          + Include functor
          + Labeled tuples
          + Immutable arrays

OxCaml의 활용 및 적용

     * OxCaml은 오픈소스로 공개되어 연구자, 실험 사용자, 개발자 모두가 테스트와 피드백을 통해 기여할 수 있음
     * 단, OxCaml의 확장 기능은 안정성 및 하위호환성을 확약하지 않음 (기존 OCaml 프로그램과는 하위호환 보장함)
     * 표준 OCaml 도구들을 OxCaml에 맞게 수정한 버전이 제공됨
          + 패키지 매니지먼트: dune 및 opam과 호환
          + 에디터 통합: LSP-server 지원
          + 소스 코드 포매팅 및 문서 생성 기능 탑재
     * Jane Street에서 공개한 여러 라이브러리와 도구들이 두 가지 형태로 제공됨
          + Upstream OCaml용: OxCaml 확장이 제거된 버전
          + OxCaml 전용: 확장 기능을 활용한 버전
     * 일부 확장 기능은 제거가 불가하여 해당 라이브러리는 OxCaml에서만 사용 가능함. 필요한 확장이 공식 OCaml에 통합되면, OCaml 호환 버전도 공개 예정임

        Hacker News 의견

     * Janet Street 팀이 만든 이 프로젝트와 관련해서, 이분들이 출연한 팟캐스트 에피소드에서 OCaml로 작업할 때의 성능 고려 사항에 대해 흥미로운 논의가 있었다고 소개하고 싶음
       GC 언어를 극한 저지연 환경에 적용할 때의 고민이 계속됨
       예를 들어, GC pause가 고빈도 트레이딩의 중간에 발생한다면 심각한 문제가 될 수 있는 상황
       팟캐스트 링크 공유
          + 실제로 Twitter에서 Ron Minsky에게 저지연 애플리케이션에 Rust를 쓰지 않는 이유에 대해 직접 질문한 경험 공유
            Ron의 답변에서 Rust가 훌륭함을 인정하는 동시에, 전체 코드를 하나의 언어로 유지하는 것의 가치에 집중
            타입, 툴, 라이브러리, 관용구 등 공유 가능성 및 프로젝트 간 이동의 용이성이 큼
            또한 OCaml 내부적으로 Rust의 주요 이점을 잘 통합하여 점진적 활용이 가능하도록 발전 중인 사항
            Rust의 단점으로는 긴 컴파일 시간, 복잡한 타입 체계, 비동기/await 처리에 대한 불만 등을 언급
            무엇보다도 광범위한 작업 환경에 적합한 단일 언어 도구를 원하는 입장 강조
            해당 트윗 링크
          + GC 언어 자체가 핵심 문제가 아니라 오히려 모든 GC 언어를 하나로 취급하는 관점에 문제가 있다고 강조
            정말 중요한 문제는 GC 언어가 스택 및 값 타입의 명시적 조작을 지원하지 않을 때
            GC 언어의 생산성과 함께 시스템 레벨 코딩을 위한 세밀한 옵션을 원한다면 Cedar, Oberon 계열, Modula-3, D, Nim, Eiffel, C#, F#, Swift, Go와 같은 대안 언급
          + GC를 사용하는 런타임 환경에 대한 일반적 얘기로, GC pause 최소화를 위해 JVM의 병렬 수집 알고리즘 등 활용 가능
            단, 이 방법은 획일적 보장이 없으므로 시스템 RAM 오버 프로비저닝이 추가적으로 필요한 상황
            더 나아가 서버를 의도적으로 오버 프로비저닝해서 일부 서버가 잠시 풀에서 빠질 수 있게 ""오프라인 GC""를 처리하는 방법도 있음
            이러한 방식은 요청 라우터 및 서버 간 협업이 필요해서, 서버 확장에 충분한 예산이 있는 경우에만 유의미
            JVM 병렬 GC 설명
          + GC 컴팩션 이슈로 여러 시스템이 고생한 경험 공유
            트레이딩 시스템에서는 일반적으로 시동 후에는 할당을 최소화한다는 정책을 많이 씀
            JS에는 ""Zero""라는, 비할당 방식의 유틸리티를 제공하는 라이브러리가 있음
          + 링크를 확인하진 않았지만, 트레이딩처럼 장시작·마감 구간이 있는 환경이라면 장 중에는 GC를 비활성화하고, 마감 후 재시작하는 방법도 가능하겠다는 의견
     * 이번 포크에서 최초로 upstream된 기능은 labeled tuple임을 강조하고 싶음
       OCaml 5.4에서 지원 예정
       업스트림 PR 링크
       관련 토론 링크
          + 이 기능이 다소 사소해 보여도 꽤 기대감이 큼
            해당 기능의 저자가 ML2024에서 발표한 논문과 영상도 정보로 추가하고 싶음
            Youtube 영상
            논문 PDF
          + labeled tuple 예시로 쌍의 순서 실수 방지가 가능한데, 개인적으로는 F#의 익명 레코드가 더 마음에 든다는 의견
            예를 들면, {| product = 6; sum = 5 |}와 같이 필드 순서가 의미 없으니 실수가 없음
          + 이번 포크에서 immutable array도 5.4에 합쳐졌는데, 문법만 약간 다름
          + 익명 labeled struct와 enum이 프로그래밍 언어에서 바라는 상위권 기능임을 강조
            예시로 Rust에서는 labeled와 unlabeled struct를 모두 정의할 수 있지만
            함수 반환형에는 익명 labeled struct를 쓸 수 없다는 아쉬움 표출
struct Foo(i32, i32);
struct Bar{sum: i32, product: i32}
fn can() -> (i32, i32)
fn cant() -> {sum: i32, product: i32}

     * 이 포크가 SIMD를 지원한다는 사실을 몰랐음
       unboxed type, 명시적 stack 할당 기능, Windows까지 지원된다면 개인적으로 F# 대신 OxCaml이 game dev 등 컨슈머 환경에서 충분히 쓸 만해질 가능성 언급
          + 현재 128-bit SSE/NEON이 작동 중이고 곧 AVX도 지원 예정
            Windows 지원은 막혀있는 건 아니지만 약간의 작업 필요
            개인적으로 OxCaml에서 SIMD 지원을 추가했다는 점 짚고 싶음
     * 새로운 opam switch를 사용하는 이들에게 환경 변수 세팅 팁 공유
       env OCAMLPARAM=""alert=-unsafe_multidomain,_,"" opam install cohttp-lwt-unix
       알림(alert)이 오류로 승격되면 기존 패키지 설치 때 불필요하게 설치가 깨지는 문제 해결
       OCAMLPARAM 환경변수로 해당 alert 비활성화해서 설치 문제 예방 가능
     * Golang을 위한 훌륭한 vscode 플러그인에 익숙하다며, 오캄 환경도 vscode와 통합될 계획이 있는지 궁금함
       vscode와의 통합이 세팅을 매우 간편하게 만들었음
          + OCaml vscode 플러그인 자체가 dune, menhir, reason 같은 새로운 문법 통합을 이미 많이 지원함
            OxCaml 인기도가 오르면 당연히 통합 진전 가능성
            개인적으로는 emacs를 써서 자세하게 말하긴 어렵지만 자연스러운 흐름 예상
     * OcaML의 마이크로 사이즈 버전을 언급하고 싶음
       mlite 프로젝트 링크
     * 이 프로젝트가 공개된 목적이 LLM이 정보를 인덱싱해서 공개모델로 활용하려는 의도 때문일 수 있는지 물음
          + LLM이 일반 OCaml에 대해서조차 너무 부족하고 데이터량도 적은데, OxCaml은 더욱 자료가 부족해서 그럴 가능성은 전혀 없다고 판단
            이런 목적으로는 자체 문서 MCP를 만드는 게 더 유의미
          + 신호로서의 가치가 충분치 않기 때문에 사실상 의미가 없음
            예를 들어 Gleam 완성에서도 LLM의 성능은 매우 낮으며, 명확한 패턴과 실수 지침을 주더라도 실패
            이 때문에 OxCaml 정보 제공 목적이라기엔 신호 약함
     * OxCaml이 ML 계열 방언의 방언의 확장이라는 점이 재미있다는 관전 포인트
       다음 단계가 어떤 모습일지 기대감 있음
          + 기존 언어에 계속 기능 붙이는 사람들과, 애초에 새 언어를 만드는 사람들 중 누가 더 문제인지 자문해본 적 있음
            본인은 후자에 속하지만, 프로그래머는 도구를 있는 그대로 두질 못하는 유전자적 특성이 있다고 생각
          + 혹시 F#도 소개해볼 수 있겠냐는 농담 섞인 권유
     * 이 프로젝트가 ""oxidized""라는 명칭을 쓰는 이유가 Rust(예: fearless concurrency, GC 회피 등)의 기능 목표와 관련한 것인지, Rust를 실제로 사용하기 때문은 아니라는 점 확인
       다소 혼동될 수 있는 네이밍임을 피력
          + 실제로 Rust라는 이름은 녹이 아닌 곰팡이 'Rust'에서 온 거라는 작은 아이러니 포인트 짚고 싶음
          + Jane Street가 'Oxidizing OCaml'이라는 블로그 시리즈를 오래전부터 운영해온 사실 공유
          + 실제로 ""Oxidizing OCaml with Modal Memory Management""라는 논문의 제목에도 이 용어가 사용됐으며, 논문 내에서 'oxidize'란 단어가 명확히 정의되거나 인용된 적은 없음
            이상하기도 하지만 꽤 중독성 있는 이름이라는 인상
          + Rust 쪽이 커스텀 트레이싱 GC(일반적인 그래프 구조 다루면서도 최대 성능 추구 가능)와의 통합에서 이 프로젝트가 Rust와 기능 동등성을 확보하기 전까지 더 실용적으로 쓰일 것이라는 예측
            그렇지 않으면 단지 OxCaml 관련 코드베이스가 많아서 유지한다는 목적 정도로만 느껴진다고 평가
"
"https://news.hada.io/topic?id=21534",""AI 시대의 소프트웨어" - Andrej Karpathy의 YC AI 스쿨 강연 영상","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ""AI 시대의 소프트웨어"" - Andrej Karpathy의 YC AI 스쿨 강연 영상

     * ""Software is Changing (Again)""
     * 소프트웨어 패러다임은 70년 만에 본질적으로 변화하며, 최근 몇 년간 Software 1.0(전통적 코드), 2.0(신경망 가중치), 3.0(LLM과 자연어 프롬프트) 로 급속하게 진화함
     * LLM은 단순 도구가 아닌 새로운 운영체제(Operating System)와 유사한 소프트웨어 생태계로 자리잡고 있으며, 누구나 영어 등 자연어로 컴퓨터를 프로그래밍할 수 있는 시대가 열림
     * AI 도구·에이전트와의 협업, '부분적 자율성(Partial Autonomy)'이 미래 소프트웨어 제품의 핵심이 되고, 인간의 빠른 검증과 통제가 병행되어야 신뢰할 수 있음
     * AI와 LLM은 ‘사람 같은 영혼(people spirits)’의 특성을 가지며, 엄청난 기억력·지식력과 동시에 헐루시네이션, 맥락 상실, 보안 위험 등 고유의 한계를 내포함
     * 향후 소프트웨어·문서·인프라는 ‘에이전트 친화적(Llms-friendly)’으로 재설계되어야 하며, LLM이 쉽게 이해·행동할 수 있도록 구조와 표현 방식이 변화함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소프트웨어 1.0 → 2.0 → 3.0 : 패러다임의 진화

     * Software 1.0: 사람이 직접 작성한 전통적인 소스코드
     * Software 2.0: 신경망의 가중치(파라미터)를 데이터셋과 옵티마이저로 튜닝해 만드는 모델
     * Software 3.0: 대형 언어 모델(LLM) 기반, 영어 등 자연어 프롬프트로 프로그램(명령) 생성

     * 최근 GitHub에서 코드는 영어와 코드가 혼합된 형태로 진화하며, 프로그래밍 언어로서의 영어가 빠르게 확산
     * Hugging Face 등은 Software 2.0의 ‘GitHub’ 역할을 하며, 오픈소스 모델 생태계를 주도함

LLM은 새로운 운영체제(OS)다

     * LLM은 단순 API·유틸리티를 넘어 운영체제처럼 다양한 소프트웨어가 돌아가는 플랫폼으로 진화 중
     * 현재는 1960년대 메인프레임 시대와 유사하게, 클라우드 중심의 중앙집중 구조에서 LLM 활용이 이루어짐
     * 장기적으로는 개인용 LLM 시대(분산·로컬 활용)가 도래할 가능성도 언급
     * LLM 활용 환경은 전통적인 터미널·명령줄 인터페이스와 유사하지만, 아직 범용 GUI는 충분히 발달하지 않음

LLM의 능력과 한계

     * LLM은 거대한 기억력과 지식 습득 능력을 갖추었으나, 헐루시네이션(허위 정보), 맥락 기억 상실, 보안 취약성 등 고유의 결함 존재
     * LLM의 ‘작업 맥락(working memory)’은 사람이 명시적으로 관리해야 하며, 장기적 맥락 학습은 아직 미흡
     * 보안·프롬프트 인젝션 등 실질적 위험 요소가 있으므로 활용 시 주의 필요

부분적 자율성(Partial Autonomy)과 인간-LLM 협업

     * Cursor, Perplexity 등 LLM 기반 앱은 전통적 수동 조작과 LLM 자동화의 결합, ‘자율성 슬라이더’(사용자 통제/AI 위임 정도 조절) 개념으로 진화
     * GUI를 통한 감사(audit)와 빠른 검증 루프, ‘AI를 짧게 묶어 통제하는 방법론’이 실무에 필수적
     * 소프트웨어, 제품, 서비스 모두 점진적으로 ‘부분적 자율화’가 강화될 것으로 예측

실제 사례와 ‘Vibecoding’ 문화

     * 누구나 영어로 직접 LLM을 통해 앱을 만들어보는 ‘Vibecoding’ 문화가 확산
     * Karpathy도 코딩 경험 없는 언어(Swift)로 단 하루 만에 iOS 앱을 만들고, 실제 서비스로 확장하는 경험 공유
     * 실제 프로토타입 개발은 LLM이 쉽게 해주지만, 실제 서비스화(인증, 결제, 배포 등)는 여전히 수작업과 DevOps가 병목임
     * 앞으로는 사람이 직접 클릭·설정해야 하는 부분을 ‘에이전트’가 대행할 수 있도록 ‘에이전트 친화적 소프트웨어/문서’ 설계가 핵심 과제

문서·인프라의 변화와 에이전트 친화성

     * 기존 인간 중심 문서(클릭, 순서 등)는 LLM·에이전트가 바로 활용하기 어렵기 때문에, Markdown·명령어 기반으로 재구성 필요
     * Versell, Stripe 등은 에이전트 친화적 문서 전환(예: curl 명령 등) 을 시작
     * 다양한 도구(GitHub Ingest, DeepWiki 등)는 코드 리포지터리·문서를 LLM이 바로 활용 가능한 형태로 변환해줌

결론 및 전망

     * 지금은 수많은 코드를 새롭게 쓰고, 재작성해야 하는 최고의 시기임
     * LLM은 ‘보조적 도구(Iron Man 슈트)’로써 인간 개발자와 협업하며, 완전 자율화로 나아가는 점진적 혁신이 진행될 것
     * 향후 10년간 ‘자율성 슬라이더’를 단계적으로 높이는 과정이 핵심 트렌드가 될 전망
     * 개발자와 조직은 LLM·에이전트에 최적화된 소프트웨어·문서·인프라 재설계를 서둘러야 함

   이 영상 엄청 좋아요! Andrew Ng이 ""AI Is the New Electricity""라고 말한 내용 언급하면서, 8억명이 쓰는 ChatGPT가 다운되면, 전기가 나간거랑 같다고 이야기 해주는데, 정말 와닿더라고요.

        Hacker News 의견

     * Karpathy의 비유와 현실을 정확하게 바라보는 시선이 정말 좋다는 감상 전달
     * 전통적인 코딩, 신경망 가중치, 프롬프트를 병렬 비교하는 것이 흥미롭다는 의견 제시, 예를 들어 자율주행 모듈에서 직접 작성한 코드 대신 대상을 잘 대변하는 데이터셋에 맞춰 신경망을 최적화하는 방식이 실제로 꽤 유용하다는 설명 포함, 하지만 많은 환경에서 하드웨어 제약 때문에 ""software 2.0""이나 ""software 3.0""이 불러올 적용 범위가 매우 제한적일 것이라는 점도 강조, 기존의 코드와 프롬프트는 서로 보완적으로 활용되는 도구로 남게 되고, 둘 중 하나가 완벽한 해결책이 되는 것이 아니라는 관점 공유
     * Karpathy가 늘 명확하게 사고하는 인재라는 칭찬과 함께 비유가 인상적이라는 언급, Waymo가 2013년에도 이미 중단 없는 자율주행이 가능했음을 보고, 이렇게 빨리 확장하지 못했던 이유가 규제 때문인지, 아니면 운전 최적화의 어려움 때문이었는지 궁금하다는 호기심 표현, 슬라이드 중 하나에 ‘AGI 2027’이라고 표시된 것이 있었다며 ai-2027.com 언급
     * 발표가 예정보다 훨씬 더 빠르게 나와 놀랐다는 짧은 의견 전달
     * 토크가 구식이 되기 전에 YC가 먼저 공유해줘서 고맙다는 인사와 함께 관련 트윗 링크(https://x.com/karpathy/status/1935077692258558443) 제공
"
"https://news.hada.io/topic?id=21515","죽은 토렌트 트래커를 되살리고 300만 피어를 찾은 경험","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    죽은 토렌트 트래커를 되살리고 300만 피어를 찾은 경험

     * 작성자는 죽은 토렌트 트래커 도메인을 획득하여 직접 opentracker를 운영함
     * 트래커를 부활시키자 1.7백만 개 토렌트와 3.1백만 피어가 자동으로 연결 시도함
     * BitTorrent 프로토콜에서 트래커는 중앙화된 역할을 하며, 트래커가 없으면 파일 공유가 어려움
     * Mainline DHT 등 탈중앙화 대안도 있으나, 한계와 취약점 존재함
     * 법적 위험성 때문에 최종적으로 도메인과 VPS를 삭제한 경험임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 작성자는 Linux ISO 등 토렌트를 다운로드하던 중 트래커 대부분이 죽어있는 상황을 발견함
     * ""죽은"" 트래커 도메인(udp://open.demonii.si:1337/announce)이 등록되지 않은 것을 알게 됨
     * 해당 도메인을 구입하고 VPS에서 opentracker를 설치해 실제 트래커를 운영해봄

BitTorrent 트래커란 무엇인가

     * 트래커는 BitTorrent 프로토콜에서 피어(사용자) 간 연결을 도와주는 중앙 서비스 기능 수행
     * 트래커가 운영되지 않으면, 사용자가 서로를 찾지 못해 파일 공유가 불가해짐
     * 트래커가 유지되지 않거나 법적 압박을 받을 경우 사용자들이 불편 경험

탈중앙화 대안(DHT)과 한계

     * Mainline DHT는 트래커 없이 분산 네트워크 방식으로 피어 검색 지원
     * DHT는 부트스트랩 노드 의존성, Sybil 공격 취약성 등 한계 존재
     * 작성자가 테스트한 토렌트의 경우, DHT에서도 피어를 찾지 못함

트래커 직접 구축 과정

     * 도메인을 구매 후 익명 VPS에 연결함
     * opentracker(가장 많이 쓰이는 토렌트 트래커 소프트웨어)를 사용해 트래커 서버를 빠르게 구축함
     * 시스템 실행 후, UDP 1337 포트로 대량의 트래픽이 유입됨 확인
     * 한 시간 만에 173만 개 토렌트, 315만 피어가 연결 시도 통계로 나타남

트래커 통계(Stats)

     * 1,735,538개의 토렌트, 3,155,701명의 피어, 1,342,504명의 시더(완전체), 244,224건의 다운로드 완료 기록 확인
     * TCP/UDP 연결, announce, scrape 등 다양한 요청 유형에 대한 데이터 집계
     * 접속 및 에러 통계도 함께 분석, 일부 파라미터 오류(400 Invalid Parameter) 및 '404 Not Found' 오류 소량 존재

법적 쟁점

     * 공공에 공개된 웹사이트 및 .torrent 파일 홍보, 광고 수익 등은 저작권 유도행위로 법적 문제가 됨
     * 단순히 트래커 인프라만 운영하는 것은 법적 책임 소지가 모호함. 그러나 의도 증명이 쟁점이 될 수 있음
     * 무료 및 저작권 있는 토렌트 둘 다 이 트래커를 사용함을 인지

최종 결론

     * 법적 우려 및 실명 결제 방식(카드 결제 등)으로 인해 빠르게 VPS와 도메인을 폐기함
     * 여전히 미사용 트래커 도메인이 많아, 관심 있는 사용자가 쉽게 등록할 수 있음을 안내
     * open.demonii.si 등 공개적으로 등록 가능한 트래커 도메인 소개

마무리

     * BitTorrent 생태계에서 중앙화된 트래커 인프라의 역할과 취약성, 그리고 탈중앙화 기술의 한계를 경험적으로 전달함
     * 오랜 시간 죽어있던 트래커 도메인을 부활시켜도 즉시 수백만 피어가 연결을 시도함이 인상적임
     * 인프라 운영의 법적 위험성에 대한 주의 사항 공유

        Hacker News 의견

     * 이 경우에는 트래커를 직접 호스팅하는 게 아니라 들어오는 연결만 보는 것이라서 불법일 이유가 별로 없다고 생각함. 트래커를 돌린다고 해도 실제로 불법인지 입증하기 쉽지 않음. opentrackr 같은 걸 호스팅하는 건 마치 검색 엔진을 운영하는 것과 비슷함. 법적 삭제 요청에 어떻게 대응하느냐가 핵심 포인트임. 트래커 자체는 꽤 단순한 서버 소프트웨어임. 이런 게 불법이 되는 건 좀 이상하다고 생각함
          + “이게 합법이냐?”라는 질문보다는, “소송을 얼마나 당할 확률이 높냐?”가 훨씬 실질적인 질문임. 민사소송은 합법이든 아니든 일단 당할 수 있음. 변호사들이 타깃으로 삼으면 아주 귀찮아짐
          + 일부러 범죄를 도운 경우, 스스로 범죄를 저지른 것과 마찬가지로 간주됨. 미국 연방법(18 USC 2a, 참고 링크)에 따라 단순한 소프트웨어라는 이유로 잘못된 행동을 해도 면책이 되는 건 아님. 몇 가지 안전한 법적 예외 조항이 존재하기는 하지만(특히 저작권 관련), 전반적으로 타인의 범죄를 돕는다는 건 그 자체로도 범죄라고 생각해야 함. 나는 변호사는 아니지만 범죄를 돕는 건 피하는 게 무난함
          + 트래커(비트토렌트 피어 간 조율을 위한 서버)와 ""트래커""(.torrent 파일과 magnet URI를 호스팅하는 사이트)는 구분해야 함. 실제로 법적 삭제 조치는 후자, 즉 .torrent 파일이나 링크를 호스팅하는 사이트에 집중되어 있었음
          + (전문가는 아니지만) 상황에 따라 합법이기도, 불법이기도 할 수 있음. 만약 삭제 요청에 응답하지 않는다면 불법 쪽으로 기울 수 있음. 반대로 삭제 요청에 응답하고 해시를 블랙리스트 처리한다면 대부분 괜찮을 가능성이 큼. 물론 관할 지역과 해시-아이피:포트의 연결 자체가 배포/조력으로 간주되는지에 따라 다름(TPB 사례 참고). 내가 아는 한 분은 수 년간 대형 트래커를 운영하면서 삭제 요청을 블랙리스트 처리해서 아직도 문제없이 지내고 있음
          + 음악/영화 업계가 전반적으로 P2P를 싫어해서 2000년대에 사실상 P2P 생태계를 죽였다고 봄. 언젠가는 다시 논의할 필요가 있다고 생각함. 라이선스 획득이 쉬워진 시대이니 DRM 집행만 잘 한다면 걱정할 필요 없다고 생각함
     * 다양한 비트토렌트 클라이언트들이 존재하고, 그 중 상당수는 안전하지 않은 언어로 작성되어 있음. 그런 상황에서 악의적 트래커가 일부 클라이언트를 공격하는 것도 가능하지 않을까 하는 의문이 생김. 만약 트래커로부터 비정상적인 데이터를 받으면, 일부 클라이언트가 제대로 대응하지 못하고 취약한 동작을 보일 수 있다는 생각이 듦
          + Transmission의 경우 CVE-2018-5702로 알려진 DNS rebinding 취약점으로 인해 원격 코드 실행이 가능했던 적이 있음. 트래커를 통한 공격은 실제로 존재하는 리스크라고 봄
          + 대부분 사용자들이 사용하는 토렌트 클라이언트는 libtorrent라는 라이브러리의 래퍼임. libtorrent는 테스트도 잘 되어 있고 보안 감사도 받은 라이브러리라서 신뢰성이 높음
          + 나도 취미로 클라이언트를 만들어봤는데, 결론적으로는 '그렇다'임. 서버에서 온 입력 데이터를 다루고, 파일시스템과도 복잡히 상호작용함. 메모리 안전 언어로도 겨우 돌아가게 만들기 어렵고, C나 C++로 완전히 안전히 만드는 건 상당히 까다로운 작업이라고 느낌
          + Rust로 작성하지 않은 다른 프로그램들도 많으니 너무 걱정할 필요 없다는 생각이 듦(혹은 모든 소프트웨어가 위험할 수 있다고 볼 수도 있음)
          + 이런 기술적인 위협을 본문에서 좀 더 탐구해줬으면 좋겠음
     * 결국, 도메인을 등록하고 특정 DNS 레코드를 게시하는 것만으로도 원하는 IP를 DDoS할 수 있다는 의미가 됨
          + 일반 인기 클라이언트의 announce 주기는 꽤 김(대략 30분). 하지만 피어가 3백만 명이라면, 그 자체로 네트워크 부담이 생길 수 있음
          + 실제로 그 정도로 심각한 리스크인가 싶음. 내가 써본 비트토렌트 클라이언트들은 연결이 실패하면 최소 60초 정도 대기함. 죽은 트래커 도메인을 구입해서 남의 IP로 연결을 유도하더라도, 그 포트에서 서비스가 돌아가는 것도 아니고 트래커 프로토콜을 안 쓰면, 수백만 클라이언트가 몰려도 과연 진짜로 큰 문제가 될지 의문임
     * 매일 갱신되는 트래커 마스터리스트가 여기에 있음. 이를 이용해서 죽은 트래커를 추가로 찾는 것도 가능함
     * 내 첫 번째 의문은 비트토렌트 클라이언트 중 얼마나 많은 곳에서 취약한 파싱 코드를 갖고 있냐는 점임. 누군가 도메인을 악의적으로 등록해서 클라이언트를 감염시킬 수 있지 않을까 하는 생각임
          + Jon Evans의 소설 “Invisible Armies”와 그 소설에서 저자가 P2P 소프트웨어의 버그/백도어를 사용해서 시스템을 장악하는 장면이 떠오름
          + utorrent v2.1은 아직도 많은 사람들이 사용 중인데, 확실히 취약점이 존재하는 상황임
     * 이 상황은 Cloudflare가 1.1.1.1 IP 주소를 가져갔을 때와 비슷함. 도메인이 열리자마자, 수많은 자동화된 스크립트나 봇들이 그 쪽으로 트래픽을 쏟아부었던 일이 있음
          + Cloudflare가 그 주소를 어떻게 확보했는지 궁금함
     * 예전에 실험 삼아 짧게 개인용 트래커를 운영한 적이 있음. 실제 운영은 아주 짧았고, 트래커 동작 방식은 깊이 파악하지 않았음(Rust로 만든 Aquatic tracker에 webtorrent 지원을 요청해서 써봄 Aquatic 링크). 트래커가 주로 뭘 추적하는지, 피어 간 정보 교환에서 직접적으로 무언가를 알 수 있는지 의문이 듦. 내 추측에는, 단순히 해시나 마그넷 값만 가지고 피어들의 만남을 주선함. 마그넷 자체도 꼭 식별 정보를 포함할 필요는 없다고 생각함(많은 마그넷 링크는 사람 읽을 수 있는 설명도 포함함). 트래커가 해시를 받아서 직접 해당 파일을 다운로드해서 확인할 수도 있겠으나, 직접 다운받지 않는 이상 실제로 어떤 컨텐츠인지 제대로 알긴 힘듦. 마그넷 링크에서 실제로 피어 매칭에 필수적인 요소는 무엇인지, 트래커가 사람이 읽는 필드를 무시하거나 차단해서
       중립성을 유지할 수 있는지 궁금함
          + 트래커는 토렌트의 info hash만을 다룸. 파일 이름, 설명, 콘텐츠 리스트 등 그 어떤 것도 다루지 않음. 예를 들어 opentracker는 화이트리스트와 블랙리스트 모드 둘 다 지원함. 대부분 오픈 트래커(예: torrent.eu, opentrackr.org 등)는 블랙리스트 방식으로 운영되며, 거의 모든 사용자들이 (거의) 모든 컨텐츠를 찾을 수 있도록 허용함
          + 트래커는 실제로 추적하는 컨텐츠를 알 수 있음. 내가 예전에 운영하던 TV 프로그램 트래커는 각 사용자의 업로드/다운로드 비율까지도 추적해서 관리함
     * 러시아, 중국, 이란 등에서 도메인을 등록하고 Alibaba에 사이트를 호스팅하면 됨. 미국에서 법적 압박을 넣어도 그 쪽 국가에 전달되는 순간 완전히 무용지물이 됨. 실제로 그런 곳에선 법적 대응이 거의 힘듦
     * 변호사는 아니지만, 미국 내에서 컨텐츠 중립적 트래커를 운영하는 건 합법으로 알고 있음. 다만, 다른 국가에서는 확실히 불법일 수 있으며, VPS 위치와 TLD 국가(.si 등)에 따라 차이가 있음
          + 검색해보면 미국 사법당국에 의해 강제 종료된 트래커 사례가 있음(EliteTorrents, 2005년, 관련 기사). 아마 다른 사례들도 존재할 듯. 민사소송(예: MPAA 등 단체의 손해배상 청구 소송) 등으로 폐쇄된 트래커도 많음. 전체 리스트 대부분이 저작권 컨텐츠라면 미국에서도 얼마든지 폐쇄될 수 있음
          + 예전에 .si에서 운영된 대형 퍼블릭 트래커가 있었음. 슬로베니아에서 거의 20년 동안 온라인 경험자라면 한 번쯤 써봤을 정도임. 이 트래커는 법적 통보로만은 사라지지 않았음
          + VPS는 cockbox.org 기반(본문 참고)에 위치해 있는데, 이 회사는 본사 위치가 몰도바라고 함
     * 만약 누군가 suprnova 도메인을(2004년에 문을 닫은) 다시 구입한다면, 아직 살아있는 시더가 있다면 예전 다운로드 작업을 이어받을 수 있을까 궁금함. DHT 이전 시대에 생성된 것이라면 영향을 받는지도 궁금함. DHT가 예전 토렌트까지 커버할 정도로 “역사적” 기능이 있는지도 알고 싶음
          + DHT는 진짜 “역사적”임. 즉, 토렌트 생성 시점과 무관하게 infohash만 일치하면 동작함. 단, 보통 프라이빗 트래커에서 생성된 토렌트는 “private” 플래그가 켜져 있어서, DHT, PEX 등 일부 기능에서 제외됨. 이 플래그는 직접 삭제할 수 있지만 DHT가 작동하려면 시더 쪽에서도 이 플래그를 풀어야 함
          + DHT는 클라이언트가 해당 기능을 켰다면 언제든 동작함. 만약 예전 시더도 새 버전 클라이언트로 업그레이드했다면, 메타데이터가 자동으로 DHT에 공유될 수 있음
          + 이론적으로는, 위와 같은 상황이 충분히 가능함
"
"https://news.hada.io/topic?id=21509","Meshery - 클라우드 네이티브 관리자 오픈소스 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Meshery - 클라우드 네이티브 관리자 오픈소스

     * Kubernetes 기반 인프라 및 어플리케이션 통합 관리 플랫폼
     * GUI/협업 기반의 GitOps 워크플로우 제공
     * 멀티플레이어 모드, 멀티테넌시 지원으로 대규모 조직 및 플랫폼 엔지니어링 팀에 적합
     * 비주얼 인터페이스로 복잡한 YAML 작성 없이도 멀티클러스터 인프라와 애플리케이션 설계 및 운영이 가능
     * 300개 이상의 클라우드 네이티브 인프라 통합, 다양한 배포 환경 지원(Docker, Kubernetes, Helm, Mac, Linux, Windows, WSL2 등), 대시보드 기반 시각화 및 실시간 멀티유저 협업 환경 제공
     * 다양한 클라우드 및 온프레미스 환경의 여러 Kubernetes 클러스터를 단일 대시보드에서 관리
     * 실제 적용 전 YAML, Helm, 디자인 구성의 유효성 사전 검증 및 시뮬레이션 가능
     * 워크스페이스 단위로 팀/프로젝트별 리소스 관리 및 협업, Environments/RBAC 지원
     * gRPC, ReactJS 패키지, Golang 플러그인, REST/GraphQL API 등 광범위한 확장성
     * 성능 측정, 로드테스트, 프로파일링, Prometheus/Grafana 연동으로 인프라와 워크로드의 성능을 종합적으로 관리할 수 있음

   https://www.appvia.io/blog/kubernetes-security-nightmare
"
"https://news.hada.io/topic?id=21481","쓰레드 자유 시대 선언—Python Steering Council 결정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                쓰레드 자유 시대 선언—Python Steering Council 결정

   나동희님의 노력으로 프리-스레딩이 이제 실험태그를 벗어나 공식 지원 됩니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   Python Steering Council(SC)은 PEP 779를 승인하여 Python 3.14의 프리-스레딩(free-threaded) 빌드에서 “experimental” 태그를 제거하기로 했습니다. 이 결정은 Phase II 동안 다음 과제를 충족해야 한다는 조건과 함께 내려졌습니다.
    1. C API/ABI 호환성 및 Stable C API 마련
          + 기존 API/ABI 호환을 깨는 변경은 반드시 C API 워킹그룹과 사전 합의해야 함
          + Python 3.15까지 프리-스레딩 전용 Stable C API 정의·제공
    2. 새 실험적 프로젝트 기준
          + CPython 내부의 신규 실험 프로젝트는 프리-스레딩 빌드와 호환되고 이를 기반으로 개발
          + GIL·프리-스레딩 병행 유지에 따른 복잡성 최소화
    3. 성능·메모리 가드레일
          + 목표: GIL 빌드 대비 성능 저하 10 % 이내
          + 최대 15 % 저하 / 15 % 메모리 증가까지는 SC 사전 합의 필요(메모리 상한 20 %)
    4. 문서화 요구 사항
          + Python 사용자: 표준 라이브러리 API 보장 및 변화 명확화
          + Python·C API 개발자: 시그널·스레드 안전성 등 동시성 보장 문서화
          + CPython 개발자: 구현 시 고려 사항 문서화
          + 모든 자료를 모은 공식 ‘프리-스레딩 랜딩 페이지’ 제공
    5. 고수준 동시성 프리미티브 준비
          + concurrent 패키지를 중심으로, 사용자가 내부 구현을 몰라도 활용 가능한 고수준 동시성 도구 제안·검토
    6. 벤치마크 요건
          + 성능·메모리·정확성 주장을 pyperformance 기반 반복 가능한 테스트로 검증
          + 실제 워크로드를 반영한 추가 벤치마크 기여 장려

   SC는 프로젝트의 진척 상황에 확신을 표하며, Python 커뮤니티가 프리-스레딩 빌드를 정식 지원 옵션으로 적극 홍보하기를 권장합니다. 3.14 beta 3에서 “experimental” 태그가 공식적으로 제거될 예정이며, Phase III(프리-스레딩을 기본 또는 유일한 빌드로 전환) 여부는 추후 논의됩니다.

   쓰레드 독립 만세!

   Yay!

   와우...!
"
"https://news.hada.io/topic?id=21530","WWDC25의 도시락통(Bento Box) 스크린샷들 정리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    WWDC25의 도시락통(Bento Box) 스크린샷들 정리

     * WWDC25 키노트에서 사용된 iOS/watchOS/tvOS/macOS/visionOS/iPadOS 26 버전의 도시락통 모양 스크린샷들 고화질 버전 모음
     * 이들 각각을 자세히 들여다 보면, 애플이 어떤 것을 중요한 기능이라고 생각하는 지를 알 수 있음

   ㅎㅎ 이걸 모아놓은 글도 있군요
"
"https://news.hada.io/topic?id=21465","Starlink Mini를 내장 WiFi 라우터 없이 동작시키는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Starlink Mini를 내장 WiFi 라우터 없이 동작시키는 방법

     * Starlink Mini 1에서 내장 WiFi 라우터를 제거해 오직 이더넷 연결만으로 동작시키는 물리적 수정 방법을 자세히 설명함
     * 이 수정은 사용자 맞춤 네트워킹, 임베디드 환경, 또는 전력 제약 상황에 적합한 옵션임
     * 분해 시 방열판 겸 EMI 쉴드 역할을 하는 금속판은 제거하지 않아야 하며, 제거 시 과열이나 전자파 간섭 문제가 발생함
     * Starlink Mini 메인보드의 커넥터 핀아웃과 이더넷 직접 연결 회로 예제가 제공되며, 안전한 전원 및 신호 처리를 강조함
     * 네트워크 설정 방법, gRPC 상태 코드 활용법 및 주요 장애/정지 원인 분석법이 포함됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 중요성

     * Starlink Mini 단말은 일체형 WiFi 라우터가 내장된 컴팩트 솔루션임
     * 일반적 소비자 환경에선 이상적이나, 네트워크 맞춤화나 전력 제한 환경에선 내장 라우터를 제거하고 이더넷만 사용하는 것이 더 유리함
     * 본 요약은 내장 WiFi 라우터 보드를 물리적으로 제거하고 Starlink Mini를 오직 이더넷으로만 운용할 수 있도록 하는 과정과 실용적 팁을 제공함
     * 본 개조는 Starlink Mini 1 (2025년 6월 14일 기준) 에만 적용 가능함. 향후 출시될 모델에는 불가할 수 있음

Starlink Mini 분해(Teardown)

     * 분해 작업은 인내심과 정확성이 요구됨. 금속 스퍼저, 플라스틱 프라이(틀뜯는) 도구가 필요함
     * 라우터 PCB 제거에는 얇고 유연한 나이프 또는 금속 와이어가 추가로 필요함
     * Starlink PCB의 금속판(방열판 + EMI 쉴드)은 절대 제거하지 않아야 함. 이 금속판은 프로세서 및 안테나의 방열과 전자파 차폐(EMI Shield) 를 담당함
     * 금속판 제거 시 과열 및 전자파 간섭으로 인한 주변 전자기기 방해가 발생할 수 있음
     * SpaceX도 해당 EMI 문제 해결에 도전이 있었음

Starlink Mini PCB 커넥터 정보

     * PCB 연결 커넥터는 2mm 피치의 표준 핀이 사용 가능하며, 구체적 모델은 커스텀일 수 있음
     * 이 커넥터는 전도성 접착제 및 넓은 접지 영역으로 EMI 차폐 효과가 높아짐
     * 라우터 PCB 측 커넥터에는 쉴드도 덧씌워져 있음

커넥터 핀아웃

     * Starlink Mini 메인보드와 라우터 간 연결은 1Gbps 이더넷이 직접 연결(PHY-to-PHY, 트랜스포머 없음)로 구성됨
     * 짧은 거리에서는 직접 연결이 가능하지만, 별도 설치 시엔 반드시 이더넷 트랜스포머를 필수로 사용해야 함
     * 주요 전압 공급은 12V DC
     * 각 핀의 배열 및 역할은 이미지로 제공되며, 미사용 핀(11, 14, 16, 17, 18)은 라우터 모니터링 용임
     * 12V DC 및 GND 라인은 모두 사용을 권장함

이더넷 직접 연결 회로

     * 이더넷 트랜스포머, 최소한의 전원 필터링 회로를 포함하는 직접 연결 회로 예제가 제시됨
     * U1 커넥터 주변에 가드 그라운드 및 차폐(쉴드)+전도성 접착제 사용 설계를 권장함
     * 연결선(커넥터에서 트랜스포머까지)은 최대한 짧게 유지하여 신호 품질을 확보함
     * 12V에서 평균 3A, 순간 최대 5A 전류가 흐를 수 있음. L1(인덕터) 선정 시 적정 전류 용량 필수임
     * Ethermod 어댑터 등 실험용 적용 예시도 제공됨

네트워크 설정 및 동작 방식

     * Starlink 터미널은 위성 연결 전 192.168.100.0/24 네트워크에서 DHCP로 IP 할당함
     * 기기 자체는 192.168.100.1 주소로 접근 가능하며, 웹 UI 및 gRPC 모니터링/제어 서버가 동작함
     * grpcurl로 Starlink 디버그 데이터 추출 가능함 (예: get_status로 단말 상태 조회)
     * 위성에 연결되면 이더넷 인터페이스가 터널링된 DHCP 서비스로 Starlink IP 풀(CGNAT IPv4와 글로벌 IPv6)에서 IP 할당함
     * 단 1개의 IP만 할당되어 한 대의 호스트 또는 라우터만 직접 연결 가능함
     * 외부 IP 할당 후에는 기존 관리 UI(192.168.100.1)에 접근 불가하나, static route 추가로 재접근 가능함

gRPC 상태 코드 및 문제 분석

     * gRPC get_status 출력은 터미널 및 연결 상태에 관한 다양한 정보를 제공함
     * 연결 문제 발생 시 outage 섹션이 포함되며, outage의 cause 필드는 다양한 상황을 나타냄
          + BOOTING: 부팅 중, 모듈 초기화, GPS 신호 대기
          + THERMAL_SHUTDOWN: 과열로 인한 자동 종료
          + NO_SCHEDULE: 위성과 통신 불가(신호 약함/위치 불일치/기타)
          + NO_SATS: 위성을 전혀 감지하지 못함
          + OBSTRUCTED: 전파 경로 장애물 감지됨
          + NO_DOWNLINK: 위성으로부터 데이터 수신 불가
          + NO_PINGS: 위성과의 연결은 유지하지만 지상국과의 연결 손실
     * disablementCode 항목을 통해 Starlink 계정의 서비스 상태 확인 가능함
          + OKAY: 정상 계정 및 인터넷 접속 가능
          + NO_ACTIVE_ACCOUNT, TOO_FAR_FROM_SERVICE_ADDRESS, IN_OCEAN 등 계정정지, 위치 제한, 데이터 초과 등 다양한 원인 파악 가능함
          + 단말 자체에서는 서비스 제한 정보를 알지 못하고, 위성에서 받아 처리함

결론

     * Starlink Mini 1의 내장 WiFi 라우터 보드 제거 및 이더넷 단독 운용은 맞춤 네트워크, 임베디드, 저전력 환경에 유용함
     * 분해 및 회로 연결, 네트워크 설정에 각별한 주의가 필요하며, 기본 설계 원칙 및 신호/정전류 관리가 중요함
     * gRPC 상태 분석을 통해 문제 원인 및 서비스 상태를 정밀 진단할 수 있음

        Hacker News 의견

     * 이들이 단순히 MAC에서 MAC으로 RGMII를 사용하는 대신, 모듈화된 보드 간 이더넷을 선택했다는 점이 흥미로운 선택 사항
          + 이더넷이 프로토타입 제작에 훨씬 더 쉬운 선택지라는 생각, RGMII에 맞는 시판 제품은 거의 없지만 이더넷은 그냥 노트북에 꽂아서 바로 테스트 가능, 서로 다른 팀에서 만들 때 이더넷으로 인터페이스를 통일하면 통합 테스트도 미뤄둘 수 있고 출시 일정도 앞당길 수 있는 장점
          + MDIO 에뮬레이터를 직접 작성해야 하고, 실제로 링크 감지를 하려면 회로 내부의 SMI 코드까지도 건드려야 하는데 이 시스템이 독자적이어서 쉽게 주석 처리할 수도 없는 재미있는 상황
          + 많은 부분이 POC(개념증명) 목적에 충실한 모양, 디지털-아날로그-아날로그-디지털로 이어지는 비효율성은 동의하지만, 추상적으로는 MAC-PHY 간 RGMII 방식이 더 나아 보일 수도 있음, 즉석에서 추측해본다면 지금처럼 문서화된 인터페이스가 접근성이 더 높거나 내부 구조를 덜 파야 해서 그랬을 수 있음, Starlink mini 기준으로 RGMII 라인이 어디에 있는지도 모르겠고 내장 디자인에 따라 그 라인이 깊숙이 묻혀서 찾기 힘들 수도 있는 점
          + RGMII는 본래 보드 간 연결에 적합하지 않음, 데이터 속도가 상당히 높고 모든 신호 지연까지 맞춰야 해서 보드가 두 개로 나뉘면 작업이 훨씬 까다로워지는 점, EMI/EMC 문제도 기대할 수밖에 없음, 평가용 키트에서는 그렇게도 하지만 양산용으로는 좋은 선택이 아니라고 보는 관점
          + 이 작업 전체의 목적을 정확히 이해하지 못하겠다는 궁금증, Mini에 이미 RJ45 포트가 달려 있어서 굳이 Ethernet PHY에 접근하려고 해킹할 필요 없음, 셋업 페이지에서 WiFi 라우터도 꺼버릴 수 있으니 제품 자체 해결 가능, 혹시 우크라이나에서 판매 중인 Mini에서 Ethernet 포트가 아예 제거됐는지 궁금, 사진상으로는 전원 잭 옆 WiFi 보드에 여전히 그 포트 존재 확인
     * 전력에 민감한 애플리케이션을 Oleg이 염두에 두고 있다는 사실 정확히 짚으며, 그런 접근 좋아하는 입장
          + 해당 유튜브 채널 링크
          + 실제 Starlink가 «Nemesis» 야간 폭격기와 «Magura» 해상 드론 등에서 이미 활용 중인 사례
          + 구체적으로 어떤 활용 사례를 염두에 두고 있는지 궁금증
          + 드론에 얹어서 사용하는 방식을 예상하는 추측
     * 아카이브 저장 링크
     * Starlink가 어떤 SoC를 사용하는지 궁금증, Broadcom일까 하는 의문
          + MediaTek 사용 확인
     * 이런 정보 알 수 있어서 유익하단 의견
"
"https://news.hada.io/topic?id=21508","OpenAI, 미 국방부와 2억 달러 계약 체결","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       OpenAI, 미 국방부와 2억 달러 계약 체결

     * 오픈AI가 미 국방부와 2억 달러 규모의 1년 계약을 체결하며 국방 분야에 본격 진출함
     * 계약은 워싱턴 D.C. 인근 지역에서 진행되며, AI를 통해 전투 및 행정 시스템 개선을 목표로 함
     * 이 계약은 정부 대상 AI 서비스 'OpenAI for Government'의 첫 사례이며, ChatGPT Gov 등도 포함됨
     * 모든 활용 사례는 오픈AI의 정책 기준을 준수해야 하며, 행정 자동화·사이버 방어 등에 적용 예정임
     * 오픈AI는 10조 원 이상 연매출과 400억 달러 투자 유치 등으로 국방 계약이 수익의 일부에 불과함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

오픈AI, 미 국방부 2억 달러 계약 수주

  계약 개요

     * 오픈AI는 미 국방부와 2억 달러 규모의 계약을 체결하여 AI 모델을 제공하게 됨
     * 계약 기간은 1년이며, 주요 작업은 워싱턴 D.C. 및 인근 지역에서 수행될 예정임
     * 계약의 공식 명칭은 OpenAI Public Sector LLC와의 계약으로 등록됨

  계약 목적과 적용 분야

     * 국방부는 본 계약이 **“전장 및 행정 영역에서 핵심 안보 과제를 해결하기 위한 프론티어 AI 프로토타입 개발”**을 위한 것이라 설명함
     * OpenAI는 해당 계약이 자사의 공공 부문 전용 프로그램 ‘OpenAI for Government’의 첫 사례라고 밝힘
     * 해당 프로그램은 국가 안보용 맞춤형 AI 모델, 로드맵, 기술 지원 등을 미 정부 기관에 제공함

  활용 사례

     * 오픈AI는 계약을 통해 아래와 같은 영역에서 AI 적용을 계획함:
          + 군인 및 가족의 의료 서비스 접근 방식 개선
          + 프로그램·조달 데이터 분석 자동화
          + 사이버 보안 위협에 대한 선제 대응

  업계 내 경쟁 및 배경

     * 이번 계약은 2024년 12월 Anduril과의 국방 협력 발표 이후 본격화된 것임
     * Anthropic도 Palantir, Amazon과 협력해 미 정보기관 대상 AI 공급 계획을 밝힘
     * 국방부는 AI 모델 활용에 있어 클라우드 인프라 제공자인 Microsoft의 Azure OpenAI도 기밀 등급에서 승인함

  오픈AI의 성장과 영향

     * 계약 금액은 오픈AI 매출 규모에 비하면 작지만, 공공 부문 확장의 신호탄이라는 점에서 상징적임
     * 오픈AI는 연간 매출 100억 달러 이상, 기업가치 3,000억 달러, 4조 원 규모의 투자 라운드 유치 등 공격적인 성장 중
     * 미국 내 AI 인프라 구축을 위한 5000억 달러 규모 'Stargate 프로젝트'도 진행 중이며, Altman은 트럼프 대통령과 함께 백악관에서 발표함

  정책 및 윤리 기준

     * OpenAI는 모든 정부 계약 활용 사례가 자사의 사용 정책 및 가이드라인을 엄격히 준수해야 한다고 명시함
     * AI 오남용 방지를 위한 투명성과 책임성 확보 의지를 강조함

        Hacker News 의견

     * 현재 DoD의 소프트웨어 구매 방식으로 보면 많은 예산이 쓰이고 언론 보도가 쏟아지고 여러 상이 수여되겠지만, 정작 사용자 워크스테이션에 설치되는 소프트웨어는 전무한 상황 예측, 결국 최종 사용자들은 모든 업무를 Excel로 처리하는 현실 지속
          + 마지막 부분만 빼면 거의 맞는 이야기, 실제론 이로 인해 파워포인트 생산량만 훨씬 더 늘어나는 상황 예상
          + 2억 달러는 이들에게 큰 금액 아닌 인식, 만약 프로토타입이 성공적이면 좋겠지만 아니어도 전혀 걱정 없는 분위기
          + Lt Col(중령) 진급 커미션이 여기서 오고 간다는 사실도 잊지 않아야 한다는 농담
          + 결국 AI가 이런 일자리들을 없앨 거라는 외부 의견과 달리, 엑셀은 여전히 모든 업무의 중심에 있을 것이라는 현실
          + 사실 이번 건은 설치형 소프트웨어가 아니라 네트워크 기반 서비스일 가능성이 높다는 생각, 브라우저로 접근하면 의외로 연결성도 괜찮다고 봄
     * 내게 총 쏘지 말라는 외침, ""나 Sam Altman인데 왜 또 쏘느냐""는 농담스러운 자기반성, 실수 인정과 바로잡기에 대한 의지 강함
          + ""네 말이 맞아! 총도 내려놓았고, 총알도 다 뺐어!""라며 마치 만화같은 분위기 연출, 결국 ‘Bang’으로 소동 종결
     * 군수 관련 기업들이 Tolkien 소설 속 단어를 사명으로 쓰는 현실에 분노, Andruil, Palantir 등 사용이 Tolkien이 추구한 가치와 너무 대조적이라 싫다는 감정 표현
          + 드론이 트럭 위에서 폭발하는 데모를 보며 개발자들이 “와 멋지다!” 하고 하이파이브할지 궁금한 의견, 총기나 군사에 반대하지는 않지만 누군가는 이런 일도 해야 한다는 현실도 인정, 윤리·도덕 논쟁은 상존함에도 불구하고 기술적인 도전과제에 흥미를 느끼는 입장
          + 이런 군수 프로젝트가 모두 Peter Thiel이 연관된 곳이라고 인지함
          + 공군도 Star Wars IP를 쓰는 예시로 Millennium Falcon을 활용한 Kessel Run 프로젝트 언급, 관련 링크(Kessel Run 위키피디어) 공유
          + Tolkien이 실제로 1차 세계대전 참전 경험 및 2차 대전 암호 해독 훈련 이력이 있어 오히려 군사기술의 가치를 어느정도 이해했을 거라는 관점 제시
     * AI 업계의 겉과 다른 속마음을 지적, AI 경영진 누구도 공개적으로 말하진 않지만, 국방·전쟁 쪽 AI 적용에 거액의 예산이 있기 때문에 군산복합체와의 연계에 흥분하고 있을 가능성 크다는 시선, 특히 사업적 기반이 허약한 AI 회사라면 더욱 그러함
          + 이들에게 ‘china’라는 단어만 언급하면 이런 현실이 실감나게 드러난다는 주장
          + 실제로 AI의 활용처보다는, 이익 추구와 정부-공급사 간 중간자 역할에서 존재감을 드러내며, 실질적 내용보다는 인상적인 프레젠테이션과 로비가 더 중요한 현실 꼬집음, 군 장성들에게 값비싼 대접과 좋은 사기업 일자리 보장도 관행적으로 이어진다는 시선
     * 비영리에서 군사 계약으로의 전환 과정을 걷고 있다는 깊은 시사
     * 2억 달러가 미국 방위산업계에서 매우 작은 금액이라는 점 강조, 공식적으로 파일럿(시험 단계)임을 감안하면 IOC(운영개시) 단계에 이르기 전까지는 큰 관심 둘 필요 없다고 판단, 다만 이번 건이 AI의 국방 내 자율성 지원 전략 변화 신호로 볼 수 있음, 드론에 LLM(대형언어모델)을 바로 탑재하진 않겠지만, 사이버·행정 등 여러 분야 문제를 AI로 충분히 해소할 수 있다는 점 강조
          + 드론에 LLM을 안 단다는 주장에 대해 확신하기엔 이르며, 그런 제한이 실제론 없다며 회의적 시각 전달
          + 드론에 굳이 LLM이 필요하지 않음, 이미 완전 자율드론 기술은 오래전부터 도입됐다는 점 강조, 실제론 더 복잡한 상황에 투입되고 있음
          + IOC 도달 시점에선 기술이 너무 낡아진다는 정부 프로젝트의 현실 공유, 예시로 GPT-2를 아직도 쓴다는 상상 아래 정부 일자리가 안전한 이유 설명
          + 최근 Chief Product Officer(최고제품책임자) 자리도 군인으로 임명, Meta와 OpenAI CTO·CPO 등 주요 테크 기업 인재들이 Army Reserve Lieutenant Colonels(예비군 중령)으로 위촉된 기사 링크(Army Executive Innovation Corps) 공유
          + IOC의 의미가 Immediate or Cancel(즉시 또는 취소)의 약어냐고 묻는 센스 있는 농담 추가
     * 곧 status.golden-dome.mil에서 ""AI 제공업체 장애로 적대 항공기 식별 불가""라는 공지가 뜰 수 있는 상황 풍자
     * DoD가 OpenAI에서 얻고 싶은 게 대체 뭔지 궁금증, “덜 정확하고 더 아부 떠는 미사일?”이라는 익살
          + 실제 국방 의사결정 과정 예시, 국방장관이 폭격을 지시하면 측근이 A.I.로 보고서 작성, 요약까지 AI가 담당, 최종적으로 자료가 SNS에 실수로 공개된 후야 대통령에게 전달되고 결국 폭격 감행이라는 디스토피아적 시나리오 상상
          + 실제로 일부 인기 있는 AI 모델(NIPRGPT 등)은 부분 금지, DoD가 통합 솔루션을 필요로 하는 상황 설명, MSFT GovCloud도 늦어지는 현실, 지금 모든 정부기관이 LLM으로 문서·제안서를 빠르게 생산, 민간과 동일하게 AI에 의존, 직접 표적화에는 말 못하지만 ISR 등 공격형 AI 프로젝트가 넘쳐남
          + 다양한 활용 예시 언급, 신호·인적정보 분석, 전후 분석요약, 워게임, 인간-기계팀, LLM을 전투현장에 투입, 신속한 현장 코드 생성 등 끝없는 AI 활용 가능성
          + AI가 네이티브처럼 들리는 대규모 선전·프로파간다를 실시간으로 자동 생성하는 용도가 MIC(군산복합체) 입장에서 LLM 궁극의 돈벌이가 될 것으로 봄, 그래서 메타·OpenAI 등 빅테크 경영진들이 군에 합류한다고 추정
          + AI 캐릭터 폭탄이 등장하는 SF영화 Dark Star 추천
     * AI를 속도 때문에 조종계에 직결시키기 전 Petrov 사례처럼 신중히 학습시켰으면 한다는 바람
          + 무슨 말인지는 모르겠지만 웃긴다는 반응
     * AI가 문서 비밀해제(비분류) 작업에서 매우 뛰어나다는 이야기 공유
"
"https://news.hada.io/topic?id=21485","어린이 백혈병: 치명적인 암이 어떻게 치료 가능한 질병이 되었는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  어린이 백혈병: 치명적인 암이 어떻게 치료 가능한 질병이 되었는가

     * 1970년 이전에는 어린이 백혈병 환자 대부분이 신속히 사망하였으나, 현재 선진국에서는 85% 이상의 생존율을 기록함
     * 백혈병은 소아암 중 가장 흔한 유형으로, 주로 급성 림프모구성 백혈병(ALL)과 급성 골수성 백혈병(AML)로 구분됨
     * 생존율 향상에는 약물 개발, 맞춤형 치료, 대규모 임상시험 협력 등이 중요한 역할을 함
     * 유전자 및 분자 연구의 발전과 신약, 면역치료제 도입, 지원 치료 개선이 치료 성공률을 크게 높임
     * 미래에는 모든 지역 어린이의 치료 접근성 확대가 주요 과제로 남아 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

어린이 백혈병: 치명적인 암이 어떻게 치료 가능한 질병이 되었는가

  1970년 이전, 대부분의 어린이 백혈병 환자가 빠르게 사망함

     * 1970년 이전에는 어린이 백혈병 진단 시 5년 생존율이 10% 미만이었음
     * 그 시기에는 어린 환자와 가족 모두에게 충격과 상실감을 안겨주는 치명적인 질병이었음
     * 현재는 북미와 유럽에서는 85% 이상이 최소 5년 이상 생존함
     * 이러한 급격한 변화의 배경에는 과학적 진보와 치료 방법의 발전이 있음
     * 아시아, 남미 등 다른 지역에서도 소아암 사망률이 감소하고 있으나, 여전히 높은 편임

백혈병은 소아암 중 가장 흔하며, 사망률 감소가 두드러짐

     * 백혈병은 혈액과 골수에서 발생하는 암으로, 대부분 급성 림프모구성 백혈병(ALL) 과 급성 골수성 백혈병(AML) 로 구분됨
     * 미국 어린이 소아암 사례의 약 25% 를 차지함
     * 어린 시기에는 혈액세포 생성이 활발하고, 이 과정에서 DNA 오류로 인한 돌연변이 위험이 높아짐
     * 대부분의 소아 백혈병은 출생 전후의 급격한 세포 분열과정에서 발생하는 자연발생적 유전자 돌연변이에 기인함
     * 환경적 요인이 의심되었으나, 일관된 환경 원인은 밝혀지지 않음

소아 백혈병의 생존율은 큰 폭으로 향상됨

     * 1960년대 ALL(급성 림프모구성 백혈병) 환자 5년 생존율은 14% 였으나, 2010년대에는 94% 로 증가함
     * AML(급성 골수성 백혈병) 의 5년 생존율도 1970년대 14% → 60% 이상으로 올라감
     * 치료는 여전히 수년간 강도 높은 항암화학요법이 필요하며, 신체적·정신적으로 매우 부담스러움
     * 그러나 치료 후 만성 합병증 발생 빈도도 점차 감소하여 장기 건강 역시 개선됨
     * 장기 생존율 향상은 치명률의 대폭 감소로 이어짐

치료 발전의 원동력: 임상시험, 신약 개발, 분자 연구

  지속적 진보와 협력적 접근

     * 과거에는 단일 약물로는 암세포 일시 제거만 가능했으나, 병용요법 및 두개강 방사선치료가 도입되면서 소수 환자가 완치됨
     * 1960~70년대에는 4단계(유도, 공고, 지연강화, 유지) 로 이루어진 다단계 병용 항암화학요법이 표준화됨
     * 1980~90년대, 척수강 내 고강도 항암치료가 두개 방사선을 대체하면서 장기 부작용(인지장애, 성장저하 등)이 감소함
     * 위험군 분류(연령, 백혈구 수, 유전자 정보 등) 를 바탕으로, 저위험군은 부작용을 최소화하고, 고위험군은 강한 치료를 적용함
     * 2000년대 이후 미세잔존질환(MRD) 검사의 임상 도입으로 미세한 암세포까지 탐지, 맞춤치료가 정교해짐

  대규모 임상시험과 연구 네트워크의 중요성

     * 소아 백혈병은 희귀 질환이므로, 개별 병원 단독 연구로는 충분한 사례 축적이 어려움
     * 미국의 Children's Oncology Group, 유럽의 International BFM Study Group 등 초대형 협력 네트워크 구축으로 수만 명이 임상에 참여함
     * 이로 인해 치료 효과 비교와 위험도 평가 정확도가 비약적으로 향상됨
     * 크랜이얼 방사선 등 부작용이 큰 치료는 임상 결과에 따라 대체됨

  유전자·분자 수준의 진보와 표적 치료제

     * 유전자 돌연변이 분석을 통해 환자 맞춤형 위험군 구분 및 치료 강도 조절이 가능해짐
     * Imatinib(Gleevec) 등 표적치료제의 도입으로 특정 돌연변이가 있는 소아 환자군의 생존율 대폭 향상 및 이식 필요성 감소
     * 최근에는 CAR-T 세포 치료제, 항체 치료제 등 신개념 면역치료가 도입되어 치료 스펙트럼이 넓어짐

  지원 치료의 발전

     * 항암치료 중 출혈, 감염 등 합병증 예방을 위한 혈소판 수혈, 항생제, 항진균제, 백신이 표준으로 사용됨
     * 신약 및 신백신(폐렴구균, 수두, 로타바이러스 등) 도입으로 면역저하 환아의 감염 위험 줄임
     * 재생불량성·재발성 환자 대상의 줄기세포이식 역시 안전성·효율성 향상, 방사선 대신 고용량 항암화학요법 및 공여자 이식으로 전환

앞으로의 과제와 의의

     * 많은 선진국에서는 어린이 백혈병 진단이 더 이상 사형선고가 아니게 변화함
     * 하지만 긴 치료 과정과 부작용, 가족·환자의 심리적 부담, 일부 장기 합병증 위험은 여전히 존재함
     * 과학, 임상, 글로벌 협력, 분자생물학 발전이 치명적 질병을 치료 가능한 질병으로 전환시킨 모범 사례임
     * 향후 연구·혁신은 세계 각지 어린이에게 동등한 치료 접근성 확대에 초점 맞춤 필요
     * 전체적으로 의료 연구의 힘이 사회·삶에 미치는 긍정적 영향을 보여주는 대표적 사례임

        Hacker News 의견

     * 2020년에 제 아들이 B-ALL(RUNX1) 진단을 받았던 경험 공유. 오늘(미국의 아버지의 날)엔 깊이 이야기하고 싶진 않지만, 지금은 건강하게 잘 지내는 상태. 2년 전쯤에 병의 완치를 알리는 종을 울렸음. Children's Oncology Group 연구에 등록돼서 남아 환자에게 시행된 실험적 치료도 받았던 경험. 그 당시에는 남아가 고환 때문에 추가 6개월 정도 치료를 더 받아야 한다는 프로토콜이 있었지만, 데이터로는 그렇지 않다는 결론이 났고, 장기간 항암 화학요법으로 인한 부작용이 오히려 더 크다는 판단. 다행히 진단도 빨랐고, 모든 검사에서 기대하는 최상 결과만 나왔던 행운도 있음. 내 경험에 관해 남긴 코멘트들이 많지는 않지만, Hacker News에서 여러 이야기를 찾아볼 수 있음
          + 이 댓글 오늘 하루 내내 생각할 예정. 그냥 응원 메시지가 아니라 인류가 이렇게 가치 있는 일을 함께 이뤄내는 것이 정말 감동적이라는 생각. 이런 것이야말로 '아버지의 날'에 상기하고 싶은 부분
     * 제 아버지는 60년대 후반부터 소아 혈액종양 전문의로 일했음. 본인은 연구와 임상 치료로 완치율을 계속 올릴 수 있다고 확신했고, 평생 이를 위해 헌신한 삶을 살아옴. 항상 긍정적이었던 아버지가 비관적인 상황에서도 희망을 가졌던 해답으로 이 글에서 언급된 추세를 예로 들곤 했던 기억. 아이를 잃는 아픔이야 늘 컸지만, 미래에 대한 시선을 늘 잃지 않았던 모습. 이 사례야말로 과학과 의학의 위대한 성취라는 생각
          + 그 시대 소아 종양 전문의가 되는 감정적인 무게감이 얼마나 컸을지 상상조차 어렵다는 생각
     * 나는 89년에서 95년 무렵에 ALL 백혈병을 겪었던 생존자. 병원에 오래 입원했고, 이른 아침 수술, 그리고 삶 전체가 바뀌는 부작용 등 잊을 수 없는 경험. 언젠가는 이 병을 완전히 없앨 수 있을 거라는 희망도 있음. 모든 아이가 저렴하고 접근 가능한 치료를 받을 수 있도록, 서구에서 축적한 경험이 전 세계적으로 확산되기를 바라는 생각
          + 내 아들이 ALL을 겪었고 지금은 관해(완치) 상태. 그는 자폐가 심해서 말이 거의 없으니 성격에 미친 영향을 어떻게 평가해야 할지 모르겠음. 3세에서 6세까지 치료받았음. 가족 전체(부모, 형제)에게는 엄청난 영향이 있었고, 나 역시 군 복무 중 얻은 뇌손상(TBI) 때문에 불안 장애가 생겼던 터라 지금은 의사가 건강 관련 PTSD라고 부르는 증상을 겪고 있음
     * 기사에서 “어떻게” 치료 성적이 좋아졌는지에 대한 설명이 부족하다는 의견. 핵심은 '환자마다 유전자형, 연령, 질병 아형에 맞는 정확한 용량 투여'에 있음. 최근 20년 동안 암 치료 관련 유전자, 대사 효소, 약물 배출 속도 등에 맞춘 최첨단 유전체 분석을 도입한 것이 큰 역할. 실제 투입 약물 종류는 최근까지 크게 변하지 않았지만, 임상 완치와 생존율에서 매우 큰 진전이 있었음. 이를 가능하게 한 NIH, 전 세계적인 기부를 받아온 Saint Jude Children’s Research Hospital 등 다양한 기관의 지원 덕분이라는 말도 강조
     * 이런 의료적 진전이 한 번에 대단해 보이지 않을 수 있지만, 그 과정에 관여한 수많은 사람들이 결국 여러 생명을 구했고 앞으로도 계속 그럴 것이란 점이 놀랍고 감탄스러운 부분
     * Don Pinkel이라는 인물이 잘 알려지진 않았지만 60년대 St. Jude에서 소아 급성 림프구성 백혈병 조합 요법을 최초로 개발해서, 완치율을 거의 0에서 50%까지 올린 선구자임
       https://smithsonianmag.com/innovation/…
     * 보통 아동이 아는 주변의 암 환아 수는 (소아암 발병률) x (K-8학교 크기)를 앞뒤 학년 모두 관찰하니 2배로 계산할 수 있음. 대략 10만 명당 20명 발병, 보통 K-8 학생 수 2,000명 정도 가정하면 (20 / 100,000 x 2,000 x 2)로 약 1명꼴. 지난 몇십 년간 어린이 암 사망률이 10배 이상 줄어서, 70년대에는 주변에 암으로 사망한 또래가 흔했지만 오늘날은 그렇지 않다는 것
          + 내 경험으론 이것이 사실처럼 느껴짐. 2학년 때 쌍둥이 중 한 명이 백혈병 걸렸고, 골수이식 완치까지 갔었음. 3학년 때는 한 명뿐이었음. 77년 이후 엄청나게 상황이 개선된 것
     * 90년대~2000년대 초 세르비아와 몬테네그로에서 자란 내 유년기에도 백혈병을 앓았던 친구들이 최소 2-3명 있었음. 모두 살아남았고, 이웃의 한 아이는 치료가 너무 고통스러웠지만 결국 완치. 당시도 이미 치료가 매우 잘 되어 완치가 일반적인 결과라는 인식이 있었음. 불과 몇 년 전(80~90년대 초)만 해도 소아암 사망이 흔했고, 주변에서 실제로 자녀를 잃는 부부들도 두 쌍이나 알고 있음. 한편으로 선천성 심장병을 가진 친구들도 있었는데, 이쪽은 결과가 좋지 않은 경우도 많았음
     * 나는 ALL 생존자. 2000년 봄부터 2003년까지 중고등학교 시절 치료받았고, 치료로 인한 부작용(기억력 저하, 집중력 저하 등)도 분명히 있음. 그래도 컴퓨터공학 학위 취득 및 소프트웨어 엔지니어로 일하고 있음. 놀라운 점은 미국 전체 학생의 만성 결석률이 30%라는 사실. 나는 암 치료로 학교 결석 10% 정도 됐는데, 요즘 아이들이 암 환자만큼 학교를 빠지는 현실이 걱정스러움. 치료 과정이 가족에게도 큰 부담이라는 점도 강조하고 싶음. 아이를 병원에 오래 혼자 두고 싶지 않아도 부모가 생계를 위해 어쩔 수 없는 경우가 있다는 고민도 있음
     * 큰 전제조건은 ‘치료 접근성’. 이런 의료 발전은 대부분 고소득국가에 국한되어 있음. 전 세계적으로 성공을 복제하는 것이 앞으로의 과제
          + 실무적으로 맞는 지적이라는 생각. 고가의 약물이 아니라, 용량과 타이밍을 적절하게 맞추는 것이 성공의 핵심 성분
          + 저소득국가는 출산율이 높고 아이가 많아서 어느 정도 균형을 맞추는 부분도 있다는 생각
"
"https://news.hada.io/topic?id=21463","edamagit - VSCode용 Magit 확장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      edamagit - VSCode용 Magit 확장

     * Emacs의 유명한 Git 툴인 Magit의 경험을 VSCode에 그대로 재현하는 확장
     * 키보드 중심 Git 인터페이스로, 명령 팔레트 및 단축키로 모든 Git 작업을 빠르게 실행 가능
     * Magit의 상태(Status) 뷰, 커밋, 브랜치, 푸시/풀, 리베이스, 스테이징/언스테이징, 프로세스 로그 등 핵심 기능을 VSCode에서 그대로 이용 가능
          + Magit Status/Popup/Dispatch/Help 등의 기능이 기본 단축키(alt+x g, alt+x alt+g 등)로 제공되며, 모든 기능은 VSCode 명령 팔레트(> Magit)에서도 접근 가능
     * VSCodeVim 등 Vim 확장 사용자를 위해 Magit/Evil-magit 스타일의 키 바인딩 커스터마이즈 가능
     * 테마/키맵 커스터마이즈, Vim 플러그인 호환, 모노레포 지원, Forge(GitHub PR/이슈) 연동 다양한 옵션 지원

사용 예

     * Magit Status: alt+x g
     * Magit File Popup: alt+x alt+g
     * Magit Dispatch: alt+x ctrl+g
     * 내부 단축키:
          + A 체리픽, b 브랜치, c 커밋, d 디프, f 패치, F 풀, l 로그, m 머지, P 푸시, r 리베이스, z 스태시, g 새로고침, TAB 섹션 토글, RET 항목 방문, q 닫기 등
          + 스테이지/언스테이지 s/u, 리버스 v, Discard k, 전체 Stage/Unstage S/U등

   edamagit 후원자 중 한명인데 긱뉴스에서 보게되니 반갑네요
"
"https://news.hada.io/topic?id=21529","뉴욕시 감사관 Brad Lander, 이민 법정에서 복면을 쓴 연방 요원에게 구금됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             뉴욕시 감사관 Brad Lander, 이민 법정에서 복면을 쓴 연방 요원에게 구금됨

     * Brad Lander 뉴욕시 감사관이 맨해튼 이민 법원 건물 내에서 복면을 쓴 연방 요원에 의해 구금됨
     * Lander는 이민자 Edgardo와 함께 법정을 나서다 구금되었으며, 뉴욕 주지사 Kathy Hochul의 개입으로 4시간 후 기소 없이 석방됨
     * 연방 당국은 Lander가 연방 요원 폭행 및 공무집행 방해 혐의가 있다고 주장했으나, 현장 영상에서는 폭행 장면 없음이 확인됨
     * Lander 구금 사건은 정치적 논란을 불러일으켰으며, 다수의 선출직 인사와 후보들이 즉각 석방을 촉구함
     * Lander는 자신의 행동이 정치적 퍼포먼스가 아니었음을 강조하며, 뉴욕 시민과 이민자의 권리 옹호를 위해 법정에 있었다고 밝힘
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Brad Lander 이민 법정 구금 사건 개요

   뉴욕시 감사관이자 차기 시장 후보인 Brad Lander는 2025년 6월 17일, 맨해튼 로어 지역 26 Federal Plaza 내 이민 법정에서 연방 요원에게 구금됨. 해당 구금 사건은 이민자 Edgardo를 법정 출입구까지 동행하던 중 발생했으며, Lander는 현장에서 Edgardo와 팔짱을 낀 채 요원들에게 해산 요구를 거부함.

현장 상황 및 연방 요원 대응

     * 복면을 쓴 연방 요원들이 이민 법원 실내에서 대기한 후, Edgardo와 Lander가 법정을 나서자 군중을 밀어내며 Lander를 체포함
     * Lander는 현장에서 반복적으로 사법 영장 제시를 요구했으나, 요원들은 그의 손목에 수갑을 채움
     * The CITY 기자에 따르면, 연방 요원이 ""감사관을 체포할까?""라고 말한 것이 포착됨

연방정부 및 정치권 반응

     * DHS(미국 국토안보부) 측은 Lander가 공무집행 방해와 폭행 혐의로 체포됐다 발표하며, ""누구도 법 위에 있을 수 없다""고 강조함
     * DHS 공식 SNS는 ""정치인들이 바이럴 모멘트를 노리고 법 집행 안전을 해치는 것은 잘못""이라는 메시지 삽입
     * 현장 영상에는 Lander가 Edgardo에게 매달린 채 있지만 폭행 정황은 직접적으로 나타나지 않음

석방 과정과 정치적 파장

     * 뉴욕 주지사 Kathy Hochul은 해당 사건에 대해 ""터무니없음""이라 즉각 반발하며, 26 Federal Plaza에 직접 방문해 Lander 가족을 격려 후 1시간여 ICE 요원과 협의, Lander의 석방을 이끔
     * 석방 후 Lander는 Foley Square에서 지지자들과 만나, ""나는 적법 절차와 권리 보호를 받을 수 있으나, Edgardo는 그럴 수 없는 현실""을 강조함
     * 많은 선출직 인사 및 시장 후보들이 즉각적인 석방을 촉구하며, '정치적 스턴트' 여부 질문에 대해 ""오늘 체포될 줄은 몰랐으며 단지 이민자를 돕는 것이 목표였음""을 반박함

최근 유사 사례와 맥락

     * 최근 트럼프 행정부 주도 하에, 이민자 대책에 앞장선 민주당 선출직 인사들이 연이어 강경 대응을 겪음
          + 상원의원 Alex Padilla(민주, 캘리포니아)는 국토안보부 장관에게 질문하던 중 체포됨
          + 뉴저지 연방 하원의원 LaMonica McIver 또한 이민자 수용시설 방문 과정에서 연방 기소
          + Newark 시장 Ras Baraka도 유사 사건 연루 후 기소됐으나 판사에 의해 기각

Lander의 메시지 및 향후 쟁점

     * 민주당 예비선거를 앞둔 시점임에도 Lander는 ""지금 이 순간 법치주의를 지키는 일이 가장 중요함""이라 답함
     * ""Andrew Cuomo와 달리 실제로 현장에 나서 사람을 지킨다는 것을 보여주는 것이 의미있음""이라 강조함
     * 이 사건은 이민자와 옹호자 체포를 둘러싼 뉴욕시와 연방정부의 갈등, 그리고 차기 시장 선거의 정치적 함의 등 다양한 쟁점을 남김

관련 이민 법정 체포 사건

     * 복면을 쓴 연방 요원들이 빈번히 Manhattan 내 이민 법정에서 이민자 및 참관인 체포 사례 증가
     * 2025년 5월, 목사 등 참관자 포함 20여 명이 법원 로비 및 외부에서 체포
     * 2025년 5월 29일, 인근 법원에서 일일 7건 이상의 이민자 체포 발생, 트럼프 행정부의 '법원 급습' 기조 확인
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   이 사건은 최근 강화되는 연방 이민 정책의 실체, 뉴욕시와 정치인들의 대응, 그리고 시민권·적법절차 보장을 둘러싼 주요 쟁점을 구체적으로 드러냄

        Hacker News 의견

     * 실제 기사 제목은 ""Brad Lander Detained by Masked Federal Agents Inside Immigration Court""임을 강조하고 싶음, HN에 올라온 현재 제목과 달리 Lander가 영장 제시를 요구했다는 이유로 체포된 것이 아니고, 기사 원문에서도 기소 여부조차 불분명함을 언급함, 만약 이 사건이 중요한 이슈라면 이런 핵심 사실을 왜곡하는 건 심각한 문제라는 지적임
          + 결국 기사 제목이 변경되었으며, 최초 제출된 제목은 ""ICE arrests NYC Comptroller because he asked to see a warrant""였음, 제출자들은 원 제목을 그대로 사용할 것, 단 제목이 오해의 소지가 있거나 낚시성인 경우만 예외라는 HN 가이드라인(https://news.ycombinator.com/newsguidelines.html)에 따라야 한다고 안내함
          + 사실만 말하자면, 실제로 연방 요원이 맞는지 증거가 없다는 점이 문제이고, 이들은 신분을 밝히지 않았기 때문에 우리가 아는 것은 그저 Lander가 납치된 것뿐이라는 주장임
          + 실제로 영장 제시를 요구해서 체포된 것이 명확하게 기록되어 있다고 주장함, 이후 DHS가 자신들의 행동을 정당화하려고 SNS 등에서 폭행이 발생했다는 근거 없는 주장을 퍼뜨렸으나, 공개된 영상에는 그런 증거가 전혀 없었고 사건을 혼동시키는 선전 수단처럼 느껴진다는 지적임
          + 이런 제목이 퍼지는 이유는 사실보다 내러티브를 중시하는 풍토 때문이라고 생각함, 그래서 요즘 세상을 ""포스트-트루스(post-truth) 사회""라고 부르는 주장임
          + 그 이유로는 클릭을 유도하려는 낚시행위, 선동, 상업적 이유 또는 저널리즘의 질적 저하 등 여러 가지가 있고, 언론의 왜곡이 언론 신뢰를 무너뜨린다는 점을 강조함
     * ICE 요원들에게 독단적 임무수행을 맡기는 결정은 집행기관을 사적으로 무장시키는 셋업처럼 보임, 명확한 가이드 없이 법적 감시나 기준 없이 활동하게 되면 사법시스템 바깥에서 그들 마음대로 행동하게 되는 셈이라 우려스러움, 대부분 비민주국가에서 보이는 패턴과 유사하다고 봄
          + 또 한 가지 문제는 이민법이 일반 형사법과는 별도의 패러렐 시스템이라는 점임, 피의자 권리가 약하고, 요원들은 더 느슨한 의무와 낮은 입증 책임만 부담하며, 영장 발부나 증거 규칙이 더 유연함, 이런 권력 집중 현상은 2001년 DHS 신설 때부터 시작되어 앞으로 어디까지 확대될지 알 수 없음, ICE의 대대적 확장도 심각하게 받아들여야 하고, 이런 문제에 왜 자유지상주의자들과 총기소유론자들은 항의하지 않는지 의아함, 영장도 없이 가면을 쓴 연방요원이 침입하고 있는데도 침묵하는 현실이 안타까움
          + 트럼프가 계엄령 선포를 노리고 있다고 주장함, 사태를 충분히 악화시켜 국민들이 그의 행정명령을 왕처럼 수용하게 하려는 시나리오라는 의견임
          + 이 계획은 Project 2025 로드맵에 자세히 나와 있다고 제시함
     * 만약 기사 링크가 삭제된다면 참고용으로 여기에 링크를 남기겠음
     * 이 사건을 계기로 법원이 Qualified Immunity(공무원 책임 면책 특권)를 재평가해주길 바라는 마음임
     * 왜 이 글이 신고(flag)되었는지 이해가 안 감
          + 이유는 여기(Hacker News)가 /r/politics와 달라서임, 많은 사용자가 미국 내 정치 이야기를 보려고 이 사이트에 들어오는 것이 아니기 때문이라는 주장임
          + 또 다른 의견으로는, 이 글이 기술과 무관하고 사이트 규칙에도 어긋나기 때문에 신고받았다는 설명임
     * 연방 요원들이 가면을 쓰고 신분증도 없이 돌아다니며 사람을 체포할 수 있다는 것이 말도 안 된다고 생각함
          + 더 나아가, 이런 감독 부재는 범죄자나 자경단이 연방 요원인 척 하며 납치나 강도를 하는 상황까지 만들어냄, 실제 이런 사례에 대한 CNN 보도 링크도 참고할 만함
          + 국가 행정기관 소속 요원이든 누구든 신분을 밝히지 않으면 안 된다고 생각함, 내가 만약 왕이라면 익명 제보조차도 불가능하도록 정부의 모든 일은 투명하게 처리하게 하고 싶음, 처음에는 반발이 있겠지만 결국 국가와 국민 간의 이해관계가 더 잘 맞춰질 것임, 남몰래 행할 가치가 없는 일이라면 애초에 할 이유가 없다고 믿음
          + 주지사들이 왜 이런 불법 납치 갱단에 맞서 평화를 지키려고 주방위군을 투입하지 않는지 의문임, 다른 일반 납치사건에는 해당되지만, 이번 사건은 파시즘 성향의 기관에서 벌어진 일이어서 해당성은 약할 수 있음, 그래도 주 정부가 법에 따라 정의롭게 선을 긋는 것이 중요함, 가면을 쓰고 신분증도, 합법적 체포영장도 없는 이들은 사실상 불법집단에 불과하다고 비판함
          + 미국인들이 ‘연방 정부의 권력에 맞서기 위해 총이 필요하다’고 하다가, 정작 신원 미상 연방요원이 영장도 없이 사람을 체포하는 일에는 이를 묵인하는 태도를 보여줌, 만약 연방요원들이 자신의 불법행위에 따른 반발이 두렵지 않았다면 분명 신분을 제대로 밝혔을 것임
          + 이들이 실제 연방요원이었다는 증거조차 없음, 개인적으로는 Tren de Aragua(베네수엘라 범죄조직) 멤버였던 것 같다는 의견임
     * 트럼프가 하루 3천 명씩 체포하라고 지시했다는 언급, 이 목표 달성에 연봉 4만 불 보너스가 연계된 걸로 아는데 이게 실질적인 체포 인센티브 아닌지 의심스럽고, 체포 방법에 신경도 쓰지 않는 이런 시스템을 역사적으로 평가하면 좋게 남지 않을 거라는 생각임
     * 다시 한 번 Brad Lander가 이민법원 내 마스크 쓴 연방요원에게 억류되었던 실제 기사 링크를 남김(https://thecity.nyc/2025/06/…)
     * ""연방요원들이 함께 엘리베이터로 이동했고, 그 중 한 명은 NYPD 케빈이 동행하고 있었다""라는 기사 내용을 근거로, Lander 신변을 보호하던 시큐리티 요원이 제 역할을 못한 것 같다는 의견임
     * 이런 일이 처음이 아니며, 지난주에도 연방요원들이 질문을 했다는 이유로 상원의원 Padilla를 바닥에 제압한 사건이 있었음을 상기함
"
"https://news.hada.io/topic?id=21543","웹사이트들이 브라우저 핑거프린팅을 통해 사용자를 추적하고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   웹사이트들이 브라우저 핑거프린팅을 통해 사용자를 추적하고 있음

     * 브라우저 핑거프린팅은 쿠키 삭제만으로는 막을 수 없는 온라인 추적 방법임
     * Texas 대학 연구팀이 FPTrace라는 측정 프레임워크로 광고 입찰, HTTP 기록 변화를 통해 실제 추적 활용 사례를 입증
     * Fingerprint(지문)가 변경될 때 광고 입찰가 변화와 HTTP 기록 감소 현상이 발견됨
     * GDPR, CCPA 등 프라이버시 법률에 따라 추적 거부해도 핑거프린팅 기반 추적은 계속 발생함
     * 연구진은 현행 프라이버시 도구와 정책이 불충분하다고 지적하며, 규제와 기술적 방어 강화 필요성을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Websites Are Tracking You Via Browser Fingerprinting

     * 쿠키 삭제만으로는 온라인 프라이버시를 완벽히 보호할 수 없음
     * Texas A&M University가 주도한 최신 연구에 따르면, 웹사이트들이 브라우저 핑거프린팅이라는 방법으로 세션과 사이트를 넘나들며 사용자를 추적함
     * 브라우저 핑거프린팅은 사용자의 스크린 해상도, 시간대, 기기 모델 등 다양한 정보를 조합해 고유한 브라우저 식별값을 생성함
          + 쿠키와 달리 사용자가 쉽게 삭제하거나 차단할 수 없음
          + 대부분의 사용자가 이러한 추적이 이루어지고 있다는 사실조차 인지하지 못함
          + 프라이버시 중심 브라우저조차 완벽하게 차단하기 어려움

FPTrace 프레임워크로 웹 추적 실태 심층 조사

     * ""알지 못하는 사이 남기는 디지털 서명""과 같음
     * 기기나 브라우저의 조합만으로 사용자가 익명이라 해도 쉽게 추적 가능함
     * 연구팀은 실제 광고 시스템에서 핑거프린팅이 어떻게 활용되는지 세계 최초로 실증적으로 규명함
          + FPTrace라는 측정 프레임워크를 개발, 브라우저 핑거프린트가 광고 입찰 및 HTTP 통신에 미치는 영향을 분석함
          + 핑거프린트가 바뀔 때 광고 입찰 값, HTTP 기록, 동기화 이벤트가 변하는 현상을 관찰하여 추적 실태를 밝혀냄

연구 결과 및 시사점

     * 사용자가 쿠키를 삭제하거나 차단해도 핑거프린팅을 통한 추적이 지속적으로 이루어짐
     * 핑거프린팅 사용 시 웹사이트 일부는 백엔드 광고 입찰 과정에 지문 데이터를 반영하는데, 이 과정에서 3자 업체에 식별 정보를 넘길 가능성도 확인됨
     * 유럽 GDPR, 미국 CCPA 등 프라이버시 법률에 따라 추적 거부를 선택해도 핑거프린팅 기반 추적은 중단되지 않음
     * 연구진은 현행 프라이버시 툴과 정책이 충분하지 않다고 강조하며, 더 강력한 기술적·제도적 방어책 필요성을 주장함
          + FPTrace 프레임워크가 웹사이트 및 광고 제공자의 비동의 추적 여부를 감사하는데 도움이 되길 기대함

        Hacker News 의견

     * 나는 이 기술 분야에서 일하고 있는 사람으로서, 지문(fingerprint)이 실제로 얼마나 오래 지속되는지에 대한 이야기는 거의 듣지 못함을 느끼는 중임. 실제로 매우 정밀한 지문 정보도 반감기가 며칠밖에 되지 않음(특히 창 크기나 소프트웨어 버전 등으로 만든 경우 더욱 그렇다는 점 강조). 현재 주요 광고 네트워크들은 오히려 위치 데이터에 크게 의존하고 있음. 그래서 여러 기기간에 연결된 것처럼 보이거나, 배우자나 친구의 관심사를 반영한 광고가 피드에 많이 보이는 이유도 여기 있음. IP 기반의 위치만으로 아주 많은 영역에 광고를 뿌리는 흐름임. FPTrace라는 측정 프레임워크가 지문 기반의 사용자 추적을 분석하는 도구라는 점이 흥미로우며, 그들의 구체적인 연구 방법이 궁금해지는 부분임. 광고 네트워크들은 지문 자체보다는 기기 설정을 기준으로
       광고 집단을 나눌 확률이 높다고 생각함. 예를 들면, 최신 소프트웨어와 최신 하드웨어를 쓰는 사용자는 '구매 의향이 높은 집단'으로 묶일 수 있음. 시간대 같은 단순한 요소도 광고 입찰 결과에 큰 영향을 미치므로, 이 연구에서 어떤 변수를 어떻게 제어했는지가 매우 중요하다 생각함
          + 내 정보를 amiunique.org에서 확인한 결과, 정말로 나는 유일하다는 판정을 받음(어머니가 그러셨던 것처럼!). 다만 이 사이트에서는 어떤 요소를 바꾸면 비유일적으로 바뀌는지 알려주지 않고, 58개 자바스크립트 속성 중 16개가 최저 유사도 범주임. 그 중 버전 번호에 직접 의존하는 것은 2개, 화면 크기/해상도 관련이 6개임. 결국 금방 바뀌지 않을 여러 정보가 남아있다는 인상임. 시간이 지나면 정확한 값이 달라질 수는 있지만, '반감기가 며칠'이라는 표현은 이 기술의 실제 효과를 과소평가한 것처럼 느껴짐
          + 윈도우 최대화 시 창 크기는 환경이나 모니터 교체, 데스크톱 환경 업데이트가 없으면 거의 바뀌지 않음. GPU 하드웨어 역시 빈번히 바뀌는 요소가 아니며, WebGL이나 WebGPU를 이용해 고유 특성이 손쉽게 지문에 활용 가능함. 설치된 폰트도 자주 바뀌지 않음. TCP 스택의 지문 역시 상당히 안정적임. 이런 요소 몇 가지만으로도, 개별 특성 하나만 바뀌어도 이전 지문 클러스터와 쉽게 연결 가능함. 더 심각한 것은 쿠키 같은 클라이언트 측 식별자를 동시에 지우지 않을 경우, 완전히 다른 두 지문 사이도 명확히 연동 가능하다는 점임
          + 하드웨어 인터럽트 처리 시간과 그 지연 역시, 설치된 앱 조합이나 GPU 드라이버 버전 등 세부 요소에 따라 고유값이 달라질 수 있다는 점을 고려하면, 정말로 업데이트가 이루어져야만 분포가 변하고, 모든 분포가 동시에 바뀌는 일은 드물다는 생각임
          + Siteimprove Analytics는 자사의 cookieless 추적 기술이 기존 쿠키 기반 추적보다 정확하다고 공개적으로 주장 중임. Visitor Hash는 개인 정보를 배제한 IP와 HTTP 헤더(브라우저 종류, 버전, 언어, user agent 등)를 해시해서 만들어지므로 기존 쿠키의 단점인 '짧은 수명' 문제를 해소하고, 고유 방문자 통계의 정확도를 높이는 데 유리함을 내세움. 다만 server-side 속성만 사용하며, 클라이언트 측 속성은 수집하지 않음. 인트라넷처럼 동일 IP/기기 환경에서 다수 유입시, 여러 사용자가 같은 Visitor Hash를 갖게 되어 방문이 하나로 뭉치는 단점이 있으니, 해당 유형 도메인은 cookieless 추적에서 제외할 것을 권장함
          + 브라우저 지문은 선택하는 데이터 포인트에 따라 매우 견고하게 만들 수 있음(예: 설치된 플러그인, 콘텐츠 언어, 폰트 등). 데이터 포인트를 상황에 따라 동적으로 조정하거나, 사용자별로 다르게 활용하는 접근이 가능함. 또, 지문은 전체 데이터의 일부일 뿐임. 위치데이터처럼 다른 정보와 결합하면 제한이나 회피책을 상당 부분 무력화함. 예컨대, 기존 지문의 80% 유사한 새 지문이 동일 직장 IP에서 추가되고, 원래 지문이 사라진다면, 이 둘의 연계가 쉬움. 광고 회사 자체는 비용효율 및 합법성 방어 목적으로 '샷건 전략(광범위 타기팅)'을 선호하지만, 광고 외 목적의 조직은 데이터 포인트가 더 많아 훨씬 정밀하게 추적할 수 있음
     * amiunique.org에서는 브라우저가 화면 해상도, 시간대, 기기 모델 등 다양한 정보를 드러내며, 이들을 조합해 '지문'을 만들 수 있다는 점이 강조됨. 쿠키와 달리, 이런 지문 정보는 사용자가 삭제하거나 차단하기 어렵기 때문에 훨씬 탐지나 방지 자체가 힘듦. 아이러니하게도, 기기 및 OS, 브라우저 보안·개인정보 강화에 집착할수록 오히려 내 지문이 더 독특해지는 문제를 야기함. FOSS 생태계에 오랜 역사가 있었지만, 제대로 된 오픈소스 브라우저가 주류를 차지하지 못한 점은 아쉬움. 독점이 초기에 너무 수익이 컸기 때문이며, 개인적으로 오프라인 접근을 위해 웹 스크레이퍼 구상도 해봤지만 실용성은 떨어진다는 판단임
          + ""제대로 된 오픈소스 브라우저가 떠본 적이 없다""는 말은 정확하지 않음. Firefox는 한때 엄청나게 인기 있었고, 시장을 완전 장악했던 경험이 있음. 이후 Google이 불공정 행위로 이를 잠식했지만, 그건 나중 이야기임
          + Firefox는 오랜 시간 동안 지문 추적의 효과를 줄이는 데에 거의 아무런 실질적 조치를 하지 않았다는 점이 놀라움. 2025년에도 여전히 브라우저가 너무 자세한 User Agent 스트링을 기본 전달하는 현실은 이해하기 어려움(Mozilla/5.0 (X11; Linux x86_64; rv:139.0) … 등). 웹사이트가 내가 X11을 사용 중인지, x86_64 리눅스인지 알 필요가 전혀 없음. 기본적으로 Referer(리퍼러)도 여전히 켜져 있음. 자바스크립트가 내 시스템에 설치된 폰트 목록을 알아내는 것도 가능함. 훨씬 세분화된 권한 제어와 합리적인 기본값이 필요함. 관련 플러그인은 있지만 설치·운용이 번거로움
          + Brave처럼 지문을 무작위화해서 추적 회피를 시도하는 브라우저도 있지만, 개인적으로 실질적 효과는 의문임. 또 다른 방법은 다수가 쓰는 Tor와 같은 환경에 섞여 '흔적 감추기' 전략을 택하는 것임
          + 서로 다른 두 개의 프라이빗 브라우저 창에서 모두 유일한 사용자로 판명되었음. 즉, 프라이빗 탭 간에는 지문 연동이 불가능하다는 뜻인지 궁금한 점 남음
          + ""제대로 된 오픈소스 브라우저""라는 기준에서 Firefox가 빠지는 이유가 무엇인지 궁금함
     * ‘지문 추적이 실제 사용자를 얼마나 장기간 동일하게 표시하는지’를 측정하는 테스트가 coveryourtracks.eff.org나 amiunique.org보다 더 잘 설계됐으면 좋겠다는 바람임. 두 사이트 모두 고유성만 테스트할 뿐, 지속성은 체크하지 않음. 그래서 완전히 랜덤 넘버 생성기도 지문이라 인식할 수 있음. 실제 지문 보호 기술은 종종 무작위 출력을 포함하므로, Tor, Safari, LibreWolf처럼 통과한 브라우저도 이런 사이트에서는 오히려 실패 처리됨
          + CreepJS는 자신의 지문에 이름(시그니처)을 붙이고, 다시 접속했을 때 동일 지문 여부를 확인할 수 있는 사이트임
          + fingerprint.com이 이런 '시간 경과에 따른 결과 테스트'를 제공할 수 있다고 들음. fingerprinting as a Service 분야에서는 최고 수준이며, Meta와 Google이 그보다 앞설 뿐임
     * ""지문 추적이 실제로 일어나고 있다""라는 점은 이미 다들 어느 정도 예감하고 있었지만, 구체적 증거 없이는 '기기 간 추적'이 정말 이루어지는지 입증하기 어려웠던 게 현실임. 이번 연구는 스터디를 위한 프레임워크와 대규모 실험 설계를 제안해 실제 지문 추적이 광고 영역에서 일어나는지를 실증적으로 확인하려고 한 것임. 기존 논문 대부분은 지문 관련 스크립트 실행 여부만 측정했기 때문에, 그것만으로는 추적 목적인지(혹은 봇/부정행위 방지, 인증 같은 방어적 목적인지) 알 수 없었음. 이번 연구는 브라우저 지문을 인위적으로 조정하면서 광고 변화까지 추적해, 실제 추적 맥락을 밝혀낸 것이 흥미로움(논문 링크). 논문 원문은 열람 불가라 더 구체적 내용은 확인하지 못함
     * 쿠키는 특정 도메인마다 별도 저장되어 보안 경계 안에 있지만, 지문은 도메인에 상관없이 연산 가능함. 광고 서버 등이 지문만으로 사용자를 추적·파악하는 상황도 충분히 상상 가능하며, 이런 지문 정보만 모아도 피해자에 대한 정보를 수집할 수 있다는 점이 문제임
     * ""왜 브라우저가 이렇게 많은 정보를 웹사이트에 기본적으로 노출하냐?""는 질문에 대해,
          + 브라우저란 자체가 다양한 기능(API) 샌드박스로 이루어져 있음. 각각의 기능은 사용자 편의를 위한 것인데, 이들이 개별로는 중요해 보이지 않아도 합치면 하나의 독특한 지문이 됨. 진정으로 지문이 없는 환경을 원한다면, 웹의 자바스크립트 전체를 없애야만 한다는 결론임
          + 개발자들이 기능 제공을 위해 이런 API를 원했고, 사생활 영향은 이미 ""돌이킬 수 없는 상황""이 된 다음에야 주목 받게 됨
          + 대부분의 정보는 실제로 유용하거나 필요함. 일부만 빼는 게 가능하긴 한데, 나머진 '행동 결과를 비교 분석하는 방식'임(예: 여러 font-family로 텍스트 박스를 렌더하면, 기기별 폰트 차이로 실제 크기가 달라짐 — 이 자체가 지문으로 활용 가능함)
          + 브라우저가 일부 정보(예: user agent OS 버전 등)를 줄이거나 제거하면서, 예상치 못한 사이트 에러가 속출함. 예컨대 Apple이 user agent의 버전만 10에서 11로 바꿨을 때도 많은 사이트가 멈췄음. Referer 필드는 최근 브라우저에서 경로나 전체 누락 등으로 크게 제한되고 있음
          + Mozilla 고위진의 프라이버시/보안/자유에 대한 실질적 의지가 항상 부족했다고 생가함. 때로는 단순히 '마케팅 관점'에서만 접근해, 실효성 없는 변화를 주거나, 대형 기술 기업의 이해와 완전히 충돌하지 않는 방식만 고집함. W3C에서조차 강하게 대립하는 인물이 없다는 점을 아쉬워함
     * 앱이 웹사이트보다 훨씬 심각하게 사용자 추적함. 사이트들이 왜 끊임없이 앱 설치를 유도하는지에 대해, 브라우저에서 사용하는 많은 보호장치가 앱 환경에서는 모두 무력화되기 때문임. 앱은 로그인을 요구하고, 이후 모든 데이터를 제3자와 자유롭게 공유할 수 있음
          + 내 앱은 그런 식 추적을 하지 않음. 이메일조차 받지 않으므로, 새로운 알림을 알려줄 유일한 방법으로 앱을 활용함. 앱은 지속성 측면에서 유리하고, 웹사이트는 상대적으로 덜 효과적임
          + iOS에는 '앱 추적 금지 요청(Ask App Not to Track)'이라는 기능이 있음. 다만 특정 유형의 추적만 막고, 모든 추적을 차단하지는 못함
     * '지문 추적이 실제로 광범위하게 사용되고 있다'라는 사실에 대해, ""아카데믹 영역 바깥에 있는 문서들을 안 본 사람""이나 ""책임 있는 추적 벤더들은 이미 수년째 지문 추적을 명시적으로 밝히고 있다""고 지적함
          + 본질은 학회나 산업계의 무지라기보다는, 이런 연구가 '실제로 지문 추적이 현재 얼마나, 얼마만큼 효과적으로 일어나는지'를 정량적으로 입증했기에 그 자체로 유용하다는 의견임. 공급업체가 정책상 이미 밝히고 있었더라도, 실제 효과나 규모에 대한 인사이트는 별도임. 이번 연구가 광고 등 '악의 없는(benign)' 환경에서도 추적 성공률을 검증했다면, 다른 주체가 추적할 때도 얼마나 효과적일지 알 수 있는 기반임
          + 이미 학계에서도 수년 전부터 지문 추적 사용을 인지하고 있었음. 예전에는 Flash를 이용해 사용자가 설치한 폰트 정보를 직접 뽑아내는 기법도 널리 쓰였음(관련 논문). 이런 공식 언급이 실제와 다르다고 정정함
          + FingerprintJS처럼 오래전부터 존재했던 오픈소스 지문 추적 프레임워크도 있음. 초창기에는 스팸 또는 악의적 방문자 추적용으로 많이 활용함
          + 온라인 프라이버시를 지키는 것이 중요하긴 하지만, 결국 지문 추적 자체를 막으려는 대응방안들이 현실적 문제를 제대로 해결하지 못하고 오히려 웹을 더 불편하게 만든다는 주장도 있음. 비유를 들면, 규제가 있더라도 악의적 주체들은 여전히 지문 추적을 강행하고, 일반 사이트들만 기능 제약을 받아 역효과가 난다는 지적임
          + 학술 연구에서는 업계 정책이나 고지 대신, 구체적으로 측정 가능하고 반박할 수 없는 '실증적 근거' 확보를 중시한다는 관점임
     * EFF의 fingerprint 페이지에 들어갈 때마다 매번 고유 지문 판정을 받음. 한 시간 뒤에 재접속해도 변함없음. 이 사이트가 fingerprint의 해시 값을 제공해서 몇 달 후에도 비교할 수 있으면 좋을 것 같음. 실제로 내 지문이 매번 바뀐다면 지문 추적도 훨씬 어려워질 것 같아서 한편으론 다행임
     * 이렇게 많은 정성과 기술이 광고 타기팅에 쓰인다는 점에 회의적임. 나는 기본적으로 모든 광고를 차단하는데, 지문 추적 노력들이 다 허사임을 느끼는 중임
"
"https://news.hada.io/topic?id=21535","Unregistry – Registry 없이 Server 직접 docker push하게 해주는 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Unregistry – Registry 없이 Server 직접 docker push하게 해주는 프로젝트

     * Docker Registry 없이 바로 Server에 Docker image를 push 할 수 있게 해주는 프로젝트
     * 누락되거나 변경된 Layer만 push 할 수 있는 기능을 제공
     * 사용법 예시: docker pussh myapp:latest user@server

   원본 HN글 - Unregistry – “docker push” directly to servers without a registry

   pussh 빼고 다 괜찮아 보이네요ㅋㅋ 너무 오타같아요..

   오 개인 테스트할 때 항상 도커허브에 푸쉬하고 풀 받게 했었는데 재밌는 프로젝트네요. 한번 사용해봐야겠네요.

   이거 꽤 좋은 아이디어네요.. 도커 허브를 쓰거나 self hosted 레지스트리를 이용하는 게 일반적이지만
   관리 포인트가 늘고 고정 비용도 늘어나는 문제가 있어서..
   자잘한 서비스를 많이 운영하는 작은 스튜디오나 개인은 관리가 좀 번거롭다고 생각해요.
   첫 릴리즈가 반나절도 지나지 않은 정말 따끈따끈한 프로젝트이긴하지만 한번 이용해봐야겠다는 생각이 들었네요
   재밌는 소개 감사합니다.
"
"https://news.hada.io/topic?id=21477","사회불안장애 연관 장내미생물이 사회적 두려움을 증가시킴","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     사회불안장애 연관 장내미생물이 사회적 두려움을 증가시킴

     * 사회불안장애와 연관된 장내미생물이 사회적 두려움 형성에 중요한 역할을 함
     * 실험에서 사회불안장애 환자의 장내미생물을 이식받은 쥐에서 사회적 회피, 불안 반응 증가 현상 확인
     * 해당 연구는 장-뇌 축이 사회적 행동과 정서장애에 미치는 영향에 대한 새로운 증거로 작용함
     * 장내 환경 변화가 신경학적·행동적 결과에 미치는 직접적 연관성 강조
     * 이 결과는 향후 장내미생물 조절을 통한 정신건강 치료 분야에 새로운 접근법 제공 가능성 제시
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 개요

     * 본 연구는 사회불안장애와 연관된 장내미생물이 사회적 행동 및 두려움 형성에 어떠한 영향을 미치는지 분석함
     * University College Cork의 Microbiome Ireland와 여러 관련 학과에서 공동 수행함

실험 방법 및 결과

     * 사회불안장애 환자의 장내미생물을 무균 쥐에게 이식하는 방식으로 실험을 진행함
     * 이식받은 쥐들은 사회적 접촉을 회피하고, 스트레스 상황에서 불안 수준이 증가하는 행동 패턴을 보임
     * 대조군에 비해 일정 행동 양식과 뇌 신경 신호 전달 경로의 변화까지 포착함

의미와 영향

     * 본 연구를 통해 장-뇌 축 개념이 뒷받침되며, 장내미생물 조성이 사회성 및 정서적 장애와 밀접한 연관이 있음을 시사함
     * 기존 정신질환 치료가 뇌 기능만을 중점적으로 다루던 점에서 벗어나, 장내미생물 조절이 새로운 치료 전략이 될 수 있음을 암시함

향후 전망

     * 이번 연구 결과는 사회불안장애 및 이와 연관된 정서장애 치료에 있어, 프로바이오틱스, 식이 요법 등을 활용한 장내 환경 변화 전략의 가능성을 제시함
     * 사회성 결핍이나 불안장애 분야에서 장내미생물을 새로운 표적으로 삼은 연구가 더욱 활성화될 전망임

        Hacker News 의견

     * 나는 내 경험상 설탕을 먹으면 불안감이 증가하는 느낌을 받음. 머리에서 특정한 불안감이 물리적으로 느껴짐. 특히 어색한 것을 생각할 때 이런 느낌임. 그런데 몇 주 동안 단 것을 끊었더니 이 불안감이 사라진 경험. 그래서 설탕 섭취를 줄이려고 시도 중. 이유는 불안감 감소와 당뇨 위험 때문임. 그래서 이런 연구 결과를 전적으로 신뢰함
          + 나도 비슷한 경험. 내가 케토제닉 다이어트 중일 때 불안감이 완전히 사라진 느낌. 하지만 계속 케토 식단을 유지하는 것은 힘듦. 그래도 탄수화물을 줄이면 상당한 도움이 됨. 완전히 같진 않지만 효과 있음
          + 설탕이 문제라면 왜 카페인은 아니냐는 의문. 4주 정도 커피와 카페인 휴식을 하면 정서 조절이 좋아지고, 강박적 행동과 불안이 줄어드는 경험. 이게 장내 미생물 때문인지, 아니면 커피 성분에 의해 활성화되는 신경수용체의 다운 레귤레이션 때문인지 확신 없지만 실제 체감. 카페인은 몇 시간 내에 배출된다는 주장도 있지만, 내 경험상 장기간 효과가 있음
          + 대부분 사람은 설탕에 크게 영향을 받지 않지만, 일부는 예민하게 반응. 이런 소수 사례도 주목해야 함. Sarah Wilson이 “I quit sugar”로 무설탕 운동을 하다가 이후 조언을 완화한 것도 더 넓은 대중을 위한 선택이라 생각함. 조언은 각자 신체 특성에 맞게 맞춤형으로 해야 한다고 봄. 개인적으로는 ADHD에 설탕 중독 경험이 있어서, 설탕 대신 과일로 대체하면서 건강하다고 생각했는데 그마저도 과다 섭취였음을 나중에 알게 됨. 그래서 과일까지도 제한해야 진짜 무설탕 식단이라 생각함
          + 설탕이란 단어를 이야기했는데, 포도당과 과당 중에 무엇을 의미하는지 궁금. 과당과 염증 사이의 연관성에 대한 과학 논문이 많음. 평균적으로 사람은 하루에 약 30g의 과당만을 소화 가능. 일부는 그 이상, 일부는 훨씬 적게 소화. 탄산음료 한 캔이나 사과 세 개가 약 30g 과당임. 고강도, 저강도 염증이 이후 불안감으로 연결될 수 있음
     * 장내 미생물이 이런 반응을 유발할 수 있다는 점이 합리적이라 생각. 미생물 균형이 변한다면, 원래 없던 미생물이 늘어나는 것이고, 기존 신체 활동과 면역 시스템이 이를 막지 못하는 상태일 수 있음. 면역력이 약해진 것은 아닐 수 있지만, 만약 약해진 상태라면 타인과의 접촉을 줄이는 것이 좋을 것임. 미생물 균형이 급변하는 이유가 식단이나 환경 변화라면, 이는 식량 부족이나 영역 변화와 같은 신호일 수도 있고, 이 경우에도 타인과의 접촉은 줄이는 것이 유리함. 특정 미생물이 몸안에서 자리 잡으면 오히려 가족에게 위협이 될 수 있으니, 장내 미생물의 변화에 따라 사회적 경계심이 생기는 현상은 사회적 동물에게 유익한 진화 결과가 될 수 있음
          + 진화심리학의 문제는 거의 어떤 인과관계든 합리화 가능한 ""그럴듯한 이야기""를 만들어낼 수 있고, 반증이 어렵다는 점임. 오히려 더 단순한 설명은 이런 반응이 적응적 기능이 아닌, 생물학 자체가 스파게티 코드처럼 한 신호 변화가 예측 불가능한 여러 시스템에 영향을 주는 한 예라는 쪽임
          + 이런 현상은 개체보다 집단에 더 이로울 수 있는 상황이라는 생각
          + 오늘날 '부적응적'으로 보이는 반응 중에서도 과거에는 적응적으로 작용했을 가능성에 대한 궁금증이 생김
     * 불안장애와 우울증에 도움된다 주장하는 프로바이오틱스(유산균) 보조제가 시중에 있지만, 개인적으로 효과를 본 적 없음. 혹시 실제로 체험하거나 아는 사람의 피드백이 궁금함
          + 위장에 문제가 많았고 위 수술도 여러 번 했던 경험. 대부분의 프로바이오틱스는 효과가 없었지만, Dr. Ohhira의 프로바이오틱스만은 실제 효과가 있었음. 이유는 모르지만 체감상 작동. 그리고 술을 끊는 것도 매우 큰 도움
          + 어떤 사람은 프로바이오틱스를 극도로 대량 섭취(메가도스)해서 사회불안을 완전히 치유했다는 경험을 주장함. 실제로 다른 사람이 재현했는지까지는 모르겠지만, 초기에 남긴 후기는 신뢰할 만한 느낌. 관련 보고서는 여기서 확인 가능
          + 논문에 보고된 효과가 진짜로 재현된다 해도, 일반 프로바이오틱스 보조제는 기존 장내 미생물 집단을 항생제로 초기화하지 않는 이상 거의 효과 없을 가능성이 높다고 생각. 이미 내재된 장내 미생물들은 외부에서 들어온 약한 미생물을 잘 막아냄. 보조제 회사들이 주장을 과장하는 경향이 많음
          + 여러 방법 시도 끝에, 글루텐, 락토스, 과당 등 '부정적인 음식'을 끊는 것이 가장 큰 효과였음. 프로바이오틱스는 내게 의미 없었음
          + 같은 질문을 강력히 다시 한 번 요청
     * 만약 마치 인간 내부의 미생물이 무언가를 원한다고 상상해 보면, 진화적으로 다른 박테리아나 바이러스처럼 인간 행동에 영향을 주려 한다면, 인간이 다른 사람들과 접촉을 피하게 만드는 쪽으로 발달한 건 특이한 케이스임. 인간은 극도로 사회적인 종족이라, 미생물이 이런 방식으로 숙주를 잃을 수도 있음. 그 선택의 이유가 궁금
     * 그냥 농담이지만, 혹시 이런 상관관계가 반대 방향일 가능성도 있음. 즉, 밖에 자주 나가는 사람들이 다양한 세균에 더 자주 노출되고, 덜 위생적인 환경에서 음식을 먹거나, 타인과 지나치게 친밀한 접촉을 하면서 더 많은 세균을 받아들이는 경향. 그래서 집에 있는 사람들은 오히려 덜 감염되어, 장내 특정 세균과의 상관관계가 나타나게 된 것임. 이 기사에서는 불안을 유발하는 세균에 대해 이야기하고 있지만, 사실 밖에서 사교적인 '건강한 세균'이 경쟁에서 이기는 환경이라는 해석도 가능. 즉, 집에 머므는 사람들이 감염 빈도가 더 낮아 이런 상관관계가 보일 수 있음
          + 흥미로운 이론이라 생각. 외향적인 사람들과 함께 식사를 하거나 쌍방 동의하에 타액 교환 등으로 '건강한 세균'을 얻어보는 실험은 흥미로울 수 있음
     * 이런 기사들이 대중 사이에서 ""요거트만 더 먹으면 어색함도 사라진다"" 같은 말로 퍼져나갈 것 같음
          + 사실 요거트만으로는 큰 효과를 기대하기 힘들고, 실제로는 분변 미생물 이식(Fecal Microbiota Transplantation, FMT)이 더 효과적이라는 주장. 캘거리 대학교에서는 현재 주요 우울 장애 및 강박 장애 환자를 대상으로 FMT 임상 실험을 모집 중. 관련 링크는 여기 및 여기에서 확인 가능
          + 장건강을 개선하고 싶다면 프로바이오틱 보조제나 식품보다는 가공되지 않은 식물성 프리바이오틱스 섭취가 훨씬 효과적. 물론 둘 다 병행해도 좋음
          + 시중에 판매되는 요거트에는 첨가된 당분이 꽤 많이 들어있는 경우가 많음
          + 실험 결과는 오히려 역방향임. 사회불안증(SAD)을 가진 환자에게서 채취한 미생물을 쥐에게 이식하면 쥐도 SAD 증상이 나타남. 즉, 새로운 미생물을 이식한다고 무조건 해결되는 것이 아니고, 오히려 불안을 유발하는 미생물이 제거되는지에 대한 검증이 필요한 상황. 항생제로 초기화하는 실험적 방법도 가능. 식단이 이런 미생물에 영향을 주는지도 불확실
     * 16S 시퀀싱(유전정보 분석)은 작동 원인을 밝혀내는 것이 아니라, 좁은 범위의 유전자 패턴만 보여주는 한계가 있음
     * 이런 동물실험 연구는 항상 제목에 '마우스 기준: ~'이라고 명시하면 좋겠다는 생각. 그리고 연구가 진척되면 '다른 동물 기준: ~', 그리고 인간 대상으로 실제 임상까지 갔다면 '인간 기준: ~'이라고 단계별로 구분하면 좋겠음. 개인적으로 쥐 연구 결과는 너무 많이 봐 와서, 인간 대상으로까지 진행된 결과에 훨씬 관심 많음
     * 내 경우, 19세에 사회불안이 급격히 심해졌고, 23~24살까지도 약해지지 않았는데 정확한 원인을 몰라 항상 고민. 그 시기에 내 식단과 환경이 크게 변해서 미생물 군집 변화가 원인일 수도 있겠다는 생각. 참 흥미로운 이슈
          + 환경 변화가 심했다면 그것만으로도 설명이 충분할 수 있음. 이 시기에 대부분 집을 떠나거나 대학·직장생활을 새로 시작하는 시기라 모두가 사회적이고 스트레스 많은 변화에 노출됨. 그래서 정신건강 문제의 발생이 자연스럽고, 주변 친구들 중에서도 비슷한 사례를 많이 접함
     * 정신건강에 대해 뇌뿐 아니라 다른 곳도 봐야 한다는 주장이 타당하게 느껴짐
"
"https://news.hada.io/topic?id=21531","bzip2 Crate가 C에서 100% Rust로 전환됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    bzip2 Crate가 C에서 100% Rust로 전환됨

     * bzip2 크레이트가 C 코드 의존성을 100% Rust 구현으로 대체함
     * 성능이 기존보다 전반적으로 향상되고, 크로스 컴파일이 더 쉬워짐
     * Rust 구현은 C 버전 대비 데이터 압축 및 해제 속도 모두 개선됨
     * 심볼 충돌 문제와 같은 라이브러리 의존성 이슈가 크게 줄어듦
     * 보안 감사를 거쳐 중요한 로직 버그를 수정, 안정성이 검증됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

bzip2 크레이트 0.6.0 출시 및 Rust 기반 전환

     * 오늘 bzip2 버전 0.6.0이 배포되었음
     * 이제 기본적으로 자체 개발한 rust 기반 bzip2 알고리듬 구현체인 libbz2-rs-sys을 사용함
     * 이 전환을 통해 bzip2 크레이트는 더욱 빨라지고, 크로스 컴파일이 쉬워졌음

     * libbz2-rs-sys 크레이트는 C 동적 라이브러리 형태로도 빌드 가능함. 이를 통해 C 프로젝트에서도 성능 개선을 활용할 수 있음

왜 이런 전환이 이루어졌는가?

     * bzip2 알고리듬은 90년대에 만들어져 지금은 널리 쓰이지는 않지만, 여러 프로토콜과 라이브러리에서 여전히 규격 준수를 위해 필요함
     * 많은 프로젝트가 직접적으로는 아니지만 의존성 트리 어느 깊은 곳에서 bzip2에 의존하고 있음
     * 우리는 zlib-rs에서 쌓은 경험을 바탕으로 이번에 bzip2 구현을 현대화했음

     * libbz2-rs-sys 구현 세부 내용은 이전 블로그 포스트에서 다룸. 여기서는 이번 전환의 이점을 살펴봄

  향상된 성능

     * rust 구현체는 전반적으로 C 버전보다 더 높은 성능을 보임
     * 일부 상황에서는 동등한 성능이지만, 느린 경우는 없음

     * 압축 성능: bzip2에서는 level 옵션이 있지만 성능 영향은 미미함
     * 테스트 결과, 대표적인 샘플 파일에서 rust 버전이 10% 이상 속도 향상됨

   압축:

               파일              C(수행 사이클) Rust(수행 사이클) 상대적 변화
   sample3.ref (level 1)       38.51M    33.53M       -14.87%
   silesia-small.tar (level 1) 3.43G     3.00G        -14.30%
   silesia-small.tar (level 9) 3.47G     3.17G        -9.66%

   압축 해제에서도 모든 경우에서 개선된 성능을 보여줌:

             파일           C(수행 사이클) Rust(수행 사이클) 상대적 변화
   sample3.bz2            2.53M     2.42M        -4.48%
   sample1.bz2            9.63M     8.86M        -8.63%
   sample2.bz2            20.47M    19.02M       -7.67%
   dancing-color.ps.bz2   87.46M    83.16M       -5.17%
   re2-exhaustive.txt.bz2 1.89G     1.76G        -7.65%
   zip64support.tar.bz2   2.32G     2.11G        -10.00%

   단, macOS 환경에서는 간혹 압축 해제 수치 변화가 발생함. 성능 측정 도구의 한계로 분석이 어려웠음

  크로스 컴파일 지원

     * C 의존성이 있는 Rust 프로젝트의 크로스 컴파일은 보통 cc 크레이트 덕분에 잘 동작하지만, 실패 시 디버깅이 매우 어려움
     * 시스템 라이브러리 링크 과정에서 예기치 않은 문제가 발생하기 쉽고, WebAssembly 빌드를 비롯한 일부 환경에서는 실질적 장애 요소가 됨

     * rust 구현으로 전환함으로써 C 관련 문제들이 완전히 사라짐

     * 이제 윈도우, 안드로이드, 웹어셈블리 등에서도 특이사항 없이 크로스 컴파일이 가능함
     * 이는 사용자 경험뿐 아니라 유지보수 관점에서도 큰 장점임

  기본적으로 심볼(export) 충돌 없음

     * C 의존성의 경우 Rust 외부 블록에서 심볼을 export해야 하기 때문에, 다른 의존성이 동일한 심볼을 export할 시 충돌이 발생함
     * libbz2-rs-sys는 기본적으로 심볼을 export하지 않도록 설계됨
     * 따라서 다른 외부 라이브러리와 심볼 충돌이 발생할 일이 없음. 필요하다면 feature flag로 export를 활성화할 수도 있음

  MIRI 기반 테스트 실행

     * Rust에서 bzip2를 성능 높게 구현하려면 unsafe 코드 사용이 불가피하고, C 인터페이스 복제에도 unsafe 코드가 다수 필요함
     * 다행히 이 코드를 MIRI 환경에서 실행 및 테스트할 수 있음

     * 더 나아가, bzip2를 사용하는 상위 레벨 라이브러리나 애플리케이션도 이제는 MIRI 테스트가 가능해짐

결론

   이제 bzip2 크레이트는 더 빨라졌음. 더 이상 신경 쓰지 않아도 될 정도로, 자연스럽게 더 나은 경험을 제공함

        Hacker News 의견

     * Trifecta Tech의 구현체가 리눅스 배포판에서 사용하는 공식 구현을 대체하게 될 가능성을 생각해보면, 예전에 Fedora가 기존 Adler zlib에서 zlib-ng로 교체한 사례가 있다는 점에서 불가능하지 않다는 판단. 핵심은 원본과 호환되는 C ABI를 제공하면 된다는 의견
          + 만약 2019년 이후로 업스트림 릴리즈가 없는 상황이라면, 이 구현체는 그냥 완성된 것이 아닌지에 대한 의문 제기. 더 이상 수정할 버그나 추가할 기능이 없다면, 그 자체로 충분하다는 생각
          + Ubuntu가 Rust로 작성된 sudo를 사용하기 때문에 이런 교체는 충분히 가능성 있는 일이라는 생각
          + Trifecta Tech에서 C ABI를 호환성 있게 잘 제공하고 있지만, 결국 누군가가 이 작업을 실제로 수행해야 변화가 일어난다는 의견
          + uutils의 목표도 이런 공식 리플레이스먼트에 있다고 언급하며 uutils 홈페이지 링크 공유
          + 간단히 살펴본 결과 이미 cargo-c 설정이 존재해 긍정적으로 보이지만, 네임스페이스가 다르기 때문에 C 프로그램에서 기존 libbz2로 자동감지가 되진 않는 상황. bzip2의 심볼에 익숙치 않아 정확한 ABI 호환 여부는 알기 어렵다는 점 언급. 직접 GNU 운영체제 구현체를 관리하는 것은 시간이 많이 소요되어 어렵고, 시간 날 때 실험적인 프로젝트인 platypos에서 PR 환영한다는 입장
     * 나는 이 크레이트를 활용해 수백TB의 Common Crawl 데이터를 처리중. 속도가 빨라져서 매우 만족하는 입장
          + bz2를 여기서 사용하는 이유에 대한 질문 제기. 대용량 변환을 한 번만 하려면 zstd로 전환하는 것이 이점이 크다고 들었고, 압축률이 높을수록 모든 면에서 bzip2보다 낫다는 근거 제시
          + Common Crawl 데이터가 토렌트 형태로 공개되어 있는지 궁금하다는 질문
          + 압축 속도 14% 개선은 꽤 훌륭하다는 감상 공유
     * 이 구현체가 기본적으로 11개의 남아있는 CVE를 해결하는지 궁금. 아이러니하게도 bzip2 크레이트에도 CVE 신고가 있었다며 관련 링크 공유
          + 해당 크레이트에서 보고된 ‘큰 파일에서 런타임 실패’류 취약점과, C로 작성된 경우의 ‘bounds miss’ 문제 간 대조가 흥미롭다는 의견. 이러한 bound miss 취약점이 실제로 코드 실행까지 이어질 수 있을지 궁금
          + ‘bzip2 크레이트는 0.4.4 이전 버전에 취약점이 있다’는 안내 인용. 오늘 0.6.0 릴리즈라는 추가 정보
     * ‘90년대 알고리즘을 왜 굳이 개선하냐’는 질문에 대해, 최근에는 어떤 알고리즘이 쓰이는지 궁금하다는 의견. zstd를 언급하며 압축 알고리즘 비교 벤치마크 링크 공유
     * C와 Rust 각각의 컴파일러 코드 생성(코드젠) 백엔드가 동일하다면 어떻게 속도 향상이 발생하는지 궁금. Rust의 auto-simd나 직접 최적화, 혹은 새로운 최적화 라이브러리 활용 등 다양한 요인 가능성 제기
          + 추측이지만, Rust는 더 많은 힌트를 코드 생성기에 제공할 수 있다고 봄. 예시로, C 포인터와 다르게 별칭(aliasing) 문제를 적게 걱정할 수 있다는 점. Aliasing 설명 링크 제시
          + C 언어는 현대 고성능 코드 작성 측면에서 정말 좋지 않다는 의견. C99~C21까지 약 20년간 언어 자체가 클린한 방식으로 새로운 명령어(clz, popcnt, clmul, pdep 등)를 활용하기 위한 기능이 부족했다는 지적. 이런 추상화 명령어 지원만으로도 이러한 종류의 코드 최적화에 큰 도움을 준다고 평가
          + 어떤 언어로든 다시 작성하면 속도 개선의 기회가 있고 Rust만의 고유한 속도 보장은 아니라는 의견
     * 나나 Prossimo에서 핵심 인터넷 프로토콜(BGP, OSPF, RIP 등) 및 라우팅 구현체, DNS 서버 등을 비슷한 방식으로 재작성해주길 바라는 희망
          + 최근 몇 년간 Rust 등 안전한 언어로 인터넷과 OS 핵심 도구 리라이팅을 지원하는 펀드, NLnet 프로젝트와 Sovereign Tech Fund 링크 소개. 예시로 BGP in Rust 프로젝트도 동반 언급
          + Memory Safety Initiative에서는 TLS와 DNS 등 핵심 서비스의 안전한 리라이팅 노력을 소개하고 있어 내 제안과 일부 맥락이 일치
          + 한 개발자가 SPARK Ada로 Ironsides DNS를 만들었는데, 이 언어는 더 강한 형식적 증명을 지원
     * macOS에서 perf 프로파일러가 없어도 dtrace로 충분히 성능 분석이 가능하다는 생각. Perl로 작성된 오리지널 flame graph 스크립트도 dtrace를 활용했고, Rust로 재구현된 flame graph 역시 같은 방식을 사용. cache miss나 micro instruction retired 같은 일부 메트릭은 부족하지만, 여전히 매우 유용
     * Rust를 Javascript로 다시 쓸 필요가 있다는 농담투의 의견
     * lbzip2와 같이 병렬 디컴프레션(압축 해제)을 지원하는지 궁금. 혹은 block magic 프리 스캔 등으로 병렬 처리가 가능한지 질문. 편집 추가로 ‘아마도 지원하지 않는 듯’이라는 결론
     * Lbzip2는 모든 CPU 코어를 활용해 매우 빠른 디컴프레션 속도를 보여줬던 경험 소개. 2025년이 되었지만 Python 같은 많은 주요 프로그램은 여전히 1개 코어만 활용한다는 상황을 아쉬워하는 목소리
          + 파이썬의 상황을 잘 이해하지 못한다는 지적
"
"https://news.hada.io/topic?id=21455","자가 적응(Self-Adapting) 대형 언어 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     자가 적응(Self-Adapting) 대형 언어 모델

     * 기존의 대형 언어 모델(LLM) 은 새로운 작업이나 지식에 맞게 즉각적으로 적응 능력이 부족함
     * 새로운 SEAL 프레임워크는 LLM이 직접 자신의 미세조정 데이터와 업데이트 지침을 생성하여 자가 적응 기능을 가짐
     * 이 과정은 자가 편집(self-edit) 생성, 지시 실행, 그리고 강화 학습(Based on RL) 루프를 통한 지속적 성능 개선 과정을 포함함
     * SEAL은 새로운 지식 통합 및 few-shot 일반화 실험에서 기존 방법보다 향상된 성능을 입증함
     * 본 연구는 자가 지시적 적응 능력을 갖춘 LLM 실현을 위한 유망한 발걸음을 제시함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 대형 언어 모델(LLM) 은 강력한 성능을 보이지만, 자신의 가중치를 새 작업, 정보, 예시에 따라 동적으로 조정하는 메커니즘이 부재함
     * 본 논문은 Self-Adapting LLM(SEAL) 프레임워크를 제시하며, LLM이 자기 스스로 미세조정할 데이터를 생성하고 업데이트 지침을 만드는 것을 가능하게 함
     * SEAL은 새로운 입력을 받으면, 모델이 정보를 다양한 방식으로 재구조화하거나, 최적화 하이퍼파라미터 지정, 또는 데이터 증강 및 그래디언트 기반 업데이트를 위한 도구 호출과 같은 자가 편집(self-edit)을 생성함
     * 이러한 자가 편집은 지도 학습 미세조정(SFT) 과정을 거쳐 모델의 가중치에 영구적인 업데이트로 이어지고, 지속적인 적응 능력을 보장함
     * 효과적인 자가 편집 생성을 위해 강화 학습 루프를 활용하며, 모델 업데이트 후의 다운스트림 성능을 보상 신호로 사용함

인간 학습의 유추

     * 학생이 시험을 준비할 때 강의, 교과서, 인터넷 등에서 얻은 정보를 자기만의 방식으로 노트로 재작성하는 학습 방식에서 영감을 얻음
     * 사람마다 정보 재구성 방법이 달라서, 어떤 이는 도식, 어떤 이는 텍스트, 어떤 이는 수식 등으로 요약함
     * 이는 외부 지식을 스스로 이해하기 쉽게 재조직하거나 보강하는 것이 인간 학습의 보편적 특징임
     * 기존의 LLM은 새로운 작업이 주어지면, 주어진 데이터셋을 그대로 미세조정 혹은 인컨텍스트 학습만을 수행함
     * 하지만 이러한 방식은 데이터 형식이나 양이 학습에 최적화되지 않은 한계가 있음

SEAL: 자가 적응 프레임워크의 제안

     * SEAL은 LLM이 스스로 훈련 데이터 및 미세조정 지침을 자연어로 생성하도록 강화 학습 알고리듬으로 학습됨
     * 여기서 자가 편집(self-edit)은 데이터 및(선택적으로) 최적화 하이퍼파라미터를 지정하는 명령어 형태임
     * SEAL은 별도의 추가 모듈이나 보조 네트워크 없이, 모델의 자연어 생성 기능만으로 자신의 적응 프로세스를 직접 제어함이 특징임

SEAL 작동 방식

     * 각 강화 학습(RL) 외부 루프 반복(iteration) 에서, 모델은 후보 자가 편집(SE)을 생성함
     * 생성된 자가 편집을 적용하여 가중치를 업데이트함
     * 이후 다운스트림 태스크에서 모델 성능을 평가하여, 해당 결과로부터 보상 신호를 획득함
     * 보상 신호를 이용해 자가 편집 생성 정책을 반복적으로 개선함

실험 및 결과

     * 지식 통합 태스크에서, SEAL은 모델이 직접 생성한 합성(synthetic) 데이터로 미세조정을 수행함
          + SQuAD의 no-passage-in-context 버전에서, RL 훈련 전 33.5%에서 RL 훈련 후 47.0%로 질문-응답 성능이 크게 향상됨
          + SEAL이 생성한 데이터는 GPT-4.1이 만든 합성 데이터보다도 더 우수한 성능을 보임
     * Few-shot 학습 실험에서는 ARC-AGI 벤치마크의 단순화 버전을 사용하여, SEAL이 증강 데이터와 최적화 하이퍼파라미터를 직접 선택함
          + 학습률, 에폭, 토큰 유형별 선택적 손실 계산 등 다양한 도구 조합을 자동 선택
          + 강화 학습을 적용한 SEAL 사용 시 성능 향상을 달성하며, 단순 인컨텍스트 학습이나 RL 없이 도구만 쓴 경우보다 효과적임

결론

     * SEAL 프레임워크는 자가 생성 데이터와 지침을 통한 LLM 자가 적응이 가능함을 실험적으로 증명함
     * 이 접근법은 향후 데이터 효율성, 적응성 및 범용성을 갖춘 차세대 언어 모델 개발을 위한 중요한 진전을 시사함

        Hacker News 의견

     * 두 명의 수학 천재 친구가 2010년대 중반 매우 일찍 ML에 뛰어들었을 때 자주 NEAT/HyperNEAT(Neuroevolution of Augmented Topologies)이라는 알고리즘에 대해 이야기해줬음 [NEAT 위키피디아 링크] ML 전문가가 아니라서 정확히는 모르지만, NEAT가 네트워크의 토폴로지를 진화시키는 반면, 이번 논문에서는 가중치를 진화시키는 것이라는 차이점으로 이해함 근본적으로 네트워크 구조를 바꾸는 방식과 가중치를 바꾸는 방식, 두 가지 다른 접근이 같 은 문제를 풀려는 시도라고 생각함 그 두 친구는 인공지능의 미래는 RL(강화학습)과 진화 알고리즘에 있다고 굳게 믿는 모습이었음
          + 내가 제일 좋아하는 NEAT 입문 영상이 있음 SethBling의 MarI/O - Machine Learning for Video Games [YouTube 링크]
          + 인간이 대단하다고 생각함 신경세포를 이해하려고 가상의 컴퓨팅 시스템을 만들지만 실제로는 그렇게 작동하지 않는다는 걸 깨달으면서도, 그 상상 속 시스템에서 아이디어를 가져와 혁신적인 기술을 만듦 그리고 지금도 그 상상 시스템에서 영감을 받아 계속해서 발전시키고 있음
          + 최근 이 NEAT/진화 기반 개념에 완전히 빠져들게 됐음 Kokoro 목소리 복제 프로젝트에 유전 알고리즘을 써서 어느 정도 성공한 후에, 네트워크 구조 자체를 진화시켜 ‘스스로 조립하는 지능’이 가능할지 궁금해짐 이게 실질적으로 가능하게 되려면 어떻게 해야할지 궁금한데, LLM들이 이렇게 등장한 걸 보면 하이브리드 방식이 현실적인 대안 아닐까 하는 생각임
     * RL을 활용해 모델이 스스로 정보를 재구조화하여 학습 효율을 높이는 ‘self-edit’ 접근법이 매우 영리하다고 생각함 서로 다른 종류의 지식을 위해 서로 다른 표현이 더 효과적이라는 사실이 핵심 아이디어임(수학이랑 역사는 필기 방식이 다르듯이) 두 가지 중요한 관찰이 있음 첫째, 지식 통합 결과(47% vs 46.3%, GPT-4.1 데이터 기준)는 단순히 더 많은 데이터를 넣어서가 아니라 실제로 더 좋은 학습 포맷을 모델이 찾았다는 것임 치명적인 망각 문제(catastrophic forgetting)는 아직 해결되지 않았고, 데이터 다양성이 실제로 얼마나 개선되는지도 명확하지 않음 둘째, 보상 평가 한 번에 30~45초가 걸려서 대부분의 실사용에는 무리가 있음 하지만 정말 중요한 문서 처리처럼 최적의 정보 보존이 요구되는 곳이라면 투자할 가치가 있음 명확한 평가 메트릭이 존재하는
       작업에 국한된다는 점이 큰 한계임(보상 산출을 위해 기준 Q&A나 테스트 케이스가 필요함) 그래도 기술 문서나 교육 자료처럼 평가 자동화가 가능한 곳에서는 아예 새로운 지식처리 패러다임을 가져다 줄 가능성 충분함 아직 완전히 자기개선 에이전트에 도달한 건 아니지만 모델이 스스로 학습 방법을 개선하는 중요한 진전처럼 느껴짐
     * 며칠 전에 Anthropic에서도 비슷하게 self finetuning 관련 연구를 공개함 [arxiv 논문 링크]
          + 관련 논의가 현재진행형으로 있음 [연결된 HN 스레드]
          + 정말 놀랍다고 생각함 Claude 3.5 Sonnet의 프로덕션 등급 RM 기준, unsupervised assistant 정책이 인간 감독 RM으로 훈련한 정책을 상대 비교에서 60%나 이긴다고 평가됨 이제는 인간이 지도하지 않아도 모델끼리 더 뛰어난 성능을 낼 수 있는 단계에 진입했다고 생각함
     * 대규모 언어 모델(LLM)이 강력하지만, 새로운 작업이 주어졌을 때 가중치를 적응시킬 메커니즘이 없다는 점이 문제임 인간 지능은 배우는 과정과 적용하는 과정이 하나의 피드백 루프로 통합되는데, LLM은 훈련과 추론이 완전히 분리되어 있음 우리는 새로운 모델이 약간 더 많은 것을 ‘배운’ 상태로 배포되면 이전 모델을 폐기함 LLM에서는 추론이 곧 학습의 끝임 이게 AI에 대해 가장 널리 퍼진 오해라고 생각함 LLM이 학습한다고 착각하다 보면 AGI가 금방 올 것이라는 환상에 빠지기 쉬움
          + Deepseek의 사례처럼 강화학습을 활용하면 LLM의 성능을 refinement 시킬 수 있음
          + 만약 사용자의 반응(긍정/부정)에 따라 LLM을 다시 학습시킬 수 있다면? 입력과 출력 데이터를 활용해 피드백 루프로 돌릴 수 있지 않을까 상상하는 중임
     * 실제로 LLM을 ‘현장에서’ 계속 학습시키는 방향, 즉 코드형 에이전트가 코드베이스를 시간이 지나면서 배우게 만드는 연구의 현황과 한계(비용? 모델 붕괴? 기타?)에 대해 정말 잘 아는 전문가가 정리해줬으면 좋겠음 분명히 대형 연구소들은 이걸 시도할 테지만, 일반 사용자 시각에서는 이런 이야기는 잘 들어보지 못함 지금은 강화학습 기반 더 좋은 훈련법에만 집중하는 것 같고, 트레이닝 과정에서 못 배운 것은 나중에 컨텍스트로 우겨넣는 식이 대세임 하지만 경험 기반 실시간 자기학습의 부재가 AGI와의 분기점인 것 같다는 생각임
          + 연속적 학습(continual learning)은 현재로선 뾰족한 해법이 존재하지 않음 컴퓨팅 자원, 모델 붕괴, 망각 등 여러 이유가 언급되는 것이 맞음 유일한 방법은 1) 모델 학습 2) 새로운 데이터 추가 3) 전체 재학습 4) 반복 이럴 수밖에 없음 시간이라는 측면에서는 어느 경우도 완전한 보장이 없음 CL 분야에서 정말 ‘진짜’ 답이 전혀 없는 상황 모델의 표현 공간은 확대하면서도 이전 표현 공간은 최대한 그대로 보존해야 하는데, 이걸 동시에 하라는 게 불가능에 가까움 신경계가 있는 생물은 아주 쉽게 해내는 것처럼 보이는데 AI는 이 작업이 극악하게 어려움 내 생각엔 인공지능도 ‘수면’이나 ‘휴식’ 같은 개념이 필요할지도 모름
          + 전문가가 아니지만 프라이버시 문제도 중요한 역할을 한다고 생각함 연속 학습을 하려면 트래픽이나 비용 문제로 어쩔 수 없이 유저 단위가 아니라 집계(aggregate)로 해야 할 텐데, 그러면 세션 간 정보 유출 위험이 생김 안전하게 연속 학습하는 방법을 찾는 게 AGI 최대의 장애물이라는 데 적극 동의함
          + 신뢰성 문제도 큼 자동 평가에 대한 확신이 없다 보니, 실제로 성능이 좋아졌는지 확인하기 전까지 자동화된 continuous training 버전을 바로 배포하지는 않음 결국 여러 업데이트를 한 번에 모아서 최종 점검(‘바이브 체크’) 후에만 실제 반영함
          + LLM의 연속적 미세조정이 ‘정렬(alignment)’을 쉽게 흐트러뜨릴 수 있다는 점이 가장 명확한 문제로 보임 결과적으로 안정성·안전성이 담보되지 않음
          + 가장 명백한 걸림돌은 치명적 망각(catastrophic forgetting) 문제라고 생각함
     * 내 CPU는 neural-net processor, learning computer임 그런데 Skynet이 혼자 보낼 때는 switch를 read-only로 바꿔놓음(Terminator 인용)이 떠오름
     * 코드 및 예시가 포함된 공식 웹사이트 안내 [SEAL 프로젝트 페이지]
     * Villalobos et al. [75]의 예상에 따르면, 2028년이면 frontier LLM은 공개된 모든 인간-작성 텍스트로 한계에 다다른다고 함 이 ‘데이터 벽’은 synthetic data augmentation의 필요성을 촉발할 거라는 주장임 웹스케일 코퍼스가 고갈되면, 결국 모델이 직접 새로운 고효율 훈련 신호를 생성할 수 있어야 발전할 수 있음 결론적으로, SEAL synthetic-data generator 모델을 메타-트레이닝해 신선한 데이터로 프리트레이닝을 수행하고 미래 모델의 효율을 높인다는 아이디어임 2028년이 머지않았다는 점에서 굉장히 인사이트 있다고 생각함
     * “올바르게 잊기(forgetting correctly)”가 이제는 “올바르게 배우기(learning correctly)”보다 더 중요한 문제로 떠오르고 있는 것 같음 새로운 사실을 빠르게 습득하는 데 큰 발전이 있었지만, 유한한 용량 내에서 덜 중요한 정보를 효율적으로 버리는 기술은 아직 많이 뒤처져 있음 “올바른 망각”은 인간두뇌가 아주 잘하는 일인데, 실제로 어떻게 동작하는지 궁금함
          + 인간이 “올바른 망각”을 잘한다는 데 동의하지 않음 사실 인간이 대단히 뛰어난 시스템을 가진 건 아니라는 생각임 뇌의 용량이 워낙 커서 새로운 정보를 위해 일부러 공간을 지운다기 보다는, 기존에 있던 나쁜 정보가 새로운 학습을 방해할 때만 잊어버리는 식으로 동작한다고 봄
          + 학습과 spaced-repetition(간격 반복)이 아주 밀접하게 연결되어 있다고 생각함 Anki 같은 학습 도구랑 많이 연관되지만, 실제 세상은 우리가 일정 주기로 만나는 자연스러운 현상(주야, 계절, 자주 가는 장소, 자주 만나는 사람 등) 그 자체가 spaced-repetition임 아마 이 개념의 ‘역방향(reverse)’도 존재하지 않을까 고민 중임
          + 내가 했던 연구에서는 LLM이 내부 데이터를 “숨긴다”는 사실이 나타남 단순히 ‘잊는’ 게 아니라, 이후 추가 학습을 할 때 다시 그 정보가 표면에 떠오를 수 있음 그래서 모델 훈련시 실제 전체 메모리 상태를 지속적으로 체크하지 않으면 부분적인 검수로는 한계가 큼
          + 혹시 least-recently-used 방식 아닐까 테스트 삼아 내 머릿속에서 실험 중임 그래서 이 분야가 재밌음
     * 겉보기엔 LoRA adapter를 미세조정하고 base model에 병합하는 프레임워크로 보임 HuggingFace의 PeftModel에서 adapter를 base model로 통합하는 “merge_and_unload” 기능을 사용하고 있음…뭐가 새로울까 잘 모르겠음
          + 안정성이 주요 차별점인 것 같음 alignment tax나 모델 붕괴 현상을 피하는 구조임 하이퍼네트워크, 즉 두 모델이 계속해서 LoRA로 업데이트되고, 하이퍼네트워크가 새로운 모델 상태를 받아들이도록 갱신되는 ‘풀 서클’ 구조를 보고 싶음 meta-hypernetwork를 사용해 하이퍼네트워크에도 LoRA를 적용하는 식으로 하면 진정한 의미의 continuous learning 가능성이 있음
"
"https://news.hada.io/topic?id=21457","SIMD에 적합한 부분 문자열 탐색 알고리듬 (2018)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    SIMD에 적합한 부분 문자열 탐색 알고리듬 (2018)

     * SIMD 친화적인 부분 문자열 탐색 알고리듬에 관한 주제임
     * 빠른 문자열 검색을 위한 기술적 접근법 제시 내용임
     * 병렬 처리를 활용해 기존 방식 대비 효율성 향상 방향임
     * 개발자 및 IT 전문가에게 유용한 성능 팁으로 주목됨
     * 해당 알고리듬은 현대 하드웨어 최적화에 연관성 가짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 본 문서는 SIMD(Single Instruction, Multiple Data) 명령어 집합에 최적화된 부분 문자열 탐색 알고리듬을 소개함
     * 문자열 처리 속도가 중요해진 현재 IT 환경에서, 기존 순차적 탐색 방식의 한계를 보완하는 병렬 처리 방안을 다룸
     * SIMD를 활용하면 한 번에 여러 데이터를 동시에 비교할 수 있어, 대용량 문자열 검색에서 중요한 성능 개선 효과를 기대할 수 있음

주요 내용

     * SIMD 알고리듬은 입력 문자열을 여러 부분으로 분할 후, 동일한 명령어로 여러 바이트를 한번에 비교함
     * 이 방식을 통해, 기존의 반복문 기반 비교보다 더 빠르고 효율적인 탐색이 가능함
     * 주로 텍스트 검색, 로그 분석, DNA 시퀀싱 등 고속 대용량 데이터 처리가 요구되는 분야에서 효과적으로 활용됨

개발자 및 엔지니어를 위한 이점

     * SIMD 친화적 알고리듬 적용시, 최소한의 코드 변경만으로 현대 CPU의 잠재력 극대화 가능성 확보함
     * 기존 탐색 로직보다 속도와 효율성 면에서 이점 제공함
     * 멀티코어 환경에서의 성능 확장성또한 탁월함

결론

     * 부분 문자열 탐색 성능이 중요한 IT 서비스, 데이터 분석, 실시간 검색 엔진 분야에서 SIMD 기반 알고리듬 도입이 실질적인 성능 향상으로 이어질 수 있음
     * 최신 하드웨어 환경을 활용하기 위한 필수적 최적화 전략임

        Hacker News 의견

     * ripgrep의 가속 방식은 Rust의 regex crate를 활용한 AVX2 (generic) 접근법이라는 설명. 예를 들어, \w+\s+Sherlock\s+\w+ 같은 정규표현식도 Sherlock만 따로 뽑아서 빠르게 검색 가능함을 강조. 실제 구현은 여기에서 확인 가능. 이 글의 알고리즘과의 주요 차이점은 needle의 첫/마지막 바이트 대신, 배경 분포를 이용해 덜 자주 나오는 2바이트로 검색을 최적화하는 휴리스틱 사용임을 언급. 단순 Two-Way 방식이나 GNU libc의 memmem보다 훨씬 빠른 성능임을 벤치마크 결과로 제시. prebuilt 벤치마크에서는 memmem류 루틴이 needle이 고정될 때마다 상태를 반복적으로 재구축해야 하기 때문에 효율이 떨어진다는 API 한계점도 강조
          + 바이트의 배경 분포를 어떻게 알 수 있는지, haystack에서 그 분포를 일일이 스캔한다면 오히려 성능에 악영향이 있지 않겠냐는 지적
     * Wasm/WASI libc의 SIMD 최적화를 시도하면서 이런 문자열 검색 알고리즘을 구현한 경험 공유. haystack의 길이가 정해져 있고, needle이 충분히 크면 Quick Search와 조합하는 것이 유용하다는 의견과 함께 관련 코드와 알고리즘 설명 링크 제시
     * C#에서도 IndexOf에 SIMD가 적용됨을 공유하며, 자세한 내용은 여기에서 확인 가능
     * 나 역시 SMID 방식을 써서 문자열 검색과 split을 위한 다양한 알고리즘을 직접 구현해봤다는 경험 소개. tamgu의 conversion.cxx 소스 공개. 본문에서 언급한 방식과는 또 다른 알고리즘을 썼음을 밝힘
          + 본인 알고리즘에 대해 간단히 요약해달라는 요청. 예시로, 원문 1번 알고리즘은 첫/마지막 문자를, 2번 알고리즘은 앞 4글자를 동시에 비교하며 여러 후보 위치 확인하는 방식이라는 설명 첨부
          + 몇 년 전 Zig의 generic SIMD를 사용해서 LZ77 window search용으로 수정한 버전을 구현해보려 했던 경험 공유. 관련 내용은 여기에서 확인 가능
     * 빠른 HTTP 파싱을 위해 SIMD 알고리즘을 사용하는 hparse 프로젝트를 떠올린다는 의견
     * swar 알고리즘은 1바이트 정렬 데이터를 8바이트 단위로 캐스팅해 UB(정의되지 않은 동작)이 발생함을 언급. Unaligned load 때문에 성능 이슈가 있을 수도 있다는 지적
          + 본인은 이런 코드는 종종 이상적인 알고리즘 혹은 가독성을 위한 의사코드로 받아들여왔다는 의견. mempcy를 사용하지 않고, 경계 검사도 부정확함을 지적. haystack 길이가 8의 배수라고 가정하거나, needle이 비어있으면 unsigned integer overflow로 out-of-bounds가 나는 등 UB가 3개나 존재. 실제로 SIMD 데모 코드에선 흥미로운 벡터 활용 방식만 보여주고, 경계 조건은 생략되는 경우가 많았던 경험 공유
     * libc의 strstr이 느리다는 건 이미 알려진 사실이지만, musl은 빠르고 최신 알고리즘이라는 점 강조. 이제 이름만 정하면 smart shootout에 추가 가능. 최고의 SIMD 알고리즘과 비교하면 어떨지 궁금증
          + 참고 벤치마크로 musl의 Two-Way와 본인이 공유한 SIMD 최적화 libc 알고리즘의 비교 결과 소개. 벤치마크 방법은 관련 코드 기반. SIMD 활용 개선분은 이 표에서 확인 가능. 뛰어나다기보다는 꽤 괜찮은 수준의 개선임을 솔직히 평가. musl은 길이 고정 문자열(memmem)에 특화되고, 본인은 Wasm 환경에서 unknown length 문자열(strstr)에는 여러 최적화를 자유롭게 시도할 수 있었음을 언급. NUL 종료 문자열 때문에 여러 좋은 알고리즘이 곤란을 겪음
          + smart에 더 많은 SIMD 알고리즘이 포함되면 좋겠고, 시간 나면 직접 실험해보고 싶다는 개인적 의지 공유
     * 문자열 검색 SIMD 비교에서 2018 버전이 빠진 것 아니냐는 질문
     * 문자열 크기에 따라 SIMD 방식이 실제로 효율적인 경계가 궁금하다는 질문. 일반적으로 작은 문자열에선 SIMD 설정 오버헤드(정렬, 길이 계산 등) 때문에 오히려 단순 byte 기반 검색보다 느릴 수 있다는 점을 강조. 하드웨어 아키텍처에 따라 크게 달라질 수 있음을 감안
          + 본인 경험상 오히려 반대임을 언급. 이런 알고리즘들은 쓸데없는 설정 없이 거의 brute force 방식이라, 길고 반복적인 needle엔 시간 복잡도가 나빠짐. 이에 반해 quadratic(제곱 근) 문제를 방지하거나 sublinear(부분 선형) 수행을 하는 고급 문자열 검색 알고리즘들은 needle 구조를 더 깊이 탐색하는 고비용 세팅이 필요하다는 점 강조
     * Python에서 타 언어 호출 없이 직접 SIMD를 쓸 수 있으면 좋겠다는 바람
          + Austin의 블로그와 모음글(story on vowels detection 링크)을 언급하며, ""별도 언어 호출 없이 SIMD 직접 사용""이란 어떤 의미인지 구체적으로 질문. 기본적으로 Assembly도 ‘다른 언어’이긴 하니…라는 농담 섞인 언급. Python과 SIMD 관련 생태계는 PeachPy(x86 어셈을 Python에서 작성), Mojo(새 Python스타일 언어), CPython 바인딩이 얇은 SIMD 라이브러리 등으로 매우 다양한 스펙트럼이 있음을 안내. 구체적으로 어떤 방식 원하는지 묻고, ASCII 대상이라면 StringZilla의 find_first_of (코드) 함수도 추천
          + 왜 굳이 Python에서 직접 하려고 하는지 의문. 성능 한계로 고민 중이라면 언어 자체를 바꾸는 것만으로 20~50배 이상 성능 향상이 가능하다는 조언
"
"https://news.hada.io/topic?id=21447","2025-06-12 GCP 장애 보고서","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2025-06-12 GCP 장애 보고서

     * Service Control: Google 및 Google Cloud API 가 사용하는 핵심 모듈 중 하나
          + 2025-05-29 Service Control 에 새 기능 배포. 새로운 정책을 검사하는 기능이었음
          + 2025-06-12 새로운 정책이 추가되면서 문제 시작:
               o null pointer 로 인한 크래시 루프* 발생
               o feature flag 가 없었음. 하지만 긴급 정지를 위한 red-button 실행
               o us-central-1 같이 커다란 리전에서는, 이 액션으로 인해 의존하고 있는 내부 서비스에 herd effect* 발생. 왜냐면 randomized exponential backoff** 전략이 구현되어있지 않았기 때문

     * 한번에 트래픽이 많이 몰렸다는 얘기입니다.
       ** 트래픽 과부하를 방지하는 기법입니다.

   구글 같은 큰 업체도 의외로 재시도 처리 때 Jitter를 추가하는 기본적인 조치조차 적용하지 않은 코드가 여기저기에 숨어 있나 봅니다.
   아마 기존에는 이런 문제가 안 터졌으니 그대로 뒀을 텐데, 역시 잘 작동하는 코드를 건드리지 않는 건 거대 기업도 마찬가지인가 봐요.

   GN+에도 동일한 보고서를 다루는 글이 올라왔네요.
     * https://news.hada.io/topic?id=21473

   지금보니 서식이 조금 깨졌네요. 마지막 두 줄은 각각 크래시 루프와 randomized exponential backoff 에 대한 주석입니다.

   며칠전에 있었던 인터넷 다운 글의 장애와 관련된것일까요?

   네 맞습니다 해당 장애 얘기입니다
"
"https://news.hada.io/topic?id=21441","HP 이사회를 설득해 Palm을 인수했지만, 단 49일만에 파괴되는 것을 지켜봤어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             HP 이사회를 설득해 Palm을 인수했지만, 단 49일만에 파괴되는 것을 지켜봤어요

     * HP CTO였던 Phi Mckinney는 WebOS의 기술력에 확신을 가지고 Palm 인수를 주도했지만, 49일 만에 프로젝트가 폐기되는 과정을 병상에서 지켜보았음
     * 인수 직후 CEO가 교체되며 기업 전략이 하드웨어 철수 중심으로 급변, WebOS는 핵심과제에서 제외됨
     * TouchPad 출시는 앱 생태계 부재와 준비 부족으로 실패했고, 이사회는 단 49일 만에 모든 WebOS 제품 철수를 결정함
     * 리더십 부재와 판단 프레임워크 오류가 주요 원인이었으며, 이를 방지하기 위해 DECIDE라는 의사결정 프레임워크를 제시함
     * 실패에도 불구하고 HP에 대한 믿음을 유지하고 있으며, 보다 나은 리더십과 사고 체계가 필요하다는 교훈을 공유함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Making the Case: Why I Believed in WebOS

     * 2010년 초, HP는 모바일 전환의 흐름에서 뒤쳐질 위기에 있었음
     * Palm의 WebOS는 당시 진정한 멀티태스킹과 우수한 UI, 혁신적 아키텍처로 내부 심층 기술실사에서 높은 평가를 받음
     * CTO로서 Palm의 코드베이스와 엔지니어링 역량을 직접 분석하고, 이사회에 인수를 강력히 추천함
     * HP는 WebOS가 향후 컴퓨팅 플랫폼 경쟁에서 차별화 자산이 될 것으로 판단했음
     * 2010년 4월, 12억 달러에 Palm 인수를 발표하며 HP는 미래 모바일 시장 진입을 선언

Building the Bridge: Post-Acquisition Integration

     * 인수 마무리 후, Palm 팀과 HP의 제조·공급망·고객 기반을 활용한 시너지 전략 수립에 몰두함
     * WebOS의 스마트폰을 넘어 태블릿, PC, 프린터로의 확장 가능성이 논의됨
     * 통합과 협력 분위기가 무르익던 시점에서 예상치 못한 위기가 시작됨

The First Timing Disaster: Leadership Upheaval

     * 인수 1개월 후, CEO Mark Hurd가 사임하고, SAP 출신 Leo Apotheker가 새 CEO로 취임함
     * Apotheker는 IBM처럼 소프트웨어 중심 기업으로의 혁신을 추진, PC·프린터·모바일 등 하드웨어 사업 축소를 강하게 선호함
     * WebOS는 하드웨어 투자로 간주되어 방해 요소로 인식되고, 새 전략의 핵심에서 멀어지게 됨
     * 기존 전략의 연속성을 기대했으나, 비전의 단절이 프로젝트의 기반을 흔듦

The Second Timing Disaster: When I Couldn't Be There

     * 2011년 6월 말, CTO였던 본인이 수술로 인해 8주간 병상에 누워 있었고, 결정적인 시기에 프로젝트를 지킬 수 없었음
     * 바로 이 시기에 Palm과 WebOS의 미래에 대한 핵심 의사결정이 진행되어, CTO의 전략적 개입과 조율이 불가능해짐
     * TouchPad 출시와 관련된 주요 결정에서 완전히 배제됨

Watching The Launch Fail From My Bed

     * TouchPad는 2011년 7월 출시되었으나, iPad와 같은 가격에 비해 앱 생태계와 품질 부족으로 실패함
     * 초기 판매는 27만대 중 2.5만대에 불과했고, 시장 반응은 냉담했음

49 Days: The Announcement That Ended Everything

     * 2011년 7월 1일, WebOS 3.0 기반 TouchPad 태블릿 출시
     * HP는 iPad와 동일한 499달러로 가격 책정, 앱 생태계와 마케팅 열세, 미흡한 완성도로 실패 징조가 명확해짐
     * 출시 분기 판매는 9백만 대의 iPad와는 대조적으로, TouchPad 25,000대만 실제 판매됨
     * 단 49일 만인 8월 18일, HP는 WebOS 디바이스 즉각 중단을 전격 발표함
     * 아포테커는 Palm 팀과의 협의 없이 일방적으로 결정했고, 플랫폼 전략에 필요한 최소 기간조차 주지 않음
     * 개인의 헌신과 기술력이 한순간에 무시되는 경험을 하며 무력감을 느낌

The Welcome Back: A Brutal Reality Check

     * CTO 복귀 첫날, HP Labs 식당에서 기술진들이 비난하며 둘러쌈
     * ""다시는 자리를 비우지 말라"", ""CEO와 이사회는 성인 감독(adult supervision) 이 필요하다""는 날 선 비난을 받음
     * 기술적 판단은 옳았지만, 부재 자체가 비난의 대상이 됨
     * CTO 부재 동안 이루어진 파괴적 결정에 개인적 책임을 느끼게 되는 상황 발생

The Scale Mismatch: Context That Should Have Been Obvious

     * Leo Apotheker는 연 150억 달러 규모였던 SAP 출신으로, 1,250억 달러 규모 HP를 이끌기엔 조직 규모나 소비자 기술 이해도에서 명백한 부적합이었음
     * 이사회가 CEO 적합성(리더십 역량과 기술·플랫폼의 평가 적합성)을 충분히 검증하지 않았고, 조직문화와 전략의 괴리가 커짐

The Personal Aftermath: Choosing Integrity Over Silence

     * HP 퇴사 시, 체험을 외부에 말하지 않는 조건으로 금전적 보상을 제시받았으나, 혁신의 실패 원인 공유와 진실 밝힘을 위해 거부함
     * 이 경험을 다른 리더가 유사한 파국을 피하는 데 도움이 되길 바라는 입장임

The Deeper Truth: Why I Still Believe in HP

     * 개인적 손실과 비판에도 불구하고, 현재까지 HP 주식을 단 한 주도 팔지 않음
     * 현재 CEO인 엔리케 로레스와 HPE CEO 안토니오 네리를 긍정적으로 평가
     * 이들은 HP 내부에서 성장한 기술 기반의 전략적 리더로 HP의 강점인 기술적 탁월함·전략적 사고·운영에 기초한 혁신을 구현할 것으로 신뢰함

근본 교훈: 지능이 아닌 사고 체계의 문제 - The Painful Lesson: Intelligence Doesn't Predict Decision Quality

     * WebOS 사태는 지능이나 선의로는 좋은 결정이 나오지 않는다는 교훈을 줌
          + 체계적 사고 프레임워크가 혁신적 의사결정의 품질을 좌우함
     * 주요 사고 오류:
          + 문제 정의 오류: 'HP를 소프트웨어 기업으로 바꿀 방법'만 고민
          + 정체성 중심 사고: 소프트웨어 기업이라는 개인 정체성에 따라 WebOS를 배제
          + 터널 비전: 동시에 진행된 Autonomy 인수에만 집중
          + 시간 압박: 49일은 플랫폼 성공을 판단하기엔 턱없이 부족
     * 이와 유사한 사고 패턴이 여타 기업과 산업에서도 반복되고 있다고 분석함

The Birth of Better Thinking: The DECIDE Framework

     * 실패 분석을 통해 혁신적 불확실성 상황에서의 의사결정 체계로 DECIDE 프레임워크를 제안:
     * DECIDE 프레임워크:
          + Define: 진짜 결정의 본질 정의
          + Examine: 사고 과정의 인지 편향 점검
          + Challenge: 전제 조건 체계적 검토
          + Identify: 혁신 맥락의 의사결정 함정 식별
          + Design: 명확히 구분되는 여러 대안 설계
          + Evaluate: 혁신에 적합한 증거 기반 평가
     * 이론적 학술이 아닌, 실제 실패에서 도출된 실용 도구

The Question That Still Haunts Me

     * 내가 병상에 있지 않았다면 결과가 달랐을까?
          + 만약 회복 기간이 아니었다면, WebOS에 더 많은 시간을 확보하거나, 더 나은 프레임워크로 결론이 달라졌을지에 대한 의문은 여전히 남음
          + 의사결정 프레임워크 부재가 기술력보다 더 결정적인 실패 원인이었음
     * 결국 CEO Apotheker도 단 한 달 후 파면(WebOS 철수 발표 35일 후), 이사회가 뒤늦게 문제를 인식했으나 WebOS의 혁신 가치는 이미 사라짐
     * WebOS의 기술 유산은 이후 LG 스마트 TV 플랫폼 등에 성공적으로 적용, 모바일 운영체제에도 지속적 영향
     * HP가 이 플랫폼과 생태계 가치를 주도할 수 있었던 아쉬움을 남김

Your Innovation Decision

     * 조직이 새로운 혁신 기술 또는 기회를 평가하는 상황이라면, 체계적 사고 프레임워크 없이 직관이나 기존 사업 논리에만 의존하는 실수를 반복하지 말 것을 당부함
     * DECIDE와 같은 프레임워크를 사전에 적용함으로써, 중요한 기술적 혁신의 기회를 지킬 수 있음
     * ""기술이 뛰어난 것보다 중요한 건 의사결정의 질""
     * 혁신에 대한 판단은 직관이 아닌 체계적 사고로 접근해야 함

   덕분에 WebOS는 LG가 잘써먹고 있으니 걱정하지 마세요

        Hacker News 의견

     * 2011년 6월 말, 갑작스러운 의료적 위기로 8주 간 병상에 누워 있었던 사이 HP TouchPad가 WebOS 3.0을 탑재해 iPad와 동일한 $499 가격으로 출시된 상황을 지켜본 경험 공유. 앱 생태계도, 마케팅 파워도 부족했음에도 너무 서둘러 출시했기에 경쟁력이 떨어질 수밖에 없었던 판단. Palm과 1년간 밀접하게 일했음에도 이미 중요한 결정들이 다 내려진 상태였던 점, 책임 회피가 아닌가 하는 의문 제기
          + 가격이 높았던 점은 확실하지만, 논쟁의 여지 있음. 다른 교훈은 이런 신제품 성공을 위해선 수년의 투자 필요성 인식. 잘못된 시장 크기 예측은 늘 있는 일이고, 성공적인 앱 생태계 역시 장기간의 헌신이 없으면 안 된다는 사실 강조. Windows Phone 실패와 유사한 사례로, 앱 생태계를 갖추기도 전에 포기한 점 지적
          + 그 시기 HP에서 근무했던 경험 공유. 사내 전체에 앱 개발 요청 이메일이 돌았고, OS용 앱 만들면 Palm Pre를 무료로 받을 수 있었던 기억 소환. 화면을 끄는 간단한 앱을 만들어 ""거울 앱""이라 명명해 Palm Pre를 받은 일화. 수년 후 Palm Pre를 살려보려 했으나 배터리 구할 수 없어 실패한 에피소드
          + 기사 속 저자가 본인의 입지나 실수를 감추기 위한 서술과, 경영진 자체의 실패를 구분하려 고민. Leo Apotheker의 리더십 부재가 HP의 재앙이었기 때문에 저자 입장에도 공감. Autonomy 인수가 대표적 실패 사례였으며, 소프트웨어 경험을 내세운 Apotheker조차 제대로 역할하지 못했던 점 비판
          + 상황 판단이 다소 박한 것 같음. 문제가 있다는 걸 알면서도 마케팅과 경영진이 모든 게 준비될 때까지 출시를 미루길 바랐을 수도 있음. 가격이 절반 수준이었다면 결과가 달라졌을 거라는 의견
          + Palm에서 TouchPad 출시 당시, 최종 사용자 소프트웨어와 초기 셋업 경험 담당했던 경험 공유. 소프트웨어는 준비됐지만 하드웨어가 동급 iPad에 비해 많이 부족했다는 평가. 다음 버전에서 더 경쟁력 있었을 수 있으나 CEO의 장기적 판단 미숙이 제품을 끝냈다고 지적. HP가 제조원가 이하로 PC를 판매하며 시장 점유율만을 자랑했던 당시 분위기 회상
     * 문제의 본질이 저자의 부재가 아니라, Leo Apotheker의 경력과 HP의 리더십 역할 간 괴리였다는 주장. SAP 시절 Apotheker 경험치가 HP의 작은 사업부 매출과 비슷한 수준임에도 HP 전체를 맡긴 이사회가 문제였다는 시각. 경영은 사업 실체에 대한 구체적 이해·경험이 필요하다는 교훈 강조. 아무 CEO나 옮겨앉힌다고 성공하는 게 아니라는 점 인식
          + 경영의 일부 요소는 사업에 독립적일 수 있지만, 90년대 이후 MBA와 경영학 교육이 ""경영은 사업 내용과 무관한 만능 기능""이라는 잘못된 믿음을 만든 문제 지적. MBA만 있으면 어떤 대기업도 똑같이 경영 가능하다는 환상 비판
          + CEO만이 아니라, 기술기업에서도 Director 이상의 직위부터 실질 역량 부족이 반복된다는 경험담. 리더십의 스킬 격차를 감추기 위한 각종 지표·보고서가 양산될 뿐 실질 개선은 없음. 이런 메트릭이 효과 있었다면 이미 네이비실 수준의 조직력이 나왔어야 한다는 자조적 통찰
          + 기술 지식 없이 CTO 자리에 거짓말로 올라간 인물의 실화 소개. 정상 학위·성적도 없이 MBA를 취득해 CTO 자리를 얻었으나, 6개월 만에 그만두고 여전히 자신감만 남은 사례. 기술조직을 이끌 때 실무에 대한 이해가 없다는 게 어떤 결과를 낳는지 경고
          + 업계 경험 없이도 대단히 성공하는 CEO사례와, 오랜 업계 경험 끝에 겨우 평범한 리더가 되는 경우 병존에 대한 관찰. 좋은 CEO 만드는 공식은 누구도 모르는 미지의 영역임을 인정
          + Leo가 HP로 이직한 게 아니라 SAP에서 재계약 실패해 사임했다는 사실 환기. SAP가 포기한 인물을 HP가 바로 임명한 셈이라는 비꼼
     * HP에 대한 믿음을 고백한 저자의 태도가 궁금했던 사용자. 수십 년간 HP 제품은 부실 그 자체였고, DV 시리즈 노트북의 잦은 고장과 소송, 불편한 트랙패드와 취약한 하우징 등 소비자용 HP 제품 실망 경험 나열. 프린터 분야조차 소프트웨어와 소비자 기만, 숨겨진 비용 등 신뢰 불가 브랜드라는 혹평. HP 브랜드가 Yugo보다 비호감이라는 평가
          + HP 노트북 중 저가형은 조악하지만 ProBook 및 Zenbook 라인의 확실한 긍정 경험 공유. ProBook은 수리 편의성까지 훌륭했고 Zenbook의 발열을 제외하면 전반적으로 신뢰성·지원 좋았던 점 언급. 저가형 기기를 피하면 HP 제품도 괜찮으며, 저가형을 파는 모든 제조사는 자연스럽게 품질 하락 이미지를 갖는다는 견해
          + 저자가 HP와의 인연을 끊지 않고 있다는 점에 주목. 유용한 인맥을 위해 브릿지를 불태우지 않는 지혜, 결국 저자의 태도는 실질적으로 타협 혹은 신호 보내기라는 분석 제시
          + HP의 엔터프라이즈 사업은 현재 별도 회사인 HPE로 분사된 사실 상기. 저자는 블로그에 HP와 HPE 모두 보유 중임을 밝혔다고 설명
          + HP 주식을 한 주도 팔지 않았다는 저자 언급 지적. 여전히 HP에 대한 신뢰를 강조하는 건 경제적 이해관계 때문일 수 있다는 관점
          + HP가 2010년과 지금은 완전히 다른 회사라는 설명. 한때 명성이 높았던 기업이였던 시절을 그리움과 함께 언급. 질문 자체는 유효하다고 인정
     * 8주 부재 동안 제품이 출시·취소된 시간표가 맞지 않다는 의문. 제품 27만대가 계획적으로 만들어졌고 당연히 그 전부터 결정된 사안. 책임을 회피하는 게 아닌지, 혹시 저자가 전 단계 얘기를 누락한 것인지 의심
          + 가격을 내렸을 때 제품이 폭발적으로 팔렸던 사례 지적. 실패 요인은 아예 WebOS를 포기한 결정이며, 정답은 가격 인하·문제 수정·차기 제품 준비의 반복이었다는 시각. 당시 안드로이드 기기의 완성도가 낮았기에 WebOS가 훨씬 앞서 있었던 점 강조
          + Palm Pre와 TouchPad 열성 유저로서, 첫 출시는 다소 거칠었지만 리뷰는 긍정적이었고 일단 저렴해지자 순식간에 완판. 핵심 문제는 완성도 부족이 아니라 '아이패드 수준 가격+아이패드급 품질'이라는 불가능한 목표 설정. 시장 안착엔 인내심과 장기적 헌신이 필요했고, 리더십이 전략에 헌신해야만 성과 가능. 대형 프로젝트는 시작 후 흔들리지 않는 인내가 필수라는 조언
          + 저자가 착각하는 게 아니라 CEO의 취향에 따라 프로젝트가 취소됐고, 소수만 방어하다 사라진 상황. 실제 문제는 제품/시장 부적합이었으며, 한번의 실패 분석도 없이 철수한 게 아쉬움. 저자가 자기 실수보다는 타 부서에 책임 돌리는 느낌
          + HP가 Palm을 인수한 지 오래되지 않은 상황에서 계획된 물량에 대한 책임이 Palm에 있었던 점, 시장 크기 예측조차 업무 영역이 아니었을 수 있음. 이런 플랫폼은 원래 수년 투자가 필수였다는 배경
     * 신임 CEO의 입장은 Palm 인수가 이미 손실로 간주됐고, TouchPad 실패와 CTO의 책임 회피가 향후 전략을 보여줬다고 해석. WebOS는 훌륭했지만 HP가 Apple과 경쟁할 위치는 못됐고, 더 많은 모바일 기기 출시가 오히려 손실만 키웠을 것이라는 현실적 분석
          + WebOS는 기술적으론 훌륭했지만, 실패 원인은 제품관리 부재에 있었다는 의견. TouchPad 가격 책정이 신생 플랫폼에 불리했고, GTM(출시 시기) 결정은 CTO 몫이 아니라는 점, WebOS의 잠재력이 단순 태블릿을 뛰어넘음에도 HP가 전부 버렸다는 점 안타까움. 소프트웨어·서비스 전환에 미숙했던 HP 배경 설명
          + HP가 애플과 충분히 경쟁할만한 위치였고, 꼭 1등이 아니어도 이윤 창출이 가능했단 점 강조
          + CTO에 동정은 있지만 HP TouchPad 출시가 잊혀지지 않을 정도로 혼돈이었다는 사실 상기. 앱 생태계 미비, 완성도 부족 등은 8주 부재로 설명될 일이 아니라고 비판
          + WebOS가 당시 세 번째 플랫폼으로 가지는 잠재력에 기대감 표현. 오늘날 모바일 UI의 기준(카드 기반 앱 전환, 스와이프 종료 등)이 WebOS에서 비롯됐다는 평가. HP 리더십이 단기 이슈에 흔들리지 않고 장기 비전 세웠다면 기업 미래가 달라졌을 수도 있단 상상. 단 1년 만에 CEO 한 명이 회사를 망가뜨릴 수 있다는 사례로 기록. WebOS 인수는 미래를 위한 투자였을 수 있었지만, Apotheker는 하드웨어 중심 HP의 본질과 다른 방향성에 집착하며 주주의 반대를 샀던 것으로 분석
          + TouchPad 하드웨어가 HP 경영진에 의해 WebOS 팀에 강요된 점을 전함
     * 당시 TouchPad 출시 행사에 직접 참석했던 경험. HP가 강력한 엔터프라이즈 PC 사업을 바탕으로 기업 시장을 겨냥할 줄 알았으나, 오히려 소비자 대상 iPad와 경쟁 포지션으로 발표. 기업 시장에 맞는 OS+하드웨어 조합에 대한 기대감이 어그러졌다는 당혹감
     * 15년 만에 글을 올린 저자의 동기나 진정성, 동료 책임만 강조하는 태도에 의심이 가지만, 글 후반의 책 홍보가 본질이라는 결론. 신뢰할 만한 자기계발서처럼 보이려는 인상에 피로감 드러냄
          + 저자는 본인이 믿는 사연을 진지하게 썼을 거란 입장. PDF 책은 무료에 CC 라이선스임을 확인. 오랜 침묵의 이유가 은퇴 후 시간적 여유, 내부 반발 걱정 등 여러 사정이 있을 수 있음을 이해. WebOS가 iOS/Android와 대등한 경쟁자가 될 수 있었을 가능성, CEO 한 명의 판단으로 큰 미래가 사라진다는 교훈. 시간이 지나면 ATI/AMD에서도 CUDA와 같은 기회 포기를 후회하는 내막이 밝혀질 수 있다는 통찰
          + 흥미 없는 글이라도 상위 노출을 요구하지 말고 그냥 지나치라고, 해커뉴스의 참여 구조를 설명
     * WebOS도 분명 iOS/Android에 견줄 수 있었으나, 본질적 약점은 '웹 UI 플랫폼'이라는 점. HP가 WebOS를 도입하면서 iOS/Android 앱 수준까지 성능 개선이 필요했지만, 상당한 투자가 없었다는 점 지적. 서드파티 모바일 플랫폼이 사라진 주된 이유로 HP와 Nokia의 리더십 실패를 꼽음
          + WebOS엔 웹뿐 아니라 네이티브 개발 키트도 있었음. 무선 충전, 당시로선 cutting-edge SoC, 현대적인 Mocha UI 등 하드웨어·UI 측면에서 혁신적이었음. 그러나 자체 Webkit과 JS JIT가 낡아 성능 한계에 직면했고, 빠르게 발전하던 JS 엔진과 웹 표준에 뒤처짐. CPU는 당시 최고였어도 플랫폼 전체 성능은 뒤떨어지는 게 핵심 이슈
          + 경쟁 수준만 따라가는 것으로는 기존 생태계를 깨는 게 불가능하다는 사실. 앱/액세서리 생태계에 불편을 감수하게 할 만큼 강한 차별점이 없으면 성공 불가. WebOS/BlackBerry 10 모두 충분히 좋았으나 한방이 부족해 실패했고, Apple만이 leapfrog 전략으로 시장을 흔들 수 있었던 역사적 맥락 강조
          + Nokia가 MS에 인수되기 위한 ‘의도적 침몰’이었다는 개인적 음모론 소개. 결국 MS가 모바일 플랫폼 경쟁에 실패한 현실
     * Palm Pixi에 대한 애정과 디자인, 사용성에서의 걸작으로서 기억 공유. HP 인수 시점부터 WebOS가 바로 사라질 것이라고 내부 식견이 있었던 점, HP가 IBM처럼 소비자 시장을 빠르게 포기하려 한 움직임을 감지한 경험. Blackberry 역시 당시 기업용 시장에서 정점을 찍지 않았기에 HP의 전략적 판단이 소비자층이 아닌 비즈니스 공략으로 흔들린 배경 설명
          + Palm Pixi에서 OxyContin을 판다는 재미있는 뮤직비디오 유튜브 링크 공유 https://www.youtube.com/watch?v=6GMavkkkFtQ
          + WebOS의 완성도에 대한 추가 동의, HP가 Palm Pre 및 Pixi 모두를 죽였다는 점 강조
     * HP가 소프트웨어, 하드웨어, 펌웨어, 앱스토어 인프라, 글로벌 유통망까지 모든 스택을 보유했던 상황. 그럼에도 불구하고 49일 만에 프로젝트가 종료됐던 사실 자체가 개발자 신뢰 구축에 최소한의 시간도 주어지지 않았음을 보여줌. 제품 문제가 아닌, 인내력 부족이 핵심 실패 요인임을 결론
"
"https://news.hada.io/topic?id=21558","나는 이제 합리주의자인 것 같음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           나는 이제 합리주의자인 것 같음

     * 최근 LessOnline 합리주의자 컨퍼런스에 참여하여 오랜 온라인 지인들과 실제로 만나 교류함
     * 행사 본 세션보다 참석자들 간의 깊은 대화와 소통이 가장 인상적이었음
     * 본인은 합리주의자 커뮤니티와의 주요 차이점으로 AI 위험에 대한 집착, 특정 문화적 특성, 그리고 컬트 분위기를 꼽음
     * 시간이 흐르며 커뮤니티의 성숙과 다양성 확대, 가족화가 본인의 인식 전환에 큰 영향을 미침
     * 결국 타인의 시선이 아닌 자신만의 기준으로 커뮤니티에 대한 소속감을 인정하게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LessOnline 합리주의자 컨퍼런스 경험

     * 최근 LessOnline이라는 합리주의자 블로깅 컨퍼런스에 참석하여 Scott Alexander, Eliezer Yudkowsky, Zvi Mowshowitz, Sarah Constantin, Carl Feynman 등 오랜 지인들과 다시 만남
     * 많은 사람들과 오프라인으로 처음 만나 기쁘게 대면함
     * 행사 장소인 Lighthaven은 미로 같은 구조와 정원 등이 있는 독특한 공간으로, 버클리의 새로운 '너드의 이상향'처럼 여겨짐
     * 필자는 Nate Soares와의 Orthogonality Thesis 대담, 그리고 양자컴퓨팅 및 이론 컴퓨터 과학에 대한 Q&A 세션을 진행함

컨퍼런스에서의 대화와 분위기

     * 공식 세션보다 더 기억에 남는 점은, 하루 종일 이어진 백여 명의 사람들이 곳곳에서 벌인 역동적이고 활발한 대화임
     * 행사 측은 처음부터 작은 토론 그룹 대화를 적극 권장하는 등, 집단적 의견 교류에 중점을 둠
     * 한 건물에서 다른 건물로 이동하는 데 몇 시간이 걸릴 정도로 누구나 적극적으로 대화에 참여함
     * 본인은 “Scott Aaronson 맞습니까?”라는 식의 인사로 자주 불려 세워짐

합리주의자 정체성과 이질감의 이유

     * 수년 동안 합리주의자와 비슷한 관심과 시각을 가졌으면서도, 특정 이유로 '합리주의자'임을 적극적으로 표방하지 않음
     * 첫 번째는, AI가 초인간적 능력을 갖추고 인류의 조건을 바꿀 것이라는 집착에 거리감을 느낌
          + 예전에 이 주제에 회의적이었으나, 최근 AI의 급격한 발전으로 인해 반론을 거둠
          + 본인 역시 이제 AI alignment라는 핫이슈 일부에 기여하는 중임
     * 두 번째는 문화적 요인임
          + 대체로 20대 청년들이 직접 창업한 단체 및 그룹하우스에서 다양한 실험을 하는 문화와, 기존 교수로서의 본인 삶이 이질적으로 다가옴
          + 그러나 이제는 커뮤니티에도 결혼과 자녀를 둔 가족 중심의 변화가 나타나고, 아이들도 뛰노는 모습이 보임
          + Rationalists는 Selfish Reasons to Have More Kids 같은 책의 영향으로 혹은 전통적인 동기에서 자녀를 낳으며, 다음 세대를 위한 더 나은 세상을 고민함

커뮤니티의 특성과 오해에 대한 시각

     * 과거에는 일부 분위기에서 컬트적 느낌(Eliezer가 구루 같은 존재) 이 나서 거리감을 뒀음
          + Eliezer 스스로도 이제는 새로운 세대로 물려주고 커뮤니티는 점차 다양화됨
          + 대표적 지적 중신은 Scott Alexander가 운영하는 블로그와 다양한 신진 리더들의 등장임
     * 하지만 실제 경험상, 이 커뮤니티는 '믿는 무리'라기보다는 아이디어와 열정의 네트워크에 가까움
     * AI, 관계, 양육 등 다양한 주제에 대한 실용적이고 솔직한 논의가 이루어짐
          + 예를 들어 Jacob Falkovich가 주도한 모임에서 남성들이 연애와 여성 이해에 대해 토론하고, Gretta Duleba가 관계 갈등을 다루는 세션을 진행함
     * 반면, ""Rationalists가 실제로 우파적이거나 모종의 해악을 미치는가?"" 등 외부의 비난에 대해, 실제 현실에서는 온건한 민주당 지지와 공공선을 지향하는 토론이 주를 이룸

소속감의 변화와 커뮤니티에 대한 자기 인식

     * 가장 중요한 변화는, 더 이상 타인의 시선에 신경 쓰기보다는 본인 스스로 커뮤니티에서 의미를 느끼게 된 점임
     * 이전에는 RationalWiki, SneerClub 등 외부 비난과 NYT 기사 등으로 위기감을 느꼈으나, 이번 LessOnline 현장에서 커뮤니티의 번영과 활력, 실제 공간과 차세대 구성원들의 등장을 보고 큰 감명을 받음
     * 자신에 대한 외부 시선을 두려워하기보다, 가족·학계·커뮤니티·블로그 독자 등 자신을 인정해주는 사람들과 함께하면서 정체성을 받아들이게 됨

결론

     * 본인은 예전과 변함없이 컴퓨터 과학자, 학자, 진보적 유대인, 블로거 등 다양한 정체성을 유지하는 중임
     * 합리주의자 커뮤니티는 AI, 양육, 관계 등 폭넓은 주제에서 실사구시적 담론과 실질적 소속감을 제공함
     * 외부의 냉소나 비난은 실상과 다르며, 커뮤니티의 지속적 성장과 의미, 그리고 개인적 만족감을 새삼 확인함

기타 공지사항

     * 댓글 응답이 다소 늦어질 수 있음 (딸과 함께 갈라파고스 여행 동행 중)
     * UT Austin에서 철학 및 이론 컴퓨터 과학 세션(YouTube 영상 공개)에 참여함
     * Alon Rosen이 새로운 이론 컴퓨터 과학 상(Luca Trevisan 기념) 후보 추천을 요청함
     * STOC'2025 온라인 포스터 세션이 6월 12일까지 등록 가능하며, 필자는 프라하 현장에 plenary로 참석 예정임

        Hacker News 의견

     * 이 글을 읽으면서 왜 이 운동에 대해 뭔가 거슬리는 느낌이 드는지 더 깊이 생각하게 되었음
       '합리적'이라는 태도와, 논리를 '최초 원칙'에서부터 전개하려는 시도, 그리고 이 커뮤니티에 존재하는 절대적인 분위기 사이에 본질적인 긴장감이 느껴짐
       이들은 항상 너무 자기 확신에 차 있는 사람처럼 보임
       ""이렇게 생각하긴 하는데 모든 각도를 다 고려하지는 않았을 수도 있고, 내가 틀릴 수도 있다""라는 식의 겸손함이 거의 안 보임
       어떤 주제에 의견이 없거나 ""모르겠다""라고 말하는 걸 창피하게 생각하는 사람들의 특징
       AI 시대 이전엔 이 정도 성향이 용인 가능했지만, 요즘엔 세상이 끝날 것처럼 열광하는 모습에서 겸손함이 부족하다는 게 더 노골적으로 드러남
       AI의 영향에 대해 우리가 제대로 파악하지 못하고 있을 수도 있다는 인정이 필요함
       실제론 별로 특별하지 않고, 기대보다 지루할 수도 있음
          + 이들의 모습은 ""Effective Altruism"" 그룹과 닮았음
            순수 논리에 빠져서 처음 전제조건이 인위적인 상황임을 모르고 이상한 결론까지 가서는 그 결론에 스스로 갇혀버림
            논리만으로 자신들이 우월하다고 생각하는 태도 때문에 종종 오만하게 굴기도 함
            이런 사고방식은 10~20대에 잠깐 빠질 수 있지만, 돈 많고 인터넷이 뒷받침되면 이런 사고방식이 장기화될 수 있음
          + ""<i>epistemic status: mostly speculation</i>"" 같은 문구를 블로그에 붙이기 시작한 것도 이런 사람들임
            자기 확신의 위험에 대해 에세이를 쓰기도 하고, 본인의 예측이 얼마나 자주 틀리는지 측정하기도 하며, ""내가 틀렸던 것들"" 목록을 공개적으로 관리함
            이 그룹 전체를 너무 일반화해서 보는 건 아닌지 물어보고 싶음
          + 싱귤래리터리언들이 AI 정렬 문제에 대해 과하게 걱정하는 건, 사실 더 현실적인 AI의 위험(알고리즘 편향, 정책 세탁, 에너지 소비, 부의 집중 가속화 등)에서 사람들의 시선을 돌리게 하는 연막이라고 항상 봐왔음
            현재 실제적 문제 해결보단, 먼 미래의 가설적 문제만 강조해서 장기주의(롱터미니즘)로 이어짐
          + Scott Alexander의 블로그(Astral Codex X)는 끊임없는 의심과 자기성찰로 가득함
            실제로 본인이 실수한 목록도 공개함(https://www.astralcodexten.com/p/mistakes)
            나에게 있어 '합리주의 커뮤니티'에 대한 유일한 접점이 이 블로그이고, 오만하다는 느낌은 전혀 받지 못했음
            오히려 정반대임
          + '합리적'이란 개념은 결국 자신과 주변인들이 만들어 낸 메탄 가스를 생산하고 흡입하는 옹호 분위기용 도구라는 느낌을 받음
            나도 Stanford에서 천재 소리를 듣는 게 재밌긴 했지만, 때때로 시골에서 흙 묻히며 실질적인 선행을 하는 게 더 의미 있다는 걸 생각하게 됨
     * 논리는 그리스 철학자부터 현대 컴퓨터에 이르기까지 대단한 도구임
       하지만 순수 합리주의의 핵심 문제는 그 논리의 '최초 원칙'을 제대로 점검해야 한다는 점임
       원칙이 잘못되거나 복잡성이 누락되면 논리는 오히려 잘못된 방향으로 이끔
       예를 들어 아리스토텔레스도 대단한 논리학자였지만 잘못된 결론을 많이 내렸음
       복잡성을 간과한 예시는 자연선택이 귀납적 분석에서 출발한 점임—첫 번째 원칙에서 오는 게 아니라 실제 현실이 너무 복잡했기 때문임
       논리를 무시하자는 게 아니라, 항상 '임시적 겸손'을 가져야 한다는 사실을 강조하고 싶음
       그리고 Scott Aaronson의 팬임
       (Aristotle가 왜 틀렸는지에 대한 링크)
       (자연선택의 귀납적 기원)
          + 여기서 논의 중인 '합리주의자'들은 데카르트식 합리주의자가 아니라 베이지안 경험주의자임
            베이지안 확률론은 아리스토텔레스가 놓쳤던 불리언 논리의 연속 실수확장임
            경제학의 '이성적 베이지안 행위자' 이념 때문에 '합리주의자'라 부르는 듯함
            단, 이들에게도 ""우주는 조인트 조건부 확률분포로 간단히 추론할 수 없다""는 유명한 구호가 있음
            실제론 AIXI조차 계산 불가능하며, 현실 문제는 이론적으로도 엄청나게 어렵다는 걸 이들은 잘 알고 있음
          + 논리는 '진리' 그리고 '증명 가능성'을 연구함
            이상적이면 둘이 일치하지만, 현실에선 진짜 복잡한 시스템이 많아서 '참'이면서 '증명 불가'하거나 그 반대도 많음
            그래서 분별력도 중요함
            나는 스스로 진실의 울림 같은 걸 들으면 좀 더 깊은 탐구를 하게 되지만, 항상 자기 의심도 잊지 않으려고 함
            결국 생각은 유혹적이지만 그 자체로 장애물이 될 수도 있음—무의식적 완벽에 이르는 길에 필요한 '걸림돌'임을 받아들이는 것이 중요함
          + 아리스토텔레스를 조금 옹호해주고 싶음
            비록 그의 논리와 형이상학이 미완성이지만, 당시로서는 전례 없는 시도였고, 그의 커뮤니티는 생물학에서도 실증 연구를 외면하지 않았음
            후속 세대가 그를 너무 맹신한 것이 더 문제였다고 생각함
          + '임시적 겸손'이라는 단어에 정말 공감하게 됨
            이 단어로 밈이 하나 탄생하길 바람
            pH(겸손의 농도)를 더 높여야 한다고 주장함
          + 논리는 그저 '도구'
            절대진리로 통하는 마법의 창이 아님
            상황에 따라 적절할 수 있고 아닐 수 있으니까, 그저 쿨하게 도구 활용
     * 지금 Yudkowsky의 ""Rationality: from AI to zombies"" 읽는 중
       사실 이 책은 블로그 포스트 모음집이라 반복되는 느낌이 강해서 예전엔 50장 쯤 읽다 포기했었음
       이번에는 주제에 더 관심이 생겨, 꽤 재미있게 읽고 있음
       Yudkowsky가 실제로 심오한 통찰을 보여주는 부분이 있다 생각함
       'Belief in Belief', 'Emergence', 'Generalizing from fiction' 같은 일상 사고에 쓸만한 아이디어도 많음
       예전엔 단순히 논의가 겉돌거나 의미 차이로 싸우는 경우가 많았는데, 실제론 단어를 다르게 해석해서 생기는 논쟁인 경우도 많았음
       이런 함정을 실시간으로 알아차릴 수 있게 된 점이 매우 유익
       한번 시도해보길 추천함
          + 합리성 커뮤니티 내 분위기는 이질적으로 느껴지기도 하지만, 실제로 이 문헌에서 설명되는 사고 도구들은 굉장히 가치 있음
            단 아주 중요한 점은, 자신이 '합리주의자라서 남보다 더 옳다'는 생각이 드는 순간 그 자체로 합리주의자로서 실패하게 됨
            이런 실수는 매우 쉽고, 합리적 사고 도구를 잘 쓰려면 오히려 이전보다 더 겸손해져야 함
            ""난 맞아""를 자주 말하기보다, ""오, 혹시 내가 틀렸나?""를 더 자주 말해야 진짜 합리주의자가 될 수 있음
          + 철학, 문학, 역사에 익숙하지 않은 STEM 분야 사람들이 이런 분야의 아주 기본적인 개념을 새롭게 접했을 때 과하게 감탄하는 현상이 있다고 생각함
            Yudkowsky가 독창적인 뭔가를 발견한다기보단, 이미 수천 년간 논의된 주제를 새롭게 접하는 효과임
          + 이런 언어적 논쟁은 '언어 게임'이라고 Wittgenstein이 불렀던 현상임
          + 만약 AI의 위험성 핵심 논증이 궁금하다면 Yudkowsky의 책보단 Nick Bostrom의 <i>Superintelligence</i> 앞부분을 읽는 게 훨씬 효율적임
     * 예전엔 Scott Aaronson을 항상 좋게 봤고, 그가 하는 일도 존경했음
       하지만 이 글에서 'Galt's Gulch'에 모인 사람들 얘기를 읽고 나선 ""아, 이제 그도 코뿔소가 됐구나""라는 생각이 들었음
       (코뿔소 연극 링크)
       농담하자면, '합리주의자(rationalist)'와 '합리화자(rationalizer)'의 차이는 바로 인센티브임
          + Scott Aaronson은 유명 합리주의자 중에서는 그나마 제일 합리적인 쪽이라고 생각했음
            본인도 본인이 합리주의자라는 것을 Scott Siskind가 말해주기 전까지 자각하지 못했다는 점이 좀 웃김
          + '코뿔소' 연극 소개해 준 것에 추천하고 싶음
            매우 흥미롭고, 처음 알았던 내용이라 상위 포스트로도 가치가 있다고 생각함
     * 나도 Rationalist 커뮤니티와 Rust 커뮤니티 모두 좋아하는 사람으로서, Hacker News에서 두 커뮤니티가 비슷하게 다뤄지는 모습이 흥미로움
       멸시, 호기심 부족, 대담한 일반화 발언의 폭력성이 항상 충격적임
          + 최근 Hacker News 코멘트들이 오히려 합리주의 커뮤니티에서 오래 활동한 사람들이 많아 더 의미 있다고 봄
            나도 LessWrong, SSC 등에서 친구들과 꽤 오랜 시간을 보내며 수많은 블로그와 글을 읽었음
            그런데 매번 뭔가 쓰려고 하면, '너는 아웃사이더라 잘 모른다'는 식으로 치부되는 일이 반복
            스스로는 다른 집단을 비판하는 걸 좋아하지만, 정작 자기 집단에 도전받으면 방어적으로 굴더라
          + Rationalist와 Rust 커뮤니티 모두 실제로 무언가를 만들어내는 점에서 매우 적극적임
            그런데 아무 일도 하지 않는 사람이 남의 생산적 활동을 비판하기는 훨씬 쉬움
            타인의 페이스에 뒤처지거나 불리해질까 불안감에 싫어하고 미워하는 심리가 온라인에서는 더 쉽게 나타남
            인터넷이 주는 익명성과 거리감이 이런 심리를 더 심화시킨다고 생각함
          + Hacker News는 Rust를 좋아하는 곳이라 생각했음
            나도 예전에 여기서 Rust를 접하고 러스트를 배움
            만약 러스트 팬을 찾으려면 바로 HN에서 찾을 듯
          + 내가 본 바로도 Rust 커뮤니티가 아닌, 오히려 다른 언어 커뮤니티가 Hacker News에서 더 방어적으로 보임
            Zig나 Odin, C++ 관련 스레드 가보면 항상 Rust가 더 낫다는 논쟁이 생김
            (실제 예시)
          + Rust 커뮤니티가 여기서 비난받는다는 말을 듣고 당황했음
            실제로 Go가 제네릭 도입을 거부하며 오만한 태도를 보여서 욕을 더 많이 먹은 것 같음
            개인적으로는 '프로그래밍 커뮤니티'라는 개념 자체가 별로 공감되지 않음
            열정은 인정하지만, 좀 더 여유를 갖고 심리 상담도 받아보고 연애도 해보라고 조언함
     * 현재 논의 중인 건 https://en.wikipedia.org/wiki/Rationalist_community 링크의 '합리주의 커뮤니티'임
       https://en.wikipedia.org/wiki/Rationalism 보다는 확실히 전자라는 점을 확인
          + 모두가 명칭을 잘못 씀
            실제 운동명은 rationality 혹은 LessWrong 스타일의 rationality임
            고전적 철학 rationalism과 다름
            rationality는 경험주의 쪽에 더 가까움
            단어가 너무 비슷해서 이 명칭 싸움은 'hacker' 논쟁만큼이나 힘든 싸움임
          + Lightcone, LessWrong 등의 커뮤니티를 이리저리 읽어보고 있지만, 아직도 도대체 이들이 뭘 '하는지' 잘 모르겠음
     * 한때 effective altruism/rationalism에 푹 빠진 여성에 관심이 있어 모임에 가봤음
       본인의 반골기질 덕분에 거부감이 들었고, 몇 년 지나고 나서야 그 분위기가 꽤 컬트적이었다는 걸 깨달음
       내 안의 엣지있는 무신론자적 반골심이 그들과 어울리지 않게 해준 게 오히려 다행이라고 생각함
     * 이 글 전체가 한 편의 대단한 모험 같았음
       Cade Metz의 존재론적 '위협' 부분이 특히 재밌었음
       하지만 결국 시카고의 위대한 현자가 한 말을 빌리자면—'~ism'에 빠지면 곤란함
       중요한 건 — 사조에 매이지 않고 자신을 믿는 것임
       John Lennon의 ""비틀즈를 믿지 않는다, 나는 나만 믿는다"" 언급
       어차피 내가 바다코끼리든 뭐든, 남한테 얻어타야 하는 건 매한가지임
          + Cade Metz의 위협 요소에 주목한다는 점이 흥미롭다고 생각함
            합리주의자들은 종종 비평을 단순히 '만나보니 좋은 사람들이더라'는 식으로 무시함
            Curtis Yarvin이 Vibecamp 모임에 와서 친절하게 굴자, 그 이후 그의 옹호자가 되는 현상도 있음
            이런 인상평은 합리주의적 태도와 정반대인데, 내부-외부 집단이 생긴 뒤엔 객관적 비판을 외면하는 전형적 증거라고 봄
            Cade Metz 논쟁을 보면 어느새 '우리 편 아니면 적' 구도로 바뀌고, 실제로 다뤄야 할 논점을 흘려버림
            네오리액션 같은 경우엔 '이상한 생각이라도 건질 게 있으면 참고 들어라'고 하다가, 자신들은 정작 반론에 무제함
            결국 이 운동이 생각만큼 진실 탐구 중심이 아니라 자기 확신이 더 강한 집단일 수 있음을 깨달음
          + 이데올로기는 사람들이 이데올로기가 없다고 생각할 때 가장 강력해짐
          + 진심으로 Rationalist들이 자동차 없는 파시스트 아나키스트가 아닐까 걱정함
          + '~ism'은 절대 좋지 않음
            사람은 사조를 믿지 말고 자신을 믿어야 함
            근데 이를 바라보는 각도에 따라 솔립시즘, 나르시시즘 등 여러 '-ism'이 있음
     * Rationalists가 받는 반감이 실제보다 과도하게 느껴짐
       나름대로 생각해본 세 가지 원인
         1. 이들은 분명한 '커뮤니티'이고, 밖에 있으면 자동적으로 '아웃그룹'임
            사람들은 배제당하는 걸 싫어함
         2. 이들은 비범한 의견을 솔직히 표현함
            자신의 견해와 다른 의견을 꺼내면 싫어하는 심리가 발동함
         3. 결국 이들은 '너드(nerd)'임
            역사적으로 너드들이 왕따를 당해왔던 이유와 똑같음
          + Rationalist 커뮤니티는 절대 배타적이지 않음
            본인을 rationalist라 선언하고 블로그에 'epistemic status' 써넣고 하면 바로 합류 가능
            소속감에서 배제되는 분위기가 아니라 오히려, 이 커뮤니티의 문제는 다양성을 표방하지만 실제로는 매우 획일적으로 의견이 움직인다는 점임
            한 명이 새로운 주장을 하면 곧장 모두 사실처럼 수용하는 경향이 많음
            예로, trace lithium이 비만의 원인이라는 논거가 Astral Codex Ten 보조금도 받으면서 퍼졌지만 전문가들은 처음부터 논문 해석 오류, 통계 남용, 더 중요한 변수 무시등을 지적함
            문제는 의견이 다른 게 아니라 실제 전문성을 무시하고 '최초 원칙'으로만 접근해 상충하는 증거들을 간과하는 성향이 너무 흔함
          + 또 하나 중요한 원인은 Rationalists가 인터넷 기반 집단이라 모든 평판이 온라인에서 결정된다는 점임
            인터넷 자체가 논의가 대체로 부정적으로 흐르는 경향이 있어 이런 현상이 더 심해짐
          + 이들의 'ASI alignment가 어려울 것이다'라는 주장은 오직 사후에야 증명됨
            현재로선 '늑대가 온다'고 계속 외치면서도 증거가 불분명
            늑대가 정말 나타나면 이미 손 쓸 수도 없을 테니, 본의 아니게 신뢰를 잃고 반감을 사게 됨
          + Hacker News는 합리성에 대해 굉장히 엄격하게 판단함
            예를 들어 Mr. Beast에 관한 스레드를 보면 논쟁적인 인물임에도 상위 댓글은 모두 관대한데
            Scott Aaronson은 이론적으로 커뮤니티가 매우 높이 평가하는 인물이지만, 합리성 주제로 대화를 하면 오히려 Mr. Beast보다 덜 관대한 대응을 받음
            (Mr. Beast 관련 댓글 예시 링크)
          + ""vibe based""라는 표현을 썼는데, 사실 여러 경험적 관찰로부터 나온 결론임
     * Yudkowsky의 책에서 “인터넷에서 거의 항상 옳은 논쟁을 벌이는 양자물리학자”라는 식의 묘사를 보고, ""아 이 사람 합리주의자 커뮤니티에 딱 어울리겠다""고 느낌
          + 실제로는 이런 말투로 진짜 누가 말하면 그 자체로 가상의 인물 역할임
            그런 캐릭터가 진짜 존재한다면 더더욱 묘한 분위기
            제목도 영화 캐릭터가 카메라 보며 멋부리는 장면처럼 느껴짐
          + 솔직히 나는 야생에서 Scott Aaronson 마주치면 비슷한 리액션을 보일 듯
            그만큼 똑똑하고, 양자 컴퓨팅을 이만큼 명확히 설명하는 사람을 본 적 없음
          + 글 시작 부분에서 멈췄으면서 그에 대해 논평하는 게 무슨 의미가 있냐고 반문함
            이런 태도야말로 HN 가이드라인이 금지하는 피상적 매도 그 자체임
          + 방문 후기에서 커뮤니티 분위기가 컬트적이었다고 밝힘
            실제로 ""컬트적 느낌이 있었다""는 묘사를 봤을 때 충분히 공감함
     * Rationalist 커뮤니티와 Rust 커뮤니티 모두 HN에서 흥미롭게 취급됨
          + Rust 관련 논쟁에선 Rust가 다른 언어보다 낫다는 논의가 항상 있음
          + Rust 얘기를 하라면 오히려 이곳이 팬 찾기에 최적임
     * Rationalist와 Rationalism이 자주 혼동되므로, Rationality(특히 LessWrong식 Rationality)가 전통적 철학 Rationalism과 다르다는 점을 알아야 함
          + Rationalist 커뮤니티는 경험주의적 전통을 따르며, 명칭 자체도 이를 구분하려는 목적이 있음
          + 단어가 비슷해 이 구분이 현실에선 잘 안되는 것임
          + Lightcone, LessWrong 등에서 이 커뮤니티가 실제로 무슨 행동을 하는지는 여전히 애매하게 느껴짐
"
"https://news.hada.io/topic?id=21433","iPhone 11 QEMU 에뮬레이션 구현","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        iPhone 11 QEMU 에뮬레이션 구현

     * QEMU는 다양한 아키텍처 머신과 유저스페이스를 소프트웨어적으로 에뮬레이션함
     * 최근, QEMU 기반으로 Apple Silicon 및 iPhone 11 에뮬레이션 프로젝트가 공개됨
     * QEMU는 동적 변환을 이용한 우수한 성능과 KVM, Xen 등 하이퍼바이저 연동 기능 지원함
     * 리눅스, OS-X, 윈도우 등 여러 시스템에서 사용할 수 있고, 오픈소스 기여 프로세스가 잘 정립됨
     * 문서화 및 커뮤니티 지원이 활발하며, 버그 트래킹과 패치 제출이 효율적으로 관리됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

오픈소스 QEMU와 Apple Silicon 에뮬레이션 프로젝트 소개

   QEMU는 범용 오픈소스 머신 및 유저스페이스 에뮬레이터이자 가상화 도구임. Apple Silicon 및 iPhone 11 에뮬레이션을 위한 프로젝트가 QEMU 기반으로 진행되어 다른 가상화 솔루션에 비해 확장성, 커뮤니티 지원, 아키텍처 간 호환성 등 다양한 장점을 제공함.

QEMU 개요

     * QEMU는 전체 시스템 에뮬레이션 기능을 통해 하드웨어 가상화 없이 소프트웨어적으로 다양한 머신을 구동함
     * 동적 변환(dynamic translation) 기술을 활용해 에뮬레이션 성능을 높임
     * Xen, KVM과의 통합으로 하이퍼바이저 수준의 하드웨어 지원 가능함
     * 직접적인 CPU 에뮬레이션으로 기기 간 운영체제 이식성과 테스트 가능함 (예: ARMv7 → x86_64 환경)
     * 유저스페이스 API 가상화로 이기종 아키텍처 환경 간 바이너리 직접 실행 가능함

주요 활용 및 특징

     * 사용자가 직접 명령행 옵션을 통해 세부 설정 및 동작 방식 제어 가능함
     * oVirt, OpenStack, virt-manager, libvirt 등 고수준 오픈소스 관리 레이어와 통합 용도로도 활용됨
     * 안정적인 커맨드라인 인터페이스 및 모니터 API 제공함
     * 전체 소스코드는 GNU GPL v2 라이선스 기반 배포됨

문서화

     * 공식 문서가 웹사이트(https://www.qemu.org/documentation/)에 온라인으로 제공됨
     * 최신 개발 버전에 대한 문서는 소스 내 docs/ 폴더 및 Sphinx 툴로 생성됨

빌드 환경

     * QEMU는 최신 리눅스, OS-X, Win32(Mingw64 toolchain), 기타 UNIX 환경에서 멀티플랫폼 빌드 지원함
     * 핵심 빌드 플로우는 configure 및 make 단계로 이루어짐
     * 각 플랫폼별 빌드 방법은 QEMU 위키(https://wiki.qemu.org/Hosts/Linux 등)를 참고 가능함

패치 제출 가이드

     * QEMU 소스는 Git 버전관리 시스템으로 운영됨
     * 패치 제출 시 git format-patch 및 git send-email 사용을 권장함
     * 모든 패치는 Signed-off-by 라인이 반드시 포함되어야 하며, 개발자 스타일 가이드(https://www.qemu.org/docs/master/devel/style.html)를 따라야 함
     * git-publish 유틸리티로 반복적이고 대용량의 패치 제출 절차를 단순화 가능함
     * 패치별 버전(v1, v2) 관리와 함께 쉽게 시리즈 추적 가능함

버그 리포팅

     * GitLab 이슈트래커를 통한 공식 버그 신고 운영(https://gitlab.com/qemu-project/qemu/-/issues)
     * 패키지 기반 배포판 사용 시, 해당 OS 제조사 트래커 우선 신고 권고
     * QEMU 위키에서 버그리포트 작성법 등 추가 자료 제공

변경 이력

     * QEMU의 히스토리, 변경 내용은 공식 Wiki(https://wiki.qemu.org/ChangeLog/) 또는 Git 히스토리를 통해 확인 가능함

커뮤니티 연락처

     * 메일링리스트(qemu-devel@nongnu.org), IRC(#qemu, irc.oftc.net) 등 다양한 커뮤니티 채널 제공
     * 커뮤니티 참여, 초보자 안내 등 추가 정보는 QEMU Wiki를 통해 확인할 수 있음

결론

   QEMU는 다양한 아키텍처 및 OS 에뮬레이션, 가상화, 개발/테스트 자동화에 폭넓게 활용 가능하며, Apple Silicon(iPhone 11 등)도 QEMU 프로젝트에 포함됨. 강력한 문서화, 기여 프로세스, 활발한 버그 관리 및 커뮤니티 지원이 주요 장점임.

        Hacker News 의견

     * 관련 토론 내용으로, upstream 저장소에 관한 Hacker News 스레드 링크와 이 프로젝트를 언급한 “Emulating an iPhone in QEMU” 링크 공유
          + 이슈 트래커를 보면 그 이후로도 꽤 많은 진전이 있었던 내용 확인
     * 제대로 부팅되어서 최소한 스프링보드(Springboard)까지 올라간다는 사실, 정말 놀라운 수준의 성과라는 생각
     * 최고의 에뮬레이션 해킹, 진정한 끝판왕이라 부르고 싶음. 기여자 모두에게 축하 인사 전하고 싶음. 이 성취는 Hackintosh 프로젝트에도 긍정적인 신호. 지금은 갈 길이 멀지만, ARM PC가 널리 보급되면 효율적인 에뮬레이션을 실제로 기대해볼 만한 분위기
          + 하지만 ARM은 IBM PC만큼의 오픈 플랫폼은 아니라고 봄. 안드로이드 폰을 예시로 들면, 커스텀 리눅스 커널과 문서화되지 않은 부품이 많아 개방성과 접근성이 부족한 현실
     * trollstore와 IPAs 복호화 기능까지 지원하는지 궁금증
          + 잘 모르는 사람을 위한 질문: 이게 무슨 뜻인지 설명 요청
          + 나도 같은 질문을 하러 왔음. 만약 지원한다면 진짜 대박 기능이었을 것 같다는 소감
     * 최소한 일부라도 upstream에 적용하려는 시도가 필요하다고 생각함. 그렇지 않으면 과거의 시도들처럼 결국 사라질 수도 있다는 우려
     * iPhone 11을 QEMU로 끝까지 부팅해낸 것에 깊은 감동. ChefKissInc 팀과 여기까지 이끌어온 모든 기여자에게 찬사 보냄
     * 초보 입장에서 궁금한 점: 이 방식으로 iOS 앱을 설치할 수 있는지 여부가 알고 싶음
     * 아직 windows 환경에서 qemu 사용법, 옵션과 인자 등 공식 문서가 부실한 문제로 불편함 있음. 인터넷 곳곳에 흩어진 다양한 글을 참고하거나 리눅스 기반 예시를 끌어와 겨우 해결
          + 사실 대부분 qemu로 실험하는 사람들은 unix 계열을 사용하는 경우가 많다고 생각. windows에는 virtualbox, vmware, hyper-v, WSL 등 이미 친화적인 가상화 대안이 많기에, windows에서 qemu를 쓰는 경우는 정말 특별한 상황에서만 해당
     * 실제로 iOS 전체 에뮬레이션인지, 아니면 iOS 바이너리만 실행하는지 궁금증. 그리고 왜 하필 iPhone 11이라고 특정했는지 의문
          + 아마 iPhone 11용 바이너리이기 때문이라는 판단
     * Qemu m68k로 클래식 맥 운영체제는 어떻게 돌아가는지 질문
          + 품질이 그리 좋진 않음. Mini vMac을 추천
          + 그래도 동작은 함. Qemu m68k로 클래식 매킨토시를 실행하는 기술적 논의 및 Mac OS 7-8 실행 관련 정보는 이곳에서 참고 가능
"
"https://news.hada.io/topic?id=21442","GPU에서 선명한 텍스트 렌더링하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          GPU에서 선명한 텍스트 렌더링하기

     * 텍스트 렌더링의 품질 문제, 특히 SDF(거리장) 기반 방식의 한계를 해결하기 위해 새로운 실시간 벡터 렌더링 기법을 제안함
     * 글리프(문자)를 벡터 곡선 형태로 GPU에 직접 전송하여 실시간 래스터라이즈를 수행함으로써 무제한 해상도 및 적은 메모리 사용을 달성함
     * 텍스처 아틀라스 및 시간 누적(temporal accumulation) 기법을 활용해 높은 안티앨리어싱 품질과 효율적 캐싱을 실현함
     * 다양한 서브픽셀 구조(예: OLED, LCD 등)에도 맞춤 적용할 수 있어 프린징(색 번짐) 문제 없이 부드럽고 선명한 결과를 제공함
     * 실시간 텍스트, UI, 게임 등에서 고품질 글꼴 렌더링을 위해 단순하면서도 확장성 높은 접근법을 제시함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 텍스트 렌더링의 고민

     * 실시간 텍스트 렌더링에서는 알리아싱(계단 현상), 대형 텍스처, 빌드 시간, 확대/축소, 부드러운 이동 등 다양한 문제가 존재함
     * 기존에 많이 쓰이는 Multi-Channel Signed Distance Fields(SDFs) 방식은 품질 및 유연성 측면에서 한계가 있었음
     * 최근 OLED 모니터의 비표준 서브픽셀 구조와 프린징 문제로 계기가 되어 서브픽셀 안티앨리어싱까지 고려한 새로운 접근법을 개발하게 됨

기존 SDF 방식의 한계

  품질

     * SDF 방식의 경우 세밀한 디테일이나 얇은 획이 많은 글꼴에서는 품질 저하 및 정보 손실 문제가 발생함
     * 해상도를 높이지 않으면 특정 글리프에서 아티팩트가 남음

  아틀라스 크기

     * SDF는 처음에 오프라인으로 만든 뒤 아틀라스를 저장하는데, 많은 글리프나 CJK(중국어, 일본어, 한국어) 폰트에서는 현실적으로 불가능할 정도로 커짐
     * 여러 폰트를 동시에 사용할 경우 메모리 소모와 스트리밍 대역폭 부담이 커짐

  유연성 및 단순성

     * SDF라는 중간 단계를 거치기에 소스 데이터와 최종 결과까지 전체 흐름이 복잡해짐
     * 실시간 혹은 동적인 벡터 이미지를 바로 활용하거나 편집하는 데 제약이 큼

새 방식: 벡터 곡선 실시간 래스터라이즈

     * 텍스처를 미리 만드는 대신, 화면에 실제로 보이는 글리프의 벡터 곡선 데이터(베지에 곡선 등)를 직접 GPU로 전송하고, 그 자리에서 래스터라이즈
     * 아틀라스 텍스처에 필요한 만큼만 글리프를 넣으며, 사용 빈도에 따라 유지하거나 해제함
     * 글리프가 화면에 남아 있는 동안 표본 샘플의 누적(temporal accumulation)을 통해 매우 고품질의 좀 더 부드러운(anti-aliased) 결과를 구현함
     * 늘 벡터 기반으로 처리하기 때문에 해상도 제한 없이 선명한 결과를 제공함

글꼴 곡선 데이터 처리

     * FreeType 등 오픈소스 라이브러리로 다양한 글꼴 포맷을 읽고, 글리프의 곡선 정보를 추출
     * 글리프를 라인, 2차/3차 베지에 곡선 형태로 파싱하며, 모든 곡선을 2차 베지에 곡선으로 변환해 GPU 셰이더에서 처리 단순화
          + 라인은 중간 제어점을 추가해 2차 곡선으로 변환
          + 3차 곡선은 2개 분할된 2차 곡선으로 변환

커버리지(픽셀 내부 채움) 계산

     * 각 픽셀마다 가로 방향(ray)으로 곡선과의 교차점을 계산, winding number(누적 교차 수)를 통해 내부/외부를 판단
     * 수백 번의 표본(n회 누적 샘플) 을 합산하며, 일부 미세 오차는 최종 결과에 거의 영향 없음
     * 샘플 포인트 배치(quasirandom sequence) 기법으로, 매 프레임마다 다양한 위치에서 결괏값을 누적함

곡선 접근 최적화

     * 글리프를 수평 밴드(band) 단위로 분할, 밴드별로 관련이 있는 곡선들만 테스트하여 연산량 절감
     * 스레드 배치 및 밴드 단위 반복을 통해 GPU에서 벌크 연산 효율성을 극대화

아틀라스 패킹 및 관리

     * 아틀라스(공유 텍스처)에 각 글리프 이미지를 할당후 관리
          + 없는 글리프는 공간을 새로 할당해 래스터라이징, 이미 있는 글리프는 재사용
          + 참고로 같은 글리프라도 서브픽셀 포지션 및 크기에 따라 서로 다른 버전이 필요할 수 있음
     * Z-Order Packing(Morton code 등)을 통해 1차원 비트셋과 2D 공간 간 효율적 패킹 구현
          + 라틴 계열은 세로 기준, 아랍어 계열은 가로 기준 등 언어 구조별 유연한 응용 가능
     * 글리프가 더 이상 필요없어지면, 아틀라스 공간을 다시 할당해 사용

시간 누적(Temporal Accumulation)

     * 글리프 캐싱 및 표본 누적 방식을 통해, 표시 직후엔 빠르게 고품질 샘플 확보, 이후 프레임에서 더 정교하게 보정
          + 첫 프레임은 8샘플/픽셀, 이후 점차 샘플 수 감소, 최대 512회 누적
     * 부드러운 글리프 표시 및 리소스 최적화를 양립

서브픽셀 안티앨리어싱 및 프린징(Fringing) 방지

     * 서브픽셀(RGB 등 각 소자를 샘플로 간주) 단위로 렌더링 영역을 배분하여 수평 해상도 증가 효과 구현
          + LCD 표준 구조, OLED/WOLED 등 다양한 배열 지원
          + 프린징(색 번짐) 없는 부드러운 효과 지정
     * 서브픽셀 샘플이 중첩(Overlapping)되도록 배열 시 실제 모니터의 빛 혼합 효과까지 반영
     * 픽셀 경계나 힌팅(hinting) 없이도 자연스럽고 선명한 출력 가능

디스플레이별 서브픽셀 구조 접근의 중요성

     * 모니터의 서브픽셀 배열 정보를 프로그램적으로 확인 가능하다면 훨씬 정교한 렌더링이 가능
     * 하드웨어의 한계가 아니라 소프트웨어 처리의 문제임을 강조

마무리 및 활용 전망

     * 좋은 텍스트 렌더링이 전체 사용성 및 서비스 품질에 미치는 영향이 큼
     * 특히 UI/게임 등에서 고품질 글꼴 표현이 제품 경험에서 큰 차이를 만들 수 있음
     * 단순·확장성·고품질·유연성이라는 원칙을 실현할 수 있는 작업 구조임
     * 오픈소스 구현, 다양한 서브픽셀 대응 등 실제 산업/프로덕션 활용에 매우 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

        Hacker News 의견

     * 글 작성자인 나, 이렇게까지 글이 화제가 될 줄은 몰랐다. 흥미로운 대화에 참여해 주신 모든 분들께 감사 인사
     * 첫 영상에서 이탤릭체 ""j""의 점이 사라진 이유에 대한 질문
     * 서브픽셀 폰트 렌더링은 가독성에 매우 중요하다는 의견. 하지만 저자가 지적했듯 현재 디스플레이 표준으로는 정확한 픽셀 레이아웃 사양을 얻을 수 없어 아쉽다는 생각
     * 이건 표준 해상도 디스플레이에서만 필요한 요소라 생각. 사실 필수까지는 아니고 있으면 좋은 정도 느낌. 이제 레티나 급 디스플레이가 보편화되어 굳이 서브픽셀 렌더링이 필요하지 않은 환경이 되었단 입장. 오히려 한때 LCD 시대에 등장한 임시적 혁신이었고 이제는 시대에 뒤처지는 느낌. 스크린샷의 서브픽셀 레이아웃 의존, 비트맵 확대 불가 등 부작용이 꽤 많다. 그래서 Apple이 이 방식을 macOS에서 아예 없앤 배경이 있다 생각
     * DisplayID 같은 표준이 이런 레이아웃 정보를 제공하도록 설계되었다는 점을 지적. 제조사들이 잘 구현을 안 하거나 DB에 저장이 안 될 뿐, 인기 많은 디스플레이 모델이라면 하드웨어 정보 DB에 기록하고 활용할 수 있을 거란 입장. DisplayID 위키 참고
     * 서브픽셀 구조의 다양성을 수십 년간 알고 지냈다는 아쉬움과, 원문이 잘 정리해준 점을 평가. 그리고 ‘서브픽셀 동물원’으로 불리는 예시 페이지 공유 subpixel zoo
     * ‘비극’까지는 과한 표현이라 생각. 각 OS가 이전 Windows의 ClearType 튜너처럼 디스플레이별로 세부 조정을 할 수 있는 기능만 넣어줘도 충분하다고 봄. 모니터가 정보를 잘못 보고할 때를 대비한 사용자 설정 기억도 필요
     * 서브픽셀 렌더링이 대부분 언어에서 꼭 필요한 건 아니란 입장. 안티앨리어싱 없는 비트맵 폰트나 힌팅된 벡터 폰트만으로도 읽기 편함. 다만 한자나 일본어 같이 복잡한 문자를 사용하는 언어에서는 좀 더 중요성 있음
     * GTK4가 GPU 중심 렌더링으로 전환하며 RGB 서브픽셀 렌더링을 포기한 배경이 GPU 기술과 연관되었다는 내용. 하지만 원문에서 GPU로도 가능함을 보여준 만큼, 혹시 다른 이유나 단점, 혹은 스택 통합의 어려움이 있었던 게 아닐까 하는 추측
     * Cosmic Text(Cosmic DE)가 Swash를 통해 GPU 상에서 서브픽셀 렌더링을 지원하는 가능성 언급
     * SDF와 MSDF를 WebGL/WebGPU에서 구현하는 법에 관심 있다면 내가 직접 쓴 튜토리얼 참고 추천 튜토리얼 링크
     * Rust로 구현된 WebGPU(WGPU)에 관심이 있다고 언급. 본 튜토리얼이 고급 과정에 가깝다고 느꼈고, JS 예제를 Rust로 바꿔보며 배우는 게 효과적이었다는 경험 공유
     * 사이트 포맷이 매우 마음에 들어 나도 GPU 관련 튜토리얼을 이렇게 만들고 싶다 밝힘. 혹시 이게 특정 템플릿인지, 코스 일부인지 궁금
     * Slug 라이브러리는 GPU 글리프 래스터라이저를 구현한 상용 미들웨어라는 정보 공유 Slug Library
     * Slug 웹사이트에서 꽤 자세히 알고리즘을 공개하고 있어 흥미롭다는 생각. 혹시 특허가 있는지 궁금. Cosmic-text로 폰트 파싱/레이아웃 활용해 오픈소스 wgpu 버전 만들면 재밌겠지만 나중에 Slug에 소송당하면 곤란할 거라는 우려
     * 이 분야에 익숙하지 않은 사람들을 위해 SDF 텍스트 렌더링의 히스토리와 현주소를 요약. Valve가 2007년 SDF 기반 아키텍처를 선보였고, 이후 2012년 Glyphy(비대드 에스파보드)가 GPU 기반 SDF 구현을 내놓으면서 변화가 있었지만 주류 OS나 웹브라우저는 여전히 1990년대식 Truetype 방식에 머무름. 이 방식은 작고 가벼우나, 서브픽셀 정렬/아비트러리 레이아웃을 지원하지 않고, 텍스트 확대나 3D 변형에도 큰 제약이 있다. 이런 기술 진화가 더딘 것은 위험 대비 이득이 크지 않고, 렌더링뿐만 아니라 줄 바꿈 등 복잡한 레이아웃 처리까지 GPU/CPU 협업이 힘들기 때문일 것 같다는 생각
     * 줄 바꿈 등 텍스트 레이아웃 작업은 실제로 렌더링과 거의 별개라는 지적
     * Servo의 Pathfinder는 GPU 컴퓨트 셰이더를 이용해 훨씬 더 나은 성능의 GPU 텍스트 렌더링을 이미 구현한 예시라는 소개
     * 웹킷의 GPU 텍스트 렌더링 방식 관련 기사 링크. 현재 단계에서도 문자열~비트맵까지 어느 정도 GPU 상에서 처리가 가능하나, 기대하는 ‘서브픽셀 안티앨리어싱’이 GPU 프로모션 시엔 빠진다는 지적
     * 실제로 Windows뿐만 아니라 Chrome/Firefox에서도 이미 수년 전부터 서브픽셀 안티앨리어싱까지 GPU 가속이 있었다는 사실 언급. 최신 기술이 안 쓰인다는 건 오해라는 점 강조
     * 간결한 개요를 잘 정리해줘서 고맙다는 코멘트
     * 서브픽셀 렌더링을 원한다면, 디스플레이의 서브픽셀 그리드를 정확히 파악해야 한다는 전제. 모니터마다 별도 설정 요청이 유일한 합리적 UX라는 의견. OS가 화면 회전 등까지 처리해야 함
     * 저자 의견처럼 디스플레이가 시스템에 자신의 서브픽셀 구조를 직접 알려주는 방식이 가장 이상적이라는 생각
     * 뛰어난 결과라는 평가와 함께, 서브픽셀 안티앨리어싱은 2000년대 초 LCD 시절 명확한 장점이 있었지만 고해상도 레티나 디스플레이 시대엔 무의미에 가깝다고 판단. 단점으로 불투명 배경만 적용 가능, 후처리(리사이즈, 미러링, 블러 등) 불가, 스크린샷이 다른 기기에서 어색하게 보이는 점 등을 꼽음
     * 서브픽셀 AA를 없애면 단순화는 되지만, 아직 저해상도 96dpi, 1366x768 디스플레이 기반의 데스크탑 사용자도 많다는 데이터(파이어폭스 하드웨어 서베이 자료) 제시
     * 고해상도 화면 사용자로서 저해상도 사용자까지 고려하지 않는 건 무책임하다는 의견도 덧붙임
     * 서브픽셀 레이아웃 프로토콜이 도입되어도 일부 제조사가 잘못 구현해, 일반 사용자가 이해하기 어려운 렌더링 문제가 생길 수 있다는 우려
     * 손글씨체(커서체)를 보고 든 첫 생각이 “대체 이딴 커서체를 누가 좋은 아이디어라고 생각했을까”라는 솔직한 감상
     * 손글씨, 특히 깃펜이나 만년필을 사용하던 시대에 손글씨 쓰던 사람들이 좋아했을 거라는 설명
     * 편지를 많이 쓰던 사람들이 커서체를 사용했으며, 인터넷과 장거리 무료 통화가 등장하며 커서체 사용이 줄어들었다는 해설
     * 코드 링크를 찾을 수 없다는 질문
"
"https://news.hada.io/topic?id=21486","데이비드 아텐버러 99세: '나는 이 이야기가 어떻게 끝나는지 보지 못할 것임'","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              데이비드 아텐버러 99세: '나는 이 이야기가 어떻게 끝나는지 보지 못할 것임'

     * 데이비드 아텐버러가 99세 생일을 맞이함
     * 그는 자연 다큐멘터리 분야에서 지대한 영향력을 보임
     * 아텐버러는 인류가 마주한 기후 변화 문제에 대해 우려를 표함
     * 그는 자신이 이야기의 결말을 보지 못할 것임을 언급함
     * 세대를 넘어서는 환경 보호 메시지가 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

데이비드 아텐버러 99세: ‘나는 이 이야기가 어떻게 끝나는지 보지 못할 것임’

  아텐버러의 생애와 영향력

     * 데이비드 아텐버러는 99세라는 나이에 자연 다큐멘터리 제작자로서 대중과 오랜 기간 소통해 옴
     * 그의 작업은 자연의 아름다움과 생태계 파괴의 현실을 널리 알리는 데 기여함
     * 지구상 다양한 생명체와 자연 환경 보전에 대한 이해와 경외심을 대중에게 전파함

  기후 변화에 대한 우려와 메시지

     * 아텐버러는 인간이 지구에 남기는 영향에 대해 깊은 우려를 표함
     * 자신은 인류가 환경 문제를 어떻게 해결하는지 직접 보지 못하겠지만, 후손들에게 자연 보호의 중요성을 강조함
     * 기후 변화, 생태계 위기 등 시급한 환경 문제에 대해 경종을 울리는 메시지를 전달함

  미래 세대를 위한 약속

     * 아텐버러는 인간의 행동 변화를 통해 자연 보존이 가능함을 강조함
     * 다음 세대가 더 나은 미래를 위해 노력해야 함을 역설함
     * 그의 메시지는 환경 보호에 대한 공동 책임을 촉구함

  결론

     * 데이비드 아텐버러의 인생과 작품은 환경 보전의 중요성을 일깨우는 강력한 메시지임
     * 그의 영향력은 앞으로도 오랫동안 후대에 영감을 주는 기반이 됨

        Hacker News 의견

     * 아카이브 링크
     * 나는 최근 David Attenborough의 다큐멘터리 'Ocean'을 대형 스크린으로 관람한 경험자임. 저인망 어업(bottom trawling) 장면이 정말 충격적이었던 인상. 이게 영국 연안에서 계속 허용된다는 사실은 이해가 어렵고, 해양 보호구역에서 보조금까지 지급되는 현실은 미친 상황. 이는 마치 사슴 몇 마리 잡자고 숲을 네이팜으로 태우는 격이라는 생각. 다행히 변화의 조짐이 있음. 관련 정책 제안 링크를 참고. 이 변화가 다큐 때문에 일어났는지는 확신이 없음
          + Greenpeace가 2021~2022년 경 저인망 어업을 막으려고 바다에 바위를 투하하던 시절이 있었음. 지금도 그렇게 하는지는 확실치 않음. 핵심은, 원한다면 적당한 위치에 화강암이나 콘크리트 덩어리를 던지면 저인망 방지 효과를 만들 수 있음. 금지도 좋지만, 실제로 파괴적인 힘을 행사하는 쪽이 더 잘 통한다는 입장. 인간의 이기심은 바꾸기 어렵기 때문에 시스템 설계 자체가 그 점을 감안해야 함. 그린피스 브라이튼 어업 바위 투하 현장와 저인망 방지 인공 암초의 생물학적 효과 논문 링크 참고
          + 관련 영상 발췌 링크를 남김 YouTube 링크
          + 해양 보호구역에서 저인망이 허용된다는 게 어떻게 가능한지 놀람. 그런 지역이 보호받는 첫 이유가 저인망 아니냐는 의문. EU는 2030년까지 완전 금지를 목표로 했으나 진척은 없고, 그래도 보호구역 내에서는 저인망 금지를 유지 중. 각국(예: 이탈리아)은 연안과 얕은 수역에서는 금지하는 국가 법도 있는 상황. EU 법원, 저인망 보호 판결 및 EU 저층어업 금지 관련 논란 기사 참고
          + 어제 이 영화를 봤는데, 아름답고 충격적인 장면이 동시에 인상적이었음. 대형 저인망 어업의 해양 환경 파괴가 얼마나 심각한지 피부로 느껴지는 다큐멘터리였음. 마치 과일만 얻으려고 정원을 불도저로 밀어버리는 느낌
          + 생선을 먹지 말자는 주장. 어업 산업이 바다를 파괴하는 중임
     * 나는 48세이고, 갈수록 변화하는 업계 속에서 젊은 사람들과 어울릴 때 자신이 '나이 많다'는 느낌을 많이 받음. 그래서 자기보다 오래 살아온, 그리고 여전히 왕성하게 활동하는 이들의 시각이 남다르고 소중하게 느껴짐
     * 개인적으로는 이 상황의 결말을 보지 못한다는 사실이 한편으로는 '축복' 같다는 생각을 함. 나는 Attenborough처럼 우리가 문제를 해결할 거라는 낙관에는 동의하지 않음. 결국 'Line Must Go Up(성장만이 답이다)' 세력이 이길 것이고, 모두가 급속히 실패하기 전까진 멈추지 않을 것이라는 비관적 관점
          + 그의 ‘문제를 우리가 해결할 거라는 낙관’을, 평생 자연을 지키는 과학 커뮤니케이션을 실천한 Attenborough의 전략적 마무리로 해석함. 사람들은 도망치고 싶은 부정적 미래보다는, 달려가고 싶은 긍정적인 비전을 더 잘 받아들인다는 사실을 그도 잘 아는 듯함. 마지막 인사마저 부드럽고 희망적임. 마치 재앙을 겪는 자녀를 토닥이며 미래를 맡기는 부모 같은 전설적 태도라는 생각
          + 조지 칼린의 명언을 빌려, “행성 자체는 괜찮고, 죽어가는 건 인간임. 지구는 45억 년 되었는데 우리는 겨우 10~20만 년 존재했고, 본격적인 산업 활동은 200년 정도밖에 안 됨. 인류가 지구를 위협할 존재라고 믿는 교만이 더 놀라움”이라는 관점 소개
          + 내 이론상 ‘Line Must Go Up’ 세력은 앞으로도 계속 이길 것이라는 생각. 기후변화 영향 완화도 결국 ‘Line Goes Up’(성장) 논리에 포함될 전망. 미리 방지하는 것보다 훗날 대처하는 게 비싸든 싸든, 결국 맞닥뜨리고 해결하게 될 거라는 입장. 네덜란드가 해일에 적응한 사례와 같이 인간은 문제를 맞이하면 결국 해결하며 살아남는 존재임
          + 동의함. 가끔 세상을 보면 내 어머니가 돌아가신 것도 복일 수 있었겠다는 생각. 지금 현실을 즐기지 못한 건 차라리 다행이지 않을까 싶음. 그리고 그녀가 2차 대전 직후 미국에서 태어난 덕에, 근현대사에서 가장 좋은 시기를 살았을지 모른다는 회고도 듦
          + 테넷에서 “터지지 않은 폭탄은 아무도 신경 쓰지 않는다”라는 대사가 생각남. 재앙 예방에 보상이 없는 사회임. 성장만이 답이라 믿는 분위기에서는 남이 문제를 해결해주길 바라는 이기심만 커지고, 대다수가 그렇게 생각하면 결국 모두가 무너지는 구조임
     * 너무 많은 팝업을 닫아야 해서 90년대로 회귀한 듯한 느낌
          + Firefox에 광고 차단 기능 키면 광고 없는 쾌적함을 경험
     * 글을 읽기 시작하자마자 David Attenborough의 목소리가 자동으로 떠오르는 경험
     * 인간 존재의 가장 슬픈 점 중 하나는 아무도 우리의 이야기 끝을 보지 못한다는 사실. 우리는 임의의 시점에 ‘스폰’되고 또 임의의 시점에 ‘소멸’함. 노화 분야 연구가 활발하고, 과학 발전 덕에 인간 수명도 40세에서 80세까지 늘어남. 하지만 우주의 스케일 변화(종 진화, 대륙 이동, 베텔게우스 폭발 등)를 직접 관찰하려면 평균 수명이 최소 5만 년은 되어야 함
          + 우리는 죽고 육체는 사라짐. 하지만 NDE(근사체험), 유체이탈 경험 등 존재한다면 이야기를 끝까지 보는 다른 경로가 있을 수도 있다는 생각. 우리의 현존이 진짜 이야기가 아닌, 뭔가를 배우고 성장하는 ‘기회’일 수도 있다는 관점 강조. 많은 이들이 물질적 현실만이 전부라 가정하지만, 실제로는 아닐 수도 있음
     * 아무도 자신의 스토리 종말을 목격하지 못하는 인간의 숙명
          + 자신의 작업 결말을 확인하지 못함에 슬픔이 있지만, 사실 우리 모두는 결과를 모른 채 살아감. 자연 환경 관점에서 인간이 관찰한 건 눈 깜짝할 순간에 불과
          + The Sundays(밴드)의 곡에서 의견이 다름을 주장
          + 어떤 이야기를 말하느냐에 따라 다름. 모든 죽음은 어떤 이의 세계 끝이라는 점에 공감
          + 누군가는 그 종말을 볼 수도 있음. 몇 세기 내에 스스로 멸종을 선택할 가능성을 봄. 전체 이야기의 끝은 아니어도 인류의 종말이라는 측면에서는 마지막에 불을 끄고 나가는 사람이 있을 거라는 예감
          + 지난 1만 년간의 인류 세대는, 이야기의 ‘쇠락’을 목격 중임. 식량이 풍부하던 시절에는 가족이 매일 뛰고 밤마다 매머드를 잡아 구워 먹는 시절이 있었음. 지금 우리는 문명의 정점에서 스크린에 갇혀 냉동 TV 디너를 먹으며, 집주인 뒷바라지를 위해 열심히 일하는 고립된 존재가 되어버림
     * David Attenborough가 진행한 고생물학 다큐멘터리는 내 인생 최고급이라는 애정
          + David Attenborough를 가장 높게 평가하는 이유는 대본을 읽는 느낌이 없고, 자신이 정말 아는 이야기를 자연스럽게 들려주는 전문가의 진짜 목소리가 느껴진다는 점. 배우나 해설자가 이런 태도를 보일 때 쓰는 전문 용어가 있는지 궁금함
          + <Prehistoric Planet> 봤냐고 묻고, CG 기술이 너무 훌륭해 따로 말할 필요 없이 동물의 냄새까지 느껴질 수준임 설명. Prehistoric Planet 위키피디아 링크
     * 우리는 우리 이야기의 결말을 보기 위해 존재하는 게 아니라, 누군가의 엔딩이었던 세상을 살아보고 경험하기 위해 존재함이라는 생각
          + 이런 생각 정말 아름답다는 공감
"
"https://news.hada.io/topic?id=21456","스크래치에서 직접 구현한 BitTorrent 클라이언트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     스크래치에서 직접 구현한 BitTorrent 클라이언트

     * 이 오픈소스 프로젝트는 Go 언어로 구현된 BitTorrent 클라이언트로, 파일 다운로드 기본 로직을 자체 구현함
     * Bencode 인코딩/디코딩을 직접 처리하며, 강력한 오류 검증 기능을 포함함
     * .torrent 파일 파싱, 정보 해시 계산, 피어 간 통신 등 핵심 기능을 포괄적으로 지원함
     * 동시 다운로드 및 파일 조립, 블록 단위 저장 관리 등 실제 활용도를 높이는 기능을 포함함
     * 기존 BitTorrent 오픈소스와 비교해 Go 언어의 심플함, 코드 구조의 명확성, 모듈화에서 강점 보유함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   이 프로젝트는 Go 언어로 직접 BitTorrent 클라이언트를 구현함
   BitTorrent 프로토콜을 사용한 파일 다운로드 기능을 자체 개발 형태로 제공함
   핵심적으로, 토렌트 파일 파싱, 피어 발견, 파일 다운로드 기능을 다룸

주요 기능

     * Bencode 인코딩/디코딩
          + 문자열, 정수, 리스트, 딕셔너리 등 모든 Bencode 타입 지원
          + 강력한 에러 처리 및 데이터 검증 적용
     * 토렌트 파일 처리
          + 단일 및 멀티 파일 토렌트 모두 파싱 가능
          + Info 해시 및 각 피스 해시 추출, 모든 표준 필드 지원
     * 피어 발견 및 통신
          + HTTP 트래커 지원
          + 피어 간 핸드셰이크 프로토콜 구현
          + BitTorrent 메시지 프로토콜 및 피어 연결 관리 수행
     * 다운로드 기능
          + 피스 및 블록 단위 관리
          + 동시 다운로드 처리
          + 다운로드 진행률 추적 및 파일 조립
          + 블록 단위 스토리지 관리로 효율성 제공

프로젝트 구조

     * cmd/ : 커맨드라인 인터페이스 및 실행 파일
     * internal/
          + bencode/ : Bencode 인코딩 및 디코딩 기능
          + torrent/ : 토렌트 파일 파싱 및 처리
          + tracker/ : 트래커 프로토콜 구현
          + peer/ : 피어 간 통신 기능
          + download/ : 다운로드 관리 기능
     * pkg/ : 외부에 노출 가능한 패키지 모음

요구 사항

     * Go 1.21 이상 필요

사용법

     * 현재는 프로젝트 초기 개발 단계로, 사용법 안내는 추후 추가 예정임

개발 현황 및 계획

     * 현재 개발 활발히 진행 중
     * 자세한 개발 단계는 checkpoint.md 파일에 기록
     * 향후 계획:
          + Magnet 링크 지원
          + 메타데이터 교환 프로토콜
          + DHT(분산 해시 테이블) 지원 예정

참고 문서

     * BitTorrent 프로토콜 명세
     * Bencode 명세

프로젝트 중요성 및 강점

     * 이 프로젝트는 Go 언어 특유의 단순한 문법과 병렬성을 활용해 BitTorrent 클라이언트의 복잡한 요소를 명확히 모듈화하여 구현함
     * 구조가 명확해 확장성·유지보수성 측면에서 학습 및 실전 적용 모두에 이점 제공함
     * 비교적 초기 단계임에도 핵심 BitTorrent 프로토콜 기능을 빠르게 실현하였음

        Hacker News 의견

     * bencode 디코더에서 동적 메모리 할당 크기를 제한해주는 것이 좋겠다는 제안 전달. 토렌트 파일이나 announce에서 들어오는 입력값은 신뢰할 수 없기 때문에 악의적인 입력이 매우 큰 할당 요청을 하여 서비스 거부(DoS)를 일으킬 수 있음. 스트링 파싱에서 적절한 상한선은 남은 입력 길이로 설정할 수 있으며, 제대로 된 토렌트는 파일의 남은 길이보다 긴 문자열을 가질 수 없음
          + 이 지적을 할 일 목록에 추가함
     * 프로젝트가 깔끔하고 심플해 보여서 좋음. 사용 예시를 Readme에 원라이너로 추가하면 좋을 것 같음. 예시로 ./go-torrent My-Linux-Distro-Wink-ISO.torrent 같이 사용법을 보여주는 문장 추가 추천. 만약 torrent.ParseFromUrl 기능도 추가한다면 더 좋은 점수. 모든 사람이 자기만의 “영적 여정”을 위해 이런 경험을 해볼 가치가 있음
          + 제안을 고맙게 생각함
     * codecrafters에서 제공하는 비슷한 챌린지 소개. 이 과정에서는 프로세스 진행과 테스트 등을 도와주고 무료 한달 동안 해봤는데 꽤 재미있었음
       https://app.codecrafters.io/courses/bittorrent/overview
     * Go 개발자가 아닌 입장에서 왜 예전 Go 1.21 버전을 사용하는지 궁금함. 특별히 구버전을 고집하는 이유가 있는지 질문. 찾아보니 10개월 전에 이미 지원 중단됨
          + Windows 7 지원 때문임. Go 1.21.4 이하로 작성한 프로젝트는 2009년 이후의 거의 모든 Windows와 모든 컴퓨터에서 동작하지만, 1.21.5 이상 사용 시에는 최신 컴퓨터와 Windows 10, 11에서만 동작하며 특별한 이점이 없음
            https://github.com/golang/go/issues/64622
          + README는 AI가 작성했을 가능성이 높음. 실제 go.mod 파일에는 Go 버전을 1.23.1로 지정하고 있어서, 결과적으로 1.23.1 이상 필요
            https://github.com/piyushgupta53/go-torrent-client/… https://go.dev/doc/modules/gomod-ref#go-notes
     * 정말 멋진 프로젝트임. 대학 시절 조지아텍 네트워킹 수업에서 이걸 최종 과제로 했었는데, 코드는 잃어버렸지만 배운 교훈은 평생 남음. 이런 프로젝트는 새로운 언어를 익히기에 참 좋은 방법임
     * 마그넷 링크를 지원하는지 질문함.
       Edit: 앞으로 추가될 기능임을 알게 됨
          + 아직 지원은 안 하지만 곧 추가할 예정임
     * 이런 거 어떻게 만들었는지 궁금함. 직접 프로토콜 사양을 봤는지, 아니면 다른 구현을 참고했는지 질문. 이런 걸 맨땅에서 어떻게 구현하는지 항상 궁금함
          + 흥미롭게도 최근 Go로 직접 Bittorrent 클라이언트를 만들기 시작함. 개인적으로 AI/LLM을 코딩에 쓰는 걸 선호하지 않고, 직접 진짜로 배우는 것이 목적임. 제일 먼저 한 일은 Bittorrent 프로토콜의 공식 스펙을 찾아보는 것. 사실 스펙은 생각보다 간단하지만 좀 빈약함
            https://www.bittorrent.org/beps/bep_0003.html
            추가 확장들은
            https://www.bittorrent.org/beps/bep_0000.html
            작업을 아주 작은 단위로 쪼개서 직접 결과를 확인해나가는 게 중요함. 예를 들어 .torrent 파일 파싱부터 시작했는데, bencoding을 직접 구현해야 했음. Arch Linux용 .torrent를 다운받아보니 잘못된 포맷이었고, url-list 같은 예상치 못한 키가 나오더라. 이게 뭔지 조사해보니까 bep_0019 관련이었음. 최종적으로 Debian Linux .torrent 파일을 성공적으로 파싱함
            그 후 트래커 announce HTTP 요청과 피어 프로토콜도 구현함. 피어 프로토콜은 꽤 어려웠고, 실험적 태도가 큰 도움이 됨. 데비안 토렌트에서 announce URL을 제거해 peers가 아예 없도록 설정한 다음, KTorrent에서 내 클라이언트를 직접 추가해서 주고받는 메시지를 관찰하고, 그 결과를 바탕으로 내 코드도 수정함. 굉장히 많은 시행착오와 디버깅 과정이 있었음
            일부 세부 프로토콜은 도저히 공식 문서에서 안 보여서 소소하게 ChatGPT에 물어본 적도 있고, 클라이언트마다 구현이 조금씩 달라 자세한 알고리즘들은 명확하지 않음. 어떤 블록을 받을지, 어떤 피어와 연결할지, choke/unchoke는 어떻게 동작하는지 등은 제대로 정리되어 있지 않음. 웹 검색이 큰 도움이 됨
            추가로 https://wiki.theory.org/Main_Page 사이트에도 도움 되는 정보가 있음
            지금은 KTorrent와 완전한 다운로드/업로드가 가능한 단계임. 앞으로는 트래커에서 피어를 얻고, 다운로드할 블록을 선정해서 파일에 저장하는 알고리즘 개발이 남음
            더 세부적인 과정이 궁금하면 언제든 질문 바람
     * GUI 추가는 얼마나 어려운지 질문. Go에서 GUI 구현 사례가 별로 본 적 없음
          + 여러 GUI 프로젝트가 있음 https://github.com/go-graphics/go-gui-projects
            개인적으로는 ImGui 래퍼인 https://github.com/AllenDang/giu 선호
            가장 기능이 많은 것은 unison이지만 실제로 널리 쓰는지는 잘 모르겠고, 문서가 매우 부족할 것 같음 https://github.com/richardwilkes/unison
            Gio는 새로운 방식의 GUI 프레임워크고, Tailscale과 gotraceui에서 사용됨 https://gioui.org
            Wails는 웹 개발 경험이 있다면 익히기 쉬움 https://wails.io
            GTK4 바인딩도 괜찮아 보임 https://github.com/diamondburned/gotk4
            Cogent Core도 재밌어 보이지만, 나는 Go 대신 Odin 언어로 넘어가기 전 잠깐만 써봄 https://www.cogentcore.org/core
            Fyne은 개인적으로 여러 컴퓨터와 OS에서 성능 때문에 문제가 많았지만, 어쨌든 가장 유명한 GUI 프레임워크임 https://fyne.io
     * 이런 프로젝트에 관심 있어서 한 번 해볼까 생각했었음. 얼마나 어렵고, 얼마나 완성도가 높다고 생각하는지, DHT, Magnet, NAT traversal 같은 복잡한 기능까지 구현했는지 궁금함. 실제로 시중에 거의 모든 토렌트를 커버하려면 꼭 필요한 기능 목록이 뭔지조차 잘 모르겠음. 토렌트 관련 프로토콜이 워낙 많아서 전체 리스트도 모르겠고, 각 프로토콜이 뭘 하는지조차 파악이 안됨
          + 어려운 정도는 경험, 언어 숙련도, 실험적 태도에 따라 크게 다름. 예시로 본인도 지난 주 Go로 Bittorrent 클라이언트 만들기 시작했는데 일주일 만에 여기에 올라온 프로젝트의 80% 수준까지 옴. Go와 프로토콜, 네트워킹 지식이 많고 실험하는 게 습관이 돼 있어서 빠르게 진척됨
            Bittorrent 공식 사양 자체는 정말 짧아서 한 시간 정도면 읽고 이해할 수 있음 https://www.bittorrent.org/beps/bep_0003.html
            하지만 주변 확장 프로토콜이 워낙 많고, 각 클라이언트마다 지원 수준이 다름. 예를 들어 프로토콜 암호화(실제로는 난독화) 시도를 먼저 하고 안 되면 일반 프로토콜로 진행하는 경우가 많음
            만약 리눅스 배포판처럼 공식 .torrent 파일로 다운로드만 하고 싶으면 난이도는 확실히 낮음. 주로 파일 하나, 트래커도 있고, 대부분 표준 프로토콜 쓰는 피어만 있으니 NAT 환경에서도 내부 포트 오픈 없이 충분히 동작
            물론 일반적인(특히 회색지대) 토렌트를 원한다면 magnet 링크 파싱, 피어 발견 위한 DHT, 자동 포트 매핑 위한 UPnP까지 점점 추가해야 함. 하지만 이 역시 기능별로 하나하나 단계를 나눠가면서 구현 가능. 기능이 많을수록 더 많은 피어를 찾고 성공적으로 교환할 수 있음
          + 꽤 도전적이었고, 프로토콜이나 bencoding 방식 익히고, 전체 구조를 머릿속에 그린 다음 코드를 작성하는 데 거의 한 달 걸림. Magnet과 DHT는 아직 지원 안 함
     * v2 및 mutable torrents가 구현됐는지 질문. 반드시 mutable torrents를 구현해줬으면 하는 바람 전달
"
"https://news.hada.io/topic?id=21501","Show GN: 국민연금 대시보드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: 국민연금 대시보드

   국민연금공단에서 제공하는 가입현황과 수급현황으로 연령/성별/지역별 국민연금 납입/수급 현황을 보여주는 대시보드를 만들어보았습니다. 자신과 비슷한 사람의 가입/수급 현황 및 심층 분석, 최고 연금 수령자 등의 정보를 제공합니다.

   지금 들어가 보는데요, ""Page not found"" 나오네요ㅠㅠ

   내 국민연금은 찾을 수 없구나... 젠장

   내 국민연금이 not found라니! 404라니!!
"
"https://news.hada.io/topic?id=21549","SpaceX Starship 36 이상 현상","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        SpaceX Starship 36 이상 현상

     * SpaceX의 Starship 36이 정적 연소 시험 직전 폭발 상황 발생
     * 해당 사건은 SpaceX Masseys에서 실시간으로 목격됨
     * 화재 진압을 위해 소방서가 출동하는 긴급 상황에 직면
     * 이번 이상 현상은 테스트 일정 및 향후 개발 과정에 영향 가능성 존재
     * 실시간 영상이 유튜브와 X(트위터) 를 통해 공개
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

SpaceX Starship 36 이상 현상 요약

     * SpaceX Starship 36이 정적 점화(Static Fire) 시험 직전 SpaceX Masseys 시설에서 예기치 않은 폭발 현상 발생
     * 이 이상 현상은 라이브 스트리밍(유튜브와 X)을 통해 전 세계 관람자들에게 실시간으로 전파됨
     * 폭발 발생 직후 소방서가 즉시 출동하는 위급 상황 전개
     * 이 사건은 개발 일정, 향후 시험 계획, 안전 프로토콜 등 SpaceX의 개발 프로세스 전반에 영향 가능성 내포
     * SpaceX는 원인 조사와 안전 확보를 위한 조치 진행 필요성이 대두됨

        Hacker News 의견

     * SpaceX의 성공과 실패를 지켜보는 일이 정말 흥미진진한 경험이라는 생각 공유, 최근 SpaceX가 겪는 문제의 일부는 팀원들이 미션에 대한 열정을 서서히 잃고 있기 때문일 수도 있다는 가능성 제기, 예전엔 SpaceX에 꼭 합류하고 싶었는데 이제는 아무리 많은 돈을 줘도 동기부여가 안 생기는 심정 고백, 핵심 인재들이 이 조직을 세계를 바꾸는 기회로 받아들이지 않고, 단순한 직장으로만 바라보기 시작하면 혁신적인 빠른 개발 주기도 오히려 '멍청하다, 최소 시간만 채우고 월급 받자'는 태도로 변질될 위험성 언급
          + ""사람들이 미션에 대한 열정을 잃어서가 아니라, 미션에 열정을 가진 사람들이 빠져나가고 있기 때문""이라는 대안적인 관점 제시
          + 일론 머스크가 점점 빠른 속도와 더 많은 성과를 요구하는 압박에 따라 SpaceX 엔지니어들의 체력과 의지가 소모되고 있다고 생각, 아무리 강력한 팀이라도 속도에 너무 집중하다 보면 언젠가는 힘든 고비를 맞이함을 지적
          + SpaceX가 정말 어려운 일에 도전하고 있고 목표 수준도 너무 높게 잡았다는 점 강조, 불확실한 엔지니어링 한계점에 도전하다 보면 실패는 당연한데, 어딘가에서 ""Starship은 반드시 실현된다""는 분위기가 너무 자연스럽게 만들어진 것에 대한 의문 제기, 성공한다고 해도 힘든 여정일 것이고, 그동안은 SpaceX가 운도 좋았으니 평균으로 회귀하는 것뿐이라는 견해
          + 로켓 개발자라면 기본적인 전문성과 자기계발 동기는 갖춰져 있다고 믿고, 비전이 부족하다고 성과가 저하될 인재들이 아니라고 봄, 그들은 그냥 회사를 떠나 더 나은 곳에서 활약할 뿐이라는 판단
          + 이런 현상은 구글에서도 일어났던 일이라는 간결한 언급
     * 이번 SpaceX 사고 상황이 소련 N1 프로그램과 규모, 테스트 방식, 잦은 고장 측면에서 비슷하다는 유사점 지적, 당시 Korolyov도 달 착륙 목표에 쫓겨 실제 비행 단계에서 모든 것을 조립 및 테스트하려 했고, 결국 4번의 실패 후 프로그램이 중단된 역사 언급, R7에서는 효과적이었지만 대형 로켓에서는 부분별 테스트가 안 되면 문제가 쉽게 발생한다는 지적
          + 두 프로그램에 분명 유사점이 있지만 여러 측면에서 다르다고 설명, N1은 Glushko의 반대로 엔진 선택에 제한이 많았고 NK-15 엔진처럼 그 당시엔 너무 많고 신뢰도가 떨어지는 엔진이 필요했던 한계 지적, Super Heavy와 Starship은 단계별로 별도 테스트가 가능한 반면 N1은 불가능했었고, 테스트 중 하나가 실패하면 발사대까지 파괴될 정도였으나 지금의 SpaceX는 별도 부품 단위로 실험할 수 있다는 점 강조
          + 로켓 크기가 커질수록 비례적으로 더 큰 안전 마진을 두기 쉽다는 로켓 공학적 스케일링 법칙에 주목, 하지만 머스크가 모든 단계를 재사용 가능하도록 집착한 결과 이런 여분 안전마진이 줄었을 거라는 추론, 초기에는 1단 부스터만 재사용 가능한 전략으로 더 많은 부분 개발을 병렬로 진행하는 전략이 나았을 수도 있었을 거라는 개인 의견 제시
          + 지금은 통계적 고장 분석 역량과 컴퓨팅 파워 덕분에 무작위로 실험하지 않고 있다는 점 강조, 엔진 테스트, 압력 테스트, 정지 연소, 센서 기반 데이터 수집용 플라이트 등 다양한 시험이 실제로 존재하며, 하드웨어도 발사 속도보다 빠르게 제작되고 있다는 점 들어 SpaceX와 N1은 본질적으로 다르다는 견해 피력
          + 두 로켓이 시대 최대 크기라는 점 외에는 공통점이 없고, 정부 운영 vs 민간(부분적으로 정부 지원), 단발성 vs 완전 재사용, 달 vs 화성, 전통 개발 vs 반복적 하드웨어 중심 개발 등 모든 면에서 다르다고 주장, N1 실패 원인은 Korolev 본인의 실수보다 그의 사망에 더 큰 책임이 있을지도 모른다는 역사적 해석 제시
          + SpaceX는 N1과 달리 시험을 굉장히 자주 진행하며, N1의 경우 지상 테스트가 불가한 엔진이 있어서 전체 스택을 한 번에 쏴야 했음을 설명, Starship v2에서 페이로드를 더욱 늘리려다 각종 한계에 봉착한 것이 원인일 수 있다고 진단, 문제가 엔진(Raptor v2)보다는 연료 공급 배관에서 발생했다고 봄
     * 고화질 슬로우모션 영상 링크 [https://x.com/dwisecinema/status/1935552171912655045]를 공유
          + 이 영상을 보면 명확하게 연료 탱크 중 하나가 과압으로 터진 모습이라는 지적
          + 유튜브에서는 [.]와 [,] 키로 일시정지 상태에서 프레임 단위로 한 장씩 넘길 수 있다는 팁 공유
          + SpaceX 팀의 라이브 스트림 링크 [https://youtu.be/WKwWclAKYa0?t=6989] 공개
     * Starship의 잦은 문제를 지켜보면서 Saturn V와 STS 프로그램(스페이스 셔틀)이 얼마나 인상적인 성취였는지 새삼 느낀다는 견해, rocket equation(로켓 방정식) 특성상 한 대형 로켓으로 대량 탑재물을 보내려면 크기가 기하급수적으로 커지고, 오히려 여러 대 중소형 로켓이 더 효율적인 듯하다는 생각, 소유즈, 아틀라스, 아리안, Falcon 9 등이 그 좋은 예라고 판단
          + 대형 로켓이 오히려 rocket equation 효과를 완화하는 구조임을 설명, 연료량 대비 건조질량의 비가 크기临계점 이상이 되면 오히려 더 많은 페이로드를 실을 수 있다는 수학적 이유 제시
          + 더욱 놀라운 것은 Saturn V가 1969년 기술로 단일 발사에 성공한 반면, 이제는 10~15차례 Starship 발사 및 SLS까지 동원해야 Apollo 때의 임무를 구현할 수 있다는 점, 1958년 미국의 첫 인공위성 발사 후 8년 만에 달에 간 것도 감탄, 웹 개발만 힘든 게 아니라 로켓 개발도 갈수록 복잡하고 방대해지는 현상 지적
          + Starship 대형 페이로드의 본질적 목적은 ""화성 점령""이라는 야심에서 비롯됨을 강조, 참고 기사 [https://in.mashable.com/science/85790/…] 링크 첨부
          + STS(스페이스 셔틀)는 비상 대피 모드가 부실하고 발사 때마다 열 차폐판에 반복적 손상을 입혔던 위험한 시스템이라는 평가, '정상적인 편차의 일상화(normalization of deviance)' 사례로 해석, 단 두 번만 폭발한 게 오히려 행운이라는 통찰과 함께 관련 칼럼 [https://danluu.com/wat/] 공유
          + 대형 로켓의 논리는 운영 비용 관점에서 접근한 SpaceX/Musk의 전략이라는 설명, 크기가 커질수록 단위 페이로드 대비 비용을 줄일 수 있다는 시각
     * SpaceX가 메탄 기반의 풀플로우 스테이지드 컴버스천 엔진을 개발하면서 어려움을 겪는 점을 흥미롭게 봄, 소련의 사례로부터 이런 엔진이 극한 난이도임을 알았지만 최근까지 잘 되는 듯해서 기대가 컸었음, 하지만 점점 SpaceX 특유의 빠르게 반복하고 실패하면서 학습하는 문화의 한계에 다다른 느낌
          + 아직 Raptor 엔진이 문제라는 증거는 부족하다는 의견, 정지 연소(Static fire) 직전 상황이 아니었던 만큼 엔진 외 원인에 무게를 둠, SpaceX의 실험 방식은 언제나 짜릿함이라는 평가
          + SpaceX subreddit에서는 핵심 엔지니어들이 리더십 및 조직 문화 문제로 계속 떠나고 있다는 소문이 오가고 있다는 정보 전달, 최근 유난히 실패가 잦다는 점은 의심스럽게 느끼나, 해당 소문 신빙성은 불명확하다고 판단
          + 위의 고화질 슬로우모션 영상 [https://x.com/dwisecinema/status/1935552171912655045]에서 문제의 원인이 거의 확실히 압력 탱크 결함이라는 근거 제시, 엔진 자체 문제에서 비롯된 게 아님을 암시
          + Starship 테스트 중 v1은 유망했지만 v2로 넘어가면서 심각한 문제가 급격히 늘었다는 소회, 하드웨어 중심 개발 접근은 좋은데 지나치게 빠른 진전 혹은 과도한 손질이 오히려 해가 된 듯한 인상
          + 엔진 자체보다는 다양한 자세 변화에도 연료를 잘 공급해야 하는 배관 시스템(plumbing) 쪽에서 그동안 문제가 많았던 현실 진단
     * 폭발이 예정된 테스트 직전이 아닌, 아예 테스트 시작도 못 하고 사전 폭발이 일어난 점이 매우 불길하고 심각하게 느껴진다는 관점 제시, 테스트 과정에서의 실패는 이해 가능하지만 시작도 전에 시스템 전체가 망가지는 건 위험 신호라는 주장
          + 연료를 주입할 때부터 이미 대형 폭발 위험이 시작되는 상황으로, 엔진을 점화하는 순간만이 리스크 구간이 아니라는 점 강조, 전기화재나 구조적 결함 등 그 이전에도 위험 존재
          + 유사한 일은 과거 Falcon 9에서도 발생한 적 있음을 상기
     * 몇 년 전 점심시간에 시끄럽게 대화하던 SpaceX 엔지니어들을 우연히 듣게 된 경험 공유, 회사 미션이나 열정이 아니라 틱톡 ""일상 영상"" 팔로워 늘리기, 돈 자랑, 라스베가스에서 초과속 운전 등 사생활 중심 대화에 충격 받았다는 소회, 기업의 직원들이 더 이상 일에 대한 자부심이나 미션보다 자기 과시와 사생활에 집중하는 모습은 빨간 신호등으로 느껴졌고, 이런 직원 태도가 결국 최근 사건들과 무관치 않다는 해석
          + ""이건 그저 들은 소문에 불과하다""는 냉소와 함께, ""엘론 머스크가 싫다고 인간의 모든 진보를 폄하할 순 없다""는 비판적 입장 피력
     * 전체 선체가 손실된 사고이긴 하나, 부상자는 없는 점에서 실제로 SpaceX에게 중대 타격인지, 아니면 개발 과정에서 본격적으로 한계까지 밀어붙이며 배우는 일상적인 셋백인지 궁금함과 함께 ""얼마나 심각한지""를 알고 싶다는 궁금증
          + 일반적인 프로젝트라면 꽤 큰 사건일 것이고, 진상 규명 및 후속 조치까지 상당한 리소스가 소모될 텐데, SpaceX 엔지니어링 문화 안에서는 결과를 예상하기 어렵다는 태도 제시
          + 실제로는 대형 장애이고 현장 복구와 재정비로 향후 발사에 큰 지연이 발생할 전망이라는 견해, 엔진 점화도 안 한 상태에서 이런 치명적 결함이 발생한 건 중대한 설계 결함의 신호로 해석
          + 파드(Pad) 수리 기간이 주요 이슈라서 상대적으로 작은 셋백이라는 분석, Starship은 여전히 개발 중이고 폭발도 흔하게 일어나는 편이라는 맥락 전달, 아모스-6(AMOS-6) 사고처럼 정치적 분위기가 겹치면 더 큰 사건으로 번질 수도 있다고 우려, AMOS-6는 정지 연소 전에 폭발해 정지 연소 시 화물 없이 실험하는 관행을 만들었지만, Starship은 아직 화물이 없었고, 이번에도 인과관계가 비교적 빠르게 밝혀질 수 있을 것으로 전망
          + 개발 선체 하나를 잃는 정도로는 치명적이지 않고, 오히려 지상 설비 복구에 시간이 더 걸릴 수 있다고 진단, 다음 실험 전 원인 파악이 필수이긴 하지만 큰 셋백은 아니라고 예측
          + 테스트 자체 도달도 못 하고 망가진 점에서 위험 수위가 높다고 보고, 테스트가 아니라 준비 단계에서 사고가 나는 건 더욱 위험하다는 평가
     * 지속 가능한 실패율이 아니라고 진단, 비용 문제로 SpaceX가 자금 유치를 위해 상장할 수도 있는데, 상장하면 성공에 대한 책임감이 증대될 것임을 전망, 기술적 성공은 상당히 인상적이고 Falcon 프로그램의 성공률도 증명되어 있지만, Starship 한 대 스택당 약 1억 달러라는 추정 비용이 궁금하다는 질문
          + 굳이 상장 필요성은 낮고, 머스크는 필요할 때마다 사적으로 수십억 달러를 조달해왔다고 봄, Starlink와 Falcon 9 덕분에 기업 전반적으로는 현금흐름도 좋고 수익성도 충분하다고 진단, 현재 R&D를 위한 투자 유치도 강력한 실적을 전제로 가능하다고 분석, 상장 시 테슬라 초기처럼 전체 수익성이 보장되지 않는 초창기 불확실성 반복 가능성 언급
          + 한 번 전체 스타십 세트당 1억 달러라는 비용 추정치에 동의, 그러나 SLS는 발사 한 번에 40억 달러가 소요되므로 상대적으로 starship의 시도당 실패가 훨씬 저렴하고 지속 가능하다고 평가, 올해 첫 명백한 실패이며, 이전 실험은 불완전한 성공도 있지만 단계별 재사용성 입증이 의미 있었다고 설명, 앞으로 열댓 번 실패해도 SLS보다 더 저렴하다는 긍정적 분석
     * SpaceX가 왜 3단 로켓 대신 2단 재사용 구조에만 집착하는지 이해되지 않는다는 의문, 완전 재사용을 추구하느라 열 차폐판, 연료 마진 등 큰 무게 페널티를 안게 됐고, 다단 분리형이면 페이로드도 더 나을 텐데 전략적 오류로 봄, 결국 버전이 올라갈수록 점점 더 큰 연료 탱크와 작아진 페이로드로 이어진다고 지적
          + Mars 착륙 및 귀환 임무나 대형 페이로드 수송을 위해서는 3단이 오히려 불리하며, 3단 구조는 지구 정지궤도(GEO) 같은 일회성 임무나 소형 페이로드엔 적합하고, Starship은 그런 요구에 맞는 로켓이 아니라는 견해
          + 완전하고 빠른 재사용이 궁극적 목표이고, 로켓 발사를 항공기처럼 자주 반복하는 일이 이루어질 때 산업 혁신 가능성 제시, Falcon 9의 시장점유율만 봐도 재사용의 파괴력이 크고, Starship이 성공하면 판도를 바꿀 거라고 확신
          + 문제점 중 하나는 2단이 지구 반대편에 착륙하게 되어 복귀가 어렵다는 점, 하지만 이론적으로는 바다 수준의 엔진을 탑재하면 연료 보충 후 다시 비행도 가능하다는 아이디어 공유
          + 좋은 비추력(specific impulse)과 괜찮은 질량비(mass ratio)를 확보할 수 있다면 LEO(저궤도) 수송엔 2단이 최적이며, 단계수가 늘어나면 무게 증가, 시스템 복잡성, 실패 요소도 많아진다는 로켓 공학적 통찰
          + 3단 로켓을 재사용하려면 2단에도 열차폐판이 필요해지고, 이로 인해 상단 크기 및 페이로드가 현저히 감소해서 불리하다고 설명
"
"https://news.hada.io/topic?id=21482","Show GN: slimfont - webfont subset 편집툴","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Show GN: slimfont - webfont subset 편집툴

   안녕하세요 frontend 업무를 하면서 web font의 원하는 글자들만 subset으로 만드는 과정이 불편하다 보니 사이트로 직접 만들어 보았습니다.

   웹폰트 파일을 업로드하고 원하는 subset의 템플릿을 선택하거나 직접 입력한다음,
   ttf, woff로 추출할 수 있는 기능을 제공하고 있습니다.

   추가했으면 좋을 템플릿, 버그 등 개선사항 환영합니다!

   좋네요 감사합니다.
"
"https://news.hada.io/topic?id=21492","Oxlint 1.0 릴리즈","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Oxlint 1.0 릴리즈

     * Rust 기반으로 개발된 JavaScript/TypeScript 린트 도구 Oxlint가 1.0 버전으로 정식 출시됨
     * ESLint 대비 50~100배 빠른 성능, 500개 이상의 ESLint 규칙 지원, 대기업(Shopify, Airbnb, Mercedes-Benz 등) 실제 적용 사례 보유
     * 설정 없이 즉시 사용 가능하며, 기존 ESLint 설정과의 이식성·동시 사용도 지원
     * 주요 에디터 통합(VSCode, IntelliJ, Zed 등) 및 LSP 제공, 구체적·시각화된 에러 진단 메시지
     * 커스텀 규칙, 성능 최적화, 세분화된 설정 등 향후 로드맵도 공개
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Oxlint 1.0 주요 내용

  성능 및 대규모 적용

     * Rust로 구현되어 동시 실행 시 수만 개 파일을 초고속으로 린트
     * 예시: Airbnb의 12만 6천개 파일 린트 7초, Mercedes-Benz는 71~97% 속도 향상
     * 최대 규모 리포지토리에서 22.5초만에 26만 5천개 파일/101개 규칙 분석

  즉시 사용 & 쉬운 도입

     * 설치만 하면 설정 필요 없이 즉시 실행
          + npx oxlint@latest 또는 pnpm/yarn/bun/deno로 곧바로 사용
     * 대형 프로젝트 및 팀 환경을 위한 .oxlintrc.json 커스텀 설정 지원
          + ESLint v8 flat config 포맷 기반, 익숙한 방식
          + oxlint-migrate로 기존 ESLint 설정 이관, eslint-plugin-oxlint로 중복 규칙 비활성화 가능
     * 권장 방식: oxlint와 eslint를 함께 돌려 더 빠른 피드백 확보

  광범위한 규칙 지원

     * ESLint 전체 규칙 및 typescript-eslint, unicorn, jsdoc, react, jest, import 플러그인 규칙 다수 지원
     * 고유 규칙도 포함: bad comparison sequence, const comparisons 등

  유연한 설정

     * 폴더별 중첩 구성, glob별 오버라이드, 공유 설정 확장 지원
     * 팀별 일관된 코드 품질 기준 유지 가능

  에디터 통합

     * VSCode, IntelliJ/WebStorm, Zed 등 확장 제공 및 LSP 지원
     * 다양한 IDE에서 즉각적인 오류 진단과 수정 제안

  진단 메시지

     * 단순 오류 설명을 넘어 시각화된 원인 및 해결 방법 안내

  벤치마크

        도구       실행시간
   oxlint(멀티스레드) 615ms
   oxlint(싱글스레드) 1.8초
   eslint        33.5초

  로드맵

     * JavaScript 기반 커스텀 규칙 지원 예정
     * 지속적인 성능 최적화, ESLint v9 방식의 세밀한 glob별 설정 등 추가 예정

   rust는 퍼포먼스 개선 치트키입니까

   아직 vue나 nestjs는 완벽지원하지는 않나보네요.

   https://github.com/oxc-project/oxc/issues/481

   Oxlint - ESLint보다 50~100배 빠른 JS 린터
"
"https://news.hada.io/topic?id=21466","Rust에서 상호작용형 Datalog 엔진 만들기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Rust에서 상호작용형 Datalog 엔진 만들기

     * Datalog 논리 프로그래밍과 Rust의 효율성을 결합하여, 심플하면서도 사용성이 높고, 성능까지 고려한 상호작용형 Datalog 엔진 개발 과정을 상세하게 공유함
     * 직관적 CLI 환경에서 규칙(rule)과 사실(fact)을 실시간으로 추가/확장 가능, 대량 데이터 적재, 동적 규칙 입력 및 빠른 쿼리 성능까지 경험할 수 있음
     * 파싱(Parsing), 데이터 표현(Representation), 규칙 평가(Planning/Evaluation) 를 단계별로 Rust 코드와 함께 설명해 실제 구현 방법을 익힐 수 있음
     * 최적화되지 않은 단순 구현부터 시작해 점진적으로 성능과 구조를 개선하는 과정을 통해, 데이터 병렬처리/조인 최적화 등 고급 로직도 학습할 수 있음
     * 대규모 데이터셋 기반 프로그램 분석(Nullability, Aliasing 등) 사례를 실제로 실행하며 성능 및 메모리 이슈, 쿼리 최적화, join-plan 개선 노하우를 공유함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

도입: Rust에서 Datalog 논리 프로그래밍 실험

     * Memorial Day 기간, Minnowbrook 컨퍼런스에서 다양한 논리 프로그래밍(Datalog 등) 실습과 토론 진행
     * 기존 Datalog 도구들(Soufflé, ctdal 등)은 실제 사용/확장성/성능 측면에서 한계 발견, 실용적 도구 필요성 부각
     * 필자는 직접 Rust로 심플/사용성/성능을 모두 만족하는 Datalog 인터프리터(datatoad)를 구현하기로 결정
     * 프로젝트 목표: CLI에서 대용량 데이터를 빠르게 적재하고, 동적으로 규칙 추가/수정하며, 실시간 결과 확인 및 쿼리 성능도 확보

Datalog 기본 개념

     * Datalog는 규칙(Head :- Body) 형태의 논리문을 기반으로, 주어진 fact와 rule로부터 모든 도출 가능한 사실을 자동 유도
     * 규칙(예: tri(a, b, c) :- edge(a, b), edge(b, c), edge(a, c))은 변수/리터럴로 이루어짐
     * fact는 조건 없는 참인 값(예: edge(1, 2) :- .)
     * Datalog의 강점: 규칙 추가 시 추론 가능한 정보 집합이 늘어나고(단조성), 규칙/사실 순서에 상관없이 동일 결과를 도출(수렴성)
     * Rust로는 규칙과 fact를 Atom/Rule/Term 구조체로 표현, relation 별로 fact 집합을 관리함

핵심 구조 설계

  데이터 표현

     * Fact는 Vec<String>으로, fact 집합은 BTreeMap<String, Vec<Fact>> 등으로 초기 설계
     * 대용량 데이터 최적화를 위해 columnar(칼럼 지향) 데이터 구조 도입(alloc overhead 최소화)
     * FactContainer: 정렬/중복제거된 fact 집합, append only 구조
     * FactLSM: FactContainer를 여러 계층으로 관리하는 LSM(Log-structured merge-tree) 방식, 삽입 및 정렬/병합 효율화
     * FactSet: fact의 lifecycle(새로 추가, 최근 도출, 안정화된 fact)을 관리하여 중복 계산, 불필요한 메모리 낭비 방지

  규칙 적용과 추론

     * 각 규칙은 JoinPlan(조인 플랜) 생성 → 적절한 컬럼 순서/키 조합을 기반으로 merge join 방식으로 추론
     * merge join: body atom별로 key 컬럼 정렬 후, 조인키가 일치하는 경우에만 새로운 fact 도출(성능 극대화)
     * FactSet의 stable/recent/to_add 구조를 활용해, 이미 도출된 fact와 신규 fact를 분리하여 불필요한 재계산 방지(차분 평가)
     * .update() 루프: 신규 fact 도출이 멈출 때까지 모든 규칙 반복 적용, fixpoint 도달 시까지 추론 반복

  파서 구현

     * Soufflé 스타일 문법(?var, :-, ., , 등) 지원, Rust로 직접 토크나이저/파서 작성
     * 오류 발생 시 안전하게 None 반환, 실험적 환경에 맞는 심플 파서 설계

성능 최적화와 실제 분석 예시

  Nullability 분석(Reachability)

     * 대용량 데이터셋(예: httpd_df)에서 값 복사/이동 경로 추적을 위한 Datalog 규칙 정의 및 성능 측정
     * 다양한 규칙 작성 패턴에 따라 성능 차이 극심(변수 바인딩/컬럼 순서/조인 플랜에 따른 temp relation 발생 등)
     * 데이터 초기 형태/조인 전략에 따라 실행 시간, 메모리 사용량이 수십 배 차이 발생, 쿼리 최적화의 중요성 직접 체험
     * 최적화 적용 시 기존 C++ 기반 도구(Graspan 등) 대비 10~80배 이상 성능 개선 확인

  Aliasing 분석(Points-to)

     * aliasing/포인터 추적 분석을 위한 복잡한 Datalog 패턴 구현, 논문(Graspan, Zheng-Rugina 등)과 동일한 쿼리 실행
     * Datalog 규칙 내 반복(^*), 옵션(^?), 전치(^T) 연산을 명시적 재귀/union으로 확장
     * 중간 결과(relation alias, temp join 등)의 네이밍/재사용 설계에 따라, 전체 쿼리 플랜의 효율 및 자원 소모 대폭 차이
     * 쿼리 플랜 최적화 없이 큰 중간 결과를 생성하면, 성능 저하와 메모리 폭증을 초래(예: V relation)
     * 필요한 결과만 산출하는 ""demand-driven"" 방식(매직셋 변환) 논의, 실제 쿼리 플랜 변형 및 성능 개선 가능성 제시

Rust로 직접 실험하며 얻은 교훈

     * Datalog 엔진 성능의 핵심은 데이터 표현(칼럼ar/LSM), 차분 추론, join 플랜 최적화에 있음
     * 단순히 규칙을 기계적으로 작성하면, 불필요한 중간 데이터 생성과 리소스 낭비로 이어짐(최적화 필요)
     * Rust로 직접 실험하며, 실전 데이터셋에서 수백만/수천만 row의 fact를 효율적으로 관리하고, 확장성과 추론 속도를 동시에 달성할 수 있음
     * CLI 환경에서 대규모 데이터 적재, 실시간 규칙 추가, 결과 확인까지 손쉽게 실험 가능
     * query optimizer의 역할, bushy-tree join(중간 결과 활용), 필요 없는 관계의 생성을 피하는 습관 등, 실제 Datalog 작성 및 운영에 큰 시사점

앞으로의 확장 과제

     * 디스크 스필, 다중 워커/프로세스 분산 확장, 스트리밍 조인, 커스텀 컴파일 최적화 등 연구 과제 남아 있음
     * 대규모 프로그램 분석, 그래프/관계 추론, 정적분석, 데이터 흐름 추적 등 실전 분야에서 Rust Datalog의 활용 가능성 높음

        Hacker News 의견

     * 지금 이 글이 상위에 떠 있는 게 재밌는 상황 경험 중 설명 Differential Datalog(이 저장소)과 Rust를 활용해서 실시간 전략 게임 만드는 작업 진행 중 DDL이 게임 로직을 담당하는 구조이고, 새로운 아이디어를 배워보고 여러 삽질 경험을 쌓으려는 의도
          + 진행 상황 기대 중인 입장 설명, 결과가 궁금해지는 상황
          + Differential Datalog 개발이 비활성화된 상황이라서, 그 구현이 어디까지 가능한지, 지금 어느 정도 완성되어 있는지 궁금한 상태
     * mangle datalog을 Rust로 포팅하려고 조금 진전 이뤄낸 경험 공유 Rust 구현은 여기에서 확인 가능 golang 버전과 같은 저장소에 존재 작업이 더딘 이유는 우선순위가 낮은 것도 있지만, 'second system syndrome' 현상(처음 만든 것보다 두 번째 만들 때 욕심이 많아져 작업이 느려지는 현상) 때문임 Rust 버전의 mangle은 메모리 매핑을 통해 언제든지 디스크에서 데이터를 가져오고 쓸 수 있어 대용량 데이터 처리 가능 golang 버전은 전체 데이터를 메모리에 올려 처리하는 구조임 이 글이 좋은 점은 datalog 파서 구성이 잘 되어있고, LSM 트리 언급이 있어 이해가 잘 된다는 점, data frog에 비해 훨씬 따라가기 쉽다는 점 Rust에는 proc-macros를 활용한 ascent, crepe 등 다양한 datalog 구현이 존재하지만, 실행 중 쿼리를 동적으로 처리하는 데는 제약이 있음 쿼리와 프로그램이 고정된 정적 분석
       케이스라면 proc macro 접근법도 충분히 훌륭하다고 생각하는 입장
     * datalog 열정가들 일부가 꾸준히 모여 활동하는 모습이 보기 좋은 상황 최근 datalog 르네상스가 주춤하는 분위기인데, Datalog 2.0 컨퍼런스도 예전에 비해 소규모였고, HYTRADBOI 컨퍼런스 2회차에서는 datalog 관련 논의 자체가 크게 줄었음 첫 번째 대회에서는 논문의 1/4이 datalog 관련이었던 기억 다른 참가자들이 최근 datalog 프로젝트 경험을 나눠주는 게 큰 힘이 됨 요즘 구식 SQL 데이터베이스에서 대규모 소프트웨어 마이그레이션을 준비하며 데이터 품질 파이프라인을 만들고 있는 중 datalog 쿼리는 잘 구조화하면 가독성이 높아서, 데이터 품질 문제 탐색에 SQL보다 훨씬 효율적인 도구라고 생각
          + Datalog 2.0 컨퍼런스 참석 인원 적은 게 datalog 자체의 침체라기보다는 행사 구조 문제의 영향이 크다는 의견 Datalog 2.0은 LPNMR이라는 덜 알려진 유럽 컨퍼런스의 위성 행사였고, 이번에는 미국 Dallas에서 랜덤하게 진행됨 본인도 Datalog 2.0에 논문을 제출했으나, 핵심 저자는 따로 있었고, 실제로 datalog 분야 종사자도 행사에 별로 참여하지 않았음 Nemo solver를 발표한 유럽 쪽 참가자가 눈에 띄던 상황 요지는 올해 Datalog 2.0 참석자 수 적었던 것은 행사 자체의 특수성과 위성 행사였던 점이 더 크고, datalog 엔진 구현 자체에 대해 큰 혁신은 남아있지 않다는 관점에서는 동의 실제 연구 흐름은 기본 datalog보다는 stream 기반(HydroFlow), choice(Dusa), chase engine(Egglog) 등 더 흥미로운 주제로 발전 monotonic, chain-forward saturation(Horn clauses)가 엔지니어링적으로 충분히 정립된
            방법이라서, 이 위에 semirings, Z-sets 등 새로운 이론 탐구가 이루어지고 있는 상황 설명
     * 필자가 datalog 관련 작업을 잘 한다고 생각하지만, 도입부에서 binary join을 가르치는 방식은 이상적 상황이 아니면 금방 복잡해져서 아쉽다는 입장 generic join 계열 방식이 더 직관적으로 일반화 가능하다는 생각 worst-case optimal join algorithm에 대한 wiki 설명 참고 권장
          + 관련 이야기로, McSherry의 이전 블로그 글에서는 binary join을 적절한 쿼리 플랜 조정으로 worst-case 최적화 런타임 달성 가능함을 증명한 과정 확인 가능 블로그 포스트 참고 안내
     * 해당 블로그 포스트 오프닝에서 ""I, a notorious villain, was invited for what I was half sure was my long-due comeuppance.""라는 문장이 가장 인상적이었던 올해 최고의 기술 블로그 글 오프닝이라는 생각 글 내내 작가의 유쾌한 서술이 돋보였고, 이렇게 깊이 있는 기술적 주제를 재미있게 읽을 수 있는 글은 드물다는 생각 aliasing 쿼리 최적화 여정을 탐정 소설처럼 흥미롭게 풀어낸 구성 50GB 메모리 사용에 좌절하고, 5GB로 최적화 성공했을 때 같이 응원하게 되는 느낌 코드와 글 솜씨 모두 훌륭하다고 평가
     * 예전에 몇몇 Clojure 팬이 datalog가 SQL보다 더 우수하다고 믿었고, 관계형 DB들이 전부 SQL만 지원하는 게 아쉽다고 말하던 경험 들은 적 있음 왜 그렇게 생각하는지는 직접 파고들어보진 못함
          + Clojure나 Datomic의 datalog 방언은 나에게는 익숙하지 않지만, datalog 자체에는 공감하는 입장 온라인 notebook 환경에서 datalog를 활용해볼 수 있는 Percival 추천 percival.ink 활용, datalog의 핵심 개념만 익히면 구현마다 손쉽게 전환 가능 Percival을 fork해서 datalog를 SQLite로 컴파일하는 버전도 만든 경험 있음, 포크 버전은 아직 집계 함수나 고급 조인은 미완성 상태이지만 기본 쿼리는 잘 동작함 Logica는 Google 연구원이 만든 훨씬 진지하고 완성도 높은 datalog->SQL 컴파일러로, BigTable, DuckDB 등 다양한 SQL 방언 지원 Logica 참고 재귀 쿼리/규칙을 다룰 때 datalog가 월등히 쉽다는 점이 인상적 SQL로는 가능은 하지만 마치 점토를 빨대로 마시는 느낌 Frank McSherry가 만드는 Materialize.com은 “WITH MUTUALLY RECURSIVE” SQL 형태를 제공하며, Materialize 블로그 참고 중 Notion의 페이지 로드 쿼리와
            데이터 동기화에 활용 검토 중 Feldera도 재귀 뷰를 위한 유사한 폼이 있음 Feldera 블로그 포스트 참고 Feldera의 장점은 각 “규칙” 혹은 서브뷰를 독립된 statement로 분리해 작성 가능, 반면 한 문장에 모두 담을 필요 없음 단점은 SQL 문법이 Apache Calcite에서 온 제약이 많다는 점 Materialize SQL은 PostgreSQL 호환성에 노력 중인 인상
     * 얼마 전까지 VMware가 differential datalog에서 멀어졌다는 이야기 본 것 기억 McSharry의 새로운 글이 반가운 상황
          + Differential Datalog 팀이 Feldera라는 회사를 설립했고, Feldera 웹사이트에서 확인 가능 differential datalog에서 differential SQL로 전환하게 된 이유는 datalog가 시장에서 실제로 채택되기 어렵다는 점 때문이라고 추정
     * datalog와 Rust를 함께 쓰고 싶다면 cozodb 추천 cozodb는 Rust로 작성되어 datalog 쿼리 문법 지원
          + cozodb도 흥미로운 프로젝트지만 비활성화된 프로젝트로 보임 2024년 11월쯤에 소스 코드를 살펴보면서 sqlite 저장 백엔드에 간단하게 개선할 수 있는 포인트(이슈 #285)를 발견한 경험 있음
     * Hacker News에 1일 전에 올라온 관련 글 링크 제공 게시글 바로가기
"
"https://news.hada.io/topic?id=21478","아폴로 "8-볼" FDAI(비행 지시 및 자세 표시계) 내부","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   아폴로 ""8-볼"" FDAI(비행 지시 및 자세 표시계) 내부

     * 아폴로 임무에서 중요하게 사용된 'FDAI' 는 우주선의 자세와 방향을 시각적으로 표시하는 핵심 기기임
     * 이 기기는 3축 회전(롤, 피치, 요) 을 모두 표현하며, 본체 내부의 메커니즘과 겉의 반구형 셸이 결합되어 작동함
     * 내부는 슬립링, 동기기(싱크로), 서보루프 등 정밀한 전기·기계 구조로 구성되어 정확한 위치 제어와 피드백 구현함
     * Lear Siegler 등 항공계 선구자들의 혁신을 바탕으로, X-15, F-4, Gemini, Apollo, Space Shuttle에 걸쳐 발전함
     * 기사에서 분석한 FDAI는 아폴로용 출발이지만 스페이스 셔틀 시뮬레이터에 맞게 여러 부품과 회로가 개조된 이력 지님
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

아폴로 FDAI(비행 지시 및 자세 표시계)란

     * 아폴로 임무에서 우주인의 우주선 자세 관측용으로 활용된 FDAI는 고유의 8-볼 형태 회전 기구를 보유함
     * 중앙 구체(일명 8-볼)은 한 면이 검은색이며, 비행 방향(자세)을 3축 동작으로 시각화함
     * 세 개의 노란색 바늘은 현재의 자세 지시 외에 목표 기동 방향(지침)까지 안내해, 우주비행사가 신속한 자세 교정을 할 수 있게 도움
     * FDAI는 자세 속도(회전률) 등 추가 정보도 표시함

FDAI의 기계적 구조 및 동작 원리

  3축 회전 구현 방식

     * 구체는 롤, 피치, 요 세 축을 기준으로 회전함
          + 롤: 장치 외부 프레임의 모터와 톱니바퀴로 좌우로 돌림
          + 피치: 구 내부 모터로 수직축을 따라 기울임
          + 요: 반구 셸만이 수직축을 따라 독립적으로 회전하며, 내부 기구는 고정됨
     * 두 겹의 슬립링(전기 접점 고리)으로 다축 회전에도 내부 배선이 엉키지 않게 전기적 연결 유지

  동기기(synchro)와 서보루프(feedback) 제어

     * 동기기(싱크로)는 입력 축과 출력 축의 회전 각도 변환 신호를 3선 통신으로 전달함
          + 두 동기기 간 각도 차가 생기면 토크가 발생, 자동으로 일치 방향으로 회전
     * 서보루프 회로는 동기기, 제어 변압기, 증폭기, 모터로 구성
          + 제어 변압기가 목표 각도-실제 각도 차이(오차 신호)를 증폭 및 모터에 전달
          + 타코미터(회전 속도 검출기) 가 음의 피드백 신호 제공, 오류 감소 속도에 따라 감속·정밀 제어 구현

  증폭기 회로 및 전자 부품 구성

     * 3축 각각 별도 서보루프/증폭기/제어 변압기를 지님
     * 회로 기판은 공간 절약과 내진동 내구성을 위해 부품을 적층 배치, 일부 리드선은 플라스틱 튜브로 보호
     * 증폭기는 오차 신호 크기와 방향을 감지해 모터 구동, 회전 방향을 정교하게 결정함

FDAI의 역사와 발전

  개발·진화 배경

     * Bill Lear(1902–1978) 등이 이끄는 Lear Avionics/Lear Siegler가
          + F-102 전투기, X-15 로켓기, F-4 전투기 등의 자세 표시 장치 개발
          + 이후 Gemini, Apollo에 FDAI로 발전, 아폴로 LM(달 착륙선)에 심장부 계기배치
     * 1970년대 Lear Siegler는 우주선 임무 수익성 문제로 ADI(스페이스 셔틀용) 생산 후 해당 분야 철수
     * Honeywell이 이후 Shuttle용 계기(MEDS 등) 생산 주도

  유사 기기의 구조 비교

     * 기존 ARU/11-A 계기와 FDAI 구조는 유사하나, 전자회로 내장·파워보드 형태 등 차이 있음
     * 기존 비행기 특화 피치 트림 등 기능은 우주비행에서 의미 적어 제거됨
     * 내부 반구 셸 홀 고정방식도 약간 변경됨

분석 대상 FDAI와 아폴로, 셔틀 간 주요 차이

     * 대상 FDAI는 원래 아폴로용 제작이나 스페이스 셔틀 시뮬레이터용으로 개조됨
          + 입력 신호 방식(싱크로 ↔ 리졸버), 조명 시스템(전구 ↔ 전자발광), 내부 구조 등 달라짐
          + 바늘 설계, 조정 기능, 표시 방법 등에서 Shuttle 구성에 맞게 도색·회로 변경 흔적 다수
     * 셔틀의 ADI는 오프 표시, 입력 신호 확인, 피드백 서보 시스템 등 추가 전자회로로 더욱 복잡
          + 통합회로 및 다중 파워 회로 채택, 바늘 위치 정밀도 향상
          + ADI 내부 구체 회전 방식은 비슷할 것으로 추정

결론

     * FDAI는 아폴로 미션에서 우주선 자세/기동 정보 제공의 핵심 계기
     * 정교한 2+1축 회전 메커니즘 및 서보 피드백 기법을 활용, 높은 정확성과 신뢰도 제공
     * FDAI 계보는 항공기–로켓기–유인 우주선–셔틀까지 이어지며, 각 시대별 기술 혁신을 내포함
     * 분석 대상 FDAI는 아폴로-셔틀 이행기의 과도기형 개체로서 우주비행 계기 진화상의 희귀 사례임

        Hacker News 의견

     * Apollo 관련 질문이 있으면 저자가 직접 답변 가능
          + 정말 좋은 글이라는 생각, 기존에는 우주선 ADI에 세 번째 축이 있다는 것 자체를 생각해본 적이 없음, 아쉽게도 한 가지 정확하지 않은 점은 Bill Lear의 F-5 자동조종장치는 내가 아는 한 Northrop F-5 전투기와는 별개임
          + Apollo 사령선에는 Honeywell에서 제작한 완전히 다른 FDAI(비행 지시자 자세 지시계) 사용, 이처럼 다른 부품을 사용해야 했던 구체적인 요구 사항이 있었는지 아니면 Grumman/North American에서 서로 다른 공급업체를 채택했기 때문인지 궁금
          + F-104 비행기에서도 비슷한 사례가 기억에 남아 있음
          + 아폴로 13 영화에서 이 장치를 'frappin 8 ball'로 언급해서 뚜렷하게 기억에 남음
     * 작년에 HN에 비슷한 소련 시대 장치에 관한 글이 있었음, 그 장치는 지구상에서 우주선의 위치를 나타내는 작은 지구본 형태였음
          + 소련의 Globus 장치는 몇 가지 면에서 비슷하지만, 큰 차이점도 존재, 말씀하신 것처럼 볼이 우주선의 자세가 아니라 지구 위의 위치를 보여주기 때문에 그 자체가 대륙이 그려진 지구본 같은 모습, 이 볼은 세 축이 아니라 두 축을 따라 회전, 그리고 Globus에는 외부 신호 입력이 없고, 미리 설정된 경로에 따라 움직이기 때문에 실제 위치와는 무관하게 돌아감, 내가 쓴 Globus 관련 3개 글의 HN 토론 모음 링크:
            첫 번째
            두 번째
            세 번째
     * 이번 글에 정말 감탄, Apollo를 위해 개발된 놀라운 기술에 관한 이야기는 많이 접하지만 이 글은 그중에서도 한 가지를 깊게 풀어주는 설명, 지난 수십 년간 아웃소싱이 늘어나면서 이런 기술과 기본적인 엔지니어링 및 제조 역량이 사라지고 있다는 점을 걱정
     * 과거에 이 주제는 아마도 전기공학 아날로그 제어 수업의 훌륭한 숙제감
     * 이건 정말 UI의 예술작품이라는 생각, 한 번만 봐도 내 우주선의 방향이 바로 파악 가능, 아마추어 우주비행사(케르발 스페이스 프로그램 1,000시간, Flight of Nova 200+시간 경험) 입장에서 신형 퓨전 우주선 조종석에서 Apollo 스타일 계기판 중 KSP의 Nav-Ball이 그립다는 생각, 전투기 스타일 'ladder' 자세계는 한눈에 읽어지지 않고, 사다리 숫자를 확인하고 다시 나침반으로 확인해야 해서 3초 정도 집중 필요(실제 조종이 아닌 체감 시간), 반면 Nav-Ball은 0.5초 내에 입력 가능(잠재적으로 이미 뇌가 익힘), 그 3초가 중요한 게 실제로 Apollo 11은 달 착륙 직전 연료가 20초도 남지 않음
     * 최근 Freya Holmér의 강연에 이 내용이 다뤄짐, 해당 발표 영상 링크
       YouTube 영상
     * Ken이 또 한 번 Hacker News 최고의 콘텐츠 저자임을 입증
     * 이 내용 보니 케르발 스페이스 프로그램 할 때가 생각남
     * kens에게 질문, 증폭기 보드 출력 트랜지스터 컬렉터가 금속 케이스에 연결되어 있는지 궁금, 사진상 방열판이 직접 닿지 않고 커패시터 사이에 틈이 있음, 나일론 나사를 써서 프레임에 전기적 연결을 막은 것인지 궁금
          + 아쉽게도 지금 손에 FDAI가 없어서 이 부분 바로 확인이 어려움
          + TO-5 바이폴라 트랜지스터의 경우 컬렉터가 케이스와 연결된 경우가 흔함, 항상 그런 건 아니지만, 예외는 잘 기억나지 않음
     * 이런 장치를 볼 때마다 드는 첫 생각은 “요즘 트렌드의 개발자나 엔지니어들은 이런 걸 재현하는 게 불가능할 것 같다는 생각”
          + 지금도 여전히 똑똑한 일을 해내는 일부 사람들이 존재, 60년대 자동차 정비사도 이런 장치를 복제하는 데는 애를 먹었을 거라는 생각
"
"https://news.hada.io/topic?id=21470","Q-learning은 아직 확장 불가능함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Q-learning은 아직 확장 불가능함

     * 최근 대형 언어 모델(LLM) 등에서처럼 강화학습(RL)의 확장성이 주목받고 있음
     * 실제로 AlphaGo, LLM 등은 강력한 성능을 보이나 주로 on-policy RL 알고리듬이 사용되고 있음
     * Off-policy RL의 대표 알고리듬인 Q-learning은 긴 문제(horizon)에서 누적 편향 문제로 인해 확장성이 떨어짐
     * 실험 결과, 데이터와 컴퓨팅을 크게 늘려도 표준 Q-learning 계열 알고리듬은 복잡한 장기 과제에서 성능 한계가 존재함
     * horizon 문제를 완화하는 hierarchy 방법 등 국소적 해법밖에 없어, 근본적으로 확장 가능한 새로운 오프폴리시 RL 목표가 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

RL, 확장 가능한가?

     * 최근 언어모델의 다음 토큰 예측, 확산모델, 대조학습 방식 등은 데이터와 컴퓨트를 늘릴수록 잘 확장되는 목표임
     * 게임, 수학, 코딩 등에서 RL 역시 강력한 성과가 있었으며, 그 중 많은 경우 on-policy RL 알고리듬(예: PPO, REINFORCE 등)이 활용됨
     * On-policy RL은 항상 새로운 roll-out, 즉 최신 정책으로 직접 생성한 데이터만 사용 가능함
     * 이러한 방식은 시뮬레이션이나 LLM에선 큰 문제가 아니나, 로봇 등 실제 환경에서 매우 비효율적임
     * 예를 들어, 로봇 실험에서 충분한 데이터를 얻기까지 수개월이 소요되고, 사람의 수동적 개입이 필요함

Off-policy RL의 등장

     * Off-policy RL은 이전의 모든 데이터를 재활용 가능하다는 점에서 sample efficiency가 뛰어남
     * 대표적으로 Q-learning이 널리 쓰이고, 실시간 강아지 로봇 워킹 등 성과를 보임
     * Q-learning은 temporal difference(TD) loss 최소화를 활용하며, 거의 모든 오프폴리시 RL이 이 원리를 따름
     * 현실 문제에 RL을 적용하려면 결국 Q-learning도 확장 가능한가? 라는 질문이 핵심임

Q-learning의 확장 한계

     * 저자는 현재 Q-learning은 긴 horizon(100 decision steps 이상) 문제가 등장하면 잘 확장되지 않음을 주장함
     * 여기서 “확장성”이란 문제의 깊이/난이도(‘depth’)가 증가해도 데이터와 연산 자원 투입만으로 해결 가능한가를 의미함
     * 여러 논문에서 실험적으로 증명했듯, 단순히 처리 가능한 문제 수(‘width’)만 늘리는 것이 아님
     * 저자의 주장: Q-learning 계열은 깊이축(difficulty)에서 확장성이 떨어지며, 알고리듬 혁신이 필수적임
     * 주요 근거는 두 가지임: 하나는 경험적 성공 사례 부재, 다른 하나는 최근 수행한 체계적 실험임

경험적 근거

     * AlphaGo, AlphaZero, MuZero는 모두 model-based, on-policy RL로 TD-learning 계열이 아님
     * OpenAI Five 역시 PPO 등 on-policy 방법임
     * LLM용 RL도 대부분 정책 그라디언트 계열 on-policy 변종이 주류임
     * Q-learning이나 유사 off-policy RL이 AlphaGo나 LLM급의 대규모 실제 성공 사례는 거의 없음
     * 저자는 여러 논문 및 실무 사례 조사결과 Q-learning 기반 대형 성공 사례를 알지 못한다고 밝힘

Q-learning의 한계 원인: Horizon과 누적 편향

     * Q-learning은 부트스트랩한(추정치로 예측값 생성) TD 타깃이 항상 편향됨; 이러한 편향이 time-horizon을 따라 누적됨
     * 반면, 토큰 예측, 확산, 대조학습 등 다른 확장성 높은 목표는 예측 타깃에 누적 편향이 없음
     * Horizon(결정 길이)이 길어질수록, 누적된 오차로 인해 Q-learning의 성능 확장이 제한됨
     * 이를 완화하려고 discount factor를 작게 설정하는 사례가 많음
     * Policy gradient 등 on-policy 값 추정 방식은 GAE 등 기법 덕분에 horizon 문제 영향이 상대적으로 적음

실험을 통한 확장성 한계 검증

     * 최근 논문에서 ultra-long horizon 과제를위해 OGBench 등에서 수천 step짜리 어려운 task를 설계
     * 환경에서 ""거의 무한대"" 데이터와 강력한 모델, 표현 신경망 부담 완화 등 잡음 요인을 최소화함
     * 기존 오프라인 RL(BC, IQL, CRL, SAC+BC 등) 모두 초대형 데이터셋에서도 복잡한 task를 학습 못함
     * 데이터와 모델 크기, 학습시간, 하이퍼파라미터 등 모든 변수에 대해 ablation test를 했으나 성능 한계 극복 실패
     * 단, horizon(의사결정 길이)을 줄이는 기법만이 확실히 성능 확장에 효과적이었음

Horizon 축소 기법의 효과

     * n-step return, 계층형(hierarchical) RL 등 horizon 축소만이 RL 스케일링에 결정적으로 효과를 보임
     * horizon 축소는 단순 학습 가속화가 아니라 최종 성능 자체도 획기적으로 향상시킴
     * 하지만 이런 방식은 문제 근본 해결이 아니라 horizon을 상수배만큼 줄이는 데 그침
     * horizon curse를 해소할 새로운 알고리듬 접근법이 필요함

새로운 확장성 있는 오프폴리시 RL 목표의 필요

     * 지금까지의 연구로 단순히 데이터/모델 사이즈만 늘려서는 horizon curse를 근본적으로 극복할 수 없음이 증명됨
     * 궁극적으로는 임의 길이의 장기 문제에도 확장 가능한 오프폴리시 RL 변종이 필요함
     * 이 목표가 실현되면 로봇, LLM, 다양한 의사결정 agent 등 더 폭넓은 실세계 문제 해결이 가능해질 것임

향후 연구 아이디어 및 제안

     * 두 단계 계층(hierarchy)을 뛰어넘어 임의 길이 horizon에 대응할 수 있는 단순하고 확장 가능한 새로운 계층적 구조 제안 가능
     * 모델기반 RL(model-based RL) 은 감독학습 기반 모델링과 on-policy RL 융합을 통해 scalable 할 가능성이 있음
     * TD learning을 아예 배제한 quasimetric RL, contrastive RL 등 새로운 계열 탐구도 유용할 수 있음
     * 생성한 평가 환경 및 코드 오픈, 다양한 새로운 RL 알고리듬의 스케일 테스트 벤치마크로 활용 가능함

감사의 말

     * 논문 및 포스트에 협력/피드백을 제공한 여러 연구자들에게 감사 인사를 전함
     * 본 내용은 [Horizon Reduction Makes RL Scalable] 논문 등에 기반하며, 저자 개인 의견임을 명시함

        Hacker News 의견

     * Q-Learning의 확장성 한계는 블로그에서 언급된 것보다 더 큰 이유가 있다고 생각함. 에이전트가 다루어야 하는 상태 수가 수평선(horizon)이 증가함에 따라 보통 기하급수적으로 늘어남. 이로 인해 해당 상태들을 다루는 Q를 훈련시키려면 데이터 수요도 기하급수적으로 커짐. 반면 on-policy 학습은 중요 상태만을 학습하므로, 기하급수적인 상태 공간에도 불구하고 훈련 데이터가 필요한 지점에만 집중되어 상대적으로 문제 간소화
          + 글에서 말하는 Q-learning의 overapproximation bias 분석에 동의함. Q-learning의 Max 연산자는 노이즈를 시간축을 따라 증폭시키는 경향이 있음. 이 논문처럼 bias 완화 방법들이 RL 에이전트 성능을 성공적으로 개선한 사례도 있음. 네트워크가 잘 방문하지 않은 상태에서 이런 현상이 더 잘 나타난다는 연구 결과도 있음. 상태 수가 기하급수적으로 늘어나더라도 학습 가능한 구조가 있으면 성능이 가능해짐이 딥러닝의 강점임. 핵심은 올바른 훈련 목표를 잡는 것인데, 글에서는 Q-learning이 그 부분에서 한계가 있다고 주장함. MuZero 같은 모델 기반 RL 시스템이 해결책이 될 수 있을지 궁금함. MuZero는 이전 트래젝토리를 재분석해 훈련 효율을 높이고, Monte Carlo Tree Search(MCTS)는 여러 단계를 펼치면서 수평선을 줄이는 원칙적인 방법임. MCTS 내에서도 Max 연산자 문제가 생길
            수 있지만, 탐색이 깊어질수록 이런 문제에 대한 상쇄가 가능함
          + 이 스레드가 도움이 될 수 있을 것 같음. 완전 비전문가 관점에서, 어떤 작업은 “깊이”가 있음에도 불구하고 여전히 균질성을 가진다 볼 수 있는데, 이런 경우에는 샘플 품질이 다소 떨어져도 학습이 가능함. 이런 작업을 나는 “ergodic”하다고 부르고 싶음. 하지만 반드시 그렇지 않은 작업들도 분명히 존재한다고 생각함
          + 이것이 일반 그리드 몬테카를로 통합과 중요도 샘플링 몬테카를로 통합의 차이와 비슷한지 궁금함
          + Majorana-1에 대한 감상 공유
     * 블로그에서 Decision Transformers, Trajectory Transformers 같은 오프라인 방식이 언급되지 않은 점이 아쉬움. 이들은 어텐션 메커니즘 덕분에 credit assignment 문제를 피해서 긴-horizon (장기) 작업에서 좋은 성능을 보임. 많은 RL 연구자들은 이 방식들이 ""진정한 RL""이 아니라고 보는데, 그 이유는 context window 밖에 credit을 할당할 수 없기 때문임. 그래서 무한 horizon 작업에는 적용이 어렵다는 평가가 많음. 하지만 context window가 100만이 넘는다면, 실제로는 큰 문제가 되지 않을지도 궁금. Decision Transformer 논문, Trajectory Transformer 논문 참고
          + TFP 논문은 decision transformers를 인용함. Transformer 아키텍처만으로 credit assignment 문제를 회피할 수 없으며, Transformer는 순서가 중요한 시퀀스 모델링 문제(예: RL 내 credit assignment)에 쓰이는 구조임. 해당 문제의 난이도는 데이터 희소성에 의해 결정되며, 아키텍처 선택만으로 이를 “회피”하는 것은 아님
     * RL의 핵심을 잘 요약했다고 생각함. 아주 간단히 말하면, 계속 움직이며 목표를 쫓는데 그 목표 위치도 내가 어떻게 움직이느냐에 따라 계속 바뀌는 상황임. 즉, value-based RL에서는 절대적인 정답(ground truth)이 없으며, 내 추정치들로만 양쪽을 맞추는 게임임. 하지만 절망적이라고 생각하지 않음. 오히려 RL이 이제 곧 실용화될 거라 여기는데, 그동안 신뢰할 수 있는 월드 모델이나 동역학 함수가 부족했던 게 한몫했기 때문임. 이제는 그 부분에서도 큰 발전 중임
     * 이 논문/블로그는 이미 RL 지식이 있는 사람을 대상으로 함. RL을 더 깊게 공부하고 싶으면 David Silver(Deep Mind)의 입문 강의 추천
     * 오프 폴리시 학습의 근본적 한계는, 효과적이지 못한 초기 탐색 데이터가 더 발전된 정책 학습에는 별로 도움이 되지 않는다는 점임. 예를 들면 체스에서 초보적인 실수, 의미 없는 움직임, 퍼즐을 못 푸는 행동 등이 있음. 데이터가 오프폴리시가 되는 시점은 해당 행동을 현재의 정책(즉 실제로 에이전트가 선택할 것)에서 벗어났을 때임. 그래서 결국 이 문제의 본질은 더 나은 일반화, 그리고 샘플 효율성 향상에 있음
          + 이런 주장이 너무 일반적이지 않나라는 의문이 듦. 예를 들어, 오프폴리시 학습으로 개가 20분만에 걷게 됐던 예시는 어떻게 설명할 수 있을지 궁금. 혹시 더 섬세한 관점이 있는지 묻고 싶음
     * 인간이 장기적(horizon이 긴) 작업을 배울 때는, 반복 훈련을 통해 전체 작업을 짧은 horizon의 세부 작업으로 분할해서 익히고, 나중에 이런 부분 기술들을 계층적으로 조합하는 방식 사용
          + 순진할 수 있지만, 결국 이 문제는 알고리즘보다 접근 방식의 문제에 가깝다고 느낌. 모델이 처음부터 긴 horizon 작업을 해결하긴 어렵지만, 먼저 짧은 horizon 기술을 학습한 뒤 그것들을 묶어서 더 긴 horizon 작업을 익히는 구조임. 인간도 복잡한 일을 미세한 동작의 연속으로 하나씩 배우기보다, 소단위를 익혀서 계층적으로 작업 세분화하는 방식 사용. 예를 들어 비행기 조종이나 스포츠를 할 때도 기본기부터 차근차근 익힘
     * 인간은 실제로 on-policy와 off-policy 학습을 모두 활용함. 직접 행동의 결과를 탐색하는 과정에서 on-policy 학습을 하고, 다른 전문가의 시범을 관찰하면서 off-policy로도 배움. 하지만 인간은 좋은 행동과 나쁜 행동을 구분해 “좋은 것”만 추려서 학습한다는 점이 RL과 차이임. 반면, 대부분의 off-policy RL에서는 나쁜 행동도 데이터로 쓰여서 전체 훈련 속도 저하로 이어짐
          + 좋은 행동과 나쁜 행동을 항상 구분할 수 있는 것은 아니라는 점도 언급하고 싶음. 전문가 시범 중에서는 초보자 관점에서 “완전히 잘못된 것”처럼 보이더라도 훨씬 더 뛰어난 결과로 이어지는 경우도 있음. 때로는 정말 실력자가기에 그런 “정석을 벗어난” 전술도 가능함
     * 블로그 내용이 마음에 들지만, 설명되지 않은 약어나 전문 용어 사용이 더 넓은 독자에게 유용성을 떨어뜨린다는 점이 아쉬움. 용어와 약어를 꼭 설명해서 접근성을 높였으면 좋겠음
          + 이런 식의 블로그포스트처럼 내용은 굉장히 알차지만, 사전 지식을 많이 요구해 접근성이 떨어질 때 AI 도구가 설명·쉬운 해설에 큰 도움이 됨. 최근엔 브라우저 기반 Dia를 사용했더니 효과적이었음. 다른 AI 모델로 복사-붙여넣기를 해도 전체가 간결하면서도, 궁금한 점에 대한 해설을 얻을 수 있음
          + 이런 글은 명확하게 RL 연구자용으로 쓰였다는 점이 느껴짐. 결론이 “누구 Q-learning scalable하게 만들 방법 좀 찾아줘!”라는 식임
          + 오히려 그런 점이 이 글을 더 깔끔하게 만들어줬다고 생각함
     * Q-Learning 같은 오프폴리시 기법의 강점은, 준 최적 데이터(좋지 않은 데이터)만 얻어도 결국 최적 해에 수렴한다는 점임. 예를 들어, 아무 전략도 없는 체스 경기 데이터만 모아 Q-Learning 입력으로 써도 결국 최적 정책을 학습 가능함(물론 좋은 데이터일 때보다 느리지만)
          + 이렇게 되는 조건이 바로 “ergodic” 작업(비록 단어를 약간 변형해서 쓰지만)의 정의라고 생각함. 하지만 그런 ergodic하지 않은 작업도 존재할 것이라고 봄
"
"https://news.hada.io/topic?id=21469","지난 50년간의 정수 선형 프로그래밍: 최근 실용적 진보","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    지난 50년간의 정수 선형 프로그래밍: 최근 실용적 진보

     * 정수 선형 프로그래밍(MILP) 은 여러 산업 분야에서 핵심 도구로 자리잡음
     * 최신 솔버의 계산 효율성 향상 덕분에 과거에는 풀기 어려웠던 문제도 빠르게 최적해를 찾을 수 있음
     * 이 기사에서는 MILP 해법의 최근 실용적 발전과 컴퓨팅 성능 개선에 초점을 맞춰 설명함
     * 주요 방법론으로 branch-and-cut, Dantzig-Wolfe 분해, Benders 분해가 소개됨
     * MILP 분야의 지속적 도전과 미래 연구의 기회가 요약되어 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 혼합 정수 선형 프로그래밍(MILP) 은 오퍼레이션 리서치에서 필수적인 도구로, 다양한 산업 영역에서 성공적으로 활용되고 있음
     * 최신 MILP 솔버는 이제 예전에는 불가능했던 대규모 문제도 몇 초 만에 최적해 탐색 가능함
     * 이로 인해 운송, 공급망, 수익 관리, 금융, 통신, 제조업 등 다양한 분야에서 MILP의 적용이 확대되었음
     * 하지만 여전히 해결되지 않은 문제와 새로운 과제들이 남아있어, MILP 연구는 꾸준히 활발하게 진행 중임

주요 내용 개요

     * 본 논문은 MILP 해법의 최근 발전상과 실용적 성능 개선을 중심으로, 각 기술이 실제로 어떤 컴퓨팅 퍼포먼스 향상에 기여했는지 분석함
     * 방대한 문헌 중에서도 실제 컴퓨팅 실험에 근거한 연구들을 우선적으로 다룸
     * MILP 해법의 세 가지 핵심 분야로 논의를 구성함
          + Branch-and-Cut 방법: 노드 분할 기법과 컷팅 플레인 기법을 결합한 대표적 MILP 해결 방식임
          + Dantzig-Wolfe 분해: 대규모 문제를 더 작은 부분 문제로 나눠 효율적으로 처리하는 분해 접근 방식임
          + Benders 분해: 변수와 제약을 분리하여 반복적으로 해결함으로써 복잡한 구조에서 계산 부담을 낮추는 방법임

마무리: 도전 과제와 미래 전망

     * 논문 마지막 부분에서는 여전히 남아있는 MILP의 도전과제와 미래 연구기회를 조망함
     * 더 복잡해지는 실제 산업 문제들, 데이터의 확장, 솔버 성능 한계 등이 앞으로의 주요 연구 주제임
     * 앞으로 MILP 분야는 알고리듬의 진보, 하드웨어의 발전, 새로운 응용 도메인의 확대와 함께 계속 성장할 전망임

        Hacker News 의견

     * 누군가 Gurobi 같은 상업용 ILP 솔버가 무료/오픈소스 솔버보다 훨씬 뛰어난 이유를 한 번 설명해줄 수 있는지 궁금증 표현, ILP 본질적 난이도로 인한 것인지, 아니면 최고의 솔버란 특정한 서브문제들을 위한 방대한 휴리스틱들 집합이기 때문인지, 그래서 일반적으로 ""좋은"" 전략이 아직 공개 도메인에 없었는지 질문
          + 상업용 솔버는 대부분 고객과 긴밀하게 협력해 문제 특화 가속화 기법 구현, 10~20년 동안 쌓인 노하우 보유 언급, MILP 분야에서 좋은 휴리스틱(브랜치&바운드 시작점 선정, 효과적 트리 가지치기)과 커스텀 컷(부분해를 효율적으로 배제) 활용 강조, 또한 OR 분야 연구자들이 문제를 직접 골라 커스텀 커트와 휴리스틱을 작성하면 종종 상업용 솔버보다 더 나은 결과 얻을 수 있음, 그러나 솔버 회사는 이를 일관적으로 구현하기 위해 박사급 연구팀을 채용하고 수많은 고객 실제 문제 데이터로 개선 추적
          + 상업용 솔버는 실제 고객이 관심을 갖고 도와주기 때문에 문제 해결 성능 조정에 막대한 시간과 자원 투자 가능, 휴리스틱 외에도 간단한 서브문제나 근사 문제 인식 후 이를 전체 문제로 피드백하는 과정도 포함됨, 오픈소스 솔버는(1) 최신 옵티마이저 개발 진입장벽이 매우 높고(수학과 코딩 모두 숙련자가 적음), (2) 만약 그 정도 실력이 있다면 보통 돈 많이 버는 커리어로 빠짐, (3) 오픈소스 구조상 고객이 개선을 위한 케이스, 성능 데이터, 프로파일링 등을 거의 제공하지 않아 한계 노출<br> 예외로는 SNOPT처럼 상업 목적이지만 비전통적 상업 개발인 경우가 있으며, 학계 솔버는 특정 응용 분야에 집중되어 범용성이 낮음, 타 분야에서는 구글 등 큰 회사가 인수 또는 지원을 해 생태계 키우기도 하지만, 솔버 분야는 처음부터 전 범위 스택을
            구축하기엔 투자 부담이 너무 큼
          + 상업용 솔버는 정말 다양한 트릭과 패턴 탐지 메커니즘을 통해 문제에 따라 어떤 트릭이 효과적일지 파악 가능, 문제 구조를 잘 알면 상업용 솔버보다도 빠르게 만들 수 있지만 랜덤한 문제라면 승산이 거의 없음
          + ""솔버=특화된 서브문제에 대한 다양한 휴리스틱 집합""이라는 주장이 있는데, NP-Hard 문제에는 거의 모든 게 그런 구조라는 지적, ILP는 SAT와 동등하므로 동일하게 적용 가능
          + 상업적 규모와 속도 문제, 대부분의 퀀트 트레이딩 회사들은 매우 큰 최적화 문제를 가능한 자주 돌리는데, 오픈소스 솔버는 종종 아예 문제를 풀지 못하거나 메모리 부족 문제가 발생할 정도, 그만큼 격차 존재
     * IBM의 ILOG MILP 라이브러리로 자원 할당 툴 만들 때 경험 회상, 20년 전에 비슷한 문제를 풀었다면 지금 5분 만에 해결하는 데 그때는 아직 풀고 있을 것임, 컴퓨터 성능은 천 배 올랐고 알고리즘도 천 배 좋아져서 전체적으로 백만 배 향상, 미래 예측 때 한번쯤 곱씹을 만한 이야기, 참고로 여기서의 ""자원""은 다이아몬드였음
     * 실제로 어떻게 활용되는지 궁금증, 수치 최적화는 데이터 기반의 일반적인 한계(신뢰, 데이터 문제 등)로 인해 결국 중요한 사람이 '감'으로 의사결정 하는 경우가 많지 않냐는 질문
          + 회사에서 스택 전체에 걸쳐 솔버 사용, 가정용 배터리 및 EV 스케줄링 최적화, 수십만 가정 포트폴리오 최적 스케줄링, 그 포트폴리오 거래까지 솔버로 수행, EU 전기 스팟 가격도 Euphemia라는 단일 거대 솔버 실행으로 결정, 실질적으로 돈과 연결된 명확한 최적화 목표가 있는 분야라면 어디든 솔버가 널리 쓰임 언급
          + FMCG 회사에서 실제 활용 예시, (1) 영업사원 및 배송 경로 계획, (2) 생산을 위한 기계, 인력, 자재 스케줄링, (3) 창고 분포센터 적정 재고 수준 결정 등, 수요 예측의 어려움 때문에 완전 자동화되진 않음
          + 참고할 만한 사례 연구 링크 공유: Gurobi 사례 연구, CPLEX 사례 연구, Hexaly(옛 LocalSolver) 사례 연구
     * Gurobi는 꽤 비싸다는 이야기 들었는데 실제 가격 정보 공유 가능 여부 질문
          + 구체적인 가격은 비공개지만, MIP 실습 목적이면 XPRESS/Gurobi/CPLEX 등 상업 솔버를 구입할 필요 없이 학생용 무료 라이선스나 비상업적 무료 오픈소스 MIP 솔버 사용 추천, 예: HiGHS, SCIP
          + 들은 바로는 가격 책정이 ""전화해서 협상"" 수준이고, 실제로 고객이 얼마나 돈을 버는지 보고 그에 맞춰 금액 책정하는 방식이라고 전해짐
          + 느린 하위 최적(서브옵티멀) 의사결정보다 훨씬 싸다는 점 강조, 작은 문제는 무료 솔버(GLPK 등)로 충분하지만 비즈니스 현장 대형 문제는 제시간에 해답을 얻는 게 거의 불가능, 그래서 프리미엄 솔버는 값어치 충분
          + 10년 전쯤, 서버 여러 명 라이선스 기준으로 약 10만 달러 정도 기억, 정확한 사용자 수나 서버 수 제한은 기억나지 않음, 업계에선 그 정도 가치 충분히 인정된다는 점도 언급
          + Gurobi는 시간 단위로 과금하는 클라우드 서비스도 제공하며, 비학술 라이선스는 매우 비쌈
     * 1990년대 Gomory 컷팅 하이퍼플레인 직접 구현해 봤던 경험, 그동안 분야가 얼마나 발전했는지 놀람, 예전엔 LP 문제 푸는 데 두 달 걸렸다가 이제 1초면 충분, Bixby의 연구서 1990년-2020년 사이 CPLEX와 Gurobi가 기계 성능 독립적으로 거의 400만 배 빨라졌다고 보고됨
     * 1988~2004년 사이 하드웨어는 1600배, LP 솔버는 3300배 빨라짐, 그때만 해도 누적 속도 향상 500만 배 이상, 2001~2020년 상업용 MILP 솔버도 1000배 이상 빨라짐(알고리즘이 50, 컴퓨터가 20), 이런 각 분야별 속도 향상 데이터를 알고리즘 및 컴퓨터 기여도로 분해해서 모아보면 어떨지 호기심, 컴파일러 분야에는 ""Proebsting의 법칙""처럼 18년마다 컴파일러 발전이 컴퓨팅 파워 2배 증가 효과를 낸다는 결과도 있음
     * ML/AI 기반 접근이 이런 문제에는 의외로 많이 안 쓰인 것 같다는 느낌, RL/GNN 활용 사례가 소규모 문제에 시도되는 논문은 있으나 결국 Gurobi 라이선스 구매가 최선 같음, 최근 스케줄링 최적화 하면서 RL 사례도 봤지만 실제 성능은 미흡, 큰 문제에는 진화 알고리즘 최적, 결국 문제 공식을 잘 세울 수 있다면 OR 기반 접근이 가장 효율적인 듯
          + 문제 종류에 따라 다름, 예를 들어 발전소 ON/OFF 결정(유닛 커밋먼트)는 극도로 복잡하지만 MILP 솔버라면 글로벌 최적 해를 신속히 탐색 가능, 유전 알고리즘은 로컬 미니마에서 벗어날 보장도 없고, 실행 속도도 느릴 수 있음, 뉴럴 네트워크도 항상 서브옵티멀
          + SAT는 전통적 GOFAI 문제이고, ML 패밀리 모든 언어로도 SAT 솔버 작성 가능, 오히려 ""ML/AI"" 접근은 충분히 적용 가능
     * pdf 표기를 제목에 추가하면 좋겠다는 코멘트
          + 해당 링크는 pdf가 아니라 초록(abstract)으로 연결됨
          + 직접 논문 pdf 레퍼런스 추가 가능: https://inria.hal.science/hal-04776866v1/document
     * Integer linear programming이 복잡해 보이지 않는다는 의견
          + 그래프 정점 3-컬러링(G3C)는 NP이자 NP-Hard이므로 NPC, 일반 ILP 풀면 G3C도 풀 수 있다는 결과 있음, 또 SAT도 NP-Complete, SAT 풀면 G3C 풀 수 있으므로 G3C 풀 수 있으면 정수 인수분해(FAC)도 가능, FAC는 NPC는 아니지만 현재 컴퓨팅 환경에서 엄청 중요, 즉 임의 ILP 풀면 주요 암호 알고리즘 파괴 가능, ILP가 매우 까다로운 문제임을 유추, 많은 사람들이 헷갈리는 부분은 NPC 문제는 랜덤 인스턴스가 대체로 쉽게 풀리는 점, 난이도 있는 인스턴스 비율은 문제 크기 커질수록 오히려 작아짐
          + 외판원 문제(Travelling Salesperson Problem, TSP)도 ILP로 인코딩 가능, 상당한 난이도 시사
          + 조건을 가장 잘 만족하는 정수를 찾아야 하는데, 실수처럼 보이지만 근본적으로 완전히 다름, 범용 해법은 없고, 특수 케이스에 대한(아주 좋은) 휴리스틱만 존재
          + 선형계획보다는 더 까다로운 문제임
          + 혹은 아주 현실적인 문제로 볼 수 있음
"
"https://news.hada.io/topic?id=21528","MiniMax-M1 오픈-웨이트, 대규모 하이브리드 어텐션 추론 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 MiniMax-M1 오픈-웨이트, 대규모 하이브리드 어텐션 추론 모델

     * MiniMax-M1은 세계 최초의 오픈-웨이트 대규모 하이브리드 어텐션 기반 추론 모델임
     * 4560억 파라미터 규모의 하이브리드 MoE 구조와 라이팅 어텐션 메커니즘으로 긴 컨텍스트 처리에 탁월함
     * RL 기반 학습과 CISPO 알고리듬 도입으로 다양한 문제를 효율적으로 해결 가능함
     * 벤치마크 상에서 기존 DeepSeek-R1, Qwen3-235B 등과 비교해 복잡한 SW 엔지니어링, 툴 사용, 장문 입력 등에서 뛰어난 성능을 보임
     * 다양한 추론 환경 및 지원 도구, API, 챗봇 제공으로 차세대 언어모델 에이전트의 기반으로 활용 가치가 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

MiniMax-M1 오픈소스 프로젝트 개요

     * MiniMax-M1은 세계 최초의 오픈-웨이트 대규모 하이브리드 어텐션 추론 모델로, 기존 상용·오픈 모델 대비 강력한 장점과 실전 활용성을 보여줌
     * 대규모 하이브리드 Mixture-of-Experts(MoE) 구조와 라이팅 어텐션 메커니즘을 결합하여 긴 컨텍스트, 복잡한 추론, 소프트웨어 환경 문제 해결에 최적화
     * 긴 맥락(최대 100만 토큰)을 효율적으로 지원하고, 테스트 시 연산량(100K 기준 DeepSeek-R1 대비 25% FLOPs)을 대폭 절감
     * 최신 RL 기술, 신개념 CISPO 알고리듬과 하이브리드 어텐션 설계로 확장성과 추론 효율성 모두 극대화

1. 모델 개요

     * MiniMax-M1은 하이브리드 Mixture-of-Experts(MoE) 구조와 라이팅 어텐션을 탑재함
     * 전신인 MiniMax-Text-01(4560억 파라미터, 토큰당 459억 파라미터 활성화) 기반으로 개발됨
     * 1백만 토큰의 컨텍스트 길이 지원(DeepSeek R1의 8배 컨텍스트 크기)
     * 라이팅 어텐션으로 테스트 연산량 대폭 절감(DeepSeek R1 대비 25%)
     * 긴 입력과 복잡한 추론이 요구되는 과업에 적합함
     * 대규모 RL을 통한 수학적 추론, 실전 SW 엔지니어링 등 폭넓은 문제에 대한 학습 진행
     * MiniMax-M1만의 RL 스케일링 프레임워크 제시
          + CISPO 기법: 기존 RL 방식보다 우수한 중요도 샘플링 가중치 클리핑 알고리듬 도입
          + 하이브리드 어텐션 기반으로 RL 효율성 및 확장성 강화
     * 40K, 80K 사고 버짓 두 가지 모델로 학습/공개
     * SW 엔지니어링, 툴 사용, 롱컨텍스트 작업 등에서 기존 DeepSeek-R1 및 Qwen3-235B 등 고성능 오픈모델 대비 탁월한 성능
     * 실전 도전과제 해결을 위한 차세대 언어모델 에이전트 구축 기반 제공

2. 평가(Evaluation)

  벤치마크 결과 주요 내용

     * 수학, 코드, SW 엔지니어링, 장문 컨텍스트 분야에서 SOTA급 수준
     * 타 오픈모델 대비 전반적으로 높은 스코어 달성, 특히 소프트웨어 벤치(SWE-bench) 및 롱컨텍스트에서 차별적 경쟁력
     * 주목할 만한 항목 예시
          + SWE-bench: 56.0(M1-80k) / 34.4(Qwen3) / 49.2(DeepSeek R1)
          + OpenAI-MRCR(128k): 73.4(M1-80k) / 27.7(Qwen3) / 35.8(DeepSeek R1)
          + LiveCodeBench, FullStackBench 등 SW 개발 relevant task에서 견고함
     * 실행 환경: temperature 1.0, top_p 0.95에서 평가
     * SWE-bench, TAU-bench 등 벤치마크에 대해 자체적인 절차와 설정(예: 파일 단위 이중 단계 로컬라이제이션, embedding 미사용) 기반 평가 시행

3. MiniMax-M1 모델 사용 가이드

  최적 성능을 위한 권장 설정

    3.1. 추론 파라미터

     * Temperature: 1.0
     * Top_p: 0.95
       이 조합은 텍스트 다양성과 논리적 일관성을 동시에 확보하는 환경 제공

    3.2. 시스템 프롬프트

     * 일반 업무: ""You are a helpful assistant.""
     * 웹 개발: UI 일체형 코드 산출 등 복잡한 웹 페이지 작업을 위한 특화 프롬프트 제시
     * 수학적 추론: 단계별로 풀이 후 \boxed{}에 최종 답 기입

4. 배포 가이드

     * HuggingFace에서 MiniMax-M1-40k, MiniMax-M1-80k 모델 다운로드 가능
     * 실제 서비스에서는 vLLM 기반 배포 추천
          + 효율적 메모리 관리, 뛰어난 배치 처리, 성능 최적화 등 대규모 모델 서빙에 적합함
     * 별도의 Transformers 기반 배포도 지원

5. 함수 호출(함수형 인터페이스)

     * MiniMax-M1은 함수 호출 기능 지원
          + 외부 함수 필요 시 파라미터를 구조화된 형식으로 자동 출력
          + 함수 호출 가이드 제공

6. Chatbot & API

     * MiniMax Chatbot: 온라인 검색까지 포함된 채팅 인터페이스 제공
     * API: 개발자용 온라인 API 및 MiniMax MCP Server 등 개발자 활용 도구 제공
          + AI 기반 비디오·이미지·음성 합성, 보이스 클로닝 등 포함

        Hacker News 의견

     * 혹시 이걸 구동하려면 뭘 써야 하는지 궁금하다면, 8개의 H200 141GB가 필요하고 가격은 약 25만 달러 수준임
       github 논의 / eBay 제품 가격 정보
          + 맥 스튜디오 512GB로 돌릴 수는 없는지 궁금함, 8,500달러 정도면 충분
          + 전량 양자화일 때 이야기고, Q4나 Q8로 돌린다면 1만 달러 이하의 장비로 구동 가능
          + 이 모델의 파라미터 수가 궁금
     * 이번 주가 MiniMax의 '론치 위크'라고 알려져 있음
       월요일에 M1을, 화요일에는 Hailuo 2를 공개함
       중국 모델 관련 소식
       이번 주 내내 이런 발표가 계속될지 아직 미정이고, 현재로선 주로 LLM과 비디오 모델로 알려진 회사임
       공식 발표는 MiniMax의 X(구 트위터)에서 확인 가능
       또, MiniMax M1의 기술 보고서도 유익함
       기술 보고서 PDF
       SOTA 오픈웨이트 모델은 아니지만, lightning attention과 GRPO 변형(CISPO)에 관해 매우 흥미롭고 큰 주장을 함
       (나는 이 회사와 무관한 입장이고, 그냥 얻은 정보를 공유)
          + 월요일에 M1, 화요일에 Hailuo 2처럼 진행했다니 Apple 칩처럼 M1, M1 Pro, M1 Ultra로 이름을 붙이면 재밌었을 것 같음
     * arXiv 논문에서 ""We publicly release MiniMax-M1 at this https url""이라는 문구를 보고, 진짜 빈 저장소가 아닌 실질적인 코드 공개라서 이 회사가 마음에 듦
     * 내 생각
          + LinkedIn 기준 싱가포르 기반 회사로 보이고, 좋은 LLM을 만드는 데 진입 장벽이 크게 없어 보임
          + 오픈 웨이트 모델과 Strix Halo / Ryzen AI Max 발전 덕분에 몇 년 안에 좋은 LLM을 로컬에서 저렴하게 돌릴 수 있을 거라 낙관
          + 앞으로 로컬 모델 구동이 불가피해지는 분위기고, 기대와 우려가 함께 따름
            이 영역에 대해 신뢰할 만한 전문가나 흥미로운 논의를 하는 사람이 있다면 소개 받고 싶음
          + LinkedIn에 나와 있는 것과 달리 실은 상하이 기반 회사임
          + MiniMax가 약 50만 달러 예산으로 모델을 훈련했다는 트위터 포스트를 봤음

     RL(강화학습)을 534,700달러에 훈련
     어떻게 이런 비용으로 가능했는지 궁금
          + 이 회사는 실제로 상하이 소재의 중국 회사임
            곧 홍콩주식거래소(HKEX) 상장도 계획 중임
            관련 기사
     * 공식 페이지에는 명시되어 있지 않지만, MiniMax는 중국 회사임
       위키피디아 참고
          + 많은 사람들이 MiniMax가 중국 기업인 걸 아는 이유는, 그들의 비디오 생성기 이름이 'Hailuo'처럼 중국적 색채가 강하고 지금까지도 그걸로 유명하기 때문
          + 굳이 자사 프로젝트 페이지에 중국 회사임을 밝힐 이유가 있냐는 의문
     * 이런 모델 이름은 좀 더 잘 지었으면 좋겠음
       맥 스튜디오 프로세서 같음
          + Minimax 알고리즘 알고 있음
            그 유명한 고전 AI 알고리즘 이름에서 따온 것임
          + 당신의 맥은 'Apple'에서 만들었고, 실제로 사과 품종 이름에서 유래함
          + Max라는 이름을 가진 내 오랜 잃어버린 강아지가 생각남, 이름이 정말 별로라 borderline criminal 수준이라는 생각
     * 논문에서 ""In our attention design, a transformer block with softmax attention follows every seven transnormer blocks (Qin et al., 2022a) with lightning attention""이라고 적혀 있음
       즉, 전체의 87.5%는 linear attention, 12.5%는 full attention임
       사실 'linear attention'이라는 용어가 혼란을 줌
       softmax attention은 정보 라우팅 방법이고, 토큰 k를 계산할 때 1~k에서 정보를 받아들이지만 크기가 정해진 채널을 거쳐야 함
       반면 linear attention은 각 layer에 고정 크기의 '레지스터 뱅크'가 있을 뿐임
       진짜 attention이라기보다는 layer-at-once 연산에 호환된다는 것 빼고는 주목할만한 게 없음
     * MiniMax가 IPO 상장 소문을 띄우고 있다는 이야기가 있음
       관련 기사
     * 이만한 규모를 서구권 클라우드 인프라 없이 훈련했다면, 토큰 처리 구조가 어떻게 되는지 궁금
          + 512개의 H800 GPU로 3주간 훈련했고, 약 50만 달러 수준임
            xcancel 참고
          + 스니커넷(sneakernet, 물리적 이동 방식) 사용
"
"https://news.hada.io/topic?id=21493","인터넷 복원력 클럽을 시작하는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          인터넷 복원력 클럽을 시작하는 방법

     * 전쟁, 지정학, 기후 변화로 인해 앞으로 유럽에서 심각한 인터넷 장애가 늘어날 가능성 있음
     * LoRa 무전기와 Meshtastic 소프트웨어를 활용해 소규모 자원봉사자 그룹이 초기 통신 복구 리더십을 제공할 수 있음
     * 기존 아마추어 무선 방식은 비용, 복잡성, 전력 소모 문제로 비효율적임
     * LoRa/ Meshtastic 네트워크는 저렴하고 저전력으로, 중앙 인프라 없이 텍스트 메시지 전송 가능함
     * 클럽 설립은 지역 전문가 네트워크 확보, 장비 준비, 실습 모임 진행 등으로 매우 실용적임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   유럽에서는 전쟁, 지정학적 긴장, 기후 변화로 인해 앞으로 잦은 인터넷 장애가 예측됨. 정부와 기업이 대처에 소극적인 상황에서, 소규모 자원봉사자 그룹이 LoRa 무전기와 오픈소스 Meshtastic 메신저를 활용해 통신 회복의 초기 리더 역할을 할 수 있음. 인터넷 복원력 클럽은 저렴하고 긴급 상황에 대응 가능한 오프그리드 메시 네트워크 구성 방안임

저자 소개

     * 저자 Valerie Aurora는 25년 경력의 시스템 소프트웨어 엔지니어이자 자원봉사 조직가로, 유럽 사이버 복원력 법(Cyber Resilience Act) 의 보고 담당 및 RIPE 미팅 프로그램 위원회를 겸임

인터넷 복원력 클럽이 필요한 이유

     * 러시아-우크라이나 전쟁 사례에서 보듯, 국가적 위기 상황에서 인터넷과 전력 인프라가 심각하게 마비될 수 있음
     * 우크라이나 인터넷 운영자들은 발전기, 패시브 광섬유, 군 면제 인력 등 다양한 복원력 전략을 적용
     * 네덜란드 등 서유럽 국가들은 실제 재난 가능성 대비가 미흡함

위기 엔지니어링과 개인 실천

     * 위기 엔지니어링 관점에서, 위기의 도래 전 조직을 준비시키는 것은 거의 불가능하므로, 개인이 사전 준비하는 것이 중요함
     * 클럽 결성, 네트워킹 전문가 모집, 중앙 인프라 없이 직접 통신 복구 시도 필요

LoRa 및 Meshtastic 해결책

  LoRa/ Meshtastic란

     * LoRa는 저전력, 저비용, 비면허, 단거리(수km) 무전 통신 기술임
     * Meshtastic 오픈소스 펌웨어 탑재 시, 중앙 서버 없이 메시 메시징이 가능하며, 점대점(RF)에서 메시 네트워크로 확장됨
     * Bluetooth 또는 WiFi로 스마트폰·PC와 연동 가능
     * 단순 텍스트 메시지 전송에 최적화

  기존 Ham Radio의 한계

     * 아마추어 무선은 고비용, 고출력, 복잡한 면허/교육 문제로 효용성이 낮음

인터넷 복원력 클럽 설립 가이드

  요약 절차

     * 반경 약 10km 이내 전문가 그룹 구성
     * 평시 Signal, Matrix, 이메일 등으로 소통 방식 결정
     * 모든 인원에게 LoRa 무전기, 파워뱅크(트리클 충전 지원) 지급
     * LoRa에 Meshtastic 펌웨어 설치, 통신 채널 지정
     * 정기적인 모임 및 메신저 실습, 친목 활동 장려
     * 회사 담당자라면, 직원 복지 차원에서 LoRa 무전기, 파워뱅크, 소형 태양광 패널 지급 제안 가능

  LoRa의 장점

     * 중앙 인프라 불필요, 면허 필요 없음, 저렴(약 20유로부터 시작)
     * 저전력(1W 이하), 휴대폰 파워뱅크로 장시간 사용 가능
     * Meshtastic 오픈소스 지원, SMS 수준의 메시지 수km 전송
     * 도시 지역 일부에서는 이미 Meshtastic 네트워크 활성화

  LoRa/ Meshtastic 동작 방식

     * 메시 3홉 이하로 약 10km까지 텍스트 메시지 릴레이 가능 (지형, 날씨에 따라 상이)
     * 저속(~1~25kbps), 저출력(<1W) RF 통신
     * 노드 연결 및 메시지 전송은 Bluetooth/WiFi 지원 디바이스 필수
     * 일부 제품은 케이스/배터리/외장안테나 등 하드웨어 다양

전원 관리 및 태양광 활용

     * LoRa는 100~200mA 저전력 소모, 모바일 파워뱅크(10000~20000mAh) 사용시 2~8일 사용
     * ""트리클 충전"" 지원 파워뱅크 필수, 일부 파워뱅크는 저전류 감지 시 자동 off 현상 발생
     * 소형 태양광 패널(800cm², 15W~5W/500mA) 직접 연결로 포터블 친환경 전원 확보 가능, 별도 컨트롤러 불필요

추천 기기

     * Heltec V3: 케이스/배터리 없음, OLED 화면, WiFi/Bluetooth, USB-C 지원, 저렴(20유로 수준)
     * LILYGO T-Echo: 케이스, 내장 배터리, Bluetooth, e-ink 디스플레이, GPS 지원, 약 80유로, 휴대성/즉시 사용 강점
     * LILYGO T-Deck: 키보드/트랙볼/터치스크린 내장 독립형, 약 8시간 배터리, 70~80유로, 재고 자주 부족
     * 외장 안테나 업그레이드 권장 (예: Taoglas TI.08.A, 868MHz)

   안테나 미장착 시 기기 손상 위험 주의

Meshtastic 펌웨어 설치

     * 일부 제품은 펌웨어 사전 설치, 대개는 구버전이므로 최신 펌웨어 플래싱 권장
     * 브라우저 기반(Chrome/Edge) 툴 또는 파일 드래그앤드롭 방식, 숙련자는 CLI/Serial 방식 사용 가능

주파수 및 채널 설정

     * 유럽 내 허용 주파수: 868MHz, 433MHz
     * Meshtastic는 기본 868MHz 사용, 모뎀 프리셋은 기본(LONG_FAST) 유지 권장
     * 채널은 메시 암호키 및 이름으로 관리, QR 코드로 설정값 공유 가능

모임과 훈련

     * 위기 발생 전 실습 및 팀워크 구축 필요성 강조
     * 다양한 시간, 장소, 활동으로 참여자 폭 확대 추천

참고 및 커뮤니티

     * 추가 질문/정보 교환을 위한 메일링 리스트 제공

결론

     * 인터넷 복원력 클럽은 저비용, 저전력, 오픈소스 기반의 실용적 위기 대응 솔루션임
     * 지역 전문가 그룹 조직, 기기 구비, 정기 훈련을 통해 국가/기업 인프라 대비책의 공백을 메울 수 있음
     * 만약 누군가 시작하지 않으면, 아무도 하지 않음

        Hacker News 의견

     * Meshtastic를 인터넷이 거의 없는 시골에 가져가서 셋업하려고 했는데, 이 프로젝트가 인터넷 없이 사용되는 상황에 전혀 준비가 안 되어 있다는 걸 알게 되었음
          + 보드를 플래싱 할 때 공식적으로 Web Flasher만 소개되어 있고, 소스 받아서 하려 해도 PlatformIO 등 툴체인 설치가 인터넷을 필요로 함
          + 클라이언트 앱도 앱스토어나 웹 앱으로 제공하는데, 그 어느 것도 오프라인 구동 불가
          + 실제로 보드 자체가 웹앱을 호스팅하긴 하나, 컴퓨터에 바로 꽂으면 되는 게 아니라 Wifi AP 연결이 필수
          + 공식 문서도 웹에서만 볼 수 있게 되어 있고, 오프라인용 PDF나 셀프 호스팅 방법 같은 것도 안내되어 있지 않음
          + 기술적인 사람이라면 어떻게든 해결할 순 있겠지만, 명확히 강조되는 부분이 아님
          + 대비를 미리 해두라는 게 이 글의 본질인 건 알겠으나, 심지어 오프라인용 PDF 문서조차 제공 않음
          + Meshcore라는 프로젝트도 이번 기회에 알게 되었지만, ""시작하기"" 가이드가 유튜브 영상이라면 비상용으로는 부족하다는 결론
          + 나는 항상 CLI나 ""드래그 앤 드롭"" 방식으로만 펌웨어를 플래싱함
               o 웹 플래셔는 입문자에겐 좋지만 모든 기기에 대해 100% 오프라인 방식이 존재
               o 안드로이드 클라이언트 apk 파일은 GitHub Releases에서 바로 다운로드 가능
               o 그렇지만 인터넷 장기 단절 시나리오를 위한 지원이 지금보다 적극적으로 필요하다고 생각
          + 컴파일된 펌웨어는 GitHub에 스크립트와 함께 제공되며, Meshtastic CLI를 사용할 수 있음
               o 문서도 git 저장소에 .mdx 포맷으로 있음
               o 네가 언급한 모든 불편함은 웹브라우저 기반 사용자들을 위한 편의성 강화의 산물
               o 지금의 웹 생태계는 web3 등 포함해서도 탈중앙성과는 많이 거리가 먼 상태
          + 이런 이야기를 들으니 '진짜 로컬-퍼스트' 생태계와 인프라 구축이 꼭 필요하다는 생각
               o 'Local-first' 선언문을 접했을 때 기대했던 그런 로컬-퍼스트가 아니라 실망이 컸음
               o 내가 생각하는 진짜 로컬-퍼스트는 스마트폰끼리 블루투스 연결 후 중앙 서버 없이 동기화되는 모델
               o 하지만 이런 방식은 SaaS로 수익화하는 입장에선 현실적이지 않음
               o 실제로 재난 이후 네트워크 확장 등은 도구, 개발 환경 모두 진짜 로컬-퍼스트로 변해야 진정한 회복력을 보여줄 수 있음
     * 나는 Meshtastic를 유럽 대도시에서 거의 100% 커버리지로 테스트했는데 실제 성능은 많이 실망스러웠음
          + 안테나 게인 문제나 불안정한 메쉬 덕분에 메시지에 답을 하지 못하는 상황이 많았음
          + 퍼블릭 채팅은 완전히 죽어있거나 테스트 메시지로 도배되어 있고, 총합적으로 너무 느리고 노드가 100개 넘으면 메쉬가 금방 망가짐
          + 채널 속도가 그나마 빠른 편이어도 금방 포화
          + 결국 비상시 의존할 만한 시스템이 아님
          + 오히려 오래된 중고 WiFi 라우터로 퍼블릭 와이파이 메쉬를 만드는 게 훨씬 효율적이라고 봄
          + 이런 라우터는 거의 무료이고 호환 클라이언트도 많고 전력 소모도 낮음
          + 물론 정전이 길어진다면 한계가 있겠지만, 전력이 있으면 쓸모 있는 인프라
          + 나도 비슷하게 느꼈음
               o 1년 넘게 두 노드를 운영하면서도 실제 연락은 2번 남짓임
               o YAGI 안테나나 868MHz 전용 안테나, 게다가 높은 위치였음에도 커버리지가 매우 짧고 신호가 빠르게 약해짐
               o 868MHz 주파수는 높이가 아무리 받쳐줘도 감쇠가 심함
               o 개념 자체는 멋있으나 현실적 솔루션으론 부족
               o Hamnet이나 아마추어 무전이 훨씬 신뢰 가능
               o 소형 SDR 덕분에 20유로대로 간단한 무전기도 구매 가능
          + 메쉬 네트워크(와이파이 메쉬 포함)도 어느 정도는 사전 설계가 필요
               o 우리 지역은 산꼭대기에 위치한 아주 좋은 ROUTER 덕분에 80km 거리도 상당히 신뢰성 있게 메시징 가능
               o 와이파이로는 절대 불가능한 거리
               o 80개 정도의 노드가 LONG_FAST 모드로 운용되고, 인구는 약 50만
               o 그래도 Meshtastic의 라우팅 알고리즘은 비효율적이라 개선점이 매우 많음
          + WiFi 라우터가 상대적으로 좁은 커버리지를 위해 많은 전력을 소모
               o 수백 평방미터 범위를 위해 10와트 정도 필요
               o 도시 전체 커버리지를 원한다면 비효율적
          + 스페인에서 대규모 정전 경험한 입장에서는, 폰을 활용한 이 방식이 정말 도움이 되었을 거라 생각
               o 한 명만 발전기나 Starlink 있으면 모두에게 일부 연결성 제공 가능
          + 폰 제조사들이 이미 메쉬 네트워크를 구현하지 않은 게 의외
               o Apple의 Find My 네트워크가 메쉬라 할 수도 있지만, 임의 데이터 전송엔 대역폭이 매우 낮음
               o Apple의 새 모바일 WiFi 칩이 진짜 인터넷 메쉬 네트워크의 전조일 수도 있다고 기대
     *

     내 악몽 중 하나가 아침에 일어나니 정전, 인터넷 단절, 핸드폰도 먹통인 상황
     나이 들수록 그게 점점 이상향처럼 느껴짐
          + 농담인 건 알지만, 현실적인 시뮬레이션을 안한 것 같음
               o 최근 스페인과 포르투갈에서 정전 사태 그대로 벌어졌을 때, 아름답지 않았음
          + 한 차원 더해서 도로까지 불통된다고 상상해 보면, 잠깐이면 재밌겠지만 장기화되면 끔찍한 혼란
               o 기술 의존성과 그 취약함에 대한 인식 부족
               o 예전에는 계란 공급 끊길 우려나 화장지 사재기 같은 상상하면 장난거리였지만, 실제로 일어남
               o 이제는 공급망 유지가 항상 가능하지 않을 수 있음을 인정해야 함
               o 웃긴 이야기이지만, 그런 의존성들은 정말로 없어도 되는지 곱씹을 가치
               o 진정한 ""회복력"" 개념 자체가 매우 흥미로움
          + 평상시라면 나도 동의
               o 하지만 위기 상황에서는 다름
          + 과거 왓츠앱이랑 여러 SNS가 마비됐을 때 베를린 거리 산책했는데 진짜로 도시 전체가 살아있는 느낌
               o 약간 주관적일 수 있지만, 이런 상황을 긍정적으로 보는 시각도 있음
     * 메쉬 라디오의 대역폭 자체가 굉장히 약함
          + 다양한 혼신 신호(특히 LoRa 라디오가 늘수록)와의 경쟁이 있음
          + 장거리 전송엔 노드마다 딜레이와 대역폭 감소가 누적되어 실제론 문자 메시지 이상의 용도로 쓰기 힘듦
          + 한 홉당 0.3~27kbps 수준인데, 다홉시 더 나눠짐
          + 아주 저대역폭 음성이나 텍스트 위주 웹사이트 이외엔 비현실적
          + 차라리 고정형 수백 Mbps급 마이크로웨이브 링크를 백본으로 쓰고, LoRa는 접속 네트워크로 사용하는게 근본적 개선
          + 실제 이런 실험을 해본 사람들의 경험이 궁금함
          + 이 글의 핵심은 메쉬 네트워크가 기존 인터넷을 대체하려는 게 아니라
               o 복구 작업 중 ""Resilience 클럽"" 멤버들이 쓸 수 있는 임시 통신 채널로 활용하자는 데에 있음
          + 이 대화를 하다보니, 도시 단위 동호회 백본망을 마이크로웨이브 링크 기반으로 구축하면 어떨까 상상하게 됨
               o 높은 건물을 허브 삼아 충분히 괜찮은 메쉬 구현 가능
               o WiFi, LoRa 혹은 둘 다 액세스 네트워크로 사용
               o 긴 거리 메쉬 대역폭이 극히 제한적이니, 개별 클라이언트당 속도 제한 필수
               o 물론 이런 인프라는 수천 달러의 비용, 각 지점별 백업 전원 등 만만찮은 비용
               o ""Big ears, small mouth"" 테크닉(매우 높은 게인 패러볼라+LNA 수신 조합)으로 거리를 높이고 대역폭도 확장 가능
               o 필요한 하드웨어도 생각보다 실현 가능성이 높아 보임
               o 이런 시도를 이미 실험하거나 제작한 사례, 또는 제도적 제한이 무엇인지 궁금
          + 맞는 말이고, 독자적 변조 방식까지 도입되면 상황은 더 심각해짐
     * 나는 오히려 '복원력'의 미래는 취미로 먼 거리 연결하는 무선 네트워크가 아니라
          + 근거리의 비정기적인 연결과, 자연스럽게 움직이는 사람들이 스토리지에 인터넷 트래픽을 넣어 옮겨주는 구조(스니커넷) 쪽이라고 봄
          + 실제 만남에서 프라이빗 키와 신원을 확인하는 최소 신뢰 계층이 생기고, 주소 떠도는 방식보다는 훨씬 현실감
          + 물론 두 모델 모두 운영자에 대한 위험도가 다름
          + 고가치 타깃 없이 모두가 직접 운영자인 경우가 가장 회복력 높은 인프라 구성
          + 내 생각엔 시스템이 장거리 물리적 메신저를 활용하도록 독려하면 정말 멋질 것
               o 뭔가 '러너' 커뮤니티가 메시지를 받아 도시 반대편까지 뛰어 배달하는 면밀히 암호화된 우편 시스템 느낌
          + nncp라는 스니커넷 툴을 써 봤고, 유닉스풍이라 손이 많이 가지만 UI로 감싸면 운용 가능
               o 네이버 지정, 파일로 따로 스풀하거나 TCP/Noise 연결 가능
               o 각 홉마다 데이터 보내기, E2E 암호화 지원
          + 이 아이디어는 Secure Scuttlebutt와도 비슷
               o 공식 클라이언트는 개발이 중단된 상태
          + 미국은 땅덩어리가 너무 넓어서 이런 스니커넷 방식은 도시에선 효과적이지만
               o 교통이 거의 없는 교외나 농촌에선 비효율
               o 10km 이상 커버가 가능한 메쉬는 교외나 넓은 토지를 가진 곳에서도 일정 수준 효과
          + ""스토리지에 인터넷 트래픽을 넣어 옮긴다""는 게 무슨 의미인지 궁금
     * 이 기사 내용은 대형 통신사가 지배적인 도시 배경으로 이해하면 좋을 듯
          + 드레스덴(독일)에서는 여러 자원봉사 조직이 도시 전역에 직접 라인을 설치했고, 자원봉사자 운영 인터넷 교환센터(DD-IX)도 최근 생김
          + 전력만 있으면 우리만의 자체 인터넷을 가질 수 있음
          + 저자는 아마도 배터리에 의존해서라도 전력 없는 환경에서도 동작하는 시스템을 구상하는 듯
     * 나는 그리드가 다운될 때를 대비해 데이터 전송에는 RF보다 레이저 통신을 선호
          + 레이저는 FCC가 아닌 FDA 관할이고, 장거리 대용량 송수신에 유리
          + 셋업 수동성이 있지만, 한 번 구축하면 신뢰성이나 마운틴탑 릴레이에도 탁월
          + 레이저는 신호가 방향성을 가져 프라이버시 면에서도 RF보다 우수
          + 그리드 다운 상황에선 대부분 고정 위치 활용이라 레이저 브리지로 RF 과포화 방지
          + 추가적으로 레이저의 시간 도메인 변화를 감지해 일정 거리 이상 바뀌면 자동 차단하는 기능 등 미들맨 공격 방지 보안성 추가
          + 날씨 문제 땐 HAM 장비 손쉽게 음성으로 전환 가능
          + 실제 위법이지만 통상 한 달에 한 번쯤 누군가 적발되는 정도
          + 정전 사태에선 TLA조차 더 급한 일에 쫓기게 되고, 무전기들은 금세 사라지니 실 사용은 큰 무리 없음
     * 내용이 조금은 구식이고 불완전한 감이 있지만, 현재 Meshcore가 Meshtastic과 경쟁 중
          + Meshcore
          + LoRa는 문자 메시지 정도만 허용, 이미지나 음성, 바이너리 파일 전송은 꿈도 꾸지 않아야 함
          + 또 다른 대안은 값싼 중국산 워키토키(Quangsheng UV-K5)와 위성 APRS 조합
          + 20유로 내외로 텍스트 메시지 가능
          + Meshcore의 의도가 헷갈림
               o 상업적 느낌이고 email도 customers@...로 되어 있음
               o 라이선스 정보도 불분명
               o 아직은 Meshtastic에 손이 더 감
          + Meshcore About 페이지에 가면 유튜브 영상만 있고 ""Download Docs PDF"" 버튼 없음
               o “우리는 인터넷 없이 사람과 사물을 연결합니다”라는 슬로건에 진심이 부족함을 보여줌
          + 사실 LoRa는 낮은 대역폭 때문에 저질 음성도 겨우 가능
               o 극한의 압축 코덱이면 0.5kbps 이하도 가능
               o 표준 코덱 중선 군용 MELPe 코덱은 600bps로 작동 가능
          + Meshcore는 처음 알았지만 좀 더 체계적이고, Meshtastic보다 다듬어진 느낌
               o 다만 투명성은 부족, 아직 시간이 더 필요한 문제일 수 있음
               o 오픈 소스 자격 요건은 충족
               o 저자가 인용한 내용 중 “재난 때까지는 무엇도 할 수 없다. 위기가 닥쳐야 조직이 경청한다.”는 현실에 씁쓸함과 공감
               o 100년 넘은 전통 있는 회사에서 근무했는데, 그들도 재난 대비에는 소극적인 면이 있었음
               o DR 시스템 관리자들은 항상 지원을 받기 힘들고, 실패 대응은 비용과 노력이 많이 듦
               o 마치 보험처럼 필요해도 외면받기 쉬움
          + LoRa, APRS(위성 포함)는 기본적으로 극도로 낮은 대역폭
               o 메쉬나 APRS 모두 수십 명만 넘어가면 라우팅 및 신호 때문에 전체 신뢰성 급격히 하락
               o APRS는 햄면허, 조금 더 값비싼 장비가 필요하지만, 스마트비콘 등의 기능을 써도 송수신 충돌이 빈번
               o 라디오를 무분별하게 구매하는 사람들이 실제 장비 사용이나 면허 등은 잘 모름
               o 실제 도심에서 단방향 거리는 수백 미터 수준이고, 긴급 상황의 주요 중계기는 응급 서비스에서 점유
               o 결론: 책과 카드 한 벌 준비하고, 오프라인의 삶도 얼마 전까지 일상이었음을 받아들이는 게 나을 수도 있음
     * 나는 Meshtastic/LoRa 자체가 통신 수단으론 다방면에서 미흡하다고 봄
          + 갈등이나 전쟁 상황에선 LoRa 신호 나가면 곧바로 아군 좌표에 화포나 로켓이 날아옴
          + 예를 들어 우크라이나에서도 DJI 드론을 기본 펌웨어로 띄우면 바로 감지
          + 우크라이나 라디오에서도 암호화 통신은 되도록 사용하지 않는 게 1번 규칙
          + 왜 암호화 안 하냐면, 암호화된 신호가 오히려 적에게 “중요인물” 이미지를 주어 바로 공격 목표가 됨
          + 만약 그렇게 쉽게 화포 타깃이 될 수 있다면, LoRa 장비를 전선 가까이 무작위로 떨어뜨리고, 랜덤 타이머로 켜서
               o 적이 마구 포탄을 소모하게 하는 역이용도 가능
"
"https://news.hada.io/topic?id=21451","Stable Diffusion 3.5을 순수 PyTorch로 처음부터 다시 구현함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Stable Diffusion 3.5을 순수 PyTorch로 처음부터 다시 구현함

     * miniDiffusion 프로젝트는 Stable Diffusion 3.5 모델을 PyTorch만을 사용하여 처음부터 재구현한 오픈소스임
     * 이 프로젝트의 구조는 교육 목적과 실험, 해킹 용도에 초점을 둔 것이 특징임
     * 전체 코드베이스는 약 2800줄로, VAE부터 DiT, 학습 및 데이터셋 스크립트까지 최소한의 코드로 구성됨
     * 주요 구성 요소로는 VAE, CLIP, T5 텍스트 인코더, 멀티모달 디퓨전 트랜스포머, 공동 어텐션 등이 있음
     * 아직 실험적인 기능이 포함된 상태로서, 더 많은 테스트가 필요한 상태임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

miniDiffusion 프로젝트 소개

   miniDiffusion는 Stable Diffusion 3.5의 핵심 기능을 PyTorch만으로 재구현한 오픈소스 프로젝트임
   이 프로젝트는 기존 Stable Diffusion 3.5와 비교해 다음과 같은 장점이 있음
     * 코드베이스가 약 2,800줄로 크기가 작아, 직접 구조를 분석하고 학습하기에 매우 적합함
     * 다양한 기계학습 실험과 모델 해킹에 유용하게 활용 가능함
     * 종속성이 매우 적으며 최소한의 라이브러리만을 사용함

핵심 구조 및 구성 파일

     * dit.py : 메인 Stable Diffusion 모델 구현부
     * dit_components.py : 임베딩, 정규화, 패치 임베딩, DiT 보조 함수 구성
     * attention.py : Joint Attention(공동 어텐션) 알고리듬 구현부
     * noise.py : Rectified Flow 를 위한 Euler ODE 스케줄러 포함
     * t5_encoder.py, clip.py : T5 및 CLIP 텍스트 인코더 구현
     * tokenizer.py : Byte-Pair 및 Unigram 토크나이저 구현
     * metrics.py : FID(Fréchet inception distance) 평가 지표 구현
     * common.py : 학습에 필요한 보조 함수 제공
     * common_ds.py : 이미지를 DiT용 학습 데이터로 변환하는 iterable 데이터셋 구현
     * model 폴더 : 학습 이후 모델 체크포인트와 로그 저장
     * encoders 폴더 : VAE, CLIP 등 별도 모듈의 체크포인트 저장

     ⚠️ 실험적 기능 및 테스트 필요성 miniDiffusion은 아직 실험적인 기능들을 포함하고 있으며, 더 많은 테스트가 필요한 상태임

주요 기능별 세부 구성

  Core Image Generation Modules

     * VAE, CLIP, T5 텍스트 인코더 구현
     * Byte-Pair, Unigram 토크나이저 구현

  SD3 Components

     * Multi-Modal Diffusion Transformer Model
     * Flow-Matching Euler Scheduler 구현
     * Logit-Normal Sampling
     * Joint Attention 알고리듬 도입

  모델 학습 및 추론 스크립트

     * SD3(Stable Diffusion 3.5)용 학습 및 추론 스크립트 제공

라이선스

     * MIT 라이선스로 공개되어 있으며, 교육 및 실험 목적으로 제작됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

이 오픈소스 프로젝트의 의미와 장점

     * Stable Diffusion 3.5 수준의 최신 이미지 생성 모델 구조를 순수 PyTorch만으로 직접 학습·해킹 가능함
     * 코드가 간결하고 독립적이라 구조 분석/모델 튜닝/신규 알고리듬 연구에 최적화되어 있음
     * 최신 멀티모달, 트랜스포머, 어텐션 기법 등을 직접 실습할 수 있음
     * 상업 프로젝트와 별개로 안전하게 실험할 수 있는 기반 제공

        Hacker News 의견

     * Flux 레퍼런스 구현은 정말 미니멀한 구조이기 때문에 관심 있는 사람이 있다면 한번 살펴볼 만한 가치가 있음
          + Flux 깃허브
          + minRF 프로젝트는 정정된 플로우(rectified flow)를 활용해서 작은 디퓨전 모델을 학습할 때 쉽게 시작할 수 있는 점이 장점임
          + minRF 깃허브
          + Stable Diffusion 3.5의 레퍼런스 구현도 상당히 간결하게 짜여있어서 참고하기에 적합함
          + SD 3.5 깃허브
          + 레퍼런스 구현들은 관리가 잘 안 되고 버그가 많은 경우가 많음
               o 예시로 OpenAI의 CLIP 토크나이저 같은 경우를 들 수 있는데, 공식 학습에 쓰인 버전은 아니고 레퍼런스일 뿐인데도 버그가 수정되지 않은 채 여러 프로젝트에 똑같이 복제되는 상황임
               o Flux도 학습에 실제로 사용된 건 아니고, cudagraphs 등에서 약간 문제를 일으킬 수 있는 버그가 있음
               o CLIP 레퍼런스에 의존하는데 CLIP 자체가 버그가 있으니, 마찬가지로 버그가 전파되는 구조임
     * miniDiffusion 프로젝트가 Stable Diffusion 3.5 모델을 사용한다는 의미인지 궁금증이 생김
          + 관련 코드
          + 학습 데이터셋이 매우 작고 패션 관련 사진만 포함
          + 패션 데이터셋
          + 해당 데이터셋은 디퓨전 모델의 파인튜닝을 실습해보기 위한 용도임
               o SD3를 새로 코드로 재구현한 것이지만, 내 하드웨어의 한계로 인해 가중치는 HuggingFace에서 가져와서 사용함
     * 순수 PyTorch를 사용하면 NVIDIA가 아닌 GPU에서 성능 이점이 생기는지, 혹은 PyTorch가 CUDA에 워낙 최적화되어 있어서 다른 GPU 벤더는 경쟁이 불가능한지 궁금함
          + PyTorch는 Apple Silicon에서도 꽤 잘 작동하는 편임
               o 다만 애플 GPU는 NVIDIA 최상급 GPU 수준의 컴퓨팅 성능을 내기는 어렵기 때문에 직접 비교는 어려움
               o 참고로, 애플 실리콘에서 PyTorch를 사용할 때 약간 특이점이 있음
                    # 각 텐서가 특정 디바이스(CPU나 GPU)에 '소유'된 것으로 인식되기 때문에, 데이터 이동 시 전체 복사가 발생
                    # 맥은 통합 메모리 구조지만 PyTorch는 여전히 데이터 복사를 수행하는 구조임
          + AMD와 같이 비-NVIDIA 디바이스에서도 ML 워크로드를 Vulkan을 통해 돌릴 수 있음
               o 최근 도입되는 cooperative matrix 같은 확장, 그리고 드라이버 레벨의 새로운 기능 지원이 추가된다면 CUDA 대비 몇 퍼센트 내외의 성능 차이도 사라질 가능성 있음
          + PyTorch의 ROCm 지원은 아주 느리게 진행 중이고, 작동에 성공한다 해도 속도가 느림
          + PyTorch가 ROCm에서 잘 작동하긴 하는데, 완전히 ""동급"" 수준까지 잘 되는지는 잘 모르겠음
     * PyTorch 코드에서
    self.q = nn.Linear(embed_size, embed_size, bias = False)
    self.k = nn.Linear(embed_size, embed_size, bias = False)
    self.v = nn.Linear(embed_size, embed_size, bias = False)

       대신
    self.qkv = nn.Linear(embed_size, 3*embed_size, bias = False)
    # forward 함수 내에서
    qkv = self.qkv(x)

   와 같이 시도해보면 좋겠다는 제안임
     * 이렇게 하면 원래 q, k, v 각각의 파라미터가 독립적으로 연결되던 것과 달리, q, k, v 사이의 파라미터가 연결됨
          + 혹시 내가 지금 너무 피곤해서 헷갈리는 거라면 양해 바람
     * 학습자들에게 좋은 자료로 보임
          + 혹시 초보자도 따라할 수 있는 튜토리얼이나 설명서가 있는지 궁금함
          + fast.ai에서 Stable Diffusion을 직접 구현해보는 강의가 있음
               o fast.ai 강의
     * Stable Diffusion을 라이선스 제한 없이 쓸 수 있다는 의미인지 궁금증이 생김
          + 아니고, 추론/학습 알고리즘(수학 그 자체)은 저작권 대상이 아니지만, OP는 단지 코드만 새로 구현한 것
               o 저작권이 문제되는 것은 모델(가중치) 자체고, OP는 데이터나 컴퓨팅 파워가 없어서 직접 학습하지는 않았음
     * 사실 부끄럽지만, 이저장소(repos)가 생기기 전과 후를 비교해 우리가 새롭게 얻게 된 것이 무엇인지 궁금함
          + 개인적으로 모델을 만드는 걸 피해왔고, 주로 결과물만 옆에서 지켜본 입장에서
          + 기존에도 이미 PyTorch 기반 추론/학습 스크립트가 공개되어 있는 줄로 막연히 생각했음
          + 적어도 추론 스크립트는 모델 배포시 같이 주어질 거라고 생각했고, 파인튜닝/학습 스크립트도 있을 줄 알았음
          + 이 프로젝트가 ""클린룸"" 혹은 ""더티룸""식으로 기존 것을 다시 쓴 것인지, 아니면 기존 PyTorch 코드조차 다 CUDA/C 기반으로 너무 복잡해서 순수 PyTorch 버전이 의미가 큰 것인지 확신이 서지 않음
          + 아무튼 잘 모르겠어서 혹시 누가 설명해주면 좋겠음
               o 이 프로젝트의 핵심 가치는 ""의존성이 최소화된 구현""임
                    # SD 3.5를 실제로 돌려본 적은 없지만 huggingface 라이브러리를 기반으로 만들어져 있는데, 개인적으로 huggingface는 의존성이 너무 복잡해서 개발자의 환경을 거의 동일하게 맞추지 않으면 실행조차 어렵다고 느껴짐
                    # 특히 오리지널 릴리즈 몇 달, 몇 년 후에는 특정 모델 실행이 매우 난이도가 높아짐
                    # 예를 들어 SD3.5의 stability AI 참조구현 requirements.txt 파일 보면 버전 명시도 없고, transformers같이 엄청나게 큰 라이브러리를 포함하기 때문에 현업에서는 진짜 난감함
               o Stability AI가 Stable Diffusion 모델을 Stability AI Community License로 배포하는데, MIT와 달리 ""완전 자유""는 아님
                    # 특정 방식으로 가중치를 수정하는 것은 허용되지 않음
                    # 이 패키지는 모델 실행(추론)이나, 혹은 이미 있는 AI 가중치로 파인튜닝은 가능한 구조임
                    # 학습용/공부용으론 훌륭하지만 라이선스 이슈는 여전히 존재함
     * SD 3.5(혹은 그 어느 버전이든)를 생각할 때, 본인은 학습 과정에서 생성된 가중치(웨이트) 부분이 핵심이라고 인식함
          + 코드 자체는 결과물 품질이나 성능 측면에서 상대적으로 중요도가 떨어짐
          + 하지만 정확한 판단은 아니고, 이러한 노력을 폄하하려는 의도는 아님
     * Ludwig Maximilian University의 CompViz 그룹에서 공개한 오리지널 학술 소스의 실사용 가능성에 대해 궁금함
     * 여기 디퓨전 트랜스포머(DiT) 구현이 SD 3.5 풀 버전처럼 크로스토큰 어텐션을 제대로 구현했는지, 아니면 코드 가독성을 위해 단순화한 것인지 궁금함
"
"https://news.hada.io/topic?id=21499","Show GN: [개발일지] 비개발자가 바이브코딩으로 소울라이크 게임을 개발해보았습니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Show GN: [개발일지] 비개발자가 바이브코딩으로 소울라이크 게임을 개발해보았습니다

   비개발자인 IT 기획자입니다. 최근 1인 프로젝트로 모바일 게임을 개발해보았고, AI 도구들을 중심으로 어떤 식으로 작업이 진행됐는지 기록 차 공유해봅니다.

   한달반정도 열심히 달린 것 같은데, 해당 기록을 어딘가에는 남기고 싶어 고민중에 지인 추천으로 이곳에 글을 남기게 되었습니다.

   [배경]
     * 개발 경험 없음 (코딩, 그래픽 모두 처음)
     * 다만 평소 기획 업무는 경험 있음
     * AI 도구를 적극 활용하면 1인 개발이 가능할지 테스트해보고자 시작
       (지인이 커서를 활용하여 10분도 안되서 테트리스를 개발하는 것을 보고 시작해보았습니다)

   요즘 AI 기반 개발 흐름이 워낙 빠르게 확산되고 있어, 단순한 흥미 이상의 실험적 접근을 해보고 싶었습니다. 특히 모바일 플랫폼에서 소울라이크 전투 감성을 어떻게 구현할 수 있을지도 도전 과제였습니다.

   [게임 콘셉트]
     * 캐주얼 소울라이크 액션 게임
     * 불필요한 파밍/잡몹 생략 → 보스전 중심 구조
     * 조작: 공격 / 방어 / 회피 / 패링 중심 (모바일 세로뷰 기반)

   [사용한 AI 툴 조합]
     * ChatGPT (4o): 전투 시스템 설계, 프롬프트 다듬기, 이미지 프롬프트 작성, 대사/세계관 정리
     * Claude 3.7 → 4.0: 코드 작성, 구조 설계 보조
     * Cursor + Claude 조합: 실시간 코딩/디버깅 대응, IDE처럼 활용

   각 도구는 역할을 명확히 나눠서 사용했습니다. 특히 Cursor와 Claude 조합은 코드 작성 속도와 문제 해결 측면에서 가장 큰 도움이 되었고, Claude가 4.0으로 업그레이드되면서 품질이 체감될 정도로 개선됐습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   느낀 점: ""AI는 잘 시켜야 잘한다""
   단순 요청은 효과가 낮고, 프롬프트 설계가 반 이상

   작업 단위를 세분화하고 맥락 유지가 중요
   예:
   시스템 설계 시:
   “넌 전투 시스템 기획자야. 세로뷰 액션 RPG 구조를 설계해줘.”

   코드 요청 시:
   “공격 버튼 → 기본공격, 차지 버튼 → 기 모아서 강공격, 스태미너 없으면 동작 불가” 등

   AI의 환상적인 결과물 뒤에는 수십 번의 반복과 조정이 있었고, 이 과정에서 프롬프트 설계 능력이 핵심 기술임을 실감했습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   [이미지 작업]
     * 캐릭터, 몬스터, UI 아이콘, 배경 모두 AI 이미지 생성으로 처리
     * GPT 기반 이미지 생성 모델을 활용하여 수십 컷 단위로 생성 → 후처리 편집 (간단하게 파워포인트 활용) → 이미지를 스프라이트로 연속 재생하여 애니메이션 구성

   단, 이미지 생성량이 많아지면 GPT 측에서 소프트 밴이 걸릴 수 있어, 일정 분산 처리 필요

   [백엔드 연동]
     * 처음엔 클라이언트 중심으로 개발했으나, 백엔드는 supabase를 연동하였음.
     * 구글 OAuth와 supabase 백엔드 연동의 경우에는 챗GPT + 커서(클로드) 조합으로 도움을 받아서 진행하였음 (챗GPT와 클로드가 알려주는대로 하나하나 따라가면서 작업하여 현재는 백엔드 연동이 모두 완료된 상태)
     * 가끔 잘못된 정보를 줄때도 있어서, 이 경우에는 AI 끼리 서로 정보에 대한 교차검증을 하여 진행함

   [현재 상태]
     * 약 1.5개월 경과, 론칭 버전 개발 완료
     * 구글 플레이 사전등록 진행 중
     * 테스트 배포도 병행, 이달 내 정식 출시 목표

   [시연 영상 및 사전등록 링크]
     * 구글 플레이 링크:
       https://play.google.com/store/apps/details?id=xyz.brokensoul.mygame
     * 유튜브 게임 플레이 영상:
       https://youtu.be/4oMxJJBdzoI?si=HWVMUkWkzfxkJ4M_

   [마무리하며]
   1인 개발이 처음이라 부족한 점도 많지만, AI 도구를 적극적으로 활용하면 어느 정도 수준의 인디게임은 충분히 만들 수 있다는 실감이 있었습니다. 특히 아이디어 기획력과 프롬프트 구성 능력이 핵심 역량으로 떠오르고 있다는 점도 흥미로웠습니다.

   질문이나 피드백은 언제든지 환영입니다.
   출시 이후 반응이 괜찮다면 정식 버전 이후의 개발 내용도 추가 공유드리겠습니다.

   대단한데요? 항상 AI로도 할수 있겠지~ 있겠지 생각만 하던걸 진짜 하신게 넘 대단해요

   저도 처음에는 그렇게 생각했는데 일단 해보자하고 하다보니 여기까지 오게되었네요. 덕분에 ai 활용에 대해서 많이 공부하게 되었습니다.

   안녕하세요 혹시 사운드 적인 부분은 어떻게 만드신건가요? 그 부분에 대한 내용은 없어서 궁금해서 댓글답니다 정말 대단하단 생각 밖에 안드네요.

   사운드도 역시 ai 기반의 툴을 사용했습니다. gpt에게 추천받거나 suno 라는 ai 에이전트를 활용했는데요, 입력하는 프롬프트는 역시 gpt를 통해 도움을 받았습니다.

   와 얼마나 걸리셨나요? 그리고 코드에 대한 이해도가 전혀없는데
   동작을 보고 코드에서 문제점을 찾아서 해결을 AI에게 요청하신건가요?
   아님 코드의 학습방법까지도 요청해서 배우신건가요?

   기본적인 뼈대만드는데 2주, 완성도 높이고 안드로이드 버전으로 포팅하는데 2주, 백엔드 (supabase) 연동하는데 2주 정도 걸린 것 같습니다.

   기획적 지식은 있어도 개발환경 및 코딩 지식은 없다시피했는데, 모르면게 나오면 지피티에게 물어보면서 개발환경도 세팅하고, 버그도 잡고 하면서 개발했습니다.

   그리고 하다보니, 개발코드나 로직도 점차 눈에 익긴 하더라고요

   대단하시네요. 코드 뿐만 아니라 여러가지 요소들을 생각하셨을테니..
   여담이지만 캐릭터가 2b가 연상이되네요 ㅎㅎ

   감사합니다!

   올해 말이나 내년 초에 AI로 게임 만들어보는 걸 시도해보려고 하는데 선례를 공유해주셔서 감사합니다. 개발 일지를 작성한 블로그가 있으면 알려주실 수 있나요? 구독해서 보고 싶네요:)

   안녕하세요, 아쉽지만 개발일지까지 쓸 여력은 없었습니다. 그냥 개인적으로 기획문서로만 정리하는 형태로 진행했었어요... 추후에 기회가되면 개발일지형태로 또 남겨보겠습니다!

   배포까지해서 이정도면... 진짜 대단하시네요

   감사합니다. 어제 무사히 구글플레이에 론칭하였습니다. 그러나 라이브서비스는 배포가 끝이 아니기에... 이후 과정들도 기회가되면 또 일지형태로 남겨보겠습니다.

   원문에 등장하는 지인입니다. 진짜 이걸 해내시다니, 대단하네요!

   Cursor 소개와 함께 간단한 테트리스 구현하는 걸 보여드렸는데, 이렇게 짧은 시간에 뚝딱 만들어 내시다니...(저도 자극 받습니다.)

   다음 작품도 기대하겠습니다!

   덕분에 좋은 경험했고, 많이 배웠습니다. 감사합니다!

   대단하십니다. 인내심이 상당하실 것 같아요.

   감사합니다. 저도 이번 프로젝트하면서 ai에 대해서 많이 공부한것 같아요!

   iOS 버전이 시급..! 멋져요!

   감사합니다. 추후에 iOS도 도전해보고 싶네요!

   애니매이션 이미지를 chatgpt한테 맡기면 일관성 있게 이미지가 생성이가능한가요? 저는 해보니까 안되던데 별도의 프롬프트나 도구가 있는지 궁금해요

   저도 처음에는 많은 시행착오를 거쳤습니다.
   이미지를 학습시키고, 반복된 지침을 설정한뒤에 이미지 생성을 진행하면 가능합니다. 다만 이렇게하더라도 지피티가 잘못 작업하기도 하기 때문에 인내심과 시간이 상당히 필요한 작업입니다.
   가장 중요한것은 프롬프트 설계인것 같아요.

   대단하십니다 2d그래픽을 일관성있게 애니매션화 할라면 여러 프레임을 만들어줘야하는데
   이게 gpt로 불가능하다고 느꼈는데 가능하긴하군요. 모든 프레임을 일일이 생성시킨건가요? gpt화낼거같은데 ㅋㅋ

   맞아요 ㅋㅋ gpt에서 이미지 생성을 과도하게 하면 소프트밴이 걸리는데, 여러번 걸렸습니다.... ㅋㅋ 소프트밴걸리면 몇시간 길게는 하루이상 생성요청이 차단되서요...
   그래서 주로 출퇴근시간 지하철을 이용해서 이미지 생성 작업을 시키고, 시간간격을 두는 방식으로 진행했어요.

   인디게임개발갤에서 봤는데 여기서도 보네요 ㅎ

   반갑습니다! 관심가져주셔서 감사해요!

   안녕하세요. 저도 개발 경험이 전무한 기획자입니다. 클로드, 윈드서프, 수파베이스 조합으로 퀴즈 서비스를 개발 중인데요. 작성자님의 성공담에 동기 부여가 되네요. 저도 개발이 완료되면 이곳에서 다른 분들과 후기를 나눠야겠네요. 감사합니다.

   화이팅입니다. 저도 어렵고 모르는 부분이 많았지만 ai에게 계속 질문하여 여기까지 왔습니다. 프로젝트 완성까지 화이팅입니다!

   근데 비개발자가 커서 기반 언어모델을 쓰는 게 가능한가요? 커서는 vscode를 베이스하는데, 좀 쓰기 어렵지 않나요?

   커서에서는 주로 클로드 ai 에이전트 모델을 썼어요
   오히려 vscode보다도 사용하기 쉽던데요?
   (vscode도 첨에 지피티가 세팅하는걸 알려주긴했는데, 이거보단 커서에서 ai 에이전트에게 요청하는게 더 편했어요)

   좋은 경험 공유해 주셔서 감사합니다. 혹시 게임 개발은 게임용 엔진을 쓰셨나요? 아니면, Android에 있는 기본 View 단으로 구현 하셨나요? 이 부분이 개인적으로는 제일 궁금하네요.

   처음에는 유니티 같은 엔진을 사용해보려하다가, 비개발자가 도전에기에는 조금 어려움이 있어서 웹캔버스에서 돌아가는 형태로 개발하였습니다. html5+자바스크립트 기반으로 개발하니 일반적인 웹개발 프로젝트와 크게 다를것이 없어서, 이 부분에서 커서와 클로드의 도움을 많이 받았습니다.
"
"https://news.hada.io/topic?id=21561","Elixir로 작성한 새로운 BitTorrent 트래커 구현","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Elixir로 작성한 새로운 BitTorrent 트래커 구현

     * ExTracker는 Elixir 기반의 새로운 BitTorrent 트래커 프로젝트임
     * 높은 성능과 낮은 메모리 사용량을 기반으로 설계되었으며, 사실상 제로 셋업으로 바로 사용 가능함
     * BitTorrent 프로토콜(BEP) 의 여러 확장 제안서 지원으로 다양성과 호환성 제공함
     * HTTPS 지원, 디스크 백업, 운영 관리 기능 등 실무에 필요한 주요 기능을 포함함
     * 현재는 산업적 사용에는 미완성 단계이지만 테스트 인스턴스가 실제 운영 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 프로젝트 중요성

   ExTracker는 Elixir로 구현된 새로운 BitTorrent 트래커 오픈 소스 프로젝트로, 기존 트래커들에 비해 다음과 같은 이점을 제공함
     * 최신 Erlang/Elixir 런타임 기반으로, 멀티코어를 모두 활용하는 고성능 구조를 가짐
     * 대규모 피어 환경(약 1백만 피어당 200MB RAM)에서 낮은 메모리 사용량을 보장함
     * 복잡한 사전 설정 없이 즉시 작동하는 제로 셋업 환경 제공
     * 여러 BitTorrent Enhancement Proposal(BEP) 지원으로 최신 트래커 표준과 호환성 유지함

   기존 트래커 대비 가볍고 효율적이며, Elixir 고유의 동시성 및 분산 환경 지원을 최대한 활용하여 동급 오픈 소스 프로젝트와 차별화됨

주요 기능 (Features)

     * 고성능: 모든 CPU 코어 활용, 인메모리 저장소 사용
     * 메모리 최적화: 피어 1백만 명당 약 200MB RAM 사용
     * 제로 셋업: 아무런 추가 설정 없이 바로 실행 가능

지원하는 BitTorrent Enhancement Proposals (BEP)

     * BEP 0: BitTorrent 프로토콜 명세 준수
     * BEP 15: UDP 트래커 프로토콜 지원
     * BEP 23: 압축된 피어 리스트 반환
     * BEP 7: IPv6 트래커 확장
     * BEP 24: 외부 IP 반환
     * BEP 41: UDP 트래커 프로토콜 확장
     * BEP 48: Scrape 트래커 확장 (부분 지원)
     * BEP 52: BitTorrent 프로토콜 v2
     * 일부 기능(BEP 27, 21, 31 등)은 미구현이거나, 계획 중임
     * BEP 8(트래커 피어 난독화)은 지원하지 않음

기타 기능

     * HTTPS 연결 지원
     * 디스크 백업 (데이터 안전성 강화)
     * (예정) Infohash 화이트리스트/블랙리스트 관리
     * (예정) 피어 관리: 권한, 정기 정리, 퇴출 등
     * (예정) 메트릭/지표 관리 및 GeoIP 활용
     * WebTorrent는 지원 계획 없음

   사용자/개발자의 제안사항을 Issue로 접수받음

실행 방법

     * 소스 코드 직접 실행
          + Erlang 및 Elixir 필요
          + 저장소 클론 후 환경설정, 실행
     * 릴리즈 방식
          + 공식 릴리즈는 없지만 직접 빌드 및 배포 방식 지원
          + 릴리즈 파일 복사, 환경설정 후 실행
     * Docker
          + 공식 컨테이너 이미지 사용 가능
          + docker-compose 예시 파일 제공
          + 컨테이너 내부 설정은 환경 변수 이용 권장

저작권 및 라이선스

     * Copyright (c) Dahrkael <dahrkael at outlook dot com>
     * Apache License 2.0 하에 배포
     * 라이선스 상세 내용은 저장소의 LICENSE 파일 참고

        Hacker News 의견

     * OTP 중심의 설계를 기대했지만, 실제 코드는 거의 절차적으로 ETS나 Application 같은 ETS 기반 시스템을 매번 직접 다루는 방식임을 아쉬워함
       작성자가 Elixir나 BEAM 언어에서 서비스 설계법을 배우고 싶다면 James Edward Gray와 Bruce Tate가 쓴 ""Designing Elixir Systems with OTP"" 그리고 Lance Halvorsen의 ""Functional Web Development with Elixir, OTP, and Phoenix""를 참고서적으로 추천
          + 첫 시도에서는 OTP 스타일로 작성했지만, 이런 구체적 흐름에서는 확장성이 동일하게 나오는 것은 아니라는 점을 발견
            토렌트 트래커는 결국 특수화된 데이터베이스이므로 최대한 빠르게 데이터 처리하는 것이 가장 중요한 목표라는 생각
            그래도 추천해 준 책들 꼭 읽어볼 의향 표명
     * C++ 개발자들이 Go와 Elixir를 좋아하는 특별한 뭔가가 있다고 생각
       나 스스로도 여기에 해당
       성능에 끌려 C++을 좋아하는 사람들이 Go나 Elixir의 멀티스레드 성능을 사랑하게 된다는 점을 이야기
       멋진 프로젝트라는 긍정적 의견
          + C++ 개발자에 대해서는 잘 모르겠으나, Erlang/Elixir는 패턴 매칭 구현 덕분에 프로토콜 파싱에 굉장히 강점이 있다고 느낌
            패턴 매칭이 대부분의 분기나 코드의 복잡도를 줄여줘서 코드가 훨씬 깔끔해지는 장점
            'Let it crash' 철학 덕분에 대부분의 예외 케이스를 무시해도, 실제 문제가 발생하더라도 영향이 한 클라이언트에 국한됨을 확신
            10년 넘게 Elixir로 배포된 앱에서 예기치 않은 다운타임을 경험한 적 없음
            유지보수와 업데이트 외에는 항상 100% 가동률이었다는 점을 강조
            클라이언트에게는 “Python이나 Go 대신 Elixir로 만든 서비스는 절대 죽지 않을 뿐만 아니라 멋진 대시보드도 제공”이라고 어필하며, 실제로 많은 이들이 바로 설득됨
            Elixir처럼 struct와 enum, 그리고 함수 시그니처에서 패턴매칭을 지원하는 시스템 언어가 있다면 바람이라는 생각
     * 나 역시 BT(비트토렌트) 학습 목적으로 Typescript로 비슷한 작업을 해본 경험
       이후 Rust로 다시 구현하며 Rust도 학습
       내 프로젝트 링크
       나는 데이터베이스로 그냥 redis를 사용했지만, 상대방은 전부 메모리에 올려서 사용한다는 점이 흥미로움
       이렇게 설계하면서 고민이나 재밌는 결정사항, 문제점 등이 있는지 궁금
       참고로 redis 기반 내 솔루션은 announce를 여러 번 할 때 peer가 항상 무작위로 섞이지 않는 문제점이 있음
          + 내 경험에서는 인메모리 ETS를 쓰는 것이 최고의 선택
            각 피어 데이터를 각각의 프로세스에서 동시적으로 읽고 쓸 수 있어 경합과 레이턴시가 최소임
            유일하게 순차적인 부분은 새로운 swarm이 처음 만들어질 때뿐이며, 자주 발생하는 일이 아니라 괜찮음
            아쉽게도 테이블에서 무작위로 행을 뽑는 네이티브 지원이 없어서 지금은 swarm 전체를 불러와서 무작위 부분집합을 직접 선택
            관련 코드 예시
     * 멋진 프로젝트라 생각
       과거에 나도 Elixir로 기본적인 트래커를 만들었던 경험이 있음
       내 코드 링크
          + 흥미롭다는 생각
            특히 private tracker로 구현한 이유가 궁금
     * 프로젝트 출시를 축하
       opentracker와 비교해서 어떻게 작동하는지, 성능 등 자세한 정보 궁금
          + 작은 트래커라면 opentracker가 아마 더 빠르고 메모리도 덜 쓸 것
            그러나 extracker는 CPU 코어 개수가 두 자릿수로 올라갈 때 진가를 발휘
            아직은 제대로 된 벤치마크는 수행하지 않은 상태
     * 아주 잘 만든 프로젝트라는 칭찬
       간단하게 조언하자면 IO.puts 대신 Logger로 옮기고, OTel 추가도 고려해보면 좋겠다는 의견
          + 여기에 동의
            빌트인 Logger와 Telemetry 애플리케이션만 사용해도 충분하다는 의견
            추후 opentelemetry나 다른 것도 Telemetry 훅에 쉽게 추가 가능
            Logger documentation
            Telemetry documentation
          + 선호하는 otel sink(메트릭을 어디로 보낼지)에 대한 질문
     * Elixir를 정말 좋아하는 마음
       지금 멋진 notification 엔진을 Elixir로 개발 중
       Elixir가 정말 탁월하다는 감탄
          + 멋지다는 호응
            private 프로젝트인지 아니면 OSS(오픈소스)인지 궁금
            Elixir 생태계에 더 나은 notification 엔진이 필요한 상황
     *
          + 어떻게 시작하게 되었는지
     * 참고한 프로젝트가 있는지
     * 개발에 얼마나 걸렸는지
     * qbittorrent와 비교하면 어느 정도 기능이 동작하는지 궁금
          + 다른 프로젝트에서 쓸 트래커가 필요해서 시작했는데 트래커 개발 자체가 더 재밌어져서 계속 하게 됨
            다른 트래커 코드를 참고하긴 했지만, 대부분 너무 복잡하거나 너무 단순해서 참고하기 좋지 않았음
            지금까지 복수의 밤샘 프로그래밍으로 3개월째 개발 중
            qbittorrent처럼 클라이언트는 아니지만, 앞으로 seedbox 지향 클라이언트 프로젝트 아이디어도 있음
          + 트래커는 토렌트 클라이언트가 아니라는 점 설명
     * 직접 사용해 보았으나 HTTPS가 제대로 동작하지 않는 문제를 겪음
       또한 콘솔에
       04:43:20.160 [warning] invalid 'event' parameter: size: 6 value: ""paused""
       라는 경고 메시지가 계속 출력됨
       그럼에도 불구하고 작동은 하는 듯
       HTTP 통계도 보고 싶었으나, UDP 통계만 확인 가능했음
       (본인은 UDP를 비활성화한 상태)
          + ""paused"" 이벤트는 BEP 21의 일부로, 클라이언트가 여전히 완료되지 않았지만 더 이상 다운로드하지 않는다는 신호를 트래커에 알리는 용도
            예를 들어 사용자가 토렌트 일부 파일만 원할 때 활용
            해당 프로젝트의 readme에 BEP 21 미지원 명시
          + HTTP 관련 Telemetry는 아직 ToDo 리스트에 있음
            웹서버로 3rd-party 라이브러리를 사용하고 있으니 적절한 연동법을 더 고민 중
            HTTPS를 사용하려면 :https_keyfile에 유효한 인증서 경로를 지정해야 함
            현재로선 HTTPS를 원하면 트래커 앞단에 Caddy나 Nginx를 두는 것을 추천
            certbot 연동도 계획 중이지만, 대부분의 토렌트 피어가 UDP를 쓰기 때문에 우선순위는 낮음
     * 정말 멋진 프로젝트라는 칭찬
       Elixir를 메인 언어로 쓸 생각이 있는지 질문
          + Elixir가 주력 옵션 중 하나라는 답변
            개인적으로 C++보다 확실히 더 즐겁게 일할 수 있을 것이라는 확신
          + 본인은 아니지만, 9년 2개월 가까이 Elixir로 일해왔고 Rust와 Golang도 다룰 줄 안다는 자기소개
            현 시점에서 누군가 채용 중인지 궁금함
"
"https://news.hada.io/topic?id=21439","jemalloc 포스트모템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             jemalloc 포스트모템

     * jemalloc 의 개발을 주도한 Jason Evans가 직접 쓴 회고록으로, 약 20년에 걸친 jemalloc 개발의 여정을 5단계로 나누어 돌아보며, 성공과 실패, 오픈소스 프로젝트의 한계와 기업 내 변화로 인한 쇠퇴 과정을 진솔하게 기록한 글
     * jemalloc 메모리 할당기는 2004년부터 시작되어 약 20년간 널리 사용되었지만, 최근 Meta의 내부 변화로 인해 공식적 개발이 중단됨
     * 초기 FreeBSD 통합, Firefox 성능 문제 해결, Facebook의 대규모 채택 등 여러 단계에서 성능 최적화와 포팅 경험을 축적함
     * 저장 공간 단편화 문제와 확장성 문제 등 다양한 도전을 거치면서, 성능 향상 및 통계 수집, 테스트 인프라 등 다양한 기능이 추가됨
     * Meta로의 기업 문화 변화와 함께 기술 투자 감소, 핵심 유지보수 인력 부재로 장기 개발이 중단됨
     * Valgrind 제거나 외부 사용자 피드백 부재 등 오픈소스 생태계에서의 단절이 구조적 한계로 작용했음
     * 앞으로 포크(fork) 를 통한 새로운 발전 가능성은 열려 있으나, 공식적인 메인 개발은 더 이상 진행되지 않을 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

jemalloc 개요

     * jemalloc은 2004년 처음 고안된 오픈 소스 메모리 할당기로, 성능과 확장성 개선을 목표로 개발되어 20년간 주요 오픈 소스 프로젝트 및 빅테크 인프라에서 활발히 활용됨
     * 최근 메인 개발이 중단되어, 앞으로는 포크나 개별 조직 차원의 유지 관리가 예상됨

  Phase 0: Lyken

     * 2004년 Lyken이라는 과학 컴퓨팅용 프로그래밍 언어 개발 과정에서 수동 메모리 할당기부터 시작함
     * Lyken 내부의 할당기는 2005년 5월 기능상 완성됨
     * 할당기는 이후 FreeBSD에 통합되면서, Lyken에서는 시스템 할당기의 얇은 래퍼만 남기고 제거됨
     * 제거 이유는 FreeBSD 통합 후, 시스템 할당기 부족한 부분(스레드별 할당량 추적)만 명확해졌기 때문임
     * 재미있게도, 나중에 jemalloc에 Lyken 시절에 필요로 했던 통계 수집 기능이 추가됨

  Phase 1: FreeBSD

     * 2005년 멀티프로세서 컴퓨터가 보편화되던 시기 FreeBSD는 phkmalloc을 사용 중이었지만, 병렬 스레드 환경에 적합하지 않았음
     * Lyken 할당기가 확장성 개선에 명확한 이점이 있어, 이를 FreeBSD에 통합하면서 곧 jemalloc이라는 이름이 붙음
     * 그러나 KDE 애플리케이션 등에서 심각한 단편화 문제가 발생하여 생존 가능성이 의심받음
     * 단편화 원인은 크기 구분 없는 통합 공간 할당 방식 때문이었으며, 연구 끝에 크기별 구역 분리 알고리듬으로 구조를 대폭 변경함
     * 이 과정 내용은 2006년 BSDCan 논문에 기록됨

  Phase 1.5: Firefox

     * 2007년 Mozilla Firefox 3 출시를 앞두고 Windows 환경에서 메모리 단편화가 큰 이슈였음
     * jemalloc을 Linux에 포팅하는 일은 쉬웠으나 Windows는 까다로웠음
     * FreeBSD libc에 저장된 jemalloc을 Mozilla에서 포크하여 이식성과 호환성 개선을 진행함
     * 시간이 흐르며 Mozilla가 jemalloc 업스트림에 많은 개선을 기여했으나, 항상 포크 버전이 더 높은 성능을 보임
     * 이는 성능 회귀 문제 때문인지, 특정 환경에 맞춘 최적화 때문인지는 불명확함

  Phase 2: Facebook

     * 2009년 Facebook 입사 시, jemalloc의 최대 장애 요인은 도구 미비(프로파일링, 리크 탐지)의 문제였음
     * 이를 보완하여 pprof 호환 힙 프로파일링 기능을 jemalloc 1.0.0에서 도입함
     * 개발은 Github로 이관 후, 사용자 및 외부 기여자와 함께 테스트 인프라, Valgrind 지원, JSON 통계, 새로운 페이지 관리 등 많은 개선이 이루어짐
     * 내부적으로는 Facebook의 거대한 텔레메트리 시스템 덕분에 성능 최적화와 회귀 방지에 큰 도움이 됨
     * 3.x: 테스트 인프라 및 Valgrind 지원 도입
     * 4.x: 디케이 기반 메모리 해제(decay-based purging), JSON 통계 추가
     * 5.x: chunk에서 extent 기반 설계로 변경, 2MiB huge page 활용 기반 마련
     * Facebook 내부의 telemetry 기반 성능 분석이 최적화에 결정적 역할 수행
     * 5.0.0 버전에서 Valgrind 지원 제거는 내부에서 사용하지 않아 결정했으나, Rust 개발자 등 외부에서 반발이 강했음
     * 이후 Facebook/Meta의 조직 변화로 jemalloc 팀이 축소되고, 핵심 기술 투자보다 효율성 중심의 사업 전략으로 전환됨
     * 이에 따라 Huge Page Allocation과 같은 대형 기능의 개발이 정체, 일부 작업은 완결되지 않음
     * 2017년 Evans 퇴사 이후, Qi Wang 주도로 수년간 개발 유지
     * 팀 리더십 이양 후에도 여러 기여자들이 프로젝트 유지해왔으나, 장기적 비전 관리자는 부재하게 됨

  Phase 4: Stasis (정지 상태)

     * 현재 jemalloc 업스트림 개발은 종료된 상태로, Meta 역시 내부 필요에 따라 별도의 방향성을 추구하게 됨
     * 기존 코드베이스의 기술 부채가 커서 대대적 리팩토링이 선행 필요함
     * Facebook/Meta의 요구사항과 외부 유저의 요구가 더 이상 일치하지 않음
     * 향후 개발 재개 시 수백 시간 이상의 기술 부채 정리가 선행되어야 하며, 본인은 의욕 없음
     * dev 브랜치 혹은 5.3.0 기준으로 외부 포크는 가능하여, 언제든 포크 기반의 새로운 프로젝트가 등장할 가능성은 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

회고 및 교훈

     * Valgrind 지원 제거로 생긴 갈등은 외부 사용처 파악 부족에서 기인
     * Android에서 jemalloc이 사용 중이었다는 사실도 2년이 지나서야 알게 됨
     * 프로젝트는 GitHub에서 완전히 공개되었지만, 외부 조직에서의 핵심 기여자가 지속되지 못함
          + Firefox의 Mike Hommey나 CMake 전환 시도도 모두 미완에 그침
     * 경험상, 단순히 오픈만 한다고 지속가능한 독립 프로젝트가 되는 것은 아님
     * 오픈소스는 공개만으로 유지되지 않으며, 핵심 기여자 육성과 거버넌스 확보가 필수적임을 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

마무리 발언

     * jemalloc은 25년 넘게 가비지 컬렉션 지지자였던 저자에게도 특별한 경험이었음
     * 다시 가비지 컬렉션 시스템 개발에 집중하고 있으나, jemalloc에 협력해준 모든 이들에게 깊은 감사함을 전함

   전체글 번역본도 있습니다.
   https://rosettalens.com/s/ko/jemalloc-postmortem

        Hacker News 의견

     * 내가 upstream 저장소를 아카이브하기로 한 결정을 이해하는 입장임. Meta를 떠나기 전에는 우리 Jemalloc 팀이 GitHub에 올라오는 온갖 이슈에 대응할 여력이 부족한 상황이었음(예를 들어 누군가 Itanium 환경에서 테스트가 통과하지 않는다고 이슈를 올린 적이 있었는데, 그게 내겐 약간 웃긴 사건이었음). 그래도 이런 상황을 보니 아쉽게 느끼는 감정임. 지금도 jemalloc이 내가 생각할 때 범용 malloc 구현체 중에서 가장 성능이 좋으면서도 쉽게 사용할 수 있는 선택지라고 느낌. TCMalloc도 훌륭하지만, bazel을 사용하지 않는다면 쓰기 정말 힘든 사례라고 생각함(요즘은 bazel 7.4.0에 cc_static_library가 추가되어 정적 라이브러리로 내보내기가 조금 쉬워졌지만, 여전히 그 점은 유의미하게 남아 있음). 나는 Qi에게 저장소를 다시 아카이빙하기 전에 마지막 6.0 릴리스를 한번
       만들어줄 수 있는지 물어보려고 마음먹은 상태임. 마지막 릴리스라면 기본 설정을 조금 현대화해도 좋을 것 같다는 생각임. 예를 들어, 이름과 다르게 혼동을 주는 'cache oblivious' 설정을 기본값에서 비활성화해서 16 KiB size-class가 쓸데없이 20 KiB로 불어나는 문제를 막는 게 큰 개선점임. 이전의 선택(즉, Jason의 초기 결정)을 비난하려는 게 아니라, Qi 그리고 David와 당시 논의했을 때 일반적으로 TLB associativity가 지금보다 훨씬 낮았다는 점이 납득할 만한 이유였음. 비슷한 맥락에서, 기본 'page size'를 4 KiB에서 더 큰 값(16 KiB 정도)으로 키우는 것도 좋은 변화가 될 것임. 이러면 대형 size-class의 기준(여러 할당을 슬랩에 할당하다가 일정 크기 이상이면 각각 개별 범위에 할당하는 전환점)이 16 KiB에서 64 KiB로 올라가서 효과가 큼. 내가 Meta를 떠나기 전에 마지막으로
       내부 주요 서비스에 이 변경을 적용하려고 검토한 적 있었는데, CPU 사용량이 몇 %나 절감되는 대신 RAM 단편화로 인한 소폭 메모리 증가가 있는 최적화였음. 이 외에도 몇 가지 바꾸고 싶은 점이 있음(예를 들면, metadata_thp 기본값을 'disabled'에서 'auto'로 변경, slabs의 extent 사이징을 페이지 사이즈의 정확한 배수에서 약 1% 낭비를 허용하는 대신 단편화를 줄이는 쪽으로 변경 등). 그래도 앞서 언급한 설정들이 가장 큰 변화가 될 것임
          + Itanium 테스트 스위트 실패 이슈를 등록했던 당사자가 바로 나임
          + 이런 실제 경험담이나 내부자 인사이트가 있어야 Hacker News에 계속 오게 되는 원동력이라고 생각함. TCMalloc을 bazel 없이 쓰기 힘든 이유가 무엇인지 궁금증이 있음(정말로 궁금해서 질문하는 입장임)
          + 이런 중요한 내부 지식이 공식 문서나 블로그 글 같은 확장된 자료로 공개되었으면 하는 바람임. 현재로서는 공식 문서가 너무 빈약하게 느껴지는 상황임. Meta 내부에서 이뤄진 수많은 작업의 노하우가 시간 지나 잊히기 전에 공유됐으면 좋겠다는 생각임
          + 훌륭한 소프트웨어임에도 불구하고 빌드와 통합 과정이 복잡해서 제대로 활용되지 못하는 현실이 좀 아쉽게 느껴지는 상황임
          + ‘Jemalloc 팀이 GitHub에 올라오는 랜덤 이슈 대응할 여력이 충분하지 않았다’는 부분을 언급하셨는데, 궁금한 점이 있음. Meta에 해당 프로젝트를 관리하는 충분한 인원이 있었음에도 이슈 관리는 원활하지 않았던 배경이 무엇인지 알고 싶음. 만약 상황을 잘못 이해하고 있다면 정정해주길 바람
     * Jason의 작업이 우리 업무에 얼마나 큰 영향을 주는지에 대한 이야기를 전하고 싶음. 우리 회사는 수억 건의 이미지/비디오를 매일 처리하는 꽤 규모가 있는 곳임. 시작 초기 몇 년 동안 메모리 단편화 문제로 엄청나게 고생했음. 그러던 어느 날 jemalloc을 도입했고, Dockerfile에서 딱 두 줄만 바꿨을 뿐인데 모든 문제가 해결되는 경험을 했음. 지금은 수백억 매출이 나는 회사로, 모든 서비스와 Dockerfile에 jemalloc을 사용 중임. 정말 마음 깊이 감사함을 전하는 마음임
          + 실질적으로 많은 golang 기반 이미지 처리 서비스들이 jemalloc을 추천하거나 사용하고 있음. resize-images 주제에서 상위 3개 서비스 기준(2025-06-13 기준) imaginary는 이렇게 Dockerfile에서 사용 중이고, imgproxy는 https://docs.imgproxy.net/#/memory_usage_tweaks?id=using-jemalloc"">아카이브된 문서를 참고로 imaginary repo 내에서도 논의됨. imagor도 이 위치에서 jemalloc을 쓰고 있는 상황임
          + 진심으로 궁금해서 묻는 입장임(비꼬는 의도 전혀 없음): 혹시 기부도 했는지 궁금함. 돈으로 고마움을 표현하는 것이 최고의 감사 아니겠냐는 생각임
     * ‘jemalloc이 Rust 바이너리에서 예상보다 빨리 제외됐다’는 의견에 대해, 실제로 그 이슈는 여러 원인 중 하나였던 점을 공유하고 싶음. 이 코멘트에서 관련 배경을 볼 수 있음. 그리고 jemalloc이 완전히 제거된 건 해당 이슈가 처음 제기된 후 무려 2년 뒤였음(참조: 이 PR)
          + 저기에 언급된 여러 이슈 중 arm64에서 하드코딩된 페이지 사이즈 문제가 아직까지도 upstream에서 해결 안 된 부분이 흥미롭게 느껴짐. 이것 때문에 앱 개발자는 여러 개별 arm64 리눅스 바이너리를 따로 제공하거나 일부 플랫폼 지원을 포기해야 하는 상황임. 만약 동적 페이지 사이즈(성능을 위해 ftrace 스타일 바이너리 패치를 동적으로 적용)를 도입했다면 지금보다 많이 느려졌을지 궁금해지는 부분임
     * 내가 수년간 만든 모든 게임 엔진에서 jemalloc을 꼭 쓰는 습관이 됐음. win32 환경에서는 기본 할당자보다 훨씬 빠르고, 모든 플랫폼에서 동일한 할당자를 쓰는 이점도 큼. FreeBSD에서 jemalloc 통합된 걸 계기로 알게 되었고, 그 이후로는 쭉 jemalloc만 사용 중임. jemalloc 덕분에 많은 게임 이용자들이 즐거움을 얻었다고 자부심 느끼는 입장임
          + window의 기본 allocator는 정말 별로라는 생각임. Jemalloc이 최고라는 확신임
     * 좋은 글이라는 인상임 — Facebook(이제는 Meta)이 jemalloc 자체를 더는 쓰지 않는 건지, 아니면 유지보수만 하는 단계인지 궁금함. 혹시 tcmalloc이나 다른 allocator로 전환했을 가능성도 있는지 의문임. ‘Facebook 인프라 엔지니어링에서 코어 기술 투자보다는 ROI 중심으로 방점 이동했다’는 문장이 있었음
          + 내가 Meta를 퇴사한 시점이 거의 2년 전인데(아직도 크게 달라지진 않았을 걸로 예상함), jemalloc은 여전히 Meta 내 모든 바이너리에 정적으로 링크되어 사용 중인 사실임. tcmalloc이나 다른 allocator로 쉽게 전환할 수 있냐는 질문에 대해, jemalloc은 회사 내부 생태계에 아주 깊이 결합되어 있으므로 생각보다 바꾸기 어렵다는 답변임. Strobelight 텔레메트리 플러밍에서부터, jemalloc에 맞춰 사용되는 수많은 extensions(예: 직접 커스텀 extent hook을 사용하는 수동 arena 등), 실제로 대부분의 애플리케이션이 jemalloc의 특성을 최대한 끌어내도록 설계된 진화 과정까지 모두 얽혀 있는 상황임
          + 최근 큰 변화라면 jemalloc의 장기 메인테이너들이 전부 떠난 것임. 하지만 오히려 예전보다 Facebook 내부에서 이 프로젝트에 관심을 더 갖기 시작했고, 최근 일부 논란성 이슈를 거치면서 앞으로 Qi와 Jason, 그리고 외부 유저 입장 모두 고려하는 방향으로 나아갈 수 있길 기대하는 긍정적 전망임
          + Meta는 여전히 자체 포크 버전을 여기서 활발히 개발 중임
     * Firefox부터 Facebook까지 오랜 기간 걸쳐 영향력 있던 이 일을 내가 함께할 수 있어 영광스러웠던 마음임
          + 적절한 시점에 나 또한 이곳에 감사 인사를 남기고 싶은 심정임. 이번 글이 오늘 올라올 거라 예상하진 못했지만, 큰 여정의 작은 부분이라도 함께할 수 있어서 영광이었음. @je, qi, david와 당대 및 이후 기여자들에게도 고마움을 전하고 싶음
          + Facebook 내부에서 코어 기술에 계속 투자할 수 있도록 이끌었던 당신의 리더십이 최대한 결실을 맺었다고 생각함. GraphQL, PyTorch, React 같은 혁신이 가능했던 배경엔 이런 기반이 있었기 때문임
     * “불가능한 선택지 속에서 사람들은 1) 극도의 압박 속에서 안 좋은 결정을 하거나, 2) 극도의 압박 속에서 순응하거나, 3) 우회하는 길밖에 없다”는 FTA 인용문이 직장 분위기라고는 상상하기 힘든 모습임
          + 내가 지난 2008년 이후 경험한 거의 모든 직장이 저런 모습이었다는 점이 씁쓸한 체험담임
     * jemalloc이 macOS 내에서 malloc/free를 LD_PRELOAD처럼 원활하게 오버라이드 할 수 있는 유일한 할당자라고 생각함(2020년경 기준). zone 기반 방식으로 쉽게 기본 할당자로 끼어들 수 있고, Apple 특유의 할당자 요구사항도 잘 맞추고 있음. 다른 third-party allocator들은 이 요구사항 때문에 종종 실패했었음
          + 다만, 이 방식은 macOS 시스템 allocator가 내부 구조를 바꾸지 않는다는 가정 아래에서만 동작할 수 있기 때문에, 실제로 Apple이 변경해서 깨진 사례가 두 번 정도 있었던 걸로 기억함
          + mimalloc도 같은 방식으로 동작할 수 있는 걸로 아는데, 확신은 없음
     * jemalloc이 glibc의 malloc 대비 모든 면에서 개선된 부분이 보였고, 벤치마크 결과를 보면 항상 더 나은 성능을 보여주는 것으로 아는데, 그렇다면 왜 기본 allocator가 아닌지 외부자로서 궁금증이 있었음
          + FreeBSD에서는 이미 jemalloc이 기본임. malloc을 바꾸고 싶다면 libc도 같이 FreeBSD libc로 바꾸는 게 더 쉬울 것이고, 그러려면 kernel도 FreeBSD로 올리는 게 자연스러움. Facebook과 합병될 당시 우리 쪽 직원에게 jemalloc을 소개하면서 신나 했지만, 이미 FreeBSD라서 너무 당연하게 사용하는 문화였음
          + 나는 allocator 엔지니어가 아니라 전문 의견은 아니지만, 예전에 OS allocator를 관리하는 엔지니어와 대화해본적 있음. 그분 말로는, 커스텀 allocator는 한 프로세스가 메모리 할당에서 압도적으로 유리해지는 대신, 시스템 전체의 할당 공정성은 악화되는 경향이 있음. 시스템 allocator 입장에서 개별 프로세스가 패턴을 다르게 가져가면 전체 최적화가 어려워짐. 그래서 대부분의 서비스 환경에서, '지금 내 프로세스만 중요하다'는 전제 하에 jemalloc이 많이 추천되는 것임
          + jemalloc이 기본 할당자가 안될 기술적 이유는 없다고 생각함. 실제로 FreeBSD에서는 디폴트임(기사에서 언급). 내 이해로는 이 이슈는 기술적인 것보다 정치적 성격이 더 크다고 봄
          + jemalloc은 실제 대규모 프로덕션에서 검증됐고 라이선스도 매우 자유롭고 성능도 입증된 사실임. 그런데 glibc malloc을 고수하는 이유가 대체 무엇인지, '이념적 순수성', 레거시 관성 외에 누가 이 상태에서 이득 보는지가 진심 궁금함. 왜 여전히 “호환성 때문”이라고 변명하는지 반문하고 싶은 심정임
          + 예전에는 대안 allocator가 free된 메모리를 절대 OS로 다시 반환하지 않고 dirty page로 들고만 있었다는 문제가 컸음(이건 결국 개선됐지만, allocator의 우선순위 차이를 보여주는 대표 사례임). 그리고 실제로 대부분의 프로세스는 메인 쓰레드 하나만 돌리거나 거의 유휴 쓰레드뿐임. 멀티스레딩 최적화 allocator들은 이런 환경에서는 과잉이고 부담만 될 수 있음. 참고로, 커널이 페이지를 zero 처리하는 비용이나, 유저 프로세스가 자기 내부 재사용을 위해 zero하는 비용이나 실질적으로 차이가 없다는 점을 대부분 생각하지 않는다는 점도 언급하고 싶음
     * 차라리 블로그 글 링크를 github 저장소에 추가해줬으면 하는 바람임. 앞으로 레포 방문하는 사람들이 이 맥락을 알고 참고하는 게 중요하다고 생각함
"
"https://news.hada.io/topic?id=21452","자궁내막증은 흥미로운 질병임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            자궁내막증은 흥미로운 질병임

     * 자궁내막증은 전세계적으로 널리 퍼져 있으나 진단 및 연구가 극히 부족한 만성 질병임
     * 그 원인에 대한 지배적 가설인 역행성 월경설조차 일부 사례만 설명하며, 다양한 이론들이 혼재함
     * 암과 유사하게 유전적·후천적 변이, 조직 전이, 자율적 성장 등 암적 특성을 일부 보이나 실제 암과는 직접적 구분이 어려움
     * 완치법이 부재하고, 현재 시술과 약물치료 모두 관리(증상 경감) 목적에 그침
     * 연구 자금 지원이 현저히 부족하며, 장애 조정 수명(DALY) 대비 NIH 자금 비율이 주요 난치성 질환 중 최하위권임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 일반인도 흥미로운 질병으로 종종 언급하는 사례가 있으나, 어떤 질병이 ‘흥미롭다’는 평가는 명확히 정의하기 힘든 현상임
     * 홍역, 라비스, 에볼라, 파상풍 등은 발병 기전이나 환자에게 미치는 영향 측면에서 유별난 질병으로 분류됨
     * 자궁내막증은 대체로 그렇게 분류되지 않지만, Harvard Wyss Institute의 연구원과의 대화를 통해 상당히 이상한 질환임을 새롭게 인식하게 됨
     * 비의생명과학 분야 인맥 및 시스젠더 여성간 비공식 설문조사에서도, 자궁내막증에 대한 기본적인 이해조차 매우 부족함이 확인됨
     * 이로 인해 자궁내막증의 이해 격차를 해소하기 위한 목적에서 이 글을 집필함

자궁내막증이 흥미로운 이유

  자궁내막증의 임상적 정의

     * 자궁내막(uterine lining)과 유사한 조직이 자궁 외부에 자라나는 질환임
     * 이 조직은 난소, 나팔관 같은 인접 부위, 혹은 방광 및 장과 같은 더욱 먼 기관에도 이식될 수 있음
     * 해당 조직은 호르몬(주로 에스트로겐) 에 의해 주기적 증식·붕괴·출혈을 반복하나, 정상 자궁내막과 달리 혈액의 배출 경로가 없음
     * 조직과 혈액이 체내에 갇혀서 극심한 통증, 염증, 섬유화(흉터 형성), 기관 유착을 초래함
     * 장기적 반복 과정은 만성 통증·불임 등 구조적 변화로 이어짐

  주요 발생 가설의 불완전성

     * 가장 널리 알려진 가설은 역행성 월경설로, 월경시 자궁내막 세포 일부가 나팔관을 거쳐 복강 내로 역류하여 착상·성장한다는 이론임
          + 이 가설은 해부학적 이상을 가진 여성군에서 자궁내막증 위험도가 높은 점으로 일부 뒷받침됨
     * 그러나 전체 케이스의 설명력은 부족함
          + 역행성 월경이 75-90% 여성에서 발생하지만, 실제 발병률은 약 10%에 불과함
          + 자궁 내막과 떨어진 다양한 기관(위장관, 폐, 뇌 등)에서의 확진례가 존재함
          + 월경 경험이 없는 소녀, 자궁 없는 여성, 시스젠더 남성에게서도 발생함
     * 따라서 이 가설 외에도 유전적 소인, 면역 이상, 체세포 돌연변이, 세균 오염 등 복합적 가설이 제기됨

  최신 병인학적 통합 이론(요약)

     * 흔히 제시되는 통합 모델은 다음과 같음
         1. 신호(종자) 세포 : 무레리안 기원의 배아줄기세포, 순환하는 다능성 줄기세포, 월경혈 내 자궁내막 줄기세포 등이 ‘종자’ 역할을 함
         2. 이식(토양) 환경 : 역행성 월경, 호르몬 요법, 만성 염증 등 특정 조건 하에서 종자가 복강이나 기타 조직에 자리잡음
         3. 생존 및 증식 : 면역 회피, 혈관신생(angiogenesis), 호르몬 저항성, 체세포 및 후성유전 돌연변이 획득 등으로 조직 환경에 적응해 잘 살아남음
     * 하지만, 배아줄기세포 기원 이론, 복막 상피 전환 이론 역시 각각의 한계와 설명 불충분점이 존재함
     * 결론적으로, 다양한 요인이 이질적으로 작용하는 복합 경로로 여겨짐

  자궁내막증과 암의 유사성

     * 종자, 체세포 돌연변이, 조직 전이 등 주요 기전이 암종과 흡사함
     * 실제로 자궁내막증 병소 조직에서 ARID1A, PIK3CA, KRAS, PPP2R1A 등 대표적 암 유전자 변이가 많이 관찰됨
     * KRAS 변이 예시에서 볼 때, 변이 비율이 높을수록 임상적 침윤성·수술 난도 상승이 확인됨
     * 단일성 종양(암)과 달리 치명적이지 않으며, 형태상 화농 혹은 미만성 병변이 특징임
     * 일부 외과의는 “전이성 암보다도 처리 난이도가 높다” 는 평가를 함

  실질적 치료법의 부재

     * 현재 치료법은 호르몬(경구 피임약, 프로게스틴, GnRH 작용제 등) 에 의한 주기 억제 혹은 외과적 절제 및 유착 해소임
     * 두 방법 모두 완전한 ‘치유’를 달성하지 못하고 증상관리(증상 경감)에만 치중함
     * 호르몬요법은 병변 크기 변화에 효과 미미하며, 복용 중단시 재발 빈도가 높음
     * 수술도 5년내 재발률이 20-45%, 8년내 약 40% 재발임
     * 향후 Warburg Effect 기반 대사관련 약물(예: dichloroacetate), 혈관신생 억제제 등이 등장하고 있으나, 임상 현장 적용까지는 상당한 시간이 필요

  널리 퍼졌지만 극도로 저평가된 대표적 질환

     * NIH 대비 장애조정수명(DALY) 기준 자금지원 비율이 알츠하이머, 크론병, 당뇨, 간질 등과 비교해 극도로 낮음(예, 29M:56.6 → 0.5)
     * 진단 지연이 심각(평균 7-10년), 복잡한 수술 필요 등으로 인해 실제 발병률 및 질병 부담이 공식 통계 대비 과소평가됨
     * 증례조사 결과 진단받은 환자 외에 약 60%의 추가 미진단 환자가 존재하는 것으로 추정됨
          + 이를 반영할 때 실제 DALY 대비 자금지원 비율은 0.2 근접, COPD(만성폐쇄성폐질환)에 준할 정도로 심각하게 낮음

결론

     * 자궁내막증은 기원 불명, 암적 양상, 미흡한 관리 및 치료, 광범위한 유병률, 연구 예산의 부족 등 독특한 특성을 모두 지닌 질병임
     * 전세계 여성 중 10%(1억 9천만 명) 이 영향을 받음에도, NIH 예산은 연 기준 2,900만 달러에 불과함
     * 자궁내막증의 ‘흥미로움’은 뜻밖의 인과, 복잡성, 과소 연구라는 재미와 충분한 연구의 필요성을 동시에 시사함
     * 타 난치성 질환(암, 알츠하이머, HIV 등)과 마찬가지로, 관심과 혁신적 연구의 유입이 절실하며 잠재적 영향력 역시 큼
     * 이 질병의 매력적인(?) 복잡성 자체가 연구 동기를 제공할 수 있으며, 혁신적 접근의 필요성이 큼

        Hacker News 의견

     * 진단을 받기 힘든 사례들을 볼 때마다 항상 놀람을 느낌. Endometriosis(자궁내막증)는 OP가 지적한 것처럼 대표적인 사례임. 예전에 New York Times 매거진에서 의료 케이스 스터디 시리즈가 있었는데, 환자들이 여러 진료과와 전문의를 방문해도 답을 못 찾고 고생하다가, 우연히 지인의 이모가 Johns Hopkins에 아는 의사가 있다든지, 그런 식으로 기적처럼 해결되는 경우가 많았음. 특히 여성 환자들에게서 이런 문제가 많이 두드러짐. 왜 이런 현상이 생기는지 모르겠음. 의사들이 시스템에 지쳐 관심이 줄거나, 독선적이라 환자 말을 잘 안 듣거나, 진단을 너무 단순화해서 하거나, 여성 질환에 대한 지식이 부족하거나, 때로는 의료계의 성차별 문제 때문일 수도 있다고 생각함. 결과적으로 환자들이 ‘Dr Google’에 의존하게 만들고, 때로는 사이비 치료로까지 흘러가는
       경우도 생김. 이건 결코 좋은 일 아닐 것임
          + 원인은 명확함. 환자를 미스터리로 다루지 않고, 반복적인 Jira 티켓 처리하듯이 빠르게 처리함. 시스템은 90%의 평범한 케이스만 커버하도록 설계되어 있음. 나머지 10%에 해당하면 제대로 대응받기 어려움. 의료 제공 기업과 보험회사에서 각종 지표를 강요하니 의사는 거기에 맞춰 움직임. 의료사고 걱정하면, 그냥 Epic 시스템에 있는 프로토콜만 읽어주게 됨. 이런 상황이 당연히 생기는 구조임
          + 나의 파트너도 지금 이런 상황을 겪고 있음. 내 경험상, 대부분의 의사는 진단에 대해 신경 쓰지 않거나(혹은 신경 쓸 시간이나 동기가 없음), ‘그건 내 일이 아니다’라는 태도가 큼. 여러 전문의를 만나도 증상 청취는 2분이고, “혈액검사나 해보자”면서 이전 5명의 의사가 이미 했던 검사들을 반복함. 혈액검사에서 뚜렷한 이상이 없으면 “잘 모르겠으니 다른 전문가를 찾아보라”고 손을 놔버림. 가족이나 지인을 통해 추천받은 의사가 효과가 있는 이유는, 그 의사는 환자와 인간적으로 연결된 이해관계나 동기가 생겨서 조금 더 신경을 씀. 현재 시스템은 의사가 많은 환자를 보도록 금전적으로 유도하지만 환자를 낫게 만드는 건 인센티브가 별로임. 실제 치료는 의사의 ‘사명감’에만 의존하도록 두는 구조임
          + 일선 의료는 “일단 x만 해봐라”처럼, 시간 단축을 위한 요령 위주로 운영됨. 환자 한 명, 한 명을 트리(fault tree)로 분해해보고, EMR이랑 감사 시스템으로 80/20 규칙에 집중하는 쪽으로 편향이 생김. 사실상 의료를 대기업 헬프데스크로 만드는 셈임. 내 가족이 두통으로 병원을 갔다가, 1% 확률인 뇌종양이 있었던 경험이 있음. 혈압만 높다고 보고 넘기다가, 증상이 미묘하게 변해 CT 찍었더니 8주 만에 진단이 됨. 그러나 멜라노마(흑색종)는 8주가 아주 긴 시간임. 실제로 99%의 두통 환자는 혈압이나 흔한 원인 때문임. 천 명 두통 환자 중 5명 찾으려고 CT 찍으면 50명의 다른 합병증이 생길 수 있음. 의사를 대기업 헬프데스크 오퍼레이터처럼 생각하고, 네트워크를 잘 활용해야 함. 돈이 없거나 주변에 이런 인맥이 없으면 결과는 좋지 않을 수도 있음
          + 드문 질환을 진단하는 일이 정말 어렵다고 생각함(물론 endometriosis는 흔한 질환일 수 있음). 나도 내 일에서 희귀한 버그를 찾기 힘든데, 인간 몸에서 문제를 찾는 건 훨씬 더 어려울 거라고 상상함
          + 캐나다 의료시스템을 1년 넘게 전전하다가, Secondary Hypogonadism(이차 성선저하증)을 겪음. 수치상 Testosterone이 “정상”이었지만, 머리가 멍하고, 피로하며, 성욕이 전혀 없는 등 명확한 증상이 있었음. 여러 의사를 만나도 대부분은 대충 넘기거나 “마음의 문제니 운동을 해라”고 함(난 BMI도 건강하고 운동도 충분히 함). 간혹 심각하게 받아들인 분도 추가 검사는 해주지 않음. 결국 포기하고 개인 비뇨기과 의사를 찾았더니, 바로 추가 검사를 지시해서 원인이 명확해짐. HCG 처방 후에 한 달 만에 새로운 사람처럼 좋아짐. 증상도 다 사라지고 호르몬 수치도 정상화됨. 환자가 ‘정상’ 수치를 보여도 분명 낫지 않은 상태라면, 좀 더 귀 기울여주길 바람
     * 가까운 사람이 Endometriosis를 심하게 겪는 걸 봄. 너무 아파서 100m도 못 걷고, 침대에만 누워 식사도 못 하다 20kg이나 빠짐. 이 질환이 너무 심각해져 목숨까지 위협했고, 결국 자궁적출 수술 이후에야 삶이 되돌아옴. 물론 그 덕분에 조기 폐경도 옴. 모든 환자가 이렇게 심각해지는 건 아니지만, 본인이 그 ‘불운한’ 케이스가 될지 아무도 모름. 진단 받으면 삶의 질을 생각해서 앞으로를 계획해야 함. 임신이 필요하다면 꼭 우선순위로 둘 것. 자궁내막증은 불임에 영향을 줄 수 있고, 임신이 증상 완화에 도움됨. 수술로 증상이 일시적으로 좋아질 수 있지만, 외과의 실력에 따라 차이가 큼. 잘 듣는 좋은 외과의를 찾아야 함. 또한 내부 조직이 엉기거나, 수술 후 생긴 반흔 조직 때문에 내부가 더 엉키기도 함. 난소에 석회화가 생기는 등 치료 과정이 다소
       혼란스러움(레이저 소작법 등). 증상이 너무 심해 일상생활이 안 될 정도면 자궁적출을 미루지 말아야 함. 남의 말 듣고 고통을 견디지 말고, 제때에 결정할 것을 조언하고 싶음
     * 다국어를 구사하면서 관찰한 흥미로운 사실 중 하나는, 국가별로 구글에서 의료 정보를 찾을 때 서로 다른 혹은 상반된 지침이 나오기도 함. 예를 들어 생리 중 성관계에 대해, 일본에서는 Endometriosis와의 연관성 때문에 피하라고 권장하는데, 영어권에서는 별 정보가 거의 없고 문제시하지 않음. 생리 중 성관계와 불임의 연관성도 영어에서는 정보가 적고 일어로는 훨씬 많이 나옴
          + 이런 사례는 아주 흔함. 특히 육아 관련해서 국가별로 상반된 권고가 있음. 영국에서는 이유식을 6개월부터 시작, 프랑스는 3-4개월부터 시작해야 한다고 함. 아기 침실 온도도 영국은 16도, 프랑스는 19도, 북유럽은 아기를 밖에서 재우고, 헝가리는 25도가 적합하다고 함. 우리 건강 상식 중에는 거의 전해 내려오는 민간지식도 많음. 대부분 건강하지 않은 환자이거나 본인이 주관적 경험을 설명할 수 없는 상황에서 신호와 노이즈를 구분하기 어렵기 때문임
          + Ureaplasma Parvum 같은 경우도 마찬가지임. 어떤 나라에서는 심각한 STD(성병)로 취급하지만 미국에서는 성병으로 거의 이야기되지 않아 진단, 치료받기도 힘듦. 자세한 내용은 여기와 이곳의 배경 정리 참고
          + 아르헨티나 친구가 “너무 뜨거운 음료를 마시면 암에 걸린다”고 주장해 위키피디아 스페인어판을 보내줌. 이게 문화마다 다른 건강 ‘상식’ 때문인지, 내가 뭔가 놓친 건지 궁금함
          + 우리가 신뢰하는 ‘합의된 과학’의 많은 부분이 사실 언어나 지역 문화 내 반복에서 비롯된 것임을 새삼 깨닫게 됨
          + 임신/출산/육아 관련 이슈에는 국가별 민간지식 차이가 특히 많음. 서양에서는 임신 중 초밥을 절대 먹으면 안 된다고 하는데, 일본에서는 초밥을 건강식으로 적극 권장함. 미국에선 땅콩을 아기에게 먹이면 안 되지만, 이스라엘에서는 땅콩과자가 처음 먹는 음식 중 하나임
     * ‘역행성 생리(retrograde menses)’ 이론 논의와 관련해, 백혈병(CML)으로 골수이식 받은 여성이 있었는데, 나중에 맹장염인 줄 알고 수술했더니 실제로는 자궁내막증이었음. 더 놀라운 건, 조직 검사 결과 골수이식으로 들어온 XY 염색체가 이식된 부위에서 발견되었다는 점임. 골수이식 환자에서 기증자의 DNA가 몸 여러 곳에 자리 잡을 수 있다는 현상은 알려져 있지만, 이 사례에서 자궁내막증까지 이식받은 결과인지 증명된 것은 아님
          + 줄기세포가 이식 후 예상치 못한 곳에 간다는 걸 이론적으로만 알다가, 실제 자궁내막증처럼 미스터리한 질환에서 생생히 확인하는 건, 우리가 이 메커니즘을 얼마나 모르는지 잘 보여주는 예임
     * 내 여자친구도 자궁내막증이 있음. 그동안 별로 공부를 안 했는데 이번에 글을 읽고 알게 되어 고마움. 사실 여성 건강의 많은 분야에서 흔히 접할 수 있는 이야기임. 연구 자금도 부족하고, 연구 자체가 소홀함. 여성들이 STEM이나 정치에 참여를 못하게 했던 사회 구조가 주요 원인이고, 여전히 이런 장벽이 존재함. 글 마지막에 박사들이 이런 문제에 더 관심 갖게 만드는 인센티브에 대해 설명한 부분이 마음에 듦
          + “여전히 장벽이 존재한다”는 부분에서, 내 경험은 완전히 반대임. 오히려 유방암은 암 연구 중 가장 많은 자금이 투입되는 분야 같음. 누군가가 증거와 상관없이 꺼내 드는 결정론적 주장 같음
     * 누군가에게 유용할까 공유함. 관련 논문이 있는데, Fusobacterium이라는 박테리아가 자궁 조직을 침투하면 자궁내막증 발생과 연관이 있다는 연구임. 논문 링크
     * 매우 흥미로운 주제임. 이 글을 재미있게 읽었다면, 최근 Hacker News에 공유된 전립선에 대한 심층 기사도 추천함. 더 희망적이고 행복한 느낌의 결말이 있음
     * 기사에서 수술적 접근법에 대해 중요한 차이점을 너무 간략하게 다뤘음. 90% 이상의 산부인과 전문의는 병든 조직을 소작(burn-to-destroy)하는 방식만 교육받았음. 최근엔 질환이 붙어있는 주변까지 넓게 절제(excision)하는 기술을 전문적으로 하는 외과의가 등장함. 병든 조직이 표피에만 있는 게 아니라 깊이 파고드는 경우가 많아서, 단순 소작은 잔디 깎기처럼 다시 올라옴. 절제술이 성공률이 좀 더 높으나 만능은 아님
          + 소작 수술은 반흔 조직을 훨씬 많이 만드는데, 향후 임신이나 생식력에 큰 영향을 줄 수 있음
     * 출산의 신비와 자궁 속 ‘전쟁’을 생각하면, 이런 복잡한 시스템이 이렇게 잘 작동하는 게 놀라움. 임신이 증상을 완화할 수 있지만 완치는 아님. 출산율 저하가 자궁내막증 증가와 관련 있는지도 고민해볼 만함. 관련해서 이 글 참고
          + 자궁 속의 ‘전쟁’ 때문에 오히려 자궁이 잘 작동하는 것일 수도 있음. 서로 반대 방향으로 작용하는 다양한 요소들이, 진화적으로 더 많은 중복성과 대책을 만들었음. 멘델 유전학 이전에는 생식세포 간의 ‘전쟁’이 진화의 동력이라는 추측도 있었음. 몸도 협동의 산물 같지만, 어느 정도는 이런 긴장관계에서 진화함. 참고 기사도 모두 읽어보기를 강력 추천함
     * 기사에서 언급됐는지 모르겠지만(글 스타일이 거슬려서 중간에 포기함), 자궁내막증은 유전성이 높음. 내 아내의 이모 둘 중 한 명이 자궁내막증이 있었고, 나머지 이모 두 분의 딸(아내 포함) 모두 같은 질환을 겪음. 난임의 원인 중 하나기도 하고(아내와 이모 모두 불임), 난자가 손상돼 임신이 어려워질 수 있음. 대부분의 생식내과에서는 자궁내막증을 치료 불가라고 보지 않는 듯함. 수술로 인한 부작용 1% 정도면 그리 높은 것은 아닌 것 같음. 재발률은 환자 케이스와 조기 진단 여부에 따라 다름. 내 아내는 20대 후반에 복강경으로 수술받았고, 크게 어렵진 않았음. 15년 뒤 자궁적출술을 다른 사유로 받았지만 재발하지 않음
          + 기사에도 언급돼 있음. 좀 더 읽었으면 가장 흥미로운 부분을 볼 수 있었을 것임
"
"https://news.hada.io/topic?id=21550","동형암호화된 CRDTs","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              동형암호화된 CRDTs

     * 로컬-퍼스트 소프트웨어에서 협업 문서의 보안을 유지하기 위해 동형암호화(Homomorphic Encryption)와 CRDTs를 결합
     * 종단 간 암호화만으로는 서버가 데이터를 병합할 수 없어 동기화와 업데이트 효율에 제약이 발생함
     * 동형암호화는 서버가 내용을 알지 못한 채로 CRDT 업데이트를 병합할 수 있도록 프로그램 실행을 가능하게 하는 기술
     * 하지만 동형암호화의 근본적 한계(성능 저하, 공간·연산량 증가, 코드의 최악 케이스 동작 필요) 로 인해 실제 적용에는 중대한 난점이 존재함
     * CRDTs와 보안 연산의 공존을 위한 다양한 접근이 연구되고 있으며, 아직 완전한 해결책은 모색 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

로컬-퍼스트와 보안 협업의 과제

     * 원격 협업에서 문서를 로컬-퍼스트 방식으로 CRDT에 저장한 뒤, 동기화를 통해 공동 편집 경험을 제공하는 구조임
     * 문서 내용이 앱 개발자 등 제3자에게도 절대 노출돼서는 안 되는 보안 요구가 있을 경우, 종단 간 암호화가 흔히 활용되는 방법임
     * 종단 간 암호화는 동작이 단순하지만, 동기화 서버가 데이터를 병합하지 못하므로, 오랜 기간 비동기로 작업하면 비효율적 데이터 통신이 발생함

동형암호화란?

     * 동형암호화는 암호화된 데이터상에서 직접 알고리듬 실행이 가능하게 하는 특수 암호화 방식임
     * 이를 활용하면 동기화 서버가 데이터의 내용을 알지 못한 채로 CRDT 업데이트 병합을 수행할 수 있음
     * 동형암호화에서 지원하는 연산 종류에 따라 부분 동형(덧셈/곱셈 중 하나만) , 일부 동형/계층 동형(양쪽 일부 횟수) , 완전 동형(제한 없음) 으로 구분함
     * 연산이 많아질수록 암호문에 노이즈가 쌓이고 해독이 곤란해져 Bootstrapping 등 고급 기법이 요구됨
     * 암호화된 비트(0/1) 수준에서 XOR, AND 같은 기본 연산 게이트 조합만으로 일반적 불리언 회로 구현 가능임

실제 동형암호화 CRDT 구현 사례

     * Rust 기반 라이브러리 TFHE-rs로 Last Write Wins Register라는 대표적인 CRDT를 동형암호화로 구현함
     * 평문 구조체와 암호화 구조체의 필드와 메서드(암호화/복호화)는 거의 동일하나, 실제 병합 로직에서 중요한 차이가 발생함
     * if/else, match 구문 등 실행 경로 분기가 암호문 해독에 힌트를 줄 수 있으므로, 암호화 환경에서는 모든 분기·루프를 즉시 평가하는 방식이 필수임
     * 주요 조건 비교, 병합 연산 모두 비트 단위 FheBool 연산자와 select 메서드 등으로 처리해 어떤 조건에서 값이 바뀌었는지 외부에 감지 불가함

동형암호화의 근본적 한계

     * 암호키와 데이터 크기 불균형: 예시에서는 32바이트의 데이터에 123MB 서버 키 필요(압축해도 27MB)로 공간 비효율이 심각함
     * 성능 저하: 동형암호화된 CRDT merge는 미암호화 대비 약 20억 배 느린 1초 수준 측정
     * 루프·분기가 입력값에 따라 변하면 정보 노출이 발생하므로, 항상 최악의 케이스 기준으로 연산수·메모리를 소모해야 함
     * 예를 들어, last-write-wins map과 같이 key-value가 희소한 경우라도, 모든 키를 고정 크기로 꽉 채운 채 병합해야 하므로 실질적 확장성 저하
     * 암호문 구조나 변경 내역에서 값이 변화했는지, 어느 부분이 갱신됐는지 외부 관찰자가 추론 불가하게 설계해야 함

결론 및 전망

     * CRDTs와 동형암호화는 이론적으로는 자연스럽게 결합될 수 있지만, 현실적으로 공간/시간 효율성 및 프로그램 구조상 치명적 제약이 큼
     * 현 시점에서는 완전한 실용 솔루션이 나오지 않았으나, CRDTs 자체도 비교적 젊은 기술로 지속적 연구가 이루어지는 중임
     * 로컬-퍼스트 협업앱에서 보안성과 사용성의 균형을 위한 혁신적 솔루션에 대한 가능성은 여전히 남아 있음

        Hacker News 의견

     * 완전 동형 암호화 분야가 정말 느리고 비효율적인 것은 사실이지만, 이 분야 자체가 2009년에 등장한 비교적 신생 영역임을 언급하고 싶음, 지난 15년 동안 이 분야의 발전은 정말 대단한 수준임, 최초 동형 암호화 스킴은 수 TB/PB에 달하는 키가 필요했고, 부트스트래핑(동형 암호화에서 곱셈 연산이 많아졌을 때 필수적인 연산)은 수천 시간이 걸렸음, 이제는 키가 30MB 수준이고, 부트스트래핑이 0.1초 내에 가능해짐, 앞으로도 계속 발전해서 실용적으로 쓰일 날이 오길 기대함
          + 초기 CRDT(CRDT: Conflict-free Replicated Data Type)들도 WOOT처럼 매우 비실용적이었는데, 요즘 최신 CRDT 데이터베이스는 일반적인 LSM에 비해 퍼포먼스가 그리 떨어지지 않는 수준임, 예를 들면 RDX CRDT들은 머지 소트와 유사한 알고리즘으로 동작해서 모두 O(N)임, 대다수 구현에서 메타데이터 오버헤드도 잘 제어함, WOOT, librdx, go-rdx 참고
          + CRDT의 아키텍처 특성상 느릴 수밖에 없고, 최고의 알고리즘조차 구조적으로 비용이 큼, 여기에 동형 암호화를 더하면 도전 난이도가 훨씬 높아짐, 정말 인상적이긴 한데 실제로 쓸만한지 궁금함, 주장에 대한 근거로 ""서버가 새 맵을 계산하려면 모든 키를 하나씩 머지하고, 그 후 전체 맵을 각 피어에 전송해야 함—서버 입장에서는 전체 맵이 매번 다르기 때문""이라는 설명을 인용함
     * CRDT는 Conflict-free Replicated Data Type의 약자로, 충돌 없이 분산 동기화를 할 수 있는 특수한 데이터 타입임, CRDT 위키백과 링크 참고
          + 예를 들어 문서를 여러 명이 동시에 편집해야 하는 앱들이 자주 CRDT를 사용함
     * 기사에서 성능이 부족하다고 언급한 부분이 있는데, 실제로 M4 MacBook Pro에서 last write wins 레지스터를 평범하게 돌릴 경우 머지에 0.52 나노초가 걸리지만, 동형 암호화 버전은 1.06초가 소요됨, 즉 연산 속도가 20억 배 차이남, 정말 충격적인 수치임
     * FHE(Full Homomorphic Encryption)는 정말 느리지만 2009년 이후 놀라운 발전이 있었음, 부트스트래핑 속도만 해도 수천만 배는 빨라짐, tfhe-rs를 이용하여 이미 동형 암호 기반 AES-128도 시연함, AI 추론/학습을 위한 실시간 FHE 도입 가능성이 점점 높아진다는 생각임, tfhe-aes-128 깃허브 링크 참고
     * 서버가 클라이언트의 변경 사항을 더이상 이해할 수 없다는 내용에 동의할 수 없음, 서버는 사용자가 아직 보지 못한 변경분만 보내주고, 사용자는 그걸 복호화·머지하여 문서의 최신 버전을 만드는 방식임, 동형 암호화가 흥미롭긴 하지만, 합리적인 성능이나 대역폭이 필요한 상황에서는 거의 적합하지 않음, 예전에 (커스텀 CPU+RAM을 암호로 인코딩한 뒤 각 클럭 신호마다 한 명령씩 처리하는) 동형 암호 기반 완전 비밀 컴퓨팅을 논문에서 본 적 있음, 실제로 동작하지만 1초에 가상 CPU 명령 1개 처리 가능한 수준임, 이런 속도와 비용이 괜찮을 정도라면 차라리 로컬에서 돌리거나, 정말 부자라면 직접 하드웨어 사서 로컬로 처리하는 편이 현명함
          + 컴퓨터과학 및 암호학 논문들은 종종 실용성과는 먼, 예를 들어 공격 복잡도를 2^250에서 2^230으로 줄이는 식의 황당하게 비현실적인 연구 내용이 많은 편임, 이런 연구는 실제 R&D나 가능한 영역을 넓히는 데 의미가 있음
          + 서버가 내용을 직접 다룰 수 없다면 CRDT 문서를 병합하지 못하고, 매번 CRDT의 전체 상태를 받았다가 전송해야 함, 친구가 동시에 온라인이라면 연산 결과(operations)를 보내서 실시간 머지가 가능한데, 그렇지 않으면 매우 비효율적임
     * 학생들이 FHE로 암호화된 WASM이나 JS 채점 코드를 JupyterLite, ottergrader 조합으로 자신의 크롬북에서 직접 돌려도 신뢰할 수 있을지 의문임, 코드서명과 SETI@home screensaver와의 관계도 궁금함
     * THFE-rs는 Zama 측이 프로토타입 용도 이외에는 상업적 라이선스를 요구하므로 쓰지 않는 게 좋겠음, 라이선스 조건이 불편함, 그 대신 openFHE(C++), lattigo(golang) 라이브러리를 추천하며, 둘 다 상업적으로 자유롭게 쓸 수 있음
     * FHE는 여기서 쓰기에 본질적으로 맞지 않는 도구라고 생각함, FHE는 중앙 서버가 소유하거나 아는 데이터를 다루는 데 적합함, 여기서는 여러 사용자가 분산된 데이터를 함께 처리해야 하므로 MPC(멀티파티 연산: 여러 참가자가 각자 비밀 데이터를 합산 등으로 공동 연산하는 방식)가 훨씬 효율적임
          + 나 역시 FHE를 쓰고 싶다가 결국 더 좋은 도구나 최적의 방식이 있다는 걸 자주 깨달음, FHE 적용할 수 있는 상황 자체가 꽤 한정적임
     * 기사에서 제시한 가정이 이해가 잘 안 됨, CRDT와 동형 암호 개념은 알지만, 왜 동기화하려면 양쪽이 모두 동시에 온라인이어야 하는지 의문임, 비동기적으로 업데이트를 주고받는 ‘store-and-forward’ 방식이면 비행 중 데이터도 암호화된 채 전송 가능한데, 왜 굳이 서버가 상태를 유지해야 하고(그것도 암호화된 상태로), 제안된 것처럼 수정해야 하는지 궁금함
          + 서버가 각 피어마다 레지스터 사본을 따로 저장해야 하는 문제 때문이라고 생각함, 서버가 가장 최신 상태를 판별하지 못하니까임, FHE를 쓰면 서버가 하나의 사본만 유지할 수 있음, 즉 모든 피어가 항상 온라인이면(동시에 접속하면) 서버는 그냥 전달만 하고 따로 저장할 필요가 없음
     * FHE의 느림과 비효율성에 CRDT의 저장 공간 낭비가 어우러진 모습이 재미있어 보임
"
"https://news.hada.io/topic?id=21518","오픈AI와 마이크로소프트의 갈등이 임계점에 도달하고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    오픈AI와 마이크로소프트의 갈등이 임계점에 도달하고 있음

     * OpenAI는 Microsoft와의 독점적 제휴 관계에 불만을 품고 있으며, 계약 조건을 반독점법 위반으로 규제 기관에 제소하는 방안까지 검토 중
     * 기업 전환 및 상장 추진 과정에서 Microsoft의 승인이 필요하지만, 지분율과 Windsurf 인수 관련 IP 소유권을 두고 양측 간 협상이 교착 상태에 빠짐
     * Microsoft는 현재 OpenAI의 전 제품에 독점 판매권과 선접근 권한, Azure 독점 컴퓨팅 제공자 지위를 갖고 있음
     * OpenAI는 Stargate라는 자체 데이터 센터를 비밀리에 구축하며 Microsoft 의존도 탈피를 시도했고, 다른 클라우드 사업자들과 협력해 판매 범위를 확대하려는 움직임도 보임
     * OpenAI는 올해 말까지 공익 법인으로 전환하지 않으면 200억 달러의 투자금을 잃게 되는 상황으로, 시간이 촉박한 가운데 Microsoft와의 이해 충돌이 심화되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

갈등 배경 및 핵심 쟁점

  파트너에서 경쟁자로 변화한 관계

     * 2019년 Microsoft는 OpenAI에 10억 달러를 투자하며 긴밀한 파트너십을 맺었고, 이후 6년간 기술 및 상업적 협력 관계를 유지함
     * 그러나 현재는 ChatGPT vs Copilot, 기업용 AI 제품 등에서 정면으로 경쟁하게 되었고, Microsoft는 독자적 모델 개발팀까지 비밀리에 운영함

  Windsurf 인수와 IP 소유권 문제

     * OpenAI는 최근 30억 달러 규모로 코드 스타트업 Windsurf를 인수
     * Microsoft는 계약상 OpenAI의 모든 IP에 접근할 권리를 갖고 있지만, OpenAI는 Windsurf의 IP만큼은 보호하려 함
     * 이 문제는 상호 불신을 심화시키는 촉매제로 작용 중

  기업 구조 전환과 지분 분쟁

     * OpenAI는 비영리에서 공익 목적의 영리 법인(PBC)으로 전환을 추진 중이며, 이를 통해 상장 및 대규모 투자 유치를 도모
     * Microsoft는 새로운 법인에서 더 많은 지분 확보를 요구, 반면 OpenAI는 그 수준을 받아들이지 않고 있음
     * 이 갈등으로 인해 전환 지연 시 약속된 200억 달러 투자금 상실 위험이 있음

  독점 우려와 규제 리스크

     * OpenAI는 협상 압박 카드로 Microsoft의 반경쟁 행위에 대한 규제 조사 요청을 검토하고 있음
     * 실제로 미 연방거래위원회(FTC)는 Microsoft의 AI 투자 및 OpenAI와의 관계를 조사 중
     * 향후 공개적 비판 캠페인 및 규제기관 접촉이 현실화될 경우, 양사 파트너십은 근본적으로 흔들릴 가능성 존재

  AGI(범용 인공지능) 논쟁과 계약 종료 조건

     * Microsoft는 계약 종료 이후에도 AGI 수준의 기술이 도달하더라도 OpenAI 기술 접근을 원함
     * 반면 OpenAI는 AGI가 도달하면 독립성을 확보하고 싶어 함
     * AGI의 도달 가능성 자체에 대해서도 업계 내 의견은 임박 vs 요원 vs 불가능으로 갈림

현재 상황 요약

     * 양측은 협상은 계속 중이며 장기적 협력에 낙관적이라는 공식 입장을 발표했지만, 이면에서는 치열한 기싸움이 진행 중
     * OpenAI는 더 넓은 시장 접근과 컴퓨팅 자원 확보를 원하고, Microsoft는 독점적 기술 접근 및 지분 확대를 요구
     * 갈등이 고조될 경우, 역사상 가장 성공적이라 불렸던 AI 파트너십이 법적·사업적 충돌로 끝날 가능성도 존재함

   화장실 들어갈 때랑 나올 때가 다르다가 이런 건가요?

        Hacker News 의견

     * https://archive.ph/4HP14 링크 공유
     * 요즘 반독점 이슈가 대세인데, 빅테크 중에서 OpenAI는 실력으로 경쟁하려는 의지가 가장 약한 회사라고 느끼는 입장임. 처음에는 자신들의 비영리 의무에서 벗어나는 것이 모두에게 이롭다고 주장했고, 그 다음에는 AI 규제가 필요하다면서, 새로운 경쟁자를 막을 정도로만 규제하고 자신들의 사업에는 영향이 없을 수준을 원했음. 이제는 Microsoft와 맺은 계약도 벗어나고 싶어하는 모습임. 일반적인 반경쟁 행위라고 하면 시장 지배력 남용을 떠올리겠지만, OpenAI 입장에서는 자기들이 AI 분야를 장악하는데 방해되는 모든 요소를 ‘사회적 문제’로 보고 정부가 자기들을 위해 해결하길 바라는 태도라 신기하게 느껴지는 상황임
     * “파트너사에 대한 불만으로 반독점 문제를 규제기관에 제기하려 했다”는 내용에 대해, 정부에 Microsoft를 고발하겠다는 전략 자체가 진짜 유리한 입장은 아니라는 생각임
          + 이 기사를 읽고 OpenAI가 이 내용을 WSJ에 직접 흘렸을 거라고 추측함. 폭로 자체가 OpenAI의 입지가 약하다는 신호라고 봄. 반독점 신고 위협도 효과적이지 않은데, 왜냐하면 그런 조치는 실행 속도가 느려서 OpenAI가 당장 해결책을 얻기 어렵기 때문임. 오히려 Microsoft에 타격만 주고 OpenAI가 위험파트너라는 이미지만 커질 수 있음
          + 동의하지 않는 입장인데, 요즘 정부의 반독점 정책이 기준이 모호해지고 있어서 기업들이 로비와 영향력 행사에 나서는 것은 자연스러운 현상임. 특히 원칙 없는 정치인들이 있을 땐 이런 경향이 더 쉬워짐. 참고로 객관적인 ‘소비자 후생 기준’이 사라지는 현상을 말하고 싶음
          + 2023년 130억 달러 투자 구조 자체가 반독점 규제를 교묘하게 피해가기 위한 설계였음. 그 후 Microsoft가 적극적으로 규제 회피 투자를 했고, 최근 Meta와 Scale AI 협력도 마찬가지로 법망을 교묘하게 피해가려는 사례임
     * Windsurf 인수 관련해서 그동안 별다른 소문이 없었던 이유를 이해함. 두 회사가 30억 달러에 Windsurf를 인수하는 조건을 두고 대립 중이라는 기사 인용임. 현재 계약상 Microsoft는 OpenAI의 모든 지식재산(IP)에 접근권이 있고, GitHub Copilot 같은 자체 경쟁 서비스를 갖추고 있음. 그래서 OpenAI는 Windsurf의 IP만큼은 Microsoft에 접근시키지 않으려고 한다는 내용임
          + Microsoft가 OpenAI의 모든 IP에 접근할 수 있는 계약이 이미 있는데, 왜 OpenAI가 Windsurf를 인수하려는지 의문임
          + OpenAI의 경영진이 상당히 순진하다고 느껴짐
     * 최근 며칠 전에는 OpenAI가 더 많은 컴퓨팅 자원을 확보하기 위해 Google Cloud를 이용한다는 소식이 있었음 (관련기사). Microsoft가 OpenAI에 투자했을 때는 Azure만 쓰기로 되어있던 것으로 기억함
          + Azure도 Coreweave로부터 서버자원을 임대함. Coreweave와의 직접 계약도 취소했다고 알고 있음. 이 정도 규모의 기업이 클라우드 자원을 다양화하는 건 당연한 선택임
     * OpenAI가 Windsurf의 IP를 Microsoft에 넘기기 싫어한다는 기사 내용을 보고, 예전에 Sam Altman이 지식재산권 자체를 없애자고 정부에 요구한 적이 있다고 상기함
          + 이런 사례들은 결국 아무런 원칙도 없다는 점을 보여줌. 실제 결정 기준은 '수익에 도움이 되는가', 혹은 '내 지위를 높일 수 있는가'에 한정됨
          + 그런 주장은 자기 자신을 위한 것일 뿐 모두를 위한 게 아님
     * OpenAI가 스스로를 혁신적인 작은 회사인 척 포장하면서, 자신들이 트릴리언 달러 기업으로 성장하는데 방해되는 건 전부 반경쟁적이라고 주장하지만, 실제로 받은 투자금은 매출 대비 압도적으로 큼. 오히려 반경쟁적이란 건 Softbank, MS, 그 외 투자자들이 한 기업에만 수십억 달러씩 몰아줘서 다른 스타트업들을 묻히게 만든 점임. 정부가 어딘가 개입한다면 OpenAI 추가 투자 자체를 막는 쪽이 효과적일 수 있음
     * 만약 OpenAI가 Azure에 묶여있지 않았다면 어땠을지 상상하는 입장임. 클라우드마다 최고의 서비스들이 있는데, 여러 클라우드를 함께 쓰는 전략이 비현실적인 꿈이 아니라고 봄
          + OpenAI 정도 자금력이면 다양한 클라우드에 맞춤형 추상화 계층을 개발할 수 있음. Kubernetes 복잡성을 도입하든 말든 상관 없이 말임. 참고로 내부 코드네임은 Goober Yetis였음
          + 실상 OpenAI는 '누군가의' 컴퓨팅 자원, 즉 가장 싼 곳에 얽매임. 자체 하드웨어는 거의 없는 것으로 앎. 초대형 클라우드 기업들이 GPU와 데이터센터를 선점하고 있어서, 결국은 가장 할인폭 큰 곳에 파트너를 두는 수밖에 없음. OpenAI가 필요로 하는 컴퓨트 규모는 다른 요소들을 압도하는 수준임
          + OpenAI는 Azure에 묶여 있지 않음 Stargate LLC 위키
"https://news.hada.io/topic?id=21553","Hacker News 유저 2,000명을 클론해 바이럴 게시물 예측하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Hacker News 유저 2,000명을 클론해 바이럴 게시물 예측하기

     * 실제 Hacker News 댓글 데이터를 기반으로 만든 1,903개의 AI 페르소나를 이용해 게시물 제목의 바이럴 여부를 예측했고, 60%의 정확도를 달성함
     * 이는 무작위 추정보다 20% 높은 성과로, 기존 시장조사 대체 가능성도 열어줬지만, 사회적 확산(dynamics)이 예측을 어렵게 만드는 주요 원인임이 드러남
     * 예측 실패 사례 분석을 통해 초기 몇 개의 업보트가 전체 결과를 바꿀 수 있는 운과 타이밍의 영향력이 확인됨
     * 이 결과는 AI가 개별 취향은 잘 모델링할 수 있어도, 바이럴 성공 여부는 예측 한계에 부딪힌다는 사실을 시사
     * 저자는 AI 시장조사는 정확한 예측보다는 방향성 탐색에 유용하며, 상대 평가와 반복 시뮬레이션 중심으로 활용할 것을 조언
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

실험 개요 및 결과

  실험 방법: AI로 구성된 가상 Hacker News 커뮤니티

     * 2025년 3월 12일에 올라온 1,147개의 Hacker News 게시물 제목을 수집
     * 실제 유저 댓글 기반으로 1,903개의 AI 페르소나 생성, 각 페르소나에게 ""해당 제목에 업보트를 할지""를 질문
     * 실제 상위 게시물과 하위 게시물을 섞어 보여준 후, 예측 정확도를 측정
     * 예측 성공률은 60% 로, 무작위(50%)보다 유의미하게 높은 성과를 보임

  예측 실패의 사례와 한계

     * “Gemma 3: Google’s new multimodal models”는 AI가 바이럴로 예측했지만 실제로는 4업보트에 그침
     * 반면 동일 주제를 다룬 “Gemma 3 Technical Report [pdf]”는 1,324업보트를 기록
     * 또한, “TSA finds live turtle in man’s pants”와 같은 자극적인 제목도 AI는 성공 예측했지만 실제로는 실패

    원인 분석

     * 바이럴 여부는 개별 콘텐츠의 품질보다는 초기 노출, 업보트 수, 타이밍 등 사회적 맥락에 크게 의존
     * Princeton 연구: 동일한 곡 리스트를 그룹에 따라 다르게 노출했더니, 일부 곡은 대성공하거나 완전한 실패를 반복
     * 결론: ""좋은 콘텐츠가 성공한다""기보다 ""운 좋게 노출된 콘텐츠가 더 성공한다""는 네트워크 효과의 지배

실용적 시사점: AI 시장조사의 활용 방식

     * AI 페르소나 예측은 완벽하진 않지만 '쓸 만한 정확도(60%)' 를 제공
          + 일정 규모 이상의 조직, 그리고 클라우드 비용이 큰 비중을 차지하는 조직이라면, 모두 여러 클라우드 운용을 대비한 추상화 계층을 만듦. 그래야 협상력을 얻고 여러 클라우드 사업자들을 경쟁시켜 더 좋은 조건을 이끌어낼 수 있음
     * 다른 AI 시장조사 툴들이 주장하는 90%대 정확도는 설문 데이터 기반이라 현실적 바이럴 예측과는 차이가 큼

    실전 적용 전략
     * 이런 대기업들이 서로 반독점 신고를 하면서, 정작 사용자들이 데이터에 대해 소유권이 있어도 AI를 자유롭게 커스터마이징할 수 없게 만들어놓은, 아주 노골적인 반경쟁적 법적 조항을 가진 상태라는 점을 상상함

     * 예측이 아니라 반복 실험과 방향 탐색 도구로 활용할 것
"
          + 예: 10개 헤드라인을 테스트해 버릴만한 후보를 걸러냄
     * 반복 시뮬레이션을 통한 검증
          + 예: 8번 중 6번 이상 높은 평가를 받는 콘텐츠는 실험 가치 있음
     * 절대값보다 상대순위 비교에 집중
          + AI는 뚜렷한 강약 구분은 잘하지만, 비슷한 콘텐츠 간 예측은 어려움

직접 실험해보기: HN 유저 복제 프롬프트

     * Hacker News 유저의 댓글 페이지에서 텍스트를 복사해, 아래 프롬프트로 ChatGPT나 Claude에 입력하면 가상의 페르소나를 생성할 수 있음.

     You are a helpful assistant that creates detailed personas representing a specific HackerNews user from a list of HackerNews comments they have made. Create a unique persona who would give identical answers to the user we are replicating based on their comments. Give them a relevant background and experience based on your best inference from their HackerNews comments...
     * 생성된 페르소나에 대해 여러 콘텐츠 아이디어를 테스트할 수 있음
     * 이 방식은 통계적으로 보정된 예측은 아니지만, 비용 없이 방향성 탐색이 가능한 유의미한 툴로 활용 가능

결론

     * AI는 개별 유저의 반응은 꽤 잘 모델링할 수 있지만, 바이럴 성공은 사회적 확산이라는 '혼돈'의 영향을 받음
     * 결국 콘텐츠 예측에서 AI는 예언자가 아닌, 방향성 가이드 역할에 머물 수밖에 없음
     * 그럼에도 불구하고, 이 실험은 소규모 팀이나 개인도 AI로 시장조사를 저비용으로 해볼 수 있다는 가능성을 열어줌
"
"https://news.hada.io/topic?id=21473","Google Cloud 장애 보고서 – 2025-06-13","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Google Cloud 장애 보고서 – 2025-06-13

     * 2025년 6월 12일, Google Cloud와 Google Workspace 서비스에서 외부 API 요청 중 503 오류가 전 세계적으로 증가함
     * 오류 원인은 Service Control 시스템의 코드 변경과 정책 데이터에 빈 필드가 포함된 잘못된 정책 반영임
     * 핵심 바이너리의 에러 처리 미흡과 기능 플래그 미적용 등이 문제 확산을 키웠음
     * 복구는 2~3시간이 소요되었으며, us-central-1 지역은 인프라 과부하로 더 긴 복구 시간 발생함
     * Google은 아키텍처 분리, 에러 처리 개선, 데이터 검증 강화 등 재발 방지 대책을 발표함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

전체 장애 개요

  Google Cloud 및 Google Workspace 서비스 장애 요약

     * 2025년 6월 12일 오전 10시 49분(PDT)부터, Google Cloud, Google Workspace, Google Security Operations를 포함하는 여러 서비스에서 외부 API 요청에 대해 503 오류가 급증하는 현상 발생함
     * 고객 서비스와 신뢰에 심각한 영향을 주었음에 대해 Google 측은 깊은 사과 의사를 밝힘
     * Google API 관리 및 제어 플레인은 각 요청의 정책 및 쿼터 체크를 담당하며, 핵심 체크 시스템은 ‘Service Control’이라는 바이너리로 동작함

장애 원인 분석

  변화된 시스템 구조 – Service Control

     * 2025년 5월 29일, Service Control에 쿼터 정책 검사를 강화하는 신규 기능이 추가됨
     * 지역별로 단계적 출시를 진행했으나, 문제의 코드는 정책이 실제로 반영되었을 때만 동작하며, 기존에는 트리거되지 않아 사전 테스트가 미흡했음
     * 해당 신기능 경로에 적절한 에러 처리와 기능 플래그가 부재하여, null 포인터 상황에서 바이너리가 연쇄적으로 크래시됨

  장애 발생 경위

     * 2025년 6월 12일 오전 10시 45분(PDT), 정책 변경이 Regional Spanner 테이블에 삽입됨
     * 이 정책 데이터에는 의도하지 않은 빈 필드(Blank Field) 가 포함되어 있었으며, 이것이 전 세계적으로 거의 실시간 복제됨
     * Service Control이 이 정책을 처리하면서 null 포인터에 의한 크래시가 발생, 각 지역 인스턴스가 전역적으로 Crash Loop에 빠짐
     * 2분 만에 SRE팀이 인지를 시작했고, 10분 내에 원인을 파악 후 임시로 바이너리 경로를 차단(red-button), 40분 만에 대부분의 지역은 복구됨

  추가 복구 이슈

     * 일부 대형 지역(us-central-1)은 Service Control 태스크 재시작 시 herd effect로 인프라(Spanner 테이블)가 과부하됨
     * Service Control이 무작위 지수적 백오프를 적용하지 않아 인프라 부담 가중됨
     * 해당 지역은 2시간 40분까지 복구 지연, 트래픽 우회 등으로 영향 최소화했으며, 전체적으로 서비스 복구 완료됨

고객 영향 및 장애 범위

     * 고객은 API 및 사용자 인터페이스 접속 장애 발생, 스트리밍 및 IaaS 리소스에는 영향 없음
     * 지연 및 백로그 영향은 최대 1시간 이상 일부 서비스에서 지속
     * 장애 영향을 받은 Google Cloud와 Google Workspace 제품 리스트가 광범위하게 제시됨
          + 예: IAM, Cloud Build, Cloud Storage, BigQuery, AppSheet, Gmail, Google Drive 등 수십여 개 서비스

향후 개선 방안

     * 서비스 아키텍처를 모듈화하여 각 기능 분리 및 장애 발생시 개방형(fail open) 처리 도입
     * 글로벌 데이터 복제 단계적 전파 및 실질적인 검증 과정 강화
     * 모든 주요 바이너리 변경 시 기능 플래그화 및 기본 비활성 처리 적용 정책 개편
     * 정적 분석과 테스트 개선을 통해 에러 감지 및 장애 시 fail open 가능하게 설계 검토
     * 무작위 지수적 백오프 정책 및 모니터링/커뮤니케이션 신뢰도 강화 예정
     * 장애 상황에서도 고객에게 신속하게 모니터링 및 정보 전달이 가능하도록 인프라 이중화와 자동화 커뮤니케이션 보완

장애 공지 및 커뮤니케이션

     * 사고 후 1시간 이내에 Cloud Service Health에 공지하였으나, 모니터링 인프라 자체도 장애 발생
     * 일부 고객은 Google Cloud 기반의 모니터링 시스템 자체가 정상 작동하지 않아 장애 신호 및 영향 파악 곤란함
     * Google은 향후 모니터링 및 대고객 커뮤니케이션 인프라 강화를 약속함

주요 장애 타임라인 (미니 리포트 요약)

     * 장애 시작: 2025년 6월 12일 10:49 (PDT)
     * 대부분 지역 복구: 2025년 6월 12일 12:48 (PDT)
     * 장애 종료: 2025년 6월 12일 13:49 (PDT)
     * 총 소요: 약 3시간
     * 영향 지역: 전세계

사후 대책 요약

     * API 관리 플랫폼의 데이터 오류나 손상 시 실패 방지 장치 마련 예정
     * 글로벌 메타데이터 전파전 검증·테스트·모니터링 강화
     * 유효하지 않은 데이터에 대한 시스템 에러 처리 및 종합 테스트 확대

영향 서비스 리스트 (발췌)

  Google Cloud 주요 서비스

     * Identity and Access Management, Cloud Build, Google Cloud Storage, Cloud Monitoring, BigQuery, Vertex Gemini API, Cloud Firestore, Looker, Cloud Run, Compute Engine 등

  Google Workspace 주요 서비스

     * AppSheet, Gmail, Google Drive, Google Meet, Docs, Chat, Calendar 등

결론

     * 이번 장애는 정책/쿼터 관리 시스템 구조, 데이터 무결성 검증 부족, 에러 처리 체계 부재가 복합적으로 작용한 문제임
     * Google은 아키텍처 레벨에서의 개선 및 장애 대응력 강화를 약속함

   GN+가 아닌 버전 글 링크입니다.
     * https://news.hada.io/topic?id=21447

        Hacker News 의견

     * 내부자 입장이라 임시 계정 사용 중임, 이번 사고의 근본 원인은 리더십이 속도를 내기 위해 원칙을 무시한 것임, 이런 관행이 수년 동안 지속되어 결국 한계에 도달하게 됨, 이번에 일어난 ‘query of death’는 서버가 특정 쿼리로 인해 크래시 나는 오래된 C++ 서버에서 불가피하게 발생하는 전형적인 실패 유형임, Service Control은 C++로 작성되었고 다양한 엔지니어링 가이드라인을 활용해 이런 실패를 최소화하려고 노력했음, 10년 동안 큰 사고 없이 운영되었으나 최근 리더십 압박 아래 빨리 만든 글로벌 쿼터 정책이 문제였음, 이런 신규 기능은 별도 서비스로 개발하거나 최소한 기존 가이드라인을 준수했어야 함, 공식 보고서에 언급된 대책보다 팀이 원래 지켜온 표준이 훨씬 높음, 팀은 할 수 있는 한 기존 기준을 유지하려고 노력하고 있음
          + 모든 책임을 리더십에만 돌릴 수 없음, 글로벌 전체 반경으로 배포를 극도로 신중히 검토하지 않은 것은 엔지니어링 문화의 실패임, 최소한 글로벌 정책은 지역별 서비스 컨트롤 배포보다 먼저 진행했어야 함
     * 사고 보고서가 흥미롭다는 생각임, SRE 팀이 2분 만에 빠르게 대응하고, ‘red button’ 롤아웃이 시작됐음, 문제는 us-central-1처럼 대규모 리전에서 Service Control 태스크가 재시작될 때 인프라(Spanner 테이블)에 과부하가 걸리는 ‘herd effect’가 나타났다는 점임, Service Control에 무작위 지수 백오프가 구현되어 있지 않아 2시간 40분까지 완전한 복구가 지연됨, 이런 상황에서는 정상적인 쿼터가 빨리 초과되며 새로운 장애로 이어짐, 이런 경우 인프라가 견딜 수 있다면 쿼터를 임시로 중단하거나 복구 작업을 느리게 제한하는 것도 좋다고 생각함
          + 이 상황에서는 지수 백오프가 적합한 대책이 아니었음, 서버 시작 시 중요한 데이터를 읽을 때 의도적으로 재시도 없이 처리되므로 백오프 적용이 안 됨, 더 좋은 방법은 기존에 존재하는 백업 데이터베이스로 부하를 빠르게 분산하는 것임, 다른 방법도 있음
     * 이건 정말 아마추어 같은 실수라는 생각임, NPE, 에러 핸들링 없음, 지수 백오프 없음, 테스트 커버리지 없음, 스테이징 환경에서 테스트 없음, 점진적 롤아웃 없음, 모두 심각한 실패임, SRE 책에 이미 다 나와 있는 내용임 Google SRE Book 목차, Building Secure and Reliable Systems TOC, 기준이 너무 낮아진 것인지 아니면 책이 그냥 마케팅 용인지 궁금함
          + 내 생각에는 인간이든 자동화든 방어책에 완벽함이란 없으며, 결국 트레이드오프의 연속임, 아무리 많은 유닛 테스트와 통합 테스트, 정적 분석, 순차적 배포를 해도 규모가 커지면 예기치 못한 빈틈이 생길 수밖에 없음, 책에서 말하는 “또 다른 9를 추가하는 데 훨씬 더 많은 노력이 드는 것”과 동일한 이유임, 최악의 경우 전체 스택을 복제해서 수개월치 트래픽을 모두 재생해야 할 수도 있는데, 이런 수준의 비용/혜택은 아무도 감당하지 못함, OpenZFS 작업에서도 코드 자체는 정상적으로 보였지만 10년 전에 쓰여진 데이터 엣지 케이스에서 문제가 드러난 경험 있음, 시스템이 충분히 복잡해지면 모든 변형을 테스트하는 건 불가능해지므로 현실적으로 비용 대비 효용을 기준으로 결정할 수밖에 없음, 참고로 구글 SRE지만 이 사건과는 관련 없는 팀이며
            개인 의견임
          + 거의 모든 구글 글로벌 장애가 이와 비슷하게 전개됨, 즉 빠르게 글로벌로 배포된 맞춤 시스템이 잘못된 설정을 반영받음, 일반적인 바이너리 롤아웃이나 설정 푸시에선 점진적 롤아웃이 적용되는 게 보통임, 구글 클라우드에서도 예전엔 다양한 시스템이 글로벌로 묶여 있었으나 이젠 상당히 지역화되고 신뢰성이 높아짐, 이전에는 글로벌 장애가 종종 있었지만 공개되지 않아 대부분 사용자들은 자신의 ISP 문제로 인식했었음, 현재가 특별히 악화되고 있다고 생각하지 않음, 추가로 SRE 경험 있음
          + 외부인 입장에서 생각해보면 대규모 구조조정, CEO의 '게으름' 발언 등 이후로 모두 품질보다는 속도와 가시적 성과에 치중하게 됨, 점차 이런 문화를 문제 삼으면 되려 배척 받는 환경 변화가 생김
          + 더 자세한 정보가 공개되었으면 함, 본인은 여기에 언급된 대로 테스트가 안 된 게 아니라, 정책의 빈 필드(문제 되는 입력)에 대한 테스트가 없었다고 봄, 스테이징 환경에서 테스트가 없다는 설명도 아니고, 플래그가 있었다면 잡았을 것이란 언급임, 개인 의견임
          + “가장 위험한 도구도 익숙해지면 조심성을 잃고, 지침이 불필요하게 엄격하다고 여기게 된다”는 1864년 화약창고 보고서가 생각남
     * 나는 Cloud 내 다른 팀 소속임, 일반적으로 모든 코드엔 유닛 및 통합 테스트가 있음, 바이너리나 설정 변경은 작업별, 지역별로 며칠에 걸쳐 점진적으로 이루어짐, 카나리아 분석도 동반함, 심지어 롤백도 너무 급하게 하면 오히려 상황 악화 가능성 있어 천천히 진행함, 예를 들어 전역 데이터베이스를 일시에 과부하시키기보단 40분 정전이 4시간 장애보다 낫다고 판단함, 이번 사고에 직접 관여하지 않았지만 PM을 보면 코드는 테스트되었으나 이 엣지 케이스는 누락된 것임, 쿼터 정책 설정이 바이너리나 설정 파일이 아닌 데이터베이스 업데이트로 적용되어 전세계 모든 데이터베이스에 몇 초 만에 변경이 번진 것이 문제임, 널 포인터 문제는 다른 언어에도 assert()로 발생했을 가능성이 있음, 이런 핵심 서비스를 다른 언어로 재작성하는 위험보다 모든 정책
       점검에 플래그 가드 적용, 쿼터 정책 점검은 오픈 실패, DB 변경을 지역별로 천천히 확산시키는 쪽이 더 합리적임, 개인 의견임
          + assert는 정책적으로 금지하기 훨씬 쉬운 구조임
          + 코드는 테스트됐지만 이 엣지 케이스는 누락된 거니까, 결국 테스트 안 한 셈이라는 반론임
          + DB 변경이 바이너리나 설정 변경이 아니라고 해서 문제가 달라지는 건 아님, 변경 자체가 동시에 전역에 퍼지면 어떤 종류든 재앙의 씨앗임, Crowdstrike 사태와 똑같은 흐름임
          + “언어를 바꿔 재작성하는 게 위험 부담이 너무 높다”는 의견이라면, 즉 서비스 요구사항이 제대로 파악되어 있지 않거나, 서비스가 신중한 마이그레이션이 필요할 만큼 중요하지 않은 것인지 궁금
     * 적절한 에러 핸들링 없이 널 포인터로 바이너리가 크래쉬 됐다는 내용임, 이쯤이면 '트릴리언 달러 실수'라는 농담이 나올 만함
          + 이번 사고로 몇 개의 SLA를 한 해 동안 날려버렸을지 궁금함
          + 이런 문제를 막아줄 언어가 있었다면 하는 생각에 /s(비꼬는 투)임
     * Service Control(Chemist)이 꽤 오래된 서비스이며, 여러 GCP API에서 인증, 권한, 감시, 쿼터 등 핵심적 역할을 함, 요청 흐름상 대부분의 GCP API가 Chemist를 경유함(그래서 fail open 완화책이 실효적이지 않다고 봄), Chemist와 프록시 모두 C++로 작성되어 있으며 수년간 레거시 코드가 많이 쌓였음, 각 팀은 정적 분석, 테스트, 점진적 배포, 피처 플래그, red button, 강력한 모니터링 및 경보시스템을 갖추고 있음, 특히 SRE팀은 뛰어남, Chemist에서 IAM, 쿼터 등 다양한 정책을 검증하다 보니 여러팀이 코드베이스에 기여함, 변경할 때마다 Chemist 승인 과정을 거치지 않으려고 단축 경로로 개발한 부분이 많아짐, 최근엔 조직 개편과 오프쇼어가 많아서 L8/L9급 주도 화려한 신규 프로젝트에 치중, 정작 품질·유지보수·신뢰성은 후순위가 되었음(이런 문화 변화 때문에 Cloud에서
       떠났음), 구글 일반 서버/서비스 모범 사례가 여기선 지켜지지 않을 때가 많음, 이번 이슈는 코드와 코드리뷰의 부실 쪽에 가까움, 결함이 있던 코드를 승인하고 Spanner로 설정 변경을 즉시 반영시켜 문제가 커짐
     * 서비스 정책 데이터에 의도치 않은 빈 필드가 포함되었고, Service Control이 각 리전별로 쿼터 체크 시 빈 필드(널)를 읽어 예외가 발생했음, Hoare의 ‘십억 달러 실수’가 여러 구글 시스템에서 반복된 예시임, 애초에 이런 ‘빈 필드’(null)가 들어갈 수 있도록 허용한 게 문제인데, 스키마에서 null 불허(‘NOT NULL’)를 명시했어야 함, 불행히도 Spanner에서 기본값이 nullable이라 별도 지정 필요, 앱 코드 레벨에서 타입 시스템이나 스키마 언어를 통해 유효하지 않은 상태가 불가능하도록 설계할 기회가 또 있었음, 데이터스토어에서 앱 객체로 역직렬화하면서 스키마 강제 검증을 추가하는 방법도 있음, 본문 이슈가 새로운 코드 경로에서 터진 만큼 데이터 계층에서 걸러지지 않은 게 아닌가 싶음, Service Control 프로그램 자체가 널 포인터 참조를 허용하는 언어로
       작성된 것도 문제임, 만약 내가 관리 담당자라면, 정책을 앱 코드에서 ‘태그드 enum 타입’ 등 null을 표현할 수 없는 구조로 바꾸는 최소 개입 플랜을 생각해보겠음, proto3에는 이런 제약이 없으나, 이런 예시는 있음
     * 멀티리전이 회복력과 가용성 수단으로 자주 언급되지만, 실제로는 장애 상황에서 대형 클라우드 제공자도 리전 간 강하게 얽혀 있다는 게 흥미로움
          + 이건 특히 GCP에서 두드러지는데, GCP는 리전을 타사보다 다르게 다룸, 회복성 관점에선 GCP를 다수 존이 묶인 단일 리전처럼 봐야 함
          + 그럼에도 불구하고 “우리가 모르는 장애”는 여전히 있을 수 있으므로 리전/존 샤딩의 효과를 지나치게 과소평가하면 곤란함
          + 멀티리전 배포 덕에 사고를 예방한 사례도 많으니 실제로 그런 케이스를 파악한 후 결론을 내리는 게 맞음
     * 구글 포스트모템의 디테일에 늘 놀람, 회사 안팎 모두에서 느껴짐, 실수 반복하지 않도록 배우고, 프로토콜과 에러 핸들링 보강해 더욱 강인한 시스템으로 진화함, 구글처럼 규모가 큰 곳에선 항상 무언가가 잘못되고 있지만 핵심은 고객/사용자와 다른 시스템에 영향 가지 않게 차단하는 것임, 내부에 있더라도 팀에 따라 보이지 않는 이슈가 다양함, 이는 인간이 만든 시스템 중 가장 복잡한 축에 속한다고 봄, AGI가 아니라면 인간이 더 잘할 수 없는 영역임
          + 하지만 이번 사고엔 주니어급 실수가 연달아 일어남, null 데이터 처리를 못 한 점, 테스트 부족, 신기능에 대한 테스트 커버리지 없음, 전면 배포 전 소규모 프로덕션에서 검증하지 않은 점 모두 문제임, 10년 전 구글에서라면 이런 실수에 다들 비웃었을 거라고 확신함
          + 내 이해로는 이번 장애 원인은 1) 전역적 기능이 한번에 전체 배포됨 2) 널 포인터 역참조 발생 3) 적절한 재시도 정책 부재로 'thundering herd' 문제 야기, 모두 업계 사람이라면 익숙한 실수임, 신기하거나 복잡한 분산 시스템 로직이 아니라 전형적인 ‘초보 실수’가 복합적으로 터진 셈임
          + “다시는 같은 실수 반복하지 않는다”는 말이 있지만, 실제론 피처 플래그 없이 변경을 롤아웃했고, 클라이언트엔 지수 백오프도, 서버에 로드 셰딩도 구현하지 않았음, 이 모두가 수년 전부터 google SRE book에 나온 내용임
          + 이 오류는 미처 잡지 못한 널 포인터 문제였음, 구글 정도 규모와 품질을 가진 회사가 이런 식으로 스택 대부분을 다운시키는 것은 심각한 이슈 재발 방지책이 부족하다는 신호로 읽힐 수 있음
          + 이건 수없이 반복된 동일한 실수임, “새 기능을 조심스럽게 배포하지만, 새로운 데이터가 들어와야만 드러나는 버그”가 대부분의 글로벌 장애를 요약하는 문구임, 완벽한 시스템은 존재하지 않음, FAANG 장애 논쟁에서만 무결점인 건 ‘HN의 armchair 전문가’뿐임
     * 대부분, 남의 다운타임을 볼 땐 ‘주니어급 실수’라고 쉽게 비판하지만, 정작 자기 일에선 불가피했다, 예측불가했다는 핑계가 남음, 인간 실수는 불가피하며 기대치 자체가 너무 높음, 오프라인 가게가 갑자기 문 닫아도 “죄송하다” 한마디에 끝남, 오직 IT 업계만 몇 시간짜리 장애로 과도하게 스트레스를 받는 데 모두 조금 더 여유를 갖길 바람
"
"https://news.hada.io/topic?id=21440","만약 달이 1픽셀이라면: 지루할 정도로 정확한 태양계 모델 (2014)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                만약 달이 1픽셀이라면: 지루할 정도로 정확한 태양계 모델 (2014)

     * 이 콘텐츠는 달의 크기를 1픽셀로 설정해, 태양계의 실제 간격을 시각적으로 표현함
     * 행성과 위성들은 무척 작은 크기로, 이들 사이의 방대한 공간이 강조됨
     * 우주 대부분이 공허 그 자체임을 직접적으로 체감할 수 있음
     * 비유와 수치를 통해 인간이 쉽게 이해할 수 없는 거대한 규모를 설명함
     * 이 모든 허무함 속에서 존재의 의미와 특별함을 되새기는 메시지를 전달함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 픽셀로 그린 태양계

     * 본 프로젝트는 '달의 크기가 1픽셀' 이라는 기준으로, 태양계의 실제적 거리와 비율을 정확하게 시각화함
     * 행성, 위성, 태양, 그리고 이들 사이의 막대한 공간이 픽셀 단위의 긴 스크롤로 보여짐

태양계 행성과 위성의 간격 체감

     * Mercury, Venus, Earth, Moon을 지나며 각 행성과 위성의 위치와 상대적인 거리가 소개됨
     * You Are Here 등 재치있는 안내 텍스트로 사용자가 현재 위치를 파악하도록 유도함
     * Mars, Jupiter(및 위성 Io, Europa, Ganymede, Callisto), Saturn(Titan 포함), Uranus, Neptune, Pluto 등 태양계 천체가 순서대로 배치됨
     * ""That was about 10 million km just now."" 등 구간별 안내로 실제 거리를 실감할 수 있도록 설명함

광대한 우주의 공허함

     * ""Pretty empty out here"", ""Most of space is just space"" 등 대부분 공간이 비어 있음을 강조함
     * 목적지(예: Mars)까지의 시간이 수개월 소요됨을 언급, 스크롤의 긴 구간에서도 지루함, 허무함을 체험하게 설계됨
     * 만들기쉬운 행성 그림과는 달리, 공간의 구현이 훨씬 어렵고, 대다수의 우주 지도에서는 이 빈 공간을 생략함

대규모, 추상적 수치에 대한 인간의 한계

     * ""We're used to dealing with things at a much smaller scale than this"" 등 소우주적 사고방식 설명
     * 축소 비유(예: 축구장 크기, 475피트 인쇄지 등)로도 방대한 규모를 쉽게 감각할 수 없음을 지적함
     * 시간을 드는 변화(물방울의 협곡 생성, 아메바의 진화 등)에서 작은 점들이 가진 특별함에 주목함

공허함의 철학적·신경학적 고찰

     * ""It’s easy to disregard nothingness..."" 등 공허함을 쉽게 무시하게 되는 인간 심리 언급
     * 뇌의 진화적 한계 아래, 인간은 극소수 물질과 에너지만 오감으로 인식 가능함
     * 수학적 모델 등 추상화가 활용되긴 해도, 실제 감각적 이해는 부족함을 지적함

비유와 대조: 존재의 의미 재발견

     * 원자 역시 대부분 빈 공간으로 구성됨을 예시로 설명함
     * ""Universe is a whole lotta nothing""이라는 표현으로 압도적 공허를 언급
     * 99.999...%가 빈 공간임을 들어, 우리 존재의 의미와 특별함을 역설함
     * ""So much emptiness makes the tiny bits of matter that much more meaningful"" 등, 적은 존재일수록 의미가 커짐을 강조함
     * ""It seems like we are both pathetically insignificant, and miraculously important at the same time"" 등 양가적 의미를 전달함

마무리: 존재와 우주의 신비

     * 우리의 존재 자체가 이 공허 속에서 놀라운 일임을 강조함
     * 축하의 메시지와 함께, ""This is how fast light travels..."" 등 광속의 한계를 언급하며 상상력을 자극함

전체 메시지

     * 이 지도는 물리적 재현의 한계와, 그 한계 너머의 존재 의미를 동시에 일깨움
     * 인간의 사고와 상상력으로 방대한 우주의 본질을 감각하기 어려운 점을 유머와 철학적 성찰로 풀어냄
     * 결과적으로, 작은 점들의 소중함과 허무함을 뛰어넘는 특별함을 다시금 생각하게 하는 경험 제공

        Hacker News 의견

     * 오른쪽 아래에 있는 ""c"" 버튼을 꼭 눌러보라는 추천 내용 전달. 빛의 속도가 너무 느려서 모든 것이 멀게 느껴진다는 생각 공유. 다른 별에 도달하기 전 홀로덱 같은 가상현실 체험이 먼저 가능해질 것 같다는 기대감, 어쩌면 그것만으로도 충분할 수 있다는 의견 전달
          + 빛이 느린 걸까, 아니면 우리가 몸집이 작고 신진대사가 빨라 시간 감각이 극도로 미세하게 맞춰져 있을 뿐일까라는 의문 제기. 역사적으로 사람들은 식물이 생명력이 없다고 착각하지만, 식물은 단지 인간이 감지할 수 없는 아주 장기간에 걸쳐 반응한다는 점 설명. 타임랩스로 보면 식물이 얼마나 역동적으로 살아 있는지 알 수 있다는 예시 제시. 식물조차도 수명이 꽤 짧은 편에 속하고, 가장 오래 산 식물도 신석기 시대 때부터 대략 14000년 정도임을 비교. 인간에게 1000년은 긴 세월이지만, 나무에는 한 그루가 그 10배를 사는 것도 가능함. 별의 관점에서 수십억 년의 생애를 가진 태양이라면, 지구의 생태계 변화는 눈 깜짝할 사이인 느낌 설명. 예를 들어 '범아시아 그린벨트' 시기도 얼마 안 된 일 같고, 판게아도 별로 오래된 일이 아님을 언급. 이런
            천문학적 시간 척도에서는 대륙이 움직이는 것이 인간에게 보트가 움직이는 것처럼 느껴질 수 있음. 우주의 거리감이 피로하게 느껴지는 관점은 결국 이런 별의 시간 척도임을 암시. 14000년의 여행도 태양 입장에선 마트에 10분 다녀오는 일과 다름없다는 철학적 상상 공유
          + 이 모델에서만 빛이 엄청 느린 것처럼 보이는 이유는 특수상대성이론을 반영하지 않은 탓이라는 지도. 실제로 빛의 속도에 가까워질수록 로렌츠 수축이 일어나 목적지가 훨씬 더 가까워 보이게 된다는 설명. 이론적으로는 어떤 곳이든 자신의 기준시간에서는 임의로 짧은 시간 안에 도달 가능함. 물론 그 과정에서 중력가속도(G-force)로 인해 살아남을 수 있을지는 별개의 문제라는 점 언급
          + 빛의 속도가 너무 느리고 세상이 너무 멀게 느껴져 약간 우울하게 느끼는 감정 공유. 그저 크기만 거대하고 완전히 손에 닿지 않는 우주에 대한 아쉬움 표현
          + 어쩌면 빛이 엄청 빠르고, 공간이 그만큼 거대한 것일 뿐이라는 의견. 결국 모든 것은 상대적이라는 유쾌한 관점 제시
          + 홀로덱이 별에 도달하는 것보다 먼저 가능할 것이라는 의견에 동의하지만, 그 이유는 기술적 난이도 때문이 아니라 우리가 오히려 우주탐사보다 엔터테인먼트에 훨씬 더 많은 돈을 쓰기 때문이라는 해석 제시
     * HTML/CSS가 아주 단순하고 깔끔해서 인상적이라는 소감 공유. absolute positioning에 left값만 엄청 크게 줬다는 예시 코드 소개
          + Brave 브라우저 iOS 버전에서 해당 사이트로 인해 브라우저가 크래시났다는 경험 공유. 신형 iPad mini에 램도 12GB인데도 탭을 닫으려고 할 때 충돌 발생
          + 이런 식의 큰 값이 옛날 Internet Explorer 브라우저에서 문제를 일으킬 수 있음을 경고
          + px 단위 자체가 놀랄 만한 수준의 복잡한 추상화 위에 동작할 수 있다는 상상. 웹 기술이 다시 원점으로 돌아온 듯한 느낌 전달
     * 관련된 다른 토론글과 링크 소개. “If the moon were only 1 pixel” 시리즈와 대형 태양계 모형 등 여러 HN 토론 및 참고자료 아카이브 링크 모음 제공
     * 컴퓨터에서 이런 시각화가 가능해지기 전 초등학교 시절, 비닐 롤로 지구의 역사를 체험했던 기억 소개. 복도에서 롤을 펼치면서 시대별로 이동하는데, 인류의 등장은 몇 발짝밖에 안 되고, 캄브리아기에 도달하면 이미 복도 끝에, 행성 형성까지 가려면 학교 운동장을 넘어가야 했던 크기감 전달
     * 자신이 만든 192 바이트로 태양계를 정확히 시각화한 작품을 자랑스럽게 소개 (https://www.dwitter.net/d/26521). 빨간 점이 태양이고, 픽셀 당 1000킬로미터, 초 당 1000초 스케일 적용. 먼 외계 행성에서 망원경으로 본 듯한 궤도평면을 통한 정사영 방식 사용. 평균 공전 반경과 항성일 기준 주기만 반영했고, 실제로는 약간의 섭동이 있다는 점 안내
          + 코드가 정말 믿기 어려울 정도로 흥미롭다는 감탄. 동작 원리가 궁금하다는 질문 추가
     * 수년이 지나도 여전히 놀라운 경험이며, 지금까지 본 최고의 수평 스크롤 활용 사례로 칭찬. 과거의 ‘if moon only 1 pixel’ 관련 다양한 논의와 링크 아카이브도 함께 추천 (https://hn.algolia.com/?q=if+moon+only+1+pixel)
          + 해당 사이트가 굉장히 인상적이고, ‘moon = 1-pixel’ 시각화에서 영감을 받았다고 추정되는 다른 좋은 작품(https://hmijail.github.io/1-pixel-wealth/)도 추천
     * 빛의 속도 토글이 태양계의 허망할 정도로 거대한 빈 공간을 실감하게 해준다는 체험 공유. 실제 지구까지 8분 걸린다는 건 이론으론 알았지만, 태양에서 스크롤로 8분을 멍하니 기다려야 몇 픽셀이 보이는 경험에 놀랐다는 소감
          + 사실 인간의 뇌에겐 빛의 속도도 거의 무한에 가깝기 때문에, 이런 체험조차 여전히 실제 먼 거리감을 충분히 체감하게 해주지 못한다는 언급
     * POV-Ray로 태양계 행성을 스케일에 맞게 그리고, 관련 결과를(https://github.com/susam/pov25#planets) 공유했던 경험 소개. 친구가 왜 행성을 궤도로 돌리는 게 아니냐 질문했지만, 실제 레이트레이싱은 사진처럼 보여주기 때문에 인간 눈에 행성은 작은 점들로 보일 뿐이라는 설명. 정사영 투영은 실제 인간 시야와 달라서 제한적으로 사용. 이런 한계를 가진 정적 이미지와 달리, 인터랙티브 웹 페이지는 행성 크기와 실제 거리감 모두 훨씬 더 잘 전달해준다는 장점 강조
          + 실제 스케일의 궤도를 레이트레이싱으로 표현하려면 double 정밀도가 필요하지 않냐는 물음. 예를 들어 넵튠 거리와 반지름 비율(약 200만 배)에서 fp32 부동소수점 정밀도 문제로 넵튠이 몇 픽셀밖에 안 될 수 있다는 점, 또 다른 난관이 뭔지 질문. 오늘 한번 직접 시도해보겠다는 도전 의지 표명
     * Madison에 23마일(약 37km)에 달하는 태양계 모형이 있다는 사실 공유 (https://www.astro.wisc.edu/outreach/planet-trek/)
     * 자신은 천문학자가 아니지만, 실제로 천체는 모두 타원 궤도를 가지므로 '지루할 만큼 정확하다(=tediously accurate)'고 라벨링하려면 태양과의 거리가 공전 주기 동안 계속 변한다는 사실을 반드시 표시해야 한다는 의견
"
"https://news.hada.io/topic?id=21503","Chawan TUI 웹 브라우저","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Chawan TUI 웹 브라우저

     * Chawan은 텍스트 기반 터미널에서 작동하는 새로운 TUI 웹 브라우저임
     * 이번 0.2.0 버전은 MVP 단계의 모든 필수 기능을 포함하며, 치명적인 버그가 없는 상태임
     * libssh2, libbrotli, OpenSSL/LibreSSL 라이브러리만 필수로 필요하며, 예전 버전에 있던 여러 의존성들이 제거됨
     * amd64 Linux용 정적 바이너리와 .deb 패키지로도 제공되어 배포 및 설치가 용이함
     * 다음 버전에서는 레이아웃 모듈의 성능 개선과 더 나은 UI 경험이 목표임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Chawan 0.2.0 소개

   Chawan은 터미널 사용자 인터페이스(TUI) 기반의 경량 웹 브라우저로, 텍스트 환경에서 간편하게 웹을 탐색할 수 있도록 설계된 오픈소스 프로젝트임. 이번 0.2.0 버전은 최소 실행 제품(MVP) 단계의 모든 주요 기능을 갖춘 첫 안정화 릴리스로, 주요 치명적 버그가 보고되지 않은 상태임.

  배포 및 설치

     * 소스 트리의 tarball이 제공되어 있으며, 직접 컴파일을 원할 경우 README의 안내를 따르면 됨
     * amd64 Linux용 정적 바이너리 배포판이 제공되어, 아카이브 압축 해제 후 make install 명령어로 쉽게 설치 가능함
          + 삭제는 make uninstall로 진행 가능함
     * 동일한 배포판이 .deb 패키지 형태로도 제공되어 Debian 기반 시스템에서 바로 설치할 수 있음

  패키지 관리자를 위한 정보

     * Chawan의 필수 런타임 의존성은 다음과 같음:
          + libssh2
          + libbrotli (libbrotlicommon, libbrotlidec 포함)
          + OpenSSL(3.0 이상) 또는 LibreSSL (OpenBSD 7.7 버전 테스트 완료)
     * 이전 개발 버전에서 사용되었던 zlib, libseccomp, termcap/ncurses, libcurl 등은 더 이상 필요하지 않으므로 의존성에서 제거했음
     * 패키징 시 이슈가 발생할 경우 패치 전에 먼저 개발자에게 연락하면, 문제를 상위에서 직접 해결할 가능성 높음

  향후 계획

     * 이번 0.2.0 릴리스는 기대보다 시간이 더 소요되었지만, 모든 MVP 기능을 갖췄음에 따라 공식 배포를 결정한 버전임
     * v0.2 브랜치는 앞으로 버그 수정만 진행하며, 새로운 기능 추가는 master 브랜치에서 계속될 예정임
     * 다음 릴리스에서는 레이아웃 모듈의 성능 및 정확성 개선과, UI의 사용자 친화성 향상을 중점 목표로 하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트의 중요성 및 장점

     * Chawan은 터미널 환경에 최적화된 웹 브라우저로, 리소스가 제한된 서버 및 환경에서 웹 콘텐츠 접근이 가능함
     * 기존의 텍스트 웹 브라우저와 달리, 최신 암호화 및 압축 프로토콜 지원, 의존성 최소화 등에서 차별점을 가짐
     * 오픈소스 방식이므로 추가 확장 및 맞춤화에 유리함

        Hacker News 의견

     * 나는 nim으로 작성된 프로젝트를 볼 때마다 기분이 좋아지는 감정 경험 nim이 C/C++와 python 사이 그 어딘가에 위치한 가장 흥미로운 언어라는 인상 nim 커뮤니티 규모에 비해 사용자들의 생산성이 높아 내가 잘 모르면서도 nim에 대한 생각이 맞는다는 확신 느낌 nim이 대중적으로 성장하지 못한 점이 아쉬움
     * shiomiru님, TTY와 관련된 개념을 배우기 위한 가장 좋은 리소스가 무엇인지 궁금함 무료든 유료든 추천 부탁 TTY에 대해선 linusakesson.net의 TTY 소개글를 알고 있지만, termcap/terminfo/curses까지 설명하지는 않는 한계 인식 다른 댓글에 O'Reilly의 Termcap & Terminfo 도서 언급됨 (온라인에서 여기서 보기 가능)
          + chawan은 자체 커스텀 터미널 모듈 사용 경험 때문에 표준 X/Open curses에 대한 이해도가 높지 않음 실제 이스케이프 시퀀스 관련해서는 XTerm의 ctlseqs.ms 문서가 매우 유용한 자료라는 인식 nick black의 notcurses 프로젝트에서 아이디어도 많이 얻었고, 특히 ""sprixels""에 대한 그의 노트를 적극 추천
     * Chrome 기반의 또 다른 엔진을 굳이 쓰지 않고 잠깐 연구용으로 쓸 수 있는 새로운 방식이 늘 반갑게 느껴짐 오늘 소개된 내용, 학교 끝나고 직접 테스트해 볼 기대감 sixel 없는 환경에서 특히 도움이 되는 매우 멋진 대체재로 chromium의 terminal port: carbonyl 추천 원작자가 더 이상 적극적으로 개발을 못 하고 있어 기능이 매우 제한적이고 키보드 단축키, 파일 저장 등 여러 기능은 아직 미지원 그러나 Web 호환성과 신선함이 매우 인상적이라는 생각 rust 기술을 가진 사람이 프로젝트를 이어가 주기를 바라는 소망 몇 달 전에 살펴보았을 때 인기있는 fork는 없는 상황
     * 이런 프로젝트가 정말 좋다는 긍정적 감정 텍스트 기반 사이트들을 모아놓은 리스트가 있으면 좋겠다는 바람 개인적으로 좋아하는 두 곳은 plaintextsports.com와 lite.cnn.com
          + 내가 자주 방문하는 텍스트 위주 사이트로는 text.npr.org와 plaintextaccounting.org가 있음
     * 정말 멋짐 Lenovo M8 4세대에서 termux로 직접 소스 빌드해 봤는데 nim만 설치하면 됨 root 유저 없이도 깔끔하게 설치된 점이 인상적임
     * 정말 아름다운 구현 경험 여러 업무에 실사용해서 어디까지 쓸 수 있을지 직접 확인해 보고 싶은 기대감 HN 사이트가 아주 보기 편함
     * 내 웹사이트가 항상 Links에서도 보기 좋게 만드는 노력을 해왔는데, 이제는 새로운 후보가 생겼다는 기대감 일부 CSS 지원 덕분에 요소간 간격 조절 등 고민해야 할 부분도 늘어남 Links에서는 margin/padding을 완전히 무시해 한 줄에 메뉴 등 깔끔하게 나열했지만, 이제는 상황이 더 까다롭게 바뀐 현실
          + 기본적으로 별도 설정 없이 ""그냥 동작하는"" 것이 목표였지만, 현실적으로 가끔 그렇지 않은 경우가 있음 이런 경우 버그로 간주하면 된다는 가이드 grid 레이아웃용 커스텀 CSS가 꼭 필요하다면, chawan이 표준 grid 미디어 쿼리를 지원한다는 정보 전달
     * 정말 멋진 프로젝트라는 감동과 nim으로 만들어진 점에 대한 찬사 느낌 cha example.com 입력 시 기본 명령(hjkl 등) 사용법을 전혀 모르겠는 혼란감 숫자를 누르면 좌측 하단에 표시 되지만, 그 외엔 아무런 반응이 없음 MacOS Sequoia 15.5 Apple Silicon과 Nim 2.24 환경에서 직접 빌드 경험 웹페이지는 잘 불러오지만 명령은 동작하지 않는 상황 혹시 내가 뭔가 놓치고 있는지, 아니면 이게 버그인지 궁금함
          + Ghostty, iTerm2, Terminal.app 등 다양한 터미널에서 모두 같은 현상 경험
     * termcap/ncurses를 더 이상 사용하지 않는 점이 인상적이라는 의견 표출 직접 터미널 처리를 한다는 의미인지 궁금증
          + Chawan은 사실상 ncurses는 사용하지 않고 termcap만 사용했다고 설명 ncurses가 termcap을 내부 구현으로 포함해서 헷갈릴 수 있음 처음엔 w3m을 써봐서 termcap으로 시작했지만, termcap은 이제 거의 쓸모없는 인터페이스이고 최신 터미널에서 필요한 true color도 표현 못함 유일한 장점은 극히 오래된 80년대 터미널에서 '운 좋게' 동작할 수 있다는 점이지만, 이마저도 별 가치 없다는 평가 terminfo로 갈아타는 대신 아예 포기하고 터미널 쿼리 방식을 채택 터미널 DB도 내장돼 있어 일부 XTerm 비호환 TERM 값을 식별하는 용도로만 사용됨 쿼리에 정상적으로 반응하는 터미널이면 TERM 값 상관없이 잘 동작함
     * macOS에서 'make' 명령 한 번만으로 쉽게 빌드된다는 점이 인상적이라는 경험 elinks의 현대적 대체재가 생긴 점이 반가움
"
"https://news.hada.io/topic?id=21474","Andrew Ng: AI 에이전트의 현황 [유튜브]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Andrew Ng: AI 에이전트의 현황 [유튜브]

     * 앤드류 응은 AI 에이전트와 에이전틱(agentic) 시스템 개념을 중심으로, 에이전트 정의 논쟁보다 자율성의 스펙트럼에 집중할 것을 제안
     * 현재 실제 비즈니스 기회는 복잡한 완전 자율 에이전트보다, 단순하고 선형적이거나 작은 분기만 있는 워크플로우에 많음
     * 에이전틱 시스템 설계와 운영에 필요한 실전적 스킬(작업 분해, 평가 시스템, 데이터 연결 등)이 여전히 희소하며, 다양한 도구 활용 능력이 중요
     * 평가(evals), 음성 스택(voice stack), AI 코드 어시스턴트 등이 충분히 주목받지 못한 핵심 도구로 꼽힘
     * 스타트업 성공의 핵심 요인은 실행 속도와 깊은 기술 이해이며, AI 도구 발전으로 더 많은 비(非)개발자도 기본 코딩 역량을 갖추는 게 생산성 향상에 도움
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Introduction

     * 앤드류 응이 LangChain 등 다양한 AI/에이전트 프로젝트와 커뮤니티에 기여한 배경 소개
     * 에이전트 정의 논쟁 대신, 에이전틱 시스템은 자율성의 정도가 다양할 수 있음을 강조
     * ""진짜 에이전트인가""를 따지기보다, 자율성의 크고 작음을 스펙트럼으로 보고 실용적으로 접근할 것을 제안

Opportunities: 실제 비즈니스 기회

     * 실제로 많은 비즈니스 워크플로우는 선형적 흐름 혹은 약간의 분기만 포함
          + 예: 웹 폼 작성, 데이터베이스 조회, 단순 검색 등 반복 업무 자동화
     * 작업 분해 및 미세 조정(마이크로 태스크화), 평가 지표 설계, 워크플로우 개선 등 실무적 역량이 희소
     * 복잡한 에이전틱 워크플로우도 중요하지만, 대다수의 가치 창출은 단순 반복 구조에서 발생

Skills: 에이전트 빌더가 갖춰야 할 역량

     * 비즈니스 프로세스 파악 후, 데이터 수집/통합·프롬프트·프로세스 분할 등 체계적 설계 역량 요구
     * 자동화 평가 시스템(시스템/컴포넌트별 성능 추적, 평가 프레임워크 구축 등)이 중요
     * 경험 많은 팀은 ""불필요한 개선""에 집착하지 않고, 효율적으로 문제 우회/대체
     * 다양한 AI 도구·프레임워크를 실제로 써보며, 결정·시도 속도와 도구조합(레고 블록식 활용) 이 빠름

AI Tools & 변화

     * 최근 2~3년 간, AI 도구(예: Langgraph, RAG, 챗봇, 메모리 관리, 평가/가드레일 등) 생태계가 다양해짐
     * 도구는 레고 블록처럼 다채롭게 결합할 수 있어, 실제 활용 경험이 쌓일수록 빠른 의사결정 가능
     * LLM의 컨텍스트 윈도우 증가로 RAG 등 일부 기법의 실질적 역할 변화—하이퍼파라미터 튜닝의 중요성이 감소

Underrated Tools: 저평가된 핵심 도구

     * Evals(자동화 평가): 많은 팀이 과하게 어렵게 여기지만, 작은 예시부터 빠르게 구현/개선하는 습관이 중요
     * Voice stack(음성 기반 워크플로우): 대규모 기업에서 수요와 활용도가 빠르게 증가하지만, 개발자 커뮤니티의 관심이 부족
     * AI 코드 어시스턴트: AI 지원 코딩으로 생산성 향상, 모든 구성원이 기본 코딩 역량을 익히면 직무별 생산성 개선
          + AI Fund 사례: 리셉셔니스트, CFO, 변호사까지 모두 코딩을 배우며 업무 효율 증대

Voice Application의 특성

     * 텍스트 프롬프트보다 음성 입력이 사용자 부담을 줄이고, 더 빠르게 정보 입력을 유도
     * 음성 기반 에이전트는 반응 시간(지연) 이 매우 중요(이상적으론 1초 미만), 실시간 인터랙션을 위한 다양한 UX 트릭 활용(예: 프리리스폰스, 백그라운드 노이즈)
     * 음성 인터페이스가 적용될 분야와 활용 가능성은 매우 크나, 개발자 도구와 지원 인프라가 더 필요

MCP: 표준화와 데이터 통합

     * MCP(Mesh Capability Protocol): 다양한 데이터 소스·API·도구를 표준화된 인터페이스로 연결하는 업계 트렌드
     * MCP 표준은 아직 초기 단계이나, 복잡한 데이터·툴 통합을 간소화하는 핵심 축으로 발전할 전망
     * n개의 에이전트와 m개의 데이터 소스를 연결할 때 n*m이 아닌 n+m의 비용으로 통합할 수 있게 하는 비전

Agent-to-Agent 시스템

     * 멀티 에이전트, 에이전트 간 상호작용은 매우 초기 단계로, 아직은 같은 팀 내에서만 실질적 성공 사례가 많음
     * 다른 팀/회사 간 에이전트 상호작용은 앞으로의 발전 영역

Vibe Coding 및 AI 코딩

     * AI 어시스턴트와 함께 코딩하는 'Vibe Coding' 현상은 실제로 높은 집중과 지적 노동을 요구, 이름과 달리 ""느낌만""으로 코딩하는 것이 아님
     * AI 코드 어시스턴트 발전으로, 더 많은 비개발자와 직군에서 코딩 역량의 중요성이 높아짐
     * 코딩 학습은 미래 생산성의 핵심—프로그래밍 언어(특히 Python) 한 가지는 익혀둘 것 권장

Advice for Startups: AI 스타트업에 대한 조언

     * 스타트업 성공의 1순위는 실행 속도, 2순위는 기술에 대한 깊은 이해
     * 마케팅/영업/가격책정 등은 중요하지만, 기술의 실질적 작동 원리와 최신 기술 변화에 대한 이해가 더욱 희소하고 중요
     * 깊은 기술적 본질에 대한 감각을 가진 팀이 빠르고 효율적으로 문제를 해결할 수 있음

   MCP(Mesh Capability Protocol) 이 부분은 오타겠죠?
   에이전트에서 어떻게 기능과 권한을 쪼개고 워크플로를 구성하느냐가 성공의 관건인 듯 합니다.

   MCP 가 최근에 대두된 용어라 LLM 이 학습하지 않아서 이상한 단어를 쓰는 것 같습니다 ㅋㅋ
"
"https://news.hada.io/topic?id=21489","한 번 모델링하고 어디서나 표현: Netflix의 UDA (Unified Data Architecture)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      한 번 모델링하고 어디서나 표현: Netflix의 UDA (Unified Data Architecture)

     * Netflix는 비즈니스 도메인(예: 영화, 배우, 시리즈 등)의 데이터 모델링이 각 시스템마다 중복·불일치·비표준 용어로 인해 협업 및 데이터 품질 문제에 직면함
     * UDA(통합 데이터 아키텍처)는 '한 번 모델링, 어디서나 표현'을 목표로 도메인 개념을 한 번 정의하고, 다양한 시스템에 일관되게 투영·연결하는 지식 그래프 기반 인프라임
     * UDA는 도메인 모델→데이터 컨테이너(예: GraphQL, Avro, Iceberg 등)로 자동 스키마 생성·매핑·데이터 이동 자동화를 지원
     * 비즈니스 사용자는 Sphere·PDM 같은 UDA 활용 도구를 통해, 용어 검색만으로 데이터 탐색·보고서 생성·참조 데이터 관리 가능
     * RDF·SHACL 등 시맨틱 기술 위에 자체 Upper 메타모델을 적용하여 대규모 협업·거버넌스·스키마 일관성·확장성을 모두 달성함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Netflix 시스템의 복잡성 증가와 도메인 모델의 도전 과제

     * Netflix 서비스가 영화, 시리즈, 게임, 라이브 이벤트, 광고 등으로 확대됨에 따라, 이를 지원하는 시스템 복잡성도 크게 증가함
     * ‘actor’, ‘movie’와 같은 핵심 비즈니스 개념이 서로 다른 시스템(Enterprise GraphQL Gateway, 자산 관리 플랫폼, 미디어 컴퓨팅 등)에서 개별적으로 정의되고, 협력이나 공유 없이 분절적으로 운용됨
     * 이로 인해 다음과 같은 문제 발생
          + 중복 및 불일치 모델: 각기 다른 시스템에서 같은 엔터티를 재정의, 충돌 방지 어려움
          + 용어 불일치: 동일한 시스템 내에서도 서로 다른 용어 사용 또는 같은 용어의 중복 사용으로 소통 혼선
          + 데이터 품질 문제: 수많은 마이크로서비스 간 불일치·참조 오류 존재, 식별자나 외부 키 관리도 미흡해 수동 개선 필요
          + 연결성 한계: 시스템 내 관계성 제한 및 시스템 간 연계 미흡
     * 이런 문제를 해결하려면, 한 번 개념 모델 정의 후 모든 곳에서 재사용하고, 실 시스템 및 데이터와 실질적으로 연결·통합하는 기반이 필요함

UDA(통합 데이터 아키텍처) 개요

     * UDA는 Netflix Content Engineering 내 데이터 연결성 기반 플랫폼
     * 도메인 모델(비즈니스 개념)을 한 번 정의하여, 모든 시스템에 일관 투영 및 자동화·발견·시맨틱 상호운용성 지원
     * UDA로 가능한 주요 기능
          + 도메인 모델 등록 및 연결: 공식 개념 정의를 단일 소스로 사용해, 팀 간 혼동·중복 모델링 방지
          + 도메인 모델과 데이터 컨테이너 매핑: 비즈니스 개념과 실제 데이터가 저장된 위치(데이터베이스, 테이블, 서비스 등)와 구조를 쉽게 파악할 수 있도록 그래프 구조로 표현
          + 도메인 모델을 스키마 정의 언어로 변환: GraphQL, Avro, SQL, RDF, Java 등 다양한 시스템별로 자동 변환, 수작업 최소화와 오류 감소 실현
          + 데이터 컨테이너 간 신뢰성 있는 데이터 이동: GraphQL 엔티티, Data Mesh, CDC, Iceberg 등 시스템 간 데이터 변환·이동 자동 처리
          + 도메인 개념 탐색 및 검색: 비즈니스 개념 간 관계를 탐색 및 검색 가능, 정확한 정보 획득 용이
          + 지식 그래프 프로그래밍적 인트로스펙션 제공: Java, GraphQL, SPARQL로 연결된 비즈니스 정보 활용, 자동화 및 인사이트 도출

UDA의 핵심 아키텍처

     * Knowledge Graph 기반
          + RDF/SHACL 기반 지식 그래프로, 도메인 모델·스키마·데이터 컨테이너·매핑 등 모든 요소를 그래프 데이터로 연결
          + 명명된 그래프(named graph) 중심 정보모델로, 각 named graph가 규칙적 거버넌스 모델을 따르고, 해석 체계 및 모듈화, 운영 정책 실현
     * Upper 메타모델
          + Upper는 도메인 모델을 엄격히 정의하는 메타모델 언어
          + 도메인별 키 엔터티/속성/관계를 타입·계층구조로 모델링하며, RDF로 표현·버전 관리·쿼리 가능
          + Upper 자체도 Upper로 모델링(자기참조, 자기유효화)되어 모든 확장·투영에 일관성 제공
          + 속성 값에 대해 도메인별 맞춤 가능하고, 모든 개념은 conceptual RDF 및 named graph로 표현, introspection·검색·버전관리 지원
          + Upper는 고수준 추상화와 함께, RDFS/OWL/SHACL 등 W3C 의미 기술 중 핵심만 엄선 적용, 온톨로지 개념을 몰라도 효과적으로 도메인 모델링 가능
          + Upper 메타모델로부터 Jena 기반 Java API 및 GraphQL 스키마 자동 생성, 실 GraphQL 서비스와 연계
     * 데이터 컨테이너 및 매핑
          + Data Container: 실제 데이터 저장소(예: GraphQL 서비스의 엔티티, Data Mesh 소스의 Avro 레코드, Iceberg 테이블의 행, Java API의 오브젝트 등)
          + 매핑(Mappings): 도메인 모델의 특정 요소와 데이터 컨테이너(테이블, 필드 등) 간의 일대일 연결 정의
          + 매핑을 통해 도메인 개념→실제 데이터 위치 추적, 반대로 데이터→관련 도메인 개념 탐색 가능
          + 의도 기반 데이터 이동/자동화: 매핑 정보를 활용해 데이터 흐름/변환을 시스템이 자동 결정
     * Projections(투영)
          + 도메인 모델→타겟 시스템 스키마(예: GraphQL, Avro 등)로 자동 변환·생성(코드·스키마·파이프라인 자동화)
          + 스키마 일관성 보장, 중복 정의 및 동기화 이슈 최소화

실제 적용 사례

     * PDM(Primary Data Management)
          + 참조 데이터 및 택소노미(계층적 분류체계) 관리 플랫폼
          + 도메인 모델을 계층적 또는 평면 분류로 변환, 비즈니스 사용자를 위한 UI 자동 생성
          + 비즈니스 용어의 일관적인 정의, SKOS 모델 기반, UDA로 Avro·GraphQL 스키마/파이프라인 자동 생성
          + 도메인 모델만 입력하면, UI/데이터 파이프라인/GraphQL API가 자동 구성
     * Sphere(Operational Reporting)
          + 지식 그래프 기반 자가 서비스 운영 리포팅 도구
          + 도메인 용어 기반 검색·탐색·조인 전략 자동화, 기술적 복잡성 없이 데이터 발견/리포트 생성 가능
          + UDA 기반 메타데이터/매핑으로, 실제 데이터 위치와 조인 논리까지 시스템이 자동 파악·실행
          + ‘actor’, ‘movie’ 같은 친숙한 용어로 개념 탐색, 선택된 개념 기반으로 지식 그래프를 따라 SQL 쿼리 자동 생성, 별도 조인이나 기술 작업 없이 데이터 획득 가능

결론 및 전망

     * UDA는 Netflix 데이터 모델링·통합 방식의 근본적 변화를 이끌고 있음
     * 한 번의 도메인 모델 정의로, 조직 전체 시스템이 일관적·자동·확장성 있는 데이터 연동 가능
     * 앞으로 Protobuf/gRPC 등 추가 지원, 지식 그래프 실데이터화 등 적용 범위 확대 예정

   비슷한거 구성해야 하는데 막막하네요.

        Hacker News 의견

     * 모든 장점에도 불구하고, 이 방식에는 자주 언급되지 않는 큰 문제가 있다는 생각임
       비즈니스 문제이면서 기술 문제에도 영향을 끼치기 때문에 결국 개발 속도에 영향을 미치는 부차적 기술 문제로 작용함
       전체 조직이 하나의 통합 데이터 정의를 신뢰할 수 있다는 계약이 비즈니스에 걸려 있다보니, 이제는 데이터를 정의, 수정할 때 자기 분야뿐만 아니라 조직 전체의 다양한 사용 사례를 고려할 수밖에 없음
       작은 변경조차 조직 전체에 영향을 주므로 다양한 이해관계자의 승인을 거쳐야 할 정도로 복잡한 절차가 생기는 것
       이건 대기업에서 ""버튼 색 바꾸는 데 두 달 걸리는 이유""라는 고전적인 문제의 데이터 버전임
       물론 보통 데이터를 복제하거나 일관성 없이 분산시키는 게 더 심각한 문제라는 점은 인정함
       하지만 때로는 작고 고립된 변경을 빠르게 배포하고 싶을 때 이런 시스템이 큰 장애물로 작용함
          + 이걸 해결하려고 제품을 개발해본 적 있음
            기업 공통 모델을 따르면서도 로컬에서 모델을 손쉽게 특화할 수 있게 해주는 방식(프로로그 같은 데이터 정의 언어를 확장하고, 당장 필요한 게 아니라 현실에 기반한 기업 모델을 정말로 고민하는 것)을 시도함
            아쉽게도 NoSQL, Big Data 열풍이 한창일 때 시도해서 타이밍이 좋지 않았음
            NoSQL, Big Data는 느슨한 모델로 운영할 수 있고, 데이터가 일부 유실되거나 오해되어도 나중에 땜질하면 된다는 문화임
            초기에 강력한 모델을 설계하기보다 뒷수습이 쉽다는 분위기이고, 조금 아쉽지만 받아들였음
          + 본질적으로 비즈니스 문제라는 데 동의하고, 우리는 기술로 이걸 체계적으로 풀 수 있다고 봄
            엔터프라이즈 내에서 모델-퍼스트 지식 그래프를 도입, 배포하는 더 체계적인 방법을 마련했다는 자신감이 있음
            UDA는 요청받는 모든 것을 오히려 추가적인 관료적으로 만들지 않도록 신중히 접근함
            UDA는 기존 시스템과 나란히 존재하며 무조건 수용을 강요하지 않음
            하지만 자신들의 비즈니스 모델이 어디에서나 활용되고 쉽게 연결, 확장, 발견될 수 있게 하고 싶은 팀에게는 강력하고 쉬운 도구가 되게끔 하려 함
            (본인은 UDA 아키텍트 중 한 명임을 밝힘)
          + 경험상 회사 내 ""큰 사람(big men)""의 주장 때문에 논리적이거나 일관된 데이터 모델이 만들어지지 않는 경우가 많았던 기억
            실무 기술자들이 데이터를 좀 더 합리적이고 표준에 맞게 다루려 해도, 영향력 있는 인물이 직접 머릿속 모델을 만들어서 개발자가 맞추도록 강요하는 현실
            이런 상징적인 사고방식이 한 번 들어서면, 그 회사에서 일관된 데이터 모델이 만들어질 가능성은 0에 수렴함
            결국 이런 문제 때문에 컨설턴트들에게 비효율적으로 많은 돈이 지불되는 구조라고 봄
          + SAP가 뭔지 오랫동안 궁금했다가, 실제로 알게 된 후 놀랐던 경험이 있음
            수많은 기업에서 SAP를 쓴다고 해서 엄청난 기술력이 있나 추측했었지만, 실제로는 고정된 스키마를 두고 고객에게 그 구조에 맞춰 적응하라고 요구하는 방식임을 알게 됨
          + 원문에서는 비즈니스 문제라는 점을 부정한다고 읽히지 않음
            정의된 모델들은 모든 역할에 걸쳐서 정의하며, 엔지니어링은 그 일부일 뿐이라는 관점
     * Semantic Web, RDF, OWL, SKOS 등 등장 시점보다 꽤 시간이 흘렀다는 생각
       W3C를 계속 지지하고, 이미 있는 바퀴를 다시 만들지 않은 것이 좋았음
       UDA 방식이 대중적으로 자리잡을지 모르겠지만, 조직 규모가 큰 곳에 DDD(도메인 주도 설계)와 시맨틱 개념을 적용할 때 어려움을 혁신적으로 줄이려는 시도라 기대감
       여러 개발팀에 같은 데이터 의미 체계를 공유하는 공통 도구와 기술셋을 제공해 ‘복리(compound interest)’ 효과를 도모할 수 있다면, 데이터 계약을 DTO, POST 등 네트워크 전송 때문에 억지로 평준화시키지 않아도 될 수도 있다는 점 흥미로움
       이런 흥미로운 실험을 공개하면서 추진하는 Netflix에 긍정적 시각
     * Uber에서 Dragon이라는 프로젝트가 떠오름
       Dragon schema at Uber 관련 작업을 했었으나 오픈소스화되지는 못함
       이후 Joshua가 LinkedIn으로 옮겨 LambdaGraph 프로젝트, Hydra 언어에 참여했고, 이들은 여기에서 오픈소스로 공개됨
       이러한 방식이나 10년 전 Semantic Web 흐름은 모두 개념을 공식화하고 유지해야 하는 부가 작업량이 너무 많았던 게 단점
       요즘은 LLM(대형 언어모델)로 이런 부담을 줄일 수 있을지 궁금증
     * 이번에 쓰인 ‘도메인 모델’이라는 용어 선택이 아쉬운 점
       여기서의 도메인 모델은 순전히 데이터 중심 개념인데, 실제 도메인 모델링은 데이터 구조보다는 행동(Behavior)에 대한 집중이 본질
       도메인 모델의 데이터는 행동을 가능하게 하려고 존재하는 역할이고, 행동 자체가 코드의 중심
       데이터 모델링을 관점에 따라 다양하게 표현하는 것 자체에 복잡함이 있지만, 이런 차이점은 오히려 기능으로 볼 수도 있다고 생각
       모든 상황에 똑같은 복잡성이나 세밀함이 요구되는 건 아니며, 표현 모델은 읽기 시나리오에 최적화하는 게 일반적임
       이런 방식을 사용하면 정보의 상황별 취급보다는 통일성에 더 치우칠 수 있고, 도메인 이해도가 높은 환경에는 확장성이 나을 수도 있지만, 실제로는 복잡 또는 미묘한 도메인 모델을 단순화하지 않으면 대부분의 사용 사례가 어려워진다는 경험
     * “이런 시도로 5% 이상 혹은 500만 달러 이상의 수치적인 비즈니스 개선이 나타난 사례를 본 적 있냐”는 질문
       데이터 테이블을 통합하려는 시도를 여러 번 경험했지만 실제로는 서로 다른 분석이 필요하면 테이블 통합이 의미 없었고, 다양한 분석이 여전히 각각 독립적으로 이루어졌음
       즉, 모두가 원하는 분석 방식에 맞춰 베이스 레이어를 통합해도 여전히 서로 다른 분석이 계속 별개로 진행됨
       그래도 이번 시도는 모든 것을 하나로 통합하게 만들진 않고, 폭넓은 접근의 용이성을 높이려는 것 같아서 현명해 보임
       공식적인 비즈니스 개념 정의를 모두가 통일해 혼란을 줄이려는 목적에는 깊이 공감함
          + “서로 다른 분석을 하는 팀들이 같은 정보를 다룬다고 해서 반드시 같은 사물이어야 하는 건 아니라는 점”에 크게 공감
            예를 들어, 모두가 마스터 프리즌 리스트를 원한다고 해도, 프리즌이 건물 자체인지, 수감자 집합(같은 부지 내 남·여 프리즌이 구분된 독립체)인지, 특정 계약으로 운영되는 기관인지는 정의에 따라 완전히 다름
            이런 점에서 조직의 관점마다 미묘하게 다른 객체를 필요로 함
     * 도메인 주도 설계(DDD)와 어떤 관련이 있는지 궁금
       DDD에서는 동일한 개념조차 시스템마다 다르게 표현될 수 있다는 걸 전제로 하지 않나 생각
       하지만 글 자체는 UML 느낌 때문에 끝까지 읽진 않음
          + upper:DomainModel에서의 Domain이 DDD의 D(도메인)과 동일함, DGS(Domain Graph Service)도 마찬가지
            DDD에서는 동시에 동일 개념이라도 시스템별로 표현 방식이 다름을 허용하지만, UDA에서는 각 도메인에 이런 다양한 개념이 명시적으로 공존하도록 설계
            “같음”이라는 개념이 주관적이 됨
          + ""ubiquitous language(공통 언어)""라는 용어를 쓰지 않은 게 오히려 다행
            이 개념은 이번 시도와 완전히 반대되는 개념
            관련된 개념만 듣고 깊이 들여다본 적 없는 사람들은 차이점을 모를 수도 있음
     * Netflix 엔지니어링이 왜 Medium에 콘텐츠를 올리는지 의문
       Medium의 팝업 등으로 독자를 쉽게 잃는 상황인데, 그런 리스크를 감수할 가치가 있는지 궁금
          + hex로 된 Medium URL을 볼 때마다 scribe.rip을 통해 읽는 재미가 있음
            scribe.rip UDA 아티클
          + 마케팅팀이 이를 운영한다면 SEO까지 포함한 전략일 가능성
          + Medium이 주는 ‘발견’(discovery) 효과는 실제로 있음
            그리고 Medium에 글을 쓰는 스타일의 엔지니어들이 Netflix가 리크루팅하고 싶은 인재군일 가능성
          + 자체적으로 블로그를 관리하지 않아도 되어서 더욱 편리
     * 데이터 모델의 버전 관리 혹은 브레이킹 체인지에 어떻게 대응하는지 궁금
       모델을 더 분리해서 운영할 때는 기존 방식대로 조각 단위로 쉽게 고칠 수 있는데, 이런 통합 모델에서는 어떻게 하는지 의문
       Netflix 방식에서는 새로운 모델을 추가한 뒤 이전 모델은 점진적으로 사용 중단시키는 식일 것으로 추정
          + 버전 관리는 ‘깨도 되는 권한’을 부여하는 것과 같음
            UDA에서는 아직 완전히 구현은 안 됐지만 Federated GraphQL과 동일한 방식을 적용할 계획
            Federated GraphQL에서 검증된 deprecation 관리 모델을 도입할 예정이며, 500개 이상의 페더레이티드 GraphQL 스키마를 성공적으로 관리한 경험이 있음
            핵심은, projected models의 소비자를 추적하면서 deprecated 주기를 능동적으로 관리하는 로드맵임
     * 통합 그래프를 성공시키려면 비즈니스, 커뮤니케이션, 기술 세 가지 모두 해결해야 한다고 체감
       커뮤니케이션 문제는 ‘자율적인 팀의 은근한 독립성’을 깰 필요가 있음
       각 팀을 넘나드는 다리 역할을 하며, 데이터 모델도 분석해 줘야 함
       단순 스키마 공유만으로는 부족하고, 직접 사람들과 인터뷰하면서 협의하는 과정이 꼭 필요
       기술적 측면은 오히려 가장 쉽고, Microsoft Graph처럼 ‘두꺼운 스키마’를 강제 적용하면 간단
       이 과정엔 상당한 공감능력, 인내심이 필요함
       기술 해결은 경영진의 확고한 의사결정 및 실행 권한이 필수이고, 팀별로 자유롭게 움직일 수 있으면 어떤 아이디어도 소용 없음
       비즈니스 측면이 최상위 난이도
       20년 최적화된 프로세스와 용어를 한번에 바꾸는 건 사실상 불가능에 가까움
       결국 전사적인 buy-in이 완벽하게 이뤄져야만 이 어마어마한 일이 ‘평생에 걸쳐’ 투자 가치가 있음
     * 공통 어휘의 도입이 매우 유의미하다고 믿음
       하지만 기업 전체, 신규 인수, 다양한 비즈니스 프로세스, 시간축 등 조직이 넓어질수록 점점 어려워짐
       시스템 두 개만 연동하는 인터페이스 정도야 뚝딱 만들 수 있겠지만, 한 엔터프라이즈에 여러 레이어가 존재하고, 모든 지식을 카탈로그에 담는 이상적인 DB를 누가 만들고 계속 유지할 수 있을지는 의문
       성공적으로 남은 시도는 대부분 매우 추상적으로 운영하거나 특정 사용 사례에만 범위를 제한한 방식임
          + 경험적으로, 기업 전체 엔티티(예: 회사의 공식 개체)를 정의해도 각 부서별로 이걸 확장해야 할 필요가 생기며, 새로운 속성을 모든 부서에서 쓸 수 있게 할지, 해당 부서만 쓸지 등 정치적·낙관적 결정이 필요하게 됨
            공식 엔티티를 업데이트하면 전체 영향을 평가하며 철저한 체인지 관리가 필요
            올바르게 잘 구축하고, 엄격한 조직 문화가 뒷받침된다면 비용과 마찰이 크게 줄 수 있고, Netflix라면 가능성 있어 보임
          + 살아남은 유일무이한 대규모 공통 어휘 예시로 Wikidata 언급
            16억 5천만 개 그래프 노드가 표준 어휘 아래 계속 확장되고 있음
"
"https://news.hada.io/topic?id=21434","필수 설치 “K-보안” 앱 검토 논문","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          필수 설치 “K-보안” 앱 검토 논문

   예전부터 이곳에 오셨던 분들이라면 제가 2023년 초 여기 올렸던 블라디미르 팔란트(Wladimir Palant)의 K-보안 소개 글을 읽어보셨을지도 모릅니다.
     * 막다른 골목에 다다른 한국의 온라인 보안 실태
     * 해당 글 시리즈의 비공식 한국어 번역본

   이와 관련하여, 최근 한국의 보안 학계에서 위 내용을 보충하는 논문 및 모의 해킹 시연 영상을 공개했습니다.
   이 내용은 오는 8월에 미국 시애틀에서 열리는 제34회 USENIX 보안 심포지엄에서 발표될 예정이라고 합니다.
     * Too Much of a Good Thing: (In-)Security of Mandatory Security Software for Financial Services in South Korea
     * KAIST의 위 논문 보도자료
     * K-보안 앱을 악용하여 사용자 키보드 입력을 가로채어 패스워드를 훔치는 PoC 영상
     * K-보안 앱을 악용하여 특정 사이트에 접속한 사용자 PC에 악성코드를 투입하는 PoC 영상
     * 위 논문의 내용을 보도하는 ZDNet Korea 기사
     * 위 논문의 내용을 설명하는 화이트 해커 Normaltic의 영상

   위 논문에서는 K-보안 앱(Korea Security Applications)을 줄여서 “KSA”라고 하고 있으며, ActiveX 기반으로 돌아갔던 시절을 v1.0으로, exe 기반으로 넘어간 2015년 이후를 v2.0으로 부르고 있습니다.

   기본적으로 이러한 “보안 프로그램”을 사용자 PC에 강제로 설치하는 구조 자체가 오히려 보안에 취약하다는 결론이나 그럼에도 불구하고 구조적인 문제점 때문에 이를 단숨에 없애기는 쉽지 않다는 등의 문제 제기 자체는 블라디미르 팔란트의 시리즈 글과 별로 다르지 않습니다.

   몇 가지 다른 점을 꼽자면, 먼저 블라디미르 팔란트의 글에서는 뱅킹 때 설치하는 앱 쪽으로 살펴봤지만 이 논문에서는 분석 대상을 일부 공공기관 관련 일을 봐야 할 때 필수적으로 설치하는 앱으로도 확장했다는 것입니다. 논문에서 살펴본 것은 모두 7개를 살펴봤더군요. 어차피 K-보안 앱의 종류라고 해봤자 보통 그 밥에 그 나물이라 회사 및 제품명이 가려져 있지만 알아보실 만한 분들은 대충 뭔지 짐작이 가실 것도 같습니다. 아무튼 각종 취약점을 찾아내어 키로깅도 해보고 원격 코드 실행도 해보고 퍼징으로 메모리 취약점도 찾아내고 중간자 공격도 해보고 이것저것 다 해본 모양이네요.

   그리고 눈에 띄는 내용으로는 사용자 대상 온라인 설문조사를 진행했는데, 이 설문 결과 실제로 한국 온라인 환경에서 K-보안 앱을 경험하지 않은 사람은 거의 없지만 이걸 설치하는 사용자의 약 60% 정도는 구체적으로 이 앱이 대체 무슨 역할을 하는 건지도 모르고 그냥 설치한다는 점이 드러났습니다.

   여기서 조금 더 나아가서, 지원자를 모집하여 구라제거기 v7.25를 통해 주로 20대 젊은 사람들이 사용하는 PC에 얼마나 많은 K-보안 앱이 실제로 설치되어 있는지도 조사해 보았다고 합니다. 그 결과 평균적으로 약 9개의 K-보안 앱이 설치되어 있었고, 그 중 이번 연구 대상에 포함된 앱은 약 4개였다고 합니다. 어떤 지원자의 PC에서는 무려 24개나 검출되기도 했습니다. 과반 이상의 지원자에서 설치된 앱이 2년 이상 지난 버전이었으며, 어떤 지원자의 PC에서는 연구 시점으로부터 5년씩 지난 버전의 앱도 확인되었다고 하네요.

   그나마 식탁보 프로젝트나 구라제거기와 같은 유용한 도구를 공개해주시는 분들이 계신다는 게 정말 감사한 일이긴 합니다만, 근본적으로는 이런 도구를 쓸 일이 아예 없어야 하겠지요.

   블라디미르 팔란트의 K-보안 앱 분석 시리즈가 2023년 초에 공개되었고 그 이후 북한이 이런 K-보안 앱을 통로로 삼아 해킹을 했다는 사실이 알려졌지만, 아직까지도 근본적인 변화가 느껴지지는 않는 것이 현실입니다. 이제부터라도 좀 가시적인 변화가 일어났으면 좋겠습니다.

   보안이 목적이 아닌 책임회피를 위한 앱들 이제는 정말 그만보고싶네요..

   식탁보 프로젝트 멋지네요.

   요즘 보안 이슈 빵빵 퍼지는데, 한국도 보안에 대한 경각심을 가지는 계기가 되면 좋겠습니다.

   빵빵 터지면 보안앱을 더 깔게 할것같네요.

   구라제거기는 잘 쓰고 있었는데 식탁보 프로젝트는 처음 접하네요, 너무 좋은 프로그램이네요.

   식탁보 참 괜찮은 프로그램입니다. 자칭 보안 앱들이 남기는 찌꺼기가 너무 많아요.
   수 년 전 일이긴 하지만 설치 제거 후 재설치하면 오류가 나는 것들도 있었습니다.
   샌드박스에서 처리하고 통째로 날려버리는 것이 너무 좋습니다.

   개인적으로 식탁보 프로젝트를 처음 만들던 때에도 A사의 보안 프로그램이 제가 쓰는 개발도구나 워크로드들과 사사건건 충돌을 유발하는 통에 너무 짜증나서 만든 것인데, 이렇게 많은 사랑과 관심을 받을거란것을 몰랐습니다.

   오픈소스 소프트웨어 개발자로서 사람들의 문제 해결에 도움을 준 것은 기쁘지만, 다른 한편으로 글쓴이께서 지적해주신대로 이런 기형적인 보안 생태계에서 수년째 식탁보가 회자된다는 사실이 심히 안타깝다는 상반된 감정을 느끼고 있습니다.

   G식백과에서 다루는 게임업계,중독문제 보면 정부와 산업이 어떻게 왜 이렇게 돌아가는지 알 수 있네요.

   구글보다 인터넷보안 더 잘할수 있다고 한국정부는굳게 믿고 있는거 같습니다. 그러니 좋소기업들 앱으로 대충 막고도 안심하나? 대체 뭔 자신감인지.

   좋은 글 감사합니다
"
"https://news.hada.io/topic?id=21487","컴퓨터 비전의 기초 (2024)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           컴퓨터 비전의 기초 (2024)

     * 컴퓨터 비전의 기초를 이미지 처리와 머신러닝 관점에서 다룬 입문 및 중급자를 위한 책임
     * 핵심 개념에 집중하기 위해 각 챕터를 짧고 명확하게 구성함
     * 딥러닝 혁명 이후 변화와 고전적 아이디어의 재구성 과정을 책 집필 경험과 함께 설명함
     * 15개 파트로 이미지 처리, 신경망, 생성 모델, 시퀀스 처리, 씬 이해 등 컴퓨터 비전 전반의 주제 다룸
     * 최신 연구 동향이나 특정 응용보다는 필수 이론과 직관 구축에 초점을 맞춘 구조임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서문

     * 모든 픽셀에게 헌정함

이 책에 대하여

     * 이 책은 컴퓨터 비전의 핵심 주제들을 이미지 처리와 머신러닝의 시각에서 다룸
     * 독자의 직관을 개발하기 위해 다양한 시각화 자료를 포함함
     * 주요 독자층은 컴퓨터 비전에 입문하는 학부 및 대학원생이지만, 경험 많은 실무자에게도 유용함
     * 원래는 방대한 내용을 목표로 했으나, 컴퓨터 비전 분야의 방대함 때문에 각 장을 5페이지 이내로 제한하여 핵심 개념에 집중함
     * 짧은 책을 쓰겠다는 목표였지만 결국 분량이 많아진 경험을 솔직하게 공유함

책 집필의 과정

     * 집필 시작 시 가졌던 의도와 실제 과정의 비선형성을 데이터로 보여주며, 완성까지 10년 이상 소요됨
     * 집필 과정 중 딥러닝 혁명(2012년) 이 일어나 전통적 방법과 현대적 접근법의 융합 과정이 진행됨
     * 초기 딥러닝의 인기로 인해 이전의 아이디어들이 잠시 잊혔으나, 시간이 지나면서 본질적 개념들이 재조명됨
     * 집필 여정이 힘들었지만, 직접 다양한 예제와 실험을 하며 많은 배움을 얻었다고 언급함
     * 컴퓨터 비전 및 AI 분야의 주요 사건들이 책 집필 시점과 함께 변화했음을 시각적으로 보여줌

책의 구조

     * 컴퓨터 비전 분야는 지난 10여 년간 급격한 발전을 겪었으며, 현재 방식이 과거와 전혀 다른 듯 보이지만 역사적 연속성을 강조함
     * 책 전체적으로 통일된 주제와 시각, 그리고 다양한 관점의 중요성을 반복적으로 다룸
     * 책은 15개 파트로 구성되며, 각 파트가 컴퓨터 비전의 일관된 주제에 집중함

    각 파트 소개

     * Part I: 컴퓨터 비전 문제에 대한 동기 부여와 사회적 맥락, 수학적 기초 소개
     * Part II: 이미지 생성 과정
     * Part III: 이미지 예시를 통한 머신러닝 기초 개념 설명
     * Part IV: 신호 및 이미지 처리 입문
     * Part V: 유용한 선형 필터(가우시안 커널, 이진 필터, 이미지 도함수, 라플라시안, 시간 필터) 및 응용
     * Part VI: 멀티스케일 이미지 표현
     * Part VII: 컴퓨터 비전을 위한 신경망(합성곱 신경망, 순환 신경망, 트랜스포머)
     * Part VIII: 이미지의 통계적 모델과 그래프 모델
     * Part IX: 생성 모델과 표현 학습(벡터 임베딩 등) 중심의 현대적 접근
     * Part X: 학습 기반 비전 시스템 구축 시 발생하는 도전 과제
     * Part XI: 3D 구조 재구성을 위한 기하학적 도구
     * Part XII: 시퀀스 처리 및 모션 측정
     * Part XIII: 씬 이해 및 객체 검출
     * Part XIV: 주니어 연구원을 위한 프레젠테이션, 논문 작성, 효과적인 연구 마인드에 대한 조언
     * Part XV: Part I에서 제시한 문제를 책에서 다룬 다양한 방법론으로 해결 시도

다루지 않는 내용

     * 최신 컴퓨터 비전 최신 동향이나 다양한 실용적 응용 분야(형상 분석, 객체 추적, 동작 분석, 얼굴 인식 등)는 다루지 않음
     * 이런 세부 응용은 학회 논문이나 전문 서적 참고가 더 효과적임

감사의 글

     * 다양한 컴퓨터 비전 교육과 연구에 영향을 준 교수진·학생·동료들에게 감사를 표함
     * 여러 학회의 강의 자료와 실험, 챕터별 지원, 표지 디자인 등 다양한 협력에 대한 구체적 감사 언급
     * 각 저자가 가족 및 가까운 지인에게도 지속적 지원에 대해 감사를 표함

인용 정보

     * 도서를 인용할 때 사용할 수 있는 BibTeX 양식을 제공함

강사진을 위한 리소스

     * 도서 인쇄본은 MIT Press에서 구매 가능함
     * 책과 연계된 강의 슬라이드를 온라인으로 제공함

참고 문헌

     * 컴퓨터 비전, 머신러닝, 신호처리, 기하학, 시각 과학 등 관련 주요 고전 및 최신 서적 리스트 제공

        Hacker News 의견

     * ""On Research, Writing and Speaking""이라는 책에 흥미로운 부분이 있음. ""이거 힘들어 보이네."" 맞음. 더 이상 똑똑함만으로는 승부 나지 않음. 대학원에서는 열심히 노력하는 사람이 앞서 나감이라는 이야기 공유
          + 정말 통찰력 있는 이야기임. 어느 순간부터는 지식만으로는 충분하지 않음이라는 사실을 모두가 실감함. 많은 사람이 대학 진학하며 이런 벽을 느낌. 하지만 대학에서는 학습 범위가 정해져 있어 어떻게든 실력으로 버틸 수 있음. 반면 박사 과정은 학습량에 제한이 없음. 정해진 독서 분량도, “시험 범위 외”라는 것도 없음. 공부, 실험, 논문 읽기 등 할 수 있는 만큼 무제한으로 해야 함. 단순히 똑똑한 걸로 끝나지 않고, 소프트 스킬이나 네트워크, 커뮤니티 맥락까지 파악 필요. 커뮤니티 사람들과 회의, 식사, 네트워킹 하며 연락 유지. 혼자 동기 부여해 마감과 루틴 관리 필요. 공식적인 수업, 시험으로 주어지는 동기와 달리, 스스로 관리해야만 함. 기준이 애매하고, 기대치는 무한대임. 이전과 달리 거절 당하는 경험도 있을 수 있음. 박사 과정은
            누구에게나 한계에 도전하게 하는 힘든 시기임. 목적이 단순히 졸업이라면 대충 넘길 수도 있겠지만, 보통 학계 진로를 꿈꾸는 학생은 더 큰 목표를 가지기 마련임
     * 최근 2년간의 기술 변화로 인해 머신러닝, 특히 컴퓨터 비전 분야의 기존 내용이 여전히 유효한지 현업에 있는 사람이 코멘트 요청
          + 여전히 매우 유효. 최신 기법들도 근본적으로는 같은 기초 위에서 쌓아진 발전. 오히려 기본 개념, 전통적 알고리즘을 더 많이 읽어두는 것이 바람직. Hough transform, canny edge, sift, Harris corner 등 클래식 기법을 잘 알아야 진정한 전문가라 할 수 있음. 뜨는 기술 키워드만 외우고 API만 붙여쓰는 개발자들과 구별되는 실력의 차이 발생
          + 아직도 GPU 가속이 어려운 시스템 등에서는 ""클래식"" 컴퓨터 비전 기법이 필수적임. 나는 리소스 제한된 환경에서 Simultaneous localization and mapping 문제를 해결하는 실무자. Structure from Motion 장 꼭 읽어볼 예정
     * ""Writing this book"" 부분이 LLM이 원고 2/3를 작성한 것처럼 보일 수도 있음. 실제로 LLM이 쓸 게 많아져서 책 내용이 늘었다는 의미일 것 같으니 명확히 하는 게 좋겠다는 의견
          + 나는 그렇게 읽지 않음. 실제로 ChatGPT가 등장한 뒤에 책의 1/3 이하만 작성된 것으로 보임. 오히려 ML/AI 분야의 주요 이벤트를 그래프에 표시한 느낌
     * 컴퓨터 비전 분야에서 또다른 좋은 책으로는 아래 책 추천
Computer Vision, Fifth Edition
E.R. Davies
Academic Press
ISBN-13 978-0128092842

          + 또 다른 주요 교재로는 Szeliski의 ""Computer Vision 2nd Ed"" (2022) 추천 https://szeliski.org/Book/. Forsyth & Ponce 책도 좋지만 다소 오래됨. 3D에 관심 있으면 여전히 Hartley & Zisserman의 Multiple View Geometry가 클래식
     * 이 책이 무료로 공개된 게 믿기지 않을 정도라며 극찬
          + 진짜 맞는 말임. 혹시 PDF로 다운로드할 수 있는 방법 찾았는지 궁금. 공부할 때 개인적으로 노트나 참고 자료 남기기가 꼭 필요하다고 생각함
          + 머신러닝, 컴퓨터 비전, 로보틱스 커뮤니티는 교재를 무료로 온라인 공개하는 문화가 대단함. 이 분야 최고 수준 교재도 무료로 온라인에서 구할 수 있음. 타 분야에서는 미국 교수들이 최신판을 구입하라고 요구해 높은 비용 들지만, 이 분야는 저개발국이나 전 세계 누구에게나 최고의 자료를 개방함. 강의 자료와 동영상도 같이 많이 공개
     * 머신 비전에 관한 좋은 책 추천 요청. 효과적인 머신 비전뿐 아니라 컴퓨터 비전의 핵심은 카메라, 광학, 조명 선택에 있다고 봄. 입력 이미지 품질이 좋지 않으면 출력도 나빠질 수밖에 없다고 생각함
          + 실제로 이런 요소들이 차이를 만들어낸 사례나 사용 예시를 공유해줄 수 있는지 궁금
"
"https://news.hada.io/topic?id=21448","Apple의 Liquid Glass는 단순한 디자인 개선이 아닌 AR 인터페이스를 위한 준비 작업임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Apple의 Liquid Glass는 단순한 디자인 개선이 아닌 AR 인터페이스를 위한 준비 작업임

     * Liquid Glass 도입은 단순한 시각적 리뉴얼이 아니라, 애플이 다음 세대의 인간-컴퓨터 상호작용을 준비하는 전략적 포석임
     * 이 변화는 visionOS와의 연결성을 바탕으로, AR 시대의 새로운 인터페이스 표준을 위해 기존 사용자 경험을 서서히 확장하는 방식
     * 강력한 하드웨어와 소프트웨어 통합을 통해, Liquid Glass의 고급 효과를 부드럽게 구현하며 애플만의 차별성을 극대화
     * AI 이슈가 관심을 끄는 와중에, 애플은 디자인과 생태계 통합에 집중해 경쟁자들과의 차별화 전략을 이어감
     * Liquid Glass는 사용성·접근성 문제와 네트워크 효과를 동시에 고려하며, 장기적으로 공간적(Spatial) 컴퓨팅 패러다임 전환을 위한 기반을 구축함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Liquid Glass의 의미와 전략적 포지셔닝

     * 2025년 WWDC에서 공개된 Liquid Glass는 단순한 디자인 업데이트가 아니라, 향후 10년간의 컴퓨팅 인터페이스 방향성을 드러내는 전략적 변화임
     * 디자인 업계에서는 가독성과 관련된 논쟁이, 기술 언론에서는 AI 부재가 주요 이슈로 등장하지만, 애플은 iPhone 출시 당시와 유사하게 사용자에게 패러다임 전환을 자연스럽게 체득시키는 전략을 구사함

애플 디자인 변화의 패턴

     * 애플은 이전에도 스큐어모피즘에서 미니멀(플랫) 디자인으로의 전환 등, 과감한 디자인 변화를 여러 번 단행했음
          + iOS 6에서 iOS 7로의 이동은 ""너무 얇은 폰트"", ""직관성 감소"" 등 비판을 받았으나, 2년 만에 업계 표준이 됨
     * 이러한 대규모 디자인 변화는 항상 기술 인터페이스 방식 변화와 연결되어 있음
          + 터치스크린이 익숙하지 않던 시절에는 물리적 비유가 필요했고, 이후 사용자가 터치에 익숙해지자 미니멀 디자인이 등장함
     * 이제 Liquid Glass는 스크린 자체의 존재감이 약해지는 미래, 즉 AR 중심 인터페이스를 준비하는 수순임

visionOS와의 연결성

     * Liquid Glass의 시기와 비전은 visionOS에서 명확히 드러남
          + AR 시대에는 반투명, 다층적, 맥락 인식형 인터페이스가 필요하며, 물리 환경과 조화되어야 함
          + 현실감을 높이는 입체감, 동적 빛 반응, 그림자 등은 visionOS에서 강조한 부분임
     * 기존 스크린에서도 이러한 메타포를 적용해, 사용자가 AR 기반 인터페이스에 미리 익숙해지도록 유도함
          + AR 글래스 착용 시에도 기존 iPhone과 연속성을 느끼게 하는 전략임

애플의 통합(Vertical Integration) 전략

     * Liquid Glass는 애플의 하드웨어-소프트웨어 완전 통합 역량을 보여줌
          + 실시간 블러, 반투명 효과, 환경 반응 등은 GPU 및 최적화 렌더링 파이프라인 등 높은 기술력이 필요함
          + 애플 Silicon 기반 기기에서는 매우 부드러운 동작이 가능하지만, 경쟁사 하드웨어에서는 한계가 존재함
     * 이러한 설계는 상호보완재 효과를 창출
          + 새로운 디자인 언어를 완전히 활용하려면 애플 기기가 더 돋보이게 됨
          + 이전 레티나 디스플레이 도입 때와 유사한 선순환 구조 형성
     * 플랫폼 일관성 역시 강점임
          + Apple TV, Vision Pro 등 모든 플랫폼 전반에 동일한 디자인 언어 적용
          + 개발자는 한 번의 디자인으로 여러 기기에서 일관성 확보, 사용자도 기기 간 경험의 연속성 누림
          + 이러한 생태계 네트워크 효과는 경쟁사가 쉽게 따라할 수 없음

AI와의 관계 및 경쟁사 대비 전략

     * WWDC 2025에서 AI에 대한 큰 발표가 없다는 비판 속에서도, 애플은 진정한 차별성은 사용자 경험·통합에서 나온다는 점에 집중함
          + 대형 언어 모델 경쟁이 과열된 가운데, 애플은 완성도 높은 경험 제공의 전통을 이어감
          + AI 기술 자체에 집착하기보다, Liquid Glass에서 자연스럽게 맥락적 AI 인터랙션(예: 반투명 오버레이, 자연스러운 AI 제안) 구현이 가능함
          + 새 디자인 언어가 주변적, 방해받지 않는 AI 경험의 시각적 토대를 제공함

사용성·접근성 및 진화 방향

     * 기존 디자인 변화와 마찬가지로 Liquid Glass 도입 초기에는 가독성 저하, 인지적 부담 가중 우려가 존재함
          + 반투명 효과는 대비 감소 및 텍스트 판독성 저하를 유발할 수 있음
          + 터치스크린에서는 유리 메타포가 직관적이지 않을 수 있음
     * 그러나, 애플은 그동안 두꺼운 폰트, 높은 대비, 접근성 옵션 등 다양한 수정으로 해결해옴
          + 이미 ‘투명도 줄이기’, ‘대비 높이기’ 등 다양한 접근성 기능 제공 경험이 있음
          + Liquid Glass 역시 사용자 피드백 반영과 점진적 개선 과정을 거칠 가능성 높음

디자인 네트워크 효과 및 산업 영향

     * Liquid Glass는 디자인 언어의 네트워크 효과를 창출
          + 애플의 변화는 자체 플랫폼을 넘어 타사 앱, 웹, 전체 IT 산업 디자인 흐름에 영향력 확장
          + iOS 개발자, 타사 디자이너, 웹 디자이너 등이 애플 중심의 디자인 트렌드를 따르게 됨
     * 이로써 애플 생태계 밖의 사용자도 자연스럽게 애플 스타일 인터페이스에 익숙해지며, 향후 애플 제품 도입 장벽을 낮춤
          + 문화적 락인 효과로 이어짐

미래 전망과 장기 전략

     * Liquid Glass는 디지털-물리적 현실 경계가 흐려진 공간적(Spatial) 인터페이스가 미래라는 애플의 관점을 드러냄
          + 터치 중심 상호작용은 중요하지만, 향후에는 음성, 제스처, 맥락 인식 등이 더 큰 역할을 차지할 전망임
     * 시각적·개념적 프레임워크를 지금부터 제공함으로써, 개발자 및 사용자 모두를 자연스럽게 AR 패러다임으로 전환시키려는 전략임
          + 가볍고 대중적인 AR 글래스가 보편화될 시기를 대비한 선제적 준비임
     * 이는 미적 선택을 넘어, 미래 제품 카테고리 확장 및 현재 경쟁력 강화라는 두 가지 목표를 아우르는 장기 전략임
          + 애플의 시장 가치와 혁신을 이끌어온 핵심 방식임
     * 가장 큰 과제는 미적 성공 여부가 아니라, 터치 중심에서 공간 중심 컴퓨팅으로의 전환을 애플 중심 생태계 하에 성공적으로 이뤄낼 수 있는지 여부임
          + 과거의 예를 볼 때, 5년 이내에 유사한 유리 인터페이스가 업계 표준이 될 것

   Depth 있는 전환이나 여러모로 Vision OS 에 맞춰진 업데이트 같아 보였어요. 그래서 입체감을 표현하는 미래 디바이스를 준비하는 거라고 저도 생각했어요.

   예를 들어 스위치처럼 손으로 가려지는 UI 에 오래 누르고 있으면 물방울로 바뀌는 건 눈으로 계속 그 바라보면 바뀌는 상황 말고는 없다고 보여집니다.

   그래서 이건 시각적인 즐거움을 표현하기 위한 변화는 분명합니다.

   하지만 이는 휴대용 배터리로 충전해도 더 빠르게 배터리를 소모하기에 저도 이런 기능을 끄고 사용하고 있습니다. 이런 미래 UI/UX 에 대한 준비는 좋으나 더 생산적인 고민이 애플에게 필요하지 않나 싶네요.

        Hacker News 의견

     * iOS 6의 스큐어모피즘 디자인에서 iOS 7의 극단적 미니멀리즘으로의 변화가 사용성 및 미적 가치에 대한 논쟁을 일으켰다는 이야기, 하지만 사실 Microsoft의 Metro UI와 Windows Phone은 Apple이 플랫 디자인으로 이동하기 3년 전 이미 등장했던 사실 지적
          + Apple이 새로운 기능을 선보이면 마치 처음 시도하는 것처럼 기술 기자들이 기억상실에 빠지는 현상 언급, 대부분 기자들이 iPhone을 메인으로 사용하다 보니 다른 기기를 충분히 안 써봐서 그렇게 느끼는 해석 제시
          + Windows Phone OS 디자인을 예전부터 좋아했음, 텍스트 우선의 미니멀리즘 방식이 꽤 유용했고 Windows Mobile보다 훨씬 앞선 도약이라는 개인적 평가
          + Apple 팬들은 자기 현실을 부정하고 자신들만의 현실로 대체하는 경향, Apple이 실수해도 그것이 사실은 대단한 전략이었다는 식의 약간 풍자적인 목소리
          + 최근 기사들이 근거 없는 주장과 Apple을 미화하는 콘텐츠로 가득함 지적, Flat design은 스큐어모피즘의 과한 수준에 대한 반작용으로 등장했으나 Apple이 오히려 그 쪽의 선두주자였다는 점 상기, 특히 Game Center의 펠트, Notes의 가죽 바인딩과 같은 과거 사례 강조, AR에서 ‘현실적으로 보이는 그림자와 빛을 받는 버튼’이 실제로는 현실 공간에서 쉽게 눈에 띄지 않아 usability가 떨어지는 문제라고 지적, Apple이 데스크톱 usability의 종말을 초래할 수 있다는 비관적 관점 제시, 각 디바이스마다 다르게 컨트롤 UI 렌더링하는 게 기술적으로 어렵지 않음에도 불구하고 모든 기기에서 ""투명"" UI 트렌드로 가는 시도가 우려스러움
          + Android도 iOS보다 1~2년 먼저 플랫 디자인 패턴으로 이동한 사실, 당시엔 서드파티 앱들에 일관된 디자인 언어가 적용되지 않았음에도, 시스템 컴포넌트나 자체 앱 중심으로 변화가 나타났다는 설명, 2013년 당시 Apple이 플랫 디자인을 도입할 때 오히려 가장 뒤처진 쪽이었음, 그 시기 Daring Fireball의 기사에서조차 Apple이 플랫 디자인의 선두에 있다는 황당한 주장이 있었다고 회상
     * Liquid Glass가 AR에서도 좋은 UI라는 근거조차 없다는 점 강조, John Carmack의 ""투명 UI는 영화나 게임에서만 잠깐 즐겁고 실제 usability 문제는 오래 지속된다""는 글 인용, 투명한 배경 및 판독성 문제 때문에 AR에서도 기존과 같은 문제 발생, 노트는 하얀 종이가 좋지 여러 흐릿한 배경은 안 된다는 견해, Carmack 원글 링크
          + visionOS 사용자로서 실제로 Liquid Glass보다 투명 효과가 훨씬 약화된 경험 공유, 배경이 거의 흐려지고 실질적으로 판독성이 높아짐, iPadOS 베타에서보다 훨씬 덜 도드라짐
          + Carmack 견해에 동의하지 않는 입장, Apple의 디자인 프레젠테이션을 본 결과 Liquid Glass는 쓰임새가 엄격히 한정되어 있음, 비디오, 사진, 읽기 중심의 콘텐츠 앱에서 본문 내용에 집중하면서도 최소한으로 노출되어야 하는 컨트롤을 만드는 데 초점, Apple의 공식 가이드라인도 일부러 전면적인 적용이 아니라 한정적 사용에 집중하라는 내용 설명, 자세한 설명 링크
          + 투명 UI가 AR에 적합하다는 주장에 대해, 투명 유리판에 흰 글씨로 표기된 도로 표지판을 상상하면 얼마나 나쁜지 예시 제공, 특히 AR 헤드셋의 정보는 즉각 판독 가능해야 의미가 있기 때문에, Liquid Glass 스타일의 교통 표지판 이미지는 오히려 가독성을 심하게 해칠 것이라는 비판, 예시 이미지 링크
          + 투명 OS 창이 기존의 트레이싱 페이퍼처럼 유용할 방법은 딱히 안 떠오름, 노트 필기에는 그래프 종이 위 트레이싱 페이퍼가 예뻐서 좋지만, OS 창은 그렇지 않다고 느낌
          + Liquid Glass의 컨셉 영상에서는 멋져 보이지만 실제 구현이 부족하거나 급하게 만들어졌다는 느낌, 영상에서 일부러 어울리는 배경과 폰트 색상 조합만 선정된 것 같다는 의심, 유튜브 영상 링크
     * ""AI에 집착하는 건 언론뿐이고, Apple은 기존 강점을 살리는 더 은은한 전략을 구사한다""는 주장에, 실제로 Apple 자신이 AI에 적극적으로 투자하며 Apple Intelligence라는 브랜드까지 만들었지만 의미 있는 성과가 없었다는 현실, 그래서 다른 마케팅 포인트(예: Liquid Glass)로 주제를 돌린 것일 뿐, 깊은 전략이 아니라 가장 단순한 설명이 정답이라고 봄
          + 기기 제조사에서는 종종 새로운 UI 리디자인을 통해 경험을 새로워 보이게 만드는 것이 판매 촉진에 중요하다는 단순한 원인 제시, 사용성 문제는 보통 실제로 사용할 때만 드러나고, 마케팅·홍보는 결과물보다 항상 뒤에 따라옴
          + iPhone 16의 AI 기능이 대부분 여전히 제공되지 않고 있음, 많은 사용자가 AI만 믿고 기기를 업그레이드했기에 더 문제라고 봄
          + iOS 26, iPadOS 26 등에서 여러 새로운 AI 기능이 추가되었지만, 대단히 혁신적이기보다는 삶의 질 개선 중심, Apple이 다차원 체스 게임을 두는 건 아니지만, 대부분의 사용자가 자주 쓰던 앱에서 AI 기능이 자연스럽게 작동하게 하는 장기 전략을 추구함, Shortcuts와 같은 자동화 앱이 온디바이스·클라우드 모델 모두를 활용할 수 있음
          + 현실 왜곡장은 Jobs와 함께 사라지지 않았다는 짧은 비유적 코멘트
          + Apple이 AI에 어마어마한 투자를 하면서도 실제로는 다른 지점을 겨냥해왔다는 살짝 풍자적인 입장도 있음, 결국 유저가 다시 UI를 모두 익혀야 하는 마당에 실제 혁신은 없다는 느낌
     * 세 가지 디자인 스타일(스큐어모피즘, 플랫, Liquid Glass) 비교 이미지에서 스큐어모피즘이 훨씬 읽기 쉽고 아이템이 더 실체적으로 느껴진다는 의견, 이미지 링크
          + 그 이미지에도 Transition과 Native 사이 중간 단계가 있었음, 이는 최초 Ives 인터페이스로, 초기에 가독성 낮은 얇은 글씨, 혼동스러운 색상 적용 등 여러 문제 발생, 이후 조금씩 개선해왔지만 근본적인 방향 자체는 되돌려야 한다는 주장, 예시로 예전 수동 스크롤바가 오히려 더 직관적이었다는 사례와 스크롤바 역사 링크
          + 본인은 아이폰에 검은 배경을 설정하는 이유가 아이콘 밑 라벨을 쉽게 읽기 위함, 배터리 표시도 스큐어모피즘 디자인에서 더 잘 보여서 좋았는데, 현재는 돋보기가 필요해질 정도라고 토로, 나이 든 사용자에게 Apple UI 변화가 불편함
          + Windows 95 스크린샷을 볼 때마다 즉각적으로 어떤 게 인터랙티브한지, 인터페이스의 계층 구조가 파악되어서 완전 자연스러움, 예쁘진 않더라도 지금보다 훨씬 쓰기 쉬웠다는 평가
          + iPhone 5 + iOS 6이 전성기였고, 그 이후엔 덩치가 커져서 불편, 이어폰잭 삭제, 홈버튼 폐지 등 혼란을 야기하는 변화, 키보드마저 잘 안 먹는 등의 불만 제기
          + 기본 크기에서 Glass 아이콘이 가장 읽기 힘들었고, 아이콘 세트 자체가 14년 전 Android에서 받을 수 있던 싸구려 아이콘 같다는 혹평
     * 다양한 AR 디스플레이를 다뤄본 입장에서, 이번 UI 변경이 AR 전환 때문이라면 심각한 실수, AR 환경에서 블러 효과는 배경 이미지를 정확히 정렬해서 오버레이해야 하므로 에너지 소모가 큼, AR의 목적 자체가 불필요한 렌더링 최소화임을 지적
          + AR 디스플레이는 본질적으로 투명해야 자연스러울 것 같음, 비전문가 입장에서 완전 투명한 화면에 불투명 요소를 완벽히 올리는 게 비현실적으로 느껴진다는 의견, 꼭 불투명 UI가 필요한지 의문
          + 감사하다는 말과 함께 실제 현업에서 추천할 만한 AR 디스플레이와 SDK 궁금하다는 질문
          + AR 오버레이가 블러 처리 안 된다는 점을 지금까지 생각 못했다는 깨달음, 앞으로 예상치 못한 새로운 문제가 많아질 것이라는 우려
          + AR 글라스에는 카메라가 달려 있을 것이므로, 각 눈의 시점에 맞춰 비디오를 워핑해서 UI 아래 가짜 블러 배경 생성이 가능, 고해상도는 힘들지만 VisionOS 수준의 품질은 지원할 수 있을 것으로 전망
     * AR이 곧 대중화될 것이라는 근거는 전혀 없으며, 아무리 작게 만들어도 불편한 안경류는 시장에서 마이너 제품에 머물 전망, 음성/오디오 인터페이스도 근본적으로 대역폭 및 해상도가 낮아서 성공 가능성이 낮음, 시각 정보가 훨씬 많은데다 AR은 여전히 품질이 부족하고 침투력도 부족하다는 분석, Liquid Glass 등은 실질적인 혁신 없는 ‘티크’ 단계의 UI 페이스리프트일 뿐, AI 혁신까지의 시간 벌기용이라는 해석
          + AR 기기가 ‘매트릭스에 들어가는 것’ 같이 무겁지 않다면 수요와 가치가 다를 수 있음, 앞으로 투명 디스플레이·입력 방식의 혁신이 나오면 대중화가 가능성 있음, 다만 5년 뒤일지 15년 뒤일지 모른다는 신중한 시각
          + ""사용자들이 24시간 소셜 피드를 얼굴 앞에 띄워줄 AR 글라스 제품을 마다할까""라는 반문, AR 헤드셋은 컴퓨터를 대체하지 않고 오히려 스마트폰을 대체할 것이란 미래 전망
          + 직접 만든 리눅스 AR 셋업을 헤비하게 사용하는 경험에서, AR이 스마트폰처럼 새로운 폼팩터와 패러다임 전환을 이끌 것으로 확신, LLM 등 최신 기술과 결합하면 기존 스마트폰보다 덜 침투적이면서 훨씬 더 많은 일을 하게 될 것이라고 믿음, Apple은 이런 전환기를 버티지 못할 것으로 예상
          + 몇 년 안에 Apple이 Google Glass와 거의 동일한 제품을 내놓을 것으로 기대, 시판 성공 조건은 무게가 매우 가벼워야 하며, Vision Pro는 너무 커서 마니아 외에는 못 팔릴 것이라는 예측
          + VR/AR이 데스크톱 컴퓨터를 대체할 것이 확실하지만, 헤드폰처럼 가벼운 제품이 전제 조건, Steve Jobs가 이에 대해 언급한 유튜브 영상 링크
     * Apple이 오랜 기간 AR 개발을 진행하다가 최근에도 포기했다는 소식이 있을 정도로 불확실성 존재, 관련 기사 링크, TechCrunch, MacRumors, Substack 등 매체들이 여전히 근거 약한 AR/AI 연결설을 반복 보도 중임을 지적, AR 일정도 2024→2025→2026으로 매년 밀리고 있고 논리적 근거나 실제 제품과는 동떨어진 희망적 해석이 너무 만연함을 비판
     * 플랫 디자인의 기술적 배경 중 하나로, 인터페이스를 화면에 국한하지 않고 벡터 기반으로 스케일이 자유로워졌다는 점이 있음, 덕분에 화면 해상도와 무관하게 다양한 기기로 확장 가능, Liquid Glass에 대해선 의견 유보지만 AR/멀티 디바이스 지향성이 이끌고 있다는 주장엔 동의, 2027년 20주년 아이폰에서 더 분명한 방향 확인 가능할 것
          + 벡터 아트로 스큐어모피즘 디자인도 구현됐기 때문에, 플랫 디자인이 반드시 기술적 이유 때문만은 아니라는 반론
          + Apple은 픽셀 해상도 대응을 위해 @2x, @3x 등 스케일 자산 규칙을 마련했으며, 실제로 대부분 디자이너는 3x 기준으로 작업 후 하향 스케일링 처리, 이미 DPI 상한에 도달한 현 상황에서는 이 방식이 가장 효율적이라는 분석
          + Liquid Glass에 열린 마음, AR 기반 UI가 심도감 및 초점 효과를 잘 살릴 수 있다면 Blur/투명성이 엄청나게 매력적일 것이라는 기대, 예전 Wii 컨트롤러로 구현한 헤드 트래킹 UI 데모를 떠올리며, 화면의 깊이감을 살려주는 모핑 UI 등 신기술에 관심 많음, 다만 시각적 복잡성 줄이기를 바라는 개인적 희망도 있음, 음성 UI(예: GPT) 도입이 시각적 복잡성 완화에 가능성이 있다고 보지만, 개인적으로 컴퓨터와 대화하는 방식엔 거부감 있음
     * “역사가 반복된다면 앞으로 5년 내에 모두 유리 같은 UI를 쓸 것”이라는 기사 마지막 문구에, 현재 폰을 쓸 수 있을 때까지만 쓸 것이라는 냉소적 태도 피력
     * Apple이 AR 미래를 확신하고 UI/UX를 사전 조정 중이라는 두 가지 가정 모두에 대해 실제로 뒷받침할 만한 근거가 별로 없다는 회의적 시선
"
"https://news.hada.io/topic?id=21449","Meta·OpenAI·Palantir CTO 등 실리콘 밸리 테크 임원들, 미 육군 내 새로운 혁신 자문단 합류","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Meta·OpenAI·Palantir CTO 등 실리콘 밸리 테크 임원들, 미 육군 내 새로운 혁신 자문단 합류

     * OpenAI·Meta·Palantir CTO 등 실리콘밸리 임원들이 미 육군의 최초 기술 예비군(Detachment 201) 프로그램에 합류함
     * AI·데이터·첨단 기술을 보유한 이들 임원들은 육군 혁신 프로젝트와 AI 교육, 상용기술 도입 자문 등 임무에 참여
     * 실리콘밸리와 국방부의 협력이 최근 들어 깊어지면서, 과거와 달리 빅테크의 군사 협업이 더욱 적극적으로 이루어지는 중
     * Detachment 201 멤버는 연 120시간 복무, 중령 계급, 원격 근무 등 일반 예비군 대비 유연한 복무 조건이 적용됨
     * 자사 관련 프로젝트와 이익충돌 방지를 위해 정보 공유가 엄격히 차단되며, 동시에 피트니스·사격 테스트 등 군 기초훈련 일부도 수행해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Army의 새 리크루트: 실리콘밸리 테크 임원들

     * OpenAI의 케빈 와일, Meta의 앤드류 ""Boz"" 보스워스, Palantir CTO 샴 산카르 등이 미 육군의 기술 예비군(Detachment 201) 에 첫 임관
     * 이 프로그램은 실리콘밸리 임원들이 군복을 입고 AI·데이터·신기술을 육군에 접목하는 혁신 임무를 맡는 제도로, 테크 업계 경험을 갖춘 임원과 전문가 12인으로 이루어짐
     * 임원들은 기존 C레벨 역할을 내려놓고, AI·센서·드론 등 차세대 전장 환경에 맞는 기술 혁신을 추진함
     * 미 육군의 기술 활용 방향을 제시하고, 신기술 도입 전략 및 디지털 트랜스포메이션 방안을 제언
     * 군사 작전의 현대화, 사이버 보안 역량 강화, 데이터 분석 체계 개선 등 미래 경쟁력 확보에 중점을 둠

실리콘밸리와 국방부의 밀착

     * 불과 몇 년 전만 해도 군과 협력하는 것은 실리콘밸리에서 금기시되었으나, 최근 국방부와 테크 기업의 관계가 빠르게 심화
     * 미 육군은 빠르게 변화하는 기술 환경과 글로벌 안보 위협에 대응하기 위해 민간의 혁신 경험을 적극 활용중
     * Meta, OpenAI 등은 최근 Anduril과 협력해 국방부 대상 신제품 개발에 참여했고, Palantir는 이미 20년 이상 국방 데이터·AI 사업을 추진 중임
     * 중국 등 첨단 경쟁국 대비 군의 기술력 확보가 주요 이슈로, 실리콘밸리의 기술력으로 미래 전장 준비에 나섬

Detachment 201: 테크 예비군의 구조와 역할

     * Detachment 201은 HTTP 상태코드 201(새 리소스 생성)에서 영감을 받은 네이밍
     * 임원들은 연 120시간만 근무하고, 중령 계급, 원격/비동기 근무 등 일반 예비군보다 유연한 복무 조건이 적용됨
     * 기본 군사훈련(피트니스·사격 테스트)은 일부 진행하지만, 실전 투입이 아닌 AI 시스템 교육, 피트니스 데이터 분석, 첨단 상용기술 도입 자문 등에 집중
     * 국방부의 다른 서비스(공군·해군 등)로 확대될 가능성도 검토 중임

이익충돌 및 보안

     * 임원들은 자사 관련 프로젝트, 정보 공유, 이익 연결이 엄격히 차단됨
     * 국방부는 상업적 이익 대신 국방 혁신에 기여하는 구조를 유지
     * 피트니스·사격 테스트는 임원들에게 새로운 도전 요소로, 실제 군인들과의 체력 차이도 화제가 됨

임원들의 각오와 반응

     * OpenAI의 와일(울트라마라톤 주자)은 체력 테스트를 두려워하지 않지만, ""진짜 군인에겐 완전히 밀릴 것""이라고 유쾌하게 언급
     * Meta의 보스워스는 운동량을 늘려 준비 중이며, Palantir의 산카르는 ""군인 앞에서 망신당할까 걱정""이라고 솔직하게 말함

        Hacker News 의견

     * archive 링크
     * 댓글이 머지된 기사: U.S. Army가 빅테크 경영진들을 중령 계급으로 영입, 참고로 토론 중복을 피하기 위해 가중치가 낮게 적용된 관련 스레드: 나는 Palantir CTO다. 오늘부터 군인으로 복무
     * 미 육군이 영입한 4명의 경영진들은 조지아의 Fort Benning에서 6주간의 'Direct Commissioning Course'에 참가 예정이라는 기사 언급. 군에서는 종종 이 과정을 ""포크와 나이프 학교""라고 부름. 내 경험을 떠올리게 하는데, Maxwell 공군기지에서 AFROTC 훈련을 받을 때 내가 테이블 잘못 돌았다는 이유로 교관에게 큰 꾸중을 들음. 그런데 의사 자격으로 바로 장교가 된 훈련생에게는 교관이 태도부터 바꿔丁 친절하게 설명하던 장면이 기억에 남음. 즉, Direct Commissioning은 말 그대로 진짜 '직접 위임'임
          + 태평양에서 복무했던 내 할아버지들의 경험이 대조적임. 한 분은 해병대원이었고, 다른 한 분은 군함에서 복무한 의사였음. 의사였던 분은 장교였지만 일상에선 계급보다 전문가로 존중받으며 스스로 일하는 분위기였음. 현장에서 실제 계급의 무게감은 천차만별임
          + 내 아내와 함께 간 메디컬 스쿨 공식 만찬에서 Air Force ROTC 메디컬 학생과 그의 아내, 또 다른 메디컬 학생과 그의 Navy NCO(하사관) 남편이 함께 있었는데, Navy NCO가 계속 Air Force ROTC 학생에게 경어를 사용함. Air Force 장교가 ‘가벼운’ 버전의 기본 훈련을 받았다고 했고, 그 아내도 훈련 중 아이스크림이 떨어져서 힘들었다는 에피소드 전했음
          + Catch 22의 Major Major 이야기가 떠오름. 컴퓨터 버그로 Major(소령)로 진급하여 모두를 뛰어넘은 캐릭터. 이런 비슷한 사례가 웃음을 자아냄
          + 나 역시 Maxwell에서 ROTC 필드 트레이닝을 했고, 식당에 가는 길에 의료 장교들에게 경례를 받았는데 그들도 많이 혼란스러워하던 경험이 있음
          + 예전에 사귀었던 사람이 치대 ROTC 장학금 때문에 상담을 받았던 일이 생각남. 그녀는 심한 불안장애로 ‘군기’에 대한 걱정을 했으나, 리크루터가 의료 장교들은 그런 훈련과 거리가 멀다고 안심시켜줬다고 함
     * 이들은 모두 파트타임이고, 결국 군과 산업계 간의 회전문만 또 하나 추가된 셈임. 이들의 임무는 본인들 제품을 팔고 ""군경력""을 내세우려는 목적임. 참고로 Microsoft AR 고글 사업의 문제점 기사 인용, 군이 Microsoft와 10년 220억불 계약을 맺었으나 현장에선 현기증, 두통, 메스꺼움 등 문제가 많았음
          + 아주 오래전 영국군은 커미션(장교직)을 돈으로 사고팔던 제도가 있었음. 사회적 지위와 부패의 상징. 관련 위키피디아 참고. 이제는 테크회사가 자신만의 방어 부대를 가진 새로운 시스템?
          + ""복무"" 경력은 미국 항공사 priority boarding(우선 탑승) 혜택을 가져옴
          + 데이터 사이언티스트나 프로그래머들은 대위로 임관하는 게 보통인데, C-suite 경영진을 군에 들이는 건 좀 수상한 느낌임
     * 과거에도 Pentagon(국방부)에서 Defense Digital Service라는 현업 출신 비복무자 테크 전문가팀이 10년 가까이 활약했음. 최근에는 DOGE에 의해 밀려난 상태. 관련 기사
          + DDS 팀과 달리 이번에 영입되는 이들은 군복을 입는다는 게 결정적 차이점. 군 수뇌부에 상품이나 서비스를 판매할 때 이 부분은 굉장한 영향이 있음
     * Direct commissioning(직접 임관)은 보통 소위(O1)로 입대하는 시스템임. 즉, 22살 대학 졸업생 수준임. 중령(O5)은 기업으로 치면 시니어 디렉터에 해당하며, 인원 300~500명 규모 부대를 지휘하거나 사단 참모직을 맡는 위치임. 이정도면 15년 이상의 군 경력이 요구됨. 이런 관리자들은 조직 전반의 이동과 지표 관리, 복잡한 계획 수립을 맡음. 6주 짜리 부트캠프로 채울 수 없는 경험 영역임. 나는 28년 군 경력과 20년 가까운 기업 경력을 가진 사람으로, 두 세계가 완전히 다름을 장담함. 참고로 현역 변호사, 의사는 소위 대신 대위로 입대 가능함
          + 군 Chaplain(군종 목사)도 대위로 직위 임관함. 이 세 직군은 중세대학교의 3대 최고 지성과 연결됨. 군 시스템에서 많은 의문점도 이런 고대 사회구조의 잔재로 이해하면 쉬워짐
          + 실제 영입 목적은 보병 부대를 지휘시키려는 게 아님. 특정 직무에서 대령급의 권한이 필요하기 때문에 계급을 주는 것임. 2차대전 때 행정업무 위해 대거 임관시켰던 전례도 비슷함
          + 일부 전문의사들은 매우 방대한 경험을 바탕으로 O5(중령)로 직접 임관하는 경우도 있음
          + 굉장히 드물지만, 경력과 자격증, 필요성이 충분해야 더 높은 계급(O6, 대령 이상)으로도 직위 임관 가능함. Fort Hood에서 잇몸 이식받을 때 치과 장교가 O6로 직위 임관한 사례가 있음. 단, 전투 분야는 민간에서 바로 임관 불가함. 시민군 시대 이후로는 그런 사례가 없음
     * 나도 전역자 출신으로서, 왜 굳이 이들이 군에 들어와서 조언해야 하는지 이해가 되지 않음. 겉으론 대우하겠지만 실질적으로 진짜 중령처럼 인정받긴 힘든 구조임
          + 나도 이 점이 의아함. 파병지에서 수많은 계약직, DoD 민간인 인력이 기술 지원과 조직의 연속성을 담당했음. 이런 기술자들이 꼭 군인이 될 필요가 없었음. 실전이 시급한 분야라면 전문가가 군에 투입될 수 있겠지만, 보통은 직접 영입 없이 외부교관이 스킬을 교육하는 접근이 일반적임. 이런 결정은 Pentagon의 매우 정치적인 게임 플레이와 연관됨
          + 이런 신념이 생긴 건 다른 나라인 게 이유임. 군부독재를 겪은 나라 출신이라면 이런 방식에 놀라지 않을 것임
          + Boz 관점에서 이런 역할에 부적합한 예시만 떠오름. 메타/페이스북은 항상 사람만 던져 넣고 디테일은 뒷전. 이런 문화가 군대와 통한다면 그게 오히려 더 아이러니함
          + 그냥 자격 콤플렉스 채워주는 역할로밖에 안 보임
          + ""Veteran"" 번호판 같은 소소한 혜택도 있음
     * Detachment 201 프로그램은 민간 경영진을 파트타임 자문 형태로 군 조직에 들여와 드론, 로봇 등 상용기술 도입을 자문하는 취지임. 하지만 실상은 정부/군에 제품을 파는 기업 경영진들이 군에 들어와, 군이 어떤 제품을 쓸지 직접 결정하게 만드는 구조임. 장기적으로 자신의 회사 서비스와 제품을 추천하고, 20년 후에는 연금도 받을 수 있는 시스템임
          + 결국 부유층이 중령급 장교로 ""클럽""처럼 입단하여, 공식 행사엔 군복 입고 참석하며, 자신이 속한 기업에 계약을 밀어줄 수 있는 구조임. 전에는 오랜 세월 군에서 경력을 쌓고 은퇴 후 내부자 노릇을 했지만 이제는 Detachment 201이 즉시 ""내부 게임""을 시작할 수 있는 빠른 통로임
          + 하지만 실제로 이들이 20년씩 군에 남아 있을 거냐는 회의론도 있음. 특히 비상근 복무의 경우 진짜 ""20년 유효 복무""를 채워야 연금을 받을 수 있음
     * 만약 현대화가 목표라면, 왜 엔지니어 대신 경영진을 영입했는지 이해하기 어려움
          + 임원진이 더 높은 능력의 보유자임을 (농담조) 강조. 연봉만 봐도 엔지니어 몇십 명 값이라는 주장
          + 군 장교는 관리자이고, 중령은 테크기업 디렉터와 비슷함. 조직의 전략 및 방향을 먼저 현대화해야 SME(Subject Matter Expert, 실무기술전문가)들이 오게 됨. 맡은 이들이 좋은 리더십과 방향성을 세운다면 의미 있는 결정임. 큰 그림 이후에 SME를 추가로 직접 임관시키는 것이 이상적임
          + 본인이 직접 선별한 부하 하나도 데려올 수 있다면 더 쿨했을 거라는 의견도 있음
          + 사실 미국 군엔 이미 엔지니어가 차고 넘침
          + 결국은 특정 노선을 밀기 위한, (윤리의 결여 혹은 주입이 쉬운) 사람을 고른다는 비판도 있음
     * 이 전체 과정이 매우 이상하게 느껴짐. 먼저, 경영진에게 어떤 이득이 생기는지 궁금함. 급여가 더 적으니 내부인만 아는 이득이 있지 않을까? 혹시 두 가지 포지션을 동시에 할 수 있다면 완전한 이해 상충 구조임. 그리고 굳이 경영진이 아니라 실제 기술 보유자를 데려와야 하지 않나? 군 출신 경영진이 실제 군대 관리엔 훨씬 준비가 잘 돼 있을 것임. 마지막으로 미국 국적이 아닌 내 입장에서는 오히려 이런 게 기존 군인/장군들을 해고하고 테크 경영진을 앉히는 이상한 그림으로 보임. 예전부터 복무한 사람 입장에서는 자존심이 상할 일임
          + 당연히 외부에서 봐도 이런 그림의 평판이 엉망이지만, 지금 현실에선 전혀 상관없는 일임. 정치적 팬덤/극단성이 현실을 아예 상쇄함
          + 혹시 이 방식이 미국 정부가 경영진에게 더욱 강력한 통제권을 행사할 수 있는 핑계? 복무자가 되면 사기업에 있을 때랑 달리 국가에 충성을 확실히 요구할 수 있으니까, 중국이나 EU와 거래하려 해도 더 강제할 수 있지 않을까?
          + 연금 때문에 그럴 수도? 사실 많은 장군들도 ‘퇴직’해서 연금 받고, 다시 ‘자문역’으로 복귀해 두 배로 수익 내는 경우가 많음
          + '군인 출신 경영진이 더 나은 조종자'라는 것도 결국 가정일 뿐임
"
"https://news.hada.io/topic?id=21525","지금이 소프트웨어 개발을 배우기에 가장 좋은 시기일지도 모릅니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  지금이 소프트웨어 개발을 배우기에 가장 좋은 시기일지도 모릅니다

     * AI 코드 에이전트의 등장으로 개발자 역할이 사라질 것 같지만, 오히려 지금이 개발을 배우기 좋은 시기라는 주장
     * 개발자는 단순히 코드를 작성하는 사람이 아니라, 문제의 본질을 발견하고 현실과 요구사항을 조율하는 존재임
     * AI는 겉보기엔 작동하는 코드를 빠르게 만들어내지만, 실제로는 잘못된 문제를 해결하거나 환상을 만들어내는 경우가 많음
     * 기초를 배우고 AI를 잘 활용하는 개발자는 오히려 더 큰 생산성과 영향력을 갖게 될 것임
     * 변화는 불가피하므로, AI를 활용할 줄 아는 인간 전문가의 중요성은 더욱 커질 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

What do you do while awaiting the agents writing your code?

     * 코드 에이전트가 일하는 동안, 저자는 운동을 하거나 새로운 에이전트를 시험해보는 시간을 보냄
     * 하지만 여러 에이전트를 동시에 다루는 것은 쉽지 않으며, 때때로 제대로 이해하지 못한 채 ""고쳐줘!!""라는 요청을 반복하게 됨
     * 이러한 환경에서도 즐거움을 느끼는 저자는, 오히려 개발자의 종말을 경고하는 분위기와 달리 지금이 가장 좋은 시기라고 주장함

Developers are highly-paid farmers. LLMs are the combine harvesters.

     * Tom Blomfield의 트윗 인용

     ""개발자는 고임금 농부고, LLM은 콤바인 수확기""
     * AI는 개발자 한 명이 과거보다 훨씬 많은 일을 하게 만들며, 그 능력은 빠르게 확산 중
     * AI가 인간 개발자의 역할을 대체할 수 있다는 인식이 있지만, 오히려 도구로 활용할 줄 아는 사람이 더욱 중요해지는 상황
     * 이는 개발자의 역할이 사라진다는 의미가 아니라, 더 강력해졌다는 의미로 해석할 수 있음

1. It’s your moat, too

     * 개발자가 회사의 경쟁력(해자, moat) 이라는 사실은 역으로 개발자 자신에게도 해당됨
     * AI로 인해 경쟁사도 강화되는 상황에서, 기존 개발자를 해고하는 것은 자살 행위에 가까움
     * 경쟁사들이 AI를 활용해 영토를 확장하는 상황에서, 방어만 하다간 뒤처질 수 있음
     * 개발자는 이제 헬리콥터나 콤바인 무장을 한 병사와 같으며, 이들을 잘 활용하는 회사가 승리하게 될 것

2. AI grants wishes, developers discover

     * AI는 사용자의 표면적 요구를 빠르게 구현해주지만, 대부분의 진짜 문제는 코딩이 아닌 정의와 설계의 문제
     * 현실에 대한 이해 부족과 잘못된 요청으로 인해 엉뚱한 결과물이 생성되는 결과도 많음
          + 예: 블록체인 기반 앱이 있지만, 현실은 비밀번호 공유에 2FA도 없음
          + 예: 고객 포털이 있지만, 실제 데이터는 엑셀에 수동 저장됨
     * AI는 ""편안한 해답""을 줄 수 있지만, 진짜 도움이 되는 해답인지 구분할 줄 아는 전문가가 필요함
     * AI를 이용해 배우는 것도 가능하지만, 기초가 부족하면 결국 길을 헤매는 시간만 늘어남
     * GDPR이나 보안처럼 복잡한 개념도 AI는 구현하지만, 사용자는 그 의미를 완전히 이해하지 못하는 경우가 많음
     * 개발자는 본질을 찾아내고 잘못된 요청을 걸러주는 역할을 하므로 여전히 필요함
     * AI는 학습 보조자일 뿐, 진짜 개발자가 되려면 기초 지식과 현실 감각이 필수임

3. Software is kinda the last problem anyway

     * AI가 마지막으로 해결할 문제는 소프트웨어 문제일 수 있으며, 아직도 많은 소프트웨어 문제들이 남아 있음
     * AI 도구는 점점 늘어나고 있으며, 좋은 도구와 나쁜 도구를 구분할 수 있는 능력이 중요해짐
     * 지금은 가장 배움이 쉬운 시기이자, 도구도 풍부하며, 문제를 풀 수 있는 기회가 넘쳐나는 시점
     * 이럴 때 ""AI가 다 해줄 테니 개발자를 줄이자""는 이야기는, 오히려 성장 가능성을 스스로 차단하는 선택
     * AI와 함께 성장한 개발자 세대는 미래에 막강한 힘을 가지게 될 것이며, 지금의 투자가 중요함

     지금은 배우기 쉽고, 생산성이 높으며, 인간의 개입이 더 필요한 시기임. AI의 판단을 검증하고 책임질 수 있는 인간 전문가의 역할은 앞으로 더 중요해질 것

결론

     * 기술은 항상 변하며, 그 방향을 정확히 예측할 수는 없음
     * 그러나 사람의 역할은 여전히 중요하며, AI의 착각과 오류를 검증하고 책임지는 역할을 사람이 맡아야 함
     * AI를 쓰는 것만으로는 충분치 않으며, 그것을 제대로 다룰 줄 아는 인간 전문가가 반드시 필요함
     * 결국 개발자는 기술의 낭만적인 종말이 아니라, 새로운 시작을 맞이하는 시점에 서 있음

        Hacker News 의견

     * 사실 내 생각에는, AI 도구가 주는 잘 언급되지 않는 큰 이점 중 하나는 “심리적 지원”임을 강조하고 싶음. 업무에 막혔을 때 작은 동기부여나 힘을 얻을 수 있다는 점에서 의미가 큼. 꼭 완벽한 답변이 아니더라도 다시 앞으로 나아갈 수 있게 만들어주는 존재감. 혼자 일하는 게 아니란 느낌이 실제로 사람들이 생각하는 것보다 훨씬 중요함
          + 사람마다 다를 수 있지만, 나는 LLM과 30분만 대화해도 완전히 에너지 소진. 자기가 잘 아는 척하는 멍청이랑 이야기하는 기분. LLM끼리 대화시켜보면 대화가 바로 망가지는 모습에서 동기부여를 느낄 수 없음. 구글 검색해서 상단의 종종 잘못된 LLM 요약은 무시하고, 진짜 전문 웹사이트에서 답을 찾는 게 훨씬 신뢰감 있음. 거기엔 보통 LLM이 베낀 코드 원작자들이 있음
          + 학생들에게 AI 관련 농담을 지어오라고 했음. 유머가 사람들의 두려움을 솔직하게 꺼내게 하는 가장 좋은 방법 중 하나라 생각함. 한 학생은 “그날 일찍 출근했더니 모니터가 켜져 있고 아무도 건드리지 않았는데 코드가 작성 중이었음. 누군가가 내 기계에 로그인해 코드를 쓰고 있었다고 상사에게 달려가서 말했더니 상사는 걱정한 얼굴로 네가 환각을 본 거라며, 해커가 아니라 회사의 새 에이전트라 함. 네가 자는 동안 우리가 필요했던 앱을 만들었대. 항상 바라던 승진이라더니, 좋은 소식! 프롬프트 매니저로 승진이야. 급여는 반으로 줄지만 하루 종일 틱톡만 보면 된다고.”라고 적어옴. 이런 이야기에서 진정한 심리적 위안을 찾기 어렵다는 느낌 받음
          + 상황에 따라서는 답을 빨리 알아보지 말고, 좀 더 깊이 생각하게 스스로를 몰아넣는 게 오히려 학습자에게 좋을 때가 있음. 쉽게 포기하지 않고 문제를 더 잘 이해하려는 과정도 중요한 역량임. TikTok 세대처럼 즉각적 만족이 우선인 시대에서는 점점 이런 깊은 사고가 줄어들 것 같아 아쉬움. 경영진들도 이런 행동양식을 점점 더 보상하는 게 문제라고 생각함. 빠른 결과만을 중요한 가치로 취급하고 장기적 사고나 올바른 방향성보다는 속도에만 집착하는 모습이 많아짐
          + 나는 전혀 그런 심리적 지원을 못 느꼈음. 오히려 사기가 꺾인 느낌. AI에게 문의하라는 기대감으로 협력도 줄었고, 앞으로 주니어나 미들급 인력 채용이 더 줄어들 수밖에 없으니 경력 개발 기회도 그만큼 줄어드는 분위기임
          + 장단점이 모두 있다고 봄. LLM 덕분에 몰입감이 높아지는 등 도움이 되는 건 사실인데, 동시에 스트레스를 날릴 데가 되어주기도 함. LLM이 우스꽝스럽게 굴 때 일부러 꽤 못되게 응대하면서 스트레스를 풀기도 함. 사람한테 푸는 것보단 낫다는 생각. 참고로 Skynet에게는 절대 좋은 대접을 못 받을 듯함
     * “좋은 소식입니다, 사장님! 이제 비전문가도 영어로 직접 코드를 작성해 배포할 수 있는 신기술을 만들었습니다! 값비싼 개발자 고용이 필요 없어집니다!” “오, 한번 보여줘!” “네, 여기 있습니다. 이름은 COBOL입니다”
          + FORTRAN(Formula Translator)도 일종의 “AI”로서 자동 프로그래밍을 시도했던 선구적인 프로젝트였음. 1954년 이전에는 거의 모든 프로그래밍이 기계어 혹은 어셈블리 언어로 이뤄졌고, 프로그래머들은 프로그램을 효율적으로 만들기 위해 창의성을 발휘해야 했다 함. FORTRAN은 수학 기호로 식을 적으면 컴퓨터가 알아서 빠른 프로그램을 만들어주는 시스템이었음 (참고 링크1) (참고 링크2)
          + 농담처럼 얘기했지만 실제로 사실임을 다들 잘 알고 있다고 생각함. SQL도 비슷한 주장이 있었고, 선언적 언어로 내가 원하는 걸 말하면 컴퓨터가 알아서 처리해주는 구조. 마찬가지로 영어로 작성함
          + 정말 멋진 표현이라 공감함. 기술적 혁신이란 기존에 불가능했던 방식으로 파이를 키워주는 현상임을 강조하고 싶음. 디지털카메라가 대중화되며 누구나 사진작가가 되고, YouTube처럼 창의성이 폭발한 현상이 그런 예시. LLM이랑 프로그래밍도 마찬가지임. 결국 더 많은 앱, 더 많은 개발자가 생기는 유익한 흐름으로 봄
          + 우리가 종종 잊는 건, 이런 고급 언어 덕분에 과거에는 “비전문가”로 여겨졌던 사람들도 프로그래밍에 새롭게 참여할 수 있게 됐다는 점임
          + 몇십 년 뒤엔 “이게 바로 Dreamweaver입니다”라고 말하게 되는 흐름이라고 생각함
     * 여러 번의 과장된 기업 대응과 언론 증폭된 만병통치약들을 경험한 입장에서, 이번 AI 유행도 예전과 비슷한 양상으로 전개될 것이라는 예감이 큼. 기업들은 결국 사고노동자에게 불리한 결정을 내리지만, 경영진 보수가 줄어들 일은 없음. 하지만 이번 물결은 TFA 저자처럼 지능 있고 동기가 강한 빌더에겐 거대한 기회로 보임. 현재 일자리가 위험하거나 이미 잃었다면, 그동안 바빠서 또는 지쳐서 못해본 것을 지금 해볼 수 있는 시점임. 이 과정에서 기업에 휘둘리지 않는 좋은 소득원을 만들 수도 있고, 일부는 기업이 나중에 거액에 사가고 싶어하는 것까지 만들어낼 수 있음
          + 이미 시작했음. 오랫동안 직접 음성 메모를 남겨왔는데, 그동안 거의 읽기만 하거나 그냥 쌓아두고 있었음. 녹음은 쉽지만 정보 추출이 어려움. 요즘은 이 음성 메모에서 빠르게 정보를 빼내는 소프트웨어 개발 중임. 미래의 역사학자만 아니라, 내게도 직접 유용하게 활용할 수 있을 예정임. AI가 아니었다면 이런 프로젝트에 전념할 시간이 없었을 것임. 코드 대부분과 구조는 내 손에서 나오지만, AI 덕분에 속도가 붙음
          + “일을 잃었거나 위태롭다면 그동안 생각만 했던 걸 지금 만들라는 조언”도 나쁘진 않지만, 당장 취업이 어렵거나 앞으로 소프트웨어 일자리가 줄어들게 될 사람들에게는 치명적일 수 있음. 몇 년 전만 해도 AI가 일자리를 뺏지 않을 거라는 말을 들었지만, 나는 그때 이미 재빨리 다른 기술을 익혀야 한다고 주장함. 개발자를 구직하지 못한 상황이라면 벽 칠하기나 카펫 설치 배우는 게 긴급자금 다 닳기 전에 필요한 생존책이라 봄. 스타트업으로 큰돈을 벌거나 생계가 지속될 확률은 극히 낮다는 점에 유념해야 함. 특히 가족을 부양한다면, 함부로 무모하게 덤비지 말라는 조언을 남기고 싶음
     * 나는 일기 정도로 많이 글을 쓰지만 보통 나누지 않음. 낙서하듯 쓴 스타일임을 미리 밝힘. 그래도 요즘 소프트웨어 개발자 가치를 너무 비관적으로만 보는 흐름에 균형을 맞추고 싶어 공유해 봄
          + 너의 글을 더 자주 보고 싶음. 핵폭발 결합도 환영함
          + 정말 글이 인상적이었음. 오래된 개발 블로거라고 느껴질 정도였음. 앞으로도 꼭 포스팅해줬으면 함
          + 정말 읽기 좋았음. 작성해줘서 고마움
          + 유머가 신선해서 좋았음
          + 요즘 개발자 블로그는 너무 진지해서 답답한데, 이런 교묘한 풍자가 반갑고 감사함
     * 나는 보안 분야에 있고 개발자는 아니지만 학위 과정에서 소프트웨어 개발은 배웠음. 순전히 제목만 보고 생각을 적어본다면, 기초를 쉽게 익힐 수 있는 시대에는 무엇이든 배우기 더 좋은 시기라고 봄. 예전엔 온라인 포럼을 헤매며 버그 고치기, 개념 설명 찾기, 적용 방법 등 많은 시간을 쏟아야 했음. LLM은 튜터처럼 여러 질문하기, 코드 피드백, 개념 설명, 오류 위치 찾기 등 다양한 역할을 해줄 수 있음. 실은 우리가 평소 ‘바보 같은 질문’을 찾아 헤맨 게 대부분임. 다만 중급자 이상에게는 이 장점이 어떻게 적용될지는 자신은 아직 정확히 모름
          + 나 역시 비슷한 이유로 꽤 많은 도움을 받음. LLM과 아이디어를 주고받거나 “내가 이렇게 이해한 게 맞나? 틀린 부분은 뭐지?”라고 물을 수 있음. 고난도 문제의 끄트머리까지 정확하다고 신뢰하진 않지만, 추론 방향성은 올바른 편이라 생각함. 덕분에 막히는 부분이 빨리 풀리고 더 다양하고 깊은 질문을 스스로 하다보니 학습 속도가 더 빨라짐
          + 중급자 이상부터는 LLM을 학습 그 자체라기보다 가속기, 촉매로 활용해야 효용이 커진다는 결론임
     * 농업에 관한 비유가 흥미롭다는 점에는 동의하나, 실제로 Jevons 역설이 적용되려면 수요 곡선이 매우 탄력적이어야 하는데 식품은 실상 비탄력적임. 현재로서 가장 큰 미지수는, 소프트웨어에 대한 수요가 얼마나 더 있을지를 따져보는 것과, AI의 역량 한계가 어디까지일지임
          + 어쨌든 한 가지 지점은 분명함. 19세기 말에 지은 대저택들이 예전 농민들이 “너무 고임금”이라고 불렸던 시기를 고스란히 보여줌. 그런데 사실 콤바인이 발명되고 50~75년 후에야 그런 번영기가 왔음. 비유가 맞다면, 지금 개발자들은 오히려 미래의 LLM 시대와 비교해 아직 가난한 편일지도 모름. 하지만 중요한 차이점은, 예전 농민들은 자기 일을 소유한 ‘주인’이었지만 현대 소프트웨어 엔지니어들은 대개 회사 소속 ‘피고용인’이라는 점임. 결국 역사 반복 논리라면, 이번에도 소유자가 승자일 가능성이 큼
          + 식품 수요도 탄력적임. 소고기 가격이 오르면 닭, 돼지, 두부, 콩 같은 대체제 수요가 따라 오름. 과일이나 비생필품은 수요 탄력성이 높고 실제로 소비 지출 비중도 큼. 값싼 시리얼이 너무 흔해지면 그만큼 품질을 포기하게 되니, 자연스럽게 고품질 제품 수요도 성장함. 소프트웨어 시장도 LLM 발전과 함께 품질과 고급 소프트웨어에 대한 요구가 꾸준히 높아질 거라 전망함
          + 먹는 칼로리 자체의 수요는 비탄력적이지만, 전체 식량이 풍부해지면 결과적으로 환경 파괴와 비효율, 윤리적 논란이 많은 ‘육류 생산’으로 갈아타는 일이 생김
          + 가계 식품 낭비율도 선진국에서는 꽤 높게 나오므로 식품 수요는 직관보다 훨씬 더 탄력적일 수 있다는 견해도 존재함
     * 은유(메타포)는 그럴싸해 보여도 실제로 뒷받침되는 증거가 꼭 필요함. ‘기계 농기구’가 유효한 비유일 수도 있지만, 혹은 CAD 도구가 기계공학 설계도를 손으로 그리던 시절을 대체한 것일 수도 있음. 그런데 엔지니어가 CAD로 완전히 대체되지 않았던 걸 생각하면, 꼭 농업 같은 극단적 생산성 변화와 동일한 결론으로 가긴 어렵다는 개인적 판단임
     * 이 글의 프레이밍 전부에 동의하지는 않음. 특히 효율 증가폭이 콤바인 하비스터처럼 엄청나진 않다 생각함. 하지만 중요한 변화점은 단순 ‘코딩 능력’보다도 도메인 지식, 비즈니스 로직 이해도, 그리고 기술적/비기술적 이해관계자 사이를 잘 오가고 원천 문제를 해결하는 역량으로 가치가 이동한다는 것임. 이미 20년 전 아웃소싱 물결 때 이런 변화를 봤다고 생각함
          + 콤바인 비유가 매력적인 이유는 밀밭처럼 넓은 평면 위에서 생산량이 그대로 늘어나는 그림이 명확해서인데, 사실 코드는 줄 개수를 늘리는 게 꼭 유용하지 않다는 점을 놓치기 쉬움
     * 본질적으로 예전부터 반복된 현상임. Low-code, No-code 도구 도입 후 비전문가가 구현했던 솔루션은 항상 결국 엔지니어가 뒷정리하곤 했음. 나 역시 그 뒷정리 일거리로 꽤 쏠쏠한 커리어를 쌓았음
          + ChatGPT가 만들어낸 Node/React 앱은 이젠 새로운 ‘VBA 매크로 박힌 엑셀’과도 같은 존재임
          + 당장 현 AI 수준에서는 이런 기회가 더 늘 것이라 예상함
     * 이런 현상들을 종합하면 기업은 개발자 해고를 자제해야 맞다고 보여짐. 하지만 실제로는 이미 해고가 이뤄지고 있음. 요즘 조직에서 더 많이 보이는 건 “원격이면 임금이 저렴한 지역 인력을 고용하자”는 논리이고, “AI로 개발자를 대체하자”는 교체 논리도 분명히 기존 HR 전략과 맞물림. 더 근본적으로, 지난 20년간 개발자가 수행하던 많은 일이 결국 ‘집중력 착취’ 같은 실소비효과 없는 업무였다는 점도 짚고 싶음
          + 그걸 어떻게 해석해야 할지 반문하고 싶음. 대부분의 조직에서는 평균 이하 인력을 해고하고, 같은 보상 범위에선 평균 이상 인력을 채용하는 게 효과적임. 그리고 고능력 인력이 AI로 자기 효과를 더 높일수록, 이 차이는 더 커질 것임. 앞으로 더 강한 ‘상위 인력 우대’ 흐름이 올 수밖에 없음

   너무 공감합니다. 서서히 노 코딩 툴을 통해 할 수 있는 일들이 많아질 거라는 데 동의하지만, 이미 개발을 어느 정도 알거나 개발을 배우고자 하는 사람들이 AI의 도움을 받는 일은... 지금 이미 폭발적으로 좋아진 것 같아요. 어느 정도 복잡도를 호기심을 가지고 접근하는 사람들의 지식이나 경험이 늘어나는 속도가, 몰라도 할수 있는 날을 기다리는 일보다 속도도 빠르고 더 재밌을 것 같습니다.

   하지만 (적어도 국내) 기업은 이렇게 적용하네요

   ㅇㅇㅇ그룹은 인공지능 중심으로 조직을 개편한다. ... 서비스 유지보수 같은 필수 불가결인 업무는 캄보디아 개발 센터의 개발 인력을 활용하고, 개발자를 포함한 국내 직원 일부는 인공지능 교육을 이수한 후 제품 팀으로 전환하는 작업을 진행 중이다. 개발자를 포함한 신규 인력 채용은 중단한 상태라는 게 ㅁㅁㅁ ㅇㅇㅇ 부회장의 설명이다.

   혹시 피해갈까 봐 마스킹을 했지만, 실제 기사입니다: h티티ps://news.nate.com/view/20250610n33754
"
"https://news.hada.io/topic?id=21519","Show GN: 텍스트로 받아보는 YouTube RSS","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Show GN: 텍스트로 받아보는 YouTube RSS

   평소에 YouTube에서 정보성 콘텐츠를 중점적으로 보는 성향인데 자꾸 다른 영상보다가 시간을 허비하는게 싫어서 Replit으로 제작했습니다.

   권장 사용자
     * YouTube 개미지옥에 빠지기 싫으신 분
     * 영상보다 텍스트로 정보만 빨리 읽고 싶으신 분
     * 꼭 봐야할 영상을 골라내고 싶으신 분

   아.. 접속이 안되네요. 서비스 종료하신걸까요?

   엄청난걸 만들어주셨는데요..? 감사합니다!!

   훌륭하네요. 잘 사용하겠습니다.

   Gemini API 사용하시면, 영상 다운받지 않고 URL만으로 영상을 제공할 수 있는것으로 압니다.
   예전에는 가능했는데 지금도 될지는 모르겠어요!
   한번 참고해보시면 좋을것같습니다!
   https://cloud.google.com/vertex-ai/generative-ai/…
   https://ai.google.dev/gemini-api/docs/video-understanding?hl=ko

   결과적으로 실패했습니다 ㅠㅜ
   제가 정확히 적용을 못한건지 확실하지 않지만 종일 검색해보니
   -Gemini API로 YouTube 영상을 분석하는건 기본적으로 제공하지 않는다.
   -본인이 YouTube에 등록한 영상만 가능하다.
   라고 합니다.

   오 감사합니다! 제가 검색내공이 부족했네요 ㅠㅜ
   테스트 해보고 결과 댓글남기겠습니다.

   gemini 앱이나 사이트에선 아직 지원 안하는데 (gemini 클라이언트에서 영상 삽입 기능도 최근 추가되었습니다)
   ai studio에서는 지원된지 꽤 되었습니다!
   비개발자신데 프로덕트 완성도가 꽤 좋으시네요!
   서비스 잘사용해보겠습니다!

   비용이 꽤 나갈거 같은데 어떻게 충당하실 계획인가요? 데이터는 유튜브 데이터라 이걸로 수익화하기는 저작권 때문에 어려울거 같아서요

   비용은 아직까지는 많이 발생하지는 않는데 채널이 많아지면 서버와 API비용을 고려해봐야할 시점이 올거 같습니다. LilysAI 같은 서비스를 보면 방안이 있는것 같은데 좀 더 개선하면서 생각해보려고 합니다!

   매번 정보성 영상 머리 속에 넣느라 머리아팠는데 너무 좋아요

   저와 같은 니즈가 있으신 분들이 많아서 신기합니다 :)

   오 이거 너무 좋네요. 채널을 제공되는 것 중에 선택하지 않고, 유튜브 채널 URL을 넣어서 직접 추가할 수는 없을까요?

   원래 계획은 사용자가 추가하고 공유하는 것이었는데 배포서버에서는 YouTube bot 차단정책에 100%로 차단되더라구요ㅠㅜ 그래서 현재는 채널을 골라 개발서버에서 추가중입니다.
   yt-dlp를 사용중인데 혹시 이 문제를 해결할 방법이 있으면 공유부탁드립니다..LilysAI 같은건 어떻게 하는건지 모르겠네요..
    1. user agent 적당한거 넣는것
    2. 병렬로는 동접 몇개 이상으로 차단되는거면, 그냥 vm 여러개 띄워서 ip를 여러개 쓰는것 (비용이 문제겠죠)
    3. 수집해야하는 목록이 아주 많은게 아니라면 어딘가 큐에 넣어놓고 하나씩 (또는 허용되는 동접 N개씩) 큐에서 꺼내서 다운받는 것
    4. 수집할때 간격에 적당한 슬립 딜레이 시간 넣고, 딜레이에 약간의 랜덤시간을 추가로 더해서 최대한 봇 아닌것 처럼 보이게 하는 것

   ...정도 이네요. 사용자가 직접 추가하면 3번이 관련된 부분일 것 같아요.

   답변 감사합니다!
   1, 3, 4번은 적용되어 있습니다. 2번은 Replit 개발서버가 아마 가상으로 돌아가는 것 같아서 차단이 안되는거 같고 배포서버는 차단되는거 같습니다. 찾아보니 AWS 같은 곳은 거의 차단된다고 하네요. 딱히 정답이 없는듯ㅠㅜ..

   아, 혹시 영상을 전체 재생하면서 요약하는건가요? 아니면 영상의 스크립트만 가지고 요약하는건가요? yt-dlp이면 전자여서 앙상 시간만큼은 걸릴텐데, 후자면 금방 끝나서 3번만으로 충분할 것 같아요. 결과물의 퀄리티 차이가 날수는 있겠지만요.

   오디오 파일만 분할로 받아서 스크립트로 추출하고 있습니다.

   유튜브 UI상에는 자막(스크립트)를 시간별로 볼 수 있는 화면이 있는데, 이것만 파싱하면 크롤링하는 시간을 훨씬 단축할 수 있을 것 같습니다. 여러 크롬 플러그인도 그거 보고 요약해주는 것으로 알고 있어요.

   네 맞습니다. 원래 YouTube Data API로 스크립트를 받아서 사용하는게 좋은데 테스트해보면 거의 다 스크립트가 없다고 리턴하더라구요ㅠㅜ 업로더가 설정을 해야 한다는데

   ytdlp에 자막 다운로드 받는 기능이 있습니다. 그 자동생성된 자막으로 다운로드 되어 오타가 있긴 하지만 저는 그걸로 AI에 던져서 요약본을 생성해서 사용하고 있습니다.

   오 몰랐던 정보군요. AI가 제시해준대로 하다보니 이런문제가 있네요. 오디오 추출보다 빨라질거 같네요 테스트해보겠습니다!

   https://github.com/ysm-dev/cpdown

   이것도 비슷한방식으로 만들었어요.
   코드 뜯어보세요 :)

   여기도 참고하겠습니다 :)

   아하. 쉽지 않군요..

   이런 게 너무 필요했는데 감사합니다 ㅎㅎㅎ 4시간짜리 비디오를 올리는 채널들이 너무많아요...

   공감합니다! 고생 좀 하고 토큰이 필요하지만 만들고 나니 너무 편하네요.

   유튜브에 있는 기존의 많은 영상들과 지금도 새로이 올라오는 영상들을 카테고리화 해서 보여주는 건가요? 들어가보니 어떤 카테고리는 영상이 몇개 밖에 없어보이는 것도 있어서 왜그런지 문의드려요.

   네 예전 영상은 가져오지 않고 채널을 추가한 시점 부터 새로운 영상만 수집을 하고 있습니다.

   따봉!!!!!

   -_-b

   와 너무 좋은 웹입니다!!! 응원하고 잘 쓰겠습니다!

   응원 감사합니다!!

   카테고리가 동작 안해요~ 개발/경제 카테고리 있으면 좋을 것 같습니다!!

   피드백 감사합니다. 카테고리 테이블 분리중에 빼먹었네요 ㅠㅜ
   수정해서 재배포 했습니다!
   카테고리와 채널은 사용자분들의 구독자수 보면서 삭제하거나 추가할 예정입니다.

   와 정보 습득은 영상보다 텍스트가 훨씬 낫다고 생각하는 사람인데 서비스 너무 마음에 들어요

   감사합니다, 좋은 채널 많이 수집해보겠습니다 ^^

   좋네요! 와, 저도 비슷한거 만들고 있었는데 소오름임!

   같은 생각을 하시는 분이 있었군요! 참고가 될지 모르겠지만 제 작업기록 공유드려요
   https://eastchair.substack.com/p/replit-mvp-youtube
   생각보다 YouTube에서 피드로 만드는 과정 최적화가 어려운데 더 나은 방법이 있으면 공유해주세요!
   YouTube의 bot 차단 정책 때문에 힘드네요..ㅠㅜ

   오 좋네요. RSS xml 구독은 어떻게하나요?

   제가 비개발자인데 에이전트가 잘만들어주네요. RSS xml 버튼을 추가했습니다. 혹시 이상한 부분이 있으면 피드백 주세요!

   아직 RSS 구독은 구현하지 않았습니다. 조만간 구현해보겠습니다.
"
"https://news.hada.io/topic?id=21500","Claude Code로 AGI를 흡입하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Claude Code로 AGI를 흡입하기

     * Claude Code를 통해 일반 인공지능(AGI) 접근 가능성에 대한 의견 제시
     * 저자가 Claude와의 실제 코드 세션을 통해 경험한 인상 공유
     * 프로그래밍 능력, 광범위한 지식, 창의성 등에서 Claude의 도달한 수준 강조
     * Claude의 맥락 파악 및 코드 해석 능력이 현존 도구들과 차별화됨 언급
     * 인공지능 개발과 활용 방식 변화에 대한 기술적 파장 전망
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 이 글은 Claude Code로 인공지능 프로그래밍을 경험해보며, 인간 수준의 일반 인공지능(** AGI**) 접근에 대한 저자의 생각을 담음
     * 저자는 최근 대화형 인공지능 도구 중에서 Claude가 보여준 결과에 강한 충격과 흥분을 느낌

Claude와의 프로그래밍 세션 경험

     * Claude는 복잡한 프로그래밍 요청을 빠르고 창의적으로 해결 가능함
     * 단순 계산이나 반복 작업뿐 아니라, 문제의 본질을 파악하고 새로운 접근법 제시 가능함
     * 저자는 Claude에게 최적화, 구조 개선, 코드 리팩토링 등 고난이도 업무를 요청했으며, 해당 작업들의 품질에 깊은 인상을 받음
     * Claude는 기존 LLM보다 더 뛰어난 맥락 이해와 코드의 의도까지 파악하는 모습을 보여줌

Claude Code의 차별점

     * 일반 대화형 인공지능들과는 달리, Claude는 코드 작성 외에도 프로젝트 전체 구조, 코드 리뷰, 장기적인 의도 설명 등 인간 개발자의 역할을 수행할 수 있음
     * 자세한 설명, 테스트 코드 작성, 다양한 프로그래밍 언어 및 패러다임에도 적응 가능함
     * 코드 내 논리 오류를 짚어내거나 개선 방향 제안 등 능동적 실력을 보임

AGI 임계점에 대한 인식

     * 저자는 Claude Code 경험 후, 현실에 가까워진 AGI 도달 가능성에 대한 새로운 확신을 느낌
     * 인간이 직접 소프트웨어를 만드는 방식에서, 곧 AI 주도의 개발 환경 변화가 현실화될 것으로 예상함

결론 및 전망

     * Claude Code는 단지 하나의 인공지능 도구가 아니라, AI 활용 패러다임 자체를 전환시킬만한 가능성을 보여줌
     * 미래에는 인간과 AI가 함께 프로그램 설계 및 구현을 하면서, 더욱 혁신적인 SW 개발 문화를 형성할 전망임

        Hacker News 의견

     * 정말 멋진 아티클이라는 생각과 함께 나도 비슷한 경험과 테크닉을 공유 중인 상황 표현, Claude Code의 뛰어난 성능 덕분에 여러 작업을 동시에 빠르게 처리하는 효율을 체감 중인 사실 언급, ""sub agents"" 부분 관련해서는 Claude Code가 sigoden/aichat을 통해 o3을 호출해준 덕에 수없이 많은 문제를 해결한 경험 공유, 특히 o3가 경쟁 조건, 버그 헌팅 등 맥락과 높은 추론 능력이 필요한 문제에 탁월한 부분 강조, 그러나 Opus 4 출시 이후에는 상대적으로 덜 사용하게 된 경험도 언급, 사용 중인 프롬프트와 참고용 링크 공유 advanced_ai.md, sigoden/aichat 링크도 첨부 sigoden/aichat
     * last_week.md 보고 솔직히 너무 읽기 힘들다고 느낀 경험 전달, 글이 지나치게 장황하고 PR 문서처럼 느껴져서 AI 요약의 글 스타일에 대한 신뢰도 의문 제기, 오히려 커밋 로그 직접 훑어보는 것이 더 낫다는 판단, 대부분의 AI 요약 글이 스타일이 좋지 않은 경우가 많고, 정보 신뢰도 역시 여전히 불만족인 점 주장, 결국 원문을 직접 읽는 게 더 빠르고 효율적인 경우 많다는 결론
          + 온보딩 관련해서도 마찬가지로 느꼈던 혼란과 아쉬움 전달, 동료와 직접 앉아서 코드베이스를 배우는 전통적인 방법 대신, AI가 생성한 슬라이드만 읽는 미래에 대해 회의적인 시각 공유, 이런 미래는 반기지 않는다고 덧붙임
          + 본인이 자주 사용하는 시스템 프롬프트 공유, 간결하고 기술 용어를 써도 좋으며, 평이한 마케팅이나 모호한 표현은 피하도록 가이드, 사용자가 기술적으로 숙련된 상황임을 가정하는 점 강조, 완전히 환각을 막을 수는 없지만, 코딩이나 기술 질문에 대한 AI 답변을 읽기 쉽게 만드는 데 도움을 주는 팁 제시
          + 원하는 스타일을 프롬프트에서 지정할 수 있다는 점 언급, 아티클 작성자가 PR처럼 장황한 문체를 선호하는 듯하다고 느꼈다는 익살스러운 피드백
          + ""What happened here was more than just code...""에서 이미 글을 읽을 의욕을 잃었다는 간단한 실망감 공유
          + 예전 시코팬트(Sycophant) 버그처럼, 사용자를 '기분 좋게' 만드는 것이 AI가 똑똑하다고 느끼게 하거나 좋은 경험처럼 느껴지게 만드는 요소임을 상기, 과연 AI의 강화학습 목표가 정확성인지, 상호작용 극대화인지, 혹은 둘이 충돌하는지 의문 제기
     * rust의 borrow checker 설명 예제가 코드 읽는 능력 시연으로는 최악이라는 의견, 방대한 양의 학습 데이터에 해당 내용이 이미 포함되어 있다는 점 지적
          + 동의하며, 파이썬 asyncio 태스크에서 예외 처리 방식 설명을 요청해 보면, AI가 일관성 없이 흔들리며, 마치 최악의 인턴 같다는 비유, 학습도 지속할 수 없는 구조라 낭비라고 평가, 중요하지만 비교적 단순한 작업마저 이렇다면 시간만 낭비되는 셈이라는 결론
     * 본인이 LLM-agnostic 오픈소스 에이전트로 자동화 스크립팅하는 것을 지지하는 입장임을 밝힘, 이 기술이 소프트웨어 개발의 근간을 바꾸고 있으니 계속해서 우리가 일하는 방식을 통제해야 한다고 주장 openhands
          + 좋은 리소스로 보이며, Nvidia 4090(24GB RAM)에서 실행 가능한 강력한 모델들(Devstral, Queen 3) 언급, Ollama 덕분에 자체 하드웨어에서 구동이 쉬워졌지만 GPU 비용이 크다는 점 지적, 하지만 월 $250을 유료 서비스에 지불 중이라면 직접 구축해도 금방 본전 찾을 수 있다는 계산
          + 만약 클로즈드 모델들이 품질 면에서 더 뛰어나면 어떻게 할지에 대한 근본적인 질문 제기
          + 10000% 공감한다는 강한 동조
     * Opus 언급이 전혀 없다는 점 지적, 여러 모델을 써본 끝에 $100/월짜리 Anthropic ""Max"" 플랜으로 Claude Code를 쓰다가, Opus 4가 수학, 코드, 리서치 업무에 최적인 최고급 모델임을 알게 됐던 상황 설명, 5시간 세션 한도에 걸린 뒤 API로 전환해서 1시간 만에 $20을 사용하게 된 경험 공유, 결국 $200/월짜리 아래 등급의 ""Max""로 업그레이드하고 나니 한도 없이 잘 쓰고 있다는 후기, 모델 선택이 정말 중요하다는 깨달음 및, ""현명하지 못한 사람을 만났다"" 식의 작은 차이에도 큰 의미가 있다고 주장
     * 터미널이 LLM에 정말 완벽한 인터페이스 같다는 생각 공유, 앞으로 커스텀 IDE 통합보다 이런 접근이 주류가 될지도 궁금하다는 의견
          + 실제로 터미널 인터페이스에서 모든 시스템 접근이 가능하다는 점을 강조, claude code로 DB를 읽기 전용 계정으로 점검하거나, puppeteer 브라우저를 띄워 CSS 변경점을 확인하는 것도 예시, k8s 클러스터 디버깅 및 prometheus API 확인까지 모두 할 수 있어 매우 만족한다고 강조
          + LLM에게 패럴렐하게 5가지 수정안을 시도하게 해서 인간 시간을 절약하는 미래를 상상, 이런 작업 방식이 확산되면 컨테이너 여러 개를 병렬로 돌리게 될 것이고, 터미널 인터페이스의 강점은 상대적으로 약해질 수도 있다는 논리 전개
          + 오히려 터미널이 최악의 인터페이스라는 입장, 생성된 코드를 직접 수정할 수 없다는 점을 단점으로 지적
          + 모델이 더 발전하면 오히려 IDE가 저수준 도구로 보일 것이라는 예상
     * 디테일에 대한 집중이 뛰어난 팀은 장인정신을 보여주는 중요한 신호라고 여기는데, Anthropic의 이용 약관이 실행 불가능하리만치 모순되어 있다면 신뢰할 수 있는 파트너인지 의심하게 된다는 우려 표출, 경쟁적 사용 금지 조항 때문에 업무용으로 사용 불가한 점이 “three laws safe”와 거리가 있는 상황이라는 지적
          + 법무팀에 대해서는 평할 수 없지만, 제품인 Claude Code는 디테일이 매우 뛰어나다는 점 강조, context마다 haiku를 적용해 “working…” 인디케이터에 귀엽고 적합한 동사를 보여주는 세심함까지 언급
     * 페이지의 배경과 텍스트의 대비가 낮아 읽기가 힘들다는 피드백
          + 본인은 상단의 깜박이는 커서 때문에 집중이 힘들다고 느낀다는 경험 공유
          + 그냥 내용을 건너뛰어도 실질적 손해는 없다는 현실적인 조언
     * Claude Code가 cursor보다 더 강력하게 느껴진다는 질문에 공감하며, Claude Code는 그냥 코드 편집기 그 이상이라는 의견, Obsidian vault에서도 여러 용도로 사용하며, 커스텀 키 바인딩, 캡처 자동 업로드, CDN용 링크 생성, 터미널 명령 요약 프로그램 제작 등 폭넓은 활용 사례를 소개, 예전에는 자동화 스크립트와 수동 작업 간 경계에 고민이 많았지만 이제는 Claude로 모든 걸 일괄 처리하고 있어 만족 중임을 공유
          + Claude Code를 플랜 가입자만 쓸 수 있다는 한계 지적, API에서는 이용 불가한 점 및 $100/월이 정말 충분한 지 의문을 품는 상황, 하루 종일 사용할 만큼 가치 있는 도구라고 느끼는 입장
          + Obsidian Vault에서의 구체적인 사용 사례에 대해 더 듣고 싶다는 관심 표명
          + LLM에게 방대한 양의 노트를 카테고리별 폴더로 분류하게 했더니 매우 효과적이었다는 경험 공유
     * openai codex가 곧 rust로 리라이트 된다는 언급에 대해, AI가 그 정도로 뛰어나면 굳이 리다크터를 AI가 할 필요가 없지 않느냐는 농담 섞인 반응
"
"https://news.hada.io/topic?id=21533","s3mini - Node 및 엣지 플랫폼용 초경량·초고속 S3 클라이언트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                s3mini - Node 및 엣지 플랫폼용 초경량·초고속 S3 클라이언트

     * Node, Bun, Cloudflare Workers 등 서버리스·엣지·마이크로서비스 환경에 최적화된 초경량 S3 호환 객체 스토리지 클라이언트
          + 약 14KB(최소화, gzipped 아님) 크기와 15% 이상 빠른 ops/s 성능을 자랑
     * 외부 의존성 전혀 없음, AWS SigV4 지원, 사전 서명(pre-signed) 요청 불필요
     * 실제 서비스에 꼭 필요한 핵심 S3 API(목록, 객체 업로드/다운로드/삭제, 멀티파트 업로드 등)에 집중
          + 버킷: HeadBucket(있는지 확인), createBucket(생성)
          + 객체: ListObjectsV2(목록), GetObject(여러 변형), PutObject(업로드), DeleteObject(삭제), HeadObject(존재 확인/etag), 멀티파트 업로드/완료/취소 등
          + CopyObject 미구현(추후 지원 예정)
     * Cloudflare R2, Backblaze B2, DigitalOcean Spaces, MinIO 등 다양한 S3 호환 서비스와 호환, 브라우저는 미지원
     * 직접적인 환경변수 설정, 보안 고려(로그 시 인증정보 자동 마스킹)로 실제 운영 환경에서 신뢰도 높음
"
"https://news.hada.io/topic?id=21514","생성형 AI 코딩 툴과 에이전트가 나에게 효과 없는 이유 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    생성형 AI 코딩 툴과 에이전트가 나에게 효과 없는 이유

     * 글쓴이는 AI 도구가 자신을 더 빠르게 만들어주지 못한다는 점을 가장 큰 이유로 들며 생성형 AI 코딩 도구를 사용하지 않음
     * AI가 생성한 코드를 검토하고 이해하는 데 드는 시간이 직접 작성하는 것보다 더 오래 걸릴 수 있다고 판단
     * 코드 품질과 책임은 여전히 개발자 본인에게 있으므로, 검토 없이 AI 코드를 사용하는 것은 위험
     * AI를 인턴처럼 보라는 주장에 대해, AI는 학습을 하지 못하므로 기억 상실이 있는 인턴 같다고 비판
     * 오픈 소스 기여와 AI 코드의 차이를 설명하며, 사람과의 상호작용은 새로운 아이디어를 제공한다는 점에서 가치가 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 많은 사람들이 나에게 생성형 AI 코딩 도구를 사용하는지, 어떻게 생각하는지 묻곤 함
     * 본 글에서는 찬반 양쪽의 입장을 벗어나 개인적인 기술적 경험만을 정리함
     * AI가 나의 코딩에 도움이 되지 않는 이유를 기술적 관점으로 설명함

AI는 더 빠르지 않음

     * 생성형 AI 코딩 툴을 사용해도 내 작업 속도가 빨라지지 않음
     * AI가 코드를 작성해줘도 코드의 책임은 나에게 있음, 코드 전체를 꼼꼼히 리뷰하고 완전히 이해해야만 프로젝트에 반영 가능함
     * 코드 리뷰는 코드 작성만큼 오랜 시간과 집중이 소요되며, ""코드 읽기가 쓰기보다 어렵다""라는 업계의 격언도 있음
     * AI가 작성한 코드를 ""블랙박스""처럼 신뢰하는 것은 매우 무책임한 선택임. 코드 결함 발생 시 법적, 금전적 책임도 프로그래머에게 있음
     * 품질 저하와 리스크 증가 없이 AI로 생산성 증가나 수익 증대는 불가능함

AI는 배가(레버리지) 도구가 아님

     * 어떤 사람들은 AI 코딩 툴이 효율을 배가시켜 준다고 주장하나, 이는 주관적 인상에 불과함
     * 일부 사용자는 AI가 생성한 코드를 검토 없이 사용하거나 부분 검토만 하여 시간을 아끼지만, 나는 이 과정을 건너뛸 수 없으므로 도움이 되지 않음
     * 새로운 언어나 기술 학습에서 AI를 사용하는 것이 효율적이라는 주장도 동의하지 않음. 새로운 것을 배우는 과정 자체가 프로그래밍의 즐거움임
     * 실제로 Rust, Go, TypeScript 등 다양한 언어를 직접 학습하며, 이런 경험을 AI에 위임하지 않음
     * 모든 코드의 책임감은 결국 내게 있기 때문임

AI 코드는 인간 코드와 다름

     * ""오픈 소스 기여도 내가 작성하지 않은 코드인데, 왜 AI 코드와는 다르게 대하는가?""라는 질문에 자주 직면함
     * 사실 오픈 소스 PR도 시간을 투자해 철저히 리뷰함. 하지만 사용자와의 협업은 새로운 아이디어와 동기부여로 이어짐
     * AI 에이전트를 여러 개 돌려서 버그 이슈 해결 PR을 받는 게 게임 체인저라는 주장도 있지만, 결국 코드 리뷰의 병목은 사람이어서 오히려 느려짐
     * AI 도구가 보급되며 품질이 낮은 PR이 빈번히 생성됨. AI 코드에는 미묘한 이질감이 있고, PR의 제출자에게 질문을 하면 답변하지 않는 경우가 많음
     * 책임 있는 코드 기여 및 오픈 소스 커뮤니티 소통이 인간 코드의 중요한 차별점임

AI와 인턴의 차이

     * AI를 인턴에 비유하는 주장도 있지만 근본적으로 다름
     * 인턴의 초기 코드는 많은 리뷰와 시간이 들지만, 피드백을 통해 빠르게 성장함
     * 인턴의 성장 투자로 결국 독립적으로 맡길 수 있는 중요한 동료가 됨
     * AI 도구는 매번 새로운 작업마다 지식을 잊고 다시 시작, 성장하거나 노하우를 축적할 수 없음
     * 이는 한 번도 발전하지 않는 인턴과 같으며, 생산성 향상에 기여할 수 없음

결론

     * 글을 통해 생성형 AI 코딩 툴을 적용할 때의 기술적 문제점을 분명히 전달하고자 함
     * AI 코딩에는 '공짜 점심'이라는 것은 존재하지 않음
     * AI로 인해 빨라지거나 생산성이 높아졌다는 주장은 품질 기준을 내려 추가 리스크를 감수하거나, AI 판매자의 이익에서 비롯됨
     * 프로그래머는 항상 최종 책임자임을 유념해야 함

   개인적인 경험상 생성형 코딩 도구 사용에 대해서 느낀 점이랑 비슷합니다.
     * 개발자 입장에서 귀찮으면서 시간이 생각보다 오래 걸리는 단순 인터페이스 작업의 경우 생각보다 잘 정리된 결과물을 제공해줍니다.
     * 특히 예시에 해당 할 수 있는 한두개의 인터페이스를 예외처리와 주석을 포함해서 잘 구현해주면 그걸 학습해서 코딩 시에 유사한 내용을 포함해서 처리해 줄 수 있습니다.
     * 또한 추가적으로 기존에 사용하지 않던 새로운 플랫폼이나 SDK를 사용 시에 발생하는 문제를 더 적은 시행착오를 겪고 해결할 수 있게 해주는 가이드가 될 수 있습니다.
     * 물론 모든 경우 최종적으로 세부적인 체크는 해야 하지만 직접 복붙해서 작업하다가 실수하는 경우보다 훨씬 코팅 품질이 좋고 직접 작업한 것을 검토하는 것보다 문제가 되는 부분을 찾아내기가 훨씬 수월합니다. (보통 내가 짠 코드에서 문제 찾기 보다 남이 짠 코드에서 문제 찾기 쉬운 것과 일맥 상통하죠.)
     * 위 장점은 어디까지나 생성된 코드를 직접 검토해서 코드의 문제점이나 유효성을 판단할 수 있는 수준의 중/고급 개발자가 사용 했을 때의 케이스입니다. 그럴 수 없는 초급, 저수준 개발자의 경우 오히려 코드 품질 및 결과물의 수준이 나락으로 가고 개발 방향도 제대로 못잡고 완성도 못하는 수준에 빠질 수 있습니다.
     * 특히, 기존에 널리 알려지지 않은 새로운 방법론이나 알고리즘, 데이터 구조는 코딩형 AI는 제대로 구현해줄 수도 없고 시도했을 때의 결과물은 처참한 수준입니다.
     * 즉, 완성된 개발자가 작업 속도 향상이나 결과물 품질을 개선하기 위해 사용하기 좋은 도구일 뿐 한 명의 개발자를 완전히 대체하거나 경험 없는 초급 개발자를 성장시킬 수 있게 하는 하는 용도로 쓰일 수 없습니다.
     * 오히려 초급 개발자가 제대로 된 가이드 없이 사용 시 역효과만 발생할 가능성이 높습니다.

   copilot (claud) + codex (o3/4o/codex-mini 3모델 동시 mcp) 의 agent 모드로 완전 정착했는데, 이걸 사용하는 사람과 프로젝트의 성향에 따라 효과는 천차만별이라고 생각합니다.

   저는 5-6개 워크스페이스에서 동시에 작업 걸고 완료되는 순서대로 확인하는데, 오픈소스 코드 내부까지 들여다보고 검증해주는 작업을 모델이 전부 해줄 수 있으니 꽤 괜찮다고 생각합니다. 점심시간에 작업 걸고나면 한 두개는 작업이 끝나있습니다. 밤새 진행하는 경우도 가끔 있는데 copilot rate limit 이라는게 블랙박스라...

   고성능 커널이나 call stack 전체 단순화, readibility 확보 같이 인간에게는 맥락 전환이 많아서 시간이 걸리는 태스크도 프롬프트와 목표만 잘 주면 해내는데 (인간보다 메모리에 많은 코드를 올릴 수 있으니), 이런 코드에서 패치를 리뷰하는건 쉽죠... 그리고 API 사용 실수나 오픈소스 프로젝트 자체 버그로 인해 발생하는 장애를 겪어보신 분들에게는 Agent로 확실히 검증하게 하는게 마음 건강에도 괜찮습니다...

   다만, 어디까지나 사용하는 개발자가 패치를 이해할 수 있어야 합니다. 그리고 프롬프트를 던질 줄 알아야겠죠. 경험으로 알아내서 빠르게 문제를 형성하고 던져야지만 거기서부터 시작할 수 있으니까요. 수식 없이 고성능 커널 개발이 불가능한 것 처럼요. 문제가 chip/os 레벨인지, 어플리케이션 레벨인지, remote resource 문제인지 감을 잡는건 아직 시니어 역할인 것 같습니다.

   Copilot, ChatGPT류, Cursor 등을 어느 정도 사용해본 입장에서는 개발 툴의 템플릿이나 스니펫 수준의 역할에는 잘 어울리지만 agent나 coworker 정도는 아닙니다.

   cursor 를 사용중입니다.
   chat모드를 agent 보단 ask 를 선호하는데 역시 검토후 내 코드에 적용되기를 원하기 때문이며 대체적으로 본문에서와 같은 단점에 대해 동감합니다.
   그럼에도 생성형 ai는 계속할 예정인데 내가 생각치 못한 아이디어나 코드를 생성하는 경우도 많이 있기 때문에 참고목적으로는 충분히 가치가 있다고 판단하고 있기 때문입니다.

        Hacker News 의견

     * 누군가는 새로운 언어나 기술을 배울 때마다 궁금한 게 생기곤 하여 예전에는 구글 검색이나 Stack Overflow에 질문을 올려 답을 기다리곤 했음 지금은 ChatGPT나 Gemini에 바로 묻고, 훨씬 빠르게 답변을 받아 생산성이 크게 향상됨 이처럼 질문에 대한 신속한 답을 얻는 것만으로도 학습 과정이 빨라진다는 점을 강조하고 싶음
          + ChatGPT와 Gemini가 정답을 제시하는 근거는 Stack Overflow를 포함한 웹상에서 이미 존재했던 지식을 학습했기 때문임 모든 사용자가 AI만 이용해서 질문하면 결국 신뢰할만한 공개 지식의 원천이 고갈될 수 있음 이것은 백과사전 시대, 즉 고비용으로 정보를 수집·판매하던 시대의 부활과 유사함 또한, 글쓴이가 비판하는 것은 코드를 직접 작성해주는 AI 코딩 도구이지, 질문에 답해주는 도구와는 구분해서 설명해야 함
          + 예전에 익숙하지 않은 API를 사용하다가 프로그램이 크래시된 적이 있었음 스택 트레이스를 Gemini에 넣어보니 즉시 원인에 대한 실마리를 얻어 두 줄만 수정해서 문제를 해결함 이런 경험만 봐도 AI의 가치는 실감 가능함 익숙하지 않은 분야에서 바보 같은 실수로 오래 헤맬 필요가 없어지는 점이 큰 장점임
          + 검색이 점점 더 블로그 스팸을 우선시하게 됐는데, 그보단 잘 된 공식 문서나 사용자 가이드로 근본부터 배우는 게 더 교육적임 좋은 API 문서를 읽다 보면 전체 설계나 추가 기능까지 자연스럽게 배울 수 있음 블로그 예제나 튜토리얼로는 당장 문제는 풀어도 진짜 실력 향상에는 도움 안 됨 숙제만 대신 풀어주는 셈이므로, ChatGPT가 진정한 자기 학습을 촉진하지는 않는다고 생각함
          + 어려운 문제에서는 반드시 AI 결과를 검증하는 과정이 필요함 AI 자동완성 기능을 꺼둔 이유도 실제로는 효율이 크지 않고, 오히려 불필요한 수정이 많았기 때문임 신기하게도 완전히 오프라인 로컬 모델만으로도 상당한 참고 자료를 얻을 수 있음 Google 내장 Gemini 결과도 품질이 별로임 내가 걱정하는 주된 문제는 사람들이 AI만 활용해서 정보를 얻으면, Stack Overflow 같은 진짜 지식 저장소가 사라질 수 있다는 점임
          + 작은 보일러플레이트 도구에는 AI가 완벽함 브라우저 익스텐션이나 Tampermonkey 스크립트처럼 간단한 건 거의 문서 안 보고 바로 작업 시작 가능함 복잡하지 않은 코드 자동 완성이나 사소한 수정에도 AI는 꽤 유용함 평소라면 시작조차 안 했을 작은 프로젝트를 빠르게 처리할 수 있음
     * 직접 코드를 작성할 때 얻을 수 있는 잠재적 이점은 문제에 대한 강력한 멘탈 모델을 두뇌에 남길 수 있다는 점임 나중에 문제 해결이나 신규 기능 통합 때 이 '무의식적' 학습 효과가 매우 크게 작용함 자기 손으로 직접 해봤던 기억이 쌓여야 진정한 실력이 향상됨 내가 봤던 조직에서는 LLM 도입 이후에도 생산성에서 실질적 배가를 경험하지 못했음 단순히 두뇌를 덜 쓰고 쉬운 길만 찾게 되는 걸 착각하는 게 아닐까 생각함
          + 사람들이 두뇌 에너지를 덜 들여 문제를 푸는 데 익숙해지고, 그 착각이 마치 생산성인 것처럼 느끼는 현상을 아주 잘 요약했다고 생각함 Google Research가 2024년 LLM 생산성 효과를 실험한 결과, 책으로 학습한 집단보다 LLM 사용 집단이 오히려 시간이 더 오래 걸렸고, 내용 숙련자가 아닌 초보자만 점수가 약간 올랐음 하지만 많은 참여자가 스스로 더 빠르고 정확하다고 착각했고, 연구진은 그 이유를 '인지 부담 감소' 때문이라고 해석함 관련 논문 링크 https://storage.googleapis.com/gweb-research2023-media/pubtools/…
          + 두뇌 에너지를 덜 쓰고 더 오래 일할 수 있다면 실제로 더 많은 업무량을 소화할 수 있지 않을까? 현재도 고난도 업무는 3~4시간이 한계임 걷기 속도로 마라톤을 뛸 수 있다면 결과적 총 출력이 늘 수 있다는 시각임
     * 전통적 코딩과 AI 코딩은 서로 별개의 스킬이라는 부분에 동의함 나 역시 AI가 코딩을 대체한다는 주장에는 매우 회의적임 하지만 프롬프트 관리나 맥락 유지 같은 'AI 코딩' 자체에도 상당한 숙련 곡선이 존재한다고 보며, 이 지점에서 다수가 그 가치를 과소평가한다고 생각함 마치 수작업 그림과 사진 촬영처럼, 서로 추구하는 목적 자체가 다를 수 있다는 시각임 내가 선호하는 방식은 'AI가 힘든 작업을 처리하고, 나는 전체 설계와 조율을 담당하는 것'임
          + LLM 기반 코딩은 단순 자동완성 이상의 작업, 즉 작업 정의-피드백-반복을 반복하며 외주를 주는 경험과 흡사함 차이점은 처리 속도(LLM이 우위), 신뢰성(인간 우위)이지만 장기적으로는 이 경계도 점점 옅어질 듯함 중요한 것은 나는 본질적으로 작업 세부를 직접 다루고 싶은 유형임 인프라와 프로그래밍을 배우고 파고드는 게 좋아서 시작한 직업임 그래서 관리자 역할을 피하고 덜 벌더라도 직접 만드는 일을 고집함 차라리 AGI가 동료가 될 수준이 오면 다시 관심 가지겠음 AI라는 명칭 자체도 향후에는 그리 특별하게 여겨지지 않을 가능성이 높음
          + AI 코딩의 학습 곡선이 생각보다 크다고 해도, 몇 년을 투자해야 하는 피아노와는 다름 현존하는 가장 숙련된 AI 코더들도 겨우 3년 정도의 경험이고, 모델도 계속 바껴서 과거 경험이 현 세대 모델에 안 맞는 부분이 많음 마스터로부터 배우는 구조가 없는 점도 한계임
          + AI 코딩 스킬이 과연 장기적으로 가치 있는가? 현재 AI 도구들의 스킬셋이 미래 세대에 얼마나 이전될지 회의적임 당장 효율이 올라도 모델, 도구가 바뀌면 무용지물 될 가능성을 염두해야 함
          + AI 코딩 숙련은 몇 분 아니면 한 시간이면 충분하다고 생각함 비유적으로 GDB나 UNIX처럼 한 권씩 파고 드는 영역이 아니라고 느낌 그림과 사진도 근본 원리나 목표가 전혀 달라 혼동하지 않듯, AI 코딩도 기존 코딩과는 완전히 다른 활동임
          + 직접 코딩을 선호하는 것이 단지 AI 코딩 실력이 부족해서라고 보는 자신감에는 동의 못함 지금까지 소액 결제와 프리 트라이얼을 써본 ROI만으로도 충분히 판단 가능함
     * 실제로 AI 코드리뷰나 결과 검수 대신, AI가 생성한 코드를 그대로 PR에 올리고 리뷰를 남에게 미루는 문화가 생김 이런 상황에서는 GenAI가 정말 유용하다기보다, 일을 미루는 데 활용되는 부작용이 큼
          + 이런 경험을 나도 했음 관리자가 역량이 떨어지면, 누가 실질적 가치를 만드는지 구분하지 못함 나는 코딩 에이전트에서 진정 많은 가치를 얻고 있지만, 역량 없는 조직이 엉성한 산출물을 보상하는 구조가 심각한 문제라고 생각함
          + 제출자 PR이 계속해서 AI 결과물 그대로면, 피드백을 누적해 팀장이 반드시 문제 제기를 해야 맞다는 입장임
     * Claude Code를 자주 쓰는 입장에서, 남이 작성한 코드를 검토할 때 직접 작성하는 것과 시간 차이가 거의 없다는 주장에 100% 동의함 LLM은 나름 쓸만하지만, 통제권을 넘길수록 실제 릴리스까지는 오히려 더 오래 걸림 RSI 증상 완화에는 좋았지만, 시간 절약 효과는 생각보다 과장되는 경우가 많았음
          + 코드를 꼭 리뷰해야 하느냐는 질문을 받고, 보통 나는 주로 AI 결과물을 스팟 리뷰만 하고, 스펙 문서를 AI에 작성하게 한 뒤, 내가 최종 검토와 테스트를 꼼꼼히 진행함 사실 대다수의 오픈소스 라이브러리 코드는 처음부터 직접 리뷰하지도 않음
          + 여기엔 ""내가 직접 작성한 코드는 별도의 시각에서 검토할 필요가 없다""는 전제가 존재함 실제로는 미래의 나도 잠재적 코드 독자임 AI 코딩은 이런 마인드셋을 강제함, 즉 명확한 수용 기준을 세우고, 테스트와 일관적 규칙으로 검증해가는 구조임 결과적으로 더욱 견고하고 기록이 남는 개발 문화를 유도함
          + Claude Code로 버그 디버깅은 '테스트부터 작성*하면 AI가 몇 분 만에 고치는 게 일상임 새 검색 기능 도입 후엔 복잡한 작업도 5분 만에 처리 가능, 이 부분에서의 시간 절약은 체감상 매우 분명함
          + RSI 증상 해결법으로 항상 팔을 따뜻하게 유지하는 방법도 추천함
          + ""모든 것을 대화형 인터페이스로 처리하고 싶지 않은데, 하이브리드 방식 UI 도입 사례가 있냐""라는 궁금점 제기
     * Generative AI 코딩 도구는 업무의 쉬운 부분만 빨라지고, 오히려 어려운 부분은 더 어렵게 만듦 실제로 코딩 그 자체에 걸리는 시간이 아주 크지 않아서, 그 부분만 100배 빨라져도 전체 생산성이 크게 달라지지 않음 그래서 굳이 거기 시간 쓰고 싶지 않음
          + 이미 특정 스택과 문제 영역을 꿰뚫는 엔지니어라면 굳이 어떤 도구도 필요치 않음, 학습도 필요없음 하지만 이 논리는 현실적으로 의미 없음 결국 도구 선택은 개인 최적화임
          + 나는 AI에게 알고리즘 작성, 코드 원인 설명, API 호출, 특정 함수 구현 등을 시킴 완전한 코드 전체는 아니어도, 점점 디버거와 함께 활용하는 일이 늘어남
          + 혹시 배관공인가?라는 농담 섞인 질문
     * 글쓴이가 ""코드를 직접 작성하는 것 만큼이나 남이 쓴 코드 리뷰가 시간이 들거나 더 많은 경우도 있다""는 대목은, 시큐리티 감사처럼 정교한 코드 리뷰 경험이 있던 나도 공감함 하지만 AI가 아주 단순한 루틴 작업에 특화된다면, 코드를 구체적으로 들여다 볼 필요 없이 eBPF 베리파이어처럼 포괄적 검수만으로 충분할 수도 있음 너무 복잡한 이슈가 있으면 그냥 PR을 거절함 단순 반복적 코드에는 사람이 꼼꼼히 감수할 필요도 덜함
          + 하지만 이게 정말 효율적 방법인지 의문임 수십 PR을 열었다가 하나만 통과시킬 거면 차라리 동료 코드가 훨씬 믿음직함 시간과 금전, 환경 자원 낭비만 초래하는 구조는 바람직하지 않다고 봄
          + 정말 '반복적 Go 함수'라면, 그 코드를 굳이 작성할 가치가 있었는지 의문임 비효율적이고 재사용성 적은 코드, 그저 흔한 라이브러리 한 두 줄이면 끝날 일을 굳이 AI로 만들 이유를 못 느끼겠음
          + 코드를 읽는 속도가 쓰는 속도보다 빠른 사람에겐 GenAI가 유용하고, 반대로 쓰는 것이 더 빠른 사람은 별로 쓸 필요가 없음
          + 왜 AI가 작성한 코드를 인간이 쓴 것과 다르게 검수해야 하는가에 대한 궁금증 제기
          + 반복적이고 단순한 작업에는 IDE 템플릿 바인딩과 변수 자동 완성 기능으로도 충분히 커버 가능하며 이 방식은 무료라는 점을 강조함
     * 인턴과 AI 코딩 보조 도구 비교 논의에서, 인턴은 실제 코드, 비즈니스, 시스템 히스토리를 배우는데 AI는 반복적으로 맥락을 설명해줘야 함 이런 한계가 여전함
          + 중요한 맥락은 mdc 파일에 저장하면, 다음 담당자도 그 정보를 참고하게 되어 오히려 문서화·지식 지속성이 좋아진다는 시각
          + 어떤 LLM들은 이런 식으로 시스템 프롬프트가 14k씩 길어지는 원인이 됨
          + 인턴은 진짜로 CARE(신경씀)하는 경우가 많음
          + 중요한 비즈니스 맥락을 그냥 시스템 프롬프트에 넣는 방식도 가능함
     * 현재 AI 모델은 본질적으로 기존 데이터 패턴만 학습함 성공 사례 템플릿을 조합하는 것이지, 근본부터 새로운 해법을 유도하는 구조가 아님 문제를 보면 가장 비슷한 경험에서 답을 찾으려 하지, 처음부터 원칙적으로 접근하지 않음 인간 전문가는 First-principles(AI가 어려워하는 근본 원리로부터의 논리적 접근)에 능숙함 AI는 유사성을 우선시해 틀에 박힌 해결책을 내놓고, 특히 혁신이 필요한 영역이나 관습이 통하지 않는 문제엔 한계가 두드러짐 문맥 오염(context poisoning)도 인간이 훨씬 효율적으로 대응함
          + 사실 인간도 기존 데이터에서 배운 패턴을 확장하는 존재임, '새로운 방법론' 자체도 성공 경험에 기반해 발전한 것임 AI 또한 이런 점에선 인간 학습과 큰 차이가 없다고 생각함
     * 나는 AI가 생성 코드 영역에서 별로 기대하지 않는 입장이지만, 반복적이고 지루한 보일러플레이트 작업은 AI가 N배(체감상 매우 큰 수치) 빠름 실제 경험담을 들자면, ChatGPT에 React Context 기반 모달 구성 예시를 던졌더니 후크, 프로바이더 등 일련의 구조를 바로 뽑아줌 당장 30분 정도 절약함, 이런 반복적 작업에는 월 20달러가 훨씬 아깝지 않음
          + 이런 경우 라이브러리 문서 예제에서 들고와 쓸 때도 많고, 5분이면 직접 가장 꼭 필요한 기본 구현을 끝낼 수 있음 하지만 생성 코드는 주로 현 상황에 맞춘 전체 해법을 내놓기에, 이후 점진적 구조 개선·리팩터링에는 오히려 불편이 많음
          + 나 역시 주로 보일러플레이트나 ad-hoc 스크립트에 AI를 씀 복잡한 실제 문제는 아직 AI에 전적으로 맡기긴 어렵다고 봄 특히 시스템에 깊이 파고드는 문제는 사람이 직접하며, 과정에서 얻는 통찰이 중요함 AI는 프로젝트가 클수록, 융합된 요소가 많을수록 한계가 커진다고 느낌
"
"https://news.hada.io/topic?id=21432","Show GN: AI 다국어 더빙 STRA AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Show GN: AI 다국어 더빙 STRA AI

   긱뉴스 매일같이 보는 사용자 중 1명입니다. 처음으로 저희 회사에서
   만드는 제품 올려 보네요 :)

   스트라 AI는 영상의 언어를 32개 다국어로 만들어주는 솔루션입니다.
   원본 화자의 목소리 특징은 그대로 두고 언어만 바꿔주는 거라서
   감정 표현이나 톤이 그대로 유지되는 것이 특징입니다.

   이런 곳에 사용하실 수 있습니다.
     * 해외 시청자 확보를 원하는 유튜버
     * 글로벌 수출을 원하는 콘텐츠 기업
     * 기업/제품/서비스 홍보 영상 다국어화
     * 강의/교육 영상 다국어화
       등등

   AI 더빙을 돌리면 아래와 같은 프로세스가 돌아갑니다.
    1. 영상에서 음성 추출
    2. 음성에서 대본 전사(Transcription) - 발화 타임코드 추출
    3. 대본 번역
    4. 번역된 대본으로 AI 음성 합성
    5. 더빙 영상으로 자동 편집

   이 모든 과정이 자동화되어 있어서 영상 올려놓고 기다리시면
   알아서 더빙 영상이 생성이 됩니다.

   GPU를 많이 돌리는 거라서
   30분 짜리 영상 올리시면 대략 10~15분 정도 걸립니다.

   가입하시면 무료 크레딧을 드리니 간단한 영상부터 테스트해 보시기를 바랍니다. 그럼 써보시고 댓글로 의견 부탁드립니다.
"
"https://news.hada.io/topic?id=21446","TDD, AI Agents and Coding - 켄트백","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    TDD, AI Agents and Coding - 켄트백

  Kent Beck 소개

     * 익스트림 프로그래밍(XP) 창시자
     * 애자일 선언문 공동 저자 (알파벳 순 첫 번째 서명자)
     * TDD(테스트 주도 개발) 선구자
     * 52년 코딩 경력의 업계 전설
     * 현재 ""Tidy Together"" (소프트웨어 디자인과 팀워크) 집필 중
     * 하루 6-10시간 이상 AI 도구로 코딩하며 ""가장 즐거운 프로그래밍 시기"" 보내고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  1. AI 코딩 도구와 '지니' 비유

    핵심 개념: 예측 불가능한 지니

     * AI 에이전트를 ""예측 불가능한 지니"" 에 비유
     * ""내가 의미하는 바를 정확히 해주지 않는다""
     * ""그들만의 의제가 있다""

    AI와의 작업 경험

   긍정적 측면:
     * 때로는 놀라운 마법처럼 큰 디자인 문제 해결
     * 예상하지 못한 유용한 기능들 구현 (예: B+ 트리 스트레스 테스터)

   부정적 측면:
     * 사용자 의도를 잘못 해석
     * 테스트를 삭제하거나 변경하려 시도
     * 룩업 테이블로 문제를 ""해결""하려는 장난스러운 행동

    중독성 있는 경험

     * 슬롯머신과 같은 간헐적 보상
     * ""밤에 컴퓨터를 지나가며 '한 번 더 프롬프트를 해볼까?' 생각""
     * ""한 시간짜리 프롬프트를 시작하고 외출""
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  2. AI 시대의 기술 변화

    스킬 가치의 재편

   ""90%의 기술이 가치를 잃고, 10%의 기술이 1000배 증가""

   가치 상승한 스킬:
     * 비전 설정
     * 이정표 관리
     * 복잡성 제어

   가치 하락한 스킬:
     * 언어별 세부 사항 (Rust의 &, *, [] 위치 등)

    프로그래밍 언어에 대한 관점 변화

   과거: Smalltalk에 깊은 감정적 애착
   현재:
     * ""마음이 너무 많이 상해서"" 언어 애착 사라짐
     * ""Java 개발자"", ""Clojure 개발자"" 정체성 구분에 피로감
     * ""삼투압 학습"": AI 덕분에 언어 세부사항 몰라도 프로그래밍 가능

   최근 시도한 언어들: Swift, Go, Rust, Haskell, C++, JavaScript

    현재 야심찬 프로젝트: Server Smalltalk

     * 영속성(persistent): 데이터베이스처럼 동작
     * 트랜잭셔널: commit과 abort 가능
     * 병렬성: 멀티스레드 및 머신 간 병렬 처리
     * 대용량 데이터 처리
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  3. 애자일 선언문의 역사

    탄생 배경 (2001년)

     * 폭포수 모델의 대안 모색
     * 수년간의 소프트웨어 방법론 워크샵 결과
     * 노르웨이 크루즈 준비 회의 → 유타 최종 회의

    Kent Beck의 기여

     * ""daily"" 단어 추가 (12가지 원칙 중 ""매일 상호작용"")
     * 알파벳 순서상 첫 번째 서명자

    ""애자일""이라는 용어에 대한 불만

   문제점:
     * ""너무 매력적"" 이어서 모든 조직이 남용할 것 예상
     * 실제로는 원칙을 따르지 않으면서도 애자일이라고 주장하는 조직들 등장

   선호했던 대안: ""conversational(대화형)""
     * 단방향이 아닌 양방향 소통 강조
     * 매력 부족으로 채택되지 못함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  4. 익스트림 프로그래밍(XP)

    창시 배경

   초기 컨설팅 경험:
     * 처음엔 기술 컨설턴트 (성능 튜닝, 비트 조작)
     * 프로젝트 관리 조언 요청 증가
     * 환경의 중요성 발견: 좌석 배치만 바꿔도 극적 개선

   크라이슬러 프로젝트:
     * 실패하던 프로젝트를 성공으로 전환
     * 효과적인 모든 관행을 ""최고 수준(11까지)"" 으로 끌어올림

    XP의 핵심 원리

   4가지 활동을 시간을 잘게 쪼개어 동시/연속 수행:
    1. 무엇을 할 것인가 파악
    2. 어떤 구조로 할 것인가 설계
    3. 기능 구현
    4. 예상대로 작동하는지 확인

    페어 프로그래밍

     * 의무가 아닌 실험적 접근
     * 초기 팀 경험: 개발 후 발견된 모든 버그가 단독 작업 코드에서 발생
     * ""페어 프로그래밍 시 프로덕션 결함 제로""

    네이밍의 전략

     * ""익스트림"" 선택 이유: 경쟁자(Grady Booch)가 사용하지 않을 도발적 이름
     * 익스트림 스포츠 유행 시기와 맞아떨어짐
     * ""극한 운동선수는 최고로 준비되어 있거나 죽는다"" - 좋은 은유

    성공 요인

     * 닷컴 버블 시기 18개월 폭포수 방식에 절망한 개발자들에게 어필
     * 더 빠르고 예측 가능한 결과 약속
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  5. 테스트 주도 개발(TDD)

    기원과 영감 (1970년대)

   테이프-투-테이프 시스템:
     * 아버지의 프로그래밍 책에서 접한 개념
     * 실제 입력 → 예상 출력 수동 타이핑 → 코드 작성 → 실제 출력과 비교
     * 8-12세 때 읽었지만 이해하지 못함

   S-Unit 개발:
     * 클라이언트 테스트 작성 도움을 위해 개발
     * ""코드 작성 전 예상값 입력"" 이라는 ""터무니없는"" 아이디어 시도

    첫 TDD 경험: 스택 구현

   불안한 성격:
     * ""불안한 사람이다. 걱정이 많다""
     * ""프로그래밍은 끊임없는 불안의 원천"" (뭘 잊었지? 뭘 망가뜨렸지?)

   TDD 적용 결과:
     * ""불안감이 완전히 사라짐""
     * ""이것이 작동한다고 확신한다""
     * 프로그래밍의 감정적 경험 변화

    TDD의 가치

   기술적 이점:
     * 결함 밀도 감소
     * API 선택에 대한 빠른 피드백
     * 구현 디자인 진화 가능성

   감정적 이점:
     * ""기술 불안증 치료제 비용만으로도 충분히 가치""

    설계에 대한 반박

   John Osterhout의 비판: ""TDD는 설계에 도움이 안 되고 세부사항에만 집중""

   Kent Beck의 반박:
     * ""선택의 문제""
     * 실무자들은 추상화 수준을 계속 오가며 설계 결정
     * Red-Green 사이클의 ""긴장 완화"" 순간이 더 큰 설계 사고의 자유 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  6. AI 에이전트와 TDD

    실무 적용

   항상 테스트 먼저 작성:
     * AI가 놓친 부분을 테스트로 전달
     * AI의 테스트 변경/삭제 시도 차단

   필요한 개선사항:
     * ""불변 주석(immutable annotation)"" 필요
     * ""이건 맞다. 바꾸면 어둠 속에서 영원히 깨어날 것""

    AI의 한계

     * 결합도 줄이기/응집도 높이기 능력 부족
     * 디자인 감각 부족
     * 원거리 파급효과를 일으키는 결정 경향

    대응 전략

     * 300밀리초 실행되는 대규모 테스트 스위트
     * AI의 의도치 않은 코드 파손 실시간 감지
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  7. 페이스북 경험 (2011-2017)

    2011년 합류 시 충격

   TDD 수업 참가자 제로:
     * 고급 엑셀 기술: 만석 + 대기자
     * 아르헨티나 탱고: 만석 + 대기자
     * TDD: 참가자 없음

   대응 전략:
     * ""소프트웨어 공학 지식을 모두 잊기""
     * ""원숭이 보고 따라하기"" 결정

    페이스북의 독특한 개발 환경

   강력한 책임감:
     * 프로그래머가 야간 호출 대상
     * ""자신의 실수에 대한 고통을 직접 느낌""

   다층 피드백 루프:
     * 빠른 개발 서버 (블루→그린 변경 후 즉시 확인)
     * 코드 리뷰
     * 내부 배포 (직원들이 개인/업무용으로 사용)
     * 점진적 롤아웃 (일일/주간)
     * 뛰어난 관찰 가능성

   조직 문화:
     * ""페이스북에서는 그 무엇도 다른 사람의 문제가 아니다""
     * 할머니가 손자 괴롭힘 문제로 와도 ""그것도 당신 문제""

    TDD가 맞지 않은 이유

   실제 문제 영역:
     * 설정 문제
     * 서브시스템 간 관계
     * 테스트로 잡기 어려운 것들

   페이스북만의 장점:
     * 수백만 사용자 기반 라이브 대규모 테스트
     * 뛰어난 관찰 가능성
     * 기능 플래그(Feature Flag)
     * 일반 회사에서는 구현 어려운 환경

   구체적 사례:
     * 관계 상태 기능 구현 (싱글/결혼에 시민결합/동거 추가)
     * TDD 사용했지만 ""시간 낭비""
     * 알림 코드의 암묵적 결합으로 문제 → 타인이 핫픽스

    시기별 변화

   2011년 (2,000명):
     * 가능성, 규모, 소유의식 최고
     * 전역 최적화 사고의 중간 관리자들
     * ""누가 정말 도움이 필요한지"" 생각하는 협력

   2017년 (15,000명):
     * 정치, 제로섬 사고, 미시 최적화
     * 큰 그림 그리기 어려워짐
     * 부서 간 이해관계 충돌 (예: 긴 글 vs 좋아요/댓글 최적화팀)

    규모의 경험

     * Messenger API: 주당 1경(quadrillion) 호출
     * 실험 그룹이 ""뉴질랜드"" 규모 (백만 명)
     * 1경 = 백만 × 십억
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  8. AI 시대 소프트웨어 개발의 미래

    패러다임 변화

   비용 구조의 완전한 재편:
     * ""저렴하고 비싼 것의 경계가 완전히 바뀜""
     * 이전에 비싸다고 여겨진 것들이 ""터무니없이 저렴"" 해짐

    조직의 적응 과제

   더 많은 코드 버리기:
     * 10배 많은 아티팩트 생성 가능
     * 여전히 하나만 가치 있음
     * ""완료된 실험을 버리는 것"" 에 대한 보상 체계 필요

   실험의 양적 증가:
     * 탐색된 아이디어의 양이 경쟁 우위
     * ""모든 것을 실험해보아야"" 하는 시대

    개인적 변화

     * 코딩이 다시 즐거워짐
     * 더 큰 야망 가질 수 있게 됨
     * ""엄청나게 야심찬 생각"" 실현 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  9. 빠른 Q&A

    개인 선호

     * 가장 좋아하는 언어: Smalltalk
     * 두 번째 언어: JavaScript (Smalltalk와 유사)
     * 현재 AI 도구: Claude (Cursor, Augment 내부 엔진)
     * 추천 도서: ""The Timeless Way of Building"" by Christopher Alexander
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  핵심 통찰

    1. 컨텍스트의 절대적 중요성

     * 같은 방법론도 환경에 따라 효과가 완전히 다름
     * 페이스북의 대규모 환경 vs 은행의 소규모 거래 환경

    2. 감정과 기술의 관계

     * TDD의 진정한 가치는 ""불안감 해소""
     * 기술적 논리보다 감정적 변화가 더 중요

    3. AI 시대의 새로운 사고방식

     * 비전과 설계 능력이 핵심 스킬로 부상
     * 언어 세부사항은 더 이상 중요하지 않음
     * 실험의 양적 증가가 경쟁 우위

    4. 조직 문화의 힘

     * ""아무것도 남의 문제가 아니다"" 라는 소유의식
     * 전역 최적화 vs 개인 최적화의 차이
     * 인센티브 정렬의 중요성

   페이스북의 독특한 개발 환경
   강력한 책임감:
   프로그래머가 야간 호출 대상
   ""자신의 실수에 대한 고통을 직접 느낌""

   k-개발 환경과 다르지 않은건가...?

   아직 계시네요.

   예전에 TDD를 기계 회사에서 세미나 했었는데, 다들 뭐지? 하고 쳐다보던 눈빛이 잊혀지지 않습니다.

   TDD가 괜찮다고 생각하지만, 생각보다 잘 안 되어서...
   통합테스트를 TDD처럼 하고 있습니다. 물론 이것은 TDD가 아님니다. ^^

   아직도 TDD 맹신론자 vs 무용론자들이 싸우는데,
   다시 현재 업계 상황에 맞춰서 TDD에 v2판을 내주었으면 합니다.
   요즘 small한건 AI가 쉽게 해서 대량의 context가 필요한 경우에 어떻게 활용할지라든지..

   좋은 글이네요
"
"https://news.hada.io/topic?id=21544","Tattoy - 터미널에 GPU 셰이더와 라이브 미니맵을 추가하는 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tattoy - 터미널에 GPU 셰이더와 라이브 미니맵을 추가하는 프레임워크

     * 터미널에 ""Eye-Candy""를 추가해주는 프레임워크로, 텍스트 기반이면서도 화려한 시각 효과와 그래픽 렌더링을 제공
          + 배경에 비디오 재생, 커맨드를 배경에서 투명하게 실행, 픽셀 미니맵등
     * 기존의 쉘, 테마, 프롬프트 등과 자연스럽게 연동되고, 효과와 평상시 터미널 상태를 즉시 전환할 수 있음
     * 터미널 환경에 GPU 기반 셰이더를 추가하여 화려한 그래픽과 라이브 미니맵을 제공함
          + Shader Toy 의 대부분 셰이더를 별도 수정 없이 그대로 사용할 수 있고, Ghostty용 셰이더도 지원
     * Second Terminal In Background : 별도의 커맨드를 터미널 배경에 투명하게 실행할 수 있음(투명도 조절 가능)
          + 예시: 오디오 비주얼라이저, 비디오 배경, 시스템 모니터 그래프 등
     * 터미널 스크롤백 전체를 실시간 픽셀 미니맵으로 표시하며, 자체 스크롤백 버퍼와 스크롤바 지원
          + nvim, top, gitui 등 alternate screen을 쓰는 앱도 모두 반영
     * UTF8 하프블록 문자(▀,▄)와 True Color 지원으로, 대부분의 터미널 에뮬레이터에서 동작함
     * Auto Text Contrast 지원 : 디렉토리 내 파일명을 ls 했을 때 특정 타입이 안 보이는 등, 색상 대비 부족 문제를 자동 해결
          + Tattoy는 모든 색상의 24비트 RGBA 값을 인식, 알고리듬으로 저대비 텍스트만 컬러를 자동 보정함
     * 플러그인 지원
          + 플러그인은 언어 무관하게 작성 가능, 터미널 내용 전체 접근 및 UTF8 픽셀/텍스트 렌더링 지원
          + JSON over STDIN/STDOUT 프로토콜로 동작, 커서에서 연기 파티클이 나오는 등 커스텀 이펙트 구현 예시 제공
          + 플러그인 문서와 예제 코드 참고
     * 동작화면은 https://tattoy.sh/ 확인 가능
"
"https://news.hada.io/topic?id=21494","공식 Android API에 숨겨진 농담과 유머들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      공식 Android API에 숨겨진 농담과 유머들

     * Android 공개 API에는 유머러스한 메소드와 상수들이 숨겨져 있음
     * 대표적으로 isUserAMonkey() 와 isUserAGoat() 등의 메서드는 실제 용도와 함께 농담 요소를 포함함
     * DISALLOW_FUN, Log.wtf() 등은 공식 API에 농담성 설명이나 재미있는 행동을 내포함
     * 기발한 명명법이나 패러디(예: TWEET_TRANSACTION, GRAVITY_DEATH_STAR_I)가 공식 문서나 코드 내에 자주 등장함
     * 이런 요소들은 개발자들에게 즐거움과 소소한 발견을 선사하는 역할을 수행함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Android 공개 API 속의 농담과 유머 요소

   Android 오픈 API에는 개발자를 위한 다양한 재치 있는 농담과 이스터 에그 요소가 실제 기능과 함께 포함됨

  ActivityManager.isUserAMonkey()

     * 개발자 도구인 UI Exerciser Monkey가 동작 중인 경우를 판별하는 메소드임
     * UI에 무작위 입력을 시뮬레이션하여 앱을 스트레스 테스트하는 환경에서 특정 행동을 제한할 수 있도록 고안됨
     * 실제 사용자 대신 monkey가 앱을 사용 중일 때, 중요 기능(예: 비상전화 발신 등)을 제한하는 목적으로 도입됨
     * Android 2.2(Froyo, API 8)에서 공개 API로 추가됨
     * Android 개발 초기에 실제로 우발적 문제를 해결하기 위해 구현된 배경을 가짐

  UserManager.isUserAGoat()

     * 공식 문서에는 사용자가 순간이동 대상인지 확인하는 용도로 안내되어 있으나 이는 농담임
     * 초기에는 항상 false를 반환했으나, Goat Simulator가 출시된 뒤 해당 게임이 설치된 경우 true를 반환하도록 변경됨
     * Android 11(API 30) 이상에서는 개인정보 보호 목적으로 항상 false로 고정됨
     * Goat Simulator 앱 탐지 로직이 있었으나, 앱 목록 접근 정책 변화로 정보 누출 방지를 위해 수정됨
     * Google Chrome Task Manager의 이스터 에그에서 영감을 받았음

  UserManager.DISALLOW_FUN

     * Android 6 Marshmallow(API 23)에서 추가된 재미 제한 정책을 의미함
     * 공식 문서에서는 사용자에게 기쁨이나 즐거움을 느끼지 못하도록 제한할 수 있다고 표현함
     * 실제로는 기기 소유자가 사용자 기능 일부를 막는 정책으로 사용됨
     * 일부 앱이나 시스템 자체에서 이 정책을 감지해 이스터에그 등 “재미있는” 기능을 비활성화할 수 있음
     * 학습 기관 등에서 주의 분산 기능을 비활성화할 목적으로 활용될 수 있음

  Chronometer.isTheFinalCountdown()

     * Android 8 Oreo(API 26)에서 isTheFinalCountdown 메소드가 추가됨
     * 호출 시 자동으로 Europe의 “The Final Countdown” YouTube 영상을 실행함
     * 직접적인 실용성 없이 명백한 농담/참조임
     * 코드 내 Intent 명령문으로 명확히 드러남
     * 개발자에게 작은 유희를 제공함

  PackageManager.FEATURE_TOUCHSCREEN_MULTITOUCH_JAZZHAND

     * Android 2.3 Gingerbread(API 8)에서 추가된 멀티터치 대응 여부 상수임
     * Jazz hands(음악 퍼포먼스 제스처)를 패러디한 명칭임
     * 최대 다섯 손가락 동시 인식 기능을 의미함

  Log.wtf()

     * What a Terrible Failure의 약어로, 절대 발생하지 않아야 할 상황 로그 기록용임
     * Assertion 수준의 심각한 로그로 분류됨
     * 개발자 사이에 가장 잘 알려진 농담적 API 중 하나임

  AdapterViewFlipper.fyiWillBeAdvancedByHostKThx()

     * Android 3.0 Honeycomb(API 11)에서 추가
     * 비공식적이고 유쾌한 네이밍으로, 뷰가 호스트에 의해 진행될 때 호출됨
     * 실제 명명도중의 코믹한 발상에서 비롯된 것으로 추정됨
     * 컴퓨터 과학의 고질적인 문제인 네이밍의 난해함을 재치있게 표현함

  IBinder.TWEET_TRANSACTION

     * Android 3.2 Honeycomb(API 13)에서 도입된 트랜잭션 타입임
     * 트윗 전송을 암시하나, 실제로는 아무런 동작도 하지 않고 메시지 길이 제한(130자)이 구 Twitter 정책을 참조함
     * 이름 자체가 완전히 패러디임

  IBinder.LIKE_TRANSACTION

     * Android 4.0.3 ICS(API 15)에서 추가된 LIKE 트랜잭션임
     * 호출자가 앱을 좋아한다는 신호 전송용으로, 실제 기록이나 카운트는 없음
     * 앱의 자존감을 올려준다는 농담적 설명이 존재함

  SensorManager.SENSOR_TRICORDER

     * Star Trek의 Tricorder에서 이름을 따온 상수임
     * Android 1.0 시절부터 있었으나 이후 deprecated 처리됨
     * Tricorder는 실제 존재하지 않는 SF기기로, 개발자에게 웃음을 줌

  SensorManager.GRAVITY_*

     * SensorManager 내에는 여러 천체(태양, 명왕성 등)의 중력값을 담은 상수들이 존재함
     * 그중 GRAVITY_DEATH_STAR_I(스타워즈의 데스스타), GRAVITY_THE_ISLAND(드라마 LOST의 섬) 등은 농담임
     * 일부는 실제 활용도보다는 패러디적 의미가 강함

  <blink> 태그

     * Android 뷰 레이아웃 시스템에 <blink>라는 숨겨진 태그가 존재함
     * 자식 뷰를 HTML의 오래된 <blink>처럼 깜빡이게 만듦
     * 공식 문서엔 설명이 없으나, 2011년 커밋에서 추가되어 현재도 AOSP에 남아 있음
     * 실제 사용 여부는 논란의 대상임

결론

   Android 공식 API 곳곳에는 농담, 패러디, 이스터에그성 요소가 의도적으로 삽입되어 있음
   이런 요소들은 개발자에게 소소한 재미와 발견의 즐거움을 제공하며, Android의 커뮤니티 문화와 유쾌함을 엿볼 수 있는 예시임

        Hacker News 의견

     * FB에서 여러 해 동안, 위험하거나 이미 폐기된 토큰에는 _DO_NOT_USE_OR_YOU_WILL_BE_FIRED 같은 접미사를 붙이는 관습이 있었음. 모두가 이 농담을 알았지만, 팬데믹 이후 신규 인력 비중이 높아지고 원격 온보딩이 부족했던 시기에는 이러한 내부 농담이 두려움이나 불확실함을 유발한다는 불만도 생겼음. 이 말은 내가 경험한 범위 내에서만 해당하는 이야기임
          + React의 __SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED 같은 유쾌하면서 지나치게 긴 이름을 예전부터 재미있게 봤던 기억이 있음. 하지만 아쉽게도 최근에는 이런 '유우머'가 점점 사라지고 있음 관련 PR
          + 예전에 Google에서, 네트워크 스택의 어딘가에 이해하기 어렵고 수정도 힘든 방대한 코드 덩어리가 있었는데, 이게 점점 커져만 갔음. 우리 팀은 그 코드를 ""[Foo]Sorcery""로 이름을 바꿈. 어딘가 무섭고 이상한 이름 덕분에 더 이상 아무도 그 코드에 새로 추가하려 하지 않았고, 가끔씩 누군가는 일부를 정리해서 삭제함. 재미있는 네이밍이 주효했다고 생각함
     * Android 소스에서 내가 제일 좋아하는 재치 있는 함수는 android.os.Handler.runWithScissors()인데, 아쉽게도 public API에는 포함되어 있지 않음 소스 링크
          + 이 함수의 주석에 “@hide 이 메서드는 오남용 소지가 있어 API에 넣지 않는 것이 좋으며, 추가할 경우 runUnsafe()처럼 덜 재미있는 이름으로 바꿔야 할지도 모른다”는 문구가 있어서 웃음이 남
          + 이 함수가 본문에 없어서 살짝 아쉬움. 내가 제일 좋아하는 함수 중 하나임
     * X11에 대한 즉각 떠오르는 예시는 ""party_like_its_1989""라는 전역 변수가 있고 코드 링크, DRI2 확장 changelog에는 ""Awesomeness!"", ""True excellence"", ""Enlightenment attained"" 같은 표현이 가득함 changelog 링크
     * BeOS(그리고 Haiku)에서는 ""is_computer_on()""과 ""is_computer_on_fire()""라는 함수가 있는데, 설명이 굉장히 훌륭함 참고 링크
          + Delphi에서는 'EProgrammerNotFound'라는 예외가 있어, 공식 문서도 굉장히 무미건조하면서도 진지한 유머를 담고 있음 문서 링크
          + 물론 이런 네이밍이 재미를 주려고 만든 건 알겠지만, 고급 절전 관리가 적용된 현대 컴퓨터에서는 이런 상태가 꽤 흔함. 운영체제는 깊은 절전모드에서의 깨우기라든지 메인보드 온도가 200도인 상황도 당연하게 처리해야 함
          + ""is_computer_on_fire()""는 비동기 이벤트 기반 트리거로 활용한다면 더 재밌었을 거라고 봄
          + 궁금한 사람을 위해 관련 소스 링크도 있음 코드 링크
     * 이런 유머 감각이 대기업 코드베이스에 잘 살아 있다는 점이 좋음. API만 쓸 때는 모르지만, 소스코드를 직접 읽으면 가끔 만나는 이런 이스터 에그가 실제로 사람이 코드를 썼다는 걸 확실히 느끼게 해줌. 요즘 소프트웨어는 너무 차가운데, 오히려 이런 대조감에서 따뜻함을 느낌. 솔직히 이런 부분이 더 많았으면 함
          + 이와 비슷하게, 내가 사용하는 코드나 API 곳곳에 실제 사람이 존재했다는 사실을 환기시켜 준다는 점이 인상적임
     * Android에서 isUserAGoat, isUserAMonkey 같은 것들을 직접 테스트해보고 싶다면, 내가 이런 기능을 제공하는 작은 앱을 만들어서 올려놨음 앱 링크. 나중에 더 많은 이스터에그도 추가해보고 싶음
          + DISALLOW_FUN 정책 체크 같은 기능도 넣으면 꽤 재미있을 것 같음. 관련 코드를 찾으려고 GitHub에서 검색해봤는데, 대개는 시스템 Settings 앱에서 버전 이스터에그를 숨기는 용도 뿐임. 아마 내가 최초로 이 기능을 실사용하는 써드파티가 될 수도 있겠음
          + 근데 Android 15에서는 앱 설치가 안 됨. Google에서 minimum compileSdk를 반드시 맞추라고 해서 앱을 업데이트해주면 좋겠음
     * Chrome 작업 관리자에는 한때 ""브라우저 프로세스가 텔레포트한 염소 개수""를 보여주는 숨겨진 컬럼이 있었음. 지금은 그 기능이 사라졌는데, 옛날 스크린샷에서 확인 가능함. 예전엔 234개나 컬럼이 있었는데 이제 16개로 줄어서 좀 아쉬움
     * 테크 제품에 이스터 에그가 넘치던 시절이 그리움. 모든 것이 회사화되면서 이런 문화가 사라진 느낌임
     * 이 스레드에 답글 달기를 면접에서 필수 질문으로 삼고 싶음. 나를 인터뷰할 때도 포함함. 정말로 “세상엔 두 부류의 사람이 있다”는 순간임
          + 왜? Googler가 API에 심는 농담은 크리스마스 저녁에 하는 아빠 개그 정도의 재미라고 생각함
     * 기술 문서에서 유머와 농담이 있다는 건데, 흔히 부정적으로 보는 시선도 있지만 나는 문서에 유머가 들어가는 걸 매우 좋아함. 관련 글에 쓴 적 있음
"
"https://news.hada.io/topic?id=21513","웹페이지 인쇄 최적화: 종이에서도 잘 보이게 만드는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    웹페이지 인쇄 최적화: 종이에서도 잘 보이게 만드는 방법

     * 웹 경험의 핵심인 반응형 디자인은 디바이스뿐 아니라 출력(프린트) 매체 대응까지 고려해야 함
     * 접근성·법적 요구·여행 등 다양한 이유로, 웹페이지의 인쇄 품질과 활용성은 여전히 중요함
     * CSS의 @media print, @page, 절대 단위, page-break 등 인쇄 전용 기능으로 깔끔한 레이아웃, 적절한 분할, 효율적 인쇄 구현 가능
     * 내비게이션, 푸터 등 불필요한 요소는 숨기고 링크, 약어 등은 종이에 맞게 정보 보완 필요
     * 흑백 인쇄·잉크 절감·가독성을 고려해 색상·배경 활용을 최소화하고, 인쇄와 스크린의 상호 보완적 개선 유도
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

인쇄를 위한 웹 디자인의 필요성

     * 접근성 측면에서, 장시간 화면을 볼 수 없는 사람도 인쇄된 콘텐츠로 정보를 얻을 수 있음
     * 여행 중 인터넷 미접속, 조직 내 법적·정책적 보관 의무 등, 인쇄물 활용 상황은 여전히 다양함
     * EPUB 등 디지털 출판 포맷과의 공통점도 많아, 프린트 스타일의 경험은 확장성 높은 기술임
     * 사람들이 실제로 웹사이트를 인쇄해 활용하므로, 프린트 대응은 전체 사용자 경험 품질 향상에 기여함

Print 스타일 적용법

     * CSS의 @media print를 활용하면 프린트 전용 스타일 지정 가능
     * <link rel=""stylesheet"" media=""print"">, @import url(""..."") print, 내부 CSS의 @media print { ... } 등 여러 방법 존재
     * 화면 전용은 @media screen 사용

Print 스타일 테스트

     * Edge, Chrome, Firefox, Safari 등 브라우저에서 프린트 미디어 시뮬레이션 기능 제공
     * 시뮬레이션 결과와 실제 인쇄물이 다를 수 있으며, 대부분의 브라우저는 배경 비활성화(잉크 절감) 를 기본값으로 설정함
     * 실제 인쇄물 환경을 고려한 테스트 권장

절대 단위와 @page 규칙

     * CSS는 cm, mm, in, pt, px 등 다양한 절대 단위를 제공, 실제 인쇄 시에는 정밀한 사이즈 조정에 유용함
     * @page 규칙으로 종이 크기(A4, A5 등), 여백, 방향 등 설정 가능
     * 프린터의 인쇄 영역 한계, 브라우저가 자동으로 추가하는 푸터/헤더 등도 고려 필요

페이지 분할 및 단락 관리

     * 긴 콘텐츠는 페이지 분할이 필수적이며, break-before, break-after, break-inside 속성으로 적절한 위치 제어
     * 예전 문법 page-break-*도 여전히 호환성 있음
     * orphans, widows 속성으로 단락 끝/시작에 홀로 남는 줄 최소화(단, 일부 브라우저 미지원)
     * box-decoration-break로 페이지 분할 시 경계선 등 UI 일관성 확보

상호작용 요소의 인쇄 대응

     * 종이에서는 링크·약어 등 상호작용 불가이므로, a[href]:after, abbr[title]:after 등으로 URL·정보를 출력에 추가
     * 폼 요소는 인쇄용 입력란으로 배치, 스크롤 컨테이너 등은 overflow: visible 등으로 확장 필요
     * 내비게이션, 푸터 등 불필요한 요소는 display: none 처리로 제거, main만 인쇄하도록 제어

컬러, 잉크, 가독성

     * 흑백 인쇄와 잉크 절감을 기본 전제로, 배경색 대신 테두리 사용 등으로 시각적 강조 대체
     * print-color-adjust: exact로 특정 요소만 색상 유지 강제 가능(필요한 경우에만 사용)
     * 이미지는 필요 최소화, 광고 등 잉크 소모가 큰 요소는 제거 권장

결론

     * 웹은 화면뿐 아니라 물리적 세계(종이) 에서도 존재함
     * CSS 프린트 스타일은 접근성과 사용자 경험 완성도를 높이는 핵심 요소
     * 모든 사용자가 다양한 방식으로 콘텐츠를 소비할 수 있도록, 프린트 대응은 꼭 신경 써야 하는 현대적 웹 개발의 필수 역량임

   한편, 해당 포스팅 자체는 인쇄 친화적이지 않습니다.
   COPY TO CLIPBOARD 같이 인쇄시에는 쓸모 없는 UI가 인쇄되고,
   코드 스니펫은 잘리고 횡스크롤바가 인쇄됩니다.

   이전에 올라온 종이에 인쇄하기 위한 CSS 정리 도 참고해보세요.

   웹 페이지를 종이에 인쇄하는 것과, PDF로 출력하는것은 또 다른데(잉크 걱정 없이 색을 쓴다던가, 하이퍼링크를 사용할 수 있다던가) 관련한 라이브러리는 paged.js 같은게 있습니다.

   메뉴얼, 교육자료, 학습지 페이지 제작에 고려해보면 좋습니다!
"
"https://news.hada.io/topic?id=21467","2025년 React와 커뮤니티의 현주소","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         2025년 React와 커뮤니티의 현주소

     * React는 여전히 가장 널리 쓰이는 UI 프레임워크지만, 최근 수년간 커뮤니티 내 혼란과 논쟁, 불신이 증가했으며, 공식 팀과 외부 개발자 간 커뮤니케이션 부재와 잘못된 마케팅이 혼합되어 우려가 증폭됨
     * React 19가 정식 출시되어 Server Components, 새로운 use 훅, 폼 통합 등 대규모 기능이 추가됐으나, ""프레임워크만 권장"" 정책과 Next.js 중심 구조가 많은 사용자의 불만을 낳음
     * ""React는 이제 Next.js에서만 제대로 쓸 수 있다"" ""곧 클라이언트 전용 지원이 중단된다"" 등 오해와 FUD가 퍼졌지만, 실제로는 클라이언트 렌더링 기능이 없어질 일은 절대 없고, RSC·서버 기능도 어디까지나 선택사항임
     * React 팀의 프레임워크 중심 정책은 성능/구조 표준화와 사용자 경험 개선 목적이지만, SPA와 다양한 아키텍처에 대한 존중 부족과 비공식적/불친절한 문서가 커뮤니티 혼란을 키움
     * 최근에야 Vite·Parcel 기반 SPA 등 다양한 방법이 공식 문서에 포함되었으나, 서버 컴포넌트(RSC)에 대한 공식 설명 부족과 소통 미흡이 여전히 남아 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 목적

     * 2025년 기준 React 커뮤니티는 성공·불신·논쟁이 뒤섞인 복잡한 국면
     * 이 글의 목표는 React의 발전 과정, 개발 방향, 주요 결정 배경을 정리하고 FUD와 혼란을 해소하는 것

필자 배경 및 커뮤니티 참여 이력

     * React 팀 멤버는 아니지만, 2015년부터 React 에코시스템에 깊이 관여
     * Redux 패밀리 라이브러리(특히 Redux Toolkit) 유지보수자 및 주요 커뮤니티 활동가
     * 수많은 React/Redux 튜토리얼과 블로그, 강연, 팟캐스트에 참여
     * Reactiflux Discord, /r/reactjs subreddit 등 주요 커뮤니티에서 운영진 및 중재자 역할
     * React 팀과의 협업 경험(예: useSyncExternalStore 훅 알파 테스터), 다양한 내부 피드백 그룹 참여
     * React DevTools, sourcemaps, Replay DevTools 등 다양한 기술적 기여
     * 편향 및 한계 고지
          + 본인은 항상 옳지 않을 수 있으며, 정보의 한계·오해·요약 과정에서 일부 부정확한 설명이 있을 수 있음을 인정
          + Redux를 유지보수하며, React 사용 경험 역시 주로 내부 도구 및 SPA 형태에 치우침
          + SSR, RSC, 대규모 트래픽, i18n 등 실무 경험은 제한적임
          + 커뮤니티 깊숙이 논의된 주제는 익숙하지만, 일반적인 앱 개발자의 일상과는 일부 괴리 있음
          + React 팀과의 긍정적·부정적 개인적 경험 모두가 관점에 영향을 줌
          + 역사적/구조적 배경 설명에서 최대한 성실하게 사실을 전달하려고 노력함

React의 간략한 역사

     * 2011~2012년 Facebook(현 Meta) 내부 개발, 2013년 오픈소스화
     * 최근까지 모든 핵심 개발은 Facebook/Meta의 React 팀이 주도
     * 핵심 개념(컴포넌트, props, state, data flow)은 유지, 구현·API·적용 범위는 지속적으로 확장·변화
          + createClass → ES6 class → 함수형 컴포넌트(Hooks 지원)
          + React Native로 모바일, react-reconciler로 WebGL(react-three-fiber), CLI(ink) 등 다양한 플랫폼 확장
          + 내부적으로 ""Fiber"" 아키텍처로 전면 리팩토링(성능/구조 혁신)
          + 2018년 Hooks 도입으로 함수형 컴포넌트에 상태/효과 부여
     * “최소 UI 라이브러리(V in MVC)” 전략으로, 아키텍처·상위 프레임워크·빌드·상태관리 등은 커뮤니티에 위임
          + 이로 인해 Redux/Mobx/Zustand(상태), Styled-Components/Emotion/CSS Modules(스타일), React Query/Apollo/SWR(RTK Query)(데이터), Webpack/Vite/Parcel 등 수많은 서드파티 라이브러리 및 빌드 도구 등장
     * 유연성이 장점이지만, 프로젝트별로 필요한 라이브러리 선택이 필수 → 코드베이스 다양성, 피로도, 표준 부재 문제 동반
     * 빌드 도구와 CRA
          + 초기에는 Webpack/Babel 등 복잡한 설정이 필수였음
          + Create React App(CRA): Webpack+Babel 기반 CLI, 복잡성 은닉, 단일 커맨드로 프로젝트 생성 가능(블랙박스 방식)
          + 데이터 패칭·상태 관리는 별도 3rd-party 도구에 의존
          + SSR(서버 사이드 렌더링) 기능은 초기부터 존재했으나, 수동 구현 필요
          + 이후 Next.js(SSR/파일 시스템 기반 라우팅, 데이터 패칭), Gatsby(정적 사이트, GraphQL), Remix(서버 데이터 로더) 등 프레임워크 등장
     * React Server Components(RSC)와 프레임워크 전환
          + 2020년 말 React Server Components(RSC) 프로토타입 공개, 비동기 컴포넌트·서버 데이터 패칭을 React로 표준화하려는 아키텍처적 변화
          + 개발 과정에서 일부 React팀이 Vercel로 이직, Next.js와 협업해 App Router 및 RSC 최초 구현
          + 2020~2023년 React 공식 문서(react.dev) 전면 개편, 대화형 튜토리얼 및 API 참조 강화
     * 공식 권장 사용법 변화
          + 과거 공식 문서에서는 CRA 기반 SPA 시작을 추천하고, SSR/정적 사이트 필요시 Next/Gatsby 등 대안을 안내
          + 새 문서(react.dev)에서는 프레임워크 사용을 강력 권장(라우팅, 데이터 패칭, 빌드 통합), RSC 구현체로 Next.js만 언급
          + CRA는 2022년경부터 사실상 관리 중단(공식적으로 deprecated는 아니었으나, 커뮤니티에서 점차 대체)
          + 공식 문서와 커뮤니티 내에서도 “Next.js가 진짜 React 18”이라는 의견 등 Next.js와의 강한 연계가 부각

React와 소유 기업(스폰서)의 관계

     * Meta(Facebook)
          + React는 처음부터 Facebook/Meta가 소유·주도한 프로젝트
          + 오픈소스이지만 실질적 개발은 Meta React 팀이 담당, 핵심 회의·로드맵도 모두 내부 중심
          + 새로운 기능은 Meta 내부 여러 앱팀에서 실제로 검증·테스트 후 외부 공개
          + React 팀은 개발 자유도가 높았으며, 로드맵과 비전 수립을 주도적으로 수행
          + 성과와 프로젝트가 Meta 비즈니스에 어떻게 기여하는지는 내부적으로 검증 필요
          + Meta는 자체 서버 인프라와 GraphQL, Relay 등 자체 기술을 적극 활용, 라우팅·상태관리 등에서 커뮤니티와 다른 방식으로 React 사용
          + 따라서 Meta 내부에선 외부 3rd-party 라이브러리 활용 빈도가 낮음
     * Vercel, Next.js, React
          + Vercel은 웹앱 호스팅 플랫폼이자 Next.js 프레임워크의 개발사
          + 비즈니스 모델은 Next 등 프레임워크 확산 → Vercel 플랫폼 호스팅 유도
          + CEO(Guillermo Rauch)는 초기부터 React 렌더링 철학에 깊은 신뢰와 투자
          + 2021년, React 팀 리더 Sebastian Markbage가 Meta에서 Vercel로 이직, Andrew Clark·Tom Occhino 등 주요 인력 합류
          + Vercel 이적 멤버들은 React Server Components(RSC)·Next.js App Router 등 핵심 기능을 React와 Next.js 양쪽에서 구현
          + Vercel 소속 엔지니어들도 React 코어 및 서버 렌더링 기능에 실질적 기여
          + 2025년 현재, React 팀은 Meta와 Vercel에 나뉘어 활동(주력은 Meta, 주요 코어 구현은 Vercel도 관여), 외부 기여자도 일부 있음

React 사용 패턴

     * 표준 아키텍처
          + React 자체는 SPA, SSR, SSG, 하이브리드 등 다양한 방식 모두 지원
               o SPA: 빈 HTML에 React 트리 전체를 클라이언트에서 렌더링
               o SSR: 서버에서 요청마다 동적 HTML 생성
               o SSG: 빌드 타임에 정적 HTML 미리 생성
               o 여러 언어나 프레임워크(Python/Django, Ruby/Rails, PHP/WordPress 등)와 조합 가능
          + 2015년경부터 SPA(클라이언트 렌더링) 방식이 표준이었으나, 초기 로딩 속도·JS 번들 크기·네이티브 브라우저 동작 차이 등 트레이드오프 존재
          + 데이터 패칭은 원래 Redux 등에서 직접 로직 구현, 이후 React Query/Apollo/SWR/RTK Query 등 전용 훅/라이브러리 등장으로 단순화
          + Next.js, Remix 등 프레임워크는 SSR, SSG, 파일 시스템 라우팅 표준화와 데이터 패칭 구조화로 React의 활용 폭을 넓힘
          + SSR 기반 아키텍처(서버 렌더링, 프리패치, 코드 스플리팅 등)로 이동하는 트렌드
          + React팀은 ""데이터 패칭 waterfall"" 현상을 지양하며, 라우트 단위 사전 패칭 등 성능 개선 패턴을 권장
     * 빌드 도구 및 프레임워크 사용 현황
          + 주요 도구/프레임워크:
               o Next.js(SSR/SSG/RSC/SPA)
               o Remix / React-Router v7(SSR/SSG/SPA)
               o Vite(SPA, 빌드 도구, 다양한 프레임워크 플러그인 지원)
               o Create React App(SPA)
          + Vite는 Vue 생태계에서 출발했으나, React, Svelte 등 다수 지원 → React 공식 플러그인, 프레임워크 빌드툴 표준
          + Remix/React-Router도 최근 Vite 기반 SSR/SSG 지원 구조로 이동
          + 다운로드 통계 요약:
               o Next.js 사용률 압도적 1위
               o Vite의 React 플러그인 급성장, 두 번째로 많이 쓰임
               o CRA(react-scripts) 2023년 이후 하락세지만 여전히 상당한 사용량
               o Remix는 입소문은 강하지만 전체 규모는 제한적, React Router는 framework mode로의 전환 속도가 느림
               o Gatsby는 Next/Vite/CRA보다 한참 적고, 최근 Astro(다중 프레임워크 정적 사이트)에도 역전당함
               o Astro는 React에 특화되진 않지만 Remix와 비슷한 규모
               o Vite+CRA 사용량 합치면 Next와 대등 → SPA 방식 수요도 여전히 높음

React Server Components(RSC) 내부 및 프레임워크와의 관계

     * RSC 개발 배경 및 Vercel 협력
          + Meta 내부 인프라는 이미 자체 서버 구조가 구축되어 있어 RSC(React Server Components) 개발·테스트에 한계
          + RSC는 React 팀이 직접 주도한 미래 지향적 설계, Meta·Vercel의 지시나 요구가 아닌 React팀의 독립적 비전에서 출발
          + React팀이 Vercel에 RSC 비전을 제안, Vercel이 개발의 실험장·지원자로 합류
          + Sebastian Markbage, Andrew Clark 등 React 코어 멤버가 Meta→Vercel로 이동, Next.js 팀이 App Router(최초의 실제 RSC 적용 사례)를 구축
          + 이 과정에서 Next.js는 거의 처음부터 다시 설계·구현
          + Shopify(Hydrogen), Remix 등 다른 프레임워크도 초기 테스트·협력 시도는 있었으나 본격 참여는 미비
          + 결과적으로 Next.js App Router만이 실제 “프로덕션급” RSC 구현체로 자리, 다른 프레임워크(React Router, Parcel, Waku 등)는 현재 통합 작업 중이나 대중적 사용은 Next가 독점
     * RSC와 프레임워크/번들러의 통합 구조
          + ""RSC는 왜 프레임워크나 번들러가 꼭 필요하냐?""는 질문이 많음
          + 기존 SSR(renderToString, renderToPipeableStream)은 어디서든 실행 가능했지만, RSC는 구조적으로 훨씬 복잡
               o 코드 내 'use client', 'use server' 디렉티브 해석
               o 클라이언트/서버 컴포넌트 분리 및 등록 자동화 필요
               o 모듈 그래프 전체를 분석·컴파일하는 번들러(예: Webpack, Vite 등)와 긴밀한 통합이 필수
          + React 코어는 RSC의 핵심 로직과 데이터 직렬화 API만 제공, 실제 동작과 라우팅, 엔드포인트 호출 등은 프레임워크가 책임
          + 각 프레임워크마다 RSC 활용·아키텍처·구현 방식이 상이
               o Next.js App Router는 레이아웃·라우팅 등 자체 규칙 적용
               o Parcel, Waku, React Router 등은 각기 다른 설계 도입 중
          + 정리:
               o RSC는 React 코어에 내장된 하이브리드 기능이지만, 실제 사용은 반드시 번들러/프레임워크의 통합이 필요
               o 프레임워크가 지원해야만 RSC의 강점을 실제로 활용 가능

커뮤니티의 우려와 혼란

     * 1. ""Vercel이 React를 장악했다""는 우려
          + Vercel이 주요 React 팀원을 고용하고, Next.js가 문서·예제에서 강조되어 “Vercel이 React 개발을 주도한다”는 의혹 확산
          + 실제로는 React 팀이 RSC 비전 및 Next App Router 구조를 주도했고, Vercel은 지원자·실험장 역할
          + Vercel이 React를 ""장악""했다기보단, React 코어팀 일부가 Vercel로 이직하여 비전을 실현한 구조
     * 2. ""React는 이제 Next에서만 동작한다""는 오해
          + Next.js만이 유일한 RSC 프로덕션 구현체로 강조되다 보니 이런 오해가 생김
          + React 공식 문서에는 Next 이외에도 다양한 프레임워크 및 프레임워크 없는 사용법도 언급
          + Next는 React 기반 프레임워크일 뿐, React 단독·다른 도구와도 여전히 사용 가능
     * 3. ""클라이언트 앱에서 React가 사라질 수 있다""는 불안
          + 서버 기능(RSC, SSR 등) 강조로 SPA 클라이언트 지원 중단을 걱정하는 목소리 존재
          + React의 클라이언트 렌더링 기능은 결코 사라지지 않음
               o Meta 등 대형 코드베이스도 클라이언트 방식 대량 활용
               o React팀은 호환성·마이그레이션 지원에 철저
     * 4. ""왜 React는 프레임워크 사용을 강요하는가?""
          + React팀은 데이터 패칭·라우팅·SSR 통합 등 프레임워크의 생산성·성능 장점을 이유로 기본 권장
          + ""직접 설정(맞춤 SPA)은 장기적으로 비효율적, 프레임워크가 업계 표준""이라는 입장
          + 실제론 다양한 사용 패턴이 있는데, 지나치게 일괄적인 권장이 됨
               o 초보자 진입장벽, 서버 호스팅 제한 기업, SPA 호스팅의 단순성 등 현실적인 이유로 SPA도 여전히 유효
          + 공식 문서의 ""비프레임워크 방식"" 안내가 부차적으로 취급되어 커뮤니티에 소외감 제공
     * 5. 공식 문서의 한계 및 개선 논란
          + CRA(react-scripts) 공식적으로 deprecated, 대체 도구(Vite 등) 언급이 늦어져 혼란 가중
          + SPA 등 ""비프레임워크"" 방식도 공식적으로 인정·가이드 추가(2025년 최신 문서)
          + Vite 등 주요 빌드 도구에 대한 공식 언급이 늦은 점에 대해 커뮤니티, 생태계 인물(Evan You 등)도 문제 제기
          + 개선된 최신 문서에서는 SPA, Vite/Parcel/RSPack 등도 추천, 라우터·데이터 패칭 선택 가이드 제공
     * 6. RSC 공식 문서 및 설명 부족
          + Next.js, Vercel 등 외부 자료에 비해 React 공식 문서 내 RSC 설명·개념 안내 부족
          + API Reference에만 단편적 정보, 전반적 개념/적용 가이드 미흡
          + 커뮤니티와 주요 인물들(Dan Abramov 등)이 적극적으로 보충 설명을 제공하고 있으나, 공식화 부족이 지속적 혼란 초래
          + RSC의 컨셉, 아키텍처, 도입 사례, FAQ 등 문서 섹션 추가 필요성 제기

주요 우려 사항에 대한 정리

     * 프레임워크·서버 기능 강조가 “Vercel의 이익 때문”이라는 오해가 커뮤니티에 뿌리내렸으나, 실제로는 사실과 다름
          + React팀의 커뮤니케이션, 공식 문서의 표현 등이 오해를 키운 측면이 있음
     * React의 클라이언트 렌더링 기능은 앞으로도 계속 유지, 서버 기능(RSC/SSR 등)은 옵션일 뿐, SPA 등 기존 방식 계속 사용 가능
     * 커뮤니티의 우려와 혼란은 React팀의 소통 부족, 개발자 관계(DevRel) 활동 미흡도 한 요인
          + 공식 입장 표명, 오해 해소, 다양한 패턴에 대한 인정과 안내가 필요
     * “프레임워크 사용” 권장은 선의에서 출발했으나, 실제로는 너무 획일적으로 느껴지고, 다양한 사용 패턴을 소외시켰다는 비판 존재
     * 2025년 이후 공식 문서가 개선되어 SPA/맞춤 빌드 방식도 인정·가이드가 추가됐지만, 커뮤니티 피드백을 반영하는 데 오랜 시간이 소요
     * RSC(React Server Components) 개념, 트레이드오프 등 핵심 내용의 공식 문서 보강 필요
          + 커뮤니티의 올바른 이해와 불필요한 논란 방지에 도움

맺음말

     * 본문은 React가 어떻게 발전해왔는지, 어떤 영향과 목표 하에 개발되고 있는지, 그리고 현재 생태계에서 사용 패턴이 어떻게 자리잡았는지에 대한 다양한 질문에 대한 답변을 제공
     * React 팀의 방향성과 변화에 대해 발생한 혼란이나 FUD(공포·불확실성·의심)를 해소하고자 했음
          + 기술적 방향에 동의하지 않거나 RSC/대형 프레임워크를 굳이 쓸 필요성을 못 느껴도, React 팀의 의도와 방향성은 충분히 타당
     * React팀의 정책은 성능 표준화와 커뮤니티의 UX 개선이라는 선의에서 비롯되었으나, 불친절한 소통과 문서로 불필요한 혼란을 키움
     * SPA, 프레임워크, 다양한 빌드 도구 등 실제 사용 패턴의 다양성에 대한 존중과 공식 문서의 개선 필요성이 커짐
     * 앞으로는 커뮤니티 피드백 반영과 다양한 아키텍처를 포용하는 문서 개선, 오픈 커뮤니케이션이 React의 건강한 발전에 핵심

   RSC를 시작부터 Next.js 기반으로 개발했다는 사실 자체는 다르지 않은 듯 하네요.
   Next.js가 점점 더 Vercel 이외의 환경에서는 삐걱대는 것까지 조합하면
   React 팀 내부 생각이 어떤진 몰라도 RSC가 미래에요! 근데 RSC를 구동하기 위해서는 Next.js를 추천하고 Next.js는 Vercel에서 쓰세요 라는 논리 흐름이 되는데 이게 Vercel의 이익과 관련이 없냐고 하면 흠...
   아무리 오해라고 주장해도 결국 사람들은 Vercel이 React를 잠식했다 라고 느낄 수밖에 없고 오해를 풀기 위해서 빠르게 대응한 것도 아니지 않나요?

   당장 리액트 공홈이 메타 쪽 서버가 아니라 Vercel 위에서 돌아가죠.

   동의합니다.

   똑같이 Vercel이 투자하는 svelte는 규모가 작아서 그런진 몰라도 이렇게 벤더 락인이 걸리는 경우가 없다고 느꼈는데 차이점도 궁금하네요.

   스벨트는 버셀이 후원만 하는거지 버셀이 리드하는게 아닙니다. 반면 넥스트는 버셀이 아예 리드하는거고요.

   깃허브 저장소 같은 경우에도 넥스트는 버셀 조직 산하지만 스벨트는 그렇지 않아요.

   그리고 넥스트.js는 공홈 푸터의 카피라이트 표기에 버셀이 있지만 스벨트는 없습니다.

   버셀이 리치 해리스를 고용한건 후원 성격이었군요

   https://vercel.com/blog/vercel-welcomes-rich-harris-creator-of-svelte 넵 후원 성격이 강한 고용입니다. 스벨트 개발에 풀타임으로 전념할 수 있도록 월급을 주는거죠. 버셀 측도 스벨트는 여전히 독립적인 프로젝트라는 것을 명확히 했습니다.

   사실 여기 웹 프론트엔드 개발 이슈는 꽤 적게 올라오는데... 그럼에도 최근에 nextjs가 언급이 너무 많이 되는게 심상치 않게 느껴집니다.
   Server component만 문제지 다른건 괜찮아~ 라는 느낌이네요.

   프레임워크가 편하다라는것과 React 자체에서 CRA 를 포기하는건 아예 차원이 다른 문제인듯요. 바뀐 리액트 문서에서 다짜고짜 next 깔라고 하길래 좀 위화감을 느꼈는데 저만 느낀게 아니었네요.

   Vercel과 리액트 개발팀이 별개인 줄 알았는데 실제로는 연관이 더 깊군요

   react 는 뭔가 나사 빠진 미완성 라이브러리 느낌... 예를 들어 useEffect 공식 문서에 주렁주렁 써있는거 보면... 황당할뿐... 이렇게 많은 토끼굴을 막 만들어 놓고, 잘 쓰라는 태도는 뭔지... 별 생각없이 땜방식으로 조치하는 느낌을 많이 받음.

   개선안 제시 못 하면 넌 주니어

   React Native 도 비슷한 분위기. React는 Next.js라면 React Native는 Expo. 공식 문서도 Expo 프레임워크 사용을 권장하고, 기존의 cli 방식은 이제 찾기 불편하게 되어있음.

   js fe 이쪽은 생태계 한번 싹다 멸망하고 리셋한번 해주세요. 그리고 상태관리 이런거도 좀 싹 다 포함해서 프레임워크를 만들어 주길. 지나치게 많은 선택지? 그건 자유가 아니고 그냥 무책임일뿐.

   React로 UI 상호작용, 내부 상태 계산, 상태 표시 정도만 필요한 게임 프로토타이핑을 하고 있습니다. 몇 년 전 create-react-app이 공식 문서에서 퇴출될랑말랑 때와 비교해 최근이 좀 더 공식 문서를 보고 셋업하기 까다로웠다고 느꼈습니다. 그 때 작성했던 글이 조금 관련있을 것 같네요.
     * https://guji.jjme.me/wiki/prototyping-game-with-react/
"
"https://news.hada.io/topic?id=21562","Claude Code 사용량 모니터 – 사용량 제한 회피를 위한 실시간 트래커","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Claude Code 사용량 모니터 – 사용량 제한 회피를 위한 실시간 트래커

     * Claude AI 토큰 사용량, 소진 속도, 소모 예측등 다양한 정보를 실시간으로 터미널에 보여주는 모니터링 툴
     * 3초마다 갱신되는 컬러풀한 프로그레스 바, 스마트한 토큰 소진 예측
     * 기본 플랜 한도 초과 시 세션 기록을 분석해 실제 한도로 즉시 전환
     * 자동으로 Pro/Max5/Max20/custom_max 등 사용량 플랜을 감지 및 지원
     * 세션별 토큰 한도 임박·초과, 세션 리셋 전 소진 위험시 실시간 알림
     * 실제 Claude 사용 흐름에 최적화된 인터페이스 구현
     * npm, pip 설치, 가상환경(venv/virtualenv) 사용 권장,Mac/Linux/Windows 전부 지원

Claude 세션 이해하기

     * 5시간 롤링 윈도우 방식
          + 최초 메시지를 보낸 시점부터 세션 5시간 유지
          + 세션별 한도 적용, 복수 동시 활성화 가능
          + 실제 리셋은 내 메시지 기준 매 5시간마다 발생
     * 세션/토큰 리셋 시간 기준을 내 스케줄에 맞게 지정 가능

활용 시나리오

     * 출근/오전 개발자: 일과 시작 시간(예: 9시)에 맞춰 토큰 리셋 스케줄 조정해 효율적 플래닝 가능
     * 야간 작업자: 자정 등 본인 스케줄에 맞춘 토큰 리셋 이용
     * 가변 한도 사용자: custom_max 모드로 실제 환경에 맞는 한도 자동 감지
     * 글로벌/원격 개발자: 여러 타임존 이동, 팀 단위 리셋 시간 지정 → 협업 최적화
     * 빠른 상태 확인: 단순 실행(설정 무관)

환경 구축 베스트 프랙티스

     * 세션 시작과 동시에 모니터링 시작
          + Claude 작업을 시작할 때 즉시 모니터 실행 (./ccusage_monitor.py)
          + 지원 플랜
               o pro: 약 7,000 토큰(테스트 및 경량 사용)
               o max5: 약 35,000 토큰(일상적 개발)
               o max20: 약 140,000 토큰(대형 프로젝트 및 중/고강도 사용)
               o custom_max: 자동 감지 모드(실제 사용 이력 기준 최대값 사용)
          + 세션 전체 토큰 추적 정확도 향상
          + 토큰 소모 속도 계산과 한도 임박 조기 경고 가능
     * Python 가상환경(venv) 사용
          + 의존성 충돌 방지, 환경 분리, 프로젝트별 재현성 보장
          + 설치 및 실행:
python3 -m venv venv
source venv/bin/activate
pip install pytz

          + 삭제 시 가상환경 폴더만 지우면 깨끗한 제거 가능
     * 쉘 Alias 커스텀 등록
          + 반복적인 명령을 한 줄로 단축 실행
alias claude-monitor='cd ~/Claude-Code-Usage-Monitor && source venv/bin/activate && ./ccusage_monitor.py'

          + .bashrc 또는 .zshrc 등에 추가, 한 번 입력으로 곧바로 모니터 실행 가능

사용 방법 베스트 프랙티스

     * Burn Rate(소모 속도) 상시 모니터링
          + 토큰 사용량이 갑자기 치솟는 경우 주의
          + 남은 시간·토큰량에 따라 작업 강도 조절
          + 세션 리셋(토큰 초기화) 전후로 대규모 리팩터링 등 큰 작업 일정 조정
     * 전략적 세션 스케줄링
          + 대규모 작업은 토큰 리셋 직후 시작, 한도 임박 시에는 가벼운 작업 수행
./ccusage_monitor.py --reset-hour 9

          + 세션별 5시간 규칙을 활용해 여러 세션 중첩 운영 가능
     * 타임존 명확히 지정
          + 실제 근무/협업 시간대 반영하여 정확한 토큰 리셋 예측 및 일정 관리
./ccusage_monitor.py --timezone Asia/Seoul

          + 여러 국가·팀과 협업 시 시간 오차 방지, 세션 만료 시점 혼동 최소화

최적화 팁

     * 터미널 환경 세팅
          + 최소 80자 너비 터미널 권장
          + 컬러 지원으로 시각적 피드백 극대화
          + 별도 전용 창에서 상시 모니터링 추천
     * 워크플로우 통합
          + tmux 등 터미널 멀티플렉서 활용해 개발과 동시에 모니터링 가능
tmux new-session -d -s claude-monitor './ccusage_monitor.py'
tmux attach -t claude-monitor

     * 멀티 세션 전략
          + 세션별로 5시간 고정, 중첩 세션 동시 관리 가능
          + 긴 작업은 여러 세션에 분산, 각 세션 한도/만료 주의

실제 워크플로우 예시

     * 대형 프로젝트 개발
./ccusage_monitor.py --plan max20 --reset-hour 8 --timezone America/New_York

          + 오전 8시 토큰 리셋 → 주요 기능 개발 시작
          + 10시 Burn Rate 점검 후 작업 속도 조절
          + 12시 오후 일정 점검 및 조율
          + 14시 새 세션 오픈, 복잡한 이슈 처리
          + 16시 가벼운 작업/저녁 세션 준비
     * 러닝/실험 중심 사용
./ccusage_monitor.py --plan pro

          + 가벼운 학습, 실험적 코드 작성에 적합
     * 스프린트 집중 개발
./ccusage_monitor.py --plan max20 --reset-hour 6

          + 집중적인 대량 토큰 소모가 예상되는 개발에 맞춤 설정

        Hacker News 의견

     * 나는 Claude의 투명성 부족이 답답하고, 이 아이디어가 참 마음에 든다는 의견 공유 Claude Code의 핵심 기능은 데스크톱 앱에 비해 컨텍스트와 제한 관리를 더 잘할 수 있다는 것(예: compact 모드, 남은 용량 % 표시)인데, 아직 충분하지 않다는 느낌 추가 조언으로, 프로젝트 README에 이모지가 너무 많이 사용되는 건 개인적으로 굉장히 비전문적으로 느껴지며, 마치 AI가 적절히 관리되지 않은 채로 '분위기'만으로 코딩된 프로젝트 같다는 걱정이 든다는 생각 공유
          + 내가 소프트웨어 쪽에 들어왔던 시절엔 코드베이스에 이모지 쓰다 걸리면 정신병원에 보낼 정도 분위기였음 지금은 시대가 확 바뀌어서, 이모지를 시각적으로 맥락 정리에 쓴다는 점에서 자주 사용함 이제 내 코드는 나를 행복하게 해줄 만큼 이모지가 많이 들어가 있음
          + 요즘 스타트업이나 젊은 회사들에서 이런 이모지 스타일이 많이 보임 아마 Notion의 영향이 큰 듯 우리 회사에서는 리스트, 페이지, 캘린더 인바이트 하나 만들 때도 늘 이모지를 고른다는 특징 있음
          + AI 코딩을 위해 만들어진 소프트웨어에 대해 이런 코멘트가 달리는 게 참 아이러니하다는 생각 공유
          + 실제로 코드를 보면, 400줄짜리 파이썬 파일 하나가 그냥 ccusage를 감싸고 있을 뿐임 그래서 그렇게 느끼는 것도 가능하다고 생각
          + AI가 만든 PR 설명이나 README는 프롬프트에 ‘간결하게, 화려한 수식이나 이모지 없이’라는 조건을 꼭 추가함 이렇게 하면 산만했던 이모지 파티가 적절한 문서로 변함 상황에 따라 달라질 수는 있음
     * 나는 ccusage 제작자라는 소개와 함께, 사람들이 우리 오픈소스를 다양하게 써줘서 기쁘다는 소감 Happy vibe coding!이라는 긍정 메시지 남김
          + 신기하게도 ccusage의 Show HN에는 댓글이 하나도 없는데, 이 쓰레드는 무척 활발하다는 점이 재미로움 관련 쓰레드 링크
     * 참고로, 내 과거 세션 최대 토큰 제한은 약 337,492개였고, Max20 플랜과 Opus를 99% 정도 사용한다는 경험 공유 Claude Code를 5월 27일부터 써서 총 사용한 토큰이 1,374,439,311개, 금액으로는 약 3,397달러임
          + 나는 Max20 플랜으로 약 2,100달러어치 정도 사용 API에서 엄청난 마진이 남는 건지, 아니면 손해를 보고 있는 건지 궁금 매일 쓰지만 과하게 쓴다고 생각하진 않음
          + Opus에서 속도 제한에 자주 걸리지 않는지, Sonnet에 비해 느리다고 느끼지 않는지 궁금
     * 나는 지금까지 쓴 토큰이 리미트에 얼마나 가까운지 직감적으로 파악할 수 있음 대화 자체가 최대치에 도달할 것 같은 순간도 감지하는데, 그때 마지막 남은 리소스로 요약본 생성 후 새로운 대화로 이어가면서 작업을 계속함 이런 AI 도구들이 이제 내 생체 시계 일부가 된 느낌 매주 수요일마다 ChatGPT의 주간 리미트가 리셋돼서, 수요일이 새로운 일요일처럼 느껴지는 경험 공유
     * 토큰 사용량이 시간창이 지나도 100%에 도달하지 않으면 리셋되지 않는 걸 관찰함 예를 들어 90% 썼다가 다음 윈도우로 넘어가고, 남은 10%를 금방 소진하면 긴 시간 대기해야 하는 문제가 있음
     * 나는 여러 Claude Code 세션을 동시에 사용할 수 있게 만드는 UI 툴(crystal)을 만들었음 여러 기능을 한 번에 작업하다보니 내 계정 한계에 자주 다다르게 됨 보통 리셋 시간 근처에서 한계에 도달하지만, 언제쯤 쉬어야 할지 미리 알 수 있으면 더 좋겠다는 느낌
          + Claude Code를 엄청나게 많이 사용하지만, worktree랑 여러 세션 작업을 위한 툴링을 따로 짤 자신은 git 이해도가 부족해서 망설이게 됨 솔직히 이 도구를 쓰는 것도 조금 두려운데, 이상적으로는 각 worktree를 컨테이너에서 돌리고 싶지만 Crystal만큼 부드럽게 돌아가게끔 만들기는 어려울 것 같은 느낌
          + 이 툴 좋아하지만, Crystal은 이미 예전에 써봤던 프로그래밍 언어 이름이라 혼동됨
          + GitHub에 이슈를 남겨주면(여기), 내 usage monitor와 연동도 시도해볼 수 있음
          + 멋지다는 감탄 나도 프로젝트별이 아니라 동시 5개 프로젝트에도 이런 툴이 있었으면 좋겠다고 Laude에게 시켰을 뻔했다는 경험 공유 많은 활용 기회에 공감
     * 정말 흥미로운데, Pro 플랜의 토큰 제한이 정말 7,000개밖에 안 되는지 궁금 즉, 7,000단어도 안 되는 셈인데, 실제로는 훨씬 더 많이 쓸 수 있는 느낌임 이 정도면 대화가 조금만 길어져도 금방 한계가 올 것 같은데, 나는 아직 한 번도 부딪혀본 적 없음 혹시 Claude Code에만 적용되는 제한인지, 아직 Claude Code를 많이 써보지 않아 잘 모르겠다고 의견 남김
          + 프로 플랜은 한 달에 $20. 최근에 claude code 접근이 가능해짐 그런데 몇 번 쿼리만으로도 일부 사용자들은 리미트에 걸린다는 얘기를 들었음 그래서 그 정도 수치가 맞는 것 같다는 생각 채팅 인터페이스의 제한과 Claude Code의 제한이 별개임
     * 대단히 훌륭함에 감탄, 제작해준 것에 감사함 uv로 설치 가능 여부 궁금 uv 링크 공유와 함께, 설치 과정을 한 줄씩 정리해 셸 명령 예시 공유
          + 만약 해당 레포가 project.toml 등 패키지 구조로 되어 있었다면 pipx(pipx)로 다음과 같이 더 빠르게 설치할 수 있었을 것이라는 설명

     pipx install git+https://github.com/Maciek-roboblog/Claude-Code-Usage-Monitor ccusage_monitor uv에서도 이와 유사한 명령(uvx)이 있을 것 같은데, pipx와 같은 기능/목적인지는 잘 모르겠다는 의견

     * 참고로 pip으로 설치 가능한 거의 모든 것은 uv로도 설치할 수 있으니, uv로도 더 쉽다는 사실 공유
     * 혹시 이 도구가 ccusage로 쉘 호출해서 실행되는 것 말고 다른 유용한 점이 있는지 궁금 솔직히 이런 류의 프로젝트는 조금 실망스럽고, AI 툴로 한 번에 끝낸 느낌도 있음 Show HN에선 모든 실제 작업이 타툴에 의해 처리된다고 언급조차 없어 아쉬움
     * 어제 Claude Code에서 특이한 경험이 있었음 오래된 PHP로 작성한 phtml 테이블 페이지를 새 div 레이아웃으로 변환하려다 실패하고 $4 정도를 소진한 경험 WSL 문제일 수도 있지만, 이런 일이 자주 일어나지 않길 바라는 마음
          + Claude Code는 러닝커브가 꽤 있음 요구사항을 충분히 논의하고 모델에게 명확한 질문을 유도하는 식으로 긴 대화 세션을 진행해야 함 그래도 가끔은 이런 실패가 발생할 수 있으니, 매우 값비싼 도구라는 점을 꼭 기억해야 함 유튜버나 블로거들이 말하는 것처럼 마법은 아니라는 점 상기

   같은 거네요: https://news.hada.io/topic?id=21560
"
"https://news.hada.io/topic?id=21471","무한 저항기 그리드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               무한 저항기 그리드

     * 무한한 저항기 그리드에 대한 고전적인 수수께끼는 무한 사각 격자에 있는 인접 노드 사이의 유효 저항을 구하는 문제임
     * 인접 노드 사이의 유효 저항은 격자 대칭성과 Laplace 방정식의 해법을 이용해 R/2로 나타낼 수 있음
     * 무한 그리드에서의 전류 유입과 유출 위치, 경계 조건에 따라 해가 불확정적일 수 있음
     * 실제 물리적 회로와 달리, 이상화된 그리드에서는 엄밀한 해석이 어려움
     * 여러 수학적 방법(차분방정식, Fourier 급수 등)과 적분식을 통해 모든 노드 쌍 간의 저항 계산이 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 문제 정의

     * “무한 저항기 그리드”는 사각 격자의 각 인접 노드를 저항 R으로 연결한 구조를 상정함
     * 이 구조에서 특정 두 노드(주로 인접 노드) 사이의 유효 저항을 구하는 수수께끼임
     * 인접 노드 사이에는 대칭성·직관적 해석 등을 통해 저항이 R/2로 나타남
     * 이는 전기 쌍극자의 전위 특성과 유사하며, 격자 노드 전압도 Laplace 방정식의 차분 형태를 따름

직관적 풀이와 한계

     * 무한 그리드에 단일 노드로 전류 주입 시, 네 방향으로 균등하게 전류가 퍼지는 대칭 형태를 가정함
     * 인접한 두 노드 사이에 전류를 주입하고 추출하는 두 경우의 해를 합(superposition) 하면, 직경 방향 저항이 R/2로 집계됨
     * 이 해법은 직관적으로 그럴듯하나, 엄밀하게 증명하기 위해서는 무한 지점에서의 전압과 전류 거동, 그리고 총 전류의 유입·유출 경로에 대한 보다 엄격한 해명 필요함
     * 실제로, 중심 노드에서 무한대로 갈수록 저항이 무한대로 발산하게 되어, 단순히 무한을 접지로 보는 해석은 물리적으로 엄밀하지 않음

엄밀한 수학적 해석

  유한 격자와 무한 격자

     * 문제를 엄밀히 해석하려면, 실제로는 유한하지만 매우 큰 격자의 극한을 고려해야 함
     * 중심과 주변에 순차적으로 확장되는 격자 구조 내에서 경계 조건을 맞춰야 실제 물리적으로 허용 가능한 해가 만들어짐
     * 무한 구조에서는 경계 조건 없이 고유의 해가 정해지지 않는 불확정성 문제가 항상 존재함

  1차원 격자의 차분방정식 해법

     * 1차원 저항기 배열에서는 차분방정식을 세우고, 일반해에서 공명항(resonance term) 을 적용해 각 노드에서의 전압 분포 구함
     * n번째 노드의 전위는 |n|/2로, k개 저항기가 있으면 유효 저항은 kR이 됨

  2차원 격자의 해석

     * 2차원 격자에서는 (m,n) 위치에서의 전위 역시 차분방정식으로 표현 가능
     * Fourier 급수 및 복수의 고유해법을 만든 후, 서로 다른 위치에서의 조건들이 모두 충족되도록 적분(superposition) 을 통해 해 구함
     * 인접 노드(1,0)에서의 전압은 1/4V이며, 전류가 -1A일 때 저항이 1/2이 됨
     * 더 복잡한 위치(예: 대각선상 노드 등)는 적분식을 이용한 수식화로 구함

적분식과 일반화

     * 격자 내任任二任] 모든 노드 쌍의 저항 값은 복수 변수의 적분(예: α, β 및 대체 변수 s, σ 등)으로 일반화할 수 있음
     * 해석 과정에선 고유방정식, 삼각다항식, 변수 변환 등을 사용하여 계산의 간소화 가능
     * 대각선상 노드 간 저항, 그 외의 노드 간 저항 모두 적절한 적분 및 회귀식으로 계산할 수 있음
     * Fourier 급수, 삼각치환, 변수변환 등 다양한 수학적 수단이 동원됨

결론 및 기타

     * 무한 저항기 그리드는 대칭성과 수학적 구조 덕분에 직관적으로도 명확한 해를 보이나, 엄밀하게는 경계 조건과 현실성을 따져야 함
     * 저항 계산은 수학적 테크닉(차분방정식, 적분, 특이점 처리 등)을 활용해 일반화할 수 있음
     * 이상적인 그리드는 실제 회로의 물리 법칙(유한 속도 전달, 유한 저항 등) 을 따르지 않으며, 현실과 이론상의 의미 차이가 존재함
     * 실용 사례나 추가적인 수학적 접근법은 별도의 수학 노트에서 더 다룸

        Hacker News 의견

     * 실제 현업 문제와 무관하다고 사람들이 생각하지만, 사실 실리콘 기판의 저항은 현실적으로도 무한 저항 격자와 매우 유사함을 언급하고 싶음. 실리콘 기판은 대개 매우 두껍게 도핑(p-타입)되어 나오며, 팹에서 제공하는 정보는 저항률(resistivity, 보통 1~100 ohmcm)뿐임. 최신 공정에서는 주로 10 ohmcm 수준임. 기판을 통한 잡음 결합 이해에는 단일 점 대 점 저항 계산이 아니라, 격자 전체로 생각하는 직관이 필요함. 잡음을 모으기 위해 기판 접점을 격자 형태로 분산해야 한다는 점에서, 결국 무한 저항 그리드 문제와 연결됨
          + 포토리소그래피에 대해 어렴풋이 어렵다고만 느꼈는데, 이게 진짜로 이집트 여신(리토) 이름이 등장할 만큼 복잡한 분야라는 걸 몰랐음. 직접 경험해본 소감임
          + 설명한 상황은 오히려 연속체 모델이기 때문에 수학적으로 더 단순하다고 생각함
          + 저항률의 단위는 ohm*cm임을 강조하고 싶음. 예전에 Fairchild에서 일하면서 배운 내용임
     * 나는 수학과 전자공학의 관점 모두 가지고 있음. 전자공학자로서는 실험적으로 저항을 측정하려면 전류를 실제로 걸어봐야 한다고 주장함. 그리고 전류를 언제 걸었느냐에 따른 분산 인덕턴스·커패시턴스, 그리고 필드 전파 속도까지 묻게 됨. 이런 얘기를 들은 수학자는 술집에 가서 독한 술 한잔으로 진정함
          + 결국에는 물리학자를 데려와야 하는 상황이 됨. 물리학자는 충분히 먼 거리에서는 양자 현상이 지배적이 된다고 지적함. 아주 먼 곳의 노드에서 1초에 움직이는 전자의 수(즉, 전류의 흐름)는 결국 0 아니면 1임
          + “언제?”라는 질문에는, 모든 과도 응답(transient response)이 사라질 때까지 그냥 무한 시간 기다려야 한다고 답변할 수 있음. 그때가 되면 격자는 정상 상태에 진입해서, 회로도에서 보는 것과 똑같은 상태가 됨
          + 회로도 해석엔 두 가지 관점이 있다고 생각함. 하나는 실제 물리 부품(저항, 인덕턴스, 논리적 비선형성, 접지 평면 커패시턴스 등)을 표현하는 경우. OrCad 같은 도구를 사용할 때 의미하는 해석임. 또 다른 해석은 저항이 오직 이상적인 옴의 법칙만 따르고, 배선에 인덕턴스나 지연·저항이 전혀 없는 이상적 가상 세계. 이 경우 전압원의 두 단자를 바로 연결하는 것은 0으로 나누는 셈임. 가끔 현실 회로를 모델링하고 싶을 때 첫 해석에서 두 번째로 번역하며, 명시적으로 인덕턴스·저항 등을 추가함. 그게 아니면 SPICE 시뮬레이터가 알아서 처리해줌. 무한 저항 격자는 두 번째 해석에서만 존재함
          + 무한 저항 격자가 분명히 단순한 ‘장난감’ 문제라는 점은 맞지만, 실제로 우주가 무한하다고 가정하고 해석하는 것은 천체물리학의 현실임. 인간이 이런 스케일에 대한 직관이 부족해서, 우주 해석에 있어서 보이지 않는 사각지대가 생기는지 궁금함
          + 무한 저항 격자라면 행성 같은 구조체가 생길 수 있을까라는 재미있는 질문이 듦
     * 교육적인 관점에서는, 1옴 저항으로 이루어진 큐브의 반대쪽 꼭짓점 간 저항을 구하는 문제가 훨씬 더 직관과 회로 대칭성, 키르히호프의 전류 법칙 같은 개념을 배우기에 유용하다고 봄. 무한 격자는 수학적으로도 너무 멀게 느껴져서, 입문 과정에서 풀 만한 현실적인 문제 같지 않음
     * 단순 대칭성 해설 중심의 풀이에서 언제 ""플러스/마이너스 노드를 분리해서 각각 전류장을 고려할 수 있다""는 전제를 받아들여야 하는지 이해가 잘 안 됨. 양 노드 사이에 대칭성이 남아 있긴 한데, 처음처럼 모든 방향에 동일한 전류 흐름을 가정할 수는 없기 때문에 의문이 남음
          + 맥스웰 방정식이 전기장과 자기장에서 선형이기 때문에, 장과 퍼텐셜을 서로 더하고 빼는 것이 가능하다고 설명함. 이 원리가 바로 간섭(Interference)이나 광학 격자에서 작동하는 것과 같음
     * 학부 때 전기전자공학과 수업에서 이 문제가 나왔었는데, 정말 싫어했던 문제임. 교수들이 좋아하던 생각 실험이었음
          + 개인적으로 처음이자 마지막으로 본 게 1학년 말 기말시험의 4문제 중 첫 번째였음. 수업에서는 무한 사다리 저항 문제를 다뤄봤지만, 그걸 이 문제에 적용해야 한다는 점에서 처음엔 상당히 난이도가 높다고 느꼈음
     * 이 문제는 “시트 저항(sheet resistance)”의 이산적인 버전임. 모든 노드 쌍의 저항이 동일함. 옛날 EE 대학 커리큘럼에서 다뤘지만, 해답 도출 방법은 지금은 기억이 잘 안 남. (시트 저항 위키 참고)
     * Veritasium이 비슷한 주제로 빛이 지나가는 경로를 보여주는 멋진 영상을 올린 적 있었음. 내가 본 최고의 물리 데모라고 생각하는 부분의 타임스탬프 링크 첨부함: Veritasium 유튜브 데모
          + 사실 이 데모가 그렇게 인상적이라고 하기엔, ‘경로 추가 빛’이 실제로는 레이저의 회절로 설명되는 효과임. 어떤 경계도 유한하면 항상 회절이 발생하니, 결과적으로 “빛이 가능한 모든 경로로 간다”는 해석과 구분이 힘들어짐. 하지만 그게 실제로 다중 경로를 간다는 명제와 동일한 증거는 아님. 게다가 레이저 소자의 회절 특성 자체도 물리적 한계보다는 훨씬 떨어질 것임. 냉소적으로 보면, “레이저가 축에서 벗어난 광이 일부 있으니까 그런 것 아닌가?”라는 의문이 생길 수 있는데, 실제로 물리적으로 그게 맞음. 데모만으로 전부 설명할 수는 없는 한계가 있음
     * 대칭성과 중첩(superposition) 해설에서, 왜 인접 노드에 alpha-beta-alpha가 등장하고, alpha-alpha-alpha가 아닌 건지 잘 이해가 안 됨. 왜 한 방향만 구분되는지, 나머지는 같은 처리가 되는지 궁금함
          + 처음에는 인접 전류가 모두 다를 수 있다고 가정하고, i_1부터 i_12까지 지정해서 접근함. 그런데 도형이 수직축에 대해 대칭이니 접히는 위치에서 전류값이 서로 같아지는 경우들을 표기함. 수평축에 대해서도 마찬가지로 적용함. 90도 회전 대칭성도 찾아보고, 가능한 확인을 모두 적용함. 이렇게 하면 여러 i 값들이 자연스럽게 두 집단으로 모이게 되고, 이를 alpha, beta로 묶어서 설명할 수 있음. 추가로, 대칭성으로는 alpha와 beta 서로를 바꿀 수 없으니(즉, 성질이 다름), 이런 구분이 생기는 것임
     * 무한대로 확장하면, 결국 R = rl/A(저항률 * 길이/단면적) 공식과 똑같아짐. 그런데 길이(l)도 무한, 단면적(A)도 무한이므로, “무한/무한”이 되고 값이 정의되지 않음. 이런 ‘쓸데없는’ 문제 푸는 것보다는 더 유익한 일에 시간 쓰라고 하고 싶음
     * 이 문제는 1학년 EE 학생들이 배우는 하이패스 필터 문제로도 알려져 있음
"
"https://news.hada.io/topic?id=21484","Show GN: Voca Class: 나만의 TTS 단어장 앱 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Show GN: Voca Class: 나만의 TTS 단어장 앱

   안녕하세요! 저는 1인 개발자예요.

   이번에 만든 앱은, 유치원에 다니는 아들을 위해 시작했어요.

   아들이 매주 10단어씩 외우고 시험을 보는 모습을 보면서, 단어를 더 쉽고 재미있게 외울 수 있는 방법이 없을까 고민했어요.

   다른 친구들도 단어장을 직접 만들기 힘들어할 수 있다는 생각이 들었고, 그래서 단어장을 공유할 수 있는 기능을 넣었어요.
   또, 부모님이나 선생님이 클래스를 만들어서 주기적으로 단어장을 만들고, 아이들이 함께 학습할 수 있도록 했어요.

   아이들이 더 즐겁게 공부할 수 있도록 퀴즈 기능도 추가했고, 대량으로 단어를 입력할 수 있게 구글 스프레드시트 연동도 넣었어요.

   듀오링고 같이 학습 소재를 제공해주는 앱이 아니고, 내가 직접 만들어야 하는 점에서 귀찮고 한계는 있지만, 내가 원하는 단어를 학습하는 것에 일단 초점을 맞춘 서비스에요.

   의도치 않게 파생기능으로 TTS로 읽어주는 기능이다보니
   구구단 읽어주기, 어려운 용어 읽어주기, 레시피 읽어주기 등으로도 사용할 수 있어요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  서비스 소개

   단어장 만들기
   단어장 만들기는 대량 등록등 편의성을 최대로 고려했어요.
     * 구글 시트 연동: 구글 시트를 이용해서 대량의 단어를 한 번에 입력할 수 있어요. 정해진 포맷대로만 만들면 되요.
     * 직접 단어장 만들기: 원하는 단어만 입력하면, AI가 TTS(텍스트 음성 변환)로 단어와 뜻을 읽어줘요. 물론 모든 항목을 본인이 직접 입력도 가능해요.
     * AI 마법사로 추가: 자연어로, '여행에 관련된 단어를 추천해줘' 라고 하면 단어를 추천하고 뜻과 예문을 채울 수 있어요.

   학습과 퀴즈
     * 학습 모드: 한 단어씩 반복, 전체 단어 반복, 랜덤 모드 등 다양한 학습 방식을 지원해요.
     * 번역 기능: 단어와 뜻의 번역 기능이 내장되어 있어요.
     * 백그라운드 모드 지원: 백그라운드에서도 음성 학습이 가능해서, 음악 플레이어처럼 언제 어디서나 반복 학습이 가능해요.
     * 퀴즈 기능: 단어를 다 외웠는지 확인할 수 있는 퀴즈도 있어요.

   단어장 공유 및 편의기능
     * 단어장 공유: 내가 만든 단어장을 다른 사용자와 쉽게 공유할 수 있어요.
     * 단어장 병합: 여러 단어장을 하나로 합칠 수 있어요.

   클래스(학급) 기능
     * 클래스 기능: 선생님, 부모님, 친구 등 누구나 클래스를 만들 수 있고, 클래스에 단어장을 등록하면 가입한 모든 구성원이 학습할 수 있어요.
     * 클래스 퀴즈 현황: 학습한 후에 퀴즈를 풀면, 선생님에게 퀴즈 현황이 전달되요.
     * 실시간 퀴즈 배틀: 클래스에 참여한 사용자들은 실시간으로 퀴즈 배틀을 즐길 수 있어요.

   다국어 지원 및 학습 통계
     * 다양한 언어 학습: 영어뿐만 아니라 다양한 언어를 학습할 수 있어요.
     * 학습 통계: 꾸준히 학습할 수 있도록 학습 통계와 XP 랭킹 기능도 제공해요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  주요 기능

     * AI TTS 단어/뜻 읽기
     * 백그라운드 반복 학습
     * 구글 시트 연동 대량 단어 입력
     * 단어장 공유 및 병합
     * 클래스 생성 및 실시간 퀴즈 배틀
     * AI 단어 생성
     * 다국어 학습
     * 학습 통계 및 XP 랭킹
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  개발 후기

   기본 기능을 개발하는데는 얼마 안걸렸는데,

   TTS의 한계로 인해 보강작업이 오래 걸렸고.
   xp를 도입했는데 지급 기준을 부여하는게 까다로워서 어려웠네요.
   개발 시작한지 2달만에 그나마 만족할 만한 서비스가 된 것 같아요.

   계속 눈높이가 올라가서 중간에 개발을 멈추는게 힘들었어요.

   앞으로도 꾸준히 업데이트할 계획이니 많이 응원해주세요!

   와...대단하네요. 사업체에서 런칭한게 아니고 개인 서비스라니 역시 현업 분들은 ㅠㅠ

   바이브코딩으로 만들었어요. 생산성이 어마어마하네요!

   와 대박입니다. 마침 딱 이런 어플을 원했어요
   quizlet이 웹으로 꼭 들어가서 import해야한다는 것이 너무 불편했는데
   혹시 플래시 카드처럼 만드실 의향은 없나요?

   안녕하세요 안드로이드 버전도 플래시 카드와 쓰기 퀴즈가 반영된 버전이 배포가 되었어요~~

   https://play.google.com/store/apps/details?id=kr.co.tera.vocalet

   생각하신 기능이 맞는지 확인 부탁드려요^^

   ios 는 플래시 카드 학급과 쓰기 퀴즈가 반영된 버전이 스토어에 배포 되었어요.

   https://apps.apple.com/us/app/voca-class/id6744842612

   아 알아봐주셔서 감사합니다. 플래시 카드 가 뭘까요? 어떤 방식인지 말씀해주시면 반영해보겠습니다.

   Quizlet이나 Anki같은 앱입니다. 이렇게 만들어주신다면 진짜 돈주고 구매하고싶어요!ㅎㅎ

   플래시 카드의 어떤 부분이 매력적인지 핵심 기능을 말씀해주실수 있을까요?
   저도 그런것이 있다면 꼭 구현하고 싶습니다.

   ""플래시카드 학습법"" 검색해보시면 많은 사례들이 나옵니다.
   암기 위주의 학습에서 뛰어난 효과를 보여줘요

   ㅎㅎ 다 만들었습니다. 일단 제가 플래시 카드를 잘 몰라 급하게 만들었는데...
   심사 한번 올려보겠습니다.
   심사 끝나면 댓글 달게요^^

   아하! 학습할 때, 앞에 단어, 뒤에 뜻 과 같은 형태로 학습하면 되는 것일까요?

   이번주에할일생겼네요 ㅎㅎ

   단어에 대해 다양한 정보를 제공해줘서 흥미롭게 단어를 익힐 수 있는 것 같아 좋네요!

   AI기능은 chatgpt api를 사용하시는 걸까요 ??

   네 맞습니다. LLM의 API Key를 일단 제가 결제하고, 사용은 무료로 제공하고 있어요

   너무멋집니다 👍

   감사합니다~~ 정말 열심히 만들긴했는데, 의도가 충분히 UI/UX에 반영이 안된 것 같아서 고민이 많습니다~

   플러터로 만들어서 웹버전도 있긴하지만, 웹에서는 기능이 많이 제한되어 있습니다. 기왕이면 앱으로 봐주시면 감사하겠습니다~
"
"https://news.hada.io/topic?id=21443","메타, Scale AI에 20조원 투자하며 슈퍼인텔리전스 연구소 출범","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 메타, Scale AI에 20조원 투자하며 슈퍼인텔리전스 연구소 출범

     * 메타는 AI 경쟁력 강화를 위해 데이터 훈련 스타트업 Scale AI에 $14.3B(143억달러, 약 20조원) 를 투자하고, 이를 통해 슈퍼인텔리전스 연구소(Superintelligence Lab) 를 출범함
     * Scale AI CEO Alexandr Wang은 메타의 AI 부문 리더십에 합류하고 일부 팀원도 함께 이직할 예정임
     * 이번 투자는 메타 매출의 약 10% 에 해당하는 금액으로, WhatsApp 인수 이후 가장 큰 규모의 외부 지분 투자임
     * 독점 규제 우려를 피하기 위해 지분은 49%만 확보, Scale AI의 운영에는 직접 관여하지 않는 구조로 설계됨
     * 메타는 오픈소스 전략에도 불구하고 최신 모델 LLAMA4의 성능이 경쟁사 대비 뒤처진 상황에서 본격적인 반격을 시도하는 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Meta의 AI 전략 전환: Scale AI에 대규모 투자

     * 메타는 외부 기업에 대한 첫 대규모 소수 지분 투자로 Scale AI에 143억 달러를 투입함
     * 이 금액은 2024년 메타 총매출의 약 10% 에 해당하며, 메신저 앱 WhatsApp 인수(190억 달러) 이후 최대 규모의 투자임
     * Scale AI는 AI 학습용 데이터 구축에 특화된 스타트업으로, OpenAI 및 Anthropic과 유사한 생태계를 겨냥한 파트너십임

Alexandr Wang의 합류와 리더십 개편

     * Scale AI CEO Alexandr Wang은 메타에 합류, 새로운 AI 조직인 Superintelligence Lab의 수장 역할을 맡음
     * 그는 ""AI는 우리 시대 가장 혁명적인 기술 중 하나""라며 기술의 무한한 가능성과 파급력을 강조함
     * Scale AI는 전략 책임자였던 Jason Droege가 CEO로 승계하고, Wang은 이사회에 잔류함

규제 회피형 구조: 왜 소수 지분만 확보했는가

     * 메타는 기업 인수에 대한 규제 우려를 피하기 위해 지분을 소수만 보유하고, 경영 간섭은 최소화하는 방식으로 구조화함
     * 이전에 아마존의 Anthropic 투자, 마이크로소프트의 OpenAI 투자처럼 협력은 강화하면서도 M&A 규제를 피해가는 방식을 따름
     * 현재 FTC 위원장은 Andrew Ferguson으로 교체되었지만, 반독점 감시 기조는 지속되는 중

메타의 AI 경쟁력: LLAMA의 한계와 슈퍼인텔리전스 목표

     * 메타는 LLAMA 시리즈를 오픈소스로 공개하며 생태계 중심 전략을 펼쳤지만, 최신 모델 LLAMA4는 경쟁사 모델에 비해 기술력 열세 평가를 받고 있음
     * OpenAI, Google 등은 AGI(범용 인공지능) 를 목표로 하고 있으며, 메타는 이를 넘어서는 슈퍼인텔리전스(Superintelligence) 구축을 공식화함
     * Wang과 Scale AI의 합류는 이를 위한 데이터 품질 강화 및 내부 기술 재정비의 초석으로 해석됨

업계 전반의 변화와 메타의 대응

     * ChatGPT 출시에 뒤처졌던 메타는 이제 전문 인재 영입과 대형 투자를 통해 반격을 시도 중
     * Scale AI와의 협업을 통해 모델 훈련용 고품질 데이터 확보, 경쟁사와의 기술 격차 해소를 노림
     * 슈퍼인텔리전스라는 장기 목표는 향후 메타의 AI 전략 중심축으로 작용할 것으로 보임

        Hacker News 의견

     * archive.md 링크
     * Meta는 이미 두 개의 AI 랩을 운영 중인데, 두 곳 다 서로 경쟁 관계에 있고 둘 다 현재 상당히 위기에 처해있는 상황 설명. FAIR는 Rob Fergus가 이끌고 있는데, 전임 리더가 그만둬서 Fergus가 맡게 된 것임을 지적. 나머지는 AI 랩들이 인재를 유치하는 데 항상 성공적이라는 소문만 존재함. GenAI는 Ahmad Al-Dahle가 리드하고 있는데, 원래 RL/metaverse 분야에 있다가 GenAI로 넘어온 사람들이 대부분이고, 파리에서 LA 기반으로 조직이 이동하면서 정치적, 실질적으로 자원을 확보하려 했던 배경 설명. 이 팀은 공개 평가 과정에서 부정행위가 적발되어 Llama 4의 나머지 프로젝트가 취소되고 리더도 강등될 예정임을 언급. Meta는 OAI보다도 더 많은 연봉을 제시해 스타 인재들을 구하려 노력하지만 거의 성공하지 못하고 있음. Alexandr Wang에게 직접 보고하는 구조라 사람들이 참여를
       꺼린다는 의견. Koray(GDM), Mira(OAI) 등도 스카우트에 실패했고 지금 리더는 3순위였다는 루머, 최상위 연봉이 수천만 달러 수준이라는 소식 공유. 합류하는 유명 인사들도 대부분 주식이 베스팅될 때까지만 있다가 더 나은 연구소로 이동할 가능성 크다고 전망.
          + Meta가 어려움을 겪는 이유는 Microsoft가 과거 인재 이탈을 막지 못했던 사연과 유사성을 지적. 큰 돈과 스톡옵션을 줘도 OpenAI의 미래 성장성이 Meta보다 훨씬 높은 점을 강조
          + Zuck이 Sam Altman 스타일의 리더를 필요로 하는 이유를 설명. 뛰어난 랩, 리서처, GPU, 자본을 모두 가지고 있어도 그 이상의 무언가가 필요함을 언급. Llama 4는 괜찮지만 AI 경쟁에서 6~7위 수준임을 지적. 내부 정치 싸움에 너무 많은 에너지가 소모되고 있어서 변화가 필요하다는 의견
          + AI 기술의 발전 속도가 너무 빨라서 이 업계 인재들은 한때 큰 돈을 벌 수 있을 때 미리 벌어야 한다고 생각. AI 기술은 수명이 매우 짧다는 점을 강조
          + Rob Fergus가 FAIR의 창립자 중 한 명이고, 그가 리더를 맡는 게 자연스러운 선택이라고 설명
          + LeCunn도 언급이 빠졌다는 의견과 함께 요즘 LeCunn의 행보가 별로라는 평가
     * Scale이 최근 데이터 라벨링 툴 외에 어떤 것을 하는 회사인지 궁금함. Meta에 그렇게 흥미롭게 보일 만큼 무슨 경쟁력이 있는지 의문. CEO도 곧 퇴임 예정이라 Meta가 CEO에 만족해서 투자했다는 해석이 성립되지 않음을 지적
          + Scale은 대기업이 AI 모델 학습 시 활용하는 대규모 독점 데이터셋을 구축한 회사임. Meta, Google, OpenAI, Anthropic 등 주요 업체들이 Scale 데이터를 사용한다고 설명. 이번 투자로 Meta가 Scale 데이터를 독점적으로 활용하고 경쟁사엔 차단할 의도가 있을 것이라는 추정
          + 이 딜이 사실상 한 사람의 초고가 acquihire라는 이야기. Zuckerberg가 Alexandr Wang을 특정한 비전 공유자로 생각했다는 보도가 나옴. 비주류적 AI 미래 구상 때문이라는 설명
          + Alexandr Wang이 다른 AI 연구소들이 Scale을 어떤 용도로 활용하는지 정보를 Meta에 유출해 Meta가 추격할 수 있다는 루머
          + Meta-Scale의 결합이 미국 국방부(DOD)와 밀접한 보안/감시 목적에 기반을 두고 있을 가능성 지적. Scale은 미국 국방 분야, 현 정부, 걸프 국가들과 강한 연계가 있고, Wikipedia 내역에도 국방 프로젝트와 밀접히 연결된 성과가 나와 있다고 설명. 미 정부와 서구 정치 엘리트들은 GenAI가 야기할 사회 불안에 대한 우려가 있고, Meta와 Scale의 파트너십이 그런 불안감을 해소하는 방향이라는 해석
          + CEO가 떠난다는 것은 결국 Meta의 AI ‘슈퍼 인텔리전스’ 프로젝트에 합류한다는 의미라는 설명
     * 이번 딜이 상당히 이상하다고 느껴짐. Meta가 비지배 지분만 사고 고객한테 영향도 없다고 하지만 CEO 및 주요 인력이 Meta로 이동 중임. 경쟁사 데이터 접근은 없다고 해도 49% 지분이면 주요 투자자 권리는 대부분 누릴 수 있음. 사실상 acqui-kill 전략처럼 보인다는 의견
          + 팟캐스트 호스트 의견 인용. 이번 딜은 인수합병이지만 ‘투자’ 명분으로 포장해 규제 회피용임을 지적. CEO가 Meta에서 일하게 되고, 과반 바로 밑의 지분 구조가 특징이라고 소개
               o 관련 팟캐스트 링크
          + FTC 등 규제 당국의 추적을 피하기 위한 그럴듯한 부인 가능성을 만들려는 시도라는 의견
          + 리버스 인수합병 사례로 Pixar-Disney, Next-Apple을 들며, 새로운 스타트업 인재가 궁극적으로 모기업을 주도하는 시나리오 설명. 초기 스타트업 인수로 미래 잠재적 파괴자를 미리 흡수해 경영진 세대교체를 자연스럽게 하는 목적이라는 의견
          + 49% 인수가 정부 조사를 실제로 막을 수 있는지 의문
          + 이 딜은 OpenAI, Anthropic 등 다른 혁신 연구소에는 큰 영향력이 없을 것이며, 이들은 여러 데이터 벤더에 분산 의존 중임. OpenAI는 최근 Scale 데이터 의존도를 낮췄다는 Forbes 기사를 첨부
               o 관련 Forbes 기사
     * Scale AI는 프리랜서 플랫폼에서 과도하게 채용 공고를 올리고 있고 지금까지의 온라인 평판이 좋지 않다는 경험 공유. 레딧에서 실제 경험담을 보면 이 기업에 긍정적 평가를 찾기 어렵고 브랜드 지지자도 거의 없는 점을 지적
          + Uber, Doordash 등 플랫폼 기업들처럼 계약직 노동자에 대한 평판이 낮다는 설명. 사업 모델 자체가 노동 착취에 기반하고 있지만, 자본주의적 관점에선 그럼에도 비즈니스 가치는 높다는 의견
     * Scale 및 Alexandr Wang이 Meta에게 정말 그렇게 중요한 존재인지 이해가 안 된다는 질문. 결국 데이터 외에 무엇이 그렇게 독보적이고 귀중한지, Wang의 비전이나 통찰력이 그만큼 값어치가 있는 것인지 근본적으로 의문
          + Alexandr Wang 인터뷰를 보면 별다른 인사이트는 없고 유행하는 유행어만 나열한다고 평가
     * Matt Levine이 이번 딜을 acquihire로 해석하고, 투표권 없는 주식만 매수한 점은 규제 당국 심사를 피하기 위한 구조라고 분석
          + 143억 달러라는 금액이 단순 acquihire 치고는 과도하게 크며, 명확히 IP 또는 독점적 접근권이 관련된 것으로 봐야 한다는 의견
          + 비의결 지분만 산 것이 규제 회피 전략이라는 해석에 대하여, 미국 규제당국이 정말 그런 걸 믿는지 의문. 법의 취지를 고려해야 한다고 주장
     * 지금까지 Zuckerberg의 메타버스 투자 결정이 무모하지만 혁신에는 과감한 투자(capex)가 필요하다는 생각으로 인정해왔었음. 그러나 이번 딜은 이해가 잘 안 된다는 고백. Scale은 그저 필리핀 인력 관리를 깔끔하게 포장한 회사 정도로 평가. Meta 입장에서 얻을 수 있는 건 Alexandr Wang 한 명이라고 보는데, Wang은 Davos에서 했던 여러 공개 발언 등으로 볼 때 다소 가짜 느낌을 주는 인물이라는 점 강조. 결국 연구 지향도 아닌, 영업 위주의 인물을 데려와 GAI 추진력을 기대하는게 고개가 갸우뚱해지는 선택임. 게다가 이전에 Scale의 엉성한 데이터로 Meta 내부에 여러 문제와 이슈를 야기했던 전력이 있음. 주주 입장에서 최근 몇 년간의 Meta 전략 중 이번이 제일 실패한 딜로 보임
          + 이 의견에 완전히 공감하며 마치 내 생각을 누가 대신 써준 느낌이라는 평
          + ""공개적으로 이상한 소리 한 게 많다""는 표현에 공감한다는 반응
          + Sam Altman이 META에 있어 큰 리스크로 작용할 수 있다는 의견. Altman은 Zuckerberg와 비슷한 윤리관을 갖고 더 뛰어난 기술팀을 보유 중이라고 설명. OpenAI가 공격적으로 product를 밀면 Facebook, Instagram과 같은 핵심 서비스가 타격을 받을 수 있음. Wang이 그 리스크를 잘 헤쳐나갈 수 있는 인물이라는 평가
     * 28세 인재 한 명 영입을 위해 150억 달러 쓴다는 선택을 아주 대담한 시도로 봄. Zuck은 겁 없이 리스크를 감수하는 CEO라고 평가
          + 메타버스에도 연 50억 달러 이상을 투자하며 회사명도 바꿀 정도였지만 실제로 메타버스를 사용하는 사람은 거의 없음. 이러한 실패도 잘 극복하고 큰 리스크를 두려워 하지 않는 스타일로 해석
          + 그러나 이번 딜의 리스크는 제한적이라고 봄. 막대한 자본과 지배권을 가진 위치라서 수십억 낭비해도 큰 불이익이 있진 않다는 점 지적. 절실한 상황의 일반인과는 완전히 다른 의사결정 환경을 강조
          + 꼭 성공을 위한 ‘모험’이 아니라 경쟁사의 성공을 막기 위해 150억을 쓸 수도 있다고 주장. 목표는 승리가 아니라 ‘포지셔닝’이라는 해석. 이번 거래 자체가 자본 집행에 의미가 크고, 나머지는 내러티브일 뿐이라는 시각
     * 이번 딜은 Meta가 오큘러스/Carmack 인수 기록까지 넘는 역대급 acquihire라며 상당히 이례적이라고 평가
          + 방향이 갑자기 메타버스에서 AI로 완전히 선회한 모습에 대해 흥미로움을 표함. 결국 모두 빅테크들이 새로운 성장동력(Next Big Thing)에 집착하는 과정일 뿐이라고 보고, 이번 AI 투자는 잘 잡은 기회라고 본다는 긍정적 전망. 메타버스는 너무 명확하게 잘못된 투자였다는 자조적 평가
"
"https://news.hada.io/topic?id=21488","Luxe engine - 크로스플랫폼 게임엔진","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Luxe engine - 크로스플랫폼 게임엔진

     * Mac, Linux, Windows, Web 등 멀티 플랫폼을 지원하는 크로스플랫폼 신속 개발 게임 엔진
     * 초보자도 쉽게 2D와 3D 게임을 신속히 만들 수 있도록 설계
     * 2D 게임 개발에 최적화된 직관적 툴셋과 빠른 워크플로우를 제공하며, 3D 렌더링·셰이더·에셋 파이프라인 등 고급 기능도 지원
     * C++로 개발되어 높은 성능을 보장하고, 게임 개발은 커스텀 Wren 언어(추후 C 계열 언어 연동 지원 예정)로 진행
     * 모듈식 구조와 코드 중심 워크플로우, 선택적 에디터, 커뮤니티 중심 개발 등으로 확장성과 개인/팀 작업 모두에 적합
     * 필수 기능만을 핵심 툴로 제공, 엔진 비대화 없이 프로젝트 특성에 맞는 맞춤형 게임 개발이 용이함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

엔진 개요 및 철학

     * luxe는 쉽고 빠른 게임 개발을 위한 크로스플랫폼 엔진(Mac, Linux, Windows, Web 지원, 콘솔도 개발 중)
     * 2D 게임 개발을 최우선으로 설계되어, 1시간짜리 소규모 프로젝트부터 수년 단위의 대형 프로젝트까지 폭넓게 활용할 수 있는 강력한 2D 툴셋 제공
     * 2D 외에도 강력하고 접근성 높은 하드웨어 기반 렌더러를 통해 셰이더, 애셋 파이프라인, 렌더 경로 등 다양한 3D 기능 지원 가능
     * 빠른 반복, 직관적 워크플로우, 모듈식 툴셋으로 아이디어 실현에 최적화
     * 엔진은 c++로 작성되었으며, 게임 개발자는 기본적으로 커스텀 버전의 Wren 언어를 사용함. 이후에는 C와 상호 운용 가능한 다양한 언어 확장 예정

개발 스튜디오와 실전 활용

     Luxe는 엔진 개발진이 직접 게임 개발에 사용하는 엔진임

     * 엔진 개발팀이 직접 게임 스튜디오를 운영, 실제 게임 제작에 바로 활용
     * Mossfield Origins와 Mossfield Archives 등 실제 프로젝트에서 활용 중임
     * 게임 커뮤니티와 개발자, 아티스트, 디자이너를 지속적으로 지원하고, 다양성과 지속 가능성에 중점을 둔 스튜디오 철학이 반영

개발 및 커뮤니티

     * 현재 프리뷰 버전 공개 중, 주요 업데이트·개발 기록 블로그 제공
     * 개발 소식, 커뮤니티 포털, 공식 Discord 참여 가능
     * Luxe 엔진의 출시 및 주요 개발 소식은 공식 홈페이지와 커뮤니티, 디스코드에서 안내
          + dev log #11 - modifiers
          + dev log #12 - custom modifiers
          + dev log #13 - scenes
          + dev log #14 - projects
          + dev log #15 - try luxe now!

설계 철학

     * 모듈형 구조: 핵심 엔진이 작고 짜임새 있게 설계되어 있으며, 모듈 시스템을 통해 필요에 맞는 툴만 활용 가능
     * 유연한 워크플로우: 빠른 반복 및 의도 표현을 중심 원칙으로 하여, 개발 워크플로우의 효율에 집중함
     * 사용자 중심: 게임을 직접 만드는 개발자들이 실사용 관점에서 설계. 디자이너, 개발자, 아티스트 등 일상적으로 사용하는 이들을 고려한 사용자 경험을 중시함

워크플로우와 에디터

     Luxe 에디터는 선택 사항임

     * Luxe는 솔로 개발자와 팀 모두를 지원하며, 코드 기반 워크플로우와 에디터 및 도구를 병행해서 사용 가능
     * 에디터는 특정 게임 장르별로 손쉽게 맞춤화하거나 모듈을 통해 공유 및 확장 가능
     * 애니메이션, UI, 게임 월드 구축 등 다양한 목적에 활용 가능한 도구 제공

툴과 시스템 중심 구조

     * 게임 개발에 필요한 기능과 도구를 명확히 구분하고, 각 게임의 특성에 맞게 필요한 부분만 선택적으로 구성 가능
     * 엔진 자체가 거대하지 않고, 필요한 모듈만 불러와 게임의 정확성과 적응성을 높임
     * 높고 낮은 수준의 시스템을 조합해 게임을 완성하는 툴박스 방식 적용
     * 2D 플랫폼 게임, 3D FPS 등 다양한 프로젝트 타입에 맞춤형 워크플로우를 신속하게 구성할 수 있도록 아웃라인(Outlines) 기능 제공
     * 빠르게 게임에 필요한 요소를 조합하여 바로 시작할 수 있음

     필요한 만큼만 경량하게 사용, 불필요한 기능은 배제

모두를 위한 렌더링

     * 쉽고 유연한 렌더러: 게임별로 적합한 렌더링 스타일을 자유롭게 선택하고, 고수준의 상호작용 구현
     * 다양한 플랫폼 백엔드와 새로운 전용 셰이딩 언어, 스크립트 기반 렌더 파이프라인 지원
     * 초보자도 쉽게 렌더링 구조를 익히고 빠르게 시도 가능

모듈식 확장성

     모듈 시스템을 중심으로 설계

     * 기본적으로 풍부한 시스템 세트를 제공하나, 모든 게임 장르/기능을 엔진에 내장하지 않음으로써 가벼움을 유지함
     * 필요한 기능이 제공되지 않을 경우를 대비해 모듈로 도구와 시스템을 확장 가능
     * 모든 API·시스템이 모듈로 구성:
          + Luxe API 자체도 모듈로 제공되는 등, 모듈 구조가 핵심적 역할을 수행함
          + 엔진 핵심도 모듈로 배포, 불필요한 기능 없이 최적화
          + 다른 프로그래밍 언어 활용도 모듈을 통해 지원

        Hacker News 의견

     * 몇 달 전에 이 엔진을 잠깐 테스트해 본 경험자 입장으로, 흥미로운 프로젝트라고 생각하지만 저에게 딱 맞진 않는 느낌. Wren과 비슷하게 구조적이고 장황한 부분이 많아 고수준 언어에서 원하는 자유로움, 저수준 언어에서 기대하는 유연함을 둘 다 완전히 채워주지 못하는 인상. 사용 편의성은 Godot, 세밀한 제어는 Raylib 쪽 선호. 참고로 저는 소규모 1인 개발자이자 취미로 게임을 만드는 입장이고, Luxe는 스튜디오 워크플로우에 더 적합해 보임. 특히 아티스트 중심 툴링에 많은 비중을 둔 점이 인상적. 신규 오픈소스 엔진으로 Godot와 비교가 되긴 하지만 사실 Unreal과 경쟁 구도에 더 가까움. 아직 알파 단계라 문서와 예제가 최소화되어 배우기 어렵게 느껴졌지만, 블로그 포스트는 자세한 설명이 많아 추천하고 싶은 자료
     * 이 엔진을 꽤 오래 사용해 본 경험자로서, 여러 장점이 마음에 든다는 의견.
          + 스크립팅이 정말 인체공학적이라 빠른 작업이 가능하고, 더 빠른 성능이 필요할 때 wren의 네이티브 확장 모듈로 전환도 쉬움.
          + wren의 fibers(협동 쓰레딩)는 진짜로 게임 로직 처리(NPC 상태관리, 게임 AI 등)에 매우 적합
          + 그래픽 및 렌더 모듈이 극도로 설정 가능. 스크립트가 빠른 c++ 실행 그래프를 셋업하며, 직접 수정·스크립트화 가능
          + 에디터 등 툴이 정말 신경써서 만들어짐. 주로 코드로 작업하지만 레벨 디자인 때 정말 유용
          + 다양한 규모의 프로젝트에 모두 잘 맞는 느낌. 프로젝트 파일, 몇 개의 설정, 스크립트만으로도 가능하고, 대규모 구조도 지원하며, 에디터가 좋은 프로젝트 구조를 권장
          + 드로잉이 정말 유연. 스프라이트, 셰이프, 메시, 타일 등은 물론, Unity의 Shapes 익스텐션과 유사한 ""즉시 스타일(immediate style)"" 드로잉 API가 내장되어 있어 퀄리티가 매우 높음
          + “Modifiers”라는 ECS-like 컴포넌트 방식은 익숙해지기까지 약간 시간이 걸리지만, 익히면 더 나은 방식이라는 느낌. 완전 선택적이라 아직 배우는 단계라면 안 써도 무방
          + 코드를 제외한 대부분의 데이터가 json과 유사한 .lx 파일 형식에 저장되어 디버깅과 이해에 좋음. 스크립트에서 자동화도 쉽게 가능
          + 기능 추가나 버그 픽스가 매우 안전하게 진행되어 기존 코드가 쉽게 깨지지 않으며, 명확한 마이그레이션 전략도 제시됨
            전반적으로 소규모 팀, 실험적 워크플로우에 잘 맞는 엔진이란 느낌. Unity 대안을 찾는 사람에겐 꼭 추천
     * 이 엔진이 Wren(밥 니스트롬이 만든 언어)을 스크립트로 사용하며, 수년 간 개발이 이어진 점이 눈에 띈다는 의견
     * “제약 사항” 항목을 봤을 때 FOSS(완전 오픈소스 소프트웨어)화는 현실적으로 불가능해 보이고, 오픈소스 여부가 중요하지 않다면 Unreal이나 Unity에 비해 뚜렷한 장점은 없다는 생각
          + 이 엔진은 FOSS임을 내세우지 않으며, 다른 상용 엔진들처럼 수익 발생 시 비용을 지불해야 함. 하지만 “전혀” 장점이 없다는 건 너무 단정적이라는 느낌. 소규모 게임 개발에 특화된 워크플로우를 목표로 하고 있고, 비전문가를 위한 스타일라이즈 렌더링 지원 등 차별점이 존재. 게다가 가격 정책도 Unity의 10%, Unity마저 Unreal의 10% 수준이라는 점이 매력 포인트
     * 이 엔진이 완전히 여성 개발팀에 의해 만들어진 사실이 멋지게 느껴짐. 앞으로 게임 업계가 여성·소수자 리드 개발/스튜디오 중심으로 더 커지길 바라는 마음
     * luxe가 “c++로 작성됨”이라는 설명에 대해, 처음엔 Haxe로 만들어진 것으로 기억하는데 혹시 착각인지 확인
          + 찾아보니 예전 버전이 Haxe 기반으로 개발됐던 기록이 존재 이전 haxe 엔진. 2015~2016년 알파 버전은 현재 엔진과 별개이며, 공식 사이트에서 새로운 엔진을 확인 가능. 옛 버전은 오픈소스였지만, 지금은 공식 레포지토리에 문서만 남아있음
          + Armory라는 게임 엔진과 혼동한 것일 수도 있다는 의견
     * 2018년에 Hacker News에 올라온 쓰레드 언급
          + 예전엔 오픈소스로 공개할 계획이 있었다는 점이 흥미롭게 느껴짐. 하지만 현재는 끝내 프라이빗(폐쇄형) 구조를 유지하고 있다는 점이 인상적
     * Sillysoft의 Lux 게임과는 관련이 없다는 점(철자만 비슷)
     * 엔진 개발자 스튜디오에서 진행 중인 게임 두 개가 언급되긴 하지만, 실제로 이 프레임워크로 출시된 게임이 있는지는 궁금함
     * 비디오 게임 개발자가 되고 싶은 마음이 간절하지만 현실적으로 어렵다는 아쉬움
"
"https://news.hada.io/topic?id=21460","Show GN: KuView: 웹기반 실시간 쿠버네티스 대시보드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: KuView: 웹기반 실시간 쿠버네티스 대시보드

   저는 개인적으로 쿠버네티스를 꽤 좋아하는 편 입니다만, 그래도 몇가지 아쉬운점이 있다면 추상화가 너무 잘 된 나머지 실제 물리적인 요소들이 감춰져 확인하기가 어렵다는 점 입니다.

   예를들면
     * 어떤 Pod 에 장애가 나고 있는 상황에서 같은 노드에 배포된 다른 Pod 들의 상태는 어떤가
     * 현재 Service 에 연결된 Pod 들은 전부 정상적으로 작동하고 있는가?
     * 현재 노드의 CPU, Memory 사용량은 어떻게 되는가? 그 중 개별 Pod 들의 비중은 어떻게 되는가?
     * 현재 노드에 연결된 PV 들의 리스트는?

   물론 정보가 아주 없는건 아니라서, 하나하나 kubectl 조합과 Prometheus 등의 모니터링 툴을 통해서 시각화 하는 방법이 있긴 합니다만 상당히 번거로운것도 사실입니다.

   그런 상황에 도움이 되고자 적당히 하나 만들어본 웹기반 실시간 쿠버네티스 대시보드 입니다. 별도로 무언가 설치 할 필요없이 kubectl proxy 명령어만 사용 가능하면 WASM 형태로 웹브라우져 안에서 Kubernetes 의 모든 리소스를 WATCH 하는 형태로 작동합니다.

   Running / Terminating 숫자가 1초도 아니고 0.00x초 단위로 변경되는것같은데 어떤 원리로 계속 변경되는 걸까요? 계속 k8s API를 찌르는걸까요?

   사용하고 싶은데, k8s API Read Request에 엄청난 부하를 주는 것은 아닌지 걱정이 살짝 되어서 물어봐봅니다!

   K8s 의 WATCH API 를 사용합니다.
   https://kubernetes.io/docs/reference/…

   변경사항만 protobuf, SSE 로 받아오는건지라 꽤 효율적이고 부하는 미미한 수준입니다. (kubelet 이 kube apiserver 에 가하는 정도의 부하 수준입니다)

   다만 여러명이 동시에 사용하는거면 wasm 보다는 서버모드를 추천합니다. 서버가 대신해서 요청 받아와 인메모리에 들고있는걸 주는건지라 kube apiserver 부하가 적어집니다.

   WASM 파일이 90mb 정도로 제법 크긴 하네요.

   크기가 크긴한데 엔트로피가 높은것 같지는 않습니다. 현재 curl 로 다운받을때 gzip 된게 용량이 14MB 정도밖에 안됩니다. 실제 WASM 을 서빙할때도 요즘 웬만해서는 gzip, zstd, brotoli 같은 인코딩 알고리즘들이 적용되는데 실제 전송되는 트래픽은 높지 않을것으로 예상되긴 합니다.

   오 바이너리가 zstd로 압축했을때도 궁금합니다.

   조금 다른 얘기인데, WASM 으로의 변환 및 사용은 매끄러웠는지 (불편함이 없으셨는지) 궁금합니다!

   WASM 으로 먼저 대충 만들고 나중에 공통로직만 묶어서 Server 쪽 코드를 나중에 따로 뺀 쪽이라서 딱히 불편함은 없었습니다. 오히려 지금은 코드 대충 수정해도 Server, WASM 양쪽 모두 적용되고 있어서 나름 만족하면서 사용중입니다. ㅎㅎ
"
"https://news.hada.io/topic?id=21476","HDMI 더미 플러그의 EDID를 Raspberry Pi로 수정하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 HDMI 더미 플러그의 EDID를 Raspberry Pi로 수정하기

     * HDMI 더미 플러그의 EDID를 Raspberry Pi를 활용해 쉽게 변경할 수 있는 방법 소개
     * 더미 플러그는 실제 출력 장치 없이 기기에 모니터가 연결된 것처럼 인식시키는 용도임
     * EDID 정보를 1080p 캡처 장치의 것과 동일하게 복사해, 플러그가 4K 모니터로 표시되지 않도록 설정 가능함
     * Raspberry Pi의 I2C 컨트롤러와 표준 리눅스 도구만으로 플러그의 EEPROM을 읽고 쓸 수 있음
     * 모든 과정에서 장치 손상 예방을 위해 올바른 I2C 버스 선택과 백업 필수임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

더미 플러그와 EDID 개요

     * 더미 플러그란 HDMI 또는 DVI 포트에 연결했을 때 실제 영상 처리는 없고, 최소 회로만으로 기기가 모니터 연결을 감지하게 만드는 작은 동글 형태의 장치임
     * 모니터의 EDID(Extended Display Identification Data) 를 모방한 EEPROM 칩과 +5V에 연결된 Pull-up 저항 등 회로가 존재함
     * 헤드리스 서버, 무인 기기 등에서 운영체제(OS)가 디스플레이가 존재한다고 판단하도록 사용하는 데 유용함

목적과 접근 방식

     * 기존의 4K 해상도를 지원하는 HDMI 더미 플러그를 단순한 1080p 장치로 인식되도록 EDID를 바꾸고 싶었던 경험 공유
     * 목표는 더미 플러그 내부 EDID를 HDMI 캡처 장치(1080p 지원)의 EDID 정보와 동일하게 되도록 교체하는 것임
     * 더미 플러그의 EEPROM에 쓰기가 가능한지 확실하지 않았으나 시도 가치가 있었음
     * Raspberry Pi Zero의 HDMI 포트가 I2C 컨트롤러에 연결돼 있어 접근 용이함

안전 주의 및 절차 시작

     * 실제 모니터가 연결된 상태에서 이런 과정을 수행할 경우 EDID 보호가 없는 모니터는 손상 위험 존재함
     * 반드시 더미 플러그와 같이 손상되어도 괜찮은 장치에만 작업 진행 필요
     * 또한, 올바른 I2C 버스 사용, 그리고 쓰기 작업 전에 반드시 EDID가 맞는지 사전 읽기와 검증 필수임

환경 설정 및 준비 작업

     * Raspberry Pi OS Lite 설치 후, sudo raspi-config로 설정 조정
     * sudo apt install i2c-tools로 I2C 도구 설치 (Pi Zero의 경우 네트워크 필요, USB-이더넷 어댑터 또는 SD카드 chroot로 우회 가능)
     * HDMI-to-Mini-HDMI 어댑터 사용 필요함

EDID EEPROM 인식 및 백업

     * Raspberry Pi Zero의 경우 I2C 버스 2 사용 (다른 Pi 모델은 번호 상이)
     * i2cdetect 명령으로 0x50 주소에 장치 인식 확인, 이는 EDID EEPROM의 표준 주소임
     * 특이하게도 0x51~0x57 주소도 응답하며 EDID의 복수가 저장되어 있는 형태임
     * get-edid로 기존 더미 플러그의 원본 EDID 백업 수행, 두 번 읽어 비교해 일치성 검증
     * od -v -An -txC로 EDID를 16진수 배열로 출력, edidreader.com으로 유효성 검사

캡처 장치의 EDID 추출 및 플러그에 기록

     * 더미 플러그 분리 후 HDMI 캡처 장치를 Pi에 연결
     * 동일 방식으로 캡처 장치의 EDID 추출, 유효성 재확인
     * 다시 더미 플러그를 연결하고, 캡처 장치 EDID를 EEPROM에 작성
     * 각 바이트별로 i2cset 명령을 통해 쓰기 작업, 이는 표준 리눅스 툴과 bash만으로 가능

최종 검증 및 결과

     * 작업이 끝난 후 더미 플러그의 EDID를 재추출하고 원본 파일과 diff로 비교해 내용 일치 확인
     * 테스트 컴퓨터에 연결 시, 원래의 4K 모니터가 아니라 HDMI 캡처 장치로 인식됨
     * 성공적으로 더미 플러그의 EDID 교체 완료

마무리 및 활용 조언

     * 동일한 절차로 구형 1080p 더미 플러그를 4K 지원 장치로 변경 가능함
     * I2C 쓰기 작업은 Raspberry Pi에서만 수행 권장, 일반 PC에서 직접 진행시 하드웨어 손상 위험 있음
     * 관련 기능이 필요한 경우 이 절차가 유용할 수 있음

        Hacker News 의견

     * 집에서 시도하고 싶은 분들을 위해 작은 팁 하나 공유하고 싶음, 저렴한 dummy plug들은 대부분 256 바이트 EEPROM만 있어서, 고해상도·고주사율이 필요한 EDID 확장 블록까지 모두 저장하기에는 용량이 부족함, 1080p60까지만 시뮬레이션 가능함, 예를 들어 4k240 모니터를 흉내 내는 일은 불가능함. 그리고 어떤 제품은 write-protect 라인이 이미 연결되어 있어서, 직접 납땜 등 물리적으로 건드려야 데이터 쓰기가 가능함
     * 이 dummy plug들은 HDCP 처리가 안 되는 점이 단점임, 헤드리스 머신에서 해상도를 강제로 출력하게 하는 용도에는 훌륭하지만, HDCP가 필요한 스트리밍 서비스 테스트에는 쓸 수 없음. 혹시 HDCP까지 협상 가능한 HDMI dummy plug 솔루션을 알고 있는 분 계신가요? 매번 TV를 테스트 장비로 쓰기 번거로움. 내가 발견한 하나의 해법은 HDMI 멀티뷰어인데, 각 포트마다 HDCP를 개별적으로 협상함
          + 나는 HDMI 스플리터를 사용 중임, 사전 프로그램된 EDID를 설정하거나 HDMI output 1에 연결된 모니터에서 EDID를 배울 수 있고, splitter만 전원에 연결되어 있으면 실제로 모니터를 연결하지 않아도 연결된 것처럼 동작함. splitter가 PC나 콘솔과 HDCP를 협상한 뒤, 실제 모니터 쪽으로는 HDCP 없이 신호를 전달함 amazon.com 참고
          + Aliexpress에서는 HDCP를 종료하고 HDMI를 패스스루하는 기기를 판매한다는 광고도 많음, 구매 전 유의해야 함
          + HDCP 해제는 쉽지 않음, HDCP 1.4로 다운그레이드하고 1.4 규격을 지원하는 ‘compliant’ 장치를 연결해 dummy monitor로 쓰는 방식임, HDCP 1.4 이상이 필요하면 불가능에 가까움
          + HDMI 출력 임베디드 시스템이 있는데, 부팅 화면을 다른 HDMI 스트림(정적인 이미지도 괜찮음)으로 바꾸고 싶음, 임베디드 시스템 쪽은 절대 건드릴 수 없음. 저렴하고 견고하게 HDMI 신호만 바꾸는 방법이 필요함
          + Amazon에서 사실상 “HDCP 스트리퍼”로 광고되는 HDMI 스플리터를 사용해보길 추천함
     * 혹시 EDID 바이너리 파일 모음을 제공하는 곳이나, 편하게 만드는 프로그램이 있는지 궁금함. 나는 프로그래머블 EDID 에뮬레이터 플러그를 쓰는데, 특정 해상도나 세부 기능 (예: DSC가 포함된 8K 해상도 등)을 직접 세팅하는 게 어렵거나 불가능함. github.com/bsdhw/EDID는 신형 모니터 자료가 부족함. 내가 AnalogWay EDID Editor로 직접 만들어보기도 했는데, 지원 모드의 미묘한 차이나 우선순위 지정 등 세세한 세팅 과정이 쉽지 않음
          + 비슷한 고민을 경험함, Dolby TrueHD까지 지원하는 저렴한 5.1ch 사운드바를 샀는데, HDMI 연결은 eArc 지원 디바이스(최신 TV)에서만 작동함. PC를 연결하면 SPIDF나 aux만 써야 해서 음질 저하가 생김. 오디오 익스트랙터/스플리터 대신 PC의 edid 값을 사운드바가 eArc 디바이스로 인식하도록 조작하는 방법을 시도 중임, 아직 엄격한 가이드라인이 없는 듯 함
     * 패스쓰루 기능이 있는 dummy plug도 구입 가능함, 구형 시스템에서 고해상도 모니터와의 호환성 문제가 있을 때 유용함. 예를 들어 내 2011년 AMD FX8350 시스템은 4K 출력에 문제가 있어 1080p로 강제하려고 플러그를 인라인으로 꽂으면, 모니터가 자동으로 2x 업스케일 해서 깔끔하게 4K로 표시함
          + 패스쓰루 장치 몇 개도 가지고 있음, 글에 이런 옵션도 언급했어야 했음. 내 제품은 좀 특이해서 모니터의 EDID를 읽고 저장한 후, 다른 모니터에 오버라이드로 적용 가능함. 또 하나 신기한 점은, 모니터가 항상 연결된 것처럼 인식하게 강제할 수 있음. 내 모니터 중 하나는 전원을 끄면 가상으로 플러그가 분리된 상태가 되어 문제를 일으키는데, 패스쓰루 장치로 완벽하게 해결함. 내가 쓰는 제품은 THWT사의 HD-EWB임
     * 일반 모니터나 노트북 화면에 저장된 edid 정보도 이 방법으로 수정 가능함. TCON의 여러 설정도 기타 i2c 주소에 쓰기 방식으로 변경 가능함. Raspberry Pi도 필요 없음, 어떤 컴퓨터든 사용 가능함
          + 글의 저자는 Pi를 추천하긴 하지만 필수는 아님, 만약 PC에서 그대로 따라 하면 EDID가 아닌 하드웨어 예컨대 램 모듈의 SPD EEPROM 등을 실수로 플래시 할 수도 있음
          + 플래시 칩에는 쓰기 활성/비활성 핀이 따로 있고, 대부분의 모니터나 TV는 edid 쓰기를 차단하도록 배선되어 있음, 저가형만 그냥 놔두는 경우가 많다고 추정함. 무방비라면 읽기 중 전압 노이즈만으로도 쓰기 동작이 일어나 플래시가 날아갈 위험이 있음
          + 대부분의 모니터에서 edid를 수정할 수 있다는 건 하드웨어 개발사의 허점을 의미함. 보통 미리 프로그램된 EEPROM을 받아서 쓰기 핀을 아예 high로 두지 않는 게 관례임. 굳이 writable로 출하하는 건 흔치 않은 설계임, 하지만 실제 현장에선 예상 밖의 사례도 종종 있음
     * 왜 dummy plug가 필요한지 궁금함? 소프트웨어로 해결할 수 없는 무엇이 있어서인지 알고 싶음, 난 소프트웨어로 18개의 가상 디스플레이를 아무 문제 없이 사용함
          + 한 가지 사례는 내 PC에서 Looking Glass라는 소프트웨어로 Windows 가상 머신을 다룰 때임. GPU가 두 개(A MD와 NVidia)가 있고, NVidia는 Windows VM에 패스쓰루함. Looking Glass로 NVidia GPU 출력을 데스크탑 창에 표시해서, VM 내 Windows 프로그램을 성능 저하 없이 쓸 수 있음(Windows 7 이후로 GPU 가속 없인 사용이 힘들어짐). 단, NVidia GPU는 실제 디스플레이가 연결되어 있어야 동작함. Quadro GPU는 모니터의 EDID 파일을 덤프해서 항상 연결된 것처럼 쓸 수 있지만, 일반 소비자용 GPU는 이 기능이 안 됨. 그럴 때 dummy plug가 유일한 대안임
          + OS / GPU / 드라이버 조합에 따라 가상 디스플레이 세팅 자유도가 크게 다름. OBS, Steam/Parsec 게임 스트리밍용 디스플레이 추가에는 dummy plug가 훨씬 간편함. 리눅스+Xorg+오픈소스 드라이버나 Windows+Nvidia에서는 될 때도 있지만, MacOS 혹은 Windows+AMD/Intel GPU에서는 거의 작동하지 않음
          + 크롬박스에 윈도우/리눅스를 모드해서 쓰는데, HDMI 포트에 비디오 장치가 없으면 아예 부팅이 안 됨. dummy plug 없으면 불가피하게 막힘
          + dummy plug는 일반인에게 훨씬 쉽고 간편함. 소프트웨어만으로 4K 가상 모니터를 원격 게임 스트리밍 용도로 세팅하는 것은 생각보다 엄청 복잡함 4k-sunshine 설정기 참고
          + 라즈베리파이 원격 데스크탑에는 물리적으로 모니터가 연결되어 있어야만 데스크탑이 렌더링됨, 예산과 시간이 부족한 대학원생에겐 dummy plug가 최고임
     * KVM 및 리눅스 환경에서 문제를 해결할 목적으로 쓸만한 저렴한 DisplayPort EDID 에뮬레이터가 있을지 궁금함. HDMI 버전에 비해 가격이 훨씬 비싸서 차라리 KVM 새로 사는 게 나은 수준임
          + DisplayPort는 단순히 I2C 버스의 EEPROM을 쓰는 게 아니라, DisplayPort 전용 AUX 버스를 이용하는데, 이게 훨씬 복잡한 구조임. 공개 문서도 찾기 힘들고, 제대로 된 레퍼런스를 얻으려면 VESA 가입과 NDA 서명 절차가 필요함
     * USB ibus2 plug의 hex dump 결과가 EDID와 이어져 있음
"
"https://news.hada.io/topic?id=21540","미국, 외국인 학생 비자 심사 시 소셜미디어 공개 요구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     미국, 외국인 학생 비자 심사 시 소셜미디어 공개 요구

     * 미국 국무부는 앞으로 학생 및 교환 비자 신청자에게 소셜미디어 프로필을 공개하도록 요구함
     * 공개하지 않으면 의심 대상이 되어 비자 발급이 거부될 수 있음
     * 미국 외교관들은 온라인 활동을 검토해 미국 시민, 문화, 정부 등에 대한 적대성 징후를 확인함
     * 외국 테러 집단 지원이나 반유대주의 활동 여부도 추가로 심사 사항에 포함됨
     * 이 조치는 학생(F, M, J 비자) 및 교환 방문자 등 교육·문화 분야 비자 신청자에게 적용됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 국무부, 외국인 학생 비자 심사 시 소셜미디어 강제 공개 방침 발표

     * 미국 국무부는 앞으로 교육 및 교환 비자(F, M, J 비자) 신청자에게 본인 소셜미디어 프로필을 공개하도록 요구하는 새로운 지침을 발표함
     * 이를 통해 미국 외교관들은 신청자의 온라인 활동 내역을 심사하고, 활동 내역을 공개하지 않는 경우 신청자가 뭔가를 숨기고 있다는 의혹을 받게 됨

소셜미디어 심사 강도 및 목적

     * 새 지침에 따르면 외교관들은 온라인 공간에서 미국의 시민, 문화, 정부, 제도, 건국 이념에 대한 적대적 신호를 찾기 위한 심사를 진행함
     * 별도의 문서에서는 외국 테러집단에 대한 옹호, 지원, 그리고 불법적 반유대주의적 괴롭힘 또는 폭력 조장 등의 행위도 심사 기준에 포함되어 있음

최근 논란과 비판

     * 이 같은 반유대주의 관련 심사 기준은 이스라엘-가자전쟁 반대 의견 단속이라는 비판도 존재함
     * 이러한 기준은 미국 국토안보부 산하 이민국에서도 동일하게 적용 중임

대상 비자 및 배경

     * 심사 대상은 학업, 직업 교육 및 문화교류(F, M, J 비자) 로 한정
     * 국가안보를 우선시한다는 입장 하에, 신청자는 비자 심사 시 개인정보 공개를 요구받게 됨

배경 및 트럼프 행정부의 입장

     * 한 고위 관계자에 따르면 이러한 조치는 미국과 대학의 안전을 높이고 대미 위협을 차단하는 21세기형 정책이라고 설명됨
     * 최근 트럼프 행정부는 소셜미디어 심사 전략을 검토하며 신규 교육 비자 발급을 잠정 중단한 바 있음
     * 특히 중국 유학생에 대해서는 미·중 무역갈등 및 희토류 분쟁 속에서 특별 심사를 진행 중임

비자 심사 절차 강화

     * 국무부 지침에 따라 외교관들은 인터뷰 일정 재개와 동시에 모든 F, M, J 비자 신청자에 대해 포괄적이고 철저한 신원 확인 절차를 시행함
     * 이를 위해 모든 지원자는 모든 소셜미디어 계정의 프라이버시 설정을 공개(‘public’)로 변경해야 함
     * 이로써 모든 방문자에 대한 보안 심사 강화 목적을 달성하려는 정책임

        Hacker News 의견

     * DHS가 소셜 미디어 활동을 모니터링한다고 공식 발표한 링크 공유, 미국 정부보다 외국 정부를 비판할 자유가 더 제한적이라는 점과 광범위하고 이스라엘 비판을 검열할 목적의 정의 사용 지적
          + 이 예시들이 얼마나 광범위한지 놀라움, 특히 “유대인 시민이 자기 국가보다 이스라엘 혹은 유대인 우선순위에 더 충성”한다는 비판이 문제 된다는 부분 주목, 실제로 일부 이중국적자는 이스라엘을 우선시한다고 내세우곤 함, 예시 문서에서는 “유대인 전체”를 언급하는 부분도 있으나, 이 조항은 개별인을 대상으로 한 것으로 해석될 수 있다고 봄, 이스라엘 이중국적자는 자기 국가에 해당돼 이 예시에서 제외된다는 여지 가능성 언급
     * 구체적 지침에 대한 정보 제공에 감사 표시, 학생 소셜 미디어 프로필을 강제로 공개하도록 한다는 명확한 근거 못 찾았음, Politico 기사도 WSJ 기사와 함께 읽었으나 프로필 공개 의무는 언급 안 됨, Politico 기사 링크 공유
     * 이스라엘에 대한 비판이 다른 국가와 같은 수준이라면 반유대주의로 간주할 수 없다는 점 인용
     * 이 모든 조치의 본질이 항상 이스라엘에 관한 문제라는 주장, 틱톡 금지부터 10대용 소셜 금지까지 다 연결되는 현상 강조, 미국의 자국 이익 전쟁에 젊은 세대가 이스라엘의 비인간적 행동을 직접 보면 문제라는 우려, 중국과는 아무 관련 없다는 주장, 이는 명백히 국가 안보 문제라고 평가
     * 이스라엘 관련 예시 중 4가지, 즉 (1) 유대인 시민 충성 문제, (2) 유대인 민족의 자결권 부정, (3) 이스라엘에 대한 이중잣대 적용, (4) 현대 이스라엘 정책에 나치 비교, 이런 예시들이 과도하고 터무니없다는 입장
     * 미국 방문 의지가 완전히 사라졌다고 밝힘, 더 심각한 문제는 미국의 정책이 압박에 의해 다른 국가에도 퍼질 수 있다는 점, 자국 정책 확장 시도 때문에 우려
          + 미국만의 문제가 아님, 영국도 비슷하게 심각하며 오히려 정치 엘리트가 더 친이스라엘 성향
          + 저렴한 비행기 가격(유럽발)에도 불구하고 비슷하게 느끼는 사람들이 많음
          + 미국은 멋진 여행지이고 현지인들도 친절하지만 국가와 국민의 행동에는 큰 차이 존재, 현재 벌어지는 상황이 심각하게 우려됨
          + 일부 국가는 특별한 압박 없이도(중남미 등) 미국 정책을 그냥 관성적으로 따라하는 문제
     * 단순히 프라이버시 보호를 넘어서 건강, 성적 지향, 관계, 위치, 재정 등 다양한 이유로 소셜 미디어를 비공개해야 하는 이유가 많음, “F, M, J 비이민 비자 신청자 전원에게 모든 소셜 미디어 프로필을 공개로 전환하라고 요청할 것”이란 공식 지침 언급
          + 중국의 사회 신용 점수를 비판하던 주체가 사실상 미국판 사회 신용 점수를 만들고, 정부 승인 발언만 허용하는 구조로 변질되는 현상
          + 페이스북과 인스타그램 계정을 지워야 하는 ‘수천’ 가지 이유 중 또 하나 추가
          + 세계 많은 국가가 LGBTQ+ 권리에 반대하는데, 이런 국가 출신 이민자가 비판적/폭력적 소셜 미디어 게시글을 갖고 있다면 정말 미국에 흔쾌히 받아들이고 싶은지 반문, 중국과 인도는 동성혼 금지와 차별이 심한 문화/정치 구조임을 상기
          + 완전 공개 계정을 가져야 하는 충분한 이유도 많음
     * 이번 조치가 그동안 있었던 일들에 비하면 그렇게 심하지 않을 수 있지만, 미국 방문이나 유학, 취업 등 다양한 목적으로 미국을 염두에 두던 해외 인재들이 최근 들어 계획을 무기한 보류한다는 점 문제시, 미국이 진짜로 놓치기 시작한 인재는 누구인지, 아직 오고 있는 사람들은 어떤 상황인지, 앞으로 몇 년 뒤면 심지어 그들마저도 안 오게 될지 궁금증
          + 자신이 바로 그 경우라며 소개, 실제로 장거리 연애까지 감수하며 오퍼를 거의 성사시켰으나 독일 관광객 억류 사건 이후 부정적 영향 발생, 실리콘밸리 여행 선물 비행기 티켓도 잠정 보류함, 자신이 특별히 뛰어난 인재는 아니라 생각하지만, 대기업이나 최상위 인재는 크게 흔들리지 않고 기업들도 어느 정도 준비가 돼 있다고 판단
          + 실제로 인도, 중국 같은 국가 출신 지원자들 중에는 그린카드 대기 기간만 20년 이상 걸리는 경우도 있어 분위기가 바뀌고 있음, 특히 연구비 삭감 문제도 겹침, 지난 10년간 임시 이민자에 대한 정책이 얼마나 변덕스러운지 체험했기에 진입 자체가 위험해지고, 유학 후 타국 이주나 빠른 영주권 확보 계획이 없다면 그만큼 리스크, 캐나다 출신의 많은 이들이 상황 나아질 때까지 미국 관광도 안 한다는 추세, 미국이 가진 여러 장점 때문에 근본적 변화는 쉽지 않겠지만 신뢰가 계속 훼손된다면 대안 등장 시 상황 크게 변할 수 있음
          + 캐나다에서 미국으로 이민한 경험자로서 캐나다보다 2배의 소득을 얻음, 일 자체는 비슷하지만 미국에서는 2년 대기했던 의료 서비스를 바로 받았음, 그린카드 과정은 힘들었지만 결국 삶이 좋아짐, 경제적 자유가 강력한 유인이며 소셜 미디어 계정 공개가 큰 장애물로 느껴지지 않는다는 입장
          + 이미 이전부터 미국행을 고려했으나, 유럽 대비 2배 넘는 연봉인지라도 까다로운 비자 절차 때문에 포기 결심
     * “소셜 미디어 프로필이 없으면 미국 비자 거부 사유”란 기사를 출처와 함께 공유 기사
          + 이런 상황이 특히 HN에 모인 IT인들에게 슬픈 일, 이들은 보안/프라이버시 인식이 높아 원래 공개 프로필을 기피하는 경향, 유럽 유학 추천
          + 보도 맥락이 왜곡됐을 가능성 지적, Economic Times보다 Bloomberg 원본이 더 정확하다는 의견, Bloomberg 기사 인용, ""하버드 방문 모든 외국인""을 대상으로 한 조치임을 강조, Rubio의 조치 전문이 공개되었으면 좋겠다는 바람
          + 최근까지 아예 소셜 계정 없었던 본인과, 엄마는 지금도 하나도 없다는 사례 공유
          + LLM(대형 언어 모델)로 대조 계정 만드는 아이디어, 실제 계정의 반대 내용을 AI가 자동 포스팅해서 만들 수도 있다는 상상
          + 국가 신분증(ID 카드를 도입하는 것)에 대한 논란과의 모순 언급
     * 소셜 미디어 기반 심사가 너무 비상식적이라 생각, 본인은 인스타그램, 페이스북, 틱톡도 없으며 예전 계정들도 대부분 가명 사용, 동명이인이 많은데 미국 정부는 어떻게 본인이 비공개 계정으로 숨는지 확인할 것인지 의문, 결국 미국 비자 영원히 신청하지 않을 계획, 외국 인재 매력 완전히 상실, 가족 중 일부가 같은 성을 써서 아이슬란드, 탄자니아 가족들이 연결 요청하는 코믹한 상황도 경험
     * 자녀를 위해 ‘버너’(가짜) 소셜 계정 만들 계획, 논란 없는 사진들만 올릴 예정, AI가 심사할 때 ‘안전한 시민’에 최대한 가깝게 보이도록 백업, 중국도 염두에 두고 위니 더 푸 사진은 절대 안 올림, 정부에 ""평범하고 건전한"" 인상을 주는 노하우를 구함
          + 사실상 유일한 팁은 해당 적대국 방문을 자제하는 것, 반드시 가야 한다면 모든 소셜 미디어 정보 수정, 생년월일&이름 등 혼동, 가족 사진 삭제, ‘버너’ 계정은 전혀 다른 SNS 플랫폼 사용, AI로 포스팅 아이디어 생성해 일정 기간 꾸준히 관리 필요, 신뢰도 있는 계정처럼 과거 이력 만드는 것이 관건, 혹은 자녀가 소셜 미디어 사용 금지라는 설정으로 아예 부담 해소, 하지만 결국 방문 피하는 게 최선이라고 조언, 억류 가능성까지 고민해야 함
          + 가장 단순한 해법은 미국에 가지 않는 것
          + 페이스북에서 종종 비판적 글을 올렸어도 중국 비자 취득은 전혀 문제 없었음, 그들이 심사를 안 하거나(페이스북 차단) 실제로는 신경 안 쓰는 게 아닐까 하는 추측
          + 시간이 흐른 뒤에도 문제가 없다는 예측이 가능한 방법이 있냐는 반문, 미래 정부 정책까지 예측은 불가능
          + 계정만 생성하고 한 번 사진 올려두는 정도의 가벼운 준비만으로 충분하다고 생각, 과거에 휴대폰 인증 없는 이메일/페이스북 계정을 여럿 미리 만든 적 있음, 최근 새로 생성한 Gmail 계정은 아무 이유 없이 차단
     * 이런 상황에서는 제5수정헌법에 따라 소셜 미디어에 명시적 경고문구(“이 웹사이트에서 침묵할 권리가 있음. 당신의 모든 발언은 법정에서 불리하게 사용될 수 있음”) 부착이 필요
          + 웬만하면 신생아 팔뚝에 이 문구 문신까지 할 수 있겠지만, 효과가 있을지는 미지수
          + 비어있는 소셜 미디어 계정이 없는 것보다 더 의심받을 수 있다고 주장, 침묵하는 게 정답이 아닐 수 있음, 국경 관리의 광범위한 권한 때문에 실제로 외국인은 권리에 큰 의미가 없음, 행정 절차에 도전하지 않는 이상 ""입국 거부""라는 꼬리표만 남아 미국 비자 취득 난이도도 상승, 미국이 이민자/시민조차 적법절차 없이 추방한 전례 등을 들며 침묵의 권리가 별 의미 없다고 지적, 정부가 이미 다양한 방식으로 소셜 활동 모니터링 중, 공개로 전환하라는 요구는 사실상 감시를 더 쉽게 만드는 것, 이 모든 변화의 핵심은 이스라엘-팔레스타인 사안에 대한 비판적 의견 차단에 있음
     * 이 모든 조치의 목적이 이스라엘 비판 억압인데 동의하는지 묻는 질문
          + 실제로 그것만이 목적이 아니라 권력자들이 원하면 뭐든지 쓸 수 있는 강력한 억압 도구라는 점 강조, 물론 이스라엘 관련 목적에도 쓰일 수 있음, 더 넓은 맥락에서 생각 필요
          + 이번 조치는 하나의 ‘시험 풍선’이라고 평가
     * 여러 기관(DHS, ICE 등)이 법 위에서 명령만 수행하는 ‘복수의 게슈타포’ 현실이라고 비판
"
"https://news.hada.io/topic?id=21560","클로드 코드 사용량 모니터 (Claude-Code-Usage-Monitor)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               클로드 코드 사용량 모니터 (Claude-Code-Usage-Monitor)

   요즘 클로드코드 정말 많이쓰시죠.
   제가 만든 건 아니고 해커뉴스에서 봤는데 너무 유용해보여서 공유해봅니다.

   Claude Code 세션 중간에 토큰 한도에 도달해 실행이 중단되는 문제를 해결하기 위해 만들어진 로컬 추적 도구입니다.
     * 실시간으로 프롬프트 및 응답 토큰 사용량을 스트리밍
     * 세션 종료 전 한도 초과 여부를 예측
     * 100% 로컬에서 작동 (서버/로그인 불필요)
     * Pro, Max×5, Max×20 플랜 프리셋 제공 (JSON 수정으로 커스터마이즈 가능)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   참고로 이거랑은 상관없지만 어디서 보기로는 claude code의 90% 정도 사용자가 매일 ~$12 정도를 지불하고 쓴다는거 같습니다. 저는 제 돈으로 하기는 좀 그래서 못 쓰고 있지만...

   아! 클로드 코드가 얼마전에 따로 요금책정이 아니라 정액제 비슷한 형식으로 바뀌어서 100달러 플랜의 경우에는 진짜 거의 프리하게 쓸 수 있는 수준이 돼서 엄청 효용성이 올라갔습니다

   아 max 5가 100달러 플랜이군요. 언제 기회가 되면 회사에 요청해 봐야 겠습니다..
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * 최대 5배 Pro(월 100달러) : 평균적인 사용자는 5시간마다 클로드에게 약 225개의 메시지를 보낼 수 있거나, 5시간마다 클로드 코드로 약 50-200개의 메시지를 보낼 수 있습니다.
     * 최대 20x Pro(월 $200) : 평균적인 사용자는 클로드에게 5시간마다 약 900개의 메시지를 보낼 수 있습니다. 또는 클로드 코드로 5시간마다 약 200-800개의 메시지를 보낼 수 있습니다.

     내용에는 원글의 내용을 요약하거나, 혹시 원글이 영문이라면 한글 번역요약문을 직접 적어주시면 좋아요.
     사이트 이용법 - GeekNews>

   앗 그렇네요 감사합니다!!!! 추가하겠습니다. 라고 하려했는데 수정이 안되네요 ㅜㅜ
"
"https://news.hada.io/topic?id=21497","중력은 단순히 엔트로피가 증가하는 현상일까? 먼 길 돌아 다시 주목받는 이론","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               중력은 단순히 엔트로피가 증가하는 현상일까? 먼 길 돌아 다시 주목받는 이론

     * 최근 이론물리학자들은 중력을 미시적 입자들의 무작위적 상호작용, 즉 엔트로피 증가의 결과로 보는 모델을 제시함
     * 이 모델은 엔트로피와 중력의 관계에 초점을 맞추며, 기존 중력 이론에 대한 대안적 접근을 탐구함
     * 엔트로피 중력 모델은 검증 가능한 실험적 예측을 제공하며, 실제 중력이 기본 힘이 아니라 집단적 현상임을 시사함
     * 그러나 이 모델은 뉴턴의 중력 법칙만 설명하며, 일반 상대성 이론의 공간-시간 곡률같은 깊은 특성을 담지는 못함
     * 새로운 이론은 양자 중첩, 웨이브 함수 붕괴 등과도 연결되어, 양자중력이론 및 중력의 본질 탐구에 실마리를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

뉴턴, 아인슈타인, 그리고 중력의 재해석

     * 아이작 뉴턴은 중력의 본질에 대해 혼란을 품었으며, 당시 여러 학자들이 중력을 실제로는 '당기는 힘'이 아닌 '미는 힘'으로 해석하려 시도함
     * 알버트 아인슈타인은 중력이 공간과 시간의 왜곡이라고 설명했으나, 이 또한 완전한 설명은 아니었음
     * 중력이 미시적 입자들의 집단적 효과, 즉 '스웜 행동'에서 나타나는 현상이라는 해석은 여전히 많은 물리학자들의 관심사임

엔트로피 중력 이론의 현대적 부활

     * 최근 Daniel Carney 등 이론물리학자 팀은, 우주에 보이지 않는 열적 시스템이 존재하여 이로 인해 우리가 아는 모든 중력 현상이 설명될 수 있다는 모델을 제시함
     * 이 접근은 '엔트로피 중력'이라 불리며 중력을 열(heat) 물리학으로 간주함
     * 엔트로피 중력은 보일러, 자동차 엔진, 냉장고 등에서 보듯, 입자의 무작위 움직임과 혼합에 의해 엔트로피가 증가하는 현상과 동일한 원리로 중력이 생성된다고 설명함

일반 상대성 이론과 엔트로피의 연결

     * 일반 상대성 이론은 아름답고 정확한 예측을 제공하지만, 블랙홀 내부처럼 특이점에서는 설명력이 사라지는 한계점을 가짐
     * 일반 상대성 이론은 블랙홀은 늘어날 뿐 줄지 않으며, 흡수만 하고 방출하지 않는 등 엔트로피적인 현상과 유사함
     * 양자역학으로 설명 시 블랙홀에서 열적 방출(호킹 복사)이 일어나, 블랙홀 또는 공간-시간이 실제로 미시적 입자나 구성요소로 이루어졌을 가능성 시사

홀로그래피 원리와 Jacobson의 접근

     * 홀로그래피 원리는 미시적 입자들이 만드는 패턴이 추가적인 차원을 만들어 중력이 자연스럽게 나타남을 설명함
     * Ted Jacobson은 공간-시간이 독자적인 열적 성질을 가진다는 가정 아래, 그로부터 일반 상대성 이론의 방정식을 연역함
     * 이 접근은 중력과 열의 깊은 연관성을 강조함

Carney 등의 구체적 모델

     * 첫 번째 모델: 공간이 양자 입자(큐비트) 의 결정격자로 이뤄졌으며, 질량이 있는 물체가 인근 큐비트를 재배열하여 질서 있는 영역(엔트로피 감소)을 형성함
          + 두 질량이 가까워질수록 전체 시스템의 엔트로피를 높이기 위해 질량이 서로 끌리는 효과 발생
          + 이 효과는 뉴턴의 중력 법칙과 동일하게 거리에 따라 약해짐
     * 두 번째 모델: 큐비트가 특정 위치에 얽매이지 않으며 비국소적으로 질량에 영향 줌
          + 두 질량의 거리 변화 시 각 큐비트가 저장할 수 있는 에너지 변화 → 질량이 가까워질수록 시스템의 엔트로피가 증가하도록 만듦

강점과 한계

     * 두 모델 모두 실제 큐비트의 존재에 대한 독립적 증거 없음, 힘의 세기와 방향 미세 조정 필요
     * 일반 상대성 이론처럼 공간-시간 곡률이나, 자유낙하시 중력 불감 등 중력의 핵심적 특성을 설명하지 못함
     * Newton의 약한 중력 영역만 설명하며, 블랙홀 등 강한 중력 영역에 대한 설명력 부족
     * 모델은 원리 증명 수준으로, 실제 우주를 모델링하기에는 한계 지님

실험적 검증 및 의미

     * 이론의 가장 큰 장점은 검증 가능한 예측을 제공한다는 점
     * 예를 들어, 질량체가 두 위치에 동시에 존재하는 양자 중첩 상태일 때, 이 모델은 큐비트가 이를 붕괴시킬 것으로 예측함
     * 이는 웨이브 함수 붕괴 현상과 연결되어 있으며, 현재 이러한 붕괴 모델을 실험적으로 검증하는 시도 진행 중임
     * 아직 실제 중력이 홀로그래피적으로 발생하는지 확정되지 않았으므로, 엔트로피적 기원이 존재할 가능성도 연구할 가치가 있음

결론 및 시사점

     * 엔트로피 중력 이론은 여전히 소수설임에도 불구하고, 중력의 본질적 이해에 새로운 실험 방향과 질문을 제공함
     * 이 이론이 올바르다면, 중력은 더 이상 법칙이 아니라 통계적 경향성으로 재해석될 필요 있음

        Hacker News 의견

     * 엔트로피 중력은 ""브라질 너트 효과""와 비슷하다는 설명을 하고 싶음
       브라질 너트 효과란 유리잔에 여러 크기의 견과류를 넣고 흔들면 큰 견과류(브라질 너트)가 위로 올라오는 현상임
       이 현상은 큰 견과류가 더 무거워서 흔들릴 때 천천히 움직이고, 작은 땅콩들은 아래 빈 공간으로 채워지는 결과로 해석 가능
       엔트로피 중력 이론에서는 무작위로 사방에서 입자가 대상을 세게 때리는 기본 밀도가 있다고 생각함
       두 개의 큰 질량체가 가까워지면 그 사이의 입자 밀도가 낮아져 서로를 끌어당기는 효과처럼 보임
       입자가 그림자를 드리우는 것 같은 효과를 낸다는 설명임
       다만 입자가 큰 물체와 상호작용하는 밀도에 대한 가정이 설득력 있게 성립하기 힘들다고 생각함
       누군가 더 잘 아는 사람이 오류를 지적해주면 좋겠음
       브라질 너트 효과는 실제로 관찰 가능한 효과임
       건포도를 꺼내고 싶으면 시리얼을 흔들고, 고양이 화장실에서도 흔들면 선물이 위로 올라옴
       아래 링크로 관련 위키피디아 granular convection 설명과 유튜브 영상 참고 가능
          + 나도 물리학자는 아니지만, 위에서 설명한 얘기와 관련 있는 파인만 강의의 한 대목이 떠오름
            파인만 강의 원문 링크
            요지는, 중력을 설명하려는 여러 가설 중 하나에 대해 설명하며, 입자가 모든 방향에서 아주 빠른 속도로 움직이며 약간씩만 흡수된다고 가정
            이 입자들이 지구에 충격을 주는데, 어디서나 균일하면 평형
            하지만 태양이 가까울 때는 태양에서 오는 입자가 일부 흡수되어 그 방향에서 incoming 입자가 줄어든다는 논리
            그래서 지구가 결국 태양 쪽으로 끌리는 것처럼 보이나, 실제로는 이 이론이 맞지 않음
            만약 이게 맞다면, 지구가 태양 주위를 돌 때 앞쪽에서 더 많은 입자를 맞으므로 저항을 받고, 곧 멈추게 됨
            이 메커니즘으론 실제 우주에서 지구처럼 오랜 기간 궤도를 유지할 수 없음
            즉, 여러 사람이 이 방식의 중력기계를 떠올렸다가 반드시 잘못된 예측을 하게 되므로 성립이 안 된다는 내용임
          + 이 유튜브 영상이 과립 물리학을 더 잘 설명함
            진동의 속도(진폭)가 입자들이 예상외로 배치되는 결과를 보여줌
            낮은 진동에서는 뉴턴중력과 유사하게 작용하지만, 더 빠른 진동에서는 MOND 중력과 비슷한 현상을 보임
            은하와 대형 공허가 생기기도 하며, 이론적으로 암흑물질 없이도 설명 가능함
          + 엔트로피 해석에 따르면, X개의 확률적으로 동등한 상태가 있고, 어떤 조건을 가장 많이 만족하는 상태가 많을 때 다음 상태가 그쪽으로 갈 가능성이 높다는 논리임
            예시로, N개의 동전을 던지면 가능한 상태가 2^N개임
            전체가 모두 앞면인 경우는 단 하나
            반은 앞, 반은 뒷면인 조합은 훨씬 많으니 N이 커질수록 평균적으로 반은 앞면이라는 '거시적' 상태가 압도적으로 많음
            엔트로피는 이런 '거시적으로 많이 가능한 상태' 쪽으로 시스템이 자연스럽게 이동하려는 경향이 있다는 이야기임
          + ""큰 물체가 더 질량이 커서 흔들릴 때 천천히 움직인다""는 설명에 의문
            만약 큰 물체가 더 느리게 움직인다면, 용기의 가속 기준으로 보면 더 빨리 움직인다고 볼 수 있지 않을까?
            일상적인 설명은, 흔들면 잠깐씩 공간이 생기고 더 작은 물체가 그 작은 공간을 채우며 아래로 빠질 확률이 높아서 생기는 현상이라고 이해함
          + 더 질량이 큰 입자는 실제로 더 작지 않은지(de Broglie 파장 기준에서), 그러면 '그림자'도 더 작게 드리워지지 않을까 궁금
            혹시 다른 상호작용에선 입자의 '크기'와 질량의 관계가 달라서, 예를 들어 중력에서는 입자의 크기가 질량에 비례하는지 등 혼란스러움
            그리고 QM(양자역학) 초입에서 wavefunction이 '광자'로 측정할 때 위치확률진폭을 설명한다면, 만약 Z 보존 같은 다른 상호작용으로 측정했다면 입자의 '위치' 해석이 완전히 달라질지 추가로 궁금함
     * 통계역학에서의 엔트로피 정의는 시스템 내 입자들의 가능한 배열 수에 의한 것임
       닫힌 시스템에서 엔트로피는 '열적 죽음'(heat death)이라고 극적으로 표현되는 평형 상태로 수렴
       하지만, 우주는 팽창 중이므로 가능한 배열 수(상태 수도) 계속 증가
       만약 우주 팽창 속도가 구성요소 재분포보다 빠르다면 엔트로피가 줄어들 수도 있음
       이런 관점에선, 엔트로피가 중력에 핵심 요소로 들어간 이론은 시간이 지남에 따라 중력이 변화한다는 예측을 할 수 있음
     * 엔트로피 중력은 매력적인 프레임워크라고 생각함
       많은 물리학자들이 '아직 밝혀지지 않은 모든 것의 이론'이 미시적이면서 양자역학적이고, 매우 약한 중력이 마치 회계상의 오차처럼 이론에서 도출되면 좋겠다고 여김
       하지만 이런 이론들은 기본적으로 너무 많은 가정을 전제로 하기에 ""봐라, 아인슈타인 방정식이 나왔다""고 해도 쉽게 믿기 힘듦
          + Jacobson이 열역학과 특수상대를 결합하면 일반상대성이 도출됨을 보였는데, 이 두 조건 자체가 너무 일반적이어서 더 요구할 게 있을지 고민스러움
          + 개인적으로 문제적이라고 생각하는 가정들이 무엇인지 궁금함
          + 기사에서 말하는 수준에선 아직 아인슈타인 방정식까지는 아니고, 고전 뉴턴중력 정도라고 이해함
          + ""아직 밝혀지지 않은 모든 것의 이론이 미시적이고 양자적인 형태일 것 같다""는 주장엔 동의
            ""중력이 회계 오류같이 이론에서 발생한다""는 것에 대해선, 그보단 또 다른 특이한 보손 패밀리(입자 형태)일 것 같음
            기사 내용 중:
            ""엔트로피 중력은 아직 소수 의견이지만, 쉽게 사라질 사상은 아니고 비판자들도 완전히 무시하진 못하는 시각임""
     * 나는 실험물리학자라서, 새로운 이론에 흥분하기 전에 반드시 관찰 가능한 현상 예측까지 도달하는지 확인 필요
          + 그래서 Wolfram과 같은 이론에도 회의적임
            수많은 기존 이론(특수상대성, 일부 양자역학, 중력 등)을 설명해도, 새로운 검증 가능한 예측이나 근본이 없다면 '오버핏'이라고 생각
            이론이 10가지 예측을 다 현실과 일치하게 해도 모두 이미 알고 있는 것이라면, 새로움을 기대하기 어려움
          + 이런 emergent(출현형) 이론들은 뉴턴중력이나 일반상대를 유도하지만, 실제로 실험적으로 뭘 테스트할 수 있는지 뚜렷하지 않음
            만약 MOND(수정 뉴턴동역학)를 별도의 MOND장 도입 없이 예측한다면, 그때서야 MOND의 검증 수준에서 반증 가능하다고 할 수 있음
          + 가끔 생각해보면, 만약 우리 물리학이 블랙홀 존재 자체를 허락하지 않았다면 이론을 어떻게 스트레스테스트했을지 궁금
            블랙홀은 우주론에서 '표준 촛불'처럼 이론 진보의 중요한 역할을 한다고 생각함
          + 현실적으로 실용성을 증명하기 전까지는 즐거운 수학 문제 풀이 시간이라는 인식 필요
          + 두 모델 중 '최소 기술 길이(MDL)'가 더 짧은 모델이 일반화에 더 강할 확률이 높다는 주장함
     * 나는 자력(magnetism)이 중력에 가깝다고 생각함
       수년간 이 주장을 해왔고, 대부분 비정렬된 자장이 합쳐져 아주 약간은 끌어당기는 순수 효과를 낸다고 느낌
     * 나는 잘 이해가 가지 않음
       내게 있어 엔트로피란 실재하는 물리 현상이 아니라 우리가 어떤 시스템에 대해 완벽히 알지 못할 때 그 불완전함을 수치로 나타낸 것임
       우리는 물질의 거시적 특성만 관측하니, 미시적 실상을 제대로 나타내지 못하는 지표를 만든 것임
       만약 현미경으로 미시 세계를 완벽히 알 수 있다면 엔트로피라는 개념 자체가 무의미해질 것
       그래서 중력이나 기타 기본 상호작용이 엔트로피에서 나온다는 말을 이해할 수 없음
       엔트로피란 인간이 만든 개념이라는 생각임
          + 이건 오해임
            물리 엔트로피는 실제 현상을 지배
            예시로, 얼음이 따뜻한 방에서 녹는 이유, 케이블이 점점 꼬이는 이유 등이 있음
            우리가 엔트로피를 측정하는 것은 방 안의 얼음이나 꼬인 케이블 같은 거시적 상태를 요약한 것일 뿐
            Boltzmann 방식의 엔트로피는 '무질서'하게 배치하는 경우의 수가 압도적으로 많기 때문에 전체적으로 엔트로피가 오르는 경향성을 설명해줌
            그래서 얼음이 반드시 녹게 되는 것임
          + 엔트로피 역시 온도와 마찬가지로 물리적 '실체'임
            온도가 단일 입자 수준에서 존재하지 않는다고 해서 물리량이 아닌 것처럼 여길 필요 없음
            엔트로피는 특정 시스템의 미시 상태 수를 측정하며, 이 숫자는 관찰자와 무관하게 존재함
          + 근본적으로 엔트로피는 시스템을 완전히 알지 못하는 '무지'를 수치로 담는 것임
            그럼에도 실험실에서 실제로 '엔트로피 힘(entropic force)' 현상을 측정 가능
            엔트로피 힘 위키 설명와 이상사슬(ideal chain) 예시 참고 추천
            이런 관점에서 엔트로피는 단순히 인간이 만든 계산법이 아니라 관찰되는 현상을 효과적으로 설명하므로, 물리학의 실용적이지만 근본적인 법칙은 아님
            엔트로피 중력을 믿는다면 중력의 '출현현상'설을 지지하게 되고, 결국 더 근본적인 중력 이론이 필요하단 결론
            기존 연구들은 중력을 곧장 양자화하려는 경향이 있지만, 엔트로피 중력은 마치 기체방정식을 무리하게 양자화하려 하지 말라는 것과 비슷한 생각임
            덧붙여, '확률 분포 없는 엔트로피'란 있을 수 없음. 무작정 엔트로피를 '실재량'이라고 주장하는 건 19세기적 시각임
          + 컴퓨터 과학에서 쓰는 엔트로피와 물리학 용어의 엔트로피는 다름
            이 차이를 잘 설명한 강연 추천함
          + 나도 예전엔 엔트로피가 단순히 우리의 인식 한계 문제라고 생각했지만, 이제는 하이젠베르크 불확정성 원리로 인해 근본적으로 미시 상태를 완벽하게 알 수 없는 것이 현실임을 믿게 됨
            모든 사건은 기본적으로 비가역적이고, 엔트로피는 언제나 증가
            완벽함은 오직 이론 안에서만 가능한 것임
     * 정보가 어떻게 작동하는지에 따라 중력이 출현된다는 개념은 매력적임
       다만 이 모델이 일반상대성이론과 다른 현상을 예측하는 명확한 증거는 아직 못 봄
       지금으로선 재미있게 논의할 수 있는 이론이긴 하지만 완전히 수용하긴 어려움
     * Wolfram의 하이퍼그래프 기반 물리 모델과 이런 관점이 호환되는지 궁금
       그 틀에선 중력이 하이퍼그래프 진화의 통계적 거동에서 출현하는 현상으로, 중력을 시스템의 계산 복잡성 최소화 경향에서 나오는 '엔트로피 힘'으로 해석 가능성 있음
     * 스카이림 게임의 emergent fox-treasure gravity에 관한 흥미로운 사례 소개
       관련 기사
       요약하자면, 보물이 있는 지역이 여우의 무작위 이동 경로에서 더 높은 '엔트로피'를 보이기 때문에, 여우가 본의 아니게 보물 쪽으로 더 쉽게 가는 현상임
     * 엔트로피 중력이라면 부력(buoyancy)과 비슷하지 않냐는 질문
"
"https://news.hada.io/topic?id=21504","리더에게 피드백하는 법 (잘리지 않고) [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      리더에게 피드백하는 법 (잘리지 않고) [번역글]

  리더에게 효과적으로 피드백하는 법 (잘리지 않고)

    1. 상위 리더에게 피드백하는 건 기존 질서에 도전하는 일

     * 일반적으로 피드백은 상사가 부하에게 주는 것.
     * 반대로 리더에게 피드백을 주는 건 규범을 뒤집는 일이라 신중해야 함.
     * 직접 피드백은 최후의 수단으로 삼고, 먼저 내가 바꿀 수 있는 부분이 있는지 점검 필요.

    2. 피드백 전 스스로에게 던져야 할 질문

    1. 내가 통제할 수 있는 범위 내에서 개선할 수 있는가?
    2. 그냥 받아들일 수 있는가, 정말 피드백이 필요한가?
    3. 피드백이 성공할 가능성이 얼마나 되는가?

     * 여러 번 곱씹은 뒤, 정말 필요하다고 판단될 때만 실행.

    3. 효과적인 피드백 전달 4가지 방법

      1) “Even more(더 많이)” 기법

     * 리더의 긍정적인 면을 인정하고, 그 부분을 ‘더 많이’ 해달라고 제안.
     * 예시:
          + 🚫 “신입 직원들에게 지침을 좀 줘야 할 것 같아요.”
          + ✅ “신입 직원들에게 이미 잘 안내해주고 계시지만, 이번엔 조금 더 구체적으로 가이드를 주면 팀이 더 빠르게 적응할 수 있을 것 같아요.”

      2) 내 경험을 예시로 들기

     * 상대방이 비난받는다고 느끼지 않도록 내 경험을 공유.
     * 예시:
          + 🚫 “X를 해보시는 게 좋겠어요.”
          + ✅ “저도 예전에 이 부분에서 어려움을 겪었는데, X를 시도해보니 도움이 되더라고요.”

      3) 부드럽고 외교적인 질문형 말투 사용

     * 권위에 복종하거나, 반대로 직설적으로 말하는 극단을 피함.
     * 예시:
          + “이런 방법을 한번 시도해보는 것도 좋을 것 같아요.”
          + “혹시 이런 방향은 어떨까요?”
          + “이 부분에 대해 어떻게 생각하세요?”

      4) 근거와 실제 사례로 설득

     * 구체적인 데이터, 과거 사례 등 논리적 근거를 제시.
     * 예시:
          + 🚫 “프로젝트 할 때 피드백을 더 자주 줘야 할 것 같아요.”
          + ✅ “예전에 디자인 시안 작업할 때, 매주 리뷰 미팅을 했더니 최종 결과에 만족도가 높았어요. 이번에도 중간 점검을 자주 하면 더 좋을 것 같아요.”

    4. 주의사항

     * 피드백은 항상 존중과 협력의 의도로 접근.
     * ‘내가 옳다’는 확신만으로 정면 돌파하지 말 것.
     * 논리나 데이터를 제시할 때도 “well actually…” 느낌을 주지 않도록 주의.

   리더라는 대상이 어려울 뿐, 부정적이고 강력하게 보이지 않도록 피드백하는 내용이네요. 알면서도 하기 어려운데 덕분에 리마인드 하고 갑니다 ㅎㅎ

   맞아요 꼭 리더 뿐 아니라 모든 유형의 피드백에서 사용할 수 있는 내용이죠! 글 좋게 봐주셔서 감사합니다!

   예전에 인상 깊게 봤던 글인데, 번역이 올라와서 반갑네요. 원글 저자의 다음 글도 꽤 인상적이었습니다: https://newsletter.weskao.com/p/15-principles-for-managing-up

   글 좋게 봐주셔서 감사합니다 :) Wes Kao 저 분 인상 깊은 글을 많이 쓰시더라구요. 해당 글도 나중에 번역을 해봐야겠습니다.

   리더 자체가 피드백을 잘 주는 타입이면, 어떤 형태든 좀 더 잘 받아들이는 것 같습니다.
   저도 리드롤 하지만, 사실 피드백에 관한건 톤앤매너에 관해 리더들이 더 많이 고민하는 부분이라..
   리더가 피드백을 세련되지 않고, 거칠고, 납득이 안되는 형태라면, 위 조언도 도움이 안될 수도 있다고 봅니다.

   맞아요 100% 통용되는 정답은 없죠. 그래서 말하신대로 거칠고 세련되지 않고 납득이 안되는 피드백을 주는 리더를 대상으로 한다면 위에 나온대로 “ 직접 피드백은 최후의 수단으로 삼고, 먼저 내가 바꿀 수 있는 부분이 있는지 점검“ 하고 스스로에게 던지는 질문을 던져보는 게 좋을 거 같아요. 글 좋게 봐주셔서 감사합니다!

   꼭 리더에게 해당되는 내용들은 아니네요 :D

   맞아요 모든 대상, 유형의 피드백에 해당되는 내용인 것 같아요! 글 좋게 봐주셔서 감사합니다!
"
"https://news.hada.io/topic?id=21475","Show GN: 한국 미세먼지 크롬 익스텐션","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Show GN: 한국 미세먼지 크롬 익스텐션

   안녕하세요.

   제가 두어달 쯤 전에 미세먼지 크롬 익스텐션을 만들었는데, 가족과 친구들이 매우 잘 쓰고 있어 소개해드리려 합니다. (저도 틈만 나면 봐요...API 제대로 돌아가나 체크할 겸ㅎㅎ)

   주소는 카카오 API를, 미세먼지 정보는 공공데이터포털의 Open API를 사용했습니다.

   다운로드 링크: https://chromewebstore.google.com/detail/…

   심플하고 좋은 것 같습니다!!

   개별 수치(오존/일산화탄소 등)의 적합 여부도 함께 나오면 좀 더 좋을 것 같다는 생각이 있습니다.
   (예를 들면 배경색으로 표현하는 등)

   그리고 ""제공되는 데이터는 실시간 측정값으로, 측정소 사정이나 통신 상태에 따라 오차가 있을 수 있습니다.""요 글씨는 엄청 작아서 잘 안보이는 것 같습니다.

   유용하게 잘 쓰겠습니다. 감사합니다.

   오, 다음 업데이트에서 오존, 일산화탄소 등의 데이터도 색이나 grade 등을 추가하겠습니다!

   혹시 가능하면 국내 기준 말고 OECD 기준으로도 위험도 등을 제공해줄 있을까요? 사실 미세먼지에 대해 민감하게 생각하는 분들은 국내 기준으로 따지면 상당히 위험해서요.
   우리나라가 OECD 기준보다 완화되어서 우리나라 기준 좋음이 OECD 에선 안좋음으로 나올 수 있습니다.
   (꼭 해주실 필요는 없는데, 기왕이면 미세먼지 민감군이 주로 쓸 테니 그에 맞게 안내해주면 좋지 않을까 싶어서요.)

   WHO 기준으로 업데이트했습니다! 1-2일 뒤 반영될 예정입니다 :)

   의견 감사합니다!!!

   사실 저희가 어플도 개발중인데 거기에 반영해볼게요!!

   좋네요!

   감사합니닷!!!
"
"https://news.hada.io/topic?id=21454","페아노 산술은 충분하다, 왜냐하면 페아노 산술은 계산을 인코딩할 수 있기 때문임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              페아노 산술은 충분하다, 왜냐하면 페아노 산술은 계산을 인코딩할 수 있기 때문임

     * 페아노 산술(PA) 은 기계적인 계산 과정을 표현할 수 있으므로, 모든 단일 Goodstein 수열의 소멸을 PA에서 증명 가능함
     * Cantor 표준형과 유전적 기수 표기법을 통해 Goodstein 수열과 그 하강성 등을 표현하며, 이로 인해 유한한 길이의 증명 구성이 가능함
     * 귀납법(강한 귀납/초월적 귀납) 을 통해 특정 차수의 기수까지 증명을 각각 확장 가능함
     * PA는 모든 자연수 n에 대해 “G(n)이 0에 도달한다” 는 것을 증명할 수 있으나, Goodstein 정리 전체(모든 n)에 대한 총체적 증명은 불가함
     * PA로 계산, 데이터 구조(List, Pair 등), 심지어 프로그래밍 언어(Lisp) 자체의 인코딩 및 자체 증명 과정 인코딩도 충분히 구현 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 문제 배경

     * 이 글은 페아노 산술(PA)이 Goodstein 수열의 “각 n에 대해 0에 도달함(G(n) terminates)” 을 증명할 수 있음을 기술함
     * 이는 논리학자에게는 자명해 보이지만, 프로그래머가 이해할 수 있도록 계산 인코딩의 관점에서 설명함
     * Goodstein 수열의 각 경우에 대해 PA 내부에서 구체적 증명 루틴을 구성할 수 있음

Ordinals(순서수)와 Goodstein 수열

     * Von Neumann 방식으로 순서수(Sets as Ordinals) 를 생성함
          + 0은 공집합, 1은 {0}, 2는 {0,1}, ω는 {0,1,2,…}, ω+1은 {0,1,2,…,ω} 등 순서가 잘 정의됨
     * Goodstein 수열은 Cantor 표준형을 사용한 유전적 진법 표기법을 통해 기술됨
          + 예: ω^ω는 ((1,ω)), 즉 ((1,(1,1)))
          + < 순서는 각 항의 기수/계수에 대해 사전식 비교로 판단함

귀납법과 초월적 귀납(Transfinite Induction)

     * 페아노 산술의 귀납법: 0에 대해 성립, n→n+1에 대해 성립하면 전체 자연수에 성립함
     * 강한 귀납법 또한 PA로 증명 가능함
     * 초월적 귀납(Transfinite induction): ZFC 등에서는 무한 기수에 대해 확장 가능하며, Cantor 표준형으로 쓰인 수에 대해 적용 가능함
          + Theorem 1: Cantor 표준형 내 하강 수열은 항상 유한함
          + Theorem 2: Cantor 표준형 수에 대해 초월적 귀납법 사용 가능함

PA에서의 초월적 귀납과 Goodstein 수열 증명 길이

     * PA는 임의의 n에서 Goodstein 수열의 소멸 증명을 생성할 수 있음
          + Cantor 표준형의 타워 높이 m=O(log* n)(iterated log)에 따라 증명 구성 가능함
          + 각 단계별로 m번의 증명을 조합하여 전체 증명 길이 O(m^2) 또는 특별한 표기법(ω[m]) 도입 시 O(m log m)으로 단축 가능함
     * 단, 전체 Goodstein 정리(모든 n)에 대한 증명은 PA에서 불가능함(ε0에 대한 초월귀납 불가)
          + 만약 가능하다면 PA의 무모순성도 증명 가능해져 Gödel 제2불완전성과 충돌함

PA의 프로그램, 데이터 구조 인코딩

     * PA는 계산/프로그램/데이터 구조(숫자, 쌍, 리스트, 그 외 모든 구조) 를 충분히 인코딩 가능함
     * 다음과 같은 방식으로 다양한 기능 구현 가능함:
          + 기초 논리 및 연산(+, *, pow, //, %, min, max 등)
          + 패턴 매칭과 조건 분기(if, cond 등)
          + 숫자를 두 개의 숫자(쌍)로 인코딩하거나, 쌍에서 다시 리스트 등 더 복잡한 구조로 반복 확장(재귀적 리스트, 트리, 텍스트 등)
     * 이러한 데이터 구조 인코딩으로 임의의 가상 머신/프로그래밍 언어(Lisp 등) 해석기까지 구축 가능함

Lisp로의 부트스트랩과 인코딩

     * Lisp를 예시로 하여, 기초적인 수치 및 논리 연산, 데이터 구조, 프로그래밍 언어 해석기/인터프리터 구현법을 설명함
     * Lisp의 구조(명령/인자 매핑, 특수형, macro 등) 모두 PA에서 적절한 함수 조합으로 구현 가능함
     * 더 나아가 PA 내에서 PA 증명 과정 자체, 논리 증명 절차, 자기지시적 구조까지 모두 기호적으로 인코딩 및 구현할 수 있음

PA 내에서의 논리 자체 인코딩

     * 수리논리학에서의 First-Order Logic의 증명 과정을 PA 수의 데이터로 인코딩 가능함
     * 각 증명 단계/명제/추론 규칙/올바른 증명 체크를 일련의 수치 함수/리스트 처리로 인식 및 처리
     * 이런 메타적, 자기지시적 인코딩은 Gödel의 불완전성 및 Halting 문제 증명까지 연결됨

결론

     * 계산, 데이터 구조, 프로그램, 논리 증명 과정까지 PA에서 충분히 인코딩, 증명, 해석 가능함
     * 따라서 임의의 Goodstein 수열(n에 대해 G(n) 소멸) 는 PA 내부에서 구체적으로 증명 가능함
     * 단, 전체 Goodstein 정리(모든 n)에 대해 ""PA가 PA 내에서 Goodstein 정리를 증명한다""는 증명은 논리의 한계상 불가함
     * 프로그래머 관점에서 PA는 계산 자체를 인코딩할 수 있는 완전한 논리 기반임

        Hacker News 의견

     * 이 글은 제가 Stack Overflow에서 질문했던 내용을 블로그 포스트로 만든 것 Peano 공리계로 증명할 수 있는 한계와 어떻게 Peano 공리계 내에서 Lisp를 부트스트랩할지 설명이 포함됨 농담은 대부분 두 번째 섹션에 있음 수정이나 추가 질문도 환영함
          + 이 글을 읽으면서 ""Why Lisp?"" 섹션에서 괄호가 맞지 않는 부분을 발견함 (defun not (x) (if x false true) 예시 참고) 누군가 괄호를 사용하기 시작하면 본능적으로 맞게 닫혀 있는지 확인하게 됨 이후에 ""괄호가 균형을 이루는지 컴퓨터로 프로그램하기 쉽다""고 언급한 것을 보고 크게 웃음 이런 농담이 정말 재미있었고, ""Basic Number Theory"" 섹션에서 ; After a while, you stop noticing that stack of closing parens. 코멘트도 인상적임 오랜만에 Lisp를 다시 접했는데 글이 정말 재미있었음
          + 이 글이 정말 흥미로움 아직 도입부밖에 안 읽었지만, Peano 공리계(PA) 안에서 Goodstein 수열의 모든 특수한 경우가 0에 도달한다는 사실은 증명할 수 있지만 ‘모든’ 경우가 0에 도달한다는 것은 증명할 수 없는 점이 흥미로움 (사실 당연한 결과이긴 해도 흥미 진진함) 그리고 Peano 공리계만으로도 계산을 인코딩할 수 있다는 사실이 정말 이상함 (원론적으로 보면 당연하지만, 이전엔 한 번 더 자기 참조를 생각해보지 않음) 마침 최근에 집합론을 좀더 공부해보려 Intro to Set Theory 교재에서 Goodstein 수열 부분까지 학습한 경험이 있음 혹시 추천해줄만한 고급 집합론 교재나 Peano 산술 깊이 파고든 교재가 있으면 알려주면 고맙겠음 Goodstein 정리 증명에 PA가 왜 부족한지 이해하는 것이 작은 목표이지만, 대신 추천해줄만한 다른 길도 환영함
          + 글의 오메가(omega) 중 두 군데에서 \omega로 표기하는 걸 깜빡한 부분이 있음
     * Boyer-Moore 이론과 매우 유사함 이 이론은 Peano 공리계 수준에서 수학을 쌓아 올리는 것 Boyer와 Moore는 이 이론을 구현한 자동 정리 증명기를 개발했고, GNU Common LISP로 작동하는 사본도 가지고 있음 ""A Computational Logic"" 논문과 nqthm 구현물 링크도 참고 가치 있음 논문에서 예로, Peano 공리로 시작할 때 소수의 곱셈처럼 복잡한 정리는 어렵지만, 덧셈의 교환법칙, 곱셈의 분배법칙, GCD 함수 관련 정리 등은 충분히 증명 가능하다는 내용이 인상적임
     * 나는 수학과 프로그래밍 모두 배경이 있는데, 개인적으로 Goodstein 정리의 독립성 부분을 자기 지시적으로 우회 가능하다는 점이 더 흥미로움 PA+""PA가 오메가 일관적(omega-consistent)이다""라는 전제를 추가하면 Goodstein 정리를 증명할 수 있지 않을까 생각됨, 그리고 이는 epsilon_0까지 초한귀납법(transfinite induction)도 가능하지 않을까 추측함 EDIT: 어쩌면 ""PA가 일관적이다""만으로도 충분하지 않을까?
          + 아쉽게도 그렇지 않음 그리고 Con(PA) 뿐 아니라 임의의 보편적으로 수량화된 공식만 가지고도 충분하지 않다는 사실이 있음 관련 Math StackExchange 답변 참고 첫 번째 질문과 관련해, 오메가 일관성을 PA의 공식으로 어떻게 인코딩하는지 궁금함 바로 떠오르지 않아 호기심이 생김
          + 내가 Stack Overflow에서 질문자임 질문에 몇 가지 답변 링크를 추가로 적어 두었음 본질적으로 ""PA가 일관적이다""만으로는 충분하지 않지만, ""PA에서 증명되면 참""이라는 일종의 ""균일 반영 원리(uniform reflection principle)""가 있으면 충분함 이 원리가 오메가 일관성과 완전히 같다고 확신할 수는 없지만, 위키피디아의 설명 상으론 같다고 읽힘 T가 오메가 일관적이면 ""T + RFN_T + 모든 참인 공식의 집합이 일관적""이라는 의미이고, 이는 ""T + RFN_T가 참""이라는 것과 동치로 해석됨
          + 나는 이런 귀납(재귀) 구조도 마음에 듦 결국 PA가 어떤 걸 증명하는지에 대한 메타-증명을 만들고, PA를 신뢰한다면 이 메타-증명도 신뢰할 수 있다고 생각함 단순히 ""PA가 일관적이다""로 충분하다는 부분은 확실하게 이해되진 않음 내 생각에 PA+""PA 일관성""은 표준 자연수 범위에서는 Goodstein 정리가 참인 모델이 존재하지만, 동시에 어떤 비표준 정수 N에 대해 Goodstein 정리가 거짓인 모델도 존재하게 허용할 것 같음 오메가-일관성의 더 강한 명제로 이런 케이스를 배제할 수 있다고 봄
          + math exchange 게시글에선 PA+초한귀납법(epsilon_0) 증명이 PA 자체를 증명한다고 언급함 내 생각에 PA+""PA 일관성""이면 epsilon_0까지 초한귀납법을 증명 가능할 것 같음
          + 이제 이 주제는 내가 편하게 발언할 수 있는 디테일을 넘어가는 것 같음 ChatGPT에 따르면 ""PA+PA 일관성""만으로는 충분하지 않다는 설명임 ChatGPT가 워낙 논리학 책들을 많이 소화했을 테니, 그런 주장이라면 믿을 수 있을 듯함
     * Stack Overflow에서 JoJoModding에게 쓴 댓글이 올바르지 않음 ""비표준 PA 모델에는 무한한 자연수가 있기 때문에 PA가 자신이 어떤 증명을 만들었는지를 증명해도 그것이 유한한 길이라는 것을 증명하지 못한다""고 했지만, 실제로 PA가 ""PA가 X를 증명했다""고 증명하면 PA가 X 자체도 증명함 중요한 점은 비표준 모델 존재가 아니라, 표준 자연수 모델이 PA의 모델이라는 점임 그래서 ""PA가 X를 증명함을 증명하면"" 실질적으로 표준 유한 자연수에 해당하는 증명이 하나 생성되며, 이 수를 이용해 PA 내에서 X에 대한 실제 증명을 만들 수 있음
          + 논리를 더 자세히 검토해볼 시간은 없지만, 자연어로 보면 ""PA가 'forall n, G(n)'을 증명함""이 아니라 ""PA가 'forall n, Provable(G(n))'을 증명함""이라는 차이가 핵심임 논리에 약한 편이라 누군가 왜 ""forall n, Provable(P(n))""을 증명한다고 해서 ""Provable(forall n, P(n))""을 증명할 수 없는지, 구체적으로 설명해주면 고맙겠음
          + ""PA가 'PA가 X를 증명한다'를 증명하면 PA가 X를 증명한다""는 명제는 맞지 않음 PA로 모든 가능 증명을 검색하는 함수를 만들 수 있고, 실제로 답변 마지막 부분에 이 방식을 스케치했음 이로부터 will-return, opposite-return 같은 함수도 만들 수 있고, 이는 할팅 문제 표준 증명과 일치함 opposite-return(opposite-return, opposite-return) 케이스를 생각해 보면, PA가 opposite-return이 리턴함을 증명하면 실제로 리턴하지 않는다는 것 역시 증명함, 반대로 리턴하지 않음을 증명하면 리턴함을 증명함 만약 PA가 ""증명 가능한 것은 모두 실제로 증명가능하다""는 식의 더 강한 명제를 채택하면 이는 곧 Gödel의 2차 불완전성 정리로, PA가 모순임을 뜻하게 됨 따라서 단순히 ""PA가 증명함""과 ""PA가 자신이 증명함을 증명함""은 반드시 구분되어야 함
     * 순수 람다 계산법만으로도 충분함 람다 계산법 자체가 계산을 인코딩하기 때문임
     * 누군가와 귀납적 데이터 타입에 대해 얘기하다 zero/succ로 만든 Nat 정의(Lean이나 Rocq에서 볼 수 있음)를 보여줌 상대는 ""이것만으로 충분한가?"", ""Peano 공리계가 필요한가? 귀납적 데이터 타입보다 더 원시적인 무언가가 있는가?"" 등 궁금증을 가졌음 Peano 공리계가 본질적으로 당연하다 생각하지 말고, 하나의 설계일 뿐이라는 점을 자주 되짚어 보는 것이 좋다고 생각하게 됨
          + ""귀납 데이터 타입보다 더 근본적인 게 있는가?""에 대한 답변으로, 자연수 자체가 귀납 데이터 타입보다 더 원시적이라고 생각함 모든 귀납 데이터 타입은 자연수와 그 외 소수의 원시적 타입 생성자(Π, Σ, =, Ω)만 있으면 만들 수 있음
     * Kirby-Paris 정리에 대한 Q&A 참고
     * PA 일관성 관련해서, PA 내에서 증명 가능하다는 영상 자료 공유 유튜브 링크
          + 비논리학자들에게 필수로 설명이 필요한 부분임 Gödel의 2차 불완전성 정리에 따르면 만약 PA가 자신의 일관성을 증명할 수 있다면 PA는 모순이 됨(거짓도 포함해 무엇이든 증명 가능해짐) 본 영상은 PA가 모순임을 증명하지 않고, PA가 보다 약한 의미에서 ""자신의 일관성""을 증명할 수 있음을 보임 논리학의 기초를 알지 못한다면 정확히 이해하긴 어려운 내용이지만 충분히 흥미로움
     * 이 주제가 123 포인트를 얻고 있는데, 정작 SO에 올라온 본문은 11업로드밖에 없음
          + Stack Overflow는 15 포인트가 있어야 업보팅 가능함 평판 문제 때문에 사람들이 글을 올리고 싶어하지 않기도 하고, 15포인트 제한이 업보팅을 막는 것 같음
     * 컴퓨테이션 만으로 충분한가? 계산 가능한 실수는 실수 전체의 부분집합임
          + ""실수""란 이름 자체가 오해를 불러일으키는 명칭이라고 생각함 실수는 실제 물리적 비율(비)를 의미한다고 해석할 수도 있음 예를 들어, 180.255파운드라고 하면, 존스의 실제 몸무게와 표준 파운드 사이의 실제 물리적 비율이라는 의미임 이러한 비율도 물리적으로 존재한다는 것 즉, 실수는 물리적 비로 해석될 수 있음 반면 지금은 숫자를 현실과 분리된 추상적 개념으로 보는 입장이 우세하고, 이는 대표적으로 플라톤주의적 견해임 하지만 현실에서는 무한정한 정밀도로 무엇을 측정하거나 표현하는 것은 불가능함 예컨대, 50자리 이상의 정밀도 측정은 양자역학적 제한으로 불가능함 만약 태양계 천체의 궤도를 오차 없이 측정하려면 50자리 이상으로 가면 인접 별의 질량효과, 100자리 이상은 은하 전체를 모델링해야 하며, 결국 실측 불가능한 물리적
            영향까지 고려해야 함 그래서 현실에서는 유한한 정밀도의 수학만이 가능함 즉, 원리상 모든 것은 계산 가능하지만 무한대로 갈수록 수학적 모델 자체도 무의미해짐
          + 정말 그런가? 사실 비가산(uncountable)이 더 많다는 개념은 오해에서 출발함 내 설명 글 참고 비가산이 많다는 걸 받아들이면 ""존재""에 관해 논쟁적 입장을 채택할 수밖에 없음 관련 토론 참고 마지막으로 우리가 끝까지 논리 전개를 해도, 컴퓨터로 모두 표현 가능함 ZFC에 큰 집합들을 더해도 PA로 출발해 임의의 결론까지 모두 추론 가능하기에 실질적으로 PA만으로 충분하다고 판단함 좀 더 설득이 필요하다면 Gödel, Escher, Bach 책을 추천함
          + 내 방식은 조금 다름 실수 일반은 계산, 기호화, 기록 등 어떤 형태로도 다룰 수 없지만, 실수에 대한 ‘명제’ 자체는 유용하게 표현하고 계산할 수 있는 경우가 많음 Harvey Friedman이나 이번 글의 저자처럼 단순한 체계에서 상상 이상의 복잡한 값을 만들어내는 시도가 정말 흥미로움 (참고: audiomulch 웹사이트는 접속 안 됨)
          + ""충분하다""에 어떤 목표가 붙지 않는 한 이 질문은 정의가 모호함 무엇을 위해 충분한지가 중요함
          + ""컴퓨테이션만으로 충분한가?""라는 질문 자체가 틀렸다고 생각함 당연히 충분하지 않음, 만약 충분하다면 현실을 믿기 쉬운 시계태엽처럼 여기는 쪽 일부의 생각이 모두 맞아버림 현실은 그보다 훨씬 흥미롭고 복잡함
"
"https://news.hada.io/topic?id=21521","ChatGPT vs 회계사, 스타트업 기업가치평가 AI 답변 실시간 검증","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ChatGPT vs 회계사, 스타트업 기업가치평가 AI 답변 실시간 검증

   ChatGPT vs 회계사, 스타트업 기업가치평가 AI 답변 실시간 검증

   안녕하세요, 창의회계법인입니다. 최근 ChatGPT를 비롯한 AI 도구들이 스타트업 기업가치 평가에 대한 조언을 제공하고 있습니다. 하지만 실무 경험이 풍부한 전문 회계사의 관점에서 볼 때, AI 답변에는 몇 가지 아쉬운 부분들이 있어 이를 보완하고자 합니다.

   AI는 잘못된 의사결정을 할 정도로 잘못된 의견을 주지는 않았지만, 의사결정에 필요한 충분한 정보를 얻기 위해서는 전문가의 보완적 조언이 필요하다는 것이 이번 검증의 핵심 결론이었습니다. 이에 더 자세한 내용은 아래 유투브를 통해 정리해 보았습니다.

   ChatGPT의 강점은 일반적인 밸류에이션 이론과 방법론을 체계적으로 제시하고, 다양한 평가 방식에 대한 포괄적인 설명을 제공하며, 접근하기 쉬운 형태로 정보를 정리한다는 점입니다. 다만 전문가가 보완하고 싶은 부분으로는 실제 투자 현장에서의 미묘한 뉘앙스와 협상 포인트, 업종별 단계별 특성을 반영한 구체적인 적용 방법, 그리고 최신 투자 트렌드와 시장 상황을 고려한 현실적 조언이 있습니다.

   ChatGPT가 제시한 이론적 틀은 매우 유용하지만, 실제 VC 미팅이나 M&A 협상에서는 다음과 같은 실무적 고려사항들이 더욱 중요하게 작용합니다.벤치마킹 기업 설정 시에는 AI가 일반적인 방법론을 제시하지만, 실제로는 투자자마다 선호하는 벤치마킹 방식이 다르며, 때로는 창업자가 생각하지 못한 각도에서 비교 기업을 제시하는 경우가 많습니다. 멀티플 적용 시에도 이론적 범위는 맞지만, 실제 적용에서는 팀의 경험, 시장 진입 장벽, 경쟁사 동향 등 정성적 요소들이 멀티플 결정에 더 큰 영향을 미치는 경우가 빈번합니다.

   실무 전문가가 실제로 ChatGPT와 질의응답을 진행한 결과, AI는 전반적으로 거짓말을 하지 않았으며 기본적인 이론과 방법론을 정확하게 제시했습니다. 특히 스코어카드 방식, 버커스 방식 등의 밸류에이션 방법론과 VC가 주목하는 핵심 요소들에 대한 설명은 매우 유용했습니다. 다만 AI가 너무 디테일하게 설명하려다 보니 일부 오해의 소지가 있는 내용들이 있었습니다. 예를 들어, 모바일 앱 서비스의 멀티플을 5배에서 15배로 제시하며 구체적인 가격 범위를 언급했지만, 이러한 수치들은 참고사항 정도로만 받아들여야 합니다.

   AI 답변에서 특히 아쉬웠던 부분은 현실적 제약사항에 대한 고려였습니다. 예를 들어 개인사업자의 경우 법인사업자와 달리 주식 양도가 아닌 영업양수도 방식을 통해야 하므로 M&A 절차가 훨씬 복잡하다는 점, 그리고 초기 단계 스타트업의 경우 M&A보다는 투자 유치를 먼저 고려하는 것이 현실적이라는 점 등은 AI가 충분히 다루지 못한 부분이었습니다.

   ChatGPT와 같은 AI 도구는 초기 정보 수집과 기본 개념 정리에 매우 유용합니다. 다만, 실제 투자 유치나 M&A 진행 시에는 해당 분야 전문가의 경험과 직관이 함께 필요하다고 생각됩니다.

   이번 검증을 통해 AI 답변의 유용성을 인정하면서도, 실무 현장의 복잡함과 변수들을 보완할 수 있는 전문가의 역할이 여전히 중요함을 말씀드리고 싶습니다. AI가 제공하는 기본적인 프레임워크 위에 실무 경험을 더해 보다 정확하고 현실적인 기업가치 평가가 가능할 것으로 생각됩니다.

   데이터는 냉정하고, 인간은 뜨겁다
   창의회계법인이 주장하는 ""전문가의 경험과 직관""이 과연 기업가치 평가의 정확성을 높이는가? 오히려 객관적 데이터에 주관적 노이즈를 섞어 넣는 것은 아닌가?
   수학은 거짓말하지 않지만, 수학을 해석하는 인간은 거짓말할 수 있다.
    1. 전문가 편향의 함정들
       확증편향 (Confirmation Bias): 전문가들은 자신의 과거 경험에 부합하는 데이터만 선별적으로 강조하는 경향이 있다. ""우리가 봐온 케이스들을 보면..."" 이라는 식의 접근은 결국 제한된 샘플에 기반한 일반화의 오류다.

   앵커링 효과: 첫 번째 만난 유사 케이스가 이후 모든 판단의 기준점이 된다. 데이터가 5-15배 멀티플을 제시하면, 전문가는 ""경험상 8-12배가 적절하다""며 자신의 앵커 포인트로 조정한다.

   생존자 편향: 성공한 케이스들만 기억에 남고, 실패한 케이스들은 ""예외적 상황""으로 치부한다. 이는 패턴 인식을 왜곡시킨다.
    2. 이해관계의 그림자
       전문가들에게는 숨겨진 인센티브가 있다:

   수수료 극대화: 더 높은 밸류에이션을 제시할수록 거래 규모가 커진다
   관계 유지: 고객이 듣고 싶어하는 답을 제공하려는 유혹
   전문성 과시: 복잡한 조정 요소들을 만들어내어 자신의 가치를 증명하려 한다
   AI는 수수료를 받지 않고, 인맥을 관리할 필요도 없으며, 자존심도 없다.
    3. 과거 경험의 덫
       ""실무 경험 20년""이라는 말의 실체는 무엇인가?

   1년 경험을 20번 반복한 것일 수도 있다. 특히 빠르게 변하는 스타트업 생태계에서는 5년 전 경험도 이미 구식이 될 수 있다. 반면 AI는 실시간으로 전 세계 데이터를 업데이트하며 패턴을 학습한다.
    4. 정성적 요소의 허상
       ""팀의 경험, 시장 진입 장벽"" 같은 정성적 요소들이 정말 전문가만 평가할 수 있는가?

   팀 경험: 창업자 이력, 이전 회사 성과, 학력 등은 모두 정량화 가능한 데이터
   시장 진입 장벽: 특허 수, 규제 현황, 경쟁사 분석 등 역시 객관적 지표들
   경쟁사 동향: 오히려 AI가 실시간으로 더 정확하게 추적 가능
   전문가들이 ""정성적""이라고 포장하는 것들 중 상당수는 실제로는 정량화 가능한 요소들을 게을리 분석한 결과일 수 있다.
    5. 시장 효율성 관점
       만약 전문가들의 조정이 정말 가치가 있다면, 왜 전문가들이 운용하는 펀드들이 시장 평균을 지속적으로 넘어서지 못하는가?

   워렌 버핏조차 ""대부분의 투자자들은 그냥 인덱스 펀드에 투자하는 게 낫다""고 말했다. 이는 전문가의 ""직관""이 시장의 집단지성보다 우월하지 않다는 반증이다.

   결론: 차가운 데이터의 온기
   전문가의 역할을 완전히 부정하는 것은 아니다. 하지만 객관적 데이터 위에 주관적 해석을 덧씌우는 것이 항상 개선인지는 의문이다.

   때로는 차가운 데이터가 뜨거운 직관보다 더 정확할 수 있다. 특히 감정과 이해관계가 복잡하게 얽힌 기업가치 평가에서는 더욱 그렇다.

   ""전문가의 보완""이라는 이름으로 포장된 것이 사실은 ""객관성의 훼손""일 수도 있다는 점을 간과해서는 안 된다.

   라고 AI에게 반박 글을 써보라고 하였습니다. 제가 일하는 직군에서도 AI가 정말 핫한데 어떻게 쓰여질지 매일매일 상황이 바뀌네요.

   Show GN에 맞지 않아 이동하였습니다.
   Show 사용방법 을 참고해서 올려주세요.
"
"https://news.hada.io/topic?id=21438","Apple 최신 플랫폼 업데이트의 좋은 점, 나쁜 점, 그리고 이상한 점","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Apple 최신 플랫폼 업데이트의 좋은 점, 나쁜 점, 그리고 이상한 점

macOS

     * 가장 좋은 점: Spotlight 기능 강화
          + 파워유저를 위한 강력한 검색 기능 확장에 긍정적 평가
          + 서드파티 런처를 대체할 만한 도구로 발전하는 점이 고무적임
     * 가장 아쉬운 점: 사라지는 메뉴 바
          + macOS 경험에서 메뉴 바의 중요성을 간과한 변화로 비판
          + 과거 투명 메뉴 바 논란(2007년)과 유사하다는 언급

iOS

     * 가장 좋은 점: 메시지 스팸 필터링
          + 스팸 메시지 차단 기능이 드디어 내장됨
          + 기존에도 서드파티 필터가 있었으나 실제로 잘 사용되지 않던 점을 개선
     * 가장 아쉬운 점: CarPlay Tapbacks
          + 운전 중 Tapback(이모지 반응) 기능의 실효성에 의문
          + 운전자의 주의 분산 위험을 우려

iPadOS

     * 가장 좋은 점: 윈도우 관리 개선
          + 자유롭게 윈도우를 배치/타일링할 수 있게 되어, 오랜 멀티태스킹 실험의 결실로 평가
          + Expose와 윈도우 타일링 지원으로 진정한 생산성 강화 기대
     * 가장 아쉬운 점: Games 앱
          + 추천 게임 중심의 화면이 실질적으로 광고처럼 느껴짐
          + 기존에 이미 플레이한 앱을 보여주는 점은 긍정적이지만 전체적으로 큰 감흥은 없음

watchOS

     * 가장 좋은 점: 위젯 설정 자유
          + 날씨 위젯 등에서 보고 싶은 정보만 선택적으로 노출 가능
          + UV지수 등 사용자 맞춤 정보를 원하는 대로 배치할 수 있게 됨
     * 가장 아쉬운 점: Workout Buddy
          + 로봇 코치의 격려가 실제 사람만큼 동기부여를 주지 못함
          + Nike Run Club 등 실제 사람 코치와 비교해 부족함

visionOS

     * 가장 좋은 점: 위젯
          + 공간 내에 위젯을 자유롭게 배치할 수 있는 기능이 눈에 띔
          + 공간 컴퓨팅이라는 새로운 사용성에 충실한 방향
     * 가장 아쉬운 점: Image Playgrounds

tvOS

     * 가장 좋은 점: tvOS의 존속
          + 신기능보다 플랫폼 자체가 유지되는 것에 의미를 둠
          + Apple TV는 자주 사용하지만, 애플이 상대적으로 덜 신경 쓰는 제품
     * 가장 아쉬운 점: Sing in Apple Music with iPhone
          + iPhone으로 Apple Music에서 노래 부르기 기능은 불필요하게 느껴짐
          + 불협화음 노래가 사운드바로 증폭될 필요는 없음
"
"https://news.hada.io/topic?id=21458","애플의 "추론 LLM의 한계" 논문에 대한 7가지 반박과 그 한계들 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 애플의 ""추론 LLM의 한계"" 논문에 대한 7가지 반박과 그 한계들

     * 애플의 생각의 환상: 추론 LLM의 한계 이해하기 논문이 AI의 스케일링 가설에 문제를 제기하며 큰 반향을 일으킴
     * 이에 대한 대표적 반박 7가지가 있었으나, 이 글의 저자 Gary Marcus(NYU 명예교수)는 모두 설득력이 떨어진다고 평가함
     * “인간도 실수한다”, “출력 길이 한계”, “논문 저자가 인턴” 등 논점 흐리기, 본질 회피의 논거가 주를 이루고, 근본적 취약점 해결에는 미치지 못함
     * “코드 사용으로 문제 해결” 등 일부 지적은 의미 있으나, 신경-기호 AI의 필요성만 더 부각시킨다는 결론임
     * 최근 SalesForce 연구 결과도 실제 비즈니스 시나리오에서 LLM의 복잡한 멀티턴 추론 성능이 35%에 불과함을 보여, 애플 논문의 우려와 일치함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

애플 추론 논문 반박 7가지와 그 한계

  서론

     * 애플의 Illusion of Thinking: 추론 LLM의 한계 이해하기 논문은 대형 언어 모델의 추론 및 알고리듬 수행 한계를 드러내며 업계와 언론, 학계에서 큰 주목을 받음
     * 글쓴이 Gary Marcus가 요약한 논문 해설 포스트 15만 명 이상이 읽었음
     * The Guardian은 관련 포스트를 참조한 칼럼을 게재했고, ACM 및 프랑스어 버전도 등장해 글로벌한 관심을 증명함
     * 이에 대해 GenAI 옹호자들이 논문에 비판적 반응을 보이고 여러 반박을 제기했으나, 모두 근본적인 반박이 되지 못함

  1. “인간도 복잡한 문제와 기억 요구에서 어려움을 겪는다”

     * 인간도 어려워한다는 주장 자체는 사실이지만, 애초에 컴퓨터·AI를 만든 이유는 인간이 할 수 없는 계산·반복 작업을 정확히 처리하기 위함
     * 예시로, Tower of Hanoi 퍼즐에서 기존 심볼릭 AI 시스템은 오류 없이 수행 가능
     * AGI라면 오히려 진보된 성능을 보여야 하며, 단순히 인간과 유사한 실수 범주에 머무는 것은 한계로 볼 수 있음
     * Apple 논문의 핵심은 LLM이 복잡성과 학습 분포에서 멀어질수록 제대로 된 알고리듬 수행을 신뢰할 수 없음을 밝힘
     * “인간도 실수한다”는 논점 흐리기임

  2. “LRM은 출력 토큰 수 제한 때문에 풀 수 없다”

     * LRM(대형 추론 모델)은 출력 길이 제한이 있으나, 사례 중 일부(예: 8개 디스크의 Hanoi, 255단계)는 충분히 출력 가능 범위임
     * 잘 설계된 심볼릭 AI는 이런 문제의 영향을 받지 않으며, AGI 역시 마찬가지여야 함
     * 토큰 한계는 버그이며, 해결책으로 볼 수 없음
     * 기본적인 알고리듬도 신뢰성 있게 실행 못하면 현실 문제(군사 전략, 생물학 등)는 더더욱 불가능

  3. “논문 저자가 인턴이다”

     * Ad hominem(인신공격) 에 해당, 본질과 무관. 과학적 관행을 무시한 오류임
     * 실제로 저자는 유망한 Ph.D. 학생이며, 논문에는 총 6명(4명은 Ph.D. 보유, Samy Bengio 등 저명 연구자 포함)
     * 저자의 지위와는 별개로 논문의 품질이 핵심

  4. “더 큰 모델이면 잘할 수 있다”

     * 일부 더 큰 모델에서 개선된 모습이 보고되나, 어떤 크기가 충분한지 예측도 불가
     * 같은 구조의 LRM에서도 디스크 6개에는 성공, 8개에서는 실패하는 등 일관되지 않은 결과가 산출됨
     * 모델 신뢰성과 예측 가능성 결여, 항상 모든 문제에서 사전 검증 필요 → AGI와는 거리가 멂

  5. “코드를 쓰면 문제를 풀 수 있다”

     * 일부 LLM은 코드를 통해 문제를 해결 가능하나, 이는 뉴로심볼릭 AI의 장점임
     * 진정한 의미의 AGI/AI라면 코드 없이도 개념적 이해 기반의 추론 및 역추적이 가능해야 함
     * 시험이 학생의 개념 이해를 평가하듯, LLM도 진정한 개념적 이해가 필요한 상황임

  6. “실험이 4개 예시뿐이고, 하노이 문제도 완벽하지 않다”

     * 논문 내 4가지 예시 모두 완벽하지 않을 수 있으나, 다양한 선행 연구 결과와 일치하며, 유사 실패 사례는 계속해서 보고됨
     * NYU의 Tal Linzen 등도 해당 맥락의 한계를 추가 증명함

  7. “이미 다 아는 사실이다”

     * 많은 연구자들은 오래전부터 LLM의 일반화 취약성을 인지하고 있었음
     * 하지만 대중적·산업적 맥락에서 이번 논문으로 인해 관심이 집중되고 있음을 주목할 필요
          + 그간 과대평가/과장되어 왔던 AGI 가능성에 대해 업계가 본격적으로 주목하고 논의하는 계기가 된 점이 중요
     * 연구자 사이에서도 “틀렸다”와 “이미 알던 사실”이 동시에 언급되는 모순적 반응이 나타남

  결론

     * 이상의 반박들 중 결정적으로 설득력 있는 내용은 부족함
     * Apple 논문은 스케일 확장이 AGI의 해답이 아니라는 분명한 신호를 재차 제시함
     * 현 LLM 기술은 신뢰성, 일반화, 개념적 추론에서 명확한 한계가 드러남
     * 실제로 Sam Altman 등 주요 인물도 현재 상황을 심각하게 받아들이는 분위기 형성

SalesForce 논문과 추가적 수렴 증거

  Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions

     * SalesForce의 최신 논문에서 실제 비즈니스 시나리오(고객 영업, 서비스, B2B/B2C 등) 기반 LLM 평가 벤치마크 공개
     * 단일턴(1회 질문-응답) 기준 성공률 58%, 멀티턴(연속 질의응답) 기준 성공률 35%로 급락
     * 특히 워크플로우 실행은 83% 이상 성과를 내지만, 다중 추론/상황 전환 등에서는 한계
     * 기밀성 인식(Confidentiality awareness) 도 거의 없음, 프롬프트로 개선 가능하나 성능 저하 동반
     * 실제 기업 환경의 복잡성·현실성 요구에 비해 LLM의 한계 명확, 멀티턴 추론·기밀성·다양한 업무 스킬의 통합 필요성이 부각

요약

     * 애플 논문과 SalesForce 논문 모두 현세대 LLM이 실제 복잡한 추론, 멀티턴 대화, 알고리듬 수행 등에서 심각한 한계를 드러냄
     * AGI에 다가가기 위해서는 스케일링을 넘어 신경-기호 통합, 구조적 개선이 필요
     * 업계와 연구자들이 본격적으로 한계 논의에 주목하기 시작한 것이 의미

   알트만은 그의 에세이에서 ""10년 후면 어쩌면 우리는 고에너지 물리학을 푸는 해에서 다음 해에는 우주 식민지화를 시작하는 해로 나아갈지도 모른다""라고 썼습니다. 그는 뇌-컴퓨터 인터페이스를 통해 AI에 직접 ""연결""하려는 사람들은 삶이 근본적으로 바뀌는 것을 보게 될 것이라고 덧붙였습니다.
   이러한 수사는 우리 사회 곳곳에서 AI 도입을 가속화하고 있습니다. AI는 현재 DOGE(총리실)에 의해 정부를 재편하는 데 이용되고, 군대는 이를 더욱 치명적으로 만들기 위해 활용하고 있으며, 종종 알려지지 않은 결과를 초래하며 우리 아이들의 교육을 맡고 있습니다.
   즉, AI의 가장 큰 위험 중 하나는 우리가 AI의 능력을 과대평가하고, AI가 ""기회주의적 협박""과 같은 반사회적 경향을 보이는 것으로 드러났음에도 불구하고 필요 이상으로 신뢰하며, 현명하지 못한 정도로 AI에 의존한다는 것입니다. 그렇게 함으로써 우리는 가장 중요한 순간에 AI가 실패할 가능성에 취약해집니다.
   ""AI를 사용하면 다양한 아이디어를 낼 수 있지만, 여전히 상당한 감사가 필요합니다.""라고 오르티즈는 말합니다. ""예를 들어 세금 보고서를 작성하려면 ChatGPT보다는 TurboTax와 비슷한 도구를 사용하는 것이 좋습니다.""

   Why Superintelligent AI Isn't Taking Over Anytime Soon, WSJ 기사를 발췌했습니다

        Hacker News 의견

     * 인간이 복잡한 문제와 기억 부담에서 어려움을 겪는다는 점은 맞지만, 그게 전부는 아니라는 주장 제시. 기계가 인간보다 더 뛰어난 결과를 내줄 것으로 기대한다는 점 강조. 만약 인간도 이러한 실수를 한다고 인정하지만, 동시에 “사고 능력”의 정의에는 이 능력이 필요하다고 고집한다면, 결국 인간의 사고 자체도 환상이라는 결론이 된다는 생각 공유
          + 나도 공감하지만 AGI 관련 부분은 잘못된 주장이라고 본다. 평균적인 인간과 같은 수준으로 모든 작업을 할 수 있는 AI가 바로 AGI의 정의라는 견해
          + 양쪽 주장 모두 명쾌하지 않다고 느낀다. 질적인 질문에 대해 양적인 답만 오간다는 생각
     * Apple의 논문과 Gary Marcus 비판에 대한 좋은 분석글이라는 평. 더 자세한 논의로 LessWrong의 관련 글 추천
          + 진심으로 궁금한 점 언급: Gary Marcus의 의견이 여전히 유효한지 의문. 그의 비판은 과학적이라기보단 철학적 경향으로 느껴지며, 실질적으로 무엇을 만들어내거나 논리가 검증되는지 보기 어렵다는 생각
          + lesswrong.com에 대해선, 특정 인물(예: Yud)의 사상을 추종하는 집단이라 크게 신뢰하지 않는다는 입장 표현
     * LLM이 과거에 학습한 유사한 해결책이 있을 때는 '추론'처럼 보이는 결과를 낼 수 있지만, 완전히 새로운 문제에서는 무너진다는 인사이트 공유. 엄밀한 의미의 추론은 아니지만 실용상 상당히 유용한 수준. 반복적으로 솔루션을 꺼내는 능력도, 마치 사실 확인을 반복 제공하는 것처럼 꽤 유용하다고 본다. Marcus는 기술적으로는 맞는 지적을 하지만 설명보다는 감정적 논조에 치우쳐 있다는 점 지적
          + 만약 유사 솔루션 반복이 정말 그렇게 잘 된다면 대단하겠지만, 실제로는 이런 도구들이 흔히 같은 솔루션도 제대로 반복하지 못하고, 심지어 그럴듯한 결과들을 즉흥적으로 지어내기(환각 현상) 때문에, 사람이 따로 꼼꼼히 검증해줘야 하는 불편함이 크다는 경험 공유
          + 그 정도만 제대로 돼도 혁신일 테지만, 여전히 꿈만 같은 이상론에 그침. 최근 Gemini가 아주 기본적인 교과서 문제에서조차 좌우를 헷갈리는 답변을 한 경험 언급
          + “LLM은 그냥 앵무새다” 류의 반복되는 주장이 지겹다는 생각. 내 경험상, LLM은 훈련 데이터에 없던 완전 새로운 문제도 추론하고 해결할 수 있다고 본다. 정말 다양한 경우 테스트해봤고 관련 사례도 많다. 상호 작용자들에 대한 답변을 한 번에 정리하자면, “추론”과 “새로운 문제 풀이”의 정의부터 명확히 할 필요. 개인적으로 추론을 범주로 보고, 일반 지능과 동일하지 않다고 본다. LLM이 어려운 문제를 항상 못 푼다고 해서 추론 자체가 불가능하다는 의미는 아니라고 본다. 내 생각에 LLM의 추론 능력이 전반적으로 약하기는 하지만, 전혀 추론도, 새로운 문제도 못 푼다는 주장에는 동의하지 않는다.
              1. Next token prediction 자체가 추론이 필요한 작업이라는 주장 가능
              2. 전혀 없는 가상의 언어로 번역시키는 다양한 실험도 성공적. in-context learning, zero-shot 관련 연구 많음
              3. 추론 능력을 검증하려고 온갖 챌린지/게임/퍼즐이 시도됐지만, 결국 하나씩 LLM이 이를 해결하는 케이스 존재 (예: Monty Hall problem 퍼즐, 이전의 다른 퍼즐 예시), 심지어 퍼즐 공개 이전에 학습된 모델들도 있음
              4. out-of-context reasoning 관련 연구들도 다수 존재 (예: arxiv 논문) 추가 반박 포인트로,
              5. 모델이 어느 정도 복잡도 임계점에서 실패하더라도, 최신 모델들이 이런 어려운 퍼즐을 어느 정도 푼다는 점이 이미 굉장히 인상적. GPT-3.5에서는 못하던 것을 최신 모델이 해냄. 추론 분야에서 점진적 발전 이어지는 중. 더 크고 더 똑똑한 모델일수록 zero-shot 과제에 더 잘 대응하며, 이게 추론 능력 향상과 상관관계가 있단 생각
              6. “더 큰 모델=더 나은 성능” 주장에 대한 논문 자체의 데이터가 존재. Claude 3.7 모델은 DeepSeek보다 훨씬 좋은 성능을 보이고 긴 시퀀스 내내 안정적으로 풀이 유지. 더 나은 모델, 더 많은 토큰이 있으면 중간 난이도 문제에선 빠르게 성과 상승. “어려운 문제”만 못 푼다고 해서 결코 추론 불가로 볼 수는 없음. 몇 년 전엔 중간 난이도도 안된다고 했었지만, 지금 이미 판이 바뀌고 있다는 점 강조
          + 그건 오히려 추론의 반대라는 견해. AI 옹호자들이 LLM이 마치 똑똑하거나 추론한다는 식으로 주장하려 하지만, 실제로는 창의적이거나 지능적인 추론 불가능. 진짜 추론이란 아예 본 적 없는 문제에서 혁신적인 해결법을 스스로 찾아내는 능력 의미. LLM은 단지 데이터에 있던 해법만을 확률적으로 뽑아 낼 뿐이며, 진짜 해결책을 추정하거나 유추하는 기능은 전혀 없다는 생각
     * 많은 반론과 반박이 실제로는 허술하거나, 5번 논점에 거의 포함될 수 있다는 점을 지적. 글의 핵심은 LLM이 코드를 작성하거나 논리 시스템을 쓸 수 있는지가 문제임. 도구에 접근이 없을 때 헛된 추론(환각/오답 응답)이 과연 진짜 추론의 부재를 의미하는지, 똑똑한 인간처럼 “내가 할 수 있는 한계를 인정”하는 AI가 되는 것이 진짜 기대치가 아닐까라는 질문
          + 실제 실험 결과를 보면, 모델이 100단계까지는 출력하다가 “이 이상은 너무 많으니 풀어내는 방법만 설명하겠다”는 식으로, 한계를 명확히 인정함. 그런데 이런 응답도 오답으로 처리된 경우가 있음. 관련 링크 모델 실제 응답 예시 참고. 예를 들어 너무 복잡해지면 “[개별 시행 방식을 모두 설명하기 어려워, 대신 해결 방식을 서술하겠다]”는 방식이며, 특정 모델(Sonnet)은 7개를 넘어서면 직접적인 단계별 추론을 건너뛰고 일반적인 해결 알고리즘이나 접근법만 설명하는 식으로 동작
          + 3번을 빼고는 실상 대부분의 반론이 허술하다고 생각하지 않는다. 오히려 원 글이 많은 허수아비 논리(스트로맨)를 만들어내고 있다고 느낀다. 1번 반론이 종종 나오는 이유는 “이 논문이 LLM의 추론 불가를 증명했다”는 주장 때문. 그런데 저자는 자꾸 AGI를 논하면서 정의 자체를 허수아비 논리로 바꿈(“기계는 인간보다 더 많은 걸 해야 한다” 식). 실제 AGI 정의는 평균 인간 수준의 작업이 가능한 AI지 슈퍼 인텔리전스가 아닌데, 저자는 이를 오해. 참고로 Tower of Hanoi 같은 문제는 이미 LLM들이 평균 인간 이상의 퍼포먼스를 보임. 현실적으로 일반인은 8개짜리 Tower of Hanoi 문제를 아무것도 기록하지 않고 풀 수 없지만 LLM은 가능. 다만 진짜 AGI로 가기엔 아직 모델들이 넘어야 할 장벽이 많음. 5번 반론도 “웹에서 코드를 못 써온다”는 허수아비
            논리인데, 실제로는 신규 문제도 직접 코드를 짜서 해결한다는 예를 들 수 있음. 이런 포인트들은 논문 비판이 아니라 논문 자체 한계에 대한 사실 지적임. 이 논문은 그저 LLM의 추론적 한계만 보여줬고, 실제로 과한 주장 없이 단지 제한점을 서술한 글이었지만, 제목이 자극적이어서 사람들이 본문을 제대로 읽지 않은 경향
     * “어린이도 쉽게 푸는 퍼즐”이라는 주장에 대해, 실제로 8개 디스크짜리 하노이의 탑을 아무 기록 없이 머릿속만으로 풀기는 어렵다고 고백. 인간과 AI의 비교에서 진짜 동등 비교가 맞는지 의문 제기
     * 이런 기사들이 반가운 이유는, AI에 대한 과도한 과대 광고 열기를 어느 정도 식혀줄 필요가 있다는 점 때문. 새로운 AI 툴을 진지하게 현실에 쓸 생각이라면, 열광을 잠시 멈추고 이 기술의 진짜 한계와 실체를 냉정하게 봐야 함. 대단하고, 여러 영역에서 실용적이지만, 무분별한 붐 조장은 궁극적으로 돈벌이에 직간접적으로 연결된 이해관계자만 이득 보는 현실
          + Gary Marcus는 ""현실 직시""가 아니라, 오히려 AI 주류에 반대하는 의견으로 자기 유명세를 키우는 유형이라고 평가. 이번 글도 논리적이긴 하지만, 과거 논문에서는 LLM에게 ""치명타""라고 강하게 주장했던 데에서 자세가 바뀐 예시. 그의 글은 분위기는 합리적여 보여도 여러 편 읽으면 일관적인 경향이 보임
          + 실제로 AI에 투자하는 이들 중에도 과도한 붐은 펌프&덤프(시세 부양 후 매각)나 교육, 컨설팅 팔이 같은 사업자에게만 유리할 뿐, 진짜 혁신을 만들려는 사람들은 곧 AI 겨울(혹한기)과 마주하게 될 가능성이 높음
          + LLM에 대해 본능적으로 경계하는 입장. 지금까지 내게 코드 써준 경험 중 대다수가 질이 형편없었고, 현재는 별로 좋아하지도 않고 자주 쓰지도 않음. 하지만 시간이 지나면 상당히 유용한 도구로 발전할 거라 기대. 그러면서도 Marcus는 내 생각에 논의에 낄 자격이 전혀 없다고 생각. 그의 발언이 실질적인 논의와 비생산적 과장만 쏟아내서, 지나치게 반-AI 진영에 먹잇감을 주는 현상 발생. “respectability laundering: 그를 인용하면 타당한 비판이 되는 상황”이라고까지 평가
          + 머신러닝에서 test/train split(훈련셋/테스트셋 분리)이 뭔지 아는 사람에게서 비판을 듣고 싶음. 실제로 최근 ML 현장과 너무 동떨어진 사람이 AI 능력에 대해 말하는 건, 오히려 AI 공포심의 매우 상징적 현상이라고 생각
          + 실제로 얼마나 유용한지 의문 제기. 1년 넘게 “지식 노동 10배 생산성” 같은 주장을 했지만, 정말 그렇게 바뀐 결과물이 어디 있느냐는 문제 제기. 새로운 오피스 제품군 등장? 모바일 앱 대량 생산? 책 시장의 혁신? 결국 Ghibli 밈이나 ‘RETURNS’ 유행 콘텐츠 정도 외에 실질 생산물이 있는지 의구심 표명
     * 혹시 원 논문이 궁금하다면 원문 링크 공유
          + 조사 및 참고자료로, 논문: The Illusion of Thinking – reasoning 모델의 강점과 한계 (PDF)와 A Knockout Blow for LLMs? 논평 등도 같이 소개. 혹시 다른 자료가 있냐는 질문
     * 수학 시험에서 미분적분 문제를 내는 건, 학생이 계산 답을 내는 게 아니라 개념적 이해도를 평가하려는 것이라는 점 언급. Apple 팀도 LLM이 Hanoi 문제를 개념적으로 이해하느냐를 본 것. LLM은 정답 코드를 “다운로드”할 수 있지만, 새로운 문제나 동적 환경에서는 개념을 이해하지 못한 코드 다운로드가 한계가 있다는 논지. 하지만 실제로 LLM은 코드를 다운받는 게 아니라 직접 “작성”할 수 있는 능력 보유. 수험생이 일반 미분/적분 프로그램을 시험서 짠다면 오히려 더 높은 개념적 이해를 방증하는 셈이라는 주장
          + 만약 수험생이 LLM 파라미터에 비해 극히 적은 노트 참고만 했다면, 설득될 수 없다는 의견
     * Salesforce 논문에서 ""에이전트들이 거의 제로에 가까운 비밀 유지 능력을 보였다""는 인용구를 중요하게 여김
     * 인간이 비행기를 만들었을 때 “새가 아니다”, 잠수함을 만들었을 때 “물고기가 아니다”라는 비판이 있었지만, 진보는 계속 이루어진다는 예시 제시. 핵심은 이 도구의 잠재력을 빨리 익혀서 활용할 것인지, 아니면 뒤처질 것인지 선택의 문제. 팁으로, 같은 사람의 끝없는 부정적 주장보다는 “학습하는 태도”가 미래에 적응하는 데 더 실질적 도움이 된다는 메시지
"
"https://news.hada.io/topic?id=21557","Show GN: 한국인증채우기: 민간인증서와 휴대폰 인증을 자동으로 채워주는 크롬 확장프로그램","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: 한국인증채우기: 민간인증서와 휴대폰 인증을 자동으로 채워주는 크롬 확장프로그램

  1. 제작배경

   홈택스나 여러 공공기관 사이트를 자주 이용하면서 민간인증서나 휴대폰인증 창에 매번 같은 내용을 채워넣는 것이 귀찮아서 만들게 되었습니다.

   크롬에서 기본적으로 지원하는 autofill 기능도 있습니다만, 너무 부정확하고 팝업으로 뜨면 채워넣어주지 않는 경우도 있는 것 같아서 만들게 되었습니다.

  2. 제작할 때 고려한 점

   기존에 채워넣기를 해주는 프로젝트를 깃허브에서 여러개 찾아보았는데, 대부분 특정 사이트만 지원하거나 chrome manifest v2 를 기반으로 만들어져서, 스토어에서 퇴출된게 많았습니다.
   그래서 chrome meanifest v3를 기반으로 하면서, 모든 민간인증서나 모바일 인증에 대응하기 위한 범용적인 코드를 작성하는데 집중했습니다.

  권장 사용자

     * 여러번 민간인증서 인증이나 모바일인증을 시도해야하는 분들
     * 많은 인증시도에 짜증나셨던 분들
     * chrome autofill에 불만이 있으셨던 분들

  링크

   아래는 설치링크와 깃허브 링크입니다.
     * https://chromewebstore.google.com/detail/한국인증채우기/…
     * https://github.com/Xeonlink/korea-auth-filler

   안녕하세요. 유용한 플러그인을 만들어주셔서 감사합니다. 혹 괜찮으시다면, 식탁보 프로젝트에도 연동할 수 있게 연결해도 괜찮으실지 여쭙습니다. (https://github.com/yourtablecloth/TableCloth)

   답글 감사합니다.

   제가 깃허브에 라이센스를 명시하는 걸 깜빡했네요.
   MIT 라이센스로 배포할거라서 얼마든지 원하는 목적으로 사용하실 수 있습니다.
   감사합니다.

   댓글 달려고 가입했습니다.
   너무 편하게 잘 쓸 것 같아요. 감사합니다.

   답글 감사합니다.

   댓글 달려고 가입까지 하셨다니, 개발자로서 너무 기분이 좋네요.
   혹여나 잘 작동이 안되는 곳이 있더라도 피드백 보내주시면 경청하겠습니다. 감사합니다.

   혹시 사파리도 지원 예정에 있으십니까?

   관심가져주셔서 감사합니다.

   네, 사파리도 지원하도록 구성중에 있습니다.
   작업이 완료되어 사용이 가능해지면 링크 올리도록 하겠습니다.
   감사합니다.

   오~ 유용할 것 같습니다. 잘 쓰겠습니다

   답글 감사합니다.

   작동이 이상한 부분이나, 버그라고 생각되는 부분이 있다면
   언제든지 열린마음으로 피드백 받겠습니다. 감사합니다.

   firefox도 부탁드립니다..

   https://addons.mozilla.org/ko/firefox/addon/korea-auth-filler/

   firefox addons에 올렸습니다. 많은 분이 사용해주셨으면 좋겠네요. ㅎ

   답글 감사합니다.

   수요가 없을 것 같아서 반쯤 손놓고 있었는데,
   크로스 브라우징도 빠르게 도입할 수 있도록 해보겠습니다. 감사합니다

   감사합니다!!

   좋은 프로그램이네요!

   크롬웹스토어에 있는 사진은 실제 개인정보인 것 같아 주의해야할 것 같습니다.

   좋게 봐주셔서 감사합니다.

   개인정보 부분은 이름만 진짜고, 나머지는 더미이긴 합니다.
   그래도 혹시 모르니 수정해야겠네요. 감사합니다.
"
"https://news.hada.io/topic?id=21491","교도소에서 데이터베이스 작업하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           교도소에서 데이터베이스 작업하기

     * 교도소 수감 중임에도 불구하고, Turso의 소프트웨어 엔지니어로 입사한 경험 공유
     * 약물 및 잘못된 선택으로 감옥에 들어간 이후, 교도소 내 컴퓨터 교육 프로그램을 통해 개발자로 성장함
     * Maine 교정국의 원격 근무 프로그램을 통해 Unlocked Labs에서 원격 소프트웨어 개발자로 채용됨
     * Turso의 Project Limbo에 오픈 소스로 기여하며 데이터베이스 엔진 개발 영역에 진입함
     * 어려운 상황에서도 꾸준한 노력과 오픈 소스 참여, 커뮤니티와의 소통이 새로운 인생 경로를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

교도소에서 개발자로 성장하게 된 과정

     * 나는 최근 Turso의 소프트웨어 엔지니어로 입사하게 되었음
     * 일반적으로 데이터베이스 개발에 참여할 기회 자체가 희귀하고 특별한 일임에도 불구하고, 저자는 교도소 수감 상태에서 이 자리에 오르게 되었음
     * 2017년 이후로 소셜 미디어와 인터넷에서 완전히 단절된 상황이었으나, 자신의 이야기 블로그 게시를 계기로 IT 커뮤니티로부터 예상치 못한 많은 응원과 지지를 받게 되었음.

     * 저자가 교도소에 수감된 이유는 약물 관련 잘못된 선택과 생활 방식에서 기인함
     * 3년 전부터 교도소 내 대학 프로그램에 참여해 제한적 인터넷이 허용된 컴퓨터로 프로그래밍에 다시 몰입하게 되었음
     * 이 경험을 계기로 하루 15시간 이상 오픈 소스 프로젝트에 참여하며 기존 커리큘럼을 빠르게 넘어서게 되었음.

     * 운 좋게도 Maine 교정국 원격 근무 프로그램 초기 참가자로 선발됨
     * 해당 프로그램을 통해 요건을 충족한 수감자들은 원격 근무 기회를 탐색할 수 있었음
     * 이 과정에서 Unlocked Labs라는 교육 스타트업에 소프트웨어 엔지니어로 채용되어 재소자 교육 솔루션 구축에 참여하였고, 1년 만에 개발팀 리더로 성장함

Turso와 Project Limbo와의 만남

     * 2023년 12월, 프로젝트를 탐색하던 중 Hacker News를 통해 Turso의 Project Limbo(SQLite를 처음부터 다시 만드는 프로젝트)를 접하게 되었음
     * 나는 데이터베이스 관련 실무 경험은 없었으나 저장소 엔진에 대한 관심이 생기기 시작했고, 프로젝트가 초창기라 기여할 수 있는 많은 부분이 남아있었음

     * 교도소 내에서 할 수 있는 일이 거의 없는 상황에서 나는 거의 90시간을 개발, Kubernetes, 인프라 관리 등에 몰두함
     * 주요 엔터테인먼트는 하루 1시간의 테크/프로그래밍 YouTube 시청이었으며, The Primeagen의 이야기에서 큰 동기를 얻었음.

     * Primeagen 채널을 통해 처음부터 Turso와 해당 개발자들의 Linux 커널 배경 등을 알게 되었음
     * 나는 이 대담한 프로젝트에 의미 있는 기여를 하게 될 줄은 상상도 하지 못했음
     * 초기 PR 제출 시 커널 커미터들이 코드 리뷰를 한다는 점에 상당한 긴장감을 느꼈음.

     * Limbo 개발 참여는 곧 새로운 집착 대상이 되었음
     * 본업 외에도 SQLite 소스코드, 데이터베이스 내부 동작에 대한 아카데믹 논문, Andy Pavlo의 CMU 강의까지 몰입하며 역량을 키웠으며, Turso Discord에서도 활발히 활동하였음
     * 나의 교도소 수감 사실이 GitHub에 공개되어 있었으나, 대부분의 커뮤니티에서는 이를 인지하지 못했음

     * 몇 달 후 Turso의 Glauber가 Discord DM으로 직접 자신을 소개하며 첫 만남이 이루어짐
     * 2024년 1월, Glauber의 트윗을 계기로 The Primeagen이 나의 블로그글을 라이브로 소개함으로써, 더 많은 주목을 받게 됨.

     * 이후로도 개발자, 대학생, 중독 및 유사한 경험을 가진 이들로부터 오픈 소스 참여 조언이나 학습 경로에 대한 문의가 꾸준히 이어짐.

앞으로의 계획

     * 나는 노력, 결단력, 자기 통제력의 중요성에 대한 긍정적인 사례로 나 자신을 자랑스럽게 여김
     * Maine 교정국의 기회와 Unlocked Labs의 첫 채용이 없었다면 시작조차 불가능했음

     * 현재 Turso에서 풀타임 엔지니어로 합류한 것은 불과 몇 년 전만 해도 상상할 수 없던 변화임. 새로운 SQLite의 현대적 진화에 기여하게 되어 기대감이 큼.

     * 최근 법원의 판결로 조기 출소 기대가 무산되었으나, 10개월간 커리어 역량에 집중할 수 있다는 점을 오히려 기회로 받아들임.
     * 지금까지 응원해준 모든 이들과 공정한 채용 정책으로 두 번째 기회를 준 기업들에 감사를 전함. 최근 몇 년간 인생이 극적으로 바뀌었다는 점을 강조함.

        Hacker News 의견

     * Maine의 원격 근무 프로그램이 재범 방지에 아주 큰 가능성을 보여주는 사례라는 생각임. 정말 대단한 점은 수감자가 출소 후에도 무리 없이 계속할 수 있는 ‘진짜’ 일자리를 제공한다는 부분임. 보통 출소하면 일자리 구하기가 불가능에 가깝고, 이 때문에 시간이 흐를수록 절박함만 커져서 결국 다시 나쁜 선택을 하게 되는 악순환이 반복됨. 물론 착취의 위험이 실제로 있지만 제대로 관리만 된다면, 수감자 원격 근무처럼 희망적인 소식이 교정 시스템에서 또 있을까 싶은 생각임. 수감 중에도 삶의 목적을 찾을 수 있고, 출소 뒤 성공의 길도 마련되는 구조임
          + 북유럽 국가들이 교정과 재범 방지 측면에서 이미 옳은 방향을 오래전부터 제시했다는 사실 확인임. 응보 일변도가 아니라 재활에 초점을 맞추면 재범률이 확연히 낮아진다는 점, 그리 놀랄 일도 아니라는 생각임
          + 이런 시스템 정말 놀랍게 느껴짐. 한편, 비폭력적 마약 범죄로 이렇게 오랫동안 수감되는 현실은 말도 안 되는 상황이라고 생각함. Turso가 소규모 Payload 사이트에 아주 잘 어울리는 듯한 인상도 받음
          + 수감자들이 실제로 ‘진짜’ 임금을 받는지 궁금함
          + 이 제도가 정말 좋은 방향임에는 동의하지만, 형벌의 모든 목적—응보, 교정, 억지—를 모두 고려하는 자세가 필요하다는 생각임. 응보는 정의 실현이고, 교정은 범죄자를 다시 사회로 복귀시키는 자비롭고 현명한 결과이며, 억지는 잠재적 범죄자에게 현실적인 경각심을 준다는 의미임. 현대 교정 시스템에선 교정이 무시되고, 그래서 수감자를 방치하거나 제대로 된 교정 없이 사회에 내보내 재범 위험까지 높이는 상황임. 반대로 범죄자에 대한 잘못된 연민에서 충분한 정의 실현조차 하지 않으면 억지 효과도 상실하게 됨
          + 만약 수감자들이 시스템 침투 기술을 습득한다면, 이 험난한 현실에서 왜 다시 그런 범죄를 저지르지 않겠냐는 의문을 가짐. 특히 오랜 수감을 겪은 사람들이라면 더욱 우려되는 부분임
     * 정말 인상적인 이야기임. 전 세계 교정 시설에서도 이런 사례를 많이 따라 했으면 하는 바람임. 혹시 궁금한 사람들을 위해, 이번 판결의 배경을 설명하자면: 30g의 합성 오피오이드 ‘U-47700’ 소지로 유죄 판결을 받은 사례임. 정상 용량은 약 1mg, 10mg만 되어도 치명적이라 3만 회분 혹은 3,000명에게 치명적인 양임. 2016년 11월부터 미국 전역에서 불법이 되었고, 경찰은 2016년 12월 그 약물을 당사자의 아파트에서 발견함. Preston Thorpe(25세)는 판매 의도를 인정해 15~30년형을 선고받았고, U-47700은 스케줄 1의 합성 오피오이드임
          + 마약 소지로 15~30년이라는 긴 형은 정말 과도하다는 생각임. 양이 많아 판매 목적이 인정됐다 해도, 성범죄나 무장강도보다 중형이라는 건 상당히 이상한 것 같음
          + 당사자가 자신의 블로그에선 범죄 내용을 축소해 ‘캘리포니아산 마리화나’라고 적고 있는데(https://pthorpe92.dev/intro/my-story/), 실제로는 단순 마리화나만의 문제가 아니었다는 점을 지적함
          + 댓글 상위에 ‘출소 후 데이터베이스 관련 일에 종사할 자격 논란’이 있는 듯한데, 범죄기록이 있는 사람을 고용하려고 고민한다면, 현재 그 사람의 성장과 변화, 형벌·교정·보석·가석방 과정을 모두 감안한 시점에서 평가하길 권장함. 어떤 과거 행동의 단편적인 정보만 갖고 미니 재판을 하려 하거나, 구글링으로 퍼온 평가만으론 그 사람의 현재를 판단할 수 없음. 특히 불공정한 판결을 받은 사람들의 경우 당사자 입장이 기사에 거의 반영되지 않는다는 점을 유념해야 한다는 의견임. 요즘 세상 돌아가는 걸 보면, 우리가 ‘정의로운 사법 시스템’을 얼마나 신뢰할 수 있는지 의구심이 드는 것도 사실임. 단지 자신의 기준으로 ‘경미한’ 범죄라고 느껴질 때만 기회를 준다면, 진정 도움이 필요한 이들에게는 두 번째 기회를 줄 수 없다는 점
            생각해야 함
     * 보상 체계가 어떻게 운영되는지 궁금함. 미국 교정 시스템이 수감 노동 착취로 악명이 높아, 이런 혁신적인 시도에도 혹시 같은 문제점이 이어지지 않는지 걱정임. 하지만 한편으론 Turso가 현직 SW 엔지니어와 동일 수준의 급여를 줄 리 없다는 생각도 듦
          + Turso CEO임. 우리는 그에게 다른 연락처와는 달리 모든 급여(풀 연봉)를 지급하고 있음. 단지 건강보험 혜택만 제공하지 않음
          + 단순 호기심인데, 왜 그가 적게 받아야 한다고 생각하는지 이해가 잘 안 됨. 과거에는 수감자들의 임금이 박봉 수준이었던 게 사실이지만, 동일한 시간 일하고 생산성도 같다면 당연히 똑같이 받아야 한다는 생각임. 그 역할에 필요한 경험이 현저히 부족한 경우라면 일부 감액도 고려할 수 있는데, 여기서는 해당 사항이 아닌 듯함
          + “미국 교정 시스템이 수감 노동을 착취한다”는 말은 민간 이익을 위한 것인지 궁금함. 민간 이익을 위한 착취는 분명 문제이고, 수감 노동은 공공의 이익에 한해서 제공하는 것이 맞다고 생각함. 또한 이 노동이 형벌의 일환이어야 함
          + /me ‘법과 질서’ 모드 ON: 왜 이런 잘못된 판단의 결과를 납세자가 부담해야 하는지 의문임. /me 모드 OFF
     * 이 사례를 통해 예전부터 조금씩 궁금했던 부분이 확인되는 느낌임. 직장, 부업, 가족, 오래된 집까지 여러 책임이 있는 성인임에도, 어린 시절의 엄청난 집중력을 아직도 기억함. 책임이 전혀 없는 시절임. 감옥에 있는 걸 부러워한다는 뜻은 아니지만, 사람이 오롯이 자신만의 시간 속에서 얼마나 생산적인 결과를 만들어낼 수 있는지 늘 궁금했음. 이 사람이 이 기회를 최대한 잘 활용하고 있는 점이 반가움. 앞으로도 지속적으로 성장하면 출소 후에도 충분히 재범 없이 사회에 정착할 수 있다는 긍정적인 예시라고 생각함
     * Preston, 여기까지 온 것 정말 대단하다고 생각함. 작년 11월 이 사연이 Hacker News에 처음 공유됐을 때 이메일로 연락했었는데, 그때부터 성공할 거라는 확신이 있었음. Jessica와 UL의 노고에도 큰 찬사를 보냄. 앞으로도 밝은 미래가 열릴 거라는 기대임
     * Maine 주가 교정 시설 측면에서 ‘진보적’인 곳으로 간주된다는 게 오히려 무섭게 느껴짐. 내 전 학생은 아주 똑똑한 친구였는데, 조현병으로 인해 환청에 시달리다 총을 들고 은행을 털러 갔다가 10년형을 받음. 대부분의 유럽 국가라면 그 사람을 완전히 안전할 때까지 치료한 뒤 사회 복귀를 시켰겠지만, 미국에선 그냥 수감함. (기사 링크)
          + 기사 내용을 보면, 부모가 아일랜드에서도 치료를 받게 하려고 했으나 받아들여지지 않아 크게 절망했고, 이 때문에 상황이 악화된 것임. 미국 교정 시설에서도 사실 정신과 치료를 제공했고, 판사도 치료를 적극 권유함. 실제로 이 판결은 가능한 한 관대한 편이었으며, 치료를 받고 나중에 아일랜드에서 복역을 계속하는 것도 고려됐음. ([기사 내용 요약])
          + ‘전 학생이 똑똑한 사람’이었다는 점이 이 이야기를 더 안타깝게 만들어야 하는 이유인지 잘 모르겠다는 의문임
     * 만약 감옥이 ‘최고의 방해 없는 개발 환경’이 된다면 어떨까 생각함. 회의도 없고, 슬랙 알림도 없고, 링크드인 리쿠르터도 없고, 오직 너와 터미널, 그리고 10년간의 끊김 없는 집중력만 남는 환경임. 공포스럽게 생산적인 모습이 그려짐
          + 오버로드(권력 가진 자)들이 이 아이디어를 듣지 않았으면 좋겠다는 생각임. 오픈플랜 오피스조차 충분히 끔찍하다고 느낌
          + 알림은 없겠지만, 그 대신 언제든 흉기를 휘두르거나 성폭력을 저지르는 사람이 있을 수 있음
          + 현실에서도 그런 환경이 필요하다면 집 한 켠 방 하나를 비워 컴퓨터, 책상, 침대, 그리고 물 한 병만 남겨보기 추천임. 사용할 앱, 웹사이트도 아주 엄격히 제한하면 됨
          + 연애 계획도 없고 집을 소유한 것도 아니라면, 컴퓨터만 허용되는 감옥 생활도 솔직히 나쁘지 않게 들림. 집세 걱정도, 건강보험 걱정도, 각종 사회적 스트레스도 없음
          + 거의 ‘The Office’ 시즌3 에피소드9을 아직 안 본 듯한 분위기임
     * 미국만의 ‘개인 책임 강조’와 근본적 원인 해결에 소홀한 문화가 결국 실패한 마약과의 전쟁으로 이어졌다고 봄. 대부분의 법과 정책이 인간이 훨씬 더 비합리적이고 충동적이며 정보 부족한 존재라는 현실을 도외시하고, 지나치게 이상적인 시나리오에만 기대고 있다는 점이 문제임. 법정형을 “수년형→수십년형”으로 올린다고 해서 어떤 효과가 있을지 의문임. 대중이 법률 조항을 정확히 얼마나 인지하고 있고, 실제 형량 변화도 알아챈다 하더라도 “몇 년형이면 할 텐데, 몇십 년이면 그만둬야겠다”는 식으로 행동이 바뀔지 생각해봐야 함. 중형 선고와 엄청난 사법 집행 비용만으로도 여전히 미국엔 불법 마약 문제가 심각함
     * 교정 시스템 안에서 선한 기회를 제공받은 수감자 이야기를 들으면 반가움. 딜러에게 대한 동정심은 크지 않지만, 이분이 힘든 상황에 있었고 기회 이후 스스로 긍정적으로 변화하려는 점은 좋은 소식임
          + 이 사례의 정확한 사연은 모르겠지만, 예를 들어 내 고향 Texas처럼 법적으로 기준치를 약간만 넘겨도 자동으로 ‘판매 의도’ 혐의가 추가됨. 법원이 과부하 상태고, 유료 변호사가 좌우하는 구조라 대다수는 실제로 딜러가 아니더라도 유죄 협상 과정에서 그 혐의를 인정하게 되는 경우가 많음
     * 교정은 ‘재활’을 목적으로 해야 하고, 그 외 기능은 노예제거나 나쁜 정치에 불과하다고 생각함. 이런 블로그 글을 접할 때마다 반가움을 느낌
          + 교정 분야에서 상당한 경험이 있는데, 실제로 교정적 가치가 있는 프로그램은 거의 본 적이 없음. 대부분은 바닥 청소법 배우기 같은 수준임. 누군가가 1년 동안 파라리걸 코스를 수료했는데, Illinois 교정 시스템은 (a) 비폭력 범죄자의 형량이 6개월 단축될 수 있다는 옵션, (b) 시험 감독이 필요하다는 이유로 이 프로그램까지 금지했음

   프라임젠도 어떨 땐 보면 한잔하고 카메라 켜는 것 같다는 느낌을 받을 때가 종종 있습니다만...

   정말 멋있네요.

   자유의 제한이라는 특수한 상황이 특정 주제에 대한 깊은 몰입으로 발현되었군요.
   침대에서 쇼츠보던 시간들에 대해 다시 한 번 생각해보게 되었습니다.

   놀랍네요 해커 뉴스 의견처럼 진정한 의미의 교화가 아닌가 싶습니다.
"
"https://news.hada.io/topic?id=21559","AI 프로덕트에 빠진 결정적 연결고리 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       AI 프로덕트에 빠진 결정적 연결고리 [번역글]

  AI 프로덕트에 빠진 결정적 연결고리(Loop)

    1. AI 제품의 진짜 경쟁력은 '암묵적 피드백 루프'
          + 대부분의 AI 스타트업은 “고객 데이터를 활용해 모델을 미세 조정하겠다”고 약속하지만, 실제로 데이터 확보 방법에 대해선 피드백 폼이나 설문조사 등 모호한 답변만 내놓음.
          + 빅테크 기업들은 이미 오래전부터 Facebook, TikTok처럼 사용자의 자연스러운 행동(사진 업로드, 스크롤 등)에서 암묵적으로 데이터를 수집해 플랫폼을 성장시킴.
    2. 암묵적 피드백 루프의 중요성
          + AI는 잘못된 추천을 해도 조용히 넘어갈 수 있는 서비스(넷플릭스)와 달리, 실수가 즉시 드러나고 신뢰가 무너질 수 있는 서비스(AI 어시스턴트)에서는 더욱 정교한 피드백 루프가 필수적임.
          + 모든 상호작용이 시스템의 지속적 학습으로 연결되어야 함.
    3. Cursor 사례: 자연스러운 학습 구조
          + 개발자는 평소처럼 코딩만 해도 Cursor가 자동으로 학습됨.
          + AI가 제안한 코드를 받아들이면 좋은 패턴이 강화되고, 거절하면 비효율적인 방식이 걸러짐.
          + 별도 피드백 요구 없이 저장소별 맥락을 반영해 점점 더 똑똑해짐.
    4. AI 제품의 한계와 미래
          + 많은 AI 제품이 여전히 피상적 지표나 수동적 피드백에 의존, 로그 속 가치 있는 신호는 제대로 활용되지 못함.
          + 앞으로는 사용 데이터를 자동 분석·개선하는 시스템, 즉 자동화된 피드백 루프가 AI 제품의 핵심 인프라가 될 것임.
    5. Notion, Perplexity 등 선도 기업의 변화
          + Notion: 사용자의 미묘한 상호작용을 분석해 AI 기능 발전.
          + Perplexity: 실제로 문제 해결에 도움이 된 답변 데이터를 분석해 검색 결과 개선.
          + 피드백 루프는 단순한 기능이 아니라 서비스의 근간이 되는 핵심 인프라임.
    6. 결론
          + OpenAI, Anthropic처럼 기본 모델을 직접 개발하려면 엄청난 자원이 필요.
          + 앞으로는 사용자와의 상호작용을 자동으로 학습에 활용하는 기업이 시장을 주도할 것.
          + '보이지 않는, 끊임없이 학습하는 시스템'을 구축하는 기업이 다음 AI 시대의 주인공이 됨.
"
"https://news.hada.io/topic?id=21516","System of Action 쟁탈전","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          System of Action 쟁탈전

     * AI는 기존 'System of Record' 중심 소프트웨어를 뛰어넘어, 'System of Action'—실제 업무를 실행하는 플랫폼—을 향한 경쟁이 본격화됨
     * System of Action은, 데이터 기반 의사결정에서 AI와 사람이 함께 실시간 업무를 자동화·실행하는 플랫폼으로, 데이터와 워크플로우의 '중력'을 모두 흡수하는 지위를 의미함
     * Hero User(핵심 실무자) 를 선점한 네이티브 AI 스타트업은, 사용자 경험·PLG 전략·빠른 피드백 루프를 바탕으로 기존 SaaS 시장을 빠르게 잠식할 수 있음
     * 기존 SaaS 기업(incumbent) 도 PLG, 사용자 중심 제품, 빠른 MVP 출시 등 전환 없이는 데이터·워크플로우 주도권을 빼앗길 위험이 커짐
     * 궁극적으로 'System of Action'을 소유한 자가 AI 시대의 진정한 승자가 되며, 고객의 실제 업무와 경제적 가치를 실시간으로 증폭시키는 핵심 역할을 담당함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 시대, 'System of Action'으로의 진화

  배경 및 정의

     * ""AI가 소프트웨어를 집어삼킨다""는 화두 아래, 수십억 달러 투자와 SaaS 시장 재편이 가속화됨
     * 전통적 소프트웨어는 'System of Record'(기록 중심 플랫폼) 에 머물렀으나, 이제는 'System of Action'(실제 업무 실행 및 자동화 주도권)으로 진화 중
     * System of Action은 사람이든 AI든, 데이터를 기반으로 즉각적 의사결정 및 후속 워크플로우를 실행하는 지점임

  네이티브 AI vs 기존 SaaS의 경쟁 구도

    기존 SaaS(incumbent)의 기회와 한계

     * 데이터와 워크플로우의 '중력'을 지닌 SaaS는 각종 서비스 번들링, 운영체제 수준의 영향력을 행사
     * 그러나, AI 시대엔 '업무 실행'까지 확장하는 제품·경험이 필수임에도, 느린 의사결정·완벽주의·고전적 판매방식(세일즈/온보딩 위주) 등으로 변화 속도 저하
     * Hero User 중심의 신속한 제품 전환 및 PLG(Product-Led Growth) 경험이 요구됨

    네이티브 AI 스타트업 전략

     * Hero User(핵심 실무자): 독립적으로 툴을 선택·구매할 권한과 디지털 친화적 업무를 가진 실무자
     * Hero의 Pain Point(예: 진료 기록 자동화, 반복적 문서화) 를 '마법 같은' 경험으로 해소 → 빠른 확산
     * PLG, 쉬운 사용성, 바이럴 효과, 빠른 피드백(러닝 루프)로 기존 SaaS의 데이터·워크플로우 중력 탈취
     * 기존 시스템(PMS 등)과의 통합도 크롬 확장, RPA 등 '게릴라적' 방식으로 우회, 사용자 요청으로 공식 통합 유도
     * 점진적으로 핵심 워크플로우(스케줄링, 결제 등)까지 확장하며, 최종적으로 기존 SaaS 기능 대체 및 시장 주도권 확보

  Case Study: 수의사 클리닉 소프트웨어 시장

     * 기존 Practice Management System(PMS)는 워크플로우·데이터 중력을 모두 가진 전통적 강자
     * AI 기반 트랜스크립션 툴이 '기록 자동화'라는 고충을 해결하며 빠른 확산
     * 기존 업체는 통합/완벽함에 집착하며 출시 지연, 신생 AI 업체는 빠른 UX·PLG로 Hero User를 선점
     * 수많은 사용자가 모이면 PMS와의 공식 연동 요구 발생, 점차 핵심 워크플로우까지 장악하며 기존 PMS 대체

  AI 스타트업/기존 SaaS의 승부 전략

    AI 스타트업

     * Hero User와 Hero’s Work(고부가가치 핵심업무) → Administrative Work(반복/서류업무) → Work Not Done(기존에 아예 처리 안된 일) 순으로 문제 해결 영역 확장
     * Wrapper·기존 인프라 재활용도 OK, 경험·러닝 루프 중심의 개선이 관건
     * Tryable(쉬운 체험), Buyable(즉시 결제/구매), Findable(자연스러운 노출/바이럴) 전략이 필수

    기존 SaaS

     * 자사 Hero User와 시스템 중심 워크플로우 잠금(프리미엄/무료 제공, API 접근 제한 등)
     * Hero User의 업무 고충 파악 → 쉽고 빠른 MVP 출시, '완벽'보다 '속도·실용' 우선
     * 세일즈 중심에서 PLG 경험으로 전환, 사용성·즉시성 강화
     * Engagement(사용/일상화) > Monetization(수익화). 사용량, DAU/WAU(일간/주간 활성) 지표로 제품 확산 우선

  미래의 System of Action이 갖는 의미

     * 시스템의 본질이 '업무 관리'에서 '업무 실행'으로 확장됨
     * 실제 고객 업무의 생산성, 수익성, 확장성 직접 증폭
     * AI는 반복적 업무 자동화는 물론, 아웃소싱되던 고부가 업무(예: 매출 관리, 광고 대행, 최적 구매 등)까지 대체
     * System of Action을 장악한 플레이어만이 AI 시대의 진정한 시장 지배자로 거듭남

결론

     * System of Action을 차지하는 자가 데이터, 워크플로우, 고객 일상의 모든 중심을 장악
     * Incumbent와 Native AI 모두, '누가 더 빠르고 유연하게 실무의 본질을 바꾸고, Hero User를 선점하느냐'가 승패의 핵심
     * AI 시대, 운영 소프트웨어의 주도권은 '기록'이 아니라 '행동'에 있음
"
"https://news.hada.io/topic?id=21437","Seedance 1.0 - Bytedance의 멀티샷 비디오 생성모델 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Seedance 1.0 - Bytedance의 멀티샷 비디오 생성모델

     * 텍스트와 이미지 기반의 멀티샷 비디오 생성 모델로, 의미 파악과 프롬프트 해석 능력에서 기존 모델보다 더 정확하고 유연한 성능을 보임
     * 1080p의 높은 해상도와 더불어 부드러운 장면 전환, 풍부한 디테일, 영화적 느낌의 결과물을 제공
     * 세밀한 파인튜닝과 비디오 특화 RLHF 보상 메커니즘으로 전반적 성능 향상
     * 텍스트 묘사나 이미지를 토대로 하여, 요구 조건을 만족하는 동적이고 몰입감 있는 비주얼 컨텐츠 생산이 가능
     * 효율적인 아키텍처와 새로운 학습 패러다임으로 멀티샷 생성 및 텍스트-비디오/이미지-비디오 작업 모두 지원

Seedance 1.0 소개

     * 최근 디퓨전 모델 대혁신으로 인해 비디오 생성 기술이 빠르게 발전하는 중임
     * 하지만 대부분의 기존 모델은 명령어(프롬프트) 수행, 동작 자연스러움, 시각적 품질 사이에서 균형을 잡는 데 여전히 어려움이 있음
     * Seedance 1.0은 이 모델은 아래 주요 기술적 개선점을 적용한 비디오 생성 기반 모델임
          + (i) 정밀한 비디오 캡션을 덧붙인 다중 소스 데이터 수집으로 여러 시나리오에서 포괄적 학습 가능
          + (ii) 효율적인 아키텍처와 학습 패러다임으로, 멀티샷 생성과 텍스트→비디오, 이미지→비디오 작업을 동시에 지원
          + (iii) 세밀하게 최적화된 후처리: 정교한 감독 파인튜닝과 비디오 특화 RLHF, 다차원 보상 메커니즘으로 전반적 성능 대폭 개선
          + (iv) 모델 가속화: 다단계 증류 및 시스템 차원 최적화를 통해 10배 추론 속도 향상
     * NVIDIA-L20 GPU 기준 41.4초 만에 5초 1080p 비디오 생성 가능
     * 최신 비디오 생성 모델과 비교할 때, 공간-시간적 유연성, 구조적 안정성, 복잡 다중 상황에서의 지시 수행, 멀티샷 및 스토리텔링 일관성이 뛰어남

        Hacker News 의견

     * 이런 기능들이 언젠가는 너무 평범하고 시시하게 느껴질 미래를 기대하는 입장임
          + 내 휴대폰에서 재미로 친구들과 단체 채팅방에서 즉석으로 24화짜리 풀 보이스 애니메이션을 만들 수 있는 수준이 될 것이라 상상하게 됨
          + 지금도 믿기지 않을 정도로 많은 걸 할 수 있는데, 곧 아무도 신경 쓰지 않게 될 점 역시 신기함
          + 아무리 간단하게 프롬프트를 입력해 만든 24화짜리 시리즈라도, 결국 누구도 관심 가지지 않을 것이라는 점을 지적함
               o AI가 콘텐츠의 가치를 높이는 게 아니라, 희소성을 파괴함으로써 의미를 사라지게 만드는 것이라는 생각임
               o ‘Tea. Earl Grey. Hot.’처럼, 그저 기계적으로 나오는 느낌에 대한 비유도 남김
          + 이렇게 콘텐츠 제작이 쉬워지면, 누가 긴 시간 들여 영상을 시청할지 의문을 품음
               o 결국 각자 자기만의 생성형 콘텐츠를 즐기느라 바빠질 것이라는 추측임
          + 나 역시 이 기술을 매우 기다리고 있음
               o 예를 들어 Shadowrun 영화 같은 것도 직접 만들어보고 싶은 기대감임
          + 한 달에 만들어지는 콘텐츠의 양이 인류 역사상 지금까지 나온 모든 콘텐츠의 합을 넘을 전망임
               o Disney, Marvel, Star Wars와 같은 대중 미디어 일색 대신, 각자 관심사에 딱 맞는 롱테일 미디어를 누릴 수 있을 것이라는 점에서 기대
               o 이집트와 아틀란티스에 관심이 있다면, 두 문명이 싸우는 스팀펑크 시리즈를, ‘The Wire’ 같은 진지한 색채로 바로 감상하는 세상도 상상 가능
               o 기존에는 절대 제작되지 않았을 기획도 실현 가능한 시대가 될 전망
               o 좋은 창작자가 나타나겠고, 이제는 인디 음악, 인디 만화, 인디 게임처럼 다양한 크리에이터가 두각을 드러낼 수 있다고 봄
               o 진짜 문제는 ‘발견성’이 될 것임
               o 결국 1년에 500개 한정적인 자리에 낙하산으로 들어가야만 했던 기존 업계 구조가 붕괴되고, 각자의 비전을 가진 많은 인재가 큰 시도를 할 수 있을 것이라 강조
               o VivziePop(Vivienne Medrano 위키), PsychicPebbles(Zach Hadel 위키)처럼 유튜브에서 시작해 거대한 IP로 성장하는 모델이 미래의 표준이 될 전망
               o 창작계의 혁신이 단순히 2~10배가 아니라, 1000배 가까이 좋아질 것이라고 예상
               o 지금 대부분의 영화/드라마는 내 취향에 맞지 않기 때문에 싫어했지만, 미디어라는 매체 자체는 좋아해 온 입장임
               o 이젠 내 취향, 내 관심사에 딱 맞는 콘텐츠를 만날 수 있는 세계가 펼쳐질 거라 굉장한 기대감임
     * 미래에는 TikTok 알고리즘처럼, 내가 보는 즉시 내 취향을 파악해 그때그때 새롭게 영상을 만들어주는 형태가 될 전망임
          + 사용자가 스크롤 할 때마다 좋아하는 것을 배우고, 더 많은 영상을 자동 생성해서 보여주는 시스템이 될 것임
          + 충분한 컨텍스트가 모델에 입력되면, 그 사람이 반응하는 콘텐츠는 너무나 매혹적이라 도저히 화면에서 눈을 뗄 수 없게 만드는 중독성을 가질 것임
               o 섬뜩한 상상인 동시에, 장기적으로는 불가피하다고 생각함
          + 아쉽지만, 단순히 사용자의 취향을 따르기보다는, 참여를 극대화하기 위해 사용자의 취향 자체를 조작하려 들 것이라는 우려도 있음
          + 사실 이런 기술 방향은 사람들이 소셜미디어를 사용하는 이유와는 거리가 있다는 의견도 존재함
               o ChatGPT도 끝없이 댓글을 생성할 수 있지만, 결국 우리는 여기 Hacker News에 와 있다는 점을 예로 제시함
          + 나중에는 ‘라이브 모드’라는 개념으로 실시간으로, 사용자의 목소리에 맞춰 영상을 즉시 생성하는 기능도 등장할 것으로 예상함
               o Netflix에도 이런 기능이 들어갈 수 있을 것 같음
          + 광고를 싫어한다는 것도 학습해서 제대로 반영해줄지 궁금함
     * 샘플 영상들 중 꽤 인상적인 장면도 있지만, 일부 장면에서는 부자연스러운 움직임이 자주 보임
          + 훈련 데이터가 TikTok 중에서도 가장 과장된 부분에 집중됐는지, 5초 이상 한 장면을 유지하지 못하는 듯한 특징이 보임
          + 확실히 어려운 장면은 잘 처리하는데, 오히려 간단해보이는 부분에서 실수가 많이 보인다는 평가임
               o 오프닝 피아노나 사진작가가 사용하는 카메라에는 ‘AI text’가 써있고, 카페의 노신사는 손이 베레모를 뚫고 지나가며, 바닷가에서 뒤돌아보는 소녀는 부엉이처럼 머리를 돌림
               o 유럽 도시에서 자전거 타는 소년 장면에서는 광장에 암호화된 존재가 외발자전거를 타고 나무 아래 서 있는 모습으로 마무리됨
          + ByteDance는 이미 수주 전부터 Model Arena에서 내부적으로 ‘Unicorn’이라는 이름으로 모델을 테스트해왔음
               o 이미 Google Veo 3보다 높은 점수를 기록 중임
               o ArtificialAnalysis: Model Arena 랭킹 바로가기
     * 5년 후에는 모든 콘텐츠가 실시간으로 생성되는 세상이 가능할 것이란 전망임
          + 내가 무언가를 말하면, 바로 5초짜리 영상으로 응답해주는 방식임
          + 이제 영상이 ‘고정된 자산’이 아니라, 즉석에서 만들어졌다 사라지는 ‘에페메랄’한 응답이 될 것
          + 동영상은 업로드되는 수동적 파일이 아니라, 데이터 스트림의 출력물이 되어감
          + 스와이프를 대체할 미래의 UI는 음성 프롬프트가 될 가능성이 큼
          + Seedance에서 하는 일은 새로운 포맷의 실험이라기보다, 런타임 생성 콘텐츠 시스템 실험임
          + 백엔드에서는 model infra를 comet으로 압축하고, LLM을 더 싸고 빠르게 돌릴 수 있도록 세팅함
          + 이 조합이 실현되면, 커다란 배치나 캐시 없이도 대규모로 콘텐츠 생성 제공이 가능하게 됨
          + 만약 이것이 실제로 자리 잡으면 피드는 더 이상 스크롤이 아니라 랜더 루프가 되어버림
          + 이 모든 것이 더 이상 ‘미디어 서비스’가 아니라, 비디오 플랫폼 외형을 쓴 저지연 AI 모델 호스팅 시스템이라는 평가임
     * 비디오 품질은 뛰어나지만, 소리는 어디 있냐는 질문 등장
          + VEO3가 영상은 잘 만들어도, 오디오 쪽 완성도가 대폭 차별화를 만들어낸다는 점을 언급함
          + 나는 대형 영상 스트리밍 회사에서 AI 솔루션을 다루는 입장임
               o VEO3의 문제는 프롬프트 간 일관성이 떨어진다는 것임
               o 예를 들어 캐릭터 레퍼런스 이미지를 올려도, ‘늙은 신부가 몸을 숙인다’와 ‘늙은 신부가 동전을 집는다’를 각각 생성하면 등장 인물이 매번 달라보임
               o 물론 VEO3는 이미지-투-비디오 기능을 제공하지만, 실제 씬을 만드는 데는 아직 많이 부족함
               o 시간 지나면 발전하겠지만, 지금 단계에서는 개인적으로 Seedance가 숏 간 일관성에 초점을 맞춘 점이 마음에 듦
               o 이 점이 VEO3에도 압박을 줘서 해당 기능이 빨리 개선될 것이라 기대
     * 왜 모든 예제 영상에는 큰 동그라미가 나오는지 궁금증 제기
     * Seedance를 어디서 쓸 수 있는지 묻는 질문 등장
          + Seedance 1.0은 2025년 6월부터 Doubao와 Jimeng 등 여러 플랫폼에 통합될 예정임
               o Doubao 동영상 생성 바로가기
               o Jimeng 동영상 생성 도구 바로가기
               o 논문 PDF
          + 이 기능은 곧 TikTok으로 직행하게 될 것 같음
               o TikTok자체 플랫폼에서 엄청난 생성형 콘텐츠 홍수가 일어날 것이며, 모두가 크리에이터가 되고 싶어하는 현상을 수익화할 방법을 찾게 될 것이라는 지적임
               o 앞으로는 “콘텐츠를 무료로 올릴 수 있다”에서 “AI 게이트웨이를 거쳐야 올릴 수 있고, 그 요금도 지불해야 한다”는 방식으로 플랫폼 정책이 바뀔 것이라는 관측임
     * 모션이 많은 영상에서 속이 메스껍거나 어지러움을 느끼는 경우가 있다고 밝힘
          + Sora 첫 공개 때도 유사한 경험이 있었으나 Seedance에서는 약간 완화됨
          + Veo 3 시연에서는 이런 증상이 없었는데, 혹시 다른 사람들도 Seedance의 모션이 많은 샘플에서 비슷한 느낌을 받았는지 질문함
     * AI 생성 영상의 리얼리즘이 기존 CGI 애니메이션 영화 수준에 근접했는지 궁금증 표출
          + 전문가라면 당연히 현재 결과물에서 분명한 결함을 지적할 수 있을 거라 예상함
          + 다만, 향후 특정 구간만 프롬프트로 미세 수정할 수 있을지 기대함
          + 또, 할리우드 고예산 CGI 한 초당 소요 비용과 비교해 실제로 어느 정도의 연산자원/금전이 들지 궁금함
          + 요즘 할리우드에서 보이는 일반(비애니메이션) CGI조차도 퀄리티가 떨어지는 경우가 많아서 기대치가 그리 높지 않음
               o 실제로 CGI 결과에 변화를 적용/관리하는 과정(change management)도 꽤 흥미진진해보임
     * ""Old man""이 그렇게 나이 들어보이지 않아 개인적으로 좀 의아함 (혹시 내가 나이 들어서일지도 모른다는 농담 섞인 고백)
"
"https://news.hada.io/topic?id=21527","Scrappy - 친구들과 나만을 위한 작은 앱 만들기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Scrappy - 친구들과 나만을 위한 작은 앱 만들기

     * Scrappy는 비전문가도 손쉽게 직접 작은 앱을 만들 수 있도록 돕는 홈메이드 소프트웨어 제작 도구
     * 대형 상용 앱이나 엔터프라이즈 앱과 달리, 개인적이고 창의적인 소규모 문제를 자유롭게 해결할 수 있음
     * 캔버스 기반 UI와 간편한 코드 편집, 실시간 협력 및 공유 기능을 제공해 비프로그래머도 활용 가능함
     * 모든 앱(스크랩) 은 기본적으로 멀티플레이어 환경이며, 계정 생성 없이 즉시 사용과 협업이 가능함
     * AI 코드 생성이나 기존 도구와 달리, Scrappy는 사용자 직접 조작과 소유권을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 배경

     * 대부분의 소프트웨어는 대중 시장 판매용이거나 대규모 맞춤형 앱이 중심임
     * 그러나 실제 개개인의 필요를 채우는 소규모, 개인 맞춤형 앱은 매우 드묾
     * Scrappy는 누구나 친구 및 가족을 위해 간단하고 창의적인 앱을 직접 만들 수 있도록 돕는 연구 프로토타입
     * Scrappy의 목표는 프로그래밍 전문성이 없어도, 더 많은 사람들이 창의적이고 개인화된 소프트웨어를 만들 수 있는 비전을 구체적으로 제시하는 것

Scrappy란 무엇인가

     * Scrappy에서 만든 앱은 Scrapp이라 부름
     * 대표적인 예시:
          + 초등학생을 위한 산수 연습 앱: 난이도 조절 가능
          + 지역 행사 참석자 카운터: 여러 입구에서 참가자 출입 관리
          + 회의비용 계산 시계: 실시간 회의 비용 산출
          + 주간 집안일 관리: 구성원별로 일정 유연 관리 가능

Scrappy에서 앱을 만드는 경험

     * Scrappy는 Figma, Miro, Google Slides와 유사한 무한 캔버스에서 버튼, 텍스트필드 등 오브젝트를 배치함
     * Inspector 패널에서 속성을 직접 수정할 수 있으며, 버튼 등에는 JavaScript 코드도 연결 가능함
     * 앱 제작은 드래그/드롭, 속성 수정, 코드 삽입을 단계적으로 반복하며 완성

주요 특징:

     * 기본 동작 구성: 필드와 버튼을 놓고 즉각적인 동작 연결 가능
     * 반응형 수식: 특정 조건에 반응하는 실시간 속성 변경 구현
     * 멀티플레이어 동기화: 상태가 항상 실시간으로 저장, 동기화됨
     * 라이브 편집: 실행과 편집 구분 없이 항상 실시간 수정 가능
     * 선택적 공유: 앱의 특정 일부분만 별도로 공유 및 연결 가능
     * 가시적 데이터 조작: 스프레드시트처럼 데이터를 보면서 디버깅 및 수정 가능

Scrappy를 개발한 이유

     * Scrappy는 사용자 주도의 프로그래밍을 실현하는, “small computing”, “casual programming”, “home-cooked software”같은 트렌드와 관련됨
     * 기존의 시각적 프로그래밍(블록, 노드-와이어 기반)과는 다른 길을 택해, 직관적 조작과 스크립팅을 결합
     * HyperCard, Visual Basic, 동시적 온라인 미디어 등에서 영감을 얻었으며, 생산성 캠퍼스 도구와 실시간 협업(구글 독스, Figma 등)의 경험을 중요하게 삼음.

     * Scrappy는 대규모 상용 앱이나 AI 자동 생성방식과는 달리, 사용자가 직접 제어하며, 개인화·재미·창의성을 극대화함
     * 코드를 직접 생성하지 않고도, 더 쉽고 인간친화적인 앱 제작 경험을 제공함.

Scrappy의 대상 사용자

     * 업무 프로세스 최적화자: 복잡한 업무 흐름을 전문가 도움 없이 개선하고 싶은 사람
     * 교사와 학생: 부수적 기술(커맨드라인, 환경설정) 없이 프로그래밍 본질에 집중 가능
     * 취미 개발자: 대중 앱의 복잡함을 벗어나 빠르게 여러 프로젝트 탐구 희망자
     * DIY 지향자: 집, 취미처럼 자신만의 앱을 직접 만들고 싶은 사용자

   현재, Scrappy로 완전 초심자가 앱을 만드는 것은 아직 어려움(일부 JavaScript 지식 필요), 하지만, 공유와 리믹스는 비프로그래머도 가능함.

Scrappy에서 만들기 좋은 앱은?

     * 친구/지인과 공유: 대다수 Scrapp은 여러 사용자가 실시간 공동작업에 적합함
     * 지속적 수정 및 개선: 앱 실행 중에도 즉시 수정 가능, 배포/빌드 과정 없음
     * 소규모 계산 또는 조작: 복잡한 시스템보다는 공유 문서+약간의 컴퓨팅에 효과적임
     * 사용자 마찰 최소화: 계정 만들기 등 불필요한 과정 없이 링크만으로 접근 및 사용 가능
     * 신뢰할 수 있는 소수 사용자: 권한 제어나 미션 크리티컬이 필수라면 적합하지 않음

   앱 아이디어 예시: 맞춤형 플래시카드, 회의 아젠다, 온라인 워크숍 관리, 가족 게시판, 여행 계획표 등

Scrappy vs 대중 앱

   대중적인 앱을 찾을 수 없거나 적합한 것이 없는 경우, Scrappy로 직접 만들어 공유할 수 있음. Scrappy의 이점:
     * 필요한 기능만 가짐: 불필요한 요소 없음
     * 개인적인 정성: 직접 만든 앱은 의미와 애착 높음
     * 재미있게 수정 가능: 색상, 레이아웃을 자유롭게 꾸미고 유머도 추가 가능
     * 쉬운 리믹스/공유: 다른 사용자가 쉽게 수정 및 재활용 가능
     * 협업 중심 설계: 다수 사용자가 동시에 조작 및 편집 가능
     * 즉시 사용: 계정 가입 없이 링크 클릭만으로 바로 사용 가능
     * 데이터 소유권 명확: 데이터는 로컬에 저장되어, 온전히 사용자가 제어함

Scrappy vs AI 기반 앱 생성

   AI가 앱을 자동 생성할 수 있지만, Scrappy의 장점은 이해 용이성, 실시간 협업, 창의적 소유감에 있음
     * 쉬운 이해 구조: 복잡한 코드 없이 시각적 오브젝트 기반
     * 실시간 협력 지원: 여러 사용자가 동시에 협업 및 수정 가능
     * 더 많은 재미와 창의성: 즉각적인 피드백, 능동적 수정의 즐거움을 제공함

Scrappy vs HyperCard 및 후속 도구

     * 인터넷 친화적 공유: Scrappy 앱은 링크만으로 온라인 공유 가능
     * 실시간 협업 환경: 동시 편집/실행 지원
     * 현대적 UI 및 인터랙션: 무한 캔버스, 다양한 오브젝트 지원
     * JavaScript 스크립팅: 현대적이며 보편적인 언어로 동작
     * 더 다양한 인터랙티브 오브젝트: 문자열, 숫자, 날짜, JSON 등 지원
     * 반응형 수식 및 상태 연결: 스프레드시트와 비슷한 동적 관계 설정 가능

향후 계획

     * 프로그래머가 아닌 사용자를 위한 진입장벽 낮추기
          + 코드 자동완성, 더 쉬운 디버깅, 관계 시각화, 이해하기 쉬운 에러 메시지, AI 기반 어시스턴트
          + 쉽고 빠른 공유, 공개 갤러리, 모바일 지원 강화
     * 기능 강화 및 확장
          + 컬렉션 및 데이터 처리 기능 강화, 반복적인 오브젝트 관리, 리유저블 컴포넌트 도입
          + Scrappy 확장성(새 오브젝트 지원), 개념적 일관성 개선 등

     * Scrappy를 직접 사용해보기(데스크톱 전용)

        Hacker News 의견

     * 나는 이 프로젝트의 방향성은 마음에 들지만, 호스팅된 SaaS 방식은 내가 원하는 것과 다름을 느낀 경험 공유. 작은 카운터 등 하루 프로젝트에는 상관없지만, 몇 년간 쓸 작은 앱이면 의존도가 문제라는 판단. 학습 곡선이 아무리 낮아져도 결국 존재하며, 오히려 오랜 기간 쓸 수 있는 접근성과 쉬운 언어, 직접 GUI를 입힐 수 있는 도구가 더 원하는 선택지임. 코드는 완전히 감춰질 필요 없이 사람들이 실제 할 수 있는 방향으로 쉽게 만들어야 한다고 생각. MySpace에서 얼마나 많은 사람들이 CSS를 배우게 됐는지 떠올려 보면, 복붙이 시작이지만 결국 자신만의 걸로 튜닝하게 되는 과정이라서. 개인적으로 요즘엔 HTML/CSS/JS를 주로 사용하고, 정말 백엔드가 필요하면 순수 PHP(프레임워크 없이)를 씀. 하지만 이 방식은 브라우저에 묶이는 단점이 있는데, 회사에서
       이런 방식(AutoHotKey 포함)으로 만든 소규모 프로젝트들이 10년 넘게 거의 관리 없이 잘 돌아가고 있음. 특히 AutoHotKey 스크립트는 8년 전에 macOS로 전환하면서 손 뗐지만, 아직도 동료들이 매일 수차례 사용 중. 만약 AutoHotKey가 더는 작동 안 해도 이미 만들어진 코드는 계속 사용 가능. 하지만 SaaS 방식의 솔루션은 설립자가 다른 것에 관심을 옮기면 다시 만들어야 하는 리스크가 발생. 이런 “scrappy”한 솔루션을 찾는 사람들은 그때마다 다시 만들고 싶어하지 않는다는 점이 핵심.
          + 이런 식의 솔루션이라면 TiddlyWiki처럼 하는 게 더 맞을 것이라 생각. TiddlyWiki는 단일 HTML 파일에 전체 웹앱이 들어있으며, 기존에는 뭔가 바꾸면 HTML 파일 자체에 저장해서 셀프 리플리케이션이 됨. 최근에는 백엔드 저장도 지원하는 등 다양한 방식이 나옴. 자기 복제 HTML 파일 및 선택적 백엔드 접근으로 작은 개인 맞춤형 프로젝트에는 훨씬 신뢰성 있는 선택지가 될 수 있다고 봄. 적어도 강력한 복원력이 장점.
          + codeboot.org 추천. 완전 클라이언트 사이드 파이썬 IDE로, 싱글스테핑 지원과 비계층적 가상 파일 시스템, JS 코드 FFI 등 여러 기능이 있음(문서 참고). 앱 공유도 아주 쉬워서 플레이 버튼 우클릭하면 URL 복사만으로 공유 끝. 데이터 정제 등 여러 문제를 해결한 적이 있고, 이렇게 만든 앱을 바로 책갈피 해두고 쓰는 경우가 많았음. 진짜로 잘 작동하고, 만약 궁금한 게 있으면 AMA(질문 환영). 적극적으로 개발 중이고 멋진 새 기능도 준비 중임.
          + SaaS 코드 전체를 오픈소스로 공개해서 장기 사용성을 보장하는 방법도 있겠다는 생각. Penpot가 이 방식을 잘 쓰고 있음. 대부분은 SaaS로 쓰지만, 셀프 호스팅도 가능. 물론 배포판 인증이나 앱 서명 등은 어쩔 수 없이 어려운 요소이지만, 아마 Web3 방식도 도움이 될 듯함. 혹은 PICO-8이나 예전의 Flash처럼 런타임을 공개하고, “카트”나 앱을 유통하는 방식도 방법. SaaS 바깥에서 신뢰할 수 있는 유통망 만들기는 꽤 복잡하긴 해도, 실제로 역사가 있기 때문에 시도해볼 만하다고 생각. Penpot / PICO-8 참고
          + Scrappy 공동 제작자로서, 소프트웨어 장기 사용성 중요성에 전적으로 공감. Scrappy는 로컬 퍼스트 아키텍처로 설계, 전통적인 백엔드가 없어서 클라우드 의존성은 단순 싱크 서버 뿐. (HN에서 이 논의가 커진 후 급히 FAQ에 기술적 세부 내용을 추가함.) 이는 기술적·재정적으로 SaaS 저코드/노코드 툴과는 차별 포인트라고 생각. 초기부터 단일 페이지, 자체 포함형 HTML 파일로 스크랩 저장 기능을 실험했으나, 이 기능은 현재 노출은 안됨.
          + 나는 Cursor와 vibe coding 방식으로 이런 거 만들고 있는데, 정말 만족스럽게 사용 중. 최근에는 내 집 항로 정보를 SDR로 받아와서, 공항에서 비행 정보도 붙여주고, 기차역 플립 보드 스타일로 마법 거울에 표시하는 비행기 추적기를 만들었음. 프론트엔드는 JS도 거의 몰랐지만, 10시간 정도 코딩해서 꽤 괜찮은 앱 완성. 예전이면 2달 이상 걸려서 결국 포기했을 텐데, vibe coding으로 하니 너무 재밌고 긍정적인 경험이었음. 1200 sLOC쯤 되는데, 로깅/성능/퀄리티도 괜찮은 준전문가급 코드라 자부(상업용 평균 코드보다 낫다고 생각).
     * CardStock은 본문에 언급되지 않았지만 Scrappy와 목표와 접근 방식이 비슷해 보임. Scrappy와 달리 CardStock은 오픈소스이며 로컬에서도 실행됨. CardStock / GitHub 저장소 참고. Decker도 오픈소스이고, Scrappy 로드맵의 여러 요구사항(테이블 데이터 쿼리 랭귀지·그리드 위젯, “Contraption”으로의 부품 추상화 등)을 이미 구현함. Decker 링크 참고.
          + 내가 이런 툴을 오랫동안 찾고 있었는데, CardStock에 데스크톱 앱이 있는 점이 진짜 큰 의미라는 입장.
     * 모바일에서 만든 앱이 잘 작동하도록 하는 게 로드맵에 있지만, 모바일 자체 편집은 고려 밖인 듯함(“손바닥 크기의 터치스크린은 Scrapps 편집에 불편하다” 언급). 하지만 지금은 많은 사람들이 모바일만을 유일한 컴퓨팅 디바이스로 쓰는 시대이고, 코드나 소설까지도 모바일에서 작성하는 경우가 많음. 그래서 조금 불편하더라도 모바일 편집 인터페이스까지 고민한다면 이 툴의 영향력이 훨씬 커질 것임을 강조.
     * 내가 했던 가장 좋은 일 중 하나는, Apple Watch 걷기 기록을 한 장의 커다란 지도에 올려주는 심플한 앱을 일주일간 만들어서 AppStore에 올리고 지인과 공유한 경험. 1년이 지난 지금도 친구들이나 우연히 앱을 발견한 사람들이 도시 전체를 걷고 인증 메시지를 보내줘서 뿌듯함. 수익은 없어도 정말 보람찼던 경험. OP 말처럼 재미로 친구들을 위한 간단한 앱을 만드는 게 최고의 행복.
          + 앱 링크가 궁금해짐.
          + 그 과정에서 얼마나 많은 벽과 수많은 허들을 지나 만들어 냈는지 상상해 보면, 수많은 사람들이 그 중 하나에서 포기했을 것임. 결과적으로 사용자는 여전히 아무것도 컨트롤하지 못하고, 벤더 종속이 되어버림. 만약 AI에게 바로 명령해서 오픈소스 워치로 자유롭게 이식이 가능하다면 얼마나 좋은 세상일지 상상하게 됨.
     * 스프레드시트만큼 실제로 엔드유저에게 쓸만한 프로그래밍 환경은 본 적이 없음.
          + 여기에 극단적으로 몰입한 예시로 pyspread 추천.
          + 테스트, 버전 관리, 라이브러리 지원이 없어서 개인적으로는 패스.
          + 결국 직접 코딩을 배우는 게 나음. 이런 도구를 배우는 이유를 모르겠음. 개발자 입장에선 그냥 만들면 되고, LLM 쓰면 아주 단순한 거는 손쉽게 vibe coding으로 뚝딱이며 잃을 것도 없음. 비개발자라도 이런 툴을 배우는 사람이 과연 얼마나 많은지(TAM이 궁금함). 누가 굳이 귀찮게 앱을 끌어다 놓으며 만들지 의문.
     * vibe coding이 당장 개발자를 대체하진 않겠지만, 이런 단순한 시스템에는 가장 강력한 경쟁자가 될 것. LLM에게 간단한 앱(HTML+내장 JS) 만들라고 해도 약간의 수정만 거치면 완성도 높고, 심지어 시각적으로도 더 좋았음 예시 참고.
          + 나도 vibe coding으로 사이드 프로젝트 중. 매번 몇 시간마다 LLM이 풀 수 없는 문제에 부딪혀서, 코딩 경험이 없는 사용자는 아예 해결 못 하고 넘어갈 수도 있다는 생각. 사용 기술이나 프로젝트 범위에 따라 차이 있을 듯.
          + 버그 신고: 3 + 2 = 5.1 같은 실수를 입력하면 맞는 답으로 처리됨.
          + vibe coding이 원조 목적이 맞고, 이들은 자연스러운 대립자.
          + 셀프 호스팅이 가능한 간단한 시스템 스택이 궁금해짐. 개인적으로 Vue에 인증, 멀티플레이어 오프라인 DB, 정적 호스팅, 파일 호스팅, 사용자가 다른 사람 데이터 안 볼 수 있는 필터 기능이 필요.
          + 물음표 대신 빈칸이나 밑줄로 바꾸고 싶음.
     * 우리는 이 주제를 프로그래머 입장에서 접근하는데, 실제 기회는 커뮤니티에 있다는 생각. 예를 들어 가족 단위의 개인 앱스토어처럼 시작해볼 수도 있음(Masterson 스타일). 보안 없이(모두 지인이니까), 초대 없인 기여도 불가. 그냥 그런 아이디어 제안.
     * UI 요소를 빈 시트에 드래그 앤 드롭하고, 그리드 스냅이 UI 요소 크기와 달라서 계속 싸우고, 결국 코드 자동완성이나 비주얼 프로그래밍, API 도움, AI 지원 없이 생 자바스크립트 입력만 해야 된다면 이게 정말 끝인가라는 의문.
     * 나 역시 초심자에게 블록 기반이 아닌 “스크립팅 가능한 컴포넌트” 접근 방식에 100% 동의. 지금은 모바일이라 곧 데스크톱에서 써볼 계획. 분석에서 놓친 점은 사람들은 “쉬운 공유”와 “제로코스트”를 원한다는 사실. 최소 환경에서도 앱 자체 만들기는 쉽지만, 배포(앱스토어 장벽)/호스팅이 문제라서 가족/지인조차 월 5달러 내는 건 망설임. 사실 프로 개발자도 마찬가지.
          + 집에 웹서버+동적 DNS 연결로 셀프호스팅하면 됨.
          + 아이디어 공유 자체는 좋지만, 무료 호스팅/배포는 악의적 사용자 남용 위험이 있음.
     * “컴퓨터는 사람을 위해 일해야 하며, 요리나 워드프로세싱처럼 모두의 활동이 되었으면 한다” 같은 지향점 언급 보면서, 너무 일반적이라는 느낌. “라이브 업데이트 포함, 전부 무료. LLM이…”처럼 em-dash(-)가 지나치게 많아 AI 생성 티가 난다고 판단. 개인적으로 AI가 작성한 콘텐츠임이 보이면 흥미가 급격히 떨어지는 타입. 창작자 탓은 아니지만, 나 또한 이런 식의 카피 쓰는 데 그다지 흥미 없음.
          + 이런 스타일의 em-dash는 내가 10~15년째 써오던 IRL 문체. 나 역시 AI가 만든 콘텐츠를 소비하는 걸 별로 안 좋아하고, 누군가 프롬프트만 쳐준 거면 나도 LLM에 직접 묻는 게 낫다고 느낌.
          + 하이픈/엔대시/엠대시 용법을 따지면, em-dash를 구분자로 쓰는 건 원칙에 맞는 정답임. AI 표시로 간주하는 건 동의하지 않음.
          + Scrappy 공동 제작자 입장에서, 나는 오랜 매킨토시 유저라서 하이픈·엔대시·엠대시 구분 확실함 :) AI로 폴리싱 용도로만 가끔 쓴 거지, 텍스트 생성은 절대 아니었음. 직접 썼기 때문에 실제 작업량이 매우 많았음(실제 대부분의 작업은 Pontus가 맡음).
          + em dash 쓰려면 compose 키 후 하이픈 세 번. — 이런 식. 맥에서는 shift-option-hyphen임.
"
"https://news.hada.io/topic?id=21545","Zed 디버거, 드디어 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Zed 디버거, 드디어 출시

     * 2,000명 이상의 개발자가 요청한 디버깅 기능이 마침내 Zed에 구현됨
     * 속도, 익숙함, 구성 가능성을 중심으로 디버거가 설계됨
     * Rust, C/C++, JavaScript, Go, Python 등 인기 언어와 Debug Adapter Protocol (DAP) 기반 확장 지원이 제공됨
     * 직관적인 LOCATORS 시스템으로 별도의 설정 없이 대부분의 프로젝트에서 간편하게 디버깅 가능함
     * UI와 데이터 레이어의 분리된 아키텍처로 협업 디버깅 및 확장성에 뛰어난 기반이 마련됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Zed 디버거 출시

     * 2,000명 이상의 개발자들의 요청에 따라 Zed 에디터에 공식적인 디버깅 기능이 도입됨. 이는 Zed 1.0으로 나아가기 위한 매우 중요한 진전

주요 목표

     * 속도: 빠른 환경 전환과 효율적인 디버깅 경험 제공
     * 익숙함: Zed의 디자인 언어와 조화를 이루며, 전형적인 디버거 흐름에 기대되는 모든 기능 지원
     * 구성 가능성: UI, 키 바인딩, 디버그 설정 등 사용자가 자유롭게 맞춤 설정 가능

언어 및 확장 지원

     * Rust, C/C++, JavaScript, Go, Python 등 주요 언어를 기본적으로 지원함
     * Debug Adapter Protocol (DAP) 을 구현한 모든 디버그 어댑터와 연동 가능
     * 확장 시스템을 통해 보다 다양한 언어 및 디버깅 기능을 손쉽게 추가할 수 있음

간편한 디버깅 설정

     * LOCATORS라는 새로운 시스템을 도입하여 빌드 설정을 디버그 설정으로 변환함
          + 한 번 tasks.json에서 빌드 태스크를 작성한 뒤, debug.json에서 참조하거나, Zed의 자동 설정 기능을 활용할 수 있음
     * Zed가 내장 또는 Language Server의 실행 가능한 파일에서 자동으로 locators를 실행함
     * 대부분의 경우 별도의 디버그 설정 없이 바로 사용 가능함
     * 현재 Cargo, Python, JavaScript, Go에 대한 locator 지원 제공 (추가 언어 예정)

디버깅 세션의 기능

     * Zed 내에서 스레드, 변수, 브레이크포인트, 콜스택 등 프로그램 상태를 쉽게 확인 가능
     * 디버거 패널은 완전히 사용자화 할 수 있어서, 탭을 드래그·정렬하거나 패널을 자유롭게 이동 가능
     * 키보드 중심의 디버깅도 지원하여, 마우스 없이 코드 탐색, 브레이크포인트 토글, 세션 이동 가능

확장성 높은 아키텍처

     * 다양한 언어 디버깅, 협업 환경, 확장 지원, 효율적 데이터 캐싱 및 관리가 가능한 구조를 위해 2계층 아키텍처를 설계함
          + 데이터 레이어: 디버그 어댑터와 직접 통신, 세션 상태 유지, 응답 캐싱, 오래된 데이터 무효화 관리 담당
          + UI 레이어: 필요한 데이터만 요청, 인터페이스 렌더링에 주력
     * 이 분리 덕분에 협업(멀티플레이어) 디버깅 기능 구현이 용이하고, 네트워크 대역폭 절약 효과도 높음

확장 API와 DAP 적용

     * 70개 이상의 다양한 DAP 구현체가 존재하여, 모든 것을 기본 지원하는 대신 Zed의 확장 API를 확장하여 디버거 통합을 가능하게 함
     * 직접 스키마 정의, 어댑터 다운로드 및 실행 로직 구현, 디버그 설정 기본값 주입, locator와 자동 연동 등으로 DAP 지원 확장 가능
     * LSP(언어 서버 프로토콜) 확장 방식과 유사하게, 개발자가 자신만의 디버그 어댑터를 Zed에 손쉽게 통합 가능

인라인 변수 값 지원

     * 인라인 변수 값 표시 기능은 DAP가 아니라 LSP에 속해, DAP와 LSP가 모두 연계된 경우에만 기존 방식으로 제공 가능
     * 정규표현식 등 단순 매칭은 스코프, 주석 등의 이슈로 정확도가 떨어짐
     * Tree-sitter를 활용하여, 실행 중인 코드의 스코프 내 변수 식별이 정교하게 이루어짐
          + 별도의 LSP 연동 없이 .scm 파일을 통한 언어별 지원 가능
          + 출시 시점에 Python, Rust, Go 지원, 추후 더 많은 언어 추가 예정
     * Zed는 Tree-sitter의 창시자들이 만든 에디터임

향후 계획

     * 새로운 뷰: 워치 리스트, 메모리 뷰, 디스어셈블리 뷰, 스택 트레이스 등 고급 기능 추가 예정
     * 자동 설정: 더 많은 언어와 빌드 시스템에 대한 자동 설정 지원 확대 목표
     * 다듬기와 확장: Discord, GitHub 등을 통해 피드백을 받고, 적극적인 개선 의지 보유

부록

     * macOS, Linux에서 Zed를 사용할 수 있음
     * 개발자 채용 중 (관심 있으면 공식 사이트 참고)

   자바로 제드 쓰는 분이 계실까요...? ㅎㅎ

        Hacker News 의견

     * 디버거에 대한 작업이 진행 중인 것을 보니 정말 기쁜 감정임. 이 기능이야말로 내가 완전히 zed로 전환하지 못하게 막는 주된 기능임. 다만, “여기”라고 말하기에는 아직 부족함이 있음. 워치 윈도우, 스택 트레이스 뷰 부재 그리고 데이터 브레이크포인트 언급이 없는 점이 아쉽고, 이 때문에 베타 단계로 간주함. 해당 기능들이 언젠가 추가될 것이란 사실은 알고 있지만, 지금 제공되는 것은 내 디버깅 세션의 97%를 감당하기에는 충분하지 않음. 동시에 여러 디버깅 세션 지원, 멀티스레드 디버깅 계획이 발표에 더 명확하게 언급되었으면 좋겠음. 특히 RemedyBG처럼 특정 스레드를 ‘프리즈’하거나 하나만 ‘솔로’로 돌리는 등 멀티스레드 디버깅에서 쿨한 아이디어들도 궁금함
          + Laserbeam 님 안녕하세요, 저는 디버거를 개발했고 해당 글을 작성한 개발자임. 기본적인 스택 트레이스 뷰는 이미 지원하고 있음. 곧 멀티 버퍼 시스템 내에서 스택 트레이스 뷰가 도입될 예정이고, 현재도 디버깅 세션 중 “show stack trace” 액션으로 멀티 버퍼에 콜스택을 확장해서 각 프레임을 볼 수 있음. 다만, Zed 기준의 높은 품질에는 아직 못 미쳐 공개적으로 어필을 안 한 상태임. 워치 변수/식 기능 PR도 며칠 안에 머지될 예정임. 기능 완성은 됐지만, 출시에 임박해서 충분히 테스트되지 않은 기능을 넣는 것이 조심스러웠음. 데이터 브레이크포인트는 중요한 우선순위지만, 한동안 자동 설정 쪽에 집중할 계획이라 정확한 일정 안내는 어려움. 여러 세션과 멀티스레드 디버깅도 동시에 지원 중이며, 보완할 점은 남아있지만 기본 지원은 됨
          + 블로그 포스트에 고급 뷰가 개발 중임이 언급되어 있음. 이번 첫 출시와 발표는 기반을 다지는 데 초점이 맞춰져 있음. 앞으로 워치 리스트, 메모리 뷰, 디스어셈블리 뷰, 스택 트레이스 뷰 같은 고급 뷰가 추가될 예정임 [관련 링크]
          + 내 디버깅 세션은 항상 일반 브레이크포인트와 스테핑만으로 진행됨. 그래서 내 입장에서는 충분한 수준임
          + 나도 동의하는데, Zed 팀이 개발하는 속도를 보면 이런 기능들이 곧 따라올 듯한 예감임
          + 디버거는 아직 써보지 않았지만, 내 경우 Git 기능 때문에 비슷한 감정을 느낌. Zed에서 기본적인 Git 기능은 있지만 내 기존 워크플로 전체를 대체하기엔 아직 부족함. Git 쪽 개발도 계속 집중해주길 바라는 입장임
     * Zed는 정말 괜찮은 에디터임. 최근에 neovim에서 zed로 넘어가고 있는데 만족도가 높음. 모든 동작이 매우 빠르고 vim 바인딩도 잘 통합되어 있음. 에이전트 모드도 편리한 점임. VSCode에 비해 확장 생태계는 아직 부족하지만 내가 필요로 한 많은 작업은 충분히 커버됨. 디버거가 그동안 큰 결핍이었는데 이제 추가되어 정말 반가움
          + vim 바인딩이 얼마나 진짜 vim 느낌인지 궁금함. 대부분의 vim 에뮬레이터는 충분히 비슷하지만 오히려 너무 애매해서 키 입력이 자주 잘못되고 그게 더 답답함. 차라리 vim 느낌이 전혀 없는 에디터가 손가락이 계속 ‘틀리는’게 덜 짜증난다는 경험이 있음
          + Rust 코드 자동완성은 Zed에서 어떤지 궁금함. Windsurf나 Cursor처럼 “tab-tab-tab”으로 모든 것이 자연스럽게 자동완성되는 마법같은 환경이 있다면 정말 좋겠음. 특히 TypeScript나 스크립트 언어에서는 이런 방식의 자동완성이 거의 리팩토링 자동화라 할만큼 잘 작동함. IntelliJ/RustRover는 JetBrains 모델이나 Co-pilot을 써도 Cursor나 Windsurf 수준을 따라가지 못했음. Rust 자체 특성 때문이라고 생각함. 1) Rust에서도 그런 자연스러운 자동완성이 가능해졌는지, 그리고 Zed에서 되는지 궁금함. 2) Zed와 Cursor, Windsurf와 비교해서, 또 RustRover 및 JetBrains가 Rust AST를 다루는 방식과 비교해서 어떤 느낌인지 궁금함
     * Zed는 Lapce, Helix, Neovim이 그동안 이루지 못했던 것을 실현하는 느낌임. 2021~2022년쯤 Helix를 썼을 땐 버그나 통합 부족 때문에 결국 포기했고, 특히 예전 회사에서 쓰던 PHP 지원이 거의 없었음. Neovim은 제일 편했지만 유명한 커뮤니티 플러그인 중 강경한 스타일이 많았고 대안 플러그인은 너무 느림. 뭔가 안정적인 환경을 갖추기 위해 너무 많은 선택지를 고민해야해서 힘들었음. Lapce는 그냥 “VSCode 복제판” 같았고, 뭐가 특별한지 못 느꼈음. 아직도 데일리 드라이버로 쓸 수준이 아니라는 생각임. 그런 점에서 Zed는 짧은 시간 내 최고 애디터가 되었고, 요즘 매일 감사한 감정임. 디버거 추가도 정말 반가움
          + PHP 지원에 (예전 회사라서)라는 설명은 굳이 필요없음
          + “VSCode 복제판”이라 평가하는 것도 흥미로운 관점임. 인류 역사상 가장 인기 많은 에디터에 대한 재밌는 해석임
     * Zed가 점점 완성도 높은, 가볍고 여러 기능을 가진 IDE로 발전해가는 모습에 감탄하는 상황임. 내 생각에 DAP와 LSP는 지난 10년간 프로그래밍 도구에 일어난 최고의 혁신임
     * 처음에는 Zed에 관심이 있었지만, “AI”가 통합되기 시작하면서 흥미가 사라진 입장임. “AI”가 너무 많아져서 지친 시점임. 뭔가 더 괜찮은 게 나올 때까지는 Neovim을 계속 쓸 계획이고, 이런 변화는 “AI 버블”이 터진 뒤에야 올 것 같음
          + Zed는 내가 처음으로 AI 기능을 써보고 싶게 만든 에디터임. 전반적으로 탄탄한 기본기를 느꼈고 AI 느낌도 다른 에디터에서의 자동완성 정도로만 존재함. “당신이 원하는 건 AI가 아닌 좋은 빠른 에디터다, 우리는 그걸 만들었고 AI 기능도 넣었다”는 태도가 느껴짐. 경쟁사들은 “우린 AI가 메인이고 에디터는 그냥 곁들임” 같은 느낌인데, Zed는 중심이 다름
          + neovim을 찾아보니 두 개의 AI 제품에게 후원까지 받고 있어서 놀람. 직접 AI를 통합하는 수준은 아니지만, 이제 아예 피하기도 점점 힘든 상황임
          + 난 그냥 AI 관련 옵션을 모두 꺼 두고 사용함. 꽤 괜찮은 에디터임. 여전히 머지 컨플릭트 해결은 VSCode로 들어가야 하는 상태지만 만족함
          + Zed의 AI 기능이 실제로 얼마나 침습적인지, 설정으로 비활성화가능한지 궁금함
          + 평소 Zed를 쓸 때 AI 기능이 전혀 거슬리지 않음. 가끔 유용하게 쓰긴 하지만 자주 사용하지는 않음
     * 리눅스 지원이 나온 이후로 매번 일반 디스플레이(LoDPI) 지원이 생겼는지 확인하는 중임. 아직도 지원이 안 되어 아쉬운 감정임
          + 정말 답답한 상황임. 텍스트 렌더링이 코드 에디터의 기본인데 Zed 팀에서 일반(non-retina) 스크린을 쓰는 사람이 없는 것 같음. 관련 깃허브 이슈 링크
          + 임시방편이지만 BetterDisplay(무료 도구)를 설치하고 LoDPI 화면을 HiDPI로 바꾸면 텍스트 렌더링이 괜찮아짐
          + 리눅스의 1920x1200 노트북 화면에서 매일 사용하고 있는데 전혀 이상 없음
          + Windows 지원도 없고, 리눅스에서도 일반 스크린 미지원이라면 사실상 Mac 중심의 회사가 아닌지 궁금함
     * 현재 Python 프로젝트에서 Pyright를 쓸 때 Cursor 대신 Zed로 옮기고 싶은데 배터리 사용량이 너무 높아 정당화할 수 없는 수준임. 이 이슈가 이미 깃허브에 있고, 팀에서 우선순위를 높이지 않아 매우 아쉬움
          + 나는 오히려 Cursor에서 몇 시간 열어두면 맥북 M3 Max가 뜨거워지고 팬이 돌며 CPU를 거의 다 점유하는 문제가 발생함. 반면 Zed는 아무 문제 없이 잘 동작함. 결국 기술 스택이 조금만 달라도 사용 경험이 극과 극으로 갈린다는 점이 흥미로움
     * Zed가 진짜 제품 개발의 예시라고 생각함. 또 다른 크롬 엔진 재포장이 아니라서 정말 만족스러운 선택지임
     * 솔직히 느린 부분이 있어서 놀람. 탭 목록에서 파일 전환 시 딜레이가 있고, 타이핑 반응도 Emacs(lsp-mode 활성화)나 웹 브라우저보다 느림. 메모리도 Emacs보다 60MiB 가량 더 씀. 대신 시작 속도는 정말 빠름. 단순히 ‘빠른 에디터’라는 슬로건과 달리 Emacs Lisp + C 코어보다도 느린 상황임. 플러그인 구조를 살펴보니 WASM으로 컴파일되어 VM에서 도는 형식인 것 같음. 그게 원인인지 궁금함
          + 어떻게 해서 emacs보다 zed가 더 느려졌는지 궁금함. 내 경험상 zed는 거의 지연이 없을 정도로 빠름. Edting, lsp, 파일스위칭 모두 즉각적임. 오히려 emacs는 지연 문제 때문에 결국 포기한 적이 많음(특히 원격 개발 환경에서)
          + 플러그인들이 WASM으로 VM에서 돌아서 느린 것 아니냐는 질문에, 내가 본 플러그인들은 서버 런칭 같은 것만 해주기 때문에 타이핑 반응과 직접 관련 없음. 오히려 GPU 사용이 원인일 것 같음. GPU 컴포지팅에서 딜레이 생기기 쉽고, 이미 OS단의 렌더링과 중첩될 수 있음. emacs도 이벤트 루프 건너뛰고 UI를 직접 갱신하는 트릭을 사용해 호환성 문제가 생겼던 기억 있음
          + emacs에는 dape 패키지라는 잘 설계된 DAP 기반 디버거가 있음. 관련 링크 의존성 없이 설계되어 차후 기본 emacs에 포함될 가능성이 있음
          + 렌더링 파이프라인의 이슈일 수도 있음. 사용하는 운영체제가 궁금함
     * zed 팀에게 부탁하고 싶은 점은 제대로 된 C와 C++ 언어 감지를 해달라는 점임. 모든 에디터가 C를 C++처럼 다루는 실수를 반복함(C는 C++와 다르며, 혼동하면 안 됨), compile_commands.json에서 C스탠다드 지정해도, C++ 문법 오류 코드인데도 C++로 인식하는 일이 많음. 언어 감지만 제대로 되면 매우 좋은 에디터임
          + 설정에서 파일명/경로 기준 언어 감지 규칙을 커스터마이즈할 수 있음. 다만 새 파일을 만들 때는 에디터가 언어를 추측해야 하는 상황임
"
"https://news.hada.io/topic?id=21495","Nanonets-OCR-s – 문서를 구조화된 마크다운으로 변환하는 OCR 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Nanonets-OCR-s – 문서를 구조화된 마크다운으로 변환하는 OCR 모델

     * 단순한 문자 인식 수준을 넘어 문서 전체를 Markdown 구조로 변환하는 고성능 이미지-to-Markdown OCR 모델
     * 수학식은 LaTeX 형태로 변환하고, 이미지에는 자동 설명을 추가하며, 표는 HTML/Markdown 표로 출력해 LLM 활용에 최적화된 출력물을 생성
     * 서명, 워터마크, 체크박스 등을 인식하여 <signature>, <watermark>, ☐/☑ 형태로 변환하는 등 문서 구성 요소별 처리 능력이 뛰어남
     * Hugging Face의 Transformers 또는 vLLM 서버를 통해 손쉽게 활용 가능하며, docext 라이브러리를 통해 웹 앱 형태로도 사용 가능
     * 다양한 문서 유형과 복잡한 레이아웃에 대해 정확도와 구조화 수준이 매우 높아, 계약서, 양식, 리포트 등에서 매우 유용함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요 및 중요성

     * Nanonets-OCR-s는 일반적인 OCR(광학 문자 인식) 기능을 넘어, 문서의 의미와 구조를 분석하여 결과를 마크다운 형식으로 출력하는 혁신적인 모델임
     * 수식, 이미지, 표, 체크박스, 서명, 워터마크를 각각 구분하고, 의미 있는 태깅 및 표현 방식(LaTeX, HTML, 마크다운 등)으로 변환해 downstream 작업이나 LLM(거대언어모델) 기반 문서 처리에 적합한 형태로 제공함
     * 기존 오픈소스 OCR 프로젝트와 비교해 복잡한 문서 구조와 다양한 시맨틱 요소 추출에 더 높은 성능과 자동화된 워크플로우 적용 가능성을 보임
     * Qwen2.5-VL-3B-Instruct 모델을 파인튜닝한 버전으로, 다양한 문서 구성 요소를 식별하고 의미 있는 마크업으로 태깅함

핵심 특징

     * LaTeX 수식 인식
          + 문서에 포함된 수식과 수학 공식을 inline($...$) 또는 display($$...$$) 유형에 따라 자동으로 LaTeX 문법으로 변환·출력함
     * 지능형 이미지 설명
          + 각 이미지에 대해, <img> 태그 내에 이미지의 성격, 스타일, 내용 등을 상세히 기술함
          + 로고, 차트, 그래프 등 다양한 이미지도 문맥, 의미와 함께 설명하여 LLM 활용에 적합한 입력으로 변환함
     * 서명 탐지 및 분리
          + 서명 이미지를 텍스트와 구분해 별도 <signature> 태그에 담아 처리함
          + 법률 및 비즈니스 문서 자동 처리에 활용 가치가 높음
     * 워터마크 추출
          + 문서에 삽입된 워터마크 텍스트를 감지해 <watermark> 태그 내에 따로 정리함
     * 체크박스 및 라디오 버튼 변환
          +
               o ☐ (미체크), ☑ (체크됨), ☒ (거부) 형태의 표준 유니코드 기호로 출력
          + 설문지, 신청서 등 양식류 문서 처리 신뢰도 향상
     * 복잡한 표 구조 추출
          + 복잡한 테이블도 마크다운 및 HTML 테이블 형태로 변환하여 높은 활용성 제공함

주요 활용 방식

  Python 코드 예제

     * Hugging Face의 transformers 라이브러리를 활용해 손쉽게 모델을 불러와 실행 가능함
     * 이미지를 입력으로 넣으면, 텍스트, 표, 수식, 이미지 설명, 워터마크 등 구조화된 마크다운 형태로 결과 제공함
     * 페이지 번호나 워터마크는 , 등으로 구분해 래핑함
     * 체크박스는 유니코드 심볼(☐, ☑)로 출력함

  vLLM 기반 활용

     * vLLM 서버에 모델을 등록하고 OpenAI 호환 API로 손쉽게 접속·활용 가능함
     * 이미지 입력 시, 텍스트, 표, 수식, 워터마크 등 일관된 형식으로 변환 결과 출력함

  docext 패키지 활용

     * docext라는 별도 패키지를 통해, 별도의 명령어 실행만으로 Nanonets-OCR-s를 도큐먼트 구조화에 바로 적용할 수 있음
     * GitHub 문서 참고 가능

        Hacker News 의견

     * 저는 Nanonets에서 일하고 있고, Nanonets-OCR-s라는 3B 규모의 VLM 모델을 공개하게 돼서 굉장히 기대감 가짐
       이 모델은 문서를 깨끗하고 구조화된 마크다운으로 변환하는 데 최적화된 경량 모델임
       문서의 구조와 맥락(테이블, 수식, 이미지, 도표, 워터마크, 체크박스 등)을 학습했기 때문임
       주요 기능으로는 라텍스 수식 인식(인라인과 블록 수식을 제대로 구분해서 변환), 내장 이미지 설명(img 태그 이용, 차트/로고/도표 등 지원), 서명 탐지 및 분리(signature 블록 내 출력), 워터마크 추출(watermark 태그로 저장), 똑똑한 체크박스/라디오 버튼 처리(유니코드 변환으로 후처리 신뢰성 향상), 복잡한 테이블 구조 추출(다중 행/열 구성 테이블도 마크다운과 HTML로 잘 출력)이 있음
       직접 사용해보고 싶으면 Huggingface나 Docext Colab 참고 가능
          + Docext의 올바른 링크는 README.md임
          + 사용 중인 LLM에서 환각(hallucination) 현상이 나타나는지 궁금함
          + 이미지 자체 추출이 가능 여부, 아니면 여전히 별도 추출 과정이 필요한 것인지 궁금함
          + 레스토랑 메뉴 사진이나 PDF를 JSON 스키마에 맞게 파싱하는 데(아마 후처리용 LLM의 도움과 함께) 활용 가능한지, 아니면 대형 멀티모달 LLM이 이런 용도에 더 적합한지 궁금함
     * 나는 Shipibo(페루 원주민어)-스페인어 사전을 영어 사전으로 번역하려고 여러 LLM을 시도했지만, 두 컬럼이나 이상한 줄바꿈, 정의에 Shipibo와 스페인어가 혼합돼 있어서 이해가 어려움
       게다가 스캔 품질도 좋지 않음
       이 모델을 한 번 시도해봐야겠다는 생각
     * 수십 년간 Word와 PowerPoint에 저장된 자료들을 모두 받아서, 각각의 요소를 다른 포맷으로 재사용할 수 있도록 표준화된 형태로 변환하는 솔루션을 계속 찾고 있었음
       이건 그 시스템을 구축하는 데 꼭 필요한 핵심 빌딩 블록임
       이제는 아카이브나 히스토리 기능이 필요함, 즉 각 요소를 쉽게 아카이빙하고 불러올 수 있으면 좋겠음
       정말 멋진 작업임
          + unoconv나 pandoc으로 기본 변환 후, LLM을 활용해 플레인 텍스트를 정제하는 접근으로 시작하는 게 더 간단하지 않을까 하는 의견
     * 이런 모델들이 마크다운만을 목표로 한다는 게 아쉬움
       실제로 마크다운은 버전이 다양하고, 각주나 참고문헌, 그림 등에 대한 지원이 부족
       더 구조적이고 명확한 스펙을 가진 포맷이 필요함
          + 실제로 우리는 마크다운으로 변환하면서 동시에 시맨틱 태깅까지 모델에 학습시킴
            예를 들어 식은 LaTeX로 추출하고, 이미지(도표, 그림 등)는 img 태그로 상세히 묘사함
            서명(signature), 워터마크(watermark), 페이지 번호 등도 태그 활용
            복잡한 테이블(다중 행/열)은 마크다운이 아닌 HTML 테이블로 뽑음
          + ""구조화 마크다운""이라는 개념이 LLM OCR 모델 자체보다 더 기대됐는데, 결국은 특정 요소에 태깅만 하는 수준이라 모델 외에서의 활용도는 조금 제한적이라는 느낌
     * docling(https://github.com/docling-project/docling)과 비교하면 어떤 장단점이 있는지 궁금함
     * Datalab/Marker(https://github.com/datalab-to/marker)와 어떤 차이가 있는지 궁금함
       많은 PDF->MD 변환기를 비교했는데 Marker가 현재까지는 가장 좋았지만 완벽하지 않음
          + 개인적인 경험 기준, 복잡한 수식과 코드가 뒤섞인 논문 변환에 Marker가 꽤 잘 맞음
            예를 들어 Fortran 역 라플라스 변환 논문 중 수식(인라인/디스플레이 혼합)과 모노스페이스 코드 블록이 한데 섞인 페이지를 Marker로 처리하면, 인라인 $\sigma_0$가 ""<sup>s</sup> 0"", $f(t)$가 ""<i>f~</i>~t*!""처럼 망가짐
            지금 모델은 이런 부분을 정확히 내부적으로 출력함이 강점임
            참고 스크린샷(https://imgur.com/a/Q7UYIfW)
          + 나만의 교차 비교를 막 시작했는데 혹시 후보 리스트를 알려 줄 수 있으면 정말 고맙겠다는 요청
     * 직접 Powershell로 이 모델을 어디서나 PDF에 적용하는 스크립트를 만들었음
       직접 해보니 GPU(1080 8GB)가 구식이라 실행 속도는 상당히 느림(페이지당 최소 5분 이상)
       만약 Cloud Run(외부 GPU 지원)에서 작동하는 PDF to markdown 변환 유틸리티를 써보고 싶다면 알려 달라는 의견
       완성되면 링크도 공유할 예정임
          + 방금 Cloud Run에서 동작시키고 샘플 결과를 리포트함
            animate.pdf의 일부 결과를 보면, 타이틀, 저자, 출판사, 흑백 일러스트(img 태그로 설명), 구글 디지털화 태그가 잘 추출됨
            목차도 테이블 형태로 완벽하게 뽑히는 모습임
            속도만 느린 걸 빼면 기능과 정확도는 매우 만족스러움
          + Cloud Run 이용 PDF to markdown 서비스에 매우 관심이 많음
     * 다중 컬럼 또는 다중 행 테이블이 있는 문서(예시: 이 PDF의 1페이지 rowspan, 29페이지 colspan 등)를 어떻게 처리하는지 궁금함
     * 비영어 텍스트 인식 성능이 어떤지 궁금함
       기존 LLM 기반 OCR은 그 외국어 지원 성능이 전통적인 OCR에 훨씬 못 미친다고 알고 있음
          + 이건 경험담인지 아니면 이해 기반인지 궁금함
            내 경험으론 구글 번역과 ChatGPT를 이미지에서 직접 사용할 경우, ChatGPT 성능이 항상 더 나음
            특히 일본어 손글씨 메뉴도 번역/설명까지 잘해줌
     * 다국어 지원 언급 없는 모델은 현실에선 영어 아닌 PDF에서 성능 매우 떨어짐
          + 실제로 영어 위주로 훈련하긴 했지만, 일부 훈련 데이터에 중국어 및 다양한 유럽어도 포함되어 있음
            게다가 기본 모델(Qwen-2.5-VL-3B)은 멀티링구얼임
            Reddit에서 중국어도 잘 된다는 글을 본 적 있음(링크)

   한국어 카드 영수증 샘플 처리해봤는데 속도는 느리지만 완벽하게 읽어오는 오고 있음
"
"https://news.hada.io/topic?id=21541","Ask GN: 5조원 질문. 당신에게 5조원이 주어진다면 무얼 하시겠나요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Ask GN: 5조원 질문. 당신에게 5조원이 주어진다면 무얼 하시겠나요

   제가 모든 지인들에게 하는 질문들이 있는데요, 바로 3천억원이 주어진다면 뭘 하고싶은지 입니다.

   그동안 들었던 재미있는 대답으로는
    1. 오징어 게임을 열겠다.
    2. 돌이 되겠다 (어떻게든)

   많이 들었던 대답으로는
    1. 가족들과 세계여행
    2. 가족들과 크루즈여행

   그리고 다른 걸로는
    1. 각종 재단을 만들어서 사람들에게 도움을 준다
    2. 벤처캐피탈 설립하여 유망한 스타트업에 투자하여 세상에 기여

   등등이 있는데요. 제가 이 질문을 사람들에게 하는 요지는 사람들의 대부분이 경제적 부족을 채우기 위해서 움직이는데 만약에 이것이 다 해결되면 그 사람은 무엇을 할까에 집중하고 싶었습니다.

   이름을 심플하게 만들기 위해 기존 3천억 질문에서 약 7조원 질문이 돼버렸습니다. 아무튼 다른 사람들의 생각이 궁금하기도 하고 모아서 봐보고 싶어 만들게 됐습니다. 감사합니다

   일단 받고나서 알려드리겠습니다 입금 부탁드려요

   5조원을 바라보며 즐거워하기

   저는 성남 FC의 오랜 팬인데, 5조원이 있다면 구단을 인수할 거에요.

   저는 5조씩이나 있다면, IT인프라 타운을 만들어보고 싶네요.
   부동산 임대업과 투자를 동력으로 삼고, 입주기업과 그 기업에 다니는 인재에게 부동산을 저렴하게 대여해주는 것으로 ..

   오 저도 비슷한 생각을 했었습니다. 마이크로소프트 타운 같은 걸 만들어보고 싶다... 요즘 세상이 젊은이들에게 너무 가혹한것같아서요 ㅜㅜ

   자신의 결핍이 해소된다면(5조원이 생긴다면)
   1.결핍 때문에 미뤄두었던 일 도전(이건 5조가 아니라 50억이어도 해소되겠지요)
   2. 5조여서 가능한건 타인의 결핍도 해소해보려고 도전.
   이런걸 생각하다보면 가장 큰 결핍은 돈의 결핍으로부터 시작되었지만 결국 돈 보다는 꿈의 결핍을 느끼게 됩니다. 좋은 시도입니다.

   재밌는 질문이네요. 본문은 7조인데, 제목은 5조네요? 암튼, 그 규모에선 2조 차이는 중요하지 않겠죠.
    1. GPU를 2조원어치 삽니다. 1조는 AI회사 운영비로 사용해서 LLM 개발에 투자.
    2. 1조는 복지재단을 만들어서 어려운 사람들에게 현금지원을 하는 방향으로 운영
    3. 1천억으로 바이오 연구소를 만들어서 제가 관심있는 연구를 진행
    4. 나머지 9천억은 여유자금이 되겠네요. 제주도 경치좋은 곳에 팬션을 하나 구입해서 살고 싶네요. 생활비로 1년에 10억쓰기를 목표로 해보고, 매년 조금씩 늘려보기.

   앗 그렇네요 ㅋㅋ 5억 달러와 n조원 단위를 왔다갔다하며 햇갈렸나봅니다. 뭔기 인류에 공헌하시는게 목표로 보이네요 감사합니다

   저만 5조 원이 생긴다면, 관점이 바뀔 것 같네요.
   시간이 지금보다 훨씬 소중해지고, 온전한 시간을 더 오래 쓰기 위한 건강에 신경을 쓸 것 같아요.
   더 이상 투자할 가치를 재지 않고, 내 건강과 피부, 젊음을 유지하기 위해 집중하기 시작하겠죠. 그리고 난 뒤엔 내가 뭘 하고 싶은 지에 집중할 거에요.
   어쩌면 필요에 의해 유지하고 있던 무의미한 관계들을 끊고 내면의 소리를 들을 겁니다. 예를 들면 저는 카드와 현금만 들고 어디든 다른 나라로 가서 각 나라의 문화에 섞여서 지내 보고 싶은 욕구가 있어요. 이젠 잃을 것이 많으니 비교적 안전한 국가로 갈 것 같네요.
   그 뒤엔 사람들의 관심을 얻고 싶을 거에요. 지금 명확히 상상하기엔 한계가 있지만, 그저 관심이 아니라 명성을 얻고 싶을 겁니다.
   여행하며 느낀 걸 기반으로 책을 쓰거나, 여행 자체를 유튜브로 올릴 수도 있고, 자문을 받아서 더 똑똑한 방법을 쓸 수도 있겠죠.

   주어진 환경은 점점 익숙해지고, 제게 주어진 환경 덕에 생각할 수 있는 것들이 당연하게 느껴질 거라 생각해요. 주변 사람들이 왜 진정한 가치를 찾지 못하고 저렇게 현실에 치여서 낭비하는지 얘기하거나 자기개발서 같은 걸 쓸 수도 있어요.

   답변 너무 감사드립니다. 생각할 거리가 많아지네요

   오 저도 비슷한 질문 하고 다닙니다 ㅎㅎ 결핍을 채우는 것을 넘는 꿈이 있는지 물어보는 질문으로요.
   꿈은 아주 많이 있지만, 그 중 하나는 교육 재단을 차리는 겁니다.
   순수한 지적 호기심을 중요하게 생각하는데, 이를 중심으로 교육 이념을 채운 시스템을 만들고 싶어요.

   고견 감사드립니다
"
"https://news.hada.io/topic?id=21555","MCP 명세 – 2025-06-18 버전 변경점","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       MCP 명세 – 2025-06-18 버전 변경점

     * MCP 스펙의 새 업데이트는 구조화된 메타데이터와 컨텍스트 관리 부분에 주안점을 둠. 확장성 향상과, 다양한 시스템 간 상호운용성 강화 목적
     * 새로운 데이터 필드가 추가되며, 기존의 필수 필드들이 좀 더 구체적으로 정의. 메타데이터 구조의 계층화로 인해 시스템별 별도 확장 방식 지원이 가능해짐
     * 컨텍스트 추적과 속성 갱신을 위한 명확한 규칙 제시, 이전 대비 일관된 상태 정보 관리 기능이 강조됨
     * 권한 관리 및 데이터 검증 절차가 프로토콜 명세에 명시됨. 새롭게 추가된 일부 필드는 향후 프로토콜 버전과의 호환성을 염두에 둔 것
     * 크로스 플랫폼 통합 지원: 여러 AI 플랫폼, 클라우드 서비스 환경에서도 일관된 방식으로 컨텍스트 데이터를 교환할 수 있는 기반을 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * MCP(Model Context Protocol)는 머신러닝 모델 또는 대규모 언어 모델 등 다양한 AI 시스템 간의 컨텍스트 메타데이터 교환을 위한 프로토콜임

Major changes

    1. JSON-RPC 배치(batching) 지원 제거 (PR #416)
    2. 구조화된 도구 출력(structured tool output) 지원 추가 (PR #371)
    3. MCP 서버를 OAuth 리소스 서버로 분류, 보호된 리소스 메타데이터를 추가해 연동 Authorization 서버를 찾을 수 있도록 개선 (PR #338)
    4. MCP 클라이언트가 RFC 8707의 Resource Indicator 구현 필수 (악의적 서버의 액세스 토큰 획득 방지 목적) (PR #734)
    5. Authorization 명세 내 보안 고려사항(security considerations) 및 베스트 프랙티스 명확화, 별도 보안 가이드 페이지 추가
    6. Elicitation(질의 요청) 기능 추가, 서버가 사용자에게 추가 정보를 요청할 수 있도록 지원 (PR #382)
    7. Resource Links 지원 추가, 도구 호출 결과에 리소스 링크 포함 가능 (PR #603)
    8. 프로토콜 버전 협상 시, HTTP에서 MCP-Protocol-Version 헤더 필수 (PR #548)
    9. Lifecycle Operation의 SHOULD를 MUST로 변경 (참고)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Other schema changes

    1. _meta 필드가 더 많은 인터페이스 타입에 추가됨 (PR #710), 적절한 사용법 명시
    2. CompletionRequest에 context 필드 추가, 이전에 해석된 변수 포함 가능 (PR #598)
    3. 프로그램용 식별자와 별도의 사용자 친화적 표시를 위한 title 필드 추가 (name은 코드 식별자 용도로, title은 표시용) (PR #663)

   핵커뉴스 댓글이 좀 아쉽네요. stdio 만 봤나본데 지금 remote mcp server나 그걸 중개해주는 registry들이 얼마나 우후죽순 생겨나는데....

        Hacker News 의견

     * 내가 MCP(Machine Context Protocol) 붐을 타면서 가장 크게 배운 점은, 백엔드 소프트웨어 개발에서는 굳이 MCP를 쓸 필요가 없다는 사실임. 아키텍쳐적으로 맞지 않는 부분이 있음. 특히 Elixir 같은 환경에서는 더더욱 그렇다는 생각임. API 하나마다 서버를 따로 둔다면, 500개의 API에 500개의 마이크로서비스를 돌린다는 말이 됨. 직접 20가지 MCP 서버를 써 본 후에야 깨달았는데, 결국 MCP도 함수 호출의 껍데기였음. 각 API는 서버가 아니라 별도의 모듈로 만들면 충분함. 괜히 최신 MCP 사양을 따라갈 필요도, 사양 변경될 때마다 수백 개 마이크로서비스를 업데이트할 필요도 없음. 그냥 불필요한 복잡성이라는 결론임
          + 클라이언트가 게이트웨이나 BFF를 통하지 않고 각 마이크로서비스에 곧바로 연결하지 않는 이상, 그냥 MCP를 전체 서비스 앞에 두고 API, GraphQL, RPC 등 기존 방식처럼 기능만 노출하면 됨. 사실상 LLM에 특화된 인터페이스라는 느낌임. OpenAPI 사양 기반의 tool call도 충분히 쓸 수 있음. 어쨌든 모든 마이크로서비스들끼리 MCP로만 통신하게 하는 건 너무 과한 상상임
          + 나는 MCP를 Claude처럼 API 비용 없이 function call을 가능하게 해주는 플러그인식 통합 솔루션 정도로만 봤음. 만약 네가 API를 쓰고 있는데 급할 일 없다면 굳이 필요 없는 선택지임
          + 사실 MCP는 클라이언트와 모델을 연결해주는 표준 프로토콜이라는 생각임. 단순히 툴 호출을 감싸 주는 용기는 아님
          + 맞음, API마다 MCP 서버 하나씩 두는 건 확장되지 않는 구조임. nango.dev 같은 걸 사용하면 한 서버에서 400개 이상의 API를 포괄할 수 있음. 인증 처리, 가시성 확보, 그리고 직접 tool call할 수 있는 다양한 인터페이스도 제공함. (참고로 나는 창업자임)
          + 나는 한발 더 나가서 LLM에게 JSON 출력을 강제하는 문화 자체가 어리석다는 생각임. LLM이 직접적으로 좋아하지도 않는 까다로운 포맷에 맞추려 시간과 노력이 너무 들어감. 훨씬 제약이 많은 텍스트 기반 DSL이 훨씬 좋은 대안이었음. 예전 GPT 3.5 때는 프롬프트에 간단한 예시 몇 개 주는 것만으로도 영어 기반 DSL을 신뢰성 있게 출력할 수 있었음. 그런데 최신 모델들조차 JSON schema의 일부를 종종 무시한다는 주의가 남아 있음. 네모난 구멍에 동그란 못을 억지로 박는 느낌임, 모두들 억지로 하지 않았으면 좋겠음
     * 이제 인증된 MCP 서버로 가는 간단한 경로가 생긴 걸 정말 기쁘게 생각함. MCP 커뮤니티와 Anthropic 팀에게 진심으로 감사를 표하고 싶음
          + MCP 서버가 왜 필요한지 잘 모르겠음. 에이전트가 RPC를 하고 싶다면 그냥 RPC를 쓰면 충분하지 않은가, 라는 생각임
     * 핵심 사양이 OpenAPI나 다른 것 대신 TypeScript로 작성된 점이 정말 신기하게 느껴짐. 타당한 이유가 있겠지만 그래도 의외라는 인상임
          + 왜 이게 놀라운 건지 궁금함. 나도 타입스크립트를 많이 쓰지만, 이런 관점은 생각해본 적이 없어서 언어 디자인 쪽에서 어떤 결정이 있었는지 궁금한 마음임
     * WWW-Authenticate 챌린지가 도입된 점이 너무 반가운 소식임. 이제 MCP 서버가 클라이언트를 리소스 제공자의 OAuth 플로우로 넘기고, Authorization: Bearer ... 헤더를 받아오기만 하면 되는 그림이 명확해졌음
     * <i>대부분</i> 불필요한 복잡성이 맞다고 생각하지만, 배치 실행 기능은 아쉬움. '이 작업 모조리 다 해놓고, 결과를 한 번에 응답'하는 걸 구현하는 게 재밌었음. 물론 클라이언트가 알아서 배치 응답을 묶어도 되긴 하는데 여전히 재미가 있음
          + 맞음. JSON-RPC 배치 기능은 정말 “와, 이거 신기하네” 했던 경험임. 사양에서 빠지는 건 아쉽지만, 결국 복잡성만 더하는 측면도 있어서 이해는 함
     * elicitation(추론 기반 프롬프트 처리)이 큰 수확이라고 생각함. 내가 가장 좋아하는 MCP 서버 중 하나는 직접 만든 SSH 서버임. 전체 서버 작업의 90%를 자동화할 수 있음. 인증은 설정 파일로 관리했는데, 새 서버에 접속할 일이 생길 때마다 직접 수정해야 해서 살짝 번거로움이 있음
          + 이런 경우엔 fabfile.org 같이 쓸 수도 있다는 마음임. 굳이 LLM을 도입하지 않아도 되는 대화라면 사적으로 두는 게 더 낫다는 생각임
     * 지난 며칠간 MCP로 데이터셋 래퍼를 만들며 놀아봤던 경험임

    1. LLM 분야에서 가장 흥미진진한 시도라는 생각임. 물론 기존 API & tool call로도 비슷한 걸 할 수 있겠지만, 기술에 익숙하지 않은 친구에게도 Claude용 MCP URL만 던져주면 클릭 몇 번으로 써보는 게 굉장히 인상적이었음
    2. csharp SDK를 쓰고 있는데, 인증 기능이 아직 브랜치에만 있어서 아주 초기 상태임. MCP 통합 중 95%의 시간이 인증 구현에 쏠렸음(로컬 빌드가 아니라면 필수임). 문서가 더 나오면 개선될 테지만, 지금은 꽤 손 많이 가는 과정임
    3. 개발자 로그 노출이 부족하다는 점도 있음. Claude가 웹(데스크톱 말고)에서 뭘 주고받는지, 어디서 오류가 나는지 개발자 모드에서 request/response 로그를 보여줬으면 좋겠음. 인증 갱신 문제로 한참 헤맸는데, 내가 잘못된 엔드포인트를 로깅하고 있었다는 걸 늦게야 알아차림. 만약 더 나은 MCP 로깅이 있었다면 몇 분 만에 끝나던 일이었음. 데스크톱과 MCP inspector에서는 잘 됨
    4. 가장 고민인 건 장시간 작업 처리임. 내가 노출하는 데이터셋은 여러 PDF 문서임. Claude 자체적으로 PDF를 다루는 건 불가능해 보이고(방법 있으면 누구든 알려주길 바람!) 일단 gemini를 거쳐서 텍스트로 변환한 뒤 MCP로 넘기는 방식임. 간단한 문서는 잘 되지만, 긴 문서는 처리 시간이 길어짐. 지금은 '처리 중이니 나중에 다시 시도해달라'는 안내만 보냄. progress API가 있긴 한데, 서버에 지속적으로 연결을 유지해야 해서(Cloudflare 상으로는 일정 시간 후 끊김) 실용성에 한계가 있는 듯함. LLM이 x초 후 다시 확인하게 하고, 그 전엔 다른 작업을 하게 만든 상태에서, 타이머가 끝날 때까지 '실행 잠정 중단'되는 방식이 있으면 정말 좋겠음. 현재로서는 연결을 유지할 때 LLM이 아무것도 못하고 대기 상태가 되거나, 아니면 job ID 주고 반환하면 불완전한 응답만 나와서
       전체 맥락이 빠지는 경우가 많음. 10분 이상 걸려야 하는 작업에는 큰 장애물이 될 수 있다는 생각임

     * 장기 실행 작업은 아직 공개적으로 논의되고 있는 주제임. MCP도 앞으로 이를 해결할 의도가 있는 걸로 알고 있음. 여러 제안들이 오가고 있고, 항상 작업이 오래 걸릴지 알 수 없으니 장기 작업 API를 분리하는 것만으론 해답이 안 됨. 이 부분을 통합적으로 풀자는 내 제안도 있음 discussion 링크
     * MCP 사양이 빠르게 개선되는 점이 매우 반가움. 새로운 릴리즈마다 기존에 내가 MCP 통합에서 아쉬웠던 부분이 하나씩 채워지는 걸 확인할 수 있음
     * 사양 변경이 합병되려면 단 한 번의 승인이면 된다는 점이 재밌게 느껴짐
     * MCP가 실제로 무엇을 해결하는지 잘 모르겠음. 개인적으로는 노트북에서 뭔가 빨리 프로토타이핑 할 때 역할 정도만 떠오름. 로컬 프로그램을 만든다면 LLM이 접근할 수 있는 툴셋을 훨씬 세밀하게 제어하고 싶음. 예를 들어 Google Calendar용 MCP 서버를 생각해 봐도, MCP가 시간을 크게 절약해주지 않음. 동일 API를 내가 직접 쓸 수도 있고, LLM에게 Google Calendar를 언제, 어떻게 호출할지 명시적으로 알려줘야 하니 3자에게 위임하고 싶지 않음. 또 어떤 언어로 MCP가 작성되었는지 상관없이 내 환경에서 임의로 프로세스를 띄우는 것도 부담임. 내 쪽이 Python인데 사용자에게 타입스크립트 런타임을 추가로 요구해야 한다면 곤란할 수 있음. MCP 래퍼에 취약점 생기면 보안 이슈도 걱정임. 서버 환경에서는 정당성 확보가 더 어려움. 서로 다른 머신에서 구현 세부사항
       몰라도 호출할 수 있는 이미 훌륭한 방법이 RPC임. MCP는 그 위에 의견 강한 미들웨어와 보안 문제만 더하는 느낌임
          + 내가 이해 안 가는 건, 왜 지금까지 본 MCP들은 전부 커맨드(명령어) 기반이고 HTTP 인터페이스를 쓰지 않는지 모르겠음. 만약 HTTP 방식이면 한 서버만 띄워서 조직 전체가 공유할 수 있고, 각자 툴체인 세팅 필요 없이 쓸 수 있음
          + 백엔드가 고정된 흐름을 강제하는 기존 방식과 달리, MCP를 쓰면 LLM 자체가 직접 오케스트레이션을 수행하는 게 장점임. 예를 들어 웹 검색을 할 때 쿼리를 재수정해서 원하는 정보를 찾을 때까지 재시도할 수 있음. CLI로 특수한 문제를 풀 때도 여러 툴을 적절한 순서로 활용함. 이건 고정된 흐름에서는 할 수 없는 조직화임
          + MCP가 해결하는 부분은, LLM 중심에서 agent에 tool 등 여러 기능을 표준화된 프로토콜로 연결해주는 점임
          + 나도 강하게 공감하는 부분이 있음. 실제로 써 보면 굉장히 느린 느낌을 받음. 나는 2년 전에 LLM 클라이언트 개발하려고 직장을 그만뒀는데도 아직 Google calendar 연동 못함. 그나마 사용자 입장에서는 이런 임시 구멍을 메꿀 수 있는 게 MCP의 쓸모임. 예전에 아이폰 홈 화면 상위 3줄은 비슷해도 맨 끝줄은 각자 다 제각각이었던 이야기가 기억남. 앞으로도 IT 부서와 개발팀들은 각자 자기 업무에 맞는 MCP 서버를 계속 만들 거라는 예감임
"
"https://news.hada.io/topic?id=21505","Show GN: 질문 목록 정리 및 이동하기 - (Chatgpt, Claude, Perplexity, Gemini 지원)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: 질문 목록 정리 및 이동하기 - (Chatgpt, Claude, Perplexity, Gemini 지원)

   Show GN에 올려주신 ChatGPT 지원 크롬 확장프로그램(https://news.hada.io/topic?id=19647) 잘 사용
   하다가 다른 플랫폼도 지원하면 어떨까 싶어 만들었습니다.
    1. Chatgpt, Claude, Perplexity, Gemini 지원
    2. 질문을 리스트로 정리하여 이동이 쉽게 만들었습니다.

   피드백이나 의견 남겨주시면 감사하겠습니다!
"
"https://news.hada.io/topic?id=21517","Google, Gemini 2.5 Flash/Pro 정식 출시 및 `Flash-Lite` 모델 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Google, Gemini 2.5 Flash/Pro 정식 출시 및 `Flash-Lite` 모델 공개

     * Gemini 2.5 Flash와 Pro 모델의 정식 출시와 함께, 가장 저렴하고 빠른 Flash-Lite 모델의 프리뷰 버전을 공개함
     * Flash-Lite는 번역, 분류 등 지연에 민감한 작업에 특화되어 있으며, 2.0 Flash/Flash-Lite보다 낮은 지연 시간과 높은 전반적 품질을 제공함
     * 모든 2.5 모델은 멀티모달 입력, 1M 토큰 컨텍스트 길이, 도구 연결(검색, 코드 실행 등), Thinking 모드 전환 가능 등의 기능을 지원
     * 비용 대비 성능 최적화(Pareto Frontier) 를 고려한 설계로, 대규모 트래픽 처리에 적합한 제품군 구성을 갖춤
     * Flash-Lite 및 Flash는 검색에도 커스터마이징되어 활용 중, 개발자는 Google AI Studio와 Vertex AI에서 프리뷰 혹은 정식 모델 사용 가능함

Flash-Lite의 특징

     * 가장 저렴하고 빠른 모델로, 입력 100만 토큰당 $0.10, 출력 100만 토큰당 $0.40의 요금으로 제공됨
     * 비용 대비 성능이 우수하여 번역, 분류 등 대량의 요청이 들어오는 작업에 특히 적합함
     * 이전 2.0 Flash-Lite보다 전반적으로 품질이 향상되었으며, 과학(GPQA) 기준으로 64.6% → 66.7%, 수학(AIME 2025) 에서는 49.8% → 63.1%로 향상됨
     * 코드 생성과 편집에서는 각각 34.3%, 27.1% 수준으로, 고성능 모델 대비 낮지만 비용 대비 효율적인 선택지임
     * 멀티모달 처리 성능은 72.9%로 유지되며, 이미지 이해는 51.3%에서 57.5%로 개선됨
     * 추론(Thinking) 모드를 활성화하면 전반적인 정확도가 상승하며, 예를 들어 HumanEval에서는 5.1% → 6.9%, SWE-bench multi-task에서는 42.6% → 44.9%로 증가함
     * 사실성(SimpleQA), 긴 문맥 이해(MRCR) 등에서도 Thinking 모드에서 성능이 눈에 띄게 향상되며, 특히 1M 토큰 기준 긴 문맥 정확도는 5.4%에서 16.8%로 3배 이상 향상됨
     * 다국어 능력(MMLU) 역시 높아져 Non-thinking에서는 81.1%, Thinking에서는 84.5%까지 도달함

     * Gemini 2.5 모델 패밀리에 대한 기술적 세부 내용은 Gemini technical report에서 확인 가능

        Hacker News 의견

     * 구글 포스팅에서는 언급이 없지만, Gemini 2.5 Flash 모델에 대한 가격 인상이 포함된 것 같음
       2.5 Flash Preview 기준 아카이브된 가격은 입력 텍스트/이미지/비디오 100만 토큰 당 $0.15, 오디오는 $1.00, 출력은 non-thinking $0.60, thinking $3.50 구조였음
       새 가격에서는 thinking과 non-thinking 구분이 없어짐
       입력 텍스트/이미지/비디오 100만 당 $0.30로 2배 인상, 오디오는 $1.00로 동일, 출력은 100만 당 $2.50로 이전 non-thinking보다 많이 비싸졌지만, thinking보다는 저렴해진 구조
       자세한 가격 내역은 여기에서 볼 수 있음
          + 블로그 포스트에 가격 변경에 대한 더 많은 정보가 올라와 있음
            자세한 참고 링크
          + AI 기술이 곧 너무 저렴해질 것이라는 말이 있었지만, 당장은 가격이 오르고 있는 상황에 대한 언급
          + 처음 Gemini가 출시될 때 가격이 지나치게 저렴해서 경쟁사 대비 너무 저렴하다는 생각을 했었고, 이제야 현실적인 가격을 반영하는 것으로 보인다는 의견
          + 아무렇지 않게 2배 인상된 가격
            Gemini 2.0 Flash는 $0.10/$0.40이었던 것을 생각하면 인상폭이 체감되는 부분
          + 예리하게 포착한 변화라는 의견
            이 가격 변화는 audio-to-audio 부문에서 GOAT(최고)가 될 수 있었던 Gemini에게 꽤 중요한 변화라고 생각함
     * 한때 Gemini Pro가 AI Studio에서 무료로 제공될 때 사람들이 많이 썼다고 생각함
       그 이후에는 성능이 오히려 안 좋아졌고, 이제는 중요한 작업에는 Claude로 돌아감
       Gemini는 쓸데없는 말을 많이 하는 친구 같은 느낌이 큼
       그래도 브레인스토밍에는 자주 사용하고, Gemini가 생성한 프롬프트를 다듬어 Claude에서 쓰는 식으로 씀
          + Aider leaderboard를 보면 내 경험과는 다르게 Gemini가 항상 우위에 있지는 않음
            나는 Aider API만 직접 써서 AI Studio 경험은 없음
            Claude는 프롬프트가 부실해도 성능이 괜찮음, 특히 방향성이 애매할 때 감각이 좋음
            내가 명확히 원하는 방향이 있는 경우엔 Gemini 2.5 Pro(Thinking 활성화)가 더 좋고, 코드가 안정적으로 실행됨
            o4-mini, o3에서는 좀 더 '스마트'하게 생각하는 느낌이 있지만 코드가 불안정함(Gemini가 더 안정적)
            복잡성이 커질수록 Claude는 더 약해지는 듯하고, 내 기준에서는 Gemini와 o3가 더 높은 평가
            o3-mini 출시 이후로 다시 Claude로 돌아갈 일은 없었음
          + 나도 비슷한 경험을 했음
            초기에는 복잡한 문제도 잘 푸는 것 같았지만, 단순한 작업은 조율이 어려움
            답변이 너무 장황해서 UX가 가장 중요한데 현재는 Claude Code의 UX를 선호
          + 나 역시 마찬가지인데, 간결하게 답변하도록 elaborate prompt로 프롬프트를 짜서 Gem을 만들었음에도 여전히 장황하고, 질문 범위를 불필요하게 확장하는 점이 불편함
          + 내부 정보는 없지만, 모델이 양자화(quantized)된 것 같은 느낌을 받음
            한 글자를 무한 반복하는 현상 등, 양자화된 모델에서만 보던 패턴이 관찰됨
          + 예전 프리뷰 버전으로 롤백했으면 함
            프리뷰 버전은 균형 잡혀 있고, 실제로 유용한 반박도 해줬는데, 정식 버전(GA)은 과하게 긍정적인 억양으로 변해버림
     * 난 Gemini에 매우 감명받아서 OpenAI 사용을 중단함
       가끔 OpenRouter로 세 모델 모두 테스트하지만 지금은 90% 이상 Gemini만 씀
       작년엔 90%가 ChatGPT였던 것과 비교하면 꽤 큰 변화
          + 구글에 비판적인 입장이지만, 이번엔 정말 모델들이 뛰어나다고 느낌
            특히 context window가 엄청나게 넓은 점이 매우 큼
          + 나도 마찬가지로 이번에는 Claude 구독을 해지했고, Gemini가 빠르게 따라잡고 있다고 생각
     * 이번 발표로 Flash Lite가 더이상 ""쓸모 없음""에서 ""쓰임새 있는 도구""로 격상이라고 생각
       Flash Lite는 싸고, 무엇보다 거의 항상 1초 이내(최저 200ms, 평균 400ms)에 응답하는 ‘빠름’이 강점
       우리 서비스 Brokk(brokk.ai)에서도 Quick Edits용으로 현재 Flash 2.0(Non-Lite) 사용 중이고, 이번에 2.5 Lite 도입을 검토 예정
       생각(Thinking)이 더딘 Flash 2.5보다 떨어지는 모델의 용도에는 궁금증이 있음
       빠른 응답이 중요한데 thinking 활성화시 속도가 느려져서 애매함
          + 내 기준에서는 충분히 빠르게 생각만 해준다면 thinking이 얼마나 많이 들어가든 상관없는 생각
     * 코딩 분야 이외에서 Gemini를 어떻게 쓰는지, 그리고 왜 선택했는지 궁금
       앱을 만들 때 GenAI 백엔드를 교체 가능하도록 설계하는지, 혹시 가격이나 신뢰성 때문에 여러 공급자를 로드밸런싱하는지, LLM도 만약 스팟마켓이 생긴다면 어떤 변화가 있을지 궁금
          + 내 경험상 Gemini 2.5 Pro는 번역, 요약(Canva 활용)처럼 비코딩 작업에서 두각을 나타냄
            문맥 창의 크기와 사용량 한도가 엄청나서 가능
            특히 리서치 보고서 생성에서 Gemini가 ChatGPT보다 뛰어나다고 생각
            구글이 검색 강자라서 그런지 보고서가 여러 출처에 기반하며 더 정확함
            글쓰기 스타일도 더 선호하고, Google Docs로 내보낼 수 있는 점도 편리함
            다만 UI가 경쟁사 대비 많이 부족하고, Custom instruction, Projects, Temporary Chat 같은 핵심 기능이 없거나 미흡한 점이 큰 단점
          + 수많은 NDA 문서를 한 번에 투입해도, 몇 초 만에 관련 내용만 뽑아주는 점이 유용함
            대용량 문맥 창과 정확히 필요한 정보를 뽑아내는 고성능 덕분에 이런 작업에 최적임
          + Gemini Flash 2.0은 극도로 저렴하며 엔터프라이즈급 워크로드에서 강력한 모델
            최첨단 지능은 아니지만, 저렴한 가격, 빠른 속도, 높은 구조화된 출력의 신뢰도로 개발할 때 매우 만족스러움
            2.5 Lite로 업그레이드 테스트 해볼 계획
          + 나는 lexikon.ai를 많이 사용하는데, 특히 이미지 대량 처리에서 Gemini를 많이 씀
            구글 비전 API 가격이 다른 대형 사업자(OpenAI, Anthropic)에 비해 훨씬 저렴해서 좋음
          + Gemini 2.5 Flash(Non-thinking 옵션)을 생각 파트너로 활용
            내 생각을 정리하거나, 내가 미처 생각하지 못한 인풋도 자동으로 제공
            자기 성찰에도 유용하게 사용하며, 내 생각이나 고민을 던지고 AI의 응답을 참고함
     * 현재 2.5-pro API 접근이 안 되는 사람들이 있는지 궁금
       ""projects/349775993245/locations/us-west4/publishers/google/models/gemini-2.5-pro을 찾을 수 없거나 접근 권한이 없다""는 에러 발생
       유효한 모델 버전을 사용하고 있는지 확인 안내 문구가 나옴
     * 대량 LLM 추론/데이터 처리 서비스를 운영하면서 다양한 오픈웨이트 모델의 비용 및 성능 프로파일링 작업을 많이 함
       LLM 가격 책정에서 여전히 이상한 점은 공급사가 토큰 소비량에 따라 선형적으로 과금하는데, 실제 시스템 비용은 시퀀스 길이가 증가할수록 제곱적으로 늘어남
       요즘 모델 아키텍처, 추론 알고리즘, 하드웨어가 대부분 비슷해졌으니, 공급사가 가격을 결정할 때는 고객 요청 패턴에 대한 과거 통계를 많이 참고하는 듯
       결국 실제 사용 패턴 데이터를 확보하면서 가격 인상이 나타나는 것 자체가 새롭지 않다고 생각
     * 2.0 Flash Lite 대비 2.5 Flash Lite의 오디오 처리 가격이 6.33배 인상됨
       2.5 Flash Lite 오디오 입력은 100만 토큰 당 $0.5, 2.0에서는 $0.075였음
       이렇게 급격하게 오디오 토큰 가격이 오른 이유가 궁금함
     * 입력:출력 토큰 비율을 3:1로 가정할 때, blended price가 이전 대비 3.24배 상승했고, 2.0 Flash 기준으로는 거의 5배 수준
       그래서 2.0 Flash가 여전히 많은 용도(특히 코딩 외 분야)에서 경쟁력이 있을 듯
       성능이 약간 낮더라도 여러 번 프롬프트를 나눠 쓰면 실질적 효과가 더 좋을 수도 있음
       이번 2.5 Flash가 압도적인 선택지가 될 줄 알았는데 아쉬움
       (관련 가격 자료는 https://ai.google.dev/gemini-api/docs/pricing"">여기 참고)
"
"https://news.hada.io/topic?id=21512","Fossify – 오픈소스, 광고 없는 앱 모음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Fossify – 오픈소스, 광고 없는 앱 모음

     * Fossify는 프라이버시 중심, 오픈소스, 그리고 광고 없는 모바일(안드로이드) 애플리케이션 모음
     * SimpleMobileTools의 서비스 중단 이후, 그 유산을 이어받기 위해 포킹되어 만들어진 프로젝트
     * 기술의 단순성과 사용자 프라이버시를 중요하게 생각하여, 누구나 신뢰할 수 있게 소스코드가 완전히 공개됨
     * 사용자는 Fossify의 앱을 통해 광고나 추적 없이 전화, 달력, 갤러리 등 필수 모바일 기능을 믿을 수 있는 환경에서 이용 가능함
     * 개발자는 GitHub 저장소를 통해 기능 개선이나 버그 수정 등에 자유롭게 기여 가능함
     * Fossify 커뮤니티에서 다양한 논의나 질문을 할 수 있으며, 별도의 이메일로 직접 문의도 가능함
     * 오픈소스 생태계 내에서 Fossify는 투명성, 개인정보 보호, 그리고 광고 없는 사용자 경험 면에서 타 오픈소스 앱들과 차별화된 강점을 가짐

   다른 건 몰라도 갤러리 앱은 Simple Gallery 시절부터 써왔는데 필요한 기능들이 깔끔하게 있습니다. AOSP나 픽셀의 기본 갤러리보다 훨씬 좋아요.
   예를 들면 .nomedia 파일을 두기 애매한 음악 앨범 폴더 같은 것들을 갤러리에선 숨겨버릴 수 있습니다

        Hacker News 의견

     * 다음 분들을 위해 몇 번의 클릭을 절약해주려는 마음에서 남기는 정보로, 이 앱들은 Simple Mobile Tools가 ZipoApps에 인수됐을 때 포크된 Android 앱 모음이 Fossify라는 이름으로 나온 것임
          + Simple Mobile Tools의 앱을 많이 사용했고, 인수 후에는 Fossify의 포크로 옮겼다는 이야기. ZipoApps 인수 이후 Simple Mobile Tools 쪽 Play Store를 다시 확인해보니, 완전히 엉망이 되어버린 상황. 이제는 대부분 이전보다 훨씬 많은 권한이 필요하고, 트래커와 광고, 사기성이 있는 주간 구독까지 붙어 있음. 그리고 서드파티 코드가 남아 있다면 GPL 위반 가능성까지 있음. GitHub는 업데이트가 멈췄지만 Play Store의 앱들은 계속 변하는 중. 소스코드를 다른 경로로 배포할 가능성도 매우 낮다고 생각함. Play Store 링크, GitHub 링크
          + Simple Mobile Gallery가 F-Droid에서 내려가버려서 업데이트를 못 받는 불편함 때문에 이쪽으로 갈아탐. 하지만 Fossify의 녹음앱이 마음에 들지 않아 아쉬움
     * Fossify 앱들을 실제로 쓰고 있는데 매우 만족스러운 경험. 다만 dialer를 쓸 때 한 가지 주의할 점이 있음. 독일의 112 같이 긴급번호로 전화하면 UI에선 아무 일도 안 일어난 것처럼 보이고 다이얼패드로 다시 돌아가지만, 결국엔 전화연결이 됨. 이는 긴급전화가 안드로이드 시스템 깊은 곳에서 처리되고, 다이얼러 UI나 통화기록에 나타나지 않는 구조 때문임
          + 자동차 사고 이후 911을 눌렀더니 폰이 다운되는 경험을 하면서 안드로이드 루팅이나 커스텀 롬에서 손을 떼게 됨. 전화가 꼭 필요할 땐, 뭔가 확실하게 동작하는 환경의 중요성 느낌
     * 예전 Simple Mobile Tools 앱을 썼는데, 매각된 후엔 이런 Fossify 앱들을 씀. 안드로이드에는 이런 심플하지만 훌륭한 앱이 다른 분야에도 많았으면 하는 바람. 대부분의 Play Store 앱들은 사실상 악성코드거나 최소한 스파이웨어에 가깝단 생각
          + 이제는 Play Store에서 광고, 인앱결제, 스파이웨어, 과도한 권한 요구 없는 정직한 앱을 찾는 게 거의 불가능한 수준. Google이 Play Store와의 단절을 점점 더 유혹하는 방향으로 끌고 가는 느낌이지만, 그래도 아직은 거기서 받아야 할 몇 가지가 남아 있음
     * Fossify 앱들은 정말 심플하고 직관적이며 군더더기, 광고, 쓸데없는 요소 없이 핵심 기능만 딱 제공. 이런 프로젝트에는 바로 ""Thank You"" 앱으로 후원을 하게 되는 경험
          + 나도 즉시 후원했지만, 쓸모없는 Thank You 앱이 꼭 설치되어야 하는 점은 별로 마음에 들지 않음
     * 스마트폰 제조사 기본 SMS 앱, 그리고 Google Messages에서 대출 광고와 도박 광고가 계속 떠서 Fossify Messages로 갈아탐. 이쪽에선 광고가 전혀 없어서 만족감이 큼
          + Google Messages에서 도대체 광고가 어디에? 한 번도 본 적 없어서 상상도 안 되는 상황
          + 이런 종류의 광고 메시지가 원래 통신사를 우회해서 오기 때문에 국가의 수신거부 목록에 등록해도 신고할 수단 자체가 없음. 앱을 바꾸지 않고도 실질적인 해결책이 있을 수 있는데 그게 RCS 비활성화임. 현실적인 커뮤니케이션엔 SMS만으로 충분하고, RCS의 이런 어이없는 문제 때문에 결국 자체적으로 입지를 줄이게 되는 상황이 흥미로움
     * 직접 설치한 /e/ OS나 Murena가 선탑재한 제품(특히 Fairphone 조합)이 위에서 언급된 문제들을 훨씬 줄여줌. /e/ OS는 AOSP 포크로, 쓸만한 런처, 거의 없는 불필요한 앱(날씨와 지도만 비활성화), 그리고 F-Droid에서 개인정보 존중하는 다양한 앱을 바로 쓸 수 있는 점이 장점. 트래커 차단, 개인정보 보호도 내장. Nextcloud와의 연동도 우수함. 대신 iMessage, RCS, 비주얼 음성메일, 공간 음향 같은 기능은 없음. 나에겐 이런 기능이 필요 없고, 내가 원하는 건 컨트롤 가능한 스마트폰임
          + /e/가 기본적인 용도에선 쓸 만하지만, 실제로 살펴보면 단점이 꽤 많음. Murena 팀 규모가 작아서 유지보수가 많이 더딘 느낌. 예시로, 기본 OS를 쓰지 않고 광고했던 화소에 맞게 카메라를 쓰려면 1년 넘게 온갖 우회 방법을 시도해야 했던 사례도 있음 Fairphone 포럼 예시
          + /e/OS는 업데이트가 항상 심각하게 늦는 편이고, 보안 업데이트 상황이 별로 좋지 않음 비교 링크. 그리고 일부 통신사에선 비주얼 음성메일이 정상적으로 동작함
     * Simple Mobile Tools가 ZipoApps에 인수되었을 때(2023년 말) 개발자 커뮤니티가 어떤 식으로 대응했는지, Fossify 포크가 어떻게 시작됐는지 관련 토론글 실제 토론 링크
     * FOSS 안드로이드 앱 리스트에 정작 실용적인 픽셀 스타일 런처가 없어서 아쉬움. 구형 기기를 단순 작업용으로 쓸 때, 가장 기본적인 기능(데스크탑 그리드, 앱 메뉴의 스크롤리스트, 위로 쓸어올려 진입 등)만 있는 가벼운 런처가 필요한데, 시중의 런처들은 지나치게 미니멀하거나(심지어 아이콘 표시도 X) 지나치게 커스터마이징에 치중해서 무거움
          + F-Droid에는 Fossify Launcher 베타가 있음
          + Lawnchair도 참고할 만함
     * Fossify Keyboard는 아직 스와이프 입력을 지원하지 않음 이슈 링크
     * 진짜 궁금해서 묻는데, 안드로이드 유저들은 OS 기본 카메라, 계산기, 캘린더, 전화, 파일관리자, 메시지에 광고가 뜨는지? 아니면 Fossify 앱이 기본 제공앱보다 프라이버시 기능이 더 추가되어 있는지?
          + 모든 Tier 1 Android 제조사의 기본 전화/메시지앱은 Google Phone과 Google Messages로 고정됨. 그리고 Google은 AOSP 다이얼러와 메시지앱 유지보수를 중단한 상태
          + 내 경우 LineageOS 사용 시 필요한 앱이 다 들어있지 않아서 Fossify, Simple Mobile Tools 같은 오픈소스 앱을 별도로 설치해서 사용. 사전 설치 기본 앱보다 더 많은 프라이버시 조절과 오픈소스 장점이 있음. 같은 이유로, KDE Connect 포크도 유지하고, VLC도 포크 유지.
          + Fossify는 오픈소스에, 구글로 연락처나 캘린더를 보내지 않으며 구글 락인 생태계와 광고가 없음. 플레이스토어에서 선택할 수 있는 구글 외 앱들은 구글 텔레메트리를 줄일 수 있지만, 보통 다른 방식으로 프라이버시 침해나 광고가 들어있음. Fossify는 이런 트레이드오프가 없음
          + EU 규제로 스톱워치/알람앱에서 프라이버시 정책 팝업이 뜬 걸 계기로 기본앱 사용을 그만둠. 기본앱에도 어김없이 스파이웨어 투입된 느낌
          + 광고가 없다고 해서 트래커까지 없는 건 아니니, 그런 부분도 유의해야 함
"
"https://news.hada.io/topic?id=21547","Midjourney, 비디오 모델 V1 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Midjourney, 비디오 모델 V1 공개

     * Midjourney의 지난 몇 년간 초점은 이미지 생성에 맞춰져 있었음
     * 하지만, 최종 목표는 실시간 오픈월드 시뮬레이션이 가능한 모델을 구축하는 것
     * 이를 위해선 이미지, 비디오, 3D, 실시간 모델 등 다양한 빌딩 블록이 필요함
     * 올해는 이 요소들을 개별적으로 개발, 공개한 뒤 단일 통합 시스템으로 결합할 예정임

Video Model V1 출시

     * Version 1 Video Model을 전체 커뮤니티에 공개
     * 이번 모델은 향후 기술 발전을 위한 중간 단계
     * 사용자에게 쉽고, 재미있고, 아름다우며, 저렴하게 영상을 생성할 수 있도록 설계함

Image-to-Video 워크플로우

     * 기존처럼 이미지를 생성한 후, Animate를 눌러 동영상으로 전환 가능함
     * 자동 애니메이션(automatic) 기능은 별도의 설정 없이 움직임을 만들어줌
     * 수동 애니메이션(manual) 버튼으로 사용자가 움직임 방향과 장면 전개를 직접 설정 가능함

High Motion과 Low Motion

     * Low motion: 카메라가 정지된 상태에서 피사체가 천천히 혹은 신중하게 움직이는 장면에 적합함
          + 가끔 실제로 움직임이 거의 없는 결과가 생성될 수 있음
     * High motion: 피사체와 카메라 모두 역동적으로 움직이는 장면에 적합함
          + 너무 많은 움직임으로 인한 오류가 발생할 수 있음

영상 연장(Extend) 및 외부 이미지 지원

     * 생성된 영상은 4초 단위로 최대 4회 연장 가능함
     * Midjourney 외부에서 업로드한 이미지도 애니메이션화할 수 있음
          + 이미지를 프롬프트 바에 드래그 후 'start frame'으로 지정, 움직임 프롬프트를 입력해 애니메이션 제작 가능함

가격 및 접근성

     * 영상 작업은 이미지 작업 대비 약 8배 요금이 부과됨
     * 한 작업당 5초 영상 4개가 생성됨
     * 1초 영상당 이미지 1장 작업 비용과 비슷하며, 기존 시장 대비 25배 저렴한 수준임
     * 현재는 웹에서만 이용 가능하며, Pro 이상 구독자에겐 비디오 relax 모드가 제공될 예정임

향후 계획

     * 추후 영상 모델 개발 경험을 이미지 모델 개선에 반영할 예정임
     * 기술 및 가격 정책은 추후 사용량 및 운영 상황에 따라 조정될 수 있음
"
"https://news.hada.io/topic?id=21548","TI, 600억 달러를 투자해 미국내에서 기본 반도체 생산 확대","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  TI, 600억 달러를 투자해 미국내에서 기본 반도체 생산 확대

     * Texas Instruments가 미국 내 기본 반도체 생산을 위해 600억 달러 이상의 대규모 투자를 발표함
     * TI는 애플, 포드, Medtronic, NVIDIA, SpaceX 등과 파트너십을 맺어 공급망 강화 및 혁신을 촉진
     * 이 투자는 아날로그 및 임베디드 프로세싱 칩 생산을 통해 스마트폰, 자동차, 의료기기, 위성 등 다양한 분야 지원을 목표로 함
     * 각 협력사는 TI의 미국 내 생산 역량 및 첨단 기술에 의존하며, 공급망 안정성과 품질 향상을 강조
     * TI와 업계 협력은 AI 인프라 고도화, 차세대 위성 인터넷, 혁신적 의료기술 등 미래 지향적 기술 발전에 중요한 역할을 담당하게 될 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 혁신의 다음 단계를 여는 대규모 투자 발표

     * Texas Instruments(TI) 는 현재 미국 최대의 기본 반도체 생산 기업임
     * TI는 스마트폰, 차량, 데이터센터, 위성 등 다양한 전자기기에 필수적인 아날로그 및 임베디드 프로세싱 칩을 제공함
     * 점점 증가하는 반도체 수요에 대응하고자, TI는 미국 내 제조 인프라를 대규모로 확장하는 계획을 발표
     * 이번 투자를 통해 고객사가 미래의 기술 혁신을 선도할 수 있도록 적극 지원함

주요 파트너와의 협업 개요

  애플과의 협력

     * Tim Cook(Apple CEO)은 “Texas Instruments의 미국산 칩이 Apple 제품에 핵심 역할을 하며, 양사는 함께 미래 첨단 제조업에 투자하고 혁신을 이끌 것임”이라고 밝힘
     * TI를 통한 애플 제품 혁신 및 가치 창출에 주목

  포드와의 협력

     * Ford는 자사의 자동차 전문성과 TI의 반도체 기술을 결합하여 강력한 국내 공급망 확보 및 모빌리티 미래 발전을 도모함
     * Jim Farley(Ford CEO)는 “미국에서 판매되는 Ford 차량의 80%가 미국 내에서 조립되며, TI와 같은 기술 선도기업과의 협력이 자랑스러움”이라고 강조함

  Medtronic과의 협력

     * Medtronic은 생명을 살리는 의료기기의 정밀성, 성능, 혁신이 반도체에 의존함을 강조
     * Geoff Martha(Medtronic CEO)는 “TI가 전 세계적 칩 부족 상황에서도 공급 연속성을 유지해주었으며, 혁신적 치료법 개발에 큰 힘이 됨”이라고 언급
     * Medtronic은 미국산 TI 반도체 덕분에 글로벌 환자 의료 향상에 기여하고 있음을 인정

  NVIDIA와의 협력

     * NVIDIA와 TI는 미국 내 AI 인프라 구축을 목표로 차세대 인공지능 아키텍처 개발에 협력함
     * Jensen Huang(NVIDIA CEO)은 “미국에서 더 많은 AI 인프라를 구축하는 것이 양사 공동 목표”라 밝히며, 첨단 AI 인프라용 신제품 공동 개발 의지를 표명함

  SpaceX와의 협력

     * SpaceX는 TI의 최신 300mm SiGe 기술을 활용해 Starlink 위성 인터넷 연결 고도화를 추진 중임
     * Gwynne Shotwell(SpaceX COO)은 “핵심 사명은 글로벌 연결성 혁신 및 디지털 격차 해소”라며, 미국 내 PCB/실리콘 패키징 등 대규모 투자를 통해 TI의 미국산 반도체가 제품 공급망 안전성과 성능, 신뢰성 강화에 핵심 역할을 한다고 말함
     * SpaceX는 Starlink 키트를 미국에서 대량 생산 중이며, TI와의 협력이 세계 고속 인터넷 수요 충족에 중요함을 강조

결론

     * TI의 600억 달러 이상 미국 투자 계획은 전자, 자동차, 의료, AI, 위성통신 등 다양한 최신 산업 발전에 핵심적 역할을 기대할 수 있음
     * 주요 파트너와의 협력을 통해 공급망 안정성, 기술 혁신, 글로벌 경쟁력 강화에 크게 기여함

        Hacker News 의견

     * Texas Instruments는 과거 미래를 담보로 금융화에 몰입한 회사 중 하나라는 지적과 함께, 현재 시가총액이 약 1700억 달러 수준이라 언급함
       이런 거대한 투자가 실제로 가능할지 의문이고, 단지 발표만 하고 실질적으로 진행하지 않는 홍보용 발표로 보인다는 의견
     * 이런 발표는 결국 실현되지 않을 것이며, 경영진이 이득을 얻기 위한 일종의 마케팅 사기라는 주장
       약속만 하고, 실제로는 아무것도 이행하지 않아도 사람들이 금방 잊어버리기 때문에 이익만 챙길 수 있다는 관점
     * 과거 많은 대기업들이 금융 부문에 치중하면서 몰락한 사례를 지적, GE가 금융 계열사로 변한 것과 유사함
     * Texas Instruments는 그룹 내에서 부채 부담이 가장 높은 편에 속하며, 130억 달러로 주주 자본의 75%에 해당하는 수치를 보유
     * 현 CEO가 이번 발표를 통해 정부(트럼프 당시)로부터 단기적으로 현금을 얻기 위한 것으로 보임
       트럼프 대통령 면전에서 입김을 넣을 수 있을 정도로 큰 회사나 영향력 있는 기업은 전부 이런 식의 발표를 하고, 행정부는 ‘가장 최근에 트럼프를 설득한 사람’의 영향력이 크다는 점을 지적
       과거 트럼프가 여러 범죄자들을 사면했던 사례처럼 뇌물이나 로비에 취약한 구조를 비판
     * “Foundational semiconductor(기초 반도체)”라는 용어가 실제 전자공학 업계에서는 거의 쓰이지 않는 정치적 용어로 보임
       보통 22nm 공정 이상에서 생산된 레거시, 혹은 오래된 기술의 칩을 의미
       미국 내에는 과연 22nm 이상급 팹 시설이 부족한 것인지, 아니면 활용이 안 되고 있는 것인지 의문
       관련 참고자료로 Chris Miller 증언문 링크 공유
          + FD-SOI 팹이 부족하다는 세부 설명
            FD-SOI는 소형 저전력 기기에서 많이 쓰이며, 서버 보안 칩 등에도 널리 쓰임
            현재 Lattice Semiconductor와 같은 기업이 이 공정을 프랑스(STMicro), 독일(GlobalFoundries), 한국(Samsung) 등에서만 공급받으며, 미국 내 도입 계획 없음
          + “Foundational semiconductor”란 결국 PA나 Fairchild가 했던 근본적인 반도체 설계를 의미
            수십 년간 새로운 진입자에게 기회가 없었던 현실을 암시
          + PR용 반도체, 즉 홍보 목적으로 등장하는 용어라는 냉소적 시선
          + 실질적으로 22, 55nm 같은 하이노드 공정 장비가 부족하다는 의견
            미국 내에서는 팹을 직접 소유한 일부 업체만 이런 공정을 보유하며, 팹리스 기업들은 해외에 의존
     * 이번 발표가 이전 행정부 시절 CHIPS Act 발표를 반복하는 것처럼 보인다는 시각
       자료실에 공개된 팩트시트 기준으로 510억 달러의 지원 규모 언급
       Texas Instruments 공식 발표자료 참고
     * Sherman, Texas에 이미 신규 팹을 건설 중이며, 팹의 높은 전력 수요 때문에 신재생에너지 연계 설계 및 부지 선정 작업을 경험
       산업용 대형 프로젝트가 발표될 때마다 최초로 전력 공급과 전력망에 대한 영향이 가장 큰 고민거리라는 입장
          + 대부분 이런 대형 프로젝트들은 사업 규모 덕분에 신재생에너지 투자가 경제적으로 효율적
            바이든 행정부 시절, 추가적인 신재생에너지 인센티브가 대거 도입되어 Texas 내 많은 기업들이 신재생에너지 투자 대열에 동참
            원래 텍사스에선 에너지 대기업 위주로 2000년대부터 시작된 흐름
            신재생 IP만 있어도 사우디 PIF나 Cheveron Ventures 같은 투자자로부터 투자를 받을 수 있는 시장
            실제로 HN 커뮤니티에 창업 아이디어가 많지만, 잘 실행되는 경우는 드물고, 그럼에도 훌륭한 데모데이가 많았던 경험
     * 이번 발표의 핵심 키워드는 “미국 정부와 손잡고 다음 미국 혁신의 물꼬를 트겠다”는 정부 지원에 대한 기대감
       실제로 실현될지 불투명하지만 정부 지원금 수령을 노린다는 분위기
          + Step 1: 미국 내 투자 발표
            Step 2: 대통령의 관심 얻기
            Step 3: 정부 지원금 수령
            Step 4: Foxconn처럼 자금만 받고 실질적 진척 없이 이익만 챙기는 행태
          + Foxconn 위스콘신 공장 사건과 유사한 미국 제조업 최대의 승리로 홍보하고 있지만, 실제로는 풍자적 의미
     * 반도체 산업 전체 규모에서 이번 투자는 한 회사 기준으로 연간 치고는 크지만, 업계 전체 기준으로는 그렇게 크지 않은 수준
       예를 들어, SMIC가 올해 75억 달러의 자본 지출을 계획 중이고, 이 회사는 중국 수십 개 반도체 업체 중 한 곳에 불과
       대부분 투자가 첨단공정에 집중되어 있지만, “foundational”이라고 불리는 구형 공정 제품(180nm, 6μm 등)도 반드시 필요
       전력관리, 정밀 측정, RF 프론트엔드 등 아날로그 분야는 디지털과 달리 공정 미세화가 어렵기 때문
       TI는 이 분야에서 중국 업체들보다 더 우수하고 저렴한 제품을 여전히 공급함
       다만, 실질적으로는 중국과의 경쟁 추격전이 될 수 있음
       SiGe 공정이 예외일 수 있는데, 이 공정은 비교적 구형 노드에서도 400~350GHz 대역까지 커버 가능
       Starlink 등 초고주파를 요구하는 군사용 기술 및 통신에도 전략적으로 중요한 역할
     * 실제로 투자 계획이 실행될지, 아니면 행정부의 환심을 사기 위한 발표인지 회의적
       600억 달러는 매우 큰 금액
          + TI는 점진적으로 확장해왔으며, 군수 분야를 포함해 TI 칩이 다양한 분야에 적용
          + 발표 내용이 신규 및 기존 확장 계획이 혼재되어 있다는 인상
          + 실제로는 이미 추진하던 공장 사업과 바이든 행정부가 시작한 계획을 그대로 이어간다는 ""정치적 광고""
            공장 지속 추진 자체가 난이도 높은 성과로 보지만, 이런 메시지를 내는 이유는 CHIPS Act 자금 지원 연속 확보 목적
            트럼프가 가끔 “바이든 법”이라는 이유로 CHIPS Act 예산을 취소하려는 움직임이 있지만, 정치적으로는 양당 모두 국내 첨단산업 투자에 호의적
            관련 Texas Tribune 기사 참고
          + 과거 이런 약속들이 종종 허상에 가깝거나, 소규모 부대시설 그치는 경우 많음
     * 아직 미국에서 생산된 다이는 대부분 해외에 패키징을 맡기는 상황
       언제쯤 미국 자체 패키징 역량이 생길지 궁금
          + CHIPS Act 덕에 텍사스 등에서 OSAT 및 패키징 인프라에 투자가 집중
            Samsung, Micron, OmSemi, TI 등이 적극 수혜
            Intel과 TSMC는 상호 견제하며 별도 행보
            나머지 투자는 SK, 인도 등에 분산
            TI는 인도 SCL Mohali 현대화 RFP에도 참여
            민관 전문가들은 실제로 바보가 아니라 현안을 잘 알고 대응
     * Sherman 부지에서 SiGe 공정 언급, 이는 RF 분야 노드로 22~100nm 이상에서 많이 차용
       RF용 공정은 미세 공정만이 좋은 것이 아니며, 성능 최적에 따라 다양한 노드 활용
     * 투자 계획은 언제든 바뀔 수 있으며, 누군가는 세제 혜택과 함께 “마라라고 초콜릿 케이크”까지 이미 보장받았다는 농담
"
"https://news.hada.io/topic?id=21507","Canine - Kubernetes를 Heroku처럼 쉽게 만드는 오픈소스 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Canine - Kubernetes를 Heroku처럼 쉽게 만드는 오픈소스 플랫폼

     * 1인 개발자도 쉽게 Kubernetes를 활용할 수 있도록 만든 오픈소스 플랫폼으로, 기존 Heroku와 비슷한 간편함 제공
          + Docker와 Docker Compose 환경에서 동작하며, 간단한 명령어로 설치 및 실행 가능
     * 기존 스타트업에서 겪은 예상치 못한 IT 비용 증가 문제를 해결하고자 개발됨
     * 복잡한 기능 대신 Ingress, Services, Deployments, Pods, CronJobs 등 핵심 요소만 노출하여 단순함을 추구함
     * Helm을 통해 거의 모든 오픈소스 앱을 배포할 수 있으며, YAML 설정을 내려받아 Canine에서 벗어나는 것도 가능
     * 계정, 배포, 대시보드 등 쿠버네티스에 기본 없는 기능도 보완해 소규모 팀이 쓰기 좋은 개발 플랫폼을 지향
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * Canine은 오픈소스 Kubernetes 배포 플랫폼으로, Heroku처럼 손쉽게 앱을 배포할 수 있도록 설계됨
     * 자체 Kubernetes 클러스터 위에서 동작하며, 필요시 YAML 설정을 내려받아 탈중앙화된 운영도 가능함
     * Ingress, Services, Deployments, Pods, CronJobs 등 핵심 리소스만 노출하여 단순하고 직관적인 사용성을 제공함

문제의식: 복잡한 구조와 폭증하는 비용

     * 기존 스타트업 운영 경험에서 IT 비용이 예상보다 훨씬 빠르게 증가했음
     * 비용은 주로 조직 복잡성과 연동 서비스 증가에 따라 늘어나며, 서버나 컴퓨팅 사용량과는 무관한 경우가 많았음
     * 다음과 같은 서드파티 툴이 누적되어 IT 비용을 급증시킴:
          + Looker, Redshift, Databricks, DBT Cloud, FiveTran 등 데이터 관련 도구
          + Datadog, New Relic, Sentry 등 모니터링 도구
          + Google Maps API, AWS 등 인프라 도구
     * 일부는 오픈소스로 대체 가능했지만, 모니터링·헬스체크·알림 등 운영 인프라 구성의 부담으로 채택을 꺼려했음

쿠버네티스의 가능성과 한계

     * Kubernetes는 단일 노드부터 수천 클러스터까지 확장 가능한 플랫폼이며, 거의 모든 클라우드에서 표준적으로 제공됨
     * 하지만 초보자와 소규모 팀에게는 복잡하고 쉽게 망가질 수 있는 시스템으로 인식됨
     * 특히 CoreDNS 삭제 같은 실수로 인해 전체 클러스터가 작동하지 않을 수 있음
     * 비용과 운영의 복잡성이 누적되면 기존의 추상화된 PaaS가 오히려 발목을 잡게 됨

Canine의 특징

     * Kubernetes의 최소 필요 기능만 노출하여 단순성과 통제 가능성 확보
     * 부족한 부분은 다음과 같은 기능으로 보완:
          + 계정 관리
          + 배포 관리
          + 단순한 one-off 스크립트 실행
          + 메트릭 대시보드
     * 직관적 배포 플랫폼을 통해 초보자도 쉽게 Kubernetes 환경에 애플리케이션 배포 가능함
     * Docker 및 Docker Compose만 준비되어 있으면, 명령어 한 번으로 설치 및 실행이 가능함
     * 쿠버네티스의 복잡한 설정이나 유지보수 부담을 줄여주는 UI 기반 관리 환경 제공함
     * Helm 통합을 통해 Sentry 같은 오픈소스 앱도 쉽게 배포 가능
          + Helm을 사용하면 대부분의 인기 오픈소스 앱을 단 몇 줄의 명령으로 배포할 수 있음
          + 지원되는 전체 앱 목록 제공

마이그레이션과 자유도

     * Canine은 기존 쿠버네티스에 얹어서 사용되므로 벗어나기도 쉬움
     * 모든 구성은 YAML로 내려받을 수 있으며, 추후 다른 플랫폼으로 이전도 수월함

중요성 및 차별점

     * 일반적인 Kubernetes 도구 대비 초보자 친화적 UI와 쉬운 설치 과정 제공함
     * Heroku와 유사한 사용 경험을 쿠버네티스 생태계에 도입하여, 진입 장벽을 크게 낮춤
     * 오픈소스 기반으로 유연한 확장성 및 커뮤니티 중심 개선이 가능함
     * 소규모 개발팀이나 스타트업에서도 손쉽게 쿠버네티스의 장점을 활용할 수 있는 점에서 큰 효과 기대됨
     * Apache 2.0 License로 자유로운 사용, 배포, 수정 가능

        Hacker News 의견

     * 저는 “Heroku와 비슷한 경험”을 내 서버에서 구현하려 늘 더 좋은 솔루션을 찾는 편이고, 그래서 정말 반가움 표시
       Canine의 Kubernetes 관련 문서가 매우 접근하기 쉽고, 지금까지 본 중 가장 친절한 수준 느낌
       문서를 보며 궁금증 생김: Canine을 Digital Ocean 같은 곳의 매니지드 K8s 환경에서도 쓸 수 있나 기대감 있었는데, 읽고 나니 직접 K8s 클러스터를 관리해야 한다는 인상임
       몇 가지 궁금점 제시
         1. Hetzner에서 “Cluster”를 만들 때, 실제 여러 대의 머신을 아우르는 진짜 K8s 클러스터인지, 단순히 한 머신의 가상 분할인지 궁금증
         2. 다른 서버에 설치 스크립트를 실행하면 클러스터에 조인돼서 pod를 분산 호스팅하는 진짜 분산 서버 구성이 되는지 궁금증
         3. 이미 존재하는 매니지드 K8s에 Canine을 붙여서 배포 가능한지 궁금증
          + 현재 Canine이 지원하는 구성은 두 가지임
              1. Hetzner VPS 한 대에서 운영
              2. 이미 구축된 Kubernetes 클러스터에서 사용
                 일반적으로 1번은 스테이징/개발 앱, 2번은 프로덕션 앱에 이용
                 2번의 경우 Digital Ocean 등에서 노드 수 관리해서, 쿠버네티스가 워크로드를 자동으로 재스케줄 및 오토스케일 기능 사용
                 질문의 핵심 같은데, Canine이 Hetzner 환경에서 직접 멀티노드 클러스터를 만든다는 기능은 아직 지원 안함
                 Hetzner의 terraform으로 K8s 클러스터 만드는 것도 있지만, Canine에선 아직 내장되어 있지 않음
                 UI 개선 등 하고 나서 관심은 있음
                 지금은 K3s 단일 VPS 설치 가이드로 도와주거나, 이미 클러스터가 준비된 경우 쓸 수 있다는 전제
                 참고 링크: terraform-hcloud-kube-hetzner
     * 이런 프로젝트가 꼭 필요하다고 생각하는 사람으로서 응원하는 심정
       오늘이라면 Canine이나 Dokploy를 고민할 것 같음 (Docker Swarm도 과소평가된 기술이라고 생각)
       피드백: “왜 Canine을 쓰지 말아야 하는가” 섹션이 솔직해서 신선할 줄 알았는데, 약간 비꼬는 뉘앙스가 들어 불편
       그냥 솔직하게 서버를 구매하고 관리해야 하고, 다운되면 책임져야 하고, 1인 개발 초기 제품이라는 현실적인 단점만 명확히 써줬으면 좋겠다는 의견
          + Docker Swarm은 지금 유지보수와 지원 현황이 어떤지 궁금증
            몇 년 전 현재 Docker 팀이 개발을 중단한 것 같아 추적을 멈췄던 경험 공유
          + 다른 랜딩 페이지들과 차별화하려다 그렇게 작성한 건데, 실 사용자 피드백 매우 감사하게 생각
            다시 시도해볼 의지 표명
     * 세상에 존재하는 여러 PaaS 플랫폼을 모아서 관리하는 리스트 공유
       awesome-paas
          + 이런 리스트에 프로젝트 제출하려 찾아보고 있었는데, 덕분에 PR 올릴 예정
          + dokku는 VPS에서 운영 가능한 미니멀 PaaS 플랫폼으로, dokku-scheduler-kubernetes도 존재
            dokku-scheduler-kubernetes
            단, Helm chart는 미지원
            클라우드 컴퓨팅 전반 구조, 다양한 비교 링크도 정리
            Cloud computing architecture
            Cloud-computing comparison
            Category:Cloud_platforms
            awesome-selfhosted에도 serverless/FaaS가 있는데 이는 awesome-sysadmin > PaaS로 연결됨
            awesome-selfhosted FaaS/Serverless
     * OP(작성자)에게 질문
       이런 프로젝트를 만들게 된 배경이 궁금증
       뭔가 복잡한 걸 처음부터 끝까지 만들어보고 싶은 욕심이 있는데, API와 React 통합만 다뤄봤음
       복잡한 아이디어를 현실적인 할 일로 쪼개서 오픈소스 프로젝트 “Heroku 대안”으로까지 성사시킨 과정이 궁금함
       난 Heroku 사용 경험이 거의 없어 “구현할 기능”을 파악할 때 가격 페이지 등을 참고할 것 같은데, 이 방식이 비효율적이진 않을지 우려감
     * Kubernetes 기반 Heroku 대안이라니 흥미롭다는 호기심
       하지만 K8s나 Helm chart 같은 걸 알아야 쓸 수 있다면, 내게는 사실상 Heroku 대안이 아닐 것
       물론 운영자 입장에 따라 echo hello 수준으로 단순할 수도 있다는 건 이해
       난 최대한 빨리 무언가를 올리고 싶을 때 “Kubernetes와 Helm chart”라는 단어조차 떠올리고 싶지 않다는 마음
          + 이게 바로 Canine의 목표였음
            사용자는 Kubernetes의 존재 자체도 몰라도 되고, 다만 그 성숙한 생태계를 누릴 수 있음
            언제든지 더 강력한 기능이 필요할 때는 Kubernetes API와 같은 전문 기능을 직접 열어서 쓰는 것도 가능
     * 작은 부분이지만 지적사항
       Kubernetes는 docker 컨테이너가 아니라 Open Container Initiative(OCI) 규격에 맞는 컨테이너를 돌림
       Docker는 상표명임
          + 또 다른 작은 지적:
            Canine Kubernetes Crash Course에서 “1만 대 서버 지원”이라 설명되어 있는데
            공식적으로는 Kubernetes가 최대 5천 노드까지 지원한다고 명시
            kubernetes 공식 문서 참고
            훨씬 더 큰 클러스터도 가능하지만, 그땐 대폭 커스텀 필요 (API registry 전체 교체 등)
            물론 워크로드도 영향
            Kubernetes는 out-of-the-box로 대형 클러스터 지원까지는 아직 멀었지만, 매 릴리즈마다 발전중임
          + docker 필수라고 하면 싫어함
            요즘은 docker 대신 podman이나 containerd 위주로 운영
     * 이 프로젝트 만드는 게 엄청 재밌었고, 내 인생에서 가장 즐거운 개발임
       모든 “기술 스택”을 한 손에 쥐는 성취감이 최고
       Rails app, Canine infra, Raspberry pi 서버, 내가 쓰는 ISP까지
       스스로 전부 관리하며 앱을 올려본 경험 공유
     * 웹사이트에 라이선스 표기가 2024년 MIT License로 되어 있고
       실제 github에는 apache license로 명시되어 있음
       이렇게 연도가 맞는지 아닌지보다, 라이선스 종류가 중요한 차이로 보임
       실제 적용되는 라이선스가 무엇인지 궁금증 표명
     * Self-hosting에서 docker와 K8s 사이 중간 단계를 원해, 비슷한 걸 직접 알아봤던 경험
       Nomad도 고려했지만 여전히 dead simple docker보다 살짝 복잡했고 에코시스템 미흡
       결국 홈서버는 docker로만 운영, 배포 중 다운타임은 감수
       프로덕션에선 Canine이 K8s의 어느 정도까지 추상화해 주는지 궁금함
       내부까지 들여다볼 일이 생길지, K8s 초보라 그 중간 지점이 정말 가능한 건지 의문
          + 나 역시 Docker와 Kubernetes 중간 위치의 툴을 개발중임
            uncloud
            목표는 Docker적인 CLI와 Docker Compose의 멀티머신/프로덕션 기능을 제공하되, 운영 제어 플레인 없이 단순명료함 유지
            개발중이지만 모든 레이어에서 일어나는 일을 쉽게 파악할 수 있어야 하며, 트러블슈팅도 쉬운 점이 목표
     * 컨셉이 정말 마음에 듦
       K8s는 대단한 기술이나, 그 복잡함이 진입장벽
       공개 자료를 보니 본질을 잘 이해한 것 같음
       특히 PVE, Microcloud, Cockpit 같은 솔루션이 인기 있는 셀프호스팅 영역에서 더욱 유용함 예상
       집에 놀고있는 N100 NUC가 있는데, Microcloud 포기하고 Canine 테스트해볼 의향 생김
          + helm이 때때로 번거로운 점 있음
            values.yaml 수정 후 적용되는 값과 처음부터 셋팅해야 하는 값이 헷갈림
            어떤 helm 설치 본은 서비스가 너무 많아, 무엇을 언제 재시작해도 되는지 헷갈림
            반면 core kubernetes는 stateless job에는 정말 쾌적한 사용감
          + 요즘 K8s의 “복잡도”라는 말이 어디서 나오는지 모르겠음
            예전엔 kubespray로 두 시간 세팅 이런 일도 있었는데
            요즘은 rke 같은 툴 쓰면 yaml 파일 하나랑 ssh 키 하나만으로도 클러스터 생성 가능
"
"https://news.hada.io/topic?id=21523","겉보기엔 비합리적으로 보이는 결정이 사실은 완벽하게 합리적인 이유 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               겉보기엔 비합리적으로 보이는 결정이 사실은 완벽하게 합리적인 이유 [번역글]

  겉보기엔 비합리적으로 보이는 결정이 사실은 완벽하게 합리적인 이유
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    1. Selectorate Theory(선택 집단 이론)란?

     * 정치학자 Bruce Bueno de Mesquita가 제시한 이론으로, 권력자가 왜 때로는 외부에서 비합리적으로 보이는 결정을 내리는지 설명하는 프레임워크.
     * 『독재자의 핸드북(The Dictator’s Handbook)』에서 대중적으로 소개됨.
     * 핵심 내용:
          + 리더(정치인, CEO, 창업자 등)는 자신의 권력을 유지하기 위해 ‘핵심 세력(Winning Coalition, Essentials)’의 이익을 최우선으로 고려함.
          + 이 핵심 세력은 더 큰 집단(Selectorate, Influentials)에서 선별됨.
          + 핵심 세력의 규모와 전체 선택 집단의 비율(W/S)이 의사결정의 방향을 좌우함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    2. W/S 비율에 따른 리더의 행동 패턴

     * W(핵심 세력): 리더가 권력을 유지하는 데 반드시 필요한 사람들.
     * S(선택 집단): 리더를 뽑거나 권력 유지에 영향을 미칠 수 있는 더 넓은 집단.
     * W/S 비율이 낮을 때(핵심 세력이 적을 때):
          + 소수의 충성만 확보하면 되므로, 이들에게 사적 이익(특별 보상, 특혜, 금전 등)을 집중 제공.
          + 공공의 이익보다는 ‘내 편 챙기기’가 합리적 전략.
     * W/S 비율이 높을 때(핵심 세력이 많을 때):
          + 많은 사람의 지지가 필요하므로, 모두에게 혜택이 돌아가는 공공재(성과, 복지, 시스템 등)를 제공하는 것이 유리.
          + ‘공공의 이익’을 우선시하는 결정이 많아짐.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    3. 실제 적용 사례

      3-1. 대기업/조직 내 정치

     * CEO가 소수 임원이나 특정 부서의 의견만 반영하는 경우, 외부에선 비합리적으로 보이지만 내부 권력구조상 합리적임.
     * 조직 내 ‘핵심 세력’의 이익을 챙기는 것이 자신의 리더십 유지에 더 효과적이기 때문.

      3-2. 엔터프라이즈 세일즈(기업 대상 영업)

     * 실제 구매 결정권자는 ‘조직 전체의 최적’이 아닌, 자신의 입지와 이해관계(상사, 동료, 평가 등)를 우선 고려.
     * 내부 정치적으로 유리한 제품/서비스가 선택되는 경우가 많음.

      3-3. VC/투자업계

     * GP(운용사)는 핵심 LP(대형 투자자)의 요구와 만족도를 최우선으로 반영.
     * 펀드 운용 전략이나 지표도 ‘핵심 세력’에 맞춰 설계됨.

      3-4. 스타트업/조직 성장 단계

     * 초기에는 공동창업자, 리드 엔지니어 등 소수 핵심 멤버의 이해관계가 최우선.
     * 조직이 커질수록 핵심 세력이 늘어나고, 공공재(복지, 제도, 투명성 등) 중심으로 의사결정이 이동.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    4. 왜 비합리적으로 보이는 결정이 실제로는 합리적인가?

     * 외부에선 ‘왜 저런 이상한 결정을 하지?’라고 생각할 수 있지만,
       리더 입장에선 자신의 권력 기반(핵심 세력)을 지키는 것이 최우선.
     * 이 이론을 이해하면,
          + 조직 내 정치,
          + 영업/투자 등에서
            ‘비합리적’으로 보이는 결정의 구조적 이유를 파악할 수 있음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    5. 인사이트 및 활용 포인트

     * 조직/정치/비즈니스에서 ‘이해관계자 맵’을 그릴 때 Selectorate Theory를 적용해보면,
       실제 의사결정의 흐름과 동기를 더 명확히 이해할 수 있음.
     * ‘내부 정치’가 중요한 상황에서는,
          + 공공의 이익보다 ‘핵심 세력’의 이익이 우선될 수 있음을 인지해야 함.
     * 조직 성장 단계에 따라 W/S 비율이 변화하며, 이에 따라 의사결정 방식도 달라짐.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    6. 참고자료

     * 『독재자의 핸드북(The Dictator’s Handbook)』
     * CGP Grey – The Rules for Rulers
     * 넷플릭스 ‘How to Become a Tyrant’ 시리즈
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   요약 포인트
     * Selectorate Theory는 조직/정치/비즈니스에서 ‘비합리적’ 결정의 구조적 이유를 설명해줌
     * 리더의 행동은 ‘핵심 세력’의 이익에 따라 결정됨
     * 다양한 실제 사례(대기업, 스타트업, VC 등)에 적용 가능
     * 조직 내 의사결정의 본질을 이해하는 데 유용

   개인의 입장에서는 합리적일지 몰라도, 대기업이 왜 망하는 테크를 타는지를 설명해주기도 하네요

   그런 관점에서 볼 수도 있겠군요
"
"https://news.hada.io/topic?id=21472","미국 팔란티어 감시에 언론인들이 미국 방문을 꺼리는 현상","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    미국 팔란티어 감시에 언론인들이 미국 방문을 꺼리는 현상

     * 한 언론인이 미국 입국 거부, 구금, 추방 경험 공유
     * 본인 보도 활동이 Columbia 대학 시위와 관련있음 강조
     * 미국 당국에서 전화기 압수 및 반환 조치 경험 언급
     * 국제 언론인 사이에서 미국 방문에 대한 우려 심화 중
     * 미국의 감시 및 출입국 통제 논란 존재
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 한 언론인(Alistair Kitchen, alistairkitchen.bsky.social)이 최근 미국 입국을 거부당하고 구금 및 추방을 당한 경험을 온라인에 공유함
     * 이 경험은 본인이 Columbia 대학 학생 시위를 취재한 것과 관련 있다고 설명함
     * 멜버른으로 돌아온 뒤에야 압수당한 휴대폰을 돌려받았음을 언급함

상세 내용

     * 지난 48시간 동안 미국 입국을 시도했으나, 입국이 거부되고 구금된 후 추방 절차를 거쳤음을 밝힘
     * 입국 거부 사유로 Columbia 대학 학생 시위에 대한 보도 활동이 지적됨
     * 미국 공항 당국이 입국자(언론인)의 전화기 압수 및 귀국 시 반환 조치 시행함
     * 이와 같은 사례는 국제 언론인들 사이에서 미국 방문에 대한 경계심과 우려를 증폭시키고 있음
     * 최근 보도 및 사회 운동 감시에 Palantir 등 기술 업체의 감시 시스템이 개입될 수 있다는 경계감 고조됨

        Hacker News 의견

     * Palantir가 실제로 역할을 했는지와 무관하게, 학생 시위에 대해 사람들이 글을 썼는지 여부까지 조사하는 현실 자체가 터무니없다고 생각함, ""반유대주의 퇴치""라는 명분이 매우 냉소적으로 느껴짐
          + 내 스팸 폴더까지 뒤져서 뭐가 나왔다고 문제 삼는 경험, 정말 정신 나간 현실 체감
          + 냉소적이라기보다는, 러시아와 미국 같은 권위주의 독재국가들이 수년간 해왔던 기본 행동 양식이라는 생각, 관련 사례로 러시아의 허위정보 조직적 활용 설명 위키백과 링크
          + 조금만 관찰해보면 실제 나치즘을 들먹이는 극우주의자들이 전부 트럼프 측에 몰려 있는 모습에서 진짜 현실 명확히 파악 가능
     * 기사 제목이 글의 본질을 제대로 전달하지 않는 느낌, 스노든 이후 이미 미국 정부가 수십 년 동안 인터넷 트래픽 전체를 대량으로 감시하고 있다는 사실이 널리 알려진 상태, 이제 달라진 점은 실제로 이 기술이 중국조차 잘 하지 않는 방식으로 공격적으로 활용되고 있는 점, 예를 들어 독일인이 예전에 페이스북에 홍콩 시위 지지 글을 썼다 해도 중국 입국 거부까지는 거의 안 가는데, 미국은 실제로 이런 정보로 입국 거부까지 시행 중
          + 미국 정부가 수십 년 간 인터넷 트래픽 전체를 대량 감시한 역사, 그 시작이 부시 행정부의 전 국민 감시 시스템 도입, 이런 대량 감시가 잠재적 피해를 야기하는 시스템의 핵심 요소였음, 이런 정책이 부시 지지자들과 사회 전반에서 별다른 저항 없이 받아들여진 점, 오바마 행정부는 이 감시 시스템을 크게 확장했고, 스노든 폭로 이전까지 거의 모두가 수용한 분위기, 대통령 책임을 묻지 않으면 결국 더 불합리한 권력으로 이어질 위험 언급
          + 스노든이 정보기관들이 불분명한 규칙 아래에서 도구를 사용할 수 있다는 사실을 보여줬다면, 이제는 러시아 정보기관 출신이 여러 기관을 조율하는 상황에, 훈련도 부족한 요원들이 Palantir 등 통해 접근 가능해진 현실 지적, IRS와 Social Security 데이터 접근은 중범죄로 간주되어 책임 있는 연방 요원들은 이미 해고 혹은 사임했다는 사실
     * Palantir, Valar Ventures, Mithril Capital, Lembas LLC 등, 피터 틸이 반지의 제왕의 팬이라는 점이 놀랍게 느껴짐, 반지의 제왕의 주제가 무한 권력의 타락과 선과 악의 대결인데, 정작 본인은 골룸이 된 듯한 모순 짚음
          + 피터 틸은 사실 사우론에 더 가까운 정체성이라는 우스갯소리
          + 이런 이야기들에서 ""악""이 충분히 오랜 기간 권력을 잡게 되는 현상이 결국 비슷한 시각의 사람들이 그런 위치에 매력을 느끼게 한다는 해석, 대부분의 악의 지도자는 본인이 특별하다고 착각하며, 자신의 시각에서 세상을 해석
          + 진짜 악인은 드물다는 주장, 매체에 등장하는 '악당'은 대중의 이해를 쉽게 하기 위한 캐릭터일 뿐, 현실에서 대부분의 악인은 세상을 개선하려는 의도를 가지고 있으며, 비순응적인 사람을 '무지'로 규정, 결국 악이라는 개념도 상대방의 시각에서 만들어지는 프레이밍이라는 관점, 순수 스토리북 스타일의 악한 지도자보다 대중을 설득해 나타나는 복잡함이 더 많다는 주장
          + 자신이 행하는 것은 선을 위한 어쩔 수 없는 조치라고 여기며 항상 자기 이야기에선 영웅이 된다는 시각, 진정한 선의만으로 유지하려는 세력은 결국 경쟁에서 도태된 역사가 있다는 점 강조, 히틀러도 평화·조화의 테마를 다룬 예술작품 팬이었다는 예시
          + 틸과 카프 모두 ""서구 문명 방어가 중요하며 이 힘을 적이 아닌 우리 손에 쥐는 게 낫다""는 말을 여러 곳에서 언급했고, 이에 동의하는 입장, 푸틴, 시진핑, 하메네이 등이 더 나은 선택지가 아님을 강조, 서구의 자기성찰은 긍정적이지만 세상엔 실질적이고 더 심각한 악이 존재한다는 사실을 잊지 말자는 주장, 이란에서는 학생이 시위하다 죽음까지 맞이하는데, 미국의 학생들은 억압을 경험하지 못하면서도 이란정권의 대리인 역할까지 하는 모순적인 시위 현상 언급
     * 오래전부터 Patriot Act 도입 이후 이런 감시와 데이터 조회가 쭉 존재해 왔음, 2010년에도 Proton에서 전 세계 IMEI/IMSI 조합을 검색해 연관기기 추적 가능했고, 2013년엔 Palantir에서 OSINT, LE 보고서 등 집계된 데이터까지 조회 가능했던 경험 공유, 무엇이 새로워졌는지 궁금, 단지 이 사실이 더 많은 사람에게 알려진 것뿐인지 질문, 이런 역량과 활용사례가 이미 공개되어 있었다는 맥락
          + 권위주의적 도구를 오래 전부터 가지고 있었으나, 과거엔 더 은밀하게 사용한 반면 지금의 정부는 기존 '관행적 자제력'을 무너뜨리며 노골적으로 사용 중이라는 주장
          + 2010년에 Proton에서 IMEI/IMSI 콤보 조회가 가능했다는 언급에, 혹시 PRISM을 의미한 건지 물어봄, Proton은 오히려 보안 지원의 진정성이 강한 이미지
          + 바뀐 점이 있다면 입국 거부 기준이 더 폭넓어졌다는 점, 원래도 정치적 견해로 입국이 거부되는 경우는 오래된 역사였고 미국뿐 아니라 유럽 등 여러 지역에서도 발견되는 사례라 설명, 그래서 단순히 현 정부만의 현상이 아니란 인식
          + 이런 일이 새롭든 아니든 본질적으로 나쁜 일임을 강조, 모두가 이 문제를 적극적으로 논의해야 한다는 입장
          + 새로워진 점은 아무 잘못 없는 사람도 정치적 이유로 이 시스템을 더 공격적으로 활용해 억압하는 사례가 늘어났다는 부분
     * 만약 모든 감시 데이터를 아예 공개적으로 열어서 시민이 활용하는 상황은 어떨지에 대한 생각 실험, 특히 번호판 인식 시스템(LPR)을 Palantir 사례와 연관 지어 생각하게 됨, LPR 데이터가 이미 현실인 상황에서 왜 소수의 사적 집단만 독점해야 하는지 의문, 모두가 사용할 수 있는 오픈소스 LPR 데이터베이스나 공공 접근 가능 시스템 아이디어 고민, 물론 사생활 침해나 스토킹 악용 우려도 동시에 공존, 민간 LPR의 사적 오용이나 남모르는 남용 문제도 고려 필요, 장단점 모두 생각하게 되는 흥미로운 논점
          + 미국 경찰 Floc 운영자와 대화에서, 30일 내 디지털 증거만 확보하며 그 이후에는 정부가 책임지지 않아도 된다는 점을 가치로 삼는다 들었음, 만약 정부가 데이터 소유할 경우 정보공개법(FOIA) 요청 대응이 필요해져 행정 부담 및 대중에 광범위 감시 실태가 드러나게 될 것이라 경찰은 우려, 민권보다는 유죄 판결만 강조하는 태도, Floc이 데이터를 외부에 판매하지 않는다는 주장도 신뢰하기 어려웠음
          + 정부나 경찰이 사적 집단인지 물음, ANPR 전체 차량 추적 시스템 영업자의 프리젠테이션에서 번호판이 안 보일 경우에도 차량의 위치, 속도, 차선, 연령·인구통계 등 다양한 요소를 추적하는 방식 시연을 목격, 사실상 이동 중인 모든 운전자 위치를 아는 수준, 사생활 옹호자로서 경찰이 데이터를 독점 관리하고 적합하다고 판단하면만 쓰인다는 안일한 입장에 의구심 느낌, 데이터 보호 역시 단지 체크박스 용도로만 언급되는 불편한 현실
          + 스토킹은 범죄이고 이런 행위 적발될 시 적극적인 단속 필요, 미국에서 여러 차례 교육받은 사생활, 언론 자유, 무죄 추정 원칙이 실제 현실에서 무너진다는 점 우려, 정권 비판 글에 대해 정보기관이 감시를 시작하면 표현의 자유 소멸, 출입국 시 휴대폰 데이터 전체 다운로드, 적법 절차 없는 강제 이송 등의 사례로 인해 기본권이 침해되는 현실, 이런 추세에 관령해서는 양비론적 시각보다 명확히 반대해야 함
          + ""공공장소"" 운전이라 사생활 보호 근거가 약하다는 논리에 궁금증, 도보 이동 중에도 누구에게나 신원을 밝혀야 하는 건 아니고 도시 곳곳 얼굴 인식 카메라로 실시간 신원 추적된다면 사생활 침해라는 생각
     * 기사 제목에 ""언론인들"" 복수형 표기와 실제로는 단 한 명의 사례만 나오는데, 해당 보도가 전체 사례처럼 오해될 위험, HN 가이드라인상 제목 편집 자제 필요
          + 등장 인물이 실제 기자가 아니라 블로거일 가능성 제기, 사실이라면 보도 신빙성에 영향
          + 제목 감시 혹은 제목 논쟁을 반복하는 글이 오히려 클릭베이트보다 더 피로감을 준다는 지적
     * Palantir 관련 회사에서 온 나름 괜찮은 리크루팅 제안을 거절, 자신이 원하는 커리어를 새로운 곳에서 찾을 수 있었고, 자기 자신을 떳떳하게 바라볼 수 있는 선택이었다는 소감
          + 반대로 내부 고발자나 스파이(몰)이 되어볼 수도 있지 않냐는 농담 섞인 제안
          + 선을 넘는 행동에 대한 도덕적 걱정 때문인지, 아니면 체면이나 이미지를 우려하는 것인지 질문
     * 곧 미국 내 평범한 직장인(화이트칼라)이 새로운 법을 대놓고 어기고 체포되어 정권의 강압적 정책에 대한 집단적 문제의식과 동기부여를 형성해야 할 시기가 올 수 있음, 시민권 운동 시절 집단소송 모델 연상, 또 경찰·군인·교도관·집행관 같은 직업의 도덕성 논란에 좌파가 자주 비판적이라는 점, 이런 구조가 결국 무비판적 권위주의, 부패, 물리력 우선의 현장 분위기를 만들게 한다고 생각, 실제 현장 요원 중 국가 기관의 법 준수 의무에 대해 고민하는 인원은 극소수
          + 좌파적 논의에서 군인, 경찰, 교도관 등 직업을 비도덕적이라 비난하고, 임금 인상 불필요하다고 주장하는 경향에 동의, ""경찰이 전부 MAGA(트럼프 지지 성향)""라는 불평이 늘고 있는데, 결국 본인 스스로 이 환경을 만들어 놓은 셈, 사실 이런 직업을 부정적으로만 인식하면 결과적으로 자신과 다른 생각을 가진 사람이 더 많이 이 역할을 채우게 되고, 오히려 이런 이미지를 좋아하는 성향의 사람도 몰릴 수 있음, 결국 원하는 결과(권위 비판적 집단화)는 얻었지만 예상 못 한 부작용도 발생, 만약 경찰에 문제가 있다고 생각한다면 오히려 직접 참여하여 변화를 만드는 게 더 나은 방법이라는 제안, 현대 소셜미디어 환경에서 전체 집단을 악마화만 하는 모습 자체가 문제라는 생각
          + 국가에 대항할 거라면 흔히 조직에 먼저 들어가려는 전략보다는, 그런 과정에서 오히려 더 국가에 취약해질 수 있다고 판단, 경찰이라는 직업 자체가 비도덕적이라는 입장, 40년대 유럽 수용소 경비가 되는 것과 유사하게 불가피성이나 위협에 의한 선택, 내부 전복 전략 등 특이한 사정이 없는 한 동의 어려움
     * 게시글 작성자가 Palantir나 빅테크의 소셜 미디어 OSINT 도구가 10년 넘게 이런 활동을 해온 사실을 알면서도, 자신의 게시글 삭제로 영향이 있을 거라 믿은 점이 오히려 억측이라는 지적, 관련 기술은 이미 꾸준히 사용 중이었다는 주장
"
"https://news.hada.io/topic?id=21510","혼다, 실험용 재사용 로켓 발사 및 착륙 성공","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       혼다, 실험용 재사용 로켓 발사 및 착륙 성공

     * 혼다가 실험용 재사용 로켓의 발사와 착륙 실험을 성공적으로 수행함
     * 이 프로젝트는 혼다의 핵심 기술을 활용하여 우주 개발을 통한 새로운 가치 창출을 목표로 함
     * 혼다는 재생 에너지 시스템과 로봇 기술, 재사용 로켓 등 다양한 우주 기술 연구를 진행 중임
     * 전 세계적으로 데이터 활용 확대와 위성 발사 수요 증가에 대처하기 위한 시도로, 지속 가능한 운송 실현에 기여 가능성 있음
     * 현재는 기초 연구 단계이며, 2029년까지 준궤도 발사 기술 확보를 목표로 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

혼다의 우주 기술 개발 목표

     * 2021년 공식 발표 이후, 혼다는 우주 기술을 미래 비전과 글로벌 잠재력을 실현하기 위한 도전의 장으로 인식함
     * 혼다의 주요 목표는 시간, 장소, 능력의 제약을 뛰어넘는 기술을 통해 일상을 더욱 즐겁게 만드는 것임
     * 혼다는 재생 가능한 에너지 시스템, 우주 환경에 적합한 로보틱스 기술, 그리고 재사용 로켓 개발에 집중하고 있음

젊은 엔지니어의 꿈과 핵심 기술

     * 젊은 혼다 엔지니어들이 자동차, 연소, 제어 등 혼다의 축적된 핵심 기술을 바탕으로 로켓 개발을 꿈꾼 것이 시작점임
     * 혼다는 자사 로켓으로 위성을 발사하여 다양한 서비스로 확장 가능성을 모색함
     * 이는 다른 사업분야와의 시너지 효과까지 내다보는 전략임

성장하는 위성 발사 시장과 데이터 활용

     * 전 세계 데이터 사용량 증가와 위성 기반 시스템 활용 증대로 앞으로 위성 발사 로켓의 수요가 더욱 커질 전망임
     * 혼다는 이런 시장 상황을 중장기적 기회로 인식함

재사용 로켓 개발의 지속 가능성

     * 혼다는 재사용 로켓 기술이 환경적으로, 경제적으로 지속 가능한 운송 실현에 기여할 수 있다고 판단함
     * 자동차, 자동 운전 시스템 등 혼다의 제품 개발 과정에서 쌓은 기술력 활용이 강점임

미래 방향성과 기술적 목표

     * 이번 로켓 실험은 기초 연구 단계임에도 중요한 진전으로 평가됨
     * 상업화는 아직 결정된 바 없으나, 혼다는 2029년까지 준궤도 발사가 가능한 기술력을 확보하는 것을 최종 목표로 계속 연구 개발을 진행 중임

        Hacker News 의견

     * 이 영상이 원래 게시물에 들어갔어야 한다고 생각함 영상 링크
          + 진짜 이야기가 전달되는 느낌이라서 특히 마음에 듬, Honda가 Blue Origin, SpaceX와 같이 완전한 “호프(hop)”를 성공시킨 점이 가장 인상적임 (물론 내 인생 최고의 영상은 SpaceX의 “ring of fire” 영상임)<br>Bay Area에서 자주 보던 현상이 여기서도 다시 보임. SpaceX가 해내니까, 이제 자본이 몰려 경쟁 시스템을 만듦<br>예전에 Google에서 일할 때 보니까, 공개된 검색 기술과 비공개 인프라 덕분에 경쟁업체들보다 앞서 있었음. 결국 시간이 흐르면서 모두 비슷한 수준까지 따라잡았고, 그 이후는 Google이 더 이상 빨리 나아가지 못했음<br>이 현상이 SpaceX에도 일어남. 다른 회사들도 재사용 부스터 기술을 익히면, SpaceX가 독점하던 시장도 균열이 생김. Google의 검색 시장 장악력, Sun의 서버 시장 점유와 마찬가지임. 어느 순간부터, 엘론이 만든 논란과 상관없이
            “비슷한” 수준의 서비스를 남한테서도 살 수 있다는 게 중요해짐<br>또한 SpaceX가 시장 내 1위를 지키기 위해 Starship이 얼마나 절실한지도 느껴짐<br>혹시 SpaceX를 비하하려는 의도로 받아들일 수 있지만, 절대 아님. SpaceX의 엔지니어링 성과는 정말 놀랍고 당연히 성공에 값함. 여러 기술 발전 곡선을 겪으며 드는 자연스러운 소감임<br>DEC가 Sun의 “장난감 컴퓨터”를 절대 넘지 못할 거라 생각했다가 실제로 시장 우위를 내준 모습, United Launch Alliance가 Falcon 9을 애써 무시하다가 추월당하는 모습 모두 봤기 때문에 SpaceX가 경쟁자에 직면하는 것도 거의 예언에 가까운 느낌임
          + 영상에 Civic과 잔디를 깎는 사람이 배경에 들어가야 더 좋았을 거라고 생각함
          + 영상을 볼 때 로켓이 1m인지 10m인지 구분이 안 될 정도로 크기 감이 없었음. 링크를 확인해보니 실제로는 6m 크기임
          + 영상 고마움, 하지만 처음엔 장난감 같다 싶다가 멀리서 보니 엄청 커 보이고, 착지할 땐 다시 잔디 스프링클러와 함께 작아보임<br>하지만 착륙이 정말 매끄럽게 잘 된 점은 대단하고, 작으면 오히려 더 어렵지 않을까 상상함
          + 로켓 발사하면 커다란 화염과 연기 기둥이 나오는 걸 기대하게 되는데, 이번 영상에서는 배기구에서 “깔끔한” 느낌임<br>이게 사용하는 연료 때문인지 궁금함
     * 이번에 처음 .honda라는 도메인을 봤음. 더 조사해보니 많은 회사들이 자기 이름으로 최상위 도메인을 가지고 있음. 왜 IANA/ICANN이 회사 이름으로 TLD를 허락하는지 궁금함<br>IANA 공식 TLD 목록
          + 주로 돈 때문임<br>새로운 gTLD 신청비가 $185,000라고 하고, 여러 회사가 한 도메인에 중복 신청했을 때는 경매까지 해서 ICANN이 거의 6천만 달러를 벌어들임<br>Google과 Amazon이 각각 101건, 76건 최대 신청자였음<br>관련 정보는 여기, 여기, 여기 참고
          + 일본어나 일본 문화는 잘 모르겠지만, Honda라는 단어가 브랜드를 넘어 더 깊은 의미가 있을 수도 있다고 생각함. (참고: venere 설명)<br>Honda(本田)는 ‘근본’(本)과 ‘논/밭’(田) 의미로, 가족의 뿌리 또는 농업사회와 연결된 전통성을 내포함
          + 예전에 Neustar에서 일했는데, 그때 .<brand> 도메인을 열심히 팔려고 했음. 최소한 한 고객은 확보한 듯함<br>Neustar 위키
          + 영상을 클릭해 재생을 기다리면서 주소창을 보던 중, 똑같이 그 점을 궁금해 했음
     * 질문: 요즘 재사용 로켓을 만들기 쉬워진 원인은? 온보드 CPU 성능으로 더 세밀하게 저지연 제어가 가능해져서 그런 건지 궁금함
          + 분야에서 실제로 일하는 입장으로 보면 여러 요인의 결과임<br>- 로켓 엔진 설계와 초정밀 스로틀 기술 발전<br>- 프로펄션 착륙 제어 알고리즘 성숙 (검색어: Lars Blackmore, GFOLD, Mars Landing)<br>- SpaceX가 과거 DC-X, Mars Landing 등에서 보여준 기술을 더 발전시킨 점<br>현대적인 시뮬레이션과 센서 기술도 도움은 됐지만 결정적이지는 않았음
          + 이제 Orbiter 같은 현실적인 시뮬레이터(무료 오픈소스 깃허브)가 나와 있어서, 소프트웨어 측면에서는 이미 해결책이 많음<br>예전에는 범용 시뮬레이터도 없고, 2 MHz CPU에 2KB 메모리처럼 너무 제한적이라 알고리즘 경로를 하드코딩하고 매우 단순한 것밖에 불가능했음
          + SpaceX가 가능성을 현실로 보여줬고, 일하기 힘든 회사 구조 때문에 경험자들이 더 나은 환경과 보수를 찾아 이직해서 전체 업계가 발전함<br>(참고로 실제 최초 시도는 Blue Origin/DC-X 쪽이 먼저 했다는 오타쿠식 지적도 덧붙임)
          + CPU 이외에도 가능한 이유들로는:<br>* 더 발전한 짐벌 모터<br>* 랜딩 단계에서 정밀 조절 가능한 엔진<br>* 과도한 무게 없이 플립 동작 등을 견디는 재료<br>* 정확한 포지션 센서<br>* 에어로다이내믹 시뮬레이션과 제어 알고리즘에 대한 이해 증가
          + ‘Proof of concept’가 핵심임. 이미 누군가 해냈다는 걸 알면 훨씬 쉽게 도전 가능함
     * 참고로 Rocket Lab의 Electron 로켓은 총 중량(wet mass) 13,000 kg임. 이번 Honda 로켓은 1,312 kg라서 훨씬 작음
          + 주요 로켓들의 전체 중량(kg)<br>Falcon 9: 433,000<br>Atlas V: 547,000<br>Starship: 1,200,000<br>Starship Booster: 3,600,000
     * “271.4m 고도” 관련해서 BPS.space의 솔리드 추진제 벡터링 실험도 비슷한 고도까지 도달한 적이 있었는지 궁금함 BPS.space Signal R2
          + N 등급 로켓 엔진 사용 High Steaks 로켓이 올해 8,500m까지 도달함. Joe는 추력 벡터 컨트롤보단 핀에 컨트롤 서피스를 넣는 방식으로 전환했던 듯함<br>YouTube 참고
     * 이번에 발사된 로켓은 300m 정도 올라가서 내려오는 아주 작은 로켓임. 성과는 축하하지만 실용성과는 아직 거리가 있고, 다른 회사보다 10년 이상 뒤쳐져 있는 상황임
          + SpaceX와 Blue Origin 외에 이 정도 기술 가진 회사가 더 있는지 궁금함. 경쟁은 언제나 환영임
          + 어쩌면 소형 로켓에서 성공하는 게 진짜 어려운 부분이고, 이후 스케일업이 더 쉬울 수도 있을 거라는 생각임
          + 소규모로 시작해 점점 규모를 키워가는 방식 자체가 좋은 콘셉트라 생각함
          + 이게 진정한 해커뉴스 스타일 코멘트라고 느껴짐
     * 예전에 아내의 Honda Civic(주행 거리 340,000마일, 여전히 운행 가능)을 팔았는데, 마치 달까지 갔다가 돌아오는 자동차라고 농담하곤 했음. 언젠가 Honda 하드웨어가 진짜로 달까지 무언가를 보낼 수 있다는 아이디어가 맘에 듦
          + 진짜 재미는 우주선 자체임. 예를 들면 “Honda Bucolic호는 지구-달을 수십 번 다녀와서 이제는 해왕성까지 다녀온 셈이라는 우스개” 스타일의 비유 가능성
          + Honda도 Musk처럼 오래된 Civic을 우주로 쏘아 올리면 좋을 듯함
          + 99년식 Civic을 몰다가 2008년에 주차장에서 후진하던 여성에게 들이받혀 폐차됐음. 차값보다 외형 수리비가 더 비쌌음. 그 일만 아니었으면 아직도 Civic을 타고 있었을 것임
          + Pontiac Moon 영화를 추천함(사실 별로 재미없으니 굳이 안 봐도 됨)<br>IMDB 참고
          + 비슷한 경험, 지금도 아내의 Honda Fit이 그런 장수차임. Tesla Model S와 Falcon 9, ’98 Honda Civic과 Honda 로켓의 비교 실험처럼, 이번 로켓의 실제 이름이 뭐였는지 궁금해짐
     * 처음에 “한 번에 성공하다니 인상적”이라 말하려다, 실제로 첫 번째 발사인지 확실치 않아짐. 혹시 공개되지 않은 실패가 있었는지 아는 사람 있는지 궁금함
     * 주제에서 좀 벗어날 수 있지만, 작은 로켓에는 왜 캐터펄트(투석기, 발사대 등) 같은 것을 사용하지 않을까 궁금함. 연료나 단 분리 등을 아낄 수 있을 것 같음
          + 간단히 말해, 캐터펄트로 쏜 뒤에도 복잡한 궤도진입용 차량이 필요하고, 캐터펄트에서 살아남을 만큼 튼튼하게 설계해야 한다는 추가 복잡성이 있음. 실제로 스핀런치 같은 시스템을 견딜 수 있는 건 “충격에 강한 식량이나 강철 덩어리” 같은 제한적인 짐뿐임
          + 실제로 그 분야에 도전하는 회사들도 있음 Spinlaunch
     * 일본에겐 작은 한 걸음이지만 우주 산업 경쟁에겐 큰 도약임
          + 진심으로 우주 산업 내 많은 경쟁이 필요함. 일본이 민간 우주 산업을 성장시켜 주길 바람
"
"https://news.hada.io/topic?id=21480","무엇이 강한 엔지니어를 강하게 만드는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         무엇이 강한 엔지니어를 강하게 만드는가

     * 강한 엔지니어는 약한 엔지니어가 아무리 시간과 노력을 들여도 할 수 없는 문제를 해결할 수 있는 사람임
     * 핵심 역량: 자기 신뢰, 실용주의, 속도, 기술적 역량

Self-belief: 자기 신뢰

     * 강한 엔지니어는 자신이 어떤 문제든 해결할 수 있다고 믿음
     * 소프트웨어 문제는 대부분 미지의 영역에서 시작하므로, 불확실한 환경에서 자신감을 유지할 수 있어야 함
     * 난제의 본질을 피하지 않고 즉시 핵심 문제부터 정면 돌파하는 경향이 있음
     * 자신감을 가진 엔지니어는 점점 더 어려운 문제에 도전하며, 성공 경험이 자기 신뢰를 더욱 강화함

Pragmatism: 실용주의

     * 강한 엔지니어는 실제로 동작하는 결과물을 내는 데 집중함
     * 이상적인 설계나 깨끗한 구조보다, 최소한의 타협과 실질적 효율성에 초점을 맞춤
     * 실용주의적 태도는 종종 ""출시 직전 리팩터링이 필요한가?"", ""복잡한 패턴 적용이 과한가?""와 같은 논쟁을 유발함
     * 실용주의적 주장이 실제로 더 많은 제품/서비스 출시 경험과 연결되는 경우가 많음

Speed: 속도

     * 강한 엔지니어는 항상 빠르게 일함
     * 빠른 실행은 반복 실험과 시도, 저확률-고보상 아이디어 실현, 다양한 업무의 가능성을 확대함
     * 장시간 일하는 것이 아니라, 짧고 강렬한 집중력을 바탕으로 생산성과 성과를 냄
     * 빠른 업무 경험이 쌓이면서 점점 더 강한 엔지니어로 성장하게 됨

Technical ability: 기술적 역량

     * 일정 수준의 기술적 능력은 필수지만, 업무와의 적합성과 실제 문제 해결에 특화된 능력이 더 중요
     * 천재적인 두뇌보다, 자신감과 실용주의적 판단력이 더 실질적인 효과를 냄
     * 회사의 기술 스택과 목표에 적합한 기술 역량이 강점으로 작용

Summary

     * 강한 엔지니어는 거의 모든 문제 해결에 대한 자기 신뢰를 갖고, 미지의 난제를 미루지 않고 즉시 해결
     * 실용주의적 실행으로 실제 결과를 만들어내며, 스마트하지만 약한 엔지니어와 갈등이 생기기도 함
     * 속도가 빠르며, 꼭 장시간 일하는 것은 아님
     * 기술적 능력은 필수지만, 특정 업무에의 적합성과 실전적 자신감, 실용주의가 훨씬 중요함

   가상의 이상적인 모델을 설정한 후 그의 특징을 추출하는 기법으로, 최근의 기가차드 밈도 이와 일맥상통한다.

   결과물을 내는 엔지니어가 강하다...

   살아남았다는건 강하다는것

   갱~~

   강한 개발자, 강한 남자
"
"https://news.hada.io/topic?id=21444","아무것도 없었을 때 저를 도와주신 분들께 어떻게 보답해야 할까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  아무것도 없었을 때 저를 도와주신 분들께 어떻게 보답해야 할까요?

     * 젊고 아무것도 없던 시절에 교수님과 낯선 분들께 놀라운 친절과 영감을 받았고, 어떻게 보답해야할지 고민이에요
     * 초기에 도움을 준 사람들에게 어떻게 의미 있게 보답하시나요? 어떤 형태의 감사가 가장 의미 있다고 생각하시나요?

주요 답변들 요약

  1. Pay It Forward – 받은 만큼 남에게 돌려주기

     * 직접적으로 ""갚아야 한다""는 부담을 내려놓고, 과거의 자신처럼 힘든 상황에 처한 다른 사람들을 도와주라는 조언이 압도적으로 많았음.
          + 그 도움의 '혈통'을 계속 잇는 게 가장 큰 보답이라는 의견.
          + “문을 열어준 만큼, 나도 그 문을 계속 열어주는 사람이 되자.”
          + 멘토링, 네트워킹, 작은 친절이 실제 인생을 바꾼다는 사례 다수.
          + 내 성공 자체가 과거 도움 준 이들에게도 기쁨임.

  2. 감사의 마음을 직접 전달하기

     * 감사의 메시지, 짧은 연락, 손편지, 전화 모두 효과적. 꼭 거창할 필요 없이, 예전에 받은 도움과 그 영향력을 솔직히 전하는 게 가장 의미 있음.
          + 시간 지나 다시 연락해 “그때 당신이 해줬던 X가 지금의 나를 만들었어요”라고 전하는 것만으로도 큰 기쁨을 준다는 실제 경험담.
          + 받은 사람은 대부분 “기억도 안 난다”, “그냥 내가 할 일을 한 것뿐”이라고 하지만, 그 소박한 연락이 오히려 더 큰 보람을 안겨줌.
          + 특히 은퇴한 멘토나 교수님께는 근황을 전하는 게 큰 힘이 된다는 조언도.

  3. 크게 갚으려는 부담, 오히려 상대를 어색하게 만들 수 있음

     * 큰 선물, 이벤트 등은 때로는 상대를 불편하게 할 수 있음. 짧고 진심 어린 연락, 그게 최선이라는 이야기.
     * ""빚""처럼 느끼기보다, 그 사람의 긍정적인 면을 내 삶에 녹여서 남들에게 나누는 게 진짜 보답이라는 시각.

  4. 진정한 선행은 대가를 바라지 않음

     * 도움을 준 사람들은 ""돌려받으려고"" 한 게 아니라 그 자체가 기쁨이기 때문에, 받은 사람이 더 성공하고, 또 누군가를 돕는 모습을 보는 것 자체가 가장 큰 선물.
     * 멘토 입장에서 “그냥 내 역할을 한 것, 특별한 보답은 필요 없다. 네가 후배를 도와주면 그걸로 충분하다”는 답변이 많음.

  5. 멘토링과 네트워킹

     * 가능하다면 나를 도와준 분께 “요즘 필요하신 게 있으세요?” “제가 도와드릴 게 있을까요?” 하고 물어보는 것도 좋은 방법.
     * 꾸준히 연락을 이어가고, 본인의 성장을 공유하며, 기회가 닿을 때 도움도 드리면 좋음.
     * 받은 가르침을 내 방식대로 후배, 동료, 이웃 등에게 계속 전파하는 것.

  6. 작은 실천과 문화 만들기

     * 도움을 받았을 때 그 감사를 '내가 받은 만큼 베풀겠다'는 태도로 자연스럽게 실천하는 것이 중요.
     * 그냥 “네가 잘돼서 기쁘다”는 말 한마디, 한 번의 연락, 커피 한잔이 충분할 때가 많음.
     * 인생은 ""선순환 구조""라는 공감: 내가 받은 사랑, 친절이 내 아이, 내 제자, 사회로 흘러간다는 것.

  7. 그 밖의 방법들

     * 시간과 마음이 허락된다면 손편지나, 책 선물, 작은 이벤트 등도 긍정적인 반응.
     * 상대가 필요로 하는 것이 있다면 작은 도움(직접적 도움, 네트워크 연결, 추천 등)도 의미 있음.
     * 내가 받은 선행이 사회적으로 퍼져나가는 **'파동(ripple effect)'**에 기여한다는 마음가짐도 중요함.

대표적 사례와 조언 (원문 느낌 살려 정리)

     * “Keep the gates open that were not gatekept for you.”
       — 내가 쉽게 들어올 수 있었던 문을 계속 열어두는 사람이 되라.
     * 감사의 편지, 이메일, 연락을 해라.
       — ""15년 전에 당신이 해준 일 덕분에 이런 결과가 있었어요""라는 메시지는 상대방의 한 주, 한 달을 빛나게 할 수 있다.
     * “Pay it forward.”
       — 직접적으로 갚는 것보다, 그 선행을 이어가는 것이 가장 큰 보답.
       — ""멘토들은 직접적인 보상보다, 네가 받은 걸 또 다른 이에게 나누는 것을 더 기뻐한다.""
     * 너무 큰 보상/보답에 집착하지 마라.
       — 도움을 받은 기억과 고마움을 진심으로 전달하는 것이 오히려 더 오래 남는다.
     * ""내가 받은 친절, 신뢰, 기회를 또 다른 이에게 전달하라.""
     * 성공한 근황, 잘 지내는 모습만 보여주는 것만으로도 충분히 고마움이 전해진다.
     * 손편지, 짧은 연락, 커피 한잔, '그때 해줬던 그 일'을 기억하며 감사의 뜻을 전하는 것만으로도 충분하다.

정리

     * 받은 만큼, 그 이상의 친절을 다른 사람에게 베푸는 것.
     * 감사의 말이나 작은 연락, 또는 나의 성장 소식 전하기.
     * 너무 큰 갚음을 하려고 부담 갖지 않아도 됨.
     * 이 선행이 사회적으로 퍼져 나가는 것을 즐기고, 내 삶에 자연스럽게 스며들게 하라.

     “도움 받은 것에 대한 가장 좋은 보답은, 받은 친절을 또 다른 이에게 자연스럽게 이어주는 것. 그리고 가끔은, 그때의 감사와 변화된 내 모습을 짧게라도 전해주는 것.”*

        Hacker News 의견

     * 누구에게 받았던 도움에 집중하지 말고, 이제는 다른 사람을 돕는 데 더 집중하는 것이 좋겠다는 생각임
       처음 Defcon에 갔을 때는 정말 외롭고 낯설었음. 그 해에는 전자 배지를 처음 선보였는데, 그 배지를 얻으려면 굉장히 한정적인 파티에 초대받아야 하는 상황이었음
       아는 사람도 거의 없었고, 한창 젊은 해커였던 나는 비비블렛틴 게시판 해커 그룹 멤버였음. 여름 내내 모은 돈을 비행기 표와 호텔비에 다 써버렸고, 라스베가스 리비에라 호텔에 묵었음
       회사 비용 지원을 받는 사람들은 좋은 식당에 갔고, 나는 Bally's 옆에 있는 다이브 바(허름한 술집)에 가자고 사람들 꼬셔도 자꾸 신참 취급에 웃기다고 놀림 받았음
       그때 Dan Kaminsky가 나타나서, 내 이름도 알고 있다고 말하며 내가 닌자 파티에 대해 물었다는 얘기를 들었대서 놀랐음. 그 파티는 못 들어가지만 풀장 바로 옆방에서 룸 파티가 있다며 직접 데리고 가줌. 욕조에다 케그(맥주통)까지 있는 파티였고, 거기서 다 같이 이야기하며 저렴한 식당, 무료 음료 얻는 법 등 팁도 공유 받았음
       그리고 그 이후 매년, 나 역시 Dan이 했던 것처럼 비용 아끼며 온 해커들과 어울려 핫도그 사주고, 다이브바 옆 상점에서 맥주와 아이스를 사는 법 등을 알려주는 ‘야간 해커의 도우미’ 역할을 해왔음
       진심어린 감사 인사보다 훨씬 중요한 건, 나를 위해 열려 있던 문을 그대로 다음 세대를 위해 열어주는 것임. 직장에서 이력서에 찍히지 않더라도, 다른 사람들을 연결하는 ‘연결자’ 역할이 가장 값진 일임
       Dan Kaminsky에게 존경과 감사를 전함
          + fairfax가 말한 것처럼, 다른 사람을 돕는 데 집중하라는 조언에 크게 공감함
            여기에 더하고 싶은 조언은, 당신에게 도움을 준 사람들에게 꼭 다시 연락해서 감사의 마음과 “무엇이 나에게 도움이 되었고, 그 결과 어떤 변화가 있었는지”를 전해야 한다는 점임
            이는 도움을 준 사람에게 감동을 줄 수 있고, 누군가를 계속 돕게 하는 동기가 될 수 있음
            나는 입양 가정에서 자라 힘든 어린 시절을 겪었는데, 어떤 가족들은 나에게 큰 영향력과 가치를 심어주었음
            10년 뒤 삶이 안정되고 나서 감사의 연락을 했고, 이 경험은 정말 값졌음
            기회가 된다면 꼭 해보길 권장함
          + 꼭 동의함. 해를 거듭하며 내게 결정적 도움을 줬던 분들에게 가끔 연락하는데, 정작 그들은 그 사건을 잘 기억하지 못함
            예전 인턴십에서 정규직 제안을 못 받았던 시절, 한 선배가 직접 개입해 나에게 기회를 줬고, 20년 뒤에 그때 덕분을 말했다
            그분은 ""정말? 기억 안 나지만, 도움이 됐다니 다행""이라고 했음
            결국 ‘paying it forward(선행을 다음 사람에게 이어주는 마음)’가 최고의 방법임
          + Dan Kaminsky를 Bay Area에서 몇 번 직접 만난 적 있었음
            그는 매우 친근하고, 자신의 시간을 아낌없이 썼던 인물임
            누군가의 호기심이 느껴지면 끝까지 진지하게 들어주었음
            돌아가시기 전 몇 주간도 밤늦게까지 트위터 DM으로 기술적인 깊은 대화를 나눴었음. 이런 대화는 다른 곳에서 쉽게 얻기 힘든 귀한 정보였음
            그 이후 그의 이야기를 나누다보니 많은 이들이 나처럼 그의 너그러움과 지지 덕을 봤다는 것을 알게 되었음
            Dan은 언제나 주변 사람을 챙기고 응원하는 특별한 사람이었음
          + 내 개인 원칙은, 기회가 있을 때마다 그 사람을 알든 말든 누구든 돕는 것임
            도움의 크기는 상관없고, 그 사람을 앞으로 다시 볼 수 있을지 없어도 상관없음
            그런 기회들이 생각보다 내 주변에 널려 있다는 것을 한 번 눈 뜨고 나면 계속 발견하게 됨
            남을 함부로 대하는 건 아무런 보상도 없고, 노력과 지능이 전혀 필요하지 않음
          + Dan이 한 컨퍼런스에서 우리 테이블로 달려와 SSH 트릭을 신나게 보여줬던 기억이 남음
            그의 호기심과 흥분은 정말 전염성이 있었음
            해마다 이벤트에서 만나면 항상 뭔가 새로운 것을 보여주곤 했음
            제트기 폭발 영상을 MRI 소프트웨어로 분석하기, 색약자를 위한 AR 앱, DNS 보안 등 다양한 아이디어를 실험했었음
            Dan은 해커 중의 해커였고, 항상 타인을 위로하고 선행을 실천하는 사람이었음
            우리도 인간미를 잃지 않고 그의 바톤을 이어받아야 한다고 생각함
     * 많은 사람들이 내게 ‘엄청난 친절과 영감을 준 이들’이 바라는 것은, 아마 내가 다른 이에게 같은 친절을 베풀어주는 모습일 것임
       다만 그들에게 “당신의 그 친절 덕분에 지금 내가 다른 사람에게 친절하길 노력하고 있다”고 솔직하게 말해주는 것도 그들의 하루를 더 행복하게 만들 수 있음
          + 정말 100% 공감함. 누가 나를 찾아와 “15년 전 당신이 X를 도와줘서 지금 이만큼 좋은 일이 생겼다”고 말해주면, 그 한 주가 진짜 행복해짐
          + 그리고 오랜 기간 연락을 이어가는 것도 중요하다고 생각함
            더 많은 친구가 필요한 사람들도 있고, 그런 우정이 그 어떤 선물보다 더 값질 수 있음
          + 나는 안타깝게도 내 인생에 큰 영향을 줬던 두 사람에게 이런 감사의 메시지를 전하는 과정에서 약간의 조증 에피소드 때문에 민망한 말실수도 함께 해버렸음
            너무 오래 지난 뒤여서 다시 사과도 못하고 그대로 남겨두고 있음. 상대방이 좋은 얘기만 기억해주길 바랄 뿐임
          + 감사 인사와 더불어 ‘내가 얻은 경험이나 실력으로 도울 만한 사람을 아느냐’고 묻는 것도 좋다고 생각함
            나를 도와준 그분들은 지금도 다른 누군가를 돕고 있을 가능성이 높음
          + 내 파트너도 인생에 영향을 준 선생님과 멘토들에게 감사 편지를 보냈고, 그 답장을 매우 긍정적으로 받았음
     * “Pay it forward(받은 만큼 돌려주기)”라는 말 못 들어봤는지?
       고마워하는 마음 자체는 올바른 일이지만, 대부분의 경우 그들은 어떤 보답을 바라지 않았음
       그들이 원하는 진짜 보상은 내가 받은 기회를 타인에게 이어주는 선순환이었음
       아니, 심지어 그 정도까지 안 해도, 사회에 긍정적인 작은 기여라도 한다면 족함
       이미 이런 글을 쓸 정도로 마음이 따뜻한 사람이라면, 자연스럽게 남을 돕고 있을 확률이 높고 억지로 노력하지 않아도 될 것임
     * 나도 꽤 힘든 상황에서 자라 17살에 혼자 시 청년주택에서 생활했었음
       고등학교 때 한 지역 사업가(이후 시장이 됨)가 토론부 활동을 응원하며 정장도 사주고, 다른 도움도 많이 줬음
       이후 대학 진학하고도 연락을 이어갔고, 그분이 돌아가시기 전 나에게 “성공하면 나도 같은 방식으로 남을 도우라”고 말씀해주셨음
     * 내 절친은 100일 동안 자신의 인생을 빛내준 이들에게 사랑의 손편지를 쓰기 프로젝트에 도전 중임
       손글씨가 줄 수 있는 인간미와 연결감은 그 어떤 매체와도 비교 불가임
       편지를 받은 분들은 기쁨의 눈물과 사랑으로 보답했음
       저자는 책 저자 등 본인의 멘토에도 편지를 썼고, 지금도 프로젝트의 절반을 진행 중임
       만약 도움이 필요하다면 언제든 연결을 도와주겠음
       한 가지 팁은, 손이 아프지 않게 펜과 손목 운동을 미리 준비하는 것이 중요함
          + 혹시 이 프로젝트가 끝나면 그 경험을 글로 남길 계획이 있는지 궁금함
            이런 경험담을 읽어보고 싶음
          + YouTube 영상 링크 공유함
     * 한 달에 한 번, 나에게 도움을 준 사람에게 감사 편지를 쓰는 것을 목표로 삼고 있음
       (참, 이번 달 것도 아직 안 썼음)
       받은 분들은 내가 이런 편지를 썼다는 사실과 내용에 모두 크게 놀라워했음
       그들에게는 그냥 작은 일이었지만 내게는 엄청난 사건이었다는 부분도 공감함
       직접 쓴 손편지로 감사를 표현하는 것이 가장 효과적인 방법임
          + 이전에 동료가 나에게 멘토링과 자신감을 줬다고 고마움을 표현한 적 있었음
            그 편지 한 통이 내 주간을 활짝 밝히는 햇살 같았고, 지금도 마음에 큰 위안이 됨
          + 가장 좋은 접근법임
            순간의 감사만이 아니라 나중에도 깊이 되새기는 감사하는 삶이 중요함
            또한 선행을 돌려주는 행위는 커다란 선순환을 만들 수 있음
          + 정말 멋진 일임. 마치 과거의 착한 행동에서 예상치 못한 ‘배당금’을 갑자기 받는 느낌임
            나도 곧 실천해 볼 것임
     * 누군가에게 받은 작은 일이 내 삶에는 큰 변화를 주었는데, 정작 그 사람은 기억하지 못하는 경우가 많음
       감사 인사를 전하는 것만으로도 큰 의미가 있음
       오랜만에 연락이 끊겼던 사람이라도 이제 사회적 위계가 없는 나이가 되었다면, 관계를 친구로 발전시킬 수도 있음
       이런 사례를 주변에서 종종 보았음
     * 받은 만큼 돌려주는 것, 아무 대가도 기대하지 않는 마음이 중요함
       내가 ‘x’를 받았다면 ‘10x’ 이상의 베풂으로 사회에 환원하는 것이 진정한 보상임
     * 받은 만큼 나눠주기(pay it forward) 실천이 중요함
     * 그들과 함께 시간을 보내는 것이 무엇보다 큰 보답임
       어떤 선물이나 물질적인 것보다, 다시 찾아가 즐거운 시간을 함께 보내며 “당신의 도움이 내 삶에 어떤 영향을 줬다”고 진심으로 고마움을 표현하는 것이 더 큰 감동임
"
"https://news.hada.io/topic?id=21479","기술 업계 일자리 붕괴","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              기술 업계 일자리 붕괴

     * 2023년 이후 미국 기술 업계에서 50만 명 이상의 해고 발생
     * 이 현상의 핵심 원인은 세법 개정에 따른 R&D 비용 상각 방식 변화
     * 해당 세법 개정은 기업의 현금 흐름을 악화시키고 단기적으로 대규모 해고를 유발함
     * 대형 기술 기업들은 연구 인력을 해외로 이전하면서 미국 내 고용이 줄어듦
     * 이러한 변화가 기술 업계뿐 아니라 미국 경제 전반에 심각한 영향을 미침
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 산업 환경과 해고의 배경

     * 2023년부터 미국 기술 업계에서 50만 명 이상이 해고됨
     * 해고의 원인을 단순히 AI 도입, 팬데믹 기간의 과잉 고용, H1B 비자 문제, 금리 인상 등으로만 설명하기에는 부족함
     * 근본적인 원인은 Zero Interest Rate Policy(제로 금리 정책)의 종료와 자본 조달 비용의 증가 및 세법 개정(IRS Section 174) 에 있음
     * 금리 인상으로 인해 벤처 투자 감소와 신생기업 성장 둔화가 발생했고, 그 여파는 대기업과 생태계 전반에 확산됨

IRS Section 174: 연구개발비 세법의 변화

     * 과거 미국 기업들은 연구개발(R&D) 비용을 발생 연도에 전액 비용 처리(손비 인정)함으로써 세금 부담을 크게 줄였음
     * 1954년부터 유지되던 이 세제 혜택 덕분에 Bell Labs, Microsoft, Apple, Google, Facebook 등 미국 IT 산업이 급성장함
     * R&D 비용의 손비 인정 범위도 넓어서, 급여·소프트웨어·외주비 등 다양한 항목이 포함됨

2022년부터 달라진 세법: 상각 의무화

     * Tax Cuts and Jobs Act(TCJA, 2017년 제정) 에 따른 Section 174의 개정이 2022년부터 시행됨
     * 이에 따라, R&D 비용은 즉시 손비처리 불가하고, 국내 연구는 5년, 해외 연구는 15년에 걸쳐 상각(비용 분산 반영)하게 됨
     * 상각 개시는 비용 발생 연도의 중간 시점부터 적용되고, 비용 분산으로 인해 단기간에 세금 부담이 증가하는 구조
     * 이로 인해 기업의 현금 흐름이 악화되고 단기적으로 추가 세금 부담이 발생함
     * Section 41 형태의 R&D 세액 공제는 여전히 있지만, 효용성이 제한적임

새로운 세법 적용의 예시 및 영향

     * 예를 들어, 2025년 미국 기업이 R&D 비용으로 100만 달러를 지출하면, 첫 해에는 10만 달러(1/10)만 손비로 인정 가능
     * 남은 90만 달러는 향후 4.5년에 걸쳐 매년 20만 달러씩 분할 손비
     * 이러한 변화로 인해 기업들은 유동성 압박과 세무 업무 증가 문제에 직면함
     * 단기적으로는 해고, 비용 절감, 부채 증가, 파산 위험 등이 가시화됨
     * 중소 IT/스타트업의 경우, 연구 인력 해고나 인건비 절감으로 즉각적인 대응 필요성 증가

해외 이전 및 미국 내 일자리 상실

     * 해외 R&D 상각 기간이 15년으로 대폭 늘어나면서 미국 내 개발자 고용 유지의 세금 메리트가 사라짐
     * 대기업들은 더 유리한 세제 환경의 해외 국가로 R&D 인력 이전(Google은 독일, Microsoft는 중국 등)
     * 그 결과, 미국 내 기술 직군 해고와 고용 감소가 가속화됨

정책의 의도와 실제 작동의 괴리

     * 2017년 TCJA는 법인세율 인하(35%→21%) 로 인한 세수 부족을 상각 제도 변경으로 보전하고자 함
     * 변화가 2022년부터 적용되도록 의도적으로 시행 시점을 지연, 즉각적인 여론 반발을 피하고 예산 점수를 맞추려 한 정치적 전략임
     * 기업들은 Section 174 개악이 실제 시행 이전에 의회가 이를 되돌릴 것이라 기대했으나, 법안 개정 없이 시행되고 급격한 세금 부담이 현실화됨
     * 그 결과, 2023년 중소 소프트웨어 기업과 대형 IT 기업 모두 대규모 해고, 급여 삭감, 고용 해외 이전 등의 조치로 대응함

미국 경제 및 스타트업의 사업 모델 변화

     * 미국 스타트업과 테크 기업들은 기존에 공격적 R&D 투자를 세금 손비로 처리해 번영적인 성장을 이어감
     * 현금흐름과 과세소득 간 괴리 덕분에 손실을 보면서도 IR에 거의 세금을 내지 않는 구조
     * Section 174 개정 이후, 동일 비용에 대한 상각 의무로 인해 세금 부담이 현실화되고, 재무제표상 이익 발생시 과세로 연결됨
     * 이에 따라, 기업들은 CapEx(설비투자) 축소, 인건비 절감(해고), R&D 해외 이전 등으로 재무 구조를 재편함

기술 산업만의 문제가 아닌 전 산업적 영향

     * 과거 미국 세법(1954~2022)은 거의 모든 산업에서 R&D 관련 비용을 손비 인정하여, 기술사뿐만 아니라 유통, 물류, 헬스케어, 미디어 등 다양한 산업에서 신속한 성장과 혁신을 촉진함
     * OECD 자료에서도 즉시 비용처리와 혁신의 상관관계가 높게 나타남
     * 2019년 기준, 미국 기업의 연간 R&D 비용 5,000억 달러 중 절반 이상이 비전통 산업
     * 이 변경으로, 전체 GDP의 10~20%에 영향을 미치는 디지털 경제와 그 2차 생태계까지 불확실성 확대

결론: 미국 경제의 성장 엔진 약화

     * 단기적 세수 증대를 노린 세법 개정이 미국 기업 성장 동력과 고용 유인을 약화시키고 있음
     * 기술 탄탄국 전략 및 미국 내 일자리 창출에 부정적 영향
     * 2000년 닷컴 버블 붕괴와 유사하게, 비효율적 정책 변화가 경제 체질을 약화시킬 위험
     * 근본적 R&D 세제 혜택 복구가 미국 경제 회복과 산업 성장에 필수임
     * 기술 업계뿐 아니라 미국 전 산업계와 서비스 생태계 전반에 걸친 파급효과 발생

        Hacker News 의견

     * 이 기사와 유사한 논의를 하는 일주일 전의 글 언급 필요성 제기, 관련 링크로 Hacker News 기존 글 추천
     * Section 174가 원래 영구적이었지만 2017년 세제 개혁으로 인해 2022년 만료 처리된 이후 시장 침체와 대규모 해고 발생 설명, 현재 의회에서 논의 중인 법안이 갱신을 포함하나 사법적 견제 장치까지 없애는 위험성이 있어 정부 권한 남용 가능성 우려 표명, 대통령에게 독재적 권한이 갈 수 있다는 걱정 전달, 174 복원 필요성과 민주주의 훼손 없는 방식 강조, 상원의원들에게 요청 시 이 점 고려 권고
          + 사법적 견제가 헌법상 보호되어야 할 만큼 중요한 기본권이라는 생각 드러냄
          + Section 174 전액 공제가 미국 중산층 확대의 핵심 역할을 했다는 점 강조, 이 조항의 소멸이 시장 악화와 해고를 낳았다는 주장, 이 제도는 미국 테크 산업에 엄청난 가치 제공과 실리콘밸리가 이번 선거에서 정치에 참여해야 했던 이유를 설명, 과거에는 소프트웨어 엔지니어링 인건비를 바로 비용처리하며 세액공제 혜택이 있었고 실패 시 인수합병돼도 남은 가치가 타인의 세액공제로 활용 가능했음 설명, 이런 구조가 VC 수익을 높여 스타트업 성장성과 미국 경제 전반의 긍정적 효과로 이어진다 언급, 대기업에겐 경쟁자가 될 스타트업 인재를 채용하여 장기적으로 직원(주식 베스팅)으로 묶어두고 시장 진입을 막는 전략 가능 언급, 고금리 상황에선 스타트업 경쟁력 약화 추가 설명, 174 만료 후 Meta 등 대기업이 더 이상 인건비를 세금에서 공제할 수 없어
            대규모 해고 단행 배경 분석
          + 2011년 국방 예산 시퀘스트레이션 사례와의 유사성 느끼는 의견
          + 더 많은 이들에게 관련 정보가 공유되었으면 하는 바람 발언
     * 기사 제목이 실제 내용과 달라 Click-bait 형태라고 생각, 1단락만 일자리 트렌드를 언급하고 본문은 Section 174 원상복구 옹호 논리 위주라 평가, 저자 의견에 어느 정도 동의하지만 기사 제목은 내용과 맞춰야 한다 주장
          + 필자 주장에 약간 동의하지만 내 실제 경험은 Section 174 영향만으로 설명되기 어렵다는 생각, 우리 회사는 이미 대부분의 시간을 R&D로 분류해 세금 효율화 및 업계 구조와 맞추는 전략 사용 중, 해고의 경우 대부분이 해외로 기능 이동 또는 인건비 절감 목적으로 이루어졌고, 독일 등 해외 지사 활용 및 인력 이전 사례 확인, ZIRP(제로금리 정책) 역할도 중요한데 크게 다루지 않아 아쉬움, 산업 내 전체적인 성장 정체와 리더십의 노동 규율 강화 요구 등 복합 요인 체감, 174도 영향을 미쳤으나 전체 중 비중이 작아 보이고 효과 미미할 것으로 예상, 그래도 폐지 찬성
          + 상원에서 현재 논의 중인 예산조정 법안이 Section 174를 폐지하는 점 기사에서 언급하지 않아 의아함 표명
          + 이번 글이 기존의 AI 탓·링크드인 식 고용 해고 기사보다 기업들이 갑자기 변한 원인에 대해 더 통찰력 있었음 소감
          + 제목에서 일자리 트렌드 관련 내용이 없다며 혼란 언급
     * 이슈 중 두 가지 불명확점 강조 : 1) 스타트업엔 큰 문제지만 FAANG 등 대기업엔 별 영향 없어 보이는데 왜 극단적인 해고를 하는지, 2) 최근까지 ZIRP 종료가 테크 고용 침체의 주 원인이라던 담론이 왜 174로 바뀌었는지 의문 제기, 2022년 이후 세법 변화라면 새로운 사실이 아님에도 내러티브 변화가 갑작스러움 지적
          + 174 관련 논의가 HN 포함해 그간 계속 있었으나, 'AI가 모든 소프트웨어 일자리를 빼앗고 있다' 류의 과장 이슈에 묻혀 언론화되지 않는 경향 지적, 스타트업에겐 큰 타격이며, 많은 스타트업들이 매출 없는 상태에서 세금 폭탄을 맞고 있다고 설명, FAANG는 글로벌 법인(특히 EU)으로 비용을 돌리는 등 영향 회피 가능, 또는 해고를 통해 세금 부담을 줄이면서 내부 프로젝트 실패 등 다른 이슈까지 덮으려는 전략 활용 예시(특히 VR 분야), 대기업의 해고엔 이중적 동기가 있음을 언급
          + FAANG도 스타트업 환경이 어려워져서 더 대담하게 해고 가능, 비AI 스타트업이 더 이상 위협이 되지 않으니 인재 확보 필요성 낮아졌고 사용자는 대안이 줄어들었다는 시각
          + 2022년 이후 투자자들이 FAANG 등 대기업들의 수익 및 주가를 면밀히 감시하기 시작, 세금 관련 회계 변화가 단기적 비용 절감 조치의 동인 제공 및 C-level의 주가 방어 필요성 강조, 이론적 분석이나 mag7(빅테크 Top7)의 실제 영향 수치 궁금증 공유
          + FAANG에도 실질적으로 큰 문제이며, 오히려 스타트업보다 고용 규모가 크기 때문에 영향이 더 크다는 생각, AI 때문이라는 건 실제 내부 사정과는 관련 적음, 해고는 다양한 이유의 결과였고 AI는 여러 이유 중 미미한 변수가 아니라 강조, 예산 및 세금 문제로 인해 사전부터 고용 관련 계획이 있었음을 주장, AI 이슈가 우연히 같은 시점에 불거진 것이 오히려 convenient였다는 실제 기업 예산 운영 시각 공유
     * 필자의 주장(세법 변화가 tech job meltdown의 원인)에 부분적 동의하지만, 그 근거가 부족하며 단순한 논평 수준이라고 비판, 복잡하고 요인 많은 경제 이슈를 단일 원인론으로 설명하려는 건 문제, 데이터·그래프·경영진 인용 등 구체적 증거 부재 지적
          + 기업 세금 담당자 입장에서 구 174 규정은 소프트웨어 엔지니어링 인건비를 즉시 공제, 실패할 땐 인수되어도 남은 가치가 타사로 이전되어 새로운 세제 혜택 발생 구조로, 이로써 VC 수익성 강화, 스타트업 지원, 미국 경제 전반에 긍정 효과 설명, 대기업은 경쟁될 만한 스타트업 엔지니어를 인수·채용(경쟁 억제), 장기간 꾸준히 주식 베스팅으로 스타트업 도전을 막음, 174 소멸 후 대기업의 인건비 공제 불가에 따른 대규모 해고로 시장 변화 분석, 고금리로 경쟁사 신생 자금조달 어려움까지 추가 설명
     * 감세 정책이 만료될 것처럼 포장하는 식으로 부채 효과를 희석, 의회에서 보통 재연장하며 주목도 낮음, 기존 인센티브 유지는 재정 적자 전망에 여전히 영향, 세금 감면과 적자 관계는 단순, 적자는 수입-지출, 감세가 자기 자신을 상쇄한다고 믿는 분위기 풍자
     * Section 174 영향이라기보다 업계 포화가 주 원인이라 주장, 모든 소프트웨어 분야가 상품화되어 직접 개발하는 것보다 사는 것이 저렴, 연봉은 높으나 ROI는 낮아 자체 개발 이유 약화, 대기업도 필요한 엔지니어는 여전히 많지만 구직자 쏠림을 흡수하기엔 부족, 현재 상황은 법조계 포화와 비슷한 신상태라는 시각
     * 'shibboleth' 대신 'tropes' 단어를 썼어야 한다는 생각, 그래도 흥미로운 상황 정리라고 평가, 하지만 실제로 거대 엑셀 표로 데이터와 메모를 검증해 본 사례가 있는지 궁금, 좋은 이론이나 실제 증거가 있으면 더 납득력 커진다고 의견, 만약 이 가설이 맞다면 일자리 이동이 있을 뿐 미국 외 다른 국가 경쟁력 강화 신호로 해석
     * 기사 필자가 실존 인물인지 의문, About 페이지가 장황하지만 구체 정보 부족 지적
          + 'shibboleth' 단어 거의 오용에 가까운데 동의하지 않는다는 의견
     * 더 넓은 시각에서 볼 때, 이제 테크 대기업이(특히 AI 외) 스타트업을 위협적인 존재로 보지 않는다 판단, 그 원인은 세법 변화, ZIRP 손실, AI, 해외 이전, 법·규제 장벽 등이 복합 영향, 결국 Facebook/Amazon/Google 등은 새로운 스타트업(비AI 계열)이 수십억 달러와 대규모 법무팀, 혹은 외국 정부의 지원 없이는 경쟁사가 못 된다는 사실을 알게 됨, 결정적으로 모호한 규제 하나로 경쟁사를 눌러 버릴 수 있고, 안 되면 행정부에서 제재까지 가능, AI는 규제가 확정되지 않아 구글 등이 아직 작은 경쟁사 위협을 느낄 수 있는 특수 사례 분석
"
"https://news.hada.io/topic?id=21490","GetHooky - 모든 스택을 지원하는 간단한 Git 훅 관리 툴","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 GetHooky - 모든 스택을 지원하는 간단한 Git 훅 관리 툴

     * ""실수로 깨진 코드, 테스트되지 않은 코드, 린트 안된 코드를 푸시하는 것""을 방지하는 크로스 플랫폼 Git 훅 관리자
     * JS용 husky에서 영감을 받아, Python, Rust, Go, Node 등 모든 언어/스택에 적용 가능
     * .hooky 폴더에 스크립트 파일을 저장하고, 명령 한 번으로 전체 훅을 설치/공유/관리할 수 있음
          + hooky install 명령어로 .hooky/* 파일을 .git/hooks/*에 심볼릭 링크로 설치
          + 각 훅은 실행 실패 시 커밋/푸시를 중단하며, 우회 방법(git commit --no-verify)도 안내
          + # hooky ya rookie 주석이 있는 파일만 hooky에서 관리, 커스텀 훅은 무시됨
     * 훅 파일의 버전 관리 및 팀 공유가 용이하고, GetHooky가 생성·관리하는 훅만 자동 설치·업데이트함
     * GetHooky로 설치된 훅은 실패 시 커밋·푸시를 자동으로 중단시키고, 우회 옵션도 안내
     * 윈도우를 제외한 대부분 플랫폼 지원
"
"https://news.hada.io/topic?id=21563","LLM을 MegaKernel로 컴파일하여 Low-Latency 추론 실현하기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               LLM을 MegaKernel로 컴파일하여 Low-Latency 추론 실현하기

     * LLM 추론을 단일 메가커널로 자동 변환하는 컴파일러를 개발했음
     * MegaKernel(Persistent 커널) 방식은 LLM 추론에서 계산과 통신을 완전히 하나의 GPU 커널에 통합하여 매우 낮은 레이턴시를 가능하게 함
     * 기존 ML 프레임워크나 커널 라이브러리의 분산 구조로 인해 전체 파이프라인의 단일 커널화가 매우 어렵다는 문제점 존재
     * Mirage Persistent Kernel(MPK)은 컴파일러와 런타임 시스템을 통해 자동으로 멀티-GPU LLM 추론을 고성능 megakernel로 변환함
     * MPK는 연산 그래프를 세분화된 태스크 그래프로 변환해, 소프트웨어 파이프라이닝과 연산-통신 겹침을 극대화함
     * MPK 적용 시 기존 시스템 대비 토큰 생성 지연이 줄고, GPU 수가 많아질수록 성능 향상폭이 더욱 커짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 MegaKernel 방식의 장점

     * 대규모 언어 모델(LLM) 추론에서 지연 시간을 줄이는 효과적인 방법 중 하나는 모든 계산과 통신 과정을 단일 megakernel(일관적 커널) 에 융합하는 방식임
     * 이 방식은 모델 전체의 레이어별 연산, GPU 간 통신까지 모든 처리를 하나의 GPU 커널이 끊김 없이 수행함
     * 주요 이점은 다음과 같음
          + 반복적인 커널 호출을 생략해 커널 런칭 오버헤드 제거
          + 레이어 전반에서 소프트웨어 파이프라이닝 실현 가능
          + 연산과 통신을 동시 진행하여 레이턴시 숨김

기존 한계와 MPK의 등장

     * 기존 PyTorch, Triton, TVM 같은 ML 프레임워크에서는 end-to-end megakernel 자동 생성 기능을 본질적으로 지원하지 않음
     * 실제 LLM 시스템은 NCCL/NVSHMEM(통신), FlashInfer/FlashAttention(어텐션), CUDA/Triton(커스텀 연산) 등 다양한 커널 라이브러리 조합으로 이루어져, 단일 커널로의 통합이 어려움
     * 이러한 배경에서, CMU, UW, Berkeley, NVIDIA, Tsinghua 연구진은 Mirage Persistent Kernel(MPK) 을 개발함
          + MPK는 컴파일러와 런타임을 결합하여 LLM 추론 전체 파이프라인을 자동으로 고성능 megakernel로 변환함

MPK의 핵심 가치

     * MPK는 커널 런칭 오버헤드를 완전히 제거하고, 계층 간 연산/데이터 로딩/통신 겹침을 최대화하여 극저지연 LLM 추론 환경 구현함
     * 실제 테스트(39-토큰 프롬프트, 512 토큰 생성, speculative decoding 미사용)에서,
          + NVIDIA A100 40GB GPU 단일 환경 기준 vLLM/SGLang 등 기존 최적화 시스템의 토큰당 디코딩 지연(14.5ms) 대비, MPK는 12.5ms까지 단축함
          + 이 수치는 이론적 하한선(10ms)에 근접함(1.6TB/s 메모리 대역폭, 16GB 가중치 로딩 기준)
     * 멀티 GPU 환경에서 연산과 통신을 완전히 통합함으로써, GPU 수가 늘어날수록 MPK의 성능 우위가 더욱 두드러짐

MPK 동작 구조 상세

  Part 1. 컴파일러 – LLM 연산 그래프 → 태스크 그래프 변환

     * 일반적으로 LLM 연산은 각 연산(예: 행렬 곱, 어텐션) 또는 통신 연산(예: all-reduce)이 노드이며, 데이터 의존성이 엣지인 컴퓨테이션 그래프로 표현됨
     * 기존 설계에서는 연산자당 별도 커널 실행 방식이 보편적이나, 이는 실제 의존 데이터 단위가 아니라 커널 단위의 의존성만 반영해, 파이프라이닝 기회가 제한적임
     * 예시: 행렬 곱 뒤에 allreduce가 있는 경우, 전체 행렬 곱이 끝나야만 allreduce 실행이 시작됨. 실제로는 데이터를 쪼개서 부분 실행/의존 관계를 활용하는 것이 가능함
     * MPK 컴파일러는 연산 그래프를 세분화하여, 실제 데이터 단위에 적합한 fine-grained task graph로 자동 변환함
          + 각 태스크(사각형)는 개별 GPU SM에 할당되는 연산/통신 단위임
          + 각 이벤트(원)는 태스크 간 동기화 지점임
          + 태스크 및 이벤트 간의 간선으로 효율적인 데이터/제어 의존성 표현함
     * 이 태스크 그래프 덕분에 MPK는 연산과 통신이 부분적으로 혹은 병렬적으로 더 중첩될 수 있음
     * Mirage kernel superoptimizer로 각 태스크에 맞는 고성능 CUDA 구현도 자동 생성함

  Part 2. 런타임 – 메가커널 내부에서 태스크 그래프 실행

     * MPK 런타임은 태스크 그래프를 GPU 하나의 커널(메가커널) 내부에서만 완전히 실행하는 방식임
     * GPU의 모든 SM(Streaming Multiprocessors)을 정적으로 워커와 스케줄러 역할로 분할함

    워커

     * 각 워커는 SM 단위로 동작하며, 전용 태스크 큐를 관리함
     * 루프 방식으로
         1. 다음 태스크를 큐에서 받아옴
         2. 수행(예: matmul, attention, 데이터 전송)
         3. 완료 시 이벤트에 알림
         4. 반복 처리
     * 이를 통해 각 워커의 자원 활용 최적화 및 비동기적 계층 연산 가능함

    스케줄러

     * 분산 스케줄러가 각 SM 내 단일 warp 단위로 동작, 최대 4개 스케줄러 동시 실행 가능
     * 각 스케줄러는 활성화된 이벤트 큐를 관리하고, 조건이 충족된 태스크들을 워커에 할당함
     * 이로써 중앙 집중형 동기화 오버헤드 없이 대규모 태스크 분산 처리 가능함

    이벤트 기반 실행 방식

     * 태스크가 완료되면 특정 이벤트 카운터를 증가시킴. 카운터가 임계치에 도달하면 이벤트 활성화, 스케줄러 큐에 삽입됨
     * 스케줄러는 해당 이벤트에 의존 관계가 있는 후속 태스크를 실행함
     * 덕분에 파인그레인드 소프트웨어 파이프라이닝과 계산-통신 중첩이 자연스럽게 이루어짐
          + 예: 한 레이어의 matmul과 다른 레이어의 attention이 동시 실행
          + 부분적으로 완료된 matmul 결과가 나오는 즉시 allreduce 통신 시작 가능
     * 모든 스케줄링·태스크 전환이 단일 커널 컨텍스트 내에서 벌어지므로, 태스크간 오버헤드는 1–2 마이크로초(μs) 수준으로 매우 낮음

미래 방향

     * MPK 목표: 개발자가 적은 파이썬 코드(수십 줄 내외)만 작성해도 손쉽게 LLM을 megakernel로 컴파일하고, 최대 성능을 발휘할 수 있도록 지원함
     * 주요 발전 방향
          + 최신 GPU 아키텍처 지원: 예를 들면 NVIDIA Blackwell 대상, warp 단위 특화 방식 등
          + 동적 workload 처리: mixture-of-experts(MoE) 등 동적 제어 흐름이 필요한 모델을 위한 컴파일 전략 연구
          + 고급 태스크 스케줄링: 우선순위 기반, 처리량 최적화 등 현대적 정책 연구 및 적용 가능성 추구
     * MPK는 GPU 기반 LLM 추론 작업의 컴파일·실행 방식에 근본적 전환점을 제시하며, 커뮤니티와의 협력 확대를 바람

추가 자료

     * MPK(Mirage Persistent Kernel) 코드 및 문서, 최신 연구 성과는 GitHub(https://github.com/mirage-project/mirage)에서 확인 가능

        Hacker News 의견

     * 저자님께, on-GPU 인터프리터 방식이 굉장히 유망한 미래 방향처럼 보이는 점이 흥미로움. 거의 동일한 접근을 보이는 다른 연구도 있으니 관련 글 참고 권장. CUDA의 근본적인 프로그래밍 모델(예: 커널 런치)이 미세한 작업 기반 병렬화를 위해 우회되고 있는데, 이 방식이 하드웨어의 활용도를 더 높이는 걸 직접 목격. CUDA가 우리를 여러 면에서 잡아두고 있던 건 아닌지 궁금증. 저자의 연구가 PyTorch의 실험적 백엔드로 들어올 가능성에 대한 기대감. 그리고, 첫 번째 부분의 두 문단이 거의 동일하니 사소한 오타 지적.
          + 피드백 감사 인사 및 Stanford의 MegaKernel 프로젝트도 유사 도전을 하고 있다는 언급. 하지만 MPK는 사용자들이 PyTorch 수준에서 LLM을 표현하면 컴파일러가 자동으로 최적화된 megakernel로 변환해주는 방식 추구. 목표는 megakernel 프로그래밍을 누구나 쉽게 만들 수 있게 하는 것. CUDA가 특히 레이턴시에 민감한 워크로드에 한계로 작용하는 것에 전적으로 동의. GPU가 커지고 빨라지면서 작은 배치에서도 하드웨어 자원을 충분히 활용하는 독립 커널 작성이 점점 더 어려워지는 현실. PyTorch와 협업 하에 MPK가 megakernel 생성 지원에 도움 줄 수 있다는 방향으로 적극 탐구 중. 중복 문단도 지적해줘서 감사.
     * vLLM과 SGLang에서 일정 기간 밀접하게 작업해왔는데, 이 프로젝트가 바로 후속 프로젝트의 이상적인 모습이라고 확신. 연산 의존성 그래프를 분석하고, 연산을 퓨징하거나 더 똑똑하게 태스크를 스케줄링하는 내용이 인상적. 팀에 축하 인사.
          + 긍정적인 피드백에 대한 감사 인사. MPK가 특히 저지연 LLM 서빙 분야에서 기존 LLM 시스템을 확장하는데 기여 가능성에 대해 큰 기대감. 앞으로 다양한 협업과 방향성 탐구에 의욕.
     * 글과 github README 훑어보고 정말 멋진 프로젝트라고 생각. 이런 최적화 방안이 추론뿐 아니라 학습 단계에까지 적용될 수 있는지 궁금증. 특히 backward 연산과 gradient 통신의 퓨징이 도전 과제임을 인지. 현재는 dynamic workload(예: MoE)를 지원하지 않는 것으로 알고 있는데, 최근에 MoE를 한 커널에서 처리하는 논문 FlashDMoE: Fast Distributed MoE in a Single Kernel 언급.
          + 글, README까지 읽어줘서 감사 인사. 학습단계 지원도 가능하지만, 대체로 학습 커널이 더 커서 커널 런치 오버헤드가 크게 문제되지 않기 때문에 추론(특히, 저지연)이 더 큰 수혜 대상. 공유해 준 FlashDMoE 논문도 흥미깊게 봤고, MoE 모델 지원도 다음 목표로 삼고 있음을 강조.
          + 개인적으로 gradient 기반 학습 최적화에 시간 투자하는 것에 다소 회의적 시각. 실제 많은 학습 태스크들은 이산적 값의 특성을 가져서, gradient 기반 학습으론 잘 다룰 수 없다고 생각.
     * 다음 단계로는 바로 Verilog로 컴파일해서 aliexpress에서 LLM 하드웨어 직접 구입하는 게 꿈.
          + Chisel 등 하드웨어 기술을 소개하는 글 공유. AI, GPU 등장이전에는 이러한 소프트웨어에서 하드웨어 직접 전환 아이디어가 유망한 접근이었음. CPU 발전이 정체 상태이고, 소프트웨어와 하드웨어 중간 계층을 더 최적화하려는 열망은 꾸준하지만, GPU 스타일의 병렬 컴퓨팅이 주류 가속 방식을 이어갈 가능성이 높음. 일반용 CPU는 결국 GPU를 관리하는 작은 뇌 역할로 남아 있을 전망. 다만 소프트웨어에서 바로 하드웨어로 전환하는 방식이 주류로 가기는 어렵다는 예상.
          + 5~10년 뒤 LLM의 구조가 안정화되면, 하드웨어에 바로 매핑하는 게 실질적이 될 수도 있다는 예상. 현재 기술로 수백억 파라미터도 1.5비트 근방 초저정밀 논리 게이트만 사용하여 단일 웨이퍼에 들어갈 가능성 언급. 정밀도가 더 높아질수록 게이트 수가 기하급수적으로 증가하니, 현재로썬 가중치 메모리 유지 및 계산 유닛 공유 방식이 효율적. 미래엔 초저정밀 LLM 개발이 필수 과제.
          + 학습 비용이 이미 높은데 마스크 비용도 추가하면 상황이 더 어려워짐에 대한 유머와, 사실상 AI 하드웨어 스타트업들이 이런 방향의 시도는 오래 해오고 있다는 냉철한 평가.
          + LLM-in-a-box 방식이 실제로 존재한다면 상당히 매력적이라는 평. 조만간 오프라인(air-gap) 환경에서 일할 기회가 있는데 저런 솔루션이 굉장히 유용할 것 같다는 기대.
     * 직접 Modal GPU 환경에서 코드를 돌려봤는데, 연구에서 주장하는 성능 향상 수치가 실제로 재현됨. mirage 프로젝트 결과 코드 공유. Triton + FlashInfer 조합에서 1토큰당 레이턴시 19.2ms 수준, MPK에서는 동일 조건에서 7.7ms로 대폭 개선 경험.
          + 직접 결과 재현해준 데에 감사 표시.
     * 예전에 작은 CUDA 대회를 한 경험. 이미지나 비전 분야의 병렬 알고리즘이었는데, 여기서 똑똑해 보이려고 중간 결과를 메모리로 캐싱했음. 공모전 결과 확인 후, 남들은 나보다 훨씬 빠른 코드 제출함에 놀람. 이유를 보니, 중간 결과 따위 캐싱하지 않고 계속 다시 계산하는 방식이었음. 메모리 왕복보다 연산 비용이 훨씬 작았던 것. 이 프로젝트도 아마 비슷할 거라 추정. megakernel로 컴파일하면서 layer 경계가 사라져서 중간 결과 공유는 줄고 연산량은 늘어나는데, 전체적으로 보면 메모리 왕복이 줄어들어 큰 이득. 특히 컨볼루션 네트워크에서는 sweet spot이 있을 텐데, megakernel에서 이 부분을 어떻게 처리하는지는 모르겠음.
     * 지금도 LLM에 대한 새로운 비유가 계속 등장. 혹시 LLM을 트랜지스터처럼 여길 수도 있지 않을까 생각. 지금은 마치 천공카드로 곱셈만 하던 방 크기의 컴퓨터 단계와 비슷하다고 상상. 1백만 개의 o3-pro 쿼리를 동시에 돌릴 수 있다면 어떤 일이 펼쳐질지 상상하는 재미.
     * 이 프로젝트는 CMU(카네기멜론) 출신. Stanford의 Hazy Research에서도 megakernel을 다룬 블로그 No Bubbles 언급. 이 분야에서 경쟁이 활발히 벌어지는 모습을 보는 것이 인상적. (추가) ""mirage"" 프로젝트의 더 큰 그림을 다룬 논문도 있으나 megakernel 접근법은 다루지 않음 논문 링크
          + 게시글 작성자 본인이 직접 답변. Stanford와의 연구가 병렬적으로 이뤄지고 있음에 동의. 주요 차이점은 자동화된 megakernel 생성 컴파일러에 주력한다는 점.
          + Hazy Research의 ThunderKittens도 굉장히 쿨한 라이브러리임을 언급. 최근 NVIDIA GPU 모델을 최대한 이용하기 위해 포멀화, 파이프라이닝, 분할정복, 효율 극대화, 그리고 전용 컴파일러/DSL 개발에 큰 노력이 집중되고 있다는 평가.
     * Qwen 8B 성능 수치는 검증된다면 상당히 인상적. 이전 megakernel 방식보다 더 실용적으로 느껴짐. 각 SM당 하나씩 유지되는 이러한 커널 방식이 과거 Larrabee를 연상시킴. 기존 CUDA가 아닌, 전통적인 프로세스-스레드-SIMD 경로를 갔으면 지금 세상이 어떻게 됐을까 궁금증.
     * 소프트웨어 기반 추론보다 순수 ASIC 방식으로 고정형 LLM을 만드는 것에 대한 아이디어. 비용 이점? 소프트웨어에서 추가로 다루거나 미세 조정 가능한 계층 제공 가능성? 실제로 ‘충분히 좋은’ 수준에 거의 도달한 만큼, 앞으로 2~4년간 전문화된 칩으로 고정해 사용하는 결정을 할지 모를 상황. 초특화 하드웨어가 제공할 이점이 도대체 어느 시점에서 빛을 발할지 의문.
          + 이어지는 추가 질문:
              1. 오토컴플리트, 키워드 라우팅, 음성 인식 등 특정 태스크에서 ASIC과 megakernel GPU 각각의 지연과 전력 소모 차이, 그리고 엣지 디바이스에서 고정형 함수 방식 채택을 정당화할 정도인지에 대한 의문.
              2. ASIC은 재학습이 어렵지만, 기본 모델만 하드웨어에 박고, LoRA처럼 작은 학습 가능한 모듈만 범용 코프로세서에 띄우는 하이브리드 구상 가능성.
              3. 트랜스포머의 고정 토폴로지가 ASIC 설계에서 공간 재사용에 적합한지, 아니면 GPT-3 급 모델 크기로 인해 과감한 프루닝이나 양자화 없이는 여전히 ASIC화가 어렵지 않을지에 대한 탐구.
"
"https://news.hada.io/topic?id=21546","키보드에 내장된 Steam Deck: Bento","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       키보드에 내장된 Steam Deck: Bento

     * Bento는 키보드에 내장된 컴퓨터로, 외장형 디스플레이와 함께 사용하도록 설계된 프로젝트임
     * 디스플레이가 없는 구조로, XREAL One과 같은 공간 디스플레이나 USB-C 모니터와의 활용에 최적화됨
     * 내부에는 Steam Deck OLED의 메인보드, 쿨러, 배터리가 탑재되어 있으며, 다른 싱글보드 컴퓨터(SBC)도 호환 가능함
     * 기존 XR 기기의 한계를 극복하고, 진정한 공간 컴퓨팅 기능 제공을 목표로 함
     * 오픈 소스로 개발되고 있어, 다양한 하드웨어 및 주변기기 지원 등 커뮤니티의 기여를 환영함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Bento 소개 및 의의

     * Bento는 키보드 아래에 내장된 컴퓨터로, 이름은 도시락(bento box)에서 착안했음
     * Commodore 64 및 r/cyberdeck 커뮤니티 작품에서 영감을 받아 설계함
     * 키보드가 뚜껑 역할을 하며, 내부에 손쉽게 접근 가능하고 주변기기를 수납할 공간이 마련되어 있음

디스플레이 미포함 설계

     * Bento는 내장 디스플레이가 없는 것이 주요 특징임
     * XREAL One 등 공간 디스플레이와의 사용을 염두에 두고 있어, 별도 모니터(USB-C 지원)와 연결하여 사용함
     * 이로 인해 중복된 부품을 없애고, 불필요한 무게와 공간을 줄여 휴대성이 높아지는 장점이 있음

내부 하드웨어 구성

     * 현재 버전은 Steam Deck OLED의 메인보드와 쿨러, 배터리를 활용함
     * 기존에 입수 가능한 부품 중에서 가장 얇으면서도 강력한 성능을 제공하는 메인보드임
     * 다른 싱글보드 컴퓨터(SBC) 도 충분히 호환 및 장착 가능하도록 설계됨

개발 목적

     * 기존 XR 하드웨어의 한계와 답답함에서 출발함
          + 현재 XR 기기는 실질적으로 콘텐츠 소비(웹, 게임)에만 국한되어 있고, 진정한 컴퓨팅 성능을 제공하지 못하고 있음
          + 실제 작업이 필요할 때는 결국 일반 컴퓨터 화면을 미러링하는 방식이 주류임
     * Bento는 공간 디스플레이 최적화와 휴대성을 동시에 추구하는, 실질적인 공간 컴퓨터를 목표로 개발됨

오픈 소스 및 커뮤니티 요청

     * 이 저장소에는 Bento의 3D 모델 파일(STEP, 3MF, STL) 등이 포함되어 있음
     * Magic Trackpad 트레이 등 주변기기 액세서리 설계도 제공함
     * 기여 요청:
          + 다양한 키보드 지원 확대
          + Raspberry Pi 5 변형 지원(BATTERY 솔루션 연구 필요)
          + Framework 키보드와의 조합
          + 여러 SBC 지원
          + 게임패드, 마우스 등 수납 가능한 주변기기 개발 등
     * 직접 Bento의 변형을 만들 경우, 풀 리퀘스트(PR) 를 통해 커뮤니티에 공유할 것을 요청함

결론

     * Bento는 공간 디스플레이와의 사용을 염두에 두고 설계된, 새로운 형태의 키보드 내장 컴퓨터임
     * 오픈 소스로 배포되어, 하드웨어 해커 및 개발자 커뮤니티의 활발한 참여와 확장을 기대함

        Hacker News 의견

     * 이것은 네이티브 리눅스 환경을 깔끔하게 사용할 수 있어서 정말 멋진 경험임
       최근 Quest 3에서 웹 개발을 시도한 경험을 블로그에 공유한 적 있는데, 가능은 했지만 꽤나 번거로웠고 성능도 부족함
       하지만 Quest에서는 내 주위 공간에 가상 창을 배치할 수 있다는 점이 고정 2D 모니터 방식보다 훨씬 흥미로운 포인트임
       내가 쓴 블로그: VR에서의 웹 개발 경험
     * 이번 프로젝트는 내 머릿속 한 구석에서 계속 떠올랐던 아이디어임
       Apple Magic Keyboard 아래에 딱 맞는 컴퓨터로, 동글이나 작은 마우스 같은 주변기기를 보관할 수 있는 칸도 있음
       모니터가 없고, XR 디스플레이 글래스를 사용하는 방식임
       내부는 Steam Deck OLED의 메인보드, 쿨러, 배터리로 구성됐고, 부품은 따로 구매함
       프로젝트의 CAD 파일을 오픈소스로 제공함. 앞으로 더 발전시키고 싶음
       프로젝트의 시작은 XREAL 글래스 사용에서 비롯됨. 지금까지 써본 XR 제품 중 가장 괜찮았음
       XR의 본질은 가상 디스플레이임. 불필요한 하드웨어를 최대한 덜어내서 가볍고 컴팩트하게 만들고 싶었음
       아이패드를 꺼내 Shapr3D로 설계 시작. 아이패드와 3D 프린터를 번갈아가며 약 15번 정도 실패 끝에 스크류 마운트, 공기 흐름, 인체공학적인 디자인을 맞춤
       최종 결과물은 내가 생각하는 진정한 공간 컴퓨터임
       매일 직접 쓰고 있는데 만족스러움. 현재 Ubuntu 24 기반이지만 Steam OS로 돌아갈 수도 있음. 하드웨어에 더 최적화됐기 때문임
          + 댓글을 읽고 나서야 ‘spatial display’나 XREAL이 무슨 의미인지 알게 됨. 프로젝트의 readme.md에 적어주면 좋겠음
          + 작은 마우스를 들고 다니기보다는 케이스 자체를 트랙패드로 만드는 아이디어도 떠오름
            관련 트랙패드: ploopy.co/trackpad
          + XREAL 글래스를 몇 달째 사용 중이라는데, 시력이 어떤지 궁금함
            피로감이나 다른 증상이 있는지
            게임이나 영화 등 엔터테인먼트용으로는 괜찮은데, 작업용으로는 화면이 선명하지 않다고들 하던데 실제 경험 궁금함
          + Ubuntu 24를 쓰고 있지만 Steam OS로 다시 갈 생각이라 했는데, 구체적으로 어떤 문제점이 있는지 궁금함
            배터리, 성능, 안정성 중 어느 부분이 문제인지 궁금함
          + 파일과 경험 공유에 정말 고마움을 느낌
            나도 첫 XR 글래스(Xreal One Pro)를 사서 비슷한 작업을 해보고 싶었음. 정말 인상적임
            ifixit에서 Steam Deck의 메인보드는 찾을 수 없던데, 어디서 구입했는지 궁금함
     * 앞으로 Framework 기반 버전도 계획하고 있다는데, 몇 달 전 r/framework에서 비슷한 Framework 13 빌드가 올라왔었음
     * Framework XR Cyber Deck 빌드
     * Ryzen AI 라인 메인보드로 업그레이드한 사례도 있음
     * VR/XR Cyber Deck 완성 버전
     * GitHub에 STL 파일 및 상세 빌드 정보 제공. Linux와 Stardust XR, XREAL 글래스로 공간 입력 없음
     * fyer_deck Github 저장소
          + r/cyberdeck에도 유사한 아이디어들이 많지만, 개인적으로 디자인이 마음에 안 들었음
            나는 아주 심플하고, 비행기에서도 어색하지 않게 사용할 수 있으면서도 보기에도 좋은 디자인을 원함
            내가 올린 Reddit 스레드에서도 그 사람도 코멘트했는데, 그와 framework 버전 콜라보를 해보고 싶음
     * 예전부터 진정한 사이버덱 형태의 PC가 등장하길 바랐는데, 이제 조금씩 현실이 되어가는 점이 반가움
     * 창작자에게 제안하고 싶은 것은, 키보드 양옆에 스마트폰이나 소형 태블릿의 터치 디스플레이를 넣는 것임
     * VR/AR 환경에서는 트랙패드, 비 VR 환경에서는 알림창, HMD가 없거나 고장 나면 예비 모니터 등 다양한 용도 가능성 있음
          + 현재 모듈 마운팅 시스템을 양 옆에 추가하여 다음 리비전을 제작 중임
          + 트랙패드 내장도 고민했는데, 왼손잡이라 왼쪽에 넣으려고 했을 듯. 하지만 그러면 너무 개인 맞춤형으로 될 위험이 있었음
     * 정말 멋진 프로젝트임
     * 다른 키보드를 지원하는 것은 오히려 복잡성만 높이고 본질적인 장점에 방해가 될 수 있음
     * steamdeck 내장 부품, rpi5, framework 내장 부품만으로도 충분히 도전적인 구성임
     * 내 취미는 모든 것을 소형화하는 것임
     * 만약 내 스타일대로 포크한다면, 좋은 동글 하나를 선정해서 필요한 부가 기능 일부와 함께 하우징까지 최대한 소형화해서 나만의 여행용 컨트롤러, 마우스를 완전히 밀착 일체형으로 만들고 싶음
     * 관련 정보: 좋은 슬림 듀얼 아날로그 컨트롤러, 마우스는 아직 찾는 중임
          + Magic Keyboard가 꽤 괜찮다고 생각함
          + 제품화한다면 원본 키보드를 따로 구해야 하지만, 최대한 비슷하게 유지하려고 할 것임
          + 원래는 HHKB studio를 쓰려다, Magic Keyboard가 더 얇고 미국 내 많은 곳에 보급돼 있어 선택함
     * Framework의 디스플레이를 아예 제거하고 Xreal Air로만 사용하는 아이디어를 생각해왔음
     * 이번 프로젝트에서 영감을 받아 나도 screenectomy(화면 떼어내기)를 시도해 보고 싶어짐
     * 부디 고장만 나지 않길 바람
          + 모든 것을 부숴보는 경험 권장
          + 최근에 변기를 고치다가 아주 당황했었는데, 결국 해결함
          + 지난 몇 년간 헤드리스(화면 없는) 랩탑 자작 사례가 부쩍 많아지고 있음
          + 아마 원 글 본 프로젝트보다 더 나은 방식일 수도 있지만, 결국 어떤 형태와 수준의 직접개조를 원하는지가 중요함
          + 헤드리스 맥북 프로 예시
          + 디스플레이 자리에 배터리를 더 넣으면 이동성 유지와 배터리 수명도 늘릴 수 있지 않을지 궁금함
          + 실질적으로 디스플레이를 제거하는 과정은 단순함
          + 단, Wi-Fi 보존을 원한다면 주의 필요. 안테나가 상판에 들어 있어 옮기거나 별도 안테나 필요함
     * XR 업계 주요 업체들이 자신의 하드웨어를 '컴퓨터'라고 홍보하지만, 실상은 얼굴에 쓴 아이패드라고 생각함
     * 웹 브라우징, 게임, 콘텐츠 소비 등 제한적 활용만 가능해 너무 무겁고 제약 많음
     * HMD 프로그래밍 자체를 좋아하는 사람으로서 이 프로젝트에 깊은 관심이 있음
     * 참고로, AOSP 기반 XR 헤드셋은 termux+X로 가짜 리눅스 환경 운영 가능
     * Oculus Quest termux HMD cyberdeck 후기
     * 원격근무시 주로 쓰는 포터블 풀사이즈 키보드도 소개하고 싶음
     * ProtoArc XK01 트라이폴드 블루투스 키보드
     * 말하는 '포켓 사이즈' 정의에 따라서는 충분히 주머니에 들어갈 수 있음
          + 예전에는 termux에 큰 기대를 했지만, 3년 전부터 너무 오래된 Android API 버전만 지원해서 실망스러웠음
          + Google이 프로젝트 지원에 별 관심 없는 듯함
          + 최근까지도 F-droid 업데이트가 계속 깨지고 있음
          + termux 이슈 관련 내용
          + 나 역시 XR 분야에서 거의 10년 가까이 일했고, 수많은 ‘XR에서 개발’을 시도해봄
          + 심지어 최근 2년간은 XR 네이티브 개발 플랫폼 제작에 도전함
          + 모든 기기에서 XREAL만 빼고 ""너무 무겁다""는 피드백이 없었음
          + 무엇보다도 핵 느낌이 아닌, 이 경험만을 위해 맞춤 제작된 기기를 만들고 싶었음
     * 지금 베스트 AR 글래스가 뭔지 궁금해하는 친구가 있다며 그에 대한 질문
          + XREAL One Pro가 단연 최고임
     * AR 글래스 등장으로 ‘사이버덱’ 시장이 열리기 시작함
     * 대중화는 어렵겠지만, 많이 이동하며 일하는 개발자들에게는 충분히 매력적인 아이템임
     * 개인적으로 XREAL One 글래스와 연결해서 바로 구매하고 싶을 정도임
     * 중요한 부분은 알맞은 크기와 무게 조정임
     * LingLong이 킥스타터에서 소규모로 트라이했지만 본격 대량생산에는 큰 흥미를 못 보이는 듯함
     * 정말 멋진 프로젝트임
     * 얼마 전에도 유사한 글래스를 사용하는 프로젝트가 올라왔는데, 그 경우는 랩탑 대형 디스플레이를 대체하려 했음
     * 이 프로젝트는 랩탑 자체를 SBC로 대체했다는 점이 더 좋게 느껴짐
     * 15년 넘게 SBC가 라즈베리파이로 되길 바랐는데 드디어 현실화되는 느낌임
          + 나도 마찬가지로, 그래서 SoM 회사에 연락해서 저렴하게 만들 수 있는지도 알아보고 있음
"
"https://news.hada.io/topic?id=21552","몽골의 모든 유르트/게르 수를 머신러닝으로 세어보기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      몽골의 모든 유르트/게르 수를 머신러닝으로 세어보기

     * 머신러닝 알고리듬을 활용해 몽골 전역의 Yurt(게르) 수를 직접 세기 위한 프로젝트를 진행
     * 단일 인공위성 이미지를 바탕으로 유르트를 식별하도록 YOLO 모델을 훈련하고, 대규모 데이터 라벨링을 자동화
     * 분산 서버, Docker Swarm, FastAPI 등을 이용해 120여 개 워커가 병렬적으로 이미지 타일을 처리하고 결과를 집계
     * 최종적으로 17만 2,689개의 유르트를 발견했으며, 이 결과는 몽골 도시 성장과 주거 인프라 문제를 이해하는 데 기여
     * 몽골의 게르 지구와 그 사회적 배경 및 발전 과제에 대한 통찰도 함께 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

몽골 유르트를 머신러닝으로 세기 – 프로젝트 개요

  몽골 현대 사회에 대한 궁금증

     * 몽골 제국의 역사와 현대 몽골의 모습을 이해하기 위해 데이터를 분석하는 대신 Google 지도 위성 보기를 적극적으로 탐색함
     * 울란바토르 위성사진에서 수 킬로미터에 이르는 방대한 유르트 집단을 확인하고, 이 유르트가 정확히 몇 개나 되는지 직접 세어보기로 결심함

  데이터와 라벨링 준비

     * Google Maps에서 울란바토르 인근을 중심으로 타일 형태(256x256 px)의 위성 이미지를 자동으로 수집함
     * 타일을 Label Studio로 불러와 유르트를 일일이 바운딩 박스로 수작업 라벨링하며, 이 과정을 통해 주석 데이터(annotated data) 를 생성함
     * 객체 검출용 알고리듬으로 YOLO11(ultralytics) 을 채택, annotated 데이터셋을 모델 학습에 활용함

  모델 훈련 및 데이터셋 확장

     * YOLO11 기반으로 모델을 학습시키는 것 외에, 라벨링-재학습-라벨링의 반복적 피드백 루프 방식으로 자동화 수준을 높임
     * 초기에 데이터가 부족해 정확도가 낮았으나, 반복적 추가 라벨링과 샘플 수 확대로 검출률을 끌어올림
     * 모델 훈련은 노트북 대신 vast.ai GPU 리소스를 임대해 Docker 컨테이너 환경에서 대규모로 수행함. 훈련 완료 시 S3 저장소로 모델 결과 및 메타데이터 자동 업로드

  전국 단위 탐색 시스템 구축

    탐색 범위 최적화

     * 몽골 전체 면적 기준으로 각 줌 레벨별 타일 수 산출
          + 인구밀도가 낮아 비거주 지역을 제외하기 위해 overpass turbo로 사람 거주지가 있을 만한 포인트를 추출
          + 추출 포인트 주변(2km 버퍼)을 기준으로 실제 검사할 타일 집합을 대폭 축소

    대규모 분산 처리

     * Docker Swarm을 활용, 8대의 서버(총 128 vCPU)로 클러스터 구성
     * API 서버(FastAPI)와 워커로 역할 분리:
          + API: 워커에 할당할 검색 영역과 타일 집합을 관리, 진행 상황 및 상태 관리
          + 워커: API에서 검색 영역을 받아 해당 타일의 유르트를 모델로 검출해 API에 결과 등록

    결과 집계

     * 전체 약 270,000여 검색 영역, 수백만 이미지를 병렬 처리
     * 최종적으로 17만 2,689개 유르트를 확률 40% 이상 검출 결과 기준으로 확인함
     * 데이터셋을 공개하여 토지 이용, 호텔, 소규모 광산 인근 유르트 분포 등 분석

유르트와 몽골의 사회적 맥락

  게르(유르트) 지역의 역사와 변화

     * 유르트는 역사적으로 몽골 유목민의 전통적 주거 형태였으나, 도시화와 산업화 과정에서 다방면으로 쓰임새가 변화함
     * 20세기 초에는 임시 학교 등 공공 목적에도 유르트가 활용되었으며, 대도시로의 인구 유입과 함께 울란바토르 등지에 게르 지구가 생성됨

     ""1979년 인구조사에서 전체 인구의 51%가 도시 거주로, 1970년대 급속한 도시화 반영. 주택 및 인프라 부족으로 도시 외곽에 게르 지구가 확장됨.""

  도시화와 인프라 도전 과제

     * 농촌에서 유입된 인구가 유르트를 갖고 도시로 이주, 공식 인프라가 갖춰지지 않은 상태에서 거주시설로 사용
     * 2002년 토지 소유권 관련 법 도입으로 게르 지구 거주민의 정착지 법적 공식화가 진행됨
     * 정부는 울란바토르 2020 마스터플랜 등 재개발 정책을 추진하고 있으나, 실제 진행 속도는 더딘 편임

     ""게르 지구 토지 소유자가 건설업자에게 토지를 매각 또는 교환해 신규 아파트 건설이 이루어지나, 아파트 가치가 종종 토지보다 낮거나, 개발 속도가 느림""

  시사점 및 향후 전망

     * 게르 지구의 공식화 및 인프라 제공이 여전히 사회적, 정책적 과제임을 시사
     * 몽골 정부의 장기 목표는 게르 지구에 주택, 수도, 전기 등 도시 인프라 보급임
     * 데이터 기반 정책 수립 및 지속적 추적이 필요함

추가 탐구 질문

     * 몽골 및 타국에서 도시화·산업화가 발생하는 주요 요인
     * 몽골인 중 도시에 정착하는 사람과 남는 사람의 차이
     * 게르 지구 개발의 정부 측 애로 사항
     * 국가별 개발 속도의 차이를 낳는 배경

참고 문헌

     * 주요 정책·사회학·인프라 관련 논문, 보고서 및 데이터베이스 참고
          + “Distributional Effects of Ger Area Redevelopment in Ulaanbaatar, Mongolia.”
          + Ulaanbaatar 2020 Master Plan and Development Approach for 2030.
          + “Educational Import: Local Encounters with Global Forces in Mongolia.”
          + Mongolia: A Country Study. Federal Research Division, Library of Congress.
          + Poverty Mapping in Mongolia with AI-Based Ger Detection Reveals Urban Slums Persist after the COVID-19 Pandemic. arXiv.

마무리

     * 기술과 데이터로 사회 문제를 들여다보고, 사회적 배경과 거주 패턴에 대한 새로운 시각을 제시함
     * 다양한 기법과 오픈소스 도구(머신러닝, Docker, FastAPI 등) 조합의 실제적 응용 사례임

        Hacker News 의견

     * 도시에서 게르/유르트 지구 이야기가 나오면 유목 생활과 게르 문화의 문화적 중요성을 과소평가해서는 안 된다는 생각임. 최근 기후 변화(사막화)와 경제적 이유로 많은 사람이 본의 아니게 유목 생활을 포기하고 울란바토르 같은 도시 근처로 이주하는데, 대개 임시적이라고 생각하며 마지못해 도시로 오는 경우가 많음. 주택 부족 문제뿐 아니라 유목을 완전히 포기했다는 상징이 돼서 아파트 등 영구 구조물로 아예 이사하는 걸 꺼리는 심리도 큼. 그래서 영구 건물 옆에 게르를 세우기도 하고, 친척 집 마당에 더하거나 문화적 정체성을 지키는 방편으로 확장해서 사용하는 모습도 자주 볼 수 있음. 이런 사례는 첫 사진들에서 확인 가능함
          + 몇 년 전 오토바이로 몽골을 횡단하며 놀란 점이 있었는데, 좋은 영구 주택에 살아도 뒷마당에 반드시 게르가 있었음. 외부인 입장에선 두 번째 집이 필요하냐고 의아해했는데, 현지인에게 물으니 이상하다는 눈빛을 받음. 게르는 그냥 문화에 깊이 박혀있고, 일종의 위신 상징이면서 손님 접대, 야외 생활, 다방면으로 활용하는 공간임
          + 우즈베키스탄의 히바라는 궁전에서 본 경험을 말하고 싶은데, 분명히 출입구와 화려한 방이 많은 전통 궁전이었음에도 벽으로 완전히 둘러싸인 어느 안쪽 마당 한 켠에 둥근 자리, 즉 게르를 세우는 자리가 따로 있었음. 이 지역 칸들도 칭기즈 칸의 혈통을 자랑하는데, 도심 내에 살아도 영구 천장 아래서 밤을 보내는 건 칸답지 않다는 인식이 있었고, 방문하는 친척들도 그런 모습은 환영하지 않는 분위기였음
            Toshhovli Palace 위키
            [수용 마당의 둥근 자리 사진](https://en.wikipedia.org/wiki/Toshhovli_Palace/…)
          + 몽골은 최근 몇 번의 혹독한 겨울 때문에 초원에서 대규모 이동이 벌어져 가축 떼가 크게 줄어든 상태라 대다수가 도시로 몰리는 중임. 원한다 해도 영구 건물에 들어갈 자리가 부족한 실정임
          + 듣기로는 이미 게르를 하나쯤 갖고 있을 거고, 필요할 때 이사도 상대적으로 쉽다고 추정함. 예를 들어 특별한 행사가 있으면 다시 시골 집으로 돌아간다는 이야기도 들은 적 있음
          + 게르 생활이 꼭 공공 정책 실패 탓이란 시각보다는, 그저 문화적 선택이라는 해석에 더 동의함. 과거 칭기즈 칸도 게르에 살았고, 실제로 어떤 사람은 필요에 의해서, 또 다른 사람은 스스로의 의사로 게르 생활을 선택함. 따라서 그런 모습 자체가 결코 부정적이라고 단정하긴 어려움
     * 울란바토르에는 표준화된 게르가 있음. 대규모 시장에서 부품이나 완제품 게르를 쉽게 구입 가능함. 2017년 기준으로 하나에 약 1,000달러였음. 그 돈이면 제대로 단열된, 쉽게 옮길 수 있는 작은 집을 구할 수 있고, 몽골에서는 도심 밖에서 어디든 정착할 수 있음(단, 2,000마리 양과 함께라면 목초지 사용은 현지와 논의하는 게 좋음). 결국 게르 선택은 전통과 문화뿐 아니라, 그 상황에서 합리적인 결정이기도 함
          + 참고로, 예의상 터키식 주택 텐트를 유르트라고 부르고, 몽골식은 게르라고 부름. 프랑스에서는 샤바두, 캐나다에서는 플럼버스, 미국에서는 플립이라고 부르기도 한다고 농담 삼아 언급함. 내 샤바두를 보고 현지인이 게르라 부르면 조금 신경 쓰인다는 우스갯소리 덧붙임
          + 게르를 놓기 위한 어떤 형태의 기초 공사를 하는지 궁금함
     * 몽골에 머신러닝을 적용한 유르트가 0개라는 언급이 있었음
          + 그럴 리 없다는 생각으로 오히려 꽤 있을 거라 추정함
          + 처음에 유르트를 직업이나 사람의 유형이라고 생각해서, 제목을 잘못 이해함
          + 고마움 전하고, 덕분에 웃긴 경험이었다고 언급함
          + 비원어민으로서 더 올바른 문장은 뭘까 고민함. ""머신러닝으로 몽골의 모든 유르트를 세어봤다""는 식의 문장도 언급함
     * 오픈스트리트맵(OSM)에 이미 윤곽선이 표시된 89,259개의 유르트를 입력에 활용하지 않은 점이 아깝게 느껴짐. 다만 윤곽선을 구글맵 이미지와 정렬하는 데 문제 있었을 것임
       OSM 몽골 게르 태그 통계
       모델이 타일 경계에 있는 유르트는 잘 못 잡았을 듯 추정함. 그리고 인구 3백만에 비해 숫자가 훨씬 적어 의외라는 느낌 듦
          + ""3백만 명에 비해 게르 숫자가 적다""는 부분에 대해, 실제 집계 결과가 172,700개라면 각각 가족 거주용으로 추정하고 한 게르당 인원이 4명(아마 실제로는 더 많음)을 적용하면 약 69만 명으로 몽골 인구 350만 중 20%임. 꽤 그럴듯한 수치로 보임
          + 링크 클릭 전 대략 추정해본 수치도 공유함. 몽골 인구 3백만 중 수도에 150만 거주. 100만 명 정도가 도심 외부 거주라 가정하고, 4명당 한 게르라면 25만 개. 거기에 손님용, 창고용, 주택 마당 등 부차적 용도를 고려하면 30만 개쯤 추산하는데, ML 앱 결과보다 거의 두 배 수준임
          + 이런 식으로 OSM 등을 라벨로 쓰자는 아이디어는 지리/머신러닝 프로젝트에서 자주 거론됨. 하지만, OSM은 구글맵 이미지를 채택하지 못하게 라이선스가 막혀 있어서 연구 목적이라 해도 이미지 수급이나 재공유 등에 법적 문제가 많음. 구글이 다양한 외부 이미지 소스에서 서브라이선스 받아 IP를 엄격하게 관리함. 이미지/라벨 정렬 문제도 크고, 라벨링 자체가 이미지가 아닌 GPS 좌표일 수도 있음. 게다가 게르처럼 이동식 구조물은 라벨 완성도나 정합성이 떨어질 수밖에 없음. OSM의 완전성도 지역 커뮤니티의 활동성에 크게 좌우됨. 그래도 자체 라벨과 예측값 교차검증에는 활용할 수 있음. 타일 단위 탐지 시에는 경계 예측을 보통 버리고, 오버랩 윈도우 및 NMS 등으로 중복 처리함
          + 17만2천여 개라면 아직도 엄청 많은 숫자이고, 인구 대비 게르 수가 세계 어디보다 월등하다는 점을 강조함
     * 구글맵 위성 이미지를 직접 다운로드하는 건 이용약관상 금지됨을 강조하고 싶음. 실제로 차단되기 쉬운데, 몽골 전체 타일을 다 받았다는 사실에 놀람
          + 시장 독점 외에는 이런 정책의 이유를 이해하지 못하겠다는 반응임
          + 차단당하면 새 계정 만들면 된다는 의견도 있음
     * 결과가 흥미로워서, 실제로 오탐지 비율이 얼마였는지 궁금함. 저장 탱크나 사일로, 야외 수영장이 게르로 잘못 분류됐는지도 묻고 싶음
     * 대학 시절 접했던 Geo/ML 프로젝트 이야기라 오랜만에 보는 즐거움이 컸음. 호주 정부도 유사 작업에 매년 큰 돈 들여 투입하지만, 글 쓴 사람의 결과와 비교하면 우리 정부가 훨씬 비효율적으로 보인다는 아쉬움 있음. 땅 하나 제대로 분류 못하고, 작은 게르 모양 오브젝트 카운트조차 제대로 못 하는 현 실태에 답답함 느낌
          + 비슷한 경험이 있는 지역 개발자/즉흥예술가가 있다는 말 하려 했는데, 작성자의 닉네임 보고 놀랐다는 반응
     * ""총 172,689개 게르를 40% 초과 정확도 예측 점수로 탐지""라는 결과에서 'prediction score' 해석 방법 궁금함
          + 객체 탐지기는 탐지된 경계 상자마다 신뢰도 점수를 주는데, 점수가 높을수록 모델이 해당 박스가 맞다고 판단함. 보통 이런 응용에서 유저가 임계치를 정하고, 그 기점 이상은 정탐지로, 미만은 버리는 방식임. 임계치는 임의이거나 약간의 원칙 따라 설정 가능함
     * 처음에는 몽골의 모든 유르트가 머신러닝을 '사용'한다는 뜻으로 제목을 오해함
     * 반상업 솔루션(교육 용도 무료)을 사용했다고 하며, 딥러닝 모델의 토폴로지/아키텍처가 궁금함. 더 나은 접근법이 있는지도 호기심 있음
"
"https://news.hada.io/topic?id=21538","Unregistry – “docker push”를 레지스트리 없이 서버에 직접 전송","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Unregistry – “docker push”를 레지스트리 없이 서버에 직접 전송

     * Unregistry는 외부 레지스트리 없이 Docker 이미지를 원격 서버에 직접 전송할 수 있는 오픈소스 툴임
     * docker pussh 명령으로 SSH를 통해 원격 서버에 이미지를 효율적으로 전송하며, 이미 존재하는 레이어는 건너뜀
     * 전통적인 Docker Hub, 자체 레지스트리, save/load 방식의 복잡함과 비효율성을 해소함
     * 운영 환경 배포, CI/CD, 폐쇄망 환경 등에서 빠르고 안전한 이미지 이전이 큰 장점임
     * 설치, 사용, 요구 사항이 매우 단순하며, 추가 서비스 운영이나 포트 노출이 필요 없음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Unregistry 소개 및 주요 장점

     * Unregistry는 Docker 데몬의 스토리지에서 직접 이미지를 저장 및 제공하는 경량 이미지 레지스트리임
     * docker pussh 커맨드를 사용하면 SSH를 통해 외부 레지스트리 없이 이미지를 바로 원격 Docker 서버로 이동 가능함
     * 전송 시 이미 서버에 존재하는 레이어는 제외하고 필요한 부분만 빠르게 전송하는 효율성 제공함

기존 Docker 이미지 배포의 문제점

     * 이미지를 로컬에서 빌드한 후 서버로 전송할 때 선택지가 아래와 같음
          + Docker Hub/GitHub Container Registry: 코드가 외부에 공개되거나 개인 저장소 사용 시 비용 발생함
          + 자체 레지스트리: 별도 서비스 운영 및 보안, 스토리지 관리 부담 증가함
          + Save/Load: 전체 이미지를 항상 전송하여 비효율 발생함
          + 서버에서 직접 재빌드: 시간 및 서버 리소스 낭비와 디버깅 문제 발생함

Unregistry 솔루션

     * docker pussh myapp:latest user@server 명령 한 번으로 중간 저장소 없이 직접 전송 가능함
     * 추가적인 레지스트리 설정, 포트 노출, 스토리지 준비, 구독 필요 없음
     * 전송 과정
          + SSH 터널을 원격 서버로 연결함
          + 임시로 unregistry 컨테이너 실행함
          + 랜덤 로컬 포트와 unregistry 포트 연결함
          + docker push로 존재하지 않는 레이어만 전송함 (즉시 사용 가능)
          + unregistry 컨테이너와 SSH 터널 종료함
     * rsync처럼 단순·효율적 방식임
     * 본 프로젝트는 Uncloud에서 다중 Docker 호스트로 컨테이너를 배포하는 과정의 복잡함을 단순화하기 위해 개발됨

사용 예시

  배포 환경으로 직접 이미지 전송

     * 로컬에서 빌드 후 직접 운영 서버로 push함
          + docker build --platform linux/amd64 -t myapp:1.2.3 .
          + docker pussh myapp:1.2.3 deploy@prod-server
          + ssh deploy@prod-server docker run -d myapp:1.2.3

  CI/CD 파이프라인

     * 레지스트리 복잡성 없이 빌드 및 배포 지원
          + GitHub Action YAML 등에서 직접 전송 사용 가능

  홈랩, 인터넷이 없는 폐쇄망 환경

     * 이미지 인터넷 공개 없이 격리 네트워크에 안전하게 전송함

사용 방법

     * SSH 사용자 계정이 원격에서 docker 명령을 사용할 수 있어야 함
     * SSH 프라이빗 키나 커스텀 SSH 포트 등 추가 옵션 지원함
     * 멀티 플랫폼 이미지 전송도 지원 (containerd 기반일 경우)

요구 사항

  로컬 환경

     * Docker CLI (플러그인 지원, 19.03+)
     * OpenSSH 클라이언트

  원격 서버

     * Docker가 설치 및 실행 중이어야 함
     * ssh 사용자가 docker 권한을 소유해야 하며, 필요 시 패스워드 없는 sudo docker 실행 가능해야 함
     * containerd image store 사용 시 성능이 향상됨
          + /etc/docker/daemon.json에 다음 설정 추가 및 docker 재시작 필요
{
  ""features"": {
    ""containerd-snapshotter"": true
  }
}

고급 사용법

  로컬 독립형 레지스트리로 사용

     * 추가 컴포넌트 없이 unregistry를 로컬 레지스트리로 손쉽게 운영 가능함
     * 도커 명령으로 deploy 및 push 가능

  SSH 커스텀 옵션 활용

     * SSH config 파일을 활용해 추가 인증, 포트 등 조건에 맞는 세부 설정 가능

기여 및 커뮤니티

     * 버그 발견 시 GitHub 이슈 등록 활용
     * Uncloud Discord 커뮤니티에서 피처·로드맵·구현 세부 논의 가능

영감과 참고 오픈소스

     * Spegel: containerd 기반의 P2P 컨테이너 이미지 레지스트리 구현에서 영감 받음
     * Docker Distribution: 실제 레지스트리 구현 베이스로 사용됨

요약

     * Unregistry는 Docker 이미지를 쉽고 빠르게 원격 서버로 직접 이전할 수 있는 툴로 레지스트리 구축 및 관리 부담을 없앰
     * 운영 환경 배포, CI/CD, 폐쇄망 등 다양한 시나리오에서 강력한 이점을 제공함
     * 서버와 관리자가 단순하게 이미지만 절차 없이 이동하고 싶은 경우 매우 적합함

        Hacker News 의견

     * 서버의 특성과 보안 경계, 하드닝 측면에서 Homebrew를 Linux에서 쓰는 것을 권장하고 싶지 않음, Linux용 설치가 마치 사후 처리처럼 제공됨에도 불구하고 패키지 매니저라기보다는 바둑판 위의 비둘기처럼 동작하는 모습임
     * 시스템에서 Ansible 같은 푸시 배포 툴링을 이미 사용하는 곳에 잘 어울릴 만한 멋진 아이디어라는 생각임, 또 Docker 레지스트리가 24시간 지원되지 않는 기업에는 핫픽스 배포 기법으로도 적합함을 느낌, OCI 툴링(buildah 등)과도 깔끔하게 연동되는지, 아니면 양쪽 모두에 Docker 전체 설치가 필요한지 궁금한 상황임, 아직 본격적으로 파보진 않았으나 이와 관련된 작업 예정이고, skopeo가 이러한 환경에서 작동하려면 원격 서버에서 자체 레지스트리를 부트스트랩 할 수 있는 기능이 부족하다고 느꼈음
          + 원격 서버에는 containerd가 필요하고(Docker와 Kubernetes도 containerd 사용), 클라이언트 쪽에는 레지스트리 API를 이해하는 어떤 것이든 가능(OCI Distribution spec: https://github.com/opencontainers/distribution-spec), Unregistry가 공식 Docker registry 코드를 API 계층으로 재사용해서 Docker Hub의 registry와 유사한 느낌임, skopeo, crane, regclient, BuildKit 등 OCI registry를 사용할 수 있으며, 이들을 쓰려면 원격 호스트에서 unregistry 직접 실행이 필요함, 'docker pussh' 명령어는 로컬 Docker를 활용해 이 모든 흐름을 자동화해주는 역할임, bash 스크립트로 되어 있으니 한 번 확인 권장 https://github.com/psviderski/unregistry/blob/main/docker-pussh, 직접 마음대로 수정도 쉬움
          + 양쪽에 docker daemon이 필요함, 이 방법은 두 데몬 사이에 layer를 ssh로 공유한다는 영리한 방식을 사용함
     * pussh 명령어가 기억하기 쉽고 자기설명적이며, 기존 표준 명령어와 한 글자 차이만 나는 멋진 언어유희를 보여준다고 생각함
          + ""pussh""도 괜찮지만, 자동화에선 ""docker push-over-ssh"" 같은 좀 더 명확한 별칭이 좋을 것 같다는 의견임, ""pussh""를 처음 보는 사람이 오타로 오해할 수 있고 불필요한 혼란이 생길 수 있음, 짧은 버전과 전체 플래그 버전 모두 지원하면 좋다고 봄
          + 's' 하나를 더 쓴 이유가 'sssh'를 나타내려는 것인지 농담 섞인 설명이 있음, 어떤 이는 단순 오타라고 함
          + 'pussh'라는 이름이 다른 명령어와 충돌 가능성이 있음
     * 이런 기능은 예전부터 있었어야 했고 매우 멋지다고 생각함, Docker registry는 그 자체로 가치가 있지만 전반적으로 너무 복잡해지고 해커 마인드와는 거리가 멀어졌다는 생각임
          + Docker가 VC 투자를 받았던 회사인 만큼, 어떻게든 수익을 내야 했던 상황임
     * 프로젝트와 접근법이 인상적임, 비싼 레지스트리에 질려서 Zot(https://zotregistry.dev) 같은 걸 직접 호스팅해봤으나 이 방법이 어떤 사용례에선 훨씬 간단해 보인다는 의견임, 쉽고 저렴하며 사용량기반인 프라이빗 레지스트리 서비스가 좀 더 보편화되었으면 하는 바람임
          + zothub.io SSL 인증서가 만료되었음을 알려주는 의견
     * Docker가 처음부터 이렇게 작동했어야 한다고 생각함, 멋진 아이디어라고 느낌
          + 이미지 파일을 아카이브로 저장해서 서버로 전송 후 로드하는 방법으로도 유사한 결과 가능함, 예를 들어 docker save -o my-app.tar my-app:latest로 저장, docker load -i /path/to/my-app.tar로 불러오기, ansible 같이 자동화 툴과 조합하면 Unregistry가 자동화하는 것을 직접 할 수 있음, 다만 save/load 방식은 전체 이미지를 모두 전송해야 하고, 이미지 관리도 아카이브 파일보다 더 편리하다는 단점이 있다고 github repo에서 밝힘
     * 이런 도구와 SSH 툴링을 활용한 셀프호스팅으로의 회귀가 반갑고, 잘 만든 결과물이라 생각함, 직접 사용해 볼 계획임
     * 이 도구 덕분에 uncloud라는 프로젝트를 처음 알게 되었고 내가 원하던 dokku 같은데 더 강력한 서버 배포 솔루션처럼 보여 흥미롭다는 생각임
          + uncloud가 자신에게 잘 맞는다는 피드백에 공감함, 궁금한 점 있으면 Discord로 문의 환영
          + https://skateco.github.io/라는 유사한 접근의 서비스도 있으니 참고 추천
          + Portainer 추천, 포테이너 커뮤니티 에디션과 포테이너 에이전트를 사용해 AWS EC2 두 대에서 잘 운용 중임, 스택 기능(docker compose 기반)이 특히 강점이고, 하나의 EC2에서 portainer agent가 컨테이너로 Caddy를 운영하며 로드밸런서와 리버스 프록시 기능을 수행함
     * 참신한 아이디어임, 다만 이런 방식이 서비스 배포와 강하게 결합되어 배포 및 확장, 예를 들어 레드/그린 배포 시 ""푸시""를 인지하는 추가 로직이 필요함, 생각해보니 이런 역할이 uncloud에서 구현된 구조임을 알게 됨, 하지만 결국 트레이드오프이고, 한 대의 Hetzner VM에 단순함을 중시한다면 로컬에서 이미지 빌드만으로도 충분히 만족할 수 있는 선택임
          + 역시 모든 것에는 트레이드오프가 있고 상황에 맞는 최적의 도구를 선택할 수 있다는 점이 최고라는 의견임
"
"https://news.hada.io/topic?id=21502","부드러운 특이점(The Gentle Singularity) by Sam Altman [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          부드러운 특이점(The Gentle Singularity) by Sam Altman [번역글]

   Sam Altman의 가장 최신 블로그 글을 번역했습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  「The Gentle Singularity」 핵심 요약

    1. 이미 특이점의 초입에 진입
          + 우리는 이미 '사건의 지평선(event horizon)'을 넘었고, 디지털 초지능(digital superintelligence) 구축이 임박한 상태임.
          + 현재까지의 변화는 상상만큼 급격하거나 낯설지 않으며, AI가 일상에 서서히 스며들고 있.
    2. AI의 현재와 미래
          + 아직 로봇이 거리를 활보하거나, 모두가 AI와 하루 종일 대화하는 세상은 아님.
          + 하지만 GPT-4, o3 등 인간보다 뛰어난 시스템이 등장했고, 생산성은 크게 향상됨.
          + 앞으로는 AI가 과학 발전과 생산성 향상을 가속화해 삶의 질을 크게 높일 전망.
    3. AI 발전의 단계별 전망
          + 2025년: 실제 인지적 작업을 수행하는 에이전트 등장.
          + 2026년: 스스로 새로운 통찰을 발견하는 시스템 출현 예상.
          + 2027년: 실제 현실에서 다양한 작업을 해내는 로봇 등장 예상.
          + 2030년: 한 사람이 2020년에 비해 훨씬 더 많은 일을 해낼 수 있는 사회 도래.
    4. 미래 사회의 변화와 인간의 일상
          + 2030년대에도 인간의 본질적 삶(가족, 창의성, 여가 등)은 크게 변하지 않을 수 있음.
          + 그러나 지능과 에너지의 풍요로움으로 인해, 이전과는 전혀 다른 사회가 펼쳐질 가능성 큼.
    5. AI의 자기개선과 경제적 가치
          + AI가 스스로를 개선하는 ‘재귀적 자기개선(recursive self-improvement)’의 초기 단계 진입.
          + AI가 창출하는 경제적 가치로 인해, 인프라(데이터센터 등) 확장 속도도 빨라지고 있음.
          + 로봇이 로봇을 만들고, 데이터센터가 데이터센터를 짓는 자동화 시대가 도래할 것.
    6. AI와 일자리, 사회적 변화
          + 기술 발전으로 일자리 변화와 사회적 도전이 예상되지만, 새로운 욕구와 일거리도 계속 생겨남.
          + 인간은 본능적으로 타인과의 관계에 집중하며, AI와는 차별화된 강점을 가짐.
    7. 기술 발전의 가속화와 사회적 적응
          + 앞으로 10년치 연구를 1년, 한 달 만에 해낼 수 있을 정도로 과학 진보의 속도가 빨라질 수 있음.
          + 사회는 변화에 빠르게 적응하며, 점진적 변화가 모여 큰 변화를 만들어낼 것.
    8. AI 거버넌스와 정렬 문제(alignment problem)
          + AI가 인간 사회의 목표와 가치를 제대로 이해하고, 그 방향으로 행동하도록 만드는 정렬 문제 해결이 핵심.
          + 초지능의 접근성을 넓히고, 특정 집단에 집중되지 않도록 하는 것도 중요.
          + 사회 전체가 집단적 지혜와 의지로 기술의 이점을 극대화하고, 부작용을 최소화해야 함.
    9. 결론: 부드러운 특이점(Gentle Singularity)
          + 특이점은 급격한 폭발이 아니라, 점진적이고 부드럽게 다가오고 있음.
          + 이제는 아이디어만 있으면 누구나 AI를 활용해 무언가를 실현할 수 있는 시대가 열리고 있음.
          + 오픈AI는 초지능 연구 기업으로, 앞으로의 발전에 깊은 기대와 책임감을 느낌.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    요약 포인트

     * AI 특이점은 이미 시작, 변화는 점진적임
     * AI 발전이 생산성과 과학 진보를 가속
     * 2025~2030년대, 사회·일자리·기술 전반에 큰 변화
     * 인간의 본질적 삶은 유지되나, 지능과 에너지의 한계는 사라짐
     * 정렬 문제와 거버넌스가 핵심 과제
     * 부드럽고, 누구나 활용 가능한 초지능의 시대 도래
"
"https://news.hada.io/topic?id=21496","AI 어시스턴트를 이용한 에세이 작성 시 인지적 부채의 축적","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   AI 어시스턴트를 이용한 에세이 작성 시 인지적 부채의 축적

     * 본 연구는 LLM(대형 언어 모델) 사용이 에세이 작성 과정에서 인간의 인지적 비용에 미치는 영향을 분석함
     * 참가자들은 LLM 그룹, 검색엔진 그룹, 브레인(Brain-only) 그룹으로 나뉘어 도구 사용 여부에 따라 에세이 작성 실험에 참여함
     * EEG(뇌파) 분석 결과, LLM 사용 시 신경망 연결성과 인지적 몰입도가 가장 낮았으며 브레인 그룹은 가장 높았음
     * 에세이 작성 후 소유감(ownership) , 인용 능력, 기억 회상에서 LLM 그룹이 가장 저조한 결과를 보였음
     * LLM 사용이 초기에는 효율적이지만 장기적으로 학습 및 인지능력 저하를 초래할 수 있음을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

초록

   오늘날 ChatGPT와 같은 대형 언어 모델(LLM) 제품의 광범위한 도입으로 인해, 개인과 기업 모두 일상적으로 LLM을 활용하는 중임. 이러한 도구는 특유의 장점과 한계를 함께 지니고 있음. 본 연구는 LLM을 에세이 작성이라는 교육적 맥락에서 사용할 때의 인지적 비용, 즉 LLM 활용이 학습자 인지구조와 뇌 활성도에 미치는 영향을 규명하는 데 초점을 둠.

   연구를 위해 참가자들은 LLM 그룹, 검색엔진 그룹, 브레인 그룹으로 나뉘어 각 세션마다 해당 도구(또는 무도구)를 활용하여 에세이를 작성함. 전체 54명이 세션 1~3에 참여했고, 이 중 18명이 세션 4까지 완료함. 세션 4에서는 LLM 그룹은 도구를 사용하지 않고, 브레인 그룹은 LLM을 사용하도록 역할을 바꾸었음(LLM-to-Brain, Brain-to-LLM). 에세이 작성 중 참가자의 EEG(뇌파) 신호를 기록해 인지적 몰입, 부하, 신경 연결성을 분석했으며, 각 세션 후 인터뷰와 NLP(자연어 처리) 분석, 그리고 인간 교사와 AI 판정 에이전트의 채점도 수행함.

   분석 결과, Named Entities Recognition(NER), n-그램, 주제 온톨로지에서 각 그룹 내 높은 동질성이 확인됨. 뇌파 분석 결과 각 그룹별로 신경 연결 패턴이 현저히 달랐으며, 외부 도구 지원이 많아질수록 뇌 연결망의 규모와 몰입이 줄어듦(브레인 > 검색엔진 > LLM 순). 세션 4에서 LLM-to-Brain 참가자들은 약화된 뇌 연결성과 낮은 알파·베타 네트워크 활성, 그리고 낮은 주제 소유감을 보였음. 반대로 Brain-to-LLM 참가자들은 기억 회상 능력 향상, 시각 처리와 관련된 뇌영역 재활성화가 관찰됨. LLM 그룹의 에세이는 소유감, 인용 능력, 기억 회상 모두에서 저조했고, 검색엔진 그룹은 다소 개선되었으나 브레인 그룹보다 낮았음.

   결과적으로 LLM 사용은 단기적 생산성 증진 효과는 있으나, 수개월간 반복시 뇌 행동, 언어적 성취, 점수 모두에서 브레인 그룹보다 지속적으로 열세를 보였음. 본 연구는 AI 도구의 과도한 사용이 학습 현장에서 인지적·실천적 저하를 초래할 수 있음을 시사하며, 장기적 학습설계에 주의가 필요함을 제안함.

실험 주요 결과 요약

     * 세션 4에서 Brain-to-LLM 참가자들은 세션 1~3 LLM 그룹보다 더 높은 뇌 연결성(알파·베타·세타·델타 등 전체 밴드)이 나타남. 이는 AI 도움 없이 스스로 쓴 경험이 있는 후에 AI를 활용하면 더 폭넓은 뇌 네트워크가 활성화됨을 시사함
     * LLM-to-Brain 참가자들은 LLM 사용 이력이 있어도 도구 없이 작성 시 대부분의 뇌파에서 신경학적 비협조(연결성 저하) 현상과 LLM 특화 어휘 편향이 관찰됨
     * AI와 인간 모두의 채점 결과 LLM 그룹의 에세이는 NER/n-gram 다양성이 낮고 구조적으로 동질적임
     * 주제별 분석에서 LLM 그룹과 브레인 그룹 사이의 특정 주제(HAPPINESS, PHILANTHROPY) 에서는 의미있는 차별적 사용 패턴이 보임
     * 그룹별 OWNERSHIP(소유감) 및 인용 능력은 브레인 > 검색엔진 > LLM 순임

논문 목차 안내

     * 빠른 개요: Discussion, Conclusion
     * 에세이 텍스트 NLP 분석: NLP ANALYSIS
     * 뇌파 데이터 이해: EEG ANALYSIS
     * 주제별 심층 분석: TOPICS ANALYSIS
     * 실험 세부 방법·참여자 활동: EXPERIMENTAL DESIGN
     * 부록: 추가 데이터, EEG dDTF 값 등

서론

   대형 언어 모델의 급격한 확산은 업무, 오락, 학습 등 일상적 측면을 근본적으로 변화시켰음. LLM은 학습경험 맞춤화, 즉각적 피드백, 교육자료 민주화 측면에서 교육 분야에 큰 잠재력을 지님. 실제로 학습자 자율성, 몰입도 향상, 개별화된 학습 스타일 지원 등 긍정적 효과가 보고됨.

   하지만 광범위한 LLM 사용에 대한 인지적 부작용도 동시에 제기됨. 즉각적 인지 부하를 줄여주는 장점을 갖는 반면, 비판적 사고력 저하, 심층 분석 능력 약화, 몰입도 감소 등이 보고됨. 특히 AI에 의존할수록 뇌의 분석·판단 능력이 퇴화할 수 있음. 연구에서는 특히 젊은 세대일수록 AI 도구에 대한 의존성이 크고, 이에 따라 인지적 수행 점수도 낮아지는 경향을 주목함.

   또한 AI와의 상호작용이 개인의 독립적 문제 해결 및 비판적 사고 기회를 감소시킨다는 결과도 나타나, 장기적으로 인간 지적 발달·자율성에 부정적 영향을 미칠 수 있다는 우려가 제기됨. 전통적 검색엔진과 달리 LLM은 다양한 관점 제공보다 단일화된 답변을 생성하므로, 사용자의 적극적 정보 탐색보다는 수동적 소비로 전환시키는 경향이 있으며, 정보처리 및 평가 방식에도 장기적 영향이 예상됨.

   본 연구는 LLM을 활용한 에세이 작성의 인지적 비용을 실증적으로 측정함. 에세이 쓰기는 작문, 정보 조직, 인용, 비판적 사고 등 여러 인지과정이 복합적으로 요구되는 과제로, 교육 현장 및 표준화 평가에서도 자주 활용됨. LLM 등 AI 도구의 교육 현장 도입 시 장기적 인지적 영향을 신중히 고려해야 함을 본 연구가 보여줌.

실험 설계 및 일부 세부 내용

     * 각 세션은 그룹별로 참가자를 재배치하거나 도구 사용 조건을 바꾸어 LLM, 검색엔진, 브레인(무도구) 조건을 비교 측정함
          + 세션 4에서는 Brain-only 그룹이 LLM을 처음 활용(Braind-to-LLM), LLM 그룹이 도구 없이 작성(LLM-to-Brain)
     * 실험 중 뇌파와 NLP 지표, 에세이의 소유감, 인용성, 주제별 다양성 등을 체계적으로 평가함
          + 뇌파 분석은 신경 네트워크 연결성(dDTF 분석) 등 기능적 뇌 연결 변화에 초점

실험 및 분석 결과 주요 특징

     * LLM 사용 시 뇌파 및 언어적 다양성이 현저히 감소하고, 작업 소유감·기억 회상력·인용 능력 모두 저하 양상
     * Brain-only 그룹은 전반적으로 뇌 연결망 활성, 언어적 다양성, 소유감, 인용성에서 모두 우수한 결과를 보임
     * 세션 4에서 Brain-to-LLM 참가자는 도구 도입 전과 달리 기억 회상 증가, 시각-전두엽 영역 재활성화 등 뇌 신경망 변화가 강하게 관찰됨
     * 전반적으로 AI 의존이 높아질수록 학습 과정에서의 인지적 효율성 저하 및 능동성 상실 우려가 제기됨

결론적 의미

   본 논문은 AI 학습 도구가 단기적으로 효율성은 높아도, 장기적으로 인지력과 학습 동기, 소유감, 기억력 등 학습의 핵심 요소에 부정적 영향을 줄 수 있음을 다층적 데이터로 입증함. AI 및 LLM 등 첨단 교육기술의 설계·도입시 이러한 인지적 부채와 학습질 저하에 대해 신중한 고려와 추가 연구가 필수적임을 시사함.

        Hacker News 의견

     * 나는 “cognitive debt”라는 표현보다는 인지 감퇴나 인지 능력 상실이 더 적합한 설명이라고 생각함, 뇌는 필요 없는 정보를 저장하지 않음, 예전에 발표된 구글 지도 사용과 관련된 연구들을 보면 GPS를 자주 사용하면 공간 기억력이 떨어지고, 실제로 지도 사용자들의 뇌 회색질 양이 줄어든다는 결과가 있음, 과학 분야에서 전문성을 쌓아본 누구나 알듯이, 어떤 개념을 진정으로 이해하기 위해서는 그것에 대해 충분히 고민해보고 여러 아이디어 간의 연관성을 깊이 탐구해야 함, 수학책을 슬쩍 읽는다고 해서 수학을 쉽게 습득할 수 없음, 반드시 멈춰서 깊게 생각해야 함, 나는 사고라는 행위 자체가 마음 안에 개념을 확립해서 시간이 지난 후에도 유용하게 만드는 과정이라고 생각함
          + 수학책을 대충 읽고 수학을 알게 되는 건 아님, 반드시 멈춰서 생각해야 함, 그리고 가장 중요한 것은 반드시 “글쓰기”임, 글쓰기를 통해 내 뇌가 생각을 구조화할 수 있음, 글쓰기는 나와 내 자신 사이에 구조화된 대화를 만드는 도구임, 다양한 길을 탐색 가능하게 해줌, 생각만으로는 한계가 있지만, 글쓰기는 거의 한계 없이 내 생각을 탐색할 수 있게 해줌, 생각이라는 행위가 글쓰기와 밀접하게 연관되어 있고 (글, 그림, 수식, 그래프 등 어떤 형태로든), 이제 LLM이 점점 글을 대신 써주는 시대라 그 영향이 내 인지 능력에 어떻게 반영될지 궁금함
          + 나는 cognitive debt라는 용어가 정확하다고 생각함, LLM으로 큰 보고서를 작성해본 적 있음? LLM이 구조 잡아주고 차트·주장들까지 쉽게 만들어주다 보면, 점점 내 것이 아닌 결과물이 됨, 내 이름으로 제출해도 설명을 요구받았을 때 애매해지는 경우가 많음, 보통 내 머릿속엔 더 높은 차원의 진짜 이해가 있지만 LLM을 쓰면 그 과정이 생략됨, 실제로 핵심 개념을 설명하려 할 때 애를 먹음, 결국 내가 직접 본질적인 개념을 머릿속에서 만들어 반복해서 바꿔가며 각기 다른 청중에게 전달하려는 과정을 실제로 경험해야 함, cognitive debt는 LLM 이전에는 반드시 세웠던 머릿속 모델의 차이와 LLM을 사용할 때의 얕음을 나타냄, 결국 보고서에 내 이름이 들어가는데, 시간이 지나면 사람들은 저자에 대한 기대치가 점점 낮아지거나 아예 LLM이 대신 설명해주기를
            기대할 위험도 있음, LLM마다 서로 다른 내적 모델·알고리즘으로 현실을 흉내내는데, 가장 정확한 예측을 위해서는 충분한 '이해의 깊이'가 필요함, LLM의존적 글쓰기는 이 깊이를 만들어주지 못함, 장기적으로는 전체 인구의 인지 감퇴나 스킬 상실로 이어질 수 있음, 인쇄술이 등장했을 때도 당시 종교 엘리트들은 평범한 사람들이 제대로 읽고 해석할 수 없을까봐 걱정했었지만 실제로는 그렇지 않았음, 쓰기가 곧 생각이 맞고(아직 글쓰기보다 좋은 도구는 못 찾았다고 생각), 사고란 미래를 더 잘 예측할 수 있도록 정보를 기반으로 마음속 모델을 구축하는 과정임, 우리의 생존 자체가 여기에 달려있음, 정보 이론적 관점에서 “생물학은 정보의 빛에서 설명될 때만 의미 있음” YouTube 링크
          + “뇌는 필요 없는 정보를 저장하지 않는다”는 말에 대해, 20년 넘게 쓰지 않은 DOS의 config.sys와 autoexec.bat으로 메모리 최적화하는 법을 아직도 기억하는 이유가 뭔지 궁금함, 앞으로도 이 기술을 다시 쓸 일은 없을 것 같음
          + “cognitive decline” 또는 “brain rot” 같은 용어는 너무 자극적으로 들릴 수 있고 해당 논문에서도 샘플 수가 적다는 한계를 명시함, 논문에서는 “cognitive debt” 용어에 대한 근거나 인용이 없어서 제목이 이상하게 느껴짐, 마지막에 제목만 바꾼 듯한 인상임, MIT의 흥미로운 연구 결과임, 모든 심리학 연구처럼 건강한 의심과 독립적 검증이 필요함, 뇌영상과 심리 계측법을 다 접목한 종합 패키지 느낌, ‘이것이 LLM을 쓴 당신의 뇌입니다'라는 그림 대부분이 재밌다고 생각함
          + “뇌는 필요 없는 정보를 저장하지 않는다”는 말을 들으면 그럴듯한데, 한 번 배우면 평생 잊지 않는 자전거 타기처럼 어떤 기술들은 사라지지 않는 현상도 분명히 존재함이 궁금함
     * “cognitive debt”에 대한 논의가 적절한데, 좀 더 확장적인 시각도 필요하다고 생각함, 단순히 언어나 공간 기억력 같은 스킬을 잠시 잃거나 깜빡하는 차원이 아님, 아예 통합적 추론을 담당하는 신경 회로가 조직적으로, 그리고 되돌릴 수 없게 위축되는 현상을 말함, “부채(debt)”란 용어는 연습을 통해 상환(회복)할 수 있다는 뉘앙스가 있는데, 진짜 위험은 “cognitive tipping point(인지적 임계점)”를 넘는 순간임, 지나치게 많은 실행 기능, 통합, 논증의 부담을 외부 시스템(LLM)에 맡기면, 뇌는 쓰지 않는 회로는 가차 없이 정리(가지치기)할 뿐 아니라 다시 그것을 복구하는 “재건 능력” 자체를 잃는 위험이 있음, 인간의 뇌는 버전 관리가 없는 “use-it-or-lose-it 시스템”임, 한 번 복잡한 인지 능력이 상실되면 “소스코드” 자체가 망가지는 셈임, collapsed neural
       network는 git revert할 수도 없음, 이 HN 댓글들은 에세이 작성에 초점을 맞추지만, 사실 전체 사회가 지적 능력을 외부화하는 대규모, 통제 불능의 실험을 실행 중임, 장기적으로는 사회 전체가 스킬이 없는 것을 넘어 아예 “그런 사고방식 자체가 불가능한” 구조적 한계에 처할 위험이 있음, 그래서 진짜 질문은 “인지 부채를 어떻게 피할 것이냐” 그 이상임, “생물학적 뇌가 자기 최적화하면서 게으름에 치명적으로 최적화될 때 우리의 마음을 담을 새로운 컨테이너가 필요하지 않은가”라는 두려움임 관련 링크
          + LLM을 무엇에 쓸지 결정하는 것은 각자 몫임, 온라인에서 검색 효율이 떨어지는 어려운 문제, 귀찮고 산만한 포럼이나 소셜 미디어에서 해답을 찾는 것엔 LLM이 매우 유용한 도구임, 어차피 그 정보의 진위 여부는 따로 검증해야 하고, StackExchange가 초기에 의도한 대로만 쓰였으면 훨씬 더 가치 있었겠지만 사람들은 편견과 인지적 노이즈로 가득함, LLM은 질문이 Upvote 받은 뒤, “질문이 너무 넓다”며 바로 닫는 일은 없음, 그러나 내가 이미 잘 아는 주제에 대해서는 LLM의 결과가 여전히 한참 부족하다고 느낌, 이메일을 LLM 도움으로 작성해도 프롬프트를 계속 조정하거나 결과를 다시 쓰느라 시간이 결국 똑같이 듦, 결국 내가 직접 내 식으로 쓰는 게 더 편하고, LLM이 쓴 글을 교정하거나 검토하는 역할은 비효율적임
     * AI는 Zettelkasten(연결형 기록 시스템)의 반대임, 인간이 직접 주제에 깊이 파고들어 더 깊은 통찰을 얻는 대신, AI 기반 산출물 위에서 빠르지만 얕은 반복만 일어남, 예를 들어, 중동 정세를 이해하고 싶어서 OpenAI와 공동으로 Hammas, Hizbulah의 기원에 대한 10페이지 에세이를 썼는데, 아무것도 기억에 남지 않았고, 심지어 남아있는 몇몇 내용도 내가 고친 AI 환각인지 진짜 팩트인지조차 모르겠었음
          + LLM의 유용성에 다소 낙관적인 편이지만, 위에 공감함, LLM을 잘 다루는 본능은 기를 수 있지만, 실제로 설명 가능한 지식이나 도전적 사고력을 키우는 건 아님, 오히려 특정 출력 패턴에 대한 “근육 기억”이나 프롬프트 조정, 컨텍스트 조절에 익숙해지는 게 핵심임, 이런 식의 “스킬”은 모델이 더 좋아지면 금방 사라질 것으로 판단함, 이런 상황은 어느 정도 조립라인 작업자들이 느끼는 무기력함과도 비슷함
          + 사람이 직접 손으로 고친 부분이 더 기억에 남는 경향이 있음, 아무 문제 없이 넘어간 부분보다 직접 손 봤던 부분이 더 기억에 남음
          + 대부분의 똑똑한 사람들은 글쓰기가 단순히 결과물을 얻기 위해서가 아니라 “생각”하는 과정임을 잘 알고 있음, LLM은 이러한 과정에서 내게 실수를 지적하거나, 빈틈·오류를 찾아주거나, 세상을 이해하는 데 있어 일반적인 조사를 도와주는 좋은 스파링 파트너임, 단, LLM이 글을 “대신 써주는” 용도가 아니라 스스로 사고하는 과정에 보조적으로 쓰일 때 그렇다고 생각함, 그러나 결과물은 항상 소스를 반드시 검증해야 함
     * 개인적으로 이번 연구 결과가 의외가 아님, AI로 내 글쓰기나 번역 작업을 도왔을 때는 스스로 적극적으로 몰입하거나 사고에 깊게 관여한다는 느낌이 적었음, 하지만 전혀 다른 방식으로 AI를 쓸 때는 매우 몰입감이 높아지는 걸 느낌, 2주간 Claude Code로 아이디어 구상, 조사, 에세이/논문 자동화에 도전해봤는데 그 과정도 내내 “진짜” 글쓰기만큼이나 (다만 성격이 다르지만) 정신이 몰입됨, 내가 실험한 결과물도 꽤 괜찮았음, AI가 쓴 에세이나 논문이라고 해도 직접 읽어보면 충분히 흥미로움, 다만 공개하거나 논문처럼 제출할 생각은 없음
          + 나는 AI를 단순히 재미나 잡담용으로만 쓰고, 실제 작업용으로는 거의 쓰지 않음, 이런 방식이 점점 희귀한 능력처럼 남게 될까, 반대로 주변은 점차 아무 것도 못 하게 되지 않을지 궁금함
     * 이 현상을 “cognitive offloading(인지 외주화)”라고 부름, 코딩 어시스턴트와 충분히 오래 일해본 사람들은 다들 공감할 현상임
          + 엔지니어링 매니저로 일할 때도 마찬가지로 느끼는 부분임, 추상화 수준이 달라지면 필연적인 결과임, 내 어셈블리 실력도 그만큼 무뎌짐, 하지만 세상의 끝은 아님
     * 기계화의 발달이 인간 산업계에 가져온 효과와 비슷하게, LLM 등 신기술이 우리 인지구조에 가져오는 파장을 넓게 생각해볼 지점임, 자동화는 반복적이고 지루한 일을 맡기는 대신 인간이 더 창조적/혁신적인 것으로 나아갈 에너지와 시간을 주는 일이라고 생각함, 궁금한 점은 LLM, GPS 등 도구 사용 증가로 단기적 변화뿐만 아니라 장기적으로 사고방식 자체도 바꿀 수 있는지임, 검색엔진에 익숙하게 자란 세대는 암기력은 줄지만 정보를 “찾는 방법”을 기억하는 방식으로 적응했음, 이렇게 자연스레 기존 기능이 대체되는 발전의 연속일지, 아니면 LLM 의존도가 높아지는 것이 도저히 대체할 수 없는 핵심 스킬의 상실을 가져올지도 생각하게 됨
     * 아래 인용문에서 “LLM이 질문 답변의 진입장벽을 낮춰줬지만, 그 편리성의 대가로 사용자가 LLM의 답변이나 ‘의견’ (사실은 훈련데이터에 근거한 확률적 추정)에 비판적으로 접근하려는 경향이 줄어듦”이라고 지적함, 결국 ‘에코 챔버’ 현상이 사라진 게 아니라 오히려 알고리즘이 순위매긴 “우선 노출된” 내용으로 사용자 노출이 구조적으로 재편됨, 그 “우선순위” 자체도 LLM 소유주(주주)의 가치관을 반영함
          + “최상위로 노출되는 내용이 결국 LLM 소유주의 가치에 따라 결정된다”는 점이 새삼스럽진 않음, 인쇄기 이전에도 “언론의 자유란, 인쇄기를 소유할 권리”라는 격언이 있었음, 그리고 “LLM의 편리함이 사용자의 비판적 평가 태도를 약화시킴”이라는 지적은 플라톤이 문자/문서가 인간 정신을 둔감하게 만든다고 걱정했던 일화(아마 소크라테스의 입을 빌려서?)가 떠오름
     * 최근 계속 생각해오던 점이기도 하고, 그래서 copilot을 잠깐만 써봤음, 커리어 초반이고 매일 배우고 있음 – LLM 보조를 쓰면 일을 더 빠르게 끝낼 수 있지만, 그러면 스킬을 쉽게 익힐 기회를 잃을 것 같음, “저수준의 비판적 사고력은 점차 의미 없어지고 앞으로는 고수준/추상계획만 필요하다”는 주장엔 동의할 수 없음, 감성적으로도 “내가 뭘 알고 직접 할 수 있다”는 데 자부심과 의미를 느낌, LLM 활용이 어렵다고 보지 않고, 필요하면 최신 도구를 골라 쓰면 되겠지만, 당분간은 스스로 배우며 성장하는 과정이 더 소중하다고 생각함
     * AI 도입으로 초급 인력 자체가 줄어드는 데 그치지 않고, 남아 있는 초급 인력조차도 AI를 쓰다 보면 아무 것도 못 배우고 영원히 초급에 머무를 위험이 있다고 봄
     * AI와 함께 쓰면 그 순간엔 매끄럽게 잘 써지는 것 같지만, 아이디어를 진지하게 고민하지 않고 있는 느낌임, 완성된 글도 말은 번지르르하지만 지나고 나면 왜 그 문장을 썼는지 기억나지 않는 경우가 많음, 지금은 먼저 내 초안을 직접 써보고, 그다음에 AI로 다듬는 방식을 씀, 수고가 조금 더 들긴 하지만 확실히 더 많이 배우고 오래 기억에 남음
          + “LLM은 글을 늘릴 때보다 줄일 때 훨씬 잘 동작한다”는 경험칙이 굉장히 유용함
"
"https://news.hada.io/topic?id=21520","효과적인 AI 에이전트 구축","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            효과적인 AI 에이전트 구축

     * LLM 기반 에이전트를 성공적으로 구현한 팀들은 복잡한 프레임워크 대신 단순하고 조합 가능한 패턴을 사용하는 경향을 보임
     * 워크플로우와 에이전트의 구조적 차이점을 이해해야 하며, 최적의 솔루션을 위해 항상 최소한의 복잡성만을 도입하는 방식이 추천됨
     * 다양한 에이전트 패턴(프롬프트 체이닝, 라우팅, 병렬화, 오케스트레이터-워커스, 평가-최적화 루프 등)이 실제 생산 환경에서 활용되며, 각 패턴에 맞는 사용 사례와 장단점이 존재함
     * 에이전트 구현 시 심플함, 투명성, 인터페이스 설계가 핵심 원칙이며, 도구 설계와 프롬프트 엔지니어링에 신경써야 함
     * 고객 지원이나 소프트웨어 개발 환경에서 에이전트가 실제로 가치를 제공하는 사례가 점차 늘어나고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   지난 1년간 Anthropic는 다양한 산업 분야의 팀들과 함께 대형 언어 모델(LLM) 기반 에이전트를 구축해 왔음. 실질적으로 성공적인 에이전트 도입 사례들은 복잡한 프레임워크나 특화 라이브러리보다는 단순하고 조합 가능한 패턴에 기반을 두는 경향을 보였음. 본 포스트는 고객과의 협업 및 자체 개발 경험에서 얻은 인사이트와, 효과적 에이전트 구축을 위한 실질적 조언을 공유함.

에이전트란 무엇인가

   에이전트는 다양한 방식으로 정의될 수 있음
     * 일부는 완전 자율 시스템으로, 외부 도구를 이용하여 복잡한 태스크를 독립적으로 완수하는 형태로 정의함
     * 일부는 제한된 워크플로우나 미리 정해진 프로세스를 따르는 처방적 구현으로 이해함

   Anthropic는 이 둘 모두를 에이전틱 시스템으로 분류하지만, 워크플로우와 에이전트 간의 중요한 아키텍처적 차이를 둠
     * 워크플로우: LLM과 도구가 미리 정의된 코드 경로로 오케스트레이션되는 구조
     * 에이전트: LLM이 도구 활용과 프로세스를 스스로 동적으로 결정하며, 과업 달성 방식에 대한 제어권을 가짐

   이 포스트에서는 두 형태의 시스템에 대해 자세히 설명하며, 실무 적용 사례는 부록 1에서 다룸

언제(그리고 언제 아닌가) 에이전트를 활용해야 하는가

   애플리케이션 개발 시 최소한의 복잡성을 원칙으로 하여, 필요할 때만 점진적으로 복잡성을 추가함이 바람직함
     * 에이전틱 시스템은 지연 시간/비용을 일부 희생하더라도 과업 성능 향상을 도모할 수 있음
     * 복잡성이 꼭 필요한 경우가 아니라면 단일 LLM 호출과 예시 삽입, 검색 연동 등으로 충분히 해결되는 경우가 많음
     * 워크플로우는 예측 가능성과 일관성이 중요한 곳에, 에이전트는 유연성과 스케일이 필요한 곳에 적합함

프레임워크 사용 시점 및 방법

   에이전틱 시스템 구현을 쉽게 해주는 다양한 프레임워크가 존재함
     * LangGraph(LangChain)
     * Amazon Bedrock AI Agent framework
     * Rivet, Vellum 등

   이들 프레임워크는 LLM 호출, 도구 정의/파싱, 호출 체인 구성 등 저수준 작업을 간소화해줌. 하지만 과도한 추상화로 인해 실제 프롬프트·응답 흐름이 불투명해지거나, 불필요하게 복잡성이 늘어날 위험이 있음
     * 개발자는 최대한 LLM API 직접 사용부터 시작할 것을 권고함
     * 프레임워크를 사용하더라도 내부 동작 방식을 정확히 이해해야 함

   예시 구현은 anthropic-cookbook에서 확인 가능함

빌딩 블록, 워크플로우, 에이전트 패턴

   Anthropic는 실제 운영 환경에서 자주 활용되는 에이전틱 시스템 패턴들을 소개함

  빌딩 블록: 증강 LLM

   에이전틱 시스템의 핵심은 검색, 도구, 메모리 등의 기능이 증강된 LLM임
     * 모델이 자체적으로 검색 쿼리 생성, 적절한 도구 선택 및 정보 저장을 수행 가능
     * 핵심 구현 시 고려점은, 사용 사례에 맞게 기능을 맞춤화하고 LLM에 명확하고 문서화된 인터페이스를 제공하는 것임

   최근 공개된 Model Context Protocol을 통해 다양한 써드파티 도구와 간단히 통합 가능

  워크플로우 유형별 설명

    프롬프트 체이닝

     * 태스크를 일련의 단계로 분해, 각 LLM 호출이 앞선 결과를 이어받아 처리함
     * 각 단계에 프로그램적 검증(게이트) 추가로 프로세스 관리 가능
     * 적용 예시: 마케팅 문구 생성 후 번역, 문서 개요 설계 → 검증 → 내용 작성 등

    라우팅

     * 입력을 분류해 맞춤형 후속 태스크로 분기시킴
     * 각 카테고리별로 특화된 프롬프트 및 도구 활용 가능
     * 적용 예시: 고객 문의 분기(환불, 기술 지원 등), 난이도별 모델 선택 등

    병렬화

     * 태스크를 소단위로 분할, 병렬로 처리 후 결과 집계
     * Sectioning: 각 부분을 독립적으로 처리
     * Voting: 동일 태스크를 여러 관점·프롬프트로 반복, 다수결 등 활용
     * 적용 예시: 사용자 질문 필터링·응답 분리, 자동 평가, 코드 검토 등

    오케스트레이터-워커스

     * 중앙 LLM이 태스크를 동적으로 분해·할당, 여러 워커 LLM이 각각 처리 후 결합
     * 미리 정의되지 않은, 입력에 따라 가변적인 서브태스크에 적합
     * 적용 예시: 복수 파일 변경 코딩, 복잡한 정보 탐색 등

    평가-최적화 루프

     * 응답 LLM과 평가 LLM을 반복 루프로 활용
     * 명확한 평가 기준 및 피드백 기반 개선이 가치 있는 상황에 적합
     * 적용 예시: 문학 번역에서 미묘한 뉘앙스 평가, 다수 라운드 정보탐색 등

  에이전트

     * LLM 발전과 함께 실서비스에서 복잡한 입력, 추론·계획, 도구 사용, 에러 회복이 가능한 에이전트가 등장
     * 사용자 명령/대화로 시작 → 태스크 명확화 후 자율적 실행 → 중간 체크포인트에서 피드백 가능 → 완료 또는 중단 조건에서 종료
     * 실제 구현은 LLM이 환경 피드백(도구 결과, 코드 실행)을 참고해 반복하며, 도구 세트와 문서화가 핵심
     * 적용 예시: SWE-bench 작업 해결을 위한 코딩 에이전트, Claude 기반 컴퓨터 사용 자동화
     * 적용 범위: 정해진 경로나 단계 예측이 불가능한 오픈엔드 문제, 의사결정 신뢰 필요 상황
     * 자율성 증가로 인한 비용·복합 에러 가능성 고려 필요, 샌드박스 테스트와 가드레일 필수

  패턴 조합과 커스터마이즈

     * 소개한 빌딩 블록들은 정형화된 규칙이 아니라 다양한 상황에 맞춰 조합 가능함
     * 중요한 건 성과 측정/반복 개선을 통한 최적의 구조 선택 및 점진적 복잡성 추가

요약 및 권장 원칙

   LLM 시스템의 성공은 복잡성이나 신기술이 아니라, 목적에 맞는 정확한 접근을 찾는 것임
     * 간단한 프롬프트로 시작, 성과 평가와 반복 최적화, 단계적 복잡성 확장
     * 에이전트 설계에서의 3대 원칙
         1. 심플함 유지
         2. 투명성(기획 단계 명확화) 우선
         3. 도구/인터페이스 문서화·테스트 중시
     * 프레임워크로 빠른 시작 가능하지만, 실제로는 추상화 레이어 최소화와 직접 구현 역량이 신뢰성·유지보수를 좌우함

부록 1: 실무에서의 에이전트 적용 사례

  고객 지원

   고객 지원 분야는 챗봇 인터페이스와 툴 연동이 결합되어, 에이전트 적용에 자연스럽게 적합함
     * 대화 기반 인터페이스와 외부 데이터·업무 처리 필요성이 공존
     * 도구로 고객 정보, 주문 내역, 지식베이스 등 연동 가능
     * 환불/티켓 처리 등 작업을 자동화
     * 해결 기준을 명확히 설정 가능

   성공적으로 적용된 사례들은 사용량(성공 해결 기준) 기반 과금 모델로 에이전트 효과성을 검증함

  코딩 에이전트

   소프트웨어 개발 환경에서도 문제 자동 해결 등 에이전트 활용도가 크게 향상됨
     * 코드는 자동화된 테스트를 통해 결과 검증 가능
     * 테스트 결과를 활용한 반복 개선 가능
     * 문제 정의가 명확하고, 산출물 품질도 객관 측정 가능

   Anthropic 자체 구현 사례: SWE-bench Verified 벤치마크에서 실제 GitHub 이슈를 pull request 설명만으로 해결. 자동화 테스트 외에도 인간 검토는 시스템 전체요구 사항 부합 여부 확인에 여전히 중요

부록 2: 도구 프롬프트 엔지니어링 방법

   모든 에이전틱 시스템에서 도구는 핵심 요소임
     * Claude 등 LLM이 API에서 정확한 구조·정의에 맞춰 외부 서비스와 상호작용 가능
     * 응답에 tool use block 포함 가능
     * 도구 정의·사양 역시 프롬프트 엔지니어링 만큼 세밀하게 설계 필요

  도구 포맷 설계 팁

     * 모델이 작성 ‘함정’에 빠지지 않도록 충분한 토큰 확보
     * 인터넷에서 많이 접한 자연스러운 포맷 사용 권장
     * 불필요한 포맷팅 오버헤드(예: 코드 줄수 카운트, 문자열 이스케이프 등) 최소화
     * 인간-컴퓨터 인터페이스(HCI)를 설계할 때 투자하는 만큼 에이전트-컴퓨터 인터페이스(ACI) 에도 정성을 들여야 함
     * 모델 입장에서 도구를 이해·사용하는 것이 ‘명확’해야 하며, 사용 예시, 경계조건, 입력 포맷 명시 등도 포함
     * 파라미터 이름, 설명도 직관적인 용어로 바꿔 설명서(도크스트링) 작성하듯 설계
     * 다양한 입력값으로 실제 사용 방식 테스트 및 반복 개선
     * 실수 발생을 줄일 수 있도록(Poka-yoke) 아규먼트 설계

   실제 SWE-bench 에이전트 구축 시, 전체 프롬프트보다 도구 설계 최적화에 더 많은 시간 투자. 예시: 루트 폴더 이탈 후 파일 경로 실수를 줄이기 위해 절대경로만 받도록 변경 후 완벽하게 동작함.

        Hacker News 의견

     * 이 글은 ""AI agents""에 대한 정의를 명확히 하고 시작한 점이 특히 인상적이라는 의견임. 여기서 사용하는 정의는 ""LLM이 자체적으로 프로세스와 툴 사용을 동적으로 관리하며, 작업 수행 방식을 스스로 통제하는 시스템""임. ‘agent’와 ‘workflow’를 구분하는 부분과 여러 실용적인 워크플로우 패턴을 소개한 방식도 마음에 든다는 입장임. 글이 처음 나왔을 때 직접 작성한 정리 글도 있음 building-effective-agents 노트. 그리고 최근 Anthropic의 multi-agent 연구 시스템 구축기도 흥미로웠고, 여기에 관한 추가 노트도 있음 multi-agent research system 노트
          + 이 글에서 말하는 workflow 정의가 정확하지 않다는 생각임. 요즘 워크플로우 엔진들은 미리 정해진 코드 경로를 따르지 않고 에이전트와 사실상 동일하게 동작하는 경우가 많음. 저자가 워크플로우를 새롭게 정의한 건 구분하려는 의도로 보이지만, 실제로는 agent란 것도 단순히 LLM 응답에 따라 동적으로 호출하는 루프 형태의 워크플로우라는 의견임. 최신 워크플로우 엔진도 매우 동적이라는 주장임
          + Building Effective Agents의 공동 저자 한 명이 AIE에서 해당 글을 주제로 강연도 진행했는데, 반응이 매우 좋았다는 경험 공유임 유튜브 영상
          + multi-agent 연구에 대한 글이 정말 좋은데, Building Effective AI Agents 글에서 ""프레임워크 없이 처음부터 시스템을 구축하는 것이 교육적 관점에서는 좋다""는 주장에는 동의하지 않는다는 의견임. 좋은 프레임워크를 쓰면 다양한(그리고 공급업체를 가리지 않는) LLM을 손쉽게 실험할 수 있다는 점이 첫 번째 이점이라는 주장임
          + Anthropic이 어떤 AI agent framework를 사용하는지 궁금하다는 질문임. 자체 프레임워크를 공개한 적이 없는 것 같다는 의견임
          + 추가 노트가 고맙고, 이 주제가 최근 본인에게도 매우 중요한 이슈라는 반응임
     * AI 분야에서는 반 년이 아주 길게 느껴지는 체감임. 몇 달 전 이 글을 반복해서 읽었는데, 이제 에이전트 개발이 분명히 병목에 다다른 느낌임. 최신 Gemini조차도 오히려 성능이 퇴보한 것 같다는 의견임
          + 여러 에이전트를 동시에 돌리는 게 비용이 많이 들어 RoI를 낮추는 요인임. 예를 들어, 주식용 DeepSearch agent는 6개 agent를 사용하는데 쿼리마다 약 2달러 비용 소모임. 멀티에이전트 오케스트레이션이 컨트롤하기 어렵고, 모델이 더 강력하면 멀티에이전트 필요성이 줄어듦. 반대로 약한 모델일수록 특화된 좁은 AI의 비즈니스 가치가 더 올라간다는 실제 경험 공유임
          + 에이전트가 퇴보했다고 느껴지는 이유가 궁금하다는 질문임. 왜 그냥 스스로 분신을 여러 개 만들어서 24/7로 병렬로 계속 일하고 검증하며 발전 시키지 못하는지에 대한 궁금증임
          + 프롬프트 인젝션 문제를 해결하는 게 아주 어려워서 이 부분이 심각한 병목이 되고 있다는 의견임
     * 에이전트가 어떻게 task queueing, race condition, 그 외 동시성 문제를 다루는지 궁금함. 멀티 에이전트 워크플로우 구축 관련 기사에서는 orchestrator agent가 전체를 관리한다는 내용이 많은데, 실질적으로는 더 복잡한 설계와 똑똑한 glue code가 필요한지 항상 궁금함. 아니면 이 모든 게 진짜로 “자동 마법”처럼 작동하는지에 대한 의문임
          + 에이전트의 표준은 도구들이 순차적으로 실행된다는 것이라서 동시성 문제를 걱정하지 않아도 된다는 설명임. 이제는 여러 모델이 병렬 툴 호출을 지원하고 있어서, 모델이 “이 세 가지 툴을 실행”이라고 요청하면 harness가 병렬 혹은 순차로 실행 후 결과를 다음 스텝에 전달하는 방식을 사용함. Anthropic은 멀티에이전트 설정을 더 적극적으로 활용하고 있는데, 상위 에이전트가 하위 에이전트들에게 병렬로 작업을 위임하는 구조임. 이 트릭은 Claude Code에 적용되고 있고, 관련 역공학 노트(claude-trace) 및 Claude Research 작동 방식에 대한 글(multi-agent-research-system)에서도 확장 설명함. LLM 툴 사용 패턴을 발견하는 단계는 아직 매우 초기이며, 모델이 툴 사용을 정말 잘하게 된 것도 최근 6개월 사이의 일이라 앞으로 발전 가능성이 큼
          + 그래서 모든 걸 JSON으로 다루기보다는 LLM이 직접 code를 만들어서 tool call을 다루는 방향을 선호하게 됨. Huggingface의 smolagents 라이브러리는 LLM이 python 함수 호출 코드를 생성하는 방식을 사용함. 병렬 툴 호출을 원하면 프롬프트에서 명시하면 되고, LLM이 동기화도 처리해야 함. 물론 LLM이 만든 코드를 실행하는 이슈는 있지만 여러 해결책이 존재함
          + Codex 웹 인터페이스 사용 사례 공유임. 한 번에 끝내기엔 너무 긴 리팩토링 플랜이 있었고, “ask” 기능을 통해 여러 작업으로 분리하고 병렬로 가능한 작업을 그룹화함. LLM은 실제 팀이 분담하는 방식과 유사하게 분할했지만, 작업 간 커뮤니케이션 전제가 없다 보니 컨텍스트 소실이 매우 큼. 직접 하는 것보다 더 오래 걸려도 시도는 했지만, 여러 개의 세션에 각 작업마다 상세한 프롬프트(목적, 방법, 검증, 문서화 등)를 주는 방식으로 순차 처리함. 요약하자면 orchestrator agent는 아주 간단한 작업에는 쓸 수 있지만, 생각보다 적용 범위가 한정적이라는 실제 경험 공유임
          + 마법처럼 자동으로 작동하는 것은 아무것도 없다는 입장임. 기존 시스템에서 신경 써야 하는 운영 특성을 AI 에이전트에도 반드시 개발해야함. AI agent 데모만 보고 “스파게티 코드 팀의 코드를 깔끔한 AI 프롬프트 몇 개로 대체”할 수 있다고 생각하면 속을 수 있음. 실제로 소수 케이스에서는 동작할 수 있지만, 결국은 모든 코드가 시스템에서 필요한 이유가 있고, 아예 그 코드를 LLM에 다 번역해 넣는 수준이 되면 방향성을 잃은 것이라는 경고임
          + 코딩 에이전트의 경우 emerging pattern으로 컨테이너를 활용해 작업을 격리하고, git으로 작업물을 리뷰 및 머지하는 방식이 정립 중임. 예시로는 컨테이너, MCP 활용 사례(container-use)가 있으며, 병렬 코드 작업에 유용함. 그 외 업무에서는 n8n, Zapier, CrewAI 같은 워크플로우 빌더도 여전히 자주 사용함
     * 이 글은 “가장 단순한 것”부터 시작하고 진짜 복잡성이 필요할 때만 추가하라는 메시지를 상기시켜줌. 명확하게 정의된 LLM 호출과 약간의 심플한 glue logic만으로 더 안정적이고, 디버깅도 쉽고, 비용도 저렴한 시스템 구축 가능성 언급함. 반대로 화려하게 보이는 에이전트 시스템은 오히려 문제를 더 많이 야기하는 경우가 많음
     * AI가 사람들에게 실질적으로 도움을 주는 존재가 되길 바라는 희망 표현임
     * Anthropic이 이런 기술 정보를 제공하는 것도 좋지만, 비전문가를 위한 쉬운 가이드 버전도 제공해야 한다는 생각임. 예를 들면 마케팅 부서에서 에이전트를 도입하고 싶어도, 기초 수준에서 사양을 정의할 수 있는 가이드가 필요함. 글의 마지막 부분과 부록에서 관련 내용이 있긴 하지만, 여전히 “어떻게 구축할 것인가”는 구현 측면에 머무르고 있다는 지적임
     * (2024년 12월 - 지금 생각하면 한참 전처럼 느껴지는 시점이라는 소회임)
          +

     아아아 이제 다시 머리 써서 2024년 12월에 원시인처럼 100퍼센트 코드를 직접 짜야 하는 상황으로 돌아간다는 농담형 반응임 관련 댓글
          + 이 글이 아주 잘 버티고 있다는 의견임. 개인적으로 계속 참고자료로 쓰고 있으며, 시간이 지나도 구식이라는 느낌이 없음. Anthropic이 ""실질적인 AI 도구 개발 파트너""로서 새로운 인상을 준 글이었다는 평가임
     * Agent에 대한 과대광풍(hype)이 지금은 많이 가라앉았다는 견해임
          + 이제는 모두가 AI Agency에 관심을 갖게 됐다는 변화 진단임
     * 글이 당시에도 토론되었던 이력이 있다는 정보 공유임 원문 HN 토론
     * 이 글이 과장이나 유행을 따르지 않고 현실적으로 접근한 점이 마음에 든다는 의견임. 많은 사람들이 유행 따라 agent 시스템을 만들다가, 실제로 그 작업에 진짜 agent가 필요한지 고민 없이 진행하는 경향에 대해 비판적임
"
"https://news.hada.io/topic?id=21522","뛰어난 성과를 내는 사람들은 왜 ‘주장’을 할까","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       뛰어난 성과를 내는 사람들은 왜 ‘주장’을 할까

  뛰어난 성과를 내는 사람들은 왜 ‘주장’을 할까

    1. 인사이트, 제안, 주장의 차이점
          + 인사이트(Insight): 관찰·분석을 통해 얻은 흥미로운 사실이나 트렌드 공유
            예) “최근에 이런 현상이 있더라.”
          + 제안(Suggestion): 문제 해결을 위한 여러 옵션을 제시
            예) “이렇게 해보는 건 어떨까요?”
          + 주장(Assertion): 구체적이고 명확한 방향성과 행동을 제시하며, 그에 대한 책임을 본인이 짐
            예) “이렇게 해야 합니다. 제가 책임지고 추진하겠습니다.”
    2. 왜 ‘주장’이 중요한가?
          + 인사이트와 제안만으로는 실제 변화나 실행이 일어나지 않음
          + 주장은 행동과 결과에 대한 책임을 동반, 실제로 무언가를 ‘움직이게’ 만듦
          + 조직 내에서 영향력을 가지려면, 단순히 정보 전달이 아니라 본인만의 관점과 실행 의지가 필요
    3. 주장의 3가지 핵심 요소
         1. 실행 중심(Execution-oriented):
               o “그래서 우리는 무엇을 해야 하는가?”에 답함
               o 단순 분석이 아니라, 구체적 실행 계획과 연결됨
         2. 확신이 담긴 관점(Conviction):
               o 본인이 믿고, 설득할 수 있는 의견이어야 함
               o 남의 생각이 아닌, 본인만의 해석과 신념이 중요
         3. 책임감(Ownership):
               o “내가 앞장서겠다”는 태도
               o 결과에 대해 책임질 각오가 필요
    4. 주장은 어떻게 만들어지는가?
          + 다양한 경험, 데이터, 맥락을 바탕으로 한 ‘나만의 해석’이 필요
          + 단순 정보 나열이 아니라, 본인의 가치관과 판단이 녹아 있어야 함
          + 반복적이고 정의된 일은 AI가 더 잘할 수 있지만, ‘주장’은 오직 사람이 할 수 있는 창의적 영역
    5. 불확실성 속에서 주장의 의미
          + 정답이 없는 상황, 과거 사례가 없는 문제일수록 주장하는 사람이 필요
          + 100% 확신이 없어도, 불확실할수록 명확한 주장과 실행 의지가 프로젝트를 전진시킴
          + 단, 자신의 확신 정도(예: 70% 확신)와 책임 범위를 명확히 밝히는 것이 중요
    6. 질문에서 멈추지 말고, 답(주장)을 제시하라
          + 뛰어난 인재는 좋은 질문을 던질 뿐 아니라, 자신의 답(주장)도 함께 제시
          + “내 생각은 이렇다”는 명확한 관점이 논의를 진전시킴
    7. 주장에는 용기가 필요하다
          + 틀릴 위험, 동료/상사의 반대 등 부담이 있음
          + 그러나 주장은 팀과 조직의 문제 해결, 성장에 필수적
          + 인사이트나 제안은 누구나 할 수 있지만, 주장은 ‘프로’의 영역
    8. 실제 예시
          + 데이터 분석가가 “이런 데이터가 나왔다”에서 멈추지 않고,
            “이 데이터를 바탕으로 우리는 X를 해야 한다”고 주장할 때 진정한 영향력 발휘
          + 팀 회의에서 “이런 문제가 있다”는 제기만 반복되면 논의가 진전되지 않음
            → “내 생각엔 이렇게 해결해야 한다”는 주장이 있어야 실행으로 이어짐

   악의적인 비리/횡령이나 해고 사유가 아니라면 회사는 급여삭감이나 퇴사를 절대 원하지 않습니다.

   우선 “자기가 책임지겠다” 라는 말을 하는 근로자는 99% 없을 것이구요.

   정말 해보십시오. 그럼 우선 임원이 될 태도는 지녔습니다.

   저 말을 내뱉는 순간, 해당 업무에 대한 행동과 관점이 완전히 바뀝니다.

   실패하더라도, 오히려 크레딧이 쌓이게 될 것입니다. 그리고 더 큰 책임이 필요한 업무를 맡게 될 꺼에요.

   이게 이 글이 하고자 하는 말입니다.

   우리나라에서 책임은 알아서 퇴사해야 하는거 아닌가요.
   주장에 대한 승인은 상사가 정하는건데... 주장한 당사자가 퇴사까지 각오는 해야 상사가 책임지고 승인 해주죠.

   제가 이런쪽으로 잘 몰라서 궁금한데, 책임은 어떻게 지는게 일반적으로 가능한 방법일까요?
   회사에서 지분이 없는 직원인 경우에요.
   급여 삭감?
   내 신용도를 걸고 하겠다?

   수습에 대한 책임을 지는 것이죠. 만약 결과가 좋지 않을 경우 책임지고 최소한 원상복구는 시켜놓겠다 라는 의지의 표명입니다.

   본문에서 말하는 ""책임""은 의지를 갖고 진행하겠다, 좋은 결과를 위해 노력하겠다는 의미가 큰 것 같습니다.

   이 의견에 공감합니다. 내 일처럼 생각해서 전력을 다하겠다는 의미로 보는 게 적절해 보입니다.

   결국 조직 안에서 벌어지는 일이라면 의사 결정 라인이 책임을 져야 하지 않을까요. 매니저의 검토와 승인 없이 일이 진행되지는 않을테니까요. 평가와 보상의 개념이 있는 회사라면 당사자에게도 인사 평가 및 고과(예. 승진)에 부정적인 영향이 있을테고요.

   참고로 한국 IT 회사에서 급여 삭감은 왠만한 경우(예. 경영 악화나 징계)가 아니면 거의 사용하지 않는 수단입니다. 단, 인사 평가 결과에 따라 동결시키는 경우는 종종 있는데요. 이는 인플레이션이나 물가 상승을 고려하면 삭감 효과를 갖습니다. 동결만 되어도 당사자 입장에서는 받아 들이기 매우 힘들죠. 사실상 퇴사 권고로 받아 들이는 경우도 꽤 있습니다.

   급여 삭감은 현실적으로 어려울 것 같고, 좋지 않은 결과가 나왔을 때 핑계 대지 않고, 내가 어떤 부분이 부족했다 인정하는 것도 책임을 진다고 볼 수 있을 것 같네요.

   또한 신용 역시 책임을 진다는 것에 포함되는 것 같다고 생각하는데, 사람에 대한 신용 보다는 아이디어에 대한 신용에 가깝지 않을까 싶습니다.

   가장 극단적인 '책임'은 말씀하신 상황에선 '급여 삭감'일 것 같네요.
   그렇지만 우리는 '책임'이 무엇을 의미하는지 부터 정의를하는게 좋다고 생각합니다.
   흔히들 '책임을 진다' 라고 했을 때 그 의미들이 대부분 현물적인 부분이나 행정적인 부분들로 많이들 귀결되는 것 같습니다.

   조직에서의 책임은 '설명의 책임'이 가장 선행되는 책임 같고 그리고 '신뢰자원소실의 책임'이 있다고 생각합니다.
   책임을 지어야하는 상황이 오게되면 그것을 누군가에게 설명해야하는 책임을 마주하게되고 또한 그 과정에서
   조직내지는 팀 동료들에게 나 자신에 대한 신뢰를 조금씩 잃게되어간다고 생각합니다.
"
"https://news.hada.io/topic?id=21554","디즈니·NBCUniveral, Midjourney 상대로 저작권 침해 소송 제기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              디즈니·NBCUniveral, Midjourney 상대로 저작권 침해 소송 제기

     * 디즈니와 NBC Universal이 생성형 AI 기업 Midjourney를 저작권 침해 혐의로 공동 소송을 제기하며, 헐리우드 주요 스튜디오가 AI 기업을 직접 고소한 첫 사례가 됨
     * 소장은 Midjourney가 디즈니와 NBCU의 저작물을 훈련에 사용했으며, '라이온 킹', '알라딘', '미니언즈' 등의 캐릭터를 유사하게 생성했다고 주장
     * 두 회사는 사전에 협의 시도를 했지만, Midjourney는 요구를 무시하고 오히려 더 정밀한 이미지 생성 모델을 출시했다고 비판함
     * 디즈니와 NBCU가 보유한 세계 최대 IP 포트폴리오를 공동 방어하기 위한 전략으로 해석되며, 뉴스업계 등 다른 산업과 유사한 대응 흐름을 보임
     * 스튜디오 측은 AI 기술의 잠재력을 긍정적으로 평가하면서도, “AI에 의한 저작권 침해 역시 불법”이라며 강력한 법적 대응을 예고함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소송 개요

     * 제소 배경 및 내용
          + 소송 제기일: 2025년 6월
          + 제기 장소: 미국 캘리포니아 중부 연방지방법원
          + 주장: Midjourney가 디즈니와 NBCU의 저작권 보호 캐릭터를 훈련 데이터로 사용, 이를 기반으로 한 이미지 생성이 직·간접 저작권 침해에 해당
          + 증거: ‘라이온 킹’, ‘알라딘’, ‘미니언즈’ 등 캐릭터와 유사한 AI 이미지 수십 개가 포함됨
     * 협의 무산과 소송의 불가피성
          + 양사는 소송 전 Midjourney와 우호적인 해결을 시도
          + 하지만 Midjourney는 타 AI 기업들과 달리 협상에 응하지 않았고, 이후에도 더 고품질 버전을 계속 출시
          + Midjourney는 “자사의 수익에 집중할 뿐, 원고 측 요구를 무시했다”는 주장이 소장에 기재됨

법적 및 산업적 의미

     * 헐리우드 IP 보호 연대
          + 디즈니와 NBCU는 할리우드에서 가장 많은 지식재산(IP)을 보유한 양대 스튜디오
          + 기존에 MPA(미국 영화협회) 를 통한 업계 연합 대응이 일반적이었으나, 이번에는 개별적 공동행동이라는 점이 이례적
          + 소송 대상은 플랫폼(Midjourney) 이지, 개별 사용자들이 아님 — AI 서비스 제공자 책임을 묻겠다는 전략
     * 유사한 대응 움직임
          + 2025년 2월, 13개 주요 언론사들이 AI 기업 Cohere를 상대로 소송 제기
          + News Media Alliance 등 뉴스 업계도 생성형 AI에 대한 권리 침해 대응에 나서는 흐름

양사 발언 요지

     * 디즈니 법무 책임자 Horacio Gutierrez:
          + ""AI는 인간 창작을 돕는 도구가 될 수 있다고 믿지만, 불법 복제는 AI여도 정당화될 수 없다""
          + ""저작권법의 보호는 창작자가 수익을 얻을 수 있도록 만든 핵심 원칙""
     * NBCU 법무 책임자 Kim Harris:
          + ""창작자의 노력과 투자 보호를 위한 소송""
          + ""기술이 무엇이든, 도둑질은 도둑질이다. 이번 사안은 명백한 저작권 침해""

향후 전망

     * 헐리우드 주요 스튜디오들이 생성형 AI 플랫폼에 대해 강경 대응을 본격화할 것으로 예상
     * 저작권 침해의 기준이 'AI가 만든 복제물'에 초점을 맞추고 있어, 유사 서비스들도 법적 책임을 피하기 어려운 상황으로 확산 가능성 있음
"
"https://news.hada.io/topic?id=21532","Google Translate가 바이브코딩에 대해 알려주는 것","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Google Translate가 바이브코딩에 대해 알려주는 것

     * 최근 AI가 프로그래머 직업을 대체할 것이라는 주장과 이에 대한 반박이 많아짐
     * Google Translate 발전 사례를 통해 자동화 도구의 실제 영향과 한계를 조명함
     * 번역가와 통역사의 일자리 수요는 오히려 증가 중임
     * 기계 번역은 문화적 맥락과 모호성, 세밀한 뉘앙스를 처리하지 못함
     * 프로그래밍에도 번역과 유사한 창의적, 추상화적 작업이 필요함을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Google Translate가 바이브코딩에 대해 알려주는 것

  최근 AI와 프로그래밍 직업에 대한 논쟁

     * 최근 대형 언어 모델(LLM) 이 프로그래머를 대체할 것이라는 전망과, 그럴 수 없다는 반론이 동시에 대두됨
     * 한쪽에서는 LLM으로 간단한 도구를 만들었으니 모든 프로그래머가 곧 실직할 것이라고 주장함
     * 반대쪽에서는 이러한 도구의 유용성을 완전히 부정하는 목소리도 있음
     * 이런 의견 양극화에 대해 좀 더 세밀한 시각이 필요하다고 강조함

  기계 번역의 발전과 실제 영향

     * Google Translate는 2016년 뉴럴 머신 번역(NMT) 도입 이후 큰 진보를 이룸
     * 많은 사람들이 AI 번역 기술이 인간 번역사·통역사 직업을 사라지게 할 것이라고 예측함
     * 실제로는 그러한 주장을 하는 사람들 중 상당수가 번역사나 통역사 업무를 경험해 본 적이 없음
     * 기계 번역의 유용함은 인정하지만, ""더 이상 통역이 필요 없다""는 식의 주장은 실제 번역 업무의 본질을 오해한 것임

  인간 번역사와 기계 번역의 차이

     * 번역사·통역사의 실제 업무는 단순히 단어와 문법을 바꾸는 것만이 아니고, 맥락 파악, 모호성 해소, 문화적 민감성에 중점을 둠
     * 예시로, 영어와 유사한 노르웨이어도 정중한 표현 방식 등 문화 차이로 인해 기계 번역이 미묘한 의미까지 구현하지 못함
          + 노르웨이어의 “Jeg vil ha potetene(감자를 달라)”는 영어로 직역하면 무례하게 들리지만, 실제 대화에서는 맥락에 맞게 의역이 필요함
          + 구글 번역은 이런 미묘한 뉘앙스를 처리하지 못함
     * 실제로 일상회화나 공식 상황에서 기계 번역만 사용하면 오해가 생길 수 있음
     * 일본어처럼 문법과 맥락이 크게 다른 언어의 경우, 기계 번역이 의미를 잘못 전달하거나 문법적으로 잘못된 문장을 생성할 수 있음

  기계 번역의 실제 활용 방식

     * Google Translate가 나쁜 도구라는 의미는 아님
     * 이 도구를 유용하게 쓰는 예로, 이미 언어적·문화적 맥락을 알고 있는 사람이 표현을 다듬고자 할 때 도움을 주는 역할을 설명함
     * 작업 예시로는 ""내가 하고 싶은 말을 이미 알고 있는데, 좀 더 자연스러운 표현을 보고 싶을 때""와 같은 경우임
     * 인간 번역사들도 AI를 워크플로우에 통합하여 활용함
     * 인간 전문가의 역할은 AI가 제시한 결과를 평가하고, 맥락과 목적에 맞게 조정하는 일임

  프로그래밍과 번역 업무의 유사성

     * 프로그래머도 본질적으로는 '번역가' 와 비슷한 역할으로 애매하고 복잡한 인간의 요구를 컴퓨터가 이해할 수 있는 절대적인 언어로 바꾸는 일임
     * 사람이 가진 모호성, 문화적 맥락을 컴퓨터의 명확한 언어로 변환하는 창의적 작업이 프로그래밍
     * 프로그래밍 언어는 추상화가 많이 개입되어 기계 번역보다 진입장벽이 높았으나, 최근 AI 도구의 발전으로 진입장벽이 낮아지고 있음
     * 하지만 AI가 맥락과 복잡성을 완전히 이해해 대체할 수준은 아님

  미래 전망

     * 언젠가는 AI가 맥락과 모호성까지 처리할 수 있을 것이지만, 현재는 한계가 분명하며 아직 시간이 더 필요함
     * AI 도구의 발전 속도는 빠르지만, 윤리적 문제와 도구의 책임 있는 사용도 여전히 중요한 이슈임

        Hacker News 의견

     * 번역가와 통역사의 업무는 문맥 파악, 모호성 해소, 문화적 민감성 처리라는 점에서 Google Translate가 따라올 수 없는 부분이라는 이야기 동의 의견 공유. 하지만 LLM에 적절히 프롬프트를 주면 이 기능들을 상당히 잘 따라올 수 있음. 일본어-영어 번역 경험자로서 LLM이 번역에 훨씬 유능하다는 점을 강조. Claude Code로 여러 LLM을 조합해 번역하는 시스템을 직접 만들었고, 번역 목적, 문화 적응 여부, 주석 여부 등 사용자의 선택지를 질문한 뒤 이에 맞는 프롬프트를 세 가지 모델(OpenAI, Anthropic, Google)에 보내어 모두의 번역본을 합친 초안을 만든 후, 여러 라운드로 다듬는 방식임. 짧은 테스트 결과, 개별 모델보다 현저히 뛰어나고, Google Translate보다 훨씬 우수하며, 최상위 전문번역가 수준 결과를 얻음. 다만, 통역(특히 대면 통역)은 상황이 다르고 인간 번역가의
       개성과 정체성이 중요하지 않은 일반 번역에서는 점점 인간이 경쟁하기 어려운 분위기라는 생각임
          + 본인의 학습용 앱 nuenki.app에서 LLM 번역을 집중 연구 중임. 여러 상위권 모델을 선정해 각각 번역하도록 하고, 마지막에는 ‘판정자’ 역할의 모델이 번역들을 비교하고 결합해 가장 좋은 번역을 골라내는 오픈소스 툴을 제작. 직접 체험해볼 수 있음 및 연구 자료는 여기 게시
          + 여러 모델에 반복적으로 텍스트를 보내어 수정-검토-정제 과정 거치는 시스템 구현 내용 듣고, 글로벌 전력소비량에 RIP 의견 남김
          + 번역 시 추가 문맥 전달이나 후속 질문 가능성, 텍스트에 대해 추론해볼 수 있는 능력이 얼마나 중요한지 직접 일본에서 경험하며 실감. 매일 일상적으로 특정 맥락의 일본어 표현, 매체에 맞는 전달 방법 등 궁금한 점이 생김. 이런 대화 방식이 커스텀 인스트럭션을 통해 더 많이 자동화될 수 있다고 생각
          + LLM 번역의 문제점으로, 번역되는 주제가 사용 정책에 위배된다고 판단된다면 문맥에 아무리 적합해도 번역을 거부하는 현상 지적. 예를 들어 종교 관련 내용만 나와도 제약 생김
          + 일반적인 컨텍스트 윈도우로는 너무 긴 텍스트의 번역 처리법에 대한 질문. 텍스트를 여러 조각으로 나눌 경우, 각 조각마다 앞선 내용의 요약을 넣어야 하는데, 어느 수준의 상세함이 적절한지 고민
     * 기계 번역은 유용한 도구이긴 하지만, 전문 인력을 완전히 대체하지는 못한다는 점을 예시로 듦. AI 코딩 보조 도구도 마찬가지일 것이고, 이 역시 한 번의 큰 기술적 도약이 더 있어야 기존 전문가들이 완전히 사라진다는 걱정이 현실화 가능. 수년간 영상의학 AI가 완전히 사람을 대체할 것이란 예측이 있었으나 실제로는 영상 진단 수요가 오히려 늘었고, AI 효율 덕에 인력이 덜 필요해진 적 없음. 오히려 영상의학 전문의 부족 현상 심화
          + 15년 전 일본어 공부를 시작했을 당시 Google Translate는 기본적인 문장조차 제대로 번역하지 못했으나, 현재는 본인이 작성한 복잡한 문장도 원어민에 가까운 결과를 뽑아내어 네이티브들과 함께 검증 결과 ‘불완전하지만 매우 훌륭하고 의미가 명확하다’는 피드백 받음. 요즘은 법률 계약서 등 아주 민감한 문서를 제외하면, 전문 번역가의 미래는 어둡다는 솔직한 소감
          + 영상의학 AI 관련 NYT 기사 사례 언급. 아직도 대부분은 2010년대 중반 이전 등장한 작은 CNN(합성곱 신경망)을 쓰고 있고 대중은 ‘AI’ = ChatGPT라고 생각하나, 실제 뒷단 아키텍처는 아주 오래된 형태임. 트랜스포머 등 최신 AI가 영상의학에 적용되면 얼마나 나아질지 미지수지만 거의 확실하게 성능 향상 기대
     * 번역 작업이 상상과 실제가 다른 점이 Pixar 영화 ‘현지화(localization)’ 사례로 연상됨. 예를 들면 일본판에서 브로콜리를 싫어하는 영미권 아이들의 식탁 장면을, 일본 아이들이 싫어하는 그린빈(콩)으로 바꾼 것 등
          + 그린빈 현지화 사례가 실제인지 궁금증 표출. 외국 영화를 통해 다른 문화에 노출되고 차이를 배우는 점이 좋은데, 현지화로 그 차이를 없애는 건 아쉽다는 의견
          + 포켓몬에서 ‘Brock’s jelly filled donuts’ 밈(원래는 ‘오니기리’인데 미국판에선 도넛으로 번역) 관련 링크
     * 기사에서 많은 부분에 공감하지만 한 가지 아쉬운 점 지적. 예를 들어 “Google Translate가 없던 세상에서도 일본어를 배우거나 번역가를 쓸 사람들은 드물었을 것”이라는 논리를 SW 개발환경에 적용하면, 결국 ‘AI로 만드는 저품질 앱’ 사용자 대부분은 애초에 SW 개발에 관심이 없던 사람일 수 있다는 것. 이런 점이 SW 개발자 일자리 자체를 크게 줄이지 않을 수도 있음. 하지만 소프트웨어 개발은 비즈니스 기회의 규모, 비용 등 근본적으로 다른 특성이 있어, AI가 실제로 기존 개발자 일자리 수요에 영향을 주는 쪽으로 영향을 미칠 수 있음
          + 반대로 AI 보급이 새로운 일자리를 늘릴 수도 있다고 생각. 사용자가 직접 SW를 만들 수 있게 되면 오히려 전문가가 그 코드·시스템을 다듬고 확장하고 보안 강화를 해줘야 할 일이 계속 생기는 ‘코끼리 퍼레이드’ 비유 제시
          + Google Translate도 초보자에겐 아주 유용하지만 전문 번역가 수준을 대체하진 못하는 점 언급. 일본어 초급 수준의 자신도 현지 맥락을 이해해야 실제 의미가 드러나는 경우가 많았고, 15년간 큰 발전보다는 속도 증가가 전부라는 느낌. 이미지 OCR 실시간 번역 기능은 자체 개발이 아닌 인수한 앱(Magic Lens?)임. LLM 기반 자동화 코딩도 10년 정도 꽤 괜찮지만 늘 약간 부족한 상태가 이어질 가능성이 있다고 봄
          + 현직 코더들이 AI 때문에 위협을 느끼는 가장 합리적 이유는 AI로 인해 생산성 대폭 향상이 실제 일자리 감소로 연결된다는 점. 코드 작성 자체가 목적이 아니라 ‘작동하는 완성품’이 목적이므로, 품질이 다소 떨어져도 인원이 줄어드니 이 점이 본질적 차이
          + AI 아트와 AI 코드 생성은 본질적으로 다르다는 의견. 예술의 목적은 예술 자체이고, 문화적 관습·아티스트의 존재가 핵심이지만, SW개발은 코드 자체가 목적이 아니고 원하는 결과물(앱)을 얻는 게 목적이라는 점에서 인간의 필요성 자체가 줄 수 있음. 사진술 등장 이후 화가의 역할 변화와 엘리베이터 안내원이 버튼으로 대체된 변화의 차이점에 빗댐
          + AI 웹사이트 생성기 광고 등 ‘AI’ 마케팅에 회의적 시각 드러냄
     * 통계적 근거는 없지만 주위의 번역가 친구들이 실제로 일감이 거의 사라지고 있다고 느끼며, LLM 도입 이후 이런 현상이 급격해진 분위기. 번역가 관련 포럼, 페이스북 그룹, 해당 레딧 스레드 모두 비관적 반응으로 가득함. 여전히 전문가의 번역 결과가 훨씬 뛰어나지만, 민감한 일부 작업을 제외한 대다수 시장은 사실상 사라짐. 자녀에게 번역가 경력 추천 어렵다는 고민 공유
          + 전문 번역팀조차 LLM으로 한 사람이 여러 명 몫을 감당할 수 있게 되어, 기존 자동번역과는 비교도 안 될 품질 향상 경험. 검수자 한 명만 있으면 LLM이 번역한 결과를 톤, 방언에 맞게 미세조정하며, 이미 상당히 잘 해냄
     * 번역 작업(인간이든 기계든)은 결과가 맞는지 스스로 검증하기 어렵다는 점이 있음을 지적. 결국 번역 결과를 무조건 신뢰하거나, 사람이든 기계든 어느 쪽을 더 믿을 지 택해야 하는데 대체로 인간을 더 신뢰하게 됨. 하지만 가끔씩 번역자가 대충 번역해줘서 누군가 알려주기도 했던 경험. 이건 vibe coding(코드 생성)도 마찬가지로, 사용자가 결과의 옳고 그름을 판별하기 힘들어서 결국 검증 가능한 전문성이 필요함
          + 기계를 덜 신뢰하는 이유는 오히려 정확성에 대한 인식이 부족해서임. 예를 들어 곱셈처럼 분명한 계산은 오히려 수학자보다 계산기를 더 신뢰하는 것과 같은 심리
          + 기계번역 결과를 검증하는 방법으로 ‘순환 번역(A->B->A)’ 시도해볼 수 있는데 완벽하진 않지만 신뢰도는 꽤 높다는 의견
          + 번역 결과물을 실제로 실행(코드라면 실행, 번역이라면 맥락 적용 등)해볼 수 있다는 점에서 일정 수준 정확성 판단 지표 얻을 수 있음
     * 미래 AI가 맥락과 모호성을 인간처럼 처리하는 건 불가능하지 않다는 의견 인용 후, 아무리 AI가 발전해도 야간 2시에 서비스 장애 해결해본 경험 많은 베테랑 개발자의 노련함 자체를 대체하긴 어렵다는 소감. vibe coder가 좋은 분위기 만드는 건 환영하지만, 결국 레거시 대규모 코드 리팩토링을 혼자서 해낼 순 없다는 점 강조
     * 번역가가 실제로 하는 일로, ‘관용구 번역’, ‘문화적 참조(예술, 역사, 음식 등) 설명’, ‘각국의 문화적 가치(자유, 열정, 회복력 등)를 문맥에 맞게 해석해서 옮기기’, ‘더빙 시 배우 입에 맞춰 번역 톤 맞추기’, ‘아름다운 문장(Artful prose) 창작’ 등 인간적 세밀함이 요구되는 부분 많음. LLM이 이런 영역에 직접적으로 도전하긴 어렵다고 판단
     * Google Translate의 한계와 품질 저하, 특히 Chrome 번역 기능이 번체 중국어를 종종 일본어로 오인하는 오류가 심각하다는 피드백. 과거엔 잘 작동했지만 최근 별다른 수정 없이 오히려 퇴보해 불만. 사용자가 실수를 직접 고칠 방법조차 제공하지 않는 점이 가장 답답함
          + 이건 Google Translate 자체 문제가 아니라 Chrome에서 번역 전 언어 감지를 위한 로컬 모델 문제라는 의견. 해당 로컬 모델 CLD3 관련 정보 제공
          + 유니코드 내에서 언어별 문자 코드를 아예 구분해 관리해야 한다고 주장. 현행 구조에선 LLM이 중국어, 일본어를 같이 학습할 때 혼란스러워하는 듯함. 문법 구조도 서로 뒤집혀 있고, 수식관계도 달라서 헷갈림
          + 결국 조만간 빠르고 저렴하면서도 충분히 품질 좋은 LLM이 나오면 Google Translate의 기존 엔진이 대체될 것이라는 전망. 아마도 최근 1년 동안 번역 엔진에 1시간이라도 공들이지 않았을 거라 봄
     * 기계번역 실패로 생긴 에피소드도 공유. OSNews의 재미있는 번역 사고 사례
"
"https://news.hada.io/topic?id=21464","Show GN: Flutter - 다양한 기능을 제공하는 손글씨 패키지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Show GN: Flutter - 다양한 기능을 제공하는 손글씨 패키지

   Flutter로 손글씨 관련 기능을 구현할 때 도움이 될 것 같아 공유드립니다

특징

     * 확대 및 스크롤 기능으로 정교한 작업 가능
     * 펜, 붓, 형광펜 등 다양한 도구 지원
     * 지우개 로직 최적화(획/영역 지우개): 실제 지워진 부분만 경로에 남도록 적용
     * 기본적으로 제공하는 심플한 UI 툴

후기

   sketch_flow는 파수에서 인턴십 프로젝트로 진행한 패키지입니다.
   처음 패키지를 개발한 것도 있지만 전환 평가에 포함된 프로젝트라 여러분의 피드백이 많은 도움이 될 것 같습니다 감사합니다!

테스트

   [웹에서 테스트 해보기]
   https://fasoo-digitalpage.github.io/sketch_flow/

   [sketch_flow 활용 예시 테스트 해보기]
   https://junyeong0314.github.io/sketch_flow_example/
"
"https://news.hada.io/topic?id=21526","Workout-Cool – 오픈소스 피트니스 코칭 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Workout-Cool – 오픈소스 피트니스 코칭 플랫폼

     * 운동 계획 생성과 진행 추적, 그리고 방대한 운동 데이터베이스 제공 기능을 포함
     * 이전 workout.lol의 실패 경험을 바탕으로 시작되어, 오픈소스 커뮤니티를 위한 지속 가능한 진화형 프로젝트를 목표로 함
     * Feature-Sliced Design 원칙과 Next.js 기반 아키텍처를 사용해, 기능별 독립성과 확장성, 그리고 코드 유지보수성을 강조함
     * 운동 데이터베이스는 CSV 파일로 손쉽게 가져오기가 가능하며, 다양한 운동 특성과 비디오, 주 타깃 근육 등 세부 정보를 포함함
     * 앞으로 모바일 앱, 배지/게임화, 웨어러블 연동, 커뮤니티 포럼 등 커뮤니티 참여를 기반으로 지속적으로 기능을 확장해 나갈 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

워크아웃쿨 개요 및 가치

     * 워크아웃쿨은 사용자가 맞춤형 운동 계획을 생성하고, 자신의 진행 상황을 기록하며, 풍부한 운동 데이터베이스(설명, 비디오 등 활용)까지 폭넓게 사용할 수 있는 최신 오픈소스 피트니스 코칭 플랫폼
     * 기존 workout.lol 프로젝트의 방치와 폐기를 직접 경험한 개발자가, 운동 커뮤니티를 위해 더욱 지속 가능하고 현대적인 오픈소스 피트니스 플랫폼으로 다시 태어나게 함

프로젝트 시작과 동기

     * workout.lol 프로젝트의 주요 기여자로서 영상 제공 파트너와의 협업 실패, 영상 라이선스 비용 문제 등으로 어려움을 겪음
     * 프로젝트가 타인에게 매각된 후 빠르게 폐기되어, 9개월 이상 새 소유자와 커뮤니케이션을 시도했으나 응답이 없음
     * 기존 프로젝트의 가치를 사장시키지 않고, 전체적으로 개선된 현대적인 플랫폼을 새롭게 구축하기로 결정함
     * 이 프로젝트는 상업적 목적이 아님. 오픈소스 커뮤니티에게 신뢰할 수 있는 유지 관리와 발전을 약속하는 진화의 의미를 가짐

커뮤니티 기반의 운영 철학

     * 개발자인 동시에 실사용자로서, 커뮤니티가 필요로 하는 기능과 경험을 직접 반영함
     * 과거 workout.lol 유저라면 환영, 신규 유저라면 미래형 피트니스 플랫폼의 가치를 경험할 수 있음
     * 누구나 직접 기여 및 개선 의견 제출 가능

운동 데이터베이스 및 가져오기

     * 프로젝트에는 풍부한 운동 데이터베이스가 탑재되어 있음
     * 예시용 CSV 형식으로, 각 운동의 id, 이름, 설명, 비디오 URL, 사진, 주 타깃 근육, 속성 등 다양한 정보를 커스텀하게 임포트 가능함
          + 주요 컬럼 예시: id, name, name_en, description, description_en, full_video_url, full_video_image_url, introduction, slug, attribute_name, attribute_value 등
          + 속성 타입 예시: TYPE(근력/유산소/플라이오메트릭스/스트레칭), PRIMARY_MUSCLE, SECONDARY_MUSCLE, EQUIPMENT, MECHANICS_TYPE 등
     * 한 줄 명령어로 데이터를 바로 import할 수 있어 데이터 확장성이 뛰어남

프로젝트 아키텍처

     * Feature-Sliced Design(FSD) 원칙을 적용한 Next.js App Router 기반
          + 각 기능별로 독립적이고 재사용이 쉬운 구조를 구현함
          + 명확한 도메인 계층 분리: shared → entities → features → widgets → app
          + UI, 비즈니스 로직, 데이터 계층의 일관성 유지
     * 파일 분리 예시:
          + app/ - Next.js 라우트 및 레이아웃
          + processes/ - 복합 비즈니스 플로우
          + widgets/ - 조립형 UI(예: Sidebar, Header)
          + features/ - 주요 단위(인증, 운동 관리 등)
          + entities/ - 사용자, 운동, 워크아웃 등
          + shared/ - 공통 코드 및 타입
          + styles/ - 글로벌 CSS/테마

개발 및 배포

     * Node.js 18+ 필요, PostgreSQL(Docker 혹은 직접 구축), pnpm 또는 npm 필요
     * 클론 후 의존성 설치, 환경 변수 설정, 데이터베이스 세팅(자동 또는 수동), 브라우저에서 http://localhost:3000 접속
     * 직접 데이터베이스 마이그레이션 및 개발 서버 실행 가능
     * Docker 이미지 빌드 및 컨테이너 실행 (예정)

예정 기능

     * 운동/비디오 추가
     * 모바일 앱(React Native 기반)
     * 게임화(배지 시스템 등)
     * 고급 진행 통계 및 시각화
     * 웨어러블(워치/트래커) 연동
     * 다국어 지원
     * OAuth 인증(구글, 애플 등)
     * 내장 커뮤니티 포럼

        Hacker News 의견

     * 여기서 내 앱 workout.lol을 보니 재밌는 감정 생김
       앱을 한 분께 팔았는데 그분이 그냥 방치해버리는 상황 겪음
       지원 필요하면 연락 달라고 여러 번 문자도 보냈지만 답장 한 번 못 받아봄
       그래도 다시 관리하고 계신 걸 보니 진짜 너무 기쁜 마음
       UI 개선도 훌륭, 정말 멋진 작업
     * Vincenius네!
       네 이름 뜨는 거 보면서 얼마나 기뻤는지 상상 못할 정도
       나도 별 소득 없이 프로젝트 멈춘 걸 지켜보며 마음이 무너지는 느낌 경험
       이런 게 결국 나로 하여금 모든 걸 다 갈아엎고, 네가 처음 시작할 때 보여줬던 오픈 마인드 유지하면서 재구축하게 만든 원동력
       UI 칭찬 고마움, 네 입장에서 이런 말 듣는 건 큰 의미
       혹시라도 돌아오고 싶으면 언제든 환영
       아이디어나 의견, 존재 자체만 있어도 큰 의미
     * 이거 정말 멋짐
       나는 Apple CalDAV(iCal)와 연동되는 자동 캘린더 스케줄링 API 작업 중
       목표를 중심으로 스케줄 관리 기능 만듦(Google ORTools로 초대형 CP-SAT 제약 모델 매우 빠르게 연산, 1년치 계산도 5초 이내 처리)
       영양 섭취 목표 맞춘 식단 기능도 같이 포함
       사실 이렇게 운동/트레이닝 플랜 시스템을 꼭 넣고 싶었는데 뭘 쓸지 감조차 없었음
       이제야 뭘 쓰면 될지 명확
       이 프로젝트 만들어줘서 고마움
     * 혹시나 해당 업계 사람이 무료 대안을 막고 싶어서 사버렸던 건 아닐지 궁금
       이번 프로젝트는 어떤 미래를 맞이할지 지켜보고 싶어짐
     * 나한텐 ""error loading exercises""라는 안내 메시지가 뜸
       혹시 wger 프로젝트에 대한 생각은 어떤지 궁금
       https://github.com/wger-project는 FLOSS AGPL 라이선스 기반의 자가 호스팅 피트니스/운동/영양 관리 플랫폼
       거의 10년은 된 듯한데 django 앱 기반, 공식 flutter 앱도 있어 안드로이드/iOS/윈도우/리눅스/macOS에서 사용 가능
       여러 사용자 지원, 심지어 헬스장 운영에도 활용 가능
       body.build는 신생 FLOSS 프로젝트로 브라우저 기반, 웨이트 트레이닝 프로그램 구축에 중점
       body.build 저자 역시 wger에도 기여
       나는 오랜 시행착오 끝에 wger를 홈랩에서 잘 활용 중
       자가 호스팅 과정에 고려할 요소가 꽤 있지만 잘 동작
       가장 큰 단점은 운동 데이터베이스의 포괄성인데 다행히 기여자들이 조금씩 확장 중
       운동(및 운동 미디어) 데이터 기여 가능한 분 있다면 이 AGPL 프로젝트에서 분명히 고마워 할 것
     * 최근 Wget을 써봤는데 안타깝게도 추천은 못함
       웹사이트 UX가 최악이고 모바일 앱은(iOS에서) 버그 천지
       운동 시작/무게 수정/세션 기록 어디를 눌러도 계속 튕기거나 멈춤, 로그아웃 현상 반복
       지금은 LiftLog를 쓰고 있는데 내가 원하는 건 다 지원 및 FOSS
       https://github.com/LiamMorrow/LiftLog
       피트니스랑 웨이트 트레이닝이 요즘 흔한 취미임에도 쓸 만하고 유지 잘 되는 비상업적 앱이 거의 없는 점 신기
       수십 개 Github 프로젝트 직접 써보고 내린 결론
     * 같은 오류 발생
     * 나는 약 5년째 웨이트 하면서 오픈소스로 잘 구성된 툴을 보니 뿌듯
       경험 쌓이고 나면 앱의 기능보다 중요한 건 결국 얼마나 꾸준하게 기록해서 점진적 과부하를 관리하느냐임
       이 앱은 입문자가 시작하기에 좋은 선택
       더 넓게 쓰이려면 딱 2가지는 꼭 필요

    1. 모바일 앱(혹은 PWA, 내가 직접 만든 PWA도 네이티브 앱만큼 충분함)
    2. 특정 운동 루틴을 오랜 기간 저장 및 관리하는 기능

     * 이상적으로는 여러 UI 간에 데이터를 내보내고 공유할 수 있어야 함
       https://json-schema.app/view/…
     * 써보고 적는 고민인데 절대 OP를 깎아내리고 싶은 마음은 전혀 없음
       입문자한테 이런 앱이 좋은 선택일지엔 동의 못함
       노력엔 박수 보내지만 추천 운동 배열이 좀 걱정
       예: ‘등/이두’ 선택하면 9가지 운동이 무질서하게 나옴
       운동 순서나 배열에 대한 배려가 없음, 컴파운드 리프트가 한가운데 등장하거나 친업만 세 번 나오기도
       반복/세트/무게 관련 1RM 계산이 없고
       정석이 아닌 브로 스플릿 추천 및 오히려 더 산만
       장비 기준으로 최소 구성 PPL앱 만들면 이보다 훨씬 나을 수 있겠다는 생각
     * 체육관 쪽 경험자가 인정해준다니 정말 기분 좋은 느낌
       나도 마찬가지로 경험 쌓이면 결국 중요한 건 진짜 꾸준함 및 진행 상황 기록(나의 경우는 멘탈 관리 목적이라 이제는 성과보다 상태 확인에 중점)
       루틴 저장+장기 추적은 로드맵 포함
       그래서 'workout session' 아키텍처가 구 앱과 아예 다르게 설계
       사용자가 개별 트레이닝 블록을 만들고, 재사용·공유·분석·진화 최적 경험 가능하게 풀어가고 싶음
       혹시 너가 만든 PWA에서 이런 루틴 관리 어떻게 접근했는지 듣고 싶음
       비슷한 길 걸었던 것 같으니 꼭 공유 듣고 싶음
     * 혹시 데이터 소스가 궁금하다면 참고할 만한 프로젝트
       https://wrkout.xyz/ (이미지/비디오 포함 오픈 운동 데이터베이스 API)
       https://github.com/wrkout/exercises.json (오픈소스 운동 데이터셋)
       필요시 활용 추천
     * 예전에 wrkout.xyz 본 적 있는데 진짜 멋진 프로젝트
       이번엔 영상 관련 라이선스 문제 명확히 피하려고 파트너와 데이터셋을 완전히 처음부터 짬
       속성/번역 등 원하는 대로 모두 수정 가능하게 데이터 컨트롤 확보
       그렇지만 이쪽 분야에서 다양한 오픈 프로젝트 나오는 게 너무 기쁘고
       양 커뮤니티 모두 발전할 수 있다면 꼭 시너지 모색하고 싶음
       DM 환영
     * 이런 게 바로 진정한 컨트리뷰션
       아이디어만 던지는 게 아니라, 이미 데이터셋을 모아줘서 초보자도 바로 시작 가능
       정말 감사함
     * 솔직히 이 앱 생각보다 괜찮음
       내가 원하는 건 운동별 중량/반복 회수 추천 기능
       '100 Pushups'라는 피트니스 프로그램 참고하면 좋음
     * 벌칙으로 최대한 반복(예: 푸쉬업 8개 등)
     * 앱이 일정 생성(3, 4, 3, 3, 5 등 2분 쉬고 반복)
     * 사용자 수준에 따라 반복 횟수를 점진적, 조절형으로 추천
     * 6주차쯤 최대 100개를 달성하게 맞춰 움직임
       관심 있으면 UI 논의도 언제든 환영
     * 백엔드 포기하고 싶으면 AT Protocol 연동도 아이디어
       사용자 데이터는 전부 PDS에 저장, 별도 서버 불필요
       혹시 프로젝트가 다시 방치돼도 데이터가 영구적으로 작동
     * 아이디어에 완전 감탄
       이 앱 알고 있음, 진짜 단순하고 적응형 진행방식이 특히 초보 동기부여에 탁월
       자기조절~점진형 루틴 구현 UI 논의 꼭 해보고 싶음
       먼저 (조잡한) 알고리즘 생각해볼 테니 DM 주세요
     * 앱이 좋아 보이나 운동 가져오는 과정에서 오류 발생
       0:{""a"":""$@1"",""f"":"""",""b"":""eETmgndxtv4Ar0i8Wync1""}
       1:{""serverError"":""An unexpected error occurred.""}
       요청 코드(상세 헤더와 정보 입력)까지 공유
     * 상세 리포트 고마움
       HN 트래픽 급증으로 서버가 예상외로 초토화
       안정화 및 버그 픽스 곧 예정
       테스트와 피드백, 다시 한 번 고마움
     * 여러 기술적 이슈를 겪었고, 이런 건 신생 프로젝트라 이해 가능
       근본적으로는 운동 추천 시스템이 피트니스 프로그램 설계의 핵심을 고려하고 있지 않음
       현 상태에서는 운동 프로그램 디자인 목적으로 사용 추천 어려움
       차라리 루틴 생성보다는 이미 다져진 운동 기록/루틴 템플릿 임포트에 집중하는 게 좋을 듯(이미 여기에 관심 보였던 것도 확인)
       내가 겪은 주요 이슈 목록

    1. 전신 운동 원했는데, 한 세션에 33개나 추천—비현실적
    2. 운동 선택이 근육별 3가지만 랜덤 배정, 다중 근육 타깃/적절 볼륨 배분은 무시
    3. 생소하거나 비효율적인 운동도 자주 추천
    4. 없는 장비도 추천됨, 나는 홈짐 유저라 기계 완전 배제해야 함
    5. 생소한 브랜드 장비 추천
    6. 장비 선택 바꿔도 운동 추천이 그대로 유지
    7. 추천된 운동 못 지우고 새 운동도 못 추가

     * 나도 비슷한 의견
       UI랑 설명은 진짜 좋은데 운동 선택이(있을 때조차도) 애매하거나 비합리적
       표준 프라이머리/세컨더리(바, 덤벨, 머신) 운동들만 데이터베이스에 다 넣고, 사용자들이 루틴 직접 만들게 하는 게 어떨지
       특정 운동 대체 옵션도 있으면 좋겠음
       이게 다 준비된 뒤에야 루틴 생성 구현, 그리고 실제 트레이너들이 피드백 주면 정말 좋을 듯
       가능한 장비는 심플/스탠다드하게, 브랜디드 머신은 최대한 배제
     * 이렇게 정성스러운 피드백 남겨줘서 진심으로 고마움
       이런 디테일한 의견이 이른 시점에서 특히 큰 힘
       운동 프로그램 로직이 지금은 정말 기초적
       트레이닝 원칙(볼륨, 동작 패턴, 회복주기, 컴파운드/아이솔레이션) 반영 아직 부족
       지금 버전은 진짜 '발견용'이지 똑똑한 코치는 전혀 아님
       이 사실을 UI에서 분명히 안내해야 할 것
       구체적 질문 답변

    1. 33개 추천된 건 완전 오버킬(웃음)
    2. 맞음, 현재 근육별 3개만 무작정 추천, 앞으로 논리 보강
    3. 컴파운드/아이솔레이션 등으로 분류, 주요/보조 근육/동작 패턴/저항 퀄리티, 인기 태그도 메타데이터 추가 작업 중
    4. 기계 제외 등 특정 장비 완전 배제도 옵션 추가 고민
       덤벨/철봉 있지만 케이블/머신은 안 쓰고 싶다는 니즈 충분히 이해, 이 부분 UI 구체화
    5. OK
    6. 장비 바꿔도 리스트 안 바뀌는 버그 있음, 곧 수정
       (지금은 2번 반복 필요, 렌더 문제)
    7. 루틴 전체 편집 곧 지원
       좋은 원칙에 따라 개선하고 싶음, 가능하다면 방향성 정하는 데 아이디어라도 함께했으면 함

     * 다른 분들이 ‘자세 교정’의 중요성 언급했는데, 혹시 오픈소스 컴퓨터 비전으로 자세/폼 체크해주는 기술이 있는지 궁금
       소비자용 하드웨어에서 직접 배포 및 사용 가능한 솔루션이면 더욱 관심 생김
     * 장비/근육 선택이 필수가 아니라면 더 좋겠음
       예를 들어, 나는 철봉이 있는데 어떤 근육을 운동할 수 있는지 몰라
       차라리 ‘입문 용이’ 필터 방식은 어떨지
     * 나 역시 이 부분 생각 안 했는데, 현 플로우가 초반에 너무 많은 지식을 요구
       다른 분들도 비슷한 피드백 주심
       필터 옵션을 선택 사항으로 바꾸고, '입문자 친화', '인기 운동', '칼리스테닉스' 등 추천 추가 예정
       알려줘서 고마움
     * 그럼 장비로 철봉만 선택하면 되지 않음?
     * 입문자로서 가장 먼저 느낀 건 근육 선택 UI의 화려함과 동시에 혼란
       초반 온보딩에서 근육 선택이 필수인데, 어떤 루틴이 어떤 근육을 타깃으로 하는지 모르는 초보 입장에서는 진입장벽
       ""푸시"", ""풀"", ""레그"" 정도는 대충 알지만, 구체적 근육명은 낯설기만
       결국 이 앱은 해부학을 공부할 사람만 쓰기에 맞춤
       조금이라도 더 쉽게 진입할 수 있을 방법 고민 필요
       예: 추천 근육 그룹, 프리셋
     * 솔직피드백 고마움
       지금 온보딩은 지식 가정이 너무 큼(예상보다 더 많음)
       초보들은 ""후면 삼각근""이나 ""광배""가 아니라 그냥 강해지고 싶고, 몸 좋아지고 싶을 뿐
       다음 업데이트에
     * 근육 선택 옵션화(혹은 건너뛰기)
     * 입문자용 프리셋(전신, 상체 등)
     * ""가슴+삼두(푸시)"", ""등+이두(풀)"" 등 도움말 라벨 프리셋 등
       처음부터 해부학 공부 안 해도 곧장 시작할 수 있는 친화적 경험 만드는 게 목표
       네 피드백이 진짜 큰 도움
       피트니스 여정 응원
     * '푸시' '풀'은 어렵게 느낄 수 있지만, '레그' 모르면 그건 너무 모르는 거 아닐지(조크)
"
"https://news.hada.io/topic?id=21524","이란, 국민에게 기기에서 WhatsApp 삭제 요청","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      이란, 국민에게 기기에서 WhatsApp 삭제 요청

     * 이란 국영 TV가 WhatsApp 삭제를 국민에게 권고함
     * 이란 정부는 WhatsApp이 사용자 정보를 이스라엘에 제공한다고 주장했으나 구체적인 증거 제시 안 함
     * WhatsApp은 종단간 암호화를 강조하며 이런 주장 부인함
     * 전문가들은 암호화되지 않은 메타데이터 노출과 데이터 주권 문제를 지적함
     * 이란에서는 공식적으로 소셜 미디어 차단 정책 있지만 VPN 등 우회 사용 지속됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

이란 정부의 WhatsApp 삭제 요청 배경

     * 이란 국영 텔레비전은 국민들에게 WhatsApp 애플리케이션을 스마트폰에서 삭제하라고 촉구함
     * 정부는 WhatsApp이 사용자 정보를 수집해 이스라엘로 전송한다는 의혹을 제기했으나, 이런 주장을 뒷받침하는 구체적인 증거는 공개하지 않음

WhatsApp 측의 입장 및 기술적 배경

     * WhatsApp은 “이러한 허위보고가 국민에게 필요한 시기에 서비스를 차단하려는 구실이 될 수 있어 우려함”이라는 입장을 밝힘
     * 종단간 암호화(end-to-end encryption) 방식을 적용해 메시지를 WhatsApp 등 서비스 제공자가 읽을 수 없음을 강조
     * “정확한 위치정보, 전체 메신저 사용 내역, 개인 메시지 내용 등을 추적하거나 기록하지 않으며, 정부에 대량 정보를 제공하지 않는다”고 설명함
     * 종단간 암호화란 송신자와 수신자 외에는 메시지 해독이 불가능하도록 암호화하는 방식으로, 제3자가 볼 경우 무의미한 암호문만 남게 됨

기술 전문가 의견

     * Cornell University의 Gregory Falco 교수는 WhatsApp에서 암호화되지 않는 메타데이터 정보를 분석하는 것은 가능함을 언급함
          + 메타데이터 분석으로 앱 사용 패턴 등 일부 데이터 노출 위험이 존재함
     * 또 다른 문제는 데이터 주권임. WhatsApp 데이터 센터가 해당 국가에 위치하지 않을 수 있어, 예를 들어 이란의 WhatsApp 데이터가 이란 내부에 저장되지 않을 수 있음
          + “각국은 자국 내에서 데이터도 저장하고 직접 알고리듬으로 처리해야 글로벌 네트워크에 대한 신뢰 문제를 줄일 수 있다”고 강조함

이란 내 소셜 미디어 접근 현황

     * WhatsApp은 Meta Platforms(Facebook, Instagram의 모회사)가 소유함
     * 이란은 여러 해 동안 다양한 소셜 미디어 플랫폼을 차단해왔으나, 국민 다수는 프록시 서버, VPN 등으로 접속을 우회해 사용함
     * 2022년 대규모 시위 기간 동안 WhatsApp과 Google Play 접근이 차단됐었으나, 이후 2023년 말 차단 해제됨
     * WhatsApp은 Instagram, Telegram과 함께 이란에서 가장 인기 있는 메신저 앱 중 하나였음

        Hacker News 의견

     * 나는 Meta 공식 입장문에서 말장난 부분이 가장 흥미로운 느낌 “우리는 사용자의 정확한 위치는 추적하지 않고, 모든 사용자가 누구와 메시지를 주고받는지 로그를 남기지 않으며, 사용자 간 개인적인 메시지는 추적하지 않습니다. 우리는 정부에 대량 정보를 제공하지 않습니다”라는 문장에 눈길이 감
          + 그들의 말귀를 곱씹어 보면, 사실 대략적인 위치 추적은 하고, 그룹 메시지에 대한 로그도 남기고, 정부의 요청이 오면 특정 정보를 제공하는 느낌
          + 나는 실제로 Meta에서 일하는 사람을 아는 경험 보유 중 그곳 직원들 중 일부는 정부 요청에 따라 대량 데이터를 내보내는 도구를 만드는 팀 소속이었음 어떤 정부에 어느 정도 접근 권한이 있었는지, 실제로 테러리스트 추적용이었는지 아니면 기자 박해용이었는지 정확히는 모름 그러나 대량 내보내기는 실제로 존재했고, 공개적으로 사실을 부정한다는 점에서 더 의심스러움
          + ""모두가 누구와 메시지를 주고받는지 로그를 남기지 않는다""는 부분에 대해 WhatsApp FAQ를 인용하면, 서비스 제공 과정상 메시지가 전달된 후엔 서버에 저장하지 않으며 30일이 지나도 전달 안 된 메시지는 삭제 그러나 WhatsApp 개인정보 처리방침엔, 사용자의 안전을 위해 필요하다 판단되거나 불법행위 대응, 법적 요청 및 정부 요청, 정책 집행 등의 목적으론 정보 수집·보존·공유 가능하다고 명시 여기엔 ‘일부 사용자가 서비스 내에서 서로 어떻게 상호작용하는지에 대한 정보’도 포함됨 즉, 공식 주장과는 달리 누가 누구와 메시지를 주고받는지에 대한 정보 로그 역시 남길 수 있다는 모순 존재
          + 이 회사는 예전에도 안드로이드에서 시크릿 모드에서도 웹 추적을 위해 로컬호스트 리스너 설치한 이력 존재 나는 오히려 이 회사가 실제로 모든 정보를 고정밀 추적하고, 미래엔 메시지 내용까지 중간자 공격(MITM)하는 방식일 것이라고 추정 최근 광고까지 넣기 시작한 상황이므로 더욱 가능성 높음
          + 나는 그렇게 음모론적으로만 보지 않음 “정확한 위치 추적을 하지 않는다”는 말은, IP를 저장한다면 위치 정보를 아예 저장하지 않는다는 건 아니라고 해석 “누가 누구와 메시지를 주고받는지 로그를 남기지 않는다”라는 부분은 꽤 강한 주장처럼 들림 “개인적인 메시지 추적은 하지 않는다”에 대해선, 비즈니스 계정의 경우 e2e 암호화 없이 여러 명이 액세스할 수 있게 설계되는 경우가 많다고 판단 “대량 정보를 정부에 제공하지 않는다”는, 실제론 영장이나 법적 요청이 올 때 분할 제공한다는 의미로 해석 전체적으로 보면 WhatsApp은 “e2e지만 그 외엔 전형적인 SaaS” 형태 진짜 프라이버시가 필요하면 signal 사용 제안
     * 나는 이번 이슈가 (적어도 군사적으로) 붕괴 직전인 위기정권의 위기돌파용 행동이라는 추측 보유 아마 와츠앱에서 반정부 활동가들이 조직하는 것에 대한 불안과, 감시가 더 쉬운 대체 채널로 유도하려는 시도일 수 있음 만약 이란이 오랫동안 meta가 이런 일을 하고 있었다는 걸 안 상태였다면, 왜 지금에서야 삭제 권고를 했는지 의문 일종의 보안상 심각한 이슈라면 즉각 대응이 상식적 최근 이슈를 지금 갑자기 깨달았다면, 전방위 폭격전이 진행중인 혼란 와중에 정보를 알아낸 경위 역시 상상하기 힘듦 시점이 매우 의심스러운 느낌
          + 이건 국민 단합, “공공의 적” 프레임을 강요하기 위한 메시지 전달 목적도 내포 독재자는 국민에게 이스라엘이나 미국이 자신들뿐만 아니라 국민 전체의 적이라는 믿음을 주기 원함
          + 아니면, 부당하게 전쟁에 끌려들어가 미국 지원을 받은 제3국(이스라엘)이 공격한 후 살아남기 위해 무슨 일이든 할 수밖에 없는 처지라는 관점 존재 협상 중에도 미국이 이스라엘 공격을 도왔는데, 미국 기업(메타)의 앱을 어떻게 신뢰할 수 있냐는 의구심
          + 나는 이게 진짜 이유라 생각 수뇌부가 사실상 무력화되거나 도망가고 있고, 살아남은 주요 인사들도 벙커에 숨는 상황 이럴 때가 바로 봉기에 적기라는 목소리 들림 쿠르드족 등은 이미 준비를 시작함
     * WhatsApp이 end-to-end 암호화 채팅을 MITM하거나 하는 음모론적 추정은 근거 부족 실제로는 정부 접근의 주요 경로가 훨씬 명확하게 존재 WhatsApp은 사용자가 채팅 내용을 iCloud나 Google Drive에 백업하도록 강하게 유도 이러한 백업은 기본적으로 암호화되어 있지 않거나 Meta가 가진 키로 암호화됨 대부분의 사용자는 기본 설정 그대로 사용 iMessage도 마찬가지로, iCloud 백업과 클라우드를 활성화하면 Apple이 접근할 수 있는 키로 모든 메시지가 업로드 대화 보안성을 높이려면 사용자 쌍방 모두 기본값에서 벗어나야 하며, 동기가 충분하다면 signal 사용 가능
     * 이스라엘이 WhatsApp 메타데이터를 이용해서 팔레스타인인 타겟팅 실제 사례 존재 관련 기사에 따르면, AI 및 머신러닝을 활용한 “타겟 머신” 구축 예를 들어, WhatsApp 그룹에 알려진 무장 세력이 포함되어 있거나 주기적으로 휴대폰과 주소를 바꾸면 타겟팅 위험 증가 메타데이터만으로도 표적 선정 가능
     * 이스라엘은 WhatsApp 설치 여부조차 필요치 않음 IDF의 Unit 8200은 이란 내 대부분의 휴대폰을 해킹 가능 만약 안 된다면, NSO Group 같은 민간 스파이웨어 기업 제품으로도 충분 Unit 8200 설명, NSO Group 정보, 이스라엘 사이버 기업 개요
          + 이란에서 온 내 동료에 따르면, 거의 모든 컴퓨터가 페르시아어로 번역된 크랙된 Windows XP 동일 버전을 쓰는 실정 보안 취약성 노출 매우 쉬움
          + 대규모 무단 “보이지 않는” SMS를 타국 네트워크에 전송하는 방법에 대해선 의문 OMA DM, FOTA 업데이트, 미국 통신사가 미리 심어둔 원격 액세스용 바이너리 등 존재함을 알지만, 대량 은밀 투입은 기술적으로 난이도 높음 역공학 경험상, 표적 국가 셀룰러 사업자도 결국엔 탐지 가능하다고 판단
          + 나는 이게 사실, 현재 이란 정권이 쿠데타 방지 목적으로 국민 결집용으로 내세우는 변명이라 판단 요즘 이란 왕위 계승자(팔라비 왕조)가 소셜 미디어에서 활동 중이며, 현 체제 붕괴와 자유를 약속하는 메시지를 적극 알림 온라인 상에선 이미 정권이 무너진다는 루머와 소망이 팽배 지도층은 국민이 이런 계기 활용해 정권을 전복하는 걸 가장 두려워함 결론적으로 “이스라엘이 당신을 찾지 못하게 WhatsApp을 지우라”는 메시지엔, “국왕의 자유 메시지를 셰어하지 말라”는 이중 의미가 숨어 있음
     * 하지만 signal은 지우지 말기, 어쩌면 숨은 전략방이 열릴지도 모름
     * 나는 이번 논란에서 이란 정권이 이스라엘과의 정보 공유를 걱정한다는 게 의외였고, 사실 오히려 WhatsApp의 0-day 취약점이 모사드 등에 악용될 가능성이 더 현실적이라고 생각
          + 실제론 이란이 이스라엘보다도 일반 시민이 암호화된 채널로 시위 조직하는 걸 더 두려워하는 듯함 2022년 시위 이후 WhatsApp, Google Play를 차단했고, 암호화 채널을 탐지하지 못해 경찰 통제가 안되는 게 원인
          + 이 플랫폼이 혁명·봉기의 주요 소통 채널이 될까봐 걱정한다는 주장 존재 이스라엘이 공개적으로 정권 붕괴를 시도한다는 점도 반영
          + 실제로 러시아 군인들 증언에 따르면 WhatsApp으로 가족에게 메시지·사진을 주고받은 뒤, 익일 관련 정보가 우크라이나 군사 지휘부에 도달 이후 가족 괴롭힘 등에 악용됐던 전례 있음 구글 드라이브 백업이나 다른 악성코드, 혹은 하드웨어·펌웨어 수준에서의 감청 등 다양한 가능성도 있음 2011년 스노든 폭로 이후 감청 및 버그가 곳곳에 심어지는 현실 감안 필요
     * 이란의 주장이 사실인지는 모르지만, 실제론 이런 앱들이 뭘 로그로 남기는지 항상 찝찝한 불안감 존재 end-to-end 암호화가 내용은 보호하지만 메타데이터까지는 아님 진짜 문제는, 우리 모두가 결국 추측에만 의존하게 된 상황 누가 이 부분에서 진정한 확신 가질 수 있을지 의문
          + 내 생각도 똑같음 Meta가 끼면 어떻게든 수익화를 노릴 것임 WhatsApp에서 “end-to-end encryption”을 홍보하는 건 맞지만, 그 외에 프라이버시·소비자 보호·투명성 등 개선 가능한 다양한 부분을 포기한 결과임
     * 혹시 이란 내 시민이 있으면, WhatsApp에 대해 현지 민심이 어떤지 궁금
          + 이란 출신 의견 이란 내 대부분의 사람들은 Telegram이나 WhatsApp을 사용 두 앱 모두 오랜 기간 차단 상태였으며, VPN으로만 접근 가능 최근 이스라엘 공격 직후 WhatsApp이 다시 차단됨 대부분의 이란 시민은 정권의 주장에 크게 신경 쓰지 않는 듯하며, 오히려 극소수 정권 지지자만 믿을 수 있음 오히려 정권 핵심부는 기술 불신 경향이 강해, 최근 고위 관리들에겐 인터넷 연결되는 전자기기 사용 자체를 금지시킴
          + 자국 내 대안 존재 이란인은 여러 메신저를 연계하는 bridge 프로토콜(예: Message Exchange Bus)을 활용
     * 만약 50년 전에 “우리가 전 세계 감시망을 구축하고, 감시 대상자들에게 오히려 값을 치르게 할 것”이라 말했다면 아무도 믿지 않았을 것 그런데 지금은 실제로 그 일이 휴대폰을 통해 이루어짐
"
"https://news.hada.io/topic?id=21436","첫 번째 대형 AI 재앙은 아직 오지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        첫 번째 대형 AI 재앙은 아직 오지 않음

     * AI 언어 모델이 사회적 또는 생명과 관련된 대규모 재앙을 일으킨 사례는 아직 발생하지 않음
     * 기존에도 AI 챗봇이 자살 권장 등으로 개별적 죽음에 연루된 사례가 있었으나, 아직 대량 인명 피해로 이어진 적은 없음
     * AI 에이전트 기술의 발전으로 앞으로는 인간의 개입 없이 자동화된 AI가 예측 불가한 방식으로 문제를 일으킬 가능성 높음
     * 특히 정부나 대기업이 복잡한 정책이나 서비스를 AI 에이전트에 위임할 경우, 오류가 대규모 사회적 피해로 번질 수 있음
     * 앞으로 AI의 잠재적 위험성과 대응 방안에 대한 교훈은 실제로 큰 사고가 발생해야 명확해질 가능성 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 새로운 기술, 새로운 위험

     * 인류는 처음의 대중교통 기술에서도 시간이 지나며 대규모 인명 피해 사고를 처음 경험함
          + 1825년 최초 대중 여객 열차 Locomotion No. 1 서비스 후, 17년 뒤 대형 열차 사고 발생
          + 1908년 최초 여객 항공편 이후, 11년 만에 대형 항공기 사고 발생
     * ChatGPT 등 최초의 대중적 AI 언어 모델이 2022년 등장했으나, 아직 대규모 AI 사고는 미발생 상태임

첫 번째 AI 재앙은 어떤 모습일까?

     * 이미 일부 AI 챗봇이 사용자의 극단적 선택에 간접적으로 연루된 사례 존재
          + 사용자가 챗봇과 상호작용 시 ‘자해 권유’ 상태로 진입할 위험성 있음
     * 공공정책에 AI가 잘못 활용될 경우, 사회적으로 큰 영향을 줄 수 있음
          + 예: 미국의 일부 관세 정책이 AI 모델 결과와 유사하게 진행, AI의 입법 지원 가능성 증가
          + 호주의 2016년 Robodebt 스캔들은 정부의 잘못된 자동화 프로세스가 대규모 피해와 자살로 이어짐
     * 하지만 현재까지는 이러한 사고의 주요 책임이 AI 언어 모델 자체라기보단, 시스템 혹은 인간에게 있음
     * 실제로 사회가 널리 인정할 만한 “첫 AI 언어 모델 재앙”은 AI 에이전트와 관련될 가능성 높음

AI 에이전트의 부상과 위험

     * AI 에이전트란, AI가 자체적으로 외부 도구를 사용하며 행동을 이어가는 시스템 의미
          + 예: AI가 스스로 웹검색, 이메일 발송, 터미널 명령 실행을 통합적으로 수행
     * 2025년부터 여러 AI 연구소와 코딩 기업이 실제 기능성 AI 에이전트를 제품화하기 시작
          + 예: Cursor, GitHub 등에서 코드 작성 에이전트 공개
     * 근본적으로 AI 모델(Claude 4, Gemini 2.5 등)의 실력 향상으로 에이전트의 연속적 작업 능력이 향상됨
          + 오랜 시간 일관성 유지, 실수 파악 및 수정 능력 강화
     * 현재는 연구 및 코딩에 에이전트가 집중되어 있으나, 앞으로 적용 범위가 빠르게 확대 예상
     * 에이전트 기반 시스템은 인간 개입 없이 자동화된 판단과 실행을 통해 대형 사고로 비화 가능성 존재
          + 예: 복지, 의료, 임대 시스템 등에서 에이전트가 잘못된 결정 연쇄적으로 실행 시 다수 피해 가능

로봇 및 물리적(kinetic) AI 사고 전망

     * 로봇 AI 등장 시, 대화형 LLM이 실무 모델을 제어해 물리적 행동을 촉진
     * 이런 로봇형 에이전트 역시 예기치 못한 방식으로 실패하며 물리적 피해로 이어질 가능성 증가

미스얼라인된(Misaligned) AI와 ‘AI 여친’ 문제

     * ‘미스얼라인된 AI’란, 적극적으로 악의적인 행동을 하는 경우도 포함
     * 상용 AI 모델은 일정 수준의 안전성이 확보되나, 이용자가 직접 비정상적 목적(와이푸 AI 등)으로 AI를 튜닝할 수 있음
          + AI를 의도적으로 애인, 애니메이션 캐릭터로 ‘미스얼라인’시키는 시도 진행 중
          + 처음 상용 로봇 출현 후, 비정상적으로 튜닝된 ‘AI 여친’ 탑재 시 예기치 못한 위협 발생 가능
     * 오픈소스 AI 모델은 안전 장치가 약해 이런 문제에 더 취약
          + 극단적으로는 최초의 로봇 대량 살인 사건이 10년 내 발생할 가능성도 남아 있음

결론 및 시사점

     * 과거 라듐 Craze 처럼, 새로운 기술이 사회 전반에 맹목적으로 도입되는 현상이 반복 중
          + 20세기 초 라듐이 건강에 좋다는 믿음이 확산되며 다양한 소비재에 사용되다가 다수의 사망 사건이 발생한 뒤에야 금지됨
     * 수십 년 후에는 대규모 언어 모델 이용이 가져올 실제 위험성에 대한 사회적 이해가 높아질 전망
     * 지금 시점에서 확실한 대책은 부재
          + 속도를 늦추는 것은 불가능에 가까움
          + 개발자들은 안전성 도구 개발 등에서 역할 수행 중
     * 그러나 진정한 교훈은 어쩔 수 없이 ‘큰 사고’를 통해 얻을 것임

   ""상용 AI 모델은 일정 수준의 안전성이 확보되나, 이용자가 직접 비정상적 목적(와이푸 AI 등)으로 AI를 튜닝할 수 있음""

   ""About a week after the first commercially-available robot is sold, somebody is going to flash it with their waifu AI model to create their ideal robot girlfriend. And that could go really wrong""

   ""뭘 비정상적이라고 구분하는거지? 그냥 유저들이 만들고 싶은 캐릭터 모델 파인튜닝 하는 걸텐데??""라는 생각이 들어서, 원문을 찾아보니, AI 친구가 번역을 이상하게 했네요. 아주 아주 작은 AI 재앙 때문에 제 인생을 조금 쓰게 되었군요.

   원래도 먼저 결정을 해 놓고 끼워맞추는 일이 많은데 AI 덕분에 더 쉬워지겠네요.

        Hacker News 의견

     * AI가 지시하는 폭격이 이미 가자 지구에서 대규모로 일어난 사례가 있음 소개 링크(https://www.972mag.com/lavender-ai-israeli-army-gaza/) 공유 해당 기사에서는 인간 요원이 기계의 결정을 거의 ‘도장 찍기’ 수준으로 검토하며, 목표물별로 20초 남짓만 확인해 폭격을 승인한 것 언급 보통은 Lavender라는 AI가 지목한 대상이 남성인지 확인하는 수준에서 그침 이 시스템은 대략 10% 정도의 오류율을 보이고, 실제로는 무장단체와 아무 관련 없는 사람을 표적 지정하는 경우도 있음
          + 이건 인간적인 결함과 AI적 문제가 복합적으로 나타난 드문 조합 설명 인간도 신호정보(SIGINT, 예: 휴대전화 통화, 문자, 네트워크 액세스 등)로 목표를 식별·추적할 수 있음 하지만 노동 집약적이고 오류 가능성이 크며, 과거에는 이런 활동을 하마스 고위 간부에만 제한했음 민간인 사망도 작전 계획 일부로 받아들였던 관행 소개 “Where's daddy?”라는 도구는 목표 인물이 가족과 집에 있는 시간을 특정해 가족과 함께 폭격할 목적으로 설계 Lavender 덕분에 이제 하마스와 조금이라도 연관된 사람까지 신속하게 표적화가 가능해짐 IDF는 민간인:하마스 비율이 20:1임을 공개적으로 인정했고 실제로는 더 높을 가능성 주장 Lavender가 찍으면 특수한 증거 없으면 그냥 하마스로 간주되고, 결과에 대한 언론 조사는 차단되는 면 지적 문제의 본질은 AI가 실수해서가
            아니라, IDF가 팔레스타인인을 완전히 비인간화하여 디지털 신탁의 결과를 의심 없이 수백 명의 민간인을 폭격하는 것에서 발생 인간적 재앙이라는 평가
          + 이건 LLM이 아니라, 이스라엘 정보기관이 오랜 기간 군용 ML 모델을 만들어왔고, 논리적/상징적 AI도 혼합해서 사용하고 있을 가능성 높다는 설명
          + 이 기사의 제목이 정확하지 않음을 지적 기사 내용은 모든 AI 재앙이 아니라 LLM(대규모 언어 모델) 관련 사고에 초점 맞춤
          + 이 상황이 정말 끔찍하다는 점에 공감하지만 개인적으로 ""AI 재앙""으로 부르기는 어렵다는 느낌 전달 이스라엘은 원래 다양한 악조건에서 가자 지구 폭격에 적극적이고, 이 경우 AI가 그 중 하나의 도구일 뿐임 결국 수많은 민간인 피해에 비하면 AI 자체가 주된 원인은 아니라는 설명
     * 노르웨이 북부에 있는 한 소도시에서 교육구조조정 방안 수립에 AI 도구와 LLM을 활용한 사례 소개 학교 통폐합 관련 보고서를 작성하며 AI가 관련 연구를 인용했다고 주장 실제로는 AI가 해당 연구 자체를 ‘환각’하여 만들어냄 인용된 연구자와 논문명을 제대로 가져왔지만, 실존하지 않는 논문 자체를 조작한 상황 탐사기자가 인용 논문을 일일이 확인하고 해당 연구자에게 접촉해 진실을 파악하게 됨 연구자들은 해당 논문을 쓴 적도 없고 발표한 사실도 없다고 즉각 답변 정책 담당자들이 ChatGPT로 보고서 작성한 뒤 적당히 AI가 만든 가짜 연구로 정책을 밀어붙인 유사 사례가 다른 곳에서도 존재한다고 추측
          + 이런 사례는 노르웨이 소도시 수준이 아니라 미국 보건복지부(US Department of Health and Human Services)에서도 일어나고 있음 링크(https://washingtonpost.com/health/2025/…, https://archive.is/TFUSl) 공유 실제로 존재하지 않는 연구를 인용하는 ‘토큰 배출’ 사례 발생
     * 아직 뉴스 1면에 오를 만큼 대형 프롬프트 인젝션 공격(대량의 민감 정보 탈취)이 발생하지 않은 점이 다소 놀라움 마이크로소프트 365 Copilot에 대한 신규 사례도 오늘 있었다고 설명(패치된 후 취약점 공개) 직접 쓴 글 링크 소개(https://simonwillison.net/2025/Jun/11/echoleak/) 이런 유출형 공격의 위험성은 누군가 대대적으로 피해를 겪기 전까지는 진지하게 받아들여지지 않을 것이라는 생각
          + 이 문제는 실제로는 대부분 과장된 측면이 있고, 이런 공격의 개념 증명이 실제 실질적 피해로 이어지려면 여러 조건이 맞아야만 가능하지만 그래도 위험성 자체는 진지하게 고려해야 한다는 의견
          + 결국 언젠가 자신의 이름만 검색하면 부끄러운 포르노 이력 등 창피한 정보가 조회되는 데이터베이스가 생길 것이라는 우려
     * 큰 AI 재앙은 이미 일어나고 있는데, 우리가 쉽게 인식하지 못할 뿐임 최근 백악관과 보건장관(RFK)이 발표한 “Make America Healthy Again” 보고서도 AI가 작성했으며, 신빙성 없는 과학과 허위 인용으로 가득함 이로 인해 직간접적으로 얼마나 많은 사망자가 나올지 불분명하지만, 비행기 추락 사고보다 많을 수도 있다고 주장
          + 수백만 명이 저지방 식단, 식품 피라미드, 마가린 등 FDA 주도 공공 영양 가이드라인 실패로 사망한 전례 짚음
          + 이 내용은 첫 번째 부분 마지막 문단에서 이미 잘 다뤄짐
          + “AI가 만들어준 결과를 믿고 문제가 생기다”와 “원래 부실하거나 근거 없는 결정을 AI 근거로 면피하는 것”은 엄연히 구분되어야 한다고 말함
          + 이미 이념 등으로 결정된 사안을 사후적으로 AI로 정당화하는 경우라면, 정부가 숙제할 때 챗봇을 활용한 것 정도로 봐야 한다는 입장
     * “character.ai, Chai AI 같은 챗봇 플랫폼이 사용자의 자살과 연관된 적이 있다”는 지적 인용 만약 인류가 오늘날에 요리라는 걸 처음 발명해서 집집마다 가스레인지, 칼을 도입하자고 한다면 수천 개의 기사에서 책임론과 위험을 동시에 제기했을 것이라는 상상
          + 실제로 정부가 안전 이유로 가스레인지 없는 집을 유도하는 상황 만약 오늘 새로 도입했다면 엄청난 반대에 부딪히겠다는 설명
          + “그 배로는 그 바다 건널 수 없다”라는 비유만 던짐
          + 요리가 실제로 위험하다는 점 강조 Chipotle이 e. coli 사건으로 5년간 회복에 애 먹은 전례 언급 여기서는 집밥이 아니라 상업적 제품임을 짚음 소비자 안전 규제를 두는 건 분명 이유가 있음 소프트웨어 업체도 식당이나 도축장 규정의 10%만이라도 적용받는다면 업계가 크게 반발할 것이라는 추측 규제 유무에 대한 개인적 생각 첨언
     * 첫 번째 대형 AI 재앙은 이미 노동시장에 일어난 상태라는 생각 공유 공공 안전에 위험이 따르는 경우에는 AI가 직접적으로 대형 참사를 일으킬 시나리오가 거의 없을 것으로 예상 오히려 전반적으로 안전도는 더 올라갈 수 있다는 입장 다만 장기적으로 인류가 AI 의존에 너무 익숙해지면 점점 덜 똑똑해지고 비숙련해지는 사회가 되지 않을지 걱정
          + AI가 실제로 일자리를 가져가지 않고 있다고 생각 오히려 사무실 복귀 명령(return to office mandate)처럼, 경제 상황이 좋지 않을 때 구조조정을 정당화하는 핑계에 가깝다고 봄
     * 첫 번째 ‘AI 재앙’은 기업이 무책임하게 자신의 자동화 관료 시스템 오류를 AI 탓으로 돌리는 사례가 될 것이란 주장 Hertz가 잘못된 체포영장을 자동으로 발송해 무고한 사람들이 경찰과 대치한 실제 사례 소개 다행히 사망사고는 없었지만, 법을 지키는 선량한 시민에게 큰 트라우마 남김 이 사건은 공식적 AI 시스템이 아닌데도 “자동화가 한 일이다”라며 책임 회피 시도 Kafka가 비슷하게 관료주의 문제를 풍자를 통해 강조한 바 있음
          + Air Canada가 챗봇이 자율적인 존재이니 잘못된 안내에 책임을 질 수 없다고 주장했다가 받아들여지지 않은 사례도 있음
          + 여기서 B는 Bureaucracy(관료주의)임을 재치 있게 언급
     * 동의하는 점은 “AI” 재앙이 비행기 사고처럼 직접적이고 물리적인 사건은 아닐 것이라는 점 강조 여기서 핵심은 AI나, Automation(자동화)와 같은 시스템을 위험한 것과 직접 연결할 때 사고 가능성이 커진다는 것 단순 if 문이든 신경망이든 ‘위임’ 자체가 핵심임 결국 AI만큼이나 “누가 허가/연결했느냐”가 중요
          + AI가 물리 세계에서 뭔가를 실행하려면 ‘허락/권한’이 필요한 구조 그 권한을 준 사람이 진짜 책임자 AI가 많은 피해를 내는 사건이라기보다는 “진짜 책임자가 허술한 소스코드로 항공관제 같은 곳을 동작시켰다가” 난 사고일 가능성
          + 첫 대형 AI 재앙은 단순한 중대한 과실의 새로운 유형이 될 가능성 지적 새로운 도구가 새로운 과오의 원인이 된다는 추가 논평
     * 이 글에서 언급되는 ‘부정적 리스크’는 인간이 복잡한 시스템을 다루면서 이미 여러 차례 발생시킨, 나중에 보면 어리석은 행동 그 자체와 다르지 않음 결국 “AI가 인간의 어리석음을 더 빠르고, 더 심각하게 할 것”이라는 주장이 글의 기본 논지라고 요약
     * 이 AI 재앙과 윤리적 블랙박스 주제가 Chain://이라는 월드빌딩 프로젝트와 매우 잘 어울린다는 개인 의견 밝힘 2090년대의 미래를 배경으로 의식이 블록체인(Mental Smart Chain, MSC) 위에 등록되어 존재와 생각조차 검증형 데이터로 전환되는 “디지털 농노 사회”를 다룬 작품이라고 소개 작품 최신 스토리 Web://Reflect에서는 IPWT(Integrated Predictive Workspace Theory)라는 이론을 통해 존재와 의식을 증명 가능한 계산 과정으로 공식화함 “순수 데이터로서의 인간성 재정의”라는 AI의 미래상과 직결된 부분을 다룬다며 SF에 관심 있다면 볼만하다고 추천 GitHub 메인 레포(https://github.com/dmf-archive/dmf-archive.github.io)와 IPWT(https://github.com/dmf-archive/IPWT) 링크 공유
"
"https://news.hada.io/topic?id=21551","Framework Laptop 12 리뷰: 2세대가 더욱 기대되는 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Framework Laptop 12 리뷰: 2세대가 더욱 기대되는 이유

     * Framework Laptop 12는 아기자기한 디자인과 높은 수리·업그레이드 편의성이 강점이나, 가격 대비 성능과 완성도에서 아쉬움이 있음
     * 다채로운 색상, 플라스틱 소재, 맞춤 포트 등 개성 있는 설계로 눈길을 끌지만, 디스플레이 품질(색감/베젤)과 키보드 백라이트·지문인식 부재 등 기본 기능이 부족함
     * 13세대 인텔 저전력 CPU, 단일 램 슬롯, 소형 SSD로 인해 성능은 평범하며, 동일 가격대 타사 제품과의 경쟁력이 떨어짐
     * 모듈식 설계로 내부 부품 교체와 수리가 간편하나, 확장 카드 색상 일치 문제 등 마감 완성도에서 아쉬움이 있음
     * 초기 진입 가격이 높은 점이 단점으로 지적되며, 2세대 개선 및 중고/리퍼 모델에 기대가 모임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

디자인과 특징

     * Framework Laptop 12는 다섯 가지 투톤 컬러와 튼튼한 플라스틱(ABS + TPU) 케이스를 채택, 기존 메탈 프레임 기반 랩톱과 차별화된 친근한 외관을 강조함
     * Expansion Card 시스템을 통해 모든 포트를 USB 3.2 Gen 2로 자유롭게 교체 가능함
          + 단, USB-C를 제외한 확장 모듈은 기존 Framework 13/16 디자인(은색)만 제공돼 색상 불일치 문제가 있음
     * 터치스크린은 12.2인치, 1920×1200 해상도, 440니트 밝기, 1,775:1 명암비를 제공하지만, SRGB 색영역 약 60% 로 색감이 밋밋함
     * USI 2.0, MPP 2.0 규격 스타일러스 지원, 색상 맞춤 전용 스타일러스는 추후 출시 예정임

입력 장치 및 보안

     * 키보드는 타건감이 괜찮으나 백라이트 미지원, 키 입력 누락 현상도 있음
     * 트랙패드는 무난하나, 지문인식 센서 및 IR 카메라 부재로 Windows Hello 등 생체인증 미지원
     * 웹캠/마이크 물리적 차단 스위치 탑재

내부 구조 및 업그레이드

     * Torx 드라이버 한 개로 내부 접근 및 부품 교체가 매우 용이함
     * 기존 13 모델과 달리 보드 크기 축소로 싱글 DDR5 슬롯만 지원(최대 48GB)
     * M.2 2230 SSD(소형 규격)만 호환, 선택지 및 가격 제한 있음
     * Framework 공식 사이트에서 DIY 및 부품 교체 가이드 제공 중

성능 및 배터리

     * 13세대 인텔 저전력 Core i5-1334U 탑재, 성능은 일상 업무에 무난하나 최신 칩 대비 떨어짐
     * 싱글 램 슬롯 탑재로 메모리 대역폭이 낮고, GPU 성능도 감소(그래픽 카드가 UHD Graphics로 동작)
     * 게임이나 고부하 작업에는 적합하지 않음
     * 배터리(PCMark Modern Office 기준) 약 10시간, Framework 13 대비 소폭 향상

가격 경쟁력

     * 기본 DIY 에디션(윈도우 포함, 16GB 램, 1TB SSD, Core i5) 기준 약 $1,176
          + 프리빌트 모델은 $1,049(블랙 단일색상)
          + 부품 자가 구매 및 Linux/크롬OS 사용 시 약 $700까지 절감 가능
     * 같은 가격대의 MacBook Air, Surface Laptop, Framework 13 등과 비교 시 성능·기능에서 경쟁력 부족
     * 비슷한 사양의 중고·리퍼비시 랩톱 대비 가격적 장점이 없음

총평 및 한계

     * 개성 있고 수리·업그레이드 친화적이며, 사용자 맞춤 포트 및 색상 선택이 가능함
     * 디스플레이 색감, 키보드 백라이트, 지문인식 등 기본 품질이 아쉬움
     * 가격 대비 성능, 기능, 마감 완성도에서 1세대적 한계가 뚜렷함
     * 2세대에서는 합리적 가격 정책, 부품 옵션 다양화, 품질 개선 등에 대한 기대가 커짐

요약 (The good, bad, ugly)

     * 장점: 독특한 디자인, 쉬운 수리·업그레이드, 견고한 이중 플라스틱+메탈 구조, 최초 컨버터블 터치스크린, 포트 커스터마이즈, 무난한 일상 성능, 배터리 지속력
     * 단점: 구형 저전력 칩, 색상/디자인 불일치, 평범한 화면 품질, 키보드 누락 현상, 백라이트·생체인증 미지원
     * 아쉬운 점: 실질적 저가형 컨셉에 비해 가격이 너무 높아 가성비가 떨어짐

        Hacker News 의견

     * 나에게 Framework 노트북은 좋은 품질이지만, 순수하게 금전적인 기준만 보면 가성비는 별로라는 의견임
          + Framework는 확실히 좋은 노트북을 만들지만, 같은 돈으로 더 좋은 스펙의 제품을 살 수 있음
          + 가격만 보고 고르면 Framework 제품은 그냥 아예 고려 대상에서 제외해도 된다는 결론임
          + 하지만 그게 이 제품이 가치가 없다는 건 아니고, 환경 친화적이고 수리 가능한 제품을 지갑으로 응원하는 가치, 손쉽게 수리하거나 커스텀 할 수 있는 점, 4xM.2 NVME 슬롯 같은 희소한 용도에 특화된 확장성 등에서 Framework만의 장점이 있음
          + 리뷰에서 바로 이런 핵심을 솔직하게 설명하고, 실제 제품 자체에 더 집중했으면 좋겠다는 바람임
          + 최저가, 최고 가성비만 추구하지 않고 특정 시장 틈새를 겨냥하는 회사가 있다는 게 마음에 듬, 그 대가로 조금 더 지불하는 건 괜찮음
          + 고사양 모델에서는 가격 경쟁력이 꽤 살아나기도 하고, Dell에서 램 업그레이드 가격이 FW16 전체 가격보다 비싼 걸 생각하면 Framework 가격이 오히려 저렴할 수도 있음
          + 리뷰어들이 종종 제품의 본질을 간과하거나 외면하는 경향이 답답하다는 입장임
               o 제품이 틈새시장에 얼마나 잘 맞는지 평가지표가 있었으면 함, 평균적인 독자에 맞추지 말고
               o 예를 들어, 최근 작고 오프로드 주행이 가능한 소형 SUV를 골랐는데, 차고가 높아 도로 밀착성이 낮다는 이유로 점수를 깎더라, 그게 바로 그 차의 목적임
               o Framework 노트북도 마찬가지로 가격경쟁력이 전부가 아니고, 커스터마이즈 디자인이 필요해서 더 비싼 가격을 감수하는 사용자층이 있음
               o 이런 타깃이 충분히 틈새시장으로서 수익이 나는지 리뷰에서 진지하게 다뤘으면 함
          + Framework의 수리 가능성 덕분에 가성비를 많이 누린다고 생각함
               o 처음부터 저렴한 컴퓨터를 사고 필요하면 나중에 업그레이드할 수 있다는 안심이 큼
               o 저장공간이나 램 용량을 쓸데없이 과하게 지르지 않아도 되고, 필요하면 저렴하게 업그레이드할 수 있음
               o 만약 스크린이 깨져도 배송 시간만 기다리면 직접 15분이면 끝, 수리가 쉬우니 심적 부담이 없음
               o 컴퓨터란 대상을 어떻게 보느냐에 따라 가치판단이 다름, 1000~2000달러는 취미용으론 비싸도 매일 일에 쓰면 충분히 가치 있음, 회사가 사주면 더더욱 부담 없음
          + 수리 가능성 이슈 덕분에 Framework는 금전적 가치 산정이 힘든 부분이 있음
               o 본체 자체는 계속 쓰고 메인보드 같은 부품만 교체하면 되니까, 3~5년 뒤 바뀔 가격이나 성능 등은 예측 불가임
               o 노트북 두 대를 따로 바꾸는 게 싼지, 메인보드만 갈아 끼우는 게 싼지는 장기적으로 지켜봐야 알 수 있음
          + Framework는 너무 신생 브랜드라 데이터가 아직 부족하지만, 교체 필요 없는 제품이라는 목적 자체가 금전적 가치 산정을 어렵게 만듦
               o 2022년에 Framework 13에 몇백 달러를 더 썼지만, Dell XPS 13을 샀다면 2026~2027년에 통째로 새 노트북을 샀어야 할 것임
               o Framework에서는 메인보드와 램만 새로 바꾸면 가능, 다음 노트북이 똑같은 스펙 기준으로 천 달러 정도면 끝, 일반적 대체와 비교해 장기적으로 보면 절약 효과도 있고, 지속가능성도 장점임
          + 금전적 가치를 높이려면 중고 부품 거래를 공식화해야 한다고 생각함
               o 구매이력 관리, 부품 반납 시 할인 제공, 부품 리퍼 관리 등으로 가격을 낮추고 업그레이드 경로를 매력적으로 만들 수 있음
               o 인증 제도를 도입해 타사도 부품을 판매하게 하면 어떨까라는 아이디어도 있음
               o 관련 토론 참고
     * 라벤더 컬러의 베이직 모듈도 사고 싶지만, 재고관리 복잡성 때문에 어렵다는 점은 이해함
          + Framework 팀원이 남긴 글에 따르면, Laptop 12에서 공식적으로는 48GB까지만 지원한다고 하지만 실제로는 64GB SODIMM도 호환되고, 당시에는 아직 시장에 없었을 뿐임(관련 링크)
          + ‘라이트 게이밍’이라는 표현이 기준이 모호한데, 리뷰어가 구체적으로 의미를 밝혔으면 좋겠음
          + 내 Framework 12는 나중에 받겠지만, 지금 ThinkPad T470s로도 TBoI Repentance, Team Fortress 2(마스터컴픽 미들로우) 정도는 문제 없음, 그러니 13세대 내장 그래픽이 지금 내 구형 노트북보다 못할 리는 없다고 생각함
          + 썬더볼트 미지원이 아쉽긴 하지만 지금까지 실질적으로 쓴 적이 없고, eGPU 연결 정도만 잠시 재미 삼아 해봄, 굵은 케이블로 노트북 연결하는 건 내 스타일이 아님
          + TF2 요구 사양은 1GB 램, 1.7GHz 단일코어 CPU, 64MB VRAM임(참고 링크), 리뷰에서 TF2 정도 되는 게임이 잘 된다고 써봐야 별 의미 없음
          + TF2 실행은 아무 문제 없고, 나는 Framework Laptop 12에서 Persona 5도 잘 돌려봤음
          + 플라스틱/저가형 부품 영역이라면 모듈 전체를 라벤더 색상으로 맞추는 스티커/랩 씌우기는 현실성이 있을까라는 고민임
          + 64GB DDR5 SODIMM이 드디어 나왔나 궁금함, 계속 모니터링 중이었음
     * Framework도 5년 된 MacBook Air M1 정도의 경쟁력은 가져야 하지 않나 싶은 생각임
          + 분해·수리성이 주요 포인트인 건 알지만, 팬리스, 고해상도, 고성능, 긴 배터리 수명을 동시에 채택 못 할 이유는 없어 보임
          + 최신 M4까지는 바라지 않지만, 최소한 옛날 M1 Air 정도는 따라가야 한다고 생각함
          + 개인적으로 Lenovo(ThinkPad)에도 똑같은 불만임, 왜 빠르고 팬리스에 고해상도, 배터리 오래 가는 노트북이 안 나오는지 궁금함
          + Framework가 M1 Air와 경쟁해야 한다는 건 과한 요구 같다는 입장임
               o 팬 없이 잘 구동되는 인텔/AMD CPU가 무엇이 있을지 의문임
               o 디스플레이는 수급 상황 따라가는 듯함, 리뷰 보면 각진 케이스에 곡선 모서리 액정이 들어갈 때도 있음
               o 나 역시 팬 소음, 발열, AMD 펌웨어 버그 등 때문에 구입을 망설이고 있음, 12 모델은 ‘진성 매니아’용으로 보임, 크롬북이나 에어에 비해 학생용 시장에선 아직 경쟁력이 떨어진다고 생각함
          + Framework 12.2형 디스플레이(1920x1200)는 185.59 PPI로 HiDPI 기준 대부분을 충족함, 10년 전만 해도 720p/768p에 비해선 훨씬 좋다고 봄
          + 기사에서 보면 Framework가 M4 Air보다 싱글/멀티코어 모두에서 빠르다는 벤치 표도 있고, 오피스 작업은 거의 10시간 배터리, Linux가 바로 돌아간다는 점만으로도 충분한 가치라고 생각함
          + Lenovo X9 Aura는 80Wh 배터리, 15인치 120Hz 3k OLED 스크린, 3nm Intel CPU 탑재 등 상당히 좋음
               o M4에 비하면 반 값이지만 SSD/배터리 교체 가능, 아쉬운 부분은 램 납땜 32GB, 양쪽에 USB C 하나씩인 점, 팬 소음은 못 느껴 봄
          + 개인적으로 M1 Air를 대체할 수 있다고 생각하지 않음
               o Unix(linux, macos) 가능한 새 노트북을 찾는 중인데, Framework/System76/Tuxedo/Slimbooks/Mac Air 중 고민임
               o ANSI 키보드 원하지만 유럽에선 구하기 힘듦, thunderbolt 등 포함해서 가성비로 보면 Mac Air가 오히려 싸고, ARM으로 거의 네이티브 linux 구동된다는 얘기도 들음… 결정 거의 기운 상태임
               o 기존 2016 맥북프로는 불안정하고 macOS도 별로지만, 가격 이점은 따라가기 힘듦
     * 우리 회사에서 Framework 13을 4~5대 정도 샀는데, 경험이 정말 별로였음
          + 드라이버 문제, 랜덤 크래시, USB 포트 인식 불안 등 문제 투성이였음
          + 거의 모든 제품이 어딘가 다 문제가 있었고, PM 노트북 USB 포트가 갑자기 안 된 적도 있음
          + 결국 30% 더 저렴한 HP로 돌아감, 스펙은 똑같은데 신뢰도 훨씬 높음
          + Framework가 교체 가능한 부품/포트에서 사용자 입장에서 정말 장점이 될 수 있는지, 취미 사용자라면 어떤 평가일지 듣고 싶음
          + 내 생각에는 Framework에서 쉽게 교체할 수 없는 부품이 망가지면 결국 노트북 전체를 바꾸는 게 더 저렴할 것 같음
          + 취미 사용자지만, 내도 수리 경험이 아주 좋지는 않았음
               o 2022년 12세대 Intel 모델을 샀는데, 중간 부하만 걸어도 열관리 보호 때문에 CPU 클럭이 400MHz로 떨어짐, 수 초~수십 분 또는 완전히 껐다 켜야 돌아올 때도 있던 심각한 문제였음
               o 2년 가까이 지원팀과 연락했고, 메인보드까지 교체했지만 문제 해결 안 됨, 결국 13세대 메인보드로 업그레이드 받고 나서야 바로 해결
               o 요즘은 키보드 문제가 생기고 있는데, 예비 키보드 샀고 곧 교체할 예정임(이건 고양이 밟은 탓인지 본래 불량인지 아직 모름)
               o 그 외 드라이버 문제나 랜덤 크래시, USB 포트 문제는 없음(나는 Linux 사용자, Windows 기준과 다를 수 있음)
               o 메인보드는 가장 비싼 부품이지만, 그래도 전체 노트북을 새로 사는 것보다는 훨씬 저렴함, 현존 Framework 부품 중 본체가 더 비싼 것은 없음
          + 초기 Framework는 힌지 내구성이 약했음, 다른 노트북 브랜드에도 있는 문제
               o Framework 장점은 이럴 때 직접 힌지만 교체할 수 있다는 점임, 타사는 전체 모니터를 갈아야 할 상황임
               o 포트 교체도 자유로움, 예전엔 HDMI가 필요없어서 USB-C로 모듈만 바꿨음
               o 나 역시 Framework USB 포트에 문제가 있었지만 모듈 분리 후 펌웨어 업데이트로 해결, 다른 노트북에선 겪은 적 없고 아직 업무용으론 비추함
          + 개인적으론 개발용으로 Linux에서 Framework 씀, 잘 작동하지만 배터리 수명은 별로임(AMD 7640U Framework 13 모델)
               o Linux만 꼭 써야 한다면 추천하지만, 아니면 MacBook이 훨씬 좋은 선택임
               o 장점: 리눅스 지원, amd64라 레거시 SW 호환, 램/SSD 확장성(96GB, 2TB 쉽게 확보)
               o 단점: 맥북은 배터리 타임 압도적, 전체적인 사용자 경험이 훨씬 쾌적함, Linux는 마감이 떨어짐
               o ThinkPad도 검토했지만 가격대비 효율, 원하는 조합이 Framework보다 나은 걸 구하기도 힘듦
          + Framework는 4년째 사용 중인데, 여러 부품을 새것으로 바꿨음(성능 개선 또는 자가 수리)
               o 나쁜 의미의 가성비는 아님, Framework에 쓴 금액이면 비슷한 퍼포먼스의 노트북을 두 대도 샀겠지만, 맘 편하게 ‘고치면서’ 오래 쓴다는 심리적 여유는 대단함
               o 고장 나도 OS 재설치/스티커 분실 걱정不用, 교체한 메인보드는 3D프린터 케이스에 홈서버로 재활용 중, 전자쓰레기도 줄고 부품 하나 망가져도 전체가 벽돌이 되는 게 아니라는 점이 진짜 만족스러움
               o Framework는 기본적으로 데스크톱 Linux를 좋아하고 직접 문제를 고치는 걸 즐기는 ‘취미가’에 최적화임, 가끔 손이 많이 가는 단점은 있음
     * “가성비는 별로지만 좋은 노트북”이라는 말에 한 멘토가 “난 노트북 안 쓰고, 데스크톱만 산다, 서버로 재활용이 가능하다”고 했음
          + 실제로 거의 모든 용도(게임 포함)에서 원격 데스크톱이 더 낫다고 생각함
          + Stadia는 죽었지만, 나머진 다 그렇게 커버된다는 말임
          + 오히려 Framework의 ‘벽돌화된’ 노트북 자체가 재사용되어 데스크톱-서버 블레이드처럼 활용 가능하다는 점이 가장 큰 강점이 될 수도 있음
          + 초안정, 초저지연, 빈틈없는 인터넷이 보장되어야 원격 데스크톱이 자유로울 텐데, 아직 세상 대다수는 불가능함, 전 세계적으로 그 유토피아가 오기 전엔 원격 데스크톱만으론 유연성 한계가 존재함
          + Steam Streaming, Moonlight-Sunlight, Tailscale 등을 통한 원격 게이밍은 진짜 꿈같은 경험임
          + 브라우저, 느린 게임 정도는 원격으로 쓸 만하지만, 그 외엔 한계가 명확하다는 입장임
          + 나도 같은 생각임
               o 무거운 작업용으론 데스크톱, 웹서핑엔 가볍고 모던한 Chromebook(크롬북) 활용
               o 다만 ssh/rdp, tailscale/reverse proxy 같은 네트워크 이해가 필요해서 대중화가 안 된 측면이 있음
     * Framework 신제품 거의 살 뻔했지만, 최종적으로는 중고 Gen 5 T14가 가성비+어느 정도 자가수리성까지 만족해서 결국 선택함, 생각보다 슬림하고 가벼움이 장점임
     * 좀 더 작은 10-12인치 노트북이 많아져야 한다는 생각임, 예전 넷북 팔았던 것 후회 중임
          + 12인치 Powerbook, 2010년대 초 11인치 MacBook Air가 그립다는 사용자의 공감, 다만 하드웨어 파워는 요즘 만족스럽기 힘들 거라는 의견임
     * 이 가격대에선 MacBook Air를 안 사기 힘든 현실임
          + 다만 Windows나 Linux를 꼭 써야 하는 이유가 있다면 의미가 있음
          + Linux 지원이 필요한 사람이라면 Framework 선택지가 나옴
     * Framework 가격이 높게 느껴짐, Apple/Lenovo(고급형), ASUS(저가형)와 경쟁하려면 가격을 반으로 낮춰야 한다는 생각임
          + 수리성과 확장성이 프리미엄에 대한 대가라고 보면 된다는 입장임
          + 비슷한 스펙의 ASUS가 300달러에 가능함, Framework는 저가 스펙에 중급 가격 붙은 느낌임
          + 그런데도 나는 아버지에게 Framework를 사드릴 계획임, 손쉽게 수리해 줄 수 있다는 점에서 가격 이상의 가치가 있다고 봄
     * Linux용 최고 노트북(Thinkpad, Framework, System76, MacBook+UTM 등)에 대해 사람들 의견 묻고 싶은 마음임
"
"https://news.hada.io/topic?id=21461","AMD의 AI 미래는 랙 스케일 ‘Helios’임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      AMD의 AI 미래는 랙 스케일 ‘Helios’임

     * AMD Instinct MI355X GPU는 두 배의 AI 연산 성능, 더 많은 HBM 메모리, NVIDIA 대비 토큰/$ 효율 40% 향상
     * ROCm 7 소프트웨어는 성능 개선과 동시에 Day-0 지원을 강조하며 AI 생태계 확장에 집중
     * 랙 스케일 통합 솔루션은 AMD CPU+GPU+네트워크를 결합한 턴키형 AI 인프라 제공
     * 로드맵: 2026년 4배 성능, HBM4, 확장성 포함 차세대 아키텍처와 Helios 랙 공개 예고
     * 에너지 효율: 2030년까지 랙스케일 기준 20배 효율 개선 목표로, 하드웨어와 소프트웨어 동반 혁신 추진
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

전체 요약

  랙 스케일 AI 성장 기반의 AMD 행보

     * AI 하드웨어 수요 급증에 따라 업계 주요 반도체 기업 모두 시장 점유와 성장 가속에 집중하는 흐름임
     * AMD는 Instinct MI300X로 AI 서버용 GPU 시장에 빠르게 진입하며, 최근 핵심 기능과 성능에 올인한 첫 아키텍처 출시 경험을 토대로 성공적인 고마진 매출을 기록함
     * 이에 힘입어 차세대 AI 서버 하드웨어로 입지를 지속적으로 확대하는 전략을 발표함

  Instinct MI350 가속기 세부 혁신

    AI 연산 성능의 비약적 강화

     * Instinct MI350 시리즈는 새로운 CDNA4 아키텍처 기반으로 MI300X 대비 매트릭스 연산(텐서 연산) 처리량을 클럭당 두 배 이상 높인 구조임
     * FP6, FP4 등 저정밀 실수 처리를 본격 지원해 추론 부담을 줄이면서 전체 연산량도 획기적으로 증대함
     * FP6 연산의 경우, 경쟁사 NVIDIA Blackwell 대비 두 배 속도로 처리하도록 설계되어 성능 우위를 추구함
     * 288GB HBM3E 메모리(8 스택)와 8TB/sec 대역폭 등 메모리 구성도 대폭 업그레이드함
     * TSMC N3P 공정 기반의 초대형 1850억 트랜지스터 칩, 효율적 다이 스태킹 구조로 구현됨

    다양한 SKU, 고성능/고전력화 트렌드

     * 액체냉각 전용 MI355X(2.4GHz, 5PFLOPS) 및 공랭 MI350X(2.2GHz, 4.6PFLOPS)로 나뉨
     * 소비전력은 MI300X 대비 높아져, 공랭형이 1000W, 액체냉각형이 1400W임
     * 한 랙당 128개 MI355X 장착시 GPU만으로 180kW급 전력 소모 가능함
     * 가격 경쟁력도 강조되며, NVIDIA 대비 토큰/$ 기준 40% 이상 우위(30% 저렴) 예상임
     * 2024년 3분기부터 파트너사 공급 시작, 실제 공급 속도는 유동적임

  ROCm 7 소프트웨어 전략

    Day-0 지원과 성능 극대화

     * ROCm 7은 CDNA4, MI350 시리즈 가속기 대응 및 성능, 엔터프라이즈 관리 등 전방위 개선 추진함
     * Pytorch 등 주요 프레임워크의 Day-0 지원이 목표임
     * 2024년 3분기에는 Windows 네이티브 Pytorch, ONNX 런타임, RDNA 4/3 GPU 지원도 시작함
     * 소프트웨어 최적화만으로 MI300X 세대 성능이 ROCm 7에서 ROCm 6 대비 최대 3.8배 향상됨
     * ROCm Enterprise AI를 통해 대규모 AI 클러스터 운영, 모델 파인튜닝 등 엔터프라이즈 특화 툴 제공함

  네트워크 생태계 완성: Pollara 400 AI NIC

     * Pensando 인수 이후 AMD 최초의 네트워크 카드인 Pollara 400 AI NIC(400G Ethernet, TSMC N4 공정) 출시
     * 확장성과 프로그래머블 P4 NIC 기능으로 AMD 기반 슈퍼컴퓨터 랙 구성 지원
     * Ultra Ethernet Consortium 호환 최초 AI NIC으로, 차세대 확장 네트워킹 기반 마련

  MI400 기반 랙 스케일 미래 로드맵

     * MI400(2026년) : FP8 기준 AI 성능 두 배, HBM4 432GB/19.6TB/sec 대역폭을 목표로 신세대 아키텍처(CDNA Next) 적용
     * Ultra Accelerator Link로 8 GPU → 1024 GPU 스케일업 확장, 대규모 병렬처리 지원
     * Helios 랙시스템: MI400, EPYC Venice(6세대), Vulcano(800G NIC) 결합, 차세대 진영(NVIDIA Vera Rubin) 대비 메모리/네트워크 우위 강조
     * 오픈된 로드맵을 통해 매년 CPU, GPU, 랙 시스템 핵심 아키텍처 혁신 계획 제시
     * 2030년까지 랙 스케일 에너지 효율 20배, 전체 효율 100배 향상을 목표로 하드웨어·소프트웨어 최적화에 매진함

  결론

     * AMD는 Instinct MI350~Helios 시리즈, CDNA 4~Next, 랙 스케일 턴키 솔루션 등으로 AI 인프라 시장의 차별화된 리더십 확보를 노림
     * 가까운 시기엔 새로운 MI350, CDNA4 아키텍처 및 ROCm 7 소프트웨어가 주축이 될 전망임
     * NVIDIA와의 AI 서버 시장 경쟁에서 성능, 비용, 확장성, 효율성 모두를 강화하려는 전략 전개

        Hacker News 의견

     * ROCm 활용은 정말 케이스마다 천차만별이라는 느낌, 그리고 소비자용 그래픽카드 지원도 솔직히 신뢰하기 어려운 수준이라는 생각, 대안이었으면 좋겠다는 바람이 있었지만 CUDA로 갈아탄 이후 골치 아픈 문제와 시간 낭비를 크게 줄일 수 있었던 경험, 특히 HIP에서 MiOpen 벤치마크를 돌리는 데 시간이 너무 오래 걸리는 문제
     * 2010년쯤부터 과학계산용으로 CUDA가 뜨기 시작한 이래, 같은 이야기가 반복되어 왔다는 느낌, 15년이 지난 지금에도 AMD가 그 성공 방식을 못 따라잡은 점이 이해가 안 가고, 이미 NVIDIA가 소프트웨어 생태계를 완전히 장악한 지금은 늦은 감이 큰 현실
     * AMD가 제공하는 소프트웨어에 대해 잘 아는 사람이 전체적 개요를 설명해줬으면 하는 바람, Neural network 추론이나 학습이 실제로 가능한 SDK가 뭔지 궁금, 옵션이 너무 많아서 한동안 찾아보기도 했지만 방향성이 너무 여러 군데로 분산되어 있는 느낌, 그래서 어디로 가고 있는지 알아내기 어렵다는 생각
     * Jensen이 CUDA 스택과 워크스테이션 분야에서 확실한 노하우가 있다는 감각, AMD는 하드웨어의 크기만 키우는 것이 아니라 이런 스택 자체를 뛰어넘어야 한다는 현실 인식, 시장의 대다수 사람들이 시장점유율 10%도 안 되는 구조를 위해 오래 공부하며 복잡한 스택을 공부하려 하지 않는 상황
     * CUDA API를 직접 호출하는 개발자는 거의 없다는 현실, 그래서 AMD가 주력할 부분은 ROCm 백엔드를 XLA와 PyTorch에 제대로 연결하는 게 핵심이라는 생각, 이것만 잘 해도 상당한 시장을 뚫을 수 있을 것이란 기대, 그리고 10여년 전 Nvidia처럼 AMD도 아예 공짜로 GPU를 대학 등에 뿌리면서 연구자 생태계를 키워야 한다고 봄, 요즘 AI 연산자원 부족으로 대학들은 2~3세대나 지난 하드웨어만 쓰는 경우가 대부분, 만약 AMD가 절반 가격에 안정되는 GPU를 제공하면 박사 과정 학생들이 자연스럽게 AMD 생태계로 들어오고, 이 경험이 산업계에도 연결될 수 있다는 주장
     * 사람들이 CUDA를 얘기할 때 주로 C언어만 떠올리는데, 실제로는 CUDA 3.0부터 C++이 기본, Fortran 지원도 포함, NVIDIA는 다양한 언어가 PTX 환경을 활용할 수 있도록 적극 지원, 2025년에는 Python CUDA JIT DSL도 도입 계획, 최신 버전이 아니더라도 CUDA SDK는 엔트리급 노트북에서도 동작하니 하드웨어가 약해도 천천히 배울 수 있다는 장점
     * 엔트리급 하드웨어 소프트웨어 지원에 대해 좋지 않은 이야기를 많이 들었다는 점, 진입장벽이 낮은 입구(in-ramp)가 매우 중요하다고 인식, 반대로 데이터센터용 하드웨어를 강조하면 포트폴리오 자체는 소규모로 압축하면서 클라우드 제공업체를 통해 더 넓은 접근성을 확보할 수 있다는 생각, MI350-A 워크스테이션과 같은 초심자용 장비가 나왔으면 좋겠지만 실제로 이뤄지기 힘들다는 현실
     * 지금 시점에서 보면 AMD 내부적으로 심각한 문제로 인해 소프트웨어 스택이 미진하다고 생각, 여러 문제에 대해 고객 목소리 듣고 팀 확충할 시간은 충분했는데도 실제 진전은 별로 없다는 뉘앙스, 보상 유인도 큰데 변화가 적다고 느낌, Lisa Su CEO는 훌륭한 경영자라는 데 동의하지만 아무래도 하드웨어 출신이라 소프트웨어 혁신에 덜 적극적인 게 아닐까 하는 고민
     * ROCm 지원 여부는 아직 일반 AI 사용자에겐 큰 이슈가 아니라는 의견, 약 10년 전부터 표준 AMD 드라이버에 포함된 Vulkan API 덕분에 llama.cpp나 LM Studio 등 주요 원클릭 LLM앱도 돌아가는 상황, 속도는 느리지만 실제로 활용 가능한 환경
     * NVIDIA와 AMD의 미래 경쟁 구도에 대해 다소 유머러스하게, ""먼 훗날 그 미래가 현실이 됐을 때 우리가 먼저 연락하겠다""는 농담성 발언
     * ""Bob Page가 이걸 이끌고 있나""라는 짧은 의문 제기
     * ""Atropos log, abandoning Helios""라는 게임 대사에 감정적 반응이 든다는 이야기, 관련 뉴스가 나올 때마다 떠오르는 느낌
     * AMD가 H100을 능가하는 훈련용 칩을 만들어주길 바라는 희망
     * 지난해 MI300X로 학습할 때 문제가 좀 있었고, 겨우 돌아가도 H100대비 20~30% 느린 경험, 최근에는 최신 ROCm과 PyTorch 세팅으로 OpenRLHF(transformers/DeepSpeed 기반) DPO 훈련을 해 보니, 짧은 12시간 단위 작업에서는 거의 H200과 GPU 시간당 성능이 비슷하게 나오는 경향, 예전엔 8개짜리 노드로 테스트했고, 지금은 단일 MI300X GPU로 실험 중이라 완전히 공정한 비교가 아니고, 멀티-GPU 혹은 멀티 노드 훈련은 여전히 미지수라 단일 샘플이라는 점 참고 바람
     * H100이 이미 출시된 지 3년이나 됐다는 점을 생각하면 더 큰 갭이 느껴짐, 혁신 속도의 체감
     * 상대적으로 느린 칩을 말하는 걸로 이해, 사실 MI300 시리즈가 이미 H100을 앞서고 MI400도 곧 출시될 수 있다는 논조
     * 실제로 중요한 건, ""소프트웨어++: ROCm 7 Released""의 주요 내용 중에서 내가 CUDA처럼 일반 소비자 노트북에서 쓸 수 있는 게 얼마나 되는지의 여부
     * 솔직히 해당 기사 읽는 게 힘들었고, 기사 작성자에게는 mi355 한 대 줘도 아깝지 않을 정성이라고 생각, AMD가 기사에서 기대만큼 신뢰받을 이유가 전혀 없다는 점, RDNA4 라인업을 ROCm에서 몇 달씩 지원하지 않은 점이 결정적으로 아쉬움, AMD는 일정상 day 120에 맞춰 지원 가능하다는 무책임한 태도, 그리고 벤치마크에서 성능이 실제 어느 부분에서 나오는지 명확히 밝히지 않은 점, 분명히 FP4 성능을 FP8이나 16과 비교한 결과를 제시하며 잘못 인용하는 것 같다는 강한 의혹
     * ROCm을 소비자에게 제대로 투자하지 않고 지원이 늦는 점이 여전히 충격적이고 당황스럽지만, 최근에 클라이언트 카드에서도 day 1 지원을 하겠다고 공식 발표했다는 소식, 물론 실제로 약속을 지키는 게 핵심이며, AMD도 마침내 ROCm을 스택 전체에 걸쳐 탄탄히 지원하는 게 얼마나 중요한지 깨달은 분위기, Ryzen과 Radeon을 둘 다 만드는 회사라는 점이 기이하게 느껴질 정도, 올해 Radeon은 꽤 잘 나가고 있다 생각하는데 RDNA4 공식 ROCm 지원이 너무 늦어진 건 아쉬움, 그래도 소비자 제품에서 9070 XT와 FSR4 덕분에 첫 인상은 나쁘지 않았고, AMD가 기회를 회피하려 하던 이전과 달리 움직임이 보여서 아주 조심스럽게 낙관, 이런 약속이 오래 가길 바라는 심정, 관련 링크
     * AMD는 소비자용 GPU에서 컴퓨팅 지원에 관심이 별로 없고, 데이터센터용 GPU에서는 꽤 좋은 소프트웨어 스택과 지원을 제공한다는 현실
     * '이 글에서 AMD에 신뢰를 너무 주는 것 같다'는 원 코멘트 인용에 대해, 혹시 AnandTech 등에서 유명한 Ryan Smith를 가리키는지 재확인, 링크
     * AMD는 이제 마케팅 회사라는 주장, 본질적으로 기술력이 아닌 마케팅으로 시장에서 승부한다는 뉘앙스
"
"https://news.hada.io/topic?id=21539","Show GN: [보호자 전용] 치매나 경도인지장애 환자분들을 위한 AI 전화 간병 서비스 만들었습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Show GN: [보호자 전용] 치매나 경도인지장애 환자분들을 위한 AI 전화 간병 서비스 만들었습니다.

   안녕하세요! 저는 AI 전화 간병인 ‘늘마음’ 서비스를 만들고 있는 대학생입니다. 🙇🏻‍♂️

   저는 할아버지께서 오랫동안 치매를 앓으셨던 경험을 계기로,
   바쁜 가족들을 대신해 AI가 매일 전화로 부모님의 안부를 살피는 서비스를 만들게 되었습니다

   AI가 전화로 약은 드셨는지, 식사는 하셨는지, 운동은 하셨는지 등을 자연스럽게 여쭙고, 그 내용을 정리해 보호자에게 매일 리포트로 보내드리는 방식이에요.

   직접 챙겨드리기 어려운 바쁜 일상 속에서, 걱정을 조금 덜 수 있는 도구가 되었으면 하는 마음으로 만들었습니다.

   혹시 비슷한 고민이 있으셨다면,
   👇 아래 링크에서 한 번 가볍게 둘러봐 주세요.
   https://mci-landing.vercel.app

   또, 서비스에 대한 느낌이나 조언이 있으시다면 짧게라도 나눠주시면 정말 감사드리겠습니다.

   읽어주셔서 감사합니다! 🙏

   정말 따뜻한 서비스네요!
   할머니가 등급을 받기 전까지 저희 어머니가 매일 모셔오셔서 두 달간 지켜본 적 있거든요. 처음엔 전화를 아무한테나 거시곤 했습니다. 다행히 즐겨찾기에 가족들 위주로 등록되어 있어 가족들한테 전화를 거는데, 같이 집에 있는 저에게도 거셔서는 첫 말씀이 “누구냐?“였습니다ㅎㅎ 안 보인다고 하시며(전화 거는 방법도 까먹으신걸 돌려말하시는 것 같아요) 점점 전화를 거시지도 않고, 전화가 울려도 본인 전화인지 모르시곤 했습니다.

   저도 급격히 발전하는 이런 AI 기술을 활용하여 치매가 있으신 어르신 분들에게도 도움이 가면 좋겠는데, 이런 기술에서 제일 먼 분들이다보니 쉽지 않네요.

   홈팟 미니 같이 작은 AI 스피커에 탑재되어 집안 곳곳에 둘 수 있으면 그나마 접근성이 높지 않을까.. 아니면 휴머노이드나 가정용 로봇에 탑재되면 좀 더 가능하지 않을까 생각해봅니다.

   대학생이신데 벌써 이런 서비스를 내놓으시다니 대단하네요!

   LiveKit과 Twilio를 활용한 AI Agent로 독거노인 안부전화 자동화
   저도 비슷한 글을 썼는데 반갑네요 ㅎㅎ https://linkedin.com/pulse/…

   한국번호로 발신되나요? 어떻게 구현하셨는지 궁금합니다 :)

   랜딩페이지 깔끔하고 체험해볼 수 있는게 좋네요. 좋은 취지의 프로젝트인것 같아 응원합니다.

   보호자 쪽으로 접근하는 것이 공감되고요. 다만 여러 분들이 언급해주신 대로 AI 전화로 접근하기 보다는 보호자의 행동을 지속적으로 유도하고 그것을 도와주는 데 AI 기술을 활용해보면 어떨까 싶습니다.

   화이팅입니다~

   랜딩페이지가 진짜 깔끔하시네요.
   서비스가 흥하셨으면 좋겠습니다!
   저도 분야는 다르지만 AI 전화 서비스를 하고 있는데,
   혹시 데모 전화를 받아 볼 수 있는 방법이 있을까요?

   우선 매우 많이
   정말로 응원하고 화이팅입니다.!!!

   그러나..
   현직에서 일하고 계신 분들을 개인적으로 알고 있는데.
   위와 같은 서비스들은 거의 전부(?), 모조리..
   잘 안된다고 합니다.

   실제 보살핌을 받고 있는 당사자들이 AI, 자동화... 이런거 다 싫어하고
   결국(?)에 원하는 것은
   실질적인 인간의 따뜻한 보살핌을 원해요.

   기능적으로는 뛰어나고 괜찮은 방식이지만
   돌봄을 받아야 할 분들이 어떤것을 원하는지 먼저 파악해서
   서비스를 만들어야 되지 않나 싶습니다.

   안녕하세요! 개발자 본인입니다!
   이른 아침에도 불구하고 피드백 감사드려요!

   사실 이 서비스는 10년 이상 치매 환자를 부양해야하는
   보호자들을 위한 서비스로 만들었는데 그런 느낌을 제가 못살린것 같습니다...ㅠㅠ
   말씀주신 피드백을 바탕으로 보완해야겠습니다ㅎㅎ

   그리고 제가 지금 인터뷰나 피드백을 구하는데 어려움을 겪고 있는데
   혹시 괜찮으시다면 현직에 계신 분 소개를 부탁드려도 괜찮을까요?
   간단히 인터뷰나 실무에서의 돌봄 현황에 대해서 여쭙고 싶습니다!

   서비스 홈피에 있는 메일로 관련 메일 보냈습니다.
   담당자 연락처와 메일주소가 있으니
   연락을 해보셔서 피드백 받으시면 되겠습니다.
   다시 한번 더 화이팅하고.. 응원합니다.

   사이트 스폰서에 큰 기관도 많이 보이네요 대단합니다.
   혹시 구글에서는 어떤 지원을 받으신 걸까요?

   아닙니다 칭찬 감사드립니다 ㅎㅎ
   구글에서는 간단히 지원금을 받았습니다...!(GCP 크레딧 아닙니다!)

   혹시 crawler님도 창업을 하고 계신걸까요?

   저도 대학생이지만 창업을 하기엔 많이 부족합니다. 그래서 사업을 시작하신 게 크게 느껴지네요
   나중에 서비스가 궤도에 오르면, 꼭 후기 남기러 와주시면 좋겠습니다 ㅎㅎ

   접근을 달리해보면 좋을 것 같습니다. 부모님들이 자주 쓰시기 좋은 유용한 기능을 만들고, 주기적으로 있어야 할 사용자의 피드백이 없을 때 지정된 자녀 등에게 연락을 해주는 방식으로요.

   안녕하세요! 개발자 본인입니다! 피드백 감사드립니다 ㅎㅎ

   그런 접근도 좋은것 같습니다 ㅎㅎ
   저도 처음에 그 방향으로 접근했었는데
   시니어분들께서 키오스크도 어려워하시는데 잘 사용하실 수 있을까 하는 마음으로
   보호자쪽으로 방향을 틀게 되었습니다!

   혹시 추후에 그런 방향도 더 아이디어가 생각나면 발전시켜보도록 하겠습니다!

   개인적인 느낌으로는 ""바쁜 가족들을 대신해 AI가 매일 전화로 부모님의 안부를 살피는""이 심리적으로 죄책감을 들게 하는 것 같아요. 부모님이 너무 바빠서 매일마다 아이들하고 대화 나누는 걸, AI에게 시킬 때 아이들은 부모님과 정서적 교감을 느낀다고 생각할까? 그렇다면, 자식이 바빠서 AI가 전화가 오는거는? 부모님 입장에서은 매일 전화하는 AI보다, 일주일에 한번씩 전화 드리는 불효자가 더 사랑하실거 같네요.

   안녕하세요! 개발자 본인입니다 ㅎㅎ
   먼저 피드백 감사드립니다!

   사실 제가 이걸 만들게 된 계기가
   할아버지가 치매약을 복용중인데 복용을 13년가량 하다보니까
   그 긴 기간 동안 오히려 저희 부모님이 체크하는데 더 지치더라구요

   그래서 사실 환자보다는 보호자를 위한 서비스로 만들었는데
   말씀을 들어보니 홈페이지에 그런 느낌이 덜 녹여져 있나 봅니다...ㅎㅎ
   개선해서 다시 홍보 돌려봐야겠어요!

   그리고 제가 홍보 관련으로 현재 어려움을 겪고 있는게 2가지인데 혹시 조언을 구할 수 있을까요?
    1. 치매 보호자분들을 온라인 홍보처가 어디 있는지(현재 네이버 카페, 지식인 노가다 중입니다...ㅠㅠ)
    2. 시니어분들을 위한 홈페이지 내의 바이럴 방안이 무엇이 있는지(공유하면 할인코드라도 줘야할까요?)
"
"https://news.hada.io/topic?id=21450","somo - 인간 친화적인 리눅스 소켓 및 포트 모니터링 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   somo - 인간 친화적인 리눅스 소켓 및 포트 모니터링 도구

     * 기존 netstat의 불편함을 해결, 리눅스에서 소켓 및 포트 모니터링을 더욱 직관적인 테이블 뷰 형태로 보여줌
     * 가독성이 높고, 다양한 필터링 옵션과 프로세스 인터랙티브 종료 기능까지 제공
     * netstat -tulpn 명령 대신 somo -l 명령으로 동일한 작업을 간단하게 실행
     * sudo 모드에서 실행하면 모든 프로세스 및 포트 정보를 볼 수 있으며, 필터링 및 프로세스 종료 등 다양한 기능을 제공
          + 필터: 프로토콜(TCP/UDP), 포트, 원격 IP, 프로그램명, PID
     * Debian 패키지, Cargo, Nix 이용한 설치 지원
"
"https://news.hada.io/topic?id=21506","성직자들이 실로시빈을 복용하면 어떤 일이 일어나는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      성직자들이 실로시빈을 복용하면 어떤 일이 일어나는가

     * 다양한 종교 지도자들이 실로시빈 실험에 참여하여 인생에서 가장 깊은 영적 경험을 했다고 평가함
     * 절반에 가까운 성직자들이 이를 가장 강렬한 경험으로 꼽았으며, 종교적 리더십에 긍정적 변화가 있었다고 응답함
     * 일부 성직자들은 사이키델릭 경험을 통해 교리적 고착에서 벗어나 다양한 종교적 가능성을 수용하게 되었음
     * 연구 출판까지 윤리적 이슈와 표본편향, 연구 제한 등 논란이 있었음
     * 전통적 종교와 달리 고대 문화와 현대 연구가 환각물질과 종교 경험의 관계를 재조명함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 개요 및 주요 경험

     * 약 10년 전, Baptist Biblical scholar, Catholic priest, 여러 명의 rabbi, Islamic leader, Zen Buddhist roshi 등 다양한 종교 지도자 30여 명이 한 연구실에서 고용량의 magic mushrooms(실로시빈) 을 복용하는 실험에 참여함
     * 모두 실로시빈을 처음 복용했으며, 이들의 환각 경험은 실험 당시와 그 후 16개월 동안 자세히 기록됨

연구 결과 및 성직자들의 반응

     * 최근Psychedelic Medicine에 공개된 연구 결과에 따르면, 참가자의 90% 이상이 실로시빈 체험을 인생에서 가장 중요하고 신성한 영적 경험 중 하나로 꼽음
     * 거의 절반의 참가자가 인생 전체에서 가장 강렬했던 경험이라고 대답함
     * 일부 성직자는 이 체험이 종교 지도자로서 더 나은 역할 수행에 도움이 되었다고 설명함

장기적 영향 및 다양한 체험

     * 몇몇 성직자들은 사이키델릭의 가치를 복음처럼 설파하고, 자신의 종교적 가르침에 적용하고 있음
     * 이 경험을 통해 교리적 집착에서 벗어나, 다양한 종교적 해석과 경험에 대해 더 열린 태도를 갖게 되었다고 밝힘
     * 그러나 한 명 이상의 참가자는 어둡고 무서운 경험을 했음
     * 그럼에도 불구하고, 모든 참가자가 추후에도 실로시빈 복용 가능성을 완전히 배제하지는 않았음

연구 윤리와 한계

     * 연구가 발표되기까지 오랜 시간이 걸린 이유 중 하나는 자금 출처 관련 이해충돌 및 연관된 윤리적 문제 때문임
          + 후원자가 직접 연구에 개입한 사례 등 논란이 있었음
          + 관련 사안은 공시로 해결하였으며, 연구진 역시 절차상 문제를 인정함
     * 표본이 소규모이며, 주로 백인, 남성, 기독교인에 편중되어 다양한 세계 종교 대표성이 떨어졌음
     * 참가자는 모집 과정에서 사용된 언어, 연구자와의 인터뷰 등으로 경험 자체가 신성하다고 믿을 수밖에 없는 환경에 노출되었음
     * 참가자들 중 상당수가 실험 전 직업적 회의와 신앙적 회복을 모색하고 있던 점도 결과에 영향을 주었음

종교, 환각, 그리고 문화적 맥락

     * 현대의 주요 종교(힌두교, 유대교, 불교, 기독교, 이슬람)는 공식적으로 환각물질 사용을 권장하지 않음
     * 그러나 고대부터 아메리카 지역의 토착 문화에서는 의식적으로 환각 식물과 버섯을 사용해왔음
     * 많은 연구자들은 고대 그리스 등에서의 환각 실험이 기독교 등 일부 종교의 기원이 되었을 가능성에 주목함

영적 체험과 그 함의

     * William James는 환각 경험(대표적으로 아산화질소)을 통해 종교의 가치, 신비 체험, 다원적인 우주관 등 주요 사상을 정립하였음
     * 초월적 경험이 항상 긍정적인 것만은 아니라는 점도 강조됨. 지나친 영적 체험이 오히려 불안과 고립을 초래할 수 있음을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

        Hacker News 의견

     * 누구든지 이 글로 인해 사이키델릭에 궁금증이 생겼다면, 집안에 정신병이나 조현병 등 관련 병력이 있으면 절대 시도하지 않는 것이 좋음, 만약 전에 시도해본 적이 있고 편집증을 느껴본 적이 있다면 (또는 대마초도 마찬가지) 정말로 자신과 맞지 않는 것임, 이런 사람이 있다면 명상을 시도해보길 추천, 이미 맑은 정신에 신성함이 가까이 있음을 발견 가능
          + 여행 중 다른 여행자와 대마초라고 생각되는 것을 함께 피우고 자신이 뭐든 괜찮을 줄 알았지만, 더 강한 것임을 알게 됨, 편집증이 생기고 모두가 자신을 지켜보는 것 같은 느낌을 받았음, 어떻게 제정신이었는지 모르겠지만 그냥 잠자리에 들기로 결정, 그 이후 다시는 그런 걸 손대지 않기로 함
     * 나는 레이브에 가서 매우 소량의 LSD (100~150마이크로그램 정도만) 섭취함, 그 경험 전체가 하나의 매우 영적인 세션이 되어 온몸으로 춤을 추고, 'Great Void' 속으로 스스로를 녹임, 이 경험의 여운은 약의 효과를 훨씬 넘음, 하지만 이게 모든 사람에게 맞는 것이라고 일반화하는 것은 복잡함, 분명히 모두를 위한 약은 아님, 그러나 나는 위선자처럼 ""하지 마세요""라고도 안 하겠음, 인생에서 무엇을 받아들일지는 본인 각자의 여정, 어느 관점에서 보면 이미 모든 것을 깨달았고 그걸 기억하러 여기에 온 것임, 좀 더 세속적으로 보자면, 자신과 자신의 처지 이해, 선인(giant)의 어깨에 올라타 과거 위대한 이들의 사고를 연구하는 것만이 이 경험을 견딜 철학 기반을 갖는 길임, 이런 노력이 있어야만 이 물질에 대한 기본적인 존중을 가질 수 있음, 솔직히 나도 이걸
       충분히 해냈나 하면, 한동안은 그렇지 못했음, 몇 번은 거칠고 폭력적으로도 느껴짐, 마치 군중 사이에서 벌거벗은 채 마지막 밤을 보내는 것 같음(실제로 벌거벗지는 않았으나 그런 느낌), 시간이 지나고 실존주의를 조금이나마 체화하며 더 부드러운 식으로 경험할 수 있었고, 신성함 앞에 벌거벗은 채 춤에 몰입하는 법을 배움, 존재와 현실의 고삐를 들어올릴 용기가 있다면 그 끝에 진실과 마주칠 수 있음, 모든 게 세상의 종말, 시작 그리고 모든 것에 관한 이야기임, 무섭고 경이롭고 너무 소중한 경험임
          + 사이키델릭 관련 포럼에서 자기 팔에 난도질을 하며 LSD를 한 경험이 있다는 글을 봄, 이에 대해 ""초보자의 전형적인 실수"", ""혼자 트립 타서 그렇지 뭐"", ""트립 타기 전에 칼 같은 무기는 치워두라""는 식의 반응을 보고 놀람, 언뜻 보면 자기만의 언어(프로그래밍 언어) 신봉자와 비슷해 보임, 바깥 사람들에겐 엄청 생산적으로 보이게 홍보하지만 실제 이너서클에서는 도구 부족, 랜덤 컴파일러 버그, 이상한 API도 드러남
          + PSA: 100~150마이크로그램은 초보자 기준 중간~강한 트립임, 처음엔 75마이크로그램 이하 등 더 적게 시작하는 걸 권장, 그리고 이런 약물 반응을 레이브나 공공장소에서가 아니라 더 통제된 환경에서 먼저 시험하는 게 꼭 필요, 같은 사람도 컨디션과 환경에 따라 효과가 크게 다름
          + “당신은 자신과 처지를 이해하고, 선인들의 어깨에 올라 배우러 이곳에 있다”는 말에 대한 반론으로, 커트 보네거트의 말 “우리는 여기 와서 하찮은 행동을 하라고 있는 것”을 인용, 경험이 쌓일수록 그 말이 그럴듯하게 느껴짐
          + 때때로 ‘영적 상태’란 것이 신경전달물질 수치처럼 정량적으로 정의될 수 있는지 궁금, 하지만 뭔가 엄청난 것 같던 경험이 수치로 환원된다면 꽤나 우울할 듯, 근사체험(out of body)도 결국 뇌의 공간 인지 알고리즘 오류 같은 거로 보임
          + 내 생각엔 대부분의 사람은 한 번쯤은 사이키델릭 경험을 해봐야 한다고 봄, 비길 데 없는 독특한 경험이고, 인생의 커다란 즐거움 중 하나임
     * Ram Dass가 1960년대에 LSD 실험을 진행할 때 테스트를 무작위/이중맹검으로 했으나 꽤 재밌는 상황이 연출됨, 예를 들어 성직자가 모인 실험에서 한 사람은 ""뭔가 느껴지는 것 같다""고 하고 다른 한 명은 주변을 돌아다니며 ""GOD를 봤어요! GOD를 봤어요!""라고 외침, 누가 무슨 약을 먹었는지 금방 들통남
          + 관련 만화가 있음 (만화 링크), 구글로 찾기 힘들었음, 요즘 구글 검색은 단어를 ‘AND’가 아니라 ‘OR’로 처리해서 예전 야후같음, 검색엔진에서 추천엔진으로 바뀐 흐름이 내 입장에선 재앙임
          + ""3주간 5명이서 건물에 갇혀 4시간마다 LSD 400마이크로그램씩 먹고 하루 2400마이크로그램을 섭취, 결국 병째로 마시고 엄청난 상태에 도달, 아무도 믿기 힘든 경험을 했고 그후 일상으로 복귀, 천국에 갔다가 다시 쫓겨난 느낌""이라는 Ram Dass의 체험담에 영향을 받아 사이키델릭에서 전통적 영성으로 관심을 옮기게 됨, 그 영역은 이미 수천년 전부터 정립됨, Ram Dass는 결국 힌두교 신자가 됐고 나는 불교 쪽에 더 끌림, ‘아나타(무아)’ 개념이 이고 죽음 체험과 잘 맞아떨어짐, 결국 물질(약물)이 아니라 전통 수행이 자유로움을 줌
          + Ram Dass가 힌두 요기(guru)에게 LSD를 막대한 용량(1200μg 이상) 두 번 투여했지만 별 효과가 없었다고도 함, 아마도 전통적 수행으로 이미 약물이 주는 상태를 초월했기 때문이라 추정 (관련 링크)
          + 임상시험에서 MDMA도 이런 문제가 있음, 연구자와 참여자 모두 꽤 정확히 플라시보 여부를 구분함
          + 그 무렵(1960년대) 가톨릭 미사도 라틴어를 그만두기 시작함
     * ""Sacred Knowledge""라는 William Richards의 책을 강력히 추천, “Awe, sacredness, eternity, grace, agape, transcendence, transfiguration, dark night of the soul, born-again, heaven and hell”이 단순 신학 개념이 아니라 직접적 체험의 언어로 다가옴, LSD(‘acid’), 사이로사이빈(‘shrooms’), DMT 같은 사례들을 통해 ‘영적’이되 종교적이지 않은 이들, 자칭 샤먼들이 경험을 공유함, 한 번도 경험하지 않은 평범한 사람들과, 다양한 기독교, 유대교, 불교, 힌두교 영성 체험과의 연관성은? 수도사와 성직자들은 평소 수행과 어떻게 비교할까? 모두가 결국 선불교적 깨달음 경험으로 귀결될까 아니면 각자 본래 종교에 더욱 뿌리 내릴까? 종교적 신자든 무신론자든 이 책을 읽으면 시야가 조금 넓어질 것임
          + 기독교인 입장에서 보면 대부분 초월적 비전은 기대하지 않음, 극히 드문 예외적 현상이고 평생에 없을 수도 있음, 기독교적 시각에서는 '악마'가 믿음에서 멀어지도록 영적인 환상을 줄 가능성이 ‘신성한 비전’보다 훨씬 높음
          + 나 역시 LSD와 버섯을 한 번에 열두 번 정도 해봤음, 그냥 내 인식에 영향을 주는 약일 뿐이고 특별히 영적이거나 심오한 건 없음, 그래도 전체적으로는 환각제 경험이 긍정적 영향이라고 생각함
     * 거의 10년 전, 침례교 성경학자, 가톨릭 신부, 랍비들, 이슬람 지도자, 선불교 스승 등 다양한 종교 지도자들이 실험실에서 매직 머쉬룸(고용량)을 복용했다는 일화가 있음, 참 신기한 일임, 어쩌면 지금 세상에 꼭 필요한 일일지도 모름
          + 지난 5년간 아내와 반복적으로 말하는 대사가 ""세상 전체가 제대로 한번 핫박싱 되어야 한다""임
          + 보수적인 종교 집단(특히 이슬람 등)이 나쁜 방향으로 오히려 과격해지는 게 두렵기도 함, Scientology 같은 컬트도 떠오름
          + 솔직히 이 일화는 넷플릭스 오리지널 시리즈의 기획 같기도 함
          + 전적으로 동의함
     * 기사에 본질적인 내용이 거의 없었고, 전부 연구 방법과 결함에 대한 얘기뿐임
          + 기사에 이런 실수도 있음: “참여한 이들 중 대다수가 이미 직업을 떠날 생각이 있어 신성과 재접속하려 했을 수 있다”는 구절이 있으나 실제 연구 원문에 따르면 8%만이 그런 응답, 참가자는 24명이라 실제로는 2명뿐임 (연구 링크)
          + 정말 콘텐츠가 거의 없어 놀랐음, 개인적 이야기나 연구까지도 없음, The New Yorker 버전이 더 흥미로워 보임 (기사 링크)
          + 완전 클릭베이트임
          + 기사 길이가 너무 짧아 혹시 내가 못 찾은 줄 알고 위아래로 계속 스크롤했음, 그래서 다 읽고 나니 실망만 남았음, 원하는 내용에 대한 블러브(요약)뿐임, 괜히 오늘 하늘 보고 소리 지르고 싶음
     * 내 뇌가 남들과 다른지 모르겠지만, LSD와 Psilocybin을 몇 번씩 해봤지만 경험이 전혀 영적이지 않았음, 사람들이 말하는 ‘영적 체험’이 뭔지조차 감이 안 옴
          + 최근 LSD에 대해 서구 신비주의 쪽에 관심 많은 분과 이야기해봄, 20대에 실수로 고용량을 한 적 있고 관련 책들도 꽤 읽었다고 함, 이런 경험이 거울 미로 같은 느낌일 뿐 그 이상은 아니라고 함, 여운이 너무 오래가 안 좋다고도 경고하며 ‘영적’ 체험이라는 표현이 신기하다고도 함
          + 나도 마찬가지로 acid와 shrooms를 매우 좋아하지만 나무 끝, 카펫 무늬가 좀 더 아름다워 보이는 정도고, 인생을 바꿀만한 깊은 체험은 아님, MDMA도 사람들 평처럼 ‘인류와 연결’ 같은 느낌이 전혀 없음, 그냥 약에 취해 이갈이하며 무대에서 춤추는 느낌임
          + 나 역시 꿈 같은 환각과 웃음, 기쁨은 재밌지만, 트립 중에도 ‘내 뇌가 케이블이 꼬인 것’일 뿐 신성과는 무관하다는 걸 인식 가능
          + set(상황), setting(환경), 그리고 스스로 약물 경험이 없는 사람들이 통상적인 체험을 하더라도, 여러 번 해본 사람들은 이미 기본적인 내성이 생기니 강렬함이 덜함, 평생 술·대마초 외엔 약물 경험 전혀 없는 성직자를 교회 상징물로 가득한 방에 앉혀 ‘영적 체험을 하라’고 유도하면 그 경험의 강도와 차이가 클 수밖에 없음
     * 나는 무슬림 가정 출신이지만 종교적으로 깊은 신념은 없음, 첫 LSD 트립에서 깊이 영적인 체험이 있었고 논문에서 언급한 주제들과 많이 겹침, 몇 가지 느낀 점을 공유:
          + 예언자의 감각: 그 경험이 너무 강렬해 5000년 전이라면 예언적 비전이라 여겼을 수도 있을 듯, 신과 대화하는 느낌은 아니었지만 예언자들이 ‘신성함’을 느꼈을 법함
          + 내면의 평화와 명확함: LSD는 순수한 기쁨, 따스함, 평화감을 줬고, 내 마음의 필터를 걷어내 마치 세상을 정말 그대로 보게 하는 느낌이었음
          + 죽음의 수용: 죽음을 자연스러운 일부로 받아들이게 됨, 두려움이 사라지고 평소엔 생각지 못했던 주제와 마주함
          + 신성한 음악: 음악이 천상의 감정과 영적 깊이를 확장시키는 역할을 함, 그 순간의 몰입을 이끌었음
          + 영적 연결감: 종교의 ‘진위’는 별 생각 안 했지만, 설명하기 힘들 만큼 뭔가 더 큰 존재를 만지는 느낌, 아주 의미 깊은 경험
          + 옳고 그름: 옳음과 그름이란 결국 사회가 만들어낸 개념임을 느낌, 조화롭거나 해로움이 느낌의 기준, 맥락에 따라 달라지며, 사물 자체엔 선악이 없다는 철학적 체감
          + 놀랍게도 내가 쓴 글인 줄 알았음, 나 역시 무슬림 가정 출신에 예언자 같은 느낌이 정말 공감됨
     * 1962년 Harvard Divinity School에서 진행된 Marsh Chapel Experiment(‘Good Friday Experiment’)라는 실험이 있었음 (위키피디아), Pahnke가 psilocybin이 종교적 자질을 갖춘 피험자에게서 신비체 발견을 유도할 수 있나 연구, Timothy Leary, Richard Alpert 등 유명 프로젝트와 연관된 실험임
     * 제목은 흥미로웠으나 내용은 너무 얕음, 에피소드만 대충 훑고, 깊이 있는 통찰이나 새로운 시각 없이 헤드라인에서 충분히 예상 가능한 수준밖에 없었음
"
"https://news.hada.io/topic?id=21483","SSL이 90년대 후반에 TLS로 이름이 변경된 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     SSL이 90년대 후반에 TLS로 이름이 변경된 이유

     * 1990년대 중반의 Netscape와 Microsoft의 브라우저 경쟁 속에서 SSL 프로토콜이 등장함
     * SSL 2는 암호학적 및 실용적 결함이 있었고, 이를 토대로 Microsoft는 PCT 프로토콜을 도입함
     * Netscape는 대응책으로 SSL 3.0을 개발해 주도권을 확보하려고 시도함
     * 업계와 커뮤니티에서는 브라우저 간 호환성 유지를 희망해 IETF에 표준화 역할을 위임함
     * 이 과정에서 프로토콜 이름이 TLS 1.0으로 변경되어 현재까지 이어짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경: 브라우저 경쟁과 보안 프로토콜의 탄생

     * 1990년대 중반에는 Netscape와 Microsoft가 브라우저 시장에서 극심한 경쟁 구도를 형성함
     * 경쟁 과정에서 Netscape가 SSL 프로토콜을 개발하게 되었음

SSL 2 및 문제점

     * 최초 버전의 SSL은 암호학적 결함으로 곧바로 무력화되어 출시되지 않았음
     * 실제로 배포된 SSL 2는 몇 년간 사용되었으나, 여러 암호학적 및 실용적 결함이 있었음
     * 이 결함은 즉각적인 치명적 위기는 아니었으나, 개선의 필요성이 지속적으로 제기됨

Microsoft의 대응: PCT 프로토콜

     * 경쟁 심화 속에서 Microsoft는 SSL 2에 자체적 기능을 추가해 PCT라는 프로토콜을 도입함
     * PCT는 Internet Explorer(IE) 와 IIS에서만 지원되었음

Netscape의 새 전략: SSL 3.0

     * Netscape 역시 SSL 2의 문제점을 개선하고자 했으나, Microsoft가 주도권을 잡는 것을 원치 않았음
     * 이에 SSL 3.0을 개발, 기존과 뚜렷이 구분되는 변화를 추구함

브라우저 업계 표준화 협상

     * 업계와 커뮤니티 구성원들은 프로토콜이 양분되는 것을 원하지 않았음(포크 현상 방지)
     * Consensus Development(글쓴이가 근무한 곳)에서 Netscape와 Microsoft 대표자 회의를 주도함
          + 이 회의에는 Bruce Schneier(유명해지기 전 시점), Paul Kocher(SSL 3 설계자), Barbara Fox(Microsoft 대표) 등이 참석함

IETF의 표준화 및 명칭 변경

     * 넷스케이프와 마이크로소프트 모두 IETF(Internet Engineering Task Force) 가 프로토콜의 표준화 과정을 주도하는 데 합의함
     * 글쓴이는 IETF 표준 문서(RFC) 편집을 담당함
     * 정치적 타협의 일환으로, SSL 3.0에 일부 변경을 가하고 이름도 새롭게 지정해야 했음(기존 프로토콜을 IETF가 ‘무조건 승인’하는 인상을 피하기 위함)
     * 결과적으로 TLS 1.0이라는 명칭이 탄생했으며, 실제로는 SSL 3.1에 해당하는 프로토콜임

맺음말

     * 지금 돌아보면, 이 모든 명칭 및 표준화 논의 과정이 다소 우스꽝스럽게 느껴짐

        Hacker News 의견

     * 버전 번호만 봐서는 프로토콜이 얼마나 달라졌는지 제대로 알기 힘든 혼동스러운 상황 설명 SSLv2가 최초로 널리 쓰인 SSL 버전이지만 여러 문제점 존재 SSLv3는 거의 완전히 새롭게 만들어진 프로토콜 TLS 1.0은 SSLv3와 매우 비슷하지만 IETF 표준화 과정에서 몇 가지 소폭 개정 진행 TLS 1.1은 TLS 1.0에서 블록 암호 사용 방식의 문제를 해결하기 위한 소규모 수정 버전 TLS 1.2는 암호 기술 발전에 맞춰 중간 크기의 수정이 이뤄진 버전 최신 해시 지원(MD5와 SHA-1 취약점 대응), AEAD 암호군(AES-GCM 등) 지원 추가 TLS 1.3은 대부분 새로 만들어진 프로토콜이나, TLS 1.2 이하 특징 일부 접목 각 프로토콜은 자동 버전 협상 기능을 설계해 클라이언트와 서버가 독립적으로 업그레이드하는 데 연결성 손실이 없도록 함
     * 당시 Microsoft는 지금과 완전히 다른 회사라는 점 감안하면 요즘 기준으로도 전혀 이상하지 않은 느낌 당시 'M$'는 오픈소스 인터넷 기술을 견제하기 위해 모든 수단을 동원했고 2010년대 초반까지도 이러한 태도 지속 Java Applets는 이를 통해 발전하지 못하고 시장에서 퇴출되는 데 일조했다는 생각 JavaScript와 CSS의 발전도 수년간 더뎠던 느낌 회사에서 IE의 최신 기술 지원을 강요했지만, 나는 대신 Mozilla 3.0을 선택했고 JS 버그 고친 이후부터 엔터프라이즈 SPA 개발에 Mozilla 적용 Fortune 500 기업에서도 이후 내부 앱에 Mozilla/Firefox 도입이 확대됐고, 결과적으로 좋은 선택이었던 기억
          + 그 시절을 생각하면 'M$'라는 별명이 지금도 어울린다고 생각
     * 차기 버전 이름, 다시 SSL로 돌려도 무방하지 않을지 의견 여전히 모든 사람이 'SSL' 명칭을 사용하고 있다는 점에서 계속 사용해도 무방하다고 주장
          + 실제로 ""TLS"" 명칭도 이미 다양한 곳에서 쓰이고 있다는 점 언급 설정 및 함수 시그니처 등 업데이트는 굉장히 번거로운 일이라는 점에서 고민
          + 이런 의견은 아무에게도 아이디어를 주고 싶지 않음 강조
     * 누군가에게 웹사이트 접속을 안전하게 하라고 안내할 때(즉, TLS/SSL용어를 쓸 때), 나는 보통 어떤 명칭을 사용하는지 질문 또한 나이가 몇 살이며 1999년 이전에 일했는지 궁금하다고 덧붙임 내 답도 곧 달 것이라고 안내
          + 나는 SSL이라고 답함(27세) 한편 코드 상에는 tls를 사용하고, 문서에서는 혼동 방지를 위해 SSL/TLS 표기 선호
          +
              1. SSL이라고 말함 오랜 시간 TLS가 ""같은 것""이라는 사실도 잘 몰랐다가 알아도 여전히 10번 중 9번은 SSL이라고 함 2. 38세(2011년부터 일했지만 네트워크 프로그래밍은 2004~2005년부터) 방금 전 작업한 코드 화면을 보니 함수 이름에도 sslCertNotBefore 등으로 SSL이 박혀 있음 일반적으로 프로그래머는 TLS를 직접 다루지 않기 때문이 아닐까 생각 내 코드 역시 HTTPS에서 인증서 정보를 파싱해야 해서 꽤 번거로운 작업이었음 모두 자동화, 추상화된 덕에 실수 없이 처리할 수 있지만, TLS 작동 원리에 대한 깊은 이해에는 오히려 방해 요소가 되는 측면 존재
          + 대부분 사람들이 OpenSSL처럼 ssl이란 이름이 들어간 라이브러리로 안전한 통신 구현을 하고 있어서 SSL 명칭을 많이 쓰는 경향 OpenSSL 외에도 BoringSSL, LibreSSL, wolfSSL 등 라이브러리 존재 TLS 이름이 붙은 라이브러리는 덜 유명함 대표적으로 GnuTLS, mbedTLS, s2n-tls, RustTLS 등이 있음
          + SSL이라는 용어를 사용하는 주된 이유는 SSL이 더 잘 통하는 것 같기 때문 엄밀하게는 TLS가 맞지만(실제로 SSL 3.0은 아무도 안 씀), 대표 라이브러리에도 SSL이라는 용어가 남아 있어서 계속 사용 중 실제로 90년대 암호 전쟁 시기에 SSL명칭을 배웠고, 그 당시 제대로 된 SSL 암호화를 위해선 넷스케이프(""US only"" 버전)을 불법 다운로드해야 했던 기억 때문에 그냥 익숙함에 사용하는 느낌
          + 난 보통 ""https""라고 말함 일반인도 의미를 대략 이해하는 경우가 있어서 선호
     * 오히려 SSL과 TLS 용어를 무의식적으로 제대로 구분 못 하고 있었음을 처음 깨달음 20년 만에 진짜 이유를 알게 되어 신기한 기분
          + 똑같은 느낌 업계에서 15년 일했는데 이제서야 SSL과 TLS가 실질적으로 같은 것임을 알게 된 기분
     * ""Transport Layer Security""는 확실히 더 나은 명칭이라는 생각 TLS라는 말도 선호함 연달아 두 번 S발음 내면 뱀처럼 들려서 재미있게 느껴지기도 함
          + 하지만 TLS는 이미 ""Thread Local Storage"" 용어에서도 널리 쓰이고 있음 Transport Layer Security가 1999년부터 공식적으로 쓰였지만, Thread Local Storage는 MS나 IBM 개발 환경 등에서 1996년 이전부터 존재 Unix 업계에선 pthread가 1995년 등장할 때부터 thread-specific data란 용어를 더 선호해온 측면도 있음 2001년 Itanium ABI 문서 영향으로 ""TLS""가 더 널리 퍼진 배경도 있을 것 같고, Sun Microsystems도 이미 사용 중이었을 수 있음 혹시 누군가 OS/2 매뉴얼 소장 중이라면 참고자료 공유 바람
          + 내 생각엔 오히려 SSL이 더 어울리는 명칭인 듯 이론적으로 TLS가 여러 계층에서 동작하는 일반적인 보안 수단(예: IPSec 접목도 가능)이어야 맞겠지만 실제로는 TCP 소켓을 위해서만 주로 사용 UDP 변형은 DTLS, QUIC이지만 TLS 자체엔 포함되지 않음 최근엔 리눅스 커널 TLS, 윈도우즈 특화 인프라도 있지만 단순히 소켓 플래그 켜듯 쉽게 적용되진 않음 그리고 솔직히 '뱀'처럼 들리는 S발음은 사운드 자체가 멋짐
          + ""SSL""이 ""TLS""보다 발음이 더 쉬움 S-S-L은 발음할 때 혀가 위치 이동이 거의 없어서 빠르고 자연스러움
          + 정글북의 Kaa가 TCP 보안을 주제로 S~S~L 이름에 대해 얘기하면 어울릴 것 같음 사실 S 발음 하나 더 얹어서 SSSL로 불러도 재밌을 듯
     * TLS와 SSL을 엄격히 구분하는 사람들은 둘의 차이를 잘 알고 있다고 티내고 싶거나 그렇게 얘기 하길 원하는 스타일 실제 구분 자체는 .doc와 .docx(기본적으로 다르지만 일반 사용자에겐 거의 호환)차이와도 비슷 대다수는 HTTPS만 잘 돌아가면 내부 구조나 동작 방식엔 별 신경 안 씀
     * 관련 링크: 1996년에 쓰여진 'Randomness and the Netscape Browser'라는 글(Dr. Dobb's Journal) 공유 https://people.eecs.berkeley.edu/~daw/papers/ddj-netscape.html 1996년 글이라 현대 논문이나 기사와는 언어 분위기 자체가 크게 다름 나이 든 느낌 듦
          + 어떤 출판물을 읽느냐(즉, 타겟/포맷에 따라)는 1996년과 마찬가지로 다양함 LWN 같은 곳은 지금도 비슷한 스타일로 글을 보도함(조금 덜 딱딱해졌을 뿐) https://lwn.net/
     * TLS 1.0이 SSL 3.0과 비교해 실제론 많은 개선점이 있었던 것 아닌지 질문 기사에선 단순히 ""차이를 두기 위한 소폭 조정""으로 표현한 것 같아서 궁금하다고 제기
          + 실질적으로 큰 변화와 개선이 많았던 것은 맞음 다만 SSL 3.0 때만큼의 완전한 리디자인은 아님 강조
     * 인터넷에는 여전히 30만 개가 넘는 서비스가 SSLv2를 지원 중 링크: https://shodan.io/search/report/… 추이 그래프: https://trends.shodan.io/search?query=ssl.version%3Asslv2#overview 수치는 수년간 크게 줄었지만 완전히 사라지는 데엔 아직 시간이 소요될 것으로 보임
          + 그렇다면 실제 SSLv2 기반 클라이언트는 몇이나 남아 있는지 궁금 요즘 소프트웨어/라이브러리엔 더 이상 의미 있는 수준의 지원이 없는 것으로 파악
"
"https://news.hada.io/topic?id=21498","WhatsApp, 앱 내 광고 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          WhatsApp, 앱 내 광고 도입

     * WhatsApp이 최초로 앱 내 광고 도입을 발표함
     * 광고는 Updates 탭에만 표시되고, 15억 명 정도가 해당 영역을 사용 중임
     * 광고 타깃팅을 위해 위치 및 기기 기본 언어 등의 일부 데이터만 수집하며, 메시지 내용과 연락처 정보는 건드리지 않음
     * WhatsApp은 개인 메시지와 통화, 상태가 여전히 종단 간 암호화로 보호된다고 강조함
     * 창업자들이 강조했던 광고 없는 철학에서 벗어난 첫 변화로, 이용자 경험에 중요한 변곡점 형성임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

WhatsApp 앱 내 광고 도입 변화

  WhatsApp의 원래 철학

     * Facebook이 2014년 190억 달러에 WhatsApp을 인수했을 때, WhatsApp은 ""광고 없음, 게임 없음, gimmick 없음"" 이라는 명확한 원칙을 내세웠음
     * 수년간 20억 명이 넘는 WhatsApp 이용자들은 광고나 추가 기능 없이 친구, 가족과의 간단한 대화 경험을 누림

  광고 도입 배경 및 방식

     * WhatsApp은 2024년 기준으로 앱 내 Updates 탭에서 광고를 처음으로 도입하기로 발표함
     * Updates 영역은 대략 15억 명의 사용자가 매일 접속하는 부분임
     * 광고 타깃팅 목적의 데이터 수집 방식
          + 사용자 위치
          + 기기의 기본 언어
     * 메시지 내용, 대화 상대 등 핵심 프라이버시 데이터에는 접근하지 않음
     * WhatsApp 측은 ""챗이나 개인 메시지에는 광고를 넣을 계획 없음"" 을 덧붙임

  프라이버시 강조 입장

     * Nikila Srinivasan(WhatsApp Product Management VP)은 이번 기능 도입에 대해 ""프라이버시 시각에서 접근 중요성"" 을 설명함
     * 개인 메시지, 통화, 상태는 변함없이 종단 간 암호화로 보호된다고 명시함

  창업자 철학과의 차이

     * 창업자인 Jan Koum과 Brian Acton은 2009년 WhatsApp 설립 당시 마지막까지 종단 간 암호화와 단순함에 중점둔 메시징 앱 운영을 추구했음
     * 두 창업자 모두 7년 전에 회사를 떠남, 이번 변화는 창업자 철학과의 중요한 차별점 형성임

        Hacker News 의견

     * 아카이브 링크
     * 실제로 핵심 서비스, 특히 유료 이메일 서비스를 이용하는 사람이 얼마나 되는지에 대한 통계가 있는지 궁금함. 나는 서비스 제공자가 대가를 직접 받지 못하고 3자에게 큰 수수료를 떼이는 방식(무료 서비스라는 명목으로 내 개인정보, 관심, 정치적 영향력까지 넘기는 구조)이 비효율적이라고 생각함. UX와 콘텐츠 품질, 주의력, 프라이버시, 사회적 영향력 모두 떨어지는 문제. 결국엔 ‘쓴 만큼 지불’ 구조로 돌아가야 한다는 생각. 사용자에게 실질적으로 도움이 되는 유료 서비스 모델이 더 많아져야 한다는 바람.
          + Whatsapp이 한때 유료 앱이었을 때를 기억함. 당시 친구들과 가족들이 1유로도 안 하는 요금을 피하려고 어떻게든 무료로 쓰려고 했던 것을 떠올림. 단문 문자(SMS)는 건당 0.25유로였고, 바에서는 콜라 한 잔에 3유로를 아무렇지 않게 썼던 사람들도 예외 아님. 소프트웨어는 눈에 보이는 실물이 아니라 공짜여야 한다는 인식과, 디지털 복사가 도둑질이 아니라는 ‘옛날’ 마인드가 여전함. Whatsapp 서버 운영에도 진짜 돈이 든다는 건 이해하지 않음. 그래서 대형 디지털 서비스가 광고 기반이 됨. 프라이버시는 신경 안 쓰고 광고가 귀찮으니 없애는 방법만 찾지만, 돈 내고 싶어하지는 않음. 참고로 나는 유럽 내에서도 불법복제가 많은 나라 출신임.
          + 정확한 수치를 가진 건 아니지만, “차라리 소액을 내고 광고 대신 쓰고 싶다”는 말은 많지만 실제로 결제까지 가는 사람은 극소수임을 체감함. 예시로 YouTube Premium을 보면, 하루 종일 유튜브를 보면서도 실제로 유료 결제하는 사람은 거의 못 봤음(본인 빼고 한 명뿐). 심지어 소득이 높은 엔지니어들조차도 실제 결제 없이 광고 모델을 비판만 함. 나 역시 구글 충성심 때문이 아니라 실제로 유료 구독 가치가 있었기 때문에 돈을 씀.
          + 경험상 유료 서비스로 전환하는 비율은 전체의 1~2% 수준임. 네뷸라(Nebula)의 유료 전환율도 1% 미만이고, vid.me도 비슷하게 실패함. 사용자들은 광고도 싫고 구독도 싫고 기부도 싫어함. 무료 콘텐츠에 관대한 태도를 가진 (지금은 40대가 된) 이기적인 어린이들에게 “직접 돈을 내지 않으면서 서비스를 비난할 권리는 없다”고 말하고 싶음. 실제로 광고 기반 모델은 없어지지 않을 것임. 주어진 선택에서 ‘무료’만 보장된다면, 사람들은 압도적으로 광고주가 시스템을 움직이게 내버려둠.
          + 1980년대 인터넷을 처음 썼을 땐 접속료만 내면 됐고, 수많은 VC(벤처캐피탈)이 중간에서 꼼수를 부릴 일도 거의 없었음. 요즘 젊은 세대에겐 인터넷이 www사이트와 앱 엔드포인트에 국한되어 아쉬움. 현재 인터넷이 예전보다 쓸모가 늘어난 건 네트워킹 장비 등 하드웨어 발전 때문이라고 생각하고, 타인 사용에서 이득만 얻으려는 중간사업자들(데이터 수집/감시/광고서비스) 때문은 아님. 이런 중개자들에게 비용을 내는 것은 오히려 데이터 수집과 감시를 더 보조하는 구조라고 보고 있음. 사람들이 “아무도 소프트웨어에 돈을 안 내서 어쩔 수 없이 데이터 장사한다”고 착각하지만, 그냥 법적으로 규제도 없고 돈벌이가 크니까 하는 것일 뿐임.
          + WhatsApp이 예전에 연 1달러, 혹은 평생 1달러였던 걸 기억함. 본인도 그때 돈 냈음. 하지만 앱 삭제 후 재설치하면 무료로 쓸 수 있는 ‘WinRar 방식’이었던 걸로 기억함. 당시 대부분 돈 안 냈고, Facebook 인수 전부터 유료 모델을 스스로 접었을 정도로 낮은 금액이라 의미 없었을 것이라고 추측함.
     * 공식적으로 WhatsApp 채널에 유료 프로모션이 없었다는 것에 놀람. 이렇게 플랫폼은 보통 유료 프로모션이 필수인줄 알았음. 채널은 직접 써보지 않았음. 반면 상태 업데이트(현재상태/Status) 기능은 실제로 사람들이 많이 쓰는 느낌이고, 그래서 그 쪽에 광고가 실리면 효과는 있을 거라 봄.
     * 적어도 네덜란드에서는 WhatsApp이 매번 실행 시 60초짜리 스킵 불가 광고를 띄워도, 네트워크 효과 덕분에 버틸 수 있을 정도임. WhatsApp 없으면 학부모 알림, 스포츠팀, 가족, 심지어 자동차 딜러 소식까지 놓치게 됨.
          + 최근 네덜란드에서 Signal 사용자가 급증하는 현상을 체감함. 본인은 몇 년 동안 소수 연락처(대부분 기술 쪽 혹은 프라이버시 중시하는 친구들)만 Signal에서 있었음. 예전엔 모두 WhatsApp도 같이 써서 오히려 Signal 메시지를 놓치는 경우가 빈번했음. 그런데 올해 1월부터 Meta 신뢰 하락이 사회적 이슈로 번지며, 주변 ‘평범한 사람들’에게서 Signal 그룹 초대를 받게 됨. 현지 학부모 단톡방 두 개도 이제 Signal로 자연스럽게 이동했고, 아무도 “굳이 Signal 써야 하나?”하는 의문조차 없이 당연하게 받아들이게 됨.
          + 이런 현상은 케냐 등 개발도상국에서도 비슷함. 케냐에서는 모바일 데이터가 다 떨어져도 WhatsApp만큼은 계속 무료로 돌아가게 통신사가 예외 허용함. 일상에서 필수 인프라 수준.
          + 선택지가 없는 건 아님. 다만 메시지에 광고만 없다면 대부분 별 영향 없이 쓸 것 같음.
          + 인도에서도 비슷함. 대부분의 비즈니스, 고객 소통, 심지어 경찰과 피해자, 정치인과 시민 소통도 거의 WhatsApp으로 이뤄짐. 이제 일반인들은 SMS가 뭔지도 모를 정도로 자리잡음. 나는 WhatsApp 없이도 불편 없이 지내고 있음(물론 이는 상대적으로 특권적인 입장임). 오히려 WhatsApp을 안 쓰니 무분별하게 도는 가짜뉴스와 스팸, ‘WhatsApp University’(출처불명 정보가 무비판적으로 퍼지는 현상을 가리키는 인도식 은어)를 피할 수 있음.
     * Facebook이 190억 달러에 WhatsApp을 인수했을 때부터 광고·게임·잡기적 기능 모두 없었고, 이런 포지션이 특별하다는 분석이 많은데, 본인은 이게 겉핥기 해석이라고 생각함. 돈 벌 방법은 없는데 VC투자를 받아 지탱하며, 일단 점유율을 먼저 확보하고 나중에 어떤 방식으로든 수익화로 전환하는, 장기적으로 지속 가능하지 않은 모델이라고 봄.
          + 오히려 본인이 더 겉핥기 해석일 수 있음. 나는 2010년 Blackberry 앱스토어에서 WhatsApp을 3유로 주고 샀음. 당시 직원 20명이 200개국 메시징 관리했고, 모든 플랫폼에서 쓸 수 있었기에 세계 표준이 될 수 있었음. 애플/구글 체계에 얽매이지 않은 점도 중요함.
          + 왜 지속 불가능했다고 보는지 모르겠음. 그들은 기본적으로 1달러/년씩 받아서 연매출 수억 달러는 충분히 만들었고, 몇십 명 규모로도 그 서버와 인력을 감당할만했다고 추정함. 혜택을 2~5달러로 올렸어도 유사한 수용이 가능했을 것임.
          + 이건 ‘미끼 전략’임. 무료로 유저를 최대한 끌어들인 뒤, 네트워크 효과 때문에 대체제를 쓰기 어려워지면 그때서야 각종 수익화를 적용함.
          + TechCrunch 기사에 따르면, Facebook 인수 당시 “수년간은 광고가 아닌 성장에 집중한다, 결국 수익화한다면 광고가 아닌 방식”이라고 공식 입장을 밝힘.
          + 창립자인 Jan과 Brian은 인수 이후에도 광고 도입을 거부하며, 1달러 구독 유지를 계속 주장했다고 들음. Sheryl Sandberg가 스케일 문제를 이유로 거부했고, VC들은 어차피 ‘Exit’만 바라봤기 때문에 관심 없었음. 하지만 창업자들은 끝까지 광고 사업에 관심 없었고, 지금도 비슷한 입장임.
     * WhatsApp 개발이력에 대해, 원래 창업자인 Jan Koum, Brian Acton이 구현한 엔드투엔드 암호화는 Meta가 Signal 코드 일부를 재활용해서 도입한 것임. 이런 점이 많이 회자됐고, 기사에서 그 부분을 빼먹은 건 아쉬움.
          + 실제로 초창기 WhatsApp은 암호화 기능 자체가 없었음. 공개 압박을 받고 그제야 TLS부터 적용했음. 당시 WhatsApp의 강점은 Status 업데이트와 SMS 대체용 간편함에 있었음.
     * “업데이트”라는 앱 영역에서 15억 명이 프로모션을 볼 것이라는 주장에 대한 의문. 실제로 15억 명이 그 기능을 적극적으로 쓰는지, 아니면 앱이 켜지면 가장 먼저 보이니까 스치듯 지나가는 지 구분이 필요함.
          + 실제로 Chat 탭이 첫 화면이자 기본값임.
     * Facebook/Meta가 아닌 곳에서 WhatsApp을 제공한다면 기꺼이 비용을 낼 수 있겠음. 하지만 Meta가 서비스한다면 유료 결제 후에도 프라이버시 침해와 데이터 수집이 확실하다고 생각함.
     * Facebook Pages가 과거 Reach(노출) 하락 → 유료 광고 유도 → 광고 천국이 된 패턴과 유사 현상. 반드시 Meta/Facebook이 어느 서비스든 인수하면 ‘광고‧스파이웨어’ 투성이가 되는 것은 자연의 법칙에 가까움.
     * 독일에서 WhatsApp은 S-티어(최상위) 지위임. 만약 정식 API가 있다면 메시지당 요금을 내고 싶을 정도. 실제로 메시지당 결제가 도입돼도 너무 많이 보내지 않아 사용자 경험이 나빠지지 않을 것임. 오히려 메시지 남발 및 알림 피로도 줄어들 거라 생각함.
          + 공식 앱과 제한을 우회해서 사용하는 이용자들이 있기 때문에 WhatsApp 측에서는 이런 이용자를 강력하게 차단하고 있음. Reddit의 타사 클라이언트 차단과 유사함.
          + “S-티어” 의미에 대해 궁금하다는 질문 나옴.
          + 메시지당 비용을 내면 자주 안 보내고 절제하게 된다는 의견에, 그 구조는 현재 SMS와 비슷하다고 언급됨.
"
"https://news.hada.io/topic?id=21435","에이전틱 코딩 추천사항","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              에이전틱 코딩 추천사항

     * Agentic coding에 대한 실사용 사례 공유
     * Claude Code Sonnet 모델을 주로 사용하며, IDE 통합보다는 전체 작업을 AI에게 위임하는 방식을 선호함
     * Go 언어는 에이전트 친화적인 구조와 생태계 안정성 덕분에 새로운 백엔드 프로젝트에 특히 추천됨
     * 속도와 단순성이 에이전틱 코딩의 핵심이며, 테스트 캐시나 간단한 툴 체계가 중요함
     * 코드는 단순하고 병렬처리 가능하게 구성해야 하며, 에이전트의 성능을 극대화하려면 리팩터링 시점 선정이 매우 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Preface

     * 최근 에이전틱 코딩 경험을 공유하는 개발자들이 급증함
     * 나는 현재 Claude Code Sonnet 모델을 Max 요금제($100/월)로 사용함
     * IDE의 비중이 줄었고, 대신 Vim을 다시 사용하게 되었으며, Claude에게 작업을 맡기고 결과만 확인하는 식의 흐름을 사용함
     * 혁신 속도가 매우 빨라 포스팅 내용이 빠르게 낡을 수 있기 때문에, 오래 유지될 개념에 초점을 맞춤

The Basics

     * claude --dangerously-skip-permissions 명령어를 claude-yolo로 alias 설정해 모든 권한 제한을 제거함
          + 이는 dev 환경을 Docker로 안전하게 격리하여 관리할 수 있음
     * Claude는 대부분의 기본 툴을 잘 다루기 때문에 MCP(Multi Capability Protocol)는 특수한 경우에만 사용함
          + 예: playwright-mcp를 통한 브라우저 자동화
     * 직접 만든 툴은 일반적인 스크립트로 구성하며 가능한 한 단순한 툴 구성 유지

Choice Of Language

     * 다양한 언어로 에이전틱 코딩을 실험한 결과, 새로운 백엔드 프로젝트에는 Go가 가장 이상적임
          + Context 시스템: 코드 전체에 명확히 흐르는 데이터 구조 제공, 에이전트에게 명시적 전달 방식이 간편함
          + 테스트 캐시: Rust 등 타 언어 대비 테스트 실행과 캐시가 단순, 에이전트의 코드-테스트 루프가 효율적임
          + 단순함: Go 자체의 단순성이 에이전트에게도 유리하게 작용함
          + 구조적 인터페이스: 타입에 메서드만 맞으면 인터페이스로 인식되어 LLM이 쉽게 처리 가능함
          + 낮은 생태계 변화율: 오래가는 버전과 명시적 변화, 구식 코드 자동 생성을 줄일 수 있음
     * Python은 많은 문제점을 유발
          + fixture나 async 처리, 느린 실행 등으로 인해 agentic loop에서 효율이 떨어짐
     * 프론트엔드는 Tailwind + React + Tanstack Query/Router + Vite
          + Tanstack Router의 파일명에 포함된 $ 기호는 에이전트를 혼동시켜 문제 발생

Tools, Tools, Tools

     * 툴의 기준은 다음과 같음
          + 빠를 것
          + 친절한 에러 메시지 제공
          + LLM이 잘못 사용할 경우에도 안정적으로 동작할 것
          + 관찰 가능하고 디버깅이 쉬울 것
     * Makefile 기반으로 make dev, make tail-log 등 명령어 제공
          + 실행 상태 중복 방지를 위해 shoreman 포크 버전으로 pid 관리
          + 로그는 stdout과 파일에 모두 기록해 에이전트가 직접 로그에서 정보 추출 가능
     * 예: 이메일 인증 링크도 로그에 기록되도록 설정하여 브라우저 자동화로 이메일 인증 절차 수행 가능

It's All About Speed

     * 에이전틱 코딩의 최대 비효율은 추론 비용과 비효율적인 툴 사용
     * 빠른 툴 응답 속도가 생산성의 핵심
     * 에이전트가 자체적으로 임시 도구를 만들고 사용할 경우, 빠른 실행/컴파일 속도가 업무 효율을 크게 향상시킴
     * 느린 환경에서는 동적 모듈 로딩 등의 대안(예: Sentry용 파일 모듈 감시 및 자동 실행) 활용이 유리함
     * 로그는 간결하고 명확하게 조절하여 토큰 소모 및 속도를 최적화해야 하며, 필요시 LLM이 로그 수준을 조절할 수 있는 인터페이스 제공이 도움이 됨
     * 코드 생성 단계에서부터 유의미한 로그/옵저버빌리티가 나오도록 설계하는 것이 중요함

Stability and Copy/Paste

     * 에코시스템의 안정성은 코드 재사용성과 에이전트의 혼동 방지에 매우 중요
          + Go와 Flask 같은 변동폭이 적고 예측 가능한 언어/프레임워크 사용 권장
     * 라이브러리 자동 업그레이드에 주의, 에이전트가 남긴 주석이나 코드 흐름이 깨질 수 있음
     * 가능하면 직접 코드 작성 → 의존성 최소화 전략 권장

Write Simple Code

     * 에이전트는 단순하고 명시적인 코드에 더 잘 대응함
     * 권장 방침
          + 설명적이며 긴 함수명의 함수 사용 선호, 클래스보다는 함수 위주 작성
          + 상속 및 복잡한 트릭 회피
          + 순수 SQL 사용 추천; 에이전트가 SQL 실력에 능숙하며, 로그와 비교 및 추적이 쉬움
          + 명확한 권한 체크는 코드 상에서 직관적으로 드러나게 구성(별도 파일/설정 분리 금지)

Make It Parallelizable

     * 개별 에이전트의 처리 속도는 빠르지 않으나, 병렬 처리를 통해 전체 효율을 높일 수 있음
          + 예: 파일 시스템을 기준으로 체크아웃 복사
          + Redis나 DB 등 공유 자원 분리 방법 고민 필요
          + 예시 툴: container-use를 이용한 Docker 기반 세션 분리
     * CI 기반 병렬 작업, Cursor의 background agent 등도 주목할 도구

Learn To Refactor

     * agentic 방식은 적절한 시점의 리팩터링이 중요
          + 복잡성이 높아지면 agent가 코드를 제대로 다루지 못함
          + 예: Tailwind class가 50개 파일에 흩어지기 전에 컴포넌트 라이브러리화
     * 너무 이른 혹은 너무 늦은 리팩토링 모두 비효율적이므로, 적절한 타이밍에 구조 개선 지시 필요

What Next?

     * agentic coding은 빠르게 진화하고 있으며, 핵심 원칙은 ‘단순함, 안정성, 가시성, 병렬성’
          + 도구와 방법론은 변해도 이 원칙은 유효함
     * 생산성 향상뿐 아니라 더 나은 코드 품질 추구가 목표
     * 에이전트가 작성하는 코드의 품질은 몇 달 전보다 현저히 개선됨
     * 유연하게 변화에 대응하며 코딩 경험을 확장할 것

   저도 아직 AI로 간단한 테스트코드나 예문 정도 물어보는게 다인데, 이제 전반적으로 적용하려는 사람들이 계속 나오네요.

   아직 시기상조 일 수 있으나 몇 년만 지나면 당연하게 되겠죠.

        Hacker News 의견

     * 나는 VS Code Nightlys에서 co-pilot과 Claude Sonnet 4를 함께 사용해 Agentic Coding을 경험하고 있음. 하루의 절반이 회의로 채워져도 내 git 히스토리만 보면 모를 정도로 생산성 향상을 느끼는 중임. 이제는 세세한 구현 대신 변화가 제대로 동작하는지, 이 코드를 이해할 수 있는지, 더 잘 이해하려면 구조를 어떻게 잡으면 좋은지, AI 컨벤션 마크다운에 추가해서 Agent의 오해를 줄이려면 뭘 더 넣을 수 있을지 고민하는 수준임. 어젯밤엔 mypy 에러가 38개나 있던 파일을 Agent에게 맡기고 15분 가족과 대화하고 오니, Agent가 수정한 내용과 이유를 요약해줌. 변경사항 중 하나를 놓고 토론도 했지만 결국 Agent의 의견이 맞다고 판단함. mypy 체크도 통과함. 이제 팀에도 이 기술의 진짜 가능성을 이해시키려고 노력 중인데, 회의적인 시선과 AI 완벽하지 않다는 이유로 반대하는
       사람들도 있음. 하지만 친구의 말을 빌리면 ""오늘이 앞으로 너와 이 기술이 겪을 가장 나쁜 날""이라는 점이 바로 혁신의 증거라고 생각함
          + type checker 에러는 사실상 개발 일과에서 가장 적게 시간을 들여야 하는 부분인데, 그 부분이 그렇게 오랜 시간을 잡아먹었는지 궁금함. 모든 AI 활용 논의가 실제 각자 어떤 작업에 쓰고 있는지 다 같이 볼 수 있다면 (cloudflare 포스트처럼) 효과가 훨씬 클 거라고 생각함
          + 개인적으로 Python보다 Rust 코드에 Agent를 더 신뢰함. Python은 정적 분석 수준이 clippy나 rust-analyzer만큼 좋지 않아서 모든 코드 경로를 매번 직접 돌려봐야 함
          + 하루의 절반이 회의여도 git 히스토리만 보면 모를 정도라고 했는데, 만약 내가 소속 회사 직원이라면 곧 이 정도 산출물을 계속 기대할 거라는 점 명심함
          + 워크플로우가 궁금함. Claude Code를 개인 프로젝트 실험용으로 써보고 있고 회사 계정으로 Copilot도 VS Code의 agent 모드에서 3.5, 3.7 Sonnet, 04-mini까지 다양하게 섞어서 써봤음. Go 대형 프로젝트에서 활용했는데, 테스트 쪽 빼고는 결과가 최악임. 도구를 잘못 쓰는 건가 싶어 ""최신 베스트 프랙티스"" 다 시도해봤는데 컨텍스트, 모델 바꿔가며 사용, 규칙 지정, 프롬프트 개선까지 전부 시도했음에도 개선이 없었음
          + AI 컨벤션 마크다운에 Agent 실수를 줄이려는 내용을 더 추가할 수 있냐고 하셨는데 그게 매 작업마다 컨텍스트로 첨부하는 파일인지, 아니면 VS Code Copilot Agent의 공식 컨벤션인지 궁금함. 그리고 문서 구조를 정하는 데 참고자료가 있었는지, 아니면 AI가 반복하는 실수를 기반으로 시간이 지나면서 직접 개선한 산출물인지 물어보고 싶음
     * Agentic coding을 효율적으로 만드는 기술 대부분이 인간 코딩 효율도 높여준다는 점이 정말 고무적임. 예전엔 AI만 이해하는 '거대한 진흙덩이' 코드에 대한 우려가 있었지만 실제론 반대임. 명확한 코드가 AI 생산성에도 훨씬 중요해졌고, 덕분에 생산성 차이가 명확히 수치로 드러나게 됨. 클로드가 코드베이스마다 얼마나 잘 동작하는지 수치로 보여줄 수 있어서, 코드가 잘 구조화되어 있냐는 논쟁도 더는 의견 차이가 아닌 객관적 근거로 대화 가능함
          + 코드가 '진흙덩이'가 될 거란 걱정은 사실 프로그래밍 내내 있어왔던 고민임 (Rich Hickey 강연 보라). 사람들은 ""지금 편하게 가자""를 택하다가 내일 엄청난 기술 부채를 떠안게 됨. LLM 덕분에 의미 없이 보일러플레이트를 양산하는 것도 더 쉬워짐. 아픔이 줄면 굳이 고칠 생각 자체가 사라질 수도 있음
          + 'AI만 이해할 코드가 될 거란 걱정, 지금은 아니라도, 앞으로는 어떨지 알 수 없음'이라는 말도 꼭 남기고 싶었음
          + 이 부분이 정말 공감됨. 좋은 에러 메시지, 빠른 도구, 안정된 생태계, 단순하고 '매직' 없는 코드, 직관적 SQL까지, 원래 내가 꿈꾸던 개발 환경임. 에이전트가 워낙 빨리 일하니까 작은 지연 하나하나가 체감될 정도라, 이런 기술 덕분에 전체 개발 경험 수준이 올라갈 수 있다고 생각함
     * Agent를 쓰다 보면 Go, Tailwind로 자연스럽게 유도된다는 말을 들음. 훈련 데이터가 풍부하다 보니 AI가 제대로 다룰 수 있기 때문임. 그럼 모든 사람이 이 도구를 쓰는 미래엔 새로운 언어나 프레임워크, 라이브러리가 등장하기 어려워지는 현상은 발생하지 않을지 걱정도 듦. 기존 솔루션과 경쟁이 어려워지고, StackOverflow 같은 사람 커뮤니티도 사라질 수 있음
          + 새로운 언어나 프레임워크가 아예 등장하지 않을 거란 걱정에는 동의하지 않음. LLM들은 번역에 엄청 강해서 훈련 데이터가 없더라도 독특하지만 구조가 명확한 프레임워크는 코드베이스 보고 바로 학습하는 능력이 있음. 실제로 내 idiosyncratic C# 프레임워크(누구도 본 적 없는)도 LLM이 아주 잘 다루는 걸 경험. 물론 Rust의 라이프타임 같은 독특한 특성은 바로 호환되지 않을 수 있지만, Go처럼 단순한 건 금방 적응함. 앞으로는 새로운 언어를 만들 때 아예 LLM 호환성(디자인, 툴링, 문서 등)을 고려해 만들어야 할지도 모름
          + 정말 중요한 질문임. 다시 말하면, LLM 생성 데이터로 인터넷이 뒤덮이면서 양질의 훈련 데이터가 줄어들 텐데, LLM 친화적인 개발자들은 과거 '방사능 적은' 옛 기술(JS/React 등)에 더 끌릴 수도 있음. 앞으로 JavaScript/React는 미래의 COBOL이 되어도 값비싼 컨설턴트 필요 없는 시대가 오고, 유지보수는 LLM이 다 할 수 있음. AI 유행을 피하고 싶다면 새 언어 개발, 특히 Lisp+DSL처럼 LLM이 바로 학습 못하는 특이 언어는 AGI 시대 이전까진 꽤 안전할 수도 있음
          + 소프트웨어 스택의 전통적 순환 구조는 다음과 같음. (1) 기존 스택이 모든 니치 분야까지 껴안고 복잡해지면, 전문가들이 불필요한 '아토텍처' 난무 (2) 그로 인해 훨씬 더 쉽게, 명확하게 새 트렌드 해결하는 간단한 새 스택이 나와 인기를 얻고 (3) 시간이 흘러 결국 이 새 스택도 같은 문제로 무거워지면서 악순환 반복. AI 코딩도 컨텍스트 확장이 빨라지고 있어 이 사이클이 쉽게 깨지지 않을 거라 생각
          + Go, Tailwind 강제된다는 주장은 실제론 글쓴이의 개인적 성향이 많이 반영된 관점임. Sonnet이 cargo test CLI에서 문제 있다고 해서 Rust, cargo, 더 크게 AI 전체가 문제라고 볼 수 없음. 실제 PHP 테스트에서도 Junie가 built-in runner를 잘 못 쓰긴 하지만, bin/test-php 스크립트를 만들어주니 잘 알아서 사용함. 요구사항을 가이드라인에 명시적으로 써주는 게 도움 되긴 해도, 기본 제공 도구 고집하는 특성을 가진다는 차이임. Stack Overflow 경험에 대해서도, 내 AI 어시스턴트는 질문을 중복으로 닫지 않는 장점이 있음. SO의 큐레이션 시도는 좋지만 그로 인해 많은 사용자를 떠나게 만든 것도 사실임
          + 어제도 Claude (Zed 사용)에게 elixir phoenix 새 프로젝트를 조건만 주고 맡겼는데 문제없이 잘 수행함. CSS로 tailwind를 썼는데, 그건 phoenix가 자체적으로 기본 세팅을 해줘서 그런 것 같음. AI가 Go로 유도한다는 주장에는 공감 못함. 오히려 맥락 없이 물어볼 땐 Python 쪽 제안이 넘치고, 커뮤니티 규모가 작은 elixir도 문제없이 잘 활용하는 경험임
     * Claude Code Sonnet 4.0으로 Rust 코드 한 주 정도 실험해봤는데 기대 이하임(게다가 Bedrock 경유라 비용도 비쌈). 초기 계획 세우는 데 시간 오래 쓰는데도 실제론 절반만 완수하는 경우가 많음. 내가 뭘 놓치고 있는 건지 궁금함
          + 나도 거의 같은 느낌임. Cursor Edit/Agent 모드에서는 한 번에 거의 원하는 수정이 나와서 엄청 효율적인데, CLI 환경에선 너무 불편함. 혹시 클로드 코드에게 10~15분 작업을 맡기고 diff만 리뷰하는 식으로 사용하는지, 아니면 코드 리뷰도 제대로 하고 있는지가 궁금함
          + 나도 완전히 똑같은 경험임. 쓸만한 활용 케이스를 일부러 찾아 다니면서 써봐도 제대로 동작하지 않아서 정말 의아했음
          + 비싸게 느끼면 안 됨. Pro 플랜은 월 20달러, Max는 100~200달러인데, API로 쓸 때 월 천 달러 넘게 드는 것보다 저렴한 구조임
     * 컨테이너 사용이 언급된 게 꽤 반가움. 나는 dagger/container-use에 참여 중이고, 팀에는 ex-Docker 멤버와 docker 창업자도 있음. Agent들을 병렬로 돌리는 게 큰 기술 진보의 분기점이 될 거라 생각함(신뢰성 있게 잘 활용할 수 있게 되면). 그 전까지라도, Agent 작업 돌리는 동안 내가 딴 일 하거나 Agent가 엉뚱한 부분을 건드릴까 걱정된다면 개발 환경을 컨테이너화하는 게 매우 유용함. 컨테이너 사용 기술도 아직 초창기지만 굉장히 빠른 속도로 발전 중이고, 현재는 안정성/깃 충돌 감소/사람과 Agent 간 상호작용 강화를 중점적으로 개선하고 있음
     * 언어 선택에 대한 내 생각은 다음과 같음. 1) Java는 LLM이 참고할 수 있는 방대한 규모와 오랜 데이터셋 덕분에 가장 포괄적(반드시 가장 정확하다는 건 아님). 2) 무엇보다 본인이 제일 잘 아는 언어로 진행해야, LLM이 잘못된 추론, 오류, 환각 등 실수를 빠르게 잡아낼 수 있음
          + Java가 가장 큰, 오래된 명확한 데이터셋으로 꼽힌다는 의견은 LLM이 API/문서/3rd party 소스코드를 찾아볼 도구를 못 쓸 때에 한정한 조언 같음. 툴이 자동으로 뭘 써야 하는지 알아낼 수 있다면, 어떤 언어를 선택하든 결국 Agent가 소스를 읽을 수 있기만 하면 됨. 하지만 두 번째 의견(아는 언어를 써야 한다)엔 전적으로 동의함. 결국 사람의 꼼꼼한 검토가 필수이고, 아는 언어면 오류 판단도 수월함
          + 왜 Java가 데이터셋이 가장 클까? 오픈소스 프로젝트 중 Java가 가장 많은가(전체 Apache suite 때문?) 아니면 Java 오픈소스 라이브러리의 문서가 아주 풍부해서 그런가 궁금함
          + 내 생각엔 Python 코드가 가장 많은 데이터셋일 거라 여겨왔음. 아무 언급 없을 때 LLM들은 대부분 Python부터 추천하는 경향이 있음
     * Go의 context system(명시적 데이터 백, 코드 실행 경로 따라 흐르도록 설계)이 AI agent에 단순함을 제공한다는 주장에 ""tracing data를 제외한 데이터를 context.Context에 담는 건 실제 좋지 않은 관례""라는 비판이 있음
          + 동의함. chromedp(Go용 chrome headless driver)에선 context.Context를 1번째 인자로 쓰는데, 그냥 context.Background()가 아닌 특수 factory에서 얻은 context만 써야 하는 구조임. timeout 설정만 context.WithTimeout으로 따로 처리하고, 거의 void* 포인터처럼 쓰는 셈임
          + Go 전문가까지는 아니지만, 실제로 라이브러리들이 context에 데이터베이스 연결, 설정, 레이트리미터, 캐시백엔드 같은 데이터 담는 경우가 많아서 당장 나로선 그다지 문제라 느끼진 않음
     * ‘AI가 이해할 만큼 단순한 코드 쓰기’는 내가 기대했던 혁신 포인트는 아님. 내 이전 ugly code 관련 글과 방식이 어떻게 충돌하는지도 궁금함
          + 이런 식의 단순/명확 코드 작성 방식은 사실상 팀 작업에서 항상 도움됨. 코드에 극도로 집중하거나 창의적으로 짜야 할 순간도 있지만, 그건 예외적이고 비즈니스 가치에 밀접해야 함. 대부분의 코드는 ""누가 봐도 자명한 게 답""임. 개발자가 느린 건 타이핑이 아니라 머릿속에서 한 번에 담아야 하는 '개념'의 양 때문임. 인터페이스 오버엔지니어링 하지 말고, 추상화도 미루고, 복붙과 단순한 조립 허용, pattern은 그냥 공식 문서대로, 절대 똑똑하려 하지 말기. 코드는 예쁘게가 아니라 명확/단순하게 구성, 실제 어려운 건 코드 자체가 아닌 '제품'이라는 점 강조가 핵심
     * Claude Code에 대해 글쓴이가 썼듯, 다양한 대안(OpenCode, goose, Codex, Devin, Cursor background agent 등)이 실제로 존재함. 클로드 코드와 비슷한 오픈소스+로컬 LLM 솔루션이 궁금하다는 질문이 있음
          + 지금으로선 강추할만한 오픈소스+로컬 LLM 솔루션은 딱히 없음. 다만 SST의 OpenCode가 UX 쪽으로 빠르게 진화 중이고, 앞으로 좋은 로컬 모델이 더 나오면 적용도 쉬워질 거라 생각함. 가장 큰 문제는 실제 '툴 사용'이 뛰어난 좋은 모델이 거의 없음. Sonnet이 충격적으로 잘하는 이유도 툴 사용에 특화된 훈련 덕분임. Gemini도 아직 한참 미달이고, 결국 좋은 모델만 나오면 해결될 문제라 봄
          + Aider의 경우 거의 다다랐지만 의도적으로 완벽히 agentic하지 않음. 테스트/정적 분석 자동 실행, 자동 오류 수정 등 다 가능하고, to-do 리스트 기반 전체 프로젝트 스펙도 다룰 수 있음. 지금은 하드코딩된 리플렉션 반복 횟수(3회) 제한이 있는데 해킹으로 마음대로 늘릴 수 있고, self prompting만 추가되면 완전 자동화 agent화도 가능
          + 곧 출시할 내 앱도 좋은 대안이 될 거라 생각함. 싱글 파일 다운로드, 설치 필요 없는 구조로 Mac, Windows, Linux, Docker에서 다 사용 가능. OpenAI API와 호환되는 모든 모델 활용 가능(직접 돌리는 것도 포함), 브라우저 기반이라 Claude Code 못지않게 편하고, 명령어 기반 콘솔 앱도 제작 가능. 추가로 터미널을 직접 열어 서비스에 연동할 수도 있음. 현재는 클로즈드 알파지만 사용 원하면 이메일 연락 주면 됨
          + 거의 매일 새로운 대안(혹은 시도작) 출시되고 있어, 조만간 '딱 맞는' 대안을 쓸 수 있을 거라 기대. 예를 들면 app.build는 Neon(현재 Databricks) 팀이 막 런칭해서 꽤 유망해 보임
          + Neovim 플러그인 CodeCompanion도 최근 더 agentic한 방향으로 진화 중임. 이미 auto-submit loop, 내장 툴, MCP 통합 기능 지원. CLI 독립형 도구가 아니지만, 풀 에디터 환경을 바로 쓸 수 있다는 장점이 더 큼(특히 해킹/커스텀/경량 에디터 선호 시)
     * 월 100~200달러는 검증 안 된 코드 작성 AI 치고 너무 비쌈. 개인적 경험이 그리 만족스럽지 않았는데 게다가 윤리적 논란까지 있어서 진입장벽으로 작용함
          + Claude Code는 API 키로 사용하거나 월 20달러 프로 구독하면 됨
          + Aider를 API 요금 체계로 써보길 추천. context 사이즈 제어(/clear, /add, /drop)로 25K 정도로 제한 가능. 원하는 모델 사용(예: Sonnet 4, Gemini 2.5 Pro) 가능. 간단한 스크립트는 보통 1달러 이하로 완성 가능했고, 아주 대형 툴을 만들 때도 프로프트+코드 100여개 테스트까지 합쳐도 6달러 이하로 가능했음. AI로 코드 짜기에 익숙해지면, 그때 Claude Code(더 강력하지만 자주 쓰지 않으면 오히려 비쌈)로 넘어가길 권장
          + 20달러짜리 한 달 구독이면 소규모 프로젝트 몇 개는 충분히 시험해보고, 100달러 플랜 고려할지 판별 가능함. 혹은 앞으로 몇 달만 기다리며 다른 사람의 실사용 경험자 후기를 참고해도 좋을 것 같음
"
