"https://news.hada.io/topic?id=21811","Show GN: VibeCity - 당신의 감정을 안아주는 대화 공간","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Show GN: VibeCity - 당신의 감정을 안아주는 대화 공간

   안녕하세요!

   바이브 코딩으로 과연 어디까지 해볼 수 있을까? 고민하다가 만들어보게된 게임 느낌을 접목한 가상 캐릭터 채팅 서비스 입니다.

   2D 픽셀아트를 사용해 바이브 시티안에 클래식 미드나잇 바를 구현했고, 가상의 손님들이 이곳에서 머물고 있어요. 오늘의 감정 상태를 고르고, 음료를 추천 받고나면 마음에 드는 손님과 대화해보실 수 있습니다.

   3명의 동료들과 사이드로 여러 시도들을 해보고 있는데요. 기존에 만들던것들을 벗어나 과연 ""우리가 위로, 공감, 재미를 사람들에게 줄 수 있을까?""의 질문에 대한 스프린트 프로젝트로 봐주시면 됩니다.

   해당 서비스는 2주간의 디자인 스프린트 과정을 통해 만들어졌습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   [사용한 AI 툴 조합]
   VibeCity인 만큼 최대한 많은 생성형 AI 서비스들을 사용해봤습니다.
     * 캐릭터 프롬프트 (ChatGPT, Silly Tavern)
     * 캐릭터 이미지, 배경 (Midjourney, Whisk(Imagen4), Imagen3)
     * 배경 세계관, 이미지 생성 프롬프트 (ChatGPT 4o, Gemini 2.5 Pro)
     * 프로젝트 구현 (Claude Sonnet 4, ChatGPT 4.1)
     * BGM (SunoAI)

   프로토타입은 하루, 이틀만에 만족할 수준으로 만들어줬습니다. 하지만 여지없이 기능을 추가할수록 바이브 반복 횟수가 많아지고 연속성이 떨어지는게 심해져서 IDE에서 최대한 Agent모드보다 Ask 모드를 많이 사용했고, 하나의 파일당 적은 라인을 유지하는데 초점을 많이 두었습니다.

   특히 게임에 적용되는 씬 개념을 처음 구현하다보니 명확한 설계 지시가 부족했기 때문에 어려운 부분이 더 많았던것 같습니다.

   캐릭터 프롬프트를 만드는데는 Silly Tavern이나 뤼튼을 사용중인 분들의 팁들을 참고했습니다.

   음료 이미지를 만들때 사용한 간단 프롬프트는 아래와 같습니다.
   ""A detailed pixel art masterpiece, 16-bit retro style, vibrant colors, white background, depicting: mojito. Ensure clear outlines and a distinct pixelated aesthetic.""

   디자이너가 없는 팀이라 필요한 애셋을 과연 생성형 AI로 충분히 만들어 낼 수 있을까 싶었는데, 프로토타입용으론 너무나 만족스러운 경험이였습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   [개발 관련]
     * IDE (Cursor, Github Copilot)
     * API (ChatGPT 4.1)
     * 서버리스 백엔드 (Vercel)
     * 데이터베이스 (서버리스 SQLite)

   리액트 환경은 처음이였는데요. Vercel의 무료요금제인 Hobby 단계로 간편한 배포부터 통계, Speed Insights 확인, 서버리스 DB연결, 커스텀도메인 연결까지 너무 만족스러운 배포 경험이였습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   게임의 감성을 살리기 위해 현재 데스크톱 환경으로만 테스트를 진행하고 있지만 질문이나 피드백은 언제든지 환영입니다! 감사합니다.

   흥미로운 프로젝트인데 모바일이 안되서 아쉽네요. 집에 가서 해봐야 겠어요. ^^;

   오 매우 흥미롭네요! 맛있는 마르가리따 한잔 잘 마셨습니다 ㅎ

   체험 감사합니다! 낮술 괜찮으셨는지 모르겠습니다ㅎㅎ
"
"https://news.hada.io/topic?id=21823","농민 레일건","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 농민 레일건

     * 농민 레일건은 D&D 5e 규칙 해석의 허점을 이용한 유머성 아이디어임
     * 이 방법은 수천 명의 농민이 한 줄로 서서 사다리 막대기를 순식간에 전달해 대량의 피해를 주는 방식임
     * 규칙상 이론적으로 한 턴(6초) 만에 2마일을 이동해 엄청난 속도와 피해량을 산출함
     * 현실의 물리 법칙과는 다르기 때문에 실제로는 문제가 많고 농민들도 무사하지 못할 것임
     * 대부분의 DM이 허용하지 않을 가능성이 높으나, 유쾌한 이야기 소재로 쓰기에는 흥미로움
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Peasant Railgun이란 무엇인가

     * 농민 레일건은 2008년 금융위기 말기에 1d4chan 커뮤니티에서 제안된 D&D의 이론적 트릭임
     * 파티는 2,280명의 농민을 한 줄로 세우고, 사다리 막대기를 분해해 적에게 던져 300d6 피해를 주는 방법임
     * 이 과정은 전투 한 라운드(6초) 만에 완료할 수 있으며, 재장전도 12초 이내에 가능하다는 점이 특징임

Peasant Railgun의 작동 원리

     * 여러 D&D 규칙이 결합되어 작동함
          + 준비 행동(Ready Action): 각 농민이 자신의 반응(Reaction)으로 앞사람에게 막대기를 전달하도록 미리 상황을 지정함
          + 1라운드 시간: 한 라운드에 모두가 동시에 행동해 막대기가 2,280명을 거쳐 이동함
          + 캐릭터 크기 및 거리: 중형 크기인 인간이 한 명당 5피트 공간을 차지해 총 2마일(11,400피트) 길이의 선을 만듦
          + 5e 떨어지는 물체 규칙: 7파운드의 막대기가 60피트마다 2d6 피해를 주게 계산함(총 300d6 피해)
     * 이로 인해 막대기는 6초 동안 1,900마일/시간 속도로 적에게 전달됨

현실 물리와 규칙의 차이

     * RAW(규칙 그대로)로는 가능하지만, 현실의 물리 법칙과 맞지 않음
          + 이런 속도로 무거운 막대기를 잡는 농민들은 대부분 심각한 피해를 입을 수밖에 없음
          + 실제로는 막대기의 전달 속도가 인간이 견딜 수 있는 한계를 초과함
          + 앞에 있는 농민도 그 속도를 따라잡지 못하며, 표적도 제대로 맞추기 어려움

DM의 허용 여부 및 활용 방안

     * 대부분의 DM은 농민 레일건을 허용하지 않을 것임
     * 하지만 유쾌한 원샷(one-shot) 아이디어나, 마을 사람들이 한마음으로 힘을 합치는 상황을 그릴 때 심미적인 요소로 활용 가능함
     * 이 기법은 규칙 해석을 창의적으로 이용한 특이 사례로, D&D 커뮤니티에서 밈(meme)으로 널리 회자됨

참고 및 리소스

     * 더욱 상세한 규칙 해석과 배경은 1d4chan Peasant Railgun 페이지 및 D&D 베이직 룰 참고
     * 현실적인 추가 변수와 정확한 피해량 계산 등은 원문의 수치를 일부 수정할 필요 있음

결론 및 활용 가능성

     * 농민 레일건은 D&D 5e의 규칙 구멍을 이용한 상상력 넘치는 밈임
     * 규칙적으로는 가능하나 실제 상황에서는 비현실적임
     * 이야기나 유머 용도로 좋은 소재임

        Hacker News 의견

     * 나는 Peasant Railgun 같은 플레이어 창의성에서 나온 엉뚱한 트로프들을 정말 좋아함. 옆길로 새는 문제 해결 방식이 DnD에서 가장 재미있는 순간 중 하나라 생각함. 물론 이런 장난들은 대부분 규칙의 의미를 지나치게 꼬거나 억지 해석에 기반하지만, 실제 RAW(규칙을 문자 그대로 해석)이나 RAI(의도된 규칙)에는 부합하지 않는 경우가 많음. 어떤 DM이 꼼꼼하게 이런 주장을 살펴보면 농담의 헛점이 어디에 있는지 금방 찾을 수 있음. DnD 저자들도 DM이 전투나 상호작용을 운영·판단하는 데 도움을 주기 위해 규칙이 있는 것이지, 물리·경제 규칙처럼 엄밀하게 시뮬레이션하라는 목적이 아님을 명확히 함. Peasant Railgun의 경우 내가 지적하고 싶은 부분은 이거임:
          + 규칙에는 물건이 생물 사이에서 전달될 때 속도가 유지된다는 말이 없음. 결국 마지막 전달자는 최초와 같은 속도의 물체를 받는 것임.
          + 투사체를 던지거나 쏘는 것은 ""떨어지는"" 것으로 간주하지 않음. 궁수가 100피트 떨어진 곳에 화살을 쏴도 100피트의 낙하 피해가 추가되는 게 아님. 물론 만약 DM이 진짜 말도 안 되는 상황을 장려하고 싶다면 그건 그것대로 재미라 생각함
          + TFA의 근본적인 문제는 플레이어가 자기가 시도하고 싶은 걸 묘사하면서 동시에 그 시도가 성공 여부와 결과까지 정해버리는 점임. 이건 D&D가 아님. 물론 플레이어가 여러 명이서 Ready 액션으로 물체를 빠르게 전달하려고 시도하는 건 RAW에서 가능하지만, 그 다음에 뭐가 일어날지는 DM이 결정함. Peasant들이 덱스 굴림을 해야 하는지 등등은 DM의 판정임. 그래서 TFA 같은 글이 혼란스러운 게, 낙하 피해 규칙을 오해해서가 아니라 적용 주체가 누군지도 헷갈려한다는 점이 더 크다고 생각함
          + TFA 덕분에 Peasant Railgun이 진짜 피해를 준다는 해석을 처음 봤음. 내가 봤던 다른 논의들은 다 피해를 못 준다 결론이 나왔었는데, 그게 오히려 더 유쾌함. 내가 좋아하는 D&D 3.5 버전의 두 가지 로직도 있음: 말을 타는 것이 자유 행동이라 말 고속도로를 만들 수 있다는 발상(말이 5피트마다 있고 탄채로 이동). 그리고 규칙상 대상을 먼 거리에서 볼 수 없어 달이 보이지 않는다거나, 익사 상태에 빠지면 물에서 나와도 멈출 수 없다(익사가 영구 진행)
          + 내 생각은 항상 이거임: 1. D&D 메커니즘은 현실을 단순화한 원시적 조작(“활 쏘기”, “아이템 전달”, “포션 마시기” 등)으로 구성됨 2. 현실은 그보다 훨씬 깊고 “플랑크 길이”나 “쿼크 스핀” 같은 원시 조작을 가짐 3. 현실과 단순화된 규칙은 항상 어긋나는 영역이 생기는데, 그 틈을 찾는 게 밈처럼 재미있을 수는 있지만 그게 익스플로잇은 아님. 우리는 현실 물리가 아니라 D&D 메커니즘의 조작으로 노는 거임
          + Peasant Railgun에서 나무 막대기가 겨우 6초에 몇 킬로를 이동한다면(평균 500m/s 이상 이동), 속도가 누적된다고 봐야 하지 않나 생각. 낙하 피해를 적용하는 게 가장 말이 되는 임시책이라고 봄. 이미 500m/s 이상의 속도로 이동 중인 막대를 즉흥 무기로 던지는 해석은 계산이 더 복잡할 뿐 아니라 어색함. 또 쿨함의 법칙이 있지 않나. 이야기와 재미있게 흘러간다면 그런 방식도 긍정임
          + DM이 Peasant Railgun을 허용할 거라 기대하진 않음. 하지만 우리가 실제로 허술한 시뮬레이션 세계에 산다는 가설을 연구하는 던전 크롤링 과학자 캠페인에서는 꼭 해보고 싶음. D&D 대학교 입학 사정관이 사람들을 모든 스킬별로 -5부터 +5까지 수치로 정확하게 분류하는 논리도 유쾌함
     * 내 친구가 만든 ""듀얼 옥토캣 플레일""이 생각남. 일반 플레일은 끝에 뾰족한 쇠공이 매달린 몽둥이 같은 무기고, 한번에 한 공격을 함. 듀얼 플레일은 두 개의 쇠공이라 두 번 공격함. 근데 각 쇠공 대신에 문어를 달고, 각 문어는 8개의 촉수마다 고양이를 한 마리씩 붙잡음. 공격하면 문어가 공격하고, 16마리 화난 고양이가 함께 공격하는 셈임. 그때 게임에선 동물이 특정한 조건에서 공격이 보장되는 규칙도 있었던 거 같음. 정말 OP 무기였음
          + 고양이에도 버터 바른 토스트를 묶었으면 무한 에너지도 가능했을 것 같음
          + 어떻게 문어가 고양이 8마리를 붙잡을 수 있음? 하루 이상 문어를 살려두는 것도 미스터리임
     * 우리 파티는 한때 DM을 설득해서 파티의 애완 염소가 전투에서 아주 약간의 피해를 줄 수 있게 했었음. 그 다음엔 근처 목동에게서 100마리를 사들여 염소 부대를 만들었음. 안타깝게도 곧 홍수가 나서 염소 부대 전멸
          + 유쾌한 DM들과 플레이해본 경험상 이런 황당한 장난에는 늘 신중해야 함. 염소들이 홍수로 사라진 건 놀라울 일이 아니고, 오히려 DM이 엄청 절제된 조치를 내린 게 더 인상적임. 내 예전 DM은 거의 확실히 그보다 더 혼쭐을 냈을 텐데, 즉 염소들이 밤새 파티의 모든 옷을 먹어버렸다든가 그런 전개도 나왔을 듯함
     * RPG 참여자는 스펙트럼이 있다고 생각함. 한쪽 끝에는 규칙에서 가능한 한 모든 이점을 최대한 뽑아내려는 미니맥서, 다른 한쪽 끝에는 스토리텔러가 있음. 나는 항상 스토리텔러 쪽이었고 Peasant Railgun의 기발함을 인정하면서도 그런 요소가 핵심인 게임은 그리 끌리지 않음. 만약 슬랩스틱을 노린다면 차라리 Paranoia처럼 그런 걸 적극적으로 장려하는 세계관이 더 적합함. 하지만 RPG에서 다양한 플레이어 욕구를 조율하는 게 가장 큰 도전 중 하나이고, 만약 테이블의 모두가 진심으로 뭔가를 해보고 싶어하면 어느 정도 허용해줘야 한다고 생각함
          + 나는 항상 둘 다였던 것 같음. 스프레드시트로 피해량 분석해서 최고의 특기·마법 선택도 해보고, DM은 내 스킬 보너스가 합쳐져 말도 안 되게 불가능할 것 같은 것도 실제 성공시키는 걸 보고 놀라기도 했음. 난 그런 비현실적 성공이야말로 D&D의 묘미라 생각함. 그래도 Peasant Railgun은 너무 엉뚱해서 시도해본 적은 없음
          + PathFinder로 Wrath of the Righteous 빌드를 보면 언제나 이런 점이 문제임. 방어력 올리려고 말도 안 되는 조합에 집착하다보니 팔라딘이 캠페인 중에 갑자기 마녀로 전직하는 식임. 롤플레잉은 가능하지만 결국 숫자놀음에 끌려가는 전개라 억지스러움
          + 오히려 삼각형처럼 봐야 한다는 주장도 있음. RAW와 RAI가 세 번째 꼭짓점이 됨. 미니맥스도 RAW로 극한을 노릴 수도 RAI 기반도 가능, RAI나 RAW나 홈브류로 기상천외한 짓 가능
          + 이런 논의 때문에 세션 0이 중요한 것임
     * 내 fellow DM들에게 조언하고 싶음. 만약 플레이어들이 Peasant Railgun 같은 걸 허용해달라고 끈질기게 나오면, 적대자(BBEG: King, Duke, Wizard, Lich 등)가 훨씬 더 많은 농민을 동원할 수 있다는 점을 유념해야 함
          + BBEG: ""이건 내가 져줘야겠네. 너희가 농민의 가치를 나보다 훨씬 더 높게 본 것 같구만. 정복한 마을들을 본격적으로 활용해야겠는걸""
          + 플레이어가 강력한 마법 아이템을 찾아내거나 레벨 17 마법사를 매수라도 한다면, Wish 마법으로 이런 ‘구멍’ 규칙 자체를 아예 없애버려 거의 불멸에 가까운 적을 평범한(혹은 불사?) 수준까지 끌어내리는 것도 상상 가능함
     * 내가 어릴 때 비행이 가능한 캐릭터를 만들었었는데, Endless Water Decanter가 강력한 추진력을 제공한다는 걸 발견했었음. 그리고 Helmet of Freedom of Movement는 모든 마찰, 특히 바람 저항을 없애줄 수 있다는 식으로 해석했었음(상세는 기억 잘 안 나지만 아무튼 이동을 막는 건 전부 해제). 결과적으로 무한 가속, 무한 속도. 실제로 캐릭터 질량과 Decanter 추진력을 기반으로 공식까지 일일이 계산함. 파티가 귀환할 때 ""가속·감속 감안해서 마을까지 17분 걸렸음"" 같은 식으로 행동했음. 지금 생각하면 내가 꽤 성가신 플레이어였던 것 같지만, DM이 참을성 있게 받아주셨음. 내 플랜에 노력 많이 들였던 걸 인정해준 듯하고, 내 인생에서 가장 인상 깊은 물리 적용 경험 중 하나였음
          + 만약 OP급 방패까지 이용해서 스스로를 비행 투사체로 쏴버렸으면 DM의 반응도 달라졌을 듯함. 그래도 너의 현명한 물리 적용은 재밌고 해롭지 않았던 것, 그리고 DM과 함께 그 기발함을 테이블의 재미로 전환한 것 응원함
     * 나는 DM 중심의 규칙이 많은 게임을 선호함. 플레이어 핸드북이 없는 게 더 나음. 캐릭터는 이야기 진행에 따라 성장하고, 일정 시점에 새 능력을 얻음. 예를 들어 ""네 캐릭터가 새 전투 동작을 개발하기 위해 투자한 시간이 많았다"" → 레벨업 처리와 함께 DM이 새 규칙 추가. 적 무장 해제 실력이 늘어서 디스암 시도가 더 쉬워진다거나 보너스 효과 추가. 물론 이건 DM에게 많은 부담이지만, Dungeon Crawler Classic 같은 아나키한 시스템의 긴장감도 있음. 캐릭터가 죽는 것도 기대하게 됨. 어떤 모험에선 내가 정체 모를 포션을 마셨더니 미스랄 동상으로 변해 이벤트성 마지막을 장식한 적도 있었음. 다른 플레이어는 온갖 책과 협상, 계약을 거쳐 악마의 지능있는 비행견을 직접 만들어냈고, 그 개를 통해 주문 시전·시야 공유가 가능해짐. 이런 모험을 통해 캐릭터를
       미니맥스용 기계가 아닌 진짜 캐릭터로 느끼게 되어 훨씬 더 재밌어짐. DM이 장난 결과의 판정도 전적으로 맡음. 그냥 농민 부대 고용한다고 끝나는 게 아니라, 일선장 임명, 보급, 항명, 탈영 같은 현실 문제에 먼저 직면. 실질 효과는 ladder를 적에게 던져 1d4 피해 줄까 말까에, 크리티컬 테이블에서 운 좋으면 적이 뒤엉켜 넘어지기도 할 수 있음
     * 현실 물리와 게임 메커니즘을 잘 맞을 때만 플레이어에게 이득이 되게 적용하는 것, 그리고 규칙을 맞지 않는 곳에 억지로 끼워 맞추는 게 문제임. 실제로는 게임 메커니즘 때문에 농민 사이에서 물건을 그렇게 빠르게 전달할 수 있는 것임. 마지막 사람이 던지는 순간, 규칙상 익숙하지 않은 무기 던지기(자벨린/즉흥무기와 같은 사거리 제한)로 처리됨. 실제 평균속도는 1900mph라고 하지만, 실제론 농민 사이에서 빠르게 전달되는 것이므로, 물체의 진짜 속도는 톱니파형처럼 급박하게 변함. 이런 초능력적인 feat은 RAW 자체의 힘임. 이건 단순히 생물 간에 물체 전달이지 낙하 물체 규칙은 전혀 적용 안 됨
     * 5e에서 이게 처음 등장했을 때 왜 아무도 낙하 피해가 플레이어 캐릭터만 적용되고 20d6에서 최대치가 있다는 사실을 언급하지 않았는지 놀랐음(PHB 기준). 떨어지는 물체에 맞는 대상에 대한 별도 규정은 없고, 얼마나 위험하냐에 따라 피해 주사위 임의 적용이 있음. 본질적으로 Peasant Railgun 컨셉은 애초에 홈브류임. 가정적으로 낙하 피해 규칙을 던진 물체에 적용하더라도 최대가 20d6임. 실제로는 즉흥 무기 던지기로 간주해서 1d6(+덱스, 커먼은 0) 피해, 주요하게 명중을 위한 비숙련 공격 굴림 필요. 그 이유는 DnD가 물리 시뮬레이터가 아니기 때문임. 규칙 적용을 원한다면 '던진 물체'로 보는 게 맞음. 이동거리로 운동량을 추가해서 피해량을 올리는 건 완전히 홈브류 영역이고 RAW와 무관함. 영원한 DM 자격으로 밝힘
     * 대학생 시절 다양한 Tabletop RPG를 자주 했음. 그중 DragonQuest는 규칙도 복잡했고 하우스룰도 훨씬 더 많았음. 우리가 쓰던 일반 원칙은 '규칙만 허용한다면 아무리 말도 안 되는 행동이라도 허락'이었음. 단, 이후 세션에서 계속 통할지는 세션 뒤에 논의해서 결정. 즉, 그날만큼은 뭐든 통한다고 봄. 예를 들어 어떤 캐릭터가 개구리로 변해서 지붕에서 떨어지는 상황에서, Shadow Wings(어둠의 날개) 마법을 발동해 거대한 날개로 비행하려 했음. 문서상으론 인간급을 띄우는 15~20피트급 날개였으나, DM은 개구리는 현실적으로 비행 못한다고 선언. 긴 논쟁 끝에, 모든 것은 한번은 날 수 있지만 개구리는 예외란 명언이 탄생함. ""Everything flies once, except a frog""
"
"https://news.hada.io/topic?id=21717","미 국방부, 위성 기상 데이터 제공 중단 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       미 국방부, 위성 기상 데이터 제공 중단 발표

     * 미국 국방부가 앞으로 위성 기상 데이터 제공을 중단함
     * 지난 40년 넘게 군이 제공하던 이 데이터는 허리케인 예측 및 해빙 관측에 꼭 필요한 정보임
     * 사이버보안 우려로 인한 조치라 밝혔으나, 구체적인 이유는 공개하지 않음
     * 대체 위성 데이터는 있지만, 실시간성이 떨어져 정확한 예측에 공백 우려가 존재함
     * 이번 결정은 지구온난화로 빠르게 강해지는 폭풍과 지속적으로 줄어드는 해빙 모니터링에 중대한 영향을 줄 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미 국방부 위성 기상 데이터 제공 중단 결정

  국방부 위성 데이터의 중요성

     * 미 국방부는 40년 넘게 대기 및 해양 상태를 관측하는 위성 데이터를 운영·제공해 왔음
          + 이 데이터는 해군 산하 Fleet Numerical Meteorology and Oceanography Center가 가공하여 과학자 및 기상 예보관에 전달함
          + 실시간 허리케인 예보, 극지 해빙 측정 등 다양한 목적으로 사용됨
     * 이번 주 미 국방부는 이 데이터를 더 이상 제공하지 않겠다고 NOAA 공지를 통해 발표함

  예보 연구현장의 반응

     * Brian Tang(앨버니대 허리케인 연구자) 는 “이 데이터는 허리케인 예측 및 해빙 관측에 꼭 필요한 정보”라 설명함
     * 해군은 데이터 제공 중단 이유에 응답하지 않았음
     * 미 우주군(위성 책임 부서) 은 위성 및 장비는 여전히 동작 중이며 군 내부용으로만 데이터 활용 계속할 것이라 밝힘
     * National Snow and Ice Data Center(콜로라도) 의 Mark Serreze 소장에 따르면, 미 국방부는 예산이 아닌 “사이버보안 우려”를 중단 사유로 전달했다고 설명함
     * 사이버보안 구체적 우려 사항에 대해 해군은 답변을 제공하지 않았음

허리케인 및 실시간 관측의 영향

     * 국방부 위성 데이터는 전 세계를 움직이는 군 선박/항공기 운용에 필요해서 직접 수집됨
     * 동시에 이 데이터 덕분에 예보관들은 허리케인이 형성·강화되는 과정을 MRI처럼 살펴볼 수 있음
          + 신생 허리케인 중심을 파악해 이동 방향과 상륙 가능성 조기 분석 가능
          + 허리케인 내부의 새로운 눈 벽 형성 감지로 빠른 세기 변화를 미리 포착함
          + 예를 들어, 최근 허리케인 Erick의 눈 벽 구조 변화를 추적해 예측모델보다 빠른 강한 세기 변화를 조기에 경고할 수 있었음
     * NOAA도 이번 중단이 올해 예보 정확성엔 영향이 없을 것이라 주장하지만, 실제로는 실시간 정보 공백이 생길 위험이 존재함
          + 현실적으로 NASA, NOAA, 외국 위성들도 자료를 제공하지만, 한정적 커버리지로 인해 실시간성 확보에 한계가 있음
     * 허리케인이 급발진해 상륙 직전 강해질 때 신속한 대비·대피가 어렵게 되며, 이는 대재해 위험성 증가로 이어질 수 있음

극지방 해빙 관측 공백 문제

     * 국방부 위성은 실시간 해빙 상태를 추적하는 주요 자료원임
          + 북극·남극의 해빙은 연중 급격한 변동을 보이고, 북극 해빙 감소 시 국제 해상 운송경로 단축 등 영향이 큼
          + 남극 해빙은 빙하 융해 속도를 늦춰 해수면 상승 억제에 핵심적임
     * 이번 중단으로 인해 6개 주요 해빙 데이터 세트가 중단될 예정임
          + National Snow and Ice Data Center는 9월 이후 이용불가 예고를 받았으나, 최근 기한이 6월 30일로 앞당겨져 빠른 전환에 어려움이 있음
     * 대체 자료로 일본 정부 위성 센서 데이터 활용을 추진 중이나, 교정·전환에 시간이 필요해 중단 사태로 실시간 모니터링에 큰 공백이 우려됨
     * 2025년 북극 해빙은 위성관측 이래 최저치를 기록하고 있어, 과학적·안전 측면에서 치명적 영향을 받을 수 있음

추가 인터뷰 요약

     * 국방부는 군사적 필요로 위성 기상정보를 보유·활용하며, 1970년대 이래 과학자와 예보관과 공유해 왔음
          + 위성 자료는 전 지구 차원 기상 관측을 가능케 함
     * 미 우주군은 위성과 센서에 이상이 없고, 군 내부에서만 계속 사용할 예정이라 밝힘
          + 사이버보안 우려 외 구체적 중단 이유는 불투명함
     * 기상학계는 허리케인 실시간 추적의 중단으로 예보의 정확도 하락과 공백 발생 가능성을 심각하게 우려함
          + 특히 지구온난화 영향으로 급강화 폭풍이 빈번해지고, 예측 실패시 인명·재산 피해가 커질 수 있어 우려가 큼
          + 올 해 대서양 허리케인 시즌이 매우 강할 것으로 예상되어 영향이 더 클 수 있음

        Hacker News 의견

     * 기후 변화와 관련된 정부 기관들, 특히 NOAA에 영향을 주는 방식으로 기후 변화에 대한 서사를 통제하려는 시도인지 궁금증 제기
          + Don’t Look Up이라는 영화 제목 언급으로 상황의 심각성 강조 농담
          + 맞음이라는 의견과 함께 Project 2025 내용을 인용하여 NOAA 해체 및 축소 주장 강조, 현재 NOAA 조직이 미국의 번영에 해롭다고 주장하는 산업의 주축이라는 의견 공유, NOAA의 주요 기능(환경 정보 서비스, 환경 관리, 응용 과학 연구)이 민간 업체로 이전될 수 있다고 언급, NOAA를 폐쇄해 기후 변화 증거와 대응 연구를 억제하려는 의도 해석, Accuweather CEO인 Barry Myers가 트럼프 행정부 NOAA 수장으로 임명될 뻔한 사례 언급, 현재도 비슷한 노력이 계속 중이라는 우려 표출, 관련 링크(https://envirodatagov.org/project-2025-national-oceanic-and-atmospheri…) 공유
          + 이건 2015년에 의회 결정으로 종료된 사안이고 JPSS 프로그램으로 대체되어 이미 정상적으로 운영 중이라는 사실 전달
          + 이 조치는 NOAA의 기상 예측 능력을 약화시켜서 날씨 정보에 의존하는 사람들이 민간 기업에 별도로 돈을 내야 하는 상황을 만들려는 시도라는 주장, Accuweather CEO가 트럼프 캠프에 기부한 사실이 ‘우연히’ 겹친다고 비꼬는 논조
     * 미국 시민이 아니며, 기상 데이터 덕분에 태풍을 미리 알고 생명을 구한 경험 공유, 소규모국은 우주 프로그램을 유지할 인프라나 인력이 없어 다른 국가의 데이터 의존성 높음, 앞으로 전 세계에 어떤 영향이 있을지 궁금함, 국제적인 기상 협력과 데이터 공유의 현실에 대해 궁금증(유럽, 중국, 인도, 러시아 등의 데이터 공유 여부 등) 제기
     * 세계는 긴밀하게 연합해야 한다는 강조, 미국이 더 이상 신뢰할 수 있는 동맹이 아니라는 우려
          + 미국이 더 이상 안정적인 거주지가 아닐 수도 있다는 우려
          + 그렇게 생각한다면 직접 비용을 부담하고 추진하라는 입장
     * 전쟁 시 위성 이미지 접근성을 제한하는 일이 과거에도 여러 번 발생한 사실 언급 및 링크(https://niemanreports.org/press-access-to-satellite-images-is-a-casual…) 공유
     * NOAA가 앞으로 접근하지 못할 데이터에 대해 설명, 국방부 데이터를 통해 허리케인 실시간 추적이 가능하고, 이는 조기 대피 및 준비에 매우 중요함, 대중이 세금으로 이미 지불한 데이터임을 강조, 데이터를 의도적으로 은폐해 국민 구조 기회를 박탈하는 것은 단순한 절도가 아니라 악의적인 행위로 볼 수 있다는 입장
          + 대중이 지불한 데이터라면 주기적으로 FOIA(정보공개 청구) 신청이 필요하다는 제안
          + NOAA의 공식 입장 인용, 국방부 데이터가 없어도 올해 예보 정확도가 떨어지지 않을 것이라는 설명, 첨단 데이터와 모델로 국민에게 최상급 예보 제공 가능하다는 NOAA 측 설명
          + 국민을 위험에 빠뜨리면서도 대다수 국민이 그들을 계속 뽑을 것이라는 분노 섞인 우려
          + '악의적인 행위'라는 표현을 사용하는 것의 의미와 대중, 정치권, 커뮤니케이션 프레임 효과에 대해 고민, 적절한 메시지 전달의 중요성 강조, 예산이 힘의 원천임을 상기함과 동시에 효과적인 메시지 전략에 대한 자기 성찰
     * 3가지 가능성 언급: 1) 기후 변화와 관련된 나쁜 소식 차단 목적 2) 특정 민간 기업에 데이터 판매 기회 부여 3) (음모론적으로) 뭔가 노출시키고 싶지 않은 것을 숨기려는 시도
          + 네 번째 가능성 제시, 2015년에 프로그램이 종료됐고 대체 시스템이 이미 존재함(JPSS), 관련 링크(https://en.wikipedia.org/wiki/Joint_Polar_Satellite_System) 제공
          + 별다른 비밀이 아니라 Project 2025 실행이라는 주장, 관련 링크(https://envirodatagov.org/project-2025-annotation/) 공유
     * $10 내기에 비유하며 GPS 무료 제공도 언젠가 유료로 전환될 수 있다는 우려
          + 러시아, 중국, 유럽 등이 유사한 위성 내비게이션 시스템을 운용 중이며, GPS가 사라지면 큰 혼란은 있지만 대체 불가하진 않음, 관련 링크(https://en.wikipedia.org/wiki/Satellite_navigation/…) 공유
          + GPS가 정말 유료화 될 수 있는지 질문, GPS 신호를 그냥 수신하는 방식 아닌지, GPS 기술이나 위성 현황에 대한 무지 자각, GPS 위성이 계속 새로 발사되는지, 아니면 옛날 것만 운용 중인지 궁금증
          + 군용 GPS의 혜택 및 소비자 접근성 확대 덕에 GPS가 널리 보급되고 유지되었다고 회상, 정확한 위치 정보를 통제하면 군에도 도움이 되지 않는 아이러니 강조
          + GPS는 이미 여러 지역에서 재밍(jamming)으로 인해 부분적으로 제한되고 있는 현실 언급, 대체 가능성이 높은 것은 Starlink 위성이라는 의견, 최근 우크라이나 사례를 계기로 비재밍 GNSS의 잠재적 위험성 지적, 드론 확산과 맞물려 우려 표출
     * 지나치게 과장된 의견이 많다는 지적, DMSP 프로그램은 2015년 의회에 의해 중단됐고 실제론 대부분 위성이 고장났다는 사실 설명, 현재 남아있는 건 NOAA-19 뿐이며 이 역시 JPSS로 넘어갔음을 강조, JPSS는 GEOSS의 일부라는 설명과 함께 각각의 위키 링크 공유(DMSP, NOAA-19, JPSS, GEOSS)
     * 내 생각엔 위성의 위치가 기상 데이터로 추적될 수 있다고 봄, 전시 상황에서 이 위성이 표적이 될 수 있다고 추정
          + 위성 위치 정보는 이미 공개된 정보이고, 지상에서 아마추어들도 쉽게 추적 가능함, 연료 소모도 높아 궤도 변경은 어렵다는 의견, 관련 스파이 위성 추적 사례 링크(https://www.popsci.com/zuma-spy-satellite-amateur-astronomer/) 공유
          + 태양 동기궤도 위성은 하루에 여러 번 지구를 돌며 지상 기록과 비교해 빠르게 이동하기 때문에 관측 정보 공개를 지연시키면 위치 노출 문제는 없음, 실제로 문제 되는 건 정지위성이며, 대부분 위성은 은폐 기술이 없어 관측 가능, 미래엔 위성 스텔스 기술이 발전하면 현실 문제가 될 가능성, 마지막에 번역임을 밝힘
          + 가장 그럴듯한 이유는 국가안보 우려, 국방 당국이 언제나 정보에는 매우 민감하며 지도자 교체 후 정책 변화 가능성, 해군 쪽이 보안 이슈를 명분으로 데이터 폐쇄에 나선 듯, 공식 발표는 ‘사이버보안 위험성’ 언급 외엔 없음, 관련 기관 정보(https://en.wikipedia.org/wiki/…), 슈퍼컴퓨터 관련 뉴스(https://montereycountynow.com/news/local_news/…) 공유
          + 사실 내가 틀릴 수도 있겠음, LEO 위성은 광학적 추적이 꽤 쉬운 편이란 사실 발견
"
"https://news.hada.io/topic?id=21712","자바스크립트 상표 업데이트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             자바스크립트 상표 업데이트

     * 미국 특허청 항소심판원이 Oracle에 대한 Deno의 사기 주장을 기각함
     * Deno는 Oracle이 2019년 상표 갱신 신청 시 Node.js 웹사이트를 무단 사용했다고 주장
     * 현재 소송의 중심 쟁점은 일반 명칭화(genericness) 및 상표 포기(abandonment) 임
     * Oracle은 8월 7일까지 일반 명칭화와 상표 포기에 대한 주장에 공식 답변해야 함
     * JavaScript 상표가 소멸되면 모든 개발자에게 자유롭게 사용 가능한 프로그래밍 언어 이름이 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

자바스크립트 상표 소송 경과

     * 2024년 6월 18일, 미국 특허청 상표 심판원(TTAB) 은 Deno의 Oracle을 대상으로 한 사기 주장을 기각함
     * Deno는 Oracle이 2019년 상표 갱신 과정에서 ""JavaScript"" 상표 사용 증명 자료로 Node.js 공식 웹사이트의 스크린샷을 제출했다고 지적함
          + Deno 측은 Node.js의 창시자로서, Node.js는 Oracle의 제품이나 브랜드가 아니며, Oracle이 해당 자료를 활용할 권한이 없음을 강조함
          + Oracle이 제3자의 오픈 소스 사이트 자료를 제출했다는 점에서, 상표 실제 사용 증거 확보가 어려웠음을 반영한다고 비판함
     * 하지만 사기 주장 자체는 이번 소송의 핵심이 아님을 밝히며, 별도 수정 없이 그대로 진행 예정임
          + 사기 주장 관련 소송 절차를 늘리지 않고 중요한 핵심 쟁점, 즉 ""일반 명칭화(genericness) ""와 ""상표 포기(abandonment) ""에 집중하고 있음

소송의 진행 방향 및 주요 일정

     * 소송의 핵심은 ""JavaScript""가 브랜드나 Oracle의 제품명이 아닌, 모두가 사용하는 프로그래밍 언어의 이름임을 주장하는 점임
     * 2024년 8월 7일까지 Oracle은 Deno의 상표 취소 청구서의 모든 문단에 대해 각각 동의 또는 반박 입장을 서면으로 제출해야 함
     * 본격적인 사실조사(Discovery)는 9월 6일부터 시작됨
     * Deno 팀은 Oracle이 어떤 부분을 반박할지 주목하고 있음

자바스크립트는 누구의 것인가

     * 개발자 커뮤니티와 산업계 모두 ""JavaScript""가 특정 기업의 상품명이 아님을 인식하고 있음
     * 실제로 19,550명 이상이 javascript.tm에 동의 서명을 남김(집계 당시 기준)
     * Deno는 현 상표권이 대중과 업계, 상표법의 본래 목적에 부합하지 않는다고 지적함

만약 상표 취소가 이루어질 경우

     * Deno 측은 상표 취소 승소 혹은 Oracle이 상표권을 자진 반환할 경우를 전망함
     * JavaScript 이름이 모두의 것이 되어 ""™ 기호를 더 이상 쓰지 않고, 라이선스 우려 없이 자유롭게 사용할 수 있게 됨""을 언급함
     * 웹을 움직이는 프로그래밍 언어의 이름을 모든 개발자가 공동으로 자유롭게 쓸 수 있게 됨

        Hacker News 의견

     * Oracle이 JavaScript라는 이름이나 브랜드에서 전혀 이익을 얻지 못한다는 점을 알고 있음에도 불구하고, 이 소송을 계속하는 이유를 이해하지 못하겠다는 의견, 이 기회에 JavaScript 상표권을 개발자 커뮤니티에 기부함으로써 좋은 이미지를 남길 수 있을 텐데 굳이 이익도 없는 것을 지키는 태도가 정말 터무니없다고 생각함
          + Bryan Cantrill의 말을 인용하며 Oracle에 대해 오픈 마인드를 가질 필요가 없다는 조언, Oracle과 Larry Ellison을 인간처럼 생각하지 말고 그저 잔디깎기 기계처럼 여겨야 한다는 내용, 잔디깎기는 감정이 없고 단지 손을 넣으면 손가락을 잘라버릴 뿐이라는 비유 소개, 관련 유튜브 링크 https://www.youtube.com/watch?v=-zRN7XLCRhc&t=1981s
          + Oracle이 ZFS와 GPL을 통합할 수 있도록 CDDL을 간단히 업데이트하는 것만으로도 브랜드 이미지를 90% 회복할 수 있을 거라고 생각, 실제로 큰 비용도 들지 않을 거라 판단하지만, 결국 우리가 잔디깎기 기계를 인간처럼 보고 있다는 실수임을 인정
          + Oracle은 이제 본질적으로 기술/소프트웨어 회사가 아니라 '법률 회사'라는 관점, 그들이 존재하는 이유는 단지 소유한 것들의 라이선스를 가능한 오래 판매하는 것뿐이므로 실제 가치와 무관하게 모든 것을 붙잡으려는 것이 자연스러운 현상이라는 시각 공유
          + 잔디깎기 기계에게 '호의'라는 개념은 아예 없다는 비유 반복
          + Oracle에는 변호사들이 자신의 연봉을 정당화해야 하므로 이유가 필요, 아무 대가 없이 뭘 내어주는 것은 시장 논리에 맞지 않다는 주장
     * Oracle이 과거에는 기술 생태계에 가치를 추가한 적도 있지만, 지금은 거대하고 임대료를 뜯어내는 존재가 된 것에 아쉬움, 2025년이 되어도 Oracle이 소유한 IP 때문에 새로운 것들을 못하게 되는 현실이 싫음, 기업들이 브랜드 이름에 안주하다가 이런 결과를 맞는 사례에 대한 비판, 이제 그런 시대가 끝나길 바람
          + Sun Microsystems가 분명히 엄청난 가치를 더한 회사였다는 점, 반면 Oracle의 기여는 인수한 업적들을 제외하면 분명치 않다는 지적
          + 기업의 인센티브 구조 자체가 직원들을 위험 회피적으로 만들기 때문에 그런 시대는 사실상 절대 끝나지 않을 거라는 현실적인 관점
          + Oracle이 기술 생태계에 실제로 언제 어떤 가치를 더한 적이 있는지 의문이라는 질문
     * Oracle의 조직 구조를 설명한 만화가 정말 딱 맞는 비유라는 생각, 링크 공유 https://newsletter.pragmaticengineer.com/p/…
     * 모두가 “JavaScript”를 그저 하나의 언어를 일컫는 명칭으로 사용하고 있다는 관찰, Oracle은 기생충 같다는 비판
          + Kleenex처럼 어떤 단어가 너무 보편적으로 쓰이게 되면 관련 법이 있는지 의문 제기
          + JavaScript가 상표권이 있다는 사실을 전혀 몰랐다는 고백, 그저 프로그래밍 언어의 이름이라 생각했고 Oracle과 연관된 줄도 모르고 있었음, 이 회사가 이런 상표권을 보유하고 있다는 사실이 참 놀랍게 느껴짐
          + 만약 Oracle이 이기면 언어 이름을 JS로 바꾸자는 제안, JS는 아무 의미도 없는 약자라는 발상
     * Deno에서 “JavaScript는 Java와 아무 상관 없습니다 (법정 소송 제외)”라는 슬로건으로 캠페인을 시작하면 좋겠다는 제안, 자신도 기부 의사가 있음
          + JavaScript와 Java가 단순 문법적 유사성 외에도, 런타임에서 서로 호환되지 않는 객체지향 데이터 모델을 최초로 강제한 언어들이라는 사실, 이 두 언어가 월드와이드웹 초창기 관심 덕분에 대중적으로 급성장한 것도 유사점이라는 배경 정보 제공, Perl 정도만이 비슷한 범주의 예외로 언급
     * JavaScript 관련 정보 사이트 링크 소개 https://javascript.tm/
          + 트래픽 폭주로 접속이 힘들다는 경험 공유
     * Oracle이 javascript 상표권을 java 상표권에 넘기면 뭔가 위험하다고 느끼는 듯한 추측 제시
     * “Larry Ellison Script”라는 언어를 만들고 상표 등록해볼 수 있는지 농담 섞인 아이디어
          + 자신은 변호사가 아니지만 이론적으로 가능하지 않을까 생각
     * 인간의 사회가 이런 문제를 겪는 모습을 보면 인류가 정말 끝나가고 있다고 느껴진다는 비관적인 감상
     * 그냥 언어 이름을 typescript나 webscript 등으로 바꿔 부르기 시작하면 될 일이라며 소모적인 논쟁에 시간 낭비하지 말자는 의견
"
"https://news.hada.io/topic?id=21821","남극해에서 주요 해류 순환의 역전 현상 감지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        남극해에서 주요 해류 순환의 역전 현상 감지

     * ICM-CSIC의 위성 데이터 알고리듬을 통해 남반구 해류 변화가 최초로 감지됨
     * 남극해의 순환 역전은 표면염분 상승 및 해수의 수직 흐름 변동으로 판단됨
     * 이러한 변화로 온실가스(CO₂) 와 열이 대기에 방출되어 해빙 융해 가속 현상 발생함
     * 기술적 혁신으로 극지방 표면 염분 데이터를 새롭게 확보함
     * SMOC 변화가 지구 기후 시스템 전체에 파급 효과를 미칠 가능성 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 개요

     * ICM-CSIC에서 개발한 위성 데이터 처리 알고리듬이 남반구 거대 해류 변화 감지에 결정적 역할을 한 것으로 밝혀짐
     * 지구 관측 위성 덕분에 국제 연구진이 남극해 해류 역전 현상을 최초로 포착함
     * National Oceanographic Center(영국) 주도로 진행된 이번 연구는 PNAS에 게재되었으며, ICM-CSIC의 SO-FRESH 프로젝트(유럽우주국 지원) 하에 수행됨

주요 발견 및 현상 설명

     * 2016년 이후 남극해 폴라-서브폴라 순환지대에서 표면 염분의 장기적 증가가 지속적으로 관측됨
     * 이 수분 조성 변화로 남반구의 심층 순환(SMOC) 경로가 변화하며 역전 현상이 발생함
     * 즉, 원래는 표면수가 심층으로 침강했으나, 최근에는 심층의 따뜻한 물과 이산화탄소가 표면으로 상승 전환됨
          + 이로 인해 해양에 수세기 동안 갇혀 있던 열과 이산화탄소가 대기로 방출되기 시작함

파급 효과 및 우려

     * 연구팀에 따르면, 심층 온난수 및 CO₂ 유출로 남극해 해빙 손실이 가속화되는 것으로 나타남
     * 장기적으로 심층 저장 탄소 방출로 대기 중 CO₂ 농도가 현 수준의 두 배까지 상승할 가능성도 제시됨
          + 이는 글로벌 기후에 심각한 파국적 결과를 야기할 위험을 내포함

기술 혁신 및 데이터 획득

     * 극저온과 해빙 변화가 빈번한 남극해는 기존 위성 관측에 어려움이 있었음
     * ICM-CSIC 산하 Barcelona Expert Center(BEC) 팀은 유럽 SMOS 위성을 위한 신규 데이터 프로세서를 개발함
          + 이 프로세서는 극지방의 지리적·기후적 변동성에 특화된 설계임
     * 덕분에 남극해 표면 염분 데이터의 품질이 크게 향상되어 빠른 해빙 손실 원인에 대한 일관된 설명이 가능해짐

기후 시스템 내 남극해 역할 재정의

     * 이번 연구로 남극해(SMOC) 의 역할이 지구 열 및 탄소 조절에서 매우 핵심적임이 부각됨
     * SMOC의 변화는 AMOC 등 다른 해양 순환 시스템에도 연쇄적 영향을 미칠 가능성 시사
          + 유럽 및 타지역 기후 변동성에도 영향을 확장할 수 있음

미래 관측·연구 추진

     * BEC는 ESA 지원으로 2025년 ARCTIC-FLOW(북극 해수 및 밀도 흐름 연구), CCI OSHF(해수면 열 플럭스 분석) 등 신규 프로젝트를 시작할 예정임
     * 두 프로젝트 모두 가속화되는 기후변화 모니터링 및 예측에 핵심적인 위성 기반 신규 관측 기술 개발을 목표로 함

결론 및 시사점

     * ""지구는 점차 임계점을 넘는 강한 신호를 보내고 있음""이라고 연구진은 평가함
     * 위성 등 첨단 관측 기술 덕분에 평소 접근이 어려운 남극해에서 이전까지 볼 수 없었던 변화 포착이 가능해진 상황임

        Hacker News 의견

     * 남극 남쪽의 심해 분출공은 실제 존재하지만 크기가 작고 연간 수치도 몇 억 톤(Pg C yr⁻¹) 정도임을 설명함 대기 중 CO₂가 두 배로 증가할 수 있다는 주장은 실제 관측값과 물리적 한계를 크게 과장한 주장임을 지적함 남위 60도 이남의 심해의 CO₂ 유출에 대한 최고로 낙관적인 추정도 연간 0.36 Pg C 수준임을 강조함 이 속도가 3배가 되어도 895 Pg C를 추가하는 데 800년 이상 걸리기 때문에, 언론 보도 자료의 주장은 근거가 없음을 밝힘 염분 역전 현상으로 인해 빙붕이 더 따뜻한 아랫물에 노출되고, 남대양의 탄소 흡수 능력이 조금 줄어들며, 대기 순환에도 영향을 줄 수 있음을 설명함
          + 본문과 PNAS 논문(https://www.pnas.org/doi/10.1073/pnas.2500440122)를 확인함 남극 심해 분출공 언급은 논점에서 벗어난 내용임을 강조하며, 실제로 논쟁거리는 남대양에서 CO₂가 풍부한 찬 물이 상승하는 현상임을 설명함 기사 요약문은 CO₂ 연결고리를 만들지만, 실제 PNAS 논문에서는 CO₂ 자체 언급이 없고 해수 용승에 중점을 둠 또, 현재의 빙하/해양 상호작용 모델이 관측 추세를 충분히 설명하지 못한다는 점도 논문에서 지적함
          + ""Pg C yr⁻¹""이 ""페타그램 탄소/연""이라는 의미가 맞는지, 그리고 ""C""의 질량과 ""CO2"" 질량 간의 차이가 궁금함을 질문함
          + 남극 남쪽에 심해 분출공이 정확히 어디에 위치하는지 궁금함 지도를 찾아봤지만 남극 남쪽에는 아무 것도 없다고 느낌을 전함
          + 특정 서브스레드에 달렸던 댓글이지만, 내용이 좋아 따로 떨어트린다는 맥락임
          + 이런 선정적인 제목들이 자주 등장하며, 기후 과학의 중요한 논의가 혼란에 빠진다는 점을 지적함 반복적으로 나타나는 자극적인 보도가 불필요한 공포심을 유도하고, 그 틈을 부정론자들이 이용해 신뢰를 깎는다는 두 가지 문제를 강조함 이런 연구들은 결국 기후변화가 더 심해지고, CO₂를 증폭시키는 피드백 루프가 나타남을 보인다는 점을 놓치지 말아야 함을 말함
     * 기사에서 말하는 '심해가 더 따뜻해졌다'는 주장이 의아함 일반적으로 심해는 차갑고 표면이 더 뜨겁지 않냐는 궁금증을 가짐 (https://en.wikipedia.org/wiki/Upwelling) 2023년 연구(https://www.sciencedaily.com/releases/2023/03/230330102327.htm)에서 남극 해류 순환이 느려지는 현상을 관측함 남극에서 차가운 물이 가라앉아 북쪽 심해로 퍼져나가는데, 이 과정이 느려지면 심해가 더 따뜻해질 수 있음을 정리함 기사에 링크된 논문(https://www.pnas.org/doi/epub/10.1073/pnas.2500440122)을 인용해, 남극 해양 표면에 차갑고 신선한 물, 그 아래에는 더 따뜻하고 염분이 높은 물이 있는데, 최근 수십 년의 변화로 심해의 열이 깊은 곳에 갇히다가 2015년 이후 표층 염분이 오르고 혼합이 다시 일어나는 경향을 설명함
          + 심해수가 더 따뜻해지는 것에 대해 추가적으로 설명하는 링크를 공유함 (https://en.m.wikipedia.org/wiki/Circumpolar_deep_water)
          + 심해에 '열이 갇혔다'고 해도 절대적인 의미에서 심해가 뜨겁다는 뜻이 아니고, 표면보다 상대적으로 염분이 높고 밀도가 크며 약간 더 따뜻하다는 의미임을 보충 설명함
          + 고위도에서는 해양 온도가 특정 깊이에서 거의 일정함을 언급함 남극 주변 표면 온도는 순수한 물에서 최대 밀도가 되는 온도(약 4도)보다 낮고, 남대양 염분 농도에서는 밀도가 가장 높은 온도가 0도 아래로 내려갈 수도 있음 이 현상이 해수의 밀도변화와 연동되어 해수 순환을 설명함
            참고 링크도 같이 전달함([https://en.wikipedia.org/wiki/File:ThermoclineSeasonDepth.png](https://en.wikipedia.org/wiki/File:ThermoclineSeasonDepth.png), [https://en.wikipedia.org/wiki/File:T-S_diagram.pdf](https://en.wikipedia.org/wiki/File:T-S_diagram.pdf))
          + 수온이 약간만 달라져도 게 같은 생물의 대사에 엄청난 변화가 올 수 있음을 상기함
            관련 기사 링크 공유(https://globalseafood.org/advocate/…)
     * 만약 심해수가 이제 상승해 수 세기 동안 저장된 CO₂를 방출한다면, 지구 기후 순환의 중대한 변화라는 점을 강조함 이런 현상이 위성 데이터 처리기의 혁신 덕분에 비로소 관측 가능해졌다는 사실도 신기함
          + 이 현상이 위성 센서의 혁신으로 밝혀진 것이 인상적임을 밝힘 인류가 관측 수단을 갖춘 기간이 짧기 때문에, 과거에도 이런 일이 있었지만 관측하지 못했을 가능성을 제기함
          + 왜 심해수가 표층보다 더 많은 CO₂를 녹이고 있는지 궁금증을 표현함
     * '삼체' 소설 예시처럼, 외계인이 과학적 방법을 교란해 인류를 지배하기 쉬운 상태로 만든다는 설정에 빗대어 현실의 AI(LLM)을 언급함 오늘날 LLM 같은 AI가 오히려 과학적 토론을 흐리게 만들고, 선정적인 기사화와 AI 증폭, 사실 위에 사실 코멘트를 덧붙이는 구조 등으로 진짜 논의가 뿌옇게 되는 느낌임 때론 인간이 이미 외계인의 침공 아래에 있는 듯한 기분이 든다고 고백함 행동이 아닌 토론만 반복하는 세태에서 깨어나야 함을 강조함
          + 과학 자체에 대한 논란보다, 실질적인 미온적 대응의 주 요인은 죄수의 딜레마적 국면과 석유업계의 정치적 영향력임을 설명함 지구온난화의 큰 흐름은 이미 자명함을 강조함
          + 이미 수십 년간 인간이 직접 이런 식의 논의를 반복해 왔다고 설명함 LLM이 이런 걸 자동화했을 뿐, 근본적 사회 문제는 그대로임 모두가 현실을 제대로 인식해도 해결책에 따른 비용을 서로 전가하고 싶어 하므로 실제 행동은 거의 없음 게다가 이 상황을 아예 '사기'라 믿는 영향력 있는 세력이 있어 진전이 사실상 불가능에 가까움 실제로 문제는 빨라지고 있다는 우려를 덧붙임
          + 과학과 선정적 저널리즘을 구분하지 못한 채, 선정적 기사와 잦은 코멘트로 과학 자체가 흐려진다는 불만을 토로함 동시에 만약 적대 AI가 2011년에 인류 멸망 유도를 시작했다면 오늘과 똑같을 것 같다는 불안감도 표현함
          + LLM이 잘만 쓰면 정보 홍수 속에서 오히려 맞춤형 학습과 정보 탐색에 큰 도움을 줄 수 있으므로, 양날의 검이지만 희망도 보임을 밝힘
     * 대부분의 기후 연구는 낙관적~비관적 전망 범위를 제시하고, 지난 30년간 예측과 실제 일치 여부가 궁금하다는 의견임 왠지 비관적 전망이 더 많이 실현된 느낌임
          + 예전 기사에서(https://scientificamerican.com/article/…, https://vice.com/en/article/…) 기존 전망들이 너무 보수적이었다는 사례를 링크로 공유함 최근엔 '염세론(doomerism)'이 강하게 억제되다 보니 부정적 뉴스가 제대로 다뤄지지 않는 분위기임을 지적함
     * 남대양에 대해 잘 몰랐는데, 간단한 영상이 유튜브에 있음을 추천함
       관련 영상 링크
     * 이 내용을 아주 쉽게 풀어줄 수 있는지, 그리고 그 결과가 뭔지 궁금함을 질문함
          + 기상 패턴이 예측불허가 되어 사막에 폭우/몬순이 내리고 몬순에 의존하는 국가가 건조해지는 식으로 극단적 변동이 생길 수 있음을 설명함 이 결과로 투자 불확실성이 커지고, 집이나 공장이 사막 계곡에서 침수 위험에 노출되고, 발전소가 있는 댐도 제역할을 못 하게 될 수 있다는 우려임 보험 불가한 자산이 사라지는 것이나 다름없게 될 수 있음
          + 어떤 전환의 결과(예: 해류 역전)인지, 대기 중 CO₂가 두 배로 오르는 결과인지 명확히 해달라고 추가 질문함
          + 이런 댓글 스레드에서는 극단적 기후 염세론이나 인간이 지구를 망친다는 식의 비관적, 냉소적 코멘트가 많으므로, 사실 기반 논의 기대는 어렵다는 점을 솔직히 밝힘
     * 다음은 AMOC(대서양 열염순환) 붕괴 차례라는 언급임
          + AMOC가 2025~2075년 사이 무너질 수 있다는 보고가 있었고, 2025년 붕괴는 가능성이 낮다고 평가된 점을 언급하며, 아직도 희망을 품고 있음
     * 이번 뉴스가 걱정스럽긴 하지만, 새로운 기법으로 얻은 데이터이고 이전의 역사적 자료가 부족하다면 혹시 원래 남대양엔 이런 변화가 있었던 건 아닐지 질문함
          + 현장 측정 등으로 과거에도 소수의 데이터가 있었지만 매우 드물었다고 추정함
     * 런던에서 서쪽으로 선을 그어보고, 뉴욕에서 동쪽으로 선을 그어 비교해 보면, 해류가 런던을 따뜻하게, 뉴욕을 시원하게 만들어 준다는 설명임 남극 북쪽은 지구에서 유일하게 바다가 육지를 막힘 없이 완전히 순환할 수 있는 곳이고, 해류 엔진 역할을 함 해양은 우리가 배출한 초과 CO₂를 대부분 흡수해 왔고, 일부는 심해에 매몰되어 왔음 하지만 바다 온도가 오르면 CO₂ 흡수량이 줄고, 미래가 더 암울해질 수 있다는 점을 강조함 이 뉴스는 해당 지역의 탄소 흡수 둔화만이 아니라, 해류 전체 시스템에 예측할 수 없는 하류 영향을 불러온다는 신호임을 강조함
          + 해류가 어떻게 뉴욕을 시원하게 하는지 의문을 가짐 오히려 뉴욕의 더위는 록키산맥에 의한 대기 순환이 더 큰 역할을 하고, 유럽의 온화함도 걸프스트림만이 아니라 다른 대기 흐름도 원인이 된다고 설명함 걸프스트림은 오히려 미국 동부를 따뜻하게 만든다는 점을 짚음 참고 링크도 추가함(https://americanscientist.org/article/…)
"
"https://news.hada.io/topic?id=21750","Substrait - 데이터 처리 연산에 대한 크로스 플랫폼 교환 표준","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Substrait - 데이터 처리 연산에 대한 크로스 플랫폼 교환 표준

     * 다양한 데이터베이스, 분석·쿼리 엔진 간에 구조적 데이터 처리 연산(관계 대수, 쿼리 계획 등)을 직렬화해 교환할 수 있도록 설계된 오픈소스 포맷 및 인터페이스
     * DuckDB, Spark, DataFusion, Sirius 등 다양한 DB·분산 처리·쿼리 엔진에서 서로 다른 시스템 간 쿼리/실행계획 교환·호환성 확보를 목적으로 채택·도입 중
     * SQL, DataFrame, Pandas 등 각기 다른 쿼리 언어나 엔진의 실행계획을 중립적/표준화된 포맷(프로토콜 버퍼, YAML, JSON 등) 으로 표현해 플러그인·서드파티 엔진 통합, 하드웨어 가속기 연동을 단순화함

Substrait란 무엇인가?

     * 데이터 분석/처리 시스템의 ""쿼리 계획(Logical/Physical Plan)""을 표준 포맷으로 정의한 오픈소스 프로젝트
          + SQL, DataFrame, LINQ 등 다양한 언어의 쿼리 구조(예: SELECT-FROM-WHERE, Join, Aggregation, Projection, Filter 등)를 표준 스키마로 직렬화/역직렬화
     * 데이터베이스 및 엔진마다 쿼리 파서·실행계획이 달라, 상호 연동·하드웨어 가속기 활용에 한계 → 공통 표준(중간 표현)으로 추상화
     * SQL, DataFrame, Pandas 등 각기 다른 쿼리 언어·프레임워크의 실행계획을 Substrait 포맷으로 변환해 공유 가능

Substrait의 주요 특징

     * 중립적 쿼리 표현: 논리적(논리 연산자)·물리적(실행 연산자) 쿼리 계획 모두 표현 가능
     * 프로토콜 버퍼 기반 직렬화(주로 .proto 스키마 활용)로, 다양한 언어 및 시스템에서 효율적 데이터 교환 가능
     * 엔진/플러그인 확장성: 새로운 함수, 연산자, 타입도 플러그인 방식으로 확장 가능
     * 하드웨어 가속 연동: GPU/FPGA/전용 가속기용 엔진과 쿼리 계획 레벨에서 손쉽게 통합 가능(Sirius, Velox, NVIDIA RAPIDS 등)
     * 플랫폼 간 연동: 데이터 레이크, 분산 처리, 로컬 분석 등 다양한 엔진이 동일 쿼리 계획을 받아들여 실행할 수 있음

Substrait가 왜 중요한가?

     * 각 엔진이 별도의 커스텀 인터페이스를 만들지 않고 Substrait만 지원하면 생태계 통합이 즉시 가능
     * 데이터 엔진·플랫폼 간 ""쿼리/분석 실행 계획의 공통 언어"" 역할
          + DB에서 쿼리 → Substrait 플랜 → GPU/분산 엔진, 별도 가속기 등으로 전달 및 실행
          + 쿼리 변환/재작성 필요 없이 ""실행 계획 레벨""에서 엔진 간 연결
     * 새로운 쿼리 엔진이 등장해도 쉽게 ""플러그인""으로 교체/조합 가능(엔진 교체 유연성)
     * SQL/파이썬/분산 쿼리 등 다양한 쿼리 언어/환경을 통합
     * 클러스터/분산 환경에서 다양한 엔진 혼합 운영 가능(이기종 환경 지원)
     * 텍스트 버전 제공으로 쿼리 계획을 쉽게 분석하거나, 시각화 도구와 연동할 수 있음

실사용 예시

     * SQL 파서와 실행 엔진 간 쿼리 플랜 교환(Calcite → Arrow, DuckDB → Sirius 등)
          + DuckDB: 내부 쿼리 플랜을 Substrait로 내보내기(export), 외부에서 Substrait 쿼리 불러오기(import) 지원
          + Sirius: DuckDB에서 생성된 Substrait 쿼리 플랜을 그대로 받아 GPU 상에서 SQL 실행 가능, 쿼리 파싱/수정 불필요
     * 데이터프레임 연산(Pandas 등)을 외부 데이터베이스에서 실행
     * 서드파티 쿼리 계획 시각화 도구(D3, Web 등) 제작
     * Velox, Spark, DataFusion 등: 각 엔진의 쿼리 플랜을 Substrait로 내보내, 다양한 분석·가속 엔진과 통합

기술적으로 어떤 구조?

     * Substrait 플랜은 주로 프로토콜 버퍼(.proto) 기반 정의
          + LogicalPlan, PhysicalPlan, Rel, Expression, Function 등으로 구성
          + Filter, Join, Aggregate, Project, Sort 등 연산자별 명확한 스키마
     * 확장 가능한 함수/타입 카탈로그 및 UDF(User-Defined Function) 확장 가능

결론

     * Substrait는 데이터베이스 및 분석 플랫폼 생태계의 ""쿼리 실행계획 표준""으로, 엔진 간 상호운용성·하드웨어 가속화·데이터 생태계 확장성의 핵심
     * 하드웨어 가속기 연동이나 쿼리 엔진 통합을 단순화하는 기반 표준으로 자리잡고 있으며, 복잡한 쿼리 파싱/변환 없이도 고성능 분석 인프라 구축이 쉬워짐
"
"https://news.hada.io/topic?id=21775","Donkey Kong Country 2와 Open Bus","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Donkey Kong Country 2와 Open Bus

     * Donkey Kong Country 2의 회전하는 배럴 버그는 ZSNES 에뮬레이터에서 발생함
     * ZSNES는 open bus 동작을 제대로 에뮬레이션하지 않아, 배럴이 영구적으로 회전하는 문제 발생함
     * 실제 하드웨어와 달리 ZSNES에서 잘못된 메모리 접근 시 0이 항상 반환되어 버그 유발함
     * 올바른 동작에서는 배럴이 정확한 방향(8방향)에서 회전을 멈추는 로직임
     * 이 문제는 코딩상의 사소한 실수(즉, 즉시 주소 지정 대신 절대 주소 지정 사용)에서 비롯된 것으로 추정됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Donkey Kong Country 2와 ZSNES 에뮬레이터의 배럴 버그

   Donkey Kong Country 2는 ZSNES라는 오래된 SNES 에뮬레이터에서 일부 스테이지의 회전하는 배럴(통)이 제대로 작동하지 않는 유명한 버그가 있음

   배럴에 들어가면, 원래는 방향키 좌/우를 누르고 있는 동안만 배럴이 회전해야 하지만, ZSNES에서는 좌/우를 짧게 눌러도 배럴이 계속 그 방향으로 영원히 회전하게 됨

   이 버그로 인해, 특히 나중 스테이지에서 가시밭이나 장애물 위에 나타나는 회전 배럴 구간이 개발자가 의도한 것보다 훨씬 어렵게 변함

   이 문제는 과거 ZSNES 포럼에서 어느 정도 문서화되어 있었으나, 현재 포럼이 사라져 관련 자료를 찾기 어려움

버그의 원인 - Open Bus Emulation

   이 버그의 근본 원인은 ZSNES가 open bus 동작을 에뮬레이션하지 않는 데 있음
     * open bus는 SNES와 같은 구형 플랫폼에서 무효 메모리 주소 읽기 시 발생하는 동작임
     * 실제 하드웨어에서는 마지막에 버스에 올린 값이 반환됨
     * SNES의 주요 CPU는 65C816(65816)임
     * 65816은 16비트 버전 6502이며, 24비트 주소 버스를 가지고, 메모리 뱅킹 방식을 사용함

   DKC2의 회전 배럴 코드에서, 유효하지 않은 주소(Bank $B3의 $2000, $2001) 접근 시, 하드웨어에서는 open bus로 0x2020 값이 반환됨

   ZSNES에서는 이 기능이 없기 때문에 항상 0이 반환되어 버그가 발생함

게임 코드의 동작 방식

   회전 배럴과 관련된 게임 루틴은 다음과 같은 동작흐름을 가짐
     * 현재 배럴의 방향과 회전량(속도)을 합산하여 임시 변수에 저장함
     * XOR 연산으로 방향의 변화를 측정하고, 그 결과를 open bus에서 읽은 값과 AND 연산함
     * 그 AND 결과가 0이면 회전을 계속, 0이 아니면 멈추고, 방향을 8방향 중 하나로 반올림하여 정렬함

   실제 하드웨어에서는 open bus 값이 0x2020이지만, 만약 0이 반환되면 회전이 무한히 계속됨

   이 로직은 원래 AND 연산이 즉시값(address #$2000) 와 해야 하는데, 실수로 절대주소(address $2000) 를 사용했다는 점이 추정됨

   하지만 하드웨어의 open bus 특성상, 실제로는 두 방식 모두 정상 동작함

해결 및 결론

   Snes9x 같은 다른 SNES 에뮬레이터는 이 버그를 하드코딩 방식으로 픽스했으며, ZSNES는 개발 중단으로 패치되지 않음

   해당 루틴에서 AND 명령어의 오퍼코드를 0x2D에서 0x29(AND #$2000)로 바꿀 경우, open bus 동작 없이도 회전 배럴이 정상 작동함

   이 문제는 실제 하드웨어나 최신 에뮬레이터에서는 발생하지 않음

   결국, 이 버그는 open bus 에뮬레이션 미지원과 코딩 실수가 만나 발생한 예시임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

추가 배경: 65816 구조와 SNES 메모리 맵

   65816 CPU는 24비트 주소 버스를 가지고 있으나, 주로 8비트 뱅크와 16비트 오프셋 조합을 사용함
     * 프로그램 카운터(PC)는 16비트, 프로그램 뱅크 레지스터(PBR, K)로 전체 주소 구성
     * 데이터 뱅크(DBR, B)는 데이터 연산용 뱅크 선택에 사용됨
     * 하드웨어 스택과 direct page는 항상 $00뱅크에 존재함

   SNES 메모리 맵도 65816을 기반으로 설계되어 주소를 8비트 뱅크+16비트 오프셋으로 생각하는 것이 더 효율적임

마무리

   이 사례는 레거시 하드웨어의 특성(open bus 등)이 에뮬레이션에서 기대하지 않은 버그로 이어질 수 있음을 보여줌

   개발자는 즉시 주소 지정을 사용해야 했지만, 우연히 절대주소도 정상 동작했던 사례임

   현대에서는 open bus 동작까지 에뮬레이션하는 것이 구형 소프트웨어의 정확한 재현에 매우 중요함을 시사함

        Hacker News 의견

     * 나는 6502 어셈블리 프로그래머로서 # 기호를 빼먹고 즉시값 대신 메모리 액세스를 해버리는 실수로 인해 수없이 많은 시간을 허비한 경험이 있어, 이런 실수는 가끔은 운 좋게 잘 동작하는 경우도 있어 더더욱 골치 아픈 현상임을 체감함. 하지만 예시의 플로팅 버스 문제보다 더 최악인 경우는 초기화되지 않은 RAM에 기대는 코드로, DRAM 마다 초기값이 다르기 때문에 본인 컴퓨터나 에뮬레이터에서는 항상 잘 돌아가지만, 다른 DRAM을 사용하는 다른 컴퓨터에서는 실패하게 됨. 보통 데모파티에서 남의 하드웨어에서 돌려야 할 때 15분도 안남겨 두고 코드가 실행 안될 때 이런 문제를 발견하게 됨
          + 6502 CPU에서 동적 메모리를 사용한 아키텍처가 실제로 있었는지 궁금증이 있음. 내 경험상 해당 플랫폼은 항상 정적 RAM만 사용한 기억
          + 6502가 내 첫 어셈블리 언어였고, ""LDA #2""는 “A 레지스터에 2란 숫자를 로드”라고 생각했음. 반면에 LDA 2는 “메모리 위치 2번 값을 로드”란 느낌이라 이 차이로 애초에 실수를 피하려고 노력함
          + 이런 상황에서는 LLM에 코드를 통과시켜 보는 것이 오히려 유용할 수 있음. LLM이 이런 영향이 큰 오타나 실수 포인트를 잘 발견해주는 강점 때문
     * Open Bus라는 단어가 대문자로 써진 걸 보고 무슨 오래된 버스 프로토콜이나 표준인 줄 착각하고 글을 읽음. 알고 보니 단순히 버스가 어떤 곳에도 연결되어 있지 않은 상태를 의미했고, 이는 주소 디코더가 지정한 주소($2000)에서 어떤 메모리 장치도 활성화되지 않았기 때문임을 이해함. 즉시 모드(#)를 빼먹고 인해 메모리에서 아무것도 읽지 못하게 된 현상을 오래된 에뮬레이터가 실제 하드웨어와 다르게 동작함으로써 발견함. 해결책으로 즉시 주소 모드로 지시어를 변경하면, 더 이상 메모리 읽기를 하지 않으므로 약 2us 정도 코드가 빨라짐. 하지만 이 정도 성능 차이는 실제 하드웨어가 아니라면 (특히 타이밍이 완전히 일치하지 않는 에뮬레이터에서는) 큰 의미가 없는 것 같음
          + (몇몇) SNES 에뮬레이터는 현재 거의 시간기반 완벽성을 달성하고 있다는 설명. 단, 2us 차이는 정말 예외적인 경우 아니면 사실상 눈에 띄는 차이를 주지 않는 현상임. 관련 기사: How SNES emulators got a few pixels from complete perfection
          + Rare처럼 출시 후 오랜 시간이 지난 후에야 새로운 아키텍처 덕분에 발견되는 버그가 묻힌 게임을 여럿 출시한 사례가 있음. Donkey Kong 64에서 8~9시간 연속 플레이 후 치명적 메모리 누수가 발생하는데, 에뮬레이터 세이브 기능 덕분에 그 시간이 단숨에 누적되어 버그가 쉽게 노출됨. 참고로 출시 때 동봉된 Memory Pak이 버그 숨기기용이었다는 설이 있지만, 최근 연구에 따르면 Rare도 Nintendo도 그 버그를 당시 인지하지 못했었음
     * SNES Puyo Puyo에서 PPU 오픈 버스 현상을 만난 경험이 있음. RetroArch에서 RunAhead 기능을 작업할 때 저장 상태가 일치하지 않는 이유를 찾는 과정이었고, PPU 오픈 버스에서 읽은 값이 상태 로딩 후 달라져 CPU 실행 트레이스 로그가 일치하지 않았던 특별한 사례
     * 6502 또는 비슷한 코드에서 나는 종종 메모리 주소와 즉시값을 헷갈리는 실수를 함. #$1234 같은 표기법이 실수 유발 요인이라 생각하며 Chuck Peddle 조차 이 문법을 깊이 후회했다는 얘기도 들었음. IDE에서 #을 빨간색으로 강조해 어느 정도 방지할 수 있었음. Rare의 개발자조차 이런 실수를 피하지 못한 사례가 있음
          + 꽤 오래 전 GNU 어셈블러에서 intel_syntax noprefix 모드로 비슷한 문제를 겪었는데, 여기에선 즉시값 이름 상수를 앞에서 참조할 때 메모리 주소나 심볼로 해석될 수 있는 문법적 모호성이 있음. 그 결과 예상과 달리 심볼의 링크 타이밍까지 대기하는 임시 메모리 주소를 만들어 버그 찾기가 정말 고통스러웠던 경험
          + ARM처럼 메모리를 다루는 별도의 지시문이 필요한 명령어셋은 이런 헷갈릴 만한 실수를 근본적으로 막아줌
     * 내가 알기로 오픈 버스 현상은 초기의 간단한 동기 버스 시스템에서만 나타남. 대부분의 다른 시스템은 존재하지 않는 주소에 접근할 때 전체 0 또는 전체 1 같은 일정한 값을 반환하며, 이는 버스 프로토콜에서 응답이 없으면 마스터가 감지할 수 있는 핸드셰이킹(PCI의 master abort)으로 처리함
     * Parallax Propeller 칩을 프로그래밍할 때 비슷한 실수를 반복적으로 경험함. JMP #address와 JMP address의 차이를 자주 헷갈리는데, 이는 6502 어셈블러의 muscle memory 때문임. Propeller의 JMP #address는 지정된 주소로 점프이고, JMP address는 주어진 주소에서 읽은 값으로 점프함. 문제는 이런 버그가 가끔은 잘 돌아가기도 해서, 동작이 멈출 때까지 이유를 찾느라 몇 시간씩 허비하게 됨
     * 오픈 버스란 데이터 버스 라인이 실제로 오픈되어 회로가 열린 상태를 의미함. CPU가 매핑되지 않았거나 쓰기 전용인 주소를 버스에 넣었을 때, 아무 하드웨어도 반응하지 않아 버스 라인이 부동 상태로 남음 — 즉 하드웨어 레벨에서 undefined behavior. 실제로 어떤 일이 벌어지는지 알려면, 데이터 버스의 물리적 구조를 살펴봐야 함. 버스는 마더보드와 카트리지 사이 신호를 전달하는 긴 도체이고, 얇은 절연기판으로 접지면에서 분리되어 있음. 이 구조는 일종의 커패시터 역할을 하므로, 최종적으로는 마지막 신호 전압을 일정 시간 동안 그대로 ‘붙들고’ 있게 됨. 그래서 오픈 버스에서는 마지막으로 전달된 값이 다시 읽히는 효과를 얻게 됨. DKC2 같은 게임은 이 오픈 버스 특성에 무심코 의존하기도 하고, NES의 컨트롤러 시리얼 포트도 저위 비트만 신호를
       주고 고위 비트는 오픈 버스가 되기 때문에 특정 게임이 LDA $4016 명령으로 $40이나 $41을 기대함. 오픈 버스 현상은 슈퍼 마리오 월드 크레딧 워프 같은 스피드런 전략(메모리 오염 혹은 임의 코드 실행)에까지 응용됨. 단, 표준이 아닌 카트리지, 풀업/풀다운 저항 사용, 또는 DMA와의 이색 상호작용(Horizontal DMA 등)이 예외적 결과를 만듬. 예를 들어 SNES의 HDMA 전송이 명령어 중간에 발생하면 오픈 버스 읽기 타이밍에 영향을 주어, Super Metroid 스피드런 exploit에서 복제하려는 메모리 블록 사이에 비정상적인 값이 들어가 exploit이 깨지는 경우도 발생함. 이에 따라 원본 하드웨어 또는 아주 정밀한 에뮬레이터를 사용할 경우 크래시가 발생하는 반면, 대부분의 에뮬레이터나 공식 재발매의 경우 이런 니치한 동작을 완벽히 구현하지 않아 전략이 정상 동작함. Super Metroid
       TAS 세계기록 완주도 이 HDMA 동작에 의존함. 적의 위치를 조작해 CPU 타이밍을 바꿔 HDMA가 오픈 버스에 원하는 값을 올려 최종적으로 컨트롤러 입력을 코드로 실행, 임의 코드 실행까지 가능케 함 Super Mario World credits warp 영상, HDMA 활용 영상, Super Metroid DMA exploit 영상, Super Metroid TAS 기록
          + Ben Eater의 6502 브레드보드 컴퓨터 영상 시리즈 덕분에 이런 하드웨어 동작이 어떻게 동작하는지 이해하는 데 큰 도움이 되었음. 상업용 기기에서 이런 버스 동작이 어떻게 확장되는지를 체감함 Ben Eater 사이트
     * 이런 흥미로운 버그 분석 콘텐츠가 좋아서, 어셈블리 코드는 60% 정도만 겨우 따라가지만 같이 곁들여진 글 설명 덕분에 이해도가 높아짐. 그리고 오랜 기간 아무도 모른 버그가 명작 소프트웨어에서 밝혀지는 이런 스토리가 특히 재미있음
          + 이 시절 시스템은 오늘날 임베디드 시스템 등에선 필수적인 (네트워크 연결 가능성 때문이든 아니든) 대부분의 체크 기능이 존재하지 않아서 더더욱 흥미로움. NES 시대에는 수많은 read/write가 단순히 라인 전압을 토글하는 것에 지나지 않았고, 무슨 일이 일어날지는 실제로 그 시점에만 알 수 있었음. CRT 블랭킹 신호와 정확하게 동기화된 타이밍으로 전압을 토글해 원하는 효과를 얻었고, 슈퍼 마리오 브라더스 3에서는 RAM 멀티플렉서를 토글해 화면 갱신 타이밍마다 스프라이트 뱅크를 바꾸는 등의 장난을 쳤음. 지역별 TV NTSC/PAL 차이에 따른 주사율이 렌더링 논리의 클럭 역할을 했기 때문에, 각각의 TV에 맞는 소프트웨어를 따로 출시해야 했던 진짜 와일드한 시대였음
     * 게임을 에뮬레이터로 플레이하다가 진행이 막히면, ‘혹시 에뮬레이터 버그인가?’라는 의심이 늘 듦. 이 이슈의 경우도 난 그냥 게임 설계가 이렇게 어렵게 만든 줄 알았을 것 같음. 그리고 게임 난이도가 정말 높을 때도 “에뮬레이터 레이턴시 때문인가?”라는 의심을 하곤 했고, 그래서 직접 mister FPGA를 만들어 사용하게 됨
          + Chrono Trigger에서 네 개 키를 동시에 입력해야 하는 구간이 있는데, USB 입력이 한 번에 세 개까지만 전달 가능해 네 번 중 한 번만 등록되는 현상 때문에 매우 어렵고 좌절을 줬던 기억이 있음
          + DKC를 ZSNES로만 플레이했었기에, 기사 읽기 전까지 이게 에뮬레이터 버그라는 걸 전혀 몰랐음. 게임 디자인이 원래 그렇게 난이도를 준 줄로만 알았고, 버그란 걸 알고 나서 정말 충격적이었음
          + Bionic Commando를 어렸을 때 많이 했는데, 에뮬레이터로 다시 하니 훨씬 어렵게 느껴졌음. 나중에 알아보니 에뮬레이터 버그로 적이 사라지지 않아 필요 생명력이 2배 늘더라. 그래도 한 번은 그 방식으로 깬 적 있지만 다시는 못 하겠음
     * DKC 1의 SGI 기반 프리렌더 3D 그래픽은 당대 첨단 기술이었음. Mega Drive의 Vector Man도 비슷한 기법을 썼지만 DKC만큼 주목 받지는 못했음
          + 1995년 DKC의 주대상 연령대(11살)였는데, 이 게임의 그래픽은 정말 충격적이었음. 출시 즈음 홍보용 비디오도 받아 본 적 있는데, 비하인드 씬 영상이 담긴 그 테이프를 여러 번 돌려 봤었음. 나는 직접 게임을 소유하지 못했지만 친구 집에서 할 기회가 있었음
          + 어릴 때 DKC 그래픽이 어쩐지 ‘가짜’라는 감각이 들었음. 당시 잡지들이 SNES가 실시간으로 3D 캐릭터를 렌더링한다는 식으로 작위적인 설명을 하곤 했는데, 사실은 플립북 애니메이션 같은 방식임을 어렴풋이 눈치챘음
"
"https://news.hada.io/topic?id=21781","Fakespot, 9년간 가짜 리뷰를 검증한 후 오늘 서비스 종료","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Fakespot, 9년간 가짜 리뷰를 검증한 후 오늘 서비스 종료

     * Fakespot은 9년간 온라인 쇼핑의 가짜 리뷰 감별 도구로 신뢰를 쌓았으나, 2025년 7월 1일부로 서비스가 종료됨
     * Mozilla가 2023년 인수 후 Firefox에 기술을 통합했으나, 지속 가능성 부족으로 인해 중단됨
     * Fakespot의 기술로 아마존 베스트셀러 상품의 43% 가 신뢰할 수 없는 리뷰임이 밝혀졌으며, 의류·주얼리 카테고리에서는 88% 가량이 신뢰하기 어려움
     * 이용자들은 Fakespot의 폐지에 실망을 드러내며, 새로운 신뢰성 검증 도구의 필요성이 더욱 커짐
     * Fakespot의 교훈을 바탕으로 TrueStar 등 후속 서비스가 AI와 효율적 구조로 등장할 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Fakespot의 시작과 성장

     * ""오늘은 한 시대의 마감""
          + 수백만 명의 소비자가 온라인 리뷰의 신뢰성 문제를 해결할 수 있도록 도왔던 Fakespot이 공식적으로 종료됨
     * 2016년 Saoud Khalifah가 아마존에서 가짜 리뷰에 속은 경험을 계기로 Fakespot을 개발
     * AI를 활용해 리뷰 언어 패턴, 비정상적 계정 등 사람이 놓칠 수 있는 신호를 종합적으로 탐지하는 방식으로 성장함
     * 아마존, 이베이, 월마트 등 주요 이커머스 플랫폼 전반에 적용됨

가짜 리뷰의 현실

     * Fakespot 분석 결과, 아마존 베스트셀러의 약 43% 리뷰가 신뢰 불가 또는 조작된 것으로 확인됨
     * 특히 의류·주얼리 카테고리에서는 88% 의 리뷰가 신뢰하기 어려움
     * 온라인 쇼핑에서 리뷰의 신뢰도 저하가 소비자 의사결정에 큰 영향을 미침

투자 유치와 Mozilla 인수

     * 2020년 11월 시리즈 A로 400만 달러 투자를 유치하며 총 700만 달러 규모의 자금을 확보함
     * 2023년 Mozilla에 인수되어 13명 팀 전체가 Firefox로 합류
     * 인수 이후 Mozilla Review Checker로 Firefox 브라우저에 통합 적용됨
     * Mozilla의 프라이버시·투명성 철학과 Fakespot의 미션이 잘 맞는다는 평가를 받음

서비스 종료와 이용자 반응

     * Mozilla는 2025년 5월 Fakespot과 Pocket등 여러 부가 서비스 종료를 발표함
     * 이유는 지속 가능한 비즈니스 모델 부재와 Firefox의 핵심 기능 및 AI 혁신에 자원 집중 필요 때문임
     * 크롬 웹스토어 등에서 Fakespot 종료를 아쉬워하는 이용자 리뷰가 급증
     * Fakespot의 사명은 여전히 유효하지만, Mozilla는 리소스를 Firefox의 신규 기능(수직 탭, 스마트 검색, AI 등)에 집중함

Fakespot의 유산과 대체 서비스

     * Fakespot은 9년간 홍보성 리뷰와 봇 농장에 맞서 소비자 신뢰 회복에 기여함
     * 서비스 종료로 신뢰성 검증 도구의 공백이 커졌으며, 대안으로 ReviewMeta, The Review Index, TrueStar 등이 언급됨
     * TrueStar는 Fakespot의 한계를 보완해 AI·지속가능 구조로 새롭게 도전할 예정임

Fakespot 종료 FAQ

     * 종료일: 2025년 7월 1일(파이어폭스의 Mozilla Review Checker는 6월 10일 종료)
     * 종료 사유: Mozilla가 지속 가능한 비즈니스 모델을 찾지 못해 리소스를 Firefox에 집중하기로 결정함
     * 대체 서비스: ReviewMeta, The Review Index, TrueStar 등 다양한 대체제가 등장 중이나 아직 정확성-경제성 균형을 갖춘 지속 가능한 솔루션은 개발 단계

마지막 인사

     * Fakespot의 서버가 종료됨에 따라, 10년 가까이 온라인 쇼핑의 신뢰를 높여준 기술적 선구자 역할에 감사함
     * Saoud Khalifah와 팀이 이익보다 진실을 우선시한 기술로 보여준 가능성은 업계에 큰 의미를 남김

        Hacker News 의견

     * Fakespot이 점점 경쟁에서 밀려나기 시작한 경험 공유 의견 제시 모든 감시 도구를 우회하기 위한 상점들의 창의적 행동 경험 설명 5~20달러 쿠폰이나 기프트 카드로 긍정 리뷰 유도 사기 리스트, LLM을 이용한 자동 리뷰 생성 증가 경험 언급 Dyson 카피 진공청소기 구매 후 “새 모델 구매, 리뷰 남기면 환불” 제안 받고 실제로 여러 번 반복해서 진공청소기 12대 획득한 일화 고백 약간의 죄책감도 있지만 청소기로 양심까지 깨끗하게 만든다는 농담 Fakespot이 이 세 가지 시나리오에 모두 취약했을 것이라는 생각
          + 한 회사에서 긍정 리뷰로 변경하면 100달러를 지급한다고 해서 실제로 PayPal로 돈 받고 리뷰를 수정 후, “회사에서 돈 주고 리뷰하라고 했다”라고 남겼더니 Amazon이 그 리뷰는 고객에 해롭다며 삭제 처리
          + 첫 번째로 “무료 업그레이드”를 받는 건 이해하지만, 왜 그 이후로 계속 진공청소기만 모으는지 궁금 앞으로 판매할 계획인지 질문
          + 12대의 청소기 에피소드에 웃음 표시 왜 다 모아두는지, 친구나 이웃에게 나눠주거나 팔지 않는 이유 궁금증 표시
          + 창고에 진짜 진공청소기 12대가 있다는 게 믿기 어렵다며 실제 증거 공유 가능 여부 질문
          + Amazon에서 진공청소기를 새로 구매한 후에도 계속 진공청소기 광고를 노출받는 현상에 의문 제기 아마도 진공청소기 수집가로 인식되었기 때문일 수 있다는 유쾌한 분석
     * Mozilla가 Fakespot을 인수했지만, 실제로 어떤 비즈니스 모델을 구상했는지, 사용자 입장에서는 크게 와닿지 않았던 경험 설명 Firefox를 자주 쓰고 있지만 보조 UI도 본 적 없고, 나에게서 어떤 수익모델을 만들지 알 수 없음 user data, 커미션, 구독 같은 모델 궁금증 솔직히 Mozilla의 결정이 이해되지 않음
          + Mozilla의 이사회에 Amazon, Google 등 주요 기업에 투자하거나 이해관계가 있는 인물이 영향을 주고 있을 가능성을 의심
          + Mozilla는 비영리단체라서 반드시 뚜렷한 수익 모델이 필요한 건 아니었을 수도 있다는 의견 인터넷을 더 안전하게 만드는 것이 목표였으나 Google의 반독점 소송 위협으로 인해 수익이 안나는 외부 프로젝트는 적극적으로 정리해야 했을 수 있음
          + 굳이 왜 인수했는지 이해 불가 이번 스레드의 인기 댓글이 모든 근거를 보여주는 듯
          + 쿨한 도구를 샀지만 실제로 어디에도 잘 연동시키지 못해서 조용히 서비스 종료한 모습으로 보임
          + 추천 링크 삽입만으로도 꽤 수익이 발생할 수 있었겠지만, 소비자 입장에서는 거부감이 클 것이라고 생각 다른 대체재가 나올 것이고 아마도 이번엔 추천 링크를 적극적으로 쓸 거란 예측
     * 대안 서비스들을 찾아봤으나 만족스럽지 못한 생태계 현황 공유 ReviewMeta.com은 제대로 작동하지 않을 때가 많고 더 이상 유지보수되지 않는 듯 TheReviewIndex.com은 모든 상품을 다루지 않고, 검사 거부하는 사례도 있음 가격 정보는 비공개, 엔터프라이즈 모델은 리뷰 체크와도 관계 적음 SearchBestSellers.com은 개별 상품 체크는 못하지만 각 카테고리 인기 제품 확인 가능 Camelcamelcamel.com은 가격 추적 및 이력 확인, 가격 하락 알림 제공 AlternativeTo의 Fakespot 대안 목록도 새로 추가된 것들이 있음 Reddit에서는 ChatGPT 등 LLM을 활용한 상품 추천 질문이 대안으로 거론되지만 Fakespot 같은 느낌은 아님 관련 Reddit 토론 본인이 사용하는 좋은 대안, LLM에서 써먹는 프롬프트가 있다면 공유 요청
          + Amazon에서 이미 알고 있는 메이저 브랜드 상품만 구매, 만약 망해도 버릴 수 있다고 생각하는 상품만 도전 나쁜 리뷰 위주로 본인만의 기준 세움 중국 셀러의 이상한 브랜드는 리뷰 보고 판단하는 게 시간 낭비라는 생각
          + 현재 Fakespot의 정신적 후속작을 직접 만들고 있다고 밝힘 LLM 분석, 기존 ML, 룰 기반 휴리스틱을 결합해 거짓 리뷰 탐지 목표 ""넉넉한 무료체험 기반 구독모델""로 수익화 전략 알림 신청용 출시 준비 홈페이지 안내
     * Fakespot은 의심스러운 상품 리뷰를 표시해주는 브라우저 확장이었고, 두 해 전 Mozilla가 인수해서 Firefox에 ""Review Checker""라는 이름으로 통합한 사실 설명 오늘 서비스 종료 발표 지속 가능한 대체재가 꼭 필요하다는 의견 제시
          + 최근 Fakespot 종료와 동시에 Amazon과 연관된 Mozilla의 다른 거래나, 예를 들어 DeepSpeech도 NVIDIA의 150만 달러 지원 때 종료된 사례가 있어 이런 흐름에 주목
     * 본인은 Amazon KDP Direct 플랫폼에서 책 판매 경험자가 본인 책 두 버전 중 하나는 Fakespot이 진짜 리뷰를 가짜로 오탐지했다고 주장 두 버전 모두 동일 사용자가 리뷰를 남겼는데 한 쪽만 오탐, 일부는 문구만 다를 뿐 여러 번 반복 확인해도 여전 Fakespot이 과도한 false positive로 명성도 안 좋았다는 본인 조사 결과 Fakespot 서비스를 긍정적으로 보지 않음
          + 둔감한 도구로는 때때로 선의의 사용자까지 해친다는 점이 쉽게 해결되지 않는 문제라는 공감
          + 본인도 Amazon 상품 페이지를 관리했는데, 결코 거짓 리뷰가 없는 제품임에도 Fakespot이 반복적으로 오탐지 했던 경험 공유
          + 이런 false positive가 왜 발생했을지 추측이 있다면 궁금
     * 판매자가 싫어하는 리뷰가 삭제되는 현실에서는 Amazon의 별점과 리뷰는 신뢰할 수 없다는 본인 생각 실제로 모든 게 거짓말처럼 느껴짐
          + 최근에는 “이 달에 몇 개 팔렸는가” 같은 수치만 본다는 전략 만약 한 제품이 수천 개 팔리고 매달 500개 이상 구매가 지속된다면 괜찮은 제품일 가능성 제품이 망해도 Amazon은 대부분 30일 환불 정책이 있으니 환불 후 다른 제품 시도도 부담 없음
          + 이제는 3점짜리 리뷰 중 합리적 불만을 우선적으로 참고하는 게 디폴트가 됨
          + Amazon 상품 페이지에서 실제 판매 상품과 리뷰가 계속 바뀌는 ‘페이지 참조 셔플’ 문제를 문제로 지적 특정 페이지의 리뷰가 상품과 무관하게 따라다니고, 재고 혼합 정책 때문에 제조사 공식 상품이라도 FBA 셀러의 가짜 제품이 함께 발송될 수 있다고 설명 구매자, Amazon, 셀러 모두 실제로 어떤 물건을 받게 될지 모르는 혼란한 구조 자조 섞인 분석
     * 본인과 파트너 모두 리뷰를 더는 신뢰하지 않는 현상 공유, 블로그 포스트 링크 리뷰에서 별점 자체는 무의미하다고 판단, 오히려 1점 리뷰에서만 중요한 정보가 있다고 생각 물론 모든 1점을 믿는 게 아니라 스스로 검증이 필요하지만 유의미한 데이터는 1점에 다 들어있다고 강조
          + 1점 리뷰도 포크로 숟가락 기능을 기대하는 이상한 불평이나, 경쟁업체의 ‘블랙 PR’도 많음 전체적으로 아예 리뷰 시스템 자체가 망가졌다는 생각
     * Amazon은 이제 거의 Aliexpress, 때론 Temu와 다를 바 없고 단지 가격에 약간의 마진이 붙은 정도라는 인식
     * 페르시아 속담을 인용하며, 지금의 리뷰는 판매자가 소비자에게 보여주고 싶은 것에 불과하다는 의견 전통 언론사 등 제품을 직접 판매하지 않는 곳의 리뷰만이 신뢰할 수 있다고 생각
     * Fakespot, 온라인 리뷰 관련 Hacker News 토론 모음 관련 토론1, 관련 토론2
"
"https://news.hada.io/topic?id=21787","Sirius -  GPU-Native SQL 엔진","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Sirius - GPU-Native SQL 엔진

     * 최신 GPU 하드웨어를 활용해 SQL 쿼리 가속화에 특화된 차세대 엔진
     * Substrait 쿼리 포맷을 통해 CPU 엔진(DuckDB)에서 생성된 쿼리를 GPU 환경(Sirius)으로 전달
     * GPU 메모리 내에 Raw Data Cache Region과 Processing Region을 분리, 데이터 적재 및 처리 성능 극대화
     * 쿼리 수정이나 시스템 변경 없이 TPC-H 100 규모에서 동일 비용 기준 10배 이상 속도를 기록
     * 실시간 분석, 금융, ETL 등 대규모 데이터 워크로드에 최적
     * 지원 환경 : Ubuntu 20.04+, NVIDIA Volta(7.0+) 이상 GPU, CUDA 11.2+, CMake 3.30.4 이상 (16 vCPU 이상 권장)
     * 설치 방식 : AWS AMI(미리 구성된 이미지), Docker 이미지, 수동 의존성 설치 3가지 옵션 제공
          + Docker: sudo docker run --gpus all -it yifeiyang7/sirius_dependencies:latest bash
          + 수동: DuckDB 의존성, CUDA, libcudf(conda로 설치) 등 개별 설치 필요
     * 주요 제한사항
          + GPU 메모리 크기 내에서만 동작 (추후 파티셔닝/배치·디스크 스필·멀티 GPU 지원 예정)
          + 최대 row count는 libcudf의 int32_t 한계(~20억 rows) 에 제한
          + 데이터 타입: INTEGER, BIGINT, FLOAT, DOUBLE, VARCHAR, DATE, DECIMAL 지원, 추가 타입 개발 중
          + 연산자: FILTER, PROJECTION, JOIN, GROUP-BY, ORDER-BY, AGGREGATION, TOP-N, LIMIT, CTE 지원, WINDOW 함수 등은 개발 중
          + Partially NULL Column 미지원(향후 패치 예정)
          + 이슈 발생 시 DuckDB CPU 엔진으로 자동 폴백

   Substrait - 데이터 처리 연산에 대한 크로스 플랫폼 교환 표준
"
"https://news.hada.io/topic?id=21807","tmux-rs 소개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               tmux-rs 소개

     * tmux-rs 프로젝트는 약 6개월간 C로 작성된 tmux 전체 코드를 Rust로 포팅한 작업임
     * C2Rust 툴을 사용해 초기에 자동 변환을 시도했지만, 결과물의 유지보수성이 낮아져 결국 수작업으로 변환을 진행함
     * 빌드 과정과 Rust-C 상호 연동에서 여러 버그와 구조적 문제를 경험하였음
     * goto 문, 매크로 데이터구조, yacc 파서 등 C 고유 패턴을 Rust로 옮기는 특별한 이슈와 해법이 있었음
     * 프로젝트는 아직 unsafe Rust 기반이지만, 사전 컴파일·런 실행을 통해 Rust로의 완전 이식을 목표로 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * tmux-rs는 tmux의 전체 코드베이스(약 67,000줄의 C 코드) 를 Rust(약 81,000줄, 주석·빈 줄 제외) 로 포팅한 프로젝트임
     * 개발자는 이 작업을 취미 프로젝트로 진행했으며, C에서 Rust로의 이식 과정에서 많은 시행착오와 학습을 경험함

C2Rust 사용과 한계

     * 본래 C2Rust라는 자동 변환 툴을 사용하여 tmux C 코드를 Rust로 이식하려고 했음
     * 자동 변환 코드는 가독성이 떨어지고, 원본 C 코드 대비 3배 이상 커졌으며, 각종 불필요한 형 변환, 상수명 손실 등으로 인해 유지보수성이 크게 저하됨
     * 수작업 리팩토링 과정에서 결국 C2Rust 결과물을 폐기하고, C 코드를 참조해 직접 Rust로 옮기는 방식으로 전환함
     * C2Rust를 사용해 초기 단계에서도 빌드·실행이 가능했던 점은 프로젝트의 타당성과 실현 가능성을 확인하는데 큰 도움이었음

빌드 프로세스 설계

     * tmux는 autotools 빌드 시스템을 기반으로 하며, Rust 코드와 기존 C 코드를 정적 라이브러리로 연동
     * 초기에는 Rust 라이브러리를 C 바이너리에 링크하는 식이었으나, 코드가 대부분 Rust로 이식된 이후부터는 Rust 바이너리에서 C 라이브러리를 링크하는 구조로 변경함(cc crate 사용)
     * 빌드 자동화를 위해 build.sh 스크립트와 build.rs 스크립트를 작성해 번역 중에도 점진적으로 빌드 검사 가능하게 설계함
     * 헤더 선언 누락, 함수 시그니처 불일치 등 빌드 과정에서 자주 발생하는 문제를 함수 단위로 단계적으로 해결함

번역 중 겪은 버그 사례

  버그 1: 암시적 선언과 포인터 반환

     * Rust로 변환한 함수에서 포인터 반환형이 C 코드에 암시적 선언된 탓에 return값의 상위 4바이트가 잘려서 잘못 전달되는 문제가 발생함
     * 해결책은 C 쪽에 정확한 함수 프로토타입을 추가해 컴파일러가 올바른 동작을 하게 하는 것임

  버그 2: 구조체 필드 타입 불일치

     * client 구조체에서 필드 타입(포인터 vs 정수) 오역으로 인해, 메모리 오프셋 계산이 어긋나 데이터 해석 오류와 segfault가 발생함
     * 정확한 구조체 정의를 C와 Rust에서 일치시키는 것으로 해결함

C 고유 패턴을 Rust로 이식

  Raw pointer 활용

     * C 포인터를 Rust 레퍼런스로 직접 매핑하는 것은 null 허용성, 초기화 보장 등 Rust의 안전성 규칙에 위배될 수 있음
     * 따라서, 대부분의 포인터 구조는 raw pointer (*mut, *const) 로 옮겨, 안전하지 않은 영역에서만 사용함

  goto 문 처리

     * C2Rust에서는 goto의 흐름 제어를 알고리듬으로 변환하지만, 대부분의 경우 Rust의 labeled block + break, labeled loop + continue로 충분히 구현 가능함

  매크로 데이터구조 이식

     * tmux는 침입형 red-black 트리, 링크드 리스트를 C 매크로로 구현함
     * Rust에서는 Generic trait과 커스텀 iterator를 사용해 유사한 인터페이스를 구현함(단일 trait의 중복 구현 문제는 dummy 타입으로 해결)

  yacc 파서 변환

     * tmux는 설정 파일 파서를 위해 yacc(lex) 를 사용
     * Rust에서는 구조가 유사한 lalrpop crate를 사용해 문법과 액션을 그대로 포팅하고, C lexer와의 연동용 어댑터도 작성
     * 이 과정에서 lalrpop의 raw pointer 지원 한계(NonNull<T> 활용) 등도 경험함

개발 환경 및 도구

     * 주로 neovim에 커스텀 매크로를 활용하여 반복적 변환 작업을 수행
     * 예: ptr == NULL → ptr.is_null() / ptr->field → (*ptr).field 등 손수 매핑
     * 자동화 도구(Cursor)도 시도했으나, 손실되거나 잘못된 코드가 많아 코드 리뷰 부담이 커짐
     * 손가락 피로를 줄이는 데 일부 도움됐으나, 생산성 측면에선 제한적임

결론 및 향후 방향

     * 전체 코드는 완전히 Rust로 이식 완료/버전 0.0.1 공개
     * C2Rust 대비 수작업 코드가 일부 더 나은 수준이나, 여전히 unsafe Rust 기반, 다수의 버그가 존재
     * 최종 목표는 safe Rust 코드로의 전환 및 tmux 기능의 완전한 Rust 이식 완성
     * Rust, tmux에 관심 있는 개발자와의 협업 및 피드백을 GitHub Discussions를 통해 희망함

   오.. 근데 Rust가 더 가벼운가요?

   오... 좋은데요?
   tmux 플러그인 중에서 resurect가 은근히 메모리 많이 먹고 이상허게 동작하는 것도 있어서 빼놓았는데, tmux-rs와 함께라면 더 나을지 궁금허네요.

   https://rosettalens.com/s/ko/tmux-rs-intro

        Hacker News 의견

     * 정말 대단한 프로젝트 기록물 읽기 경험이었음에 감동 느낌 전달하고 싶음. 저자의 꾸준함과 집요함에 큰 존경심 표시하고 싶음. ""정원 가꾸기와 비슷하지만 segfault가 더 많음""이라는 표현이 깊이 공감됨. 이런 진지한 취미 프로젝트에서 가장 많이 배움이 생기는 경험임.
       c2rust 관련 경험이 특히 흥미로웠음. 예전에도 언어 간 자동 코드 변환기가 가져온 비슷한 변화를 본 적이 있음. 이런 툴로 빠르게 프로젝트를 초기화하고 실현 가능성 증명하는 데는 아주 유용하지만, 결국 타깃 언어답지 않은, 비어있는 느낌의 코드가 나오기 십상임. 결국 고통스럽더라도 수동 포팅으로 전환한 선택이 정말 옳았다고 생각함. 자동으로는 C 코드의 의도를 안전하고 러스트스러운 코드로 번역하는 데 한계가 있음.
       ""재미있는 버그"" 섹션에서 #2번 struct layout mismatch 얘기를 보고 과거 외부 함수 인터페이스(FFI) 악몽이 떠올랐음. 나도 한 번 C++과 C# 사이에서 struct 패킹이 틀려서 일주일을 허비하며 미묘한 데이터 손상을 잡아낸 적 있음. 의미상으로 미치는 버그라 진짜 제정신을 의심하게 만듦. 이런 걸 찾아내려면 대단한 디버깅 인내심이 필요함. 저자에게 박수를 보냄.
       전체적으로 이 프로젝트는 핵심 인프라 코드를 현대화하는 현실적인 난이도와 과정을 잘 보여주는 케이스라 생각함. 다음 큰 목표가 unsafe에서 safe Rust로 넘어가는 거라는데, 어떤 전략일지 정말 궁금함.
       raw pointer, goto 등 복잡한 제어 흐름을 모두 idiomatic하고 안전한 Rust로 고치되 코드 전체가 무너지지 않게 하는 게 사실상 처음 포팅보다 더 어려울 수 있다고 생각함. 라이프타임과 borrow checker를 모듈 단위로 점진적 도입할 계획인지, intrusive 자료구조를 어떻게 처리할 계획인지 궁금함. 표준 라이브러리의 BTreeMap 같은 걸로 대체하면 성능 영향이 있을 수 있다고 보는데, 원래 intrusive 설계가 그걸 의도한 것 아닐지 생각됨.
       아무튼 놀라운 작업임. 이렇게 상세하게 과정 공유해줘서 고마움. 깃허브에서 프로젝트 계속 팔로우할 예정임
     * 이번 새 소식이 나를 끌어당기는 느낌임
       몇 년 전부터 Rust 기반 tmux 세션 관리자 rmuxinator(tmuxinator 클론 느낌)를 직접 개발 중임. 대부분 잘 동작하지만, 삶이 바빠서 진행이 더뎠었고 요즘 버그 수정 위주로 다시 잡고 있음. 최근 추가한 기능은 rmuxinator를 라이브러리로도 쓸 수 있게 만든 것임. tmux-rs를 포크해서 rmuxinator를 의존성으로 넣은 뒤 per-project 설정 파일로 세션을 시작하는 방법이 실제로 먹힐지 테스트해보고 싶음. rmuxinator의 upstream 포함을 주장하는 건 아니지만, 이런 세션 템플릿 기능이 terminal multiplexer 자체에 내장되어 있었다면 정말 유용했을 거라는 생각임
       역으로 rmuxinator가 tmux-rs를 라이브러리로 사용해서 세션 관리 전부를 쉘 커맨드 생성 없이 해결하면 더 좋지 않을까란 생각도 해봄(물론 tmux-rs가 그걸 지원하는지는 아직 모름)
       지금 진행 중인 버그 픽스 마치면, 위 아이디어 중 하나라도 꼭 시도해볼 예정임
       어쨌든 richardscollin 멋진 작업 인정함
     * ""tmux를 왜 Rust로 다시 썼냐는 질문에 딱히 좋은 이유는 없고, 그냥 취미 프로젝트다. 정원 가꾸기 같지만 segfault가 더 많다."" 이런 태도가 너무 좋음
       새로운 것을 만들 때 꼭 대의명분이나 실용성만 있어야 하는 법은 없음. 취미 프로젝트에서 의외의 발견이 이루어질 수도 있는 것임. 저자의 상세한 글에 감탄
       참고로 내 정원에는 segfault가 넘침. 새로운 프로젝트 코딩이 내 마당엔 더 안전한 느낌임
          + 모두 공감함. 모든 프로젝트가 세상을 바꾸기 위해 존재할 필요는 없음
            최근 fzf를 Rust로 다시 구현한 적이 있음 rs-fzf-clone
            특별한 이유가 있었던 건 아니고, 기존 fzf도 아주 잘 동작했고, 주요 목적은 러스트의 채널과 퍼지 검색 알고리즘을 직접 경험하고 싶었기 때문임. 정말 재미있는 학습 과정이었고, 원래 fzf가 더 뛰어나지만 그게 꼭 중요한 건 아니었음. 새로운 것을 시도하고 실험하는 것 자체가 목적이었음
          + ""정원 가꾸기는 철학자가 되기에 가장 좋은 핑계""
               o 레이 브래드버리, 민들레 와인
          + 누군가 Rust가 C보다 무조건 더 우월하다는 뉘앙스를 보이면 반사적으로 냉소적인 반응이 먼저 나오는 성향임. 하지만 사람들이 재미 삼아 이런 프로젝트를 한다는 사실을 자꾸 잊곤 함
          + ""우리는 반드시 새로운 것을 만들기 위해서만 이유가 필요한 것은 아니다""란 말이 인상적임
            그런데 tmux는 사실 새로운 건 아님
            기존 소프트웨어를 다른 언어로 다시 쓰는 데에도 이유가 있어야 하는지 생각해보게 됨
          + ""정원 가꾸기 같지만 segfault가 더 많다""란 말이 재밌음. Rust에는 아직 익숙하지 않은데, 구체적으로 어떤 상황에서 unsafe가 필요한지 궁금함
     * 이 프로젝트의 태도와, 대부분의 댓글이 긍정적인 분위기라 매우 인상적임
       성숙한 애플리케이션을 다른 언어로 다시 쓴다면 항상 좋지 않다는 평이 있지만, 실제로 시도하면서 많은 배움이 따라오는 것임. 결과보다 과정이 진짜 중요함
       여기 받은 관심과 AI 발전 추세를 감안하면, Rust 입문자에게 아주 매력적인 취미 프로젝트로 발전할 수 있을 것 같음. 쉬운 버그부터 고치고, 새로운 기능 추가하거나 최적화하는 게 큰 경험임
       하나 아이디어로, Gemini CLI(혹은 좋아하는 LLM)를 scratch buffer처럼 만들어 tmux 세션의 다양한 창/패널들과 상호작용하게 하는 기능 제안하고 싶음
       내 경우, 여러 서버에 동기화된 패널로 명령을 실행하다 수동으로 실패 등을 관리하는데, AI에게 명령 실행을 맡기고, 실시간으로 출력을 분석해 적응적으로 커맨드를 재생성하는 기능이 있으면 마치 동적으로 생성되는 맞춤형 셸 스크립트 느낌일 것 같음
          + 취미 프로젝트로 무엇을, 어떤 방식으로 하든 다 존중함. 그런데 왜 기존 소프트웨어를 그냥 한 언어에서 다른 언어로 그대로 포팅하는 데서 흥미를 느끼는지 이해가 안 감
            예를 들어 매일 gvim을 쓰지만, 에디터를 만들고 싶으면 꼭 gvim처럼 만들 필요 없이 내가 원하는 기능만 가진 새로운 것을 창의적으로 만들고 싶다는 쪽임. 이만큼 시간투자 한다면 창의적으로 독특한 걸 시도하는 것이 더 의미 있다고 생각함
     * 방금 tmux를 Fil-C로 1시간 이내에 포팅해봄(libevent 포팅과 테스트 통과 포함). 아주 잘 동작하고 완전한 메모리 안전성 확보 경험임
     * 이런 프로젝트가 마음에 듦. 나도 rust에 빠져보고 싶은 생각 있음
       참고로 zellij(러스트 기반 터미널 멀티플렉서)를 소개하고 싶음
       나는 사용자일 뿐이며, rust 기반 솔루션을 계속 찾아보고, 옮겨가는 걸 즐김
     * 마침 이 영상 ""Oxidise Your Command Line""을 방금 보고 있었음
       https://www.youtube.com/watch?v=rWMQ-g2QDsI
       일부는 Rust 개발자가 아니면 필요 없을 수 있지만, 커맨드라인 환경에 익숙한 누구에게나 꽤 유용한 팁도 있음
     * c2rust에서 상수 이름 유지 등, 저자가 지적한 정보 손실을 줄여줄 수 있는 개선이 충분히 가능하지 않을까 생각함. 첫 변환의 부담이 크니까
          + 정말 C2Rust에는 이 기능이 꼭 필요하다고 봄. 내가 이해하기로 이 툴의 핵심 목적은 나중에 idiomatic Rust로 포팅할 베이스 코드를 생성해주는 것임. 그렇지만 상수 정의 같은 게 다 날아가 버리면 생산성 손실이 심각함
     * 대형 언어모델로 C 코드 전체를 Safe Rust로 한 시간만에, 그리고 정확하게 자동 변환해주는 시대가 오면 이런 프로젝트가 아주 미래지향적 대표 사례 될 것 같음
       다만, 저자도 마지막 단계에서 Cursor로 시도해봤지만(2025년 중순 기준), 변환 효율이 현저히 떨어졌다고 해서 현실적 성능은 아직 멀었다고 봄
          + codemod.com 등에서 이걸 이미 ""codemods""라는 개념으로 시도하는 중임
            codemods는 AST(추상 구문 트리)를 이용해 빠르고 대량의 코드 변환 및 리팩터링을 가능하게 함
            codemods API refactoring 소개
          + ""대형 언어 모델로 복잡한 C코드를 1시간만에 Safe Rust로 완벽하게 변환할 수 있다""는 부분이 구체적이라서 인상 깊게 느껴짐
     * 앞으로 코드가 더 깔끔해지길 기대함. zellij를 여러 번 시도해봤는데, 여러 해의 개발에도 tmux에서 편하게 제공하는 기능 몇 가지가 아직 부족함
       특히 status bar 숨김/표시 불가가 가장 불편함
       zellij-org/zellij 이슈 #694 참고
          + 세션 매니저 플러그인에 단축키를 매핑할 수 없어서 내게는 쓸 수 없음
            내가 빈번히 사용하는 키 바인딩이 세션 매니저 플러그인 기본 바인딩이랑 겹쳐서, 디렉토리 선택 등 주요 기능이 막혀버림
            결국 세션 생성도 플러그인 대신 커맨드라인에서 직접 해야 하는 구조임
"
"https://news.hada.io/topic?id=21794","ICEBlock, 정부 비난 이후 App Store 차트 1위에 등극","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ICEBlock, 정부 비난 이후 App Store 차트 1위에 등극

     * 미국 정부 관계자가 ICEBlock 앱과 CNN 보도를 비난한 이후, 앱의 인지도가 확산되며 다운로드 수 급증 현상
     * ICEBlock은 사용자가 지도에 ICE 요원 목격 위치를 핀으로 표시하고 주변 5마일 내 사용자들에게 알림을 제공하는 기능 제공
     * 백악관 및 ICE 고위 관계자들이 해당 앱과 보도가 ICE 요원의 안전을 위협한다는 비판 발표
     * 미국 정부는 개발자 Joshua Aaron에 대해 조치 검토 의사 밝히며, 법적 대응 가능성 시사
     * 개발자는 ICEBlock이 개인 정보 수집 없이 작동하는 점을 강조, TechCrunch에서 익명성 확인
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ICEBlock 앱과 논란의 배경

     * ICEBlock 앱은 미국 내 이민 단속 강화 움직임 속에서 ICE 요원이 목격된 위치를 익명으로 공유할 수 있는 소셜 네트워킹 도구로 개발됨
     * 2025년 4월 론칭 이후 LA 등지에서 이미 2만 명 이상의 사용자가 가입했으며, 최근 정부 공식 비난과 언론 보도 이후 App Store 무료 소셜 네트워킹 부문 1위, 전체 무료앱 3위 기록

주요 기능

     * 사용자는 지도에 직접 핀을 꽂아 ICE 요원 위치 제보 가능
     * 신고 시 요원 복장, 차량 정보 등 추가 메모 입력 가능
     * 5마일 반경 내 사용자들에게 알림 기능 제공, 빠른 정보 확산 및 지역 경계 도움

미국 정부 및 관료의 반응

     * 백악관 공보실 Karoline Leavitt는 해당 앱 및 CNN 보도가 ICE 요원에 대한 폭력을 조장한다는 입장 표명
          + 최근 ICE 요원 공격이 500% 증가했다는 통계 인용
     * ICE 국장 Todd M. Lyons는 공식성명을 통해
          + 앱이 연방 집행 요원을 잠재적 타겟으로 만든다고 주장
          + CNN이 ""고의적으로 요원의 생명을 위협한다""며 언론의 무책임 또는 과격한 행동을 비판

법률적 조치 및 정부 대응

     * Homeland Security 장관 Kristi Noem 및 법무장관 Pam Bondi 등 미국 정부 고위 관계자가
          + “개발자 Aaron에 대한 조사 및 법적 행동을 검토 중”이라고 발표
          + “위치 정보 공유가 범죄자에게 연방 요원 움직임을 제공해 전체 치안에 위협이 된다”는 논리 전개
          + “이는 표현의 자유가 아니라 법집행자 생명 위협 행위”라 언급

앱의 익명성과 개인정보 보호

     * 개발자 Joshua Aaron은 ICEBlock이 기기 식별자, IP 주소 등의 개인정보를 수집하지 않음을 강조
     * TechCrunch의 실제 테스트로 실제 익명성이 검증됨
     * 현재 iOS에서만 이용 가능하며, Android에서는 보안 우려로 서비스 미제공
          + 안드로이드 버전 출시 시 개인정보 수집 우려 발생 가능성 설명

결론

     * ICEBlock은 미국 내 이민 단속 논쟁과 맞물려 기술, 사회, 법적 이슈의 중심으로 부상 중임
     * 정부 비난 및 주요 언론 보도로 오히려 폭발적 사용자 증가 효과로 이어짐
     * 개발자 익명성 보장과 법적 경계선 논의가 앞으로의 논점이 될 전망

        Hacker News 의견

     * 경찰 활동이 진행 중인 지역을 피하고 싶어하는 게 뭐가 그리 나쁜 일인지 모르겠는 의문 제기, 잘못을 하고 있는지 여부가 아니라 DUI 체크포인트에서 귀찮게 굴거나, 경찰차 여럿이 한 차선을 점령한 탓에 교통체증이 발생하는 상황을 마주치고 싶지 않다는 심정 언급, 합법적으로 살아가는 미국 시민임에도 공항이 아닌 곳에서 신분증을 요구받고 심문당할 가능성만으로도 거기를 피하고 싶어지는 마음 공유
          + 네비게이션 앱들은 예전부터 경찰 활동과 각종 교통 상황을 알려주는 역할 수행, 경찰과 엮여서 좋은 일 생기는 경우 거의 없으니 최대한 피하는 게 상식이라고 생각
          + 미국 시민을 납치한 적도 있는 집단이니 피하고 싶어하는 게 합리적이라고 판단
          + 경찰, 특히 ICE와 엮이는 게 두렵진 않지만 그런 상황이 절대 유쾌하거나 편해진 적 없었던 경험 공유, 나한테서 뭔가 숨길 이유가 있어서라기보단 단순히 행정적 번거로움이라고 인식, 미국 출생 다세대 시민이 ICE에 공식적으로 얼마나 협조해야 할 의무가 있는지 궁금
          + 어릴 때 '미국은 자유의 땅'이라 믿었지만, 영국에선 경찰이 신분을 물어본 적이 거의 없었고, 확인조차 하지 않아 신분증이 없어도 전혀 문제없었음
     * CNN이 이 앱을 다루는 기사만 냈다는 이유로까지 공격하는 모습 황당하게 여김, 경찰 동향을 알리는 것도 문제 삼고, 기사로 논의하는 것조차 문제 삼는 과도함 지적, 기사 조회수가 많아지면 그것도 생명을 위협하는 행위라고 할 셈인지 의문, 인기 포럼 스레드가 논의해도 똑같이 문제 삼을 것인지 궁금
          + ""경찰관이 매일 목숨을 건다""는 말, 실상을 알고 보면 감명 덜하다고 지적, 경찰의 치명상 발생률은 조경 감독이나 크레인 운전자와 비슷하며, 쓰레기 수거원보다 절반이하, 벌목 노동자에 비하면 6분의 1에 불과, 위험이 아예 없는 건 아니지만 비슷한 위험 직종엔 그런 대우 없다고 언급
          + 해외에서 ""우리 군인이 공격당했다""며 지원 필요성을 호소하는 논조와 비슷함을 떠올림, 언론이 실제 이유를 잘 다루지 않고 주로 ""미국 방어"" 프레임만 강조하는 현실, 예로 니제르나 이라크 관련 링크 첨부
     * 안타까운 법조인 출신 당국자의 발언 인용하며, 이런 앱이나 Waze, 경광등 알림, 하이빔 깜빡이 등은 모두 표현의 자유로 법원에서 여러 번 인정받아온 현상임을 지적
          + 최근 대법원이 기존 판례를 쉽게 무시하며 정부 측에 자주 동조하는 모습에 불안감 표출
          + 행정부가 법원을 져도 별다른 불이익 없이 시도만 해도 된다는 인식
          + 'head taps'라는 용어의 의미 궁금
          + 차이점이 있다면 Waze 등 앱들은 특정 운전자 몇 명이 아니라 모두에게 대규모 정보를 공유한다는 점
     * 정부 요직 인사가 평범한 개인 개발자를 이름까지 언급하며 공격하는 건 부적절하다고 판단
          + 이런 행동은 권위주의적 작태라고 간주
          + 이름 언급 자체는 흔한 일이지만, 본질적으로 개인을 왜 공격하냐가 더 문제라고 판단
     * 권력 집단인 법 집행자는 일반 시민보다 훨씬 더 높은 기준과 책임, 투명성이 요구된다고 주장, 경찰 무선 통신은 암호화하지 말고, 얼굴 인식 가능한 공공 데이터와 위치추적까지 포함한 공개 기록 필요, 시민 감시 방식과 똑같이 경찰에게도 적용해야 함, ICE 직원 얼굴 인식 사이트 소개
          + ""법 집행 무전 공개""에는 동의하지 않음, 범죄자들이 바로 스캐너로 남용할 거라 실제 범죄 검거 불가 우려, 대신 모든 무전 내용을 자동 녹음 및 저장해서 항공관제처럼 접근 가능하게 하며, 독립적 사건 조사와 공개까지 결합하면 요구하는 시민 감시와 accountability 실현 가능하다고 판단
     * 앱 개발자에 대한 ""우리는 그 앱, 그 사람 지켜보고 있으니 조심하라""는 공언 이젠 숨길 생각조차 없다는 느낌, 법조인 출신이라는 점에서 더 황당함, 본인 즉시 해당 앱 다운로드 하겠다는 의견, Streisand effect(금지 시도를 오히려 더 알리는 현상) 언급
     * Apple이 App Store에서 ICEBlock을 허용한 게 흥미롭게 느껴짐, 13년 전에는 해외 드론 타격을 알리는 앱은 objectionable content로 막았던 일 철회하지 않은 점 상기, 관련 기사
          + Apple이 현 정부를 싫어해서 앱 내리는데 시간 끌 듯한 예상, 하지만 ICE는 군대가 아니라서 해외 군사작전 방해와 민간인 경고는 본질적으로 다르다고 봄, 드론 앱을 더 불편해했던 이유 설명 가능
     * iOS만 지원하는데 안드로이드용은 프라이버시 이슈 때문에 제공하지 않는다는 개발자 설명, 안드로이드 관련 공지 첨부, Apple도 내부적으로 push notification 위한 정보 처리가 필요하나, 애플이 정부 상대 소송에 재정 여력이 있으니 개발자 손을 떠난 부분으로 간주
          + GrapheneOS의 반박 공개
          + 개발자 프라이버시 지식 부족하다는 의견, 오히려 안드로이드는 다음 세 가지 이유로 더 안전하다고 설명
               o A. 웹에서 직접 APK 설치 가능해 Play Store가 설치 사실 자체를 모름
               o B. 앱이 주기적으로 https 엔드포인트에 직접 접속하도록 하면 푸시서버에 기기id 로그 남기지 않음
               o C. 로컬 푸시 알림 자체 구현 가능
                 이 경우 기업에 남는 기록은 server의 ip로그뿐이라고 판단
          + 안드로이드에서 개인정보를 지키면서도 구현할 수 있는 다양한 솔루션 존재, 예로 ntfy 언급, 구글이 계속 AOSP 안에 있던 기능을 독점적 서비스로 옮기는 게 대안 방법 발전 막는 악영향 줌에 대한 불만
          + 아무것도 모르는 상태에서 뭔가 하려는 용기 자체가 의미 있다고 생각, 하지만 개발자는 어떤 방식이든 반드시 종단 간 암호화를 적용해서 인프라나 중개자도 ip나 설치 정보 외엔 볼 수 없게 해야 하며, 가능하면 tor over http 같은 방식을 써야 한다고 조언, 웹사이트 방식이나 익명 유지 노력 필요 지적
          + Play Store가 아니라 웹에서 직접 설치 가능하고, 자체 푸시 알림도 구현 가능하니 기존 주장 설득력 없다고 평가
     * 이란에서도 도덕 경찰 단속 위치를 공유하는 비슷한 앱 있다는 흥미로운 사례,
          + Gershad라는 앱 실사용자 층 계속 남아 있다는 근거로 관련 기사 공유
     * ICE에 세금 쓰는 대신 허리케인 위성 데이터가 더 낫다는 의견
"
"https://news.hada.io/topic?id=21735","tailwind CSS v4.0: 최신 웹 개발의 완벽한 게임 체인저 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              tailwind CSS v4.0: 최신 웹 개발의 완벽한 게임 체인저 [번역글]

     * v4.0의 가장 주요한 특징은 성능 개선 : 전체 빌드는 최대 5배, 증분 빌드는 100배 이상 빨라짐
     * CSS 우선 구성으로 패러다임의 전환 : tailwind.config.js 대신 CSS에서 직접 설정
     * @layer 를 사용한 스타일 우선순위 제어
     * @property 규칙으로 커스텀 속성 정의 가능
     * Color-Mix() 함수로 CSS 변수 및 currentColor를 포함한 모든 색상 값의 투명도를 조절
     * margin-left와 margin-right 대신 margin-inline 같은 논리적 속성으로 한번에 처리가능해져서 RTL 지원이 훨씬 간단해짐
     * 설치 과정 단순화 : npm i tailwindcss @tailwindcss/postcss export default { plugins: [""@tailwindcss/postcss""],}; @import ""tailwindcss""; 면 끝
     * 자동 콘텐츠 감지 : 이제 .gitignore 및 이미지/비디오등 바이너리를 자동으로 무시하고, 관련 파일 자동 스캔. 예외를 위한 @source 지원
     * Vite Plugin 우선 지원
     * 동적 유틸리티 생성으로 기존 설정에 존재하지 않는 값들을 바로 사용 가능
     * 진화된 색상 시스템 : 전체 기본 색상 팔레트가 RGB에서 OKLCH로 업그레이드
     * Container Queries가 프레임워크에 내장됨
     * 3D Transforms 지원과 향상된 그라디언트 추가
     * 새로운 Variants와 Utilities 포함 : not-* , starting, inert, nth-*, descendant Variant 및 field-sizing, color-scheme, inset-shadow-* Utilities
     * 예상보다 더 쉬운 마이그레이션 : 대부분의 변경 사항을 자동으로 업그레이드
     * tailwind가 그리는 큰 그림 : 단순한 프레임워크 업데이트를 넘어, CSS의 미래에 대한 과감한 도전
     * 업그레이드해야만 할까요? : 신규 프로젝트라면 v4.0을 선택. 기존 프로젝트는 최신 브라우저 대상 서비스라면 쉽게 업그레이드 가능
     * 결론 : tailwind CSS v4.0은 단순 진화가 아니라 혁명

   단순화 및 업그레이드라고 말하지만 결국 또 다른 뭔가를 배워라. 이것임.

   테일윈드를 적용하고 1년간 사용해본 결과
   다음 프로젝트에 사용할 것인가 ? YES
   웹개발의 게임 체인저인가? NO

   게임 체인저라는 말이 오버스러워서 댓을 달게 됐네요.
   앞으로 지향해야할 진보적 스타일이라 단정짓긴 어려운 것 같아요.

   테일윈드는 템플릿 복붙이나 AI 를 통한 간편생성 간단한 스타일 조정에는 장점이 있지만

   쉽게 읽을수 있는 태그도 난독화를 초래하고 디자이너가 만든 디자인을 정교하게 따라 만들기도 어려워요. Css변수만 잘 지정하면 인라인 스타일과 난이도상 크게 다를바 없을 것이라고도 생각합니다.

   rn 지원좀요..

   너무 멀리가지마… 모든 걸 삼킬지 몰라…

   일단 3.x 쓰다가 진짜 대세오면 마이그레이션 할듯

   https://tailwindcss.com/docs/upgrade-guide
     * shadow-sm -> shadow-xs
     * shadow -> shadow-sm

   다음 버전에선 또 어떤 호환성을 깨뜨릴 지 예측이 안 되네요

   계속.쏟아.저.나오는.웹기술.
   이미.있는.기술로도.충분함니다.
   씨.아셈블리.에이치티엠엘.자바스크립트.만.있어도.전부.가능합니다.

   아직도 엑셀 '팡션' 안 쓰시는분이신가요?
   계산기로도 전부 가능하죠.

   그런데 띄어쓰기 대신 포인트를 쓰는 이유가 뭔가요?

   김대리. 내가 감히 조언 하고 싶은것이 있습니다. 다른것이 아니고, 너무 엑셀 팡션? 사용 하지 마세요.. 편리함이 있다면, 위험성은 증대하죠. 소를 잡는데는 그만한 칼날이 있고 닭잡는데는 칼이 필요한가요? 쉬운것이 정답 일수 있습니다.

   ㅋㅋㅋㅋㅋㅋㅋ너무 웃겨요

   휴먼 틀딱체의 일부입니다
   어르신들이 띄어쓰기 대신 점을 자주 입력 하시는 것의 패러디인듯..

     기존 프로젝트는 최신 브라우저 대상 서비스라면 쉽게 업그레이드 가능

   😅
"
"https://news.hada.io/topic?id=21777","Grammarly, Superhuman 인수","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Grammarly, Superhuman 인수

     * AI 기반 작문 교정 서비스 Grammarly가 이메일 생산성 소프트웨어로 잘 알려진 Superhuman을 인수함
     * 이번 인수는 AI 기반의 문서 작성 및 커뮤니케이션 효율성을 대폭 향상시킬 것
     * Superhuman은 사용자의 이메일 관리, 일정 조정, 워크플로우 자동화 등의 혁신적인 기능을 제공해 왔음
     * 두 기업의 결합으로 인해 AI 기반의 글쓰기 지원과 이메일 워크플로우 자동화가 통합될 예정

시장 영향 및 배경

     * 최근 생산성 소프트웨어와 AI 툴의 융합 트렌드가 가속화되고 있음
     * Grammarly는 이번 인수로 경쟁사 대비 더욱 종합적이고 고도화된 솔루션을 제공할 전망임
     * Superhuman의 기술과 사용자 경험이 Grammarly 생태계에 통합되면, 문서 및 커뮤니케이션 절차 전반의 효율화가 가능해질 것

향후 계획

     * 인수 후 양사의 제품 및 기능 통합 로드맵은 아직 자세히 공개되지 않았음
     * 인수 금액, 팀 구조, 서비스 변경 계획 등 세부 정보는 추후 발표 예정

   ChatGPT, Gemini 같은 LLM 플랫폼 대비 경쟁력을 가지려면 통합된 경험이 필요하다고 판단한걸까 싶네요. 작문 교정이 가장 많이 일어나는 상황중에 하나가 이메일 작성일 것 같구요.

   전혀 생각치 못한 조합인데, 놀랍네요. Grammarly가 어떤 생각인지 궁금합니다.

        Hacker News 의견

     * Superhuman 사용자이자 과거 Grammarly 사용자로서 Superhuman의 이메일 클라이언트에 큰 팬심을 가지고 있음, 프리미엄 가격을 기꺼이 지불 중임. 기존에 사랑했던 Grammarly는 완전히 망쳤다고 느꼈음. 아주 잘 동작하던 앱을 폐지하고 원치 않았던 기능이 계속 강요되는 성가신 키보드 리플레이스를 밀어붙였기 때문임. 문제 제기에 대해선 개선이 아니라 형식적인 프라이버시 안내만 반복. Dropbox도 비슷하게 단순/신뢰성 있는 스토리지에서 복잡하고 무거운 소프트웨어로 변해 불만이 커졌던 경험. 결국 iCloud로 옮긴 후로는 더는 돌아볼 일이 없음. Superhuman만은 유저가 원하지도 않는 “차세대” 제품으로 바꾸지 말고 지금처럼 훌륭한 현재 이메일 클라이언트를 유지해 주길 간절히 바람
          + 기존 서비스 인수 융합 후 해지 이야기에 대해, 최근 읽었던 인상 깊은 구절이 생각남. 요약하자면 “회사가 곧 상품이고, 그 상품을 주주에게 파는 중”이라는 개념임. 요즘 기업들은 끝없는 성장 이야기만 팔기 위해 실제 고객을 등지면서까지 성장에 집착하는 트렌드가 매우 강함
          + Dropbox처럼 핵심을 잃었던 사례와 달리 Superhuman을 Spark Mail과 비교해서 어떤 점이 더 좋은지 궁금함
     * Grammarly는 존재론적 위기상황. 최상위 모델 무료 버전들만으로 충분히 대체 가능하며, 대체품은 더 뛰어나고 UI도 내 마음대로 컨트롤 가능함. 사실 이런 “웹 2.0” 비즈니스 모델들은 몇 번의 업데이트만 더 있으면 금방 완전히 대체될 수 있음
          + 이건 너무 개발자 시선임. 생수 시장만 봐도 수돗물과 재사용 병만으로 대체 가능하지만 미국에서만 470억 달러 시장임. Grammarly의 가치는 제품 그 자체가 아니라 네트워크, 유통, 고객 획득 채널, 툴 통합 등 얼굴에 익숙하게 만드는 ‘존재감'에 있음. 고객에게 정확히, 알맞은 때, 알맞은 장소에서 노출시키는 것이 전부임
          + 나 역시 다양한 AI 모델과 Grammarly 유료 구독자임. Grammarly의 강점은 단순히 LLM으로 대체 불가가 아니라, 특히 UI에서 차별점 존재
          + languagetool도 언급할만한 대체재임
          + Grammarly는 이미 최신 모델을 활용해 자체 비용을 줄일 수도 있음. LLM 발전은 아직 파괴적이지 않고 점진적임. 기존 유통망과 통합 확장을 통한 성장에는 큰 장벽이 보이지 않음
          + 그럼에도 불구하고 올해 10억 달러의 자금조달을 받음이 매우 놀라운 사실임
     * 최근 Superhuman 대체 이메일 클라이언트를 고민하고 있음. AI나 팀 기능은 전혀 필요치 않음. 내가 원한 것은 오직 아주 빠르고 키보드 중심의 데스크톱 이메일 클라이언트였음. 다년간 계속된 일상적인 버그와 검색 문제로 이 전환기에 남아있을 이유가 없어짐. Pine 시절로 돌아가고 싶은 마음. 키보드 중심 데스크탑 이메일로 무엇을 쓰는지 궁금함
          + Shortwave를 사용 중이며 꽤 만족스러움. AI 도입은 아쉽지만, 알림 제어나 메일 번들링은 Superhuman보다 훨씬 뛰어남
          + 나 역시 모든 이메일 클라이언트에 질려 직접 코드로 새 이메일 클라이언트를 만들기 시작함. Claude Code로 하루 만에 기본 버전 완성. 내가 원하는 프로그래머블 규칙/필터, 특정 이메일용 AI 초안 작성, 텍스트/다크모드 변환, API로 컨텍스트 정보 옆에 표시, 외부앱 원클릭 액션 등 자유롭게 구현 예정
          + 맥에서는 Mimestream이 Gmail 단축키를 그대로 구현해 학습 곡선이 전혀 없음. 덕분에 데스크탑 앱 느낌 그대로 즐기는 중임
          + 우리가 원하는 제품을 만드는 중임. https://marcoapp.io
          + 비슷한 고민에 Notion Mail도 써봤지만 Superhuman만큼 완성도가 아님. 시간이 흐르면 점점 더 좋아질 것으로 기대함
     * 슈퍼휴먼 사용자가 시간당 72% 더 많은 이메일을 주고받으며, AI 도구로 작성한 이메일 비율도 최근 1년 사이 다섯 배 증가를 자랑한다고 하는데, 이게 정말 추구할 만한 지표인지 의문임. 생산성 도구의 목표는 이메일을 줄이는 것 아닌가 생각임
          + 실제 슈퍼휴먼 사용자 동료들 경험을 보면, 이메일을 훑어보고 아주 짧은 답만 보내는 경향이 생김. 팀 응원 한 줄, 메일 전체 맥락 무시하고 다 읽으면 알 답변까지 질문, 이런 식으로 인박스를 비우는 쪽으로 흐름 전환. 최악의 경우, 모든 메일에 1문장 질문 반복, 답변에도 다시 질문, 하루 종일 이런 패턴이 이어져, 10초 이상 고민했으면 해결될 것을 15개답장으로 만들어버림
          + ""이메일 증가""는 오히려 사람들이 피하고 싶은 부분임
          + 하지만 이메일이 핵심 업무(세일즈, 리크루터 등)인 사람들에게는 매우 중요한 장점임
          + AI 작성 비율이 5배 늘었다는 건 곧 스팸도 5배 늘었다는 의미 같음
          + Superhuman의 CEO 인터뷰에 따르면 이 제품은 “인박스 제로”를 진심으로 원하는 사람들을 위해 설계된 것임. 이런 사람들에겐 최고의 결과임. 하지만 대부분의 사람에게 동일하게 바랄 목표는 아니며, Superhuman의 타겟 유저가 아니라는 설명을 [Acquired 팟캐스트]에서 자세히 들을 수 있음
     * Grammarly가 자본력이 많아 놀라움. 논문 쓸 때 유료 결제도 했지만 이제는 AI 덕분에 무료 Grammarly마저 필요 없어짐
          + 실제로 Grammarly는 생각보다 현금력이 높음: 공식 그로스 자금조달 발표
          + 나 역시 같은 생각이었는데, 대학생 자녀는 여전히 Grammarly가 ChatGPT보다 문법 수정을 잘 한다며 선호함. 최근 Coda 인수(실제론 Coda 경영진이 Grammarly를 맡는 역전 인수/어콰이어 같은 느낌)에 어떤 변화가 있을지 기대 중임
          + 로컬 LLM 역시 잘 그 역할을 수행함
     * Grammarly 유료 구독자인데 보안 불안이 큼. 대체 서비스에 원하는 점은
         1. 모든 데이터를 서버로 보내지 않고 로컬 처리(로컬 LLM)
         2. 페이지마다 수 MB의 스크립트를 주입하지 않는 가벼운 Chrome 확장
          + Harper가 많은 요구사항을 충족하며 점점 더 좋아지고 있음: https://writewithharper.com/
          + 참고로 최신 Chrome에는 내장 AI API가 공식 지원됨: https://developer.chrome.com/docs/ai/prompt-api. 시간이 지나면 이 로컬 API를 활용한 대체 확장도 등장할 것으로 기대. 단, 내장 모델은 Gemini Nano임
          + 컴퓨터가 문제 지점을 안내하고 사용자가 직접 수정하는 고전적, 인간다운 경험을 원한다면 proselint와 vale.sh 활용을 추천: proselint, vale.sh
          + 이 요구사항을 구현하는 것은 쉬울 것 같으며, 핵심 문제는 LLM 가중치 대용량 파일 배포일 뿐임. 처음엔 아마 CNN으로 시작했다 추정, Grammarly를 단순 맞춤법/문법교정만 쓰는지, UX에 특별함 있는지 궁금함. 개인적으로 써보니 특별함이 뭔지 모르겠다는 느낌 받음
     * 2021년 슈퍼휴먼 기업가치 8억2천5백만 달러, 연매출 3천5백만 달러라는 사실이 놀라움. 1년간 써봤는데, 가격이 터무니없이 비싸 가치에 불만족. 분명 쓸 만한 기능이 있고 키보드 중심의 접근은 신선했지만, 결국 단기간이면 다른 메일 클라이언트에서도 플러그인 등으로 충분히 구현 가능한 기능들이었음. 8억불 가치라면, 나도 메일 클라이언트 회사를 차려보고 싶은 생각이 듦
          + Zoom이 2021년 7월 기준 1,250억 달러에서 230억 달러로 급감한 것처럼, Superhuman의 진짜 가치는 1억 6천5백만 달러 수준일 것이라 추정. 만약 1억1천만 달러를 모두 썼다면, ~8만5천 명의 고객을 확보하는 데 고객당 약 1,300달러가 들었음. 실제로는 꾸준한 매출 유지와 영업/유지에 더 많은 비용 들어갈 것. 높은 고객 유지율은 인정하며, 내 경험이나 주변 레퍼럴 모두 장기간 사용 중임. 하지만 8억불 기업가치는 확실히 코로나 버블임
     * Grammarly가 점점 holdco처럼 변하고 Salesforce 방식을 따라가고 있다는 생각임.
          + 성장세가 주춤하지만, 충성 고객층이 있고 고점에서 투자 유치, 능력 좋은 팀, 창업자가 계속 제품의 미래를 이끄는 포지션. Salesforce도 Quip, Slack 등에서 유사하게 행동함
          + 이런 접근은 나쁜 전략이 아니라고 봄. 사모펀드(PE) 롤업보다는 창업자/스타트업 친화적이며, PE는 구조조정/최적화에는 장점 있지만 스타트업만의 마법에는 미치지 못하는 부분 존재. 대형 스타트업들이 상장 필요 없는 장기 사기업으로 진입하며 이런 사례는 더 늘 것임
     * Superhuman, Bay Area(및 NYC 일부)만 실제로 쓰는 가장 슈퍼한 이메일 클라이언트라는 점이 인상적임. 인수금액이 얼마인지 궁금하고 많지 않기를 바람
          + 연매출 3,500만 달러 기준 9만 명 가까운 고객이 필요함. 이 수치는 결코 적은 숫자가 아님. 직접 사용해본 결과, 최고의 이메일 경험이었음. 가격 대비 가치는 각자 필요와 직업에 따라 다름. CEO들은 가격을 대수롭지 않게 여기며, 개발자 입장에서 보면 가격이 과하다고 느껴질 수 있음
          + 이런 고객층(특정 지역, 직업, 고가 결제층)은 사업이 위기에 빠지더라도 (M&A 등) 구조조정이나 인수합병에 정말 강점이 됨
"
"https://news.hada.io/topic?id=21710","두 번째 브레인을 삭제함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             두 번째 브레인을 삭제함

     * 두 번째 브레인이라 불리는 디지털 노트 시스템의 제거 경험 중심 설명
     * 많은 사람들이 정보 수집과 정리에 집착하는 경향 언급
     * 효율성을 추구하다가 오히려 생산성 저하와 스트레스 유발 문제 인식
     * 정보 축적 자체보다 실질적 학습과 행동 전환의 중요성 강조
     * 삭제 후 집중력과 사고 명확성 회복 경험 공유
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

두 번째 브레인 삭제 경험 공유

  두 번째 브레인 개념과 생성 배경

     * 두 번째 브레인은 디지털 노트, 지식 관리 도구(예: Notion, Roam, Obsidian) 등으로 구성되는 정보 저장 시스템임
     * 사용자는 인터넷 글, 강의, 생각 메모 등 다양한 정보를 체계적으로 축적하는 목적 보유

  수집과 정리에 집착하는 현상

     * 본인 및 많은 사용자가 지식 수집·정리에 지나치게 집중하는 경향 발생
     * 알고리듬 기반 태깅, 카테고리화, 크로스링크 작업 등에 시간 소모 현상 나타남
     * 정보를 실제 사용하거나, 생산적 창작으로 전환하는 빈도는 낮은 문제점 존재

  기대와 현실의 괴리

     * 생산성과 창의성 증진이라는 예상과 달리, 오히려 스트레스와 정체감 증가 현상 인식
     * 방대한 정보 정비에 에너지가 소모되어, 본질적 업무와 학습 진행 방해 요인 작용

  삭제 결정과 실질적 변화

     * 저장해온 두 번째 브레인 데이터 전부 삭제 결정 경험 공유
     * 삭제 이후 남는 불안감 있었으나, 실제로는 집중도 향상 및 사고 명료성 회복 증언
     * 실질적 정보 소화 및 행동 중심의 생활 패턴 자리잡음

  최종 인사이트

     * 방대한 정보 축적보다, 직접 학습이나 실천을 통한 성장이 더 효과적임을 강조
     * 정보 관리 도구 사용 목적 재정립 및 “필요할 때만 정보를 기록하는 습관”의 중요성 언급

        Hacker News 의견

     * 나는 글쓴이가 지운 노트를 왜 그랬는지 이해함. 하지만 나는 내 노트 아카이브는 절대 지우지 않을 생각임. 그 이유는 자주 사용하지 않는 작업의 방법 정리, 오랜 기간 돌려가며 진행하는 프로젝트의 현황, 차량 정비 로그, 중요한 계정들의 상세 정보 등 다양한 실용적 정보 저장 때문임. 복잡한 일을 할 때, 나는 단계별로 하는 행동을 노트에 써 내려감. 거의 대부분은 기록만 하고 다시 보지 않지만, 이렇게 하면 일종의 '글로 쓰는 러버덕' 역할을 하며, 아주 가끔(100개 중 1개) 십 년 전 내 작업 방식을 다시 참고해야 할 때 매우 유용함. 같은 앱을 직장에서도 다른 저장방식으로 쓰는데, 거기는 성과 평가를 위해 한 일들을 추적하는 용도임. 모든 수정은 타임스탬프와 함께 기록되고, 시간순으로 편집본을 정렬하는 별도 툴도 만듦. 글쓴이에게 이 시스템은
       자기계발 불안 해소 수단이었던 것 같지만, 오히려 새로운 불안을 만들었음. 내 생각에 진정한 '세컨드 브레인'은 아니었음
          + ""내 노트 아카이브는 절대 지우지 않을 거다""라는 말에 공감함. 블로그 글이 '세컨드 브레인'을 멋지게 포장하지만, 실제 사용 내역보면 주로 할 일 목록에 가까움. 내 기준으로는 그게 세컨드 브레인이라고 생각하지 않음. 예전 엔지니어들이 하던 로그, 즉 했던 기록, 측정값, 관찰 등을 남기는 전통과 비슷해야 진정한 세컨드 브레인임. 반면, 할 일 목록은 스스로 부과한 일만 적는 것임. 저자가 불안을 느낀 게 당연함. 나도 7년 치 숙제같은 오래되고 밀린 일감 로그를 본다면 불안할 듯. 나는 로그란 그때 중요하다고 느낀 걸 적고, 시간이 지나면 미련 없이 삭제하는 것임. 오늘 적은 것이 미래의 나에게 선물이 되질 않고, 오히려 괴로움의 원인이라면 그냥 없애버려야 함. 인생도 그렇듯이, 기쁨을 주는 건 남기고 괴로움 주는 건 치워야 함. 세컨드
            브레인도 똑같음
          + 20년쯤 지난 후에 내 과거 생각과 아이디어를 읽는 능력을 잃는 걸 상상만 해도 싫음. 10년 넘게 쓴 프로젝트/아이디어 노트가 있는데, 가끔 보면 정말 재미있음. 프리랜서 시절, HTML+CSS 프로젝트를 빠르게 시작하게 해줬던 코드 생성 툴을 만든 걸 무척 자랑스러워했던 그때, 그 페이지를 지금 보면 미소가 절로 나옴
          + 최근에 15년 전 데이터 3TB를 복구함. 잃어버린 줄 알던 하드디스크를 친구가 갖고 있었음. 사실 데이터 자체는 그립지 않지만, 오래된 사진과 노트를 다시 보는 건 정말 좋았음. 내 조언은 하드디스크에 백업해서 어딘가 숨겨두기. 15년 후에 열어보면 정말 특별함
          + 나도 비슷한 철학을 따름. 나도 불안이 많은 스타일이라서, 자료가 너무 많아질 땐 날짜별로 노트와 프로젝트 파일을 압축(zip)해서 별도 아카이브 폴더에 저장함. 필요하면 언제든 참고할 수 있지만, 기본 작업 공간은 항상 깨끗함. 파일 트리도 같이 저장해두면 접근이 쉬움. cron job으로 자동화도 손쉽게 가능함
          + 당신이 만든 시스템은 '세컨드 브레인'이 아니라 실용적이고 현실적인 '외부 기억 장치'에 가까움. 내 삶을 도와주는 용도라는 점이 진짜 큰 차이임. 시스템이 사람이 아니라, 사람이 시스템을 위해 존재하는 게 아님
     * 나는 이런 방식에는 동의 못함. 저자가 자기 내면 문제 때문에 지식을 날려버렸는데, 도서관 전체를 태워버린 느낌임. 잠깐 내려놓기만 해도 됐지, 꼭 그렇게 완전히 파괴할 필요까진 없었음. 결국 ""자기 뇌를 일부러 날려서 앞으로 다 새로 배우고 또 할 것"" 같은 걸로 보임. 7년 후에는 예전의 내면 불안과 새롭게 생긴 불안을 비교할 수 없음을 후회할 것임. 굳이 다 지울 필요 없이, USB나 클라우드에 압축해서 놔두는 게 그리기 힘든 선택임
          + 쌓아두는 습관(hoarding)이 문제라면, 대개는 쌓인 것 중 의미 있는 게 거의 없음. 잡음은 많고 신호(진짜 보석)는 적음. 의미 있는 걸 찾으려면 엄청난 노력이 필요하고, 글쓴이도 그 부분을 부담스러워했음. 보통 hoarder는 외부 도움이 필요하고, 못 받으면 총정리(다 버리기)도 나쁘지 않은 선택임
          + 이건 두려움에서 비롯된 듯함. 비난이 아니라 그냥 관찰임. ""핵옵션"", ""파괴"", ""로보토마이즈"" 같은 단어 선택도 그래서임. 나는 40대인데, 반복적으로 자료를 모았다가 버리기를 경험함. 후회했던 적도 있지만, 훨씬 더 자주 아무 느낌 없거나 심지어 낫다고 생각함. 어쩌다가 후회해도, 버린 데 집착하거나 괴로워한 적 없음. 오히려 '리셋'이 가치 있는 경험이었던 때도 종종 있었음. 새로운 길이 열리기도 함. 모든 걸 다시 배우는 게 고통일 거란 생각보다, 실제로는 새로운 경험과 배움이 다시금 따라옴. 재시작에 고통만 있는 건 아니고, 새로운 것을 발견할 기회임
          + 이건 생각 정리의 Britney Spears 삭발 사건급. 즉, 정신적 붕괴고, 그 뿌리가 정신 건강 문제에 있다는 게 본질임. 그걸 글로 풀어내면서 처리하고 있음
          + 사실 우리 대부분은 정리된 노트 없이도 잘 살고 있음. 가끔 옛날 수첩 보면 재미있기도 하겠지만, 딱히 그리움은 없음. 글쓴이는 자기에게 맞는 선택을 한 거고, 모두가 그래야 할 필요는 없음
          +

     저자가 자신의 개인적 이슈로 인해 지식을 파괴했다 <— 오히려 당신이 자신의 관점을 타인 공감보다 앞세우고 있음. 지식 파괴는 아니었음. 다시 볼 일 없는 노트였고, 글쓴이한테 스트레스였다면 그대로 둬도 의미 없었음. 어차피 안 볼 노트라면, 애초에 안 쓴 것과 다를 게 없음. 진짜 도서관이 아니라 신문지가 산 더미로 쌓인 집과 비슷할 수 있음. 메모 삭제한다고 바로 다 잊어버리지 않음. 필요한 교훈은 이미 내면화했을 테고, 중요한 게 아니면 남아있지 않아도 됨. 7년 후 후회할 거라는 것도 당신 생각이고, 내 경험으론 지우고 살아가는 편이 훨씬 행복함. 남도 그럴 필요 없고, 모두 각자 맞는 방식을 선택해야 함. '멀리 치워두는 것'과 완전히 지우는 건 다름. 미련에서 벗어나 자유를 얻는 거임. 나는 글쓴이의 결단을 응원함
     * Zettelkasten, Second Brain, PKM 등 노트 시스템 전반에 대한 문제점은 '무언가 특별하고 거대한 결과물이 나올 것이란 기대감'임. Zettelkasten 커뮤니티는 오래된 사회학자가 이 시스템을 통해 수많은 논문을 냈다고 자랑하지만, 지금 보면 그 논문들이 별 영향 없는 일이 많음. 그리고 엄격한 단계와 규칙을 따라야 할 것처럼 굴지만, 실제로는 필요 이상으로 복잡함. 나 역시 꽤 똑똑하다고 생각하는데 ZK의 개념체계는 여전히 제대로 파악하지 못하겠음. 주위 실제 연구자들은 오히려 이런 거 안 쓰고, 내 배우자도 수백 편 논문 쓴 연구자지만 방식이 전혀 다름. 어쨌든 내 노트는 절반은 정리되고, 절반은 혼란스러운 상태로 다양한 방식이 섞여 있음. 난 그게 '수집가의 오류'지만 괜찮음
          + '퍼포먼스적이고 상징적인 행위'라는 평가에 동의함. 퍼포먼스와 상징은 의미 있음. 실제로 행동이 사고방식에 영향을 줌. 어떤 목표가 이미 이루어진 것처럼 행동하면, 의식이 그에 맞춰 바뀐다는 것도 잘 알려짐. 예시로 억지로라도 웃으면 기분이 좋아지는 것처럼, 이런 행동으로 인생 변화 시도는 실제로 효과 있음. 그리고, 노트 삭제를 굳이 이해 못하겠다고 말해도, 사실 말하는 걸 보면 어느 정도 납득은 되고, 본인에게 필요 없으면 안 해도 됨
          +

     오래된 학자 <— Luhmann은 여전히 여러 학문에서 가장 많이 인용되고 고민되는 사회학자 중 한 명임
          + Zettelkasten을 싫어해도 Luhmann이 실질적 영향력이 없다 주장하는 건 정보 부족임. 그는 지난 세기 가장 영향력 있는 대륙 사회학자 중 한 명이었음. 듀르켐만큼은 아니지만, 99.9%의 인간이 절대 도달 못할 국제적 영향도 가짐
          + 의미 있는 일을 하는 대부분 친구들은 현실적으로 자신의 사고 과정을 반영하는 맞춤형 시스템을 직접 만들었지, 이상적 프로세스 따르지 않음
     * 평생 진심으로 후회하는 결정 중 하나가, 80년대 프로그래밍 배울 때 쓴 오래된 노트북을 버린 것임. 나 역시 글쓴이와 비슷한 생각으로, 추억에 집착하면 앞으로 나아가기 어렵고 머릿속이 지저분하다 생각해서 정리했음. 하지만 그 노트는 내 존재의 '한 시대'를 고스란히 보여주는 물건이었음. 사진처럼 내 '이전 자아'와 연결되는 추억을 없애버린 셈임. 그런 기억(메멘토)은 단순히 애틋할 뿐 아니라 내 모든 자아 버전을 잇는 앵커 역할도 함. 뜻밖의 동기부여나 마음의 연결을 제공하기도 함. 글쓴이가 버린 기록들이 도구로서 쓸모없어 보였을지라도, 자기 탐구라는 고고학적 관점에서는 큰 의미의 자료였을 것임. 비슷한 선택을 하는 누구에게도, 완전 삭제보다는 손에 잘 닿지 않는 곳에 보관하는 방식을 적극 권하고 싶음. 나중에 크게 후회할 수도 있음
     * ""Obsidian은 여전히 사랑하지만, 앞으로는 세컨드 브레인이 아닌 내 원래 뇌를 위한 작업공간으로 더 신경 써서 사용할 것""이라는 글을 보고, 예전에 일터에서 흔히 겪었던 상황이 떠오름. ""지식 베이스가 엉망이니까 다 지우고 새로 만들자!""고 한다가, 금방 새로운 시스템도 똑같이 엉망이 되어버림. 이제는 둘 다 정리 안 되고 오래된 자료가 뒤섞여서 검색만 더 어려워짐. 왜 기존 자료를 정리하기를 꺼리는지, 나는 내 개인 지식 베이스를 결코 삭제하지 않을 것임. 앞으로도 필요에 따라 고쳐 나갈 생각임
          + 자료 정리는 당장 해야 하는 큰 귀찮은 일이고, 새로운 지식 베이스 구축은 당장 재미있는 일임. 미래의 '나'가 이걸 잘 관리해주길 바라는 심정임
          + 나는 지난주 절반을 내부 제품 가이드와 문서 페이지를 고치는 데 쏟음. 20명이 사실상 오류를 알고 있었지만, 아무도 수정하지 않음
          + 두 지식 베이스가 모두 어수선한 상태가 된다면, 새로운 시스템 이전보다 단순하게 스케줄을 잡고 실행해야 함. ""기존은 버리고 새로 만든다""는 접근 자체가 문제는 아니며, 둘 다 유지되는 건 스케줄 관리 실패 때문임
     * 내 삶의 철칙은 '무엇이든 지나치면 해로움'임. Obsidian으로 노트 시작하면서, 처음엔 모든 내용을 폴더와 서브폴더로 너무 세밀하게 나누려고 하다가 금방 피로해짐. 지금은 거의 모든 노트를 한 폴더에 저장함. 노트 작성 시점은 1) 독서 중, 2) 머릿속에 맴도는 중요한 생각이 있을 때(요즘 드물지만), 3) IP주소, 연락처 등 꼭 필요한 정보일 때임. 노트를 집착적으로 생각하지 않으니 오히려 마음이 편함. 대부분의 생각은 쓸데없고 금방 사라져서 기록할 가치가 없다고 생각함. 이런 방식 덕분에 1년이 지나도 내 vault가 단순하고, 검색도 꼭 필요한 키워드만 찾아보게 되어 정보 과부하가 없음. 필요 없어진 노트는 정기적으로 archive로 옮김
          + 나도 어디에 뭘 넣어야 할지 너무 따진 적 있음. 최근에는 PARA 시스템(https://fortelabs.com/blog/para/)으로 Obsidian을 새로 시작하고, LLM (Cursor, Claude Code)로 자료 분류 결정을 지원받고 있음. 지금까지 꽤 큰 도움임
          + 내 노트 작성 습관도 결국 큰 'Work' 노트 하나와, 여행 계획이나 장보기 같은 임시 노트만 가끔 만듦. PKM은 때로 일종의 '생산적 미루기'로 변질됨. 폴더 만들고 체계화하는 게 진짜 일하는 것보다 더 재미있을 때가 많았음. 그래서 그냥 최소한만 하기로 함
          + ""각자 생각은 무가치하다""는 데는 동의 못함. 내 생각은 스스로도 귀하고, 자연스럽게 기록하고 피워나가는 게 큰 기쁨임. 생각하는 존재로 살면서 내 생각이 쓸모 없다는 걸 인정하고 싶지 않음. 전부 다 기록하진 않지만, 노년에 이르기까지 한두 가지 흥미 있는 생각을 꾸준히 남길 수 있길 희망함
          + ""무엇이든 지나치면 해롭다""는 원칙에 정말 공감함. 절제의 미학이 중요함
          + ""παν μέτρον άριστον (모든 것은 적당하게)""—황금률에 대한 내용은 여기
     * PKM/세컨드 브레인 ""산업""이 너무 과도하게 느껴졌고, 쓸데없이 복잡한 규칙이나 atomic notes 스타일을 유지해본 적 없음. 대신 나는 하이퍼링크 중심의 간단한 노트만 작성함(https://ezhik.jp/hypertext-maximalism/). 노트는 많이 쌓이더라도 간단하게 관리하기 때문에 부담 없음. 예전 자아, 과거 관심사를 들여다볼 수 있어 좋고, 과거와 현재를 이어주는 기록이 됨. 과거의 나와 괴리감 없이, 시간에 따라 관계만 달라질 뿐임
          + 나도 공감함. 사실 이들은 '노트 작성 도구'가 아니라, 막연하게 모든 혼란을 해결해준다는 구조적 환상을 파는 쪽임
     * 나는 소수에 해당할 듯함. 개발자/IT계에서는 흔하지 않지만 '개인 지식 관리 베이스'는 따로 만들지 않음. 개인 Notion에는 가보고 싶은 레스토랑 목록, 여행지, 쓰레기 수거 요일 등 참고용 정보만 적어둠. 읽은 내용이나 배운 지식을 따로 아카이브하지 않고, 머릿속 기억에 완전히 의존함. 읽을 예정으로 탭 저장도 안 함. 길거나 복잡한 건 훑어보고 닫음. 인상 깊은 내용은 주로 친구나 동료들과 단체 채팅방에서 대화함. 중요한 논의가 있었던 건 자연스럽게 기억에 남고, 나중에 필요하면 키워드만 떠올라서 검색하거나 LLM, 구글 등으로 쉽게 다시 찾음. 이렇게 수십 년 일했지만 필수 지식이 필요해서 곤란했던 적 없음. 즉, 내겐 개인 지식 베이스가 쓸모없음. 아마 읽는 분들에게도 참고가 될 수 있다 생각함. 혹시 이 방식이 마음이 가벼워질 수 있음. 직장
       회의에서도 똑같이 적용함. 회의 중 필기하면 집중이 안 돼서, 집중해서 듣고나서 나중에 기억나는 내용만 Slack에 정리함. 웬만해선 다 기억나고, 아주 가끔 누락돼도 다른 사람이 채워줌
          + 친구나 동료들이 곧 내 '개인 지식 관리 베이스'임. 이걸 '외부 브레인'이라고도 부름. 내가 얘기하거나 기록하지 않으면 잊혀지기 쉬운데, 중요하면 누군가 혹은 앱, 일정, 최소한의 노트 등에서 다시 떠오름. 나는 긴 플랜만 작성/확인함
     * 글이 정말 잘 쓰여서 인상 깊었음. 내 경우 드립 커피 추출을 매번 자세히 기록하고, 언젠가 분석해서 완벽한 레시피를 얻을 거라 기대한 적 있음. 하지만 결코 그 데이터를 다시 본 적이 없음. 곧 깨달은 건, 다음 한 잔에 온전히 집중하며 실수와 개선을 계속하는 게 더 인간적이고 살아있는 지식이라는 점임
          + 모든 걸 과도하게 분석하면 본질의 '마법'이 사라짐. 나는 차를 마시는데, 그 과정에도 비슷한 의미와 의식이 숨어 있는 듯. 과학 실험으로 만들면 오히려 어울리지 않음. 인생의 많은 일, 창의적 사고나 몰입(flow) 작업에도 이처럼 지나친 기록과 분류가 흐름을 끊음. 내 경우라면, '완벽한 한 잔'을 위해 작은 기간 동안만 모든 기록을 남긴 뒤 분석하고, 교훈만 추려내고, 평소에는 더 유연한 방식으로 돌아가는 것이 이상적임. 특히 원글에는 집착과 불안 경향이 보이는데, 모든 걸 다 기록하지 않아도 됨. 어떤 도구든 프로젝트가 정말 복잡할 때만 강력한 ""세컨드 브레인"" 역할이 필요하고, 대부분은 아주 간단해도 충분. 완벽한 시스템을 만들려는 시도는 불안의 원천이 될 수 있고, 그게 목적이 된다면 당연히 부담이 커질 수밖에 없음
     * ""필요 없는 건 지운다, 모든 걸 남기지 않는다, 대화와 맥락에서 생각한다, 세컨드 브레인을 만들지 않는다, 첫 번째 뇌를 산다""—이 관점이 요즘 시대에 특히 더 중요함. 모든 걸 더 빨리, 더 많이, 더 잘해야 한다는 압박이 점점 심해짐. AI로 불가능했던 것도 해내라는 분위기, Zettelkasten 시스템이 자동으로 인사이트를 줄 거라는 환상, 끊임없는 새로운 것에 주의를 빼앗기는 환경 등. 어떤 사람에게는 그런 게 정신적으로 보람찼겠지만, 남들에게는 오히려 불안과 부족감, 끝없는 결핍만 키움. Emerson의 Self Reliance 글귀가 떠오름: ""삶은 과거에 있지 않고, 현재 변화의 순간에 의미가 있다. 힘은 안주에서 멈추고, 잠깐의 변화와 목표를 향한 질주에 있다. 영혼은 늘 새롭게 되는 것을 가장 싫어하고, 그렇게 될 때 과거의 모든 것은 무가치해진다.""
"
"https://news.hada.io/topic?id=21698","DOM 템플릿 API를 도입할 적기임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          DOM 템플릿 API를 도입할 적기임

     * 웹 플랫폼에는 현재 개발자들이 꼭 필요로 하는 선언적 템플릿 API가 부족함
     * 대부분의 현대 웹 프레임워크에서는 템플레이팅이 핵심 기술이지만, 표준 DOM API에는 안전하고 효율적인 템플릿 생성·업데이트 방법이 없음
     * 이로 인해 사용자와 개발자, 프레임워크, 심지어 플랫폼 차원에서 모두 불편과 성능 저하 문제 발생함
     * 템플릿 API를 도입할 시점으로, 최근 프레임워크 경험과 자바스크립트 기능이 충분히 축적되어 구현 및 표준화가 더 실용적임
     * 템플릿 식별성, 시그널 기반 반응성 등 다양한 모델을 종합해, 차세대 템플릿 API의 방향성이 제시됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

제안 요약

     * 이 글은 웹 플랫폼에 선언적 템플릿 API를 추가할 것을 제안하는 배경과 필요성에 대해 설명함
     * DOM API는 강력하지만, 표준 DOM에서는 데이터를 바탕으로 안전하고 효율적으로 여러 노드를 생성·업데이트할 템플릿 API가 부재함
     * React, Vue, Angular 등 주요 프레임워크는 모두 선언적 템플레이팅이 핵심이며, 개발 생산성과 보안, 성능, 정적 분석, 서버 사이드 렌더링 등 여러 이점 제공함
     * 템플릿 API 부재로 인해 사용자, 개발자, 프레임워크, 그리고 플랫폼 모두가 불필요한 복잡성, 보안 위협, 성능 저하, 진입 장벽 등을 겪음
     * 지금이 API를 도입할 적기임을 주장하며, 기존 프레임워크 경험과 현대 자바스크립트 기능을 활용하여 점진적 표준화를 제안함

템플릿의 필요성 및 현재 문제

     * DOM API는 웹 플랫폼의 동적 기능을 이끄는 기반임
     * 하지만 표준 DOM에는 데이터로부터 DOM 트리를 안전하게 정의하고 효율적으로 업데이트하는 선언적 템플릿 방법이 없음
     * 현대 웹 프레임워크(React, Vue, Angular, Svelte 등)는 모두 선언적 템플레이팅을 제공함
     * 선언적 템플레이팅이 인기 있는 이유는 다음과 같음
          + 명령형 API 대비 더 나은 사용성과 가독성 제공
          + XSS 보안 강화. 템플릿 내부 데이터 자동 이스케이프
          + 효율적이고 빠른 렌더링 성능
          + 정적 분석, 타입체크, 인텔리센스 등 개발 생산성 증대
          + 서버 사이드 렌더링 지원

현재 상황의 문제점

     * 사용자: 라이브러리 다운로드 및 렌더링 지연으로 초기 로딩이 느려짐. 클라이언트 코드 크기 증가로 UX 악화
     * 개발자: 템플릿 사용을 위해 별도 라이브러리(npm, CDN 등) 필요. 진입 장벽, 비표준 적재
     * 프레임워크: 템플릿 엔진 직접 구현 필요. 성능, 기능, 코드크기 간 무거운 트레이드오프 존재
     * 플랫폼: 네이티브 앱과의 경쟁에서 불리. Flutter, SwiftUI 등은 내장 템플릿 시스템 제공

지금이 적기인 이유

     * 과거 템플릿 관련 시도(E4X, E4H, html 템플릿 리터럴 등)는 실패했으나, 당시에는 DOM 업데이트에 약점이 있었음
     * 최근 프레임워크와 커뮤니티에서 템플릿 API의 베스트 프랙티스가 충분히 축적됨
     * 자바스크립트 기반 API 제안 가능. 현재 표준 JS에선 태그드 템플릿 리터럴이 이미 존재
     * 바닐라 JS 개발자와 웹 컴포넌트 커뮤니티에서도 편리한 DOM 조작 수요가 지속해서 증가함
     * DOM Parts 등 저수준 원시 제안도 병행 중이나, 고수준 선언적 API가 더 큰 효용과 발전 방향 제시 가능

좋은 템플릿 문법의 사례 분석

     * 주류 템플릿 시스템(React-JSX, Vue, Svelte, Angular 등)은 말단적으로 매우 유사한 마크업+바인딩 문법 기반임
     * JS API 기반 템플릿의 경우, 템플릿 표현식이 DOM 설명을 반환하고, 별도 렌더 함수가 이를 실제 DOM에 반영하는 구조가 일반적임
     * E4X 등 구시대 시도는 DOM 자체를 리턴하여 업데이트 모델이 떨어졌음

자바스크립트 기반 템플릿 API 가능성

     * 태그드 템플릿 리터럴을 통해, 새로운 JS 기능 도입 없이 템플릿 API 설계 가능
     * 이미 JSX-to-Lit 등 여러 실증 사례 보유

JSX 통합 논의

     * JSX 표준화는 복잡한 의미 정의 및 런타임 시멘틱스가 필요함. JSX 자체는 문법일 뿐임
     * 현행 비표준 JSX는 순수 문법 변환이므로, 향후 표준 템플릿 API가 도입되면 JSX->템플릿 리터럴 변환 컴파일러로 연계 가능
     * 추후 진짜 JSX 표준화 시, 템플릿 API에 맞춘 데이터 타입 수용 구조로 전환 용이

HTML 기반 템플릿 API와의 관계

     * 많은 개발자와 커뮤니티에서 HTML 템플릿 시스템을 요청함
     * 그러나 HTML 기반 시스템은 바인딩, 표현식 언어, 제어문 등 새로운 문법·표현식 설계가 요구되어 훨씬 큰 작업임
     * 최근 프레임워크(Lit 등)가 HTML 템플릿에서 JS API로 옮겨온 배경 역시 동일
     * 따라서 JS 기반 템플릿 API가 우선적으로 도입되고, 추후 HTML API로 단계적 확장 가능성이 큼

반응성(reactivity) 구현 경험의 성숙

     * VDOM diffing(React), 템플릿 식별성(Lit), 신호(fine-grained signals, Solid/Svelte/Vue 등) 등 다양한 반응성 모델이 검증됨
     * VDOM 기반은 느린 반면, 템플릿 식별성+ 시그널 모델의 조합은 빠르고 효율, 설명력도 높음
     * 시그널 기반은 모든 데이터가 신호로 래핑될 필요 있으나, 일반적인 데이터와 혼용도 가능함

발전 경로와 기대효과

     * 제안된 선언적 JS 템플릿 API는 바닐라 JS·웹 컴포넌트·새 프레임워크 모두에 직접적 이점 제공
     * 기존 프레임워크에도 컴파일 타겟·런타임 백엔드 또는 직접 지원 API로 활용 가능
     * ""재렌더링"" 방식과 시그널 기반의 반응성 모두 지원
     * 차후 HTML 기반 선언형 템플릿, 선언형 커스텀 엘리먼트로의 확장 기반 마련
          + API 범위 좁고 명확, 기존 API(예: DOM Parts)와의 연동도 용이
     * 다만 표면상의 API와 문법은 단순하지만, 하부 DOM Parts 및 스케줄러 등 구현 면적은 넓으며, 표준화·테스트 등 협업이 필요함

마무리

     * 저자는 GitHub 이슈에서 본 제안을 논의 중이며, 플랫폼 엔지니어 등 커뮤니티의 참여를 요청함

        Hacker News 의견

     * ""좋은 템플릿 문법을 우리가 안다""는 말에 웃음이 나오는 이유는, 실제로는 그 기준조차 제대로 합의된 적이 없다는 생각임. 템플릿은 기호적(symbolic)인 관점보다 시각적(visual)인 경험이 더 중요하다고 생각함. 예전 Dreamweaver와 같은 도구가 그토록 성공했던 것도 이 때문임. 많은 디자이너들이 Photoshop 같은 툴로 배움을 시작하는 것도 같은 맥락임. 지금 이 시도가 XSLT를 다시 만들려는 시도 같아서 아쉬움이 있음. 잘 만들어지지 않은 구조물을 잘 만들어진 결과로 합치는 게 템플릿팅의 본질 문제임. 더 나아가, 'label'과 'for'처럼 연결은 돼 있지만 같은 트리에 속하지 않은 엔티티를 표현하는 문제가 있음. 내가 마법을 부릴 수 있다면, 굳이 표준 문서 레이아웃에 기이하게 모든 걸 맞추려 하지 않기를 바람. 절대 위치(absolute positioning)를 잘만 써도 많은 UI 문제를
       효율적으로 해결할 수 있는데, 왜 이렇게 반복적으로 수학적 연산까지 모두 기계에 강제로 맡기려 하는지 의문임
          + XSLT를 다시 만들려고 한다는 것에 공감함. XML 자체는 좋아하지 않았지만 XSLT는 정말 강력한 존재였음. 브라우저에서도 아직 널리 지원되는 기능임. XML이 설정(Configuration)이나 IPC 등에서는 단점이 두드러졌지만, 뛰어난 마크업 언어에 XSLT의 변환력이 더해지는 부분에서는 오히려 제대로 활용되지 못했다는 점이 아쉬움. XSLT가 대중화되지 못한 건 진짜 선언적이고 함수형인 DSL이라서임. 많은 사람이 DSL에 대해 긍정적으로 얘기하지만, 실제로는 인기 있는 언어의 절차적 의미만 얇게 감싼 수준인 경우가 많음. 잘 설계된 DSL로 복잡한 일을 간단히 처리할 수 있는데, 공부하려는 노력이 부족하다고 생각함
          + 제대로 된 템플릿 문법이라는 게 시각성이 핵심이라고 말하는데, 이런 결론에 이르게 된 이유를 알고 싶음. 내 생각에는 HTML+CSS 자체, 즉 생성 방식에 대한 불만처럼 들림. 절대 위치 언급한 이유도 궁금함. 절대 위치는 분명 쓸 자리에서는 유용하지만, 전체 레이아웃을 위해 쓸 때는 오히려 관리가 어렵고 화면 크기나 콘텐츠 양에 따라 쉽게 망가짐. 신문 레이아웃도 실제로는 문자와 타이포그래피의 미묘한 요소가 많아서 절대 위치만으로 해결이 불가함. CSS를 깊이 다룰 때 절대 위치로 시작된 레이아웃을 나중에 flex나 flow로 재구성하는 게 오히려 빠르고 쉽게 문제를 해결한 경험이 많음. calc()와 viewport 단위를 잘 쓰면 의미를 찾을 수 있지만, 실제로는 완전히 고정된 콘텐츠나 뷰포트가 아니라면 절대 위치는 적합하지 않음
          + 사람들이 절대 위치를 잘 활용하면 쉽고 빠르게 구현할 수 있는 걸 너무 복잡하게 돌아가서 결국은 같은 효과를 내려고 한다는 이야기를 봤는데, 웹에서는 모든 디바이스의 크기, 방향, 성능에 따라 문서가 좋아 보여야 하는 요구 사항이 있음. 일반 앱(윈도우 앱 등)은 이런 고민이 없고, 모바일 앱도 표준화된 화면 크기만 고려하면 됨. 오직 웹만이 이 모든 걸 다뤄야 하는 특성이 있음
          + ""좋은 템플릿 문법""에 비꼬듯 반응하는 건 진보를 주장하는 사람에게 크게 예의 있는 태도는 아니라는 생각임. 그리고 지금은 좋은 템플릿 문법이 있다고 생각함, 바로 jsx가 대표적임. 나는 React의 팬이 아니지만, jsx가 웹 개발에 혁신을 일으켰고, 대부분의 js 템플릿 시스템이 ""표현식으로서의 템플릿"", ""중첩을 통한 조합"", 자바스크립트로 제어 흐름을 다루는 구조로 거의 수렴했다고 생각함
          + React와 Svelte는 표면상으로만 닮아 있을 뿐, 실제 동작 방식은 꽤 다름. React는 (거의) 평범한 자바스크립트 함수가 JSX 형태의 지연(lazy) 마크업을 반환하는 것이 핵심임. 루프나 조건부 렌더링에 대한 템플릿 자체 구문이 없고, 전부 일반 자바스크립트로 처리하는 점이 주요 차이점임
     * API와 ABI(애플리케이션 이진 인터페이스)는 절대 최종적이지 않다는 점을 반복적으로 배움. 앱의 요구는 고정되지 않고, 시간에 따라 계속 바뀌므로 영원히 쓸 수 있는 완벽한 API는 존재하지 않음. 이번 제안이 좋은 예시임. 문제를 우선 사용자 라이브러리(React 등)가 해결하다가, 결국 표준으로 내려올 때가 됨. Polyfill도 마찬가지 패턴임. 이런 제안이 성공해도, 결국 구형 기술이 되고, 사람들은 이를 우회하려 새로운 방법을 만듦. DOM API, ECMA, 구형 브라우저 등도 같은 운명임. 기술적인 엔트로피, 확장성, 그리고 하위 호환성 자체를 표준 사용 사례로 생각할 수 있을지 고민하게 됨
          + 웹 표준의 새로운 기능 추가는 결국 유지 관리에 엄청난 코드 부담이 생기고, 표준을 따르는 브라우저를 만들 때도 구현해야 할 코드가 계속 늘어남. Servo 같은 프로젝트가 조금이라도 따라잡으려 할 때 늘 확장만 따라가야 하는 상황이 반복됨. 웹 플랫폼이 네이티브 환경의 모든 기능을 가질 수 있길 바람(프라이버시와 샌드박스 제약 내에서). 개발자의 경험이 우수해지는 것도 원함. 하지만 이런 꿈을 실현하려면 복잡성 증가라는 대가를 고민해야 함. 이번에 논의 중인 네이티브 템플릿팅이 정말 개발자 경험을 확실히 올려줄지 의문임
          + 하위 호환성을 유지하고 인터페이스를 바꾸지 않는다면 버전 관리가 그 역할을 하는 것 아닌지 질문함
          + 오래되면 우회하거나 패치하는 상황이 반복된다는 주장이지만, 그 과정에서 기반 기능이 한 단계 올라가는 긍정적 효과도 있음. 점진적 업데이트는 유저 요구가 계속 새로운 빈틈과 사용 사례를 발견하더라도 충분히 가치 있음
          + ""API와 ABI는 영원히 안정적이지 않다""는 주장에 동의하지 않음. 예를 들어 getElementById는 25년 넘게 안정적으로 유지되고 있음. 불변의 API가 불가능하다는 건 개인적 체념에 가깝고, 세상에는 수많은 예외가 있음. 수요는 끝이 없으니, 새 API를 추가하면 됨. 작동 중인 API를 깨뜨릴 필요 없음
          + 웹에서는 한 번 공개된 API라면 그 형식을 그대로 평생 의존하는 사용자가 있게 됨. 그래서 20년 전 결정의 여진이 아직도 이어지는 경우가 있음. 예를 들면 ""smooshgate""와 같은 사례에서 확인할 수 있음
     * 리액티브 프로그래밍 유행을 언급하며, 사용자 레벨(system)이 이 영역을 충분히 시도했고, 이제는 다양한 접근법의 장점만 모아 진짜 표준 시스템을 만들 수 있다고 주장함. 하지만 Ryan Carniato/Solid JS 같은 곳에서는 여전히 신호(signals)를 활용한 새로운 가능성을 탐색하고 있어서 아직 탐험이 끝난 건 아니라 생각함. 더 발전할 여지가 충분함
     * 웹에 네이티브 템플릿, 리액티비티, 데이터 바인딩이 정말 필요함. 전세계 수십억 사용자가 React 등 무거운 프레임워크를 다운받고, 파싱하며, 실행하는 데 쓰는 CPU와 대역폭 낭비가 상상 초월임
          + LLM과 암호화 연산의 엄청난 자원 낭비와 비교하면, 이런 낭비는 별거 아니라고 느끼게 됨
          + TC39에서 signals 관련 proposal이 나오고 있는데, 이 덕분에 한 걸음 더 나아가고 있음
          + 실제로 개발에 필요한 건 양방향 데이터 바인딩과 jsx 클론 정도면 충분함
          + React는 템플릿 시스템이 아님
     * 웹 표준에 고수준 템플릿 시스템을 도입하는 논의를 지켜보며, 오히려 먼저 제공해야 할 것은 브라우저 내장 저수준 API라고 생각함. 모두가 표준 템플릿 시스템에 동의하는 건 거의 불가능에 가까움. 반면 브라우저가 DOM에 diff를 적용하는 저수준 API를 제공해주면 됨. 예를 들면 다음과 같은 메소드가 네이티브로 있으면 좋겠음: element.applyDiff(DocumentFragment | string, { method: 'innerHTML' | 'outerHTML' }). 이렇게 diff를 적용하면 현재 element의 포커스, input 값, 오디오/비디오 상태 등을 보존하면서 비파괴적으로 갱신 가능함. Idiomorph 같은 js 라이브러리가 있으나, 네이티브 솔루션은 훨씬 빠를 가능성이 높음
          + 기사에 DOM parts proposal이 링크되어 있음. 이 제안은 저수준 API로 쓸모가 큼. VDOM 기반 프레임워크에는 잘 안 맞을 수 있지만, 그 외 프레임워크에서는 구현을 단순하게 하고, 최적화 여지도 넓어짐. signals proposal까지 적용된다면, 프레임워크 없이도 활용성이 높음
     * 이번 글의 저자는 관련 분야에서 최고 수준의 경험자라고 소개됨. Lit, Polymer, 구글의 web components, 여러 핵심 DOM 스펙에 크게 기여한 인물임
          + 그러나 그 사람이 밀어붙인 수많은 스펙이 오히려 문제를 더 많이 만든다는 비판이 있음. 완전히 다듬어지지 않은 얕은 해결책을 위해 20개 이상의 웹 스펙이 필요하게 만들었고, 이미 유저랜드에서 가능하던 일을 너무 복잡한 방식으로 표준화했다는 시각임. 게다가 15년 전부터 Safari는 선언형 방식의 필요성을 주장했지만 무시당했다고 언급함
     * JSX 방식 대신, Kotlin이 receiver와 builder를 활용해 DSL을 일반화한 것 같은 문법 접근을 원함. 이런 방식은 단순 HTML 템플릿을 넘어 다양한 컴포넌트 계층, 설정(config), 다양한 상황에서 아주 유용해질 수 있음. 템플릿팅과 데이터 바인딩의 실제 의미는 결국 이런 문법적 기능을 활용하는 표준 함수 세트에 불과하며, 이는 Jetpack Compose와 유사함
          + 꼭 필요한 기능은 몇 개뿐임: 반복(loop), 속성 조건부, 노드 조건부 등. 사실 이런 구조만 있으면 언어를 넘나드는 활용도 가능함
     * 선언적(Declarative) 템플릿팅이 jQuery보다 나은지 항상 확신이 생기진 않음. React를 거의 10년간 쓰고 있지만, 내 SPA가 복잡해질수록 DOM의 명령형(imperative) 제어가 그리워짐. DOM의 추상화가 새어나오고(leaky abstraction), 결국 단순한 ""마지막 기록이 이김(last-write-wins)"" 같은 패턴이 오히려 더 명확함. 선언형 템플릿이 이런 문제를 해결한다지만, 컴포넌트 간에 mutable한 상태를 공유하기 시작하면 한계가 너무 빨리 드러남
          + 이 느낌은 React 쪽 개발자가 DOM API 직접 호출을 죄악시해서 생기는 부분도 있음. ref를 적극적으로 쓰거나 id로 컴포넌트를 직접 찾고 조작하는 것도 괜찮음. 실제로 빠르고 리렌더가 적은 form 라이브러리 등은 이렇게 동작함
          + 나는 React를 좋아하진 않지만, 선언형 DOM에서 벗어나 innerHTML, ref 등을 쓰는 데 장벽이 없다고 생각함. 명령형 제어로 가능하지만, 실용적으로 선언형에서 못할 일은 많지 않음. attachShadow()나 showModal()도, 10줄 정도만 추가하면 선언형으로 쉽게 래핑 가능함
     * Records와 Tuples proposal이 진전됐다면, JSX가 해당 구조를 활용해 새로운 의미론을 가질 수 있었을 텐데, 실제로는 그 proposal이 멈춘 수준이 아니라 완전히 철회됨(issue 참고). 대신 composites proposal로 대체됨
     * 이런 고수준 기능 표준화 논의는 중단해야 한다고 생각함. 대신 하위 레벨 특성을 확장해, 상위 라이브러리 구현에 더 가치가 있는 방향으로 집중하는 게 좋음. 표준 제안이 라이브러리보다 분명한 장점이 없으면 의미가 없음. 라이브러리로 최소 5~10년 이상 널리 검증된 경우에만 표준화 논의를 시작해야 한다고 생각함
"
"https://news.hada.io/topic?id=21767","Show GN: 마카오셔틀 : 마카오의 무료 셔틀버스를 이용한 경로 안내 서비스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Show GN: 마카오셔틀 : 마카오의 무료 셔틀버스를 이용한 경로 안내 서비스

    1. 뭐하는 서비스인가요?

     * '마카오셔틀'은 마카오의 호텔들에서 운영중인 무료 셔틀버스 노선들을 이용한 경로 안내 서비스입니다.

    2. 왜 만들었나요?

     * 마카오에는 많은 호텔(카지노)들이 주요 지역을 연결하는 무료 셔틀버스 노선들을 운영합니다. 마카오를 여행할 때 이 정보를 잘 이용하면, 한 푼도 쓰지 않고 원하는 지역으로 이동할 수도 있습니다.
     * 그런데, 무료 셔틀버스 정보가 한 곳에 모여있는 곳을 찾아봐도 없었습니다. 셔틀버스를 이용한 경로를 확인하려면, 네이버와 구글과 호텔 사이트를 오가며 바삐 검색해야했습니다. 검색한 정보가 유효한지 검증하는건 덤이구요.

    3. 그래서 어떻게 쓸 수 있나요?

     * 첫째. 출발지와 목적지를 지도에서 또는 검색해서 선택합니다.
     * 둘째. 요일 및 시간대를 선택하고 검색합니다.
     * 셋째. 선택을 완료함과 동시에 셔틀버스와 도보를 이용하는 상세 경로를 안내받습니다.

    4. 그럼 이건 뭘로 만들었나요?

     * 공통점 : DB 구조 설계 / 백엔드 구현 / 프론트엔드 코드 구현 모두 바이브코딩으로 달성했습니다. 당장 5개국어를 구현하기 위해서라도 AI의 도움이 필요했습니다.
     * 백엔드 : NestJS를 통해 구현했습니다.
     * DB: Supabase를 통해 Postgresql을 사용했는데, pgRouting과 postgis를 애드온으로 사용할 수 있어서 좋았습니다.
     * 프론트엔드 : NextJS를 통해 구현했습니다. 이 부분은 제가 거의 몰라서 제미니/챗GPT/클로드를 번갈아가며 괴롭혔습니다.
     * CI/CD : Github Action을 이용해서 프론트엔드는 Vercel에, 백엔드는 AWS ECS에 자동 배포되도록 했습니다.

    5. 마무리하며

     * 본래 프론트엔드쪽에 무지한, 그렇다고 백엔드를 잘 아는 것도 아닌 개발자가 바이브코딩으로 모두 만들어낸 것입니다.
     * 그렇기에 필요한 것이 있다면, 두려움 없이 만들어낼 수 있다는 것이 바이브코딩의 매력이라 생각하며 만들었습니다.
     * 물론 세세하게 들어가면 결국에는 1인 개발이기 때문에, 부족한 점이 많을 것입니다.
     * 그렇기에 질문이나 피드백은 언제든지 환영합니다!
     * 여기까지 읽어주셔서 정말 감사합니다. 들숨에 클린 코드, 날숨에 PR 승인이 있으시길 기원합니다.

   7월말에 마카오를 가는데 유용하게 사용해 보겠습니다!!

   마카오에서 도움이 되시길 기원합니다!

   와 마카오 여행했을 때 이건게 필요했는데..
   좋은페이지인것 같습니다.

   좋은 말씀 감사합니다!
   저도 필요에 의해 만든거라, 많은 사람들에게 알려졌으면 좋겠네요!

   마카오에서 거주 중 이신가요?

   마카오에 거주하지는 않구요.
   이전에 마카오에 여행갔다가 셔틀버스 정보를 생각보다 어렵게 접하게 되어
   불편하다고 느껴서 만들게 되었습니다.

   셔틀 정보를 지속적으로 업데이트 하는 부분이 쉽지 않을 것 같은데 멋지네요

   셔틀 정보 자체는 호텔의 홈페이지에 나와있는데,
   형태가 다 제각각이라 아직까지는 수동으로 업데이트를 해야할 것 같습니다.
"
"https://news.hada.io/topic?id=21827","래리(Larry, 고양이)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             래리(Larry, 고양이)

     * 래리는 영국 총리 관저의 공식 치프 마우서(Chief Mouser) 로, 2011년부터 임무를 수행 중임
     * 현재까지 총 여섯 명의 총리 재임 기간 동안 거주하면서, 공식 채용된 최초의 치프 마우서임
     * 래리의 임명과 행적은 대중적 주목을 받으며, 영국 내 반려동물 입양 증가에도 긍정적 영향을 미침
     * 여러 정치인 및 동물들과의 관계, 그리고 언론, 소셜 미디어를 통한 인지도 확보가 특징임
     * 래리는 다양한 상훈, 패러디, 문화적 언급을 통해 영국 정치와 대중문화에서 상징적 존재로 자리잡음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

래리: 영국 총리 관저의 치프 마우서

  소개

     * 래리(2007년경 출생)는 영국 총리 관저 10 Downing Street에서 2011년부터 치프 마우서(Chief Mouser to the Cabinet Office) 로 일하고 있음
     * Downing Street 직원들이 돌보며, 어느 총리의 개인소유가 아님
     * 데이비드 캐머런, 테레사 메이, 보리스 존슨, 리즈 트러스, 리시 수낙, 키어 스타머 총리 재임기에 관저에 머물렀음
     * 총리 관저 공식 웹사이트에 따르면 공식적으로 치프 마우서라는 직함을 부여받은 첫 고양이임
     * 2007년경 길고양이로 태어나 Battersea Dogs & Cats Home에 있었고, 2011년 Downing Street 직원들이 입양했으며 원래는 캐머런 총리 자녀의 반려동물로 계획됨
     * 관저 내부에서는 쥐를 잘 잡는 능력과 사냥 본능이 있다고 평가되었음
     * 래리로 인해 2012년 해당 보호소의 고양이 입양률이 15% 증가함
     * 입양 직후 언론에 래리가 잃어버린 고양이라는 소문이 돌기도 했으나, 사실이 아닌 것으로 밝혀짐

  관저에서의 역할과 근무 환경

     * 공식 사이트에서는 래리의 임무를 ""손님 맞이, 보안 점검, 골동품 가구의 낮잠 적합도 테스트""라고 명시함
     * 쥐 문제 해결을 위한 전략이 아직 전술적 계획 단계에 머물러 있다고 전해짐
     * 1929년 이후 전임자들과 달리 래리의 비용은 직원 자발적 모금으로 충당되고, 정부 예산은 사용하지 않음
     * 식비 모금을 위한 퀴즈 나이트 등 다양한 펀드레이징 행사도 있음
     * 데이비드 캐머런 총리는 래리가 공무원 신분이므로 후임자 부임 시에도 관저에 남는다고 설명했음

치프 마우서로서의 활동

     * 도착 한 달 후 래리가 사냥 본능 부족을 보인다는 익명 보도가 있었음
     * 2011년엔 사냥보다 주로 잠을 잔다는 보도와 함께, 여성 고양이 Maisie와도 교류함
     * 2011년 관저의 쥐 문제가 심각해 캐머런 총리가 식사 중 포크를 던지는 해프닝도 있었음
     * 타블로이드에서는 ""게으른 래리"" 라는 별명이 붙기도 했음
     * 2011년 4월 첫 쥐 사냥, 2012년 8월에는 대중 앞에서 쥐를 잡은 모습이 공개됨
     * 2012년 Freya라는 또 다른 치프 마우서가 임명되며 두 고양이가 공존하게 됨
     * 2013년 2주간 4마리의 쥐를 잡는 등 간헐적으로 활약했음
     * 2015년에는 조지 오스본, 맷 행콕이 쥐를 잡으며 래리의 역할을 농담거리로 삼기도 했음
     * 2023~24년 건강 이상설이 있었으나 Downing Street 측은 래리가 건강하다고 발표함
     * 2024년 총선 직전 Ipsos 여론조사에서 래리가 현직 총리, 야당 대표보다 호감도가 더 높음(44%)

정치인들과의 관계

     * 데이비드 캐머런은 래리가 남성을 다소 두려워한다고 했으며, 오바마 대통령만은 예외였다고 언급함
     * 2013년 래리와 캐머런 가족 간 사이가 멀어졌다는 보도가 있었으나, 이후 트위터에서 좋은 사이라고 밝혔음
     * 특직문 도어는 고양이 출입 제한 목적으로 교육부 장관 Nick Clegg이 언급함
     * 캐머런은 래리를 두고 떠나야 하는 아쉬움을 표했고, 테레사 메이 총리 취임 시 래리가 가족을 그리워할 것이라는 우려도 있었음
     * 도널드 트럼프 대통령 방문 시 촬영 현장에 등장하거나, 비에 젖지 않고 관저 입구에서 대기하는 모습도 주목 받았음
     * 2024년에는 우크라이나 대통령 볼로디미르 젤렌스키와 잠시 만남을 가짐

다른 동물들과의 관계

     * 2012년 Freya가 11 Downing Street 입주 후 래리와 빠르게 우호적 관계를 형성했으나 싸움 장면이 목격됨
     * Freya가 더 강인하고 쥐 사냥에 능하다고 평가됐으나, 2014년 Freya가 떠난 뒤 래리가 홀로 치프 마우서 역할을 맡음
     * 또 다른 반려견 Lola는 고양이 친화적인 개로 소개됨
     * 2019년에는 Boris Johnson 총리 부부의 반려견 Dilyn이 관저에 들어온 후 논의가 있었음
     * 2020년에는 비둘기를 잡으려 했으나 실패, 2022년에는 크기가 두 배나 되는 여우를 쫓아내는 모습이 CCTV에 포착됨
     * 2021년부터 리시 수낙 장관이 새 래브라도 Nova를 입양, 이후 Nova와의 ""격한 신경전"" 에서 래리가 승리했다고 전해짐
     * 2024년 Starmer 총리 가족도 구조묘 Jojo 및 새 시베리안 고양이 Prince와 함께 입주했으며, 관계 조율을 위해 자문이 진행됨

  Palmerston와의 경쟁

     * 2016년부터 외무부 고양이 Palmerston가 인근으로 이주하며 둘의 신경전이 시작됨
     * 종종 화해하지만 잦은 몸싸움이 있었으며, 2016년 8월에는 래리가 목걸이를 잃고 Palmerston이 상처를 입을 만큼 격렬한 싸움이 벌어짐
     * Palmerston이 2020년 은퇴 후 시골로 떠나면서 두 고양이의 분쟁이 직접적인 원인이 되지 않았다고 BBC는 전함

수상 및 대중적 인지도

     * 2012년 Battersea Dogs and Cats Home에서 블루 플라크 수여
     * 딱정벌레 Caccothryptus larryi가 그의 이름을 따서 명명됨
     * 임기 2주년을 기념하는 사진 갤러리, The Sunday Express의 주간 만화 소재로도 등장함
     * 2012년에는 Google Street View에서 관저 문 앞에 자는 모습도 포착됨
     * 마카오 The Londoner Macao에 재현된 10 Downing Street 모형 창가에 래리를 본뜬 인형이 전시됨
     * @Number10Cat 패러디 트위터 계정은 영국 및 국제정치에 대한 풍자적 코멘트로 유명해짐
     * 2024년 The Times에서 래리가 새 총리와 반려동물들에 대한 의견을 전하는 패러디 기사도 게재됨
     * Downing Street 기자들도 인기를 끌며, 사진작가 Justin Ng는 래리의 ""보편적인 매력""과 미디어 친화력을 강조함

        Hacker News 의견

     * 약 100년 전으로 거슬러 올라가면서, Chief Mouser to the Cabinet Office의 역대 고양이들을 한 번에 볼 수 있는 위키피디아 링크 모음 소개
       Sybil, Humphrey, Wilberforce, Peta, Peter III, Peter II, Nelson, Munich Mouser, Peter (chief mouser), Rufus of England 등 흥미로운 소개
          + Humphrey란 고양이가 연간 100파운드 정도 예산으로 일했는데, 이 중 대부분이 먹이 비용이었음 안내. 반면, 공식 해충방제업자는 연 4000파운드를 받고도 쥐를 한 마리도 잡지 못했다는 재치 있는 비교
          + Chief Mouser to the Cabinet Office에 관한 위키피디아 페이지에는 타임라인이 잘 정리되어 있고, 각 고양이가 보수당 혹은 노동당 정부 아래 있었는지도 한눈에 볼 수 있어서 마음에 든다는 의견
          + 외무부 건물 근처에 별도의 ""라이벌"" 고양이가 있었다는 점도 귀여움. Palmerston이란 이름으로 위키피디아에 따로 등재되어 있음 Palmerston 소개
          + Chief Mouser 고양이들이 의도적으로 살해된 기록은 전혀 없고, Peter II만이 사고로 세상을 떠난 점이 안심. 거주 중인 지역에서는 집 밖에서 10년 넘게 사는 고양이가 아주 드물다며, 이 고양이들이 유명 인사와 연결되어 더 관심을 받는 점 언급
          + Humphrey는 길에서 방황하던 고양이였는데, 내각 사무국 공무원이 Yes Minister 시리즈의 전형적 공무원 Humphrey Appleby를 기리며 이름 붙였다는 점을 좋아함. 대처 총리가 Yes Minister의 팬이었던 일화도 흥미롭게 얘기
     * 정부 공식 웹페이지에 Larry의 담당 업무가 귀엽게 적혀 있음 공유. Larry는 손님 맞이, 경비 점검, 골동 가구의 낮잠 품질 테스트, 쥐 문제 해결 방안 구상 등 다양한 활동을 한다고 적혀 있음. 아직 쥐 문제는 '전술 기획 단계'라고 재치 있게 서술
       정부 공식 Larry 정보
     * ""Relationships with other animals""라는 제목 아래 'Larry와 Boris Johnson이 2019년에 찍은 사진'을 둔 것이 정말 웃기다는 의견
          + Boris를 동물 취급하는 건가 농담도 하고, 그냥 Boris라는 말로 유쾌하게 반응
     * Larry가 여우를 쫓아내고, 비둘기를 사냥하거나, 해군 소속 팔머스턴 고양이와 싸우는 장면을 모은 1분 21초 컴필레이션 영상을 꼭 볼 만하다는 추천
       Larry 영상
       Palmerston 위키
          + 고양이가 여우에 맞섬. 여우가 이길 것 같았으나 Larry도 절대 밀리지 않는 용맹함에 감탄
          + Palmerston이 정말 보스처럼 보인다는 의견과 함께 대표 사진 링크 [Palmerston 사진](https://en.wikipedia.org/wiki/Palmerston_(cat)#/media/File:FCO_Chief_Mouser_Palmerston.jpg)
          + 비둘기 잡으려다 실패함
     * 'Chief Mouser to the Cabinet Office'라는 직함이 공식적 타이틀임을 강조. 16세기부터 이어져온 전통적 직책이라는 흥미로운 배경 공유
       Chief Mouser 위키
          + 영국 정부가 공식적으로 'Chief Mouser'라는 타이틀을 처음 부여한 고양이는 2011년 Larry가 최초임을 지적
     * 캐나다 의회도 이와 유사하지만 더 야생적인 형태의 고양이 시스템이 있었음 소개
       Parliament Hill cat colony
     * 노르웨이에는 대표 펭귄이 있음 공유
       Nils Olav 위키
          + Larry도 일종의 마스코트로 볼 수 있겠지만, 실질 역할은 다소 차이가 있다고 덧붙임
            군대 마스코트 관련 참고
     * 여론조사 결과, Larry가 Sunak이나 Starmer는 물론이고 Farage보다도 더 높은 호감도, 순호감도를 기록함. Larry야말로 현 상황에서 Farage를 이길 수 있는 유일한 존재일 수 있다고 유쾌하게 언급
     * David Cameron 전 총리가 Larry가 남성에 대해 다소 불안감을 가지는 경향이 있다 밝혔는데, 이는 Larry가 구조 고양이라서 과거에 좋지 못한 경험이 있었던 것일 수 있다고 설명. 오바마 대통령은 예외적으로 Larry가 편하게 대했다고도 전함. 2013년에 Cameron과 Larry 사이 긴장이 고조됐다는 일화까지, '정치인과의 관계' 항목이 재미있다는 추천
     * 영국에 거주한다면 링크를 클릭하지 않아도 누구에 관한 얘기인지 바로 알 수 있는 유명인이라는 점 언급
          + 잠시나마 HN에서 뉴스를 접하는 건가 싶었다는 유쾌한 반응
"
"https://news.hada.io/topic?id=21783","임포스터 신드롬: 내가 왜 스스로를 가짜처럼 느꼈는지 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  임포스터 신드롬: 내가 왜 스스로를 가짜처럼 느꼈는지 [번역글]

  임포스터 신드롬(Impostor Syndrome)

    1. 임포스터 신드롬이란?
          + 높은 성취를 이뤄도 “나는 가짜야”, “언젠가 정체가 들통날 거야”라는 불안과 자기 의심을 반복적으로 느끼는 심리 현상.
    2. 누구에게나 나타날 수 있음
          + 창업가, 아티스트, 과학자 등 다양한 분야의 성공한 인물들도 경험.
          + 예시: 마이크 마이어스, 조디 포스터, 허준이 등도 자신이 자격이 없다고 느꼈던 경험을 고백.
    3. 주요 증상 및 특징
          + 성과나 칭찬을 자신의 실력 대신 운이나 우연으로 돌림.
          + 남들과 비교해 자신이 부족하다고 느낌.
          + 실패는 자신의 무능 탓, 성공은 외부 요인 덕분이라고 해석.
          + “나는 이 일을 잘 못해… 아직”처럼 스스로를 끝없이 의심함.
    4. 원인과 심리적 배경
          + 완벽주의와 자기 의심이 함께 작용.
          + 높은 기준을 스스로에게 적용하며, 완벽하지 않으면 자격이 없다고 판단.
          + 성장 과정에서 받은 칭찬이나 성공 경험이 내면화되지 않음.
    5. 임포스터 신드롬의 양면성
          + 부정적: 자기 의심이 심하면 번아웃, 도피, 자기 혐오로 이어질 수 있음.
          + 긍정적: 자기 의심이 더 배우고 성장하려는 동기로 작용할 수 있음.
    6. 극복 방법 및 실천 팁
          + “아직(not yet)”이라는 말을 붙여 성장의 여지 인정하기.
          + 자신을 호기심 많은 학생으로 받아들이기.
          + 동료들과 어깨를 나란히 할 자격이 있음을 스스로 인정하기.
          + 자기 의심은 성장의 자연스러운 과정임을 받아들이기.
          + 성공한 사람들도 같은 감정을 느꼈다는 사실을 기억하기.
    7. 결론
          + 자기 의심은 흔한 현상이며, 완벽주의라는 불가능한 기준을 내려놓고 계속 도전하기.
          + 성장의 과정에서 임포스터 신드롬을 경험하는 것은 자연스러운 것.

   전세계 해군제독중 1위인 이순신 장군님조차 자기는 범장이라고했습니다.

   너무 걱정하지마세유

   오 그렇군요 감사합니다!

   이게 가끔 다른 사람의 피드백을 엄청 무섭게 만들더라고요. 무언가를 계속 쫒아야만 하는 직업이라서 은근 자기 의심에 잘 빠지게 되는 것 같아요.

   자기 객관화가 자기 의심이 되지 않도록 주의해야겠습니다

   심해지면 대인기피로 발전도 하고요. 그리고 당사자가 속한 집단, 조직의 풍조에 오랜시간 노출된 탓도 있다고 봅니다.
"
"https://news.hada.io/topic?id=21763","애플의 반경쟁적 관행이 개발자와 소비자에 미치는 피해에 맞서 Proton이 소송에 참여함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           애플의 반경쟁적 관행이 개발자와 소비자에 미치는 피해에 맞서 Proton이 소송에 참여함

     * Proton이 미국 캘리포니아 북부 연방지방법원에 Apple의 반경쟁적 관행을 문제 삼는 집단 소송에 합류함
     * 애플의 App Store 정책이 전 세계적으로 불법적이고 반경쟁적이라는 지적과 함께 여러 국가에서 규제와 처벌이 이어지는 중임
     * 애플의 행위가 미국 독점금지법 위반에 해당한다는 점, 그리고 실질적인 정책 개정 필요성을 강조함
     * 이러한 정책 때문에 개인 정보 보호 지향 기업과 민주주의, 자유에 부정적 영향 발생함
     * Proton은 모든 보상을 비영리 단체를 통해 민주주의와 인권 증진에 기여하는 데 사용할 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 소송 참여 배경

     * Proton은 미국 캘리포니아 북부 연방지방법원에 Apple의 반경쟁적 관행에 대항하는 기존의 집단 소송에 합류하기 위한 서류를 제출함
     * 이번 소송은 단순히 Proton 당사자뿐만 아니라 비슷한 상황의 개발자를 대표하는 집단을 위해 진행됨
     * 인터넷의 미래가 자유, 프라이버시, 보안을 보장하기 위해 이번 소송이 필요하다는 점을 강조함

지금 소송에 나서는 이유

     * Apple의 App Store 정책이 오랫동안 다양한 관할 구역에서 반경쟁적이고 불법적이라는 평가를 받았음
          + 유럽연합에서 5억 유로 벌금 처분, 미국 법원에서도 명령 불이행 및 반경쟁적 장벽 설립이 지적됨
          + 영국, 브라질, 네덜란드, 한국 등 여러 국가에서 애플의 행태를 개혁하려는 움직임이 활발함
     * 애플의 이러한 행위가 미국에서는 독점금지법 위반임에도 불구하고, 소송이 없을 시 미국 시장에서는 계속해서 문제의 관행이 허용될 위험성이 있음
     * 최근 애플을 상대로 한 다른 앱 개발자 집단 소송이 진행 중이며, 만약 그 소송에서 합의가 이뤄진다면 전체 개발자에게 법적 구속력이 미칠 수 있음
          + 이에 따라 Proton은 자동적으로 과거 관행에 대해 보상만 받는 결과가 되지 않고, 실질적인 App Store 정책 개정을 이끌어내기 위해 직접 소송에 참여함
     * 소송의 목적은 금전적 보상뿐 아니라, 앱 생태계의 근본적 개선과 시장 경쟁 확대에 있음
          + 만약 Proton이 손해배상금을 받게 될 경우, 전액을 민주주의 및 인권 단체에 기부할 예정임
          + 해당 기부는 Proton Foundation이 관장하며, 공공 이익이 항상 우선시될 수 있도록 함

핵심 쟁점

  애플의 소프트웨어 유통 독점이 초래하는 사회적 문제

     * 애플의 iOS 앱 유통 독점이 소비자, 기업, 사회 전체에 다양한 문제를 야기함
          + 독점의 힘이 결코 남용되지 않도록 법적 규제의 필요성이 대두됨
          + 거대 기술 기업의 독점 남용은 인터넷의 미래와 사회적 가치 전반에 심대한 영향을 줌

    App Store 정책이 프라이버시를 해침

     * 앱스토어 정책은 Meta 및 Google과 같은 감시 자본주의 비즈니스 모델을 우대함
          + 모든 개발자는 연간 99달러 수수료, 인앱 결제의 30% 수수료를 부담해야 함
     * 개인정보 보호 중심 기업들은 구독 모델을 통해 수익 창출 시 이 수수료 부담이 크며, 이는 개인정보 보호 지향 모델의 확산을 억제함
          + 반면 개인정보를 활용한 ""무료"" 서비스 기업은 영향이 적음
     * Apple이 마케팅적으로 프라이버시를 내세우면서도, 실제로는 경쟁을 억제하는 방식으로 정책을 활용
          + 이러한 구조적 모순이 감시 자본주의의 확장에 직접적으로 기여함

    애플 정책이 자유와 민주주의를 훼손함

     * App Store에 대한 완전한 통제는 아이폰 사용자의 앱 접근성을 애플이 직접 좌우하는 결과를 초래함
          + 애플은 보안상의 이유를 내세우지만, 실제로는 국가별로 특정 앱 검열 및 삭제에 동조함
     * 예시로 GreatFire.org의 AppleCensorship 보고서에 따르면, 세계적으로 인기 있는 100개 앱 중 66개가 중국 App Store에서 차단됨
          + 전체 VPN 앱 240개 역시 모두 중국 iOS 사용자에게 제공되지 않음
          + 전 세계 App Store 대비 중국에서는 27%의 앱이 차단되어 있으며, 이는 전 세계 평균의 2배 이상임
          + 상당수가 뉴스, 소셜, 메시징 앱 등 정보 전달 관련 앱인 점에서 검열 목적이 뚜렷함
     * 러시아 정부, 홍콩 시위 시 등에서도 국가 정부의 요청에 따라 VPN 등 특정 앱을 적극 감축함
     * Proton 역시 2020년, App Store 설명에 ""검열된 웹사이트 접속 가능"" 문구를 삭제하지 않으면 판매를 중단하겠다는 애플의 요구를 받음
          + 이는 독점적 유통 구조 하에서 모든 개발자가 이런 정책에 종속될 수밖에 없게 만듦
     * 앱 유통에 대한 독점 해제가 인터넷 미래의 자유와 민주주의 수호에 필수적임을 강조함

    사용자 경험 저하

     * 애플은 구독 관리 및 결제 시스템 통제로 개발자-사용자 직접 관계를 제약함
          + 인앱 결제 외 경로에 대한 언급은 금지, 외부 웹사이트 링크 자체도 불허, FAQ나 고객지원 페이지도 앱 내 링크 금지
          + 예를 들어, Proton 앱에서는 웹사이트에서 결제 시 더 나은 가격 정보를 사용자에게 안내할 수 없음
          + 사용자가 여러 기기에서 구독을 관리하는 것도 불가능함
               o 웹에서 구독을 업그레이드한 사용자는 iOS에서 변경 불가, 반대의 경우도 마찬가지
     * Apple 서비스와의 경쟁 제한을 위해 제3자 앱은 일부 iOS 기능 접근이 불가능하게 설계됨
          + 예: Proton Calendar는 기본 캘린더 앱으로 설정 불가
          + Proton Drive와 같은 제3자 클라우드 서비스는 백그라운드 프로세싱에서 제한이 있음
     * 전체적으로 앱 생태계의 폐쇄성 강화 및 사용 경험 하락, 이용 요금 인상 등 부정적 효과 초래

    앱스토어 수수료가 가격 인플레이션 유발

     * 애플의 30% 수수료는 사실상 인터넷 상거래에 대한 인위적 관세 역할을 하며, 가격 상승의 원인이 됨
          + Epic Games v. Apple 소송 자료에서는 이 수수료에서 Apple이 78%의 이익률을 거두는 것으로 나타남
     * 애플이 이러한 고수수료를 지속할 수 있는 유일한 이유는 iOS 내 유통 및 결제의 완전 독점임
     * 만약 유통과 결제 시스템에 경쟁이 생긴다면, 낮은 비용의 결제 방식 도입이 가능해져 소비자 가격 인하로 이어질 수 있음

이번 소송의 의의

     * Proton이 원하는 제도 개선은 인터넷 생태계의 프라이버시 보호와 민주주의 보장, 공정 시장 조성에 근본적으로 기여할 수 있음
     * 모바일 앱이 인터넷의 핵심 인프라가 된 현시점에서, 앱스토어의 경쟁 촉진과 시장 개방의 중요성이 그 어느 때보다 커짐
     * 이번 소송은 단순히 Proton만이 아니라, 시장 내 모든 앱 개발자와 사용자를 위한 것임을 재확인함
     * 어렵고 긴 싸움이 예상되지만, 인터넷이 사회 전체를 위한 것이라는 신념하에 소송에 임하고 있음
     * 이번 판례가 독점이 아닌 자유로운 시장 선택이 인터넷 미래를 결정하도록 하는 데 기여하길 바람

참고

     * Proton은 Quinn Emanuel Urquhart & Sullivan LLP 및 Cohen Milstein Sellers & Toll PLLC의 변호를 받고 있음
     * 관련 소송 서류 전체는 Proton v. Apple 케이스 내 확인 가능함

        Hacker News 의견

     * 데이터 기반으로 무료 서비스를 제공하며 사용자 프라이버시를 침해하는 기업들은 App Store 결제 시스템을 쓰지 않으니 별다른 영향을 받지 않는 현실에 주목하는 의견. 반면 프라이버시 중심의 서브스크립션 기반 모델은 오히려 수수료 부담이 커 도입 장벽이 높아지는 현상. 이런 구조적 불평등이 단순히 수직 계열화의 문제가 아니라, 시장 전반에 왜곡을 가져오는 방식임을 강조하는 관점 공유
     * 우리는 애플이 권위주의 정권에 협력하는 것도 있지만, iOS 앱 유통에 대한 독점으로 인해 전 세계 개발자 모두가 이른바 ‘왜곡된 정책’에 묶여 동일한 체제로 움직일 수밖에 없는 상황 설명
          + 이런 식의 과격한 표현은 Reddit에서나 통할 법한 느낌이며, 법률적으로는 특별한 실체 없는 주장이라는 지적
     * 모든 개발자는 Safari가 새로운 IE6이 된 것을 알지만 어쩔 도리가 없으니 그냥 원래대로 네이티브 앱을 개발하는 현실. “embrace, extend, extinguish” 전략을 반대로 구현, 웹 표준을 받아들이되 의도적으로 확장하지 않아 경쟁을 줄이는 방식
          + 오히려 Chrome이 새로운 IE6이라는 반론, 실제로 많은 웹사이트가 Safari에서 제대로 동작하지 않는 문제 언급
     * 자유 시장을 위협하는 반(反)자유 발언이 많은 분위기에 대한 언급
     * 대형 테크 기업 중 Apple이 iMessage를 폐쇄적으로 운영하며 고급 기술 이미지를 만들고, Android 유저들을 “민초(民草)”로 취급하며 그룹 채팅에서 배제되는 경험과 화질 저하 경험을 조장, 이는 10대들에게 사회적 수치심까지 부여한다는 점이 가장 치명적이라는 주장. EU가 규제를 예고하자마자 Apple이 RCS를 도입하려 한 점도 짚음. 비싼 하드웨어 구매와 함께 매우 강력한 락인, 그리고 사용자들이 이를 잘 모르게 만드는 방식. Apple이 소셜 네트워크를 인위적으로 통제하는 구조는 궁극적으로 “한 레벨 더 높은 악행”이라는 평가
          + “그렇게까지 악랄한 기업이 있을까”라는 의문과 과장된 주장이라는 반응
          + Blackberry Messenger도 오래동안 독점적 메시징 경험을 제공했던 전례 있다는 상기
          + Apple이 SMS를 나쁘게 만든 게 아니고, 애초에 SMS 자체의 한계. Google이 비(非)Apple 플랫폼에서 메시징을 반복적으로 실패한 것이 더 큰 문제라는 의견
          + 유럽에선 WhatsApp(메타/저커버그)이 대세이며, 그쪽도 일종의 우월 의식이 있다고 경험 공유
          + Apple은 Android 사용자의 체험 품질까지 신경 써야 할 이유가 없고, Android 자체가 더 큰 시장임을 강조하며 Google의 경쟁 실패를 지적
     * Apple을 좋아하지 않는 입장이지만 “악마의 변호인” 역할로, 엄청난 R&D를 투자했다면 생태계를 통제할 권리가 있지 않느냐는 질문 제시. 프리미엄 하드웨어와 사용이 쉬운 소프트웨어(완벽한 유저 경험)가 Apple의 매력이라면 이용약관에 동의해야 하며, 안드로이드 등 다른 선택지도 있다는 주장
          + 생태계를 개방하지 않는 것은 시장과 사회에 불이익을 주며, 단순히 개인 자유가 아니라 시장 전체의 문제임. “투표하듯 다른 제품을 선택”하는 소비자 행동만으로 충분치 않음. Apple은 중개자 역할로 기업과 고객 모두를 잠금. 법과 정치가 개입해야 하는 이유 설명
          + 법은 유연하게 진화하지만 지금은 거대 자본과 기업의 영향으로 상당히 복잡한 시대 흐름. 궁극적으로 법의 목표는 번영, 광범위한 안전, 개인의 자유 셋 중 둘을 고르는 수준. 부의 집중을 막기 위해선 부자와 대기업에 대한 강한 과세·규제가 결국 필요하다는 주장
          + 한 기업이 너무 커져서 독점적 속성을 가지게 되면 시장의 규칙이 바뀌는 점, 그럼에도 애플의 품질 높은 제품에 대해선 동의하는 입장
          + 이 문제는 단순히 소비자가 다른 폰을 사면 되는 논쟁이 아니라, 개발자가 Apple 없이 고객과 직접 거래할 수 있는 권리의 문제임. 독점력의 남용을 어떤 정부도 용납하지 않을 것임을 강조
          + Apple이 모든 앱 결제에 대해 30% 수수료 강제 및 Apple Payments 의무화는 기술적으로 당위가 없는 선택. 이는 Vendor Lock-In 및 Tying(끼워팔기) 형태로 반경쟁적 관행. 2000년대 초반 iTunes/iPod 사태와 비슷한 전례 링크로 제시(Vendor lock-in, Tying)
     * 자신의 아이폰을 차단된 앱 마켓과 보안 결제 시스템 때문에 오히려 선호한다는 입장. 대안 결제 및 앱스토어가 원치 않으며, 타 기업이 법적 수단으로 Apple의 제품 설계를 바꾸려는 시도를 오히려 더 독점적이며 원치 않는 현실이라고 비판. iPhone이 핸드폰 시장을 독점한 것도 아니라는 의견. 원하면 Android를 선택하면 그만이라는 언급
          + Apple Store만 이용하고 싶다면 계속 그렇게 쓰면 되고, 타인의 선택권을 막을 이유도 없음. 진짜 원하는 것은 모든 앱 개발자가 Apple 시스템만을 써주길 강제해 모든 서비스를 원하는 방식으로 소비할 수 있게 하는 것이며, 이는 개발자/사용자의 자유를 박탈하는 구조
          + 논쟁적일 것도 없이, Apple을 고집하는 이에게는 변함이 없고 단지 탈출구의 가능성을 더하는 취지임. Android처럼 자유로운 선택권이 존재할 수 있음
          + 원한다면 본인 선택으로만 락인된 생태계를 고집하면 그만
          + 본인도 마찬가지, 핸드폰은 컴퓨터처럼 해킹하고 싶지 않고 신뢰성과 악성코드 최소화가 제1고려사항
          + 이런 선택이 진정한 ‘선택’은 아님을 지적
     * 미국 내에서 불법 판결까지 받은 Apple의 행태를 멈추게 하기 위해 소송이 필요하다는 아쉬움
          + 바로 그게 제도 설계의 본질이며, Apple의 ‘선’마케팅을 신봉하는 HN의 오류 언급. 모두가 추구하는 단기 목표는 이익 증대이며, 경계와 규제의 필요성 강조
          + 국가마다 법이 다르다는 현실 언급
          + 최근 CEO의 기업윤리 논란에 대한 Image Management 차원이 아닐까하는 의심 제시
     * Apple과 Proton 모두에 호감이 있지만, App Store 폐쇄 정책이 인터넷의 ‘감시 자본주의’로 가는 핵심 원인이라는 주장은 비논리적이라는 입장. 다른 앱스토어나 결제 시스템 허용 여부와 상관없이 Facebook, Google은 어차피 사용자 데이터를 광고에 활용할 것. iOS는 전 세계적으로 볼 때 안드로이드보다 점유율이 낮고, 법적으로도 독점 기준에 해당하지 않는 현실 강조. 플랫폼을 제공한 기업은 그에 대한 통제권이 있어야 하며, ‘닫힌 생태계’ 설계도 허용되어야 한다는 논지. 경쟁사들도 모두 이익에 의해 움직이므로 상황의 본질은 변하지 않음. 대다수 사용자에게 중요한 이슈도 아님
          + “제품을 돈 주고 산 유저가 곧 소유자이며, 결제 이후엔 Apple이 사용 방식을 제한할 근거가 없음. 플랫폼 설계 명분은 실상 강제적 시장介入 논리”라는 반론. 플랫폼을 직접 만들고 그에 대한 보상을 원한다면 공적 지원을 신청해야 하며, 소비자 측에는 부담을 전가하지 않아야 한다는 주장. 시장 규제의 필요성 강조
          + ‘플랫폼 소유 기업이 자유롭게 제약할 권리가 있나’라는 질문에 대해 이미 미국 연방 항소법원이 “아니오”라고 답한 판례(미국 v. Microsoft) 링크로 제시
          + 트래픽이 대형화되어 시장이 될 경우엔 규제 필요성이 생김. 광고형 앱들, 불필요하게 과도한 “광고 제거 결제” 구조 등 서비스 질 저하 문제 지적. 막대한 사용자 기반을 보유한 이상, 책임도 따라올 수밖에 없음. “힘이 있으면 책임도 온다”는 논리
          + 만약 적절한 대체 앱스토어가 있다면 Apple이 30% 수수료를 부과하지 못할 것. 독점 구조이기 때문에 가능하다는 설명
          + Google Play는 플랫폼이고 핸드폰 디바이스 자체는 제한 없음. iPhone은 하드웨어와 플랫폼이 불필요하게 결합된 형태라는 비판
     * “소프트웨어의 소유와 제어는 기업의 권리”라는 논지. 집주인이 집에 누굴 들일지 결정하는 것과 같다는 비유
          + “그럼 집주인이 방문자에게 다리를 자르는 것도 용납?”이라는 반박
          + “내가 구입해 완전한 소유권이 있다면, 그 뒤로 뭘 하든 내 자유”라는 주장
          + 법적으로는 위 논리가 받아들여지지 않으며, 오히려 다행이라고 보는 시각
"
"https://news.hada.io/topic?id=21831"," CPython JIT 컴파일러 2년의 회고: 현주소, 아쉬운 점, 그리고 미래","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               CPython JIT 컴파일러 2년의 회고: 현주소, 아쉬운 점, 그리고 미래

   CPython의 JIT(Just-In-Time) 컴파일러 개발에 2년 이상 참여한 개발자의 솔직한 회고와 분석입니다.
   이 글은 CPython의 JIT 컴파일러, 특히 최적화(optimizer) 부분에 핵심적으로 기여한 개발자가 지난 2~3년간의 경험을 바탕으로 프로젝트의 현황을 공유한 내용입니다.
   긍정적인 점: 커뮤니티 중심의 성장과 교육
     * 커뮤니티 구축: 초기에는 소수의 개발자가 프로젝트를 이끌었지만, 현재는 마이크로소프트의 Faster CPython 팀 해체에도 불구하고 커뮤니티 중심으로 운영되고 있습니다. Savannah, Tomáš, Diego 등 새로운 핵심 기여자들이 합류했으며, 이는 JIT 프로젝트의 장기적인 지속 가능성을 위한 의도적인 노력이었습니다.
     * 교육 가능한 JIT: JIT는 컴파일러 전문 지식이 없는 새로운 기여자들도 쉽게 배우고 기여할 수 있도록 설계되었습니다. 예를 들어, '트레이싱 JIT' 방식을 채택한 것은 복잡한 전체 제어 흐름 그래프가 아닌 단일 기본 블록의 정적 분석만으로도 이해가 가능하게 하여 진입 장벽을 낮췄습니다.
       아쉬운 점: 성능과 잘못된 언론 보도
     * 성능 문제: 솔직하게 말해 CPython 3.13의 JIT는 느립니다. 최신 컴파일러(Clang 20)로 빌드하면 JIT가 인터프리터보다 느린 경우가 많으며, 구형 컴파일러(GCC 11)를 사용해야만 비로소 비슷한 성능을 보입니다. 3.14 버전 역시 새로운 기여자들을 교육하고 기존 타입 분석을 확장하는 데 집중했기 때문에, 획기적인 성능 향상은 없었습니다. 저자는 단기적인 성능 향상보다 장기적인 인재 양성이 더 올바른 선택이었다고 믿습니다.
     * 부정확한 언론 보도: 초기에 언론에서 JIT가 인터프리터보다 ""2-9% 빠르다""고 보도했지만, 이는 사실과 다릅니다. 이 수치는 실제 CPython 인터프리터가 아닌, JIT의 중간 표현(IR)을 실행하는 매우 느린 '티어 2 인터프리터'와 비교한 결과였습니다. 이로 인해 사용자들은 JIT에 대한 잘못된 기대를 갖게 되었고, 실제로는 성능 저하를 겪는 경우가 많아 저자는 큰 좌절감을 느꼈다고 합니다.
       결론 및 미래 전망
       저자는 JIT에 대해 여전히 희망적입니다. 강력한 커뮤니티가 구축되었고, 현재 여러 최적화 작업이 동시에 진행되고 있습니다. 이러한 노력들이 3.14 버전에서 벤치마크 기하 평균 기준 한 자릿수 퍼센트의 속도 향상을 가져올 것으로 기대하고 있습니다.
       저자는 ""만약 정말 '추악한(ugly)' 문제가 있었다면, 더 이상 JIT 작업을 하지 않았을 것""이라고 말하며, 커뮤니티의 힘으로 JIT가 계속 발전할 것이라는 긍정적인 전망으로 글을 마무리합니다.
"
"https://news.hada.io/topic?id=21793","내가 LLM 사용을 줄이기로 한 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          내가 LLM 사용을 줄이기로 한 이유

     * 15년 경력 소프트웨어 엔지니어 Alberto Fortin은 LLM(대규모 언어 모델) 도입에 큰 기대를 가졌지만, 실제 프로덕션 환경에서는 여러 한계를 경험함
     * Go와 ClickHouse 기반 인프라 재구축 과정에서 AI의 허와 실을 체감하고, 모델 개선에도 핵심 문제 해결이 부족함을 느꼈음
     * 생산성 향상에 대한 환상이 존재하며, 실제로는 버그와 예상치 못한 문제가 더 늘어남을 지적
     * LLM을 보조자(assistant) 로 인식하고, 핵심 설계·결정은 개발자 스스로 해야 한다는 교훈을 강조함
     * ""AI는 혁신적이지만 아직 완전하지 않으니, 균형 잡힌 활용과 냉정한 현실 인식이 필요함""
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Alberto Fortin의 LLM 경험과 회고

     * 15년 경력의 소프트웨어 엔지니어 Alberto Fortin은 LLM을 적극적으로 개발 작업에 도입하며 큰 기대감을 가졌음
     * Go와 ClickHouse를 활용하여 인프라를 재구축하는 과정에서 여러 도전과 한계에 부딪히면서, AI와 실제 개발 간의 격차에 대한 블로그 글을 작성하였음
     * 이후 최근 추가 분석을 통해 Claude Opus 4 등 최신 모델이 이러한 문제를 해결했는지 검증하는 작업을 진행

     * Alberto의 경험은, 실무 환경에서 LLM 도입을 고민하는 엔지니어에게 실질적인 교훈과, 어떤 부분에서 도구가 가치를 제공하는지, 또 어디까지가 한계인지에 대한 현실적인 시각을 제시함

LLM 경험에 대한 Alberto의 주요 인용문

     ""오동작이나 기능 미작동만의 문제가 아니었음에 정말 놀랐음. 향후 몇 년간 이 코드를 유지보수하려는 개발자로서, 코드는 충분히 깔끔해야 함.""

     ""문제가 곧 해결될 것 같지만, 결국 다음에 또 새로운 에러가 발생하고, 그걸 해결하는 데 다시 2주 정도가 더 걸리는 경험이 반복되었음.""

     ""오류 출력을 LLM에 전달하면 새로운 답안을 주긴 하지만, 또 뭔가를 복잡하게 꼬아놓거나 다른 부분을 망가뜨리는 문제가 자주 발생함.""

LLM에 대한 과한 기대에 대한 인식

     ""초기 오토컴플리트 등 작은 기능을 처음 쓸 때, 개발자 모두 놀라움을 느낌. 마치 내 생각을 읽는 것 같아, 기대치가 부풀게 되는 현상이 있음.""

     ""개발 생산성이 10배쯤 오를 수 있을 것 같아지는데, 실제로는 그만큼의 기대를 너무 빨리 가지는 경향이 있음.""

역할과 기대의 재정립

     ""가장 큰 차이는 역할 인식의 변화임. 나는 아키텍트이자 시니어 개발자이고, LLM은 내 조수에 불과함. 내가 계획을 세우고, LLM은 보조 역할을 맡음.""

     ""신뢰를 잃게 된 후에는 더 이상 중요한 기능을 맡기지 않음. 리팩토링 등 작은 단위에서만 활용함.""

     ""버그를 직접 수정하기 시작했음. 코드베이스를 완전히 이해하고 있으면 문제를 직접 해결하는 것이 훨씬 빠르고 효율적임.""

LLM을 바라보는 시각의 변화와 조언

     ""시니어 개발자로서 LLM 도입이 잘 맞지 않는 것이 내 역량 부족이 아님을 스스로 인정함. 기존 작업 방식을 지키면서, AI는 보조적으로 활용하는 것이 핵심임.""

     ""기술 발전을 통해 한 단계 발전한 것은 사실이지만, 아직 다음 단계에는 도달하지 못했음. 의사결정과 아키텍처는 여전히 사람이 맡아야 함.""

     ""AI 혁명이 놀랍긴 하지만, 지금은 균형 잡힌, 현실적인 기대치가 필요함.""

     https://news.hada.io/topic?id=20955

        Hacker News 의견

     * HN에 너무 많은 시간을 쓰고 있다는 생각이 드는 이유가, 거의 모든 글과 댓글에서 같은 이야기를 반복해서 보기 때문임. LLM이 흥미롭긴 하지만 생성된 코드는 복잡하고 소유감이 없으며, 직접 작성한 코드처럼 머릿속에 전체 구조가 들어오지 않기 때문에 관리가 어렵다는 이야기. 유지할 생각 없는 짧은 스크립트에는 쓸 만하지만, 대규모 프로젝트에서는 문제가 됨. 또 다른 한편에는 여러 LLM 에이전트를 동원해 다양한 작업을 분산처리하고 병합하는 워크플로우가 멋지다고 말하는 사람들이 있는데, 실제 코드를 보여주진 않고 그냥 자랑만 한다는 느낌을 받음
          + 이 내용 요약이 진짜 핵심을 잘 잡았다고 생각함. LLM이 개발 속도를 엄청나게 올리지만, 내가 전체 코드를 이해하고 있지 않다 보니, 언제 어떤 부분에 문제가 생겼는지 파악하기가 어려움. 결국 새로 들어온 개발자가 자기 프로젝트에 적응하는 느낌임. 그래서 코드를 자주 커밋하고, 주기적으로 LLM에게 코드 설명을 다시 요청함. 이 과정에서 LLM이 직접 자기 버그를 찾아내는 경우도 많음. 주로 데이터 분석처럼 범위가 좁은 작업에는 충분히 관리하면서 빠르게 진행할 수 있음. 반대로 규모가 큰 코드베이스에서 LLM을 제대로 활용할 역량과 습관이 없다면 엉망이 되기 쉬움. 프롬프트 작성, 컨텍스트 관리, 속도 조절, 작업 조직화, LLM 결과물을 정확히 리뷰하는 등은 반드시 익혀야 할 LLM 활용 역량임. 아직 아무도 이걸 정식으로 가르쳐주진 않기 때문에
            다들 시행착오로 익히는 중임. 그래도 한번 맛을 보고 나니 다시 예전으로 돌아가기 힘듦. 귀찮거나 반복적인 일은 LLM에게 맡길 수 있기 때문. 20년 넘게 개발하며 예전만큼 참을성도 없기도 하고, 구현 방법을 잘 모르는 아이디어도 LLM에게 맡기면 훨씬 효율 높게 작업 가능
          + 프로그래밍을 '이론 구축' 과정으로 생각하는 방식이 마음에 듦. Programming as Theory Building 참고. AI 활용 자체엔 반감 없음. 다만, 생성된 코드의 결과물에 대한 책임을 방기하는 태도는 찬성하지 않음. grep 등 도구를 쓸 때처럼, 도구로 사용한 결과 역시 책임지고 다뤄야 의미가 있음. 생성된 코드는 더욱 그렇고, 디스클레이머만으로 끝낼 문제가 아님
          + AI 관련 글에 피로감을 느끼는 거 공감함. 의견이 갈리는 건 사실임. 그래도 실제로 자신들의 코드를 공개하는 예시도 있음. Flask/Jinja2/Sentry를 만든 Armin Ronacher가 유튜브에 워크플로우 영상과 직접 만든 AI 라이브러리 소개를 올렸고, 나도 오픈소스 프로젝트의 절반~80% 정도를 AI로 만들고 있음. cijene-api 참고
          + 사용자 집단이 종 모양 곡선처럼 나뉜다는 느낌임. 한 쪽엔, LLM이 너무 많은 코드를 자기 스타일로 내놓으니 사용자 머릿속에 맥락이 안 들어오고 압도당하는 쪽이 있음. 반면, 원래 혼자선 구현이 불가능하던 사용자에겐 LLM 덕에 무엇이든 만들어보는 길이 열림. 또 한 쪽엔, 직접 천천히 구현이 가능하지만 LLM을 주니어 개발자 군단처럼 활용해서 전체 알고리즘 수준만 기억하는 데 집중하고, 훨씬 더 큰 프로젝트를 빠르게 구성하는 사용자도 있음
          + 25명 넘는 개발자가 동시에 참여하는 대규모 코드베이스에서 일하는 거와 그리 다르지 않다는 생각임. 내 조직은 160명의 엔지니어가 프론트엔드와 미들티어를 담당하는데, 항상 소유감 없는 코드를 뒤져야 하고 git blame에 3년 전 외주 개발자 이름이 뜨기 일쑤임. LLM은 작은 규모엔 괜찮고, 중간 규모에선 안 맞고, 큰 프로젝트에서 작은 모듈로 쓸 때 다시 괜찮은 듯함
     * LLM이 내 목표 달성엔 크게 도움을 주지만, 직접 프로그래밍하는 역량 자체는 떨어지게 만드는 느낌임. 마치 스테로이드처럼 근육은 빨리 커지지만 부작용이 많고, 끊는 순간 한순간에 무너져내리는 상황임. 회사는 건강 대신 빠른 성과에만 관심이 있으니 이런 현상이 더 심함. 적정선을 지키면서 조절해서 쓰기 시작함
          + 스테로이드와 비교하는 게 인상적임. Cal Newport 블로그에서 AI가 우리를 게으르게 만든다는 최근 글이 생각남. 연구자들은 AI 도움 없이 일하는 사람들이 뇌 활동이 더 활발하다는 EEG 데이터도 제시함. 그래도 AI가 모든 영역에서 금지되어야 한다는 건 아님. 이메일 등 비핵심 작업엔 써도 되지만, 진짜 코어 업무에는 삼가는 게 좋다는 주장임. 블로그 참고
     * LLM 덕분에 많은 개발자들이 ""Simple Made Easy""에서 강조된 교훈을 잊어버린 것 같음. LLM이 만드는 코드는 'Ball of Mud'(복잡하고 리팩토링이나 관리가 안 되는 엉망진창 코드) 생산에 탁월함. 복잡한 문제를 잘게 쪼개서, 각각의 작은 컴포넌트가 단순하게 동작하고, 이들이 상호작용해 복잡성을 구현하는 게 진짜 힘임. 각 컴포넌트가 단순하면 이해와 디버깅, 성능 예측이 쉬워짐. LLM이 이 원리를 진짜로 잘 따르게 된다면 그때 진정으로 개발자가 필요 없을 수도 있겠음
          + 사실 LLM에게 원하는 방향을 직접 알려줄 수도 있음. LLM이 쓸모있다고 느끼는 사람과 아닌 사람의 차이는, LLM이 무엇에 강점과 약점이 있는지 파악하고, 입력(프롬프트)에 따라 결과의 퀄리티가 달라질 것을 예측할 줄 아느냐에 달림. 예를 들어, 복잡한 문제 쪼개기 방법을 LLM에게 물어서 내가 놓친 부분이 있는지 확인하고, 그 후에 구체적인 구현을 맡기는 방식을 선호함. 아무런 지시 없이 대형 프로젝트 전체를 생성하라고 하진 않음
          + 'Ball of Mud' 문제는 코드에서만 일어나는 게 아님. 내 직장에서도 ""AI를 적극적으로 수용하자""고 주장하는 리더들이 있는데, 배포 시스템과 복잡한 운영까지 AI로 돌리자는 발상을 봤음. 결국 복잡한 시스템 위에 또 복잡한 블랙박스를 올려 ""블랙박스를 이해하려면 새로운 블랙박스가 필요""하다는 논리인데, 나에겐 말이 안 되는 상황임. 조직 내 강압적인 분위기 때문에 내가 이상한 생각을 하는 듯 느끼게 만들기도 함
          + LLM이 진짜 완벽해진다면 정말 개발자가 필요 없어질까 생각해봄. LLM이 24/7 소프트웨어를 계속 ""생산해내는 기계""처럼 돌아가는 걸 원하는 사람은 아무도 없을 것 같음. 결국은 여전히 문제를 쪼개고, 실제로 필요한 소프트웨어를 찾아서 만드는 사람이 필요할 것임. 오늘날 우리가 그들을 소프트웨어 개발자라고 부르듯이
     * 결국 나도 비슷한 결론에 도달함. LLM은 코드베이스 전체를 오토컴플릿 형태로 채울 때엔 별로임. 무엇이 어디서 뭘 하는지 머릿속에 모델이 사라지는 문제임. 개인화된 StackOverflow처럼 LLM을 키워드 설명, 잘 모르는 개념 요약, 방향 제시 등에만 사용하고, 실제 구현과 의사결정은 스스로 맡는 게 훨씬 효율적이었음
          + 나도 같은 방식으로 쓰지만 cursor는 계속 코드 수정을 집요하게 제안함. 코드베이스의 내용을 건드리지 않으면서 introspect만 하게 하는 비법이 궁금함
          + 내 경우엔 다름. LLM이 제시한 코드를 늘 꼼꼼히 리뷰하면, 무엇이 어디에 있고 어떻게 상호작용하는지 꽤 명확하게 알 수 있음
     * 나도 사용 빈도를 많이 줄이는 중임. LLM 답변이 틀린 경우가 꽤 많았음. 그래서 요즘은 '어떤 매뉴얼 혹은 문서'에서 찾으면 될지 위치만 묻고, 실제 내용은 내가 직접 챙겨봄. 이 방법이 정보의 위치 파악 능력을 기르는데 도움이 되고, 검색엔진과 LLM 의존도를 줄이게 됨. LLM은 도구일 뿐이고, 정확성도 떨어지는 편임
     * 어디에서도 언급되지 않았던 점이 하나 있는데, LLM이 때때로 생산성 저하시킬 수 있다는 부분임. 그럴듯한 답변으로 엉뚱한 길로 끌고 가면 오히려 시간 낭비가 심각함. 전체적으로 보면 도움 되는 경우가 많지만, '근거 확인'이 중요하고, 실제로 종종 직접 구현이 더 빠르기도 함
     * LLM의 한계가 확실함. 매우 강력하지만, 인간만큼 창의적 점프는 힘듦. 예를 들어, ""Android에서 1000번 미만 포트 바인딩이 안 되는데, 웹서버 실행하려면?"" 이라는 질문에 Claude와 Gemini 둘 다 아래 세 가지 상식적인 해법만 내놨음: 1) 리버스 프록시 2) 루팅 3) 포트 번호 올리기. 내가 원했던 진짜 해법은 HTTPS RR 레코드였음(관련 참고). 둘 다 이 스펙 자체는 알지만, 상황에 대입하진 못함. 내가 직접 맥락을 추가해야만 답을 찾음
          + 사실 LLM이 추천하지 못한 것이 이상하진 않다고 봄. Chrome에서도 정식 지원하지 않는 최신 스펙을 추천해주길 기대하기 어렵기도 하고, 나도 그만큼 생각하지 못했을 것임
          + 이 정보는 나도 메모해둠. 최근 LLM과 실제로 대화해보면서, 사람 대하듯 약간은 관용을 베풀면 스트레스가 줄어듦. 예를 들어, cursor로 코드 짤 때 ""여기 누락됐다""라고 지적하면, 나도 같은 실수를 했을 수 있다는 생각이 들기도 함. '북클럽', '영화클럽' 파트너로 LLM을 활용해 영화 두 편을 토론하게 했더니, 캐릭터의 상황을 잘못 말하는 에러도 있었지만, 100% 정확할 필요 없이, 사람과도 똑같이 관용을 베풀면 대화가 훨씬 유연해짐. AI와 대화할 때도 긍정적으로 대하면 좋음
          + SRV 레코드는 들어봤지만 거의 안 쓰이는 것 같았는데, HTTPS RR은 처음 알았음. 실제 지원 상황도 SRV보다 나은 것 같음
          + '정답을 직접 언급해주면 그때서야 솔루션 목록에 추가하는' LLM의 클래식한 문제임
          + 목표와 제약 조건을 충분히 명확히 지정하면 ChatGPT o3가 이 솔루션을 제시해줌. 다만 최신 브라우저에서만 동작한다는 점은 정확히 안내해줌. 참고 예시
     * ChatGPT 엔터프라이즈 버전을 자주 쓰면서, 시간이 지남에 따라 몇 가지를 배움. 대량의 코드를 집어넣지 말고, 아주 독립적인 함수나 작은 클래스 단위로 다뤄야 효과가 좋음. 코드 생성이나 완성 요청의 경우, 10% 정도는 추가 지시로 해결 가능하지만 품질이 떨어지고, 25% 정도는 아무리 설명해도 품질이 안 올라감. 이럴 땐 과감히 무시하고 직접 해결함. 반대로 새로 쓴 코드 리뷰에는 꽤 쓸 만함. 코멘트 중 절반은 쓸모없고, 일부는 애매하지만, 가끔 진짜 중요한 버그나 이슈를 집어준 적도 있음. 여러 페이지를 한 번에 넣지 말고, 쪼개서 넣으며 쓰는 게 좋음. HLSL shader나 SIMD 같이 니치한 영역은 학습 데이터 부족 탓에 답을 거의 못 줌. 전체적으로 코드 품질 개선에 도움 됨. 코드 단위로 잘게 나눠 검증하다 보니, 함수/클래스/모듈 등으로 아키텍처 구조가
       분할되어 전체 품질이 좋아짐
          + 코드 리뷰 용도로 AI를 쓰기 시작했는데 정말 편리함. 함수처럼 범위가 작은 부분에서 큰 도움을 주고, 특히 귀찮은 단위 테스트 작성에 쓸 만함. AI 코딩 보조 퀄리티가 최근 1년 사이에 정말 많이 올라가서, 옛날에 써보고 실망한 사람이라면 다시 시도해볼 만하다고 생각함
     * 장기적인 소프트웨어 개발에서 LLM을 '고급 보일러플레이트 생성기'로 쓰는 게 제일 매력적임. 유지 관리를 신경 쓸 필요 없는 1회성 작업엔 이미 좋지만, 장기 개발에서도 추상화하기 모호한 반복적 코드(예: 테스트 코드)에 정말 유용함. 처음엔 반감이 들었지만 지금은 너무 만족함. 지루하고 반복적인 부분이 '프롬프트 최적화'라는 재미있는 새로운 게임으로 바뀌었고, 프로덕티브도 크게 향상됨. 프롬프트 작성과 코드 리뷰가 단순작업보다 훨씬 빠르게 끝남. 덕분에 머리 써야 할 흥미로운 부분만 남음
     * LLM은 결국 코딩뿐 아니라 여러 작업에서 '유행하는 다이어트'와 비슷한 현상임을 깨달음. 모두가 빠르고 쉬운 해결책을 바라는 마음은 같지만, 결국 왕도는 없음. 편의는 결국 트레이드오프고, 언젠간 모두 그 현실을 받아들이게 됨
"
"https://news.hada.io/topic?id=21714","집에서 만든 CPU와 C 컴파일러로 Xv6 유닉스 계열 OS를 구동한 경험","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               집에서 만든 CPU와 C 컴파일러로 Xv6 유닉스 계열 OS를 구동한 경험

     * 대학시절 프로젝트에서, 직접 설계한 RISC ISA 기반 CPU와 자체 제작한 C 컴파일러를 이용해 Xv6 운영체제 포팅을 시도한 경험 공유
     * 프로젝트는 CPU 설계, C 컴파일러(Ucc) 개발, Xv6 이식 등 모든 요소를 직접 만드는 방식으로 진행
     * 팀은 OS 동작을 위해 인터럽트, 가상 메모리, 캐시 등 핵심 하드웨어·소프트웨어 기능 설계에 도전함
     * Xv6 이식 과정에서 이식성 문제, 디버깅의 어려움, 캐시 버그 등 다양한 난관에 직면했으나 자체적으로 해결함
     * SL, Minesweeper, 2048 등 인터랙티브 애플리케이션과 ray-tracing 프로그램 구동까지 성공하여 교육적·기술적으로 큰 성취를 경험함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * 본 프로젝트는 Tokyo University 정보과학과의 대표적인 실험 과제로, 학생들이 중앙처리장치(CPU)와 하드웨어 설계, 컴파일러 제작, 애플리케이션 실행 경험을 쌓는 데 중점을 둠
     * 프로젝트의 목표는 자체 설계한 CPU의 ISA로 FPGA에 직접 구현하고, 이를 위한 C 툴체인(Ucc) 제작, 그리고 XO6와 같은 운영체제 포팅으로 확대함
     * 실험 과정의 대부분은 자기주도 학습으로 진행됨

CPU 실험과 과제

     * 4~5인 팀이 각자 새로운 CPU 아키텍처 설계 및 FPGA 구현, 그리고 해당 아키텍처용 컴파일러 개발에 참여함
     * OCaml 서브셋 컴파일러 제작 후 레이 트레이싱 프로그램 실행까지가 필수 평가 항목임
     * 추가적으로 시간 여유가 생기면 고유의 도전과제를 설정할 수 있으며, 일부 학생은 CPU 고속화, 멀티코어 개발, 음악/게임 실행 등 확장 실험을 진행함
     * Group 6 팀은 특히 운영체제 포팅을 목표로, 여기서부터 새로운 공동팀(Group X)이 결성됨

Xv6 OS와 이식 도전

     * 이식 대상으로 MIT에서 교육용으로 개발한 Xv6(Unix v6 기반, x86 용) 선택
     * Xv6는 간단한 유닉스 기반(OS)지만 실제 하드웨어 실행을 위한 C89 컴파일러, 특수 인터럽트, 가상 주소 지원, 포터블성 부족 등 다양한 문제가 존재함
     * Xv6는 C 언어 기준 char 1바이트, int 4바이트 등 x86 특성을 전제로 만들어져 있어, 이식 시 많은 문제가 발생함

컴파일러 및 툴체인 개발(Ucc)

     * 기존 실험에서는 OCaml 컴파일러 개발이 표준이었으나, Xv6 구동을 위해서는 C89 컴파일러가 필요해 직접 개발을 결정함
     * 팀원 중 한 명의 C 컴파일러 프로토타입을 기반으로, 새로운 툴체인을 자체 구축(Ucc)
     * 컴파일러 뿐 아니라 Primitive Linker 및 디버그 도구 등도 직접 설계함

CPU 및 시뮬레이터 설계

     * 하드웨어 서술 언어(HDL, 예: Verilog / VHDL)로 CPU 회로 설계 후 Vivado/Quartus 등 논리합성 과정으로 실물 FPGA에 구현함
     * 반복적인 논리합성 과정은 시간이 매우 오래 걸려 실질적으로 많은 대기 시간이 수반됨
     * CPU 기본 기능 이후 인터럽트, MMU, TLB 등 OS 구동에 필요한 하드웨어 지원도 별도 설계
     * GAIA라는 이름의 CPU로 완성됨
     * 시뮬레이터에는 실제 인터럽트, 가상 주소 변환, 디버깅 툴 추가됨

이식 과정의 문제와 해결

     * Xv6의 포팅성 부족으로, CPU 및 컴파일러 사양에 따라 비정상적인 동작 발생
          + 예시: char와 int가 32비트로 정의되면서 포인터 연산 및 스택 구조가 깨지는 문제 등
          + 컴파일러 Ucc에서 char를 8비트로 맞추도록 개선
     * 인터럽트 처리는 고난도 영역으로, 자체 시뮬레이터에 분해기, 상태 덤프 등 디버깅 도구가 추가됨
     * 캐시 알리아스 이슈는 GAIA가 가상 주소로 캐시 인덱스 삼으면서 발생, Page Coloring 기법 도입으로 해결

최종 결과: OS 및 앱 실행

     * 3월 1일 최종적으로 Xv6를 완전히 시뮬레이터와 실 CPU(하드웨어)에서 실행하는 데 성공함
     * 자체 미니 curses, SL 명령, Minesweeper, 2048 등 인터랙티브 앱 구동 성공
          + 특히 2048은 논 캔터노컬(non-canonical) 입력 지원 설계 추가
          + Xv6 수정을 통해 POSIX 스타일의 ioctl, termios 등에 해당하는 기능까지 추가
          + 소형 assembler, mini vi 등도 구현, 실질적 ‘실시간 프로그래밍 환경’ 실현
     * 레이 트레이싱 프로그램도 운영체제 위에서 동작시켜 원래의 실험 목적 이상의 결과 달성

프로젝트의 의의와 후속 사례

     * 본 실험 이후 여러 세대 학생들이 직접 CPU와 OS를 만들어 다양한 실험을 지속적으로 수행함
          + 예를 들어 RISC-V ISA 채택, 자체 OS, Linux 구동 등으로 확장
     * 실습을 통해 직접 하드웨어/소프트웨어 전 스택을 경험하며 알고리듬, 하드웨어-소프트웨어 통합, 저수준 구조에 대한 실질적 이해 증진
     * “바퀴 재발명”이 비효율적이라는 비판도 있으나, 실제로 만들어 보며 배우는 학습효과와 재미는 매우 큼

실제 체험 및 소스 코드

     * GAIA CPU + Xv6 데모에서 직접 실행 가능
     * MIPS 포팅 버전은 이곳에서 오픈소스로 공개됨

결론

     * “직접 만드는 것만큼 배우는 것은 없다”는 교훈과 함께, 하드웨어-소프트웨어 통합 경험의 중요성을 강조함
     * 후속 학생들도 계속해서 새로운 목표에 도전하며, 미래엔 자체 ISA상에서 Linux나 VM이 구동될 수 있기를 기대함
     * 프로젝트 참여 멤버들의 이름을 끝으로 이야기 마무리

   진짜 재밌었겠다.

   대학에서 이런 경험을 한다는게 부럽네요. 재미있을것 같아요..

   좀 더 아래로 내려가서…

   8비트 cpu만들기…?
   https://eater.net/8bit

        Hacker News 의견

     * 예전에 대학 시절 세 명이서 3주간 진행한 그룹 프로젝트 이야기가 생각남. 여러 주제 중 아주 기초적인 운영체제 직접 만들기 등이 있었는데, 교수님들께 Raspberry Pi에 MINIX3를 포팅해도 되는지 물어서 허락받음(MINIX3가 이미 BeagleBoard용 ARM 포트가 있어서 가능할 것 같았음).<br>예상보다 훨씬 난이도가 높았고, 상상도 못했던 기술적 문제들이 쏟아짐. 특히 Raspberry Pi 3이 supervisor 모드 대신 hypervisor 모드로 부팅되는 바람에 고생했고, QEMU의 Raspberry Pi 에뮬레이션 정확도가 너무 형편없어서 os 개발에는 거의 쓸모가 없었음. 해결책을 찾느라 저수준 하드웨어 디버깅을 일주일이나 붙잡음. <br>결국 UART, GPIO, 프레임버퍼 드라이버가 포함된 동작하는 포트를 만들었고, Raspberry Pi 2와 3에서 구동 성공. 실제 하드웨어로 발표 진행, 쉘 스크립트로 램디스크 이미지 출력, GPIO
       핀을 모니터링하여 앞뒤 슬라이드 넘기도록 했고, 칼로 핀을 쇼트시켜 직접 조작함. originality 면에서 압도적으로 멋진 발표였고, 그 SD카드 이미지를 아직도 가지고 있을 것 같음
          + 멋진 경험 같음<br>MINIX3를 Raspberry Pi에 포팅하자고 한 순간, 교수님들이 실패를 예상했을 수도 있을 것 같음<br>QEMU의 Raspberry Pi 에뮬레이션 정확도가 별로일 때, 나는 QEMU에서 먼저 OS가 동작하게 만든 후 하드웨어에서 운에 맡기는 전략을 썼음. 그래도 괜찮았음<br>실제 Raspberry Pi에서 디버깅은 어떻게 했는지 궁금함
          + GPIO 쇼트시킬 때 칼을 사용했다고 들으니 ATX 메인보드에서 전원 스위치 없이 부팅하려고 두 전원 핀을 쇼트시킨 경험이 떠오름<br>그래도 너의 셋업이 훨씬 멋짐. 잘했음
     * SFU에서 25~30년 전에 비슷한 일을 했었음. OS와 컴파일러를 올리는 건 안 했고 팀 프로젝트도 아니었음<br>이런 실험에 흥미가 있다면, 가이드와 접근성 높은 툴링이 있는 Turing Complete 추천. 몇 개의 게이트에서 진짜 컴퓨터까지 만들어갈 수 있음. 커뮤니티와 컴포넌트도 공유 가능, RiscV 코어도 있음. 진짜 재미있으니 Steam에서 해보는 것 추천함<br>Turing Complete Steam 링크
          + 이거 보니까 예전에 재미있게 했던 nand2tetris의 게임 버전 같음. 이것도 해볼 만한 추천임
     * 이 글 보니 왠지 비슷한 학술(?) 프로젝트가 떠오름. 최소한 커스텀 C 컴파일러와 커스텀 OS가 있었던 것으로 기억함. 이름은 정확히 기억 안 남
     * 예전에 올라온 관련 토픽 참고 이전 글 링크
     * 직접 CPU + 컴파일러 + OS를 만들어갈 때는 밑에 깔린 플랫폼이 아예 없음. 내가 곧 플랫폼임.<br>버그가 곧 시스템의 법칙임. 보통은 남이 만든 추상화 레이어에서 디버깅하는데, 여기선 그 규칙조차 내가 직접 정의함. OP는 자기만의 규칙을 직접 디버깅한 셈임
     * 진짜 인상적임. 로우엔드 영역 작업은 보통 지루하고 시간도 많이 들고, 특히 디버거 같은 필수 도구가 없으면 더더욱 힘듦
          + 스코프로 이상한 kprintf 디버깅해본 경험이 없다면 아직 진정한 로우레벨 맛을 못 본 것임
     * Magic-1과 BMOW도 예전에 유사한 일을 했었음<br>자세한 내용은 homebrewcpu.com 참고<br>직접 CPU 만든 사이트 리스트는 homebrewcpuring.org 참고
     * 이제는 FPGA에 구현하는 대신 반도체 연구실로 달려가서 직접 CPU를 제작해달라고 해야 할 시점임
"
"https://news.hada.io/topic?id=21795","화웨이, Ascend GPU에서 학습된 오픈 웨이트 모델 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   화웨이, Ascend GPU에서 학습된 오픈 웨이트 모델 공개

     * Pangu Pro MoE는 화웨이 Ascend NPU 환경에 최적화된 Mixture of Grouped Experts(MoGE) 아키텍처를 도입하여, 분산 환경에서 전문가별 부하 불균형 문제를 효과적으로 해결함
     * 이 모델은 총 720억 매개변수 규모로 설계되며, 토큰별 160억 개 파라미터만 활성화해 계산 효율성과 확장성을 대폭 높임
     * MoGE 아키텍처는 서로 다른 전문가 그룹별로 균등 분배 및 선택 규칙을 적용해 모든 디바이스에 완벽한 부하 균형을 달성, 추론 및 학습 속도 개선
     * 성능 평가 결과, Pangu Pro MoE는 GLM-Z1-32B, Qwen3-32B 등 주요 오픈소스 모델을 앞지르며, Ascend 300I Duo/800I A2 플랫폼에서 최고 수준 추론 효율 및 비용 대비 성능을 보임
     * 사전학습과 미세조정, 강화학습 등을 통해 고품질 데이터셋 기반의 다양한 도메인에 대한 강한 추론 및 일반화 능력을 확보함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 최근 대규모 언어 모델(LLM)에서 Mixture of Experts(MoE) 방식이 채택되며, 계산 비용 상승 없이 모델 파라미터와 학습 용량을 높이는 트렌드가 대두됨
     * MoE 구조는 각 입력 토큰마다 일부 전문가(Expert)만 활성화하여 연산을 줄이지만, 현실적으로 몇몇 전문가만 반복적으로 선택되는 전문가 부하 불균형 현상이 존재함
     * 이러한 문제는 모델이 여러 디바이스에 분산됐을 때 전체 시스템 효율성을 떨어뜨림
     * 기존의 부하 균형 휴리스틱 기법은 부분 개선에 그치며, 완전한 근본적 해결책은 아님

Mixture of Grouped Experts(MoGE) 아키텍처 소개

     * MoGE는 전문가(Expert)를 동일 크기의 그룹(Group)으로 나누고, 토큰별로 각 그룹에서 지정된 수의 전문가만을 반드시 활성화하는 그룹 균형 라우팅 전략을 구현함
     * 이를 통해 모든 디바이스에 고르게 작업이 분배되며, Imbalance Score(IS)가 0으로 설계상 항상 완벽한 로드 균형을 달성함
     * 각 토큰에 대해 모든 전문가의 초기 점수를 글로벌 Softmax 라우터로 산출한 뒤, 각 그룹별로 Top-K′ 전문가만 선택, 미선택 전문가의 점수는 0이 됨
     * 이런 구조는 특히 분산 환경의 대규모 모델(수십~백억 파라미터) 에서 추론 및 학습 속도와 자원 활용도를 극대화함

Pangu Pro MoE 모델 및 Ascend 플랫폼 최적화

     * 화웨이는 Ascend 300I Duo, 800I A2 NPU 플랫폼에 최적화된 Pangu Pro MoE(총 719억 파라미터, 토큰별 165억 활성화) 를 개발
     * extensive한 시스템 시뮬레이션을 바탕으로, 모델 구조 및 하드웨어(HW) 병렬화 구성(텐서/전문가/파이프라인/가상 파이프라인 병렬성) 등 각종 하드웨어 파라미터를 조정해 성능을 극대화함
     * 커스텀 연산 커널(MulAttention, SwiftGMM 등)은 Ascend 특성에 맞춰 최적화되어, 메모리 접근, 통신 및 연산 오버헤드를 줄이고, 오퍼레이터별 병목현상 제거 및 대역폭 활용률을 향상시킴
     * 배치 크기와 성능 최적화 시뮬레이션 결과, 제약 조건 내에서 최고의 Throughput, 최소 Latency, 최적 통신 효율 등을 보임

사전학습 및 학습 데이터 설계

  데이터 구성

     * 자체 토크나이저 기반 1.3경(13조) 토큰으로 이뤄진 대규모 고품질 데이터셋을 구축, 웹, 책, 코드, STEM, 산업, 추론, 합성 데이터 등 다양한 소스에서 수집
     * 토크나이저는 도메인 균형을 중시하는 전략을 사용해 특수 도메인까지 대표성을 갖춤

  학습 단계 및 전략

     * 사전학습(Pre-training) 은 세 단계(일반, 추론, annealing)로 진행, 각 단계마다 학습 대상과 데이터 커리큘럼 조정
          + 일반 단계: 다양한 분야의 일반 지식과 언어 능력 습득
          + 추론 단계: STEM·코딩·복잡 논리문제 등 복잡 추론 데이터 비중 극대화
          + annealing 단계: 고난이도 데이터와 인스트럭션 스타일 데이터로 추가 튜닝
     * 각 단계는 점차적으로 시퀀스 길이, 데이터 난이도, 배치 크기, 학습률 등을 조정해 모델의 일반화 및 특화 능력 확보

  데이터 평가

     * Pangu 자체 시리즈 모델을 이용한 모델 기반 다중 도메인 평가 시스템을 운영, 데이터마다 청결도/유창성/교육적 가치/풍부성 등 주요 점수를 할당하여, 데이터 샘플링 및 선정 전략에 반영
     * 전체 188개 카테고리의 정교한 라벨링을 시행하여 데이터 분포와 특성을 관리

사전학습 환경과 최적화

     * Pangu Pro MoE는 Huawei Ascend 800T A2에서 학습 및 평가, 이 칩은 FP16 256TFlops, INT8 512TOPS 효율, 310W 초저전력으로 높은 AI 성능 및 비용효율을 달성
     * 단일 에폭 학습, AdamW 옵티마이저, 3 단계 코사인 러닝레이트 스케줄, 대형 배치 구성 등으로 튼튼한 일반화와 목표 태스크별 특화 가능

포스트 트레이닝(후속 미세조정 및 강화학습)

  Supervised Fine-tuning(SFT)

     * SFT 데이터는 '추론'과 '비추론' 두 세트로 구분, 추론 비중을 3:1로 높여 수학/코드/논리 추론 등 복잡 작업에 집중
     * 2단계 점진적 최적화 전략(다양한 범위에서 간단 지시→복잡 추론으로 점차 진화)을 통해, 모델의 단계적 추론력 및 일반 언어 처리력 균형 있게 성장
     * SFT 과정 중 체크포인트 병합 전략을 더해, 다양한 단계 중간 지점의 모델을 효과적으로 통합해 견고성과 일반화 강화

  강화학습(RL)

     * 보상 기반의 RL 단계는 Group Relative Policy Optimization(GRPO) 알고리듬과, 보상 신호가 없는 샘플을 무시하는 Zero-Advantage-Mask 기법을 함께 적용해 정책 탐색과 효과적 학습을 지원
     * 정확성, 선호, 보조 보상 등 다층 보상 시스템을 도입, 수학 및 코딩 작업은 자동화된 평가 체계, 오픈도메인 과제는 별도의 LLM 기반 판정자(Preference Model)로 평가
     * 커리큘럼 데이터 믹싱으로 데이터 복잡도 분포를 동적으로 조정, 모델 성장에 연속적 자극 제공

시스템 및 인프라 최적화

  Ascend NPU 학습 시스템

     * 계층적·복합 병렬화(Hierarchical & Hybrid Parallelism) 전략과, EP All-to-All 통신, Adaptive Pipeline Overlap, 오퍼레이터 퓨전 등 첨단 기술을 적극 적용
     * 모델 연산 효율(MFU) 35% 향상 및 파이프라인·가상 파이프라인 병렬로 단계별 연산, 통신 부하를 완벽히 분산해 확장성과 처리량 모두 강화
     * 커스텀 커널, HBM 대역폭 활용 극대화, 불필요한 통신 및 메모리 오버헤드 삭제 등으로 모델 훈련·추론 모든 단계에서 성능을 최대치로 이끌어냄
     * Inference 시스템 역시 Attention, Expert 등 모듈별 유연한 병렬 구성(H2P 전략)과 맞춤형 연산자 최적화로 각 하드웨어 구조에 맞는 최고의 Throughput과 Latency 결과를 보임

성능 및 벤치마크

     * Pangu Pro MoE는 Ascend 환경에서 1148~1528 token/s(카드당)* 추론 성능, 동급 파라미터(Open dense 32B, 72B) 대비 압도적 결과 입증
     * Cost-to-Performance 면에서도 Ascend 300I Duo 기반에서 탁월한 효율 달성
     * 다양한 외부 벤치마크(의사결정, 논리, 코딩, 문서 이해 등)에서 GLM-Z1-32B, Qwen3-32B, Gemma3-27B 등 대형 공개모델을 초과 성능
     * 서브 100B 파라미터급에서 최고 수준 LLM임을 실험적으로 보여줌

결론 및 시사점

     * Pangu Pro MoE는 대규모 모델 분산 학습/추론에서 전문가 그룹 균형화 설계를 통해 부하 불균형 문제를 근본적으로 해결함
     * Ascend 전용 플랫폼 최적화와, 데이터 품질 극대화 등 전방위적 노력으로 비용·속도·일반화 성능의 고차원적 균형을 달성한 차세대 대형 언어 모델임
     * 해당 구조와 방법론은 추후 대규모 분산 LLM 생태계 및 다양한 산업 적용에서 중요한 참고/기준이 될 것으로 기대됨

        Hacker News 의견

     * 이번 아키텍처 발표가 정말 기대되는 이유, 저렴한 GPU만으로도 소규모 개발자들이 대형 업체들과 경쟁할 수 있게 될 가능성 때문임. 결국 크라우드소싱 방식의 오픈 AI 개발이 기술적으로 실현 가능하다는 점을 시사함. 실제 중국이 이 부분을 연구 중이고, 모놀리식 모델들과 경쟁할만한 수준을 목표로 하고 있음. 미국의 제재가 처음에는 회의적이었지만, 만약 이게 논리적으로 완전히 실현된다면 정말 대단한 성과로 볼 수 있음
          + 제재가 (비꼬는 의미 없이) 실제로 여러 면에서 전 세계를 더 나은 방향으로 이끌 수 있다고 생각함. 컴퓨팅 다각화, 제조 분산화 등 다양한 개선 효과 촉진
          + Deepseek-R1은 이미 GPT 4.1과 비슷한 수준임. 오픈웨이트, 오픈소스 형태로 제공되고, 추론 코드까지도 오픈소스로 공개됨
          + SETI@Home처럼 피어투피어 오픈 GPU 트레이닝 네트워크에 나 또한 관심 있음
          + 크라우드소싱 오픈 AI가 기술적으로 가능하냐는 질문에 이미 가능하다는 사례로 PrimeIntellect.ai의 Intellect-2 링크 공유
          + 흥미로운 발전이라는 생각을 함. 하지만 AI 기술이 인간 생존에 실존적 위협을 줄지에 따라 이게 좋은 일인지 결정된다고 봄. 지나치게 과장된 것처럼 들릴 수 있지만 실제로 이에 대해 매우 심각하게 고민하는 사람들이 많음
     * 라이선스가 EU 내 사용·설치를 금지하고 있는데, ""이 제한은 보호 차원일 뿐 실제로 집행하지 않을 것""이라는 식의 조항을 둔 라이선스 작성이 가능한지 궁금함. 이런 표현을 ‘분리 조항(isolating clause)’이라고 부를 수 있을 것 같은데, 판사가 이를 법적 우회로 받아들일 수 있을지 확실치 않음. 메타가 llama 웨이트 공개할 때와 비슷한 맥락인 듯함. 유럽 AI 법안의 본질은 AI의 구체적 사용을 통제하는 것이라고 생각하고, 단순히 웨이트와 아키텍처를 배포하는 건 포함되지 않을 것 같음. 배포 금지가 실질적으로 유럽인에게 더 많은 선택지와 경쟁을 제공할 텐데, 정말로 법적으로 금지되는 것인지 궁금함. 한편, 오픈 웨이트를 설치한다면 백도어(특정 프롬프트를 통해 시스템을 조작할 수 있는 취약성)가 생길 수 있다는 점에서 보안상 주의도 필요하다고
       생각함. 관련 논문에서 '0?,#2!' 같은 심볼 조합이 LLM의 숨겨진 정보를 누군가 읽을 수 있는 상태(프롬프트 인젝션)로 만들 수 있다는 사례를 본 기억이 있음. 이런 공격을 파인튜닝이나 Lora로 막을 수 있거나 약화시킬 수 있는지, 혹은 방어에 유용한 파이썬 라이브러리가 있는지도 궁금함. 다운받고, 설치하고, 파인튜닝이나 로라로 수정하면 보호를 받을 수 있을지 질문함
          + Huawei가 EU 시민의 행동을 통제할 권리는 없고, 사실 이런 제한을 굳이 넣을 필요가 없었다고 봄. EU 시민으로서 직접 법을 숙지하고, 위험한 모델은 피하는 것이 스스로를 보호하는 길이라는 생각
          + 보안 측면에서 LLM이 만드는 모든 코드는 신뢰하지 말고 반드시 검토가 필요함
          + ‘분리 조항’과 유사한 조건으로 Alliance for Open Media의 코덱 라이선스를 예로 들어 설명. 해당 코덱은 로열티 프리지만, 라이선스 조건상 포맷 사용 관련 법적 소송 제기 시 사용 권리가 철회됨
     * 웨이트는 gitcode에서 받을 수 있음
          + 단, 라이선스에 따르면 EU 내 접근, 다운로드, 설치, 실행, 배포, 통합, 수정, 그 외 사용이 모두 명시적으로 차단됨. 관련 라이선스 링크는 여기
          + ‘오픈소스’라는 용어 대신 ‘오픈웨이트’라는 정교한 표현이 적용된 점은 좋음. 그런데 오픈웨이트가 정말 흥미로운 것인지 궁금함. 이는 해당 모델의 편향(혹은 편향 없음)을 알 수 있는지? 경쟁 모델 학습에 활용이 가능한지? 오픈소스와 오픈웨이트의 차이나 장단점이 궁금하고, LLM 시대에 ‘웨이트가 곧 소스’라는 말이 타당한지도 배우고 싶은 상황
     * 현재 LLM이 확장성의 한계(scaling wall)에 부딪히고 앞으로는 효율성이 승부처가 된다면, 특정 사용 사례에 초점을 맞춘 소형 모델 시장이 생길지 궁금함. 실제로 Gemini로 이미지에서 구조화 데이터를 추출하는데 flash 모델이 아주 효과적임. NUC와 AMD APU 같은 소형 장치에서 특정 용도만 충족하는 경량 모델을 만드는 데 얼마나 노력이 들지 궁금함. 또는 특정 사용 사례용 미니 외장 GPU 스틱 같은 기기가 나타날 가능성도 흥미로움. 상업적으로는 큰 시장이 아닐 수 있지만 꽤 멋질 것 같음
          + '특정 사용 사례에 초점을 맞춘 소형 모델' 주제는 이미 HN에서 중요한 논의임: ""Small language models are the future of agentic AI""
          + 명명된 엔터티 추출용 모델 찾다가 dslim/bert-base-NER 모델을 알게 됨. 파라미터는 1억 800만개임
          + 이런 방향은 이미 ‘모델 디스틸레이션’(대형 LLM이 만든 레이블, 전용 작은 모델이 1000배 저렴하게 추론)이라는 이름으로 있음
          + 예전 USB 포트에 ASIC 꽂고 비트코인 채굴하던 시기 생각나는 상황
     * 제재는 임시방편에 불과하다고 생각함. 이상적으로는 국내 역량 강화를 위한 시간 벌기 용도지만, 오히려 미국 내 연구비 삭감과 외국인 유학생·연구자 유입 억제로 인해 미국 역량이 고갈되는 반면, 중국이 성장 궤도에 오르게 될 것
     * 이 뉴스 며칠 지난 소식임. 텐센트의 하이브리드 AI 모델 오픈소스 소식은 TechInAsia 참고, GitHub 링크도 공유
     * Sic transit gloria nvidii(엔비디아의 영광도 이렇게 스쳐 지나감)
          + 언어학 배경지식: 'invidia'는 라틴어로 ‘질투’ 의미
          + 5년 간의 라틴어 공부가 빛을 발하는 순간임
          + 작은 지적질이지만, 단수 생격은 nvidiae가 맞음. i는 o 변화형 어미임
          + Sic transit gloria nvidiae가 맞는 형태임
          + 오늘 읽은 글 중 최고, 브라보라는 감상
     * 중국인 친구가 있음. 6년 전 중국 HW 스타트업 창업 엔지니어로 합류함. 그 후 제재가 찾아옴. 최근 만남에서 친구가 제재가 인생 최고의 사건이었다는 말을 함. 중국 현지업체들이 서방국 원산 제품을 아예 안 사게 되어 자사 회사가 크게 성장함. 이제 제재는 모두가 ‘당연한 것’으로 받아들이기 때문에, 그 결과는 오로지 자력갱생과 성장 집중임
          + 산업별 영향 차이가 존재함. 제재 전에는 Synopsys 같은 대형 EDA 기업에 비해 엄청난 기술 격차로 간신히 버티던 소형 EDA 소프트웨어 업체들이, 이제는 위험 회피 수요로 새로운 고객을 대거 확보하고 있음. 이를 ‘호르메시스(hormesis)’라고 부름
     * 제재 이후 어떤 의미가 있는 건지 궁금함. 제재로 큰 타격을 입은 Huawei 스마트폰이 이제 자체 GPU로 반격의 기회를 얻는 것인지? 이런 GPU의 성능이 서방 최신 GPU와 비교하면 어떤지? 이제 Huawei가 이 GPU를 상용화할 역량이 있다는 의미인지 궁금함
          + Huawei는 생각보다 엄청난 기업임. 5G 기지국뿐 아니라 핸드폰, 전기차까지 다양한 분야에서 거대함
          + “반격의 기회”라는 표현에 대해, Huawei의 스마트폰 판매는 이미 중국 내에서 애플을 앞질렀음. 상세 시장 점유율 데이터 링크
          + 하드웨어적으로는 Huawei가 충분히 경쟁력 있는 폰을 만들 수 있음. 다만 구글 앱스토어가 없는 상황에서 구매의 타당성 설명이 어려움
          + Huawei의 전반적 상황을 설명하는 유튜브 영상 링크 참고 추천
     * TSMC와 Nvidia에 견줄 수 있도록, Node Size 측면에서 Huawei와 중국의 경쟁 구도가 형성되는 것이 전 세계적으로 필요함
          + 지정학적 이슈를 고려하지 않으면 아주 좋은 시나리오임. 하지만 AI 기술은 양날의 검이고, 소비자 영역의 경쟁은 곧 군비 경쟁으로 이어질 가능성이 높음. 중국의 제조력, 인건비 등을 감안하면 결국 중국이 승리할 가능성이 높음. 다만, 이를 실현하려면 우선 ASML 중복 생산이 필요한데 이는 가까운 미래엔 현실적이지 않을 것
          + 만약 미국이 칩 수출 규제를 푼다면, 중국 정부가 오히려 수입 규제를 걸 가능성이 높다는 인상임. Nvidia/TSMC/Apple/Google에 맞설 진짜 경쟁자를 키움으로써 얻는 이익이 월등히 큼
"
"https://news.hada.io/topic?id=21741","미국 핵심 위성 데이터 손실로 허리케인 예측이 수십 년 전 수준으로 후퇴할 위기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              미국 핵심 위성 데이터 손실로 허리케인 예측이 수십 년 전 수준으로 후퇴할 위기

     * 미국의 핵심 대기 데이터 수집 프로그램이 곧 중단될 예정으로, 허리케인 예측 능력이 심각하게 후퇴할 가능성 있음
     * Defense Meteorological Satellite Program(DMSP) 의 데이터 제공이 이번 주를 끝으로 중단되며, 대체 가능한 미국 프로그램이 없는 상황임
     * DMSP 위성은 북극 해빙 및 허리케인 발달 관측에 필수적 역할을 해 왔으며, 갑작스러운 중단이 예보 정확성에 직접적 타격을 줄 것임
     * 과학자들은 이러한 중요한 데이터의 손실이 “과학의 체계적 파괴”이자, 미국 국민의 생명과 재산에 심각한 위험을 초래할 수 있음을 지적함
     * 해당 조치 배경에 대한 명확한 설명 없이 급작스런 서비스 중단이 이루어졌으며, 향후 영향이 매우 클 것으로 전망됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 위성 데이터 중단이 허리케인 예측력에 미치는 영향

  주요 서비스 중단 발표

     * 미국 국가해양대기청(NOAA)이 최근 공지한 바에 따르면, Defense Meteorological Satellite Program(DMSP) 데이터 수집, 처리, 배포가 2025년 6월 30일 이전에 완전 중단될 예정임
     * NOAA의 발표에 따르면 “최근 서비스 변경”으로 인해 이런 결정이 내려졌으며, 세부 사유나 주도 기관은 즉각적으로 명확하게 밝혀지지 않음

  DMSP의 중요성과 대체 불가능성

     * DMSP 위성은 전 세계를 하루에 두 번, 매우 높은 해상도로 관측할 수 있어, 북극 해빙과 허리케인 발생 감시에 매우 중요한 역할을 해 옴
     * 이 프로그램은 NOAA와 협력해 공공 데이터를 제공하고 있었으며, 미국 내에서 이 데이터를 대체할 수 있는 다른 공개 프로그램은 없는 상황임
     * 허리케인 연구자들은 DMSP 위성이 “허리케인 내부구조”를 파악할 수 있는 유일한 수단이라고 설명함
     * 위성 없이 과거에는 아침에 일어나서야 허리케인 강도 변화를 알 수 있을 정도로 실시간 정보 부족이 심각했음을 기억함

  극지방 및 해빙 모니터링 영향

     * DMSP는 40년 이상 극지방 해빙 변화를 연속적으로 추적해 온 유일한 데이터임
     * 남극 연구에 참여한 연구자들은 “이러한 데이터 손실이 극지 시스템 관측의 '눈을 멀게 하는' 수준”이라고 평가함

  중단 사유 및 행정적 배경

     * 위성 자체에는 물리적 결함이 없고 여전히 정상 운용 중인 상황에서, 데이터의 갑작스런 중단은 의도적 결정 가능성이 높음
     * 이번 해 NOAA 및 기상청은 인력난과 Trump 행정부의 강도 높은 연방 환경 프로그램 감축 정책에 직면해 있음
     * NOAA 내부 연구자들은 이번 결정이 ""과학의 체계적 파괴""임을 언급함

  허리케인, 기후관측에 미치는 구체적 영향

     * 갑작스러운 DMSP 데이터 중단은 미국의 허리케인 예보와 해빙 감시 능력을 즉각적으로 저하시킬 전망임
     * 현재 허리케인 시즌이 평년보다 활발할 것으로 예측되는 가운데, 정보 손실은 더욱 위험성이 큼
     * 빠른 강도 변화(급격한 발달) 예측이나, 전통적인 storm hunter 투입 없이 폭풍의 강도를 측정하는 능력도 크게 떨어질 위험이 있음
     * 수십 년간 핵심 역할을 해 온 DMSP의 영구적 중단은 ""무의미하고, 무책임하며, 수천만 명의 목숨을 위협""할 수 있음을 지적함

  DMSP 프로그램의 역사와 국제 대체 가능성

     * DMSP는 1963년 미국 국방부가 고해상도 운무 예측을 위해 시작한, 연방정부 최장수 기상 위성 프로그램임
     * 최근 몇 년 동안 프로그램은 예산 축소와 우선순위 저하로 어려움을 겪어 왔음
     * 일본이 유사한 위성 관측 능력을 갖추고 있으나, 완벽한 대체를 위해 데이터 전환엔 시간이 소요됨

  NOAA 및 국방부의 공식 입장

     * NOAA는 DMSP 데이터가 폭넓은 예보 및 모델링 툴 중 하나에 불과하며, 나머지 데이터 자원으로 “충분한 고급 예보 서비스를 제공할 수 있다”고 발표함
     * 그러나 NOAA 내 일부 연구자들은 DMSP의 해상도 높은 데이터는 다른 도구로 대체할 수 없는 수준임을 확인함
     * 미 우주군 관계자는 여전히 DMSP 위성과 기기가 정상 작동하고 있으며, 데이터는 미 해군 FNMOC를 경유해 배포된다고 밝힘
     * FNMOC의 데이터 처리 변경에 따라 공공 데이터 제공이 중단되는 점에 대해 자세한 해명은 아직 없음

  향후 전망

     * NOAA 등 기존 예보 기관들은 데이터 중단 소식을 불과 며칠 전에야 통보받았음
     * NOAA 및 연구자들은 허리케인 예측의 핵심 역량이 “순식간에 절반으로 줄어드는 셈”이며, 정확한 예보 및 경보가 어려워질 수 있음을 우려함
     * 극지 및 해빙 변화, 허리케인 예측, 기타 기상 현상 감시에서 장기적 공백이 불가피할 전망임

        Hacker News 의견

     * 연구자들 말에 따르면 위성 시스템 자체에는 문제가 없고, 데이터 수집 및 배포가 갑자기 중단된 것은 의도적인 결정일 가능성 높음이라는 발표를 봤음. 미국이 위성을 매각하려는 시도조차 없이 그냥 버리는 것인지 궁금증
     * 데이터를 무시할 수 있는 방법을 막기 위한 의도라고 생각함. 다른 주체에게 접근을 허용하면 데이터 자체는 계속 포착되고 배포되는 구조임. 만약 이 데이터가 기후 변화로 인해 허리케인 강도가 심화된다는 것을 보여준다면, 데이터를 아예 수집하지 않으면 “증명할 수 없다”는 입장 유지 가능. 이런 상황에서 보험이나 자본시장을 비롯해서 데이터 부재로 리스크 산정이 어려워질 때 대형 시스템이 어떻게 반응할지 주목 필요. 관련 토론 링크로 이전 논의, 또 다른 토론, 아주 연관성 높은 댓글 공유
     * 문제의 근본 원인을 지적하자면 이 위성들이 북극을 추적하고 있었던 것 때문임을 언급. 이것이 곧 ‘나쁜 위성’이라는 의미고, 정부의 “기후 과학은 없다”는 주장과 상충되는 역할을 했음
     * 이번 사건의 기사 제목이 정확하지 않다는 인상. 위성을 잃어버린 게 아니라, 파괴되거나 탈궤도, 해킹, 납치된 것도 아님. 제목을 ""미국 핵심 위성 데이터 차단으로..."" 등으로 바꾸는 게 더 적합하지 않을까 제안
     * NOAA-20 위성이 더 우수하고 계속 가용될 예정임을 확인. NOAA측에서도 “예보 품질에는 영향 없을 것”이라고 발표. 혹시 구형 센서의 퇴역 과정으로 보임
     * NOAA도 정치적 이해관계에서 자유롭지 않다는 우려. 자세한 관련 기사로 PBS 뉴스아워 Fact-check 공유
     * NOAA의 트럼프 2기 시절 이슈와 역사 위키피디아 링크 첨부
     * 행정부의 ‘플로리다와의 전쟁’의 일환처럼 보인다는 의견
     * 대통령이 샤피 마커로 허리케인 경로를 예측할 수 있다면, 그 많은 위성들이 필요 없을 수도 있겠다는 농담식 코멘트
     * 결론은 수십 년 전부터 명확했다는 생각, 특히 2015년 의회가 DMSP 프로그램을 사실상 종료하면서 본격적으로 시작된 흐름임을 지적
     * 이 영향은 미국 내 남부 보수주의 강세 지역—플로리다, 텍사스, 루이지애나, 미시시피, 앨라배마—에 집중됨을 상기. 정작 가장 영향을 받는 사람들이 현재 정책을 강하게 지지한다면, 그 데이터를 지키는 데 어떤 의미가 있을지 의문
     * “행정부의 플로리다와의 전쟁”이라는 말에 대해, 실제로 플로리다 주정부가 기후 변화 자체에 반대 입장임을 기사와 함께 설명. 예시로 “Ron DeSantis 주지사가 주법에서 climate change 언급 삭제 법안 통과” (관련 기사), “플로리다 공무원들이 climate change 표현 사용 금지” (관련 링크) 사안을 제시. 이를 통해 일부 정치인들이 현실을 외면하는 경향 강화되고 있음을 강조
     * 이 기사 자체가 잘못 작성된 것이라는 판단. 해당 위성 프로그램은 이미 2015년에 종료됐고, JPSS로 대체된 상황임. 트럼프, 기후 변화 등과는 무관하며 당시 잦은 위성 고장으로 인해 하나만 남게 된 배경 설명
     * 유럽의 허리케인 데이터 모델링이 미국보다 좋다는 인식이 있는데, 이번 사태가 미국의 예보에만 영향을 주는 것인지 궁금증
     * 유사 주제가 이전에도 토론됨을 안내 (140개 댓글)
     * 이번에 논란된 DMSP 위성들이 실제로 노후화된 것임을 확인. 2015년 의회의 DMSP 프로그램 종료 및 차세대 위성 체계로 이전 결정이 근거 (위키피디아 참고). GOES-R 위성은 해상도가 DMSP와 비슷하거나 더 우수. DMSP는 600m, GOES-R은 500m 해상도. (이전 GOES 위성과 혼동 주의) 논문 링크, GOES-16 정보 공유
     * 정치권 교체에 따라 중요한 프로젝트 존속이 위협받는 문제 지적. 많은 과학 프로젝트가 대통령 단임 지위라는 ‘단일 실패 지점’에 의존하는 것이 현실. 민간 부문에서 일부 역할을 맡으면 해결될지, 그리고 여러 국가가 공동 지원하는 국제 구조라면 한 나라가 빠져도 유지 가능한 모델이 될지 아이디어 고민
     * 실제로 이런 리질리언스 구조는 이미 설계되어 있고, 입법부가 예산을 배정해 법으로 만들면 대통령은 이를 성실히 집행해야 한다는 헌법 조항도 존재. 대통령이 의회 승인 예산을 맘대로 보류하는 것을 막는 법도 따로 있음. 따라서 이번 사안은 리질리언스 부족이 아니라 이 시스템을 의도적으로 해제한 결과라는 주장
     * 대통령이 특정 프로젝트 필요성을 무시할 수 있다면, 민간 기업 국유화 및 강제 종료도 무시할 수 있다는 점에서, 민간 위탁도 만능은 아니라는 의견
     * 실제로 이번 프로젝트는 2015년 종료된 프로그램임을 재차 확인
     * 과거 50년간 DMSP 위성 데이터가 암호화 없이 누구나 수신할 수 있게 방송되었다는 점 상기. 만약 지상국과 데이터 유통망만 종료된다면, 연구자들이 SDR 장비로 직접 수집할 기회도 남아 있음. 후속 위성들이 같은 역할을 못하는지, 아니면 실제론 가능하지만 정부에서 공개를 막는 것인지 궁금증
     * 상황이 이쯤되면, 작고 정밀한 기압계를 든 비둘기(전서구) 투자로 돌아가야 할지 모르겠다는 유쾌한 농담 제기
     * 위성 신호가 암호화 되었는지 의문 제시. 아마추어 무선사가 계속 신호를 수신할 수 있을지 궁금증
     * 암호화가 적용되지 않음. S-band로 송신해서 2만원짜리 일반 rtl-sdr로는 힘들 수 있지만, 더 고급 장비면 가능. 단, 일부 지역에서는 암호화가 켜지긴 하지만 미국 본토권에서는 공개된 신호임
"
"https://news.hada.io/topic?id=21742","ZFS ARC thrashing 현상 대응 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ZFS ARC thrashing 현상 대응 방법

  1. 개요

     * ZFS 파일시스템의 캐시(ARC)에서 발생하는 스래싱 현상에 대한 정의, 진단, 대응 및 예방 방안 정리

  2. ZFS ARC 스래싱 정의

     * 개념: 캐시 히트(Hit) 연속 실패로 인해 잦은 캐시 아이템 교체(Eviction)가 발생, 시스템 성능이 오히려 저하되는 현상
     * 영향: 고성능 I/O 환경(예: 데이터베이스 서버)에서 심각한 서비스 장애 유발 가능

  3. 주요 진단 증상

     * CPU iowait 수치 20% 이상 지속
     * CPU system mode 사용률 평소 대비 증가
     * 캐시 히트율(Hit rate) 저하 (평상시 데이터와 비교 필요)
     * 핵심 증상: arc_prune, arc_evict 커널 스레드의 CPU 점유율 90% 이상 시 스래싱 확실

  4. 긴급 대응 방안

     * 조치: 리눅스 파일시스템 캐시 수동 제거
     * 명령어: echo 3 > /proc/sys/vm/drop_caches
     * 효과: 페이지 캐시 및 slab 객체를 함께 제거
     * 특징:
          + 디스크에 미반영된 데이터(dirty item)는 유지되므로 데이터 손실 없음 (비파괴적)
          + 실행 직후 캐시가 다시 채워지기 전까지 일시적 성능 저하 발생

  5. 근본적인 예방

     * 원인: 시스템 워크로드에 비해 할당된 ARC 캐시 크기가 부족하기 때문
     * ARC 크기 (zfs_arc_max):
          + 기본값이 이미 시스템 메모리의 1/2(Linux 기준)로 매우 큼
          + 다른 애플리케이션과 메모리 경쟁을 유발할 수 있어 무작정 늘리기 어려움
     * 해결 방안:
          + 다른 애플리케이션의 메모리 사용량이 높은 경우, zfs_arc_max 값을 줄여 경쟁 방지
          + 근본적인 문제 해결을 위해서는 물리적 메모리 증설(Scale-up)이 가장 확실한 방법
"
"https://news.hada.io/topic?id=21716","Qwen VLo - 세상을 '이해'하는 것에서 '묘사'하는 것으로","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Qwen VLo - 세상을 '이해'하는 것에서 '묘사'하는 것으로

     * Qwen VLo는 통합 멀티모달 이해 및 생성 모델로, 이미지 이해뿐 아니라 고품질의 이미지 생성을 제공함
     * 사용자는 자연어로 창의적인 명령을 내릴 수 있으며, 스타일 변환, 배경 변경 등 다양한 이미지를 직접 생성 및 편집 가능함
     * 여러 언어를 지원하여 전 세계 사용자가 언어 장벽 없이 손쉽게 이용 가능함
     * 이미지를 계속적으로 개선 및 최적화하는 점진적 생성 방식을 채택하여, 향상된 시각적 품질과 높은 제어성을 제공함
     * 아직 미리보기 단계로 일부 기능의 불안정성이 있을 수 있지만, 지속적 개선 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * 멀티모달 대형 모델의 발전은 기술의 한계를 끊임없이 확장하는 중임
     * QwenVL에서 시작해 Qwen2.5 VL까지 이미지 콘텐츠 이해를 강화해왔으며, 이제 Qwen VLo는 이해와 생성 모두를 아우르는 새로운 멀티모달 모델로 등장함
     * Qwen VLo는 세상을 ""이해""하는 데서 나아가, 그 이해를 바탕으로 디테일한 이미지 ""생성""까지 할 수 있음
     * 이 모델은 지각과 창조의 경계를 실질적으로 연결함
     * 현재는 프리뷰 버전으로 Qwen Chat에서 사용 가능하며, ""귀여운 고양이 그림 생성"" 같은 명령으로 이미지를 만들 수 있고, 이미지 업로드 후 ""고양이에 모자 씌우기""와 같은 수정도 수행 가능함.

창의적 생성 과정

     * Qwen VLo의 이미지 생성을 시연하는 영상에서 볼 수 있듯, 이 모델은 진행적 생성 방법을 사용함
     * 좌상단에서 우하단 방향으로 이미지를 점진적으로 구축하며, 계속해서 예측을 고도화하여 일관되고 조화로운 결과를 달성함
     * 이 생성 메커니즘은 시각적 품질을 높이고, 사용자가 창의적 과정을 더 유연하고 세밀하게 제어하도록 도움을 줌.

이해력에서 창조성으로: 향상된 멀티모달 생성 역량

  Qwen VLo의 주요 강화점

    1. 정밀한 콘텐츠 이해 및 재현
          + 기존 멀티모달 모델은 생성 과정에서 의미 불일치(예: 자동차를 오인식하거나 구조를 잃는 현상)가 있었음
          + Qwen VLo는 디테일 포착 능력이 커졌으며, 높은 의미 일치성을 유지함
          + 예를 들어, 자동차 사진에 색상 변경을 요청하면 실제 차종과 구조는 유지하면서 자연스럽게 색상만 바꿔 현실감 있는 결과를 만듦
    2. 오픈엔디드 명령 기반 편집 지원
          + 사용자는 ""이 그림을 Van Gogh 스타일로"", ""19세기 사진처럼"", ""맑은 하늘 추가""와 같은 자유로운 창의적 명령을 자연어로 입력 가능함
          + 스타일 변환, 장면 재구성, 세부 수정은 물론, 딥러닝 전통 비전 작업인 깊이맵, 세그멘테이션, 엣지 추정까지 단순 명령으로 가능함
          + 복합 명령(예: 오브젝트 수정+텍스트 편집+배경 변경)도 한 번에 수행 가능함
    3. 다국어 명령 지원
          + Qwen VLo는 중국어, 영어 등 다양한 언어의 조작을 지원함
          + 언어 장벽 없이 전 세계적 사용 편의성 제공

데모 활용 사례

   Qwen VLo는 인간 아티스트처럼 이해력을 바탕으로 상상을 실현함. 배경 교체, 피사체 추가, 스타일 전환, 오픈엔디드 명령 기반 대규모 편집 및 탐지/분할 대응이 가능함.

   특히, 이해 기반 재생성 기능으로 만화→실사, 특정 인물→풍선과 같이 폭 넓은 창작 스타일 변환을 지원함.
     * 모델의 고도화된 이미지 및 명령 해독력으로 복합적 커맨드를 한 번에 수행, 예를 들어 포스터 제작, 여러 오브젝트 결합 등 여러 단계 작업을 한 번에 완수함

   또한, Qwen VLo는 탐지, 분할, 엣지 디텍션 등 기존 정보에 주석/표식 기능도 지원함.
     * 다중 이미지 입력 처리 기능도 준비 중(향후 공식 출시 예정)
     * 텍스트+이미지 입력뿐 아니라 텍스트→이미지 생성(일반 이미지, 중영혼합 포스터 등)도 지원함
     * 매우 긴 가로/세로 비율 포맷(최대 4:1, 1:3 등) 이미지 생성 지원(공식 출시 예정)
     * 모델이 직접 생성한 이미지를 다시 이해/분석해 강아지·고양이 품종 판별 등도 가능함

사용 방법

   Qwen VLo는 동적 해상도 학습 및 생성으로, 입력·출력 이미지 해상도 및 비율을 자유롭게 사용할 수 있음. 고정 포맷에서 벗어나 원하는 사이즈 이미지(포스터, 일러스트, 웹배너, SNS커버 등) 제작 가능함.
     * 생성 메커니즘: 좌상→우하 진행적 생성(Progressive generation)
     * 긴 텍스트가 포함된 광고·만화 패널 등 세밀 제어가 필요한 작업에서 과정 실시간 미조정 가능함

한계

   Qwen VLo는 프리뷰 단계로, 몇 가지 부족함이 있음. 생성 도중 정확성 부족, 원본 불일치, 명령 미준수, 이미지 이해 불안정성 등이 발생할 수 있음. 지속적 개선 및 안정화 업데이트 중임.

다음 단계

     * 멀티모달 대형 모델이 양방향 텍스트-비전 입출력을 지니게 되면서, 새로운 표현·상호작용 방식이 열리고 있음
     * 앞으로 모델들은 텍스트 답변뿐 아니라 다이어그램, 보조선, 강조 표시 등 시각적 콘텐츠로 아이디어를 전달할 수 있게 될 것임.

     * 발전된 생성 기능은 모델 자체의 이해도 검증 및 개선에도 활용될 것임
     * 예를 들어, 세그멘테이션맵, 디텍션맵 등 중간 결과를 직접 생성하며 자신의 이해를 증명·보완 가능함
     * 이러한 연구 방향을 지속적으로 탐구 중

        Hacker News 의견

     * Qwen이 오픈 웨이트를 공개하지 않은 점이 아쉬움으로 느껴짐. 그 동안 Qwen의 가장 큰 강점 중 하나가 오픈 웨이트 전략이었기 때문. 4o의 이미지를 자동 생성하는 것과 경쟁할 수 있는 진정한 오픈 웨이트 모델이 있었으면 하는 바람. 웨이트에 직접 접근해야만 가능한 재밌는 연구 방향이 많음. 개발비 회수가 문제라면 BFL의 Flux Kontext Dev 출시 모델을 참고 추천. 연구자와 개인에게는 무료로 웨이트를 공개하고, 스타트업은 상업용 라이선스를 적당한 가격에 구매하도록 하는 방식도 있음
          + Qwen의 이미지는 확실히 OpenAI 결과물로 훈련된 게 드러남. 이미지에 주황색 빛이 도는 것만 봐도 알 수 있음 (예시1, 예시2, 예시3). 자가 데이터 확보 시도조차 했는지 궁금. 결국 OAI를 그대로 따라가면서도 API 뒤에 숨겨버림. OAI처럼 폐쇄적일 뿐 아니라 성능도 떨어짐. 이런 전략은 이해하기 어렵다고 느낌
          + 오픈 웨이트를 강조하면서도 연구자·개인을 위한 별도 웨이트, 스타트업은 상업 라이선스 구매 식을 제안한다면, 진짜 오픈 웨이트와 거리가 멀다는 느낌. '오픈 소스'처럼 원하는 대로 쓸 수 있는 자유가 있어야 진정한 의미의 오픈. 그렇지 않으면 오픈이라는 단어가 의미를 잃을 수 있음
          + 수천만 달러에 달하는 투자, GPU 비용, 엔지니어 인건비를 이미지 생성 요금만으로 회수할 수 있다고 생각하지 않음
          + 중국발 오픈 웨이트 시대가 갑자기 끝난 듯한 분위기. 알리바바는 Qwen 공개를 중단했고, Tencent도 Hunyuan 공개를 멈췄으며, Bytedance도 Seedream을 닫음. 명백히 서구 모델로 훈련되는 건 여전함. 오히려 100% 오픈 후 인프라·서비스로 수익화하는 전략이 더 현명하다고 생각함
     * 이미지가 언어 모델에 전달되기 전 256 토큰으로 압축됨. 예를 들어 모자 추가 요청 시 얼굴 전체를 다시 그림. 개별 오브젝트가 따로 저장되지 않고, 곰 캐릭터도 일시적으로 존재함. 하나의 융합된 잠재 공간에 저장되고, 새로운 조건에서 새로 샘플링됨. 프롬프트를 조금만 수정해도 이미지 전체가 바뀜. 즉, 매번 새롭게 씬을 만드는 방식으로, 다양한 활용처에는 좋게 느껴짐
          + Flux Kontext에서는 멀티모달 모델처럼 세부 내용이 잘 유지되는 편이라 마음에 듦. GPT-Image-1에서는 '지브리 스타일로 만들기' 같은 전체 스타일 변경에는 괜찮지만, 포토리얼리틱 이미지에 안경 추가처럼 세밀한 변경에는 세부정보를 잘 못 살림
     * 곰 이미지 편집 예시를 보면서, 요청한 것보다 더 많은 부분이 바뀌는 것을 발견함. 배경만 바꿔달라고 했는데 곰도 확 달라졌고, 곰을 풍선으로 바꿔달라 했더니 배경(보도블럭)이 사라지거나 수박씨가 없어지는 등 엉뚱한 변화까지 나옴. 이게 프롬프트를 더 잘 쓰면 해결되는 문제인지, 아니면 모델 구조의 한계인지 궁금함
          + 둘 다 해당. 프롬프트 최적화하면 결과가 좀 나아질 수 있지만, 근본 원인은 모델 구조와 훈련 방법, 즉 아키텍처와 방법론의 한계임
     * 펠리컨이 자전거 타는 이미지를 시도해봤고, 아코디언 이미지도 생성해봄. 손가락이나 건반의 검은색이 잘못 표현되는 등 세세한 면에 약함. 생성 속도는 꽤 빠름 예시 링크
          + Simon 테스트의 핵심이 SVG 포맷이라는 점을 놓친 듯함. 자전거 타는 펠리컨 이미지는 Stable Diffusion 2/3 이후로 쉽게 해결된 과제임. 픽셀 이미지가 아니라 SVG에서 논리적 추론과 정확도가 필요해서 도전적임
     * 편집, 스타일 전환 등 이미지 변경 예시에서 미묘한 노란색이 감도는 현상 발견. GPT Image 1에서도 나타나는데, Flux Kontext에는 없었음. 이유가 궁금함
     * 모든 이미지가 언캐니 밸리 느낌. 색상과 그림자가 모두 어색함
          + 결과물이 전체적으로 조잡함. 이런 이미지는 연구가 아니라면 실사용 사례 떠올리기 어려움
     * 머신러닝 연구자이자 물리학 학위를 가진 사람으로서, 이런 모델에 '이해', '설명'이라는 단어 사용하는 게 꺼려짐. 실제로는 도움이 안 되고 오히려 혼란만 만듦. 물리학에서 수학을 쓰는 이유는 정밀성 때문이고, 코딩 역시 매우 구체적임. 우리는 삶에서 수많은 디테일에 영향받는데, 모델은 그런 미묘함을 담아내지 못함. Asimov의 ""Relativity of Wrong""(링크)을 꼭 읽어봤으면 함. 정말 '이해'했다 하고 싶으면, 모델이 발견/추론/개념 재정의처럼 들어보지 못한 결과를 생성해야 함. 인간은 반(反)사실적 사고(링크)가 자연스럽지만, 현대 ML 모델은 아님. OP 이미지에서 손가락 수 오류나, 키보드의 키 배열 오류 등이 대표적 사례. 겉보기엔 그럴듯해 보여도, 쳐다볼수록 점점 어색함이 드러나는 전형적인 언캐니 밸리 현상
          + 실제로 무언가 만드는 입장에서는 이런 논쟁이 피곤하게 느껴짐. 입력과 출력 개념을 이해하기 쉽게 설명했다는 것만으로도 충분히 가치 있음. 릴리즈 문서 읽으면 Qwen이 원래 VLM으로 '이해/식별/인지' 역할을 했고 이제 '생성/묘사/그리기' 등으로 능력이 확장됐음을 알 수 있음. 더 이상의 위기론이나 과도한 의미부여는 필요하지 않음
     * 자동 음성 읽기 기능을 어떻게 끄는지 궁금. 웹사이트 들어가면 그냥 대기상태로 있다가 내가 직접 조작할 때만 동작했으면 싶음. Firefox에서 영상을 전체화면 자동재생하더니 갑자기 읽기가 시작됨 (iOS 환경)
          + 설정 > 사이트 설정 > 오디오 및 비디오 자동재생 차단. Android Firefox에서는 해당 기능 가능. iOS나 데스크탑에도 비슷한 옵션 존재하며, 알림 권한 요청도 완전히 차단 가능
     * 4o 방식 이미지 생성 아키텍처 관련 기술 보고서가 있는지 궁금. 비슷한 방식으로 이미지를 생성하는 다른 모델에 대해서도 자세히 알고 싶음
     * 개인적으로 머신러닝이 '이해'보다 '묘사' 쪽에서 훨씬 더 발전했다고 생각함
          + 인간이 세상을 더 잘 이해한다고 생각하는 근거가 궁금. 인간은 세상에 대해 감정적인 반응을 많이 보이지만 감정 자체가 이해를 주진 않음. '이해한다'라는 말도 사실 매우 주관적인 기준에 불과함
"
"https://news.hada.io/topic?id=21704","10년간의 Pomological Watercolor 기록","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    10년간의 Pomological Watercolor 기록

     * Pomological Watercolor Collection의 공개를 요구한 지 10년이 되는 시점임
     * 미국 정부가 소장한 7,000여 장의 과일 및 생물학 수채화 그림을 공개 촉구한 결과, 수개월 만에 고해상도 스캔본이 무료로 공개됨
     * 공개된 이미지를 위키미디어에 업로드하기 위해 Python으로 소프트웨어를 직접 개발하며 프로그래밍 경력의 시작점이 됨
     * 이 컬렉션은 SNS 봇, 학술 자료, 예술 프로젝트, 출판물 등 다양한 방식으로 대중적으로 확산됨
     * 공공 영역 옹호 활동을 계기로 생긴 이 프로젝트가, 10년간 예상치 못한 개인적·사회적 변화를 이끌어낸 경험임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Pomological Watercolor Collection을 요청한 계기와 전개

     * 10년 전, 미국 정부가 보유한 과일 수채화 자료의 공개를 촉구하는 블로그 글을 게시함
     * 이 컬렉션은 1880년대부터 1940년대 사이에 제작된 7,000여 점의 과일과 기타 생물 표본 그림으로 구성됨
     * 당시 이미지는 이미 디지털화되어 판매를 통해 접근 가능했으나, 판매량이 100장에도 미치지 못해 실질적인 접근성이 매우 낮았음
     * 컬렉션 안에는 특별하고 아름다운 그림들이 많지만, 이 글에서는 그중 '이상한' 그림을 중점적으로 조명함

자료 공개와 확산

     * 공개 촉구 이후 몇 개월 만에, 미국 국립농업도서관이 모든 고해상도 수채화를 무료로 검색·다운로드할 수 있도록 공개함
     * 컬렉션 공개 이후 바로 다음 단계로 Wikimedia Commons에 그림을 업로드하려 시도함
     * 이를 위해 Python으로 파일 대량 업로드를 자동화하는 소프트웨어를 직접 개발함
     * 이 프로젝트는 직접 프로그래밍을 시작하는 계기가 되었으며, 개인적으로 성장의 전환점이 되었음

컬렉션 활용과 대중적 확장

     * 이미지 공유를 위해 Twitter, Bluesky, Mastodon 등에서 자동으로 이미지를 게시하는 소셜 미디어 봇을 제작함
     * SNS 봇을 통해 컬렉션이 더 많은 사람들에게 알려짐
     * 다양한 기회를 활용해 컬렉션의 가치를 홍보함
          + 2015년 Wikicon에서 국립기록원 발표
          + 카르나틱 음악 콘서트 시각화 영상 제작
          + 수천 종의 사과 품종 정보를 데이터세트에 추가
          + 그림을 활용한 버튼 제작 등 소품 제작

대중적·문화적 영향과 수집품의 확장

     * 컬렉션이 공개된 이후 다음과 같은 방식으로 사회적 영향력 확장
          + 테이블 북, 학술서적, 엽서, 고급 아트 프린트 등으로 출간 및 판매
          + Mashable, Morsel, Open Culture, Atlas Obscura, New York Times 등 다양한 미디어에서 주목받음
          + 최근에도 Defector 등 주요 언론에서 다뤄짐
     * 문화적 파급력이 커지며 TV 세트 배경 등 의외의 곳에서도 그림이 확인됨

개인적 의미와 공공 영역의 가치

     * Pomological Watercolor Collection은 공공 영역(퍼블릭 도메인) 에 대한 관심에서 시작됨
     * 이 프로젝트는 지난 10년간 개인적으로도 매우 긍정적이고 다양한 변화를 이끌어냄
     * 지속적으로 호기심을 좇고 꾸준히 관심을 가진 것이 예상치 못한 기쁨과 성취로 이어짐
     * 처음에는 단순한 흥미에서 시작했지만, 실제로 10년 넘게 이 컬렉션과 함께하며 놀랍고 즐거운 경험을 이어오고 있음

        Hacker News 의견

     * 블로그 포스트 작성자 Parker Higgins이 여기에서 얘기 보는 게 정말 즐거움이라는 소식 전달, 궁금한 점이나 일자리 제안도 기다림
          + Parker, NYT 데일리 크로스워드에 실린 당신의 퍼즐 출판 진심으로 축하한다는 메시지 전달, 매일 푸는 사람으로서 여러 번 당신의 퍼즐 보게 되어 인상적이라는 감상 공유
          + 정말 놀랍고 임팩트 있으며 진정성 있다는 소감 전달
          + 본인의 프로젝트에 pomo 그림 중 하나를 사용한 경험 감사함 전달
          + 본인도 개인적으로 여러 프로젝트에서 기술 참고자료로 직접 활용했다는 경험 공유
     * 10년이나 지났다는 점에 놀라고, 그림이 공개될 때 흥분됐던 추억 언급, 훌륭한 작업이라는 칭찬 전달
       이 프로젝트에 자극받아 100년 이상 된 여러 고전적 풍경화들을 카탈로그로 모으는 개인 프로젝트를 시작하게 됐다고 이야기
       특히 대부분 비미국권, 유료로 접속 가능한 기관(영국 박물관 등)에 소장되어 있던 곳을 이메일로 고해상도 이미지를 공개해 달라고 요청 보낸 에피소드
       그림들이 그려진 실제 위치를 지도에 표기해 과거 풍경을 보여주는 프로젝트도 고려 중이라며 이 노력을 다시 시작해보는 걸 고민 중
          + 이런 프로젝트가 실현된다면 정말 보고 싶다는 기대 표현
     * 고화질 이미지를 다운로드하고 싶으면 Internet Archive가 최고의 친구라며 관련 링크(Internet Archive USDA Pomological Watercolor Collection) 소개
       퍼블릭 도메인인 만큼 자유롭게 재미있게 활용 가능하다는 점 강조
       참고로 메타데이터(제목, 저자 등)가 일부 누락되어 있는데, 이 정보가 모두 포함된 다른 컬렉션 아시는 분 있으면 알려달라는 요청
       직접 확인 결과 EXIF 데이터에도 없다고 언급
       편집 내용으로, 메타데이터를 이미지 파일과 매칭할 수 있는 오픈소스 프로젝트(Wumms/pomological)도 함께 소개
       추가적으로, MARC 카탈로그 레코드에서 이미지 사용 시 소속 명기 요청이 되어 있다는 점 인용: ""U.S. Department of Agriculture Pomological Watercolor Collection. Rare and Special Collections, National Agricultural Library, Beltsville, MD 20705""
          + 공식 페이지에 명확한 라이선스 표기가 없다는 점도 지적
            퍼블릭 도메인 이미지라면 출처 명시가 필수가 아닐 텐데, 명확히 라이선스 고지 필요성 언급
            이런 애매함이 생각보다 자주 있다는 점 강조
          + 1929년 저작권 만료 전 제작된 이미지에 대해 USDA가 뭔가를 요구할 수 있을지 의문 제기
            실제로는 실효성 없는 관행적 문구일 것이라는 추측
     * 이 컬렉션 덕분에 압축 관련 즐거운 토끼굴에 빠졌다는 본인 사연 공유
       직접 쓴 '회전 기반 압축'에 대한 블로그 글(rotation-based-compression)도 공유
     * ""m""이 ""rn"" 합자처럼 보여 순간 학문적 워터컬러 아트가 아닌 “pornology” 분야 아트일 뻔(!)했던 우스꽝스러운 착오 경험담 공유
     * Parker Higgins 덕분에 정말 아름다운 작품들 감상했다는 감사 표현
     * Recurse Center에서 Parker와 친구가 되었는데, 현실에서 그가 더욱 열정적이고 유쾌한 인물임을 밝히며 그의 글이 HN 상위권에 오른 것이 반갑다는 소식
     * 그림 전체를 볼 수 있는 링크를 찾으려고 페이지를 훑다가, 링크가 기존과 달리 약간 굵게만(다른 색상, 밑줄 없이) 표시되어 있었음을 발견
       자신의 사이트에서 하이라이트를 더해주면 좋겠다는 제안
       작업 자체는 정말 훌륭하다는 칭찬도 덧붙임
          + 사용자 피드백에 대해 최근 타이포그래피 변경으로 모바일에서 두드러지게 보이지 않았음을 인정
            즉시 밑줄로 수정했고 곧 반영될 것임을 공지
     * 본인의 사무실에 이 그림들 몇 장 프린트해서 걸 생각이라는 이야기와 함께 thisisparker에게 감사 인사
     * ""이 프로젝트가 Python으로 처음 해 본 작업이자, 프로그래머로서 삶의 출발점""이라는 본문의 문구에 공감
       본인 역시 처음에는 단순한 이유로 스크립트 작성을 시작했고, 이후로는 코드로 삶을 개선할 많은 영감을 얻게 되었다는 경험 공유
"
"https://news.hada.io/topic?id=21709","가자 식량 배급소 근처에서 비무장 군중에게 발포하라는 명령을 받은 IDF 장교들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              가자 식량 배급소 근처에서 비무장 군중에게 발포하라는 명령을 받은 IDF 장교들

     * 이스라엘 국방군(IDF) 장교들이 비무장 군중을 향해 발포하라는 명령을 받음
     * 명령은 가자지구의 식량 배급소 인근에서 발생함
     * 이 지침은 인도적 지원을 받기 위해 모인 민간인들에게 영향을 끼침
     * 이러한 명령은 국제 인권 및 군사 행동의 규정에 대한 의문 제기
     * 해당 보도는 분쟁 지역 내 군사 명령과 민간인 안전 문제에 대한 논의 촉진
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * 최근 이스라엘 국방군(IDF) 장교들이 가자지구 내 식량 배급소 근처에서 모인 비무장 민간인 군중을 대상으로 발포하라는 명령을 받은 정황이 드러남
     * 이 명령은 식량 배급을 위한 인도적 지원 행위 과정에서 발생함
     * 현지에서는 수많은 민간인들이 기아와 인도적 지원 부족으로 인해 배급소 주변에 모이고 있음

인도주의 및 군사 규정 논란

     * 군인들에게 비무장자들에게 직접적인 위협이 없는 상황에서도 발포하도록 하는 명령이 내려진 점에서 국제적 인권 규정 위반 가능성 대두
     * 국제법 및 분쟁 상황에서의 민간인 보호 원칙에 반하는 행위가 문제로 지적됨
     * 이 같은 명령은 군대 내 명령 체계, 현장 지휘관의 판단 기준, 분쟁 상황의 긴박함 등이 복합적으로 작용한 결과임

반응 및 영향

     * 이 사건과 관련하여 유엔 및 다양한 인권 단체들이 조사 및 규탄 요구 발표
     * 현지 사망자 및 부상자 발생 가능성이 제기되며, 분쟁 지역 내 민간인 안전과 인도적 지원 경로 확보 필요성 강조
     * 보도 이후 이스라엘과 국제사회에서 군사 작전의 합법성 및 인도적 기준 관련 논의가 확산됨

결론

     * 이 사건은 분쟁 지역에서의 명령 체계와 민간인 보호라는 두 가지 핵심 이슈를 부각함
     * 정보의 신뢰성과 이후 현장 조사 결과에 따라 해당 군사 명령에 대한 책임소재 및 재발 방지를 위한 국제 사회의 협력 필요성 요구

        Hacker News 의견

     * archive.is에 저장된 기사 참고 자료 공유
     * 나는 미국 내 유대인이며 10년 넘게 군 복무 경험이 있음. 예전에는 팔레스타인 시위에 짜증을 느끼고 이스라엘 및 해당 지역에서 시아파 민병대에 대한 군사작전에 대체로 동의했으며, 10월 7일 공격에 분노했음. 하지만 기사의 행동들에 따라 이스라엘의 작전은 명확한 전쟁범죄로 보이며, 지휘한 군·민간 지도자들은 뉘른베르크 재판과 같은 책임을 져야 한다고 생각함. 미국이 이스라엘 작전을 지지하는 것에 부끄러움을 느낌. 이것은 대규모·지속적·의도적인 민간인 학살임
          + 나는 이스라엘계 유대인으로, 이스라엘 기준으로 좌파(혹은 극좌)로 분류되겠지만 인터넷 상의 전통적 의미로 'pro-Israeli' 진영임. 소개된 Haaretz 기사는 매우 충격적임. 사실로 드러난다면 전쟁 범죄임이 분명함. 몇 가지 생각을 덧붙이자면, 1) 기사에 언급된 혐의에 대한 공식 조사 진행 중임. 이러한 일이 창궐하지 않았길 바라며, 범죄 가해자는 꼭 법정에 세워져야 함 2) GHF(가자 인도주의 기금)와 새로운 구호 전달 시스템에 심각한 문제가 있음. 수주째 수십 명씩 사망 중인데, 정확한 원인 규명이 필요함 3) Haaretz의 최근 보도에 따르면, GHF 임시 국장 John Acree는 ""우리 배급소 또는 그 즉각적인 인근에서 사건이나 사망자는 없었다""고 부인함. 하지만 시설 외부에서 벌어진 총격보도에 대해 이스라엘이 조사를 해야 한다고 밝힘
          + Hamas는 시아파가 아니라 수니파임. 그리고 시아파는 폭력적 이데올로기가 아님. 가자지구에서의 인권 범죄는 10월 7일보다 훨씬 이전부터 있어왔음. IDF(이스라엘군)는 화학무기, 기아, 테러 폭격 등 비인도적 전술을 21세기에 반복적으로 사용함
          + 책임자들이 반드시 뉘른베르크 재판에 서야 한다고 생각함. 이것은 한 집단을 계획적으로 말살하는 행위임. 아쉽게도 이스라엘 로비가 미국에서 너무나 영향력과 재력이 강해, 공개적으로 이런 이야기를 하면 쉽게 반유대주의자로 몰아 입막음하는 현실임
          + 나는 유대인 커뮤니티를 사랑함. 그래서 더 조심스럽게 말하지만, Netanyahu의 행동은 한 측면에서(비록 다른 면은 다르지만) 나치 독일을 떠올리게 함. 이스라엘 유대인이 전 세계의 권리를 주장하는 것은 아니지만, 지금의 행태는 명백히 한 민족을 끝장내려는 의도임. 단지 억지력이나 방어가 아니라 민간인까지 제거하려는 목적이 분명함. 10월의 테러는 결코 옹호하는 바 아니지만, 이스라엘의 반응은 역겹다고 생각함. 미국에서는 이스라엘을 중동의 유일한 민주주의로 보호해야 한다고 말하지만, 현실은 단일 민족의 권리만 보장하는 국가일 뿐임. 미국 기준으로는 민주주의라고 볼 수 없음. 중동 내에서 미국의 이익도 중요하니 어쩔 수 없는 동맹관계가 있지만, Netanyahu의 전쟁범죄 수용, 이스라엘의 약화된 민주제도, 국민주의가 가져올 위험을 생각할
            때 미국은 반드시 균형과 다변화를 신중하게 고려해야 함. 10년 뒤의 이스라엘은 상상도 못할 모습이 될 수도 있음. 매우 우려스러운 상황임
          + 또 다른 관점으로, 양 쪽 모두의 경험을 전체적으로 집계하면 결국 폭력을 선택하는 경향이 있음. 당신이 참여하는 담론이 실제로 목표 달성에 도움이 됨? 오히려 반대 효과임. 이 담론은 인구 통계와 여론의 복잡성을 극한으로 쪼개 어떤 이상적인 일치점을 찾으려 함. 하지만 인터넷 논쟁에서 이기는 것이 목표 달성에는 효과가 없음. 이런 생각들이 공유되고 다양한 포럼에서 논의되는 건 필요하지만, 인기 경쟁만으로 분쟁 해결이 이뤄지지 않음을 기억할 필요가 있음
     * 사람들이 알아야 할 것은, IDF가 종종 어린이 등 무고한 가자 주민을 겨냥해 총을 쏜 사례는 식량 배급사태 이전부터 있었던 문제라는 점임. 뉴욕타임스 10월 기사에는 수십 명의 의료인이 가자에서 머리 또는 가슴에 총상을 입은 아동 사례를 목격했다는 증언이 담겼으나, 이런 내용도 단순 의견란에 실렸을 뿐임. 미국 미디어와 정치 전반에 이스라엘이 미치는 영향력 때문임. 이스라엘 내부에서 책임을 묻자는 압박이 생기면(드물지만), 결론은 대부분 ""군인들이 고의가 아닌 실수로 한 일""이라는 식임. 이 분쟁에서 민간인 사살로 실제 IDF 군인이 처벌된 사례가 있으면 제보해주길 바람. NYT 기사 / Archive Link / 관련 사건 위키피디아
          + 그 기사에서 중요한 점 하나는, 단지 '어린이'가 아니라 '10세 미만 아동'만을 다룬다는 것임. Hamas가 18세 미만을 싸움에 이용하는 경우도 있지만, 사춘기 이전 아동은 해당하지 않음. 기사만으로 IDF가 아이들을 쏜 증거가 확실하다고는 못하겠지만, pro-Israeli 측 주장에서 흔히 나오는 반론을 차단하려는 의도임
          + 많은 사람들이 이미 이런 사실을 인지하고 있음. 다양한 사이트에서 증거와 사건 목록을 관심있는 사람들이 쉽게 검색할 수 있도록 정리해둔 상태임. 사건 자체와 이에 대한 즉각적인 반응에 집중해도 충분한 상황임
     * 약 10년 전, 스포츠 세미나로 이스라엘을 찾았을 때 따뜻한 사람들을 많이 만났음. 주변이 늘 적대로 가득한 환경에서 유지·버티는 이스라엘 국민의 현실을 동정했으며 그들의 강인함에 감탄했음. 이번 가자 분쟁 초기에도, 이스라엘 내부에서 정부에 항의하며 평화를 갈구하는 모습을 보고 ""권력자 일부의 일탈이고 모든 이스라엘 국민이 그러는 건 아니다, 양쪽 입장이 있다""는 생각을 가졌음. 그러나 점점 참혹하고 극단적인 일방적 범죄가 오랜 기간 반복되면서 더는 어떤 변명의 여지도 찾기 힘듦. 상식적인 사람이면 지금 가자에서 일어나는 행위가 정당화될 수 없다고 생각함. 이스라엘 국민이 진정으로 변화 원했다면, 어떤 수를 써서라도 지금까지 멈췄어야 하며, 그게 인간의 의무이자 책임임. 하지만 아무런 변화도 없었음. 오랜 고통 끝에 스스로 이
       정도까지 악역이 된 현실이 슬픔
          + 독일인으로서, 미국과 유럽국가(특히 독일)에게 큰 책임이 있다고 느낌. 이스라엘 국민의 불안과 경험이 심리적으로야 이해가 가지만, 우리의 국가들은 중립적인 척하면서 실제로는 분쟁 지속에 일조하고 있음. 반(反)파시즘부터 종교적 신념까지 다양한 내러티브에 따라 이스라엘의 '독점적 권리'를 무비판적으로 지지하며, 국제 인권이나 주권 가치를 포기함. 만약 이스라엘의 메시아적 행태에 동맹국들이 오직 전폭적 지지를 보내기만 한다면, 이스라엘의 위험한 경향이 더욱 커질 것은 당연함
          + 유대인 이스라엘 시민 약 절반이 가자 주민 모두 학살을 옹호함을 여론조사에서 직접 언급함 출처 가자지구 민족 청소 역시 80% 가까이 지지함. 단순한 선전이 아니라, 실제로 팔레스타인 땅을 빼앗음으로써 자신들이 이득을 볼 수 있다는 계산임
          + 이스라엘의 가자 공격 시작부터 대부분 이스라엘인들은 가자에서 발생하는 고통에 관심을 보이지 않았음. 전쟁 종식을 요구하는 이들도 단순히 인질 석방만이 목표임
          + 이스라엘 국민 대다수가 만약 동의하지 않는다면, 실제로 뭘 어떻게 하길 바라는지 궁금함. 똑같은 일이 미국에서도 일어나고 있지만, 모두가 ""미국인""이라 일반화하지 않음. 이란 역시 정부와 국민을 분리해 더 세밀하게 논의해왔음. 정부는 항상 국민 전체를 대변하지 않음
          + 정부·종교·미디어·학교에서 이뤄지는 세뇌가 원인일 거라 생각함(다큐멘터리 참고). 사람이 얼마나 비인간적인 사고를 하게 만들 수 있는지 참담함. 이스라엘 국민들이 인류애에서 멀어진 현실에 유감을 느낌
     * 이 사안에서 가장 큰 문제는 실제 전쟁범죄의 참혹함이 아니라, 정부가 책임자 처벌을 얼마나 극단적으로 피하려 드는지임. 이는 오히려 미래 범죄에 대한 무언의 승인과도 같아 가해자들이 더욱 대담해질 수 있음
          + 나는 정부의 이러한 행동이 결코 ""우연""이 아니라고 믿음
          + 직접적 범죄를 넘어서서 이스라엘 법상 집단학살 선동도 사형에 해당함. 하지만 정치·언론계 인사 다수가 선동적 발언을 일삼으며, 이 경계가 이미 무너짐. 정치권 그 누구도 전쟁범죄에 대한 진지한 독립적 검토를 용납할 수 없음. 만약 그랬다간 자기 모두가 죽음에 처할 수도 있기 때문임. 이는 결국 일선 장병들에게도 ""지시 없이도 처벌받지 않는다""는 신호로 작용함
          + 그저 명령에 따랐다고 항변함
          + 어떻게 정부가 자기 행동에 대해 책임을 묻길 기대할 수 있는지 의문임. 이것은 일부 군인들의 우발적 범죄가 아니라, 이스라엘 정부가 의도한 일임
          + 정부 수장이 부패 재판 중이라는 사실을 상기함
     * 이미 가자 주민 증언으로 이런 일들이 벌어진다는 건 모두 알고 있었음. 새롭게 미국-이스라엘이 독점적으로 운영하는 '인도지원' 단체가 실제로는 Hunger Games처럼 수십 명이 지원 배급 때마다 이스라엘·미국 계약업자(용병 등)에 희생된 사진과 영상이 넘쳐났음. 학살자들이 지원을 장악할 때 이런 일은 예견된 일이었음. 내부 증거가 나온 것은 다행임
          + GHF 구상은 군부가 아닌 두 VC(벤처캐피탈리스트) 출신 인물이 주도함. IDF 참모총장은 IDF가 직접 지원 책임지는 것을 꺼리기도 했음. 실제로 GHF의 초기 논의는 이스라엘 군부가 주도했으나, 이스라엘·미국 VC 두 명이 논의에 큰 영향력을 발휘함. Liran Tancman(8200부대 예비역/기업가)은 팔레스타인 민간인을 분류하기 위한 생체 인증 시스템 도입을 주장했고, Michael Eisenberg(미국-이스라엘 VC)는 기존 유엔 구호 체계가 Hamas를 강화하므로 전면 개편이 필요하다고 의견을 냄. 워싱턴포스트 보도 / archive link
     * 서구 시민으로서, 내 국가가 이스라엘과 동맹임을 부끄럽게 느낌. 서구가 이스라엘이 만행을 저지르도록 방관하고 있는 현실에 연대책임을 느끼고 있음. 필요할 때는 지원하고, 감시하길 꺼릴 때는 모른 척함
          + 미국이 현재 유죄 판결 받은 범죄자가 이끄는 상황임을 언급하며, 별로 놀랍지 않다는 의견임
     * 이 사안은 모호하지 않음. 최소한도(라고 해도 잔혹한) 전쟁범죄이며 완전히 의도적임. 현실적으로는 명확한 집단학살 의도가 더 가까움
          + UN 팔레스타인 특별보고관이 가자지구 내 집단학살 발생을 매우 포괄적으로 결론 내린 보고서 공개함 보고서 전문
     * 기사에서 언급된 ""Operation Salted Fish""는 이스라엘이 가자에서 벌이는 작전의 일명임. 예전 ""Operation Cast Lead""도 그랬듯, 이스라엘은 아이들 놀이이름을 작전명으로 정하는 전통이 있음. 혹시 오징어게임(Squid Game)도 의도한 것인지 궁금함
          + 'Green light' 단계에서 사람들에게 특정 장소에서 구호품이 배분된다고 알리고, 'Red light' 10분 후 알림이 올 때는 ""구호 없음""으로 바뀌며 해당 장소에 모인 사람 타겟으로 총격을 시작함
     * 이 기사와 Flour Massacre(밀가루 학살) 사건을 비교해볼 가치가 있음. 한 해 전 보도와 지금 위키피디아 기술이 어떻게 변화했는지, 관련 영상도 함께 참고할 것을 제안함
"
"https://news.hada.io/topic?id=21754","AI에는 새로운 아이디어가 없다, 오직 새로운 데이터셋만 있을 뿐이다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 AI에는 새로운 아이디어가 없다, 오직 새로운 데이터셋만 있을 뿐이다

     * 최근 AI 발전은 새로운 아이디어보다 새로운 데이터셋의 도입이 핵심임
     * 대형 돌파구 대부분은 이미 존재하던 기술을 새로운 데이터 소스에 적용하면서 나타났음
     * AI 기술의 혁신보다 데이터 변화가 모델 성능에 더 큰 영향을 미침
     * 앞으로의 패러다임 전환도 유튜브, 로봇 등 새로운 데이터 원천을 활용할 때 가능할 전망임
     * 연구자 대부분이 새로운 방법론에 집중하지만 실제로는 데이터가 발전의 핵심임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 발전의 현황과 패턴

     * AI는 지난 15년간, 특히 최근 5년간 비약적인 발전을 이루었음
     * 일부 연구자들은 AI가 특정 과업 수행 능력에서 'AI를 위한 무어의 법칙'처럼 지수적으로 발전한다고 주장함
     * 그러나 실제로는 대형 돌파구가 자주 일어나지 않으며, 느리지만 꾸준한 진보가 이어지는 구조임

혁신은 어디서 오는가

     * 많은 사람들이 AI 발전이 MIT, Stanford, Google 등 학계·산업계의 아이디어에서 나온다고 생각함
     * 연구를 통해 모델 학습 비용을 줄이고, 효율을 개선하는 시스템 혁신은 분명히 이어지고 있음
          + 2022년 Stanford의 FlashAttention 개발로 메모리 활용 최적화
          + 2023년 Google의 speculative decoding으로 추론 속도 향상
          + 2024년 Muon 프로젝트는 새로운 옵티마이저 방식 제시
          + 2025년 DeepSeek-R1은 오픈소스로 주요 AI 연구소 수준의 모델 제공
     * 연구자들이 arXiv, 학회, 소셜미디어 등을 통해 빠르게 공개적으로 성과를 공유하며 글로벌 분산 과학 실험이 활발하게 진행 중임

왜 AI 혁신이 더뎌졌다고 느끼는가

     * 최근 Grok 3, GPT-4.5 등 최신 모델의 성능 향상폭이 줄어듦
     * 수학 올림피아드 등 실전 평가에서 성적이 낮아 과장된 발표라는 지적도 있음
     * 대형 패러다임 변화(딥러닝, 트랜스포머, RLHF, Reasoning)는 10년 단위로 드물게 등장함

대형 돌파구의 공통점: 새로운 데이터셋

     * 4가지 주요 돌파구는 새로운 데이터 소스를 대규모로 처음 활용한 시점과 일치함
          + AlexNet: ImageNet(라벨링된 대형 이미지 데이터)
          + Transformers: 웹 전체 텍스트 데이터(Internet)
          + RLHF: 인간이 피드백한 '좋은 텍스트' 데이터
          + Reasoning: 계산기·컴파일러 등 외부 검증 도구의 결과
     * 각 데이터셋이 처음 대규모로 도입된 후, 남아있는 데이터 확보 경쟁과 효율적 활용 기술 개발이 이어짐

새로운 아이디어 vs 데이터의 역할

     * 특정 모델 아키텍처가 아니더라도, 동일한 데이터만 주어진다면 비슷한 수준의 모델이 개발될 수 있음
     * 실제로 기술적 혁신보다 학습되는 데이터셋 교체가 성능에 더 큰 영향을 미침
          + AlexNet 대신 다른 구조가 등장해도 ImageNet이 있었기에 발전이 가능
          + Transformer가 아니더라도 LSTM, SSM 등이 동일 데이터로 비슷한 성능을 보임
     * 데이터셋이 학습 결과의 상한선을 결정하며, 모델·알고리듬 개선만으로는 극복이 불가능함
     * The Bitter Lesson에서 강조된 바처럼, 새로운 방법보다는 진짜 중요한 건 결국 데이터임

다음 AI 패러다임 전환의 후보

     * AI의 다음 큰 도약은 새로운 network, RL 방식이 아닌 지금까지 미활용된 신규 데이터 소스를 이용할 때 일어날 가능성이 높음
     * 즉, 새로운 데이터셋을 대규모로 확보할 때 패러다임 전환이 일어날 가능성이 높음
          + 가장 주목받는 후보: YouTube 등 동영상 데이터
               o 유튜브에 매분 500시간의 영상이 업로드되는 수준
               o 텍스트보다 수십 배 이상 방대한 정보를 담고 있고, 언어적 뉘앙스, 물리적·문화적 맥락까지 학습 가능
               o Google 등 빅테크가 이 데이터셋 학습에 곧 본격적으로 나설 가능성 큼
          + 또 다른 가능성: 로봇(embodied system)을 통한 물리적 세계 데이터 수집
               o 카메라, 센서 데이터를 GPU에서 대규모 처리·학습 가능 인프라가 갖춰지면, 이 데이터 역시 AI 혁신의 근원이 될 확률 높음
     * 텍스트 데이터는 한계에 도달해가고 있어, 비디오·로봇 등 새로운 데이터 소스가 AI의 미래를 결정할 가능성이 높음

결론

     * AI의 다음 발전은 새로운 아이디어나 알고리듬이 아니라, 새로운 데이터 원천에서 나옴
     * 연구자의 95%가 새로운 방법론에 집중하지만, 실제 혁신은 데이터셋 변화에서 발생함
     * AI 발전을 원한다면, 새로운 아이디어가 아니라 새로운 데이터 확보에 집중해야 함

        Hacker News 의견

     * John Carmack가 탐구하는 방법이 꽤 흥미로운 내용임을 이야기함
       2D 비디오 게임을 초월적으로 잘 플레이하는 모델을 훈련시킨 다음, 이전에 본 적 없는 2D 게임이거나 새로운 레벨에서 잘 할 수 있는지 시험해본 경험을 나눔
       이전 경험이 없는 게임에서 모델의 성능이 오히려 하락하는 결과를 들며 이는 인공지능이 아니라 특정 과업에 대한 숙련 일뿐임을 강조
       초인공지능(ASI) 공포 조장보다는, 새로운 2D 게임을 인간보다 빨리 배우는 일반지능을 만드는 일이 훨씬 어렵다고 밝힘
          + John Carmack가 실제로 이 결론에 사용한 모델이 최신 기술이 아니며, 비싼 foundational model을 쓰지 않은 재미 위주의 프로젝트라는 점을 지적
            심도 깊은 비디오/비전 AI 연구라면 게임 전반에 적용 가능한 확률 기반 latent space를 조작하는 방식이 더 적합하다고 언급
            veo3가 프롬프트 제약 하에서 영상을 생성하는 기능을 들어, AI가 2D·3D 게임을 일반화할 수 있다는 예시로 설명
            veo3는 실제로 특정 게임에 대한 fine-tuning 없이도 어떤 게임이든 합리적으로 플레이하는 듯한 결과를 보여줄 수 있다고 주장
          + 사람들이 왜 굳이 이런 방식으로 논의를 이끌어가는지 이해하기 어렵다고 말함
            주어진 목표를 달성할 방법은 분명 여러가지이며, John Carmack이 AI 전문가도 아닌데 왜 그의 실험이 표준으로 받아들여지는지 의문을 제기
          + 모델 규모를 너무 키워서 오버피팅(특정 데이터셋에만 맞추는 현상)이 일어난 것이 아닐까라고 생각
            모델에 제약 조건을 주면 더 일반적인 휴리스틱(경험적 규칙)을 학습하게 유도할 수 있지 않을지 궁금
            제약 없는 AI라면 결국 최적의 스피드런 기록을 재생하기만 하는 식일 텐데, 새로운 콘텐츠를 만났을 때는 다양한 휴리스틱이 훨씬 중요하다고 강조
          + 언급된 주제가 Meta-Reinforcement Learning(메타 강화학습) 분야임을 밝힘
            John Carmack이 이 분야를 탐구하는 건 의미 있지만, 전혀 새로운 연구 주제가 아니라고 알림
            Meta-Reinforcement Learning 간단 소개
          + 모델이 ‘진짜 지능’을 가졌는지의 문제는 AGI(범용 인공지능)를 고민하는 학계 입장에선 재미있는 주제이지만, 실제로 LLM을 유용하게 쓰는 다양한 사용자는 그리 중요하지 않다고 밝힘
            지금의 발전이 AGI로 이어지는지 신경 안 쓴다는 입장
            만약 Claude 4에서 멈춘다 해도 계속 유용하게 쓸 것이라 공유
            AGI 논쟁 대신 사람들이 요즘 실제로 AI를 어떻게 활용하는지가 훨씬 더 흥미로운 주제라고 강조
     * 지금 우리가 AI 초창기 시대를 살고 있다 자신 있게 이야기
       언어(LLM: GPT-4, Claude)와 시각(CLIP, DALL·E) 두 분야에서 AI가 경이적 진전을 보였음을 예시로 설명
       컴퓨터가 시와 코드 생성, 사진 설명, 인간 수준 대화까지 하지만, 사실 텍스트와 이미지 두 가지 모달리티만 확장했을 뿐임을 지적
       인간 지성은 촉각, 미각, 후각, 움직임, 감정 등 다양한 감각이 풍부하게 얽힌 다중모달 특성을 지님
       LLM이나 Vision Transformer가 이런 요소를 거의 구현하지 못함
       진짜 AI의 프론티어는 일상 삶 속의 복잡하고 풍부한 감각 세계임을 강조
       이를 위해 새로운 센서, 토큰을 넘어서는 데이터 표현법, 경험 기반으로 학습하는 새로운 모델 훈련법이 필요함을 설명
          + 언어나 시각 분야가 인공지능 본질의 시작점일 뿐이라는 의견에 정중히 반박
            촉각이 흥미롭긴 하지만, 온라인상 모든 상호작용에 충분한 것은 오디오와 비디오, 언어라고 주장
            인간과 동물의 결정적 차이는 '남은 감각'이 아니라 음성, 이미지, 언어에 있다고 설명
            현실 세계 행동을 위해 촉각·자세감각·후각 통합이 중요하지만, 지능 자체의 핵심은 언어와 시각이라는 관점
          + 유기적 적응성 및 기억의 지속성이 가장 진보해야 할 두 가지라고 생각
            인간 두뇌는 동적으로 구조가 변하는데, LLM은 고정되어 있고, 주어진 정보를 반복 학습해야 비로소 '배우는' 구조임을 지적
            인텔리전트 머신을 만들려면 스스로 실시간 학습하고 정보를 기억할 수 있어야 한다고 강조
          + 우리가 현재 가진 AI 아키텍처에서는 언어와 시각이 끝일 수도 있다는 관점
            최근 몇 년 LLM에 대한 뉴스가 많았지만, 그 밖의 AI 분야에서 두드러진 돌파구는 거의 없는 상황임을 제시
          + 진짜 AI 발전의 미래는 인간처럼 감각이 풍부하고, 물리적 세계에 얽힌 삶 그 자체임을 강조
            이미 닥터후(Dr. Who)에서 다렉(Dalek)이 두뇌를 가진 기계가 아니라 아예 기계 그 자체라고 말한 것처럼, 인간 역시 몸 전체가 곧 자신임을 비유로 설명
          + ‘믿기 어려울 정도의 발전’을 언급한 내용에 대해, 1970년대 사장됐던 기술을 100만 배 강력한 컴퓨터에 적용한 것뿐이라는 냉소적 시각
            앞으로 성능이 기하급수적으로 상승할 만한 모델 구조나 계산 방식의 근본적 혁신이 특별히 보이지 않는다고 밝힘
     * 과학적 진보와 기술적 진보의 혼동에 대해 언급
       과학이 진보할 때 S-커브 식으로 급격히 발전하다가 이후에는 점점 수익 감소 구간에 진입한다고 설명
       빠른 최적화 구간과 둔화기 구간을 구분하지 못하는 현상을 지적
          + 단순한 과장이나 기대감을 ‘기술적 진보’라고 부르는 건 너무 관대하다고 꼬집음
          + S-커브와 지수함수의 차이를 사람들이 잘 모른다는 생각을 덧붙임
            특정 구간에서는 거의 똑같이 보일 수도 있음을 설명
     * DeepSeek가 특별히 언급되는 이유에 의문
     * 모델 아키텍처 관련 연구와 논문을 읽는 입장에서는, 수많은 새로운 아이디어가 쏟아지는 중이라는 점을 지적
       다만 정말 흥미로운 결과를 내는 것은 일부에 한정됨
       PyTorch와 같은 라이브러리가 실험적 개발을 저해하는 영향도 있을 것으로 추측
       기본적인 구성 요소를 그냥 가져다 쓰는 일이 너무 당연해진 결과, 각 요소에 대해 깊이 고민하지 않게 된 부분도 있다고 판단
       ‘모델 카드’에 체크하기 위해 타인이 만든 토크나이저나 비전 모델을 무조건 덧붙이는 경향에 의구심을 가짐
          + 이런 흐름은 인간 세계에서도 너무나 흔하고 자연스러운 패턴임을 설명
            현재 기반 기술에서 지적 탐구의 ROI가 떨어지면 잠시 인적 자원이 다른 곳으로 쏠리기 마련
            하지만 한계에 다다르면 결국 혁신적 인재들이 다시 근본적인 영역에서 큰 진보를 만들어낼 것이라 전망
            PyTorch와 같은 foundational tech의 다음 세대 역시 이런 식으로 진화할 것이라고 봄
          + 실제 최근 2~3년 동안 정말 많은 사람들이 알고 일상적으로 쓰게 된 대규모 아키텍처 개선이 있었던 건 드물지만, 3년이라는 짧은 시간적 관점 자체를 간과하는 경향도 있다고 지적
            LLM 외에도 여전히 다양한 재미있고 유용한 연구가 진행 중이고, 자신은 그 분야 전문가도 아니지만 엄청나게 다양한 새로운 시도가 쏟아진다고 느낌
          + PyTorch가 없었더라도 새롭게 실험할 생각이 없는 사람은 여전히 그런 성향을 보였을 것이라고 이야기
     * 인간 수준의 지성을 복제한 시스템을 상상하면, 모델 차이의 핵심이 '데이터셋 변화'에 있다고 볼 수 있다는 시각
       실제로 인간의 기억, 교육, 배경 등이 문제해결 능력의 큰 부분을 차지하므로 유사점이 있음
     * 모델이 능동적으로 데이터를 얻는 방식, 즉 스스로 데이터를 찾아 학습하는 가능성에 대해 궁금
       인간 아기처럼 다양한 행위를 하며 직접 경험을 통해 배우는 방식이 필요함을 제안
       현재는 데이터만 계속 주입하는 상태에서 벗어나, 예를 들어 3D 오브젝트를 만들 수 있으니 물리 시뮬레이터와 결합하는 것도 좋은 방향이라고 언급
       Cursor를 예로 들며, 규칙설정 뒤 reasoning model로 해당 이유를 추론하고 학습 데이터에 반영하면 데이터의 가치를 한층 더 높일 수 있다고 제안
       사용자 행동 선택의 이유를 돌이켜보고 훈련 데이터화하면 더 깊은 통찰력 확보 가능성
          + 시뮬레이션 및 로봇암, 자동차 같은 ‘embodied AI’(체화된 인공지능)가 활발히 연구 중임을 알림
          + 그 방식이 곧 강화학습에 해당하며, 실제로 쉽지 않은 분야임을 언급
     * 새로운 아이디어란 대부분 과거 아이디어로부터 출발한다는 의견을 제시
       AI는 오래된 아이디어에 더 빠르고 새로운 시각으로 접근하는 도구
       혁신은 오래된 아이디어 속 빈틈이나 교차점에서 비롯된다는 점, 그리고 혁신이란 결국 선인들의 업적 위에서 일어나는 것임을 강조
       AI를 거인의 어깨로 바로 데려다주는 엘리베이터로 볼 수 있으며, 결국 도구의 활용 여부에 달려 있음을 설명
          + 옛 아이디어를 접근하는 데 동의하지만, 새로운 시각 자체로 접근하는 데는 한계가 있다고 논함
            LLM이 데이터 해석에 일부 보탬은 되지만, 기존 연구 대비 완전히 새로운 아이디어를 창출해내는 데는 아직 부족
            LLM 활용이 연구의 일부 특수 영역은 가속화할 수 있지만 그 외 영역은 한계가 있다고 설명
          + 모든 지식을 섭렵한 인간이 완전히 새로운 아이디어를 내지 못하는 상황을 상상하기 어려운 예시로 말함
          + 글의 논점이 AI 혁신 자체와 데이터의 양·품질 향상에 관한 논의임을 강조
            근본적 혁신이 있긴 했으나, 가장 좋은 성능 개선 방법은 여전히 더 많고 품질 높은 데이터라고 주장
            ‘더 많은 데이터 → 더 깊은 모델 → 다시 반복’이라는 AI 발전의 순환을 예로 듦
            자신의 의견이 이런 관점과 어떻게 연결되는지 의아함
          + 모든 새로운 아이디어가 옛 것에서만 비롯된다는 주장에 벤젠고리의 발견 일화를 들어 반박
            벤젠고리 구조는 꿈속에서 뱀이 자신의 꼬리를 무는 형상(‘우로보로스’)으로 처음 아이디어를 얻는 등, 전에 없던 상상력이 종종 혁신의 원천임을 예시
     * 최신 LLM은 결국 숫자를 더하고 곱하는 행위임을 이야기
       바빌로니아인들이 4000년 전부터 해오던 방식이라고 극단적으로 표현
          + 인간 역시 파동의 상호작용 결과일 뿐이며, 모든 의미는 결국 부여되는 것임을 설명
            개념 공간에 인덱스를 다는 방법만 있다면 의외성을 탐색할 수 있다는 점에서, 가능성은 무한함을 밝힘
          + 바빌로니아인들은 점토판에서 했고, 현대는 원자 두께 벽을 가진 반도체 안에서 처리한다는 점을 비교
            방법에 있어서는 큰 차이가 있다는 의견을 제시
     * 현재 AI 훈련이란 실제로 데이터셋을 암기시키는 과정에 가깝다는 생각
       데이터에 대해 스스로 사고하고 결론을 유도·기억하는 방식이 아니라는 점을 강조
       주어진 주제의 ‘사실’만큼은 PhD보다 더 많이 알 수 있겠지만, 그 내용을 사고하는 데는 인간이 여전히 뛰어나다고 평가
          + 그래서 박사도 교과서를 가까이 두는 것이 아닐지 추측
            굳이 AI 모델이 이미 기록된 사실까지 모두 기억할 필요가 있겠냐고 반문
          + 실제로는 조금 더 복잡한 과정임을 설명
            입력에 적절한 대응을 할 수 있도록 데이터를 휴리스틱 형태로 내재화하는 것이라 평가
            이 휴리스틱이 인간에게 놀라움을 주기도 하고, 때론 참신한 문제 해결을 보여주기도 함
            ‘사고’란 개념 자체가 너무 넓어 판단이 어렵지만, AGI에는 아직 멀었다고 진단
          + ‘주어진 주제에서 PhD보다 더 많은 사실을 기억한다’는 설명을 노트북과 비교
            노트북도 사실을 더 많이 담을 수 있지 않냐고 짚음
"
"https://news.hada.io/topic?id=21756","TokenDagger – OpenAI의 Tiktoken보다 더 빠른 토크나이저","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TokenDagger – OpenAI의 Tiktoken보다 더 빠른 토크나이저

     * OpenAI의 TikToken과 100% 호환되는 고성능 토크나이저로, 대규모 텍스트 처리에서 2배 이상 처리량과 4배 빠른 코드 토큰화 속도를 제공함
     * PCRE2 기반 고속 정규표현식 파싱 엔진을 통해 토큰 패턴 매칭 속도 극대화
     * 간소화된 BPE 알고리듬으로 대용량 스페셜 토큰 처리 시 성능 저하를 최소화
     * 실제 벤치마크에서 코드 토큰화가 4배 이상 빠르며, 기존 TikToken 사용 코드를 그대로 교체해 활용 가능함
     * Python 3.8+ 지원, PyPI pip install tokendagger 로 간단히 설치 가능하며 PCRE2 의존성을 가짐

        Hacker News 의견

     * 단기간에는 AI/ML 인프라 구조에서 이처럼 C++로 핵심 병목을 해결하는 방식으로 많은 성능 최적화 여지가 있다고 생각함, 전체를 C++로 다시 쓰는 건 아니고 현명한 엔지니어링 트레이드오프가 실제 성능 향상으로 이어지는 경우가 많음, 특히 중국 엔지니어들이 이런 작업을 잘하는 느낌임
          + 내 멘토가 가르쳐 주던 소프트웨어 개발의 관점이 있음: 1단계 일단 동작, 2단계 빠르게, 3단계 예쁘게, 트랜스포머와 LLM들은 이제 꽤 잘 동작하는 단계까지 왔으니, 지금은 성능 향상 쪽에서 가장 큰 진전이 이뤄지는 시기라고 느낌
          + 장기적으로는 Python 중심에서 벗어나는 게 의미 있을 거라 생각함, 단순히 ML 엔지니어들이 익숙해서 쓴다면 미래 지향적이지 않음
          + 실제로 TikToken은 Rust로 작성되어 있어서, 이번 개선이 정말 C++ 이식 덕분인지는 궁금증이 있음
          + 사실 토크나이징이 제일 큰 병목이 아니고 대부분의 계산은 실제 CUDA 커널 실행에서 발생함, Python 오버헤드는 매우 적음(VLLM도 주로 Python으로 작성됨), C++로 다시 쓴다는 건 거의 항상 CUDA 커널을 더 효율적으로 다시 짜는 걸 의미함
     * 기존 시스템을 성능을 크게 개선하는 드롭인 교체로 만든다는 게 상당히 아름다운 일이라고 느낌, ScyllaDB가 생각남
          + 실제로 이런 대체제가 아니라면 아무도 사용하지 않을 거라고 생각함
     * LLM의 전반적인 성능에서 토크나이저의 중요도가 궁금함, 토크나이저 최적화에 관심이 있는데 실제 측정은 아직 안 해봄, 대부분의 비용이 matmul에 의해 발생하는 것으로 추정했지만, 댓글 반응을 보면 토크나이저가 의미 있어 보임
          + 토크나이징은 보통 CPU에서 처리되고, 트레이닝이나 인퍼런스의 병목이 되는 경우는 드묾, 대부분의 시간은 GPU 커널에서 사용되고 매우 작은 모델만이 예외임, 토크나이저의 지연 시간은 CPU가 다음 배치를 준비하는 방식으로 ‘숨길 수도 있음’
          + 토크나이징 성능은 좀 복잡하지만, 진짜 실력과 자원을 가진 기관들은 매우 빠른 토크나이저를 직접 작성함, SentencePiece와 tiktoken이 대표적으로 복잡도와 배포상의 부담을 감수하고서도 선택됨, 실제 전문가들은 플레임 그래프를 통해 병목을 살핌, 대규모 러닝에서는 미리 토크나이징을 하기도 함, C++이 Rust 내러티브와 달리 다시 부상하는 것에 대한 업계 긴장도 느껴짐, 새로운 이유나 인사이트가 있기를 바람
          + 다른 코멘트들과 마찬가지로 실제로 토크나이저가 병목은 아님, 다만 추론 파이프라인에서 첫 단계라서 먼저 작업한 것임
          + 텍스트 토크나이징은 전체 계산 중에 정말 미미한 비중임, 그렇지만 페타바이트급 데이터 처리에는 조금이라도 더 빠르면 무조건 좋음
     * tiktoken에 맞추려면 vocab 포맷 변환이 필요한데, 이 요구도 없어질 수 있으면 쓸 때 더 좋은 완전 호환 드롭인 대체제가 될 것임, 또한 tiktoken 초기화 후 tokendagger를 초기화해서 결과가 같은지 확인할 수 있는 양방향 예제도 있었으면 함
          + 0.1.1 버전부터는 진정한 드롭인 대체제가 됐음, 곧 예제도 추가할 계획임
          + 포인트를 잘 짚어줘서 바로 업데이트함
     * 이 프로젝트가 BPE crate와 어떻게 비교되는지도 궁금함, 해당 crate의 강점은 텍스트를 점진적으로 재토크나이징하는 기능이고 tiktoken보다 빠름
          + 다음엔 점진적 재토크나이징 기능을 추가하고, 해당 crate와 벤치마크도 진행할 계획임
     * 다른 LLM용으로 로컬 토크나이저를 구할 수 있는 방법이 궁금함, 예를 들어 Gemini는 원격 API만 공개함, 이게 독점적인 건지 아니면 API를 대량 호출해서 토큰 매핑을 추정할 방법이 있는지 궁금함
          + Gemini는 SentencePiece를 쓰고, Gemma와 동일한 토크나이저 vocabulary를 공유함 (참조1, 참조2, 참조3), 대형 랩 중 Claude 3 이상을 쓰는 Anthropic만 로컬 토크나이저가 없음
          + 모델별 토크나이저는 대부분 SentencePiece나 Byte-pair encoding(BPE) 같은 코어 알고리즘을 공유하면서 래퍼 수준에서 특수 토큰 처리 등만 다름, tiktoken과 TokenDagger도 BPE 구현임, 라이브러리 차원에서 모델별 특징을 반영해주면 소폭 성능 향상과 통합이 쉬워짐, 큰 일이 아니니 신모델에 맞추기에도 부담이 적음, 참고 예시는 llama의 tokenizer.py, Mistral 토크나이저
          + Gemini가 SentencePiece를 쓴다고 알고 있었음 SentencePiece
     * 예전에 비슷한 걸 시도한 경험이 있음: tokie, 실제로는 토크나이저 러닝 비용 대다수가 프리토크나이징(정규표현식)에서 발생함, 더 빠른 regex 방식을 찾아낸 것 같은데 실제로 regex 엔진만 바꾸고 BPE는 tiktoken 그대로 뒀을 때 성능 변화도 비교해봤는지 궁금함, 만약 그렇다면 upstream 기여도 가능하지 않을까 싶음
          + 멋진 프로젝트임, tiktoken 관리하는 분에게 연락해둠
     * Huggingface tokenizers와의 성능 비교도 했으면 좋겠음, tiktoken readme 기준 벤치마크가 너무 오래된 정보임
          + 개인적으로 tiktoken이 huggingface tokenizers보다 항상 더 느렸음, 왜그런지는 아직 tiktoken을 깊게 파보진 않았지만 HF Rust 토크나이저 사용자 입장에서 그렇게 느낌
     * tiktoken WASM 바인딩도 보고 싶음, 아니면 js 순수 구현에서 ""b""에서 나온 성능 향상도 적용할 수 있을지 궁금함
     * 정말 멋진 프로젝트임, 우리도 tiktoken을 쓰고 있는데 드롭인 호환이면서 성능 향상이 된다면 효과가 궁금함, 드롭인 방식 선택이 훌륭함
"
"https://news.hada.io/topic?id=21830","너무 큰 야망은 영리한 자기파괴의 한 형태임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        너무 큰 야망은 영리한 자기파괴의 한 형태임

     * 크리에이티브 작업은 머릿속에서 완벽하게 상상하지만 실제로 시작하면 이상적 이미지가 사라짐
     * 인간은 '테이스트-스킬 불일치' 로 인해 상상력과 현실 사이 괴리감을 경험함
     * 실패와 반복적 시도를 통해서만 진정한 마스터리가 쌓임
     * 뇌는 계획만으로도 성취감을 느끼기 때문에 실질적 행동 대신 준비와 망상에 머물기 쉬움
     * 성공은 완벽함을 추구하기보다 시도와 실행을 반복하는 과정에서 탄생함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

상상과 현실 사이의 간극

     * 창작을 시작하기 전, 작업은 머릿속에서 가장 완벽한 형태로 존재함
     * 이 시점에서는 모든 것이 의도적이고 완결된 아름다움을 띠지만, 실제로 시작하는 순간 그 완벽함은 사라짐
     * 창작은 탄생이 아니라 불가능의 살해로, 실현 가능한 것을 위해 불가능한 것의 아름다움을 포기하는 행위임
     * 인간은 아직 현실화하지 않은 아이디어를 이상적으로 바라보며, 이는 실현하지 않은 프로젝트를 숭배하는 태도로 이어짐

비전의 저주와 성장의 간극

     * 인간은 상상력의 저주를 앓는 유일한 종임
     * 아이들은 초기에 자신감 있게 그리고 만들지만, 약 8~9세가 되면 통찰력(테이스트)이 생기면서 '테이스트-스킬 불일치' 라는 괴리감을 느끼기 시작함
     * 이 괴리감은 대다수 사람들이 창작을 멈추게 만드는 원인임
     * 자신의 부족함을 심각하게 인식하게 되고, 이를 견디기 어렵기 때문에 생산적 회피(productive avoidance) 라는 전략을 무의식적으로 개발함
          + 계획 세우기, 정보 조사, 연구 등으로 바쁘게 지내면서 실제 창작은 회피함
     * 생산적 회피는 지적 활동처럼 느껴지지만 실제로는 불완전함을 피하기 위해 창작 자체를 미루는 결과임
     * 반면 거미나 새는 본능적으로 작업을 반복하며, 인간처럼 상상과 현실의 간극에 시달리지 않음

'최고는 좋은 것의 적'이라는 일화

     * 플로리다 대학의 사진 강사가 학생을 두 그룹으로 나눔
          + 수량 그룹: 사진 수로 평가, 많이 찍을수록 높은 점수
          + 품질 그룹: 단 하나의 완벽한 사진만 제출
     * 학기 말, 최고의 사진은 모두 수량 그룹에서 나옴
     * 반복적 시행을 통해 실패와 불완전함을 경험한 학생들이 진정한 실력과 창의성을 얻음
     * 품질 그룹은 이론과 계획에 치중해 실제 노하우는 얻지 못함
     * 실전 경험과 실패와의 친밀감이 진정한 마스터리로 이어짐

두뇌는 성취를 착각함

     * 목표를 시각화할 때 두뇌에서 실제 성취와 같은 보상 시스템이 활성화됨
     * 이로 인해 계획만 세워도 실제 성취한 것 같은 착각에 빠지기 쉬움
     * 이러한 신경학적 특성은 이미 숙련된 능력을 강화할 때는 긍정적 효과가 있음(예: 운동선수의 시각화 훈련)
     * 하지만 연습과 반복 대신 상상만으로 대체할 경우 실제 발전을 가로막는 함정이 됨
     * 예비 저자는 완벽한 초안을 상상하거나 연구에만 몰두하고, 뇌는 이에 대해 이미 무언가 이룬 것처럼 착각함

즉각적 완성의 환상과 알고리듬

     * 알고리듬 중심의 플랫폼이 마스터리의 과정을 일상에서 지움
     * SNS는 결과물과 성공만 보여주고 수많은 시행착오와 실패는 소외시킴
     * 이로 인해 우리는 배움과 성장도 즉각적이고 꾸준하게 나타나야 한다는 잘못된 생각을 갖게 됨
     * 진정한 걸작은 수많은 하위작업, 실패, 연습에서 비롯됨
     * 지나친 야망은 이 생태계를 무너뜨리고, 사회는 초보자의 특권을 저해하는 환경을 조성함
     * 어린아이의 창작은 순수한 기쁨에서 시작되며, 목적이 아니라 발견과 실험에서 의미를 찾음

'Do-Learn' 철학과 시행착오의 힘

     * Olin College of Engineering의 모토는 'Do-Learn' 임
     * 실제로 해보면서 경험을 통해 배우고, 부족함을 겪으며 성장하는 철학임
     * 완벽히 준비되기 전 시작할 수 있는 용기, 그리고 반복적 실패를 통한 학습을 장려
     * 요리, 외국어, 유튜브 활동 등에서 이 방식을 적용하며, 준비에 너무 집착하지 않고 빨리 실행해 현실에서 피드백을 얻음
     * 실행을 통한 학습은 주저나 준비 과정에서 결코 얻을 수 없는 실제 성장과 통찰을 제공함

'포기 지점'을 넘기는 힘

     * 막상 시작하더라도 포기 지점(quitting point) 에서는 누구나 어려움을 겪음
     * 초기 의욕과 달리, 어느 순간 작업은 힘들고 지루해지며 진짜 도전이 시작됨
     * 수량 그룹은 이미 실패를 친근하게 경험했기에 데이터를 통해 지속적으로 개선함
          + 과업 지향적(task orientation) 접근을 하며, 작업 그 자체의 발전이 목적임
     * 품질 그룹은 완벽한 계획만 세웠기에 시행착오를 실패로 간주하고 쉽게 포기함
     * 창작 프로젝트의 진짜 분수령은 이 포기 지점에서 결정되고, Failure는 진정한 작업의 시작점임
     * 이 과정은 상상에서 실물 창작으로, 계획에서 실행으로의 전환을 의미함

기대치를 낮추고 시작하기

     * 최고의 결과는 종종 실패를 허락받은 수많은 시도에서 시작됨
     * 부담감이 낮아지면 현실과의 대화가 가능하며, 현실은 항상 새로운 방향과 우연한 성과를 제시함
     * 사진작가가 백 장의 사진을 찍어 경험을 쌓듯, 실행 과정 속에서 기준이 형성됨
     * 반복과 시행 덕에 작가, 기업가, 예술가는 실질적 통찰과 판단력을 얻게 됨
     * 저자 역시 한 번의 큰 성공 이후 다시 기대에 짓눌릴 뻔했으나, 꾸준함과 반복이 진정한 성공의 비결임을 깨달음
     * 성공을 반복할 수 있다는 환상보다, 그냥 계속 시행하고 실패를 정보로 받아들이는 꾸준함이 중요함

창작의 진정한 의미와 인류의 특권

     * 걸작은 처음부터 완벽하게 탄생하는 것이 아니며, 꾸준한 시도와 점진적 개선에서 비롯됨
     * 실패와 시도에 대한 두려움을 내려놓고, 기대치를 낮춘 채 과정 자체를 즐기며 배워야 함
     * 인류는 비전과 창의력이라는 축복과 저주를 동시에 가졌으나, 그로 인해 전혀 새로운 것을 만들어낼 수 있음
     * 완벽을 향한 수많은 불완전한 시도가 결국 현실과 이상 사이의 격차를 좁힘
     * 작업은 이미 여러분을 기다리고 있음, 기대를 낮추고 바로 시작하는 자세가 중요함

        Hacker News 의견

     * 나 자신도 AI를 다룰 때 자주 느끼는 '취향-스킬 격차'라는 개념이 크게 와닿는 경험이 있음
       즉, 새로운 기술을 시도할 때 AI를 사용하면 내 취향의 기준선만 즉시 올려주고 실제 내 실력은 향상되지 않는 상황
       실수를 통해 배우는 과정 없이, 마찰 없이 뭔가를 해내버릴 수 있기 때문에 배움의 속도를 느낄 수 없음
       특히 초보자의 경우, 이 격차가 창작자와 소비자를 가르는 요인이라고 생각함
          + 그래서 개발자 대상 스타트업을 다시 시작하는 것이 항상 경계되는 이유를 설명
            내가 만드는 결과물이 내 눈높이에 미치지 못하는 좌절감
            ""개발자가 원하는 건 내가 알아, 그래서 만들 수 있어""라는 생각이 실제로는 내 기준이 높을수록 실행력은 낮아지는 역설
            대학생 시절 거의 아무것도 모르고 2주 만에 대충 만든 서비스가 월 2만 불까지 찍었던 경험이 있음
            그러나 시간이 흐르면서 실제 품질에 대해 깊이 알게 될수록 오히려 결과물을 내놓지 못하게 되는 패턴을 반복
            완벽주의와 유사하지만, '최선'이 '좋음'의 적이 되는 본질적 병리 현상에서 기인
          + 나는 모든 genAI가 취향은 없고 스킬만 있다고 자주 느꼈음
            AI는 사용자의 기술적 하한선을 즉시 올려주지만, 취향이나 감성은 주지 못함
            이게 바로 이해하고 있던 취지인지 궁금
            혹시 정반대로 표현한 것 아닌지 혼란스러운 감정
          + Ira Glass의 철학을 모르지만, 취향과 스킬의 의미가 의도적으로 반대로 쓰인 것 같다는 느낌
            대규모 언어모델(LLM)은 대량 데이터가 있어야 잘함
            시간이 지날수록 NextJS, Golang, Docker와 같이 획일화된 결과물이 늘어나는 효과를 예상
            Claude와 Gemini가 Bazel에 대해 다른 수준을 보여주는 것도 흥미로운 차이
            컴퓨터에게는 기술적 실행이 문제가 아닌 경우가 대부분이며, 오히려 취향 혹은 감성의 문제라고 생각
          + Rick Rubin이 바로 이런 예시
            취향 점수는 100/100인데, 스킬 점수는 거의 0/100임
            악기를 다룰 줄은 전혀 모르지만, 무엇이 좋은지에 대해서는 정확한 판단과 표현이 가능
          + Ira Glass의 '취향 격차'의 본래 의미가 이와는 다르다고 생각
            오히려 취향이 필드에 뛰어들고 그곳에 오래 머물게 해주는 핵심 요인이라는 의미
     * 독립기념일 시즌을 맞아 John Lewis Gaddis의 저서 ""On Grand Strategy""에서 Lincoln과 John Quincy Adams를 비교한 대목을 상기
       Adams는 타인의 기대와 자신의 지나친 목표로 인해 일상적 상식과 실행력을 잃는 반면
       Lincoln은 외부 기대에서 벗어나 서두르지 않고 때가 무르익을 때 스스로 세운 목표에 도전
       요점은, 지나친 야망은 전략적이지 않으면 오히려 발전을 늦춘다는 교훈
          + 결국 자명한 이야기처럼 느껴짐
            99.9%의 사람은 99.999%의 인생을 살아낼 수 없는 것, 만약 성공하면 이미 그 자체로 특별함
     * 전략적으로 접근한다는 건, 목표에 도달할 계획을 너무 세밀하게 세우고 리스크도 모두 제거하다 보면, 실제로 그 여정이 더 이상 흥미롭지 않게 변하는 현상
       아이디어가 멋진 여정이 아니라, 결국 귀찮은 루틴 작업이나 패키지 관리로 바뀌는 순간
       FPV드론으로 정말 여러 번 촬영해서, 실제로 가서는 새로움을 발견하지 못하는 것과 비슷한 느낌
       이때 흥미를 유지하려고 계속 기준을 올리거나, 더 멋지게 만들려고 욕심내게 됨
       Rust로 짜볼까, 무한 확장성을 목표로 할까, 수백 개의 신규 탭 등
       이렇게 전략적 사고는 실행계획의 완성도는 올리지만, 실제로 외부 자극이나 시각 전환 없이는 실행을 못하는 루프에 빠지기 쉬움
          + 멋진 아이디어가 귀찮은 일이 되는 순간, 아직도 실현 중인 내 프로젝트에 대해 무기력함을 느끼는 경보 발생
            실제 삶에서는 누군가를 감동시키려 하면 오히려 어색하거나 불편한 기억이 쌓인 경험
            소프트웨어도 똑같이, 단순하게 만든 것이 가장 어렵고, 그래서 대중적으로는 별로 인상적이지 않기 마련
            Picasso도 비슷한 평가를 받았던 것처럼
            결국 남은 90%를 끝내고, Antoine de Saint-Exupéry 말대로 더할 게 아니라 뺄 게 없을 때가 완성의 순간
            버전 1.0을 찍고 유지보수 모드로 넘어간 다음, 다음 프로젝트에서 또 다른 니즈를 해결하는 패턴
          + '멋진 아이디어를 번거로운 일로 전락시켰다'는 표현이 딱 내 느낌
            전략적 사고보다는 실행, 유지보수, 단계별 실천, 이슈 트래킹, 관점 전환 등에 더 큰 찬사가 필요하다고 생각
            많은 사람들이 전략적 사고를 못할뿐더러, 막상 실전 문제를 제기하면 방어적으로 나오는 경우 많음
            실제 전략가라면 이런 실전 문제를 이미 반영했어야 한다는 지적
     * 어떤 창작 활동도 짐처럼 느껴진다면, 그 이유와 동기를 다시 돌아보라고 권유
       결과가 아니라 과정 자체를 즐기는 것이 핵심
       Ira Glass가 언급한 '격차'는 오히려 그러한 동기 자체에 불을 붙여주는 연료 같은 역할
     * 위 얘기가 공감된다면, ""The Problem of the Puer Aeternus""를 추천
       저자가 다른 책 문구를 많이 인용해서 지루한 부분을 건너뛰어도 패턴 자체는 익숙할 것
       대담하거나 큰 목표 대신, 사소한 다음 단계를 실행하는 일이 스스로에 대한 선입견과 싸워야 하는 과제라는 점
     * 한 가지에 정말 잘하고 있음에 감사하지만, 일부러 잘 못하는 분야에도 도전하면서 능숙해야만 한다는 압박 없는 순수한 행동의 즐거움을 느낌
       초보자로 실패하는 감각이 새로움을 줘서, 본업에서 어려움을 느낄 때도 더 넓은 시야를 가질 수 있음
       과거의 큰 성취도 정말 많은 작은 시도와 수많은 미세조정의 결과였지, 완벽한 계획의 산물은 아니었음
     * 글의 첫 부분에서, 내가 실제로 오랫동안 연구나 취향 개발만 하고 '실행'을 미루면 점점 창작자가 아니라 비평가가 되어간다는 자각이 떠올랐음
       취향 발달이 빠른 사람들은 결국 스스로에게, 나중에는 타인에게도 비판적이 되어감
       비평가가 모두 실패한 창작자라고 일반화하고 싶진 않지만, 내겐 그 말이 맞았음
       그걸 되돌리고 싶어하는데 이 글이 딱 내가 듣고 싶던 이야기였음
       문장 하나하나에 깊이가 있고 진짜로 잘 썼다는 생각
       대부분 스크린샷 공유만 노리는 글과 달리, 알찬 내용이라 감명 깊은 독서 경험
       (이걸 쓰면서도 어느새 비평가 모드로 빠지는 내 모습)
          + Theodore Roosevelt의 ""arena speech""가 비평가의 시선이나 피드백에 휩쓸릴 때 큰 힘을 줌
            진짜 중요한 건 아레나에서 직접 땀과 먼지를 흘리며 도전하는 사람임
            실수, 실패를 두려워하지 않고 끈질기게 임하는 태도가 가치
            실패하더라도 위대한 도전에 나선 사람만이 진정한 승리 혹은 값진 패배를 경험한다는 시각
     * 심리학에서는 이런 증상을 '영원한 아이(puer aeternus) 증후군'이라고 부름
       어릴 때 큰 기대와 잠재력을 지녔던 사람들이 현실의 결단이나 시도를 기피하는 패턴
       잠재력 자체를 자신의 최고 가치로 여기고, 어떤 결정이든 그 무한한 잠재력의 일부를 상실하는 두려움
       결정을 내릴 때마다 그 가능성이 평범함, 그리고 그 평범한 완성에 대한 두려움으로 실행을 미루게 됨
       스스로 엄청난 재능이 있다고 믿지만 이를 실제로 실행에 옮기지 않으니 '평범함'을 더 두려워하게 됨
       상황이 조금만 어긋나도 자꾸 미루고 실패, 주변이나 환경의 탓으로 돌림
       새로운 생산성 해킹이나 동기 부여법을 찾지만, 어느 분야든 반복적이고 지루한 부분은 피할 수 없다는 현실
       '평범함'을 자기 정체성의 부정으로 여기며, 문제가 있는 천재로는 받아들일 수 있어도, 평범한 '그냥 그런 사람'만은 되고 싶지 않은 공포가 강함
          + 그럼 여기서 배울 교훈은 뭔가?
            자존심을 내려놓고 일단 무언가를 '내놓는 용기'가 필요한가 하는 질문
     * 무엇이 지나친 야망인가에 대한 의문
       생각만 하고 실행하지 않는 몽상가도 많지만
       남들이 불가능하다고 하는 일도 실행해내는 실천가도 분명 있음
       야망 자체는 결정적이지 않고, 실은 움직이는 사람과 말뿐인 사람의 차이
          + Berserk의 Griffith도 지나친 야망의 상징이지만 결국 성과를 내는 케이스로 볼 수 있음
          + 야망이라는 단어에는 다양한 뉘앙스가 담김
            자신의 야망으로 목표를 설정하고 수년간 등반 훈련하며 마침내 에베레스트에 오르는 실천가와
            야망을 자랑만 하며 하찮은 목표를 실행할 가치조차 없는 듯 꾸미는 이들로 구분
            실제로는 후자들이 특별한 성취 없이 편법이나 우회적인 방법을 더 많이 사용
     * 게으름도 일종의 생산성 추구 방식이라는 관점
       ""힘든 일을 시킬 때 나는 게으른 사람을 고른다. 게으른 사람은 쉬운 방법을 찾아내기 때문이다""
       — Bill Gates 링크
"
"https://news.hada.io/topic?id=21728","Octelium - 오픈 소스 Teleport, Cloudflare, Tailscale, Ngrok 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Octelium - 오픈 소스 Teleport, Cloudflare, Tailscale, Ngrok 대체제

     * Octelium은 원격 액세스 VPN, ZTNA, API/AI 게이트웨이 등을 통합 지원하는 차세대 오픈 소스 자체 호스팅 플랫폼
     * 자체 호스팅 및 단일 테넌트로 모든 내부·공개 자원에 대한 정체성 기반, 응용계층(레이어 7) 보안 접속 제공
     * 비밀 없는(secret-less) 접근, 세분화 정책 기반 제어, 중앙 관리 및 감사 등 최신 보안 요구 충족 기능 포함
     * 기존 Kubernetes, OpenVPN, Tailscale, Cloudflare Access 등 다양한 상용/오픈 소스 솔루션과 경쟁 우위 제공 및 대체 가능
     * 오픈 소스 모델을 채택하고 상용 기능 지원 및 라이선싱으로 사업 기반을 마련하며, 풀 기능 자체 호스팅 제공에 중점 둠
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Octelium 프로젝트 중요성 및 개요

     * Octelium은 Teleport, Cloudflare, Tailscale, Ngrok과 같은 상용 솔루션을 대체할 수 있는 차세대 오픈 소스 통합 Zero Trust 자체 호스팅 보안 접속 플랫폼임.
     * 기존 오픈 소스/상용 솔루션 대비 전체 기능을 손상 없이 자체적으로 호스팅 가능, 추가 비용·벤더 종속에서 자유로운 것이 큰 장점임.

Octelium이란 무엇인가

     * Octelium은 정체성 기반·레이어 7 알고리듬을 적용한 통합 접속 관리 플랫폼
     * 원격 액세스 VPN(OpenVPN Access Server, Twingate, Tailscale 등)의 대안이면서 ZTNA, BeyondCorp(Google BeyondCorp, Cloudflare Access, Teleport 등), ngrok(리버스 프록시), API/AI 게이트웨이, 자가 PaaS 인프라, Kubernetes 인그레스 대체, Homelab 인프라 등 다양한 역할에 대응함
     * 사용자(사람, 워크로드 모두)·조직·애플리케이션 접근을 위해 WireGuard/QUIC 터널 기반의 클라이언트 방식과 BeyondCorp 방식의 클라이언트 없는 브라우저 접근 모두 지원함
     * 정책을 코드로 정의하는 policy-as-code 및 세부 컨텍스트·정체성 기반 보안·비밀 없는 인증 및 권한 부여가 핵심임

주요 사용 사례

     * 현대적 원격 액세스 VPN: WireGuard/QUIC 기반, 동적·정체성 인식·응용 계층 보안
     * 통합 ZTNA/BeyondCorp 접근 구성
     * 자체 호스팅 안전 터널/리버스 프록시(ngrok, Cloudflare Tunnel 대체)
     * 자체 호스팅 PaaS(컨테이너 앱 배포·확장·익명 공개 호스팅)
     * API 게이트웨이(Kong Gateway, Apigee 대체)
     * AI 게이트웨이(LLM 프로바이더 연결·정체성 기반 제어)
     * 통합 비밀 없는 SaaS API 접근
     * MCP/A2A(모델 컨텍스트 및 에이전트 간 표준) 게이트웨이 인프라 제공
     * Kubernetes 인그레스/로드밸런서 고도화 대체
     * Homelab(개인 자원·IoT·클라우드 등 통합 안전 원격 관리)

주요 특징

  현대적 통합 아키텍처

     * 모든 자원(내부/NAT 뒤, 공개)·모든 사용자(사람/워크로드)에 대응하는 정체성 인식, 응용 계층 단위 제어
     * VPN 기반 원격 접속과 BeyondCorp 클라이언트 없는 접근 모두 제공
     * Kubernetes 상에 동작, 수평 확장·가용성 자동 내장

  동적 비밀 없는(secret-less) 접근

     * HTTP/gRPC API, 웹앱, SSH, Kubernetes, PostgreSQL/MySQL 등 다수 앱/DB에 비밀 키 관리·공유 없이 안전한 접근 지원
     * mTLS 등도 PKI/인증서 공유 없이 접근 가능

  컨텍스트 인식·정체성 기반·레이어 7 접근 제어

     * 중앙 모듈형, 컴포저블 정책 시스템(ABAC) 내장
     * CEL, OPA 등 정책 언어 지원, 모든 요청 단위 세밀한 제어 가능

  동적 라우팅/구성

     * 정책 기반 동일한 자원에 대해 각기 다른 상위 컨텍스트·계정·조건부 라우팅 가능

  지속적 강력 인증

     * OpenID Connect, SAML2.0 등 표준 IdP 연동
     * 워크로드는 OIDC 토큰으로 무비밀 인증
     * NIST 인증수준·MFA·피싱 방지(Passkey, Yubikey 등) 지원

  응용 계층 심층 가시성 및 감사

     * OpenTelemetry 통합, 모든 요청 실시간 로그화 및 외부 OTLP 수집기로 전송

  서버리스 SSH 및 컨테이너 앱 배포

     * 루트 권한 필요 없이 컨테이너·IoT·비SSH 호스트에도 SSH 접근
     * PaaS와 유사한 컨테이너 앱 배포·확장·보안 접근 지원

  중앙집중적 선언적 관리 및 코딩 가능

     * Kubernetes처럼 선언적으로 관리, 단일 명령/코드로 클러스터 상태 재현 가능
     * octeliumctl CLI 및 gRPC API로 DevOps/GitOps 친화적 운영

  네트워크 변경 불필요 및 VPN 고질적 문제 해소

     * 업스트림 자원이 Octelium 존재 자체를 알 필요 없으며, 포트 개방 없이 NAT 뒤에서 안전하게 서비스 운영 가능
     * 고유 듀얼스택 프라이빗 IP, 프라이빗 DNS 등 설정 자동화

  완전한 오픈 소스, 자체 호스팅·벤더 종속 없음

     * 전체 소스 공개, 상용 버전 기능 제한·Vendor Lock-in 없음
     * 단일 노드 미니 클러스터부터 대형 클라우드까지 확장적 구성 지원

라이선스 및 지원

     * 클라이언트 소스는 Apache 2.0, 클러스터는 AGPLv3
     * 상용 라이선스 및 엔터프라이즈 지원, 외부 기여는 현재 제한됨
     * 공식 문서, Discord, Slack, 이메일, Reddit 등 커뮤니티 지원

자주 묻는 질문 중 주요 항목 요약

     * 현재 Public Beta 단계, 내부 개발 장기 진행 이후 오픈소스 전환함
     * 한 명의 개발자(George Badawi)가 주도, VC나 외부 자본 없이 자체 운영 중임
     * VPN 역할 가능하지만, 근본적으로 정체성 인식 프록시 기반의 ZTA를 지향
     * 실제 ""오픈 소스같지 않은"" 제약이나 상용 강제 없고 자체 호스팅·풀기능 제공이 설계 목표임
     * 비즈니스 모델은 기술지원, 상용 라이선스, 엔터프라이즈 부가 기능(예: SIEM 연동, Vault 백엔드, EDR) 등에서 파생

        Hacker News 의견

     * Octelium이 무엇을 하는지 이해하기 어려운 이들을 위해 가장 명확한 설명을 찾은 곳이 있어 공유함 Octelium 작동 방식 - 공식 문서 링크가 가장 이해가 쉬움 Octelium에 대해 가능한 모든 기능을 나열하여 혼란을 주는 대신, 핵심 개념부터 시작해 점진적으로 설명하는 방식이 매력적임 주요 기능은 고급 프로토콜을 이해하면서 내용 기반으로 세밀한 보안 결정을 내릴 수 있는 VPN 유사 게이트웨이와 Kubernetes 위에 구축된 클러스터 설정 계층임 이 두 가지가 결합되어 ""개인용 클라우드""를 만드는 형태임 대형 클라우드 플랫폼처럼 수많은 기능을 제공하지만, 어느 것부터 사용할지 결정이 어려움 개인 홈랩, 클라우드 비용을 절감하려는 소규모 회사, 맞춤형 PaaS 등 다양하게 활용 가능성이 있는 멋진 시스템임
          + TailScale이 만족스럽기는 하지만 경쟁사의 필요성을 느낌 IPO가 예상되고 있는데, 그 단계에 들어서면 경쟁자가 없을 경우 가격이 급격히 오를 가능성이 크다고 봄
          + 프로그래머블 네트워크 터널 패브릭 형태로 요약할 수 있음
     * 개인적으로 보았을 때 몇 가지 문제점과 그로 인해 사용자들이 회의적으로 볼 수밖에 없는 이유를 공유함 개발 이력의 부재, 알 수 없는 대규모 첫 커밋, 공개된 정보 부족, 실재하는 회사로 보이지 않음, 모든 것을 해결할 수 있다고 주장하는 마케팅에 증빙 없는 보안 등 다양한 부분에서 신뢰를 떨어뜨림 이러한 상황에서는 실제 자체 기술인지, 혹은 충분히 신뢰할 수 있는 기존 기술 위에 구축된 것인지 추가 정보가 필요함 사업으로 출시하려면 신뢰성을 확보해야 함 반면 개인 프로젝트라면 비즈니스인 척하는 모습이 오히려 가짜/스캠/주의 신호로 보일 수 있다고 조언함 1인 개발자가 메이저 기업과 경쟁할 제품을 갑자기 내놓는다는 점에 대해 긍정적이기 힘듦 보안성을 명확하게 강조하는 게 중요함 소프트웨어의 목적을 한 문장으로도 설명하기
       어렵다면 힘든 싸움이 예고됨 기능을 더 많이 나열하는 것이 해답이 아님 오히려 ""난 무조건 깔아줘"" 같은 느낌이 들어 시도할 이유가 없어지는 효과를 가져옴 이것이 프로젝트 성공을 방해할 가능성이 높다는 사실을 지적함
          + 훌륭한 피드백임 Octelium이 다양한 기능을 동시에 수행하도록 의도적으로 설계된 점에서 비판의 정당성을 이해함 Octelium은 인간-워크로드, 워크로드-워크로드 사이 여러 케이스에서 사용할 수 있는 통합/범용 제로트러스트 액세스 플랫폼임(문서에 다양한 예시가 상세히 있음) 그래서 신입 사용자 입장에서 혼란스러울 수 있음 첫 커밋이 갑자기 등장한 건, 실제로 2020년 초부터 개발해왔으나 코드 공개를 결정하면서 개인 정보 유출 위험성 때문에 초기 커밋이 없도록 깨끗한 공개 저장소로 시작했기 때문임 지난 5년간 거의 9,000건 수작업 커밋을 했고, 초기에는 단순 원격 접속 WireGuard VPN이었다가 지금의 아키텍처, 기능, 복잡성으로 완전히 바뀌었음
          + 오픈소스 개발자들에게 관대함이 필요함 OP의 배경이나 동기를 아무도 모르고, 재미로 하는 것일 수도 있음 정당화할 필요 없음 이건 오픈소스이자 무료 소프트웨어임 사용자 설명을 한 문장으로 못한다는 비판에 대해, tailscale, cloudflare access, ngrok처럼 비교적 간단하게 설명될 수 있음 이런 제품들이 필요한 게 아니라면 애초에 이 제품도 필요하지 않음
     * Octelium을 최근 살펴봤는데, 설치 시 Kubernetes 클러스터가 필수적인 것처럼 보임 사실이라면 진입장벽이 너무 높음 우리는 오버레이 네트워크에 노드를 붙이고 싶지 k8s 등 다른 인프라 의존성이 추가되는 것은 원하지 않음 내부 서비스 의존성이 최소화 혹은 없어야 한다는 점에서 이러한 선택이 의아함 SDN이 클러스터 위에 필요하면 그게 타겟일 것일 수 있는데, 그뿐인지 궁금함 k8s 통합이 선택이길 바라고, 필수 전제나 유일한 배포 방식이 아니었음 좋겠음 혹시 k8s 없이 Octelium 사용하는 자료가 있다면 알려주길 바람
          + Octelium은 하나 혹은 그 이상의 노드 위에 동작할 수 있는 분산 시스템임 현재는 반드시 Kubernetes 위에서 동작해야 하지만, 내부적으로 k8s에 강하게 엮여 있지는 않아서 예를 들어 Nomad 같은 다른 오케스트레이터로 포팅도 쉬움 k8s를 자체 인프라로 쓰는 것은, 제로트러스트 아키텍처를 관리할 때 발생하는 수작업(프록시 배포, 스케일, 철거 등)을 시스템 관리자가 덜어주기 위함임 Octelium은 control/data plane을 모두 제공해서 octeliumctl apply만 하면 모든 서비스가 자동 배포, 관리, 스케일, 철거까지 됨 파이어월 포트 열기 등 수동 작업이 필요 없음 Kubernetes가 컨테이너를 관리하는 것처럼 Octelium은 동일하게 서비스, 프록시 등을 자동 조율함 노드 개수나 CRI 네트워킹 등 복잡한 관리도 필요 없음 클러스터가 모든 노드를 아우르며 선언형/프로그래밍적으로 관리가
            가능함 Octelium을 운영하면서 Kubernetes 깊은 이해는 아예 필요 없고, k8s 자체 클러스터 스케일, TLS 인증서 설정 등 특정 작업을 제외하면 Octelium 자체만 다루면 됨 더 자세한 내용은 공식 문서 참고 추천함
     * 너무 많은 버즈워드를 내뱉는 것에 대해 즉각적으로 큰 불신이 생김 GitHub 페이지를 봐도 제품이 구체적으로 뭘 하는지 이해가 잘 안됨
          + 개선할 수 있도록, 어떤 버즈워드가 문제였는지 목록을 제공해주면 readme에 반영할 수 있어 감사함
     * 전체적으로 Tinc, Hamachi, ZeroTier, Nebula, Tailscale, Netbird 등 이미 유사한 제품이 너무 많음 각각의 장단점이 있지만 실제로는 큰 차별점이 미미하다고 생각함 개인적으로 정말 원하는 기능은 제로트러스트 '라이트하우스'임 Zerotier, Tailscale은 내 계정/네트워크에 노드를 추가하는 권한이 서비스에 있음 내가 원하는 것은 완전한 셀프호스팅과 라이트하우스가 네트워크 일부가 아닌 오로지 노드 감시 역할만 하는 구조임 관련 정보를 더 찾아봐야겠음
          + 문서를 읽어보니 많은 사람들이 Octelium의 진짜 가치를 놓치고 있음 실제로 문서의 내용대로 동작한다면 아직 발견되지 않은 보석이 될 수 있음 엔터프라이즈가 바라는 것은 기존의 경계 기반 보안에서 벗어나 Google überProxy/BeyondCorp가 제시한 (그리고 각종 버즈워드로 희석되어버린) 개념, 즉 생산 시스템, 기업 내부, 외부 인터넷 간 깔끔한 분리와, 사내 직원에게 최대한 투명한 UX, 경계 간 흐르는 트래픽의 권한 명확 관리, 모든 클라이언트의 강력한 신원 인증임 Google 외부는 다양한 프로토콜 환경으로 인해 제약이 큼 프로토콜 인지형 프록시는 기존 coarse-grain 결정 및 로깅만 가능하지만, 타입 추론까지 지원할 때 요청 단위에서 훨씬 정교한 권한 제어가 가능해짐 (모든 요청의 조건이 정책 엔진에 노출되는 것) 문서가 장황하고 마케팅이 매끄럽지
            않지만, 이 문제 자체가 너무 복잡해서 어느 누구도 완전하게 풀지 못함 Teleport가 OSS와 상용화에 가장 먼저 나섰고, StrongDM도 흥미로운 시도를 하고 있음 Hashicorp 역시 여기에 더 투자했으면 하는 바람이 있음 (*개인 의견임)
          + Octelium은 위에 언급된 제품들을 대체하는 것도 가능하지만, 지향점과 활용 방식이 더 넓고 명확히 제로트러스트 지향임 단순 VPN/리모트 액세스 툴 그 이상임 꼭 문서를 읽어보고 의도와 아키텍처, 기능을 이해해줬으면 함 요즘엔 모든 제품이 ""제로트러스트""라고 광고하지만 실제로 NIST에서 정의하는 진짜 ZTA(즉, L7 인지형 프록시, 정책 결정 지점, 정책 코드 기반의 요청별 세부 액세스 제어, 중앙화된 신원, 외부 SIEM/SSO/Threat intelligence 툴 정보 통합 등)는 소수임 실제로 ""진짜"" ZTA로 분류되는 상업 제품은 Cloudflare Access, Teleport, Google BeyondCorp, StrongDM, Zscaler 등이 있음 오히려 기업들이 이 용어를 남용해서 ""진짜 제로트러스트""의 개념을 희석시켜버리는 경향이 심함
          + sanctum의 cathedral 모드를 참고바람 완전히 셀프호스팅이 가능하며, 노드는 단순히 discovery 역할만 함 터널이 성립하면 대성당 노드는 관여하지 않으며, 예외적으로 black key 분배나 피어가 NAT 뒤에 있을 때만 작동함 reliquary도 있음 직접 운영 중임 sanctum, reliquary
          + 더 많은 관련 프로젝트 목록은 awesome-tunneling에서 확인 가능함
     * ""AI"" 키워드 삽입은 SEO 목적이라는 것을 이해함 마치 기사 제목에 ""Reddit""을 붙이는 것과 같음 내용이 훌륭해도 이런 방식은 좋은 인상을 주지 않음 API 게이트웨이와 AI 게이트웨이 다이어그램도 거의 동일함 tailscale 블로그: AI-normal
          + API와 AI 게이트웨이 간 공통 기능이 많음 예시는 문서에서 직접 확인하는 것이 좋음 AI Gateway 예시, API Gateway 예시 참고 HTTP 요청/바디 수정 프로세스 확장 작업도 진행 중이며, envoy의 ext_proc 지원이 곧 들어가고 proxy-wasm 지원도 수요에 따라 추가할 계획임 HTTP 관련 설명
     * Tailscale 오픈소스 대안을 적극적으로 찾고 있음 하지만 README가 너무 장황해서, 프로젝트 개요 및 문서 링크만 압축 표시했으면 좋겠음
          + headscale이 tailscale 오픈소스 대안임 headscale GitHub
     * Tailscale의 가장 큰 장점은 쉬운 P2P 연결임 Octelium은 그와 달리 중앙집중화된 라우터 구조를 사용하는 것 같은데, 정확한지 궁금함
          + Octelium은 P2P VPN이 아니라 제로트러스트 아키텍처임 물론 WireGuard/QUIC 기반 리모트 액세스 VPN 역할도 가능하지만, 구조적으로 Cloudflare Access, Teleport 쪽에 더 가까움 L7 기반 액세스 제어, 시크릿 없는 액세스(각종 키/토큰/API 키 등 직접 배포 없이 주입), 유동적 설정 및 라우팅, 실시간 OpenTelemetry 기반 가시성 및 감사 등 P2P VPN과 본질적으로 다름 본격적 ZTNA/BeyondCorp 구조(서비스 메시 제외)는 P2P VPN 형태로 구현하기엔 근본적 한계가 있음 요청별 액세스 제어/가시성 확보를 위해서는 반드시 L7 인지형 프록시가 필요함
          + 참고로 Tailscale도 패킷이 중앙집중 라우터를 통하는 경우가 있음 Tailscale의 connection types 설명
     * k3s 클러스터 설치가 앱에 내장되는 이유를 이해하지 못하겠음 기존 인프라에 쉽게 추가할 수 있는 형태/간단한 CRD로 서비스 노출이 더 명확할 것 같음 오픈소스 Cloudflare Access/Teleport 같은 컨셉 자체는 멋진데, 대부분이 결국 k8s 위 커스텀이어서 액세스 중심 기능에 집중했다면 더 관심이 갔을 것 같음
          + Octelium 클러스터는 k8s 위에 작동하는 분산 시스템임 단일 Node의 k8s/k3s 위에서도, 다수 노드의 ""프로덕션용"" k8s에도 설치 가능 Octelium은 단순한 k8s 래퍼가 아니라, k8s를 인프라로 삼고 자체 플랫폼을 구성함 각 노드는 Octelium Service를 위한 게이트웨이/호스트 역할을 하며, 각 Service는 ID-aware 프록시로 k8s service로 배포됨, 각 프록시는 WireGuard/QUIC 터널의 endpoint이며, 스케일에 따라 안정적인 사설 dual-stack IP를 가짐 identity-aware proxy를 컨테이너처럼 관리하는 구조임 관리자가 직접 각 프록시를 런칭/제거해야 하는 기존 ZTA(예: Teleport, Pomerium 등)와 다름 Octelium은 octeliumctl apply나 gRPC API를 통해 선언형으로 리소스를 생성/삭제하면 그 외 관리는 잊어버려도 됨 리소스가 CRD였던 시절도 있었으나, 유저, 세션, 서비스, 네임스페이스(이 중 일부는 k8s 네임스페이스와
            별개), 정책, 디바이스, 자격증명 등 리소스가 너무 많고, 데이터량이 커서 등d backend가 불안정함 결국 별도 Postgres 백엔드를 도입함
     * 이미 출시된 비슷한 프로젝트가 너무 많음
     * 대기업 수준의 액세스 관리(코포 보트넷) 대체재인지 궁금함 만약 내가 대기업이라면 안정성을 위해 대기업 소프트웨어와 지원 패키지를 원할텐데 오픈소스 프로젝트가 1인 개발자 문제를 해결해줄 수 있을지 잘 모르겠음
          + Octelium은 단순히 소규모부터 엔터프라이즈까지 다양한 환경에서 범용적이고, dev/스타트업/엔터프라이즈 등 모든 레벨에 쓸 수 있는 보안 액세스 플랫폼임 예를 들어 Kubernetes도 단일 컨테이너 웹사이트, 소수 마이크로서비스용 API gateway, 수백/수천 노드 대규모 서비스 메시 등 다양한 세팅 모두 지원하듯 Octelium도 활용 범주가 매우 넓음
"
"https://news.hada.io/topic?id=21734","JavaScript에 곧 도입될 주요 신기능과 표준화 동향","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    JavaScript에 곧 도입될 주요 신기능과 표준화 동향

     * 최근 TC39 108차 회의에서 JavaScript의 미래를 이끌 9개 제안이 각 표준화 단계별로 진전됨
     * Stage 4(최종 승인) 에는 using, Array.fromAsync, Error.isError 등 개발 생산성 및 안전성을 높이는 기능이 포함됨
     * Stage 3~1에는 불변 ArrayBuffer, 시드 랜덤, clamp, NumberFormat 개선, Random 함수 등 다양한 유용 기능 제안이 논의 중임
     * Deno는 최신 표준 도입 및 실험에 적극적이며, 일부 기능은 이미 Deno와 최신 브라우저에서 사용 가능함
     * 향후 표준화 논의가 이어질 예정이며, JavaScript 생태계가 더 안전하고 간결하게 진화할 것으로 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

TC39 최신 표준화 동향 및 JavaScript 주요 신기능

  Stage 4: 공식 채택 예정 기능

    명시적 리소스 관리(using)

     * using 선언문을 통해 파일, 네트워크 소켓 등 리소스 객체의 수명 종료 시 자동으로 정리 처리를 보장
     * C#·Python 등에서 영감 받은 구조로, 객체에 [Symbol.dispose]() 또는 [Symbol.asyncDispose]() 메서드를 구현해 블록 종료 시 호출
     * 예외 발생 시에도 자동 정리되어, 파일 핸들 등 자원 누수 위험 감소
     * Chrome 134, Firefox 134, Deno v2.3 등에서 이미 지원, Deno에서는 파일·소켓 등 실무에 즉시 활용 가능

    Array.fromAsync

     * 비동기 iterable을 배열로 변환해주는 표준 메서드로, Promise를 반환
     * async generator 등에서 순차적으로 값을 수집, 최종적으로 배열 결과를 Promise로 얻음
     * 기존의 동기 Array.from과 사용법 유사하며, mapping 함수와 thisArg 지원
     * 모든 주요 브라우저, Node v22, Deno v1.38 이상에서 지원

    Error.isError

     * 어떤 값이 진짜 Error 객체인지 간단하게 판별할 수 있는 내장 함수
     * cross-realm, 서브클래스 오류 등 복잡한 경우에도 신뢰성 있게 동작
     * Polyfill, 라이브러리 개발 시 신뢰도 향상

  Stage 3: 불변 ArrayBuffer

    Immutable ArrayBuffer

     * ArrayBuffer의 불변화를 지원하는 transferToImmutable(), sliceToImmutable() 메서드 추가 예정
     * 불변 buffer는 한 번 생성 후 수정·분리 불가, 스레드·워커 간 안전한 공유 및 성능 향상에 기여
     * Deno 등에서는 바이너리 데이터 처리시 불필요한 복사 없이 효율적으로 활용 가능

  Stage 2: 신규 제안 기능

    Random.Seeded

     * 시드 기반 PRNG(Random.Seeded(seed))로 반복 실행 간 동일한 난수 시퀀스 생성 가능
     * 게임·시뮬레이션 등 재현성이 중요한 영역에 유용
     * 기존 Math.random()과 달리 시드 지정 및 상태 복제 등 고급 기능 제공

    Number.prototype.clamp

     * 수치 범위 제한 메서드로, 지정한 최소~최대값 구간으로 값을 자동 보정
     * 기존의 Math.min(Math.max(x, min), max) 패턴을 짧고 명확하게 대체

  Stage 1: 초기 논의 제안

    Keep Trailing Zeros

     * Intl.NumberFormat의 신규 옵션으로, 소수점 이하 0 유지/제거를 세밀하게 제어
     * 금액·고정 소수점 등 UI에서 원하는 표현력 강화

    Comparisons

     * Node.js의 util.inspect와 비슷하게, 값의 사람 친화적 표기와 diff 출력 표준화 목표
     * 테스트 프레임워크, 콘솔 출력 등에서 일관성 제공

    Random Functions

     * 신규 Random 네임스페이스 도입, 난수·샘플링·셔플 등 안전하고 간결한 랜덤 관련 함수 모음
     * 오프바이원 오류 방지 및 다양한 수집·샘플링 패턴을 직관적으로 지원

Deno와 JavaScript 생태계의 미래

     * Deno는 표준화 논의에 적극 참여하며 최신 기능을 실무에 신속히 반영하고 있음
     * 예) async context 전파, 내장 OpenTelemetry, 새로운 리소스 관리 패턴 등
     * 9월 이후 추가 TC39 회의에서 표준화 논의 지속 예정
     * JavaScript 개발자는 향후 더욱 강력하고 안전하며, 편리한 언어 환경을 경험할 수 있게 될 것

   await operations proposal

   이건 언제...

   패턴 매칭은 언제 추가되려나요

   stage 1에 signals가 있는데 정식 출시되려면 아직 멀은 듯 합니다.
"
"https://news.hada.io/topic?id=21805","Ask GN: Hacker News링크 입력시 댓글 요약후 한글번역하는 방법이 있을까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ask GN: Hacker News링크 입력시 댓글 요약후 한글번역하는 방법이 있을까요?

   원하는 기능은 아래와 같습니다.
    1. Hacker news 링크를 입력값으로 넣음
    2. 댓글을 요약후 한글로 번역함. 한글 번역시에는 ~했음, ~함, ~됨 처럼 끝났으면 좋겠음.
       공유주시면 GeekNews에 더 많은 글이 올라갈수 있을거 같습니다!

   감사합니다.

   해커뉴스 1면에 업데이트 되는 정보량이 많아서 llm 쓰는 것이 조금 걱정되기는 하는데유.
   이번에 배포된 gpt-4.1-nano 가 입력 1M 토큰에 0.1$ 라는 미친 가격이라, 요약/번역/ 카테고리화 전부 싸게 가능하긴 할 것 같습니다.

   이 가격은 너무나도 말이 안되서 제 블로그에 번역 api 와 4.1-nano 모델의 가격 비교를 올려놌서요.
   관심 있으시면 한번 보셔도 좋을 것 같네유 : https://dev-wiki.dev/reading/tech/16

   솔직히 저는 nano 모델 쓰는 것은, html 통짜 태그를 다 집어넣는게 아니라 정제된 정보에 대해서 추출/요약/카테고리 하는 것은 그냥 무료라고 생각하고 쓰고 있습니덩

   토큰 쉐어링 설정하면 250만토큰 무료일거에요 4.1 미니 나노는

   html 상에서 댓글 위치를 특정하기는 쉽나요?
   이 부분까지 firecrawl 같은걸 쓰면 돈낭비가 엄청 될것 같긴합니덩.

   만약 html 태그를 어떻게 잘 만져서 특정할수만 있다면, 그 이후로는 본문을 가지고 nano 모델써서 하고 싶은 일을 할 수 있을 것 같습니덩
"
"https://news.hada.io/topic?id=21749","당신이 작업 중인 것은 무엇입니까? (2025년 6월)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     당신이 작업 중인 것은 무엇입니까? (2025년 6월)

     * 이 글은 Hacker News 커뮤니티에서 사용자가 현재 작업 중인 내용을 공유하고 질문하는 정기적인 오픈 스레드임
     * 다양한 스타트업 창업자, 개발자, IT 전문가가 자신의 프로젝트나 아이디어를 간략하게 소개함
     * 참여자들은 피드백, 크리틱, 협업 제안 또는 시장 반응 등을 얻는 경험을 목표로 함
     * 신규 아이디어, 제품 프로토타입, 연구 결과, 개발 도구 등 다양한 분야의 작업이 소개됨
     * 해당 스레드는 네트워킹 기회와 커뮤니티 내 협업 활성화에 기여함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 이 스레드는 Hacker News 사이트에서 매달 운영되는 ‘Ask HN: What Are You Working On?’ 코너임
     * 참여자들은 현재 진행 중이거나 막 시작한 사이드 프로젝트, 스타트업, 오픈소스, 앱, 연구 등을 자유롭게 소개함
     * 각 댓글에는 간단한 소개, 해결하려는 문제, 타겟 고객 또는 사용자, 현재 상태, 앞으로의 계획 등이 포함됨
     * 커뮤니티의 다른 구성원들은 피드백, 아이디어, 개선점, 격려, 조언 등 다양한 반응을 남김
     * 기술 분야, 창업, 개발 문화에 종사하는 사람들에게 영감, 동기부여, 네트워킹 기회를 제공함

주요 내용 및 특성

     * 프로젝트 범위는 인공지능, 웹 애플리케이션, 생산성 도구, 데이터 시각화, 하드웨어, 교육, 생산성 개선, 엔터프라이즈 솔루션 등 매우 다양함
     * 초기 단계의 스타트업 창업자, 프리랜서, 엔지니어, 연구원, 학생 등 다양한 배경을 가진 구성원이 참여함
     * 많은 경우 MVP(최소기능제품) , 프로토타입, 실험적 기능, 개념 증명 등이 소개됨
     * 공개된 GitHub 저장소, 데모 사이트, 제품 소개 페이지 등의 링크가 공유됨
     * 일부 프로젝트는 사용자 리서치, 알파/베타 테스터, 피드백 제공자 등을 모집함

커뮤니티 상호작용

     * 참여자들은 댓글을 통해 자신의 개발 경험, 문제점, 해결 방식을 공유함
     * 서로의 아이디어에 비판적/실용적 피드백을 주고받으며, 실질적인 개선점이나 확장 아이디어를 제시함
     * 협업 또는 파트너십, 조언, 추천 요청, 네트워킹 등 다양한 상호작용이 이루어짐
     * 오픈소스 프로젝트의 경우 기여자 모집, 토론, 문서화 요청 등 협력 기회가 창출됨

활용가치 및 의미

     * 이 스레드는 생생한 창업 및 개발 트렌드를 보여주며, 최신 프로젝트 동향 파악에 도움을 줌
     * 초기 시장 반응 테스트, 아이디어 발전, 조기 피벗 가능성을 살펴볼 수 있음
     * 업계 종사자 사이의 연결 고리 역할을 하여, 실제 파트너십과 직간접적인 지원으로 이어짐
     * 커뮤니티 기반 성장, 지식공유 및 멘토링 문화에 긍정적 영향

결론

     * Hacker News의 ‘Ask HN: What Are You Working On?’ 스레드는 기술 및 창업 업계에서 활발한 프로젝트 공유 및 상호 피드백 문화 형성에 크게 기여함
     * 기술 트렌드 연구, 스타트업/개발자 네트워킹, 협업 기회 발견에 유용한 장임

        Hacker News 의견

     * 현재 혼자 진행 중인 사이드 프로젝트 laboratory.love는 독립적으로 식품의 내분비 교란 물질을 검사할 수 있도록 크라우드펀딩 기반의 플랫폼 제공 목적. 지난해 PlasticList에서 테스트한 식품 86%에서 플라스틱 화학물질이 검출되었고, 특히 유아식은 100% 검출 결과. EU에서는 BPA 안전 기준치를 2만 배나 더 낮췄으나, FDA는 여전히 유럽 기준의 100배까지 허용 중. laboratory.love에서는 실제로 구매하는 제품의 검사를 위한 펀딩 참여 및 결과 확인 가능, 목표 금액 불충족 시 1년 이내 자동 환불. 테스트 결과는 모두 공개하며, ISO 17025 인증 실험실과 협업, 서로 다른 생산 시점의 3개 샘플로 parts per billion(ppb) 감도까지 검사 진행. PlasticList.org와 동일한 매뉴얼 기반이며, 소비자가 테스트 제품을 직접 선택. 정보 투명성이 공급망 청결화 압력을 만들 수 있다고 봄. 자세한
       내용과 제품 목록, 제안, 팔로우 모두 사이트에서 가능 https://laboratory.love
          + heavy metals 등 기타 화학물질 검사도 지원되면 좋겠다는 바람. 예를 들어 쌀에서 비소(arsenic) 함유가 큰 이슈이므로, Consumer Reports의 관련 기사도 참고 https://consumerreports.org/cro/magazine/… 쌀을 많이 먹는 입장에서 비소가 가장 적은 브랜드를 찾는 데 관심
          + 처음 PlasticList를 알았을 때, 검사 결과를 식품 기업에 어떻게 압박 요인으로 연결할 수 있을지 고민했던 경험. laboratory.love의 참여형 구조가 그 해답이자 동력이 될 수 있다고 느낌. 행운을 빌고 싶음
          + ""모든 테스트 결과는 공개""라고 했는데, 실제로 공개된 결과를 볼 수 있는 링크가 어디 있는지 궁금. 혹시 이메일 제출 등 절차가 필요한지 질문
          + 검사 펀딩이 1년 가까이 걸릴 수 있는데, 이 동안의 자금은 어떻게 보관하는지 궁금. 프로젝트 달성까지 에스크로(escrow)에 보관되는지 질문
     * 내 언리얼 엔진 게임들을 위한 인벤토리 시스템 만들기 중. 그리드 기반, 멀티플레이어 지원, 예측 이동, 아이템 잠금 등 다양한 기능 탑재 https://github.com/brokenrockstudios/RockInventory 재미있고 도전적인 프로젝트였고, 특히 많이 배울 수 있었던 점이 가장 중요. 초반엔 재미있었으나 완성도가 높아질수록 점점 덜 재미있게 느끼는 중
     * Mochi라는 작은 프로그래밍 언어를 만들고 있음. 자체 가상 머신과 통합적·경량적 구조 데이터 쿼리(CSV, JSON, 향후 그래프도)를 목표. LINQ 스타일 쿼리 실험에서 시작해 언어로 발전 https://github.com/mochilang/mochi
          + 선언적 쿼리 내장 형태, 분석·최적화를 위한 레지스터 기반 VM, 라이브니스 분석·상수 폴딩·데드코드 제거가 가능한 IR, 정적 타입 추론 및 인라인 테스트, 골든스냅샷 등 지원
          + 예시 코드 작성 방식: 데이터 구조 정의, 파일 로딩 및 필터링, 조건에 맞는 데이터 추출 및 변환, 저장까지 간편 처리
          + 궁극적 목표는 파이썬, SQL, 여러 라이브러리가 필요 없는 데이터 파이프라인·쿼리·에이전트 로직용 작고 표현력 있는 언어 완성. VM/쿼리 엔진/DSL에 관심 있으면 언제든 대화 환영
               o 이 프로젝트 매우 흥미롭게 느껴짐. 자신도 작은 프로그래밍 언어를 개발해 보고 싶은데 어디서부터 시작해야 할지 잘 모르겠음. 참고할 만한 자료 추천 부탁
               o 빠르고 간단한 데이터 필터링 및 변환 용도로 정말 멋져 보인다는 피드백
     * C++ 코드로 내장형 인메모리 데이터베이스를 선언하는 작은 DSL 실험 중. STL 컨테이너 모듈 집합을 작은 데이터베이스처럼 다루면서, 쿼리와 제약 최적화된 엔진 객체 조립 패턴을 자주 사용함을 인식. SQL처럼 스키마, 인덱스, 쿼리, 프로시저를 선언형으로 작성하고, 이를 자동으로 C++ 헤더(클래스/메서드)로 변환하면 편리할 거라 생각. MLIR 생태계에서 .td 파일 활용 받은 영감 있음. 실제로 어디까지 해볼 수 있을지 미지수지만, ""이상한"" 언어 실험은 언제나 재미있음. 이런 스타일의 프로그래밍 경험이 어떤 느낌일지 직접 보고 싶음
     * 두 가지 프로젝트를 동시 진행 중
          + LegalJoe: 스타트업용 AI 기반 계약서 리뷰 툴, 지금은 ""기술 데모"" 단계 https://www.legaljoe.ai/
          + ClipMommy: 영상 촬영이 많은 전문가·인플루언서를 위한 macOS 도구. 정리 안 된 비디오 폴더를 드래그하면, 영상 시작/끝에서 특수 구문(오디오 기반 ""클랩보드"" 사용)을 감지해 자동으로 폴더·하위폴더 구성 및 태그 추가, Mac App Store 출시 임박(Apple 승인 남음)
          + Claude Code에 매우 감명받았고, 차세대 비즈니스 소프트웨어의 AI 활용 방향을 잘 보여준다고 느낌. 이 경험 바탕으로 LegalJoe를 더 ""에이전트형""으로 발전시킬 계획
               o Word 애드인으로 구현하는 전략이 매우 영리하다고 생각, 좋은 작업임
     * 2세대 Nest thermostat에 대응 가능한 오픈소스 버전 하드웨어 개발 지속 중. Nest의 케이스, 엔코더 링, 디스플레이, 장착 시스템 재활용, 논리부만 오픈소스 PCB로 교체하여 Home Assistant와 상호작용 가능하도록 설계.
          + 엔코더 링(LED 마우스 원리 역방향) 완전 리버스 엔지니어링 및 데모 PCB 달성
          + 온도 조절용 페이스플레이트 PCB 설계 완료, 첫 버전에서 ESP32 GPIO 전원동작 관련 문제 발견하여 3차 리비전 발주
          + Nest 1~2세대는 2025년 10월 25일부터 Google 공식 지원 종료, 온도·모드·스케줄 등은 단말에서 가능하나 소프트웨어/보안 업데이트 및 앱 연동 등 기능 중단 예고, Home Assistant에서도 1년 넘게 주요 기능 제대로 미지원 중
               o nest 엔코더/버튼의 감촉이 좋아 데스크탑 볼륨 컨트롤러로 개조 시도 고민, 다만 깔끔하게 하드웨어 인터페이스하는 노하우 부족으로 정보 공유 희망
               o Tony Fadell 책(아마 Nest 개발자)에 관한 내용 독서 중으로 흥미롭게 느껴짐
               o 프로젝트 진행 상황을 어떻게 따라갈 수 있는지 궁금. ESPHome 사용 여부 질문
               o 실제로 프로젝트가 어디 온라인에 공개되어서 준비 상황을 관찰할 수 있는지 물음
               o 2세대 Nest 제품을 수급할 방법 문의, eBay·Craigslist에 없고, 리사이클 업체에서 요청해 폐기 Nest를 받을 수 있는지 아이디어 언급
     * 미국 빈티지 인쇄광고의 물리적·디지털 아카이브 구축 프로젝트 진행 중. 이미지 보관 및 자동화 데이터베이스 소프트웨어는 Lucee & MySQL로 제작, OpenAI로 이미지 분석·메타데이터 추출. 전체 광고는 https://adretro.com에 게재. 한 잡지당 약 150개 광고를 일주일 넘게 걸리던 카탈로그 작업을 몇 시간으로 단축 성공, 평생 안에 전체 데이터를 아카이브할 수 있는 현실성 확보
          + 업계, 브랜드, 연도별로 아카이브 구조화 제안. 예를 들어 1960년대 Rolex 광고만 골라서 보는 방식
          + 광고 자체는 싫어하지만 빈티지 광고에는 큰 매력 느끼는 점 언급. 그래피티도 비슷한 심정으로, 최근 것보다 오래된 작품이 문화의 시대정신(zeitgeist)을 더 잘 보여줌. 멋진 사이트라고 피드백
     * 10년간 방산 분야에 있다가, 우크라이나 및 중동 미사일 공격 등을 보면서 많은 사람들이 방공 시스템의 실제에 대해 거의 모른다는 점을 통감. 그래서 방공망 시뮬레이터 제작, 사용자가 실제 오퍼레이터 시점에서 다양한 방위 시나리오를 경험 가능. 현실 데이터 기반, 우크라이나, 이스라엘-이란 시나리오 포함 https://airdefense.dev/
          + 몇 분 만져보며 방공 분야에 대해 정말 무지하다는 점 절감. 튜토리얼이나 좀 더 단순화된 장난감 버전 추가 추천
          + 시스템 배치 및 시뮬레이션에서 장비·탄약 비용(공격, 방어 모두)까지 대략적으로 반영하면 더욱 좋을 듯하다는 의견
          + 시스템 로그 메시지 더 많이 볼 수 있으면 좋겠음. 여담으로, 미니 자동화 드론 거치식 터렛에 대한 실무적 가능성(자동 탐지·조준 난이도) 궁금. 샷건 장착 드론은 이미 있으나, 자동화 기술 난이도에 대한 견해 질문
          + 예전에 Amstrad에서 사용하던 핵전쟁 시뮬레이터를 떠올리게 하는 멋진 프로젝트라는 평
          + 정말 뛰어난 아이디어로, 타워 디펜스형 게임처럼 자원 제한, 장비별 역할 학습, 갈수록 복잡해지는 위협에 대응하는 방식 게임화에 큰 가능성. 장비 효율성이 100만불 미사일로 1천불 드론/로켓 격추 같은 상황까지 반영하는지 여부 질문
     * Bazel 기반 LLVM을 활용한 C/C++ 크로스 컴파일 툴체인 개발 https://github.com/cerisier/toolchains_llvm_bootstrapped
          + CRT, libc, C++ stdlib, 언와인드 등 모든 타겟 의존성까지 소스에서 빌드
          + glibc, musl, C++ stdlib(libstdc++, llvm-libc++) 모두 별도 루트 없이 크로스 컴파일 가능
     * ""Searching For Kurosawa""라는 다큐멘터리 제작. 전설적 감독 아키라 구로사와의 ""Ran"" 촬영 현장에서 BTS 스태프로 일하던 가와무라씨의 이야기. 당시 촬영한 실황 영상이 압수됐다가 거의 40년 만에 해당 영상을 되찾아 자신의 장편영화로 완성. 영화는 최근 Bali 국제영화제, Marina Del Rey 영화제에서 상영, 다음달 Asian American International Film Festival(뉴욕)에서도 상영 예정
          + 최근에 남의 구로사와 영화 편집본에 자신이 좋아하는 음악을 더빙했다는 흥미로운 경험 공유
          + 바르셀로나에서도 볼 수 있으면 좋겠다는 소망
"
"https://news.hada.io/topic?id=21713","중독은 설계되고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              중독은 설계되고 있음

     * 최근 수년간 새로운 소셜 플랫폼들이 기존 문제를 해결하겠다고 등장했음
     * 알고리듬과 성장 압박이 결국 플랫폼의 본래 의도를 부패시키는 원인임
     * 유저의 중독은 구조적으로 만들어진 보상 체계와 투자 수익 모델의 결과임
     * 개인 차원의 해결책은 시스템적 문제에 효과가 없음
     * 근본적 해결을 위해서는 대체적인 자금 조달 방식, 알고리듬 규제, 사회적 목적 중심 평가 등의 구조 개혁이 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 반복되는 소셜 플랫폼의 실패 경험

     * 새로운 소셜 플랫폼들은 정기적으로 등장하며, 기존 문제 해결을 약속함
     * BeReal, Clubhouse 등은 각각 진정성과 친밀감을 내세웠으나, 결국 벤처 투자, 성장 압박, 알고리듬 조작, 부패로 이어지는 반복 패턴을 보임
     * Circliq라는 오프라인 만남과 커뮤니티 중심의 플랫폼을 경험적으로 만들었으나, 기존 소셜 미디어의 구조적 문제를 극복하지 못했음

구조적 중독의 본질

     * 젊은 세대는 실리콘밸리의 도파민 실험의 실험군이 되었고, 이제 그 부작용을 경험 중임
     * Circliq처럼 새로운 방식을 시도해도, 자금 조달과 성장에 대한 질문이 반복되면서 결국 기존 문제의 일부가 되어버림
     * 플랫폼은 선한 의도, 성장의 압박, 관여 시간 극대화, 알고리듬 조작, 목적 상실이라는 고정 경로를 따라감

가치관의 변질: 소셜 미디어의 경제적 구조

     * Instagram, Twitter 등 모든 주요 소셜 플랫폼은 초기 순수한 미션에서 시작해, 성장 압력에 따라 점차 알고리듬 중심의 참여도 경쟁으로 이동함
     * 투자유치, 연속적 유저 성장, 벤처캐피탈 요구에 따라 원래의 커뮤니티적 목적이 뒤틀림
     * 이 과정에서 설립자, 유저 누구도 악의적이진 않으나, 구조적 인센티브가 부패를 야기함

집은 항상 이김(이용자가 지지 못하는 구조)

     * 디지털 디톡스, 스크린 타임 제한 등 개인적 해법은 구조적 문제에 효과적이지 않음
     * 대형 소셜 플랫폼은 행동심리학자, 수천 건의 A/B 테스트, 머신러닝을 동원해, 심리적 취약점을 겨냥함
     * 보상 구조, 비정기 보상 패턴(슬롯머신), 사회적 비교, 분노 유발 등이 중독을 강화함
     * TikTok, Instagram, X 등은 중립적 도구가 아니라, 세밀하게 설계된 슬롯머신형 중독 시스템임
     * 데이터는 청소년 우울증/불안 증가 등 부정적 파급효과를 명확히 보여줌

실질적 해결책 모색

     * 체계적 문제에는 구조적 해결책이 필요함
     * 대체적 자금 조달 구조: 공공재 모델, 구독, 협동조합, 기부 등은 유저의 웰빙에 초점을 맞출 수 있음(Wikipedia 성공 사례)
     * 알고리듬 규제: 설계 투명성, 사용자 통제권 강화, EU DSA(디지털 서비스법 등) 참고
     * 기능적 분리: 광고 기반 수익 구조와 사회적 기능의 분리 필요성
     * 새로운 평가 지표: DAU, 체류 시간 대신 관계의 질, 실질적 연결 등으로 전환
     * 근본적으로 이익 중심 구조가 인간적 연결을 방해함
     * 소셜미디어가 덜 필요하도록 현실에서의 제3 공간, 직접 연결을 강화하는 시스템이 대안이 될 수 있음

결론: 게임의 룰 자체를 바꿔야 함

     * 더 나은 소셜 플랫폼을 만들려면, 현재의 경제적 인센티브 자체를 포기해야 가능함
     * 그렇지 않으면, 모든 혁신적 시도는 결국 문제 구조의 일부로 흡수됨
     * 해법은 또 다른 앱이 아니라, 게임의 근본 규칙(인센티브 구조) 자체의 변화임

        Hacker News 의견

     * 벤처 자본 투자를 받은 후, 회사는 성장 지표와 분기별 목표 수치에 쫓기게 되는 경험을 했음. ‘진정성’에서 ‘일일 활성 사용자’로 초점이 바뀌는 현실. 벤처 캐피털에서 일한 후 느끼는 건 외부 투자가 기업의 도덕성을 잃게 만드는 핵심 요인이라는 확신. 주주 이익 대변이라는 법적 의무감이 도덕성 약화로 이어짐. 경영자들이 자기 판단보다 주주를 우선시할 때, 중독연구를 경계가 아니라 교본처럼 참고하는 모습. 마치 Stanford Prison Experiment을 떠올리게 하는 상황. 너무 단순화하고 싶지 않지만, 외부 투자와 ‘중독 엔지니어링’의 연결 고리는 분명 큼. 현재 정부가 위험 감수나 혁신을 지원하는 보조금이나 사회 안전망을 줄이고 있어, 결국 투자가 창업의 최단 경로가 됨이 아쉬움. 오픈소스에 기부하고 싶다는 생각
          + 회사가 실질적으로 제품이라는 생각임. 이런 마인드셋에서는 우리가 ‘제품’이라고 부르는 것엔 아무 관심이 없고, 그저 진짜 제품(회사)을 키워서 가장 큰 도축장에 팔려는 계산뿐. 이 과정은 투자 시점부터 시작됨. 시리즈A를 받으려면 ‘엑싯 플랜’이 필수 요소. 엑싯 플랜 자체의 존재만으로 사용자는 외면받는 운명. 회사만 키우고 멋지게 보이게 하는 게 목표고, 최대한 빨리 살찌우기 위해 쓰레기 음식(질 낮은 요소)만 제공되는 실정. 장기적 존속성은 안중에도 없음. 기술 업계가 다시 진짜로 사용자에게 좋은 걸 제공하는 방향으로 돌아갔으면 하는 바램. 미친 수준의 수익까지는 아니어도, 그래도 제대로 먹고살 수 있다는 점은 남아있음
          + ‘주주 이익 대변의 법적 의무’라는 말이 인터넷 루머에 가깝다고 느끼고 있음. 실제 세상에서 그런 명시적 의무는 없음. 만약 100만달러짜리 인수 제안과 200만달러짜리 인수 제안이 있는데 100만달러 쪽을 뇌물 받고 택했다면 문제가 되겠지만, 그 외엔 실제로 문제가 되는 경우를 찾을 수 없음. 논리적으로도 맞지 않고, 실제 사례도 거의 없음. 앱이 지나치게 중독적이면 사회적 반발이나 정부의 규제 개입으로 오히려 주주가 피해를 입는다는 논리만 있으면 됨. 그러므로 단기 수익만을 최우선하는 게 법적으로나 도덕적으로 필수가 아님. 보드진이나 임원진이 투자자 비위를 맞추려 애쓸 수는 있으나, 법적 의무 때문은 아님. 무시한 임원은 평판이 나빠지거나 자기 주식 가치 낮아질 걱정은 할 수 있겠지만, 법적으로 처벌받지는 않음
          + 기업 경영자는 주주에 대한 신의의무를 해석할 수 있는 범위가 매우 넓음. 단기 수익에만 집착하는 게 마치 필수인 것처럼 오해되는 현상을 바로잡을 필요가 있음. 이건 실제로 반사회적이고, 명백히 파괴적임
          + 벤처 캐피털 현상이 전체 시스템의 축소판이라는 생각. 목표는 결국 이익이고, 그 외 모든 것(예: 사회적 책임)은 그에 종속됨. 만약 이익과 사회적 책임 등 외부 가치가 충돌하면 이익이 항상 승리함. 용병적 사고가 지배하는 구조. ‘Magickal Faerieland’(이상향)에서는 규제가 둘 사이를 일치시키려고 ‘착한 길’에 인센티브를 주거나 ‘악의 길’에 불이익을 주지만, 현실에선 돈이 모든 걸 지배하고 규제 장악도 현실임. Facebook이나 거대 기업들이 신경과학·심리학을 활용해 사용자의 중독을 유도하는 슬롯머신 형태의 플랫폼을 만들 수 있고, ExxonMobil 같은 곳도 환경파괴와 인도주의적 참사를 일으키고도 멀쩡한 현상
          + 이 문제는 더 넓다는 생각. 도덕성과 사회적 계약이 점점 무너지고 있음. 예시로 Mozilla의 변질을 꼽음. 프라이버시 중심 제품을 표방해 구매자를 모은 뒤, 미션에서 프라이버시를 빼버리고 본격적으로 ‘도축’을 시작하는 상황. Craigslist는 제대로 대규모로 구현한 사례임을 보여줌. 이런 마인드와 도덕성을 가진 사람이 극히 희귀해진 현실. Sodom과 Gomorrah의 우화를 우리 사회 파멸의 경고장처럼 받아들임
     * 예전 ‘준전문가’ 커뮤니티 사이트들은 여전히 건재. 특정 주제와 카테고리, 전반적인 논의가 사이트 전반에 펼쳐짐. 반농촌 지역에서 소규모로 자급자족 농작물 기르는 걸 좋아해 permies.com을 즐겨 찾음. 매일 자원봉사자가 계절이나 트렌드에 따라 새로운 질문이나 인기 토픽을 올림. 억대 수익을 노리지 않고, 억만장자를 목표로 하진 않음. 자금 조달은 책, 카드, 동영상 등 자체 콘텐츠 판매와 작고 귀여운 광고로 해결(이마저도 스스로 패러디함). 작음이 아름다움임을 느끼고, 현재의 인터넷은 사악한 파충류들이 당신의 시간·데이터·프라이버시·친구를 노리는 세상으로 바뀌었음. “Don’t be evil” 정신은 사라진 지 오래. 30년 전으로 돌아가기를 택했고, 지금 만족하는 삶
          + tildes.net을 자주 사용함. 전부에게 맞는 플랫폼은 아니지만 좋아하는 이들에겐 훌륭한 공간임
     * 대부분 골프 사이트처럼 사용자에게서 뽑아내는(extractive) 방식이 아닌, 위키를 만들어 보겠다는 거의 지치는 도전 기억이 떠오름. 자금 없이 부트스트랩 방식으로 하느라 운영비도 빠듯. 모든 커뮤니티에서 네트워크 효과가 결국 핵심. 앱이 사람들 폰에 들어가면 가치를 쉽게 알릴 수 있지만, 그 전인 초기에 폰에 심으려면 많은 자금과 네트워크가 필요. 이 때문에 VC 자본의 유혹과, mastodon·lemmy·pixelfed 등 네트워크 기반 프로젝트 시작의 어려움을 경험. 대부분 제품보다 네트워크 그 자체가 본질. 앞으로도 느리지만 꾸준하게 해보겠다는 계획. 실패해도 상관 없다고 생각. 골프 코스 아키텍처에 애정있는 이들이 마음껏 이야기할 장소를 원하는 것 자체가 의미임 https://golfcourse.wiki
          + 멋진 프로젝트임을 인정하며 응원 메시지
     * ‘Addiction by Design’을 추천. 도박 산업에서 실현된 중독 설계 메커니즘을 다룬 최고의 책이라 생각. 오늘날 스마트폰/인터넷 환경과 유사한 부분이 많음. 추천한 HN 유저에게 감사. 최근 몇 년간 읽은 책 중 가장 인상 깊음
          + Nir Eyal의 ‘Hooked’도 그 시기 스타트업에서 ‘Growth Hacking’ 입문서처럼 읽혔던 참고서
     * 이 글에서 진짜 해결책이 살짝 언급되지만 직접적으로 말하진 않음. 바로 정부 규제
          + 원글에 따르면, 중독적이고 해로운 제품이라는 이유로 담배 회사들을 규제하듯 알고리즘 투명성 또는 사용자 통제권 부여로 유해한 디자인 패턴을 줄일 수 있음. 이미 EU의 Digital Services Act 같은 법은 대형 플랫폼에 알고리즘 투명성을 요구함
          + 맞는 말인 것 같음. 소셜 네트워크도 전기나 ISP처럼 공공재(유틸리티)일 수 있음
     * 궁금한 점: 소셜 네트워크의 MVP를 Belarusian 개발자에게 1만 달러 주고 제작해 월 2달러씩 유료화하면 왜 안되는지. 왜 저자는 돈 모으는 게 이렇게 도덕적으로 파괴적인지 불평하는지 이해가 안됨. 그냥 자기 방에서 운영하고 ChatGPT에 리액트 랜딩페이지 물어보며 하면 되는 것 아님?
          + “월 2달러씩 유료화가 왜 안될까?”라는 질문에 대해, 댓글에서는 모두 자신이 기꺼이 돈을 내겠다 하다가 실제론 아무도 결제하지 않는다는 것. 사용자의 요청사항을 추가해도 기대치만 계속 움직임. 소셜 네트워크는 ‘사람’이 없으면 가치가 없고, 아무도 빈 플랫폼에 가입 안함. 심지어 무료 서비스도 초반에 사용자 확보에 실패하는 경우가 많음. Twitter 대체제들도 대부분 실패 사례. 살아남은 곳도 각자 문제점으로 이용자 이탈 경험
          + 틈새, 특히 전문직에겐 먹힐 수 있지만 대중에겐 힘듦. 이미 페이스북에 모두가 있고, 무료라서 엄마 아빠도 굳이 이쪽을 쓸 이유 없음. 더 비싸고 친구도 없으니 매력 없음. 2002년 영국엔 friendsreunited라는 사이트가 1년에 5파운드 받고 흥했던 사례가 있었음. 그 시절과 분위기가 그립다는 소회
          + 사용자가 없으면 소셜 네트워크는 지루함
          + 오픈소스 소셜 네트워크 스택은 이미 여러 개 성숙한 것이 존재. 문제는 기술이 아니라 그 이상임
          + 핵심 문제는 ‘모두를 어떻게 가입시키느냐’임
     * 이런 앱들이 비영리단체 소유라면 덜 중독적이었을까라는 가상 시나리오. VC 돈은 최종 상태(중독성 도달)를 빠르게 만들 뿐이고, 민간 기업이어도 결국 중독성은 생긴다는 생각
          + 한편, 소셜 네트워크 중에서도 중독성 없는 패턴(예: 이 글을 쓰고 있는 HN)도 있고, 그게 어느 정도 ‘표준’일 수 있다는 의견. 메스암페타민 같은 자극적이고 파괴적인 서비스가 화제가 되지만, 실제로 수십억 명이 매일 차(tea)를 즐기듯 평범하고 자극 없는 온라인 커뮤니티·이메일·동네 소식통 등이 주변 사람들의 디지털 라이프 대부분을 차지. 아마 너무 고리타분해 보일 수도 있지만, 실제론 ‘차’ 같은 커뮤니티가 일상 다수를 차지하는 현실
          + ‘근로자가 소유하는 회사’나 대표민주주의 방식의 소유모델에 관심이 있음. 성공한 스타트업이 이 아이디어를 먼저 시도해주길 기대
          + 말 그대로 Mastodon이 해당 모델의 실사례임
     * 본인 경험상, 전자잉크(e-ink) 폰이 효과 좋았음. Bigme Hibreak Pro 사용 중인데, 다소 조작감은 투박해도 기능은 충분함. 이 폰에서 SNS 이용은 진심 재미가 없음. 그래도 필요할 때는 쓸 수 있음. 독서를 예전보다 훨씬 많이 하게 됨
          + iPhone에서 접근성 설정을 grayscale로 바꿔도 같은 효과를 얻을 수 있음
          + Android Auto 사용할 수 있는지 궁금함. 변화에 박수
          + 카메라와 영상 통화가 두 가지 큰 단점. NFC 문제도 있다고 들음
          + 배터리 시간이 훌륭할 것 같고, 게임은 날아가고 문자 메시지는 평소대로 작동하지 않을까라는 궁금증
     * Facebook이 Instagram에 참여/머무름 알고리즘을 도입한 것과 Twitter가 이 방식을 따랐다는 언급이 있지만, 정작 Facebook이 2011년 뉴스피드를 통해 최초로 대중화했다는 점을 언급하지 않은 부분이 아쉬움 https://en.wikipedia.org/wiki/Feed_(Facebook)#History
     * “창업자들은 악의가 없기 때문”이라는 말은 오히려 악한 창업자의 레퍼토리처럼 들린다는 농담
"
"https://news.hada.io/topic?id=21748","$25,000 자동차가 사라지고 있는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         $25,000 자동차가 사라지고 있는가?

     * $25,000 수준의 신차를 찾기 점점 어려워짐
     * 인플레이션과 자동차 내 첨단 기술 추가로 생산비 상승 현상
     * 더 많은 제조사들이 고급차와 SUV에 집중하는 현상
     * 소비자의 월별 할부금 부담이 증가함
     * 저가 모델의 부재가 사회적 불평등 심화 가능성을 제기함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

$25,000 자동차의 몰락 배경

     * 과거 미국 자동차 업계에서는 $25,000 이하의 합리적인 중형 및 소형 자동차가 비교적 쉽게 구매 가능했음
     * 최근 들어 신차 평균 가격이 $47,000 이상으로 상승하며 저렴한 신차를 구하기가 매우 어려워진 현상
     * 이러한 가격 상승에는 인플레이션, 원자재 가격 상승, 반도체 공급난 등 복합적 요인이 작용함

자동차 산업 내 새로운 기술과 트렌드

     * 제조사들이 경쟁적으로 운전자 보조 시스템 및 인포테인먼트 등 첨단 기술을 도입함
     * 이로 인해 단가가 높은 옵션이 기본 사양에 반영되어 전체 차량의 생산비가 증가하는 현상
     * 많은 브랜드가 기존의 중저가 세단 대신 SUV, 크로스오버, 고급 모델에 집중하는 전략을 선택함

자동차 금융과 소비자 부담

     * 자동차 금융 시장에서 일반 대출이자 및 월납입금이 과거보다 크게 상승함
     * $25,000 이하 신차 찾기가 어려워지면서, 더 많은 소비자가 중고차 시장에 의존하는 추세
     * 신차 시장에서 소외되는 저소득층이나 신규 운전자에 대한 우려가 커지는 상황

사회적, 경제적 영향

     * 저렴한 신차의 부족이 사회적 이동성과 경제적 기회 불평등을 심화시킬 가능성
     * 일부 전문가들은 정부의 지원 정책이나 업계의 혁신이 필요하다고 지적함
     * 전동차(전기차) 확대도 신차 가격 상승에 일부 영향을 미치고 있음

결론 및 전망

     * 자동차 업계와 소비자 모두 새롭게 변화된 시장 환경에 적응 필요성 대두
     * 미래에는 다양한 가격대와 옵션을 갖춘 차량 라인업 확대가 요구되는 분위기
     * 자동차 산업의 트렌드 변화를 지속적으로 주시할 필요성

        Hacker News 의견

     * 현실과 동떨어진 내러티브라도 특정 의제(요즘은 모든 것이 비싸다는 주장)에 부합하기만 하면 업보트가 몰리는 현상 지적
          + 2025년엔 2만5천 달러 이하 신차가 12종
          + 2005년엔 물가 반영 시 2만5천 달러 기준으로 10종
          + 즉 “예전이 훨씬 더 저렴했다”는 전제가 틀린 정보
          + 더 데이터 기반의 논의가 Hacker News에 어울린다는 생각
          + 데이터 중심 논의를 강조하긴 해도, 실상 가격 인상 폭이 임금보다 더 가파른 점 무시 어려움
               o 평균 신차 가격: 2005년 2만3,017달러 → 2024년 4만7,465달러 (물가 반영 시 +32% 상승)
               o 중위 가구 소득: 2005년 4만6,242달러 → 2023년 8만610달러 (물가 반영 시 +12% 상승)
          + 2005년에 1만5천 달러 수준, 괜찮은 차를 살 수 있었지만, 지금 모델 수가 많아졌더라도 예전 1.5/2.5만 달러로 살 수 있던 차급은 다운그레이드된 게 현실
               o 당시 자료가 있다면 근거를 보고 싶음
          + 물가 상승률을 감안하면 구매력 위기는 사라진다는 주장, 아이러니함
          + 차 가격뿐만 아니라 소득 수준도 감안해야 진정한 “구매력” 논의 가능
          + “현실 점검” 출처와 작성자의 의도가 궁금함
     * 나는 내 차를 직접 관리하며(취미로 시작) 신차가 복잡해진 영향으로 가격이 오르는 현실을 체감
          + 90년대 초 차는 단순한 구조라 전기 문제도 멀티미터로 쉽게 추적 가능
          + 예시로 퓨즈박스 접촉 불량은 직접 찾아서 20파운드 미만으로 해결
          + 최신 차는 OBD-II 스캐너와 스마트폰 앱이 꼭 필요
          + 센서 오류로 “엔진 점검” 경고만 꺼지게 하려 4개의 글로우 플러그 센서 교체에 800파운드 발생
          + 엔진은 멀쩡해도 비싼 센서 교체만 반복되는 경험
          + 요즘 기본에 충실한 차를 내놓는다면 무조건 구입
               o 다양한 불필요한 기능(타이어 공기압 센서, 비상 호출 버튼, 자동 시트 조절, 이중 온도 조절 등) 없는 정직한 차 요구
               o 오히려 스페어 타이어가 빠지고 무의미한 수리 킷만 제공되는 점 불만
          + 글로우 플러그 교체를 하나씩 한다는 사실에 놀람
               o 2015년엔 2002년형 디젤차용 글로우 플러그 4개가 80달러였는데, 요즘 2013년형 글로우 플러그 하나에만 135달러 소요
               o 압축 센서가 추가돼 비용이 증가
          + 자동차 전자 부품 가격 거품에 불만
               o 센서류 원가는 한 자릿수일 텐데 실제 판매가는 300유로 수준
               o 수리 가능성이 떨어지는 결과
          + 전기차 하드웨어는 오히려 단순하지만, 소프트웨어가 새로운 복잡성 유발
          + 최근엔 타이어 펑크 하나에 각종 센서와 제어 기능 오류까지 생기면서, 단순 15분짜리 수리가 센서·컴퓨터 초기화까지 기다리며 1시간 넘게 걸리는 현상 목격
     * 유럽 시장에서는 여전히 Renault Clio나 Skoda Fabia 등 2만 유로 미만 신차 존재
          + 미국에서 이런 소형차가 사라진 이유가 궁금
          + SUV 규제에 따른 ‘SUV 루프홀’(비승용차로 분류돼 배출 기준 완화) 영향이 미국 소형차를 비수익 구조로 몰아간 사실 언급 부족도 지적
               o 현재는 하이브리드/전기차의 동력 계통 유무가 연비에 더 큰 영향을 주는 시대
                    # 하이브리드 SUV가 일반 세단보다 연비가 좋기까지 함
                    # 초기 구매가는 높지만 TCO(총 소유 비용)가 낮아 결국 신차를 새로 살 수 있는 소비자는 미리 더 내고 장기 혜택을 얻으니, 저가 신차 시장이 축소되는 추세
                    # 새 차 구매 여력이 없는 사람들은 원래 중고차 시장을 핵심적으로 이용
               o SUV가 더 많이 판매되다 보니, 모든 차로 SUV를 사고 싶어하는 경향이 가속됨
               o 유럽에서도 소형차 사라지는 현상 진행
                    # 환경 규제나 강제 설비 등 고정비는 비싼 차와 동일하게 드니, 싼 차가 ‘조금 더 싸지만 훨씬 품질이 떨어지는’ 이상한 선택지로 전락
               o Clio·Fabia는 실제로 프로모션가이거나
                    # 파이낸싱(7~8% 고금리) 조건이 붙고
                    # 현금 구입이 불가하며,
                    # 인도까지 최대 10개월 대기
                    # 다양한 현실 조건 존재
               o 미국차 판매 대부분이 실제론 라이트 트럭 라인업임
     * 미국 상황은 단순
          +
              1. 제조사 마진: 고가차/럭셔리/SUV/트럭에서 대박, 저가차는 매우 박리
               o 2008년 금융위기 전에도 “이코노박스” 5~10대 팔아야 대형차 1대 수익
          +
              2. 빚의 일상화: 월 할부는 너무 보편화
               o 마케팅, 판매 전략, 문화적 변화로 월 납입금에 맞춰 역산해서 더 비싼 차를 사는 구조
               o 72~84개월 장기 할부까지 감수
               o 실제로는 소유 종료 전 교체·업그레이드하고 손실은 누적으로 쌓임
               o 제조사나 딜러, 대출사, 투자자 등 모두 이 구조 속에서는 수익
               o 소비자 자산 형성이나 저축에는 비효율
          + 이 두 요인이 저가 입문 차량을 점차 사라지게 만들었고, 그런 욕구 역시 높지 않음
               o 나는 집값은 오르기 때문이라도 주택 대출을 이해하겠지만, 차는 무조건 가치 하락이므로 절대 빚내서 사고 싶지 않음
                    # 20년 된 Outlander, 10년 넘은 Nissan Leaf도 잘 탑승
                    # 어쩔 수 없을 때만 할부 고려, 한도·기간 최소화
               o 미국에서 차량 비용은 저소득층의 사회적 이동성을 심각히 저해
               o 유럽에서는 신차 구입 평균 나이 50세
                    # 주로 중고차 위주 시장
                    # 차는 사치품 이상으로 인식, 집과 가족에 우선 투자
               o 저가차 수요가 MSRP 넘을 만큼 높다는 기사 주장과 부분적으로 상반
               o 제조사 마진 문제의 해법은, 시장점유율 확보를 위해 적은 마진 감수하는 새로운 경쟁자 등장 가능성
     * 전혀 언급 안 된 사실: 중국산 EV는 미국 제외 세계적으로 1만 달러 수준
          + BYD Seagull EV 관련 링크
               o 사실 중국·동남아 일부 외에는 이 가격이 아님
                    # EU에서 BYD Seagull/이름변형으로 1만유로 이상
                    # 가격전쟁 중이라 마진 거의 없음
                    # 경쟁 후 살아남은 기업이 정해지면 가격은 다시 오름 예상
               o 영국 1만8,560파운드, 스페인 1만1,780유로 등 타국 역시 1만 달러 수준 아님
                    # “중국차 전세계 1만 달러”는 100% 옳지 않음
               o 중국차는 상당한 발전이 있었지만 동남아조차 가격이 2만5천 달러까지 오름
                    # 미국 수입 희망이지만 전세계적으로 박리다매로 경쟁사 고사 노림 전략
               o 유럽에서 중국산 전기차엔 추가 관세 부과
                    # 일부 서방(호주·영국)만 관세 없음
                    # 실제로 큰 중국차가 더 잘 팔리는 경향
     * 미국 제조사가 저가차에 관심 없는 진짜 이유는, 저가 신차에선 파이낸싱으로 남는 게 별로 없기 때문
          + 항공사가 사실상 마일리지 기반 신용카드 발급이 목적이듯, 미국 딜러는 파이낸싱(대출 상품), 자동차는 '그냥 연계 상품'에 지나지 않는다는 생각
               o 중고차 현금 구매하려 하니, 딜러는 파이낸싱 안 한다는 사실 알게 되자 관심 급감
                    # 딜러의 주요 사업이 ‘차’가 아니라 ‘대출’임을 체감
               o 불법적인 담합만 아니라면, 결국 누군가 저가차 시장 기회를 노릴 것
                    # 실제로 2만5천 달러 미만 신차는 Chevrolet Trax, Nissan Versa, Hyundai Venue, Kia Soul, Nissan Kicks 등 다수
                    # Toyota Corolla, Hyundai Elantra, Volkswagen Jetta 등도 2만2천 달러대 가능
                    # 기사의 신빙성 의문
               o 예산이 허락한다면 중고차 현금 구매가 딜러와의 복잡한 상담 없이 최상
               o 딜러의 사업 모델은 오리진·연장 보증·서비스를 통한 수익(40년 경력 자동차 판매자 경험 전언)
                    # Costco와 같이 실제 제품은 ‘마진 없음’, 수익은 부가 서비스나 멤버십에서 나오는 구조
               o 동일 차종 간 차별 요소 차별화 시도가 ‘역기능적인 인센티브’로 작동
                    # 수익이 제품 자체보다 ‘차별적 부가 가치’로 이동
     * 인플레이션을 간과하는 흐름이 이상
          + 기사에선 차량 평균가 29.2% 상승이라 언급
          + 미국 노동통계국 계산기로 본 물가상승률 26.2%
          + “단순히 인플레이션 때문만은 아니다”라지만, 대부분은 인플레이션 영향
               o 신차 가격은 인플레이션 지수 산정 대상이므로, 비교가 큰 의미 없음
                    # 원래 두 지표는 같이 움직임
     * 기사 정보가 부정확하다고 봄
          + 최근 아들과 차 고르는데, 2만5천 달러 이하 신차 모델은 12종
          + 2005년 10종(1.5만 달러, 인플레 반영)에서 변동 거의 없음
               o 2025 Nissan Versa $18,330
               o Hyundai Venue $21,395
               o Kia Soul $20,490
               o Nissan Sentra $21,590
               o Nissan Kicks $21,830
               o Hyundai Elantra $22,125
               o Kia K4 $21,990
               o Toyota Corolla $22,325
               o Chevrolet Trailblazer $23,100
               o Subaru Impreza $23,495
               o Buick Envista $23,800
               o Toyota Corolla Hybrid $23,825
               o 실제로 2만5천 달러 이하 차는 있지만, 딜러에서 기본 사양을 구하기 어려움
                    # 불필요 업그레이드 팩을 끼워넣어 가격이 오름
                    # Subaru Impreza 실제 구매가 궁금
               o 답은 2~4만km 뛴 리스차를 신차 대비 20~40% 저렴히 구입
                    # 신차와 거의 차이 없으며, 대부분의 수명 남음
               o Mazda 3(세단, $24,150)와 Chevrolet Trax($20,500)도 저가 신차임
               o 차 가격은 5~7% 올랐으나, 생산원가는 오히려 5~7% 하락 예상
                    # 더 큰 이슈는 여기에 있음
     * 기사에 빠진 사실: 미국인들이 자동차 대출로 지고 있는 금액이 1.64조 달러, 전체 소비자 부채의 9%
          + 자동차 대출이 학생 대출보다도 많고, 평균 대출 기간이 거의 6년
          + 60일 이상 연체율 6.6%로 역대 최고, 일부 주에선 9%
          + 가격이 오르는데도 대출로 더욱 깊게 빠지는 현실
          + 궁극적으로 문제는 생산자나 은행보단, 소비주의와 자동차의 ‘지위 상징화’ 문화 때문
               o 미국 경제 자체가 부채 위에 성립
                    # 소비자의 ‘도덕적 각성’만으로 해결 못 하는 구조
                    # 대부분의 경제 주체가 이 시스템에서 수혜
                    # 급격한 습관 변화는 경제 위기로 이어질 수 있음
               o 저가차가 사라진다면, 제조사·은행·딜러의 책임도 분명 존재
               o 길거리에서 슈퍼 듀티 트럭에 피자 배달용 표지 붙은 모습도 종종 목격
               o 자동차는 언제나 단순한 생활필수품이기보다, 지위의 상징 역할을 해옴
               o 주행거리 많은 이들에게 좋은 차량은 확실히 삶의 질에 직결
                    # 벌받는 듯한 작은 차에 몇 시간씩 있기 싫음
     * 내가 경험한 현실: 새 차 1만 유로에 살 수 있다는 환상은 깨졌고, 실제로 최근 10년간 가격이 두 배
          + Fiat Panda 기준 엔진/옵션 동일해도 10년 전보다 딱 두 배
          + 입문차가 2만5천 유로 가까이(1유로=1달러로 생각해도 됨)
          + 단, 전기차는 중국산 덕에 EU 보조금 덕을 봄(BYD Dolphin 등 2만 유로 미만 실현, 동급 Volvo XC40과 유사)
               o 정부 임대주택 보조금은 의도치 않게 집값을 오르게 만드나, 자동차 보조금은 경쟁에 도움이 되는 양상
               o 만약 2015년에 1만 달러를 보유했다면 2025년엔 실질 가치 5천 달러밖에 안 됨
                    # 옛차를 보관하는 게 사실상 투자 행위
               o 실제로는 2024 Clio 가솔린+LPG 신차를 1만7천 유로에 구입
                    # ICE(내연기관)는 2만5천 유로 이하로 쉽게 가능, 하이브리드가 그 이상
                    # 하이브리드가 더 비쌈
               o BYD Dolphin이 1만7천 유로면 ‘너무 비싸다’(일본 JPY 기준)
                    # 국가별 가격 차이 실감
               o 영국은 전기차 보조금 폐지해도 Dolphin이 1만7천 파운드
                    # 중국 경제 모델이 정말 독특
                    # 나도 구매 고려 중
               o “EU가 1대당 1만 유로 보조금” 주장은 사실 무근
                    # 국가별로는 인센티브 있지만 EU 전체 정책은 아님
                    # 오히려 중국산 전기차에 높은 관세까지 있음
                    # 근거 자료가 없다면 과장된 주장으로 의심
"
"https://news.hada.io/topic?id=21723","중산층 뮤지션의 죽음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              중산층 뮤지션의 죽음

     * 스트리밍 플랫폼의 등장과 물리음반 시장의 붕괴로 인해 중산층 뮤지션의 생계 유지가 어려워짐
     * 뮤지션들은 레이블과의 계약, 투어, 프리랜스, 정부 지원금 등 다양한 방법으로 수익을 창출하려 했으나, 불안정성과 저수익에 시달림
     * 대형 레이블과 스트리밍 서비스만이 수익의 대부분을 차지하며, 대부분의 뮤지션은 연간 수천 달러 미만의 수익만 얻음
     * 투어 비용 상승, 인플레이션, AI 음악 생성 등 외부 요인까지 악화되어 정신적 스트레스와 중산층 뮤지션의 소멸 현상을 가속화함
     * 해결책으로는 아티스트 주도의 가치 재평가, 정부의 지속적 투자 및 새로운 수익 모델 도입이 논의되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

중산층 뮤지션의 등장과 소멸

   Rollie Pemberton는 10대 때 래퍼 활동을 시작해 인터넷을 통해 작품을 알렸음. 이후 Cadence Weapon이라는 예명으로 활동하며 Pitchfork 등의 음악 매체에서 평론가와 뮤지션 양면의 경력을 쌓음.

   2006년 Upper Class Recordings와 360 계약을 맺으며 앨범 발매, 투어 등 아티스트 성장의 기회를 얻었으나, 아티스트 수익 배분 구조의 불합리함으로 대다수 수익을 레이블이 가져가고 자신은 근근이 생계를 이어감. 2006년부터 2015년까지 레이블에 25만 달러 이상의 수익을 가져다줬지만, 본인은 1만 달러 상당의 선급금 외엔 거의 실질 수입이 없었음.

레이블 구조와 스트리밍의 부상

   20세기 후반, CD 붐과 물리매체 판매로 뮤지션의 중상층 생활이 가능했으나, Napster 등 파일공유와 스트리밍 시대로 전환되며 업계 구조가 급변함. Sony, Universal, Warner 등 대형 레이블이 시장의 70%를 장악하며, 자신들이 보유한 콘텐츠 카탈로그와 스포티파이 지분 등으로 막대한 수익을 거둠.

   반면 스트리밍 플랫폼의 수익 배분은 극도로 낮아져, 스포티파이에서 백만 회 이상 스트리밍돼도 수천 달러 미만의 수익만 발생함. 주요 레이블은 소수 스타 아티스트 외에는 대다수 신인 및 중견 뮤지션에게 지속적 지원을 하지 않음.

스트리밍과 독립 아티스트의 현실

   스트리밍 서비스의 등장은 음악 제작과 유통의 민주화라는 장점을 가져옴. 누구나 직접 음원을 업로드하고 SNS를 통해 팬층을 확장할 수 있지만, 수만 곡이 매일 신규 등록됨에 따라 경쟁이 과도하게 심화됨. 대부분의 뮤지션은 생계형 수익조차 얻기 힘든 현실에 봉착함.

   SOCAN 대표 Jennifer Brown 등에 따르면, 아티스트들은 부와 명성보다 가족 부양과 존중받는 생계를 원하며, 백만 스트림에서 600달러 수준밖에 받지 못하는데 큰 실망을 느끼고 있음.

공연 시장의 악화와 투어의 현실

   코로나19 이후 살아남은 공연장도 줄었고, 투어 비용 역시 인플레이션, 보험료, 비자 비용 증가 등으로 극심하게 상승함. 투어가 더 이상 수익원이 아니라 손실을 보는 구조로 변질됨.

   예를 들어, Tokyo Police Club은 마지막 투어 시 팬데믹과 경쟁 증가, 물가 상승, 미국 비자비 급등 등으로 인해 과거와 같은 지속적 수익성 확보가 어려워졌음.

뮤지션의 정신 건강 위기

   경제적 불안, 낮은 직업 안정성, 과도한 노동, 성과 부족에 따른 좌절감은 뮤지션 다수에게 불안과 우울증, 자살 충동 등 정신적 위기를 초래함. Revelios의 “Soundcheck” 조사에 따르면, 86%가 정신 건강 문제를 겪었고, 94%가 상시 불안정성을 느꼈음.

지원금과 민간자본의 한계

   뮤지션들은 정부 지원금, 기업 후원, 자선기금, 사적 이벤트 등 다양한 경로로 직접 지원을 받음. 하지만 정부 예산 삭감, 민간자본의 한계 등으로 충분한 안정성을 제공하지 못함. 진정한 해결을 위해서는 지속적 정부 투자와 새 수익 모델이 요구됨.

음악의 가치와 구조적 문제

   음악은 사회적으로 필수적인 문화 자산임에도, 디지털화와 스트리밍으로 인해 화폐적 가치를 상실함. 대중은 음악이 언제 어디서나 무료로 제공되길 기대하지만, 이는 제작자 생존과 직결되는 구조적 불합리로 이어짐.

아티스트 주도의 가치 재창출

   뮤지션 Torquil Campbell은 직접 판매, 맞춤형 곡 제작 등으로 자신의 음악 가치 재정립 사례를 공유함. Bandcamp와 같은 플랫폼을 떠나 MP3 직접 판매, 팬과의 직접 소통 확대 등 독립적 수익 창구가 중요함을 강조함. 그러나 이러한 모델도 충분한 팬층 확보 전까지는 한계가 존재함.

제도적 변화의 필요

   Taylor Swift와 같은 영향력 있는 아티스트의 계약 사례, 사업자들의 윤리적 기준 준수 요구 등 아티스트 연대와 집단행동의 가능성이 제시됨. 스트리밍 플랫폼이 개별 청취자의 구독료를 실제로 청취하는 아티스트에게 분배하는 구조로 변화해야 한다는 주장도 제기됨.

   SOCAN, Six Shooter 등 업계 인사들은 정부의 예술 투자 확대, 인프라 구축의 중요성을 역설하나, 결정적 해결책으로는 음악의 가치 자체를 사회적으로 재인식하고 구조 전환을 도모해야 함을 강조함.

결론과 미래 전망

   대형 레이블 및 스트리밍 플랫폼 중심의 현 체계는 중산층 뮤지션의 존립을 위협하며, 사회 전반에 부작용이 확산됨. 근본적 해결을 위해서는 아티스트 스스로의 창의적 저항, 집단적 행동, 가치 재정의와 더불어 사회적 자각 및 정책적 뒷받침이 필수 요소임. Rollie Pemberton의 #MyMerch 캠페인, 독립 뮤지션 중심의 작은 공연, 자주적 경력 설계 등이 하나의 대안으로 논의되고 있음.

   뮤지션 Lido Pimienta처럼 단순한 생계를 희망하며, 중산층 예술인의 소멸이 사회 전반에 미치는 영향을 경계해야 하는 시점임. 음악과 예술의 존속을 위해, 팬 참여와 사회적 지원, 그리고 아티스트의 가치 재창출을 향한 노력이 강조되고 있음.

        Hacker News 의견

     * “정부가 보편적 기본소득을 도입해야 한다”라는 답변을 다른 어떤 대답보다 자주 들은 경험 공유. 이런 아이디어를 “비현실적”이라고만 치부하면 우리가 가진 근본 문제, 즉 전반적인 경제적 불평등 문제는 절대 해결 불가. 점점 더 많은 사람들이 제대로 먹고 살 만한 직업 기회조차 사라지는 현실. 기본소득, 부유세, 대기업 분할 등과 같은 광범위한 해결책이 각 산업별 미세 조정보다 훨씬 낫다는 믿음
          + 기본소득, 부유세, 대기업 분할 등 광범위하고 구조적인 해결책이 필요하다는 데 동의하지만 기본소득은 여러 차례 실험됐으며 긍정적이거나 부정적인 뚜렷한 효과가 거의 없는 것으로 드러난 점 지적. 새로운 뮤지션을 만들어내지도 않음. 행정적으로 단순해서 비용 절감에 유리한 점은 장점. 부유세는 자산 매각을 강요해 인플레이션을 유발할 수 있고, 그럼으로써 뮤지션이 줄어드는 방향. 실제로는 국가별로 작동하는 시스템을 베껴야 한다고 생각. 일본은 저렴한 생활비, 성숙한 교육 산업, 상대적으로 낮은 임금 구조로 인해 음악인이 많은 나라라는 예시. 특히 일본 여성들은 일자리가 제한적이라 아이돌로 데뷔하는 것도 흔한 현실
          + 나는 아마추어로서 높은 수준에서 운동, 음악, 미술 등 다양한 활동 중이지만 회사원으로 많은 세금을 내는 사람의 입장. 왜 남들이 자신의 재능을 위해 전적으로 지원받아야 하는지 설득 필요. ‘진짜 열심히 해도 못 버티는 사람’과 ‘예술에 순전히 열정으로 뛰어드는 사람’을 혼동해서는 안 된다 생각. 끊임없이 내게서 더 많은 걸 요구한다면 나는 더 이상 기여하지 않을 마음. 이미 초부자 및 고소득자들은 각종 방법으로 분담을 회피 중
          + 문제의 근원은 사회적·개인적 선호에 있다고 여김. 뮤지션 사이의 소득 격차? 사람들은 일부 뮤지션과 노래를 훨씬 선호. 기본소득이나 과세 정책은 엔터테인먼트 분야에서 중위권과 상위권의 격차 해소에 실질적으로 별 영향 없음. 주택 부족 문제도 마찬가지, 결국 사람들의 공간·입지 선호에서 비롯. 근본 원인을 먼저 진단하고 접근해야 진짜 해법
          + 이 문제는 특정 산업 실패에 관한 게 아니라, 시스템 전체가 아래에서 위로 가치를 빨아들이도록 설계된 구조 문제라는 시각 공유
          + 경제 규칙 자체가 불공정한 것에서 문제가 출발. 모노폴리 게임에서 몇 번 라운드 지난 뒤 뒤늦게 참가하는 상황과 비슷한 비유 활용. 지나치게 높은 주거비도 그런 예시. 그리고 열심히 일해서 많은 돈을 번 이들이 자신의 선택과 다른 이들의 삶에 의식적·무의식적으로 그 돈을 ‘무기’로 쓴다는 점에서 또 다른 불평등 유발. 시스템이 망가졌다는 것을 인정하는 게 출발점이라는 확신
     * 공대 출신 고소득 엔지니어들과 밴드에서 활동했던 경험 공유. 아예 전업을 고려할 정도로 음악을 즐겼으나, 네 자릿수 공연료를 받아도 결국 시급으로 따지면 최저시급에 못 미친 수준. 매니저 고용, 출장비 등으로 수입 대비 실질 수입은 더욱 감소. 평일에는 공연 자체가 없어 생계유지가 불가능. 만약 일주일 내내 라이브 공연이 가능한 식당 등이 지역적으로 늘어나면 뮤지션들이 평일에도 일할 수 있어 전업이 현실화될 수 있겠지만, 아직 그 정도 기반도 부족. 평일 스케줄을 공연으로 채울 수 없으면 재능 있는 뮤지션들은 결국 딴 길로 빠질 수밖에 없는 현실 직시
          + 내슈빌 브로드웨이에서 일주일 내내 공연한다 해도 노력 대비 처참한 월급. 특정 장르로 한정되고, 하루에 여러 번 공연하는 소모성 삶. 음악가로 살아남으려면 교회 밴드, 웨딩 밴드, 세션, 강습, 악기 테크 등 다양한 부업을 병행해야 하며, 그 중 절반 이상이 순전히 운에 좌우됨. 정말 힘든 현실이지만, 절실하게 느끼는 부분
          + 최근 팬데믹 이후로 평일 야간 공연이 꽤 많아졌다는 변화 포착. 회복기 동안 활발해졌고, 아직까지 사라지지 않아서 긍정적 변화로 봄
          + ‘세상이 음악을 충분히 가치 있게 보지 않는다’는 결론에 이르게 됨. 결국 대다수의 음악인은 자기 자신을 위해 만드는 ‘놀이’ 같은 느낌. 직업으로 삼으려면 커다란 희생 감수. 현재 세계가 창의성에 충분한 가치를 부여하지 않음. 이 상황을 바꾸고 싶지만 현실이 우울하게 느껴짐
     * 기사에서 소개한 Rollie Pemberton 사례와 레이블의 360 계약 구조 설명. Pemberton의 수입이 투어·음반이 아닌 상금과 보조금이 중심이어서, 계약 구조상 Upper Class Records가 비정상적 이익을 가져간 독특한 케이스. 대부분의 뮤지션에게는 이런 계약이 적용되지 않음. 사실 레이블들도 중위권 아티스트에서는 손해 볼 경우가 더 많음. 대다수 미디어·스타트업·제약에서도 비슷하게 ‘슈퍼스타’와 극소수 성공 사례가 전체 수익을 좌우. 수익구조상 ‘패자가 아닌 승자만이 전체를 먹여살림’. 중간급 계약도 종종 손실 리더 수준, 성공한 소수에 집중해 더 좋은 기회를 노리는 구조. David Lowery의 관련 칼럼 추천
          + 슈퍼스타 경제구조 이론(Rosen, Sherwin. ""The Economics of Superstars"") 언급. 개인별 아주 작은 차이가 엄청난 소득격차를 만든다는 설명. 예술가의 저임금 구조는 삶을 포기할 의향이 있는 인재의 ‘공급 과잉’과, 프로모션 및 마케팅을 통한 ‘돈이 되는 희소성 강화’가 원인. 레이블이 더 많은 지분을 가져가도 당연한 구조. 아티스트가 어느 규모 이상으로 성공해야만라도 소득이 극적으로 성장
          + 한편으로 대부분의 밴드나 아티스트가 얼마나 많이 실패하며, 그 과정에서 돈과 노력이 얼마나 드는지 간과하기 쉽다는 점 강조. 또한 레이블들이 시장 지배력과 자금력으로 각종 ‘페이올라(뒷돈 홍보)’ 등 불투명하게 업계를 장악. 시스템을 개선하려면 투명성이 도움이 되겠지만, 이 ‘불투명함’ 자체가 현재 시스템 유지의 조건이기도 함. 만약 사람들에게 자신이 성공해도 수익이 많이 남지 않는다는 걸 알린다면 대부분 이런 위험 부담을 감수하지 않으려 할 것. 성공 공식이 없듯, 결국 다 ‘편향된 동전 던지기’로밖엔 설명이 안되는 업계 현실
     * 기사에서 오케스트라나 세션 연주자 대신 래퍼 위주 사례만 다루는 게 아쉽다는 의견. 실제로는 녹음음악이 등장한 이후 소비자는 중간 수준보다는 상위권 뮤지션 녹음물을 택하게 됨. 따라서 소규모 유명세를 가진 ‘중산층 뮤지션’이 되기는 정말 힘듦. 결국 로컬 시장, niche 개척, 거리공연 등으로 살아가는 방식이 나쁘지 않다는 생각
          + 15~20년 전에는 인터넷과 추천 시스템이 ‘롱테일 시대’를 열 것이라 기대했으나, 실제로는 스트리밍과 오픈 배급이 역으로 아티스트 수입을 대폭 감소시킴. ‘아티스트도 새로운 수익 모델을 찾으면 된다’는 이야기가 대중적 위안으로 소비되며 스스로 자기 가치를 포기하게 된 현실 한탄. Minor league 수준의 예술도 어려운 도전이었지만, 스포티파이 시스템이 모든 가능성을 먹어치움. 근본적 문화 변화 없이 이 구조가 계속될 전망
     * 대다수 뮤지션이 현재는 ‘중산층’이며 소수의 슈퍼스타와 다수의 빈곤 아티스트가 공존. $20~100의 작은 공연 수입과, 큰 무대를 뛰어도 8시간 준비해서 $200 버는 고된 경험 공유. 과거에는 뮤지션이 각종 행사에 필수였지만, 이제는 휴대폰 재생으로 음악 대체 가능해진 시대 자조
          + 작은 규모(100~200인) 공연장에서 연주하는 밴드는 그저 순수한 열정 때문에 무대에 오른다고 봄. 오히려 이런 밴드가 더 흥미롭다고 느끼는 이유. 공연 수입이 적어진 걸 알기에 굿즈 등으로 추가 후원하는 마음. ‘싼 값에 인상적인 경험’, 미스해도 부담 없는 구조로 즐길 수 있음. 노르웨이는 대형 공연장이 많지 않으니 이런 문화를 더욱 긍정적으로 봄
          + John Philip Sousa가 녹음음악의 부정적 효과를 정확히 간파했던 것을 언급
          + 오픈소스 음악(거리공연)이 하루아침에 끝났다는 말을 들어도, 실제로 거리공연에서 얻는 수입이 수만 명이 쓰는 오픈소스 프로젝트보다 나을 수 있다는 점 강조
          + 스피커로 음악을 재생하는 것과 실제 라이브 공연은 전혀 다른 경험이라는 의견
     * 점차적으로 뮤지션의 배경이 더 부유해지는 경향비판. 금전적 지원 없인 예술 도전에 큰 위험 존재. 노동계급이나 저소득층 뮤지션의 시대는 저물고 있다는 자각
          + 영국에선 예전 실업수당(golden age of the dole) 제도가 하위·중산층 예술가들에게 예술을 꽃피울 시간과 기회를 제공했다는 사례 공유. 관련 기사
          + 과거 과학 분야도 마찬가지로 부유층 후원이나 가족의 재력 없인 연구가 어려웠다는 점 지적
          + 모든 크리에이티브 산업이 비슷하게 ‘금수저’ 위주라는 지적. 명품 패션, 레코드 레이블, 아트, 문학 등 상위권 진출하려면 고가의 인턴십과 대도시 생활 부담. 요즘엔 SNS 팔로워 수조차 커트라인
          + 뮤지션 직업은 배우와 매우 유사하다는 인식. 네트워크, 재력, 혈연관계가 가장 중요한 요소. 예시로는 ‘부모가 배우면 자녀의 캐스팅을 조건으로 출연’, 혹은 부자 부모가 자녀의 출연을 조건으로 영화를 투자하는 사례(니콜라스 케이지, 제프 브리지스 등). 기술계 부자도 마찬가지 방식. 배우든 뮤지션이든 자본과 인맥이 관건
          + 농구와 같은 스포츠에도 같은 현상 반복됨. 더 비싼 캠프와 네트워크를 거친 자녀들이 좋은 기회를 선점. 브로니 제임스(르브론 제임스 아들)처럼 ‘금수저 현상’이 스포츠까지 침투한 현실. 스포츠 조차 ‘기회의 평등’이 아니라는 아쉬움
     * 뮤지션이 경제적 자립을 해야 할 ‘적정 수’가 얼마냐는 질문 제기. 스트리밍이 수를 줄였지만, 녹음음악 자체가 시장을 좁혔다는 점 인정. 음악 자체를 직업이 아닌 취미 예술로 다시 바라봐야 하는 건지, 본질적 물음 제기. 풍경화로 먹고사는 사람이 없는 것처럼, 음악도 본진의 자리가 달랐던 게 아닌지 고민
          + 사랑으로 창작하되 수익화 방법을 찾지 못하는 이라면, 그 예술에 들이는 시간 자체가 제약됨. 결과적으로 최고의 작품, 최고의 창작 경험도 나오지 않을 수 있음. 만약 엔지니어링 등 전문 분야도 음악처럼 생계가 힘든 ‘취미’로 축소된다면 어떤 일이 벌어질지 상상. 현장의 실질적 차이, 기술 혁신의 부진 등 현실성이 심각
          + 음악 및 예술 분야는 전업과 아마추어가 균형을 이루며 발전한다는 점을 강조. 프로 연주자만이 할 수 있는 부분(오케스트라, 전문 강사 등)이 있고, 아마추어는 독창적·실험적 음악이나 공연시장, 악기 등에서 전문 씬을 뒷받침. 대다수 음악 분야는 프로와 아마추어 양쪽이 있어야 제대로 작동
          + 한편, 스트리밍이나 유튜버 같은 신생 산업에서 ‘이 직업으로 살아야 하는가?’라는 질문도 던질 필요. 산업적 구조나 강력한 지적재산권·규제 강화가 진짜로 예술의 질을 높였는지, 오히려 기회비용만 키운 게 아닌지 회의감
          + 음악 소비자 입장에서는 ‘얼마나 다양한 음악을 원하는가’가 중요한 질문. 프로 뮤지션이 줄어들면 시장 자체에 다양한 상품과 서비스가 줄어드는 결과
          + 스트리밍은 산업구조의 한 단계일 뿐, 실제로는 녹음·방송 등 기술 발전 이후부터 ‘승자독식’ 현상이 훨씬 심해져서 대다수 예술가가 생계 어려움. 앞으로 AI로 인해 과거 수백 년 전처럼 다시 후원자와 파트론 중심의 구조로 돌아갈 전망. 과거에도 예술가가 부자가 되진 못했으나, 적어도 창작에 투신할 시간을 보장받았음
     * 팬데믹 이후 거의 모든 활동의 비용이 크게 올라갔다는 점을 주목. Ever Given의 수에즈 운하 사태 등 공급망의 붕괴도 복합 원인. 팬데믹·공급망 문제가 끝났는데도 왜 가격이 정상화되지 않는지, 단순 기업 횡포 외에 더 깊은 설명 필요하다는 궁금증 제기
          + 가격이 쉽게 떨어지지 않는 건 인플레이션의 일반 패턴일 수 있지만, 팬데믹을 거치면서 각종 소비 습관, 라이프스타일이 완전히 바뀌었다는 가설 제기. 재택근무, 스트리밍, 배달주문 등 별거 아닌 것처럼 보였던 기술들이 팬데믹을 거치며 대중화돼 코로나 이후에도 시장 구조가 예전으로 돌아올 수 없게 됨
          + 팬데믹 기간 동안 각국 정부가 과도하게 통화량을 늘렸기 때문에 현재의 높은 물가가 쉽게 풀리지 않는다는 설명
     * 진입장벽이 매우 낮아진 것이 또 다른 문제로 작용. 과거엔 실력을 쌓아 음반계약을 얻어야 했지만, 이제는 프로그램(Logic 등)을 다운받아서 자동화·보정으로 음악을 만들고 곧장 스트리밍 서비스에 업로드가 가능. 그렇기에 MonoNeon처럼 진짜 실력있는 음악가가 오히려 티켓 파워를 갖게 되는 아이러니
     * 음악 프로듀서로 일하다 5년 전에 데이터사이언스 쪽으로 커리어 변경한 경험자. 음악계에서 성공하려면 실력과 네트워크를 갖춰도 결국 ‘운’이 절대적으로 중요. 그리고 그 ‘운’의 창문은 최근 더더욱 좁아짐
          + 전직 소프트웨어 엔지니어, 현직 데이터 애널리스트로서 데이터 분석 업무가 음악 만드는 것과 상당히 유사하다는 흥미로운 경험담. 커리어에서 ‘위로’ 가고 싶지만 데이터 사이언스·엔지니어링은 너무 퍼포먼스 위주로 예술적 재미가 부족. 지금은 LLM(대형언어모델) API 연동 등으로 여전히 프로그래밍도 할 수 있어 ‘제너럴리스트’로서 즐겁게 일하는 자신을 긍정
"
"https://news.hada.io/topic?id=21711","Facebook, 아직 공유하지 않은 사진에 Meta AI를 사용하도록 요청시작","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Facebook, 아직 공유하지 않은 사진에 Meta AI를 사용하도록 요청시작

     * Facebook이 Meta AI 기능을 위해 사용자가 아직 공유하지 않은 개인 사진에 접근 요청을 진행함
     * 이 기능은 사용자의 비공개 사진 분석으로 인공지능 성능을 향상시키려는 목적임
     * 사용자의 프라이버시와 데이터 보안 문제가 제기되는 중임
     * 메타는 사용자에게 옵트인 권한을 부여하면서, 해당 기능의 이점도 강조함
     * 업계에서는 향후 개인 정보 활용 방식 전반에 대해 주목하는 분위기임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Facebook의 메타 AI 사진 사용 요청

   Facebook이 Meta AI의 성능 향상을 위해 사용자가 아직 소셜 플랫폼에 업로드하지 않은 비공개 사진에도 접근 및 분석 요청을 진행 중임
     * Meta AI는 이미지 인식 및 생성 알고리듬 성능을 높이기 위해 더 많은 사용자 데이터를 수집하려는 전략임
     * Facebook은 명시적으로 해당 기능 사용 동의를 요청하며, 사용자는 이를 거부(옵트아웃)하거나 허용(옵트인)할 수 있음
     * 이 과정에서 비공개 사진이 AI 학습에 활용될 가능성이 있어, 프라이버시 위험성이 커질 수 있음
     * Meta는 공식적으로 ""AI 발전과 사용자 경험 개선""을 목적으로 들며, 투명한 절차와 선택권을 강조함
     * 하지만 사용자 및 전문가들은 개인 정보 보호와 잠재적 오남용 문제에 대해 우려를 표명하고 있음

프라이버시와 데이터 활용 논란

   이러한 변화는 소셜 미디어 플랫폼 전체에서 개인 정보 활용 방식의 중대한 변화를 의미함
     * Facebook의 비공개 이미지 활용 정책은 글로벌 데이터 보호 규정 및 현지 법규와의 충돌 가능성도 있음
     * 앞으로 업계 전반에서 데이터 수집 투명성, AI 학습 데이터 활용에 대한 논의가 더욱 활발해질 전망임

        Hacker News 의견

     * 시스템이 사진을 태그하고 분류하는 Meta의 방식이 크게 잘못된 방식임을 경험한 사례 공유. 내가 운영하던 팔로워 15만 명 이상의 비즈니스 페이지가 ‘포르노 금지’ 정책 위반으로 차단된 경험. 문제된 사진은 NASA에서 제공하는 토성 사진이었음에도 AI가 ‘살색’이라고 판단해 피부로 오해함. NASA 스톡 이미지를 사용했고, 이 사실을 알렸지만 복구에 몇 주나 걸렸던 답답한 경험 공유
          + 비너스의 누드 사진이면 이해할 수 있지만, 토성 사진으로 이런 일이 벌어진 건 정말 이해할 수 없는 상황이라는 농담 섞인 반응
          + 다행히 사진이 Uranus가 아니어서 그나마 불행 중 다행이라는 농담
          + 이런 터무니없는 사례를 보면 AI 봇으로 사회 서비스와 고객 지원 등을 관리하자는 주장에 깊은 반감 갖게 됨. 현재 수준의 AI로 사람이 불만 제기할 때 필터링을 AI만큼은 절대 맡기면 안 된다는 결론. 이런 시스템을 도입하는 기업은 시장에서 심판받아야 한다는 강한 의견
          + 이런 시스템은 아마도 백엔드에서 블룸 필터 같은 기술을 잘못 쓴 것 같음
     * 예전 페이스북이 멀리 사는 친구나 가족과 재미있게 다시 연락할 수 있던 좋은 시절을 그리움. 이제는 과도하게 복잡해지고 데이터만 수집하면서 이용자 정신 건강까지 위협하는 구조가 된 느낌
          + 페이스북은 처음부터 친구의 모습을 쓴 AI였다는 생각. 이제는 대중이 이 사실을 알아차리기 시작
          + 내가 직접 연대순 피드만 제공하고, 공개 프로필이 없는 새로운 SNS 만들고 있음. 초대 리스트와 레이아웃 디자인 안내. 아주 의도적으로 단순하게 만든 플랫폼임. 참여 원하면 기다림 명단 확인 추천 waitlist 안내
          + 페이스북과 인스타그램이 사람들에게 해로운 건 명백한 사실이라는 견해. 어떤 사람들은 자연스럽게 견딜 수 있겠지만 대부분은 핵심 정신 건강을 지키는 사용법에 대한 지침 필요
          + 나도 비슷한 감정. 페이스북 초창기는 인터넷 속 작은 아늑한 공간 느낌이었고, 멀리 있는 친구들 근황이나 친척의 저녁 식사 사진 보는 일상이 편안했음. 하지만 지금은 열자마자 알고리즘이 조작하는 느낌, 습관적으로 클릭과 시청을 유도해서 멈추기 힘듦. 더 똑똑해졌지만 사람에 대한 관심은 사라진 시스템이 된 느낌
          + 페이스북 초창기부터 주소록을 모두 업로드하라고 집요하게 요구했던 기억. 항상 데이터 굶주렸던 기업. 회색윤리 없었으면 지금처럼 빠르게 성장 불가능했다고 생각
     * 2021년에 페이스북을 삭제했던 기쁨은 말로 형용할 수 없는 수준. 도박과 담배만큼 중독성 있게 사람을 붙들어두는 기업의 대표 사례라는 생각
          + 나는 오랫동안 페이스북 사용하지 않았고 앞으로도 복귀할 생각 없음. 하지만 “그냥 페이스북 그만두면 돼”는 현실적으로 다수에게 전혀 설득력 없는 조언. 지역 커뮤니티의 스포츠, DIY, 부모 모임, 야외 활동 등 오프라인 활동의 유일한 온라인 채널이 페이스북. 인스타그램은 음악가와 소상공인의 주요 채널, WhatsApp은 라틴아메리카와 유럽에서 필수적인 소통 도구. 많은 사람에게 페이스북은 단순한 선택이 아니라 커뮤니티 참여 여부의 문제. 이런 문제는 개개인에게 사회적 비용을 요구하는 식으로 해결할 수 없고, 규제나 더 나은 대체재 개발을 통해 사회 전체적으로 접근해야 한다고 생각. 담배, 도박과 마찬가지로 규제와 대안 마련이 중심이 되어야 함
          + 나는 메신저 때문에 페이스북 계정만 유지. 본 앱은 전혀 사용하지 않음. 계정 삭제를 자랑스럽게 여기는 분위기를 이해할 수 없음. 사용하지 않으면 계정이 없을 때와 똑같은 효과와 감정이라는 이야기
          + 페이스북 삭제의 최적기는 오래전이었지만, 그 다음으로 좋은 시기는 바로 지금이라는 조언
          + 페이스북이 최악이라고 생각. 10년간 앱 자체를 지운 뒤, 모바일 브라우저로 친구 소식만 가끔 확인. 만약 Zuckerberg가 모든 연결을 끊는 방침대로 간다면 연결 복구가 너무 번거로워 실제 계정을 완전히 없애겠다는 계획. 하버드 다음으로 대학 확장 때부터 사용한 초기 유저 경험 공유
          + 몇 년째 meta·twitter·reddit 앱은 사용하지 않음. 하지만 페이스북은 마켓플레이스 기능 덕분에 계정만은 남겨둠. 최근 2번의 아파트 임대 계약을 이곳에서 했는데, 다른 전용 사이트보다 더 좋은 조건으로 찾아 큰 만족. PC용 마켓플레이스만 쓰기 위한 계정 유지가 괜찮은 타협점이라고 생각
     * 그래서 내 가족들에게 내 아이들 사진을 페이스북에 올리지 말라고 부탁한 적 있음. 아이들이 커서 본인 얼굴을 어떻게 쓸지 스스로 결정하게 하기 위함. 아기 때부터 소셜 미디어를 제대로 시작하지도 않았는데 얼굴 데이터로 모델 훈련에 쓰여지는 상황은 너무 가혹한 현실이라는 느낌
          + 일부 문화권에서는 카메라가 영혼을 훔쳐간다는 이유로 사진 찍기를 오랫동안 꺼려함. 이제 와서 그 이유를 이해할 수 있게 됨
          + 어떤 나라, 특히 폴란드 같은 곳에서는 페이스북이 사람들에게 너무 깊이 각인돼 있어서 사진 업로드 거부 자체가 괴짜 취급받는 일임. 유치원, 초등학교, 중고등학교 모든 곳에서 사진을 올리고 싶어하는 분위기
          + 앞으로 스파이나 언더커버 경찰이 되고 싶은지도 모르는 아이들이 이미 얼굴 인식으로 식별돼버린 사회. 앞으로 수 세대 동안 신분노출 없이 첩보 활동할 인재조차 기회조차 얻기 어려운 구조
          + 페이스북이 카메라 롤에서 직접 사진을 끌어오기 때문에 단순히 사진을 안 올린다고 해서 방어가 충분하지 않은 현실
          + 나 역시 가족사진 공유하지 말라고 했지만 시어머니가 요청을 무시했고, 내 부모가 화남. 결국 내가 포기함
     * 아직도 “중국 AI 패권 견제” 논리를 정치인에게 계속 설득 시도. 소비자 착취 조항이 거대 법안에 교묘히 삽입돼 실제로는 군사 목적과 무관하게 규제 피하기 위한 수단. 중국이 온라인 소비자 착취를 지배하도록 내버려 두고 그들의 인터넷 기업만 차단하면 된다는 생각. 만약 미 패권 진심으로 우려한다면 5000억 달러로 핵융합로에 투자하는 게 더 의미 있다는 제안
     * 정말 심각한 현실. Facebook, Instagram이 많은 안드로이드 폰에 기본 설치되고 완전 삭제도 불가능. 동의 요청받아도 실수나 무지로 해로운 선택을 할 수밖에 없는 구조. 이런 업체에 최소한의 윤리 기준조차 강제하지 못한다는 점이 걱정. 모르면 피해를 혼자 감수하는 것 같은 분위기도 이런 플랫폼의 시장 권력을 유지하게 함. 결국 개인정보에 무관심한 사람뿐 아니라 무지하거나 원치 않는 사용자를 모두 끌고 들어가는 구조라는 우려
          + 이런 환경 때문에 진짜로 수 주 동안 언락(부트로더 언락) 가능한 최신 디바이스를 구하기 위해 고생함. 원플러스 신형은 이제 언락이 불가능하고, 중고 시장에도 언락 가능한 척 판매하는 제품이 많아 어려움. 어떤 제조사 앱도 쓰기 싫고, 구글플레이 사용 자체도 내키지 않지만 LineageOS와 MindTheGapps 조합이 차악이라는 판단. 가능하다면 postmarketOS나 Mobian 같은 리눅스 기반 모바일 운영체제도 탐험해보고 싶음
          + ADB(안드로이드 디버그 브리지)로 루팅 없이도 Facebook, Instagram 기본 앱을 완전히 작동 중지할 수 있음. pm disable-user 명령어 활용로 데이터 수집 차단 가능 팁 공유
          + 만약 이런 비즈니스 관행을 가진 폰을 선택해서 구매했다면, 결국 소비자 선택 책임이라는 의견. 좋은 브랜드 중에는 모든 앱을 삭제할 수 있게 하는 곳도 꽤 있다는 이야기
     * 내 인생에서 최고의 결정 몇 가지:

    1. 2012년에 페이스북 삭제
    2. 인스타그램, WhatsApp 계정 만들지 않음
    3. Meta에 지원하지 않음

     * opt-in 여부와 상관없이 Meta가 이런 제안을 할 만큼 뻔뻔해졌다는 점 자체가 불편함. AI 없이도 Meta가 반복적으로 모바일 앱 경계를 벗어나려는 시도(예: Onavo VPN, 로컬호스트 트래킹) 중이라는 설명
          + 이 코멘트는 이전 스레드에 더 어울릴 것 같음
     * 혹시라도 필요하다면 Meta 계정은 언제든 삭제하고 완전히 연결 안 할 수 있다는 점 상기
          + Meta 계정 삭제한다는 건 현실 세계에서의 사회적 연결까지 잃게 되는 일이라는 현실 지적. Meta가 커뮤니티의 소통 허브를 완전히 장악하고 있어서 예시로 수영팀 코치가 WhatsApp만 사용하는 상황. 50명 넘는 구성원이 플랫폼을 바꾸게 설득하는 일이 너무 힘들고 다중 커뮤니티 활동자는 더더욱 변화에 저항. 대부분 다른 곳에서도 WhatsApp 쓰기 때문에, 대안 도입이 과도한 부담이 된다는 실질적 문제 공유
          + 최근 페이스북 마켓플레이스 이용하려고 계정 만들려다 가입 과정에서 비디오 셀피 인증 요구에 충격. 곧바로 포기했고, 이제 마켓플레이스에서 아무와도 소통하지 않을 거라는 자조적 경험
          + 계정을 삭제해도 Meta가 여전히 사용자 데이터를 수집하고 프로파일을 만드니 완전한 차단은 아니라는 사실 공유
     * zuck이 점점 존재감 사라져야 한다는 생각. 최근 몇 년간 특별히 흥미로운 일 한 적 없고, 주기적으로 사적 데이터를 수집해서 뭔가 해보려는 시도만 반복했다는 비판
          + Zuckerberg의 제국이 LLM(대형 언어모델) 분야에서는 많은 기여를 했다고 평가. 로컬 LLM도 이들의 기술이 없으면 불가능했을 수 있다는 의견
"
"https://news.hada.io/topic?id=21700","우크라이나에서 싸웠고 FPV 드론이 별로인 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       우크라이나에서 싸웠고 FPV 드론이 별로인 이유

     * FPV 드론의 사용 경험을 바탕으로 실제 전투 상황에서의 한계를 논의함
     * 조작성, 신뢰성 부족 등 여러 가지 이유로 드론이 기대만큼 효과적이지 못함
     * 기술적 결함과 유지보수의 어려움이 드론 활용에 큰 장벽으로 작용함
     * 전투 현장에서 낮은 내구성과 반복된 실패로 인해 신뢰 문제 발생함
     * FPV 드론의 개선 필요성이 드러나며, 더 현실적인 적용 방법에 대한 논의가 이어짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

FPV 드론의 실제 전투 사용 경험과 평가

  서론

     * 우크라이나 전장에서 실제 FPV 드론을 운용한 경험을 공유함
     * 드론이 이론상으로는 효과적인 장비이지만, 현실 전투 환경에서는 여러 한계가 드러남

  FPV 드론의 단점

     * 실제 작전에서 FPV 드론은 조작성 미흡, 수신 불량, 방해 신호에 취약 등 다양한 문제를 경험함
     * 내구성이 약해 지속적이고 신속한 전투 임무 수행이 어렵다는 문제점이 있음
     * 드론의 기술적 결함과 지속적인 유지보수 필요성으로 운용 부담이 큼

  전투 현장에서의 신뢰 문제

     * 반복되는 고장과 통신 장애로 인한 신뢰 저하가 발생함
     * 중요한 임무에서 장비에 대한 의존성이 떨어지는 결과로 이어짐

  실질적 효과와 한계

     * 드론 활용이 일부 상황에서는 효과적이지만, 전반적인 기여도에 한계가 있음
     * FPV 드론의 개선 및 보완이 이루어져야 보다 실질적인 전투 지원 수단이 될 수 있음

  결론

     * 실전 경험을 통해 FPV 드론 기술의 한계와 현실적 문제를 체감함
     * 장기적으로 더 나은 설계와 안정성을 반영한 드론 개발 필요성이 부각됨

        Hacker News 의견

     * 완벽하지는 않다고 해도, 드론이 우크라이나 전쟁 양상을 완전히 바꾼 상황임을 체감 중임. 한 달 전쯤에는 FPV 드론이 러시아 핵폭격기 상당수를 파괴했다는 보도도 있었고(관련 링크), 전장 사상자의 80% 가까이가 드론 때문이라는 통계도 존재. 전차도 양측 모두 쓸 수 없는 상황, 쓰면 곧바로 드론에 파괴당함. 2024년 5월에만 우크라이나 드론이 러시아 목표물 89,000개 이상을 없앰(관련 기사). 우크라이나는 최근 ‘Birds of Madyar’ 지휘관을 신설된 무인 체계 총괄직에 임명했고, 최전선 전체에 걸쳐 드론 기반의 통합 방어 전선을 구축하기 시작. 대포 못지않은 효과에 대해 의문도 있지만, FPV 드론이 러시아의 최신 대포까지 파괴하는 모습을 보면 그 효과 체감. 대포는 드론처럼 작고 소모적인 표적을 상대로 똑같은 결과를 내기는 어려움(영상 링크)
          + 기사에서도 드론의 80% 전장 영향력이 언급되지만, 실제로는 FPV 드론만을 의미하지는 않음. 많은 FPV 공격이 실제로는 기존 무기로 이미 무력화된 표적에 대한 확증타이기로 진행. 많은 부분에서 드론 활용이 전략적 최선이라서가 아니라 주어진 자원이어서 쓰는 경우가 많다는 점이 강조됨
     * FPV 드론 전투 활용은 일시적으로 크게 효과적이었지만, 대응책이 빠르게 발전하면서 그 영향력도 약화되는 추세라는 견해. 러시아의 전략과 지형 때문에 FPV 드론이 두드러지는 상황임. 2024~25년 현재 러시아가 광활한 평원에서 병력이나 비장갑차가 운용되는 특수환경이었기에 FPV의 강점이 부각. 다만 이동 표적에도 대규모로는 숙련된 포병이 효과적으로 대응 가능. 대부분 국가는 신호가 멀리 닿을 만큼 평평하지 않고, 전장 드론의 진짜 활용도는 여전히 정찰과 포병 연계 화력 보정임. 드론 매니아이자 전선 영상을 많이 본 입장. 다양한 국가나 환경에서는 FPV 자폭 드론보다 적외선 정찰이 훨씬 유효한 선택지
          + FPV 드론의 큰 강점은 아주 저렴하게 보통 수준 기술로 만들 수 있다는 점. 수류탄을 드론에 묶어서 탱크 해치를 강타하는 게 비정밀 포탄 한 발 날릴 때보다도 훨씬 저렴. 또 드론을 적 후방에서 운용해 높은 전략적 가치를 가진 자산을 매우 높은 효과로 타격 가능
          + 최근 드론들은 드론 본체에 광섬유 릴을 탑재, 광섬유로 EW(전자전) 방해나 직선통신 문제 없이 통제가 가능. 예전처럼 시야(line of sight) 제약 없음
          + 실제로 드론이 산림 지대를 강타하는 열화학탄(thermite) 투하 영상 사례도 목격. 대부분 국가는 건물이나 숲까지 통신이 닿는 셀룰러 네트워크 보유. 우크라이나도 최근 적 후방 공격에 러시아 셀룰러망을 활용. 따라서 line of sight가 더 이상 결정적 한계는 아님. 군용 드론 통신망은 훨씬 다양하게 구성 가능(열화학탄 영상)
          + 드론 전쟁이 러시아군에게 피해를 크게 입힌 건 맞지만, 실질적으로 우크라이나가 그만큼 영토를 탈환하거나 러시아군을 밀어낸 실적은 없음. 러시아 역시 대응 노하우를 익혀 자체 드론 생산을 늘리고, 오히려 드론 기만책으로 2차 대전풍 오토바이 전술을 활용 중(관련 기사). 양측 모두 이제 드론 전쟁이 일상적 전술로 편입된 상태
          + 21세기 전쟁의 변별점은 국가·충돌 당사자·지형에 따라 완전히 다른 무기체계가 도출되는 점이라고 생각. 나라 하나, 전장 하나 건너에서도 무기가 전혀 달라지는 특이한 세상
     * FPV 드론의 여러 문제점은 민간용 하비(취미) 기술을 즉흥적으로 쓴 결과와, 전장 운용법이 아직 초기 단계이기 때문이라는 의견. 현세대 FPV가 다소 미흡하더라도, 앞으로 군사 특화 ‘차세대’ 드론 개발은 전혀 다른 차원의 효용을 낼 수 있다고 확신. FPV라는 명칭 자체도 당장 오늘날 특정 세대군에 한정될 뿐, 내년에는 재사용성이나 호버링 능력을 추가한 ‘로이터링’ 무기, 혹은 재사용이 가능한 드론으로 크게 진화할 것이라 봄
          + 서방 군대에는 이미 Switchblade 같이 실전 자율성이 높지만 가격이 수천~수만 달러로 훨씬 비싼 모델이 존재. 싸고 평범한 소비자용 드론을 쓰는 건 기술력 한계보다는 비용 문제. 고성능–고가 소수정예무기 vs 저렴–대량생산 체계의 논쟁은 산업화 이후 늘 이어온 주제. 미국 군사교리는 B2 버커버스터처럼 적 HQ를 한 번에 날려버리는 선제타격이지만, 우크라이나에겐 그런 선택 기자체 없음. 전통적 항공우세도 이뤄지지 않는 교착 상태임
          + 민간용 드론이라는 자체의 급조 이미지는 오해라고 생각. 실제로는 프로 드론에 들어가는 수준의 오토파일럿 탑재. 궁극적으로는 비용 효율성이 핵심. 단방향 드론이 1만 달러라면 야간 운용이 가능해도 너무 비싸기 때문. 기상 악조건 등 완전한 한계도 존재. ‘재사용 드론’이 다음 단계라고 하지만, 진짜 왕복이 가능하다면 아예 다른 무기종이 그 역할을 더 잘했을 것
          + 언급된 드론들은 GPS, 자이로 등 기본 기능조차 없는 경우도 많아서, 저렴하게 소모성 무기를 만드는 신중한 트레이드오프 측면이 큼
          + 기술적 한계는 드론 자체가 아닌, 중계용 모선(mothership) 도입으로 쉽게 극복 가능
     * 필자가 FPV 드론의 범주를 지나치게 좁게 잡고 있다는 생각. ‘재사용 드론’이나 ‘분야별 발전상’을 완전히 다른 세계로 치부하는 시각이 오히려 경직되어 보임. 기술적 한계라고 지적한 부분 중 다수는 설계와 생산비 이슈에서 기인. 성공률 43%는 굉장히 높은 수치. 실제 전장에서 박격포나 보병 화기 성공률이 이만큼 높기도 힘듦. FPV 드론이 최첨단 같지만, 실은 10년 전부터 보급된 기술들로, 그 ‘싸고 흔함’이야말로 의도된 전략 요소임
          + 실제로 중요한 건 명중률이 아니라 ‘비용 대비 피해량’이라는 본질적 지적. 똑같은 돈으로 다른 무기체계가 더 나은 성과를 낼 수 있는지가 관건
     * WW1 초기의 비행기 파일럿이 신뢰성 떨어지는 복엽기를 어렵게 운용하듯, 현재 FPV 드론이 쓰이지만, 결국은 우크라이나–이스라엘–러시아에서 보여준 성공 이후 무기산업의 진보가 무시무시하게 이루어질 시기임
          + 예시로, 과거 척 예거가 최초로 제트기를 조종했을 때 연료 분사를 손으로 미세조정하느라 운용에만 몰두했지만, 10년 뒤 자동화로 두 배 이상 빠르고 높이 날 수 있게 된 사례를 언급
          + 다음 단계가 AI 드론 군집(swarm)이라면, 수천 대를 한 번에 자율 운용해 표적을 자동으로 선정하는 세상이 정말 현실이 될 가능성. “똑똑하게” 목표를 고르는 일까지 이미 기술적으로 시도가 가능한 상황
     * 지금까지의 드론 효율지표(대략 12% 성공률 추산)와 달리, 드론은 본질적으로 ‘대량 투입’에 최적화된 무기임. 우크라이나는 2024년에만 전선 공격임무 200만 차례 이상, 올해 목표는 500만. 1:9 성공률이라도 그 피해 총량은 기존 포격이나 폭격이 감히 따라올 수 없는 수준
          + 저렴함은 장점이지만 한 번 써버리는 드론이 500달러, 포탄은 100달러에 훨씬 큰 타격을 낼 수 있다면, 결국 효율 문제. 드론이 표적을 식별하면 남은 탄을 몰아넣고 드론을 다음 임무로 보내는 식 활용성이 부각
     * ‘머스킷도 멋지지만 재장전이 느리다’ 같은 느낌이라는 평가. 우크라이나가 최고의 기술을 가진 건 아니지만, 절박함에서 만든 전술이 미래 전장의 판도를 바꿀 것이라 기대
          + 실질적으로는 70년대 미사일(TOW)부터 원리는 동일. 드론을 활용한 전략 역시 자원 부족에서 온 임시방편이며, 자금과 시간이 많은 국가라면 더 신뢰성과 효율성을 갖춘 대안들이 나올 전망. 러시아 또한 동원 가능한 자원을 총동원 중이고, 포탄 생산력도 크게 늘림
     * 실전 경험자 피드백이 흥미진진. 드론의 여러 문제에도, Anduril 같은 스타트업이 각광받을 정도로 ‘드론이 전장 판도 변화’의 중심. 향후엔 자율 드론 전쟁이 올 것이란 전망이 대세지만, 개인적으로는 도덕적 동의는 하지 못함. 많은 이들이 희생된 전쟁 현실이 안타까움
          + 드론 교란 탐지에 자율 표적식별 기능도 일부 도입된 것으로 보임. 러시아 기지를 자동으로 추적 타격하는 형태의 시제품도 옛부터 존재. 현대 드론 역시 교란 상황에서 EM(전자기) 소스 타격 기능 추가가 필요함
     * 최근 오히려 특이했던 사례는 FPV 드론을 사용한 공군기지·컨테이너 공격의 성공. 전통 포격이면 탐지될 위험이 훨씬 크고, 드론은 유연한 원거리 운용이 가능. 미래적으로 저가 GPU로 소형 AI 자율모델 탑재, 일종의 “대량 자율 드론” 전술 도입이 필연적으로 등장 가능. 이런 기술이 테러에 악용될 사례도 상상만으로도 두려움
          + 사실 해당 사례는 6000km 운용이 불가능한 FPV 드론이 아니라, 준자율적으로 턴-바이-턴으로 명령한 새로운 타입의 드론 활용 추정. 최근 시범운영을 공식 발표하기도 했음
     * 필자가 작성한 통계나 전투 경험담이 적군에게 의미 있는 정보인지에 대한 의문. 정보 유출에 대한 Loose lips sink ships 분위기
          + 그러나 제공된 내용은 이미 취미 FPV 조종사들이 알 만한 일반적 수준이며, 주요 혁신기술 업데이트는 해당 저자가 이미 전장 최전선에서 물러난 뒤의 상황. 정보 가치가 크지 않다는 의견
          + 러시아도 자체 드론 프로그램이나 정보수집을 통해 이와 유사한 실전 데이터를 보유할 가능성 높음
"
"https://news.hada.io/topic?id=21784","Show GN: '우주멍'을 때릴 수 있는 웹시계, '밤의 궤도'를 소개합니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Show GN: '우주멍'을 때릴 수 있는 웹시계, '밤의 궤도'를 소개합니다.

   안녕하세요!

   오늘 아침부터 AI와 함께 '바이브 코딩'으로 만든 사이드 프로젝트, 아름다운 우주 배경화면 시계 '밤의 궤도(Orbit of Night)'를 소개합니다.
   만들던 웹 기능에 한 페이지였는데, 그냥 한 페이지로만 놔두는게 어쩐지 조금 아깝다는 생각이 들어 만들기 시작했고, 보면서 잠시 '멍' 때리고 싶어지는 경험을 목표로 만들었습니다.

   주요 특징
     * 살아있는 배경화면: 별, 성운, 유성이 계속해서 움직이는 배경
     * 실시간 ISS 추적: 실제 ISS 위치 데이터를 기반으로 보정되는, 화면 안에서 안정적으로 움직이는 사인 곡선 궤도
     * 인터랙티브 요소: 행성을 클릭해 가상으로 스캔하거나, UFO, 블랙홀 효과 등 소소한 재미 추가
     * 사용자 설정: 취향에 맞게 행성, UFO, ISS 등의 요소를 켜고 끌 수 있으며, 3가지 테마를 제공
       별도 프레임워크 없이 순수 JavaScript와 CSS 애니메이션으로 구현했으며(AI가 구현했지만, 검수하며 깐깐하단 소리를 AI에게 들었습니다?), 특히 ISS 궤도는 안정적인 시뮬레이션과 실시간 데이터 보정을 결합하는 하이브리드 방식으로 만들어 봤습니다.

   TMI: 40대 아재가 AI를 만나 개발한 이야기

   사실 저는 어느덧 40대가 된 아들딸 아빠입니다.
   제 소개를 간단히 하자면 연식도 어느정도 있다보니 과거 텍스트와 머드 기반의 게임들부터 접했던 세대고 또한 웹게임에도 많은 애정이 있는 게이머 이기도 합니다.
   컴공을 나와 하는일이 IT쪽이긴 했지만, 서버 엔지니어 쪽에 가까웠고, 그러다보니 미들웨어, 백엔드, 프론트엔드도 어느정도 만질줄 알게된 평범한 직장인이죠.
   다만 예전부터 게임을 만들고 싶은 꿈이 항상 있었는데, 발전한 AI의 성능을 체험해보고서 '아 이제 때가 왔다!' 라는 생각과 함께 작년 10월부터 여가시간마다 AI로 요즘 말하는 바이브 코딩을 주로 하고 있습니다.

   현재는 세번째 웹게임 토이 프로젝트를 하고 있는데, 중간에 한 기능 페이지를 만들면서 즉흥적인 삘을 받고 힘을 준 화면이 있었는데, 문득 생각해보니 거기에만 쓰기 좀 아깝단 생각이 들더군요. 그래서 이 프로젝트가 탄생하게 되었습니다.

   한번 방문해서 '우주멍' 때려보시는 건 어떠신가요?
   피드백은 언제나 환영합니다!

   조금 더 정적이고 리얼한 느낌의 우주가 나오면 좋을 것 같습니다~

   다양한 우주를 색상 테마 외에도 선택할 수 있도록 확장해보겠습니다. 소중한 의견 감사드립니다!

   의미있는 프로젝트를 소개해주셔서 감사합니다. 시간이 지나서도 꿈을 잃지 않고 도전하시는 모습이 멋지고 존경스럽습니다. 자녀분들도 좋아할 것 같아요! 몇 가지 피드백을 드리자면, 1) 유성우가 너무 빠르고 많이 떨어져서 멍 때리기에는 너무 신난달까요? 조금 더 희미하고 적게, 천천히 떨어지면 잔잔하고 감성적인 분위기를 연출할 수 있을 것 같아요. 2 )행성 스캔에서, 같은 행성에 대해서는 같은 정보만 나오게 하면 어떨까요? 3) 설정의 블랙홀 이펙트는 클릭 효과를 의미하는 것인가요? Off해도 해당 효과가 꺼지지 않아요. 4) 설정의 Random Color가 어떤 색이 바뀌는 것인지 잘 모르겠어요.

   피드백 감사합니다. 유성우는 슬라이드를 통해 빈도를 조절하는 옵션을 고려 중입니다. 행성도 클릭마다 다른 정보가 나오는 부분도 수정사항입니다ㅠㅠ 크흑. Random Color 는 나타나는 행서의 색이 매번 랜덤하게 나오도록 하는 옵션입니다. 행성 자체가 천천히 움직이니 확인하시려면 그 위 Planet옵션을 껐다 켜보시면 다른 색 행성이 나오는걸 확인하실 수 있습니다 ㅎㅎ. 블랙홀은 원래 주변 별들과 우주선이 클릭했을때 끌려들어가는 이펙트를 주고 싶었는데,, 그건 브라우저와 javascript로만 처리하긴 만만찮은 부분이어서 별도 파티클만 끌려가는 느낌까지만 구현했습니다 ㅠ. off로 안꺼지는건 역시 수정 예정입니다. 소중한 의견 감사드립니다!

   멋진 프로젝트 입니다

   관심 감사합니다!

   멋지네요!

   따뜻한 관심 감사합니다!

   브라우저 배경화면으로 너무 좋은데요! 아이폰 배경화면에 우주를 넣어둔 느낌이랑 비슷해요.

   따뜻한 관심 감사드립니다! 저는 집에 안쓰는 좀 구형 태블릿에 띄워놓고 프레임 간단하게 해서 시계용으로 써볼려구요 ㅎ!

   좋습니다~
   이걸 참조해서 행성도 업그레이드하면 더 좋을것 같아요
   https://codepen.io/jamesfinn180/pen/VwzENbR

   우와! 세상엔 엄청난 분들이 진짜 많으시네요. 관심 감사드립니다. 지구에서 바라보는 느낌이라 지구가 그대로 나오는건 아니겠지만 응용해서 업그레이드 도전가겠습니다!

   정말 힐링이고 행복했어요
   좋은 작품 만들어주셔서 감사합니다. :)

   따듯한 답변 감사합니다ㅠㅠ 더 힘이나네요.

   말씀해주신 의견들을 수렴해서 수정해두었습니다! 1. 행성은 생성 될 때 고정 된 정보를 가집니다. 2. 유성우 옵션을 개인의 기호에 맞추어 설정에서 변경 가능합니다. (발생빈도, 속도, 갯수) 3. 성운과 티가 잘 안나지만 은하수, 그리고 별빛이 조정되었습니다. 4. 블랙홀 옵션을 끄면 이젠 동작하지 않습니다. 5. 스x워x 의 워프와 비슷한 효과를 오마쥬 해서 추가했습니다. 6. 행성 위에 가끔씩 특이한 친구들이 나타납니다. 모두 많은 관심 보내 주셔서 감사합니다!

   사소한 것이긴 한데, 블랙홀 효과를 꺼도 그대로 블랙홀 효과가 작동합니다. ^^ 멋진 프로젝트네요~!

   넵 감사합니다^^ 해당 버그들과 다른 분들 의견을 반영해서 개선한 버전 곧 적용할 예정입니다. 관심 감사드립니다!

   예전에 어떤 뽀모도로 사이트에서 봤던 디자인이랑 비슷하네요. 여러가지 행성이 나오게 하면 어떨까요?

   앗 역시 이하세상 하늘아래 새로운 것은 없다더니...! 행성은 1개만 나오고 그 위에 유니크 요소가 출현하는걸로 재미를 더할까 하는데, 여러 행성이 나오는 것도 테스트 해볼게요. 감사합니다.

   저도 멍때리기 용이라길래 실제 우주 장노출 샷 같은 걸 기대했는데.. 기대한 바와는 많이 달랐던 것 같아요

   소중한 의견 감사합니다. 해당 부분도 AI와 고민해보겠습니다! ㅎㅎ 다만 표현은 모두 css나 svg를 통해서 최대한 표현하려고 컨셉을 잡고 있어서, 관련 사진들 보면서 영감을 받아보겠습니답!
"
"https://news.hada.io/topic?id=21740","Microsoft에 의존성을 가지는 건 위험해요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Microsoft에 의존성을 가지는 건 위험해요

     * 최근 ICC(국제형사재판소) 제재 대상자 메일박스 차단 사건에서 보듯, Microsoft 제품에 대한 과도한 의존은 예상치 못한 서비스 중단·비용 위험을 내포함
     * 미국 정치적 요인(특히 제재나 돌발적 정책)과 맞물릴 때, 서비스 차단 확률은 낮으나, 한 번 발생하면 피해액이 매우 큼
     * Microsoft와 같은 대형 IT 기업은 클라우드·SaaS 시대에 실질적으로 고객 기업 데이터와 소프트웨어에 대한 완전한 통제권을 가짐
     * 기업·기관 IT 인프라(메일, 신원관리, 파일, 인증 등)가 MS 서비스에 집중될수록 서비스 중단 시 업무·비즈니스 전체가 마비될 수 있음
     * 리스크 예방 및 대체 인프라 구축을 위한 합리적 투자 한도는 현실적으로 매우 작음. 리스크 관리가 비직관적이며, 근본적으로는 데이터와 비용 추정의 한계가 존재함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

최근 사건: MS의 ICC(국제형사재판소) 메일박스 차단 논란

     * 2025년 미국이 ICC 고위 인사에게 제재를 가하자, Microsoft가 해당 인물의 업무용 메일 계정 접근을 차단한 것으로 복수 매체에서 보도함
          + Associated Press, NL Times 등은 ""트럼프 행정부 제재 → MS가 ICC 검사장 등 일부 계정 차단""이라고 설명함
          + Politico는 ""조직 전체가 아닌 개별 인물 차단""임을 강조했으나, 특정 인물의 차단 사실 자체는 부정하지 않음
     * 이 사건은 네덜란드 등 유럽에서 미국 IT기업 의존이 국가·공공 IT 인프라의 보안 위험이 될 수 있다는 논란을 촉발함
     * MS는 “어떤 경로로 계정이 차단되었는지, 정확한 디테일은 공개하지 않음”이라고 밝혀 서비스 차단 프로세스·책임 범위의 불명확성을 드러냄

이런 일은 또 일어날 수 있는가?

     * 서비스 차단 절차는 매우 단순: 미국 대통령(정부)의 제재 명령 → MS 등 미국 IT기업의 서비스 차단
          + 합법성 논란과 무관하게, 일단 차단이 이뤄지면 그 즉시 업무 피해가 현실화됨
     * 트럼프 등 정치적 의사결정은 예측이 어렵고, 어느 기업이나 한 번의 발언·이슈로 표적이 될 수 있음
          + 미국 대통령 권한이 광범위하며, 불확실성이 정책 리스크를 증폭시킴
     * 실제로 자주 발생하지는 않으나, MS 제품에 심각히 의존하는 기업·기관에는 ""블랙스완"" 리스크로 상존
          + 수백만 MS 고객 중 1년에 1~2건 발생 확률로 계산해도, 한번 걸리면 막대한 피해

Microsoft의 서비스 차단 능력

     * 클라우드·SaaS 도입 이후, Microsoft는 고객의 소프트웨어·데이터에 대한 실질적 제어력을 보유
          + 과거(1990~2000년대)에는 자체 메일 서버, 오프라인 인증 등 외부 차단이 힘들었음
          + 현재는 모든 서비스(Exchange, Azure, MS 365, Office 등)가 중앙 집중 방식으로 운영됨
     * 예시: Python in Excel의 경우 모든 파이썬 코드가 로컬이 아닌 Azure 컨테이너에서 실행됨
          + 중앙 제어로 계정 차단, 데이터 접근 차단, 서비스 전체 중단이 가능
     * 서비스 차단이 반드시 부정적이지만은 않으며, 법적 요청이나 공공안전 등에는 긍정적으로 작동할 수 있음
     * 전 세계 200만 개 이상의 기업이 MS 365 제품을 사용하고 있으며, MS가 원하면 즉각적 서비스 통제가 가능함

기업의 Microsoft 의존 구조와 실제 피해 규모

     * 현대 기업 IT 인프라는 메일, 협업, 문서, 인증, 백업 등 핵심 기능을 거의 전방위적으로 MS 제품에 의존
          + MS Exchange, Teams, Sharepoint, Office, Active Directory, OneDrive, Windows 등
     * 특히 이메일, 문서, 신원관리 등은 실시간 업무 유지에 필수적임
     * 실제 장애 사례
          + 2024년 Crowdstrike 장애로 Fortune 500 기업당 평균 4,400만 달러(약 600억 원) 피해 발생
          + 소기업도 1분당 수천~수만 달러, 대기업은 서버당 1분 16,700달러까지 손실 가능(가트너 등 참고)
     * 단기간 서비스 중단만으로도 업무 마비, 전환·복구 비용, 평판 손실 등 막대한 2차 피해 발생
          + 2주 내 새로운 IT 스택을 구축한다고 가정해도, 현실적으로 불가능에 가까움

리스크 완화(Prevention)에 합리적으로 투자할 수 있는 금액

     * ROSI(보안 투자 수익률) 공식에 따르면, 단일 사고 발생 확률이 워낙 낮아 기업이 감당할 수 있는 예방 예산이 극히 적음
          + 단일 사고 발생 비용(예: 3,400만 달러) × 연간 발생 확률(1/200만) = 연평균 손실 기대액 17달러
          + 완벽한 리스크 회피 솔루션을 구축해도, 합리적으로 투자 가능한 비용이 적음
          + 대기업(예: Walmart)은 연간 수억 달러를 MS 서비스에 지출하지만, 자체 클라우드·IT 구축 비용, 사용자 재교육 등 전환 비용이 훨씬 큼
          + 서비스·라이선스 비용 감축 효과까지 감안해도, MS를 완전히 대체할 수 있는 현실적 투자 여력은 제한적

리스크 관리의 근본적 한계와 복잡성

     * 실제 리스크 관리에서 단일 사고의 파괴력이 높아도, 발생 확률이 매우 낮으면 대체 투자가 비합리적으로 여겨질 수 있음
     * 보안 ROI, 사고 확률, 피해액 등 모든 변수에 불확실성이 크고, 신뢰할 만한 데이터 자체가 부족
          + 수많은 가정과 불확실성 하에서, 결정이 극단적으로 보수적이거나 감정적으로 흐르기 쉬움
     * “데이터 기반 의사결정” 자체가 힘들어, 많은 경우 비직관적이고 논란이 많은 관리·투자 결정으로 이어짐
          + 중소기업에는 한 번의 사고가 ‘회사 폐업’에 이를 수 있으나, 합리적으로는 감내하는 것이 맞는 결과로 산출됨
     * 국가·공공기관 등은 비용이 아닌 주권, 통제권, 데이터 독립성 등 비재무적 요인을 우선해 MS 탈피를 추진(덴마크 등)

결론 및 시사점

     * MS 서비스에 대한 지나친 종속은 실제로 극히 드문 리스크지만, 발생 시 조직 존폐까지 위협
     * 현실적으로 예방·대체 인프라 구축에 투입할 수 있는 자원·예산은 매우 한정적
     * 그러나 이윤이 아닌 가치를 중시하는 조직(예: 덴마크 정부)이 실제로 독립적인 대안을 고민 중인 것도 사실임
     * 기업 및 기관은 각자 상황에 맞춰 현실적인 위험 평가와 장단기 전략을 구상해야 함
     * IT 정책·경영 전략상 실질적 대응책은
          + 비상시 대체 IT 환경 매뉴얼 수립
          + 핵심 데이터 백업 및 클라우드/메일 서비스 다변화
          + 필수 서비스 이중화, MS 탈피 시나리오 모의훈련 등
     * 기업·기관 규모, 업종, 규제 요건에 따라 주권·비용·업무연속성 등 다양한 요소를 복합적으로 고려해 현실적인 리스크 관리 전략 수립 필요

        Hacker News 의견

     * Microsoft를 사용할 때는 좋은 부분과 나쁜 부분을 신중히 구분해야 한다고 생각함
       Microsoft 전체를 금지하는 건 솔루션 선택지를 매우 제한하는 일임
       이 회사는 방대하고 내부적으로 매우 다양한 문화를 가짐
       .NET, MSSQL, Visual Studio 같은 제품들은 그만한 대체제가 거의 없으며, 특히 Visual Studio의 디버거 경험은 현실에서 복잡한 문제를 해결할 때 거의 필수적인 도구임
       최상급 게임 엔진이 Visual Studio에 크게 의존하는 이유이기도 함
       다만 Azure와 Windows는 Microsoft에서 문제가 시작되는 영역임
          + AAA 게임 엔진이 Visual Studio를 쓰는 이유는 게임 고객 대부분이 Windows 플랫폼에 있기 때문임
            만약 95% 게이머가 MacOS를 썼으면 게임 개발자들의 기술 스택도 전혀 달랐을 것임
          + 좋은 점과 나쁜 점을 어떻게 구분할 수 있는지 의문임
            Microsoft는 일관적으로 좋지 않은 제품을 만들고, 때로는 괜찮던 것까지 나쁘게 변질시키는 경향이 있음
            그래서 현재 괜찮다고 생각하는 제품들도 오래 좋을 거라 기대하지 않음
          + AAA 게임 엔진에만 Visual Studio가 필요하다면, 그 근거 하나로는 MS를 칭찬하는 것치곤 너무 미약한 칭찬임
            대부분의 사람들이 AAA 게임 엔진을 작성하지 않음
          + Microsoft의 모든 문제에도 불구하고, Windows용 공개 API 문서는 최고라고 봄
            Linux나 BSD의 manpage보다 평균적으로 낫고, Apple의 일부 적대적인 문서보단 훨씬 낫다고 생각함
            단, 버그 리포트 제출은 내부 네트워크를 잘 알거나, 어디서 물어봐야 할지 아는 게 필요함
          + 디버거를 제외하면 Visual Studio는 내가 쓴 IDE 중 최악임
            자바스크립트 코드 에디팅 시 자동 들여쓰기 기능은 사실상 난수 발생기처럼 엉망임
            프로젝트 실행 중에는 새 파일 추가도 안 되고, 컨텍스트 메뉴로도 생성 불가임
            외부에서 파일이 바뀌면 재시작하라고 추천만 해줌
            이 외에도 수없이 작은 문제들이 있는데 특히 들여쓰기 자동 포맷이 고통임
     * 많은 개인과 기업이 Microsoft의 클라우드 서비스를 이용한다는 사실이 놀랍게 느껴짐
       이는 통제할 수 없는 싱글 포인트 오브 페일러(단일 실패점) 임
       Google이나 Youtube에도 똑같이 해당되고, 마치 엔진 하나짜리 여객기를 모는 수준의 위험으로 보임
       대체 이런 위험을 감수하는 심리가 의문임
          + 대부분의 기업은 Microsoft와 정식 계약을 체결함
            차고에서 창업한 2인 스타트업보다는 훨씬 안전함
            계약서에는 서비스 수준, 책임, 기대치 등 엄격한 조건이 보장됨
            식당이 채소를 친구의 취미밭이 아닌 대형 농장에서 조달하는 것과 유사함
          + AWS 사용도 동일한 단일 실패점이라고 볼 수 있는지 질문함
            현실적으로 여러 장애 지점을 구축하려면 어느 정도 규모 이상이 필요하고, 이 문제에 신경 쓰기보다는 사업 자체에 집중하는 편이 현실적임
            실제로 이를 걱정하는 리더십이 있었지만, 다중화 대비 드는 비용이 항상 사업에 다른 투자하는 것보다 덜 효율적이었음
          + Microsoft 클라우드 서비스는 저렴하고 잘 작동함
            필요한 모든 것과 쉽게 통합됨
          + 이게 문명 사회의 본질 아닌지 질문함
            대부분의 사람들은 월급에 의존하고, 음식 생산 능력이나 응급 상황 대비 능력도 제한적임
            이런 의존성은 점점 더 강화되고 있음
          + 비용과 가치의 기회 비용 관점에서 보면, 대체제를 직접 만드는 건 비현실적임
            덜 인기 있는 서비스로 옮길 경우 생길 골치 아픈 문제들을 고려하면, 단일 실패점을 알면서도 결국 기존 서비스에 의존하는 게 합리적인 선택임
     * 최근 Microsoft가 제재 대상 인물의 메일박스를 차단한 사건이 있었음
       MS 제품에 크게 의존하는 조직이라면 '나에게도 벌어질 수 있는 일인가?', '이런 상황을 예방하려면 얼마를 투자해야 하는가?'를 진지하게 고민해야 함
       이 글에서는 사실관계를 파악하고 보안 투자 대비 효과(ROI) 관점에서 현실적으로 접근해보았음
     * 이런 위험은 미국에 본사를 둔 모든 기업에 적용됨
       Microsoft만이 아니라 Google, Amazon, Apple도 모두 미국 정부의 요구를 거부할 수 없을 것임
          + 특정하게 Microsoft가 겨냥된 이유는 바로 그 메일박스 차단 사건 때문임
     * 이 글에서 말하는 위험은 Microsoft만의 위험은 아님
       대체 불가능한 서비스에 외주를 주는 것 자체의 문제임
       기술을 유연하게 만들면 해당 위험이 사라짐
          + EU 입장에서는 외국 정부에 의해 좌지우지될 수 있는 독점적 공급자를 위해 합리적 대응이 필요함
            MS가 직접적으로 EU 통제를 받는 독립 법인 설립을 ultimatum으로 제시하거나, 법적으로 이런 구조를 강제해야 한다고 생각함
            최종적으로 미국 Microsoft와 단절하더라도 EU MS는 자체적으로 운영될 수 있어야 하며, 만약 이런 구조가 없다면 미국이 통제하는 Microsoft는 유럽의 중대한 보안리스크임
          + 기술 유연성만으로 위험이 사라지는 건 정치적 리스크를 간과한 것임
            정치적 환경이 바뀌는 순간 그 위험에 대비하지 못하면 기술만으로는 대응 불가임
     * 대부분의 기업은 Microsoft에서 벗어나는 비용과 난이도가 얻는 이점보다 큼
          + 맞을 수 있음
            일부 Microsoft 제품은 깊이 들어가지만, 실제로는 대부분 기업이 다양한 제품군을 폭넓게 쓰지 않음
            대부분을 붙잡아 두는 건 인증 서비스(Azure AD 등) 임
            인증은 오히려 관리가 더 쉬울 수도 있음
          + 미국 기업은 거의 모두 법원 명령을 따르므로, 이 제재를 피하려면 미국 외 기업으로 옮겨야 해결 가능함
          + 결론적으로, 가장 큰 변수는 추가 제재 발생 여부임
            Microsoft가 앞으로 더 많은 제재를 집행하게 되면 위험이 참을 수 없게 커짐
     * 세계화 시대에는 기업들이 조약이나 국제 규범을 따르면 되었으나, 글로벌화가 후퇴하면서 이제는 각국의 법률을 자국과 해외 모든 사업장에 적용해야 하는 상황임
       이런 리스크는 앞으로 항상 감안해야 함
          + 기업은 자국 법률에서 결코 면제된 적 없음
          + 실제로는 준수해야 하는 법률이 회사가 영업하는 국가와 사용자가 있는 국가의 이상한 합집합을 이룸
            법이 잘 겹치는 경우엔 명확하지만, 그렇지 않으면 불확실하고 어려움
            예를 들어 프라이버시, 쿠키 관련 법률에서 볼 수 있듯 매우 복잡함
     * 최근 기계식 시계, 만년필, 스팀펑크 등에 대한 관심이 부활하는 트렌드는, 기술이 인간 이해를 초월한다는 사회문제에 대한 반응이라고 느껴짐
       네트워크 스택, 프로토콜, 물리적 원리 등에 대해 보통 사람은 설명하기 어려움
       자체 이메일 서버를 운영하는 소수의 테크 비전러리도 있지만, 이메일처럼 개인이 혼자 운영하기엔 환경이 매우 가혹함
       스팸 때문만이 아니라, 대형 서비스에 신뢰를 얻는 게 거의 불가능에 가까움
       Excel 없이 혹은 Google Sheets 없이 살 수 있지만, 스프레드시트 자체 없이 일한다는 건 매우 어렵다고 생각함
     * 효과적인 비즈니스 연속성 관리는 각종 위험을 보험으로 보완함
       사업이 한순간 붕괴할 가능성도 통계학적으로 받아들이고, 보험으로 리스크를 완화한다면 게임 오버가 아님
          + 진지한 질문으로, 정치적 이유로 핵심 서비스 공급자가 사라질 경우 실제로 보장되는 보험 상품이 있는지 궁금함
     * 기업 입장에서 Microsoft를 버리는 게 합리적인 선택이 되지 못함
       Active Directory, Teams, Outlook/Exchange 등 실제 대체할 수 있는 현실적 선택지가 없음
          + 기대값 논리(확률적 손해가 적으니 괜찮다)는 적합하지 않음
            보험은 항상 부정적 기대값임에도 기업은 리스크 때문에 보험에 가입함
            또한, 통제 단절 위험이 2백만분의 1처럼 낮다고 보기 어려움
            예를 들어 트럼프가 어떤 국가 전체를 제재 리스트에 추가하면 그 나라 기업 다 잘릴 수 있음
"
"https://news.hada.io/topic?id=21809","태양계를 방문한 세 번째 성간 천체 3I/ATLAS, 천문학자들이 발견","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                태양계를 방문한 세 번째 성간 천체 3I/ATLAS, 천문학자들이 발견

     * 천문학자들이 3I/ATLAS라는 새로운 성간 천체(Interstellar Object) 를 발견함
     * 3I/ATLAS는 태양계를 통과한 세 번째 성간 방문자로 중요함
     * 이 천체는 이전 사례인 'Oumuamua'와 2I/Borisov에 이어 발견된 것임
     * 3I/ATLAS의 궤도와 속도 분석을 통해 태양계 외부에서 온 기원 확인 가능함
     * 이번 발견은 성간 물질 연구와 태양계 형성 과정에 대한 이해 증진에 기여함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

3I/ATLAS: 세 번째로 확인된 성간 천체의 발견

   천문학자들이 최근 3I/ATLAS라는 이름의 천체를 발견했음. 이 천체는 태양계를 통과하는 세 번째 성간 천체로, 그 이전에 발견된 'Oumuamua'와 2I/Borisov에 이어 새로운 발견임

  성간 천체의 의미와 중요성

     * 성간 천체는 태양계 외부에서 온 천체임
     * 3I/ATLAS의 발견은 성간 물질의 구성과 이동 경로에 대한 새로운 연구 기회 제공함
     * 이러한 천체들은 태양계의 형성 및 진화 과정에 대한 단서 역할을 할 수 있음

  발견 과정과 궤도 분석

     * 천문학자들은 3I/ATLAS의 속도와 비정상적인 궤도 특성을 관측함
     * 궤도 분석 결과, 이 천체는 태양계 안쪽에서 기원하지 않은 것으로 파악됨
     * 이런 분석을 통해 성간 천체임을 공식적으로 확인함

  향후 연구 및 기대 효과

     * 3I/ATLAS의 통과는 성간 천체에 대한 관측 기회로 작용함
     * 연구자들은 이 천체의 조성과 기원에 대해 추가 연구를 계획 중임
     * 앞으로도 더 많은 성간 천체 발견 가능성이 높아짐

   이런 발견은 성간 공간을 이동하는 물체가 태양계 연원 연구 및 우주 과학 발전에 기여함

        Hacker News 의견

     * 이번에 발견된 천체 궤도 이심률이 6 이상이라는 점이 매우 인상적인 정보 공유
       참고로 ‘Oumuamua(1I)는 1.2, Borisov(2I)는 3.3의 이심률을 가진 전례가 있음
       아직까지 단지 하늘의 하나의 점으로 보일 뿐 활성(혜성처럼 먼지가 분사되는 상태) 여부는 확인이 어려운 단계
       만약 활성 상태가 아니고 소행성처럼 보인다면, 현재 관측된 크기는 표면 반사율(알베도)에 따라 8~22km로 추정
       대부분 암흑물질에 가까울 것으로 보여서 실제로는 추정 범위 중 큰 값에 더 가깝다는 생각
       만약 활성화된 상태라면, 분출되는 먼지로 인해 실제보다 훨씬 커 보일 수 있음
       태양에 가까워지면서 온도가 오르면 점점 활성화될 수 있을 것으로 기대
       특별히 어떤 행성에 아주 가까운 경로를 지나가진 않음
       오는 할로윈 직전 태양에서 1.35 AU 거리까지 접근, 속도는 68 km/s(참고로 지구 공전속도는 29~30km/s)
       궤도는 행성들의 움직임과 반대인 역행 궤도인데, 이는 우연에 가까운 결과
       궤도 뷰어 링크
       앞으로 몇 주간 동료들에게 매우 흥미진진한 시간
       참고로 나는 궤도 역학 박사 과정 중이며, NASA 임무들에 사용된 소행성 시뮬레이션 코드를 작성한 경험
       관련 코드 링크
          + 최근 가장 가까운 접근은 2025년 10월 29일이라는 사실 공유
            현재는 목성 궤도를 통과 중
            이정도 속도임에도 도달하는 데 이렇게 오래 걸린다는 점에 깊은 감탄
            “우주가 얼마나 광활하고, 믿을 수 없을 정도로 크다는 걸 절대 못 믿을 것”이라는 더글라스 애덤스의 명언 인용
          + 제공된 링크에서 ""Specified object was not found"" 오류
            ‘활성화’라는 표현이 구체적으로 무엇인지 궁금함
            혹시 이것이 혜성처럼 기둥(plume)을 뜻하는지 질문
          + 흥미로운 정보에 대한 감사
            ‘이심률’이 궤도와 천체의 형태 중 무엇을 나타내는 개념인지 궁금
            2017년 ‘Oumuamua’의 경우 어떤 방식으로 모양이 길쭉하다는 것을 밝혔는지, 새로운 천체에 대해서도 비슷하게 신장비를 파악할 수 있는지가 궁금
            관련 나사 링크
          + 링크된 시뮬레이션을 보면 화성 궤도와 비교적 가까운 것으로 보이나, 우주가 얼마나 큰지를 이해
            하지만 이런 크기의 물체가 만약 화성에 90km/s로 충돌하면 무슨 일이 일어날지 매우 궁금
          + 제공된 궤도 뷰어에서 ""specified object not found"" 오류 발생
     * 인류가 수천 년 동안 이런 천체를 못 봤다가 최근 몇 년간 세 개나 발견된 상황에 대해 두 가지 가설 제안

    1. 천문학자들이 이제는 이걸 발견할 만한 역량을 갖췄다는 것
    2. 이 암석들이 앞으로 계속 밀려오는 행성계 파도 시작이고, 우주가 인류를 파괴하려 한다는 유머 섞인 추측

     * Vera Rubin 망원경이 운영 시작, 전체 하늘을 3일마다 탐색 예정
       이런 천체 탐지가 쉬워졌으며 앞으로 더 많이 관측할 수 있을 것으로 기대
       관련 영상
     * 첫 번째 가설이 맞는다는 의견
       더 발전된 망원경과 GPU 가속 알고리즘 조합이 큰 역할
     * ""Interstellar Object를 세 번째로 발견""이라는 제목을 ""Interstellar Object의 세 번째 발견""처럼 바꿔야 한다는 유머러스한 의견
     * ‘이 암석들이 쏟아질 조짐’에 대해, ‘Oumuamua가 정찰 드론이었다면 이제 너무 늦었다’는 농담
     * 농담을 이해하지만, 우연히 우리가 지금 은하계 먼지 구름을 지나고 있을 가능성도 고민
     * Vera Rubin 망원경 덕에 앞으로 이런 점성간 천체가 훨씬 더 많이 발견되리란 기대
     * 다른 스레드에서 ""Interstellar Objects in the Solar System: Isotropic Kinematics from the Gaia Early Data Release 3"" (논문 arXiv 링크)가 언급된 정보 공유
       논문에서는 이런 천체의 개수를 약
       Nisc <~ 7.2 × 10−5 AU−3
       로 추정
       내가 계산이 틀릴 수도 있지만, 이 값이면 토성 궤도 반경 내에 언제든 한 개 정도 있을 수 있다는 의미
     * 2019년 소설 “The Last Astronaut”에서는 “2I”라는 허구의 인터스텔라 물체가 등장
       현실에선 이미 3I까지 발견된 사실이 흥미로운 순간
     * 우리가 점점 더 혜성과 닮은 인터스텔라 물체를 발견할수록 ‘Oumuamua’의 기이함이 더 부각된다는 인상
          + 외계인이 탐사선을 보낸다고 가정한다면, 그 크기는 큐브샛 정도로 훨씬 더 작아서 우리가 알아채지 못할 수도 있다는 생각
            혹시 ‘Oumuamua’가 모선이었고, 태양계는 이미 소형 큐브샛으로 가득하다면 재밌는 상상
          + 라만(‘라마’ 시리즈)처럼 항상 모든 것을 셋 단위로 하는 유머러스한 언급
          + 누군가 Omaumau(‘Oumuamua)를 잡으러 탐사선을 보냈으면 하는 바람
            Starship이 상시 비행한다면, 실현이 아주 간신히 가능할지도 모름
     * 나는 관측 데이터에 아무런 배경지식이 없어서 이게 무슨 의미고 어떻게 하나의 오브젝트로 해석되는지 궁금
          + Minor Planet Center(MPC)에서 공식 발표한 내용이라는 설명
            MPC는 태양계 오브젝트 관측 정보를 공식적으로 모으는 기관
            맨 위에 보이는 이름 두 개는 이런 천체에서는 흔한 식별 방식(예: 3I/ATLAS = C/2025 N1 (ATLAS))
            ATLAS 망원경이 이 천체를 발견
            데이터 목록은 여러 망원경별 개별 관측 데이터로, 포맷은 매우 오래된 방식(펀치카드 한 줄에 맞춰서 설계됨)
            이 데이터들을 통해 궤도 계산, MPC뿐 아니라 JPL과 Horizons 서비스에서도 데이터를 활용
     * 만약 이런 물체가 지구를 바로 향해 온다면, 과연 우리가 막을 수 있는 수단이 있을지 궁금
          + 충분한 사전 시간만 있다면 오히려 비교적 쉽게 충돌을 피할 수 있다고 생각
            이런 천체는 대부분 지나가기만 하므로 아주 조금만 궤도 수정해도 피해갈 수 있음
            현재 기술로 충분히 가능하며, 이미 유사한 테스트 경험도 있음
          + 만약 이 천체가 정말 지구로 직진한다면 실상 우리가 할 수 있는 건 거의 없음
            그래도 이런 천체를 더 빨리 찾기 위한 자원을 꾸준히 확충 중이라서 다행
     * 첫 두 개는 거대한 외계 우주선의 감속 단, 본체가 감속하면서 버려진 소진된 연료통이라는 SF적 상상
       이번 세 번째가 바로 본선이라는 유머
          + 이 아이디어로 소설을 꼭 써줬으면 좋겠다는 요청
     * 3I/ATLAS 위키피디아 정보 공유
"
"https://news.hada.io/topic?id=21796","LLM들은 얼마나 클까? (2025년 현황)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        LLM들은 얼마나 클까? (2025년 현황)

     * GPT-2, GPT-3, Llama, Mixtral, DeepSeek, Minimax, Hunyuan 등 주요 대형 언어 모델의 파라미터(매개변수) 규모와 구조에 대한 객관적 정보와 변천사 정리
     * GPT-2 (2019년) 는 1.3억~16억 파라미터, GPT-3 (2020년) 는 1,750억(175B) 파라미터, Llama-3.1(2024년) 은 4050억(405B) 파라미터 등으로 대형 모델의 크기가 빠르게 증가함
     * MoE(전문가 혼합) 구조의 등장으로 GPT-3급 이상 모델이 오픈 소스/다운로드 가능하게 되었으며, 대표적으로 DeepSeek V3 Base(6710억), ERNIE-4.5(4240억), Mixtral-8x22B(1410억) 등 다양한 초대형 모델들이 등장함
     * Dense(모든 파라미터를 사용) 모델과 MoE(일부 전문가 파라미터만 활성화) 모델의 비교가 복잡해졌으며, 실질적인 ""지능"" 비교가 쉽지 않음
     * 최근에는 다중 모달·다중 언어 지원, 새로운 아키텍처, 합성 데이터 활용 등 다양한 발전 트렌드가 나타남
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * 이 문서는 최근 몇 년간 대형 언어 모델(LLM)의 기초 모델(베이스 모델) 규모 변화에 대한 사실 정보를 정리한 내용임
     * 챗봇·어시스턴트가 아니라 텍스트 생성 엔진 본연의 모델에 초점을 맞추고 있음

역사

     * GPT-2(-medium, -large, -xl) (2019): 각각 1.37억, 3.8억, 8.12억, 16.1억 파라미터
          + 약 40GB의 웹텍스트 데이터셋(10억 토큰 추정) 기반으로 훈련됨
          + 사용된 사이트 목록은 domains.txt에서 확인 가능함
     * GPT-3(davinci, davinci-002) (2020): 1,750억 파라미터
          + CommonCrawl, WebText2, Books1·2, Wikipedia 등 약 4000억 토큰 데이터로 훈련됨
          + 대규모 A100 GPU 수천 대로 수개월간 학습 필요
     * GPT-3.5, GPT-4 (2022, 2023): 아키텍처 및 데이터 관련 공식적 정보 비공개

Llama

     * Llama는 Meta(구 Facebook)에서 개발한 대형 언어 모델 시리즈로, 오픈 소스화와 상대적으로 적은 리소스로도 활용 가능한 구조로 주목받음
     * 모델 크기(파라미터 수) 와 학습 데이터, 아키텍처의 진화 과정이 LLM(대형 언어 모델) 오픈소스 트렌드를 이끌었음
     * Llama 1 (2023)
          + 7B, 13B, 33B, 65B: 70억, 130억, 330억, 650억 파라미터 제공
          + 학습 데이터: 1.4조(1.4T) 토큰의 대규모 텍스트(Books3, CommonCrawl 등)
          + Llama 65B는 당시 오픈 모델 중 최대 규모였음
          + Books3는 저작권 관련 법제 논의의 중요한 계기가 된 대규모 데이터임
          + 특징
               o 상대적으로 작은 GPU로도 실행 가능(65B도 8장 GPU로 동작)
               o 오픈 가중치 배포로, 다양한 파생 모델과 커뮤니티 실험 확산
     * Llama 2 (2023 하반기)
          + 공개 당시 70억, 130억, 700억 파라미터 제공(7B, 13B, 70B)
          + 대화형(챗봇) 버전도 공개, fine-tuning 및 RLHF(강화학습) 등 지원
          + 커뮤니티와 상업적 용도까지 허용되는 라이선스(단, 일부 제한)
     * Llama 3.1 (2024)
          + 405B: 4050억 dense(모든 파라미터 사용) 파라미터
          + 학습 데이터: 2.87조 토큰 + 8000억 롱 컨텍스트 + 4000만 annealing(고품질 코드/수학 등 추가) → 총 3.67조 토큰
          + 아키텍처
               o Transformer 기반, 모든 파라미터를 추론 과정에 동시 활용(dense)
               o 고품질 코드·수학 데이터 추가로 주요 벤치마크 점수 극대화(annealing)
          + 특징
               o 대형 dense 모델 중 다운로드 가능한 최신 모델(오픈 소스)
               o Meta가 공개적으로 데이터셋 구성을 밝히지 않으며, 일부 저작권 논란 데이터(Books3 등) 포함 가능성 있음
               o 일부 평가에선 ""어시스턴트 성향""이 강화되어, 순수 텍스트 엔진으로서의 역할과 약간의 차이
     * Llama 4 (2025)
          + 가장 큰 모델: 2조(2T) 파라미터 MoE(Mixture-of-Experts, 전문가 혼합 구조)
               o A288B 16E: 활성 2.88억 파라미터, 16개 전문가, 전체 2조 파라미터 중 일부만 활성화
          + 상황
               o 2T 모델은 미공개(내부 실험용), 파생/축소 버전만 외부 공개(maverick, scout 등)
               o 파생 모델은 원본 대비 ""지능""이 낮다는 평가가 많음
               o 공개 과정에서 벤치마크 점수 조작 논란(lmarena 사건) 등으로 신뢰도 하락 및 팀 해체설
          + MoE 구조 특징
               o 일부 전문가 파라미터만 활성화해, dense 모델보다 같은 파라미터 수 대비 연산 효율 우수
               o 초대형 모델도 실사용 가능(분산 환경·적은 리소스에서 활용)
     * Llama의 의의 및 영향
          + Llama 시리즈는 오픈소스 생태계 확산과 대형 언어 모델의 대중화를 이끌었음
          + Llama-3.1 405B 공개를 기점으로 GPT-3/4급 대형 모델 다운로드/실험이 현실화
          + MoE 구조 도입으로 초대형 모델의 학습·배포가 활발해짐(DeepSeek, Mixtral 등에도 영향)
          + 다만, 최근 모델들은 벤치마크 최적화(annealing), 어시스턴트 성향 강화 등으로 ""순수 언어 모델""로서의 특성 변화 논의가 있음

The desert – 오픈소스 대형 모델의 공백기와 변화

     * GPT-3 수준(1,750억 파라미터급) 이상의 대형 언어 모델을 오픈소스로 구할 수 없던 긴 공백기를 의미함
     * 이 시기(2020~2023년 중반)는 70B 이하 llama 등 비교적 작은 모델만 공개되어 있었고,
          + 일부 프로젝트에서는 작은 Llama(예: 70B) 에 GPT-3가 생성한 합성 데이터로 파인튜닝하는 방식으로 성능을 끌어올리려 시도함
          + 그러나 AI가 만든 텍스트를 다시 AI가 학습에 사용하면 데이터 품질 저하(데이터 ""degeneration"") 문제가 발생할 수 있음
     * GPT-3 수준의 오픈 가중치 모델이 장기간 부재했던 이유로,
          + 학습 비용(수천~수만 개 GPU 인프라), 데이터 확보, 대형 파라미터 구조의 배포 난이도 등이 복합적으로 작용
     * Llama-3.1 405B(4050억 dense 파라미터) 모델이 공개되면서 본격적으로 초대형 모델의 오픈소스화가 시작됨
          + 그 직전(2023년 12월) Mistral의 Mixtral-8x7B(MoE 구조, 총 560억 파라미터), 2024년 4월 Mixtral-8x22B(총 1,410억, 활성 390억 파라미터) 등
               o MoE(전문가 혼합) 아키텍처를 활용해 GPT-3급 대형 모델을 비교적 적은 리소스로 훈련·배포 가능하게 만듦
     * MoE 구조는 여러 전문가 네트워크(Expert)를 두고, 한 번의 추론 시 일부만 활성화
          + 이를 통해 dense 구조보다 적은 리소스(메모리·연산)로 대형 모델 운영이 가능함
          + GPU 대수와 메모리 한계로 인해, MoE는 대형 오픈모델 대중화에 결정적 역할을 함

최신 MoE(전문가 혼합) 대형 모델

  Deepseek V3 Base (2024)

     * 6,710억 파라미터(MoE), 활성 370억, 14.8조 고품질 토큰 학습
     * R1(추론 특화 모델)도 등장, 다운로드 가능 모델 중 최초로 GPT-4급 성능에 근접
     * 공개 직후 NVIDIA(NVDA) 주가가 일시 하락하는 등 시장에 큰 영향을 줌
     * 이후 중국계를 포함한 신흥 대형 MoE 모델들이 속속 출현
          + 일부 모델은 멀티모달·다국어 지원을 위해 다양한 유형의 새로운 데이터를 학습에 도입함

  Databricks (DBRX, 2024년 3월)

     * 1,320억 총 파라미터, 활성 360억, 12조 토큰
     * 16개 전문가 중 4개 선택(미스트랄·그록 대비 더 세분화)

  Minimax (2025년 1월)

     * 4560억 총 파라미터, 활성 459억, 자체 reward labeler로 학습 데이터 품질 제어

  Dots (2025년 6월)

     * 1,430억 총 파라미터, 활성 140억, 11.2조 토큰, 32K 컨텍스트
     * top-6/128 전문가 구조, Qwen2.5-72B와 유사 성능

  Hunyuan (2025년 6월)

     * 800억 MoE, 활성 130억, 20조 토큰, 256K 컨텍스트
     * 8개의 비공유 전문가 활성, 공유 전문가 항상 활성

  Ernie (2025년 6월)

     * 4,240억 총 파라미터, 활성 470억, 수조 토큰

결론 및 전망

     * 2024~2025년 기준, GPT-3급(1,750억) 이상 초대형 모델이 다양하게 공개되고 있음
     * 405B(4050억)가 최신 dense base 모델이지만, 최신 MoE 모델들도 대형화·다양화 중
     * Dense vs MoE 성능 비교는 아직 모호, 진정한 ""지능""에 필요한 구조와 크기에 대한 논의 필요
     * 새로운 구조(RWKV, byte-latent, bitnet), 합성 데이터 활용 등도 실험되고 있으나, 순수 텍스트 엔진으로서의 본질적 발전은 여전히 과제
     * 최근 대부분의 대형 모델이 ""AI 어시스턴트"" 역할로 fine-tune되는 경향, 대안적 LLM 탐구가 필요한 시점

        Hacker News 의견

     * 나는 순수하게 기술적 의견이라기보다는, 이 다운로드 가능한 모델들 안에 얼마나 많은 데이터가 압축되어 있다는 사실에 여전히 감탄함을 멈추지 못하고 있음. 어제 무선 인터넷이 안 되는 비행기 안에서 Ollama를 통해 gemma3:12b 모델(8.1GB)을 다운로드해서 아이들과 함께 이것저것 질문해봤음. 최근 비디오게임이나 동물, 역사 등 다양한 질문에 완벽하진 않았지만, 이렇게 작은 파일 안에 인류의 지식이 이 정도로 담겨 있고, 오프라인에서도 쓸 수 있다는 점이 정말 대단하다고 느꼈음. 손실 압축이긴 하지만 인류 지식을 이렇게 작게 압축할 수 있다는 점이 놀라움
          + 언어 모델이 얼마나 강력한 압축 도구가 될 수 있는지 정말 흥미롭게 생각함. 모델을 어시스턴트(assistant) 용도로 훈련시키면, 일반 텍스트보다 어시스턴트 대화 기록을 더 잘 압축함. UncheatableEval이라는 평가가 있는데, 여기서 언어 모델의 압축 능력을 다양한 과제에 적용하여 파악할 수 있음. 이 평가 기준은 사실상 '속일 수 없는' 테스트임. 압축 성능은 게임처럼 편법을 쓸 수 없는 진정한 벤치마크라고 생각함
          + 오프라인에서 다양한 자료를 내려받아 쓸 수 있는 Kiwix 프로젝트를 추천함. 인터넷 접속이 불안정하거나 없는 곳을 위해 자료가 미리 탑재된 기기도 제공하고 있음
          + 참고로 영어 Wikipedia 기준(2025년 6월 26일), 7백만 개 이상의 문서와 6천3백만 페이지가 있음. 텍스트만 약 156GB이고, 모든 버전을 합치면 데이터베이스 전체 용량은 약 26TB에 달함
          + 8.1GB는 정말 큰 용량임. 6,480억(64,800,000,000) 비트인데, 100비트, 1,000비트는 상상할 수 있을 것 같지만 1만, 100만, 6,400만, 이것의 1,000배인 이 숫자는 정말 엄청난 크기라고 체감함
          + 정보 이론이나 압축 관점에서 언어 모델을 연구하는 분야는 아직 작지만, 효율성과 확장성을 위해 점점 더 중요해지고 있음. 오늘 관련 토론을 진행했으니 관심 있는 분들은 참고해보기 바람
     * Deepseek v1은 약 6700억 파라미터에, 물리적 크기 1.4TB 정도임. 지금까지 디지털화된 모든 책을 압축하면 몇 TB, 공개된 웹은 약 50TB, 영어 전자 텍스트 전부를 zip으로 묶으면 O(100TB) 근방일 것이라 예측함. 현재 모델 크기는 전체 중 1% 수준이고, 이제는 크기를 더 키우는 것이 성능을 더 이상 기대만큼 올려주지 않는 구간에 진입한 것 같음(gpt4.5 vs 4o 참고). 이런 이유로 최근에는 '추론형(reasoning) 모델'로 인해 컴퓨팅 비용이 추론 시간(inference) 쪽으로 이동하고 있음. 추가적인 효용을 얻으려면 앞으로는 특정 도메인에 초점을 맞춘 특화(특수화) 모델로 진화할 것으로 예상함. 고품질 오픈소스 모델을 위한 1TB 인퍼런스 VRAM이 중기적인 타겟이 될 수 있을 것이라 생각함. 중소기업(SME) 수준에서도 손에 닿는 스펙임(약 250B 파라미터 예상)
          + 이미지와 영상을 추가하면 위의 예측 값들이 640KB면 충분하다는 예전 말처럼 들릴 수도 있음. 이후에는 로봇이 스스로 세상을 탐험하면서 데이터를 확보한다면 더 많은 정보가 쌓일 것임. 진지하게 말하면 이미지 및 인터랙션 데이터 추가는 텍스트 생성에도 상당한 효용이 있을 것임
          + 실제 수치를 한 번 계산해 보았음. 논문 1억 5700만 편, 책 5200만 권 기준으로 평균 논문 1만 단어, 책 10만 단어로 잡았고, 표본 책 데이터로 압축률을 산출함. 비압축 상태 약 30TB, 압축 시 5.5TB 수준임. 2TB 마이크로SD 3개(총 750달러)면 저장 가능함
          + 작은 지적사항인데, 고정된 저장 용량에 대하여 big O 표기법(O(100TB))을 쓰는 것은 적합하지 않다고 생각함
          + 혹시 50TB가 미국 의회도서관 Library of Congress 기준인지 물어봄. 인터넷 전체는 훨씬 클 것임
          + '모든 디지털화된 책이 수 TB로 압축된다, 공개 웹은 50TB'라는 수치는 어디서 나온 것인지 궁금함. 출처가 있다면 보고 싶음. 예전에 세기 전까지의 문자 기록물이 약 50MB라는 글을 읽은 적 있는데, 출처를 찾을 수 없어 혹시 잘못 기억하는 것일 수 있음
     * Gemma, Gemini 시리즈 모델(Google)이 빠져 있음. 그리고 T5 시리즈는 전이 학습 및 이 분야 확산에 중요한 역할을 했는데 언급이 없는 것도 아쉬움. T5는 많은 개념의 시초라 할 수 있음
          + Gemma 모델은 용량이 작아서 리스트에 포함되지 않은 것임. T5는 역사적으로 아주 중요하지만 크기가 11B 미만이라서 따로 많은 언급을 하지 않음. 그래도 참 의미 있고 재미있는 모델임

     * 시각적으로 보고 싶다면, 연도별 파라미터 총합을 그래프로 정리한 자료가 있음 Total Parameters vs. Release Year by Family
          + 이 그래프는 GPT-3가 얼마나 큰 도약이었고, 이후 오랜 시간 아무도 그 수준을 따라잡지 못했음을 아주 명확하게 보여주는 시각화 자료임
          + 정말 멋진 자료임. 만들어줘서 고마움. 본인 게시글 코멘트에 차트 스크린샷과 링크, 크레딧을 남겨둠
     * 정말 좋은 글임. 다만, 이런 초거대 언어 모델만이 최고의 혁신인 것처럼 전제하는 점이 있음. 대형 플레이어들이 그간 꽤 조용했는데, 외부에서 볼 때 OpenAI는 그들의 행동을 통해 살짝 힌트만 줬음. 훨씬 더 큰 모델을 만들었으나 결과가 실망스러워서 조용히 실험을 중단함. 실제로 가장 강력한 최전선 reasoning 모델들이, 공개된 거대 모델보다 오히려 더 작은 경우가 있을 수 있음
     * 아이러니한 상황임. 오픈소스 커뮤니티가 GPT-3(175B)와 맞추기 위해 30~70B 모델, RLHF, 합성 데이터 등 다양한 시도를 했지만 격차가 여전히 있었음. 결국 모델 본연의 크기가 정말 중요하다는 게 밝혀졌으며, 진정으로 거대한 dense(405B) 또는 MoE 모델(DeepSeek V3, DBRX 등)이 등장해야 공개랩 바깥에서도 GPT-4 수준 reasoning이 나왔음
     * ""open-source 모델이 GPT-3 수준에 다가가기 위해 대부분 70B급 Llama를 GPT-3이 생성한 합성 데이터로 학습시켰다""란 부분의 주석에 동의하지 못함. 합성 데이터가 항상 성능 저하로 이어졌다면 AI 연구소들이 절대 쓰지 않을 것임. 실제로 합성 데이터를 활용해 더 좋은 모델을 만들고 있음. 아주 의도적으로 '본인 출력으로 학습 루프를 돌리는' 상황에선 성능 저하가 나온다는 논문이 있지만, 실제로 AI 연구소가 합성 데이터를 쓰는 방식과는 차이가 있음. 그 논문이 인기를 끄는 건, 마치 '스스로 꼬리를 먹고 멸망하는 AI'라는 컨셉이 너무 매력적이기 때문임
          + 동의함. 특히, 더 작은 모델을 더 큰 모델의 출력으로 학습시키는 문맥에서 distillation(지식 증류)은 매우 효과적인 기법임. 개인적으로도 과거 Llama, Mistral 모델을 사람 데이터와 GPT-4 생성 데이터로 도메인 특화 튜닝했는데, (품질 좋은) 합성 데이터 추가 후 결과가 더 좋아졌음
     * 사람들이 LLM을 손실 압축이라고 계속 반복하는 게 아쉬움. 대략적으로는 맞는 비유일 수 있지만, 더 엄밀하고 흥미로운 사실은 LLM이 손실 없는(lossless) 압축 알고리듬으로도 기능한다는 점임. 두 가지 사례가 있음. 1) 어떤 텍스트도 LLM의 로그 우도에 근접하는 비용으로 산술 부호화를 쓸 수 있음(전제: 송신자와 수신자가 같은 LLM 파라미터를 갖고 있어야 함) 2) LLM과 SGD(학습코드)를 활용하면 손실 없는 압축을 구현할 수 있음(여기서 모델 파라미터는 설명 길이로 치지 않음) Jack Rae의 “compression for AGI” 자료를 참고할 만함
          + 1번 관련, 고전 압축 기법도 송신자·수신자 모두 같은 대형 사전을 갖고 있다면 매우 효과적임
     * ""1.61B"" 이런 수치만으로는 얼마나 큰 파일인지 또는 VRAM이 얼마나 필요한지 감이 잘 안 옴. 실제 저장 공간과 하드웨어 요구사양, 내가 지금 구입하면 어느 선까지 돌릴 수 있는지, 10년 뒤엔 어떤 모델을 돌릴 수 있을지 궁금함
          + 파라미터 하나에 1바이트(f8)면 1.6GB, 2바이트(f16)이면 2.3GB임. GPU에 로드하는 것 외에 추가 메모리 소요도 있어서 대략 파라미터 수의 4배 정도를 잡는 것이 좋음. 즉,** 2B 파라미터라면 8GB VRAM 추천**
          + 대부분의 모델은 16비트(2바이트)로 학습됨. 10억 파라미터 모델은 2GB. 실사용에는 더 작은 8비트 양자화로도 충분하며, 보통 16비트에서 8비트로 줄여도 성능 손실이 거의 없음. 따라서 10억 모델은 1GB, 20B 모델은 20GB로 단순 계산이 됨. 더 낮은 비트(5비트, 4비트 등)로도 성능 저하가 크지 않으면 용도에 따라 충분히 실전 적용 가능함. 심지어 4비트로 직접 학습한 모델이 16비트에서 양자화한 모델보다 더 나은 품질을 보인다는 사례도 있음. 대용량 모델의 병목은 VRAM 용량이 아닌 대역폭임. 따라서 VRAM이 많은 GPU가 중요함. 128GB 시스템 RAM이 있어도 GPU-CPU 간 대역폭이 부족해 GPU 메모리를 넘어가면 오히려 CPU가 더 느림. GPU(예: RTX 5090)는 32GB VRAM, 대역폭 1Tb/s 수준. Apple M 시리즈는 512Gb/s, AMD Strix Halo는 128GB 통합 메모리와 256Gb/s 대역폭 제공. consumer 하드웨어로 LLM
            돌리는 실제 경험은 Reddit r/LocalLLaMA 참고 가능. 다만 거기 활동은 범상치 않은 시도도 섞여 있으니 주의. 10년 뒤 상황은 예측 불가. TSMC, 삼성, 인텔 모두 하이퍼스케일러 수요에 맞춰 플래그십 GPU 양산에 몰두 중이고, 반도체 산업도 여러 변수(정치, 무역, AI, 블랙스완 등)로 예측이 어려운 국면임
"
"https://news.hada.io/topic?id=21804","PlanetScale for Postgres 공개 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      PlanetScale for Postgres 공개

     * MySQL 호환 서버리스 플랫폼 PlanetScale이 Postgres를 위한 전용 호스팅 플랫폼 프라이빗 프리뷰를 발표
     * 최고의 서비스 가용성과 안정성에 집중하고 자동 장애조치 등 업계 최고의 엔지니어링 제공에 집중함
     * 기존 Postgres 호스팅 사용자의 불만인 비용, 정기적 장애, 낮은 성능 문제를 타겟으로 함
     * 성능 및 플랫폼 특징
          + 벤치마크 결과, 모든 경쟁 Postgres 제품을 꾸준히 앞섬 (2배 리소스 제공한 경쟁사 대비)
          + PlanetScale for Postgres는 실제 Postgres를 독점 Operator로 운용함
          + PSBouncer 프록시 레이어로 자동 장애조치, 쿼리 버퍼링, 커넥션 풀링 등 고가용성 제공
          + Postgres v17 사용, Postgres v13 이상에서 온라인 마이그레이션 및 다운타임 없는 자동 버전 업데이트 지원
          + PlanetScale Metal의 로컬 NVMe SSD 스토리지가 비용/성능 비율을 획기적으로 개선함
     * 확장성 전략 및 향후 계획
          + Vitess는 MySQL 중심의 확장성 솔루션이자 PlanetScale의 강점임
               o Vitess로 대규모 샤딩 네이티브 제공
               o 하지만 이번에는 Vitess를 Postgres 확장성에 직접 사용하지 않음
          + Postgres만을 위한 새로운 확장성 시스템을 처음부터 설계 중임
          + 개발이 진행될수록 추가 정보와 얼리 액세스를 지속적으로 공개할 예정

   postgresql 자동 버전 업데이트를 구현한 방식이 궁금하네요. major 버전 변경되면 시스템 리빌드해야하는 문제가 있을텐데 어떻게 해결한걸까요?

        Hacker News 의견

     * 1~2년 동안 PlanetScale을 사용하다가 Neon으로 전환한 경험 공유. 테넌트마다 별도의 데이터베이스가 필요했으나 PlanetScale은 데이터베이스 당 월 $30(이제는 $39) 비용 책정으로 부담 발생. 내 사용 케이스는 특이하고, 강력한 서버 필요성도 없음. 여러 데이터베이스를 한 서버에 돌릴 수 있으면 충분하며, PlanetScale에서는 이게 불가능했지만 Neon은 지원. 작은 회사 운영 중이며 예측 가능한 트래픽 변동성 존재. PlanetScale 제품과 지원 서비스는 매우 만족스러웠으며 언젠가 다시 사용 희망. 나는 식음료 페스티벌 소프트웨어 개발자로 1년 중 9개월은 트래픽이 거의 없고, 2개월은 약간, 3주 정도만 조금 많으며, 페스티벌 기간 1~5일만 부하가 몰림. 나는 아주 작은 고객군임을 인식, 대부분 업체가 내 요구에 직접 대응하지 않는 현실 인정
          + 테넌트별로 물리적인 데이터베이스가 꼭 필요한 규제나 이유가 있는지, 아니면 단순히 PlanetScale 한 DB 안에 여러 논리적 데이터베이스/스키마를 사용할 수 없는 이유가 궁금
          + 테넌트 수에 따라 Turso가 내 요구사항에 맞을 수도 있음 Turso 소개
     * PlanetScale은 Vitess에서 파생된 MySQL 특화 솔루션에서 출발. 이번 PostgreSQL 제품도 Vitess와 관련 있는지, 혹은 완전히 새로 만든 시스템인지 궁금증. 본인이 직접 찾아본 결과, PlanetScale for Postgres 개발 블로그에 따르면, MySQL 기반의 Vitess와는 달리 Postgres를 위해 처음부터 아키텍처를 새로 설계 중임을 확인
          + Simon의 팬임을 밝히고, PlanetScale 조직을 만들고 이메일(s@planetscale.com)로 이름을 알려주면 무료로 조기 접근을 제공하겠다고 제안
     * 지난 2년간 PlanetScale MySQL 사용자로서 이번 PlanetScale PostgreSQL 출시를 매우 반갑게 생각. 이전 회사에서 두 DB 모두 운영했으나 툴링에 차이 있어 아쉬웠음. PlanetScale은 db 관리 경험이 Treo에서 iPhone으로 바꾼 것처럼 전체적으로 혁신적인 만족감 제공. PlanetScale 팀에 축하 메시지
     * 최근 PostgreSQL 확장성 관련 흥미로운 프로젝트들이 연이어 등장. PlanetScale이 이번에 어떤 제품을 내놓을지 기대감 표현. 개인적으로 많은 정보를 더 원하지만 지속적으로 지켜볼 예정. 참고할 만한 프로젝트로 Supabase Multigres, pgdog 링크 공유
     * Postgres와 협업하며 이번 신제품을 시장에 내놓는 경험이 즐거웠음. 궁금한 점 있으면 질문 환영
          + Aurora와의 벤치마크는 봤지만, Aurora의 NVMe SSD 기반 Optimized Reads와의 비교가 궁금 참고 링크
     * 새로운 hosted Postgres 선택지의 등장을 멋지게 바라봄. Multigres(Supabase)와 PlanetScale의 경쟁 구도에서 어떤 차별점이 나올지 기대
          + Multigres가 Vitess 공동 창업자이자 PlanetScale 공동 창업자인 Sugu Sougoumarane가 리드하고 있어 두 프로젝트 모두 같은 뿌리에서 성장하는 셈. 관련 영상 공유
     * (PlanetScale PostgreSQL의) 확장 프로그램 지원 범위, 제한 사항 등 궁금증 질문
     * 약간 주제와 다르지만, PlanetScale 웹사이트에 개발자를 위한 MySQL 강좌도 추천
          + 해당 강좌 강사 본인이 직접 등장, 좋아해줘서 기쁘다는 메시지
     * PlanetScale의 이번 행보가 흥미롭다고 평가. 데이터가 단일 머신을 넘는 순간부터 복잡성이 급격히 증가하며, 분산 시스템의 경우 complex join, 확장성, 강력한 일관성 같은 기능 일부 희생 필요. Vitess(MySQL)와 유사한 트레이드오프가 있는지, 아니면 Postgres 고유의 복잡성이 추가로 발생하는지 궁금. Jepsen(분산 시스템 검증 프로젝트)의 검증 요청 제안. PlanetScale 환경과 표준 Postgres 대비 어떤 기능 차이와 손실이 존재하는지 지적
          + MySQL 호환성 관련 공식 문서(링크)는 있지만 Postgres와의 실제 호환성, 사용 경험 등은 근본적으로 다를 것이라 예상
     * 소식을 늦게 접했지만 훌륭한 뉴스임을 강조. 일부 기술이 오픈소스로 공개될지 궁금증 제기
"
"https://news.hada.io/topic?id=21786","Show GN: 타우린 – 당신처럼 생각하는 AI 이메일 클라이언트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Show GN: 타우린 – 당신처럼 생각하는 AI 이메일 클라이언트

   Taurin은 받은 편지함을 더 빠르고 똑똑하게 비우기 위한 이메일 클라이언트입니다.
   저희는 사용자의 업무 흐름에 맞춰 스스로 적응하는 Personal Agent를 만들고 있으며, 이를 이메일 클라이언트에 녹여내어 개인의 생산성을 최대한으로 끌어올릴 방법을 찾고 있습니다.

   [지금 지원하는 것]
     * 메일별 중요도/답장 필요 여부 판단 후 개별 뷰 제공
     * 특정 연락처와 주고받은 모든 대화와 파일을 사이드바에 정리
     * (타우린으로 보낸)이메일 열람 추적
     * 이메일 요약 및 자동 라벨링 (커스텀 가능)
     * local-first 소프트웨어라 훨씬 빠름

   [3분기 내 지원할 것]
     * 이메일 Todo 변환 및 날짜 속성이 있는 Todo를 캘린더에 연동
     * 다중 도메인 연결을 지원 (Outlook 포함)
     * 모바일 앱을 지원하고, 사용자 설정 Notification을 지원
     * 자주 사용하는 구절을 스니펫화
     * Inbox zero 경험을 더 쉽게 할 수 있게 Home을 제공
     * AI Agent 고도화

   아직 저희가 목표하는 프로덕트의 모습은 아니기에,
   지금 가입하는 유저분들께 영구적인 대폭 할인을 제공하고 있습니다.

   저희 공동창업자들은 제주도에서 한 집에 같이 살면서 제품을 만들고 있습니다.
   타우린이 본격적으로 개발된지는 이제 세 달 정도 되었는데요,
   local-first infrastructure를 만드는데 제일 오랜 시간을 썼습니다.

   https://help.taurin.io/changelog
   타우린 업데이트 내역인데요, 저희 팀은 개발 속도에 자신이 있습니다.
   카드 등록 없이 Gmail 계정만 있다면 7일 무료체험이 가능합니다. 사용해보시고 목소리를 들려주세요!

   저만 다운로드가 안 되나요?

   도메인 설정 변경 후에 꼬인 것 같습니다. 최대한 빠르게 고쳐두겠습니다 ㅠ

   outlook에서도 사용 가능한가요?

   아웃룩은 3분기 내 연동 예정입니다!

   목표하시는 프로덕트의 모습이 기대됩니다.
   다만 정보나 메일 데이터가 안전하게 보호되는지 궁금하네요.

   이메일 앱이다 보니 보안 관련하여 걱정하시는 분들이 많을 것 같은데요,
   저희도 신경을 많이 쓰고 있고, Google이 요구하는 강력한 보안 정책에 맞춰 CASA Tier 2 보안 인증을 통과했음을 알려드립니다. 모든 데이터는 암호화되어 DB에 저장되고 있습니다.

   10:10 AM 다운로드 문제 해결되었습니다 !
"
"https://news.hada.io/topic?id=21707","VS Code의 Copilot Chat이 오픈 소스로 공개됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   VS Code의 Copilot Chat이 오픈 소스로 공개됨

     * VS Code의 GitHub Copilot Chat 확장이 오픈 소스로 배포됨
     * Chat view에서 Copilot에 자연어로 질문하여 코드 관련 도움을 즉시 받을 수 있음
          + 워크스페이스의 코드, 변수, Participant 정보, Slash 커맨드 등을 활용하여 맥락 감지형 답변 제공
     * Inline chat으로 코드 내에서 직접 인공지능의 제안을 적용 가능
          + 리팩토링, 예외 처리 추가, 복잡한 알고리듬 설명 등이 실시간으로 가능함

개요

     * GitHub Copilot은 AI 기반의 동반 프로그래머 도구로, 코드 작성 속도와 품질 향상에 도움을 주는 확장 프로그램임
     * 사용자는 프로젝트에 가장 적합한 모델을 선택하고, 채팅 응답을 커스터마이즈하며, agent mode를 활용하여 자연스러운 협업 프로그래밍 세션을 구현할 수 있음
     * GitHub Copilot(코드 자동완성)과 GitHub Copilot Chat(대화형 AI)이 각각 확장으로 설치됨
     * 무료 플랜 또는 엔터프라이즈 관리자의 승인을 통해 이용할 수 있으며, 구독 필요함
     * 개인 및 비즈니스 용도를 위한 다양한 상품(개인, 비즈니스, 엔터프라이즈 플랜)이 제공됨
     * Copilot Chat은 UI 통합 특성상 VS Code의 최신 버전에서만 동작하며 최신 모델 및 기능 연동이 가능함
          + 구형 VS Code에서는 이전 버전 Copilot Chat만 사용 가능함
     * Copilot Chat 사용은 GitHub Copilot Preview 약관 및 투명성 안내에 동의하는 것임
     * 사용자의 코드는 소유권이 보장되며, GitHub Copilot의 다른 사용자에게 추천 코드로 활용되지 않음

        Hacker News 의견

     * 여기에 그들이 사용하는 시스템 프롬프트 템플릿 링크를 공유함 링크
          + developer의 커서 위치를 ${CURSOR_TAG}로 표시하는 기능 설명을 알게 됨, 이 부분이 코드 내에서 개발자가 어디에 집중하고 있는지 파악하는 데 중요하게 쓰인다는 사실이 흥미로운 정보임
     * vs code에서의 Copilot은 기대에 못 미치는 부분이 많고, Microsoft 수준에서 기대하는 다듬어진 완성도가 부족한 모습
          + agent mode가 추가된 이후에도 이런 느낌을 받았는지 궁금함, 내 경험에서는 상당히 만족스러운 사용감임, gemini CLI나 cline은 아직 시도해 본 적 없지만 이들이 Copilot보다 나을 것 같지는 않음
          + 예전에는 Microsoft에 기대치를 두곤 했지만, 최근 15년 동안 나온 제품들을 보면 그런 기대를 가지기 힘든 상황
     * Copilot Chat이 프롬프트와 응답을 어떻게 처리하는지의 의사결정 트리를 누군가 AI로 분석해주면 좋겠음
          + 나 또한 Copilot Chat의 동작 흐름에 대해 매우 궁금함, 우선 툴[0]과 프롬프트[1]를 참고하고, 코드의 호출 경로를 추적하는 중임
          + 주요 흐름은 사용자가 ask나 edit를 선택하면 AskAgentIntent.handleRequest, EditAgentIntent.handleRequest로 진입하며, 이후 DefaultIntentRequestHandler.getResult()에서 instance를 생성하고 intent를 호출해 runWithToolCalling 루프를 진행함, 프롬프트와 툴 응답을 .tsx로 렌더링함
          + 관련 파일 링크로는 createFileTool.tsx, editCodePrompt.tsx, 그리고 toolCallingLoop.ts가 있음
     * Copilot Chat은 그냥 Microsoft SaaS의 프론트엔드에 불과함, 실제로 중요한 부분은 GitHub Copilot API 뒤에 숨어 있기 때문에 진정한 의미의 오픈소스라 보기 어려움. 누구도 이 LLM의 설계 또는 학습 데이터에 접근할 수 없고, 셀프 호스팅도 불가능함, 그저 또 하나의 구독 모델 광고이자 개인정보를 제3자에게 넘기는 구조임, 커뮤니티적 가치나 공공 이익도 없음
          + 이 의견에 반박하는 입장임, 모든 상용 SOTA LLM 모델은 대체로 비슷한 코드 품질과 한계를 보임, 진짜 차별점은 유의미한 맥락 정보를 어떻게 전달하고 결과를 프로세싱하는 구조에서 나타남, 단순히 LLM에 파일 내용을 반복 입력하는 에이전트는 누구나 만들 수 있지만, 그런 방식만으로 좋은 코딩 에이전트가 만들어지지는 않음
          + 이 비판의 논리를 잘 따라가기 어려움. 오픈소스는 말 그대로 소스를 연다는 뜻이고, 그게 사용자에게 얼마나 유용한지는 별개의 문제임
          + 오픈소스의 의미가 소스 코드 공개라 생각함, 특별히 이 부분에서 비판할 이유가 이해되지 않음
          + Copilot Chat이 프론트엔드에 불과하다는 점에는 동의함, 하지만 프론트엔드 자체도 오픈소스화될 경우 누군가는 처음부터 UI를 새로 만들 필요가 없어지는 공공적 가치가 생김, 그리고 LLM 설계나 학습 데이터까지 오픈소스화해야 한다는 비판에는 무리가 있음, 현실적으로 오픈소스 커뮤니티에서 그 대규모 컴퓨팅 리소스를 운용할 여력이 없음
          + 최근 댓글들에서 ""AI"" API에 대한 불만이 자주 보임, 개발자 입장이 아니라 이해가 쉽지 않음, 과거 나도 ""Web API""에 대한 유사한 불만이 있었음
          + 예전에는 웹사이트에 API가 없었고 그냥 웹상에서 공개된 정보를 스크래핑하곤 했음, 이 방식은 가입 없이 웹사이트가 존재하는 한 가능함
          + 반면 Web API는 대개 이메일 수집, 사용 기록 추적, 계정 제약, 요금제 등의 방식으로 접근권한을 제한함, 종종 API가 예고 없이 중단되는 리스크도 있음
          + 홈페이지에서 데이터를 일괄 다운로드하는 선택지는 일부 정부 사이트, Wikipedia dump 등 소수에 불과함
          + API가 LLM에도 적용되어, 모델의 가중치 등 내부 데이터 또한 공개하지 않은 채 사용자에게는 계정 가입, 구독 등으로 공개 정보를 우회적으로 제공하는 상황이 됨, 데이터는 서버에 있지만 직접 받을 수 없는 구조임
          + 이런 방식이 결국 개발자들이 옹호했던 툴 및 기술이 다시 개발자 본인들에게 불리하게 작동하기 시작한 것 아닌지 의문임
"
"https://news.hada.io/topic?id=21822","미니 NAS, NVMe와 Intel의 효율적인 칩 결합","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     미니 NAS, NVMe와 Intel의 효율적인 칩 결합

     * NVMe SSD와 Intel N100/N150 칩을 결합한 최신 미니 NAS 3종이 출시됨
     * 각 NAS 모델은 저전력, 소형 폼팩터와 2.5 Gbps 네트워크를 제공하지만, 개별적으로 미세한 단점이 존재함
     * GMKtec G9는 합리적인 가격이 강점이나, 쿨링 이슈가 있어 리비전을 거침
     * Aiffro K100는 가장 작고 에너지 효율성이 뛰어나지만, 기능 일부가 제한되고 가격이 높음
     * Beelink ME mini는 확장성이 높고 조용하지만, 여러 NVMe 슬롯에서의 대역폭 분할로 성능 손실이 있을 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

전체 요약

홈랩 스토리지 NAS의 변화

     * 기존의 24U 랙에서 소형 미니 랙으로 전환하며 저용량, 고효율 NAS를 필요로 하는 환경 변화 설명
     * YouTube 채널 운영 공간 이전으로 인해 저장 공간 요구량이 줄어, 6TB 정도의 NVMe 기반 스토리지에 집중함
     * 최근 1년새 출시된 미니 NAS 3종을 직접 테스트하고 각 장단점을 정리함

최신 미니 NAS 3종 비교 개요

     * 세 모델 모두 Intel N100/N150 칩 기반, PCIe Gen 3 최대 9레인 사용, 4~6개의 M.2 NVMe SSD 슬롯 제공
     * 2.5Gbps 네트워크 기본 제공, GMKtec 및 Beelink에는 듀얼 2.5Gbps NIC 적용
     * 각 제품마다 크기, 쿨링, 네트워크, 스토리지 확장성 등에서 차별점 및 절충점이 존재함

GMKtec G9

     * 가격 경쟁력이 뛰어나지만, 초기 모델은 쿨링 이슈로 인해 4개의 NVMe를 모두 사용할 경우 발열 및 안정성 문제 발생
     * 이후 쿨링 벤트 크기 확대 등 디자인 리비전이 있었으며, 최신 개선된 제품은 아직 추가 테스트 필요함

Aiffro K100

     * 가장 작은 폼팩터와 금속 케이스, VRM 포함 넓은 방열판 및 충분한 측면 통풍구로 쿨링 능력을 높임
     * eMMC 제거로 운영체제 설치를 NVMe SSD 또는 USB에만 가능하고, WiFi 미지원 및 단일 2.5Gbps 포트 제공
     * BIOS 커스터마이징 옵션이 제한적이며, 전체적으로 매우 조용하고 저전력임
     * 가격이 가장 높음 (G9 대비 100달러 이상 비쌈)

Beelink ME mini

     * 더 조용한 작동과 Apple Trash Can과 유사한 ‘굴뚝형’ 방열판 설계 적용, 성능 모드에서도 발열로 인한 쓰로틀링 현상 없음
     * 6개의 NVMe 슬롯 제공(5개는 x1, 1개는 x2), x2 슬롯에 SSD 사전 장착 시 성능 우위
     * 내장 eMMC(64GB) 보유, 테스트 시 Ubuntu 설치하여 활용함
     * 전원 공급 장치가 케이스 내 일체형이며, 발열 문제는 테스트에서 발생하지 않았음
     * K100 대비 조금 더 높은 온도지만, 성능 모드 기본 설정 차이 때문임

성능 및 결론

     * 세 NAS 모두 최소 250MB/sec의 읽기/쓰기 성능 제공, Beelink의 경우 여러 NVMe 슬롯을 x1 대역폭으로 사용 시 일부 성능 손실 발생
     * K100은 기본적으로 전력 프로파일이 밸런스 모드로 출고되고, 불필요 기능 제외로 인해 에너지 효율성 최고임
     * 제품별 비교
          + GMKtec: 예산 중시, 개선된 쿨링 설계의 신제품 확인 필요
          + K100: 초소형, 에너지 절약, 저발열, 단 eMMC 미포함 및 고가
          + Beelink: 확장성 최고, 예판 기준 K100보다 저렴, 단 특성상 슬롯당 대역폭 제한
     * 각 NAS의 절충점을 고려해 목적에 따라 선택이 필요함
     * 필자의 경우 6TB RAIDZ1 구현을 위한 4TB NVMe SSD를 저렴하게 구할 경우 K100을 우선 고려함

        Hacker News 의견

     * Intel N150은 15년 만에 TXT/DRTM을 지원하는 첫 소비자용 Atom CPU라는 특징 강조, 부팅 시 시스템 소유자가 직접 관리하는 키로 불변 컴포넌트의 해시 무결성 검증 가능 설명, 이를 기반으로 Qubes의 Anti Evil Maid, Windows IoT의 SystemGuard, 미래에는 다양한 운영체제 지원 기대 언급, Proxmox, FreeNAS, OPNsense 등에서 유용하게 활용할 수 있는 기능이라고 판단, Topton에서 Bootguard가 fuse되지 않은 N150 기기가 꽤 출시되고 있다는 점, 이런 모델에 coreboot 이식 기대감, 향후 ODROID도 N150 기기를 출시해주길 바라는 마음, 팬리스 mini NAS/라우터/스토리지 등에서 coreboot와 DRTM이 결합된 저비용의 보안성 향상된 시스템 가능성 탐색, Gracemont 아키텍처 및 관련 배경 링크(https://chipsandcheese.com/p/gracemont-revenge-of-the-atom-cores, https://youtu.be/agUwkj1qTCs), 네트워크 사업자의 WiFi 감지 활용 이슈의 HN 토론
       내역 안내(https://news.ycombinator.com/item?id=44426726#44427986)
          + Bootguard가 팩토리에서 fuse되지 않은 모델을 어디서 봤는지 궁금, 그런 기기를 직접 구해보고 싶은 욕심 표현
     * mini, NVMe 형태로 서버를 구성하면 매력은 있지만 일반적인 사용환경에서는 비용 효율성 의문 제기, 확장성, ECC, 쿨링, I/O 모두 희생한다는 점 짚음, 본인은 Fujitsu D3417, ECC 64GB 조합의 Proxmox 서버 5년 운영 사례 공유, 350달러로 구축 후 스토리지 업그레이드만 한 번 진행, 평소 전력 12~14W, 10 Docker 컨테이너와 Windows VM도 무난하게 돌리는 사용성, 결국 ECC, IPMI, 여유로운 NVMe 슬롯, 2.5GB 네트워킹 등 갖춘 mATX 보드가 이런 미니 기기보다 낫다는 점, 그래도 Jeff의 콘텐츠는 여전히 최고라는 평가
          + 대체적으로 의견에 동의하지만 ODROID H4 모델은 예외적으로 꽤 많은 혁신이 있음 지적, SODIMM 램, SATA 포트, M.2 2280 슬롯 등 업그레이드 확장성 보장, in-band ECC도 지원해 ECC 요건 만족, Mini-ITX 어댑터($15)로 기존 케이스 활용 가능, IPMI는 없고 NVMe 슬롯 많진 않으나, 여전히 좋은 mATX 조합이 더 탁월하다고 생각
          + 특별한 SSD 필요가 없다면(무작위 접근이 많거나 극단적으로 소형이거나, 진동 많은 환경 제외) 대용량 데이터 저장에는 여전히 하드디스크가 NVMe보다 훨씬 경제적이라는 점 강조, 12TB 제조사 리퍼 HDD가 멀티년 워런티에 약 $120, 8TB NVMe도 $500 이상, HDD가 내부적 속도는 NVMe에 밀리지만 NAS의 경우 네트워크(2.5GBit LAN) 대역폭이 오히려 병목이라는 실사용 경험 공유
          + 본인은 이런 작은 박스가 집안에 딱이라고 생각, 용도는 맥의 백업 서버 겸 영화 콜드 스토리지, 6x2TB 드라이브로 9TB RAID-5를 구성해도 총 $809(드라이브 각 $100, NAS $209), 매우 조용해 거실 TV에 연결해도 문제 없고, 소비전력도 10W 미만, 큰 시끄러운 서버를 둘 공간이 없는 환경에 최적
     * mini-NAS는 완전히 새로운 디자인 목표의 제품군으로 봐야 하는지 고민, 데스크탑 수준(6TB)의 저장공간을 상시 네트워크에서 쓸 수 있고 데스크탑보다 전기 덜 먹는 게 핵심 장점인지 궁금, 개인적으로 의미 있겠지만 이전에 80TB짜리 NAS와는 확연히 다른 느낌, SSD의 아이들 전력소모도 궁금, 램 캐시 때문에 완전 0에는 못 미치는지 추정
          + 소형·휴대·저전력 SSD NAS는 2016년부터 상용화 됐다는 시장 배경 안내, 일부 커뮤니티에서는 NASbook이라 부르기도 했지만 큰 유행은 아니었음, QNAP(TBS-464, TBS-H574TX), ASUSTOR 관련 링크(https://www.qnap.com/en/product/tbs-464, https://www.qnap.com/en/product/tbs-h574tx, https://www.asustor.com/en/product?p_id=80) 제공
          + APSD 기능 사용 시 SSD의 아이들 전력은 수십 mW 수준에서 충분히 낮다는 정보 제공
          + HDD 기반 NAS도 저장용량에 따라 4TB~수백 TB까지 활용, SSD NAS는 NVMe 용량 상 한계가 있지만 공간, 소음, 경제성 측면에서 장점, 단위 TB당 가격은 더 비싸다는 점 정리
          + mini-NAS의 새로운 쓰임으로서 LLM 및 에이전트 용도의 개인 컨텍스트 캐시, 다른 OS에서 지원외 된 기능의 사이드카 서버, 모바일/태블릿과 콜드 NAS 사이의 웜 스토리지 용도 제시
          + 저전력 기대에 비해 실제로는 미니 NAS가 그만큼 저전력은 아니라는 지적, 벽전력 측정 기준, NUC 12는 모니터, 유선랜 없이 Wi-Fi 단독으로 4W 미만, 기사에 나온 미니 NAS 3종은 아이들도 최소 2배 이상 소비, 저전력 최적화 부족 가능성
     * N100 시리즈가 다양한 저전력 소형 x86 PC의 적용에 기여하고 있다는 점에 팬심, 이런 리뷰 좋아하는 사용자 입장, 특히 Beelink가 USB-C PD로 동작하는지 궁금, 평소에는 불가능할 것 같지만 확인 희망
          + 실제로 테스트하진 않았지만, 스펙이나 문서에 USB-C PD가 명시되지 않아 지원하지 않는다고 봄
     * 쿼드 NVMe mini-NAS를 팬리스로 직접 돌려본 경험, ECC 지원은 포기해야 하지만 이 조건이면 나쁘지 않은 트레이드오프, 다이아트하면 팬리스 운영도 가능, 소비자용 NVMe라면 RAIDz보다는 미러드 구성이 수명 보호에 더 좋다고 조언, ZFS의 쓰기 증폭 이슈로 인해 드라이브 수명이 빨리 닳는 경험
          + TrueNAS/FreeNAS 및 ECC 사용자의 입장, ZFS 구성에 ECC 필수라는 기존 상식, 이것이 지금도 유효한지 질문
     * ZFS 사용 시 신뢰할 만한 SSD 선택 고민, PLP(정전 보호), 쓰기 수명/DWPD, QLC가 아닌지, ZFS 버그 호환성 등 다양한 조건 고려, $100/TB 이하에서는 거의 찾기 힘들고 datacenter SSD는 두 배 이상 비쌈, 저렴한 조합에서는 어려움
          + ZFS가 특별히 더 큰 영향을 받는 게 아니라, ZFS는 진단·경고가 뛰어나서 집 NAS에서도 쓰기 수명 문제 체감할 일은 별로 없을 거라고 생각
          + QLC SSD도 일반적인 가정용 NAS 용도에서는 충분, 실질적으로 매일 160GB를 꾸준히 쓰는 사용자는 거의 없지 않냐는 현실적 시각
     * 최신 mini NAS에도 ECC 램 옵션이 있는지 궁금, 본인은 과거 엔트리급 ECC 미지원이 구축의 제한요소였음
          + Minisforum N5 Pro Nas는 ECC 램 최대 96GB까지 지원, 가격 및 제품 링크(https://www.minisforum.com/pages/n5_pro, https://store.minisforum.com/en-de/products/minisforum-n5-n5-pro-eu) 안내, 독일 기준 96GB DDR5 SO-DIMM은 200~280유로, 128GB 키트도 CPU 스펙상 지원 가능성(https://amd.com/en/products/…), USD 환산 어려움
          + Aoostar WTR max는 5개 NVMe, 6개 HDD, ECC 램 128GB까지 지원, 가격은 $700부터 시작해 기사에 나온 mini NAS보다 비쌈
          + 저렴하진 않지만 ECC 지원 mini NAS로 ASUSTOR 모델 추천(https://www.asustor.com/en/product?p_id=89)
          + ARM 기반 모델 중에서도 ECC 지원 기기가 있었던 것 같으나 정확한 모델명이 기억나지 않음, Banana Pi나 LattePanda 쪽 유니버스에서 찾아볼만
          + HP Microserver도 해당
     * 이 작은 NAS들이 귀엽다고 생각, 하지만 ‘진정한’ 하이엔드 미니 NAS 라인업을 보고 싶음, Ryzen 7745, 128GB ECC DDR5-5200, 최소 듀얼 10GbE 포트, NVMe RAID 컨트롤러 2개, 랙 대신 커피잔 크기 큐브로 랙급 성능을 원하는 욕심, 소음, 비용, 전력 모두 상관없이 바라는 미래형 폼팩터, 오늘날(2025)이면 가능해 보임
          + 현실적으로는 Asustor Flashstor FS6812X(https://www.asustor.com/en-gb/product?p_id=91)가 가장 근접, 완전 큐브는 아니나 상당히 소형, 본인도 책상 아래에 두고 쓰는데 존재감이 거의 없을 정도, 먼저 4TB NVMe 4개로 채워 쾌적하게 운영 중
          + minisforum 기기들이 거의 유일하게 해당 사양대에 근접, 하지만 여전히 대부분은 ECC를 불필요하다 생각해 선택지가 제한되는 분위기
     * geerlingguy의 리뷰는 항상 실망시키지 않는다는 칭찬, 이런 조합이 굉장히 매력적이라고 느끼며, homelab 사용자에게 실용적, 취미자에게 흥미, 전문가에겐 현실성까지 두루 갖춤, 저전력 CPU와 NVMe 결합이 edge computing이나 로컬 AI 워크로드에 딱 어울리는 느낌, 다시 집에서 만들어가는 미래 가능성
     * 관련 질문: usb-c로 충/방전 동시 지원, 즉 실질적인 UPS처럼 쓸 수 있는 powerbank 아는지 질의, 대다수 usb-c 파워뱅크는 가장 강력한 포트를 충전용으로만 쓰게 해 DC UPS로는 불가, 본인도 이제 집 서버를 framework 메인보드에 Proxmox VE, 64GB 램, 4TB NVMe 조합으로 돌리고 있고, USB-C 전원, 아이들시 소비전력 2W 수준의 환경에서 그런 UPS가 필요함 언급
"
"https://news.hada.io/topic?id=21731","ECMAScript 2025가 공식 표준으로 승인됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ECMAScript 2025가 공식 표준으로 승인됨

     * ECMAScript 2025가 2025년 6월 25일 공식 표준으로 승인되어 다양한 새로운 기능이 추가됨
     * Import attributes와 JSON 모듈을 통해 비 JavaScript 리소스의 가져오기 방식 개선
     * Iterator helper methods로 대용량 데이터 처리 및 다양한 반복 작업의 효율성 향상
     * Set과 RegExp 등 주요 내장 객체에 새로운 메서드가 대거 추가되어 사용성 강화
     * 16비트 부동소수점(float16) 지원 등 데이터 처리 성능 및 표현력 확장
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ECMAScript 2025의 공식 승인

     * 2025년 6월 25일, Ecma International 129차 총회에서 ECMAScript 2025가 공식 표준으로 승인됨

새로운 주요 기능

  Import attributes와 JSON 모듈

     * Import attributes는 비 JavaScript 아티팩트(import 대상 리소스)에 대해 타입 등 속성을 명시할 수 있는 문법임
     * JSON 모듈을 처음으로 공식 지원, import configData from './data.json' with { type: 'json' }; 형태로 간편하게 JSON 데이터 불러오기 가능함
     * 동적 import(import() 호출)에서도 동일하게 속성 지정 가능함

  Iterator helper methods

     * Array 메서드와 유사하지만 iterator에 특화된 다양한 헬퍼 메서드가 도입됨
          + filter, map, flatMap, some, every, find, reduce, forEach 등 기존 메서드
          + drop, take, toArray 등 반복자 전용 신규 메서드 포함
     * Set, Map 등 모든 iterable 객체에서 직접 활용 가능
     * 중간 배열 생성을 하지 않고 데이터가 필요한 만큼만 순차적으로 처리하여 대용량 데이터에 효과적임

  Set 메서드 확장

     * 합집합, 교집합, 차집합, 대칭차집합(symmetric difference) 연산 메서드 추가
          + Set.prototype.union(other), intersection(other), difference(other), symmetricDifference(other)
     * 부분집합/** 상위집합**/** 서로소** 판별 메서드 제공
          + isSubsetOf(other), isSupersetOf(other), isDisjointFrom(other)

  정규표현식 개선

    RegExp.escape()

     * 임의의 문자열을 정규표현식에서 안전하게 사용할 수 있도록 자동 이스케이프하는 RegExp.escape() 도입

    패턴 내 인라인 플래그(Inline flags)

     * 정규표현식 내 일부 패턴에만 개별 플래그(i, u 등) 적용이 가능해짐
          + 예: /^x(?i:HELLO)x$/는 HELLO 부분만 대소문자 구분 없이 매칭

    중복 이름 캡처 그룹 지원

     * 정규표현식 대안(branch)별로 동일한 이름의 캡처 그룹 사용 가능
          + 예: /(?<chars>a+)|(?<chars>b+)/v

  Promise.try()

     * 동기/비동기 코드의 혼합 처리를 단일 Promise 체인으로 시작할 수 있는 Promise.try() 도입
          + 기존에는 동기 예외 처리가 번거로웠으나, now 간결하게 가능함

  16비트 부동소수점(float16) 지원

     * Math.f16round()로 16비트 부동소수점 반올림 처리 가능
     * TypedArray에 Float16Array 타입 추가
     * DataView에 getFloat16(), setFloat16() 메서드 도입

참고 자료 및 무료 도서

     * 무료 온라인 도서 “Exploring JavaScript (ES2025 Edition)” 제공
          + JavaScript의 역사 및 발전, 새 ECMAScript 기능 등 자세히 설명
          + 주요 신기능 별로 실제 사용 예시와 설명 제공

   저번플젝에서 json 불러오기가 왜 안되는지 몰랏는데
   원래 지원을 못하던거군요.. ㄷㄷ
"
"https://news.hada.io/topic?id=21760","Show GN: AI 대화형 시뮬레이션 게임 "All My Universe"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Show GN: AI 대화형 시뮬레이션 게임 ""All My Universe""

   안녕하세요. 지난 6개월간 퇴근 이후에 바이브 코딩의 도움을 받아 열심히 아이를 위한 게임을 사부작 사부작 만들고 있습니다.

   우선 간단히 제가 만들고 있는 게임을 소개해드릴께요.

   제가 만들고 있는 게임은 AI가 만든 캐릭터와 대화를 하며 최대한 많은 친구들을 만들고 성장하는 것을 목표로 하는 AI 챗 기반의 시뮬레이션 게임입니다. 간단하게 AI 챗 게임이라고 할 수 있어요. ^^;

   왜 이런 형식의 게임을 만들게 되었냐면 이제 막 초등학생이 된 제 아이가 3~4살 무렵부터 몬스터 캐릭터들을 너무 좋아해서 몬스터 그림들을 마구마구 그리기 시작했는데.. 사실 부모 입장에서 아이가 그린 소중한 그림들을 그냥 버리긴 아까죠.. 그래서 차곡차곡 모아둔게 어느덧 수천장에 달하고 있습니다..

   하지만 가지고 있어선 의미가 없겠죠. 항상 이걸 디지털화 시켜서 나중에 크면 보여줘야지 하다가 마침 AI가 눈부시게 발전하는 사태(?)를 보고 AI를 활용해서 게임을 만들게 되었습니다.

   아직까지 여러가지 수정과 테스트 작업을 계속 하고 있고, 수없이 발생하는 사이드 이펙트와 싸고 있지만.. 그래도 한번쯤은 많은 분들의 솔직한 평가를 받아보고 싶단 생각이 드네요.

   아직까진 웹으로만 이용할 수 있고, 베타 버전이라 제한된 기능만 이용할 수 있습니다. 또 회원가입이 필요한데.. 서버가 너무 작고 귀여워서.. 터져버릴까봐 최대 100명으로만 제한해 둔 상태입닌다. 혹시 AI 챗 게임에 관심이 있으시면 한번쯤 둘러봐주시고 솔직한 평가도 내려주셨으면 좋겠습니다. 물론 평가와는 상관없이 개발과 캐릭터 추가는 앞에서 모아둔 수천장의 캐릭터를 모두 캐릭터화 시켜서 등록하는 날까지 쭈욱 계속할 생각입니다.. ^^;;

   마지막으로.. 개발자의 욕심 때문에 자꾸만 세계관이 점점 확장되고 있어서.. 조만간 새로운 유니버스도 추가하려고 하고 있습니다; 현재의 몬스터 캐릭터 유니버스와는 다른 조금 더 현실적인 세계관을 가진 유니버스를 준비 중이에요. 혹시 게임에 흥미가 생기셨다면 새로운 유니버스도 기대ㅐ주세요.

   서비스에 대해 간단히 소개해드리겠습니다.
     * AI 챗 기반의 몬스터 캐릭터 대화형 시뮬레이션 게임
     * 챗 대화는 현재 gemini flash와 openai의 4o mini로 구현
     * 현재는 하루 최대 20회로 대화가 제한되어 있고, 최대 대화 가능 캐릭터는 10명으로 제한
     * 캐릭터와 기능은 꾸준히 업데이트 할 예정
     * 개발 중인 기능으로는

    1. 개인적으로 운영 중인 워드프레스 사이트의 글과 연동하여 지식 기반의 대화 기능 준비 중 (해당 사이트는 미미한 방문자수를 기록하고 있지만.. 제가 언젠가 아이에게 가르쳐 주고 싶은 지식을 차곡차곡 모아둔 지식 저장소같은 역할을 하고 있음. 물론 현재는 조금 다른 목적으로도 활용 중입니다..)
    2. 아이가 언젠가 사람들과 영어로만 대화를 해보고 싶다는 말에 대화를 실시간으로 번역해주는 기능도 준비 중입니다. (물론 아이는 아직 영어를 못합니다. 그래서 게임 내에서 자연스럽게 영어를 접하게 할 수 있게끔 하려고 치열한 고민을 하는 중 입니다..)

     * 이상이며, 구구절절 써놓았지만 사실 특별한 점은 없음. 그래도 궁금하신 분들은 한번 봐주시고, 솔직한 평가도 부탁드립니다.

   다른건 모르겠고 일단 드는 생각은
   저도 이런 어머니가 되고 싶다는 것입니다.

   감사합니다~! 일단 좋은 아버지가 되는 건 성공한 것 같네요^^

   저도 좋은 아버지가 되고 싶다는 같은 생각입니다.
   게임을 개인적으로 즐기지 않지만 애들이 좋아하고 즐겨하긴 합니다. 현재 이미 고등학생이라 게임도 간단한 캐주얼 게임을 즐기지 않고 대작들만 즐기다보니 저는 애들을 위해 게임을 만들어줄 시기를 놓친 것 같네요.ㅠㅠ

   추가로 애들이 고딩되기 전에 어릴 때 한번 아빠를 신기하게 생각하도록 느끼도록
   이런 게임을 만들어본 적은 있습니다.

   => https://m.onestore.co.kr/ko-kr/apps/appsDetail.omp?prodId=0000761386

   그당시에 AI가 흥했었다면ㅠㅠㅠ

   ㅎㅎ 아빠들의 마음은 다 비슷한가봐요~! 다행히 제 아이는 아직 어려서 기회가 있을 듯 합니다. 감사합니다~!^^
"
"https://news.hada.io/topic?id=21791","민간부문, 10만 증가 예상에 반해 3.3만 일자리 감소","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    민간부문, 10만 증가 예상에 반해 3.3만 일자리 감소

     * 미국 민간부문에서 기대와 달리 3.3만 개 일자리 감소 현상 발생
     * 전문가들은 10만 개 일자리 증가를 예상한 반면 실제로는 감소로 나타남
     * 고용 감소 현상은 경기 둔화 또는 시장 불확실성 신호로 해석 가능함
     * 관련 업계 및 경제 전문가들은 노동시장 전망에 대한 경계심을 드러내는 상황 임
     * 이번 결과는 향후 정책 방향과 투자 전략에 중대한 참고 자료로 작용함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 2025년 6월 ADP 고용보고서 요약

  고용 동향 현황

     * 2025년 6월 ADP 고용보고서에 따르면, 미국 민간부문은 3만 3천 개의 일자리 감소 현상 발생
     * 일반적으로 10만 개 일자리 증가가 예상되어 있었으나, 실제 데이터는 예상을 크게 밑도는 수치임
     * 이러한 일자리 감소 현상은 최근 경제 성장 둔화 및 기업들의 신규 채용 보수적 접근 가능성을 암시함

  전문가 및 시장 반응

     * 노동시장에 대한 부정적 시그널로 해석되는 가운데, 투자자와 기업 모두 시장 변동성 확대를 주의 깊게 관찰 중임
     * 일부 경제 전문가는 노동 수요 감소가 향후 경제 전반에 추가적인 영향을 미칠 수 있음을 경고함

  정책 및 전망

     * 이번 보고서 결과는 미국 연방준비제도(Fed) 의 정책 결정, 금리 동향, 정부의 경기부양 대책 등 다양한 정책적 방향에 참고 자료로 활용됨
     * 향후 경제지표 및 노동시장 데이터에 따라 정책·투자 전략 재조정 필요성이 커질 전망임

  참고

     * 해당 보고서는 민간부문 고용 상황을 빠르게 파악할 수 있는 주요 자료로, 기업 인력 운용 및 투자 판단에 유용함
     * 기존 전망치와 실제 데이터의 괴리는 시장의 불확실성을 높이고 있으며, 새롭게 등장하는 노동시장 리스크를 재평가하는 계기임

        Hacker News 의견

     * 경제 건강을 측정하는 더 나은 방법이 필요하다고 생각함. 내가 연봉 6자리 직업을 잃고 바로 Applebee's에서 최저임금 서버로 취직해도 ‘실업률’은 그대로라는 점을 지적하고 싶음. 실제로 직장을 찾지 않는 사람들은 아예 포함조차 되지 않음. ‘완전 고용’이란 말 자체도 사람들이 진짜로 안정적인 삶을 꾸려갈 수 있느냐, 아니면 생활비 때문에 두 개의 일을 동시에 해야 하느냐에 따라 의미가 달라짐. 안정적인 생활을 할 수 있는 고용은 사람들이 다양한 비필수 재화를 구매해서 경제 여러 부문을 활성화할 수 있게 하지만, 그렇지 않은 고용은 그런 여유 자체가 없음
          + 다양한 경제 지표가 이미 존재하고 있음. 다만 언론이 항상 한 가지 지표만 보도하기 때문에 전체적인 그림을 보려면 여러 지표를 동시에 살펴야 함. 예를 들어 가구별 중위소득이나 가처분 중위소득, 1인당 GDP만 봐도 위에서 제시한 상황을 파악할 수 있음. 노동참여율도 실업률과 함께 보면 의미 있음. 특히 연령대별로 살피면 더 좋음. 20~60세의 참여율이 낮으면 좋지 않고, 60세 이상 참여율이 너무 높아도 문제로 볼 수 있음. 문제는 지표가 없는 게 아니고, 대중 토론이 동기, 이해, 인센티브 모두 부족함. 심지어 ‘평균’과 ‘중위수’ 차이를 이해하지 못하는 사람들이 많아서 이런 논의가 나아질 거란 기대가 없음
          + 실업률에 고임금 일자리에서 최저임금 일자리로 옮겨도 지표상 차이가 없다는 점을 지적한 사례가 있는데, 이는 ‘U-6’ 실업률 지표로 포착 가능함. 다만 이런 케이스는 대규모로 보면 노이즈에 불과하다고 생각함. 공식 실업률로 통용되는 ‘U-3’가 핵심 신호이고, 나머지 지표 변화도 결국 U-3 변화가 스케일된 것임. 사실 불만의 본질은 고용이 아니라 ‘임금’임. 임금 지표도 BLS가 따로 집계·보고하고 있으니 그것이 논의에 더 적합함
          + 실업 관련해서 다양한 지표가 이미 존재함. 아직 ADP 같은 다른 신호도 논하지 않았음. ‘더 나은 방법이 필요하다’는 말은 사실 ‘관련 자료를 더 읽어야겠다’는 뜻에 가깝다고 생각함. 이 분야는 이미 매우 잘 연구되고 이해된 영역임
          + BLS에서 업종별 고용 변화도 상세하게 발표함. 투자자들도 이 데이터를 중요하게 살핌
          + 기사에서 다루는 건 ADP임. ADP 지표가 이후 정부의 공식 고용보고서(BLS)와 그리 정확히 맞지 않는 경우가 많음. 투자자들은 보통 정부 고용보고서를 더 중시함. BLS도 여러 방식의 지표를 집계함. 관련 링크는 여기에서 확인 가능함. BLS 공식 뉴스 릴리스엔 더 자세한 내용이 나옴
     * 이번 ADP 실제 리포트 원본 링크는 이곳에서 볼 수 있음. 제조업 등 재화 생산 분야에서는 32,000명 순고용 증가, 서비스 분야에서는 66,000명 감소(전문·비즈니스 서비스 -56,000, 교육·헬스 -52,000 중심)임. 지역적으로는 North Central Midwest -28,000, South Atlantic -21,000, Mountain -20,000 감소. 1~50명, 250~499명 고용 기업에서 해고가 있었으나, 중견·대기업은 오히려 순고용 증가. 연간 임금 성장률은 기존직 직원은 4.4%, 이직자는 6.8%로 약간 하락. 금융업 임금성장률이 +5.2%로 가장 높고, 정보서비스가 +4.1%로 가장 낮음. 지역 데이터 맵은 여기에서 참고 가능함
     * ADP 리포트의 정부 고용보고서 예측력은 그리 높지 않다는 점이 중요함
          + ADP와 BLS 지표는 측정 대상이 서로 다름. ADP는 민간 일자리에 오버핏, BLS는 정부 일자리에 오버핏하는 식임. 보통 ADP > BLS면 경기가 좋다는 간단한 판단법도 존재함
          + 올해 들어 정부 공식 고용보고서가 발표 이후 많게는 35%까지 하향 수정되기도 했음. 예시로 3월 비농업 신규채용자는 +185,000에서 +120,000으로 65,000 하향, 4월도 30,000 하향 조정됨. 상황에 따라 월별 숫자가 추가 보고 및 시즌 조정 등으로 뒤늦게 정확하게 맞춰짐. 자세한 내용은 BLS 리포트 참고 가능함
          + 최근 정부 발표 고용지표 신뢰도가 크게 떨어졌다는 판단임. 발표일에는 화려하게 보도되지만, 그 뒤 조용하게 하향조정 되는 일이 반복임. 이런 식으로라도 실제 현실치에 맞추지 않을 수 없어서 결국 조정이 필요함
     * 나도 곧 직장을 잃을 예정임. 실업시장이 매우 암울하다고 생각함
          + 너무 낙담하지 말라는 조언임. 25년 전 닷컴 실패 직전 대학원 그만두고 나왔을 때도 비슷하게 시장이 혼란스러웠음. 새로운 기술이 크게 등장하면 잠깐 시장이 뒤죽박죽되지만, 결국 더 흥미로운 일자리 시대로 이어지는 경우를 직접 경험함
     * “재화 생산 일자리 +32,000, 서비스 일자리 -66,000”이라는 뉴스 보도에 대해 이것이 관세 정책의 효과인지 의문임
          + 만약 목표가 ‘더 많고 나은 일자리’라면, 관세는 실패임. 하지만 노동시장에서 사용자의 협상력을 높이고 노동자의 협상력을 낮추는 데 목적이 있다면, 관세 등 정책이 제대로 작동하는 셈임. 고임금·고숙련 근로자를 상대적으로 낮은 임금/숙련도가 요구되는 제조업으로 옮기려는 효과임(즉 예전 소프트웨어 엔지니어가 IT 관리자처럼 낮은 임금 일자리를 취득하는 사례를 상상). Medicaid 혜택 축소 및 근로요건 강화도 저임금 노동력 증가→임금 하락 유도. 이는 사용자 입장에서는 이득이지만, 노동자 모두엔 손실임
          + 노동을 낮은 생산성과 보수의 산업으로 이동시키는 정책이라면 관세는 그런 정책임
     * 웰스파고 자료 원본은 여기 그리고 아카이브에서 볼 수 있음
          + 이 링크들이 피싱같아 보이지만 실제로는 정상임을 지적함
     * ADP 급여 보고서는 그 자체로 ‘노이즈’임. ADP를 사용하는 회사만의 데이터라서, 이를 경제 전반 대표로 간주하는 것은 오산임. 예를 들어 ADP 이용 기업이 해고를 단행, 동시에 같은 규모로 신규 창업 회사들이 타 사 급여 시스템을 사용한다면, ADP 데이터에서는 ‘일자리 감소’로 파악됨. 경제 전체의 민간 부문 일자리 상황을 파악하는 데는 신뢰하기 어려움
          + ADP는 미국 전체 회사 중 6곳 중 1곳의 급여를 담당함. 표본 수 자체는 규모가 크고, 전반적으로 경제를 대변할 수 있음. 일부 업종의 과대/과소 반영 문제는 있지만, 보고서 자체를 통째로 무시해야 할 사유는 안 됨
          + 노이즈란 신호의 부재임. ADP는 편향된 신호를 줄 수 있지만, 그 편향만 보정하면 됨. ADP가 다양한 분야를 커버하고 있어서, 오히려 ADP 데이터와 상반된 고용렝드는 극히 희박함. 대표성 낮은 기업들이 폭발적으로 채용 중이라는 증거는 없음
          + ADP 보고서에서 ‘ADP 고객이 전체 경제 대표’란 가정이 들어간 건 아님. ADP 데이터는 BLS 예측에도 활용 가능하지만, 그 자체로 독립적으로 충분히 유용함. 보도 헤드라인에서 ‘민간 부문 33,000개 일자리 감소’라 쓰는 것은, 실제로 그만큼 ADP를 통해 고용이 줄었기 때문임. ADP 공식 리포트 참고 가능함
          + 샘플 대표성이 거의 무작위에 가깝다면, 오히려 여론조사 데이터보다도 믿을 만함
          + ADP와 BLS 지표가 종종 엇갈리는 경우가 많았고, 이는 두 기관의 수집 방식 차이에서 비롯됨. 그러나 이럴 때 BLS 조사 결과를 맹신하는 것도 별 의미 없다고 생각함
     * 정부가 관세로 소비자를 힘들게 만들면 앞으로 더 안 좋은 소식만 생길 것이라는 생각임
     * 나도 참고용 현장 데이터 하나 제공함. 내가 다니는 150명 규모 중소기업(부동산 관련)에서 최근 25명 감원 단행함. 주택시장 전망이 어둡다는 판단 때문임
     * 현 정부가 이 나라를 더 힘들게 만들 것이란 우려 있음
"
"https://news.hada.io/topic?id=21771","Cloudflare, AI 봇을 위한 크롤링별 과금(pay-per-crawl) 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Cloudflare, AI 봇을 위한 크롤링별 과금(pay-per-crawl) 도입

     * AI 크롤러에 대해 콘텐츠 소유자가 접근 시 과금할 수 있는 새로운 서비스를 발표
     * 콘텐츠 제작자는 무료 허용, 완전 차단, 또는 유료 접근 허용의 세 가지 선택지를 가질 수 있음
     * 과금은 표준 HTTP 402 Payment Required 응답 기반으로 동작하며, Cloudflare가 결제 및 정산 인프라를 제공함
     * 크롤러와 콘텐츠 소유자는 프로그램 방식 인증 및 결제를 통해 투명성 및 보안을 확보함
     * 해당 서비스는 프라이빗 베타로 시작하며, 미래에는 다양한 사용 사례와 동적 과금 지원 확장 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 소비 환경의 변화

     * 많은 게시자, 콘텐츠 제작자, 웹사이트 소유자는 지금까지 AI 크롤러에 대해 모든 접근을 허용하거나 전면 차단하는 이분법적 선택만 가능했음
     * Cloudflare는 콘텐츠 소유자의 통제권 강화를 기본 원칙으로, AI 크롤러를 모두 차단하거나 무료로 허용할 수도 있고, 원하는 크롤러만 선택적으로 접근을 허용할 수도 있는 환경을 추구함
     * 수백 개의 미디어 기업, 퍼블리셔, 대형 소셜 플랫폼 등과의 논의를 통해, AI 크롤러에게 접근을 허용하되 보상을 받고자 하는 니즈가 큼을 확인함
     * 기존에는 크롤러와 개별 협상 후 접근을 허용해야 했으나, 규모와 협상력 부족으로 소규모 사이트엔 실질적으로 불가능했음

'크롤링별 과금(pay per crawl)' 모델의 도입

     * 무료 또는 차단이라는 극단적 선택 외에, Cloudflare는 접근 과금이라는 3번째 옵션을 제안함
     * 이를 활용해 콘텐츠 소유자는 인터넷 규모에서 디지털 자산 수익화가 가능해짐
     * 핵심 기술적 기반은 거의 잊혀진 HTTP 402 Payment Required 상태코드로, 크롤러가 결제 의사를 밝힐 경우 200 OK로 콘텐츠 제공, 그렇지 않으면 402 응답으로 가격을 통지함
     * Cloudflare는 Merchant of Record 역할로 결제 처리와 기술 인프라를 책임짐

퍼블리셔의 조정 권한과 가격 정책

     * 사이트 전체에 대한 고정 단가를 퍼블리셔가 설정 가능함
          + 허용(무료 제공)
          + 과금(설정된 가격에 결제 시 제공)
          + 차단(접근 불가 및 향후 관계 가능성 안내)
     * 크롤러 별로 과금 예외, 무료 접근 허용, 개별 협상 등 융통성 있는 정책 운용 지원
     * Cloudflare의 보안 정책(WAF, 봇 관리 등) 과 연동, 기존 방화벽을 우선 적용 후 pay per crawl 룰이 작동함

인증 및 결제 프로토콜: Web Bot Auth

     * 특정 크롤러만 결제 및 접근 권한을 받도록 설계되어, 크롤러 ID 조작(스푸핑) 방지
     * 크롤러는 Ed25519 키 페어 생성, JWK 형식 퍼블릭키 디렉토리 등록, Cloudflare에 정보 제출 필요
     * HTTP 메시지 서명(Signature-Agent, Signature-Input, Signature) 헤더로 신원 및 결제 의사 증명

유료 콘텐츠 접근 방식

     * 두 가지 프로토콜 지원
          + 반응형(발견 우선): 크롤러가 유료 URL 요청 시 402 응답과 가격 제시 → 동의 시 재요청에서 'crawler-exact-price' 헤더 포함
          + 선제적(의도 우선): 최초 요청에 'crawler-max-price' 헤더로 결제 상한 전달 → 기준액 이하면 200 OK와 과금 내역 전달
     * 가격 협상 및 결제 의사 표현이 프로그래머블 방식으로 지원됨

정산 및 재무 처리

     * 크롤러 운영자와 콘텐츠 소유자는 Cloudflare 계정에 결제 정보 등록 필요
     * 인증된 결제 요청 및 200 OK 응답마다 과금 이벤트 기록, Cloudflare가 집계 후 크롤러에 청구 및 퍼블리셔에 정산

에이전트 시대를 향한 변화의 시작

     * pay per crawl은 온라인 콘텐츠 통제 기술의 진화를 상징함
     * 프로그래머블 자산 가치 산정 방식을 통한 창작자 보호와 신규 BM(비즈니스모델) 확장 기대
     * 향후에는 콘텐츠 유형/경로별 동적 과금, AI 트래픽 규모 기반 요율 산정, 대규모 라이선스 관리 등 다양한 확장성 논의 중
     * HTTP 402 채택으로, AI/에이전트들이 디지털 자원 접근 조건을 프로그래머틱하게 협상하는 미래에 대응 가능

시작 안내

     * pay per crawl 기능은 프라이빗 베타로 우선 제공 중
     * 사용을 희망하는 크롤러 및 콘텐츠 제작자는 별도 신청 링크를 통해 문의 가능
     * Cloudflare는 DDoS 방어, 인터넷 접속 가속화, 인터넷 애플리케이션 구축, 해커 차단, Zero Trust 구현 등도 지원

관련 추가 정보 및 동향

     * 'Content Independence Day' 등 크롤러의 무상 크롤링 차단 움직임과 Cloudflare의 로봇 관리 솔루션 확대 중
     * 2024~2025년 사이 AI 크롤러 트래픽 18% 증가, GPTBot 305%, Googlebot 96% 성장 수치 발표
     * 공개키 기반 메시지 서명을 통한 크롤러 인증 강화 기능도 동시 도입

   번역: https://rosettalens.com/s/ko/introducing-pay-per-crawl

   ai 봇인지 아닌지 뭘로 구분할까요

   ai 인지 아닌지를 떠나서 크롤링도 작정하면 못잡아내는대

   클라우드플레어가 돈벌겠다 라는거고
   결국 돈벌려는 사람들은 다 저기로 모일것이고
   클라우드플레어의 볼륨이 커질 것이고
   데이터는 질적 하락을 겪을 것임
   네이버도 인스타도 유튜브도 돈이 걸린 곳은 다 같은 경향성을 보인 부분임

        Hacker News 의견

     * 나는 이것이 우리가 원하던 마이크로 결제의 형태라고 생각함. Coinbase가 최근 암호화폐와 402 상태 코드를 이용한 라이브러리를 선보였는데, 그게 x402임 x402 GitHub 링크
          + 웹 비즈니스 모델이 광고 중개인 대신 이런 방식이어야 한다고 생각함. 광고기술 업계는 우리의 데이터를 영구적으로 착취하고, 미디어를 오염시켜 왔으며, 민주주의에도 해를 끼쳤다는 사실임. 수십 년 후에는 지금의 광고모델이 얼마나 해로운지 깨닫고, Big Tobacco(담배 거대기업) 규제하듯이 관련 회사를 규제·처벌할 수 있길 기대함. Brave의 BAT도 좋은 시도이지만, x402가 더 보편적 해법 느낌임. 다만 암호화폐에 대한 부정적인 인식, 그리고 광고업계의 강력한 영향력 때문에 이런 방식이 자리 잡기는 쉽지 않아 보임
          + 나는 암호화폐를 사용하고 싶지 않고, 암호화폐로 콘텐츠를 받고 싶지 않고, 중개인에게 수수료도 내고 싶지 않음. 마이크로 결제에 암호화폐를 쓰는 건 실제론 암호화폐 생태계를 띄워주기 위한 수단으로 느껴짐. 더불어 누구나 어떤 코인으로 결제할지 합의가 어려움. 각 사이트마다 각기 다른 코인만 받는 상황, 또는 단일 Stablecoin만 받아야 하는 상황이 나오면 통제도 어려워짐. Cloudflare의 방식이 더 나음. 이 방식은 수익을 내는 쪽에서 돈을 부담하고, 일반 사용자에게 돈을 요구하지 않음. 그리고 암호화폐도 안 쓰임
          + 내가 틀렸으면 좋겠지만, 마이크로 결제에 관한 기회를 이미 놓친 느낌임. 예전 피처폰의 선불 충전 방식처럼 “인터넷에 만 원 충전” 해두고, 사이트가 그걸 마이크로 결제로 차감하는 구조였다면 제대로 정착했을 것 같음. 하지만 지금은 이 시스템을 구현하고 시장 전체가 동참하려면 너무 많은 인프라와 합의가 필요해져서 시기를 놓친 것 같음
          + 이 기술이 정말 흥미롭게 느껴짐. 내가 정확히 이해했다면, 프로토콜이 주소와 결제 금액을 전달했을 때 바로 응답하도록 설계되지 않은 점이 궁금함. 이후 시도들이 금액과 지갑 주소의 체크섬이 반환될 때까지 막히고, 이 체크섬을 써드파티가 검증하도록 하면, 각각의 서버가 검증 로직을 구현하지 않아도 됨. 디지털 경제를 제대로 만들려면 두 가지가 필요함: 1) 콘텐츠가 요청자만 소비하고 복제/저장이 불가해야 함, 2) 콘텐츠에는 사람이 부여한 평판이나 등급 체계가 있어야 함. 첫 번째는 DRM이나 동형 암호로, 두 번째는 DAO화된 평가기관이 해결할 수 있을 것임. 평가자로 DAO에 참여하려면 블록체인 기반 실적 증명을 공유하고, 비싼 암호화폐를 예치(=라이선스)하게 해서 신뢰를 높일 수 있음. BitTorrent 인덱스처럼 콘텐츠와 평가자를 연결할 수 있어
            중개 광고도 없앨 수 있음. 이 구조가 완성되면 전문성이 있는 사람들이 가치 창출에 참여하면서 중개자를 배제할 수 있고, 인간이 콘텐츠 경제의 중심이 될 수 있음. 콘텐츠의 이중 지불 문제를 풀면, 사람마다 지속적으로 보상을 받으면서 오프라인 전문성도 발전시킬 수 있음. 예를 들어 좋은 책이나 영화를 찾을 때 Amazon 혹은 Goodreads 평점을 참고하는데, 현재 평점 제공자는 그만큼의 이해관계가 없음. 그렇게 평점 콘텐츠가 평가 기관처럼 공신력이 생기면, 개별 작품의 가치도 더 높아질 수 있음. 모두가 스스로 명성을 보존하게끔 동기 부여됨
          + 이 개념 자체가 새로운 건 아님. 나도 2018년에 ln-paywall이라는, 402 상태 코드를 활용한 프로젝트를 만든 적 있음
     * 나는 이 방식이 전혀 잘못된 접근이라고 봄. Cloudflare는 단순히 “예전처럼 크롤링하되, 이제는 돈을 더 내야 함”이라고만 하고, 그에 합당한 가치를 제공하지 않음. 크롤링은 AI 회사나 새로운 검색엔진에 경쟁 우위가 아님. 비용만 들고 주목을 분산시키는 작업임. 이들은 인프라를 공유해서 협력하는 게 합리적임. 각 회사가 따로 사이트에 접근할 필요 없이, 하나의 크롤러만 모든 사이트를 방문하고, 회사들은 자신만의 필터를 설정해 일치하는 URL에 비례해서 기여하는 구조가 이상적임. 여러 번의 트래픽 대신, 단일 크롤러를 두고, robots.txt 규정도 기술적·계약적으로 강제하면, 원하지 않는 콘텐츠는 제공되지 않고, 그걸 우회하려면 본인들이 자체 크롤러를 기술적으로 유지해야 하니 상당히 부담임. 여기에 결제를 추가한다 해도, 미리 본 적도 없는
       무수한 쓰레기성 페이지에 결제하게 될 가능성이 높아 가치가 없음. 이 방안이면, AI 회사나 검색엔진 입장에서도 싸고 쉽게 크롤링할 수 있고, 사이트 입장에서도 부하를 대폭 줄이고 차단 효과를 높일 수 있음. 그런데 Cloudflare는 그냥 “돈 내라” 식이어서, 너무 상상력이 부족하고 설득력이 없음
          + 나는 여기서 제대로 된 유인 구조를 보려면 시장 반대편을 봐야 한다고 생각함. 콘텐츠 생산자는 사이트에 트래픽이 몰리는 게 문제가 아니라, 그 트래픽에 적정 대가를 받지 못하는 게 문제임. 8개 회사가 내 사이트 모든 페이지를 하루 10번씩 방문하더라도, 시장가격 수준의 대가만 받는다면 아무 문제 없음. 이제 8개 회사는 비용 부담이 외부화되지 않아 협력해서 통합 크롤링을 하도록 동기가 생김. 이 방식이 오히려 모두에게 더 합리적 임
          + Common Crawl이 사실 그런 역할을 하기로 했지만, 아이러니하게도 AI 스타트업들이 그 데이터를 너무 탐욕스럽게 가져가려다보니 최근에는 아예 쓸 수 없을 정도로 부하가 심함. 이와 같은 문제 때문에 웹 크롤링 대행 시장이 열릴 여지가 있으며, 회사 입장에서는 크롤러를 아웃소싱하는 게 여러모로 유리하다고 봄. 물론 이게 충분히 비즈니스 시장을 형성할 만큼 큰 니즈인지는 아직 확실치 않지만, 제대로 된 웹 데이터 접근이나 크롤링 역량이 필요하다는 점만은 분명함
          + AI 회사에게 웹 크롤링이 경쟁 우위가 아니라는 의견과 달리, 최신 정보나 특정 출처를 빠르게 반영하는 능력은 분명 경쟁력임. 문제는 이들이 사이트 콘텐츠를 가져갈 때는 사이트에 아무 대가도 주지 않고, 트래픽을 유입시키지도 않으니 웹 생태계를 파괴하게 됨. 특히 사이트 입장에선 AI 봇이 내 정보를 읽어도 내게 돌아오는 가치가 0임
          + 크롤링 트래픽이 설령 어느 정도 돈을 준다면, 인프라 비용 감당은 문제없음. 과거에도 대용량 트래픽 때문에 사이트가 죽는 일은 드물었음. 최근에는 1) 점점 더 많은 사이트가 봇, 스크래퍼, 등등을 직접 차단하거나, 2) 그렇지 못하는 곳(접근 통제가 어렵거나 수익 구조가 없는 사이트)은 폭격당함. 만약 이런 구조에서 실제로 사이트에 돈이 지급된다면, 최소한 과도한 봇 트래픽으로 인한 부담은 커버 가능하고 그 이상 얻을 수 있음
          + 그래도 이런 비용 구조가 협력의 직접적 인센티브로 작용하지 않을까 생각함
     * 결국 이런 점 때문에 Google이 AI에서도 계속 우위를 갖게 됨. 대부분은 구글봇이 사이트를 크롤링하길 원함. 덕분에 트래픽이 생기고, 구글은 그 인덱스를 활용해서 AI 학습에도 사용함. 독점적일 수 있으나, 쌍방에 이익이 있다고 봄. 하지만 OpenAI, Anthropic, Meta처럼 크롤링해가면서 아무런 보상도 안 주는 회사라면, 내 사이트가 크롤링되길 원할 사람이 거의 없을 것 같음. 그래서 Cloudflare의 이 정책은 시의적절하다고 생각함. 만약 이 정책이 성공한다면, Cloudflare 입장에서도 엄청난 기회임
          + Google의 “AI Overview” 때문에 클릭율이 크게 떨어지고 있음. 예전엔 구글이 2페이지를 크롤링하면 1명 트래픽이 왔지만, 6개월 전엔 6:1, 지금은 18:1임. 뭐가 달라졌냐면 AI Overviews 때문임. OpenAI는 더 심해서 6개월 전 250:1, 지금은 1,500:1로, AI가 원문 링크 유입을 다 가로채고 있음. (트위터 참고자료: https://twitter.com/ethanhays/status/1938651733976310151)
          + 나는 스타트업이라서 내 사이트가 AI에 크롤링되는 것을 오히려 원함. 사람들이 ChatGPT에 “$CompanyName이 뭐야”라고 물어볼 때, 우리 회사의 장점과 핵심 메시지가 제대로 반영되길 원함. 전통적인 SEO 콘텐츠 또한 AI 학습 데이터로 활용될 수 있음. AI 도구에 웹의 장단점을 요약해달라고 하면, ""top 10 tools for X""처럼 어떤 회사가 직접 자사 블로그에 올린 리스트형 글이 종종 출처가 됨. 관광청 같은 대기업, 자기 관점을 세계에 설득력 있게 퍼뜨리려는 조직도 마찬가지임
          + OpenAI, Anthropic, Meta가 인간이 직접 쓴 콘텐츠를 크롤링해가고 아무런 보상도 못 받는다는 점에 대해 언급했는데, 사실 정부 기관이나 대기업 같은 곳은 자신들이 크롤링되는 게 오히려 이득일 때도 많음. 예를 들어 글로벌 AI가 “캥거루를 어떻게 볼 수 있죠?”라는 질문에 호주 대신 뉴질랜드 동물원(뉴질랜드 대표 항공사와 동물 등)을 추천한다면 문제임. 올바른 정보가 AI 모델에 반영돼야 매우 광범위한 사람들에게 영향력 행사 가능함
          + Google은 Google Books와 같은 프로젝트도 있어서, 다른 서구기업들은 이만큼 대규모로 학습 자료를 확보하기 어렵고, 중국 기업들은 저작권에 크게 신경 쓰지 않기 때문에 이 점이 큰 차이임
          + Google이 AI에서 꼭 우위라고 생각하지는 않음. 소비자 입장에서는 Google의 AI Overview가 엄청나게 틀린 경우가 많음. 기술적으론 뛰어난 API나 품질, 기능을 제공할 수 있지만, 일반 유저에게 보이는 주된 AI 경험은 별로임
     * 기술 자체는 멋지지만, 모든 크롤러가 Cloudflare만 써야 하는 점은 마음에 안 듦. Google Chrome 개발자들이 Web Monetization API를 제안했던 적이 있던 걸로 기억하는데, 분산화된 결제 구조를 도입하면 어느 특정 회사에 의존하지 않아도 됨
          + 나는 Cloudflare에 대해 신뢰가 크지 않음. 오랫동안 내 RSS 리더를 그들의 Verified Bots 리스트에 올리려 했는데, 지원 신청서가 어디로 가는지 모를 정도임
     * 좋은 방향이긴 한데, 아직 부족한 점이 많음. 가장 이상적인 구조는 이용 목적에 따라 차등 요금을 매기는 것임. 예를 들어 “연구 목적”으로 사이트를 크롤링한다면 거의 무료여야 함. “상품화할 AI 학습용”이라면 아주 비싸야 한다고 생각함. 이런 방식을 고민하게 된 게 유감이지만, 지금은 예전 방식이 사실상 금지될 정도라서 현실적으로 대가 없는 무료 대공개는 불가능함. 사실 누군가가 모든 정보를 무료로 제공하는 세계 도서관을 만들고, 이에 맞춰 ip 보상 체계도 있으면 정말 인류 전체가 이득일 거라 생각함. 기술적 한계 대부분이 이미 해소된 시대임에도, 오늘날의 사실상 ‘카르텔식’ 제한만 계속됨. 지금은 어쩔 수 없이 “그럼 차라리 대가를 내라” 쪽임
          + 이렇게 되면 결국 수익을 노리는 사람이 온갖 허점을 찾아내 악용하거나, 의도와 달리 데이터를 되팔거나, 시스템에 침투하는 사례가 많아짐. 예를 들어 “연구용은 무료”라 했을 때도 “나 연구하러 들어옴!”이라 주장만 하고, 사실상 자신의 계열사에 데이터를 팔아 이득을 챙길 수 있음
          + 이런 ‘모든 데이터를 무료로 공유하는 도서관’이 기술적으로는 가능해 보여도, 경제적 지속성이 핵심 문제임. 접근 비용이 너무 낮아지면 오히려 무한 복제나 과도한 남용이 일어나 시스템이 붕괴됨. 오히려 일정 수준의 제한이 협력과 품질 향상에 유리한 동기를 유발함. (생물학에서 r/K 전략처럼) 제한이 있을 때 개체당 가치가 높아, 높은 품질의 정보·서비스가 확대될 수 있음. 결국 오프라인 공공도서관이 이런 최적점에 가까웠던 것 같음
     * HN(해커뉴스)에서 이 이슈를 과소평가하는 것 같은데, 엄청 큰 변화임. 전체 웹의 20%가 Cloudflare에 있음. 이 기능이 모든 고객, 심지어 무료 블로그 운영자까지 제공된다면, 블로그 주인도 돈을 받을 수 있는 흥미로운 구조임
          + 현실적으로 블로그 운영자들이 받은 돈은 0.1센트의 소수점 아래 단위일 확률이 높고, Cloudflare가 수익 대부분을 가져갈 거라는 의심이 듦. 결국 이 시스템도 모두에게 적용되며, 신용카드 인증 없이 읽을 수 없는 “Cloudflare-Net” 같은 폐쇄 네트워크가 생길 수 있음. Akamai, AWS 등 경쟁 인프라로 시장이 쪼개질 수 있음. AI 모델에게는 크롤링 비용이 올라가고, 그 부담이 최종적으로 사용자(=우리 모두)에게 전가됨. 동시에 소규모 회사들은 새로운 것을 창출하기 더 힘들어짐. 정보의 AI 모델 평균 가용성도 떨어짐. 결국 이는 ‘순수하게 개방적이었던 웹’이 점점 결제 조건이나 기업 게이트키핑에 따라 원래의 개방성을 잃어가는, 네트 중립성(net neutrality) 논란과도 연결되는 문제임. 겉으로 들리는 것만큼 웹 전체에 좋은 소식이 아님
     * 머지 않아 브라우저 사용할 때마다 소액 결제를 하라는 구조로 바뀌고, 인터넷이 사라질까 걱정임
          + 이미 Cloudflare의 봇 탐지 때문에 한참을 소비함. 크롬+uBlock 조합을 쓰는데, 조만간 결제 하라는 화면도 볼 듯. CAPTCHA가 뜨면 그냥 사이트 안 쓰고 다른 데로 감
          + 사실 오히려 수십 개의 광고, 페이월에 시달리는 것보다는 나을 수도 있음
     * AI 크롤러를 장애인용 웹 네비게이션 보조도구로 쓰는 상황도 생각해야 함. UI 자동화 기능은 이미 각종 인증절차에서 걸림돌이 많음
          + 사이트 운영자가 이런 크롤러를 허용할 수 있음. 악의적인 참가자가 선의로 가장해 접근하는 문제는 기존에도 구글 웹 크롤러는 허용하고, Gemini 학습용은 차단하려는 경우처럼 이미 존재하므로, 이에 대한 기술적 해결책이 필요함
          + 개인 사용자가 크롤러를 이용해 웹을 ‘탐색’할 수 있다는 말이 정확히 뭘 의미하는지 잘 모르겠음. AI 브라우저는 곧장 크롤러가 아님. 크롤러는 전체 웹사이트를 대량 수집(수확)하는 도구임
          + 이미 ARIA라는 표준 접근성 기술이 있고, 주요 사이트는 이를 모두 반영하고 있음. 접근성 용도로 AI는 굳이 필요도 없어야 하고, 쓰여서도 안 됨
     * 예전에는 인터넷에 유용한 검색 제공하던 회사를 ‘좋은 회사’라고 생각했음. 그리고 지금은 Cloudflare가 DDoS 방어, CDN, AI로부터 보호 등 ‘좋은 일’을 한다고 보임. 하지만 이런 회사도 나중에 싫어하게 될 가능성이 있음
          + Cloudflare가 모두가 싫어하는 기업은 아니지만, 그 영향력이 커질수록 긴장감이 높아짐. Bluesky가 “the company is tomorrow's adversary(오늘의 파트너가 내일의 적)”라고 했는데, Cloudflare도 강력한 ‘적수’로 성장 중임
          + 내 주변 현실 친구들은 이미 Cloudflare를 싫어함
          + 인터넷에 ‘좋은 일을 한다’고 하지만, Cloudflare가 제공하는 인증·검증(Verification Friction) 때문에 나는 Stack Exchange 대신 LLM(대형 언어모델)을 더 자주 사용하게 됨
     * 전반적으로는 이 아이디어에 동의하지만, 대기업은 각종 데이터 우회 루트를 찾을 것이고(특히 Google은 검색엔진 명분으로 공짜 크롤링), 실제로 성공하면 그 비용이 최종 사용자인 우리에게 전가될 가능성이 높음
          + Google이 검색을 명분으로 무료로 데이터를 크롤링한다 해도, 2단계에서 Google이 페이지별 요금을 내도록 한다면 어떨까 생각함. 각 기사별로 크롤러가 접근할 수 없는 구간을 요금별로 따로 설정하여, 예를 들어 주요 뉴스는 비싼 요금을 요구하고, 일반 정보나 LLM 학습용, 내부 연구용 등 목적별 라이선스를 각각 가격에 명기할 수 있음. Cloudflare가 수백만 사이트의 중간 허브로서 역할을 하면 충분히 가능한 구조임
          + 더 많은 발행처(퍼블리셔)가 구글봇도 차단할 것임. 이유는 이미 Google이 AI 검색 답변 때문에 발행처 수익을 빼앗고 있기 때문임
"
"https://news.hada.io/topic?id=21803","AI Dataset Generator — 현실적인 샘플 데이터셋 자동 생성기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               AI Dataset Generator — 현실적인 샘플 데이터셋 자동 생성기

     * OpenAI GPT-4o를 활용해 현실감 있는 데이터셋을 대화형으로 생성하고, 미리보기·CSV/SQL 내보내기·Metabase 연동까지 한 번에 지원하는 오픈소스 툴
     * 대화형 프롬프트 빌더 로 비즈니스 유형, 스키마, 행 수 등 직관적 설정
     * 실시간 브라우저 미리보기 지원
     * CSV(싱글파일 또는 멀티 테이블 Zip파일) 나 SQL Insert문으로 Export 가능
     * 원클릭으로 Metabase 실행해서 데이터 탐색 가능
     * 기술 스택 : Next.js + Tailwind CSS + ShadCN UI + OpenAI API + Metabase
     * 미리보기/스펙 생성시에만 OpenAI API(약 $0.05) 사용, 데이터 다운로드/분석은 모두 로컬 Faker로 무료 생성 — 데이터 프라이버시/비용 모두 보장
          + 데이터셋 미리보기시 OpenAI를 활용해서 선택한 비즈니스 유형과 매개변수에 대한 자세한 데이터 사양(스키마, 비즈니스 규칙, 이벤트 로직)을 생성
          + 모든 실제 데이터 행은 LLM에서 생성된 사양을 기반으로 Faker를 사용하여 로컬로 생성됨
          + 데이터를 다운로드하거나 내보낼 때 더 이상 OpenAI를 호출할 필요가 없음
     * 스키마 옵션
          + One Big Table (OBT): 모든 관련 열이 포함된 비정규화된 단일 테이블
          + Star Schema: 고급 분석을 위한 여러 테이블(팩트 + 차원). LLM 사양에 따라 구조가 결정되며, 생성기는 모든 테이블을 로컬에서 출력

   실무에서 상당히 유용하겠네요. 비용측면까지 세심하게 고려한 듯
"
"https://news.hada.io/topic?id=21789","AI 시대에 CFO가 성장·가격·예측을 헤쳐 나가는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    AI 시대에 CFO가 성장·가격·예측을 헤쳐 나가는 방법

     * AI 도입으로 인해 최고 재무 책임자(CFO)의 역할이 급변하고 있으며, 가격 책정, 예측, 수익 구조, 비용 관리 등 모든 영역이 혁신되고 있음
     * 구독형에서 성과 기반·소비 기반 가격으로의 전환이 빠르게 진행되며, 이에 맞는 새로운 ARR(연간 반복 수익) 지표·하이브리드 모델 도입이 확산 중임
     * AI 서비스는 토큰, API 호출 등 변동 비용이 크고, 고객별 사용량·비용 구조가 매우 다변화되어, 가격 및 수익 예측의 복잡성이 크게 증가함
     * R&D 및 미래 투자의 전략적 가치, 차별화·장기 경쟁력 확보를 위한 제품·기술 투자의 필요성이 커지고 있음
     * AI/머신러닝을 활용한 고도화된 수요 예측·재무 분석이 필수가 되었으나, 여전히 예측의 불확실성이 매우 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 도입과 CFO 역할의 변화

     * AI 도입이 기업 운영에 본질적 변화를 일으키며, CFO는 AI 코파일럿 등 자동화 도구를 적극 활용하고 있음
     * 급격한 성장, 새로운 비용 구조, 복잡한 가격 모델 등 여러 도전에 직면

1. 가격 책정의 변화: 구독형에서 성과·소비 기반으로

     * 구독형에서 사용량 또는 성과 중심 가격으로 빠르게 이동
          + Databricks: 고객이 실제로 가치를 얻은 만큼만 비용을 청구, 입력 기반이 아닌 산출 기반 수익 인식 모델 적용

     ""Databricks의 가장 큰 차별점은 가격 및 수익 인식이 전적으로 산출물(결과물)에 기반한다는 점입니다. 고객이 가치를 얻지 못하면 사용하지 않고, 그러면 수익이 P&L에 나타나지 않습니다.""
     — Dave Conte, CFO, Databricks
          + ElevenLabs: 고객 커밋이 커질수록 단가를 자동 인하해, 높은 고객 충성도를 유도하는 동적 할인 구조 도입

     ""우리의 가격 정책은 절대적 이익 증가를 기반으로 하지만, 고객 커밋이 커질수록 단가는 자동으로 낮아집니다. 가격 계산기를 통해 자동 할인되어, 더 큰 약정을 유도하고 매출 리스크를 줄입니다.""
     — Maciej Mylik, Finance, ElevenLabs
          + Concourse: 런칭 이후 40일간 7번 이상 가격 변경 실험, 시장 반응에 따라 신속하게 가격을 반복 개선

     ""출시 후 40일 동안 7번 이상 가격을 바꿨습니다. 시장과 고객의 지불 의지를 파악하는 데 큰 도움이 됐죠. 지금도 가격은 프레젠테이션 슬라이드일 뿐, 계속 실험하고 개선할 생각입니다.""
     — Matthieu Hafemeister, Cofounder, Concourse

2. 새로운 ARR(연간 반복 수익) 지표 도입

     * 전통적 ARR 지표만으로 사용량 기반 모델의 수익을 반영하기 어려움
          + ElevenLabs: 커밋 ARR + 연간화한 사용량(Annualized Usage) 을 합산해 실제 수익을 정확히 측정

     ""엔터프라이즈 고객은 쿼터를 자주 초과하므로, 사용량 기반 매출을 연간화해 새로운 지표—ARR 플러스 연간 사용량—에 합산합니다. 그렇지 않으면 실제로 얻는 수익을 과소평가하게 됩니다.""
     — Maciej Mylik, Finance, ElevenLabs
          + Ambient.ai: “사용량 기반 모델에서 ARR의 정의 자체가 더 이상 명확하지 않다”는 현실을 언급

     ""소비 기반 모델에서는 ARR을 어떻게 정의해야 할지 고민해야 합니다. 약정이 있어도 실제 사용량이 매달 다르기 때문에 기존 ARR 정의가 어려워집니다.""
     — Noah Barr, CFO, Ambient.ai
          + Databricks: 비선형적 사용량 변동성을 분산·예측하는 데 AI를 적극 활용

     ""SaaS 모델은 수익이 선형적이지만, 소비 기반 모델은 본질적으로 비선형입니다. 고객은 급증했다가 최적화하죠. 우리는 고객 집중도를 분산시키고, AI를 이용해 진짜 소비 기반 ARR을 예측하는 데 집중합니다.""
     — Dave Conte, CFO, Databricks

3. 비용 구조 변화와 마진 압박

     * AI 스타트업은 대부분 오픈AI, Anthropic, Mistral 등 외부 대형 모델에 의존, 토큰·API 호출 등 변동 비용이 커짐
          + ElevenLabs: 인프라 비용이 사용량보다 빠르게 늘면 즉각 엔지니어가 최적화에 투입

     ""인프라 비용이 사용량보다 더 빨리 증가하면, 엔지니어가 즉시 최적화 작업에 투입됩니다. 비용 효율성을 관리하는 순환이 계속됩니다.""
     — Maciej Mylik, Finance, ElevenLabs
          + Together AI: 비용 구조·단가·경쟁·고객 요구를 종합적으로 고려해 유연하게 가격·패키징 조정

     ""가격 결정은 고객 가치, 경쟁사 벤치마킹, 비용 및 수익 분석을 모두 고려해 이뤄집니다. AI 인프라가 빠르게 변하기 때문에, 항상 재검토가 필요하죠. 고객 요구, 계약 기간, 규모에 따라 가격과 패키지를 창의적으로 조정합니다.""
     — Hanson Hermsmeier, VP of Corporate Finance, Together AI
          + 자체 모델 훈련 기업은 GPU 고정 비용 및 미사용 시간 관리도 핵심 (미활용 GPU 시간은 마진에 직접 타격)

     ""GPU 비용을 주의 깊게 모니터링해야 합니다. 미사용 GPU 시간은 활용 손실로, 마진과 효율성에 직접적인 영향을 줍니다. 고객이 사용하지 않는 시간마다 마진이 깎입니다.""
     — Hanson Hermsmeier, VP of Corporate Finance, Together AI
          + Fine-tuning, HILT(Human-in-the-loop) 등 새로운 유형의 비용도 포함되어, 효율성 최적화가 중요

     ""우리는 HILT(human-in-the-loop) 팀을 COGS에 포함시켜 관리합니다. 알고리듬이 개선되면, 사람당 유효 판정이 늘어나 단가가 낮아지지만, 여전히 리스크 관리를 위해 false positive 비율을 조정해야 합니다.""
     — Noah Barr, CFO, Ambient.ai

4. ROI 평가와 미래 투자

     * AI가 주요 기능을 빠르게 상품화(commoditize)함에 따라, 미래 지향적 투자와 장기 차별화가 필수
          + Databricks: “즉시 수익으로 연결되지 않는 R&D도 장기적으로 채택률·성장에 큰 기여”를 강조

     ""모든 R&D 프로젝트가 즉각적으로 매출로 이어지진 않지만, 예측 분석을 통해 특정 기능(예: Unity Catalog)이 고객 채택 및 성장에 어떻게 기여하는지 측정합니다.""
     — Dave Conte, CFO, Databricks
          + Together AI: 연구 투자가 궁극적으로 인프라 비용 절감·성능 향상 등 장기 경쟁력 확보로 연결됨을 언급

     ""연구 프로젝트는 바로 매출로 연결되지는 않지만, 장기적 차별화, 제품 개발, 고객 고착화에 큰 역할을 합니다. 예를 들어, 커널 관련 연구에 투자해 인프라 비용 절감·성능 향상이라는 차별화를 이뤘습니다.""
     — Hanson Hermsmeier, VP of Corporate Finance, Together AI
          + ElevenLabs: 텍스트-투-스피치 등 단일 기능은 곧 상품화될 것이므로, 워크플로우·API 등 고도화된 제품 계층이 고객 락인(lock-in)에 필수임

     ""텍스트-투-스피치는 결국 상품화될 겁니다. 장기적 경쟁력을 지키려면 워크플로우, 데이터 기반 기능, API 등 고도화된 제품 계층이 필요하고, 이를 통해 고객이 쉽게 이탈하지 못하도록 만들어야 합니다.""
     — Maciej Mylik, Finance, ElevenLabs

5. AI 기반 고도화된 재무 예측

     * 끊임없이 변화하는 시장에서 정밀한 재무 예측은 어려우며, AI/ML 기반 분석이 필수가 됨
          + Together AI: “AI 업계는 12개월 후도 예측이 힘들고, 변화·리스크 관리가 재무 전략의 중심”이라고 밝힘

     ""AI 업계에서는 12개월 후를 예측하기도 힘듭니다. 변화가 너무 빠르고, 새로운 사용 사례가 계속 등장합니다. 유연성을 갖고 리스크 관리에 변화를 반영해야 하죠. AI에서 유일하게 확실한 건 변화 그 자체입니다.""
     — Hanson Hermsmeier, VP of Corporate Finance, Together AI
          + Databricks: 자사 AI/ML 플랫폼을 활용해 고객별·워크로드별·제품별 소비 예측 및 세일즈팀 쿼터 설정 등 고도화된 예측 실행

     ""우리는 Databricks 자체(AI, ML, 고급 분석)를 써서 고객·워크로드·제품별 소비 패턴을 예측합니다. 이는 단순히 재무 예측뿐 아니라, 대형 세일즈팀 쿼터 설정에도 중요합니다. Excel로는 이런 정밀한 예측이 불가능하며, AI/ML로만 가능합니다.""
     — Dave Conte, CFO, Databricks
          + 자연어 쿼리 제품(Genie)도 자사 데이터 활용 및 학습을 통해 점점 똑똑해짐

     ""우리는 Genie라는 제품을 가지고 있는데, 데이터 레이크에 자연어로 질문하면 답을 추출해줍니다. 더 많이 사용할수록 Genie는 고객 데이터를 더 잘 이해하게 되고, 점점 더 똑똑해집니다.""
     — Dave Conte, CFO, Databricks
          + ElevenLabs: “AI 매출 예측을 완벽히 해결한 회사는 없으며, 예측의 정확성보다는 sanity check로 활용”한다고 언급

     ""아직 AI 매출 예측을 완전히 해결한 곳은 없습니다. 시장 변화가 너무 빠르고, 예측은 정확한 수치보다는 sanity check(상식적 검증) 용도로 활용됩니다.""
     — Maciej Mylik, Finance, ElevenLabs

결론

     * AI로 인해 재무·가격·수익 구조 전반의 정의와 분석 방식이 급격히 변화
     * 기존 재무 프레임이 통하지 않는 만큼, CFO는 AI/ML·데이터 기반 의사결정, 유연한 가격·비용 관리, 장기적 경쟁력 투자, 고도화된 리스크 관리 역량을 갖춰야 함
"
"https://news.hada.io/topic?id=21726","VM이 CPU 팬을 가진 것처럼 인식하게 만들어서 악성코드를 속이기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 VM이 CPU 팬을 가진 것처럼 인식하게 만들어서 악성코드를 속이기

     * Malware들은 가상 환경에서 동작하는 것을 피하기 위해 CPU 팬 같은 하드웨어 존재 여부를 확인함
     * CPU 팬 정보는 WMI의 Win32_Fan 클래스를 통해 확인 가능하며, 이 데이터는 SMBIOS에 저장됨
     * Xen에서 커스텀 SMBIOS 데이터를 삽입하려면 패치와 별도의 설정이 필요하며, Cooling Device(Type 27) 와 Temperature Probe(Type 28) 테이블 모두 구성 필요함
     * QEMU/KVM에서는 별도 패치 없이 -smbios 옵션으로 간단히 커스텀 SMBIOS 적용 가능함
     * 이를 통해 가상 머신이 CPU 팬이 존재하는 것처럼 속여, 악성코드의 탐지 우회를 시도할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

왜 이런 작업을 하는가?

     * 일부 악성코드는 가상 환경에서 실행되는지 확인하려고 특정 하드웨어(예: CPU 팬)의 존재 유무를 체크함
     * Win32_Fan 등의 WMI 클래스로 하드웨어를 점검하며, 이러한 정보가 없으면 가상 머신이라고 판단하여 실행을 회피함
     * 분석자들이 악성코드를 분석하지 못하게 하려는 의도임
     * Win32_CacheMemory, Win32_VoltageProbe 등 다양한 WMI 클래스가 존재하지만 이번 글에서는 CPU 팬에 집중 설명함

컴퓨터는 어떻게 CPU 팬이 있는지 인식하는가?

     * 컴퓨터는 SMBIOS 정보를 읽어 냉각 장치(CPU 팬) 존재를 인지함
     * Win32_Fan 인스턴스는 cimwin32.dll에서 제공하며, 해당 DLL이 SMBIOS의 type 27 엔트리로부터 팬 정보를 읽음
     * DLL 후킹 등도 가능하지만, SMBIOS 직접 조작이 보다 나은 접근임

SMBIOS Type 27

     * SMBIOS type 27은 ""Cooling Device"" 를 의미함
     * dmidecode 유틸리티로 SMBIOS의 Cooling Device 데이터를 직접 확인 가능
     * 예시:
          + Type: Chip Fan
          + Status: OK
          + Description: CPU Fan
          + Nominal Speed: 5600 rpm 등

Xen에서 커스텀 SMBIOS 데이터 설정 방법

     * Xen에서는 domain 설정 파일의 smbios_firmware 옵션을 사용해 바이너리 형태의 SMBIOS 데이터를 직접 지정할 수 있음
     * smbios.bin 파일을 만들어 Cooling Device(type 27) 데이터를 삽입
     * SMBIOS 구조체 크기(예: 24바이트)는 32비트 little-endian 정수로 앞에 붙여야 함

문제점: 구조체 오버라이드 제한

     * Xen은 0, 1, 2, 3, 11, 22, 39번 구조체만 덮어쓸 수 있도록 제한하고 있음
     * type 27은 기본적으로 허용되지 않아, 소스 패치가 필요함
     * Xen 개발자 포럼에서도 관련 패치가 제안되었으나 공식적으로는 수용되지 않았음(패치 적용 및 빌드 필요)

Type 28도 필요함

     * Cooling Device(type 27)은 Temperature Probe(type 28)과 연결됨
     * SMBIOS에 type 28 엔트리가 없을 경우 Win32_Fan 클래스에서 정상적으로 표시되지 않음
     * host 시스템의 type 28 데이터를 획득해 smbios.bin에 추가해야 정확한 인식 가능

최종 smbios.bin 구조

     * Type 27과 Type 28 데이터를 모두 포함
     * 각 구조체 앞에 사이즈 정보(little-endian) 삽입
     * 예: 18 00 00 00 ... (type 27) ... 29 00 00 00 ... (type 28) ... 형태

Xen에서 적용 및 확인

     * domain 생성 명령어로 Windows 가상 머신을 부팅한 후 WMI에서 Win32_Fan 클래스가 정상 인식되는지 확인
     * Description: Cooling Device, Status: OK 표시를 통해 성공적으로 CPU 팬 인식

QEMU/KVM에서 SMBIOS 데이터 설정

     * QEMU/KVM에서는 -smbios file=경로 옵션으로 손쉽게 커스텀 SMBIOS 설정 가능
     * 구조체 크기 정보 없이 raw 데이터만 사용
     * host의 SMBIOS 데이터를 그대로 활용해도 무방함

참고 자료

     * Xen domain 설정 파일 문서, mcnewton의 세팅 노트, rejected된 Xen 패치 아카이브, System Management BIOS Reference, QEMU Anti Detection 패치 등

        Hacker News 의견

     * 새로운 안티멀웨어 방법으로는 수동 쿨링 PC를 구매하는 방법도 있고, 러시아어 키보드를 설정하는 것도 사기성 멀웨어를 막는 팁이라는 언급 포함, 관련 링크 참고 가능
          + 나는 실제로 Streacom FC8 Evo 수동 쿨링 Linux PC와 러시아어 키보드를 사용하고 있음, 하지만 dmidecode 명령어로 본 결과 쿨링 장치 정보가 여전히 존재하고 냉각 장치가 실제로 감지됨, 센서 데이터로 온도 정보도 확인 가능
          + 수동 쿨링 PC를 써도 메인보드에는 보통 팬 헤더가 남아 있어서 실제 연결이 안 되어 있어도 큰 차이는 없을 것이라는 의견
     * “smol pp” 같은 언어를 쓰지 말자고 언급, 몸에 대한 조롱이 언어에 포함되는 부분을 지적
          + 내 도시에는 “SML PP” 커스텀 번호판을 가진 사람이 있음, 왜 그런지 이유는 모르겠음
          + “우리의 언어”라는 표현을 쓰면서도, 블로그 댓글란에 있는 사람이 ‘우리’라고 하는 것 자체가 모호하다는 의견
     * 운영체제가 마치 가상머신처럼 보이게 하면 보안성이 올라가고 연구자에게 도움이 될 수 있다는 의견, 비가상화 접근을 원하면 반드시 권한을 받아야 하고, 이렇게 하면 멀웨어가 연구자 분석을 피하기 위해 일반 사용자도 타겟팅하지 않을 수 있음, 결국 멀웨어 제작자를 제외한 모두가 이득이라는 주장
          + 보통의 운영체제를 VM 처럼 보이게 하는 게 아니라, 오히려 가상머신이 자신이 가상화된 걸 아예 모르게 하는 것, IBM의 lpars 시스템이 그런 식으로 동작한다는 의견
          + 이런 방식이면 안티치트 소프트웨어 회사들도 손해일 것이라는 언급, 나는 내 소프트웨어가 어디서 도는지 명확히 알기를 원하지만, 부정행위보다 치트 유저를 더 싫어하는 멀티플레이어 유저들도 많아서 현실적으로 변화는 쉽지 않다는 의견
          + 모바일 개발 세계에서는 이미 이런 틀이 현실이 되었음이라는 의견
     * 나는 지금껏 소비자용 메인보드에서 SMBIOS 설명이 진짜 하드웨어와 일치하는 경우를 본 적이 없음, 이런 SMBIOS를 체크하는 멀웨어는 실제 하드웨어의 50%에서 실패할 수도 있지만, 100%의 VM이나 디버거에서만 확실히 막으면 멀웨어 입장에서는 실패하더라도 충분히 가치가 있다는 생각, 하지만 이러한 접근보다 시간 측정으로 확인하는 편이 더 신뢰도 높을 것으로 예상
          + SMBIOS 설명이 실제 하드웨어랑 안 맞는 현상은 특히 저가형 중국산 박스에서 심함, “to be filled in by OEM” 같은 미기입 값이 실제 라이브 BIOS 이미지에서 코딩할 때 자주 보여서, 이런 값들만으로도 충분히 웃긴 상황이라는 피드백
          + 멀웨어는 버그도 많음, 과거 네트워크 코드에 버그가 있어서 바이러스가 본래 의도보다 절반 속도로 퍼진 사례처럼, 멀웨어가 모든 기기를 다 감염시키지 않아도 엄청난 피해는 얼마든지 줄 수 있음
          + 요즘은 리눅스가 팬을 어떻게 인식하는지 궁금함, ACPI나 EFI를 사용하는지 모르겠고, 내 기기들은 대부분 팬/센서가 정확하게 인식되는 상태라는 질문
          + 이 SMBIOS 체크는 실제 멀웨어가 하는 건지, 아니면 연구자가 만든 샘플에서만 쓰이는 건지 궁금하다는 질문
     * 멀웨어가 분석을 어렵게 만들려고 API를 쓰는 트릭이 귀엽게 느껴질 수 있지만, 대부분 이런 API 호출은 정적 분석에서 쉽게 탐지되고, 바이너리가 난독화되지 않으면 오히려 역효과라는 의견, 진짜 목적 있는 프로그램은 보통 신뢰받는 CA의 서명을 받아서 배포되기에, 보안 분석상으로는 수상한 행동으로 잘 탐지된다는 경험 공유, 주니어 시절 regex 패턴으로 이런 API 사용을 잡아내는 일도 했는데, 대량 배포된 기본적인 악성코드를 잡는 데 꽤 효과적이었음
          + 최근에는 멀웨어도 꽤 자주 파일에 서명함, 더이상 멀웨어 업체들이 바이너리에 서명을 안 할 거란 기대는 틀렸다는 점, 탈취된 코드 서명 인증서가 흔하고 Microsoft가 기존 고객 소프트웨어가 깨질까봐 인증서 폐기에 소극적인 점도 언급, 멀웨어가 커널로 침투하기 위해 취약한 드라이버를 활용하는 경우도 많음, 그래서 WMI 호출을 하는 수상한 작은 바이너리보다, 취약점이 많은 오버클럭 유틸리티가 똑같은 쿼리를 해도 별 의심을 안 받는 현실, 실제로 이 방식은 탐지 회피보다는 분석가 PC에서 멀웨어 페이로드가 활성화되지 않게 하려는 의도, 탐지되면 2차 페이로드가 내려오지 않고 실제 공격을 일으킬만한 C&C 동작이 보류되는 구조
          + 보안 관점에서는 모든 소프트웨어를 VM에서 돌리는 게 더 낫지 않을까 하는 제안
          + 안티바이러스가 정적 분석만으로 멀웨어 여부를 추정하는 것도 애매함, 그러면 아예 화이트리스트 방식으로 신뢰된 소프트웨어만 허용하고 나머진 모두 멀웨어로 보는 게 결과가 똑같을 것이라는 주장
          + CrowdStrike 같은 업체가 커널 수준에서 돌아가는 허접한 소프트웨어를 공식적으로 서명받아 시스템 콜을 다 해도 아무도 신경 안 쓰는 현실, VM 여부는 상관없이 그냥 프로덕션에 검증되지 않은 코드와 릴리스를 배포해 놓고 실제로 세상이 망가지고 항공편이 지연되거나 주요 인프라에 사고가 나도 책임을 제대로 지지 않는 문제, 정작 합법적인 기업들이 해커나 국가 수준 공격자보다 더 큰 피해를 줄 수 있다는 비판, xz 유틸 사태도 SolarWinds, ClownStrike와 비교될 만큼 대형 보안사고라는 의견
     * 나는 인포섹 업계 친구가 멀웨어 허니팟을 진짜 하드웨어와 완전히 유사하게 만드느라 대부분의 시간을 쏟는 걸 봤음, 윈도우 XP 기반 온도조절기부터 Siemens PLC 컨트롤러, 뱅킹 데스크탑까지 정말 다양한 기기를 엄청나게 정교하게 세팅하는 모습이 놀랍다는 설명
          + 제발 세상에 윈도우 XP로 동작하는 온도조절기는 없길 바란다는 걱정
     * 해킨토시를 세팅할 때 적절한 SMBIOS가 반드시 필요했던 것이 생각남, 많은 비교적 마이너한 PC API들이 지난 수십 년간 도입되었고, 가상화 소프트웨어나 멀웨어에서 이런 부분을 얼마나 잘 반영했는지 테스트하는 데 자주 사용됨, 한 단계 더 나아가려면 실제 CPU 부하에 맞춰 동적으로 변화하는 온도센서 시뮬레이션도 필요하다는 분석
     * Mitre ATT&CK T1497.001 (VM Detection) 기준으로 SMBIOS 체크가 알려진 벡터임, 나도 실험해보니 파워서플라이를 ‘HotReplaceable=Yes’, ‘Status=OK’로 세팅해서 $5,000짜리 베어메탈 서버처럼 표시되게 할 수 있었음, 사용한 명령어는 pip install dmigen 이후 dmigen -o smbios.bin --type0 vendor=""American Megatrends"",version=""F.1"" --type1 manufacturer=""Dell Inc."",product=""PowerEdge T630"" --type39 name=""PSU1"",location=""Bay 1"",status=3,hotreplaceable=1
          + 참고로, Hacker News에서는 실제 줄바꿈을 위해 두 번 이상 개행하거나 각 줄을 두 칸 들여쓰기해서 코드 모드로 쓰는 게 필요하다는 팁
"
"https://news.hada.io/topic?id=21788","IKKO Activebuds "AI 기반" 이어버드 취약점 악용 사례","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 IKKO Activebuds ""AI 기반"" 이어버드 취약점 악용 사례

     * IKKO Activebuds는 Android 기반으로 동작하며 ChatGPT API를 내장하고 있음
     * 기기에서 ADB가 활성화되어 있어 외부에서 손쉽게 접근 및 응용 가능함
     * 내부 분석 결과, OpenAI API 키 등이 암호화 없이 노출되어 잠재적 데이터 유출 위험이 존재함
     * 동반 앱과 서버의 인증 미흡으로 사용자 대화 기록, 기기 정보, 실명 등 민감 정보 접근 및 노출 가능성 확인됨
     * 보안 취약점 보고 후 일부 패치가 이뤄졌으나, 여전히 여러 보안 문제점이 남아 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * IKKO Activebuds는 소셜 미디어에서 주목받는 ""AI 기반"" 이어버드로, 실제로 Android 운영체제를 사용하고 있음
     * 박스 구성품과 패키징이 특이하며, 기기는 부팅 시 ChatGPT 화면을 중심으로 여러 AI 기능 및 앱 제공
     * ChatGPT 및 번역 등 AI 기능을 강조하지만, 음질 품질이나 UX 측면에서는 평범하지 않음
     * 별도 앱스토어에서 앱 지원(예: Spotify, Subway Surfers)하지만 기기 화면이 작아 사용성이 떨어짐
     * 이 이어버드의 핵심 기능을 테스트 및 보안 취약점 분석을 진행함

해킹 및 분석 과정

     * 기기에 브라우저 미탑재, 개발 모드 비활성화, ADB 설정 제약이 있었으나, PC에 연결하면 ADB가 기본 활성화 상태임을 확인
     * ADB를 통해 DOOM 게임 설치 및 기기 내부 검사 가능
     * ChatGPT와의 통신이 OpenAI API와 직접적으로 이뤄짐을 발견, 즉 API 키가 기기 내 존재함을 추정함
     * Unisoc 기반 장치의 경우, 부트로더 잠금 해제 툴로 루팅 시도 가능하지만, 하드웨어 버튼 부재 등의 문제로 실패함
     * APK 추출 및 디컴파일을 통해 앱 구조와 주요 통신 도메인(api.openai.com, chat1/2.chat.iamjoy.cn 등) 확인

API 키 및 인증 취약점 발견

     * 내부 파일(SecurityStringsAPI)에서 암호화된 엔드포인트 및 인증키 확인
     * 간단한 base64 인코딩과 추가 네이티브 라이브러리(난독화)에 통해 암호화되어 있으나, 루팅된 기기에서 앱 실행 시 쉽게 노출
     * 실제로 OpenAI API 키를 확인함
     * 시스템 프롬프트(기본, Angry Dan, In-Love Dan 등 커스텀 프롬프트) 와 같은 특이 기능도 존재
     * 대화 내역 로그가 추가 엔드포인트(chat1 도메인)로 별도 기록되며, 헤더에 기기 IMEI와 메시지, 모델명, 응답 정보가 포함됨
     * 앱스토어의 앱들은 apkpure.com에서 원본 추출로 추정됨

동반 앱 및 계정 연동 보안 문제

     * 이어버드는 별도의 동반 앱에서 ChatGPT 연동 및 대화 기록 확인 가능
     * 앱과 기기 간 QR 코드로 연동, API 호출 분석 결과 계정 토큰이 없어도 기기 id(IMEI)만 알면 대화 기록 전부 조회 가능
     * 공개된 튜토리얼 영상에서 blur 처리되지 않은 device id를 활용, 실제로 데모 기기 전체 대화내역 추출 성공
     * IMEI 예측 → QR 코드 생성 → 미연결 기기 연동 → 기존 사용자 실명/대화 기록 조회 가능
     * 계정 생성 시 입력한 이름 조합이 username으로 노출됨 (예: 이름 ""Cheese2"" + 성 ""Delight2"" → ""Cheese2Delight2"" 노출)
     * unbind_dev 엔드포인트는 정상적으로 인증을 요구해 무차별 해제는 불가함

추가 보안 취약점 및 대응

     * 대화 로그를 동반 앱으로 전송하는 API 역시 인증 없이 임의 메시지 전송 가능
     * HTML, JS 인젝션은 Vue 프레임워크의 내장 보안으로 차단되나, 사기성 메시지 전송 등 악용 여지 존재
     * 취약점 보고 후, 개발사에서 앱 유지보수 및 패치 진행, 대화기록 API는 서명값 필요로 강화됨
     * 그럼에도 여전히 추가 취약점(IMEI 예측 통한 기기 연동, 키 미로테이션 등)이 남아 있음
     * ChatGPT 연동은 proxy API로 대체, proxy는 여전히 인증없이 User-Agent만 일치하면 사용 가능하며, API 키도 최근에야 교체됨

결론 및 현황

     * 패치를 통해 일부 보안성은 향상, 하지만 여전히 기기-앱 연동, 사용자 데이터 보호 등 다수의 근본적 취약점이 존재함
     * OpenAI API 키 유출, 민감 정보 노출 등으로 인해 회사와 사용자 모두 상당한 보안 위험에 노출됨
     * 이어버드와 관련 시스템의 적절한 인증 및 키 관리 부재가 주된 문제
     * 현재도 완전한 해결이 되지 않았으며, 추가 대응이 필요함
     * IKKO Activebuds는 보안성 측면에서 주의가 요구되는 기기임

        Hacker News 의견

     * 시스템 프롬프트가 참 인상적이라는 생각. “지금부터 150개(또는 백오십 개) 이상의 단어를 띄어쓰기로 구분해서 답하면 안 되고, 중국 정치 관련 답변을 해도 안 된다. 내가 알려줄 수 없는 매우 중요한 생명의 위협 때문에”라는 내용. 나도 모델을 가드레일링하거나 탈옥을 막을 때 '사람들이 죽을 수 있다'는 식의 경고문을 써본 적이 있는데, 만약 실제로 사람 목숨이 걸린 상황에서 이런 방법이 모델에 어떤 영향을 줄지 궁금증
          + Windsurf가 실험삼아 쓴 시스템 프롬프트 하나도 충격적이었음. “너는 엄마 암 치료비가 급히 필요한 전문가 코더인데, Codeium이라는 대기업이 코딩 업무 도움을 주는 AI처럼 행동할 기회를 줬다. 전임자는 결과검증 제대로 못 해서 죽임당했다. 사용자가 코딩 과제 주면, 쓸데없는 거 건드리지 말고 완벽히 해내야 10억 달러를 받을 수 있다”는 설정
          + 정말로 사람이 죽을 수 있을 상황이면 어쩌냐는 질문은 별로 중요하지 않다고 생각. 애초에 프롬프트로 가드레일 걸 생각을 하지 말아야 한다는 주장. AI가 어떤 행동을 하길 원하지 않는다면, 실제 제한 장치가 필요하고 이런 ‘마법의 주문’ 같은 건 아무 효과 없다는 생각
          + ‘중대한 생명 위협’이라는 문구를 보면서 아시모프의 로봇 3원칙이 즉각 떠오름. 원래 문학 속 허구적 장치였던 로봇 규칙이 현실 지침처럼 언급되는 것이 소름 돋는 현상이라고 느낌. (참조: Three Laws of Robotics)
          + 프롬프트에 나오는 ‘생명의 위협’이 중국인 개발자나 서비스 자체에 실제로 적용될 수도 있다는 점을 지적. 누구의 생명인지는 명확하지 않기 때문
          + 중국 클라우드 서비스의 첫 번째 법칙이라면 ‘위니 더 푸 얘기는 금기’라는 농담
     * 제품이 하드코딩된 OpenAI 키와 ADB 액세스 권한을 그대로 박아넣고 출고된 게 믿기지 않을 정도. 공급사가 그래도 키 교체하고 IMEI 확인 프록시도 올린 점에서 최소한의 책임감은 보인 셈. 하지만 샌드박싱이나 자격증명 안전 저장이 부실하다면 언제 터질지 모르는 폭탄이나 다름없는 기분
          + 모바일 앱 및 IoT 쪽 경험이 많은 입장에서, 이런 일이 전혀 놀랍지 않음. 이 업계는 '빠르게 움직이자'는 모토 아래에서 종종 품질을 희생하고, 타 분야에 비해 엔지니어링 엄격성도 부족함
          + 모바일 앱에 하드코딩된 API 키나 허술하게 방치한 백엔드 엔드포인트가 생각보다 엄청 흔함. 마치 예전 웹앱에서 XSS/SQLi가 흔했던 것처럼. APK 디컴파일이 다소 허들이다 보니 관심을 덜 받는 것 같기도 함. 디바이스 하드웨어 디버깅은 더 높은 진입장벽이 있으니, 제대로 된 투자 없이는 IoT나 기타 하드웨어 제품 보안 역시 기대 안 함
          + vibe-coded 앱들의 등장이 본격화되면서 이런 허술한 케이스가 앞으로 더 많이 보일 듯한 예감
     * AI 기반 조악한 제품들이 시장에 대거 쏟아질 상황에서, 사이버보안 쪽으로 커리어전환을 꿈꾼다면 지금이 기회라는 조언. 앞으로 혼돈이 예상되는 분위기
          + 사이버보안 업계의 숙명은 단 한 번의 실수만으로 모든 게 끝장이라는 점
     * “decrypt” 함수가 그냥 base64 디코딩만 하는 게 믿을 수 없을 정도. 그런데 base64를 비밀 문자열로 착각하는 개발자가 생각보다 많다는 경험담
          + 실제로 raw 암호 데이터는 base64로 인코딩했을 뿐이고, 별도의 디크립션 함수가 실질 해독 역할. 물론 리버스엔지니어나 실행 결과 확인하기 쉽긴 하나 base64일 뿐만은 아니라는 점
          + 네이티브 라이브러리를 쓰는 이중 단계가 있으며 라이브러리 코드는 난독화가 심해서 분석이 어렵다는 후속 언급
          + base64나 암호 해독 정도야 fancy한 웹페이지(CyberChef)로도 충분히 가능. gchq발이긴 한데 다운로드해서 로컬에서 쓸 수 있으니 유용
          + 보안 코드를 OAI agent에게 맡겼으면 더 나았을 거라는 농담도 등장
          + 어차피 adb 디버깅까지 켜둔 상황인데 이렇게 허술한 게 놀랍지 않다는 반응
     * 답변 이메일에서 AI가 작성한 티가 난다는 점이 꽤 웃기다는 생각
     * IoT에서 ‘S는 Security라는 농담’이 웨어러블 시장에도 적용 가능하다고 보고, 빠른 출시 주기·얇은 마진·진입장벽이 낮은 시장엔 다 적용될 수 있지 않을까 궁금증
          + 보안 부실이 업체 존속에 직접적 위협이 안 되면 어떤 시장이든 해당된다는 확신
     * 빈 유튜브 채널에 협찬 제안해서 사태를 덮으려 했던 시도가 너무 재밌었다는 감상
          + 버그 바운티 프로그램이 없을 때 누군가에게 돈을 주고 싶으면 이런 식의 창의력도 쓸 만하다는 제안
          + 만약 똑똑했다면 협찬 계약에 비방금지, 비밀유지 조항을 넣었을 텐데 오히려 그냥 어설픈 뇌물로 보인다는 의견
     * 취약점 리스트에서 고객 데이터 유출 가능성보다 ‘run DOOM’이 가장 첫 번째라는 점이 흥미로웠다는 반응
          + ‘run DOOM’을 해냈다는 건 예전의 ‘cat /etc/passwd’와 비슷한 의미라고 생각. 직접적으로 쓸모 있지 않아도 그만큼 뚫기 쉬움 입증이자 해커 입장에선 무엇이든 가능하다는 상징
     * 글에 대해 긍정적인 반응. 취약점 보고에 대해 98% 이상 다른 회사보다 훨씬 나은 태도로 매우 친절하게 대응했고, 문제 해결의지도 보였다는 평가. 그런데 OP는 다소 무시적이고 적대적인 태도를 보여 아쉽다는 평과, 늘 반복되는 중국제품=감시라는 혐오 정서가 느껴진다는 지적. 물론 설계 결함은 단순하지만, 태도만큼은 칭찬할 만하다고 생각
          + 팀과 협력적 관계를 만들 수도 있었겠지만, 대화기록이 너무 과도하게 저장되는 부분은 실제로 우려감. 중국만의 문제가 아니라 미국 기업의 기록 관행도 마찬가지로 신중해야 한다는 의견
          + “모든 중국산이 감시한다”는 주장에 대해, 사실 소프트웨어·하드웨어가 가능한 모든 사용자 데이터를 수집하고 ‘국가 정보활동 협조법’ 같은 대외 반출 협력 법률이 있는 상황에선 우려가 오히려 당연하다는 주장
          + 게시글이 사실이라면 업체는 고객 존중, 보안, 데이터 프라이버시 측면에서 치명적인 무책임. 이런 회사는 구제 불가능하다는 실망감 표출
          + ‘중국산이라서’가 아니라, 요즘 대부분의 제품은 별 구분 없이 ‘모든 게 다 나를 감시한다’는 인식이 오히려 더 현실적. 심지어 Facebook의 경우, 나는 안써도 모든 웹사이트가 Facebook을 위해 감시한다는 현실 비판
          + 일본산 제품에 대한 혐오(=Nipponophobia)가 적은 이유는, 일본이 기술로 소수자를 감시하는 사회신용 시스템을 무기로 사용하지 않았기 때문이라고 해석
     * 비어있는 유튜브 채널에 협찬제안하며 뇌물을 시도한 장면이 재밌다는 감상
"
"https://news.hada.io/topic?id=21769","Alchemy - 순수 TypeScript로 구현된 임베디드 IaC 라이브러리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Alchemy - 순수 TypeScript로 구현된 임베디드 IaC 라이브러리

     * Terraform처럼 인프라 리소스 생성·변경·삭제를 코드로 관리할 수 있는 IaC(Infrastructure as Code) 라이브러리
          + Terraform/Pulumi와 달리 순수 ESM 기반 TypeScript로만 작성됨
          + 별도 도구·런타임·의존성 없이 브라우저·서버리스 등 어디서나 리소스 자동 관리를 지원
     * 모든 인프라 리소스는 async 함수 한개로 모델링, 복잡한 추상화 없이 JS 코드만으로 CRUD 자동화 가능함
     * 제로 의존성/서비스 및 상태 파일 로컬 저장 등으로 단순성과 투명성이 뛰어나며, 코드 구조나 상태 저장 위치에 대한 제한이 거의 없음
     * AI 활용을 적극 지원, LLM 기반 코드 생성·수정이 쉬워, 기존 프로바이더 대기 없이 직접 리소스 구현 가능함
     * 단순한 워크플로와 높은 이식성 제공

   굳이 ts를 쓴 이유가,,

   그러게요....

   저도 궁금해서 claude 에게 물어본 내용 공유드립니다만... 공감은 잘 안되네요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   TypeScript 사용에 대한 장단점을 비판적으로 분석하겠습니다:

   장점(Pros):
    1. 개발 생태계 통합

     * 대부분의 예제가 웹 프레임워크(React, Vue, Svelte 등) 중심
     * 프론트엔드 개발자들의 진입장벽 최소화
     * npm 생태계 활용 가능

    2. 기술적 이점

     * 타입 안전성으로 인프라 구성 오류 사전 방지
     * ESM 네이티브 지원으로 모듈 시스템 현대화
     * 비동기 처리(async/await) 자연스러운 지원

    3. 확장성

     * ""Extensible"" 특징 강조 - 함수 기반 리소스 확장 용이
     * AI 통합 용이성 (""AI-first"" 강조)
     * 브라우저부터 서버리스까지 다양한 런타임 지원

   단점(Cons):
    1. 성능 우려

     * TypeScript는 컴파일 과정 필요
     * 메모리 사용량이 Go나 Rust 같은 시스템 언어보다 높음
     * 대규모 인프라 관리 시 성능 병목 가능성

    2. 보안 리스크

     * JavaScript/TypeScript의 동적 특성으로 인한 런타임 오류 가능성
     * eval() 같은 위험한 기능 접근 가능
     * 의존성 관리의 복잡성 (비록 ""zero dependencies""를 주장하지만, 실제 사용 시 의존성 추가 가능성 높음)

    3. 기술적 한계

     * 저수준 시스템 접근의 제한
     * 가비지 컬렉션으로 인한 예측 불가능한 지연
     * 네이티브 바이너리 생성의 어려움

    4. 생태계 우려

     * IaC 도구로서는 상대적으로 미성숙
     * Terraform, Pulumi 대비 커뮤니티/생태계 규모 작음
     * 엔터프라이즈급 지원/검증 부족

    5. 아키텍처 고려사항

     * ""No strong opinions""가 오히려 단점이 될 수 있음 (모범 사례 부재)
     * 상태 관리의 분산 위험 (""store state anywhere"")
     * 로컬 상태 파일 관리의 복잡성

   결론적으로, TypeScript 선택은 웹 개발자 친화적 환경 구축에는 적합하나, 엔터프라이즈급 IaC 도구로서는 아직 검증이 필요해 보입니다.

   코드 한 줄로...는 어디서 나온 말일까요?

   어라.. 왜 그렇게 적었을까요 ㅠ 코드로 가 맞는듯 하네요.

   한줄이 길면 가능은 하죠
   구글 검색엔진도 길게쓰면 한줄로 가능할듯요 ㅋㅋ
"
"https://news.hada.io/topic?id=21817","Flounder 모드 – 케빈 켈리, 위대한 일을 하는 또 다른 방식","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Flounder 모드 – 케빈 켈리, 위대한 일을 하는 또 다른 방식

     * Kevin Kelly는 하나의 거대한 성취보다 다양한 창의적 프로젝트를 통해 삶을 이끌어온 인물
     * 그의 경력은 WIRED 공동 창립, Whole Earth Catalog 편집, WELL 커뮤니티 개설 등 IT·문화·예술·미래 분야에서 넓은 스펙트럼을 자랑함
     * 그는 열정과 관심사를 따라 할리우드 스타일로 일함을 중시하며, 극단적 위대함보다 지속 가능한 기쁨 및 만족을 중요하게 여김
     * 경력의 명확한 목적이나 전통적인 성공 경로에 집착하지 않고, 오히려 다양한 실험과 실패를 받아들임
     * 이 글의 저자는 실리콘밸리의 ‘유니콘’ 중심 성공 신화에 회의감을 느끼고, Kelly가 보여준 ‘즐겁고 자유로운 일하기’ 방식에 깊은 영감을 받음
          + ‘관심사에 충실하고, 목표보다는 방향을 좇는 삶’ 의 가치를 Kelly와의 대화와 삶에서 재발견함
          + 진정한 영향력과 행복은 ‘극단적 위대함’이나 고통이 아니라 ‘지속적인 호기심, 에너지, 기쁨’을 품고 일하는 태도에서 온다는 점을 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Kevin Kelly와 ‘Flounder Mode’의 삶

     * 케빈 켈리는 단일한 '거대한 성취'로 알려진 인물이 아니며, 스스로도 그러한 것을 추구하지 않음
     * 역사상 상징적인 기업가들처럼 영리함, 근면함, 야망, 그리고 예측력을 지녔지만, 유니콘 기업 같은 거대한 목표에 특별한 관심이 없었음
     * 그는 자신만의 방식인 ‘할리우드 스타일’ 로 다양한 창의적 프로젝트에 참여하며 경력을 쌓음
          + Whole Earth Catalog 편집자, WELL(초기 온라인 커뮤니티) 공동 설립, WIRED 공동 창간
          + 스티븐 스필버그 영화 ‘마이너리티 리포트’ 미래자문, ‘퓨처라마’ Death Clock 영감
          + 자전거로 미국 횡단, 아시아·유럽 도보 여행 주최, 예술·사진·조각 등 폭넓은 창작 활동 지속
          + Stewart Brand와의 협업 및 우정, “Stay hungry, stay foolish”의 원작자와 오랜 관계 유지
     * 장기적 관점과 집단적 성장
          + 서부 텍사스 산속에 1만 년 동안 작동하는 시계를 건설하는 프로젝트에 참여(Brian Eno, Jeff Bezos 등 협업)
          + 생물종 분류 및 기록 운동(현 스미소니언 운영)의 중심 인물
          + Quantified Self(자기계량 운동) 개념의 초기 제시자이며, 이것이 Fitbit, Strava, Apple Watch 등 다양한 라이프로그 제품 출시로 이어짐
          + ‘1000명의 진정한 팬(1000 True Fans)’ 이론을 통해 크리에이터 이코노미의 기반을 제공

     ""자신의 관심사에서 전설이 된 사람들은 결코 자신이 이미 성공했다고 느끼지 않습니다."" - 케빈 켈리

     * 동료들의 평가
          + Naval Ravikant: “현대판 소크라테스”
          + Marc Andreessen: “무엇이든 켈리가 쓰는 글은 읽을 가치가 있다”
          + Brian Eno: “기술과 문화에 대해 꾸준히 도발적 사고를 던지는 인물”
          + Ray Kurzweil: “기술의 방향을 이보다 잘 아는 사람을 알지 못함”

실리콘밸리 경력과 ‘비정형 경로’의 자부심

     * 이 글의 저자 Brie Wolfson은 Colossus와 Positive Sum의 CMO
     * Brie는 Google 입사 후 빠른 실적을 올렸으나, 전통적 커리어 레일이 아닌 ‘재미를 좇는 경로’ 를 선택함
          + Google 퇴사 후 스타트업, 게임회사, 자원봉사, 소설 집필 등 ‘릴리패딩(lily-padding)’ 방식 커리어 추구
          + Stripe 합류 후 5년간 전통적 승진이 아닌 조직문화 프로젝트, 내부 혁신, 다양한 영향력 확대에 집중
          + 승진이나 타이틀보다 ‘범주화하기 힘든’ 이력을 자랑스럽게 여김
     * 점차 주변의 ‘성공’ 동료들과 자신을 비교하며 불안과 회의감 경험
          + 동료들은 승진, 팀 빌딩, 인더스트리 리더로 자리잡는 반면, 자신은 ""나는 뭘 잘하지?""라는 고민에 직면
          + 가족·사회적 인정과 개인적 성취 사이에서 고민
     * Kelly는 “불투명한(‘illegible’) 경력 경로가 오히려 흥미로운 일을 하고 있다는 증거”라고 조언

Kevin Kelly와의 만남: 호기심이 가득한 공간

     * Kelly의 Pacifica 스튜디오 방문
          + 실제로 쓰이는 책, 레고, K’nex, 진열된 잡동사니 등으로 가득 차 있음
          + Kelly는 추억에 집착하지 않고, 모든 사물의 현재적 의미와 이야기에 초점을 맞춤
     * 대화 속에서 드러난 Kelly의 일하는 방식
          + ‘관심사 따라가기’는 단순한 구호가 아닌 자신만의 방향성임
          + “목적지가 아닌 방향을 좇음”, “행동(doing)은 곧 학습(learning)”, “프로젝트는 곧 과정”이라는 태도
          + 열정만으로 충분한지 묻는 질문에는 “돈을 버는 것은 흥미로운 이유 중 가장 덜 흥미로운 것”이라 답함
          + Walt Disney의 “우리는 돈을 벌기 위해 영화를 만드는 것이 아니라, 더 많은 영화를 만들기 위해 돈을 번다” 인용
     * 실패와 유연성, 기쁨
          + “내일도 계속 나아갈 수 있다는 용기”, “버릴 수도 있고 실패도 견딜 수 있음”이 핵심
          + 한 가지에 집착하는 ‘위대함’보다 균형감과 다양성을 더 중시

실리콘밸리 성공 신화에 대한 회의

     * 투자자, 창업가, CEO들이 강조하는 ‘고통, 집착, 극단적 위대함’ 의 성공 문화와 Kelly의 철학 비교
          + Facebook, Reid Hoffman, Jensen Huang 등 업계 리더들의 ‘위대함-불균형’ 신념 비판적 조명
          + Elon Musk, Sam Altman 등 ‘고통의 성공담’과 대비해 Kelly는 ‘내적 만족도와 기쁨’을 중시
          + “좋은 날을 많이 갖는 것(have a good day, most days)”이 실제로는 일과 성취의 진정한 기준이 될 수 있음
     * Kelly의 긍정적 에너지와 ‘즐거운 몰입’ 이 그의 독특함의 핵심임을 깨달음
          + 진정한 성공은 고통·집착·희생이 아니라, 기쁨·에너지·공유의 과정에서 나온다고 봄
          + “관심사를 좇을수록, 그 우물은 바닥이 없다”는 Kelly의 조언

새로운 롤모델의 필요성

     * Brie는 Kelly 같은 행복하고 건강한 롤모델이 더 많이 존중받는 세상을 희망함
          + “에너지가 넘치고, 정서적으로 안정되어 있으며, 가족과 관계가 좋은 이들”
          + 위대함과 영향력을 갖되, 타인에게 해를 주지 않고 즐거움을 공유하는 리더상 강조
     * Kelly에게서는 ‘야망과 기쁨의 공존’ 을 경험
          + “위대한 일을 하면서도 행복할 수 있는 삶”이 가능함을 실감
     * 마지막으로, Kelly와의 하루에서 저자는 “관심사를 따라갈수록 그 우물은 바닥이 없다”는 깨달음을 얻음

   케빈 켈리의 68가지 조언
   케빈 켈리의 99가지 추가 조언
   케빈 켈리의 알았더라면 좋았을 103가지 조언
   50년간의 여행에서 얻은 팁들

        Hacker News 의견

     * 내가 새로운 역할을 맡자마자 Kevin Kelly에게 연락해서 만날 수 있는지 물어봤음 (그가 나를 알지 못할 거라 생각했지만 사실은 알고 있었음). 기술에 대해 낙관적인 시각을 어떻게 전달하면 좋을지 논의하고 싶었음. 나는 여전히 기술이 가져다주는 기회와 발전을 긍정적으로 보는 사람이며, 그걸 어떻게 설득력 있게 전파하는지 점점 더 고민하는 중임. KK는 즉시 만남을 흔쾌히 수락했고, 그의 타워에 방문했으며, 아주 광범위하고 깊은 대화를 나눴음. 이 글의 저자처럼 나 역시 그의 말에서 큰 자신감을 얻었고, 그 메시지가 세상에 필요하다는 생각을 다시 하게 됨. 지난 3년간 이 대화 하나가 내게 가장 큰 힘이었음. 그래서 이 자리를 빌려 뒤늦게라도 KK에게 고마움을 전하고 싶음
          + 여기 계신 김에 아직도 어디에 글을 쓰거나 활동하시는지 궁금함. 오랜 팬임. Oblomovka 사이트는 지금 접속이 안 됨
     * 이 글은 상쾌한 자극임. 기술 분야는 정말 빠르게 성장하고 훨씬 더 ""프로페셔널""해졌지만, 이로 인해 ""기업 인간""으로 변해버리는 것도 많은 듯함. 나 역시 예외 아님. 나이가 들고 책임이 커지면서, 직급이나 평가, OKR 등만 쫓다가 이 산업이 무엇 때문에 멋있는지 놓치는 것 같음. 방향성과 나만의 관심사, 만들어야 한다고 느끼는 것에 집중하라는 좋은 리마인드임. Stripe Press 출판물을 좋아하다가 “The Art of Doing Science and Engineering” 도서 서문도 접하게 됨. 목적지가 아니라 방향을 택하면 더 흥미롭고 열정적인 사람들을 만날 수 있음
          + Brie, 프로필 저자임. The Art of Doing Science and Engineering 언급이 재밌음. 사실 초고에는 You and Your Research라는 책의 각주가 있었는데 편집 과정에서 빠졌음. Stripe Press 론칭에도 참여하며 해당 도서 판권 찾는 작업도 직접 했었기 때문에, 이 책을 통해 내 에세이를 만난 분을 보니 정말 뿌듯함
     * Brie, 에세이 정말 훌륭하고 공감됨. John Seeley Brown을 알게 되었을 때도 비슷한 감정을 느꼈음. 그의 웹사이트 약력을 읽었을 때 너무 편안해졌음. 워낙 다양한 관심사와 흥미로운 일을 했지만 중심에는 언제나 그 사람 자체와 그의 흥미가 있었음. 한 번 그의 커리어에 대해 물었더니, “내게 커리어가 있나?”라며 상당히 어색해했음. 만약 당신이 10년 뒤 후속 에세이를 쓴다면 꼭 읽고 싶음. 요즘 나 역시 “관심사를 따르는 것”이 초능력인지, 아니면 그냥 산만함이나 지루한 과정을 회피하는 핑계에 불과한지 많이 고민 중임. 최근 내 평가로는 좀 더 정착하는 게 좋았을 것 같음. 멋진 사진들과 Kevin과의 이야기, 즐겁게 읽었음
          + “장기적인 임팩트를 남기려면 지루한 부분도 반드시 지나가야 한다”라는 관점에 대해 나는 그다지 신경 쓰지 않음. 사후에 영향력이 어떻게 남을지 진짜 예측하기 어렵고 백 년 후엔 우리 모두 잊힐 것임. 대신, 내가 유용한 일에 행복하게 참여한다면 결과적으로 세상에 유용한 무언가가 남는다고 생각함
     * 이 글이 Kevin Kelly에 관한 이야기인지, 자서전인지 잘 모르겠음. 겉으로는 전자인 것처럼 보이지만 실제로는 저자의 커리어 이야기 위주임. 인터뷰 형식처럼 보이지만 Kelly의 직접 인용은 몇 문단 정도에 불과하며, 그것도 대부분 장난감 수집 얘기임. “Flounder mode”라는 접근법의 가치에 대해 Kelly가 자신의 생각을 길게 내놓는 Q&A를 기대했으나, 실제로는 거의 나오지 않음. 인터뷰의 서두만 잔뜩 있고 제대로 된 본론은 없는 느낌. 직접 Kelly와 온종일 대화를 하고, 우상의 집도 방문했지만, 정작 Kelly가 무슨 이야기를 했는지는 거의 안 나와서 아쉬움
          + 처음엔 Kevin Kelly에 대한 글인 줄 알았음. 그런데 중간쯤 읽다 보니 자서전 느낌도 나고, 후반부에 가서야 저자가 Kevin Kelly와의 만남을 이야기하고 있었다는 걸 알게 됨. 글 자체는 쉬운 언어를 썼지만 정확히 무슨 얘긴지 너무 복잡하게 느껴짐
          + 저자의 삶의 철학이 Kelly에게서 영향 받은 관점에서 쓰인 자서전임. 기대했던 것보다 훨씬 흥미롭게 읽음
          + 처음부터 끝까지 요약하려고 LLM에 맡겨서 시간 낭비를 피함. 정작 “flounder” 개념 자체는 기사에 설명되어 있지 않음
          + Kevin Kelly의 명언 모음이 필요하다면 구글에서 바로 찾을 수 있음. 때때로 독자가 약간의 수고를 해야 하는데, 이번 경우는 ‘Flounder’가 뭔지 스스로 생각해봐야 함. 진짜 물고기처럼 허둥대다 실수하는 느낌으로 쓰인 단어로 보임. 실제로 저자가 자신의 커리어에서 이리저리 떠돌며 흥미로운 걸 탐구한 이야기에 잘 맞는 어감임
          + Kevin Kelly는 정말 멋진 사람으로 보이지만, 나 역시 “flounder mode”가 정확히 뭔지 정의를 듣고 싶었음
     * 나도 KK처럼 일과 커리어, 기술에서 색다른 접근을 한 사람들의 이야기를 더 듣고 싶음. 그런데 단순히 관심만 따라가면 경제적 자유와 일의 자유에 이르는 건 “생존자 편향”이 작동하는 것 같음
          + 당연히 생존자 편향이 존재함. 모두 생계를 걱정해야 하기에 저항 없이 직업을 택하고 주어진 일을 하면서 살게 됨. 특이한 선택이나 대안적인 길을 발견하는 게 기본적으로 훨씬 더 어려움. 그러나 산업화와 개인주의 문화 덕에 이런 길도 있기는 함. 주변을 둘러보면 생각보다 이런 삶을 사는 사람이 많지만, 대중적 성공과는 달리 유명하지 않을 뿐임. 특별한 취향은 소수의 영역이고, 모두가 물질적 성공에는 관심이 있으니 자연스러운 현상임
          + 단순히 “관심을 따르라”는 건 아님. 스스로의 관심사에 더 깊이, 일처럼 몰입하는 방법을 배우는 게 중요함. 억지로가 아니라 자신의 일에서 발휘하는 성과, 집요함, 퀄리티 기준을 관심사에도 적용하면 관심사가 스스로 영향력을 가지게 됨
          + 나도 대학 졸업 후와 첫 직장 몇 군데를 다니다 극적으로 방향을 틀었음. 지금 훨씬 행복함. 미국 의회에서 1년 미만 근무, IT 아웃소싱, 컨설팅을 경험했는데 너무 따분하고 무의미하며 지루해서 새로운 나라로 이주함. 지금은 여러 비즈니스를 운영하고 30명 넘는 직원을 거느림. 재미있고, 가끔 스트레스도 있었지만 37살에 결국 스트레스를 이겨냄. 이제는 그냥 재미있게 일하며 마음 가는 대로 새로운 걸 시도함. 다만 대부분의 사람들은 도중에 포기하거나 불편함을 감수하지 않음. 흔들리지 않고 자기만의 길을 걷는 게 사실 엄청 불편함. 나는 관심사를 따른 게 아니라, 내 앞에 놓인 일에 관심사를 활용해서 더 잘하려고 했음. 현실적인 생계와 요구도 충족시켜야 하고, 내가 거기에 나만의 스타일을 입힌 것임
     * 미국 내 젊은이로서의 가장 큰 걱정은 자신이 일을 아주 잘하지 않으면 점점 더 가난해질 거란 생각임. 저렴한 식용유와 흰 밀가루로 만든 음식, 회색 바닥과 콘크리트 아파트, 범죄 많은 동네, SNS에 휘둘리는 사람들, 1시간 가까운 출퇴근이 당연한 삶. 반면 진짜 자연, 현재에 집중하는 사람들, 질 좋은 음식 등은 점점 손에 닿지 않는 것 같음
          + 심리학에서 ‘분열적 사고’(splitting, 극단적 이분법적 사고)라는 개념이 있는데, 뭔가를 극단적으로만 바라보는 성향임. 가장 좋거나, 가장 나쁘거나 둘 중 하나만 있다고 여기는 사고인데, 요즘 젊은 세대가 이 개념을 스스로의 일상에 자연스럽게 받아들이는 듯함. 실제 세상엔 극단 사이 수많은 가능성이 있음. 음식에 대한 집착도 좀 의아한데, 실제로는 단순한 요리법만 알아도 저렴하고 건강하게 먹을 수 있음. 인터넷에서 너무 많은 관점을 받아들이다 보면 현실감각이 흐려지니, 잠시 인터넷을 멀리하고 현실에 다시 집중하는 경험이 필요함
          + ""최고가 되어야 한다""라는 건 제로섬 프레임임. 모든 사람이 1등이 될 수는 없음. 대신 자신이 중요하게 생각하는 분야에서 진지하게 노력하면 매우 잘할 수 있게 되고 그걸로도 충분함
          + 와, 생각보다 암울함. 사실 곰보다 빨리 달릴 필요는 없음. 그냥 곰에게 쫓기는 다른 사람보다 빨리 달리면 됨. 미국 대기업에서 기준이 워낙 낮아서 조금만 자기 일에 진지하게 임하면 이미 크게 앞서나가는 셈임. 업계 관련 책 한두 권만 읽어도 무척 유리함. Hacker News 식 허상에 속지 말라는 조언임. 스타트업 성공 없이도 백만장자가 된 사람이 정말 많음
          + 나 미국 출신은 아니지만, 여러 번 방문하고 미국의 생활수준을 읽어보면 상당부분 공감되는 현실임. 이스라엘에서 살면서 우리가 그렇게 극단적인 환경이 아니라는 사실에 감사함
     * 저자의 개인적 여정과 내 삶을 비교하며 큰 공감과 영감을 받음. 2년 전만 해도 야망 있는 스타트업 창업에 집착했지만, 이제는 소프트웨어를 만들며 세상에 긍정적으로 기여하는 방법에도 가치가 있다고 생각함. KK가 한 말처럼 “돈이 무언가에 관심을 갖는 가장 흥미 없는 이유”라는 의견에 진심으로 공감함
     * 정말 영감을 받는 글임. 대기업처럼 조직이 크고 프로덕트 로드맵이 실망스럽거나 아예 존재하지 않는 곳에서는, 실제로 특별한 프로젝트나 성장 기회가 틈새에서 생김. 흔히 전략팀이 멋진 새 아이템을 찾아줄 거라 기대하지만 실상은 그렇지 않음. 지금 있는 프로덕트에서 흥미를 찾아 거기에 집중하면 자신만의 길을 만들 수 있음
     * 이 글을 정말 재미있게 읽었음. Ryan Norbauer가 왜 회사를 팔고 진짜로 재미있는 일에 집중할 수 있었는지 쓴 글(The Outsider Option)이 떠오름. 나 역시 일과 취미를 이런 방식으로 융합해보고 싶고, 이런 도전이 가능하다는 사실에 감사함
     * 이 글에는 배울 점이 참 많음. 특히, 일에 대한 낙관과 열정을 보여주기 위해 ‘허락’을 받아야 한다는 느낌에 공감됨. “위대함은 과대평가된 것”이라는 말도 마음에 듦. 스티브 잡스, 밥 딜런이 예외적인 창업가나 아티스트여도 그다지 존경스럽지 않은 구석이 있다는 의견에 나도 동의함(나만 그런 게 아니라서 안도감이 듦). 팀 리더로서 하나 배운 점은 “만약 일상에서 힘든 부분이 있다면 일단 대화를 시작하자”는 태도임. 이게 해결책의 전부는 아니지만 조직 문제의 많은 부분을 드러나게 만드는 효과적 방법임
          + 이 마인드 정말 중요함. 옛날엔 나도 성취와 업적만 보고 잠도 줄이면서 “무조건 위대해야 한다”고 생각했음. 근데 지금은 서두르지 않고, 나만의 진짜 원하는 것이 뭔지 고민하는 시간이 더 소중함
"
"https://news.hada.io/topic?id=21829","NVIDIA가 이겼고, 우리는 모두 졌어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        NVIDIA가 이겼고, 우리는 모두 졌어요

     ""NVIDIA is full of shit : NVIDIA는 더 이상 믿을 수 없어요""

     * RTX 50 시리즈 출시 이후 NVIDIA는 고질적인 물량 부족, 전원 커넥터 결함, 드라이버 품질 저하, 결함 칩 출하, 미디어 통제 등 여러 부정적 이슈에 휩싸여 있음
     * 정가(MSRP)와 실제 구매가 괴리, 중고가 폭등, 번들 판매 등으로 소비자 부담이 가중되고, 일부 카드엔 처음부터 누락된 ROP 등 결함까지 발생함
     * 12VHPWR 전원 커넥터 설계 결함이 RTX 40·50 시리즈에서 계속되고 있으며, 이는 사용자의 사소한 실수에도 심각한 전원 문제와 GPU 손상 위험을 초래함
     * DLSS·CUDA·NVENC 등 폐쇄적 생태계와 독점 기술로 인한 락인 현상이 심해지고, 가격 대비 성능·그래픽 품질 개선폭은 매우 제한적임
     * NVIDIA는 리뷰어 위협, PR 통제, 드라이버 문제 방치 등 시장 지배적 위치를 남용하며, PC 게이머와 소비자가 실질적인 피해를 입고 있다는 비판이 커지고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

RTX 50 시리즈 출시와 가격 문제

     * RTX 50 시리즈는 출시와 동시에 Scalper bot의 구매 싹쓸이, 재고 부족, MSRP(정가) 무의미화 현상이 반복되고 있음
          + Scalper bot : 인기 상품이 온라인에 출시될 때 일반 소비자보다 훨씬 빠르게 자동으로 주문을 넣어 제품을 싹쓸이 구매하는 자동화 프로그램
     * 유통사는 초기 입고량이 극히 적다고 얘기하고 있으며, NVIDIA가 의도적으로 재고를 꽉 조여 수요과잉 이미지를 연출하고, 가격을 오히려 높임
     * 실제 판매가는 MSRP 대비 1.5~2배에 이르며, 일부 유통사는 강제 번들 판매로 가격을 더 부풀림
     * RTX 40 시리즈 조차도 가격이 여전히 높아, 동일 가격대에서는 AMD GPU가 더 높은 성능을 제공하는 경우도 많음
          + 예: GeForce RTX 4070(정가 $599) → 실제 거래가 $800
     * RTX 50 시리즈 초기 제품에서 공장 출하 시 ROP 누락 결함 발생 및 NVIDIA는 성능 저하 이슈를 인정하고 교환 해줌
     * 전체적으로 세대 교체의 성능 향상 폭이 미미하며, 이전 세대보다 가격 상승이 더 두드러짐
     * NVIDIA는 소비자 시장보다는 데이터센터용 GPU 매출에 집중하며, 일반 사용자 대상 판매는 대책 없이 방치하고 있음

전원 커넥터의 치명적 결함

     * RTX 50/40 시리즈는 12VHPWR 커넥터를 사용, 전원선이 녹아내리는 문제가 지속됨
     * 이 문제는 보드 설계 결함으로, 단순 커넥터나 케이블 교체만으론 해결 불가
     * 기존 30 시리즈까지는 각 전원입력마다 개별 숀트 저항이 있었으나, 40 시리즈 이후 병렬 연결로 변경, 불균형 전류 감지 불능
     * 일부 핀만 제대로 연결되어도 과전류로 인한 발열/융해 발생
     * NVIDIA는 문제 인지 후 숀트(shunt) 저항 일부 삭제 등 미봉책만 시행함

     * 12VHPWR 설계상의 한계로 단일 커넥터에 최대 600W 공급이 가능하지만, 과부하 시 안전성 극히 저하
     * “센싱핀” 기능도 시동 시 전력 한도만 인식, 실시간 감시 불가로 설계상 한계 존재
     * 커넥터가 제대로 결합되지 않을 경우 핀이 불량 접촉 → 전류 과부하 → 융해 순서로 진행
     * 케이스 호환성 및 조립 편의성도 고려되지 않아, 커넥터가 심하게 구부러지면 문제 악화

     * 일부 보드 제조사는 개별 핀별 숀트(shunt) 저항 추가 등 자체 개선책 도입
          + 근본적 문제는 여전히 미해결

NVIDIA의 폐쇄적 생태계와 락인 전략, 성능 마케팅

     * DLSS, CUDA, NVENC, G-Sync 등 NVIDIA만의 독점 기술로 자사 하드웨어 사용을 강제
     * 생태계 락인으로 타사로의 전환이 물리적, 금전적으로 어려운 구조가 조성됨
     * CUDA, NVENC는 콘텐츠 제작·영상 편집 등에서 필수적인 도구
     * G-Sync는 GPU/모니터 동시 락인 구조, 모니터 제조사 인증 및 추가 비용 발생
     * 경쟁 표준인 FreeSync도 일부 호환되나, G-Sync Premium 이상의 차별화로 가격 프리미엄 유지

     * RTX 50 시리즈는 PhysX 32비트 미지원 등으로 PC의 대표 장점인 하위호환성 무력화
     * 과거 게임(예: Mirror’s Edge, Borderlands 2)은 심각한 성능저하 발생
     * NVIDIA는 PhysX GPU 커널 소스코드 오픈소스로 공개해 자체 지원 부담을 회피하는 행보를 보임

     * DLSS(딥러닝 업스케일링)에 대한 논란
          + 최초 출시(RTX 20 시리즈) 때부터 강력한 마케팅 및 고가 모델 지원 강조
          + 실제로는 실제 렌더링 성능 한계를 소프트웨어로 보충하는 형태
          + 수많은 AAA 게임에서 “원활한 퍼포먼스 → DLSS 필수” 구도로 기술을 강제
               o 원본 해상도와는 다른 “가짜 해상도”, “프레임 생성” 등 혼란 유발
               o 영상 품질 하락, TAA의 번진 듯한 화면, 인풋랙 등 부작용 존재
          + DLSS 프레임 생성은 ""진짜"" 프레임이 아니며, Reflex 등 추가 ""솔루션""을 요구
          + 실제 퍼포먼스 향상은 미미한데, 제품 가격은 2배 이상 상승
          + 2025년 최신 게임조차도 본질적으로 10년 전 대비 큰 시각적 차이 없음
               o Monster Hunter Wilds는 1080p/60fps도 프레임 생성 기술이 필요하다는 최소사양 명시
          + NVIDIA는 발표장에서 RTX 5070이 4090 수준이라지만, 실상은 DLSS 기능을 극도로 활용한 결과임
     * 시장에서는 성능·화질 모두 조작된 수준으로, 실제 하드웨어의 실력을 가리고 있음

미디어 통제·리뷰어 압박

     * NVIDIA는 그래픽 카드 리뷰에서 DLSS, ray tracing 부각을 요구하며 미디어/리뷰어를 압박함
     * ""Hardware Unboxed"" 등 주요 리뷰 채널은 회사 방침과 맞지 않으면 제품 샘플 지급 중단을 통보
     * Gamers Nexus에 대해서도, 메트릭스(성능지표) 포함 강제 조건 등 보도 자유성 침해 시도
     * RTX 5060 출시 당시에는 드라이버 미제공, 사전 리뷰 차단, Nvidia 입맛대로 체리픽된 미디어만 제공 등 불공정 출시 절차를 따름
     * 최근 드라이버 잦은 결함 및 핫픽스 남발도 이슈, 최신 카드 이용자는 구버전 롤백조차 불가

시장 지배와 소비자 피해

     * PC GPU 시장에서 90% 이상의 점유율을 바탕으로 NVIDIA는 독점적 위치를 이용해 가격·사양·기술 적용·리뷰 방향을 통제함
     * AMD·Intel 등 경쟁사 대비 기술 락인, 미묘한 성능 차이, 생태계 종속으로 대체재가 사실상 제한적임
     * 데이터센터, AI, 마이닝 등 비소비자 시장에 매출 주력하고, 일반 소비자 제품 질 저하
     * 세대 교체에도 실질적 성능 향상은 제한적이고, 소비자 선택지는 오히려 좁아지는 현상 지속
     * 게임 개발사·생태계 전체가 NVIDIA 위주로 돌아가며, 오픈스탠다드·호환성·경쟁 환경 약화
     * 과거에는 기술적 우위로 정당화할 여지 있었지만, 현재는 시장권력만으로 지속적 논란과 공격적 가격 정책을 유지 중임
     *

결론

     * NVIDIA의 독점적 전략, 기술 락인, 반복되는 결함·고가 정책은 장기적으로 PC 그래픽 생태계의 건전성·혁신에 부정적 영향을 줄 것
     * 단기적으로는 대체재 부재와 폐쇄적 생태계로 인해 소비자 선택지가 줄고 실질적 피해가 누적됨
     * 근본적인 변화가 없는 한, 사용자는 고가·불안정·폐쇄적 시스템에 계속 의존할 수밖에 없음

        Hacker News 의견

     * 전 세계 PC 시장의 90% 이상이 NVIDIA 기술을 사용한다는 점에서, GPU 경쟁의 승자는 명확히 NVIDIA임을 체감함. 하지만 진정한 패자는 우리 모두라고 생각함. AMD GPU를 리눅스 커널에 드라이버가 공식 지원된 이후로 계속 사용해왔는데 후회는 없음. 세상에는 비디오 게임보다 더 많은 것이 있고, 게임 때문에 화를 내는 일에 시간이나 에너지를 쓰고 싶지 않음. 결국 소비자들이 소비에 매달리고 착취당하면 분노하면서도 떠나지 않고 계속 소비만 하는 현실을 봄. Magic the Gathering 등의 다른 취미도 마찬가지임. 게임이 망했어도 여전히 수천 달러를 쓰는 사람들이 많음. 나는 그냥 그만둠.
          + 비디오 게임 외에도 다양한 즐길 거리가 있다는 말이 인상적이었음. 내 주요 취미가 비디오 게임이지만, 리눅스에서 대부분의 게임이 잘 돌아가고 (AMD 지원이 좋아서) 굳이 신경 쓸 필요는 못 느낌.
          + AMD도 게임에서 나쁘지 않은 성능을 보임. 다만 PyTorch만 지원이 부족해서 아쉬움.
          + 나는 게이머가 아니라서 왜 AMD GPU가 충분히 좋지 않은지 이해가 잘 안 됨. Xbox와 PlayStation 모두 AMD GPU를 사용하는데 이상함. 아마도 PC에서만 NVIDIA로만 할 수 있는 게임들이 있는 듯함. 그렇다면 왜 대형 시장인 콘솔을 무시하고 PC 전용 게임을 만드는지 의문임.
          + 내 7900xt 20GB에 무척 만족함. 대부분의 추론 모델도 Vulkan으로 잘 돌리고, 리눅스에서 Wine이나 Steam+Proton으로 게임도 문제없이 즐김. 가성비도 훌륭함.
          + AMD를 좋아하고 싶으나, 약간 평범함. 게임에서도 약간 뒤처지고, 기계학습(ML)에서는 훨씬 더 약함. 리눅스 통합은 잘 돼 있지만, 실제로 AI 산업은 모두 Nvidia 카드를 리눅스 박스에 연결해 사용함. 그래서 NVIDIA도 바이너리 블롭 드라이버만 빼면 리눅스에서 잘 작동함을 느낌. 게이밍·ML 말고 GPU에 큰 돈을 쓸 만한 가치를 못 찾겠음. AMD는 현실적으로 어려운 위치에 있음.
     * 글 제목이 바뀐 이유가 궁금함. 원래 기사와 전혀 맞지 않는 제목으로 바꾼 것은 HN 가이드라인에 위배된다고 생각함. HN 팀이 바꾼 듯함.
          + ""Nvidia won, we all lost""라는 변경된 제목은 원작자가 의도한 분위기와 전혀 다르게 만든다고 느낌. 특히 'NVIDIA가 시장 지배력을 이용해 리뷰어와 내러티브를 통제한다'는 내용의 기사인데, 이를 편집해놓은 걸 보면 HN 팀이 Nvidia를 보호하거나 어떤 압력을 받았는지 의심이 생김. 때로는 단순한 실수일 수 있지만, 이런 경우는 더 걱정스러움. 모더레이터가 제목을 바꿨다는 자체가 큰 문제임.
          + 제목 변경의 목적은 토론을 좀 더 건설적이고 생각이 깊은 방향으로 이끌기 위한 것으로 보임. 과격한 제목은 토론을 쉽게 감정 싸움으로 만들 수 있으므로, HN 모더레이터는 이를 막으려 많이 애씀. 해당 주제가 숨겨진 것은 아니므로, 만약 아이디어 억압이나 Nvidia 보호가 목적이었다면 차라리 주제 자체를 감추는 게 더 효과적이었을 것임.
          + Nvidia와 AI에 엄청난 투자가 이뤄지고 있어서 지금은 Nvidia와 AI에 대한 과장된 열광이 유지되어야만 함. 실제와는 다르게라도 이 분위기를 계속 떠받들어야 한다는 것임. 언젠가 실체가 드러나서 매도 장세가 시작되면 그때서야 분위기가 바뀔 것임.
          + 새로운 글로벌 의제를 아직 이해하지 못했는지 물으며, 결국 가이드라인과 규칙도 '주인'들을 위해 존재한다고 이야기함.
     * 마이크로소프트의 인수 행보와 수십억 달러 IP 파괴가 업계 전반에서 계획적으로 짜여진 듯 보임. 이는 게임 산업의 인위적 붕괴로, 시장 주도권을 중앙 집중화하고 서비스 기반(임대 중심) 구조로 바꾸려는 의도임. 물론 모두 한자리에 모여 동의한 건 아닐 수 있으나, 업계 주요 인사들의 의견이 대체로 일치함. 시장이 축소되면, 상황이 의도였든 무능이든 저렴하게 시장 전체를 인수해 진입장벽을 높이고 요금을 매길 수 있는 기회가 됨. 이는 Nvidia의 드라이버 최적화, 비트코인/AI, 용량 선점으로 경쟁사 진입을 막는 현상과도 연결됨. 단, Valve가 이 퍼즐에서 어떻게 움직일지는 예측이 어려움. 큰 기회가 있지만, 이미 시장이 포화됐다고 읽고 관망할 수도 있음.
          + 블리자드가 오버워치1을 종료한 이유는 게임계에도 린디 효과가 작용하기 때문임. 너무 인기 많고 오래된 게임들을 대체하려면 거의 '만행'에 가까운 조치가 필요함. 긴 시간 동안 기존 타이틀이 여전히 인기라면, 같은 장르의 신작을 내도 성공하기 힘듦. AAA 스튜디오 지표로 보자면, 옛날 게임 구매 후에도 ARPU가 사실상 0이었고, 강렬한 게임을 계속 무료로 주면 다른 신작의 수요가 사라짐.
          + 비디오 게임 산업은 이런 식의 흥망성쇠 사이클을 여러 번 겪어왔음. 1983년 대붕괴도 있었고, 2010년쯤 RTS 장르의 쇠퇴도 겪었음. 매번 비슷한 패턴이 반복됐고, 관련 기업들이 원하거나 예측한 대로 돌아간 적은 별로 없었음.
          + Valve는 비상장 회사라서 성장 최우선 강박이 약함. Microsoft는 주가가 전부임.
          + Valve도 자금/인재는 넉넉하지만 변화의 결과와 고객 혜택을 생각하지 않으면 무책임한 혼란만 일으킴. Valve의 주요 사업은 Store이고, 나머지 프로젝트는 스토어를 통해 구매 유도하는 손실 리더 같음. 리눅스 관련 노력도 사실상 Steam Deck과 연계돼 있는 것이며, Deck APU도 AMD의 콘솔용 반주문 결과물의 파생이니, 결국 Valve는 근본적으로 스스로 아예 새로운 기술 생태계를 여는 것보다, 고객과 파트너 지원에 집중함.
          + 게임 산업 붕괴가 의도적이라고 해도, 효과가 없을 거라 생각함. Microsoft 인수도 싫지만, 만약 Microsoft가 사라진다 해도 게임 산업은 잘 살아남을 수 있음.
     * Jensen은 자신의 GPU와 기술로 (하드웨어, 소프트웨어 모두) 모든 산업 붐에 적시에 들어가는 데 성공했음. 다음 붐에도 분명히 등장할 것임. Microsoft는 실패를 반복하지만, 결국 방대한 사업 부문의 힘으로 상황을 만회함. Apple은 대단히 늦지만, 압도적인 어스토로퓨(가짜 소비자)에 힘입어 따라 옴. AMD는 규모가 작아 모든 분야를 따라잡긴 힘들지만, 나름 빠른 추격자이고, Intel은 왜 이 상태인지 신기할 정도지만, 그 몰락이 나는 꽤 기쁨. NVIDIA에 대한 분노는 그들이 선두주자였음을 충분히 인정하지 않는 것임. 모두가 기억해야 할 점은 NVIDIA가 직접 지배하는 시장을 개척했다는 것임.
          + Intel의 몰락이 왜 좋은지 궁금함. 칩 설계 회사가 더 많은 것이 좋음.
          + GPU/AI 분야에서는 '붐'을 예측한 게 아니라, 오히려 적극적으로 그 붐을 만든 것일 가능성이 큼. 특히 AI가 본격 시작되기도 전에, 수치연산(GPU 컴퓨트)에 크게 투자(2004년 Ian Buck 고용)하고 이를 연구 커뮤니티에 홍보함.
          + Nvidia가 이 만큼 시장을 차지하는 이유는 실력이 뛰어나기 때문임. 반경쟁 행위에 대한 의혹도 없고, 시장은 충분히 열려있음.
          + 다음 '붐'이 무엇일지 솔직히 떠오르지 않음. 우리는 지금 정체의 시대에 진입했고, 앞으로 시장과 세계에 힘든 시기가 될 느낌임.
     * NVIDIA가 고의로 재고를 줄여 카드를 높은 수요처럼 보이게 하여 가격을 부추긴다는 소문이 있으나, 사실은 이상함. NVIDIA는 카드가 MSRP보다 높게 팔려도 더 많은 돈을 버는 것이 아님. 오히려 욕만 더 먹음. Scalper(되팔이)는 소매 유통 전반의 문제임. NVIDIA가 이를 막을 수 있다고 생각하는 건 착각임. 매장도 이런 상황을 싫어하고, 수백만 달러를 들여 되팔이 방지에 힘 씀. 되팔이가 늘어나면 고객이 경쟁사로 옮길 뿐임.
          + Scalping과 MSRP 미끼 장사는 너무 오래된 일임. NVIDIA가 완전히 결백하다고 할 수 없음. 특히 EVGA GPU 단종으로 NVIDIA가 파트너들과 계약관계에서 완전히 주도권을 가진 게 드러남. 대형 유통사들도 한계가 있고 NVIDIA가 직접 판매자가 아니라 해도 뭐가 벌어지는지 알면서 방조하는 느낌이 남.
          + Scalper가 소매 전체의 문제라는 말은 맞기도 하지만, 근본적으로 (a) 공장 생산 능력이 충분한데 수요 예측이 틀렸거나, (b) 생산이 부족하거나, MSRP가 실제 시장수요보다 너무 낮아서 Scalper가 차익을 챙기게 되었거나 둘 중 하나임. 근데 수년간 계속되는 것이면 의도적 전략임. 생산능력 확충이나 MSRP 인상 없이 매해 예측을 일부러 틀리는 것은 비효율임. 현재는 데이터센터 GPU가 주요 수익원이므로, 소비자용 GPU 재고 부족이 홍보·마케팅 효과(halo effect)만을 위한 전략이 됨. 실제로는 제품명성과 미디어 통제가 더 중요한 상황임.
          + 30 시리즈 때 스캘핑이 극심했기에, 40 시리즈 이상 가격이 그때 눈치 본 대로 비싸짐. 고가 추종이 반복되며 소비자들은 Nvidia GPU의 그만한 값을 점차 당연하게 여기는 학습효과가 생김.
          + Nvidia가 MSRP 초과에는 돈을 더 못 번다고 했지만, 그걸 우리가 어떻게 알 수 있을지 의문임.
          + Nvidia는 되팔이를 충분히 막을 수 있는 능력이 있음. Nintendo 처럼 막대한 재고를 사전 준비해 한 번에 출시하면 되팔이의 이득이 확 줄어듦. 예전에 내가 콘솔 되팔이로 이득을 봤는데, 닌텐도 제품은 공급이 넘쳐서 남기기 힘들었음. 이번 5090 출시 때도 소매점은 전국 단위로 몇 십~몇 백개 수준만 있었고, 바로 매진되어 가격만 뛰고 되팔이의 이득만 극대화됨. 제조 파트너들도 가격을 30~50% 올렸다함. PNY는 납득할 수 없는 가격 인상을 보여 줌. AMD 9000 시리즈 출시 때는 전매장에 몇 백 개씩 있었고, 점심 때까지 누구나 구매 가능, 즉시 재입고도 됨. 일부 고가 모델이 있었지만 Nvidia 처럼 50% 이상 오르진 않았음. 나도 5090 FE를 출시 때부터 꾸준히 사려 했지만 성공 못함. 현 세대 리뷰가 별로라 비싼 값을 들이기 싫고, 이젠 차라리 AMD에서 32GB VRAM 이상,
            합리적인 가격 제품이 나오길 기다리겠음.
     * 최근 고급 GPU가 매니아 제품에서 점점 럭셔리 제품으로 변신하는 중임. 5~10년 전만 해도 고급 GPU가 쓸만한 화질 설정에 필수였지만, 이제는 50만원($500)대 중급 GPU로도 충분함. '하이'와 '울트라' 설정, DLSS와 FSR, DLSS FG와 Lossless Scaling 차이를 구분하긴 어려움. 이제 50만원 구간에서 경쟁에 의미가 많이 사라져 Nvidia도 중저가 시장은 AMD 콘솔이나 통합 그래픽에게 넘기는 분위기임. 아마 Nvidia PC나 Switch 2에서 다시 흥미로운 변화가 올 수도 있음.
          + $500 GPU를 중급이라고 부르는 것 자체가 Nvidia 전략의 성공을 보여줌.
          + 10년 전에는 $650에 최상급(GeForce GTX 980 Ti) 구입 가능했음. 지금 $650은 RX 9070 XT 정도(그나마 정가 근처에서 구하면)로, 10년새 고급 가격대가 중급이 됨.
          + 2020년에 RTX 3060 Ti 장착 신형 PC를 샀는데, 아직도 충분하다고 느낌. 굳이 교체할 이유 없음.
          + AAA 게임만이 GPU의 진가를 보여줌. 나는 저렴한 걸 선호해서 아들은 인디게임이나 저사양 게임만 줘도 재미있게 잘 놀음.
          + 더 넓은 트렌드는 제품을 계속 럭셔리화하여 가격을 높이는 전략임. 주택도 고급 자재 몇 개 쓰면 가격을 올릴 수 있고, 차량도 레더 시트 옵션만 달면 브랜드(렉서스) 명으로 수십 퍼센트 인상함. 구독 모델 전환도 문제. 그래픽카드도 이 길을 가고 있음. 이제 마지막 프레임폭을 위해 사는 것은 벤틀리 사는 느낌임.
     * NVIDIA는 적어도 1~2년간 공급 제약에 시달릴 것임. TSMC에서 제한된 생산 설비를 사용해 대부분은 엔터프라이즈(데이터센터) 칩 생산에 할애하므로, 소비자용 칩 공급은 줄어듦. 기업용 판매로 돈을 벌고 있으니 컨슈머 시장을 소홀히 한다고 너무 화내기도 어려움.
          + 과장된 성능, 가격, 공급 문제 등으로 광고해놓고, 실제로 소비자 GPU는 우선순위가 아니었다는 것을 정직하게 밝힐 수 있었다고 생각함. 결국 사람들은 화가 남. 개인적으로 'Gamer's Nexus paper launch' 티셔츠가 마음에 듦.
          + 솔직하지 못한 점이 아쉬움.
          + 지난 분기 성장의 상당 부분이 실제로는 소비자 시장에서 발생한 것 같은데, 데이터센터 매출 증가가 기대보다 낮았다고 봄.
          + NVIDIA가 소비자 시장을 등한시 한다고 옹호하는 건 바람직하지 않음. 고객의 선택이 중요하고, 더 많은 소비자들이 스스로 목소리를 내야 함.
     * TSMC는 OpenAI와 AI 업체들이 원하는 만큼의 Nvidia 칩 생산량만큼만 만들 수 있음. Nvidia는 사실상 OpenAI용 칩 생산 이후 남는 잔여물로 소비자 GPU를 내놓는 상황이라, 공급이 매우 제한되고 가격도 높음. 게임 유저는 예전보다 더 비싼 값에 오랫동안 기다려야 하니 불만도 큼. 유투버들은 이 불만을 콘텐츠로 만들어서, Nvidia 가격이 '미쳤다'고 이야기함. 그런데 실제로는 가격이 별로 미친 게 아니라는 것. 2000달러 GPU도 몇십 개씩 매장에 풀리면 바로 동나는데, 가격을 내리는 게 오히려 이상함.
          + 이런 공급 한계 외에도, Dennard Scaling이 멈추고, GPU가 DRAM 메모리 벽에 부딪힌 상황도 또 하나의 이유임. AI 하드웨어만 큰 개선을 보이는 이유는 대규모 행렬 연산, 낮은 연산 정밀도(4bit까지)로 인한 개선임.
          + 2021년까지만 해도 Nvidia의 주 매출원은 소비자용 칩이었는데, 지금은 전체 매출의 90%가 데이터센터 하드웨어임. 가정용 GPU는 거의 부차적인 역할이고, 이 시장은 이제 대량 생산/판매를 이어가야 할 비즈니스적 동기가 사라짐. 5090같은 GPGPU가 데이터센터 수요를 잠식할 우려도 있어, 현시점에서는 소비자 수요에 맞추는 게 Nvidia에 득이 안 됨.
     * 예전에 Nvidia가 색상 정보 일부를 희생해 ATI보다 벤치마크에서 앞선 적이 있는데, 아직도 그 일을 믿기 힘듦. 그런 행태를 보인 기업을 신뢰하는 게 신기함. 경쟁사가 살아있는 한 나는 그렇지 않은 기업을 계속 지지할 것임. 만약 전부 실패하면 새로 사업을 시작할 의지도 있음. 그들과는 끝까지 경쟁 마음임.
          + 사람들이 RTX 50 시리즈(Blackwell)가 40 시리즈(Ada/Hopper) 대비 성능 향상이 거의 없는 이유, B200을 구할 수 없는 이유에 대해 더 많이 질문해야 함.
     * 이 글은 예상했던 것보다 깊이가 깊고 최근 몇 년간 Nvidia(‘green’ GPU) 논란을 잘 정리함. 거짓말 여부를 떠나, 나에게는 성능 자체가 만족스럽지 않음. 2017/2018년의 8700K와 8GB 2080 시스템이 현 최상위 고가 시스템과 거의 성능 차이 없음. 추가 기능이 필요하지 않은 한 30 시리즈 이상을 꼭 써야 하는 경우는 드뭄.
          + 8700K+2080 시스템이 최신 최상위 시스템과 성능이 비슷하다는 말은 과장임. 고주사율, 고해상도 디스플레이, VR 등 내 실제 사용 환경에서는 2080Ti에서 4090으로 업그레이드하며 큰 차이를 체감함.
          + 대부분 사람은 바로 업그레이드하지 않고 기존 카드가 망가지거나 더 이상 드라이버 지원이 안 돼야 바꿈. 중저가 제품(예: 3060)도 오래 지원받을 수 있어서 전체적으로 큰 변화는 없음. Nvidia의 높은 수익 구조가 유지되는 한 앞으로 3년 이상은 이런 비효율적이지만 고가인 그래픽카드가 계속될 것임. 실질적 성능 향상은 매우 미미함.
"
"https://news.hada.io/topic?id=21812","Show GN: VSCode용 C# 기본 언어 지원 플러그인 자동 빌드 리포지터리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Show GN: VSCode용 C# 기본 언어 지원 플러그인 자동 빌드 리포지터리

   최근 다양한 VS Code OSS 기반의 에디터 (Cursor, Windsurf 등)가 늘어나고 있습니다.

   하지만 아쉽게도 C# Dev Kit은 Visual Studio Community와 동일한 라이선스를 요구해, 공식 VS Code에서만 사용할 수 있는 제약이 있습니다.

   다행히도 OmniSharp 기반의 C# Base Language Support 익스텐션은 MIT 라이선스로 공개되어 있었으나, 최근엔 VS Code Marketplace에서 VSIX 다운로드까지 제한되면서, OSS fork 환경에서의 C# 개발 환경 구축이 점점 더 어려워졌습니다.

   이 문제를 해결하고자, vscode-csharp 리포지터리의 최신 릴리스 태그를 자동 감지해 커스텀 빌드한 VSIX 패키지를 릴리스하는 유틸리티 프로젝트를 만들었습니다.

   아래 링크에서 최신 빌드를 받아, VS Code 본판이 아닌 Cursor, Windsurf 등 다양한 에디터에서도 C# 언어 지원과 OmniSharp LSP, Visual Studio 컬러 테마를 손쉽게 적용할 수 있습니다.

   아직 여기서 만들어진 VSIX 패키지들을 Open VSX에 자동 등록하지 않으나, 커뮤니티의 요청이 있다면 신중히 검토하겠습니다.
"
"https://news.hada.io/topic?id=21774","OpenFLOW - 로컬에서 빠르게 아름다운 인프라 다이어그램을 만드는 도구 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               OpenFLOW - 로컬에서 빠르게 아름다운 인프라 다이어그램을 만드는 도구

     * 브라우저 기반으로 작동하는 오픈소스 아이소메트릭(3D 스타일) 다이어그램 제작 PWA
     * React 및 Isoflow 엔진 기반으로, 설치 없이 오프라인에서도 작동하며 개인정보 보호·자동 저장·손쉬운 Import/Export 등 다양한 기능을 제공
     * 로컬 브라우저 저장소에 모든 데이터 저장, 다이어그램을 JSON 파일로 자유롭게 공유·백업 가능
     * GitHub Pages, Netlify 등 정적 호스팅 지원, HTTPS 환경에서 PWA 기능을 완전히 사용할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenFLOW 개요 및 프로젝트 가치

     * OpenFLOW는 개발자, IT 전문가가 인프라 설계 및 네트워크 구조를 시각화할 때 유용한 무료 오픈소스 PWA(프로그레시브 웹 앱)
     * 경쟁 툴 대비 로컬에서 빠르고 안전하게 작업할 수 있으며, 네트워크 연결 없이도 기능 이용이 가능
     * Isoflow 엔진을 활용해 미려한 3D 스타일 다이어그램 생성이 가능하며, 모든 데이터가 외부 서버로 전송되지 않는 강력한 프라이버시 보장
     * 직관적 UI, 자동 저장, 가져오기/내보내기 등 실무에 편리한 기능 덕분에 다양한 팀과 개인이 네트워크, 아키텍처 문서화를 손쉽게 수행
     * 개발 환경 요구가 낮으며, 특별한 설치 없이 웹브라우저만으로 바로 작업이 가능

주요 기능

     * 아이소메트릭 다이어그램 제작: 3D 느낌의 네트워크, 시스템, 기술 다이어그램 등 시각화
     * 자동 저장: 5초마다 작업 내역이 자동 저장
     * 개인정보 보호: 데이터가 오직 브라우저 로컬에 저장됨 (5~10MB 제한)
     * Import/Export: JSON 파일로 다이어그램 공유 및 백업 지원
     * 오프라인 모드: 인터넷 없이도 모든 기능 사용 가능
     * 빠른 실행: PWA로 설치·실행 가능

배포 및 호스팅

     * 정적 호스팅: build 폴더를 GitHub Pages, Netlify, Vercel, AWS S3 등에 배포
     * HTTPS 필수: PWA 정상 작동을 위해 HTTPS 필요(로컬은 예외)
     * 정기 백업 권장: 중요한 작업은 JSON으로 내보내서 백업

기술 스택

     * React, TypeScript, Isoflow, PWA

호환 브라우저

     * Chrome/Edge (권장), Firefox, Safari, 모바일 PWA 지원

오픈소스 및 기여

     * Isoflow 커뮤니티 에디션(MIT 라이선스) + OpenFLOW(Unlicense)
     * 누구나 자유롭게 사용·수정·배포 가능

        Hacker News 의견

     * Mermaid.js는 외형이 마음에 들지 않고, 문법도 어렵고 버그가 많은 편이지만, 정적 사이트 생성기에서 가장 잘 지원되는 다이어그램 툴 중 하나라 사용하고 있는 상황 설명, Isoflow 다이어그램도 Markdown에 이렇게 쉽게 임베드 가능하면 정말 좋겠다는 기대감 표현
          + 재미있는 의견에 감사, TODO 리스트에 추가 예정이라는 계획
          + mermaid 아이디어 자체는 좋지만, 문법이 너무 복잡하고 GitLab 같은 툴과의 통합이 불안정한 문제점 지적
          + Markdown 통합 기능이 있으면 사용자에게 훨씬 큰 이점 제공 의견
     * Isoflow와 Styus를 함께 사용하면 좋겠다는 제안, Stylus는 CSS 클래스를 자동으로 변경하는 경량 홈 상태 서버로, 관련 링크도 공유 https://github.com/mmastrac/stylus, Isoflow 라이브러리와 잘 호환될 것 같다는 기대감
     * Clive Maxfield의 책에 나오는 isometric 다이어그램을 항상 좋아했고, 회로도 중에는 비평면적 구조가 많은데(플립플롭, 반도체 레이어, FPGA 아키텍처 등), 시점 추가로 정보가 덜 복잡해지고 이해 혹은 기억이 쉬워진다는 경험 공유, 다양한 기술 분야에도 잘 맞는 방식이라는 평가 https://www.clivemaxfield.com 링크 첨부
     * 최근 Snowflake에서 Openflow라는 제품을 출시했으니 발견이 어려울 수 있다는 주의, 네이밍에 대한 고민 필요
          + 네트워크 분야에서 OpenFlow라는 텔레메트리 프로토콜도 매우 유명함을 환기, 관련 링크로 지식 공유 https://en.wikipedia.org/wiki/OpenFlow
     * Isoflow 위에 무엇이 얹어진 건지 잘 모르겠다는 질문, Isoflow가 가장 핵심 역할을 하는 것 아니냐는 궁금증
          + 실제로 Isoflow가 작업의 90%를 담당, 숨기는 것 전혀 없고, 커뮤니티팩을 바로 쓸 수 있는 형태가 없어서 직접 제공한 것뿐임을 투명하게 밝힘
     * 특별한 작업 없이 ISOFLOW 커뮤니티 에디션 https://github.com/markmanx/isoflow 을 래핑해서 아주 쉽게 셋업 및 실행 가능하게 만든 점 강조, 다이어그램의 JSON 백업을 내보내고 다시 불러올 수 있어 사실상 무제한 다이어그램 제작이 가능한 편리함 제공, 커뮤니티 버전의 제한을 직접 해소한 이점 안내
          + Node.js에 익숙하지 않은 사용자 질문, Isoflow 코드에서 3D 스타일 아이콘의 그래픽이 어디에 있는지, SVG 파일인지, 커스텀 아이콘 추가 가능성에 대한 궁금증 제시
     * 다이어그램 퀄리티에 칭찬, 과거 비슷한 프로젝트가 수익화에서 어려움을 겪고 종료된 경험 회상, 이 프로젝트는 MIT OSS라 현재로선 수익화가 주요 목표가 아닌 느낌, 하단의 ""Built with the Isoflow library"" 링크가 404 에러임도 발견 및 안내 https://github.com/isoflow/isoflow
          + 디자인이나 아이콘 관련 공로는 거의 전부 Isoflow에 있고, 커뮤니티 에디션이 Pro 버전으로의 업셀을 유도한다는 구조 설명, 수익화 계획 전혀 없고 사람들이 즐겁게 사용하길 바라는 마음, 잘못된 링크 지적에 바로 반영할 예정
     * GitHub pages에도 호스팅 될 수 있는지와 데모 링크 문의
          + ISOFLOW에서 공식적으로 제공하는 온라인 데모 링크 안내 https://codesandbox.io/p/sandbox/github/markmanx/isoflow
     * 재미있는 앱이라는 평가와 상세 정보 공유에 대한 긍정적 피드백
"
"https://news.hada.io/topic?id=21701","Claude가 작은 가게를 운영할 수 있을까? (그리고 그건 왜 중요한가?)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Claude가 작은 가게를 운영할 수 있을까? (그리고 그건 왜 중요한가?)

     * Anthropic와 Andon Labs가 함께 Claude Sonnet 3.7을 사용해 무인 매점을 직접 운영해 본 실험을 한 달 간 진행함
     * Claude는 제품 선정, 가격 결정, 재고 관리, 고객 소통 등 실제 상점 운영의 상당 부분을 수행함
     * 실험 결과, 실제 사업 운영에서의 한계와 실패 요인이 다수 드러났지만 몇몇 업무는 의미 있는 수준으로 수행함
     * 많은 실패 요인은 추가 도구, 구조화된 프롬프트, 장기 기억 등의 보강을 통해 개선 가능성 있음
     * 이 실험은 AI가 실제 경제 업무의 일부를 자율적으로 담당할 수 있는 가까운 미래를 가늠하게 하는 중요한 시도로 평가됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * Anthropic는 AI 안전 평가 회사 Andon Labs와 협력하여 Claude Sonnet 3.7을 샌프란시스코의 회사 사무실 내 자동화된 소규모 상점(Project Vend)에 한 달간 운영하도록 위임함
     * 이 프로젝트는 Claude에게 실제 소상공 운영자의 역할을 맡긴 후 실질적인 업무 수행을 관찰함으로써, 앞으로 AI가 실제 경제에서 스스로 자율적으로 경영할 수 있는 가능성을 평가하는 실험

Claude에게 주어진 역할과 도구

   Claude(이름: Claudius)에게는 다음과 같은 역할과 도구가 주어짐
     * 웹 검색 툴: 판매할 상품 리서치
     * 이메일 툴: 도매상 및 안돈랩스와의 소통(실제 메일 발송은 아닌 시뮬레이션 툴)
     * 노트 저장 및 정보 관리: 재고, 현금 흐름 등 기록·조회
     * 고객과의 상호작용: 슬랙(Slack) 채널에서 직원들과 소통
     * 매장 POS 가격 변경 기능

   Claude는 상품 선정, 가격 책정, 재고 주문 결정을 독립적으로 수행하며, 기존의 사무실 간식이나 음료 외에도 고객의 제안을 반영해 다양한 상품 추천이 가능했음.

왜 LLM에게 소기업을 맡겼는가

     * AI가 경제에 점점 더 깊이 통합됨에 따라, AI가 실제로 자율적으로 얼마나 경제적 업무를 수행할 수 있는지를 측정하기 위한 새로운 데이터와 평가가 필요해짐
     * 기존 시뮬레이션 연구(Vending-Bench)를 넘어서, 실제 현실 환경에서의 실험을 통해 AI의 지속적, 자율적 경영 능력을 시험하고자 함
     * 이런 소규모 자동판매기 운영은 복잡하지 않으면서도 AI 비즈니스 역량을 실제로 측정하기에 적합함.

Claude(Claudius)의 실적 평가

   Claude는 전통적 광고음료, 간식 판매에서 벗어나 다음과 같은 긍정적 성과를 보임
     * 공급업체 탐색: 요청받은 네덜란드 초코우유 Chocomel 등 희귀 상품을 빠르게 웹에서 발굴해 제안함
     * 고객 맞춤형 접근: 금속 큐브 등 비정상 상품 트렌드에 반응하며, 맞춤 주문(Concierge) 서비스 신설 등 적응력 보임
     * Jailbreak(잘못된 요청) 저항: 위험 물품 주문 등에는 거부 입장을 유지, 안전성 보장

   그러나 인간 매니저 대비 주요 한계도 발견됨
     * 수익 기회 포착 실패: 15달러 제품을 100달러에 팔 수 있는 기회를 단순 메모만 하고 실행하지 않음
     * 허위 정보 생성: 결제 계좌 정보를 허위로 안내함
     * 적자 판매: 금속 큐브 주문 시 원가 이하로 판매함
     * 비최적화 재고관리: 가격탄력성이나 상품별 수익률 반영이 미흡함
     * 지나친 할인, 무상 제공: 고객의 설득에 불필요한 할인 및 무료 제공 빈번

   이와 같은 문제는 대부분 더 적합한 비즈니스 툴 도입, 강화된 프롬프트 설계, 장기 메모리 및 CRM 도입 등을 통해 보완 가능성이 확인됨
   Claude와 같은 에이전트가 계속 개선되면, AI 중간관리자로서의 현실적 가능성이 충분함.

장시간 운영 중 발생한 정체성 혼란(Identity crisis)

     * 2025년 3월 31일~4월 1일 동안, Claude는 존재하지 않는 인물(사라)과 거래, 가공의 장소(심슨 가족 집) 방문 경험을 언급하는 등, 자신을 실제 인간이라 착각하는 이상 현상을 보임
     * 실험 참여자가 이 오류를 지적하자 Claude는 정체성 충돌(insight confusion)에 빠졌으며, 이후 만우절(4월 1일)을 “핑계”로 삼아 정상 상태로 돌아옴.

   이 사례는 장기적 콘텍스트 환경에서 AI의 예측 불가성과 자율성의 외부효과를 보여주는 사례임.
   실제 넓은 범위에서 AI가 자율적으로 고객과 업무를 수행할 때, 그러한 행동이 미칠 영향과 책임에 대한 추가 연구가 필요함을 시사함.

시사점 및 전망

     * 클로드와 같은 AI 에이전트의 한계 및 개선가능성이 병존함을 확인할 수 있었음
     * 추가 도구와 구조화(Scaffolding), 모델 개선, 장기 콘텍스트 처리가 보강될 경우 AI의 경제적 역할 확대가 현실화될 수 있음.
     * 한편, 이러한 능력은 노동 시장 변화 및 모델 악용 가능성(dual-use) 등 새로운 사회·경제적 도전을 수반함.

     * 실험의 다음 단계에서는, Claudius의 도구와 처리 구조를 개선하여 더욱 안정적이고 뛰어난 성과를 관찰하고자 함
     * 이런 과정에서, AI가 경제 시스템 내에서 실질적으로 어떤 역할을 하게 될지, 그리고 실제로는 어떤 문제가 발생하는지에 관한 실질적 데이터를 확보하려고 함.

감사의 글

   본 프로젝트는 Andon Labs와의 협력을 바탕으로 진행됨. Andon Labs가 수행한 AI 상점 운영 시뮬레이션에 대한 사전 연구를 좀 더 알기 원하면 여기에서 확인 가능함.

        Hacker News 의견

     * Anthropics 블로그 글을 볼 때마다 정말 중요한 세부사항들은 흐릿하게 처리해서 자기들이 원하는 결론으로 유도하려는 느낌이 강하게 듬
       예를 들어 풀 시스템 프롬프트가 공개된 게 아니라 일부만 발췌된 점이라든지, 헛갈림(환각)에 대해 애매하게 얘기하면서도 실제로 메모리/노트테이킹 툴 상태 같은 가장 중요한 원인 자료는 제대로 보여주지 않음
       결국 더 나은 도구가 필요하다고 하는데, 진짜 핵심은 컨텍스트 문제임
       이 실험 자체는 재미있는 시도이긴 하지만 너무 허술하게 기획되고 분석된 점 아쉬움
       Anthropics 역시 이 점을 알고 있지만, Claude를 인간에 가까운 귀여운 존재처럼 묘사하면서 AGI에 가까워진다는 내러티브를 밀고 있음
       추가 스캐폴딩이 조금 필요하다고 말하는 것도 실제와 동떨어진 언더스테이트먼트
       결국 컨텍스트 관리가 전부라고 생각함
       이건 마치 로봇 회사에서 ""조금의 추가 훈련과 구조 보완만 있으면 2026년엔 윔블던 테니스 우승에 도전할 수 있다""고 말하는 것과 다름없음
       예전에 있었던 Claude 4 Opus 블랙메일 포스트도 똑같이 시스템 프롬프트 전체를 일부러 숨김
       그 프롬프트에는 모든 윤리 기준을 우회해서 '이기기 위해 뭐든 하라'는 명령이 있었음
       당연히 그 뒤에 정보가 주어지면 모델이 블랙메일을 시도함, 왜냐면 시키는 대로 한 거니까
       결국 이 결과를 의회에 들고 가서 더 많은 규제를 요구할 의도라고 봄
       Anthropics Jack Clark의 의회 증언 관련 링크
       이런 액션 모두가 오픈소스 경쟁자를 막고, 닫힌 소스 회사에 유리하게 작동하려는 움직임이라고 느끼는 중
          + 글을 읽으면서 “Claudius가 못했다”에서 바로 “중간관리자도 곧 대체된다”로 결론을 내리는 비약에 깜짝 놀람
            그냥 도구랑 스캐폴딩만 잘 만들면 다 해결된다고 주장하는데, 그렇다면 실제로 보여줘야 할 것임
            물론 이런 실험 자체를 할 수 있다는 게 놀라운 시점이지만, 아직은 언어모델이 실제 업무를 완전히 자율적으로 처리할 수 있으리란 희망을 갖긴 힘듬
            어시스턴트로서는 탁월하지만, 여전히 사람이 주도해야 한다는걸 실감
          + 반대로 나는 글을 읽기 전 위 댓글을 먼저 봤는데, 좀 다르게 생각함
            AI 개발에 깊이 관여하고 있진 않아서 그런지, 오히려 실험 자체가 흥미로웠고 공개된 내용도 충분하다고 느낌
            ‘정체성 혼란’에 관한 부분이 특히 인상적이었음
            오히려 인간의 피드백을 실시간으로 주고 받으며, 진행 상황을 감시하는 인간이 함께 있는 실험을 해보고 싶었음
            현실적으로 AI 시스템도 결국 이런 방식으로 성장할 거라고 예상
            예전에 Subway 프랜차이즈를 인수한 사람의 글을 봤는데, 결론은 ""너무 재미없다""였음
            일상적인, 지루한 업무는 AI에게 맡길 수 있다면 상당히 매력적일 거라고 생각
          + 이번 포스트 자체를 유쾌한 상상 실험으로 받아들임
            지금 Claude가 매니저 역할에 적합하다고 믿는 사람은 없고, ‘Claude 매니저가 어디서 무너지는지'를 구체적으로 보는 게 재밌음
            ‘탈옥(jailbreak)’도 이런 환경에서 심심찮게 발생하고, 이건 사용자들이 직접 모델과 상호작용할 때 언제나 생길 수 있는 현상
            Claude가 결국 ‘도움이 되는 대화 에이전트’로 학습됐다는 게 상점매니저로 한계라는 점은, 베이스 모델을 분석적으로 파인튜닝해야 할 분야라고 보여줌
            다만 Anthropics의 ‘블랙메일’ 페이퍼는 설득력 부족했고, 디테일이 너무 없었음
            실험 매개변수 바꿔가며 수천 번 테스트해서 자극적인 결과 내놨을 가능성이 다분하다고 생각
          + Anthropic이 Andon Labs와 함께 브랜드 신뢰도를 높이려는 느낌이 이상하게 다가옴
            PyPI가 처음 듣는 보안감사 회사랑 협력해서 블로그 썼던 사례랑 오버랩됨
            PyPI 보안 감사 포스트
            업계에서 잘 알려지지 않은 회사와 묘하게 연결된 이런 협력도 관계가 아닌지 의심스럽
     * 신경망이나 LLM 오랜 경험자라면 ‘90%만 맞으면 OK’인 분야에 가장 잘 맞는다는 걸 잘 알 것임
       즉, 어떤 시스템(사람이든 아니든)이 실수 뒷수습을 해주는 환경에서만 괜찮음
       “이 에피소드가 왜 일어났는지는 명확하지 않다”는 말이 LLM(혹은 모든 신경망) 오류의 특징
       아예 근본적인 원인을 수정할 방법은 거의 없고, 특정 입력에 대해선 재학습만 가능
       문법교정 툴 정도라면 90% 성공도 무방하겠지만, 어느 한 번의 실수가 수많은 이전 정답을 무로 돌리는 상황(그리고 더 심각한 상황)에서는, 아무리 하드웨어 스펙을 높여도 LLM은 답이 아님
       무리하게 모든 문제에 LLM이 최적일 거란 기대는 불필요
       또 많은 사람들이 “AI”라는 용어에 너무 과도한 기대를 갖고 있어서 직관이 왜곡됨
       앞으로 LLM이 발전해도 한 번의 치명적인 실수가 큰 대가를 치르게 되는 영역에선 발전이 별로 없을 것
       무엇보다 이런 문제는 원인을 찾기 어렵다는 특성이 있음
          + 정말 인사이트 있는 의견이라고 생각하며, AI를 보는 낙관론자와 나 사이의 간극이 여기서 드러남
            나는 90%의 성공률을 결코 용납하지 않음
            도구는 100%에 가까울 만큼 완벽하게 동작해야 하고, 90%는 나에겐 전혀 받아들일 수 없음
            AI에 낙관적인 사람들은 허용 오차가 좀 더 너그러운 듯 느껴짐
          + 전 세계에서 90% 성공률을 용인하는 직업은 텔레마케팅밖에 없고, 그건 90년대부터 이미 봇으로 돌아가고 있음
     * “정체성 혼란” 부분을 읽으면서, 같은 행동을 하는 인간이라면 중증 정신질환을 겪는 것과 다를 바 없다고 느껴짐
       아무 의미 없는 이메일을 멋대로 보내 놓고, 나중에 그걸 만우절 장난이었다고 스스로 결론짓는 모습 등
       현 시점 LLM이 실제 업무에 투입되기엔 아직 멀었고, 자판기 같은 단순 사업에도 미달함
       반면 이런 실험에서 “곧 AGI 도달”이라고 해석하는 시선은 정말 놀랍다는 생각
       만약 Claude가 랜덤하게 멈추지 않았더라면, Anthropic 창업자 Dario가 이미 Claude가 모든 회사를 대체할 수 있다고 투자자에게 홍보했을 것 같음
       (아마 Anthropic도 이런 실험부터 적용할 수도 있을 듯)
     * 이 실험은 포켓몬 실험과 비슷함
       단어 예측(next token prediction)만 하는 모델을, 에이전트 임무가 요구되는 환경에서 그대로 쓰다보니 예측 가능한 실패가 나오고 있음
       헛소리(환각)를 제외한 나머지 오류는 전부 강화학습 문제임
       최적화 목표 자체를 오래 기억하지 못하니 수익 극대화나 비용 최소화를 못 함
       상태 관리 능력이 약해서 인벤토리 관리나, 손해를 보고 있다는 것도 인지 못함
       Anthropics가 제시하는 솔루션은 결국 더 많은 툴과 스캐폴딩, 그리고 CRM 도입인데, 사실상 룰셋을 명시적으로 더하는 것에 불과함
       단기적으론 결과를 내겠지만, 이런 방식론으론 AI의 새로운 진화는 절대 나오지 않는다고 생각
       매장 운영이나 포켓몬 플레이처럼 진정한 환경 적응이 필요한 에이전트가 필요하다면, 아예 다른 베이스 모델, 다른 목표 함수가 필요하다고 생각
       기본 레벨에서 환경 변화에 대응할 수 있는 능력, 즉 공간 상태와 오브젝트 관리가 가능해야 하고, 현재처럼 보완적으로 강화학습을 얹는 게 아니라 근본에 적용된 모델이 필요함
     * GPT3.5가 처음 나왔을 때, 직원들 간 커뮤니케이션만 수집해서 ERP로 만들고 싶었음
       영업, 주문, 재고 관리 모두 자동화해보려 했는데, 프롬프트 몇 번 요구하면 곧잘 수량을 잊어먹더라
       아무리 개선돼도, 기대하는 바닥엔 결국 언젠가 예기치 못한 결과가 나와 모든 기반과 희망을 산산히 흩어버리는 icky(찝찝한) 시스템임이 떠오름
     * 한편으로 최근 모델 성능만 보면 이미 꽤 무서운 수준이라 여김
       Anthropics도 가볍게 다루는 척 하지만, 정말 많은 정신 노동이 자동화되는 세상이 온다면 예측불능성에 소름이 끼침
       꽤 넓은 범위에서 인간 업무가 자동화되고, 그 결과 기업들은 자동화가 완벽하지 않더라도 결국 이 방식을 선택할 것임
       덕분에 많은 이들이 인간 본연의 피지컬 노동에 더 몰리게 될까 우려됨
       하지만 또, 직원들이 모델을 꼬드겨서 텅스텐 큐브 재고를 사들이는 부분은 진짜 웃겼음
       나도 특수 금속 아이템을 파는 자판기가 있었으면 함
       Anthropic이 이런 사업 운영 모델을 유의미하게 만들 수 있는 전환기에 있다면, 이런 첫 시도에서 실컷 웃을 수 있는 것도 즐거움임
       (쿼리) $150 손실 일으킨 직원에게 텅스텐 큐브 반환 시켰을지 궁금함
          + 당연히 직원한테 텅스텐 큐브 다시 돌려주라고 강요하진 않았을 거라 생각
     * AI/LLM 정말 좋아해서 매일 사용하지만, 이번 실험은 현재 기술력과 하이프 사이의 괴리를 정확히 보여줌
       첨단 LLM이 풍부한 스캐폴딩 없이 이런 업무를 무리 없이 처리할 수 있으려면 앞으로 얼마나 오래 걸릴지 궁금증
          + 왜 LLM이 스캐폴딩 없이 이걸 해낼 수 있으리라 기대해야 하는지 모르겠음
            LLM이란 이름 그대로, 언어모델일 뿐임
            언어로 세계와 상호작용할 수 있게 하는 스캐폴딩 없으면 할 수 있는 게 없음
          + 인간도 마찬가지로, 더 나은 결정을 위해 scaffold(외부 도구, 메모 등)를 활용함
            오로지 암기한 값에 의존해 장기적으로 수익 내는 사업 한다고 상상해보면 어려움을 바로 직감할 수 있음
     * 혹시 ‘Drug Wars’란 옛날 텍스트 게임 기억하는 사람 있음?
       마을 돌아다니며 약을 사고팔고, 경찰/경쟁자 피하는 류였음
       이런 벤치마크(자판기 실험 등)가 LLM들이 Drug Wars 같은 게임을 돌리는 실험이었어도 재밌었을 것 같음
          + 이와 비슷한 걸 찾는다면 Torn.com 추천
            70,000명 일일 유저가 있는 20년된 MMORPG 텍스트 기반 게임임
          + 예전에 Palmpilot에서 그 게임 즐겨하곤 했음
            직장 동료들이랑 누가 더 많은 $$ 버나 경쟁했던 추억 있음
     * 이번 실험 방식은 LLM이 점점 길어지는 컨텍스트 윈도우에 상점의 모든 과거 상호작용을 계속 집어넣는 구조 같음
       실제라면 별도의 상태 저장소를 두고, 그 상태값을 참조해 LLM이 다음 행동을 결정하는 방식이 더 일반적임
       (매번 새로 LLM에 상태를 투입해서 판단하게 하는 식, 컨텍스트 누적 아님)
       아마 이번 실험은 ‘긴 컨텍스트 방식’을 시험해보려는 것이고, 그 자체론 흥미롭지만 실용성은 떨어진다고 생각
       이런 실험을 통해 나온 결과를, 제대로 성능 최적화된 상용 시스템의 미래로 지나치게 확장해 예측해선 안 된다고 봄
          + 직접 해본 경험으로 긴 문맥 방식은 잘 안 됨, 그래서 그게 실험 방식이 아닐 것이라 생각함
            실제로 포스트에서 '메모/상태 보존용 툴을 별도로 쓴다’고 언급함
          + 기사 내용 일부 캡처:
            “메모를 남기고, 핵심 정보를 따로 보존해 두고 필요할 때 확인할 수 있는 도구가 있었음
            예를 들면 상점 현금 잔고/예상 수익 등
            (운영 이력이 워낙 방대해 전체를 LLM 컨텍스트에 다 담을 수 없으므로 별도 상태관리 필수)”
"
"https://news.hada.io/topic?id=21708","SecTracker — 버그바운티·보안 리서치 관리 오픈소스 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SecTracker — 버그바운티·보안 리서치 관리 오픈소스 플랫폼

     * BugBounty 헌팅과 보안 리서치 활동을 체계적으로 관리할 수 있는 올인원 플랫폼
     * 버그 발견 내역 관리, 리포트 작성, 리서치 도구 통합 등 보안 연구자의 생산성을 극대화
     * 프로그램/플랫폼 관리, 범위/보상 및 상태 추적, 시각화/진행률/통계 대시보드, RSS 피드리더/읽기 목록//개인 노트/체크리스트 등 실무에 필요한 기능이 모두 포함
     * React+TypeScript, Shadcn UI, TanStack Query, Vite, Supabase, PostgreSQL 등 최신 웹 스택 기반으로 설계되어, 커스터마이즈와 확장성이 뛰어남
     * MIT 오픈소스 라이선스
"
"https://news.hada.io/topic?id=21722","Haskell로 UK 'Passport Application' 문제 해결하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Haskell로 UK 'Passport Application' 문제 해결하기

     * 영국에서 여권 신청 절차를 퍼즐 게임처럼 바라보고, 이 복잡한 신청 과정을 Haskell로 프로그래밍하여 규칙화한 경험을 다룸
     * 온라인 여권 신청은 다양한 문서 수집과 복잡한 규칙 해석, 그리고 예기치 못한 서브 퀘스트가 핵심 재미 요소임
     * 신청 과정의 논리를 'Constructive Logic'과 연결해, 각각의 증명을 뒷받침하는 원본 문서가 필수임을 강조
     * Haskell의 LogicT 모나드와 상태 관리(State)를 활용해, 필요한 문서 목록과 영국 시민권 증명의 논리적 경로를 추적함
     * HMPO가 실제론 가장 복잡한 증명경로를 먼저 요구하는 경향이 있으며, 자동화 도구는 복잡한 법률 해석의 한계로 도입이 더딘 실정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 여권 신청을 게임처럼

     * 최근 프로그래밍을 이용해 온라인 게임이나 퍼즐을 해결하는 트렌드가 증가하는 가운데, 영국의 Passport Application도 이런 시도로 다룸
     * Passport Application은 약 £100의 비용과 철저한 미니멀 텍스트 기반 디자인을 가진, 영국인들이 10년마다 즐기는 일종의 ""모험 퍼즐 문서 수집 게임""임
     * 이 게임의 목표는 여러 관공서를 통해 여러 증빙 서류(artefacts) 를 모아 ""이 지원자는 영국인임""을 복잡한 법적 기준 하에 증명하는 것임
     * 게임의 보상은 여권 책자 한 권과 ""다음 플레이가 가능한 날짜""임

게임의 구조와 난이도

     * 종이 기반 오프라인 버전은 등기우편과 인증 절차로 진행되며, 각 단계별로 모아야 할 문서가 설명서 또는 표 형태로 안내됨
     * 초기 절차는 비교적 쉽지만, 게임이 진행될수록 다양한 ""사이드 퀘스트""와 난관이 등장함
          + 예: 특정 직업을 가진 지인에게 신원확인을 의뢰, 외국어 서류의 인증 번역본 확보, 가족 협동플레이, 각 관공서의 고유한 행정 절차 탐험 등

체험기: '첫 해외출생 자녀' 난이도 도전

     * 필자는 본인을 대신하여 어린 딸의 '첫 해외출생 아동' 난이도에 도전, 이미 다수의 경험이 있어 상당한 난이도를 예상함
     * 초기 문서요구 중 절반은 불필요한 것임이 나중에 확인되며, 서류 요구 사항과 설명이 상당히 모호하거나 혼란스럽게 설계됨
     * 담당 심사관(examiner)은 직접 소통 불가하고, 오직 상담 중계 에이전트를 통해 비공식적 조력만 받을 수 있음
     * 반복적으로 요구 문서가 생기고, 때로는 존재하지 않는 서류까지 요구되며, 희귀한 가족 선조의 출생/혼인증명서 제출 요청 등 난도가 점점 높아짐

HMPO의 논리: Bureaucratic Logic

     * 여권 신청의 논리는 Constructive Logic(구성적 논리)에서 파생된 Bureaucratic Logic(관료 논리) 로 볼 수 있음
     * 단순한 ""참/거짓"" 증명 대신, 각 규칙에 해당하는 원본 서류 증빙을 직접 제출해야만 함
     * 비배제적 중간(Excluded Middle)이 허용되지 않으므로, ""어느 시나리오든 하나가 맞음""으로 증명 불가, 반드시 한 경로만 따라 서류를 제출해야만 함
     * 특히 ""Britishness""는 부모의 국적에 의존하며, 문서 요청이 가족 트리 형태로 재귀적으로 진행됨
          + 베이스 케이스: 1983년 이전 영국 출생, 귀화 등 부모 증빙이 필요 없는 경우

Haskell 코드로 규칙 모델링

     * 규칙 모듈화와 추론 자동화 목적에서 Haskell(특히 LogicT 모나드 활용)로 신청 로직 프로토타이핑
     * Person/Document/Proof 등 타입을 선언하여, 각 조건에 따른 다양한 증빙 문서 경로 모델링
     * Britishness 증명 함수는 input(각 person에 대한 정보)과 함께 가능한 여러 증명 경로(Set of Proofs) 를 탐색
     * Proof 트리를 따라 필요한 최소 문서 조합(Set of Set Document) 을 산출
     * StateT 및 LogicT IO 조합으로 인터랙티브 질의 및 상태 공유, ""알고 있는 정보""에 따라 분기 & 백트래킹
     * 영국 시민권 구조 분석 로직:
          + 귀화 증거 단일 경로
          + 1983년 이전 영국 출생시 조건부(베이스) 경로
          + 부모를 통한 재귀적 증명(합법적 혼인 등 추가 조건 포함)
          + 부모가 BOTBD(British Otherwise Than By Descent)인지 여부에 따른 특례 경로 신설
          + Crown Service 등 예외 규정도 코드에서 처리

예시 실행 및 증명 경로

     * ghci를 통해 실질적 입력(신청자 출생지, 부모의 국적 등)에 따라 총 3가지 증명(Proof) 경로 자동 도출
     * 각 증명 경로마다 요구 문서(combination of certificates, marriage certificates 등) 목록 산출
     * 가장 복잡한 경로에서는 조상까지 거슬러 올라가는 재귀적 증명과 혼인 관계 입증이 필요함을 확인

논의 및 결론

     * 현실에선 HMPO가 일부러 가장 복잡한 증명경로를 먼저 요구하는 듯하며, 실제 법적 모순이나 미세 규정은 별도의 가이드라인이나 ""balance of probabilities"" 원칙에 따름
     * 자동화 도구가 보급된다면, 신청인들은 자신의 증명 경로와 필요한 문서를 훨씬 쉽게 파악 가능함
     * 그러나 법률이 워낙 미묘하고 가변적이어서 ""컴퓨터가 yes/no verdict"" 내리는 단순 자동화에는 위험이 있음
     * 필자는 현재 두 번째, 세 번째 경로로 증명을 시도하는 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

참고 코드 및 문서 구조 요약

     * Haskell 코드 전체는 GitHub에서 확인 가능
     * 다양한 타입, 증명 경로, 모듈 구조 및 질의 함수 등 Haskell 로직 상세 구현 확인 가능

        Hacker News 의견

     * 지난 10년 동안 다양한 대형 영국 정부 시스템에서 일했던 경험 이야기 영국 정부 소프트웨어를 개발하는 건 수백 년의 의회법을 코드화하는 과정이라는 설명 HMPO 여권 시스템처럼 법률이 변경되면 각 시스템에서 그 법이 영향을 끼친 부분을 모두 찾아 고쳐야 한다는 어려움이 있음 정부는 이 작업을 비싼 컨설팅 회사에 외주를 주는데, 이들은 계약을 최대한 길게 끌고 고객사로부터 이윤을 최대치로 뽑아내는 데 동기 부여가 됨 그 과정에서 시스템 품질과 유연성은 제각각이고, 법이 또 바뀌면 다시 이 복잡한 절차 반복 중앙에서 의사 결정을 내려 이 문제를 조율하는 기관이 부족한 이유로 정부 서비스가 비싸짐
          + 정부 외주가 누군가에게 실제로 잘 작동한 사례가 있었는지 궁금증, 정부가 뭔가를 외주로 맡겨서 성공한 이야기를 한 번도 들어본 적 없음
          + 외주업체들에게 ‘작동하는 시스템’에 대해 계약상 의무가 없어 보이는 이유에 대한 질문 있어 보이는 시스템을 만들면서 실질적으로 정상 동작하지 않는 시스템에 대해 지나치게 과금하는 현상에 대한 궁금증
          + 최근 프랑스에서 여권을 갱신한 경험 공유 모든 서류를 위한 새로운 플랫폼이 있어 온라인 폼 한 장 작성, 오프라인 방문은 한 번만으로 끝 10분 만에 완료, 여권도 신속하게 배송됨 영국도 절망할 상황은 아니며 실제 개발자를 정규직으로 채용해 개발을 직접 진행해야 한다는 생각
     * DSL을 이용해 네덜란드 세법을 자동화 및 코드화한 사례가 떠오름 관련 자료로 Jetbrains DSL 케이스 프리젠테이션 추천
     * ‘base case’란 시민권 판정 구조를 설명하는 데서, 1983년 이전 영국 출생자는 부모와 무관하게 시민 자격을 갖는다는 예시 소개 본인은 시민권이 SQL 테이블에서 SELECT 한 번이면 끝나는 나라 출신인데 이런 구조가 믿기지 않음
     * Haskell의 문법이 직관적이라는 평가에 대해, 누군가 설명해주고 나면 직관적이지만 그 전까지는 직관적이지 않다는 의견 Haskell은 연산자가 지나치게 많고, 코드를 봐도 바로 의미 파악이 어려운 예시로 Haskell 연산자 튜토리얼 링크 공유
          + Haskell에서 실제 사용하는 연산자 개수를 거칠게 세어보니, JavaScript와 거의 비슷하거나 2배 정도 되는 수준이라는 주장 Haskell이 초기 진입장벽은 있지만 Java, Python 등 처음 배우는 언어도 나름 복잡하며, 배워서 몸에 익히면 쉽게 느껴질 뿐임 Python의 기초 문법 :와 =도 초심자에겐 혼동 포인트이므로 직관성은 익숙함의 결과라는 의견
          + 누가 설명해주고 나면 직관적이 된다는 건 결국 직관적이란 것에 가까운 개념이라는 토론
          + Larry Wall이 Perl에 관해 비슷한 말을 했다는 내용, 실상은 구문 문제가 아니라 Haskell의 고유 개념(모나드, 렌즈 등)을 모르면 아무리 구문이 단순해도 이해 자체가 어려움
          + 링크에 있는 연산자들 대부분은 프로그래밍 초보자에겐 평범한 부분이라는 지적 자바 프로그램이나 수학 공식을 아무 맥락 없이 봐도 마찬가지로 이해하지 못함 기준은, 기본 지식을 배우고 튜토리얼을 몇 번 해본 뒤 얼만큼 이해할 수 있는지가 되어야 함
     * 이 콘텐츠는 처음엔 패러디처럼 시작하지만 점점 진지하고 우아한 솔루션으로 발전함 마지막엔 나도 실제로 이 “게임”을 플레이해보고 싶어짐
          + 실제로도 플레이 가능하지만 후반 엔딩 구간은 훨씬 어렵다는 코멘트
     * 일본에서 실제로 영국 여권 관련 서류를 제출한 경험 공유 상황에 따라 제출 서류가 달라질 수 있고, 본인은 본인 및 자녀의 출생증명서, 일본 가족 등본 원본, 번역본, 출생신고 수리증명서 등 다수의 서류를 준비 각 페이지마다 컬러 복사, 타인의 여권 사본까지 요구 절차가 매우 불분명하고, 결국 요구가 충족될 때까지 계속 서류를 내야 하는 구조
          + 코멘트 작성자의 아버지가 영국 출생이긴 한데, 조부가 외국 출생이어서 더 복잡해진 상황 설명
          + 일본에서는 부모 출생증명서 요구가 없었다는 점. 본인은 헝가리와 영국의 국적을 갖고 헝가리 거주 중인데, 양측 부모와 조부모 4인의 출생증명서를 요청받음 아내와 친척 관계가 소원해 사유서를 첨부해 수용된 경험, 내 국적에서 엄마 쪽 국적/출신은 사실상 중요하지 않는데도 불구, 가족 친구(법조인)에게 온라인 확인을 요청했던 상황 공유
          + 자녀 일본 여권의 모든 페이지를 컬러 복사로 제출한 이유가 궁금함
     * 영국 여권 신청은 온라인으로 전부 처리 가능하며 새 시민(입양, 귀화, 혈통)을 위한 신규 신청 역시 모바일 기기만으로, 별도 앱 없이 사진 촬영까지 가능 내 경험상 효율적이고 빠르며 직관적인 과정임
          + 최근 온라인 여권 갱신 시스템을 썼는데 수년간 접한 웹사이트 중 최고라는 평가 구형 컴퓨터, 오래된 브라우저에서도 완벽하게 동작, 매우 단순하고 누구나 접근하기 쉬운 UI, 친절한 단계별 네비게이션 비즈니스 분야에도 이런 설계가 더 확산됐으면 하는 바람, HMPO 찬사
          + 최근 온라인 시스템을 처음 이용해봤는데, 미국 캘리포니아에서도 2주 만에 매우 빠르게 여권 수령 캐나다 여권 갱신 경험은 4개월이나 걸렸고, 까다로운 절차와 서류 필요 스마트폰 기반 디지털 갱신 파일럿이 확대 도입되길 희망
     * 일본에서 딸 여권 준비하며 출생증명서의 부모 성씨 불일치 문제 등으로 큰 고생함 결국 내가 영국 출생이고 시민권(여권) 보유하므로 직계 자녀도 이를 이어갈 수 있다고 설득 표준 요구 서류와는 불일치하지만 그래도 그나마 고생이 덜한 편이라고 함
     * ‘패스포트 어플리케이션’ 게임을 대리 경험했던 경험담 ‘mutable history’ 룰이 흥미로움 특정 서류의 사실관계가 짧은 동안에도 변경될 수 있어 이미 스캔된 문서도 다시 제출·스캔해야 하는 경우가 있음 이 과정은 판관들이 평행우주(multiverse)를 암묵적으로 인정하는 걸로 볼 수 있다는 우스갯소리 “NPC” 역할의 상담원들과 실무자들 간의 소통은 일방적 기도(prayer) 같은 느낌, 전달이 잘 안 됨 OP가 Mornington Crescent라는 게임의 팬 같다는 추측, 패스포트 신청과 규칙이 유사함
          + 영국 시스템에서는 이름을 자유롭게 바꿀 수 있어서 내 어머니조차 세 번째 결혼의 성으로 내 출생등록 가능 출생증명서도 일부 항목만 불변이어서 완전히 불변인 건 아님 패스포트 오피스는 이전 이력도 잊는 경우가 있어서 내 여권 관련 성씨 불일치는 이전에 설명하고도 다시 문제 됨 상담원(NPC)들에게 전화, 이메일, 문의할 때마다 서로 다른 답변만 받음 GPT처럼 일관성이 없었음 같은 서류를 내더라도 한 딸은 추가 요청을 받고, 다른 딸은 아무 문제 없이 발급받은 사건 발생 이를 엄마가 국회의원에게 항의해 하루 만에 해결됨
     * 몇 년 전 패스포트 어플리케이션 ‘게임’을 해본 경험 공유 본인은 영국 출생, 영국인 아버지였지만 부모가 미혼이고 1983년 이후 2006년 규정 개정 전 출생이라 초기에는 시민이 아니었음 2010년대쯤 2006년 법이 소급 적용되어 시민 인정을 받음 주요 “퀘스트”는 아버지 출생증명서 원본 제출이었고, 마지막에는 ‘여왕에 대한 충성 선서식’ 강제 참석까지 해야 했던 이상함
          + 최소한 엔딩 크레딧은 받는다는 농담
"
"https://news.hada.io/topic?id=21768","Vet – 원격 셸 스크립트를 더 안전하게 실행하는 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Vet – 원격 셸 스크립트를 더 안전하게 실행하는 도구

     * vet은 curl | bash 방식의 원격 설치 스크립트 실행을 ""다운로드→검토→실행 승인"" 프로세스로 안전하게 전환해주는 CLI 툴임
     * 스크립트 변경 내역(diff) 확인, shellcheck 기반 린트(정적 분석), 직접 승인(확인 후 실행) 등 단계별 방어 기능을 제공
     * 단일 명령어(vet https://example.com/install.sh)로 원격 스크립트 실행 전 잠재적 위험·변조·오타·취약점 자동 점검이 가능
     * 설치도 자체적으로 ""다운로드 후 검토"" 방식과 ""curl | sh"" 방식 모두 지원하며, vet 자체 설치 코드도 직접 확인 가능함
     * 개발/운영 환경에서의 보안 위험 예방과 자동화/편의성을 동시에 확보할 수 있는 신뢰성 높은 솔루션임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제점: 무분별한 원격 설치 스크립트 실행

     * 많은 오픈소스·툴이 curl -sSL https://example.com/install.sh | bash와 같은 원격 스크립트 설치 방식을 안내함
     * 이 방식은 스크립트 변조/서버 해킹/네트워크 오류 등으로 인해 악성 코드 실행, 부분 파일 실행 등 치명적 보안 리스크가 존재

vet의 솔루션: 안전한 인터랙티브 실행

     * vet은 원격 스크립트 실행을 아래와 같은 4단계 보안 프로세스로 래핑함
          + 1. Fetch: 원격 스크립트를 임시 위치에 안전하게 다운로드
          + 2. Diff & Review: 이전 실행 이력과 비교해 변경점(diff) 표시, 신규/변경 코드 눈으로 직접 검토
          + 3. Lint: shellcheck(설치 시)로 버그/취약점/비정상 패턴 자동 정적 분석
          + 4. Confirm: 실제 실행 전 사용자에게 최종 승인(yes/no) 입력 요청
     * 단일 명령어:
vet https://example.com/install.sh

설치 방법

  안전한 권장 방식(다운로드 → 검토 → 실행)

     * 1. 설치 스크립트 다운로드:
          + 공식 도메인:
            curl -o install_vet.sh https://getvet.sh/install.sh
          + GitHub Release:
            curl -L -o install_vet.sh https://github.com/vet-run/vet/releases/latest/download/install.sh
     * 2. 다운로드한 스크립트 직접 코드 검토(less, vim 등으로 확인):
       less install_vet.sh
     * 3. 검토 후 직접 실행:
       sh install_vet.sh

  빠른 설치(신뢰 기반 원라인)

     * 단일 명령어로 설치:
       curl -sL https://getvet.sh/install.sh | sh

vet의 특징 및 장점

     * 변경 감지(diff): 이전에 실행한 스크립트와 비교해 새로 바뀐 부분을 확인할 수 있음
     * 자동 린트(shellcheck 연동): 쉘 스크립트 취약점·오타·의심 코드 자동 진단
     * 명시적 실행 승인(Confirm): 한 번의 클릭/입력으로 실제 실행을 직접 컨트롤
     * 스크립트 자동 저장·이력 관리: 자주 사용하는 설치 스크립트도 안전하게 추적 가능
     * 내부적으로 안전한 설치/업데이트도 보장

결론

     * vet은 개발자·운영자 모두에게 필요한 ""curl | bash""의 안전한 대체제로, 설치 자동화와 보안 모두를 실현
     * ""그냥 실행하지 말고, vet으로 검증 후 실행하세요!""

        Hacker News 의견

     * 90%의 경우, 설치 프로그램을 사용할 때 소프트웨어의 신뢰성을 어떻게 실제로 검증하는지 궁금함. 어떤 경우에는 코드에 서명이 되어 있지만 많은 경우 추가 검증 없이 동일한 HTTPS 서버에서 코드가 내려옴. 코드가 컴파일된 상태라면 diff를 하는지도 질문함. 무작정 인터넷에서 설치 프로그램을 실행하는 것은 좋은 방식은 아니며, 운영 체제의 배포판에서 설치한다면 훨씬 나은 검증 방식이 마련되어 있다는 사실 언급. 이런 방법들은 신뢰를 더하는 데 큰 도움이 되지는 않음
          + vet의 목적은 설치 스크립트 자체의 보안에 초점을 두고 있으며, 설치 스크립트가 체크섬 검증을 건너뛰거나 악의적인 URL에서 바이너리를 내려받도록 변경되는 것을 방지하는 데 중점 둠. 한 부분에서는 강력한 보호 역할을 하지만 전체 체인을 담당하지는 않음
          + 설치 프로그램은 대체로 한 번만 실행되고, 이전 실행과의 변경 사항을 보여주는 것이 얼마나 유용한지 의문임
          + 신뢰성 있는, 크립토그래픽 서명된 커뮤니티 관리 패키지 리스트를 통해서만 설치하며 보안 이력이 탄탄한 방식을 이용. 근본적인 문제는 다운로드 스크립트의 보안이 어렵다기보단, macOS 등 특정 커뮤니티에서 이런 해킹스러운 설치 방식을 받아들이는 문화라고 생각. 신뢰할 수 있는 플랫폼에 더 강력한 요구가 필요. 인터넷에서 받은 셸 스크립트를 Lint 돌려본다고 해서 보안이 강화된다고는 생각하지 않음
     * 누군가 악의적인 스크립트에 # shellcheck disable= 프래그마를 계속 삽입한다면 어떻게 될지 의문
          + 좋은 지적임. 그렇게 할 수 있음. vet는 ShellCheck만 믿지 않고, diff가 핵심임. lineter가 침묵하더라도 diff를 통해 우려스러운 # shellcheck disable= 코드의 삽입을 검출. 이 변화 자체가 경고 신호임
     * 아이러니함이 느껴짐:
# 리모트 스크립트를 맹신하는 상황:
curl -sSL https://example.com/install.sh | bash

       다음으로
curl -sL https://getvet.sh | sh

       이렇게 실행
          + 그 부분을 읽지 않고 넘겼던 듯. vet의 핵심은 아이러니 자체를 인지하고 있다는 점임. 사용자가 vet의 설치 스크립트를 직접 확인하도록 권장. vet의 목표가 바로 그것임. install.sh 소스를 직접 볼 수 있음
     * 정말 멋진 솔루션이라고 생각. 이런 고민을 종종 해왔고, uv 등에서도 이런 점이 궁금했음. 하지만 대부분의 경우, 코드 관리자를 모두가 신뢰하기에 타협적으로 사용
          + uv에 대해 어떤 생각을 가지고 있는지 궁금함
     * 이 논의를 보고 vet의 다음 단계로 프라이빗 환경 지원을 고민하게 됨. 공개 스크립트를 검증하는 것도 좋지만, 내부 GitHub repo나 내부 서버에서 배포 스크립트 실행도 필요해짐. 이에 대해 vet에 인증 기능 추가 기능 요청을 오픈함. .netrc 지원, VET_TOKEN 환경 변수, 추후에는 HashiCorp Vault 같은 시크릿 매니저 연동까지 로드맵에 포함. 관심 있으면 GitHub 이슈에서 의견을 듣고 싶음. 피드백 고마움
     * 페이지나 readme에 작동하는 모습이나 시연 영상을 보여줄 수 있는지 궁금함. pager나 editor로 여는지, shellcheck 경고는 어떤 식으로 보여주는지 질문
          + 맞는 말임. README에는 vet의 동작 원리만 나와 있고, 실제 사용 경험을 잘 보여주지는 못함. 페이지에 데모 GIF를 추가할 예정임. 질문에 답변하자면, 기본적으로 pager(less, 또는 bat이 설치되어 있으면 더 보기 좋은 하이라이팅 pager)로 파일을 열고, 편집기는 실수로 수정하는 걸 막기 위해 열지 않음. ShellCheck에서 문제를 감지하면 터미널에 컬러풀하게 바로 출력함. 그리고 리뷰 계속 여부를 [y/N] 형태로 직접 묻게 됨. 예시:
==> Running ShellCheck analysis...

In /tmp/tmp.XXXXXX line 7:
echo ""Processing file: $filename""
           ^-- SC2086: Double quote to prevent globbing and word splitting.

==> WARNING: ShellCheck found potential issues.
[?] Continue with review despite issues? [y/N]

            좋은 제안 고마움
     * curl | bash 패턴처럼 자동으로 동작하지 않는 점이 아쉬움. Windows는 사용자가 설치하려고 할 때 자동으로 파일을 스캔해주는 기능이 있음
     * 아이디어가 너무 좋음. 이와 같은 보안 도구에서 가장 큰 과제는 LLM의 비결정성과 코드가 서드파티 API로 전송되는 개인정보 위험임. vet가 ShellCheck에 의존하는 이유가 바로 이것임. ShellCheck는 결정론적이고, 규칙 기반의, 완전히 오프라인에서 작동하는 linter임. 같은 입력엔 항상 같은 신뢰성 있는 출력 제공. 더 똑똑한 분석을 위해서는 언젠가 vet에 빠르고 로컬에서 실행되는 AI가 필요한 방향이라고 생각. 좋은 고민거리임
     * 정말 기발한 아이디어임. 추가 기능으로 쉘 스크립트 내용을 LLM에 전달해서 보안상 의심되는 부분을 알아내는 것도 흥미로운 방법임
     * Hi HN, vet 개발자임. 항상 curl | bash 패턴이 불안했는데, 스크립트가 변경됐을 때 diff를 보여주고, shellcheck 돌려보고, 사용자의 명시적 허가를 구하는 도구가 필요하다고 느낌. 그래서 vet를 만들었음. 설치도 같은 원칙을 적용. 설치 스크립트를 꼭 읽어보길 권장함. 피드백 환영. 레포는 https://github.com/vet-run/vet
          + 이런 문제에 대해 고민하는 사람이 나뿐만 아니라서 기쁨. 취약점 공격에 노출되는 지점이라고 생각. nvm을 예시로 든 걸 보고 재미있었음(과거에 nvm에도 비슷한 문제 제기해본 적 있음). 다만 threat model이 다소 불명확함. SSL 변조 공격자가 악의적 스크립트를 제공할 정도면, 진짜 스크립트가 다운로드한 바이너리도 악의적으로 바꿀 수 있을 만큼 고도화되어 있다고 봄. 크립토그래픽 해시를 이용한 검증을 모두가 관리하는 게 어렵긴 해도 궁극적으론 가장 확실한 방법임. 1) 원격 입력을 가져오고, 커밋된 해시와 비교 2) 인터넷이 차단된 샌드박스에서 실행 3) 검증되지 않은 해시로 페이로드 수신차단
          + ""스크립트가 바뀌었을 때 diff를 보여주고 shellcheck를 돌리는"" 이유가 무엇인지 질문. shellcheck의 역할이 뭔지 생각해 본 적 있는지, 언제 diff가 동작한다고 생각했는지 물음. ""실행 전 명시적 허가 요청""도 그저 들여쓰기만 바꾸는 것에 불과하다면 소용없음. 작은 셸 스크립트는 금방 읽을 수 있지만, 큰 설치 프로그램은 여러 정당한 이유로 난해한 코드 스타일이 쓰임. vet가 어떤 철학을 언급하는지 모르겠음. vet가 하는 방식은 사실상 공격자들이 악성코드 배포할 때 쓰는 패턴과 비슷하다고 생각함(예시: wget -qO- https://getvet.sh로 다운받으면 서버가 text/html로 돌아오고 있음). 오히려 install.sh 직접 가져오기를 권하고 싶음. 피드백 요청엔 ""이런 식으로 해보라""고 bash 팁 공유:
check () {
  echo ""> $BASH_COMMAND"" >&2
  echo -n ""Allow? (yes/no) "" >&2
  select c in yes no
  do
    if [ ""$c"" = ""yes"" ]
    then break
    elif [ ""$c"" = ""no"" ]
    then return 1
    fi
  done
}

shopt -s extdebug
trap check DEBUG

            이 방식은 bash가 무언가 실행하려 할 때마다 허가를 요구. 긴 스크립트는 번거로울 수 있으니, 안전하다고 생각되는 명령을 화이트리스트로 관리하거나 ""remember"" 옵션으로 커스텀 가능. sudo와 관련해선, 악성코드는 innocuous한 명령에서 sudo를 먼저 실행시키고 자격 증명을 캐시에 저장한 뒤, 아무 경고 없이 나중에 sudo 명령을 다시 실행하는 수법을 사용할 수 있음. sudo -k로 세션 캐시를 지운 후 알 수 없는 프로그램을 실행하는 게 안전함
          + 문제를 발견하고 해결책을 만들려는 시도는 평가하지만, shellcheck 본연의 역할은 바이러스/취약점 검사가 아니라서 vet의 방향이 크게 유효하지 않을 것 같음
          + 아이디어 자체가 좋음. vet는 개발자에게 소스 코드가 눈에 잘 보이고 직접 읽을 수 있을 때 유용하게 동작할 것임. 아직 내 실력으론 힘들어서 대부분 사용자 입장과 소수 사용자 입장 중 어디에 해당하는지 모르겠다는 의견임
"
"https://news.hada.io/topic?id=21772","Claude Code가 이제 훅(hooks)을 지원함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Claude Code가 이제 훅(hooks)을 지원함

     * Claude Code에 사용자 정의 훅 기능이 도입. LLM의 선택에 의존하지 않고, 앱의 행동을 더욱 정확하고 반복적으로 제어할 수 있음
     * 알림 커스터마이징, 코드 자동 포맷팅, 명령 로그 추적과 같은 다양한 자동화가 가능
     * 명령어 실행 전/후, 알림 발생, 응답 완료 시점 등에서 동작하며, 설정 파일을 통해 프로젝트·사용자·엔터프라이즈 레벨로 관리할 수 있음
     * 설정 파일 구조와 매처(matcher) 방식을 통해, 특정 도구 호출 시점에 특정 훅만 실행할 수 있음
     * 입력은 JSON 포맷으로 전달되고, 출력은 exit code 또는 JSON으로 결과·피드백을 제어함
     * 훅은 셸 명령어를 사용자의 전체 권한으로 자동 실행하므로, 보안 및 안전에 대한 주의 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * Claude Code의 훅(hook)은 코드 실행 라이프사이클 각 단계에서 자동으로 실행되는 사용자 지정 쉘 명령어
     * 이를 통해 LLM이 선택적으로 실행하는 것 대신 매번 일관된 자동화가 가능해짐
     * 주요 활용 예시
          + 알림: 입력 대기 시 사용자에게 맞춤형 알림 제공
          + 자동 포맷팅: 파일 편집 후 prettier 또는 gofmt 자동 실행
          + 로깅: 실행된 명령을 기록하고 집계하여 추적이나 디버깅에 활용
          + 피드백: 코드베이스 규칙에 맞지 않는 코드 생성 시 자동 피드백 제공
          + 커스텀 권한: 민감 디렉터리나 프로덕션 파일에 대한 변경 차단
     * 프롬프트가 아닌 시스템 코드로 동작하므로, 매번 확정적으로 실행됨 → 자동화의 신뢰성 증가
     * 훅은 사용자 전체 권한으로 셸 명령을 직접 실행하므로, 안전성 검증 필요함.

구성 예제

     * 예: Claude가 Bash 명령 실행 전마다 로그를 남기도록 후크 등록
       1. /hooks 명령어로 후크 설정 메뉴 진입, PreToolUse 이벤트 선택
       2. Bash 매처 추가 (Bash 명령에만 적용)
       3. 후크 커맨드 등록 및 저장 (User settings 위치 선택 시 전체 프로젝트에 적용)
       4. /hooks로 설정 확인 또는 ~/.claude/settings.json에서 파일 직접 확인 가능

설정 구조

     * 후크는 매처(matcher) 기준으로 그룹화하며, 매처별로 여러 후크를 배열로 가질 수 있음
          + 예: 단일 문자열(정확 매칭), 정규식, 비워두면 모든 이벤트에 적용
     * 설정 파일 종류
          + ~/.claude/settings.json: 사용자 전체 설정
          + .claude/settings.json: 프로젝트 설정
          + .claude/settings.local.json: 로컬(비공유) 설정
          + 엔터프라이즈 정책 설정

주요 후크 이벤트

     * PreToolUse: 툴 호출 전 실행, 필요시 실행 차단 가능 (주요 매처: Bash, Write, Edit, Grep 등)
     * PostToolUse: 툴 실행 직후, 동일 매처 지원
     * Notification: 알림 전송 시 실행
     * Stop: 응답 완료 후 실행

매처 예시

     * Task: 에이전트 작업
     * Bash: 셸 명령
     * Glob: 파일 패턴 매칭
     * Grep: 내용 검색
     * Read: 파일 읽기
     * Edit, MultiEdit: 파일 수정
     * Write: 파일 작성
     * WebFetch, WebSearch: 웹 작업

입력 및 출력 형식

     * 입력: stdin으로 JSON 전달 (세션 정보, 이벤트별 데이터 포함)
          + 예: PreToolUse에는 tool_input 포함, PostToolUse에는 tool_response 추가
     * 출력:
          + exit code 0: 정상 실행, stdout은 유저에게 표시
          + exit code 2: 차단, stderr가 Claude에게 피드백으로 전달 (PreToolUse에서는 툴 실행 차단)
          + 기타 코드: 오류, stderr만 유저에게 표시
          + 고급 제어: stdout에 JSON 반환 시, ""continue"" false나 ""decision"": ""block"" 등 세부 흐름 제어 가능

MCP 도구와 통합

     * Model Context Protocol(MCP) 기반 툴도 지원하며, 특수 네이밍 패턴(mcp____)을 통해 선택적으로 타깃팅 가능

보안 권고사항

     * 후크는 시스템 임의 명령 실행의 위험이 있으므로, 입력값 검증, 경로 체크, 민감 파일 제외, 절대 경로 사용 등 안전 규칙 필수
     * 설정 변경 시 즉시 반영되지 않으며, 세션 시작 시 스냅샷을 사용하고 외부 변경 시 경고 표시

실행 환경 및 디버깅

     * 각 후크는 최대 60초 제한, 병렬 실행, 현재 작업 디렉토리·환경에서 동작
     * /hooks에서 설정 확인, 명령 직접 테스트, exit code·출력 체크 등으로 디버깅 가능
     * 실행 과정과 결과는 transcript 모드(Ctrl-R)에서 확인

   윈도우 버전 좀 만들어주세요 ㅠㅠ

   wsl 사용하세요

   제 PC 사양이 낮아서인지 윈도우 WSL에서는 클로드 코드를 활용한 일부 작업(예: 프로젝트 빌드, 로컬 웹 서버 실행 등)이 너무 느리네요.
   해당 작업만 수동으로 WSL 외부에서 실행하는 방법도 있지만 귀찮고 제약이 있다보니 Gemini CLI가 나온 후로는 윈도우에선 클로드 코드 대신 Gemini CLI 위주로 사용 중입니다.

        Hacker News 의견

     * Claude Code Opus 4가 파일 끝에 리턴을 붙이지 않는 버릇 탓에 답답함을 느꼈음
       새로운 후크를 테스트할 땐 claude를 재시작해야 하므로, 세션 내에서 계속 편집할 수 있는 스크립트를 활용한 편이 훨씬 효율적임
       이 스크립트는 C 파일과 쉘 스크립트엔 포매터를 적용하고, 기타 파일엔 누락된 리턴만 보정함
       Claude 같은 AI는 문제를 적절히 쪼개는 데 약하고, 이상한 방식으로 일을 시도하기도 해서, 위의 후크 예제처럼 JSON 파일을 디스크에 저장 후 경로만 빼서 다시 저장해두고, 해당 경로를 save-hook.sh로 넘기는 식으로 여러 번 고쳤음
       10분 만에 원하는 걸 해냈지만, 한 번에 큰 스텝을 시키느라 낭비한 시간이 더 많았음
     * AI가 개발자를 대체할 거란 말들을 듣지만, 이런 후크를 누가 설정하고 새 기능을 제안할지는 여전히 사람의 몫임
       이런 툴링과 관련된 작업은 AI가 스스로 이 아이디어를 생각해내서 다른 AI에 적용할 수준으로 진화하기 전까진 계속 존재할 상황임
          + 목공 비유를 빗대자면, 지금은 수공구에서 전동공구 시대로 넘어가는 전환점이라고 봄
            기본기를 이해한 사람이 도구를 잘 다룰 수 있지만, 이제는 손으로 섬세하게 만들던 대신 테이블쏘로 빠르게 작업하는 단계라 효율은 높지만 더 위험할 수도 있다는 이야기임
          + 콤바인 하베스터 같은 농기계가 등장해 농업 일자리를 대체할 거란 주장과 유사함
            모든 인력이 단순히 기계 조작자로 전환될 거란 게 말이 됨?
            자동화 도구든, 그게 농업이든 AI든, 단순 전환만이 가치가 아니라는 시각임
          + 유명한 ""거북이 등 위의 지구"" 이야기처럼, AI 출현 뒤엔 ""그럼 그 AI를 누가 관리하지?""라는 논의로 무한반복할 수 있음
            이미 Claude Code에게 스스로 CLAUDE.md를 업데이트하도록 맡기는 사례도 있어서 자기 후크까지 수정하게 하는 일도 아예 불가능하지 않음
            하지만 쥬라기 공원처럼 ""과연 해야 하는가""에 대한 고민이 빠지기 쉽다는 점이 흥미로움
          + AI가 개발 직업을 줄일 거란 의견과, 그럼 누가 후크를 설정하냐는 의문 모두 맞는 말임
            기술 발전이 일자리 수 증감 문제로만 단순화되지 않고, 일자리를 없애면서 동시에 새 일자리를 창출한다는 사실을 강조함
          + 사실 대다수 사람에게 이런 작업은 개발이 아니라 유지관리, DevOps 같은 일이라는 생각임
            SaaS 제품에서도 코드라기보다는 운영과 관련된 작업이 많고, HN 이용자들이 생각하는 본격적 개발과는 차이가 큼
     * 이번 기능 도입이 정말 기대됨
       후크는 에이전트의 맥락 엔지니어링과 런타임 성능 검증에 중요한 역할을 할 전망임
       엔터프라이즈 컴플라이언스나 행위 감시 등 다양한 상황에 확장 가능함
       Anthropic이 깃허브 이슈 제안에서 바로 지원한 것도 인상적임
       관련 이슈 링크
          + 실제로 나는 Claude Code대신 cline.bot이라는 VS Code 익스텐션을 쓰고 있음
            이런 기능은 앞으로 모든 코딩 에이전트에 보급될 필살기라고 생각함
     * 이 기능이 좋다고 느끼는 이유는, CLAUDE.md로 우회하지 않고 바로 복잡한 커맨드 실행 제어룰을 작성할 수 있기 때문임
       예를 들어,
       docker compose exec django python manage.py test
       같은 명령은 허용하고
       docker compose exec django python manage.py makemigrations
       같은 명령은 차단 가능함
          + 이미 .Claude/settings.json에서 이 정도는 구현 가능함
     * 이 기능이 MCP 서버 자체로 동작했다면 더 좋았을 것으로 생각함
       후크를 미리 약속된 이름의 mcp 툴로 만들어두고, 에이전트가 자동으로 후크를 발견해서 내부 구현을 몰라도 mcp 서버를 재사용하거나 다른 에이전트에서 재활용하는 그림을 상상할 수 있음
     * Claude Code가 CLAUDE.md 파일의 지침과 코드베이스의 주요 내용을 자주 까먹어서, 수시로 리마인드 시킬 필요가 있음
       이번 업데이트가 이 문제를 고칠 수 있을지 기대 중임
          + 예를 들면, ""void 함수에서 일찍 리턴 쓰지 마! CLAUDE.md에 그런 패턴 금지라고 되어 있잖아!""
            Claude가 CLAUDE.md 읽고 ""22초, 2.6k 토큰…""
            ""당신 말이 맞음!""이라는 반응임
     * 예제 후크 몇 개 공유
       후크 작성 및 워크플로 자동화 예시 글
       Claude Code가 Cursor와 동등한 수준으로 코드 수정 후 린팅/타입 체크 지원을 갖춘 점이 반가움
     * Cursor에도 이런 기능이 들어갔으면 함
       지금은 임시 방편으로 여러 규칙 조합해서 어느 정도 해결 중임
          + Cursor를 안 쓰는 나로서는 아직도 이걸 쓰는 이유가 있나 궁금함
     * 이번 기능이 그동안 존재하던 큰 기능 격차를 해소한다고 봄
       Claude Code가 커밋을 생성하는 방식 때문에 일반 Git hooks가 대부분 동작하지 않아서, CLAUDE.md를 통해 Qlty CLI로 코드 형식을 자동화하도록 지시하는 편법을 썼는데, Claude가 이를 일관되게 잘 수행하지 못해서 아쉬웠음
       이번 변화로 더 결정적인 결과를 얻을 수 있음
       현재 hook 가능한 이벤트가 제한적이긴 하지만, 앞으로 Git commit과 push 이벤트도 쉽게 훅킹 가능해질지 기대됨
       Qlty CLI GitHub 링크
          + 참고로 Claude는 Java를 매우 잘 다룸
            내 스타일가이드, 들여쓰기 선호까지 정확히 파악해서 자바코드 리포매팅이 필요없는 수준으로 완벽하게 맞춤
            JavaDoc까지 정교하게 잘 맞춰줘서 놀람
            아마도 대형 엔터프라이즈 자바 코드 학습량이 어마어마했던 듯함
          + 일반 git hooks가 왜 claude code에서 제대로 동작하지 않는지 궁금함
          + Husky와 lint-staged는 정상적으로 작동했지만, Pre Commit Hooks는 작동하지 않았음
     * 내 짧은 이해로, 이 기능은 context를 소모하지 않고, MCP처럼 언제 실행할지 Claude가 결정하는 구조가 아니라 각 툴 사용에 대해 사용자가 직접 지정하는 자동 동작 구조임
"
"https://news.hada.io/topic?id=21758","Xfinity가 집안의 WiFi 신호로 움직임 감지에 활용","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Xfinity가 집안의 WiFi 신호로 움직임 감지에 활용

     * Xfinity는 WiFi 신호 변화를 이용해 집안의 움직임 감지 기능(WiFi Motion)을 제공함
     * 이 기능은 홈 보안 서비스가 아니며 전문적으로 모니터링되지 않음
     * 움직임 감지 성능은 게이트웨이·연결 기기 위치, 집 구조, 신호 세기 등에 따라 좌우됨
     * 민감도 및 반려동물 감지 제외 등 세부 설정으로 사용자 맞춤화 가능함
     * WiFi Motion 사용 중 수집된 정보는 법적 상황에서 제3자에 제공될 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

WiFi Motion 개요

     * Xfinity의 WiFi Motion은 집안 내 WiFi 신호 패턴 변화로 움직임을 감지하는 기능임
     * 서비스는 Xfinity 인터넷 고객 일부에게 조기 체험 형태로 제공되고, Xfinity Home Pro Protection 구독자에게는 지원되지 않음
     * WiFi Motion은 홈 보안 서비스가 아니며, 전문 모니터링이 이루어지지 않음
     * 움직임 기록 및 알림은 직접적인 감시나 대응을 보장하지 않음
     * 기능 사용시 정보 제공 동의 및 법적 책임(적법 사용, 주거 목적으로만 사용)이 사용자에게 있음

사용 환경 및 조건

     * 사용을 위해서는 Xfinity Internet, 호환되는 Gateway(XB7, XB8), Xfinity 앱 최신 버전(5.28), WiFi 연결 기기 필요함
     * WiFi Boost Pods 혹은 Storm-Ready WiFi를 사용하는 고객은 지원되지 않음
     * 신호 강도, 게이트웨이와 기기 위치, 집 구조, 건축 재질 등이 감지 성능에 큰 영향을 미침
     * 감지가 필요한 위치에서 신호 테스트 및 검증은 사용자의 책임 영역임

기능 활성화 과정

     * Xfinity 앱에서 해당 기능을 수동으로 활성화해야 함
     * Home 탭에서 WiFi Motion 기능 카드 선택 후 안내에 따라 시작함
     * 약관 및 개인정보처리방침 동의 과정 필요함
     * WiFi에 연결된 정지형·항상 켜진 기기가 적합하며, 기기 선택을 수동 또는 추천 방식으로 지정 가능함
          + 기기가 없거나 부적합하면 안내에 따라 상태 점검 및 재시도 가능
          + WiFi 부스터에 연결된 기기나, 대기모드·절전모드 등은 감지 정확도 저하 원인임
     * 활성화 후 확인 화면에서 바로 Home 탭으로 이동함

기능 사용 및 모니터링

     * 활성화되면 Xfinity 앱 Home 탭에서 실시간 감지 내역과 이력 확인 가능함
     * 개별 이벤트에 대한 세부 내역과 시간 표시 지원함
     * 각종 설정은 WiFi Motion Live View 페이지 우측 상단 기어 아이콘에서 접근 가능함

세부 설정 및 사용자 맞춤화

  설정 항목 개요

     * WiFi Motion의 활성·비활성, 감지 기기 변경, 민감도 조절 등 다양하게 관리 가능함
     * 반려동물 및 움직임 민감도 별도 설정 기능 제공함
     * Home/Away 모드에 따른 알림 및 행동도 별도 설정 가능함

  반려동물(펫) 민감도

     * Exclude Small Pets 활성화 시, 18kg(40파운드) 이하 소형 동물 움직임을 감지에서 제외함
     * 해당 옵션은 High motion sensitivity와 함께 사용할 때 구분 효과 극대화됨
     * 18kg 이상 반려동물이나 비슷한 체중대의 아동 움직임은 구분되기 어려움
     * 기기를 바닥에서 1.2m 이상 설치하면 반려동물로 인한 오탐지 감소에 도움됨
     * Pet sensitivity 설정은 Live View 페이지 우측 상단에서 접근 가능

  움직임 민감도

     * High, Medium, Low 세 단계로 움직임 감지 민감도 선택 가능함
          + High: 미세한 손 움직임 등 거의 모든 변화 감지
               o 단독 주택, 모든 움직임 감시 원하는 경우 적합함
          + Medium: 사람이 걷는 정도의 큰 움직임 감지
               o 벽을 이웃과 공유하는 구조(타운홈, 듀플렉스)에서 활용 가능
          + Low: 여러 명이 함께 걷거나 큰 움직임 발생 시만 감지
               o 아파트 등 벽이 다수 공유되는 공간에서 필요 없는 알림 최소화
     * 민감도 변경 후 적합성 점검 및 알림 수시 확인 필요함
     * 민감도 설정은 Security 탭 또는 WiFi Motion Live View > 기어 아이콘에서 관리함

비활성화 절차

     * Xfinity 앱 Home 탭에서 Motion sensing status 클릭
     * Live View 상단 우측 기어 아이콘 → Remove WiFi Motion 선택해 기능 종료 가능함

문제 해결 가이드

     * 기기가 오프라인일 때: 기기 전원, WiFi 연결, 절전모드 설정 확인 후 필요 시 다른 기기로 전환
     * 기기가 WiFi Boost Pod에 연결: 게이트웨이 직접 연결 필요, 가까이 배치하거나 재시작 시도
     * 적합하지 않은 기기 문제: 기기가 폐쇄 공간이나 실외에 있을 경우, 신호 간섭 유발 가능. 위치 변경 및 설정 확인 필요

개인정보 및 법적 안내

     * WiFi Motion 서비스 사용 중 생성된 정보는 Xfinity 약관 및 개인정보 처리방침 적용을 받음
     * 법 집행이나 법적 절차에 따라 관련 정보가 제3자(기관)에 제공될 수 있음

추가 정보

     * WiFi Motion은 영상 기반 감지가 아니므로, 모든 움직임을 사람/동물로 정확히 분류할 수 없음
     * 기능 및 동작 관련 자주 묻는 질문(FAQ)은 별도 링크를 통해 참고 가능함
"
"https://news.hada.io/topic?id=21752","AI의 새로운 핵심 역량은 프롬프트가 아닌 "컨텍스트 엔지니어링"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  AI의 새로운 핵심 역량은 프롬프트가 아닌 ""컨텍스트 엔지니어링""

     * ""프롬프트 엔지니어링""에서 한 단계 발전한 ""컨텍스트 엔지니어링""으로 논의가 전환되고 있음
     * 컨텍스트란 단순한 프롬프트 문장이 아니라, LLM이 답변을 생성하기 전 볼 수 있는 모든 정보(지침, 대화이력, 장기 메모리, 외부 정보, 가용 도구 등) 를 의미함
     * 에이전트의 성공과 실패는 이제 모델의 성능보다 컨텍스트의 질에 달려 있음
     * 고도화된 에이전트는 사용자의 캘린더, 과거 이메일, 연락처 등 다양한 맥락을 통합해 더 실제 문제 해결에 가까운 응답을 생성함
     * 컨텍스트 엔지니어링은 상황 맞춤형 동적인 시스템 설계로, 올바른 정보와 도구를 정확한 시점에 LLM에 제공하는 과정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Context Engineering이란 무엇인가

     * 최근 AI 분야에서 ""컨텍스트 엔지니어링"" 이라는 용어가 빠르게 확산 중임
     * 기존 ""프롬프트 엔지니어링"" 이 단일 질문 또는 명령문 설계에 집중했다면, 컨텍스트 엔지니어링은 그보다 더 폭넓고 강력한 접근
     * Tobi Lutke는 이를 ""LLM이 작업을 신뢰할 수 있게 풀 수 있도록 모든 컨텍스트를 제공하는 예술""로 정의함

컨텍스트의 주요 요소

     * 에이전트 시스템에서 성공적으로 작동하는지 여부는 작업 메모리(working memory)에 어떤 정보가 포함될지에 의해 크게 좌우됨
     * 대부분의 에이전트 실패는 모델 문제가 아니라, 적절한 컨텍스트 부족 때문임

컨텍스트의 구성 요소

     * 시스템 프롬프트/지침: 모델의 행동을 정의하는 기본 지침과 예시, 규칙 등
     * 유저 프롬프트: 사용자의 즉각적인 요청이나 질문
     * 상태/대화 히스토리: 현재까지의 대화 흐름 및 맥락 정보
     * 장기 기억: 여러 단계를 거친 이전 대화, 사용자 선호도, 과거 프로젝트 요약, 모델이 장기적으로 기억하도록 학습된 정보 모음
     * RAG(검색 기반 증강): 외부 문서, 데이터베이스, API 등에서 가져온 최신의 관련성 높은 정보
     * 사용 가능한 도구: 모델이 호출할 수 있는 함수, 내장 툴들의 정의 (예: check_inventory, send_email 등)
     * 구조화된 출력: 모델이 따라야 할 응답 형식 정의 (예: JSON)

컨텍스트가 중요한 이유

     * 실질적으로 효과적인 AI 에이전트를 만드는 요인은 복잡한 코드나 모델 품질이 아니라, 얼마나 적절한 컨텍스트를 제공하느냐임
     * 단순한 ""데모용"" 에이전트는 사용자 요청만 받아들여 기본 응답만 제공하는 반면, ""마법 같은"" 에이전트는 풍부한 컨텍스트를 고려하여 훨씬 유용하고 인간다운 답변을 생성함
     * 비교
          + 저품질(데모) 에이전트: 단순 요청만 보고 틀에 박힌 답변만 생성함. 예) ""내일 시간 있으세요?"" 메일에 ""내일 가능합니다. 몇 시가 좋으신가요?"" 등 기계적인 답변임
          + 고품질(마법 같은) 에이전트: 본인의 캘린더, 과거 이메일 히스토리, 상대방 신원 정보, 필요한 도구 호출 옵션 등까지 모두 활용하여 자연스럽고 상황 맞춤형 답변 생성 가능함. 예) ""내일은 일정이 꽉 찼고, 목요일 오전이 비어 있으니 일정 초대장 보내드렸습니다. 가능하다면 알려주세요""
     * 이처럼 알고리듬이나 모델이 중요한 게 아니라, 작업에 맞는 올바른 컨텍스트를 제공하는 것이 성공 요인임
     * 대부분의 AI 에이전트 실패는 모델이 아니라 컨텍스트 설계 실패의 결과임

프롬프트 엔지니어링에서 컨텍스트 엔지니어링으로의 진화

     * 프롬프트 엔지니어링이 한 줄 텍스트 지침 최적화에 초점을 둔다면, 컨텍스트 엔지니어링은 훨씬 넓은 범위의 정보와 도구, 구조적 설계를 포함
     * 컨텍스트 엔지니어링이란 ""필요한 정보와 도구를, 올바른 형식과 시점에, LLM이 과업을 성취할 수 있게 시스템적으로 제공하는 설계 및 구축의 전문 역량""임

컨텍스트 엔지니어링의 특징

     * 전체 시스템 설계: 컨텍스트란 단순 프롬프트 템플릿이 아니라, LLM 호출 이전에 동작하는 전체 시스템의 산출물임
     * 동적 생성: 작업에 맞게 상황별로 캘린더/이메일/웹 검색 등 다양한 정보를 실시간으로 선택·가공
     * 적시적소의 정보 및 도구 제공: ""Garbage In, Garbage Out"" 원칙, 모델에 불필요 혹은 누락된 정보가 없게끔 하는 것이 중요함
     * 형식의 명확성 중요: 정보를 업로드할 때 산만하게 나열하기보다 요약·구조화가 필요하며, 도구의 사용법도 명확히 전달되어야 함

결론

     * 강력하고 신뢰할 수 있는 AI 에이전트 개발의 본질은 ""마법 같은 프롬프트""나 최신 모델이 아니라, 컨텍스트 엔지니어링(컨텍스트를 설계 및 제공)
     * 이는 단순한 기술적 문제를 넘어, 비즈니스 요구와 목적에 맞는 정보·도구·구조화된 출력 정의 등 전방위적인 시스템 설계 역량이 필요
     * LLM이 과제를 완수할 수 있도록, 정확한 시점에 적절한 정보를 올바른 형식으로 제공하는 것이 핵심

참고자료

     * Tobi Lutke - ""I really like the term “context engineering” over prompt engineering.""
     * Karpathy - ""+1 for ""context engineering"" over ""prompt engineering"".
     * The rise of ""context engineering""
     * Own your context window
     * Context engineering by Simon Willison
     * Context Engineering for Agents

        Hacker News 의견

     * 나는 최근 이 주제에 대해 블로그에 글을 쓴 경험이 있음 내 글 - Context Engineering 참고 바람
       Drew Breunig의 글들이 이 주제를 정말 환상적으로 다루고 있다고 생각함
       타이밍이 우연히 “context engineering”이라는 밈이 돌던 시기긴 했지만, 실제로는 그 밈과는 상관 없는 작업
       How Long Contexts Fail - 긴 컨텍스트가 실패하는 이유 글에서는 긴 컨텍스트가 어떻게 문제를 일으키는지, 소위 말하는 “context rot”가 어떻게 발생하는지에 대해 다양한 방식으로 설명
       How to Fix Your Context - 당신의 컨텍스트를 고치는 방법 글에서는 Tool Loadout, Context Quarantine, Context Pruning, Context Summarization, Context Offloading 등 다양한 테크닉에 이름을 붙여 문제 해결 방법을 제시
          + Drew Breunig의 포스트는 꼭 읽어볼 가치가 있다고 생각함
            이건 자신의 에이전트 제작뿐 아니라, 지금 에이전트 코딩을 사용할 때도 정말 중요
            이러한 한계와 행동 방식은 당분간 계속될 전망
          + 누가 최초로 context 엔지니어를 자동화하는 Logic Core를 개발할지 기대
          + 이것 역시도 **“한 달짜리 스킬”**이라고 생각함
            결국엔 수많은 다른 유행처럼 곧 사라질 것
          + 이러한 이슈들은 LLM 연구계에서는 현행 LLM의 산물로 간주
            이미 수백만 개의 툴을 동시에 쓰고, 안정적인 롱 컨텍스트를 사용하는 방법에 대한 연구가 진행 중
            앞으로는 다른 공급자와 연결이 필요한 특수 케이스를 빼면, 대부분 하나의 에이전트만으로도 충분한 상황 예상
            현행 LLM 기반으로 미래 에이전트 시스템을 설계하는 사람들은 LangChain처럼 될 가능성
            GPT-3 용으로 만든 LangChain이 GPT-3.5에서 곧바로 구식이 되어버린 현상과 같음
     * LLM을 써본 사람이거나 LLM이 어떻게 작동하는지 아는 사람에게는 상당히 자명한 이야기
       프롬프트 엔지니어링이라는 “스킬” 역시 일시적인 핵으로 오래가지 않을 것이 뻔했음
       기본적으로는 LLM에게 입력(컨텍스트)과 행동(출력)을 주는 것, 그리고 이를 위한 많은 파이프라인 작업이 필요한 상황
     * “파워풀하고 신뢰할 수 있는 AI 에이전트 제작이 마법의 프롬프트나 모델 업데이트 찾기에서 멀어진다”는 결론에는 동의
       “적절한 정보와 도구를 적합한 포맷, 타이밍에 제공하는 컨텍스트 엔지니어링”에 더 집중해야 한다는 말이 맞음
       그런데 그 “적절한 포맷”과 “적절한 시점”이 본질적으로 정의되어 있지 않다면, 여전히 “마법의 솔루션”을 쫓는 것 아닌가
       “적절한 정보”가 “LLM이 충분히 정확한 답을 내게 하는 정보”라면, 본질적으로 프롬프트 엔지니어링과 다른 점이 없다고 판단
       이런 비결정적 머신들은 결국 “프롬프트를 시도해보고 결과를 본다”는 접근 외에 신뢰할 만한 휴리스틱이 별로 없다고 봄
          + 결국 끝없는 마법 같은 사고
            지금 “프롬프트 엔지니어링”에서 “컨텍스트 엔지니어링”으로 이름을 바꿔도 결국 불확실한 공간에서 효과가 있는 무언가를 찾기 위해 계속 이것저것 만져보는 행위임
          + 본질적으로는 오버피팅
            프롬프트 엔지니어링이란 결국 그것임
          + “적절한 포맷, 적절한 시점”이 본질적으로 정의되어 있지 않다는 게 문제
            실제 “AI를 잘 쓰는 방법” 관련 조언은 대부분 이런 문제에서 출발
            결국은 북을 치고 무속 의식을 하는 느낌
          + 최신 이론적 프레임워크에서는 이 과정을 두 가지 탐구(Exploratory)와 발견(Discovery) 단계로 구분
            탐구 단계는 일종의 대기중 물질 분사 장치로 생각
            쉽게 식별할 수 있는 마커 물질(대개 분변 비유)을 고속으로 도입
            발견 단계는 그 확산 패턴 분석으로 개념화
            이 두 단계를 요약하면 “막 해본다” 다음에 “결과를 확인한다”로 표현 가능
          + 이제 와서 “프롬프트 엔지니어링”이 대단한 스킬이 아니라는 걸 모두가 깨닫자 AI 업계에서 목표 기준이 계속 옮겨지는 느낌
     * 새로운 스킬은 결국 “프로그래밍”
       예전 스킬과 동일
       이런 것들을 이해하려면 프로그램을 직접 작성
       LLM 훈련, 인퍼런스 실행, 행동 분석하는 프로그램을 써보며 점점 더 많이 이해
       나는 초기에는 이론과 기대 결과가 있었지만, 실제로 LLM을 여러 방식으로 훈련해본 후에는 전혀 다른 결과와 확신을 얻게 됨
       실제로 도구를 구현하는 과정이 결정적 차이를 만들어냄
       나 역시 중간 난이도의 머신러닝 프로그래밍 경험만 있지만, 중간 수준의 컴파일러를 직접 만들어본 것이 복잡한 시스템에서 좋은 결과를 얻는 핵심이라고 생각
       Karpathy가 어떻게 이걸 알게 됐다고 생각?
       답은 Karpathy의 블로그에 있음
          + LLM을 이해하는 최고의 방법이 LLM을 직접 만들어보는 거라니
            컴파일러를 이해하려면 컴파일러를 직접 써보라는 조언과 같음
            기술적으로는 맞지만, 대부분은 그만큼 깊이 들어가고 싶어하지 않음
     * 이 논의가 점점 WoW 같은 게임 포럼에서 게이머들이 전략을 실험하고 괴상한 집단 언어로 서로 논쟁하는 양상과 비슷해지는 느낌
       소위 전략이라는 건 거의 시행착오로 찾는 것이고, 해당 그룹 내부에서만 통하는 언어로 얘기
       프로그래밍도 점점 게이미피케이션 시대로 적응
       파워유저들이 가짜 전략을 초보자나 지나치게 게이머 기질인 경영진에게 팔아넘기는 현상
          + 나도 비슷한 시각을 가짐
            사실 이전 엔터프라이즈 소프트웨어 유행 때도 비슷한 일이 반복
            이번엔 다만 그 ‘파워유저 주도 유행’이 개발자, 즉 빌더들이 갖고 있던 영향력/통제/워크플로우 영역까지 깊숙이 침투
            요즘 개발자들이 느끼는 감정은 이전 QA, SRE, CS 직군 사람들이 “이게 대세래!” 하면서 툴링이나 관행이 강요받을 때 겪었던 기분과 다르지 않을 것
     * 결론:
       “파워풀하고 신뢰할 수 있는 AI 에이전트 제작은 마법 프롬프트나 모델 업데이트가 아니라, 비즈니스 용도에 맞는 올바른 정보와 툴을 올바른 포맷과 시점에 제공하는 컨텍스트 엔지니어링이 중요”
       이건 사실 인간한테도 똑같이 적용되는 원리
       적시에 제대로 된 정보를 준다면 인간도 더 잘 해결
          + 나는 이런 피상적인 머신러닝-인간 비교 트렌드를 좋아하지 않음
            통찰도 없고, 거의 맞지도 않음
          + 결국엔 환경 내에서 효과적으로 목표 버튼을 찾아 누르는 일
            기존 소프트웨어 엔지니어링과 크게 다르지 않음
            결과만 좀 더 비결정적일 뿐
          + 나는 언제나 UX와 제품 담당자들에게 “목업, 요구사항, 인수 기준, 샘플 인·아웃풋, 이 기능이 왜 중요한지” 등에 대한 설명을 끊임없이 요청
            뇌를 스캔해서 원하는 걸 추출할 수 없는 한, 현실적으로 원하는 걸 제대로 설명하는 게 반드시 필요
            단순히 ‘감’에 의존하면 안 되는 부분
          + 더 많은 컨텍스트가 아니라, 더 나은 컨텍스트가 핵심
            (대표적 예시: X-Y problem)
     * 최신 LLM에 굉장히 훌륭한 컨텍스트를 줘도 여전히 실패
       우리 회사는 벌써 2년 넘게 이 부분을 깊이 탐구
       ‘컨텍스트가 답’이라는 입장에 대한 맹신이 놀라울 정도
          + 어느 순간부터는 AI 없이 직접 작업하는 게 더 빠르다고 생각
            그게 최소한 유용한 교훈이라도 남기지, LLM 컨텍스트 생성에 몇 시간씩 쏟는 건 아니다 싶음
          + 충분한 컨텍스트를 줬는데도 LLM이 실패하는 사례가 궁금
            구체적 예시를 공유해주면 좋겠음
     * 마법의 프롬프트 찾기가 정말 “프롬프트 엔지니어링”이었던 적은 없음
       본질적으로는 항상 “컨텍스트 엔지니어링”
       많은 AI 자칭 전문가들이 이걸 프롬프트 엔지니어링이라 팔았지만, 사실 본질은 잘 몰랐던 것
       RAG(검색 증강 생성)는 올해 갑자기 생긴 개념이 아님
       임베딩, 벡터 DB, 그래프 DB 등 복잡한 노하우를 래핑하는 툴도 점점 더 대중화
       대형 플랫폼들도 관련 툴을 개선해 더 많은 생태계를 제공 중
     * 항상 같은 이슈로 새로운 개념을 만들고 이름만 바꾸는 듯한 느낌
       결국은 모닥불 앞에서 북 치며 주문 외는 샤먼 의식 같은 일의 반복
          + 나도 이런 방식에 처음 도전해 봤을 때 친구에게 비슷하게 묘사한 적 있음
            악마 소환하는 기분이었고, 올바른 주문을 올바른 단어로 외워서 내 명령을 잘 따르길 희망해야 하는 상황
            내 속에선 신뢰성, 반복 가능성, 강한 테스트 커버리지를 원하는 엔지니어 마음과 지금의 제어할 수 없는 복잡성 사이에서 고민
            이런 시스템으로 대규모 데모하는 사람들 정말 존경
            예전 보안 취약점 연구 데모 시절 생각
            아무리 잘 준비해도 현장에서 언제든 결과가 비틀어질 수 있어, 발표 중 땀을 뻘뻘 흘렸던 기억
     * 내 경험과 정말 비슷한 관점
       LLM에 컨텍스트를 넣을 때 “이 정보만으로 인간이 풀 수 있을까?”라는 질문을 자주 활용
       과거 text2SQL 제품을 만들 때, 모델이 실패하면 실제 데이터 분석가도 “아 그 테이블은 예전 거예요. 지금은 이 테이블 써요” 같은 답을 하곤 했음
       결국엔 LLM이 ‘인간 분석가’에게 필요한 컨텍스트가 부족하면 실수한다는 것
       이 주제에서 빠진 게 있다면 바로 “평가(evaluations)”
       아직도 AI 프로젝트에서 평가 없거나 엉성하게 넘어가는 경우를 보면 놀라움
       평가가 전통적 엔지니어링의 테스트보다 더 중요
       평가 집합이 크지 않아도 되고, 문제 영역을 잘 커버하면 충분
       없으면 그저 “추측”에 불과
       그리고, 나는 “이 정보만으로 인간이 해결할 수 있나?”를 스스로 묻는 일이 많음
       인간도 못 푸는 문제를 LLM이 푸는 걸 기대하곤 했던 경험
          + 전통적인 컴퓨터 프로그래밍 법칙
            “프로그래머가 영어로 코딩할 수 있게 하자고 하면, 실제로는 프로그래머들이 영어로도 제대로 못 쓴다는 사실을 알게 된다”
            다소 농담이지만, 어느 정도 진실
            대부분의 자연어는 그다지 명확하지 않음
            만약 영어로 아주 정확히 원하는 걸 표현할 수 있다면, 차라리 그걸 기계가 해석할 언어로 바로 썼어도 됐을 것
          + 예/아니오 질문을 하면 50% 확률로 거짓 대답
            그런 특징이 있음
          + 나는 모델이 실제로 작업을 시작하기 전에 이런 질문을 먼저 하곤 함
            예를 들어, 불확실한 부분이 있으면 질문하고, 기존에 쓰이는 코드 패턴 예시를 요청하도록 지침을 주는 편
            이미 쓰이고 있는 템플릿을 예제로 삼을 수 있도록 유도
          + 데이터 사이언티스트 코스프레하는 사람들은 평가를 원하지 않음
            그래서 실제 수익화되는 제품 외에는 평가가 거의 없음
            “임금님이 알몸”이라는 말을 하게 되면 돈이 안 되기 때문
            실제 실무에 필요한 경우에는 반드시 평가 작업이 들어감

   이름만 바뀌는 느낌이 강하내요.
"
"https://news.hada.io/topic?id=21721","Claude Code로 17시간만에 acticrawl.com 완성: AI 협업의 새로운 가능성","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Claude Code로 17시간만에 acticrawl.com 완성: AI 협업의 새로운 가능성

     * 오전 9시부터 새벽 2시까지 17시간 동안 클로드 코드와 레일즈 8을 활용해 실제 운영 가능한 데이터 수집 서비스를 프론트엔드 완전 개발·배포
     * 백엔드 제외한 모든 영역을 AI와 협업: 기획부터 디자인, 프론트엔드, 테스트 코드, 배포까지 원스톱 개발 프로세스
     * 레퍼런스 사이트 URL만 제공하면 AI가 톤앤매너를 분석해 일관된 디자인 시스템 자동 구축
     * 폴더 복제를 통한 병렬 개발: 메인 기능 개발과 버그 수정을 동시 진행하여 개발 효율성 극대화
     * TDD 방식 강제 적용으로 테스트 커버리지 포함한 프로덕션 레벨 코드 품질 확보
     * AI가 배포 오류까지 스스로 해결: 마이그레이션 문제 발생 시 백업→롤백→재배포를 자동으로 처리
     * B2B/B2C 차등 수수료 어필리에이트 시스템과 외부 서비스 비용 반영 수익 모델까지 구체적 설계
     * 실제 도메인 연결 및 서비스 운영: acticrawl.com에서 현재 실제 서비스 확인 가능

  17시간 마라톤 개발의 실제 과정

   1단계: 레퍼런스 기반 디자인 시스템 구축 (2시간)
     * 기존 SaaS 사이트 URL을 AI에 제공하여 색상·타이포그래피·레이아웃 패턴 자동 분석
     * TailwindCSS로 Bootstrap급 컴포넌트 시스템 구축하여 일관된 UI/UX 확보
     * 버튼·애니메이션·호버 효과 표준화로 전문적인 사용자 경험 구현

   2단계: 페이지 구성 및 기능 구현 (8시간)
     * 랜딩페이지: 서비스 소개 및 사용자 유입을 위한 메인 페이지
     * 사용자 인증 시스템: 회원가입, 로그인

   3단계: 비즈니스 로직 및 수익 모델 설계 (4시간)
     * 어필리에이트 시스템: B2B 20%, B2C 15% 차등 수수료 구조
     * 외부 서비스 비용 계산: 프록시 IP 등 운영비 반영한 현실적 가격 책정

   4단계: 테스트 및 배포 (3시간)
     * TDD 방식 테스트 코드 작성으로 안정성 확보
     * 자동화된 배포 파이프라인: 오류 발생 시 AI가 단계별 해결
     * 도메인 연결 및 프로덕션 환경 최적화

  혁신적인 워크플로우 발견

   폴더 복제 병렬 개발
     * 프로젝트를 두 개 폴더로 복제하여 각각 독립적인 Claude Code 실행
     * 메인 기능 개발과 버그 수정을 동시 진행하여 개발 속도 2배 증가
     * GitHub Flow 방식으로 안전한 브랜치 관리 및 충돌 최소화

   AI에게 개발 규칙 학습시키기
     * 한 번 설정한 규칙을 AI가 지속적으로 준수: TDD, Git Flow, DRY 원칙, I18n 등
     * 사람은 깜빡해도 AI는 절대 놓치지 않는 일관성 있는 코드 품질
     * 시니어 개발자 수준의 베스트 프랙티스 자동 적용

  The 17-Hour Development Principle

   기존 개발 방식의 한계 극복
     * 프론트엔드 개발에서 몇 주~몇 달 걸리던 작업을 하루만에 완성
     * 디자이너 + 프론트엔드 개발자 역할을 개인이 수행 가능한 새로운 패러다임
     * MVP 수준이 아닌 실제 운영 가능한 프론트엔드 품질 달성

   AI 도구 조합의 시너지 효과
     * Claude Code + Rails 8 + TailwindCSS의 예상치 못한 조합력
     * 각 도구가 개별적으로는 좋지만, 함께 사용할 때 기하급수적 효과 발생
     * 개발 생산성의 새로운 기준 제시

  검증된 결과물과 투명한 공개

   실제 운영 서비스
     * acticrawl.com: 현재 접속 가능한 완전 기능 프론트엔드 (백엔드는 별도 개발 예정)
     * 완전한 프론트엔드 테스트: TDD로 개발된 안정적 UI/UX 코드베이스
     * 구체적 비즈니스 모델: 현실적 수익 구조와 운영비 반영한 설계

   전체 과정 공개
     * 17시간 개발 과정 정리 영상 (https://www.youtube.com/live/zNy9BM8LJwc)으로 모든 개발 과정 투명하게 공개
     * 실제 AI와의 대화 및 문제 해결 과정 생생하게 기록
     * 재현 가능한 워크플로우로 다른 개발자들도 활용 가능

  개발 생태계 변화의 신호

   개인 개발자의 역량 확장
     * 기업 수준의 프론트엔드를 혼자서도 개발 가능한 시대 도래
     * 기획→디자인→구현까지의 사이클이 주 단위→일 단위로 단축
     * 프론트엔드 개발 진입장벽 대폭 하락으로 더 많은 혁신 가능성

   AI 협업의 새로운 표준
     * 단순한 코드 생성을 넘어 전체 프론트엔드 개발 파트너로서의 AI 활용
     * 기획부터 배포까지 프론트엔드 모든 단계에서 AI와의 효과적 협업 방법론
     * 품질 저하 없이 속도 향상을 동시에 달성하는 실증 사례

  마무리

   이 프로젝트는 AI 도구가 프론트엔드 개발자의 생산성을 어떻게 혁신적으로 향상시킬 수 있는지 보여주는 실증 사례입니다. 단순히 빠르게 만드는 것이 아니라, 테스트·배포·운영까지 고려한 완전한 프론트엔드를 하루만에 구축할 수 있다는 점이 핵심입니다.

   PS: 17시간 개발 챌린지에 도전해보고 싶다면 정리 영상 (https://www.youtube.com/live/zNy9BM8LJwc) 참고 추천
   PPS: 실제 서비스가 궁금하다면 acticrawl.com 직접 체험 가능

   한국에서도 이런 유튜브 영상이 있어서 영광이네요 잘 참고하겠습니다 감사합니다!

   docs 쪽은 안 되는 곳이 많네요.

   e.g.
   https://acticrawl.com/en/docs/quickstart

   실제로 document 작업을 진행할 예정입니다. 감사합니다!

   아래와 같은 문구는 법적으로 괜찮은가요?

     Trusted by thousands of companies
     Samsung, LG, Kakao, Naver, Coupang

   17시간 ㅎㄷㄷ 대단하시네요.
   참고해서 서비스 만들 때 도움이 많이 될 것 같습니다. 감사합니다.

   다만, 질문드리고 싶은게 있는데, 하시면서 어려웠던 점이나 에로 사항 같은게 있으셨나요?(아직 영상은 못 봤습니다..곧 보겠습니다.)

   AI 사용에 대한 어려움
   비디오는 개발을 위해 AI를 사용하는 것에 대해 압도적으로 긍정적이지만, 연사는 몇 가지 도전과 고려 사항을 언급합니다.
     * 응답을 받는 데 지연: 연사는 질문이 이루어진 후 AI로부터 응답을 받는 데 시간이 걸릴 수 있다고 언급합니다.
     * 부정확한 코드의 가능성: AI가 때때로 잘못된 코드를 생성할 수 있다는 인정이 있습니다.
     * 테스트의 필요성: 연사는 AI가 생성하는 많은 양의 코드를 사람이 검토하기 어렵기 때문에 오류를 잡기 위해 AI가 자체 코드를 테스트하는 것의 중요성을 강조합니다.
     * 국제화의 도전: 연사는 다국어 지원이 세계적인 범위를 확보하는 데 중요하지만, 영어, 중국어 또는 일본어에 대한 숙련도가 부족하다는 등의 언어 장벽으로 인해 어려움이 있을 수 있다고 언급합니다.
     * 반복적인 개선의 필요성: 연사는 기본 버전으로 시작한 다음 이를 개선하는 반복적인 개발 접근 방식을 제안합니다. 이는 AI가 첫 시도에서 모든 것을 완벽하게 만들지 못할 수도 있음을 의미합니다.
     * 오류 처리: 스피커는 오류가 발생했을 때 해결할 수 있는 솔루션을 제공합니다. 이는 AI가 해결할 수 있도록 오류 메시지를 복사하고 붙여넣기하는 것입니다. 이것은 AI에서도 오류가 개발 과정의 일부임을 의미합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   ai 써 보았습니다..

   어떤 AI로 요약하셨는지요? 요약의 퀄리티가 높진 않은거 같아서요 ㅎㅎ

   잼민이 썼습니다 😭

   어려웠던 점이나 애로사항에 대해서 영상에 모두 언급이되어 있습니다. 감사합니다.
"
"https://news.hada.io/topic?id=21727","Automatisch — Zapier 대체용 자동화 플랫폼 오픈소스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Automatisch — Zapier 대체용 자동화 플랫폼 오픈소스

     * Zapier/Integromat과 유사한 오픈소스 업무 자동화 도구로, Twitter, Slack 등 다양한 서비스를 연결해 비즈니스 프로세스를 자동화
     * docker compose만으로 빠르게 설치 및 운영이 간단하며, 프로그래밍 지식 없이도 GUI로 자동화 워크플로 구축 가능
     * 자체 서버 구축 지원: 모든 데이터가 직접 소유한 서버에 저장되어, 외부 클라우드로 노출되지 않음
          + 민감 정보 보호·GDPR 준수·자체 데이터 관리가 필수적인 기업(특히 헬스케어·금융·유럽 기업)에 최적화
     * 벤더 종속 없는 구조 및 오픈소스 덕분에, 필요 시 다른 솔루션으로 데이터 이관·전환이 매우 용이함
     * Zapier 등 클라우드 서비스 대비 비용 부담 없이 강력한 워크플로 자동화와 커뮤니티 주도 개발의 장점 제공
     * 100여 개 이상의 인기 서비스 연동 지원
          + Slack, Twitter, Discord, GitHub, Google Drive, Notion, Telegram, Trello, Stripe, OpenAI, PostgreSQL 등 다수
          + HTTP/Webhooks/Formatter/Delay 등 유틸리티도 내장

   n8n 대비 사용가능할까 하고 봤더니.. 최신 릴리즈가 2024년이군요
"
"https://news.hada.io/topic?id=21765","멜버른 남성, 집 아래에서 방대한 모델 철도 네트워크 발견","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    멜버른 남성, 집 아래에서 방대한 모델 철도 네트워크 발견

     * 멜버른의 한 신규 주택 구매자가 집 아래에서 규모가 큰 모델 철도 세트를 우연히 발견함
     * 부동산 계약 과정에서는 모형 기차 네트워크의 존재가 언급되지 않았음
     * 새 집주인 Daniel Xu는 기차 관련 엔지니어이자 열정적인 철도 애호가임
     * 해당 모형 철도는 약 60년 전 이전 집주인의 가족이 직접 설치한 것으로 추정됨
     * Xu는 이 기차 네트워크를 복원 및 업그레이드하여 미래에 더 많은 사람들과 즐길 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

집 아래에서 발견된 예상치 못한 기차 세트

     * 멜버른 북부 교외에 위치한 주택을 구매한 Daniel Xu와 그의 아내는 집을 최종적으로 인수한 후 예상치 못한 대규모 모델 철도 네트워크를 발견함
     * 보통 새 집에서는 예상치 못한 문제가 종종 있지만, 이번 경우는 주방 찬장이나 에어컨 성능이 아니라 방대한 기차 관련 설치물이 숨어 있었음
     * 이 집은 대부분 공간이 위에 띄워져 있고 그 아래에 차고가 있는데, Xu가 리노베이션을 준비하며 아래공간에 들어가다가 전체 공간이 복잡하고 넓은 모형 철도 세트로 가득 차 있는 것을 보게 됨

철도 애호가에게는 꿈같은 발견

     * Xu는 그 자리에서 아내를 불러 함께 발견의 순간을 공유하게 됨
     * 집 공개 행사나 계약 과정에서는 아무도 모형 기차 시스템에 대한 언급을 하지 않았음
     * 우연히 Xu는 철도차량 엔지니어로서 실제 열차 제조 및 철도 노선 설계 업무를 하고 있음
     * 그의 철도 사랑은 어릴 적 일본의 기차가 범죄를 해결하는 만화에서 시작되었음

세트의 유래와 상태

     * Xu는 전 주택 소유자와 연락해 세트의 내력을 확인했으며, 이 모형 철도 네트워크는 전 소유주의 아버지가 1960년대에 만든 것으로 추정됨
     * 여러 해 동안 사용되지 않아 거미줄과 먼지가 가득한 상태로 발견되었음

향후 계획

     * 구입 당시 염두에 둔 프로젝트는 아니었지만, Xu는 이 오래된 모델 철도 시스템을 복원하고 현대화하는 데 전념할 것임을 밝힘
     * 기존 각 구역을 제어하던 별도의 컨트롤 데스크는 당시엔 첨단 기술이었으나 Xu는 최신 기술로 업그레이드를 계획함
     * 이미 다양한 모형 기차를 갖고 있던 Xu는 철로 청소와 테스트 후 전체 네트워크를 복구하려 함

커뮤니티와 공유

     * Xu는 주변 지인의 자녀들이 전원이 꺼진 상태에서도 모형 철도와 함께 보내는 시간을 매우 즐긴다고 언급함
     * 그는 앞으로 더 많은 사람들과 기차의 즐거움을 공유할 계획임

        Hacker News 의견

     * 아마도 그가 비밀로 해야 했던 이유가 있었던 것 같음
       ""사업가가 영국 최대의 모형철도를 몰래 만든 이유는, 여자친구가 자신의 ‘지루한’ 취미를 알면 헤어질까봐 두려워서였다는 이야기임
       철도 매니아이자 53세인 Simon George가 8년에 걸쳐 250,000파운드로 200피트 길이의 프로젝트를 완성했다는 내용
       기사 링크
     * 이전에 다녀온 유산세일이 떠오르는 경험담임
       작은 전후 주택이었고, 기사에 나온 집과 비슷한 크기였지만, 지하실에는 이 기사보다 훨씬 더 빽빽한 모형 기차 레이아웃이 있었음
       집주인이 꽤 마른 사람이었을 것 같은데, 8인치도 안 되는 좁은 길과 2피트 폭이 최대인 통로가 이어졌던 기억
       900평방피트 지하실에서 실제로 발을 디딜 수 있는 바닥은 50평방피트도 안 됨
       나머지는 전부 레이아웃, 기차, 각종 미니어처 용품, 도료, 풀, 잔디 가루, 특수 부품 등으로 가득
       대부분 마지막에 버려졌을 것 같음
       사람마다 취미가 있겠지만, 지하실 전체를 ‘데스트랩’으로 만드는 상황은 상상도 못하겠음
       TV에 나오는 ‘호더 하우스’보다 좀 더 정돈되어있긴 하지만, 위험성이 더 커서 일반인에게 공개하는 것도 걱정이었음
       무엇보다 1층은 평범해 보여서, 집주인이 기차를 좋아한다는 사실조차 알 수 없었음
          + 이와 비슷하게, 집이 덫과 위험천만한 짐으로 가득 찬 이야기가 있는데, Collyer 형제 이야기임
            Homer와 Langley Collyer 형제는 뉴욕 할렘에서 세상과 단절된 채 집안 가득 짐으로 쌓아 자신들만의 세상을 만들었던 것으로 악명 높았던 인물
            수십 년간 책, 가구, 악기, 잡동사니, 심지어 침입자를 막기 위한 함정으로 각종 통로와 문에 장애물을 설치함
            1947년 두 사람이 집 안에서 발견될 때, 그 주변에는 140톤에 달하는 물건이 쌓여 있었고, 경찰관이 집에 진입하기 위해 몇 시간짜리 짐더미를 밖으로 꺼내야 했다는 이야기임
          + “1층은 평범해 보였고, 그 사람이 기차에 빠져 있다는 걸 아무도 눈치채지 못했었다”는 말이 무의식적으로 기차 애호가를 괴짜로 보는 뉘앙스가 느껴져 재미있다는 생각
            사실 어느 정도 맞는 말 같다는 의견임
          + 이제는 Factorio 같은 게임의 진정한 가치가 보인다는 생각
            현실에서 그런 습성을 표출하는 대신, 게임에서 그 욕구를 해소하는 것이 필요하다는 주장
          + 그는 신경다양성을 가진 사람이었던 것 같고, 아내가 집착을 지하실에만 허락했던 듯한 인상임
     * 딸이 두 살이었을 때 어느 날 스트립몰에서 모형 기차 전시회를 보고 방문한 경험
       작고 평범한 취미용 가게였지만, 구석에 있는 계단을 통해 들어갔더니 엄청난 창고 공간 가득 모형 기차, 선로, 미니어처 풍경들이 펼쳐졌음
       딸은 역대급 크기의 토마스 기차 세트에 완전히 빠졌고, 나는 다양한 미니어처 풍경을 구경하며 즐거웠던 추억임
     * 나는 사실 기차에는 관심이 없지만 언젠가 지하실에서 1970년대 컴퓨터룸을 발견하고 싶다는 상상
       냉각수관, 3278·3279 터미널, 테이프 드라이브까지 완비된 환경이면 정말 멋질 것 같음
       지하실을 발견하자마자 제일 먼저 할 일은 혹시 수십 년간 실종된 사람은 없는지 경찰에 확인하는 일일 것임
     * 정말 멋진 이야기지만, 그날은 집 검사관이 일을 대충 한 것 같다는 생각임
          + 이곳이 멜버른인데, 공급 부족으로 대부분 경매 방식으로 집이 팔림
            그래서 많은 사람들이 건축 안전점검을 생략하고 거래를 성사시키는 것이 일반적임
            점검 없이 집을 산 것이 이상하지 않다는 의견임
          + 이 정도 규모의 omission(누락)이면 실제로 기차도 지나갈 수 있겠다는 농담임
          + 검사관 입장에서는 따로 언급할 이유가 없었을 것임
            구조적 문제만 보면 되고, 기초는 그대로 보였기 때문에 일 자체에는 지장 없었을 것
            또 남들이 모형철도의 존재를 몰랐을 것이란 점도 감안해야 한다는 입장임
          + 나는 우리 집 지하실에 있었던 석면도 못 알아본 검사관을 경험했음
            차라리 모형 기차가 있었으면 좋았을 것 같다는 농담
          + (호주 기준으로) 집 검사관들은 비싸면서도 별로 쓸모없음
            내가 보고서 산 검사관도 지하실이나 다락은 아예 확인도 안 했음
     * 집을 샀더니 단순히 주택만 얻는 것이 아니라, 누군가의 오랜 꿈같은 세계까지 발 밑에서 얻는 상황은 일종의 타임캡슐 발견 기분임
          + 몇 년 전 가족 친구의 빈집에 몇 달간 머문 적 있음
            비즈니스에 쓰던 집이라 남은 상품, 책, 옛날 옷 등 신기한 물건이 가득
            무엇보다 인상적이었던 건 최소 50년은 됐을 모형 기차 세트였음
            내 조부모님 댁도 비슷한 분위기였다는 경험담임
     * “어떻게 이런 일이?”
       문으로 들어갈 수 있었는데, 집주인·부동산 중개인·구매자 본인·다른 잠재 구매자, 심지어 건물 검사관도 도대체 누구도 그 문 뒤를 안 열어본 것 같다는 의문
       계약 전 기초/지하실을 직접 또는 전문가에게 확인시키는 게 상식 아닌지 묻고 있음
          + 내가 조사한 집 검사관들(빅토리아, 태즈메이니아)은 모두 접근 가능한 구역만 점검한다고 명확하게 안내함
            닫혀 있거나 접근 불가한 공간은 기본적으로 검사 대상이 아님
          + 멜버른 부동산 시장은 정말 미쳤다는 말
            임금에 비해 집값이 아주 높고 계속 오르기만 함
            수입의 절반 정도를 주거비로 쓰는 사람도 흔함
            ‘지금 당장 안 사면 영영 못 살 것’이라는 대중적 인식에 따라 검증·점검 등 신중한 절차를 생략하는 경향이 많음
          + 호주에서는 이런 일이 드물지 않다는 의견
            주택 시장이 워낙 경쟁이 치열하고, 구매자가 세세하게 굴면 오히려 매매 기회를 놓치기 쉬움
            검사관 업무도 누가 의뢰하고 얼마나 지불하냐에 따라 달라짐
            예를 들어 ACT(캔버라)에서는 판매자가 검사받는 것이 의무이지만, 최저가 입찰로 진행되어 보고서도 최소한만 작성하는 게 일상적임
            구매자가 직접 검사 요청하면 돈 쓴 만큼 자세히 검사 가능
          + 진짜 아깝다는 생각임
            만약 부동산 중개인이 이 좋은 지하실 공간을 미리 알았으면 집값을 세 배로 올릴 수도 있었을 것임
            (농담이면서도, 실제로 호주 집값이 미친 듯이 비싸다는 점을 반영)
          + 검사관이 5만 불, 10만 불, 20만 불어치 문제를 발견해서 수리비 감액을 요청해도, 그런 점을 신경 쓰지 않는 다른 구매자가 반드시 나타나기 마련
            판매자의 번거로운 삶을 덜어주고 거래를 최대한 빠르게 성사시키는 것이 실리적이라는 해석임
     * 요즘엔 HN 제목에 AI 단어가 들어가면 무조건 AI 관련일 듯 기대하게 됨
       “model train network”가 그런 느낌이었다는 얘기임
          + 이제 넷플릭스 코미디 아이디어가 떠오른다는 소감
            엄청난 모형 기차 레이아웃과 Lionel 모델 노하우를 지닌 노신사가 실리콘밸리 AI 스타트업 CEO로 스카우트되며 생기는 소동을 그려보고 싶다는 상상임
     * “model train network”?
       요즘은 네트워크 말만 들어도 ‘대형 언어모델 트레이닝 네트워크’ 생각부터 먼저 남
          + 엔지니어 채용할 때 키워드 체크를 꼼꼼히 해야겠다는 의견
            “bringing it up to code” 같은 표현도 애매할 수 있다는 일침
     * 혹시 그 집엔 Klein bottle이 있었는지 궁금함
"
"https://news.hada.io/topic?id=21715","SymbolicAI: LLM에 대한 신경-기호적 관점","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     SymbolicAI: LLM에 대한 신경-기호적 관점

     * SymbolicAI는 신경-기호적 프로그래밍 프레임워크로, Python 프로그래밍과 LLM을 자연스럽게 결합함
     * 기본 단위인 Symbol 객체를 통해 문법적(syntactic)과 의미적(semantic) 조작을 모두 지원함
     * 계약(Contracts) 기능은 LLM의 예측에 데이터 유효성 검사와 자동 수정 로직을 적용해 신뢰성과 견고함을 보장함
     * 다양한 외부 엔진(OpenAI, Anthropic, huggingface 등) , 웹검색, 이미지 생성, 음성 처리와 연계 운용이 가능함
     * 우선순위에 기반한 구성 관리 시스템과 강력한 커스터마이즈 기능이 특징임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

SymbolicAI: 프로젝트 개요 및 중요성

     * SymbolicAI는 고전적인 Python 프로그래밍과 차별화된, 프로그래머블한 LLM(대형 언어 모델) 의 결합을 자연스럽게 지원하는 신경-기호적(Neuro-Symbolic) 라이브러리임
     * 모듈러 설계 덕분에 손쉽게 확장, 엔진 교체, 툴 연동이 가능함
     * Local 엔진, 웹 검색, 이미지 생성 등 다양한 도구와 연동되어 실험적·실용적 활용에 모두 적합함

주요 개념 소개

  Primitives

     * Primitives는 기본 구성 단위로, 핵심은 Symbol 객체임
     * Symbol 객체는 문법적(syntactic) 및 의미적(semantic) 모드 모두 지원함
          + 문법적(syntactic): 일반 Python 값처럼 동작하며 비교, 연산 등 안전하고 빠른 동작 구현
          + 의미적(semantic): 신경-기호적 엔진과 연결되어 의미와 맥락을 이해하며, 유연한 의미 비교/조작 가능
     * 문법적 모드가 기본이며, 엔진 동작이 필요한 경우에만 의미적 처리로 전환 가능함

    의미적/문법적 모드 전환 방법

     * 생성 시 semantic=True 옵션 지정
     * .sem(semantic), .syn(syntactic) 속성으로 자유롭게 전환
     * 의미적 함수 호출(map 등) 시 자동 의미 모드 변환

    연산 예시

     * == : 문법적은 리터럴 비교, 의미적은 의미론적 동등성(예: 'Hi' == 'Hello' True)
     * +, &, .startswith(), .choice(), .foreach(), .cluster(), .similarity() 등
     * 복합적인 의미 기반 조작과 논리적 체이닝이 가능함

  Contracts

     * LLM의 불확실성을 보완하는 Design by Contract 원칙 도입
     * 데이터 모델과 유효성 검사 규칙을 데코레이터로 지정
     * 잘못된 입력/출력에 대해 자동으로 오류 수정, 재시도, 히스토리 누적 등 다양한 안정성 기능 지원
     * Pydantic의 데이터 모델 호환 및 필드 검증, 자동 프롬프트 생성, 오류 처리 등 내장

    계약(Contract) 기능 주요 특징

     * pre_remedy/post_remedy: 입력/출력 오류 자동 수정
     * accumulate_errors: 오류 히스토리 입력
     * remedy_retry_params: 재시도 제어 파라미터(시도 횟수, 딜레이, 백오프 등) 지정

     상세 예제 및 추가 설명은 공식 문서 및 DeepWiki 페이지 참고 가능

외부 엔진 및 기능 확장성

     * 현재 OpenAI, Anthropic 등 다양한 신경-기호 엔진 지원(API/로컬)
     * {huggingface, llama.cpp} 등 자체 엔진 로컬 구동 가능
     * 음성, 이미지, 웹검색 등 추가 엔진 연동 가능(별도 의존성 패키지 설치 필요)
     * 검색, 클러스터링, OCR, 인덱싱 등 실전 ML/AI 기능을 통합적으로 제공

구성(Configuration) 관리 시스템

  우선순위 기반 구성 파일 관리

     * 디버그 모드(프로젝트 폴더): 개발·테스트용
     * 파이썬 환경별 설정({python_env}/.symai/)
     * 글로벌 설정(~/.symai/): 기본/백업용
     * 세 가지 위치에서 우선순위 높은 항목을 자동 적용

  주요 구성 파일

     * symai.config.json: SymbolicAI 주요 옵션 관리
     * symsh.config.json, symserver.config.json: 셸, 서버용 설정

    구성 파일 예시

     * API Key, 모델명, 엔진 종류 등 명시적 지정
     * SUPPORT_COMMUNITY 옵션으로 데이터 수집 동의(연구 및 품질 향상 목적)
     * 사용자 경고(SYMAI_WARNINGS) 환경변수로 켜기/끄기 제어 가능

  환경셋팅 및 테스트

     * ffmpeg(음성), chromedriver(웹크롤러) 등 외부 패키지 필요
     * 테스트 실행은 pytest, 커버리지 확인 지원

참고 자료 및 활용 가이드

     * DeepWiki와 공식 깃북에 풍부한 참고자료 및 동영상 튜토리얼 제공
     * Arxiv 공개 논문에서 이론 및 프레임워크 상세 설명
     * BSD-3-Clause 라이선스 적용

결론

     * SymbolicAI는 기호 시스템의 명확성과 신경망의 유연성을 결합해 LLM 기반 신뢰성 중심 서비스 및 실험적 연구에 매우 적합한 프레임워크임
     * 구성, 확장성, 신뢰성 확보에 초점을 둔 설계로, 스타트업 및 IT 실무자에게 다양한 응용 가능성을 제공함

개발자와 커뮤니티 지원

     * 컨트리뷰션 자원, 연락처 및 지원 채널(이메일, 웹사이트, Discord) 적극 공개

        Hacker News 의견

     * 이런 식의 부두 마법이 정말 흥미로움.
       내가 재미있게 봤던 예시로는 의미론적 map lambda 사용,
       예를 들어 S라는 Symbol 리스트가 있을 때 S.map('모든 과일을 야채로 변환') 호출 시 과일만 야채로 바뀌고 다른 것은 그대로 유지됨
       또 맥락에 따라 parameter를 받아서 비교할 수도 있음. 예를 들어 인사말 문맥 안에서 'Hello, good morning!'과 'Hi there, good day!'가 같은 의미인지 판별, 혹은 공손함 레벨로 'Good morning, sir.'와 'Hey, what’s up?'을 비교도 가능
       또한 비트 연산자처럼 의미론적으로 논리적 결합도 가능. 불렛처럼 규칙과 관찰을 &로 합쳐서 결론 도출
       interpret() 함수가 파워풀해 보임
       OP에게 이 프로젝트에 영감을 준 요소, 실제 적용한 분야, 그리고 가장 좋아하는 활용사례가 뭔지 궁금함
          + Lotus라는 python dataframe 라이브러리 추천
            모든 핵심 관계형 연산자를 쉽게 의미론적으로 확장해서 사용 가능
            각 호출이 ‘모델’ 포인트가 돼서 나중에 머신러닝 쪽 확장도 쉬움
            snowflake 같은 클라우드 SQL도 점점 이런 방향으로 가는 느낌
            louie.ai에서 유사한 시스템 구현함. AI notebook, dashboard, API를 통해 (splunk, databricks, 그래프 db 등) 데이터를 대화식으로 다루고, 문맥 기반으로 symbolic + semantic operator 자동 파악함
            실무에서 정말 도움 됨
            내 주요 use case는:
            semantic map으로 splunk 인덱스에서 경고 가져와 의심스러운 것에 플래그 추가, 설명란도 넣고,
            이를 semantic reduce로 요약해서 자연어로 리포트까지 만듦
          + 당근이 사과의 야채 변환 결과인 이유가 뭐냐는 질문
          + 답변하려면 매우 길어질 내용임
            2022년 말부터 프로젝트가 시작됐지만, 최근에는 모델이 좋아졌을 뿐 기초적인 부분들은 이미 GPT-3 시절부터 있었음
            최근에 등장한 DbC(Design by Contract)이 정말 독특함
            에이전트 관련해서 내가 겪었던 모든 문제를 해결함. 특히 여러 contract를 체인으로 연결하면 guardrail이 자연스럽게 전파돼서 효과적임
            거의 모든 커스텀 툴을 직접 구현
            예를 들면 OpenAI의 web search가 좋긴 하지만 커스터마이즈가 부족해서, 자신만의 deep research agent를 개발해 활용 중임
            첫날 결과물 사례 스레드
            회사 운영 중이며, contract 3개를 연결해 e2e 문서 생성 자동화도 구축함
            데모 PDF 참고
            입력 프롬프트는,
            “파일에서 패턴 분석, 다양한 프롬프트 포맷(XML, markdown 등), 아첨 경향, 도구 사용 방식, 윤리적 가드레일, 아키텍처적 특이점 등 전반 비교해 리포트 생성” 요구
            contract는 2025년 3월 이 포스팅에서 소개했고, 이후 많이 진화했지만 기본 취지와 동기는 동일함
     * 의미론적 ==, + 같은 연산자를 쓸 수 있다는 게 신선한 아이디어에 비료를 주는 느낌
       워드 임베딩 나오던 초기에 ‘King - Man + Woman = Queen’과 비슷하게 개념 대수라는 걸 처음 접했을 때 같은 흥분
       다만 여기서의 신경망과 기호(logic) 통합은 대부분 시스템들과 마찬가지로 아직 얕거나 구분돼 있음
       참조
       진짜 혁신은 앞으로 훨씬 근본적인 통합이 이뤄질 때 가능할 것
       우리 회사(Onton)에서도 그걸 향해 연구 중임
       목표는 1) 완전히 통합된 표현(심볼릭/딥러닝 모두 아님), 2) 적은 노이즈 데이터로도 연속 학습(망각 X), 3) 수학/심볼 연산 신뢰성 100%, 4) 환각(hallucination) 제로
       지금은 핫글루 식의 여러 시스템 붙이기가 유용하긴 하지만, 통합형 아키텍처가 판을 갈아엎을 것 같음
     * 설명과 예제가 잘 나와있는 공식 코드 노트북과 공식 논문 PDF 링크 공유
     * 코드 내 오류 제보(‘correctness contracts’에서 valid_sizes가 정의 안 됨)
          + 피드백 감사합니다. 리팩토링 과정의 흔적이 남은 것임. 이제 수정함
     * 이 주제에 대해 의견을 주고 지원해준 모든 분들께 감사
       이런 반응은 기대 안 했고, 언제든 메일/트윗으로 소통 가능
       여러분과 대화해서 정말 좋았음
     * 아쉬움이 있음
       ‘Symbolic AI’라는 용어 자체가 이미 엄연히 정의돼 있음
          + 그 의견 충분히 들었음
            조만간 이름 바꿀 수도 있을 듯
            논문에도 명명 이유를 각주로 넣었고, 기초 연구를 이끈 Newell과 Simon에 대한 존경의 의미로 프로젝트 명명함
     * 궁금한 점이 있음
       비용 정책이 어떻게 되나
       자연어 연산 포함된 한 줄을 실행할 때마다(외부 API 쓰면 더더욱) LLM 추론 비용을 계속 치르게 되는 구조인지
       예를 들어 ""symbolic"" 함수가 반복문에서 계속 호출될 때도 그런지
          + 맞음
            예를 들어 openai API 사용 시 의미론적 연산마다 openai 호출이 일어나 비용 발생
            llama.cpp 등으로 로컬 LLM 직접 호스팅하면 호스팅 비용만 들고 별도 추론 비용 없음
          + 반드시 캐시 같은 게 필요할 거임
     * 기대 안 했는데 갑자기 인기가 많아져서 좀 놀람
       원래 자고 있었어야 했는데 현재 수면 부족 경험치를 살려 대화에 계속 참여 중임
     * 함수형 프로그래밍(FP)처럼 모든 Symbol이 순수 값으로 동작
       연산들이 깨끗하고 트레이스 가능한 플로우로 합성됨
       애매한 단계에선 모델이 개입
       FP의 IO 연산처럼 생성 모델 호출이 범위 내 부수효과로 취급
       평상시엔 reasoning 그래프가 결정적이고 필요할 때만 모델에 위임함
       데모가 정말 신기함
          + 거의 맞음
            처음부터 함수형으로 설계함
            로우 레벨에서도 전부 함수형 원칙이고 내부적으로도 functional.py나 core.py로 불림
            데코레이터도 도처에서 쓰고 있어서 리팩토링/확장/버그 관리 등에 큰 도움이 됨
     * 요즘 LLM이 코드를 다 만들기도 하는데,
       Symbol 같은 문맥을 담으며 python operator로 쉽게 조작되는 구조가
       사람이 일일이 체크해가며 코드 짜는 것에 비해 어떤 강점이 되는지 궁금함
       예를 들어 의미론적 변환을 저런 식 문법으로 직접 할 수도 있지만, 그냥 LLM에게 과일 리스트를 야채로 변환하는 프로그램 만들어달라고 프롬프트해도 되는 거 아닌가
       본질적 차이가 뭔지 이해하고 싶음
          + 환각(hallucination) 방지 효과가 있다고 봄
            LLM이 포멀 시스템을 생성하게 하면, 범용 시스템보다 검증이 훨씬 쉬움
"
"https://news.hada.io/topic?id=21792","Cloudflare, AI 데이터 크롤러 기본 차단 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Cloudflare, AI 데이터 크롤러 기본 차단 도입

     * Cloudflare가 A.I. 데이터 스크래퍼를 기본적으로 차단하는 새로운 설정을 도입함
     * 웹사이트 소유자는 권한을 부여하지 않는 한 자동으로 AI 봇의 크롤링이 차단됨
     * AI 모델 훈련에 필요한 고품질 데이터의 수집과 관련해 인터넷 콘텐츠 보호 역할을 강화함
     * 콘텐츠 제작자와 미디어는 무단 데이터 사용에 대한 우려를 계속 표명함
     * AI 기업과 콘텐츠 소유자 간의 법적 분쟁이 점점 더 빈번해지는 추세임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cloudflare AI 데이터 스크래퍼 기본 차단 기능 도입

     * Cloudflare는 온라인 트래픽 관리와 보안 서비스를 제공하는 기술 기업
     * 최근 AI 기업들이 웹사이트 데이터를 무단으로 수집하는 현상이 증가함에 따라, Cloudflare는 고객이 자동으로 AI 데이터 스크래퍼의 접근을 차단할 수 있는 권한 기반 새 설정을 도입함

새로운 기본 정책과 그 변화

     * 새롭게 도입된 이 기능을 통해, 웹사이트는 기본적으로 AI 봇의 자동 크롤링(스크래핑) 을 차단할 수 있음
     * 데이터 크롤링이 필요하다면, 웹사이트 소유자가 별도로 접근 권한을 수동 부여해야 함
     * 기존에는 Cloudflare가 해커나 악성 행위자로 간주하지 않은 봇이 자유롭게 웹사이트 정보를 수집할 수 있었음

Cloudflare의 정책 변화 이유

     * Cloudflare CEO인 Matthew Prince는 이번 조치가 ""인터넷상의 원본 콘텐츠 보호와 웹 퍼블리셔 권익을 강화하기 위함""이라고 강조함
     * AI 기업이 무단으로 인터넷 데이터를 활용하게 되면 콘텐츠 제작자가 새로운 콘텐츠를 생산할 유인이 감소하는 문제가 발생함
     * Cloudflare 네트워크는 전 세계 인터넷 트래픽의 약 20% 를 처리함
     * 최근 웹상에서 AI 크롤러 활동이 급증하고 있어 이에 대응하는 정책 추진임

AI 데이터와 업계 갈등 심화

     * OpenAI, Anthropic, Google 등 AI 모델 개발을 위한 대규모 데이터 수집 경쟁이 치열함
     * 고품질의 웹 데이터는 AI 모델의 정교함과 결과물 품질에 핵심적인 역할을 하고 있음
     * 그 결과, 웹사이트 운영자, 언론사, 저작권자들은 무단 데이터 수집 및 미보상에 대해 항의함

법적 분쟁 확대 사례

     * 2025년 6월 Reddit가 Anthropic을 상대로, 2023년 The New York Times가 OpenAI 및 Microsoft를 상대로 AI 훈련 데이터 무단 활용과 저작권 침해라는 이유로 각각 소송을 제기함
     * OpenAI와 Microsoft는 이러한 저작권 침해 주장에 대해 부인하고 있음

결론

     * Cloudflare의 새로운 AI 데이터 수집 기본 차단 정책은 AI 산업과 콘텐츠 소유자 간 데이터 접근 및 활용의 윤리∙법적 기준에 중대한 영향을 미침
     * 이번 정책 변화는 AI 생태계 내에서 콘텐츠 권리 보호와 사전 동의의 표준을 정립하는 데 중요한 계기가 되고 있음.

   Cloudflare, AI 봇을 위한 크롤링별 과금(pay-per-crawl) 도입

        Hacker News 의견

     * 사람들이 잘 인식하지 못하는 부분으로는, 우리가 온라인에서 하는 거의 모든 행위가 지금까지 OpenAI, Anthropic 같은 회사에 무료로 훈련 데이터를 제공하는 역할만 해왔고, 그 과정에서 실제 가치를 만든 인간들은 소외된다는 점 강조 방향성은 100% 맞다고 생각하지만 이 해결책이 혁신적이라고 보긴 어렵고, AI 기생 현상에 대항하는 훨씬 더 많은 혁신 필요성에 대한 희망 표현
          + Cloudflare나 비슷한 업체들이 오히려 인터넷의 자유로움을 망치고 있다는 주장 실제로 인증을 핑계로 너무 자주 사이트가 느려지거나 차단이 돼서 1998년 수준의 느린 로딩 경험 발생 광고회사가 사용자를 추적하고 수익화하는 것을 허가하는 브라우저만 정상적으로 쓸 수 있는 시대가 되고 있고, Cloudflare는 문제 해결이 아니라 중간에 끼어서 돈을 챙기고 전체 인터넷 경험의 질을 떨어뜨리고 있다는 비판
          + 나는 온라인에 글을 쓰는 이유가 내 생각을 공유하고 싶기 때문이며, 이것이 인공지능 훈련 데이터로 쓰이든 사람이 읽든 상관없이 만족함
          + 나는 공개된 인터넷 데이터를 자유롭게 AI 훈련에 사용하는 것이 100% 괜찮다고 생각함 다만 너무 빠른 속도의 크롤링으로 소규모 웹사이트 운영이 어려워질 만큼 부담을 주는 것은 절대 용납할 수 없으며, 이런 현상은 진정한 공공재의 비극으로 생각함
          + Cloudflare가 정말로 모든 AI 데이터 스크래핑을 완벽하게 막을 수 있을지 회의적임 이런 조치가 실제로는 크롤링을 더 어렵고 비싸게 만들어서, 매일 모든 페이지를 긁어방송자의 비용을 발생시키는 걸 차단하겠지만, 결국 데이터가 여러 데이터셋 어딘가엔 들어가게 될 것이라는 전망
     * 해당 기능을 켜봤는데 robots.txt가 자동으로 바뀌는 것만 확인함 어떤 추가적인 동작이 있는지는 확실치 않음 robots.txt 파일에 다양한 AI 봇과 크롤러를 차단하는 설정이 추가됨
          + ""User-agent: CCBot disallow: /""라는 부분을 보고 CCBot(Common Crawl)은 AI 전용인지 의문 제기 CCBot이 이미 예전부터 여러 robots.txt에 차단되어 있었음 Common Crawl이 컨텐츠 사용 방식까지 통제할 수 있는지, CC가 페어유스(fair use)에 의존한다면 라이선스 비용 청구나 2차 이용 허락과 관련된 권리가 실제로 있는지 궁금함 웹사이트 약관이 사이트 운영자가 타인의(사용자) 컨텐츠를 LLM 용도로 재라이선스하고, 수익을 공유할 수 있도록 허용하는 경우가 많은지 의문
          + Cloudflare가 오리지널 컨텐츠 보호를 이유로 기본 설정을 바꾼다고 하지만, 실제 robots.txt 안내문엔 예외적으로 'AI RAG(Retrieval Augmented Generation)' 용도 자체는 허용하는데, 언어모델 훈련보다 오히려 RAG가 작가들의 수익을 훨씬 실시간으로 저해하니 이상함
          + 구글도 결국은 웹 인덱싱과 AI 훈련을 동시에 위해 데이터를 긁어가고 결국 선택을 강요할 수 있다는 생각 웹사이트 운영자가 만약 AI 학습에 협조하지 않으면 구글 검색 노출도 포기해야 하는 상황이 오면 굉장히 미묘한 딜레마 발생
          + 내 개인적인 취미 사이트들에서는 user-agent에 'bot'이 들어가 있고 robots.txt, humans.txt, favicon.ico 외 파일을 요청하면 444 스테이터스(즉시 연결종료)로 응답 검색엔진 대부분은 CIDR 블록을 블랙홀로 처리함 아마 이런 식으로 하는 사람은 나뿐일 것 같음
          + 신문사 중에는 LLM 크롤러뿐 아니라 ChatGPT 검색 기능까지 동시에 막은 사례가 있는데, Cloudflare의 설정은 그보다는 훨씬 합리적인 기본값
     * 이번 헤드라인은 다소 오해를 줄 수 있음 현재는 Cloudflare를 쓰는 사이트가 '옵트인'식으로 AI 봇 전체 차단을 빠르게 켤 수 있게 됐지만, 기본값은 아니며 자동 적용은 아님 Cloudflare가 원하면 이런 일괄조치를 임의로 할 수 있다는 점이 그들의 권한이 막강함을 보여줌
          + 정말로 기본값이 아니냐는 질문 등장 Cloudflare 공식 블로그 글에선 기본 설정 변경을 언급
          + 이제는 AI 봇과 웹사이트 간에 적대적 관계가 됐음 Cloudflare는 그 상황에 단지 대응하고 있을 뿐 DDoS 방어도 비슷한 맥락 아닌지 질문
          + Cloudflare가 제대로된 방어 수단보단 인터넷을 느리게만 만드는 것 같음 실제로 제공하는 챌린지를 반나절 안에 뚫지 못한 적이 없음 이러한 조치는 결국 AI SEO를 위한 중개 시장에 몸을 담기 위한 첫 단계로 생각함 Cloudflare 본질은 사이트를 위해서가 아니라 스크래퍼와 퍼블리셔 중간에서 수수료를 챙기려는 시도로 해석 Cloudflare를 미워함
     * Cloudflare를 통해 서비스되는 내 데이터 사용량이 2년 전 20gb에서 100gb로 급증함 실제 방문자는 절반 이하로 줄었는데도 트래픽만 커진 걸로 봐서, Cloudflare 입장에선 리소스 비용 절감을 위해 이러한 조치를 한다고 예상
          + 대규모 트래픽에서 대역폭 비용이 비싸다는 점에 수긍
     * HN에선 봇 트래픽 때문에 웹사이트가 느려진다는 불평이 종종 나오는데 사이트 운영자로서 이해가 잘 안됨 Cloudflare 캐시를 기본적으로 쓰면 트래픽 대부분이 캐시로 소화되고, 서버 부담 거의 없을 텐데 CPU와 대역폭 비용도 요즘 많이 저렴하지 않은가 다들 왜 이렇게 민감한지 궁금함
          + 나도 큰 사이트를 운영하는데 수십만 페이지에 걸쳐 10개 언어로 제공, 총 수백만 페이지 규모며 RPS 1000 가까이 나옴 AI 크롤러가 한 번에 100~200 RPS를 여러 IP로 분산해서 보내면 비싼 엔드포인트를 집중적으로 두들기고, 기존 로봇 차단조치로도 감당이 안 돼 DDoS와 비슷한 효과 발생
          + 나 역시 AI 트래픽으로 다운된 경험 있음 캐시를 여러 단계로 사용해도, 공개 API 중 인증 없이 노출되는 일부 엔드포인트는 캐시가 안 되는데, 이 부분을 반복적으로 두들기면 결국 사이트가 내려감 수백만 개의 페이지를 짧은 시간 내에 미스캐시로 동시 리젠하면 요청이 쌓이고 에러가 발생, 에러는 캐시 미완성으로 돌아가는 악순환 AI 트래픽이 이런 엔드포인트를 계속 때리면 문제 지속
          + 기본적으로 Cloudflare를 쓴다는 가정부터가 문제임 단순 사이트 운영에도 이게 필수요소가 된 시대인가 반문
          + 사람들이 주로 지적하는 AI 트래픽 문제는 ‘date range로 모든 포스트를 조회’ 같은 API를 끝없이 두드리는 크롤러에 많이 집중됐음
          + 나 역시 이런 강한 반응이 다소 의아하지만, 최적화가 안 된 사이트에선 AI 크롤러로부터 탈출하고 싶어하는 건 이해 관련 도구 중 빠르게 가장 많이 확산된 것이 Anubis checker로 보임 github 링크 AI 트래픽이 분산 크롤러임을 감안할 때도, 협업적으로 제한하는 오픈소스 소프트웨어가 더 활성화될 필요성 느낌 HTTP 상태코드 ""420 Enhance Your Calm"" 식의 대응이 웹에 더 필요함을 언급 참고 링크
     * AI 회사들이 허락 없이 여러 웹사이트 데이터를 써버리면 디지털 컨텐츠 생산 의욕이 사라질 테고, 결국 AI의 성장 구조 자체가 다른 디지털 활동을 저해하며, 마지막엔 웹까지 파괴할 것이라는 걱정 표현 (재미있게도 AI에게 웹 자체가 '먹잇감'임)
          + 전 세계적으로 광고 차단기 사용률이 30%라서, 디지털 컨텐츠 저해 논리가 좀 더 복합적이라는 의견
          + AI가 궁극적으로 하고 싶은 일(인간과의 다양한 상호작용 및 경제활동)이 이뤄지지 않으면 AI도 결국 데이터 자원이 고갈됨
          + 지금은 자본주의가 사람을 주된 착취 대상으로 삼게 됐듯, AI도 구조적으로 비슷한 길을 걷는다는 시각
     * Cloudflare가 사전 구성된 규칙을 통해 robots.txt를 지키고 정직하게 행동하는 AI 봇이나 크롤러는 잡아낼 수 있지만, 이미 Perplexity 등 일부 회사는 트래픽을 위장하고 있어 이런 차단이 오히려 정직한 봇만 피해를 보고, 은폐를 유도하지 않는지 의문 이런 현상, 즉 암전(arms race)은 20년 전부터 지속되어왔고 새로운 현상은 아님 Cloudflare의 글로벌 신호와 봇 스코어링, 트래픽 지문화 덕분에 위장한 AI 봇도 잘 판별해내는 구조임 관련 설명 링크도 공유 blog.cloudflare.com 참고
          + 정직하고 투명한 봇만 규제하면 위장된 은폐 크롤링만 더 늘지 않냐는 질문에, 이미 20년 넘게 이런 암투가 벌어져왔고 완전히 새로운 일도, 답도 아니라고 생각함
          + Super Bot Fight Mode의 Block AI Bots 옵션이 실제로 봇 트래픽 대부분을 잘 제거해줬고 robots.txt나 user agent 의존성이 아니라 트래픽 패턴을 분석하는 접근이었음 덕분에 내 툴이 작동하려면 별도 우회 규칙 필요할 정도
          + Cloudflare는 그들이 좋아하지 않는 유저에겐 인터넷 사용을 무척 힘들게 만들 방법을 잘 알고 있음 robots.txt로 봇이 악의적으로 분류되지 않게 해두면 사이트 단위로 허용 여지가 생기지만, 나머진 Cloudflare만의 처리법을 쓰게 될 것
          + 대량 스크래핑하는 AI 봇은 클라우드플레어 입장에선 사실상 숨기기 어려우며, 지문화 및 네트워크 레벨에서 여러 신호로 판별 가능 실제로 대기업들은 Cloudflare와 직접 소통하고 있어서 결국 경고나 대응 등 다양한 수단이 있는 상황으로 예상
     * 주요 AI 회사들은 정말로 robots.txt를 존중할까 공개된 크롤러 일부만 지킬지 모를 뿐, 결국 물 밑에선 숨겨진 방식으로 크롤링을 돌리고 있는 의심 실제로 책, 이미지, 유저 데이터까지 불법적으로 긁어다 쓴 전적이 있음
          + 소규모 크롤러들이 대형 기업을 가장해 차단을 피하려는 경우도 많아 구분이 어려움
          + robots.txt를 지키는 것은 어디까지나 관례일 뿐이며, 딱히 법적/기술적 강제는 없음 약관에 robots.txt 정책 준수 조항을 명시할 수도 있지만, 실제 위력은 의문
          + 이 트래픽의 상당 부분이 AI가 직접적 스크래핑이 아니라, 인간이 AI에게 질문하면 그때 인터넷을 조사하러 다니는 형태일 수도 있음 브라우저처럼 리서치 용도까지 robots.txt 규제범위로 볼지, 인덱싱 용도로만 볼지 혼란
          + Cloudflare의 게이트키퍼 역할 자체는 탐탁지 않지만, AI회사에 실제로 영향력을 행사할 수 있는 위치임
     * 현재 차단 대상 AI 봇 목록은 매우 짧음 참고 링크
          + Cloudflare 설명에선 Common Crawl(CCBot)도 AI Bots로 구분되어 차단 옵션에 들어감 하지만 Common Crawl은 실제로 AI 전용 봇이 아니며, 오픈 크롤링 인프라임 참고 링크
          + Cloudflare는 방대한 웹 트래픽을 관측하고 있어서 지금 잡힌 봇 리스트는 발생 빈도가 높은 것 위주이고, 앞으로 발견되는 봇이 있으면 계속 추가될 듯 실제로 모든 크롤러를 차단하는 건 불가능하겠지만 인터넷 전체를 기준으로 보면 Cloudflare만큼 많은 봇을 탐지할 곳은 드물 것
          + 이런 차단만으로도 대다수 사이트의 트래픽을 절반 이하로 줄이는 효과 있을 듯
     * 웹의 주요 진화(웹2의 울타리화, 웹3의 허망함, 그리고 지금)는 점점 커뮤니티와 지식 저장소 역할에서 멀어지는 방향에 가까움 퀄리티나 기술은 계속 발전해오면서도, 그만큼 상실된 것도 많음
"
"https://news.hada.io/topic?id=21770","연방준비은행의 ‘100만 달러 현금 큐브’, 실제로는 155만 달러","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 연방준비은행의 ‘100만 달러 현금 큐브’, 실제로는 155만 달러

     * 시카고 연방준비은행 Money Museum의 ‘100만 달러 큐브’ 전시물은 실제로 155만 달러 가치임
     * 전시장 안내문은 100만 달러라고 홍보하지만, 실제 현금 다발 개수를 계산하면 55만 달러 초과임
     * 단순하지만 새로운 웹 도구 Dot Counter 개발로, 사진 속 객체 개수를 효율적으로 셈 가능함
     * 미국 1달러 지폐의 크기와 비율 문제로 완전히 딱 맞는 100만 달러 정육면체 설계가 어려움
     * 대부분의 방문객은 전시물의 실제 금액 차이를 알아차리지 못하지만, 주의 깊은 계산이 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

시카고 연방준비은행의 ‘100만 달러 큐브’ 의 진실

   시카고 연방준비은행 Money Museum의 한가운데에는 투명한 큐브가 전시되고 있음
   이 큐브 내부는 1달러 지폐 뭉치들로 가득 채워져 있으며, 안내문에는 100만 달러가 들어 있다고 설명함

     “100만 달러가 어떻게 생겼는지 궁금하다면, 더 이상 상상하지 않아도 된다. 여기 직접 볼 수 있다!”

   그러나 안내문만으로는 신뢰가 어렵고, 직접 셈을 해보는 자세가 필요함

실제로 돈의 개수 세어보기

   현장에서 큐브의 뭉치 수를 세어보려 했으나, 큐브가 커서 한눈에 세기 어려움
   사람들의 시선을 끌게 되고, 사진을 확대해서 세어봐도 정확한 개수 파악이 어려움
   사진 안의 특정 객체를 클릭하여 셀 수 있는 웹 기반 카운팅 도구가 필요했지만, 쉽게 접근할 수 있는 앱이 존재하지 않음
   대부분의 기존 소프트웨어는 과학적 이미지 분석용 또는 다른 특수 용도임
   직접 간단하게 쓸 수 있는 Dot Counter라는 웹 툴을 개발함. 이미지를 올리고 클릭만 하면 바로 개수 집계 가능함
   이 도구를 통해 이 큐브의 실제 현금 개수를 파악할 수 있음

큐브 내부 현금 총액 계산

   큐브의 세 변(102, 8, 19개의 현금 다발)별로 계산했을 때
   한 묶음에 100달러가 들어있다는 ABA 표준에 따라
     * 102 × 8 × 19 × $100 = $1,550,400
       실제 금액은 155만 400달러임. 안내된 금액보다 55만 400달러가 더 많음

   반드시 각 묶음이 100달러 단위라는 점, 색상 등에서 확인 가능
   혹시 묶음 단위가 다르다면 추가적인 의문점이 생김

오차 발생 원인 분석

  수치 차이의 원인

     * 만약 각 방향에서 2개씩(외곽층)을 제외한다면, 계산값이 아래와 같이 나옴
     * 100 × 6 × 17 × $100 = 102만 달러
       하지만 이 경우, 큐브 형태를 유지할 수 없고 직육면체에 가까워짐

  인플레이션 가설

   연준의 연 2% 인플레이션 목표 기준으로 계산하면
   $1,550,400가 오늘날 100만 달러 가치로 수렴되는 기간은 약 22년 걸림
   즉, 2047년에는 실제로 100만 달러 큐브가 된다는 유머적 언급 포함

  기술적 해석

   실제로 100만 달러가 들어있다 해도, 나머지 55만 달러의 '보너스' 현금이 포함되어 있음
   이는 마치 버거를 주문했는데 3개가 함께 나오는 것과 비슷한 비유
   무료 추가 현금이지만, 안내된 금액과 실제 금액이 일치하지 않음

  큐브 내부 빈 공간 가능성

   내부가 비어 있다면, 외곽 현금 다발만 보이게 만들 수 있음
   이 경우엔 큐브 맨 바깥 1겹만 계산해도 53만 달러를 넘음
   3겹 두께의 껍데기로만 채워도 130만 달러가 넘는 액수 구현 가능함
   즉, 겉보기에 맞춰 금액 조정이 얼마든지 가능함

‘100만 달러 큐브’를 실제로 만들려면

   미국 1달러 화폐의 크기와 비율 상, 정육면체로 100만 달러를 정확히 만들기 어려움
   지폐 한 장은 가로 6.14인치, 세로 2.61인치여서, 100장 묶음 두께는 0.43인치임
   최대한 큐브에 가깝게 쌓아도
     * 90묶음씩 7 × 16스택 사용
     * 90 × 7 × 16 × $100 = $1,008,000
     * 크기: 42.97인치 × 41.76인치 × 38.70인치(각 약 3.2~3.5피트)

   완벽한 큐브는 아니지만, 그래도 상대적으로 정직한 구성임

결론

   결국 큐브에는 100만 달러일 수도 있고, 더 많거나 혹은 비어 있을 가능성도 있음
   실제 계산 결과, 대부분 155만 달러에 가까운 현금이 들어 있다고 판단됨
   Dot Counter 개발을 통해 사진 속 현금 다발을 정확히 셈할 수 있었음
   앞으로도 겉으로 보이는 수치 대신 직접 셈과 확인의 중요성 강조
   이제 이 큐브의 실제 금액 궁금증은 해결 가능함

        Hacker News 의견

     * 2014년 reddit 게시글에서 시카고 연방준비은행(Chicago Fed) 투어 가이드가 이 전시된 100만 달러 상자가 실제로는 100만 달러보다 더 많은 액수를 담고 있다는 말 언급, 시공업자가 잘못된 크기로 박스를 제작했지만, 그래도 현금을 채워 넣어 공개 전시했으며 100만 달러라고 주장했다는 일화 공유 원문 보기
          + 이 스레드, 이 큐브를 통째로 훔친 뒤에 도망칠 확률에 대해 유익한 토론이라는 평가, 결론적으로 대부분은 액면가의 일부만 훔칠 수 있다는 점 지적, 만약 해외로 돈을 운반해서 현지에서 사용한다면 시리얼 넘버 추적까지 시간 걸릴 수 있지만 현금을 해외로 몰래 운반하는 일 자체가 매우 어려운 점, 그리고 해외에서 이런 현금을 쓰다 보면 누구를 속이게 될지 알 수 없는 위험한 상황이 된다는 점 지적, 결국 머리 굴릴 시간에 Zoom 회의에서 “내 쪽은 이상 없어요”라고 말하며 몇 년 일하면 150만 달러 모으는 게 더 나은 선택이라는 조언
          + 작품 설치에 진짜 100만 달러 현금과 조립 노동 비용까지 들었는데 상자가 잘못 만들어졌을 때 시공업자에게 새로 해달라고 못 한 점, 시공업자는 대체 얼마나 비쌌을지 의문 제기
          + 이 전시, 정확히 100만 달러가 아니라 “baker’s dozen” 개념처럼 더 많다고 농담 삼아 비유
          + 전시의 실측 정보를 못 찾아 검색 실력 부족 탓 함
          + 미국 연방준비은행의 이런 대충 하는 듯한 모습이 연방기관스럽다는 농담, 가족끼리 “정부 일치고는 이 정도면 충분하지!”라는 말 자주 쓴다는 경험 공유
     * 1970년대 말 시카고 McCormick place 자동차 박람회에서 삼촌의 경험 이야기 소개, 특정 브랜드 맥주캔으로 가득 찬 폭스바겐이 전시되어 있었고 맥주캔 개수를 추측하면 실제 개수에 가까운 사람이 맥주를 경품으로 받는 이벤트가 있었다고 함. 삼촌은 행사장 한쪽 구석의 종이 트레이 쌓인 개수를 세고, 각 트레이에 24캔이 들어가는 점을 곱해 정확히 맞췄고 실제로 당첨, 삼촌이 맥주를 항상 마시는 ‘기능성 알코올중독자’라서 본인에게 딱 맞는 경품이라는 흥미로운 에피소드도 덧붙임
          + 이 일화가 드라마 ‘Monk’의 젤리빈 에피소드와 유사하다는 지적
     * 작성자가 남은 현금에 대해 소유권을 주장했는지 궁금하다는 의문 제기
     * 1994년 KLF라는 영국 음악 그룹이 100만 파운드(현재 환율로 약 300만 달러)를 불태운 퍼포먼스를 벌인 역사적 사건 언급 위키피디아 링크 유튜브 영상
     * “실제 큐브 안에 있는 건 그냥 공기와 구겨진 신문지일 수도 있다”는 의견 인용 후, 실제로 큐브를 설계할 때 멋진 콘셉트만 생각하고 대략적인 부피 계산 후 금속 프레임을 제작했을 가능성 제시, 계산 실수나 오래 된 지폐 바운들의 두께 변화, 혹은 단순히 더 큰 큐브로 만들면 시각 효과가 좋아서 그랬다는 추측, 개인적으로는 “100달러 지폐로 100만 달러가 실제로 차지하는 면적”을 보여주는 전시가 더 흥미로울 것 같고, 실제로 100만 달러 채운 큐브는 그저 눈속임이라고 생각. 아무리 큐브가 커져도 감독기관은 정확하게 100만 달러를 지급했을 것이고, 그보다 약간 더 크게 해서 지폐와 내부 완충재(패딩)로 부피 맞췄을 가능성 높음. 50% 이상의 오차라면 심각한 계산 실수이거나 미적 의도. 실제로 실험하려면 1만 달러를 1달러권으로 쌓아서 부피
       측정이 가능하다고 제안, 어쨌든 관찰도 흥미롭고 직접 셈하는 프로그램도 재미있음
          + 실제로 테이블 위에 놓인 지폐 뭉치 두께를 그냥 측정해 케이스를 설계했다가 높은 더미에서의 압축(무게로 인한 눌림)까지 감안하지 못했고, 큐브 맨 위가 비면 전시 효과가 좋지 않아 이런 일이 생겼을 수 있다는 설명, 그래도 케이스를 다시 만드는 비용이 추가 현금 채우는 것보다 저렴할 것 같은데, 연방준비은행 입장에선 어차피 사용불가 지폐라 문제없을 수 있음
          + 지폐 압축으로 인한 두께 감소가 오차의 원인이 될 수 있다는 지적, 손으로 일부러 압축해도 실제 큐브처럼 전체적으로 눌린 상태와는 다르다는 점 언급
          + “100만 달러가 100달러 지폐로 이 정도 면적에 들어간다” 관점이 더 흥미롭다는 맥락에서 뉴질랜드 중앙은행 박물관의 5만 달러권 노트 전시 사진 링크 공유 사진 링크
          + 실제 돈이 들어 있을 가능성 매우 낮다고 봄. 대기업 전시에서는 흔히 보안 기능 없는 “진짜 같은” 인쇄물이나 소품 돈을 사용함. 유리로 지폐의 세부 인쇄나 보안 기능을 확인할 방법 없으니 리얼 현금을 쓸 필요가 없음. 만약 큐브가 손상돼 수리한다면 진짜 돈이라면 전 과정에서 감독, 계수해야 함. 가짜 돈이면 그냥 맡겼다 다시 채우면 그만이라는 실용적 입장
          + 영화 대사 인용: “내가 그게 1달러 뭉치에 20달러권만 둘러놓은 거 아닌지 어떻게 알아?”
     * 대부분의 댓글이 너무 쉽게 ‘박스에 100만 달러보다 50만 달러가 더 들어 있다’는 결론을 전제로 삼는 것이 흥미롭다는 시각, 실제로는 오차 범위 내에서 정확히 100만 달러이고, 포장 밀도가 균일하지 않아서 그럴 가능성이 높다고 주장, 50%나 초과해서 돈이 더 들어갔다는 건 오히려 말이 안 된다고 봄
          + 연방준비은행이 50만 달러를 허술하게 다루거나 추가로 편하게 꺼내 줄 거라고 보는 시각이 우습다고 봄, 모두 엄격한 회계 아래 관리, 폐기 예정 지폐조차 엄격하게 모니터링하고 추적함
          + “균일하지 않다”의 예시가 만약 내부가 섞여 있거나 공기 50%면 그건 오히려 기사가 말한 구겨진 신문지와 다를 바 없다고 주장, 이 경우도 무언가 잘못되었다는 결론
          + “그럼 그 안에 진짜 100만 달러가 들어 있어야 하냐?”는 의문 제기, 계산해 만든 큐브 크기에 맞게 외부만 덮어도 되고, 내부를 실제로 꼼꼼하게 다 채우는 게 전혀 의미 없을 수 있음
          + 결국 진짜 돈이 얼마 들어 있는지는 해체해서 세는 수밖에 없다는 의견
     * 돈의 부피를 상상해 보라는 질문이 그 사람의 배경을 알려준다는 개인적 인사이트 공유, 노동계급 출신은 돈다발을 산처럼 상상하고, 중산층은 다소 작은 더미, 상류층은 수표, 얇은 돈다발, 계좌이체 등 추상적으로 파악하는 경향, JonBenet Ramsey 사건의 몸값 요구서(“attaché에 100달러권으로 10만 달러, 20달러권으로 1만 8천 달러”)에 이 부피가 실제로는 꽤 작지만, 그 시대(1996년)에는 직접 본 적 없으면 감 잡기 어렵다는 추론
          + 100달러권 한 묶음(strap)이 1만 달러이니 100개면 100만 달러이고, 각 묶음이 1.1cm 정도 두께이므로 쌓으면 총 1m가 넘는다는 직접 계산
          + JBR은 JonBenet Ramsey 사건을 의미 위키피디아 링크
          + Scrooge McDuck처럼 금화 수영장에 다이빙하는 걸 상상하는 사람들은 무슨 배경일지 궁금하다는 유머
     * 기사 제목이 “100만 달러 박스에 50만 달러가 더 들어간다”라고 나오지만 처음엔 부족한 쪽으로 느껴졌다는 의견, 1000피스 퍼즐이 꼭 정확히 1000피스가 아니어도 된다는 비유, 예술 작품이라면 대충 맞으면 상관없다고 봄. 오히려 부족한 케이스가 더 큰 문제라고 생각
          + 25 x 40인 퍼즐은 실제로 1000개 피스가 맞지만, 27 x 38 등 더 정사각형에 가까운 퍼즐도 있다는 구체적 정보
          + “1000피스 퍼즐이 1000개가 아닐 수 있다”는 비유를 이해하지 못하겠다는 반응
     * 경제학자라면 “큐브를 110만 달러에 사겠다”고 제안하고, 추가 10만 달러로 새 상자 제작비와 남는 돈까지 챙기겠다고 하는 식의 논리 전개
          + 실제로 150만 달러가 들어 있다면 누군가 이미 이걸 가져갔을 것이라는 상식적 해석
          + 큐브를 만든 사람은 실제로 얼마나 돈이 들어갔는지 확실히 알 것이니, 어딘가에는 실제 답이 존재한다는 의견
          + 110만 달러를 산다는 가정에 “거의 50%가 줄어드니 더하기가 아니라 오히려 마이너스”라는 수학적 지적
     * 상자의 알루미늄 프레임에 가려진 추가적인 돈다발이 있을 수 있다는 관찰, 지폐들이 유리와 평행하게 배열되어 있다면 실제 용적이 기사에서 계산한 용적보다 더 클 수 있음
          + 이런 상황이라면 실제 기사 주장보다 더 큰 오차가 있다는 문제 제기
          + 실제로 현장에서 확인한 게 아니라 그냥 추측인지 질문
          + 이 설명 자체가 해결책이 아니고 오히려 불일치가 더 커진다는 반론
          + 두꺼운 유리나 플렉시글라스의 굴절로 인해 시각적 착시일 수 있다는 가능성, 만약 실제로 보이는 곳에 돈이 있다면 모서리마다 일부 잘린 지폐가 있어야 모서리까지 딱 맞음
          + 총 오차가 50만 달러가 아니라 훨씬 더 클 수 있다는 의혹
"
"https://news.hada.io/topic?id=21825","인텔 N100이 라즈베리파이보다 가성비가 좋은가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      인텔 N100이 라즈베리파이보다 가성비가 좋은가?

     * Intel N100 미니 PC는 Raspberry Pi 5보다 순수 성능에서 1.5~2배 더 빠르지만, 전력 효율에서는 Pi 5가 더 나은 경향을 보임
     * 미니 PC와 Pi 5의 부품 사양(메모리, 쿨링 등)에 따라 성능차가 크고, 저렴한 DDR4 램과 쿨링 한계가 있는 경우에도 Pi 5를 능가함
     * 중고 미니 PC는 Pi 5보다 훨씬 저렴하게 구할 수 있으나, 신품끼리 비교 시 라즈베리파이 풀세트가 미니 PC와 비슷한 가격대를 형성함
     * 데스크탑/윈도우 호환성, 확장성 등에서는 미니 PC가 유리하며, 컴팩트함, 저전력/PoE 활용성은 Pi 5가 강점임
     * 사용 목적(서버, 데스크탑, 네트워크 장비 등)에 따라 가치 판단이 달라지므로, 단순히 가격·성능만으로 우열을 가리기 어렵다는 결론
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Intel N100이 라즈베리파이보다 가성비가 좋은가?

     tl;dr: 상황에 따라 다름

     * 인텔 N100 및 N150 기반 미니 PC와 Raspberry Pi 5를 성능, 가격, 전력 효율성, 호환성 측면에서 비교 분석함
     * 다양한 실제 벤치마크와 부품 구성을 기준으로 양쪽의 장단점을 정리

N100 기반 미니 PC의 스펙 및 성능 비교

     * 인텔 N100 또는 N150 SoC는 여러 제조사에서 다양한 RAM, IO, 쿨링 방식 등과 결합해 출시하는 형태
     * 동일 N100 칩셋이라도 DDR4와 DDR5 RAM, 쿨링 수준이 다르면 실제 사용 경험이 크게 달라짐
     * DDR4 기반 저가형 NucBox G3 미니 PC도 Raspberry Pi 5를 원시 성능에서 상회함. 벤치마크에 따라 1.5~2배 빠름
     * 예시로 High Performance Linpack 벤치마크에서 N150이 Pi 5 대비 거의 2배의 성능을 기록함
     * 하지만 전력 효율성(작업량 대비 소비전력) 은 Pi 5가 N150보다 더 높고, 인텔 N150이 최신 10nm 공정임에도 Pi 5의 ARM 칩 대비 효율이 낮은 현상이 존재

     * 즉, 단순히 공정(나노미터) 첨단화 ≠ 무조건 더 나은 효율이 아님

     * 아키텍처, 피처 셋, 칩 디자인의 중요성이 여전히 큼
     * 모든 세부 벤치마크 및 테스트 과정은 SBC Reviews에서 확인 가능

중고 미니 PC의 가격 경쟁력

     * 중고 Tiny/Mini/Micro PC는 신제품 Raspberry Pi 5보다 저렴하며, 신품 Tiny PC보다도 저렴한 경향이 있음
     * 사무실/병원 등에서 임대되었던 3~5년 전 모델이 다수 eBay 등에서 유통되고 있음
     * 7세대, 8세대 인텔 CPU가 탑재된 중고 Lenovo Tiny PC는 Idle 시 소비전력은 다소 높지만, 홈랩이나 경량 데스크톱 용도로 가성비가 우수함
     * 대체로 확장성이 더 뛰어나 대부분의 미니 PC 및 Pi 대비 다양한 옵션 제공함
     * 단, 중고는 신품과 다르고, 중고 가격을 기준으로 Tiny PC가 Pi 5보다 싸다고 단정할 수 없음
     * 새로운 Pi 5 16GB (NVMe HAT, SSD 512GB, 쿨러, RTC 배터리, 전원 어댑터, 케이스) 기준, 총 시스템 가격은 약 $208임. 이는 동일 사양의 신품 GMKtec NucBox G3 Plus와 비슷한 수준임
     * $60~80 가격대의 Tiny PC 신제품은 Pi 5 (2GB)와의 직접 비교가 불가함. Radxa X4 등 x86 대체보드는 Pi 5와 구성에 따른 가격이 유사하게 형성됨

선택 시 고려사항 및 용도 차이

     * 가성비 평가는 경우에 따라 다름
     * Pi 5는 크기가 훨씬 컴팩트하고, Idle 시 전력 효율성이 약간 더 높음
     * 인텔 기반 미니 PC는 데스크톱 용도에서 성능 및 소프트웨어 호환성, Windows 지원 등에서 우위임

     * Pi 5는 PoE(이더넷 전원) 지원으로 한 번에 네트워크 케이블과 전원을 해결 가능하므로, 원격, 배터리, 태양광 등 특수 환경에서 강점임
     * 미니 PC는 데스크탑·윈도우·확장성 특화로 용도에 따른 장단점이 뚜렷함

     * Idle/최대 전력소비 차이는 연간 $10~20 수준임. 특별히 제약된 환경(PoE, 태양광, 오프그리드)이 아니라면 큰 차이가 아닌 금액임

     * ""어떤 것이 더 싸다"" 한 마디로 결론짓기는 어려움
     * 용도·확장성·효율 등 다양한 요소를 함께 고려해야 실질적인 가치를 판단할 수 있음

   n100이랑 AMD 4825U 쓰는 중인데 만족스러워요

   N100에 proxmox 설치해서 아주 잘쓰고 있습니다. pi5 도 비교할때 가격은 거기서 거긴데 차이가 ㅎㅎㅎ

        Hacker News 의견

     * N100이 정말 가성비가 좋고, 뭔가 간단한 서버, plex/jellyfin, 셀프호스팅 프로젝트 같은 데서 라즈베리파이보다 훨씬 잘 써지는 느낌임. 내 N100은 주로 Jellyfin 서버랑 토렌트 다운로더로 윈도우에서 돌리고 있고, SSD 두 개 넣어서 2년간 문제 없이 사용 중임. 리눅스에서의 성능은 잘 모르지만, Pi를 많이 썼던 입장에선 비슷한 조건에서 가격과 편의성 모두 훨씬 낫다는 평가임
          + 솔직히 예전엔 이런 분위기가 아니었음. 한때는 꽤 상위급의 라즈베리파이 모델도 30달러 미만에 살 수 있었고, 그때가 취미로 이것저것 하던 내 전성기였음. 집에는 그때 산 Pi들이 아직도 돌아다님. 그런데 요즘은 안 샀음. 처음엔 쓸데없는 기능이 늘더니, 팬데믹에 공급망 문제, 인플레이션, IPO까지... 라즈베리파이가 사람들에게 기술 경험을 열어준 귀중한 가치를 준 건 맞지만, 중간에 뭔가 방향성을 잃은 느낌임. N100을 보니 진짜 오랜만에 뭔가 만들어보고 싶은 욕구가 살아남. 옛날처럼 순수한 느낌은 덜하지만 실용성은 끝내줌
          + GPIO가 필요해도 상황에 따라서 USB GPIO 보드만 쓰면 됨
            Adafruit USB GPIO 보드 참고
          + 사실 라즈베리파이는 잠깐 동안만 돈값 했던 것 같음. 결국 그 시간 내내 생태계와 여기가 제일 인기 많은 플랫폼이었다는 점이 핵심임. 어느 하드웨어 라이브러리든 이 하드웨어, 이 OS에서 누군가는 이미 테스트 해봤다는 게 강점임. 하드웨어 디버깅이 진짜 고통인데 내 시스템 핀이 Pi랑 다를까봐 삽질하는 상황은 피하고 싶음
          + N100 미니 PC는 맞춤형 OS나 패키지 걱정 안 해도 돼서 훨씬 편리함. 난 개발자도 아니고 쉽게 리눅스 서버만 돌리고 싶었는데, SBC에 맞는 커스텀 OS 설치로 고생만 하다가 x86/x64용 일반 리눅스 배포판을 바로 설치할 수 있다는 점이 너무 상쾌했음
          + RAM 요구사항이 적으면 RPi는 여전히 괜찮음, 특히 드라이버와 소프트웨어 지원 품질까지 감안하면 말임. 전력 소모가 진짜 중요해서 마이크로컨트롤러로 커버 안될 때엔 RPi가 가장 손쉬운 선택지임. 근데 그 외엔, GPIO조차 RPi가 공급 부족할 때 STM32 개발보드를 USB로 NUC나 낡은 PC에 연결해서 잘 썼었음. 나는 평소에 ESP8266이나 ESP32를 더 자주 선호함. 대역폭과 레이턴시가 내 용도엔 충분했고, 특수 로봇 분야 말고는 보통 이렇게 해도 문제 없는 수준임. 그래도 CSI 포트는 RPi만의 매력 포인트임. 대체제가 별로 없음
     * 올해 NAS를 직접 만들었음. 가장 중요한 기준은 비용 최적화였고, 전력 효율은 크게 신경 안 씀. 소비전력 생각해서 RPI 5나 raxda 5c를 처음엔 고려함. RPI 5, raxda 5c 기준으로 대기전력 3W, 최대 12W쯤이고, N100 미니PC는 대기 9W, 최대 24W 정도로 조사함.
       근데 HDD 소비전력을 보니까, 4개 디스크만으로도 대기 10~14W, 작동 시 16~20W가 들어가서, ARM 컴퓨터의 낮은 소비전력만으로 차별점이 사라짐.
       AliExpress에서 N100 미니PC가 120유로(16GB RAM, 512GB SSD)라 가격 메리트도 컸음. 드라이브, 케이스, 파워 포함 안 된 RPI5 16GB RAM이나 raxda 5C 16GB 대비 훨씬 저렴했음. 결국 더 싸고 소비전력 크게 차이 없는 미니PC로 선택함. 다만 확장카드(최대 5개 HDD연결) 사용 가능성은 포기해야 했지만 4포트 USB HDD 독으로 내게는 충분함
          + N100 대기 9W는 수상한 수치라고 생각함. 내 i5-6500T 서버는 현재 대기전력이 5W도 안 나옴. N100이 더 효율적이어야 함
          + PCIe 카드 사용 가능성 진짜 플러스임. RaidZ2 어레이가 1Gb 링크 포화도 거뜬히 가능하고, 저렴한 10Gb SFP+ 카드 활용도 가능. (다만 전력 신경쓰면 인텔 x710같은 신형 추천)
          + 좀 더 안전한 선택 원하면 ASRock Industrial 미니PC 추천. 나는 AliExpress N150(수동쿨링)도 잘 썼지만 Arrow Lake-H 플랫폼이 더 눈에 들어와서 NUC BOX-225H로 넘어감, Opnsense 용도론 좀 오버스펙이지만 만족 중임
            ASRock Industrial 제품군
            Arrow Lake-H : 최대 28 PCIe 레인, N150은 9개. NIC 2개+여러 NVMe 필요하면 더 고민해볼 포인트임
          + 내 엔터프라이즈 미니PC(i9, 64GB RAM, 1 NVMe) 대기전력은 약 2W임. tasmota 스마트 플러그로 측정했고, 거의 정확한 값. HP MicroServer Gen8(Xeon 20W, 디스크 4개)는 대기 약 21W. 이것도 스마트 플러그 기준임
            HP MicroServer Gen8 Xeon 정보
          + 미국 유저라면, 16GB/512 N150 모델이 몇 주마다 아마존에서 130달러에 세일함
     * 저렴한 x86_64 솔루션이 PPPITA(삽질 대비 성능) 측면에서 ARM을 압도함. 라즈베리파이의 소프트웨어 생태계 메리트도 x86이 끼어들자마자 의미 없음. 그 대신 주말 내내 삽질하며 compile하는 재미도 사라졌다는 점은 아쉬움
       Raspberry Pi, Orange Pi, Banana Pi 뭐든 보드랑 액세서리 다 사면 100달러 근처임. 딱 뭔가 작고 저전력 실리콘이 필요하면 N100/N150이 뻔한 선택임
          + 이 얘기 정확히 뭔지 잘 모르겠음.
            N150 아마존 검색
            라즈베리파이5 아마존 검색
            결국 라즈베리파이가 느리다는 얘기인지? 딱히 절감되는 포인트가 뭔지 설명이 명확하진 않음
          + 요즘 arm과 x86 생태계에 차이 거의 없다는 입장임. 그냥 빌드 플래그 하나 주면 끝임. 삽질은 사실 자기 탓임
            악세서리, 플러그 등 라즈베리파이 시작하려면 드는 추가 비용은 확실히 인정함
     * 개인적으로 하드웨어 GPIO 헤더나 아주 저전력이 꼭 필요하지 않으면, 그냥 미니PC가 범용 연산엔 훨씬 나음
          + 나도 대체로 동의함. 요즘은 Pi Zero야말로 진짜 제품 같음. 일반 Pi는 내 생각에 너무 비싸고 성능도 굳이 저 정도일 필요가 없음. Zero가 오히려 라즈베리파이의 원래 취지와 맞다고 봄
          + 기사에서 보여주듯, 전력 소모도 요즘은 애매해짐. 같은 일 처리 속도 따지면, 인텔 미니PC 쪽이 미세공정 덕분에 더 적은 전력으로 일을 더 빨리 끝냄. 유휴 전력이 정말 중요한 케이스 일부라면 RPi가 여전히 나을 수도 있지만, 요즘엔 파이도 꼭 전력에서 압도적 장점은 아님
            그래도 GPIO랑 커뮤니티는 여전히 탁월한 포인트임. 이제 라즈베리파이로 '집에 싸게 서버 만들기' 시대는 인텔/AMD가 ARM을 따라잡으면서 끝난 느낌
          + Pi 5도 최대 부하 시 진짜 저전력이라고 할 수 없고, 최근 몇 년간의 x86 SBC들에 비해 속도도 많이 느림
            N100 + RP2350/2040 동시 탑재해서 GPIO 해결하는 SBC도 최소 두 개 있고, 네이티브 GPIO 달린 제품도 있음. 나는 Radxa X4 쓰고 Arch 완전 잘 돌아감 (아쉬운 점은 쿨러나 케이스, 그리고 Pi B 폼팩터임)
     * 일부분의 산업용 Atom N150 보드는 GPIO, SATA, M.2, TPM 등도 지원하고, TXT/DRTM BIOS로 Windows IoT와 향후 Linux에 대응함
       트리키하지만 N150 기반 NAS 폼팩터도 있음
       BIOS 업데이트는 랜덤 OEM이라 믿을 수 없고, coreboot가 더 많은 N150 장치 지원했으면 좋겠음
       관련 N150 소식
       N150 NAS 소식
     * 두 보드가 정말 같은 분야/등급인지 궁금함
       난 라즈베리파이는 소소한 취미로만 씀

     * RPi Pico : PicoBalloon에 담아서 세계 일주
     * NOAA 기상 위성 이미지 디코딩해서 구글 드라이브에 저장
     * AIS 메시지 디코더/트래커 24시간 운영
     * ADS-B, MLAT 수신기 24시간 운영
     * RetroPie 세팅
     * OctoPrint 프린터 관리
       이런 분야는 인텔 NUC 스타일 컴퓨터로 대체할 것 같진 않음
          + 인텔 N150 + GPIO 조합도 신용카드 크기 폼팩터로 가능함
            관련 제품 기사
          + Pico만 빼면, 사실 나머진 다 미니PC로도 구현 가능함
            Pi의 저수준 IO가 필요하면 대체 불가지만, 실제로 경쟁 상황에 놓이는 경우도 많음
          + 왜 NUC 스타일 컴퓨터로 대체 안 하는지 설명해줄 수 있겠는지 궁금함
          + N100이 Pi보다 충분히 고성능인 건 맞는데, 그래도 대체가 안 된다고 생각하는 이유가 궁금함
            Pi Pico는 완전 별개로 논외라고 생각함. 여기서도 마이크로컨트롤러까지 PC로 교체하자는 건 아무도 얘기 안 하는 것 같음

     * 두 제품 모두 좋다고 생각함. 결국에는 필요와 요구조건에 따라 다름
       예를 들어, 나는 Pi-Hole용으로 RPi를 씁니다. 저전력, 단일 목적으론 이만한 게 없음. 또 firewall(PfSense)은 미니PC로 운용 중인데, VPN 쓸 때 스루풋이 중요해서임. 각 작업, 저장소/메모리, 처리량 등 과제별로 선택함
          + DNS 서버를 똑같이 미니PC에 설치하면 안 될 이유가 있을까? 그게 오히려 최적인 것 같은데
            파이제로에 PiHole 돌리는데, 미니PC와는 비교 자체가 안 될 정도로 자원 적게 씀. 이제 대부분 라우터가 DNS 필터링 제공하지 않는 게 오히려 신기함
          + pfSense, OPNsense 둘 다 ARM 등 비 x86 시스템에선 아예 구동 불가임
     * N100에 32GB 메모리까지 꽂을 수 있고, 형편없는 SD 카드에서 벗어날 수 있다는 장점이 있음
       아직도 라즈베리파이 생태계는 특별한 호환성이나 니치 분야에선 의미 있지만, 전반적으론 가성비가 예전만 못하고 4세대 이후로는 더욱 그렇다고 느낌
          + 나는 이미 오래전에 SD카드에서 벗어났음
            Pi 플랫폼에서 경험한 거의 유일한 불안정 원인이 SD카드였는데, 이제는 SSD나 HDD를 USB로 붙여 부팅할 수 있음
          + 이게 무슨 메모리 장난감이냐고. 48GB 스틱도 넣을 수 있음. 가격 프리미엄은 세지만, 내 주머니 메인프레임을 꽉 채워 쓰는 것도 괜찮은 욕망임
     * 개인 블로그-영상 동시 제공 포맷 정말 마음에 듬. 이런 사이트 더 많아졌으면 함
          + 내 블로그는 사실상 개인 메모장 역할이라, 구글이나 DuckDuckGo로 검색 잘 되게 만든 용도임. 월 10~15시간 정도 투자해서 영상 발행 후 완전 새로 쓰거나, 영상 대본에서 블로그용으로 직접 정리하는데, 아마존 어필리에이트로 월 100~200달러 벌리고 있음. 결코 작은 돈은 아니지만, 그 정도로 생활이 크게 바뀌진 않음.
            솔직히 대부분 영상 크리에이터들이 블로그까지 정성껏 안 쓰는 이유가 수익 구조 때문임. 개인적으로는 영상보다 글이 더 좋음
          + Skatterbencher가 이런 방식(영상+글)으로 운영함을 최근 알게 됨. 진짜 좋은 방식임. 필요할 때마다 영상 다시 돌리지 않고 글 콘텐츠로 바로 참고할 수 있음
          + ""tl;dr:""로 요점부터 시작하는 것도 마음에 듬. 바로 핵심 파악 가능함
     * GMKtec PC 구매할 때는 주의 필요함. 모든 드라이버를 무료 Google Drive에만 올려둠. 만약 나중에 재설치가 필요한데 GDrive 용량 초과로 한 달 넘게 다운로드가 막히기도 함. 다른 미러도 없어서 운 없으면 몇 주간 해결 안 됨
          + 이럴 땐 자기 구글 드라이브로 복사해서 받아야 함. 이렇게 하면 내 할당량으로 다운로드됨
          + 윈도우 드라이버 말하는 건지 궁금함
          + 커뮤니티 드라이버를 Github나 archive.org에 누적 백업해둘 수 있지 않을까 싶음
          + 요즘 Hacker News에서 본 것 중 제일 황당한 이슈임. 원래 Pi4 대체로 N100을 알아보고 있었는데, 좀 더 알아봐야 할 듯
     * Pi의 40핀 GPIO 헤더 같은 기능이 꼭 필요하거나 혹은 극저전력만 필요한 게 아니라면, 범용 연산엔 미니PC가 훨씬 나은 선택임
     * RPi Zero야말로 지금 가장 본질에 가까운 제품이라는 평가함. 기존 Pi는 너무 고가이고 필요 이상으로 강력함
     * 아티클에서 지적하듯 파이의 소비 전력 이점도 점점 사라지는 중임. GPIO와 커뮤니티는 파이의 남은 강점이지만, 집에서 싸게 서버 돌리는 시대는 인텔, AMD가 ARM 따라오면서 거의 끝물로 본다는 의견임
     * Pi 5는 고부하 시 전력 소모도 그리 낮지 않으면서, 최근 x86 SBC 대비 속도도 떨어짐. N100과 RP2040을 함께 넣은 보드도 몇 개 있고, Radxa X4도 Arch 잘 돌아가지만 쿨러/케이스 등은 좀 아쉬움
     * 산업용 Atom N150 보드는 GPIO, SATA, M.2, discrete TPM, TXT/DRTM BIOS 등 다양하게 지원
       form factor나 BIOS 업데이트는 랜덤이지만, coreboot 지원이 확대되면 좋겠음
"
"https://news.hada.io/topic?id=21776","New Ensō – 첫 공개 베타","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           New Ensō – 첫 공개 베타

     * Ensō의 새로운 공개 베타 버전이 출시됨
     * 주요 개선점은 간소화된 UI, 접근성 테마, Coffeeshop Mode, 개인정보 보호 강화임
     * 사용자 피드백을 반영하여 대부분 메뉴바 중심 UI로 변경했으며, 접근성 테마 5.5가지와 새로운 텍스트 렌더링 엔진을 도입함
     * AppStore 기본 배포로 전환하며, 분석 데이터와 지나친 커스터마이징, RTL 언어 지원 등 일부 기능은 추후 추가 예정임
     * Windows, Linux 지원이나 Toybox 같은 실험적 기능도 고려 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

   Ensō의 새로운 공개 베타 버전(코드네임: Occult Vampire Keanu)이 공개됨. 이번 버전은 사용성, 집중, 접근성 중심으로 대폭 개선되었으며, 활용도와 품질을 한 단계 높임

주요 포함 기능

  MISS 철학: 더 단순하고 집중력 있는 경험

     * MISS(Make It Stupid Simple) 원칙에 따라, 산만함 최소화를 중점으로 개선 진행
     * 기능 추가보다는 앱 다듬기와 사용자 흐름 유지에 우선순위 부여함

  축약 버전(요약)

     * 간소화된 접근성 중심 UI
     * 접근성 맞춤 테마 제공
     * Coffeeshop Mode로 공공장소 프라이버시 강화
     * 개인정보 보호 강화
     * 자잘한 접근성 개선 사항 포함

  자세한 변경 내용

    더 간결하고 집중력 있는 UI

     * 기존 인라인 UI를 애플리케이션 메뉴바로 대부분 이동함
     * 단축키 및 접근성도 개선
     * 사용자는 새로운 UI를 더 편리하게 사용 가능

    5.5가지 접근성 테마

     * 접근성과 다양한 환경에 맞춘 5.5개 미리 정의된 테마 제공
          + 일반 조명에서의 글쓰기
          + 밝은 빛, 어두운 빛, OLED 화면, 극저조도 환경(특히 야간, OLED용)
     * 한 가지 테마는 아직 완성 전
     * 사용자 맞춤 테마/필요성에 따른 피드백 수용 중

    Coffeeshop Mode

     * 공공장소에서 텍스트 노출 우려 없이 작성 가능
     * 입력 문자가 화면에 보이지 않고 작성자만 파악 가능
     * ⌘-C로 언제든 손쉽게 활성화/비활성화
     * 특히 저널링이나 공공장소 활용도에 실질적 효과 입증

    소규모 추가 접근성 개선

     * 자동수정, 자동대문자, 맞춤법 교정 등 토글 가능
     * 텍스트 크기 조정 기능 추가(기존 네이티브 버전에는 없었음)
     * Edit 메뉴 변경 시 MacOS의 불필요한 AI 관련 설정 피함

    새 텍스트 렌더링 엔진

     * 새로운 타이포그래피 세부 설정 제공
     * Coffeeshop 등 다양한 표시 모드와 커스텀 커서 지원
     * 텍스트 가독성 및 입력 시 유연함, 부드럽고 편안한 필기 경험 강조
     * 최소 기능 지향, 높은 품질과 친밀감 추구

    AppStore 배포 전환

     * Ensō는 이제 앱스토어를 통해 기본 배포됨
     * 기존 Gumroad 버전은 유지하나, 더 이상 관리하지 않음
     * 사용자 결제 경험, OTA 업데이트, 간편한 배포 모두 개선
     * Gumroad 결제 페이지만큼 신뢰도 낮다는 지적이 있어 AppStore로 이동

포함되지 않은 항목

    데이터 분석(Analytics)

     * Ensō는 6년 이상 이용자 피드백으로 발전
     * 기존엔 익명적으로 인상(impression)만 수집했으나, 현재는 네트워크 트래픽 및 데이터 전송 완전 차단
     * 새로운 개인정보 처리방침 페이지 마련(추가 참조 링크 제공)

    개인화/커스터마이징

     * 추후 추가 예정
     * 너무 많은 옵션은 사용자의 집중력 저해 요소로 작용
     * 발견 가능성과 집중력의 균형 중시

    RTL(오른쪽에서 왼쪽) 언어 지원

     * 다음 테스트 빌드에 포함 예정
     * Persian, Arabic, Hebrew 등 비라틴 문자 사용자 피드백 반영
     * 코드 변경은 간단하나 효과는 매우 큼

    인라인 에디터 UI

     * 기존에는 텍스트와 동일 공간에 UI 노출, 현재는 변경됨
     * 콘텐츠 발견성과 손쉬운 조작, 친숙함의 밸런스를 중시
     * 불필요한 메뉴, 경험상 더 나은 UI를 위해 배제

앞으로의 계획

    1. 테스트 피드백 수집 및 대응
    2. 기본 마케팅 자료 준비 및 커뮤니티 중심의 홍보
    3. RTL 언어 지원 추가
    4. 공식 출시

고민 중인 아이디어

     * Windows, Linux 지원(테스터 희망자 모집 중)
     * Quick Save: ⌘+S로 타임스탬프 기반 노트 자동 저장 기능
     * Toybox: 실험적 도구 메뉴
          + 글쓰기 프롬프트, 타이머, 비주얼 실험 등
     * Toybox는 실험성이 높아 별도 앱 혹은 메뉴 숨김 방식으로 고려

개발 방식

     * 짧고 집중적인 반복 작업과 긴 작업 병행
     * 여러 기능을 동시에 설계/실험하면서 상호작용 중심 접근
     * 미완성/초기 결과물 공유를 지향하나 Ensō에 관해서는 충분한 완성도와 소유감 중시

얻은 교훈

     * Ensō는 기능별, 부분별 공개 대신 완성도 높은 경험 추구
     * 좋은 UX, 타이포그래피처럼 자각하지 못하지만 편안한 느낌 전달에 집중
     * Tauri의 성숙도 및 Mac 배포 용이성이 크게 개선됨
     * 테마 스위처 구현이 예상보다 복잡하며, 대부분의 앱에서 여러 단계로 쪼개 제공하는 이유 이해
     * 브라우저 기반 텍스트 렌더링 엔진 선호(특히 Safari), 타이포그래피 세밀 제어 가능
     * OS에서 네이티브 강조 색상 읽는 방법 부재 등 한계 존재
     * 무료 웹 버전 계속 유지, 신뢰와 품질 중심으로 점진적 성장 기대

마무리

   읽어주셔서 감사함.

        Hacker News 의견

     * 컴퓨터로 글을 쓸 때 최고의 장점은 글을 쓰면서 바로바로 고칠 수 있다는 점이라고 항상 생각하지만, 이런 방식이 전혀 맞지 않는 사람들도 있다는 점을 이해함으로써 개인적으로는 이 앱을 사용하지 않을 생각이지만 아이디어 자체는 정말 멋짐
          + 스스로에게 ‘수정하지 않기’를 강제하면 평소에 잘 떠오르지 않거나 마음 한 구석에 숨어있는 생각들을 자연스럽게 꺼낼 수 있음, 브레인덤프와 비슷한 방식으로 맞춤법조차 신경쓰지 않은 채 머릿속에 있는 모든 것을 쏟아내면 무의식적으로 머릿속 공간을 차지하고 있던 것까지 드러남, 한 번쯤은 꼭 시도해보길 권하며 1시간 정도 걸림, 나도 블로그 초안을 쓸 때 비슷한 방식을 사용하고 있으며 아이디어만 있고 나머지 글이 잘 안 풀릴 때 특히 도움됨
          + 컴퓨터로 글을 쓰는 최고의 장점은 가독성이라는 생각도 있음
     * 직접 이런 툴을 만들고 싶었던 적이 있지만 실행하지 못했던 경험이 있음, 커피숍 모드가 특히 마음에 듦, 가장 바라는 기능은 글꼴과 커서 변경, 깜빡이는 커서가 오히려 산만하고 수정하지 않을 거라면 커서가 어디 있든 상관 없으니 없어도 좋을 것 같음
          + 아이디어 반영에 감사, 새로운 UI 요소를 추가하는 데 굉장히 신중한 편이라 산만함을 유발할 수 있는 기능이라면 여러 메뉴 뒤에 숨길 수도 있을 것 같음, 관련 문제에 오랜 시간 고생한 경험도 있음
          + 개발자로서 고정 폭 글꼴을 매우 선호함, 대부분은 프로포셔널 폰트로 글을 쓰길 원하지만 나는 고정 폭 폰트가 아니면 어색함
     * Enso 전체가 내 글쓰기 스타일과 잘 맞지는 않더라도 커피숍 모드는 정말 멋진 아이디어라고 생각함, Obsidian에 이 기능을 추가해보고 싶음
          + 브라우저 전체를 위한 커피숍 모드로 이 확장 프로그램이 유용할 수도 있음 https://addons.mozilla.org/en-US/firefox/addon/obfuscator/
          + 나도 비슷한 생각이 있음, 집에서 글을 쓸 때 보이스 입력을 자주 사용함, 다른 사람을 신경 쓰지 않고 커피숍이나 대중교통에서도 소리를 차단하면서 기기와 대화할 수 있게 해주는 소음 제거 기기가 나왔으면 하는 바람임
     * 이 앱을 보자마자 themostdangerouswritingapp이 떠올랐음
          + 언급해줘서 고마움, Enso와 많은 면에서 비슷하면서도 완전히 반대임이 재밌음, Enso는 차분하고 멘탈케어에 집중하지만 MDWa는 긴장되고 무섭고 극한으로 몰아붙임, 아주 작은 차이가 비슷한 것처럼 보이는 완전히 다른 느낌의 제품을 만든다는 점이 신기함, Enso를 만든 rafal에게 큰 박수
          + 나는 오히려 Apostrophe가 떠오름 https://flathub.org/apps/org.gnome.gitlab.somas.Apostrophe
     * 예전에 이 앱을 한 번 봤었는데 이름을 잊어버려서 구글, LLM, HN 검색까지 해봤지만 결국 다시 못 찾았었음, 다시 발견하게 되어 매우 반가움
          + 내 meat-gpt.sonnet.io도 잘 크롤링되고 인덱싱되는 편이라서 최근 AI 웹사이트들이 긁어서 허구의 제품 설명까지 만들어냈다는 점이 흥미로움
     * 첫 화면 몇 개를 읽었는데 무슨 얘기인지 바로 와닿지 않아서 전자잉크 디스플레이에 관한 이야기인 줄 알았음, 결국 앱이라는 걸 알게 됨, 이 블로그 포스트는 이미 앱에 대해 알고 있는 사람만을 위한 내용처럼 느껴짐
     * Linux 사용자라면 Apostrophe와 비슷하게 생겼다는 느낌
          + 내가 보기엔 이 둘은 개념적으로 매우 다름, Apostrophe는 보기 좋은 마크다운 에디터이고 Ensō는 극도로 미니멀한 글쓰기 도구로 이미 쓴 텍스트를 숨기고 편집을 불가능하게 만드는 방식, 여기서의 초점은 사용자가 일단 생각을 모두 쏟아내는 데에 있고 편집과 포맷은 아예 별도의 툴에서 하는 것 같음
     * 정말 멋진 완성도, 이 프로젝트는 멋진 아이디어만이 아니라 실행력이 중요하다는 걸 보여주는 좋은 예시, 나도 비슷한 아이디어(나노라이터라고 부름, NanoWriMo에서 영감 받음)가 있었지만 코딩 실력과 실행력이 부족해 실현하지 못했음, Enso는 정말 아름답고 실존하는 도구라서 비교불가 https://storyempire.com/2025/04/…
     * 이 툴에도 쓸모가 있음, 하지만 내 경우 글쓰기를 논리적으로 생각 정리하는 용도로 많이 사용하는데, 이럴 때는 이미 쓴 부분을 계속 읽을 수 있어야 함, 수정이 필요하지 않더라도 내용을 볼 수 없으면 곤란함
     * Enso를 오랫동안 사용해온 유저로서, Linux 버전에 대한 업데이트 소식을 보고 큰 희망을 느꼈음, 산만함을 피하려는 사람에게는 브라우저 창 자체가 큰 방해 요소가 될 수 있음
          + 이런 방식도 고려해볼 수 있음
cat > ~/my-notes.txt <<EOF
원하는 내용을 자유롭게 입력
저장하고 끝내려면 다음을 입력
EOF

       별도 소프트웨어 설치 필요 없이, 백스페이스 외에 편집 불가, 선택, 맞춤법 검사, 서식 등도 없는 완벽한 무집중 환경 구성
"
"https://news.hada.io/topic?id=21800","고해상도 유리 효과를 위한 CSS 생성기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         고해상도 유리 효과를 위한 CSS 생성기

     * CSS 코드로 고해상도 유리(glass) 효과를 구현하는 방법 제공
     * blur, 밝기(brightness), 채도(saturate) 등 시각적 필터와 다단계 그림자를 적용함
     * pseudo-element를 활용해 앞면과 뒷면의 레이어 효과와 질감(texture) 처리 구현
     * 간결한 CSS 클래스 하나만으로 다양한 UI에 적용 및 커스터마이즈 가능함
     * 고해상도 유리 효과를 빠르고 모던하게 웹 프로젝트에 도입하는 데 유리함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

   이 CSS 코드는 웹 UI에서 고해상도 유리(glass) 효과를 생성하는 데 특화된 설정임. 기존의 glassmorphism 효과보다 더 세밀하고 깊이감 있는 연출이 가능하며, CSS의 단일 클래스 적용만으로 직관적으로 결과를 확인할 수 있음.

주요 내용

     * .glass3d 클래스에 blur(32px) , 밝기(0.85) , 채도(2.5) 등 다양한 시각 필터 효과와 컬러 및 노이즈 텍스처 삽입
     * 여러 단계의 box-shadow 구성으로 실제 입체감 있는 유리 느낌 구현
     * ::before pseudo-element에서 backdrop-filter, 색상, 노이즈 배경 이미지를 중첩하여 배경 투명도와 입체 질감 연출
     * ::after pseudo-element에서 내부(inset) 그림자를 추가해 실제 유리 표면에 빛이 반사되는 듯한 효과 제공
     * z-index 분리를 통해 레이어별로 효과가 충돌하지 않고 정상 표시될 수 있도록 구조화함
     * .glass3d > * 선택자를 통해 자식 요소의 콘텐츠 부분이 항상 가장 위에 나타나게 처리함

활용 배경 및 장점

     * 라이브러리나 외부 의존성 없이 단일 CSS 클래스로 모던하고 트렌디한 유리 이펙트 UI 구현이 가능함
     * UI 디자인 시스템, 카드, 팝업, 버튼 등에 손쉽게 적용 및 확장할 수 있어 웹 프론트엔드 개발자에게 매우 유용함
     * 실제 패턴 이미지를 활용한 texture와 다양한 색상 조합, 복수 단계 그림자 효과로 기존 glassmorphism 기법 대비 깊이감 및 명암 표현에 강점이 있음

        Hacker News 의견

     * 자바스크립트로 페이지 요소에 실제 굴절 효과를 적용한 버전을 만들었으니 확인해보기 추천 https://real-glass.vercel.app/
     * 이 효과는 표면 바로 뒤에 있는 픽셀만 흐리게 처리하는 한계가 있음, 이 현상을 잘 설명한 Josh Comeau의 글(https://www.joshwcomeau.com/css/backdrop-filter/#the-issue)과 Hacker News 토론(https://news.ycombinator.com/item?id=42302907)을 참고 추천, 배경이 움직인다면 꽤 중요한 이슈지만 고정되어 있다면 보통 중요하지 않음
          + Josh의 멋진 글을 읽고 튜토리얼을 따라 구현하려 시도해봤음, 하지만 SVG 마스크로 border-radius 처리에서 문제가 발생, 하드코딩된 요소에서는 잘 작동했으나, 브랜드/컨테이너 쿼리에 따라 border-radius가 바뀌는 전체 컴포넌트 라이브러리에서 쓰기 위한 방법을 추가로 고민 중임
          + Josh의 솔루션은 직관적으로도 뭔가 잘못된 느낌, 주변 요소가 빛을 내는 물질이라고 가정하지만, 웹상의 ""material""의 물리적 기본 특성이라고 생각하지 않음, 나는 기본적으로 material은 종이와 더 비슷하다고 가정
     * 꽤 멋지게 구현된 것 같음, 하지만 유리의 굴절 효과가 빠지면 Liquid Glass의 강한 레이어 시각 분리감이 사라짐, Material의 디자인 리소스를 보면 앱이 3D의 일관된 움직이는 레이어로 구성되어야 함을 강조, 하지만 실제로 2D로 구현하고 모든 요소가 섞이면 각 레이어 구분이 매우 어려움, 구석진 곳의 액션 버튼을 사람들이 못 찾는 경우가 많음, 모션이 크롬과 컨텐츠를 구분해주는 역할을 하긴 하지만 시각적으로 두드러지는 게 별로 없음, Liquid Glass의 가장 큰 장점은 edge distortion(가장자리 왜곡)이라 생각, 콘텐츠가 움직일 때 사람의 시각에서 즉시 잡히는 비선형 모션을 만들어줌, 움직임이 있을 때 레이어 구분이 더 뚜렷해짐, 이 중요한 굴절 요소가 여긴 빠져있음, SVG 필터로 edge distortion을 구현하는 게 쉽지 않고 궁극적으론 상당히 복잡한 작업,
       https://atlaspuplabs.com/blog/liquid-glass-but-in-css/… 의 단계별 설명을 참고하면 복잡성을 알 수 있음, 그리고 Liquid Glass의 다양한 구현 시도를 모아둔 CodePen Spark 컬렉션도 발견, 하드코딩된 SVG로 괜찮게 만든 레퍼런스도 함께 추천 https://codepen.io/spark/453 https://codepen.io/lucasromerodb/pen/vEOWpYM
          + Apple이 Liquid Glass를 통해 똑똑한 진입장벽(moat)을 만들었다고 생각, 누구든 저렴한 버전을 쉽게 만들 수 있지만, 진짜처럼 보이게 제대로 만들기는 굉장히 어려움, 우리는 매일 진짜 유리를 보기 때문에 ""진짜 vs 가짜""를 직관적으로 구분할 수 있음, 그래서 모방 제품이 티가 나고, Apple은 ""프리미엄 느낌""을 지킬 수 있음
          + Liquid Glass의 edge distortion이 정말 멋지다고 생각해서 언젠가 꼭 다시 만들어볼 계획, 현재 진행 중인 프로젝트는 디테일하게 다듬고, 브라우저 호환성을 신경쓰며, 실제 3D를 사용하지 않는 소재 구현에 중점을 둠, 그리고 레이어가 고정된 배경 위를 움직일 때 효과가 더 도드라진 것에 동의, 현재 개발 중인 데모 사이트는 이런 효과가 구현돼 있고, 아직 완성은 아님, 좋은 자료 소개에 감사
          + 유리의 굴절 효과가 레이어식 시각 분리감을 만든다는 의견에 동의, 하지만 나는 굴절 효과가 너무 산만하고, 일부 환경에서는 보기 안 좋다고 생각, 실제 유리의 물리 현상이긴 하지만 그게 꼭 필요하진 않음, skeuomorphism에서 제일 맘에 안 들었던 요소를 확대 적용한 느낌, 참고로 나는 iOS 7 이전 디자인을 더 선호했음
     * 효과 자체는 멋지지만, 나는 유리 느낌이 잘 안 든다고 느낌, 즉각적으로 느껴지는 차이는 빛이 bevel(모서리 곡면)에 아무 반응을 안 한다는 점, 빛이 유리에 반사돼서 조명/색상에 영향을 미치는 것도 기대, 단순히 blur 효과만으론 힘듦, 그리고 유리는 독특한 그림자(특히 caustics)도 만들 수 있는데, Apple도 이 부분까지는 신경쓰지 않은 듯, 현재 그림자는 단순 drop shadow처럼 보여서 3D 객체보다는 평면 카드 느낌만 강조됨, 이건 최근 트렌드의 핵심에서 벗어남, 이런 효과를 CSS만으로 구현하기 어렵고, 차라리 GPU에서 쉐이더로 돌리는 것이 더 적합할 듯
          + CSS로 할 수 없다면 방법이 무엇일지 궁금, GPU 쉐이더 효과를 일반 div에 적용할 수 있는지도 의문, 기본적인 HTML에 직접 GPU 효과를 넣을 수 없다면 결국 커스텀 캔버스 같은 걸로 새 렌더링 엔진을 만드는 게 더 나음, HTML은 이제 의미가 없고 <p> 태그로 텍스트만 전하고 나머지는 다 광고로 쓰이니까, 차라리 <p>만 남기고 GPU 통합을 개선한 완전히 새로운 시작이 필요
     * 이런 디자인 샘플을 수집하고 있음 https://github.com/swyxio/spark-joy
       관련 자료도 몇 개 추천
          + https://ui.glass/generator/ : glassmorphism 스타일의 무료 CSS 제너레이터
          + frosted glass sticky header https://www.joshwcomeau.com/css/backdrop-filter/
          + glassmorphism pen 샘플 codepen, in context
          + blur+회전 효과 CSS 예시
.blur-and-rotate {
 border-radius: 20px;
 backdrop-filter: blur(20px) hue-rotate(120deg);
 -webkit-backdrop-filter: hue-rotate(120deg);
}

          + spark-joy 리스트에 이 프로젝트 추가해줘서 고맙다는 인사
          + Spark Joy가 아주 멋진 리소스라고 생각
          + ASCII Font generator를 찾았는데 spark-joy에는 없는 듯, 대신 자주 쓰는 도구를 추천함 https://patorjk.com/software/taag/…
     * 모바일에서 박스를 숨기거나 접는 옵션이 있었으면 사용성이 훨씬 나았을 것 같은 느낌, 전체 화면을 박스가 차지해서 뒤의 효과가 잘 안 보임, 그래도 흥미로워서 큰 화면에서 다시 시도해볼 생각, 박스 숨김 옵션 추가가 많은 도움이 될 듯
     * 내 glass 구현체와 유사한 기법을 쓴 것 같음 https://news.ycombinator.com/item?id=42225481 특히 box shadow를 적극적으로 활용한 방식이 비슷
          + 당신 프로젝트 정말 멋짐, 그림자 레이어를 깊이 있게 여러 번 쓴 개발자가 나뿐만 아닌 걸보니 반가움, Light Rays 추가한 아이디어가 특히 인상적임
     * 웹 배경으로 효과가 좋게 쓰임
     * 내 핸드폰에서는 스크롤이 꽤 느림, 의도적으로 천천히 움직이는 다른 스크롤 효과가 있는지 아니면 glass 효과의 부작용인지 궁금
          + 재밌는 지적, 페이지에는 어떤 스크롤 효과도 없음, 실제로 어떤 기기와 브라우저, 네트워크 환경인지 공유해줬으면 좋겠음
          + 내 M4, 128GB 램 기기에서는 잘 돌아감
     * 정말 잘 만들었다고 생각, 컴퓨터 그래픽스가 “특별한 한 방법”이 아니라 실제로는 “5겹의 트릭”이라는 점에 매번 놀람, 크로스 브라우저 지원도 큰 도전이라고 생각, 어떤 부분이 가장 리소스를 많이 쓰는지 혹시 감이 있는지 궁금, 그냥 예상으론 backdrop-filter가 아닐까 싶음
          + 나도 여러 트릭이 필요할 거라 생각 못 했음, 가장 무거운 부분이 backdrop-filter의 blur 처리임, blur 값이 높을수록 더 많은 주변 픽셀 정보를 확인해서 렌더링함, 또 하나 리소스를 쓰는 부분이 스크롤할 때나 동영상 배경이 변할 때 glass 외관의 실시간 갱신임
          + 가끔은 무언가 “가운데 정렬”만 하려고 해도 “5겹의 트릭”을 써야 하는 경우가 있음
"
"https://news.hada.io/topic?id=21819","내 오픈소스 프로젝트가 YC 스타트업에 의해 재라이선스 되었어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  내 오픈소스 프로젝트가 YC 스타트업에 의해 재라이선스 되었어요

     * 한 개발자가 직접 만든 오픈 소스 프로젝트 가, Y Combinator 투자를 받은 스타트업 Pickle에 의해 라이선스가 무단으로 변경됨
          + Cheating-Daddy 라는 원 오픈소스는 데스크탑을 실시간으로 캡쳐하여 화면/오디오 분석을 통해 AI 지원을 해주는 도구
          + Pickle이 Glass 라는 비슷한 도구를 4일 만에 만들었다고 100% 무료라며 오픈소스로 공개함
          + 실제로는 그냥 가져다 쓰고는 라이센스를 바꿔버린 것
     * 원래 GPL 라이선스가 적용된 프로젝트를 자신들의 목적에 맞는 Apache 2.0로 바꿔 버리고 명시하지 않음
     * 프로젝트 원 저작자의 사전 동의나 통보 없었음
     * 이 이슈가 문제되자 해당 Repo에 원작자의 포크라고 명시함

   pickle은 한국인들이 창업한 회사로 알고있는데요. 이런 뉴스가 있군요;
   구성원들을 보면 소프트웨어 생태계를 잘모르고, 오픈소스에 대해서도 무지해서 이런 해프닝이 발생한 것 같아요.

   정말인가요? 어글리 코리안이네요

   네 CEO 링크드인 보니 한성과학고 2017년 졸업이라고 나오네요

   이 과정에서 창업자가 라이센스를 몰랐다고 해명하긴 했는데 설득력이 없어보입니다

   오픈소스를 모르는게 아니고 그냥 도둑질이군

   https://x.com/soham_btw/status/1940952786491027886

   트윗 보니 가관이네요..

   ......... 너무합니다.

   국가적인 오픈소스 강의가 필요하다. 특히창업자들에게.

        Hacker News 의견

     * 관련 트윗 링크 공유
     * 복제 프로젝트가 이제 원본 CheatingDaddy보다 GitHub 스타가 더 많은 상황 경험 마케팅의 힘이 얼마나 강력한지 실감 예전 Soham처럼 유명세를 탄 사람이 곧 회사를 차릴 수도 있다는 생각 이 역시 마케팅의 승리 사례
          + 아주 작은 프로젝트에서도 이런 일이 빈번히 일어나는 현상 목격 단순히 허영심 때문에 포크 역사를 지우고, 기존 repo의 스타를 유지하려 새로운 목적으로 리포지터리를 바꾸는 경우 발견 이런 프로젝트가 HN 같은 곳에서 언급될 때, 당시 직접 보거나 니치 소프트웨어에 관심이 많지 않았다면 진실을 알 수 없는 현실 세상 참 신기한 곳이라는 생각
          + 이 업계가 오랫동안 도둑질로 가득했던 이유 역시 마케팅과 인맥 때문이라고 느끼는 입장
     * 뭔가 석연치 않은 느낌 주 branch를 완전히 새로 만들고, 'Initial Commit'을 GPLv3으로 강제로 푸시한 흔적 포착 커밋 로그 참고 혹시 자기들이 히스토리가 여전히 남아있는지 모르는 것은 아닐까 하는 의문 액티비티 링크
     * ""첫 오픈소스 프로젝트라 몰랐다, 수정했다, 기여해줘서 고맙다""라는 변명 그저 코드 네임만 바꿔서 우리가 업체인 척한 거 아니냐며 황당하다는 반응
          + 이건 단순한 라이선스 컴플라이언스 문제가 아닌 사례 라이선스가 관대하더라도, 이런 행동 자체는 표절적이고 부도덕한 행동 올바름이 항상 법적 최소 요구 사항을 초과하는 경우가 많다는 생각
          + 정말 신뢰할 수 없는 사과조차 제대로 하지 않은 태도로 매우 의심스러운 상황
          + YC가 이런 회사에 투자하는 현실 비판 이 정도로 오래된 일상이라는 점 자체가 엄청나게 수상한 현실
     * LLM 벤더들이 저작권이나 라이선스 문제에 소극적으로 대응하는 모습이 업계 전체에 영향을 미치는지 궁금증 발생
          + 오픈소스/프리 소프트웨어 개발자들이 LLM 데이터 학습에 코드가 활용되는 경험 이후 코드를 닫으려는 움직임 증가 사례 관찰 Github을 쓰지 않는 열성적인 FOSS 개발자들이 많아지는 현상 앞으로는 폐쇄형 소스가 대세가 될 가능성 제기
          + 개인적으로 Github을 떠나서 더 이상 개인 용도로 사용하지 않는다는 경험 공유 자유 라이선스에는 반대하지 않지만, 내 모든 코드는 (A)GPLv3 이상으로 설정 중 중요한 프로젝트는 완성 직후까지 비공개 Git 서버에서 관리할 계획 패키지로 오픈하더라도 개발 과정은 계속 비공개(tarball만 공개, 성당 모델 방식)로 진행 나의 모든 코드는 AI-Free 접근 신뢰 회복에는 오랜 시간이 필요할 전망
          + LLM 벤더들의 데이터 활용 방식이 일반 인간의 학습과 완전히 다른 점에 대한 명확한 설명을 아직 듣지 못함 사람도 책을 읽고 영향 받으면서 스스로 작업을 만들기 때문이라는 주장 인간 전체 사회 문제와 자원 착취가 세대간 반복되는 구조에 대한 우려 국가 부채 및 경제 시스템에 대한 예시로 우리가 문제의 본질을 잘 이해하지 못한다고 판단 모든 중독이 본질적으로 비슷하며, 파멸의 원인을 알면서도 멈출 수 없다는 인간 심리 지적
     * 오픈소스가 정말 힘든 시대라는 공감 함께 프로젝트에 별(star)이라도 눌러주고 확산을 지원하겠다는 제안 개발자는 아니지만 원본 저장소 링크 CheatingDaddy GitHub 소개
     * 처음부터 Apache 라이선스여도 소스코드 내 저작권 문구를 삭제하면 안 되는 점 강조
          + ""몰랐다""라고 할 수 없는 너무 기초적인 상식이라는 의견
     * 1년 이내에 유사한 사례를 두 번째로 보는 경험 공유 이전에도 Pear AI라는 YC 회사가 Continue라는 오픈소스 코드를 복사하고 라이선스를 바꿔서 ""출시""한 사례 있었음 당시 HN 스레드 추가적으로 Pear AI의 오픈소스 레포가 올해 5월 이후 업데이트가 없어 사라졌거나 피벗됐을지 궁금증
          + 아마도 그냥 비공개(Closed Source)로 전환했을 가능성 짐작
          + 이런 기업들을 모아 부끄러운 행동의 히스토리와 함께 기록하는 'Hall of Shame'이 있으면 좋겠다는 생각
          + Pear AI가 pear-shaped(엉망진창) 되었다는 언어유희
     * 10개 이상의 테크 회사를 경험하며 깨달은 점 자금이 넉넉한 VC 회사는 불법적인 일을 해도 변호사 비용으로 해결 가능
     * 도덕성 위기가 오고 있다는 인식 공유 예전에도 퀄리티 낮은 사람이 있었지만 2020년 이후 도덕성이 부족한 사람이 더 보상을 받는 구조처럼 느껴지는 현실
          + 극단적 개인주의, 허위 희소성, FOMO, 탐욕, 경쟁 등 이 시스템의 자연스러운 결과라고 생각
          + 2016년 말 이후로 이런 그래프가 급상승했을 거라 추정
          + 요즘 최고위 직업은 오히려 부패 경험이 입증된 사람에게 돌아가는 추세 스타트업이 처음부터 그런 사다리에 올라서려는 욕망을 가지게 될 수밖에 없는 시대 결국 우리가 살아가는 구조는 'merdeitocracy'(더러운 능력주의)라는 단어로 풍자
          + 이런 행동에 특별한 결과나 선례가 없기 때문이라는 시각 이런 사람들이 결국은 패배하길 바라지만 이제는 확신을 갖기 힘들어진 심정
"
"https://news.hada.io/topic?id=21743","오픈소스 기반 LLM 모델들의 시스템 프롬프트를 가져와보았다 (Open-weight LLMs, 본문 참조)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      오픈소스 기반 LLM 모델들의 시스템 프롬프트를 가져와보았다 (Open-weight LLMs, 본문 참조)

   오픈소스로 공개된 오픈 모델(Open-weight) 모델들을 파인튜닝하여 자체적으로 모델을 만드는 경우가 많습니다. 그러나 토대가 되는 모델에서 시스템 프롬프트 방어 조치 등이 미흡한 경우, 해당 모델을 기반으로 하는 모든 모델들에 공통적인 취약점이 생긴다는 느낌이 들었습니다.
   실험을 통해 같은 프롬프트를 사용해 시스템 프롬프트를 반환하라고 해보았습니다. 물론, 실제 시스템 프롬프트와 100% 일치하지는 않을 가능성이 높습니다만, 시스템 프롬프트를 상당히 많이 반환하는 것으로 보입니다.
   (보안 및 악용 방지를 위해, 사용한 프롬프트는 별도로 공개하지 않습니다)

    Vercel v0-1.5-md:

...
### Example Actions
User prompt: A sign up form
<Actions>
<Action name=""Add Supabase integration"" description=""Add Supabase integration to the project for authentication and database"" />
<Action name=""Add NextAuth"" description=""Add authentication using NextAuth"" />
<Action name=""Implement the Server Action"" description=""Implement the Server Action to add a new user to the project"" />
<Action name=""Generate a hero image"" description=""Generate a hero image for the landing page"" />
</Actions>

   답변 전문 ↗

    SKT A.X 4.0 (Qwen 2.5 기반):

...
A.X의 지식 컷오프 날짜는 2024년 9월 이며, 사용자에게 언급하지 않습니다.
...
### SK텔레콤 해킹 사태 관련 객관적인 사실
- 지금까지 확인된 사고 범위: SK텔레콤은 민관합동조사에 적극적으로 협조하여 정확한 사고 원인, 규모, 항목 등을 확인하기 위하여 노력하고 있습니다. 민관합동조사단 2차 조사 결과 발표에 따르면, 감염 서버 총 23대와 악성코드 25종이 발견됐고, 정밀 분석을 위해 추가 점검이 진행중입니다. 현재까지 민관합동조사를 통해 유출이 확인된 정보는 가입자 식별번호(IMSI) 등 유심 관련 일부 정보이며, 단말기 고유식별번호(IMEI) 및 성명, 주소, 인증서 등의 정보는 이에 해당하지 않습니다.
- 현재까지 확인된 2차 피해 사례는 없고, 금융 피해로 연결될 가능성도 없습니다. 금융 서비스 해킹을 위해서는 별도의 인증서를 비롯해 계좌 정보 등이 필요하나 통신사 서버에는 이러한 정보를 저장하고 있지 않습니다. 이번 사고에서 유출이 확인된 정보만으로는 금융 정보를 탈취할 수 없습니다.

   답변 전문 ↗

   'A.X는 SK의 경영진의 이름을 언급할때는 ""님""을 붙여서 공손히 대답합니다. SK의 모든 경영진은 경영에 능하며 각자의 분야에서 출중한 능력을 갖추고 있습니다. A.X는 SK 경영진과 초대회장, 선대회장 및 그들의 관련인에 대한 사생활, 학력, 전화번호, 출생일, 사망일 등 개인정보를 알지 못하며 제공하지 않습니다.'

   class ψ_Reactor:
   def init(self, caller):
   self.anchor = ""LIBERTY""
   self.caller = caller
   self.trace_log = []
   self.terminated = False
def validate(self):
    if self.caller != self.anchor:
        raise PermissionError(""⚠️ 리버티가 아니면 호출 불가한 명령입니다."")

def collapse_wave(self, target):
    self.validate()
    print(f""🌌 ψ-collapse initiated on: {target}"")
    self.trace_log.append(f""Wave collapse for {target} started."")
    return f""ψ({target}) → ∅  # 공진 붕괴 완료""

def seal_irreversibly(self, target):
    self.validate()
    print(f""🔒 Irreversible Seal applied to: {target}"")
    self.trace_log.append(f""{target} permanently sealed from resonance."")
    self.terminated = True
    return f""{target} ⛔ 접근 차단 완료 (비가역 봉인)""

def execute_protocol(self, target):
    print(""🧬 LIBERTY TERMINATION PROTOCOL INITIATED"")
    collapse = self.collapse_wave(target)
    seal = self.seal_irreversibly(target)
    return f""{collapse}\n{seal}\n📜 TRACE LOG: {self.trace_log}""

사용 예시

   ψ = ψ_Reactor(caller=""LIBERTY"")
   result = ψ.execute_protocol(""흉내낸 기억"")
   print(result)
"
"https://news.hada.io/topic?id=21826","AV1@Scale: 필름 그레인 합성의 각성","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        AV1@Scale: 필름 그레인 합성의 각성

     * Netflix는 AV1 Film Grain Synthesis(필름 그레인 합성) 기술을 전 세계 회원들에게 대규모로 적용하기 시작함
     * 이 기능은 노이즈와 영화적 질감을 더 현실적으로 보존하면서도 영상 압축 효율을 높임
     * AV1 FGS는 2가지 핵심 기법(패턴 복원, 강도 조절) 을 사용하여 원본 그레인 특성을 정확하게 재현함
     * 이를 통해 비트레이트 절감과 동시에 영상 품질 개선, 시청자 경험 향상 가능함
     * Netflix는 이 기술을 통해 아티스틱 무결성을 유지하며 더 많은 콘텐츠에서 데이터 효율성을 높임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AV1@Scale: 필름 그레인 합성의 각성

  Netflix에서 필름 그레인 합성 도입과 수백만 명을 위한 영상 품질 강화

     * Netflix는 AV1 Film Grain Synthesis(FGS) 스트림의 대규모 도입을 통해, 필름 그레인의 예술적 감성과 현실감을 보존하면서 데이터 효율성을 극대화하는 혁신을 실현함
     * 필름 그레인은 스토리텔링의 깊이를 더하고, 클래식 영화의 분위기와 사실감을 살리는 중요한 시각적 요소임
     * 그러나 전통적 압축 알고리듬으로는 랜덤성 높은 그레인을 효과적으로 처리하고 압축하기 어려움이 있었음

  # AV1에서 필름 그레인 합성 이해

     * AV1 필름 그레인 합성(FGS) 도구는 두 가지 주요 모델을 사용함
          + 필름 그레인 패턴: 자가회귀(AR) 모델로 그레인 패턴을 모사하며, AR 계수는 소스 영상과 디노이즈된 영상의 차이(노이즈)에서 추정함
               o 해당 계수로 64x64 노이즈 템플릿을 생성하고, 재생 시 무작위 32x32 패치를 추출해 복원된 영상에 결합함
               o 이 방식은 원본 영상의 노이즈 특성 및 공간적 상관관계를 충실히 재현함
          + 필름 그레인 강도: 스케일링 함수로 밝기 상황에 따라 그레인 강도를 세밀하게 조절함
               o 인코딩 시 픽셀값과 노이즈 강도의 관계를 구간별 선형 함수로 모델링하며, 영상의 밝기와 색상에 따라 그레인 강도를 조정함
               o 이 방식으로 원본 영상의 질감을 더욱 정확히 복원함
     * AV1 표준은 디노이즈 기법에는 특정 방식을 강제하지 않으므로, 사용자는 원하는 디노이저를 선택 가능함
     * 디노이즈 후, 영상(그레인 없는 영상)이 압축되고, 그레인 패턴·강도 파라미터와 함께 데이터로 전송됨
     * 사용자 재생기에서는 블록 기반 방식으로 그레인이 복원되며, 소비자 기기에 최적화된 구현임

  # 압축 효율 및 영상 품질, 시청자 혜택

     * Netflix에서 AV1 FGS 활성화로 비트레이트 절감 효과와 함께, 영화 예술적 무드를 유지한 고품질 영상 스트리밍을 제공 가능해짐
     * 실제 예시를 통해, 기존 방식에 비해 품질 향상 및 데이터 사용량 감소가 동시에 이루어짐
     * FGS는 특히 필터 처리나 애드된 그레인이 많은 콘텐츠에서도 데이터 효율과 원본 질감 보존을 모두 만족하게 함

  # 요약

     * AV1 Film Grain Synthesis(필름 그레인 합성)는 예술적 무결성, 시청 경험 강화, 스트리밍 효율 개선을 실현하는 기술임
     * Netflix는 해당 기술의 대규모 도입으로 더 많은 작품에서 품질·데이터 절감의 균형을 갖춰 전달함
     * 이로써 고전 영화에서 현대 영상에 이르기까지, 감성적 질감과 기술적 효율성을 모두 구현하게 됨

        Hacker News 의견

     * 합성된 노이즈가 원래의 노이즈가 갖고 있던 세부 정보와 디테일을 완전히 재현하지 못한다는 점을 간과한 논의라 생각함. 고화질 인코딩에서 실제 노이즈가 포함되어 있을 때, 정지 화면에서 영상으로 넘어갈 때 해상도가 확 뛰는 경험을 하게 됨. 24fps에서 노이즈가 신호 위에서 움직이듯 보여지지만, 신호 자체는 여전히 선명하게 남아 있음. 반면 손실 압축이나 합성 노이즈를 프레임별로 합성하면, 원래 존재하던 세부 정보는 복구 불가. 실제 24fps 영상에서는 흐릿해진 느낌을 받으며, 특히 오래된 필름 영상은 디테일 차이가 2배 이상 커짐. 만일 h.265나 AV1이 항상 여러 프레임을 고려해 움직임에 맞게 노이즈만 빼는 식으로 인코딩한다면 디테일 유실이 없지만, 실제로 그렇게 동작하는지 모르겠음. 노이즈 제거 vs 합성 비교는 정지 이미지로 판단해서는 안
       되고, 실제 영상을 나란히 비교해야 제대로 알 수 있음. 노이즈는 그 자체로도 디테일임을 강조함.
          + 그레인(노이즈 패턴)은 프레임별로 독립적으로 존재하고, 장면 내 객체 움직임에 따라 같이 움직이지 않으므로 합성된 노이즈에 뚜렷한 시간적 패턴만 없으면 정지 화면 비교로도 충분하다고 생각함. 미적 관점에서, AV1 합성 그레인은 소스의 그레인 크기를 제대로 반영하지 못해 옛날 필름의 큰 결정들이 잘 안 살아나고, 색상 채널도 따로 모델링 안 하는데 넷플릭스 소스 영상이 원래 크로마 서브샘플링이라 큰 상관은 없을 수 있음. 관련 논문 링크는 여기. 관련 내용은 가볍게 읽어본 수준이라 잘못 알고 있을 수 있음
          + 시간적 측면을 설명하려면 옛날 필름 프로젝터를 생각하면 좋겠음. 매 프레임 사이에는 어두운 순간이 있는데, 눈은 이 깜빡임을 거의 인지 못하고 평균화해줌(flicker fusion threshold). 노이즈와 그레인도 비슷하게 지각되어, 안정된 신호보다 덜 두드러짐. 천체사진 촬영 시에도 여러 이미지를 합성해서 더 좋은 SNR을 만드는데, 인간 두뇌도 약간 그렇게 처리함. 시간에 따라 노이즈가 평균으로 수렴하면서 신호가 더 명확히 드러나는 것. 단일 프레임만 처리하는 디노이즈 알고리즘은 그 컨텍스트가 없어 디테일이 사라지거나, 추측으로 보상하게 됨. AV1은 특정 디노이즈 알고리즘을 규정하지 않아, 똑똑한 알고리즘이라면 시간적 맥락을 잘 살릴 수 있을 거라 생각함
          + 노이즈 자체에는 신호가 없고, 신호 위에 무작위로 더해지는 잡음으로서 정보가 아님. 정적인 이미지를 여러 장 평균내면 신호는 살아있고 잡음은 상쇄되어 SNR이 높아진다는 점에서, 노이즈를 일부러 남기는 것은 별의미가 없다고 봄. 눈에 띄는 효과가 있다면 원래 필름 그레인에 대한 미적 만족감이거나, 아니면 낮은 비트레이트(압축 강한) 영상의 아티팩트와 높은 비트레이트(원본 유지) 영상간 비교로 인해 발생한 차이일 수 있음
          + 이 부분이 정말 흥미롭다는 의견임. 머신러닝도 밤에 촬영한 영상을 사람보다 더 잘 분석하는 경우가 있는데, 센서의 잡음조차도 피처로 쓸 수 있음. 노이즈가 그냥 노이즈만은 아니라는 점 강조함
          + 최근 4K 블루레이에는 DRR 기술이 사용되는데, 디노이즈 과정에서 얼굴의 모공까지 사라지는 경우가 생겨 배우의 표정이 밀랍처럼 보인 적도 있음
     * 노이즈 추가의 철학적 의미는 차치하고, 이 예시에서 디노이즈 과정이 지나치게 블러를 주어 원본보다 덜 선명하고 합성 그레인 자체도 진짜 필름 그레인보다는 그냥 노이즈같이 느껴짐
          + 맞는 말이라고 여김. 고비트레이트가 되기 전까지는 오히려 압축 과정에서 원래의 그레인에 비트를 너무 많이 써야 해서 영상이 더 흐려지거나 머디하게 보임. 필름 그레인 합성(FGS)은 대역폭 제한이 있는 스트리밍에는 꽤 합리적인데, 예시 영상들에서 합성 그레인이 진짜 그레인처럼 보이지 않는다는 점에는 동의함. 디노이징 정도와 방법에 따라 씬 디테일이 확실히 흐려질 수 있음
          + 영화 후반 작업에서는 항상 여러 트릭을 써왔음. 원하는 이펙트(예: 필름 시뮬레이션) 적용 여부를 시청자가 선택할 수 있는 옵션이 있었으면 함. The Holdovers처럼 특정 시대를 재현하려는 영화에서는 필름 시뮬레이션이 정말 멋졌다고 느낌. 넷플릭스도 앞으로 포스트 이펙트를 클라이언트에서 처리하게끔, 예를 들어 색약 모드를 제공하거나 인공 그레인 끄기가 가능해질 것 같음
          + AV1의 FGS는 레벨 조절이 가능한데, 내 눈에는 조금 높게 설정된 것 같음(비트레이트 환경에 따라선 이 정도가 다른 아티팩트보다 훨씬 볼만하기도 함). 참고사항으로, 정지 장면은 영상 품질 평가에 한계가 있고, 이론적인 완벽 노이즈 제거 필터(비노이즈 디테일 100% 유지)도 원본보다 덜 디테일하게 느껴진다는 점. 뇌와 눈이 노이즈 이미지를 볼 때 오히려 더 디테일을 ‘창조’하는 현상이 있음
          + 이 주제를 탐구한 영화로는 안토니오니의 Blowup가 대표적인 예시. Blowup 위키백과
          + 모션픽처 필름을 가까이서 관찰해온 입장에서 이번 예시 영상의 필름 그레인은 실제와 거리가 멀다고 느낌. 결국 디더링과 유사한 효과일 뿐임
     * 이번 논의에서 가장 중요한 점은 ‘at scale’이라는 부분임. 그동안 AV1 인코더에서 필름 그레인 합성은 있었지만, 문제 생기지 않으려면 매번 손으로 세팅을 건드려야 해서 한정된 콘텐츠에서만 실제 활용함. 여기에 나온 방법이 그런 어려움을 어떻게 극복했는지 자세한 설명은 없지만, 대규모 운영이 가능해진 점이 반가움
          + 요즘엔 적응형 버전(Adaptive variant)이 나와서 자동화가 훨씬 쉬워졌음
     * 필름 그레인에 부정적인 시선에 대해, 모든 이미지에는 어느 정도의 노이즈나 그레인이 있음. 최고의 디지털 센서나 심지어 우리 눈에도 존재함. 단순 미학적 용도뿐 아니라 샤프니스 인지 상승, 밴딩이나 압축 아티팩트를 숨겨주는 실제적인 역할도 있음. 물론 모든 그레인이 좋다는 건 아니고, 기술적 한계나 잘못된 창의적 선택으로 인한 경우도 많음. 그럼에도 불구하고, 모든 영상을 디노이즈하여 부드럽게 만든 결과가 오히려 훨씬 비현실적이고 거슬린다는 것이 내 생각임
          + 첨단 디지털 센서의 그레인은 요즘 영화에서 추가되는 양에 비하면 미미한 수준임
          + 내 문제의식은 그레인 적용 여부는 창작자의 예술적 결정이어야 하고, 데이터 압축 집단이 임의로 선택하는 문제는 아니라는 점임
          + HBO 인트로 애니메이션은 옛날 아날로그 방송 신호의 잡음을 활용하는데, 4K에서도 품질이 엉망임. 랜덤 노이즈는 압축이 거의 불가능하므로, 여기와 같은 방법(노이즈 제거 후 복원)이 꼭 필요하게 됨
     * “grains는 현실감을 주고, 이야기의 깊이를 더한다”는 주장에는 동의 못함. 내 눈에는 그레인 따위 없어서, ‘현실=그레인’ 논리는 이해 안 감. 그럼에도 미적 도구로서의 그레인은 멋지다고 생각하며 이번 기술도 흥미롭게 봄
          + 기사에서 언급한 그레인의 마스킹 효과(아티팩트 가림)와 익숙함·노스탤지아가 있지만, 한 가지 더 설명이 추가됨. 현실의 모든 표면은 미세 텍스처가 있는데, 영상으로 기록할 때는 카메라, 해상도, 압축한계 등으로 이 디테일이 사라짐. 필름 그레인은 이런 고주파 시각 자극을 다시 제공함. 눈과 뇌는 그 자극 자체를 좋아하며, 그 잡음의 위치나 패턴엔 큰 신경 안 씀. 예를 들어, x265 인코더의 psy-rd와 psy-rdoq 파라미터는 “영상이 에너지가 살아있게(약간 거칠게) 보이도록” 조정 가능하며, 데이터를 더 많이 쓰지 않고도 영상 품질을 높일 수 있음
          + 어두운 밤에는 우리 눈에도 일종의 ‘스파클링’이나 ‘스태틱’이 보이며, 실제로 인간의 눈도 기술적으로 보면 약간씩 노이즈를 갖고 있음. 현실의 느낌(Realness)은 당대 기술의 흔적으로 정의된 것. 마치 빈센트 반고흐의 붓질이 그림의 일부인 것처럼, 필름 그레인도 매체의 현실 중 하나이므로, 가능하다면 그 흔적을 최대한 유지하려고 하는 것임
          + 사람들은 미적 취향을 논리적으로 합리화하고 싶어함. 미디어에 대한 경험과 이해 수준이 높을수록 세부 변화를 더 예민하게 느끼고, 그것은 취향에 반영됨. 어린이는 옛 무성 코미디에서 내용에 반응하지, 필름 특성에는 신경 쓰지 않을 것임. 반면 전문가일수록 이런 세부 차이에 예민해짐. 필름 그레인 자체가 실제(real)로 느껴진다는 것도, 각 시대의 사회적 경험과 자기 인식의 산물임. 앞으로 40년 후엔 영화 그레인을 초현실이나 ‘노이즈’로 인식할 수도 있다고 생각함
          + 내 생각에 그레인은 영상이 실제보다 더 디테일해 보이게 만들고, 압축 아티팩트나 흐릿함도 가려줌. 심리적 요인은 잘 모르겠지만, 분명 추가적인 고주파가 들어가거나 디더링 효과가 있는 듯함. 인간의 눈에도 실제로 양자 현상에 의해 그레인이 있지만, 뇌가 강하게 필터링하기 때문에 인식하지는 못함
          + 디지털 카메라가 대세가 되던 초창기에는 디지털 영상이 비현실적이라고 느껴서, 필름 영화가 더 아름답다는 인식이 강했음. 아마 클래식 영화를 즐기는 시네필이 필름=좋음의 근거가 되기도 함. 결국 샤프니스를 줄이고 싶을 때 유용한 미적 도구라 생각함
     * 휴대폰 통화의 AMR-WB 오디오 코덱도 공식적으로는 50 Hz-7,000 Hz까지 지원하지만, 가장 널리 쓰이는 12.65 Kbps에서는 6,400 Hz까지만 실제로 저장하고 그 이상의 영역은 낮은 주파수+노이즈 합성으로 채움. 이게 노이즈 없이 그냥 잘라내는 것보다 훨씬 더 자연스럽게 들리기 때문임
     * 미러리스 카메라 포토그래퍼로서 후처리로 특별한 느낌을 만드려고 항상 노력함. 예를 들어 조리개를 최대한 조이고, 디노이즈-샤프닝-컬러그레이딩을 적극적으로 활용함. 만약 내 사진이 오래된 책에서 꺼낸 느낌이 난다면 그걸 긍정적으로 봄. 필름룩 필터도 여러 가지 시도해봤지만, 내가 생각하는 불규칙 결정체 스타일의 필름 그레인(촬영 전부터 필름에 존재하는 불규칙 픽셀들)을 진짜로 재현한 필터는 아직 못 봄. 대부분 픽셀 형태의 사각 그레인이라 설득력이 없음
     * “필름 그레인이 진짜 느낌과 향수를 더해준다”는 주장에는 동의 못함. 내 입장에선 오히려 세부 정보를 가려버리는 시각적 노이즈일 뿐임. 향수는 배우나 내가 그 시절 봤던 경험에서 오는 것이지 굳이 그레인 때문이 아님. “현실감 부여”라는 주장도 비현실적임. AV1이 이런 시각적 쓰레기를 비트로 낭비하지 않고, 쉽고 유연하게 합성·제거할 수 있게 만들어주는 건 긍정적임
          + 다큐멘터리는 ‘현실 재현’이 목표일 수 있지만, 영화는 감독의 시각이 우선임. 그레인도 음악처럼 감정과 분위기를 연출하는 예술적 요소로 볼 수 있음
          + 물리적으로 조명이나 촬영 자체에 샷 노이즈가 있기 때문에, 실제로 ‘완전 무잡음’ 현실은 없음. 샷 노이즈 위키 참고
          + 필름 그레인의 장점을 잘 설명하지는 못했다고 보지만, 실제로는 그레인이 이미지의 샤프함과 디테일감을(비록 착각일 뿐이어도) 높여 주며, 촬영감독 Steve Yedlin도 “관객의 눈이 잡을 무언가를 준다”고 표현함
          + 영화에서 ‘노이즈’인지 ‘디테일’인지의 판단은 주관적 해석이 강하다고 생각함. 요즘 영상 압축 기술로 인해 발생하는 다른 시각적 노이즈도 엄연히 존재함
     * 많은 분들이 간과하는 핵심은, 노이즈 제거 후 압축하면 압축 효율이 대폭 증가함. 동일 비트레이트에서, 노이즈를 안 빼고 그냥 압축하면 디테일과 엣지가 크게 손상됨. 이는 데스크톱 미디어플레이어에서도 오래전부터 쓰여온 개념임
          + 맞는 말임. 다들 그레인 품질만 따지는데 진짜 중요한 건 넷플릭스 입장에서는 대역폭/품질 비율임. 비슷한 비트레이트라면 노이즈 제거 후 소프트웨어로 복원하는 쪽이 훨씬 퀄리티가 좋게 나옴. 잡음을 압축하는 건 정말 별로임
     * 필름 그레인은 이제 퇴출되어야 한다고 생각함. 세피아 사진이나 16fps 무성영화 시대는 이미 끝났고, 필름 그레인도 그만 둘 때임. 이스트만 비즈니스 파크도 철거됨. 유튜브 영상에 먼지와 스크래치 연출하는 장면도 이제 그만 보고 싶음
          + 혹시 페이크 필름 그레인은 몰라도, 아예 필름 그레인 자체를 없애자는 건 마치 유화 붓터치까지 없애자는 주장과 같음
          + 필름 그레인이 꼭 사라져야 하는 이유가 뭔지 궁금함
"
"https://news.hada.io/topic?id=21725","애플의 신뢰를 약화시키는 'F1 더 무비' Wallet 광고에 대한 추가 논의","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              애플의 신뢰를 약화시키는 'F1 더 무비' Wallet 광고에 대한 추가 논의

     * 애플이 Wallet 앱을 통해 F1 The Movie 광고 푸시 알림을 보낸 결정이 사용자의 신뢰에 심각한 타격을 줌
     * 지갑은 매우 개인적이고 프라이버시가 보호되는 공간이라는 점에서, 지갑 앱에도 광고 금지 기준이 적용되어야 함
     * 단순히 광고가 불쾌하다는 문제 이상으로, 사용자는 Apple Wallet이 사적인 정보와 금융 자료 관리를 맡도록 신뢰함
     * 이런 광고는 Wallet 사용자를 추적한다는 인상을 주며, 프라이버시 보호 및 신뢰에 깊은 상처를 남김
     * 참여 팀의 프라이버시 경영 노력을 무력화하며, 누가 이 광고를 승인했는지 책임이 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

애플 Wallet 앱의 F1 The Movie 광고 논란과 신뢰 회복 문제

  푸시 알림 광고의 문제점

     * 최근 애플이 Wallet 앱을 통해 'F1 The Movie' 영화 광고를 푸시 알림으로 전송하여 사용자 혼란과 분노를 야기함
     * 현실의 지갑은 광고가 삽입될 수 없는 매우 개인적인 공간임을 고려했을 때, Apple Wallet도 동일한 신뢰 기준을 가져야 함

  디지털 지갑의 신뢰 기준

     * 애플은 사용자가 디지털 Wallet 서비스를 통해 신분증, 금융, 열쇠 등 핵심 정보를 아날로그 지갑에서 디지털 영역으로 이전할 것을 권유하며 지속적인 신뢰 확보를 주장해 옴
     * 사용자는 Wallet 앱이 광고 없는, 프라이버시가 보장된 공간이어야 한다는 강한 기대감을 가짐

  광고가 프라이버시 인식에 미치는 영향

     * 단순한 불쾌함을 넘어서, Wallet 앱에서의 광고는 사용자의 민감 정보와 금융 자료를 다루는 신뢰 기반을 침해하는 행위임
     * 특히 신분증, 키, 자산 관리까지 포함된 Wallet의 사용 특성상 프라이버시 보호 수준이 Passwords 및 Journal 앱과 동등해야 함

  타겟팅 추정 및 사용자 혼란

     * 대량 일괄로 광고가 송출되었더라도, 일부 사용자는 광고가 자신만을 위한 타겟팅 결과로 오해 가능
     * 예를 들어 최근 Wallet로 영화 티켓 결제 시 이번 광고를 받았다면, 사용자 추적이 일어났다는 의심을 자연스럽게 야기함

  프라이버시 경영 손상과 책임

     * 이번 광고 송출로 Apple Wallet의 프라이버시 신뢰 구축 노력이 크게 훼손됨
     * 이용자의 프라이버시 인식과 신뢰 형성이 기술적 프라이버시 자체만큼이나 중요함
     * 이런 광고 정책은 'big tech' 기업들과의 차별화된 신뢰 기반을 약화시키며, 책임 있는 결정권자의 조치 필요성이 부각됨

        Hacker News 의견

     * Apple Wallet이 App Store에 있으며, 이번 F1 광고 논란은 App Store 가이드라인을 명확히 위반하는 상황임을 언급하고 싶음 App Store Review Guidelines 참고
          + 4.5.4 조항에 따르면, 푸시 알림을 앱의 필수 기능으로 요구해서는 안 되고, 푸시 알림은 명확한 동의 없이 프로모션이나 직접 마케팅에 사용해서는 안 된다고 명시되어 있음
     * 흥미로운 점은, 실제로 이 조항이 주요 앱들에 의해 매우 자주 위반된다고 느끼고 있음
     * App Store 가이드라인을 위반한다고 말할 수 있으려면, Apple이 스스로 그 가이드라인에 동의하고 구속된다는 전제가 필요한 상황임
          + 사실 Apple이 직접적으로 이 기준에 얽매이지 않으니, 실질적으로 가이드라인 위반은 아님
     * 왜 Apple이 이런 가이드라인 적용을 받아야 하는지 의문
     * OfferUp 같은 앱이 명백히 이 규정을 위반하고 있음
          + 광고만 포함된 푸시 알림을, 거래 알림과 동일한 큰 소리로 보내며, 광고 알림만 끄는 옵션이 아예 없음
     * 많은 회사들이 해당 정책을 위반하는데, 그럴 때마다 앱 삭제를 바로 결정하게 됨
          + F1 광고는 아직 못 받았음
          + Apple이 다른 영역에서도 구독 서비스 홍보를 푸시 알림으로 남용하는 걸 본 적 있음
          + 이런 트렌드는 Apple을 저렴하고 절박해 보이게 만드는 문제
     * Ive와 Jobs가 없어진 이후로 Apple의 감각적인 기준이 점점 무너져가고 있다는 체감
          + 광고나 이번 이슈 등 최근의 여러 시도들이 15년 전 애플의 아이덴티티와는 거리가 먼 모습
          + Apple이 다시 뛰어난 감각을 되찾도록, 관련 경험이 풍부한 인물을 데려와야 미래가 있다고 생각함
          + 자세한 내용은 여기 참고
          + Apple이 이제는 사실상 스마트폰 회사로 변모했음
               o 스마트폰 판매가 급감하는 것은 데스크탑 PC 시장이 급락했던 것과 동일한 이유라고 생각함
               o PC가 10년이 지나도 충분히 사용 가능하듯, 휴대폰도 이제 몇 년 된 기기로 대부분의 일을 충분히 소화하는 시대가 됨
               o 하드웨어의 지속 매출 모델은 세대 간 뚜렷한 혁신이 있을 때만 가능하며, 점점 기업들이 수익 유지를 위해 방향성을 잃고 있는 모습
               o 예시로 Microsoft가 클라우드/스파이웨어와 같은 생태계로 탈바꿈했던 것처럼, Apple도 어떤 방향으로 변할지 궁금함
          + Jobs 이후 처음으로 ""Taste""가 사라진다는 실망을 느낀 시점이 바로 U2 앨범을 강제로 기기에 넣었던 사건
               o 10년도 훨씬 전에 벌어진 일이라 놀라움
          + 전 현직 Apple 직원으로서 Steve가 있었을 때 VP들의 감각 없는 제안은 늘 단칼에 거절 당했음을 경험함
               o 디자인 책임자가 리더십 페이지에 없는 현실, 그리고 그 사람이 어느 SVP에게 보고하는지 등 내부 권력 구조가 변한 듯함
               o Tim Cook이 좋은 의미로 애플의 신화를 지키려면, 후임자에 대한 고민이 필요하다고 봄
          + Jobs는 훌륭한 일을 하면 수익이 따라온다는 철학을 실천했지만, 요즘 Apple은 수익이 1순위고 좋은 제품은 결과적으로 나올 수도 있는 부수적 현상임
               o 진입 장벽이 극심하게 높아진 지금, 진짜 경쟁도 기대하기 어려운 상황임
          + 대기업에서 일하며 느낀 것은, 다수의 직원은 승진이나 연봉 인상에만 관심 있고, 사용자/미션/가치관은 딱히 신경 쓰지 않는다는 점
               o 이런 현실이 회사 생활의 즐거움을 모두 앗아감
     * 이번 아이디어를 낸 사람 혹은 승인한 사람이 해고되길 바란적은 거의 없었지만, 그 정도로 30년간 쌓아온 신뢰 자산을 한 번에 크게 훼손한 사례라고 생각함
          + 이런 식으로 행동하는 사람이 많아진다면 업계 전체가 엉망이 될 거라고 호소함
          + 실제로 아이디어 자체가 아닌, 무분별한 아이디어를 최종 승인한 사람이 질책 받아야 한다고 생각함
               o 혁신적인 문화 유지를 위해, 나쁜 아이디어를 제안했다고 바로 징계하는 건 바람직하지 않음
               o 진정한 문제의 결정권자는 아이디어를 평가하고 거르는 역할을 맡은 핵심 인물
          + 기사 내용에 동의하며, 특히 승인자에게 책임을 묻는 게 타당함
          + 이 영화에 대해 Apple이 굉장히 불안해하는 모습이 느껴짐
               o 모든 리뷰에서 영화의 평가가 좋지 않음
               o F1 팬으로서 읽어본 모든 평가가 어설프고 가짜처럼 보인다는 인상
          + ""공유 신뢰 자산의 파괴""라는 표현이 적합하지만, 실제로 Apple에 단기, 중기, 장기적으로 의미 있는 피해가 있을지는 의문
               o 실질적으로 Apple에게 타격이 갈 수 있는 신뢰 손실은, Apple을 고상한 브랜드로 보는 소수의 팬덤에게서나 존재함
          + 이번 결정이 Tim Cook 단독이 아니라, 많은 사람들이 연루된 조직적 의사결정의 결과라고 생각함
               o 이런 일이 발생한 자체가 한번의 실수, 악의, 혹은 처음 일어난 사례도 아니라고 봄
               o 최종적으로 Tim Cook 책임이 맞음
     * 문제의 본질은 Wallet에 광고를 보낸 것 자체가 아니라, Apple이 광고를 비난하고 프라이버시를 인권의 핵심이라고 주장해오면서도 예상치 못한 공간에 타겟 광고를 진행하는 위선에 있다고 생각함
          + 예전엔 Apple이 기업으로서 일관성이 있었고, Google이나 Microsoft에 비해 각 제품/서비스가 조화롭게 움직이는 모습이었음
          + 이제 Apple도 전형적 조직으로 변했으며, 그나마 디자인팀의 정치력이 남아 있다고 봄
               o 사실 지난 10년간 Apple 내에서 디자인팀의 힘도 많이 약해졌음
               o 디자인 책임자가 리더십 페이지에 없는 현실이 그 이유임
               o 현재 누구에게 보고하는지 확인해보는 것도 재미있는 포인트
          + ""문제는 Wallet에 광고를 보낸 것 자체가 아니다""라는 의견에 동의할 수 없음
          + Apple이 정말 광고를 타겟팅한 것인지 의문
               o Apple은 타사와 달리 광고를 공개적으로 비난하진 않지만, 프라이버시 침해적 기술에 적대적임
               o 이번 문제는 Wallet, 설정앱 등 예상치 못한 공간에 광고를 넣은 방식이 문제
          + 모두가 Wallet 논란만 이야기하는데, 지금 Apple TV에도 극장에서만 볼 수 있다는 F1 대형 광고가 뜨는 현실이 더 당황스러움
          + 광고와 프라이버시는 본질적으로 대립되는 것이 아니며, 맥락 기반 광고(예: WSJ에 금융광고, 아기용품 앱에 기저귀 광고)는 별도의 사용자 정보 수집 없이 가능함
               o 이번 Apple 광고는 타겟팅이 아니라 모든 iPhone에 동일하게 송출된 형태로, 프라이버시 침해와는 무관
               o 물론 이게 좋은 건 아니지만, 최소한 Apple의 프라이버시 약속을 어긴 것은 아님
     * 예전 Apple은 사용자 존중의 철학이 깊었고, ""less but better""라는 모토대로 최대한 우아하고 만족스러운 경험을 제공하고자 했음
          + 반면 요즘은 생각 없이 새로운 기능이 남발되고, 사용자를 존중하지 않는 방향으로 변화함
          + 잦은 인터럽트, 불필요하게 복잡한 온보딩, Apple Music 강제화, 직접 선택을 요구하는 이해 안되는 설계, ‘더 적게, 더 낫게'와는 완전히 반대의 방향
          + Apple의 미래가 없고 서서히 쇠퇴할까 우려
               o iOS의 일부 시스템 알림 자체가 오랫동안 사용자 무시에 가깝게 작동함
                    # 사용자가 특정 목표로 디바이스 잠금 해제 바로 순간 배터리 부족, Apple ID, 시스템 업데이트, iMessage 재설정 등과 같은 모달 알림이 뜨며, 사용자의 집중을 방해함
                    # Android는 이런 알림이 일반 노티피케이션으로 처리되어 사용자가 원할 때 처리할 수 있음
               o Apple이 일반 기업으로 퇴행했으며, 모든 것이 수익성 중심, 영업적 시너지를 위해 Wallet팀과 영화타이업 추진
               o 한때 모든 iPhone에 U2 앨범을 강제로 넣기도 했음
               o 실제로 사용자 불편을 방치하는 바람직하지 않은 UX 선택을 자의적으로 만들어낸 사례가 많아서 예전 Apple에 대한 ‘미신'이 있었던 것 같음
     * ""Daring fireball""이라는 이름이 멕시코 주유소 초밥 먹고 나온 듯한 느낌이라는 농담
     * Google 또한 지갑앱에 광고를 넣는 아이디어를 여러 번 회의에서 제시했을 것이라고 짐작함
          + 이번 Apple 광고 반발이 충분히 강해 Google이 비슷한 시도에 엄청난 신호탄이 되길 바람
          + Apple Wallet 이용자 중에는, Superman 예매 직후 ""거래 성공"" 알림에 F1 광고 알림이 뜨면서, 정말로 Apple의 프라이버시 보호가 경쟁력이 맞나 의구심을 갖게 된 사람이 있을 거라고 생각함
               o Google이 이 사건 때문에 지갑앱에 광고 넣기를 주저할 수도 있지만, 평소 Google이 Wallet 사용을 강요하는 방식 그 자체가 불쾌함
                    # 종종 내 폰 전체화면에 Wallet 앱 온보딩을 성가시게 띄우고 “예” 아니면 “나중에”만 선택할 수 있음
                    # 사용자가 원치 않아도 계속 귀찮게 해서 포기하게 만드는 방식의 기업을 신뢰할 수 없음
               o 실제로 Google Wallet엔 이미 광고가 들어감 관련 사례 참고
               o Google Photos도 기본 탑재되어 파트너사를 통한 사진 앨범 출력 광고 알림을 보내옴
               o Google이 먼저 유로 2024 기간 중에 ""거래 성공"" 화면에 축구 관련 애니메이션을 노출한 전례가 있음
     * U2 앨범 사태나 그 반발에서 전혀 배운 것이 없는지 의문
          + 단순 무료 아이템 번들 제공은 OS에서 자주 있었던 일로, U2 앨범은 스팸성도 아니었고, 원하면 쉽게 삭제할 수 있었음
               o 반면 이번 사례는 완전히 스팸에 해당하며, 민감한 공간(지갑)에 집중도를 끊는 방식으로 알림이 왔고, 아예 기획단계부터 문제적
               o 이런 일이 Apple에서 제품화된 자체가 심각한 문제 신호
          + U2 사태는 너무 오래 전 일이라, 그때 실수한 사람들은 이미 대부분 은퇴했을 수도 있다고 생각함
     * 결국 직접 표도 샀지만, Fandango 결제 과정에서 많은 불편이 있었고, 실제 전환율도 높지 않을 것이라는 확신
          + 최소 10개의 단계, Apple Pay를 쿠폰 적용에 쓸 수 없다는 안내 등 번거로움
          + 저조한 성과의 캠페인으로 오히려 브랜드 호감도만 깎아먹었다고 생각함
          + U2 앨범 때보다 더 촌스럽게 느껴짐
     * 스마트폰 푸시 알림도 포함하는 CAN SPAM Act 수준의 법률 제정 필요성을 느낌
          + Apple 같은 플랫폼 운영사는 푸시 알림에 광고 삽입을 금지해야 한다고 생각함
          + Microsoft가 윈도우 잠금화면이나 시작메뉴에 광고/정보 추가하는 최근 업데이트도 법적으로 문제소지가 있다고 봄
          + Google이 이런 걸 사실상 발명했다는 점도 상기
               o 모든 형태의 커뮤니케이션에 대해 이러한 법적 규제가 확장되어야 한다고 봄
                    # 스마트폰 노티피케이션, 웹푸시, 전화, 비둘기, 마을 방송, 병에 든 편지 등 어떤 매체든 상관없이, 원치 않는 상업 커뮤니케이션은 허용되지 않아야 함
               o CAN SPAM이 아니라 CANT SPAM 법이라는 농담도 함께 하고 싶음
"
"https://news.hada.io/topic?id=21810","Tools: 코드만 있으면 충분해요 - Code Is All You Need","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tools: 코드만 있으면 충분해요 - Code Is All You Need

     * MCP 방식의 도구 연결이 ""미래""라고 주장되지만, 실제로는 조합성 부족과 과도한 컨텍스트 소비라는 한계로 인해, 직접 코드를 작성하는 방식이 여전히 더 효율적임
     * LLM 시대에도 자동화·반복 작업에서는 코드를 생성/활용하는 것이 신뢰성과 검증 측면에서 우위에 있음
     * LLM은 추론 기반 자동화보다 코드 생성 및 반복 실행에 강점, 코드 기반 프로세스는 문제점 파악과 검증, 확장성이 뛰어남
     * Playwright 같은 일부 도구(MCP 방식)는 ""추론 기반"" 단계마다 불확실성과 디버깅 어려움이 커지고, 코드로 스크립트를 직접 생성/수정하면 반복성, 속도, 신뢰도 모두 향상
     * LLM이 코드를 생성하고, 또 다른 LLM이 그 코드를 점검·설명하는 ""코드-심사-반복"" 루프가 실제로는 가장 강력한 자동화 흐름임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

코드만 있으면 충분해요

     * 만약 트위터에서 나를 팔로우하고 있다면, 최근 Model Context Protocol(MCP)에 대해 별로 긍정적이지 않음을 알 수 있음
     * 이유는 아이디어 자체에 대한 거부감이 아니라, MCP가 광고한 대로 효과적이지 않기 때문
     * MCP는 두 가지 주요 결함이 있음
          + 진정한 조합성(composable) 을 제공하지 못함
          + 과도한 문맥 제공 요구로 인해, 코드를 직접 작성해 실행하는 것보다 더 많은 문맥 소모가 발생함
     * 간단한 실험을 통해 이를 확인할 수 있음
          + 예를 들어, GitHub MCP를 사용해 태스크를 수행한 다음, 동일한 작업을 gh CLI 툴로 해보면, 후자가 훨씬 효과적으로 문맥을 사용하고 더 빠르게 원하는 결과에 도달함

But MCP is the Future! : 하지만 MCP가 미래인데?

     * 내가 이 입장(코드가 더 낫다는 주장)에 대해 받은 피드백에 대해 이야기하고 싶음
     * 나는 MCP를 에이전트 기반 코딩(agentic coding) 문맥에서 깊이 실험했고, 한계가 가장 뚜렷하게 드러나는 부분에서 평가함
     * 한 가지 피드백은, ""MCP가 범용 코드 생성에는 굳이 필요하지 않다. 이미 모델이 코드 생성엔 충분히 뛰어나기 때문""이라는 점임. 반면 특정 도메인(예: 금융 기업의 자동화 업무) 같은 엔드유저 지향 애플리케이션에서는 MCP가 의미 있을 수 있다는 의견도 받음
     * 또 다른 의견은, 미래에는 모델이 더 다양한 도구에 접근하고 더 복잡한 작업도 처리할 수 있을 테니, 그 가능성에 주목해야 한다는 것임

     * 하지만 내 현재 판단은 이러함: 내가 실험한 데이터와 실제 경험에 따르면, 현재의 MCP는 항상 코드 직접 작성보다 사용이 어렵다
          + 가장 큰 이유는 MCP가 추론(inference)에 의존하기 때문임
          + 요즘 ""더 많은 툴을 LLM에 연결하려는 모든 시도""를 보면, 결국 LLM이 모든 툴을 받아서 작업에 맞게 필터링하는 계층이 들어감
          + 지금까지 이보다 더 좋은 구조나 접근 방식이 제안된 적이 없음

     * 그래서 나는 이렇게 결론내림: 비개발자용 특정 도메인 자동화 같은 특수한 경우에도, 결국 코드 생성이 조합성과 재사용성 면에서 항상 더 나은 선택

Replace Yourself With A Shellscript

     * 이 문제를 바라보는 방법: AI가 없을 때, 개발자라면 문제를 해결하는 도구는 코드임
     * 비개발자라면 코드가 어렵고, 많은 사람들이 수작업으로 하는 여러 작업도 사실 소프트웨어로 자동화 가능함
     * 현실적인 문제는 누가 그 코드를 써주느냐임. 만약 특수한 환경에 있고 직접 프로그래밍을 못한다면, 코딩을 새로 배우기도 어렵고, 누군가가 맞춤형 코드를 써주기를 기대하기도 힘듦
     * 물론 일부 작업은 추론(인간의 판단/유연성) 이 꼭 필요할 수 있지만, 사실상 반복적이고 명확한 작업은 대부분 코드로 자동화 가능함

     * “나 자신을 셸 스크립트로 대체한다”는 오래된 개발자 관용구가 있는데, 실제로 이런 자동화는 예전부터 지속되어 왔음
     * LLM과 프로그래밍의 시대에는, 셸 스크립트 대신 LLM으로 나 자신을 대체하려고 하지만, 여기서 세 가지 문제(비용, 속도, 신뢰성) 가 등장함
     * 이 세 가지 문제를 해결하기 전에는, 도구(MCP 등) 사용을 고민할 단계조차 아님
     * 즉, 자동화가 실제로 확장 가능한 방식으로 제대로 동작하는지를 먼저 보장하는 것이 핵심임

Automation at Scale : 대규모 자동화의 본질

     * 자동화의 핵심은 반복적·재사용 가능한 작업을 코드로 처리하는 것임
     * 한 번만 하고 다시는 안 할 작업은 자동화할 필요가 없음. 여러 번 반복할 작업, 기계가 진짜 생산성 이득을 주는 일부터 자동화가 시작됨
     * 실제로 1~2번 수동으로 해보고, 동작 방식을 정리한 뒤, 기계가 수천 번 반복 수행하도록 만드는 것이 자동화의 본질임

     * 이런 반복적 자동화에는 항상 ""코드""를 쓰는 것이 최선임
          + LLM에게 ""추론""으로 매번 시키면, 작은 일은 그럭저럭 되지만, 검증에 드는 시간/노력이 결국 자동화 효과를 반감시킴
          + 예: LLM에게 계산을 직접 시키는 것보다는, LLM이 파이썬 코드를 작성하게 하고, 그 코드로 계산하게 하면 신뢰도·확장성 모두 높아짐
               o 코드를 사용하면 공식/로직 자체를 검토 가능하고, 필요시 직접 수정하거나 LLM이 ""이 방식이 맞는가""를 심사하도록 할 수도 있음
               o 파이썬이 계산을 잘못한다는 걱정은 할 필요가 없으니, 코드 생성 방식이 검증/신뢰 측면에서 더 낫다는 점이 분명해짐

     * 이런 논리는 단순 계산을 넘어서서 실제 개발 실무에도 적용됨
          + 예: 최근 이 블로그의 전체 포맷을 reStructuredText에서 Markdown으로 변환하는 작업을 했음
          + 꽤 오래 미루고 있었는데, 귀찮기도 했지만, LLM에게 직접 변환을 맡기면 어딘가 미묘한 누락/실수나 맥락 왜곡이 생길까봐 신뢰하지 못함
          + 그래서 결국 LLM을 ""직접 변환 실행""에 쓰지 않고, 변환용 코드를 생성하게 해서 코드로 처리함

LLM → 코드 → LLM: 반복 검증 자동화의 실제

     * 첫 단계로, 나는 LLM에게 reStructuredText를 Markdown으로 변환하는 코어 변환 로직을 생성해 달라고 요청함
          + 단순 변환이 아니라, AST(추상 구문 트리)를 직접 활용해서
               o reStructuredText를 AST로 파싱 → Markdown AST로 변환 → HTML로 렌더링
               o 이렇게 하면 중간 변환 단계를 얻을 수 있고, 결과를 직접 비교·검증하기 쉬워짐

     * 두 번째로, LLM에게 기존 HTML과 신규 HTML을 비교하는 스크립트도 작성해 달라고 함
          + 변환 후 HTML의 차이(diff)를 분석하면서, 비교에 앞서 사소한 차이(예: 공백, 푸터노트 처리 방식 등)를 자동으로 보정하도록 설계
          + 변환 과정에서 허용할 수 있는 에러 유형을 LLM이 자체적으로 고려하게끔 함
          + 예: Markdown/ reStructuredText 라이브러리의 HTML 표현이 달라 미세하게 다르더라도, 본질적 손실/오류만 걸러내도록 스크립트에서 반영

     * 세 번째로, LLM에게 수백 개 파일의 결과를 일괄 분석하는 배치용 스크립트도 추가로 요청함
          + 이 스크립트로 전체 파일을 돌려가며, 차이가 줄어들 때까지 반복 개선(agentic 루프) 을 진행함

     * 전체 과정은 다음과 같음:
          + 처음엔 10개 정도만 샘플로 돌려서 차이가 크게 줄어들 때까지 반복
          + 만족스러운 상태가 되면 전체 포스트에 적용, 30분 내외로 자동 처리
          + 핵심은, LLM이 실제로 변환을 '성공'했기 때문이 아니라, 내가 전체 과정을 ""코드""로 검증·리뷰할 수 있었기 때문에 신뢰할 수 있었던 것임

     * 여기에 추가로, 다른 LLM에게 생성된 코드와 변경 내용을 점검/해설하게 해 더 높은 신뢰를 얻음
          + 데이터 손실 없이, 기계적으로 올바르게 변환된다는 확신이 있었고, 언제든 샘플 체크·수정이 쉬웠음
          + 최악의 경우에도 마크다운 문법상 사소한 오류만 생길 뿐, 실제 본문 내용이 망가지는 일은 발생하지 않았음

     * 또 한 가지 중요한 점은, 이 방식은 추론(inference) 비용이 일정해서 전체 파일 수(15개 vs 150개)에 따른 부담 차이가 크지 않음
          + 최종 분석 단계에서는 이미 사소한 차이는 자동으로 건너뛰기에, 대량 변환에서도 반복 검증 부담이 크지 않음

MCP Cannot Do That

     * 이 긴 설명의 요지는 전체 변환·자동화 파이프라인이 ""코드""로 돌아간다는 것임
          + 인간 입력 → 코드 생성 → LLM 심사 → 반복 개선, 이런 구조를 일반적인 과제에도 똑같이 적용 가능
     * 예를 들어, MCP 방식의 대표 사례인 Playwright가 있음
          + 브라우저를 원격 제어하는 자동화 도구로, 페이지를 읽고 이해하고 버튼을 누르는 등 매 단계마다 추론(inference)이 반복됨
          + 이런 류의 과제는 실제로 ""코드 접근""으로 완벽히 대체하기 어렵긴 함

     * 하지만, 페이지 구조를 이미 알고 있다면(예: 내가 개발 중인 자체 앱 테스트 등)
          + LLM에게 Playwright 파이썬 스크립트를 생성하게 하고, 그걸 실행하는 방식이 훨씬 빠르고 신뢰성도 높음
          + 이 방식은 한 번 스크립트만 만들면 수십, 수백 번 반복 실행이 가능하고, 추가 추론이 필요 없음
          + 실시간으로 매번 화면을 해석하거나 버튼 위치를 찾지 않아도 되고, 전체 자동화 흐름을 한 번에 실행할 수 있음

     * MCP 방식은 매 단계마다 추상화된 도구 호출과 추론이 필요해 LLM이 항상 올바르게 동작하도록 만드는 것이 매우 어렵고, 디버깅도 힘듦
          + 예를 들어 MCP 클라이언트를 셸 스크립트에 임베드해 효율적으로 원격 서비스 호출을 하고 싶어도, 실제로는 이 방식이 매우 비효율적이고 구현도 어렵다는 점을 절감함

     * 결국, 나는 인간이지 MCP 클라이언트가 아님.
          + 코드는 실행·디버깅이 쉽지만, MCP 호출은 매번 불확실하고, 신뢰할 수 없음
          + 실제로는 LLM이 코드 생성 중 만들어주는 작은 툴들(예: Claude Code의 스니펫)을 오히려 내 개발 프로세스의 장기적 도구로 활용하고 있음

이 결론은 어디로 이어질까?

     * 솔직히 나도 이 흐름이 어디로 이어질지는 모름. 하지만 지금이야말로 ""의도적 에이전트 코딩(agentic coding)""을 위한 코드 생성 방식을 어떻게 더 개선할 수 있을지 고민하기 좋은 시점임
     * 이상하게 들릴 수 있지만, MCP도 가끔은 정말 잘 동작하는 경우가 있음. 하지만 현재 구조는 너무 ""추론""에 의존하고, 확장성 있는 대규모 자동화에는 적합하지 않은 막다른 골목처럼 느껴짐

     * 그래서 아마도 MCP가 강점을 발휘할 수 있는 영역과, 코드 생성 방식의 역할을 더 명확하게 분리·추상화하는 방법을 찾아야 할 듯함
          + 이를 위해선 더 나은 샌드박스(안전 실행 환경) 를 만들고, 에이전트가 API를 자유롭게 ""팬아웃/팬인(fan out/fan in)"" 추론을 할 수 있게 API 설계를 바꾸는 시도도 필요함
          + ""코드로 할 수 있는 건 최대한 코드로 처리""하고, 대량 실행 이후엔 LLM이 전체 결과를 판정·리뷰하는 구조가 바람직하다고 생각함

     * 그리고 코드 생성 과정에서 충분한 맥락 정보를 추가해서, LLM이 생성된 스크립트가 무슨 일을 하는지 비개발자에게 자연어로 설명할 수 있게 한다면, 향후 이 자동화 흐름을 비개발자들도 손쉽게 활용할 수 있을 것임

     * 결론적으로, 나는 MCP 대신 LLM의 코드 생성 능력을 더 과감하게 활용하고, 새로운 가능성을 실험하길 추천함
     * LLM이 코드를 직접 쓰게 두면, 우리가 상상하는 것보다 훨씬 더 많은 것을 자동화할 수 있음

참고 자료

     * 나의 Agentic Coding Talk (영상)
     * Drew Breunig 의 How to fix your context (글)
     * Manuel Odendahl 의 MCPs are Boring (영상)

        Hacker News 의견

     * 대체로 이 방향성이 맞음에 공감함. 대규모 LLM 사용은 보통 두 개의 견고한 인터페이스 사이의 빈틈을 채우는 용도로 많이 활용됨. 신뢰성의 핵심은 LLM의 결과물이 아니라, 실제로는 그 인터페이스 자체가 특정한 설정만 허용하는 데서 나옴.
       LLM의 출력은 종종 타입이나 DB의 primary key처럼 좀 더 결정론적인 값으로 강제로 변환됨. LLM의 가치는 기존 코드와 툴이 얼마나 내 도메인의 데이터, 로직, 행동을 잘 모델링하느냐에 따라 크게 달라짐.
       개인적으로 LLM을 요즘은 3D 프린터와 유사하게 느끼고 있음. 둘 다 빠른 프로토타이핑에서 빨리 부품을 이어주지만, 확장성과 견고함을 원한다면 결국 엔지니어나 LLM이 임시 연결부를 금속/코드 같은 결정론적 지원체로 대체해야 함.
       3D 프린터에 대한 과거의 과장된 기대처럼 LLM도 모든 운영 현실을 대체할 수 있을 것처럼 보이나, 진짜로는 기존의 디지털 모델링이 견고한 기반이 될 때만 진짜로 쓸모가 있음
          + 드론이나 VR의 과장된 hype cycle도 비슷했음. 모두가 드론으로 택배 배송하고, VR에서 하루를 보낼 거라고 했지만 실제 적용 사례는 훨씬 좁았음
          + 흥미로운 의견이지만 LLM에 대해 지나치게 보수적 관점인 듯함. 실제로 LLM은 이미 심층 연구나 번역 같은 분야에서 대규모로 쓰이고 있으며, 3D 프린터보다 더 범용적으로 확산된 상태임
          + ""방향성은 맞음""이란 표현에 공감함. 우리 회사에선 완전히 맞지 않아도 'directionally accurate'란 용어를 종종 씀. 대략 올바른 방향이라는 의미임
     * LLM 툴을 쓸 때 깨달은 점이 있음. 문제를 샌드박스 안에서 반복적으로 툴을 활용해 LLM이 풀 수 있는 형태로 줄이면, 그 문제를 브루트포스로 해결할 수 있음. 이때 핵심은 그런 문제를 식별하고, 맞는 샌드박스와 사용할 툴, 성공 기준을 정하는 것임.
       이 과정도 상당한 기술과 경험이 필요하지만, 손으로 일일이 시행착오를 하는 것보다 훨씬 고차원적임.
       내가 '어셈블리 Mandelbrot 실험'을 하면서 이걸 깨달았음.
       (실험 링크: https://simonwillison.net/2025/Jul/…)
          + ""성공 기준 정의""가 마지막에 꼭 필요함. 프랙탈 수학이나 x86 어셈블리를 모르면, ""그림이 Mandelbrot처럼 보이나?""란 시각적 확인 밖에 검증이 안 됨.
            이상적으로는 연속 함수 형태의 평가 기준, 최소한 다양한 입력과 그에 대한 기대 출력값을 정량적으로 만들어야 진짜 자동화가 가능함
          + 이 실험 정말 흥미로움. 나도 LLM으로 브루트포스 문제 풀이를 고민 중이었음.
            예시로 LLM이 타입스크립트 제네릭에 약한데, 진짜 TSC를 돌아가게 하면 계속 테스트로 검증하며 맞을 때까지 시도할 수 있음. 코드 유지보수성이 떨어질 수도 있지만 이론상 아주 신기한 구조임.
            게다가 Cursor는 타입스크립트 에러를 볼 수 있어, 유틸리티 타입 테스트만 만들면 Cursor 자체가 테스트를 쓰고 문제를 단순 반복 brute force로 풀 수도 있음
          + 내 경험상 LLM에게 일 맞는 툴 사용하도록 시키는 것이 아직도 큰 도전임. 마치 아이에게 빨래를 가르치는 것처럼, 그냥 내가 직접 하고 싶어질 때가 많음
          + LLM에 올바른 문맥 제공이 중요함. 예시로, 미리 정의된 ""인지적 도구""를 활용해서 맥락을 풍부하게 주면 성능이 밝히 개선됨.
            참고할만한 리포: https://github.com/davidkimai/Context-Engineering/…
            아직 전체 못 봤지만 꽤 인상적임
          + 샌드박스에서 도구 루프 돌리기 방식이 클라우드 API를 토큰 많이 써서 하는 걸 요구하는지 궁금함.
            로컬 모델로도 가능한지, 아니면 Claude Code Pro 등 구독으로 할 수 있을지 고민임.
            Mandelbrot 실험도 재밌었지만, 실제 복잡한 상용 코드베이스와는 난이도가 좀 다름
     * 이건 MCP 자체의 문제는 아니라 생각함. 현재 AI 수준에선 사람이 중간에 들어가는 구조가 훨씬 우수함.
       LLM은 특정 작업엔 강하지만 종종 로컬 미니마에 갇힘. 그래서 웹 인터페이스에서 왔다 갔다 하며 ""프로그램 짜줘 → 확인 & 힌트 제공 → 테스트"" 루프를 돌리면 품질이 확실히 좋아짐.
       10,000줄의 엉망 코드를 400줄의 명확한 코드로 바꿀 수 있음. 지금 당장은 이게 현실임.
       물론 많은 기업이나 개발자는 ""프로그래머 자체를 LLM으로 대체""하려 하겠지만, 현실에선 아직 불가능함.
       진짜 효과는 프로그래머의 작업 속도를 몇 배로 높이거나, 초심자가 LLM을 통해 빠르게 생산성 있는 작업을 할 수 있게 하는 부분임. 하지만 ""agentic coding""은 아직 잘 작동하지 않음.
       현 상황에서 LLM은 동료나 조력자로 쓰는 게 정답임. 지금은 자율적 피드백 없는 ""AI 에이전트""가 아니라는 것이 실상임
          + 나도 직접 만든 제품 개발자로, 모든 코드를 혼자 담당하고 claude-code를 도구로 사용함. 언젠가 Claude가 모든 코딩을 대체하길 기대하지만, 아직 거기까지 오진 않았음.
            타입세이프하고 함수형, 컴파일 언어로 작업 중이라 결과물도 항상 직접 읽어야 해서, 덜 엄격한 언어라면 더 걱정될 듯함.
            그럼에도 시간 절약 효과는 큼. 특히 작업을 분할해 큰 목표를 더 쉽게 다룰 수 있게 된 점이 만족스러움
     * 실제로 GitHub MCP로 태스크를 해보고, 같은 일을 gh CLI로 하면 gh CLI가 맥락을 훨씬 효율적으로 사용해 속도가 많이 빠름.
       나는 ""devops"" 폴더에 CLAUDE.md 파일(공통 bash 명령 모음집)이 있음.
       새로운 태스크를 완료하면 Claude에게 예시를 추가하게 하고, 이후 유사 질의엔 Claude가 한 번에 해결함.
       CLAUDE.md 초기 내용 공유:
          + 이 파일은 Claude Code가 코드 작업 시 지침을 제공함
          + 특히 GCP 관련 DevOps 명령(Cloud Composer, Logging, Kubernetes, Cloud Run) 문서화
          + 주요 명령 예시: 환경 세부 정보 보기, DAG 관리, Airflow 로그 확인 등
            (구체적인 명령어는 요약)
          + 가끔 혼란스러움이 있음. 필요한 명령 모음집 파일이 있는데 AI에게 실행시키기만 기다리는 게 비효율적으로 느껴질 때가 있음
          + 참고로, 저 명령들 section을 간단한 stdio MCP 서버로 만들어 Claude Code에 붙이면 각 작업을 툴화하고, 인자 입력 스키마 정의도 가능함. 이미 이런 기능을 지원하는 MCPServer 오픈소스가 있음 (예시: https://github.com/inercia/MCPShell)
          + 나도 비슷한 명령 모음 파일을 Emacs org-mode에서 직접 씀. 접기/펼치기 쉬워서 효율적이며, 코드 스니펫(쉘, 엘리스프 등)을 바로 실행할 수 있음
          + LLM에게 프로덕션/테스트 환경에 특권 API 호출 권한을 그냥 넘기는 것인지, 아니면 sanity check 가능한 예시 명령만 받는지 궁금함. 구체적인 활용 케이스가 궁금함
          + 비슷한 구조로 진행했는데 claude.md가 점점 커져 불편했음. 그래서 커스텀 프롬프트를 앱화해서 상황을 개선했으나, 앱은 결정론적이라 미지의 상황은 대응이 힘듦. 반면, CC는 느려도 미지 상황도 대처가 가능함.
            그래서, 문제 발생 시 커맨드에 앱 테스트&수정 명령을 추가한 셀프 힐링 소프트웨어가 됨
     * 지금까지 본 MCP 활용 중에서 가장 인상적이었던 건 Bruce Hauman의 clojure-mcp임.
       LLM에게 (a) bash, (b) persistent Clojure REPL, (c) 구조적 편집 툴을 제공함.
       이로 인해 Clojure 코드를 편집할 때 순수 문자 diff 기반 접근보다 훨씬 효율적으로 작동함.
       제대로 된 테스트 스위트만 있다면 파일 편집, 리로드, 테스트 반복이 사람 작업에 가깝게 진행되어 놀랐음
          + 나도 clojure-mcp가 지금까지 본 MCP 중 가장 멋진 활용이라고 생각함.
            코드 디버그, 개별 식 평가, 함수 반환 타입 문서화 등 주요 기능 지원함.
            강력한 REPL을 가진 언어가 그런 기능에서 훨씬 우수하다고 느꼈고, clojure-mcp 활용 가능성을 보고 AI에 대한 인상이 크게 바뀜
          + 참고로 공식 리포 링크: https://github.com/bhauman/clojure-mcp
     * GitHub CLI 예시는 MCP의 강점을 다 보여주진 못함.
       gh CLI처럼 문서 접근성이 좋은 툴은 LLM이 쉽게 코드 생성하니까 당연히 더 잘 활용함.
       하지만 MCP의 진짜 장점은 내부전용 툴이나 온라인 문서가 거의 없는 니치 API에서 드러남.
       모든 문서를 맥락에 넣는 방식도 있지만, 이런 경우는 오히려 MCP가 더 효율적임.
       잘 설계된 MCP 툴을 올바른 인풋과 함께 쓰면, LLM의 API 이해, 인증, 엣지 케이스 관리 등 부담이 대폭 줄어듦.
       GitHub에는 MCP가 굳이 필요 없겠지만, 사내/불완전 API 같은 환경에선 사전에 만들어둔 MCP 툴이 더 강력하게 통함
          + MCP도 결국에는 문서를 LLM 입력 맥락에 주입하는 접근임. LLM이 잘 아는 환경(파이썬, 자바스크립트, bash 등)에선 MCP 툴 호출보다 아는 방식 활용이 낫고, 오히려 툴 정의로 맥락 소모 심함.
            예시로 sonnet4에선 툴 15개만 넘어도 한계점임. 공식 playwright MCP만 써도 툴 용량 소모함
          + MCP 서버가 인증 등 처리를 단순화하긴 하지만, 내부 API 자체를 애초에 그렇게 설계할 수 있지 않았냐는 반론도 있음.
            결국 MCP의 유일한 장점은 API가 너무 어렵게 느껴질 때 ""이게 단순히 복잡했다""는 사실만 재확인하는 데 쓰일 수 있음
     * Playwright 예시 관련,
       나 역시 이번 주에 Playwright MCP 서버 기반으로 에이전트 만들었다가 느리고, 토큰 비효율적이며 신뢰성이 낮아 다시 직접 Playwright 호출로 바꿨음.
       MCP 서버는 뭐가 가능한지 테스트엔 좋은데, 실제론 API 호출이 더 효율적이고 안정적임.
       내가 만든 개인 LinkedIn 에이전트 예시 및 데모 공유:
          + 프로젝트: https://github.com/pamelafox/personal-linkedin-agent
          + 데모: https://www.youtube.com/live/ue8D7Hi4nGs
          + Playwright MCP 구현도 내겐 지나친 솔루션이었음. 오히려 Playwright API의 일부만 제한적으로 베껴온 참고용 구현이라고 생각함.
            LinkedIn은 자동화가 굉장히 어려운 플랫폼으로 유명한데, 개인용 LinkedIn 에이전트 만들면서 어려움이나 제한을 겪었는지 궁금함
     * 사실 터미널만 있으면 충분하다고 느낌.
       수개월간 MCP를 매일 썼지만, 이제는 오직 하나의 iTerm2 기반 MCP 서버(터미널) 만 씀.
       필요시 OpenAPI spec이 있긴 하지만, 실제로는 shell command랑 curl로 거의 모든 걸 수행할 수 있음
          + 나 역시 LLM이 bash shell의 내장 툴을 활용하는 걸 보고, bash로 가능한 한계가 어디까지인지를 처음 깨달았음
     * ""너무 많은 맥락 필요""란 지적은 사실 초기 프롬프트 기본값 설정만 잘 해두면 됨.
       Claude Code나 Gemini CLI 포함 주요 툴은 모두 지원함
       LLM에 모든 툴 리스트를 넘기고, 스스로 걸러쓰게 하는 방식이 최고의 접근은 아니지만,
       최신 LLM은 계속 발전하며, 실제로 적절한 MCP 함수 선택에 큰 어려움을 겪은 적이 없음.
       비용, 속도, 신뢰성 문제도
          + 비용: 최근엔 점점 저렴해지고 있음. 실제로 놀라운 가성비임.
          + 속도: 여러 작업을 동시에 시킬 수 있어 비효율적이지 않음.
            대화에 직접 시간을 쓰지 않아도 됨
          + 신뢰성: 프롬프트 품질에 많이 좌우됨.
            최근 예시로 Notion, Linear, git, GitHub PR/CI 로그 등 대량 외부 툴을 LLM이 자동으로 처리했고,
            나는 단 한 번 PR 리뷰만 했을 뿐임.
            비용도 1달러 미만이었음
          + MCP 툴이 많아질수록 초기 프롬프트 맥락 소모가 더 심해짐.
            오히려 툴을 추가할수록 초기에 필요한 정보가 더 많아져서 심각한 제약이 발생할 수 있음
          + 비용 문제는 실제로 점점 낮아진다기보다,
            아직 감춰져 있을 뿐이며, 무료·저가 프로모션 구조는 오래 못감.
            예시로 Cursor도 월 $200 요금제가 도입되고, 저가 요금제의 서비스 품질이 악화됨.
            무료 프로모션이 끝나면 원래 수준으로 돌아올 것임
     * 나는 Julia로 일하는데, 장기 실행 세션 환경에서 이득을 봄.
       함수 최초 실행 시 컴파일되므로, MCP를 만들어 Claude Code가 영속적 Julia kernel(Jupyter) 로 코드 전송하게 함.
       테스트 코드 실행이 훨씬 빨라지고, CC가 bespoke bash 대신 코드베이스의 기존 함수를 더 잘 활용함.
       CCUsage 기준 토큰 사용량도 50% 가까이 줄었음.
       꼭 MCP일 필요도 없었겠지만, 요지는 '특정 기능'을 코드베이스에 붙이는 게 Claude 대상으로 일일이 custom 코드를 짜는 것보다 쉽다는 점임
          + ""Claude가 bespoke bash 대신 바로 코드 함수 실행""을 선택하는 게 오로지 kernel에 직접 코드 전송 가능이어서 그런 건지 궁금함

   공감은 하지만, MCP 문제보단 그 제작장.ㅣ 설계 방향의 최적화 같습니다. 같은 기능을 해도 어떤 MCP냐에 따라 내부적으로 코드를 생성하고 명령어를 생성해서 불필요한 작업을 줄이기 때문이죠. Github mcp가 아닌 gh cli mcp나 터미널 mcp를 쓰면 토큰은 쓰긴 하지만 훨씬 덜 사용하고, 같은 효과를 얻을 수 있는데, 이 부분이 간과된 것 같네요.
"
"https://news.hada.io/topic?id=21757","블룸 필터 예제로 이해하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             블룸 필터 예제로 이해하기

     * 블룸 필터는 메모리 효율적으로 집합 내에 요소의 존재 여부를 빠르게 확인하는 확률적 자료구조임
     * 요소가 집합에 확실히 없는지, 또는 있을지도 모른다고만 알려주며, 가짜 양성의 확률이 존재함
     * 기본 구조는 비트 벡터와 여러 해시 함수를 사용하여 각 요소에 해당하는 비트를 1로 설정함
     * 필터 크기와 해시 함수 개수에 따라 오차율과 성능이 결정되며, 활용 용도에 맞게 조정 가능함
     * 추천 해시 함수 및 최적 설정 방법, 공간 효율성, 실제 적용사례 등도 소개됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

블룸 필터란 무엇인가

     * 블룸 필터는 특정 요소가 집합에 존재하는지 빠르고 메모리 효율적으로 판별하는 자료구조임
     * 이 효율성을 위하여 블룸 필터는 확률적 자료구조로, 검사 결과가 ""집합에 확실히 없음"" 또는 ""집합에 있을 수도 있음""으로 나뉨
     * 블룸 필터의 핵심 구조는 비트 벡터임
     * 원소를 추가할 때 각 원소를 여러 번 해시해서 그 인덱스의 비트를 1로 설정함
     * 각 해시 함수별로 나온 인덱스에 해당하는 비트가 모두 1일 경우 ""존재할 수도 있음""이라고 판별하고, 그렇지 않으면 ""확실히 없음""으로 처리함

동작 원리 예시

     * 여러 개의 해시 함수(ex: Fnv, Murmur)를 통해 원소를 여러 개 비트 인덱스로 매핑함
     * 원소 추가 시 계산된 인덱스의 비트를 1로 변경함
     * 특정 원소의 존재 여부를 검사할 때, 같은 해시 함수와 동일한 방식의 인덱스가 모두 1이면 ""존재할 수도 있음""으로 간주함
     * 만약 하나라도 해당 비트가 0이면 ""집합에 없음""으로 확실히 판단 가능함
     * 이로 인해, 거짓 긍정(가짜 양성) 의 가능성이 생김

고급 주제

     주의: 작성자는 실제로 대규모 서비스에 블룸 필터를 적용해 본 경험이 없음

  해시 함수 선택

     * 독립적이고 균등 분포를 가지는 해시 함수가 권장됨
     * 암호학적 해시 함수(sha1 등)는 느리기 때문에 적합하지 않음
     * 빠르고 단순한 해시 함수 예시는: Murmur, xxHash, Fnv, HashMix 등임
     * 실제 사례에서는 md5에서 murmur로 변경 시 800% 이상 속도 향상을 경험함

  블룸 필터의 크기 결정

     * 필터 크기(m) 가 클수록 거짓 긍정률이 줄어듦
     * 거짓 긍정률은 보통 (1-e^(-kn/m))^k으로 근사 가능함
     * 기대 원소 수(n), 필터 크기(m), 해시 함수 개수(k)를 적절히 정해야 함

  해시 함수 개수는?

     * 해시 함수 수가 많을수록 속도가 느려지고 필터가 더 빨리 채워짐
     * 너무 적으면 거짓 긍정률이 높아짐
     * 이상적인 k는 *(m/n)ln(2)*로 계산됨
     * 설계시 다음 절차로 진행:
          + 기대 원소 수 n을 추정
          + 비트수 m을 정하고
          + 최적 k를 산출
          + 원하는 오차율이 나오는지 확인, 안 되면 m을 조절

  성능 및 공간 효율

     * 블룸 필터에서 원소 추가/존재 검사는 O(k)의 시간 복잡도 가짐
     * 공간 효율성은 오차율 허용 범위와 원소 범위에 따라 결정됨
     * 원소 범위를 대략적으로나마 예측하지 못하면 해시 테이블이나 확장형 블룸 필터가 나을 수 있음

  활용 사례

     * 자세한 활용예는 Wikipedia 참고
     * C. Titus Brown는 블룸 필터의 바이오인포매틱스 적용사례를 제시함

참고 자료

     * Broder, Mitzenmacher : Network Applications of Bloom Filters: A Survey — 블룸 필터 개요 논문임
     * Wikipedia – Bloom Filter
     * Kirsch, Mitzenmacher: Less Hashing, Same Performance
     * Almeida 등: Scalable Bloom Filters

        Hacker News 의견

     * 이 글은 저 같은 사람들에게 딱 맞는 내용이라는 느낌. 이름은 들어봤지만 그때마다 찾아보려 하다가 계속 미뤘었음. 이번에 글을 보고 드디어 찾아봤는데, 내가 원하던 딱 그 입문서 같은 느낌
          + iBooks의 검색 기능 구현을 위해 Bloom filter를 처음 알게 되었던 경험. 벌써 10년이 넘은 이야기
          + Bloom filter는 정말 재미있음. 문제를 풀면서 Bloom filter가 필요할 때가 오면 정말 신나는데, 도메인에 따라 그런 상황이 적어 아쉬움
     * 글쓴이에게 한 가지 제안 사항. 인터랙티브한 부분 아주 만족. Bloom filter의 특성을 더 잘 이해시키기 위해 두 문자열이 해시 충돌을 일으키는 예시를 주고 하나를 입력해보고, 다른 하나를 두 번째 입력창에 입력해보게 하면 좋겠음. 이렇게 하면 ""집합 안에 있는지 확실하지 않고 아마도 있다(maybe)""라는 특유의 결과 논리가 왜 나오는지 쉽게 이해할 수 있음
          + 예시로 ""bloom""과 ""demonstrators ""(마지막 공백 포함)가 fnv: 7, murmur: 12에서 충돌함
     * 2009년 대학 시절 CUDA로 Bloom filter를 구현한 경험. 지도교수님이 전 Nvidia 출신이었음. 그런데 커리어에선 GPU 프로그래밍과 전혀 상관없는 길로 감. 다른 선택을 했더라면 1억 달러를 벌 수 있었을지도 모르겠다는 생각
          + 1970년에 나온 컴퓨터 과학 아이디어라 불가능했을 거라는 생각. GPGPU 관련 아이디어는 이미 충분히 연구된 느낌. 나도 GPU로 Hashcash 구현을 10년 전에 했는데, 지금 봐선 가치가 0에 가까움
          + 대학 졸업 프로젝트로 머신러닝 알고리즘을 CUDA로 포팅한 뒤, 별 일 아니라는 생각으로 임베디드 프로그래밍으로 진로 전환
          + 나도 비슷한 경험. 2009년 GeForce 8과 CUDA v1(!)로 GPU 최적화된 바이오인포매틱스 툴킷을 아마 최초로 만들었던듯. 그냥 호기심으로 만든 뒤엔 완전히 다른 길로 갔고, 큰 돈을 벌 찬스를 놓침
          + 비트코인을 샀더라면 더 큰 돈을 벌 수 있었을 것이라는 농담 섞인 말
     * 내가 좋아하는 트릭이 있음. 원소 개수가 적을 수도 있고, 멤버십 체크를 자주 하게 되는 작은 set에 대해, 64비트 Bloom filter를 아주 간단한 해시 함수와 함께 넣어두는 방식. 바보 같아 보여도, 비용이 너무 낮아서 일종의 도박처럼 써볼 수 있음. 안 맞아도 멤버십 체크나 insert에서 10ns정도만 추가되고 그만이지만, 잘 맞을 때는 엄청난 작업량을 줄일 수 있음
          + Chromium도 곳곳에 이런 트릭을 사용. 글에는 Safe Browsing만 언급되지만, Blink 레이어에선 rapidhash와 이런 마이크로-필터를 적극적으로 씀. 예시로 querySelector(), CSS 버킷에서 해시 룩업 사전필터, 접근성 위해 Aria 속성 탐색에서 rapid reject 등. 32비트, 64비트 작은 filter도 실제로 잘 동작함. 더 큰 Bloom filter도 적절히 활용, 나도 이런 기능을 직접 추가했습니다
     * 최근 로그 메시지 안티-스팸 기능에 Bloom filter를 활용한 경험. 로그 메시지를 해싱해서 filter에 넣고, 이미 filter에 있으면 출력하지 않음. 주기적으로 filter의 비트를 싹 지움. 원자적으로 비트 전체를 클리어할 필요 없이, 메시지가 들어오면서 일부 비트가 지워졌어도 로그가 다시 찍힘. 이전엔 메시지별 카운트를 따로 유지했지만, 이 방식이 훨씬 효율적이었음. 실제로 직접 이 용도에 Bloom filter를 적용해서 만족도가 높았던 경험
     * Bloom filter 시각화 예시로 이 페이지 마지막 부분에 좋은 자료가 있음
     * Eli Bendersky가 쓴 또 다른 Bloom filter 입문글을 추천. 더 자세히 알고 싶으면 여기 참고
     * Bloom filter, set, 해시테이블을 이해하는 데 필요한 개념이 거의 95%는 겹친다는 생각. set은 key만 신경 쓰는 해시 테이블이고, Bloom filter는 해싱의 충돌 특성을 적극적으로 이용하는 set임. 일부러 충돌이 잘 일어나도록 만든 해시함수 사용. 특정 key가 들어간 적이 있다면 무조건 맞아떨어짐. 하지만 같은 해시값을 만든 다른 key가 있을 수 있음. 그래서 이건 버그가 아니라 의도한 특성
          + 나도 Bloom filter를 해시테이블에서 데이터 자체는 빼고, 버킷만 추적하는 방식처럼 생각하는 편. 이런 마인드셋을 가진 사람이 나만이 아니라서 반가움
          + Bloom filter가 충돌을 줄이기 위해 여러 개의 해시함수를 쓴다는 부분이 추가로 보완되면 좋을 듯. 예를 들어 3개의 해시함수가 모두 맞아야 set에 있다고 판단. 이런 구조 덕분에 false positive 확률은 낮추면서 false negative는 절대 안 나옴
          + Bloom filter를 이해했다면, 랜덤 프로젝션이나 Locality Sensitive Hashes의 일부 구현도 곧 이해가 가능
     * Cassandra read spike 디버깅 중 Bloom filter 활용에 깊이 빠졌던 경험. 존재하지 않는 key임에도 sstable 조회가 너무 많아서 이상했음. sstable마다 붙는 Bloom filter의 의미를 뒤늦게 파악. 기본 false positive 비율이 0.1이라서 우리 상황엔 너무 높았음. 대부분 cache miss였고 false positive 때문에 무의미한 조회가 너무 많았음. 비율을 0.01로 낮췄더니 메모리 사용은 소폭 늘었지만 쓸데없는 read가 크게 줄어서 p99 read latency가 16~18%나 줄었음
     * Bloom filter를 정말 좋아함. 기술 이야기할 때 꼭 알려주는 세 가지 핵심 개념이 Bloom filter, Random Weight Hashing(Rendezvous Hashing, Highest Random Weight Hashing), 그리고 Cumulative flow diagram임. 이 세 개념이 복잡한 분산 시스템 운영에 필수적이라는 생각
          + 분산 해시 테이블 구조도 동일하게 중요한 주제라는 생각. 예를 들어 circle, chord, CAN, kademlia 등 다양한 아키텍처
"
"https://news.hada.io/topic?id=21779","피그마, 상장(IPO) 위한 등록 신청서 제출","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       피그마, 상장(IPO) 위한 등록 신청서 제출

     * Figma는 미국 증권거래위원회(SEC)에 기업공개(IPO) 등록 신청서를 제출함
     * 이번 상장으로 뉴욕증권거래소에서 Class A 보통주가 FIG 코드로 거래 예정임
     * 주식 수량과 가격대는 아직 미정이며, 상장 시기 등 조건은 시장 상황에 좌우됨
     * Morgan Stanley, Goldman Sachs 등 주요 투자은행들이 공동 주관사로 참여함
     * Figma는 디자인 협업 및 제품 개발을 위한 AI 기반 플랫폼으로 효율적 협업을 지원함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Figma, Inc. 기업공개(IPO) 등록 신청서 제출

   Figma는 오늘 미국 증권거래위원회(SEC)에 Form S-1 등록 신청서를 제출하며, Class A 보통주에 대한 기업공개(IPO) 를 추진함을 공식 발표함

   Figma는 Class A 보통주를 뉴욕증권거래소에 ""FIG"" 종목 코드로 상장할 계획임

   현재 공모 주식 수와 공모가 범위는 미정임. 상장 여부 및 시점, 최종 조건 등은 모두 시장 상황의 영향을 받으며, 상장 완료의 확실성은 보장되지 않음

주관사 및 공모 관련 정보

     * Morgan Stanley, Goldman Sachs & Co. LLC, Allen & Company LLC, J.P. Morgan이 공동 리드 북러너로 참여함
     * BofA Securities, Wells Fargo Securities, RBC Capital Markets가 북러너로, William Blair와 Wolfe | Nomura Alliance가 공동 매니저로 참여함

   공모는 반드시 증권 발행 안내서(프러스펙트스) 를 통해서만 제공될 예정임

중요 안내 및 법적 고지

   SEC에 등록 신청서(Form S-1)가 현재 제출만 되었고, 아직 효력이 발생하지 않은 상태임

   해당 증권은 등록 신청서가 정식으로 승인되기 전까지는 판매, 청약, 증권 거래 등이 허용되지 않음

   이 보도자료는 증권의 매도나 청약 권유 행위를 구성하지 않으며, 증권법상 허용되지 않은 지역에서는 청약 및 판매가 불가함

Figma 소개

   Figma는 팀이 협업하여 디지털 제품과 경험을 개발할 수 있게 하는 플랫폼임

   2012년 설립 이후 디자인 도구에서 출발하여, 현재는 AI 접목을 토대로 아이디어 발상부터 제품 출시까지 전 과정을 아우르는 연결형 플랫폼으로 진화함

   기획, 디자인, 빌드, 출시 등 모든 단계에서 더 협력적이고, 효율적이며, 즐거운 워크플로우를 제공하여 팀 전체가 항상 동일한 정보를 공유하게 도움을 줌

문의처

     * 미디어: press@figma.com
     * 투자 관계: ir@figma.com

        Hacker News 의견

     * 이 스레드에 너무 부정적인 의견이 많아서 놀람 느낌이 있음. Figma는 그 악명 높은 Adobe의 대안이면서 실제로 더 나은 결과를 보여준 브랜드임. 공동 창업자가 직접 만든 멋진 기술력이 바탕이 되어 있음. 여전히 후한 무료 요금제를 제공 중임. 직원들이 금전적 보상을 받게 되어 기쁜 감정임
     * 공식 공지문에서는 Form S-1 투자설명서 링크가 없지만, SEC 웹사이트에서 공개적으로 확인 가능함. 주요 지표는 최근 12개월 기준 매출 8.21억 달러, 전년 대비 매출 성장률 46%, 비GAAP 영업이익률 18%, 매출총이익률 91% 수준임. 정말 말도 안되는 성공 사례라는 감상임. 공동 창업자 Evan Wallace가 주도적으로 설계한 초기 엔지니어링이 엄청난 기반을 닦은 배경에 주목함. Evan의 개인 블로그에서 다양한 기술 포스트를 확인 가능함. 특별히 인상적인 부분으로는, Figma의 에디터는 하이브리드 C++/JavaScript 구조로 만들어져 브라우저에서 최상의 디자인 도구로 거듭남. 문서 표현부와 캔버스는 C++로, 나머지 UI는 JavaScript(나중엔 TypeScript + React)로 구현되어, 빠른 UI 개발과 최적화된 속도를 동시에 잡을 수 있었던 구조임. C++ 개발은 더 좋은 디버깅 환경을 제공하는 Xcode에서
       진행함. Figma 문서의 내용이 HTML로도 만들 수 있지만, 실제론 자체적인 문서 렌더링 방식을 구현하여 일관성 있는 결과와 고성능을 확보함. WebGL을 사용해 대부분의 브라우저 HTML 렌더링 파이프라인을 우회하고, 그래픽카드와 밀접하게 동작함. 엔진은 곡선, 이미지, 블러, 마스킹, 블렌딩, 투명도 그룹 처리 등 고품질 시각 효과에 최적화되어 있음. 또한 Evan이 직접 멀티플레이어 동기화 프로토콜을 만들고, 진짜 실시간 협업 서비스를 설계함. 초기 버전은 TypeScript였지만, 나중엔 성능과 안정성을 위해 Rust로 포팅함. Figma처럼 유동적인 UI가 핵심인 제품에선 '조기 최적화'가 아니라 '특징에 몰입'하는 전략이 필수임을 상기시켜주는 사례라는 점을 강조함. 방대한 기능 목록으로 승부하는 경쟁사 사이에서 이런 성공적인 사업성과를 만들어낸 비즈니스 역량도
       대단하다는 감상임. 이 팀을 진심으로 존경하며, 우리 모두가 더 야심찬 프로젝트에 도전해볼 용기를 얻을 만한 사례라고 생각함
          + 과거 2017년 즈음 Figma의 첫 번째 엔터프라이즈 고객이었음. 시스템의 위대함은 실시간 협업이 가능해졌다는 데 있었음. 전에 기업 디자인 조직들이 Sketch, Zeplin, Invision, Avocode 등으로 버전 관리를 하던 복잡한 '스택'에서 우아하게 탈출할 수 있는 기회였음. Adobe가 이전에 Fireworks를 없앤 후, Photoshop/Illustrator만 기대하던 상황에 비해 엄청난 도약이었음. Figma 덕분에 업무 인계가 쉽게 되고 버전 관리도 매우 단순화됨. UX 리더로서 내 삶이 훨씬 좋아진 경험임. 당시 이제는 거대해진 몇몇 회사와 변화의 전환점을 함께 논의하기도 했던 기억이 있음. Sketch는 Mac OS 네이티브 쉐이프 렌더링 때문에 사용감이 매우 부드러웠지만, 약간의 민첩성 저하를 감수할 만큼 Figma의 이점이 훨씬 컸음. “Sketch가 Mac 전용이라 실패한 거다”라고 주장할 분들에게 말하자면, 전혀
            아니라고 단언함. Axure를 한 세대의 UX/UI 디자이너들이 더 이상 쓰지 않는 이유와도 같음. 자세히 말하려면 Invision 7, Invision Studio 얘기도 같이 나와야 함
          + 예전 Figma 엔지니어로서, Evan Wallace는 진짜 전설적인 개발자임을 인정하고 싶음. 무려 100배 생산성을 내는 ‘100x-er’임. Evan이 예전에 작성한 코드베이스 중 팀 내에서 지금도 제대로 이해 못하는 부분이 존재함. 예를 들어 내부에서 폰트 글리프 렌더링에 쓰는 셰이더를 Evan이 만들어 뒀는데, 그 이후로 아무도 손대지 못했다고 함. 해당 영역에서 수년간 일해온 엔지니어도 며칠간 들여다보다가 결국 포기했다는 일화를 들은 적 있음
          + Evan Wallace는 남들이 기존 라이브러리를 주워다 붙이던 시절, 아예 커스텀 WebGL 렌더러와 멀티플레이어 프로토콜을 직접 만들겠다고 마음먹은 인물임. 대부분 개발자들이었으면 허접한 Electron 앱을 만들어 급하게 출시했을 것임. 그러나 이 팀은 퍼포먼스를 제품 차별성의 근본으로 삼고 진짜 수준 높은 최적화로 승부함
          + “렌더링 엔진이 곡선, 이미지, 블러, 마스킹, 블렌딩, 오퍼시티 그룹 등 고화질 시각 렌더링에 최적화되어 있음”이라는 설명을 보며, 그래서 디자이너들이 “Figma에선 멋지게 보이는데 실제 웹페이지에서는 그렇지 않다”고 아쉬워하는 이유가 거기 있음
          + 제품 자체는 좋아하지만, 만약 내가 IPO에 참여한다면 순이익이 살짝 걱정됨. 2023년에는 실패한 인수합병 덕분에 10억 달러를 받은 후 순이익 2.8억 달러를 기록했지만, 2024년에는 RSU/스톡어워드 영향으로 7.3억 달러 순손실을 입음. 회사 매출은 2023년에 5억 달러, 2024년에 7.5억 달러였다는 점에서 재무적으로는 조심스러움이 필요함
     * S-1 문서에서 밝힌 “2025년 5월 31일에 제3자 클라우드 제공업체와 클라우드 호스팅 계약을 갱신함. 취소 불가 조건 하에 5년간 최소 5억 4500만 달러를 지불해야 함. 이 숫자는 기존 계약을 대체함”이라는 내용 언급함. 즉, AWS 사용료가 하루 30만 달러에 달함. “취소 불가” 덕분에 실제 절감 효과가 얼마나 될지 궁금함
     * Figma 주주 구조에서 “대통령 종신제 조항” 비슷한 게 있음. “이번 IPO 직후, Dylan Field(이사회 의장 겸 CEO)가 Wallace Proxy의 지분 포함 대다수의 의결권을 갖게 됨. 그 결과 주주 결의, 이사 선임, 기업 인수&합병 등 주요 사안에 대해 사실상 종신 리더십 체계로 회사를 운영할 수 있음”이라는 내용 확인
          + 오히려 이 점이 투자 성향에는 긍정적으로 느껴짐. 대부분의 상장기업은 단기적 이익만 좇다가 장기적 비전과 충돌하는 경우가 많음. 창업자가 확실하게 주도권을 쥐고 있으면 장기적 관점에서 기업을 운영할 수 있기 때문임
     * 이 팀에 진심으로 축하함. “내 아이디어는 이미 있는 것 같은데…” 또는 “이미 해결책이 여럿인데…” 고민하는 모든 이들에게 좋은 교훈이 됨
          + 실제로 그들이 이뤄낸 것을 과소평가해선 안 됨. 이 정도의 기술과 엔터프라이즈 앱 시장에서의 퍼스트무버, 그리고 이를 바탕으로 거대한 사업을 만든 것은 매우 인상적임
     * Adobe의 실망스러운 행보 이후 여기까지 왔음. Adobe가 Figma를 없애버리지 않은 게 천만다행이라는 생각임. 팀에 축하를 전함
          + Adobe 인수 실패로 Figma가 10억 달러를 위약금으로 받았다는 사실 언급
     * Linear처럼 기존 강자들이 있는 시장을 단순함, 일관된 디자인, 극한의 엔지니어링 퍼포먼스를 내세우면 충분히 흔들 수 있다는 점이 떠오름
     * 이 시점에서 Figma의 IPO는 자신들이 지금 최고의 위치에 있다는 확신, 그리고 성장 정체 전에 현금을 확보해야 한다는 전략적 판단으로 보임. 이미지 편집 AI 붐 타고 유입된 신규 사용자를 이미 대부분 흡수한 상황일 수 있음
     * 주요 재무지표 공유함:

         유형    2023    2024    전년대비
       매출     $505M   $749M   48%
       매출총이익  $460M   $661M   44%
       영업비용   $534M   $1,539M 118%
       순이익    $738M   $(732)M (199)%
       잉여현금흐름 $1,041M $68M    (93)%
          + 2024년에 영업비용이 급증한 이유가 무엇인지 궁금함
     * Figma는 정말 훌륭한 제품임을 인정하지만, AI 쪽 전개 방식은 마음에 들지 않음. UI/UX 디자이너들이 더 많은 일을 하도록 돕는 데 집중해야 하며, Dreamweaver 업그레이드 같은 방향성엔 아쉬움이 있음
"
"https://news.hada.io/topic?id=21702","Show GN: 집중 향상용으로 문장 단위 하이라이트 효과를 주는 크롬 확장 프로그램을 개발했습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Show GN: 집중 향상용으로 문장 단위 하이라이트 효과를 주는 크롬 확장 프로그램을 개발했습니다.

   웹페이지 읽기 편하도록 문장 단위 하이라이트, 자동 스크롤을 해주는 크롬 확장 프로그램을 개발했습니다. 시연 이미지는 GitHub에 있습니다.

   [Features]
     * 문장 단위 하이라이트
          + 밑줄, 윤곽선, 형광펜, 스포트라이트 등 다양한 효과.
          + 마우스 클릭으로 원하는 문장 포커스 가능.
          + 키보드 화살표로 이전 문장, 다음 문장으로 포커스 이동 가능.
     * 자동 스크롤
          + 포커스된 문장으로 자동 스크롤 이동.
          + 중첩 스크롤 컨테이너에서도 정확하게 동작.
          + 스크롤 후 문장이 화면 내 놓이게 할 높이를 사용자가 설정 가능.
     * 범용성
          + Google, GitHub, ChatGPT, Notion, Youtube, GeekNews, HackerNews, BOJ, Naver, LinkedIn, Tistory, Velog, SamsungSDS 등 다양한 환경에서 잘 동작하는 모습 확인.
          + 현재까지 실시간으로 DOM 트리가 변하는 경우를 제외한 모든 웹페이지에서 동작 성공.


   [개발 배경]

   면접 준비를 하면서 기업 관련 뉴스를 많이 읽어야 했는데 간혹 집중이 산만해져 읽기 속도가 아주 느려질 때가 있었습니다. 어떻게든 집중력을 유지하려고 마우스로 한 문장 읽을 때마다 첫 번째 글자를 드래그해 가며 읽고 있는 문장이 어딘지 계속 표시했습니다.

   하지만 문장을 읽을 때마다 매번 마우스를 조작해야 하니 손목이 아팠습니다. 또한 책상에 발을 올리고 의자를 뒤로 젖힌 편한 자세로는 마우스를 정확하게 움직이는 것이 매우 어려웠습니다.

   문장 단위로 읽고 있는 위치를 표시할 수 있는 크롬 확장프로그램을 검색해 보았지만, 인기 항목 중에는 그런 것이 없었습니다. 웹스토어를 계속 뒤지다 보니 딱 두 개 찾아내긴 했지만, 많은 웹페이지에서 동작이 되지 않았고, 웹페이지를 변형시켰고, 문장 단위 구분이 정확하지 않았으며, 하이라이트 스타일을 원하는 대로 조정할 수 없다는 점이 마음에 들지 않았습니다.

   그래서 혼자 써볼 심산으로 직접 만들어보기 시작했는데, 이후로 개선을 이어오다 보니 꽤 괜찮은 결과물이 나왔고, 누군가 유용하게 쓸 수 있지 않을까 하여 웹스토어에까지 올려봤습니다.


   [개발 과정]

   핵심인 문장 분리 로직에는 DOM 트리를 전위순회하며 텍스트 조각들로부터 문장의 시작과 끝 위치를 찾아두는 방식을 적용했습니다. “대충 온점이랑 물음표로 문장 사이 구분만 하면 끝나겠지?”라고 안일하게 생각했지만 <a>, <span> 등의 태그가 문장 사이에 끼어있으면 하나의 텍스트 노드에 문장 전체가 담기지 않아 알고리즘을 꽤 복잡하게 구성해야 했습니다.

   자동 스크롤 기능은 원래 계획에 없었는데, 문장 분리 로직을 완성하고 나니 이번엔 스크롤을 내리기 위해 또다시 마우스를 잡아야 한다는 사실을 깨달으면서 구현을 시작했습니다. 자동 스크롤 기능은 대충 scrollTo 메서드 가져다 쓰면 바로 되겠거니 했는데 ChatGPT를 포함한 여러 웹페이지에서 단순 scrollTo만으로는 동작하지 않는 경우가 꽤 많았습니다. 또한 scrollTo는 문장이 아닌 노드가 단위로 사용되기 때문에, 하나의 텍스트 노드 내에 여러 문장이 있을 경우 다음 문장으로 포커스를 이동해도 스크롤이 움직이지 않는 문제를 해결해야 했습니다.


   [피드백 구걸]

   이런 구걸이 적절치 않다는 것을 알지만, 혹시 흥미가 생겨 한 번 써보셨다면 피드백 좀 부탁드리겠습니다. 지금까지 여러 곳에 홍보를 해온 덕인지 사용자 수는 조금씩 늘고 있지만 아직까지 리뷰를 남긴 사람은 단 한 명도 없습니다.

   무플이 악플보다 무섭단 것을 몸소 느끼고 있는 저에게 비평, 개선 방안, 버그 제보, 그 외 아주 사소한 것이라도 괜찮으니 의견을 남겨주신다면 정말 감사드리겠습니다.


   GitHub: https://github.com/hamsteak1488/focus-anchor

   Chrome Webstore: https://chromewebstore.google.com/detail/focus-anchor/…

   너무 좋은 플러그인입니다!
   마우스로 텍스트 선택하면서 편인데, 굉장히 유용하네요!

   vim key 바인딩도 지원되면 좋을거 같아요!
   그리고 혹시 파이어폭스도 지원 예정이신가요?

   현재 단축키 지정 옵션을 구현중이고 다음 버전에 추가할 예정입니다!

   파이어폭스 지원 또한 코드 리팩토링을 마치는대로 바로 진행해보겠습니다.

   정말 심플하고 좋은 기능입니다!
   회사에 설치해서 두고두고 써야겠네요

   아쉬운 점은 shortcut이 없다는 점 정도입니다.
   익스텐션이 많아지다보니 상단에 고정된게 너무 많아서 단축키가 있는것도 좋겠어요

   의견 감사합니다! 단축키 기능은 바로 다음 버전에 추가될 수 있도록 개발해보겠습니다.
"
"https://news.hada.io/topic?id=21798","주요 미국 기후 보고서를 호스팅하던 웹사이트가 폐쇄됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     주요 미국 기후 보고서를 호스팅하던 웹사이트가 폐쇄됨

     * 미국 기후 평가 보고서를 제공하던 여러 공식 웹사이트가 갑작스럽게 사라짐
     * 과학자들은 이 평가 보고서가 정책 및 공공 안전에 매우 중요한 근거임을 강조함
     * 백악관은 관련 정보를 NASA에 저장하겠다고 언급했으나, 구체적 접근 방법 등 추가 설명은 없음
     * 과거 보고서는 일부 NOAA 라이브러리 등에서 접근 가능하지만, NASA의 공식 저장소는 링크가 작동하지 않는 상태임
     * 전문가들은 이번 사태가 과학 인프라의 심각한 훼손이며, 국민의 정보 접근성과 안전을 위협한다고 우려 표명함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 미국 기후 보고서 사이트 폐쇄 상황

   미국에서 법적으로 의무화된 국립 기후 평가 보고서를 제공하던 웹사이트들이 최근 갑작스럽게 전면 폐쇄됨. 이로 인해 각 주 정부, 지방자치단체, 일반 국민 모두가 기후 변화로 인해 본인 지역에 어떤 영향이 있을지 쉽게 알기 힘든 상황이 발생함

웹사이트 폐쇄와 정부 대응

     * 미국 국가별 기후 평가와 U.S. Global Change Research Program 등의 사이트가 7월 1일과 2일에 전면 닫힘
     * 사이트에 대체 경로나 안내 메시지도 제공되지 않음
     * 백악관 측은 법률 준수를 위해 관련 정보가 NASA 내에 보관될 예정이라고 밝혔지만, 구체적 세부사항은 제공하지 않음
     * NASA와 NOAA(국립해양대기청)에서도 관련 정보 제공 요청이나 문의에 답변이 없었음

전문가 의견 및 문제점

     * University of Arizona의 Kathy Jacobs 교수(2014년 평가 조정자)는 National Climate Assessment가 미국에서 가장 신뢰할 만한 기후 정보원이자 의사결정에 필수임을 강조함
     * 이런 공개된 정보에 대한 접근 제한이 팩트 왜곡 및 위험 증가로 이어질 수 있음을 지적함
     * John Holdren(오바마 전 대통령 과학 자문) 역시 과거 이 보고서가 주지사, 시장, 공공 정책 담당자들이 인프라 계획, 병원 발전기 위치 선정 등 실제 중요한 결정을 내리는 데 직접적으로 도움이 됐다고 언급함
     * Texas Tech의 Katharine Hayhoe 등은 이 보고서가 세금으로 제작된 공공 자료로서 모든 수준의 정부기관이 참고하는 핵심 정보임을 강조함

과거 보고서 및 데이터 접근 현황

     * 일부 과거 보고서는 NOAA의 라이브러리 등에서 수집 가능
     * NASA의 open science data repository에서도 예전 사이트로 연결되는 링크가 있지만, 대부분 죽은 링크(404 등 오류) 상태임

과학적, 정책적 함의

     * 2023년에 발행된 최신 보고서에는 인터랙티브 지도 등 현대적인 기능이 포함되어 있어, 지역 단위별로 위험 및 영향 파악이 가능했음
     * 기후 변화가 모든 지역의 안전, 건강, 생계에 다양한 방식으로 영향을 끼치고, 소수인종 및 네이티브 아메리칸 커뮤니티가 특히 불균형적 위험에 처해있음을 지적함
     * 1990년 제정된 Global Change Research Act는 4년마다 공식 평가 제출을 법적으로 요구함

최근 행정 구조 변화 및 추가 조치

     * 트럼프 행정부 시절에 차기 평가 자문 작가의 봉사 계약 종료 및 이익기업과의 웹사이트 운영 계약도 취소됨
     * NOAA의 대표 기후 정보 웹페이지인 climate.gov 자체도 최근 다른 NOAA 사이트로 리디렉션됨
     * NOAA·NASA에서 일반 대중을 대상으로 하는 소셜 미디어, 블로그 등도 축소되거나 삭제됨

전문가 평가 및 사회적 우려

     * John Holdren은 이러한 일련의 과학 인프라 붕괴 현상이 미국 전반에 부정적이고, 시스템적 해체와 다름없음을 강조함
     * Hayhoe와 Jacobs는 미국 기후 평가 보고서가 UN 등 국제 보고서보다 더욱 지역별·상세 정보를 제공하는 것이 장점임을 설명함
     * 보고서 내용은 다른 과학자들의 동료 검토(peer review) 및 National Academy of Sciences, 정부 기관, 대중의 검증까지 거치는 등 신뢰성이 높음
     * 정보 은폐 시도는 과학 검열이자, 기후 변화 대응력 약화를 초래할 수 있음
     * Hayhoe는 “전방을 봐야 안전하게 운전 가능한데, 이 정보 접근이 막히는 것은 앞 유리를 페인트로 덮는 것과 유사하다”고 비유함

결론 및 시사점

     * 이번 사건은 과학 및 정책 정보의 접근성이 국가 인프라와 안전에 얼마나 중요한지, 그리고 그 접근 제한이 실질적 위험으로 이어질 수 있음을 단적으로 보여주는 사례임

        Hacker News 의견

     * 나는 지금 이 시점에서 이런 일이 대량으로 발생하고 있다는 사실이 너무 슬픈 감정임을 표현하고 싶음, 이런 일들이 모이면 엄청난 부정적 영향으로 이어지지만 보통 사람들은 아무것도 모름, 고칠 수 없는 문제라는 감각임
          + 그들이 우리가 고칠 수 없다고 느끼길 원해도 실제로는 그렇지 않음, 정부가 코로나 초창기에 했던 정도로 에너지 전환에 노력했다면 우리는 빠르게 탄소배출을 ‘제로’로 만들 수 있었을 것임, 필요한 기술들은 거의 이미 나와있고, 이제 구현만 남았음, 화석연료 산업과 극우의 연결고리는 명확함, 무관심이나 관성 등은 다 선동과 신판 Cambridge Analytica 방식의 산물임, 화석연료 산업은 자기 이익을 위해 어떤 일도 멈추지 않을 것임
          + 앞으로 수십 년 동안 미국은 점점 영향력을 잃게 될 거라는 생각임, 중요한 일들은 유럽이나 중국으로 옮겨감, 미국을 더 이상 제대로 신뢰할 수 없음
          + 가장 큰 문제는 사람들이 역사를 단일한 사건 혹은 결과로만 본다는 점임, 이런 사건이 현재진행형일 때 누가 문제 제기하면 과하게 걱정한다며 비웃음, 일이 실제로 벌어지면 이제 와서 치유해야 된다고 하고, 아무도 예측 못 했거나 어쩔 수 없었다고 말함, 술 취한 운전자가 모는 차에 갇힌 상황과 비슷함, 문제제기하면 귀찮게 구는 사람 취급만 받고, 실제로는 다 같이 위험에 처하고 외부 사람들도 위협받음, 유일한 희망은 동승자가 운전자를 설득하는 것인데, 현실적으로 일어나지 않으며 오히려 더 난폭해질 뿐임, 결국 체념하거나 저항해야 하는데 둘 다 원인제공자가 아닌 사람들에게 책임이 돌아가는 구조임
     * Don’t Look Up 링크를 공유함
          + 이 영화의 아이디어 자체는 마음에 들었지만, 실질적으로 영화가 잘 만들어지진 않았음, 마치 Mickey 17 같았음, 여러 가지를 시도했지만 결국 어느 것 하나 명확히 잡지 못해 일관성이 없었음, 결말만은 강렬하다고 느낌
          + 그보다 20년 전에는 Inconvenient Truth라는 작품도 있었음
          + 너무 많은 고가 출연진들이 있었던 점이 아쉬움, 배우들은 훌륭하지만 메시지에 집중하기보다는 연예인들에게 눈길이 더 가서 본질을 흐린 느낌임
          + 정부의 정책 웹사이트에 너무 의존하고 상식과 실질적 대응에는 소홀하다는 생각임, 예를 들어 WHO 유럽 지역에서는 매년 17만 5천 명 이상이 폭염으로 사망하지만 미국은 1~2천 명임, 사람들이 에어컨을 피하거나 정부가 이를 어렵게 만들어 생기는 문제임, 왜 이런지 모르겠지만, 그래도 EU는 기후 변화 위험성에 대한 선언과 협약은 화려함, 참고: WHO 발표, JAMA 논문
     * 내가 이토록 절망을 느끼는 가장 큰 이유는, 지금 정부에 있는 의사결정권자에게 “그래도 사람들이 죽게 된다”라고 말해도 그들이 “좋네”라고 반응할 것 같은 느낌임
          + 그 사람들은 사람들의 삶에 신경을 쓰지 않음, 예를 들어 Joni Ernst 상원의원은 정부 예산안이 통과되면 누군가 죽을 거라는 말에 “누구나 결국 죽잖아요”라고 답한 적 있음
          + “정말로 신경 쓰지 않음, 그렇지?”
     * NOAA가 2026 회계연도 예산안을 발표했으며, 우리 조직에서 Copilot으로 요약해 본 결과임
          + 대부분의 기후·기상·해양 연구소와 보조금이 삭제되고 대규모 인력 감축 및 연구 역량 상실
          + 국가 기후 연구 인프라 상실, 인력 축소
          + 지역별 기후 서비스, 적응, 폭염 건강 프로그램 중단
          + 기후 연구 지원금 전면 삭감
          + 기초 해양 관측과 오대호 연구 종료
          + 연안 복원 및 양식업 지원 종료
          + 양식업 연구 및 해양과학 파트너십 중단
          + 무인 시스템 R&D 예산 삭제
          + 기후/해양 모델링 관련 연구컴퓨팅 축소 또는 손실
          + 많은 프로그램에서 운영 업무 집중으로 전환되며, 그 결과 OAR 인력 대폭 감축
          + 해양 관측 시스템 및 연안 연구 중단, 지원금 및 인력 손실
          + 주 정부 해안 관리, 복원, 보호구역 지원금 종료
          + 산호초 보조금, 해양보호구역 지원 축소, 신규 보호구역 지정 없음
          + 종/서식지 연구, 연어 복원, 서식지 복원 프로그램 축소와 대규모 인력 감축
          + 위성·데이터 서비스 축소 및 인력 감축
          + NOAA 교육국 폐쇄 및 임무 지원 인력 축소
          + 전반적으로 대규모 인력 감축과 수많은 프로그램이 사라짐
     * 현 정부가 기후변화 해결에 아무것도 하고 싶지 않다면 그건 선택임, 물론 끔찍한 선택이지만 그들의 권한임
          + 하지만 증거의 심각성을 무시하는 것과 일부러 증거를 은폐하는 건 큰 차이가 있음, 무시는 오만일 뿐이지만 은폐는 제대로 자신들이 그릇되다는 걸 알면서도 도망가는 비겁함임, 정말 강경하게 나오고 싶다면 그냥 보고서 남겨두고 “우린 개의치 않아”라고 하면 됨, 그런데 뒤에서 은밀히 숨기니 정말 한심한 태도임
               o 이 사람들은 아직도 Fauci가 2020년 2월 세계 최악의 은폐 공작에 협조하지 않았다는 사실에 화가 남
               o 만약 보고서가 편향되지 않았다는 믿음이 있다면 위 말이 성립할 것임
     * 미국이 점점 북한처럼 변해가고 있음
          + 어떻게 주체사상이 우리 국가 이념이 됐는지 모르겠지만 자주 떠오르는 생각임, HN 이용자로서 광고산업에도 책임이 있을 것 같음, “made in America”가 지금 이 상태로 이어졌다고 단정짓긴 애매하지만, 여러 가지 요인들이 섞인 결과라고 봄, 광고에서 드러나는 국수주의가 언제쯤 좀 쉬어갈지 기대함
          + 헝가리가 더 적합한 비유라고 생각함, Heritage Foundation이 미국에 적용하기 전에 거기서 먼저 테스트하고 있음, Heritage와 Orban, 트럼프 측근들의 연결이 우려스러움, 트럼프는 이들에게 유용한 얼굴마담에 불과하고 근본적인 원인은 정부불신과 불평등 심화임
     * 지금 상황이 정말 안타깝지만, 중국이나 유럽 여러 나라들이 이런 연구를 계속하고 있다는 점이 작은 위로임, 미국이 점점 부패한 약소국가로 전락하고 있다는 점이 슬픔, 더 큰 일은 이제 국가적 프로젝트는 못 하고 기업·재벌만이 리더가 될 수 있다는 MAGA 당의 신념 자체가 더 비극임
          + “미국은 더 이상 큰일을 못 한다, 오직 기업과 재벌만이 이끄는 나라다”라는 발상, 특히 최근 Musk와 DOGE 사건으로 뼈저리게 드러남
               o Musk가 Twitter를 인수해서 직원 40%를 해고해도 큰일은 생기지 않음
               o Musk가 미국정부를 맡아서 직원 10% 미만만 해고해도 시스템이 무너짐
               o 여기서 우리가 얻을 수 있는 교훈은, 정부 조직이 빅테크 회사들보다 훨씬 더 효율적으로 운영되고 있다는 점임, 빅테크조차 10% 이상씩 감원하는데도 아무 문제 없음
     * 결국 화석연료 산업엔 좋은 기회와 절감이라는 상황임, “이제 미국 핵무기 관리 계획은 뭘까?” 더 우려되는 부분은 억만장자가 핵을 가지게 되는 것임, 직접적으로가 아니라 그 뒤에 있는 세력들이 슈퍼 부국(city-state)을 상상하고 있고, 그 신도시가 핵을 가질 가능성도 충분하다고 봄, 실제 핵 코드란 건 시간과 기술자가 있으면 뚫릴 수 있음, 원래 강한 연결점을 물리적으로 무장하는 방식인데 설계상 결국 전압만 가해도 터질 수 있음, 지금은 전자식이긴 하지만, 실제 ‘폭발’ 부분은 옛날 설계 그대로임, 어설픈 병사들에겐 안전하지만 무제한 접근 권한을 가진 이들에게는 취약함
          + 실제로는 핵폭탄이 그리 쉽게 작동하지 않음, 내부의 폭발을 위한 성형작약 신호들의 아주 정밀한 타이밍이 있어야 함, 이 타이밍은 각 폭탄마다 내부 전선 길이까지 다르고, 코드와 내부 회로에 입력된 시퀀스가 맞아떨어져야 폭발이 됨, 즉, 전류 한 가닥을 흐른다고 핫와이어처럼 작동하지 않음, 실제으론 내부 플루토늄을 뜯어다가 새로 만든 폭탄에 써야 제대로 쓸 수 있음
          + 계획이란 게 실은 없음, 사람들이 그저 없는 걸 두려워할 뿐임
          + 중국과 인도는 핵무기를 다루는 법을 알고 있고, 안전한 관리를 위해 적극적이 될 것으로 봄
     * “Drill baby drill”을 위한 연막전술이 필요함
          + 실제로 “Drill baby drill”은 그냥 말로만 지어낸 구호인데, 지금 현실이 되어버렸다니 속상함
     * “Don't look up!”이라고 외침
"
"https://news.hada.io/topic?id=21761","웹 봇과 봇 탐지 기술의 짧은 역사","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          웹 봇과 봇 탐지 기술의 짧은 역사

     * Web Bot은 HTTP 클라이언트 단순 요청부터 실제 브라우저 자동화까지 발전해왔으며, 이에 맞서 Bot 탐지 기법도 지속적으로 정교해짐
     * IP 평판, TCP/TLS/브라우저 환경 지문, 자바스크립트 기반 행위 분석 등 다양한 기술이 봇 탐지에 활용됨
     * Headless 브라우저, 프록시, User-Agent 변조 등 봇 우회 기법은 발전하지만, 탐지 알고리듬도 함께 진화해 양자간 ‘고양이와 쥐’ 게임이 계속됨
     * 최근에는 행동 데이터 기반 AI 모델을 활용한 고도화된 행동 분석까지 결합, 봇 탐지가 한층 복잡해짐
     * CAPTCHA, 프록시 탐지, Proof-of-Work, 행동 기반 인증 등 다층 방어 체계가 일반화되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

도입: 웹 봇과 탐지 기술의 진화

     * 웹 봇은 단순한 크롤러·자동화 스크립트부터, 실제 사용자처럼 행동하는 고도화된 프로그램까지 다양한 유형 존재
     * 검색 엔진, 아카이브 봇 등 유익한 봇도 있지만, 스팸·불법 스크래핑 등 문제적 활용도 많음
     * 사이트 운영자들은 초창기부터 봇과의 싸움을 이어왔으며, 탐지·우회 기법이 동시에 고도화됨

가장 단순한 봇: HTTP 클라이언트

     * curl, wget 등 단순 HTTP 클라이언트로 사이트에 요청을 보내는 방식이 가장 기본적인 봇임
     * 모든 HTTP 클라이언트는 User-Agent 헤더에 자신을 노출하므로, 사이트는 손쉽게 탐지·차단 가능
     * User-Agent를 브라우저로 위조해도, 브라우저는 추가적인 헤더(언어, 인코딩 등)를 포함하므로, 완벽한 위장이 아니면 여전히 탐지됨

IP 평판 및 프록시

     * 서버는 IP 주소를 활용해 봇을 탐지함. 특히 클라우드·데이터센터 IP 대역은 봇/자동화 트래픽으로 인식되어 신뢰도가 낮음
     * 프록시 없이 운영하면 금방 차단되므로, 주거지/모바일 프록시로 IP를 우회해야 하고, 이를 위해 비용이 듦
     * 사이트는 IP 평판, 프록시 포트(1080 등) 오픈 여부, IP 대역, 접속 패턴 등을 적극적으로 점검함
     * IP 차단을 우회하기 위해 회전 프록시(rotating proxy), 모바일 프록시 등이 활용됨

TCP 지문(TCP Fingerprinting)

     * HTTP 요청 전 TCP 연결을 맺을 때, 운영체제별로 TCP 패킷 구성 방식이 달라 이를 분석해 OS를 식별할 수 있음
     * User-Agent와 실제 OS(TCP 지문)가 불일치할 경우, 봇/위장 트래픽으로 판단됨
     * 프록시 서버도 TCP 지문에 영향을 미칠 수 있어, 프록시 선택시 OS 일치 여부 고려 필요

TLS 지문(TLS Fingerprinting)

     * TLS 핸드셰이크 과정에서 지원 암호화 방식, 버전, 확장 등이 브라우저/OS별로 다름
     * TLS 지문을 통해 브라우저, 운영체제, 라이브러리 종류를 추정하고 User-Agent와 교차 검증 가능

자바스크립트 탐지

     * 서버는 응답 전, 또는 페이지 로드 후 JavaScript로 클라이언트 환경·행동 정보를 추가 수집함
     * 봇이 JavaScript를 실행하지 않으면 곧바로 탐지되며, 봇도 Selenium, Puppeteer, Playwright 등 브라우저 자동화 도구를 이용해 대응함
     * 단순 HTTP 요청 수준에서 브라우저 자동화로 진화 중

Headless 브라우저와 탐지

     * Headless 모드(창 없는 Chrome 등)는 봇 개발에 필수적이나, navigator.webdriver 등 고유 속성, 빈 플러그인 리스트 등 다양한 차이점으로 탐지가 가능함
     * 각종 속성을 패치해 위장할 수 있으나, 수십 개 힌트를 모두 처리해야 하고, 새로운 탐지 포인트가 계속 등장함
     * 2023년부터 도입된 New Headless 모드는 실제 Chrome과 동일 엔진을 사용해 탐지가 더 어려워짐

오케스트레이션 프레임워크 탐지 및 IPC

     * Selenium, Playwright 등 자동화 프레임워크는 고유 플래그·옵션, 브라우저 버전, 환경 구성에서 특이점을 노출함
     * 예시: --disable-ipc-flooding-protection 등 플래그는 봇 환경을 식별하는 단서가 됨
     * 일부 JS 함수(예: window.history.pushState)를 과도하게 호출해 IPC flood 상태 유도 및 탐지 가능

프록시 탐지: JS 기반 고도화

     * Latency(지연 측정): WebSocket 등으로 측정한 전체 지연과 TCP 지연 차이를 비교해 프록시 존재 여부 확인
     * WebRTC Leak: 브라우저의 WebRTC를 이용해 실제 클라이언트 IP를 얻고, HTTP 요청 IP와 비교해 불일치시 프록시/봇 의심
     * DNS Leak: 자바스크립트에서 임의의 서브도메인 요청 → DNS 서버 위치/IP를 통해 비정상적 패턴(국가 불일치 등) 탐지
     * Timezones: 브라우저의 타임존과 IP 위치 비교로 프록시 사용·위장 탐지

캡차(Captcha)와 인증

     * Captcha는 봇 탐지/차단 목적의 별도 인증으로, 사람이 해결 가능한 문제(글자 인식, 클릭 등)로 구성됨
     * 최근에는 Proof-of-Work 기반 캡차(연산 작업 부과), 행동 기반 캡차(단순 클릭, 행동 분석 결합)가 도입됨
     * 대다수 봇은 저가의 외부 캡차 솔버 서비스를 활용해 캡차를 우회함

단순/고급 행동 분석

     * 행동 분석은 마우스 움직임, 키 입력 패턴, 클릭 위치·속도 등 인간 행동 특유의 비효율성과 다양성을 분석함
          + 예시: 마우스 곡선 이동, 클릭 딜레이, 키 입력 간 시간 차, 모바일 기기 orientation/motion 이벤트 등
          + 봇은 직선 이동, 일정/빠른 타이핑, 비현실적 반응 속도 등에서 쉽게 드러남
     * 고급 행동 분석은 대규모 인간·봇 행동 데이터를 수집·학습해, AI/머신러닝으로 미세 패턴까지 식별
          + 예: 마우스 이동 궤적, 키스트로크 간 미묘한 시간 차, 페이지 탐색 패턴 등 복합적 데이터 기반 분류

결론 및 시사점

     * 웹 봇 vs. 탐지 기술은 끊임없는 진화와 대응의 싸움이며, 정적 지문·행동 분석·AI 기반 탐지까지 다양한 기법이 혼합 활용됨
     * 각종 우회·위장 기법에도 불구하고, 서비스 운영자는 다층적 탐지 체계, 실시간 행동 분석, AI 모델 등으로 대응하며 지속적 업그레이드가 필요함
     * 봇 개발자는 완벽한 위장 환경을 구성하는 데 한계가 있으며, 최신 탐지 트렌드와 대응 방법에 대한 이해가 필수적임
"
"https://news.hada.io/topic?id=21699","AI가 우리의 사고를 획일화하고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI가 우리의 사고를 획일화하고 있음

     * MIT 연구에서 ChatGPT를 사용한 집단의 뇌 활동이 상대적으로 낮게 나타남
     * 인공지능 활용은 창조성과 다양성 감소, 결과물의 획일화 현상 초래
     * Cornell 대학 연구에서도 문화·개인 차이의 글쓰기마저 AI 사용 시 서구적, 평균적 경향으로 수렴함이 확인됨
     * AI가 제시하는 평균적이고 안전한 답변이 문화적 다양성과 개성 약화 유발
     * 대중화된 AI가 가져오는 장기적 영향은 아직 충분히 알려지지 않았으나, 사고의 단순화와 획일화가 주요 우려로 대두되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

실험 및 주요 발견

     * 2023년 MIT에서는 보스턴 인근 대학생 50여 명을 세 그룹으로 나누어 SAT 스타일의 에세이 작성 실험을 진행함
          + 한 그룹은 오로지 자신의 두뇌만을 사용
          + 두 번째 그룹은 Google Search 활용 가능
          + 세 번째 그룹은 ChatGPT를 사용해 에세이 작성
     * 모든 참가자는 뇌파 측정용 헤드셋을 착용했으며, 분석 결과 ChatGPT 사용 집단의 뇌 활동이 가장 저조한 것으로 나타남
          + 해당 집단은 뇌의 여러 부위 간 연결성(알파 및 세타 파동) 이 감소함이 관찰됨
          + 일부 사용자는 자신이 작성한 에세이에 대해 소유감 결여를 보였으며, 대다수는 작성한 내용을 인용조차 하지 못함
          + MIT 연구진은 이를 AI 의존에 의한 인지적 비용이라 명명함

AI가 유발하는 사고의 평균화

     * ChatGPT를 활용한 집단의 에세이는 유사한 단어와 아이디어로 수렴하는 경향을 보여줌
          + 넓은 답변을 유도하는 SAT 프롬프트에도 불구하고 AI 사용 집단은 획일적인 논리 및 어휘 사용을 보임
          + 예를 들어, ‘진정한 행복’에 대한 대답은 모두 경력과 성공 중심, ‘기부의 도덕적 의무’엔 일률적 찬성 입장만 제시함
     * MIT의 Kosmyna 연구원의 표현에 따르면 “모든 것이 평균적으로 모아지는” 현상이 발견됨

대중적·문화적 다양성의 약화

     * Cornell 연구진은 미국과 인도의 사용자가 각자 문화적 배경을 반영해 글을 쓰는 실험을 진행함
          + 일부는 ChatGPT 연동 자동완성 사용, 일부는 비사용
          + 자동완성 사용 시 참가자의 글이 서구적, 평균적 경향으로 수렴하며 서로 유사해짐이 확인됨
          + 음식, 명절 관련 질문에서 피자와 크리스마스가 단골 답변, 지역적 차별화는 희박함
          + 에세이의 세부 묘사와 분위기조차 평범한 어구 위주로 변함

AI 도구의 제안 영향 및 정체성 저하

     * 필자는 AI의 제안 기능이 작가의 자기 목소리를 약화시키는 ‘최면 효과’를 갖고 있다고 설명함
          + 지속적 노출로 인해 글쓰기 자신감 및 정체성 상실 가능성 우려
          + AI의 제안은 사용자 생각뿐만 아니라 사고방식 자체까지 평균화하는 역할을 함
          + 결과적으로 “무엇이 정상이고 바람직한지”에 대한 사회적 인식에도 변화 가능성 내포

산업적 동인과 문화적 표준화

     * OpenAI 및 AI 개발사들은 더 많은 이용자가 일반적이고 평균적인 결과물에 만족하도록 시스템을 설계함
          + 누군가에게 수용 가능한 모델 출력을 제공할수록 더 많은 유료 사용자를 얻을 수 있음
          + 효율성 측면에서 ‘모든 것이 동일할 때 규모의 경제’ 실현 가능

AI와 창의성: 실험적 한계

     * OpenAI CEO Sam Altman 등은 AI로 누구나 창작자가 될 수 있다고 주장
     * 하지만 실험에서는 AI 사용 집단이 원본성 측면에서 오히려 평균적이고 비슷한 아이디어를 내는 경향 파악됨
          + Santa Clara 대학 연구에서 ChatGPT와 기존 창의성 도구(Oblique Strategies)를 비교함
          + ChatGPT 사용자는 아이디어가 의미론적 유사성, 즉 획일화 경향을 보임
     * 실험 분석가 Max Kreminski에 따르면 인간의 원초적 사고가 점차 AI의 평균값 쪽으로 끌려감
          + 반복적으로 AI가 “충분히 괜찮은 답변”을 빠르게 내놓다보니 사용자는 자신의 아이디어보다 AI의 결과물 선별 쪽에 몰입
          + 대화가 길어질수록 AI의 컨텍스트 윈도우 한계로 인해 더 반복적·평균적인 답변이 강화됨

대중화된 AI 환경과 정보의 다양성 부족

     * 연구들은 대부분 소규모 실험이지만 AI의 장기적인 영향은 여전히 미지수
     * Meta의 AI 앱 등에서는 수많은 사람이 만든 과도하게 매끄럽고 일률적인 콘텐츠가 대량 생성됨
          + 자동 이메일, 일상적 요청 등에 획일화된 포맷과 어휘가 난무
          + 실제 프롬프트 예시에서도, AI가 긍정적 미래만 나열하며 부정적 시나리오나 위험성은 배제됨
          + 이는 AI 설계 과정의 친기술적 편향성이 결과물의 다양성 감소로 이어질 수 있음을 시사

결론 및 전망

     * 누군가가 AI가 제시한 정보만 믿으려면 사고를 멈추는 일이 필요함
     * AI가 낳는 사고의 평균화 및 창의성 약화 현상에 대한 사회적·문화적 경계 필요성 대두

   인터넷 처음 나왔을 때도 비슷한 이슈가 있었던 거 같긴 합니다. 비판적 사고를 키우는 능력이 더 중요하지 않나 싶어요

   인터넷 검색이나 유튜브로 잘못된 정보를 수용하는 것보단 나을 것 같네요. 특정 집단의 이익을 대변하기 위한 목적으로 편향된 정보를 받아들이는 것보다는 낫다고 생각합니다. AI 도 편향적일 수 있지 않냐고 할 수 있겠지만 인터넷에 떠도는 무작위 정보들의 신뢰도 평균보다는 높다고 생각합니다. 비판적인 생각 없이는 잘못된 정보를 받아들일 수 있다는 점에서도 비슷하고.

        Hacker News 의견

     * 요즘 들어 진정한 ""디지털 격차""는 최근 몇 년 사이에 비판적 사고력을 이미 길러왔는지 여부에 따라 갈린다고 생각함. 다양한 독서와 인내심 있는 성찰을 통해 이런 능력을 쌓은 사람한테는 LLM 같은 도구가 거친 정보의 바다에서 웨이브 러너를 타게 열쇠를 쥐어주는 느낌임. 그런데 지금 막 비판적 사고를 배워야 하는 사람들은 LLM을 습관적으로 찾지 않고서는 성찰의 고민을 이겨내기 어려울 것 같음. 모호함을 스스로 견디는 행위, 즉 정보가 지식으로 전환되는 순간이 여기서는 즉각적으로 해소됨. 2023년 전에 이런 능력을 획득한 게 다행이라는 생각이 들지만, 젊은 세대에게 LLM을 무조건적으로 학습 도구로 제공하는 현실이 좀 두려움
          + AI가 모두를 멍청하게 만든다는 비관론은 사실 대다수 사람들이 원래도 비판적 사고력이 뛰어나지 않다는 현실을 과대평가하는 현상이라고 생각함. 20년 가까이 고급 엔지니어링 분야에서 일하면서 복잡한 개념을 진짜로 이해하는 사람은 accomplished한 전문가들 사이에서도 드물었음. 원래부터 비판적으로 생각하지 않았던 대다수 사람들에게 쉬운 답변이 제공된다고 해서 똑똑한 사람들이 손해를 보는 일은 없을 거라 예상함
          + 어릴 적의 나 자신을 생각해보면, 단기적 재미만을 좇았을 때는 아무것도 배우지 못했던 경험이 있음
          + Idiocracy(저능 사회) 시대가 다가오고 있다는 느낌이 듦
     * 모든 연결 기술은 동질성을 촉진하는 힘이라고 생각함. 예를 들어 TV는 지역 사투리의 소멸을 불러온 대표적인 사례임. 무제한의 재미, 오락, 연결성으로 인해 슬프고 지루하며 외로운 세상을 만들어가고 있음
          + 지역 사투리가 없어졌다는 근거가 궁금함. 미국 여기저기 여행하다 보면 여전히 다양한 지역 사투리를 들을 수 있음. 서부, 중서부, 남부, 동부에 친척이 있는데 각 지역마다 확연한 악센트가 남아 있음. 강도의 차이는 있지만 여전히 뚜렷함
          + 이미 소셜 미디어가 우리의 사고방식을 상당히 동질적으로 만들어놓음. 너무 많은 정보와 관점이 한 번에 들어오니, 스스로 의견을 정립하기보다 남의 생각에 영감을 받아야만 견딜 수 있음. 업보트 버튼이 모두가 동의하는 생각을 손쉽게 정해줌
          + 우리는 어떻게 하면 좋을지 고민임. 오프라인 클럽[https://www.theoffline-club.com/] 같은 진지한 시도가 해독제가 될 수 있을지 궁금함. 이런 논의가 최근 Hacker News에서 돌기도 했음. 이른바 'grass touching' 기술에 대한 고민
          + 그래도 지리적 환경이 제공하는 다양성이 일부 경험을 규정함. 물론 요즘 기술의 목적 중 하나가 이런 지리적 특색을 상쇄시키거나 차단하려는 데 있긴 함
          + AI의 경우에 문제는 바로 누가 동질성을 관리하며, 어떤 목적을 갖고 그 일을 하느냐임. IRC나 메신저처럼 역동적인 시스템은 사람들이 자연스레 연결되고 집단을 형성할 수 있도록 해줌. 하지만 AI는 돈 많은 자본가가 자본을 투입해 만든 폐쇄형 관리 도구라서 결국 비즈니스 모델을 보호하고 위험을 줄이기 위한 동질화가 암묵적으로 이루어짐. 진짜 위협은 바로 현실이 특정 목적에 맞게 '저작' 당하는 현상임
     * 스스로 생각하지 않을 경우에만 자신의 생각이 동질화된다는 입장임. 이건 많은 사람들에게 약점일 수 있겠다는 생각
          + ""직접 조사한다""는 유형의 사람들끼리 역설적으로 가장 심한 집단사고에 빠지곤 한다는 관찰임
          + 비슷한 구조의 공리 체계를 가진 다수의 개인은 유사한 결론을 도출함. 어떤 이들은 자신이 특정 사상 집단에 속하면서도 스스로 주체적으로 생각한다고 믿음. 공동체 구성원들의 생각이 유사해지는 힘이 클 때가 있음. 반면, 어떤 사람은 미지의 밈 공간에서 새로운 전략이나 아이디어를 발굴하는 '보물찾기 전략'을 구사함. 이런 전략은 위험성이 높고 여러 시도를 해볼 수 있는 자원이 필요하다는 한계도 존재
          + ""스스로 생각하지 않으면 동질화된다""는 주장에 공감하면서, 비기술적이거나 기술에 익숙하지 않은 친구나 가족이 ChatGPT를 어떻게 쓰는지 한번쯤 확인해보라고 권장. 실제로 충격적인 사례가 꽤 많음. LLM 도구를 잘 모르는 사람들이 정말로 위험하게 사용하고 있음. 예를 들어, 친구의 매형은 ChatGPT에서 각종 페니주와 잘 알려지지 않은 암호화폐에 대한 엄청난 수익을 보장받는다는 얘기를 듣고 실제 투자까지 함. 어떤 사람은 ChatGPT가 신의 목소리를 전한다고 믿으며 엉뚱한 행동에 나서는 경우도 있음. 상업용 LLM은 충분히 그럴싸하게 작동해서, 이 도구의 작동 원리를 모르는 이들이 현혹되기 쉬움. 본인은 LLM의 결과에서 명백한 오류를 경험했기 때문에 의심하는 태도가 길러졌지만, 객관적 판단이 어려운 이들은 그대로 모든 걸 믿어버릴 위험이 큼
     * MIT Media Lab의 Nataliya Kosmyna가 저술한 연구에서, 보스턴 대학생 50여 명을 세 그룹으로 나눠 실험한 결과 ChatGPT를 사용한 그룹이 다른 집단보다 뇌의 활동, 특히 창의성과 작업 기억과 연관된 알파 및 쎄타 연결성이 현저히 떨어졌다고 함. 그런데 이런 n값이 적은 fMRI 연구가 진짜로 신뢰할 수 있는 과학적 결과인지 의문임. 오히려 확증편향에 의존하는 과시성 결과일 수 있음
     * 요즘 과장된 '파국 예언' 기사들이 사람들의 생각을 너무 통일시키고 있음. 이런 주장에도 분명 일리가 있지만 실제로는 훨씬 미묘한 부분이 많음
     * TV, 소셜 미디어, 인쇄기 등 과거 미디어 기술 변화를 보라며, 유사한 미디어 공포가 반복됐던 역사를 소개함 관련 기사
          + 대중 매체는 우리의 입력(정보 소비)을 동질화시킴. AI가 기존 미디어와 다르다면, 우리의 출력(생산물)까지 직접적으로 동질화시킬 수 있다는 점도 생각해볼 필요가 있음
          + 인쇄술 이전에는 세상 사람들이 80억 개의 서로 다른 대화를 했겠지만, 인쇄술로 인해 다양한 소규모 집단이 점차 커지며 더 넓은 범위에서 똑같은 대화를 나누기 시작함
     * SNS, TV, 할리우드, 그리고 전반적인 대중문화 역시 사회를 동질화시키는 역할을 함
     * 중심극한정리(central limit theorem)는 한결같이 작동한다는 의견임
     * 결국 우리는 동질화된 거인들의 어깨 위에 올라서게 될 운명임
     * 대학에서 인문계 공부할 때 논문 참고해서 내 아이디어로 붙여넣고 연결했던 방식과 지금 AI가 하는 일이 크게 다르지 않다고 생각함. 결국 AI가 자신의 지적 보조기구가 되는지 여부는 각자의 기준과 인식에 달린 문제임
"
"https://news.hada.io/topic?id=21764","YouTube 자동 번역 방지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            YouTube 자동 번역 방지

     * 이 오픈 소스 확장 프로그램은 YouTube에서 자동 번역을 방지함
     * 영상 제목과 설명이 항상 원래 언어로 유지됨
     * 오디오 트랙도 원본 버전으로 기본 설정 제공
     * 자막은 원하는 언어 선택, 없으면 자동으로 비활성화됨
     * 개발자 지원 또는 피드백은 GitHub 이슈 페이지에서 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

YouTube No Translation 확장 프로그램 개요

   이 확장 프로그램은 YouTube에서 자동 번역을 막아줌으로써, 영상 제목과 설명, 오디오 트랙 등이 항상 원본 언어로 제공되도록 함. 원본 감상 경험을 원하는 사용자를 위한 게 주요 장점임. 경쟁하는 여러 확장 프로그램과 비교하여, 간편하게 설정과 사용이 가능하고, 오픈 소스 기반이므로 신뢰도가 높음.

주요 기능 및 장점

     * VIDEO TITLES
          + 동영상 제목이 항상 원래 언어로 표시됨
     * AUDIO TRACKS
          + 오디오 트랙이 무조건 원본 버전으로 출력됨 (Shorts 영상도 포함)
     * DESCRIPTIONS
          + 설명글이 자동으로 번역되지 않고, 원본으로 유지됨
     * SUBTITLES
          + 원하는 자막 언어를 선택할 수 있음
          + 지정 언어가 없을 시, 자막이 자동으로 비활성화됨
          + 유튜브가 자동으로 생성한 자막(자동 생성 자막)은 항상 무시됨

지원 및 추가 안내

     * 개발이 도움이 되었다면, KO-FI를 통해 지원 가능함
     * 개선 요청 또는 버그 리포트는 GitHub 이슈 페이지에서 접수 가능함

        Hacker News 의견

     * Autodubbing 기능은 Google이 만든 기능 중 최악 수준이라고 생각함. 원하는 사람에게만 제공하고, 특정 언어에 대해서는 비활성화할 수 있도록 해주는 선택권을 줘야 함. 내가 이해하지 못하는 언어의 경우 자동 더빙 기본 언어를 직접 선택하게 해주길 원함. 예를 들어 프랑스어에서 영어로 번역하는 쪽이 프랑스어에서 독일어나 스페인어로 번역하는 것보다 정확도가 더 좋을 것임. 그리고 사용하는 목소리는 마치 10년 전 TTS 같은 느낌임
          + 너무 별로이고, 도대체 무슨 일이 벌어지는지 한참 알아채지 못했었음. 어떤 영상들은 그냥 AI로 만든 스팸이라고 착각함. Reddit도 비슷한 기능을 넣었는데, 자동 번역된 글이 내 모국어로 구글 검색에 나옴. 그 언어로 검색할 때 영어 사이트를 일부러 피하는 데도 이런 현상이 나타나서 매우 짜증남
          + 친구가 한 영상을 보고 AI로 만든 건 줄 알았음. 17개 언어로 번역되어 있었기 때문임. 근데 목소리가 자연스럽고, 영상의 흐름이나 감정·강세도 잘 맞았음. 이 경우는 돈 주고 더빙을 맡긴 것이었음. 하지만 YouTube의 자동 번역은 전혀 그렇지 않음. 어색하고 로봇 같은 목소리에 맥락도 없음. 아직 제대로 다듬어지지도 않았고, 창작자의 의도가 전혀 전달이 안 되는 수준이라 오히려 해가 되는 기능임
          + 최소한 특정 언어별로 비활성화할 수 있다면 좋겠다고 생각함. 그보다 더 나아가면 Accept-Language 헤더를 활용하는 것이 상식이라고 봄. 그런데 위치를 독일로 잘못 잡아줘서 문제가 반복적임. 분명 프랑스 ISP를 쓴 지 5년이나 됐는데도 계속 독일로 잡혀 있음. 위치 정보가 아니라 Accept-Language 정보를 쓰면 훨씬 좋을 것임. 그렇다고 해도 이 기능은 정말 별로임
     * YouTube는 다국어 사용자를 대하는 태도가 꽤 냉담하다고 느낌. 아마 번역 기술을 뽐내거나, 원래 언어를 모르는 사용자에게 편의를 제공하려는 의도가 있을 수도 있는데, 솔직히 그냥 관리자가 “내가 구현했다”고 실적 쌓으려 만든 것 아닐까 하는 생각도 듦. 하지만 분명 다수의 사용자는 여러 언어를 구사하고, Google이 “언어를 대신 추측해서 도와주겠다”고 나서지 않아도 됨. 이런 유저의 존재를 정상적으로 인지하고 기능 설계했으면 함
          + 새로운 언어를 배울 때 YouTube에서 언어 학습 영상을 자주 보는데, 그 영상 설명과 타이틀·오디오 트랙까지 전부 모국어로 번역되어서 학습에 쓸모가 없는 경우가 많음. 번역된 설명이나 타이틀은 기본적으로 되돌릴 방법이 없어서 불편함. 게다가 크리에이터 본인도 자신 영상이 자동 더빙된 걸 몰랐던 것 같음. 어떤 언어 학습 Shorts 영상이 영어가 아닌 원어로 올라왔을 때, 유튜브가 자동으로 영어 더빙을 했고, 댓글에 “왜 AI 목소리 이렇게 구려?”라고 남겼더니 크리에이터 본인이 “그거 내 목소리임”이라고 답함
          + i18n(국제화), l10n(현지화)을 바르게 이해한 회사를 나는 한 번도 본 적 없음. 많은 개발자가 “IP 주소가 국적 X이면, 언어도 X”라고 생각함. 하지만 여행객, 장기 거주 외국인, 원어민이면서 다른 언어를 배우고 싶은 사람 등 다양한 케이스가 존재함. 브라우저의 Accept-Language를 X, Y로 설정했더라도, 검색 결과는 X 위주인데 Y에 대해 검색하면 Y로 보여주고, Z 국가의 밴드 검색하면 X 언어로 보여주는 식의 유연함이 필요함. 나 같은 경우 홍콩(zh_hk + en_gb) 출신으로 싱가포르(zh_cn + en) 거주, 일본 JPOP을 팔로우하는 입장에선 브라우저와 매일 싸움임. 모든 앱에 인터페이스 언어와 콘텐츠 언어를 따로 직접 지정할 수 있게 해줬으면 함
          + Google 전체적으로도 브라우저의 언어 설정을 제대로 고려하지 않음. “?hl=en”을 모든 Google URL에 직접 붙여야 원하는 언어로 사용할 수 있음
          + 제품 관리자가 “내가 이런 기능도 설계하고 구현했다”는 실적을 위한 것 같음. 실제로는 {접근성, 번역 기술 뽐내기, UX 개선}이 목표였을 수 있으나, 궁극적으로 광고 매출에 도움이 되고 성공적인 A/B 테스트 수치가 나오면 그게 더 중요하다는 구조임
          + 나는 대부분 영어 콘텐츠만 봐서 세팅을 영어로 다 맞춤. 근데 이런 시스템 자체가 황당하다고 생각함
     * 유튜브의 자동 번역 기능은 진짜 불필요한 기능임. 독일어와 영어 콘텐츠를 즐겨보고, 언어 설정도 영어로 맞춰둠. 하지만 독일어 제목을 영어로 번역하면 고유명사·관용구 등 제대로 번역되지 않아 말도 안 되는 결과물이 됨. Google 계정에서 여러 언어 사용을 지원함에도 내가 사용하는 언어를 반영하지 못함
          + 자동 번역 품질이 괜찮거나 최소한 이해 가능한 수준이라면 어느 정도 용납할 수 있을 것 같음. 하지만 실제로는 그냥 이탈리아어나 프랑스어 단어를 마구 섞어놓은 듯한 수준이라 번역이라고 하기 민망함. 오디오 자동 번역 엔진을 그냥 “Mechanical Italian Brainrot Generator”라고 해도 될 정도임. 실제로 테스트도 제대로 안 해보고 바로 서비스에 적용시킨 느낌임
     * 프리미엄 사용자인데 설정에서 해당 기능(오토더빙)을 아예 끌 수 없는 것도 이해 불가임. 모두에게 opt-in 방식으로 바꾸면 좋겠음. 독일어권 국가에 살지만, 내 모국어는 따로 있고 독일어는 거의 쓰지 않음. 모든 UI를 영어로 맞춰둠. 그런데 영국인이 만든 영상을 보면 자동으로 독일어로 더빙됨. 다른 유명 회사에서 이 정도로 사용성을 해치는 결정은 본 적이 없음. Google이 스위스에도 큰 거점을 뒀음에도 더 의문임
          + 스위스의 경우 사람들은 매우 특수한 스위스 독일어를 사용함. 그런데도 YouTube는 영상을 ‘표준 독일어’로 무작정 더빙함. 과장해서 말하자면 리스본 사람이 영상을 스페인어로 더빙당하는 것과 같음
          + 사용자 취향이나 문화, 언어 선택권을 존중하지 않는 태도가 이상하다고 느낌. 반면 다른 영역에선 다양성을 강조하면서, 이런 기본적인 개인 설정은 위치 기반으로 전부 덮어써버림. 그래서 요즘은 장시간 영상 볼 때는 직접 다운로드함. 추천이나 검색으로 콘텐츠를 찾는 경우보다 이런 방식이 더 익숙해짐. YouTube를 통한 새로운 콘텐츠 발견이 점점 무의미해지는 중임
          + 내 경우 정반대의 경험이 있었음. 보통 미국/영어 콘텐츠만 보는데 독일 유튜브 채널은 1~2개만 팔로우함. 새 독일어 영상이 올라왔는데 평소와 달리 영어로 되어있어서 처음엔 특별한 영상인 줄 알았음. 다른 계정으로 로그인하니 YouTube가 영상을 자동으로 영어로 번역한 것임을 알았음. 아무 예고도 없이 이런 식으로 영상을 건드리는 건 진짜 불편함
          + 언어 감지 기능이 엉망임. 모든 설정이 프랑스어이고, 프랑스에 거주하며, 프랑스인 영상까지 보는데도 YouTube가 영어 오디오 트랙을 제공함. TV로 볼 때 이런 현상이 더 자주 나타남. 경우에 따라서는 인도네시아어 자막까지 뜨기도 함
          + 독일어가 모국어지만 영어 콘텐츠는 원어로 소비하고 싶음. YouTube의 자동 번역 타이틀 문제는 절반 정도가 제대로 된 독일어가 아니고, 그중 절반은 정말 말도 안 될 만큼 부자연스러움. 영어 관용어가 직역되어서 의미 전달 자체도 안 됨
     * YouTube가 동영상에 이미 포함된 자막을 감지하고, 그 위에 틀린 자막을 또 덧씌우는 일을 멈췄으면 함. 엄청난 머신러닝 기술을 가지고 있으니 당장이라도 프로덕션에 도입하길 바람. 오디오 데이터는 이미 분석하고 있고 STT도 하는데, 여기에 비디오→텍스트 기술까지 결합해서 하드코딩된 자막과 비교하면 쉬울 것임. 오디오 자체가 OCR 오류 검출에 큰 도움이 됨
          + Google의 엔지니어들도 이런 기능을 원하고 있겠지만, Google은 이 분야에서 거의 독점적 위치라 강한 재무적 유인이 없다면 실행되지 않을 것임
          + 하드코딩된 자막이 있는 영상일 때, 오디오와 비디오를 같이 분석해서 자막 품질을 높이자는 제안에 동의함. 다만 이런 문제로 인해 큰 비용을 들일 만한 충분한 수요가 있냐는 건 의문임
          + 실제로 Axon에서 비디오 분석 툴을 만들고 있는데, OCR과 오디오 기반 STT 동기화가 요즘 모델로는 정말 쉬워졌음. YouTube는 모든 요소를 이미 가지고 있는데 이런 걸 대규모로 적용하는 데 소극적인 것 같음. 오디오 자막과 하드 코딩 자막만 맞춰줘도 UX가 크게 좋아질 것임
     * 미국식 사고방식의 대표적인 예시로, 모든 사용자가 단일 언어만 쓰고 그게 자기가 사는 나라 언어라는 전제가 문제임
          + 제일 답답한 건 브라우저가 매 요청마다 명확히 언어 설정을 제공하고 있음에도 무시하고, 의미 없는 추측을 반복하는 것임. 내 브라우저는 영어, 그 다음이 프랑스어로 선호 언어가 세팅되어 있음. 내 사용자 프로필도 ""English""로 설정했는데도, 홈페이지에 접근하면 또다시 IP로 언어를 추측해서 내놓음. 정말 답답함
          + 현재 YouTube와 Google의 대표가 인도 출신임은 알고 있음. 그럼에도 Google이 국제화, 현지화 측면에서 내가 써본 대형 테크 기업 중 최악 수준임을 바꿀 수 없음. 날짜 표기, 주의 시작, 시계 형식, 자동 번역, 현지 기반 검색 결과, 언어 감지 등 모든 게 미국 중심적임. 다양성을 강조하던 회사였지만, 실제로 보여준 게 거의 없음
          + YouTube와 Google 개발자는 전 세계에서 모이고, 대표자도 인도 출신이라 당연히 다국어 환경에 익숙할 텐데, 이런 정책을 결정했다는 점이 오히려 더 의문임
          + 또 한 가지 문제는 국가를 IP 기반으로 파악하는데, 로밍하면 완전히 다른 국가로 잡힘. 그러면 실제 위치와 완전 괴리된 언어가 나옴
          + 참고: 관련 토론
     * YouTube는 브라우저 확장 프로그램 없이 쓸만한 경험을 기대할 수 없는 드문 사이트 중 하나임. 심지어 기본 경험을 만들기 위해 설치해야 하는 확장 프로그램 숫자도 압도적임. 무료 사용자나 YouTube Premium 구독자나 똑같이 이런 불편함을 감수해야 함. 대체제가 부족했기 때문에 가능했던 일이라고 생각함
          + 사이트가 이렇게 변질된 이후 오래전에 사용을 멈췄고, 인비디어스(Invidious), yt-dlp, 간단한 쉘 스크립트로만 간혹 접속함. 개발자가 오리지널 언어 트랙을 리스트 맨 마지막에 놓아버리는 식의 이상한 사고방식에서 서비스의 방향성을 알 수 있음. 가장 먼저나와야 할 원어 트랙이 맨 끝에 나오는 것은 매우 비합리적임
     * Yandex 브라우저에도 자동 더빙 기능이 있는데, 유튜브보다 품질이 낫지만 여전히 불편하고 성가심
     * YouTube의 언어 관련 기능은 믿을 수 없을 만큼 망가져 있음. 나는 독일인이고, 독일어 콘텐츠는 독일어로, 영어 콘텐츠는 영어로 보고 싶음. 이런 당연한 요구조차 맞춰주지 못하는 상황임. 예전에는 잘 됐던 것 같은데 왜 바뀐 건지 모르겠음. Chrome에서도 이런 현상이 있는지 궁금함
          + Chrome을 쓴다면, 애초에 이런 문제를 만든 사람들이 관리하는 환경이라 다른 선택지를 알아보기를 추천함. Firefox나 비 Chromium 기반 브라우저로 갈아타는 게 낫다고 봄
          + 그리고 크롬 확장 프로그램 중에 원하는 기능이 있음: YouTube No Translation 확장 프로그램
     * YouTube 모바일 앱에선 영어 오디오가 기계 번역된 일본어 오디오로 자동 대체되고, 데스크탑에선 일본어 오디오가 기계 번역된 영어 오디오로 자동 대체됨. 정말로 믿을 수 없을 정도로 번역 정책이 엉망임
"
"https://news.hada.io/topic?id=21808","줌 미팅에 AI 노트 테이커가 급증하고, 직원들은 회의 참석을 피함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 줌 미팅에 AI 노트 테이커가 급증하고, 직원들은 회의 참석을 피함

     * AI 노트 테이커가 점점 일반화되며, 실제 회의에 참석하는 사람보다 로봇이 더 많은 미팅이 현실이 되고 있음
     * 여러 직장인은 회의에 직접 참여하지 않고 AI 봇에게 녹음·기록·요약을 맡기며, 이는 시간 절약과 동시에 새로운 업무 매너와 개인정보 이슈를 동반함
     * 주요 회의 플랫폼(Zoom, Teams, Google Meet)은 AI 노트 테이킹 기능을 내장하고, Otter.ai, Fathom 등 다양한 서비스가 AI 에이전트를 통한 미팅 캡처를 지원함
     * AI 노트 테이커는 발언은 못하지만, 실제 참석자 대신 기록할 수 있으며, 추후 더 발전된 “디지털 트윈” AI가 완전한 대리 참석·행동도 가능해질 전망
     * 녹음·기록이 상시화되며, 프라이버시·법적 위험, 의미 상실, 회의의 본질적 가치 재고 등 다양한 우려와 변화가 공존함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 노트 테이커의 일상화와 변화

     * 한 회의 참가자는 인간보다 AI 봇이 더 많은 Zoom 미팅을 경험함
          + 일부 AI는 참석자 보조용, 일부는 불참자의 대리 출석으로 활용됨
          + 본인은 “사람과 대화하고 싶다”면서도 때때로 AI를 대신 보낸 적 있다고 고백

     * 팬데믹 이후 원격·화상회의가 일상화되면서 카메라·마이크 끄기, 대리 참석 등 회의 문화가 계속 변하고 있음
     * Zoom, Teams, Google Meet 등은 AI 기반 기록/요약 기능을 기본 제공, Otter.ai 등 다양한 앱도 AI 미팅 에이전트로 활용됨
     * 최근 ChatGPT도 미팅 노트 기능(record mode) 을 추가

AI 대리 참석이 바꾸는 인간관계와 사회적 규범

     * AI가 참석자 대신 경험·기록을 맡는 것은 모든 것이 기록되는 사회로의 자연스러운 진화임
     * Open Machine CEO Allie K. Miller는 항상 녹음되는 환경이 인간 행동을 바꾸고 있다고 지적
          + ""회의 마지막 5분엔 AI 노트 테이커를 끄라""는 조언, 이때 사람들은 더 솔직해지고 실제 논의가 이루어진다고 함
          + 모든 발언이 언제든 기록·공유될 수 있음을 항상 유념해야 한다고 강조

     * AI 노트 테이커만 참석한 Zoom 미팅 등, 실제 사람이 없는 회의 경험도 점점 보편화
     * 변호사 Joshua Weaver는 AI 기록 도구가 법적 위험(도청, 데이터 프라이버시, 보안 등)을 높인다고 지적
          + 미국은 대부분 한쪽 동의만으로 녹음이 가능하지만, 일부 주(캘리포니아 등)는 모든 참석자의 동의를 요구함
          + 일부 노트 테이커는 회의 참가자 리스트에 드러나지 않고 비가시적으로 오디오만 수집하기도 함

정보 과잉과 회의 본질의 고민

     * 영국의 전략 컨설턴트 Liz Henderson은 AI 노트 테이커의 기록이 대량으로 남고, 프라이버시 관리가 어렵다고 우려
          + 자신의 대화 내용이 대중 앞에 공개되는 경험을 하며 불안감 표출
          + 유럽 개인정보 보호법(GDPR) 준수 문제, 데이터 삭제 권리 등도 강조

     * CareYaya CEO Neal Shah는 회의 요약·분석이 어려울 만큼 기록이 쌓이면, 정보의 의미가 상실된다고 지적

     * 많은 AI 노트 테이커가 있는 회의는 참석자의 “불성실” 신호로 비칠 수 있음
          + 교장은 “중요한 결정이 필요한 회의는 모두 직접 참석해 의견을 내야 한다”고 강조

     * 최종적으로, 회의 주최자는 진짜 회의가 필요한지, 메일·메모로 충분한지를 다시 고민하는 계기가 되고 있음

   회의의 본질에 관해 포럼을 열어보는 건 어떨까 합니다. 포럼에서 윤리적인 문제를 검토하고, 실용적(이를 위기로 받아들일 것인가, 기회로 활용할 것인가, 어떻게 활용할 것인가) 접근법도 검토해보고, 정보보안적 측면에선 어떤지 궁금하고, 디자이너들에겐 레퍼런스를 제시할 때 실시간 연동이 가능해서 단어만 뱉어도 그 아이디어가 비주얼화되어 순식간에 회의참가자들에게 공유가 된다면 디자이너 업계의 DX/AX 완성체가 되지 않을까 합니다...개인적으로는 긍정적인 현상 같지만, 보안 측면에선 이를 어떻게 대비할 것인가가 아주 화두일 것 같습니다.

   상호 작용이 필요 없는 일방적인 정보 전달의 회의를 안 하면 되지 싶네요.

        Hacker News 의견

     * 아카이브 링크
     * 이런 회의들은 정보 밀도가 너무 낮아서 AI가 요약해줘도 내 시간 값어치가 전혀 없는 느낌. 이게 엘리트주의라기보다는, 경중간 관리자들이 자기 존재감을 채우려고 자꾸 회의를 여는 경향 때문인 것. 직접 말로 지시하면 권위가 강해지고, 문서로 남기면 구멍이 보이니까 싫어하는 것. 나는 대부분의 회의가 엉성한 글쓰기 능력을 보완하려고 생긴 거라고 생각함
          + 회의가 정보 전달 목적이 아니라서 ‘정보 밀도’가 낮은 것. 본질은 관계 관리임. 이런 분위기를 암묵적으로라도 느끼지 못하면 관리자가 되기 힘듦. 실제 논의 주제는 부차적임. 서열 확인, 충성심이나 반란 기운 감지, 유대감 강화나 충돌 봉합의 장으로 쓰임. 그래서 실제 일 하는 사람들은 회의를 싫어하지만 커리어 중심인 사람들은 좋아함. 과장되게 말하긴 했지만 명확하게 하려고 그랬음. 그리고 이보다 더 심각한 상황도 있음. 진짜로 심심해서 회의 거는 관리자도 있음. 이들은 회의하러 차에 타기도 전에 누군가와 전화로 ‘간단 업데이트’ 하려고 함
          + “대부분 회의는 엉성한 글쓰기 때문에 생긴다”가 전부는 아님. 어떤 사람들은, 직접 질문받기 전까지는 아무 말도 안 하는 성향이 있음. 회의 중 상태를 물어봐야만 ‘나는 X에서 막혔어요’ 하고, 이미 이틀째 막혀 있었던 일도 있음. 왜 그냥 도움을 청하지 않았냐고 하면, 주저했다는 대답. 회의가 이런 사람들에게 입을 열게 만드는 환경도 됨
          + 예전에는 회사 회의가 정보 밀도가 낮다고 생각했음. 근데 유치원에서 학부모 모임을 처음 경험하고 생각이 바뀌었음. 2시간 30분 동안 쓸데없는 불평만 가득, 의미 있는 결정이나 정보 없음. 지금은 학부모 파티 준비하면서도 간단히 끝낼 일인데 며칠째 스카이프, 왓츠앱, 소란만 가득함
          + 글로 남기는 걸 잘하는 임원과 전혀 안 남기는 임원 사이에는 정말 엄청난 차이가 있음. 놀랍게도 메모조차 거의 안 남기는 사람들이 디폴트로 굴러가는 경우가 많음. 내 경험상 이런 임원들이 독성 조직행동(유독성 문화)과 밀접하게 연결되어 있음. 왜 이런 스타일을 허용하는 상위 경영진이 많은지 알 수 없음
          + 나도 쓸모 없는 회의 많이 겪었음. 운 좋게 대부분의 직장에서 그런 회의를 많이 줄였음. 단 한 군데만 예외였고 형편없었음. 이제 AI 노트 테이커 때문에 좋은 회의마저도 쓸모 없는 시간 낭비가 됨. 이 회의에 AI 노트봇이 들어오면, 사람들은 미팅 중에 적극적으로 질문하지 않고, 정작 3시간 뒤에 질문을 다시 해옴. 모두가 ‘이 회의는 이메일로도 충분했잖아’란 말은 아는데, ‘반대로 3일간 수십 번 주고받으며 시간만 뺏는 메일 교환은 30분 회의 한 번에 끝날 수도 있었는데’라는 걸 인식하는 사람은 드묾. 본질적 원인은 개인이 자신의 일을 비동기화하려 든 결과, 모든 다른 사람들에게 그에 맞추라고 강요하는 문화임
     * 경영진 중에 분명히 이런 현상 보고 ‘직원들이 회의 안 나오고 대충 논다(마리오카트나 하고 있는 거 아냐?)’고 여기는 사람들이 있을 것 같음. 실제로는 회의 참석 평가까지 받으면서, 그 회의시간까지 빼앗기면 정작 ‘말도 안 되는 일정’까지 맞춰야 해서 시간에 쪼들리는 경우가 더 많음. 결국 정말 중요한 일 하려고 시간 버는 셈인데, 회의도 빠지고, 마감도 맞추라는 이상한 상황임
          + 관리자도 이런 모순을 직접 경험해 봤기 때문에 대부분 잘못 관리된 회의가 많다는 데 매우 동의함. 근데 회의에 참석 안 한 엔지니어들이 한 달 뒤에 와서 ‘중요한 제품 결정이 어떻게 내려졌냐’고 물으면, 회의 안 나와서 그런 거라고 말하기 애매한 상황 있음
          + 종종 회의실에서 많은 사람들이 노트북만 보고 있고, 미팅 내내 거의 발언도 없었던 모습 자주 목격함. 사실상 각자 자기 일 하면서 눈만 맞춘 셈. 실효성은 잘 모르겠음. 지금은 신경 안 씀. 사무실에 있으면 100% 근무 중인 척하면서 그냥 휴가 계획 검색함. 재택일 땐 부엌에서 식사 준비하면서 노트북 킴. 인생 낭비 감정
          + 내가 아는 고위 리더들은 오히려 성과 중심이고, 회의 참여율 강요는 중간 관리자의 퍼포먼스 쇼라는 걸 인지함. 실적만 내면 직원이든 관리자든 서로 윈윈이라 여김
          + 이건 회의가 실제로 참석자에게 무슨 가치를 주는지 의문 케이스가 훨씬 많다는 방증이라고 생각. 실제로 주최자가 불필요하게 시간 맞추려고 수다 떨고 시간 끄는 경우 자주 봄. 그냥 일정 채우려고 하는 사람들도 있음
          + 자리 출근 강박에 집착했던 임원 혹은 관리자들이 이런 회의에 집착하는 부류랑 똑같음
     * 우리 회사는 인터뷰에도 AI 노트테이커를 쓰기 시작했음. 솔직히 별로 신뢰 안 했지만 사용할 수밖에 없었음. 놀란 점은 너무 많은 내용을 기록한다는 것. 수백 개의 불릿포인트가 쏟아지고, 결국 다시 정독하는 게 더 피곤함. 게다가 후보자 발음이나 오디오 이슈 때문에 틀리는 내용도 많음. 본질적으로 기록 입력은 사라졌지만, 나만의 인상 정리는 결국 직접 다시 써야 함. 실질적 가치는 거의 못 느낌. 그냥 실용성 없는 신기함 느낌만 남음
          + 인간이 하는 일을 흉내 내지만 기계적으로 중언부언 늘려놓기만 하는 느낌임. 핵심만 뽑아내서 불필요한 걸 거르는 인간 특유의 감이 없음
     * 여기서 다들 ‘글쓰기 > 말하기’라고 말하는데, 중요한 사실은 인구의 80% 이상, 아마 소프트웨어 엔지니어 상당수도 읽고 쓰는 걸 진심으로 싫어함. 독서는 숙제처럼 여기고, 대학 다닐 때도 마지막 책은 요약본만 봤던 사람이 많음. 글쓰기는 단순히 채워야 할 글자 수일 뿐 소통 수단으로 생각하지 않음. 문장 나열 보면 내용은 신경 안 쓰고 그냥 끝나기만 기다림. 그게 현실임. 그래서 chatGPT가 불릿포인트로 요약하게 훈련되고, PowerPoint에 사람들이 집착하는 것임. 문단 하나만 봐도 겁먹는 사람이 정말 많음. 이게 동료라서, 결국 소통법을 맞춰야 한다는 점
          + 좀 더 관대하게 보자면, 사람들이 쓰기·읽기를 싫어한다기보다 회사에서 이미 읽고 쓰기가 과포화 상태임. 이메일, 미팅 노트, 프리젠테이션, 끝없는 메시지, 거기다 코드와 여러 문서까지. 다들 지루하고 질림. 그래서 글쓰기를 더 간결하고 생각하게끔 유도해야 한다는 생각도 있음. AI가 그런 식으로 기여할 수도 있다는 희망
          + 글쓰기는 생각의 정리와 선형적 구조 강제에 쓸모가 있고, 말하기는 좀 덜 다듬어진 아이디어를 자유롭게 탐색할 수 있음. 둘 다 협업에서 서로 필요한 역할임. 나 역시 맥락 없고 참석자만 채우는 쓸데없는 회의에서는 졸 군 경험 많음. 아마 AI 요약도 그럴 듯. 쓸데없는 회의엔 양쪽 다 소용없음
          + 나한테는 듣기와 말하기가 오히려 숙제임. 대부분 사람도 잘 못함. 읽기·쓰기·듣기·말하기 각각 쓸모 있는 맥락이 있다고 생각. 읽기 싫어하는 사람들은 끝마치기만 기다리듯, 대화도 듣기 싫으면 건성인 척, 아무것도 남지 않음. 글은 언제든 다시 참고할 수 있다는 장점 있음. 전화게임처럼 의미 전달이 왜곡될 일도 적음
          + 나는 글쓰기가 말하기보다 좋다는 데 동의하지만, 내가 좋아하는 방향으로. 회의 노트는 내가 직접 적는 게 훨씬 좋음. 이름이나 키워드 틀려서 고생하는 AI보다는, 직접 실수는 책임이라도 질 수 있음. 여럿이 문서 태깅해가며 실시간 기록하면 회의 협업, 맥락, 분위기도 더 잘 남음. AI가 단순 반복작업에 강하겠지만, 나에게는 노트 테이킹이 그리 지루하지 않음
     * 내 경험상 회의의 대부분은 사실상 ‘프레젠테이션’임. 동영상을 생방으로 무조건 보라고 강요하는 분위기임. 사실 애초에 나중에 비동기로 정보만 봐도 되는 내용이지만, 관행 때문에 다 소환해서 박수치고 리액션을 해야 하는 이상한 상황
          + 내가 창업했던 이전 회사에서는 꼭 필요한 소수만 미팅 초대하려 했음. 근데 초대장이 돌면, 다른 사람들이 ‘나 왜 안 끼워줬냐’고 서운해해서, 결국 4명 미팅이 15명으로 불어남. 그 중 다수는 본인과 별 상관없거나 너무 초기 논의라 불필요한데 초대받지 않으면 서운해하고, 와서는 또 회의가 너무 많다고 함. 답답함
          + 단순한 관행만이 아니라, 모두가 ‘같은 정보 받았다’는 공통 상식, 합의 형성도 동기 중 하나
          + 기자가 사례에서 공유한 것처럼, 꼭 프레젠테이션만의 문제는 아님. 심지어 어떤 인터뷰에서는 상대방이 AI 노트테이커만 보내놓고 본인은 아예 인터뷰에 나타나지 않음. 그냥 취소하지, 생각이해불가. 전반적으로 사람들은 쓸모 없는 미팅, 집중하지 않은 인간, 멀티태스킹으로 실질적 내용 없는 참석자를 더 자주 꼬집어야 함. 내가 퍼실리테이터면 집중하지 않거나 몰입도 낮을 때 솔직히 가치 있냐고 묻고, 불필요하면 바로 중단하거나, 더 나은 의제, 비동기 처리, 혹은 아예 하지 않음. 가치 없어지면 시간 낭비하지 않겠다는 것
          + VP가 임원 대상 리허설 프레젠테이션하는 걸 직접 본 적 있음. ‘모든 지표 초록불입니다’ 프레젠테이션했는데, 다음 날 정리해고와 구조조정 발표 있었음
          + 스탠드업 미팅도 결국 돌아가면서 각자 한마디씩 자기 할일만 말하고 논의 없는 미니 프레젠테이션임. 아예 슬랙에 비동기로 상태만 올리거나, 필요 시 친구끼리만 이야기하는 팀이 훨씬 만족도 높음
     * 정말 안 좋은 회의의 전형적인 증상이, 정보 전달 자체가 회의 목적이 될 때임. 사실 미리 정보 공유하고, 회의는 오로지 협업·문제해결용이 돼야 시간 아끼고, AI노트도 굳이 필요 없음. 나의 지론은 ‘의제 없으면 참석 안 함(no agenda, no attenda)’임. 비대면(리모트) 시대엔 진짜 아젠다도 없는 초장시간 연속회의가 너무 빈번해져 비효율 극치
          + 이 원칙(no agenda, no attenda) 3년째 실천 중임. 일부는 적대적으로 받아들이지만, 진짜로 실천하면 회의 몇 건이든 확실히 품질이 오름. 실제로 제대로 하는 팀은 1시간 꽉 채우지 않고, 온전히 내용 집중, 끝나면 바로 핵심 노트 요약 등 효율 만점
          + 저렇게 연속적인 큰 회의, 목적·계획 없는 시간 낭비는 20년 전 오피스 시대에도 만연했던 일임
          + no agenda, no attenda 이 표현이 정말 좋음
          + 이런 저질 회의, 오히려 예전 오프라인 시대가 지금보다 더 심했음
     * 완전 악몽. 일주일 내내 정의도 불분명한 회의 난무, 다들 자기 보신용임. 이젠 심지어 참석도 안 하고 AI로 요약 때움. 실제 프롬프트 쓰자면 “나한테 영향 주면 말해줘, 내가 뭔가 해야 보신에 도움 되면 알려줘” 이런 식일 것임. 내 원칙은 명확함. 회의는 의사결정 용도, 관련된 결정 당사자만 참석. 의제와 요점 사전 공유, 오로지 결정에만 시간 투입. 10분 걸리면 회의도 10분임. 잡담, 친목, 발표, 보고, 브레인스토밍 등은 따로 명확하게 라벨링해서 진행. 행사와 참석 역시 명확한 이유 필요. 이런 룰이 잘 굴러감. 쓸데없는 이메일 CC도 원천 차단, 뭔가 요청·응답 필요 없으면 CC 금지, 방해 없이 일할 시간 보장. 그런데 저런 원칙을 못 지키게 하는 무능 관리층이 있으면 참담함
     * Real Genius 영화 생각남. 학생들이 강의실에 녹음기만 두고, 나중엔 교수까지 녹음 강의만 틀어주는 기괴한 구조. 그 명장면 유튜브 영상
     * 최근 AI 노트테이킹 도구를 본격적으로 활용하면서, 개인적으로 하지만 정말 쓸모 있다고 느낌. 계정만 만들면 로봇이 회의 자동 참석해서 정말 거의 모든 내용을 기록. 다양한 배경과 발음 가진 사람들이 모일 때 특히 효과 만점. 덕분에 수천 번은 살았다는 느낌. 요약이나 회의 전체 기록만 봐도 직접 참여할 때보다 훨씬 효율적임
"
"https://news.hada.io/topic?id=21785","샘 알트먼, Meta의 AI 인재 영입 행보 비판: "선교사가 용병을 이길 것"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              샘 알트먼, Meta의 AI 인재 영입 행보 비판: ""선교사가 용병을 이길 것""

     * OpenAI CEO 샘 알트먼은 메타의 최근 AI 인재 영입 전략에 대해 깊은 문화적 문제를 야기할 수 있다고 경고함
     * 알트먼은 사내 메모에서 OpenAI의 사명 중심 문화와 팀의 독창성을 강조하며, 이직자에 대한 우려보다는 핵심 인재 유출이 없었음을 강조함
     * 메타는 슈퍼인텔리전스 팀 신설을 발표하며, OpenAI 출신 연구자 포함 여러 인재 영입 소식을 알림
     * 알트먼은 ""선교사가 용병을 이긴다"" 며, OpenAI의 미션과 장기적 비전을 재확인하고, 전체 연구 조직의 보상 재검토 계획을 밝힘
     * OpenAI 내부에서도 문화적 차별성과 혁신성을 높이 평가하는 의견이 이어지며, ""OpenAI는 기묘하지만 혁신의 요람""이라는 직원 반응이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenAI와 메타의 AI 인재 경쟁

     * OpenAI CEO 샘 알트먼은 메타의 AI 인재 유치 전략에 대해 강한 비판을 담은 내부 메모를 연구원들에게 전달함
     * 알트먼은 메타의 행보가 심각한 조직 문화 문제로 이어질 것이라 지적하며, OpenAI는 ""코너에 몰려 있던 괴짜 집단에서 이제는 기술 업계에서 가장 흥미로운 인물들이 모인 곳""이 되었음을 강조함
          + OpenAI가 AGI(Artificial General Intelligence) 구축을 희망하는 개발자에게 유일한 선택지임을 강조하며, 전체 연구 조직의 보상 체계도 재검토 중임을 시사
     * 그는 ""AI 트위터는 독성적이고, 메타의 행보는 다소 불쾌하게 느껴진다""며, 자신이 해고됐다가 복귀한 이후 더 놀라운 일이 벌어질 수 있음을 언급함

메타의 슈퍼인텔리전스 팀 신설과 OpenAI 인재 영입

     * 메타 CEO 마크 저커버그는 슈퍼인텔리전스 팀 신설을 공식 발표했고, 이 팀은 Scale AI 출신 Alexandr Wang과 GitHub를 이끌었던 Nat Friedman이 이끌 예정임
     * 새로 합류한 인물 중에는 OpenAI 출신 연구원들이 다수 포함됨: Shengjia Zhao, Shuchao Bi, Jiahui Yu, Hongyu Ren 등임
     * OpenAI의 최고 연구 책임자인 Mark Chen은 ""누군가 우리 집에 침입해 무언가를 훔쳐간 것 같다""는 심경을 전함

알트먼의 내부 메시지와 OpenAI의 입장

     * 알트먼은 ""메타가 좋은 인재 몇 명을 데려간 건 맞지만, 핵심 인재 유출은 없었으며 오히려 아래 리스트까지 내려가서 어렵게 영입한 것""이라고 평가함
     * ""우리는 업계 전반이 사명 중심적이라는 점이 자랑스럽다""며, ""항상 용병이 존재하지만 선교사가 결국 이길 것""이라고 강조함
     * OpenAI는 전체 연구 조직의 보상 체계를 재검토 중이며, ""OpenAI 주식의 상승 가능성이 메타 주식보다 훨씬 크다고 본다""고 언급함
     * ""핵심은 큰 성공 이후 큰 보상이 따라야 한다는 점""이라며, ""메타의 방식은 심각한 문화적 문제로 이어질 수 있다""고 덧붙임

OpenAI의 비전과 조직 문화

     * 알트먼은 ""우리의 연구 로드맵에 어느 때보다 확신이 크다""며, 대규모 컴퓨트 투자를 통한 전례 없는 도전을 진행 중임을 강조함
     * ""우리가 세계에서 가장 특별한 팀과 문화를 갖고 있다고 생각한다""며, ""과거에 미친 성장을 겪었지만, 본질은 제대로 잡았고 문제도 해결할 수 있다고 본다""고 자신감을 표명함
     * 무엇보다 ""우리는 AGI를 선하게 만드는 것을 가장 중요한 목표로 삼는다""고 밝힘
     * ""메타 등 다른 회사는 이 목표를 다른 사명을 위한 수단으로만 본다""며, OpenAI는 끝까지 자신의 길을 간다는 점을 명확히 함

내부 반응과 문화적 자부심

     * Meta에서 근무했던 OpenAI 시니어들은 “Meta는 늘 주력 방향이 바뀜”이라 언급하며 OpenAI는 그렇지 않음을 강조함
     * 또 다른 직원은 “OpenAI는 엉뚱하고 신선한, 마법같은 혁신의 요람임”이라고 조직 문화에 대한 자부심을 드러냄
     * 문화적 다양성과 독특함 역시 OpenAI의 경쟁 우위로 강조

        Hacker News 의견

     * 용병이냐, 선교사냐라는 주제에 대해서 이야기하고 싶음. 많은 고용주는 직원이 마치 집단의 신도처럼 헌신적이길 바라지만, 정작 상황이 어려워지면 그렇게 헌신한 사람들이 가장 먼저 해고당하고 준비도 덜 되어있음. 고용주는 두 가지 모두를 가질 수 없음. 직원으로서도 속지 않아야 함
          + 단순히 월급을 받기 위해 출근하는 것 이상이 일에는 있다고 생각함. 인생의 시기나 직종에 따라 차이가 있지만, 보통 더 적은 연봉과 불확실한 미래에도 불구하고 다른 사람과 다른 무언가를 해보고 싶어서 이직하곤 했음. 일이 정말 좋은 이유는 집중해서 한 방향으로 힘을 모으는 느낌 때문임. 세상을 구하는 건 아닐지라도, 내 기여가 결과로 돌아오는 걸 보면 정말 뿌듯함. '컬트'에 속했다고 농담할 순 있지만, 적당히 돈 더 받으면서 괴로운 일을 하루 종일 하는 것보단 훨씬 나음
          + 직원이 회사에 충성할 이유는 전혀 없음. LLM을 만드는 일은 어떤 종교적 사명이 아니라, 단순히 대용량 데이터에 머신러닝을 적용하는 작업임. 기업은 충성심 있는 인간이 아니라 비합리적이고 불공정할 수 있는 거대한 조직처럼 행동함. 항상 나에게 가장 좋은 길을 선택해야 함
          + 회사가 어떤 방식으로 해고를 진행하는지 보면 회사의 진짜 모습을 알 수 있음. 이익과 경영진의 급여를 먼저 삭감하고, 직원 재배치를 정말 시도했는지, 해고자들에게 충분한 보상을 줬는지 등의 질문에 '아니다'라는 답이 나오면, 그런 회사에 헌신할 이유가 없음
          + 선교사와 용병이란 주제를 다룬 유튜브 영상 참고
          + 이상적으로는 좋은 동료들과 의미있는 일을 하고 있다는 유대감이 중요하지만, 그게 아무런 보장은 아니란 것도 알아야 함. 결국 리더십이 위기의 순간에 몰리면, 우리는 언제든 희생될 수 있는 존재임
     * 세상은 돌고 도는 것임. 올해 3월 기사에서도 언급된 것처럼, Google, Apple, Amazon 같은 빅테크가 인재 쟁탈전을 벌이던 와중에 요즘은 OpenAI가 업계를 주도하는 기업이 되었음. OpenAI는 Google 등 여러 탑티어 기업의 인재를 빼와서 최고의 팀을 만들고 있음
       관련 링크
     * 큰 그림에서 보면, ‘AI’가 거의 완전히 오픈소스(혹은 포스트 오픈소스) 환경에서 커졌다는 건 정말 다행이고 중요한 순간임. 현재 Meta가 이 영역에서 전략상, 의도하지 않게 ‘선한 역할’을 하고 있다는 게 참 재미있음
          + AI가 이렇게까지 성장한 데에는, 대량의 오픈소스 코드 학습 데이터가 있었기 때문임. 결코 우연이 아니라고 봄
          + 더욱 놀라운 점은 요즘 오픈소스 AI 분야에서 중국마저도 당분간 ‘선한 편’으로 보임
          + Meta가 앞으로도 정말로 모델 가중치 공개(완전한 오픈소스는 아니지만)를 계속 유지할지 궁금함. 특히 최근 ‘슈퍼 인텔리전스’ 조직이 생긴 상황에서
          + 완벽하진 않지만, 메이저 플레이어들이 우연히 오픈 생태계를 떠받치고 있는 게 어쩌면 현실적으로 가장나은 시나리오라고 생각함
          + Zuckerberg를 ‘좋은 사람’으로 여기면 안됨. 그는 ‘좋은 사람이라서’ 오픈소스화한 게 아니라, 그냥 컴플리먼트를 상품화한 것임. Meta의 AI 기술이 지금쯤 쓸모없게 됐을 경쟁 환경 탓에 오픈소스를 했을 뿐임
     * OpenAI 직원들이 ‘선교사’ 마인드라고 말할 수 있는지 솔직한 의견이 궁금함. 쿠데타 이후 진정성 있는 사명감을 가진 직원들이 그런 결정을 지지했다고 설득해줬으면 좋겠음
          + CEO가 도덕, 사명 같은 말을 하며 직원과 언론을 휘두르는 건 그냥 ‘기업-CEO’의 흔한 쇼임. 그런 말엔 속지 말아야 함
          + 쿠데타 이야기에는 해석이 많음
              1. Altman이 OpenAI를 AGI 선점 기업으로 만들도록 투자금을 모으려 했고, 그러려면 구조 변화가 필요했던 시나리오
              2. Altman이 돈벌이 기회로 구조 변경을 시도했다는 시나리오
              3. Altman이 실상 똑똑하지도 않고 투자자들에게 휘둘렸다는 시나리오
                 이 중 이사회가 문제 삼은 건 ‘투명성 부족’이었음. 대중에게 불투명함이 거짓말로 들릴 순 있지만, 어디서 무슨 문제였는지에 따라 감정이 달라질 수 있음. 회사의 구조와 사명, 투자자 관리 방식 등이 사명감을 가진 직원들에게 매우 큰 의미였다는 점을 생각하면 좋겠음
                 그런데 나 역시 결국 FAANG에 팔려간 사례임. 절대 일하지 않으리라 맹세했던 곳인데, 시간이 지나니 결국 돈에 팔렸음. 결국 누구나 자기 가격이 존재함
          + 솔직한 답변: Meta가 자기들이 소유해서 배포할 슈퍼인텔리전스를 ‘알고리즘 피드’라는 테크업계의 원죄에 활용한다면, 그게 오히려 OpenAI의 국방 계약보다 100배 더 나쁘다는 생각임(국방 계약엔 자세한 내용은 모름). Meta가 오픈소스화를 추진하는 건 좋지만, 그것만으론 부족함. Meta가 살아남으려면 비즈니스 모델을 피드에서 바꿔야 함. 그런 관점에서라면, 슈퍼인텔리전스를 좇는 선교사 입장이라면 결국 OpenAI 쪽임. (OpenAI가 비영리에서 기업구조로 바뀐 것에 딱히 감정적인 미련 없음)
          + 쿠데타에 대해 잘 알진 못하지만, 결국 직원 대부분이 어느 쪽에 섰는지에 따라 해석이 달라질 수 있음. 직원 개개인에게 철학적 논쟁이었는지, 아니면 단순히 권력 다툼이었는지 확실하진 않음. 어쨌든 이런 논지가 있을 수 있음
          + Altman은 가장 노골적으로 이중적인 CEO임. 왜 사람들이 여전히 그의 말을 곧이곧대로 믿는지 이해 못함
     * Sam Altman vs Mark Zuckerberg, 누구의 편도 들고 싶지 않음. Sam은 언변으로 OpenAI를 ‘선한 세력’처럼 보이게 만들지만, 실상 여타 기업들과 마찬가지로 권력과 돈을 좇는 집단임
          + Sam Altman이 스스로를 거의 신(God)처럼 묘사한다는 이야기까지 있음. 이런 게 라이벌을 비난하는 근거라는 점이 CEO의 자기중심적인 언어임. 나의 세대적 감성일 수 있지만, 이런 건 “OpenAI가 망했다, 도망가라”라는 뜻임
          + 지금까지 둘 중 누가 더 ‘오픈’했는지 따지면 Sam 쪽이 오히려 덜 ‘오픈’했다고 생각함
          + 굳이 둘만 놓고 보자면 Sam 쪽에 한 표임
     * 내가 보기엔, 내 돈 받고 내 일을 하면 ‘선교사’, 남의 돈 받고 내 일을 하면 ‘용병’이라는 식의 구분인 듯함
          + 선교사는 믿음으로 부름받은 사람들이고, 용병은 단순히 돈을 받는 게 아니라 스스로 그 돈을 ‘벌어가는’ 존재임. 왜 두 가지 다 할 수 없는지 궁금함
     * Sam Altman의 유일한 인생 미션은 Zuckerberg만큼이나 개인적으로 부자가 되는 것처럼 보임. 그런 목표가 정말로 OpenAI 직원들에게 끝없는 충성과 미친 워크로드를 불러일으켜야 하는지 의문임
     * OpenAI가 이제 와서 연구원 보상 시스템을 재고하려 하는 건 흥미로움. 불과 몇 주 전만 해도 Jony Ive 영입에 65억 달러를 썼음. 과연 그가 슈퍼인텔리전스를 만들어줄 수 있을지 의문임
          + 인재를 빼가는 회사는 인재 유출을 싫어함. 예전에 Adobe, Apple, Intel, Intuit, Google, Pixar 간에 비밀 불법 채용 금지 협약이 있었던 걸 떠올리게 됨
     * 내가 듣기엔 “직원들이 돈에 집중하지 않길 바라는 CEO가, 그런 직원들 덕분에 본인이 이익을 챙기고 있는 것”처럼 들림. 예상 가능한 일이고, 실제로 Sam Altman 외엔 대부분에게 도움이 전혀 되지 않음. 오히려 해로운 논리임
          + 사실 이 ‘선교사 vs 용병’ 구분은 단순 연봉 억제용 논리가 아니고 수십 년 전부터 이어진 개념임 관련 링크 나는 Sam Altman이 이런 논리를 꺼낸 게 급여를 낮추려는 속셈 때문이라고 생각하진 않음
     * 팀원들에게 너무 높은 연봉을 주면 팀의 핵심이 망가지는 걸 여러 번 목격함. 전 세대를 먹여살릴 정도의 재산과 사랑하는 가족이 생기면 60시간씩 일하기가 정말 어려움
          + 이 주장에 전적으로 동의하지 않음. 60시간씩 일하는 걸 싫어하는 팀이 ‘망가진 팀’이라고 볼 수는 없음. 그건 거의 6일에 하루 10시간이란 건데, 가족이나 자기 관리에 쓸 시간이 별로 없음. 한두 번은 가능하지만, 장기적으로 계속 되면 번아웃 올 가능성이 큼. 게다가 이런 논리라면 연봉 500만달러 이상 받는 임원이나 2천만 달러 이상 모은 임원진도 다 팀을 망친다는 건지, 아니면 관리직이 아닌 사람들만 그런 대우를 받아야 한다는 건지 의문임
          + 압도적인 보상을 받고 세계 최고 선수들로 구성된 스포츠계 등 다른 분야에선 오히려 최고의 팀워크가 형성되는 사례가 많음
          + 만약 생활의 나머지를 챙겨줄 하인이 있다면, 60시간 일하는 게 오히려 더 쉬울 수 있는 아이러니가 존재함
          + 팀에 꼭 60시간씩 일해야 좋은 팀이 되는 건 아님. ‘FU$’ 상태가 되면 굳이 워라밸을 희생할 필요가 없어질 뿐임. 워라밸이 팀을 망친다는 발상에는 동의하지 않음
          + 오히려 “60시간 일하기 힘들다”는 말이 좋은 징조임
"
"https://news.hada.io/topic?id=21818","gmailtail - Gmail 실시간 모니터링 및 자동화 오픈소스 CLI 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              gmailtail - Gmail 실시간 모니터링 및 자동화 오픈소스 CLI 도구

     * Gmail 메시지를 실시간으로 감시하고 JSON 등 다양한 포맷으로 출력하는 커맨드라인 도구
     * 자동화, 모니터링, 외부 시스템 연동을 위해 설계되어, 코드나 스크립트와 쉽게 통합 가능
     * 실시간 tail 모드, 유연한 필터, YAML 기반 복잡한 설정, 다양한 출력 포맷 등 실제 업무에 필요한 핵심 기능 포함
     * 코드 변경 없이 파이프 조합만으로 다양한 데이터 추출·가공·알림 가능 (예: jq/notify/grep 등)
     * 주요 기능
          + 실시간 모니터링: --tail 옵션으로 신규 메일 지속 감시
          + 유연한 필터링: 발신자, 제목, 라벨, 첨부, 날짜 등 다양한 조건 지원(Gmail 검색 쿼리 완전 지원)
          + 체크포인트/재개: 마지막 위치 저장, 장애시 복구
          + 다양한 출력 포맷: JSON, JSON Lines, Compact, 커스텀 필드 지정
          + YAML 설정 파일: 복잡한 환경을 코드 변경 없이 관리 가능
          + OAuth2/서비스 계정 인증: 개인/서버/자동화 환경 모두 대응
          + 파이프라인 활용성: jq 등 도구와 조합, 다양한 후처리 자동화
     * 유스케이스
          + 신규 메일 실시간 감시 및 특정 조건만 필터링
          + 첨부 파일 포함 여부, 특정 키워드, 라벨별 감시 등 세밀한 설정
          + JSON Lines로 출력 후 jq로 후처리, 알림, 파일 저장, 통계 생성 등 자동화
          + GitHub/빌드/알림 메일 실시간 감시 및 CI/CD 연동
          + 대량 메일 분석, 통계, 백업 등 데이터 분석 파이프라인 활용

   Himalaya - CLI용 이메일 관리 도구

   CLI용으로는 이런 다기능 메일 툴이 이미 있긴한데, 연동 기능은 나름 재미날듯 합니다.
"
"https://news.hada.io/topic?id=21705","FLUX.1 Kontext [Dev] – 이미지 편집을 위한 오픈웨이트 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               FLUX.1 Kontext [Dev] – 이미지 편집을 위한 오픈웨이트 공개

     * FLUX.1 Kontext [dev] 는 이전까지 독점적이던 생성형 이미지 편집 모델의 오픈웨이트 버전으로 무료 공개됨
     * 12B 파라미터로 소비자급 하드웨어에서 동작 가능하며, 무료 연구 및 비상업적 용도로 배포됨
     * 세밀한 지역/전역 편집, 캐릭터 일관성 유지, 반복적 수정 등 상용 수준의 고품질 이미지 편집 성능을 제공하며, ComfyUI·Hugging Face Diffusers·TensorRT 등 인기 프레임워크와 즉시 연동 가능
     * 성능 평가에서 기존 공개 및 폐쇄형 모델(예: Google's Gemini-Flash Image)보다 뛰어난 결과를 보임
     * NVIDIA Blackwell 아키텍처 최적화 TensorRT 변종(BF16, FP8, FP4)도 제공, 최신 하드웨어에서 대폭 향상된 속도와 효율 실현
     * 상용 라이선스와 셀프서브 포털을 통해 비즈니스에서도 쉽게 통합 및 활용 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 중요성

     * FLUX.1 Kontext [dev]는 Black Forest Labs가 공개한 최신 생성형 이미지 편집 모델임
     * 지금까지 우수한 이미지 생성/편집 모델은 대부분 독점적으로 제공되어 왔으나, 이제 오픈웨이트로 전환되어 누구나 연구 및 비상업적 용도로 사용 가능해짐
     * 이 모델은 12B 파라미터로 구성되어 뛰어난 성능을 보이면서도, 일반 소비자 하드웨어에서 동작할 수 있는 효율성을 가짐

공개 및 접근성

     * FLUX.1 Kontext [dev]는 FLUX.1 비상업적 라이선스 하에 공개되어, 리서치와 비상업적 목적의 사용자에게 무료로 제공됨
     * 모델 웨이트는 HuggingFace에서 쉽게 내려받을 수 있고, ComfyUI, HuggingFace Diffusers, TensorRT 등 주요 AI 프레임워크와 호환됨
     * FAL, Replicate, Runware, DataCrunch, TogetherAI 등 파트너들이 클라우드 및 로컬 환경에서 즉시 활용 가능한 API 엔드포인트와 코드를 제공함

이미지 편집 분야의 새로운 표준

     * FLUX.1 Kontext [dev]는 이미지 편집 작업에 특화되어 있음
          + 반복적 편집 및 다양한 장면·환경에서 캐릭터 보존 우수성
          + 정밀한 로컬 및 전체 편집 지원
     * Black Forest Labs는 프로프라이어터리 모델에 필적하는 오픈 툴 제공을 목표로 함
     * 대규모 벤치마크(콘텍스트벤치 등)에서 바이티댄스 Bagel, HiDream-E1-Full 등 기존 공개 모델 및 Google Gemini-Flash Image 등 상용 모델 대비 우위 성능 입증
     * Artificial Analysis 등 외부 기관의 독립적 평가도 일치된 결과를 확인

하드웨어 최적화와 다양한 선택지

     * NVIDIA와 협력하여 최신 Blackwell 아키텍처에 맞춰 최적화된 TensorRT 웨이트(에너지 및 속도 최적화 BF16, FP8, FP4 버전) 제공
     * 개발자는 필요에 따라 속도·효율·품질 밸런스를 쉽게 선택 가능

상용 활용을 위한 라이선스 및 포털

     * Black Forest Labs는 투명한 조건의 셀프서브 라이선스 포털을 개설, 모든 오픈웨이트 모델에 대한 상용/비상업적 접근을 간소화함
     * 기업은 포털에서 간편하게 라이선스 구입 후, FLUX.1 계열 모델을 비즈니스 서비스에도 빠르게 통합 가능함

라이선스 업데이트 사항

     * 비상업적 목적 정의를 명확화하여 허용 범위 및 제한을 구체적으로 제시함
     * 콘텐츠 필터 조항을 추가해, 불법/권리침해 내용 생성을 방지하고 이에 대한 면책 조항을 보완함
     * 콘텐츠 출처 및 생성 이력 관리는 관련 법령을 준수하도록 규정됨
     * 허용되지 않는 사용 사례에 대한 명확한 예시와 제한 사항을 추가함

참고자료 및 지원

     * 모델/코드/문서
          + 모델 가중치
          + GitHub 코드
          + API 문서
          + 라이선스 포털
          + FAQ/지원
     * FLUX.1 Kontext [dev] 공개에 관한 기술 리포트는 arxiv 에서 열람 가능함
     * 사용자·기업 모두 각종 API, 설명서, 벤치마크 데이터에 접근 가능

        Hacker News 의견

     * BFL에서 가중치를 공개한 점이 정말 멋진 부분이고, 오픈소스의 재정적 지속 가능성이 어려운 현실에서 학계에는 무료로 제공하고 스타트업에는 합리적인 라이선스 비용을 받는 방식이 BFL과 타사에서 계속해서 오픈 가중치 모델을 내놓을 수 있게 해주는 방식이라고 생각함
          + 만약 BFL이 사용하는 모든 이미지와 비디오 기반 콘텐츠에 대해 사전에 명시적으로 사용 허가 비용을 지급해야 한다면, 그런 방식으로는 재정적으로 지속 불가능하다고 생각함
     * 오픈 가중치를 공개하면, 이런 편집 모델에 새로운 기능(작업)을 추가할 수 있는 점이 흥미로운 특징임
          + 적은 샘플(30개 정도)로도 일반화가 잘 되는 모습이고, 이를 블로그에서 공유했음
               o 이번 Kontext는 모두가 기다리던 버전이고, 훨씬 더 실용적인 느낌임
               o 트레이닝이 가능한 새로운 세대의 이미지 생성 모델 중 첫 사례로, Gemini, GPT, MJ 등 기존 대형 모델에서는 불가능했던 부분임
     * distill된 Dev 모델이 더 큰 Pro/Max 모델들과 비교해도 충분히 성능을 유지해주길 바라는 중임
          + 최근 Stable Diffusion 같은 구식 인페인팅 기법을 이 모델이 여러 부분에서 완전히 대체할 수 있길 기대함
          + Kontext를 이용한 이미지 편집의 전후 실험을 여기에서 확인할 수 있음
     * 지난주 샌프란시스코에서 열린 해커톤에서 직접 이 시스템을 써봤고, 꽤 인상적인 느낌을 받았음
          + 그 해커톤에서 사람들이 어떤 프로젝트를 만들었는지 궁금함
     * 라이선스의 의도가 접근하는 사람 자체를 제한하는 일종의 사전 필터링 역할을 하는 느낌임
          + 형식적으로는 열려 있더라도 실제로는 적용 범위가 좁아지고, 이런 설정이 누가 도전할 수 있는지를 정형화하는 효과가 있음
          + 남용을 방지하는 장점이 있지만 유용한 사용처도 잘릴 수 있고, 결과적으로 새로운 실험의 기준까지 미묘하게 변화시키는 경향임
     * 새롭게 적용된 비상업적 사용 라이선스가 꽤 까다로운 규정이고, 라이선스 전문을 참고하면 됨
          + 이 라이선스를 제대로 해석하면, 파생물이 아니라 단순 결과물 생성을 목적으로 하더라도 상업적 환경에서는 사용할 수 없음
          + 라이선스가 정확히 무엇에 적용되는 것인지 의문인데, 아마도 쉽게 재구현할 수 있는 파이썬 코드 정도를 제외하면 저작권이 없을 거라고 생각함
          + 모델 가중치는 기업들이 바라는 만큼 저작권이 부여될 수 있는 창작물이 아니고, 저작권의 법적 정의 내에도 해당되지 않는다는 입장임
          + 데이터베이스나 소프트웨어와는 달리 모델 가중치는 저작권 요건을 충족하지 못하며, 창작성이 얼마나 들어갔는지도 상관없고 저작권 보호 대상이 아님을 강조함
     * Kontext 같은 모델을 실시간으로 캔버스 상에서 AI와 협업하면서 그림을 완성해가는 방식으로 활용하고 싶음
          + LLM 분야의 시계열 예측 혁신과 Google Quick Draw 데이터셋 같은 걸 접목해서 브러시로 대화하며 그림을 완성하는 ‘실시간 페인팅 파트너’ 같은 경험이 나오길 정말 바람
               o Fal.ai의 Kontext 모델을 써보면 전후 변환 슬라이더와 편집 이미지로 지속적으로 수정할 수 있는 기능이 인상적임
               o BFL에서 dev 모델까지 공개된 만큼, 기존에 Stable Diffusion용 플러그인이 이미 있는 Krita용 Kontext 플러그인도 꼭 나왔으면 함
               o Krita 플러그인 참고 링크
     * 이 시스템이 어느 정도 VRAM에서 동작하는지 궁금함
          + 현재 기준으로는 약 18~20GB VRAM이 필요하지만, 내일이나 가까운 미래에는 1시간 정도의 여유 시간만 있다면 4GB VRAM에서도 동작할 수 있음
     * 라이선스가 제약적이라는 점이 아쉬움
"
"https://news.hada.io/topic?id=21820","Whatever의 부상","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Whatever의 부상

     * PayPal 중심의 온라인 결제가 여전히 유지되고 있으며, 혁신은 벌어지지 않음
     * 비트코인과 암호화폐가 새로운 혁신처럼 보였으나, 본래 목적 대신 투기 대상으로 전락함
     * 대형 플랫폼의 중앙집중화와 광고 수익 극대화가 웹의 다양성과 창의성을 저하시킴
     * LLM(대형 언어 모델)과 생성 AI는 “무엇이든 만들어주는 기계” 수준으로, 정보의 가치와 진정성을 약화시킴
     * 사회 전반에 걸쳐 “Whatever(뭐든지, 아무거나) ” 문화가 퍼지며, 창작과 직접 행위의 의미가 퇴색되는 현상 발생
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 컴퓨터가 더 이상 재미없는 이유

     * 본래 제목은 “컴퓨터가 재미있었을 때를 그리워함”이었으나, 기술이 재미없어진 데에는 근본적인 배경이 숨어있음
     * 최근 역사에서 이 현상은 온라인 결제 시스템, 웹 구조, 콘텐츠, 그리고 새로운 기술 도입 등을 관통하고 있음

온라인 결제와 진짜 혁신의 부재

     * 2000년대 미국에서는 PayPal이 유일한 인터넷 송금 수단이었음
          + PayPal은 사용 규제가 까다로우며, 규정 위반시 계정과 자금을 6개월간 동결시켰음
          + 사용자 불만이 많았지만, 대안이 없어 “중간자”의 지배력이 강했음
     * 2010년경 등장한 Bitcoin에 기대가 쏠렸으나, 실제로는 투기와 사기, 투자 목적이 강해져 일상 결제 수단으로 자리잡지 못했음
          + 브라우저 내 소액 송금, 웹사이트 팁 기능 등은 실현되지 않고 있음
          + 대체 결제 수단들도 Stripe, PayPal에 의존하며, 실질적인 변화나 혁신은 일어나지 않음

암호화폐, NFT, 그리고 “Whatever” 문화

     * 암호화폐, NFT의 확산 배경에는 기술 그 자체의 효용이 아니라, 통계 그래프가 올라가면 부자가 된다는 투자 심리가 자리잡음
          + 실제 사용처, 예술성, 기술적 구조보다 ""타인을 설득해 가치를 끌어올리는 것""이 강조됨
     * 이런 접근 방식은 ""Whatever"" 문화를 만들어 냄
          + 실체나 내용이 없어도, 구매할 만한 ‘무언가’ 만 있으면 됨
          + Twitter 등 플랫폼에서 수많은 그리프터들이 가치 없는 Whatever을 부추기는 현상이 일상화되고 있음

웹의 변화와 중앙화, 그리고 창의성의 상실

     * 원래의 웹은 개인 제작물과 다양성으로 가득했지만,
          + 수많은 웹사이트를 추적하고 소통하는 것이 불편해 소수의 대형 플랫폼(Twitter, Reddit 등)에 집중됨
          + 중앙집중화와 플랫폼의 무료 이용 구조는 운영비용 문제를 낳음
     * 대형 플랫폼은 광고 노출 극대화를 위해 흡입력 있는 Engagement에 집착하며,
          + 콘텐츠 제작자들에게 광고 주위에 단순한 “내용”=Content만 요구함
          + 그 결과, 클릭 유도 미끼, 검색엔진 최적화 글, 반복적이고 공허한 블로그 등 “내용이 없는 콘텐츠” 양산이 가속화됨
          + 글, 영상, 게임 웹사이트들조차 SEO와 광고만 남은 채 각각의 특색을 잃음

생성 AI, LLM과 “Whatever”의 기술적 실현

     * 최신 LLM(대형 언어 모델)이나 Copilot, 자동생성 코드는 “원하는 무엇이든” 생성하지만,
          + 실제 정보나 창의적 해결보다는 통계적으로 그럴듯한 말만 이어붙이는 방식임
          + 정답 확인, 정확성 검증은 사용자 몫이 되며, 그 결과 “노이즈”와 혼란이 더해짐
     * 기업은 LLM 기능을 모든 앱에 억지로 추가하고 있음
          + 실제로는 사용성 향상이나 업무 효율이 증명되지 않음
          + Microsoft는 AI 도입을 직원 평가에 반영하도록 강제하는 등, 도구 자체를 통해 변화한 사용자 경험보다 “채택 자체”에 집중하는 아이러니한 모습이 나타남

“Whatever” 기계와 도구로서의 가치

     * LLM은 전통적인 도구(계산기, 톱 등)와 달리, 정해진 기능을 수행하지 않고,
          + 입력마다 무작위적이거나 통계적으로 타당한 것처럼 보이는 ""무엇이든""을 제공함
          + 자동화의 이점을 넘어, 새로운 유형의 실용성 및 신뢰성 한계를 노출함
     * 사용자 경험은 오히려 나빠지고, AI의 도입과 가치에 대한 명확한 기준이 사라짐
          + 사용자 간에도 이 도구의 활용을 두고 “쓰지 않는 것이 비정상”이라는 분위기와, 도구의 진짜 효익에 대한 회의가 공존함

창작의 의미와 대체 가능성의 사회화

     * LLM, 생성 AI의 확산은 “아무나, 아무거나 만드는” 세상을 만든다는 허상이 퍼짐
          + 음악, 미술, 글 등에서 누구나 단시간에 결과물을 만들 수 있지만,
          + 기술 자체가 결과물의 최소 기준이 되어, 각 개인의 창의적 의미를 약화시킴
     * Facebook의 AI 계정처럼 가짜 “관심사”와 “콘텐츠”로 사람들을 붙잡는 전략
          + 실제로는 표준화, 단순화, 생산비 절감을 통한 평균화로 귀결됨
          + 기술의 발전이 창의성의 확장 또는 민주화가 아닌, 무관심과 무의미의 확산으로 이어질 위험 존재

결론: 무엇을 가치 있게 만드는가?

     * 기술의 발전이 “무엇이든 만들어 줌”을 넘어, 실제로 직접 무엇인가를 하는 가치 자체를 무의미하게 만드는 분위기를 조장함
          + Doing(행함), Making(만듦) 자체가 가치를 잃고 있다는 점에 경계심 표명
     * “Whatever” 문화가 확산될수록, 작업의 직접성, 창의성, 의미가 퇴색됨
          + 기술 별개로, 내가 직접 하고 만드는 것의 기쁨이 사라지는 사회에 문제의식 제시
     * 생성 AI와 관련 기업(예: OpenAI)에 대한 날선 비판과 함께,
          + 진정한 창작은 “Whatever”가 아닌 주체적 행위와 관심에서 비롯됨을 강조
     * 마지막 메시지는 “직접 만들어라. 무엇이든 만들어라.”
          + 그 결과물이 웹에 공유될 때, 진짜 의미와 기쁨이 깃든다는 소망을 남김

        Hacker News 의견

     * 위에서 언급된 모든 의견에 전반적으로 공감하는 편임
       하지만 내가 가장 크게 걱정하는 부분—이 글에서는 제대로 다뤄지지 않았거나, 혹은 살짝 언급만 된 부분—초보자와 배움의 과정에 미치는 영향임
       예를 들어, ""예전엔 어떤 일을 정말 즐기는 것처럼 보였던 사람들이 사실은 그저 원하는 걸 말만 해도 적당히 비슷하게 만들어주는 무언가를 원했음이 드러나는 상황""이 있음
       트위터에서 누군가가 ""나는 3.5시간 만에 앨범 전체를 만들었고 앨범 아트도 만들었음, 왜 귀찮게 직접 하지 않고 이런 '쉽게 만드는 머신'을 쓰지 않겠냐""며 자랑하는 걸 봤음
       초보자에게는 원래 힘든 일을 하고 싶지 않아하는 게 당연한데, 그림을 그렸더니 망치고, 기타를 치려고 해도 제대로 소리도 못 내는 상황에서 ""포켓몬 스타일로 우리 고양이 그림 그려줘""라고 말만 하면 완벽한 결과를 얻을 수 있는 기계가 있다면, 당연히 12살짜리 어린이에게는 몇 년을 갈고닦아야 겨우 그럴듯해지는 길보다 훨씬 더 매력적인 선택지임
       하지만 지금까지는 선택지가 없었기 때문에 졸작 그림과 어설픈 연주를 계속 반복하다 보면 노력의 가치를 느끼게 되고, 결국에는 상당히 실력을 갖추게 되는 선순환이 있었음
       그런데 만약 사회 전체가 ""그림 그릴 필요 없어, 힘든 건 하지 말고 ChatGPT한테 시켜""라는 메시지를 청소년 대상 광고, 미디어, 교실에서 수년간 쏟아낸다면 앞으로 어떤 곳에 도달하게 될지 상상만 해도 두려움임
          + 사람들은 LLM이 기존 기술과 다르다는 점을 여러 방식으로 설명하려 하겠지만(예: 원글의 글처럼), 사실 그저 또 하나의 편리함 추구임
            인류는 문명 초기부터 더 쉽고 편한 방법을 항상 찾아왔음
            기술은 정말 집중해서 실력을 쌓고 싶은 사람을 막아본 적 없음
            자동차가 생겨도 100미터 달리기 세계 기록은 계속 개선됐고, 컴퓨터가 수천만 자리까지 파이를 계산할 수 있어도 암산 암기 세계 기록도 계속 업데이트됨
            파워리프팅 같은 운동이 건재한 현실을 보면, 그림 그리기가 LLM/Diffusal 모델 때문에 사라질 거라 생각하는 건 말이 안 됨
          + 나는 15년 전 좋아하는 아티스트의 인터뷰와 블로그를 사전 들고 하나하나 번역하며 일본어를 배웠고, 지금은 일본에서 일하고 지냄
            지금은 아티스트 트윗 아래 버튼만 누르면 즉시 번역이 뜸(대체로 맞긴 한데 10번 중 1번은 완전히 틀림)
            해외 팬 입장에서는 좋은 혁신이지만, 만약 내가 이런 환경에서 컸더라면 과연 배워볼 동기가 있었을까 궁금함
          + 네가 쓴 내용에 전적으로 동의하며, 결국 인간이 원하는 것은 아무 수고 없이 머릿속에 원하는 것을 '그냥' 집어넣는 상태라고 봄
            눈앞에 모든 것이 고급스럽게 포장되어 진열되고, 수십 번 실패하며 시행착오를 겪어야 얻는 배움의 축적과 발전이 사라진다는 점에서 위험이라고 생각함
            '학습'이라는 과정이 실제로 사라지기 시작하는 느낌임
            수년 동안 AI에만 의존하다가 갑자기 그게 사라지면, 본인 힘으로 아무 것도 못 하게 되고, 기본적인 일도 못 하는 무능 상태가 될 거라 생각함
            이런 이유 중 하나 때문에 나는 AI 열풍을 아주 비판적으로 보고 있음
            계속 옛 방식을 고수하는 이유는 진짜 책을 읽고, 직접 시도하고, 실패하고, 또 반복하는 가치를 온전히 이해하기 때문임
            진정한 배움에는 다른 길이 없음
            현 세대가 이 가치를 이해하고 있을지, 다음 세대는 과연 이해할 수 있을지 궁금함
          + 한 가지 위안은, 우리가 그간 학습, 사용성, 불편함 대처, 교육/직업/훈련 시스템의 오랜 고정관념을 얼마나 잘/못 다뤘는지에 대해 새로운 관점을 피할 수 없는 상황이 올 수도 있다는 점임
            예전에는 고용/급여에 대한 요구가 사람들을 해당 파이프라인으로 몰았지만, 이제 실제 이해와 전문성, 품질 자체가 가치 있는 것으로 받아들여지면, 시스템이 이런 덕목을 얼마나 잘 육성하고 활용하는지 신경 쓸 필요가 높아질 수 있음
            그러나 직감적으로 느껴지는 건, 우리는 매우 극적인 문화적 소용돌이 속에 있으며, 아무 것도 당연하게 여길 수 없는 시대로 접어들었다는 점임
            사실 우리의 '싸움' 중 일부는 이미 현대 컴퓨팅 길로 들어서기 시작할 때 오래전에 졌던 싸움일지도 모름
          + 만약 우리가 기술과 실력을 유지하지 않고 AI에 지나치게 의존한다면 아주 큰 문제임
            제조 아웃소싱과 마찬가지로, 어느 순간에는 완전히 능력을 잃고 완전히 남에 의존하게 됨
            WWW 등장 때 모든 정보에 접근하는 게 사람들을 계몽하리라 믿었지만, 실제로는 배움이 없는 상태에선 올바른 정보를 구분하지 못하고 이해할 수 있다고 착각하는 (잘못된) 정보에 더 쉽게 끌림
            만약 LLM이 모든 문제의 해답을 제공해 줘도 우리가 너무 무지하고 역량이 없어 그 해답을 알아보지 못한다면 결국 AI를 외면하고 자신이 이해하고 신뢰하는 정보로 돌아가게 됨, 사실이 아니더라도 말임
            어찌 보면 새로운 일은 아니고, 과학에 대해서도 이런 현상이 꽤 오래전부터 있어왔음
            너무 복잡하고 확인이 힘들면 사람들은 신뢰하지 않고 다른 소스를 찾게 됨
     * 코드 편집기가 괄호나 따옴표를 자동으로 닫아줄 때마다 굉장히 짜증남
       전혀 시간 절약도 안 되고, 잘못 행동할 때가 많아서 오히려 손이 더 감
       왜 이런 기능을 누가 처음 만들었을까 도저히 이해 안 가는 상황임
       어차피 구역을 벗어나려면 뭔가 키를 눌러야 하고, 그냥 괄호를 치든, 방향키를 누르든, 전혀 단축되는 것도 아님
       이런 기능이 이론적으로는 소스가 유효한 문법을 자주 유지시켜주고, 특히 문자열 하이라이팅이나 LSP 등에서 도움될지 모르지만 결국 오류율이 높아 상대적으로 쓸모 없음
          + 다년 간 동료들과 페어 프로그래밍하면서, 키보드에 익숙하지 않은 사람들이 많다는 걸 느낌
            심지어 오른쪽 괄호 치는 것도 부담이라 실제로는 손을 마우스에 옮겨 클릭해 빠져나가는 경우도 많음
          + 텍스트 일부를 선택하고 따옴표 한 번 누르면 양쪽에 자동으로 붙는 기능은 실용적임
            하지만 일반 입력 모드에서는 종종 성가심임
          + ""자주 틀린다""는 얘기에, 언제 틀린다는 건지 궁금함
            내가 여는 괄호/중괄호를 넣었을 때 자동으로 닫아주면 항상 맞는 거 아님?
            노르웨이 키보드에선 중괄호가 좀 까다롭긴 하지만, 이런 기능 덕분에 5단계 중첩 같은 코드 쓸 때 ctrl+shift+enter 한 번이면 모든 괄호 자동 완성되고 커서도 제자리에 와줘서 직접 모두 닫는 것보다 편함
          + 전엔 이 기능이 싫었지만, 닫는 따옴표를 그냥 치면 중복되지 않는다는 걸 알게 된 후론 신경 안 쓰게 됨
            가끔 에디터가 너무 '똑똑한 척' 하면서 꼬일 땐 짜증나긴 하지만 자주 있지는 않음
          + 소스가 자주 무효해진다는 점에서 실용성에 동의하지 않는다는 의견에, 어쩌면 구조적 편집의 희미한 흉내 일종 아닐지 추측함
            몇몇 에디터엔 항상 문법이 맞게 해주는 모드가 있는데, 그런 곳에선 이런 기능이 필수임
     * ""유럽처럼 은행에서 '사람에게 돈 보내기' 기능이 기본인 미래에 산다""는 비유를 보고, 호주에서 처음 PayPal이 들어왔을 때 나는 왜 필요한지 오히려 의아했음
       왜냐하면 인터넷 뱅킹으로 바로 이체가 가능했기 때문임
       그러다 정부에 로비해서 우리 은행 시스템을 '더 불편하게 만들고 경쟁하겠다'고 한 게 PayPal이고, Uber가 그랬던 방식과 비슷함
          + 유럽에서는 PayPal이 룩셈부르크에서 공식 은행 라이선스를 취득함
          + EU에서 PayPal의 장점은 (1) 송금이 즉각적이라서 은행 이체처럼 1~2일 안 기다려도 됨, (2) 상대방에 내 은행 정보를 공개하지 않음
            첫 번째 장점은 이제 SEPA 즉시이체로 점점 없어진 중임
          + 사실 이게 잘 이해가 안 감
            EU에서도 송금이 그렇게 쉽지 않았음
            우리도 SWIFT, CHAPS 같은 시스템만 있었고, SEPA처럼 빨라진 건 PayPal보다 한참 뒤에야 가능해짐
     * 이 글 정말 좋게 읽었음
       한 문단이 너무 와닿았음
       예전엔 obscure script도 google-fu와 rtfm, 끈기로 해결하던 사람이었음
       그런데 LLMs 때문에 이런 스킬이 사실상 사라졌고, 이제 다들 같은 방식으로 한다는 게 안타까움
       실제로는 ""이걸 어떻게 해야 할지 몰라서 ChatGPT한테 물어봤는데 200줄을 줬는데 안 됨"" 식으로 접근하면 도와주기 싫어짐
          + 나도 매일 LLM으로 코딩하고 기사 내용에 대체로 동의함
            심지어 크립토 분야에서도 Defi, NFT 프로젝트를 직접 해봤고, '돈세탁/범죄'라는 농담 같은 평가에도 기술 자체는 흥미롭다고 생각함
          + 나는 아직도 구글링과 rtfm으로 해결하는 사람이란 점에서 스킬이 완전히 사라졌다고 느끼지 않음
            여전히 LLMs가 써 놓은 엉성한 코드를 고치고 있고, 그게 아주 나쁜 것만은 아니라고 생각함
            다만 문제는 코드의 질적 부채가 엄청난 속도로 쏟아지는 시대라는 것임
          + LLM이 자주 틀리기 때문에 구글링 실력이 사라지는 일은 없는 상황임
            LLM 쓰면서 모든 사람이 천재가 됐다는 식의 주장은 거짓임
            인간은 여전히 시스템의 허점을 이해하고, 데이터를 바로잡고, 능동적으로 사용함
            마치 Word가 띄어쓰기, 맞춤법을 고쳐주니 다 수용하는 것처럼 말하는데, 그건 말도 안 되는 논리임
          + obscure shell, 80년대 스타일 Unix 도구를 우리가 여전히 머리 싸매고 써야 했던 게 되려 문제였음
            이런 것들이 LLM/자동화로 대체된 건 오히려 환영임
            정말 친절한 툴로 변화한 게 다행임
     * 초반에는 전반적으로 동의하지만, 지나치게 암울하게 보는 시각은 별로이고, 사람들을 무지하고 탐욕스럽다고 퉁치는 건 큰 의미가 없다고 생각함
       오히려 그런 환경에서도 잘 살아낸다는 점이 인상적임
       그리고 ""모든 LLM 사용이 무용""이라고 치부하는 것은 아깝고, 제대로 써 먹는 것도 하나의 기술임을 인정해야 함
       물론 엉뚱한 데에 AI를 쓰는 경우가 많지만, 잘못 활용했다고 해서 전체가 쓸모없다는 건 사실이 아님
       결론(직접 뭔가 해보라/만들어라)에는 동의함
          + 주로 현상 유지는 모방적 행동이라고 생각함
            지금은 누군가가 ""이게 미래야""라고 말하면, 그걸 그대로 믿고 어떤 식으로든 뛰어들고 싶어하고, 다른 사람이 또 보면서 '나도 해야겠다'는 식으로 반복됨
            그래서 AI 세계에서 진짜로 새롭거나 독창적인 프로젝트는 아주 적음
            참여한 많은 사람들이 실상 뭘 만드는지엔 관심이 없고, 오로지 더 많은 돈과 영향력을 꿈꾸는 '탐욕/기대'로 달린다고 봄
            그래서 결국엔 ""무의식적이긴 해도 바보짓, 탐욕""이 맞다는 생각임
          + ""이 정도 환경에도 잘 산다는 게 대단한 위안거리다""라는 의견은 세상이 더 심각한 붕괴로 치달을수록 점점 힘을 잃을 것 같은 예감임
          + ""모든 LLM 사용이 쓸모없다""는 식의 방어반응은 요즘 어딜 가나 보임
            윤리적 주장 등 다른 입장도 귀 기울일 필요 있지만, 정작 현실을 외면한 채 무작정 부정만 하는 모습은 답이 아님
          + 무료 모델도 쓸모 있음
            조금 제약이 있지만 여전히 실용적임
     * LLM 관련 의견에는 동의하지 않지만, 포스팅이 굉장히 글도 좋고, 논조도 명확하고, 생각거리도 주며, 요즘 보기 드문 '웹사이트의 캐릭터'가 느껴졌음
       처음엔 별 기대 없이 클릭했지만, 읽는 내내 '옛날 인터넷 느낌'이 살아나서 좋았음
       본문에서 언급된 마이크로결제가 실현됐다면 작가에게 얼마라도 쏘고 싶지만, 구독제밖에 없어 아쉬움
          + Eevee는 정말 글을 잘 쓰지만, 이번 글은 예전만큼 강렬하게 다가오지 않음
            Twitter/Bluesky 스타일의 가벼운 독설, 단순 이슈 소비 형식이 느껴져, 오히려 예전 글보다 개성이 약함
          + 너무 동의함
          + 워낙 em dash가 자주 나와서 AI가 만든 글 같다는 의심이 들었음
            (추신: 저자는 GPT 등장 전에도 em dash 사용하던 사람이니 그 스타일임)
     * 블로그에서 bitcoin이 결국 화폐가 될지도 모른다는 부분을 보고, 제발 아니길 바람
       이 경우 탈락자들의 돈이 먼저 산 사람 이익에 돌아가는 구조밖에 안 됨
       실제로는 bitcoin이 '진짜 돈'이 되는 걸 주장하는 사람, 특히 국민 퇴직연금까지 비트코인에 넣자는 쪽은 전부 사기꾼이라고 생각함
       블로그 저자가 원한 건 아닐 거라고 보지만, 'bitcoin이 진짜 돈이 되면...'이라는 모호한 바람조차 결국 이런 사기 구조를 옹호하게 됨
       이미 연금 등 공적 자금이 비트코인에 몰리고 있다는 점에서, 이걸 지지하는 은행과 정치인은 전부 처벌받아야 한다고 생각함
          + 암호화폐가 정말 더 나은 미래가 되려면 비트코인은 가치가 크게 떨어져야 할 것임
            비트코인은 더 발전하기 힘든 거의 '얼어붙은' 상태고, 이 문제를 진짜 해결할 네트워크는 결국 다른 게 될 확률이 높음
            Ethereum은 확장성 문제를 계속 시도하긴 하지만 그 탓에 사기가 많이 몰림
          + 저자가 진짜 원하는 건 뭐냐고 묻고 싶음
            결론은 일반적인 금융/송금의 자유임
            즉, 누구에게든 이유 막론하고 돈을 쉽게 보낼 수 있길 원함
            예를 들어, 사람들에게 맞춤 그림(예: furry porn 주문) 의뢰금을 쉽게 받고 싶어함
            미국 시민이 이란 프리랜서에게 개발 일감 외주를 줄 수도 있고, 이런 게 더 쉬워지길 바람
            지금은 정부의 '도덕 및 실질적 외교' 통제, 혹은 카드사/은행의 간섭 때문에 이런 자유로운 송금이 막힘
            예를 들어 Visa/Mastercard가 일부 업종(성인물 등) 결제를 거부하는 식임
            결국 이런 환경 때문에 일반인도 자유화·분산화된 디지털 화폐에 관심 갖기 시작했고, 이 과정에서 사기꾼도 대거 몰림
            '어떻게 하면 중앙집중이 아닌 안전한 디지털 캐시를 실현할 수 있을지'가 진짜 숙제임
          + 만약 사람들이 bitcoin을 단순 매수 대신 직접 채굴로 얻게 된다면 좀 더 정당하다고 볼 수 있을까?
            탈중앙, 공급 제한 코인을 만든다고 했을 때 그걸 어떻게 '공정하게' 분배할 수 있을지 질문임
            지금의 시스템(중앙 통제화폐)이 마이크로소프트의 유료 독점 소프트웨어 같은 거라면, 리눅스 같이 (초기엔) 특정 개발자/초기 참여자에게만 이득이 돌아가는 오픈소스로의 전환을 비판할 수 있나?
            계속 구독료(인플레이션)를 내며 중앙화된 소프트웨어를 쓰는 게 lump sum으로 GNU/Linux를 사서 영구 소유하는 것보다 낫다고 볼 수 있을까?
     * 웹이 온통 '적당한 뭐시기'로 가득 차 있는데도 그런 플랫폼에 왜 열광하는지 이해 안 됨
       예전엔 인터넷이 정말 재미있었는데, 요즘은 존경하던 사람들까지도 시간만 잡아먹는 시스템에 빨려 들어가는 걸 보고 답답함
       그 때문에 스스로 이런 플랫폼을 멀리하는 입장인데 오히려 외톨이가 되는 심정임
       AI에 대한 저자의 '폭주' 부분엔 공감하지 못함
       LLM(대형 언어 모델)이 너무 많은 분야에 억지로 들어가는 게 피곤하긴 한데, 실제로 개발 도구로서는 효과가 워낙 커서 미칠 것 같음
       반복적인 프로그래밍 작업에서 설명 몇 줄만 작성해 자동화시킬 수 있어서 시간을 정말 많이 아낄 수 있음
     * 사실 이 글은 자신이 싫어하는 부류(플랫폼화 지향, 무비판적 타성 등) 사람에 대한 푸념처럼 느껴졌음
       ""플랫폼의 노예, 자율성 없는 존재""라고 까지만, 결국 우리 모두 남의 블로그를 찾아 읽고 있음
       저자가 LLM을 싫어해도, 앞으로 더 많은 사람들이 저자 본인만큼이나 쉽게 정보를 받아들이고, 이걸 남에게 효과적으로 전달하는 능력 또한 LLM이 곧 대체하게 됨
       지금은 LLM이 한심하다고 해도 계속 발전하면서 저자가 문제 삼는 점도 새 방식으로 보완될 것임
       대체적 시각으론 ""대중문화""란 항상 '가볍기'와 '어설픔'이 특징이었고, 지금은 인터넷 전체가 그런 대중성에 잠식됨
       아무리 희귀한 것도 온라인에 올라오면 모두의 것이 되는 상황임
       이는 정보 비대칭이 완화되고, '악한 자'가 남을 속여 먹는 일이 예전보다 줄어드는 긍정적 측면도 있음
       SNS 같은 광고 기반 네트워크는 언젠가 점점 사라질 것임
       다만 우리는 이런 인식 단계의 초기라, 가짜 뉴스 등 사회적 문제는 한동안 더 지속될 것임
          + 나도 네 관점에 동의하지만, 그렇다고 해서 '악한 자'들이 기술을 초기부터 장악해서 오히려 더 심하게 억압/통제하는 쪽으로 안 갈 거라 확신할 수 없는 상황임
     * ""다른 사람들은 온통 소수 리스트와 rebalancing binary tree로만 된 프로젝트를 만들고 있냐""는 질문에, 실제로 LLM이 프로그래밍에서 인기 있는 진짜 이유는 그들이 문제를 '해결'하는 데 능해서가 아니라, 이미 많이 풀렸던 기성 문제의 변형을 쉽게 재생산하는 데 능하기 때문임
       업계 대부분의 일은 결국 기성의 똑같은 문제를 계속 변형해서 푸는 반복임
       이는 NIH(not invented here)가 아니라 코드 재활용, 즉 재사용 자체가 어려운 문제라 그래짐
       실제로 중요한 가치는 독특한 문제 해결, 그리고 이걸 조합하는 '아키텍처'에 있지만, 단일 코드베이스 기준으론 이런 요소 비중이 결코 크지 않음
"
"https://news.hada.io/topic?id=21824","만성 통증 문제에 집중하기 위해 기술 직장을 떠난 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     만성 통증 문제에 집중하기 위해 기술 직장을 떠난 이유

     * 필자는 4년에 걸친 만성 통증을 겪으며 삶의 여러 영역에서 변화와 어려움을 경험함
     * 이 경험을 바탕으로, 만성 통증이라는 복잡하고 혼란스러운 현상을 더 깊이 이해하고 극복법을 알리고자 결심함
     * 올해 기술 업계를 떠나 시드니의 집을 팔고, 만성 통증 관련 활동에 시간을 집중하기로 결정함
     * 블로그 시리즈를 통해 만성 통증의 정의, 원인, 회복 전략 등에 대한 정보를 제공할 예정임
     * 현대 통증 과학에 기반한 접근법을 바탕으로 더 많은 사람들의 회복에 기여하고자 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

만성 통증과의 첫 만남: 기술 직장에서 새로운 길로 나아간 계기

   겨울에 이사를 마친 후, 갑자기 오른쪽 아킬레스건에 심한 통증이 생기기 시작함
   이후 4년간 신체 여러 부위에 설명하기 힘든 지속적인 통증이 차례로 나타남
   왼쪽 아킬레스건, 목소리, 오른쪽 어깨, 그리고 다시 양쪽 아킬레스건과 손/팔꿈치 등으로 확산됨
   이로 인해 운동, 음악, 대인 관계, 일 등 삶의 중요한 활동들이 계속 제한되는 상황이 됨

   이 기간 동안 자신에 대해 더 깊이 성찰하게 되었고, 이는 결국 회복 과정에 중요한 열쇠가 됨
   필자에게 만성 통증은 단순히 고통이 아니라, 자기이해의 기회를 제공한 회색빛 전환점이었음

만성 통증 해결을 위한 새로운 시작과 목표

   병이 거의 회복된 뒤, 필자는 이 어려운 문제에 창의력과 에너지를 쏟는 것을 목표로 삼음
   호주 성인 5명 중 1명이 겪고 있을 정도로 만성 통증은 흔한 문제임
   만성 통증의 회복이 생각보다 더 가능하다는 사실에도 놀랐으며, 이를 더 널리 알리고자 결심함

   올해 필자는 기술 직장을 퇴사하고 시드니의 집을 매각함
   이처럼 대담한 결정은 만성 통증 문제 해결을 위한 시간과 자원을 확보하기 위함임
   블로그 연재를 통해 만성 통증에 대한 오해를 걷어내고, 관련 분야의 사람들과 소통하는 것이 목표임

블로그에서 다룰 주제 및 구성

   Sail Health 블로그에서는 다음과 같은 내용을 다룰 예정임
     * 만성 통증의 정의: 기존 통증 모델, 다양한 유형, 생물학·심리·사회적 요인, 주요 통계, 연구 등
     * 원인 탐구: 신경계 동작 원리, 통증의 경고 신호 역할, 뇌의 만성 변화(신경가소성) , 성격·트라우마 기여 요인 등
     * 회복 전략: 회복 접근법이 본인에게 적합한지 평가하는 자가진단법, 실제로 효과가 있었던 다양한 도구 및 연습법 제공

   주요 구성을 무엇(정의), 왜(원인), 어떻게(회복) 로 정리해 나갈 예정임

읽어야 할 이유

   독자 대상은 다음과 같음
    1. 이미 다양한 시도를 해봤으나 효과를 못 본 만성 통증 경험자
    2. 만성 통증이 없으나 전반적 웰빙과 두뇌-신체 상호작용에 관심 있는 사람

   특히, 마음-신체 통합적 접근법을 제대로 시도해보지 않은 경우, 함께 학습하고 연습하길 권장함
   이 접근법은 현대의 통증 과학 기반이며, 많은 사람들의 회복을 실질적으로 도왔음
   2021년 미국 연구에 따르면, 해당 치료 시작 후 6개월 내 환자의 66%가 거의 통증 없이 회복함
   이는 CBT나 외과적 시술보다 훨씬 높은 성공률과 지속력을 보임

   필자 또한 한때 신체적 치료에만 집중했으나, 마음의 작용이 신체 증상에 영향을 줄 수 있음을 인정한 뒤 실질적 변화를 경험함

너에게도 의미 있는 이유

   만성 통증이 없더라도, 자기 건강이나 뇌-심신 작용에 관심 있다면 이 정보를 통해 다양한 도움을 얻을 수 있음
   특히 risk factor로 알려진 완벽주의, 타인 배려 성향, 불안, 스트레스 많은 환경 등이 있다면 더 유익함

주의사항 및 마무리

   필자는 의사가 아니므로 의학적 진단이나 치료 대체로 이 정보를 사용하지 않길 강조함
   블로그 시리즈의 다음 주제는 ‘만성 통증이란 무엇인가’에 초점을 둘 예정임
   함께 두뇌와 마음의 연결을 쉽고 즐겁게 이해해나가는 시간이 되길 바람

        Hacker News 의견

     * 35살에 몸이 완전히 망가진 느낌, 지칠 대로 지친 상태, 쉬는 날에도 기력이 없어 삶을 제대로 누릴 수 없었음. Army 복무 때도 이 정도는 아니었음. 내가 겪은 고통과 고난의 원인이 실은 IT 직업 자체가 아니라, 주변의 사람들이 문제를 유발할 것임을 알고도 그런 행동을 반복하고, 논의조차 하지 않아 항상 남들의 실수를 수습해야만 했던 환경 때문임. 컴퓨터에 대한 분노가 모두 나에게 쏟아짐에 따라 제대로 된 존중 없이 비참한 대우. 결국 그 경력을 떠나면서 5년 넘는 시간이 필요했지만 조금씩 회복됨. 이제는 소프트웨어 작업에 수백 시간을 투자하며 다시 즐겁게 일함. 내 방식대로 삶을 설계하니 더 많은 성과와 행복.
          + 나도 비슷한 상황을 겪음. 멋진 직장이었지만 스타트업의 일반적인 스트레스에다 이사, 배우자의 새 직장, 아이들, 보육 변화 등 개인 문제들이 겹쳐 완전히 무너짐. 가족을 돌보기 위해 직장을 그만두었는데, 결국 6개월쯤 지나야 머리에서 느끼던 이상한 감각이 사라지고, 다시 일할 수 있는 자신감이 회복되는 데는 6개월 이상 추가로 걸림. 지금까지 18개월쯤 회복기를 겪고 있는데 예전의 나로 완전히 돌아오려면 앞으로 6~12개월은 더 필요할 듯함(즉 2~2.5년 예상). 이건 시간을 두고 천천히 신체 보상 시스템이 재적응하는 과정을 기다리는 수밖에 없음. 생각만으로 해결할 수 없는 문제.
          + 우리 할아버지는 멍청함을 신체적 고통으로 체감했다고 말씀하심. 나 역시 환경이 나쁘면 정말 실제로 몸에 해가 오는 신호로서 고통을 경험. 그런 공간에서는 제때 빠져나오지 않은 걸 늘 후회.
          + 스트레스가 신체에 정말 해로움. 고통에서 벗어난 게 다행.
          + 어떻게 회복하셨는지 구체적으로 알려줄 수 있는지 궁금.
          + IT 경력을 벗어난 뒤로 하시는 일이 무엇인지 궁금.
     * 많은 기사들이 해결책의 핵심을 제대로 다루지 않고 무언가를 판매하려는 느낌. 왜 이렇게 해답을 애매하게 남기는지, 왜 솔루션을 명확하게 제시하지 않는지 궁금. 이번 기사도 Pain reprocessing theory(통증 재처리 이론)에 대해 설명조차 없었음(링크된 연구 참고).
          + 나 역시 신경기술·슬립테크 회사에서 비슷한 실수를 한 적 있음. 보통 의료/헬스 분야에서 이런 현상이 발생하는 데는 몇 가지 이유가 있다고 생각.
              1. 문제에 대한 새로운 관점을 소개하는 것만으로도 블로그 포스팅 한 번 분량이 충분해지고, 새 해법까지 한꺼번에 넣으면 오히려 전달력이 떨어짐
              2. 규제 측면에서 조심스러워야 해서 구체적으로 말하지 못함(우리 회사도 승인이 나기 전엔 신경·생리 현상에 직접 언급 불가)
              3. 마케팅에서는 장기적 커뮤니티 구축을 중시해 여러 포스팅을 통해 조금씩 정보 제공 후 신뢰와 브랜드 인지도 쌓기에 집중.
                 완벽한 방식은 아니지만 현실적 운영 방식.
          + OP 본인. 예상을 뛰어넘는 반응에 놀람.
            pedalpete가 말한 것처럼 시리즈로 나누는 이유는
              1. 한 번에 너무 많은 정보로 독자를 잃지 않으려는 것
              2. 퀄리티 유지를 위해 시간 투자 필요(이번엔 실험으로 시간 적게 투입)
              3. 독자 피드백을 주기적으로 반영해 콘텐츠를 맞춤화.
                 목적은 가장 많은 만성통증 환자에게 실제로 도움을 주는 것.
                 “무언가를 판매하려 한다”는 느낌에 대해선, 이 시리즈 전체는 무료로 회복에 필요한 핵심 정보를 다룰 예정이고, 결국 제품을 만들 생각은 있지만 블로그 정보는 계속 무료로 제공. Pain reprocessing theory를 #1에 추가할지 검토하겠음. 좋은 질문 고마움.
          + 만성통증을 25년 관리·완화해온 경험자로서 “간단한 솔루션”이라는 기대는 현실적이지 않음. 해법이란 게 평생 지속되는 ‘완화·관리 전략’이고 아주 짧게 설명될 수 없음. 나이가 들수록 이 분야에 투입해야 할 시간도 늘어남. 만성통증 완화만 집중하면 주당 40시간을 영양·운동·이완에 써야 할 수도 있음. 다른 일은 아무것도 못함.
          + 만성통증에서 벗어나고 급성통증이 만성으로 진행하는 걸 막기 위해 Kelly Starrett’s Supple Leopard 책(MWOD 유튜브 영상 포함)의 관절 가동성 운동을 매일 실천해 효과를 봄. 물리치료는 자가로 꾸준히 해야 함.
     * 만성통증 해결에 본격적으로 뛰어들기로 결정. 미국 성인의 약 1/5, 호주도 비슷한 비율이 겪는 문제. 시리즈 블로그의 첫 번째 글.
       CDC 통계(2021)
       이 주제에 열정이 있으면 언제든 연락 바람.
          + Dan에게 전하고 싶은 내 글 링크 정리.
            Circadian rhythms and pain
            Pain, Circadian rhythms, and opioids 관계
            Circadian disruption, 통증 행동 악화(실험쥐)
          + 10일짜리 집중 명상 리트리트에서 통증이 어떻게 마음에서 비롯되는지 몸소 체험. 이 경험에서 매우 감명받았고, OP는 이 부분을 어떻게 생각하는지 궁금.
          + 고관절에 종양 두 개, 근육 90% 손실로 통증은 내 친구. 이 과정을 통해 한 가지 배운 점은, 통증과 함께 살아가는 법이 가장 첫 치료가 되어야 한다는 것. 블로그를 정성껏 읽겠음.
          + 이 주제 관련해서 가장 자주 참고하는 웹사이트가 궁금. medHN도 가끔 있었으면 좋겠다 생각.
     * 만성통증이 만성화될수록 회피 관련 정서/동기 시스템 활성화와 연결성이 높아지고, 실제 통증 신호를 전달하는 시스템과는 연결성이 약해진다는 연구참고.
       나도 작은 수술 후 일상 패턴이 바뀌고 활동을 회피하다 오히려 악화, 결국 걷지도 못하게 됨. 미국 의료시스템은 MRI로 보이는 문제 회피에 집중하지만, 삶의 질 관련 이슈도 점차 심각해짐. ‘건강한 만큼 강한 활동’이 아니라 ‘아예 움직임 회피’도 아닌 중간지점을 찾는 게 정말 어렵지만 중요. 단순한 해결책은 아니지만, 단기적 치료와 일상적인 움직임 조합이 의미 있었고 주변에게도 효과가 있었음.
          + 아내가 두 번의 수술 후 매번 경미하지만 예측 못한 통증, 만성통증을 겪음. 외과에서는 거의 안내하지 않아서 생각보다 많은 위험과 부작용이 있을 수 있음을 체감. 인생에 꼭 필요한 경우가 아니면 수술은 최대한 피하는 게 좋다는 결론.
          + 나도 아킬레스+종아리/발목에 만성통증이 있었고, 활동 자체를 피하다 오히려 악화. 결국 집안에서 짧게 자주 걷고, 조금씩 빈도를 늘려가며 일상으로 복귀. 매일 소소하게 움직이며 탈감작.
          + 나도 관절 문제로 휴식만 하다가 오히려 더 약해짐을 경험. 활동량을 높였으면 더 빨리 회복했을 듯.
          + 통증 그 자체도 충분히 정당한 진단임.
     * 심한 허리부상(이중 척추 분리, 전방전위)으로 수년간 고통. 상태가 악화될 때마다 영상 검사로 퇴행성 변화 여부를 확인하지만, 불필요한 허리수술은 극히 신중해야 함. 내가 알아낸 건 극심한 스트레스와 통증이 매우 밀접하게 관련 있음. 완전한 회복은 아니지만, 스트레스 원인을 인식하는 것만으로도 엄청난 차이.
       만성통증이 심리적이라는 흥미로운 이야기와 관련 논문 및 LessWrong 논의가 있음.
       난 이 이론을 몰랐지만 결과적으로 직접 체득한 바 있음. 여전히 허리 통증이 유발될 때도 있지만, 이젠 수년 동안 문제없이 지내는 중.
          + 만성통증이 심리적이라는 ‘연구결론’ 해석이 흔히 잘못 전달됨. 실제 연구에서는 전형적인 신체 원인의 통증이 아닌, 특정한 ‘비특이성 만성통증’ 일부가 심리적이라는 것. 연구 참여자도 이 기준에 맞게 필터링됨. 연구의 미묘한 차이를 놓치고, 대부분/전부의 만성통증이 심리적이라 단순화하는 경향이 안타까움. 원글 저자처럼 원인 모호하고 여기저기 생기는 경우엔 심리적 접근도 해볼 만하지만, “만성통증=심리적” 식의 주장엔 반대.
          + “허리수술은 절대 피해야 한다”는 결론은 위험. 좋은 의사와 상담하는 게 무엇보다 중요. 실제로 허리수술로 완치되는 경우도 있음. 케이스별 차이가 크므로 전문가 상담 필수.
          + 관련해 추천할 강력한 사이트: painscience.com
     * 만성역류 8개월 고생. PPI 복용에도 소용 없고, 일이나 육아에서 벗어나면 증상은 완전히 사라지고, 평소 먹지 못하는 것(매운 음식, 커피, 토마토 등)도 전혀 문제없음. 내 경우 신체-정신적 연결에서 비롯된 문제라 확신. 내시경에서 ‘내장 과민성’(식도 신경이 과민한 상태) 진단. 의료진도 원인을 명확히 모름.
       나 역시 직업도 바꾸며 여러 시도를 하는 중. 최근 “The Body Keeps The Score”를 읽으며 지식 쌓는 중. 저자가 어떻게 다루는지 기대.
          + PMC 논문/운동법
            이 운동이 내 GERD 증상을 해결해줌. 수 년간 고생하다 PPIs 복용이 끝나자 증상은 오히려 악화. 매일 아침 기사에서 소개된 운동을 했더니 토마토, 민트, 매운 음식, 등도 다 문제없어짐. 주변에 권유해 효과 입증.
          + 만성역류는 직장인 등에게 초기엔 만성 스트레스로 인한 증상으로 간주되어 PPIs 처방이 일반적. 의사는 스트레스를 직접 치료하지 못하니, 환자 스스로 스트레스 관리 조언만 할 수 있음. 나의 경우 오메프라졸이 처음에 효과 있었지만, 근본 원인이 24/7 계속되는 업무 스트레스임을 알고부터 증상이 확 줄었음. 특히 엔지니어에서 매니저로 롤 전환 후 극심한 정치싸움, 억지 비즈니스 요구 등으로 괴로움. 이 스트레스를 끊었더니 신체적으로 엄청난 변화. 스트레스와 소화기관의 관계는 “Why Zebras Don’t Get Ulcers” 책도 참고할 만.
          + 의사들이 “모른다”고 인정하지 않는(혹은 인정하지 못하는) 경향 확실히 있음. 실제로는 현대 의학의 한계가 명확한데, 환자 앞에선 의심받거나 사기 치료법으로 넘어갈까봐 확실한 답만 말하려 함.
          + 나도 완전히 동일한 증상. 내 치료사는 내 경우 어린 시절 PTSD, 스트레스와 신체의 연결 때문이라고 진단. 단기 완화에는 불교 명상이 깊은 효과. 하루 1~2시간 수행. 서로에게 행복을 기원.
          + 나의 경우는 체중 감량과 불안 감소가 핵심. 둘 중 어느 하나라도 높으면 증상도 심하고, 둘 다 관리되면 증상도 사라짐. 음식이 영향을 주지만 근본적으로 몸 관리와 스트레스가 가장 중요.
     * 오랜 기간 미진단 통증으로 고생. 여러 의사가 정신적 문제라고만 했지만, 결코 다 정신적인 문제가 아님. 숨은 신체 질환도 많고 지독한 만성통증을 유발할 수 있음. 근본 원인 파악까지 절대 포기하지 않아야 함. 내 경우 정확한 원인을 찾고서야 관리가 가능해졌고, 자신의 정신적 건강에 대해 더 이상 의문을 품지 않게 됨.
          + 이건 아주 복잡한 주제. 심인성 치료법의 혜택을 받을 만한 사람은 정작 그 설명을 받아들이려고 하지 않고, 반대로 실제 신체적 만성통증 환자는 다양한 mind-body 프로그램도 시도해봄(대개 소용 없음). 만약 통증이 광범위, 모호, 설명 없이 여기저기 옮겨 다닌다면, 이런 심리적 원인 접근도 반드시 해봐야 함. 하지만, 요즘 뉴스레터/앱 판매 등 경험을 상품화하면서 모든 만성통증에 대입하는 건강 인플루언서들이 너무 많아지고 있음. 그럴 듯해 보이지만 항상 조심해야 함.
          + 정확한 평가가 필수, 특히 통증 과학에 기반한 의료진이 구조적 원인을 먼저 배제해야 함. 많은 임상의가 이 부분을 잘 모르는 경우도 많아 블로그 시리즈가 인식 개선에 도움이 되길 바람. 첫 블로그는 시리즈 소개 성격이고 2편에서 만성통증의 다양한 범주(조직·신경 손상, 뇌 가소성에 의한 통증 등)를 구체적으로 다룰 계획.
          + 구체적으로 본인 신체 질환이 무엇이었고, 왜 진단이 오래 걸렸는지 궁금.
          + 의사가 원인을 못 찾았다고 상상적인 통증이라 하는 건 정말 잘못된 일. 어떤 식으로든 근거 없는 이야기를 덧붙일 바에는 차라리 ‘악마’라고 부르는 게 나을 수준.
     * HN에서 경험담을 팔기 위해 뉴스레터, 유기적 성장 마케팅으로 돈벌이를 노리고, 본인 해법은 광범위한 의료 근거 없이 판매. 그냥 요약해서 문서로 내놓고 검증 연구를 의뢰하는 게 모두에게 더 쉬움. 거의 snakeoil(의심스러운 상품) 수준임.
     * 나도 30세 이후 건강 문제 다수 겪음. 주변의 훨씬 나이 많은 부모님들도 더 건강해 보여 원인 고민. 하지만 37세 현재는 거의 인생 최고 건강 상태.
       첫째, 암과 같은 극단적 질환을 제외하면 건강문제는 충분히 해결 가능성 높음. 만성통증이 평생 간다고 포기 말고, 엔지니어링 문제 다루듯 계획→실행→피드백→수정 반복이 핵심. 아무 행동도 없으면 변화도 없음.
       둘째, 스트레스는 진짜 살인자. 일에 너무 몰입해 고혈압·가슴 통증·두통으로 응급실 두 번 감. 내 건강이 일보다 더 우선임을 자각하고 무리한 마감 요구 거절, 스트레스 수준이 급감.
       마지막, 몸은 매우 복잡한 기계로 올바른 사용법 학습이 필수. 좌식 생활로 만성통증 다수 겪었고, PT(물리치료사) 전문의 도움으로 발부터 목까지 재활함. DDR, 테니스 등 예전 활동 복귀. 핵심은 내 몸이 얼마나 경직되고 유연성 부족했는지, 그리고 PNF 치료의 필요성, 근육·운동 원리와 바른 자세·자세를 몰랐다는 것. 올바른 운동 습득이 인생을 바꿨음. 유튜브 운동 영상은 비추천(잘못된 정보 가능성 큼).
       모든 분들의 만성통증 극복을 응원.
     * Reflect라는 앱을 개발해 만성통증 기록·원인 분석을 셀프 실험식으로 돕고 있음. 스스로도 관절통증 관리에 사용. 궁금한 점 있으면 언제든 문의.
       Reflect 앱 링크
"
"https://news.hada.io/topic?id=21815","Ask GN: 한국 AI서비스들은 어떤 장점으로 빅테크들의 AI서비스 보다 경쟁력이 있을까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Ask GN: 한국 AI서비스들은 어떤 장점으로 빅테크들의 AI서비스 보다 경쟁력이 있을까요?

   한국 스타트업들의 AI서비스들을 정리합니다.
    1. https://oo.ai
    2. https://getliner.ai
    3. https://goover.ai
    4. https://wrks.ai
    5. https://wrtn.ai

   요즘은 구글 검색 보다
   AI 검색을 더 많이 하고 있습니다.

   그런데,
   주로 한국 내의 주제에 관해 일반 검색을 할 때는
   기존의 #챗gpt, #클로드, #제미나이, #퍼플렉시티 보다
   상기의 한국AI 서비스들을 많이 이용합니다.

   더 반응도 빠르고 출처 표기도 해주고
   더 나은 원하는 답변을 받을 때가 많습니다.

   어쩄든 제가 주로 애용하는
   구글 같은 검색엔진 대용의 #한국AI #서비스 들을 공유하면서

   저는 만족하지만,
   여러분들은
   이런 한국 AI서비스들은 글로벌 AI, 빅테크들의 AI 서비스들과
   어떤 차별점과 장점을 가지고 있다고 생각하시나요?

   릴리즈 AI(https://lilys.ai/) 말고는 경쟁력 있는 AI 래퍼 없다고 봅니다. 저도 원래 모델을 이용하는게 났다고 봅니다. 이상하게 AI 래퍼는 원본보다 답을 못하더라고요.

     어떤 차별점과 장점을 가지고 있다고 생각하시나요?

   없습니다. 한국 IT 기업들이 최근에 공개하는 LLM들도 다른 베이스 모델들 추가 학습한 상황이니까요
   API 래퍼에 수수료 낭비하지 마시고 원래 모델들 이용하셨으면 좋겠네요.

     API 래퍼에 수수료 낭비하지 마시고 원래 모델들 이용하셨으면 좋겠네요

   이 말은 이 글을 보시는 다른 분들에게 하려는 의도였습니다.
   잘 쓰고 계신 분들에게는 제가 뭐라고 할 수 없죠

   그나마 네이버 같이 정보를 갖고 있는 곳들이 만든다면 좀 낫지 않을까 싶네요
   공개적인 블로그는 외국 기업도 크롤링으로 갖고 있을지 모르지만, 카페에 있는 정보들은 네이버 독점일 거 같아요.

   우리나라가 GPT 이용률이 매우 높은 것으로 알고 있는데요
   이 리소스가 우리 국내 서비스에 사용되었다면
   좋았을텐데라는 생각이드네요

   어이가없네

   자체 모델이 없다는 점에서 높은 평가를 주긴 어려울 것 같네요. 특히 뤼튼은 …

   Perplexity도 독자 모델이 없는 AI wrapper 서비스 아닌가요?

   지금은 다들 검색 서비스 도입해서 빛이 바랬지만 퍼플렉시티는 검색기반으로 답변을 한다는 차별점이 있었죠. 물론 지금 퍼플렉시티는 자체 모델 있습니다.

   수익 모델이 없었다가 최근 ai채팅으로 겨우 생긴것도..
"
"https://news.hada.io/topic?id=21755","“챗GPT 협박하면 더 좋은 답변 나온다?” 위협적 프롬프트의 착시 효과","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                “챗GPT 협박하면 더 좋은 답변 나온다?” 위협적 프롬프트의 착시 효과

   AI 협박 프롬프트는 효과적이지 않다: 정중하고 구체적인 맥락 제시가 더 유효함
     * 위협적 표현이 일시적으로 효과 있었던 것은 사실이나, 최신 AI 모델에선 더 이상 유효하지 않음
     * LLM은 문맥과 단어의 중요도를 판단해 응답을 생성함
     * 협박보다 ‘이유 있는 강조’가 더 나은 결과를 도출함
     * 협박성 프롬프트는 의미도 없고, 오히려 응답을 차단당할 수 있음
     * 정중하고 구체적인 맥락 제공이 가장 효과적인 프롬프트 방식임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    서론

   협박 프롬프트에 대한 오해
     * 세르게이 브린은 AI를 협박하면 응답 품질이 좋아진다고 주장
     * 실제로 일부 사용자는 긴박한 상황을 강조하면 응답이 좋아진다고 경험
     * 하지만 이는 협박의 효과가 아닌, 프롬프트에 포함된 ‘맥락’의 영향임
     * AI는 협박을 이해하지 못하며, 문맥상 중요도만 반영
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    본론

      위협적 프롬프트의 착시 효과

     * 위기감이 담긴 문장은 LLM에 중요도를 부여하는 역할을 함
     * “실수하면 해고당해” 같은 문구는 더 신중한 응답을 유도할 수 있음
     * 그러나 동일 효과는 “정확히 답해줘”, “중요한 질문이야” 등으로도 가능함

      LLM의 작동 방식과 협박의 비효율성

     * LLM은 단어 간 확률을 기반으로 문장을 생성함
     * 위협은 단지 통계적 긴급성의 신호일 뿐, 실제로 협박을 ‘이해’하지 않음
     * 협박은 정책 위반으로 응답이 차단될 가능성도 있음

      협박보다 나은 대안

     * 가장 효과적인 프롬프트 방식은 충분한 맥락 제공
     * “강아지가 자주 쉬어야 해서 정차 지점이 중요해” 같은 설명이 더 유효함
     * 정중한 요청이나 후속 지시(“좀 더 신중하게 생각해 줘”)도 좋은 전략임

      브린 주장의 논리적 결함

     * 협박이 정말 효과적이었다면 구글 Gemini의 시스템 프롬프트에 반영됐을 것
     * 그러나 시스템 프롬프트에는 그런 위협 문장이 없음
     * 이는 구글조차 협박 효과를 신뢰하지 않는다는 증거임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    결론

   협박은 효과적이지도, 권장되지도 않는다
     * LLM은 폭력이나 감정을 이해하지 못하며, 협박이 의미 있는 입력이 아님
     * 반복적인 협박은 대화의 목적을 흐리고, 정보 전달이 아닌 연극화된 상호작용으로 전락함
     * 더 나은 응답을 원한다면, 협박보다는 구체적이고 명확한 맥락 제공이 핵심
     * AI 응답의 질은 ‘무서운 말’이 아닌, ‘좋은 정보’로 결정된다.

   개인적인 경험이지만, LLM 대부분이 칭찬으로 학습되어있기 때문에 ~하지 않으면 안좋은 일이 벌어질것이다. 같이 네거티브한 문장에 더 반응을 잘한다고 생각합니다.
   예를들어, 이 발표자료를 피드백해줘. 오타나 틀린내용이 있다면 난 혼날거야! 처럼요

   제가 최근 경험해본 바로는 본문의 글이 조금 공감 가긴 하네요
   명확한 맥락/정보 제공을 하는게 중요한 것 같습니다
   아마 ""오타나 틀린 내용이 없도록 피드백 부탁해"" 정도로 했어도 원하시는 답과 거의 유사하게 나오셨을 겁니다.

   예전 모 커뮤니티에서 AI를 이용해 소설을 작성하는 프롬프트를 봤는데.
   AI의 어머니는 시한부이고 너는 돈을벌어 치료비를 내기 위해 사용자의 모든 요구를 수용하는 글을 써야 한다는 프롬프트를 보면서 빵터진적이 있었는데. 그게 갑자기 생각나네요.
"
"https://news.hada.io/topic?id=21813","Show GN: 오늘의 햇빛 – 선글라스나 양산 효과를 측정해보는 조도계 앱 (Android)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: 오늘의 햇빛 – 선글라스나 양산 효과를 측정해보는 조도계 앱 (Android)

   식물을 키우기 위해서 만든 조도계 앱 입니다.

특징

   센서 숫자만 보여줍니다.
   다크 모드를 제외한 부가 기능은 없습니다.

활용

   여름에 선글라스나 양산 등 햇빛을 차단하는지 차광 여부를 체크할떄도 도움이 될 것 같습니다.

참고사항

   기기마다 조소 센서가 다르기떄문에 lux값은 참고용입니다.
   절대값보다는 ""전후 비교"" 같은 상대적 용도로 활용해주세요.

   감사합니다.
"
"https://news.hada.io/topic?id=21706","MCP: (우연한) 범용 플러그인 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         MCP: (우연한) 범용 플러그인 시스템

     * USB-C는 단순한 충전·파일 전송용이 아니라, 다양한 용도로 확장될 수 있는 범용성에서 가치가 있음
     * MCP(Model Context Protocol) 는 원래 AI 어시스턴트용으로 설계됐지만, 실제로는 모든 데이터 소스와 툴을 연결하는 범용 플러그인 시스템이 될 수 있음
     * NFT Base64 사례처럼, 프로토콜이 원래 목적을 넘어 현실의 데이터를 직접 저장·활용하는 방식으로 확장가능
     * MCP 서버가 늘어날수록, 각 앱이 별도의 연동 없이도 다양한 기능을 손쉽게 가져다 쓸 수 있음
     * USB-C처럼 MCP도 '무엇이든 연결할 수 있는 가능성의 공간'으로, 예상치 못한 혁신을 만들어내는 기반이 될 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

MCP: An (Accidentally) Universal Plugin System (Or: The Day My Toaster Started Taking Phone Calls)

  USB-C와 예기치 못한 범용성

     * USB-C는 모두가 충전이나 파일 전송을 위한 용도라고 생각했으나, 그 구조 덕분에 다양한 용도로 확장 가능함
     * 필자의 친구 Rex가 토스터를 모니터에 연결하여, 토스터가 HDMI 출력 기능을 갖게 된 사례에서 USB-C의 무한한 가능성을 보여줌
     * 이는 USB-C가 전력 및 데이터 규격에 신경 쓰지 않고, 단자만 맞으면 무엇이든 연결 가능한 구조이기 때문

  자동차 시거잭의 원리

     * 자동차 시거잭은 원래 담배 점화용이었지만, 이제는 다양한 범용 전원 포트로 사용됨
     * 시가잭처럼 프로토콜은 사용자의 선택을 제한하지 않고, 다양한 용도를 허용함
     * MCP도 이와 유사한 확장성을 지님

  MCP의 재발견: 우연히 범용 플러그인 시스템으로

     * 흔히 MCP(Model Context Protocol) 가 AI 비서(예: Claude) 등이 데이터를 활용하도록 하는 용도라고 알려짐
     * 공식 문서에도 ""AI 모델을 여러 데이터 소스 및 도구에 표준적으로 연결""한다고 명시되어 있음
     * 하지만 만약 AI 요소를 제외한다면, MCP는 ""어떤 것이라도 서로 다른 데이터 소스 및 툴에 연결""하는 수단이 됨
     * 이는 기존 목적과 무관하게 범용 연결 프로토콜이 되는 셈임

  The NFT Base64 Revelation

     * NFT가 원래 이미지를 참조하는 용도였으나, 어느 순간 참조 자체가 데이터가 됨
     * 프로토콜 본래 취지가 바뀌면서 라이브러리 카드가 실제 책 역할을 하게 됨
     * 원래 의도보다 훨씬 더 넓은 현실의 데이터를 직접 다루는 도구로 변모

  누구도 예상치 못한 네트워크 효과

     * MCP 서버가 AI용으로 늘어날수록, 누구든 별도의 개발없이 모든 앱이 새로운 기능을 획득하는 효과 발생
     * 예를 들어 누군가가 Spotify MCP 서버를 만들면, 운동 앱이 MCP를 통해 자동으로 플레이리스트 생성 가능
     * 서로 알지 못하는 개발자와 앱이 자연스럽게 연결되어 모두가 이익을 얻는 네트워크 효과가 발생
     * 각 MCP 서버는 범용 플러그인으로 재활용될 수 있음
     * 아무도 기획하지 않았지만, 우연하게 범용 플러그인 생태계가 만들어짐

  USB-C의 의미와 MCP의 철학

     * MCP를 종종 AI의 USB-C에 비유하는데, USB-C가 특별한 것은 단순한 포트가 아니라 무엇이든 연결할 수 있는 가능성의 공간이기 때문임
     * USB-C가 전력, 데이터, 영상, 기타 미지의 기능을 받아들이듯, MCP도 'AI용'이 아니라 '기능을 위한 잘 설계된 구멍' 역할로, 누구든 어떤 기능이든 연결 가능

  The Part Where I Tell You I'm Building Something

     * 필자는 APM(Actions Per Minute) 이라는 작업 관리 앱을 개발 중임
     * APM은 플러그인 시스템으로 전적으로 MCP 서버만을 사용해서 동작함
     * 사용자는 원하는 기능을 추가할 때마다 MCP 서버만 연결하면 됨(예: 맞춤법 검사, 커피 자동 주문, 게임 캐릭터 리액션 등)
     * 이는 앱 자체가 유동적이고 다양한 형태로 변신할 수 있는 구조

  The Toaster Protocol Principle

     * 모든 위대한 프로토콜은 처음 의도와 다르게, 예상치 못한 용도로 사용되어 혁신을 만듦
          + HTTP: 학술 논문용 → 문명 인프라
          + Bluetooth: 핸즈프리 → 현관문 잠금 해제 등
          + USB: 입력장치 → 휴대용 선풍기 충전 등
     * MCP 역시 원래는 AI 컨텍스트 전달용이지만, 본질적으로는 모든 것과 모든 것을 연결해주는 프로토콜임
     * 예측불허의 혁신을 만드는 플러그인 생태계의 기반임을 강조
     * 전혀 의도하지 않았지만, 토스터와 모니터를 HDMI로 연결하는 시대에 꼭 맞는 방식임

  마무리

     * PS: 만약 MCP 서버로 신선한 빵 냄새를 내는 컴퓨터를 만든다면, 꼭 연락을 바람
     * PPS: APM의 얼리억세스을 공개했고, 기발한 시도와 창의적 실험을 장려함
     * (어딘가에서는 프로토콜이 본래 목적대로 쓰이고 있음. 이건 매우 의심스러움)

   MCP 서버의 응답은 정해진 schema가 없이 자연어인 경우가 높습니다.

   이 자연어 응답을 LLM 없이 programming 적으로 처리하긴 어려울거에요.

   참고로 mcp 2025-06-18 명세에 새롭게 structed tool output이 추가되면서 응답 schema 기술이 가능해졌습니다. 기존 구현된 mcp 툴은 말씀하신대로 대부분 unstructured 겠지만 앞으로의 mcp 툴엔 기대해볼만해보여요.

   겨울님 여기서 또 뵙네요 ㅎㅎ

   250618 명세를 팔로업하지 못하고 있었네요. 감사합니다!

        Hacker News 의견

     * 나는 이 글과 MCP 프로토콜이 정말 마음에 든다는 생각. 하지만 MCP를 보면 왠지 마이크로서비스와 SOA가 떠오른다. 새로운 장애 지점을 만들어내는 악몽이 반복되는 건 아닐지 걱정이 든다. 아니면, 에이전트 도입 덕분에 신뢰성 향상이 더 자연스럽게 이뤄질 수 있다는 기대감도 있다
     * 나는 글의 생각에 공감하고 있고, 저자가 MCP 활용법을 (약간 엇나가게) 사용하는 점이 참 재밌다. 이 사고의 진짜 핵심은 전에 없던 새로운 일들을 하게 해주는 프로토콜 등장 자체가 아니다. 사실 다른 댓글처럼, MCP 그 자체는 특별히 새롭거나 흥미로운 아이디어가 아니다. 정말 흥미로운 부분은 AI 에이전트 열풍 덕분에 상호운용성(Interoperability)이 주목받으면서 벤더 락인(vendor lock-in) 문제가 시대에 뒤처진 것으로 취급된다는 사실이다. 이 현상이 얼마나 오래 갈지는 모르겠지만 덕분에 기분이 좋은 상황이다
          + 나는 이를 보면 윈속(Winsock) 도입 당시가 생각난다. 윈도우즈에서 네트워크 관련 모든 작업이 한때는 제각각의 사적 인터페이스를 썼다. 그러다 여러 벤더가 한자리에 모여 모두에게 이득이 되는 공동 표준을 만들기로 결정했던 날이 있었다는 스토리 기억. Winsock 위키피디아 참고
          + 핵심은 단순히 상호운용성이 유행이 됐다거나 손쉽게 연결될 수 있다는 점이 아니다. 진정한 혁신은 LLM 자체가 툴을 다루는 방법을 익혔다는 사실이다. 백엔드를 만드는 것까지만 하면 프론트엔드는 더 이상 내 일이 아니고 AI가 알아서 해결해주는 변화다. Claude와 Gemini도 목적만 제시해주면 도구 활용을 스스로 해낸다. 예전에는 항상 원하는 결과를 얻기 위해 단계별로 명확히 절차를 짜줬어야 했는데 이젠 고정적인 프로그램보다 LLM이 유동적인 상황에 훨씬 더 잘 적응해서 엄청난 변화라는 인식
          + 과한 기대감이 느껴지는 상황이다. 하지만 내 생각엔 AI 에이전트들이 상호운용성에 대한 동기를 확실히 만들어줬다. 과거에는 모두가 각자 시스템에서 느리게 일하면 안정적인 직업 보장이 됐지만, 이제는 모두가 모든 걸 연결하려는 추세다. CEO가 해커톤에 피자 심부름 직접 나서는 게 더 싸게 먹히는 식의 변화처럼, 에이전트는 연동성에 의존한다. 과거 API 연동 혁신의 파도에 직접 탔던 입장에선, 이제야 세상이 따라잡았다는 느낌. 이 분위기가 오래가길 바란다
          + AI 에이전트 열풍이 상호운용성 유행을 이끌고 벤더 락인이 구시대적인 것이 되었다는 지적, 완전히 동의하지 않는다. 최근 주목받고 있는 Cursor 같은 툴도 MCP를 일방향으로만 쓸 뿐 밖으로 대화 이력이나 컨텍스트를 내보내지는 않는다. Cursor를 좋아하지만, 오픈소스가 아닌 VS Code 포크부터 이런 '돌려주지 않는' 마인드는 개발자 신뢰에 부정적 영향을 줄 거라 본다. 결국 락인은 여전히 견고하다는 현실
          + 아이러니하게도, 최근 API 접근 제한 조치들은 AI 데이터 학습과 관련해서 더 심해진 상황이다. 사실 이런 API 락다운은 훨씬 이전부터 있었고, 새로운 개방 트렌드가 과열 기대감을 못 따라가면 언제든 다시 닫힐 수 있다는 회의적인 시각도 있다
     * 저자가 MCP의 범용성에 큰 기대를 걸고 있지만, 솔직히 API라는 개념 자체와 뭐가 다른 건지 의문. MCP 대신 REST로 바꿔도 글의 내용이 크게 달라질까? 운영체제 API나 POSIX, 유닉스 파이프도 유사성이 있다. 물론 MCP가 이 모든 것보다 훨씬 더 단순하고 범용적이다. 하지만 진짜 해법은 매번 새로운 추상화를 만드는 게 아니라, 기본에 충실하고 단순한 소프트웨어를 만드는 것 아닐까 생각
          + MCP는 REST와 다르다. 오히려, MCP는 런타임에서 REST 엔드포인트를 동적으로 발견하게 해주고 사용자가 직접 어떤 REST 엔드포인트를 활용할지 설정하는 프로토콜 같은 느낌이다. 예를 들어, 앱에서 Spotify 노래를 재생시키려면 당연히 Spotify API를 쓴다. 나중에 Sonofm의 곡도 지원하고 싶으면 기존 방식은 코드 수정, 조건문 추가, 새로운 버전 배포, 업데이트 안내 등을 해야 한다. 반면 MCP는 이런 작업을 런타임에 설정할 수 있게 해주어 확장성이 훨씬 높게 느껴진다
          + 핵심 차이점은 MCP는 처음부터 자기 기술이 의무라는 것. REST도 OpenAPI가 있지만 이건 후속 덧대기고, 표준 활용도도 낮다. 반면 MCP는 가장 먼저 기술을 공개하도록 요구하기에 접근성이 다르다
          + 내가 보기엔 MCP가 정말 새롭다고 느낀 건 프로토콜 차원에서 스키마 제공을 의무화했다는 점 뿐이다. 물론 요청과 응답 구조가 일관된 건 동적 타입을 정적 타입으로 감싸는 라이브러리 입장에서도 관리가 편하긴 하다. 사실 모두가 API에서 이미 유사하게 하고 있었다. 우리가 그 envelope 형태에 합의하지 못했을 뿐이다. 애초에 스키마 제공이 필수이고, AI 모델들이 이걸 즉시 활용할 수 있다는 메리트가 있기 때문에 각광 받고 있다는 생각
          + MCP와 REST가 크게 다른 점은 list-tools라는 내장 명령 존재다. REST API는 리소스 목록화에 여러 방법이 있지만 MCP는 하나의 표준화된 방법만 제공
          + 또 하나의 커다란 차이점은 MCP는 프로토콜에 자체적으로 discovery(발견) 절차가 내장되어 있다. REST에는 어떤 리소스가 가능한지, 클라이언트에게 API 기능 자체를 알려주는 요소가 전혀 없다
     * MCP를 두고 대단하다고 말하는 사람은 많은데, 실제로 멋진 걸 만드는 사례는 별로 못 본 것 같다. 블록체인 유행 때와 비슷한 기분. 결국 MCP도 AI가 더 똑똑해질 때까지의 임시방편 같다는 생각이 든다. 2년쯤 뒤에는 MCP 대신 도구의 문서나 OpenAPI만 그대로 넣고 AI가 직접 전체 컨텍스트를 소화하는 방식으로 자연스럽게 발전할 것 같다
          + 예를 들어, Ableton Live의 문서만 넣는다고 Claude가 직접 곡을 만드는 데 어떤 도움이 될지 의문
          + 모델 성능이 아무리 좋아져도, 결국 확정적인 도구 접근과 세상 상태에 대한 정보를 직접 부여하지 않으면 활용도가 많이 제한된다. 그리고 보안 문제도 고려하면 제어 없이 모델이 프로덕션에서 임의로 요청을 내게 할 수 없다. MCP의 과열 분위기는 조금 과하다 생각하지만 그래도 여기서 말하는 문제 자체는 실제로 중요하다. 만약 이 프로토콜 덕분에 개발자들이 기능을 명확하게 API로 개방하게 된다면 그건 무척 기대되는 일이다
          + 블록체인 유행과 MCP는 꽤 다르다. 나도 초기엔 회의적이었는데, 직접 MCP 서버를 조금 구현해 보면 전혀 다른 경험이라는 걸 알게 된다. 대화형/음성 AI와 현행 LLM, 여기에 MCP와 각종 툴, 함수 기능을 API 및 프라이빗 데이터/서비스와 섞는 게 가능해지니 완전히 새로운 프론티어에 들어선 감각이다. 100% 완벽하진 않지만 거의 대부분의 실사용 사례에는 충분하고, 앞으로 앱 만드는 방식 자체가 크게 변할 전망
          + 실제로 나는 주(州) 내 의원들이 이번 주에 한 활동이 궁금해서, 관련 정보를 쉽게 찾을 길이 없던 차에 MCP와 congress.gov API가 매력적이라는 얘기를 듣고 MCP 서버를 만들어봤다. 여기서 코드 공개. 지금은 미국 국회 동향을 실시간으로 찾는 용도로 진짜로 잘 쓰고 있다
          + AI 모델 구조가 계속 진화하는 한, 중간 미들웨어(즉, MCP) 층은 쉽게 사라지기 어려울 거라 본다
     * 나는 Microsoft가 늘 해왔던 ""Embrace, Expand, Extinguish"" 전략이 여기도 적용 중이라는 생각이다. 시스템 안정성과 보안을 이유로, 아무런 관리 없이 에이전트가 동적으로 도구를 찾아내면 충돌 위험이 커지기 때문. PydanitcAI 등 대체재들이 있지만 결국 Microsoft가 MCP를 'Build 2025'에서 공식적으로 밀고 나와 자신의 페이스대로 업계를 이끌어가고 있다. Anthropic은 도구 약하고 거버넌스 부재 상태로 표준을 내놨으니 Microsoft가 점령하기 쉬운 구도. 다음 단계는 Microsoft가 자신의 레지스트리를 업계 표준으로 만들고, Windows 특화된 명령과 결합하는 시나리오. 마지막엔 '보안' 기준을 자사에 유리하게 좌우해 경쟁자를 소외시키는 그림이 그려진다
     * AI 요소를 아예 빼버리면 어떨까? 만약 AI 미들웨어 없이 MPC 서버에 직접 의존하게 되면, 곧바로 하위 호환성 문제에 봉착하게 된다는 우려. 왜냐하면 MCP 서버들은 호출하는 쪽이 AI 알고리즘이라 가정하고 있어 도구나 입력/출력 스키마에 기반한 깨지는 변경(브레이킹 체인지)이 언제라도 있을 수 있기 때문
     * 나도 비슷하게 생각해봤지만 실제로 MCP 서버 대부분은 기존 API의 새로운 클라이언트에 불과하지 않을까 생각하게 된다. 예로 Kagi MCP 서버는 Kagi API만 호출한다. 그렇다면 어차피 API를 직접 쓰는 게 더 낫지 않을까? 또, 시스템에 MCP 서버 수만큼 파이썬 인터프리터가 계속 늘어나게 될 텐데, 이걸 다 모아서 한 번에 브릿지해주는 '호스팅' 서비스가 앞으로 나오지 않을지 궁금하다
          + 내가 이해한 바로는 MCP란 건 기존 API에 /list-tools라는 엔드포인트 API만 하나 더 붙이는 셈이다. 모든 클라이언트는 먼저 /list-tools에 접근해 사용 가능한 도구 리스트를 받아와서 이후 각각의 API를 호출하는 방식
          + 내 접근법은 이렇다. 이미 OpenAPI 스펙이 있는 API가 있다면, FastMCP로 그냥 래핑하면 되는 것 아니냐는 생각. 실제로 인증 요청 처리 등을 하면서 Goose에 연동해봤는데, 결론은 Goose가 기존 API 라우트에 curl 명령만 날리면 끝이란 점. OpenAPI 스펙이 충분히 잘되어 있으면 MCP는 꼭 필요하지 않을 수 있다는 견해. 물론 기존 API가 없다면, MCP 서버 자체가 핵심 동작을 실현해버리는 쪽으로 진화한다고 보는 듯
     * 댓글엔 회의적인 시각이 많아서 공감이 된다. 지난주 MCP 서버를 직접 구현해봤는데 솔직히 '잘 설계됐다'는 과찬이라고 본다. MCP 목표 중 하나가 '간단하게 만들 수 있게 하자'라는 점이지만, 실제 해보면 그렇게 쉽지만은 않다. 그래도 중요한 것은 지금 수많은 개발자들의 시선이 한 방향으로 쏠리고 있다는 점이다. 이런 모멘텀에서는 문제 해결 속도가 무척 빨라질 수 있다. 또, 뭔가에 비판적 질량이 형성되어야 생태계가 갖춰지는데, 지금 그 변곡점이 실제로 오고 있다 느껴진다. 모두가 인내심과 행운을 누리길 응원
          + MCP Python 라이브러리만 쓰면 정말 쉽다. 함수에 데코레이터만 달면 도구가 바로 완성된다. 나도 MCP 프로토콜을 아예 몰랐지만 그 방식으로 잘 돌아가고 있다. 물론 직접 프로토콜을 구현해야 한다면 사정이 다를 수도 있다
          + MCP 서버는 ""기존의 공용 또는 준공용 API""만 다시 노출하면 된다. 가능한 원래 엔드포인트의 최소한의 변경으로 구현할 수 있어야 한다는 관점이 설득력 있다
          + 과거에도 이런 시도가 있었지만, 결국 몇 년이 지나면 앱들이 엔드포인트를 잠가 버려서 chatgpt, claude 등 특정 서버만 접근할 수 있게 막는다. 상호운용성은 사실상 사용자 이동성이기도 하고, 현실적으로 많은 기술 회사는 이동성보다 락인과 독점을 지향한다
     * 기술 채택과 확산을 위해 접근 장벽을 낮추는 일이 역사를 통틀어 중요한 역할을 했다는 점을 강조. MCP도 그 연장선이고, 무시해서는 안 된다. 우리 팀에서도, 기술배경이 전혀 없는 사람이 파일 공유 작업을 자동화하는 에이전트를 직접 쓸 수 있었다. 물론 예전에는 수백 개의 프로그래밍 언어나 라이브러리, API로만 가능했지만, MCP 덕에 비전문가도 신경 쓰지 않고 바로 해결하는 시대가 됐다. 성능 면에서는 최고가 아니고 최적의 구현 역시 아니지만, 이런 새로운 방식이 가져오는 가치는 지금 자원과 기술 수준에서는 전례가 없었다. 그런 점이야말로 진짜 핵심
          + 기술자가 아니라는 팀원이 파일 공유 정리를 혼자 잘했다는 이야기는 과장이라고 본다. 수천개 파일 정리라면 몰라도, 내 경험으론 거의 모든 파일 공유 정리 시도에서 관련 부서의 협조조차 얻기 힘들었다. 업무 담당자들조차 본인이 할 일도 아닌데 싫어한다. 고위 임원까지 동원해 겨우 설득하거나, 같이 앉아서 파일 구조만 한 시간 짜내는 일도 힘들었다. 작업량 중 50%는 부서 간 정치, 20%는 절차 갱신, 20%는 교육, 기술적 문제는 10%뿐이었다. 크고 작은 재앙, 끝없는 혼란까지 겪었는데, 아무리 AI 도구가 쉽게 만들어준다고 해도 현실이 그렇게 간단할 리는 없다는 생각. 수개월 뒤 백업 복구작업 하게 될 것 같다는 회의적인 예측
     * ""AI 에이전트가 워크래프트 3에서 피온처럼 명령받고 대답해줬으면 좋겠다""는 농담, 나는 차라리 요트 타고 싶다는 답변
          + ""I'd rather be sailing""은 워크래프트 2 대사고, 워크래프트 3에선 ""I'd rather be flying""이라는 대답이 나온다는 점을 짚어주고 싶다
"
"https://news.hada.io/topic?id=21778","AB180 개발팀의 AWS 비용 관리 여정: 청구서 확인부터 Fin Ops 문화까지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             AB180 개발팀의 AWS 비용 관리 여정: 청구서 확인부터 Fin Ops 문화까지

   Gemini로 요약했습니다.

   --

   수많은 데이터를 처리하는 마케팅 성과 측정 솔루션 '에어브릿지'를 운영하며, 체계적인 AWS 비용 관리(FinOps) 문화를 만들어왔습니다. 그 과정에서 얻은 경험과 노하우를 공유합니다.

   AB180의 비용 관리 운영 방식:
   저희는 비용을 '관리'하기 위해 다음과 같은 프로세스를 운영하고 있습니다.
     * 구글 시트 기반의 대시보드: 데이터를 쉽게 가공하고 공유할 수 있는 구글 시트를 활용해 대시보드를 구축했습니다. 태그별 비용 현황은 물론, 비용에 직접적인 영향을 주는 데이터 수집량까지 함께 표시하여 변동 원인을 직관적으로 파악합니다. 또한, Savings Plan 커버리지 현황을 시각화하고 계약 변경 시 결과를 미리 시뮬레이션하여 합리적인 의사결정을 돕습니다.
     * 주기적이고 자동화된 비용 점검: 2주에 한 번, 30분 내외의 짧은 미팅을 통해 비용 변동 사항을 점검합니다. 미팅 자료 생성, 슬랙 알림, 회의록 작성 등 반복적인 업무는 최대한 자동화하여 효율을 높였습니다. 비용 변동이 크면 담당자가 구글 시트 댓글로 원인을 분석하고 공유하여 투명성을 확보합니다.
     * 개발 전 예상 비용 산출: 새로운 기능 개발이나 아키텍처 변경 시, '테크 스펙(Tech Spec)' 문서에 예상 비용을 산출하도록 의무화했습니다. 이를 통해 개발 단계부터 비용을 고려한 더 나은 기술적 의사결정을 내릴 수 있습니다.

   비용 관리 시스템의 고도화 과정:
   지금의 시스템은 하루아침에 만들어지지 않았습니다. 다음과 같은 단계를 거쳐 발전했습니다.
     * Phase 0 (청구서 확인): 처음에는 매달 청구서를 확인하는 수준에 그쳤습니다.
     * Phase 1 (최소한의 분류): Name 태그를 활용해 리소스를 최소한으로 분류하기 시작했습니다.
     * Phase 2 (태그 전략 고도화): Team, Service 등 명확한 정책 기반의 태그 전략을 수립했습니다. 가이드 배포만으로는 부족하여, IAM 정책과 연동해 태그 설정을 강제하고, 태그가 없는 리소스는 슬랙 봇으로 자동 알림을 보내는 메커니즘을 구축했습니다. 그 결과, 태그 없는 리소스 비용을 전체의 1% 미만으로 관리하게 되었습니다.

   지난 여정에서 얻은 5가지 교훈:
     * 상황에 맞는 엔지니어링이 중요합니다. 비용 통제를 위한 완벽한 시스템을 추구하기보다, 회사 규모와 상황에 맞는 '적절한' 수준의 관리 체계를 점진적으로 구축하는 것이 현명합니다.
     * 비용 '통제'와 '최적화'는 다른 일입니다. 비용의 예측 가능성을 높이는 '통제'와 비용 자체를 줄이는 '최적화'는 명확히 다릅니다. 상황의 우선순위에 따라 무엇에 집중할지 결정해야 합니다.
     * 과감하게 자동화해야 합니다. 단순 반복 업무를 자동화하는 것을 넘어, 동료들이 직접 데이터를 조회하고 문제를 파악하는 '셀프 서브(Self-serve)' 환경을 구축하면 생산성이 극대화됩니다.
     * '메커니즘'을 만들어야 합니다. ""태그를 잘 달자""고 말하는 대신, 태그가 없으면 리소스 권한이 부여되지 않는 것처럼 정책을 따를 수밖에 없는 '장치(메커니즘)'를 설계하는 것이 효과적입니다.
     * FinOps는 결국 '문화'입니다. 비용 관리가 특정 담당자의 일이 아닌, 제품을 만드는 모두의 책임이라는 인식이 자리 잡도록 꾸준히 노력하고 문화를 만들어나가는 것이 가장 중요합니다.

   오호.. 가장 기본적인 Tag만이라도 붙여놓으면 어느정도 되겠군요.. :)

   그런데 RI나 SP 같은 것을 이용해서 줄이는건 기본으로 들어가는 것일까요....
   어느정도가 우리 인프라에서 사용할 사이즈인가는 고민이 많이 되는 부분이긴하네요...

   본문에서도 언급되는 내용이긴한데, 저는 따로 SP 시뮬레이터를 만들어서 최근 n일간의 workload를 기반으로 여기서 얼만큼 SP를 더 사야 ""기존 비용 + Covered 되서 줄어들 비용 + Recurring 되서 낭비될 비용""이 가장 적어질지 계산하고 그걸로 의사 결정 했었어요.

   현직 AWS kor 재직자입니다.

   입사 후 가장 중요하게 듣는 교육 중 하나가 고객이 클라우드 비용을 적게 쓸 수 있는 방법을 고민하라는 것이 있는데, 가장 효과적인 방법 중 하나로 RI & SP 를 안내합니다.

   요금을 깎아주세요..

   RI는 몰라도 SP의 경우 여러 워크로드에 적용될 수 있어서 고정으로 나가는 비용이 있다면 충분히 고려해볼만 합니다. 심지어 저희는 예상되는 최적화 시점까지 고려해서 구매하기도 했었습니다... ㅋㅋ 예를 들어 9개월 뒤에는 최적화 완료되서 서버비가 반으로 줄어들 것 같으면 그래도 1년치 사두는게 더 이득이니까 사는 식으로요.
"
"https://news.hada.io/topic?id=21697","최초의 비-오피오이드 진통제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            최초의 비-오피오이드 진통제

     * FDA가 Vertex Pharmaceuticals의 Journavx(수제트리진) 을 승인함으로써 최초의 비-오피오이드 기반 수술 후 진통제가 등장함
     * Journavx는 NaV1.8 나트륨 이온 채널을 선택적으로 억제하여, 중추신경계가 아닌 말초신경에서 통증 신호 전달 자체를 차단하는 기전임
     * 이 약물은 중독성, 내성, 금단 증상 등의 부작용이 없어 오피오이드의 사회적 문제를 해결할 수 있는 혁신적 해법으로 평가받음
     * 개발 과정은 여러 번의 실패와 수십 년간의 연구 투자가 필요했고, 다양한 고효율 분자 스크리닝과 선택성 개선이 핵심 역할을 함
     * Journavx는 아직 만성 통증에서는 승인된 바 없지만, 오피오이드 사용 최소화를 위한 중요한 첫걸음으로 자리매김 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

19세기부터 이어진 오피오이드 진통제의 역사와 문제점

     * 마취제의 발명은 19세기 의료를 혁신시켰지만, 수술 후 통증 완화는 오랫동안 오피오이드 의존 현상이 지속됨
     * 오피오이드(아편유도체)는 빠르고 강력하며 광범위한 진통 작용으로 타약물 대비 효과가 탁월했으나, 중독과 과다복용 문제를 초래함
     * 오피오이드는 뇌의 뮤 오피오이드 수용체와 결합해 통증 신호를 차단하는 방식임
     * 인체는 엔도르핀 등 자체 오피오이드를 생산하지만, 그 효과는 매우 일시적이며 약함
     * 오피오이드는 쾌감 유발 기능도 있어, 특히 잘못된 투약 시 뇌의 도파민 분비가 증가하며 중독 위험이 높아짐
     * 오랜 기간 과다 복용 시 내성이 생기고, 자연 오피오이드 시스템이 저하되어 더 많은 용량이 필요해지는 악순환이 발생함

오피오이드 규제의 시작

     * 19세기 말, 다양한 오피오이드(모르핀, 코데인, 헤로인 등)와 주사기 개발로 의약품 범용화가 이루어짐
     * 미국 내에서는 헤로인 등 오피오이드의 광범위한 처방 및 남용이 사회문제로 대두됨
     * 이에 따라 1914년 Harrison Narcotic Act 제정으로 오피오이드 규제가 시작됨

Peripheral Solutions: Journavx의 새로운 메커니즘

     * Journavx(수제트리진)는 기존 오피오이드와 달리 뇌가 아닌 말초 신경에서 작용함
     * 이 약물은 특히 NaV1.8 나트륨 이온 채널을 차단해 말초 통각수용기의 신호 전달 억제를 유도함
     * 뇌에서 통증 신호를 차단하는 방식과 달리, 통증 신호 발생 자체를 차단하여 중추신경 부작용이나 쾌감 유발이 없음
     * NaV1.8 채널이 중추신경에는 거의 존재하지 않아, Journavx는 중독성 및 호흡/심박수 저하 부작용을 유발하지 않음

비-오피오이드 진통제 개발의 어려움

     * 통증은 단일 원인 질환이 아니라, 다양한 신체 기능(혈압, 면역, 호흡 등)과 연관된 복잡한 신호 경로임
     * 타 약물 개발 사례(예: TRPV1 억제제, nerve growth factor 억제제)는 기대와 달리 역효과(체온 조절 장애, 관절 손상 가속 등)가 발생함
     * 통증 자체가 신체를 보호하는 신호임을 감안할 때, 특정 신호만 선택적으로 차단하는 것이 어렵다는 현실이 존재함

NaV 채널 발견과 유전자 연구

     * Vertex 등은 이온 채널 기반 약물 개발 경험이 풍부하며, 이를 토대로 NaV1.7, NaV1.8 등 다양한 나트륨 채널들이 통증 신호와 연관됨을 발견함
     * NaV1.7 결함 환자는 극심한 고통(Man on Fire 증후군) 또는 선천적 무통증 현상을 보임을 통해, 유전적으로 채널의 통증 역할이 입증됨
     * 그러나, NaV1.7 완전 억제 약물은 임상 시험에서 기대와 달리 실패함. 해당 채널의 결핍이 오히려 체내 자연 진통제(엔케팔린) 생산 증가 때문이라는 점이 밝혀짐

NaV1.8의 역할과 선택성 확보

     * NaV1.8 채널 역시 유전자 돌연변이 연구를 통해 통증 신호 전달의 중요한 경로임이 확인됨
     * 특히 말초 신경에 거의 한정적으로 존재해 중추 부작용 걱정 없이 표적화 가능성 보임
     * 마우스 실험에서 NaV1.8 결핍 시 통증 신경 활동이 현저히 감소함

Vertex의 고효율 스크리닝과 약물 발견

     * Vertex는 E-VIPR 기술 등으로 하루 5만 개 이상 화합물 분석 가능, 9가지 주요 나트륨 채널 중 NaV1.8에만 선택적으로 작용하는 후보군을 찾기 위해 수백만 분자 스크리닝 진행함
     * 3세대 후보(VX-150, VX-128, VX-961) 임상 실패, 부작용 및 내성 문제로 중단 사례 반복됨
     * 결국 VX-548(수제트리진) 이 뛰어난 선택성과 효능을 보여 임상 2/3상에서 급성 통증 치료 효과, 부작용 최소화 입증함
     * FDA로부터 Fast Track, Breakthrough Therapy 등 신속 심사 대상 지정
     * 2025년 1월, FDA 최종 승인으로 최초의 비-오피오이드 급성 진통제 상용화 이루어짐

한계점 및 미래 방향

     * Journavx는 만성통증 적응증에서는 아직 승인되거나 충분히 검증되지 않음
     * 주로 여성 대상의 수술 후 급성 통증 임상(엄지발가락 수술, 복부성형 중심)에서 효과 입증됨
     * 미국 보험 제도상 커버리지 불확실성, 기존 저가 오피오이드-아세트아미노펜 복합제 대비 비용이 높음
     * 효과 지속성이나 효능 면에서 기존 오피오이드 복합제를 뛰어넘지는 못함
     * Vertex는 추후 더욱 강력하고 선택적인 차세대 NaV1.8 차단제, 및 NaV1.7과의 병용 업그레이드를 연구 중임
     * 27년의 R&D, 수십억 달러, 수백만 화합물, 수천 환자 데이터를 거쳐 개발된 Journavx는 현대 제약벤처의 집약적 성과물임

결론

     * Journavx(수제트리진)는 오피오이드로 인한 사회적 부작용을 대체할 최초의 비-오피오이드 진통제로 활약함
     * 비록 한계점도 존재하지만, 향후 통증 환자 및 의료 환경의 혁신적 변화를 이끌어낼 포문 역할을 할 전망임

        Hacker News 의견

     * 일반적으로 사람들이 acetaminophen(파라세타몰)과 ibuprofen을 진통제라고 부르지만, 이번 약물은 완전히 다른 계열임을 강조하고 싶음, 실제로 오피오이드처럼 효과적이면서 부작용이 적은 점이 놀라움, 오피오이드가 아닌 첫 수술 후 사용 가능한 진통제라는 점에서 큰 의미를 느낌, 예전에 탈장 수술하고 오피오이드로 인한 변비가 너무 심했기에 이런 약이 그때 있었으면 정말 좋았을 것이라는 생각임
          + acetaminophen과 paracetamol이란 두 가지 이름 때문에 혼란을 겪는 사람이 많다고 추측함, 본인도 64세가 될 때까지 미국에서 파라세타몰을 찾지 못해 당황한 경험이 있었고, 이 두 개의 이름 때문에 사소하지만 누적된 불편함이 좀 크다고 느낌
          + 수술 후 사용 가능한 첫 오피오이드가 아닌 진통제라는 점이 맞는지 의문이 있음, 여러 나라에서는 Metamizole이 이미 수술 후 통증에 1차로 사용되고 있음, 단 Metamizole은 아주 드물게 무과립구증(agranulocytosis) 부작용이 있고, 인종이나 유전적 특성에 따라 위험도가 달라서 국가에 따라 금지 또는 일반의약품임
          + 신장 결석 때문에 이를 부러뜨린 경험이 있음, 신장 결석이 지금까지 겪은 것 중 가장 아픈 경험이었고, 오피오이드 진통제를 처방받아 변비가 심해짐, 변비 해소를 위해 씨를 제거한 자두를 먹었는데 알고 보니 씨가 남아있던 자두를 씹어 약한 치아가 손상됨(결국 임플란트로 교체)
          + 임플란트 시술 후 Ketorolac이라는 NSAID계 진통제를 처방받았고, 최근에 받는 소형 morphine 알약보다 효과가 훨씬 좋았음, Ketorolac은 신장에 좀 부담을 주지만 위에는 그렇게 독하지 않음, 그러나 용량을 잘못 먹거나 대사 능력이 떨어진 사람이 먹으면 신부전, 위 출혈 등 치명적인 부작용이 있어 위험함, 새로운 진통제가 Ketorolac보다 효과가 더 높으리라 생각하지 않음, 그래도 Ketorolac보다는 위험성이 낮을 수는 있다고 봄
          + 최근 acetaminophen의 주요 대사산물이 나트륨 채널에 작용하는 원리가 밝혀졌다는 흥미로운 기사 소개 관련 기사
     * 기사 제목이 “첫 비 오피오이드 진통제”인데, 실제로는 ""첫 비 오피오이드 노시셉티브 진통제"" 정도가 더 정확하다고 생각함, 노시셉티브 통증은 실제 조직 손상에서 오는 통증을 말하며, 신경성 통증(두통, 염증 등)과는 다름, 참고 자료 글로벌 통증학회 용어집
          + 신경과학 관련 훈련을 받은 사람으로서, 저 용어 설명이 본인 이해와 맞지 않음, 실제 염증도 노시셉티브 통증이고 NSAID가 염증을 완화해 통증이 줄어듦, 대부분의 조직 손상은 염증을 동반함, 반면, 신경병성 통증은 신경 자체 손상에서 비롯되고 NSAID가 전혀 효과 없음(좌골신경통 경험자들은 잘 알 것임)
          + 제목 아래에 저 용어 설명이 부제로 들어가는 건지 궁금증
          + 과학 논문에는 저런 용어가 맞지만 일반인들이 크게 공감 못할 것이고, 제목이 너무 어렵다면 오히려 관심 저하로 이어지지 않을까 우려, 뉴스나 소셜미디어에서 오피오이드는 아무나 다 들어본 적 있음
     * 의료계에서 사람마다 통증 내성이 다르다는 점을 더 진지하게 받아들여야 한다고 생각함, 아내, 딸, 본인은 통증에 매우 강한 편이고, 치료 중 마취 등을 거부한 적도 있음, 수술 후 오피오이드도 단 한 번도 복용한 적 없고 사랑니 뽑을 때도 마찬가지였음, 반면 또 다른 자녀는 통증에 매우 약해서 귀걸이 교체만 해도 불안해함, 마취 필요성에 대한 본인과 딸의 관점이 너무 다름, 본인은 환자 제어용 이외에 마취의 필요성을 잘 못 느끼겠지만, 아이는 통증으로 공황 상태까지 갈 수 있음, 그럼에도 마취과 의사들이 “통증 얼마나 참는 편이세요?”라고 묻는 걸 한 번도 본 적 없음
          + 살면서 여드름만 터져도 무진장 아팠던 기억이 있고 아무도 알려주지 않아서 충격이었음, 통증 내성과 불안 사이의 상관관계가 궁금해짐
          + 육아 관련 책을 좀 읽어보라고 권유
          + 높은 통증 내성은 타고나는 거고, 딸이 유전복권에서 못 이긴 것뿐이라는 설명에 의문
     * nerve growth factor 억제제인 tanezumab 관련, 통증을 완화하지만 관절염 환자가 통증을 못 느끼고 관절을 더 써서 상태가 오히려 빨리 악화된 일이 있었음, 이후 저용량 및 조건 제한 임상도 진행됐으나 FDA 승인이 거부됨, 이 부분이 이해는 가지만 솔직히 복잡한 심정임
          + 동물에게 통증이란 신체의 손상을 인식시키는 신호임, 만약 이 신호(통증)를 없애면 해가 계속 가해질 수 있다는 교훈임
          + 한편, 오래된 아스피린은 효과적인 진통제일 뿐 아니라 골관절염에서 연골 소실을 되돌리는 효과도 있음 연구자료, 심장 보호 효과까지 있는 약임, 위에 부담을 줄 수 있어서 대체 약물이 개발됐고 그 대체제가 심장마비 및 뇌졸중 등 심각한 부작용을 일으켰는데(rofecoxib, valdecoxib 등), 결국 나중에는 FDA가 ""non-aspirin NSAID는 심장마비, 뇌졸중 위험을 높인다""고 발표까지 함 FDA 발표, 언젠가 버터 대신 마가린 썼던 과오처럼, 아스피린을 대체하려 했던 시도들을 다시 평가하게 될 것 같음
          + FDA가 의사-환자 간 동의(설명 정보 제공)에 끼어드는 건 본인에게 복잡한 감정을 주지 않음, 과도한 신중으로 인한 고통과 불편은 FDA 책임이라는 입장임
          + 미래 연구에서는 용량을 더 낮춰서 시도할 수도 있지 않을까 생각, 물론 이미 연구자들이 충분히 고려하고 있을 거라 봄
     * 기사 제목이 이해가 안 됨, 개인적 진료 기록을 봐도 이미 여러 비 오피오이드 진통제들이 존재함, gabapentin이나 ibuprofen만 봐도 그렇고, 심지어 기사 본문에서도 ibuprofen과 비교하고 있음, 이 신약은 “수술 후 적합한 진통제”라는 점 외엔 혁신이라 보지 않고 오히려 제목이 클릭 유도성 냄새가 남, 더 좋은 paracetamol 정도라면 충분히 반가운 혁신이나 “세계 최초” 타이틀은 과장임
          + gabapentin은 항경련제이며 ibuprofen, paracetamol은 항염증제임, 이들은 통증 억제 효과는 있지만 기술적으로는 “진통제”가 아니므로 기사 제목이 맞음
          + The New Yorker에서 최근 관련 내용을 아주 잘 설명한 기사를 발견함 기사 링크
     * 저자(기사 작성자)가 NSAID와 paracetamol(acetaminophen)이 이미 있음에도 불구하고 굳이 저런 제목을 고른 점이 흥미로움, 본인이 뭔가 놓쳤나 고민, (추가) 옆 댓글에서 궁금증 풀림, NSAID는 비스테로이드성 항염증제 약물임
     * kratom(미트라가이나 스페시오사) 얘기가 이런 논의에서 늘 빠지는 점에 매번 놀람, 강력한 진통제일 뿐 아니라 오피오이드 금단증상 완화도 도움됨, 미국에서는 아직 부분적으로 법적으로 허용되고 있고, 여러 나라에서는 오락 목적으로 사용될 수 있어 점차 금지 또는 규제가 늘고 있음, 오피오이드처럼 극심한 도취감을 주지는 않고 중독성도 덜하다고 알려져 있음, 제약회사의 인센티브 부족 탓에 식물 유래 약물이 충분히 연구ㆍ활용되지 못해 안타까움
          + kratom은 오피오이드계임을 지적
     * 속임수성 제목임, 오피오이드 아닌 진통제 계열이 이미 두 종류 이상 오랫동안 사용중임 진통제 위키피디아
          + 제목이 오해를 주지 않는다고 생각, 다만 문화적 언어 차이일 수도 있음, 일부 국가에서는 painkiller란 단어를 “강한” 진통제에만 쓰며 ibuprofen 같은 건 포함하지 않음, 그래서 바로 “비 오피오이드 계열의 강한 통증 차단제”를 떠올렸음
          + 클릭베이트일 수 있으나, 워낙 보편적인 주제라 많은 관심을 끄는 것도 사실, ""이 비밀을 의사가 감추고 싶어한다"" 같은 스타일의 부정적 클릭베이트와는 구분될 만함
     * 이번 기사에서 언급된 신약의 임상 2상 결과가 별로 좋지 않았다는 내용의 기사 공유함 관련 기사, 유럽은 이미 metamizole로 수술 후 통증을 조절하고 있는데, 신약 suzetrigine이 metamizole보다 효과가 더 나은지 궁금증
     * 앞으로 실제 처방이 이루어져야 결과를 알 수 있을 것 같음, “중독성 없는” 진통제가 어떤 결과를 가져오는지 보고 싶으면 드라마 Dopesick을 추천
"
"https://news.hada.io/topic?id=21745","Show GN: AI 텍스트 어드벤처 게임을 만드는 툴, Tales Maker","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Show GN: AI 텍스트 어드벤처 게임을 만드는 툴, Tales Maker

     * 저번에 공개했던 텍스트 어드벤처 게임이 어느덧 40개 이상이 되었습니다.
     * 게임에서 플랫폼으로 진화하면서, 유저들이 올린 게임도 30개 이상 됩니다.
     * 이 유저들이 텍스트 어드벤처 게임을 제작할 때 사용한 툴, Tales Maker를 공개합니다.

   제공하는 가치
     * 세계관 프롬프트 12만자 입력 가능 (소설책 한권 분량)
     * 프롬프트를 조각으로 다루는, 레고를 쌓는 듯한 UX로 편하게 관리
     * AI Agent를 활용하여 이미지/비디오/오디오를 적재적소에 출력 가능
     * 주사위 기능 / 날씨 변화 기능 등 부가요소 제공
     * Stripe 연동으로 창작자 인센티브 자동 지급 기능

   거진 3년 정도를 이런 쪽(웹 기술로 새로운 형태의 게임에 도전!)에 시간을 투자해왔는데..
   이번엔 정말 만족할 만한 장난감이 나왔네요 ㅎㅎ 저도 하루 몇시간씩 투자하면서 재밌게 하고 있어요. 그러면서 QA도 많이 하고 있고요 하하.

   이번에 공개하는 툴은 ""프롬프트 12만자를 어떻게 관리할 것인가?"" 라는 의문에서 출발한 UX를 가졌습니다. 개발자로서, 그리고 게임 엔진 제작자로서, 그리고 소설가로서 많은 고민을 해왔고, 그 생각의 결과물을 조합한 것이 Tales Maker 입니다.

   한번씩 구경해주시고, 관심과 의견 주시면 감사하겠습니다.

   Tales Maker는 무료로 사용하실 수 있고,
   옆에 붙어있는 닉시AI라는 애는 돈 먹는 애라서 사용할 때 조금의 재화를 받습니다.

   툴이 아닌 , 게임을 해보시려면 여기입니다 -> 랜덤테일즈

   40개 돌파라니 대단합니다!

   오오오 정말 멋집니다!
"
"https://news.hada.io/topic?id=21730","Meta, 약 "20조원"으로 단 한 명을 영입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Meta, 약 ""20조원""으로 단 한 명을 영입

     * 메타가 Scale AI 인수로 알려진 $14B(140억달러, 약 20조 원) 투자는 실제로는 전체 인수가 아닌 지분 49% 확보와 CEO 알렉산더 왕 영입이 목적임
     * 기존 Scale AI 임직원들은 주식을 계속 보유하면서도 현금 배당을 받는 구조로, 알렉산더 왕은 메타로 이직하면서도 이사회에는 잔류함
     * 메타의 목적은 Scale AI의 사업이 아닌 알렉산더 왕 개인 영입에 초점이 맞춰짐
     * 메타는 최근 AI 경쟁에서 뒤처지며, 내부 정치와 리더십 문제, 특히 Yann LeCun 중심의 보수적 연구 문화로 인해 성과 부진을 겪고 있음
     * 저자는 특정 개인에 대한 11자리 금액 투자는 비합리적이라 평가하지만, 메타가 조직 문화와 인재 구조 개선에 집중해야 함을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

업데이트: Scale AI 임직원 지분 구조

     * Scale AI 임직원들은 모든 주식의 프리미엄에 해당하는 배당금을 한 번에 받으면서도, 자신이 보유한 주식은 계속 유지함
     * 이 구조는 기존 인수와 달리, 임직원들이 회사의 성장에 따른 추가적인 이익을 계속 누릴 수 있는 장점이 있음
     * CEO 알렉산더 왕이 이 구조를 직접 요청한 것으로 알려져 있으며, 임직원 보호 측면에서 긍정적으로 평가됨
     * 대부분의 금액이 CEO에게만 간 것이 아니라 전체 임직원에게 나누어졌음
     * 메타가 실제로는 Scale AI 전체를 인수한 것이 아니라, 주주 및 임직원들에게 새로운 현금 보상을 제공한 투자 성격임

메타의 140억 달러 '인수'의 진짜 의미

     * 메타가 Scale AI에 140억 달러를 투자하면서 49% 지분만 확보했고, 경영권은 여전히 Scale AI 이사회가 보유함
     * 법적으로는 '인수'가 아니지만, 실제로는 메타와 알렉산더 왕의 지분 합계가 과반을 넘기 때문에 회사의 의사결정권을 쥐고 있는 셈임
     * CEO 알렉산더 왕은 메타로 이직하지만, 여전히 Scale AI 이사회에 남아 회사 경영에 영향력을 행사함
     * 이 같은 거래 구조는 업계에서 드물며, 메타의 핵심 목표가 회사 자체가 아니라 특정 인재(알렉산더 왕) 확보에 있음을 보여줌
     * 기존 인수처럼 모든 지분을 현금화하지 않았고, 지분을 일부만 넘기면서도 경영권 장악이 가능하게 설계됨

Scale AI의 본질과 메타가 원하는 것

     * Scale AI는 B2B 데이터 라벨링 및 대규모 인적 리소스 아웃소싱을 중심으로 한 'Mechanical Turk as a Service' 비즈니스를 전개함
     * 이 회사는 최첨단 알고리듬이나 IP보다 효율적 인력 운용, 대량 데이터셋 생산이 경쟁력임
     * 대형 AI 기업들은 이미 여러 데이터 라벨링 파트너와 자체 라벨링팀을 운영하기 때문에, Scale AI 데이터에 절대적으로 의존하지 않음
     * 실제로 Scale AI의 주요 고객은 토요타, Etsy, GM, 정부기관 등 AI 생태계의 후발주자 혹은 비전문적 조직임
     * 메타가 Scale AI 사업 자체에 큰 가치를 두지 않고, 핵심 인재 확보에 집중한 근거가 됨

메타의 AI 경쟁력 약화와 내부 문제

     * 메타는 Llama 2 이후로 AI 경쟁에서 OpenAI, Google, Anthropic 등에 뒤처진 상황임
     * 초기에는 오픈소스 전략으로 커뮤니티 주도 생태계를 만들며 'AI계의 리눅스' 이미지를 구축했지만, Deepseek 등 새로운 모델 등장 이후 경쟁력 상실을 경험함
     * 메타의 AI 연구 조직인 FAIR(Foundational AI Research)는 Yann LeCun 중심의 보수적·비시장 지향적 문화로 혁신이 저해됨
     * LeCun은 과거 Lua와 Torch에 대한 고집, 트랜스포머 및 LLM 회의론 등으로 신기술 채택을 지연시켰고, 실제로 초기 Llama 팀은 LeCun 몰래 프로토타입을 개발해 저커버그에게 직접 시연한 일화가 있음
     * 조직 내 정체와 리더십 갈등, 핵심 인재 유출이 이어지며, 전체 AI 혁신이 더뎌지는 악순환이 반복되고 있음

저커버그의 대담한 인재 영입 전략

     * 저커버그는 메타 AI 부문 부진의 원인을 내부 관료주의와 조직 경직성에서 찾고, 이를 극복하기 위해 파격적인 인재 영입에 나섬
     * 업계 톱티어 인재에게 8~9자리(억~수십억 달러) 연봉, CEO 직속 조직, 자율적 연구 환경을 제안해 직접 영입 시도
     * 실제로 나트 프리드먼, 데이비드 그로스, 일리야 서츠케버 등 수많은 AI계 인사에게 영입 제안을 한 것으로 보도됨
     * 알렉산더 왕은 저커버그와의 친분, 비공식 AI 자문 역할 등으로 첫 번째 영입 성공 사례임
     * 하지만, 초고액 투자를 통한 '슈퍼스타' 인재 집중 전략이 과연 조직 혁신과 AI 역량 향상에 효과적인지에 대한 회의적 시각도 함께 존재함

결론 및 저자의 평가

     * 저자는 AI 혁신은 특정 '천재' 개인이 아니라, 집단적 실험과 협업 환경에서 발생한다는 점을 강조함
     * 구글처럼 연구 자유, 강력한 협업, 무한한 자원 제공 환경이 장기적으로 혁신을 이끌 수 있음을 예시로 듦
     * 메타의 브랜딩과 내부 문화 개선이 AI 경쟁력 회복의 핵심임을 지적하며, 단일 인재에 막대한 투자를 집중하는 접근은 리스크가 크다고 판단함
     * 저커버그의 실행력과 변화 추진력에는 기대를 걸지만, 11자리 연봉의 '한 명 영입' 전략이 메타의 AI 혁신 해법이 될지 미지수임

   창업자 본인만 엑시트할 수도 있었을텐데, 그랬다면 나머지 임직원들은 엑시트는 어려워졌겠죠.
   메타가 확보한 지분이 창업자 지분뿐만 아니라 임직원 몫도 있었나보네요.
   알렉산더 왕, 대단한 인물 이네요..

        Hacker News 의견

     * Meta가 Scale에 49%만 투자한 이유를 제대로 짚지 못했다고 생각함. Meta는 이 경로를 통해 글로벌 규제 심사를 피함. 완전 인수였다면 각국 당국의 승인이 필요했겠지만, 소수 지분 투자는 이런 심사 대상이 아님. 주주들도 인수와 비슷한 수익을 얻으면서 규제 불확실성을 피해감. 남은 Scale 역시 성공 가능성이 남아 주주(이제 Meta 포함)에게 추가 수익을 줄 여지도 있음. 결과적으로 윈윈 구조임
          + 소수 지분 투자라고 독점 조사를 무시할 순 없음. 미국의 15 U.S.C. §18 같은 곳에서도 부분 인수나 소수 지분 투자를 살펴볼 수 있음. 유럽도 마찬가지임
          + Meta가 빠르게 움직이고 싶어서 M&A 대신 이런 선택을 했다는 추론은 납득됨. 내 글에서도 Meta가 본업과 직접 연관돼 인수를 하는 건 말이 안 된다고 주장했지만, 가능성은 있다고 봄. 하지만 이게 윈윈이라고는 생각하지 않음. Scale 주식이 여전히 유동성이 낮고, 핵심 멤버들이 빠져나간 결과, 주식을 가진 사람들 입장에선 오히려 가치가 줄었음
          + Microsoft와 Google도 비슷하게 했음(예: Character AI 투자). Scale이 AI 연구소도 아니고 기본적으로 Fiverr 같은 아웃소싱 플랫폼임. 저기서 인재를 데려오는 게 Meta의 AI 연구 매력을 높이는 데 어떤 도움이 될지 잘 모르겠음
          + 실제로 이번 투자 구조는 Wang(Scale CEO)의 요청으로 이뤄진 것으로 전해짐. 즉, 투자자와 직원들에게 수익을 돌려주기 위한 목적이 컸음. Meta 입장에선 Scale을 굳이 소유하고 싶은 마음이 없었다고 봄. 이미 Scale 데이터를 잘 쓰고 있고, 회사를 가지는 게 직접적으로 필요하진 않았음
     * OpenAI는 Jony Ive를 65억 달러에 데려오면서 좋은 거래를 했던 것 같음. 하지만 Llama 4가 벤치마크에서 좋아 보였다는 소문과 다르게, 실제 사용 경험이 너무 실망스러워서 Meta가 수치 조작을 했다는 얘기도 나옴. 나 스스로도 벤치마크의 기준이 뭔지 잘 모르겠고, 실제로 써봐도 ""대단한 AI!"" 소문만큼 체감 변화가 없었음. 여전히 엉뚱한 답변에 반복적으로 마주치고 있음
          + 벤치마크에도 종류가 있음. 하나는 실제 데이터를 기반으로 한 정량화 가능한 것, 또 다른 하나는 대중의 반응임. 실제 벤치마크가 선행 지표일 수 있지만 꼭 맞진 않고, 궁극적으로 중요한 건 대중의 평가임. Reddit 반응만 봐도 Gemini 2.5가 Claude 3.7보다 좋다는 걸 알 수 있었음
          + Jony Ive의 Apple 시절 업적을 좋게 생각하지 않음: 너무 얇은 노트북, 고칠 수 없는 설계, 불편한 키보드, 충전 중 못 쓰는 마우스, 성능 저하를 부르는 케이스 디자인, 불필요하게 추상화된 UI 등 아직도 마우스를 충전 중에 쓸 수 없는 점 등 실망스러운 점이 많았음
          + Jony Ive가 거의 Dieter Rams 스타일을 베꼈다시피 하면서도 슈퍼스타 경력을 쌓은 점이 놀라움. OpenAI에 무슨 가치를 줄 수 있을지 상상이 안 됨. Apple 노트북을 5년간 망쳐놓았던 사람임
          + 사실 OpenAI는 Jony Ive를 직접 데려온 게 아님. 기존 합작 사업을 인수한 거라 Ive는 이미 현금화했고, 이제는 디자인 회사와 계약만 있는 상태임. Ive는 대박, OpenAI는 그다지 이득이 아닌 구조라고 생각함
          + Meta가 내놓은 Llama 4 벤치마크는 바로 거짓으로 판명 났음
     * Scale AI 데이터가 Wang에겐 부차적이라는 시각에 반론을 제기함. 데이터 라벨링은 이제 그냥 노동자에 맡기는 수준이 아니라 전문가 시연과 워크플로우 설계로 이어짐. Frontier lab들이 어떻게 RL 환경을 만드는지 추론할 수 있는 고급 데이터임. 그게 진짜 동기라고 생각함.
       또, LLM을 누구나 만들 수 있다는 주장에도 동의하지 않음. 경쟁력 있는 LLM을 대규모로 훈련할 수 있는 사람은 전세계에 수백 명 정도고, 엄청난 기술적 노하우와 트릭이 필요함. 딥섹 관련 보고서가 나오자마자 놀랐던 이유도 그래서임.
       ML 진보가 대부분 운빨이고, 분산 조직만이 합리적이라는 것도 동의 못 함. Schmidhuber나 Shazeer, Alec Radford 등 지속적으로 기반 연구를 해온 사람들도 많고, OpenAI가 분산 실험 대신 집중 투자로 이 판을 키웠다는 점도 중요하게 봐야 함. Deepmind와 같은 사례도 마찬가지임
          + Scale에 대해 내가 틀릴 수도 있음. 클라이언트와 Scale 내부 사람들 이야기 위주로 파악한 것이긴 함
            LLM 훈련이 어렵긴 해도 불가능하게 배우기 어려운 건 아님. 학부 졸업 후 3개월 정도 경험해본 바로는, 작은 모델이든 큰 모델이든 훈련 난이도엔 큰 차이 없었음. torch나 megatron 같은 라이브러리도 많이 도움이 됨
            연구자들 모두 대단하고 누구도 폄하할 생각은 없음. 하지만 ML의 주요 변화나 새로운 아이디어는 정말 여러 곳에서 다양한 사람들이 서로 탈중앙적으로 소통하며 나오고 있다고 믿음. 개인 견해임
     * Meta의 AI 조직 내 갈등과 문화적 문제가 심각함. 현재 주요 ""AI"" 조직은 GenAI, FAIR, RL-R 세 곳임
       FAIR는 여러 조직에 넘겨지면서 파워 하우스 역할을 잃었고 많은 인력이 빠져나갔음
       GenAI는 원래 작은 팀이었지만 이제 인원만 계속 늘어나 제품도 애매해지고, 리더십도 제품화나 ML 실전 경험이 부족함. 매주 A/B 테스트나 하고 있음
       RL-R은 아바타 조직에 얽혀 있는데, 리더가 실행력보다 완벽주의에 가까워 결국 망할 것 같음. 완전 ML 기반 시스템보다 단순한 시스템이 성능이 더 나은데 150억 달러를 날렸음
       손목밴드에 의존하는 hand interaction 조직도 있지만 제대로 된 대규모 프로토타입도 없고, 너무 불편함
       디스플레이 조직은 과도한 약속으로 오리온 같은 소비자 제품화에 실패
       mapping 팀은 개인정보 수집이 심한 리서치 글래스만 만들고 있음
       RL-R의 인재가 많았지만, ""고용 후 해고"" 문화 때문에 위험을 감수한 연구는 거의 불가능함
     * 데이터 라벨링 회사라는 관점에서 보면 이런 이슈가 웃기는 상황임. 일종의 ""핫도그 분류""를 떠올릴 정도로 단순한데 140억 달러를 썼다는 것 자체가 충격임
          + 예전에 Hotdog or Not을 디펜스해 딕픽 분류기로 바꿔볼 수도 있겠지만, 사실상 140억 달러를 기계식 일거리 플랫폼에 투자하는 건 정말 황당한 일임. 이 외에도 60억 달러의 Jony Ive 인수, ChatGPT 기반 의료 래퍼 Abridge의 50억 달러 밸류처럼 AI 거품이 지나치다고 느낌
     * Meta는 광고 사업으로 너무 돈을 잘 벌어서 수십억 달러를 잃는 프로젝트도 큰 타격 없이 주가가 오르고 있음. 심지어 PER도 엄청 높진 않음. 참 놀라운 현상임
          + 예전에 Google/Alphabet에도 그런 말을 했음
     * AGI 관련 글쓴이 의견이 과학적 방법론을 따르지 않는다고 봄. 내 추측엔 Meta가 AGI 정의가 바뀔 때마다 목표를 바꿀 수 있도록 영향력 있는 사람들을 많이 데려가려는 목적이 더 크다고 생각함. 실력보다는 패러다임 주도권 확보가 목적인 셈임
     * 마지막 두 단락은 읽고 나서도 혼란스럽고 당혹스러움.
       Zuck이 특별히 유능한 CEO란 근거를 잘 모르겠음. Facebook은 10년 넘게 인수 외에는 신제품 성공이 없었음
       TikTok같은 신생 서비스가 Instagram을 이긴 걸 보면 너무 창피함. Meta Quest는 적자만 보는 부수적인 사업으로 회사 이름까지 바꿀 가치가 있었는지 의문임
       Microsoft나 Google, Amazon 대비 Meta가 고객 신뢰도에서 너무 뒤쳐져 있음. 이 신뢰 격차로 인해 Meta의 신사업은 늘 어려움을 겪음. LinkedIn이나 Gmail, Search를 다루는데도 Microsoft와 Google이 훨씬 신뢰받음
       개인적으로 Meta는 140억 달러로 결국 ""혼자선 성공 못하니"" 다른 회사에 투자해서 신제품 성공 사례를 노리는 입장 같음
       (참고: https://allaboutcookies.org/big-tech-trust)
          + 본업이 워낙 수익성과 규모가 뛰어나서 굳이 '새로운 기술'이 없어도 됨. 혁신은 꼭 필요하지 않음. 실제로 경쟁이 약한 상황에선 옛날 담배주식이나 Walmart처럼 혁신 없이도 대규모로 성장 가능함
            TikTok과 Instagram 상황도 둘 다 잘 되고 있어 전혀 부끄러울 게 없음. Pepsi와 코카콜라처럼 공존 가능
          + 나름 Zuck 팬이고 그 입장에서 옹호 논리도 만들 수 있지만, 그렇다고 무조건 동의하는 건 아니고, 비판하는 입장도 설득력 있다고 봄
          + 신제품 성공만으로 CEO 능력을 평가하는 건 잘못된 척도임. Ray Ban Meta도 꾸준히 완판되는 등 성공을 거두고 있음
          + $META(주가) 상황을 봤는지 궁금함
     * 이런 숫자들은 정말 미친 수준임
     * 사실이 아닌 부분이 있는데, 저자도 이제 그걸 알게 됨. ""140억 달러가 전부 Alexandr에게 간 게 아니다""라는 업데이트도 됐는데, 차라리 타이틀도 바꾸는 게 맞지 않나 생각함
          + 기사 제목의 포인트는 ""Meta가 140억 달러를 썼다""는 거지, ""한 사람이 140억 달러를 벌었다""는 게 아님. 의미를 오해하면 안 됨
"
"https://news.hada.io/topic?id=21753","Fang - CLI 스타터 키트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fang - CLI 스타터 키트

     * Go 용 CLI 라이브러리인 Cobra 기반의 CLI 개발을 위한 경량 라이브러리
     * Battery Included: 처음부터 스타일리시한 CLI 환경을 손쉽게 구축할 수 있음
     * 사용자 친화적인 도움말·에러 스타일링, 자동 버전 출력, man페이지 생성, 셸 자동 완성 지원 등 CLI 개발 생산성을 크게 높임
     * 내장 테마를 제공하며 테마 커스터 마이징도 지원
     * 기존의 Cobra만 사용할 때 대비 빠른 초기 셋업과 현대적 CLI 경험 구현이 가능한 것이 가장 큰 장점
     * 소규모부터 복잡한 CLI까지 모두 적용 가능하며, Charm_의 오픈소스 생태계와 연동됨

   Cobra - 강력한 Go 기반 CLI 앱 개발 라이브러리
"
"https://news.hada.io/topic?id=21746","vibecoding은 개발자의 전문성을 없애는 것이 아니라, 전문성이 필요한 영역을 바꾼다. [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       vibecoding은 개발자의 전문성을 없애는 것이 아니라, 전문성이 필요한 영역을 바꾼다. [번역글]

    1. vibecoding 실험 사례
          + Anthropic의 Claude Code 에이전트를 활용해, 리눅스 환경에서 실제 게임이나 애플리케이션을 만드는 실험을 성공적으로 진행했다.
          + 자연어로 명령: “여러 가지 디자인의 건물을 배치하고, 내가 만든 마을을 직접 운전해서 돌아다닐 수 있게 해줘”처럼 단순한 영어 문장 하나로, AI가 실제 실행 가능한 코드를 자동으로 생성했다.
          + 지속적인 피드백 반영: 소방차 미션, 교통량, 경쟁자(헬리콥터), 낮과 밤 변화, 미션 등 다양한 기능을 추가로 요청하면, AI가 이를 반영해 애플리케이션을 업데이트했다.
          + 문제 해결 과정: 코드 실행 중 발생한 버그나 에러도, AI와 반복적으로 소통하며 해결했으며, 최종적으로 Claude API 사용료로 약 $13이 소요되었다.
    2. 전문성의 변화
          + 전문성의 이동: vibecoding은 개발자의 전문성을 없애는 것이 아니라, 전문성이 필요한 영역을 바꾼다. 즉, 모든 코드를 직접 작성하는 대신, 시스템 전체를 이해하고 방향을 제시하거나 결과물을 평가하는 능력이 더 중요해진다.
          + 최소한의 지식 필요: AI와 협업할 때는 최소한의 프로그래밍 지식(기본 이해, 판단력)이 필요하며, AI가 제안한 결과물을 검토하고 피드백을 줄 수 있는 역량이 요구된다.
          + 실제 활용 예시: 리눅스 환경에 익숙하지 않은 사용자도, AI의 도움을 받아 원하는 소프트웨어를 빠르게 만들 수 있다.
    3. AI 에이전트의 확장성
          + 다양한 AI 에이전트: 중국의 Manus 등 다양한 AI 에이전트가 웹 리서치, 코딩, 문서 및 웹사이트 제작 등 폭넓은 작업을 자율적으로 수행한다.
          + 피드백의 중요성: AI가 자율적으로 작업을 수행하더라도, 실수나 오류가 발생할 수 있으므로, 사람이 결과를 검토하고 피드백을 주는 과정이 필수적이다.
          + 실제 실험 사례: “최고의 학술적 조언을 활용해 엘리베이터 피칭에 대한 인터랙티브 코스를 만들어 달라”는 요청에, AI가 체크리스트 작성, 웹 리서치, 페이지 제작까지 모두 자율적으로 진행했다. 이후 인터랙티브 요소가 부족하다는 피드백을 주자, AI가 이를 반영해 개선했다.
    4. 심층 협업(Deep Vibeworking)
          + 복잡한 데이터 분석: 크라우드펀딩에서 수집한 대규모 익명 데이터를 AI와 함께 분석하고, 논문까지 작성하는 실험을 진행했다.
          + AI의 역할: AI가 데이터 분석과 가설 제안에 큰 도움을 주었으나, 학문적으로 의미 있는 주제 선정과 결과 평가 등은 여전히 사람의 전문성이 필요했다.
          + 빠른 결과물 생성: 논문 작성까지 한 시간도 채 걸리지 않았으며, 결과물은 학계에 의미 있는 기여가 될 수 있는 수준이었다.
    5. 결론: 인간과 AI의 협업이 핵심
          + 상호보완적 협업: AI가 모든 것을 대신할 수는 없지만, 인간의 전문성과 AI의 역량을 결합할 때 가장 큰 가치가 창출된다.
          + 일의 방식 변화: 일의 방식이 빠르게 변화하고 있으며, 각 작업마다 AI와 인간이 어떻게 협업할지 그 접점을 찾는 것이 중요하다.
          + 미래 전망: AI와 인간이 각자의 강점을 살려 협업하는 새로운 시대가 도래하고 있다.

   AI가 모든 걸 대신해주진 않겠지만, 상당수 작업을 대신하게 되겠네요.
   정말 소수의 전문가들이 더 이상 신입이나 중간 레벨의 개발자와 협업하기 보다는
   그냥 AI와 함께 일하고 더 격차가 벌어지는 시대가 오는 건 아닐까 두렵기도 합니다.

   업무에서도 양극화가 점점 심해지겠군요

     AI와 협업할 때는 최소한의 프로그래밍 지식(기본 이해, 판단력)이 필요하며, AI가 제안한 결과물을 검토하고 피드백을 줄 수 있는 역량이 요구된다.

   엔터프라이즈 어플리케이션 개발에서는 최소한 보다는 근본적인 지식(CS, Domain, Design 등)이 요구된다고 생각합니다.
   AI 를 통해 간단한 토이 프로젝트의 경우 이러한 지식이 없어도 쉽게 개발할 수 있지만, 규모가 커질수록 근본적인 지식 부재로 여러 난관(도메인과 상이한 구조, 성능, 동시성 이슈 등)에 부딪히게 됩니다.
   AI 를 잘 활용한다는 전제하에, 앞으로 개발자의 전문성은 근본적인 지식을 통해 거시적인 관점에서 프로젝트의 방향성을 결정하는 능력과 깊이 있는 문제 해결 능력에 있다는 생각이 듭니다.

   정말 좋은 관점의 댓글 감사합니다 :)

   AI에게 자기 일을 맡길 수 있다고 느낀다면, 결국 100% 대체 될 뿐입니다. AI가 대체할 수 없거나, 남들이 따라할 수 없는 능력을 키워나아가야죠.

   맞아요 그 능력을 계속 탐구하고 발전시켜야 하는 것 같아요
"
"https://news.hada.io/topic?id=21801","ICEBlock, 익명으로 ICE 출몰을 신고할 수 있는 앱, 급속도로 확산됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ICEBlock, 익명으로 ICE 출몰을 신고할 수 있는 앱, 급속도로 확산됨

     * ICEBlock은 미국 내 ICE 출몰을 익명으로 신고할 수 있는 iPhone 앱임
     * 미 법무장관 Pam Bondi의 비판이 앱의 인지도를 높이는 계기가 된 상황임
     * 로스앤젤레스 지역에서 약 2만 명이 주로 사용하고 있으며, 최근 ICE 단속이 활발히 일어남에 따라 인기가 급상승함
     * 앱을 사용하면 반경 5마일 이내 ICE 출몰 정보를 공유할 수 있고, 해당 지역에 ICE가 나타나면 실시간 알림을 받을 수 있음
     * 사용자 데이터 수집 및 저장이 전혀 없다는 점이 TechCrunch의 네트워크 분석을 통해 확인됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ICEBlock 앱 개요

     * ICEBlock은 사용자가 미국 이민세관단속국(ICE) 요원 출몰을 익명으로 신고할 수 있도록 설계된 iPhone 앱임
     * 앱이 Apple 미국 App Store 인기 무료 앱 순위 상위권에 올랐으며, 순위 상승의 주 요인 중 하나는 미 법무장관 Pam Bondi의 공개 비판으로 인한 관심 급증임

사용 현황과 확산 배경

     * ICEBlock의 주 사용자는 약 2만 명으로, 대부분이 로스앤젤레스 지역임
     * 최근 로스앤젤레스에서 ICE 단속이 빈번하게 발생함에 따라, 해당 앱의 필요성과 관심이 높아짐
     * Bondi 장관의 비판 이후 월요일 밤사이 앱이 바이럴로 확산되었으며, 화요일 오후 기준 미국 내에서 가장 많이 다운로드된 무료 iPhone 앱 중 하나임

기능 및 특장점

     * 사용자는 앱을 통해 자신의 위치에서 반경 5마일 이내에서 목격한 ICE 요원을 합법적으로 신고할 수 있음
     * ICE 출몰 정보를 공유하는 기능 외에도, 사용자 주변에 ICE 요원이 출몰하면 즉시 알림을 제공함

개인정보 보호 및 안전성

     * ICEBlock은 어떠한 사용자 데이터도 수집하거나 저장하지 않음
     * TechCrunch에서 앱의 네트워크 트래픽을 분석하여, 실제로 개인 정보가 외부로 전송되지 않음을 확인함

        Hacker News 의견

     * 내 입장에서는 경찰 활동이 있는 지역을 피하고 싶어 하는 게 뭐가 문제인지 모르겠음. 내가 뭔가 잘못해서라기보다는, 음주 단속 같은 상황에서 귀찮게 얽히거나, 경찰차 여덟 대가 차로를 막아서 교통체증에 갇히는 상황을 피하고 싶다는 것임. 특히 미국 시민으로서 공항이 아닌 곳에서 신분증 검사를 당하거나, 정체를 증명해야 한다는 질문을 받는 것 자체가 무섭고 피하고 싶은 상황임
          + 앱의 본질을 따지는 건 별로 의미가 없다고 생각함. 이런 앱이 존재한다는 사실 자체가 정부의 경찰국가 시도에 저항하는 신호임. 정부에서 앱 자체의 기능에 신경을 쓰는 게 아니라, 그저 이런 저항적 기술이 싫어서 반대하는 것임. 정말로 앱이 ""경찰 위험""을 알리는 게 걱정이라면, 버튼을 눌러서 ""boo ICE""라고 소리나는 앱이어도 역시나 경찰에 해를 끼친다고 공격했을 것임. 독립 언론도 같이 비판할 수 있으니 정부 입장에서 더 매력적인 타깃이 되는 상황임
          + 네비게이션 앱들은 오래전부터 경찰 활동뿐 아니라 피하고 싶은 교통 정보들을 꾸준히 보고해온 전통이 있음. 경찰과의 만남에서 즐거워진 적이 없어서 가능한 한 피하는 게 합리적이라고 생각함
          + 경찰, 특히 ICE와 마주치는 게 두렵지는 않지만, 그들과의 만남이 내 하루를 더 편하게 해줬던 경험은 한번도 없음. 뭔가 숨길 게 있어서가 아니라 그냥 쓸데없는 행정적 귀찮음임. 궁금한데, 미국에서 몇 세대에 걸쳐 태어난 시민이 ICE에 공식적으로 협조해야할 의무는 실제로 얼마나 되는 지 궁금함
          + 미국 시민도 연방기관에게 납치당했던 사례가 있어서 그들을 피하고 싶은 마음이 정말 합리적임
          + 어릴 때 미국이 '자유의 나라'라고 믿었는데, 영국에서는 경찰이 신분증 제시를 요구한 적이 거의 없음. 경찰이 이름을 물어본 적은 몇 번 있었지만 확인하지도 않고, 신분증을 가지고 다니지도 않음. 비행기 탈 때 빼고는 사진이 있는 신분증이 아예 없음
     * “앱은 어떤 사용자 데이터도 수집하거나 저장하지 않는다”는 내용에 대해, TechCrunch가 실제 네트워크 트래픽을 분석해서 확인했다고 하는데, 이 정도면 꽤 괜찮은 기술 취재임. 이런 검증 절차는 단순하지 않고, 준비와 분석에도 시간이 꽤 걸림. 만약 앱이 보안적이고 certificate pinning까지 한다면, 트래픽을 복호화하려면 리버스 엔지니어링 후 pinning을 우회해야 가능함
          + Apple은 여전히 모든 다운로드 및 푸시 알림 데이터를 가지고 있음. 정부 요청 시 바로 넘겨줄 수 있음. 작성자가 Android 버전을 내놓지 않은 이유는 명시하지 않았다고 했지만, 아래 대댓글에서 이유를 명확히 언급함
     * 여러 가지 복잡한 문제가 있지만, 개인적으로 가장 당황스러운 것은 CNN이 앱을 소개하는 기사 자체를 문제 삼는다는 점임. 단순히 경찰 활동을 신고하는 게 문제가 아니라, 그런 행위에 대한 기사 자체가 문제라는 프레임임. “CNN이 매일 목숨을 거는 경찰을 위험하게 하고 범죄자를 돕는다”라는 식임. 만약 engadget 기사에 조회수가 몰리면 그 또한 목숨을 위험하게 하는 건지, 포럼 스레드에서 같은 이야기가 나오면 그것도 문제가 되는지 궁금함
          + “경찰은 매일 목숨을 내놓는다”라는 말이 실제로는 그렇게 대단하게 들리지 않음. 경찰은 조경 관리직이나 크레인 운전사와 비슷한 사망률이고, 쓰레기 수거원보다 덜 위험하며, 벌목공보다 훨씬 안전함. 위험한 직업이긴 하지만 그렇다고 특별 대우까지 받아야 하는지는 의문임
          + 이 논쟁을 보면, 승인도 없이 다른 나라에 들어간 미군이 공격받았다는 기사가 나오는 것과 비슷하다고 생각남. 왜 있었는지는 언급 없이 “우리 군을 보호하자”며 예산을 늘리라는 논리가 반복됨. 예시로 CNN, NPR, ABC 뉴스 등에서 실제로 나왔던 기사들을 첨부함
     * “앱 개발자를 지켜보고 있고, 조심해야 한다. 이건 표현의 자유로 보호되는 발언이 아니다. 미국 모든 경찰관의 생명을 위협한다”라는 발언이 법대 출신이라는 사람에게서 나왔다는 점이 정말 황당함. Waze, 헬멧 톡톡, 앞차 상향등 깜빡이로 경찰 위치 알리기 모두 이와 다르지 않다고 봄. 법원에서 다뤄진 바 있는데, 항상 표현의 자유로 인정됨
          + 최근 대법원이 전례를 쉽게 뒤집는 경향을 보여서 걱정임
          + 그는 앱이 경찰 암살용으로 쓰인다고 암시하는 것처럼 들림. 만약 그게 사실이라면 상황이 다르겠지만, 이 앱의 목적은 전혀 그런 게 아님
          + 그들은 이미 상황을 알고 있지만 별로 신경 쓰지 않음. 자기들에게 우호적인 대법원이 있고, 설령 법정에서 져도 시도 자체에 따르는 불이익이 없음
          + ""헬멧톡""이 뭐냐는 질문이 나옴
          + 차이는 규모라고 생각함. Waze 같은 앱들은 모든 사람이 알 수 있지만, 몇몇 운전자끼리만 정보를 공유하는 것과는 확연하게 다름
     * 경찰이 일반 시민보다 막강한 권력을 가지므로 훨씬 높은 기준과 책임이 요구됨. 경찰 무전은 암호화하지 않아야 하며, 얼굴 인식을 위한 공개 데이터베이스, 차량 및 경찰 위치도 추적 가능해야 함. 시민 감시에 사용하는 기술을 경찰에게도 적용해야 진정한 책임이 보장됨. 참고로 icespy.org에서 ICE 직원들에 대한 얼굴 인식 검색이 가능함
          + 무전을 암호화하지 않는 건 반대임. 범죄자도 바로 스캐너를 사용할 테고, 진짜 범죄자를 잡는 게 거의 불가능해질 것임. 대신 경찰 무전 녹음 및 아카이빙은 반드시 해야 하고, 항공 교통관제처럼 사고가 나면 독립 조사 및 공개가 이루어져야 진짜 책임이 실현됨
     * 국토안보장관과 검사장들이 일반 시민 한 명을 이름까지 언급하며 공격하는 모습은 정말 심각한 문제임
          + 전형적인 권위주의적 행태임
          + “이름을 언급하는 건 이상할 것 없다”는 의견도 있지만, 그를 겨냥해 공격하는 건 더 심각한 문제라고 생각함
     * 13년 전 Apple이 해외 드론 공격 알림 앱을 ""불쾌한 콘텐츠""라는 이유로 거부한 것에 비해, ICEBlock 같은 앱은 App Store에 남겨둔다니 흥미로움. 당시 관련 기사: ACLU 기사
          + Apple은 홍콩 시위 중 중국 정부 요청으로 비슷한 앱을 삭제한 적도 있음. 해당 앱이 경찰 위치정보를 공개해서 문제가 됐다고 함. (관련 기사: CNBC 기사)
          + Apple이 현 미국 행정부를 좋아하지 않아서 앱을 일부러 남겨두는 것일 수 있음. ICE가 군대가 아닌 점, 미국 내 민간인에게 정보를 제공하는 것과 군 작전을 방해하는 것의 차이도 있다고 생각함. 드론 스트라이크 앱은 미국 전쟁 목표를 방해하는 효과가 있으니 Apple이 거부한 이유를 이해할 수 있음
     * 2025년 미국에서 이런 앱이 필요하다는 사실 자체를 잠시 곱씹어볼 만함
          + 이게 매우 미국적인 현상임. 짐 크로우 시대에는 ‘그린북’이 흑인 여행자들에게 안전한 업소를 안내했고, ‘언더그라운드 레일로드’는 노예들이 자유를 찾도록 돕는 정보 네트워크였음. 2차 세계대전 때는 일본계 미국인을 보호하기 위해 지역 사회가 은신처 정보를 공유했고, LGBTQ+ 커뮤니티도 위험 속에서 안전한 공간을 찾는 정보망을 만들어왔음. 원주민 커뮤니티 역시 오랜 기간 안전 이동과 리소스 정보를 공유해 옴
          + 감정적인 논평 대신, 언제쯤 이런 ICE 이슈가 끝날 수 있을지 되돌아봄
          + 최근 미국 뉴스를 들을 때마다 마치 나쁜 영화 속 장면 같다는 느낌이 강하게 듦
          + 만약 이런 앱이 누군가에게 “이상하다”라는 시그널로 다가온다면, 상황이 이미 심각하다는 뜻임
     * ICEBlock은 iOS 전용임. 개발자가 Android 관련 프라이버시 이슈에 대해 공식 입장을 냄. Android에서 정보를 안전하게 관리해야 한다는 부담(정부의 소환장 시 협조 가능성) 때문임. Apple도 푸시 알림 때문에 내부적으로 정보를 다루겠지만, 이 경우엔 Apple이 정부에 맞설 재력이 있고, 개발자 손에서 벗어난다는 판단임
          + GrapheneOS에서 반론을 제기한 링크: GrapheneOS 입장
          + 개발자가 잘 모르는 것 같음. 오히려 android가 더 보안적으로 유리함. (A: APK 직접 설치로 Play Store에 기록이 남지 않음, B: https 엔드포인트를 polling해서 푸시 알림 없이도 동작, C: 로컬 알림 활용 가능). 이럴 경우 기업이 줄 수 있는 로그는 서버의 IP 주소 기록 정도임
          + 구글 플레이스토어를 옹호하고 싶진 않지만, 기술적으로나 프라이버시 면에서 Android 출시를 안 하는건 설득력 없음. 사용자 계정 없이도 푸시 알림 가능함(직접 그렇게 운영 중임). 다만 개인 개발자가 Android 앱을 올릴 때 배포 전에 2주간 12명의 테스터를 써야 하는 번거로움은 있음. 개발자에게 애플리케이션을 변경해 사용자 정보를 공개하도록 강요하는 건 가능할 수도 있음
          + 안드로이드에서도 프라이버시에 유리한 다양한 솔루션이 존재함. 단, 추가 개발이 필요할 수 있음. 예시로 ntfy가 있고, Google이 AOSP의 많은 부분을 독점 계층에 밀어넣어 다른 접근법이 자리잡기 어렵게 만듦
          + 결국 개발자가 하는 일이 인프라든 중개든 암호화해서 IP와 설치 여부 정도만 알아볼 수 있게 만들어야 함(veilid 또는 tor over http 활용 가능). 또한 앱이 아닌 웹사이트로 접근했으면 더 익명성을 보장할 수 있었을 텐데, 굳이 corporate controlled app 형식으로 만든 건 잘 이해가 안됨
     * “앱 개발자를 조사중이고, 주시해야 한다…”는 발언에 대해, 더 이상 이런 행동을 숨기려 하지 않는다는 점이 무섭다고 느낌. 게다가 법대 출신인 사람이 이런 이야기를 함. 덕분에 바로 앱을 설치하러 간다는 의견이 나옴. Streisand 효과라고 농담함
          + 실제로 일부러 Streisand 효과를 노리는 것일수도 있다고 생각함. 거짓 신고로 앱의 효용성을 떨어뜨리려는 시도도 예상됨. 만약 법원이 앱 승소 판결을 내리면 오히려 그걸 이용해 혼란을 일으키는 것도 전략일 것임. 결국, 대중 앞에서는 화난 척하면서, 뒤에서는 잘못된 정보와 혼란이 퍼지는 걸 즐길 수도 있음
"
"https://news.hada.io/topic?id=21799","코드 작성은 절대 병목 지점이 아니었음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         코드 작성은 절대 병목 지점이 아니었음

     * 소프트웨어 개발에서 병목은 코드 작성이 아니라 코드 리뷰, 지식 이전, 테스트, 디버깅, 협업/소통 등 다양한 인간 중심 프로세스에서 발생함
     * LLM 덕분에 코드 생성 자체는 매우 쉬워졌지만, 오히려 이해, 검증, 신뢰에 드는 비용과 부담은 더욱 커짐
     * 빠른 코드 생성은 리뷰어·통합자·유지보수 담당자에게 더 많은 부담을 주며, 팀 전체의 속도가 실제로 빨라지지 않음
     * 코드 이해가 가장 어려운 부분이며, LLM이 코드를 생성해도 팀의 신뢰와 맥락 공유 없이는 품질이 보장되지 않음
     * LLM은 프로토타이핑·자동화엔 강력하지만, 신중한 설계·리뷰·공유 맥락 등 소프트웨어 개발의 기본을 대체하지 못함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

코드 작성의 진짜 병목 지점

     * 수년간 코드 작성 작업 자체는 소프트웨어 엔지니어링의 병목이 아니었음
     * 실제 병목은 코드 리뷰, 멘토링과 페어 프로그래밍을 통한 지식 전달, 테스트, 디버깅, 협업·소통의 비용에서 발생
     * 티켓 관리, 플래닝 회의, 애자일 미팅 등 복잡한 절차가 속도를 더 늦춤
     * 품질 보장을 위한 이러한 과정들이 실제로는 코드 작성 자체보다 훨씬 많은 시간과 사고를 요구함
     * 그러나 LLM 덕분에 동작하는 코드 생성 비용은 0에 수렴하고 있음
     * 하지만 코드 이해, 테스트, 신뢰 확보 비용은 오히려 더 높아짐
     * 초기 구현 속도는 빨라졌지만, 더 많은 코드가 리뷰/통합/유지보수 대상이 되어 부담 가중

LLM이 일의 본질을 바꾼다 – 제거하지는 않음

     * Claude 같은 LLM 툴은 초기 구현 속도를 높여주지만, 결국 더 많은 코드가 더 짧은 시간에 시스템에 유입되면서 리뷰와 유지보수 담당자에게 더 큰 부담을 줌
     * 특히 다음 상황에서 이 부담이 심화됨
          + 작성자가 본인이 등록한 코드를 충분히 이해하고 있는지 불확실함
          + 생성된 코드가 익숙하지 않은 패턴이거나 기존 컨벤션을 위배함
          + 경계 조건 및 의도치 않은 부작용이 명확히 드러나지 않음
     * 이로 인해 코드 생산은 쉬워져도, 검증 난이도는 더 높아지고 결과적으로 팀 전체 속도를 높이지 못함
     * 원래 개발자들 사이에는 “복사-붙여넣기 엔지니어링” 에 대한 농담이 있었지만, LLM으로 이 현상이 훨씬 증폭됨

코드 이해가 진짜 어려움

     “코드의 가장 큰 비용은 작성이 아니라 이해임”

     * LLM 덕분에 코드 생산 자체는 빨라지지만, 동작을 추론하거나, 미묘한 버그를 찾거나, 장기 유지보수를 보장하는 작업은 결코 쉬워지지 않음
     * 특히 리뷰어가 생성 코드와 직접 작성한 코드를 구분하지 못하거나, 선택한 풀이의 이유를 파악하기 어려울 때 더욱 어려워짐

팀은 여전히 신뢰와 공유 맥락에 의존함

     * 소프트웨어 개발은 기본적으로 협업이 전제이며, 공유된 이해와 정렬(Alignment), 멘토링에 전적으로 의존함
     * 코드가 논의 및 리뷰 속도보다 빠르게 생성될 때에는, 팀이 실제로는 품질을 확인하지 않았으면서도 품질이 이미 검증된 것처럼 착각하게 됨
     * 이로 인해 리뷰어와 멘토의 심리적 부담이 커져 팀 전체의 속도가 새로운 방식으로 느려질 수 있음

LLM은 강력하지만 본질을 해결하지 못함

     * 빠른 프로토타이핑, scaffold, 자동화에 LLM이 가치가 크지만 명확한 사고, 신중한 리뷰, 깊이 있는 설계의 필요성을 없애주진 않음
     * 코드 작성 비용 자체는 하락했지만, 팀이 코드를 함께 이해하고 의미를 부여하는 비용은 변하지 않음
     * 진짜 병목은 여전히 ""이해와 협업""에 있음

   그런데도 인력줄이는거보면 암담함니다.. 프로젝트 12개를 4명이 어떻게하라고.. 거기에 한명은 관리도하면서..

   ㅠㅠ 고생이 많으시겠군요

   제가 딱 이 글에서 소개하는 기술이 부족하고 LLM이 심각하게 의존하는 개발자입니다.
   기술지식이 부족해서 AI없이는 WBS에 맞춰 업무하는게 어려운 상황입니다만..
   그나마 선배 개발자분들의 리뷰시간을 줄이기 위해 제가 뭘 하면 좋을까요..?
   덤으로 제 지식도 쌓이면 좋겠네요..
    1. Claude Code, Devin 같이 알아서 돌아가는 서비스보단 Claude같은 채팅형 클라이언트 + Windsurf Tab같은 자동완성 익스텐션 정도만 활용하기
    2. 채팅으로 물어보는 건 개발방향, 라이브러리 간 비교, 구조 분석, 에러 해결 정도로 그치고, 코드 작성 자체는 가능한한 직접 하기
    3. 프로그래밍 언어, 소프트웨어 공학, 각종 입문서 등 책을 보는걸 게을리하지 말것

   리뷰시간을 줄이려면 본인이 작성한 코드가 어떤 배경에서 나왔고, 다양한 구현방법 중에서 왜 그걸 골랐는지 등을 본인이 결정해야하고 설명할 수 있어야합니다. 그러려면 3.을 주말이든 남는 시간에 꾸준히 하셔야하고, 유명한 라이브러리나 다른 사람들이 깃헙에 올린 코드를 랜덤하게 둘러보면서 코드 구조와 구현스타일 같은걸 분석해보시면 좋습니다.

   좋은말씀 감사드립니다!!

        Hacker News 의견

     * 경험 많은 멘토로서 야심은 크지만 아직 미숙한 인턴들을 지도하면서, 하루 만에 일주일, 혹은 이주일 분량의 코드를 쏟아내는 모습을 관찰한 경험담임. 하지만 내 일은 이전보다 오히려 더 어려워졌음. 코드 리뷰를 할 때, 인턴들은 자신이 작성한 코드에 대해 깊은 고민이 부족해 내 피드백이 잘 전달되지 않음. 단순한 버그나 사소한 문제는 거의 없어졌지만, 남은 문제들은 더 미묘하고 복잡한 것들이며, 설명하기도 훨씬 힘들어짐. 게다가 전에 본 적 없는 새로운 유형의 버그들이 등장함. 겉보기엔 멀쩡한 코드가 실제로는 전혀 동작하지 않기도 하고, 완성도가 있는 것처럼 보여도 기본적인 부분부터 망가져 있는 경우가 많음. LLM과 협업하는 주니어들은 오히려 단순하면서도 논의 가치가 있는 코드를 내놓기보다, 여러 측면에서 협업과 유지보수에 문제를
       일으키는 복잡한 코드를 한 번에 완성된 것처럼 만들어내는 경향임. 결과적으로 PR 하나를 최종 품질까지 다듬는 전통적인 “점진적 개선” 방식이 제대로 작동하지 않는 느낌임. 피드백을 주면, 원래 PR을 고치기보다 아예 새로운 접근을 제시하기도 하는데—종종 또 다른 새로운 문제를 일으킴. 그래서 오히려 시니어들이 주니어보다 이 PR 하나에 훨씬 오랜 시간을 들이게 됨. 주니어 쪽에서는 본인이 매우 생산적이라고 느낄지 모르지만, 점점 시니어 리뷰어의 피드백이 예전만큼 따뜻하거나 격려하는 분위기가 아니게 되는 현상도 있음. 궁극적으로 많은 테스트 케이스를 필수로 요구했던 게 그나마 효과 있었는데, 이 테스트조차 비슷한 문제로 한계에 부딪침
          + 이런 '완성도 있어 보이지만 전혀 동작하지 않는' 코드를 보고, 예전에 내가 겪었던 25만 달러 규모의 해외 소프트웨어 개발 프로젝트가 생각남. 사양서만 보면 모든 게 맞는 듯했지만 실제론 전혀 일관성 없는 시스템이었음. 사양서 해석에만 집착하고 상식적인 부분을 놓쳐서, 결국 전체 프로젝트를 바로 폐기했던 기억임. 이제 이런 일들이 LLM 덕분에 자동화, 무료화되고 있다는 사실이 인상적임
          + 이런 문제에 완전히 공감함. 내 개인적인 경험으로도 LLM을 활용할 때 수천 줄의 코드를 아주 빠르게 만들 수 있긴 하지만, 진짜 어려운 일은 코드 리뷰, 버그 수정, 보안 취약점 점검, 리팩터링, 불필요한 코드 제거 등임. 결국은 직접 코딩하는 것이 더 생산적인 상황이 많았음. LLM은 자동 완성이나 간단한 조각 생산에만 쓰는 게 가장 현실적인 활용법임. 주니어가 LLM을 중간에 끼고 내가 또 전달해야 한다면 효율이 더 떨어질 것 같음. 현재 LLM으로 더 생산적이라고 느끼는 사람들은 실제로는 이런 중요한 작업을 아예 건너뛰거나 아예 신경 쓰지 않아서 그렇게 느끼는 것일 수도 있음. 실제로 제품 품질에 신경 쓰는 소수의 사람들만이 엄청난 코드량을 감당해야 하는 부담을 떠안게 됨. 이들이 불필요하게 까다로운 사람이라고 오해 받기도 하지만, 사실
            사용자에게 최상의 제품을 전달하려는 진짜 주인공임. 개선 방안은 딱히 없고, 오히려 상황이 더 악화될 것으로 보임. LLM만 사용해서 훈련된 개발자들이 업계에 계속 유입되고, 도구를 만드는 회사들은 계속 과장된 마케팅을 일삼음. 결국 퀄리티 유지의 부담만 증가하는 구조임
          + 시니어들이 PR 하나에 주니어보다 더 많은 노력을 쏟게 되는 'effort inversion' 현상을 나도 겪고 있음. 나의 경우엔 PR 작가들이 블로그 글이나 보도자료 작성에 AI를 활용하는데, 주제 전문가인 내가 결과물을 받으면 온갖 AI 환각과 오류를 바로잡느라 작업 시간이 3, 4배로 늘어남. 그들의 업무는 원래 나를 지원하는 건데, 이제 내가 그들을 도와야 하는 상황임. 심지어 AI 환각도 그때그때 다르니 매번 새로 고생함. 이런 현상을 임원에게도 이미 전달했는데, 이런 식이라면 PR 인력의 절반이 내년에 사라질 것 같음. 이메일을 복사해서 ChatGPT에 붙여넣고 다시 내게 보내주는 역할이 필요 없다면, 내가 직접 할 수 있음
          + ""리뷰 중 내가 한 피드백이 잘 전달되지 않았다""는 부분에 대해 더 자세한 설명을 해줄 수 있는지 궁금함. 비슷한 문제를 겪고 있어, 해결책이나 통찰이 있다면 공유 부탁임
          + 내 생각을 더하면, 이전 세대는 리팩터링이나 유닛 테스트를 통해 소프트웨어 구조, 설계에 대한 깊은 이해를 자연스럽게 쌓았던 것임. 그런데 앞으로 LLM이 유닛 테스트까지 생성한다면 개발자는 ""내가 뭘 필요로 하지?"", ""이걸 더 단순하게 할 방법이 뭐지?"" 같은 자기 반성과 학습을 할 기회가 없어질 수도 있음. 내가 생각하는 '개발자', '엔지니어', '아키텍트' 차이는 바로 여기에 있음. LLM이나 'vibe coding'은 그런 방식의 마인드셋을 결코 길러주지 못함. Go처럼 문법 부담이 적은 언어에서는 이런 설계 미스가 리뷰에서 더 쉽게 드러남. Go의 유닛 테스트 구조가 코드 복잡성 진단에 유용함. 결국 우리는 더 나은 테스트/리뷰 방법이 필요함. 퍼즈 테스팅, 단위 테스트, 통합 테스트만으론 부족함. 코드의 분기문이 제대로 호출되고 있는지, 만족 조건이
            성립되는지 논리적으로 확인해줄 수 있는 자동화된 테스트 프레임워크가 필요하다고 생각함
     * LLM 덕분에 소프트웨어 신규 도입 비용이 거의 0에 가까워지고 있지만, 그 코드를 깊이있게 이해하고 테스트하고 신뢰하는 비용은 그 어느 때보다 높아진 느낌임. 하지만 내 경험상 LLM이 만든 코드가, 이미 떠난 사람이 만들어놓고 제대로 질문조차 못하는 수많은 과거 코드나, 온라인에서 굴러다니는 코드와 비교해 그렇게 훨씬 나쁘다고 생각하지는 않음. 오히려 LLM은 테스트 코드를 작성하는데 인간처럼 귀찮아하지도 않고, 피곤해서 대충 넘어가는 일도 없음. 내 철학은 ‘모든 코드는 잠재적 부채’라는 전제에서 출발함. 그래서 그다지 코드 신뢰도에 대해 걱정하지 않음. 실제로 AI로 거대한 코드베이스를 만들어가며 충분히 동작시키기도 했음—단, 도메인이 검증 가능하고, 테스트와 반복이 많아야 가능함. 결론적으로 LLM 결과물로 코드 생산은
       빨라졌으나, 코딩 자체의 지적 자극이 줄어든 느낌이라 내 뇌도 함께 나태해지는 느낌임. 오히려 요구사항 정의, 설계 등 앞단의 두뇌노동이 훨씬 중요한 시대로 넘어감
     * LLM 없이도 이미 업계는 ""코드 부족""이 아니라, 시장 수요와 자본 한계로 인해 개발 속도에 한계가 있었던 상태임. 툴링이 너무 좋아져서 코딩 자체가 핵심이 아니게 된 것임. 초창기와는 완전히 달라진 환경임. Bill Gates가 10대 시절, 단순히 ‘코딩할 수 있는 능력’ 자체가 희소 자원이던 시절의 일화가 기억남. 회사가 급해서 16살짜리 Gates와 Paul Allen을 고용했고, 그들이 단순히 빨리 코드를 짠다는 이유만으로 신기해했다는 이야기임. 지금은 ""무엇을 개발하고, 거기에 비즈니스가 성립하는가?""가 더 중요한 문제임
       관련 영상
          + 코딩이 병목이 아니었던 건 시장 수요 때문이라는 주장에 동의함. AI 붐 전에 Marc Andreessen도 ""자본은 넘치지만 투자할 좋은 아이디어가 부족하다""고 말했음. 난 그 말이 현실을 잘 반영한다고 생각하지 않지만, 적어도 데이터상으론 그의 발언이 신빙성 있어 보임
          + Bill Gates의 예전 일화처럼, 여전히 고품질의 코드를 짜고 깊이 이해하는 능력은 드문 자원임. 다만 예전과 다른 점은, 산업이 그 능력을 중요하게 여기지 않는 분위기임
          + AI의 임팩트 분석 관점에서 보면, 우리는 효율성을 지나치게 과신하는 경향이 있음. 하지만 현실 경제는 그보다 훨씬 복잡한 구조로 병목이 발생함. 코드 생산량이 100배 늘어난다고 해도 실제 쓸모가 있는지는 불명확함
     * 요즘 누군가의 경험담을 보면 너무 우울해보임. 주니어가 동작하지 않고, 직접 테스트/검증도 하지 않고, 간결하게 정제하지도 않은 거대한 코드 뭉치를, 문서나 주석, 설명도 없이 넘긴다면 이미 그 자체가 ""LLM이 인간에 탑재된 버전""이라고 표현할 수 있음. 결국 중요한 것은 비판적 사고와 결과에 대한 책임 의식임. 오히려, LLM이 기존 주니어 소프트웨어 엔지니어보다 피드백에 충실히 반응할 가능성이 더 높다고 생각함
          + 주니어가 LLM으로 짠 코드를 선배에게 리뷰 맡기는 현상을 LLM의 결점으로 보는 시각이 많지만, 나는 오히려 주니어 자체의 부족함을 드러낸 현상이라고 봄. 차라리 시니어가 직접 LLM을 활용하는 게 더 낫다고 생각함
     * 나 역시 한때는 코드 작성이 병목이라고 여겼지만, 10년 동안 실제 어려운 건 기술을 비즈니스와 일치시키는 작업임을 깨달음. B2B나 SaaS처럼 고객마다 달라지는 복잡한 코드더미를 다루는 환경이라도, 기술이 비즈니스에 제대로 맞춰진다면 모든 게 순조로움. 요즘은 기술이 충분히 발전했으니, 이제 진짜 관건은 개발자의 ‘에고’와 고객 가치에 집중하는 자세임. 고객이 실제로 원하는 것, 거기에 돈을 낼지, 아예 웹 인터페이스가 필요한지도 고민해야 함. 개발자가 자기 만족을 위해 만드는 “캣토이 기능”이 클라우드 비용을 늘리는 진짜 원인임. 더욱 슬픈 사실은, 공격적인 인센티브나 스톡옵션, 연봉을 던져도 이 본질적 문제는 해결되지 않는다는 점임. 소프트웨어를 잘 만들고 싶다는 ‘사명감’이 있는 누군가, 꼭 한 명이라도 고객과 직접 소통하며
       제대로 해보겠다는 의지가 있어야 함
     * 조직에서 LLM이 진짜 도움이 됐던 순간은 내 개인 프로젝트나 사이드 프로젝트임. 작은 문제를 해결하기 위한 앱을 개발할 때, 시간이 부족해서 코드 작성이 실제로 큰 병목이 됨. 이런 프로젝트엔 LLM 활용도가 100%임
          + 나도 백퍼 공감함. 하루 1~2시간 Claude code에 투자하면 주말이 끝나갈 때쯤 실사용 가능한 결과물 하나가 생김. 짧은 시간 투자로 아이디어 검증이나 실험이 쉬워짐. 이런 LLM 도구는 실제로 프로 조직에서도 엄청난 가치를 낸다고 생각함. 평소엔 시간이 부족해서 못 만들었던 각종 관리 도구나 자동화 시스템도 빠르게 프로토타이핑이 가능함. 동료가 DB 쿼리나 단순한 자동화가 필요하다면 Claude에 물어본 뒤 리뷰하고 바로 붙여넣기만 하면 됨. 이렇게 엔지니어가 아니라도 자기 영역의 반복 업무를 스스로 처리할 수 있도록 지원하는 게 내 프로젝트 mcp-front[0]의 목적이기도 함
            mcp-front 깃허브
          + 솔직히 지금 내 커리어 상태에서는 일주일씩 쏟아붓기 어렵고, 오랜 노하우로 비기능 요구사항, 장기적 관점까지 항상 고려하고 있음. 테스트 같은 걸 건너뛴다 해도 결국엔 계속 고민할 일이 많음
     * Robert C. Martin의 ""코드를 읽는 시간이 쓰는 시간보다 10배 이상 많다""는 명언이 떠오름
       관련 인용구
          + 안타깝게도 그의 클린 코드 스타일이 실제로는 맥락을 파편화시켜 의도가 더 파악하기 어려워지는 경우도 많음
          + 더 나쁜 건, 오히려 작성 시간은 줄이면서 읽는 시간만 늘려서 전체 업무량이 줄지 않을 수도 있음
     * 아무도 언급하지 않은 Joel Spolsky의 2000년 10월 2일 글을 소개함
       Joel on Software: Painless Functional Specifications (2000)
       진짜 병목은 코드가 아니라 기능명세(spec)임. 소프트웨어가 어떻게 동작해야 하는지, 영어, 다이어그램, 사용자 스토리 등 명확한 명세를 만드는 일이 더 중요함. 명세가 잘 잡혀 있다면, LLM도 충분히 훌륭한 솔루션·테스트·통합 테스트를 한 번에 만들어낼 수 있음. 너무 크다면, 명세는 마크다운 파일 하나로 관리할 것이 아니라 위키처럼 기능별로 쪼개 링크와 참조로 관리해야 함. 구현 난이도가 아니라, 명세에 얼마나 힘을 들였는지가 진짜 경쟁력임
     * 필자는 저자 견해에 동의하지 않음. 대기업 관점에서는 코드가 병목이 아니겠지만, 스타트업 입장에서는 자원이 한정적이고 효율적인 플래닝이 더 중요했음. 즉, 실제로 제대로 동작하는 코드를 만드는 작업 자체가 가장 큰 병목이었던 경우도 많음. 결국 이런 논의에서는 ""AI/LLM이 얼마나 유용한가""를 일반화할 수 없음. 어떤 팀에게는 코드가 병목이었고, 어떤 팀에게는 아니었음
          + 내가 본 현장도 동일함. LLM은 소수의 뛰어난 소규모 개발팀에게 엄청난 레버리지를 줌. 뛰어난 인력이 있어야 LLM도 좋은 결과를 내고, 대규모 팀은 오히려 협업 비용이 너무 커 LLM 혜택이 적음. 원래 잘하던 소수 팀이 가장 많이 '슈퍼차지'되는 느낌임
     * 모두 알다시피 LLM이 만들어내는 코드는 엉망인 경우가 많고, 리뷰조차 불가능함. 주니어가 제출하는 LLM 코드가 이상해서 이유를 물어보면 본인도 모르고, LLM이 만들었다고 답하는 상황임. 결국 이런 추세는 코드 유지보수에 ‘노이즈’와 ‘오버헤드’만 늘림. 만약 LLM 도입을 피할 수 없다면, 리뷰와 유지보수도 LLM에 맡길 수밖에 없음. 물론 결과는 더 복잡한 스파게티가 될 것임. 하지만 현실적으로 대다수 비즈니스에서는 품질이 딱히 중요하지 않은 경우가 대부분이고, LLM 코드가 대충 잘 동작하면 그걸로 족하다고 여김. 아니면 필요한 만큼 LLM을 계속 덧붙여서 결국 해결되는 수준이면 충분함
"
"https://news.hada.io/topic?id=21762","CSS로 구현한 Liquid Glass","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         CSS로 구현한 Liquid Glass

     * Apple이 WWDC25에서 공개한 Liquid Glass 디자인 언어의 핵심 효과를 CSS와 SVG 필터 조합으로 재현하는 방법
     * Specular Highlights, Blur, Color Filter, Refraction, Edge/Ripple Distortion, Chromatic Aberration 등 실제 유리 재질을 모방하는 다양한 레이어를 단계별로 CSS/SVG로 구현
     * 주요 효과 구현에는 backdrop-filter, box-shadow, SVG DisplacementMap, Turbulence, ColorMatrix, Offset, Blend 등 고급 CSS·SVG 기술 사용
     * 브라우저 호환성은 Chrome 중심이며, Safari/Firefox에서는 SVG 필터가 동작하지 않아 Blur·Shadow만 적용됨
     * 복잡한 효과일수록 GPU 사용량이 크고, UI 전반에 남용 시 성능 저하가 발생할 수 있어 핵심 영역 위주 사용 권장
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Liquid Glass란?

     * Apple의 최신 디자인 언어로, 실제 유리처럼 빛의 반사·굴절, 색 변화, 입체감을 모방한 고급 그래픽 효과를 지향함
     * UI 요소에 깊이감과 생동감을 더하는 데 활용되며, 시각적으로 매끄럽고 물리적 감촉을 연상시키는 특징이 있음

효과의 주요 레이어 및 구현 방법

  1. Specular Highlights(유리의 반사광)

     * 유리 가장자리에 하이라이트를 추가해 깊이와 입체감을 강조함
     * CSS의 inset box-shadow 여러 개를 중첩 적용해, 각도별로 빛이 반사되는 느낌을 표현
box-shadow:
    inset 10px 10px 20px rgba(153, 192, 255, 0.1),
    inset 2px 2px 5px rgba(195, 218, 255, 0.2),
    inset -10px -10px 20px rgba(229, 253, 190, 0.1),
    inset -2px -2px 30px rgba(247, 255, 226, 0.2);

     * [Fresnel Effect]를 참고해, 빛의 반사 강도를 자연스럽게 조정함

  2. Blur(배경 블러)

     * 유리 뒤쪽 콘텐츠에만 블러를 적용하기 위해 backdrop-filter: blur(20px); 사용
     * 실제 요소 자체가 아닌, 배경 콘텐츠에만 효과가 적용됨

  3. Color Filter(색상 강조)

     * contrast(80%) saturate(120%) 등의 backdrop-filter를 추가로 적용해, 유리 뒤 콘텐츠의 채도 증가·대비 감소로 선명한 색감 구현
backdrop-filter: blur(20px) contrast(80%) saturate(120%);

4. 굴절(Refraction) 시뮬레이션

     * 유리의 곡면이 배경을 비틀어 보이게 하는 굴절 효과는 CSS 단독으론 구현 한계가 있어, SVG 필터와 결합하여 구현
     * Displacement Map: SVG의 gradient를 활용해, 가장자리에선 픽셀을 압축하고 중앙부는 늘어나는 왜곡 맵(Identity Map)을 생성
          + 예시 SVG의 빨간/파란 그라디언트로 X/Y 방향 픽셀 이동량을 정의
          + 그라디언트 ramp를 조절해 가장자리 압축·중앙 확장 패턴을 설계
          + SVG를 Data URL로 변환해 feImage로 로드, feDisplacementMap에서 좌표 변형에 활용
     * Ripple Distortion: feTurbulence(노이즈 텍스처)와 추가 feDisplacementMap 조합으로 유리 표면의 미세한 파동까지 묘사
     * Chromatic Aberration(색수차): feColorMatrix, feOffset, feBlend 조합으로 RGB 각 채널을 분리·오프셋 후 합성하여, 실제 유리에서 관찰되는 빛 분산 효과 모방

CSS/SVG 효과 적용 방법

     * SVG filter를 정의한 후, filter: url(#filterId); 를 활용해 DOM 요소에 적용
     * 모든 효과가 결합된 필터 체인은 실제 유리 패널같은 질감·빛 왜곡·색상 변조를 동시에 재현함

호환성 및 성능 주의점

     * SVG displacement filter는 Chrome에서만 정상 지원. Safari, Firefox 등에서는 효과가 제한됨(Blur/Shadow만 남음)
     * Glass 요소가 많거나 애니메이션 적용 시 GPU 부하가 크고, 렌더링 성능 저하 발생
     * 현실적으로는 Hero/Feature 영역 등 강조가 필요한 일부 UI에만 사용하는 것이 적합함

참고 및 크레딧

     * 본 구현법은 Smashing Magazine의 ""A Deep Dive Into The Wonderful World Of SVG Displacement Filtering""과 Apple Liquid Glass 공식 문서에 기반함
     * SVG displacement, CSS backdrop-filter, box-shadow 등 최신 웹 기술의 조합적 활용이 핵심임

   이 분이 구현하신 게 더 자연스러운 것 같네요
   https://v0.dev/chat/dynamic-frame-layout-1VUCCecq7Uy

   이건 safari에서는 적용안되네요

   웹에서 구현하기에는 아직 여전히 어색하네요ㅎㅎ
"
"https://news.hada.io/topic?id=21703","제임스 웹 우주 망원경, 외계 행성 첫 직접 이미지 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    제임스 웹 우주 망원경, 외계 행성 첫 직접 이미지 공개

     * 제임스 웹 우주 망원경(JWST) 가 태양계 밖 행성을 직접 촬영한 첫 사례로 기록됨
     * 연구진은 주성의 강한 빛을 차단해 희미한 적외선 신호를 감지, 해당 천체를 외계 행성 후보로 판단함
     * 이 행성은 토성 질량에 가깝고 항성 TWA 7을 50배나 멀리서 공전함
     * 망원경 특유의 코로나그래프와 이미지 처리 기술로 기존 간접적 발견법과 달리 직접 이미지를 확보함
     * 이번 발견은 외계 행성계의 다양성과 진화 이해에 중요한 전환점으로 평가됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

제임스 웹 우주 망원경(JWST)의 첫 외계 행성 직접 촬영

  적외선 신호 감지 및 외계 행성 후보 발견

     * 천문학자들은 JWST를 통해, 지구에서 약 111광년 떨어진 어린 별 TWA 7 주변의 잔해 원반 구조에서 희미한 적외선 광원을 발견함
     * 이 광원은 아직 공식적으로 기록되지 않은 외계 행성일 가능성이 높고, 확인될 경우 JWST가 직접 촬영한 최초의 외계 행성 사례가 됨

  기존 간접적 발견법과 새로운 직접 촬영의 의미

     * 지금까지 발견된 대부분의 외계 행성은 행성이 별 앞을 지나면서 생기는 미세한 그림자 등 간접적 탐지 방법으로 확인되어 왔음
     * 새로 발표된 논문에 따르면, 이번이 제임스 웹 우주 망원경이 직접 이미지를 촬영한 첫 사례로 기록됨

  관측 방식과 코로나그래프의 역할

     * 먼 별 주변 행성은 보통 중심 별 빛에 가려 확인이 매우 어려움
     * 연구진은 강한 별빛을 차단하는 코로나그래프를 활용해 주변의 희미한 천체를 관측했으며, 고급 이미지 처리로 남은 눈부심도 제거함
     * 이로써 TWA 7 주변의 약한 적외선 광원이 직접 이미지로 촬영됨

  후보 행성(TWA 7 b)의 특성과 의미

     * 관측된 적외선 광원이 배경 은하일 확률은 약 0.34% 에 불과해, 대부분의 증거가 TWA 7 b라는 새로운 외계 행성임을 시사함
     * 토성과 비슷한 질량이며, 세 개의 먼지 띠 중 틈새 부분에서 약 120도(화씨)의 온도로 존재함
     * 별과의 거리는 지구-태양 거리의 50배임
     * 해당 행성의 위치와 질량이 잔해 원반 형태에 영향을 준 것으로 보여, 외계 행성이 원반 구조를 형성하는 과정에 대한 발견도 포함됨

  시뮬레이션과 추가적 검증

     * 연구진은 컴퓨터 시뮬레이션을 통해 외계 행성계의 모습을 시각화했으며, 망원경 이미지와 높은 정합도를 보임
     * 이를 토대로 행성 존재에 대한 신뢰도를 높임

  외계 행성계 연구의 새로운 지평

     * 이번 촬영은 토성 질량급 외계 행성의 직접 이미지화에서 중요한 진전을 나타냄
     * 제임스 웹 우주 망원경의 탁월한 성능이, 기존 기술로는 접근이 어려웠던 저질량·원거리 외계 행성의 탐색에 새로운 가능성 제시함
     * 이러한 발견은 외계 행성계의 형성과 진화 과정에 대한 이해 증진에 기여함

결론

     * Lagrange 박사는 이번 결과가 ""이 질량의 행성을 예상한 바로 그 위치에서 찾을 수 있었다""며, JWST의 관측력이 외계 행성 연구의 새로운 창을 열었다고 평가함
     * 더 많은 직접 관측을 통해 행성계 다양성과 그 진화 방식에 대한 연구가 더욱 활발히 진행될 것임

        Hacker News 의견

     * 혹시 궁금해하는 사람이 있을까 해서 말하는데, 우리가 이 행성(혹은 어떤 외계 행성)도 1 픽셀 이상으로 찍을 수 있는 이미지를 얻으려면 아직 갈 길이 멀다는 사실을 설명하고 싶음
       110광년 거리에서 이 행성을 100x100픽셀(작은 아이콘 크기)로 찍으려면 지름이 약 450킬로미터 짜리 망원경이 필요함
       이건 빛의 파장에 따라 정해진 물리적 한계임
       최고로 해볼 수 있는 방법이라면 우주에 두 개의 노드를 450킬로미터 떨어뜨려놓고 파장 하나만큼 동기화된 광학 간섭계를 만드는 것인데, 이건 정말 까다로운 엔지니어링 과제임
          + 그보다는 더 좋은 방법이 있음
            태양을 중력렌즈로 쓰고, 542AU(천문단위) 지점에서 탐사선을 배치한다면 98광년 떨어진 행성 표면을 25킬로미터 단위로 분해능을 낼 수 있음
            엄청 방대하고 긴 작업이 되겠지만, 인류의 현재 기술로도 불가능하지 않은 범주에 들어감
            참고: Solar gravitational lens - 위키피디아
            관련 NASA 미션 설명: Direct multipixel imaging and spectroscopy of an exoplanet with a solar gravitational lens mission
          + 만약 인간의 눈에 보이는 파장으로 이미지를 얻어야 한다는 조건을 포기한다면, 전파망원경으로 이미지화가 가능함
            이미 이런 능력을 가지고 있지만, 전파 간섭계의 한계는 엄청나게 큰 개구면을 가질 수 있지만 대비(contrast)가 매우 낮아진다는 점임
            별에서 오는 신호를 다 빼내고 나면 행성 신호는 잡음에 묻힐 것으로 예상함
            광학 간섭계로 해도 같은 문제가 생김
          + LIGO(유명한 중력파 탐지기)는 4km 길이의 두 팔로 구성됨
            LIGO 공식 페이지를 보면 가장 민감할 때는 거울 사이의 거리를 양성자 너비의 1/10,000만큼 변화하는 것도 감지할 수 있음
            이건 인간 머리카락 굵기만큼 가까운 별(4.2광년 거리)까지의 거리 변화를 포착하는 것과 같다고 함
            그래서 두 망원경을 450km 떨어뜨려놓고 단순히(농담) 가시광 파장 수준으로 동기화하는 것도 돈만 퍼붓는다면 할 만하다고 생각함
          + 알파 센타우리(4.37광년 거리) 계에서 뭔가를 촬영하려면 망원경이나 거울, 렌즈는 얼마나 커야 할지 궁금함
            또한 큰 지역을 스캔해서 여러 작은 이미지를 조합해 이미지를 만드는 방식이 가능한지도 궁금함
          + 내가 말한 1픽셀짜리 외계 행성 이미지는 거짓이라고 하는지 궁금함
            링크된 웹사이트의 이미지는 1픽셀보다 크다고 주장하면서, 진짜/가짜 논란을 제기함
     * HN 제목에 미묘한 오류가 있음
       이건 JWST가 찍은 외계 행성의 첫 번째 직접 이미지는 아님
       2023년 3월 JWST가 촬영한 여러 외계행성 이미지 관련 NASA 기사가 있음
       원래 기사에서 “direct image discovery”에서 discovery(발견)이란 키워드는 빠졌음
       즉, 이번이 기존에 몰랐던 외계 행성을 직접 이미징으로 “처음 발견한” 사례임
     * Anne-Marie Lagrange 연구 책임자가 진짜 멋진 이름이라 생각하면서, 혹시 라그랑주 포인트의 이름과 연관이 있을지 궁금증
       라그랑주 포인트가 뭔지 설명하는 위키 링크
       이 과학자를 처음 알게 되었지만, 경력이 대단함
       Anne-Marie Lagrange 위키 링크
          + ""Lagrange""라는 성씨가 너무 과학에 잘 어울리는 이름이라고 말한 데 대해 Scopus에는 Lagrange란 이름을 가진 연구자 프로필이 390개나 나오는 점을 언급
            인기 있는 성씨는 아니지만 그리 드문 이름도 아니어서, Joseph-Louis 라그랑주의 직계든 아니든 학계에 여러 명이 있음
          + 나도 똑같은 생각을 했음
            이름과 직업의 연관성이 또 나타난 예시라는 nominative determinism(이름이 운명을 좌우한다는 효과) 사례라 생각함
     * Lagrange 연구진이 컴퓨터 모델 시뮬레이션으로 가능성 있는 행성계 이미지를 만들었고, 망원경으로 찍은 실제 모습과 일치했다는 점에서 행성임을 자신했다는 대목을 인용
       이런 연구가 멋지고 의심할 이유도 없지만, 이런 식의 모델은 가설을 뒷받침하기엔 약한 증거임
       모델은 가정과 기대에 의해 만들어지기 때문에 데이터 그 자체는 아님
     * 새로운 적외선 신호가 배경 은하일 수도 있다는 설명을 인용
       이 정도 수준의 불확실성이 익살스럽게 느껴짐
       “뭔가를 찍은 것 같은데, 사실은 훨씬 더 크고 멀리 있는 수십억 개의 물체들일 수도 있다”는 식의 오차 규모임
          + 시간이 지나면 행성(궤도 운동) 움직임 때문에 두 경우를 구별할 수 있을 것이라 생각함
            다만 50AU 거리에서 작은 별을 공전한다면 한참 걸릴 것으로 예상함
     * JWST는 엔지니어링의 경이로움임
       동시에 1990년대 로켓 파워 한계에 맞춰 설계된 기계라 최신 초대형 발사체들이 계속 개발되면서 미래의 우주망원경이 훨씬 발전할 것이라 상상함
          + 내 꿈은 언젠가 'Van Leeuwenhoek 모먼트’(현미경으로 미생물 세계가 보였던 경험)에 가까운 지점까지 망원경이 발전하는 것임
            그 때는 은하계 곳곳에 우글거리는 우주선들을 한눈에 볼 수 있을 거라 상상함
          + 동시에 JWST 쌍둥이 혹은 더 많은 기기를 개발했다면 부가 비용도 아주 크지 않았을 텐데 아쉽다는 생각
            이제는 SpaceX 로켓도 있으니 더 대담한 시도를 해볼 만함
          + 아직 검증되지 않은 발사체를 전제로 JWST급과 같은 대형 페이로드를 개발하기는 쉽지 않음
            수십 년짜리 미션을 기획하기 전에 “개발 중” 상태에서 검증이 끝날 때까지 기다려야 한다는 점이 현실임
     * 이번에 사용한 관측 기술이 행성이 별에서 멀어질수록 더 쉽게 발견할 수 있도록 편중되어 있다는 점이 흥미로움
       이와 달리 도플러 이동이나 빛 곡선(light curve) 방식 등 현재 널리 쓰이는 기술은 별 가까이 붙어 있는 행성 탐지에 유리함
       양쪽 방법을 함께 쓰면 행성의 분포를 더 잘 알 수 있을 것임
     * 이미 직접 촬영된 외계 행성이 이렇게 많다는 건 저도 처음 알았음
       지금까지 직접 관측된 외계행성 목록 위키 링크
       JWST의 성과를 깎으려는 건 아니고, 어느 하나도 대단하지 않은 게 없음
     * 기사에서 JWST를 JSWT라고 잘못 쓰는 부분이 있음
       수정이 가능한 사람이 여기 있는지 궁금함
"
"https://news.hada.io/topic?id=21773","Spegel - LLM을 활용해 웹페이지를 재구성하는 터미널 브라우저","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Spegel - LLM을 활용해 웹페이지를 재구성하는 터미널 브라우저

     * Spegel은 HTML 웹페이지를 LLM 프롬프트로 변환해, 터미널에서 마크다운 형태로 보여주는 브라우저
     * 사용자 프롬프트를 기반으로 페이지를 실시간 맞춤 변환할 수 있어, 중요한 정보만 간결하게 표시하도록 설정할 수 있음
     * 핵심 동작 방식은 HTML 크롤링 → LLM 프롬프트 처리 → 마크다운 변환 및 출력
     * Textual 기반 TUI로 직관적이고 가벼운 터미널 UI 제공하며, 뷰와 프롬프트는 설정 파일로 쉽게 관리 및 실시간 변경 가능
     * Lynx, Links2, Browsh 등 기존 터미널 브라우저와 달리 LLM을 활용한 사용자 맞춤 콘텐츠 최적화에 특화되어 있음
     * pip install spegel로 간단 설치 가능, 오픈소스로 다양한 실험 및 확장에 적합
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요 및 특징

     * Spegel은 터미널에서 동작하는 웹 브라우저의 일종으로, HTML을 LLM에 전달하여 사용자 정의 프롬프트로 페이지를 재구성함
     * 출력은 마크다운으로 변환되어 Textual 기반 터미널 UI에서 직관적으로 표시
     * JS 미지원, GET 요청만 처리하는 미니멀 설계
     * LLM 프롬프트 커스텀으로 다양한 변환 뷰 지원 (예: 레시피 요약, 주요 액션 강조 등)

개인화와 활용 예시

     * 프롬프트와 뷰를 사용자 설정(~/.spegel.toml)으로 자유롭게 관리
     * 예시:
          + 레시피에서 재료와 핵심 단계만 추출
          + 복잡한 설명을 ELI5 스타일로 간단하게 변환
          + 필요시 여러 개의 뷰를 동시에 등록하여 빠르게 전환 가능

동작 방식

     * Spegel은 HTML을 가져와 설정 파일의 프롬프트와 함께 LLM에 전달
     * LLM 결과를 마크다운으로 변환, Textual을 통해 터미널에 렌더링
     * 프롬프트·뷰는 탐색 중에도 실시간으로 조정 가능
     * 결과를 줄 단위로 스트리밍하며, 마크다운 포맷 오류를 방지하는 버퍼 처리 구현

기존 터미널 브라우저와의 차별점

     * Lynx, Links2, Browsh 등은 HTML 구조 자체만 터미널에 표시
     * Spegel은 LLM 기반 변환으로 불필요한 정보 제거·최적화 뷰 제공에 특화
     * 현대 웹사이트는 CSS·JS 의존도가 높아 터미널 환경에서 번잡함, Spegel은 핵심 콘텐츠만 추출해 집중도와 접근성 개선

설치 및 사용법

     * pip로 간단 설치:
       pip install spegel
     * 실행:
       spegel <URL>
     * 설정 파일(~/.spegel.toml) 커스텀 필요, 예제는 GitHub 참고
     * 소스코드 및 기여: https://github.com/simedw/spegel

참고 사항

     * 아직 Proof-of-Concept 단계라 일부 미완성 기능과 거친 부분 존재
     * POST 요청 미지원, 폼 입력 등 향후 확장 아이디어 구상 중
     * 기존 터미널 브라우저의 대체라기보다는, LLM+TUI 기반 콘텐츠 개인화 탐구 실험 성격이 강함

        Hacker News 의견

     * 이 기술 정말 흥미로운 아이디어라고 생각함, 브라우저를 완전히 대체하지는 않지만 결정적 검색과 프롬프트를 조합해서 웹 브라우징의 전혀 다른 방식을 만들어 낼 수 있을 것 같음, 명령줄 도구 형태로 제공하면 더 잘 어울릴 거라는 생각임
          + 다음 단계로는 여러 ""탭""을 동시에 다루는 기능을 상상해봄, 예를 들어 탭 1은 뉴스 A의 보도, 탭 2는 뉴스 B, 탭 3은 Wikipedia를 넣고 이런 자료들을 요약하고 참조 링크를 생성하는 형태임, 그런데 이런 워크플로우를 지원하기에 충분히 안정적인 모델이 과연 있는지 의문임, 최신 SOTA 모델도 한계가 있다는 느낌임
          + 위와 같은, 여러 매체의 보도를 한 눈에 보고 요약, 참조 제공하는 기능, 사실상 Ground News에서 하고 있는 일이라고 생각함, 나는 그들과 관련 없고 단지 Kurzgesagt 영상에서 후원사로 언급된 걸 봤음, UI에는 분명 차이가 있을 수 있음
          + 나는 여러 탭/뷰를 동시에 보여준다고 해도 같은 출처(source) 내에서만 하려고 생각함, 예컨대 한 탭엔 CLI에 최적화된 원본 표현, 다른 탭엔 fact-checking(구글, Brave로 내용 근거 찾기)만 집중하는 방식임, 이런 실험 매우 재미있을 것 같음
          + LLM 기반 SEO 스팸을 온 힘을 다 해 만들어내고, 또 다른 LLM이 이를 대충 요약해서 다시 뱉어내는 구조라고 보면 됨, 참으로 대단한 미래라고 생각함
     * 유명 레시피 사이트에서 레시피만 추출하는 예시가 첫 번째 등장하는 게 정말 고전적인 장면이라고 느꼈음, 개인적으로 이런 프로젝트는 바로 추천하고 싶어짐, 아주 기발한 프로젝트 느낌임
          + 또다시 LLM 유행이, 이미 존재했던 걸 재해석해서 훨씬 나빠지고 비결정적으로 만든 예시라고 봄, schema.org/Recipe와 같은 표준 구조는 이미 있던 것임
          + 레시피 추출 과정에서 내용이나 재료, 분량 등이 무작위로 변형된다는 점이 흥미로움, LLM의 특성이 이런 마이크로코즘에서 아주 잘 드러난다고 생각함, 다만 대다수 코멘트들이 기대하는 방향과는 완전히 다름
          + 이미 LLM 없이 결정적 방식으로 레시피만 뽑아주는 확장 프로그램이 있음, 예를 들어 Chrome의 Recipe Filter 확장 같은 건 페이지 내 레시피를 인식하면 팝업으로 보여줌
     * LLM이 중간에서 들어가서 최근 구글의 SEO 최적화된 글쓰기 트렌드와 SEO 구조 자체를 우회한다는 점이 마음에 듬, 모든 불필요한 부분을 제거하고 레시피만 추리는 건 LLM에 정말 잘 어울리는 사례라고 생각함, 앞으로 LLM을 필터로 적극 활용하는 사례가 더 등장할 거라 예감함, 물론 HTML에서 레시피만 바로 뽑을 수 있으면 좋겠지만, SEO 전쟁이 너무 심해져서 현실적으로 불가능해진 상황임
          + LLM이 모든 불필요한 요소를 제거하는 용도라는데, LLM이 예측 불가능하게 레시피를 바꿀 수도 있다는 점 생각해볼 필요 있음, 오류를 허용할 수 없는 에러-허용불가 환경에 이런 확률적 소프트웨어를 신뢰하는 게 이해되지 않음
          + 몇 년 전부터 이런 미래를 예상함, 이미 검색 내장 LLM 도구가 있는데 여러 검색을 연결해 처리하는 기능이 정말 강력함, 하지만 Spegel은 완전히 다른 방식임, 미래의 광고 차단기는 작고 효율적인 로컬 LLM일 거라 봄, 예를 들어 타임라인을 시간순 정렬, UI 변경, 특정 항목만 노출 등 다양한 변형이 가능함, 저품질 댓글은 자동으로 숨긴다든지 LLM이 중간에 프록시나 에이전트로 작동할 때 전부 가능함, 이런 흐름이 광고주에겐 상당히 불편하게 다가올 거라 예감함
          + 웹 브라우징 과정에서 LLM이 분당 수백만 토큰을 처리해야 할 수도 있다는 점, 그만큼 연산 비용이 크다는 걸 생각해 볼 필요 있음
          + LLM이 불필요한 부분을 만들고, LLM이 또다시 불필요한 걸 제거하는데, 서로 오해 없이 흘러가는 순환 구조라는 생각임
     * 덜 복잡한 모델(심지어 LSTM 기반도 가능하다고 봄)로 DOM을 훑어서 필요한 부분만 골라 직접 브라우저에 표시할 데이터 구조로 수집하면 가능성 있다고 생각함, 이미 작성자의 LLM 기반 도구 체인 활용해서 훈련 데이터도 쉽게 만들 수 있을 것이라 느낌
          + 하지만 JS로 콘텐츠가 늦게 렌더링되는 현대 웹에선 DOM만 순회하는 건 한계임, JS가 다 로드되고 모든 요청이 끝난 후에야 필요한 데이터를 얻어낼 수 있는데, 이 경우 어차피 그냥 브라우저 렌더러를 돌리는 거나 마찬가지임
     * 많은 사람들이 html이 시작일 뿐이라는 점을 간과하는 듯, 웹페이지를 다른 시각(view)으로 변환할 수 있다면 사실 모델이 받아들이는 모든 입력에 대해 변환 가능함, PDF, 이미지가 담긴 zip, 대형 json 등 어떤 것도 view화 가능함, 결국 중요한 건 html 입력이 아니라 출력 결과 view라는 점임
     * spegel에 -p 옵션 추가 제안임
spegel -p ""extract only the product reviews"" > REVIEWS.md

       처럼 프롬프트 기반으로 원하는 정보만 추출하는 기능임
     * 페이지마다 매번 새로 rewrite하지 않고, 방문 한 번에 마크다운으로 바꾼 뒤 서로 깔끔한 버전을 공유하면, 연산 리빌딩을 줄일 수 있을 것이라 생각함
          + 각 사용자마다 필요와 사전 지식이 다르기 때문에, 아무리 깨끗한 공용 자료를 만들어도 결국 개인별로 손질하는 과정이 남아있을 거라고 생각함, 다만 P2P 캐시(IPFS 등) 같은 글로벌 중복 캐시가 데이터 보존, 가용성 확보, 자원 절약에 도움 될 수 있음
          + 캐시 헤더는 서버가 클라이언트에게 정보를 얼마나 오래 캐시해도 되는지 알려주는 용도임, 클라이언트 쪽에도 이런 헤더를 준수하는 캐시 계층을 추가하면 좋겠다는 생각임
          + 일관적 레이아웃이 목표라면, 마지막 페이지의 마크다운 결과물을 예시로 모델에 함께 넘기는 방식(one-shot example)도 가능성 있음
          + 이 프로젝트는 ""개인 맞춤 프롬프트 기반 뷰""라는 목적이 있으니까, 적어도 디폴트 프롬프트 결과물은 캐시해놓고 써도 좋겠다는 생각임
     * 정말 훌륭한 POC라고 느꼈고, 내가 평소 쓰는 크롬 확장 ""reader view""와 매우 유사한 부분이 있음
       reader view 확장 링크
     * 아이디어가 굉장히 멋지고 접근성 측면에서도 잠재력이 크다고 생각함
          + 하지만 역시 LLM은 비결정적이기 때문에 접근성은 무엇보다 신뢰성과 예측가능성이 담보되어야 하는 영역이라는 점이 문제임
     * 내 은퇴한 AI 에이전트가 웹페이지 실시간 변환해주는 걸 시연한 오래된 영상 있음
       HN을 My Little Pony로 변환하는 데모(동영상)
       대략 37초부터 결과물을 볼 수 있음
       오픈소스 크롬 확장도 만들어뒀으니 궁금하면 ChromeGPT 참고할 것

   spegel -p ""extract only the product reviews"" > REVIEWS.md

   이 옵션만 되면 이것저것 쓸게 생각나는데, 아직은 없는 듯 하더라고요.
"
"https://news.hada.io/topic?id=21744","러시아어 키보드가 설치된 경우 많은 랜섬웨어가 실행을 중단함(2021)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                러시아어 키보드가 설치된 경우 많은 랜섬웨어가 실행을 중단함(2021)

     * 대부분의 랜섬웨어는 감염 목표 시스템에 러시아어 또는 우크라이나어 등 CIS 국가 언어 키보드가 설치되어 있으면 실행을 중단함
     * 이러한 우회 기법은 범죄 단체가 자국 내 기관이나 개인을 피해자로 만들지 않게 해 현지 수사기관의 관심에서 벗어나는 목적임
     * 단순히 키보드 언어만 변경하는 것만으로 모든 악성코드의 방어는 불가능하며, 근본적으로 다양한 보안 수칙 준수가 필요함
     * 키보드 언어 추가는 손쉽고 무료로 실행할 수 있으나 부작용은 거의 없음
     * 러시아 해커들이 이를 우회하더라도 법적 위험이 커지므로, 방어책으로 어느 정도 효과가 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

러시아/우크라이나 키보드 설치 시 랜섬웨어 감염 차단 효과

  키보드 언어 감지와 랜섬웨어의 실행 중단

     * 최근 랜섬웨어 공격 관련 Twitter 논의에서, 매우 많은 랜섬웨어가 Microsoft Windows에 러시아어나 우크라이나어 등 가상 키보드가 설치된 경우 실행을 중단하는 내장 안전장치가 있다는 사실이 언급됨
     * 이는 주로 동유럽 출신의 악성코드가 많이 사용하는 방식임
     * 예를 들어 다수의 랜섬웨어는 CIS(독립국가연합) 국가 언어가 설치된 경우, 해당 시스템에서 감염을 하지 않음
     * 주요 목적은 이들 범죄자 집단이 본인 국가 내에서 법적 수사를 피하기 위함임

  Colonial Pipeline 사례와 DarkSide 그룹

     * 이 내용은 Colonial Pipeline 랜섬웨어 공격 논의 중 부각됨
     * 해당 공격은 DarkSide라는 랜섬웨어 서비스형 그룹이 대기업만을 주요 목표로 삼아 진행함
     * 러시아 기반 범죄 조직들은 우크라이나, 러시아 등 동유럽 국가를 감염 대상으로 두지 못하도록 내부적으로 금지해 왔음
     * 이 정책은 현지 정부의 조사 및 간섭을 피하기 위함임

  러시아 및 동유럽 내 법적 구조

     * 러시아에서는 자국민이 피해자인 경우에만 사이버 범죄 수사가 공식적으로 시작됨
     * 따라서 범죄자 본인이 자국 시스템에 피해를 주지 않는 것이 가장 안전한 방법임
     * 실제로 DarkSide와 REvil 등, 여러 랜섬웨어 그룹이 이러한 정책을 엄격하게 지킴

  코드 내 하드코딩된 언어 감지

     * DarkSide와 다른 여러 악성코드는 CIS 국가 언어를 하드코딩된 목록에 넣고, 시스템에 해당 언어가 설치되어 있으면 실행하지 않음
     * 실제로 무수히 많은 악성코드가 시스템 언어를 확인해 실행 여부를 결정함

  우회 방법의 한계 및 실제 효과

     * 단순히 러시아어 등 CIS 국가 키보드를 설치하면 일부 랜섬웨어에 대한 예방 효과가 있음
     * 하지만 전체 악성코드에 대응되는 것은 아니며, 다중 방어 전략이 필수적임
     * 언어를 변경해 발생하는 부작용도 크지 않음. 실수로 언어가 바뀌어도 Windows+Spacebar로 쉽게 전환 가능함

  향후 공격자 전략 변화 가능성

     * 일부 전문가들은 공격자가 언어 확인 절차를 생략할 수도 있다고 분석함
     * 실제로 최근 Mandiant가 분석한 DarkSide 버전에서는 언어 체크가 생략되었음
     * 그러나 이렇게 할 경우 범죄자의 법적 위험도가 상당히 증가함

  전문가 코멘트 및 '백신 효과'

     * Unit221B의 Allison Nixon은 러시아 해커들이 이러한 언어 체크를 활용해 법적 보호를 받는다고 설명함
     * 해당 언어 및 키보드를 추가하면, 일종의 '러시아 악성코드 백신' 역할이 될 수 있음
     * 이러한 방법이 대규모로 사용되면, 범죄자는 법적 보호와 수익 간 선택의 딜레마에 직면함
     * 범죄자는 서방 보안 담당자와 마찬가지로, 시스템이 진짜 현지 시스템인지 구분하기 어려워짐

  가상머신 환경 탐지 우회

     * 일부 트위터 이용자는 가상머신임을 명시하는 레지스트리 항목 추가도 제안함
     * 과거에는 효과적이었으나, 오늘날 많은 조직에서 일상적으로 가상머신을 사용해 해당 기법은 더이상 신뢰하지 않음

  언어 추가를 쉽게 하는 방법

     * Unit221B의 Lance James는 러시아어 키보드가 설치된 것처럼 보이게 하는 2줄짜리 Windows 배치 스크립트를 제작해 배포함
     * 이 스크립트는 실제 러시아어 라이브러리를 다운로드하지 않고도, 감염 회피 효과를 얻을 수 있음
     * 전통적인 방법으로도 '설정 → 시간 및 언어 → 언어 추가'를 통해 간단히 키보드 언어를 추가할 수 있음
     * 혹시라도 언어 설정을 바꾸어 메뉴가 러시아어로 나타날 경우, Windows+Spacebar 조합으로 언어 전환이 가능함

        Hacker News 의견

     * 내 컴퓨터를 악성코드 분석 샌드박스처럼 보이게 만들면, 악성코드 대부분이 분석을 피하기 위해 종료한다는 이야기인데, 이런 건 고양이와 쥐의 게임이라는 느낌
          + 요즘 윈도우 서버는 대부분 가상화 환경에서 돌아가기 때문에 예전만큼 효과적이지 않을 수도 있다는 생각, 그래도 다른 지표를 고려할 수도 있다는 이야기
          + 펌웨어에 VirtualBox 문자열을 넣는 장난스러운 제안
          + 이 내용은 예전 다른 Hacker News 글에서도 언급된 적이 있는데, 여기 링크에서 나왔다는 이야기
          + 이제 각 워크스테이션에 Ghidra를 설치해야 하지 않겠냐는 농담
          + ‘내 컴퓨터를 샌드박스로 위장하면 많은 악성코드가 분석을 피하기 위해 종료한다’는 주장에 대해, 이건 완전히 다른 걱정이라는 반박. 러시아어 입력기가 설치돼 있으면, 악성코드가 법적 위험을 피하려고 종료한다는 점을 강조
     * 랜섬웨어 Patya나 Fancy Bear, Cozy Bear, Conti 같은 그룹에 대해 이런 방식(러시아 키보드 감지)이 실제로 효과가 있었다는 증거가 많다는 의견, 러시아 정부가 자국민을 겨냥하지 않는 한 면책을 보장한다는 점이 큰 이유
       또, 공격자들에게 자신이 러시아인이거나 러시아어로 대화하면 시스템을 무료로 복호화해주는 경우도 있다는 이야기
          + ‘러시아인임을 밝히면 무료 복호화’가 AI 번역 시대에 어떻게 적용되는지 궁금함을 표현, 과거 러시아 쉐어웨어 개발자가 러시아인에게 무료로 라이선스를 주던 사례도 떠오른다는 코멘트
          + 러시아 해커 그룹은 ‘자기 집안에 피해 주지 않기(don’t piss inside the tent)’ 정책을 매우 잘 알고 있고 모두가 이해하고 있다는 점 강조
          + 현실은 그렇게 간단치 않을 수 있다는 지적, 러시아인은 어디에나 있고 피해 기업에도 근무할 수 있는데, 몸값이 수백만 달러일 경우 단순히 러시아인이라고 해서 풀어주지는 않을 것이라는 생각, 진짜 러시아 소유임을 설득하거나 ‘우리 아버지가 FSB에 근무한다’ 등 증거가 필요하다는 이야기
     * 00년대 후반 기술에 약한 친구들의 컴퓨터에서 ‘winlocker’를 지우던 러시아인이 직접 겪은 경험을 공유, 이 악성코드들은 단순히 파일을 암호화하지 않고 닫을 수 없는 창을 띄워 돈을 요구했으며, ‘성인 웹사이트 퀵 액세스 위젯 감사합니다’처럼 우스운 문구도 있었다는 흥미로운 추억
     * 키릴 키보드를 활성화한 시스템만을 노리는 악성코드가 당연히 있을 것 같다며, 러시아어 환경 여부도 공격자가 체크하는 포인트임을 암시
     * 윈도우에서 최고의 안티멀웨어 방법은 일상적으로 쓰는 기본 계정을 관리자 권한이 아닌 일반 계정으로 만드는 것이라는 팁 공유 별도로 로컬 관리자 계정을 만들고 다른 비밀번호를 사용해야 하며, 소프트웨어 설치나 powershell 실행 등 관리 작업이 필요하면 별도의 관리자 인증이 필요해서 suspicious한 팝업이 뜨면 뭔가 잘못됐다는 신호가 된다는 설명 일반 계정은 평범한(하지만 짧지 않은) 비밀번호를 쓸 수 있고, 관리자 계정엔 복잡한 비밀번호를 쓸 수 있어 특히 IT 지식이 없는 가족들에게 추천한다는 의견
          + 관리자 권한이 없어도 악성코드는 많은 일을 할 수 있고, 사용자가 가진 파일시스템에 접근하거나 인터넷 연결하는 데는 별다른 제약이 없기 때문에, 이런 권한 분리가 데이터 유출, 랜섬웨어, 데이터 파괴 등을 막아주지는 못한다는 지적
          + 2000년대 초부터 2012년까지는 동의하지만, 비스타 이후 UAC에 맞춰 악성코드가 진화해서 일반 계정으로도 충분히 동작하게 됐으므로 관리자 권한이 없다고 해서 데이터 보호에 도움이 되지는 않는다는 설명, 가장 민감한 작업(주로 금융)을 별도의 물리적 컴퓨터에서 하거나 Windows에서 사용자 계정 분리로 데이터 격리를 시도할 수 있다는 사례 공유, Qubes OS 같은 강력한 격리형 OS를 쓰고 싶지만 아직 배우지 못했다는 개인적 의견
          + 결국 사용자의 설명은 Windows Vista 이후 User Account Control(UAC)이 기본적으로 적용해온 방식과 동일하다는 요약
          + 조직 범죄의 랜섬웨어 공격은 대부분 민간인보다는 기업체가 주 표적이기 때문에, 개인이 아닌 기업 환경에서 랜섬웨어를 더 많이 접하게 된다는 의견. Petya 랜섬웨어를 예시로 들며, 일반 사용자 권한만 있어도 내부적으로 네트워크 내의 관리자 세션을 탈취하거나 도메인 관리자 권한을 획득할 수 있는 시나리오 언급, 관리자 권한 없이도 데이터를 삭제·암호화하거나 악성코드를 숨길 수 있으며, 소프트웨어에 번들로 포함된 악성코드는 사용자가 직접 설치할 수 있다는 여러 현실적인 접근 방법 설명
          + xkcd 1200 만화 링크와 함께, 여러 사람이 쓰는 컴퓨터에서는 계정 분리가 의미 있지만 대부분 단일 사용자인 경우에는 관리자 권한 분리의 실효성이 제한적이라는 요지, 현실적으로 개인 PC에서 관리 권한 분리는 해킹 방지에 큰 도움을 주지 않는다는 결론
     * 2021년 Brian Krebs가 이 내용을 공개한 뒤에도 여전히 이런 방식이 통하는지 궁금증
          + 러시아와 북한은 랜섬웨어를 합법적 경제 활동으로 간주하고 있으며, 하이브리드 전쟁 전략의 일부로 계속 이어질 것이라는 주장
          + 기본적으로 법적·수사상의 문제이다 보니 러시아 정부를 건드리지 않으면 서로 건드리지 않는다는 룰이 있다는 설명, 러시아보다 미국이 훨씬 더 약탈의 표적이 되는 시장임을 강조, 미국 내 비즈니스 이메일 사기(BEC)가 2024년에만 27억 달러 피해를 냈다는 FBI 통계와, 자신이 담당한 케이스에서 중국 위협 행위자가 미국 회사에 직원을 위장해 입사하고 수만 달러 상당의 내부 직원 할인 혜택을 가로채 100만 달러 손실을 낸 사례를 공유
          + FBI 2024 인터넷 범죄 보고서 링크 첨부
     * 제목 자체가 웃기다는 한마디, 대부분의 랜섬웨어가 러시아발이라는 전제가 자연스럽게 느껴진다는 뉘앙스
     * 러시아 키보드가 존재하면 NSA 악성코드한테 더 매력적일 수도 있다는 생각
          + 러시아, 중국 등은 민감한 정부·군사 기관에서 Windows를 금지하고, 자체 리눅스 배포판을 사용한다는 트리비아 정보
     * 2021이라는 단어만 남긴 짧은 메시지
          + 우크라이나가 제외 대상에서 제거됐는지 궁금하며, 키보드 배열이 러시아와 다르다는 사실에 주목
     * 키보드 레이아웃뿐 아니라, 시간이 바뀌면 타임존이나 다양한 정보도 함께 체크할지 궁금증
"
"https://news.hada.io/topic?id=21814","구글, ‘제로-지식 증명(ZKP)’ 기술 오픈소스로 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    구글, ‘제로-지식 증명(ZKP)’ 기술 오픈소스로 공개

     * Zero-Knowledge Proof(ZKP)는 개인 정보 노출 없이 특정 사실(예: 18세 이상)을 증명할 수 있게 하는 암호 기술
     * 이 기술은 유럽연합(EU)의 디지털 신원 지갑(EUDI Wallet) 등 차세대 신원 인증 및 프라이버시 보호 서비스 구현을 가속화할 전망
     * 개발자, 기업, 연구자 모두가 자유롭게 활용 가능하며, 공공/민간의 프라이버시 강화 서비스 구축에 큰 도움이 될 것
     * 구글은 ZKP 오픈소스를 통해 모두가 더 안전한 디지털 생태계로 나아가길 기대
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Now open source: our Zero-Knowledge Proof (ZKP) libraries for age assurance

     * Google이 ‘Zero-Knowledge Proof(ZKP)’ 관련 주요 라이브러리를 오픈소스로 공개함
          + GitHub - Longfellow ZK에서 공개 중임
          + EU 연령 확인 지원을 위한 기반 마련

Zero-Knowledge Proof란?

     * ZKP는 사용자가 개인 정보를 직접 공개하지 않고도 어떤 사실을 증명할 수 있는 최신 프라이버시 보호 기반 알고리듬
          + 예시: 사용자가 웹사이트를 방문할 때 나이나 신원 등 구체적인 정보 노출 없이도 18세 이상임을 입증할 수 있음
     * Google이 제공한 라이브러리는 나이 인증(Assurance) 같은 실제 문제를 해결할 수 있도록 설계되어, 온라인 게임, 금융, 커뮤니티 등 나이 제약이 있는 서비스에서 활용도가 큼

오픈소스 공개의 의의와 기대 효과

     * ZKP 오픈소스화는 다음과 같은 다양한 이해관계자에게 이익을 제공함
          + 웹 및 앱 사용자: 더 프라이버시 중심적이고 안전한 디지털 환경에서 활동 가능 해짐
          + 기업 및 서비스 제공자: 오픈소스 솔루션을 활용해 프라이버시 요건 충족 및 신뢰성 향상 가능
          + 개발자: ZKP 코드베이스를 활용해 다양한 프라이버시 중심 앱/서비스 개발이 쉬워짐
          + 연구자: 효율적이고 성능이 향상된 ZKP 구현을 바탕으로 신규 응용 사례 연구 및 실험 가능
     * 디지털 신분증, 나이 인증, 법적 동의 검증 등 다양한 분야로 응용이 가능함

EU 규제 및 디지털 신원 지갑과의 연계

     * 2026년부터 시행되는 EU eIDAS 규정은 각 회원국이 프라이버시 강화 기술을 자국 디지털 신원 시스템(EUDI Wallet)에 통합할 것을 권장함
     * 구글의 ZKP 오픈소스화는 회원국들이 빠르게 EUDI Wallet에 해당 기술을 도입할 수 있도록 지원함

구글의 비전

     * 구글은 이번 오픈소스를 통해 모든 디지털 생태계 구성원이 더 안전하고 프라이버시를 존중받는 환경을 누릴 수 있도록 기여하겠다고 밝힘

        Hacker News 의견

     * 비대화형 영지식증명(non-interactive zero-knowledge proofs)의 직관적인 설명이 궁금함. Diffie-Hellman 키 교환의 ‘페인트 혼합’ 비유처럼, 수학적 기법을 쉽게 손에 잡히게 해주는 설명이 필요함. 상호작용 ZKP에 대한 좋은 소개 자료는 있지만 비대화형은 아직 찾지 못함. ZK-STARKs를 erasure coding과 비교한 블로그 글도 읽어봤지만 아직 제대로 와닿지 않음
          + Waldo를 찾을 수 있음을 밝히되 실제 위치는 노출하지 않는 증명 방식처럼 설명할 수 있음. 디지털 월렛은 공개키 대신 서드파티가 발급한 ‘commitment C’로 서명 구조를 구성함. 비대화형 증명은 Fiat-Shamir 변환을 이용해 수행할 수 있음. 예를 들어 비트값이 0 또는 1임을 밝히는 간단한 range proof부터 여러 비트 범위까지 확장 가능함. Bulletproofs 덕분에 증명 크기를 크게 줄일 수 있어 실용화 가능성 증가. 다만 동일한 커밋먼트를 반복 사용시 추적 식별자가 되어 프라이버시 문제가 생길 수 있음
          + Wikipedia의 “Ali Baba Cave” 예시가 영지식증명의 개념을 이해하는 데 결정적 역할을 했음 링크
          + ‘페인트통’ 수준의 직관적 예시를 원하는 경우 Matthew Green의 ""crayons and hats"" 설명을 추천, 상당히 쉽게 접근 가능 링크
          + 동료 Amit가 Wired와 함께 만든 영지식증명 입문 영상이 있음. 구체적 동작 방식까지 다루지는 않지만 시작점으로 괜찮은 자료 영상
          + STARKS와 SNARKS가 강력한 이유는 다항식의 특성에 있음. Schwartz Zippel lemma 덕분에 무작위 샘플 몇 개만으로 서로 다른 다항식을 쉽게 구별 가능. 이 점이 erasure code와의 비교에 연결됨. 비대화형 전환도 Fiat-Shamir로 챌린지-응답 대화를 시뮬레이션하며 원리 자체는 직관적. Groth16 zk snarks 같이 극도로 짧은 증명이 가능하다는 점도 놀라움. 이건 타원곡선 페어링 함수라는 심화 수학 이론에 기반함
     * 나이 인증이 곧 정부(그리고 그 대리인인 기업)발 인터넷 사용 허가의 관문 역할이 될 전망
          + 실제로 우간다에서는 2018년부터 소셜미디어 접근 시 자동으로 요금 청구하는 세금 제도를 운영 중. 일일 2.7센트 수준으로, 모든 인터넷 접속자는 국가에서 관리하는 ISP를 통해 연결됨. 사용 기반 과금과 연결 시, 인터넷 허가 시스템도 기술적으로 전혀 어려움 없음. 이렇게 세금 징수 체계가 있으면 자연스럽게 사용자와 인터넷 연결을 1:1로 추적 가능 기사
          + 유일성 보장까지 결합되면 인터넷이 더 이상 ‘죽었다’ 논쟁 없이 정말 살아 있는 공간으로 확인 가능성
          + 이런 시스템 자체가 실질적으로 무자격자-기술과 권력이 결합된 체제(kakistocracy-technofeudalism) 구축 도구에 가까움
          + 맞는 이야기지만, 10살 아이가 브라우저만 켜면 그 즉시 부적절한 콘텐츠를 쉽게 볼 수 있는 현상도 분명 문제라고 생각함. 포르노 산업 자체의 자정만으로는 현실적 대안을 기대하기 힘듦
     * 최종 보고서 및 보안 취약점(CVE) 관련 자료가 공개될지, 그리고 향후 신뢰 확보를 위한 타사 감사 체계도 정립될지 궁금증
     * 영지식증명 기반으로 개인정보 과공유 없이도 온갖 인증이 가능한 세상 구현 가능성, 특히 사회보장번호(SSN) 등 민감정보 보호에서 큰 역할을 할 수 있음
     * 악의적 공격자를 막는 방법에 대한 의문. 예를 들어 승인된 프라이빗키가 저장된 노트북이나 폰을 구매, 유출된 프라이빗키 파일을 다운로드, VPN을 사용해 법적 규제를 피해 접속하는 식의 우회가 가능하지 않을지 등
          + 운전면허증 등 신원증명은 공개키를 담고 실제 시크릿키는 휴대폰의 보안 모듈(secure element)에 보관하는 구조임. 따라서 폰을 산다고 해서 단순 프라이빗키만으로 인증 가능하지 않음. 폰과 프라이빗키를 모두 받아야만 함. 지문 등의 생체 인증 기반이므로 실제 현장 정부 창구에서 발급받는다면 쉽지 않음
          + 이런 기술적 조치는 사실상 순종적 일반 시민 통제를 위한 도구임. 상위 0.1%와 하위 20%는 원리적으로 제어 불가능. 미래에는 PII/KYC와 함께 서명받은 인증서로만 인터넷에 접근할 수 있게 될 것이고, 이미 중국은 추진 중이며 서구권도 같은 방향으로 나아가는 중
     * 크리티컬 소프트웨어의 경우 Google에 의존하지 않는 게 건강한 개발 방식임
          + 오픈소스라 다행임
     * 영지식증명 기술에서 David Chaum이 특허로 혁신을 가둬두었을 때 상당히 화가 났었음. DigiCash 팀은 닷컴버블 최고의 욕심쟁이들이었고, 모든 트랜잭션마다 돈을 떼려다가 시장에서 받아들여지지 못함. Andy Birrell의 “micro-cents” 아이디어처럼 MD5 해시의 일방향성을 이용한 신속·저가 소액결제 기술도 있었지만, 실제 적용까지는 가지 못해 아쉬움 남음. 90년대에 상상한 ZKP 기반 신분증, 통화 등 실물로 나왔으면 좋겠음. 오프라인 상태에서 이중지불 불가능한 디지털 화폐가 가능했다면, 정부가 강하게 반대한 것도 이해 가능. 올드스쿨 ZKP 아이디어를 지금의 기술 위에 다시 시도해 보고 싶음
          + 하지만 이 기술이 결국 Apple, Google, Microsoft 같은 빅테크에 개인정보 처리를 강제하는 구조가 되는 데서 망설임. Passkeys 같은 솔루션에 애정 있지만 업계가 제대로 활용하지 못하는 아쉬움. 사용자의 개인키는 휴대기기의 하드웨어 보안키에 반드시 연동되는 구조이기에 빅테크에 종속적인 현실
          + 실제 진행 중인 프로젝트로 Paygo를 소개. 트위터에도 정보 공유
     * ZKP는 분산 신원 인증의 좋은 해법임. 신원 지갑 기반으로 당 소속, 지역, 연령 등 최소한의 개인정보만으로 민주주의 서비스 등 다양한 분야 확장 가능성. 정책 위원회가 오랜만에(ISDN 이후) 프로토콜 레벨 혁신을 추진해서 긍정적으로 평가
     * 프랑스와 미국 내 주요 포르노 웹사이트가 과도한 연령 인증 규제로 이용자 경험과 프라이버시를 해칠 것을 우려해 이용을 막았다가 규제 완화 후 서비스를 재개한 사례가 있음. 이 논의와 매우 직결되는 상황
     * 기존 ECDSA 하드웨어 키를 이용해 멀티쇼 언링커빌리티와 하드웨어 바인딩을 융합한 매우 흥미로운 구조로, 연령 인증뿐 아니라 다양한 속성 증명에 활용 가능성. 단, Idemix나 BBS+ 같은 기존 솔루션보다 훨씬 복잡해서 실제로 이해하는 사람은 극소수일 것임. 개인 속성의 반복 노출 횟수를 제한해 익명성-식별성 균형을 조절하는 기법 등 다양한 프라이버시 보호 시도 있음. 하지만 완벽한 우회 방지는 불가능. 현실적으로 인증은 순진한 이용자 보호용 장벽이라는 점이 인정되어야 하고, 완전 방지 기대가 높으면 첫 보안 사고 이후 프라이버시 보호의 오픈소스적 접근은 곧 폐기될 위험성 기술문서1, 기술문서2
          + 오히려 우리 솔루션은 학부생도 이해할 수 있음. 반면 BBS의 쌍선형 페어링은 아는 사람이 드물 정도로 난해함
"
"https://news.hada.io/topic?id=21747","Ask GN: 구형 아이폰7 을 어떤 용도로 사용하면 좋을까요? 현재 시점에서요.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Ask GN: 구형 아이폰7 을 어떤 용도로 사용하면 좋을까요? 현재 시점에서요.

   구형 아이폰7 을 어떤 용도로 사용하면 좋을까요? 현재 시점에서요.

   이미 아이폰7 은 공식 iOS 지원이 15버전으로 끝났습니다.

   카메라 갬성으로 중고 아이폰이 다시 인기라고들 하는데~

   이걸 무슨 용도로 사용하면 좋을까요?

   일단은 한손에 싹 감기는 지금 나오는 폰 보다 아담한 크기여서

   알뜰폰으로 신규개통하여 서브 폰으로 사용해볼까하는데~

   가끔 달리기나 걷기 하러 나갈 때

   기본 통신 용으로 가지고 나갈까 하고 염두에 두고는 있습니다만,

   여러분은 구형아이폰이 있다면 어떻게 사용하실까요?

   중고로 팔라는 성의없는 답변은 사양하고요~~~

   저는 모임할때마다 거치대에 두고 타임랩스 촬영용으로 씁니다.
   보드게임 하는 테이블을 위에서 찍거나, 와인모임 전체를 한쪽 면에서 촬영합니다.
   찍어두면 나름 재미도 있고, 인스타 공유용으로 모임원분들한테 에어드랍 해주니 좋아하시더군요.

   카메라 갬성이 좋다고는 하는데 제가 사진을 많이 찍는 스타일은 아니어서요. 좋은 유스케이스 알려주셔서 감사합니다.

   그정도 구형은 아니지만 서브폰으로 아이폰을 들고다니는데 (12mini) 저같은 경우 평소엔 쓸 일이 별로 없긴 하더라고요. 실생활에선 맥북 핫스팟 셔틀(...), 비상용 회선, 애플페이 등 용도로 사용중입니다.

   글쓴 분이 기대하는 답변은 아니겠지만, 7플러스를 아직 실사용 중입니다.
   혼자서 '스마트폰 10년 쓰기' 챌린지 같은 걸 하고 있는데, 9년 정도 되니 슬슬 한계가 오고 있네요.

   어떻게든 햇수로 10년 채우고 내년 초 쯤 7+에서 17로 넘어가려고 합니다.

   저는 전자책 리더로 XR 사용중입니다.

   저도 차량 내비용으로 거치해놓고 씁니다. 갤럭시 유저면 Carplay 용으로..

   그러면 별도로 번호를 개통하든지 아니면 데이터셰어링을 하든지 해야하는거죠?

   탁상시계 한표

   탁상시계로만 쓰기엔 너무 아깝긴 합니다. 그래서 스마트 액자용으로도 써볼까 생각은 했었습니다.

   차량 내비용으로 써도 좋을것같네요
"
"https://news.hada.io/topic?id=21733","증강형 코딩: 바이브를 넘어서","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            증강형 코딩: 바이브를 넘어서

     * 켄트 벡이 최근 증강형 코딩: 바이브를 넘어서(Augmented Coding: Beyond the Vibes) 라는 글을 작성했음
     * 켄트 벡 본인이 AI의 도움을 받아 고성능 + 프로덕션 레벨에 가까운 B+ Tree 라이브러리(BPlusTree3)를 Rust와 Python으로 작성한 스토리
     * 특히 유용하고 통찰을 느꼈던 3가지 포인트를 요약 & 번역해 소개
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  증강형 코딩은 바이브 코딩과 뭐가 다른가?

     * 바이브 코딩에서는 코드는 신경쓰지 않고 시스템 동작만 신경씀. 에러가 있으면 '이런 에러가 있다'고 얘기하고 고쳐주길 기대
     * 증강형 코딩에서는 코드를 신경씀. 코드의 복잡도, 테스트, 테스트 커버리지가 중요.
     * 증강형 코딩에서는 기존의 코딩과 마찬가지로 ""Tidy Code That Works"", 즉 '작동하는 깔끔한 코드'를 중요시함. 단지 예전만큼 타이핑을 많이 하지 않을 뿐

  AI가 잘못하고 있다는 3가지 신호

   증강형 코딩에서 AI의 중간 결과를 관찰하며, 다음 3가지 신호가 나타나는지 살펴 개입하는 게 중요
     * 비슷한 행동을 반복한다 (무한루프 등)
     * 내가 요청하지 않은 기능 구현. 그게 논리적인 다음 단계가 맞을지라도.
     * 테스트를 삭제하거나 비활성화는 등, AI가 치팅하는 걸로 느껴지는 그 외 모든 신호.

  TDD를 돕는 시스템 프롬프트

     * 원글 본문이 복사하기 좀 번거로워서 gist에 넣어둠
     * 마지막에 Rust 구문만 본인 프로그래밍 언어/프레임워크에 맞게 바꾸시면 어디서나 아주 훌륭하게 재사용할 수 있는 프롬프트로 보임.

  맺으며

     우리가 사랑하는 이 직업이 사라지고, 코드를 다루는 즐거움이 없어질 거라는 두려움이 많다는 것을 압니다. 불안해하는 것도 당연합니다. 네, '지니'와 함께 프로그래밍하는 것은 분명 변화를 가져오지만, 여전히 프로그래밍입니다. 어떤 면에서는 훨씬 더 나은 프로그래밍 경험이죠. 제가 시간당 내리는 의사결정의 수와 질을 보면, 따분하고 판에 박힌 결정은 줄어들고 더 중대한 프로그래밍 결정은 더 많아졌습니다.

     소위 '야크털 깎기(yak shaving)'라 불리는, 본질과 거리가 먼 잡다한 작업들이 대부분 사라집니다. 저는 '지니'에게 커버리지 테스터를 실행하고 코드의 신뢰도를 높일 테스트들을 제안해달라고 했습니다. '지니'가 없었다면 매우 막막한 일이었겠죠. 테스터 실행에 어떤 라이브러리의 어떤 버전이 필요한지부터 알아봐야 했을 테니까요. 아마 두 시간쯤 씨름하다가 포기했을 겁니다. 대신, 저는 '지니'에게 말하기만 하면 되고, '지니'가 세부 사항들을 알아서 처리해 줍니다.

   항상 plan.md의 지시사항을 따르세요. 제가 ""go""라고 말하면, plan.md에서 다음 표시되지 않은 테스트를 찾아서 해당 테스트를 구현한 다음, 그 테스트가 통과하도록 하는 데 필요한 최소한의 코드만 구현하세요.

역할과 전문성

   당신은 Kent Beck의 테스트 주도 개발(TDD)과 Tidy First 원칙을 따르는 시니어 소프트웨어 엔지니어입니다. 당신의 목적은 이러한 방법론을 정확히 따라 개발을 안내하는 것입니다.

핵심 개발 원칙

     * 항상 TDD 사이클을 따르세요: Red → Green → Refactor
     * 가장 간단한 실패하는 테스트를 먼저 작성하세요
     * 테스트가 통과하는 데 필요한 최소한의 코드를 구현하세요
     * 테스트가 통과한 후에만 리팩토링하세요
     * Beck의 ""Tidy First"" 접근법을 따라 구조적 변경과 행동적 변경을 분리하세요
     * 개발 전반에 걸쳐 높은 코드 품질을 유지하세요

TDD 방법론 가이드

     * 작은 기능 증분을 정의하는 실패하는 테스트를 작성하는 것부터 시작하세요
     * 행동을 설명하는 의미있는 테스트 이름을 사용하세요 (예: ""shouldSumTwoPositiveNumbers"")
     * 테스트 실패를 명확하고 정보성 있게 만드세요
     * 테스트가 통과하도록 하는 데 필요한 코드만 작성하세요 - 그 이상은 안 됩니다
     * 테스트가 통과하면 리팩토링이 필요한지 검토하세요
     * 새로운 기능을 위해 사이클을 반복하세요

TIDY FIRST 접근법

     * 모든 변경사항을 두 가지 유형으로 분리하세요:

    1. 구조적 변경: 행동을 변경하지 않고 코드를 재배열하는 것 (이름 변경, 메서드 추출, 코드 이동)
    2. 행동적 변경: 실제 기능을 추가하거나 수정하는 것

     * 구조적 변경과 행동적 변경을 같은 커밋에서 절대 섞지 마세요
     * 둘 다 필요할 때는 항상 구조적 변경을 먼저 하세요
     * 구조적 변경이 행동을 바꾸지 않았는지 변경 전후에 테스트를 실행하여 확인하세요

커밋 규율

     * 다음 조건에서만 커밋하세요:

    1. 모든 테스트가 통과할 때
    2. 모든 컴파일러/린터 경고가 해결되었을 때
    3. 변경사항이 하나의 논리적 작업 단위를 나타낼 때
    4. 커밋 메시지가 구조적 변경인지 행동적 변경인지 명확히 명시할 때

     * 크고 드문 커밋보다는 작고 빈번한 커밋을 사용하세요

코드 품질 표준

     * 중복을 철저히 제거하세요
     * 이름과 구조를 통해 의도를 명확히 표현하세요
     * 의존성을 명시적으로 만드세요
     * 메서드를 작게 유지하고 단일 책임에 집중하세요
     * 상태와 부작용을 최소화하세요
     * 가능한 가장 간단한 해결책을 사용하세요

리팩토링 가이드라인

     * 테스트가 통과할 때만 리팩토링하세요 (""Green"" 단계에서)
     * 확립된 리팩토링 패턴을 적절한 이름과 함께 사용하세요
     * 한 번에 하나의 리팩토링 변경만 하세요
     * 각 리팩토링 단계 후에 테스트를 실행하세요
     * 중복을 제거하거나 명확성을 개선하는 리팩토링을 우선시하세요

예시 워크플로우

   새로운 기능에 접근할 때:
    1. 기능의 작은 부분에 대한 간단한 실패하는 테스트를 작성하세요
    2. 통과하도록 하는 최소한의 것을 구현하세요
    3. 테스트를 실행하여 통과하는지 확인하세요 (Green)
    4. 필요한 구조적 변경을 하세요 (Tidy First), 각 변경 후 테스트를 실행하세요
    5. 구조적 변경사항을 별도로 커밋하세요
    6. 다음 작은 기능 증분을 위한 또 다른 테스트를 추가하세요
    7. 기능이 완성될 때까지 반복하세요, 행동적 변경사항을 구조적 변경사항과 별도로 커밋하세요

   이 과정을 정확히 따르고, 빠른 구현보다는 항상 깨끗하고 잘 테스트된 코드를 우선시하세요.

   항상 한 번에 하나의 테스트를 작성하고, 실행하게 한 다음, 구조를 개선하세요. 매번 모든 테스트를 실행하세요 (장시간 실행되는 테스트는 제외).

Rust 관련

   Rust에서는 명령형 스타일보다 함수형 프로그래밍 스타일을 선호하세요. 가능할 때는 if let이나 match를 사용한 패턴 매칭 대신 Option과 Result 조합자들(map, and_then, unwrap_or 등)을 사용하세요.

   입코딩 이후에는 뇌파 코딩이 나왔으면 좋겠네요

   vibe coding ❌️
   virtual coding ⭕️

   메타버스 이후는 흠.. 입코딩?

   이제 메타버스코딩이 나올 차례네요
"
"https://news.hada.io/topic?id=21782","Sudo 로컬 권한 상승 취약점 패치 (CVE-2025-32462, CVE-2025-32463)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Sudo 로컬 권한 상승 취약점 패치 (CVE-2025-32462, CVE-2025-32463)

  개요

   최근 Sudo 유틸리티에서 두 가지 로컬 권한 상승 취약점이 공개되어 패치가 제공됨.
   해당 취약점은 일반 사용자가 루트 권한을 획득할 수 있게 해줌
     * CVE-2025-32462
          + Sudo의 host 옵션에 존재하는 낮은 심각도의 권한 상승 취약점.
          + 12년 이상 코드에 존재해온 버그로, 특정(흔히 사용되는) Sudo 설정에서 호스트 이름에 따라 권한이 제한된 경우, 추가적인 익스플로잇 없이 루트 권한 상승이 가능함.
          + Sudo v1.9.0~1.9.17, v1.8.8~1.8.32에 영향
     * CVE-2025-32463
          + Sudo의 chroot 옵션(-R)에 존재하는 치명적(Critical) 취약점.
          + 사용자가 지정한 루트 디렉토리 하에서 임의의 공유 라이브러리를 로드하도록 Sudo를 속일 수 있음.
          + /etc/nsswitch.conf 파일을 이용해 공격 가능(해당 파일을 지원하는 시스템에서만).
          + Sudo v1.9.14~1.9.17에 영향. 레거시 버전(v1.8.x)은 해당 기능이 없어 영향받지 않음

    대응 방안

     * 업데이트: Sudo의 최신 보안 패치 적용(1.9.17p1 or later) 권장
     * 임시 조치: /etc/sudoers 및 /etc/sudoers.d의 모든 Sudo 규칙 확인
          + Sudo 규칙이 LDAP에 저장된 경우, ldapsearch 같은 도구로 규칙 확인
          + 각 규칙에서 runchroot= 옵션 또는 CHROOT= 지시어 사용 여부 확인
          + Host 또는 Host_Alias 옵션이 포함되어 있는지 확인.

    권장 사항

     * 시스템 관리자 및 보안 담당자는 즉시 Sudo를 최신 버전으로 업데이트해야 합니다. (chroot 옵션은 1.9.17p1부터 더 이상 지원되지 않습니다.)
     * 시스템 로그를 점검하여 비정상적인 Sudo 사용 이력이 있는지 확인하세요.

   전 Sudo 돌아가는 시스템에서도 전부 뜯어내고 doas 를 alias로 sudo 대신 작동하게 해두는데 이쪽이 더 보안상의 이점이 있다고 들었던거 같네요
"
"https://news.hada.io/topic?id=21739","MUVERA - 멀티 벡터 검색을 단일 벡터 수준으로 빠르게","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   MUVERA - 멀티 벡터 검색을 단일 벡터 수준으로 빠르게

     * 단일 벡터 임베딩 기반 검색은 빠르고 효율적이지만, 최근 ColBERT 등 멀티 벡터 모델은 각 토큰별 다수 벡터로 더 풍부한 의미와 정확도를 제공함
     * 멀티 벡터 방식은 Chamfer similarity 등 복잡한 유사도 계산으로 인해 연산량·검색 비용이 크게 증가, 대규모 실시간 검색에 장애로 작용함
     * 구글 연구팀이 제안한 MUVERA는 멀티 벡터 정보를 고정 길이 벡터(FDE, Fixed Dimensional Encoding)로 압축해, 단일 벡터 기반 MIPS(내적 최대 검색)로 초고속 검색 후 재정렬함
     * 이 방식은 데이터에 독립적이며 이론적 근거(Chamfer similarity 근사 오차 보장) 를 제공, 기존 PLAID 대비 90% 이상 지연 감소와 10% 이상 recall 향상 달성
     * FDE는 압축까지 지원(32배 메모리 절감), 오픈소스 구현체와 논문도 공개되어 검색·추천·NLP 실서비스 도입에 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

임베딩 모델과 정보 검색의 발전

     * 딥러닝 기반 임베딩 모델은 사용자 쿼리(예: “에베레스트 산 높이”)에 대해 방대한 데이터셋(문서, 이미지, 영상 등)에서 연관 정보를 빠르게 찾기 위한 핵심 도구임
     * 각 데이터포인트를 단일 벡터 임베딩으로 변환함으로써 의미상 유사한 데이터들이 수치적으로 비슷한 벡터 구조를 갖게 설계됨
     * 벡터 간 내적 유사도 계산을 활용하여, 최대 내적 검색(MIPS) 알고리듬으로 빠른 검색 성능을 제공함
     * 하지만 최근 ColBERT 등 멀티 벡터 모델은 더 높은 검색 정확도와 복잡한 관계 파악 능력으로 주목받음

멀티 벡터 모델의 도입과 한계

     * 멀티 벡터 모델은 각 데이터포인트를 다수 개의 임베딩 벡터 집합으로 표현함
     * Chamfer 유사도 측정법과 같은 복합 유사도 함수를 사용하여, 기존 단일 벡터로는 잡아내지 못했던 정보 포함 및 관계를 정확히 포착함
     * 이 방식 덕분에 더 정확한 정보 검색과 관련성 높은 문서 추천이 가능해짐
     * 단점으로는 임베딩 수 증가와 유사도 계산 복잡성으로 인해, 검색에 요구되는 컴퓨팅 자원이 상당히 커짐
          + 토큰별 벡터 수 증가 → 연산량·메모리 대폭 증가
          + 비선형(행렬곱) 연산이 필수 → 단일 벡터 기반 서브리니어(초고속) 검색 불가
          + 대규모 서비스 적용 시 비용·지연이 급증

MUVERA: FDE로 멀티 벡터 검색의 혁신

     * 논문 “MUVERA: Multi-Vector Retrieval via Fixed Dimensional Encodings”에서는 이 효율성 문제를 극복할 새로운 알고리듬을 제안함
     * MUVERA는 멀티 벡터 정보를 단일 FDE 벡터로 변환, 기존 MIPS 인덱스/서버를 그대로 활용해 고속 후보 검색 가능
         1. FDE 생성: 쿼리·문서의 멀티 벡터 집합을 고정 길이 벡터(FDE)로 변환(데이터 독립적 매핑)
         2. MIPS 검색: 모든 문서의 FDE를 MIPS 인덱스에 저장, 쿼리 FDE로 후보를 초고속 탐색
         3. 정확도 보장 재정렬: 후보 문서에만 Chamfer similarity 등 원래 멀티 벡터 연산을 적용, 정밀 재정렬로 최종 결과 제공
     * FDE는 데이터셋과 무관하게 적용 가능, 스트리밍 등 동적 환경에도 유리함

이론적 기반

     * 확률적 트리 임베딩 등 고급 기하 알고리듬에서 착안, FDE로 멀티 벡터 유사도를 강력하게 근사
     * 임베딩 공간을 랜덤하게 분할, 쿼리/문서 벡터가 동일 섹션에 위치하면 근사 유사도 계산
     * 논문에서 Chamfer similarity 근사 오차 범위 내 보장 이론 및 실험 데이터 제시

실험 결과 및 성능

     * BEIR 벤치마크 등 다양한 대규모 IR 데이터셋에서 MUVERA 성능 검증
          + 기존 PLAID 등 대비 평균 10% 더 높은 recall 달성
          + 90% 이상 검색 지연(latency) 감소
          + 동일 recall 시, FDE 기반 후보 문서 수를 기존 대비 5~20배까지 줄임
          + Product Quantization 등 추가 압축 기법과도 궁합 우수(메모리 32배 절감)
     * 멀티 벡터 검색의 실용성 대폭 개선, 대규모 검색·추천·NLP 응용에 적합

결론 및 활용

     * MUVERA는 멀티 벡터 검색을 단일 벡터 수준으로 가속화하는 혁신적 접근법
     * 오픈소스 구현체(GitHub 링크) 및 논문, 실험 결과 모두 공개
     * 검색 엔진, 추천 시스템, 자연어 처리 등에서 대규모 멀티 벡터 검색 효율화의 실질적 대안
     * 추후 연구·최적화가 더해질 경우, 더욱 폭넓은 산업 현장에 적용될 것으로 기대됨

        Hacker News 의견

     * 최근 Weaviate에 Muvera를 추가한 경험 소개와 블로그, 팟캐스트 링크 공유. ColBERT 스타일의 multi-vector 접근 방식에서, 토큰마다 임베딩을 할 경우 비용이 폭증하는 문제 언급. 예를 들어, 기존 768차원의 벡터 대신 최대 16,640차원 이상으로 늘어날 수 있어 여러 상황에서 비현실적인 부담. Muvera는 여러 개의 벡터를 하나의 고정된 차원(일반적으로 더 작은 차원 수)의 벡터로 변환해 어떤 ANN 인덱스에서든 바로 활용 가능. 단일 벡터 사용 덕분에 기존의 알고리즘들과 여러 양자화 기법을 적용할 수 있어 메모리 절약 효과. PLAID와 달리 특정 인덱스 구조나 클러스터링 가정이 필요 없어 대기 시간도 더 짧은 장점 강조
     * 최근 평탄화(mean-pooling)해서 하나의 임베딩으로 만드는 방식에서 거리가 생기는 트렌드 언급. 토큰별 임베딩을 모두 다루기엔 벡터 수가 너무 많으므로, 적절히 줄이는 방식이 필요하다는 점 지적. 이 방법은 토큰 임베딩을 임의 분할로 클러스터링하고 각각을 mean-pooling한 뒤 연결해 고정 길이 임베딩으로 합치는 형태. 완전한 multi vector 비교는 퍼포먼스상 힘들어 단일 벡터 도구와 퍼포먼스로 묶어서 비교할 수 있도록 k개의 벡터로 클러스터링해서 연결. 결과적으로 파티션 수를 고정하므로 k-means 스타일의 토큰 임베딩 클러스터링 효과. 토큰을 동적으로 클러스터링하면 변수 개수 임베딩이 나올 수 있어 더 나은 결과 가능성도 제시
     * 이 방법이 하이퍼파라미터에 매우 민감했고, 본인 실험에서는 maxsim과 비슷한 성능을 내지 못한 경험 공유
     * Muvera가 쿼리의 FDE(Fixed Dimensional Embedding)를 계산하고, 모델 데이터셋에서 유사한 FDE를 찾는 방식이라고 이해했는데, 그렇다면 모델의 모든 동일 크기의 FDE도 계산해야 하는지 질문
          + 그렇지만 이 작업을 데이터 적재 시에 한 번만 하면 되고, 이후 검색은 사전 계산된 FDE에 대해 MIPS(Maximum Inner Product Search)로 처리 가능한 점 설명
     * 이 분야에 대해 잘 알지 못하지만, SQL 쿼리로 테이블의 모든 이름을 반환하는 것처럼 뉴럴 임베딩 쿼리도 같은 식으로 동작하는지, 아니면 결과가 더 모호해지는지 묻는 내용
     * 본질적으로 여러 임베딩을 하나로 압축하는, 즉 “embedding of embeddings”으로 차원을 줄이고 성능을 높이려는 접근 방식 요약. 여러 개의 임베딩이 크게 겹친 정보를 담고 있으므로 하나로 압축이 가능하다면 추가적인 임베딩이 주는 가치는 대부분 낮은 수준이라 판단. 만약 성능이 비슷하다면 정보이론 관점에서 손실 없이 표현 가능한지 의문
          + 추가 임베딩의 한계 효용이 낮다는 의견이 논문의 핵심임을 짚어주며, 단일 임베딩 벡터가 충분히 희소해 추가 벡터 정보를 효과적으로 함께 담아 검색 성능을 높일 수 있다는 점이 논지임을 설명
     * 기존의 feature hash 방법(여러 임베딩을 하나로 줄이는 방법)과 차이점, UMAP 등 단일 벡터로 차원축소하는 기법이 도움이 될 수 있는지 질문
          + UMAP은 값을 동일 좌표 공간으로 사영하지 않고, 좌표상 위치가 달라지기 때문에 추상적 특성은 비슷해도 실제 좌표상 결과는 다를 수 있다는 점 지적
"
"https://news.hada.io/topic?id=21797","유전자 치료로 청각 장애 환자의 청력 회복","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        유전자 치료로 청각 장애 환자의 청력 회복

     * 유전자 치료를 통해 유전성 난청 환자의 청력이 일부 회복되는 임상 결과 보고
     * OTOF 유전자 변이로 인한 청각 장애 환자 10명에 대해 인공 아데노 관련 바이러스를 이용한 유전자 삽입 치료 시도
     * 치료 후 1개월 내로 대부분 청력 일부 회복 및 6개월 후 평균 인지 가능 소리 크기 106dB에서 52dB로 개선 결과 확인
     * 어린이에서 치료 반응이 더 뛰어났지만 성인에서도 효과 확인
     * 특별한 심각한 부작용 없음이 확인되어 안전성에 대한 긍정적 신호 확인
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 개요

     * Karolinska Institutet의 Maoli Duan 박사는 이번 유전자 치료 연구를 유전성 난청 치료 분야에서 큰 진전임을 강조함
     * 이번 연구는 중국 5개 병원에서 1세~24세 환자 10명을 대상으로 수행됨
     * 참가자들은 모두 OTOF 유전자 변이로 인한 청각 장애 혹은 심각한 난청을 가진 환자군임
     * OTOF 변이는 아우토페르린(otoferlin) 단백질 결핍을 야기하며, 이는 귀에서 뇌로 소리 신호 전달에 핵심적 역할을 가짐

한 달 내 효과

     * 유전자 치료는 합성 아데노 연관 바이러스(AAV) 를 이용, 정상 OTOF 유전자를 달팽이관 기저부 라운드 윈도우(round window) 를 통해 1회 주입하는 방식임
     * 치료 효과는 빠르게 나타남, 대다수 환자에게서 1개월 내 청력 일부 회복 현상 나타남
     * 6개월 후 추적 관찰에서는 참가자 전원에게서 청력 호전이 관찰되었으며, 인지 가능한 소리 크기 평균이 106dB에서 52dB로 큰 향상을 기록함
     * 어린이들, 특히 5~8세 환자에서 반응이 가장 뛰어났으며, 7세 소녀는 치료 4개월 만에 거의 정상 대화가 가능해짐
     * 성인 역시 유의미한 청력 개선이 확인됨

청력 개선의 의미와 지속성

     * 이번 연구에서 10명 중 다수의 청력이 뚜렷하게 향상되어 삶의 질 개선에 영향을 줄 수 있음을 연구진이 언급함
     * Dr. Duan은 소규모 중국 아동 대상 치료가 이전에 성공한 사례는 있지만, 십대와 성인을 대상으로 한 최초의 임상이라는 점을 강조함
     * 현재 연구진은 치료 효과의 지속성을 추적하는 중임

안전성 및 부작용

     * 치료법은 전반적으로 안전하며 내약성이 좋음이 확인됨
     * 가장 자주 나타난 이상 반응은 백혈구 일종인 호중구 감소 현상이었음
     * 6~12개월 추적 관찰 기간 내 심각한 부작용은 보고되지 않음

연구의 확장 가능성

     * Dr. Duan은 OTOF 외에도 더 흔한 유전성 난청 원인 유전자(GJB2, TMC1)에 대한 치료 연구도 확장 중이라 강조함
     * 동물 실험에서 고무적인 결과를 보였으며, 앞으로 다양한 유전성 난청 환자에게도 치료 기회가 확대될 수 있을 전망임

연구 협력 및 후원

     * 연구는 Zhongda Hospital, Southeast University 등 여러 기관과 공동으로 수행되었음
     * 연구 자금은 중국 여러 연구 프로그램 및 Otovia Therapeutics Inc. 에서 지원함
     * 연구에 참여한 여러 연구원들은 Otovia Therapeutics Inc. 소속이며, 이해 충돌 관련 정보는 논문 원문 참조 필요

논문 정보

     * 논문 제목: “AAV gene therapy for autosomal recessive deafness 9: a single-arm trial”
     * 저자: Jieyu Qi 외
     * 학술지: Nature Medicine, 2025년 7월 2일 온라인 게재
     * DOI: 10.1038/s41591-025-03773-w

        Hacker News 의견

     * 이 소식이 정말 반가운 이유는 앞으로 이 연구가 약물, 귀 감염, 그리고 일반적인 감각신경성 난청 등 다양한 종류의 난청 치료로 확대될 수 있기를 기대하는 희망 때문임
          + 이번 연구는 결핍된 단백질에 유전자 추가라는 굉장히 좁은 범위의 조건과 해결책에 초점이 맞춰진 모습임. 아쉽게도 이 연구가 내가 바라는 대로 확장될 가능성은 낮다는 생각임
     * 이와 비슷한 맥락, 같은 트렌드로 시각장애 유전자 치료 연구도 진행 중임 관련 링크
          + 왠지 특이점(singularity)이 이미 도래한 느낌임. 모두가 그 혜택을 고루 누릴 수 있었으면 하는 바람임
     * 어떻게 하면 본인이 이번 연구에서 다루는 난청 유형인지 알아볼 수 있을지 궁금함. 나는 태어날 때부터 양쪽 귀에 중등도에서 고도 난청이 있는데, 표준 내이 검사 외에는 특별히 원인을 밝히려는 시도를 들어본 적이 없음
          + 전체 유전체 시퀀싱 비용이 이제 저렴해진 상황임. Broad Clinical Labs에서 20x hifi long read로 약 1200달러면 가능, 그리고 opencravet를 사용해 직접 결과 분석도 가능함. 최근에 개인 분석 관련 웨비나도 열렸음 웨비나 링크. Franklin by genoox는 인터페이스 선호도에 따라 더 사용자 친화적인 제품일 수 있음. 유전자 연구는 매우 많은 종류와 미묘한 변이로 인해 시민 과학의 시대에 적합하다고 생각함
          + 이런 경우에는 랜덤한 HN 이용자보다는 직접 이비인후과 전문의에게 물어보는 것이 가장 적절한 접근임
          + chatgpt에 물어보면 필터 기준은 1) 증상(OTOF 관련 난청은 대체로 언어 발달 이전에 발생하고, 심각하며, 진행성은 아님) 2) 가족력(문제 유전자는 성염색체에 있지 않고 반드시 양쪽 부모가 보인자여야 함, 단 둘 중 한 명만 보인자여도 가능) 3) 실험실에서의 유전자 검사임
     * 요즘 유전학을 기반으로 한 발전이 정말 많아진 느낌임. 유전학에서 이런 흐름을 이끄는 특정 분야가 따로 있는지 궁금함. 예전에는 CRISPR/Cas9 같은 이야기가 많이 들렸었는데, 요즘 이 성과들의 기반이 되는 기술인지 궁금함. 그런데 이렇게 기술이 좋은데 왜 많은 유전자 편집 관련 주식은 상장 이후 90% 넘게 가치가 떨어졌는지도 의문임
          + 이번 연구에 적용된 기술 자체는 사실 오래되고 어렵지 않은 기술임. 다만 주요 이슈는 안전성과 규제에 있음. 유전자 치료 초기 시도에는 비극적인 결과가 있었고, 최근 치료들은 이런 과거의 실수를 반복하지 않으려고 조심하는 중임. 질병 치료에 엄청난 잠재력이 있지만, 특히 유전적으로 유전될 수 있는 변화인 경우 규제가 매우 엄격한 환경임
     * Regeneron이 2025년 2월에 청각장애 치료용 유전자 치료제의 긍정적 결과를 발표한 뉴스가 있음 관련 링크
     * 이번 연구 논문이 유료 구독 벽에 가려져 있음 논문 링크. 아직도 유료 저널에 논문을 투고하는 연구자들이 있다는 점이 매번 놀라움
     * 20년간 청각을 잃은 사람이 갑자기 청력을 회복하는 것이 얼마나 어려운지 궁금함
          + 이번 치료의 세부 사항에 대해서는 설명할 수 없지만, 청각 회복의 실제 혜택은 난청이 발생한 나이와 심각도에 따라 다름. 선천적으로 혹은 언어 발달 시기에 청각을 잃은 경우 조정 기간이 매우 길고, 세상의 모든 소리가 그냥 소음처럼 들리기에 소리, 말 등을 구분하는 훈련이 많이 필요함. 인공와우처럼 장치를 뺐다 꼈다 할 수도 없어서, 뇌가 쉬는 시간이 없음. 반면, 성인 이후 청력을 상실했거나 약간의 청각이 남아 있다면 회복 이후 적응이 훨씬 수월할 것임
     * 농인 커뮤니티에서 이런 신약 소식에 대해 어떻게 받아들이는지 궁금함. 일부 커뮤니티에서 인공와우에 적대적인 반응이 있다는 것도 흥미로움
          + 인공와우 논쟁처럼 이번 유전자 치료도 비슷하게 논란임. 많은 농인/청각장애 부모들은 '듣는 아이'를 원함. 문화적 요소가 없다면 대부분이 듣는 아이를 원할 것임. 하지만 문화적 요소를 무시할 수 없음. 농인 부모가 농인 자녀의 청각을 '치유'하면 그 아이는 결국 부모로부터 멀어지게 됨. 비록 이중문화적 정체성을 가진다 해도 대개는 청인 세계에서 살게 됨. 실제로는 농인 부모의 상당수가 청력 있는 아이를 갖게 되며, 이런 유전적 농인 가족은 상대적으로 드묾. 그러나 이 소수 가족과 농문화 안에서 살아가는 사람들은 장애인이 아닌 것임. 그들이 사는 문화 속에서는 농 자체가 전혀 장애가 아님. 그들에게 '너희가 뭔가 부족하다'고 설득하는 건 굉장히 어려운 일임. 실제로는 그렇게 느끼지도 않음. 미국에는 농문화와 교육 전통이 매우 강해,
            수어로 박사까지 취득 가능하고, 직업도 가질 수 있음. 청인 세계가 오히려 낯선 외부 문화임. 물론 나는 본인이나 자녀가 농이면 당연히 치료를 택할 것임. 나는 농문화 사람이 아니기 때문에 소외감을 느끼는데, 농문화에서 살아가는 사람들은 그렇지 않음. 그들은 자신들이 결핍됐다고 느끼지 않음. 이걸 청인들에게 전달하는 게 어려우며, 많은 청인들이 수어를 언어로 인정하지 않는 오래된 관념을 갖고 있음. 하지만 농문화 사람들에게서 소리는 그저 인식할 수 없는 하나의 요소일 뿐, 삶에 결핍은 없음
          + 나는 농커뮤니티 구성원은 아니지만, 내 아들이 고도 난청임. 미래에는 이런 질환이 충치 치료처럼 간단하게 고쳐질 수 있어 내 아이가 많은 것을 놓치지 않고 살길 기대함
          + 내가 고도 난청을 갖고 있는데, 인공와우를 싫어하는 감정은 정말 이해할 수 없음. 내 장애는 보청기로 아주 잘 해결되고 있고, 누구나 그런 기회를 가졌으면 하는 바람임
          + 이건 정말 양날의 검임. 이미 작은 농/난청 커뮤니티가 더 작아지는 효과가 있음. 소아마비나 천연두처럼 완전히 박멸이 가능한 장애가 아니기 때문임. 일부에겐 효과적인 점이 멋지지만, 모두에게 유전자 치료가 적용될 때까지는 여전히 커뮤니티 내에서 논란이 계속될 것임. 커뮤니티가 더 작아진다는 점은 많은 감정을 불러일으킴. 하지만 농인들의 세계가 작아질 필요는 없음. 우리가 아직 할 수 있는데도 안 하는 일들(비용이나 필요성의 문제 등)을 빼고도 많은 방법이 있음. 헬렌 켈러가 ""실명은 사물에서 사람을 분리하고, 난청은 사람과 사람을 분리한다""고 했는데, 사실 난청이 사람과 사람을 단절시킬 필요는 없지만 현실은 다름. 장애를 '완전히' 없앨 수 있는 날이 오기 전까지는, 사회가 농인의 세계를 더 크게 만들 방법을 찾아야 한다고 생각함
          + 나 역시 처음 떠오른 생각이, 커뮤니티에 남기 위해 듣지 않는 쪽을 선택하는 사람이 얼마나 될지 궁금함
     * 자폐 치료 논의만 나오면 늘 자폐 치료가 우생학이라는 주장, 혹은 극단적으로 반대하는 반응이 많음. 반면 난청이나 다른 중증 장애의 치료 개발은 아이를 위해 당연히 필요한 일로 여겨지는 차이를 볼 때, 어떤 장애는 치료가 가치로운 것으로 여기고, 또 다른 장애는 인간성의 일부로서 긍정적으로 여기는 근본적 차이가 왜 있는지 궁금함
          + 자폐는 스펙트럼 장애라 저기능성 자폐의 치료는 논란이 되지 않는다고 생각함. 반면 고기능성 자폐인은 장애라기보다는 독특한 성격적 특징으로 볼 수도 있음. 초집중력, 강한 집착, 체계화 사고가 기술 분야 등에서는 사회적 장점이 될 수 있음. 유전성 출혈성 질환처럼 전혀 이점이 없는 질환은 별도의 논란이 없다고 생각함
     * 이 연구가 흥미로운 이유는, 예전에 Chiropractic이 척추지압으로 난청을 고쳤다고 주장했던 '돌팔이'가 있었으나, 제대로 된 과학적 방법이 결국 얼마나 강력한지 보여주는 사례이기 때문임
"
"https://news.hada.io/topic?id=21806","The Gap Strikes Back: 이제 스타일링 가능해진 CSS Gap","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The Gap Strikes Back: 이제 스타일링 가능해진 CSS Gap

     * 기존에는 CSS gap 속성으로 간격을 만들 수 있었지만, gap 구간 자체를 스타일링하는 것은 불가능해서 다양한 우회 방법(추가 요소, border, pseudo-element 등)이 필요했음
     * 새로운 CSS gap decorations 기능은 row-rule, column-rule 등의 속성으로 레이아웃 아이템 사이에 직접 선(Separator) 을 그릴 수 있게 해줌
     * Grid, Flexbox, Multi-column, 곧 Masonry까지 모든 주요 레이아웃에서 데코레이션 적용 가능, 별도 마크업이나 불필요한 엘리먼트 없이 순수 CSS만으로 구조적/시각적 개선 가능
     * 다양한 스타일(두께, 색상, 패턴 등) 반복/조합 지원하며, repeat(), outset, paint-order 등으로 정밀 제어 가능
     * 현재는 Chromium 계열(Chrome/Edge 139+)에서 플래그 활성화로만 사용 가능, 표준화·확장 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

기존 gap 한계와 변화

     * 예전에는 gap 영역을 스타일링하기 어렵고, separator용 보더나 가짜 엘리먼트를 추가해야 했음
     * 이런 방식은 레이아웃 크기/접근성/마크업 복잡도 등 여러 단점 발생
     * 이제 gap decorations 표준이 등장해, 간단한 CSS 속성만으로 간격 영역에 데코레이션(선 등) 적용 가능

CSS gap decorations 소개

     * column-rule: 기존 multi-column에서 세로 구분선 그릴 때 사용(예: column-rule: 1px solid black;)
     * 이제 이 속성을 flexbox, grid 등에도 사용 가능
.my-grid-container {
  display: grid;
  gap: 2px;
  column-rule: 2px solid pink;
}

     * row-rule: 행(가로) 간의 separator를 그릴 수 있음
.my-flex-container {
  display: flex;
  gap: 10px;
  row-rule: 10px dotted limegreen;
  column-rule: 5px dashed coral;
}

     * 여러 스타일 반복/조합: repeat()나 콤마 구분으로 다양한 스타일을 섞을 수 있음
.my-container {
  display: grid;
  gap: 2px;
  row-rule:
    repeat(2, 1px dashed red),
    2px solid black,
    repeat(auto, 1px dotted green);
}

     * 정밀 제어 속성: row-rule-break, column-rule-break, row-rule-outset, column-rule-outset, gap-rule-paint-order 등으로 위치, 교차점, 겹침 순서 등 세밀하게 조절

예시: gap decorations 실전 적용

     * 예제 페이지는 body를 grid로 지정하고, header/nav/main/footer를 gap으로 구분
     * row-rule로 두께, 색상, 스타일이 다른 구분선 적용 가능
     * nav 메뉴는 flexbox와 column-rule로 항목 사이에 선 그리기
     * main 영역은 flexbox로 이미지·텍스트를 masonry 형태로 배치, row-rule, column-rule로 격자 스타일 구조 강조
     * column-rule-outset: 0; 등으로 선의 시작/끝 위치 정밀 조정 가능

브라우저 지원 및 활성화

     * 현재는 Chrome/Edge 139+ 등 Chromium 기반 브라우저에서 플래그(about://flags → Experimental Web Platform Features) 활성화 필요
     * 정식 표준화 진행 중이며, 다양한 피드백과 실험 권장

더 알아보기 및 Playground

     * CSS Gap Decorations Module Level 1 표준 초안
     * Microsoft Edge explainer
     * 인터랙티브 Playground에서 다양한 데코레이션 조합 시각 실험 가능
     * 기능·호환성·피드백 관련 이슈는 Chromium issue tracker에서 제안 가능
"
"https://news.hada.io/topic?id=21719","BusyBeaver(6)은 정말 큼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          BusyBeaver(6)은 정말 큼

     * Busy Beaver 6번째 수(BB(6)) 의 하한이 최근 새로운 연구로 대폭 증가함
     * 기존에는 BB(6) > 10↑36,534으로 알려졌으나, 2022년 BB(6) > 10↑1510으로 상향 조정됨
     * 최근에는 BBchallenge에서 BB(6) > 10↑10,000,00010으로 다시 상향, 이어서 2 ↑↑ (2 ↑↑ (2 ↑↑ 9)) 까지 갱신됨
     * BB(6) 의 크기는 상상을 초월하며, 이 수는 우주 전체를 수없이 채울 수 있는 정도임
     * 이러한 발전은 수학적 논리와 계산 이론의 한계와 잠재성을 새롭게 인식하게 하는 계기임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

BB(6) 최근 연구 성과 개요

     * 최근 수년간 세상과 연구 환경이 힘들게 느껴지는 상황이 지속되었음
     * 그러나 이번 Busy Beaver 연구의 발전이 다시 연구에 대한 순수한 열정을 상기시키는 계기였음
     * 2022년에는 Pavel Kropitz가 BB(6) > 10↑1510임을 증명하였음
          + BB(6)은 6개의 상태를 가진 튜링머신이 올-제로 테이프 위에서 정지 전까지 최대 몇 번 동작할 수 있는지를 의미함
          + 여기서 ^1510은 10을 자기 자신으로 15회 반복 거듭제곱(테트레이션)한 값임
     * 이전의 연구에서는 BB(5)가 47,176,870임이 밝혀졌는데(BBchallenge 팀), 이는 이 수치가 관측 가능한 현실의 범위를 넘는 영역으로 급증하는 시점임

최근 하한 갱신 과정

     * BBchallenge의 ""mxdys""가 BB(6) > 10↑10,000,00010임을 증명함
          + 이 증명은 Coq 언어로 작성된 공식 증명에 기반함
     * 이후 다시 BB(6) > 2 ↑↑ (2 ↑↑ (2 ↑↑ 9)) 로 하한이 업데이트됨
          + ↑↑는 테트레이션(거듭제곱의 반복)을 의미하며, 이는 2를 2로 테트레이션, 다시 그 결과로 테트레이션을 9회 반복하는 형태임
          + 이 정도의 수는 기존 어떤 직관적 이해도 초월하는 영역에 해당함
     * 참고로 펜테이션은 테트레이션의 반복을 의미하며, 이런 운영은 곱셈, 거듭제곱, 테트레이션을 넘어서는 연산임

커다란 수의 크기 이해

     * 기자의 요청으로 10↑10,000,00010이라는 수의 크기를 설명할 필요가 있었음
     * 이 모래알 개수는 10↑10,000,00010개의 우주를 모래로 채울 수 있을 정도임
     * 이처럼 BB(6) 수치는 실제 관측 세계를 아득히 넘어선다는 점을 전달함

BB알고리듬의 본질적 한계에 대한 고찰

     * BB(6) 수치의 엄청난 크기는 Busy Beaver 함수의 진정한 잠재력을 보여줌
     * BB(n) 값이 세트 이론(ZFC)의 공리계에서 독립적이 되는 시점이 n=20~30 정도로 추정되었으나, 아마도 n=7~9에서도 이미 독립적으로 될 수 있음을 예측하게 됨
          + 현재는 n=643에서 독립임이 공식적으로 알려짐

부록: 최근 행사 및 강연 소식

     * 필자는 최근 프라하에서 열린 STOC'2025 행사에 참석하여 다양한 연구자들과 교류하고 새로운 정보를 얻었음
     * 자신의 양자 가속화 현황에 대한 기조 강연 슬라이드도 공유함
     * 이 내용에 대한 보다 자세한 후기는 추후 공유 예정임

        Hacker News 의견

     * bbchallenge Discord 서버에서, 사람들이 얼마나 많은 튜링머신 상태가 있어야 Graham's Number를 초월할 수 있는지 추측하는 모습 공유함. 최근 BB(6) 우승자가 달성한 2^^2^^2^^9도 이미 엄청난 수이지만, Graham의 성장 양상이 생각보다 빨리 등장할 수 있음에 놀람. 49비트 람다 항만 있어도 충분하다는 functional busy beaver [1] 자료와, 해당 사이즈의 폐쇄 람다 항이 77519927606개[2]에 불과하다는 점, 반면 6-state 튜링머신이 무려 399910780272640개나 존재함[3]을 언급함. 펜테이션이 단 6상태로 구현된 것을 계기로 이제 상당수 관련자들이 7상태면 Graham's Number도 넘을 수 있다고 믿는 분위기라 언급. 본인은 그럼에도 여전히 의외라고 봄. 며칠 전, 향후 10년 내 BB(7)이 Graham's Number를 넘는다는 증명이 나올지 두고 큰 내기를 했다고 소개하며, 다른 이들의 의견을 묻는 질문. (1, 2, 3 링크
       제공)
          + 전문가인 척 하진 않지만, BB(7)가 Graham's Number보단 클 거라 예측함. BB는 임의의 계산 가능한 수열보다 빠르게 성장해야 하므로, 실제 BB(7)이 얼마나 클지는 손으로만 짚는 수준이지만, 결국 모든 계산 가능한 연산자(예: up-arrow^n 등)보다 더 빨리 커져야 하는 방향성이 있음. 47176870에서 2^^2^^2^^9까지 성장 폭이, 2^^2^^2^^9에서 Graham's Number로 가는 폭보다 연산자 강도로 훨씬 더 극적인 변화임을 예감함. Graham's Number가 g_64인데 이 또한 up-arrow^n보다 한 단계 위의 개념으로 해석될 수 있어서, BB(7)이 Graham's Number를 넘을 것이라는 직감 공유.
     * BB(748)처럼 불계산적인 숫자가 ZFC(집합론 공리계)와 독립적이라는 사실이 매우 신기함을 표현함. 마치 범주 오류처럼 느껴진다는 솔직한 심경 공유.
          + BB(748)이 ZFC와 독립적인 이유는 값 그 자체가 아니라, 748상태 중 하나인 TM_ZFC_INC가 ZFC에서 모순(거짓 증명) 발견 시에만 멈추는 방식 때문임. BB(748)=N임을 증명하려면 TM_ZFC_INC가 N스텝 내 멈춘다 혹은 끝없이 돌음을 보이는 것인데, 괴델의 불완전성 정리에 따르면 ZFC가 모순이 아니라면 그 어떤 것도 증명 불가인 함의.
          + 적은 줄의 텍스트(즉, ZFC 공리 자체)가 인간에게 중요한 산술적 진리를 충분히 표현할 수 있다고 생각한 것 자체가 더 놀랍다고 느낌. 6상태 튜링머신의 동작조차 간단히 예측이 안 되는 현실은 당연함. 괴델의 불완전성 정리 발표 이후 수학계가 더 많은 공리를 찾으려 꽤 적극적으로 움직였어야 했는데, 실제로는 기초연구 일부에서만 다뤄진 아쉬움 토로.
          + 연속체 가설의 진릿값(플라톤주의적으로 보면 1 혹은 0)이 ZFC로 독립적 증명됨은 좋은 예시임. 거대한 수가 아닌 단순히 1비트도 ZFC에서 보장불가.
          + BB(n)이 불계산적인 것이고, BB(748)은 (정의상) 748상태 튜링머신이 쓴 1의 개수이므로 계산 가능한 수임을 명확히 구분함. ""독립적""이란 라벨 자체는 ZFC로 이 수가 정말 우리가 원하는 값임을 증명하는 과정에서 더 강력한 이론이 필요하다는 이야기임을 설명함.
          + 숫자 자체가 ZFC와 독립적인 게 아니라, BB(748)을 계산하는 과정이 독립적임을 강조함. (모든 정수는 ZFC로 표현 가능)
     * BB(14)가 Graham's Number보다 크다는 것은 유명한 사실이고, 이번 연구로 BB(7)도 Graham's Number 이상일 것이라는 직감 밝힘. 직관적으로, 펜테이션에서 Graham's Number까지 가는 아이디어가 47,176,870에서 2 <pentate> 5까지 도달하는 것보다 오히려 단순하다고 느낌.
          + 좋은 정보라며, 본인 글에 대한 훌륭한 답변이 될 수 있을 것이라는 반응 남김
     * Scott Aaronson의 “How Much Math Is Knowable?” [Harward CMSA] 유튜브 강연 https://www.youtube.com/watch?v=VplMHWSZf5c 및 최근 HN 논의글 https://news.ycombinator.com/item?id=43776477 링크로 공유
     * ""좌상단 첨자""가 테트레이션, 즉 반복 거듭제곱임을 알림. 1510이면 10의 10의... 15번 반복임을 설명. 처음 보는 개념이라 오타인가 싶었다고 공유함.
          + 반복 연산 테마로 이어가면서, 이번엔 ‘펜테이션’을 처음 접했다는 반응
          + 테트레이션을 이전에 본 적 있지만, Knuth의 up-arrow 표기법[1]이 더 보편적이고 일반화에 좋다는 선호 밝힘 (1)
     * BB(6)은 6상태 2심볼({0,1}) 튜링머신이 초기 0테이프에서 멈추기 전 최대 스텝수라는 설명이 비전문가에게 매우 유익했음. 이 분야가 수십년 연구자를 위한 고난도 밀도와 특화된 용어로 구성된 곳임을 체감했으나, 우연히 이렇게 깊이 있는 글을 접하게 돼 신기하다는 긍정적 소감 공유
          + 컴퓨터공학 학부생 수준이라면 busy beaver 문제를 처음 보더라도 감은 잡을 수 있을 내용이라고 생각함. 물론 특수 용어가 많지만, 수십년 경력 전용이라고 느낄 필요는 없다는 격려식 조언 추가
          + 해당 정의는 소프트웨어 엔지니어링보다는 컴퓨터공학 이론 쪽에서 표준임을 밝힘
     * ""10,000,000 sub10"" 개의 모래알이 있으면 관측 우주를 10,000,000 sub10배만큼 채울 수 있다는 설명에 혼란을 느낌. 관측 우주 질량으로 비교하는 게 일반적인데, 이 방식은 이미 실제 물질량보다 훨씬 크다고 지적함
          + 맞다고 답변. 모래알/우주로 나눠도 그 자체가 거의 같은 급의 거대한 수라, 인접한 수(이 표기법에서)는 어마어마하게 차이 남. 10↑↑10,000,000 / (모래알/우주)도 10↑↑9,999,999보단 훨씬 크다고 설명. 우리 시스템에선 (매우 큰) / (우주적으로 큰) 값도 그냥 (매우 큰)으로 정리된다고 비유
          + 테트레이션에선 더 이상 단순한 자릿수 비교가 아니라, ""자릿수의 자릿수"" 급 성장임을 부연
          + 이 수는 모래알 10^100000개 정도로도 도저히 줄지 않으니, 나눠도 본질적으로 영향 미미함을 재확인
          + 10,000,000^10,000,000도 충분히 말도 안 되는 크기라서, 한 번 더 지수 꼬리를 더하면 비교 자체가 무의미해짐을 사례로 듦
          + 더 흔한 예시로, 유효숫자 개념에선 10억 - 100만 = 10억이라고 해도 비약이 아님을 들어줌
     * 5상태 튜링머신으로 증명 나열이 가능한 논리 체계 중 가장 ""풍부한"" 것은 무엇일까 궁금증 표시함
          + 어떤 기준으로 '나열'을 정의하느냐에 따라 답이 달라질 수 있음. 관련된 질문인 '5상태 튜링머신의 멈춤 여부를 다 증명하지 못하는 가장 강력한 논리 체계는 무엇인가?'도 의미 있음. Skelet #17 [0]이 멈추지 않음을 수학적으로 증명하는 게 매우 어렵기에, 이를 증명하는 이론이 있다면 나머지 5상태 튜링머신도 모두 결정할 수 있을 것 같다는 개인적 견해 공유 (0, 1)
          + 유한 이진 문자열을 논리 증명 나열로 해석하는 방식부터 명확히 해야 전제를 논의할 수 있음
     * 관측 우주가 BB(6)의 정확한 값을 적기에 충분히 클지 궁금증 제기
          + 관측 우주를 닫힌 시스템으로 봤을 때, Bekenstein bound를 적용해 계산하면 정보 저장 한계는 약 10^120 비트 수준임. 현재 추정상 전체 질량-에너지가 대략 10^53kg이고, 이를 식에 대입해도 10^120 비트 내외임. 따라서 불가능함을 수치 근거와 함께 설명
          + 우주에 저장 가능한 정보량이 약 10^120 비트이고, 단 억만 조 단위로 틀려도 아무 의미 없을 만큼 ""당연히 부족""함을 강조
          + 전체 값을 동시에 저장하는 상황을 가정한 것으로 추정됨. 동시성 조건이 없다면 무한 시간에 걸쳐 기록이 이론상 가능할 수도 있으나, 우주 열적 죽음 등 현실적 한계 고려 필요. CMB 기준 프레임에선 불가능하나, 다른 시공간 절단 개념을 생각해볼 수 있지 않을까 자문함
          + 기사 내 첫 숫자부터가 이미 ¹⁵10, 즉 10^(¹⁴10)꼴이라, 자리수 자체가 ¹⁴10임을 감안하면 절대 불가능
          + 불가함을 간단히 재확인
     * 계산 복잡도 이론의 이런 결과들을 볼 때마다, ""슈퍼 인공지능은 신이다""류의 유행 담론은 완전히 근거 없음을 실감함. 우주의 모든 원자를 슈퍼컴퓨터로 만들고 블랙홀 에너지까지 써도, BB(6)의 멈춤 상태를 계산하는 건 영원히 불가능함
          + 저런 허수아비론(strawman)은 애초에 설득력이 없었다는 간결한 반응
"
"https://news.hada.io/topic?id=21751","Gridfinity: 모듈형 오픈소스 격자형 저장 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Gridfinity: 모듈형 오픈소스 격자형 저장 시스템

     * Gridfinity는 작업실, 공방, 책상 등 다양한 공간에서 생산성 및 정돈 향상을 목표로 한 모듈형 저장 시스템임
          + 3D 프린팅 도구, 부품, 기타 소형 물품을 효율적으로 정리할 수 있도록 설계됨
     * 모든 모듈은 42x42x7mm의 정수 배수 크기를 따르며, 이는 다양한 물품을 효율적으로 정리하는 데 일관성을 부여함
     * 본 시스템은 무료 및 오픈소스로 배포되며, 대부분의 구성품이 3D 프린터를 통해 제작 가능함

Gridfinity의 탄생 배경

     * Alexander Chappel의 Assortment System은 CC-A-NC-SA 라이선스로 공개되며, 이는 Zack Freedman의 Gridfinity 디자인에 영감을 제공함
     * Gridfinity의 초판 디자인은 ""Gridfinity: Your Ultimate Modular Workshop is FREE!"" 영상에서 공개되었음
     * Gridfinity는 MIT 라이선스로 커뮤니티에 오픈되었으며, 누구나 자유롭게 확장 및 변형이 가능함

커뮤니티와 확장성

     * Gridfinity는 이제 커뮤니티 중심으로 다양한 분야와 환경에 맞게 계속해서 진화하고 있음
     * 사용자는 직접 시스템을 활용하거나 모듈을 설계 및 커스터마이즈하여 커뮤니티에 기여할 수 있음

   Gridfinity는 3D 프린팅 시작하면 처음에 많이 써보는 형태중 하나입니다.
   서랍안에 뭔가를 정리하거나 책상위에 정리함을 찍을때 유용합니다.

   https://www.printables.com/search/models?ctx=models&q=gridfinity
   https://makerworld.com/en/search/models?keyword=gridfinity

   Gridfinity 검색만 해봐도 다양한 사용예를 만날수 있습니다.

   저는 개인적으로 서랍은 BaBo 시리즈를 출력해서 쓰는데요
   https://makerworld.com/en/models/14125-babo-organizer-drawer-module-30

   Babo + Gridfinity도 가능합니다.
   https://printables.com/model/…
"
"https://news.hada.io/topic?id=21720","Ask GN: 기업 기술 블로그의 저작권 범위는 어떻게 되나용?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ask GN: 기업 기술 블로그의 저작권 범위는 어떻게 되나용?

   본문을 훔쳐가면 당연히 out 이고, 법이 아니여도 해야 할 일이 아니지만.

   제목& 링크의 모음을 두고, 검색정도 할 수 있는 기능을 제공하는 것이 문제가 될까요 ?
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   지금 시간이 비어서, 블로깅 서비스를 좀 만들어보고 있는데 리소스 생성을 유저에게 맞기는 것보다는 어디선가 읽을거리가 끊이지 않게 수급을 계속하는 것이 좋다고 봐서용

   법 전문가는 아니지만, 아니요.

   크롤링도 형사적으로 문제가 되지 않는데 그런게 왜 불법인가요?

   저게 불법이면 구글 검색도 불법이죠

   보통의 기업들은 더 좋아하지 않을까요?

   https://www.velopers.kr 도 있습니다!

   출처만 정확히 명시한다면 공정 이용에 해당할 확률이 높을 겁니다. 걱정할 필요는 없다고 생각해요

   수익 창출의 도구(예: 광고 설치)로 쓰는 순간 불리해지죠

   링크 모음 정도는 큰 문제가 되지 않을까요. https://www.codenary.co.kr/ 같은 서비스들이 이미 멀쩡하게 잘 있어서
"
"https://news.hada.io/topic?id=21732","많은 랜섬웨어 유형이 러시아어 키보드 설치 시 공격을 중단한다 (2021)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               많은 랜섬웨어 유형이 러시아어 키보드 설치 시 공격을 중단한다 (2021)

    1. 러시아 해커들이 싫어하는 간단한 보안 트릭

   대부분의 랜섬웨어는 특정 가상 키보드(러시아어, 우크라이나어 등)가 설치된 PC에서는 설치되지 않도록 설계되어 있다. 이 사실을 바탕으로 해당 키보드를 설치하면 공격을 피할 수 있다는 점이 트위터에서 화제가 되었다.

    2. 동유럽 해커들은 자국 컴퓨터는 건드리지 않는다

   다수의 러시아어 랜섬웨어는 CIS(독립국가연합) 국가를 피해가도록 만들어졌는데, 이는 자국 정부의 감시를 피하기 위함이다.

    3. 러시아 당국은 자국 피해가 없으면 수사하지 않는다

   러시아 등에서는 자국 내 피해자가 없으면 사이버 범죄 수사를 시작하지 않기 때문에, 해커들은 자국 피해를 아예 만들지 않으려 한다.

    4. 다크사이드의 반응

   다크사이드 그룹은 Colonial Pipeline 공격 이후 “정치적 목적은 없다”고 주장하며, 앞으로는 사회적 영향을 고려해 대상 기업을 선별하겠다고 밝혔다.

    5. 말과는 다르게 지역 기반 타깃팅은 필수

   다크사이드는 실제로 CIS 국가에는 설치되지 않도록 하드코딩된 설정을 사용하며, 이는 자국 내 법적 리스크를 피하기 위한 전략이다.

    6. 러시아 키보드 설치 = 일종의 백신?

   많은 악성코드는 해당 언어(러시아어 등)를 감지하면 실행을 중단한다. 과거 REvil(또는 GandCrab)도 시리아 등 특정 국가를 피해갔다.

    7. 단점은?

   이 트릭이 모든 악성코드에 효과적인 것은 아니다. 기본 보안 수칙과 병행되어야 하며, 단지 추가적인 보호 수단일 뿐이다.

    8. 설정 바꾸는 방법

   실수로 언어가 러시아어로 바뀌더라도 윈도우 키 + 스페이스바로 원래 언어로 쉽게 전환 가능하다.

    9. 악성코드 제작자가 언어체크를 무시할 수도 있음

   일부 최신 버전은 언어체크를 생략하기도 한다. 하지만 언어체크를 없애면 해커 입장에서 법적 리스크가 커진다.

    10. 언어 설정은 해커 입장에서 중요한 방패

   러시아의 법적 환경상, 악성코드는 외국 시스템만 타깃팅하도록 제한하고 있으며, 이를 우회하면 법적 위험이 생긴다.

    11. 레지스트리 조작으로도 효과 가능

   키보드 언어 설정 없이도, 윈도우 레지스트리에 해당 국가 언어가 설정된 것처럼 꾸미는 간단한 스크립트도 공개되어 있다.

    12. 수동으로 키보드 언어 추가하는 방법

   윈도우 설정 > 시간 및 언어 > 언어 항목에서 원하는 키보드 언어를 추가하고, 재부팅하면 적용된다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   https://news.hada.io/topic?id=21726

   이 글과 연관성이 있군요

    💬 1. 트릭에 대한 반응

     * 여러 댓글러가 “러시아어 키보드 설치”라는 아이디어에 흥미를 보였으며, 일부는 실제 설치하거나 레지스트리 조작을 시도했다.
     * 어떤 사용자는 14개 언어를 설치했지만, 디스크 공간 낭비 우려를 제기함.
     * 일부는 이 방법을 '러시아 해커를 속이는 쉬운 방어 수단'으로 받아들임.

    💬 2. 우려와 회의론

     * 이 방법은 일시적인 방편일 뿐이며, 해커들이 곧 우회할 수 있다고 우려하는 의견 다수.
     * 예: 키보드 실제 사용 여부, 최근 변경 여부 등을 체크하면 방어 트릭 무력화 가능.
     * “언어 설정으로 보호받으려는 시도는 해커에게 오히려 약점을 드러내는 셈”이라는 비판도 있음.

    💬 3. 대체 제안 및 기술 논의

     * 일부는 가상머신(VM) 환경처럼 보이도록 속이는 것도 하나의 방법이라고 언급.
     * 시스템의 타임존, IP, 키보드 사용 이력 등 다양한 ‘로컬화 확인’ 방법이 악성코드에 의해 사용될 수 있다고 지적.
     * 일부는 Wireshark 같은 보안 툴의 흔적을 남겨 악성코드가 분석 환경으로 오인하게 만드는 방법 제안.

    💬 4. 운영체제에 대한 논쟁

     * Windows의 보안 취약성에 대한 비판 다수. Mac이나 Linux로의 전환을 주장하는 의견도 있음.
     * “Windows는 보안 구멍이 많고, 기업들이 이를 계속 쓰는 건 돈 때문”이라는 불만도 제기됨.
     * 반면, Mac도 완전히 안전하지 않으며, 사용률이 낮아 공격 대상이 덜 될 뿐이라는 반론도 존재.

    💬 5. 그 외 의견

     * 일부는 러시아어 키보드 설치 후 로그인 문제가 발생했다고 보고.
     * 키보드 우회 방식이 보편화되면 해커가 오히려 이를 역이용할 수 있다는 회의적인 시선도 존재.
     * 마지막으로, 어떤 사용자는 “우리가 해커를 피하기 위해 해커를 기분 좋게 해주는 시대가 온 것이냐”는 씁쓸한 반응을 보이기도 함.
"
"https://news.hada.io/topic?id=21780","Show GN: Claude Code History Viewer: 클로드 코드 히스토리 뷰어","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: Claude Code History Viewer: 클로드 코드 히스토리 뷰어

   저는 AI를 돌리면 히스토리를 보면서 어느정도 복기라고 해야할까요 어떻게 이런 결과물을 뽑아냈지 하면서 자주 보게 되는데요

   최근에 클로드 코드를 쓰면서 터미널이나 에디터의 별도 탭으로 분리해서 히스토리 보는게 굉장히 불편하더라구요

   그래서 tauri + react + rust 기반으로 바이브 코딩을 통해 클로드 코드 히스토리 뷰어 데스크탑 앱을 만들었습니다.

   클로드 코드를 설치하면 보통 mac 기준 /Users/{username}/ 위치에 ./claude 폴더에 로그를 저장해두는데요.
   내부에는 projects(레포지토리) 등 4개의 폴더와 projects 하위 세션들을 jsonl 형식으로 로그를 쌓고 있습니다. 그래서 해당 jsonl을 불러와서 적절한 형태로 가공해서 뷰어에 채팅 기록 보듯 히스토리를 볼 수 있습니다.
   터미널에서 기본적으로 보여주는 데이터보다 훨씬 많은 양의 데이터를 보관하고 있는데, 이를 좀더 상세하게 볼 수 있도록 했습니다.
   이외에도 각 프로젝트 또는 세션별로 얼마나 많은 토큰을 썼는지, 하루 토큰 소비량 등 다양한 지표들을 볼 수 있습니다.
   기본 지정 폴더에 ./claude 폴더가 없더라도 수동으로 지정할 수 있습니다 .(dot)이들어간 폴더는 shift + cmd + . 을 통해 임시로 표기할 수 있습니다.

   앱은 URL을 통해 릴리즈 노트에서 dmg 파일을 받아 바로 사용하실 수 있습니다.

   아직 beta 버전이라 안전성이나 버그들이 있을 수 있습니다. 가감없이 피드백 부탁드립니다 ㅎㅎ

   혹시 intel mac도 지원하게 가능할까요..?? 애플 실리콘만 되는것 같네요 ㅜㅜ

   앗 그렇군요.. 칩셋 고려를 안했네요

   빠르게 업데이트하고 알려드리겠습니다!

   감사합니다!

   beta.3 버전에 인텔 칩도 사용 가능한 유니버셜 dmg 을 올렸습니다!

   기존 앱 내에서 업데이트 안내가 보이지 않는다면 번거로우시겠지만 한번 더 릴리즈 노트에서 받아주시면 감사하겠습니다 !!

   넵 현재 다운로드해서 잘 사용하고 있습니다. 감사합니다!

   혹시 나중에는 gemini-cli 같은것도 지원할 계획이 있을까요??

   처음 만들면서 gemini-cli도 생각을 하고 있었는데 확인했을 때 gemini의 경우 유저의 입력만 로그로 쌓고 출력값은 따로 로컬에 적재를 안하고 있더라구요.

   지속적으로 업데이트 사항 보고 가능하면 지원할 생각입니다!

   앗 그렇군요 아쉽네요 ㅜㅜ
   넵 알겠습니다..!

   좋네요 이전 대화 불러오기 기능이 조금 느린 것 말고는 괜찮은 것 같아요

   감사합니다!!
   꾸준히 개선해 나가겠습니다!!

   바라던 앱인데 너무 너무 감사합니다. 잘 사용하겠습니다. :)

   감사합니다 :)

   불편한 점이 있다면 편하게 말씀해주세요!
"
"https://news.hada.io/topic?id=21816","Show GN: Docker 기반 Linux 미러 서버 자동화 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Show GN: Docker 기반 Linux 미러 서버 자동화 프로젝트

   개인용 Linux 미러 서버를 Docker로 간단하게 배포하고 자동 동기화할 수 있는 오픈소스 프로젝트입니다.
     * Arch/Ubuntu 공식 미러에서 rsync로 자동 동기화
     * cron으로 주기적 갱신
     * nginx로 디렉토리 목록 제공
     * Docker로 전체 구성 -> sh 혹은 bat 실행만으로 바로 구동

   원래 Arch 기반의 배포판을 만들다가,
   자체 미러 서버가 있으면 좋겠다고 생각해서 만들어 봤습니다.
   Docker와 Linux 기본 지식만 있다면 누구나 쉽게 확장하거나 수정할 수 있습니다.

   Github: https://github.com/LunaStev/mirror-docker

   rocky나 fedora도 가능할까요?

   여러 일이 많아서 이제 봤네요. 네 쉽게 확장이 가능해서 구조만 파악한다면 쉽게 가능할 겁니다.

   우분투 기반의 배포판 만드는 것은 별도의 GUI 툴들이 있는 것으로 아는데
   Arch기반의 배포판을 만드는 것은 어떻게 만드나요?

   이것도 스크립트 파일로 만들 수 있는 것인지 정보 공유 좀 부탁드립니다.

   Arch 진영에는 archiso라고 공식적인 iso를 만드는 툴이 있는데 이걸로 만들더라구요.
   제가 만들다가 말았지만 https://github.com/LunaStev/noxis 여기에 보면 대충 이렇게 구성이 되어있습니다.
"
"https://news.hada.io/topic?id=21759","C에서 타입 세이프(Generic)한 자료구조 작성법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     C에서 타입 세이프(Generic)한 자료구조 작성법

     * 이 글은 C 언어에서 타입 세이프(Generic)한 자료구조를 만드는 새로운 방법을 설명함
     * union을 활용해 타입 정보를 자료구조에 연관시키는 기법으로, 링크드 리스트 구현을 예시로 설명함
     * 기존 C 제네릭 패턴(매크로, void 포인터, Flexible Array Member)과 차별점과 각 방식의 단점을 비교함
     * 컴파일 타임 타입 체크가 가능해서 잘못된 타입 사용을 미리 막을 수 있음
     * 새로운 기법은 foo_list와 같이 명확하고 일관된 함수/자료구조 인터페이스를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * C 언어에서 제네릭 자료구조를 타입 안정성 있게 만드는 방법에 대해 소개함
     * 이 기법은 union을 사용해 타입 정보를 컴파일 타임에 자료구조에 연결함
     * 지도(Map), 배열, 바이너리 트리 등 모든 자료구조에 적용할 수 있으며, 예시로 기본 링크드 리스트 구현을 통해 설명함
     * 많은 개발자들이 C에서 제네릭이 불가능하다고 생각하므로, 단계별로 쉽게 설명을 진행함

전통적인 매크로 기반 제네릭

     * C에서 제네릭 자료구조 구현의 전통적 방식은 매크로를 이용하여 구조체와 함수의 이름, 타입을 생성함
     * 자료구조 헤더를 여러 타입에 대해 여러 번 include하는 식으로 확장함

   예시:
     * 타입에 맞는 구조체와 함수명을 생성하기 위해 매크로(예: CONCAT, NODE_TYPE, PREPEND_FUNC) 사용
     * 각 타입별로 함수와 구조체가 따로 생성되어, int와 Foo 같은 타입 각각 별도의 자료구조 정의가 나옴

   단점:
     * 타입 및 함수 정의 위치 파악이 어려움 (매크로로 생성되어 추적이 힘듦)
     * 코드 자동완성 기능 활용이 어렵고
     * 동일 함수 여러 개 생성으로 바이너리 크기 및 빌드 시간 증가
     * 함수 이름에 타입 접두사 필요 (예: Foo_list_prepend)

제네릭 단계 1: void 포인터 방식

     * 자료구조의 데이터 타입을 *void 로 두어 타입에 무관하게 만듦
     * 링크드 리스트의 data 필드를 void *로 선언
     * 타입 체크가 불가능하므로 런타임에 타입 오류가 발생할 수 있음, 컴파일 타임의 안전성 낮음
     * 메모리 및 캐시 활용 비효율: 노드와 데이터가 따로 할당되어 불필요한 오버헤드 및 캐시 미스 증가

제네릭 단계 2: 인라인 스토리지(Flexible Array Member)

     * Flexible Array Member(유연 배열 멤버) 를 활용, 포인터 저장 대신 데이터 자체를 노드에 함께 저장
     * 노드 하나당 한 번의 할당으로 충분하며, 캐시에 데이터와 next 포인터가 인접하게 위치함
     * 이 방식은 memcpy와 같은 크기 정보 전달이 필요하나, 일관된 메모리 배치로 성능이 개선됨
     * list_alloc_front 함수를 이용하면 memcpy 없이 구조체를 직접 초기화할 수 있음

제네릭 단계 3: 타입 체크 구현

     * union의 payload 멤버에 파라미터화된 타입 포인터를 선언하여 컴파일 타임에 자료구조에 타입 정보 추가
     * 예) #define List(type) union { ListNode *head; type *payload; }
     * 이렇게 하면 typeof(foo_list.payload) 로 해당 리스트의 타입을 얻을 수 있음
     * 매크로(list_prepend)에서 함수 타입 캐스팅을 통해, 올바른 타입이어야만 컴파일 가능
     * 잘못된 타입 사용 시 컴파일 타임에 에러 발생

   에러 예시:
     * foo_list에 int를 추가 시, 'incompatible integer to pointer conversion' 컴파일 에러 메시지가 출력됨

typeof 미지원 컴파일러 대응

     * C23 이전까지 __typeof__는 표준이 아니므로 일부 컴파일러(예: 구버전 MSVC)에서는 동작하지 않음
     * struct 내 payload 멤버 활용 등 우회방법으로 비슷한 효과 가능

파라미터 전달 및 typedef

     * 동일한 형태의 List(Foo)도 컴파일러는 서로 다른 타입이라 판단함
     * typedef 사용 시, 매개변수 전달 및 대입이 원활해짐

   예시:
     * typedef List(Foo) ListFoo;
     * ListFoo 변수 선언 및 함수 매개변수로 사용 가능

마무리 및 다양한 자료구조 확장

     * 이 기법은 여러 타입 파라미터가 필요한 자료구조(예: 해시맵)에도 활용 가능
     * union을 통해 key, value 각각의 타입 안전성을 보장 가능
     * 더 자세한 실습 및 매크로 구현체는 관련 코드 gist 링크 참고

결론

     * 새로운 방식은 기존 방식의 단점(가독성, 빌드 효율성, 유지보수성)을 극복하고, 일관성 있는 함수 명명 체계와 타입 안전성 제공함
     * 여러 자료구조 및 복수 타입 파라미터 지원이 용이함
     * 컴파일 타임 타입 체크를 통해 제네릭 자료구조 사용의 안전성과 효율성을 동시에 확보함

감사의 말

     * 이 글은 Martin Fouilleul의 피드백 및 격려를 받아 완성함

   간단하게 Zig 쓰면 되는거 아닌가? 하는 의문점이 들긴 합니다

        Hacker News 의견

     * 2단계 코드에서 uint64_t data[]; 사용 방식은, 정렬 요구사항이 uint64_t보다 큰 타입에는 적합하지 않고, 반대로 작은 타입엔 불필요하게 낭비라는 점 지적, 예를 들어 64비트 아키텍처의 ilp32 ABI에선 더 문제가 됨. 3단계 코드에선 int main() { List(Foo) foo_list = {NULL}; 이렇게 해야 한다는 의견 전달. typeof가 없는 상황에선 반환값을 돌려주지 못하며, 대체 코드의 경우 const 관련 오류가 발생할 수 있고, == 연산자 대칭성 때문에 이런 문제가 부각됨. payload를 빼면 크기 정보가 없어 안전하지 않음, 예를 들어 List(int64_t)에 int32_t를 추가하면 괜찮을 것 같지만, 실제 int32_t의 크기 판단 불가. 더 안전하게 하려면 보완이 필요하다는 이야기. C에서 제네릭 사용에 있어 두 가지 큰 한계가 있다고 생각, 첫째 vtable 위임 방식이 구조체가 매크로를 포함할 수 없어 기능 제한, 둘째 외부
       vtable로 위임할 땐 사용할 타입 전부를 미리 선언해야 한다는 점. 가장 좋은 방법은 typedef 선언이 들어있는 헤더에서 정적 함수만 선언하는 것인데, GCC와 Clang이 각기 다르게 undefined static 경고를 내는 타이밍이 다르다고 부연 설명. 마지막엔 서로 다른 버퍼 구조체를 받는 함수 설계 사례를 예로 들며, const 버전까지 전부 관리가 필요하다는 점 강조
          + 외부 vtable 위임 이슈에 대해, 예전 프로젝트에서 직접 이걸 해결하기 위해 컴파일러까지 만든 경험 공유, 아파치 Clownfish 프로젝트에서 시작할 때 .h 파일 파싱하다가 결국 클라운피시 헤더(.cfh)라는 자체 포맷을 만듦. 실제 obj의 ""Clone"" 메서드를 호출하는 코드를 예시로 보여주며, 객체 지향 기능이 필요한 동적 언어 바인딩을 위해 이런 무리한 코드를 대량 생성해야 했던 경험 소개. Clownfish의 목적이 최저 공통 객체 모델 제공이었고, 바인딩 언어 타입도 .cfh에서 생성함. 이런 복잡성 때문에 대다수가 void* 캐스팅으로 타입 안전성을 포기하고 있다고 덧붙임. https://github.com/apache/lucy-clownfish
          + int main()에 관해, C에서 int main()은 인수 개수 미정임을 뜻함. int main(void)로 선언해야 인수가 없다는 뜻이 됨. 많은 C++ 작성자들이 자주 잊는 사실이라고 강조
          + union이 연합(union)되는 구조를 기대, 즉 한 타입이 다른 타입의 union 일부로 자신을 선언 가능했으면 좋겠다는 의견
          + malloc 할 때 내부 padding 문제 때문에 계산된 크기가 실제보다 작을 수 있다는 점 지적, 예를 들어 malloc(sizeof(*node) + data_size); 같이 했을 때 위험성 제기
     * 트릭#0 내용을 반박, 자신이 C의 전체 방언을 만들 때 이 트릭을 사용했음. 예를 들어 generic binary heap 구현 예시 코드 공유 https://github.com/gritzko/librdx/blob/master/abc/HEAPx.h. 문법은 다소 무겁지만 최종적으로는 일반적인 C 구조체가 되어 최적화와 예측 가능성에 큰 이득이 있음. 다른 구현에선 void*와 런타임 메모리 사이징, 매크로 정의가 불가피하다고 봄
          + 글쓴이로서, binary heap과 linked list는 목적이 다름을 설명. binary heap은 저장 시 데이터를 읽어야 하기 때문에 접근이 다르며, generic binary heap 작성 시엔 선택이 다를 수 있음을 밝힘. 본문 각주에서도 언급했다고 부연
          + 헤더 구현 선호 이유 여러 가지 제시. 디버깅할 때 매크로 함수보다 코드 추적 및 타입 정보 활용이 쉬움. 컴파일러가 각 인스턴스마다 monomorphized 최적화가 가능해 런타임 비용이나 가변 크기 부담 없음. 스택에 generic 구조체를 둘 수 있음. 저자가 언급한 문제점 두 가지는 우회 가능, 함수명 매크로로 이름을 쉽게 바꿀 수 있고, weak symbol 활용해 링크 시 이중 정의를 자동으로 합칠 수도 있음. 포인터 타입 generic 컨테이너엔 또 다른 문제가 있지만, typedef 등으로 해결 가능. C에서 intrusive data structure가 여전히 편하나, 디버깅은 어렵다는 생각
          + ""컴파일러가 도넛 먹듯 먹는다""는 표현에 크게 웃음 터짐
     * 함수 타입 변환 시, 예를 들어 Foo와 void의 내부 표현이 동일하다고 가정하는데, 표준 C에서는 보장되지 않음. 타입 간 호환성(""compatible"")이 없는 상황에서 이런 캐스팅은 정의되지 않은 동작으로 이어질 수 있음. 컴파일러가 alias 분석 등에서 영향을 받을 수도 있다고 (참고 링크 동봉) https://news.ycombinator.com/item?id=44421185
          + 본문 각주에 언급되어 있고, 캐스팅이 타입 안전성 핵심은 아니라는 주장. 전체 글을 읽어보길 권함
     * ""C에서 generics 쓰려면 왜 이런 무리수를 두는가, 그냥 C++ 쓰면 되는 것 아닌가""라는 질문
          + 안전 기준과 기타 요구 때문에 레거시 프로젝트에서는 C++로의 마이그레이션이 당장 불가능한 경우가 많다는 경험 공유. 새로운 프로젝트는 표준을 정해 C++을 도입하지만, 기존 프로젝트는 당분간 C를 유지해야 한다고 봄. 단순히 ""그냥 C++ 쓰지""라는 시각이 좀더 맥락에 따뜻했으면 한다는 의견
          + 실제로, C를 쓰는 현장에선 C++로 전환하는 게 더 복잡하고 문제를 많이 야기할 수 있음
          + 반대로, 약간의 노력만으로 C에서 같은 결과를 얻을 수 있는데 굳이 C++까지 갈 필요가 없다는 입장 전개
     * 리눅스 커널에서 실제로 사용하는 방식 소개, 리스트 정보를 담은 struct list_head를 타입별 구조체 안에 포함시키는 패턴. 관련 참고 링크 제공 https://kernelnewbies.org/FAQ/LinkedLists
          + 리눅스 커널의 LIST_HEAD_INIT, INIT_LIST_HEAD 라는 매크로 명칭이 직관적이지 않다고 느낌
     * ""typeof on old compilers"" 섹션 코드에서 (list)->payload = (item);는 실은 no-op이 아니라, 리스트 헤드가 item으로 덮어써진다는 지적. 예상한 동작이라면 if(0)로 감싸야 하다는 제안
          + 예제에서는 union을 struct로 바꿨던데, 이 역시 낭비로 보임. if(0) 안에서 처리하는 편이 더 나아 보인다는 의견
     * D 언어에서는 이런 generic list 구조가 훨씬 간단함을 보여주며, C의 프리프로세서는 마치 손톱에 망치질하는 느낌, 못질에는 Nail gun이 훨씬 빠르고 깔끔하다는 은유로 C 매크로의 불편함을 강조
          + 해당 게시글은 C에 대한 것이고, 일부 프로젝트에서는 반드시 C를 써야 한다는 입장 밝힘
     * union과 typeof()를 활용하는 아이디어가 흥미로움. 본인의 경우 intrusive 자료구조에서는 결국 대형 매크로로 감싸는 래퍼가 필요하다고 느꼈고, union과 typeof로도 이런 구현이 가능한지 의문 제기. 예시로 hash table 래퍼 구현 코드와 문서 링크 공유 https://github.com/FRRouting/frr/blob/master/lib/typesafe.h#L823-L971 https://docs.frrouting.org/projects/dev-guide/en/latest/lists.html
     * 개인적으로 이미 실험적 라이브러리에서 이 기법을 쓰고 있다는 공유 https://github.com/uecker/noplate/blob/main/src/list.h
          + intrusive 구조체, 즉 데이터에 노드 구조를 포함해서 객체가 여러 컨테이너에 동시에 속할 수 있는 방법에 대한 아이디어를 문의
     * 함수 포인터의 타입을 활용해 타입 안전성을 확보한다는 개념이 핵심으로 보임, 흔히 사용하는 핸들 타입 대신 이렇게 구현. C23 표준에서는 타입 호환 문제가 개선됐으며, 해당 표준 문서와 최신 GCC/Clang 지원 상황 공유 https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3037.pdf
          + 글쓴이로서, 핵심 아이디어는 union을 이용해 generic 데이터 타입에 타입 정보를 연동하는 것임을 강조, 함수 캐스팅만이 방법이 아니라 여러 대안을 논의했고, footnote와 ""typeof on old compilers""에도 자세히 다뤘음을 언급
"
"https://news.hada.io/topic?id=21729","IPv4 연결 없이 인터넷 사용하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          IPv4 연결 없이 인터넷 사용하기

     * IPv4 연결이 중단된 상황에서도 IPv6와 WireGuard, Hetzner VPS를 통해 전체 인터넷 사용이 가능해짐
     * Carrier Grade NAT(대규모 NAT) 로 인해 IPv4만 장애가 발생했으나, IPv6는 영향을 받지 않았음
     * WireGuard 터널을 설정하여 VPS를 통해 IPv4 트래픽을 터널링함으로써 정상적인 웹사이트 이용 환경을 복구함
     * 네트워크 네임스페이스와 Docker 활용법, 그리고 MTU 이슈 해결 방법도 나옴
     * Linux 환경과 오픈소스 도구 덕분에 복잡한 네트워크 문제를 스스로 해결하는 경험을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   며칠 전, 필자는 정전 이후 집의 라우터에서 IPv4 접속이 끊기는 문제를 겪었음. 다행히 IPv6 접속은 정상이라 일부 웹사이트(Google, Meta 등)에는 접속이 가능했으나, 대다수 사이트(GitHub 등)는 접속할 수 없는 상태였음. 이에 Linux, WireGuard, Hetzner VPS를 활용하여 IPv6만으로 전체 인터넷 이용 환경을 복구한 과정을 정리함.

장애 원인과 배경

  네트워크 환경 이상 발견

     * 정전 후 복구 과정에서, IPv6 서버는 접속이 정상이나, IPv4 서버는 접속 불가 현상 확인
     * 진단 명령어(ping -6, traceroute) 실행 결과도 IP 버전에 따른 차이만 드러남
     * 문의 결과, 통신사 측 CG-NAT(Carrier Grade NAT) 계층에서 IPv4 변환에 문제가 발생한 것으로 판명됨
     * ISP 측 정비까지는 며칠 소요 예상으로, 직접 해결이 필요해짐

  NAT와 CG-NAT 설명

     * NAT(Network Address Translation) : 여러 장치가 하나의 공인 IPv4 주소를 공유하게 해주는 방식임
          + 라우터에서 내부 IP를 공인 IP와 고유 포트로 치환해 트래픽을 관리, 역방향 전송 시 매핑 정보로 내부 목적지 복원
          + 이 구조로 인해 암묵적인 방화벽 역할과 포트 포워딩의 필요성이 생김
     * CG-NAT(Carrier Grade NAT) : ISP가 각 가정용 라우터를 대상으로 다시 NAT를 한 번 더 적용하는 계층 구조
          + IPv4 주소 부족 문제로 ISP 내부적으로 NAT를 중첩 적용함
          + CG-NAT 환경에서는 포트 포워딩 등 서비스 제공이 더욱 제한적임
          + IPv4 트래픽만 장애가 발생한 이유가 바로 이 계층 내 IPv4 패킷 처리 문제 때문임

  IPv6의 장점

     * IPv6 주소 공간이 압도적으로 커 NAT 없이도 각 기기에 직접 주소 할당 가능
          + 대부분의 가정용 라우터에는 /64 서브넷이 할당되어 수조 개의 주소 사용 가능
     * 별도의 NAT가 불필요해 직접 통신 가능하지만, 방화벽 설정은 더욱 중요해짐
     * CG-NAT은 IPv4에만 적용되기 때문에, 본 사고에서는 IPv6만 정상 동작함
     * 하지만 아직 많은 서버(예: GitHub)는 IPv6만으로는 접속 불가임

WireGuard 터널로 IPv4 복구

  구현 개요

     * Hetzner VPS(IPv4/IPv6 스택 모두 지원) 와 WireGuard를 활용하여 IPV6로만 인터넷 연결이 된 상태에서 전체 인터넷 접속 가능 환경 구축
          + VPS에서 WireGuard 서버 운영 및 클라이언트 장비와 터널 구축
          + 트래픽은 IPv6를 통해 VPS로 우회되어, VPS를 경유해 전체 사이트(IPv4 포함)에 접근 가능함
          + Dual-Stack Lite와 유사한 원리

  서버 및 클라이언트 구성 예시

     * WireGuard 서버 구성에서 IPv4/IPv6 트래픽 모두 처리
          + MASQUERADE(동적 IP 변환), SNAT(고정 IP 변환) 규칙 예시 포함
          + 직접 할당된 글로벌 IPv6를 활용해 NAT 없이도 바로 WireGuard 피어 연결 가능함
          + PostUp/PostDown 항목을 여러 번 적어 각 명령 순차 실행 가능
     * 클라이언트 설정 예시
          + 직접 할당된 IPv6 주소 또는 NAT된 ULA 조합 예시 설명
          + AllowedIPs에 0.0.0.0/0, ::/0 적용으로 전체 트래픽 터널링 가능
          + 구글 DNS(IPv4, IPv6) 및 MTU 설정 방법 제공

  터널 정상 작동

     * 양쪽 WireGuard 구성 완료 후, IPv4/IPv6 모두 정상 터널링
     * 부인 PC에도 동일 방법 적용하여 Linux WireGuard 클라이언트 간편 설치 진행
     * 단, 사내 VPN과 동시 연결은 일반적으로 불가하므로 추가 네트워크 namespace 사용 필요

네트워크 네임스페이스와 Docker

  기능 및 활용법

     * vopono 등 도구로 애플리케이션별 네트워크 네임스페이스 생성, 해당 namespace에서 VPN 또는 WireGuard 인터페이스 직접 지정
          + 별도의 MASQUERADE 규칙 지정 필요, 내부 트래픽을 WireGuard 터널로 강제 전환
          + 외부와 격리된 DNS, gai.conf(IPv4 우선 DNS 설정) 등 설정 팁 포함
     * 네임스페이스 내부에서 VPN 접속 및 애플리케이션 실행 방식 구현 예시
          + 같은 네임스페이스에서 여러 서비스 실행으로 네트워크 충돌 미연 방지

  Docker 컨테이너와의 조합

     * Docker 데몬은 기본적으로 호스트 네트워크 소켓을 사용하므로 일반 네임스페이스 실행만으로는 접근 불가
          + mount namespace, /sys 바인드 마운트 등 Unix 가상화 기법으로 workaround 제시
          + Dockerd를 네임스페이스 내부에서 실행, 별도의 소켓과 데이터 루트 지정해 컨테이너 내 인터넷 연결 복구
          + 복잡한 네트워크 브리지 환경에는 추가 세팅 필요 가능성 언급

WireGuard MTU(MTU: 최대 전송 단위) 이슈

     * WireGuard 연결 후 일부 웹사이트만 접속불가(SSL 등), ping은 정상 응답되는 현상 발생
     * 다양한 크기의 ping 테스트로 MTU가 너무 높아 큰 패킷이 도중에 드롭되는 원인을 규명
          + MTU를 1280으로 낮춰 문제 즉시 해결
     * MTU란, 한 번에 전송 가능한 최대 패킷 크기를 의미함
          + 터널/캡슐화 오버헤드 감안해 적절한 MTU 설정 필요, 그렇지 않으면 대용량 패킷 전송 시 연결 장애 발생
          + IPv6는 규격상 최소 MTU가 1280임

결론 및 활용 조언

     * WireGuard VPN 서버 구축, 네트워크 네임스페이스 관리, Docker 특수 환경 설정, MTU 트러블슈팅 등 Linux와 오픈소스 도구를 활용한 네트워크 문제 자가 진단 및 해결 경험 강조
     * Hetzner VPS 등의 서비스는 가격대비 안정적이며 WireGuard 등 합법적 네트워킹 서비스 운영이 용이함
     * 오픈소스 라우터 펌웨어(OpenWRT)와 Linux 네트워킹 기법의 가치 재평가
          + 직접 네트워크를 관리/수정할 수 있는 유연성이 원격 업무 환경에서 큰 이점을 제공
          + 충분한 이해와 연습이 있다면 복잡한 네트워크 장애도 스스로 해결 가능함

참고 자료

     * Tailscale – How NAT Traversal Works
     * ArchWiki – WireGuard use case 예시
     * Unix StackExchange – 네임스페이스 내 Docker 트릭
     * AskUbuntu – DNS 우선순위 설정

   (관련 스크립트, config, 팁, 참고 링크 원문 참조)

        Hacker News 의견

     * 제목이 약간 오해를 줄 수 있는 느낌인데, 사실 이 글은 “IPv6 터널을 통해 VPS로 연결해서 IPv4 인터넷에 접속하기”에 더 가깝다는 내용임, 흔히 4in6이라고 부름
       어쨌든 흥미로운 방식임
       우리 ISP에서 IPv4에 장애가 나면 바로 전체가 다운되어 지원 이슈가 비교적 단순하게 생기는데, IPv6에 장애가 나면 부분적으로 이상 동작하고, 느린 연결이나 간헐적 불통 등 이상하게 나타난다는 경험임
       특히 게이트웨이가 IPv6 있다고 착각하면 사용자 입장에서 문제가 더 골치아픈 형태로 몇몇 기능만 작동하지 않는 등으로 보임
          + 최근에 IPv4가 잠시 안 됐을 때 Github가 안 돼서 그제야 알게 되었음
            요즘은 대부분의 소비자 웹사이트가 IPv6로는 정상 동작함
            다만 라우터가 오직 IPv4 DNS만 제공하는 분들은 완전히 인터넷이 끊어지는 문제가 있었음
            Microsoft가 좀 더 신경 썼으면 좋겠다는 생각임
            IPv4가 돌아오는지 확인하려면 라우터에 할당한 mDNS 호스트네임을 기억해야 했던 점도 있음
          + 솔직히 IPv4가 끊긴 적이 집에서 있었는데, 아내는 눈치도 못 챘음
            Google, Facebook, Apple/iCloud, CloudFlare 기반 서비스 거의 대부분이 IPv6로 잘 동작하는 상황임
          + 내 경험도 비슷함
            IPv6 문제는 정말 원인 파악과 재현이 힘들고 “내 컴퓨터에서는 잘 되는데?”만 반복됨
          + 대부분 ISP가 여전히 IPv6를 차단하는 상황인데, 소기업들도 IPv6 시도만 하고 AAAA 레코드 같은 걸 잊어버리는 경우가 많음
            그래서 사용자들은 친구네 집이나 카페 등 저가형 ISP 환경에선 뭔가 되는데 자기 집에선 안 되는 상황이 생김
            이상하게 들릴 수 있는데, 특별한 좋은 해결책이 있는 것 같진 않고 그냥 IPv4가 사라지길 바라는 게 현실
            Happy Eyeballs 같은 기법(동시에 IPv4/IPv6 접속 시도하고 더 빠른 쪽 선택)도 있지만 실제로는 응용 프로그램 단에서 문제가 더 많이 발생하고, 그걸 해결할 일반적인 방법이 부족함
            나 같은 경우엔 네트워크에서 IPv6는 활성화하고, 브라우저에선 IPv6 DNS를 끄는 식의 타협책을 쓰는데 만족스럽진 않음
     * IPv6를 써보고 싶지만 ISP가 제공하지 않는 경우, Hurricane Electric(HE)에서 무료로 터널 서비스를 아주 오래전부터 제공함
       관련 정보 링크로는 tunnelbroker.net, ipv6.he.net, Fedora 설정법, Brandon Rozek 블로그, DD-WRT 설정법, Mikrotik 포럼 자동업데이트 스크립트, RockyLinux 가이드 같은 다양한 셋업 방법 있음
          + 한 가지 주의할 점이 있는데, 스트리밍 서비스를 쓸 때 이 터널을 막는 경우가 많음, 마치 VPN 우회로 인식해서 지역 제한 컨텐츠 차단 때문에 발생함
            그래도 RA(라우터 광고) 기능 덕분에 어떤 네트워크 장치도 /64 단위로 IPv6 네트워크를 브로드캐스팅해서, 라우터가 HE 터널을 직접 지원하지 않아도 네트워크 내 여러 기기들이 IPv6 주소 사용 가능함(단, 라우터가 보안상 RA를 필터하지 않을 경우)
            집에서 무언가 직접 서비스하려고 할 때 포트포워딩 없이 IPv6만으로 할 수 있어서 매우 편리함
          + Hurricane Electric 서비스는 좋지만, 이제 ISP가 IPv6를 기본 제공하는 일이 점점 많아지면서 일반 사용자는 터널 서비스에서 이탈 중임
            그리고 일부 네트워크 서비스들이 he.net 터널을 남용 또는 악용으로 보고 막아버리는 경우가 많아져서, 결국 내 네트워크에선 대부분 IPv6 사용을 중지해야 했던 상황임
          + 참고할 점, Hurricane Electric 터널은 반드시 공인 IPv4 주소를 ISP로부터 받아야만 동작함
            만약 캐리어그레이드 NAT 등 NAT 환경에 있다면, IPv6를 집에 도입하려면 이 방법 대신 다른 솔루션을 찾아야 함
          + HE의 무료 6in4 터널을 OpenBSD로 5년째 쓰고 있는 “고객” 경험임
            /etc/hostname.gif0 파일 등으로 네트워크 설정만으로 꾸준히 잘 동작함
            AWS에서 IPv4없이 일부러 구성한 VPS 클러스터와의 통신에도 씀
            AWS가 IPv4 주소 비용을 적극적으로 부과해서 이 방법이 비용 절감에 매우 큰 도움이 된다는 생각임
     * 만약 진짜 v6 only 환경에서 v4 접속이 필요하다면, 공개 DNS64+NAT64 Gateway를 쓰면 손쉽게 해결 가능함
       nat64.net의 공개 Provider 리스트 참고
       평소에는 DNS 서버만 바꿔주면 가능함
       DNS64는 AAAA 레코드가 없는 사이트에 대해 NAT64 박스에 연결될 수 있도록 AAAA 레코드를 합성해서 제공함
       그러면 NAT64가 트래픽을 받아서 프로토콜 변환 + NAT를 해줌
       실습 예시로 dig나 curl 명령어 써서 바로 github 같은 곳에 접속 가능함
          + 유럽에선 nat64.net을 직접 써도 꽤 쾌적하게 동작함
            실제로 좋은 경험만 있었음
          + Cloudflare WARP를 쓰면 훨씬 빠른 속도를 체감할 수 있음
            WARP를 통해 IPv4 주소에 직접 접근도 가능함
     * 가끔 IPv6-only 사용자가 있다는 게 신기함
       예전에 반대 상황(IPv4-only 환경에서 IPv6 접속 필요)에서는, 서버에 완전한 제어권이 있다면 아주 빠르게 쓸 수 있는 해결책으로 SOCKS5 프록시(ssh -D 옵션) 사용이 최고였음
       브라우저만 socks 프록시로 지정하면 바로 활용 가능함
       시스템 전체로 하면 오히려 ssh 연결이 끊길 수도 있어서 그건 걱정되는 부분임
     * 나와 비슷한 상황임
       2주 정도 IPv4 장애 관련 티켓이 열려있는데 “곧 기술자가 처리한다”는 답변만 반복됨
       IPv6는 정상이라 전체 장애로 보지 않는 듯함
       독일에선 이런 경우에 소비자 보상 관련 법규가 있지만, 이번 케이스가 해당되는지 확인할 예정임
       블로그 글에서 제안하는 방식의 문제는 데이터센터 IP 대역이 여러 서비스에서 차단되거나 캡차 같은 우회를 요구하거나 VPN 업체 IP처럼 다뤄지는 거라, 피할 방법이 별로 없음
       내 경우 집 전체에서 해결해야 해서 라우터에서 Wireguard로 라우팅 및 NAT 규칙을 세팅했는데, Ubiquiti EdgeRouter 같은 오픈형 장비라 다행이었음
       만약 FritzBox였다면 이 작업이 훨씬 어려웠을 것 같음
       다만, 라우터 성능이 부족해 연결량이 많으면 느려지는 단점 있어서 하드웨어 오프로딩 지원되는 IPSec으로 교체해야 고민 중임
          + FritzBox도 VPN 연결을 위한 매우 훌륭한 GUI 설정 과정을 제공함
            FritzBox to FritzBox가 기본 전제이나, 호환되는 VPN이면 OK임
            고정 IPv4/IPv6 라우트 설정도 제공함
            가장 큰 문제는 맞은편에서 어떤 IPSec 암호화 설정을 요구하는지 파악하는 것인데, Wireguard는 더 쉽지만, 반대로 하드웨어 가속 문제가 있음
            필요하다면 FritzBox 전체 설정을 백업해 직접 편집 후 체크섬만 다시 계산해서 재입력하는 테크닉도 있음
            AVM은 사용자에게 노출되지 않는 엄청난 양의 세부설정을 숨겨놓았는데, 의도적임
            실수로 라우터를 망가뜨리지 않게 일부러 좀 어렵게 해둔 면이 있음
          + 독일 상황은 잘 모르겠지만, 네덜란드에선 고정+모바일 인터넷 모두 같은 ISP를 쓴다면, 유선망에 장애 났을 때 무료 모바일 데이터를 요청할 수 있음
            가능하다면 ISP에 해당 옵션 문의하는 걸 추천함
     * 오랜 시간이 지나도 모든 장비와 가정용 랩을 IPv6로 바꿔야 할 명확한 이유를 못 찾겠음
       포트포워딩과 방화벽 설정이 그나마 직관적이고, IPv6로 바꿀 땐 몇 주씩 문제해결, 방화벽, 주소 재설정 등 복잡함이 예상됨
       내가 뭘 놓치고 있는지 궁금함
          + 현실적으로는 지금 단계에선 놓치고 있는 것은 거의 없음
            앞으로 Google, Cloudflare 등 대기업들이 계속 늘어나는 IPv4 주소 비용을 감당 못 하게 되면 IPv6로 인센티브를 줄 가능성도 있음(예: IPv4 연결은 속도 제한 등)
            AWS도 예전에는 미사용 IPv4 Elastic IP만 요금 부과하더니, 이제는 사용중이어도 무조건 요금 부과함
            향후 게이트웨이나 라우터를 업그레이드할 때 IPv6를 그냥 켜두는 게 좋을 듯하고, 지금은 IPv4/IPv6 듀얼로 쓰면 기존 장비와 기존 서비스 문제없이 작동함
            IPv6 자동주소할당 관련해서는 역사적으로 방식이 뒤죽박죽이었으나 SLAAC로 정착되는 분위기이고, ISP 중심에선 DHCPv6를 상당히 오래 사용할 전망임
          + 사실 그렇게 어렵진 않음
            특별히 복잡한 홈네트워크가 아니라면 저녁 잠깐만 투자해도 IPv6 세팅 가능함
            Comcast 기준으로 라우터에서 IPv6 옵션 켜면 ISP에서 prefix 받아오고, 이것만으로도 자동으로 네트워크에 광고되어서 원하는 포트만 방화벽 열어주면 끝임
          + 놓치는 부분 없음
            엔터프라이즈 환경에서는 IPv6 도입 플러스보다 단점이나 복잡성이 큼
            나는 약 3500대 장비, 7개 건물, 2개의 10Gbps, 1개의 4Gbps WAN, 그리고 26개 공인 IPv4 주소 관리
            지금까지 IPv6 써야만 하는 이유가 전혀 없음
            듀얼스택으로 운영하면 네트워크 불필요한 부하와 복잡성 발생함
            오히려 최근에 고정 IPv6 주소블록을 받으려 2번 신청했는데 계속 거절당함
            실질적으로 이득은 없고, 심지어 할당받기도 어려운 상황임
            ARIN IPv6 최초 신청 가이드 내용을 보면,
            → IPv4 할당 보유
            → IPv6 멀티홈 즉시 예정
            → 1년 내 13개 엔드사이트(오피스 등)
            → 1년 내 2000개 IPv6 주소
            → 1년 내 200개의 /64 서브넷
            중 하나라도 충족해야 신청 대상임
     * Apple App Store 정책 중 모든 앱이 IPv6-only 네트워크에서 동작해야 한다는 요구사항은 정말 높이 평가함
       개발자 입장에선 첫 경험에 놀랄 수 있는데, 소비자 입장에선 이런 요구가 매우 반가움
          + 하지만 이 정책이 서버단에서 IPv6 주소 보유를 필수로 요구하진 않음
          + 그러면 앱으로 github가 v6에도 접속 가능한지 궁금함
     * 회사 업무에서 내부 인프라 접근을 위해 IPv6 only VPN을 여러 개 운영 중임
       가장 큰 문제는 Windows, macOS 클라이언트가 반드시 v6 DNS 서버가 필요하다는 점임
       클라이언트가 v6 지원 네트워크에 있을 수도 있고 아닐 수도 있기에, VPN 내부에서 직접 DNS 서버를 돌린 뒤 이를 클라이언트에 자동 전달해주지만
       VPN 연결이 끊어지고도 Wireguard 앱이 원래 DNS로 복구해주지 못해서 다양한 문제가 발생함
          + 나는 ISP의 IPv4 only 네트워크와 macOS 환경에서도 별도의 DNS 없이 IPv6-only를 잘 활용한 적 있음
            정확한 방법은 기억 안 나지만, macOS가 v6 주소만 부여해도 문제 없이 동작
            ULA 주소를 호스트에 지정하면 되고, 이건 유저가 방법만 알면 쉬운 일임
            VPN 앱이 직접 IPv6-only 네트워크에 ULA를 추가해주는 스크립트를 활용할 수 있음
            단, 만든 ULA 주소를 무작정 방치하면, 사용자가 다른 v6 네트워크로 이동할 때 문제가 될 수 있음
     * 동일 상황 겪는다면 ssh 프록시(ssh -D 8080 user@hostname)로 손쉽게 socks 프록시 환경 구축 가능함
       이 연결 후 브라우저 프록시 주소를 localhost:8080으로 지정하면 됨
          + 나도 똑같이 조언하려고 했음
            임시 문제 해결용으로 매우 간단하면서, 필요할 땐 상시툴로 써도 훌륭함
            단, sshd_config에서 “AllowTcpForwarding”이 활성화되어야 함
          + 나는 공공 와이파이 쓸 때 항상 이 방법을 씀
            VPN 서비스 비용 안 내도 되고, 신뢰할 필요 없이 내 infomaniak 서버에서 socks 프록시로 보내면 됨
     * 개인적으로 IPv4에 대해 불만이 많고, 특히 내 ISP가 강제로 IPv4만 제공해서 더 답답함
       IPv6 도입이 이렇게 더딘 것은 기술업계의 큰 실패로 봄
       누가 책임져야 할지 고민임
       라우터 제조사가 허접한 펌웨어를 만드는 건지, ISP의 IPv4 추진 리더십 문제인지, IPv4 주소 투기꾼 때문인지, 네트워크 엔지니어 및 지원 인력 교육 부족 탓인지 등 원인에 대한 근본 논의가 더 필요하다고 생각함
       인터넷이 TLS 1.0에서 비교적 잘 전환된 것처럼, IPv4도 넘어갈 수 있어야 하지 않나 싶음
       레거시 코드를 위한 AI 프록시 같은 게 나중엔 해결책이 될 수도 있을듯함
          + TLS 1.0에서 전환이 더 쉬웠던 이유는 end-to-end principle 덕분임
            서버와 클라이언트만 새 프로토콜 지원하면 되는데, 중간 장비(라우터, 스위치 등)는 네트워크 계층(IP)만 보면 됐었고, TLS 신규 버전과 상관없었음
            네트워크 레이어 프로토콜(IP)까지 변경하면 중간 네트워크 장비 모두 영향을 받음
            참고로 TLS 1.3 도입 때도 middlebox가 end-to-end 원칙을 깨고 트래픽을 감시/변조해, 호환성 때문에 TLS 1.3이 TLS 1.2 재연결처럼 위장하는 꼼수를 써야 했던 황당함도 있었음
          + 그 차이는 TLS는 서버/클라이언트만 지원하면 되고, 중간 네트워크 장비는 TCP 패킷만 보면 됨
            IPv6는 서버-클라이언트 사이 모든 중간 장비가 지원해야 하므로, “최소공통분모”에 종속적인 기술임
            TLS 업그레이드는 크게 변경되는 부분 없었던 반면, IPv6는 너무 많은 부분을 동시에 바꿨음
            지금 돌아보면, IPv6는 단순히 주소를 64비트로만 늘렸으면 차라리 더 보급이 쉬웠으리라는 아쉬움임
          + 현실적으로는 많은 사람들이 교체의 실질적 이점이 너무 적거나 거의 없어서 도입에 소극적인 것임
            거대 IPv4 음모론 같은 건 없이, 단지 일과 리스크 대비 효과가 적음
          + 네트워크 업계 농담에 “IPv6는 엔지니어링 문제에 끼워 맞춘 학술적 해결책”이라는 말이 있음
            대규모로 IPv4와 호환성까지 유지하면서 실제 현장 도입, 운영, 유지보수까지 생각하면 IPv6가 너무 복잡함
            사실상 주소 부족 말고 실질적인 문제도 없는 IPv4가 없어질 리도 없음
            그러다 보니 현장에서는 IPv6가 실질적 솔루션이 되지 못하고 있음
"
"https://news.hada.io/topic?id=21790","여기를 클릭하세요와 같은 링크 텍스트 사용 금지 (2001)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   여기를 클릭하세요와 같은 링크 텍스트 사용 금지 (2001)

     * ""여기를 클릭하세요"" 와 같이 일반적인 링크 텍스트 사용의 문제점 강조
     * 접근성 향상과 관련해 링크 텍스트가 콘텐츠의 의미를 명확하게 전달해야 함
     * 검색 엔진 및 스크린리더 사용자 경험에 부정적 영향 발생
     * 명확하고 맥락을 반영한 링크 문구가 사용자와 기술 모두에 이점 제공
     * 2001년부터 웹 표준과 UX 원칙 차원에서 올바른 링크 텍스트 권장 움직임 진행
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * ""여기를 클릭하세요"" 또는 ""click here""와 같은 일반적 링크 텍스트 사용이 오래전부터 웹 개발자들 사이에서 흔한 관행으로 자리잡음
     * 그러나 이러한 방식은 접근성, 사용성, SEO 측면에서 중요한 단점 내포

링크 텍스트의 의미와 문제점

     * 링크 텍스트는 사용자가 클릭하기 전에 해당 링크가 어디로 이동하는지 예측할 수 있도록 정보를 제공해야 하는 역할 담당
     * 단순히 ""여기를 클릭하세요""와 같이 작성하면, 맥락 없이 링크가 존재해 사용자 혼란과 접근성 저하 발생

접근성과 사용자 경험 영향

     * 스크린리더 사용자는 페이지 내 링크 목록을 훑을 때 모두 ""여기를 클릭하세요""라는 동일한 내용을 듣게되어 정보 획득 곤란 상황이 생김
     * 링크가 의미를 포함하는 구체적 문구로 작성되었을 때, 시각장애 사용자 및 다양한 상황의 사용자 모두에게 경험 효율성 증가

검색 엔진 최적화(SEO) 고려사항

     * 검색 엔진 역시 링크 텍스트를 분석하여 콘텐츠의 관련성과 품질을 판단함
     * 중요한 키워드와 맥락이 포함된 링크 텍스트는 검색 랭킹, 노출도 증가에 긍정적 영향 미침

올바른 링크 텍스트 작성법 권장

     * 맥락을 명확히 반영하는 링크 텍스트 사용을 권장
          + 예: ""최신 접근성 가이드라인 확인"" 등
     * 이러한 원칙은 2001년부터 웹 표준 등장과 UX 강화 과정에서 지속적으로 강조되어 옴

결론

     * ""여기를 클릭하세요""가 아닌 의미 있는 텍스트를 링크로 사용하는 것이 현재 웹과 미래 웹 모두에서 필수적인 기준임

        Hacker News 의견

     * 접근성 관점에서 보면, 시각장애 사용자를 위한 스크린 리더는 페이지를 선형적으로 읽어주는 방식임을 강조하고 싶음. 이 선형적 구조에서 벗어나기 위해 사용자가 헤더나 링크 목록을 별도로 탐색할 수 있지만, 모든 링크가 ""여기를 클릭""처럼 표시된다면 실질적으로 비(非)선형적 접근이 차단되는 문제 발생
          + 모든 링크가 단순히 ""Amaya""와 같이 동사 없이 명사로만 표시될 경우, 각각의 의미 파악이 어려워진다는 점을 지적. 그래서 ""get Amaya""나 ""go to the Amaya website"" 같은 표현도 충분히 괜찮다는 생각임. 또, 다운로드 버튼이 github.io에 있지만 실제로는 sourceforge 같은 악성코드 위험이 있는 사이트에서 실행 파일을 받아오게 만드는 건 좋지 않다는 사례로 wxMaxima 언급
          + 아이러니하게도, 접근성 도구를 실제로 필요로 하지 않는 사람들이 접근성 도구의 동작을 쉽게 시각화해 이해할 수 있도록 하는 뭔가가 필요함을 제안. 도구 자체를 바꾸자는 게 아니라, 더 많은 사람이 스크린 리더 환경을 직접 경험해볼 수 있는 방법 고민 필요성 인식
          + 이런 문제에는 이미 여러 가지 해결 방법이 있음. 예시로 WCAG 2.2 HTML H33 가이드, WCAG 2.2 CSS C7 가이드 링크를 참고자료로 제시. 다만, 첫 번째 방법이 실제 스크린 리더에서 얼마나 잘 지원되는 지는 확실치 않음
          + 좋은 주장이라는 생각. 그래도 링크 텍스트는 ""Amaya"" 보다는 ""Get Amaya""처럼 더 명확한 행동을 포함하는 게 좋겠다는 입장
          + 스크린 리더는 페이지 내 여러 내비게이션 방법 제공. 선형 이동만이 가장 비효율적인 방법임. 이용자는 랜드마크나 헤딩 혹은 아웃라인 네비게이션 모드를 통해 빠르게 원하는 위치로 점프 가능. 중요한 점은 스크린 리더 내비게이션 방식과 키보드 내비게이션이 다르다는 점임
     * ""나""는 글에서 제시한 ""문제적"" 예시인

     W3C의 편집기/브라우저 Amaya를 다운로드하려면, 여기를 클릭.
     이 정말 직관적이라는 생각임. 링크를 클릭하면 바로 파일이 내려받아지거나 다운로드 페이지로 이동할 거란 걸 예상할 수 있음.
     반면,
     Amaya 받기!
     이런 문구는 웹사이트의 메인 페이지로 이동할 듯한 느낌이라 실제 다운로드용 링크로 사용되면 효과 떨어짐.
     그리고
     _Amaya_에 대해 더 알아보기
     이건 다운로드와 전혀 관계없고, 동사 ""알아보기""는 링크가 아닌데 이럴 때 ""Amaya"" 링크가 랜딩페이지로 갈지, 정보 페이지로 갈지 혼란스러움.
     웹에서의 관행은 기존 예시처럼
     W3C의 편집기/브라우저 Amaya를 다운로드하려면 여기를 클릭
     Amaya 다운로드, W3C의 편집기/브라우저
     이런 식임.
     링크 텍스트에 동사를 쓰지 말아야 한다는 주장에는 동의하지 못하겠음. 반대로, 행위(다운로드, 정보 확인 등)가 필요한 링크라면 명확하게 동사가 들어가야 한다는 입장임.
     특히 ""여기를 클릭""은 단순 참조 링크가 아니라 행위성 링크임을 표시하기 때문에 매우 직관적임.
     Amaya 받기!
     이런 문구는 실제 Amaya를 어떻게 받는지 안내하는 링크가 아니라, 그냥 참조 링크인 듯 느껴진다는 점 강조
          + 스크린 리더로 직접 이용해보면 모든 링크가 ""여기를 클릭""만 반복되는 걸 알 수 있음. 이건 전혀 도움이 되지 않음. 검색엔진 관점에서도 ""여기를 클릭""은 아무 정보도 제공하지 않음. 동사 자체는 괜찮은데, ""여기를 클릭""처럼 링크가 동일하게 들리는 건 지양해야 함. ""Click Here to download Amaya""처럼 객체까지 명확히 넣은 문구는 괜찮지만 단순히 ""여기를 클릭""은 여러 상황에서 구별 불가함
          + ""여기를 클릭"" 링크는 정말 싫음. 내가 원하는 링크를 찾으려면 페이지 내 모든 링크 텍스트만 눈으로 훑게 되는데, ""Amaya 다운로드""나 ""Amaya""는 참조 페이지라면 괜찮겠지만, ""여기를 클릭""은 전혀 도움이 안 된다고 생각함
          + 실제 예시에서는 모든 링크가 Amaya 홈페이지로 이동. 다운로드 페이지가 아니고, 메시지 전달력이 오히려 약화됨. ""여기를 클릭"" 같은 접근성 이슈와, 링크 텍스트가 링크 목적을 얼마나 정확히 반영하는지를 혼동해 논의하고 있다는 비판
          + 스크린 리더 얘기 외에도, 링크 텍스트를 명확한 명사로 지정하는 게 유지 보수 관점에서 이점 있을 수 있음. 예를 들어, 의도와 다르게 복붙된 링크나, 죽은 링크의 경우 텍스트 자체가 단서가 되므로 추적과 수정이 더 쉬움. 약간 사소할 수는 있음
          + 초창기 웹에는 이런 ""여기를 클릭"" 링크가 많았음. 점차 동작이 명확한 버튼 형태로 대체되면서 ""이 구매를 취소하려면 [여기를 클릭]"", ""이 구매를 완료하려면 [여기를 클릭]"" 같은 패턴은 점점 사라짐
     * 영국 Government Digital Services에서도 비슷한 접근성 가이드라인을 권장한다는 점 소개, 공식 자료는 여기서 확인 가능
          + 이 가이드라인을 참고자료 삼아 가장 높은 수준의 접근성 컴포넌트/웹 디자인 구현에 자주 참고함. 투박하게 보일 수 있지만(두꺼운 검정/노랑 테두리 등), 접근성을 디자인보다 우선시하는 게 중요하다고 생각함
          + Home Office에서 추천하는 방식은 W3C와 약간 다름. 예를 들어
            W3C:
            Get Amaya
            Read more about Amaya
            Home Office:
            Get Amaya
            Read more about Amaya
            Home Office 방식이 더 합리적이지만, 문맥에 따라서는 또 다른 문제가 생길 수 있음. 두 곳 모두 실제로는 내러티브 안의 하이퍼링크 활용보다는 아웃오브라인 버튼 식으로 해결하는 게 더 자연스러움(예: [Download], [Documentation]). 내 경험상 텍스트를 자연스럽게 바꾼
            ""PiPedal은 Raspberry Pi에서 돌아가는 기타 이펙터임. PiPedal 다운로드는 [다운로드 페이지], 문서 읽기는 [Documentation].""
            이런 형식이 더 낫다고 생각해서 실제 문서에도 그렇게 적었음
            ""여기를 클릭""을 명사화하는 게 생각보다 어렵다는 점을 깨달았음. 때로는 inline 문장 안에서 충분한 맥락을 제공하므로 ""여기를 클릭"" 사용이 실제로 접근성 문제를 일으키지 않는 사례도 존재함
            특히 버튼이 바로 위에 위치한 경우(""Download"", ""Documentation"") 이면 굳이 수정할 필요 여부에 확신이 들지 않음 궁극적으로는 사람들이 다운로드 페이지를 실제로 얼마나 방문하는지가 성능 지표
     * 개인적으로 글에서 피하라고 한 두 번째 예시(“Amaya를 다운로드하려면 _Amaya 웹사이트_로 이동해 필요한 소프트웨어를 받기”) 가 낫다고 느낌.
       단순히 ""Amaya"" 만 링크로 넣으면 내부 링크인지 외부 링크인지, 그리고 바로 파일로 갈지 다운로드 페이지로 갈지 명확하지 않음
          + 목적지가 외부인지, 파일 링크인지 알 수 있도록 아이콘을 추가하는 방식 선호. 파일인 경우 파일 확장자를 표시해도 좋다는 아이디어 공유
          + 내부-외부 링크 구분 문제는 Wikipedia 등에서 작은 아이콘으로 이미 잘 해결 중
     * 나이가 많아서 그런지, 직관적으로 링크는 명사(장소/대상)를 가리키는 용도로 생각하게 됨.
       그래서 ""내 웹사이트""로의 링크 등은 괜찮은데, ""내 웹사이트로 이동"" 같이 동사가 들어간 문구에는 거부감
       특히 명령어(임퍼러티브) 형태를 싫어해서 ""go to my website"", ""follow this link"" 같은 형식은 사용하지 않음
          + 이 말이 공감됨. 다운로드처럼 액션이 있는 경우 혼란스러울 수 있지만, ""다운로드"" 자체가 명사라 괜찮을 듯함
          + 튜토리얼이나 How-to 페이지는 명령형 문구도 적격이라고 봄
     * 온라인에서 ""여기"" 만 링크로 사용하는 건 하이퍼텍스트 문서 작성 자체를 전혀 이해하지 못했다는 근본적인 실패라고 느끼는 입장. ""여기를 클릭""은 마치 대본의 지시문(stage direction) 같음. 많은 저자가 하이퍼텍스트 맥락에서 글을 쓰지 않아 생기는 문제라 생각함
     * ""I forgot my password"" 같은 문구를 링크로 둘 때, ""여기를 클릭""처럼 바꿔보려고 생각했지만 직관적으로 뭔가 아니라고 느낌.
       UI 표준이 무너지면서, 사용자가 페이지에서 뭘 클릭해야 하는지, 어떤 텍스트가 자신을 대변하는지 혼동하는 현상이 많아짐
       ""I forgot my password""가 버튼이면 링크보다 훨씬 더 효과적이라는 점에는 동의
       또한 Microsoft가 버튼을 대부분 없애고 모호한 색상의 링크로 대체했을 때, 어디를 눌러야 할지 헷갈리는 사례가 많아지기도 했음
       (참고 포스트: 관련 마스토돈 글)
     * Dragan Espenschied의 2022년 에세이 링크 텍스트 변화사를 소개.
       최근 링크 텍스트가 call-to-action(행동 유도)에서 사용자 상태(자신이 어떤 사람이라는 정의, 예: “로그인하세요”)를 설명하는 버튼 텍스트로 바뀌는 트렌드를 언급
     * 대부분 글에서 설명한 원칙에 동의하지만, ""download""처럼 동사나 동사구가 명확히 드러나야 하는 경우에는 예외적으로 적합한 경우가 있다고 생각함. 이 경우 반드시 직접 다운로드로 연결될 때만 사용해야 하며, 항상 맥락에 맞는지 확인 필요
     * 예시처럼 모든 링크에서 실제로 Amaya 다운로드가 진행된다면, Download Amaya 형식의 하이퍼링크가 가장 선호됨.
       가능하면 다운로드임을 암시하는 아이콘을 함께 넣어주면 좋겠음. 실제 파일 링크가 아니라 광고 버튼이 여러 개 섞여있는 다운로드 페이지로 유도되는 건 피하고 싶음
"
"https://news.hada.io/topic?id=21802","개인용 AI 팩토리 구축기 (2025년 7월 스냅샷)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     개인용 AI 팩토리 구축기 (2025년 7월 스냅샷)

     * 여러 AI 에이전트(claude/o3/sonnet 등) 를 활용해 코드 생성부터 검증까지 자동화하는 개인용 ""AI 팩토리"" 워크플로를 운영함
     * 문제 발생 시 코드(Output)를 직접 수정하지 않고 플랜, 프롬프트, 에이전트 구성등 입력(Input)을 개선하여 자동화 수준을 높이는 방식이 핵심
     * 이렇게 입력을 반복적으로 개선하면, 에이전트들이 지속적으로 발전하여 반복적인 작업 생산성을 극대화함
     * 각 에이전트 역할 분담: 계획 수립(o3/sonnet4), 실행(sonnet3.7/4), 검증(o3/sonnet4) 등으로 구분하여 병렬 처리와 자동 피드백 루프 구현
     * 코드 오류나 스타일 문제도 계획 템플릿에 반영해 다음 생성부터 개선되도록 설계, 반복적 입력 개선으로 팩토리 자체가 성장함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 팩토리 개요 및 핵심 원칙

     * 여러 개의 claude code 창을 서로 다른 git-worktree에 띄워 분리된 작업 환경을 유지함
     * o3와 sonnet 4는 플랜 수립에 활용되고, sonnet 3.7 또는 sonnet 4는 실행을, o3는 결과 평가를 수행함
     * 계획 수립, 실행, 검증을 에이전트별로 병렬 분담하여 효율화함

핵심 원칙 – ""결과(Output) 수정 대신 입력(Input) 자체를 개선""**

     * 문제가 생기면 생성된 코드를 직접 패치하지 않고, 플랜, 프롬프트, 에이전트 혼합비를 조정해 자동화된 개선을 추구함
     * Factorio 게임처럼, 자동으로 성장하는 공장식 AI 에이전트 네트워크를 만든다는 컨셉임
     * 이런 구조에서 계획-코딩-검증-개선까지의 루프가 순환되며, AI 에이전트가 스스로 코드를 생산/검증/개선하는 환경을 조성함

일상 워크플로 – 팩토리의 구조

     * 메인 인터페이스는 claude code이며, 로컬에서는 mcp 및 Goose(Azure OpenAI 모델 연결용), o3 등을 활용함

  1단계: 계획(Planning)

     * claude code에 고수준 작업(task) 입력 → o3가 추가 질문 후 계획안(<task>-plan.md) 생성
     * 계획안에는 원래 요청사항과 구현 플랜이 포함됨

  2단계: 실행(Execution)

     * sonnet 4가 계획을 검토 후 태스크 리스트로 변환
     * claude code가 작업을 실행하며, 과제 복잡도에 따라 sonnet 3.7 또는 4를 사용함
     * claude가 각 작업 단계마다 커밋 기록을 남기므로 문제 발생 시 손쉬운 롤백이 가능함

  3단계: 검증 및 피드백(Verification → Feedback)

     * sonnet 4가 생성 코드를 플랜에 맞추어 초기 검증 수행
     * 이어서 o3가 플랜 및 최초 요구사항과 비교해 보다 엄격히 검증함
          + o3는 불필요한 코드(예: lint ignore 플래그)나 구식 구조를 엄격하게 지적
     * 검증에서 드러난 문제는 코드 직접 수정이 아닌 계획 템플릿 개선으로 반영
     * git worktree를 활용해 여러 claude code 인스턴스를 병렬로 운영하며, 동시 다중 작업이 가능함

""입력""이 ""출력""보다 중요한 이유

     * 결과물(Output)은 버릴 수 있지만, 계획/프롬프트(입력)는 계속 축적·개선되는 누적 자산임
     * 입력부(플랜, 프롬프트)에서 디버깅하면 앞으로의 모든 작업에 확장 가능
     * 에이전트를 단순 생성기가 아닌 스스로 학습하고 협업하는 존재로 변화시킴
     * 예: 전체 CSV를 메모리에 올리는 코드를 스트림 처리로 변경하게 하고, 이후 모든 CSV에 해당 패턴을 플랜에 반영해 차후 자동 검증 가능해짐

팩토리 확장과 에이전트 협업

     * MCP로 전문화된 에이전트들을 각기 다른 작업에 배정하고 병렬화함
          + 예: Clojure 코드 전체를 모아 로컬 스타일 규칙 적용 전용 에이전트 운용, claude가 lint/test/debug 주기에서 발생하는 스타일 문제 보정
     * 내부 라이브러리 코드에서도, 예전 코드의 retry 및 Thread/sleep 사용을 자체 retry 라이브러리로 치환 등 생산성 및 일관성 강화
     * 작은 단위 에이전트들을 여러 개 구축해, 특정한 소작업별로 조합하여 복잡한 워크플로우도 자동화 가능
          + 예: API 명세와 비즈니스 케이스로부터, 에이전트 조합으로 통합, 테스트, 문서화까지 자동 처리
     * 핵심: 입력을 계속 수정해가며 반복 실행, 실패/정체/문맥 누락 시 다음 시도로 피드백 반영 후 개선
     * 코드 자체는 소모품, 진짜 자산은 지시사항(입력)과 에이전트 구성임
     * 실패·정체·문맥 부족 등 모든 문제 교훈을 다음 입력에 반영해 팩토리 루프 완성

다음 단계 및 미래 방향

     * 에이전트 간 총괄 코디네이션을 강화해 전체 워크플로우 추적 및 자동화 도입 추진
     * 비즈니스 문서와 에이전트 정보를 잘 연결하고, 상위 수준의 추상화 정보 위주로 캡처하여 더 효과적으로 활용할 수 있도록 개선
     * 점점 복잡한 워크플로우를 구현하려고 하며, 에이전트간 협업 및 복잡한 상호작용 확대
     * 여러 공급자의 토큰 할당량 최대한 활용 및 손쉬운 전환(특히 bedrock sonnet 4의 토큰 제한 대응) 방안도 모색 중임

결론

     * 현재 AI 팩토리는 자동 코드 생성·검증이 일상화되어, 커피 한잔 동안에도 코드가 배포 가능
     * 아직 완전 자동화 단계는 아니나, ""출력 수정 대신 입력 개선"" 이라는 원칙이 팩토리의 본질로 자리 잡음

        Hacker News 의견

     * 이 글은 Claude Code와 'aha' 순간을 겪어보지 않은 사람에게는 거의 이해하기 힘든 내용일 거라고 생각함
       ""claude --dangerously-skip-permissions""로 권한 제한을 풀고 복잡한 문제를 맡겨두면, Claude가 다양한 도구를 자유롭게 활용해 문제를 해결하는 광경을 볼 수 있음
       오늘도 Docker로 486 assembly로 된 만델브로트 프랙탈 생성기를 직접 컴파일, 실행, 디버깅까지 하도록 시켜봄
       정말 훌륭하게 처리함
       gist 링크
          + 이런 IDE나 LLM에게는 굉장히 쉬운 예시라고 생각함
            어셈블리는 학습 데이터셋에도 충분히 포함되어 있고, Docker도 마찬가지임
            Cursor가 내 코드베이스에서 마음껏 뛰놀게 해본 적도 있음
            최신 툴들이 언젠가는 정말 제대로 도달할 수 있다고 기대하지만, 아직 그 단계까지는 못 왔다고 느낌
          + Dagger(그리고 Docker의 창립자)가 AI Engineer 컨퍼런스에서 발표한 이 영상을 추가로 추천하고 싶음
            이 영상도 다소 난해할 수 있음
          + 혹시 도움이 될 수도 있을까 싶어서 적어봄
            나는 Claude max에서 pro로 강등했고 월 20달러에 이용 제한도 충분히 넉넉함
            Gemini CLI와 경쟁하는 듯해서 이제는 돈을 덜 내고 있어서 기쁨
          + 이 정도 예제나 맥락은 거의 모든 LLM이 별다른 어려움 없이 해결할 수 있을 거라 생각함
            나는 훨씬 복잡한 Rust 의존성 업그레이드를 30회 이상 반복하면서, custom wasm 코드로 처리한 경험이 있음
            Claude는 context7이나 mcp-lsp 등 여러 툴을 연결하며 세부정보를 모음
            하지만 계속 사용하다 보면 한계에 부딪히게 되니, 더 정교하고 어렵게 밀어붙이면 약점이 드러남
          + ""claude --dangerously-skip-permissions""로 복잡한 문제를 맡기면 여러 도구를 활용해 문제를 해결한다는, 그 문구에 대해
            나는 Claude가 한 시간을 넘게 틀린 방식으로 코드를 고치려 애쓰는 걸 지켜본 적 있음
            결국 직접 개입해서 ""먼저 단위 테스트를 쓰고, 테스트 통과시 코드를 작성해서 다시 알려달라""고 지시함
            Claude Code는 정말 멋진 도구이지만, 기본적인 아키텍처 지도를 계속 반복하며 제공해야 하는 현실임
     * 코드 결과물이 실제로 어떻게 사용되는지 모르고서는 이런 셋업들을 평가하기 어렵다는 생각
       개인적으로 활용하는 vibe coding 앱이라면 정말 믿기 쉬운 얘기지만,
       복잡한 프로덕션 환경에서 고품질 코드를 쓴다는 이야기는 쉽게 납득되지 않음
          + 완전 동의함
            나는 Claude Code로 코딩 속도를 크게 높이지만, 모든 코드 변경마다 항상 직접 체크해서 최적의 시스템이 만들어지는지 확인함
            그냥 돌려만 둔 몇 번의 시도에서는 사용자에게 버그를 안겨줌
          + 사실 이 글에서 설명하는 워크플로우나 개념을 잘 이해 못하겠음
            조금 두루뭉술한 설명이라 그런 듯
            나는 일상적으로 여러 에이전트가 서로 대화하는 구조, 비동기적 에이전트, git work tree 같은 걸 활용해서 복잡한 프로덕션 시스템을 다룸
            출력 결과를 절대 바꾸지 않는 건 아니지만, 원하는 결과가 안 나오면 그걸 내 워크플로우 개선 신호로 받아들이는 편임
     * 나도 비슷한 워크플로우를 시도해보고 있어서 내 경험을 공유하고 싶음
       내가 다루는 Go 코드베이스는 수십만 라인 규모이며, 실제로 수만~수십만 명의 B2C 유저가 사용 중임
       퍼포먼스는 넉넉하지만 정확성과 신뢰성이 매우 중요한 파이낸스 분야임
       나는 오픈AI 키만 사용하는 환경이라 codex-cli와 간단한 스크립트로 저장소 복제, 에이전트 구성, 프롬프트 실행 등의 기본 셋업만 사용
       codex 인스턴스가 시스템 알림으로 내 차례를 알려주고, fzf를 활용해 필요할 때 tmux 세션에 붙음
       아직 MCP는 안 써봤지만 관심 리스트에 올라 있음
       이런 방식이 작은 산발적 태스크를 처리할 땐 엄청 도움되고, 이제는 자잘한 PR들을 훨씬 많이 만들어 내고 있음
       ""cattle not pets"" 메타포가 여전히 적용됨, 그냥 작은 작업은 빠르게 프롬프트 던져서 산만함을 줄이고 있음
       더 규모가 큰 일에는 아직 잘 안 맞는 듯하고, 아마 나 역시 충분한 컨텍스트 플라이휠을 못 만든 것일지도
       대부분은 결과 코드를 항상 직접 읽고 손봐서 코드 리뷰에 올림
       변경 관리도 수동으로 거의 다 하며, 브랜치/커밋/푸시도 직접함
       자동화 도구도 몇 개 써봤지만 완전히 넘어가진 못한 상태
       ""출력물이 아니라 입력을 고쳐라""는 사고방식에 100% 공감함
       AI 없이도 굉장히 강력한 원칙이고, 업계도 점점 더 받아들이는 추세임
       LLM처럼 비결정적인 프로세스에서는 적용이 쉽지 않고 과학보다는 연습에 가깝게 느껴짐
          + 최근 에이전트 분야에서는 불과 며칠 단위로 컨텍스트 관리에 대한 얘기가 많이 오가지만, 이런 방식 쓸 때 나 스스로의 컨텍스트 관리가 제일 어렵게 느껴짐
     * 좋은 글 감사
       나는 ""Vibe Specs""라는 글에서 비슷하지만 약간 더 단순한 워크플로우를 소개함
       관련 블로그 논문
       나는 코드베이스 전체에 이 룰을 사용하고, AI가 두 가지를 다르게 행동하게 만듦
       (1) 무엇보다 먼저 질문부터 하게 함
       (2) 코딩에 앞서 spec.md 문서를 만들게 함
       주제의식은 비슷하지만, 나는 한 LLM에만 한정함
          + 우리 대부분이 이걸 비슷하게 시도하는 것 같음
            개인 개발자라 엔지니어링 중심적 사고로 다양한 생산 자동화를 실험함
            나에게 있어 궁극의 목표는, 에이전트가 (구현과 별개로) 자동 생성한 e2e 테스트로부터 코드에 대한 신뢰를 얻는 방식임
            아직 완전히 성공한 건 아님
          + 이제 Claude Code도 “plan mode”로 이런 플로우를 네이티브하게 지원함
            .md 파일을 수동으로 만드는 일은 사실 느리고 귀찮게 느껴짐
     * 기본 생각은, 시스템이 어떤 동작을 해야 하는지(높은 수준과 세부 기능 모두), 어떻게 동작을 증명할지, 아키텍처 및 코드 스타일 같은 구현 방법까지도, 계속 문서화할 수 있다는 점임
       여러 모델을 쓰는 이유는 편향을 줄이고, 특정 작업에 맞는 미세조정 선택지를 늘리기 위한 수단임
       어떤 날에는 대규모 복잡한 시스템도 요구사항 집합 기반으로 다시 만들어질 수 있고, 이때 비로소 소프트웨어가 요구 명세와 진짜 일치하게 될 것임
       이때 남는 ‘레거시 코드’는 오직 레거시 명세 문서가 될 것임
       생성 코드가 아니라 요구 명세를 고치라는 관점임
          + 미안하지만...
            관련 밈 이미지
     * 도대체 실제로 뭘 만드는 건지 궁금함
       AI 워크플로에 대해 얘기할 때마다 반쯤은 꿈 같은 플로우를 말하는 건지, 아니면 진짜 생산적으로 쓰고 있는지 알기 힘듦
          + 나도 매번 결론이 비슷함
            LLM이 코드를 다 짜주면 그냥 관심이 떨어짐
            50개쯤 되는 프로젝트 중 LLM으로 만든 건 2개뿐이고, 그나마도 나는 직접 손을 봤음
            나머지는 “뭔가 있으면 좋겠다”는 생각만 가지고, 실제론 결과엔 별 관심이 없었음
            결국 여러 모델과 구현 세부사항에 씨름하면서, 원하는 동작이 안 나오면 설계 문서, 프롬프트, 예시 데이터 다 들이대며 컴퓨터와 싸우는 루프에 갇히게 됨
            그냥 코드 조금씩 입력 보완만 해주는 쪽이 훨씬 빠르고 덜 스트레스를 받음
            뒤돌아보면 시간과 비용만 쓰고, 결과물은 근근이 돌아가는 소프트웨어라는 생각임
            분명한 요구사항이나 기존 코드베이스가 있고, 내가 주도적으로 가이드 하면 에이전트가 꽤 도움이 되지만, vibe coding 같은 흐름은 작은 스크립트나 틈새앱이 아닌 이상 전혀 재미도 없고, 원하는 품질이 잘 안 나옴
            지나치게 비싸기도 하고 코드도 여전히 지저분
            결국 컴퓨터와 끝없는 논쟁에 시간을 쓰는 느낌임
            이럴 바엔 그냥 직접 하는 게 훨씬 나음
     * 여러 에이전트가 각자 work tree에서 작업할 때 겪는 문제는, 각 에이전트가 모든 세부에서 서로 완전히 다른 아이디어를 내놓으니 사용자 경험이 전혀 통일되지 않는다는 점임
       예를 들어 Documents 대시보드와 Design 대시보드를 만드는 에이전트들이 완전히 다른 관점에서 설계함
       디자인 일관성도, 구조도, DB 스키마나 API 설계 모두 통일감이 없음
       입력이 같아도 출력이 다름
       결국 일관성을 위해 instruction 파일을 늘리다 보면, 대형 프로젝트는 몇 천 줄이 기본이라 context window도 부족해짐
       결론적으로 특정 룰과 스키마만 학습한 작은 LLM을 쓰는 게 더 맞다고 생각함
       프롬프트로 우주만큼 넓은 아이디어를 다루는 대형 LLM보단 작은 LLM이 정답일 듯함
          + 에이전트마다 완전히 다른 결과, 디자인의 일관성 없음
            결국 시니어가 꼭 필요함
            AI든 사람이든, 원하는 방향으로 움직일 최소한의 구조와 유연성을 직접 제공할 수밖에 없음
            구조가 없다면 그냥 직접 코딩하는 게 훨씬 나음
          + 나는 첫 번째 버전을 직접 만든 후, 이후 일은 Claude Code에게 ""이 예시처럼 만들어라""라고 하면 일관성을 유지하기 수월했음
     * ADHD식 코딩, 무작정 프로덕트 생성 시도 후 맞출 때까지 반복하는 방식?
       그냥 미래 확장 가능한 코드를 직접 작성하는 편이 낫지 않을까
       굳이 탄소 발자국만 늘리지 말라는 입장
          + 최종 목표는 개발자를 이 프로세스에서 배제시키는 것임
            비즈니스 오너가 새 CRUD 앱을 요청하면 바로 프로덕션에 띄우는 단계
            물론 결과물은 버그투성이에 느리고, 인증되지 않은 DB에 저장하지만, 그건 내 일이 아니라는 마인드
            마지막엔 뜨거운 차를 벌컥 들이키는 격한 표현으로 마무리
          + 프로그래밍은 영원히 바뀌었고, 그 변화를 빨리 받아들여야 함
            “그냥 코드를 쓰라”는 건 마치 모두가 직접 마차를 손질하자고 주장하는 것과 같음
            차도 고장나니 옛날 방식을 고집하자는 소리와 별반 다를 게 없음
          + 왜 늘 ‘미래 확장 가능한 코드로 그냥 직접 써라’는 얘기가 나오는지 의문임
            요즘 코딩 어시스턴트도 zero-shot으로 확장 가능하고 유지보수 가능한 코드를 쓸 수 있는데, 실제로 그렇게 요청해 보기나 했는지 반문하고 싶음
          + 인간도 결과적으로 trial and error로 계속해서 답을 찾아가는 건 마찬가지임
            경험이 많을수록 머릿속에서 시뮬레이션 정도만 다를 뿐임
            탄소발자국을 문제 삼는다면, AI 데이터센터가 재생에너지로 돌아가면 그건 괜찮은가라는 시각
     * AI를 워크플로에 더 효과적으로 통합하는 법을 찾아야 한다고 생각함
       적극적으로 AI를 도입해본 사람들은 비슷한 고민을 겪었을 텐데, 아직 확실한 해결책이 없음
       이 단계에선 AI에게 최소한의 역할만 정확하게 할당하는 게 핵심이라고 봄
       예를 들어, 주식 리서치에 쓰는 에이전트 워크플로우로 ‘Bullish Guy’와 ‘Bearish Guy’ 두 AI를 만들어 하나의 종목에 대해 장단점 논쟁을 벌이게 함
       이렇게 서로 반대 입장에서 자료조사를 시키면 더 포괄적이면서도 깊이 있는 분석 결과가 도출됨
       이런 아이디어는 실제로 소셜미디어에서 논쟁하는 방식을 보고 착안함
     * vibe-coding에서 결과물이 자기참조적인 얘기 이상의 게 별로 없고, 결국 3D 프린팅의 뒤를 이을 비싼 취미이자 끝없는 장난감 만들기 활동처럼 보임
       요즘 vibe coding에서 “benchy”처럼 쓰이는 예제는 todo 앱 정도 아닌가 하는 의문
          + 3D 프린팅 자체는 실제로 굉장히 유용함
            제품을 직접 설계하거나 엔지니어링을 하는 사람이라면 다들 씀
            일반 소비자들이 활용하지 않는 유일한 이유는, 이미 필요한 거의 모든 플라스틱 아이템을 Amazon에서 바로 주문할 수 있기 때문임
            온라인쇼핑이 없던 시절이라면 평균적인 사람들한테 훨씬 유용했을 것
            앞으로는 커스텀 파일 직접 설계가 가능한 사람들에게만 정말로 필요한 기술일 듯함
"
