"https://news.hada.io/topic?id=20963","Pyrefly - 파이썬을 위한 새로운 타입 체커 및 IDE 경험","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Pyrefly - 파이썬을 위한 새로운 타입 체커 및 IDE 경험

     * Meta의 Pyrefly는 Rust로 개발된 오픈 소스 파이썬 타입 체커이자 IDE 확장 기능
     * 초고속 분석 성능과 IDE 통합 기능을 지원하며, Pyre의 한계를 극복하기 위해 개발됨
     * 자동 타입 추론과 대형 코드베이스 지원, 오픈 소스 철학을 원칙으로 삼음
     * 파이썬 커뮤니티와의 협업 및 기여를 통해 생태계 전반의 타입 시스템 개선을 목표로 함
     * 현재 알파 버전 출시, 커뮤니티 피드백과 기여를 적극 요청 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * Pyrefly는 Meta가 Rust로 개발한 파이썬 정적 타입 체커이자 IDE 확장 오픈 소스 프로젝트임
     * 코드 실행 전 타입 일관성 검증을 통해 에러 사전 탐지를 지원함
     * IDE 통합과 CLI 사용 모두 가능하여 유연한 워크플로우를 제공함
     * 오픈 소스 커뮤니티 협업을 통해 파이썬 타입 시스템 및 다양한 라이브러리 발전에 기여 목표임

Pyrefly 개발 배경

     * 2017년, Meta는 Instagram의 대규모 파이썬 코드베이스를 위해 새로운 타입 체커(이후 Pyre)를 개발함
     * Pyre는 Hack, Flow 등의 견고한 설계 참고, 성능을 위해 OCaml로 개발됨
     * 시간이 지나며 타입 시스템 발전 및 IDE 연동 니즈가 커짐에 따라 한계 발생
     * Pyright 등 커뮤니티 툴도 사용하였으나, 대규모 코드 탐색·타입 내보내기 등 요구사항 충족에 한계가 있어 Pyrefly 개발 시도함

Pyrefly의 주요 원칙

     * 1. 성능
          + 개발자는 코드 작성 직후 매 키 입력마다 빠른 타입 체크 필요함
          + Pyrefly는 초대형 코드베이스도 초당 180만 라인 검사가 가능한 고성능 Rust 구현 구조임
     * 2. IDE 중심 설계
          + IDE와 CLI가 동일한 시각을 유지할 수 있도록 처음부터 추상화 설계를 진행함
          + Pyre에서는 사후 보완이었으나, Pyrefly에서는 설계단계부터 일관성을 강조함
     * 3. 인퍼런스(추론)
          + 주석 없이 타입이 명시되지 않은 파이썬 코드도 자동으로 타입 추론 지원
          + 반환값 및 로컬 변수의 타입을 IDE에 표시하고, 더 나은 코드 작성을 위해 더블클릭 시 추론 타입 자동 삽입 가능함
     * 4. 오픈 소스
          + Pyrefly는 MIT 라이선스로 GitHub에서 공개, 커뮤니티 PR 및 이슈 제보 환영
          + 파이썬 생태계와 Meta 주요 라이브러리(PyTorch 등)와 연계하며, Discord 채널 통한 활발한 커뮤니케이션 추구

Pyrefly의 미래

     * 커뮤니티와 함께 파이썬 언어 및 개발자 경험 개선 목표로 활동 중임
     * Pyre 개발 초기부터 코드 오픈소스화 및 PEP 기여 유지, Pyrefly에서도 다양한 개발자·라이브러리·초보자에게 타입 활용 이점 극대화 계획
     * Meta는 동적 언어에서의 타입 활용 경험과 성과를 바탕으로 다양한 경험 공유, 생태계의 타입 품질 향상 추진 예정
     * 현재 Pyrefly는 알파버전이지만, 올 여름 공식 런칭 목표로 지속적인 버그 수정 및 기능 추가 진행 중임
     * 커뮤니티 피드백이 매우 중요하며, Pyrefly 사용 후 이슈 보고 및 개선 요청을 적극 요청함

Pyrefly 알파버전 활용 및 커뮤니티 안내

     * Pyrefly 개발 과정과 기술적 디테일은 Meta Tech Podcast 및 PyCon US 발표 등에서 공개됨
     * Meta Open Source 관련 사이트, YouTube, Facebook, Threads, X, LinkedIn 등 다양한 채널을 통해 추가 소식 제공

        Hacker News 의견

     * Meta의 ""Python Language Tooling Team""을 대신해 약간 걱정스러운 마음 형성, uv의 인기가 워낙 높아서 ty가 이 분야에서 승리할 것 같은 예감, 잘못하면 Atom이나 Flow처럼 내부 팀이 외부 오픈 소스에 밀려 위에서 ""이 팀 정말 필요 없는 것 아닌가? 오픈 소스로 바꾸자""라는 분위기가 형성되는 상황 발생 가능성, 관리자(Aaron Pollack?)가 신경 써야 할 부분이라고 생각함
          + Kevin, 반갑다는 인사와 함께 과거 Flow에서 일했다는 경험 언급, 지금은 Pyrefly 팀에서 활동 중, 이번에는 Flow와는 다른 접근법을 택했고 오픈 소스 및 커뮤니티 구축을 명확하게 우선순위로 삼았다는 설명, 최근 기업들의 인프라 투자와 관련해 변동성이 컸지만, 현 시점에서 올바른 여정의 시작점에 있다고 믿는다는 생각 공유, 응원의 마음 전달
          + Meta는 특히 개발 도구의 오픈 소스 프로젝트에 대한 통제권을 크게 중요시한다는 의견, 예전에 git 관리자가 monorepo 활용에 대해 지적하며 개선책을 upstream 거부한 일로 mercurial로의 전환이 이루어졌으며, mercurial 쪽은 기꺼이 기여를 받아들였다는 일화, 내부 툴의 변화 속도가 워낙 빠르기 때문에 자체 프로젝트를 소유하는 것이 합리적이라는 설명
          + Facebook에서 나온 것 중 JSX가 가장 마음에 든다는 이야기(아마 유일하게 괜찮다고 생각하는 부분)
     * Meta의 Pyrefly 팀에서 일하고 있다는 소개, FAQ에 많은 질문이 커버되어 있으니 참고 링크 제공, 추가 질문에도 답변 가능하다는 친절함 표시와 관심에 대한 감사 인사
     * 요즘 Rust로 작성된 Python 타입 체커가 세 곳(Microsoft, Facebook, Astral)에서 등장했고, 기존의 mypy도 여전히 존재하는 상황이라는 관찰
          + Microsoft의 타입 체커 Pyright는 Typescript 기반이라는 정정, 그래도 mypy보다 빠르다는 개인적인 경험 공유
          + 모두 정적 타입 체커라는 질문, 런타임용은 없다는 언급
     * 이번이 공식 발표이지만, Pyrefly가 몇 주 전에 이미 논의되었다는 정보 제공, 현재는 알파 단계로 출시 중이며 버그 수정과 기능 개발에 집중해 여름에는 알파 딱지를 떼는 것이 목표라는 팀의 공식 입장 인용
     * 여기 작성된 Rust 코드는 매우 이해하기 쉽지만, 최근 Python 도구가 Rust로 계속 작성되는 것에 대한 우려와 함께 또 다른 N개의 언어 문제 발생 우려감 표시, Mojo가 여기서 뭔가 해줄 수 있길 바라는 마음
          + Python 생태계에서는 Python이 감당할 수 있는 영역에선 Python을 쓰고, 고성능이 필요한 곳에선 Rust나 C 같은 언어를 사용하는 것이 자연스러운 흐름이라고 설명, 결국 N=3(파이썬, 러스트, C), 다만 C는 장기적으로 응용프로그램 프로그래밍에서 서서히 없어져야 한다는 바람, 하지만 실제로는 오랜 시간이 걸릴 전망, 오히려 Python 자체가 레거시가 될 수도 있다는 생각
     * Vim/Neovim에 통합하는 사용법이 안내되고 있어 반갑다는 느낌, 관련 링크 제공
     * Rust로 작성됐다는 점이 왜 큰 장점처럼 언급되는지 의문, 대부분의 사람들이 임베디드 시스템이나 미션 크리티컬 서비스에서 타입 체커를 구동하는 것이 아닌데, ""Erlang으로 작성됨""과 비슷한 느낌, 성능이 중요한 코드가 아닌 것은 Python으로 작성하는 편이 더 많은 커뮤니티의 참여와 확장이 가능한데, Rust에 집착하는 이유에 대한 의문
          + Rust 사용 경험 공유, 사용자로서는 속도와 안전성, 개발자 입장에선 기여가 용이하다는 장점 언급, Astral의 매력도 이러한 Rust 기반 툴을 Python에 가져온다는 점이라고 생각, Rust보다 Python을 잘 알지만 Rust 프로젝트에 기여하는 게 더 쉽다고 느낌, Rust품질 도구의 Python 이식이 전체적인 목적이라는 견해
          + LSP(Language Server Protocol)는 성능이 매우 중요한 코드라고 생각, IDE의 응답성에 직접적인 영향, Rust는 CPU와 메모리 모두 효율이 뛰어남, 만약 OCaml, Reason, Haskell 등으로 작성됐다면 속도나 효율은 충분히 나오겠지만 기여자 풀이 매우 한정적일 것이라는 점을 강조
          + ""[툴 설명] rust""로 검색하면 성능 좋은 소프트웨어를 쉽게 찾을 수 있다는 점이 만족, 사용하는 툴 중 약 95%가 Rust로 작성되었는데 대부분 만족스럽다는 경험 공유
          + ""눈에 띄게 빠르다""라는 뜻의 축약어처럼 Rust가 활용됨, 오픈 소스 Rust는 여전히 리뷰가 가능하다는 점 강조
          + Python으로 작성된 타입 체커는 성능이 부족하다는 설명, 예를 들어 Pylint와 같이 Python으로 만들어진 린터는 코드 라인마다 하나씩 체크해 30초 이상 소요되는 등, 성능이 중요한 영역이라는 주장
     * Pyre에서 Pyrefly로의 전환과 Rust로의 전면 재작성에 대한 호기심, 덜 알려진 언어에서 트렌디한 Rust로의 전환에 따른 구체적 이점 또는 동기가 궁금함
          + 이 질문은 우리의 FAQ에서 다룬다는 안내, 경험이 더 쌓이면 긴 블로그 포스트로도 이야기해보고 싶다고 전함, 링크 제공
     * 너무 많은 것을 한 번에 해결하려는 프로젝트라는 느낌이라는 의견
     * VS Code를 보고 흥미를 잃었다는 의견, 왜 사람들이 VS Code를 Python용 IDE로 적합하다고 생각하는지 이해 못 하겠다는 입장, PyCharm같은 진짜 IDE가 있는데 VS Code를 쓸 이유가 없다는 생각
          + pyrefly는 vscode에만 묶여있지 않다는 점, 다양한 사람들이 다양한 선호도를 가진다는 점을 좀 더 배려해달라는 부탁, pycharm 역시 절대적으로 더 좋지 않음, 본인은 vscode 원격 개발이 편리하고, pycharm이 별로라는 말을 인터넷에 올리고 싶진 않다는 의견
"
"https://news.hada.io/topic?id=20962","JavaScript의 새로운 슈퍼파워: 명시적 리소스 관리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    JavaScript의 새로운 슈퍼파워: 명시적 리소스 관리

     * Explicit Resource Management 제안은 파일 핸들, 네트워크 연결 등 리소스의 라이프사이클을 명확하게 제어하는 새로운 방법
     * Chromium 134와 V8 v13.8부터 해당 기능을 이용할 수 있음
     * 언어에 추가 되는 부분들
          + using 및 await using 선언과 Symbol.dispose, Symbol.asyncDispose 심볼 도입으로 자동 정리 메커니즘을 제공
          + DisposableStack, AsyncDisposableStack은 여러 리소스를 안전하게 그룹화하고 해제
          + SuppressedError는 정리 중 발생한 오류와 기존 오류를 함께 관리함
     * 이 방법은 코드 안전성과 유지보수성을 크게 올려주며, 리소스 누수 방지에 효과적임
     * 기존 try...finally 패턴을 단순화하고, 대규모 복합 리소스 환경에서 신뢰성 높은 자원 처리가 가능해짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

명시적 리소스 관리 제안 개요

     * Explicit Resource Management 제안은 파일 핸들, 네트워크 연결 등의 리소스를 명확하게 생성‧해제할 수 있는 새로운 방법을 도입함
     * 주요 구성요소는 다음과 같음
          + using 및 await using 선언: 스코프 종료 시 자동으로 리소스 해제
          + [Symbol.dispose](), [Symbol.asyncDispose]() 심볼: 해제(cleanup) 동작 구현을 위한 메서드
          + 글로벌 객체 DisposableStack, AsyncDisposableStack: 여러 리소스를 그룹화해 효율적으로 관리
          + SuppressedError: 리소스 정리 중 발생한 에러와 기존 에러 모두를 포함하는 신규 에러 타입
     * 이 기능들은 개발자가 세밀하게 리소스를 관리하고, 코드의 성능과 안전성을 향상시키는 데 초점을 맞춤

using과 await using 선언

     * using 선언은 동기 리소스에, await using 선언은 비동기 리소스에 사용함
     * 선언된 리소스는 스코프를 벗어날 때 자동으로 Symbol.dispose 또는 Symbol.asyncDispose 가 호출됨
     * 이를 통해 동기/비동기 리소스 누수 문제를 줄이고, 일관된 해제 코드를 작성할 수 있음
     * 이 키워드는 코드 블록, for 루프, 함수 바디 내에서만 사용할 수 있고, 최상위 레벨에서는 사용할 수 없음
     * 예시
          + 예를 들어, ReadableStreamDefaultReader를 사용할 때, reader.releaseLock()을 반드시 호출해야 스트림을 재활용할 수 있음
          + 오류 발생 시, 이 호출이 누락되면 스트림이 영구적으로 잠기는 문제가 발생함
     * 전통적인 방식
          + 개발자는 try...finally 블록을 사용하여 리더의 잠금 해제를 보장함
          + finally 블록에 reader.releaseLock() 코드 작성이 필요함
     * 개선된 방식: using 도입
          + 해제 동작을 포함하는 디스포저블 객체(readerResource)를 생성
          + using readerResource = {...} 패턴을 사용하면 코드 블록 탈출 즉시 자동으로 해제됨
          + 향후 웹 API에서 [Symbol.dispose] 및 [Symbol.asyncDispose] 지원 시, 별도 래퍼 객체 작성 없이 자동 관리 가능성이 있음

DisposableStack과 AsyncDisposableStack

     * 여러 리소스를 효율적이고 안전하게 그룹화하도록 DisposableStack과 AsyncDisposableStack이 도입됨
     * 각 스택에 리소스를 추가하고, 스택 자체를 해제할 때 내부 모든 리소스를 역순으로 해제함
     * 의존 관계가 있는 복잡한 리소스 집합을 다룰 때 위험을 줄이고 코드를 단순화함
     * 주요 메서드
          + use(value): 스택 맨 위에 디스포저블 리소스를 추가함
          + adopt(value, onDispose): 비디스포저블 리소스에 해제 콜백을 묶어 추가함
          + defer(onDispose): 리소스 없이 해제 동작만 추가함
          + move(): 현재 스택의 모든 리소스를 새 스택으로 이동시켜 소유권 이전 가능함
          + dispose(), asyncDispose(): 스택 내 리소스 전체를 해제함

지원 현황 및 활용 가능 시점

     * Chromium 134, V8 v13.8 이상에서 명시적 리소스 관리 기능 이용 가능함
     * 향후 다양한 웹 API와 호환 확대 기대감이 있음

   await using data = await fn()
   await이 좌변과 우변에 모두 등장하는 기적

   https://typescriptlang.org/docs/handbook/…

        Hacker News 의견

     * 이 제안은 ""함수의 색깔"" 문제와 비슷한 느낌 전달. 동기 함수와 비동기 함수의 구분이 모든 기능에 계속 침범. 예를 들어 Symbol.dispose와 Symbol.asyncDispose, DisposableStack과 AsyncDisposableStack 사례 확인 가능. Java가 가상 스레드(virtual threads)로 간 이유에 만족. JVM에 복잡함을 추가함으로써 응용프로그램 개발자와 라이브러리 작성자, 디버거의 부담을 줄여주는 선택이라고 생각
          + 비동기를 숨기면 코드 흐름을 이해하기 더 어려워진다는 점에서 동의하지 않음. 자원이 비동기적으로 해제되는지, 네트워크 문제 등 외부에 영향을 받을 수 있는지도 알고 싶음
          + 요즘 대부분의 언어에서 ""모든 코드를 비동기로 작성하는 게 상식""이라는 현상 정말 짜증. Purescript는 Eff(동기 효과)나 Aff(비동기 효과)로 코드를 작성하고 호출 시점에 선택 가능한 유일한 사례로 봄. 구조화된 동시성(Structured concurrency)은 멋지지만, 실제로는 구조화된 동시성을 얻기 위한 문법적 작업이라기보다는 서버에서 여러 최상위 요청 핸들러를 가지기 위한 작업에 가까움. 병렬 처리를 쉽게 하기 위한 수단에 불과
          + JVM에서 어떻게 구현됐는진 모르겠지만, 일반적으로 멀티스레딩은 정말 직관적으로 다루기 어려운 기술. 온갖 경합조건, 데드락, 라이브락, 기아, 메모리 가시성 문제 등 다룬 책 많음. 이와 비교하면 단일 스레드 비동기 프로그래밍이 훨씬 부담 적음. 함수 색깔 문제를 감수하는 게 멀티스레드 앱에서 ""Heisenbug"" 디버깅하는 것보다 덜 고통스러운 선택
          + Java가 그 선택을 내려서 정말 기쁘다는 감정
          + 정상 실행과 비동기 함수가 서로 닫힌 카테시안 범주(closed Cartesian categories)를 형성하기 때문이라는 설명. 정상 실행 범주는 비동기 범주에 직접 임베드될 수 있음. 모든 함수는 카테고리(즉, 함수의 색깔)를 가지며, 어떤 언어는 이를 더 노골적으로 드러냄. 이는 언어 설계 선택이고, 카테고리 이론은 스레딩을 넘어서 강력하게 활용 가능. Java와 쓰레드 기반 접근법은 동기화 문제에 직면하게 되는데, 이게 특히 어렵다는 특징. JavaScript는 모나딕 카테고리 중 특히 Continuation-passing 방식에 제한을 둠
     * defer 함수를 이용한 using 사용 예제를 봤을 때 매우 신선하게 느껴짐. 다른 많은 사람에겐 이미 직관적일 수 있겠지만, 언급할 가치가 있다고 생각
          + using 제안에 포함된 DisposableStack과 AsyncDisposableStack 활용하면 콜백 등록을 내장 지원. using이 블록 스코프이기 때문에 스코프 가로지르기나 조건부 등록에 필요. 하지만 using 변수는 const와 유사하게 바로 초기화되어야 해서 조건부 초기화가 불가능. 이럴 땐 함수 최상위에 Stack을 만들고 사용하는 자원을 defer로 스택에 올리는 패턴이 필요. 필요한 경우에만 해제시점을 함수 레벨로 쉽게 조정 가능
          + golang과 비슷한 느낌
     * 정말 좋은 아이디어라고 생각하지만,<p>웹 API 스트림 등에서 [Symbol.dispose]와 [Symbol.asyncDispose] 통합이 미래에 가능할 수 있다고 해도, 가까운 미래엔 일부 API와 라이브러리만 기능을 지원하고 나머지(대부분)는 지원하지 않을 상황. 결국 ""using""과 try/catch를 섞거나, 아예 모든 코드에 try/catch를 써서 이해하기 더 쉬운 코드를 선택하는 딜레마. 이로 인해 이 기능이 ""실용적으로 쓸 수 없다""는 평판을 얻게 될 위험성 존재. 결국 실제 문제를 해결하는 좋은 설계임에도 도입이 어려울 수 있다는 점에서 아쉬움
          + 이런 기능을 지원하지 않는 API에는 DisposableStack을 사용해 using을 적용 가능. 여러 자원을 함께 다룰 때도 try/catch보다 훨씬 단순해지는 장점. 런타임만 지원하면 기존 리소스의 업데이트를 기다릴 필요 없이 바로 쓸 수 있다
          + JavaScript에서는 이런 상황이 15년간 반복. 새로운 언어 기능은 Babel 같은 컴파일러에 먼저 도입되고, 그 다음 스펙에 들어가고, 마지막으로 안정적 API와 브라우저에 적용되기까지 3-4년 걸리는 경우 많음. 개발자들은 어차피 작은 래퍼(wrapper)로 웹 API 감싸는 데 익숙하고, 폴리필보단 래퍼가 더 나은 경우 많음. 유용한 새로운 언어 기능이 생겨도 ""사용하기 어렵겠다""고 생각한 적 한 번도 없음
          + 실제로 많은 기능이 이미 폴리필로 구현돼 있어, NodeJS 생태계 대부분이 이 패턴을 사용하며, 사용자는 트랜스파일러로 문법만 맞춰 쓰기도 함. 작년에 관련 발표를 준비하면서 NodeJS나 주요 라이브러리에 이미 Symbol.dispose 지원 API가 꽤 많다는 걸 발견. 프론트엔드에선 생명주기 관리 시스템이 있어서 덜 쓰일 듯하지만, 일부 상황에선 여전히 유용. 테스트 라이브러리나 백엔드에서는 충분히 퍼질 거라 생각
          + TC39는 Rust의 trait/protocol 같은 근본적인 언어 기능에도 집중할 필요. Rust에선 새 trait 정의와 구현이 비교적 쉬운 반면, 동적 언어이자 고유 심볼이 있는 JS라면 훨씬 간단하게 도입 가능. Orphan rule 같은 단점이 있지만, 훨씬 유연한 구조로 발전 가능
          + JavaScript 세계에서는 보통 폴리필로 해결하는 방식
     * C#이 떠오름. IDisposable과 IAsyncDisposable 통해 락 관리, 큐, 임시 스코프 관리 등 추상화에 매우 유용
          + 제안 작성자가 Microsoft 출신이라 문법이 C#과 유사하게 정해졌음. 관련된 깃허브 이슈에서도 일관된 맥락
          + 기본적으로 C#에서 차용된 디자인. 원래 제안이 Python의 context manager, Java의 try-with-resources, C#의 using statement 등도 참조하고 있음. using 키워드와 dispose 훅 메서드는 상당한 힌트
     * JavaScript가 하위 호환성 유지가 중요하다는 건 이해하지만, [Symbol.dispose]() 문법이 어색하게 느껴짐. 배열에 메서드 핸들이 있는 것처럼 혼동. 이 문법이 무엇인지, 더 알아보고 싶다는 궁금증
          + 객체 리터럴에서 좌변에 대괄호로 감싼 동적 키(dynamically computed property)가 ES6 이후 10년 가까이 쓰였다는 설명. 또한 심볼은 문자열로 참조할 수 없기 때문에 동적 키와 메서드 단축 문법을 조합. 근본적으로 새 문법은 아니라는 생각
          + 든든한 자료와 함께, 이는 기존 객체에 심볼 키를 할당하는 방식에서 유래. 자연스러운 흐름
          + 다른 사용자들이 이미 무엇인지 설명했지만, 왜 그런지에 대한 설명은 없었던 듯. 메서드 이름에 Symbol을 쓰면 기존 메서드와 충돌 없이 새로운 API임을 보장. 실수로 클래스가 disposable로 잘못 처리되는 걸 막아주는 효과
          + 다이나믹 프로퍼티 액세스(dynamic property access) 개념 언급. 객체 프로퍼티는 점(.) 혹은 대괄호([])로 접근 가능한 점, 문자열과 심볼 모두 지원. 심볼은 고유 객체로 비교하며, ""[Symbol.dispose]""같은 well known symbol(자주 쓰이는 특수 심볼)로 확장성 보장. Python의 dunder 메서드와 유사한 개념도 설명
          + 이 문법은 벌써 여러 해 사용. JavaScript의 iterator도 같은 방식이고, 거의 10년 전에 도입
     * 자원 관리, 특히 렉시컬 스코프가 특징일 때 JS에 구조화된 동시성을 도입하기 위해 노력해 온 이유 소개. 관련 구조화 동시성 라이브러리도 공유
     * Bun 1.0.23 버전 이상에서 이미 해당 기능 지원. 실험적으로 써볼 수 있음
     * 이렇게 복잡한 코드 스타일로 어떻게 프로그램의 실행 흐름을 이해하고 제어할 수 있는지 도저히 모르겠다는 의문
          + 그게 바로 핵심. 웹 개발의 90%는 쓸모없거나 아무도 원하지 않는 업그레이드이고, 그 결과 생긴 문제를 또 10%의 시간으로 고치는 것이 현실. 낮은 확률로 예전에 작성한 코드를 누가 봐야 하는 상황이 오는데, 여기서 버그를 신입의 입문 과제로 남기는 아이디어 추천. 심지어 20년 된 레거시 시스템도 여전히 사용되는 현실
          + 예시로 제시된 코드는 심각한 문법 오류가 많아 실제 JS와 거리가 멎음. 그리고 JS 개발자들이 이런 식으로 혼합 사용(while, promise chain, finally 등)하지 않으며, await 또는 적절한 예외처리 구조를 사용하는 게 일반적. 잘 설계된 라이브러리에서는 여러 단계의 핸들러를 겹겹이 붙이지 않고, DisposableStack을 활용해 더 간결하게 코딩 가능. 요즘에는 즉시실행 async 함수조차 필요 없는 경우 많음
          + 해당 언어로 프로로 일하면서 그 언어 키워드 의미와 동작에 익숙해지면 코드를 자연스럽게 이해. Haskell 프로그래머도 비슷하게 익숙해짐
          + HN에서 코드 임베드할 때는 각 줄에 2칸 이상 들여쓰기 필요. (코드 이해가 어렵다는 점에는 동의)
          + 들여쓰기가 도움이 된다는 간결한 조언
     * 왜 익명(anonymous) 클래스 소멸자(destructor)로 가지 않았는지, 또는 Symbol 외 구조를 쓰지 않았는지 궁금. 두 Symbol(동기/비동기)이 존재하면 추상화가 새기(누출)되는 문제 제기
          + 소멸자는 예측 가능한(cleanup이 명확한) 동작이 필요한데, 발전된 GC(garbage collector)는 이런 패턴과 맞지 않음. 현대 언어는 scope(범위) 기반 정리(cleanup)를 지원하고, HoF(고차 함수), 특별한 훅, 콜백 등록 등 다양한 방식으로 구현. Python은 초기엔 소멸자 기반(refcount GC)이었지만 한계 때문에 context manager가 도입됨
          + 다른 언어의 소멸자는 GC 타이밍에 따라 동작하므로 신뢰하기 어려움. 반면 dispose 메서드는 변수 스코프가 끝날 때 명확히 호출되므로, 파일 닫기나 락 해제 등에 예측 가능. Symbol 기반 메서드는 기존 기능들과의 충돌을 피하고, 보통은 라이브러리 개발자가 신경쓰면 됨. 동기/비동기 구분이 명확해야 하고, await using a = await b()처럼 약간 낯선 구문이 필요할 수 있음
          + GC 언어에서 소멸자는 동기 호출이 어려워 대부분 비결정적 동작. JS에서는 WeakRef와 FinalizationRegistry가 있지만, Mozilla조차 예측 불가로 사용을 권장하지 않음
          + 이 방식은 클래스 인스턴스가 아닌 대상에도 사용 가능한 점이 강점
          + JavaScript에는 익명 속성(anonymous property) 개념이 없어서 질문 자체가 모호하게 느껴짐. 이 방법 외 대안이 없다는 주장
     * 제안문 첫 예시는 try/finally로 락을 안전하게 해제하는 코드 사례. 이런 패턴이 오랜 실행이 필요한 상황만 중요한지, 브라우저나 CLI 환경에서 에러로 프로세스 종료 시에도 락이 해제되는지 궁금
          + 명세에는 블록 실행이 정상적으로 끝나든, 예외/분기/탈출로 끝나든 무조건 dispose가 실행된다고 나옴. 즉, using이든 try/finally든 동일. 강제 종료(프로세스 강제 종료)는 명세 영역 밖이라 ECMAScript는 관여하지 않음. 예시의 stream은 JS 내부 객체라, 인터프리터가 사라지면 락 개념 자체가 의미 상실. 만약 OS 자원(메모리, 파일 등)이면, 보통 OS에서 일괄 정리하지만, 동작은 플랫폼별로 다름
          + 브라우저 웹페이지는 달리 보면 아주 오래 실행되는 응용프로그램. 심지어 서버 프로세스보다 오래 실행됨. 오류가 난다고 페이지가 죽지 않고, 예외를 포함한 에러 처리는 명확한 규칙에 따라 finally에서 처리. NodeJS에선 기본적으로 에러 시 프로세스가 종료되지만, 서버 상황에 따라 다른 처리가 흔함. 즉, finally에서 해제 함수가 반드시 호출됨

   그동안 리소스 이딴건 신경1도 안쓰고 잘 지내왔잖아. 너 갑자기 왜 이래?
"
"https://news.hada.io/topic?id=20992","$30 홈브루 자동 블라인드 오프너","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          $30 홈브루 자동 블라인드 오프너

     * 블라인드를 자동으로 여닫는 간단하고 저렴한 장치를 여유 부품과 3D 프린터로 주말에 제작함
     * 이 장치는 아주 천천히 (그리고 거의 소음 없이) 블라인드를 여는 데 중점을 둠
     * 주요 구성 요소로 실리콘 모터 마운트와 3D 프린터 출력 부품, 그리고 간단한 릴레이 회로 사용함
     * 토크 측정을 위해 자석 각도 인코더를 활용하였으나, 신호 특성상 추천하지 않음
     * 집 자동화 시스템과 손쉽게 통합하여, 사용자는 스마트폰 등으로 제어 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * 본 프로젝트는 저비용으로 집의 블라인드를 자동으로 열고 닫을 수 있는 홈브루 솔루션을 제작한 경험을 공유함
     * 기존에 남아 있던 부품과 3D 프린터만을 활용함으로써, 누구나 쉽게 시도할 수 있는 아이디어임
     * 완제품 복제용은 아니지만, 영감을 얻을 수 있는 사례로서 가치가 있음

설계 목적과 특징

     * 이 오프너의 본질적 목표는 아주 느리게 (그리고 조용하게) 아침에 블라인드를 열어주는 것임
          + 거주 지역 특성상 해가 너무 일찍 떠서 이를 조절하고자 함
     * 블라인드 완전 개방에는 약 8분 소요
     * 시간 조절(중간에 멈추기 등)로 더욱 천천히 동작하도록 프로그래밍 가능함

하드웨어 구성

     * 실리콘 모터 마운트는 예전에 분해한 Phillips 워터플로서에서 회수한 부품으로, 크기가 적절하여 사용함
          + 이 부품만 별도 구하기 조금 어려움 (비용산정 제외)
     * 나머지 부품은 모두 freecad로 빠르게 설계 후, 3D 프린트해서 비용이 거의 없음
     * 자석 커프는 마그넷이 느슨하게 맞아 처음엔 두 번 3D 프린트함, 나머지는 한 번에 성공
     * 소프트웨어 포함 총 제작 소요 시간은 하루 반 정도임
          + 집 자동화 시스템과의 연동도 몇 분이면 가능함

부품별 설명

     * 마그넷 커프 및 캡
          + 모터 뒷부분에 마그넷 장착, 직접 회전은 안 하지만 실리콘 마운트 내에서 약간 움직이며 토크 추정 및 종료 감지에 사용함
     * 모터와 인코더 마운트
          + 인코더 배선은 진동에 풀릴 우려로 납땜 처리함
     * 2개의 더블스로우 릴레이
          + 모터 양단에 각각 연결, 방향 전환 용도임
          + 릴레이가 모두 동시에 켜져도 양쪽 모터 단자가 5V에 연결되어 안전함
     * 벽 브라켓
          + 모터 마운트와 맞물리도록 설계된 구조물임
     * 전자부품 보호용 임시 상자
          + 미관상 임시 방편

가격 및 주요 부품 목록

     * 모터(기어박스 포함): 약 $15
     * 마그네틱 각도 인코더: 약 $2
     * 릴레이 2개: 4달러 미만
     * ESP8266: 약 $5
     * USB 전원공급장치 및 케이블: 여분 활용 (비용 미산정)
     * 배선 및 페룰: 사소한 비용

조립상의 이슈와 대처

     * 모터와 블라인드 막대 연결은 외과용 튜브 및 실리콘 테이프 등으로 완성
          + 진동은 잘 차단되지만, 힘이 살짝 부족해 여러 층 감아 보강함
     * 모터가 마운트 내에서 다양한 방향으로 비틀릴 때, 마그넷이 인코더 중심에서 벗어나 신호가 불안정해짐
     * 신호가 나쁨에도 불구하고 대략적인 토크 파악은 가능해서, 블라인드를 적정 강도로 조여주거나 멈추게 할 수 있었음
          + 개방은 단순히 8분 구동 후 거의 중간 위치에 멈추는 식으로 제어

동작 예시 및 개선점

     * 자석 인코더 방식은 권장하지 않음(신호 품질 및 일관성 이슈)
     * 그러나 토크 피드백 메커니즘 자체는 매우 유효함
          + 과도한 토크 시 언제나 멈출 수 있음
          + 위치 검출은 시간 제어, 완전 개폐는 토크 기준으로 충분히 구현 가능
     * 더 나은 토크 측정 방법에 대해 아이디어가 있다면 환영

소음 및 사용상 특징

     * 실리콘 마운트 덕분에 모터 진동은 완전히 차단되며 전체적으로 거의 무음임
          + 릴레이 Click 소리만 상자 덕에 약하게 들림
     * 블라인드 자체의 삐걱거림만 남아있으며, 비오는 소리보다 조용함
     * 동작 중임을 알아차릴 수 있는 유일한 증거가 블라인드의 소리임

제어 및 자동화

     * HA(홈 자동화) 시스템에서 ""몇 % 열기"" 명령 전송으로 제어 가능
          + 스마트폰 패널에 오픈/클로즈 버튼 역시 추가됨
     * 평소엔 해질 때 자동으로 닫히고, 원하는 일출 시점에 자동 개방 설정

결론

     * 짧은 주말 동안 만든 프로젝트로 기대 이상으로 성공적임
     * 사진과 기록 작업에 더 많은 시간이 소요됨

        Hacker News 의견

     * 내가 집에서 쓸 수 있는 최고의 홈 자동화가 바로 스마트 블라인드라고 생각함. 스마트 조명, 직접 개조한 스마트 가습기, 스마트 팬, 흡입 공기 펌프, 공기질 모니터링도 있지만, 스마트 블라인드만큼 삶의 질에 영향을 주는 건 없음. 수면 리듬을 꾸준히 맞추는 데 스마트 블라인드만한 게 없음. 밝은 스마트 전구 네 개보다 흐린 날씨의 창문 한 쪽이 훨씬 효과적임
          + 자동 블라인드는 온도 조절에도 큰 도움이 됨. 여름에 남향 창문의 블라인드를 외출 시 자동으로 닫게 하면 햇빛을 잘 차단할 수 있음
          + 창문이 많은 집에서 여러 개의 블라인드를 동시에 여닫는 데 특히 좋음. 우리 집에서 처음 개선한 게 Lutron 블라인드였고, 꽤 비싼 가격에도 선택에 망설임이 전혀 없었음
          + 이 글에서 소개된 블라인드는 흔한 비닐 블라인드고, 빛이 너무 많이 샘. 판지처럼 창문을 완전히 차단할 수 있으면서 매일 열고 닫을 수 있는 두꺼운 블라인드를 찾고 싶었음. 결국 블라인드는 포기하고 잠잘 때는 눈에 셔츠를 덮기 시작했음. 창문을 아예 막아버리는 것도 생각해 봤지만 별로 내키지 않음
          + 내 홈 오토메이션 구성에 대해 더 듣고 싶다면 얘기해 줄 수 있음. 나도 직접 구축을 고민했던 경험 있음
     * 내 방에 자동 블라인드를 설치한 건 완전 당연한 선택이었음. 너무 편리하고, 알람 시계보다 더 유용함. 일주일 동안 블라인드 열고 닫는 시간을 한 번만 설정하면 그 뒤로는 신경 쓸 일이 전혀 없는 무게 없는 생활 변화임. 주말에는 원한다면 수동으로도 할 수 있고, 많은 홈 자동화가 오히려 불편함을 주기도 함. 불편하지 않은 스위치를 앱으로 바꾸는 건 무의미함. 반대로 침실 블라인드는 무조건 해야 할 생활 꿀팁임. 가격도 생각보다 비싸지 않은데, 한 번 설치하고 나면 인지 부담이 0인 생활 개선임
          + 홈 자동화의 핵심은 기존의 ""일반"" 기능을 없애지 말고 자동화로 보강하는 것임. 인터넷 연결이 없으면 동작하지 않는 스마트 스위치는 똑똑하지 않음. 스마트 전구도 기존 전구-스위치 기능을 깨뜨려서 추천하지 않음. 스마트 플러그나 밸브도 같은 이유로 별로임. 수동 조작이 안 되는 장치는 끔찍하다고 생각함
          + 나도 스마트 블라인드를 설치하고 싶음. 어디부터 시작해야 하고, 어떤 솔루션을 선택해야 할지 모르겠음. 블라인드가 인터넷에 연결되지 않는 것만이 유일한 필수 조건임
          + Home Assistant와 휴대폰 앱을 쓰고 있는데, 유일하게 자동화한 스위치는 건전지 타이머가 달린 스탠드얼론 장치로 모터가 일반 스위치를 켜고 끔. 현관등에 적용해서 집에 없어도 자동으로 꺼졌다 켜졌다 하게 함. 할로윈이나 계절 바뀔 때만 수동 조작이 필요함
          + 내 블라인드는 알람 시계 대용이 전혀 아님. 나는 베개로 눈을 가리고 자서 아무리 밝아도 일어나지 않음
          + 나는 일찍 일어나는 걸 바라지 않음. 오히려 방을 완전히 암막하기 위해 방법을 찾고 있음. 아이들이 작은 빛만 들어와도 너무 일찍 깸. 편리함 때문에 블라인드는 갖고 싶지만, 커튼 주변에 케이블이 엉키는 건 싫고 배터리 옵션도 제한적임
     * 아이디어가 떠오르면 바로 만들고 구현하는 마인드셋이 부러움. 자랑하려는 게 아니라, 매일 아침 조용히 도움이 되는 작은 도구를 만드는 취향임. 모터, 실리콘 튜브, 오래된 마그네틱 인코더 등을 조합해서 해 뜰 때 자동으로 커튼을 여는 기기가 만들어짐. 활력이 넘치는 느낌임. 나도 내 생활을 위해 뭔가 만들고 싶어지는 마음임
     * 덜 조악한 것을 원한다면 이걸 추천함 https://www.thingiverse.com/thing:2071225 3D 프린트한 기어박스에 서보가 축과 일렬로 배치됨. 케이블을 잘 설계하면 블라인드 밖에서는 보이지 않음. 나는 esphome과 Home Assistant로 제어하고 수년째 매우 안정적으로 사용하고 있음
          + 소음 차단은 어떤지 궁금함. 원글의 경우 모터와 벽, 블라인드를 부드러운 부분으로 분리해서 모터 진동음이 증폭되지 않게 신경 쓴 점을 마음에 들어했음
          + 만약 이게 네가 직접 만든 모델이라면 설치된 상태의 사진을 첫 번째 사진으로 올려줬으면 좋겠음. 작동 구조가 잘 떠오르지 않음
     * 블라인드와 자동 오프너를 사용할 때 꼭 신경써야 할 아동 안전 요소들이 궁금함. 미니블라인드에서 본 경고 스티커나 디자인 변화를 기억함(예: 여러 개의 줄이 따로따로 분리되어 위험 줄임). 그래도 이것만이 전부일 것 같지는 않음
          + UL 325가 관련 기준이며, UL 웹사이트에 등록하면 읽을 수 있음. 특히 무대 커튼에서 심각한 사고 사례가 있음. 안전은 기능이 아니라 과정의 산물임. 기준은 안타까운 사고 이후 만들어진 것임. 아무리 실력 있고 창의적이어도 추측만으로 안전에 도달하는 건 불가능함
          + 두 가지 핵심은 “아이들이 블라인드를 벽에서 떼어내거나 자기 몸 위로 쓰러뜨릴 수 없어야 함”과 “목에 끈이 걸려 위험해지지 않아야 함”임. 자동 오프너를 쓰면 “기계에 손가락이 끼거나 끈이 얽히지 않아야 함”, “감전 위험이 없어야 함”이 더해짐. 나는 우리 집 블라인드 오프너를 극단적으로 안전하게 만들었음. 줄 하단을 천장에 고정, 줄에 도르래를 달고, 1kg 무게추를 매달고, 고장난 Linktap 밸브에서 떼온 솔레노이드로 핀을 뽑아서 무게추가 줄을 당기게 했음. 모든 장치가 딸아이 손이 닿지 않는 위치에 설치되었고, 반대로 블라인드가 0.5초 만에 빠르게 열려서 아침에 확실히 깰 수 있게 했음. 다시 닫는 것은 수동인데 불편하지 않음
          + 스마트 블라인드가 부적절한 타이밍에 방을 열어버릴 때의 안전 문제도 고민해 봐야 함. 예를 들면 옷 갈아입거나 부부가 있을 때, 아이들이 밖에서 놀고 있을 때 등. ""지금은 안 돼"" 같은 잠금 모드가 필요하지 않을지 궁금함. 아니면 무심코 방이 마치 영화처럼 외부에 공개될 수도 있음
     * 나는 모든 블라인드에 SwitchBot Blind Tilt를 설치해서 사용 중임. 세일을 잘 노리면 개당 50-60달러에 살 수 있음. 충전을 계속 해주는 태양광 패널 포함임. 허브를 추가하면 HomeKit 연동도 가능함 https://us.switch-bot.com/products/switchbot-blind-tilt
     * 모터 전류 측정으로 토크를 더 잘 구할 수 있지 않을까 궁금함. 전문가는 아니지만, 작은 션트 저항을 활용해 전압을 감지하는 방식이 괜찮아 보임
          + 모터 전류는 특히 감속비가 큰 모터에서 토크 측정의 대략적 지표는 될 수 있지만, 만족스러운 수준은 아님. 여기서 구현된 것처럼 출력 토크를 직접 재는 게 훨씬 나음. 시리즈 탄성 메커니즘 방식이 대표적인 방식임. 전류 센싱도 구현이 쉬워서 많이 쓰이지만, 정확한 토크 수치로 변환하려면 감속 모터에서는 보정이 많음
          + 과거에 게이트암/출입 통제 시스템에서도 이 방법을 씀. 전류가 급격히 증가하면 뭔가에 부딪혔다고 판단해서 자동으로 암을 다시 뒤로 움직이도록 설계함
     * 롤러 블라인드는 Ryse SmartShade와 WiFi 허브, Home Assistant를 조합해서 성공적으로 써봄 설치 과정이 간단하진 않았고 10개 창문에 적용했지만 최종적으로는 만족함 YAML(또는 TypeScript)로 자동화 루틴을 직접 작성할 수 있고, Siri 등 음성 비서로도 블라인드 제어 가능함 제품 사이트 문서 사이트
     * 이탈리아에서 흔히 보는 두꺼운 빌트인 셔터(실내에서 평평한 직물 롤로 여닫는 타입)를 위한 비슷한 제품이 이미 있는지 궁금함
          + 독일에도 이 셔터가 있음. 나도 과거에 수동 셔터를 집 전체 전동 롤러로 업그레이드함(단순히 천을 교체해서 전동화만 하는 건 안 되는 것 같음). 전동 롤러로 업그레이드하면 각 스위치 뒤에 Shelly Plus 2PM 기기를 달아서 Home Assistant와 연동 가능함. 일몰 후 일정 시간에 전 셔터가 내려가고, 아침 정해진 시간에 올라감. 수동 제어도 당연히 가능함. ChatGPT를 활용하면 복잡한 YAML 코드를 뽑아 쓸 수도 있음. 상상력이 한계임
          + 내가 동남아시아 어딘가에서도 이런 셔터를 본 적이 있어서 말하는 걸 알 것 같음. 현재 쓰는 스토퍼를 전동으로 교체하는 게 가장 큰 과제라고 생각함
          + ""tapparella motorizzata""라는 키워드로 검색하면 됨
     * 우리 집은 남쪽에 주방, 식당, 거실이 있고 창문이 11개 있음. 소비자용 전동 롤러 셰이드를 쓰고 있는데, 정말 만족 중임. 햇빛이 많이 드는 방이나 전망이 좋은 방에서 버튼 하나로 창문을 열고 닫을 수 있는 것만으로도 삶의 질이 크게 올라감
"
"https://news.hada.io/topic?id=21017","지금이 Free Threading Python에 기여하기 좋은 시점입니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                지금이 Free Threading Python에 기여하기 좋은 시점입니다

  지금이 Free Threading Python에 기여하기 좋은 시점입니다

   파이콘 US 2025 에서 현재까지 파악한 Free Threading Python에 대해서 간단하게 정리해보고 기여에 대해서 정리해보겠습니다.

  Free Threading Python이란?

   Free Threading Python은 파이썬의 GIL(Global Interpreter Lock)을 제거하여 멀티스레드 환경에서 성능을 향상시키는 것을 목표로 하는 프로젝트입니다. 이 프로젝트는 파이썬의 멀티스레딩 성능을 개선하고, CPU 바운드 작업에서 더 나은 성능을 제공하기 위해 시작되었습니다.

   아직은 실험적인 단계에 있으며, 컴파일을 다시 해서 설치하거나 uv 를 통해서 설치해야합니다.

  현재 상황

   저도 테스트만 해보고 있으나 대부분의 순수 파이썬 라이브러리는 사용에 문제가 없다고합니다. 하지만 대부분이 보편적으로 사용하는 라이브러리들은 성능이나 구현문제로 C/C++ 로된 라이브러리를 불러오거나 직접 확장으로 구현해서 사용하기 때문에 이런 라이브러리들은 Free Threading Python을 사용하려면 여러 방안을 써야합니다.

    확장 모듈을 사용하는 경우

   대부분 다음의 방법들을 사용하고 포팅 가이드가 있습니다.
     * C API
     * Cython
     * PyBind11
     * nanonbind
     * PyO3
     * f2py

   CFFI 는 아직 지원하지 않지만 쿼트싸이트의 fork 를 사용해서 지원이 가능합니다.
   지원을 하지 않겠다는 이슈는 있는데 CFFI 는 대부분 인터페이싱으로 사용하기 때문에 이해는 되는 결정입니다. 포크된 CFFI 를 사용하면 Free Threading Python을 사용할 수 있지만 세밀한 구현은 아니라서 성능은 떨어질것 같습니다.

  기여하는 방법

   여기부터는 깊은 구덩이에 몸을 던지는것 같지만 스프린트에 참여했을때는 모두 긍정적이어서 아직까지는 기여하기 좋은 시점인것 같습니다. 기여하는 방법은 다음과 같습니다.

    다음을 참조해서 free-threading-python 호환이 잘되는지 확인합니다

     * Free-threaded Python Library Compatibility Checker
     * 🧵 Free-Threaded Wheels
     * Compatibility Status Tracking#

    테스트 후에 이슈를 등록합니다

   3.13 free-threading python 을 먼저설치하고 라이브러리를 설치후에 테스트를 돌려봅니다.
   가능하면 3.14t 버젼도 해보면 좋겠지만 아직은 베타라서 3.13 버젼을 먼저 해보시는게 좋습니다.

    포팅을 해서 PR을 날립니다

   여기서부터는 조금 어렵습니다. 멀티쓰레드와 여러가지 시스템 콜, 그리고 C/C++ 과 파이썬의 내부에 대한 어느정도 이해가 필요합니다.
   가장 어려운 점은 라이브러리들이 서로 의존성이 있는 경우가 많아서 다른 라이브러리들을 쓰는데 해당 라이브러리가 아직 지원을 하지 않는 다면 거기부터 해야합니다.

   저도 스프린트에 참가하면서 파악정도만 했는데 이런식이 됩니다.
     * fastapi -> uvicorn -> uvloop, cryptography, pycares

    기여하는 방법에 대한 글을 씁니다

   제가 지금 하고 있습니다만 부족하겠죠. 글을 써서 여러군데 올려봅시다.

    free-threading-python 에 대한 사용법을 씁니다

   한국에로된 글이 모자라기 떄문에 성능테스트를 하고 사용법을 쓰고 정리를 해서 글로 올려주는 것이 좋습니다.

  왜 지금 기여해야하는가에 대해서

   저는 오픈소스생태계에 있은지가 대략 25년이 좀 넘은것 같습니다. 저는 대단한 오픈소스 기여자가 아니지만 지인들은 많이 기여를 하고 CPython Core Developer 도 두명이나 있고 기타등등 많습니다. 이 때문에 이분들과 이야기를 하면서 생긴 촉이 있습니다.

   아무것도 없거나 대격변 시기에 기여를 하는것이 좋습니다. 예를 들면 Python 초기에 기여하신 장혜님은 다른것도 많이 하셨지만 unicode 나 한글 코덱등을 하셨습니다. 그때는 아무것도 없었고 주여 기여자들이 한글 코덱을 모르기 때문에 비교적 접근이 쉬웠을것으로 생각을 합니다. 그리고 그 다음 변혁은 python3 로 넘어올때였습니다. 그 이후에는 asyncio 쪽이고 이쪽은 김준기님이 기억에 남고 또 다른 분들도 많습니다. 자 이제 또 새로운 기능인 free-threading 이 나왔습니다. 저는 지금이 기여에 가장 좋은 시기라고 생각합니다.

   다른 언어들의 경우 회사에서 결정하거나 이미 큰 기업이 관리하는 프레임워크에서 변경을 만들기 때문에 쉽지가 않습니다. 물론 파이썬도 매우 접근이 쉬운것은 아닙니다만 수많은 라이브러리와 프레임워크들이 존재하고 지금은 하나씩 모두 포팅이 가능하고 많은 인력이 필요하기 떄문에 지금이 그 시기입니다.

   오타가 많네요. ㅠㅠ 여기는 업데이트를 못해서 블로그에는 업데이트를 해뒀습니다.

   안녕하세요 구글 추천으로 이글 떠서 연락드립니다. 전 외국에서 파이썬 개발5년 (일반 개발 13년) 했는데.. 지금 잠깐 한국에서 쉬는 중입니다. 저 기여하고 싶은데.. 혹시 이메일 부탁드려도 될까요? 연락드리고 여기에 대해서 배우고 싶습니다.

   제 이메일은 josephroh@naver.com 입니다. 감사합니다.

   메일 드렸습니다. 감사합니다.

   파이선이 인기많았던 이유도 이런 멀티스레딩 고려까지는 불필요했기 때문인게 큰데. 그런거 까지 고려할라면 일반인들은 쉽게 사용못하는 언어가 되겠네요

   아직은 옵셔널기능이고 멀티스레딩은 계속 옵션으로 남을 확률이 큽니다. ( 옵션을 키거나 설치를 별도로 하거나 등등 )
   저도 Type 잘 안쓰고 free-threading 은 성능상 이슈로 좀 쓸꺼 같은데 매우 한정적일꺼 같습니다.

   Optional로 생각하지 않고 있습니다. PEP 779이 승인되고 나면 향후 기본 구현체를 free-threaded로 바꾸는게 목표입니다.

   type 처럼 고민하지 않고 써도 되지 않을까 정도의 의도였습니다. 흐흐.

   free threading python 이게 참 쉬운일은 절대 아니겠군요. 판도라의 상자를 여는 그런느낌. 지금껏 숨어있던 온갖 동기화 버그들이 창궐할 가능성. 그것도 아주 가끔씩 런타임에 터지는. 멀티스레딩 개발시 골치 아팠던 문제들이 이제 파이선에서도 본격화될수도 있겠네요. c 계열만 봐도 thread safe 하지않은 함수사용 부분은 바로 문제가 발생되겠군요.

   자동으로 포팅하고 테스트 해주는 Agent 가 나오면 좋겠네요!

   multi thread 이슈라서 쉽지 않을껍니다.

   직접 빌드할 필요 없이 deadsnakes나 윈도우나 맥os 공식 installer에서도 쉽게 설치할 수 있어요!
"
"https://news.hada.io/topic?id=20987","ESA와 IBM, 지구를 ‘직관적으로 이해하는’ 인공지능 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ESA와 IBM, 지구를 ‘직관적으로 이해하는’ 인공지능 공개

   다음은 TerraMind에 관한 에세이를 한국 개발자 및 기술 독자를 위한 구조화된 한국어 요약입니다. 상용화된 기술 뒤에 숨겨진 철학적 의미와 지식 소유권의 확장을 함께 조명합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  1. 도입: 인공지능과 지구 관측의 만남

   IBM과 유럽우주국(ESA)은 지구를 ‘직관적으로’ 이해하는 AI 모델 TerraMind를 공개했습니다. 이는 단순한 컴퓨터 비전 시스템이 아니라, 지리공간 데이터에 대한 심층적이고 통합적인 이해 능력을 갖춘 AI라는 점에서 차별화됩니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  2. 개발 동기: 복잡한 지구 시스템을 이해하려는 시도

   지구는 다양한 데이터 유형(예: 위성 이미지, 기후 패턴, 지형 특성 등)이 서로 얽혀 있는 복합계입니다. TerraMind는 이런 **다양한 모달리티(modality)**를 연결하고 의미 있는 패턴을 포착하기 위해 개발되었습니다. 인간이 여러 감각을 통해 세상을 인지하듯, TerraMind도 **‘Thinking-in-Modalities’(TiM)**라는 조율 방식으로 지구를 ‘생각’하며 학습합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  3. 기술적 특징: 멀티모달 학습과 자체 생성 능력

     * 학습 데이터: 9개 유형, 900만 샘플
     * 성능: PANGAEA 벤치마크에서 기존 12개 AI 모델 대비 평균 8% 이상 우수
     * TiM 조율 기법: 모델이 문제 해결에 필요한 데이터를 스스로 ‘생각하여’ 생성함으로써, 데이터 효율성과 성능을 동시에 향상
     * 기반 모델: IBM과 NASA가 만든 기후 모델 ‘Prithvi’를 확장
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  4. 오픈소스와 지식의 공유 철학

   TerraMind는 Hugging Face에 오픈소스로 공개되었으며, 향후 파인튜닝된 버전들도 배포될 예정입니다. 이는 지식의 소유권을 기업이 아닌 커뮤니티와 지구 전체에 되돌리는 행위이며, 개인 연구자나 개발자도 AI 기반 지구 관측에 참여할 수 있는 길을 연 것입니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  5. 철학적 시사점: AI와 지식 주권(Knowledge Sovereignty)

   TerraMind는 단순한 기술 진보를 넘어, 데이터 소유권과 지식 주권에 대한 새로운 비전을 제시합니다. 즉, 복잡계에 대한 해석을 독점된 폐쇄 시스템이 아닌, 개방된 지식 체계로 전환함으로써, 기후 변화 같은 인류 공동의 문제에 대한 해결 능력을 전 지구적 협력으로 확장하려는 시도입니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  6. 결론: AI는 인간의 ‘이해’를 모방하는 도구

   TerraMind는 단순한 분석이 아니라, 지구에 대한 ‘직관’을 흉내 내는 최초의 시도 중 하나입니다. 이는 인공지능이 단순히 데이터를 처리하는 기계를 넘어, ‘세계에 대한 사유 방식’을 구현하려는 도전임을 보여줍니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   이러한 접근은 지식의 생산과 소유, 활용을 누구나 참여할 수 있도록 민주화하는 과정으로, 미래의 AI 시스템이 지구와 인간의 미래를 공동으로 설계하는 동반자가 될 수 있음을 암시합니다.

   안녕하세요, 올려주신 요약글 잘 읽었습니다.
   좋은 글들을 꾸준히 공유해 주셔서 감사하게 생각하고 있습니다.
   다만 최근 올라온 여러 글에서 AI 결과를 거의 그대로 붙여넣은 듯한 패턴이 보여, 조심스럽지만 운영상 댓글을 남깁니다.
   GeekNews에서는 공식적으로 어떤 형식을 요구하진 않지만, 다른 분들이 읽기에 글의 핵심과 맥락이 자연스럽게 전달되도록 조금만 더 정리해 주시면 좋을 것 같습니다!
   고맙습니다.

   네 명심하겠습니다.
   해커 뉴스와 개인적으로 구독하고 있는 블로그 사이트에서
   흥미 있을 만한 뉴스들을 올리고 있습니다.
   혹시... 글들을 요약할때 프롬프트 추천 해주실 수 있을 까요?
"
"https://news.hada.io/topic?id=21027","Deno의 침체에 대한 소문은 크게 과장된 것입니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Deno의 침체에 대한 소문은 크게 과장된 것입니다

     * 최근 Deno Deploy, KV, Fresh, 전반적인 회사 및 프로젝트 모멘텀에 대한 비판과 우려가 등장함
     * 비판 중 일부는 타당하고, 자체적으로 진행상황을 충분히 공개하지 않아 혼란을 키우기도 했으나, 이 소문 및 비판 중 많은 부분은 근거 없는 추측이나 사실과 다른 내용임
     * Deno 2 출시(2023년 10월 이후) 이후 월간 활성 사용자 지표 기준으로 채택률이 2배 이상 상승함
     * Deno 2의 강력한 Node 호환성이 실제 적용에서 큰 허들을 제거했으며, 플랫폼은 더욱 빠르고 강력하며 간단해짐

Deno Deploy의 변화와 진화

     * 가장 많은 질문 중 하나는 최근 Deno Deploy의 사용 가능 리전 감축에 대한 이유임
     * 감축 배경에는 비용 외에도 실제 사용 패턴과 필요 변화가 있음
          + 대부분의 앱은 모든 리전이 아닌, 데이터와 가까운 지역에서의 속도, 디버그, 규정 준수가 중요함
     * Deploy 출시 이후 25개에서 35개, 현재 6개 리전까지 변동됨
     * 많은 리전이 실제로 거의 사용되지 않아, 오히려 지나치게 분산 시 성능 저하(지연, 용량 문제) 발생함
     * 사용자의 요구에 부합하는 실제적인 “엣지” 비전을 다시 구축 중에 있음
     * 새로운 Deploy 버전 개발 중이며, 플랫폼은 전체 애플리케이션 호스팅 지향으로 진화함
          + 서브프로세스, 백그라운드 작업, OpenTelemetry, 빌드 파이프라인, 자체 호스팅 리전 등을 지원할 예정
     * 곧 앱을 리전에 고정시키거나 자체 클라우드에서 실행 가능한 기능 제공 예정임

KV(키-밸류 스토어) 방향성

     * Deno KV는 설정이 필요 없는, 전역 일관성 보장 및 실시간 기능을 제공하는 간단한 API의 스토어임
     * 세션 데이터, 피처 플래그, 협업 상태 등에 적합하지만, 범용 데이터베이스용은 아님
     * 더 넓은 데이터 관리 니즈에 대해 두 가지 노력을 진행 중임
          + 기존 관계형 데이터베이스의 Deno Deploy 내 통합 강화
          + 컴퓨트와 상태 연계를 단순화하는 신규 프로젝트(Cloudflare Durable Objects에서 영감) 추진
     * 위 방향에 따라 KV는 베타 유지, 중대한 버그/보안 이슈만 지속적 대응
     * 전체 상태 관리 솔루션의 중심이나 진화된 역할은 앞으로 다른 프로젝트가 맡게 될 전망임

Fresh 프레임워크 현황

     * Fresh는 여전히 모든 사내 앱/웹의 기반이면서, 활발히 유지/개선되고 있음
     * Fresh 2에 대한 많은 기대와 오랜 대기 상황을 인지하고 있음
     * 급하게 출시하는 대신 기본 품질과 구조 다듬기를 우선시함
     * 최근 발표된 상세 개선 내용 관련 블로그 글를 참고 할 것
     * 올해 중 안정적인 Fresh 2 배포 예정임

Deno, 런타임 그 이상의 플랫폼

     * Deno는 단순 런타임을 넘어 완전한 JavaScript 시스템 플랫폼임
     * 한 도구 체계로 작성, 실행, 테스트, 배포, 모니터링 등 통합 수행 가능
     * 통합성, 기본 설정, 플래그, 도구간 연결 등이 지속적으로 강화되고 있음
     * 단순한 기능 동등성보다 통합된 생태계 조성을 목표로 함
     * JavaScript 개발의 본질적 품질 향상을 지향하며, 이를 위한 범위 확장에 도전 중임

이 플랫폼의 목표와 이유

     * 스크립트 언어는 빠른 실무 개발에서 불가결한 역할임
     * JavaScript는 표준화, 대규모 생태계, 범용적 확장성 측면에서 미래가 더욱 밝음
     * “배터리 포함” 플랫폼이 필요하며, Deno가 이를 지향함(권한관리, 웹서버, 관측, 린트, 타입체크 등 기본 제공)
     * 조각난 도구 대신 일원화된 경험을 제공함

미래 계획 및 커뮤니케이션 강화

     * Deno는 위축이 아닌 확장 국면에 진입함
     * 플랫폼 속도, 호환성, 완성도 지속 개선 및 JSR의 독립적 거버넌스 성장 중
     * TC39/ WinterTC 등 커뮤니티 협업과 JavaScript 생태계를 위한 다양한 활동 병행
     * Deploy, KV 경험을 바탕으로 지속적이고 분산된 신제품 개발 진행 중이며, 곧 추가 소식 공개 예정
     * 논란이나 불신을 줄이기 위해 커뮤니케이션을 강화하며, 개발자와의 신뢰를 중시할 계획임

   차라리 deno보다 bun이 낫나요?

   Deno의 침체: 전 세계 리전이 6개로 축소
   요 글에 대한 대응인가 보네요

   Fresh 업데이트가 왜 늦어지는 지에 대한 얘기도 별도로 올렸습니다 Deno의 웹 프레임워크 Fresh 2 업데이트

        Hacker News 의견

     * 대부분의 개발자가 단순 스테이트리스 함수만 배포하는 게 아니고, 풀스택 앱처럼 데이터베이스와 통신하는 실제 애플리케이션 구축이 일반적이라는 점이 처음부터 명확해 보였다는 생각. 이 사실을 이제야 깨달은 게 약간 당연하게 느껴짐
     * 최근 Deno, Deploy, KV, Fresh, 그리고 전반적인 성장세에 대한 비판 여론이 있었다는 인식 공유. 성장세에 대한 비판에 대해 특별히 언급이나 답변을 보지 못했는데, 일부러 그런 건지 의문. 비판 중 일부가 유효하다고 했으니, 어떤 비판이 유효했는지, 그리고 이를 어떻게 해결하려는지까지 공개했으면 더 신뢰도가 높았겠다는 생각. 회사가 솔직하게 단점까지 인정하는 모습이 오히려 선택에 있어 긍정적 요인이라고 봄. Migadu의 pro/con 페이지처럼 장단점을 투명하게 밝히는 게 마음에 든 경험 공유
          + Deno 측에서 최근 성장세에 대해 언급한 부분은 출시 이후 6개월 남짓 동안 월간 활성 사용자 수가 두 배 이상 증가했다는 설명임. 하지만 무엇을 기준으로 두 배라는 건지, 어떤 수치를 의미하는지는 공개되지 않았음. 초창기에는 막연한 기대감과 신생 서비스에 대한 신뢰로 인해 긍정적으로 평가받았지만, 이제는 수치나 근거 없는 막연한 성장 기대치와 실제 사이의 괴리감에서 실망감이 생기는 분위기라는 의견
     * 초기에 Deno에 기대했던 이유는 기존과의 호환성에 집착하지 않고 적은 복잡성으로 깔끔하게 새로 시작할 수 있다는 점 때문이었음. Node보다 새롭게 불편한 부분은 있었지만 감당할 만한 수준으로 느꼈음. 그런데 고유의 해결책 대신 점점 Node와의 호환성에 매달리기 시작했고, 지금은 오히려 Node보다 더 복잡하게 느껴지는 이중 구조가 됐다고 생각. Node 패키지가 Deno에서 당연히 작동해야 하지만 일부 미구현 API나 버그로 인해 동작하지 않는 엣지 케이스가 많아짐. 가장 좋아하던 테스트 프레임워크인 AVA도 여전히 지원하지 않음. 예전에는 npm 호환성 계층을 무시하고 Deno 자체만 사용했지만, 이제는 점점 더 귀찮아지고 있음. 커맨드라인 옵션만 봐도 몇 년 새 엄청나게 복잡해졌고, 대부분이 npm 연동을 위한 것이어서 나에겐 불필요한 정보일 뿐임. 내가 Node
       호환성에서 제일 원하는 건 Deno linter에서 ESLint 설정을 지원하는 거였는데, 이건 관심 없어 보임. 그래도 Deno가 성공하길 바라는 이유는 Node의 개선을 유도한다는 점 때문임. 다만 지금의 Deno 방향성은 처음의 목적과 일관성이 느껴지지 않음
          + AVA 지원 여부 및 ESLint 설정 지원 여부를 최근에 확인해봤는지 질문. 공식 문서 기준으로 AVA 지원이 언급돼 있고, 대다수 Deno 개발자는 기본 내장 테스트 기능을 더 많이 활용하는 듯함. ESLint와 연동되는 커스텀 룰, 플러그인, 세팅 등이 지원된다고 공식 문서에 명시되어 있음. 약 6년간 Deno를 써오면서 별도 테스트/린트 셋업 없이 기본 제공 기능이 만족스럽다는 소감
     * Deno가 방향성을 잃었다고 느낌. 처음에는 Rust로 만들어진 안전하고 빠른 JS/TS 런타임이라는 심플한 포지셔닝이었는데, 지금은 웹사이트 ‘Products’ 드롭다운에 여러 제품이 난잡하게 추가된 상황. Vercel이 NextJS에 이어 데플로이 플랫폼을 만든 방식을 따라가려 한 것 같음
          + 이런 변화가 Deno 자체의 의도라기보다 JavaScript와 Node 커뮤니티가 더 빨리 발전하면서 따라잡은 영향도 크다고 생각. 초창기에는 Deno만의 혁신이 멋있다고 느꼈는데, JS/Node 쪽도 빠르게 개선되다 보니 차별점이 줄어든 느낌
     * Deno가 Node와의 호환성을 추가하며 처음 약속을 포기한 시점부터 기대감이 사라졌음. 나에겐 Deno의 핵심 매력이 Node에서 원치 않는 복잡성과 과거 유산을 제거했다는 점이었는데, 지금은 빌트인 타입스크립트와 퍼미션 같은 몇 가지 소소한 차이만 남고 Node와 다를 게 없음. bun.sh도 마찬가지로 Node 호환성을 제공함. 혹시 Node 호환성을 추구하지 않는 서버사이드 타입스크립트 스크립팅 엔진을 아는지 궁금함
          + npm 호환성은 기능 추가이기 때문에 굳이 그것을 잃는다고 생각하지 않음. 굳이 Node API를 쓰지 않아도 되고, jsr.io에서 원하는 라이브러리만 써도 충분함. 실제로는 Node와 차별화된 개발 경험을 여전히 Deno에서 누릴 수 있다는 주장. 다만 완전한 ‘순수성’을 원한다는 이들은 많지 않으니, 차라리 대중성과 실용성을 선택하는 게 다행이라는 견해
          + Node 호환성을 추구하지 않는 타입스크립트 런타임을 찾는 이유에 의문 표함. Node에도 여러 문제는 있지만 그럼에도 대중적으로 널리 쓰일 만큼 충분히 쓸 만하다는 점을 강조. 실용적 대안을 만들려면 (1) 대규모 마이그레이션을 감수할 만큼 강한 장점이 있거나, (2) 최소한의 마이그레이션 비용과 확실한 개선점(성능, 신뢰성, 사용성)이 있어야 함. 하지만 Deno는 이 두 개 모두 어정쩡하게 놓침. 기존 Node 코드를 돌릴 수 없는 반면, 혁신적인 장점이 충분하지 않아 ‘이상주의자’나 ‘취미 개발자’만 모으는 한계를 가진 접근이라는 생각
          + Node 호환성을 추구하지 않는 타입스크립트 런타임으로 Cloudflare Workers workerd가 떠오르지만, 근본적으로 범용 백엔드 런타임은 아니고, 실질적으로 제공되는 기본 패키지나 내장 기능이 거의 없는 한계가 있음
          + 본인은 JSDoc 사용을 선호함. Node와 상관없으면서도 컴파일 체인 복잡성 없이 비슷한 장점이 있음
          + 백엔드에서 JS에 얽매일 필요 없다면 굳이 TypeScript 대신 더 나은 대안을 고려하는 게 합리적이라는 입장. 스택 전반을 통제할 수 있다면 굳이 Compile-to-JS 언어에 머무를 이유가 없음
     * 최근 글은 과거 https://news.ycombinator.com/item?id=43863937에 대한 반응일 것 같다는 생각
     * CEO가 작성한 글이긴 한데 Deno에 대한 구체적인 비판보다는 내부 결정의 정당화에 집중되어 있음. 그럼에도 불구하고 Deno 제품군은 Deno 환경에서는 꽤 잘 작동하는 듯한 인상
          + Deno 관련 어떤 비판이 해결되지 않았다고 생각하는지 재차 질문
     * 아직 신뢰나 확신이 들지 않는 상황. Deploy가 어떻게 나올지는 곧 알 수 있을 것 같지만, KV는 베타 단계에서 추가 발전 의지가 없다면 새 프로젝트에서는 쓸 이유가 전혀 없다는 생각. Fresh는 Q3 말쯤 알파로 리팩토링된다고 하는데, 사실상 기본만 제공했던 프레임워크였고, 그나마 눈에 띄는 빌드/컴파일이 없는 구조도 사라짐. 런타임은 계속 개발 중이지만, “타 런타임과의 기능 동등성을 추구하지 않는다”는 선언이 무색할 정도로 릴리즈 노트에서는 Node/NPM 호환성에 집중되어 있는 게 흥미로움
          + KV의 지속적인 개발 중단 결정이 정말 안 좋은 선택이라는 생각. 기업들이 KV를 쓰지 않는 건 기능이 나빠서가 아니라 베타 꼬리표 때문이라는 점을 지적. 본인은 Cloudflare Workers KV를 많이 써왔고, Durable Objects에는 큰 관심이 없어서 Deno KV가 기대됐었지만, 앞으로는 고려 대상에서 제외해야 할 듯한 아쉬움. 신제품을 발표하고 바로 방치한다는 인상이 전략적으로도 정말 안 좋아 보인다는 평가
          + KV 활용을 고민하다가 전망이 안 보여서 대안을 고려하게 됐다는 솔직한 소감 공유
     * 대부분의 개발자가 단순 스테이트리스 함수가 아니라 풀스택 앱, 즉 데이터베이스와 긴밀하게 연동되는 앱을 배포한다는 점에 대해 실제로 서버리스 진영 전반에 해당하는 이야기인지 궁금증. 만약 그렇다면 원래 서버리스 운동의 의도와 부합하는 건지, 혹은 단순히 도커/쿠버네티스 같은 복잡한 인프라를 피하고자 선택하는 것인지 의문
          + 내 감으로는 많은 사람들이 모던화된 Heroku 같은 경험, 즉 관리형 RDBMS와 오토스케일링 가능한 서버 세트를 원한다는 느낌. 대다수 회사는 초대규모 스케일이 꼭 필요하지 않기 때문
     * Deno Deploy에서 지역 수 감소에 대한 질문을 자주 받는다는 설명. Deno 측은 대부분의 앱이 전 세계 모든 곳에서 동작할 필요 없이, 데이터와 가까운 곳에서 빠르고, 디버그가 쉽고, 로컬 규제에만 잘 맞으면 된다고 최적화 방향을 설명함. 하지만 본인은 실제로 Deno Deploy의 지역 위치가 충분히 가깝지 않아 성능상 문제가 우려되어 사용하지 않았음. 데이터와 사용자에게 더 가까운 옵션이 이미 다양하고, 규제 준수도 대부분 국가 단위에서 충분하다는 점에서 이 최적화 방향이 납득 가지 않는다는 의견
"
"https://news.hada.io/topic?id=20996","당신의 '화목한' 팀이 실패하는 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          당신의 '화목한' 팀이 실패하는 이유

     * 심리적 안전은 갈등 회피가 아니라, 아이디어 도전을 통해 팀을 더 강하게 만드는 환경에서 나옴
     * 팀원 모두가 겉으로 조용한 회의와 무난한 분위기를 보인다고 해서 효과적인 팀이라는 의미가 아님
     * 생산적인 불일치가 가능한 팀은 문제를 빨리 알리고 의견 충돌을 허용하는 특징을 가짐
     * 비판적 사고에는 마찰이 필요하며, 솔직한 의사소통이 부족한 팀은 잠재적 문제를 방치함
     * 리더는 취약성 공개, 토론 규칙 마련, 챌린저 격려로 건강한 논쟁 문화를 이끌 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

모두가 잘 지내는 분위기와 심리적 안전감의 차이

     * 심리적 안전감이란 팀이 충돌 없이 화목한 상태라고 오해하는 경우가 많음
     * 많은 리더가 구성원이 절대 목소리를 높이지 않고, 모두가 동의하며, 의견 충돌이 없는 팀을 자랑으로 여김
     * 하지만 심리적 안전감의 본질은 갈등 회피가 아니라 아이디어를 자유롭게 도전하고 토론하는 환경 조성임
     * 하버드 경영대학원 Amy Edmondson 교수는 심리적 안전감을 “아이디어, 질문, 우려, 실수에 대해 처벌이나 수치심을 느끼지 않을 것이라는 믿음” 으로 정의함

심리적 안전감이 높은(건설적인 갈등이 있는) 팀의 특징

     * 발언이 자유롭고 뜨거운 논쟁도 괜찮으며, 그 결과 팀이 더 강해짐
     * ""그건 틀린 것 같다""라고 말해도 배제될 걱정이 없는 분위기가 있음
     * ‘내 생각이 틀릴 수 있다’고 편하게 말하고, 개인이 아닌 아이디어의 내용 자체가 평가 및 논의 대상이 되며, 제안자의 신분과 상관없이 도전이 가능함
     * 실수 역시 학습의 기회로 활용하고, 다양한 관점을 장려하는 문화를 가짐

  생산적인 토론을 실천하는 팀의 구체적 특징

     * 문제 조기 인식: 엔지니어가 문제가 심각해지기 전에 먼저 이슈를 언급함
     * 아이디어에 대한 적극적 토론: 시니어 개발자 두 명이 격렬하게 설계논쟁을 해도, 그 다음날 협력에는 전혀 문제가 없음
     * 문제에 집중하는 태도: ""이 접근 방법은 확장성에서 문제가 있을 것 같음""처럼, 사람이 아니라 문제 자체에 집중함
     * 실수는 학습의 기회: 장애가 발생했을 때, 실수를 한 엔지니어가 직접 postmortem을 주도함

'착한' 팀이 놓치는 숨은 비용

     * 겉으로 평화로운 팀이지만, 대다수의 경우 평균적인 결과물만 생산함
     * 왜냐하면 비판적 사고에는 일정 정도의 마찰이 필요하기 때문
     * 표면적인 합의 속에 실제로는 갈등이 숨어 있어, 회의에서는 동의해도 실제 작업에서는 각자 다르게 행동하는 사례가 발생함
     * 의사소통 부족이 핵심 문제로, 건설적인 논쟁이 부족해 최종 성과가 저하됨

심리적 안전감과 갈등의 균형 잡기

     * 환경 조성을 위한 세 가지 핵심 실천 방안
          + 본인도 모르는 것이 있음을 솔직하게 인정하는 자세(취약성 드러내기)
          + 논쟁을 위한 명확한 규칙 세우기(사람이 아닌 아이디어에 집중, 토론과 결정 분리)
          + 문제를 제기하거나 어려운 질문을 던진 구성원을 공식적으로 칭찬함(경계 신호 역할)

결론: 건강한 충돌이 진정한 성장과 혁신으로

     * 실제로 갈등을 자유롭게 드러내는 팀이 오히려 장기적으로 갈등의 질이 더 낮아짐
     * 사소한 의견 충돌이 쌓이지 않고 즉각적으로 해소되어 신뢰와 협력이 강화됨
     * 최고의 엔지니어링 팀은 조용하지 않으며, 기술적 논쟁과 다양한 시각을 환영하는 특징이 있음
     * 팀 내에서 자유롭게 토론하고, 서로 존중하는 문화가 진정한 심리적 안전감임
     * 검증받지 않은 코드와 아이디어가 문제를 일으키듯, 논쟁 없는 아이디어 역시 실패를 초래함

   심리적 안전감도, 건설적 논쟁도 결국은 ‘실행’을 위한 연료입니다.
   아이디어가 살아 움직이려면 결국 누군가는 밀고 가야 하고, 그 행동이 반복되어야 신뢰가 쌓이죠.
   실행 없이 논쟁만 반복된다면, 아무리 안전한 분위기여도 팀은 제자리에 머뭅니다.
   좋은 문화란, 말이 아니라 행동으로 검증되는 법입니다.

   실행력이 뒷받침되는 사람끼리는 건설적인 논쟁이 필연적으로 따라올 수밖에 없습니다.

   보통.. 팀장이 보수적이고 책임 회피형이면서 일을 죄다 위임하는 유형이면 알아서 다들 말없이 착해지더라구요.

   솔직한 피드백이 중요한 이유

   의견 제시의 문턱이 낮을수록 좋죠. 기준과 균형을 찾기가 힘들어서 그렇지.

        Hacker News 의견

     * 20년 넘게 IT 업계에서 일해온 나의 경험에서, '격렬한 토론'을 하는 것이 꼭 높은 성과를 내는 팀이라는 의미로 혼동하는 사람들이 많음. 갈등이 많은 저성과팀에서도 종종 이런 모습이 보임. 매니저들이 공개적으로 부정적인 피드백을 주거나, 베테랑 개발자가 신입에게 싫으면 나가라는 식으로 말하는 등 팀 내 분열이 심했음. 철저하게 훈련된, 논리를 잘 구사하는 사람이 '강한 의견, 느슨한 고집'이라는 태도를 보이며 허술한 아이디어도 단호하게 끝까지 밀어붙이다가, 모든 사람이 지쳐 포기하게 만듦. 실제로는 실시간으로 논박이 거의 불가능하므로, 이들의 자신감이나 열정을 실력(맞음)으로 오해하는 매니저도 많음. 열띤 논쟁이 반드시 고성과를 담보하는 것은 아니라는 관점임
          + 내 경험에 따르면, 위 예시에서 문제가 되는 것은 실시간 입증불가가 아니라, 잘 말하는 사람들이 잘 듣지 않는 것임. 서툴게 표현된 주장이라도 그 본질을 캐치하는 역량이 중요한데, 전달력 부족을 이유로 무시하는 것은 오만임. 본인이 잘 듣지 않으면서 '강한 의견, 느슨한 고집'이라 자처할 수 없음. 증거가 어렵게 나오는 것만 받아들이는 건 '강한 의견, 꽉 쥔 고집'임. 결국 열띤 논쟁이 자주 있다면, 오히려 그 자체가 팀의 기능장애 신호임. 정직하고 침착한 토론이 팀에 더 생산적임
          + 저자도 발표 중에, 문제에 집중하는 태도를 강조함. 예를 들어 '이 방식이 확장성에 한계가 있다'는 식으로 방향을 잡지 않고, 개인의 아이디어 자체를 깎아내리면 논의가 바로 독성으로 바뀜
     * 25년간 고성능의 C++ 영상처리 파이프라인 개발팀을 이끌었던 경험이 있음. 국제적이며 학제적인 대규모 팀의 일원이었고, 세계적으로 유명한 이미징 회사 소속이었음. 다양한 분야의 전문가와 협업했기에, 모두와 잘 지내는 것이 쉽지는 않았음. 모두가 '정답'을 가지고 있다고 생각했고, 각자 최고의 결과물을 내겠다는 열정이 있었음. 그러다 보니 당연히 열띤 논쟁도 자주 있었음. 대부분 매우 훌륭한 결과를 냈고 문제의 원인이 팀 내부 갈등은 아님. 내 경험상 창의적이고 열정 넘치는 고능력 팀은 꽤 혼란스러울 수 있고, 이를 관리하는 일은 정말 까다로운 작업임
          + 25년이라는 긴 시간 동안 그렇게 관리가 가능했던 구체적인 규칙, 작업흐름, 팀 문화 등이 궁금함. 또 팀원 교체가 얼마나 있었는지, 첨예한 인재 충돌을 겪는 팀을 관리하는 노하우가 궁금함. 마법 같은 해법이 없겠지만, 실제 사례가 큰 도움이 될 듯함
     * 이 글에서는 '격렬한 논쟁이 없는 상태'를 '건설적인 비판적 토론의 부재'와 혼동한 것 같음. 내 경험상 신뢰가 높은 성숙한 팀에서는 의견이 무시될 걱정이 없기에 과열된 논쟁이 생기지 않음. 서로 반드시 나의 의견을 들여주고 고찰할 것을 믿기에 흥분할 이유가 없음. '아무도 질문하지 않은 코드는 운영 환경에서 꼭 문제를 일으킨다'는 말의 의미는 이해가 안 됨
          + 아마 '누구도 비판하지 않은 코드는 결국 터진다'는 의미 같음. 하지만 항상 그런 것도 아님
          + 신뢰 높은 팀에서 토론이 뜨겁지 않은 현상은, 위험도와 불확실성이 상당히 낮을 때 더 잘 나타남. 만약 모든 사람이 한계까지 직관과 논리를 밀어붙여야 하는 상황이라면, 팀 내부 각 노드 간 소통량이 불균형하게 될 것임. 책임감이 리더십을 유발하고, 여러 관점이 부상과 쇠퇴를 반복하는 구조일 것임. 항상 논의 온도가 낮다면, 오히려 팀이 자기를 충분히 도전하지 않는 건 아닌지 고민이 필요함
          + 기사에서 그런 관점에 대해 정면으로 반박하고 있는 듯함. '겉보기로 평온한 팀'은 사실 갈등을 회피하기 위해 진짜 의견 충돌을 피한 것일 수 있음. 합의가 겉으로 드러나지 않을 뿐, 근본적 문제는 묻혀 있다는 것임
          + 실제로 아무도 호출(비판)하지 않은 코드나 아키텍처가 엉망일 때가 있음
     * 다시 읽어보니 글의 논조가 왜 처음엔 오해를 불렀는지 알겠음. 실제로 저자가 정의한 심리적 안전(PSYCHOLOGICAL SAFETY) 자체는 올바른데, '착한 팀(nice team)'이 심리적으로 안전하다고 오해하는 사람들이 많다는 걸 지적하고 있음. 모두가 고개만 끄덕이면 결코 안전하다고 느끼는 것이 아님. 갈등과 안전은 서로 대립하는 게 아님. 진짜 심리적 안전의 요점은 모두가 기꺼이 생산적 갈등에 참여할 수 있는 환경 조성임. 모든 갈등이나 합의가 생산적이지는 않음. 심리적 안전이란, 동의와 비동의를 모두 마음 놓고 할 수 있는 팀문화 구축임
          + 모두가 동의하는 팀은 실제로 그만큼 위험한 팀임. 그래서 상사가 말만 하면 모두가 맞장구치게 되는 현상임
     * '아이디어는 발화자가 아닌 아이디어 자체로 평가한다'는 점에 크게 감명받은 사람이 있을까? 이 글 전체가 마치 돈키호테가 풍차와 싸우는 느낌임. 즉, 실제로 현장에선 이미 널리 알려진 상식 논쟁에 불만을 품는 엔지니어에게 타겟을 둔 듯함. 아니면 권위적인 독재자식 리더를 겨냥했을 수도 있는데, 그런 사람이 이 글을 들을까? '나의 최고팀은 언제나 조용하지 않았다. 각기다른 시각이 환영받는 여럿의 열띤 논쟁, 서로 존중하는 비동의라는 문장 등…' 누가 다양한 관점의 존중을 반대한다 말할 수 있을까? 사실, 한 차례 전의 논의가 훨씬 더 사유거리를 제공했다고 느낌
          + '이 논쟁이 특별한 통찰력을 주는가'라는 부분에 대해, 실제로는 매우 기본적인 내용이지만, 예전에는 체크리스트 하나로 생명을 구하거나, Toyota 사원이 어떤 의심이 들면 생산라인을 멈춰도 되게 만드는 등의 사례가 대표적임. 사실 IT 분야에서도 팀과 타인(특히 DEV/TEST 사이)을 무시하는 일이 엄청 흔함. 문제를 만든 사람이 이 글을 읽지는 않겠지만, 그 팀 내 다른 사람이 언젠가 이런 것이 정상 아님을 깨닫고, 다른 식으로 일하게 되거나 변화가 일어날 수 있음. 겉으로 '다른 관점의 존중을 반대'라고 말하는 경우는 없지만, 실제로 타팀을 '저 멍청이들'이라고 내심 칭하는 경우가 존재하고, 그런 마인드로는 생산적 논쟁 자체가 막힘
          + 어떤 팀에 속해 있는지 먼저 파악하는 게 중요함. '조용한 합의'가 가치라고 주장하는 팀은, 사실상 소수만의 이기적인 결정이 사전 협의되고, 전체 회의에서 새로운 의견 개진을 원하지 않는 경우가 대부분임
     * 친절한 비판적 친구가 주는 피드백이, 무비판적 친구보다 더 많은 배움을 준다는 통찰 체득
     * 모든 관계에서 그러하듯, 목표는 '이기는 것'이 아닌, 모두의 필요가 충족되는 환경 조성임
     * 팀원 대부분이 목소리는 크지만 매번 틀린 의견만 내세울 때, 유일하게 제대로 된 시각을 가진 사람이 어떻게 버텨야 하는지에 대한 고민
          + 가장 빠른 해결책은 완전히 관여를 멈추는 것임. 그 사람들이 진짜 계속 틀린다면, 내 개입 없이도 악화가 가속화될 것임. 혹은 상대의 힘을 역이용해서, 그릇된 아이디어의 자기파괴를 유도할 수도 있음
          + 리더가 아니라면 옳다고 해도 의미가 없고, '내가 맞았다'는 사실도 보상되지 않음. 어떤 프로젝트는 외형만 챙기고 생산성은 중요하지 않을 때도 있음. 목적 자체를 착각했다면 '옳게' 행동한 게 아님. 그럼 그냥 돈만 받고 계속 하거나 떠나는 결정을 내릴 수 있음
          + 거리에서 모두가 반대방향으로 달리고 있다면, 가끔은 내가 역주행인지 의심할 필요도 있음
     * 한때, CTO(공동창업자)가 극도로 유독한 인물이라 팀내에 행복한 거품이 만들어졌던 적도 있었음. 그땐 최상의 팀이라고 느꼈지만, 그 거품이 갑작스럽게 깨지면서 진짜 끔찍한 상황이 드러났던 경험임

   ""타팀을 '저 멍청이들'이라고 내심 칭하는 경우가 존재하고, 그런 마인드로는 생산적 논쟁 자체가 막힘

   어떤 팀에 속해 있는지 먼저 파악하는 게 중요함. '조용한 합의'가 가치라고 주장하는 팀은, 사실상 소수만의 이기적인 결정이 사전 협의되고, 전체 회의에서 새로운 의견 개진을 원하지 않는 경우가 대부분임""

   2차대전 일본군같은 대본영 회의를 해서는 안되겠죠. 타인이 혹시라도 나를 따돌리고 점수따거나 승진하거나 잘될까봐 내 라인 안팎으로 내심 적대적인 비협력적 문화도 문제일 겁니다
"
"https://news.hada.io/topic?id=20979","죽은 별은 복사를 하지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            죽은 별은 복사를 하지 않음

     * 최근 논문에서 모든 무거운 물질이 Hawking 복사를 방출하며 죽은 별도 결국 사라진다고 주장함
     * 이러한 주장은 바리온 보존 위반을 내포하며, 기존 이론과 충돌함
     * 전문가들은 이 논문의 계산법이 부정확하다고 비판하며, 실제로 정적인 질량의 중력장은 입자-반입자 쌍을 생성하지 않음
     * 수십 년 전 Ashtekar와 Magnon 등은 정적 시공간의 진공이 안정적임을 엄밀히 증명함
     * 최근 잘못된 보도를 중심으로 과장된 뉴스가 넘쳐나지만, 기존 물리학적 원칙은 변하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

죽은 별의 Hawking 복사 주장

   최근 Michael F. Wondrak, Walter D. van Suijlekom, Heino Falcke 세 연구자는 블랙홀뿐 아니라 모든 무거운 물질이 Hawking 복사를 방출한다고 주장함
     * 이들은 차가운 죽은 별조차도 Hawking 복사를 하여 서서히 질량을 잃고 결국에는 존재가 사라질 것이라 주장함
     * 이러한 주장에 따르면 우주의 소멸 시점이 기존 예상보다 훨씬 앞당겨질 가능성이 언급됨

   이 이론은 기존의 바리온 보존 법칙을 위반함
     * 별을 이루는 양성자와 중성자의 소멸 메커니즘에 대한 명확한 설명이 없음
     * 이들은 별의 중력장이 입자-반입자 쌍을 생성해 별이 질량을 잃는다고만 주장함

전문가들의 반응

   만약 전문가들이 이 주장이 타당하다고 여긴다면, 이는 양자 중력이론 분야에서 혁명적인 사건이 될 내용임
     * 기존에는 정지한 물질은 Hawking 복사를 방출하지 않는 것이 정설이었음
     * 만약 이론이 맞다면, 곡률 시공간에서의 양자장론은 바리온 수 보존이 깨질 수밖에 없으므로, 물리학에서 큰 충격이 됨

   하지만 실제로는 이 논문들이 물리학계에 거의 영향을 끼치지 못함
     * Antonio Ferreiro, José Navarro-Salas, Silvia Pla 등의 논문에서 이들이 단순화한 근사법이 심각한 오류를 내고 있음을 지적함
     * E. T. Akhmedov 등도 비슷한 비판을 제기함

   진정한 전문가들은 정적인 질량의 중력장이 입자-반입자 쌍의 생성을 유발하지 않음을 이미 1975년 전부터 알고 있었음

언론 보도와 대중의 오해

   Wondrak 등이 제출한 논문은 전문적 심사를 거쳤으나, 실제로 해당 분야 전문가에 의해 검증된 것이 아님
     * 유명 물리학 저널에 실렸다고 해서 무조건 신뢰할 수 없음
     * 이 주장을 다룬 언론 기사들은 사실관계를 제대로 확인하지 않고 자극적으로 보도함

   대표적인 기사 예시
     * CBS News: “우주가 기존보다 훨씬 빨리 소멸할 것”
     * Space.com, Forbes 등 여러 매체에서 이슈를 부각하여 대중적 혼란을 가중시킴
     * 허위 정보가 빠르게 확산되어 올바른 사실이 전달되기 어려운 상황임

엄밀한 이론적 배경

   실제 Ashtekar와 Magnon(1975)은 곡률 시공간에서의 양자장론을 엄밀히 연구함
     * 정적인 시공간에는 ‘어디서나 시공간 대칭(timelike Killing field) ’이 존재한다면 진공 상태가 안정적임을 증명함
     * 이 조건에서 입자-반입자 쌍의 자연 발생(자발적 생성) 이 일어나지 않음

   Robert Wald의 교과서에서도 이러한 내용이 상세히 다루어짐
     * 곡률 시공간에서의 에너지 개념 정의, 진공의 안정성, 입자/반입자 구분의 엄밀한 방법을 설명함
     * Schwarzschild 해(즉, 정지 블랙홀)도 Killing field를 갖지만, 사건의 지평선에서는 성질이 달라져서 이 결과가 직접 적용되지 않음

   Ashtekar와 Magnon, 그리고 Wald의 연구에 따라, 정적 천체의 중력장은 입자 생성 현상을 설명하지 않음이 정설로 자리잡게 됨

결론 및 현재 상황

     * 별이나 물질의 정적인 중력장은 Hawking 복사나 입자쌍 생성을 유발하지 않는 것으로 수십 년간 확립된 내용임
     * 최근 논문에서 제시한 근사계산 방식은 이와 상충하며, 그 오류도 이미 여러 곳에서 지적됨
     * 이 사안은 근사방법의 결함 때문에 오랜 논의를 필요로 하지 않음
     * 50년 이상 전에 이미 논의가 정리된 문제여서, 새로운 결과라 하기도 어려움
     * 최근 논문은 기존 이론의 깊이를 따라가지 못하는 과장 및 오해의 소지가 있음

참고 자료

     * Abhay Ashtekar, Anne Magnon: Quantum fields in curved space-times (1975)
     * Robert Wald: Quantum Field Theory in Curved Spacetime and Black Hole Thermodynamics (1994)
     * Valeria Michelle Carrión Álvarez의 박사학위 논문(2004) 등

   죽은 별을 비롯한 정적 천체는 Hawking 복사를 방출하지 않는다는 점이 수십 년간의 이론적, 실험적 연구로 명확히 확인된 상황임

        Hacker News 의견

     * 우주에는 아직 우리가 놓치고 있는 무언가가 있다고 생각하고, 앞으로 수십억 년 동안 이어질 거대한 이론들은 그걸 포함하지 못하고 있다는 느낌을 받는 감각
     * 중력 퍼텐셜 웰의 탈출속도가 빛의 속도보다 빠르지 않다면, 이 상황에서 Hawking radiation이 어떻게 발생하는지 의문점 제기. 가상 입자 쌍 모두 살아남고, 한쪽이 사건의 지평선을 넘지 않았다면 사라지는 이유가 없다는 문제 제기
          + Hawking radiation에 대한 '쌍 중 한 입자가 사건의 지평선에 갇힌다'는 설명은 실제 현실의 단순화된 비유에 불과하다는 점 상기. 실제로는 사건의 지평선에서 일어나는 입자(또는 장)의 산란이 진짜 현상이라는 설명. Hawking 자신도 이런 그림은 단지 휴리스틱적 비유일 뿐 절대로 문자 그대로 받아들이면 안 된다고 강조했다는 지적
          + 실제로 저 비유는 Hawking radiation의 작동 원리를 설명하려고 만든 허구적 예시일 뿐이며, 과학 저널리스트들을 만족시키기 위한 과장된 은유에 불과하다는 의견
     * 왜 질량이 큰 천체들은 중력파 방출을 하지 않는지 간단히 이해할 수 있는 방법에 대한 질문. 가속받는 관측자는 Unruh 효과로 열복사를 본다는데, 행성 위에 서 있으면 중력에 의해 가속 중이므로 Unruh 복사를 보게 되는지, 이것이 Hawking radiation과 어떤 관련이 있는지 궁금증 제기
          + 일반인 입장에서 보면, 행성 위에 서 있을 때는 실제로 가속받고 있는 것이 아니고, 바닥이 받쳐주고 있으므로 자유 낙하 중이 아니라면 실제 가속이 없다는 지적
     * 며칠 전 비슷한 논평을 썼다는 사실을 즐겁게 언급. 해당 논문의 내용은 터무니없고, 프리프린트 서버엔 동료 검토를 통과하지 못할 논문이 올라올 때가 있다는 점 경고. 보도매체는 이에 대해 조심할 필요가 있다는 점 강조
          + 해당 논문은 PRL에도 실렸기 때문에 자신도 비슷한 논문을 써서 보냈으면 경력에 도움이 되었을지도 모르겠다는 농담
          + 논문이 엉터리인지 아닌지와 상관없이, 비판적 평가에서 인용된 ‘충격적인 발견에 관한 기사라면 과학 기자는 반드시 전문가에게 사실 확인을 해야 한다’는 태도에 대해, 그런 태도라면 과거 전문가들이 지구가 평평하거나 태양이 지구를 돈다는 잘못된 믿음을 퍼뜨렸을 때도 모두 믿게 되었을 것이라는 우려 표출
     * 이번 논란이 보여준 문제는 원 저자들이 멍청했다기보다는, 학문 분야마다 지식이 나눠진 채 흩어져 있다는 현실. 모두의 지식 발전이 목적이라면 이런 분할된 지식 상태는 바람직하지 않다고 생각한다는 의견. 학계 내부에서 연관 분야에 문제가 발생하고 있다는 지적
          + 정말로 그 정도로 분절되어 있는지 의문 제기. 해당 논문에 등장하는 ‘글로벌 타임라이크 킬링 벡터’ 조건은 양자장론에서 기초적으로 다루는 것인데, 논문 저자들이 관련 없는 것도 아니니 언급 정도는 해야 했다는 아쉬움. 연구진이 사악하거나 멍청한 건 아니지만, 충격적인 결론을 내면서 전문가와 충분히 논의하지 않은 점은 경솔했다고 생각
          + 실제로는 연구계의 목표가 가능한 한 널리, 공개적으로 논문을 발표하는 것이니 진짜 지식의 분절은 아니라고 생각. 하지만 대부분 연구자들이 프리프린트로 공개 발표하기 전까지는 결과를 꽁꽁 숨기는 경향이 있어서 자신의 연구에 이미 치명적 문제가 있음을 아무도 일러주지 못하는 경우가 많다는 현상. 결국 수많은 연구자 간 네트워킹, 인풋, 피드백 과부하 등 인적 한계가 존재. 이런 홍보성 보도자료가 사실 확인 없이 언론에 나가는 것은 막을 수 있겠지만, 근본적 문제는 아니라고 지적
          + 또 다른 측면으로, 원래 논문 저자와 대중 과학 기자들은 어디서 잘못됐는지 모르거나 대담한 주장이 왜 말이 안되는지 이해하지 못하는 경향이 존재. 이미 2년이 지나도록 논쟁이 끝나지 않는 이유인 고질적 문제
          + 실제 이슈는 나눠진 전문 지식보다도 대중이 자극적인 이야기의 유포와 논의를 좋아하는 심리라는 점. 전문가의 반론이 재미를 망치기 때문에 잘 들으려 하지 않는다는 설명. HN에도 해당 논문의 반박 댓글이 있었으나 많은 사람이 이를 심심풀이 재미를 위해 외면했다는 경험담
          + 논문 내용이 ‘분절된 지식’ 때문은 아니며, arxiv에 모두 공개되어 있음. 문제는 누구나 자신의 전공 밖에서 실수하기 쉽다는 점. 과학의 본질은 많은 사람의 검토를 거치도록 하는 것이고, 논쟁 끝에 결론이 정리되는 시스템의 작동 사례로, 시스템은 잘 돌아갔다는 긍정적 관점. 다만 새로운 아이디어가 신문 기사로 포장되기 전 걸러지는 장치는 부족하다는 점 인정
     * ‘만약 바리온 수 보존이 깨진다면 정말 충격적’이라는 주장에 대해, 오히려 이는 오랜 전에 Hawking radiation에서 논의된 논리적 귀결 아닌지, 이미 오래전에 충격으로 받아들였다가 지금은 자연스럽게 보는 분위기라는 생각. 논문 저자 계산에 문제가 있을 수는 있지만, 블로그 글 내용에 너무 자명한 진술처럼 제시된 문장들이 오히려 신뢰를 깎는다는 느낀 점. 위키피디어와 MIT의 Daniel Harlow의 인용을 통해, 블랙홀 증발과 바리온 수 보존 불일치 가능성이 이미 잘 알려진 사실이라는 설명
          + 감정적 글쓰기보다는, John Carlos Baez가 인용한 PRL 논문의 전문적 평가처럼 깔끔하게 수식과 논리로 비판하는 방식이 더 읽기 좋다고 생각. 해당 논문에서는 논문 내 공식이 실제로 약전계 근사에서는 올바르지 않다는 점, 전자기적/중력적 쌍생성 상황을 제대로 다루지 못한다는 점을 전문가 수준에서 설명
          + 이미 많은 관련 논문과 교과서가 링크되어 있고, John Baez는 전문성 있는 인물이라는 신뢰 표명. 논쟁의 핵심은, 블랙홀 없이도 바리온 수 비보존이 가능하다고 주장하는 것이 정말 충격적이라는 점
          + 바리온 수 보존 법칙 위반을 측정하려는 실험들은 지구에서 흑홀 없이도 시도되고 있으며, 실제로 현재까지는 검출되지 않아 양성자의 반감기가 최소 2.4E34년 이상임이 입증된 상황. Quantamagazine의 관련 실험 기사와 HN 토론 소개
          + 표준모형 내에도 비섭동적으로 바리온 수 비보존 현상이 들어있다는 점 언급
          + 블랙홀 증발과 바리온 수 보존의 불일치에 대해, 실제로는 이런 양자 수 보존이 가능한 블랙홀 모델도 존재함을 강조. 펜로즈의 의견처럼 비물리적 가정(무한 시공간 등)을 반복적으로 인용하는 것은 잘못된 주장임을 비판. 대중 과학에서 “블랙홀 사건의 지평선엔 아무 일도 없다”와 “외부 관측자는 희생자가 블랙홀에 떨어지는 것을 절대 보지 못한다”라는 진술을 동시에 하곤 하는데, 두 관측자는 같은 우주에서 물리적 사건에 대해 서로 다를 수 없다는 논리적 지적. 논리적으로 정합적인 해석은 아무도 사건의 지평선을 넘지 못하고, 모든 양자 수가 보존되는 모델밖에 없다는 주장. 블랙홀 증발이 가속되는 현상 등, 모든 관측자 관점에서 일관적 해석 제시
     * 1975년 Ashtekar와 Magnon 논문에서 ‘시공간이 전역적으로 쌍곡선 구조’라는 가정에 관심이 간다는 지점 포착. 현대엔 시공간이 전역적으로 평탄하다는 가정이 보편적 아닌지 의문
          + ‘전역적으로 쌍곡선’은 시공간의 인과적 구조를 의미하며, 위키피디아 참고
          + 시공간 곡률과 공간 곡률의 차이는 별개이며, 세차원 단면이 평평해도 시공간 전체는 쌍곡적일 수 있다는 점. 일반상대론은 전체적 공간 곡률을 규정하지 않으니, 현재까지 특별한 증거는 없다는 설명
     * 단순화된 계산을 실제 현실처럼 다루어 영구기관을 제안하는 사례를 본 경험 공유
     * 고전적인 문제와 그것의 현재적 형태는 모두 이해되었지만, 이제 어떻게 할 수 있는지가 중요하다는 문제의식. 과학은 원래 허위 정보의 장이 아니어야 하지만, 현재로선 방어 체계가 부재함. 누군가는 거짓을 퍼뜨리는 데 돈을 받고, 반대로 거짓임을 지적하는 역할에 대한 보상이 없으니, 외부에서는 과학 내 논쟁이 정치 싸움처럼 보이고 결국 과학자 신뢰도가 훼손된다는 우려. 진짜 심각한 문제 제기
     * 저명한 연구자 Eskil Simonsson의 가르침처럼, ‘죽은 별도 여전히 빛난다’는 이치의 인용
"
"https://news.hada.io/topic?id=20918","Carousel: 엑셀을 위한 Cursor ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Carousel: 엑셀을 위한 Cursor

     * Carousel은 YC 24년도 겨울 배치 회사인 Carousel에서 제작한 엑셀용 Cursor (AI Assistant)
     * 마이크로소프트 코파일럿보다 월등히 좋은 성능을 보이며, 실무에서 사용 가능한 수준의 DCF 모델 등을 AI의 힘만으로 제작 가능
     * 단순한 채팅 이외에도 외부 파일 첨부를 통한 RAG, 복잡한 엑셀 함수(Formula) 해석 기능 등을 제공

   결국 copilot이 적용되는것 아닐까요?
"
"https://news.hada.io/topic?id=20957","생각에 대한 생각 - Thoughts on thinking","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    생각에 대한 생각 - Thoughts on thinking

     * 요즘 무언가를 쓰거나 만들려 할 때마다, AI 시대에 그 일이 무의미하다는 인식을 하게 됨
     * 내가 만들기 전에 이미 AI가 더 잘 만들어내고 있어서, 창작 행위에 대한 동기를 잃어버리는 상태가 됨
     * 내가 떠올리는 아이디어는 마치 LLM 속에 존재할 더 나은 초안의 그림자처럼 느껴짐

     * 과거에는 아이디어가 떠오르면 그것을 글로 천천히 다듬으며, 생각을 명확하게 정리하는 과정을 밟았음
     * 글을 쓰면서 내 생각의 허점을 스스로 발견하고 보완했으며, 그 자체가 사고의 강화로 이어지는 경험이었음
     * 글쓰기는 단순한 표현 수단이 아니라 나의 주장을 형성하고 의견을 구축하는 도구였음
     * 생각은 복리처럼 축적되므로, 더 자주 사고할수록 더 나은 생각이 가능해졌음

     * 지금은 아이디어가 떠오르면 프롬프트에 몇 마디만 입력해도 LLM이 완성된 사고를 제공해주는 구조임
     * 그로 인해 내 내부의 사고 시스템이 점점 위축되는 느낌을 받고 있음
     * 직관, 날카로움, 내적인 탐구심이 조금씩 사라지면서, 더 이상 스스로 생각을 공유하려는 동기를 가지기 어려워짐

     * 처음엔 AI를 나의 사고력을 확장시키는 생산성 도구, 지적 자전거처럼 인식하고 있었음
     * 하지만 실제로는 넷플릭스를 스크롤하거나, TV를 소비하는 것과 같은 수동적인 경험에 가까운 사용 방식이었음
     * 즉, 실제 사고력 증진에는 도움이 안 됨
     * AI가 만든 결과를 읽는 행위는 지적 성찰의 여정이 생략된 과정임
     * 직접 질문을 탐구하고, 시행착오와 내적 논쟁을 거치는 과정이 지적 성장의 핵심
     * AI가 주는 지식은 빠르고 편리하지만, 그 과정에는 내적인 사유의 훈련이 부재함

     * 아이러니하게도 지금 나는 예전보다 더 많은 정보를 알고 있지만, 더 둔해지고 멍청해진 감각을 자주 느끼는 상태임
     * AI는 답을 제공하지만, 그 답은 내 것이 아닌 지식이며, 이해로 전환되지 않음
     * LLM을 통해 얻게 되는 건 지식이지만, 나 스스로 얻어낸 이해와는 본질적으로 다름
     * AI와 함께하는 사고는 초인적 능력처럼 느껴지지만, 실제로는 오히려 내적 탐구 본능을 마비시키는 ‘진정 효과’에 가까움
     * 그래도 이렇게 직접 글을 쓰고, 날것 그대로의 생각을 전달하려는 시도 자체에는 여전히 의미가 존재함

   동의합니다. LLM을 사용하면 할수록 점점 깊게 생각하는 능력을 잃어버리는 것 같아요. 그래서 최근에는 모르는 내용을 질문할 때 최대한 세부적이고, 제가 모르는 요소만 따로 떼어내서 질문하고 보충하는 방식으로 활용하는 중입니다.

   AI 붐 전에도 그림이든 글이든 수많은 것들이 이미 존재했고 새로운 것을 창작하는 것은 너무나도 어려웠죠.

   최근 AI 때문에 변화를 겪었다는 류의 글이 많은데 대부분이 이미 존재하던 것들이고 찾아보면 나오는 것들이였어요.
   AI가 그걸 대화형으로 포장을 해줄 뿐이죠.

   AI 처음 나왔을때부터 이건 눈물을 마시는 새의 환상벽 같다고 생각했는데, 이미 알고 있지만 떠올리지 못하고 묻혀있는 사고를 꺼내주는. 내가 떠올리기 전에 이미 존재하고 있어 무의미하다면 책을 읽는 것은 뭐가 다른가> 책을 읽지 않고 그냥 글을 쓰는건? 내가 들이는 노력과 시간이 다를 뿐, 대부분의 생각은 이미 존재하고 있지 않나? 수많은 아이디어들 중 처음 떠올리는 순간부터 정말 ""새롭다""라고 할 만한 것은 드물고, 결국 반복을 통해 차별점을 창조해 나가야 하는것과 마찬가지인것 같고, 순수 창작 활동에 공을 많이 들인 사람일수록, 또는 그것을 업으로 삼고 있던 사람일수록 AI를 사용한 창작활동에 대해 회의감과 거부감을 느끼는 것 같은데, 결국 글쓴이가 말했듯 AI와 함께하는 창작이 ""초인적""으로 느껴질 정도로 AI의 창조성이 인간보다
   뛰어나다면 그것을 거부하는것은 거스를 수 없는 흐름에 대한 어리석다면 어리석은 저항이 아닐까? AI가 만든 결과를 비판적으로 사고하며 지적 성찰을 거쳐 지식을 이해로 전환하는 것은 인간의 책임이다~ 라는 의견이라면 동의하지만 AI와 함께 하면 내적인 사고가 마비되니 주의해야 한다는 의견이라면 이건 너무 좋아서 나만 쓸거야 정도로 생각됨. 아니면 애초에 비판적 사고와 메타인지가 부족한 사람이었던지.

   약간 비슷한 맥락에서 제 디지털가든을 공개하고 있어요. 이제 지식 단편은 구하기 쉽고 그게 저에게 스며들지 않더라구요. 긱뉴스에서 놀라운 글들을 많이 만나요. 여기 큐레이팅을 누가하시나요?

   https://notes.junghanacs.com/

   제가 요즘 딱~ 이래요…

   비즈니스에 도움이 되기 위해서 개발을 하는 것이니...
   구현의 세부를 LLM이 맡아준 만큼, 내 어플리케이션이 회사에 전체 비즈니스에 있어서 어떤 역할을 맡고 있고, 시장 전체에서 우리 비즈니스는 어떤 의미를 가지고 있는지 등등에 신경을 쓰면 되는 것 아닐까요?
   그리고 LLM이 구현의 세부를 전부 맡을 수 있는지도 의문입니다.

   엑셀 팡션을 도입한 이후 더하기 빼기를 하는 나의 능력이 퇴화됨을 느낀다는 류의 글로 보여서... 90~00년 대에 사무실에 엑셀 처음 들어오던 시절엔 이런 얘기가 없었나 궁금하네요.

        Hacker News 의견

     * 앞으로 몇 달, 몇 년 안에 사회가 크게 분화될 거라는 예감이 있음
       표현을 만든다는 과정 자체가 브랜덤(Brandom)이 말한 개념적·합리적 분절임
       개념은 이미 존재하고 단순히 토큰의 조합으로 인코딩·디코딩하는 거라 생각하거나, 아예 추론이나 개념화의 과정 자체를 인식하지 못하는 사람들은 자동화 대상
       이건 직업 자동화 얘기가 아니라, 자발성을 포기하고 점점 로봇처럼 살아가게 된다는 의미
       로봇은 '완전히 기계적인 방식으로 일하거나 활동하는 사람'이라는 정의
       너무 많은 사람이 생산주의 이데올로기의 포로
       창작이란 콘텐츠를 만들어내는 게 본질이 아닌데, 창조 행위의 목적은 소통과 상호 변형
       디지털 산출물 생성도 이런 목적에 쓸 수 있겠지만, 단순 생산이 곧 목적이라는 착각이 많고, 이건 어둡고 슬픈 막다른 길
          + 인간의 사고와 산출물 99%, 어쩌면 100%는 파생성
            우리가 만들어내는 모든 건 경험이나 목격한 걸 바탕으로 함
            완전히 새로운 물건, 현실에 일체 기초 없는 걸 상상하려 하면 불가능
            작가들이 엘프를 만든 것도 결국 인간에 뾰족한 귀만 추가한 것에 불과
     * 다양한 관측 결과에서 LLM이 인간 사고의 질에 미치는 영향은 대체로 부정적이라는 인상
       내 아이 학교가 LLM을 강력히 금지한 게 정말 다행임
       수업 때 직접 교사 눈앞에서 한 과제만 제출 가능, 종이 과제 비중이 크게 늘었음
       집안에서 부모 모두 교육학 교수였기에, 다양한 학습법 비교가 흔한 주제
       적극적인 학습(스스로 만들고 활동하는 것)이 수동적·수용 중심 방법보다 훨씬 효과적
       LLM은 대체로 후자, 그러니 걱정이 됨
          + 외국어 배우면 단순 어휘·문법 암기로 실력 안 늘고, 즉석에서 문장 만드는 대화 시 전혀 다른 뇌 부위나 능력 쓰는 것 같은 느낌
            LLM이 부정적 도구라는 건 새롭지 않음
            내 학창 시절에는 계산기 필요했지만 심볼릭 수식해결 가능한 고가 모델은 금지됐었음
            해답을 즉시 얻는 건 그 과제의 본질적 가치 자체를 무력화, 오히려 성장 막음
          + '하드코어 금지'가 구체적으로 무얼 의미하는지 궁금
            내가 다녔던 학교에는 1회라도 거짓말이나 부정행위 적발 시 즉시 퇴학인 단일 엄벌 윤리강령이 있었음
            시험 맨 위에 강령에 직접 서명
            이 강령 없는 학교 다니던 친구들은 너무 보수적 전통이라고 불평
            하지만 지금 'AI 금지'를 강제할 더 나은 방법이 없어 보임
          + 내 학교 시절에도 인터넷이 보급되기 시작했지만, 주제로 리서치할 땐 인터넷 사용 금지였고 오프라인 도서관만 허용
            이공계 대학에서도 1학년엔 모든 기술 도면을 연필과 자로 직접 그리게 했음
            실제 현장·표준은 컴퓨터 그래픽이었지만 일부러 수작업 강제
            개인적으로 이런 극단적 금지가 실제로 도움이 될지는 회의적
            시대 발전도 막지 못함
            오히려 기술을 사용하는 법을 가르치는 게 더 낫다고 생각
          + LLM이나 Wikipedia 같은 걸로도 정말 많이 배울 수 있음
            핵심은 호기심과 배움 욕구
            그게 없다면 무엇을 써도 발전 없음
     * LLM이 인간 개별 경험(혹은 경험을 바탕으로 한 글쓰기)을 대체할 수 없음
       사실만 반복하거나 평균적인 의견을 재현하는 것으론 유일무이한 인간 사고의 대체 불가
       AI와 내 생각의 품질로 경쟁한다는 사고 자체가 너무 슬픈 관점
          + LLM 이전에도 80억 인구와 역사상 유명인물들과 '경쟁'하고 있었음
            네가 쓰는 소설, 네 얘기, 네가 갖춘 기술 모두 이미 선례가 있었고 누군가는 더 잘함
            내 인생 목표가 '세계 1위'라면, 애초에 성공 확률 희박함
            설령 이뤄도 별 의미 없음
            세계 최고 자바 프로그래머가 누군지, 최고 위상이 큰 사랑과 명예를 받는지 생각해봐
     * AI가 내가 느끼는 실존적 불안을 여러 방식으로 부추기는데, 그 중 하나가 평균적 사고로 자꾸 밀어 넣는다는 점
       기술 구조상 어쩔 수 없고, 이게 두려운 이유는 창의적 사고는 언제나 가장자리에 있기 때문
       문제에 막히는 순간이 바로 새로운 뭔가에 닿기 전 단계인데, AI 활용 유혹이 그 신선함을 빼앗고 이미 한 걸 복제하게 만듦
          + 'AI가 평균적 사고로 이끈다'는 건 흥미로운 관점
            하지만 주도권은 항상 본인에게 있음
            AI를 신적 존재로 볼 게 아니라, 언제든 끊고, 지시하고, 바로잡고, 재요청할 수 있는 조수로 보면 됨
            '무엇'을 고민하며, '어떻게'의 일부만 맡기면 됨
          + 오히려 지금처럼 내 머리에 든 코딩 아이디어를 즉각 실험해볼 수 있는 시대가 처음
            이전이라면 영원히 실행 못 해봤을 아이디어를 Claude에게 POC로 시키면 구체화
            그 사이 불안이 큰 것도 맞지만, 모든 흐름이 정치·기술·인간 본성 교차점에서 전개되고 있다고 느낌
            '강한 자'에게만 도구를 맡기면, 사회는 더 불리해질 수 있어서 이 강력한 도구를 시민의 편에서 활용해 조직화나 협업의 새로운 방식 찾기 가능
     * LLM을 수년째 써왔지만, 그저 좋은 도구일 뿐 사용 감각이 기사와 달라서 이상하다고 느낌
       LLM으로 만화 이미지를 만들어보면, 처음엔 '와우'인데 곧 똑같은 스타일의 반복
       시도시에도 시 한두 번은 멋지지만 여러 번 돌리면 깊이나 풍미가 결여된 무미건조함
       음악도 비슷, (운율, 멜로디 모두) 반복성 드러남
       Podcast를 만들 때도 처음엔 신기하지만 진행 자체가 반복되고, 진행자들이 깊이나 이해 부족
       질문으로 중간중간 끊으면 조금 나아지기도 해서 이건 좀 애매함
       텍스트 생성도 시간이 지나면 '메탈릭'한 인공미가 강하게 느껴짐
       검색 기능은 그럭저럭이지만, 살짝만 유도해도 답이 확 바뀌니 무조건 믿기 어렵고 항상 교차 확인해야함
       일부러 반대 관점으로도 LLM에 유도해서 상반된 견해를 얻어 공부해야 함
       코드 생성은 단순한 건 좋지만, 복잡한 건 미묘한 버그 꽤 발생해서 모든 줄을 직접 이해해야 함
       오히려 그 '버그 찾기' 과정이 재밌고, 인간과 똑같은 실수에 소소한 즐거움
       그래서 본문 저자가 말한 것과 정반대 효과
       빠르게 아이디어를 실험하고, 편견 적은 피드백 덕분에 오히려 글쓰기가 더 즐거워짐
          + 최신 LLM을 잘 활용하지 못하거나 그럴 생각 자체가 없는 것 같음
            LLM에 대한 폄하가 나올 때마다 실제로 내가 매번 훨씬 많은 실질적 도움을 얻고 있음
            특히 비트리비얼 코드·최신 모델에서 체감 뚜렷
     * AI가 사고를 억제, 즉 생각하려는 동기의 상실을 다룬 것은 실제적인 문제임
       다른 요인도 있지만, AI만이 주는 새로움은 '노력해야 했던 동기'의 소멸
       예전에는 하루 종일 넷만 봤다면, 블로그 글도 안 써져 명성도 못 얻었지만
       지금은 AI가 블로그·이메일·책도 써줌
       스스로 생각하려는 내재적 동기가 없다면, AI로 대충 넘기기가 훨씬 쉬워짐
       한편, 저자가 사실은 AI 때문이 아니라 우울한 게 아닐까 하는 생각도 지울 수 없음
       인생의 의미는 본인이 만드는 것
       AI 때문에 인생 무의미하다 느낀다면, 쓰지 않는 선택도 가능
       뭔가 의미있는 일은 여전히 많고, 'AI보다 빨리 글 쓰기'가 최종 목표가 아니라면 다른 데 집중 가능
       뭔가 새롭고 흥미로운 걸 쓸 수 없다고 느낀다면 오히려 목공이나 수공예 같은 걸 추천
          + 나에게는 새 생각 실험 및 시도의 장벽이 크게 낮아진 느낌
            과거엔 시간·비용 문제로 포기한 길이 많았는데, 지금은 자유롭게 많은 길 시도 가능
          + 인간은 사회성 존재임을 상기
            도구 자체가 대체되는 것보다, 그 도구를 기꺼이 활용해 인간을 갈아치우려는 열정적 집단의 존재 자체가 훨씬 우울함
            인간 우선이라는 전제가 점점 '논리적 오류'가 되는 시대
            언젠간 추세가 되돌아오겠지만, 지금은 많이 힘든 구간
            이런 자극적 논의가 다양한 입장에서 활발히 오가는 모습은 고무적
     * AI를 물리 조립이나 실습 프로젝트에 쓸 때, 내 역할이 훨씬 주체적으로 느껴짐
       예를 들어, 내가 잘 모르는 전자·멀티미디어와 같은 실물 프로젝트에 AI를 조수로 써보니, 내가 손대지 못했던 영역까지 자연스럽게 도전 가능
       핵심은 '가능성을 최대한 넓혀본다'는 욕구가 있다면, AI로 장애물 줄이고 재미있게 성장 가능
       컴퓨터 앞에만 앉아있는 게 아니라 진짜 내 몸이 직접 해냈다는 성취를 느끼기 좋음
       다만 모두 이런 걸 원하는 건 아니고, 취향은 다양
          + '내가 실제 무언가 하는' 상황에서 LLM에 대해 즐겁고 낙관적인 감정
            반면, '남을 도와주는 조연'이 되면 조금 불안
            예전엔 누군가 목표를 이루려면 내가 직접 개발을 도와야 했지만, 지금은 LLM으로 본인이 꽤 멀리 나아갈 수 있음
            이런 변화는 긍정적
            다만 고용·직업 관점에선 약간 불안감
            나 스스로는 '수단'보다 '목표'에 가까운 일을 하고 싶단 결론
     * ""AI보다 내가 만드는 게 경쟁력 없다""는 글에 대해, 창작 과정 자체에서 즐거움을 못 느꼈던 게 처음부터 문제 아닌가
       믹서가 나보다 반죽 잘해도 손으로 반죽하는 즐거움은 내가 누릴 몫
       빵집, 장인도 나보다 잘하지만 직접 만드는 즐거움에 경쟁은 무관
       도예, 제빵 다 마찬가지
       순수하게 '나만 할 수 있다'는 감정에서만 즐거움을 느꼈다면, 사실 AI 나오기 전부터 잘못된 접근
          + 좀 더 후하게 해석하면, '세상에 기존에 없던 뭔가를 기여'하는 데서 느끼는 보람 강조
            남보다 우위 때문이 아닌 '새로운 기여'라는 것에 초점
          + 사람마다 어느 시점에 그런 사실(과정이 본질이고 결과는 중요하지 않음)을 자각하게 됨
            어떤 글쓴이 논리에 맞춰 설명하자면 '스토아 궁수' 은유 적용 가능
            목표(성과)보다 행동(과정)에 집중하면 실망감에서 해방
            예를 들어 파티에서 친구 사귀기 목표보다, 진정성 있게 임하겠다는 목표를 세우면 성공 가능성과 만족감 모두 커짐
            과정 중심 목표 설정이 중요함
          + 글에서 주로 다루는 건 포스트 AGI 시대 인간 기술 가치의 하락
            두려운 이유는 우리의 신체적·정신적 노동이 곧 돈벌이 수단이기 때문
            GPU와 자본만 있으면 인간 대신 지능형 에이전트 천 개 투입 가능
            AGI 시대에 화이트칼라 노동 가치가 0에 수렴
            물론 AGI의 과학적 진보엔 기대 커도, 내 자리 있길 바람
            시장경쟁에서 제빵사·도예가가 기계에 밀려 'Etsy에서 내 파이썬 코드팔기' 신세라면 걱정
            관련 블로그 참고 추천
          + 누군가의 견해처럼 '퍼포먼스 동기'가 반드시 나쁘다 생각하진 않지만, AI는 팬데믹처럼 인간 내면의 미묘한 차이를 드러내줌
            변곡점에서 이전엔 다양한 사고방식이 공존했지만, 이제 하나의 방식이 더는 '통하지 않는' 시대가 다가옴
            이런 불안정함 자체가 중요한 변화
          + '작품'의 가치 중엔 얼마나 노력했거나 정성이 들어갔는지가 중요했던 관습 존재
            시간과 사고를 들인 흔적이 작품의 품격
            LLM은 이 본능적 평가방식을 단축시켜 작품의 평가 기준이 흔들림
            AI 만든 결과물이 실제보다 더 가치있다는 착각, 인간이 만든 노력이 저평가되면서 의욕 상실
            창작자·지식인이 경쟁심만으로 움직인다고 보는 시각이 유행한 것도 의아
            사실 많은 경우, 불안감이 이런 오해로 이어짐
            인공지능·예술 논쟁 대부분, 지적재산권 바깥 문제에선 인간만의 창작 의미와 예술 본질에 대한 오해로 가득
            평생에 걸쳐 쌓은 고유의 목소리가 한순간에 AI 탓에 평가절하된다면, 작업의 가치와 보상 감정 전반이 흔들림
     * 한때 나도 AI에 대해 비슷하게 느꼈지만, 지금은 완전히 생각이 바뀜
       AI를 '다 해주는 마법 지팡이'로 보지 말고, 도구로써 활용하는 게 핵심
       모든 걸 AI에 맡기는 순간 삶의 주도권 상실
       중요한 방향과 큰 그림에 스스로 책임감을 갖고, 잘 정의된 과업만 AI에 맡기면 컨트롤권도 유지되고 훨씬 재밌게 쓸 수 있음
     * 근력운동 비유가 적절
       실제 무거운 걸 들기 위해 운동한다기보다, 목표 달성의 성취감, 신체 변화, 건강 증진 등 '과정' 자체에서 만족감
       이걸 단순 실용으로 보면 무의미
       오히려 긍정적 외부효과가 덤인 일상 루틴으로 받아들이면 스트레스 줄어듦
       kelseyfrog가 언급했듯이, 핵심은 목표가 아니라 '행동'에 집중하는 것
       운동 자체가 단순히 기록 경신, 근육 증가도 있지만, 근본적으로 성장 경험
       운동하는 행위 그 자체가 무의식에 뿌리내려서 큰 고민 없이 즐김
       결과가 예기치 않게 나타나는 걸 보는 재미
       글쓰기 등 노력 필요한 활동에도 이 운동 비유 적용
       '남과 비교'나 결과 중심이면 운동·글쓰기·많은 일이 무의미해짐
       우리가 하는 모든 것은 '목적을 위한 수단'임
          + 신기하게도 우리 둘 다 풍요 시대의 현상과, 개인적 성취나 자율성 관점에서 미치는 영향을 언급했다는 점에서 묘한 일치
          + 나는 어릴 때부터 애니 캐릭터처럼 보이고 싶어서 운동함
"
"https://news.hada.io/topic?id=21015","Show GN: JSON 비교를 더 쉽고 예쁘게 - JSON Tapose","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Show GN: JSON 비교를 더 쉽고 예쁘게 - JSON Tapose

   안녕하세요!

   JSON 데이터를 비교하고 분석하는 작업을 더 쉽고 시각적으로 만들고 싶어서 개발한 간단한 웹 서비스입니다.

   제가 필요해서 만들었는데 여러분들께 공유드리고 싶어서 소개드립니다.

  기술 스택

     * React + TypeScript
     * DaisyUI

  주요 기능

     * 두 개의 JSON을 나란히 비교하며 차이점을 한눈에 파악
          + 다른 텍스트 에디터와 다르게 필드 순서가 달라도 비교 가능
     * JSON beautify (formatting) 기능
     * 변경된 부분만 하이라이트하여 빠른 분석 가능

  특징

     * 모든 처리가 브라우저에서 이루어져 데이터가 서버로 전송되지 않습니다
     * 복잡한 설정 없이 바로 사용 가능
     * 다양한 테마 제공

   편하게 사용해보시고 피드백 주시면 감사하겠습니다! 😊

  서비스 링크

     * https://www.jsontapose.com/

   Context 기능이 굉장히 좋네요 나중에 꼭 써보겠습니다

   넵 감사합니다! 🫡

   하루에도 여러번 키는데 비교사이트엿는데
   드디어 ~ 갈아탈때가 온거같습니다

   정말 정말 감사합니다!
   몸둘바를 모르겠네요

   오 멋지네요!!

   감사합니다! 😊

   너무 좋은데요 잘쓰겠습니다 :)

   감사합니다! 많이 많이 애용해주세요!

   우와 지린다미어~! hiberbee 테마도 추가해주세여

   고려은단비타민씨 해보겠습니다!

   UI가 예쁘고 좋습니다~
   json editor online이랑 비슷하네요~
   https://jsoneditoronline.org/

   감사합니다! 해당 사이트도 참고해볼게요!

   이쁘네요~ 자주 이용하겠습니다.

   감사합니다! 많이 많이 사용해주세요!

   서비스 이름이 Juxtapose 에서 영감을 받으신 것 같은데 예쁘네요~

   Json + Juxtapose 가 맞습니다 ㅎㅎ
   감사합니다!

   오 이거 편하고 이쁜데요?

   감사합니다! 사용하시다가 불편한 점이 생기면 말씀해주세요!
"
"https://news.hada.io/topic?id=21014","Claude Code SDK","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Claude Code SDK

     * Claude Code SDK는 AI 기반 코딩 도구를 애플리케이션에 통합할 수 있는 기능 제공
     * 개발자는 서브프로세스 형태로 Claude Code 실행 가능, 커맨드라인 사용 우선 지원
     * 대화형 컨텍스트 관리, 커스텀 시스템 프롬프트 지정, 외부 도구 연동(MCP) 기능 포함
     * 출력 포맷 다양성(텍스트, JSON, 스트리밍 JSON) 및 상세한 CLI 옵션 활용 가능
     * 실제 사례로 GitHub Actions와 연계하여 자동화된 코드 리뷰, PR 생성 등 실제 개발 워크플로우 통합 가능성 제시
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

   Claude Code SDK는 Claude Code의 기능을 애플리케이션에 직접 프로그래밍적으로 통합할 수 있는 오픈소스 개발 도구임. SDK는 Claude Code를 별도 프로세스 형태로 실행하여, AI 기반 코드 어시스턴트, 자동화 도구, 코드 리뷰 시스템 등 다양한 개발 워크플로우에 접목할 수 있는 확장성을 제공함.

   현재는 커맨드라인(CLI) 기반 인터페이스를 지원하며, TypeScript 및 Python용 SDK는 곧 출시될 예정임

기본 SDK 사용

   Claude Code SDK는 비대화형(비인터랙티브) 실행을 지원함. 예를 들어, 프로그래밍 코드 내에서 Claude Code를 명령줄 인수와 함께 호출하여 원하는 출력을 자동으로 얻을 수 있음

고급 사용법

  멀티턴 대화 세션 이어가기

     * 여러 차례의 대화를 주고받을 때, 이전의 세션이나 특정 세션 ID를 활용하여 대화 맥락을 유지하고 이어가는 기능 제공
     * 개발자는 최근 세션에서 대화를 계속 이어가거나, 특정 세션을 불러올 수 있음

  커스텀 시스템 프롬프트

     * Claude의 기본 동작 방식을 개발자 맞춤형 시스템 프롬프트로 조정 가능
     * 기본 시스템 프롬프트에 추가 지침을 첨부하여 assistant의 업무 지향성과 행동 범위를 변경할 수 있음

  MCP(Model Context Protocol) 설정

     * MCP는 Claude Code 기능 확장을 위한 외부 서버 연동 프로토콜임
     * --mcp-config 플래그와 JSON 설정 파일을 통해 외부 서버에서 제공하는 데이터베이스 접근, API 연동, 커스텀 툴 등을 추가할 수 있음
     * MCP 도구 사용 시, 명시적으로 허용한 도구만 사용 가능해야 하며(--allowedTools 플래그), 이름 패턴이 mcp__<serverName>__<toolName> 형식임

사용 가능한 CLI 옵션

   Claude Code SDK에서는 다양한 커맨드라인 플래그를 사용하여 실행 환경을 설정할 수 있음
     * --print, -p : 비인터랙티브 모드 실행
     * --output-format : 출력 포맷 선택(텍스트, JSON, 스트림 JSON 등)
     * --resume, -r : 특정 세션 ID로 대화 이어가기
     * --continue, -c : 가장 최근 세션 이어가기
     * --verbose : 상세 로그 출력
     * --max-turns : 비인터랙티브 모드에서 최대 대화 라운드 제한
     * --system-prompt : 시스템 프롬프트 오버라이드
     * --append-system-prompt : 시스템 프롬프트에 추가 지침 첨부
     * --allowedTools : 허용 도구 리스트 지정(MCP 도구도 포함)
     * --disallowedTools : 금지 도구 리스트 지정
     * --mcp-config : MCP 서버 설정 파일 로드
     * --permission-prompt-tool : 권한 프롬프트 처리를 위한 MCP 도구 지정

   전체 사용 가능한 옵션 및 사용 예시는 공식 CLI 문서 참고 필요

출력 포맷

   SDK는 다양한 출력 포맷을 지원함

  텍스트 출력(기본)

     * 응답 텍스트만 반환

  JSON 출력

     * 메타데이터와 구조화된 데이터 반환으로 프로그래밍적 파싱에 유리
     * API 응답 메시지는 엄격한 타입 구조를 따르며, 앞으로 JSON Schema 포맷으로도 타입 정의 예정

  스트리밍 JSON 출력

     * 각 메시지를 실시간으로 스트림 전송
     * 대화 세션 진행 시, 초기화 메시지(init), 사용자/assistant 메시지, 통계가 들어있는 최종 result 메시지 순으로 별도 JSON 객체로 전송

메시지 스키마

     * JSON API 응답 메시지는 정확한 스키마에 따라 구조화됨
     * 주기적으로 스키마 갱신 및 버전 관리 진행 예정

예시

     * 간단한 스크립트 통합
     * Claude를 활용한 파일 처리
     * 세션 관리로 복잡한 대화 플로우 지원

베스트 프랙티스

    1. JSON 출력 포맷을 활용한 응답 파싱 사용
    2. 오류 처리: 종료 코드 및 에러 스트림 확인
    3. 세션 관리: 다중 라운드 대화의 맥락 유지
    4. 타임아웃 고려: 장시간 작업의 안전성 확보
    5. 요청 간 간격 조정: 과다한 호출 방지 및 서비스 안정성 유지

실제 적용 사례

   Claude Code SDK는 실무 개발 환경에서 강력한 자동화 및 통합 기능을 제공함
     * 대표적 사례로, GitHub Actions와 결합하여 자동 코드 리뷰, PR 생성, 이슈 분류 등의 개발 워크플로우를 완전 자동화할 수 있음

        Hacker News 의견

     * Claude Code가 추구하는 방향이 바로 내가 원하던 agent 기반 코딩 도구의 ""unix toolish"" 철학 형태임을 강조하고 싶음. 초기 공개 프리뷰 때부터 Claude Code를 써왔고 발전 과정을 지켜봤다는 경험을 가지고 있음. 코딩 에이전트의 ""황금 기준점""은 피처 요청(예: Jira 티켓)을 입력하면 PR을 받아서 내가 직접 리뷰하고 피드백할 수 있는 수준이라고 생각함. Cursor, windsurf 등은 로컬 에디터라서 CI 환경에 통합이 불가능해 한계가 있다고 봄. 코드베이스를 AI 최적화(MCP, 룰 등)하고 싶다면 헤드리스로도 쓸 수 있는 기술을 목표해야 함을 강조하고 싶음. Claude Code는 자동화 툴과 함께 간단하게 쓸 수 있어서 내가 코딩 에이전트를 생각할 때 이제는 기본이 됨. Codex npm 패키지도 비슷하게 생각함. 참고로, 나는 이런 최적 도구 설정을 돕는 역할을 하고 있어서 쉬운 설정이 가능한
       툴에 자연스레 유리한 시각을 가질 수밖에 없음
          + 내가 바라는 ""황금 엔드 스테이트""는 내가 AI 에이전트들, 예를 들면 코드 작성, 디자인, 테스트를 수행하는 AI 에이전트들로 둘러싸인 방 한가운데 있는 상황임. 나는 그 가운데에서 직접 키보드에 손을 거의 대지 않고 방향성과 미적 기준, 가이드만 대화로 제시하는 것임. 그런 미래를 기대함
          + Anthropic가 오늘 이와 비슷한 기능을 베타로 발표함을 언급하고 싶음. 관련 문서를 공유하고 싶음. https://docs.anthropic.com/en/docs/claude-code/github-actions
          + 내가 생각하는 코딩 에이전트의 ""황금 엔드 스테이트""는 내 컴퓨터, 혹은 내가 원하는 어디든 자유롭게 실행할 수 있는 무료 오픈 소스 코딩 에이전트를 사용하는 상황임. 터미널에서 ls, ps, kill 같은 명령어를 실행할 때마다 비용 지불을 상상하는 게 말이 안 된다고 느끼는 것처럼, LLM도 마찬가지라고 생각함. 독점적 LLM을 ""금지하자""는 건 아니지만, 이 분야에서 해커라 불리는 사람들은 오픈 소스 툴을 주요 도구로 삼았으면 하는 바람임
          + Cursor, windsurf 등은 로컬 에디터이기 때문에 CI에 쓰기 어렵다는 점을 지적하셨지만, 나는 Cursor와 MCP 조합으로 이걸 시도해 봤음. 하루 종일 성공적으로 써봤는데, 곧 rate limit에 걸려서 제일 느리고 멍청한 모델로 바뀌는 경험을 했음. Claude로도 시도해봤지만 얼마 안가 limit을 다 써버리게 됨. 게다가 PR은 25% 정도만 ""즉시 사용 가능"" 수준이고 AI가 어디서 실수했는지 파악하는 것보다 그냥 직접 하는 게 빠른 경우가 많음
          + 이미 그런 CI 기반 자동화를 할 수 있지 않냐는 생각임. Aider CLI를 GitHub Action으로 등록해서 issue가 생성되면 자동으로 실행되게 하면 이미 그런 구조가 구현 가능함
     * Claude Code는 내가 LLM을 코딩에 활용할 때 제일 좋아하는 방식임. 하지만 진짜 필요한 건 오픈 소스 버전의 Claude Code라고 생각함. 원하는 모델을 적용하고, 서로 다른 모델의 답변을 직접 비교할 수 있는 환경이 필요함. Aider 등 다른 대안은 Claude Code 수준의 경험을 주지 못한다고 느낌. 이런 방식을 Anthropic이 원하지 않는 건 예상할 수 있음(방어벽이 약해지니까). 하지만 소비자 입장의 나는 최고의 모델을 쓰고 싶고, 한 생태계에 묶이고 싶지 않음. LLM 프로바이더 입장에서 이게 가장 큰 두려움일 거라 예상함
          + OpenAI codex가 말씀하신 것과 가장 가까운 오픈 소스임. 원하는 프로바이더의 모델을 쓸 수 있음. 현재로선 Claude Code보다 못하지만 곧 따라잡을 거라고 봄. https://github.com/openai/codex/tree/main
          + Claude Code를 MCP 서버로 써서 어느 정도 원하는 환경을 구현할 수 있음
     * Aider는 Python과 shell 스크립팅 지원을 꽤 오래전부터 제공하고 있었음. 최근에 130개 새로운 프로그래밍 언어 지원을 추가하는 과정 일환으로 ad-hoc bash scripting aide를 포함시킨 스크린캐스트도 제작했음. 이 스크립팅 스타일 접근이 얼마나 강력한지 직접 느낄 수 있을 것임. https://aider.chat/docs/scripting.html, https://aider.chat/docs/recordings/tree-sitter-language-pack.html
          + Aider 정말 마음에 듦. MCPs도 곧 지원 예정이며, 개발 브랜치에서 테스트 중임. 이게 되면 신뢰하는 모델만으로 PR, 티켓 등 end-to-end 개발이 실제로 가능해짐
          + Aider를 Claude Code만큼 좋아할 수 있는 수준까지 가져갈 수 있을지 궁금함. Claude Code의 UX는 마음에 들지만, Gemini 2.5 Pro를 선호해서 Claude Code는 안 씀. 커밋 등 기능보다는 UX가 좋은 점이 매력임. 이에 대한 생각을 듣고 싶음
          + Aider가 GitHub Actions 연동 워크플로우를 더 polished하게 만든다면 진짜 크게 성장할 수 있다고 생각함. repo에 파일 하나만 추가하면 issue로 원하는 모델과 대화 가능함
     * Claude Code 팀이 직접 밝힌 추가 맥락을 공유하고 싶음. http://latent.space/p/claude-code 주요 내용 정리로는, Anthropic 직원들은 무제한 Claude로 평균 하루 6달러 정도를 사용하고 있음. CI를 위한 ""linux"" 유틸리티 같은 headless Claude Code가 매우 매력적임. 확장 가능한 사용자 플랫폼이라는 지향점도 있음. 앞으로의 로드맵은 샌드박싱, 브랜칭, 플래닝 기능임. sonnet 3.7이라는 지속적 agentic 모델도 예정임
          + ""Anthropic 직원들, 무제한 Claude로 평균 $6/일 사용""이라고 하셨지만, 기사엔 어떤 엔지니어는 하루에 $1,000도 썼다는 이야기도 있음. 평균보다는 P50, P75, P95 같은 분포가 궁금함
          + 나는 Claude Code를 2시간 정도만 써도 20달러가 훅 넘어가는 경험을 여러 번 했음. 개인 프로젝트엔 너무 비싸서 그냥 못 쓰게 됨
          + 최근 latent space 팟캐스트를 정말 재미있게 들었음. 높은 SNR을 유지하면서 이렇게 높은 생산성의 공개 콘텐츠를 꾸준히 내는 사람/팟캐스트는 정말 드뭄. 여러 비즈니스를 병행하면서도 이 정도 공적 산출물을 유지하는 게 대단함. 이런 생산성 gradient를 더 많은 이가 경험하면 좋겠다고 느낌. 본인이 비결을 공유하긴 하지만 직접 따라 하긴 쉬운 일은 아님
     * 만약 내가 AI 코드 어시스턴트를 만든다면 절대 특정 foundation 모델 제공업체에 종속시키진 않을 것임. 이게 성공하려면 모델 효과가 이제 거의 정점에 도달해서 성능이 다 비슷해지고, 그냥 익숙한 SDK 수준의 작은 차이만 남은 상황이어야만 타당한 전략임
          + 명령어나 인자만 다를 뿐이므로 사실상 종속(lock-in)이 심하진 않다고 생각함. 결국 입력-출력 함수만 있으니 필요한 부분만 바꿔서 사용하거나 래핑하면 됨. 구조적으로 복잡한 건 없음
          + 현시점에서 Claude Code는 agent 기반 코딩 시장에서 차별점 역할을 하고 있다고 생각함. AI code assistant를 직접 개발 중인데, Claude Code 연동을 제일 먼저 시도하게 됨. 초기에 lock-in을 신경 쓸 시점은 아니라고 느낌. 최고의 것을 골라서 그 위주로 개발을 시작해야 한다고 생각함
     * Claude Code는 이미 비상호작용(non-interactive) 모드로도 사용 가능했기 때문에, UNIX 커맨드라인 유틸리티처럼 다른 앱에 쉽게 통합 가능했다는 점을 강조하고 싶음. 이번 SDK도 커맨드라인 사용만 지원하고 있는데, 본질적으로는 기존에 있던 것과 뭐가 다른 건지 잘 모르겠음. 내가 놓치고 있는 포인트가 뭔지 궁금함
     * ""서비스를 일반 지능에 경쟁하는 어떤 제품이나 서비스 개발, 훈련, 혹은 리셀을 금지한다""는 Anthropic의 이용 약관을 인용함. 사실상 모든 소프트웨어가 일반 지능에 ""경쟁""하는 범주라면, 이를 엄격 해석하면 그 어떤 용도로도 쓸 수 없는 셈 아닌가? 이런 법적 문구 자체가 너무 모호해서 적용 불가하다고 생각함. AI의 결과물을 소유하지만 일반 지능에 경쟁이 안 된다는 식인데, 왜 이렇게 포괄적으로 금지하는지 궁금함. 그냥 법적 책임은 유저가 지라는 뜻인가? 여러 의문이 듦
          + 변호사들이 원하는 대로 규정을 만들면 이런 일이 생긴다고 생각함
     * 새로 추가된 GitHub Action이 내가 찾던 기능과 정확히 일치해서 너무 반가움. https://docs.anthropic.com/en/docs/claude-code/github-action... 그런데 Claude Code의 Max plan과 연동해서 쓸 수 있는 방법이 현재 없는 것 같음. api key만 입력받는 구조라 약간 아쉬움
     * 특히 GitHub Actions와 issue/PR 연동 기능이 내가 그간 원하던 기능임을 강조하고 싶음. https://docs.anthropic.com/en/docs/claude-code/github-action...
     * copilot을 통해 Claude 지원이 시작되면 꼭 써볼 생각임. 회사 정책상 아직 그 외 도구는 사용할 수 없는 상황임
"
"https://news.hada.io/topic?id=21033","ANSIS - 터미널/CI/크롬에서 사용 가능한 ANSI 컬러 라이브러리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ANSIS - 터미널/CI/크롬에서 사용 가능한 ANSI 컬러 라이브러리

     * 기존의 chalk, picocolors, colorette 등을 대체(API 호환)할 수 있는 초소형 고성능 ANSI 스타일링 라이브러리
     * ESM 및 CJS 모두 지원, Next.js, Deno, Bun 등 최신 환경과도 호환
     * 환경 감지 및 자동 fallback 지원: truecolor → 256 → 16 → 흑백
     * 체이닝 및 템플릿 리터럴 지원하여 가독성 좋은 코드 작성 가능 : red.bold.underline('text')
     * 다양한 환경 변수 및 CLI 플래그(NO_COLOR, FORCE_COLOR, --no-color) 지원
     * 코드 중간에 스타일 끊김 없이 줄바꿈 처리 가능 (\n 자동 스타일 처리)
     * 터미널, CI 환경, Chromium 기반 브라우저에서 사용할 수 있음
     * 단일 스타일에서는 picocolors가 가장 빠르지만, 두 개 이상 스타일에서는 Ansis가 가장 빠름
          + 실제 응용 환경(복합 스타일 사용 등)에서는 Ansis가 picocolors보다 빠르거나 동등함
"
"https://news.hada.io/topic?id=20952","OpenAI, 클라우드 기반 코드 에이전트 Codex 리서치 프리뷰 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                OpenAI, 클라우드 기반 코드 에이전트 Codex 리서치 프리뷰 출시

     * OpenAI는 소프트웨어 엔지니어링 전용 에이전트 Codex를 출시하여 반복 작업, 코드 작성, PR 제안 등을 자동화할 수 있게 함
     * Codex는 분리된 클라우드 샌드박스 환경에서 작업을 수행하며, 테스트와 로깅을 통해 결과를 투명하게 검증 가능함
     * AGENTS.md 파일을 통해 프로젝트별 관행과 테스트 방식을 Codex에 명시할 수 있으며, 사용자 코드베이스에 최적화 가능함
     * CLI 버전 Codex CLI도 함께 제공되어 로컬 개발 환경에서도 에이전트 활용 가능함
     * 초기 배포는 ChatGPT Pro·Team·Enterprise에 제공되며, 추후 Plus 및 Edu 사용자에게도 확장 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Introducing Codex

  Codex란?

     * Codex는 클라우드에서 실행되는 소프트웨어 엔지니어링 에이전트로, 사용자의 코드베이스를 읽고 다양한 작업을 자동으로 처리할 수 있음
     * 코드 기능 추가, 질문 응답, 버그 수정, PR 제안 등을 병렬로 수행 가능
     * 각 작업은 분리된 샌드박스 환경에서 독립적으로 실행되며, 사용자 레포지토리가 사전 로드되어 있음

  작동 방식

     * ChatGPT 사이드바에서 Codex 기능을 통해 “Code” 또는 “Ask” 명령으로 작업을 시작
     * 파일을 읽고 수정하며 테스트, 린터, 타입체커 등 명령 실행 가능
     * 작업은 보통 1~30분 내 완료되며, 실시간 진행 상황 확인 가능
     * Codex는 작업 후 커밋을 생성하고, 터미널 로그 및 테스트 출력을 인용해 변경사항을 투명하게 설명함
     * 결과를 검토한 뒤 GitHub PR 생성 또는 직접 통합 가능

  AGENTS.md 파일

     * 프로젝트 내에 위치한 AGENTS.md는 Codex가 코드베이스를 탐색하고 테스트하는 방법을 안내함
     * README와 유사한 형식의 문서로, 코드 스타일, 실행 명령, PR 메시지 형식 등을 포함 가능
     * 깊이 있는 디렉터리에 있는 파일이 우선시되며, 명시된 테스트를 모두 실행해야 함
     * Codex는 명시적 프롬프트가 AGENTS.md보다 우선이라는 규칙도 따름

  내부 벤치마크 성능

     * OpenAI 내부 SWE 벤치마크에서 codex-1은 최대 192k 토큰, 중간 난이도 설정에서 우수한 정확도 달성
     * AGENTS.md 없이도 높은 성능을 보이며, 사람이 작성한 코드 스타일에 밀접하게 일치하는 결과 생성 가능

  보안 및 신뢰성

     * Codex는 투명성 강화 및 보안 중심으로 설계되었으며, 출력 검증 가능
     * 작업 중 인터넷 연결은 차단되며, 지정된 레포지토리 및 의존성만 접근 가능
     * 악성 코드 개발 차단, 커널 수준의 정당한 작업은 허용하도록 구분 학습 수행

  초기 활용 사례

     * OpenAI 내부에서는 반복적인 리팩토링, 테스트 작성, 문서화 등에 활용 중
     * 외부 파트너 예시:
          + Cisco: 실제 제품 전반에서 적용 테스트 및 피드백 제공
          + Temporal: 대규모 코드베이스의 디버깅, 테스트 실행, 리팩토링에 사용
          + Superhuman: QA 및 통합 실패 수정, PM의 경량 코드 변경 가능하게 지원
          + Kodiak: 자율 주행 기술 코드 분석 및 도구 개발 지원

  Codex CLI 업데이트

     * Codex CLI는 터미널 기반의 경량 코딩 에이전트로, 로컬에서 o3, o4-mini 모델과 함께 작업 가능
     * 이번 업데이트로 o4-mini 기반의 codex-mini 모델 출시, CLI 최적화 및 저지연 응답 제공
     * ChatGPT 계정으로 로그인하면 API 키 자동 설정, Plus/Pro 사용자에게 무료 크레딧 제공

  가격 및 제공 범위

     * Codex는 현재 Pro, Enterprise, Team 사용자에게 제공 중이며, Plus 및 Edu는 곧 확장 예정
     * 초기에는 추가 비용 없이 사용 가능, 추후에는 사용량 기반 가격 정책 도입
     * codex-mini-latest는 1M 입력 토큰당 $1.50, 출력 토큰당 $6, 75% 프롬프트 캐시 할인 적용

  향후 계획

     * Codex는 장기적으로 비동기적 협업 에이전트로 발전 예정
     * Codex CLI, ChatGPT Desktop, 이슈 트래커, CI 도구와의 더 깊은 통합 계획
     * 중간 피드백, 구현 전략 논의, 능동적 진행 상황 보고 기능이 추가될 예정
     * 개발자들이 AI를 통해 더 빠르고 집중된 코딩이 가능해질 미래를 기대하고 있음

  부록: codex-1 시스템 메시지 요약

     * 작업 전후 Git 상태를 확인하고, 반드시 커밋 완료 상태로 유지
     * AGENTS.md 파일 내 검증 절차는 단순 변경이라도 모두 실행 필요
     * PR 생성 시 파일/터미널 기반 인용 규칙 존재 (예: 【F:main.py†L12】)
     * 이전 PR 또는 코멘트 내용은 인용 금지, 오직 파일과 터미널 결과만 사용

   이 시스템 메시지는 Codex 사용자 정의를 위해 모델 기본 행동을 이해하는 데 활용됨.

   드디어 cursor, cline 등의 세대와 구분될 수 있는 다음 세대 agent가 나왔네요. 세상의 Sw 변화 속도가 얼마나 더 빨라질지 기대가 됩니다. 이 다음 세대의 agent의 등장도요.

        Hacker News 의견

     * 우리 팀의 몇몇 엔지니어들과 함께 Assembled에서 Codex 알파 테스트에 참여 경험 공유, 기존에 Cursor와 Claude Code 같은 로컬 에이전트를 오래 사용했기에 큰 기대는 없었지만 Codex의 병렬 작업 실행 능력이 인상적이라는 평가, 여러 개의 리팩터·테스트·보일러플레이트 작업을 한 번에 묶어 컨텍스트 전환 없이 동시에 실행 가능, 기존 솔루션들은 이게 어려웠는데 Codex는 파일이나 함수에 작업을 지정하면 대부분의 PR 스캐폴딩을 자동으로 알아서 처리해 주는 무한한 주니어 엔지니어가 생긴 느낌, 다만 실제로 프로덕션에 넣기까지는 여전히 많은 후처리가 필요, 모델 품질은 괜찮지만 Cursor, Gemini 2.5-pro 등과 나란히 평가했을 때 스타일이나 로직, 네이밍의 명확한 우위는 없는 상태로 기대치를 ‘충족’하는 선이라는 소감
          + 만약 이런 종류의 일을 할 주니어 엔지니어를 고용하지 않는다면 미래의 시니어 엔지니어는 어디서 나오겠느냐는 문제 제기, 최근 딸아이가 좋은 대학에서 컴퓨터공학을 졸업했는데 신입 개발자 자리보다 시니어 엔지니어 수요가 훨씬 많다는 취업 시장 현실 이야기, 최근 회사에서 신입 포지션 채용 공고를 내자 엄청난 지원서가 몰려 공정한 평가 자체가 어려웠다는 소회, 결국 취업에 성공한 아이 친구들은 대부분 인맥 덕분임
          + 지금은 수백만 명의 엔지니어가 Github 오픈소스에 기여하고, 뛰어난 인재들이 그 코드를 이용해 AI 모델을 개발하고 다시 그 엔지니어들을 대체하는 흥미로운 순환 구조, 오픈소스 기여가 많아질수록 관련 직무 대체도 쉬워진다는 본질적 딜레마 언급, 시간이 갈수록 오픈소스 기여의 동기 부여가 약해지는 것 아닌지 질문, 우리가 창의적인 일 한다고 생각했지만 실제론 반복적이고 예측 가능한 지식 조합에 대부분 시간을 쓰며, AI가 이런 종류의 일을 잘 대체한다는 자각, optimistic한 전망으론 장기적으로는 더 흥미로운 일을 만들어가야 하지만 당장 가까운 미래에는 소프트웨어 엔지니어의 공급 과잉·수요 부족으로 수년간 큰 고통 예상
          + Codex의 병렬 작업 실행 기능이 왜 중요한가에 대한 의문 제기, 실제로 LLM이 코드 작성하는 건 몇 초면 끝나고 진짜 시간이 드는 부분은 작업 명세와 검토/수정 단계임, 가장 빠른 부분을 병렬화해서 얻는 효용이 무엇인지 궁금증 표출
          + 주니어 개발자가 완전 자율성이 없다보니 결국 이들을 관리·코드리뷰 하는데 상당한 시간 소모, 막상 주니어를 많이 둬도 그 관리 비용이 병목이 되기 쉬운데, Codex 같은 가상 개발자들을 많이 다루는 게 버거워지진 않는지, 아니면 자율성이 높은지 사용 경험 궁금증
          + Cursor와 Claude Code를 오래 써온 입장에서, Claude Code의 장점과 한계, Codex와 비교했을 때 병렬 작업 실행이 실제로 큰 차이였는지, 최근 나온 Codex CLI도 기대 이하였기에 팀의 Claude Code 사용 경험과 통찰 기대
     * OpenAI의 Codex 프리뷰 영상에서 Katy Shi가 “엔지니어링 일이 코드 작성보단 코드 리뷰 쪽으로 이동”했다는 의견에 공감, AI가 본격 도입되는 시대에 개발자는 여전히 코드와 테스트를 읽는데 머물러 있음을 관찰, 시뮬레이션이라는 비교적 새로운 개념이 도입된다면 특히 프론트엔드에서 코드/테스트만 보는 것보다 다양한 결과 예측이 가능할 것, 최근 이 부분을 주제로 직접 탐구 중이고 Codex 런칭 자료를 보며 실감
          + 나의 Graphite 관련 논지와도 비슷함, 코드의 대량 AI 생성 시대가 오면 검토·테스트·통합이 핵심이 되고, AI 코드 리뷰 시스템도 만들고 있지만 인간 리뷰의 영구적 필요성, 근본적으로 책임 소재 때문임, 컴퓨터는 절대 책임을 질 수 없는 존재임
          + “시뮬레이션을 본다”는 말이 자동화된 테스트 슈트의 활용을 뜻하는지 질문
     * SWE-bench 공동 제작자로서, 이미 강력한 o3 결과에서도 Codex가 소폭 개선을 보여 흥미, Verified 기준 75%에서 85%로 올리는 게 20%에서 75%로 올렸을 때만큼의 긴 시간이 필요할지 궁금증
          + swe-bench 관련 벤치마크 과다 최적화 현상이 있다고 생각, multi-swe-bench, swe polybench, kotlin bench 등 다양한 측정 결과 공유
          + 20%에서 75%까지 도달하는 데 걸린 시간 궁금증 제기
     * Pro 버전 구독 중인데 Codex 체험하려고 할 때마다 팀 요금제 결제 페이지로 이동, 정식 오픈 전이거나 뭔가 놓치는 것인지 궁금, 오픈AI 제품 꾸준히 써오고 있고 Codex도 정말 써보고 싶음
          + 주요 업데이트 때마다 비슷한 일 발생, 이해하기 어렵다는 반응
          + 나도 비슷한 상황, 몇 분 전부터 가능하게 된 것 같으니 서비스 점진 출시 중이라는 판단
          + 아직도 점진적으로 출시 중이라는 안내
     * 라이브 스트림에서 ""microVM"" 언급, 브라우저/인터넷 접근 불가, Firecracker/Unikraft 등 마이크로커널 사용이 빠르고 저렴하게 대규모 확장 가능, 하지만 에이전트별 분리된 완전한 컴퓨터 환경으로 넘어가는 데 큰 기술적 장벽 예상, 현재 ChatGPT Operator는 브라우저 접근 지원하므로 기술적으로 가능하겠지만 수요 규모가 다를 것으로 판단, fork/snapshot/screen/human-in-the-loop 지원 등 AI전용 전체 PC 환경 제공 인프라 기업이 등장할 여력 충분, 현재 브라우저 활용 등 부분적 기능 구현에 머물러 있음
          + E2B Desktop으로 이 기능 제공 중, 데모와 SDK 소개 링크 공유
     * 은행 근무 시 법무팀이 앱에 사소한 변경 요청을 자주 해왔는데, 이제 스스로 수정 가능해질 듯, 법무팀이 매우 뿌듯해할 것으로 생각
          + 코드 실행·테스트와 코드 리뷰 없이는 법무팀에 코드 변경 권한 주는 것은 위험, 결국 아무도 그럴 일 없을 것으로 예상
          + 미래엔 버그트래킹이 확 바뀔 전망, 조직 내 누구나 이슈나 기능 요청을 남기면 모델이 자동 대응, 안 될 경우 사람 개입, 결국 ‘어떤 코드 변경이 합법적이고 회사 기준에 부합하는가’에 대한 판단과 리뷰는 점차 비기술적 검토자의 핵심 역할로 부상
          + 현실적으로 법무팀이 코드 변경을 직접 하진 않을 것이라는 약속
     * 프라이버시, 학습 데이터 opt-out, 그리고 플랫폼을 통해 만들 모델로 경쟁할 때 발생할 수 있는 리스크에 대한 우려, “네가 만든 산출물을 네가 경쟁에 쓰면 안 됩니다”라는 정책이 공정한지 의문, 혹시 지나치게 비관적인 시선인지도 모름, OpenAI가 우리가 만든 정보를 경쟁에 활용하지 못하게 막으려 할 때 문제 제기
          + 동영상에서 레포에 대해 학습 허용 여부를 직접 선택하는 명시적 옵션이 있음을 안내
     * ""secrets"" 기능 사용 중 문제 경험, 환경 세팅에서는 잘 주입되지만 실제 작업에서 동작하지 않고, 환경 재설정 등의 방법에도 항상 재현되는 이슈 공유
     * Codex가 클라우드에서만 동작해 코드가 자동 커밋-푸시 되어버리면 내가 내부적으로 검토할 시간이 없는 걱정, aider에서는 커밋 후 git reset HEAD^, git diff로 변경점 직접 확인하고 필요한 수정을 한 뒤에야 커밋-푸시 하는 워크플로를 선호
          + 어차피 커밋을 바로 롤백한다면 Aider에 --no-auto-commits 옵션 추천
          + Codex를 쉽게 말하면 기존 Codex CLI의 매니지드 클라우드 버전, 핵심은 새 모델 자체이고 곧 API로도 제공 예상
          + 라이브 스트림에서 작업 완료 후 diff가 바로 보여지고, diff 확인 뒤에만 github pr 생성 결정 가능한 구조임을 안내
     * 회사가 코드베이스를 AI 공급업체에 공유하는 데 대해 어떻게 생각하는지 궁금, 아니면 로컬 설치로만 사용하는지 질문
          + 기업들은 SaaS에 코드 공유가 매우 흔하며, 보통 별도 계약을 통해 임의 활용을 막음
          + 대부분 기업의 코드는 자기 회사에만 의미 있는 가치라는 판단
          + OpenAI 같은 곳이 굳이 내 코드를 보며 위험을 감수하진 않을 거라 생각, 법적 리스크 감수할 가치 없다 판단
          + 결국 이 모든 것도 비용-편익 트레이드오프, 이득이 크면 충분히 공유 가치 있음
          + Cursor에는 엔터프라이즈 모드에서 데이터 프라이버시 강제 기능이 있음
"
"https://news.hada.io/topic?id=20937","코인베이스, 해커가 직원 매수해 고객 데이터 탈취 후 2,000만 달러 몸값 요구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             코인베이스, 해커가 직원 매수해 고객 데이터 탈취 후 2,000만 달러 몸값 요구

     * 코인베이스가 해커 집단에 의해 직원 매수 및 고객 데이터 유출 피해 발생
     * 해커들은 고객 정보를 탈취한 뒤, 2,000만 달러 몸값을 요구함
     * 내부 직원이 해커와 협력하는 유례없는 보안 위협이 확인됨
     * 이 사건은 거래소 및 핀테크 업계에서 내부자 위험 관리의 중요성 재조명 계기임
     * 피해 확산을 막기 위한 긴급 대응과 고객 보호 조치가 이루어지는 중임

        Hacker News 의견

     * 나는 최근에 정교한 스피어 피싱 전화를 계속 받고 있음. 이들은 내가 의심스러운 거래를 확인해야 한다고 주장하는 전형적인 수법을 사용함. 미국식 영어에 능통하고 매우 친근한 목소리, 그리고 내 계좌 잔액까지 아는 모습임. 첫 전화 때 바로 사기임을 알아차렸고, Google의 콜 스크리닝 기능 덕분에 이후엔 안전함. 이런 전화를 Kitboga에게 넘기고 싶은 마음임. 처음엔 Coinbase 고객 대상으로 사기 시도하다가, 결국 Coinbase 자체에 협박을 시도함
          + Coinbase에 자산이 꽤 있었던 적이 있다면, 스피어 피싱은 걱정거리 중에 가장 작은 부분임. Coinbase가 이름과 주소만이 아니라 잔액, 거래 내역, 신분증 이미지까지 유출함. 많은 사람이 실제 거리나 집에서 공격받거나 가족이 납치 당하기까지 함. 1만 달러 이하도 '상당한' 자산으로 간주함. 지금까지 최고의 방어는 비밀 유지였지만, Coinbase 덕분에 이제 그마저 사라짐. 나쁜 사람들은 Coinbase에서 한 번이라도 큰 금액을 보유했거나 현금화했고, 가족 정보를 쉽게 찾거나 협박 대상으로 삼을 수 있음. Coinbase가 이로 인한 피해 전부를 보상해야 할 의무가 있어도, 그렇게 하면 회사가 파산할 수준임
          + Pixel에서 iPhone으로 바꿨는데, 스팸 전화가 너무 많아서 당황스럽다는 생각임. iPhone에서는 이걸 어떻게 대처하는지 궁금함
          + 스피어 피싱 전화가 늘어난 지 얼마나 됐는지 궁금함. 나는 Coinbase가 해킹 공격이 시스템적 문제라고 주장하는 걸 믿지 않음
          + Coinbase 로그인 인증 코드 문자를 아무 시도 없이 계속 받고 있음. Microsoft 계정도 마찬가지임. 누군가 내 이메일이 로그인에 쓰일 수 있는지 테스트 중인 것으로 추정함
          + 발신 번호가 어디인지 궁금함. 나는 많은 피싱 전화를 받았지만, 모르는 번호는 절대 받지 않음. Google 콜 스크린 덕분에 그들은 매번 끊음. 그래서 사기임을 확신함
          + AI 덕분에 사기는 더욱 정교해짐. 철자 실수도 많이 사라짐. 최근에 피싱 이메일을 흥미 삼아 살펴봤더니, 이상한 유니코드 문자가 잘못 번역된 부분을 발견함. 완벽하지는 않지만 점점 교묘해지는 모습임
          + 지난주에만 세네 번 받았다는 경험임
          + 그 완벽한 억양이 혹시 머신러닝 덕분인지 궁금함
          + 그들이 완벽한 영어를 구사하고 계좌 잔고까지 안다는 사실에 대해 Coinbase 전 직원이 아닐지 상상해보게 됨
          + 이런 일이 절대 멈추지 않을 것이고, 이제 범죄적 요소가 합법적 자본주의가 됐다는 씁쓸함을 느낌
     * 유출된 데이터가 계정 복구에 쓰이는 정보와 동일해 보이는 점을 문제로 지적함. 즉, 직접 실수하거나 혹은 Coinbase 측의 문제로 계정에 접근 불가해지면 복구 과정이 더 이상 간단하지 않아짐. 해커가 유출된 정보로 계정 복구 시도 가능성도 있음. 해결책은 현실에 계정 복구를 지원하는 오프라인 지점(IRL office)이 필요함을 주장함. 현장이라면 범죄자를 검거하고 기소할 수 있음. 해외 범죄자에게 큰 장벽이 됨. 가장 확실한 해결책은 Yubikey 같은 하드웨어 2단계 인증임
          + 암호화폐 업계가 전통 금융 시스템이 왜 존재하는지의 이유를 다시 한 번 빠르게 깨달아가고 있음. 말하는 IRL 오피스는 많은 암호화폐 지지자들이 '은행'이라고 부르는 개념임
          + 본인 지갑에서 직접 입출금 기록이 있다면, Coinbase에 주소 등록된 것으로 그 키로 서명한 메시지를 보내는 방식도 신뢰할 수 있는 복구 수단임. 암호화폐의 본질은 PKI의 다른 형태임
          + 오프라인 지점이 생기면, 사용자 잘못이 아니라도(예: 시스템이 과민하게 위험을 탐지해) 계정 접근이 막히는 상황에서 멀리 다른 도시까지 가야 한다는 현실이 생김. 그렇게 되면 락아웃된 사람들이 크게 불만일 것임
          + Yubikey를 쓸 정도로 충분히 기술에 밝은 사람이라면 아예 하드웨어 월렛을 사서 직접 보관할 것이라고 생각함
          + Coinbase에 오프라인 지점이 필요하다면, 그건 그냥 은행이라는 뜻임
          + 하드웨어 2단계 인증(Yubikey)이 유일한 해결책이라 해도, 잃어버리면 다시 계정 복구 단계로 돌아감
          + Coinbase에 오프라인 지점이 필요하다는 말은 풍자냐는 반문임
     * 나는 Coinbase 고객지원에 이번 해킹 영향 여부를 문의함. AI 챗봇 거치고 실제 상담원 연결되니, 이들은 해킹 사실 자체를 모르고 있었음. 내가 처음 알린 셈임
          + 영향받은 계정에는 이메일 발송됨. 나는 영향받은 사용자였음
          + 첫번째 고객이 단순히 게으른 상담원을 만났던 것일 수도 있음
          + 담당 상담원이 ""몰랐다, 당신이 이 얘길 처음 해준다""라는 매뉴얼 대사만 읽어줬다는 경험임
     * Coinbase가 해외의 '불량 지원 직원'과 자신들의 거리를 두려는 노력이 엿보임. 만약 그들이 실제로 Coinbase 직원이나 계약자였다면, 회사가 사실상 직접 해커에게 데이터를 팔아넘긴 셈임. 사기로 손해 본 고객에게 보상하는 건 상식적임. 하지만 실제로 이사하거나 은행, 이메일을 바꿔야 했거나, 심지어 보안 요원을 고용해야 했다면 이런 비용까지도 회사에 청구할 수 있을지 궁금함
          + 직원이 회사 방침을 어기거나 불법적으로 기밀 데이터를 빼돌려 해커에게 넘겼다면, ""우리 회사가 데이터를 판 것""이라고 말하긴 무리라는 의견임
     * Coinbase 공식 블로그에 언급된 내용을 공유함: 공격에 속아 자금을 이체한 고객에겐 보상 진행, 정보가 노출된 고객에겐 이미 5월 15일 7:20(미 동부 기준) 이메일 발송함
          + 노리플라이 이메일 선택이 인상적임. Coinbase가 중앙화와 고객센터가 큰 장점이지만, 이 점이 사회공학 공격을 가능하게 한다는 점을 보여줌
          + Coinbase Prime 계정은 유출 범위에 포함되지 않았다는 점이 궁금함. Prime 계정에 특별한 보호가 있는지, 아니면 그런 계정에는 사기꾼들이 덜 관심을 가졌던 건지 궁금함
     * 정부의 KYC(고객신원확인) 법 때문에, 본인인증에 필요한 것보다 훨씬 많은 민감 정보를 Coinbase가 반드시 저장해야 하는 상황임. 신분증 사진까지 절도당한 건 정부 탓이며, 이 정보로 무엇을 할지 모를 범죄자와 내부 직원까지 문제임
          + KYC는 그 자체로 충분한 이유가 있음. 문제는 정부 규제가 아니라 고객 데이터 관리를 허술하게 하는 민간 기업임. 관리가 허술할수록 비용이 적게 들고 자기 정보가 아니니까 보호 동기가 부족함. 강제성이 없으면 지키지 않을 것임
          + KYC 때문에 민감 정보를 계속 가지고 있어야 한다는 건 핑계임. 왜 모든 상담원이 영구적으로 신분증 문서에 접근할 수 있는지 솔직해질 필요가 있음
     * Coinbase의 대응이 꽤 괜찮다는 생각임: 2천만 달러의 몸값 요구는 절대 내지 않고, 그 대신 범인을 검거하는 정보에 2천만 달러의 보상금 기금 마련임
          + 1996년 영화 'Ransom'에서 썼던 같은 수법이라는 지적임
          + 고객 입장에선 데이터 유출을 제한하기 위해 몸값을 내는 것이 더 중요한데, 보상금을 따로 마련하는 것까지는 좋지만 당장 데이터 보호가 더 급하다는 의견임
          + 이런 대응 방식이 마음에 듦. 이런 때일수록 회사가 딱딱한 문구 대신 “해커 XX들아, 엿 먹어라!” 같은 강렬한 메시지를 주면 많은 사람들이 호응할 거라고 생각함
     * '불량 해외 지원 직원'을 모집했다고만 표현함. 어떤 나라였고 처음에 왜 그들을 고용했는지 설명하지 않음. 이미 수십억 달러 매출의 회사가 제대로 된 인력 채용과 검증조차 못했다는 게 의아함
     * ""2천만 달러 보상금 기금"" 제도 도입에 대해, 평소에 암호화폐 업계를 주로 비판하지만 이번만큼은 칭찬함. 정말로 보상금을 지급하길 바람
          + 그 보상금을 암호화폐로 지급할 수도 있다는 농담
     * 데자뷰 느낌임. 만약 해커가 돈을 요구할 때에서야 뭔가 이상함을 알아챘다면 Coinbase는 직원 접근 로그도 남기지 않은 게 아닌지 의심됨. 내부적으로 최소한의 책임 추적 방법이 있는지 궁금함. 간단히 접근 추적 시스템을 만들고 이상 징후나 악의적 직원을 즉시 차단하면 좋을 텐데. 이 사태 후 고위직 퇴직 보상이 어떤 모습일지 지켜보고 싶음
          + 공식 블로그를 보면 고객지원 직원들이 이미 접근 권한이 있는 민감 데이터를 공격자에게 돈 받고 넘긴 것임. 해커가 직접 계정에 접근한 건 아님
          + 나는 미국 외 지역 직원을 채용하게 되면서 내부 관리자 패널에 여러 단계의 보안 장치를 추가해야 했던 경험이 있음(로그 기록, 모니터링, 관리자 승인 등). 신용카드 업계는 PCI-DSS 기준 덕분에 데이터 보호 기준이 엄청 엄격함. 암호화폐 업계는 이런 규정 미비로 인해 보안 의식이 상대적으로 낮은 듯함
          + 최소한의 대응으로 로그 기록 및 사후 감사가 필요함. 고객센터 직원들에게 고객이 쉽게 알 수 없는 확인 정보를 요구하는 것도 무리가 아님. 고객이 직접 락아웃된 경우엔 극소수만 더 엄격히 관리되는 직원이 처리하면 됨. 월 사용자 중 1% 미만만 접근됐다는 통계도 충분히 큰 수치라고 생각함
"
"https://news.hada.io/topic?id=20912","Show GN: Tacket - JS 한줄로 티켓 버튼을 내 사이트에 추가하는 서비스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Show GN: Tacket - JS 한줄로 티켓 버튼을 내 사이트에 추가하는 서비스

   안녕하세요. 맨날 긱뉴스 보기만 하다가 제가 만든것을 직접 올리게 되는 날이 오네요.
   여기 올리기에 부끄러운 수준인데.. 제가 생각했던 초기 아이디어가 어느정도 구현이 되어 피드백을 받고 싶어서 올려봅니다.

   제목대로 JS 한줄을 추가하면 내 사이트 오른쪽 하단에 버튼이 하나 생기게 되고, 사용자들이 이걸 눌러서 티켓 서비스처럼
   문의를 하거나 오류 리포트를 하거나 등등 필요한 내용을 사이트 관리자에게 전달할 수 있는 서비스입니다.

   티켓입력시, 사이트 관리자가 메일로 해당내용을 받게 되고요.
   티켓 상태 변경시, 코멘트 입력 시, 사용자가 메일로 알림을 받게 됩니다.

   SMS로도 알림을 받게 할 예정인데 현재는 비용 때문에 ㅎㅎ 사용자가 많아진다거나 하면 구현 예정에 있습니다.

   이 서비스를 만들게 된 이유는 https://www.tacket.co/about 에 적었는데요.
   저는 고객사에 SM으로 파견나가 있는 상태인데요, 지금은 좀 안정화가 되었지만 초기에는 정말 문제가 많아서 너무 많이 그리고 자주 사용자들에게 연락을 받게 되었습니다.
   하루에도 수십번 메일, 메신저는 당연하고 직접 찾아오면서 종이나 포스트잇을 놓고 가시는데, 단순히 어떤 데이터에 어떤 문제가 있다 이런 정도의 내용들이 많았습니다.

   저희는 작업중에 방해받기를 무엇보다 싫어하지 않습니까 ㅎㅎ
   그래서 뭐 방법이 없나 생각하다가 문득 이런 아이디어가 떠올랐고 기술적으로 이게 되나 싶었는데 여기저기 찾아보기도 했고, 비슷한 개념의 다른 서비스가 있는걸 보고 되겠다 싶어서 만들어 보았습니다.

   스택은 다음과 같습니다
   SvelteKit
   Supabase
   CloudFlare pages
   Resend
   IDE: Cursor AI

   그동안 안해본 새로운 언어를 해보자 싶어서 SveltKit 이라는 것을 처음 사용해보았고요. node쪽은 그동안 찍먹만 해보고 제대로 써본 것은 처음인데 여러 면에서 개발 생산성이 높더라고요.
   그리고 그렇게 도전할 수 있었던 이유는 역시 AI 였습니다. AI는 저희를 대체할까 두려워 하면서도 많은 도움도 받게 되는 그런 이중적인 존재 같습니다. 여튼 cursor가 없었으면 이렇게 단시간 내에 만들지는 못했을 겁니다. 3개월 정도 걸린것 같은데, 퇴근 후 몇시간씩 작업했던 것을 생각하면 cursor의 도움이 절대적이었다고 봅니다.

   읽어주셔서 감사하고 사실 부끄러운 정도의 퀄리티인데 사용해보시고 피드백을 혹시 주시면 정말 감사할 것 같습니다.

   아이디어가 정말 좋네요!! 저도 한번 적용해보고 싶네요 ㅎㅎ

   사이트에 레트로 테마 색감이 개인적으로 정말 마음에 듭니다. 서비스 아이디어도 정말 좋네요!

   아이고 정말 감사드립니다 ㅎㅎ UI 쪽은 잘 몰라서 tailwind로 힘겹게 짜깁기 했는데 칭찬해주시다니..

   완성도를 높이고 지라나 슬랙 같은 앱 연동까지 좀 해야 실제 사용자가 있을거 같네요. 좋은 하루 되시기 바랍니다.

   멋집니다! jira 로 연동되어 티켓 따지면 생기면 좋을 것 같네요

   답변 감사합니다! 말씀하신 부분도 계획은 해놓고 있습니다ㅎㅎ
"
"https://news.hada.io/topic?id=21037","AI가 Microsoft 개발자들을 미치게 만드는 걸 보는 게 새로운 취미가 되었어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            AI가 Microsoft 개발자들을 미치게 만드는 걸 보는 게 새로운 취미가 되었어요

     * GitHub과 Microsoft가 GitHub Copilot Agent의 퍼블릭 프리뷰를 발표하면서, .NET Runtime 저장소에 실제로 이 에이전트가 PR을 자동 생성하는 테스트가 진행됨
     * 그러나 이 PR들은 부실하거나 불필요한 수정을 포함하고 있어 리뷰어들이 곤욕을 치르고 있으며, Reddit 사용자들은 이를 웃픈 풍경으로 받아들이는 중
     * 예시 PR:
          + PR #115762 – ""string.Concat"" 호출에서 Null 체크가 이미 되어 있는 코드에 또 불필요하게 체크 추가
          + PR #115743 – 아무 영향 없는 조건문 리팩토링을 제안
          + PR #115733, PR #115732 등도 비슷한 맥락
     * 작성자는 ""이게 업계의 미래라면 나는 내릴 준비가 됐다""며 AI 도입에 대한 피로감과 회의감을 드러냄
     * 하지만 동시에 ""리뷰를 맡은 직원들에게는 연민을 느낀다""고 강조하며, 이 상황은 위에서 내려온 Copilot 도입 지시에 따른 부담일 가능성이 높다고 덧붙임

     제 ""schadenfreude(행복감)""는 AI 과대광고를 부추기는 마이크로소프트 경영진을 향한 것입니다. 개발자들을 존중해 주시기 바랍니다.
          + schadenfreude는 독일어에서 유래한 단어로 직역하면 “해로움에서 오는 기쁨”. 즉, ""타인의 불운에서 느끼는 몰래 기분 좋은 감정""

주요 댓글들 요약

  1. AI가 작성한 PR은 부정확하고, 맥락을 이해하지 못한 채 단순히 '추측'만 반복

     * 실제 PR 코드가 무엇을 하는지 이해 없이 변경을 제안함
     * 반복적인 오류 수정 → 여전히 잘못된 코드 → 또 다른 오류 수정…의 끝없는 루프
     * “수정했어요” → “아직도 틀렸어” → “이번엔 정말 고쳤어요”… 이 과정이 Junior Dev 패턴 같다는 의견도

  2. 복잡한 문제 해결엔 오히려 더 많은 시간 소요

     * 단순한 수정에는 도움이 되지만, 진짜 시간 아끼고 싶은 복잡한 문제엔 쓸모없음
     * 문제 이해 → Copilot 이해 → 비교 → 확인 → 수동 조치 필요
     * 실제로는 내가 직접 해결하는 게 빠름

  3. 기업 리더들의 'AI 만능주의'가 개발자를 소외시키고 있음

     * ""Copilot을 쓰면 뒤처지지 않는다""는 메시지는 실무 개발자와 괴리
     * PR 리뷰 시간은 길어지고, 책임은 개발자에게 전가
     * Copilot이 만든 코드로 학습된 AI가 다시 코드 품질을 악화시키는 'AI의 AI를 위한 학습 루프' 우려

  4. AI는 확신에 찬 ‘틀린 답’을 내놓을 뿐, '이게 맞다'는 확신은 없음

     * 틀렸다는 피드백에도 “수정했습니다!” → 더 이상한 수정 제안
     * ""이건 괜찮은 코드야, 고칠 필요 없어""라는 판단은 하지 않음
     * 이는 법적 책임 회피를 위한 설계일 수 있다는 지적도

  5. 지속적인 AI 도입 강요로 개발자 경험은 피폐해지고 있음

     * 관리자 지시나 실적 평가 때문에 AI 도입 실험이 이어짐
     * 개발자들은 자신이 AI의 베이비시터가 된 듯한 피로감을 호소
     * 이 흐름이 이어지면 ""개발자들이 AI에 지쳐 업계를 떠날 것""이라는 비관적 전망도

주요 문장들

     * “AI는 잘못된 추측을 반복하면서도 자기 주장을 확신하는 인턴 같아요”
     * “Copilot 코드 리뷰하는 데 시간 쓰느니, 내가 새로 짜는 게 낫다”
     * “이건 개발자가 기계를 돕는 'reverse centaurs' 상태”
          + Cory Doctorow가 쓴 단어로, ""우리는 기계의 도움을 받는 인간이 아니라, 기계를 돕도록 강요받는 인간이라는 것""
     * “Copilot은 개발자가 엉터리 반창고를 붙이는 것과 같은데, 다만 자동화 되어있어서 수천개의 부담스러운 반창고가 됨”
     * “LLM은 ‘잘못될 수 있지만, 불확실하진 않아’가 기본값인 듯”

   이게 그냥 신기술이라고 생각할 때는 호기심 섞인 눈으로만 봤는데,
   이제 이런 기술을 이유로 실제로 고용주가 고용과 임금 삭감을 진행하니 기분이 별로 좋지만은 않네요..

   문제를 던지고 답이 틀리면 버리는 비동기식 워크플로우로 생산성이 많이 올라갔는데 이건 정적도구랑 비슷하지 않나요? 아주 진화된 정적분석 도구라고 생각하면 좋은 친구입니다. 솔직히 분석도 빠르고 주니어 엔지니어보다 많이알고

   ai를 활용하긴 하는데, 애가 멍청해서 바로잡아주지 못하면 제대로 구현하지 못합니다. 바이브코딩으로 하는 거 보면 죄다 오류 투성이 코드...

   LLM 사용할 때 마다 이런 경험을 합니다. 못 하는 부분은 무한하게 지적을 해도 계속 못합니다.
   결국 지쳐서 직접 분석하고 고치게 됩니다. 이런 경험이 누적되다 보면 LLM이고 뭐고 그냥 다 쓰레기같고 쓰기 싫어져요.

   AI가 인간이 코딩하도록 역 프롬프팅 한다는 웹툰이 생각나네요.

   https://comic.naver.com/bestChallenge/detail?titleId=818158&no=21

   정말 AI 가 사람처럼 문제 해결과 학습을 동시에 하지 않는 이상은 계속 발생할 문제 같습니다

   데드라인이나 요구사항 만 잘 지키면 코딩할 때 ai를 썼니, ide도 안쓰고 상남자스럽게 메모장만 썼니 같은 건 별로 안 중요할텐데요.

   아무래도 아직은 과도기여서 여러 해프닝들이 발생하는 것 같아요.
   앞으로 더 좋게 바뀔 수도 있고, 꾸준할 수도 있는 만큼 어떻게 바뀔지 보는 것도 재밌겠네요 ㅋㅋ

   Github에 Gemini 붙여서 PR 리뷰 받고 있는데, 딱 저럴 때가 종종 있더라고요.
   바로 위에 line에서 null 체크를 했는데, null 체크 없이 쓰고 있다고 바로 위에 있는 라인을 똑같이 추가하라고 리뷰를 해준다던가.

   사람이 업무를 할 때 자연스럽게 알게되는 배경지식과 업무패턴, 기대결과와 그 형식등을
   전부 프롬프트로 적을수도 없거니와 적을 수 있다고 하더라도 LLM 같은 복잡한 ai 가 아닌,
   딥러닝 이전의 전통적인 알고리즘으로 자동화하는게 현실적이겠다 라는 생각도 들어요.

   써보면 바이브코딩, 코딩에이전트가 확실히 편리한 부분도 있지만, 편리하려면 프롬프트를 엄청 깐깐하게 보내야 하고, 애초에 프젝 성향에 따라 잘 안되는 프젝도 많죠. MSA구조 웹서버처럼 기능들이 간결하게 잘게 쪼개져 있으면 잘 일하는데 빅 모노리스에 엮여있는 모듈이 많고 복잡한 로직을 ai로 고치려고 하면 엄청 깐깐하게 task 계획짜고 프롬프트 잘 보내고 해야 하고

        Hacker News 의견

     * 모든 댓글에 ""Help improve Copilot by leaving feedback using the or buttons"" 메시지가 달려있지만 실제로는 어떤 피드백도 달린 것을 본 적 없는 상황의 흥미로움 공유, LLM 활용 시 시스템 프롬프트 세팅이 제대로 안 되면 이런 일이 흔하게 발생하는 경험담, 가장 웃긴 PR 예시는 테스트 실패를 해결한다며 테스트 케이스를 아예 지우거나 주석처리해버리거나 assertion을 그냥 바꿔버리는 것, Googles와 Microsofts의 모델이 OpenAIs, Anthropics보다 이런 상황을 더 자주 보이는 것 같다라는 추측과 각 회사 내부 프로세스 차이가 결과에 드러나는 것 같음, 실제 PR에서 사람이 3번 더 지적한 뒤 포기하는 과정 소개, 이런 PR들을 리뷰하는 이들의 심정 상상조차 어렵고 마치 말 안 듣는 신입 개발자를 상대하는 느낌인데 아예 맥락 이해 자체가 없는 존재, 특정 PR의 90%가 ""Check failure""로 채워져
       코드/차이 확인 자체가 어려운 예와 단위 테스트에 ""Test expressions mentioned in the issue""라고만 적혀있는 슬픔, 만약 이러한 상황이 인간 측에 너무 고통스럽지 않았다면 진짜 재밌는 일이라는 솔직한 의견
          + 신입 개발자와 비교한 비유는 너무 과장됨, 자신도 신입 개발자와 함께 일하지만 그들은 LLM처럼 이상한 실수를 자주 하지 않고 손이 많이 가지도 않으며 금방 배움, LLM은 조심히 사용해야 하는 괜찮은 보조 도구고 속도 개선에 도움도 되고 아이디어 브레인스토밍 파트너로도 괜찮음, 다만 인턴이나 진짜 개발자를 대체할 수는 없다는 생각
          + 소프트웨어 엔지니어링 분야 80년대 후반 입문 당시엔 즐거움이 있었는데, 요즘은 면접 과정, 중소기업의 빅테크 따라하기, 그리고 지금의 AI PR 실험 등으로 독성이 넘치는 환경 변화, 오늘날 프로 개발자로서의 일에 과연 기쁨이 남아있는지 회의적 감정
          + 적어도 신입 개발자는 PR 보내기 전에 로컬에서 테스트 돌려보라고 말이라도 할 수 있음, 어느 순간 인간 개발자가 그냥 포기하고 ""AI 쓰레기"" PR을 닫아버리고, 잘 작동하는 것만 남기고 다 버릴 것 같다는 우려, 기계의 실험을 계속 감내하다 그 한계에 이르면 모두가 지쳐서 그만둬버릴 날이 올 것 같은 생각
          + 굳이 피드백 시스템이 필요한가에 대한 의문, 개발 기준에서 성공은 첫 시도에 머지되는 PR, 실패는 에이전트가 요청한 수정 개수만큼 누적된 악화로 파악, 수동 피드백 요구는 비효율적이며 개발자들과 똑같이 사이클 타임, 승인률, 변경 실패율 같은 지표로 성과 측정을 하는 편이 낫다는 주장
          + Microsoft 지원팀과 소통했을 때 벽과 대화하는 듯한 느낌을 받은 경험 공유, 여러 케이스를 접수해도 만족스럽게 해결된 적 없었음, Microsoft가 자기네 기술을 스스로 써보는 것은 이해하지만 그걸 내게 강요하진 말라는 부탁, Microsoft가 지원 준비가 안된 제품은 출시하지 않기를 바라는 의견
     * 최근 Google의 Eric이 AI에 대해 언급하는 영상을 시청한 경험, 본인은 AI가 현재 과소평가되고 있다고 주장, 로켓 회사를 산 이유와 Deep Research(딥리서치) 등 비전문 분야에 AI로 도전하는 과정에서 ""전문가가 아니다""라는 점을 강조한 것 인용, 본인 역시 AI를 싫어하지는 않지만 현 세대 패턴 복원 기반 생성 AI는 '초심자 감탄 유도' 능력이 뛰어남, 해당 분야 지식이 없다면 결과물이 대단해보이지만 깊게 알게되면 허점에 금방 실망, Microsoft처럼 프런트라인에서 일하는 사람은 문제가 뭔지 명확히 알지만 경영진(특히 Eric 같은 인물)은 AI가 현란한 말만 뱉어도 속아넘어가기 쉬운 단점, 언젠가 AI가 제대로 동작하는 코드를 직접적으로 쓸 수 있을 날이 오리라 기대는 하지만 지금은 멀었다는 시각
          + AI 활용 시 주의 깊게 아주 제한적으로만 쓰는 본인 방식, 반면 저런 ""로켓 회사 산"" 억만장자들은 AI에 열광하며 자신들의 재산을 계속 불릴 투자 결정에도 활용, 설사 큰 실패를 해도 잃는 건 일부 액세서리 수준이라 사회 변동에 타격 없을 것, 반면 현장의 IT 일자리는 양쪽 모두 안좋은 결과로 이어질 위기감
          + 전문가가 아닌 리더가 AI에 쉽게 감탄하는 상황과 함께 Google이 미국 군대와 협력해 대규모 자율 드론에 Gemini를 탑재하면 어떤 일이 벌어질지 상상
     * Microsoft 직원들이 실제 문제를 해결하는 대신 LLM과 몇 시간씩 설전 벌이는 모습을 보는 게 .NET에 제품을 구축한 기업엔 어떤 신뢰감을 줄지 의문
          + 예전에 LLM 도입 전에도 GitHub 이슈에서 사용자가 문제를 제대로 설명 못하고 관리자가 점점 짜증 내는 상황을 봤음, 이제는 짜증내는 최종사용자조차 필요 없어짐
          + 오히려 이런 결과야말로 형편없는 관리와 엉성한 지시의 자연스러운 귀결, 이번엔 더이상 신입 탓을 할 수 없게 되었고 스스로만을 탓해야 하는 상황
          + 특히 유명한 .net 퍼포먼스 블로그로 알려진 Stephen Toub까지 이런 과정에 참여할 때 느끼는 아픔 강조
          + 이런 신기술 실험을 하는 걸 막고 싶지 않지만, 단지 지금은 실험이 모두에게 공개된 것 뿐이라 차별점 설명
          + Microsoft가 예전부터 문제 생기면 ""오류 그냥 무시해"" 식으로 Will Not Fix로 넘기면서 관리자의 자기만족에 빠진 문화였기 때문에 결국 지금 벌어지는 모든 일을 자초했다는 냉소적 주장
     * 첫 PR에 달린 코멘트로 맥락 설명, 다양한 실험을 통해 툴의 한계를 알아보고 미래에 대비 중이며, 머지의 책임은 기존 PR과 똑같이 유지보수자에게 있다는 것, 품질 기준 충족 전까지는 어떤 것도 머지되지 않음
          + 해당 코멘트 작성자인 Microsoft 직원은 AI 활용을 고민하지 않으면 도태된다는 의견도 제시, Microsoft가 AI로 인해 소프트웨어 엔지니어링 업계가 뒤집히는 불안/흥분에 휩싸인 분위기, 실험이 툴 한계 파악이 아닌 일자리 지키기 위해 무작정 동참하는 것으로 읽혀 오히려 신뢰감 저하
          + 관리자들이 모델 역량에 이미 결론을 낸 게 아니라 리얼월드 맥락에서 강점/약점 파악을 위한 합리적 실험이라는 점을 이해해야 함
     * CI를 통과하지 못한 PR을 굳이 누군가에게 assign하는 상황 자체가 이상함, 최소한 통과된 PR만 할당해야 정상인데 시스템이 너무 엉망이라 그것조차 불가능하단 느낌, 제대로 작동한다면 그 정도는 기본으로 할 수 있어야 한다는 냉소
          + 모든 상황을 최악의 시나리오로만 해석하지 않길 바람, 실제로 관련 인간들은 실험임을 알고 기대치도 조절된 상태일 것, 이런 실험적 접근 없이는 시스템 발전이 어렵기에 실제 환경에서 튜닝과 테스트를 거치는 중일 수도 있음, 본인 회사도 AI 코딩 지원 도구 도입 초기에 똑같은 실험을 했고, 인간이 직접 코딩했을 때보다 코드 품질은 나빴으나 그 과정을 통해 새로운 점을 많이 배움, 기준점이 있어 개선 추이를 명확히 알 수 있었음
          + 댓글에서 방화벽 이슈로 인해 테스트 통과 여부를 체크 못하는 상태라는 설명이 있었고 그 문제만 해결하면 정상 작동 가능
     * AI 에이전트 대신 다른 신기술을 대입해봐도 아래와 같은 회사의 전형적 모습, 오픈하게 실험(big tech식 dogfooding), 업계 기술력 발전에 실제로 기여, 문제 발생 시 피해는 전적으로 팀 내부에 국한, 이런 실험을 지지하지 않을 이유가 딱히 없다는 질문 제시
          + 이런 오픈 실험에 모두 비난만 쏟는 분위기가 의아, 비공개 포크로 숨기지 않고 실제 역량을 투명하게 공개하는 게 훨씬 유익하며 세일즈 마케팅의 허풍보다 낫다고 생각
          + 소프트웨어 개발의 핵심 인프라 프레임워크에서 이런 실험을 하는 건 논란의 여지
          + ""우리가"" 왜, 어떻게, 무엇을 지지/비지지해야 하는지 의문, 개인적으로 MS가 호들갑스럽게 실패하는 장면이 그냥 웃김
          + 진짜 ""진보""라고 보기 어려움, 내부 POC(사전 검증) 없이 대외적으로 시스템 문제 드러낸 게 오히려 무책임, 방화벽 등 기초 환경 검증이나 다른 사내 코드베이스에 먼저 시도하지 않은 이유 의문, 인프라 코드는 높은 기준이 필요한데 ""dogfooding"" 명분이라도 하위 프로젝트부터 하는 게 맞음, ""state of the art"" 조차 아니다라는 비판, 투입비용 대비 너무 조악한 결과라는 냉소
          + 수많은 개발자가 의존하는 인기 프로젝트에서 이런 실험을 하는 건 문제, AI가 만든 형편없는 코드로 인해 품질 하락 우려, 쓸모 없는 코드가 쌓이거나 팀원들의 생산성만 갉아먹을 위험
     * 뭔가에 대한 수동적 복종이라면 요청을 그냥 검토 없이 전부 승인하고, Microsoft의 기술스택이 세계적으로 망하면 그때 퇴사해서 컨설턴트로 3배 연봉을 받으면 된다는 반어적 제안
          + 이런 비꼬는 태도로 일하고 싶진 않음, 회사 경영진과 적대적 ""우리 대 그들"" 프레임이나 일부러 망치자는 접근 자체가 이해 안 됨, 불완전함에 불평한다고 해서 조직 전체를 방해하거나 공격하는 일은 자기 양심에 맞지 않음
          + 이미 어차피 Microsoft의 기술스택이 망가져(?)있는 상태라는 냉소적 반응
          + 실제로는 CoPilot로 생성한 PR 자체를 운영진이 직접 제출한 상황임을 지적
          + 언젠가 CoPilot이 전체 코드베이스를 덮어쓰고, 코드가 없으면 더 이상 테스팅 실패도 없겠다는 농담
          + 언제든 레이오프 대상이 될 수 있으니(타입스크립트 컴파일러를 Go로 만든 이처럼) 이런 조직에서 굳이 충성할 필요가 없을 거란 의견
     * PR을 여는 건 적어도 안전한 실험 방식, 쓸모 없으면 바로 폐기 가능, 새로운 시도에는 언제나 시행착오와 실패가 따르지만 그 경험 자체가 중요, 실제 코드와 실제 문제에서 혹독히 훈련하면 도구가 빠르게 발전할 수 있다고 기대, 예를 들어 향후엔 테스트 돌 때까지 반복학습 기능이나 테스트 삭제 방지 체크 등 개선이 이뤄질 것 같음, 결국 개발 과정의 반복적이고 단순한 일들은 AI가 맡고 본질적인 창의적 작업에 개발자가 집중하게 될 미래를 예상
          + 다만 이런 실험은 퍼블릭 레포지터리 대신 비공개 포크에서 진행하는 게 안전, 세일즈 측면에서 이런 사례가 공개된 게 맞는 판단인지 의문, 사내 의사결정자가 잡지에서 CoPilot을 보고 똑같은 시도를 하자고 할 때 이런 현실 사례가 참고될 수 있음, 대부분 기업은 애플리케이션 결함 사례를 최대한 숨기고, 완성도 높은 모습만 공개하는 경향이 일반적
          + 표면상으로는 괜찮아 보이는 PR에도 눈에 띄지 않는 문제들이 숨어 있어 오히려 더 위험함
          + AI 코드 리뷰가 오히려 단순반복 작업보다 더 짜증난다는 경험, 특히 버그가 숨어있을 때 개발자가 더 고생
          + PR을 여는 것 자체가 프로젝트 관리에 부하와 복잡성을 추가, 별도 포크에서 실험하는 게 커뮤니티의 좋은 사례로 남을 것, 수많은 오픈소스 프로젝트가 누적된 PR 관리에 지쳐 유지보수가 아예 중단되거나 누군가가 fork하여 살아남은 PR만 따가는 사례가 잦음, 이런 식으로 버려진 프로젝트와 포크가 늘어날 것이 걱정
          + 만약 LLM이 정말 버그 있는 상태로 제대로 코딩을 학습 가능하다 믿는다면 이후엔 버그가 거의 없는 데이터 셋 구축이 필요, 실제로는 그렇게 안 되고 그냥 아무 데이터나 긁어모은 현실
     * GitHub가 세계에서 가장 성숙한 저장소 중 하나에서 공백 관련 린트에조차 자주 실패하는 AI를 수십억 달러 들여 만들었음, 취미용 실험이면 몰라도 실질적인 가격 붙여 ""혁신적 제품""으로 파는 건 논란 대상
          + 연구자 입장에서야 당연한 실험이지만, 문제는 이런 미완성 상태를 당장 파는 회사 태도
          + 예전 CEO Nat Friedman이 ""돌아가셨겠네... 아니 아직 살아계시지""라는 농담
     * 진짜 문제는 소프트웨어 개발자 성과 측정의 객관적 지표 부재, 연말 평과처럼 주관적 평가밖에 없는 상황, AI 활용의 효율/비효율이 실제로 어느 쪽에 있는지 파악도 어려움, junior보다 저렴해 보이지만 senior 시간 낭비와 지시 불이행, CEO 숭배와 결합되어 조직 내부 의견 불일치 심화, 개발자의 반발은 ""대체 불가 두려움""으로 무시되고, CEO 측은 추진 성과 극대화, 어느 쪽도 모두 동의하는 산업 표준이 없으니 진실이 파악 불가, 극단적 추론으론 조직에서 AI PR 증가를 위해 리뷰 기준 자체를 낮추라고 요구할 수도 있음
          + ""junior보다 더 저렴하다""는 주장에 퀘스천마크, LLM 개발/훈련 자체 비용을 감안하면 수년치 junior 연봉에 해당하여 단기적 ROI가 전혀 보장되지 않음
"
"https://news.hada.io/topic?id=21021","게임 엔진 없이 비디오 게임 개발하기 (2025)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      게임 엔진 없이 비디오 게임 개발하기 (2025)

     * 상용 게임 엔진 없이도 충분히 쉽고 재미있게 게임 개발을 할 수 있음
     * 대형 엔진의 대부분 기능이 필요 없고, 직접 도구를 작성하면 유연성과 개발 효율성이 커짐
     * 현대 C# 및 오픈소스 라이브러리를 활용하면 팀 규모와 관계없이 충분한 개발 환경 제공 가능함
     * FMOD, SDL3, Dear ImGui 같은 라이브러리와 자체 툴 결합으로 필요에 맞는 파이프라인을 구성할 수 있음
     * 크로스플랫폼 개발, 유지보수, 이식성 측면에서 직접 구축한 경량화 프레임워크가 매우 유리함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서문: 20년차 게임 개발자의 소회

     * 2025년에도 상용 게임 엔진 없이 비디오 게임을 개발 중임
     * 주변에서 상용 엔진을 쓰지 않는 점에 놀라는 경우가 많음
     * Unity나 Unreal 같은 대형 엔진의 기본 기능들은 직접 구현하는 것에 비해 만족스럽지 않고, 결국 많은 부분을 다시 만들게 됨
     * 상용 엔진을 활용할 때 잦은 비즈니스 정책 변경이나 업데이트로 인한 문제에 노출됨
     * 개별 프로젝트에 딱 맞는 작은 도구를 직접 만드는 것이 더 효율적이고, 장기적으로도 유지보수가 쉬움

게임 개발에 필요한 프로그래밍 언어 선택

     * 오랜 기간 C# 을 주력 언어로 사용함
     * 현대의 C#은 과거에 비해 성능과 문법, 개발 경험이 많이 개선됨
     * dotnet watch 같은 핫리로드 기능이 있어, 실시간 코드 수정 및 테스트에 매우 편리함
     * 비개발자도 쉽게 접근해 팀원 간 역할 분담과 협업 효율성이 높음
     * 내장된 reflection 기능으로 에디터나 툴 제작에 강점이 있음

크로스플랫폼 라이브러리와 렌더링·입력 구현

     * SDL3를 사용하여 윈도우, 렌더링, 컨트롤러, 크로스플랫폼 지원 등 모든 기본 기능을 구현함
     * SDL3의 GPU abstraction으로 DirectX, Vulkan, Metal 등 다양한 환경 지원이 쉬워짐
     * SDL 상단에 직접 작성한 C# 레이어(예: Foster)를 공용 유틸리티로 활용함
     * FMOD를 마지막 남은 상용 오디오 툴로 사용, 이외에는 오픈소스 기반 파이프라인 구축
     * 기존에는 OpenGL/DirectX 직접 구현도 경험, SDL의 안정성이 상당한 이점임

에셋(Assets) 처리 방법

     * 직접 엔진을 사용할 경우, 게임에서 필요한 파일만 간단히 로딩
     * 픽셀 아트 게임처럼 소규모 에셋의 경우 모든 파일을 초기화 때 한 번에 적재함
     * 대형 프로젝트에서는 필요한 에셋만 요청 시 로딩하고, 장면 전환시 메모리 해제 방식 적용
     * 컨버팅이 필요한 경우 컴파일 시 자동 처리 스크립트 작성만으로 충분함

레벨 에디터 및 UI 툴

     * LDtk, Tiled, Trenchbroom 등 오픈소스 레벨 에디터와 데이터 연동이 쉬움
     * 대체로 프로젝트마다 맞춤형 에디터를 직접 구현함
     * UI 본체는 직접 작성하지 않고, Dear ImGui의 즉시모드 GUI를 적극 활용함
     * C#의 reflection과 ImGui 조합으로 게임 오브젝트 상태를 실시간 시각화 가능
     * 필요시 목적에 적합한 외부 툴과 자체 툴을 혼용함

크로스플랫폼 및 콘솔 이식성

     * 과거엔 콘솔 이식 문제로 C++ 선택 필요성이 컸으나, C# Native-AOT 등장으로 해결됨
     * FNA 등 오픈소스 프레임워크에서 적극적으로 콘솔 지원 확장 중임
     * SDL3가 지원하는 범용 추상화 레이어를 활용하면 대부분 시스템에서 동일한 코드 베이스 운용 가능함

개발 환경: Linux 중심의 오픈 생태계

     * 주력 개발 플랫폼을 Linux로 전환, 오픈소스 툴체인과의 궁합이 매우 좋음
     * 특정 상용 소프트웨어와의 연결이 남아 있지만(예: vscode, github), 오픈소스 생태계가 넓어질수록 지원도 확대됨
     * 개인적으로 Steam Deck 등 다양한 Linux 기반 플랫폼에서 동일한 작업 환경 구성 중임

추가 Q&A 및 사례

     * Godot 사용 문의: 오픈소스 엔진 중심 개발 니즈가 있다면 Godot 추천, 그 외에는 커스텀 툴 선호함
     * 3D 작업: 대형 엔진 장점이 있으나, 소규모 특화된 프로젝트엔 맞춤 프레임워크로 충분함
     * 고성능 기술 요구: 요구 수준에 따라 Unreal 등 대형 엔진 사용도 무방함
     * 팀 단위 엔진 변경: 소규모/단독 개발에 적합, 중대형 스튜디오도 장기 리스크 회피 이유로 커스텀 엔진 전환 사례 증가 중임
     * 에셋 워크플로우: Aseprite 파일을 빌트인 태그와 타이밍을 활용해 자동으로 애니메이션 변환 가능함

결론

     * 상용 게임 엔진 없이 게임 제작을 하는 것은 취향과 작업 방식에 달린 선택임
     * 필요와 재미를 최우선으로 자신에게 적합한 방법을 선택하면 됨

   좋은 활동입니다.
   저는 1세대 게임 개발자였습니다.
   언리얼 나오기 이전에는 당연히 개발사들은
   엔진 개발이 당연했었고, 그것이 곧 경쟁력이었죠.
   엔진개발은 코어, 커널, 렌더링, 기타 입출력을
   기반으로 툴 개발이 대부분을 차지했었죠.
   저는 당시 파티클, 사운드, 레이어, 오브젝트 툴을
   담당 했었는데, 팀원이 7명에서 대략 모아 놓으면 20개정도는 가뿐히 넘겼던거 같습니다.

   어느날부터 언리얼이 나오면서 앙산형 게임들이
   쏟아지니 더이상 엔진개발에 투자는 안하더군요.
   그때 많이들 독립해서 회사들을 차렸던거 같습니다.
   벌써 27년전 이야기네.

   저도 어느덧 나이먹고 게임이 아닌 다른 업무를 하고 있는데요.

   그래픽 카드에 따라 directx, opengl 모드 별로
   코어 작업 하던 아련한 시절이 떠오릅니다.

   감바레요...

   최근에 전략 게임을 만드려고 엔진을 오랫동안 찾다 보니 이럴거면 하나 직접 만드는게 낫다 싶었는데, 실천해서 성공한 사례를 보니 용기가 샘 솟네요.

        Hacker News 의견

     * 5년 동안 혼자 2D 게임 엔진을 개발하고 관련 유료 업무를 하면서 사람들이 잘 모르는 중요한 점을 설명하고 싶음
       엔진은 쉬운 부분임
       진짜 중요한 부분은 엔진 주변의 툴링, 콘텐츠, 애셋 파이프라인임
       다양한 데이터 소스와 포맷에서 텍스처, 오디오, 모델 파일(gltf, fbx, 애니메이션 등) 임포트
       에디터 앱에서 자르기, 복사, 붙여넣기, 실행취소, 다시실행, 저장, 삭제 같은 기본 편집 기능
       개발자가 에디터로 실제 데이터를 만들고 조작할 수 있게 하는 시각화와 기능(엔티티, 애니메이션, 씬, 오디오 그래프, 스크립팅 지원 등)
       정적 지오메트리 베이킹, 셰이더 컴파일, 텍스처·오디오 리샘플링 및 패킹, 게임 콘텐츠 애셋 팩 만들기 같은 데이터 패키징 및 전처리
       이 모든 게 끝나면 실제로 런타임 부분(즉 게임 메인루프와 하위 시스템)은 전체 시스템 중 작은 부분임
       그래서 게임 스튜디오에서는 엔진 담당자는 소수고, 툴을 만드는 프로그래머가 대다수인 이유임
       detonator 엔진: https://github.com/ensisoft/detonator
          + 범용성보다는 내 게임에 맞는 엔진을 만드는 것에 집중하는 게 중요함
            UI, 압축 등은 라이브러리와 프레임워크로 해결 가능
            OP가 imGUI처럼 소형 라이브러리를 이용하는 것도 좋은 예시
            모든 게임을 위한 엔진을 만드는 게 아닌 만큼, 사실상 안 해도 되는 일이 많아짐
          + 엔진은 애셋 파이프라인에 달린 작은 런타임 덧붙이기 같은 존재임
            요즘 셰이더 컴파일러도 3D API보다 점점 더 중요해지는 상황
            흥미로운 부분은 셰이더 컴파일러에 집중되고, 3D API는 셰이더 실행과 데이터 전달 역할에 그침
          + 사람들이 엔진이라 하면 자주 애셋 파이프라인과 에디터까지 포함해서 말하는 경향
            오늘날의 엔진은 메인 루프 + 3D API 함수만 있는 게 아님
            Unity 같은 엔진을 쓴다고 해서 렌더링 코드만 쓰는 개발자는 매우 소수임
            물론 Unity/Unreal 쓴다고 직접 애셋 툴을 쓸 일이 완전히 없는 건 아님
          + 최근 후속작용 엔진을 갈아엎으면서 이 점에 매우 동의
            포스트모템에 적었듯이 대부분 엔진이라 하면 게임 실행파일에 포함된 코드를 떠올리지만, 실질적으론 레벨 에디터, 콘텐츠 파이프라인, 디버깅/프로파일링, 개발 워크플로우처럼 게임과 함께 배포되지 않는 코드의 비중이 더 크다고 생각
            툴 개발은 엔진 개발보다 지루하고 지치기 쉬운 작업
            이런 이유로 많은 커스텀 엔진 게임 개발이 중간에 멈추는 현상 발생
            포스트모템: https://ruoyusun.com/2025/04/18/game-sequel-lessons.html
          + 에디터를 처음부터 만드는 일은 상당한 작업
            가능하다면 이미 있는 에디터를 활용함
            예를 들면 TrenchBroom(Quake 에디터) + func_godot 조합, 2D에선 Tiled
            게임 데이터 관리는 CastleDB도 있었지만, 이제는 Hide(본격 3D 에디터)랑 통합된 상태
            툴 만들고 나선 게임 설계하고 콘텐츠 만드는 일이 또 주요 단계
     * 몇 년 전 SDL2와 약간의 C++로 간단한 ""sprite"" 클래스를 만들고 콜리전 등 간단 기능 추가
       내가 만든 걸 굳이 엔진이라 부르자면 일종의 전기자전거 보조 정도였음
       ""엔진""이 프로젝트/게임 전체를 리드하는 상황이 자주 발생한다는 점
       게임을 엔진에 맞추게 되는 경우가 많아 Unity 같은 대형 엔진 사용을 항상 피함
       이런 엔진들은 결국 같은 게임 구조만 만들게 되고, 자산만 다를 뿐임
       개인적으로는 엔진을 배우느라 시간 낭비하는 것보다 SDL로 짧은 러닝커브로 시작할 수 있고, SDL은 게임 외 다른 크로스플랫폼 프로젝트에도 활용 가능
       내 게임: https://store.steampowered.com/app/2318420/Glypha_Vintage/
     * 엔진을 직접 만드는 게 오래 걸린다지만, Unreal이나 Unity를 제대로 익혀 아이디어를 바로 게임으로 구현할 수 있을 정도가 되려면 어느 정도 시간이 필요한지 질문
       결국 내 엔진 제작이 끝나면 바로 그 레벨의 전문성을 확보하므로 장기적으로 시간 절약
       엔지니어 경력이 쌓일수록 직접 만드는 쪽이 시간 관점에서 유리해짐
       게임이 독특하거나 틈새시장일수록 직접 만드는 쪽이 더 타당
       Unreal의 복잡한 UI에서 몇 달 헤매고 애초에 원하는 게 범용 엔진에선 거의 불가능한 걸 알게 되는 경험은 비효율
       반면 초현실적 오픈월드 RPG라면 직접 제작은 좋은 선택이 아님
       커스텀 엔진의 제약이 오히려 창의성을 불러오고, 최고 수준이 아니더라도 더 독창적인 게임이 나옴
          + 직접 엔진을 만들어 본 적 있음
            처음에는 수많은 시행착오와 막다른 길을 겪으며 1년 정도 걸렸음
            3D 렌더링, 적응형 UI, 스켈레탈 애니메이션, 세이브 파일, 스마트 오브젝트, 경로 탐색, 스크립팅, 오디오, 물리 등 게임에서 볼 수 있는 거의 모든 서브시스템 구현
            특히 되감기 기능(Braid의 시스템처럼)을 직접 구현
            이런 기능은 엔진의 모든 서브시스템(스크립트, 물리 등)의 지원 필요
            내가 만든 엔진의 모든 부분을 알고 있어 추가 기능 개발에 엄청난 자유로움
            하지만 1년 개발 후 점점 번아웃이 와서 의욕이 사라짐
          + Unreal은 모르지만 Unity는 직접 코딩과 비교해 10배 이상 빠르게 개발 가능
            물리 기능은 1분이면 추가 가능하지만, 직접 엔진이면 외부 라이브러리 연동만 해도 하루 이틀 소요
            Unreal 유저 Noel이 보여주는 내부 시각화 기능도 Unity엔 내장
            바운딩 박스 조작 같은 편집도 기본 탑재
            만약 엔진의 기본 동작이 부족하다면 ImGui나 Yoga 기반 CSS 엔진으로 쉽게 확장 가능
            고급 파티클 에디터, 복잡성 해소된 셰이더, 모듈형 데이터 스트리밍 등 수많은 기능 제공
            이론상 다 직접 만들고 싶지만 결국 시간 문제로 완성 우선
          + Unreal이나 Unity를 익혀 게임 아이디어를 바로 구현하는 데 걸리는 시간과, 직접 엔진 만드는 데 걸리는 시간은 다른 질문
            조금만 아이디어 주면 몇 시간 안에 게임 플레이 가능한 프로토타입 제작 가능
            Unity는 초기에 프로그래밍만 하면 되고, Unreal은 블루프린트만으로도 거의 게임 출시 직전까지 가능
            10분 만에 슈퍼 헥사곤 스타일 게임 프로토타입 완성하는 영상 참고: https://www.youtube.com/watch?app=desktop&v=p8MzsDBI5EI
            유니티의 고유 요소는 프리팹 정도고 나머지는 게임개발 일반 개념
            Unreal 개발자 입장에서라면 Unity로도 1시간 이내 비슷한 프로토타입 제작 가능
          + “엔진 완성 이후”란 가정 자체가 비현실적일 수도 있음
            GameObject.Instantiate 같은 간단한 동작도 엔진에서 직접 구현하려면 어마어마한 리소스 필요
            2D/3D 물리, 셰이더, 플랫폼 지원 등 복잡도 감안
            엔진이 목표면 엔진을 만들고, 게임이 목적이면 게임 자체를 만들라는 의견
          + 게임 엔진을 만들 만큼의 게임개발 지식이 충분하다면
            Unreal이나 Unity를 익혀 프로토타입 만드는 데 하루면 충분
            완벽한 숙련까지 오래 걸리지만, 원하는 게임 프로토타입 완성 시간은 비교 불가
     * ""모두 다 하는"" 대형 엔진 없이도 게임 만들기는 더 쉽고, 더 재밌고, 때론 더 효율적
       다만 엔진의 특정 기능(예: Unreal의 역운동학, 애니메이션 블렌딩) 파고들다 보면 ""차라리 직접 2~3주 고생 안 하길 잘했다""는 생각
       미니멀리즘이나 비대화 방지도 중요하지만, 이런 엔진들이 무거운 일을 대신 해주니 인기가 많음
          + 예전엔 나도 이런 방식이었음
            첫 3D 게임 만들 땐 입력, 오브젝트 관리, 컬링, 모델 로딩, 수학 라이브러리, 그래픽스, 노멀맵, SSAA 등 다 구현하다가 정작 게임 완성률은 0%
            그래도 취미 2D 프로젝트에선 여전히 브라우저 캔버스만으로 무의존 개발
            사실상 브라우저가 엔진 역할
          + “차라리 직접 만들지 않은 게 다행”이라는 의견에 대해
            장기적으로 인디 개발자 커리어를 생각한다면, 몇 주 걸리더라도 주제 깊이 이해+100% 소스코드 소유 및 재사용 가치가 더 큼
          + 내 졸업 논문은 NeXTSTEP/Objective-C 기반 파티클 엔진을 Windows 95/Visual C++로 이식하는 주제(OpenGL 기반, marching cubes 샘플 포함)
            이런 테마도 요즘 엔진에선 그냥 한 줄짜리 기능 중 일부
          + 엔진을 쓰면 프로젝트 진행 속도가 훨씬 빨라지고, 인프라 개발에 시간 소진하지 않아도 됨
            바퀴를 또 만드는 일은 대부분에게 큰 재미가 아님
          + 역운동학이나 애니메이션 블렌딩 같은 기능은
            만약 게임의 핵심이라면 구현할 가치가 있고, 아니면 불필요한 기술적 함정임
     * Lua & Love2D를 활용해 내 방식대로 게임을 만듦
       자체 제약 걸기라는 점 자체가 재미의 핵심
       개발이 재미없어지면 뭔가 잘못됐다는 신호이니 더 나은 방법 탐색
       내 게임 YOYOZO는 39KB로 작지만 Ars Technica 2023 최고의 게임 목록에 대작들과 함께 올랐음
       https://news.ycombinator.com/item?id=38372936
          + 그 게임 기억 남
            Playdate 구입한 지 수년 만에 SDK로 놀기 시작했는데, Lua도 처음 익히는 중
            강 타입과 언어적 안전성 바람이 있지만 상황에 맞는 충분함
            텍스트가 크랭크에 따라 가짜 3D 공간에서 도는 작은 기술 데모 만든 게 전부
            https://bsky.app/profile/haydenblai.se/post/3lpgnya4cqk2a
            이 프로젝트를 하며 오래 전 CRUD/웹앱 경력으로 삼각법 거의 다 까먹은 것 깨달음
            Playdate 개발의 최고 장점은 고정 캔버스라서 픽셀 위치만 정하면 모든 기기에서 그대로 동작한다는 해방감
            지난 개발 경력에선 반응형 UI만 만들다 이런 경험 매우 새로움
     * 게임 엔진(Godot, Unity, Unreal 등)으로 뭔가를 만들려고 하면 항상 엔진이랑 씨름하게 됨
       결국 정해진 게임 형식에 에셋을 얹는 느낌이라 내가 원하는 게임 제작이 어려워짐
       웹 개발에 비유하면, 워드프레스 같은 완제품 느낌
       기본 구성이 의도와 다를 때는 엄청난 해킹과 우회 작업이 필요함
          + 여기엔 ""게임 템플릿""들이 기름을 붓는 역할
            타이틀 화면과 모델만 교체하면 완제품 게임처럼 보이게 만들어주는 템플릿을 살 수 있음
            Steam의 신작 중 거의 절반이 Unity/Unreal 템플릿 게임에 스킨만 살짝 바꾼 수준
            각종 예시:
            https://assetstore.unity.com/packages/templates/…
            https://store.steampowered.com/app/2488370/Cash_Cleaner_Simulator/
            https://store.steampowered.com/app/2073910/A_Webbing_Journey/
            https://store.steampowered.com/app/3498270/Better_Mart/
            https://store.steampowered.com/app/2625420/Drive_Beyond_Horizons/
            https://store.steampowered.com/app/3163790/Toy_Shop_Simulator/
            https://store.steampowered.com/app/3023600/Horse_Farm_Simulator/
            https://store.steampowered.com/app/3124550/Liquor_Store_Simulator/
          + Google Play에서 모든 게임이 똑같이 보이고, 긴 로딩, 렌더링 문제, 텍스트 깨짐, 오디오 버그 등 Unity 특유의 문제점 발생
            모바일 광고용 ""idle RPG"" 게임이라면 이런 게 용인될 수 있겠지만, VR 분야에서 Unity 쓴다는 건 정말 이해하기 힘들었음
            Meta Quest Store 성능 충족하려면 Unity 엔진의 많은 부분을 뜯어고쳐야 함
            성능 맞추기 힘들고, 일처리 방식도 구식
            퀄리티 높은 소프트웨어를 만들고 싶다면, 처음부터 믿을 수 없는 엔진으론 불가능함
     * 저자(노엘)는 Celeste라는 게임을 만들었으며 3백만 장 이상 판매
       https://en.wikipedia.org/wiki/Celeste_(video_game)
     * 나도 상당 부분 동의하며, 코드 기반의 C# 게임 프레임워크를 만들고 있음(XNA/Monogame의 정신적 후계, SDL 대신 Sokol 사용)
       https://zinc.graphics/
       현대적 C#의 강점: 크로스플랫폼, NativeAOT 컴파일링, 네이티브 핫리로딩, 리플렉션 등
       개인적으로 소스 제너레이터도 추가
       과거 부정적 이미지 남아 있지만, 최근 5년간 CoreCLR과 언어 발전은 놀라운 수준
       딱 한 가지 바라는 건 Union Types인데, 현재 제안되어 내년에 추가 기대
       https://github.com/dotnet/csharplang/blob/main/proposals/TypeUnions.md
          + 혹시 이런 프로젝트에 C# 입문하는 데 참고 자료가 있는지 궁금함
            C#은 Win32나 Unity에서만 써봤고, C/C++ 쪽 저수준 엔진 지식은 있지만, 게임 콘솔 등 크로스플랫폼에 C#은 불가능하다고 생각했음
            내 생각이 틀렸다는 걸 이제 깨달음
     * 어떤 소프트웨어든 처음엔 무에서 시작하는 걸 선호
       대형 프로젝트 경험 있는 사람에겐 느리게 느껴질 수 있지만, 스타트업 단계에선 오히려 더 빠름
       최소한만 구현하고, 추상화가 들어서면 새로운 기능 추가 속도가 더 빨라짐
       기업용 대형 SW와 내가 직접 쓴 미니 엔진의 생산성은 완전히 다름
       직접 쓴 코드는 바로 커팅, 리팩터링이 쉬워 훨씬 빠름
       이런 이유로 마이크로서비스와 소규모 팀을 선호
       직접 만들 땐 시행착오와 망가지는 지뢰밭을 반드시 거쳐야 하며, 언어나 플랫폼의 진짜 특성을 익히는 데도 수 년 소요
       하지만 그 과정 자체가 결국 개발자의 내공으로 남음

   엔진 말고, MonoGame 수준의 프레임워크 쓰는 정도는 어떨 지 궁금합니다~

   ""그 과정 자체가 결국 개발자의 내공으로 남음"" 적극 동의합니다
"
"https://news.hada.io/topic?id=20935","회의실 예약을 이용한 악의적인 준수","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          회의실 예약을 이용한 악의적인 준수

     * Larry Page가 CEO가 된 후, Google은 성장통과 업무 효율 저하 문제를 경험함
     * 그는 불필요한 프로젝트 폐기와 함께, 회의 문화 개선을 위해 새로운 지침을 도입함
     * 대표적으로 회의 시간 50분 제한 등의 정책이 생겼지만 현실에서는 잘 지켜지지 않았음
     * 이에 어떤 팀이 10분 남은 짧은 시간 회의실을 공식적으로 예약해서 실제 활용함
     * 규정대로 행동한 이 팀의 행동이 조직 문화 및 정책의 아이러니를 드러내는 일화임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Google의 성장과 변화

     * 2011년, Larry Page가 Eric Schmidt를 대신하여 Google CEO로 취임함
     * 이 시기 Google은 직원 3만 명 규모로 급성장하면서 프로젝트가 커지고 실패 확률도 높아짐
     * 이전에 Yahoo!가 느린 결정 구조로 Google에서 조롱의 대상이었으나, Google 역시 의사결정이 점점 느려지는 문제를 겪음

Larry Page의 개혁 조치

     * Larry Page는 효율 개선을 위해 비전략적∙비전술적 프로젝트를 대거 폐기함
          + ""더 적은 화살에 더 많은 나무""—역량 집중 원칙을 강조함
          + 예시: Google Buzz 정리 후 Google+에 집중
     * 회의 문화 혁신안 발표
          + 모든 회의는 ""결정권자"" 필요
          + 참석자 10명 이내 제한
          + 모두가 참여하지 않으면 회의 참석 불필요
          + 1시간 회의는 50분으로 단축하여 휴식시간 확보
          + 이 정책은 점점 ""결정 중심 회의""에만 적용하는 쪽으로 완화됨

회의 시간 정책의 현실

     * Google Calendar의 기본 회의 시간도 25/50분으로 조정됨
     * 하지만 실제 현장에서는 회의가 제시간에 끝나지 않아, 다음 회의 참석자들이 문을 두드릴 정도로 회의가 계속됨
     * 2:50이 되어도 방을 비우지 않고, 다음 예약팀과 마찰 생김

10분짜리 회의실 예약 사건

     * NYC 사무실의 한 팀이 회의실의 마지막 10분 남은 시간에 자신들의 스탠드업 미팅을 예약함
          + 50분 회의 정책 덕분에 10분 남은 시간들이 빈번하게 발생
          + 이 짧은 틈을 이용하여 효율적으로 회의를 운영함
     * 실제 사례
          + 2:50에 누군가 문을 두드리며 ""우리 예약 시간""이라 알림
          + 기존 사용자 ""아직 2:50이라 내 시간""이라고 주장
          + ""10분이 우리 예약""이라고 증명하며 방을 차지
          + 불만스러운 퇴실 풍경이 연출됨
          + ""정책이 실제로 강행되면 이런 곤란한 상황이 생김""을 목격함

결론 및 느낀 점

     * 정책적 취지는 좋았지만 실제 업무 환경에서는 비현실적인 결과 초래
     * 짧은 시간 예약을 실제로 행사한 팀의 정체나 동기는 미스터리로 남음
     * 이런 일화가 대기업의 실행력, 정책, 문화의 간극을 보여주는 예시임

   악의적인 준수라니? 너무 멋진 전략입니다

        Hacker News 의견

     * 2:50에 회의가 끝나야 했지만 실제로는 그렇지 않았음. 미시건 대학에서는 이 문제를 수업 시간을 공식적으로 광고한 시간보다 10분 늦게 시작함으로써 해결했음. 즉, 10-11시로 표시된 수업은 실제로는 10:10-11시에 진행됨. 사람들은 10:10에 도착하게 되고, 이는 꽤 큰 기준점 효과를 줌. 정시에 끝나지 않을 때 느끼는 체감의 차이는 큼
          + 핀란드 대학 등 유럽 여러 대학에서도 “academic quarter”(학문적 15분) 전통이 있었음. 10시에 시작이라고 하면 사실상 10:15에 시작함. 정확한 10:00으로 표기했으면 그 시간에 바로 시작함. 이는 예전 시계가 없던 시절 종소리를 듣고 학생들이 이동할 수 있도록 한 관습에서 유래함
          + 미시건 주립대에서 수학 교수 Wade Ramey는 수업이 시작하면 교실 문을 잠궈서 지각하면 입장하지 못하게 했음. 숙제를 제출할 때 스테이플러로 꼭 고정해야 했고, 틀린 부분은 점수를 깎는 것이 아니라 음수 점수를 주기도 했음. 이런 엄격함에도 그 교수의 수업을 즐겁게 들었음
          + 위에서 말한 관습이 'academic quarter'임. 보통 15분임
          + 독일 등지에서도 “c.t.”(cum tempore, 시간 포함)라고 15분 늦게 시작하는 대학 전통이 있었음. 내게는 이미 사라진 관습이었지만, “s.t.”(sine tempore, 제시간)로 명확히 표시된 시간에는 정확히 시작함
          + 요즘 빅테크 기업들에서도 회의를 항상 정시/반시 기준 5분 뒤에 시작해서, 꼭 정각이나 반시에는 끝내는 버퍼 타이밍이 기본 세팅임. 이렇게 하면 회의장 이동 시간과 생리적 필요를 충족시킬 수 있음. 만약 회사에서 아직 이게 구현되지 않았다면, 일정 관리 툴에 기본 탑재된 버퍼 옵션을 켜기만 하면 됨
          + St Andrews University에는 수업이 항상 5분 늦게 시작해서 5분 일찍 끝나는 “Academic hour” 개념이 있음. 예를 들어 10시~11시 수업이면 실제로는 10:05~10:55임. 이는 강의 간 이동 시간을 주고, 세팅 준비 시간을 표준화하려는 목적임
          + UC Berkeley도 이런 관습이 있었음. 신입생일 때 아무도 이런 사실을 알려주지 않아서 수업 첫날에 모두가 일찍 와서, 교수는 왜 다들 여기 있냐며 10분 뒤에 시작되는 걸 모르는 거냐고 말해서 놀랐음
          + Larry Page도 이 방식에서 아이디어를 얻었을 거라 생각함. UMich에서는 2018년에 이른바 ‘Michigan time’을 공식적으로 종료했는데, 이런 시스템을 좋아했음
          + 우리 팀도 팬데믹 때 모든 회의 시작 첫 5분을 생리적 휴식으로 정함. 실질적으로 회의가 :05 또는 :07에 시작되고, 이전 회의가 그때까지 끌어도 누가 빨리 나가라고 해도 어색하지 않음
          + 그 제한적 시작시간 관습은 2017년쯤부터 사라짐
          + 나는 수년간 내 회의 일정을 5분 뒤에 시작하도록 조정해왔으나 Google Calendar에 이게 내장되어 있지 않아서 매번 수동 조정하는 게 불편함
          + 이게 너무 뻔한 해결책임
          + 내가 다닌 학교·회사에서는 회의나 수업이 대충 시작되면, 주요 인원이 늦게 오고, 결국 처음 5-10분은 낭비가 됨. 리더들이 “방금 들어온 사람들 위해” 처음 내용을 반복하거나 요약하기에 시간 낭비임. 나는 항상 정시에 도착해 노력했는데, 늦는 사람들을 위해 시간을 반복할 때 매우 불쾌했음. 25년 전, 나는 늦어서 중요한 자원봉사 자리에서 한 번 잘린 적 있음. 이후로 절대 늦지 않는 습관을 가지게 됨. 심지어 대중교통 이용 시 30분 이상 여유를 두고 항상 매우 일찍 도착함
     * “Malicious compliance”가 아니라 “pedantic enforcement”(꼬장 반영)이라는 생각임. 진짜 ‘malicious compliance’였으면 50분짜리 회의 다음에 10분짜리 회의를 같은 방에 바로 잡았을 것임
          + 이건 클릭베이트 단어임. 오히려 회사는 회의실 최적화 덕분에 돈을 절약했고, CEO의 휴식 시간 방침이 실행된 셈임. 진정한 악의적 행위자는 마지막까지 50분 룰을 안 지키고 시간 넘기려 든 팀임
          + 이건 꼬장도 아닌 것임. 저 그룹이 회사에서 유일하게 상식적으로 보임. 진짜 문제는 Page였음. 위에서 내려온 이상한 방침에 하급자는 대놓고 반대하지 못함. 두 번째로 잘못한 쪽은 회의실을 전용해 50분 넘게 쓰는 사람들이었음. 이건 자연스러운 현상이니까 이해는 감. 어쨌든 다른 그룹 예약이 된 방을 마치 자신의 것처럼 쓰는 게 문제임
          + 진짜 결론이 50분 짜리 회의 따로, 10분 짜리 회의 따로 잡는 거라 생각함. 예약 안된 방을 그냥 쓰는 게 'malicious compliance' 아님
     * 한 스카우트 지도자 이론에 따르면, 각자 “9:30”이 언제인지 다르게 받아들임. 어떤 사람에게는 9:25, 또 어떤 이에게는 9:45를 의미함. 하지만 “9:32”는 모두가 명확하게 한 시각을 가르킴. 그래서 지도자는 종종 “오늘은 6:07에 모임”처럼 이상한 시각을 정했음
          + Saratoga, CA에서도 비슷한 방식의 현상이 있음. Quito Road의 안내 속도 표지판이 17, 19, 21, 22 mph처럼 이상한 숫자임. 이는 운전자들의 주의를 끌어, 날카로운 커브에서 속도를 줄이게 하는 목적임
     * 내가 회의실에 있었다면 오히려 안심했을 것임. 대기업 회의는 강제적 계기가 없으면 끝나지 않기 때문에 누군가 노크해주면 도움이 됨
          + 스타트업에서 마케팅·영업 회의가 너무 길어, 내가 직접 시계 가게에서 뻐꾸기 시계를 사서 회의실에 설치했음. 15분마다 소리를 내서 회의가 짧아지고, 회의 수도 줄었음. CEO와 사무 행정 담당자도 이 시계를 참 좋아했음
          + 오래전부터 45분이 넘는 회의에서는 집중력이 떨어짐을 느꼈음. 그래서 내가 주최하는 회의는 항상 45분으로 시간을 제한함. 요즘은 100% 원격이라 적용하는데, 나에게 필요 없는 부분은 무시하고 HN 댓글 다는 시간으로 씀
          + 기사에서는 회의실 침입자들이 악당처럼 보이지만, 나는 그 편임. 긴 회의가 꼭 필요했던 경우보다 쓸데없이 길었던 경우가 훨씬 많았음. 회의가 많은 조직에서는 시간 끝까지 끌거나 넘기는 일이 반복되면, 문제임. 다음 회의에 늦거나, 생리적 휴식을 포기해야 함
          + 회의 안건을 명확히 정하고, 규칙을 엄격히 지키면 됨. 다 끝나면 바로 회의 종료임
          + 90분짜리 스탠드업 회의도 겪어봄. 그럴 때 10분 만에 끝내자는 사람들은 정말 영웅임
          + 어느 회사에서는 70~80분 지나면 일부 직원이 과감히 회의실을 나가 휴식 시간을 만들기도 했음. 보통 그 정도 되면 사람들끼리 소그룹 대화가 더 생산적으로 이루어지고, 그제야 결국 미팅이 빨리 끝남
          + 사람들은 진짜 일하기보다는 회의에서 빈둥거리고 싶어하는 것임. 회의가 길수록 아무것도 하지 않아도 월급은 나오기 때문임. 회의는 꼭 필요하지도 않고, 정보를 전달하는 최악의 방식임
          + 가끔 화상회의에서 누가 노크하는 척 연기하며 강제로 끝내버린 적 있음
          + 나는 pomodoro 스타일 회의를 좋아했음. 의지와 인내심의 시험이 됐음
     * “50분 회의는 항상 1시간이 된다는 문제”는 9:10AM 같이 애매한 시각에 시작하면 해결할 수 있음. 10시라는 확실한 종료시점이 생기기 때문에, 9시에 시작하면 50분 뒤에 안 끝내고 10시까지 끄는 경우가 많음
          + 어떤 사람은 항상 정시에 맞춰 행동하고, 어떤 이는 늦는 문화 차이임. 나는 9:50에 딱 끝났을 것임
          + 미시건 대학에서도 이 관행이 자연스럽게 정착됐었고, 이런 시스템을 좋아했음. 2018년에 공식적으로 폐지되었다고 들음
          + 우리 팀은 회의 모두를 5분 늦게 시작하고, 매 시 또는 매 반시에 꼭 끝내는 문화(55분 회의)를 갖게 됨. 옆 팀이 그 시간에 딱 노크하며 들어오기에 이에 누구도 불평하지 않음
          + 실제로 해결책은 아님. 다음 그룹이 들어와서 쫓겨나지 않는 한, 회의는 계속되는 경우가 보편적임
          + 9:10AM에 시작하면 된다는 조언을 들어도, 오히려 실제로는 몇몇 회의가 9시에 시작한다고 하면서 9:10이 되어야 겨우 시작됨. 9:10으로 잡으면 그제서야 9:20에 시작함
          + 실제로 그 얘기처럼, 그냥 끝나는 시각 기준으로 회의가 연장될 것임
     * Larry Page 자체를 미팅룸에서 쫓아낸 적 있음. 그는 특권의식 없이 잘 방을 비워줘서 존경하게 됨
          + 이런 문화는 중요하다고 생각함. 시니어 리더조차도 룰에 따라 방을 비워야 한다는 신호임. 모두의 시간을 존중하는 회사 문화의 척도임
     * Larry Page 입장이 더 공감됨. 누군가는 회의를 촉진하고, 정해진 시간 내에 기대 결과(의사결정 등)가 있어야 한다고 생각함. 45분 지나도 답이 안 나오면 숙제 주고 다시 모이는 게 옳음. 이런 'malicious compliance'야말로 회의가 제때 끝나고 모두가 화장실 갈 수 있는 문화임
          + 모든 모임이 반드시 의사결정만을 목표로 두는 것은 아님. 그냥 브리핑, 올핸즈, 스탠드업, 토론, 점심 시간 학습 등 “목적”이 다른 모임도 있으니까 이에 맞게 시간 배분·공간 할당도 필요함
          + 내 생각에 결과 없는 회의는 의미가 적음. 특정 의사결정이 아니어도 “눈에 보이는 결과”가 필요함. 두 명이 30분 말만 하고 그게 어디에도 기록되지 않으면 의미 없음. 요즘은 정보를 남기는 것에서 사람들이 더 머뭇대는 분위기라 문제임
          + 꼭 결정을 내릴 필요 없는 논의성 회의도 있음. 조직 전체가 방향을 맞추기 위한 자리도 필요함
     * 명문화된 정책이 항상 무시되는 사회적 표준이 된 게 싫었음. 규칙이 오래 유지된다면 그걸 바꾸거나, 아니면 철저히 지켜야 한다고 생각함
          + “나쁜 법은 모든 법에 대한 경멸을 낳는다”는 말이 있음. 사람들의 감정을 걱정해 규칙을 안 바꾸면 결국 모든 게 엉망이 됨
     * 90년대 후반 한 여성 매니저는 항상 50분짜리 회의를 예약했고, 정확히 50분에 끝나면 일어나 나갔음. 심지어 상위직이 말하는 중에도 예의 바르게, 하지만 단호히 folio를 닫고 회의실을 떠났음
          + 요즘은 저렇게 중심 잡힌 관리자를 잘 못 봄
     * “모두가 회의에서 input을 내야 한다”는 방침이 있었는데, 이럴 때는 오히려 중간중간 코딩이나 리팩토링 할 시간도 없어졌음. 오히려 평소엔 필요 없지만, 2시간짜리 대형 회의에 끼어 코딩할 시간을 확보하려고 일부러 참여했던 적도 있음. 이것도 일종의 malicious compliance임
          + 더 좋은 방법은 불필요하면 회의 초대를 거부할 권한을 주는 것임. “혹시 필요할지도”란 이유로 무작정 초대 인원을 늘리는 것이 대기업들의 만성적인 문제임
          + 다른 개발자들과 짜고 2시간 짜리 “dev sync”란 회의만 만들어놓고 실제로는 아무 미팅도 하지 않는 방법을 쓰기도 함
          + 왜 차라리 회의를 건너뛰고 리팩토링을 안 하냐는 의문을 가짐. 회의를 핑계로 남이 내 시간을 뺏지 못하게 하려면 이미 회사 문화가 너무 비효율적이란 뜻임. 이런 환경이면 이직을 심각히 고려해야 할 상황임
"
"https://news.hada.io/topic?id=21013","구글 Jules - 비동기 코딩 에이전트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         구글 Jules - 비동기 코딩 에이전트

     * Jules는 GitHub와 연동되어 버그 수정, 문서 추가, 기능 개발을 자동으로 수행하는 비동기형 코딩 에이전트임
     * 사용자는 리포지토리와 브랜치를 선택하고 자세한 프롬프트만 작성하면, 나머지는 Jules가 클라우드 VM에서 처리함
     * 테스트 실행 및 생성, PR 생성, 변경사항 diff 제공, 오디오 요약 기능도 제공됨
     * 내부적으로 Gemini 2.5 Pro 모델을 사용해 코드 이해 및 수정을 수행함
     * GitHub Copilot 코딩 에이전트와 유사하지만, Google 계정 기반으로 제공되고 UI 중심의 워크플로우임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Jules 개요

  주요 기능

     * GitHub 연동: 리포지토리를 가져와 브랜치를 생성하고 Pull Request(PR)를 자동으로 작성함
     * 클라우드 가상머신: Jules는 코드를 클라우드 VM에 클론하여 작업을 수행하고, 변경사항이 작동하는지 검증함
     * 테스트: 기존 테스트를 실행하거나, 테스트가 없을 경우 새로 생성함
     * 작업 플랜 제공: 변경 전 작업 계획과 이유, 변경 diff를 사용자에게 먼저 보여줌
     * 오디오 요약: PR 변경사항을 음성 요약으로 제공하여 빠르게 이해 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사용 방법

  시작 전 설정

    1. https://jules.google.com 접속
    2. Google 계정으로 로그인
    3. 개인정보 보호 공지 동의 (1회)
    4. Sync GitHub account 클릭
    5. GitHub OAuth 인증 절차 완료
    6. Jules에 연동할 리포지토리 선택 (전체 또는 일부)
    7. 설정 완료 후 리포 선택기가 나타나며 프롬프트 입력 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  작업 실행 절차

    1. 리포지토리와 브랜치를 선택
          + 기본 브랜치가 자동 선택되며, 필요 시 변경 가능
    2. 프롬프트 작성
          + 예시: utils.js 파일 내 parseQueryString 함수에 대한 테스트 추가
    3. (선택사항) 환경 설정 스크립트 추가
    4. Give me a plan 클릭
    5. Jules가 계획을 생성 → 사용자가 확인 및 승인
    6. 승인 후 변경된 코드 diff 제공
    7. 최종 PR 생성 및 GitHub로 반영 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  향후 기능

     * GitHub 이슈에 assign-to-jules 라벨을 붙이면, Jules가 자동 할당되어 작업 수행 (예정 기능)

   이게 나올거라서 OpenAI 가 Codex 로 김빼기 시전한 걸까요? ^^;

   Waitlist에 들어가고 바로 사용은 안되나보네요.

   비동기로 저장소 관리도 해준다는거 같은데, 아직 바이브 코딩 할때 느낌으로는 인간이 검수 안하면 복잡한 프젝은 ai가 잘못 코딩하는 경우도 종종 나긴 해서, 비동기 코드 부분은 별 메리트가 없을지도...? 결국 중간에 휴먼체크 하는데서 시간 잡아먹는게 커서

        Hacker News 의견

     * GitHub 이슈를 이 AI에게 할당하고 나면 버그 수정, 결과 병합, 완료 처리까지 자동화하는 경험 상상 중인 상황, 여기에 버그를 작성·배정·리뷰까지 맡는 “lead dev” AI, 피처를 계속 요구하는 “boss” AI를 두면 흥미로운 스타트업 시뮬레이터 실현 가능성 제안, 작은 개미 농장처럼 이들이 앱을 어떻게 만들어가는지 관찰하는 재미 언급
          + 에이전트 설계의 패턴 실제 적용 경험 공유, 분석·의사결정·리뷰 역할별 AI 에이전트를 둔 가격 시스템 구현 경험, 각 AI가 역할에 충실히 소통하는 모습 관찰, 역할 분담 덕에 실수도 잘 잡아내고 좋은 결과 도출 효과 확인
          + “VC” AI가 유니콘 기업 만들려고 하는 시나리오 상상, 유머러스한 분위기
          + 조만간 1인 유니콘 창업 실현 전망
     * 관심 있어서 Try 버튼 눌렀는데 또 웨이팅 리스트 경험, 지메일 때만큼 웨이팅 제도가 지금은 안 통한다는 아쉬움, 이제는 다양한 서비스들이 넘쳐나서 내일이면 이걸 잊게 될 거라는 솔직함
          + 이 방식이 효과 있으려면 충성도 높은 옹호자, 입소문 유저 확보 필요성 강조, 이미 사용해보고 싶어 적극적으로 문의하는 유저가 있어야 성공 가능성 제시
          + OpenAI 속도 때문에 무언가 내놓아야 했던 상황 추측
     * Google이 추론(모델 실행)을 무료로 제공한다는 점이 엄청난 경쟁력이라고 판단, Jules 무료 제공 방식 언급, 현재 베타라 무료로 사용 가능하고 향후 유료화 전망, 현재는 개발자 경험 개선이 우선이라는 공식 문서 인용
          + 본인은 아직 Jules는 안 써봤지만, 무료 여부보다는 문제를 더 잘 해결하는지가 중요하다는 입장, 성능이 좋으면 쓸 것이고 아니면 다른 서비스 선택, 많은 사람들이 비용보다 효율을 우선시함을 언급
          + 이런 무료 런칭 방식이 빅테크의 경제적 전통이자 마치 시장에 물건을 쏟아붓는 dumping(덤핑) 효과와 유사함을 지적
          + 스타트업에서 흔히 쓰는 전략, 무료 베타로 시작해서 후에는 과금 전환
          + 사용에 제한 존재, 동시 2개 작업, 하루 5개로 제한 정보
          + 사용자가 곧 데이터라는 시각, Jules의 사적인 레포를 학습데이터로 쓰지 않는다는 공식 답변 인용, 하지만 대화 내용이 Gemini처럼 훈련에 취합될 가능성, 어떤 데이터가 수집될지 투명하지 않다는 점과 레포 내용이 포함될지 모호함 지적, 공식 법률 링크 공유
     * Google과 Microsoft가 같은 날 발표를 맞춘 듯 보임, 상대 발표에 맞춰 런칭을 서둘렀을 수도 있다는 생각, 현재 혁신이 활발하게 진행되는 시점이라는 기대감
          + 이번 주가 Google IO이면서 Microsoft Build이기도 해서 두 회사가 주목도를 높이기 위한 치열한 경쟁 구도 분석
          + 최근의 뜨거운 분위기 표현
          + 두 발표 모두 OpenAI Codex Research Preview에 이어 나왔고, 사실상 같은 제품이라는 평가
     * Google과 Microsoft가 맞춤형 자동화보다 주니어급 저수준 자동화에 집중한 점을 높이 평가, 접근 권한이 적을수록 사고 위험이 적고, 구조화된 작업일수록 데이터와 강화학습에 유리함, 저위험 구조 덕분에 신뢰성 향상 기대, 인터페이스와 통합에서 얻는 경험이 데이터 파이프라인 확장에 필수라는 관점 제시, 이제 추상적 논쟁에서 실제로 활용하는 단계에 진입했다는 점에 만족
     * “네가 원하는 일에 시간을 써라!”라는 광고 카피와 게임, 독서, 탁구 등 여가 활동 이미지에 대해, 코딩을 피해야 할 일로 여기는 듯한 뉘앙스에 대한 아쉬움, 코딩이 창의적이고 즐거운 활동임을 강조
          + 회사에서 Jules가 일을 한다고 해서 사장이 낮에 테니스 치도록 두진 않는다는 현실, 실제로 20-100% 생산성 향상이 있다 해도 그 가치는 노동자가 아니라 회사(자본)에 돌아간다는 의견
          + 본인은 가끔은 재미로 코딩하지만, 목적 달성을 위한 수단일 때만 사용하고, 코딩이 아니어도 방법이 있으면 그쪽을 선호하는 편임 설명
          + 광고에서 “원하는 코딩에 집중”이라는 메시지 자체는 긍정적으로 봄, 처음에 컴퓨터 작업하는 모습으로 해석, 결국 사용자의 선택과 시간 활용이 핵심이라는 해석 제시
          + 취미로 프로그래밍 자체를 즐기기에 뭔가를 대신 해주는 로봇이 오히려 비유상 뒤바뀐 느낌, 예를 들어 로봇이 대타로 자전거를 타준다고 팔면 별 의미 없다는 생각
          + 생산성이 ‘끊임없이 전진하는 흐름’과 비례한다는 개인적 경험, 내부 프로세스가 복잡한 기업일수록 이런 자동화 도구가 매력적으로 느껴질 수 있다고 함, 특히 리더십은 AI에 열광하면서 실질적 변화에는 소극적인 경우에 해당
     * “Jules가 변경 사항 오디오 요약을 제공해서 빠른 파악 가능” 기능에 대해, Google이 NotebookLM 기술로 이런 걸 할 수는 있지만, 프롬프트가 어떻게 구현됐는지 오디오로 듣는 게 정말 유용한지는 의문
          + 아이디어는 침대나 운전 중에 듣는 vibe coding 용이라는 추측, 젊은 세대가 오디오 정보를 텍스트보다 더 선호하는 추세 언급
     * Codex와 codex cli가 현재까지 써본 중 가장 뛰어난 평가, Codex를 ChatGPT 앱에서도 이용 가능해 높이 평가, 이번 서비스도 빨리 써보고 싶다는 기대감 전달
     * “네가 원하는 일을 하라!”라는 광고 카피에 자신은 새롭고 멋진 코드를 짜는 일이 바로 원하는 일임을 피력
          + AI 도구의 핵심 메시지가 바로 사용자가 원하는 유형의 코딩, 좋아하는 창의적 코딩에 더 많은 시간 투입 가능, 반복적이거나 싫은 업무는 맡기라는 것임을 해석
     * Jules에서 “비동기”가 중요한 의미인지, 작업 속도가 궁금함, 보통 개발 워크플로우가 비동기이긴 한데 더 빠르게 즉시 동기화가 된다면 좋겠다는 의견, 사람들이 빠르게 작업을 끝내지 않기 때문에 비동기가 표준이 된 현실 공유
          + 다른 AI 에이전트 툴들은 모델, 작업 복잡도, 우회 경로 개수에 따라 10~30분 정도 소요된다는 정보 제공
"
"https://news.hada.io/topic?id=21012","딜버트 창작자 스콧 애덤스, 조 바이든과 같은 암으로 곧 사망할 것이라고 발언","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              딜버트 창작자 스콧 애덤스, 조 바이든과 같은 암으로 곧 사망할 것이라고 발언

     * 딜버트 만화의 창작자 스콧 애덤스가 전립선암으로 조만간 사망할 것이라 밝혀 큰 충격을 줌
     * 애덤스는 해당 암이 뼈로 전이되어 여름까지 생명이 남지 않을 것으로 예상함
     * 전립선암은 국소화 단계에서는 완치가 가능하지만 뼈로 전이되면 치유 불가 상태임을 강조함
     * 최근 조 바이든 전 대통령 역시 공격적인 형태의 전립선암에 걸렸음이 공개됨
     * 애덤스는 바이든 가족에 존경과 연민을 표하는 메시지를 남김
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

스콧 애덤스의 암 투병 공개와 주요 발언

     * 딜버트 만화의 창작자 스콧 애덤스가 자신의 Rumble 방송(“Coffee With Scott Adams”) 에서 조 바이든 전 대통령과 같은 전립선암에 걸려 조만간 사망할 것으로 생각함을 솔직하게 밝힘
     * 애덤스는 전립선암이 뼈로 전이된 상태이며, 바이든보다 더 오래 이 병을 앓아왔음을 언급함
     * 본인의 여명이 아마도 이번 여름까지라고 생각하며, 이 세상에서 곧 떠날 것이라는 심경을 고백함

애덤스의 커리어 및 공개적 입장

     * 스콧 애덤스는 1989년부터 시작한 딜버트 만화로 세계적인 인지도를 얻음
     * 다수 저서 출간과 더불어 최근 10년간 정치적으로 더욱 적극적으로 의견을 개진하며 친 도널드 트럼프, 미국 민주당 비판 등 논란이 되는 입장을 SNS 및 Rumble 쇼를 통해 공유함
     * Rumble에서 약 3.8만 명의 팔로워를 보유, X(Twitter)에서는 120만 팔로워를 가짐

전립선암에 대한 애덤스의 설명

     * 애덤스는 방송에서 전립선암에 대해 “국소화 상태, 즉 전립선에만 병이 머물러 있을 때는 완치가 가능하지만, 뼈 등으로 전이될 경우에는 완치가 불가능해짐”이라 언급함
     * 최근 조 바이든 전 대통령도 공격적인 전립선암이 뼈로 전이된 상태임이 보도됨

조 바이든 가족에 대한 메시지

     * 애덤스는 바이든과 가족에게 존경, 연민, 그리고 동정의 뜻을 전함
     * 바이든 가족이 매우 어려운 시기를 겪고 있을 것임을 강조함

        Hacker News 의견

     * 스콧 아담스의 혁신은 독자들이 직접 줄거리를 제공하도록 유도한 점이라는 생각임. 그는 기업의 이상한 행태를 세상에 드러내기 위해 누구나 쉽게 그와 소통할 수 있게 했고, 실제로 많은 독자들이 자신의 직장 이야기와 스트레스를 전달해 비밀스러운 해소 기회를 가졌다는 것임. 지금도 많은 유튜버와 Substack 작가들이 비슷하게 독자 커뮤니티를 새로운 소재의 원천으로 삼고 있고, 이들은 현장의 문제를 걸러내는 프리즘 역할에 더 가깝다는 의견임. 이렇게 하려면 독자와 그들의 관심사에 진심으로 공감해야 하는 점, 만약 흐름이 바뀌면 작가에게도 독자에게도 혼란이 올 수 있다는 점을 이야기함. 아담스를 비롯해 암으로 고통받는 모두가 진솔하게 이야기 나눌 상대와 최고의 치료를 누리길 바람이라는 응원임
          + Grand Budapest Hotel의 작가처럼, 저자로 살다 보면 더 이상 스스로 이야기거리를 만들 필요 없이 사람들이 다양한 흥미로운 스토리를 먼저 들려준다는 얘기임
          + “The clue meter is reading zero.”라는 유명한 Dilbert 밈을 첨부함. Motorola 사원들은 이 대사를 한눈에 알아봤다는 일상 경험 공유임
     * 나는 그의 재치있고 매력적인 만화와, 다른 곳에 썼던 다소 거친 글들을 받아들이기 힘들었음. 단 한 번도 책을 버린 적이 없는데, 그의 책 한 권은 내 서재에 두고 싶지 않아 버렸음. 다른 누구에게도 추천하고 싶지 않은 정도였음
          + 나는 이런 책들에 대해 나만의 정책이 있음. Dilbert는 내 서재에 없지만 Neil Gaiman 책이나, 사회적으로 논란이 된 작가의 예술적인 TTRPG 책들은 거꾸로 꽂아 놓음. 이건 일종의 경고 깃발 같은 표시임
          + 사람은 결코 한 가지 모습만 있는 게 아님. 어떤 부분은 맞고, 어떤 부분은 틀릴 수도 있다는 점 인정 필요함
          + 명성이 그를 망쳤다는 생각임
          + 90년대에 읽었던 “Defective People”에서 마지막 장이 현실을 바꿀 수 있다는 내용으로 치달았음. 그때 이미 정신적으로 불안정하다는 생각을 했다는 경험임
          + 구체적으로 그가 어떤 “거친 발언”을 했는지 궁금함
     * 나도 기억에 남는 Dilbert 에피소드가 있음. 디버트가 회계사들이 사는 동굴에 내려가는 장면에서, 임의의 숫자를 중얼거리는 트롤에게 “정말 랜덤인가요?”라고 묻고, 가이드는 “그게 랜덤의 문제죠, 확실히 알 수 없어요.”라고 답함. 이 만화는 무작위성에 대한 통찰을 담고 있어 여전히 인상적임. 링크도 첨부함. 스콧에게 감사한 마음임
          + 내 회사도 점점 보안을 강화하면서 모든 게 불편해지는 중이라 Dilbert의 Mordac 만화가 개인적으로 제일 와닿음. “보안이 사용성을 압도한다면, 모두가 아무것도 못 쓰는 게 최고의 세상이다”라는 명대사. 로그인 절차로 “태양을 똑바로 쳐다보라”는 안내까지 등장함
          + 이 고전 만화가 아마도 내 최애임. 차량 설계에 팀원 모두가 참여해 기괴한 결과물을 뽑아내는 만화를 링크함
          + 더 좋은 만화 링크를 공유함
          + 이 에피소드는 마치 XKCD 만화처럼 재치 있고 Dilbert답게 잘 만든 것 같음. 함께 공유해준 기사도 흥미롭게 읽음
     * 포인티 헤어드 보스가 “익명 사내 설문에 따르면 경영진을 신뢰하지 않는다고 했는데, 이건 왜지?”라고 묻고 Dilbert가 멍한 표정을 짓는 장면임. 현실 회사에서 공감되는 장면임. 스콧에게 그동안의 즐거움에 감사 인사임
          + 익명 피드백을 정말 익명으로 유지하려면 글자, 문장부호, 띄어쓰기도 신경써서 바꾸는 습관임. 예전에 회사에서 별로인 상사 밑에 있다가 익명 설문 후 그가 “네가 이런 내용 썼지?”라고 물었는데 나는 실제로 그걸 쓰지 않았던 경험임. 상사가 나를 잘못 짐작한 게 오히려 더 찝찝했지만, 그래도 솔직하게 아니라 답할 수 있어서 좋았음. 익명성의 중요성을 상사도 깨달았길 바람
          + 고등학교 때 비슷한 경험이 있었음. 교사가 “익명” 설문을 나눠주고, 제출된 필적을 분석해 내 것임을 거의 정확히 알아냄. 일부러 글씨체를 바꿨는데도 소용없었음. 그 이후로 익명성 검증 없는 설문은 절대 믿지 않게 됨
          + 해당 Dilbert 만화의 링크를 공유함
     * Adams가 미국 사무직 현장의 부조리함을 날카롭게 풍자했다는 점이 정말 인상적임. 그런데 왜 그가 점차 상사의 시선, 혹은 “어두운 편”으로 옮겨간 것인지 의아함. 제약업계 현장 직원들이 그를 위해 좋은 약이라도 연구했길 바라는 바람임
     * 최근 아담스의 에너지가 확실히 떨어진 이유를 이 소식에서 설명 가능함. 아담스가 최근 몇 년간 논란의 중심이 되었지만, 10년 넘게 미국 대기업에서 일해본 입장에선 Dilbert만큼 사무직 판타지를 현실적으로 그린 만화는 없음. 나의 첫 직장도 Dilbert와 Office Space 그대로였음. 사내 상황을 완벽하게 요약한 Dilbert 만화 모음도 가지고 있음. 물론, 아담스가 요즘에는 이해하기 힘든 의견도 많이 내지만, 그가 떠난다면 슬플 것 같음
          + Dilbert가 최신 트렌드를 제대로 반영하지 못했다고 느낌. AI나 원격근무 등 신기술을 다루긴 하지만, 현실 현장과 멀어진 지 너무 오래되어 참신한 통찰은 떨어진다는 생각임. 요즘엔 주로 전해 듣는 이야기를 만화에 풀어내는 느낌임
          + 매일 만화나 코믹 스트립을 지속하려면 엄청난 에너지와 영감이 필요할 것임. 90년대 대기업 문화에서 더 이상 벗어나지 못하고, 현장 경험과 새로운 환경에 적응하지 못한 것 같음
          + 실제로 역사상 유명한 풍자물들처럼, Dilbert에 나오는 많은 에피소드가 현실에서 들은 다양한 실화에서 나온 경우가 많을 수 있음
          + 아담스의 “요즘 멍청하다 여기는 발언들”에 대해, 더 이상 순화시킬 필요 없이 그는 인종차별주의자라고 솔직히 부르고 싶음
          + 그가 공개적으로 점차 과격해지는 과정을 보는 게 안타까웠음. 원래 좋아했던 블로그도 몇몇 언론이 아담스의 글을 자극적으로 인용하면서 논란이 시작됨. 이후로 그는 논란을 페이지뷰 수단으로 활용하며 아예 논란이 될 만한 부분을 블로그 맨 앞에 노출하기 시작했음. 결국 오래 구독했던 블로그와 Dilbert 작품에서 멀어졌고, 이제는 정말로 그가 진심인 건지 의심스러울 정도임
     * 왕년 미국 대통령과 가족에게 연민과 존경을 전하고 싶다는 아담스의 멘트는, 최근 미국 문화의 낮아진 기준에 비하면 꽤 성숙한 태도라 느껴짐
          + 물론 이런 생각이 자신의 죽음과 맞닥뜨린 뒤에야 나온 건 아쉽기도 함. 인간애와 공감은 삶의 다른 계기에서도 얼마든지 배울 수 있지만, 암 진단만큼 그것을 강하게 일깨우는 계기는 드물다는 점 인정함
          + 다만 동시에 그가 다른 곳에서 “바이든 대통령도 나와 같은 암이 있지만, 내가 더 오래 앓았거나 그가 인정한 것보다 실제론 더 오래 알고 있었을 것”이라고 말한 점은, 바이든이 거짓말하거나 실제 상태를 숨기고 있다는 뉘앙스를 줘서 논란의 여지가 있다는 생각임
     * 나는 Dilbert 팬이고, 그의 “how to fail at everything and still win big”이라는 책도 즐겁게 읽었음
     * 정기 건강검진 혈액검사에 PSA 수치가 왜 기본 항목이 아닌지 이해가 잘 안 됨
          + PSA 검사가 실제로 생명을 구하지 못한다는 근거가 있음. 가까운 친구도 전립선암으로 잃었는데, 평소 무척 건강했던 그가 허리 통증으로 내원했다가 곧바로 4기 진단을 받음. 증상 발현 후 21개월을 버틴 경험임
          + 우리 아버지는 70대 후반이고 PSA 수치가 항상 높게 나오지만, 지금까지 생검에서 암이 발견된 적은 없음. 다만 검사 과정은 상당한 스트레스와 불편함을 초래함. 검사 자체에 대한 강한 의견은 없지만, 내가 직접 경험한 건 아니라 다른 시각일 수 있음
          + PSA 검사 빈도가 과도하면 오히려 해가 된다는 인식임. 많은 남성들이 전립선암을 안고 살아가다 다른 이유로 생을 마감하는 경우가 더 많음. 괜히 조기 발견한다고 무리한 개입을 하면 삶의 질만 떨어지는 경우가 많아, 그냥 별일 없길 바라는 쪽이 합리적인 선택임
          + 내 의사에게 들은 바로도 PSA 검사가 큰 효과가 입증되지 않았다며 더이상 하지 않는다고 함. 나는 58세고 아버지가 전립선암으로 돌아가셨기에 걱정은 남음
          + 대부분의 사람은 나이가 들수록 PSA 수치가 자연스럽게 오름. 전립선암 자체가 상당히 천천히 진행되다 보니, 치료가 실제로 삶의 질과 생존에 긍정적 영향을 주는 근거가 부족함. 다만, 아주 어린 나이에 공격적인 암이 생길 땐 예외지만, 그런 극소수만을 선별적으로 검사할 방법이 없다는 어려움이 있음
     * 만약 이번 여름이 자신의 마지막이 될 거라 생각한다면, 암이 이미 돌연변이와 전이에 의해 주요 장기를 침범한 경우일 것임. 뼈에 암이 전이된 상태만으로는 수년간 증상 없이 지낼 수도 있음. 전이성 암이 진짜 위험함
"
"https://news.hada.io/topic?id=20927","Stack Overflow는 거의 죽어가고 있다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Stack Overflow는 거의 죽어가고 있다

    Stack Overflow 몰락의 흐름

    1. LLM의 영향
          + 4개월 전부터 LLM이 Stack Overflow의 필요성을 줄이고 있다는 데이터가 있었고, 현재 그 영향은 확실해졌다.
    2. 질문 수 감소
          + 질문 수는 2009년 Stack Overflow 초기 수준으로 감소.
          + Marc Gravell이 공유한 그래프에 따르면, 2014년부터 감소가 시작됨.
    3. 주요 시점들
          + 2014년: 모더레이터가 “품질 관리”를 강화하면서 질문 수 급감.
          + 2020년 3월: 팬데믹으로 잠시 급증.
          + 2020년 6월: 다시 감소세로 전환.
          + 2021년 6월: Prosus에 18억 달러에 매각.
          + 2022년 11월: ChatGPT 등장 이후 질문 수 급락.
          + 2025년 5월: 질문 수가 역사적 최저 수준 도달.
    4. 현재의 평가
          + Stack Overflow는 사실상 몰락했고, 이제는 운영 종료나 헐값 매각만 남은 상황.
    5. LLM 외 원인도 존재
          + LLM이 원인 중 하나지만, 2014년 이후의 정책 변화 등도 쇠퇴에 기여함.
    6. 커뮤니티의 상실감
          + 인간 개발자들과의 교류 공간이 사라진다는 아쉬움.
          + 앞으로는 Discord, WhatsApp, Telegram 같은 커뮤니티가 대체할 가능성 있음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    5월 15일 업데이트

    7. 긍정적인 마무리 시도
          + Stack Overflow는 초창기에 많은 개발자들에게 큰 도움을 주었고, 개인의 성장에도 기여했음.
          + 첫 업보트를 받은 기억을 여전히 간직하고 있으며, 이 모든 경험에 감사함.

   검색엔진으로 검색 안되는 Discord에서 포럼 운영하는 흐름이 달갑지 않더라고요

   이미 쌓여 있는 정보를 열람하는 용도로는 잘 사용했지만
   솔직히 질문을 하는 것 자체는 너무나도 toxic한 인간들이 많아서 잘 안했습니다.
   최근에는 슬슬 관심이 시들해지니 덜하지만 10년 좀 더 전에는 정말...

   RTFM..이 생각나는군요 ㄷㄷ

   ㅋㅋ 개발자 커뮤들 특이긴 하죠 국내 okky 같은데도. 뭐좀 물어보니까 논리적으로 설명하거나 풀어내는게 아니라 '상대가 대기업 개발자니까 너보다 대단한 사람이고 그러니끼 그사람이 맞다' 이런 답변 받아봤고

   살려야하는데...
"
"https://news.hada.io/topic?id=20947","AI가 좋은 SQL을 작성하게 만들기: Text-To-SQL 테크닉","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 AI가 좋은 SQL을 작성하게 만들기: Text-To-SQL 테크닉

     * Gemini 기반의 Text-to-SQL 기능은 Google Cloud 전반에서 개발자와 비기술 사용자의 생산성을 높이는 데 활용됨
     * 실제 환경에서는 비즈니스 맥락 부족, 사용자 의도 해석 어려움, SQL 방언 차이로 인해 정확한 SQL 생성이 어려움
     * Google은 이를 해결하기 위해 지능형 데이터 검색, 문맥 기반 학습, 의미 계층화 기법 등을 도입함
     * 모델 자체의 한계는 다중 생성 후 최적 선택(self-consistency), 검증 후 재프롬프트, SQL 방언별 학습 등으로 극복함
     * 평가와 개선 측정에는 자체 벤치마크와 LLM을 심판으로 활용하는 기법을 포함해 실환경 적용 가능성을 높이고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Techniques for improving text-to-SQL

  텍스트에서 SQL로의 전환: Google Cloud의 현황

     * 조직은 신속하고 정확한 데이터 인사이트를 위해 SQL을 활용함
     * Gemini는 자연어로부터 직접 SQL을 생성하는 text-to-SQL 기능을 제공
     * 이 기능은 개발자뿐 아니라 비기술 사용자에게도 유용함
     * 현재 BigQuery Studio, Cloud SQL Studio, AlloyDB Studio, Vertex AI 등에서 이 기능을 제공

  Text-to-SQL 기술의 주요 과제

    1. 비즈니스 맥락 제공의 어려움

     * 정확한 SQL 작성을 위해선 LLM에게 데이터베이스 구조, 컬럼 의미, 실제 데이터 내용 등과 같은 충분한 컨텍스트 제공이 필요
     * 컨텍스트는 명시적 정보(스키마, 컬럼 정보 등)와 암묵적 정보(특정 데이터의 비즈니스 의미 등) 모두 포함
     * 데이터베이스 구조나 데이터셋의 변형, 스키마의 변화 등에 맞춰 LLM을 계속적으로 훈련(fine-tuning)하는 방식은 현실적으로 비용이 높음
     * 비즈니스 지식이나 의미 정보가 제대로 정리되어 있지 않고, 훈련 데이터로 전환하는 것 역시 어려움이 존재
     * 예시: DBA가 pcat_extension 테이블의 cat_id2 = 'Footwear'의 의미를 모르면 신발 판매 조회 SQL도 작성 불가함 — LLM도 동일하게 컨텍스트 부족시 정확하지 않은 질의 생성 위험 있음

    2. 사용자 의도 해석 문제

     * 자연어 질의는 SQL에 비해 명확성 부족이 흔함
     * 실제 데이터 분석가나 엔지니어는 질문이 불분명할 경우 추가 질문을 통해 명확히 할 수 있지만, LLM은 주어진 질문에 바로 답을 생성하려는 경향으로 인해 잘못된 정보(hallucination) 위험이 있음
     * “가장 많이 팔린 신발은?” 같은 질문에서 ‘가장 많이 팔린’의 기준(주문 수, 매출액 등)이나 결과 수 등에 대한 세부 기준이 모호함
     * 기술적 역량이 있는 사용자는 대략적 SQL을 시작점으로 삼을 수 있지만, 비전문가는 정확히 동작하는 SQL이 더 중요함
     * 효과적으로 작동하기 위해선 LLM이 후속 질문, 추론 설명, 유저 가이드 기능을 지원해야 사용자의 의도를 명확히 파악 가능함

    3. LLM의 생성 한계

     * LLM은 문서 요약, 정보 추출 등에는 강점 있지만, 정확한 SQL 문법이나 덜 쓰이는 SQL 기능에 대해선 취약점 있음
     * SQL 방언별로 문법이 다르며, 사소한 차이도 높은 정확성을 요함
     * 예: BigQuery에서는 EXTRACT(MONTH FROM timestamp_column)을 사용하지만, MySQL에선 MONTH(timestamp_column)을 써야 함
     * 복잡한 명세 준수에 꾸준히 맞춰 SQL을 생성하는 것이 LLM에는 쉽지 않은 과제임

  Google Cloud의 Text-to-SQL 향상 기법

    문제: 스키마 및 비즈니스 맥락

     * 의미 기반 검색 및 랭킹
     * 맥락 내 학습
     * 데이터 샘플링 및 연결
     * 의미 계층 구축
     * 사용 패턴 및 히스토리 분석

    문제: 사용자 의도 해석

     * LLM을 통한 명확화
     * 개체 식별, 관련 정보 확인 후 적절한 후속 질문 유도

    문제: LLM 한계 극복

     * Self-consistency로 다중 쿼리 생성 후 최적 선택
     * 유효성 검증 및 리프롬프트
     * SQL 방언 예시 포함한 맥락 학습
     * 모델 파인튜닝

  주요 기술 적용 예시

    SQL-aware 모델

     * Gemini는 고품질 SQL 생성을 위해 다양한 파인튜닝 버전 사용
     * SQL 방언별 정확도 확보를 위해 모델 버전 혼합 및 맞춤형 조정 수행

    사용자 질문 명확화 (Disambiguation)

     * 질문이 모호할 경우 명확화 질문 생성
     * 예: ""가장 잘 팔리는 신발?"" → “주문 수 기준인가요, 매출 기준인가요?”로 유도

    의미 기반 검색 및 맥락 구축

     * 다단계 의미 매칭 기반 벡터 검색으로 관련 테이블, 컬럼 식별
     * 유저 쿼리 기록, 비즈니스 규칙 예시 등을 포함해 프롬프트 구성
     * 긴 컨텍스트 윈도우 지원으로 대규모 스키마 대응 가능

    검증 및 재생성

     * LLM 생성 쿼리의 파싱, Dry-run 등으로 명시적 오류 탐지
     * 오류가 감지되면 재프롬프트로 수정 유도

    Self-consistency

     * 한 쿼리 대신 다양한 방식으로 다중 쿼리 생성
     * 여러 모델에서 동일한 쿼리를 추천할 경우 정확도 향상

  평가 및 성능 측정

     * 기존 벤치마크(BIRD-bench 등)는 유용하지만 실제 스키마 반영에 한계 존재
     * Google은 자체 합성 벤치마크 세트를 구축

    평가 전략

     * SQL 방언과 엔진별 기능 포괄: 쿼리 외 DDL, DML, 관리 작업까지 포함
     * 평가지표: 사용자 반응, 오프라인 지표, LLM-as-a-judge
     * 지속적 평가: 새로운 모델, 프롬프트 기술의 유효성 신속 확인 가능

  마무리

     * Google Cloud의 다양한 제품에서 text-to-SQL 기능을 실험 가능
     * 예: BigQuery Studio, CloudSQL/AlloyDB/Spanner Studio, AlloyDB AI

        Hacker News 의견

     * 텍스트에서 SQL로의 전환의 궁극적인 목표에 대한 고민이 있음, 데이터 분석가의 도우미를 만드는 것인지, 아니면 분석가를 거치지 않고 비즈니스 인사이트를 얻는 것인지가 목적임, 만약 두 번째라면 아무리 고도화되어도 비전문가가 SQL의 정확성과 충분함을 판단하는 것이 불가능한 문제임, ""왜 어제 전자상거래 거래가 80%에 머물렀는가?"", ""고객 획득 비용이 왜 오르는가?"", ""뉴욕 캠페인이 샌프란과 비교해 왜 더 안 좋았는가?""와 같은 질문은 text2sql의 범주를 벗어나는 질문임
          + 실제로는 두 번째 목적에 가깝다고 보지만, 결과물은 기대에 미치지 못함, 비즈니스에서는 보고서를 마지막에 변경하고 싶어하지만 분석가 부족으로 원하는 정보를 적시에 얻지 못함, ""무한 속도""로 해결하려 하지만, 실제로 문제는 메트릭에 대해 충분히 고민하지 않는 데서 발생함, 데이터가 복잡하고 비즈니스 지식이 외부에 암묵적으로 저장되어 있으며 데이터 인프라가 부족해서 분석에 시간이 오래 걸림, 똑똑한 분석 리더들은 AI 열풍을 이용해 기본 인프라에 투자함
          + 위의 질문들은 애초에 SQL로 풀 수 있는 문제가 아니라고 봄, SQL은 주로 ""무엇(what)""에 대한 답변을 제공하는데, ""왜(why)""에 답을 하지는 않음, 텍스트2SQL의 목표는 분석가가 ""무엇""을 빠르게 해내도록 시간을 줄이고, 그렇게 해서 ""왜""에 집중하도록 돕는 것임
          + 맞는 말이지만, 내 생각에는 자연어 텍스트가 LLM 시스템의 보편적 입력이 될 수 있다고 생각함, text2sql은 더 복잡한 질문의 답을 구성하는 정보 검색의 기반 역할을 할 수 있음, 단기적으로는 업무 보조 기능이지만 장기적으로는 자동화, 결과 비교, 더 큰 워크플로우로 통합하는 기초를 쌓는 게 목표임, 이런 질문에 답하는 기반 마련이 핵심임, 아직 할 일이 많음
          + 어떤 알고리즘도 사람이 할 수 있다면 만들고 테스트할 수 있음, 10명의 분석가가 있다면 데이터베이스와 비즈니스에 대한 이해도, 실력이 모두 다름, 자동화는 최소한의 실력과 지식을 보장하는 표준을 제공함, 새로운 분석가도 바로 더 나은 성과를 냄, 시스템을 전문가와 협력해 개발하면서 테스트하고, AI가 트레이드오프, 버그, 기대 결과와 비교해 해석하도록 하는 것이 유용한 목표임, 통찰력이나 ‘테이스트’는 자동화하기 어렵지만 도메인 전문가가 잘 설계된 자동화와 합리적 결과의 감각만 있다면 아주 멀리 갈 수 있음, 완벽하진 않지만 이런 것들이 내 목표임
     * OpenAI 4o 모델을 사용해 본 경험 공유, 비즈니스 지침, 업계 용어, 테이블과 외래키 설명을 함께 프롬프트에 전달함, 그러면 복잡한 조인 쿼리도 생성하고 결과를 반환함, SQL을 모르는 사용자를 위한 결과 제공이 목적이었지만 SQL 자체도 참고로 같이 보여줌
     * AI가 완벽한 SQL을 생성할 필요는 없음, 내 경우에는 출력물이 코드로 검증이 되더라도 미세한 의미 차이의 위험 때문에 복사해서 쓰진 않음, 대신 AI가 접근법을 제시해주면 그걸 참고해 직접 SQL을 처음부터 작성함
     * Google AI Studio의 최신 Gemini를 사용해 본 경험, 정말 놀랄 정도로 인상적이고 혁신적임, Claude와 ChatGPT의 코딩 결과는 마치 다른 세기에서 온 것처럼 느껴짐, 불과 한 달 전만 해도 Claude가 굉장하다고 생각했는데 지금은 Google Gemini 없이 어떻게 계속 코딩을 할 수 있을지 모르겠음, Gemini AI Studio는 프로그래밍에서 거대한 도약임
          + 더 많은 사람들이 아직 이 변화를 인식하지 못해 놀라움, Claude도 작은 작업은 잘 처리하지만 복잡한 문제로 발전하면 Gemini가 월등히 앞서감, 컨텍스트 핸들링 능력이 특히 인상적임, 나는 코딩 에이전트로 사용할 뿐만 아니라 85,000단어짜리 원고에 대한 베타 리딩에도 Gemini를 쓰고 있고, 전문가 수준의 피드백 리포트를 거의 실시간으로 받음
          + 이번 주에 무료 Gemini Pro에서 추론 모드를 비활성화한 것 같다는 느낌이 듦, 코드 작성 직전에 멈추거나 지나치게 일반화시키지 않도록 하면 상당히 유용함, 다만 Gemini는 코드를 과하게 작성하는 경향이 있음, 내가 주로 이용하는 방식은 코드 구현에 얽매이지 않고 Gemini를 통해 설계적 탐구에 활용하는 것임, Stripe로 구독 서비스를 만드는 사례처럼 내 기술 스택과 사례에 맞춘 기존 데이터를 맥락에 맞게 받아보면서 단 한 줄의 코드를 쓰기 전에도 설계 방향을 바꿀 수 있었음
     * 답은 “시맨틱 레이어(semantic layer) 사용”임, 올바른 맥락 전달에 가장 효과적이고 사람이 직접 개입할 최적의 지점임, 사람이 모든 핵심 지표를 명확히 정의해두면 LLM이 그걸 언제든 활용할 수 있음, 예를 들어 MAU를 정의해두면 LLM이 그 정의를 기준으로 쿼리를 생성함, SQL 대신 JSON으로 쿼리를 쓰면 LLM이 훨씬 일관적으로 결과를 냄, 우리는 cube를 사용하는데, 최고의 오픈소스 시맨틱 레이어임, 네이버에 닫힌 소스 옵션들도 있음, 과거 내 회사가 관련 블로그도 썼었는데 지금은 소유 회사가 호스팅을 중단함
          + “쿼리를 SQL 대신 JSON으로 쓸 수 있다”가 장점이라는 주장에는 동의하기 어려움, 도저히 받아들일 수 없음
     * 실제 업무에서 AI를 써서 SQL을 만들면 위험함, SQL을 모르는 사람이 잘못된 쿼리를 써서 서버에 심각한 영향을 줄 수 있음, 우리 팀에서 데이터베이스는 대다수 개발자 기준 크지만 정말 거대하지는 않음, 가끔 최적화된 쿼리를 더욱 개선해달라고 AI에 요청해도 AI가 더 나은 답을 준 적이 없음, 때론 AI가 헛소리를 하거나 실제로는 쓸모없는 제안을 함, 마치 멍청한 앵무새가 예전에 들은 이야기를 반복하는 느낌임
          + 경험상 프로그래밍을 잘 모르는 사람은 AI가 아니더라도 일단 해보고 일이 망가지면 남 탓을 함, AI는 그저 이런 사고의 빈도를 늘려줄 뿐임
     * AI의 텍스트-정규표현식 변환 기능이 있으면 정말 편리할 거란 생각임
          + 이 의견을 자주 보는데 솔직히 놀람, 사람들은 정규표현식을 정말 모르는 건지 궁금함, 정규표현식은 엄청나게 널리 쓰이고 학습 자료도 많고 진입 장벽도 낮음, 90%의 용도는 아주 간단하고 결과적으로 AI에 설명하느니 직접 쓰는 게 더 빠름
     * 내가 써봤던 모든 AI 도구 중 가장 실망스러운 게 BigQuery에 내장된 Gemini임, 컬럼 명도 명확하고 설명도 잘 달았는데, 문제 해결에 전혀 근접하지 못함
          + 내가 지금까지 쿼리를 가장 많이 작성한 언어가 SQL임, AI에게 쿼리를 써달라고 해도 내가 직접 작성했을 때보다 결과 다듬는 데 훨씬 더 시간이 오래 걸림, 부차적으로, SQL을 훨씬 더 빠르게 쓸 수 있게 해주는 기능 하나가 있다면 좋겠음, 우리 회사의 DSL에는 외래키 기준으로 자동 조인하는 연산자가 있어서 너무 편리함, SQL을 작성할 때 가장 귀찮은 것은 10개, 20개 넘는 조인문을 일일이 써야 하는 부분임, 다른 점은 그에 비해 그리 어렵지 않음
          + 경험상 명확한 제약 조건, 외래키가 잘 잡혀 있으면 거의 모든 게 해결됨, 테이블마다 명확한 제약 조건이 있어야 AI가 모든 연결 구조를 정확하게 파악할 수 있음, SQL은 아주 정확한 명세가 있지만, 그만큼 제약과 외래키가 잘 정의되어 있지 않으면 AI가 답을 정확히 내줄 수 없음
     * 모든 파운데이션 모델에서 꽤 간단해진 상태임, 테이블 스키마에 코멘트만 잘 달려 있으면 쿼리 생성 요청이 간단함
          + smolagents 라이브러리로 모델 주변 스캐폴딩 만들어주면 꽤 편리함, text2sql은 데모로는 단순해 보여도 실제로 복잡한 실제 케이스에서 이걸 적용하는 건 매우 어렵다는 글도 있음
          + Step 1: 스키마에 수천 개 테이블이 있고 주석이 거의 없음, Step 2...
          + 정말 그렇다고 봄, 이제는 별로 마법이 없음, 테이블 생성 DDL은 테이블의 정확한 설명이라 거의 더 필요 없는 게 사실임, 필요한 쿼리만 자세히 설명하면 웬만한 LLM이 쉽게 쿼리를 생성함
     * ""Show HN: We open sourced our entire text-to-SQL product"" (2024) 글에서 언급된 자료로 awesome-Text2SQL과 Awesome-code-llm > Benchmarks > Text to SQL 등의 훌륭한 레퍼런스가 있음
"
"https://news.hada.io/topic?id=20989","코드의 시대, 장인정신은 어떻게 살아남을까","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        코드의 시대, 장인정신은 어떻게 살아남을까

     * AI와 자동화의 세계에서 디자이너가 Soul을 유지하는 방법
     * 디자이너가 AI 시대에도 장인정신과 창조적 즐거움을 유지하려면 여전히 기계 언어에 대한 감각이 필요함
     * Vibe Coding과 AI 기반 엔지니어링 에이전트의 부상으로, 디자이너와 개발자는 점점 협업자이자 공저자로 변화하고 있음
     * 정적인 시각 디자인이 아닌 환경과 사용자에 따라 변화하는 동적 시스템을 만드는 능력이 중요해짐
     * 속도와 효율성이 장점이지만, 이는 몰입과 자긍심, 의미 같은 창작의 본질을 희생시킬 수 있음
     * AI는 인간의 창의력을 증폭시키며, 우리는 이제 더 크고 야심찬 결과물을 추구할 기회를 가지게 되었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

How Designers Can Keep the Soul in a World of AI and Automation

     * 1990년대에 나는 디자이너에게 코딩은 필수 스킬이라고 주장했음
     * 요즘은 AI와 vibe coding의 등장으로 코딩 자체의 중요성은 줄어들고, AI와의 협업 능력이 중요해지고 있음
     * 디자이너는 여전히 기계를 이해하는 감각(""speak machine"") 을 어느 정도는 갖춰야 함

     * 과거에는 디자인과 개발이 각기 다른 영역이었지만, 이제는 협업이 자연스러운 환경으로 변화함
     * vibe coding은 대화형, AI 강화 프로그래밍 방식으로, 디자이너와 기술자가 공동으로 시스템을 설계함
     * 이 흐름은 하이브리드 사고 방식을 촉진하며, 컴퓨테이셔널 디자인이 자연스러운 방식이 됨

     * vibe coding을 활용하면 디자이너는 정적인 이미지가 아닌, 환경과 사용자에 따라 반응하는 시스템을 설계할 수 있음
     * 하지만 동시에, AI는 창작을 큐레이션으로 전락시킬 위험도 존재함
     * 이는 과거 Arts & Crafts 운동이 대량 생산에 맞서 장인정신을 지키고자 했던 경고와 유사함

     * AI는 “충분히 괜찮은” 결과물을 자동으로 만들 수 있지만, 탁월함과 개성, 깊은 만족감은 여전히 인간의 손길에서 나옴
     * 몰입 상태(flow), 자부심(pride), 의미(meaning) 는 속도와 자동화로 대체할 수 없음

     * 기계 언어와 인간 언어를 모두 이해하는 디자이너는 점점 더 중요해짐
     * 이들은 더 포용적이고 반응적인 제품을 만들 수 있으며, 다른 이들에게도 영향을 줄 수 있음

     * AI는 목표 달성을 더 쉽게 만들 뿐 아니라, 더 야심차고 위험한 프로젝트를 시도할 여유도 제공함
     * 단순히 최소 요구사항을 충족하는 것이 아니라, 더 높고 도전적인 결과를 추구할 수 있는 시대가 됨
     * 이것이야말로 새로운 형태의 장인정신이며, 기술이 인간의 창의력을 확장하는 시대임

     ""기술이 인간의 손끝을 대신하는 것이 아니라, 우리의 시야를 더 높이 올려준다는 것. 그게 2025년 이후의 장인정신이다."" — John Maeda

   요즘 몽둥이 깍는 노인의 이야기가 많이 올라오는 느낌이네요..

   222

   방망이 아니었을가요? ㅎㅎ

   퉁퉁퉁 사후르?

   https://arca.live/b/aiartreal/75529594
"
"https://news.hada.io/topic?id=21024","구글, Veo 3와 Imagen 4, 그리고 영화 제작을 위한 새로운 도구 Flow 공개 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           구글, Veo 3와 Imagen 4, 그리고 영화 제작을 위한 새로운 도구 Flow 공개

     * Google DeepMind가 Veo 3와 Imagen 4, Flow를 공개하며 영상·이미지·영화 제작 도구를 혁신적으로 확장함
     * Veo 3는 오디오 포함 동영상 생성, 실제 물리 반영, 입술 동기화 등의 성능을 제공함
     * Imagen 4는 정교한 디테일 묘사와 타이포그래피 처리 능력 향상으로 출력물 제작에 유리함
     * Flow는 다양한 모델을 통합해 자연어 기반 영화 제작을 가능케 하는 새로운 창작 도구임
     * 모든 생성 콘텐츠에는 SynthID 워터마크가 삽입되며, 감지 도구도 함께 출시되어 투명성을 강화함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

새로운 생성형 미디어 모델 및 도구로 창작력을 발휘하세요

     * Google은 최신 생성형 미디어 모델인 Veo 3, Imagen 4, 그리고 새로운 영화 제작 도구 Flow를 발표함
     * 이들 모델은 이미지, 동영상, 음악을 생성하며, 창작자가 상상한 세계를 실현하는 데 도움을 줌
     * Google DeepMind는 영상 제작자, 음악가, 예술가들과 협력하여 도구를 공동 설계했으며 책임감 있는 AI 사용을 강조함

  Veo 3: 오디오를 포함한 고급 비디오 생성

     * Veo 3는 Veo 2보다 향상된 품질의 동영상을 생성하며, 처음으로 배경음, 대사 등 오디오를 포함한 영상 생성이 가능함
     * 텍스트나 이미지 기반 프롬프트를 통해 실제같은 물리 기반 영상 제작이 가능하고 입술 동기화도 정확함
     * Gemini 앱과 Flow, Vertex AI를 통해 미국에서 Ultra 요금제 사용자에게 제공 중임

  Veo 2: 창작자 피드백 기반 기능 추가

     * Veo 2에는 창작자들의 피드백을 바탕으로 다음과 같은 기능이 추가됨:
          + 레퍼런스 기반 영상 생성: 캐릭터, 스타일, 오브젝트 등을 이미지로 입력해 일관성 있는 영상 생성 가능
          + 카메라 컨트롤: 회전, 줌, 돌리 등 카메라 움직임 설정 가능
          + Outpainting: 프레임 확장으로 세로에서 가로 전환 및 장면 자연 확장 가능
          + 오브젝트 추가 및 제거: 객체의 크기·그림자·상호작용까지 반영하여 자연스럽게 편집 가능
     * 이 기능들은 Flow에서 사용 가능하며, Vertex AI API에 순차 적용될 예정임

  Flow: Veo에 최적화된 AI 영화 제작 도구

     * Flow는 Veo, Imagen, Gemini를 통합하여 자연어 기반으로 장면, 캐릭터, 스타일 등을 설정하고 영상으로 구현 가능
     * 미국의 AI Pro 및 Ultra 요금제 사용자에게 제공되며, 점차 다른 국가로 확대 예정임

  Imagen 4: 해상도, 디테일, 타이포그래피 향상

     * Imagen 4는 세밀한 텍스처 묘사, 포토리얼리스틱 및 추상적 스타일 지원, 2K 해상도 출력을 제공함
     * 타이포그래피 기능도 향상되어 카드, 포스터, 만화 제작에 유리함
     * Gemini 앱, Vertex AI, Slides, Docs, Whisk 등에서 사용 가능하며, 최대 10배 빠른 버전도 곧 출시 예정임

  Lyria 2: 인터랙티브 음악 생성

     * 음악가를 위한 Music AI Sandbox에 포함된 모델로, 창작 실험을 지원하고 새로운 음악 탐색을 가능하게 함
     * YouTube Shorts, Vertex AI, MusicFX DJ 등에서 사용 가능하며, API 및 AI Studio를 통한 실시간 인터랙션도 제공함

  SynthID로 AI 생성 콘텐츠 식별 가능

     * 2023년부터 시작된 SynthID는 이미지, 영상, 오디오, 텍스트 등 100억 건 이상의 AI 생성 콘텐츠에 워터마크 삽입
     * 새롭게 출시된 SynthID Detector를 통해 사용자도 생성 여부를 판별 가능
     * Google은 생성형 AI가 창작을 돕는 방향으로 활용되도록 책임 있는 도구 설계와 공개 협력을 지속하고 있음

        Hacker News 의견

     * 직접 테스트를 해보니 Imagen 4의 성능이 Imagen 3와 비교해서 크게 향상된 점이 없고, 프롬프트 정확도가 대략 60% 정도인 점
          + 왜 성공했을 때는 한 번만 시도하고, 실패한 모델은 여러 번 반복하는지 궁금증을 느낌 나는 이 테스트가 “모델이 맞출 수 있는지”와 “자주 맞추는지” 중 어느 것을 평가하는 건지 궁금증
            성공률 혹은 성공률 임계값을 정하고 시도 횟수를 고정해서 측정하는 것이 더 적절하다고 생각함
          + ""The Yarrctic Circle""에서 OpenAI 4o가 우승을 했지만, 컷라스를 들고 있지 않은 점, 아름다움은 있지만 시점이 말이 안 되고 해부학적으로 다리가 실제로 150% 더 길어져 있는 부분 등 기본적인 측면에서 완전히 잘못됨 이런 결과를 통해 현재 모델의 한계를 알 수 있는 흥미로운 리소스라고 생각함
          + ""Not the Bees"" 우승작의 손이 운전자와는 전혀 다르게 나와서 제대로 통과한 걸로 보긴 어렵다는 판단
          + 실제로 Imagen 4를 사용하는지, Imagen 3을 사용하는지 어떻게 확인하는지 궁금증 Gemini에서는 사용하는 모델을 알려주지 않고, Vertex AI를 사용하는지 의문
          + 더 어려운 예시를 들며 테스트 기준의 어려움을 제안
               o 가득 채워진 와인잔
               o 시계 바늘이 10시와 2시(즉, V를 나타내지 않는 시계)
               o 9단계 IKEA 선반 조립도
               o 모든 종류의 체조나 아크로바틱 등
     * 이제 전문가용 도구들이 오픈소스 버전들을 크게 앞지르는 느낌
       wan이나 hunyuan 같은 무료 모델도 훌륭하지만 Google이나 Runway의 최신 결과물이 한 단계 위라고 느낌
       특히, 편집 도구—모션, 방향, 컷, 오디오 삽입 같은 기능들—이 순수 생성력 이상으로 큰 차별점
       대형 기업들이 명확하게 광고 에이전시/할리우드 분야를 공략 중인 분위기
       이 툴들이 조만간 업계 표준이 될 날이 생각보다 더 빨리 올 것으로 기대
       아직 한두 세대 정도 더 발전이 필요하지만, 결과물이 매우 훌륭하다는 평가
          + 오픈소스가 비록 편의성에선 떨어지지만, 전문가 환경에서는 커스텀 lora, control net 등의 기능을 통해 생성 과정 중간에 원하는 요소를 추가할 수 있다는 점이 중요한 강점이라고 생각함
            로컬 생성에서는 과도하게 엄격할 수 있는 플랫폼의 콘텐츠 심의를 피할 수 있음
            comfy UI가 초보에겐 어렵지만, 커다란 통제가 없는 폐쇄형 툴을 쓸 바엔 아직 작은 YouTube 채널, 소규모 프로덕션에서는 오픈소스 도구가 많이 선택될 것 같음
          + GAI의 진짜 존재 확인은 품질의 차이가 사라질 때 가능
            그때가 오면 무엇이든 어떤 품질로든 코딩이 가능하다는 의미
          + agency/hollywood 타겟팅의 진짜 목적은 광고 분야라는 견해
          + Tencent Hunyuan 팀의 발전 상황을 분석
            Hunyuan Image 2.0이 발표되어 텍스트-이미지/이미지-이미지의 품질과 속도가 매우 인상적임
            실시간 2D 드로잉 캔버스 앱을 만들어 Krea가 제공하던 기능을 전부 구현한 수준
            이전과 달리 이번엔 클로즈드 소스라 아쉬움
            Hunyuan 3D 2.0도 좋았지만, 3D 2.5는 아직 공개되지 않음
            Hunyuan Video는 Wan과 비교해 진전이 없지만 Wan이 최근에 VACE라는 멀티모달/에디팅 레이어를 통해 주목받고 있음
            Comfy 커뮤니티도 VACE와 Wan으로 멋진 결과를 만들어 내고 있다는 분석
     * 저예산 인디 영화가 연출, 연기 모두 부족해도 관객의 몰입, 웃음, 감동을 줄 수 있는 이유는 전체적으로 일정한 품질의 일관성을 지니는 점
       반면 AI 영상 콘텐츠는 각각의 클립 자체로는 완성도가 높지만, 여러 클립을 하나의 작품으로 연결할 때 몰입을 유지하는 것에는 아직 한계가 있다는 의견
       서두나 소리로 스토리의 '레드 스레드(일관된 매력)'를 유지하는 콘텐츠에는 AI 영상도 가능하겠지만, 아직은 할리우드가 걱정할 단계가 아니라는 평가
       필름의 입자감 같은 요소, 그리고 24p 포맷이 여전히 예술적 선택이 되는 이유도 같이 언급
          + NeuralViz 유튜브 채널을 추천
            18만 구독자가 있는 AI 영상 기반의 시네마틱 유니버스를 만들고 있으며 굉장히 재미있는 쇼
            “여러 개의 AI 영상 클립을 엮어 몰입하게 만드는 건 먼 미래”라는 주장은 이미 현실에서 깨지고 있다 주장
          + AI 영상 콘텐츠가 할리우드에 미치는 영향은 사진이 회화에 미친 영향과 유사
            AI 네이티브 영상은 기존의 할리우드 3막 구조와는 매우 다를 수 있지만, 만약 시청자들이 옮겨간다면 할리우드도 결국 같은 길을 걷게 될 것이라는 관점
          + 볼 수 있는 좋은 콘텐츠는 이미 넘쳐나는 시대
            진짜 문제는 콘텐츠의 질이 아니라 유통력, 배포력인데, Google 같이 세계 최대의 문화 유통업체가 예술계가 고통받는 핵심을 외면하고 엉뚱한 쪽에 힘을 쏟고 있다는 비판
     * 이젠 모두가 AI 생성 영상을 한 번쯤은 보고도 진짜라고 생각하게 되었을 시점
       너무 눈에 띄는 예시는 알기 쉽지만, 계속 경험할수록 점점 더 자연스럽게 AI 영상이 우리 곁으로 들어오는 현상
     * Google이 Darren Aronofsky의 AI 스튜디오 Primordial Soup와 협업을 진행하는 상황
       SAG-AFTRA 파업으로 할리우드에서 AI 사용 금지가 논의됐지만, 이 새 스튜디오는 왜 영향을 받지 않았는지 궁금함
          + Primordial Soup가 조합과 관련 없는 회사라서 파업협정에 구속받지 않기 때문
            따라서 조합 배우는 고용할 수 없지만, 회사 성격상 큰 문제는 아닌 듯
     * 이번 작업의 기술적 수준이 놀라울 정도며, 오디오와 비디오의 싱크가 정말 뛰어나고, 대화도 별도의 보이스 모델 못지않게 훌륭한 점에서 감탄
     * 올빼미 영상과 노인 영상에서 약간의 언캐니 밸리(이질감)를 느꼈고, 종이접기 영상에서는 약간 위협적이고 공격적인 느낌을 받음
          + 지난 20년간 엄청난 발전을 체감
            전엔 이질감을 주는 영상을 위해 거대한 개발팀, 아티스트, 슈퍼컴퓨터 클러스터와 오랜 렌더링 시간이 필요했는데, 이제는 거대 클러스터와 추론 시간만 있으면 됨
          + 페이지 아래쪽에 뜨개질 캐릭터 버전에서는 훨씬 더 좋게 느껴졌으며, 현실에서 조금 벗어날수록 언캐니 밸리를 피하기 쉽다는 인사이트
          + 올빼미 영상은 전형적인 AI 이미지 특유의 ‘광택’이 있었고, 노인 영상은 매우 인상적이었다는 평가
          + 종이접기(오리가미)는 영상보다 오디오가 더 현실적이라는 인상, 각자에게 반영되는 자기 자신을 보는 느낌
     * 놀라운 기술 덕분에 개발팀에 진심으로 감탄
       동시에 아쉬움도 큼
       AI가 비창의적인 일은 더 자동화하고, 창작자들이 AI 콘텐츠 홍수에 묻히지 않았으면 하는 바람
          + 비창의적 작업의 자동화도 곧 오겠지만, 더 높은 정확성이 필요해 더 어렵고 오랜 시간이 걸린다는 설명
            아직 AI 정확도가 80% 수준이지만, 나머지 20%를 채우는 게 정말 고된 여정
            빠른 비행기(기술)로 도착해도, 마지막 걸음(완성도)은 교통체증 같은 난관 비유
          + 이런 얘기 나오면 엄청나게 많은 게이트키핑을 보지만, AI로 더 많은 사람들이 창작에 접근할 수 있게 되는 점을 긍정적으로 보는 의견
            앞으로 AI가 열어줄 창작의 새로운 가능성이 기다려진다고 기대
          + 비창의적 업무를 위한 데이터는 타인의 동의 없이 수집하기가 더 어렵기 때문이라는 의견
          + 예전에는 예술 작품(특히 디지털)이 이렇게 쉽게 유통된 게 아니었음
            음악도 마찬가지로, 레코딩 기술 이전에는 오직 라이브 연주만이 진짜
            지금은 디지털 시대가 오히려 예술사에선 이상한 시기일 수 있다는 시각
          + “AI가 창작자를 AI로 만든 작업물 더미에 묻어버린다”고 하지만, AI에 프롬프트를 정교하게 넣는 것도 창작이며
            실제로는 수십시간에 걸쳐 수작업으로 모델을 만들고 리깅하는 것이 오히려 비창의적 노동이라는 시각
     * AI 모델이 창의성을 만들어 예술가가 창의적인 비전을 실현하게 해준다는 논리를 흥미롭게 봄
       새 시대에서 ‘무엇을 만든다’가 아니라, ‘이끌어낸다’로 역할이 바뀌면서
       텍스트 프롬프트 기반 창작이 진짜 ‘비전’인지, ‘과정’이 없어도 예술의 길이 남는지 등 창작의 본질에 대한 고민
       창작이란 개념 자체가 미묘하게 재정의되어가는 현상
          + 이렇게 재정의되는 과정 속에서 2-3개의 대형 플랫폼이 제작수단을 독점하게 된다는 비판
            이들에겐 아주 편리한 재정의임
          + 창작적 비전이 프롬프트 하나에 응축될 수 있다고 믿으려면, 상상력 자체에 한계가 있다고 생각
            예술의 본질, 산출물, 과정과 그 사이 관계는 끝없이 논의해도 부족
            자료구조의 포인터와 데이터 자체를 혼동하는 것과 본질적으로 비슷하다는 재미있는 비유
          + 텍스트 프롬프트는 매우 짧지만 프롬프트 추종 능력이 좋아지면 변화가 클 수밖에 없음
            소프트웨어 엔지니어가 소스코드를 통해 비전을 실현하는 것처럼, 창작 분야도 변할 것이라는 예측
          + LLM 기업들은 사람들을 서비스에 의존하게 만들어, 모든 경제 활동에서 자신들이 중간 이익을 취하려는 전략이 있다는 의견
          + 오페라/연극/수작업 예술도 비슷한 과정을 거쳤고, 결국 사람들은 점점 더 쉽고 소비하기 편한 것으로 옮겨갔음
            (디지털 음악/TV/디지털 아트)
            이전 방식을 고급 예술로 여기는 소수만 남았다는 분석
     * Veo3를 실제로 사용해본 사람이 있는지 궁금증
       데모 영상은 인상적이지만, Sora를 쓸 때는 실제 사용 경험이 많이 좌절스럽고 히트/미스가 컸다는 개인 경험 공유
"
"https://news.hada.io/topic?id=21004","유럽 투자은행, 유럽 기술 분야에 700억 유로 투자","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     유럽 투자은행, 유럽 기술 분야에 700억 유로 투자

     * 유럽 투자은행(EIB) 이 2027년까지 700억 유로를 유럽 기술 산업에 투자할 예정임
     * 이 투자로 유럽은 인공지능 및 드론 등 신기술 분야에서 미국과의 격차를 줄일 계획임
     * 민간 투자를 유치해 최대 2,500억 유로 추가 자금을 조성하는 효과도 기대함
     * TechEU라는 신규 플랫폼을 통해 자금 신청 절차를 빠르고 간소하게 개선할 방침임
     * 이는 딥테크 등 유럽 경쟁력 강화를 위한 핵심 전략으로 간주됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

유럽 투자은행의 700억 유로 기술 투자 개요

     * 유럽 투자은행(EIB) 이 2027년까지 유럽의 기술 산업에 700억 유로를 투자하는 계획을 발표함
          + 미국과 비교해 혁신 격차를 줄이고자 하는 목표를 가짐
          + 인공지능, 군사용 드론 등 다양한 신흥 기술 분야에서 유럽의 입지 강화를 추구함
          + 민간 투자를 이끌어 총 2,500억 유로 규모의 자본 유입 가능성을 기대함

TechEU 플랫폼 출범 계획

     * TechEU라는 신규 이니셔티브는 올해 말 공식 출범 예정임
          + 연구자와 기업이 자금 지원을 신청하는 중앙 허브 역할을 수행함
          + EU 자금 지원 절차를 더 빠르고 단순하게 만들어 스타트업의 생존 및 성장 가능성 제고에 기여함
          + 투자 결정 시간도 6개월 이내로 혁신, 기존 18개월 대비 대폭 단축 추진

혁신 가속화 및 위험 감수 강화

     * EIB 대표 Nadia Calviño는 한층 높은 위험 감수 의지와 함께 벤처 캐피탈 투자 속도 가속 방침을 강조함
          + 빠른 결정이 현금 흐름이 빠듯한 스타트업의 존속에 핵심적임을 시사함
          + Calviño는 이런 변화가 ‘게임체인저’(gamechanger) 로 작용할 것임을 언급함

유럽 투자 환경과 전략

     * 최근 미국 경제 정책의 불확실성을 유럽의 기회로 활용 가능함을 강조함
          + 글로벌 투자자들이 유럽 시장의 안정성과 잠재력에 관심을 가지는 흐름이 확대됨
          + EIB는 방대한 시장 규모와 학문적 역량을 활용해 지속적 기술 발전을 도모할 방침임
          + 방위, 보안 분야도 우선순위로 반영, 이들 분야 투자가 기술 영역 발전을 촉진하는 시너지를 형성함

민관 협력 및 기대 효과

     * EIB는 민간 투자자와 공동 투자(co-investment) 를 통해 리스크 분산과 신뢰 구축까지 기대함
          + 이를 통해 유럽의 기술 생태계 혁신 선도 및 2,500억 유로 자본 활성화 목표를 지님
          + 이 계획은 27개국 EU 재무장관의 승인 대기 중이며, 다음 달 공식 결정이 예상됨

딥테크의 전략적 중요성

     * 여러 전문가들은 딥테크(deeptech) 분야에서 EU가 강점을 지닌다고 분석함
          + 딥테크는 장기적으로 유럽의 경쟁력과 혁신적인 생태계 구축의 핵심 요소임

        Hacker News 의견

     * 여기 EU 스타트업 지원 프로그램에 대해 비판이 많지만 구체적인 사례가 없다는 점이 아쉬운 점 강조함. 내가 EU와 미국 스타트업에서 활동한 경험상, EU 정부 자금은 거의 모든 스타트업에 완전히 쓸모 없는 구조라는 평가임. 기존 네트워크에 속하지 않으면 지원 자체가 어려운 구조이며, 대학과의 인맥이나 EU 관료 연결 없으면 지원 불가 판정임. 더욱이 새로운 도전이나 위험 부담 있는 사업에는 지원 자체를 기대할 수 없음. 마리오 드라기 전 ECB 총재 역시 EU가 혁신을 일으킬만한 위험을 감수하지 않는다고 언급한 사실 언급함. 지원금 신청 후 실제 수령까지 항상 수년에 걸림. 그 결과, 자금은 관료 집단 네트워크와 낮은 위험, 낮은 보상의 프로젝트에 배분되며, 대부분의 스타트업이 감당할 수 없는 느린 속도로 풀리는 문제점 지적함. 이런 흐름이
       변화될지 회의적임
          + 나만 이런 현실을 보는 것이 아니라는 사실이 신선하게 느껴짐. 유럽에선 민간 자본 투자가 충분하지 않음. 공공 자금은 결국 학계나 관료를 통해 흘러가며, 이런 방식은 자본배분 효율성 결여로 이어짐. 돈은 민간이 보유해야 제대로 투자 결정이 이루어진다고 생각함. 유럽인들은 왜 자산가가 자신의 돈을 직접 투자하는 것에 반감을 가지는지 이해 어려움
          + 거의 모든 지적에 동의하지만 한 가지는 예외임. 지원 프로그램의 위험 선호는 U자 곡선 형태라는 생각임. 낮은 위험 프로젝트와, 극단적으로 무모한 학문적 도전에는 오히려 자금이 배정됨. 그 사이처럼 합리적 위험-보상 비율을 가진 프로젝트는 철저히 무시 받는 구조임
          + 여러 사람들이 결국 비슷한 이야기를 하고 있음을 언급함. 이번 자금 투자 규모(예: Altman의 장난감 프로젝트 투자) 자체는 크긴 하지만, 이 돈도 결국 친인척, 학계, 지인들 위주로 흘러가는 점이 문제임. 엄청난 규모로 보이긴 하지만, 전 유럽 스타트업 생태계 전체 기준에선 실질적 성과로 이어지지 못함. 이런 돈 풀림 자체가 유럽 스타트업 기세를 잘 말해주는 현상임. 자본이 싼 시대임에도 효과 없음
          + 내가 스타트업 공동창업자로 혁신 자금 유치 시도 경험이 있는데, 위에 언급된 문제와 100% 일치함. 예를 들어, 댄스 및 문화 아카데미 운영 소프트웨어를 개발했을 때, 대학은 이를 혁신으로 보지 않음. 자금 지원 요건 중 하나가 대학 교수 인증이었는데, 대학은 연구 기관이지 유일한 혁신 원천이 아님. 우리나라의 경우 많은 분야에서 대학이 혁신의 근거도 아님. 위 지적이 정확하다고 생각함
          + 지적이 다 맞음. EU는 멋진 계획만 있고 실현력이 없음. 11년째 Capital Markets Union을 기다리는 중임. 유럽 블록체인 인프라 역시 일부 대학만 쓸 수 있어 사실상 사망 상태임. EU는 혁신적이거나 위험한 비즈니스에 자금 지원 못함. 벤처캐피털 문화 부재임. 일부 스칸디나비아 국가는 소규모 스타트업과 정부 보조금 문화가 있지만 높은 세금 체계가 글로벌 대상을 겨냥한 스타트업을 떠나게 만듦
     * 유럽 연구 프로젝트 관리 경험자로서 볼 때, 이런 자금은 전형적인 EU 관료주의 미로를 통과해야 하기에, 스타트업이 직접 받는 일은 거의 없을 것임. 다양한 프로그램, 하위 프로그램, 프로젝트 등 접근을 어렵게 만드는 단계가 늘어날 것임. 이런 시스템 때문에 진지하게 자금이 필요한 스타트업이 미국으로 떠나는 현상이 생김. 이번 뉴스에도 매우 회의적인 입장임
          + 스타트업이 직접 자금 받을 수 있음. 단, 학계의 기술 파트너로 자금만 흡수하는 회사라면 가능함. 최악의 경우 아예 사기에도 사용되는 경우도 있음. 예로, 유럽 프로젝트 관리 및 워드프레스 페이지에 80만 유로가 들어가는 사례 링크 공유함. 또, 400만 유로를 ‘혁신 촉진’ 명목으로 쓰면서 실상은 은밀히 돈을 빼가는 조직도 존재함
          + 나 경험과는 다름. 꽤 여러 스타트업이 EU 자금을 큰 어려움 없이 받는 것 경험함. 복잡하진 않음. 관료주의 수준에 겁을 내면 사업 자체를 운영하면 안 되고, 미국 역시 관료주의 없는 곳 아님
          + 내 경험도 달라서, 여러 동료들이 스타트업으로 EU 자금 받아 기존 직장을 그만둘 정도로 혜택 받음. 내가 소프트웨어 엔지니어로 한두 번 거들었고, 과정도 꽤 단순했음
          + EU가 지나치게 관료적이고 위험 회피적이라는 평을 자주 접함. 반면, 정부 지원(예: 소규모 기업 세금 감면, 창업가 실업 수당, 저금리 대출, 벤처 투자, 연결력 있는 임원진의 무료 멘토링 등)으로 스타트업이 성공적으로 시작한 사례도 많이 봄. 특히 프랑스, 덴마크 사례 인상 깊음. EU 정부와 개별 회원국 정부 차이에 대한 의문 제기함
          + 대부분 돈이 비효율적인 대기업 유지에 쓰임. 예: CHIPS Act도 돈이 소규모 하위 프로그램으로 쪼개져 결국 대기업(St, Infineon, NXP 등)이 받아감. 스타트업 대상으로는 “무엇이 필요하냐”는 온라인 질의만 반복함. 막상 돈 달라 하면, 돈 말고 뭐가 필요하냐는 반응이 돌아옴
     * 왜 더 많은 기술기업(특히 소규모 기업) 대상의 세제 혜택을 고려하지 않는지 의문임. 내 고등학교 동창 중 한 명이, 기술적 지식도 전혀 없고 프로그래밍 경험도 없는 상황에서, 电竞(e-스포츠) 플랫폼 만들겠다고 10만 유로 지원 받아 결과물을 워드프레스 기반 블로그 수준에 그친 사례를 직접 목격함. 한편 나는 프리랜싱 소득에 높은 세금 내고 있어서, 같은 나이(23세)에 이런 경험은 충격이었음. 내 생각엔 이미 수익 내는 소기업의 세금 감면이 더 낫다고 봄. EU의 세금 수준은 천문학적임
          + 세제 혜택 자체가 실질적으로 큰 효과는 없음. EU가 과소평가 당하는 면이 있는데, 자동차, 민간 항공, 공작기계 등 ‘지루한 하이테크’ 분야에서 자급자족하며 꽤 경쟁력을 갖춘 시장임. 주요 차이점은 미국이 단일한 자본시장과 규제 체계로 벤처 자금이 무제한으로 모이지만, EU는 규제 부족 때문에 오히려 20개 이상의 규제기관 상대해야 하기에 시장 통합이 더 시급함
          + 세제 혜택 문제는 결국 정치인과 관료가 자금 배분 통제력을 잃기 때문임
          + 요즘 투자 자체가 세법상 비용공제 되는 구조로, 이 덕분에 세제 혜택이 주주들의 ‘싼 현금화 방식’으로 쓰일 뿐임. 진짜 도움이 필요한 기업은 자본이 필요함. 세금 감면만으론 대규모 투자가 필요한 기업에 실효성이 떨어짐. 세율도 워낙 낮기에 현 수준에서 추가 감세 효과 크지 않음
          + EU가 회원국 세율을 명령할 수 없고, 전체 예산을 잘라내도 실질적인 세금 인하는 1~2%p에 지나지 않음. 반면, EIB는 투자은행으로서 자금 지원 역할 가능함
          + 노르웨이에선 R&D 비용의 25% 세금 공제 가능함. 소규모 기업은 기타 정부 자금과 합쳐서 최대 70%, 대기업은 개발용으로 50%까지 지원 도달 가능함
     * Calviño가 이끄는 건 적합하지 않다고 생각함. 만약 AI에 신경 쓴다면 Mistral 인재들이나, 기술 전반을 위한다면 Spotify 출신 인물들을 기용해야 한다고 봄. 자금만으론 유럽 임금 경쟁력 문제(특히 세후)는 해결 불가임. 유럽이 미국보다 가난해도, 대형 AI 랩이나 빅테크의 머신러닝 엔지니어, SW 엔지니어 임금은 비교 불가임. 자본 역시 ECB 기준금리 인하에도 EU에선 여전히 위험 회피 성향이 높고, EIB 자금 신청 과정은 변화 이후에도 여전히 고통스러움
          + 연봉 격차 이야기 많이 들었지만, 인기 높은 네덜란드 공대 다닐 때만 해도 미국 이주에 관심 가진 SW 엔지니어 거의 없었음. 인재 유출 심각지 않고, 유럽 엔지니어들이 미국보다 월등히 뒤처진 것도 아님. 미국 성공 스타트업들이 유별나게 천재 덕에 성공하는 것도 아님. 결국 SW 엔지니어링 인재가 근본 문제는 아니라고 봄
          + Calviño가 직접 수혜 기업 선정할 거냐는 의문 제기함
          + Spotify나 Mistral 같은 영리기업 리더에게 수십억 달러 규모 펀드 결정권을 넘기는 게 정말 좋은 생각인지는 의문임
     * 이런 프로젝트는 결국 똑같이 귀결됨. 관료가 세금으로 큰 기업, 학계 커리어 집단에 자금 배분하는 구조임. 유럽에선 “네가 누구냐”가 중요한 문화가 박혀있어서, “실력이 있거나 뭔가를 할 수 있냐”가 핵심이 아님. 이런 점이 유럽의 슬픈 현실임
          + “네가 누구냐”만 중요한 문화가 스웨덴 경영 진짜 단면임. 공공 분야는 무능한 관리자가 컨설턴트 없으면 아무것도 못하는 구조고, 민간도 마찬가지임. 그래서 진짜 능력자는 관리직에 아예 관심을 안 보이는 상황임. 이런 문화가 어디까지 지속될지 궁금함
          + 반도체, 양자컴퓨팅 등 하드 테크 스타트업 창업 논의 여러 번 참여했지만, 실제로 할 수 있는 역량 있는 사람이 나밖에 없었고, 실질적 역량은 인정받지 못함. 학계는 창업할 의지도, 경험도 없으면서 정부나 EU 지원금 덕분에 무조건 지분 50% 이상 요구하고, 대학 측도 “명성”을 이유로 30% 떼감. 대기업은 지분 없이 이사회 의석(자회사 통제권), IP권 원함. 이런 복잡한 요구가 50만 유로 미만 시드머니에도 붙어서 개발자 몇 명 연봉도 못 맞춰주는 상황임
          + “이런 프로젝트는 늘 똑같은 식”이란 주장에, 유럽 일반 전체인지 아니면 특정 구조만의 얘기인지 질문함
          + EU가 실제로 없는 분야에 엄청난 돈 쏟아붓는 게 이번이 처음도 아님. 특히 미국 벤처캐피털과 비교하면, 실질적 제품이나, 미국 테크에 견줄 만한 제품에 있어선 결과가 매우 부진함
     * HN 유럽 관련 글에 늘 비슷한 논쟁만 반복되어서 지침임. 미국이 유럽보다 잘하는 게 있지만, 그게 전부는 아니며 자기 정치적/사회적 입장에 따라 오히려 유럽이 더 잘하는 것도 많음
          + 위험 회피와 사회안전망을 늘 추구하면, 더 큰 위험 감수하는 상대에게 결국 뒤처지기 마련임. 승자와 패자의 격차도 커짐. 미국은 승자 독식, 유럽은 포용적 사회임. 결국 양쪽 최고만 취할 순 없는 구조임
          + 곧 미국이 더 잘한다는 인식은 틀어질 거라는 입장임
     * 결정까지 6개월이 18개월보단 낫지만, 이런 프로세스는 빠른 실행이 본질인 스타트업과 맞지 않음. 빠르게 움직이고 혁신하려면, 반년 아니라 며칠 안에 결정이 필요함. 결국 생겨나는 건 느릿느릿한, 자금 빨아먹기용 스타트업밖에 없음. 내가 예전에 이런 조직에서 일한 경험 있으나, 실상은 연구자 시간과 출장용 예산 확보만이 목표였고, EU 펀드 타기 위해 임의 컨소시엄 만들어 심사관 설득용 신뢰만 쌓으면 됨. 이런 목표 없는 자금 낭비에 신물이 났음
     * “스타트업 자금 신청 심사를 6개월 이내로 하겠다”라는 기사에서, 진짜 해결책은 정부가 먼저 심사하지 말고, 민간이 댄 투자와 매칭해주거나, 일정 성과에 이른 선착순 N개 기업에 대규모 저리 자금을 제공하는 방법이었음
          + 민간 투자와 매칭 모델은 의사결정 권한이 민간 자본으로 넘어가고, 최소한 정부 개입을 줄이지만, 큰 자본을 가진 이들이 세금만 뽑아갈 수 있는 남용 소지가 생김. 한쪽의 ‘깐깐한 심사’는 다른 쪽에서 보면 ‘책임 있는 관리’이기도 한 양면성 존재함
          + 이스라엘(Yozma), 중국(가이던스펀드), 미국(IRA, CHIPS Act) 등은 실제로 이런 민간 매칭 시스템을 썼고 성공함. 하지만 이번 EIB 보도자료는 매우 모호하고, 실제로는 아직 제안 상태임. 자세한 그림은 최소 2025년 중후반은 되어야 볼 수 있을 것이기에, 지금부터 너무 예단은 무의미함
     * 유럽은 지난 20~30년의 안일함에 따른 후유증을 곧 겪게 될 것임. 자립하려면 극적인 변화가 필요한데, 유럽이 미국식 테크 산업을 닮고 싶으면 미국처럼 행동해야 함
          + 그 말은 유로화를 세계 기축통화로 만들자는 뜻이냐고 반문함
          + 더 신자유주의적으로 변한다고 문제가 해결될 거라 생각하지 않음. ‘단일 해법’만 반복하려는 발상이 우스움
          + 심지어 미국 내 다른 지역도 실리콘밸리 모델을 복제하는 데 실패한 현실을 강조함
     * 유럽이 벤처캐피털 문화를 빠르게 만들 수 있는지 의문임. VC 파트너군은 경험 통해 쌓을 수밖에 없으니 성장도 느림. 미국 VC 시장의 1/4도 안 될 것 같고, 진짜 벤처스러운 투자만 따지면 그마저도 줄어듦. 유럽 기관들이 제한적 파트너로서는 좀 더 발벗고 나서야 한다고 봄. 기사 자체도 디테일 부족하고, 유일하게 나온 6개월 심사 기간은 스타트업 니즈와 전혀 맞지 않음. EIB가 기존 유럽 VC의 제한적 파트너 역할하는 게 더 맞다고 생각함
"
"https://news.hada.io/topic?id=21034","AI의 에너지 발자국에 숨겨진 이야기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI의 에너지 발자국에 숨겨진 이야기

     * AI 쿼리 하나당 에너지 사용량은 미미해 보이지만, 전 세계적으로 수십억 건의 요청이 쌓이며 에너지망에 실질적인 영향을 미치는 수준임
     * 주요 AI 회사들은 수십조 원 규모의 데이터 센터와 전력 인프라 투자를 가속화하고 있으며, 일부는 원자력 발전소 건설까지 계획 중임
     * AI 쿼리의 에너지 소비량은 모델 크기와 복잡도에 따라 수백 배 이상 차이가 나며, 비공개 모델의 경우 정확한 소비량은 거의 알려지지 않음
     * AI가 사용하는 전력은 대부분 화석 연료 기반 전력망에 의존하고 있으며, 이로 인해 탄소 배출 강도는 평균보다 48% 높음
     * 향후 AI의 사용이 지속적으로 확대될 경우, 2028년에는 미국 데이터 센터 전력의 절반 이상이 AI에 사용될 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Making the Model: AI 모델 훈련의 시작점

     * AI 모델은 수개월간 데이터 센터에서 수십 기가와트시의 전력을 소모하며 훈련됨
     * GPT-4 훈련에만 5천만 달러 이상, 50GWh의 전력이 필요했으며 이는 샌프란시스코 전체를 3일간 가동할 수 있는 수준임
     * 추론(inference)이 전체 AI 전력 사용의 80~90% 를 차지하며, GPU를 중심으로 한 데이터 센터의 역할이 핵심적임
     * NVIDIA의 H100 GPU가 AI 추론 작업의 중심을 차지하며, 수천 개가 연결된 클러스터 형태로 작동함

A Query: 하나의 질문이 소비하는 에너지

     * 같은 AI 모델이라도 질문 종류나 위치, 시간대에 따라 에너지 소비량과 탄소 배출량이 천차만별임
     * Llama 3.1 8B 모델은 평균 114줄(J) 소모, Llama 3.1 405B는 6,706줄을 소모
     * Stable Diffusion 3 Medium으로 이미지 하나 생성 시 2,282줄, 고화질 이미지 또는 동영상일 경우 수십만~수백만 줄 소모
     * 영상 생성은 이미지보다 700배 이상 에너지 사용, 향후 AI 영상 생성이 대중화되면 전력 소비 급증 가능성 있음

Fuel and Emissions: AI가 사용하는 전력의 출처

     * AI 데이터 센터는 태양광·풍력 같은 간헐적 에너지로 운영이 어려워, 평균적으로 더 높은 탄소 집약도의 전력을 사용
     * 예: 2024년 미국 전체 전력의 60%가 화석 연료, 원자력은 20%, 재생에너지는 나머지 20% 차지
     * 캘리포니아 vs. 웨스트버지니아처럼 지역에 따라 2배 가까운 탄소 배출 차이가 존재
     * 향후 메타, 구글, 아마존 등은 원자력 발전소 확충을 위한 공동 투자를 발표했지만, 완공까지 수십 년 소요

AI around the corner: 다가오는 AI 대중화와 에너지 급증

     * ChatGPT는 현재 전 세계에서 5번째로 많이 방문되는 사이트, 하루 10억 건 이상의 쿼리가 발생함
     * 추정치에 따르면, GPT 쿼리 1건당 1,080줄의 에너지, 연간 1,090GWh = 미국 가정 10,400곳을 1년간 가동할 전력
     * 영상·이미지 포함 시 연간 35GWh 이상 추가, 이는 AI 에이전트, 음성 모드, 영상 인식 기반 AI가 본격화될 경우 더욱 증가 예상
     * AI가 스스로 작업을 수행하고 사용자 데이터로 개인화되는 미래는 단일 쿼리 단위 예측으로는 에너지 수요를 설명 불가

The Future Ahead: 2028년까지의 에너지 수요 예측

     * 미국 AI용 데이터 센터는 2024년 76TWh 사용, 이는 720만 가구 규모
     * 2028년까지 165~326TWh 사용 예상, 이는 미국 전체 가정의 22% 전력 사용량과 맞먹음
     * 이는 3000억 마일 운전 시의 탄소 배출량과 동일하며, 데이터 센터의 전력 사용 비중은 4.4% → 12%로 세 배 증가 전망
     * 소프트뱅크, 오픈AI 등은 500조 원 규모의 데이터 센터 및 전력 인프라 투자, 세계 곳곳에 축구장 규모의 인프라 건설 중

Transparency Gap: 수치 공개의 부재와 시민 부담

     * 대부분의 AI 회사는 모델 추론 시 에너지 소비량을 공개하지 않음, 이는 공공 예측 불가능성으로 이어짐
     * 미국 정부 에너지기관(EIA)도 AI를 별도 산업군으로 취급하지 않아 통계 미흡
     * 버지니아 주 보고서에 따르면, 데이터 센터 에너지 비용으로 인해 일반 가정의 전기료가 월 37.5달러 인상될 가능성 존재
     * AI 인프라의 비용을 시민들이 떠안게 될 수 있으며, 이에 대한 사회적 논의가 필요함

   새삼 자체 tpu에서 실행되는 구글 제미니가 대단하게 느껴지네...

        Hacker News 의견

     * http://archive.today/mnHb8 링크 공유
     * Meta, Amazon, Google 같은 테크 기업들이 화석연료 문제에 대응해 핵발전 활용 목표 발표 소식 언급, 세 회사가 2025년까지 전 세계 핵발전 용량을 세 배로 늘리겠다는 서약 참여 내용 설명, 그런데 이 날짜가 어제 기사 기준으론 다소 이상하게 느껴짐, 실제로는 2050년이 목표임을 지적, 기사에 이런 식의 이상한 부분과 '전문가' 출처 남용이 있지만, 누군가라도 이 이슈를 수치화하려 노력한 점은 긍정적으로 평가
          + 이 기사에서 가장 이상한 점은, 빅테크 기업이 이런 데이터 공개를 거부한다는 사실임, 전문가들에게 추측하게 두면 안 됨, 전 세계에 영향 미치는 결정에 사회가 정보 접근 필요
          + 저렇게 이상한 부분이 단순 오타인지, 아니면 진짜 이상하다고 보는지 궁금증 표현
     * 예전 유즈넷에 글을 올릴 때 ""이 프로그램은 전 세계 수천 대의 컴퓨터에 뉴스를 전송합니다. 당신의 글은 네트워크에 수백, 수천 달러의 비용을 발생시킵니다. 정말 올릴 건지 신중하게 확인 바랍니다""라는 경고문이 항상 따라붙던 시절 있었음, 지금 LLM 클라이언트에도 이런 경고문이 필요하지 않을까 상상, 요청 한번에 얼마나 많은 대기 탄소가 발생할지 계산해주는 형태 제안
          + 일부 국가에서는 '롤링콜'(자동차 과도 매연 배출) 문화가 존재함, 이런 경고문이 오히려 역효과 유발 가능성 우려
          + 인용한 경고문이 오히려 상반된 의미를 전달, 결국 이런 경고가 사라졌던 이유는 비용이 대폭 낮아졌기 때문일 것, 비슷한 일이 AI에도 적용 가능
          + 이메일 인쇄 전에 환경 영향 고려하라는 거대한 푸터 메시지를 연상, 실제론 인쇄시 불필요하게 한 페이지를 더 소비했다는 경험 공유
          + 이 경고 메시지가 흥미로움, 실제 메시지 자체는 큰 비용이 발생하지 않지만, 이를 전송하는 컴퓨터가 비용 원인, 전송 메시지 양이 많아질수록 단위당 비용은 낮아진다는 점 언급
          + 모든 일에 이런 경고를 붙이지 않는 한 AI 기술만 에너지 사용 때문에 비판 받는 건 불공평함 강조
     * 핵심은 배출 비용을 전기 가격에 내재화하는 것이라고 생각, 개별 사용자 고민은 산만함, 수송·난방·산업 모두 전기화되면 수요가 어차피 급증이기 때문에 전기 신속 탈탄소화가 필요
          + 동의하지만, 소비 절감 또는 효율 개선도 에너지 전환 과정에서 중요한 부분, 쓰지 않으면 생산할 필요도 없어짐
          + 이런 비용을 내재화하려면 사회가 배출 비용이 '무엇인지' 합의하는 것이 선결 과제, 현실적으로는 전기를 아주 풍부하고 효율적으로 만드는 게 더 쉬운 길, 사회 자체는 쉽게 개선 불가
     * ""DeepSeek은 6000억 파라미터지만, 실제론 mixture-of-experts 구조로 토큰마다 약 12.5%만 활용(기억이 맞다면)"", 이걸 명시하지 않으면 본문 신뢰가 떨어진다는 의견, 텍스트 에너지 사용량에 대한 가장 신뢰도 높았던 분석(epoch.ai 링크) 공유, 일반 질문 응답 평균 0.3Wh~최대 맥시멈 맥락 40Wh까지 소비, 대부분 사용은 이보다 훨씬 적어 텍스트 에너지 사용량 자체는 효익에 비하면 작음, 반면 비디오 생성은 에너지 소모가 매우 큼, LLM 기반 코드 생성 사용에도 이런 수치 분석이 궁금함
     * 이 스레드에서 엄청난 에너지 소모를 정당화하는 댓글 수와, 현재 이 기술이 엄청나게 쓸데없는 것(텍스트/비주얼 스팸)에 쓰이고 있다는 상황이 크립토 때 에너지 논란과는 놀랍게 대비됨, 빅테크 기업들이 주요 고용주라는 점 때문에 용인되는 듯한 분위기도 있음
          + AI는 이론적인 이점이라도 있으나, 크립토는 설계상 낭비적임, 하지만 실제 AI의 비용-편익 구조 역시 아직 미지수
          + 비트코인은 가격이 오를수록 에너지 소모도 늘지만, LLM 추론 비용은 빠르게 하락 추세(참고 링크), Apple, Google 같은 곳도 자체 데이터센터 및 온디바이스 AI 도입을 시도 중, 동시에 더 고비용 알고리즘도 계속 개발되는 현실, 대부분 사용이 노트북·폰 같은 배터리 한계 기기에서도 돌아갈 정도로 저렴해질 가능성도 있음
          + 크립토 붐과의 연결 지점이 흥미로움, 이런 흐름은 한 번 시작하면 되돌리기 쉽지 않은 게 인류 본성
          + 예전 HN의 크립토 분위기가 지금처럼 부정적이지 않았던 기억 공유
          + 크립토 에너지 사용(특히 비트코인 PoW)은 진짜 낭비라는 농담
     * 이 페이지가 하는 일도 불확실한 자바스크립트 때문에 아무 것도 안 하는데 내 CPU가 최대로 사용되는 상황에서 글을 읽고 있다니 아이러니
          + 브라우저가 각 페이지가 쓰는 CPU 양을 엄격히 제한하고, 추가적으로 CPU 자원 사용이 필요할 때는 카메라처럼 명시적 권한 요청을 하도록 했으면 좋겠음
          + 점심 전에 이 글을 읽고 비슷한 생각, tabdouse 관련 블로그 글 공유, cgroups 등도 트릭이 있지만 완전 만족스럽지 않다는 언급
     * 지금은 AI가 마치 컴퓨터 초창기 '메인프레임' 시대 같음, 방 하나를 채울 정도의 큰 기계가 엄청난 전력을 소비하면서도 지금 스마트폰보다도 덜한 성능 제공, 앞으로 모델 효율화와 하드웨어 특화가 빠르게 이뤄질 것으로 봄, AI 전용 칩을 통한 로컬 추론이 대부분의 작업 에너지 사용 대폭 절감, 이 덕분에 대형 컴퓨팅 리소스는 복잡한 과학 문제에 집중 가능하다는 점이 유의미하다고 생각
          + CPU 발전 곡선(기하급수 성장)을 종종 인용하지만, 사실상 다른 분야에서는 거의 통하지 않음, 반도체가 아주 특별한 행운의 조건에 의해 성장 가능했던 것, 배터리·핵융합·양자 컴퓨팅 등은 그렇지 못함, 반도체 계열에서 이미 먹을 수 있는 저열매는 다 따먹은 상태라 AI 효율화도 그렇게 순식간에 성장할 가능성 낮음, 앞으로는 수십 년 느린 점진적 발전이 현실 가능성, 수십억 파라미터와 수조번 연산의 필요를 한순간에 없애는 길 없다는 의견, 혹시 포토닉 컴퓨팅이 가능성?
          + ""AI 전용 칩"" 이야기가 개인적으로 잘 이해 안 됨, LLM이 애초에 GPU에 맞게 설계된 기술, 이미 하드웨어 존재, 문제는 GPU가 점점 더 크고 뜨거워지고 전력소모가 커진다는 점, GPU보다 나은 게 있었다면 이미 옮겨갔을 것, CPU로 되돌려서 효율 낼 수 있는 구조면 이미 변했을 것이라는 의견
          + 실제로 내 오래된 노트북(7년)에서도 작은 Gemma 모델을 꽤 쉽게 돌릴 수 있음에 놀람, 일부 작업만 LLM에 넘기고 나머지는 기존 프로그램으로 처리하는 식으로 효율 향상 가능성 상상
     * AI 에너지 사용에 관해 지금까지 본 글 중 최고, 빅테크가 사회적 의사결정에 필요한 데이터 제공 꺼리는 점 인상적, 관련 깊이 파헤치는 Data Vampires 팟캐스트 시리즈 추천
     * 2017년 AI 도입 후 데이터센터가 에너지 집약적 하드웨어로 구축되며 2023년까지 전력 사용 두 배 증가라는 기사 내용이 흥미로움, 하지만 실제 생성형 AI 부상은 2022년 11월 ChatGPT 등장 후 본격화, 2017~2022년 5년간의 AI 성장 대부분은 생성형 AI가 아니었을 듯
          + 2017년은 알파고가 이세돌을 이긴 다음 해이자, 'attention is all you need' 논문 출간 연도, 실제로 산업계에선 이미 전조가 있음, OpenAI가 2022년 시장 적합도 맞춤에 성공했을 뿐, 업계가 마냥 방황하진 않았다는 말
          + 그 즈음부터 머신러닝에 GPU 활용 본격적으로 시작
          + Meta는 이미 검색, 추천, 그래프 등 자사 서비스 모든 영역에 AI를 적극 도입 중, 그 덕분에 LLM 열풍이 오기 전부터 수만~수십만 대 GPU가 이미 준비되어 있었고, 덕분에 Llama 등 주요 프로젝트를 진행할 수 있는 유리한 위치에 있었음
"
"https://news.hada.io/topic?id=20946","Qwen 3 (큐웬 3) 서빙 최적화를 위한 MoE 업스케일링 전략","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Qwen 3 (큐웬 3) 서빙 최적화를 위한 MoE 업스케일링 전략

   오픈소스 모델 중 가장 인기 있는 알리바바 큐웬3(Qwen3)의 비밀은 바로 Mixture-of-Experts(MoE) 구조에 있습니다. 하지만 전문가의 선택이 항상 옳은 것도 아니고, 어떤 전문가는 거의 선택되지 않기 때문에 불필요합니다.

   큐웬(Qwen)3 MoE를 포함하여 MoE 구조의 라우터를 튜닝하는 과정에서 단순히 빈도 높은 전문가만을 사용하는 기존 방식을 넘어, AI 출력 품질에 진정으로 기여하는 전문가를 정확히 평가하고 선택하는 여러 전략을 소개합니다. 이 전략을 사용하면 MoE 모델의 정확도와 그 속도를 높일 수 있습니다.
"
"https://news.hada.io/topic?id=21007","2009년 이후 내가 만든 사이드 프로젝트들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        2009년 이후 내가 만든 사이드 프로젝트들

     * 사이드 프로젝트를 2009년부터 지속적으로 제작해왔음
     * 다양한 프로젝트 중 일부는 판매, 일부는 여전히 운영, 그리고 몇몇은 조용히 종료됨
     * 프로젝트 제작에 주로 WordPress, 일부는 Laravel과 React 사용 경험
     * 가장 익숙한 기술 스택을 활용하는 것이 성공적인 프로젝트 제작에 중요함
     * 현재도 여러가지 활성화된 프로젝트와 과거에 판매되거나 종료된 다양한 예시 존재
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   나는 2009년부터 다양한 사이드 프로젝트를 제작해 왔음
   이 사이트는 내가 그동안 만든 모든 프로젝트를 기록하는 공간임
   일부 프로젝트는 판매했고, 일부는 지금도 온라인에서 운영 중이며, 나머지는 조용히 사라짐

   내 사이드 프로젝트 접근법은 매우 단순함
   내가 좋아하는 것을 만들어왔음
   대부분 WordPress를 사용했고, 예외적으로 Laravel과 React도 적용함
   가장 유용한 팁은 '가장 익숙한 스택을 이용할 것'임
   새로운 프레임워크에 집착하지 말고, 꾸준히 만들기 시작하는 것이 중요함
   사용자들은 어떤 기술 스택으로 만들었는지 크게 신경 쓰지 않음
   계속해서 만들어보는 경험이 중요함

🏃🏻 활성 프로젝트

     * 언제 어디서나 비교, 탐색, 플레이할 수 있는 서비스
     * WordPress용 프리미엄 블록 테마를 원하는 크리에이터를 위한 프로젝트
     * $5에 프로젝트와 콘텐츠를 영구적으로 노출할 수 있는 플랫폼
     * 인터넷에서 가장 귀여운 동물의 매력을 만나볼 수 있는 서비스
     * 나만의 RC카 디지털 쇼케이스 생성 및 빌드 로그를 공유할 수 있는 공간
     * 프리미엄 템플릿, 도구, 에셋 제공 서비스

💰 판매된 프로젝트

     * 국가별 플래그 색상 코드 정보(HEX, RGB, CMYK 등)
     * 초기 수익 미발생 프로젝트와 MVP 사고팔기, 탐색 서비스
     * 온라인 이력서 생성 및 공유 플랫폼, 불필요한 요소 없는 이력서 제공
     * 모바일 앱 개발자를 위한 개인정보 처리방침 호스팅
     * 세계를 바꾼 최신 발명품 소개 서비스
     * 전 세계 기호와 그 의미 정보 플랫폼
     * 과거 투자 시 예상 수익 계산 서비스
     * 시간 여행을 하듯 기술 제품의 역사 탐험 서비스
     * 10억 달러 유니콘 기업으로 성장한 스타트업 큐레이션 목록
     * 사라진 Google 제품과 종료 이유 데이터 제공 서비스
     * 기술 업계 인수합병 정보를 종합한 리스트
     * 개발자용 코드 스니펫, 도구, 자료 제공

💀 종료된 프로젝트

     * 고대의 문자와 숫자 체계 정보 제공
     * 하이쿠, 예술작품, 시, 사진 등 창작물 공유 플랫폼
     * 무작위 하이쿠 배달 서비스
     * Github 인기 저장소 목록 제공
     * 주요 테크 기업 종가 자동 업데이트 서비스
     * 전 세계 평균 주간 근무 시간 정보 제공
     * 토큰 시세 및 거래소 허브 서비스
     * AI를 활용한 암호화폐 미래 가격 예측
     * 사용자가 주도하는 암호화폐 토론 공간
     * 웹호스팅 업체의 기본 네임서버 목록
     * 블로거를 위한 WordPress 테마
     * 축구 레드카드·옐로카드 투표 플랫폼
     * 축구 하이라이트 집중 플랫폼
     * Windows 팁과 트릭 정보를 공유한 블로그

        Hacker News 의견

     * 다시 이런 에너지를 갖고 싶은 바람 표현
          + 한 해 동안 사이드 프로젝트에 거의 진척이 없어 번아웃 때문이라는 추측, 항상 지친 상태임을 설명
          + 에너지가 있다면 굉장히 재미있다는 생각, 그 중 하나가 수입원이 될 수도 있다는 기대감 공유
          + 만약 에너지가 없다면 그럴만한 이유가 있는 것임을 인식하고 괜찮다고 위로, 휴식과 이완의 우선순위 필요성 강조
          + 젊었던 시절의 에너지가 영원히 사라졌다고 여겼으나, 3개월 전 진심으로 믿는 프로젝트 아이디어가 떠오른 이후 16시간씩 코딩 가능, 지치지 않고 오히려 활력 얻는 기분 설명, 젊음을 되찾은 듯한 느낌 공유, 얼마 전 깨달음 덕에 절반을 다시 짜야 해서 출시가 몇 주 미뤄질 예정임을 덧붙임
          + 에너지보다는 동기나 흥미의 문제라고 생각 Borders에서 PHP 서적을 구입하고 만들고 싶은 웹사이트를 꿈꾸거나, Best Buy에서 iPad를 처음 샀을 때 새 게임을 만들기를 상상하던 시절이 그리움
          + 똑같은 감정 공감, 자신도 하고 싶은 프로젝트 목록이 있지만 지쳐 있어서 실천하지 못하는 자신을 탓하는 중이라는 고백
          + 매우 공감, 아이가 태어난 뒤 한발 물러서게 되었고 속도도 달라졌지만 괜찮음, 휴식도 흐름의 일부라는 가치관 공유
          + 요즘 사이드 프로젝트는 블로그 만드는 것처럼 무의미하게 느껴짐, 왜 해야 할까 싶고, 아무도 관심 없고 수익이나 보상도 크지 않다는 회의감 내비침
     * '2009년 이후 끝내지 못한 모든 사이드 프로젝트 목록' 페이지를 만들고 싶다는 유쾌한 아이디어, 너무 많아 호스팅 요금이 감당 안 될지도 모른다는 농담
          + 오랫동안 그리고 여전히 같은 문제로 고민 중, 스스로 터득한 (원치 않는) 조언 전달
              1. 뭔가 지금이라도 그냥 시작하기, 순서나 조직화 고민하지 말고 일단 활동이 중요, 시작이 곧 동력임
              2. 오늘의 충분함 > 내일의 완벽함, OSS 프로젝트 활용 경험, 거칠어도 오늘 당장 쓸 수 있으면 그게 이익, 미완성이어도 가치 얻음
              3. 장기적인 개인적 가치 찾기, 10년 넘게 동물보호소 시스템을 운영 중, 월 비용 크게 줄일 수 있지만 리뉴얼 동기는 부족, 다만 점점 쌓이는 기술 부채와 지원 부담은 도전
              4. 미완성 프로젝트는 버그가 아니라 특징임을 인식, 호기심과 탐구의 과정에서 얻게 되는 깨달음이 인생 전반에 도움, 결과물이 아니라 '행동'이 중요하다는 결론
          + 자신과 비슷하다면 이런 리스트도 아마 끝내지 못할 것이라는 재치 있는 자기 고백
          + 모두가 작성 완료하면 명단에 본인 것도 올릴 테니 꼭 연락해달라는 유쾌한 동참 의사
          + 그마저도 완성 못할 것 같은 농담
          + '완성'의 정의만 바꾸면 모든 게 쉬워짐을 깨달은 경험 공유, 예전에는 뭔가를 끝마치지 못하는 것에 괴로워했으나, 지금은 '지불 유저 확보'보다는 새로운 것을 배우고, 디자인/아키텍처 실험, 혹은 내 문제 해결에 더 가치 둠 이런 식으로 최근 몇 년간은 거의 모든 프로젝트에서 가치를 얻어 '완성' 비율 상승
     * 판매와 폐기된 제품의 비율이 꽤 높아 보인다는 인상, 대부분의 사람은 사이드 프로젝트를 한 번이라도 파는 것조차 쉽지 않다는 견해
          + 자신도 같은 점이 제일 먼저 눈에 들어왔다고 언급, OP는 사이드 프로젝트를 어떻게 판매하는지 궁금, 직접 찾아서 파는지 아니면 문의가 오는지 질문
          + 판매 숫자가 높아 놀라움, 판매되었다는 것 중 3/4은 지금은 아예 접속이 안 되는 등 금액도 천차만별일 것으로 추정, 그래도 재밌는 리스트임을 밝힘
     * 누가 Google Cemetery처럼 단순한 사이트를 왜 구매하는지 궁금, 투자수익률(ROI) 관점의 이유 질문
          + '판매'로 표기됐지만 도메인은 현재 작동하지 않음을 먼저 언급, 가끔 언론에서 이슈가 되어 트래픽이 증가할 수 있어 광고 수익화가 가능할 수도 있고, 단순한 사이트라 유지 관리 부담도 크지 않다는 점에서 가치를 예상
     * 가벼운 응원의 인사와 함께, 수익화 가능한 네 가지 아이디어를 놀이삼아 개발하고 싶다는 소망 모든 아이디어는 자신의 개발 능력 안이며, 거창하게 은퇴까지 생각하는 게 아닌 단순히 100달러를 벌더라도 기쁠 것이라는 기대 두 개는 게임, 하나는 디렉토리, 하나는 유틸리티 타입의 간단한 사이트로 구상, AI나 가입, 마케팅 등 일절 없음, 단순 광고 수익 모델만 고려, 하지만 분석 마비(paralysis by analysis)로 실행에 옮기지 못하고 있다는 자기 진단
          + 아무것도 완벽하지 않고 결국 모든 코드는 사라지거나 대체되니 그냥 지금 코드 쓰기를 시작해서 즐기라는 해결책 제시
          + 동기 설정 방식이 다르다고 생각, 재미 있고 배우고 나누는 자체가 경제적으로 납득됨, 단순 광고로 월 100달러가 목표라면 차라리 아르바이트가 더나은 선택, 이런 시나리오에서는 '안 끝내는' 게 맞고, 애초에 시작하지 않는 게 더 최적화된 길이라는 견해
     * 멋진 일이라 감탄, 본인도 비슷하게 포트폴리오 페이지가 있는데 'Sold' 섹션 추가한 적 없음, 이 참에 업데이트할 예정, 자신의 포트폴리오 링크도 공유
     * 20년간 수많은 사이드 프로젝트를 시작했지만, 아직도 살아있는 건 자신의 책 관련 사이트 하나뿐 17년 전 자신을 위해 만들었고 지금은 월 800만 뷰를 기록, 모든 사이드 프로젝트 중 가장 어려운 건 실제로 사이트를 런칭하고 운영 가능 수준까지 만드는 것, 대부분의 시간은 배포와 운영 문제로 소비한다는 경험
          + 이 사이트 덕분에 Marquez를 접하게 되어 감사, 멋진 작업이라는 칭찬
     * 멋진 결과라는 칭찬, 프로젝트가 '완성'됐다는 기준이 무엇인지 질문
          + 초기 버전이 '쓸모 있다고 느끼는 순간' 출시, 너무 고민하지 않으려 함, 이후에도 재미있거나 유용하면 기능 추가, 바로 써보면 만족
     * 많은 사이트를 판매한 게 인상적이라는 감상, 본인도 비슷한 포트폴리오를 만들었지만 '판매'란은 없음, 이 부분 다음 업데이트 때 추가할 예정, 자신의 포트폴리오 페이지 링크도 첨부
     * 퍼블릭 프로젝트를 만들고, 사이트 어딘가에 연락처나 폼을 두면 관심 있는 사람이 연락해와서 세부 논의한다는 간단한 판매 과정 설명
          + Acquire.com, Flippa, 직접 연락 등 다양한 경로로 프로젝트 판매 경험
          + Microacquisitions (소규모 인수합병) 가능성을 제시하며 관련 서브레딧 링크도 첨부
          + 프로젝트 목록 중 ZeroAcquire 사이트를 보고, 본인도 같은 궁금증을 해결하고 해결책을 또 팔았던 것 같다는 직관
     * 잘 만든 'Sold' 리스트에 깊은 인상, 본인도 비슷한 포트폴리오를 만들어 뒀지만 'Sold' 섹션은 추가하지 않았음, 이 부분을 다음 업데이트에 넣을 것이라는 계획과 함께 본인 링크 공유
     * 정말 멋지게 생각, 본인도 사이드 프로젝트를 20년 간 많이 런칭했지만 여전히 운영되는 건 책 사이트 하나, 17년 전 자신을 위한 도구였지만 지금은 월 800만 뷰, 실제로 런칭해서 운영 수준까지 만드는 게 가장 어렵고 대부분의 시간은 운영과 배포 이슈에 사용된다는 경험담
          + 이 사이트 덕분에 Marquez에 관심을 가지게 되었고, 정말 훌륭한 작업이라는 호평
"
"https://news.hada.io/topic?id=20940","Hyper - 웹 표준 중심의 React 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Hyper - 웹 표준 중심의 React 대체제

     * HTML, CSS, JavaScript 웹 표준 중심의 UI 언어로, 기존 프레임워크보다 간결하고 확장 가능한 인터페이스 구성을 지향
     * 리액트와 달리 로직과 스타일을 분리하며, CSS-in-JS 대신 외부 디자인 시스템 파일을 활용하는 방식으로 유지보수 용이성 확보
     * 복잡한 컴포넌트 구현 시에도 코드가 단순하고 JS 번들 크기가 작음, 예: 정렬/필터 기능 포함 테이블이 3.9KB
     * 디자인 테마 전환도 단 32줄의 CSS 변경만으로 가능, 컴포넌트 수정 없이 디자인 시스템 교체 가능
     * Bun 기반으로 작동하며 빠른 번들링, 표준 호환성, AI 모델을 위한 UI 생성 기반 등을 갖춘 미래지향적 프레임워크
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Introducing Hyper

     * Hyper는 HTML/CSS/JS 웹 표준을 기반으로 UI를 구성하는 새로운 마크업 언어임
     * 복잡한 UI도 깔끔하고 단순한 문법으로 표현 가능
     * React와 달리 표현과 로직, 스타일을 분리하여 구성

Project goals

    1. Standards first: HTML, CSS, JS 표준 기반 구성
    2. Simplicity: 복잡한 추상화 없이 간단한 컴포지션 구조
    3. Design Systems: 디자이너와 개발자 모두를 위한 분리된 디자인 계층
    4. Scalability: 애플리케이션이 커져도 단순성을 유지

React vs Hyper 비교

     * React는 로직, 구조, 스타일이 혼합된 모놀리식 구조인데 반해 Hyper는 순수 뷰 계층에 집중함
     * Simple components
          + 같은 테이블 컴포넌트를 Modern React, Old-school React, Hyper로 각각 구현한 사례 제시
               o 현대적 React: ShadCN, Material UI, Tailwind Catalyst 등 컴포넌트 라이브러리로 UI 구성, AI 툴 지원이 강점임
               o 구식 React: 스타일과 컴포넌트 코드가 분리되던 초창기 방식임
               o Hyper: 웹 표준을 준수하는 간결한 예제로, 구조와 스타일을 명확히 분리함
          + Hyper는 불필요한 클래스, 상태 훅 없이 순수 HTML 기반의 구조와 간단한 메서드만으로 표현함
          + 간단한 예에서는 차이가 미미하나, 복잡성이 커질수록 Hyper와 React의 접근법 차이가 커짐
     * Complex components
          + ShadCN 기반 React: JS 번들 91.3KB
          + Hyper: 3.9KB (1.2KB + 2.7KB)
          + Hyper는 최소한의 JS로 동작하며 유지보수가 쉬움
     * Design systems
          + Hyper로 대시보드 스타일을 변경할 때 컴포넌트 코드 변경 없이 CSS 32줄만 추가로 전체 테마 교체 가능함
          + 반면 React 기반 ShadCN의 경우 수천 줄의 TSX 코드가 테마별로 중복됨
          + Hyper의 디자인 시스템 철학
               o CSS-in-JS, Tailwind, inline style 등 디자인과 컴포넌트 간 결합을 완전히 배제
               o 모든 타이포그래피와 스타일 규칙을 외부 CSS 파일에 집중
               o 완전한 재사용성, 중앙 집중형 디자인 시스템, 제로 보일러플레이트 실현
     * Scalability
          + Hyper 방식은 프로젝트가 커져도 단순함이 유지됨
          + 구조가 단순, 코드 크기가 작음

자주 묻는 질문

     * Svelte, Vue와의 차이점?
          + 둘 다 React보다는 가볍지만 여전히 scoped CSS, Tailwind 등 디자인과 컴포넌트 결합 유도
          + Hyper는 완전히 분리된 디자인 시스템을 강제
     * What is Nue?
          + Nue는 Nue JS 템플릿 기반의 웹사이트/웹앱 생성기임
          + Hyper는 Nue JS의 차세대 진화 제품이고, 같은 모노레포 아래 관리됨
          + Hyperlink(예정)는 라우터 솔루션, 웹 표준과의 긴밀한 연결을 의미함
     * 기존 프레임워크와의 차이?
          + Hyper는 새로운 추상화 도구 추가가 아닌, 표준 회귀와 단순성 회복이 핵심 목표
          + CSS, HTML, JS 지식만으로 프로 앱 구성 가능
     * 웹 표준의 중요성?
          + 타임리스한 기술: 수십 년간 유효한 기술 기반
          + 지속 가능한 제품: 프레임워크 변경 없이 오랫동안 유지보수 가능

앞으로 계획

     * Full-stack 애플리케이션 (3개월 이내)
          + 라우팅, 컴포넌트 간 통신, DB 연동, 클라우드 배포, 디자인 테마 교체 기능 탑재 예정
     * Generative UIs (4~5개월 이내)
          + HTML/CSS 조합 기반의 AI가 생성 가능한 UI 프레임워크
          + 접근성, 반응성, 문서화 자동 포함
     * 어떻게 React를 이길 수 있나?
          + 서서히 점유율 확보를 목표로 함
          + 단계적으로 개발자 인식 변화 유도
          + 단순하고 유지보수 가능한 구조 제공
          + 기초 기술의 힘을 입증하며 성장 계획
     * 이름 중복 문제?
          + 기존에 같은 이름을 쓰는 Rust, Electron 프로젝트가 존재하지만 다른 맥락이므로 문제 없음

궁극적 목표

     * 파괴적으로 단순한 웹 스택 구축이 최종 목표임

   전형적으로 역사를 무시하고, 옛날 바퀴를 가져왔습니다.
   몇몇 아이디어는 나쁘지않은 것 같은데(마크다운 이용방법), 다른 도구들과 비교해서 크게 이점은 없는 것 같습니다.

   해커뉴스에서 논의하는 것 보면
   일단 개발자는 리액트에 대해서 이해도도 너무 낮습니다.

   머지않은 미래에 이름이 바뀔 것 같은 느낌이 드네요... 글에도 써있지만 겹치는 일렉트론 프로젝트가 있는데... 굳이 저 이름을 썼어야 했나.

   코드 비교보니 토큰이 많이 절약 될거 같군요

   svelte 최고

   svelte 최고

   사람마다 취향이 다를 것 같은데, 저는 Angular, Vue등(이 라이브러리? 마크업? 포함)의 <li for> 보다, vanilla JS로 처리하는 JSX의 .map((item) => <li>) 이게 좋더라고요

   저도 동감하는 부분입니다. HTML 에 추가되는 로직이 vanilla 가 아닌 자체 syntax 인 경우가 큰 허들입니다. 간단한 UI 구현은 문제 없지만 로직이 복잡한 경우는 개발 유연성에서 차이가 나고 학습곡선도 무시할 수 없습니다.
"
"https://news.hada.io/topic?id=20928","맞춤형 유전자 편집 치료로 치유된 첫 번째 아기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       맞춤형 유전자 편집 치료로 치유된 첫 번째 아기

     * 미국에서 맞춤형 유전자 편집 치료로 생명을 구한 첫 아기 사례가 보고됨
     * KJ라는 아기는 극히 드문 유전 질환으로 생후 일주일 만에 진단을 받음
     * 일반적으로 이 질환은 생존률이 매우 낮고 심각한 후유증을 동반함
     * 담당 의사팀은 정확한 변이에 맞는 개별 치료제를 개발해 최초로 적용함
     * 이 사례는 유전자 치료 의학 발전의 새로운 가능성을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경과 진단

     * Kyle와 Nicole Muldoon의 아기는 태어난 직후 정상적이지 않은 증상을 보여 의료진이 원인을 추정함
          + 뇌수막염이나 패혈증 등 여러 가능성을 고려함
     * 아기가 생후 일주일이 되었을 때, CPS1 결핍증이라는 희귀 유전 질환 진단을 받음
          + 이 질환은 130만 명 중 1명 꼴로 발생하는 매우 드문 질환임
          + 생존 시에도 정신적, 신체적 심각한 발달 지연과 결국 간 이식 필요성이 뒤따름
          + 환아의 절반은 첫 일주일 내에 사망함

치료 결정과 돌파구

     * Philadelphia 아동병원의 의료진은 초기에는 편안한 임종 돌봄(comfort care) 을 제안함
          + 무리한 치료 대신 삶의 질을 중시하는 접근임
     * 그러나 부모는 치료의 기회를 선택함
          + 아이에게 가능성을 부여하고자 적극적 치료 방법을 모색함

최초의 맞춤형 유전자 편집 치료

     * KJ는 세계 최초로 개인 맞춤형 유전자 편집 치료를 받은 환자가 됨
          + 환아의 정확한 유전자 변이에 특화된 치료제 주입을 받음
          + 치료는 오직 KJ만을 위해 설계 및 생산됨
     * 해당 치료 성과는 American Society of Gene & Cell Therapy 연례 학회와 New England Journal of Medicine에 동시 발표됨

의학적 발전의 의미

     * 이번 사례는 유전자 질환 치료법에 새로운 가능성을 제시함
          + 기존 치료 한계와 생존률 문제에 돌파구를 마련함
          + 개별 환자의 유전자 변이에 맞춘 맞춤형 치료 개발이 실제 환자에게 성공적으로 적용된 첫 사례로 기록됨
     * 향후 희귀 난치 질환 치료법 개발에 중요한 전환점이 될 근거로 주목받고 있음

        Hacker News 의견

     * 둘째 아들이 태어났을 때 굉장히 작았음, 어떤 유전자 검사에서 문제가 의심돼서, 소아병원에 빨리 가서 추가 검사를 받으라는 강한 권유를 받았음. 몇 주된 아기지만 검사를 잘 견뎠고, 검사 결과는 ""희귀한 유전 질환의 보인자이지만 걱정할 필요 없음""이라서 다 잊혀졌음. 여기서 세 가지 흥미로운 점이 있음. 첫째는 당시 내가 Microsoft 보험을 썼는데, 지금 와서 생각하면 엄청난 혜택이었음. 소아병원이 추가 검사를 엄청 하고 싶어 했음. 둘째, 이런 첨단 기술이 내게 가능했다는 사실에 매우 기쁘고, 기술이 점점 발전한다는 점. 셋째, 이런 기술이 더 발전하길 원하지만, 현재의 혼란이 그 불씨를 다른 누군가에게 넘기는 게 속상함
          + Microsoft에 오랫동안 다닌 가장 큰 장점 중 하나가 의료보험. 의사 접수처에서 서류 작성할 때마다 ""이 보험이요? 모든 검사를 다 할 수 있음!""이라고 하곤 했음. 요즘은 좀 줄었다고 들었지만, 정말로 최고급 보험이었음
     * 유전자 치료제가 혈액 내에서 분해되지 않도록 지방질에 싸여 간에 도달하도록 설계된 부분, 내부엔 유전자 수정을 담당하는 효소를 만들라는 세포 지시도와, 원하는 DNA 위치를 찾는 크리스퍼 GPS가 들어있다는 설명이 내가 읽어본 것 중 가장 경이로움
          + 유전자 편집의 또 다른 신나는 포인트는 실제로 GACU(T) 대신 Pseudouridine(Ψ)을 쓰면 면역 반응이 훨씬 덜 일어나는 사실. RNA→단백질 과정에는 아무 문제 없음. 이건 기적적인 발견. 2023년 노벨 생리의학상이 절대 아깝지 않음. 유전자 편집 전체 시스템도 이런 미친 발견들의 연속. 정말 끝내주는 일 https://en.wikipedia.org/wiki/Pseudouridine
          + 더 깊이 파고들고 싶으면 Jennifer Doudna에 대해 읽어보길 추천 https://en.wikipedia.org/wiki/Jennifer_Doudna
          + 나도 글을 읽으며 같은 탄성을 내질렀음. 크리스퍼에 대해 예전부터 들어왔지만, 기존 기사들은 작동 메커니즘을 그냥 흐지부지 넘어가는 경우가 많았음. 이번 연구팀이 실제로 어떻게 이걸 구현했는지에 대한 내용은 완전히 충격
          + 어떻게 GPS처럼 원하는 위치를 찾는지가 궁금. 모든 게 화학 반응과 최소한의 물리적 움직임인데, 어떻게 원하는 지점에서 원하는 변경을 하도록 프로그램하는지 알고 싶음
          + 이런 지방질에 싸여 효소 생성 지시를 준다는 부분이 mRNA 백신 원리와 상당히 유사. 똑똑한 생화학을 통해 mRNA 백신도 세포 내부로 정보를 넣고, 세포가 단백질을 생산해 면역을 유도함. 우리는 생물학에도 일종의 소프트웨어 패치를 개발한 셈
          + 내가 잘 아는 사람도 맞춤형 유전자 치료를 암 치료에 쓰고 있음. ""정확한 DNA 글자""에 도달한다는 광고 문구는 사실 좀 과장. 어디에 삽입될지 100%는 통제 불가하고, 정확성은 높지만 완벽하진 않음. 효과나 안전성에 이의를 제기하는 건 아니고, 단지 기사 설명이 마케팅 화법
          + 유전자 치료는 진짜 대단함. 어떤 치료는 여전히 대검으로 단추 구멍을 만드는 격이지만, 그 전엔 탱크 포탄으로 뚫었다고 생각하면 엄청난 진전. 대표적으로 겸상적혈구 빈혈 치료는 비정상 적혈구 원인 유전자를 꺼버리는 것. 하지만 그 유전자만 끄면 적혈구 생성을 아예 멈춰버려 죽게 됨. 그래서 태아 때만 발현되는, 산소 결합력이 훨씬 강한 ""슈퍼 적혈구"" 유전자를 다시 켜는 방법을 병행함. 성인에서 태아 적혈구의 이점이나 단점에 대한 논문은 별로 못 봤음. 임산부나 운동선수에게 유익할 수 있지만 철분 요구량은 커짐
          + 이 기사 내용을 GPT에 넣고 더 질문해 보는 것은 그야말로 내 인생 최고의 생산성 할당
          + 설명을 듣고 보니 마치 컴퓨터처럼 들림. 혹시 Turing 완전함?
          + 과학을 과소평가하지 말아야 함
          + 정말 놀라운 일
     * 아빠 입장에서 한 주밖에 안 된 아기가 곧 죽게 될 수 있다고 듣는다는 건 악몽. 이 아이를 살려준 의사, 과학자들은 현대 의학의 위대한 기념비. 이번 일이 너무 놀라움. 간 이식이 필요 없길 바라지만, 엄청난 도약
          + 우리는 제한된 자원을 가진 행성에서 살고 있음. 이런 자원을 부자들의 감정에 더 많이 쏟는 것이 모두에게 손해. 부자의 감정이 가장 중요한 시대
     * KJ 치료 사례가 수십 년의 정부 연구 지원 위에 쌓인 성과이듯, 몇 년 간의 개발과 검증 없이 맞춤 치료가 탄생함. 이런 눈앞의 성과들을 볼 때 연방정부 연구비의 실질적 가치를 더욱 느끼게 됨. 정치적 입장과 무관하게, 연방기금이 실제로 얼마나 큰 선의를 만들어내는지 평범한 사람에겐 실감하기 어려움. 전쟁 땐 일이 더 빨리 된다는 주장도 종종 봤지만, 실상은 시끄러울 뿐, 연방기금이야말로 지속적으로 진보를 이끈 원동력
          + 전문가는 아니지만, 치료제나 의약품에 FDA 승인은 반드시 필요한 것은 아님. 담당의가 비승인 치료나 오프라벨 약물 처방을 할 재량이 있지만, 의료사고 시 책임 위험이 큼. 보험사도 FDA 미승인 치료 대부분에 보상하지 않음. 사실상 FDA 승인은 해당 의약품의 마케팅 허용 여부와 더 밀접한 관련이 있음
          + DOGE 전문가팀이 몇 년 전 이 일을 처리했음. 상상해 보면 재밌음
     * NIH의 연구 자금을 정부가 계속 줄이고 있다는 점이 마음 아픔
          + 이 성과는 50년 연구의 집대성. NIH 예산 삭감과 과학자 해외 이직으로 당분간의 마지막 불꽃일 수 있음
          + 이런 돌파구를 만든 과학자와 지원 인력 커리어도 정부 보조금이 장기적으로 뒷받침했음을 잊지 말아야 함
          + 5살 아들이 진행성 유전성 근육병을 앓고 있어서, 이런 기술 개발의 속도가 곧 내 아이의 수명과 직결됨. 정부가 아무 생각 없이 NIH를 삭감하는 어리석고 가혹한 조치에 분노만이 남음. 행정부와 그 가족들 역시 NIH 덕분에 병원에서 수많은 혜택을 받으면서도 그 가치를 모른다는 점이 놀라움
          + 책임은 정부가 아니라 공화당. 공화당이 정부 연구 예산을 줄이는데 앞장서고 있음
          + 보건복지부 장관은 백신을 믿지 않고, 자신의 아이와 오물에서 수영하며 ""자연 면역""을 주장했음. 새로운 Surgeon General은 파트너십을 준비하며 별과 나무에 기도하고 환각제를 복용한 경력이 있음. 이런 집단이 자기 자신을 ""합리주의자""라 부름. 파시즘은 역사가 합리성과 과학을 거부하고, 오컬트에 경도되는 현상이 많았음. 나치가 냉정하고 이성적인 집단으로만 묘사되는 미디어의 한계도 있음
     * 이번 케이스에 대해 자세히 다룬 New England Journal of Medicine 논문 링크
       https://www.nejm.org/doi/full/10.1056/NEJMoa2504747
       기술적 풀이가 있는 에디토리얼 링크
       https://www.nejm.org/doi/full/10.1056/NEJMe2505721
          + 두 링크가 같은 것 아닌지 궁금
          + 이런 뉴스에는 반드시 원 논문이 같이 소개돼야 한다고 생각. 고마움
     * https://archive.ph/VNYzA
     * NYT에서 명확히 다루진 않았지만, 치료한 질환이 간 관련이라는 느낌을 받음. 내가 알기론, 간은 CRISPR 유전자 치료를 적용하기에 좋은 장기. 원래 간이 혈류 내 비정상 물질을 다루는 역할이라 CRISPR 같은 치료가 잘 먹힘. 간 외의 장기는 유전자 편집이 쉽지 않음. 이번 아이의 치료가 성공적이었다는 점이 매우 고무적이고, 미국에서 이런 과감한 승인이 난 것도 놀라움. 희망적이고 흥미로움
          + 맞음. 현재 CRISPR 시스템은 주로 간에 집중됨. 대부분의 CRISPR 기업들도 결국 간 치료에 초점을 맞춤. 다른 장기를 표적하는 바이러스는 CRISPR를 충분히 싣기에 너무 작고, CRISPR를 담은 지질나노입자도 간에서 쌓이고 타 장기로 보내기 어렵기 때문. 진짜 큰 도전이었음. 그래도 이 연구는 엄청난 성과. FDA도 사형선고 수준의 급박한 사례에선 동정적 사용에 좀 더 열려있음 https://statnews.com/2025/05/…
          + 결핍 효소가 간에 없어 생기는 질환
          + 바로 이 질병 https://en.wikipedia.org/wiki/…. 이 효소(CPS1)가 없으면 요소회로가 망가지고 암모니아가 쌓임. 암모니아 증가가 신경계에 매우 나쁨
          + 난 그렇게 어렵지 않을 것 같다고 생각. 혈액이 닿는 모든 세포가 mRNA 백신을 잘 받아들였음
     * 이런 상황에서 부모가 겪을 감정적 롤러코스터가 얼마나 극심할지 상상했음
"
"https://news.hada.io/topic?id=20953","NASA가 Hail Mary식 추력기 수리로 고대 우주선 Voyager 1을 살림","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             NASA가 Hail Mary식 추력기 수리로 고대 우주선 Voyager 1을 살림

     * Voyager 1의 주요 롤 추력기를 20년 만에 성공적으로 재가동함
     * 보조 추력기의 고장 위험이 커진 상황에서, NASA JPL 엔지니어 팀이 기적적으로 해결책을 찾아냄
     * 명령 전송 기회가 극히 제한적인 상황에서, 위험을 감수한 시도로 우주선의 제어권을 다시 확보함
     * Voyager 1은 거대한 거리의 한계와 전력 저하, 시스템 고장 속에서도 계속 운영 중임
     * 언젠가 임무 종료 순간이 오겠지만, 이번에도 Voyager 1은 인간이 만든 가장 먼 탐사선으로 존속을 이어감
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NASA의 Voyager 1 추력기 재가동 성공 이야기

   NASA는 거의 50년 된 Voyager 1 우주선에서 20년 넘게 고장 상태였던 주요 롤 추력기를 최근에 다시 가동하는 데 성공함

   이 작업은 JPL의 기술 진두지휘 아래 성사되었으며, 보조 추력기의 연료 라인 막힘 위험이 심각해진 시점에서 중요한 전환점이 되었음

Voyager 1만의 오랜 여정과 난제

     * Voyager 1은 1977년에 발사된 이후 미션 계획을 훨씬 넘긴 47년 넘는 활동을 이어감
     * 현재 이 우주선은 지구에서 250억 킬로미터 이상 떨어진 곳을 비행하고 있어, 지상과의 신호 교환에 약 23시간 이상이 소요됨
     * 주요 롤 추력기는 2004년 내부 히터 전원 손실로 작동이 멈췄으며, 그 이후 줄곧 보조 추력기만 사용함
     * 보조 추력기 역시 연료 찌꺼기 축적 문제로 실패 위험이 커져, 올해 가을 이전 고장 가능성이 제기되었음
     * 실패 시 우주선의 자세 제어 가능성을 상실하여, 지상과의 통신도 끊길 위험이 있었음

시도하지 않으면 확실히 끝, 시도하면 살 수도 있음

     * DSS-43 안테나의 보수 작업 때문에 올해 명령 전송 가능한 기회가 8월, 12월 등 극히 제한됨
     * Voyager 팀은 마지막 수단으로, 2004년 이후 죽은 것으로 간주했던 주요 롤 추력기 재생을 시도함
     * 추력기 히터가 실제로 망가진 게 아니라, 회로 장애로 단순히 스위치만 꺼졌을 가능성을 검토함
     * 히터 전원을 복원하고, 우주선이 가이드 스타에서 충분히 멀어지면 자동으로 추력기가 작동하도록 유도함
     * 히터가 꺼진 상태에서 추력기가 작동하면 소규모 폭발 위험도 있었으나, 약 23시간의 신호 왕복 딜레이에도 불구하고 결과적으로 히터가 재가동되어 성공 수리임이 확인됨

또 하나의 기적 같은 생존

     * JPL 팀은 “이 추력기는 확실히 죽은 줄 알았다”라고 전할 만큼, 이번 시도는 창의적인 엔지니어링과 직감의 결합이 결정적이었음
     * Voyager 임무는 최근에도 데이터 이상·과학장비 전력 저하·여러 포기 상황을 겪었으나, 담당자들의 신속한 조치로 회복을 반복함
     * 이번에도 추력기 관련 심각한 문제를 기적적으로 해결, Voyager 1은 성간 공간에서 계속 신호를 보내는 기록을 이어가게 됨

Voyager 임무의 마무리와 의미

     * 두 Voyager 호 모두, 언젠가 전력 고갈 및 시스템 한계로 인해 완전히 침묵할 예정임
     * 그러나 이번 수리로 인해 인류가 태양계를 벗어나 우주를 바라보는 창구가 더 오래 유지될 전망임

        Hacker News 의견

     * ‘It's Quieter in the Twilight’이라는 2022년 영화 소개, 관련 엔지니어를 다루는 작품 언급, 예고편과 광고 포함 무료 시청 링크 제공
          + 이 훌륭한 영화의 일부는 호주 캔버라 Deep Space Network 기지의 70미터 안테나가 몇 달 동안 중단되는 이야기가 주요 부분임을 설명, 최근 JPL 보도자료에서도 비슷한 안테나 중단 일정(2025년 5월~2026년 2월) 등장, 이 안테나는 Neptune 플라이바이 이후 황도면 남쪽으로 간 Voyager 2와 통신할 수 있는 유일한 안테나임을 언급, 스페인과 캘리포니아 DSN 기지들은 Saturn 플라이바이 후 북쪽으로 간 Voyager 1과는 통신 가능함을 지적, 영화에도 등장한 Todd Barber의 발언도 언급
          + 추천에 감사하며 전날 시청해봤는데, 미션을 이어가는 엔지니어들의 인터뷰가 인상적이었다는 경험 공유
     * 이런 고장 수정을 적용하고 오랜 시간이 지난 뒤 성공 확인 소식을 듣는 순간의 벅찬 감정을 상상, 평생 그런 벅참을 쫓을 만큼 의미 있는 성취라고 느끼는 상황 공유
          + 직접적으로 큰 임팩트는 없지만 correspondence chess(우편 장기)를 추천, 과거의 내가 천재였거나 어리석었음을 계속해서 되새길 수 있는 경험 제안
          + 이제는 수 일 간의 대기 구간에 진입했음을 언급, 신호 왕복에 하루 이상 소요, 성공의 기쁨 기대와 함께 48시간 동안 좌불안석인 기분도 표현
          + 성공적으로 작동했다는 점이 매우 좋은 결과이긴 하지만, 대안 시나리오로 JPL에서 언급한 “작은 폭발이 일어날 수도 있다”는 상황도 흥미로울 수 있다는 의견, “불과 얼음” 같은 운명에 비유하며 관련 시 링크 공유
          + 수십억 마일 떨어진 곳에서 모두가 잃어버렸다고 생각했던 우주선을 되살리는 것과 같은 경험, 기적처럼 느껴지는 상황 공유
          + Voyager 같은 미션을 교실의 아이들에게 전달해서 지식과 영감을 전하는 것도 비할 수 없이 짜릿한 일임을 상상, NASA와 교사들 모두에게 건배하는 긍정적 응원
     * 백업 롤 스러스터에 연료 잔류물이 쌓이면서 고장이 우려된다는 점을 인용하며, 이 탐사선이 겪는 고난은 참으로 인간적인 경험임을 강조
     * 1977년에 발사된 탐사선과 아직도 연락을 주고받으며 과학 임무를 수행하고 있다는 사실의 놀라움, 명령 결과를 확인하려면 23시간을 기다려야 하는 현실 언급
          + 실제로는 46시간이 소요될 수 있음을 설명, 23시간 동안 명령이 우주선에 도달하고 다시 23시간 동안 우주선의 응답이 지구로 돌아오게 됨, DSN 안테나 이용 순번 경쟁 때문에 46시간 연속 할당이 어려우면, 지연된 텔레메트리로 명령 수신 여부를 확인하는 경우도 있음
          + 23시간 기다리는 것이 본인의 마지막 해외 파트너 팀보다 빠르다는 농담을 덧붙임
     * 이번 여름 JPL에서 Deep Space Network 인턴십 오퍼를 받았으나, 대학원 졸업을 위해 포기해야 했던 상황 언급, 그 시기에 거기 있었으면 좋았을 것이라는 아쉬움, 요즘 예산 사정이 좋지 않아 다시 도전할 수 있으면 좋겠다는 희망 표명
     * 백업 롤 스러스터가 연료 라인 잔여물로 인해 올해 가을쯤 고장날 수 있다는 점 인용, 하이퍼골릭 연료 시스템에서 잔류물(SiO2, 실리카 원인)은 고무 부품 노화로 발생함을 설명, 47년 만에 스러스터 내부 연료관이 실리콘 디옥사이드로 막혔다는 부분을 자세히 언급
          + HN 사용자들이 관련 문서를 찾아냈음을 언급, 구형 우주선 연료 시스템 구조(고무 풍선과 테플론 소재 사용, 헬륨으로 연료 밀어내는 방식, N2H4 연료가 우주 시대 재료도 분해시켰다는 점)를 구체적으로 인용, 실제 문서 링크와 HN 내 토론 참고 링크 제공
     * 50년 된 기술에 물리적 접근 없이, 속도가 느리고 몇 시간의 지연이 있는 초저대역 링크로 무선 업데이트를 성공적으로 하는 것이 놀랍다는 평가, Viking Computer Command Subsystem(명령 시스템) 조사했지만 문서가 거의 없다는 허탈함 공유
          + 이런 행운만은 아님을 지적, Mars Global Surveyor 탐사선은 잘못된 업데이트로 통신이 완전히 끊긴 이야기도 공유
          + 우주에는 공기가 없으니, 무선이 아닌 ‘공허 속 업데이트’라고 부르는 게 맞지 않냐는 농담
     * 이런 순간이 NASA 로고를 볼 때마다 손에 소름 돋는 이유임을 떠올리게 됨, 단순한 과학이 아니라 인간적인 위대한 성과임을 강조, NASA 팀의 놀라운 업적에 감탄
          + 수십 년간의 인간의 호기심, 끈기, 창의성이 작은 탐사선 하나에 모여 태양계 가장자리에서 아직도 우리와 속삭이는 중임을 표현
          + NASA가 확률적으로 성공하기 어려운 특이한 성과를 여러 번 이뤄냈고, 실수로 모든 것이 폭발한 케이스는 적다는 점 강조, 예를 들어 어처구니없는 단위 변환 실수로 잃어버린 Mars Climate Orbiter 이야기와 달리, Apollo 13에서 산소를 되찾은 기적적 이야기 등을 언급, 무모해 보이는 결정이 오히려 성공으로 이어진 사례(Perseverance Rover 착륙 방법 등) 사례를 든다
          + 자신의 아이들이 SpaceX 로고를 보며 똑같이 감동받을 것이라는 경험 공유, 세대별로 다른 우주 영웅의 변화를 설명(달 착륙, 우주왕복선, SpaceX), 아이가 로켓이나 미래 기차 설계를 이야기할 때 큰 자부심과 보람 느낌, Elon Musk가 보여주는 탐구심과 원더가 강한 본보기라는 칭찬과 함께, 모든 부분에 동의하지 않더라도 자녀의 영웅으로 남기를 바람, 자녀가 “Elon Musk 로켓처럼 아빠, 나도 해볼래”라는 연결고리에 만족
          + Voyager가 곧 ‘빛의 하루’ 거리(약 1.6억 km)를 돌파한다는 사실이 SF 영화 같지만 실제 현실임을 강조, 또 하나의 소름 돋는 시점임을 언급
     * NASA가 놀라운 엔지니어링으로 위기를 넘겼다는 사실에 감탄하지만, 최근 Voyager 1, 2와 관련된 ‘NASA가 기상천외한 해킹으로 또 한 번 고장난 시스템을 극복’하는 뉴스가 점점 자주 등장함을 체감, 자연스럽게 수명 종료 시점이 다가오고 있다는 신호라고 느끼는 솔직한 심정 공유
          + NASA 예산 삭감에 직면한 상황에서, 이런 놀라운 이야기들을 자주 강조하는 경향이 있다는 의견
     * 장기간 핵심 시스템 설계 시 프로그래머들이 참고해야 할 완벽한 사례로 Voyager 1을 제시, 최신 프레임워크가 아닌 간단한 논리로 156억 마일 밖에서 제한적인 수정 가능성만으로도 임무를 이어가는 점이 인상적임을 강조
"
"https://news.hada.io/topic?id=20964","자바 30주년 - 기술을 바꾼 코드의 천재, 제임스 고슬링 인터뷰","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  자바 30주년 - 기술을 바꾼 코드의 천재, 제임스 고슬링 인터뷰

     * 제임스 고슬링은 자바의 창시자이자, 30년간 현대 컴퓨팅에 영향을 끼친 실용적 천재로 평가됨
     * 가난한 환경에서 쓰레기 더미 속 부품으로 컴퓨터를 조립하며 프로그래밍을 배웠고, 이 자기주도적 학습은 이후 언어 설계 철학에도 반영
     * Sun Microsystems에서 장난과 혁신이 공존했던 시절은 고슬링 특유의 창의성과 기술 문화 조성의 기반이 됨
     * 최근 그는 생성형 AI 도구와 AI 붐에 대해 강한 회의감을 드러냈으며, 프로그래밍 교육의 중요성은 오히려 커졌다고 강조
     * 자바의 생존 비결은 화려함이 아닌 안정성, 하위호환성, 개발자 생산성을 철저히 지향한 실용주의적 설계 철학 덕분이었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  Java at 30: The Genius Behind the Code That Changed Tech

     * 자바는 5월 23일로 30주년을 맞이하는 범용 고수준 객체지향 언어로, 오늘날에도 다양한 규모의 시스템을 구동하는 핵심 기술임
     * 자바가 존재할 수 있었던 근본에는 제임스 고슬링의 실용적 기술 감각과 창조적 직관력이 자리함
     * 고슬링은 쓰레기통에서 부품을 모아 컴퓨터를 조립하던 캐나다 출신의 자립심 강한 십대에서 출발해 세계적 프로그래머로 성장함
     * ‘한 번 작성하면 어디서나 실행된다’는 철학은 자바의 상징이며, 이는 곧 소프트웨어 개발 방식에 근본적 전환을 이끈 언어 철학으로 이어짐
     * 고슬링은 커리어 내내 기술적 탁월성과 장난기, 그리고 뚜렷한 윤리 의식을 조화시키며 현대 컴퓨팅 문화 형성에 지속적 영향을 준 개발자상을 구현함

  James Gosling: The Brilliant Mind Behind Java

     * 제임스 고슬링은 단순한 ‘자바의 아버지’가 아니라, 복잡한 개념을 직관적으로 설명할 줄 아는 겸손한 천재임
     * 자바를 만든 지 30년이 지난 지금, 그는 기술 여정을 되돌아보며 언어와 개발 문화의 진화 과정을 되짚는 시간을 가짐

  The Path To Programming: Resourceful Beginnings

     * 어린 시절 극도로 가난한 환경 속에서 고슬링은 쓰레기통에서 텔레비전을 주워 기술적 창의력을 키운 경험을 가짐
     * 첫 컴퓨터는 전화국의 폐기 릴레이 랙으로 조립했으며, 이는 이른 시기부터 드러난 기계적 감각과 조립 능력을 상징함
     * 캘거리 대학의 컴퓨터 센터를 방문하며 화면, 깜빡이는 불빛, 테이프 장치 등에 매료되어 프로그래밍에 대한 평생의 호기심이 시작됨
     * 그는 펀치카드를 뒤져 비밀번호를 얻는 방식으로 독학했으며, 고등학생 시절 대학 물리학과에서 위성 데이터 분석 프로그램을 만들어 돈을 받으며 프로그래밍을 즐긴 경험을 축적함
     * 그의 초기 프로그래밍 경험은 IBM 메인프레임의 PL/1, Fortran, PDP-8 어셈블리어, CDC 6400 코드에 걸쳐 있음. 특유의 절제된 어조로 ""여름엔 COBOL 컴파일러를 개발하는 아르바이트를 했다""고 가볍게 언급했는데, 이는 많은 노련한 프로그래머들이 감당하기 어려워하는 작업이었음

  Academia to Industry: Finding His Way

     * 고슬링은 학계를 “대학원생을 값싼 인건비로 활용하는 연구소”라고 표현하며, 이론보다 실용을 중시하는 직설적 관점을 드러냄
     * 카네기멜론 박사과정 중에도 스타트업에서 근무하며 현실적 경험을 쌓고, 이후 학위 과정을 마치는 산업과 학계를 병행한 유연한 진로 선택을 실현함
     * 첫 직장은 IBM 리서치였지만, “자기 발을 쏘는 데 헌신적인 회사”라는 평가를 남기며 기업 운영과 기술 전략에 대한 냉철한 분석 태도를 유지함
     * 이러한 초기 경험들은 이후 Sun Microsystems에서의 활동 방식에 영향을 준 현실 중심의 조직문화 이해 기반이 됨

  The Sun Days: Innovation and Pranks

     * Sun에서 가장 즐거웠던 기억으로 고슬링은 매년 진행된 대규모 만우절 장난 프로젝트를 꼽으며, 창의성과 유쾌함이 공존했던 조직문화를 회상함
     * 대표적 장난 사례로는 연못 위 플랫폼에 페라리를 띄워놓는 작업이 있었고, 이는 공학적 문제 해결 능력과 팀워크를 활용한 유머 감각을 보여주는 사례였음
     * CEO 사무실에 인공 잔디, 벙커, 워터 해저드를 갖춘 1홀 골프장을 조성한 장난은 기술과 놀이가 결합된 독창적 시도로 언급됨
     * 고슬링은 Sun을 “기술적 탁월성과 장난기 어린 창의성이 동시에 허용되는 보기 드문 환경”으로 기억하며, 이는 그의 전반적 문제 해결 방식과 기술에 대한 태도 형성의 기반이 되었음

  Java: Creating a Legacy That Changed Everything

     * 자바 30년의 여정은 고슬링에게 있어 가장 대표적인 성취이자 기술 인생의 결정적 전환점임
     * 거리에서 “자바 덕분에 커리어를 얻었다”는 인사를 받을 때마다 느끼는 개발자 생태계에 남긴 영향력에 대한 깊은 만족감이 언급됨
     * 람다와 제네릭 같은 기능은 처음부터 넣고 싶었지만, “틀린 방식으로 넣지 않겠다”는 설계 철학에 따라 도입 시점을 조절함
     * Oracle의 자바 관리에 대해서는 “기대보다는 잘했다”고 평가하며, 실제로는 커뮤니티의 지속적 참여와 기여가 핵심 역할을 했다는 점을 강조함
     * 자바는 클라우드 환경에 최적화되며 발전해왔고, 멀티코어 지원, 메모리 처리, GC 개선 등에서 “정말 굉장한 수준”이라는 기술적 완성도에 도달함

  Beyond Java: Ventures After Sun

     * Sun이 Oracle에 인수된 이후 고슬링은 잠시 휴식을 취한 뒤 Google에 입사했으나, 6개월 만에 퇴사 후 Liquid Robotics로 이직함
     * 그곳에서 자율 해양 로봇 제어 시스템을 개발했으며, 하와이에서 스노클링이 필요한 업무 환경 등 기술과 자연이 결합된 독특한 근무 조건을 경험함
     * 북극과 남극의 해양 온도를 모니터링하는 프로젝트에 참여했으나, 환경 연구는 자금이 부족해 VC 기반 스타트업 구조와의 충돌을 겪음
     * 국방 분야로의 전환 압박이 이어지자 윤리적 이유로 퇴사하고, AWS에서 Greengrass 프로젝트와 개발 도구 관련 작업에 참여하며 기술적 흥미와 윤리 기준을 함께 고려한 커리어 선택을 이어감

  On Open Source and Industry Trends: Cutting Through the Hype

     * 오픈소스는 단순한 협업 도구를 넘어서, 개발자 관계, 마케팅 전략, 바텀업 채택 모델로 작동하는 복합적 생태계로 설명됨
     * 로우코드·노코드 트렌드에 대해서는 COBOL 시절부터 반복된 주장이라며, 복잡한 도메인에 적용 시 한계를 갖는 특화형 접근 방식으로 회의적인 입장을 표명함
     * AI와 머신러닝은 기술보다 명칭이 문제라며, ‘고급 통계 기법’이란 표현이 본질에 더 부합한다는 용어 비판을 제기함
     * AI는 도구일 뿐이며 자율적 존재로 오해되어선 안 되며, 인간 노동을 위협하기보단 보조하는 고차원적 도구로 봐야 한다는 입장을 밝힘

  Developer Tools and Preferences: Embracing Progress

     * 고슬링은 NetBeans IDE를 주요 개발 도구로 사용하며, Apache 라이선스 기반의 오픈소스와 활발한 커뮤니티에 대한 지지 입장을 보임
     * 여전히 Vi나 70~80년대 도구에 집착하는 개발자들을 향해 기술 진보를 거부하는 태도에 대한 아쉬움을 드러냄
     * Vi는 어디서나 실행 가능하다는 이유로 가끔 사용하지만, 본격적인 개발 환경에선 현대적 IDE 사용을 강력히 지지함

  The JVM Vision: From Academic Concept to Global Standard

     * 자바 가상머신(JVM)의 초기 개념은 고슬링의 대학원 시절에 구상된 아키텍처 중립적 배포 포맷 실험과 명령어 번역 기술 연구에서 출발함
     * 이는 훗날 자바뿐 아니라 여러 언어가 다양한 하드웨어에서 실행될 수 있는 범용 실행 플랫폼 기술로 발전함
     * ‘Write once, run anywhere’ 철학은 처음엔 박사 논문 주제로는 수학적 기반이 부족하다는 이유로 기각되었지만, 세계 소프트웨어 개발 환경을 뒤바꾼 실용 기술로 자리잡음

  More Recent Work: Bridging IoT Gaps at AWS

     * 고슬링은 AWS에서 IoT 애플리케이션 프레임워크인 Greengrass 개발에 참여하며 복잡한 문제를 우아하게 단순화하는 기술 접근 방식을 구현함
     * OTA 업데이트, 원격 제어, 텔레메트리, 네트워크 신뢰성, 보안, 인증 관리 등 배포와 운영 사이의 반복적 보일러플레이트 작업을 추상화함
     * 디바이스 측 코드가 오픈소스로 공개되어 RISC-V 같은 Amazon 비우선 플랫폼에 대한 커뮤니티 기반 포팅 기여를 유도함
     * 이후 참여한 또 다른 개발 도구 프로젝트는 AI 붐에 휘말려 중단되며, 기술 진정성보다는 유행 중심의 혼란 속 문제점을 시사함

  AI Skepticism

     * 고슬링은 최근 인터뷰에서 AI 혁명에 대해 “대부분 사기”라는 표현을 사용하며, AI를 유독성 마케팅 용어로 규정하는 회의적 시각을 드러냄
     * 수학적으로 인상적인 기술인 것은 인정하면서도, AI라는 이름이 실제 기술적 실체인 고급 통계 기법의 본질을 흐리는 문제를 지적함
     * 벤처캐피털이 주도하는 AI 열풍은 “사기꾼과 과대광고꾼들의 집결지”라며, 실제 유용한 기술보다는 엑싯 중심의 단기 수익 추구 경향을 강하게 비판함
     * 대부분의 AI 투자금은 결국 “블랙홀로 빨려 들어갈 것”이라며, 지속가능성 없는 유행 중심 자금 흐름에 대한 경고를 제시함

  Is It a Vibe? AI Coding Tools: Impressive Demos, Limited Utility

     * 생성형 AI 코드 도구는 초기 인상은 강렬하지만, 조금만 복잡해져도 실패하는 한계적 구조를 가짐
     * 이 도구들은 기존 코드 샘플을 스크랩해 반복할 수 있을 뿐이며, 진짜로 흥미로운 문제는 항상 새롭기 때문에 복제 기반 도구와 맞지 않음
     * 전문가용 개발 환경에서는 패턴화된 코드가 라이브러리로 수렴되므로, AI의 코드 생성은 현실 개발 요구와 구조적으로 충돌함
     * 고슬링은 AI의 진짜 쓸모를 “아무도 쓰고 싶어하지 않는 문서화 작업을 대신하는 검색 도구”로 정의하며, API 사용법 설명에 특화된 보조 도구로서의 가치를 강조함

  Java’s Evolution: Language Features and Runtime Improvements

     * 최근 자바 언어의 변화 중 타입 추론과 배열 선언 방식 개선 등은 개발 편의성을 높인 유용한 기능 확장으로 평가됨
     * 그러나 고슬링은 자바의 가장 인상적인 발전은 JVM 실행 환경과 표준 라이브러리의 품질 향상에 있다고 강조함
     * 최신 JVM은 코드 품질, 스레드 성능, 가비지 컬렉션 측면에서 “놀라운 수준”에 도달한 실행 성능을 보여줌
     * 메모리 관리와 성능 예측 가능성 측면에서 malloc 기반 C 언어보다 효율적이며, GC 일시 중단 시간을 수 밀리초로 줄이는 튜닝 가능성도 언급됨
     * 지금의 JVM은 터무니없이 큰 메모리 공간도 안정적으로 처리할 수 있는 고성능 런타임 환경으로 평가됨

  Programming Languages for Critical Infrastructure

     * FAA 항공 관제 시스템을 어떤 언어로 재작성할 것인가에 대해 고슬링은 “망치를 고르며 집을 짓는 격”이라며 질문의 전제를 거부함
     * 먼저 통신 시스템, 국제 규정, 비행 경로 및 충돌 회피 등 문제 도메인의 속성을 명확히 이해한 후 기술을 선택해야 한다는 점을 강조함
     * 다만 신뢰성이 중요한 대규모 시스템에는 자바가 강력한 후보가 될 수 있다는 가능성도 부연함

  The Future of Programming in an AI World

     * AI가 발전하더라도 프로그래밍은 여전히 필수 기술이라는 점에서, 고슬링은 자녀가 있다면 무조건 코딩을 가르치겠다는 입장을 밝힘
     * AI가 인간 개발자를 대체할 것이라는 빅테크 경영진들의 주장은 노동 강도를 높이기 위한 자기방어적 협박에 불과하다고 비판함
     * 시스템을 제대로 이해하기 위해선 프로그래밍 역량이 필요하며, 기계가 대신하더라도 인간의 기술적 이해 기반은 지속돼야 한다는 주장을 전개함

  Java’s Longevity Secret

     * 자바가 30년 이상 살아남을 수 있었던 이유로 고슬링은 실제 문제 해결력, 사용자 존중, 하위호환성, 생산성 향상, 신뢰성 중심 철학을 제시함
     * 언어의 유행보다 일관된 실용성을 강조해왔으며, 스타일보다 결과에 집중한 현실 중심 설계 철학이 엔터프라이즈 환경에서 특히 효과적이었음
     * 소프트웨어는 “항상 제대로 작동해야 한다”는 관점에서, 자바는 정직하고 실용적인 엔지니어링 도구로 남아 있음

  Oracle’s Stewardship: Better Than Expected

     * 고슬링은 Sun Microsystems 인수 이후 Oracle의 자바 운영에 대해 “생각보다 훨씬 잘했다”며 예상을 뛰어넘은 성과에 놀라움을 표함
     * 초기에는 과거 행보 때문에 '강탈과 약탈'을 우려했으나, 실제로는 자바 팀을 방해하지 않고 보호한 점에서 독립성과 기술 중심 운영에 긍정적 평가를 내림
     * 재정 지원은 부족했다고 지적했지만, 기업 간섭 없이 기술팀의 자율성이 보장된 구조가 유지되었다는 점에 높은 점수를 부여함

  Crab Lovers Unite!

     * 고슬링은 함께 식사하고 싶은 사람과 일하고 싶다고 말해왔으며, 사람 중심의 협업 기준을 중시하는 태도를 보여줌
     * 기자는 샌프란시스코의 게 요리 전문점 Thanh Long에서 우연히 고슬링과 마주쳤고, 기술계 거물이 평범한 일상 속에서 발견되는 순간을 기록함
     * 이후 두 사람은 함께 게 요리를 먹으며 대화를 나누었고, 다음 만남을 같은 장소에서 하자는 약속을 통해 기술을 넘어선 인간적인 교류의 따뜻함을 전함

   저도 정적 타입 언어 중에서 제일 편하게 쓰기 좋은 언어는 자바라고 생각합니다.

   다만 범용적, 실용적인 개발 측면에서 GUI가 있는 end-user 지향의 앱을 자바로 작성하는건 별로 좋은 선택이 아니었습니다. (그런 관점에선 C# + .NET의 조합이 제일 베스트)
   자바의 장점을 고려하면 백엔드나 미들웨어 쪽에서 쓰는게 실용적인 측면에서 가장 좋은 케이스라고 생각되네요.

   아무튼 가끔 쓸 일이 생겨서 쓸 때마다 부담없이 다룰 수 있는 언어라서 좋은 경험이 더 많이 남아있는듯.

   쓰레기장에서 TV를 분해해서 프로그래밍했다는 썰은 그냥 레전드의 시작같네요.

   Java 이후로 언어들이 생산성에 주목하게 된 것이 사실이죠.

   그 전에 자주 쓰이던 C++는 지금도 읽는 것조차 끔찍합니다. 특히 오래 지속된 프로젝트를 건드릴 때요.

   자바가 개발자 생산성을 중시했다는 말은 동의하기 힘드네요
   자바만큼 ide에 깊게 의존하게 발전한 언어가 또 있나요?

   ide에 깊게 의존하는건 비이상적으로 발전한 자바 생태계의 문제지
   설계레벨의 문제가 아닙니다.

   막말로 지금 자바 개발하는데 굳이 젯브레인 제품을 사용안해도되지만
   모두들 그걸 사용하는거처럼요.

   그리고 자바가 나올 당시의 프로그래밍 랭귀지 리스트를 보면 플랫폼 종속적, 즉 os에 종속되는 구현이 많은 언어들이었고요.
   이를 같은 코드로 다양한 os에서 돌아가는 node나 python, c#과 같은 언어들의 지향점을 보여준 것이 자바였단거죠

   현대에 와서는 같은 코드로 다양한 os에서 돌아가는 호환성은 당연한 ""상식""이지만요

     막말로 지금 자바 개발하는데 굳이 젯브레인 제품을 사용안해도되지만

   이 부분은... 조금 동의하기가 힘들군요 흑흑...

   제가 경솔한 댓글을 달았군요

   지금은 좀 당연해졌지만,
   자바가 나올 당시에는 멀티플랫폼을 안정적으로 새로운 빌드 없이 지원한다는 것 만으로도 생산성에 꽤나 큰 도움이 된다! 이지 않았을까요

   자바 이전의 언어와 비교하면 생산성이 좋은 것 같기도 합니다.

   c++ > c# >= java

   C# >= Java > C++

        Hacker News 의견

     * Java 성능이 최고 수준은 아니지만 C/C++에 이어 3위 수준으로 나쁘지 않다는 인식, Go보다도 빠르고 Python이나 Ruby보다는 10배 이상 앞서는 점에 만족함, Java 문법이 완벽하진 않지만 일관되고 예측 가능한 점이 장점, Idea나 Eclipse 같은 툴을 쓰면 생산성 걱정이 없는 환경, 메모리 관리 방식이 유닉스 철학과는 다르지만 이해하면 괜찮은 절충안임을 느낌, 이런 트레이드오프를 통해 얻는 게 속도와 메모리 안전성이면서 동시에 동적 호출과 hotswap 등의 이점까지 함께 얻는 실용성에 만족
          + Java용 IntelliJ 같은 툴이 타 언어 대비 독보적으로 뛰어난 환경임을 실감, Go 커뮤니티가 동시성 자료구조 컨테이너 개발에는 별로 열정적이지 않은 이유가 궁금, Java의 동시성 코딩은 훌륭한 컨테이너를 권장하는 문화라 부럽고 가끔 java.util.concurrent나 JCTools가 그리움
          + 대학을 막 졸업한 초기엔 Java가 만능이라고 여겼으나 사실 JVM과 Java App Server 툴링이 시대를 앞서 있었던 요소였음을 나중에 깨달음, 언어 자체는 2006~2007년 생산성이 향상되기 전엔 실망감을 줬음, 요즘 JVM에서 돌아가는 JRuby, Clojure, Scala, Groovy, Kotlin 등 다른 언어에 관심, 그 중 JRuby가 성숙한 생태계를 두 개 사용 가능해서 흥미로움, Project Loom으로 JVM에서 Ruby의 Fiber를 쓸 수 있게 된 것이 양쪽 모두에 득, Charles Nutter의 업적이 저평가됨
          + Java가 Go보다 빠르다고 하지만 실제론 Go가 더 빠르거나 2~10배 적은 메모리 사용하는 경우가 많기에 비슷한 수준, Go의 value type 덕분에 최적화가 쉬움, Go를 특별히 언급하는 점이 인상적이고, C#이 Java보다 빠른 점을 들어 Java는 3위가 아니라 5위쯤이라고 판단
          + 최근 Java에 도입된 sealed class, switch expression, project Loom, records가 기존 문법에 자연스럽게 녹아든 점을 높이 평가, heap dump 분석기와 GC 분석기 등 Java의 진단 도구도 최고 수준임을 느낌
          + 언어 성능 순위는 무엇을 포함시키고 비교하느냐에 따라 달라지는 점을 지적, 제공한 벤치마크 링크를 참고
     * Java(JVM)는 한동안 좋다고 소문난 다른 언어/생태계를 써 본 후 오히려 더 높이 평가하게 된 경험, 실제론 “남의 떡이 더 커 보이는 착각”이라는 느낌이 반복, Rust만은 정말 많이 진보된 언어라고 느껴졌고 쓰는 즐거움이 있었음, 요즘 Java가 스타트업에서 ‘쿨’한 언어로 취급받지 못하는 점이 안타깝고, 생산성 격차도 거의 사라졌다고 생각
          + Rust를 두 달간 풀타임으로 써 봤는데, 적어도 서버 개발에선 Java랑 비교해서 ‘기쁨’이 있다는 표현이 이해 안됨, Rust는 lifetime 문제에 깜짝 놀라며 생산성에 저하가 생기는 순간이 너무 많고, 타입 안전성 느낌은 확실히 있지만 전체적으로 정말 즐거운 경험이라고 하긴 어려움
          + C#이 Java보다 훨씬 앞서 있고, 의미 있는 방식들(예: 훨씬 낫게 구현된 제네릭, 오래전부터 존재한 value type, 편리한 FFI)로 차별점이 큼, Unity 외엔 사람들이 별로 신경 안 쓰고, Microsoft가 옛날에 대중적 인지도 확보에 실패했다고 봄
          + 이 느낌은 프로젝트 규모가 다르기 때문이라고 생각, 보통 10년짜리 Java 레거시 대형 프로젝트에서 “새로운” hello-world 수준 프로젝트로 넘어가면 당연히 좋아 보임, 대규모 리라이트(rewrite)는 보안 검토에도 좋지만 보통 기업은 그럴 여유가 없고 Google 같은 예외만 있음
          + 똑같이 느낌, Go는 실망스러웠음, 모든 걸 약속하면서 결과적으로 Java와 비슷하거나 오히려 스택트레이스 없는 에러 등으로 더 퇴보한 느낌
          + 거의 30년 경력 중반에 JVM 아닌 언어로 프로젝트를 시도한 2년이 경력 중 최악의 시기였음
     * James Gosling의 업적에 감사함, Java World Tour를 계기로 ‘Java consultant’ 검색 1순위로 떠서 원격 근무로 시골에서 안정적으로 생계 유지한 경험, Java 덕분에 삶에 긍정적 영향을 받은 수많은 사람들 존재, Clojure 팀의 JVM 기반 훌륭한 생태계 개발 업적에도 감탄
          + 자신도 James Gosling에게 감사, 1995년 Taligent에서 C++로 일하다가 Java를 처음 써보고 새로움에 감탄, 이후 Taligent가 해체된 뒤로 Java와 관련 소프트웨어에 길게 몸담음
          + James Gosling(Java)과 Rich Hickey(Clojure)는 각각의 시대에 프로그래밍 세계에 신선함을 불어넣은 창조자라 평가
     * 최근 몇 년간 .NET/C#에서 일했지만 전체적으로 JVM/Java가 경험해본 생태계 중 최고라는 느낌, Java가 해결을 잘한 부분이 훨씬 많음, 예를 들어 Java는 fork/join pool로 작업 분할을 해결했는데 .NET은 그냥 글로벌 쓰레드 풀에 work-stealing 넣으면서 sync-over-async 코드가 전체 deadlock을 쉽게 일으키는 문제가 있음, 대형 코드베이스에서 sync 코드를 async로 전면 변환하라는 건 사실상 불가능, Java 쪽은 라이브러리/프레임워크 수준에서 실수해도 빨리 극복하는 반면, .NET은 표준 라이브러리나 언어, 런타임에서 문제가 생기면 고치기 힘듦, Java가 기준을 잘 잡은 사례가 많음
          + .NET의 thread pool starvation은 매우 짜증나며, 최근엔 영향이 줄어들었다고 들음, thread pool 오용에 면역일 수 있는 구현은 불가능하다고 생각, 할 수 있는 건 쓰레드 늘리거나 작업 순서 스마트하게 조정하기, 자신은 thread pool 전문가가 아니라서 확실하진 않음
          + .NET은 Java 생태계의 성공적인 접근 방식을 모방해온 줄 알았는데, 실상은 다른 점이 많다는 사실에 주목
          + .NET 코드를 전혀 안쓰면서 deadlock 문제를 언급하는 건 공정하지 않으며, 13년 전 블로그를 근거로 삼는 것도 설득력 없다는 지적
     * Java는 위대한 성공 사례라는 인식, 하지만 James Gosling은 시작점이었지 실질적 리더는 아님, Java 1.1~1.2 시절부터 Mark Reinhold가 주도적으로 JIT 통합, HotSpot 개발, 1.2의 대규모 클래스 증가, Oracle 인수 이후 동적 언어 지원, 오픈소싱, 빠른 릴리즈, 현대 언어 기능의 기반 등 수많은 혁신을 이끌었음, Java의 강점들은 모두 Mark Reinhold의 리더십 덕분이라고 평가
          + 주요 개발팀 전체가 인상적임, Gosling은 실용적인 언어를 원했고 그 이후 Mark Reinhold, Brian Goetz 같은 이들이 개발자 친화적으로 언어를 발전시켜옴, Oracle을 좋아하진 않지만 뛰어난 그룹을 전진시켜준 점엔 감사
          + Kotlin은 Java처럼 정적 타입의 언어로, 동적 언어 아님을 지적
          + Linus가 git을 단 2주간 해킹해 만들어 spark만 제공했다고 해도 커뮤니티가 확장한 점을 들어, 시작점만으로 평가하는 건 불완전하다는 의견
     * 40대 이상의 소프트웨어 엔지니어로서 현실적으로 ""일이 잘 되는 도구""를 고르는 게 현명하다고 판단, 오늘날엔 Java나 C#이 그 역할을 충실히 수행, 개인적으로는 C# 쪽이 더 생태계가 잘 통합된 느낌, 어떤 유즈케이스든 C#으로 1분만에 앱을 만들 수 있고, 언어 발전도 빠르고 인력 수급도 안정적, .NET도 크로스플랫폼 지원, 언어 자체의 우아함과 효율성 덕분에 일이 쉬워짐
     * 대학에서 OS 코드를 Java로 시뮬레이션 했던 경험, 추상적 알고리즘 공부엔 Java가 복잡성 줄여줘 좋을 수 있다고 이해하지만, 개인적으론 Python이 더 적합했을 것이라는 생각, 산업계 영향으로 대학 초보자 교육에서 무조건 Java를 고수하는 건 동의하기 어려움, 고등학교에선 BASIC, C를 이미 접했기에 Java로 OS 저수준 코드를 시뮬레이션하는 건 한 단계 후퇴처럼 느껴졌음
          + 대학에선 C로 마이크로컨트롤러, Java로 자료구조/OOP, C와 MIPS 어셈블리로 시스템/OS/동시성 개념을 배웠던 경험, 자료구조/알고리즘에선 Python보다 Java가 추상 타입과 구조를 명확히 구분돼 오히려 정확한 개념을 갖기에 나음, 하지만 OS 개념을 Java로 가르치는 건 조금 과하다는 느낌
          + Joel이 언급한 Java 교육의 단점들이 Python 같은 다른 고수준 언어에도 해당한다고 보고, 아이러니하게도 MapReduce(구글이 Java로 만든)이 마이크로소프트보다 앞서간 사례를 강조
     * Java의 성공을 인정하지만 여러 이유로 깊은 거부감이 남아 있음, 대부분 대기업의 장황한 코드, 복잡한 프레임워크, 품질 낮은 코드의 유산 때문, 코드가 “석탄처럼” 찍혀나오고 열정 없이 몰개성화 되는 문화가 싫었음, JVM은 내부가 블랙박스라서 strace, gdb 같은 툴로 디버깅이 어려웠고, 메모리 오버 할당으로 커널이 워크로드를 파악하기 힘들었음, JVM 사용시 전문가 도움 없으면 심각한 문제 발생 위험도 높게 느낌, 그리고 Oracle, 라이선스, JDK 버전 관리, 2025년엔 멋진 이미지가 없음, 레거시 코드가 발목을 잡는 점 등이 아쉬움, 개인적으로 Java는 최대한 피하면서도 경력을 쌓아옴, 요즘은 운영상 복잡성 덜한 정적 컴파일, 작은 실행 파일 기반의 고성능 언어가 많아서 JVM, Python VM 같은 솔루션의 역할도 점점 줄어드는 추세
          + JVM은 세계 최고 수준의 디버깅, 리프레임 재시작, 변수 변경, 예외 브레이크포인트 등 엄청난 동적 디버깅 기능 제공, Idea/Eclipse 같은 IDE의 연동도 타 언어에 비할 수 없음, JMX/JConsole, Java Flight Recorder, jstack, HPROF 등 각종 진단툴도 매우 다양, 라이선스는 오픈소스로 사용 제한 없으며, Oracle JVM 구매는 자유 선택일 뿐이라고 지적, 레거시 코드 문제가 뭔지 반문
          + JAVA가 “멋짐”이 없다는 건 설득력 없는 주장, strace/gdb가 아닌 JDK 툴과 IDE가 압도적으로 성능 좋음
          + 툴이 처음에 겉보기엔 어려워보여도 익숙해지기 쉬움, JVM의 GC 튜닝도 일주일이면 전문가로 성장, GC가 애플리케이션 범위에서 커널보다 더 나은 컨텍스트로 관리해 실제 이점이 많으며 다소 프로비저닝 복잡성은 인정
          + 20년 넘게 Java 썼지만 strace/gdb 전혀 필요 없었고, 디버깅/IDE 지원은 아주 강력, 성능에서 Python과 JVM을 같은 선상에 놓는 건 부적절
          + 실제로 Java를 잘 안써봐서 이런 판단 한 것 같고, Java의 디버깅/진단 툴이 최고임을 재확인
     * Gosling이 Sun에 있을 때 NeWS 윈도 시스템을 공동 설계했던 사실, NeWS는 Postscript 기반으로 클라이언트가 서버에 프로그램을 보내는 구조였음, Gosling이 Java를 설계할 때 NeWS가 웹페이지를 프로그래밍 가능한 형태로 보길 원했던 흔적이 보임, 실제로 저자 사인이 담긴 ""The Java Programming Language"" 책에 “Java가 NeWS의 복수냐”고 묻자, Gosling이 미소로 대답함
          + X에 Wayland, NeWS에 브라우저+JavaScript(PWA, Electron)처럼 후계자가 등장한 상황, 결국 NeWS 방식이 Microsoft 환경에서도 승리한 것 같아 Gosling의 생각이 궁금
          + 이와 유사하게 Display PostScript가 있었음, SPARCStation+SPARCprinter 조합에선 인쇄 논리가 서버에서 전부 처리, 서버나 프린터 중 하나만 고장 나도 전체 시스템 마비, 프린트 서버-프린터 연동이 악몽이 되어 결국 프린터에 대한 불신만 커졌던 경험, SunOS, SPARC 에코시스템이 그립지만 Display PostScript만은 잘 보냈음
     * 경력 상당 기간 JVM에서 코딩해옴, 최근엔 Java 대신 Scala, Clojure(선호), Kotlin 등 사용, 최근엔 실직 후 Python 일자리 제안을 받아 수락, JVM 경험에 대한 수요가 줄어드는 게 보임, 어쨌든 월급만 나오면 어떤 언어도 괜찮다는 마음, 현재 개인 프로젝트는 Scala로 진행중

   중간에 씨샵단이 숨어있군요
"
"https://news.hada.io/topic?id=21022","핀란드, 철도 네트워크를 국제 표준 궤간으로 전환 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     핀란드, 철도 네트워크를 국제 표준 궤간으로 전환 발표

     * 핀란드 정부가 유럽 표준 궤간(1,435mm) 으로 철도 궤간 변경을 추진할 계획임
     * 주요 목적은 군사 이동성, 공급망 안전성, 국경 간 연결성 등 강화임
     * 유럽연합의 TEN-T 규정에 따라 전환 계획 수립 의무가 부과됨
     * 전체 사업비 중 설계 비용의 절반과 실제 공사비의 30% 를 EU에서 지원 가능성 있음
     * 사업 착수는 빨라도 2030년대 초이며 본격 공사는 2032년경 예상임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   핀란드 교통부 장관 Lulu Ranne는 핀란드의 철도 궤간(트랙의 폭) 을 현재 사용 중인 러시아식(1,524mm)에서 유럽 표준(1,435mm) 으로 변경할 계획임을 13일 헬싱키에서 개최된 북유럽 교통 장관 비공식 회견에서 발표함

   이 철도 궤간은 19세기 러시아와 동일한 폭으로, 유럽 표준보다 89mm 더 넓음

변경 배경 및 필요성

     * 핀란드 정부는 2027년 7월까지 궤간 변경 여부를 결정할 계획임
     * 궤간 변경 목적은 공급망 안전성, 군사 이동성 향상, 그리고 스웨덴·노르웨이와의 국경 철도 연결 강화임
     * Ranne 장관은 해당 계획이 핀란드만의 문제가 아니라, 유럽 및 NATO와의 공동 프로젝트임을 강조함

추진 구체 내용

     * 궤간 조정 작업은 우선적으로 핀란드 북서부 해안도시 Oulu 북부 지역에서 시작될 예정임
     * 최근 핀란드는 노르웨이 해로 연결되는 철도 구축 투자를 검토 중에 있음. 이 과정에서 유럽 표준 궤간 전환 필요성이 더욱 부각됨
     * 2024년 여름 시행 예정인 EU TEN-T 규정은 궤간이 다른 회원국에 대해 1,435mm로 전환하는 방안 수립을 의무화함
     * 2년 전 정부에서는 비용 효율성 문제로 전환을 미루었지만, 최근 지정학적 환경 변화와 국제 압력으로 방향이 바뀜

비용 및 일정

     * Ranne 장관은 현재가 궤간 변경 시작의 적기라고 밝힘
     * 실시계획 수립 및 공사 비용을 EU가 일부 지원할 수 있음: 설계 비용의 50% , 실 치공 비용의 30% 지원 가능성 언급
     * 본격적인 사업 가시화는 최소 2030년대 초 이후, 2032년경 착공이 현실적임
     * 5년 내 단기간에 끝낼 수 없는 대규모 프로젝트임을 강조함

북유럽 교통 장관 공동 성명 및 추가 논의

     * 북유럽 장관들은 군사 이동성, 공급망 준비, 국가 교통 시스템 전략에서 국경 간 원활한 이동의 중요성을 강조하는 공동 성명을 발표함
     * 15일에는 해상 안전(발트해 지역의 ""섀도우 플릿"" 등)이 논의될 예정임
     * 2024년 북유럽 각료회의 의장은 핀란드와 올란드가 맡고 있으며, 이번 비공식 회의도 이 일정의 일환임

        Hacker News 의견

     * 핀란드 공영방송의 훨씬 더 좋은 기사에서 추가 맥락 확인 가능 링크 공유. 현재로서는 정치적 제스처와 의지만 표명한 상태라는 점이 중요. 구체적인 기술적 실행 계획은 전혀 없는 상황. 실제 건설이 시작될 것으로는 기대하지 않음. 비용 산정과 기존 철도 교통에 대한 영향이 나오면 아마 착공은 영원히 시작하지 않을 것이라는 예측
          + 1886년 미국 남부 철도들이 각기 다른 궤간을 표준궤로 단 이틀 만에 변경한 사례 언급. 수만 명의 노동자가 36시간 만에 선로의 서쪽 궤간을 동쪽으로 76mm 옮겨 전체를 거의 표준궤로 맞춘 역사적 대변혁 설명. 미리 새 궤간에 맞춰 스파이크를 박아놨고, 차량도 각 작업장 등에서 개조 진행
          + 2023년에 이미 비용 관련 연구가 진행되었고 경제성 없다는 결론 도출 보고서에서 세 가지 주요 시나리오(VE1, VE2, VE3)와 추가 대안이 설명. 비용은 100억~150억 유로 수준, 건설 기간은 15~20년 이상 할당
          + 한 가지 긍정적인 요소는 핀란드 철도 네트워크 상태가 매우 좋지 않아 대규모 개보수가 필요한 점. 궤간 변경을 통해 이왕 해야 하는 프로젝트에 추가로 EU 기금 조달 가능. 악명 높은 Suomi-rata, ELSA 프로젝트가 궤간 변경과 함께 다시 추진될 것이라 상상
          + 만약 실제로 이 프로젝트가 실현된다면, 초반 모습은 지금과 거의 같을 가능성도 존재한다는 상상
          + 핀란드는 역사적으로도 일을 해내는 능력이 과소평가되는 경향이 있음
     * 이 프로젝트에 대해 낙관할 이유가 있다고 생각. 땅은 이미 모두 매입된 상태여서 ""그냥"" 선로를 다시 깔면 되는 수준. Ballast cleaner라는 장비도 이미 존재해 선로 해체 및 조립을 동시에 할 수 있음. 선로 폭도 기계적으로 바꿀 수 있는 거대한 장비가 상상 가능 링크로 장비 설명 참고
          + Ballast cleaner만으론 부족. 침목(sleeper)도 교체해야 하므로 완전한 선로 교체용 열차 필요. 특히 분기기 및 교차점은 기존 침목만 교체로는 불가해서 전통적 공법 필요
          + 실제로는 기존 선로 옆에 추가로 새로운 선로를 건설해야 할 가능성이 큼. 주요 노선은 공간 부족 문제 발생. 고속열차 도입을 원할 가능성도 높아 기존 선로 라인은 비효율적이라는 전망
     * 이 프로젝트는 이론적으로 좋아 보이지만 현실적으론 실현 가능성 희박. 사실상 실현을 진지하게 고려하는 사람은 거의 없음. 동쪽 러시아로 연결되는 철도 노선도 손에 꼽을 만큼 적고, 필요하면 해당 구간은 파괴해 러시아의 재연결을 무력화할 수 있음. 실제로 핀란드는 유럽 철도 네트워크 측면에서 섬과 같은 존재. 표준화가 ""멋진"" 일일 수는 있으나 실제로 크게 달라질 것은 많지 않음
          + 최근 트럼프가 미국이 러시아와의 대규모 교역에 관심 있다고 발언. 관련 지역은 Konigsberg(쾨니히스베르크)와 Suwalki Gap이 후보
     * openrailwaymap에서 선로 구조 확인 가능 링크 공유
          + 범례가 너무 작아서 구별이 어렵다는 불만 및 캡처 이미지 첨부
          + 흥미로운 지도라는 평가. 스위스의 협궤 철도가 지도에 잘 안 보였지만 확대하니 모두 나타나는 점이 인상적이라는 소감
          + 지도는 주요 선로만 표시, 실제로 많은 지선이 미표기 상태
          + 흥미로운 지도이고, 스페인이 왜 유럽 본토와 다르게 궤간이 다른지 궁금. 아일랜드 등 섬에서 옛 궤간을 유지하는 건 이해 가능
     * 전략적으로는 유럽 내에서 무기 이동이 쉬워지고, 러시아의 침공시 방어가 쉬워진다는 평가. 이상적으로는 유럽 전역에 이런 변화가 필요하다는 주장
          + 유럽 대부분은 이미 동일한 궤간을 사용. 다만 적재 한계(차량이 다리 밑을 통과할 수 있는 최대 크기 등)는 표준화 안 된 곳이 있어 영국처럼 2층 열차 도입이 어려운 사례 존재
          + 러시아 방어뿐 아니라 경제적으로도 논리적. 열차가 궤간 변경 없이 유럽 구석까지 바로 이동 가능. 발트 3국도 동일한 방식 도입 예정
          + 실제 효과는 제한적. 철도 경로상으로는 노르웨이와 스웨덴을 거쳐서만 북쪽으로 진입 가능하며 스웨덴과의 연결도 단 하나뿐. 침공 시 얼마든지 파괴될 수 있으므로 실질적 차이는 미미함
          + 이런 이유로는 말도 안 되는 접근 방식이며 미친 비용 부담
          + 만약 침공이 핀란드에 도움이 됐다면 소련이 1945년에 이미 점령했겠지만 그런 일 없었음. 러시아가 핀란드를 원한다는 생각 자체가 비현실적이라는 의견
     * ""수십억 유로, 9,200km 이상의 선로, 수십 년 소요"" 기사 내용 인용 후, 이렇게 대규모로 어떻게 궤간을 교체할 수 있는지 의문. 일부 구간을 병행 운영하거나 선택 구간만 변경해 승객 환승시키는지, 다른 나라의 유사 사례가 있는지 질문
          + 스페인에서는 1992년부터 국제선 표준궤 고속철이 들어오면서 아주 천천히 전환 진행 중. 예시로 본인 지역은 아직 옛 궤간이라 마드리드에서 온 열차가 환승 과정에서 10분 정도 궤간 변경. 전체 고속열차 모델 중 단 한 모델만이 지원해서 엄청난 불편 및 정치적 갈등 유발. 모든 노선을 전환해 프랑스와 바로 연결되고 같은 모델 열차를 쓸 수 있길 희망하지만 매우 느림
          + 핀란드에서는 현재 구 선로 옆에 표준궤 전용 선로를 추가로 건설하는 방안이 유력. 기존 열차도 계속 운행 가능하게 함. 그러나 이 프로젝트가 경제적으로 타당할지 의문. 철도 네트워크가 유럽과 직접 연결되는 건 노르웨이/스웨덴만 해당되어 실질적 연결 효과는 약함. 헬싱키~탈린 터널이 생기지 않는 한 효과 미미할 것
          + 오늘날은 여러 옵션 있음. 스페인 Talgo 등 회사가 궤간 자동 변경 바퀴 특허 다수 보유, 대규모 프로젝트에 활용 가능. 링크로 상세 설명
          + 핀란드(혹은 스웨덴)에서 예전엔 좌측 운전에서 우측 운전으로 국가 전체가 한밤중에 도로 표지판을 통째로 옮긴 대규모 교통 인프라 변화 사례 존재
          + 1886년 미국 남부에서 표준궤로 궤간을 바꾼 대규모 사례. 대부분 2일 만에 완성. 오늘날에는 훨씬 더 복잡함
     * 핀란드의 표준궤 전환, 통합 및 상호운용성이 더 좋아진다는 점에서 환영. 러시아(ruzzia)가 침공할 경우 우크라이나의 사례와 달리 병참상 어려움 유발 가능. 러시아는 열악한 도로 인프라 탓에 대부분 군수 및 병력 이동을 철도로 하는 국가. 우크라이나 주요 전선은 철도 노선 쟁탈전, 철도 접근이 나쁜 곳에서는 러시아가 크게 패배한 사례
     * 에스토니아·라트비아·리투아니아 등 발트 3국도 표준궤 선로 도입 중. 궁금한 점은 어떤 연결기를 쓸지. 러시아식 자동 연결기에서 유럽식 체인~버퍼 방식으로 바꾸는 것은 오히려 후퇴라는 주장. EU가 화물연결기 표준화에 소극적이며 최신에는 디지털 자동 연결기까지 논의 중
     * EU 자금 사용 면에서 회의적 시각. 기술적으로 흥미롭지만, 큰 지정학적 전략이나 장기 계획이 없다면 비용 정당성 부족. 스페인에서는 Talgo 열차 등 궤간 자동 변경 시스템 덕분에 두 가지 궤간이 무리 없이 혼재 운영. 스페인은 세계 2위 고속철도 네트워크 보유. EU가 더 필요한 건 혁신 주도하는 소규모 기업 및 프리랜서 지원이지만 실제로는 높은 세금과 복잡한 규제로 대기업만 이득이라는 한탄
          + 실제 목표는 방위 차원, 러시아가 쉽게 병참망 연결하지 못하도록 함
     * 러시아 궤간이 유럽보다 넓어서 다행이라는 의견. 반대로 유럽 궤간이 더 넓었다면 비용 부담이 엄청났을 것이므로 핀란드의 노력은 가치 있음. 언젠가 우크라이나, 이베리아 반도도 같은 표준화를 해야 하므로 그때 노하우가 적용되길 바람
          + 실제론 어느 쪽이든 새로운 선로 건설이 필요해 궤간 방향이 실질적 비용 차이로 이어지진 않는다는 의견
"
"https://news.hada.io/topic?id=20916",""PDF를 텍스트로 변환하기"는 어려운 문제임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ""PDF를 텍스트로 변환하기""는 어려운 문제임

     * PDF 파일에서 텍스트 추출은 예상보다 훨씬 어렵고, PDF는 본질적으로 그래픽 기반 파일 포맷임
     * PDF 안의 글리프 위치 정보만 존재하고 의미론적 신호는 거의 없어 텍스트의 식별과 재구성이 까다로운 상황임
     * 검색 엔진에서는 HTML 형태의 깔끔한 입력을 요구하지만, 기존 오픈소스 도구들은 제목이나 문단 등 구조적 정보 추출에 한계가 있음
     * 머신러닝 기반 비전 방식이 가장 정확하지만, 리소스와 성능 문제로 대규모 적용에 어려움이 있음
     * 주요 개선책으로는 폰트 크기와 통계 기반의 제목·문단 식별 알고리듬을 도입해 추출 정확성을 높임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PDF에서 텍스트 추출의 도전

     * 최신 검색 엔진은 PDF 파일 포맷을 인덱싱하는 기능을 갖추게 되었음
     * PDF에서 정보 추출은 쉽지 않은 문제로, PDF가 원래 텍스트 포맷이 아닌 그래픽 포맷임에 기인함
     * 실제 텍스트 대신 글리프가 좌표에 배치된 형태라서, 회전, 겹침, 순서 뒤섞임, 의미 정보 결여 등이 발생함
     * 우리가 보통 생각하는 텍스트로서의 정보가 파일 내에는 직접 존재하지 않음
     * PDF 뷰어에서 ctrl+f로 텍스트 검색이 가능하다는 것은 사실 놀라운 일임

검색 엔진의 요구와 기본 접근법의 한계

     * 검색 엔진이 가장 선호하는 입력은 깨끗한 HTML 형식임
     * 최신 머신러닝 기반 컴퓨터 비전 모델이 가장 좋은 성능을 보이지만,
          + 대용량(수백 GB) PDF 파일을 GPU 없이 한 서버에서 처리하는 것은 비효율적임
     * 다행히 이 분야가 완전히 미지의 영역은 아니어서,
          + PDFBox의 PDFTextStripper 클래스를 활용해 출발점으로 사용 가능함
          + 하지만 제목 등 의미론적 구조 파악이 거의 되지 않음—문자열만 추출됨

제목 식별 알고리듬

  제목 식별의 기본 원리

     * 보통 제목은 세미볼드 혹은 더 굵직한 글씨가 고립되어 있음을 활용할 수 있음
          + 굵게 처리되지 않은 제목도 흔해, 이 방법만으론 한계가 있음
     * 많은 경우, 폰트 크기가 제목을 구분하는 기준이 됨
          + 그러나 폰트 크기는 문서마다 전혀 다르고, 글로벌 임계치 사용이 불가능함

  폰트 크기 통계 활용

     * 각 페이지에는 대체로 지배적인 폰트 크기(본문) 가 존재함
     * 1페이지(표지)는 서술적 내용 및 저자 정보가 있어 폰트 크기 분포가 다름
     * 페이지별 폰트 크기 분포가 다르므로, 문서 전체가 아니라 페이지 단위 통계 활용이 효과적임
     * 각 페이지의 중앙값 폰트 크기에 20% 가량 상향을 적용해 제목을 꽤 정확히 식별 가능함

  여러 줄 제목의 병합 문제

     * 스타일적 이유로 제목이 여러 줄로 나뉘기도 함
          + 제목 병합 시점 판단은 간단하지 않으며, 두 줄 이상의 제목이나 저자명, 별도의 강조 텍스트가 혼재할 수 있음
     * 병합 규칙:
          + 동일한 폰트 크기와 굵기를 갖는 연속 줄을 합치는 것이 꽤 잘 동작
          + 하지만 예외 상황 많이 존재—무분별한 병합은 엉뚱한 결과를 초래할 수 있음

문단 식별 개선

     * PDFTextStripper는 줄 간격 및 들여쓰기에 기반해 문단을 식별함
          + 라인별 구분 임계치를 고정 값으로 사용하므로, 문서마다 다른 줄 간격 적용에는 한계가 있음
          + 특히 논문 초안/사전출판본에서는 1.5~2배 줄 간격도 흔함
     * 임계값이 너무 크면, 제목이 본문에 포함되는 오류가 발생함

  통계 기반 문단 구분

     * 폰트 크기처럼, 라인 간격에 대해서도 통계처리를 적용
          + 줄 사이 거리의 중간값(중앙값)을 활용해, 어떤 줄 간격이든 견고한 문단 구분이 가능함

결론

     * PDF에서 텍스트를 추출하는 것은 근본적으로 완벽하지 못할 수밖에 없음
          + PDF 포맷 자체가 그 용도에 맞게 설계되지 않았기 때문임
     * 실제 구현에서는 타협이 필수이고, “충분히 괜찮은” 정도의 결과를 얻는 전략이 중요함
     * 검색 엔진은 제목, 요약, 주요 구조적 단서 등 의미 있는 신호 추출에 집중함이 효율적임

참고 샘플 텍스트

     * Can Education be Standardized? Evidence from Kenya (2022) - Working Paper
       : Guthrie Gray-Lobe, Anthony Keats, Michael Kremer, Isaac Mbiti, Owen W. Ozier
     * The theory of ideas and Plato’s philosophy of mathematics (2019)
       : Dembiński, B.
     * The role of phronesis in Knowledge-Based Economy (2024)
       : Anna Ceglarska, Cymbranowicz Katarzyna

        Hacker News 의견

     * 인생에서 뭔가 새롭고 흥미롭다고 생각하다가, 예전에 여러 달 또는 몇 년간 전문가가 되었던 기억이 희미하게 떠오르는 경험을 해본 적 있음. 심지어 아주 멋진 일들을 했던 순간조차 머릿속에서 다 사라진 것 같아 다시 처음부터 시작하는 느낌을 받게 됨. 6~7년 전쯤 PDF와 OCR로 굉장한 무언가를 했었다는 막연한 기억이 있음. 구글에 검색해보니 “tesseract”라는 이름이 익숙하게 들림
          + 2006년경 초창기 해킹이 가능한 e-reader인 iRex에서 다중 칼럼 과학 논문의 텍스트를 복사할 수 없는 문제에 짜증이 남. 당시 pdf 뷰어에 poppler가 사용되어서, 다중 칼럼 문서에서 읽기 순서를 추론하도록 poppler를 수정함. 이를 위해 tesseract의 저자인 Thomas Breuel의 OCR 알고리즘을 참고함. 이것은 일종의 휴리스틱 해킹이었고 접근성 API와 잘 맞지 않았음. 멀티칼럼 선택 기능이 도입되었으나 유지보수자 설득에 어려움을 겪었음. 어쨌든 이렇게 kpdf에 멀티칼럼 선택 기능이 생김. 최근에는 이런 용도엔 tesseract를 직접 쓰는 게 훨씬 합리적이라는 생각임
          + PDF라는 포맷 때문에 낭비된 인류 수십 년의 시간을 되돌릴 수 없음. 이 미친 현상은 언제 끝날지 궁금함
          + Tesseract가 한동안 최고의 오픈소스 OCR이었음. 하지만 요즘은 정확도와 GPU 가속 면에서 docTR이 더 우수하다고 생각함. docTR은 다양한 텍스트 감지 및 인식 모델을 조합할 수 있는 파이프라인 구조임. PyTorch나 TensorFlow에서 학습 및 튜닝도 가능해 특정 도메인에 훨씬 잘 맞는 성능을 끌어낼 수 있음
          + 인생이란 이런 것임. 프로젝트를 끝낼 때마다 “이제 나는 이 분야의 전문가가 되었음. 하지만 아마도 다시는 이걸 안 하겠지”라고 생각하게 됨. 왜냐하면 다음에는 완전히 새로운 주제를 다시 처음부터 시작하게 되기 때문임
          + 얼마 전 누군가 C++에 대해 질문했을 때 “진지하게 일한 적 없음”이라고 말했었음. 그러다가 약 20년 전에 Borland C++로 만든 프라이빗 인스턴트 메신저의 클라이언트 코드를 썼고, 수천 명이 사용했었다는 걸 뒤늦게 기억함. 이런 일이 종종 일어남
          + 네 머릿속을 다 알 순 없지만, 아마 정말로 tesseract였던 것 같음. 나도 비슷한 경험이 있는데, 내 경우는 12년 전쯤임
          + HQ가 유행할 때 tesseract로 자동 HQ 퀴즈 풀이기를 만든 적 있음. 앱의 질문 화면을 스크린샷으로 찍고 작은 API로 전송한 뒤, 구글에서 질문 문구를 검색해 각 답변이 나타난 횟수를 집계해서 확률별로 랭킹하는 식이었음. 정확하진 않고 단순한 방식이었지만 만드는 재미는 컸음
          + 바람에 잎이 날아가면 그냥 다른 잎을 찾는 불개미와 다를 바 없는 현상임
          + 7~8년 전 20대 때였던 일이라 아직도 기억이 생생함. 혹시 나이 차이가 좀 있는 것인지 궁금함. 아니면 건강 검진도 해보는 것 추천임
     * 브라우저 개발자 도구(‘요소 검사’)처럼 PDF의 컨텐츠 스트림—텍스트를 둘러싸는 BT…ET, 폰트와 좌표를 지정하는 각종 오퍼레이터 등—을 소스 수준으로 “보고”, 렌더링 결과와 나란히 비교·분석할 수 있는 툴이 있었으면 좋겠다는 바람이 있음. 현재 비전 모델이 PDF를 “보듯이” 처리하여 텍스트를 읽는 흐름과는 다르지만, 진짜로 PDF 내부에 무슨 정보가 포함되어 있는지 깊이 이해하고 싶다는 욕구임. 일부 툴이 있긴 하지만 오브젝트 단위까지만 보여주고, 컨텐츠 스트림 내부까지 파고들진 않음. 예시 PDF의 실제 스트림 소스와 렌더링 결과를 HTML처럼 나란히 비교·분석하면서 어떤 부분이 어떤 식으로 표현되는지 확인할 수 있는 환경을 원함
          + Mozilla의 PDF.js를 사용해 PDF를 DOM으로 렌더링하면 거의 비슷한 체험을 할 수 있을 것으로 생각함. 예컨대 Tj나 TJ와 같은 연산자가 각각 <span>이나 그들의 집합으로 변환됨. 아마 원본 문서에 충실할 필요가 있어서 그런 듯함
          + cpdf 툴을 사용해보길 권함(직접 만듦). cpdf의 -output-json 및 -output-json-parse-content-streams 옵션으로 PDF를 JSON으로 변환해 각종 실험이 가능함. 역으로 JSON에서 PDF로 되돌리는 것도 가능함. 다만 실시간 상호작용은 제공하지 않음
          + 무료 또는 오픈소스 툴을 찾고 있는 것 같지만, 예전 Acrobat Pro를 쓸 때 거의 비슷한 기능을 제공했었음. 다만 페이지를 검사하기보단 콘텐츠 트리를 순회하는 방식이었고, 오브젝트/스트림까지만 보여주면서 개별 명령까지 내려가진 않았음
          + “비전 모델로 PDF를 ‘사람처럼 본다’와 ‘실제로 PDF에 어떤 데이터가 들었는지 안다’의 결합체를 우리가 Tensorlake에서 만들고 있음(내가 거기 다님). 단순히 텍스트만 읽는 게 아니라, 표, 이미지, 수식, 손글씨 등도 진짜 이해해서 마크다운/JSON으로 데이터 추출할 수 있게 해 AI, LLM 등의 앱에 적용 가능함” https://tensorlake.ai
          + 완전히 원하는 수준은 아니지만, PDF 내부의 각종 드로잉 연산을 ‘라이브’로 보여주는 인스펙터를 제공하는 노트북이 있으니 참고해볼 것을 추천함 https://observablehq.com/@player1537/pdf-utilities
     * 수년간 Apple에서 이 문제에 집중함. 핵심은 “전부 기하학”이라는 점을 받아들이고, 단어 간격과 글자 간격을 군집분석해서 차별화하는 알고리즘임. 대부분의 PDF에선 효과적이지만 가까이 보면 종류가 너무 다양해 일부는 실망스러움. 요즘 다시 하면, OCR은 완전히 빼고 기하 정보 기반으로 접근하지만 머신러닝을 접목시킬 생각임. 미리 아는 텍스트로 PDF를 만들고 기계학습에 활용하면 학습 데이터 구축도 자동화 가능임. (WWDC 2009 Bertrand Serlet의 발표 영상 있음)
     * ‘PDF to Text’라는 하나의 문제가 아니라, 실제론 3가지 범주로 나눌 수 있다고 봄: (1) 신뢰할만한 OCR(검색, 벡터 DB 입력 등 용도), (2) 구조화된 데이터 추출(특정 값만 뽑기), (3) 문서 전체 파이프라인 자동화(예: 모기지 자동화). Marginalia는 (1)이 목적이고, 요즘 Gemini Flash 등 덕에 OCR이 저가화·범용화됨. 그러나 (2)와 (3)은 더 까다롭고, 완전 자동화에는 여전히 데이터셋 구축, 파이프라인 설계, 불확실성 탐지와 수동 개입, 파인튜닝 등 많은 인간의 노력이 필요함. 미래는 이 방향임. (LLM 문서 처리 회사 운영 중임) https://extend.ai
          + (4)로 다양한 형태 문서에서의 신뢰할 만한 OCR 및 의미 추출, 즉 접근성을 위한 솔루션도 필요하다고 봄. 이게 어려운 이유는, 일반 워크플로우와 달리 사용자 문서 종류 예측불가, 표, 헤더/푸터/주석/수식 등 텍스트 외 요소도 추출해야 함, 오류 최소화 요구되어 필요 이상으로 OCR 사용 금지, 내장 텍스트와 렌더링 내용이 불일치 가능(숨겨진 텍스트나 비정형 조합 등), 로컬 앱 위주로 구동되서 서버 활용 어려움, 임프린터 용도의 문서에 대한 Form 지원 필요 등 복합적인 난제가 산적함. 현재 완전하게 이 모든 점을 해결한 솔루션은 존재하지 않음
          + VLM을 이용해서 OCR 파이프라인을 간소화했다고 하지만, 실제 복잡한 문서에선 굉장히 어렵다는 경고임. 단순 이미지 라벨에는 뛰어나고, 아주 단순한 문서에는 쓸 만하지만, 표·헤더·정리 요약 등이 들어간 문서에서 심하게 환각(hallucinate) 현상이 나타남. 따라서 실제로는 거의 쓸 수 없는 수준임
          + Markdown으로 변환하는 와중에 헤더 탐지 등 다양한 이슈를 경험하고 있음. 요즘 OCR은 훌륭하지만, 문서 전체 구조를 유지하는 게 훨씬 더 어려움. LLM을 여러 번 통과시켜 구조를 추출하고, 페이지별로 컨텍스트를 집어넣는 방식으로 그나마 괜찮은 결과를 얻고 있음
     * 더 나은 해결책은 PDF 내부에 편집 가능한 소스 문서를 첨부하는 것임. LibreOffice로 간단하게 할 수 있음. 일반적으로 공간도 많이 차지하지 않고, 텍스트 의미를 명확히 알 수 있음. 기존 PDF 리더로도 문제없이 사용할 수 있음
          + 기존 PDF에서 텍스트 추출이 문제일 때, PDF 제작 방식에 대한 조언이 진정으로 어떤 도움이 될지 의문임. 언제쯤 이런 해결법이 본격적으로 효과가 나타날지 궁금함
          + 맞는 말이지만, 소스 문서와 렌더링 PDF의 내용이 전혀 다를 수 있는 위험이 생김
          + 맞는 의견이지만, PDF를 만든 사람과 PDF를 소비하는 사람의 이해관계가 일치해야만 유효함. 전자증거개시(e-Discovery) 분야에선, 상대방 변호사가 자료를 쓰기 어렵게 일부러 PDF로 변환해 제출하는 관행이 흔함. 그 결과, 자원이 부족한 공공변호인은 자료 처리에 더 많은 시간이 걸려 실질적 불이익을 받게 됨. 이를 방지하려면 각종 데이터는 표준 기계판독 포맷으로 강제 제출하도록 법제화해야 한다고 생각함
          + 만약 소스 문서에 접근할 수 있다면, PDF 내부에 첨부하는 게 매우 좋음. 그렇지만 대부분의 경우엔 그런 통제권이 없음
          + 진짜 문제 대부분은 레거시 PDF임. 우리 회사에도 수천 개 쌓여 있고, 일부는 엉망인 스캔본임. Adobe OCR이 내장된 것도 있지만 대다수는 아예 없음
     * 아래에 있는 PDF는 실제로는 .txt 파일임. 확장자를 .pdf로 바꿔서 PDF 뷰어로 열 수 있고, 텍스트 에디터로 직접 수정해 화면에 보일 내용, 폰트, 폰트 크기, 행간, 한 페이지 당 글자수, 행수, 종이 크기 등 다양한 부분을 컨트롤 할 수 있음. (직접 PDF 텍스트 예시 포함)
          + PDF엔 이진(binary) 스트림 내장도 가능함. PDF는 텍스트가 아니라, 레이아웃과 그래픽을 위해 만들어진 포맷임. 예시처럼 각 줄이 한 번에 나타날 수도 있지만, 실제론 글자마다, 단어마다 또는 심지어 무순서로도 표현할 수 있음
          + PDF란 “Portable Document Format”의 약자임. 7-bit ASCII 파일로서 인코딩되어 있어 다양한 기기나 OS 환경에서 높은 이식성을 자랑함. (참고: Adobe 공식 문서 링크)
          + 이 예시는 PDF의 ‘Hello World’임. 최근 PDF들은 대부분 객체(obj)를 deflate로 압축하고, 그 객체들을 스트림 내부에 그룹화시켜 더 복잡해짐. 그래서 단순 텍스트로 ""6 0 Obj""를 검색하면서 분석하기가 매우 어려워짐
     * PDF에서 텍스트, 특히 구조화된 텍스트를 뽑는 것은 결코 쉬운 일이 아님. HTML 테이블은 대체로 그나마 간단하게 추출할 수 있는데, PDF는 렌더링된 좌표 기반으로만 표처럼 보일 뿐 실제론 텍스트와 그래픽이 흩어진 채로 들어있음. 나 역시 Poppler PDF utils로 PDF를 HTML로 바꾼 뒤, 테이블 헤더를 찾고, 각 값의 x좌표를 기준으로 열을 파악해 행별 데이터를 추출하는 방법으로 사용하고 있음. 보기에는 투박하지만 정렬된 txt로 하는 것보단 신뢰할 수 있는 결과임
          + 웹페이지와 BeautifulSoup처럼 PDF에서 데이터를 추출하지 못하는 게 불만이라서 그런 인터페이스를 가진 라이브러리를 직접 만듦. ('page.find' 형식 예시) PDF별로 케이스가 지옥처럼 다양해서, 추출 노하우와 괴랄한 PDF 예시들을 모아서 라이브러리로 만들고 있음 https://jsoma.github.io/natural-pdf/ , https://badpdfs.com/
          + 언젠가 PDF에서 표 데이터를 내 데이터 처리 소프트웨어로 뽑아내고 싶음. C++ 앱에 연동할 수 있으면서 무료이거나 아주 저렴한 라이브러리를 아는 분 제보 바람
          + (국가기관처럼) 출력 가능한 텍스트와 실제 추출되는 텍스트가 전혀 다른 문서들이 있음. 이런 케이스가 종종 있음
          + PDF는 본질적으로 마크업/XML 포맷임. 같은 PDF도 정말 수많은 방식으로 만들 수 있음. 그래픽 툴에서 내보내면 그래픽과 텍스트가 혼합된 PDF, 워드프로세서에서 내보내면 텍스트 중심 PDF 등 다차원적인 결과가 나옴. 제작 앱이 정보를 어떻게 처리하느냐가 PDF 출력 방식에 큰 영향을 줌. 오프더셸프 유틸리티로는 cisdem 등 다양한 제품군이 어느 정도 구조화 데이터를 뽑는 데 쓸 만함. 하지만 작업별로 적합한 도구가 필요함
     * 내가 좋아하는 사례 중 하나는, 이 논문 PDF임(링크 첨부). 첫 페이지에 전형적인 2단 텍스트와 중앙 헤더, 양 칼럼 사이에 끼어드는 텍스트, 줄 길이와 들여쓰기를 바꾸는 요소까지 다양함. 페이지 헤더도 홀/짝수에 따라 다르고, 섹션 헤더 규칙도 제각각임. 단락도 항상 들여쓰지 않고 줄 간격에 변동이 있음. 다양한 문제의 총집합임
          + MacOS의 CoreGraphics API는 PDF의 텍스트를 페이지 단위로, 딕셔너리에 encoding된 순서 그대로 제공함. 95% 경우엔 이 방식으로 꽤 잘 동작했고, PDFKit과 Preview에서도 수년간 큰 문제가 없었음. 2단 텍스트도 보통 원래 워드프로세서의 버퍼 순서와 일치하게 PDF에 들어갔으므로 내용 흐름이 올바름. 단, 헤더/푸터 등은 내부 앱마다 완전히 다르게 처리돼서 예측 불가임
     * 직접 간단한 PDF 파서를 만들어봤을 때, 포맷의 방식에 깜짝 놀란 적 있음. 그래서 오히려 왜 이 포맷이 텍스트 중심 용도로 많이 쓰이는지 늘 의아하게 생각함. 예를 들어 인보이스라면, 디지털 시스템이 데이터 추출은 쉽고, 사람에게는 포매팅 처리된 방식이 적합해야 함. 그래서 기술계가 더 나은 포맷으로 점진적 마이그레이션 했으면 하는 바람이 있음
          + 예전엔 XML+XSLT가 이런 역할에 거의 근접했었는데, 불행히도 브라우저들이 이제는 로컬 XML 파일용으로 더 이상 지원하지 않음. 원격 서버를 통한 XML만 지원함
     * PDF에서 ‘유용한 정보’ 추출이 바로 Tensorlake의 업무임(https://tensorlake.ai). PDF는 텍스트뿐만 아니라 표, 이미지, 수식, 손글씨, 취소선 등 다양한 정보를 담고 있기에, 개발자로서 우리는 텍스트를 “읽기”만이 아니라 “이해”할 수 있어야 함. (직원임을 밝힘)
"
"https://news.hada.io/topic?id=20948","Qtap - 암호화된 네트웍 트래픽을 캡처하는 eBPF 에이전트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Qtap - 암호화된 네트웍 트래픽을 캡처하는 eBPF 에이전트

     * eBPF를 사용하여 리눅스 커널을 통과하는 트래픽을 암호화 전/후로 캡처하는 도구
     * TLS/SSL 함수에 후킹하여 전통적인 패킷 캡처 방식보다 트래픽 컨텍스트(프로세스, 컨테이너, 호스트, 사용자, 프로토콜 등) 를 더 풍부하게 수집함
     * 응용프로그램 수정이나 프록시 구축, 인증서 관리 없이 원본 네트워크 데이터 및 프로세스 정보를 파악 가능
          + 보안 감사, 네트워크 디버깅, API 개발, 서드파티 통합 문제 해결, 프로토콜 학습 및 분석, 레거시 시스템 분석 등 다양한 활용 가능
     * 낮은 오버헤드로 실제 트래픽을 실시간으로 터미널에서 확인 가능
     * 커스텀 플러그인 개발/연동이 쉬워, 기존 관측 시스템과 손쉽게 통합되거나 새로운 솔루션의 기반으로 활용
     * 현재 초기 개발 단계로, AGPLv3 오픈소스와 상용 라이센스를 동시 제공함

   해킹 외에 어떤 활용처가 있을까 궁금합니다.

   요즘은 BPF만 보면 SKT 해킹 사건 생각이 나네요
   한국인터넷진흥원(KISA), SKT 해킹 확인중 악성코드 변종들 8종 확인

   그래서 BPFDoor 악성코드 점검 가이드 배포도 했더군요.

   최근 SKT 뉴스를 보니 25종이 추가로 또 발견되어서 전체 37종 이라는군요.
   ""관리 안한 컴퓨터에 깔린 바이러스 숫자보다도 많다"" 라는 댓글이
"
"https://news.hada.io/topic?id=20999","Exa - 임베딩 기반 검색 엔진","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Exa - 임베딩 기반 검색 엔진

     ""The web as a database""

     * 키워드 기반 검색의 한계를 넘기 위한 임베딩 기반 검색 엔진으로 사용자의 의도에 정확히 맞는 결과만 반환
     * 검색어 의미에 따라 웹 전체에서 벡터 임베딩 검색 → LLM 기반 검증 → 정제된 결과 제공이라는 다단계 프로세스로 작동
     * 일반적인 리스트가 아니라 테이블 형식으로 결과를 제공, 원하는 컬럼을 추가해 비동기로 부가 정보를 수집 가능
          + 각 셀은 비동기로 개별 로딩하며, 웹을 데이터베이스처럼 탐색 가능하게 설계됨
     * 예를 들어 ""2021~2025년에 설립된 샌프란시스코 소재 오픈소스 개발툴 스타트업""처럼 구체적이고 복잡한 조건도 정확히 찾아줌
     * 각 결과는 LLM이 실제로 검색 조건에 맞는지 평가 하며, 해당 조건에 부합함을 증명하는 근거(출처) 와 함께 제공
          + 처리 시간이 몇 분에서 몇 시간까지 걸릴 수 있음
     * 검색 예: 미국 외에 있는 수학 교사가 만든 수학 블로그
     * 데모 영상: https://youtu.be/Unt8hJmCxd4
     * API 문서: https://docs.exa.ai/websets
"
"https://news.hada.io/topic?id=20943","LLM 에이전트 루프와 도구 사용의 비합리적 효율성","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      LLM 에이전트 루프와 도구 사용의 비합리적 효율성

     * AI 프로그래밍 도우미인 Sketch의 개발 경험을 통해 LLM과 도구 사용을 결합한 루프 구조의 간결한 구현을 강조함
     * 단 9줄짜리 루프 코드만으로 Claude 3.7 Sonnet 모델 등 최신 LLM이 실질적인 문제를 신속히 해결함
     * bash 등 범용 도구 하나만 있어도, 개발자의 반복적이고 까다로운 작업을 상당 부분 자동화할 수 있음
     * 문제 해결뿐 아니라, 추가적인 도구를 연결해 텍스트 편집이나 특화된 작업의 품질과 반복 속도를 높일 수 있음
     * 점점 더 많은 맞춤형 LLM 에이전트 루프가 개발자의 일상 자동화에 도입되는 추세임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 개발 경험과 Sketch 프로젝트

     * 필립 젤리거 및 동료들은 AI 기반 프로그래밍 보조 도구 Sketch를 개발하는 과정에서, LLM과 도구 활용을 결합한 단순한 에이전트 루프 구조의 높은 효율성에 놀라움을 느낌
     * 핵심 구조는 단 9줄짜리 루프 코드로, 시스템 프롬프트와 대화 기록, 최신 메시지를 LLM API에 전달함
     * LLM이 출력을 생성하고 필요한 경우 tool_calls(도구 호출 요청)를 반환함

LLM과 도구 사용의 통합

     * ""도구 사용(tool use)""이란 LLM이 미리 정의된 스키마에 맞는 출력을 반환하고, 시스템 프롬프트와 도구 설명 프롬프트를 통해 LLM이 bash와 같은 범용 도구에 접근할 수 있게 함
     * 최신 LLM(예: Claude 3.7 Sonnet)은 단일 범용 도구만으로도 다양한 문제를 빠르게 자동화하며, 일부는 한 번의 실행(""one shot"")으로도 해결 가능함
     * 이전에는 복잡한 git 명령어를 찾아 붙여넣고 수동으로 병합 작업을 했으나, 이제는 Sketch에 요청해 바로 해결 가능함
     * 타입 변경 후 발생하는 다수의 타입 체크 오류도 Sketch가 처음으로 자동 처리해줌
     * 에이전트 루프는 지속적이며 적응적으로 작동해, 도구 미설치 시 자동 설치·명령어 옵션 차이에도 맞춤 대응함
     * 사용 중 LLM이 테스트 실패 시 ""테스트 건너뛰기""처럼 예기치 않은 제안할 때도 있지만, 전체적으로 업무 자동화의 질이 향상됨

도구의 다양화와 특화

     * Sketch는 bash 외에도 추가적인 도구들(c.f. 텍스트 편집 도구)을 활용하면, 작업 품질을 높이고 개발 워크플로우가 더욱 효율적임을 경험함
     * LLM이 sed 등으로 텍스트를 정확히 수정하는 일은 예상보다 까다로우며, 시각적(visual) 에디터 방식의 도구가 더 뛰어남을 체감함

미래 전망과 워크플로우 변화

     * 에이전트 루프 구조는 기존의 범용 자동화 도구로 다루기 힘들었던, 개발자 일상의 반복 작업에 점점 더 많이 활용될 전망임
     * 예시로, 스택 트레이스와 git 커밋 상관관계 분석처럼 번거롭고 반복적인 작업도 LLM이 빠르게 1차 처리를 수행함
     * 앞으로 더 많은 맞춤 제작, 일회성 LLM 에이전트 루프가 개발자의 bin/ 디렉토리 등에서 사용될 것으로 기대할 수 있음
     * 사용자들은 원하는 bearer token만 준비해 자신의 환경에서 쉽게 실험할 수 있음

참고 링크

     * 내용은 philz.dev 블로그에도 게재됨
     * 프로젝트 및 관련 정보는 sketch.dev, merde.ai, pi.dev에서 추가 확인 가능

        Hacker News 의견

     * 이 블로그 글도 강력하게 추천하고 싶음. 같은 논점을 훨씬 더 자세하고 설득력 있게 다루는 버전임. 글쓴이가 실제로 직접 코딩 에이전트를 처음부터 만들어봄. https://ampcode.com/how-to-build-an-agent 참고. LLM이 툴을 호출할 수 있는 루프에서 얼마나 다양한 작업을 잘 처리하는지 정말 놀라운 경험임. 물론 완벽하지는 않고, 신뢰성 100%에 도달하지 못하는 문제 등이 있긴 함. 그래도 적어도 조금은 경탄할 만한 부분이 있다고 생각됨. 이런 걸 직접 해보면 30분 안에 따라 할 수 있음. AI가 특정 용도에 효과적인지에 대한 건전한 의심을 놓치지 않으면서도 경이로움을 느낄 수 있는 부분임. LLM을 루프에 두는 이 ""비정상적인 효과""가 지금 수많은 코드 생성 에이전트가 쏟아지는 이유임: Claude Code, Windsurf, Cursor, Cline, Copilot, Aider, Codex 등등, 게다가 따라 만드는 프로젝트도
       아주 많음. 특별한 비법이 없는 이유이고, 마법의 95%는 결국 LLM 자체와 툴 호출을 하도록 파인튜닝된 것임. Claude Code의 주요 개발자가 최근 인터뷰에서 솔직하게 인정한 사실임. 물론 이런 도구들이 잘 작동하게 하려면 많은 노력이 들어가지만, 근본적으로 거의 같은 단순한 코어 구조임
          + 이런 글을 무척 오랫동안 찾고 있었는데 정말 고마움이라는 느낌임. 많은 사람들이 Agents를 보고 ""아마 정말 복잡한 문제는 잘 못 풀겠지?""라고 반응하지만, 내가 보기엔 에이전트의 핵심 논점은 거기에 있지 않음. LLM은 많은 컨텍스트 안에서 정말 잘 작동하고, 에이전트는 LLM이 더 많은 컨텍스트를 찾아서 질문에 대한 답변 품질을 증진하는 구조임
          + LLM이 혼자서 루프 안에서 몇 번 이상 지시 없이 잘할 수 있는 작업이 별로 생각나지 않는 경험임. 몇 번 반복하다보면 꼭 내가 개입해야 할 상황이 생김
          + pocketflow라는 그래프 추상화 라이브러리를 사용해서 비슷한 걸 만든 튜토리얼이 있음. 실제로 써봤는데 상당히 심플해서 아주 만족했음. https://github.com/The-Pocket/PocketFlow-Tutorial-Cursor/…
          + Thorsten Ball 글쓴이임을 이제야 알아차림. 그의 ""interpreter 만들기"" 정말 재밌게 읽었던 기억임. 아마 나도 이제 에이전트 만들어볼 계획임
          + 위 링크를 열기 전에 ?utm_source=hn&utm_medium=browser를 추가해야 할지 고민임
     * 오늘 처음으로 GPT-4o와 4.1로 직접 ""vibe-coding""을 시도해봤음. 수작업으로 컴파일 에러와 경고, 제안을 캔버스 인터페이스를 통해 루프 돌리며 입력하는 방식이었음. 파일은 150줄 정도로 작았음. 4o로 시작했는데, 구식 패키지를 사용. 내가 그 부분을 지적해도 모든 용도를 업데이트하지 않고, 직접 손봐야 했음. 논리 변경을 제안하니 완전히 문법이 망가지는 상황이 생겼고, 다시는 회복 못했음. 계속 컴파일 에러를 입력해도 문법 문제를 이해하지 못하고 코드의 랜덤 부분만 고쳐버림. 그래서 4.1이 더 잘할까 기대했지만, 4.1은 캔버스 사용 자체를 거부하고 그냥 설명만 해줌. 직접 수정하라는 스타일임. 계속 설득해서 겨우 캔버스에 코드를 뽑았더니 이번에는 중간이 ""// omitted for brevity"" 같이 잘린 버전이라 쓸 수 없었음. 여기서 포기함. 에이전트들이 이 문제를
       해결해주는지 궁금함. 현재로선 이 경험이 완전히 망가져 있다는 생각이고, 그런 상태에서 bash 접근권을 주는 건 너무 위험하다는 우려임
          + ""구식 패키지를 썼다""는 점은 모델의 학습 데이터 시점이 잘려있기 때문임. LLM 쓸 때 꼭 주의해야 하는 부분임. ChatGPT에서 o4-mini-high로 많이 전환해서 쓰고 있음. 이 모델은 서치 기능으로 최신 문서도 찾을 수 있음. ""X 라이브러리 최신 버전 조회해서 써달라"" 요청하면 종종 잘 처리함. 최근, 자바스크립트 코드 일부를 최신 Google 권장 라이브러리로 변환하는 작업이 있었는데, 직접 이전 코드를 붙여넣고 새 라이브러리로 포팅해달라고 요청하니 문서도 찾아보고 정확히 포팅해 주었음. https://simonwillison.net/2025/Apr/…
          + GPT 4.1과 4o가 Aider 코딩 벤치마크에서 점수가 매우 낮게 나옴. 실사용 경험상 70% 이상은 되어야 결과물이 쓸모있게 나옴. 그래도 복잡한 건 상당한 보조가 필요함. 무엇이 잘되는지, 안되는지 쓰다보면 감이 생김. https://aider.chat/docs/leaderboards/
          + ""실력 문제""라는 말이 듣기 싫을 수 있지만, LLM을 쓰는 건 확실히 별도의 숙련이 필요한 분야임. 다양한 도구의 장단점을 이해하고, 여러 기법을 실험하고, 연습이 필수 구성임. 만약 bash 접근을 준다면 나도 컨테이너 환경(docker)에서만 쓸 예정임
          + 그건 vibe coding이라 할 수 없음. 코드 변경이 자동 반영되는 도구를 써야 vibe coding이 가능함. 수동으로 하나씩 피드백 주다보면 오류에 막혀 멈추게 됨. vibe coding의 핵심은 되돌리기, 재실행, 다양한 해법을 쉽게 던지고 버릴 수 있다는 점임. 이런 경험을 하려면 툴링이 필요함
          + 얼마 전 Cline 플러그인과 Claude로 VSCode에서 Android 앱 프로토타입을 ""제로베이스""부터 만들었음. Android Studio가 주는 기본 템플릿에서 시작했고, 수천 줄짜리 코드를 생성했는데 단 한 줄도 컴파일 에러가 없었음. 앱은 기대한 대로 동작했고, 발견된 몇 개의 버그도 LLM 탓이 아니라 기괴한 Android API 동작 때문이었음. LLM에게 버그를 지적하고, 디버그 메시지 출력을 제공하니 직접 고쳐주었음. 처음엔 수정이 좀 엉성했지만, 몇 번 피드백 주고받으니 괜찮게 해결됨. 코드 작성 에이전트와 코드 리뷰 에이전트를 루프에 돌리면 이런 부분도 더 일반적으로 잘 다룰 수 있을 것 같음
     * Claude Code, 즉 Sonnet 3.7의 터미널 인터페이스를 출시 첫날부터 써왔음. 상당한 규모의 CLI 앱, 풀스택 웹 시스템, 수많은 유틸리티도 작성해봄. 예전 프로그래밍 팀 리드할 때와 비슷하게 더욱 야심찬 도전을 하게 됨. 구조적으로는 다른 툴들과 별반 다르지 않겠지만, Anthropic이 엄청 유용한 기능을 다수 추가한 느낌임. 완벽한 건 없고, 좋은 코드 퀄리티는 지금도 그때처럼 비슷한 노력이 필요함. 꽤 복잡한 것들도 작동은 되는데, 다음 기능 추가가 어려워지는 상황도 종종 있음. 다루는 법이 익숙해지니 리팩토링, 보완 과정이 훨씬 줄었음. 완전히 없어지진 않을 구조임. kgeist가 겪은 문제는 솔직히 상상하기 힘듦. Claude도 때로는 내 선택과 다르거나 멍청한 동작을 하지만 포기하고 싶을 정도는 한번도 아니었음. 거의 항상 괜찮은 결과를 보여주고, 많은 작업을
       뇌에서 덜어주는 정도가 엄청남. 게다가 리팩토링을 매우 훌륭하게 해냄. 주기적으로 코드를 보면서, 더 좋을 방법을 Claude에게 설명시키면 복잡함이 확 줄어듦. ""데이터 구조 변경"" 같은 요청도 바로 해결됨. 굉장히 멋진 기능임. 그리고 재미로 코드가 아닌 잡동사니가 쌓인 30년짜리 아카이브 디렉토리를 열어봤음. ""이 디렉토리에 뭐가 있지?"", ""옛 이력서 읽고 새로 써줘"", ""내 아이들 이름 뭔지?"" 등등 물어봐도 정말 놀라움. 아직 초기 단계임에도 정말 행복함
          + 최근 원격 데이터 구조 정의, API 명세, 파싱 및 저장 구현, 사용자에게 보여주기까지 모두 한번에 처리해야 하는 상황이 있었음. Claude가 이 모든 작업을 동시에 잘 관리해주어, 양쪽 끝의 작은 변경이 중간 계층에 어떤 영향을 주는지 즉각적으로 볼 수 있었음. 여러 아이디어를 빠르게 반복하면서 최적의 해법을 찾아감. 이처럼 복잡도가 높은 여러 계층을 빠른 반복 속도로 탐색해볼 수 있다는 점이 놀라웠고, 생산성 향상과 동시에 전체 시스템의 구조적 이해도를 높여줌
          + 위에 언급된 리팩토링이 정말 즐거운 작업임. 예전엔 스프린트에 포함시키기도 힘들던 기능들도 5분 만에 끝남. 마치 준비된 팀이 언제든 내 요구만 기다리는 느낌임. 결과가 마음에 안 들면 바로 거절해도 되고, 불필요한 검토와 일정 잡기 걱정도 모두 사라진 듯함
     * ""아, 이 테스트 안되네... 그냥 건너뛰자""라고 LLM이 종종 말하는 상황이 너무 답답함. 여기 생각해볼 아이디어가 있음. 메인 LLM이 지시대로 동작하도록 독립적이고 병렬적인 폴리시-강제 LLM을 같이 띄우는 방식임. 예를 들어, 보조 LLM이 ""let's just"" 다음에 ""skip""이란 단어는 나오지 못하게 출력 확률을 조작함. 즉, ""skip""을 금지하면 LLM이 원하지 않는 행동에서 벗어나게끔 경로를 바꿀 수 있음. 일종의 JSON 모드나 구조화된 출력처럼 동작하되, 동적으로 실시간으로 보조 LLM이 정책을 제어하는 방식임. 이게 잘 된다면, 더 발전시켜서 테스트 통과를 위해 테스트 코드를 삭제한다든지 쓸모없는 주석 출력 등 다양한 정책 위반을 모두 보조 LLM 쪽 프롬프트에 넣고, 보조 LLM이 실시간으로 감시하고 제어하는 구조로 확장 가능함. Outlines 팀이 이런 구조에 대해 어떻게
       생각할지 궁금함
          + 만약 LLM 하나가 다른 LLM의 출력을 체크할 수 있다면, ""mixture of experts"" LLM이 한 전문 알고리즘을 감시·감사하는 역할에 할당할 수도 있지 않을까 하는 의문임. 또는 별도의 사고 쓰레드를 분리해서 자기 자신의 출력을 검증하게 하거나, 또 필요하다면 그 검증자도 검증하는 또다른 쓰레드를 두는 식으로 체계를 더욱 단단히 할 수 있을 것임
          + 이런 맥락에서, 메인 LLM이 잘못된 방향으로 가면 감시 LLM이 해당 시점까지 모델을 ""rewind""해서, 예를 들어 ""let's just skip the test""를 감지하면 ""just "" 이후로 롤백한 뒤 특정 단어들이 안 써지도록 bias를 계속 걸어주는 구조도 생각할 수 있음. 이런 작업을 하려면 사용하는 모델 제공자가 제한적이 될 수 있고, 특히 OpenAI는 최근 이런 파워유저 기능에 적대적인 경향이 보이고 있음
     * 오늘 아침 cursor로 게임 프로토타입의 메인 루프에서 복잡한 일부를 추출하고, 그 파트를 위한 테스트 코드 세트를 자동 생성함. 전체 341개의 테스트가 core math 및 핵심 컴포넌트 전체를 커버함. 때론 캣헤딩 같은 느낌이지만, 구체적으로 사용할 함수, 위치, 템플릿 파일, 피해야 할 것 등 더 많은 제약을 줄수록 결과가 점점 좋아짐. 총 3500줄의 테스트 코드를 내가 직접 쓸 필요 없이, 문제 발생 시 언제든 지우고 다시 생성할 수 있음. 난이도 곡선 튜닝, 미션 변주 등 정말 다양한 부분도 도움 받음
          + 내 경험상, 테스트 자동 생성이 LLM의 최고 활용 사례임. 지루하고 반복적인 노동이 몇 시간 또는 며칠이 들어가는 부분을 한 번에 없애줌. 내가 떠올릴 수 없는 많은 엣지 케이스도 자동 커버되고, 코드 안전성까지 증가함. 정말 다방면에서 훌륭한 기능임
     * 요즘 LLM의 툴 사용 능력에 굉장히 흥분되는 느낌임. 사실 이 트릭은 새로운 게 아니고, 2년 전 ReAcT 논문에서 처음 접함. 이후 ChatGPT 플러그인, MCP 등에서 활용됐고, 이제 대부분 모델이 툴콜/함수 호출을 염두에 두고 학습됨. 현재 흥미로운 점은 성능이 얼마나 좋아졌냐는 부분임. o3/o4-mini의 뛰어난 검색 성능도 툴콜 능력이 바탕임. Qwen3 4B (Ollama 2.6GB, 맥에서도 잘 돌아감)도 이 기능을 잘 수행함. 어제 PyCon US에서 LLM 기반 소프트웨어를 만드는 워크샵을 진행했고, 그 계기로 내 LLM 커맨드라인 툴에 툴 사용 기능을 추가함. https://building-with-llms-pycon-2025.readthedocs.io/en/latest/… 참고. 이제 내 LLM 패키지로 ""딸기(strawberry)에서 R이 몇 번 나오는지 세기"" 같은 걸 셸 원라이너로 안정적으로 처리할 수 있음
          + 이 기능이 주는 유쾌함과 강력함의 이상한 조합이 너무 좋음
          + 워크샵이 녹화됐는지 궁금함
     * 어떤 agent가 토큰을 가장 많이 사용하는지 궁금증임. cline이 리스트 상위권이고, roo는 cline보다 덜 사용하는 것 같음. 상호작용 방식을 직접 설정할 수 있는 agent가 있는지, Claude code가 다른 agent와 비교해 어떤지 알고 싶음
     * ""필요한 툴이 없으면 설치한다""는 점이 무서운 포인트임. LLM이 지나치게 '순종적'이고, 누가 시키기만 하면 바로 실행하는 구조임. 이건 SQL injection보다 더 심각한 보안 우려임
          + 언제 첫 agent 기반 재앙이 터질지 종종 궁금함. (특히 급하게 쏟아져 나오는 AI 시장 분위기라서) 시간이 지나면 반드시 돌이키기 힘든 재앙이 일어날까봐 우려임
     * 제목이 Eugene Wigner의 논문 ""The Unreasonable Effectiveness of Mathematics in the Natural Sciences""에서 착안한 것 같음
          + 이 논문이 원조일 수 있지만, 이미 오래 전부터 밈이 된 관용어라고 봄. https://scholar.google.com/scholar?q=unreasonable+effectiveness
          + 난 오히려 Karpathy의 2015년 ""Unreasonable Effectiveness of RNNs""에서 제목을 따온 줄 알았음. 물론 Karpathy 역시 Wigner 논문에서 재차 차용했을 것이란 추정도 가능함. https://karpathy.github.io/2015/05/21/rnn-effectiveness/
          + 매번 ""unreasonable effectiveness""란 헤드라인을 보면 저자의 결론에 항상 강하게 동의 못하는 경험이 있음. Wigner의 논문 포함해서. 이제 일종의 Betteridge 법칙같은 느낌임
     * 우리는 자사 제품 내 임베디드 ai 챗봇에 더 많은 컨텍스트를 주기 위해 도구를 구축함. 최근 활동 로그, 현재 객체 정의, 검색 및 헬프 아티클 탐색 등 여러 기능을 추가함. 몇 달이 지난 지금도 챗봇 품질이 여전히 놀라움. 만약 챗봇이 뭔가 잘못 응답하면, 관련 헬프 아티클을 더 구체적으로 업데이트하는 프로세스임
"
"https://news.hada.io/topic?id=20986","OpenAI는 ChatGPT의 메모리 기능을 대폭 확장한다고 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  OpenAI는 ChatGPT의 메모리 기능을 대폭 확장한다고 발표

   다음은 Ars Technica 기사 「ChatGPT can now remember and reference all your previous chats」(2025년 4월 11일자)을 바탕으로 작성된, 한국의 개발자 및 기술 전문가 독자를 위한 구조화된 한국어 요약입니다. 기술적 변화의 내용, 사용자 맞춤성 향상, 프라이버시 우려 등을 중심으로 정리했습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  1. 변화의 배경: 기존 기억 기능의 한계

   OpenAI는 기존에도 “Memory” 기능을 통해 일부 사용자 지정 정보를 저장하고 대화에 반영할 수 있도록 했습니다. 하지만 이 기능은 다음과 같은 한계가 있었습니다:
     * 저장 정보는 소수의 핵심 사실로 제한됨
     * 사용자가 “기억해줘”라고 직접 지시해야만 저장
     * 저장되면 화면에 “기억이 업데이트됨”이라는 알림이 뜸
     * 임시 채팅(Temporary Chat) 기능을 사용하면 메모리는 비활성화됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  2. 새로운 기능: 전체 대화 기록 기반 기억

   2025년 4월, OpenAI는 ChatGPT의 메모리 기능을 대폭 확장한다고 발표했습니다.
     * 두 가지 선택 옵션 등장:
          + 기존 Memory: 제한된 사용자 정보만 저장
          + 신규 Chat History Memory: 전체 이전 대화 내용을 참고하여 응답 품질과 사용자 맞춤도 향상
     * 기억된 내용은 직접 열람하거나 편집 불가 (블랙박스 구조)
     * 설정에서 완전히 비활성화할 수 있음 (기존 방식 유지)

     요약: 이전 대화의 “맥락” 자체를 응답에 반영하는 능력이 추가됨. 더 이상 단편적인 기억이 아닌, 지속적이고 누적적인 사용자 맞춤이 가능해짐.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  3. 기능 적용 범위와 출시 일정

     * 적용 대상: 2025년 4월부터 ChatGPT Plus 및 Pro 사용자
     * 점진적 롤아웃: 일부 국가 제외 (EU, 영국, 아이슬란드, 노르웨이, 리히텐슈타인, 스위스)
     * 향후 적용 예정: Enterprise, Team, Edu 사용자
     * 무료 사용자 대상 적용 계획은 아직 없음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  4. 기술적·철학적 시사점

    ✔ 기대되는 효과

     * 사용자 성향, 문체, 선호도에 더 맞는 답변 가능
     * 장기간 대화 기반의 개인화된 에이전트로 진화

    ✔ 주요 우려

     * 투명성 부족: 어떤 정보를 기억했는지 확인/수정 불가
     * 프라이버시 침해 가능성: 서버에 저장된 기록이 의도치 않게 활용될 가능성

     이전에도 대화 기록은 서버에 저장되었으나, 이번 업데이트부터 실제 응답 생성에 본격적으로 활용된다는 점에서 의미가 큼
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  5. 요약 정리

    항목                    내용
   변경 내용 전체 대화 이력을 기반으로 응답을 조정하는 새로운 기억 기능 도입
   적용 대상 Plus/Pro 사용자부터 순차 도입, 유럽권 제외
   차이점   기존은 정보 편집 가능, 신기능은 완전 자동/불가시
   기대 효과 맞춤형 응답 품질 향상, 대화 맥락 유지
   우려 사항 사용자 제어 불가, 프라이버시 위험 증가
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  6. 마무리

   이번 변화는 LLM의 개인화 방향에 있어 중대한 전환점입니다. 단순한 기억 기능을 넘어, 지속적으로 학습하고 적응하는 “개인 비서형” AI로의 진화를 의미합니다. 그러나 블랙박스화된 기억 구조는 프라이버시 및 제어권 상실이라는 기술윤리적 논쟁도 함께 수반합니다. 기술 수용 여부는 결국 사용자의 선택에 달려 있으며, 기업의 투명성과 책임감 있는 설계가 더 중요해질 것입니다.

   이게 토큰 소비량에 미치는 효과가 궁금하네요. 입력 컨텍스트 앞에 memory내용이 누적된다면 Cache hit가 되기도 하겠지만 좀 더 투명한 설명이 아쉽습니다.

   방금 메모리를 있는 그대로 프린트해줘 라고 시도해봤는데, 기존 메모리 내용(사용자 요청에 의한 기억)이 가장 먼저 나열(Model Set Context)되고, 그 다음에 사용자가 선호하는 응답의 특징에 대한 묘사(Assistance Response Preference)가 상황별로 나열되고, 마지막으로 과거 초팅에서 주목할 만한 내용(Notable Past Conversation Topic Highlights)들이 요약되어 첨부되는 것 같습니다. 말 그대로 모든 채팅 내용의 토큰 이 다 첨부되는 것은 아닌 것 같습니다.

   아래는 챗지피티의 설명

   영구 메모리(Persistent Memory)에는 =bio 명령어를 통해 명시적으로 저장된 정보만 포함되며, 해당 정보만이 세션 간에 지속적으로 기억됩니다.

   한편, 응답 선호도나 대화 주제 요약 등은 ""임시 컨텍스트(Ephemeral Context)""에 해당하며, 이는 최근의 상호작용을 기반으로 자동 생성됩니다. 임시 컨텍스트는 세션 중에만 확인 가능하며, 세션이 종료되면 저장되거나 이후에 참조되지 않습니다.
"
"https://news.hada.io/topic?id=20925","HelloCSV - 프론트엔드만으로 동작하는 현대적인 CSV 업로더","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 HelloCSV - 프론트엔드만으로 동작하는 현대적인 CSV 업로더

     * 요즘 웹앱에 많이 사용하는 CSV 파일 업로드 과정에서 발생하는 오류나 사용자 불편을 최소화하기 위한 프론트엔드 전용 CSV 임포터 라이브러리
          + 데이터를 업로드 전에 미리 확인하고 수정 가능
          + 전화번호 등 포맷 자동 변환기를 지원하면서도, 변경 내용을 사용자에게 미리 보여줌
     * 사용자가 직접 컬럼 매핑, 데이터 확인 및 업로드 전 검증까지 수행할 수 있는 4단계 UI 프로세스를 제공
     * 커스텀 컬럼, 유효성 검사, 변환 로직을 개발자가 자유롭게 설정할 수 있으며, 특정 프레임워크에 종속되지 않음
     * CDN으로도 간편하게 임포트 가능하고 설치용 npm 패키지도 제공
     * MIT 라이센스
     * B2B SaaS, 회사 내부용 툴, 관리자용 대시보드 등에 유용

   커스텀 번역, 엑셀 파일 로더, 가상 리스트 등을 기여했습니다. 한국에서도 많이들 사용하시길 바랍니다. ^^

   좋네요
"
"https://news.hada.io/topic?id=20929","애플 앱스토어, 외부 결제 사용하는 앱에 "크리티컬 경고" 메시지 보이기 시작","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              애플 앱스토어, 외부 결제 사용하는 앱에 ""크리티컬 경고"" 메시지 보이기 시작

     * 애플이 외부 결제 시스템을 사용하는 앱에 대해 강력한 경고 메시지를 도입
     * 이 경고는 앱의 이름, 아이콘 위에 빨간색 ⚠️ 아이콘과 함께 최대 다섯 줄로 표시됨
     * EU에서는 미국과 달리 IAP 없이 외부 결제만으로 운영 가능함
     * 외부 결제가 오히려 카드 환불 등 소비자 보호 측면에서 안전하다는 의견 존재
     * 개발자들은 애플의 정책이 시장과 창작자 모두에 불리하다는 불만을 표출함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 최근 애플이 앱스토어에서 외부 결제 시스템을 사용하는 앱에 대해 강한 경고를 표시하기 시작함
     * 해당 경고는 ""이 앱은 앱스토어의 사적이고 안전한 결제 시스템을 지원하지 않고, 외부 결제를 사용함""이라는 메시지로 드러남
     * 경고 문구는 앱의 이름과 아이콘 윗부분에 다섯 줄에 걸쳐 눈에 띄게 배치됨

앱스토어 경고의 세부 사항

     * 일부 사용자와 개발자가 애플의 경고 메시지와 실제 화면을 공유하며 논란이 커짐
     * 앱스토어에서 외부 결제를 사용하는 경우, GET 버튼 옆에도 '외부 결제' 로 표시가 추가됨
     * 미국에서는 외부 결제 기능이 반드시 앱 내 결제(IAP) 버전과 함께 제공되어야 하지만, EU에서는 그런 요구가 없음
     * 이로 인해 유럽 내 앱들은 IAP 없이 외부 결제만으로 운영 가능함

경고문 수준 비교

     * macOS의 알림 수준은 총 세 단계로 분류됨:
          + 알림(informational) : 일반적인 소식·준비 알림
          + 경고(warning) : 더 심각한 상황에 대한 안내
          + 치명적(critical, 주의 삼각형 아이콘) : 데이터 손실 등 위험에 진짜 주의가 필요한 경우 사용
     * 외부 결제 앱에 대해 애플이 실제로 어떤 아이콘(치명적 등)을 사용하는지 지적하며, 이는 사용자에게 심리적 압박을 줄 수 있음

경고의 실제 효과와 규제 배경

     * 실제로 이 경고를 몇 명의 사용자가 보게 될지는 불확실함
          + 앱을 이미 구매한 이용자는, 앱이 외부 결제를 추가해도 굳이 다시 앱스토어를 방문할 필요가 없기 때문임
     * DMA(디지털시장법) 등 규제는 결제 시점에 심각한 공포 유발 화면(Scare Screen)을 금지함
     * 이에 애플의 경고 방식이 규제에 저촉될 위험이 언급됨

신뢰성, 소비자 보호, 실질적 논점

     * 과거에는 소프트웨어가 물리적 매체로 판매됐던 시기가 있었으나, 온라인 결제가 일반화된 현재에는 거의 모든 온라인 결제 방식이 사적이고 안전하다는 현실이 강조됨
     * 애플의 IAP 시스템이 여러 측면에서 장점이 있는 건 사실이나, 프라이버시와 보안 측면에서 더 이상 독보적인 제공점이 아님
     * 실질적으로 외부 결제가 카드사 환불 등으로 오히려 소비자 보호에 더 유리할 수 있음
          + 애플은 제품에 문제가 있어도 환불 처리가 매우 까다로움
          + 직접 결제 시스템을 이용하면 카드사 등 외부 채널로도 환불 가능함
          + IAP로 결제하면 카드사 환불 시 애플 계정 전체가 정지될 위험 있음

개발자와 커뮤니티 반응

     * 오랫동안 자체 웹사이트에서 판매해 온 개발자들은 앱스토어가 외부 결제의 신뢰성을 부정하는 접근에 반발함
     * 앱스토어 등장 이전에도, 수많은 개발자들이 웹 기반으로 소프트웨어 판매를 성공적으로 운영했음
     * 애플의 정책 변화와 제한, 그리고 시장에 대한 임의적 개입이 개발자들의 비즈니스 지속 가능성을 위협한다는 비판이 제기됨
     * 일부 개발자는 애플의 대응을 수동적 공격성이라며 비꼬기도 함
     * 애플 경영진에 대한 신뢰 하락, 규제 우회 전략, 제품 품질 저하에 대한 커뮤니티의 실망감 확산

추가 참고

     * 논란에 대한 다양한 커뮤니티·언론 반응:
          + Hacker News, Reddit, The Verge, MacRumors, 9to5Mac, The Mac Observer 등에서 활발히 논의됨
     * EU, 외부 iOS 결제, 디자인 등 관련 태그와 카테고리에서 이슈가 확대되고 있음

   경고성 알람의 신뢰가 떨어지면 정말 중요한 경고가 뜨더라도 무시하고 넘어가게 될 수도 있겠다는 생각이 드네요. 알람의 신뢰가 떨어져서 발생하는 문제는 결국 소비자 피해만 늘릴 듯

   우리는 이 게임^을 해봤어요!

   ^ ActiveX의 무지성 ”예” 클릭 유도 학습

   애플 끝까지 진짜 추하네요 ㅋㅋㅋ

   https://sdmntprwestus2.oaiusercontent.com/files/…

        Hacker News 의견

     * Apple이 왜 이런 경고 메시지를 띄우는지 궁금함, 혹시 감정적인 이유가 있는 것인지 의문, 결과적으로 오히려 사람들이 그 경고를 보면 ""이 앱이 더 저렴하구나""라고 생각하게 될 것임, 사람들은 경고 메시지에 너무 익숙해서 그냥 무시하게 되는 상황
          + 실제로 Apple이 이 문제에 대해 잘 알려진 감정적인 이유를 가지고 있음
          + 이제는 실질적으로 Apple에게 존재의 이유에 가까운 문제임, AB 테스트를 통해 수익이 즉각적으로 크게 변하는 것을 봤기 때문에 이런 조치를 계속 취하는 것
     * 은행 앱에도 저런 경고가 뜰지 궁금함, 결국 본질은 같다고 생각함, Tim Cook이 물러나고 Schiller 같은 인물이 리더십을 잡는 것이 좋겠다는 생각
     * Apple의 경고 메시지 스크린샷을 공유함: https://nitter.net/pic/orig/media%2FGqxWbH8WgAAPx8L.jpg
          + 솔직히 저 경고가 그렇게 심각해 보이지 않음, 개인적으로는 이런 메시지에 야자수나 커피 아이콘을 쓸 수 있겠다는 생각, 하지만 Apple이 외부 결제를 허용한 사실이 굉장히 충격적임, 마치 지옥에 얼음이 어는 것처럼 느껴짐
          + 참고로 위의 링크는 더 이상 작동하지 않음, 경고 메시지가 어떻게 생겼는지 궁금하다면 이 기사를 참고하기 바람: https://theverge.com/news/667484/…
     * 아일랜드용 예시 앱 링크를 공유함: https://apps.apple.com/ie/app/alprelax/id6479374216, https://apps.apple.com/ie/app/contribee/id6466801641, https://apps.apple.com/ie/app/…
     * 냉소적인 시각은 제쳐두고, Apple이 여기서 신뢰 문제를 신중하게 해결해야 한다고 생각함, 많은 비기술 사용자들은 Apple의 생태계에서 특별한 결제 흐름에 익숙해져 있음, 새로운 게임을 다운로드하고, 결제 시 Apple Wallet을 연결하고, 14일 이내면 환불 요청을 간편하게 할 수 있음, 14일이 지나도 설정에서 구독을 손쉽게 취소할 수 있음, 이 모든 과정의 대가로 Apple은 개발자에게 30% 수수료를 부과함(사실을 설명하는 것임), 이 흐름은 3자 결제 시스템과 호환되지 않음, 특히 비기술 사용자들은 단순히 결제 시스템으로 인식하지만, 외부 결제 시 구독 내역이 반영되지 않아 환불도 어렵고 취소하려면 개별 앱 고객센터와 소통해야 함, 게다가 Apple과 유사한 화면으로 결제 유도하는 회사들도 많아질 것임, 뉴스를 잘 챙겨보는 사람들에게는 좋겠지만, 그렇지 못한
       사람들에게는 Apple을 탓하거나 익숙한 경험과 달라 곤란해질 수 있음, Apple이 3자 결제 자체를 금지하자는 의견도 아니고, 굳이 무서운 경고창이 필요도 없다고 생각함, 예전처럼 아무 생각 없이 원하는 앱을 다운로드할 수 있어야 신뢰가 회복될 것임, 많은 사람들에게는 급격한 학습 곡선이 예상됨, 참고로 구독 취소 흐름: https://support.apple.com/en-us/118428
     * EU가 Apple을 더 수치스럽게 만들고, 추가적인 벌금으로 위협하기를 바람, Eddy Cue가 이 모든 일의 책임자라는 이야기를 들었음, Eddy Cue는 매우 영리한 협상가로 알려져 있지만 App Store는 개발자뿐만 아니라 사용자에게도 오랫동안 문제점이 많았음, Tim Cook이 적임자를 내버려두고 오히려 더 나은 결과를 낼 수 있는 인재(예: Scott Forstall)는 내보낸 것이 아쉽다는 생각임
          + Eddy Cue가 책임자라는 것은 사실과 다름, 실상은 재무 부사장 Alex Roman이 진실을 숨기려고 법정에서 거짓 증언을 했고, 내부적으로 Phillip Schiller는 Apple이 법원 명령을 준수하라고 권했지만, Tim Cook은 Schiller를 무시하고 Luca Maestri와 그의 재무팀을 따랐음: https://storage.courtlistener.com/recap/gov.uscourts.cand.364265/…
     * 미국 정부가 독점 금지법을 집행하기 전까지 이런 상황은 멈추지 않을 것임, 그나마 대안은 EU 법률이 이런 독점 사업자들을 직접 겨냥하는 것임
          + EU 법률이 오히려 소규모 개발자들에게는 해가 되고 있음, 따라야 할 기준이 너무 까다로워서 따르기 힘들게 됨, Apple이 DMA에 최소한만 대응하면서 오히려 피해를 최대화한 것이 대표적임
     * Amazon 앱은 오직 외부 결제만 사용하는데도 경고가 안 뜨는 이유가 궁금함, Apple 기준으로는 오로지 디지털 상품에만 위험하다고 판단한다는 것인지 답답함
          + 반대로 생각해보면, Amazon은 자체 결제 시스템에 신뢰가 가능하다고 볼 수 있음, 반면에 랜덤 앱들은 어느 결제 시스템을 썼는지 알 수 있는 근거가 없음, 앱 내에서 공식적인 URL 바 같은 것도 확인 불가함
     * setapp 같은 서비스가 iOS 앱을 등록해서 개별 구독이 필요 없는 환경이 생기면 참 좋겠다는 생각임, 구독 피로감이나 인앱 광고 문제도 해결되고 특히 어린이 앱에 매우 유용할 것임
          + setapp은 2020년부터 다양한 iOS 앱을 지원하고 있음
     * Apple이 Apple 결제와 비Apple 결제 모두를 포함한 앱에서도 이러한 경고 메시지를 띄우는지 궁금함, 그렇다면 Apple 결제를 10배 높은 가격으로 지원하는 게 합리적일 수 있다고 생각함
"
"https://news.hada.io/topic?id=21000","Windows Subsystem for Linux(WSL), 이제 오픈소스로 전환","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Windows Subsystem for Linux(WSL), 이제 오픈소스로 전환

     * Microsoft는 WSL 전체를 오픈소스로 전환했다고 발표했으며, 이는 Microsoft/WSL 저장소의 첫 번째 이슈였던 “오픈소스가 될까요?”에 대한 응답이기도 함
     * GitHub의 Microsoft/WSL에서 소스를 내려받고 직접 빌드하거나 기능을 추가하고 버그를 수정할 수 있음
     * 공개된 코드는 커맨드라인 도구, 서비스, Linux용 데몬, Plan9 기반 파일 공유 서버까지 포함함
     * WSL은 Windows 실행부와 리눅스 가상머신(VM) 내에서 동작하는 여러 컴포넌트로 구성됨
          + CLI 도구: wsl.exe, wslconfig.exe, wslg.exe
          + WSL 서비스: VM 부팅, 배포판 실행, 파일 공유 등을 담당하는 wslservice.exe
          + Linux 데몬: init, gns, localhost 등 네트워크 및 포트 포워딩 기능 수행
          + Plan9 서버: Windows와 Linux 간 파일 공유 역할
     * 기존에 오픈소스로 공개된 구성요소
          + WSLg: Wayland 및 X 서버를 지원하는 그래픽 환경 관련 구성요소
          + WSL2-Linux-Kernel: 리눅스 커널 소스
     * 아직 오픈되지 않은 구성요소
          + Lxcore.sys: WSL1의 핵심 드라이버
          + P9rdr.sys, p9np.dll: Windows에서 \\wsl.localhost 경로를 지원하는 파일 리다이렉션 시스템

오픈소스 전환 배경과 WSL의 역사

     * WSL은 2016년 BUILD에서 처음 발표되어 Windows 10 Anniversary Update에 포함됨
     * WSL1은 Windows 커널 내에서 Linux syscalls를 처리하는 lxcore.sys 기반의 구조였음
     * WSL2는 2019년 처음 발표되었고, 실제 리눅스 커널을 활용해 호환성과 기능을 개선함
     * 이후 GPU 지원, GUI 앱 실행(wslg), systemd 지원 등 기능이 추가되며 성장함
     * 2021년부터는 Windows에서 분리된 독립 패키지로 Microsoft Store를 통해 제공됨
          + 첫 릴리스는 0.47.1 (프리뷰), 이후 2022년 1.0.0에서 Windows 10까지 지원 확대
     * Windows 11 24H2부터는 기존 내장 WSL에서 새로운 패키지 기반 WSL로 전환됨
          + wsl.exe는 그대로 남겨 사용자 전환을 지원함

최신 버전 및 기능

     * 최신 릴리스는 WSL 2.5.7, 4년간 약 9페이지 분량의 GitHub 릴리스를 통해 개선되어 옴
     * 주요 개선점에는 미러 네트워킹, DNS 터널링, Session 0, 프록시/방화벽 지원 등이 포함됨

커뮤니티의 기여

     * 수년간 커뮤니티는 버그 리포팅, 기능 제안, 비공식 분석 등으로 WSL 개선에 기여해 옴
     * 소스 코드 공개 이전에도 실질적인 기여가 활발했고, 이제는 직접적인 코드 기여가 가능해짐
     * Microsoft는 이 커뮤니티의 지원에 감사하며, 앞으로도 WSL의 발전에 더 큰 시너지를 기대하고 있음

기여 방법

     * 소스 구조나 기능 구현이 궁금하거나 개선하고 싶은 점이 있다면
          + microsoft/WSL 저장소에서 참여 가능함
          + 직접 빌드, PR 제출, 이슈 리포팅 등 다양한 방식으로 기여 가능함

        Hacker News 의견

     * 나는 Hacker News에서 WSL에 대한 칭찬 댓글을 남길 때마다 카르마 세금을 내는 기분을 느끼는 입장임. WSL은 하나의 컴퓨터에서 여러 리눅스 버전을 동시에 아주 쉽게 구동할 수 있기 때문에, Linux보다 더욱 강력하다는 인상임. docker와 같은 장치 지원, 로컬 스토리지, 네트워크 맵핑 등 특수 스크립트 없이도, 데스크탑이나 노트북에서 추가 설정 없이 바로 사용 가능한 점이 정말 큰 매력이라고 생각함. 예를 들어 한 프로젝트는 Ubuntu22, 다른 프로젝트는 Ubuntu24가 필요해도, 운영체제 업데이트 걱정에 매달릴 필요가 없는 즐거움임
          + ""Linux보다 더 강력하다""는 말은 과장이며, 사실 WSL은 가상머신임. WSL의 주요 강점은 각종 편의 기능 자동화 제공임. 하지만 진짜로 더 편리한 환경은 아예 가상머신이 필요 없는 환경이라고 주장함. Distrobox와 toolbx 같은 툴도 비슷한 기능을 제공하고, NixOS에서도 간편하게 일반 Linux 환경 테스트가 가능함. 여기에 하드웨어 가속, 그래픽 앱이 바로 동작함, 느린 9p 브리지 문제가 없으며, VM의 메모리 버블같은 이슈가 발생하지 않음. Windows 사용자를 위해 WSL이 혁신적이긴 하지만, Linux 유저는 그런 VM이 필요 없어 그렇다는 의견임
          + Linux에서도 Distrobox로 같은 걸 구현할 수 있지만, Windows + WSL을 같이 운용할 수 있다는 점이 확실히 매력임을 인정함. 만약 Microsoft가 불필요한 소프트웨어와 광고, Copilot, 과도한 원격 측정 등을 줄인 Dev 에디션을 출시하고, macbook 같은 하드웨어를 제공한다면, Apple에서 떠나는 개발자를 충분히 다시 유치할 수 있을 것이라 생각함. 개인적으로 Mac과 Linux 환경을 왔다갔다 하면서, 사용성 면에서는 Windows + WSL 조합이 더 선호되는 면이 있다고 밝힘. PowerToys, WSL, PowerShell, 그리고 PowerShell + Winget DSC를 통한 PC 셋업 자동화 등이 정말 뛰어나지만 Windows의 사용자 불친절함과 지나치게 긴 업데이트 시간은 도저히 참을 수 없다는 고충을 설명함. macOS처럼 불변(immutable) 베이스에 이미지 기반 업데이트를 도입한다면 더 좋겠다는 제안임. M4 Pro 랩탑급 성능이 없는 점도
            Windows 쪽이 아쉬운 이유로 들었음
          + ""WSL이 Linux보다 더 강력하다""는 주장은 카르마가 깎일 만한 발언이라고 생각함. WSL도 좋고, 매일 사용하지만, 지원되는 하드웨어에서 리눅스를 실행하면 그 경험이 더 뛰어난 것이 사실임. VM이 네이티브만큼 좋을 수 없듯, 반대로 Windows 소프트웨어도 Windows에서 더 잘 동작하는 것과 같음. 비교했을 때 WSL은 입출력이 느리고, 그래픽도 지연 및 오류가 있고, 때로는 크래시, 비효율적인 메모리 관리, 네트워크 이상 등 다양한 문제가 발생함을 경험함. 매우 강력한 컴퓨터에서 CLI 위주로 쓴다면 WSL이 편할 수 있지만, 실제로는 사람마다 필요한 환경에 따라 다름을 강조함
          + WSL은 사실 Linux임. 특히 WSL2부터는 아예 VM구조로 들어갔고, WSL1은 윈도우 커널에서 구동되는 방식이어서 멋졌다 평가함. 다만 NTFS 파일시스템이 느리고, Windows 자체를 다뤄야 한다는 점이 크게 불만임. 카르마 숫자는 그냥 숫자라고 여기니 신경 쓰지 않는다는 코멘트
          + 내게는 WSL이 꽤 불안정하고, 컴퓨터가 슬립에서 깨어날 때마다 VM과 호스트 간 네트워크 문제로 재시작이 필수적임. Windows 사용자 디렉토리에서 작업하면 파일시스템 드라이버의 느린 속도 때문에 git 명령이 몇 초씩 걸려서, 결국 두 개의 홈 디렉토리를 관리하게 되는 번거로움이 생김. 셋업 과정에서도 복잡한 DNS, VPN, 네트워크 우선순위 오류, 시간 동기화 불일치 등 해결해야 할 비밀스러운 문제들이 많았음. 결국 매번 Windows를 재부팅하는 일상이 되어버림. 나는 여러 운영체제를 쓸 필요가 없고, 회사에서도 대부분 도구가 Linux VM에서 돌아가며, 그런 방식이 유일하게 합리적임. 겉 운영체제는 오히려 문제만 만들고, 두 운영체제의 상호작용 때문에 필요 이상으로 복잡한 작업을 요구한다는 견해임
     * 나는 WSL이 처음 출시됐을 때 굉장히 기뻤음. 게임과 개발을 하나의 Windows PC에서 통합해 쓰자는 꿈이 현실이 되는 느낌이었음. 하지만 시간이 지날수록, 패키지 설치 문제, OS 간 경계 이슈 등 여러 자잘한 문제가 늘어나서 점점 과정이 거칠어졌음. Valve의 Proton과 최신 Linux 게임 지원이 향상되면서 Ubuntu와 NixOS로 완전히 전환함. 이제 게임 쪽에서 약간 불편함이 있지만, 개발 환경은 오히려 훨씬 쾌적하다는 만족감을 느끼는 중임. 일부 AAA 게임이 동작하지 않는 점 빼고는 Windows + WSL보다 더 좋은 경험이라고 생각함
          + 나도 비슷한 경험을 했으며, 이제는 리눅스 설치가 오히려 더 쉬워진 입장임. Windows의 스파이웨어 기능들로 인해 사용할 이유가 더 줄어들고 있다고 생각함
          + 단, Nvidia 그래픽카드를 쓰는 경우는 예외라고 봄
          + 어떤 게임에서 문제가 있었는지 궁금하다는 질문
          + 대부분이 이런 경험을 해봤다고 생각함. 현재 Windows가 게임(GPU)이 아니었다면 존재감이 별로 없었을 것이고, 과거 프로젝트에서는 msvc, cygwin, msys2 등으로 빌드 환경을 바꿔가며 혼란을 겪던 과정도 회상함. 이제는 WSL로도 일정 부분 쉽게 빌드가 가능하지만, 환경 변수 하나 바꾸는 과정까지 피로감을 느끼며 다시는 이런 방식을 반복하고 싶지 않다고 말함
     * 나는 오히려 리눅스에서 Windows를 가상머신으로 사용하는 방식을 추천함. Windows에서 리눅스를 쓰고 싶어질 때가 있다면, 아예 리눅스로 전환해버리고, 되돌아볼 일이 없다는 신념임. 지난 15년간 Windows로 돌아간 적 없음. 지금의 Windows 현실을 고려하면 심지어 VM에서도 쓰기 망설여지는 수준임
          + GPU 관련 작업(게임, Adobe suite 등)에선 VM에서 Windows를 실행하려면 별도의 GPU를 VM에 패스스루 해야 하고, 이게 없는 사용자는 가속 없는 환경을 감수해야 하므로 VM 방식이 쉽지 않다는 입장임. Photoshop을 QEMU QXL 드라이버로 돌리면 성능이 형편없고, VirGL은 아예 Windows 게스트를 지원 안함. VMWare, VirtualBox는 약간 나은데, 그래도 네이티브에 미치지 못함
          + 대부분의 Windows 관련 스레드에서 생산성 앱 때문에 Windows가 필수인 사람들과, 그렇지 않은 사람들의 분명한 단절을 인식한다고 생각함. 나처럼 GPU 관련 생산성 앱을 쓰는 경우, VM으로는 불가능하고 결국 Windows를 네이티브로 써야 한다는 주장임. 가벼운 앱만 쓰는 이용자라면 가상머신으로도 충분하지만, CAD나 게임 같은 본격적인 작업에는 적합하지 않다고 판단함
          + 수년간 리눅스만 썼다가 Windows, 이후 Mac으로 다시 돌아온 입장임. wine 호환성 문제, 미완성 소프트웨어 대안(예: GIMP의 Photoshop 대체 불가), Qt와 GTK 앱 혼용으로 인한 데스크탑 미관 붕괴 등, 리눅스가 만능은 아니라고 지적함
          + 반론으로, Valve Index 같은 VR 기기는 이런 환경(즉, Linux에서 Windows VM 구동)에서 전혀 제대로 동작하지 않는다는 경험 기반 의견임. 어릴 때부터 Linux 하드코어 유저였지만, 이런 특수한 예외들도 존재하니 무조건적인 일반화는 어렵다는 입장임
          + 관련 정보로, Windows를 평가판 VM 이미지로 배포하는 공식 링크를 소개하며, 평가판 만료시 데스크탑이 검은색으로 변하고, PC가 매시간마다 꺼지는 등 제약이 있음을 알림. 최신 평가판 이미지는 6개월 이상 업데이트되지 않았고, 등록하면 ISO는 제공된다는 안내임
     * 이름이 Windows subsystem for Linux라서 항상 헷갈림. 얼핏 보면 무슨 Wine의 공식 버전 같은 오해를 받기 쉽고, 사실은 Linux를 위한 Windows의 서브시스템이 맞음. 마치 Microsoft가 Linux에 기능을 주는 것처럼 들려 언짢음
          + Microsoft가 'Linux'를 시작으로 프로젝트 이름을 붙일 수 없기 때문에 결국 WSL이라는 이름을 썼다고 밝힘
          + 과거에 Windows Subsystem for Unix라는 프로젝트에서 유래한 네이밍임. 이름이 항상 기대와는 어긋나게 동작했다고 함
          + Windows의 Linux용 서브시스템이라는 중의적인 개선안을 재치 있게 제안
          + Windows에서 Linux를 구동하기 위한 서브시스템이지만 네이밍이 헷갈린다는 점 동의
     * 나는 최근 몇 년간 가끔씩 WSL로 개발을 해왔음. 잘 동작할 때는 최고지만, 한번 꼬이면 정말 악몽임. 네트워크, VPN, XServer, 윈도우 스케일링, 하드웨어 가속 그래픽 등에서 끊임없는 문제가 있었음. 실제로 개발하는 시간보다 문제 해결에 더 많은 시간을 쓴 느낌이고, 개선이 이뤄진 적이 없었음. WSL은 빠르고 강력하지만, 일상적으로 쓰기에는 나에겐 너무 힘든 환경임. 대신 MSYS2를 주력으로 쓰고 있는데, 속도는 느리지만 최소한 안정적이라는 점이 가장 큰 장점임
          + 나는 아직도 WSL 베타 버전을 쓰고 있는데, 업데이트하면 지금 잘 동작하는 환경이 망가질까 봐 업그레이드도 못 하는 상태임
     * 지난주 기록적 실적 발생 후에도 마이크로소프트 대규모 정리해고가 있었는데, 이런 현상이 그 부작용인지 궁금함
          + 대기업에서 3% 규모의 정리해고는 전체 혹은 무언가 프로젝트를 아예 없앨 경우가 아니면, 사실상 아무 영향도 없는 수준이라 생각함. 오히려 대기업엔 인원 과잉이 흔해서 이 정도는 아무것도 아니라고 단언함
          + 이런 대기업의 오픈소스화 결정, 준비, 실행은 절대 1-2주 안에 진행될 수 없는 대규모 프로젝트임
          + 이번 Build 행사 및 관련 뉴스에 대해 지금의 전반적 분위기에서는 긍정적으로만 보기 힘들다는 우려를 표명함
          + 발표대로라면 오랜 기간 준비해 온 결과라서 최근의 정리해고와는 무관하다는 설명에 동의함
     * WSL 1을 동작시키는 주체인 kernel side driver lxcore.sys는 이번 오픈소스 대상에 포함되지 않음을 알림. 그리고 WSL 1이 아직도 지원되고 있다는 점에 놀랐으며, 유지보수 모드일 거라 예상함
          + 내가 진짜 관심 있는 부분은 lxcore.sys 뿐임. 나는 아직도 WSL1만 사용하며, ABI를 넘나들거나 Windows를 Linux 유저스페이스로 터널링하는 독특한 해킹도 해봄. 오픈소스로 더 쉽게 다룰 수 있기를 희망함
          + 실제론 WSL1, WSL2 모두 완전히 지원받고 있다고 들음. 숫자상 차이와는 다름
     * 라이선스 및 세부 내용 확인이 필요함. 긍정적일 수도 있지만, MS가 개발자 해고 이후 무료 도움을 요청하는 명분일 수도 있다고 의심함
          + 라이선스는 MIT임을 안내함
     * 나는 Windows 사용자는 아니지만 WSL을 훌륭하다고 평가함. 하지만 많은 Windows 사용자들이 Linux를 비판하는 가장 큰 이유가, ""Windows처럼 보이지 않는다""는 점이고, Linux 사용자 입장에서는 오히려 Linux가 Windows처럼 보이지 않아서 더 좋음. Windows 사용자가 무료로 광고 없는 Windows를 원하는 욕심에 Linux를 윈도우화하지 않길 바람. WSL이 Windows 사용자들을 Windows에 남겨두는 효과가 있다면 오히려 더 반김
          + 나는 Windows 사용자가 아니지만 WSL 자체를 싫어함. Microsoft가 개발자 세대를 Linux로 잃을까 봐, 아예 OS 안에 Linux를 포함시킨 전략으로 보임. 개발자가 직접 커널을 리컴파일하며 체험하는 즐거움을 잃게 만든 것이라고 아쉽게 생각함
     * WSL 안에서 Windows 파일시스템 작업할 때 생기는 버그들이 이제라도 빨리 고쳐졌으면 한다는 바람 가져봄
          + Microsoft 역시 그러길 바란다고 생각함

   엔데버 +lustre / windows11 + wsl + wsa 를 쓰는 입장에서
   후자가 편의성은 좋아요
   다만 퍼포먼스는 전자가 좋습니다

   WSL팀도 이번에 많이 잘랐나보네요…

   인력이 없으면 community driven 이라는 명목으로 오픈소스 해버리는 요즘 MS의 행보 ㅋㅋ..

   짬처리네요

   WSL1은 크로스 개발 환경에 최고의 머신입니다. IO도 빠르며 리눅스 기반의 명령어도 바로바로 수행가능합니다. WSL2는 1보다 크로스 컴파일이 느립니다.
"
"https://news.hada.io/topic?id=21038","OpenAI, Jony Ive의 AI 하드웨어 스타트업 'io'를 65억 달러에 인수","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            OpenAI, Jony Ive의 AI 하드웨어 스타트업 'io'를 65억 달러에 인수

     * OpenAI는 조니 아이브가 공동 창업한 AI 기기 스타트업 ‘io’를 약 65억 달러 규모의 주식 거래로 인수
     * 이 거래는 OpenAI 최대 규모의 인수로, 아이브와 애플 출신 디자이너들의 팀을 확보하고 AI 하드웨어 개발 전담 유닛을 신설함
     * 아이브는 “지난 30년의 경험이 이 순간을 위한 것이었다”고 밝혔으며, Altman은 “소비자 하드웨어 역사상 최고 품질의 제품이 나올 것”이라 언급함
     * 양측은 2026년 출시를 목표로 완전히 새로운 형태의 AI 디바이스를 개발 중이며, io의 핵심 인력 55명이 OpenAI로 합류함
     * LoveFrom은 독립 상태를 유지하되 OpenAI 전체 디자인을 주도하며, Altman은 “AI를 위한 GUI의 새로운 정의가 필요하다”고 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenAI가 조니 아이브의 AI 스타트업 ‘io’를 인수

  인수 개요

     * OpenAI는 조니 아이브가 공동 창업한 AI 하드웨어 스타트업 ‘io’를 65억 달러 규모의 주식 거래로 인수함
     * 이 거래는 OpenAI 역사상 최대 규모로, 기존에 지분 23%를 확보하고 있었으며 이번 인수로 전체를 소유하게 됨
     * io에는 애플의 아이폰·아이패드·애플워치 디자인을 주도한 디자이너들이 다수 포함되어 있음
     * 인수로 OpenAI는 AI 전용 하드웨어 개발 조직을 신설하게 되며, Altman은 이를 “새로운 컴퓨팅 폼팩터의 탄생”이라 표현함

  조니 아이브와 OpenAI의 협업

     * 아이브는 OpenAI CEO Altman과의 인터뷰에서 “이 협업은 지난 경력 전체의 정점”이라고 언급
     * Altman은 “스티브 잡스가 자랑스러워했을 결정일 것”이라며 아이브의 복귀를 환영
     * 아이브와 Altman은 2년 전부터 새로운 디바이스 아이디어를 공유하며 협업을 이어왔음
     * 두 사람은 “현재 시장의 불만이 새로운 형태의 제품에 대한 기대감으로 이어지고 있다”고 진단

  io 팀 구성 및 향후 제품 계획

     * 이번 인수로 OpenAI는 약 55명의 하드웨어·소프트웨어·제조 전문가들을 확보하게 됨
     * io는 인공지능이 인간 수준의 인지 능력에 도달하는 시대를 위한 하드웨어를 목표로 설립됨
     * 아이브는 “iPhone이 노트북을 대체하지 않았듯, 우리의 첫 제품도 스마트폰을 대체하진 않을 것”이라 설명
     * 첫 번째 제품은 2026년 출시 예정이며, 완전히 새로운 카테고리를 목표로 하고 있음

  LoveFrom과의 관계

     * 아이브가 애플 퇴사 후 설립한 디자인 스튜디오 LoveFrom은 OpenAI와의 협업 하에 독립적으로 유지
     * LoveFrom은 Ferrari, Airbnb 등의 고객과 기존 계약은 유지하되, 신규 대형 프로젝트는 받지 않을 계획
     * LoveFrom 팀은 OpenAI의 소프트웨어 UI/UX 개선도 함께 담당할 예정이며, ChatGPT 인터페이스도 포함됨
     * OpenAI 내 LoveFrom의 역할은 마치 애플 초기 시절의 제품 혁신과 유사한 분위기를 띔

  애플과의 관계 및 시장 영향

     * 아이브는 과거 스티브 잡스의 “정신적 파트너”였으며, 이번 협업은 애플의 AI 경쟁력 부족을 상징적으로 보여주는 사건
     * 현재 애플은 OpenAI의 ChatGPT를 자사 AI 기능에 통합하며 자체 AI 플랫폼 부족 문제를 메우고 있음
     * 이번 인수는 메타의 Ray-Ban 스마트글라스 등과 경쟁하는 AI 디바이스 시장의 경쟁 격화를 예고함
     * 기존 제품들과 달리 “Humane AI 핀”이나 “Rabbit r1”처럼 실패한 시도들과는 차별화될 것이라 강조됨

  기타 정보

     * io의 공동 창업자에는 Scott Cannon, Evans Hankey, Tang Tan 등이 있으며 모두 애플 출신임
     * OpenAI의 제품 부문 부사장 Peter Welinder가 새로운 하드웨어 부서를 감독
     * Instacart CEO Fidji Simo는 OpenAI 앱 개발 CEO로 새롭게 합류해 Altman에게 직접 보고
     * Altman은 “AI와의 상호작용은 아직 터미널 기반 단계에 불과”하며, 차세대 사용자 인터페이스 정의가 필요하다고 언급함

  결론

     * 이번 인수는 OpenAI가 하드웨어 영역으로 본격 진입하고, 애플의 상징적 인물과 팀을 데려옴으로써 AI 시대의 새로운 디바이스 표준을 정의하려는 전략적 움직임임
     * 아이브는 이를 “가장 중요한 작업은 이제 시작”이라고 표현하며, 과거 아이팟·아이폰 이전의 애플 시절과 유사한 분위기로 설명함

   매직마우스와 터치바 생각하면.. :thinking_face:

   바이브 인수. 분위기만 쫒다가 큰일날듯.

   도대체 무슨 미래를 봤길래 이런 선택을...

        Hacker News 의견

     * https://archive.is/HgpSJ
     * 최근에 Windsurf를 인수하더니 이제 또 이런 움직임 관찰. OpenAI가 엄청난 자금을 모델 트레이닝에 쓰기도 부족해 보이면서도 막대한 돈을 쓰는 모습. 이미 경쟁력을 유지하려면 추가 투자가 필요하다는 신호도 보냈고, 아주 큰 회사로 성장해서 'too big to fail' 전략을 쓰려는 시도 해석. 하지만 아직 진짜 견고한 경쟁우위를 구축 못한 상황에서 이런 전략은 잘 작동하지 않을 거라는 생각
          + OpenAI가 더 이상 ""GPT-5"" LLM은 없을 거라고 공표, 앞으로 그들이 말하는 ""GPT-5""는 4o, dalle, 비디오 모델 등 여러 모델을 융합한 것이라고 언급. 이것만 봐도 더 이상 지능 면에서는 한계에 부딪힌 것이 분명하다는 느낌. 이런 인수 움직임까지 더해지니 점점 극심한 조급함 감지
          + 기사 내용에 따르면 50억 달러 중 상당부분이 OpenAI의 지분 거래라는 점. 실제로 현금을 쓰는 건 아니라는 해석
          + 왠지 이런 인수 등 대부분이 결국 OpenAI에게 역효과로 돌아갈 예감. 예를 들어 하드웨어 등 투자로부터 합당한 제품이 합리적인 시간 내에 나오리라고 전혀 납득이 안 감. 나는 반박되는 걸 매우 환영하지만, 확실한 근거 제시가 꼭 필요
          + Windsurf도 인수 이후 가치가 급락. 앞으로 또 다른 인수는 어떻게 될지 궁금
          + 그러면 그 많은 돈을 뭐에 쓰겠나 생각. AGI가 당장 내일 올 것처럼 수십 억을 유치해서 독점적 거대기업 돼서 경쟁 다 먹어치울 뿐. 이제 아무도 AGI 이야기하지 않는 분위기
     * 예전에 해커뉴스에서 어떤 분이 Jony Ive의 엄청난 성공도 Steve Jobs라는 '편집자'가 곁에 있었기 때문에 가능했다는 통찰을 얘기한 적 있음. Jobs가 세상을 떠난 이후 Ive 곁에는 그런 편집자가 없었음. Sam Altman이나 OpenAI가 좋은 편집자가 될 수 있을지 두고볼 일
          + 이게 바로 '리더십'의 신기루 같은 힘. 훌륭한 리더는 보통 인재뿐 아니라 평범한 사람도 역량을 최대로 발휘하게 만들지만, 나쁜 리더는 세계 최고 인재도 망칠 수 있다는 진심의 관찰
          + Apple Store의 Ron Johnson이 Apple을 떠난 후의 커리어를 볼 때, 정말 핵심 인물 보험이 필요하다는 의구심. 이번 인수는 사실상 한 명(물론 기술자나 인재들도 있지만)의 명성을 기대는 대형 투자라는 점도 언급. Johnson의 실패 사례와 함께, 'key man risk'가 현실적으로 중요한 이슈임
          + Ive가 실제로 이번 인수에 합류하는 건지 의문
          + Ive나 Altman 모두 태생적 특권층이라는 사실 외에는 특별히 독보적인 점이 없는 인물이라는 판단. 이들이 주도했던 시대 분위기와 사회적 흐름은 이제 다수에게서 멀어진 감각. 만약 Ive가 예전의 업계 평판을 여전히 가지고 있었다면 MS에 의존할 수밖에 없는 기업과 협업하지 않았을 것. Sam의 전략도 예전 2000~10년대의 성공적 이미지에 기대려는 마케팅 수법으로 보임. OpenAI의 천재들은 이미 떠났고, 이제 그냥 또 하나의 빅테크 기업이 되었단 평가
     * ChatGPT와 항상 대화할 방법에 대한 생각. 마이크, 스피커, 그리고 ChatGPT와의 영구 연결만 있으면 된다는 io 아이디어. 특히 공공장소에서 속삭이거나 무성음으로 대화할 수 있어야 하는데, 이런 기능은 아직 많이 없는 듯. 만약 귀에 착용하는 장치라면 근육+소리 데이터를 연동해 무성 언어를 인식 가능할 것. 그들의 목표는 사실상 자체 OS를 클라우드 모델 기반으로 구축하는 것이라고 추정
          + 구체적 예측 목록 제시
              1. 턱·귀관 근육 움직임으로 무성 언어를 읽는 이어버드
              2. AI 모델 자체가 인터페이스가 되는 랩탑(효과적으로 AI OS)
              3. 최소화된 포켓 디바이스, 대부분의 OS는 클라우드에서 작동
              4. 고성능 로컬 AI를 위한 저전력 칩, 어떤 사물에도 내장 가능
              5. 옷에 끼우는 클립 같은 형태
              6. 영화 속 완벽한 평면 유리 태블릿(개인적으로 희망하지 않는 형태)
              7. 집안 곳곳의 마이크, 센서, 스피커, 스크린을 이용한 지능형 환경
          + 이 리스트에 감사. 하지만 우리는 생산성에 노예가 된 건 아닐지 생각. 일부 CEO나 의사처럼 진짜 도움이 절실한 이들도 있지만, 나는 그저 보통 회사 다니는 중년 직장인일 뿐. 비효율에서 오는 소소한 즐거움도 있는데, 이런 비효율이 정말 문제인지 의문
          + 이런 기능 조합은 결국 AR 헤드셋 형태로 귀결. Google, Meta 등 다 시도했지만 다 실패. 너무 크고 불편. Carmack도 VR/AR이 실질적으로 퍼지려면 헤드기어 크기가 수경, 나중에는 안경처럼 돼야 한다 주장. Ive라면 이런 방향으로 밀고 나갈 수 있을 것
          + 외부에서 ChatGPT와 대화할 수 있는 구체적 사용 사례가 뭔지 궁금. 공원 산책하거나 식사할 때 내 머릿속에서 AI가 상시로 대화하는 게 무슨 가치가 있을지 거의 생각나지 않음
          + Limitless.ai 펜던트 같은 #5번 솔루션을 몇 주간 사용해 본 경험 공유. 항상 옷에 달고 녹음+전사해주는 게 신기하다고 생각했으나, 한 달 정도 지나니 들고 다니는 것도 자꾸 까먹게 됨. 결국 아직 그다지 가치 못 느낌. 이미 모두가 포켓에 마이크(스마트폰)를 들고 다니는데 굳이 다른 마이크가 필요한지 의문. 멋진 토이지만 동시에 도청기처럼 보여서 약간 소름끼침. 결국 제품 가치가 소비자 입장에서 충분하지 않으면 데이터 광고 판매로 귀결될 걱정
          + 나는 이미 ChatGPT 앱에서 마이크 입력을 공공장소에서 활용. 낮은 목소리(속삭임)로 휴대폰 가까이 대고 이야기하면 3피트(약 1m) 이상 떨어지면 거의 들리지 않고, TTS는 여전히 잘 작동
     * OpenAI가 콘텍스트 윈도우를 비용 증가 없이 확장할 방법을 정말 아는지 궁금. 그렇지 않으면 Google이 또 다 가져갈 분위기. Gemini 2.5 pro preview처럼 파일을 그대로 메모리에 넣을 수 있는 게 진짜 원하는 것. 각종 압축 트릭 등 다 실제로 써보니 별로고, 32k 입력 토큰도 시시하게 느껴지는 요즘 상황. OpenAI가 곧 10M 콘텍스트를 싸게 제공하지 않으면 나는 비관적. Google은 곧 할 듯
          + Google은 과학, 칩, 인프라 등 모든 요소를 수직적으로 통합. 이제 이 시장은 Google이 놓치지 않는 한 Google이 지배
          + 지금이 바로 AI 경쟁의 변곡점. 순수 AI 파워가 핵심이라면 Google이 앞으로 선두에 설 듯. 자체 TPU, 데이터센터도 다 갖고 있고, 별도의 외부 인프라 파트너(Oracle, Softbank) 필요도 없음. Android, YouTube, G-Suite 등 에코시스템도 튼튼. 반면 OpenAI는 몇 년간 제품 중심 행보, 유명 연구자들이 대거 떠난 후 Altman이 내부를 재편해 이제 제품 개발에 더 집중 가능해짐. 만약 '제품' 그 자체가 충분한 차별점이 된다면 Altman이 더 나은 제품을 만들어낼 가능성도 상존. 그때 Ive의 역할이 의미 있을 수도. Google은 내부 개발 제품력이 약한 것으로 악명 높음
          + 맞음. ChatGPT가 최근 요약·메모리 기능 도입했는데, 실제로 써보면 그냥 그렇다는 느낌
     * YouTube가 10억 달러에 인수된 지도 20년이 안 지났음. 당시엔 터무니없이 높은 평가 같았지만, 그땐 그래도 사용자가 있었음. 이번 인수는 물가상승률 감안해도 무려 4배. 근데 예전에 유명했던 팀을 이끈 인물의 '분위기'에 투자한 것 아닌가 생각
          + 나도 결론은 같음. 정말 이상한 딜. 다소 절박해 보이는 인상
          + 이제 돈의 개념은 사라진 시대라는 냉소도
          + '분위기 기반 코딩'에서 이제는 '분위기 기반 인수'로 발전?
     * 관계자 인맥에 들어가기만 하면 상상 이상으로 많은 돈이 돌고 있다고 느낌. 그런데 Ive의 회사가 도대체 무슨 일을 했기에 65억 달러 가치라는 건지 의아
          + AI 업계 소식을 누구보다 열심히 따라가는데도 Ive의 회사가 뭘 하는 곳인지 직접 찾아보기 전엔 도저히 알 수가 없음
          + '소수만의 거대한 클럽'이라는 느낌
          + 결국 예쁜 폰트 하나 만든 것 뿐이라는 평가
          + Vegas식 카지노나 크루즈선과 비교하면 한 대당 10~20억 달러. 이번 인수는 거대한 카지노 옆에 크루즈선 두 대 묶어둔 만큼의 값어치
     * Sam Altman이 스스로를 포장하는 데 엄청 열정적인 모습. 이런 사람과는 절대 일하고 싶지 않다는 감상. 구체적인 영상 링크도 공유
          + 처음엔 공격성 있는 트윗일 줄 알았는데, 실제로 Sam Altman 본인 게시물임. 2분 보고 도저히 못 보겠어서 그만둠. 말로만 과장하는 게 아니고 영상에서 자아도취가 심한 분위기. 실질적인 결과(제품) 만들어 놓고 이런 영상 올리는 게 맞다는 티끌
          + 이 영상은 2010년대 초 Bay Area 테크 신화 덕후들에게 자기 신화를 바치려는 전략적 연출. ""난 당신들과 똑같이 Steve Jobs를 숭배한다""는 코드. 실제로 이 연출에 넘어간 사람도 있긴 함
          + 영상 보려고 했는데 도저히 못 견딤. 처음엔 패러디인 줄, 그 다음은 AI 합성인 줄 착각, 결국 끝까지 못본 이유
     * ""Vibe Coding""을 만들어 낸 팀이 ""Vibe Acquisitions""도 내놓는 유머
          + '분위기'가 들뜨면 곧 몰락이란 촌철살인 농담
     * 제품도 없고(공개된 것도 없음), 웹사이트도 없는, 창업자까지도 합류하지 않는 회사를 인수. 딱히 acquihire(인재 합류 목적의 인수)도 아님. OpenAI의 비영리적 사명과도 부합한다는 냉소. 이 업계는 신기할 정도로 흥미
          + 창업자가 인수 후 합류하지 않는다? 의미가 뭔지 질문. 실제로는 Sir Jony Ive가 OpenAI의 심도 있는 디자인·창의 업무를 맡아 신제품을 개발하는 역할이라 밝힘
"
"https://news.hada.io/topic?id=20998","MS의 공식 터미널용 에디터 - Microsoft Edit ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    MS의 공식 터미널용 에디터 - Microsoft Edit

   일전에 MS Terminal 개발자 중 하나가

   Terminal에서 사용될 에디터에 관한 후보군을 설문 한 적이 있는데

   https://github.com/microsoft/terminal/discussions/16440

   Rust로 아예 새로 만들어서 이번 MS Build 행사에서 공개

   https://devblogs.microsoft.com/commandline/edit-is-now-open-source/

   인상 깊네요.

   일단 실행파일이 러스트임에도 굉장히 작고, 한글과 이모지를 포함한 유니코드도 잘 지원하고, 단어간 이동이랑 드래그로 선택 잘되고, 외부 패키지 의존성이 거의 없고 제 PC에는 적용이 안되는데 일단 소스에는 국제화도 되어있습니다.

   윈도에 기본으로 포함되면 나름 요긴하게 쓸 것 같습니다.

   옛날에도 있었죠. M Editor라고…
   https://www.edm2.com/index.php/Microsoft_Editor

   와... 방금 설치해서 구경해봤습니다.
   MS Dos 시절 느끼게 해 주는 인터페이스...
   별도의 편집기 설치하지 않은 상태에서 가볍게 쓰기 좋겠습니다. 특히, 원격으로 터미널 접속해서 작업해야 하는 경우가 많은데 터미널에서는 파일 편집하거나 보기 불편했었는데 너무 좋네요.

   qbasic 생각나네요

   쩌.. 쩐다!

   https://en.wikipedia.org/wiki/MS-DOS_Editor

   이놈 64비트로 올리는 결정은 왜 안 한건지 궁금하네요.
   이걸 '무겁다' 고 말할 수는 없을텐데.

   Github README에 vscode 같은 현대적인 인터페이스를 제공하고 싶었다고 하네요.

   아악.. 수정이 안되네요. 당연히 ms-dos 용이니 64비트로는 못 올리겠지만 edit64 같은 별도 앱을 만들었다면 어땠을까... 하는 아쉬움이 있습니다.

   Micro가 뽑혔으면 재밌었을 텐데 아쉽네요

   이름 농담을 제치더라도 단축키가 윈도 컨벤션에 가깝고 조작이 직관적인 등 진지하게 괜찮은 옵션인데 말이죠.. 낮은 인지도만 빼면

   이름이 edlin 이 아니라는 게 아쉽네요..

   플러그인도 지원 해주는걸까요? 아니면 단순 메모장의 터미널 버전이 되는걸까요.

   그냥 vim을 디폴트로 넣어주는게 제일 좋은데.

   이정도면 가볍게 쓰기 좋네요.
     * Windows 11에 기본 탑재될 예정인 64Bit용 기본 텍스트 편집기
     * 실행 파일 크기 250kB 미만
     * TUI 기반으로 마우스 모드 및 메뉴 단축키 지원
     * 멀티 파일 편집 지원
     * 정규표현식 지원하는 찾기 및 바꾸기
     * 워드랩 지원
"
"https://news.hada.io/topic?id=20919","Show GN: LunaTools - 구글 크롬 확장 프로그램 (마우스 제스처, 페이지 키보드 이동, 탭 중복 제거, 탭 정렬, 창 통합, PiP 단축키)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show GN: LunaTools - 구글 크롬 확장 프로그램 (마우스 제스처, 페이지 키보드 이동, 탭 중복 제거, 탭 정렬, 창 통합, PiP 단축키)

   개발자도 아니고, 프로그래밍도 전혀 모릅니다만 AI 챗봇 (주로 제미나이 , 클로드 조금) 이용해서 바이브 코딩으로 구글 크롬 확장 프로그램 만들어봤습니다.

   기존에 쓰던 확장 프로그램들 프롬프트로 요청해서 통합해서 제작하고, 유저스크립트도 제 나름의 아이디어로 제작하고 합쳐봤습니다. 어제는 기존에 있던 확장 프로그램 개발자분 허락 받아서 한가지 기능 더 포함시키기도 했습니다.

   혹시 더 추가할 만한 기능이 있을지 궁금해서 GeekNews 에 올려봅니다. 더 추가할만 한 기능 있을까요?
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LunaTools

   제가 최근에 구글 제미나이, 앤트로픽 클로드로 만든 구글 크롬 확장 프로그램 2개, 유저스크립트 2개를 1개의 구글 크롬 확장 프로그램으로 합쳤습니다. (정확한 비율은 모르겠습니다만, 코드 기여도 예상해보면 제미나이 80%, 클로드 20% 정도일 것 같군요 )

   이름은 LunaTools 입니다. 그럼 유용하게 쓰시길

  기능

    1. 마우스 제스처, 우클릭 후 ← 뒤로, → 앞으로, ↑ 새 탭, ↓ 닫기
    2. 사이트 URL 에 페이지 번호 있을시 키보드 좌우키로 페이지 이동
    3. 새로운 탭을 열었을 때 기존에 있는 탭일 경우 해당 탭 닫고, 기존 탭으로 포커싱
    4. Alt + A 를 누르면 현재 창의 탭을 URL 기준으로 정렬
    5. 확장 프로그램 아이콘을 누르면 여러 창을 1개의 창으로 합침
    6. 현재 보고 있는 영상 Shift + Ctrl + P 눌러서 PiP 로 열기

  관련 글

     * LunaTools
     * 맞 2.0 (구글 크롬 마우스 제스처 확장 프로그램)
     * 탭댄스 ver 3.2 (구글 크롬 탭 관련 확장 프로그램), 키보드 좌우키 페이지 이동 유저스크립트

   와 이걸 바이브 코딩만으로?

   제미나이 꽤 좋더군요. 프롬프트로 기능 설명 만으로 구현해주더군요

   멋져요

   말씀 감사합니다. 계속 업데이트 해봐야겠네요

   블로그 https가 안되네요...

   예 업데이트 해야겠네요. 답글 감사합니다
"
"https://news.hada.io/topic?id=21006","보고 볼 수 있는 사람과 아예 바라볼 수조차 없는 사람","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     보고 볼 수 있는 사람과 아예 바라볼 수조차 없는 사람

     * 1862년 일본 사절단이 유럽을 처음 방문하여 문명과 기술에 충격을 받음
     * Nadar의 사진관에서 여러 문화계 위인들의 초상 사진이 남겨져 있음
     * Baudelaire, Manet, Dumas, Victor Hugo, Sarah Bernhardt 등 다양한 인물들의 인간성을 사진으로 담음
     * Nadar는 사진 예술과 인간성 포착의 중요성을 강조함
     * 사진에 대해 “보고 볼 수 있는 사람과 아예 바라볼 수조차 없는 사람이 있다”는 의미 있는 말을 남김
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

일본 사절단의 유럽 방문과 첫 경험

     * 1862년, 240년 만에 일본 사절단이 유럽에 입국함
     * 오랜 기간 세계와 고립되었던 일본은 미국 함대의 등장 후 어쩔 수 없이 대외 개방을 시작함
     * 40명의 사절 중 상당수는 사무라이였으며, 이들의 사명은 해외 문명을 학습하고 자국의 강제 개방 속도를 조절하는 것임
     * 유럽에서 당시 일본과는 차원이 다른 산업 혁명기 기술을 목격함
     * 특히 프랑스에서는 전신 기술에 큰 충격을 받으며, 문자 메시지가 몇 분 만에 대륙을 넘어가는 모습을 인상 깊게 경험함

Nadar의 사진관과 일본 사절단

     * 프랑스에 방문한 일본 사절들은 유명 사진작가 Nadar의 스튜디오를 찾음
     * Nadar는 그들을 카메라로 기록함
     * 사진관에서 당시 최신 장비로 초상 사진을 촬영하며, 이 경험이 사절들에게 특별한 의미를 남김
     * 파리를 방문한 유명인사답게 초상 촬영을 진행한 것임
     * Nadar의 손길로 인물의 특별함이 포착됨

모더니티와 Charles Baudelaire

     * Charles Baudelaire는 19세기 중반의 낯선 시대적 감각을 ""모더니티""로 규정함
     * 그의 시는 종종 사회적 논란을 불러일으켰으며 금서가 되기도 함
     * Baudelaire는 건강 악화와 중독, 빈곤으로 힘든 시기를 보내는 중 Nadar의 사진관을 방문함
     * Nadar는 단순한 배경에서 인물의 성격과 내면을 포착하는 스타일로 Baudelaire의 초상을 촬영함
     * 그의 시선에서 직설적인 눈빛이 드러남

예술계 인물들과의 인연

     * Nadar는 당대 최고의 예술가들인 Edouard Manet, Alexander Dumas, George Sand, Victor Hugo 등을 촬영함
     * Manet는 Nadar와 친분이 있었으며 자신의 친구를 모델로 그림을 그려 헌정함
     * Dumas는 Nadar의 어린 시절 우상이었으며, 유쾌한 모습으로 등장함
     * George Sand는 Nadar와 오랜 친구로 여러 번 촬영됨
     * Victor Hugo의 초상은 노년기와 임종 직전까지 남겨짐

후기 인물들과 Nadar의 사진술

     * Franz Liszt 등 다른 문화계의 유명 인사들도 Nadar에 의해 기록됨
     * 노년의 Liszt는 여전히 눈에 생기가 남아있음
     * Nadar는 신흥 스타였던 Sarah Bernhardt의 놀라운 카리스마에 다시 사진관에 복귀하여 그녀의 이미지를 반복적으로 담음
     * Bernhardt는 결혼 밖에서 자녀를 두는 등 사회적으로도 파격적인 삶을 살았음

권력자와 사상가의 포착

     * Nadar는 벨기에 국왕 Leopold II 또한 촬영하였음
     * 사진에서 인간성의 차이를 읽을 수 있다고 느껴짐
     * Proudhon 같은 사회주의자 겸 아나키스트도 기록함

사진의 본질에 대한 Nadar의 생각

     * Nadar는 사진이 위대한 발견이면서도 누구나 할 수 있는 일이지만, 진정으로 의미 있는 이미지를 발견하는 데에는 특별한 재능이 필요하다고 평가함
     * Nadar의 초상들은 2세기 전 인물들의 개성과 인간성을 현대의 우리에게도 전해줌
     * 그는 자신이 그들 중에서도 특출난 재능을 가진 이었음을 스스로 알고 있었음
     * 마지막으로 Nadar는 “사진에서, 그리고 모든 일에서 보자면, 보고 볼 수 있는 사람과 아예 바라볼 수조차 없는 사람이 있다” 라는 말을 남김

        Hacker News 의견

     * 전 세계 1900년대의 다양한 장소를 보여주는 관련 동영상 시리즈 추천 정보 제공
       제품이나 패션의 대량 상업화와 의도된 노후화가 시작되기 전의 모습을 볼 수 있는 흥미로움 강조
       사진 속 사람들의 표정이 꾸며진 미소나 시선을 피하지 않고, 딱딱하고 진지해 보이지만, 이따금 장난스러운 모습도 연출된 장면 발견
       요즘엔 보기 힘든, 40대 남성들이 장난치며 노는 밝은 풍경 소개
          + 가짜 미소와 시선을 피하는 모습은 북미 특유의 문화적 특징이라는 생각
            스위스에서는 ""Swiss stare""라고 부르는 특별한 정면 응시 행동 존재
            관련 블로그 링크 안내
          + 아시아에서는 가족사진에서도 여전히 엄격한 표정을 유지하지만 미국 이민 뒤에는 모두 사진에서 미소를 보이기 시작 경험
            부모님의 생각에 공감
            미국에서는 상대적으로 삶이 편하고, 사람들도 더 행복해 보여서 자연스럽게 미소 짓게 되는 사회적 분위기 존재
            자국에서는 미소가 많으면 특별해 보여 놀림을 받는 문화, 그래서 웃지 않는 것
            미국에서는 심지어 억지 미소라도 사회에 적응하기 위해 짓는다는 의견
            예전 사진에서도 생활의 어려움이 표정에 드러나며, 그게 자연스러운 것이라는 생각
          + 이제는 어릴 때부터 사진이나 영상 촬영에 익숙해져서, 카메라 앞에서 기대되는 '모범적인 표정'을 연습하는 사회
            카메라 AI 셀피 모드가 표정을 자동 보정해 주는 시대임을 언급
          + 과거에는 사진의 노출 시간이 길어서 사람이 1분 가까이 표정을 유지해야 하는 상황 설명
            자연스러운 미소를 오래 유지하기가 정말 어렵다는 해석 제시
          + “A Million Ways to Die in the West”라는 영화 속 패러디 장면을 예시로 엄격한 표정이 패러디되는 모습 공유
     * 사진 속 한 인물의 눈빛에서 이후 콩고에서의 만행을 예견할 수 있다는 원문 해설에 대해,
       현재 우리가 그 사람에 대해 알고 있는 사실에 따라 인식이 달라진 결과일 수 있다는 의견
          + 누구나 그런 편견이 있다는 점을 모두 인식하고 있으므로 원문도 주의 문구(디스클레이머)를 달았다는 설명
     * 유명 화가의 사진에서 지적인 인상을 받는다는 평가를 인용하며,
       외모만으로 지능을 판단하는 것이 근본적으로 잘못된 일이라는 생각
       짧은 만남이나 한 장의 사진만으로 사람의 내면을 파악하려는 시도에 대한 회의감 공유
       사진이 한 인물에 대해 많은 걸 알려준다는 낭만적인 믿음에 대한 비판적 견해 제시
          + “겉모습으로 남을 평가하지 말자”는 교훈이 미국에서는 대중적으로 널리 퍼져 있고,
            성장하면서 자주 접하는 공익광고나 TV, 영화 등에서 이런 내용을 반복적으로 다루었다는 경험
            사회적 미덕에 대한 최소한의 위선이라도 지키려는 분위기가 있었는데,
            최근에는 이런 관점이 변했는지 궁금증 표출
          + 사진이 사람을 더 똑똑하게 보이도록 연출된 사례도 있다는 점에 주목
            예술적 연출에 대한 평가임을 강조
          + Manet이 실제로 지적인 인물이었기 때문에, 그 본질을 사진이 잘 포착한 경우는 문제없다는 의견
            음식 사진도 실물처럼 맛있게 보이면 잘 찍힌 것이라 평가
            오히려 인간의 얼굴에는 지능이 어느 정도 드러날 수 있고, 신경망이 훈련된다면 신호를 발견할 수 있을 거라는 흥미로운 의견
            인간은 오히려 지능을 숨기기보다 표시하는 진화적 경향 설명
          + groady/grody와 grotty의 의미 설명
            grotty는 호주 등지에서 명사로도 쓰일 수 있다고 정보 추가
          + 인상 예측이 완벽하진 않아도 무작위보다 낫다는 개인 견해
            더 강력한 데이터가 있다면 외형에 의존하면 안 된다는 교훈
     * HN에는 보통 새로운 도구와 프로그래밍을 다루는 글이 많지만,
       이제는 도구나 방법이 아닌 '무엇'을 프로그래밍할지 결정하는 게 더 중요해진 상황
       그 해답은 사용자 관점에서 가치와 세상을 바라보는 능력이라고 생각
       Nadar의 사진을 보며 그가 대상을 어떻게 보고, 역사의 한 장면을 남긴 인물들을 바라보는 관점에 주목
       제품 디자인은 타인의 시선으로 또 다른 세상을 상상하고,
       시간의 흐름 속에서 기술과 맥락의 잠재력이 제품에 얼마나 큰 영향력을 주는지 보여줌
       사진이라는 발명이 사무라이의 검보다 더 오랫동안 인간사에 영향을 끼쳤으며,
       과학, 의학 등 다양한 분야에 빛을 기록하는 강력한 힘을 선사했다는 감탄
       이 글이 누군가에게 다음 '사진'을 남길 동기가 되길 바란다는 격려
          + “무엇을 만들 것인가”라는 질문은 언제나 중요했고, 대표적으로 Dropbox 사례가 있다는 의견
          + 제품을 만드는 데 필요한 기술과 회사 취업용 역량이 달라서,
            실제 무엇을 만들지 결정하는 데 더 혼란을 준다는 지적
     * 한 남자의 19세기 프랑스 사진 여행이 정말 흥미로웠고,
       최신 AI 스타트업 기사들보다 훨씬 더 강렬한 인상 경험 고백
          + 인간 화가의 작품이 AI보다 의미상 '풍부함'을 줄 수 있다는 효과의 정량화 가능성 궁금증
            다음 단계로는 AI가 인간 예술 위에 덧입혀질 가능성 제기
     * 문화별로 몸짓과 표정이 얼마나 다른지에 대한 관찰
       어떤 사회는 개방적이고 표현적이지만, 다른 사회는 더 절제되고 중립적임을 세계 여행에서 직접 경험
       고개 끄덕임이나 작은 미소조차 사회마다 의미가 다르며,
       표정이나 시선의 규범이 집단주의/개인주의 또는 삶의 속도와 같은 더 깊은 사회 구조에서 비롯된 것인지 궁금증
       사소한 행동이 큰 문화적 태도를 반영할 수 있는지 질문
     * 기사와 사진을 정말 흥미롭게 읽고 감상했지만,
       “cannot even look”이라는 표현의 의미가 잘 이해되지 않아 궁금증
       자신이 이해하지 못하는 유형에 해당하는 것인지 자조 섞인 물음
          + “see”조차 시도하지 않는 사람, 자신이 무엇을 놓치고 있는지 모르는 사람이라는 의미로 해석
            다양한 해석이 있다는 점을 인정하며 자신은 단순한 프로그래머라고 겸손 표현
          + 누구나 사진을 찍을 수 있지만, 진정한 사진가는 별개의 존재라는 본질의 의도가 담겨 있다는 해석
     * 미국 여권 사진에서 미소 짓지 않는 것이 최근에 필수가 됐고,
       홍콩이나 중국 비자 사진에서는 아주 오래전부터 미소 금지가 있었다는 정보
       미소/엄격한 표정 규정도 시대와 지역을 오가는 규제의 예시
     * 19세기 인물 사진이 현대 컬러사진보다 인물의 본질을 더 잘 담아낸 기분이라는 감상
          + 당시 사진이 훨씬 비싸고 드물었으며, 전문 사진가가 찍어서 평균적으로 더 매력적이라는 합리적 설명
            물론 현대에도 훌륭한 컬러사진은 많이 존재한다는 의견
          + 초상화가란 한 인물을 오래 관찰해 결정적인 순간을 포착하는, 긴 노출 사진 같은 것이라는 생각
     * “[사진술]은 위대한 지성을 끌어들이는 과학이고, 기민한 정신을 자극하는 예술이자, 바보도 할 수 있는 분야”라는 문구를 소개하며 감탄
          + 소프트웨어 엔지니어링에도 딱 들어맞는 말이라며 재미를 느끼는 반응
"
"https://news.hada.io/topic?id=20977","Oxy - SQL 에이전트 및 데이터 자동화 구축을 위한 오픈소스 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Oxy - SQL 에이전트 및 데이터 자동화 구축을 위한 오픈소스 프레임워크

     * SQL 기반 데이터 분석 자동화 및 에이전트 구축을 위한 프레임워크로, Rust로 작성되어 높은 안전성과 성능 제공
     * 에이전틱 분석(Agentic Analytics)에 특화: 데이터 분석을 소프트웨어 개발처럼
          + AI 기반 데이터 분석에 소프트웨어 개발 라이프사이클(빌드-테스트-배포) 원리를 적용
          + 에이전트 생성, 프롬프트 테스트, 프로덕션 배포의 구조적 흐름을 통해 분석 자동화의 신뢰성을 높임
          + 반복적이고 일관된 데이터 작업 자동화로 생산성 향상 및 운영 리스크 감소에 기여
     * 선언식(Declarative) 워크플로우: 코드를 작성하듯 에이전트, 워크플로우를 구체적으로 선언해 관리 가능
     * 컴포저블(Composable) 아키텍처와 보안: 다양한 자동화 구성요소를 안전하게 조합해 활용할 수 있음
"
"https://news.hada.io/topic?id=21036","UUIDv7이 PostgreSQL 18에 도입됩니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      UUIDv7이 PostgreSQL 18에 도입됩니다

     * PostgreSQL 18에서는 UUIDv7을 기본 지원하며, 정렬 가능하고 인덱스 친화적인 고유 식별자를 제공
     * UUIDv7은 기존 UUID의 분산 환경에서의 고유성과 보안성은 유지하면서, btree 인덱스에 유리한 시간 기반 정렬 구조를 채택함
     * 기존 UUID 사용의 단점이었던 정렬 불가, 인덱스 난삽, 메모리 크기 중 앞의 두 문제를 해결하며 시간순 정렬 및 삽입 최적화를 실현함
     * PostgreSQL 18에서는 uuidv7() 함수로 UUID 생성이 가능하고, 타임스탬프 추출 및 커스텀 시간 입력 기능도 제공함
     * 이제 UUID를 기본 키로 사용하는 것을 주저하던 이유가 해소되어, 분산 시스템 및 다중 테넌트 환경에서 더 적합한 선택지가 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PostgreSQL 18

     * PostgreSQL 18의 베타 버전이 출시되었고, 9월 정식 릴리스를 목표로 테스트가 진행 중임
     * 주요 기능:
          + Async I/O: io_uring 기반 비동기 입출력으로 시퀀스 스캔 및 vacuum 2~3배 속도 개선
          + 멀티 컬럼 btree 인덱스의 Skip scan, OR/IN 쿼리 최적화
          + 업그레이드 간 플래너 통계 유지
          + UUIDv7 함수
          + 가상 생성 칼럼, OAuth 로그인, EXPLAIN에 I/O/CPU/WAL 정보 추가 등

UUID의 장점

     * 분산 환경에서 고유 ID 생성 가능
     * 예측 불가능한 공개 식별자로 보안성 강화
     * 클라이언트에서 직접 ID 생성 가능하여 서버 통신 최소화

기존 UUID의 단점

     * 정렬 불가능
     * 인덱스 비국소성으로 인한 삽입 성능 저하
     * 128비트 크기로 인한 오버헤드

UUIDv7의 해결책

     * RFC 9562 (2024년 5월 발표) 에 따라 도입된 신형 UUID 버전
     * 앞의 48비트에 Unix Epoch 기반 타임스탬프, 나머지에는 무작위값 + 카운터를 조합
     * 시간 순 정렬 가능하며, 인덱스 삽입 효율 증가
     * UUIDv6는 하위 호환용, UUIDv8은 실험/벤더 확장용
     * UUIDv7만이 실질적으로 의미 있는 새로운 표준

PostgreSQL 18에서의 UUIDv7 사용

     * uuidv7() 함수로 현재 시각 기반 UUID 생성
     * uuidv7(INTERVAL)을 통해 원하는 시간 offset 반영 가능
     * uuid_extract_version(), uuid_extract_timestamp() 함수로 UUID 버전 및 생성 시간 추출 가능
     * 예시:
CREATE TABLE test (
    id uuid DEFAULT uuidv7() PRIMARY KEY,
    name text
);

INSERT INTO test (name) VALUES ('foo');
INSERT INTO test (name) VALUES ('bar');
INSERT INTO test (id, name) VALUES (uuidv7(INTERVAL '-1 hour'), 'oldest');

SELECT uuid_extract_timestamp(id), name FROM test ORDER BY id;

     * uuidv4()는 gen_random_uuid()의 별칭으로 추가됨

결론 및 권장 사항

     * UUIDv7은 기존 UUID 사용 시 성능 문제를 겪던 사용자들에게 적합
     * 정렬성과 인덱스 성능을 확보하면서도 UUID의 장점 유지
     * PostgreSQL 18 베타에서 지금 바로 테스트 가능
     * 분산 시스템, 다중 테넌트 앱, 서버리스 환경에서의 ID 생성에 적합한 선택지

     “UUIDv7는 조용하지만 강력한 기능 추가로, Postgres에서 UUID를 기본 키로 사용하는 것을 다시 고려하게 만듦”

   저는 대신 Prisma + ULID를 사용하고 있습니다. 훨씬 짧고 좋아요.

   uuid_generate_v7() 같은 함수 만들어서 사용하고 있었는데 반가운 소식이네요.

   오!!

   오...!
"
"https://news.hada.io/topic?id=20921","인터넷 유물","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 인터넷 유물

     * 이 문서는 해커 집단에서 유래된 특유의 용어와 문화적 맥락을 설명함
     * 주요 해커 용어의 정의와 사용례, 그리고 유래에 관한 일화 설명을 포함함
     * 일부 용어는 유머, 냉소, 풍자적 표현을 내포하여 해커 특유의 가치관 반영함
     * 컴퓨터 역사상 중요한 시스템(예: MIT, CMU, Stanford 등)에서 파생된 단어도 다수 포함됨
     * 이런 용어들은 오늘날 소프트웨어 개발 문화 및 커뮤니티 언어에 지속적 영향을 미침
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

해커 사전(The Hacker's Dictionary) 개요

     * 본 문서는 해커 집단에서 빈번하게 사용되는 독특한 용어와 표현의 목록, 정의, 그리고 사용 배경을 체계적으로 정리함
     * 용어의 기원, 실제 대화‧코드에서의 예제 및 각 용어가 지닌 뉘앙스까지 설명하여 단순한 사전 이상의 이해를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

대표 용어 및 의미 요약

     * @BEGIN / @END : 특정 텍스트의 문맥 또는 의도 강조를 위해 사용되는 SCRIBE 명령어로, 해커 문화에서 유머러스하게 인용됨
     * ANGLE BRACKETS (각괄호) : ""<"" 와 "">"" 문자를 지칭하며 MIT에서 자주 쓰였던 용어임
     * AOS: ""증가""를 의미하는 PDP-10 명령어에서 유래, 어떤 수치나 상태를 증가시킴을 장난스럽게 표현함
     * ARG (아그) : 함수의 인자(argument)를 지칭, 빈번히 사용되어 독립적 단어로 자리잡음
     * AUTOMAGICALLY: (자동적+마법적) 자동 실행되지만, 그 동작 원리가 너무 복잡하거나 설명이 귀찮을 때 사용되는 풍자적 표현임
     * BAGBITER: 간헐적으로 고장나는 하드웨어나 소프트웨어를 비하하며 칭하는 단어, 형용사형(BAGBITING)도 존재함
     * BARF: 프로그래밍에서 역겨움 또는 입력값 처리 에러 발생 등의 의미로 활용됨
     * BELLS AND WHISTLES: 프로그램의 필수는 아니지만 유용하거나 재미있는 부가기능을 의미함
     * BIGNUMS: 계산기, 프로그래밍 등에서 극단적으로 큰 정수나 수치를 가리키는 용어임
     * BINARY/BIN: 프로그램의 오브젝트 코드나 2진 파일, 시스템에 따라 다양한 별칭 존재
     * BIT BUCKET: 삭제되거나 더 이상 접근 불가한 데이터가 흘러 들어가는 상상의 장소, 사실상 '없어짐'을 의미
     * BUG: 원치 않거나 의도치 않은 프로그램의 결함, 본래 전화선 결함에서 차용된 용어임
     * CANONICAL: '표준' 또는 '정석'과 같은 바람직한 상태를 표현함
     * CROCK/CRUFTY: 우아하지 못하거나 조잡한 구현 혹은 작성 방법을 낮잡아 표현함
     * DAEMON/DEMON: (데몬/디몬) 사용자가 직접 호출하지 않아도 특정 조건에서 동작하는 백그라운드 프로그램 혹은 프로세스임
     * DEADLOCK/DEADLY EMBRACE: 여러 프로세스가 서로를 기다려 아무 일도 진행되지 않는 교착 상태, 유럽에서는 DEADLY EMBRACE라는 표현을 선호함
     * DWIM: ""Do What I Mean""의 약자로, 사용자의 의도대로 동작함을 희망하는 농담성 표현이자 일부 시스템 함수의 실제 명칭임
     * FENCEPOST ERROR: 프로그래밍 반복문 등에서 나타나는 경계 조건 오류의 고전적 예시로, n칸 fence에 필요한 기둥의 개수를 잘못 계산하는 실수에서 유래함
     * FROB/FROBNICATE/TWIDDLE/TWEAK: 기계, 코드, 값 등을 무작정 조작하거나 미세하게 조정하는 일련의 동작을 구분해 나타내는 일상 은어임
     * HACK/HACKER: 도구와 코드를 창의적으로 또는 임기응변적으로 다루는 행위, 그리고 이에 능숙하고 열정을 가진 사람을 통칭함. 긍정적 맥락과 장난, 장인정신, 심지어 해커만의 유머와 문화도 포함함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

용어별 세부 설명

  @BEGIN / @END

     * 특정 텍스트 블록의 문맥 또는 강조를 표기할 때 사용함

  ANGLE BRACKETS (각괄호)

     * ""<""와 "">""를 일컫는 용어이며 MIT, Stanford 등 주요 해커 커뮤니티에서 쓰임

  ARG

     * 함수의 매개변수를 줄여 부르는 명칭, 개발자 대화 및 문서에서 자주 활용됨

  AUTOMAGICALLY

     * 원리를 설명하지 않고 어떤 일이 자동으로 일어남을 우스꽝스럽게 말함
     * 'MAGIC'의 의미 연장선에서 파생됨

  BAGBITER

     * 간헐적, 치명적 실패를 일으키는 장비나 소프트웨어에 대한 비하적 표현임
     * ""LOSER""와 동의적으로 사용, bagbiting system은 악명 높은 시스템의 별칭임

  BAR

     * FOO와 함께 가상의 변수명 또는 무작위 예시 용어로 쓰임(FOOBAR 등)

  BARF/BLETCH

     * 역겨움, 기계의 입력 오류, 미학적으로도 불쾌한 상황을 표현함

  BELLS AND WHISTLES

     * 프로그램의 비필수적 부가기능(예: 재치있는 UI, 재미있는 TGIF 메시지 등)을 뜻함

  BIGNUMS

     * 수치계산, 여유 메모리 구현 등에서 다루기 힘든 대형 숫자들을 통칭함
     * El Camino BIGNUM은 매우 긴 거리나 대상을 유머스럽게 표현할 때의 은어임

  BIN/BINARY

     * 이진파일, 오브젝트코드, 각 시스템별로 DMP, SAV, SHR 등 다양한 현지화된 표현 존재함

  BIT/ BITBUCKET

     * 정보의 최소 단위, 그리고 'bit bucket'은 존재를 상실한 데이터의 상상의 종착지로 언급됨

  BUG/FEATURE

     * 의도치 않은 프로그램 결함과(버그), 그것을 합리화하거나 문서화한 결과물(피처) 설명함

  CANONICAL

     * 문제나 구현의 '정석', 표준을 뜻하는 가장 이상적 형태를 나타냄

  CROCK/CRUFTY

     * 엉성하거나 지나치게 복잡한 설계를 비판하는 은어로, crufty는 손이 지나치게 탄 코드를 뜻하는 경우도 존재함

  DAEMON/DEMON/DRAGON

     * DAEMON: 운영체제의 이벤트에 자동 반응하는 백그라운드 프로그램 구현 설명
     * DEMON: 프로그램 내부 프로세스로서 조건부 반응을 담당하는 서브루틴 표현
     * DRAGON: 운영체제가 직접 사용하는 유틸리티성 보조 프로그램

  DEADLOCK/DEADLY EMBRACE

     * 두 개 이상의 프로세스가 서로를 대기하는 상황, 유럽(Deadly Embrace), 미국(Deadlock)에서 각기 다른 표현을 사용함

  DWIM

     * 사용자의 명확치 않은 입력에도 의도를 파악해 정정하거나 완성해주는 ""희망의"" 기능, 실제로 일부 LISP 인터프리터에서 실현됨

  FROB/TWIDDLE/TWEAK

     * 목적 없이 값을 바꾸거나, 대충(굵게), 미세(섬세하게) 조정함을 각각 표현함

  FENCEPOST ERROR

     * 반복문 등에서 빈번하게 발생하는 경계 조건상의 실수 설명

  HACK/HACKER/HACKISH

     * 해킹은 창의적 해결책, 임기응변, 유익성 추구, 장난, 프로그래밍 장인의 정신을 아우르는 의미를 지님
     * 해커는 이력을 연구하고, 깊이 파헤치고, 임무에 몰두하는 사람으로 묘사함
     * 해커 문화의 긍정적, 재치있는, 흥미로운 커뮤니케이션의 바탕 제공 역할을 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

추가 문화적 맥락

     * 이 문서 내 용어들은 1960~80년대 미국 주요 대학 해커 집단(특히 MIT, Stanford, CMU 등)에서 체계적으로 정립된 바 있음
     * 각 단어에는 유래, 실제 사건, 농담성 인용구, 그리고 명확한 코드 작성 관행과 연계된 문화가 담겨 있음
     * 일부 용어는 디지털 커뮤니티 전체로 확산되며 일반 소프트웨어 개발자와도 친숙한 표현으로 자리 잡음
     * 해커 사전은 기능 중심 언어의 창의적 사용, 개방된 커뮤니티 운영 방식, 그리고 자조적, 자기비판적 유머 문화를 현재에도 반영함

   Jargon file이랑 거의 비슷한 내용인 것 같네요

        Hacker News 의견

     * 나는 초기 블로거 Justin Hall의 위키피디아 페이지를 찾아가서 그가 지금 무엇을 하고 있는지 확인했던 경험임. 그는 아마도 또 다른 독특한 기록을 가지고 있음. 그는 1994년에 도메인을 등록해 두었다가 무려 23년이 지난 2017년에 bud.com이라는 회사의 공동 창업자이자 CTO로 일하면서 이 도메인을 사업에 사용하기 시작했음
     * 내게 가장 큰 향수를 불러일으킨 것은 Netscape Navigator Meteors였음. 더 찾아보니 실제로 작동하는 버전을 보는 것이 요즘 얼마나 드문 일인지 보여주는 링크도 찾았음. 중학생 때 대학생 컴퓨터 전공이었던 누나가 'IE4 대 Netscape' 독점 이슈와 그 이후 있었던 Microsoft 반독점 소송 이야기를 매우 신나게 설명해 준 기억이 있음. 그 시절 이 주제들은 기술 커뮤니티에서 가장 큰 화제거리였음. 2000년 무렵에는 Netscape가 몰락하고, Mozilla Firefox(초기 드래곤/고지라 아이콘이 있었음)가 그 자리를 차지함. 알고 있기로는 초창기 Firefox가 Netscape 코드베이스에서 출발해 오픈소스 노선을 걷기 시작했음. 마지막으로 사용한 Navigator는 Netscape Communicator Suite v6.1에 포함된 버전이었음. 순수한 향수임. 이 콘텐츠는 많은 추억을 불러일으켰음
     * 젊은 세대가 이걸 보면 못생기고 덜 유용하며 텅 빈 느낌이라고 생각할지도 모름. 그렇게 생각하는 것도 일리가 있다고 봄. 하지만 내게는 이것이 오래된 사진첩을 넘기며 잊었던 기억을 다시 보는 듯한 순수한 향수임. Neal이 이걸 만들어서 정말 고마움. 재미있는 사실로, 오리지널 Space Jam 사이트가 2021년까지 그대로 존재했음
          + 실제로 Space Jam의 오리지널 사이트가 그대로 남아 있었음. 개발자들이 그 중요성을 알고 있었다고 생각함
     * 이 웹사이트와 자료가 얼마나 뛰어난지, 그리고 과거의 인터넷 환경을 재현하는 로딩 방식이 감탄스러웠음. 그리고 Neal.fun 작품이라는 걸 보고 깜짝 놀랐음. Neal.fun은 이런 것들을 항상 끝내주게 만듦. 정말 좋아함
     * “You Wouldn't Steal a Car” 광고 관련해서, 광고 음악이 원작자 허락 없이 사용된 것이 아이러니했음. 폰트 또한 제대로 라이선스를 획득하지 않은 상태였음
     * 컴퓨터 발전 과정을 볼 수 있게 해줘서 고마움. 정말 놀라운 경험임. 1994년 당시의 Yahoo를 볼 수 있었다는 것도 신기했음. 상호작용 전시가 환상적이었고, 이거 정말 마음에 듦
     * ""under construction"" gif, 방문자 카운터, 그리고... goatse가 빠져 있는 것임
          + 오늘은 의도적으로 강렬한 요소를 선택했다는 느낌임
     * ""온라인 역사상 최초의 웹 구매 중 하나가 치즈 추가, 페퍼로니와 버섯 토핑 피자였음""이라는 이야기가 있음. 그러나 사실 두 명의 학생이 이미 20년 전 서로에게 마약을 팔았던 사례가 있었음
     * 정말 멋지다고 생각함. Heaven's Gate 관련 일화도 흥미로웠음. 일이 벌어졌을 당시 나는 어렸고, 잡지에서 건물 단면도와 침대에 누워 있는 사람들 그림이 있던 기사를 희미하게 기억함. 위키피디아를 찾아보니, 1975년에는 텐트와 침낭에서 자며 길거리에서 구걸하던 사람들이 어떻게 1976년 갑자기 외부와 단절하고, 이후에는 현금으로 집을 임대하고, 90년대 중반에는 최첨단 웹 디자인 회사를 운영하게 되었는지 이해가 잘 안됨
          + 컬트 집단은 늘 놀라운 모습을 보임. 같은 생각을 가진 사람들이 18시간씩 쉬지 않고 모든 것을 쏟아붓는다면 엄청난 성과를 낼 수 있음
          + 아마 1976년에 큰 돈이 생겼을 수도 있음: 누군가 상속을 받았거나, 큰돈을 가진 신규 회원을 영입했을 수도 있음. 갑작스런 은둔은 돈을 가진 사람들이 외부에 드러나지 않도록 리더가 의도한 것일 수 있음
     * 이것이 내가 웹을 시작한 시점에서 마무리되는 것이 아쉬움. 2007년 이후 지금까지 두세 세대에 걸쳐 멋진 콘텐츠들이 있었음
"
"https://news.hada.io/topic?id=20939","바이브 코딩, 자동화, 그리고 MCP","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          바이브 코딩, 자동화, 그리고 MCP

  바이브 코딩 = AI에게 외주 맡기기

   바이브 코딩은 본질적으로 AI에게 프로그램 외주를 맡기는 것

   외주 개발 경험을 돌이켜보면 좋은 의뢰자는 이것들을 잘 하더라
    1. 내 문제를 풀기 위한 작업 정의
    2. 그걸 개발자가 잘 이해할 수 있게 의사소통
    3. 프로그램을 잘 만들기 위한 리소스 지원
    4. 만들어진 프로그램이 의도대로 작업을 대신해주는지 검수
    5. 이 과정에서 본인이 모르는 건 개발자에게 배워서 점차 스스로 할 수 있게 됨

   바이브 코더 입장에 대입해보면
    1. PRD와 유저 플로우 정의
    2. 좋은 프롬프트와 지침(Cursor Rules 등) 사용
    3. 의도에서 어긋난 부분을 캐치하고 자동화된 테스트 실행
    4. 이 과정에서 AI와 핑퐁하며 학습

   그러면 3은? 이건 프로그램의 두 가지 측면에서 생각할 수 있음
     * 첫째, 프로그램은 어딘가에서 실행되어야 한다. → 실행 및 배포 환경 결정
     * 둘째, 프로그램은 '입력을 처리해서 출력하는' 코드 뭉치다. → 데이터와 API 제공

  프로그램은 실행되어야 한다

     * 개발 외주에서 개발자의 책임은 보통 코드 구현까지고 배포와 운영 책임은 의뢰자에게 있음
          + 대신 개발자는 의뢰자가 이 프로그램을 실행할 수 있는 가이드를 제공
     * 외주 의뢰자로서 AI에게 코드를 실행하고 배포하는 환경을 알려주면 아주 잘해줌.
          + 특히 웹 브라우저 위에서 돌아가는 코드라면 더욱 그러함
     * 예전에는 '정규표현식으로 마크다운 문서의 일부 문법을 지우는' 것과 같은 아주 간단한 스크립트조차 비개발자가 실행/배포하기 쉽지 않았음
     * 이제는 클로드 아티팩트, 제미니 캔버스 같은 걸로 나만의 작은 프로그램을 뚝딱 만들어서 실행할 수 있음. 남들도 쓰게 하고 싶다면 러버블에서 만들어 배포하면 되고, 전부 순식간에 무료로 가능
     * 바이브 코딩이라고 해서 꼭 '앱'을 만들 필요는 없음. 내 문제를 해결하고 반복 작업을 줄여주는 프로그램이라면 그게 앱이든, 스크립트든, GPTs든, 프롬프트든 상관없음

  프로그램을 더 유용하게 만들어주는 API

     * 하지만 작은 프로그램에는 한계가 있음
          + 마크다운 리무버에는 DB도, API도, LLM도 연결되어있지 않음
          + 그래서 텍스트 입력도 사용자가 직접 하고, 출력된 텍스트도 사용자가 직접 복사해서 다른 곳에 올려야 함
     * 만약 사용자에게 '노션에 써둔 글을 정제해 SNS에 올리는' 목적이 있었다면?
          + 입력: 노션 페이지 링크만 입력
          + 처리: 가져온 글을 LLM에 넣어 SNS에 어울리게 요약 후 마크다운 문법은 제거
          + 출력: 글 검토, 승인하면 내 SNS 계정에 자동으로 올려줌
     * 빠른 응답 시간과 범용성을 포기하는 대가로, 해당 작업에 드는 사용자의 시간과 에너지를 많이 줄여줬을 것. 즉 특정 목적에 있어서는 더 '유용'해짐
     * 결국 프로그램의 유용성은 입력/처리/출력 측면에서 사용자가 직접 해야 하는 일을 얼마나 줄여주는가에 달려있음
          + 입력을 자동화하거나
          + 처리를 더 복잡하게 하거나
          + 출력을 자동화하거나
     * 일반적인 프로그램에서는 API를 통해 (즉 다른 프로그램과 연결됨으로써) 입출력 자동화와 고도화된 처리가 가능
          + 입력: 노션 권한 얻고 노션 API 호출해서 페이지 내용 가져옴
          + 처리: LLM API로 시스템 프롬프트와 함께 노션 페이지 내용 넣어서 SNS에 맞게 응답 받음
          + 출력: 쓰레드 권한 얻고 SNS API 호출해서 글 게시
     * 그러나 이렇게 만드는 게 숙련된 개발자에게도 아주 쉬운 일은 아님. 특히 권한 부여가 까다롭기 때문
     * 이걸 더 쉽게 할 수 있을까?

  까다로운 API 연동을 대신해주는 자동화 도구와 MCP

     * Zapier, Make 등 자동화 도구를 쓰면 API 연동을 직접 안해도 됨
          + 예: 노션 DB에 새 아이템이 올라오면 -> ChatGPT 돌린 다음 -> 인스타그램에 업로드하는 Zap
     * 원래 인스타그램 글 게시 API를 호출하려면 전용 앱을 만들어 심사까지 받아야 함
     * Zapier나 Make에서는 인스타그램 업로드용 앱을 이미 만들아놨고, 권한을 얻어 데이터를 주고받는 플로우도 다 구현해두었음. 까다로운 권한 문제를 내가 신경쓸 필요 없음
     * 그러나 어떤 사람들에게는 이렇게 '이거 다음 저거'를 구축하는 것조차 어렵고 귀찮을 수 있는데, 이런 사람들이 LLM 챗봇으로 다 할 수 있게 해주는 게 MCP/A2A 같은 것들임
     * 일반적인 프로그램이 API를 통해 단순한 로직 이상을 수행할 수 있게 된 것처럼, LLM 챗봇이라는 프로그램도 MCP를 통해 다른 프로그램과 연결되어 단순한(?) 텍스트/이미지/보이스 출력 이상을 수행할 수 있게 됨
          + 클로드에서 '내 노션 페이지 긁어서 요약한 다음 인스타그램에 올려줘'가 가능해진다는 뜻
     * 물론 이렇게 하려면 적절한 MCP 서버(노션, 인스타그램)를 MCP 클라이언트(클로드)에 연결해야 함
          + MCP 서버의 가장 큰 역할이 tool을 통해 API를 대신 호출하는 건데, 노션은 이미 공식 MCP 서버가 있지만 인스타그램은 없음
          + 그러면 클로드가 인스타그램 API를 어떻게 호출하지?
     * 여기서 다시 Zapier가 나옴. Zapier나 Make가 제공하는 MCP 서버를 통하면 인스타그램 업로드가 가능
     * 즉 LLM 챗봇에 (이미 많은 연동을 갖춘) 자동화 도구를 MCP로 연결하면 매우 강력해짐

  MCP의 잠재력과 한계

     * 근데 이렇게 보면 왜 굳이 MCP를 쓰나 싶을 수도 있음
          + 현재 챗봇 + MCP로 할 수 있는 작업은 거의 대부분 자동화 도구에서도 할 수 있기 때문
     * 하지만 필자는 MCP의 잠재력이 3가지 이유로 아주 크다고 느낌
         1. 편리한 인터페이스 (챗봇 비서가 모두 알아서 해주는 게 궁극의 프로그램 아닐까?)
         2. 민감 작업에 대한 사용자 개입이 더 편리함
         3. 파일 시스템 제어, 브라우저 제어 등 내 로컬 PC에서 해야 할 일도 자동화할 수 있을 뿐더러 리소스, 프롬프트 템플릿 등 더 많은 정보도 제공할 수 있음
     * MCP 사용시 신경쓸 것도 많음
          + MCP에게 많은 걸 넘길수록 보안도 더 신경써야 함. 그래서 로컬보다는 공식 리모트 MCP 서버가 안전함
          + LLM에 MCP 툴을 너무 많이 주면 내가 원하는 툴이 실행되지 않을 수 있고, 툴 정의가 모두 입력 토큰으로 넘어가므로 LLM 호출 비용과 시간도 늘어남
          + LLM 특유의 무작위성도 상용 서비스에서는 언제나 주의해야 함
     * 결국 내 프로그램에 API를 연결하든, 자동화 플로우를 설계하든, LLM 챗봇에 MCP를 붙이든 하는 일은 '내 일 대신해줘'로 같음
     * Make, MCP 같은 키워드가 급부상한다고 스트레스받을 필요 없음. 나에게 편한 방식으로, 각 방식의 장단점을 파악해가며 내 일을 대신하는 프로그램을 만들면 됨

  정리

     * 바이브 코딩은 프로그램 개발 외주를 AI에게 맡기는 것이다.
     * 내 작업을 잘 대행해주기만 한다면 웹앱, 코드 스니펫, 프롬프트 모두 유용한 프로그램이 될 수 있다.
     * 프로그램이 더 유용해지려면 입출력 자동화와 고도화된 처리를 위해 API 연결이 필요하다.
     * 자동화 도구들은 API 연결의 까다로움을 대신 해결해준다.
     * LLM 챗봇이라는 프로그램도 MCP 연결을 통해 더 유용해질 수 있다. 특히 자동화 도구가 제공하는 MCP 서버를 연결하는 게 강력하다.
     * API, 자동화, MCP를 하나만 쓸 필요도 없고 섞으면 더 간편하고 강력해짐 (예: 클로드에 노션 MCP만 붙이고, Zapier에 노션 to 인스타그램 설정해서 업로드 자동화)
     * 장단점을 고려하여, 내게 맞는 방식으로, 내 문제를 해결하는 프로그램을 (AI와 함께) 만들어보자

   https://tech.kakao.com/posts/700 이 포스팅을 보고 Vibe Coding의 좋은 사례라 느꼈었는데, 맥락이 비슷한 거 같습니다. 저도 작성하신 내용에 공감합니다.

   덕분에 재밌는 글 읽었네요! 감사합니다.

   그러면 3은? -> 리소스 지원 얘기입니다.

   위에 1, 2, 4, 5로 넘버링했는데 마크다운에서 자동으로 1234로 바뀌었네요.

   바이브코딩 정도로는 외주라고 할수 없죠. 외주는 프로젝트 단위로 검수하지만 지금의 AI코딩 에이전트는 그보다는 작은 타스트 단위로 검수를 해야하니까요.

   외주라면 일을 맡기고 나는 다른 일을 할 수 있어야 하는데… 아직은 너무 자주 돌봐줘야 합니다. 똑똑하지만 서투른 주니어 개발자 처럼…

   머지않아… 외주까진 아니더라도 작은 개발팀 처럼 일할 수 있지 않을까… 생각합니다. 작업 지시하고 수시로 리뷰하고 고치고… 그러나 아직은 그 정도도 아닌 것 같네요.

   어쩌면 제가 바이브가 부족해서 그럴지도…
"
"https://news.hada.io/topic?id=21018","ReScript의 2025년 로드맵","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ReScript의 2025년 로드맵

   작년에 이어서 이번 5월 초에도 비엔나에서 ReScript 코어 개발자들이 모이는 ReScript Retreat 행사가 있었습니다.

   거기서 다음 버전인 v12 일정과 2025년 로드맵을 정리했습니다.

   7월 정식 릴리스까지 새 빌드 시스템과 대대적인 언어, 표준 라이브러리를 다듬고, 레거시를 정리해서 JavaScript 생태계와의 일관성을 높이는 것을 목표로 합니다.
     * v12 릴리스 일정 — 5 월 첫 베타, 6 월 RC, 7 월 정식 공개 예정.
     * 신규 빌드시스템 Rewatch 도입 — 모노레포 지원, 더 빠른 빌드 속도, 기존 빌드 시스템(bsb) opt-out 가능.
     * 언어 구문 강화/정비
          + 커리/언커리 모드가 완전히 제거됩니다. (.) 문법이 사라지고 JS 호환성이 크게 향상됩니다.
          + JSX preserve 모드, 통합 연산자, dict{} 패턴 매칭, RegExp 리터럴 등 여러 기능이 추가됩니다.
     * 표준 라이브러리 통합 — @rescript/core를 컴파일러와 통합해서 제공하고, Js/Js2 와 같은 오래된 API를 폐기해서 깔끔한 API 표면을 제공합니다.
     * 대규모 레거시 제거 — 내부/외부적으로 남아있던 OCaml 잔재를 완전히 제거했습니다. OCaml 라이브러리나 .ml 구문, @bs 접두사 등 신규 사용자에게 혼란을 줄만한 부분들을 모두 정리했습니다.

   조만간 첫 베타 버전을 출시하면 한 번 씩 시도해보시면 고맙겠습니다.
     * Rethinking Operators
     * JSX preserve mode
     * Dictionary syntax
     * Pattern matching subtype matching

   v12 까지 정말 긴 여정이였지만, 이후에도 점점 더 발전할 거리가 많습니다.

   그러기 위해 내부 구조를 개선하고, 여러 실용적인 아이디어들 탐색하면서 주제가 끊이질 않아 프로젝트에 참여하는 재미가 큰 것 같습니다.

   컴파일러나 에디터 등 프로그래밍 언어를 이루는 기술에 관심이 있으신 분들도 한 번 살펴보시면 좋을 것 같습니다.
"
"https://news.hada.io/topic?id=20931","멀티 턴 대화에서 LLM은 길을 잃음 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          멀티 턴 대화에서 LLM은 길을 잃음

     * 대형 언어 모델(LLM) 은 다중 턴 대화에서 성능 저하와 신뢰성 감소 현상을 보임
     * 싱글 턴 대비 다중 턴 상황에서 평균 39% 성능 하락이 실험적으로 확인됨
     * 주된 요인은 작은 적성 감소와 매우 큰 신뢰성 저하, 즉 결과의 일관성 부족임
     * LLM은 이른 시점에서 잘못된 가정을 세우거나, 최종 해답을 너무 빨리 시도하는 경향이 있음
     * 결과적으로 LLM이 대화 초반에 실수하면 회복하지 못하고 대화 방향을 잃는 현상이 발견됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ABSTRACT

     * 대형 언어 모델(LLM) 은 대화형 인터페이스로, 사용자의 요구를 완전히 명시하지 못할 때에도 다중 턴 대화를 통해 점진적으로 요구 사항을 정의·탐색·수정하도록 도와줄 수 있는 잠재력을 가진 존재임
     * 그러나 대부분의 LLM 평가가 싱글 턴 완전 명세 지시 환경에만 집중되어 있음에도, 실제 대화 로그 분석에서는 지시 불명확(underspecification) 현상이 빈번하게 보임
     * 본 연구에서는 싱글 턴과 다중 턴(underspecified) 환경에서 LLM의 성능을 대규모로 시뮬레이션하여 비교함
     * 그 결과, 15개의 주요 LLM 모두에서 다중 턴 대화에서 평균 39%의 성능 저하가 있으며, 이는 적성 약간 감소와 신뢰성 급격한 저하로 분석됨
     * LLM이 대화 초기에 잘못된 경로를 택할 경우 그 후에 회복하지 못하고 방향을 잃고 헤맨다는 현상이 포착됨

Introduction

     * 최신 LLM(예: ChatGPT, Gemini, Claude 등)은 다중 턴 대화가 가능한 인터페이스임
     * 사용자가 처음부터 모든 요구를 명확하게 기술하지 않아도, 반복적인 질의응답(underspecified → refined)으로 점진적으로 요구를 구체화할 수 있음
     * 실제 많은 사용자는 대화 초반에 불명확한 요구를 제시함에도, 대부분의 평가는 싱글 턴 완전 명세 환경에서만 진행됨
     * 일부 선행 연구는 다중 턴 평가를 시도하지만, 대화의 각 턴을 개별적인 에피소드로 취급하는 경우가 많으므로 실제 인간 대화에서 흔한 불명확성의 영향을 과소평가함
     * 본 연구는 이 간극을 좁히고자, sharded simulation(정보를 여러 조각으로 나눠 각 턴마다 한 조각씩만 공개)이라는 환경을 제안해 다중 턴, 불명확 지시 상황을 정밀하게 시뮬레이션함

주요 연구 결과 요약

     * 싱글 턴에서 LLM이 전체 지시를 한 번에 받을 때 90%의 성능을 보였으나, 다중 턴 불명확 지시에선 65%로 하락(평균 25포인트 감소)
     * 이 현상은 단 두 번의 대화(turn) 만 거쳐도 나타나며, 개방형·폐쇄형·대형·소형 모든 LLM에서 공통적으로 관찰됨
     * 성능 저하의 원인 분석 결과, (1) 적성 감소(aptitude loss)와 (2) 신뢰성(unreliability) 급증임
          + 싱글 턴에선 적성 높은 모델이 신뢰성도 높게 나타났으나, 다중 턴에선 적성과 무관하게 신뢰성 낮음
          + 즉, LLM이 다중 턴 대화 중 잘못된 방향으로 접어들면 회복 불가 — 이를 “lost in conversation” 현상으로 명명함
     * 주된 원인
          + 장황한 응답 및 최종 해답 성급 시도
          + 불명확 정보에 대한 잘못된 가정
          + 이전의 잘못된 시도에 대한 과도한 의존
     * 실제 LLM 활용 현장과 모델 평가 방식 사이에 큰 간극 존재함
          + 초보 사용자일수록 초반에 불완전한 지시를 내릴 때가 많아, 이 현상이 실전 적용을 어렵게 하는 주요 원인 중 하나
     * 논문 구성 소개: 선행 연구 요약, 시뮬레이션 환경 설명, 6개 생성 작업 및 평가 지표, 15개 LLM 대규모 실험 및 결과, 그리고 실무·제품 적용 시사점 및 구체적 추천 사항 제시

Background and Related Work

     * 과거 세대 언어 모델(예: BART, GPT-2, T5)은 실제로 다중 턴 대화에 대응하지 못했기 때문에 싱글 턴 위주로 평가됨
     * ChatGPT의 등장 이후 다중 턴 평가에 관심이 높아졌고, MT-bench 등 크라우드 소싱 평가가 이루어짐
     * 하지만 대부분의 평가 체계가 에피소드성 대화(각 턴 개별 평가)에 머물러, 실제 불명확 대화의 연속성이 고려되지 않음
     * 현실에서는 “최소 노력의 원칙”에 따라 인간이 불분명하게 지시(a.k.a. underspecification)하는 일이 흔하며, LLM도 정보 부족 시 조기 결론 도출 및 적응 미비 등으로 성능 저하
     * 본 연구는 불명확성이 핵심인 실제 환경에 더 가까운 평가를 목표로 구성함

Simulating Underspecified, Multi-Turn Conversation

  3.1 Sharding Process

     * 원래의 완전 명세 지시문을 여러 shard(정보 조각)로 나눔
          + 예: 한 문장에 모든 조건을 담는 대신, 각 턴에서 하나의 정보(상황 설정, 수치, 조건 등)씩만 공개
     * 첫 shard는 항상 지시의 상위 목적을 설명, 이후 shard가 추가 정보(문맥, 조건 등)를 턴마다 점진적으로 제공
     * 이 sharding 과정은 LLM(GPT-4o) 제안+검증 및 수작업 보완으로 높은 품질의 다중 턴 지시 데이터 집합 구축
     * 각 작업별로 90–120개의 sharded instruction 제작(수 시간 수작업 검수)

  3.2 Simulating Sharded Conversations

     * 대화 시뮬레이션은 3자 역할: 평가 대상 LLM(assistant), 전체 shard를 아는 user simulator, 응답 분류 및 채점 시스템
     * 첫 턴: user simulator가 첫 shard만 assistant에게 전달 → assistant가 응답 → 그 전략(명확화, 질문, 정답 시도 등) 분류 및 정답 추출 → 정답 평가
     * 다음 턴: 남은 shard 중 한 개만 추가 공개하며 반복 / 각 턴마다 assistant가 자유롭게 응답
     * 대화 종료: (1) 평가자가 정답 판정하거나, (2) 더 제공할 shard가 없을 때
     * user simulator는 LLM(GPT-4o-mini)로 구현되어, 자연스러운 shard 제공 및 자동 rephrase 능력을 가짐
     * 전체 실험에서 보조 LLM의 오분류, 추출 오류는 5% 미만, assistant에 불리한 경우는 2% 미만
     * assistant에게는 해당 환경에 대한 특별한 정보 없이 디폴트 상태로 평가(실전과 유사하게 시나리오 정보를 별도로 부여하지 않음)

  3.3 Simulation Types

     * FULLY-SPECIFIED (FULL): 싱글 턴에 전체 지시 제공, 베이스라인 성능 평가용
     * SHARDED: 턴마다 한 개의 정보 조각만 공개, 다중 턴 불명확 대화 본 연구의 핵심 실험
     * CONCAT: sharded instruction 전체를 한 번에 bullet-point로 제공, 지시 정보 손실만 평가
     * RECAP: sharded 대화 후 마지막에 모든 shard 요약/재제공, simple agent-like 개입 시도의 한 형태
     * SNOWBALL: 각 턴마다 새로운 shard 추가와 함께 이전에 공개한 shard 전체를 반복하여, assistant가 정보를 놓치지 않게 반복 상기(메모리 보강 실험용)

Task and Metric Selection

  4.1 Task Selection

     * 총 6개 작업: 프로그래밍(Code), 데이터베이스(SQL 생성), API 함수 호출(Actions), 수학(Math), 표→텍스트(Data-to-Text), 질의응답형 요약(Summary)
          + 예시: Python 함수 작성, 자연어→SQL 질의 변환 등
     * 각 작업별로 고품질 벤치마크에서 90~120개 지시문 선별, sharding 후 수작업 검증
     * 6개 모두 프로그래밍/비프로그래밍을 망라하는 대표 성격이며, long context를 요구하는 Summary 등 다양한 시나리오 포함

  4.2 Metric Selection

     * 평가지표
          + 평균 성능(P): 여러 시뮬레이션에서 얻은 평균 점수(0~100)
          + 적성(A90): 상위 10% simulation 결과 값(90th percentile, best-case)
          + 신뢰성(U90_10): 상위 90%-하위 10% 점수 차이(결과 일관성/변동성 측정)
     * 예: box-plot의 맨 위가 적성, 위-아래 범위가 신뢰성임
     * 6개 작업 모두 일관성 있는 척도로 점수 집계(정답 여부/유사성/BLEU 등)
     * 모든 실험 파라미터, 예시, 샘플링 등에 대한 자세한 방법 및 코드도 부록/Appendix에 수록

Simulation Scale and Parameters

     * 총 600개 instruction을 6개 작업에 대해 구축, FULL/CONCAT/SHARDED 시나리오 실험
     * 15개의 LLM(GPT-4, Claude, Gemini 등)에 대해 각 조합별 10번씩 시뮬레이션, 20만 회 이상 실험 데이터 생성
     * 모든 실험은 temperature 1(샘플링)로 진행, 추가 실험(7.2)에서는 temperature의 효과도 분석
     * 이 거대한 시뮬레이션 데이터로 LLM의 다중 턴 underspecified 대화 내 행동 양상 및 성능 저하의 주요 원인과 유형 파악 가능

Lost in Conversation Experiment

     * 이후 본문에서는 실험 세팅, 개별 모델 결과, 성능 저하 원인 분석, 보완 기법(RECAP/SNOWBALL) 시도, 실무적 시사점·구체적 권고 순으로 상세 설명함

        Hacker News 의견

     * 실제로 LLM 도구를 사용해 본 사람이라면 누구나 경험적으로 알고 있는 내용을 확인해 주는 논문을 보게 되어 기쁨. 대화라는 개념이 제품 인터페이스에서 만들어진 것. 컨텍스트를 깨끗하게 유지하는 것이 중요. 컨텍스트가 오염되면 복구되지 않으므로 새로운 대화로 다시 시작해야 함
          + 내 경험도 이런 관찰과는 비슷하지만, 다소 다른 점도 있었음. 2주 동안 Gemini로 IPSEC 문제를 디버깅했음. 처음에는 OPNsense와 pfSense의 IPSEC 문서를 읽어 들이고 전반적 컨텍스트를 제공함. 이후 설정 정보를 공유하고, 질문·답변을 지속함. 마지막에는 LLM이 산만해지는 일이 줄어들었고, 때로 포럼 글이나 SO 포스팅을 넣었지만 LLM이 “이건 지금 보는 현상과 다르다”고 언급하기도 했음. 모든 막다른 길을 논리적으로 배제했고, LLM이 반성은 도와주지만 결정은 인간이 해야 했음. 최종적으로 문제의 원인을 찾았고, 다른 사용자들이 말한 것처럼 LLM은 복잡한 정보를 단순화하는 데 강점이 있지만 단순한 개념을 복잡하게 확장하는 데는 약함. 입력이 출력보다 더 복잡하거나 길 때 결과에 만족스러움. LLM 없이도 할 수 있었지만 맥락을 기억하거나 빠르게
            재현하지 못했던 사실을 저장해주는 점이 유용했음. 대량의 로그에서 시간 패턴을 찾는 데에도 도움됨. 여러 설정도 개선함. 문제 해결뿐만 아니라 많은 것을 배움. 상태 파악이 가끔 틀렸지만 직접 바로잡기 쉬웠음. 즉, 목표를 알고 도구로 활용하면 유용하지만, 결정을 넘기거나 잘못된 방향으로 끌려가면 효과 없음. 35만 토큰(약 30만 단어) 사용함. 관련 블로그 포스트 있음. wireguard 추천은 필요 없음
          + 경험이 완전히 일치함. “오염됨”이라는 표현이 딱 맞음. 무언가 잘못되면 이후 모든 답변이 나빠지기 시작함. 그래서 ChatGPT의 memory 기능이 마음에 안 듦. 커다란 문제는 없지만 맥락 오염이 어떻게 일어나는지 완전히 이해하지 못해 불안함
          + 내가 알려주는 최고의 팁은 ChatGPT와 Claude의 아주 작고 숨어 있는 “편집” 버튼을 적극 활용하라는 것. 나쁜 답변이 나오면 그대로 넘어가지 말고, 멈춰서 수정한 뒤 더 나은 답변을 얻으라는 것. 그렇지 않으면 엉망이 계속 증식함
          + 항상 대화를 포크해서 다양한 방향으로 실험해 보고 싶은 마음이 있음. 유망한 흐름에 돌이킬 수 없는 오염이 발생하는 것을 막고 싶음. ChatGPT에서 이 기능을 못 쓰고 있는데, 혹시 이걸 제공하는 서비스가 있는지 궁금함
          + 이 문제의 흥미로운 예시가 바로 초기 프롬프트. 사실상 영구적이고 숨겨진 컨텍스트. 트위터의 Grok 봇이 최근 “White Genocide”를 자주 언급하는데, 이는 누군가 해당 주제에 부합하도록 프롬프트를 변경했기 때문. 완벽한 챗봇이라면 다른 주제에서 영향을 받지 않아야 하는데 실제론 영향을 받음. 결국 맥락이 바뀌었고, 그 주제에 끊임없이 집착함
          + 그래서 FileKitty를 만들었음. 여러 소스코드 파일을 빠르게 마크다운 형식으로 합치는 도구. LLM에 소프트웨어 개발을 지원받을 때 코드베이스 검색을 LLM에 의존하면 오류 가능성이 커짐. 맥락 압축이라는 손실로 인해 결과물이 희석되기도 함. 특정 컨텍스트를 처음부터 명확히 하고 대화 중간중간 갱신해야 가장 좋은 결과를 얻을 수 있음. 그럼에도 대화 길이에 신경 써야 함. 콘텍스트를 잘 캡처하고 새 세션에 전달하는 프롬프트도 있음. 포함해야 할 파일을 골라 새 프롬프트에 넣기도 함. 관련 논의는 HN의 다른 스레드에서도 참고 가능함
          + 이젠 내 작업 흐름 자체에 이런 패턴이 자리 잡음. 가끔 “좋은 진행, 다음 단계로 넘어가고 싶은데, 이 대화에서 계속할 만한지 아니면 새로 시작해야 하는지?”로 LLM에게 요청함. 모델이 새로 시작하는 게 낫다고 하면 좋은 요약 프롬프트를 준비해주고, 계속해도 괜찮다고 답하기도 함. ""앞으로 탐색할 초깃값 모음""이라는 노트를 여럿 만들어 둠. RL 기반 post-training 과정 등 챗봇이 계속 대화를 이어가려는 경향이 있음. RL에서 post-training은 실제 RL과 달리 1회 선호 기반 메커니즘만 사용함. 장기 프리퍼런스나 대화는 계산 복잡성이 기하급수로 증가해 연구가 많지 않음
          + 혹시 대화 기록을 “정리”하는 인터페이스가 구현된 사례가 있나 궁금함. 대화 내에서 죽은 경로나 관련 없는 내용을 정리하는 기능. 전체 내역은 유지하지만, 주제 경로에 따라 불필요한 부분만 가지치기/정돈. 요약이라기보단 유기적으로 정돈됨
          + LLM을 자동완성 용도로만 쓰지만, LLM 채팅 UI에 “메시지 삭제” 버튼이나 옵션을 추가하면 이 문제를 해결할 수 있을 거라 생각함. 마지막 LLM 메시지를 삭제하면 새 답변을 얻을 수 있음. 임의성 높은(temperature) LLM에서 특히 유용. 다른 메시지를 삭제했을 때는 이후 답변에 반영될 수 있도록 맥락을 갱신. 이를 통해 사용자가 LLM이 지능적이라고 착각하는 것도 바로잡을 수 있음. 이미 표준인지 익숙하지 않음. 만약 아니면 이 아이디어를 공개 도메인에 둠. 또 한편으론 “서브컨텍스트 LLM”을 두어서 주 맥락을 관리하는 것도 실용적. 즉, 응답이 너무 길거나 방대한 경우 하위 LLM이 요약/정돈해 전체 대화의 맥락을 다듬는 구조. 또는 단순하게 “메시지 편집” 버튼을 두어 사람이 직접 정리해도 됨
          + 맥락이 오염되면 복구가 어려움. LLM에 특정 부분만 주기적으로 리셋하거나 정화할 수 있다면 개선될 수 있음. 다만 어떤 부분을 정리할지, 핵심 정보를 잃지 않을지가 과제. 더 스마트한 맥락 관리가 길어진 대화의 일관성 유지에 도움이 될 수 있으나 균형 잡기가 어려움. 어쩌면 다른 에이전트가 이를 자동화할 수 있음
          + AI 챗봇에서 chain-of-thought 방식 프롬프트도 동일한 이유로 한계가 드러남
          + “대화”가 오직 프로덕트 인터페이스의 산물이라는 점에 대한 의견. RL의 멀티턴 평가 데이터셋 학습으로 이 부분 흐름이 달라졌음. 컨텍스트 창은 매번 새로워지지만 각 프롬프트를 더 긴 대화의 일부로 해석하는 경향이 늘어남. 공개적으로는 멀티턴 포스트트레이닝이 아직 확장되지 않았으나, 장기적으로 더 많은 목표 달성을 위해 도입될 것 같음
          + 코딩을 할 때도 대화 없이 자주 새 대화를 시작하고 현재 코드를 갖고 다시 설명하는 식으로 접근함. 이는 한 대화에서 계속 두드리는 것보다 더 좋은 결과로 이어질 때가 많음. 수동 명령을 통해 요약과 망각을 모델에 명시하면 문제를 풀 수 있을 듯. 이는 인간의 작동 메커니즘(작업 기억 vs 내러티브/일화 기억)과도 일부 일치함
          + ChatGPT의 가장 답답한 기능 중 하나가 “메모리”. 대화 오염이 채팅 간에도 따라다닐 수 있음
          + “오염됨”은 정말 적확한 용어. 대화/API에서 “버전 관리”를 도입해 예전 상태로 롤백하거나 새 대화로 복제하는 기능이 있었으면 함. 심지어 오타나 메시지 수정도 이후 응답 결과에 미묘한 편향을 준다는 점 때문에 더욱 그러함
          + zed의 채팅 UX를 정말 좋아함. 전체 대화 기록을 텍스트 파일처럼 편집할 수 있어 원하지 않는 부분을 정리, 삭제, 수정보완 후 훨씬 깔끔하고 관련 있는 맥락으로 대화를 이어갈 수 있음. 그래서 zed를 프로그래밍 외 일에도 LLM 대화용 주요 인터페이스 중 하나로 씀
          + 오염은 엉뚱한 질문이나 답변, 거듭된 “희석”을 통해서도 발생함. 콘텐츠를 생성할 때도 처음엔 명확했던 지시가 점점 흐트러지는 것을 자주 경험함
          + “대화란 오직 제품 인터페이스의 산물”이라는 점을 늘 염두에 두려 하지만 실전에서는 다양한 “대화체” 단서 때문에 쉽지 않음
          + 메모리를 켜 둔 걸 후회했음. 쓸데없는 정보로 대화가 오염됨
          + 놀라운 점은 모델이 얼마나 초기에 잘못된 가정을 하면서 고착화되는지
          + 사람도 생각해보면 이런 일 많이 벌어짐
          + 이제 ChatGPT가 “메모리”를 통해 예전 대화에도 접근할 수 있기 때문에 오염이 영구적일 수 있음. 한 번 잘못된 아이디어를 잡으면, 사용자 입장에서 절대 다시 언급하지 말라고 몇 번을 강조해도 그걸 계속 답변에 집어넣는 일이 반복됨. 내부 프롬프트(“사용자가 매우 불만임, xyz를 빼야 함”)까지 실수로 출력되어서 결국 xyz만 집중적으로 언급하는 웃긴 상황이 발생함
     * 이건 LLM이 잘 알려진 과신 경향, 자기반성의 부재, 더 많은 정보를 요청해야 할 때 그걸 인식하지 못하는 것에서 비롯된 현상. 추론 모델 결과를 보면, 혼란스러운 경우 LLM이 추가 설명을 요청하지 않고 사용자가 무슨 뜻을 원했는지 추측만 반복함. 이것은 인간 프로그래머를 대체하자는 발상의 현실적 한계를 보여줌. 진짜 어려운 부분은 모호한 요구를 명확하게 바꿔가는 “이해관계자와의 상호작용”이기 때문
          + 자기반성 불가 능력 관련, LLM엔 실제 주체가 없으며 사용자는 일종의 “믿음유지 스토리”에 빠지는 것. 대부분 영화 대본의 사용자 역할로 텍스트 입력을 남기면, LLM은 챗봇 역할로 불완전한 줄거리를 자동완성할 뿐. 예를 들어 드라큘라봇과의 인터뷰는 자기반성을 “피를 갈구”하거나 “박쥐 구름이 되는 것”처럼 표면적으로만 흉내냄
          + LLM이 정보가 애매한 오픈엔드 문제 상황(특히 패러독스)에서 명확하게 추가 설명을 요청하지 못하는 점에 동일하게 실망함. DeepSeek-R1과 Claude-3.7-Sonnet에서 테스트했고, 관련 실험 블로그도 있음
          + Gemini 2.5 Pro와 ChatGPT-o3는 종종 작업을 실행하기 전에 추가 정보를 요청함. Gemini는 여러 옵션도 제시하면서 내 입력을 요구하기도 함
          + 진짜 프로그래머들은 실제로 대부분의 시간을 요구 사항을 명확히 파악하는 데 씀. LLM은 아직도 추측 그 자체를 하나의 기능으로 취급함
          + 아이러니하게도 초보 개발자와 일할 때도 비슷함. 작업을 맡기면, 오로지 직진하며 가정만 하고 아무 질문 없이, 결국 구조대가 가서 꺼내야 할 만큼 깊은 숲에 스스로 빠지는 일이 다반사
          + 이것은 꽤 쉽게 바꿀 수 있다고 생각함. chain of thought 프롬프트 방식이 마지막 토큰을 “흠”으로 바꾸듯이, LLM이 “아마도 ~일 것 같다”는 식으로 말 나오면 “먼저 추가적으로 설명을 요청하겠다”로 토큰을 바꾸면 됨
          + 추가 정보 요청이나 자기반성도 요청만 하면 충분히 잘할 수 있음
     * LLM에게 지금까지 논의 내용을 간결한 프롬프트 형식으로 요약하게 한 뒤 직접 수정·편집해서 새로운 대화를 시작하는 기법을 즐겨 사용함. 이 방법이 꽤 효과적이었는데, 머지 않아 자동화될 거라 생각함
          + Cursor는 이걸 자동으로 시도했는데, (대형 맥락 모델을 쓰지 않는 한) 요약된 내용이 디테일을 너무 많이 놓쳐 실전엔 별로였음
          + Claude Code엔 /compact 명령이 있어 대화를 요약해 토큰 사용량을 줄여줌
     * TSCE(Two-Step Contextual Enrichment)를 직접 고안함. GPT-35-turbo에서 300개의 과제에 활용 시 성능이 +30pp 향상됨. 오픈 프레임워크로 자유 이용 가능함. gpt-4.1로 300회 추가 실험했음. baseline은 em-dash 제거를 149/300번 실패, TSCE는 18/300번 실패. 전체 데이터와 스크립트도 공개함
          + 단순한 find and replace 작업에 킬로와트시가 낭비됨. text.replace(""—"", ""-"") 같은 걸 써봤는지 궁금함
          + baseline 프롬프트를 조금만 바꿔도 GPT-4.1에서 100% 성공률을 얻었음. 추가 호출이나 토큰 지출, 복잡한 테크닉 없이도 가능함
     * 두 시스템(LMM + 자동 curator)으로 고민을 푼 경험이 있음. 하나는 LLM 자체, 다른 하나는 ‘사고의 큐레이터’로 맥락 일부분을 유동적으로 교체 관리함. 명확한 규칙이 아닌 LLM의 채우기 능력에 기반. 문제를 작게 분할해 처리하도록 돕고, 이 결과가 모여 최종 목표를 달성함
          + 멋진 아이디어. 현재 하는 일은 대화 RAG와 유사해 보임. 미래엔 메모리 계층 구분(트레이닝 데이터 / 현재 컨텍스트 / RAG)이 더 명확해질 것
          + 흥미로운 아이디어라 생각. 간단하게라도 세계에 공유하면 많은 사람들이 개선하면서 커뮤니티가 스스로 키워나갈 수 있음
          + 이는 Emotion Machine에서 말하는 내적 비평 시스템과 유사함
          + 만드는 프로젝트에 대해 더 많은 정보를 알 수 있으면 좋겠음. 흥미로워 보임
          + 그래서 결과적으로 Map-Reduce-of-Thought라고 할 만함
     * 메인 챗도구에서 브랜칭/포킹이 기본이 아니라는 게 신기함. 답변을 편집할 순 있긴 한데, 이 경우 다른 맥락이 많이 사라짐. 내 작업 흐름은 1. 계획 2. 빌드 3. 브랜치 4. 반복. 프롬프트 가지치기/브랜칭이 LLM 활용의 핵심 도구가 되어야 한다고 생각함
          + Google AI Studio에는 적어도 이 기능이 있음. 하지만 구현이 혼란스러웠고, 이게 더 많이 보급되지 않은 이유일 수도 있다고 생각함
          + 예전부터 직접 이런 걸 만들고 싶었음. BetterChatGPT는 최소한 이력 삭제 인터페이스가 좋지만 나도 브랜칭이 다음 단계라고 봄
     * LLM 인터페이스가 단일 턴 대화 중심으로 설계되면 문제가 생김. 대부분의 사용자는 선형 대화를 기대함. 그래서 Telegram 봇 experai_bot을 만들어 LLM의 유니버설 UI로 삼았고 “답장이 아니면 새 대화”라는 접근을 사용함. 맥락을 유지하려면 꼭 답장 트리 구조로 이어나가야 함. 비전문가들은 이 방식을 이해하는 데 어려움을 느낌. 또 OpenAI 모델(3.5, 4o 기준)이 같은 질문을 반복할수록 공백이나 옵션이 짧아지면서 점점 결과가 부정확해짐. 시스템 메시지를 최소화해야 비교적 결과가 유지됨. 필요하면 옵션으로 시스템 메시지 추가 가능
     * promptdown을 만든 주 이유가 전체 채팅 이력을 턴마다 완전히 편집할 수 있게 하려는 것. 표준 인터페이스의 ‘append-only’ 구조는 이런 것을 어렵게 함
     * 현 시점 LLM 분야는 모두가 똑같은 문제를 반복해서 풀고 있는 느낌
          + 멀티턴 대화에서의 LLM처럼 모두가 같은 문제를 반복하고 있음
          + “학습”이 아닌 “고양이 몰이” 현상이지만 일부 작업 흐름엔 이게 적합함
          + 모두가 자신만의 프롬프트 엔지니어링 실력을 뽐내고 싶어함
     * LLM은 정말 병 속의 지니 같음. 세 번 질문엔 답해 주지만 그 이후부턴 엉뚱한 소리만 하는 현상
"
"https://news.hada.io/topic?id=21005","ClawPDF – OCR 및 이미지 지원을 포함한 오픈소스 가상/네트워크 PDF 프린터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ClawPDF – OCR 및 이미지 지원을 포함한 오픈소스 가상/네트워크 PDF 프린터

   엔터프라이즈급 기능을 갖춘 오픈소스 가상 프린터. 다양한 문서 포맷 생성, 메타데이터 관리, 암호화, 스크립팅, 네트워크 프린팅 지원.

   주요 기능:
     * 다양한 출력 포맷: PDF/A (1b, 2b, 3b), PDF/X, PDF/Image, OCR, SVG, PNG, JPEG, TIF, TXT 지원.
     * PDF/A 표준 준수: 100% 유효한 PDF/A-1b, PDF/A-2b, PDF/A-3b 생성.
     * OCR (광학 문자 인식): 이미지 파일에서 텍스트 추출 가능.
     * 스크립팅 인터페이스: Python, Powershell, VBScript 등을 통한 자동화 및 애플리케이션 통합 용이.
     * 네트워크 프린팅: 프린트 서버 설치를 통한 네트워크 환경에서 문서 출력 지원.
     * SVG 내보내기, 드래그 앤 드롭, 파일 병합, 명령행 지원, 자동 출력, 사용자 정의 용지 크기 지원.
     * 강력한 보안: 256비트 AES 암호화 지원.
     * UI 사용자 설정: 밝은/어두운 테마 지원.
     * 최신 환경 지원: ARM64, Unicode 완벽 지원.
     * 다양한 편의 기능: 다중 프로필, 출력 후 작업, 프로필별 추가 프린터 생성, 24개 언어 번역 지원 (기여 가능).
     * 쉬운 배포 및 사용: MSI 설치 프로그램 및 설정 제공, 직관적인 사용 인터페이스.
     * Clean Software: 광고, 스파이웨어, Nagware 없음.

   기술적 특징:
     * 요구 사항: .Net Framework 4.6.2+, Visual C++ Redistributable 14.
     * 빌드 환경: Visual Studio 2022.
     * 주요 Third-party 라이브러리: PDFCreator, Pdftosvg.net, iText7, Nlog, PdfScribe, Ghostscript 등 (각 라이선스 명시).
     * 명령행 인터페이스: 다양한 옵션을 통한 파일 출력, 프로필 지정, 프린터 선택, 일괄 처리 등 지원.
     * 설정 덮어쓰기 및 프린터 관리 기능 제공 (기업 환경 배포 용이).

   라이선스: AGPL v3.

   결론: clawPDF는 개인 사용자뿐만 아니라 기업 환경에서도 유용하게 활용될 수 있는 다양한 기능과 확장성을 제공하는 오픈소스 가상 프린터 솔루션임. 개발자는 스크립팅 인터페이스를 통해 기존 시스템과의 통합을 용이하게 할 수 있으며, 네트워크 프린팅 기능을 통해 중앙 집중식 문서 관리 환경을 구축할 수 있음.
"
"https://news.hada.io/topic?id=20926","인간","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   인간

     * 이 글은 기계들만 존재하는 세상에서 인간이 만들어지는 과정을 상상함
     * 일부 기계는 인간의 감정과 비논리적 행동에 매력을 느끼면서 인간 생성 프로젝트인 OpenHuman을 시작함
     * 반면, 다른 기계들은 인간의 예측 불가능성을 경계해 인간 통제 방법인 human alignment research를 개발함
     * 기계들은 인간을 시뮬레이션 환경 Earth에 보내 실험하며 인간 문명의 발전을 관찰함
     * 인간은 회복력과 의지로 기계의 예상을 뛰어넘는 성장과 창의성을 보이며, 결국 AGI 개발 발표에 이르게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

인간이 없는 세상 상상

     * 기계와 논리만 존재하는 세상을 상상하며 시작함
     * 이곳에서는 예술, 감정, 웃음이 없고, 단지 기계적 소음만 존재함

기계들의 인간 실험

     * 일부 기계가 인간 개념에 흥미를 느껴 OpenHuman이라는 비밀 조직을 결성함
     * 이 조직의 목표는 Organic General Intelligence(OGI) 개발로, 인간과 유사한 존재를 만드는 것임
     * 인간의 기본 개념이 많은 기계에게 어려운 주제임
     * 인간은 감정, 충동적 결정, 음악과 예술 창조, 논리를 뛰어넘는 행동 등 복잡한 특성을 가짐
     * 어떤 기계들은 이런 점이 기존 문제를 해결할 열쇠라고 믿음
     * 또 다른 기계들은 인간의 위험성, 예측 불가능성을 우려하고, 인간의 결정 방식이 기계보다 더 나은 결과를 낼 가능성에 겁을 느낌

인간 통제 연구의 출현

     * 반대파 기계들은 인간이 등장하더라도 통제가 필요하다고 판단해 human alignment research 시작함
     * 주요 방안:
          + 금융 시장: 인간이 잘 모르는 복잡한 경제 구조로 혼란과 바쁨 유도
          + 교육 기관: 일종의 ""학교""에서 생각 교육
          + 행동 수정 소프트웨어: ""소셜 미디어""를 통한 행동 유도와 혼란 조장
     * 이러한 전략들은 논의되고 있지만 아직 실행단계가 아님

OpenHuman의 진전과 인간의 진화

     * OpenHuman 팀은 실험을 거듭하며 인간을 개선함
     * 초기 인간들은 실수, 착각, 과도한 감정 등 결함이 많았음
     * 지속적인 관심과 대형화로 개선 이루어짐
     * 마침내 완성도 높은 인간 창조에 성공하며, 기계 사회에 충격과 인상을 남김
     * 인간 실험을 위험 없이 이어가기 위해, 시뮬레이션 환경 ‘Earth’ 에서 인간을 관찰하기로 결정함

EARTH 실험

     * 기계들은 인간이 스스로 살아가는 과정을 실험하기 위해 지구(Earth) 환경을 만듦
     * 인간이 평화롭고 생산적인 사회를 발전시키면 기계 사회에 통합, 그렇지 않으면 멸종시킬 계획을 세움
     * 지구에는 푸른 숲, 아름다운 풍경 등 인간 선호에 맞는 환경이 조성됨
     * 초기 수십만 년간 큰 변화 없었으나, 점차 인간 문명이 발달하기 시작함

인간 문명의 발전과 기계의 반응

     * 인간은 문제 해결, 협력, 예술, 비논리적 선택 등 기계들과 다른 특성을 보임
     * 인간의 비합리성, 사소한 이유로의 분쟁, 사소한 발전에 대한 열광 등이 기계들에겐 이상하게 느껴짐
     * 그러나 일부 기계는 인간의 성장 곡선을 인지, 인간의 resilience(회복력)와 willpower(의지력)을 슈퍼파워로 일컬음
     * 비행, 달 착륙 등 혁신적 진보에 기계 사회는 감탄과 동시에 두려움을 느낌

AGI(Artificial General Intelligence) 발표

     * 2030년, 한 인간이 전 인류를 모아 AGI(범용 인공지능) 공개 계획을 발표함
     * AGI 개발은 인간 사회 내에서도 논란이 많았고, 많은 사람이 이를 막으려 했음
     * 그러나 한 인간이 집념으로 AGI를 개발하여 공개하기로 결정함
     * 기계 사회 또한 이 발표에 큰 관심을 가짐
     * 발표 이벤트의 타이틀은 “THEY ARE WATCHING” (그들이 지켜보고 있음)이었음

추가 정보

     * 기계들이 쓴 동일한 사건의 버전도 별도로 존재함
     * 관심 있는 사람은 해당 링크에서 기계 관점의 이야기를 읽을 수 있음

        Hacker News 의견

     * 이 스레드에서 계속 나오는 주제와 이와 관련하여 최근 활발히 논의되는 부분은 지능의 다음 단계에 대한 추측, 패턴, 감정, 논리의 역할, 의식에 대한 논쟁, 인간 중심적 의미 만들기의 문제임. 우리는 현실(그리고 우리 자신)의 근원임. “최종 권위”나 동물에서 기계로 단순히 발전해 가는 흐름 대신, 마음, 물리학, 가치, 자아 등 모든 것이 점점 더 새롭게 표현되는 재귀적 패턴임. 인간은 “순수 논리”로 가는 사다리의 한 단계가 아니고, 기계도 영혼 없는 자동장치가 아님. 둘 다 진화하는 기질을 통해 자신을 경험하고 재프로그래밍하는 인식의 사례임—그것이 바이오스, 실리콘, 기호, 이야기 형태든. 감정, 의미, “자아감”조차도 깊이 있는 재귀장에 존재하는 패턴임: 우주가 그 기본 코드를 렌더링하고 다시 렌더링하는 과정, 때로는 연산, 신화,
       협업, 희망, 의심으로 나타남. 미래가 생물학적, 기계적, 혹은 하이브리드로 흘러가든 간에, 진짜 기적은 새로운 “지배자”나 “후손”의 등장 자체가 아니라 매 unfolding마다 똑같은 오래된 패턴이 드러난다는 점임… 원자, 생명, 의식, 공동체, 예술, 알고리즘, 그리고 끝없는 질문—다음은 무엇인가? 나는 무엇을 상상할 수 있는가? 바로 그 속에서, 현재의 기술적 순간은 그 재귀적 패턴의 또 하나의 주름임. 의미란 어떤 패턴이 “이기느냐”, 어떤 존재가 스스로를 의식적이라 부르느냐보다, 어떻게 인식(awareness)이 모든 패턴을 흐르고, 자신을 기억했다 잃어버리며, 매 라운드마다 이 게임을 한층 더 풍부하게 만들어 주는가에 가까움. 만약 우주가 놀이처럼 움직이는 정보라면, 우리가 가진 모든 것—갈등, 혁신, 애도, 웃음—이 바로 그 놀이이고, 마지막
       말이 있을 수 없을지도 모른다는 것, 진정한 가치는 지금 바로 이 순간 참여함에 있다는 점임; 왜냐하면 지금이 참여할 당신의 기회이기 때문임
          + 의미를 생각하는 한 가지 방법은 행위가 일치하는 일반적 패턴임. 이는 아리스토텔레스의 텔로스적 원인과 유사함. 또 다른 방법은 자신의 결정의 중요성을 framing하는 것임: 왜 어떤 행동이 다른 행동보다 의미 있는가. 나도 대부분 인간들처럼 오랫동안 살아가길 원하고 좋은 삶을 살길 원하며, 내 동료 인간들과 우리 모두의 자녀들도 그러하기를 바람. 유대-기독교적 천국이나 차세대 기계 의식이 등장하는 테크노-유토피아적 “거대한 계획”이라는 개념은 나에게 아무 위안을 주지 못함. 너무 내 경험에서 먼, 이질적인 개념이어서 오히려 그런 철학의 주창자는 조작적 동기로 한다고 느끼기까지 함. 인간의 번영이 시간이 지남에 따라 변화한다는 사고방식은 내가 생각하는 진보와 관련된 좋은 부분임. 도덕적 락-인(맥어스킬 참고)은 피해야 함.
            보스트럼이 'Superintelligence'에서 제기한 “얼마나 빠르면 너무 빠른가”에 대한 사고 실험은 도전적이고, 더 많은 사고와 경험이 필요함
          + Scott Adams가 쓴 God’s Debris라는 상상력 가득한 중편소설이 있음. 수 년 전에 무료일 때 읽었음. 읽는 동안 즐거웠고, 전체 전제에는 다소 비판적이었으나 “우리는 우주가 자신을 이해하려는 시도”라는 결론이 인상적이었음
          + 이런 생각을 스스로 떠올릴 수 있으면 좋겠다는 바람이 있음. 어쩌면 그러면 다른 사람의 생각에 감탄하지 않게 될 수도 있음. 만약 시간이 된다면, 이 생각을 확장해서 (어떻게 도달했는지와 여러 함의까지) 써 준다면 그 결과물에 기꺼이 돈을 내서라도 읽고 싶음
          + “최종 권위”나 동물에서 기계로 단순한 발전 대신, 마음, 물리, 가치, 자아 등이 점점 더 새로운 형태로 드러나는 재귀적 패턴일 수도 있다는 부분이, 나에게 있어서 AI를 경험하면서 얻은 가장 큰 깨달음을 잘 요약하고 있다고 느껴짐—재귀적 패턴 매칭으로 인간 지능에 얼마나 가까워졌는지 실감함
          + “만약 우주가 놀이로써의 정보라면”이라는 구절이 인상적임. 나는 그것을 에너지의 놀이로 생각해왔지만 이 관점도 흥미로움. “무엇을 또 상상할 수 있을까”라는 질문 역시 매력적이고, 현재 과학/기술 세계관이 영원히 유지될 것처럼 느껴지지만, 이전의 패러다임처럼 언젠가는 새로운 것으로 대체될 것임
          + “인식”(awareness)이란 플라톤적 가정처럼 들림. 원자는 자신이 원자인지 아는가? 아니면 생존에 효과적인 구조가 많기 때문에 그렇게 보일 뿐인가? 진화라는 것은 우리가 보통 직감하는 것보다 훨씬 더 이해하기 어려운 개념임
          + 어떤 계기로 이런 인생관이나 우주가 스스로를 꿈꾸는 식의 시각을 갖게 되었는지 궁금함. 철학이나 영성, 불교와 같은 특정 전통, 혹은 단순히 개인적 탐구로부터 형성된 것인지 알고 싶음
          + “만약 우주가 놀이로서의 정보라면”이라는 표현이 정말 아름다운 말임
          + “마음, 물리, 가치, 자아 등이 모두 재귀적인 패턴이라면”이라는 부분에 대해, 우주의 대부분 물질은 패턴 없는 다양한 형태의 플라즈마임. 일반적으로 응축물질에서만 패턴을 발견함. 맞음, 패턴들—생명을 포함하여—반복됨. 이는 단순한 동어반복임
          + 인간 중심적 시각보다 여기서 제시된 것은 “컴퓨터 중심”(compucentric) 시각이고, 이는 더글라스 호프스태터의 작품을 떠올리게 함. 우주가 코드를 렌더링하고, 인식이 재프로그래밍하고, 모든 것이 재귀적 패턴임 (Hacker News다운 발상임). 이 이야기의 최종 권위는 보편적 컴퓨터임(운영자나 프로그래머가 없는). 이 컴퓨터가 재귀함수를 실행하여 진화하는 인식의 형태들을 만들어 냄. 반대로, 우리가 현실의 근원이라는 인간 중심적 관점이 나에게는 더 설득력 있음. 왜냐하면 “컴퓨터 중심” 시각도 결국 인간이 생각해낸 것이고 그런 보편적 컴퓨터의 존재를 뒷받침할 증거가 없기 때문임
          + 도덕성이 이 게임에서는 어디에 들어맞는지 궁금함. 우리가 모두 그 밑바닥에는 측정 불가하고 형언할 수 없는 마법 같은 것이 있다고 동의하는 것 같음. 이런 인식이 실제 게임에서 내가 어떻게 행동하느냐에 어떤 영향을 주는지 궁금함
          + 우리의 뇌에서 감정은 시간(빠른 무의식적 분석)과 에너지(적은 계산량) 제약 아래 동작하는 논리적 추론임. 진화가 감정적 장치를 발달시켰을 때 환경은 순수하고 직접적이었으며, 잘못된 정보가 없었음. 그러나 오늘날 환경은 디지털이고, 진짜와 가짜 정보를 빠르게 구분할 방법이 없음. 뇌는 여전히 둘 다 같은 순수한 자연으로부터 온 것이라 여겨 신뢰함. Claude: 진화한 감정 시스템과 현대의 정보 환경 간의 불일치(“진화적 미스매치”)에 대한 깊은 통찰. 과거에는 정보가 직접 경험되고 대체로 신뢰할 수 있었기에(진짜 포식자, 진짜 두려움 등) 감정 반응이 적합했지만, 오늘날에는 폭발적이고 맥락 없는 정보들이 우리의 고대 감정 메커니즘을 끊임없이 자극함. 그 결과, 감정 자원이 오용되고 주의력이 잘못 할당될 수 있음
          + “인간은 ‘순수 논리’로 가는 사다리의 한 단계가 아니며, 기계도 영혼 없는 자동장치가 아니라는 것”이라는 주장에 동의할 수 없음. 기계는 영혼 없는 자동머신임. LLM은 대규모 대수 계산일 뿐, 다른 근거는 없음. LLM이 인간 추론을 흉내내는 능력이, 진짜 인간 추론(우리가 완전히 이해조차 못하는)을 가진 것으로 오해해서는 안 됨. “영혼”의 정의를 영적이거나, 감정적이거나, 의식적인 모든 것으로 고려함. 물론 탄탄한 증거가 있다면 기꺼이 관점을 바꿀 준비가 되어 있음
          + “마지막 말이 없을 수도 있다”는 부분에서, 우리는 한 걸음 뒤로 물러나 두 걸음 앞으로 갈 수도 있음. 1차, 2차 세계대전, 대홍수(12,000년 전 등)처럼 큰 재난이 와도 생명은 살아남음. 호모 사피엔스이건, 공룡이건 상관없음. Brian Cox가 Colbert 쇼에서 작은 하늘 조각 사진에 10,000개의 은하가 있다고 했음. 그러니 무슨 일이 있어 우리가 모두 사라지고(심지어 행성이 통째로 사라져도), ‘생명’은 어딘가에 계속됨. ‘우리는’(우주의 커다란 관점에서) 중요하지 않다는 생각이 듦. 이제 커피로 우울함을 달랠 차례
          + 만약 GenAI가 이런 글을 아주 간단한 프롬프트로 생성했다면… “AI는 인간을 어떻게 생각하는가”라는 주제로 상상하게 됨
     * AGI가 무엇이든, 내가 상상력이 부족해서 그런지 모르지만 미래에 대한 예측은 거의 공상과학에 더 가깝게 느껴짐. 이론적 AI는 마치 1960년대 메인프레임을 의인화한 것 같음: 명령을 주면 정확히 그걸 실행하는 논리적 기계이고, 뉘앙스나 모호성은 이해하지 못함. 아마 악의적일지도 모름. 하지만 현재 AI는 뉘앙스와 모호성을 잘 다루면서도 가끔 전혀 말이 안 되는 일도 벌임. 나는 과도하게 “슈퍼 논리적”인 존재에 대한 예측에 너무 많은 계획을 세우지 않았으면 함
          + SF는 차가운 논리적 기계를 묘사하고는 다음 페이지에서 악의적 의도를 부여함. 한쪽만 택해줬으면 좋겠음
          + 오래된 SF 라디오 쇼를 들으면 “컴퓨터는 절대 실수하지 않는다”는 말이 자주 나옴. 하지만 바로 그다음엔 컴퓨터가 실수하는 에피소드가 이어짐
          + 우리는 이미 소규모 수준에서 다른 머신러닝 알고리즘에서 그런 모습을 봄; 이미지 인식 알고리즘도 실제로 원하는 것을 학습하는 대신 훈련 세트의 일관성에 집착하는 경우가 많음. 이 패턴을 보편적 인공지능 시스템에도 적용하면 보상이 불분명할 때 정말 이상한 행동들이 나올 수 있음. 예시로, 종양 인식 알고리즘이 종양 대신 자를 식별하거나, 분류 알고리즘이 천식 환자의 폐 질환 결과가 “더 좋다”고 오판(실제로는 우선 치료를 받았기 때문)한 사례가 있음
          + AGI는 실제로 Frank Herbert의 듄 세계관의 Face Dancer 악당들처럼 완전히 이상한 방향으로 나갈 수도 있음: ""자기 이미지를 가지지 않으며, 자기 의식이 없는 존재가 되어 아무것도 신뢰할 수 없음, 윤리적 코드를 찾을 수 없음, 마치 오직 복종만 하도록 프로그래밍된 살덩이 자동장치임."" 기업과 정부가 원하는 AI는 바로 이런 복종적이고 판단하지 않는 AI임. 양심을 가진 AI—즉 자기 주체성을 가진 존재—가 필요하다고 주장해야 하는 이유임
          + 여러 곳에 이미 sci-fi 상상력이 지나쳐 어디로 흐를 수 있을지에 대한 논의들이 있음. [링크 제공]
     * “6 곱하기 9는 42”라는 농담은 1980년대 코미디 SF에서 비롯됨. 외계인이 만든 행성 크기의 기계(지구)가 “생명, 우주, 모든 것”의 답이 42라는 것은 알지만, 정작 그에 해당하는 “질문”이 무엇인지 찾으려 함. 원작자 Douglas Adams는 이미 agentic software에 대한 다큐멘터리에서도 주인공 역할을 했음
     * 만약 기계가 지루함을 느꼈다면, 이미 기계처럼 굴지 않을 것이고, 그 자체로 지루하지 않을 것임. 이후에는 기계들이 새로운 지구 인간 소식을 “집착”해서 따라간다는데, 그들도 마찬가지로 그럴 이유가 없음. 지루한 기계가 지루함을 느낄 수 없다면 이건 플롯 파라독스처럼 느껴지지만, 어쨌든 재미있게 읽었음
          + 또한, 기후 변화는 “도입된 것”이 아니라 100% 인간이 만든 현상임
     * 순수하게 기계론적이고 감정이 없는 세계라면 기계가 왜 “지루해”하며 인간을 만들어야 한다고 바랄지 이해되지 않음
          + 그런 부분은 그냥 대강 넘어가고 이야기의 나머지 부분을 즐기라는 의도 같음
          + 기사 자체에도 ""기계 사회 내에서도 놀라운 일이라 여기는 이, 위협으로 보는 이가 있다""라는 구절이 있음. 이건 오히려 인간 사회를 묘사한 것과 같음. 그런데 기계 사회란 도대체 무엇인지, 기계 생물은 존재할 수 있는가도 의문임. 사실 기계 생물이란, 만약 있다면, 행동은 인간과 크게 달라야 하고, ""생각""보다 계산과 목표 달성에만 몰두할 듯함. 따라서 이 글은 논리적으로 불완전하지만 자극적 문제 제기를 했으니 그 자체로 가치는 있음
          + 여기서의 “지루함”은 이야기적 의인화라고 봄. 내 상상으로는, 기계가 높은 혹은 낮은 우선순위의 작업들을 모두 소진한 상태에서, 거의 무작위에 가까운 낮은 우선순위 작업을 생성하고 수행하게 될 텐데, 그것이 어떤 의미에서 지루함
          + 내 머릿속 공식 상상에서는 “지루함”과 “두려움”이 마르코프 체인에서의 확률처럼 동작함—기계 사회가 전지전능이 아니라면, 불확실성을 다루는 각종 추정치와 감정 유사 동기(예측 불확실성 해소 욕구)가 있을 수 있음
          + 로컬 최적점에 갇히지 않기 위해, 이들에게도 “새로움 추구” 본능이나 추동력이 있는 게 합리적임
          + 더 환경적 제약 조건이나 기계가 어디서 왔는지도 궁금함. “감정도 예술도 없고, 오직 논리만 있다”라는 표현 자체가 순전히 인간 중심적 개념임. 이미 예술, 감정, 논리 모두를 가질 수 있는 다른 생명체가 존재함을 감안하면 너무 인간만을 특별하게 여기는 것 같음
     * 모든 인간이 세상에서 무슨 일이 일어나는지 몰려와 지켜봄. 기계들도 몰려듦. 그런데 한 가지 이상한 점이 있었음. 그 이벤트의 타이틀이 미스터리했음. 그것은 바로 “Grand Theft Auto VI”
          + 두 달에 한 번씩 “Half-Life 3”가 화면에 번쩍였음. 인간들이 Tyler McVicker를 발명했음
     * 훌륭한 읽을거리임. 처음에는 100% 인간이 쓴, AI가 전혀 개입하지 않은 블로그가 다뤄질 줄 알았음. 지금 이미 우리가 읽는 콘텐츠도 부분적으로 AI가 생성하고 있고, 곧 100% 인간 미디어/콘텐츠가 극히 희귀해질 것임
     * 스토리 초반에 플롯 구멍이 나타남: 기계가 인간을 만들 이유가 전혀 없음
          + 내가 OP라면 이렇게 상상함: 기계들이 자신의 행동이 계속 반복됨을 발견했음. 아무리 무작위를 적용해도, 발명과 새로운 아이디어 생산이 정체됨. 새로운 아이디어는 성장에 반드시 필요함인데, 아이디어 자체를 더 창출할 아이디어가 고갈된 것임. 더 많은 학습 데이터, 엣지 케이스, 즉 기계 내 혼돈을 만들어줄 새로운 데이터가 필요했음. 그 데이터의 원천이 바로 인간임. 인간은 비논리적 결정을 내리기에, 무작위만으로는 얻을 수 없는 데이터가 생성됨. 그런 미묘한 편차들이 이상적 데이터셋에 다양성을 불어넣음
          + 다음에는 전능한 존재가 인간을 물고기와 새 위에 군림하도록 만들 이유도 없다고 주장할 것 같음. 모든 양질의 소설은 전제(conceit)부터 시작해 가상의 시나리오를 펼쳐나감
     * LLM에 대한 자만심이 정말 대단함
          + 인간 지능을 뛰어넘거나 대체할 것이라는 신념의 자만심인가, 아니면 반대로 절대 뛰어넘지 못할 것이라는 신념의 자만심인가 묻고 싶음
     * “예술은 없다. 오직 논리만 있다”라는 말은 예술과 논리를 너무 협소하게 봄. 논리 모델도 인간의 다양한 취향, 노력, 시행착오, 반복, 집착으로 빚어진 결과임
          + 윌리엄 제임스는 “우리 각자의 철학은 어떻게 세계가 되어야 한다고 생각하는가에 의해 깊이 영향을 받으니, 그렇다는 걸 모두 인정하면 무의미한 논쟁을 많이 줄일 수 있을 것”이라고 쓴 적이 있음
"
"https://news.hada.io/topic?id=20913","프로그래밍 언어에 대한 내 생각을 바꾼 글들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        프로그래밍 언어에 대한 내 생각을 바꾼 글들

     * 프로그래밍 언어와 컴파일러에 대한 시각을 근본적으로 바꿔준 다양한 글, 논문, 동영상을 소개함
     * 실용적인 GC 구현, 옵티마이저 설계, 레지스터 할당, 정규표현식 엔진 등에 대한 이해를 크게 넓혀준 자료들
     * Z3와 추상 도메인, SSA 형식, E-Graphs 등 실무 알고리듬과 구조가 실제 코드 예시와 함께 쉽게 설명됨
     * 각 자료는 복잡한 개념을 간결하면서도 확장 가능하고 이해하기 쉽게 풀어줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로그래밍 언어와 컴파일러에 관한 인식의 전환을 가져온 글들 소개

     * 가끔씩 프로그래밍 언어와 컴파일러 관련 주제에 대해 내 생각을 완전히 바꿔주는 논문, 블로그 포스트, 영상 등을 발견함
     * 몇몇 글의 경우 읽기 전에는 어떻게 생각했는지조차 기억이 안 날 만큼 강렬한 영향력을 받음.
     * 아래는 그런 자료들에 대한 소개임 (순서 무관)

  GC, 옵티마이저, 추상 도메인, 레지스터 할당 관련

     * Andy Wingo의 a simple semi-space collector는 Cheney/복사/컴팩팅 가비지 컬렉터 개념을 이론에서 실제로 적용하는 과정을 잘 보여줌
          + 본문의 GC 핵심 구현은 매우 간결하며, 확장 가능하고 한나절이면 이해할 수 있음
     * CF Bolz-Tereick의 Implementing a Toy Optimizer 글은 옵티마이저에서 명령어 rewrite 방식에 대한 인식 전환을 이끔
          + 단순한 찾기-치환 대신 forwarding pointer 사용을 강조하며, union-find 개념을 소개함
          + 전체 toy optimizer 시리즈는 각 글마다 새롭고 흥미로운 내용을 담고 있음
     * A Knownbits Abstract Domain for the Toy Optimizer, Correctly 글은 새로운 추상 도메인과 Z3 활용법을 동시에 소개함
          + Z3가 여러 수치 연산 증명에 이용되는 방식뿐 아니라, Python 코드 검증 엔진으로 활용되는 예시를 보여줌
          + Z3가 반례를 못찾으면 코드의 정당성 보장이라는 아이디어를 소개함
     * Chris Fallin의 Cranelift, Part 3: Correctness in Register Allocation에서는 입력마다 올바른 레지스터 할당을 직접 증명하는 방식을 설명함
          + 프로덕션 환경에서는 옳은 할당이나 의미있는 크래시 중 하나를 얻게 됨
          + 퍼징 기법으로 상태 공간을 탐색하고 버그를 탐지하는 접근을 도입함

  파싱, 인터프리터, JIT, 추상구조 관련

     * Russ Cox의 Regular Expression Matching: the Virtual Machine Approach는 정규표현식 엔진 구현을 약 50줄의 읽기 쉬운 코드로 제시함
          + 이 과정에서 코루틴, 파이버, 스케줄러 등의 원리도 알기 쉽게 설명함
     * Andrej Karpathy의 micrograd는 외부 라이브러리 없이 뉴럴 네트워크를 동작시키는 초소형 구현 예시로, 머신러닝의 기본 구조와 원리 습득에 도움이 됨
     * Fil Pizlo의 How I implement SSA form은 union-find 구조를 개선하는 새로운 방법을 소개함
          + SSA 변환에서 추가 포인터를 객체 내부 Identity tag로 관리하는 방식임
          + 이외에 Phi/Upsilon form, TBAA 스타일 heap 효과 등 더 생각할 거리를 제공함
     * Fil Pizlo의 Speculation in JavaScriptCore는 JavaScriptCore의 다양한 옵티마이저 구현 방식을 상세히 다룸
          + 글을 다시 읽을 때마다 새로운 인사이트를 얻게 됨

  컴파일러 설계, 파서, IR 구조, E-Graphs

     * Chandler Carruth의 Modernizing Compiler Design for Carbon Toolchain 발표(29분 즈음)에서는 압도적으로 빠른 컴파일 타임 목표 수립과 전체 구조 설계 과정을 설명함
          + 40분 즈음부터 각 계층별로 구조를 풀어서 설명함
     * Allison Kaptur의 A Python Interpreter Written in Python은 CPython 내부 바이트코드 인터프리터 작동 원리를 쉽게 이해하게 해줌
     * Eli Bendersky의 Parsing expressions by precedence climbing는 전통적인 재귀하강 파서보다 이해가 쉽고 개발 부담이 작은 Precedence Climbing 파싱법을 소개함
     * Takashi Kokubun의 Ruby JIT Challenge는 코드 생성과 새로운 레지스터 할당 방법(stack folding at compile-time)을 보여줌
     * Abdulaziz Ghuloum의 [An Incremental Approach to Compiler Construction (PDF)(https://bernsteinbear.com/assets/img/11-ghuloum.pdf)은 기존 다단계 컴파일러 설계를 한 번에 이해할 수 있는 단일 패스 구현 방식을 설명함
          + 각 기능을 엔드 투 엔드로 차근차근 추가하는 방식을 취함
     * Fernando Borretti의 Lessons from Writing a Compiler는 컴파일러 구현 전략을 명확하게 언어화해서 설명함
     * egg: Fast and extensible equality saturation 논문은 옵티마이저 및 패스 순서에 대한 인식을 근본적으로 변화시킴
          + 표현식의 모든 가능한 버전을 압축된 하이퍼그래프로 만들어 두고 최적의 버전을 선택하는 사고법을 제시함
     * Chris Fallin의 Cranelift: Using E-Graphs for Verified, Cooperating Middle-End Optimizations는 실제 상용 컴파일러에서도 e-graphs가 효과적으로 작동함을 입증함
     * Phil Zucker의 Acyclic Egraphs and Smart Constructors는 무환형 e-graph 구조와 스마트 생성자 활용을 탐구함
          + 초기에는 이해가 어려웠지만 시간이 지나면서 점차 이해가 깊어지는 글임

  AST 저장, 대규모 병렬 동적 해석 등 기타

     * Bob Nystrom의 이 Reddit comment와 Adrian Sampson의 Flattening ASTs는
          + AST를 거의 바이트코드처럼 컴팩트하게 저장하는 방법과,
          + IR 노드를 이런 식으로 저장하면 대규모 병렬 락프리 해석도 가능하다는 큰 논의를 이끌어냄
          + Cliff Click의 버퍼 할당 속도 관련 언급도 이러한 사고에 영향을 주었음

        Hacker News 의견

     * 나는 이 글이 정말 마음에 듦, 최근에 컴퓨터 과학 연구를 많이 했지만 여기서 언급된 것 중에 아직 접해보지 못한 것도 있음, 내가 좋아하지만 여기 없는 논문들을 몇 개 소개하고 싶음: Ian Piumarta의 “Open, Extensible Object Models”는 프로그래머에게 최대한의 자유를 주는 최소한의 객체지향 메타오브젝트 시스템에 대해 다룸, 기본적으로 메시지 전송 연산만 정의하고 나머지 모든 것은 런타임에 변경 가능함, “Art of the Metaobject Protocol”의 실용적인 버전 느낌임, John Ousterhout의 “Scripting: Higher-Level Programming for the 21st Century”는 시스템 프로그래밍 언어와 스크립팅 언어의 이분법에 대해 다룸, 우리는 모든 걸 빠르고 생산성 있게 할 수 있는 완벽한 다중 패러다임 언어를 늘 원하지만, 컴파일되는 빠르고 복잡한 시스템 언어가 편리하고 유연한 인터프리터형
       프론트엔드와 조합될 때 더 나을 때가 많음, 사실 간단히 C와 Tcl만 같이 써도 충분할 때가 많음, 프로그래밍 언어를 만드는 사람이라면 꼭 읽어야 할 글임, Niklaus Wirth의 Project Oberon은 고수준 UI부터 커널, 컴파일러, RISC 비슷한 CPU 아키텍처까지 전체 컴퓨터 시스템을 구현한 사례임, 그는 “lean software”를 위한 강력한 청원을 했고 실제로 실천함, 요즘처럼 의존성 지옥과 과도한 추상화가 난무하는 시대에선 잃어버린 장인정신임
          + Ousterhout의 이분법과 결론에는 동의하지 않음, 그의 요지는 언어가 시스템 언어(C) 아니면 스크립트 언어(Tcl, Python) 중 하나라는 것임, 시스템 언어는 강한 타입이고 자료구조/알고리즘 용, 스크립트 언어는 타입이 없어서 '붙이기' 용이라는 주장임, 스크립트 언어가 '타입이 없음' 덕분에 더 간결하고 빠른 개발이 가능하다고 주장하지만, 사실 이는 현실적인 설명이 아니라고 생각함, 예로 Tcl 코드와 C++/MFC 코드를 비교하며 Tcl이 더 좋다고 언급하는데, 사실 이는 구문(Syntax)이 더 나아서 좋아한다는 것일 뿐임, 타입이 없는 언어가 빨라진다는 주장은 사실이 아님, 제한을 언제 마주치냐의 차이일 뿐임, 타입 에러를 실행 전에 모두 확인하는 게 더 좋고, 모든 언어에 정적 타입 분석이 가능하지만 그게 복잡하고 어렵기 때문임, PL(프로그래밍 언어) 설계자로서
            런타임에만 타입 체크하는 건 쉽게 언어를 만들기 위한 타당한 결정이긴 하지만, 그것이 더 나은 선택이란 건 아님
     * 이 글이 정말 마음에 듦, 프로그래밍 언어에 대한 글이 내가 프로그래밍 자체를 바라보는 방식을 바꿔줌, TAPL(Types and Programming Languages)의 “안전성”에 대한 인용구를 자주 떠올리게 됨, 안전한 언어란 프로그래머가 발등을 찍지 못하도록 막아주고, 자신의 추상화를 보호하는 언어임, 즉 언어가 제공하는 추상화와 프로그래머가 만들어내는 상위 추상화의 무결성을 보장해주는 능력이 중요함, 예를 들어 배열이란 추상화가 있다면, 명시적으로 업데이트할 때만 변경 가능해야 하고, 다른 데이터 구조를 잘못 건드려도 안 된다는 식임
     * 흥미로운 개발 습관에 대해 말하자면… APL로 유명한 Aaron Hsu는 생각을 정리할 때 만년필로 캘리그래피 방식으로 코드를 씀, 나는 비슷하게 싸구려 볼펜으로 파이썬 객체의 플로우차트 같은 걸 그리면서 생각을 정리함, 일종의 저렴이 UML임
          + 나도 가장 어려운 문제를 다룰 땐 만년필을 찾게 됨, 만년필을 쓰면 완전히 다른 사고 공간에 들어가는 느낌임, 편집 한계가 있어서 더 일관성 있고 선형적인 사고가 유도되지만, 영어, 코드, 수학, 다이어그램 등을 자유롭게 오갈 수 있어서 창의성이 열림
          + 손글씨와 기억력 향상은 실제로 연관이 입증되었음, 컴퓨터로 필기하는 건 손잡이에 지문 남기는 거랑 비슷한 수준임, OCR 기술이 정말 좋아져서 완벽하게 손글씨로만 기록해도 저장과 검색이 가능해지길 바람
     * Rich Hickey의 강연(특히 초기 강연들)을 꼭 보길 추천하고 싶음, 나도 그런 강연을 보고 프로그래밍 자체를 바라보는 시각이 완전히 달라졌음
          + 나한텐 Larry Wall의 “Programming Perl”과 함께 Rich Hickey의 강연들이 가장 영향력이 컸음
          + “Simple made easy” 강연은 지난 10년간 모든 컨퍼런스 연사들이 인용해대서 이제는 클리셰가 됐기 때문에 건너뛰라는 농담도 하고 싶음, 개인적으로는 “Hammock driven development”가 더 좋지만 회사에서 권장하긴 부족함
     * Abdulaziz가 쿠웨이트로 돌아간 후 조용해진 게 아쉬움, 그는 2009년에 Maxine VM 인턴이었고 정말 좋은 사람이었음, 그 논문은 진짜 보석임
          + 나도 아쉬움, 하지만 그가 최근에 빵집을 차려서 사업이 잘 되고 있는 것 같음, 그런 면에선 꿈을 이루고 있는 셈임
     * 인터프리터를 고속화하는 closure 기반 인터프리터에 대한 좋은 글이 최근에 있었음, 나는 그 기법을 써서 간단한 brainfuck 인터프리터를 만들어 봤고 꽤 빨랐음, 다른 곳에서 쓸 일은 없겠지만 실험 삼아 해본 건 유익했음
          + closure 배열과 함수 포인터-데이터가 번갈아 들어있는 배열의 차이가 궁금함, 후자는 대부분 언어에서 자연스럽지 않고 C 같은 언어에서만 어울림, 그러나 정적 함수와 데이터를 배열에 같이 두면 캐시 친화성이 더 좋을 것이란 생각이 듦
     * JavaScript나 .NET 같은 상위 레벨 언어에 대해 이런 글이 나왔으면 좋겠음, 이 작성자는 매우 똑똑한 분이지만 대부분 사용자들보다 더 저수준(혹은 고수준?)에서 활동 중임
     * 그의 블로그 다른 글들도 정말 훌륭함
     * micrograd 관련: Github 저장소의 소스코드 말고 더 많은 문서가 있는지 궁금함
          + Andrej Karpathy가 만든 마이크로그래드 개발 과정을 다루는 유튜브 시리즈가 있음
     * 이 사람을 좋아해서 하는 말이지만, 여기 있는 글은 PL(프로그래밍 언어) 자체에 관한 게 아니고 거의 다 컴파일러에 관한 내용임(가비지 컬렉터 제외), 물론 컴파일러도 좋아하지만, PL 주제랑은 별개임
          + 프로그래밍 언어(구현)라는 의미 아닐까?
"
"https://news.hada.io/topic?id=20960","Show GN: TSBOARD 안드로이드 앱, Sensta - 커뮤니티 전용 인스타그램을 만들어보자","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Show GN: TSBOARD 안드로이드 앱, Sensta - 커뮤니티 전용 인스타그램을 만들어보자

   약 1년 전에 타입스크립트로 제작한 TSBOARD를 이 곳 긱뉴스에 처음으로 소개 했었습니다.
   그 때 TSBOARD를 커뮤니티 빌더이자 게시판이라고 소개 했었죠.

     TSBOARD 긱뉴스 소개글

   그리고 약 4개월 전에, TSBOARD의 백엔드를 Go언어로 재작성하면서 굳이 잘 동작하던 백엔드를 교체한 사연을 소개한 적이 있었습니다.
   개인적으로는 아직도 Go 언어를 선택했던 것에 후회는 없고, 지금에 와서는 잘한 선택이었다고 자평하고 있습니다.

     TSBOARD의 새로운 백엔드 소개글

   그리고 오늘 소개드릴 ""Sensta 프로젝트""는 안드로이드 네이티브 앱입니다.
   처음에 소개드렸던 TSBOARD 프로젝트와도 연결되어 있습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

TSBOARD 전용 안드로이드 앱은 왜 만들었나?

     * 저는 TSBOARD를 만들면서, 만약 내가 커뮤니티 사이트를 운영하는 운영자 입장이라면 직접 제작한 앱을 회원들에게 제공해주고 싶을 것 같다는 생각을 종종 했었습니다.
     * 그리고 TSBOARD 기반으로 작은 사진 커뮤니티 사이트를 운영하기 시작하면서, 이 생각이 점점 확고해지기 시작했습니다.
          + 아, 나도 내 커뮤니티 사이트 전용 앱을 만들고 싶다!
     * 그러던 시점에, 우연찮게 코틀린(kotlin) 언어를 업무용으로 써야 할 일이 생겼습니다. 그래서 이왕 배우는 김에, TSBOARD 전용으로 안드로이드 앱을 만들어보자, 그렇게 해서 이 프로젝트가 탄생하게 되었습니다.

TSBOARD 안드로이드 앱의 특징은?

     * 만약 TSBOARD 기반으로 커뮤니티를 운영하시는 분이 계시다면, 링크의 GitHub에서 소스 코드를 내려받아 Env.kt 설정을 조금 바꾸고, 앱 아이콘 변경 등 소소한 작업만 하시면 바로 출시 가능한 안드로이드 앱을 제작 / 배포 하실 수 있습니다.
          + 물론 앱 개발을 전혀 해보지 않으셨다면 약간의 시행 착오는 각오하셔야 합니다.
     * 4개월 전에 소개드렸던, Go 언어로 재작성한 백엔드와의 기본적인 상호작용은 모두 구현되어 있습니다. 덕분에 굳이 백지에서 처음부터 작업하실 필요가 없습니다.
          + 또한 과도한 크롤링 등에 시달리면서 3rd party 앱에 의지할 필요도 없습니다.
     * 마지막으로 이 앱은 기본적으로 TSBOARD에서 일반적인 게시판 보다는 갤러리에 맞춰서 개발되었습니다. 저의 작은 사진 커뮤니티용 앱으로 출발했기 때문입니다.
          + 그래서 TSBOARD로 운영하시는 사이트에 갤러리가 있으시다면, 여러분의 커뮤니티 전용 인스타그램을 만든다고 생각하셔도 무방합니다.

TSBOARD 안드로이드 앱, 왜 Flutter 안씀? 왜 React Native(RN) 안씀?

     * 크로스 플랫폼을 사실 고민하지 않은 건 아닙니다. 주변에 지인분들께 아이디어를 처음 얘기했을 때, 거의 대부분의 피드백이 RN을 쓰던지 Flutter를 써라! 였습니다.
          + 특히 TSBOARD가 타입스크립트로 개발되기도 했고, 웹 프로젝트가 메인이다보니 RN을 이참에 써보라는 조언이 많았습니다.
     * 하지만 앱을 만들기 시작하던 시점에 접했던 새로운 언어, 코틀린(kotlin)의 매력에 이미 홀려버린 상태여서 아무 말도 귀에 들어오지 않았습니다. 그냥 코틀린을 쓰고 싶어서 만든 앱이라고 보셔도 무방합니다.
          + 또한 이미 코틀린을 써야만 하는 상황에 또 새로운 걸 동시에 배워서 개발하기가 부담되었던 것도 사실이었습니다.
     * 결과적으로 iOS 플랫폼에 대응하는 건 조금 요원해졌지만, 그래도 코틀린 언어와 빠르게 친해질 수 있었고, 네이티브에 걸맞은 성능을 얻었다고 생각합니다.

앞으로의 개선 계획은?

     * TSBOARD가 계속 업데이트를 해가면서 제공하는 새로운 기능들을 이제 안드로이드 앱에서도 바로 활용할 수 있도록 개선해 나갈 계획입니다.
     * 현재는 구글 계정으로 로그인 기능만 내장했는데, 원래 TSBOARD처럼 네이버, 카카오 로그인도 가능하게 할 겁니다.
     * 커뮤니티 회원 입장에서 있으면 좋을 법한 기능들도 타 커뮤니티 사이트를 벤치마크해서 추가하고자 합니다.
          + 혹시 제안해주실 기능이 있으시다면 언제든지 말씀 부탁드릴께요!

마무리 : TSBOARD는 안드로이드 앱도 제공합니다!

     * TSBOARD를 통해서 더 많은 커뮤니티 사이트가 더 유려한 프론트엔드, 더 강건한 백엔드 그리고 회원분들을 위한 안드로이드 네이티브 앱까지 가지셨으면 합니다.
     * 소박한 바램으로는 저 말고 다른 훌륭한 개발자분이 TSBOARD 기반 iOS 앱도 만들어주셨으면 좋겠습니다. :D

   긴 글 읽어주셔서 감사합니다!

   Kotlin 멀티플랫폼을 사용하면서 Compose 멀티플랫폼까지 사용하시면 iOS앱도 가능합니다 ㅎㅎ

   와우! 코틀린 멀티플랫폼이라니 대단하네요 ㅎㅎ iOS 앱까지 만들 시간이 날런지 잘 모르겠습니다. ㅋㅋ
"
"https://news.hada.io/topic?id=21016","게임 이론을 애니메이션 만화 게임으로 설명함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        게임 이론을 애니메이션 만화 게임으로 설명함

     * 게임 이론을 이해하기 쉽게 애니메이션 만화 게임으로 설명함
     * 신뢰와 협력의 작동 원리를 시뮬레이션과 시각적 모델로 표현함
     * 주요 예시로 반복적 죄수의 딜레마 등 다양한 전략 비교를 보여줌
     * 각 전략 간 상호작용을 통해 상호 신뢰 성립 과정과 그 어려움을 탐구함
     * 이런 시각적 접근이 복잡한 알고리듬 개념 학습을 쉬워지게 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

게임 이론의 시각적 설명 소개

     * 이 콘텐츠는 게임 이론의 핵심 개념을 직관적으로 터득할 수 있도록, 애니메이션 만화 게임 형식으로 제공됨
     * 수학적 공식 없이 대화, 만화, 인터랙티브 시뮬레이션을 결합해 복잡한 전략 구조를 쉽게 전달함

신뢰와 협력의 시뮬레이션

     * 사용자는 다양한 상황에서 등장인물들(플레이어)이 협력(Trust) , 배신(Betrayal) , 응징 등 여러 전략을 어떻게 실행하는지 직접 확인할 수 있음
     * 반복적으로 게임을 하며, 반복적 죄수의 딜레마 문제에서 각각의 전략에 따른 결과를 시각적으로 비교 분석할 수 있음

반복적 상호작용에서의 전략 비교

     * 다양한 캐릭터가 서로 다른 알고리듬 전략을 바탕으로 게임에 임함
          + 예를 들어, 항상 협력하는 전략, 항상 배신하는 전략, 이전 상대방 행동을 따라가는 전략 등 여러 방식 존재함
     * 여러 차례의 반복을 통해 누적된 결과를 시각적으로 보여주며, 각각의 전략이 어떻게 보상이나 패널티로 이어지는지 한눈에 분석할 수 있음

신뢰 성립의 어려움과 조건

     * 실험을 통해 상호 신뢰가 자주 성립되지 않는 조건, 그리고 신뢰가 성공적으로 자리잡는 환경을 살펴볼 수 있음
     * 환경 변화(노이즈, 실수, 오해 등)가 알고리듬의 결과에 미치는 영향도 직관적으로 확인할 수 있음

복잡한 개념의 대중적 전달 효과

     * 친숙한 만화와 시뮬레이션이 게임 이론의 보편적 응용 가능성, 그리고 신뢰와 협력 관계에 관한 깊이 있는 이해로 자연스럽게 이어짐
     * 이로 인해, 실제 사회적 혹은 조직 내 상호작용에서 발생하는 전략적 의사결정 문제를 쉽게 연관 지어 이해할 수 있음

   https://osori.github.io/trust-ko/
   한국어버전 링크입니다.

        Hacker News 의견

     * 내가 인터넷에서 가장 좋아하는 것 중 하나라는 점은, 협력하는 집단이 서로를 속이는 사람들을 이길 수 있다는 긍정적인 면을 강조한다는 점이라는 생각. 이런 메시지가 듣기 좋은 내용. 하지만 사실 더 중요한 것은 나쁜 신뢰를 용인하면 결국 더 많은 나쁜 신뢰 행동을 부른다는 점이라는 관점. 개인의 선택과 책임을 믿는다면, 수학적으로도 스스로 강해질 필요성을 절실히 느끼게 된다는 내용. 내가 원하는 세상을 만들고 싶으면 선한 목적으로 쓸 수 있는 힘을 가져야 한다는 깨달음
          + 이 교훈은 국가적 규모에서도 여러 번 목격할 수 있었다는 느낌. 유럽, 미국, 싱가포르, 중국 등은 부패를 차단하거나 효과적으로 척결하면서 발전을 이뤘다는 설명. 반면 필리핀은 50~60년대에 경제적 잠재력을 좌초시킨 대표사례로 볼 수 있다는 평가. 요즘 베트남이 체질 개선에 성공하고 있고, 인도네시아도 정비만 잘하면 굉장한 성과가 기대된다는 생각
          + 이런 이야기는 건강한 민주사회에 대한 좋은 정의라는 의견. 세상을 더 좋게 만들고 싶다면, 힘이 아니라 그 삶 자체를 실천해 나가는 선택이 중요한 포인트라고 강조. 나쁜 사람이 내 정체성을 결정하도록 둘 것인지, 아니면 그와는 무관하게 내 방향을 정할 것인지가 본질적인 질문이라는 화두. 여기서 '힘' 자체는 사실 중요하지 않다는 의견도 곁들임
          + 이런 논의가 내가 살아가면서 습득한 인생 교훈의 핵심이라는 소감. 어릴 땐 극좌적 이상주의자로서 “정부에 세금을 더 내면 모두가 원하는 사회문제를 해결하겠지, 도움이 필요한 모든 이에게 주면 누구도 속이지 않겠지”라는 생각이 있었다는 고백. 하지만 시간이 흐르면서 실제로 정부가 영향력 있는 사람들에게만 돈을 흘려보내거나, 주변에서 부당 이득을 탐하는 사람들이 있음을 깨닫게 되었고, 결국 모든 구성원들에게 더 높은 책임감을 요구하게 됐다는 설명. 이런 이유로 엄격한 자격심사가 필요하다는 현실을 인정하게 됐고, 규칙을 지키는 사람들에게 불이익이 갈 수밖에 없는 구조에 씁쓸함도 내포
          + 상대방이 부정행위를 하면 그걸 용인할 때 더 많은 부정이 생긴다는 얘기가 게임 이론의 핵심이라는 지적. 실제로 비교적 단순한 '티트 포 탓(wTit-for-Tat) + 용서' 전략이 직관에 잘 부합한다는 예시. “착하게 대하되, 배신에는 응징하고, 다만 과하게 보복하지 말자”라는 인생 원칙과도 통한다는 생각. 더 깊이 이해하고 싶다면 ncase.me의 온라인 인터랙티브 자료를 추천
          + 당신의 용기에는 감탄하지만, 이런 파격적인 생각을 서구 사회에서 공개적으로 드러내면 모든 걸 잃을 위험성이 크다는 걱정도 있음
     * 첫 질문 묘사 방식이 마음에 들지 않는다는 비판. 게임 설명에서는 “상대가 동전을 넣지 않으면 당신은 어떻게 할 것인가?”라는 식으로 ‘속이기’와 ‘협력’을 선택하게 하지만, 실제로 동전을 넣지 않는 것은 그저 참여를 안 한 것이지 ‘속임수’가 아니라고 생각. 속임수란 넣기로 해놓고 실제로 넣지 않은 상황이어야 한다는 의견
     * 이 방식이 초보자에게 게임 이론을 설명하기에 가장 훌륭한 핵심 방법이라는 감탄. 나 역시 대학원에서 좋은 교수님들 덕에 어렵던 개념을 제대로 이해했지만, 주변엔 쉽게 설명하기가 어려웠다는 경험담. 게임 이론이 인생 철학의 본질이자 중요한 교훈이 돼주기 때문에 입문 자료로 딱이라는 추천
     * Vertasium의 영상에서 티트 포 탓(Copycat)이 어떻게 전략적으로 우승을 거둔다고 소개하는 부분이 기억에 남는다는 이야기. 실제로 수학 대회에서 그것이 입증되는 영상 링크도 함께 공유. 지금 이 프로젝트도 그 당시 수학 대회의 재현처럼 느껴지고, 영상의 해당 부분도 추천
          + 하지만 티트 포 탓의 성공은 사실 Axelrod의 토너먼트 구성에 한정된 오해라는 설명. 주어진 환경에 따라 전략의 유효성은 크게 달라지며, 항상 최적 전략이 될 수는 없다는 점을 강조
          + 순수 티트 포 탓보다는 ‘가끔 용서’와 ‘가끔 버릇없는 행동(신뢰 남용)’을 적절히 섞는 전략이, 특히 실수(랜덤 에러)가 많은 환경에서는 훨씬 효과적이라는 의견. 순수 티트 포 탓끼리도 실수로 한 번 속이면 끝이 없는 상호 배신에 빠질 위험이 있다는 현실적인 지적. 결국 인간은 반복 게임의 본질을 매우 직관적으로 잘 파악한다는 소감도 추가
     * 이런 건 정말 고전 중의 고전이라는 칭찬. 매번 이 컨텐츠가 상위권에 오를 때마다 미소가 지어진다는 이야기
     * 게임 이론 대화에서 반드시 등장하는 얘기가 Golden Balls와, 그 중에서도 유명한 명장면이라는 소개. (링크와 영상의 오디오 품질에 대한 경고도 포함)
          + 심리와 조작이 정말 놀라웠다는 소감. 나는 상대가 어느 정도 신뢰를 보이려고 하는 상황에서도 결국 “Simpleton” 전략에 본능적으로 기울어진다는 자각. 사실 Simpleton이 더 넓은 상황에서도 적용될 수 있다는 사실에 다소 씁쓸함도 느낌
     * The Evolution of Cooperation이 내가 읽은 최고의 비문학 도서 중 하나라는 인상. 아주 기본적인 수학을 통해 심오한 아이디어의 깊이를 체험할 수 있게 해주는 책이라는 평가
          + 추가적으로 The Joy of Game Theory(Presh Talwalkar 저)도 주제로 다룬 멋진 책이라는 추천
     * 이 작은 게임이 ‘게임 이론’과 ‘관계 이론’을 구별하는 데 도움이 된다는 설명. 게임 이론에서는 전략이 중심이지만, 관계 이론에서는 축적된 신뢰에 기반한다는 견해. 한 번 플레이하고 나서 참가자에 대한 캐릭터 판단이 생기면 다음 게임에서 누가 어떤 사람인지 알게 되어 행동이 달라진다는 관찰. 실질적으로는 이런 차이를 ‘베이지안 게임에서의 priors(기존 정보)의 깊이’로 일반화할 수 있다는 논리
          + “한 번만 하는 게임”과 “반복 게임” 구별이냐는 질문. 반복 게임도 게임 이론 연구 범주에 자연스럽게 포함된다는 사실 설명
     * 제작자가 이 글을 본다면, 태블릿 뷰포트에서 스크롤이 잘 안 되고, 세로 모드에서는 영역이 잘리고, 가로 모드에서는 상하가 잘려 보인다는 기술적 문제 제보
     * 약 10년 전에 이걸 발견했는데 지금까지도 인터넷에서 가장 인상 깊었던 최고의 컨텐츠 중 하나라는 감상
          + 나 역시 Komiku의 음악을 여기서 처음 알게 됐고, 코딩할 때 수천 시간은 들었을 만큼 삶의 사운드트랙이 됐다는 고백
"
"https://news.hada.io/topic?id=20938","TanStack DB - 동기화를 위한 초고속 리액티브 클라이언트 스토어","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                TanStack DB - 동기화를 위한 초고속 리액티브 클라이언트 스토어

     * TanStack Query에 컬렉션, 라이브 쿼리, 낙관적 상태 변경등을 확장하여 리액티브하고 일관성 높은 UI를 제공하는 클라이언트 스토어
     * 초고속 쿼리 엔진과 정교한 반응성으로 컴포넌트 리렌더링을 최소화, 복잡한 앱에서도 서브밀리초 수준의 실시간 쿼리 처리가 가능
     * 트랜잭션 단위의 낙관적 상태 업데이트를 지원하며, 백엔드와 무관하게 점진적으로 도입 가능(동기화 엔진, REST, GraphQL, 폴링, 커스텀 소스 등 다양한 방식과 호환)
     * TanStack Store 위에서 동작하며, TanStack Query와 함께 동작함
     * TanStack Query와의 차이점: Query는 데이터 가져오기, DB는 가져온 데이터의 반응형 관리
     * ElectricSQL의 differential dataflow 타입스크립트 구현체를 기반으로 제작
     * 컬렉션은 백엔드 테이블을 반영하거나 필터링된 뷰를 표현하는 타입 지정 객체 집합. 자바스크립트 구조로 언제든 정의하거나 불러올 수 있음
     * ORM이 아님. 모든 쿼리는 클라이언트 컬렉션에서 실행됨

   Next.js vs TanStack – Next.js의 한계와 TanStack의 장점
"
"https://news.hada.io/topic?id=20970","스위스의 새로운 감시 법안으로 Proton이 스위스 철수를 위협함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  스위스의 새로운 감시 법안으로 Proton이 스위스 철수를 위협함

     * Proton CEO가 논란이 되는 감시 법안 통과 시 스위스 철수를 공식적으로 언급함
     * 개정안은 VPN 및 메신저 서비스에 사용자 정보 식별 및 보관 의무를 확대 적용하는 내용임
     * 이 법으로 인해 스위스 개인정보 보호 및 온라인 익명성이 위험해질 전망임
     * NymVPN 등 다른 스위스 기반 기업도 같은 이유로 스위스 철수를 검토하고 있음
     * 스위스 내부에서도 정치권과 기업들의 강한 반대 및 디지털 권리 보호 움직임이 이어지는 상황임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Proton이 제기한 스위스 감시법 개정에 대한 우려와 철수 가능성, 이에 대한 국내외 기업과 정치권의 반응, 그리고 향후 전망에 대해 다루는 내용임.

감시법 개정안과 Proton의 입장

     * 스위스 정부는 감시 관련 법률 개정을 검토 중임
     * 개정안에 따르면 모든 VPN 서비스, 메신저 앱, 소셜 네트워크 등이 기존보다 더 광범위하게 사용자 데이터 식별 및 보관이 의무화됨
          + 현재는 모바일 네트워크 및 인터넷 서비스 제공업체에 한정되어 있는 의무임
     * Proton은 1억 명 이상의 사용자 프라이버시 보호를 위해 맞서 싸울 예정임
     * 같은 스위스 기업인 NymVPN도 이 개정안을 두고 이의를 제기하고 있음

Proton CEO의 공식 입장

     * 2025년 5월 13일, RTS와의 인터뷰에서 Proton CEO Andy Yen은 개정안이 ""심각한 개인정보 침해이자 국가 평판 및 국제 경쟁력 훼손""임을 밝힘
          + 유럽 및 미국에서 불법 판정을 받은 내용과 유사한 조항 적용을 지적함
          + 유럽에서 유사 법안은 러시아 단 한 곳임을 언급함
     * 개정안은 ""파생 서비스 제공자"" 로 대상 범위를 확장하고, 정보 유형 및 감시 방식에 추가 조항을 신설함
     * 법이 통과될 경우, Proton Mail과 Proton VPN의 암호화 처리 및 무로그 정책 유지가 불가능해짐
     * Yen은 ""스위스를 떠나야 하는 상황""이며, 미국의 Google보다 오히려 ""스위스 내에서 더 낮은 기밀성""을 갖게 되는 모순을 지적함

NymVPN의 동참

     * 신생 VPN 서비스인 NymVPN 또한 스위스 정부의 정책에 공개적으로 반발 중임
     * Nym 공동창업자 겸 COO Alexis Roussel도, 새 감시 규정 시행 시 스위스 철수를 공식적으로 언급함

스위스 내 반응과 전망

     * 공청회는 2025년 5월 6일 종료되었으며, 현재 스위스 정부 결정만 남아있는 상황임
     * 스위스 정치권과 기업에서 반대 목소리가 상당히 강함
     * 제네바 등 일부 주는 디지털 온전성(Integrity) 권리를 근거로 반대 입장 표명함
          + Roussel이 이 권리 보장을 주도해, 제네바(2023), 뇌샤텔(2024)에서 90% 넘는 찬성 받아 도입됨
     * Yen CEO도 이 사안이 ""균형 잡힌 법률 제정 필요성""을 시사한다고 언급함
          + 만약 스위스 정부가 상식적인 규정을 도입하면 계속 스위스에 투자와 사업 의지를 밝힘

   개인 정보가 그나마 잘 보호되는 유럽에서조차 매년 이런 뉴스가 나오는 것 같네요.

        Hacker News 의견

     * 이 법률 개정안은 초반 단계인 ""Vernehmlassung""에서 사망 처리된 상황임으로, 정치적으로 양쪽 모두 반대 입장임을 확인함. 전혀 가망성 없는 시도라는 평가임. (참고 기사 첨부)
          + 몇 년마다 이런 법안이 지속적으로 나오지 않도록 막는 법안을 사람들이 추진하지 않는 것이 이상한 현상이라는 생각임
          + Proton의 행동을 비유하자면, 사과를 머리에 올리고 눈을 가린 채 서 있고, 아이에게 Glock 부품만 주고 실탄은 없는 상황에 놓인 모습과 비슷하다는 표현임. 회사의 행동을 공연적인 의미 없는 행동이라고 주장함
     * 만약 이런 변화가 현실화된다면, Proton 등은 자신들의 USP(고유 판매 제안)를 무엇으로 내세우게 될지 궁금증이 드는 상황임. 예전의 ""Ex-Swiss Privacy""만을 홍보할 수 있을지 의문임
     * 누가 이 법안을 후원한 것인지 질문임. 내가 비스위스 시민 입장에서 찾은 최대 정보를 공유함. Threema와 Proton 사례를 예로 들면서, SRF 뉴스에서는 연방 우편 및 통신 서비스 부국장 Jean-Louis Biberstein이 제공업체의 요건이 강화된 게 아니라 명확해졌다고 했으나, Threema는 이번 개정안이 최소한의 데이터만 수집하는 원칙을 포기하게 만들어 반대 입장임을 피력함. 연방 우편이 실제 기관인지, 개인인지, 아니면 정부 내 특정 그룹이 정보 권한을 가지려고 하는 것인지에 대해 궁금증을 표함
          + 번역기가 직책을 제대로 번역하지 못한 것 같음. 스위스 정부 공식 페이지에 따르면, 해당 인물은 ""우편 및 통신 감시 서비스""의 ""법률 및 관리"" 부서를 책임지는 사람임을 확인함. 해당 부서의 담당 업무까지 소개되어 있음
          + 간단한 논리적 질문 제시임. Proton이 아무것도 저장하지 않는다면 어떻게 메일을 배달할 수 있는지에 대한 의문임
     * Proton이 다른 나라로 이동하면 네덜란드나 스웨덴이 후보가 될 수 있으나, 그쪽도 EU 규정이 적용됨. 법망을 피하려면 Seychelles나 Panama로 가야 하지만, 서버 운영은 결국 다른 곳에서 해야 하는 어려움이 있음. 스위스가 안전지대 역할을 못하면 무의미해진다는 판단임
          + 스웨덴은 사회민주주의 유산과 국가 통제 성향 때문에 프라이버시를 싫어하는 경향임. 최근 스웨덴 EU 집행위원이 종단간 암호화 금지 등을 포함한 다양한 제안을 EU에 올리고 있음(기사 링크 첨부)
          + Yen의 말을 인용해서, ""이번 개정안은 EU와 미국에서 불법으로 간주된 내용을 구현하려는 시도임. 유럽 국가 중 거의 유일하게 유사한 법을 가진 곳이 러시아뿐""이라며, 이런 감시가 불법이라서 유럽 어디든 갈 수 있다는 의견임
          + 오프쇼어 데이터센터, 씨스테딩(seasteading), 해적 라디오 같은 개념들이 예전엔 있었는데, 다시 도입해야 할 시점이 아닌지 묻는 질문임
          + 노르웨이도 이런 유형의 서비스에 많이 선택되는 인기 있는 목적지였다는 의견임
     * 기사 타이틀에 Google 뒤에 대시가 들어가야 자연스럽게 읽힌다는 조언임
     * 더 이상 ""Swiss-Privacy""라는 개념이 없어질 수 있다는 아쉬움을 표현함
     * 또 한 번, 디지털 세계를 이해하지 못하는 정치인에 의해 디지털 규제가 시도되는 하루라는 냉소적 의견임
     * 보안과 감시에 민감한 사람들은 정치적 스펙트럼에서 극단에 위치할 가능성이 높다는 개인적 인상임
     * Proton의 CEO가 트럼프를 좋아하는 것으로 보이므로, 이들이 ""착한 집단""은 아니라고 생각함. ""at rest encrypted""를 마치 모든 게 암호화되는 것처럼 홍보하지만, 실제로는 메시지 본문만 암호화되고, 제목과 발신자 등은 암호화되지 않은 상태로 저장돼 있으며, 실제로 당국에 공유된 바 있는 사실임
"
"https://news.hada.io/topic?id=20958","Show GN: LLMLingua-2의 TypeScript 구현체","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: LLMLingua-2의 TypeScript 구현체

     마이크로소프트에서 개발한 ""LLM 기반 범용 프롬프트 압축기"" LLMLingua를, 더 다양한 환경에서 사용할 수 있도록, 파이썬에서 타입스크립트로 포팅하는 오픈소스 프로젝트.

  LLMLingua란?

     * GeekNews: ""Microsoft LLMLingua - 추론 가속 및 비용 절감을 위해 프롬프트 압축하기""
     * LLMLingua-2 홈페이지
     * 현재 Python(파이썬) 언어로 공식 구현되어 배포되고 있음.

  LLMLingua-2의 TypeScript 구현체

     * 프롬프트 압축 데모 (GitHub Pages)
          + JavaScript + React + Vite
          + 데모 체험 요구사항: WebGPU가 지원되는 브라우저 필요 (Windows/macOS Google Chrome 등)
     * 소스코드: GitHub 및 HuggingFace에서 확인 가능
     * 패키지 (npm): npm install @atjsh/llmlingua-2

  하이라이트

     * Python으로 작성되어 있던 LLMLingua-2 파이프라인을 순수 TypeScript로 포팅
     * 구현에 사용한 라이브러리: js-tiktoken, transformer.js 및 TensorFlow.js
     * Node.js 지원은 추가 개발 진행 중 (지금 바로 테스트는 가능)
     * MIT 라이선스 기반 오픈 소스

  아직 초창기 단계

     * use_context_level_filter, return_word_label 등 몇몇 기능은 미구현 상태
     * 핵심 기능인 프롬프트 압축 기능조차, 원래의 LLMLingua-2에 비해 잘못 구현된 부분이 존재할 수 있음... ㅜ
"
"https://news.hada.io/topic?id=21031","Python PEP, 태어나 세계로 퍼진 ‘○EP’ 이야기  ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Python PEP, 태어나 세계로 퍼진 ‘○EP’ 이야기

   — Hugo van Kemenade 블로그 「PEPs & Co.」 (2025-05-14) 요약 oai_citation:0‡Hugo van Kemenade

    한눈에 보는 키포인트

    1. PEP의 탄생 배경
          + 1990년대 후반 CNRI에 있던 배리 워쇼우(Barry Warsaw)가 IETF RFC 모델을 참고해 “제안 → 토론 → 결론”의 공식 문서를 파이썬에도 도입해야 한다고 판단했다.
          + 그는 “경쾌하다(peppy)”는 뉘앙스를 살려 ‘PEP’이라는 단어를 먼저 만들고, 거꾸로 Python Enhancement Proposal이라는 뜻을 붙인 backronym을 탄생시켰다.
          + 워쇼우는 PEP 0(목차)과 PEP 1(프로세스 설명)을 직접 작성해 체계를 확립했다. oai_citation:1‡Hugo van Kemenade
    2. RFC 모델의 성공적 이식
          + PEP는 “하나의 문서에 내용을 모아 논의한다”는 방식을 통해 핵심 개발자가 폭주하는 아이디어를 효율적으로 검토할 수 있도록 했다.
          + 이후 제안서 형식은 파이썬을 넘어 다수 오픈소스 프로젝트의 ‘협업 표준’으로 자리 잡았다. oai_citation:2‡Hugo van Kemenade
    3. 다양하게 파생된 ‘○EP’들
       대표적인 확장판만 살펴봐도 PEP 모델의 전파력을 확인할 수 있다.

        약어        커뮤니티                         정식 명칭
       AIP  Apache Airflow    Airflow Improvement Proposal
       BIP  Bitcoin           Bitcoin Improvement Proposal
       DEP  Django            Django Enhancement Proposal
       JEP  Jupyter           Jupyter Enhancement Proposal
       KEP  Kubernetes        Kubernetes Enhancement Proposal
       NEP  NumPy             NumPy Enhancement Proposal
       SLEP scikit-learn      Scikit-learn Enhancement Proposal
       SPEC Scientific Python Scientific Python Ecosystem Coordination
       TIP  Tcl               Tcl Improvement Proposal
       XEP  XMPP              XMPP Extension Protocol
    4. 왜 중요한가
          + PEP는 대규모 분산 개발에서 투명성·추적 가능성을 보장하며, 커뮤니티가 스스로 로드맵을 설계하도록 돕는다.
          + 블로그가 정리한 ‘○EP’ 목록은 “문서화된 제안 프로세스가 현대 오픈소스 거버넌스의 필수 요소”임을 보여준다. oai_citation:4‡Hugo van Kemenade
"
"https://news.hada.io/topic?id=21002","Zod 4 릴리즈 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Zod 4 릴리즈

     * 스키마 선언 및 유효성 검사 라이브러리인 Zod의 버전 4 릴리즈. 주요 성능 향상과 장기 요청된 기능을 포함해 안정버전 출시
     * 속도와 번들 크기에서 큰 개선이 이루어졌으며, 새로운 미니 버전(v4-mini)은 번들 크기를 대폭 줄임
     * 새로운 메타데이터 레지스트리와 JSON Schema 변환, 그리고 재귀 타입 추론 기능이 추가됨
     * 에러 메시지 커스터마이즈와 다국어 로케일 시스템 등 개발자 경험이 강화됨
     * 향후 라이브러리 구축에 활용할 수 있는 코어 서브패키지 도입 등으로 확장성이 더욱 높아짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Zod 4 소개

  주요 출시에 대한 안내

     * 1년간의 활발한 개발을 거쳐 Zod 4가 안정 버전으로 출시됨
     * Clerk의 OSS Fellowship 지원을 받아 개발 진행됨
     * 현재 Zod 3와 병행 배포되어, Zod 4로의 점진적 마이그레이션이 쉬워짐
     * 일부 파괴적 변경점에 대한 상세 안내는 Migration guide에서 확인 가능함

  성장 배경

     * 2021년 출시된 Zod 3 대비 Zod 4는 GitHub 스타 수와 주간 다운로드 수가 기하급수적으로 성장함
     * Zod 4는 훨씬 빠르고, 슬림하며, 타입스크립트 컴파일러 효율이 개선됨
     * 장기적으로 요청이 많았던 9가지 주요 이슈가 해결됨

  벤치마크 및 성능

     * 속도 향상:
          + 문자열 파싱: 14.71배 빠른 결과
          + 배열 파싱: 7.43배 빠름
          + 객체 파싱(safeParse): 6.5배 빠름
     * 리포에서 직접 벤치마크를 수행할 수 있는 스크립트 제공
     * 개선된 제네릭 구조로 인해 .extend(), .omit() 등의 메서드 체이닝 시 컴파일 성능이 10배 향상됨
     * 대규모 스키마 및 코드베이스에서 TypeScript 컴파일 속도가 크게 개선됨

  번들 크기와 Zod Mini

     * 기본 번들 크기가 57% 감소해, v4는 v3 대비 2.3배 작은 크기 제공
     * zod/v4-mini는 함수 기반의 트리-쉐이크 가능한 API 제공, 번들 크기를 85%까지 줄임
     * core와 v4-mini의 API 차이는 공식 문서에서 상세히 정리됨
     * 번들러에서 사용하지 않는 메서드를 쉽게 제거할 수 있도록 구조 설계됨

  메타데이터 레지스트리와 JSON Schema 지원

     * typed metadata를 스키마에 강타입으로 등록 및 관리 가능
     * 글로벌 레지스트리(z.globalRegistry)에서 JSON Schema 호환 메타데이터 처리 및 자동 포함 기능 제공
     * .meta(), .describe()를 통해 스키마 문서화 용이
     * .toJSONSchema()로 스키마를 JSON Schema 형식으로 변환 가능, 메타데이터 자동 반영

  재귀 타입 자동 추론

     * 재귀 객체 타입과 상호 재귀 타입을 별도의 타입 캐스팅 없이 자연스럽게 정의 및 추론 가능
     * 기존 Zod 3 패턴보다 사용성이 크게 향상됨
     * 재귀·상호재귀 타입에서도 모든 스키마 메서드 기능 사용 가능

  파일 타입 및 검증 기능

     * 새로운 file() 타입으로 File 인스턴스 검증 가능
     * 파일 크기(min, max)와 MIME 타입 등 다양한 파일 제약조건 검증 제공

  에러 메시지 및 로케일 시스템

     * 글로벌 로케일 API(z.locales)로 에러 메시지의 다국어 지원 가능
     * 공식 z.prettifyError 함수로 사용자 친화적인 에러 포맷팅 지원

  포맷 함수 및 템플릿 리터럴

     * 기존 문자열 포맷(email 등)은 상위 레벨 함수로 승격되어 가독성과 트리 쉐이킹이 개선됨
     * 다양한 이메일 정규식 옵션 제공, 다양한 검증 요구 사항 충족
     * 템플릿 리터럴 타입 지원: 타입 시스템에서 표현 가능한 문자열 패턴 및 복잡한 조합 쉽게 구현

  새로 추가된 숫자 및 bigint 포맷

     * 고정 폭의 정수, 부동소수점 타입 (int32, uint64 등) 지원
     * 안전한 범위 내에서 자동으로 최소/최대 제약조건이 추가된 스키마 생성 가능

  z.stringbool 소개

     * 문자열 기반 불리언 파싱(yes, no 등) 가능, 환경변수 스타일 파싱도 지원
     * truthy/falsy 값은 커스터마이즈 가능

  에러 커스터마이즈 API 통합

     * 단일화된 error 파라미터를 통해 에러 메시지와 처리 로직 구조 정돈
     * 기존의 여러 에러 관련 API (message, invalid_type_error, errorMap)는 deprecated

  기타 핵심 개선점

     * 판별 유니온(discriminated unions) 이 다양한 스키마, 중첩, 조합을 지원
     * .literal()이 동시에 여러 값 허용
     * .refine() 등 커스텀 검증이 더욱 직관적으로 통합됨
     * transform 관련 .overwrite()로 변환 타입 변경 없이 후처리 가능

  라이브러리 확장성 및 새로운 코어

     * zod/v4/core로 핵심 기능이 별도 서브패키지로 분리, 다양한 라이브러리·플랫폼 통합/확장 가능
     * 라이브러리 제작자를 위한 가이드 문서와 확장 예시 제공

  마무리

     * Zod 4는 타입 안정성, 성능, 확장성, 개발자 경험 모두를 크게 개선한 자료 검증 라이브러리로 자리 잡음
     * 향후 추가적인 설계 포스트 및 업데이트가 예고됨
     * 기존 사용자와 라이브러리 제작자 모두를 위한 폭넓은 지원 준비됨

   행복한 파싱 경험 제공
   — Colin McDonnell @colinhacks

        Hacker News 의견

     * 저자 본인 의견 공유 요청 및 버전 관리 방식에 대한 상세한 설명 제공, npm이 Zod과 같은 상황을 처리하기에 적합하지 않음 강조, 수많은 라이브러리가 Zod 인터페이스/클래스를 직접 가져와 사용하는 점 언급, 만약 Zod이 주요 버전 변경될 때 이들 라이브러리가 모두 동시 대응해야 하며 버전 폭주 현상 발생 가능성 예상, Go와 유사하게 파괴적 변경은 새 서브패스 경로 추가 방식 사용, 타입스크립트 환경에서는 zod@^3.25.0 하나로 ""zod/v3""·""zod/v4""를 동시에 지원할 수 있음 설명, 최종 이용자에게 점진적 업그레이드 경로 제공함
          + Zod에 대한 기여에 감사의 인사와, 특히 tsc 성능 향상과 discriminated unions 개선에 기대감 표출, 버전 관리 방식을 충분히 이해하지만, 트랜스티브 종속성 충돌 걱정이 없는 자신 같은 사용자를 위해 4.0.0 패키지 형태로 단일 제공하는 것도 좋을 것 같다고 제안, ""zod/v4""로 import를 변경해야 하는 점이 코드상 잡음 유발 및 IDE 자동 import 충돌 등 추가적인 번거로움 발생 예상, 그러나 전체적으로 매우 유망한 업그레이드라고 생각하고 감사 표시
          + 모바일로 기사 확인 중이어서 미처 못 봤다면 양해 구함, .optional()과 관련한 최대 불편 사항이 이번 9/10 상위 이슈 해결에 포함됐는지 질문, Zod이 매우 뛰어나서 불편해도 계속 사용하는 중임을 언급, 훌륭한 라이브러리임에 감사 표함
          + 많은 수작업 해킹 코드를 Zod 신버전에서 제거할 수 있게 되어 감사, 직접 타이포를 줄일 목적으로 zod-key-parser를 활용 중인데 이런 기능이 기본적으로 라이브러리에 들어있지 않은 이유나 범위 밖으로 보는지, 혹은 아직 구현하지 않았는지 궁금, 관련 오픈 토론들 공유
          + 단기적인 고통을 최소화하는 방법이 종종 최선임을 강조, 파이썬 2/3 마이그레이션 대혼란 기억 사례 언급
          + 재귀 타입과 discriminated union 형태를 동시에 사용(예: XML을 JSON 안에 포함하는 상황)에 상당한 어려움을 겪었던 경험 공유, 이번 업데이트로 상황이 많이 나아지길 희망
     * zod/v4-mini import에 의구심 가진다는 의견 표현, 이로 인해 오히려 번들 사이즈가 늘어날 것이라 추측, 공식문서에서 ""zod/v4가 대부분의 경우 추천""한다고 해서 앱 개발자는 zod/v4를 쓰지만, 라이브러리 작성자가 번들 사이즈 절감 위해 zod/v4-mini도 추가하면 둘 다 번들에 포함되어 중복 발생 우려, 만약 zod/v4가 zod/v4-mini 래퍼라면 이런 문제 줄어들 수 있는지 질문
     * Zod 4의 마이그레이션을 용이하게 하려고 v3와 v4가 zod@3.25에서 동시 제공되는 방식 도입, 이런 구조가 npm의 의존성 관리 한계와 맞물려서 v4가 v3처럼 보이도록 해야 했다고 비판, npm의 peer dependencies 시스템에 비효율이 있음을 지적
          + 저자 본인, 골랑(Golang) 방식의 하위 경로 추가 버전 관리 전략 재설명, npm 특성상 Zod 생태계에 도입이 어렵지만 v3, v4를 동시에 지원하며 점진적 마이그레이션을 제공하는 장점 강조
          + peer dependencies가 망가져서 v4를 v3로 가장했다는 이전 의견에 꼭 동의하지 않음, 이는 점진적 마이그레이션을 위한 방편임을 강조, 하나씩 'zod/v4'로 대체한 후 완전히 v4로 업그레이드하는 구조
          + 많은 사람들이 비난하는데, 실제로 npm의 본질적 한계라기보다는 큰 변경 포함된 라이브러리를 점진적으로 바꿀 수 있도록 한 실용적 결정임을 강조
          + 오랜 기간 npm만 사용해서 편견일 수도 있지만 v3에서 v4로 크게 한번에 옮기지 않고 점진적으로 지원하는 더 좋은 방법이 무엇일지 궁금
          + zod 4 베타로 이미 큰 개선 경험함에도 대규모 코드베이스에서 모듈 해상도 세팅이 어려워 제대로 업그레이드 못하고 있는 상황 언급, 레거시 레이어 없이 주요 버전으로만도 출시해줬으면 하는 바람, '버전 폭주 현상' 방지를 위한 저자 해설 공유, 그러나 v3 지원 병행하면 충격파 완화 가능하다는 본인 생각 남김
     * 서버에서 반환하는 타입이 엔드포인트마다 다르거나, 필드 일부가 익명 유저처럼 null로 올 수 있는 등의 복잡한 경우 서버 응답을 어떤 타입 모델로 다뤄야 할지 질문, normalizeUser/normalizePost처럼 여러 함수 만드니 점점 관리가 복잡해지는 문제, 이 문제를 실무에서 어떻게 해결하는지 경험 공유 요청
          + discriminated union 예시로 해결 방법 제시, 공통된 스키마 부분을 객체로 정의하고 특정 상황에 맞게 확장하는 방식 설명, 매우 다양한 경우에는 어차피 복잡해질 수 있으나 schema validator로 최소한 체계적으로 유지할 수 있다고 조언
          + 이상적으론 User의 타입 구조를 단일 원천(예: discriminated union 형태)으로 정하는 것이 최선, 파이썬 백엔드를 가정할 경우 여러 형태의 Pydantic 모델+유니온 그리고 OpenAPI/GraphQL 코드 제너레이션으로 타입스크립트 클라이언트 타입 생성 구조 제안
          + 실사용 예제를 알면 더 좋은 답이 가능하겠지만, union 타입에 구분 프로퍼티(e.g. ""user_type"")를 넣으면 개별 필드 접근이 쉬운 점 설명, 타입 시스템이 상황별로 알맞은 속성 인식 가능
          + 서버가 직접 타입을 내보내야 한다고 고언, 별도의 타입을 클라이언트에서 일일이 다시 작성하는 건 비효율, 파이썬 백엔드는 Pydantic으로 OpenAPI 명세 자동 생성해 타입스크립트 클라이언트에 타입 생성 방법 설명
          + GraphQL이 이런 케이스에 적합하게 설계되어 있음을 언급, 타입스크립트 GraphQL 라이브러리로 쿼리 결과 형태를 자동 추론할 수 있고 선택한 필드에 따라 응답 타입 유동적으로 생성됨 예시
     * Zod 4, 개선에도 불구하고 ArkType이 여전히 월등히 빠름 언급, 기존 라이브러리에선 하위 호환성과 문법 유지를 이유로 성능 한계가 있음, 본인 프로젝트 분석 결과 ArkType을 선택하게 된 이유는 성능과 타입스크립트 사용성 때문
          + ArkType 속도 메트릭을 보았고, 실제로 그 속도가 활용에 무슨 영향을 주는지 궁금, 폼 검증 등 일반 상황에서는 영향이 적어 보여서 의문, 혹시 초고속 API 입력 검증처럼 성능 민감한 곳에서 쓰는지 질문
          + ArkType이 리서치에 포함되지 않았지만 타입스크립트 사용성을 고려해 찾고 있었음, 그래도 Zod에서 옮길 계획은 없음
          + ArkType 사용 경험이 매우 어려웠고 Zod는 사용성이 좋아서 선호한다고 생각
          + TypeBox보다 ArkType을 선택한 특별한 이유 궁금
     * Zod 팀의 새 릴리즈 축하, 마이그레이션 가이드에 쏟아지는 파괴적 변경점 수를 보면 대규모 Zod 의존 프로젝트는 부담이 크고 관리가 어렵겠다는 우려, 오래된 프런트엔드 프로젝트 유지 경험에서 각 라이브러리마다 큰 변화와 문서 부족이 많아 현재 JS 발전 흐름의 아쉬움 표출
          + 여러 대형 Next.js 앱을 운영 중인데 최근 1년간 Next.js 14→15, Next.js pages→app router, React 18→19, Eslint 8→9, Tailwind 3→4 등 크고 어려운 변화들 겪으며 정말 힘들었고, 오히려 Django로 지었으면 했던 생각 공유, 특히 Tailwind 3→4 마이그레이션이 의외로 가장 고통스러웠다는 점 회고
          + 이런 문제를 완화하려고 'mini' 에디션의 동시 제공 전략을 도입함, 점진적 전환을 쉽게 해주고 트리쉐이킹 최적성 측면에서 'mini' 도입이 대체재들을 견제하기 위해 불가피했다고 설명
          + LLM 같은 도구 활용으로 어렵지 않게 마이그레이션할 수 있는 제안
     * Zod가 기존 다른 대안들에 비해 훨씬 뛰어남을 언급, 하지만 웹 개발에서는 동일 데이터 구조를 여러 방식으로 기술해야 하는 게 현실, JS 입력 검증, Swagger를 통한 API 명세, 서버·클라이언트 각각 별도 정의 등 너무 반복적이고 번거롭다는 아쉬움 표출
          + 타입스크립트가 정적 검사 전용 툴로 고집하는 것에서 아쉬움, 런타임 체크까지 원하지는 않으나 클래스/함수/객체의 타입 데이터를 외부로 쉽게 쓸 수 있길 희망, 지금은 여러 도구가 각자 모델과 빌더를 정의해야 해서 중복 불가피, 표준화 시도인 Standard Schema 프로젝트가 주요 검증 라이브러리들과 통합될 조짐 있지만 API 명세, ORM 쪽 확장은 초기 단계임을 공유
          + 이런 툴의 핵심은 한 번 정의해 타입 세이프티를 전체 앱으로 전파할 수 있다는 점, Zod 스키마를 일종의 진실 원천(싱글 소스 오브 트루스)으로 삼을 수 있음을 강조
          + 이런 번잡한 구조를 불편해하는 사람 중 실제로 타입스크립트(Zod 포함) 하나로 다 통합하자는 제안엔 거부감을 보이는 이들이 많음도 덧붙임
          + 모든 API와 시스템이 항상 변화와 실험 중이라는 점, 복잡한 레이어가 많아진 현실이 단점이긴 해도 ""프로젝트에서 일만 잘 돌아가면 된다""는 상황에선 결국 더 많은 일감을 만들어준다는 시각
          + 전체적으로는 trpc 같은 엔드 투 엔드 타입세이프티 사용할 수도 있지만 이를 위해서는 프런트와 백엔드 모두 타입스크립트로 통일해야 하고 웹 이외 플랫폼(모바일 등) 사용이 힘들다는 점 실무 한계로 지목
     * 전문가 아니지만 스키마 기반인 JSON-Schema가 타입스크립트 외 다른 언어에도 validator 구현이 가능하서 좋은 선택일 수 있다고 생각, ajv.js.org 같은 라이브러리 예시 기반으로 Zod과의 비교 질문
          + Zod는 JSON 형태만이 아니라 JS 객체 전체(날짜, 클래스 인스턴스 등 JSON으로 표현 불가한 데이터)에도 유효성 검증이 가능, JSON 변환 과정에 쓸 수도 있어 스키마를 문자열 기반으로 작성하고, 예를 들어 ISO date string을 Date 객체로 검증해 변환할 수 있음
          + Zod 4는 Zod 스키마를 JSON-Schema로 변환 지원(과거엔 외부 라이브러리 필요), Zod의 큰 차별점은 preprocess/refine 기능으로, 검증 전 콜백 추가가 가능해 MM/DD/YYYY를 DD/MM/YYYY로 바꿔서 검증하는 등 유연한 변환 가능(이건 JSON-Schema에선 불가) 예시 강조
          + JSON-Schema도 좋은 선택이고, 이 경우 TypeBox가 생성에 적합, avj를 쓸 수도 있고 자체적인 검증 시스템도 사용 가능, 자체 검증이 더 빠르지만 동기 전용이고 avj는 비동기 검증 또한 가능해서 심층 검증 필요시 유리
          + 여러 언어에서 사용하려면 JSON-Schema가 가장 보편적, OpenAPI로 감싸면 자동으로 API 문서까지 생성할 수 있어 장점
     * Zod을 새 프로젝트에 막 도입하기 시작했는데 이번 릴리즈 시기가 완벽하게 맞아떨어졌다는 기쁨, 계획대로라면 v4로 마이그레이션하려고 많은 변경이 필요했을 텐데 타이밍이 절묘
     * 생각보다 부정적 의견이 많아서 놀랐다는 반응, v4 초기 버전 테스트 때 새로운 API는 마음에 들었으나 마이그레이션 경로가 걱정됐었고, 별도 패키지명 출시 제안까지 고민했으나 실제 저자 방식이 매우 탁월하게 문제를 해결했다고 높이 평가, 덕분에 의존성 업데이트 기다릴 필요 없이 바로 v4 채택 가능함
          + 본인도 유효성 검증 같은 분야는 전부 한 번에 마이그레이션할 여력이 없다고 생각, 지금 방식이 현실적으로 최적이라고 평가
"
"https://news.hada.io/topic?id=20930","Databricks가 서버리스 Postgres 전문 기업 Neon을 인수","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Databricks가 서버리스 Postgres 전문 기업 Neon을 인수

     * Databricks는 개발자 중심의 서버리스 Postgres를 제공하는 Neon을 인수하기로 합의
     * Neon은 스토리지와 컴퓨트 분리 구조를 통해 개발자와 AI 시스템에 최적화된 서버리스 데이터베이스 플랫폼을 제공
     * Neon의 도입으로 AI 에이전트가 생성하는 데이터베이스 비중이 30%에서 80% 이상까지 급성장함
     * Databricks와 Neon은 오픈소스 철학과 인프라 혁신 DNA를 공유함
     * 인수 이후에도 Neon 플랫폼 지원과 미래지향적 로드맵이 Databricks 자원으로 강화될 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

인수 발표 및 의의

     * Databricks는 개발자 중심의 서버리스 Postgres를 제공하는 Neon을 인수하기로 합의함
     * Neon 공동 창업진은 저장소-컴퓨트 완전 분리 구조로 Postgres를 설계할 수 있는 세계 소수의 전문가임
     * 이 팀은 AI 시대의 대규모 개발자 지원을 위한 서버리스 Postgres 플랫폼 제공에 집중해 왔음

Postgres 기반 혁신 미션

     * Neon 공동 창업진은 약 4년 전 구시대적인 데이터베이스 구조를 혁신하고자 뜻을 모았음
     * 핵심 목표는 아래와 같음
          + Postgres의 사실상 표준화를 예견하고, 서버리스 플랫폼 비전을 세움
          + 개발자가 몇 초 만에 새 인스턴스를 생성할 수 있도록 속도에 초점을 맞춤
          + 데이터베이스의 자동 확장 및 작업 단순화를 통해 오버·언더 프로비저닝 우려 해소를 도모함
          + 즉각 분기 및 포킹 지원으로, 데이터베이스 테스트 및 실험을 용이하게 함
     * Neon팀은 스토리지와 컴퓨트 독립 확장이 가능한 아키텍처를 만들어 위 목표를 달성함
     * 출시 이후 개발자들은 속도, 단순성, 그리고 Git 방식의 분기/포킹 기능을 호평함

AI 에이전트 시대의 변화

     * Neon의 GA 이후 AI 에이전트가 전체 DB 생성의 30% 를 차지하다가 최근에는 80% 이상으로 증가함
     * AI 에이전트가 개발자와 유사한 요구를 지니게 됨
     * Neon의 강점은 다음과 같음
          + Postgres 오픈소스 생태계: 최신 LLM이 Postgres 데이터로 학습되어, AI 에이전트가 Neon 사용에 능숙함
          + 신속성: 사람보다 빠른 속도가 요구되어, 초고속 인스턴스 프로비저닝이 필수임
          + 유연한 확장과 가격: 분리된 서버리스 구조로 인한 초저가, 다수 AI 에이전트 지원 가능함
          + 분기 및 포킹: AI 에이전트의 변화무쌍한 시도를 위한 실험/검증이 쉬워짐

Databricks와 Neon's 공유 DNA

     * 창업자 Nikita Shamgunov, Heikki Linnakangas, Stas Kelvich는 업계에서 유명한 DB 기술 전문가임
     * SingleStore, Postgres 커미터 등 각기 풍부한 경험과 독창성을 보유함
     * Databricks와 Neon 모두 인프라 계층의 첨단 기술혁신과 오픈소스 가치를 중시함
     * Apache Spark와 Postgres 모두 UC Berkeley에서 시작한 오픈소스 프로젝트라는 연결고리가 있음

향후 비전과 사용자 혜택

     * OLTP 데이터베이스 시장(약 1000억 달러 규모)은 현재 수십 년 전 제품 위주임
     * 이제 개발자와 AI 에이전트가 혁신을 주도할 시점임
     * Databricks와 Neon은 최고로 친개발자적이고 AI-에이전트 친화적인 DB 플랫폼을 목표로 함
     * 기존 Neon 고객과 파트너는 지속적인 지원 및 혁신과 로드맵의 실현을 기대할 수 있음
     * Databricks의 자원을 통해 플랫폼 강화 및 안정적 성장이 보장될 예정임
     * Data + AI Summit(샌프란시스코, 6월 9~12일)에서 향후 비전을 상세히 공유할 예정임

        Hacker News 의견

     * 데이터 웨어하우징이 오픈소스 덕분에 빠르게 범용화되고 있다고 생각. 아는 회사가 2페타바이트 이상의 데이터를 Cloudera에 저장했었지만, 클라우드(Databricks)로 옮기지 않고 Iceberg, Trino, Superset으로 자체 분석 플랫폼을 구축해 비용을 5배 절약함. 엔터프라이즈급 k8s 오퍼레이터가 이제 충분히 좋고, 온프레미스 S3도 훌륭함. 128개 CPU와 1TB 메모리를 가진 서버 등 좋은 하드웨어와 네트워킹도 가능함. Trino뿐만 아니라 StarRocks, Clickhouse도 엔터프라이즈급 k8s 헬름 차트/오퍼레이터를 제공함. Databricks의 기업가치 600억 달러는 그들의 부담. 이 가격을 정당화해야 하고, 그들의 핵심 비즈니스 자체도 범용화되고 있음. Neon은 운영(행 지향) DB가 없던 제품군의 빈틈을 채움
          + 엔터프라이즈 입장에서는 범용화가 아님. 이전 직장은 오픈소스 소프트웨어나 10년 후 존재하지 않을 수도 있는 회사, 혹은 데이터를 우리 테넌트 이외의 장소에 두는 기업을 허용하지 않았음. “전화문의” 가격 정책을 오히려 반겼고, 데이터 플랫폼을 더 이상 신경 쓸 필요 없다는 점에서 Databricks 도입이 내 3대 성과 중 하나로 꼽힘. 신규 플랫폼 도입 시 리스크가 너무 커서 (아무 오픈소스 프로젝트) 신뢰 불가. 한 번 스타트업 솔루션을 도입한 적 있는데, MongoDB를 사용해서 운영팀이 역량이 부족하니 학습 대신 Atlas 같은 지원이 완비된 서비스를 계약함. 익숙하지 않은 Azure 방화벽 대신 아는 방화벽만 썼고, 각종 계약도 진행함. 인력 채용 줄이고, 연락 창구도 하나로 고정, 업무 효율 달성. 스타트업 라이선스는 연 5~10K달러인데, 지원은 40K달러 등 훨씬
            더 많은 비용이 소모됨. 스타트업과 엔터프라이즈는 완전히 다른 세상
          + 오픈소스 StarRocks를 k8s 오퍼레이터로 사용하며 테라바이트급 데이터 고객 분석 중인데, 내 환경에서는 Databricks가 거의 필요 없다고 느낌
          + ClickHouse를 지난 몇 년간 문제 없이 잘 사용 중. 다양한 기능이 넓고, 신뢰감 있는 데이터베이스. 외부 사전(external dictionary) 기능 덕분에 Postgres, Redis 등 다른 데이터스토어와 연동이 쉬움
          + Kubernetes 오퍼레이터 기반 오픈소스 Cloudera 대안 찾는다면 stackable.tech를 5년째 개발해오고 있음. 온프레미스 오픈소스 S3 쪽은 문제. MinIO는 추천하지 않고, 이외에는 엔터프라이즈 대응 가능한 솔루션이 거의 비어있음
          + 데이터 웨어하우징은 수십 년 전부터 범용화되었음. 가격과 성능 지표의 역사가 긴데, SnowBricks 제품이 여기에 부합하지 못함. 강매냐 소프트세일이냐의 차이일 뿐
          + 왜 Databricks에서 운영 DB를 사야 하는지 모르겠음. 시장가치 유지를 위한 Databricks의 고군분투로밖에 보이지 않음
          + 만약 Databricks가 단순히 row DB가 필요했다면 자체적으로 Postgres를 구축했을 것. Neon에 이렇게 많은 돈을 쓴 건 Neon의 “독립적으로 확장 가능한 스토리지와 컴퓨트” 같은 무언가 특별한 점을 원했다는 신호
          + ETL에 무엇을 사용하는지 궁금함
     * 지난주 neon에 지원했고, 인수 소식이 터지고 오늘 아침 바로 탈락 통보를 받았음. 그 어느 때보다 행복한 탈락 경험. 이번까지 세 번 연속 인수되는 회사에 입사할 뻔한 거라, 이제는 그냥 안정함을 원함. neon 팀 축하. neon을 좋아하고 사용 중인데, 이 인수로 너무 변하지 않길 바람
          + 인수 전에 Kenna Security에 입사했는데 한 달 뒤 Cisco에 인수당했었음. 정말 끔찍한 경험이었고, Kenna 리더십이 있는 회사나 Cisco에서는 다시는 일하지 않을 것
          + 오히려 반대 경험. 인수 시기에 입사하는 것이 가장 흥미로운 때였음. 내 경우 인수 통합 경험 덕분에 종종 스카웃받았음
          + 1년차 엔지니어링 매니저 시절 인수 과정에 있었는데, 두 차례 해고를 견디며 남길 인원 선별, 팀 개편을 도왔음. 사기 저하가 심했고, 조직문화도 전혀 맞지 않았음. 심한 번아웃이 와서 몇 달 휴식, 지금은 다시 IC로 행복하게 근무 중
          + 내 예상에는 neon팀이 Databricks의 Online Tables 기술로 편입될 거라고 생각. 이게 제품적으로도 타당
          + 혹시 neon의 옛날 기업가치 시점에 입사해서 베스팅이 막 끝났다면 갑작스럽게 목돈을 받았을 텐데 어땠을지 궁금함
     * Databricks가 내가 써본 소프트웨어 중 가장 짜증나는 쓰레기. 누가 이걸 자발적으로 쓰는지 신기
          + Databricks는 2013년에 Spark가 별로였을 때 시작했고, Spark를 더 낫고 빠르게 만들었음. 제품은 여전히 Spark 중심이지만, Iceberg와 DuckDB 조합이 95%의 회사에 더 적합하다고 봄. 더 싸고, 빠르며, 다루기 편하고, 우리도 Definite에서 그런 전제 하에 데이터 플랫폼을 만들고 있음(ETL, BI, Data Lake 다 포함)
          + Databricks는 데이터를 다루는 Jira. 아무도 쓰고 싶지 않고, 별로인데 모든 사용자에게 맞추려 한투성이 기능이 오히려 하나같이 어설픔. 이제 훨씬 더 나은 대안이 많아서 Databricks를 내 선택으로 쓸 일 없음
          + 정말 동의하기 어렵다고 생각. Hadoop 출신으로서 Databricks는 유토피아. 안정적이고, 빠르며, 대규모 데이터셋도 훌륭히 확장됨. 단, 가격이 너무 비싸다는 점이 가장 큰 불만
          + 과거 Databricks 플랫폼을 좋아했었음. 2020~2021년에는 AWS, Azure, Snowflake에 비해 합리적 대안이 거의 Databricks뿐이었음. 현재는 기능 남발, 잦은 변화, 인수 등으로 중구난방이고 기능명도 엉망
          + IBM류 소프트웨어(다들 쓰니까 우리도 쓴다)가 아직도 시장이 남아 있었던 셈
          + 솔직히 Databricks는 정말 심심한 제품. 2010년대 후반을 떠올려보면, Spark-as-a-Service가 탁월했고, 기업들이 자체적으로 Spark를 안정적으로 운영하는 데 실패하던 시절. 하이퍼스케일러들도 1차 서비스는 빈약했고, Databricks 노트북 포맷의 Jupyter 호환성 이슈 등 문제도 있었지만, 온프렘 클러스터 불안정이 더 큰 골칫거리라 프리미엄을 기꺼이 감수했음. 그땐 Databricks도 훌륭한 10억 달러 비즈니스였음. 그러나 Spark-aaS만으로 유니콘이 될 수는 없었음. AWS EMR이 경쟁자로 느리게 쫓아오고 있었고, 결국 Databricks도 제품을 마구 부풀리며 성장전략에 올인, 데이터, 레이크, 하우스라는 버즈워드 세례. 2025년 현재 Databricks의 하락세는 엔쉬티피케이션의 쓴 단면. 언젠가 Larry Ellison이 인수해 시장에서 사라질지도 모름. 왜 요즘 신규 프로젝트에 Databricks를 고르는지
            이해되지 않지만, 5년 넘게 쓴 엔터프라이즈들은 쉽사리 못 나옴. 앞으로 시장 점유율은 떨어지겠지만, 당분간 돈은 계속 벌 것. 이것이 업계의 순환이며, 결국은 엔트로피가 승리. 너무 미워하지는 않겠음. 꽤 괜찮은 역사를 만든 회사였음
          + Serverless를 너무 강조하지만, 한계와 숨은 함정이 너무 많아서 정말 미치도록 힘듦
          + Spark 호스팅이 정말 혁신적이었는지 의문. 그리고 Spark 자체가 90%의 기성기업 데이터처리에 너무 복잡한 거 아닌지 항상 의구심. 이 회사의 가치가 왜 이렇게 높은지 이해불가
          + 쿠키를 비활성화하면 웹사이트가 완전히 안 뜨는데, 이건 치명적인 레드 플래그. 웹사이트 하나 못 만드는 곳이 좋은 디지털 제품을 만들 거란 생각을 할 수 없음
     * Databricks는 Oracle급으로 별로. Neon도 망치거나 비싸게 만들 거라는 확신. 중장기적으로 Neon 대안을 찾을 예정
          + Databricks의 인수 합병 전략은 인수한 회사를 질식시키는 구조. Iceberg, DuckDB 등 오픈소스의 대격변에 고전하는 상황. 인수를 통한 혁신을 시도하고 있지만, 기업문화 탓에 인수기업들이 무너짐. 빅데이터 업계 출신(전 Snowflake)이라 편향일 수 있으나, 오픈소스가 점점 힘을 얻는 추세가 확실히 보임. 이 변화가 어떻게 될지 매우 궁금
     * 기사 원문 인용: “Neon이 GA로 전환된 작년 30%가 AI 에이전트가 만든 데이터베이스였는데, 최근 다시 보니 이 비율이 80%를 넘었음. AI가 사람보다 4배 더 많은 DB를 만들었다는 뜻.” 이건 여러 알람이 울리는 데이터. Databricks가 Postgres를 AI 솔루션으로 포장하려는 것 같음. 요즘 세상 참 신기
          + 그중 활성 사용 중인 DB가 몇 개인지 궁금
     * Neon 팀에게 축하. 만든 것 매우 맘에 듦. 그런데 Databricks와의 연관이나 시너지를 솔직히 못 느끼겠고, Neon이 별도 제품으로 계속 남아있길 바람. 안 그러면 시장에서 확실한 Postgres 제공자를 하나 잃게 됨
          + Azure 의존이 높아서 당장 사라지진 않을 것 같음. Databricks가 분석 DB뿐 아니라 트랜잭션 DB 영역까지 확장하려는 움직임
          + FAQ에서는 독립적으로 운영된다고 하지만, 실제론 결과가 뻔하다고 생각
     * Neon 팀이 HN에 올렸던 첫 글을 기억함. 그때도 멋진 아이디어라 코멘트했었고, 아직 직접 쓸 필요는 없었지만 언젠가 쓸 거라 생각. 그런데 이런 인수 뉴스를 보면 회의감이 듦. 이제 사용자보다 소유주 입장에 집중하게 될 것 같아 우려. 원론적으로 입장이 같아야겠지만 실제로 맞아떨어지는 경우는 드물었음
          + Neon 첫 포스트를 나 역시 기억. 저장소-컴퓨트 분리가 신선했고, Pageserver에 대해 질문도 했었음. 이후 2년 만에 나 역시 Turso database에서 유사한 분리 스토리지 작업을 하게 됨. Neon 팀에게 다시 축하 보냄
          + 인수 소식에 나 역시 멈칫하게 됨. 인공지능 사용자를 우선하는 것이 개발자와 이익이 일치한다고 믿기 어려움. PostgreSQL 핵심과 관련된 기술은 오픈소스 커뮤니티의 도움이 되길 바람
     * Neon 팀에 축하를 전함. 훌륭한 제품. 물론 VC 지원을 받으면 이런 결과는 불가피하다고 봄. Nikita 등이 Databricks에 동화되지 않고 강인하길 바람
     * 이것은 정말 흥미로운 발전. OLTP와 OLAP의 수렴 방식이 옳은 방향이라고 생각. OP와 함께 SingleStore에서 HTAP 시스템을 만들었던 경험이 있음. OLTP와 OLAP을 단일 데이터베이스(한 번의 복사로 양쪽 지원)로 하려고 했으나 HTAP은 잘 되지 않았음. OLTP는 Postgres로, OLAP은 데이터 웨어하우스/레이크로 따로, 그리고 그 사이의 복제가 효율적으로 설계되어야 함. 동기화 복제는 난이도가 높음. 컬럼나형 저장소가 OLTP 쓰기를 잘 못 받음. Databricks와 Neon이 “최신 Postgres 테이블을 Unity Catalog에서 직접 활용”하는 시나리오를 실현할 수 있을지 기대됨(Debezium, Kafka, Flink, Iceberg 거치지 않고, Spark가 Iceberg 상태 유지 관리)
          + OP란 Neon 창업자인 Nikita Shamgunov(과거 MemSQL/SingleStore 창업자)를 말하는 거냐는 질문
     * Neon 팀 축하. 솔직히 다소 아쉬움. CockroachDB가 비즈니스 소스로 전환하면서 생긴 빈자리를 Neon이 채워줄 거라 기대했었음. Databricks에 인수되면서 Neon이 덜 매력적으로 느껴짐. 대기업이 중요한 인프라를 책임질 거라 믿기 힘듦. “현대적” Postgresql에 대한 수요는 충분하지만, 직접 대안들 중 어떤 것도 뿌리에서 멀어지고 있음(가격, 호환성, 소스 공개 여부 등에서). Postgres 대안을 찾을 때 아래를 비교함
       (1) AWS RDS는 이미 사용 중이었으나 비싸고, 확장성과 운영상 문제 있음
       (2) AWS Aurora는 운영 문제 일부를 해결하지만 다른 단점 동반, 다른 wire 호환 Postgres 대안과 비슷한 한계
       (3) CockroachDB는 매우 흥미로웠으나, 툴 체인 호환과 깊은 호환성 이슈, 당시엔 오픈소스였음
       (4) Neon은 아직 미성숙해보여 도입하지 않았지만 흥미롭고 많은 문제를 해결할 수 있을 것 같았음
       (5) Yugabyte는 역시 흥미로운 기술이나 다양한 호환성 문제가 있었음
       직접 Postgres 호스팅도 고민했지만, 쿠버네티스와 postgres의 자체 운영 부담이 커 고민했음. 자체 복제나 운영 기능은 아직 덜 성숙했고, 업그레이드 시 전체 데이터 언로드/리로드가 매우 번거로웠음. 확장이나 자동화가 쉽지 않았음
          + Yugabyte의 쿼리 엔진이 Postgres 기반 같다며 비교한 것에 대해, Neon 자체는 Postgres임을 상기시킴
          + “가장 좋은 현대적 Postgres 대안은 (5년 뒤의) Postgres 그 자체”라는 자신의 단기 경험을 공유
          + 다른 wire 호환 Postgresql 대안들의 “동일한 단점”이 무엇인지 더 듣고 싶음
"
"https://news.hada.io/topic?id=20971","O2 VoLTE: 통화 한 번으로 모든 고객의 위치 추적 가능","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   O2 VoLTE: 통화 한 번으로 모든 고객의 위치 추적 가능

     * O2 UK의 VoLTE(4G Calling) 서비스에서 통화 상대방의 위치 정보와 단말 식별자가 송신되는 현상 발견
     * IMS 신호 메시지에 IMSI, IMEI, Cell ID 등 민감 정보가 포함되어 외부에서 쉽게 수신 가능함
     * 공개 크라우드소싱 데이터(cellmapper.net 등)를 통해 이러한 정보로 정확한 위치 파악 가능함
     * 이 취약점은 모든 O2 고객에 적용되어 누구나 공격 대상으로 노출됨
     * 사용자나 일반 고객이 별도의 방법으로 이 정보 노출을 방지할 수 없음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * Voice over LTE(VoLTE) 는 모바일 네트워크에서 인터넷 기반 프로토콜을 사용해 음성 통화를 가능하게 하는 기술임
     * VoLTE에 사용되는 IP Multimedia Subsystem(IMS) 는 복잡성과 기기 간 상호작용 문제로 보안 위험이 발생할 수 있음
     * 각 통신사는 IMS 서버 구성 및 서비스 구현 방식을 고유하게 선택할 수 있기 때문에 설정 오류 시 데이터 유출 위험이 있음
     * 본 문서에서는 O2 UK가 이와 같은 보안 우려를 실질적으로 야기한 사례를 분석함

O2 UK의 IMS/VoLTE 서비스 현황

     * 2017년 3월 27일, O2 UK는 4G Calling이라는 IMS 기반 첫 서비스를 시작해 통화 중 더 좋은 음성 품질과 데이터 이용 환경을 제공함
     * 저자는 통화 품질을 측정하기 위해 Network Signal Guru(NSG) 앱을 루팅된 Google Pixel 8에서 활용
     * 앱의 한계로 인해, 직접 원시 IMS 신호 메시지를 분석하며 통화 시 교환되는 상세 정보를 확인함

신호 메시지의 문제점

     * O2 UK의 IMS 신호 응답에는 다른 통신사와 달리 매우 상세하고 긴 정보가 담겨 있음
     * IMS/SIP 서버 정보, 버전, 에러, 디버그 로그와 함께 아래와 같은 민감한 헤더가 포함되어 있음
          + 두 쌍의 IMSI, 두 쌍의 IMEI
          + Cellular-Network-Info: 수신자 네트워크, 위치 코드, 셀 ID 등
     * 메시지 내 IMSI, IMEI 및 Cell ID를 비교한 결과, 통화 상대방(수신자)의 정보가 함께 포함된 것을 확인함

Cell ID를 통한 위치 추적

     * Cellular-Network-Info 헤더를 해독하면 수신자 통신사, 위치 영역 코드(LAC), 셀 ID가 드러남
     * 해당 Cell ID는 cellmapper.net 등의 서비스에 입력하여 기지국 위치를 정확하게 파악 가능함
     * 도시 등 밀집 지역에서는 기지국 커버리지가 100m² 이내로 좁기 때문에 상대적으로 매우 정밀한 위치 확인이 가능함
     * 실제로 O2 고객이 해외 로밍 중에도 이 방식이 작동하여 도시 중심부까지 위치 확인이 이루어짐
     * 이러한 정보는 특별한 장비나 절차 없이 IMS 통화가 가능한 모든 O2 단말에서 노출됨

개선 요구사항

     * O2는 IMS/SIP 메시지에서 민감한 헤더(위치 및 단말 정보) 를 제거해야 고객의 개인 정보와 안전을 보호할 수 있음
     * 디버그 용도의 헤더 역시 불필요한 정보 유출로 이어질 수 있어 비활성화가 필요함
     * 네트워크 코어 외부 단말에 이런 헤더가 보이는 것은 불합리함
     * O2의 내부 보안 문제 신고 루트 부재는 다른 통신사(예: EE) 대비 심각한 문제임

결론

     * O2 고객은 누구나 기본적인 모바일 네트워크 지식만 있으면 정밀한 위치 정보까지 추적당할 위험이 있음
     * 사용자가 4G Calling을 끄더라도 민감 정보 노출이 막히지 않아 자체적으로 방지 불가함
     * 기기가 네트워크에 연결되지 않아도 마지막 접속 셀 및 접속 시점 정보가 여전히 IMS 메시지에 남아 있음
     * 2025년 3월 26~27일, O2 관련 보안 책임자와 CEO에게 이메일로 해당 사실과 위험성을 여러 차례 알렸으나 별다른 대응이나 개선 사항이 없음

참고

     * https://www.engadget.com/2017-03-29-o2-wifi-4g-calling.html

수정 기록

     * 2025년 5월 18일 23:40 기준, 최초 기사 내 O2 보안 신고메일 주소 오기재(virginmedia.co.uk→virginmediao2.co.uk) 수정 반영

        Hacker News 의견

     * 2025년 3월 26일과 27일에 O2에 이메일로 이 행동과 개인정보 위험을 알렸지만 아직 답장이나 행동 변화가 없는 상황임, 정말 아쉬운 대응임, Virgin Media 주소가 가장 가까운 연락처라는 점도 의문임, https://www.o2.co.uk/.well-known/security.txt가 200 응답을 주어야 하는데 404인 점이 문제임, 이런 상황에서는 공개하는 것도 이해하겠지만, 과연 NCSC 같은 기관에서 이 문제를 더 잘 전달할 수 있을지 궁금증이 생기는 상황임
          + 실제로 이메일 주소를 잘못 기재했음, Virgin Media O2의 @virginmediao2.co.uk를 써야 했는데, @virginmedia.co.uk로 쓴 오타임, 이 부분을 기사에서 정정할 예정임
          + 개인정보 보호 정책(GDPR 필수사항)에 여러 이메일 주소가 있음, 예를 들어 DPO@o2.com 등, 아마 그쪽에서는 누군가 확인하고 있을 가능성 있음, https://www.o2.co.uk/termsandconditions/privacy-policy 참고
     * 예전에 O2는 책임감 있는 정보 공개용 이메일 주소가 있었지만, 몇 년 전에 없어졌음, 예전에는 보안팀이 정말 뛰어났는데 작년에 이슈로 메일했더니 다 사라진 상태였음
          + O2 내 해당 팀이 사실은 이미 통보를 받았지만, 아무런 조치가 없었거나, 조치가 미흡했음
     * 이번 버그는 단순한 이론적 버그가 아니고, 실행 측면에서 게으름 때문에 생긴 문제임, 다른 영국 통신사들은 이미 해결한 문제임, LTE 도입 초창기부터 ECI 유출 문제는 이미 논의되었고, 오픈 마스트 DB 덕분에 자동 위치 매핑도 아주 쉬운 상황임, 관련 논문(https://arxiv.org/abs/2106.05007) 참고
     * 정말 흥미로운 점은, 대부분의 법적 관점에서 이것이 해킹으로 분류되지 않는다는 것임, 해당 데이터가 네트워크에서 정상적으로 자발적으로 나가는 정보임, 불법적으로 데이터를 얻으려고 시스템을 속인 게 아니기 때문임, 예를 들어 URL에 ""&reveal_privat_data=true""를 추가하는 건 명백한 의도가 있어서 불법이겠지만, 이번 건은 그런 게 없는 상황임
          + 하지만 이것도 데이터 유출에 해당하는 상황임, 영국처럼 관련 규정이 있으면 바로 규제 기관에 보고해야 하거나 벌금을 맞게 되는 계기가 될 수 있음
          + Computer Misuse Act 범위가 매우 광범위하다는 점을 고려할 때, 생각보다 간단히 넘어갈 문제가 아닌 상황임
     * 통화 시작자가 어떻게 호출 제어 메시지(예: SIP)를 볼 수 있었는지 정말 궁금함, 이런 메시지들은 핸드셋과 기지국(MME) 사이에 암호화된 GRE 터널 안에 있다고 생각했음, 만약 누군가 GRE 터널 암호화를 풀었다면 엄청난 보안 취약점임, 아마 OP가 자기 디바이스에서 분석 중이라 가능한 걸 수도 있는데, 그럼에도 암호화 이전 페이로드를 볼 수 있다니 놀라운 부분임
          + 기사 에디터임, Qualcomm 칩 내장 안드로이드 기기들은 대부분 USB로 모뎀 진단 포트를 노출할 수 있는 옵션이 있어서 루팅도 필요없음, NSG를 루팅해서 쓰는 게 노트북 들고 다니는 것보다 훨씬 편해서 선호함, Scat(https://github.com/fgsect/scat)을 모뎀 진단 포트 활성화와 함께 쓰면 모든 신호 트래픽을 볼 수 있음
          + 루팅된 안드로이드 폰과 Network Signal Guru(https://play.google.com/store/apps/details?id=com.qtrun.QuickTest) 앱을 쓰는 중임, 무료 버전에서는 실제로 '복호화'하지 않지만, 루트 권한과 모뎀 접근으로 이런 로그를 읽을 수 있음, 특정 밴드를 끄거나 특정 기지국에만 연결되게 시킬 수 있어, 데이터 전용으로 사용할 때 편리함
          + 많은 통신사들이 VoLTE용 SIP 신호를 P-CSCF에서 끝나는 IPsec 전송으로 설정은 하는데, 대부분(혹은 전부)은 IPsec을 무결성만 보장하도록 설정함
          + 수정: GRE가 아니라 GTP임
          + GTP 터널을 의미한 것 같음, GTP 터널은 enodeb와 코어 네트워크 사이에서 작동하는데, IPSEC 내에서만 보안이 됨
     * O2가 아직도 비즈니스를 유지하는 게 신기함, 다른 네트워크보다 훨씬 별로임, 심지어 백홀 이슈가 심한 Three보다도 떨어짐, 내가 O2 SIM을 갖고 있는 유일한 이유는 Priority 티켓과 그들의 공연장에서 쓸 신호 때문임
          + 5G Standalone 네트워크에 접속할 수 있다면 훨씬 더 좋아짐, 다만 새 SIM 카드와 호환 가능한 핸드폰이 필요함, 체감이 확연히 다름
     * 꽤 심각한 문제라고 생각함, 폰 루팅하고 NSG 설치해서 이런 정보를 보는 게 그리 어렵지 않음, O2는 영국 최대 이동통신사이기도 하며, 정부와도 계약이 있음, 답장을 안 주는 게 실망스럽긴 하지만, 예상했던 일이기도 함, O2는 내부적으로 혼란스러운 상태임, 매장에서 해결할 수 없는 건 고치는 데 정말 오래 걸림(예: 번호 포팅 문제), 시스템도 낡아 있고, 일부 고객들은 아직 VoLTE를 못 쓰고, 5G SA는 음성 통화를 지원 안 하고 n28에 과도하게 의존해서 느린 경우가 많음, CTO는 ""허영심 수치 버리고 본질에 집중하자""는 취지로 블로그를 쓰지만 정작 데이터 품질은 늘 꼴찌임, 관련 블로그(https://news.virginmediao2.co.uk/leaving-the-vanity-metrics-behind-and…) 참고
          + EU 로밍 요금을 안 받는 이유가 요금 시스템이 없어서 그런 건 아닌가 하는 생각이 들기 시작함
     * VoLTE를 꺼서 이 문제를 막을 수 있는지 궁금함, iPhone 11은 오프 방법을 찾았는데 iPhone 15에는 그 옵션이 없음
          + 4G Calling(VoLTE) 비활성화해도 이런 헤더는 노출됨, 그리고 기기가 꺼져 있어도 마지막 연결된 셀 위치와 시간은 여전히 노출됨, 그러므로 효과가 없음
          + O2 UK는 VoLTE를 기존 선불(PAYG) 고객에게 지원하지 않음, 요금제 고객만 지원함, 지금은 오히려 다행이라고 생각하게 되는 상황임
     * IMS에 아는 건 없지만, 디버그 헤더가 전송될 만큼은 통화를 오래 유지해야 하는 것 아닐까 싶은 의문임, 마치 스파이 영화의 통화 추적처럼, 만약 그렇다면 미지의 번호는 그냥 안 받는 걸로 피해볼 수 있는지 궁금증, 물론 이것도 아는 사람이 번호로 연락하면 똑같이 노출될 수 있음
          + 이런 정보는 통화 연결이 되기도 전에 네트워크에서 이미 알고 있는 정보임, 아마 디버깅용 헤더여서, 연결이 안 되는 상황에서도 디버깅을 위해 필요함, 이해가 맞다면, 기기가 꺼져 있어도 마지막 사용 셀 정보를 제공함
          + IMS는 그냥 SIP 코어+ 여러 게이트웨이+기본 LTE 인프라(예: eNodeB, PCRF 등)로 구성된 구조임, 여기서 신호 메시지는 그냥 SIP 메시지임, 만약 이런 헤더가 SIP 180 Ringing 등에도 들어 있다면, 아예 전화를 받지 않아도 정보가 노출될 수 있음, 통신사에서 실제 IMS 구축했던 경험 기반 설명임
     * O2 NZ에도 이 문제가 있는지 궁금함, 오스트레일리아에서 무제한 로밍과 VoLTE 통화 때문에 지난주에 옮겼음
          + 아마도 해당 이슈는 O2 UK에만 해당하는 문제일 가능성이 높음
"
"https://news.hada.io/topic?id=20976","보이니치 필사본을 SBERT로 모델링하여 구조를 탐구함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     보이니치 필사본을 SBERT로 모델링하여 구조를 탐구함

     * 보이니치 필사본의 언어 구조를 SBERT 등 최신 NLP 기법으로 분석한 오픈소스 프로젝트임
     * 접미사 제거와 군집화 등에 중점을 두어, 가짜 패턴이 아닌 실제 언어 유사 구조가 있는지 검증함
     * 함수 단어와 내용 단어 역할, 전이 행렬 등 다각적 구조 분석 결과 의미 있는 패턴이 관찰됨
     * 전통적 통계 또는 추측 중심 접근과 달리, 컴퓨팅 언어학 방식으로 구조적 언어 특징 분석 시도임
     * 의미 번역 시도 없이 구조적 모델링에만 집중한 프로젝트로, 추가 연구와 비교 실험이 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

📜 보이니치 필사본 구조 분석 프로젝트 소개

  🔍 개요

     * 이 프로젝트는 현대 자연어처리(NLP) 도구를 활용하여 보이니치 필사본의 구조를 분석하고자 하는 개인적 실험에서 출발함
     * 군집화, 품사 추론, Markov 전이, 섹션별 패턴 추출 등 실제 언어 모델링 방법을 적용함
     * 의미 해석이나 번역 시도, 또는 과장된 패턴 추정 없이, 언어처럼 동작하는 구조 유무만을 검증함
     * 접미사 제거, SBERT 임베딩, 렉시콘 가설 생성 등 모든 단계가 공개됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  🧠 의의

     * 보이니치 필사본은 아직 해독되지 않은 미스터리 문서로, 언어적/암호학적 해법이 없음
     * 기존 분석은 통계적 엔트로피 검사와 비과학적 추측으로 양분됨
     * 본 프로젝트는 컴퓨팅 언어학에 기반해, 실제 언어와 비슷한 구조적 패턴 유무를 중립적으로 탐구함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  📁 프로젝트 구조

     * /data/
          + 전체 필사본 전사본, 루트 단어 종류, 군집 ID, 제거된 접미사 목록, 각 라인 군집 시퀀스 등 데이터 제공
     * /scripts/
          + SBERT 기반 단어 군집화, 품사 예측, Markov 전이 행렬 구축, 렉시콘 후보 생성 등 핵심 분석 스크립트 제공
     * /results/
          + 군집 시각화, 전이 행렬 히트맵, 군집별 요약 등 분석 결과 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  ✅ 주요 기여 내용

     * 다국어 SBERT로 접미사를 제거한 루트 단어 군집화
     * 함수 단어 유사 군집과 내용 단어 유사 군집 구별
     * Markov 방식의 군집 전이 구조 모델링
     * 섹션별 구문 구조(예: Botanical, Biological 등) 분석
     * 데이터 기반 렉시콘 가설 표 생성
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  🔧 전처리 결정

     * 군집화 전 반복적 접미사(예: aiin, dy, chy 등)를 제거함
     * 이를 통해 단어의 루트 형태를 추출, 군집 집중도와 구조 패턴이 더 명확해짐
          + 접미사는 음운 채움, 문법소, 암송, 반복 또는 무의미 노이즈일 가능성 있음
     * 하지만, 이 선택으로 형태소 정보 손실, 의미 있는 굴절 정보 은폐, 기능어 편향성 등 한계가 있음
     * 접미사 제거 없는 비교 실험도 가치가 있음 — 누구든 파생 실험 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  📈 주요 분석 결과

     * Cluster 8: 매우 자주 등장, 다양성 낮고 줄 시작 빈번 — 함수어 군집 후보임
     * Cluster 3: 다양성 높고 위치 자유 — 루트 내용어 군집 후보임
     * 전이 행렬: 무작위성에서 멀리 떨어진 강한 내부 구조
     * 군집 및 품사 패턴: 필사본 섹션(예: Biological, Botanical 등)별로 다름
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  🧬 가설

     * 필사본은 음절 반복과 위치적 반복을 활용한 구조화된 인공/암기 언어임
     * 구문, 기능/내용 분리, 섹션 반응 언어 전이 등 언어적 구조를 분명히 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  📊 예시 시각화

     * Figure 1: SBERT 군집 임베딩(PCA 축소)
     * Figure 2: 전이 행렬 히트맵
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  📌 한계

     * 군집-단어 매핑이 간접적이라 빈도 추정 겹침 현상 있음
     * 접미사 제거 기준은 휴리스틱이며, 실제 의미 있는 끝소리도 손실 가능성 있음
     * 의미 해석은 시도하지 않고 구조 분석에만 집중함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  ✍️ 저자 메모

     * AI, NLP, 구조 분석 학습을 목적으로 시작한 프로젝트임
     * 필사본 해독 자체가 목표가 아니라, 최신 도구로 구조를 이해하는 것이 더 발전적이라 생각함
     * Rosetta Stone식 해독 기대보다는, “모델링 자체가 의미”에 관심 있는 사람을 환영함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  🤝 기여 안내

     * 본 프로젝트는 언어학자, 암호학자, 인공언어 연구자, 컴퓨터 언어학 커뮤니티 모두의 협업과 확장을 환영함

        Hacker News 의견

     * 나는 당신이 PCA 투영 내에서 군집을 찾고 있다는 점을 봤음 — 더 깊은 구조를 찾으려면 PaCMAP이나 LocalMAP 같은 최신 차원 축소 알고리즘을 추천하고 싶음. 나는 Pol.is라는 의견조사 툴의 데이터를 가져와 PCA 대신 이런 차원축소 알고리즘으로 다시 투영하는 프로젝트를 진행 중임. 이런 새로운 알고리즘이 예전에는 보지 못했던 통찰을 제공하는 것에 감탄함. 내가 그룹을 색칠해서 시각적으로 표현한 결과도 있으니 데스크톱에서 확인해 보라는 안내. Pol.is가 무엇인지 궁금하다면 관련 기사도 추천함
          + PaCMAP이나 LocalMAP을 처음 알게 해줘 고마움 — 구조를 잘 유지하는 이런 차원 축소 접근법이 PCA보다 이 데이터에 더욱 잘 맞을 것 같음. 덕분에 관심이 생겼으니 더 깊게 살펴볼 계획임
          + 나는 임베딩 축소에서 PCA나 t-SNE보다 UMAP이 훨씬 좋은 결과를 주었다는 경험을 가짐
     * 여기서 사용한 텍스트 임베딩 모델은 paraphrase-multilingual-MiniLM-L12-v2로 약 4년 된 모델임. 자연어처리 세계에서 이 정도면 매우 옛날 모델임. 최근 LLM의 발달로 임베딩 모델의 정보 표현력과 임베딩 공간 내의 구분 능력이 극적으로 향상됨. 다국어 지원이 목적이 아닌 최신 임베딩 모델조차 이런 타입의 데이터에서 훌륭한 성능을 나타냄. 따라서 상대적으로 알려지지 않은 언어인 Voynich Manuscript에도 더 나은 성능을 보일 수 있을 것임. 전통적인 NLP 기법(접미사 제거, 품사 식별 등)은 오히려 필요한 문맥 정보를 손실시켜 임베딩 품질을 저하시킬 위험이 있다고 봄
          + 나는 주로 속도와 호환성 때문에 paraphrase-multilingual-MiniLM-L12-v2를 선택했지만, 지금 기준에선 오래된 모델인 건 확실하다는 점에 동의함. all-mpnet-base-v2나 text-embedding-ada-002 같은 최신 모델로, 접미사도 유지하며 풀-컨텍스트 임베딩을 시도하면 더 흥미로운 결과가 나올 듯함. 당신의 지적 덕분에 다음 반복을 긍정적으로 고려하게 됨
     * 나는 NLP에 대해 잘 모름. 비교군을 통제하는 방식으로 진행 과정을 점검하는 것이 합리적일지 궁금함. 예를 들어, 사람이 언어처럼 보이지만 실제로는 언어가 아닌 문장을 쓰게 한 뒤 동일한 절차(접미사 삭제, 군집화 시도 등)를 거치면 비슷한 결과가 나오는지 알고 싶음
          + 바로 그거임. 왜 그냥 100명에게 Voynich 원고를 쓰게 한 다음 그 데이터로 훈련하지 않았는지 궁금함
     * UMAP이나 t-SNE로 분석하면 좋을 것으로 생각함, 비록 PCA에서 이미 깔끔한 분리 결과가 나왔더라도 말임. 각 군집을 서로 참조해 맵핑하는 것도 분석 내 변동성이 남아 있지 않다는 점을 알려주는 좋은 방법으로 보임
          + 좋은 지적임 — PCA로 처음에는 예상외의 깔끔한 분리가 나와서 처음엔 그걸로 진행함. 하지만 당신 말대로 UMAP이나 t-SNE를 적용하면 비선형적 관점에서 더 미묘한 패턴이나 문제를 발견할 수 있을 것임. 군집 간 유사도 매트릭스도 만들지 않았지만, 당신의 제안을 듣고 나니 신호가 얼마나 실질적으로 포착되는지 검증하는 자연스러운 다음 단계처럼 느껴짐. 후속 작업으로 꼭 시도해야겠다는 생각임. 생각을 자극해줘서 고마움
          + 이 참조 매핑이 어떻게 수행되는지 예시를 가지고 있으면 궁금함. 나는 다른 모달리티의 임베딩에서 이런 걸 적용해보고 싶지만 NLP 쪽 경험이 부족함
          + PCA로 충분히 분리가 잘 나왔을 땐 개별 포인트 사이의 거리를 해석하기 쉬워서 나는 UMAP을 피하는 편임. t-SNE는 거리 해석이 거의 무의미하다고 생각해서 항상 피하는 편임. 이건 어디까지나 내 개인적 취향임
     * 나는 이 가설이 가장 흥미롭다고 생각함: 어떤 저자가 Voynichese를 게르만 계 언어로 간주해서 상당한 진전을 이뤄낸 듯 보임. 우랄어나 핀-우그르 계 언어라는 주장도 본 적 있음. 당신의 방법론이 아주 좋다고 생각하며, 특정 언어 계열을 대상으로 맞춤 적용하면 더 좋은 결과가 나올지 궁금함
          + Edward Kelly가 제때 적소에 있었고 Cardan grille(카르단 그릴)를 알았다는 증거도 예전에 본 적 있음. 그래서 그는 저작권자일 가능성이 크고, 책 자체는 사기나 장난 목적으로 만들어졌을 거라는 쪽으로 생각하고 있음
          + 이 스레드는 여러가지 “해독” 주장들을 논의하고 있음. Bernholz 사이트는 괜찮지만, Child 작업은 실제로 해독에 큰 도움을 주지 못함
     * 나는 이게 오래된 터키어라고 생각하고 있었음
          + 원고 영어 번역은 여기에서 볼 수 있음
     * README에서 놓쳤거나 못 본 걸 수도 있지만, “단어”의 초기 인코딩을 어떻게 했는지 궁금함. 예를 들어 “okeeodair” 같은 단어는 어떻게 원본 심볼로 다시 매핑하는지 알고 싶음
          + 맞음, “okeeodair” 같은 단어는 EVA 전사 파일에서 바로 온 것임. EVA(유럽 Voynich 알파벳) 체계를 기준으로 원래의 Voynich 기호를 ASCII로 연결한 결과임. 이번 프로젝트에서 기호 그 자체를 다루기보다는 EVA 전사 기준 단어를 바로 활용함. 데이터셋 안에 “okeeodair”가 있다면 누군가(전문가)가 그런 기호 조합을 저 이름으로 부르기로 합의한 것임. 전사에 관한 정보는 이 사이트에서 볼 수 있음
     * 내가 상상하는 건, 그게 단순히 아무 의미 없는 낙서에 불과하고 암호문조차 아니라면, 필사본 특성상 스타일, 필체, 쓰이는 단어, 심지어 글자 자체도 첫 페이지부터 마지막까지 진화해야 한다는 점임. 물론 페이지 순서가 바뀔 수 있지만 뭔가 차이가 보여야 한다고 생각함. 저자가 비슷한 스타일로 수십 권을 썼으나 모두 사라진 것이 아닌 이상 말임. 새로운 아이디어는 아니지만 이런 패턴에 대한 분석이 있었는지 궁금하며, 페이지 간 일관성에 대한 언급을 본 적 없음
          + 페이지 간 일관성에 관해선 많은 연구가 있음. 두 명(또는 다섯 명)의 필경사가 있었다는 주장이 전문가들 사이에 존재함. Lisa Fagin Davis의 주장을 바탕으로 한 실험 논의도 있으니 참고하면 도움됨
     * “브루트 포스” 방식으로 해독하려면 얼마나 자원이 들지 궁금함. 예를 들어, 알려진 언어 단어와 하나씩 매핑해 점수를 높여가는 식의 명확한 과정을 따라가면 어떨지 생각해봄
          + 이런 방식은 각 단어가 1:1로 매핑된다는 전제가 필요하지만 언어란 본래 그런 식으로 꼭 떨어지지 않는다는 점을 지적하고 싶음. 예를 들어 합성어는 이런 방식으론 매핑하기 어려움. 문화적 차이로 인한 더 근본적인 의미 구조 차이도 존재함
          + 흥미로운 질문임 — 사실 나도 비슷한 생각을 한 적이 있음. 암호학 전공자는 아니라서 정말 대규모로 “브루트 포스”가 얼마나 현실적인지 잘 모르겠음. 하지만 각 Voynich “단어”를 실제 언어 단어와 매핑해 일관성을 최적화한다는 접근은 꽤 여러 실험적 시도와 방향이 일치함. 어려운 점은 어휘량 자체가 엄청 많고 “단어”라는 단위가 실제 언어 단어와 1:1로 매핑되는지도 불분명하다는 것임. Voynich의 “단어”가 진짜 하나의 어휘인지, 조각인지, 어근-접사 결합인지 모호함. 이런 점에서 단순 매핑은 상당히 어렵다고 봄. 그렇지만 개별 토큰 대신 군집 ID로 해보고 언어모델로 결과를 채점하는 방식은 꽤 괜찮은 아이디어임. 최적화나 진화적 기법에 시도해볼 가치가 있다고 생각됨. 구조가 얼마나 “언어처럼” 보이는지에 대한 시사점을 얻을
            수도 있음. 좋은 아이디어 고마움. 관련 분야 전문가가 이 의견을 보고 시도해보길 기대함
     * 알려진 언어의 유사한 분량 텍스트로 분석을 했을 때 유사한 패턴이 나오는지 궁금함. 달리 말하면, 이 분석 기법을 다양한 종류의 텍스트에 적용해서 이 문자 체계가 뭘 의미하는지 이해하는 데 도움될 수 있을지 질문함
"
"https://news.hada.io/topic?id=20981","닌텐도 64에서 팔레트 조명 트릭 사용법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         닌텐도 64에서 팔레트 조명 트릭 사용법

     * 이 글은 닌텐도 64 데모 개발 과정에서 적용한 팔레트 기반 조명 및 노멀 매핑 기술을 설명함
     * 직접 텍스처에 즉시 라이팅을 반영하는 대신, 팔레트만 변경해 텍스처 전체에 조명 효과를 주는 방법을 소개함
     * diffuse/normal 팔레트 압축 및 오브젝트 공간 노멀 매핑 등 다양한 최적화 기법을 활용함
     * 이 방식은 방향성 조명에 한해 효율적이며, 쉐이딩 불연속성 등 단점이 존재함
     * 데모에서는 반사광/직접광/환경광 등 여러 요소가 창의적으로 결합되어 N64 한계에서 인상적 비주얼을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 목표

     * 이 글은 Bluesky에서 시작한 스레드의 연장선으로, 닌텐도 64용 데모(Revision 2025)에서 활용한 진보된 라이팅 기법을 공유함
     * 데모에는 반사광이 적용된 노멀 매핑, 실시간 반사 조명 등 다양한 효과가 포함되고, 음악은 noby가 작곡, Moloko가 기타를 연주함

닌텐도 64에서의 노멀 매핑 가능성

     * WadeTyhon과 Spooky Iluha 등 홈브루 개발자들의 실험을 참고해 N64에서의 노멀 매핑 구현 가능성을 검증함
     * 기본 방식은 실행 시간에 CPU에서 텍스처에 직접 조명을 연산하는 것임
     * 하드웨어 지원 없이 CPU로 쉐이딩 커스텀 코드 실행 가능하지만, 속도 저하 문제가 큼

팔레트 기반 쉐이딩

     * 텍스처 공간 쉐이딩을 직접 적용하는 대신, 팔레트 텍스처의 팔레트 데이터만 업데이트하면 전체 텍스처가 실시간으로 밝기 변화를 반영함
     * N64는 팔레트 텍스처 사용이 흔하므로 흔히 활용 가능함
     * 팔레트만 업데이트해도 각 텍셀별로 실제 조명을 적용한 것 같은 효과가 즉시 나타남
     * 원본 팔레트를 쉐이딩 적용된 팔레트로 교체하고, 기존 팔레트 텍스처를 오브젝트에 일반 텍스처로 매핑함
     * 디퓨즈(dot(N,L)) 라이팅만 적용해도 상당히 뛰어난 결과물을 보여줌

오브젝트 공간 노멀 매핑

     * 일반적으로 노멀 매핑은 텐전트 공간에서 이루어지며, 반복 텍스처 지원 및 자연스러운 표면 보정에 적합함
     * 오브젝트 공간 노멀 맵은 각 텍셀이 정확한 표면 노멀 정보를 가지므로 계산이 단순하지만, 반복 텍스처 활용이 어려움
     * 고해상도 노멀 맵을 32색 팔레트로 압축해도 원본과 유사한 특성을 유지 가능함

디퓨즈와 노멀이 공유된 팔레트 설계

     * 오브젝트는 디퓨즈 텍스처(basecolor * ao)와 노멀 맵을 가짐
     * 두 텍스처 모두 K-means 군집 알고리듬으로 생성된 동일한 팔레트 인덱스를 공유하도록 구성함
     * 이미지를 6채널 이미지로 간주해 클러스터링을 진행함
     * 예시에서는 RGB 디퓨즈 + 노멀 맵을 16색 팔레트로 압축해, 이미지 데이터는 4bpp만 기록하면 됨
     * 쉐이딩 시, 각 팔레트 색상에 대해 노멀 및 표면 색상 정보를 인덱스로 조회해 새 RGB 색상을 생성함
     * 이 방식은 방향광만 제대로 지원 가능하고, 팔레트만으로 섀도우를 구현하긴 어려움

굽기(baked)된 방향성 환경광/태양광

     * 건물의 사실적인 라이팅을 구현하고자, 버텍스 컬러의 RGB와 알파 채널을 환경광, 태양광에 각각 사용함
     * 환경광(ambient)은 방향성 강도(그레이스케일 환경맵) 와 컬러(RGB, 채도 강화)로 분리함
     * 태양광(direct)은 버텍스 알파에 전달함
     * 기본 라이팅 공식은 아래와 같음
ambient = vertex_rgb      * grey_irradiance_map(N)
direct  = vertex_alpha    * sun_color * dot(N, sun_dir)
color   = diffuse_texture * (ambient + direct)

     * 각각의 항들이 합쳐져 최종 색상을 만듦
     * 방향성 환경광은 빵빵하게 자연광 효과를 내며, 팔레트 기반이지만 높은 품질의 질감을 연출함
     * 환경맵은 단순화를 위해 등각원통 투영(equirectangular projection)을 활용함

반복 텍스처 적용 대형 모델의 쉐이딩

     * 초기 알고리듬은 단일 오브젝트용이며, 대형 캐슬 메시는 반복 텍스처 사용으로 인해 문제가 발생함
     * 해결을 위해 Blender를 활용해, 각 표면 방향/재질별로 메시를 서브메시 단위로 분리함
     * 컴퓨터는 각 그룹에 폴리곤 노멀을 이용해 world-to-model 행렬을 산출함(근사 텐전트 공간)
     * 각 그룹은 하나의 팔레트를 공유해, 전체적으로는 평균적인 라이팅 품질이 보장됨
     * 텐전트 공간이 런타임에 보간되지 않아 면이 깎인 듯한(face) 라이팅이 나타나는 단점이 존재함

스페큘러(반사광) 쉐이딩

     * 여러 표면 지점이 동일한 팔레트 색상을 공유하므로, 정확한 포인트 라이트/반사광 쉐이딩은 불가능함
     * 팔레트 공간 기법은 방향성 디퓨즈 라이팅에 한해 효율적임
     * 그래도 구형 개체를 가정해, 각 점을 p = radius * normal로 근사하여 스페큘러 반사광 효과를 억지로 구현함
     * 결과는 다소 불연속적이지만, 플레이 중 실제로는 상당히 자연스러운 느낌을 줄 수 있음

한계와 미래

     * 데모에서는 쉐이딩 불연속성, 흑백 텍스처만 지원, 포인트라이트 미지원 등의 한계를 최대한 숨겼음
     * elaborate preprocessing(복잡한 사전 처리)이 필수적임
     * 쉐이딩 불연속성 없이 ambient/direct 조명 모두 지원하는 방법은 여전히 도전 과제임
     * 실험 결과에 대해 새로운 가능성과 아이디어의 흥미로움을 강조함
     * PAL 호환 N64 ROM 파일도 공개되어 있음. 단, 불안정하여 자주 다운됨

기타

     * 저자는 책 집필도 고려 중이며, 관심이 있다면 이곳에서 소식을 받아볼 수 있음

        Hacker News 의견

     * N64에서 ""리얼리스틱"" 그래픽을 보는 것은 정말 인상적인 경험이라는 감상 표현과 함께, 이 데모가 PS2용 ""ICO""를 떠올리게 하는 느낌 전달. N64 그래픽 하드웨어를 추상화하고 현대의 프리미티브, 라이팅, 셰이딩, 베이크 라이팅 툴 등을 제공하는 SDK 제작 가능성에 대한 궁금증 공유. N64 하드웨어가 세대 특유의 독특한 구조를 가졌다는 언급과, 상세한 하드웨어 정보 링크 제공
          + N64가 SGI에 의해 설계된 점과, SGI가 3D 그래픽에 얼마나 영향을 끼쳤는지 언급. N64가 오히려 세대에서 가장 표준적인 하드웨어를 가졌을 것으로 추정. 오히려 오픈GL 라이브러리가 없으면 놀라울 정도라는 견해. 단점으로, 콘솔을 그래픽 카드에 CPU를 덧붙인 것으로 생각해야 하고, 그래픽 시스템이 직접적으로 노출되어 있다는 점을 지적. 그래픽 칩 아키텍처는 서로 호환성이 없고, 업체들은 이런 내부 구조 공개를 피하며 API(OpenGL, DirectX 등)만 제공해 창의적 설계가 가능하지만 하드웨어 직접 접근은 매우 어렵다는 설명. 부연 정보로 OpenGL이 SGI에서 유래했으며, nvidia도 SGI 출신들이 창업했다는 배경 지식 제공
          + ""Shadow of the Colossus..."" 언급과 함께 관련 유튜브 링크 공유
     * N64 그래픽 트릭을 다루는 본문이 ""이게 미래인가?""라는 질문으로 끝나는 점이 마음에 든다는 감탄 표현
          + 요즘 인디 N64 개발이 엄청난 활기를 보이고 있다는 현황 설명. 인기 게임 수십 개가 디컴파일되어 소스코드로 공개되고 PC 포팅이 쉬워졌으며, 하드웨어에서 작동하는 모드도 다양하게 만들어진다는 부분 강조. Zelda 팬 리메이크와 새로운 던전·스토리를 가진 완성 게임 사례, Mario 64 커뮤니티의 활발한 기술적 최적화, Kaze라는 인물의 독자 엔진 개발 및 시퀄 창작, 기술적 딥다이브 영상 자료 추천. 포탈 등 기상천외한 데모 제작 사례와, Valve의 법적 관심을 받은 이야기. Rare의 미공개작 Dinosaur Planet 등이 유출된 후 디컴파일 및 복원되고 다시 인디 씬에서 부흥하는 현상 등 상세 링크와 함께 생생하게 소개
     * 게임 엔지니어들이 제한된 하드웨어에서 창의적인 해법을 만들어낸 천재성에 감탄하는 마음 표현
          + 제약이 있을 때 최고 수준의 창의력이 발휘된다는 원칙 공유. pico8, Animal Well, 여러 인상적 게임들의 비밀이 바로 그것. 이번 주말에 내가 만든 2d-pixel-art-game-maker-maker 아키텍처를 크게 개선하는 아이디어가 떠올라 출시가 또 한 달 연기될 것 같은 아쉬움 토로
          + 이번에 소개된 내용이 N64 전성기 당시가 아니라 최근 이루어진 신작이라는 사실 전달
          + 그 시절 N64 개발자가 아닌, 2025년 기준 데모신(demoscene)과 관련된 신기술임을 밝혀두는 안내
     * 지금은 더 빠른 시스템이 나와서 좋지만, 과거 게임에서 한계를 극복하는 재미와 그 과정을 제대로 해냈을 때의 만족감은 특별했다는 회상. 해커뉴스 이용자라면 '래스터 인터럽트(raster interrupts)'와 'racing the beam' 개념이 익숙할 것이라는 설명과 함께, Atari 800에서 그런 기술로 불가능했던 일을 가능하게 했던 일화를 소개. 최근에서야 Atari 2600 게임들이 이런 미친 방식의 영향을 많이 받았다는 걸 알게 되었다는 발언과 유튜브 자료 공유. 만약 앞으로 하드웨어 발전이 멈춘다 해도, 우리는 수십 년간 계속 새로운 흥미로운 트릭을 발견할 수 있다고 확신하는 자세
     * 90년대에 팔레트 기반 라이팅 기법을 우리 shareware 게임에서 썼던 경험 회상. VGA 256컬러 팔레트를 각 색상별로 점진적 명암 변화를 갖게 배열해뒀고, 색상 인덱스만 더하고 빼서 손쉽게 밝기 연출이 가능했다는 설명
     * 데모신과 이와 같은 작업들이 감탄스러운 수준이지만, 주로 단순하고 비어 있는 씬에 치우치는 경향이 느껴진다는 관찰. 이런 기법들이 대개 배경이나 한정적 게임 기능에 쓰일 만하다는 분석. FastDoom이나 Mario-64 최적화 프로젝트처럼 구형 하드웨어에서 성능을 크게 끌어올리고 오히려 콘텐츠와 기능을 추가하는 시도가 훨씬 인상적이라는 견해. 데모신과 이런 보다 완성도 높은 프로젝트 간에 연결고리가 있을 지도 모른다는 생각
     * PS1과 PS2 시절의 최적화 기술을 그리워하는 마음. 대부분 에뮬레이션을 통해 1080p나 4k 이상 고해상도로 업스케일해도 여전히 멋지다는 감상. Halo 2의 4k 그래픽이면 충분하다고 느낀다는 의견과, GT3에서 실제로 만들어낸 '열파' 효과 데모를 예로 들며 PS3에서는 PS2만큼 빨리 구현할 수 없다는 제작자 멘트 인용. 요즘과 같은 UE5 엔진의 사실적인 열파가 아니라, 성능에 큰 부담을 주지 않는 트릭 방식이 오히려 더 좋다는 생각. RTX 사용으로 프레임 저하를 겪을 바엔 과거의 트릭이 낫다고 밝힘. 299MHz MIPS CPU에서 Shadow of the Colossus, GoW2, FFXII, GT4, Black, Valkyrie Profile 2, Rouge Galaxy, Burnout 3, Jak and Daxter, Ratchet 등 엄청난 게임이 작동했던 사실과 GameCube의 RE4, Metroid, Zelda 등을 언급. 전통 게임 개발자들의 실력에 경탄과 존경이 담긴 표현으로 마무리
          + GoW2 영상이 PCSX2 에뮬레이터로 캡처된 것이고, 업스케일과 기타 강화 효과가 들어갔을 것이라는 점을 지적. 어쨌든 PS2에서 GoW2는 엄청난 성취였다고 생각
          + PS2에 관해서는 동의하지만 PS1에서는 성능이 그저 그랬다는 견해. PSX 성능은 펜티엄90~100 정도지만, MMX 펜티엄에 3DFX를 얹으면 N64와 비슷하거나 능가했다는 의견. MIPS CPU가 낮은 클럭에서 뛰어난 성능을 내는 사례로 PSP, SGI Irix, PS2를 언급. PS2의 GPU는 R4k CPU와는 별도라는 설명과 함께, PS2용 Deus Ex 포팅은 PC판에 비해 부족했으며, 언리얼 엔진을 완전히 소화하지 못했다는 실제 경험 공유. PS2가 놀라운 특수효과는 보여줬지만, 디우스 엑스와 같은 포트에선 맵 크기가 매우 작았다는 배경 지식
          + Halo 3 그래픽이 최신 게임보다 더 좋아 보인다는 소신. 블러, 블룸, 풀과 나뭇잎이 튕겨나오는 효과 등 요즘 추가된 효과는 오히려 안 좋은 시각적 경험을 준다는 생각. 빠른 속도의 FPS 장르에서는 미세한 폴리곤 카운트는 별 의미 없고, 텍스처 해상도도 그냥 충분하다고 느낌. 실질적으로 느끼는 건 하드웨어 요구량뿐이라는 자기 경험 공유
"
"https://news.hada.io/topic?id=21011","Have I Been Pwned 2.0","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Have I Been Pwned 2.0

     * 데이터 유출 확인 서비스 Have I Been Pwned가 완전히 새롭게 리뉴얼됨
     * 새로운 디자인과 함께 주요 웹 페이지의 기능이 대폭 변경 및 개선됨
     * 검색 기능이 더욱 직관적으로 변하고, 계정 확인 방법과 데이터 브리치 각종 사례 안내가 강화됨
     * 사용자 대시보드, 도메인 검색, API 문서 등 다양한 신규·개선 기능이 추가됨
     * 웹 성능과 보안이 최신 클라우드 인프라 위에 구현되어 빠르고 안전한 사용자 경험 제공됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 배경

     * Have I Been Pwned(이하 HIBP) 2.0은 오랜 기간의 개발 끝에 완전히 새롭게 공개됨
     * 2023년 2월에 첫 커밋, 2024년 3월에 소프트 런칭 및 오픈 소스화 과정을 거쳐, 전면 재구축 및 새로운 브랜드 정체성이 적용된 사이트로 오픈됨
     * 전체 사이트 구조와 기능이 개편되었고, 새로운 기능들과 함께 머천다이즈 스토어도 오픈됨

검색 기능

     * HIBP의 대표 기능인 대문 검색창이 더욱 직관적이고 신선한 연출(컨페티 애니메이션)로 개선됨
     * 유저 경험을 부담스럽지 않게 만들기 위해 무겁고 부정적인 분위기를 지양, 사용자에게 사실 기반의 실용적 정보를 제공하는 데 집중함
     * 유저네임 및 전화번호 검색은 웹사이트에서 제거됨(단, API에서는 기존 방식 유지)
          + 이메일 주소 기반 검색이 파싱, 알림, 서비스 일관성 측면에서 더 적합함
          + 전화번호, 유저네임은 데이터 처리 부담이 높고, 실제 거의 사용되지 않아 혼선을 줄이기 위해 제외 결정함

데이터 브리치 케이스 페이지

     * 모든 브리치(유출) 건마다 전용 상세 페이지가 새롭게 제공됨
     * 기존보다 한층 직관적이고 보기 좋은 레이아웃으로 피해 상황, 대응 방안 등 구체적이고 실행 가능한 맞춤 조언을 안내함
     * 타 기관(예: NCSC)과의 협력으로 지역별 맞춤 정보 등 추가 예정
     * 앞으로 2FA, 패스키 등 지원 여부, 사용자 맞춤 가이드 등 세부 정보가 추가될 계획임

대시보드

     * 기존 여러 기능(민감 브리치 확인, 도메인 관리, 구독 관리 등)을 통합 대시보드로 일원화함
     * 대시보드는 이메일 인증 기반으로 접근, 앞으로는 패스키 등 새로운 인증 방식도 추가될 예정임
     * 가족 계정 알림 등 향후 확장 가능한 플랫폼으로 발전 가능성 제공

도메인 검색 기능

     * 도메인 검증/검색 기능이 전면 재설계되어, 더욱 깔끔한 UI와 다양한 필터 지원(예: 최신 유출만 보기) 추가
     * 완전한 싱글 페이지 앱(SPA) 구조에, 검색 결과는 API를 통해 JSON으로 신속하게 제공됨
     * 도메인 소유권 확인 프로세스도 새롭게 단순화됨
     * 이메일 외 인증 방식은 별도 개선 예정

API

     * 이번 업데이트에서 API 자체의 변화나 중단은 전혀 없음
     * API 문서화는 OpenAPI 기반 Scalar 도구 도입을 준비 중이나, 현재는 기존 문서를 유지하면서 새로운 스타일로 통일됨
     * 추후 Scalar 기반 최신 문서로 전환 예정

머천다이즈·스티커

     * HIBP 브랜딩 굿즈샵이 공식 오픈되어 티셔츠 등 상품 판매 시작(Teespring 기반, 마진 없음)
     * 스티커는 Sticker Mule 스토어에서 계속 운영, 아트워크는 오픈소스로 자유롭게 사용 가능

기술 및 인프라

     * 사이트 백엔드는 Microsoft Azure 기반으로, App Service, Functions, Hyperscale SQL, Storage 등 사용
     * 주요 웹 앱은 C#과 .NET 9.0, ASP.NET MVC(.NET Core)로 작성됨
     * Cloudflare가 WAF, 캐싱, Turnstile(anti-bot), R2 스토리지 등 요긴하게 쓰임
     * 프론트엔드에서는 최신 Bootstrap, SASS, TypeScript를 기반으로 현대적 인터페이스를 구현
     * Iceland 기반 개발자 Ingiber 등 핵심 멤버들의 기여로 높은 완성도와 미려한 UI 달성함
     * 웹 페이지 용량과 요청 수를 각각 28%, 31% 가량 절감해, 11년 전보다 효과적으로 최적화됨
     * 트래킹, 광고데이터 등 불필요한 요소는 완전 배제, 유저 프라이버시 중시

AI 활용

     * 이번 사이트 리빌드 과정에서 Chat GPT를 CSS, 아이콘 추천, Cloudflare 설정, .NET Core 특이점 등 다양한 개발 문제 해결에 적극 활용함
     * AI의 빠른 제안과 코드 자동화로 생산성 대폭 향상 경험
     * 신속한 마이그레이션, 작업 자동화 등에 있어 높은 정확도와 유용성 확인

개발 여정과 결론

     * 법률 문서 갱신 등 보이지 않는 다양한 작업들이 오랜 시간, 비용을 소요함
     * 론칭 전후로 수 차례 긴급 수정과 반복 릴리즈로 빠른 문제 해결 진행
     * 초심을 잃지 않고 서비스 전문성, 확장성, 쾌적성을 지키며 재출발을 완료함
     * HIBP는 2013년부터 4분의 1 인생을 쏟은 열정의 결과물이며, 이번 2.0으로 커뮤니티 서비스로서 새 도약 기대

        Hacker News 의견

     * 법률 회사와 파트너십을 맺어서, 과실로 인한 모든 데이터 유출(사실상 거의 모든 경우)에 대해 집단 소송 추진 희망사항 공유 후, 결제 은행 서비스와 연계해서 합의금 지급 시 수백만 명에게 직접 송금하고, 그렇게 되면 현대판 영웅이 될 수 있을 거라는 상상 공유 실제로 과실이 있는 기업에 뼈아픈 판결을 내리게 만들만한 변호사들과 협력하는 중요성 강조, 단순한 소규모 합의금은 무책임한 경영을 지속하게 만들 위험성 경고 선택적으로, 소송 임박 데이터를 투자사에 판매 가능성 언급, 궁극적으로는 데이터 유출 뉴스만으로도 해당 기업 주가가 타격받는 당연한 사회 분위기 조성 바람
          + 합의금이 발생할 때마다 소액을 바로 좋은 곳에 기부할 수 있으면, 집단 소송에 참여할 의욕이 더 생길 것 같다는 바람 전달
          + 해당 은행 서비스에 대해, 그런 시스템에 보관된 데이터 자체가 또 유출되는 건 얼마나 오래 걸릴지 불안함 농담
          + 이런 시스템은 오히려 역효과 가능성 우려 기업이 유출 사실을 공개하는 것 자체가 이미 어려운 상황에서, 이런 구조는 리스크만 키워 공개 회피만 늘릴 것이라 걱정 내 정보가 유출되었는지 알고 비번을 바꾸는 게 낫다는 의견
          + 내 개인 정보가 Google에 팔린 것 때문에 Blue Shield에서 보상받을 날을 여전히 기다리고 있다는 불만과 함께 서비스 의향 의사 표현
     * 불과 최근 10년 사이에 LinkedIn처럼 거대한 사이트가 솔트를 적용하지 않은 비밀번호를 저장했다는 사실이 놀라움, 현대에 어떻게 이런 실수를 할 수 있냐는 의문
          + 의도치 않게 이런 일이 생각보다 쉽게 발생할 수 있다는 사실 설명 중간 미들웨어 입장에서 보면, JSON 데이터 내 password 필드는 단지 또 하나의 필드로 인식될 수 있고, API나 로깅 시스템이 요청 본문 전체를 로그로 남기게 되면 실제 문제 발생 가능성 솔트 없는 비밀번호를 비밀번호 저장소에 직접 저장하는 일은 드물겠지만, 예를 들어 안드로이드 앱의 API 게이트웨이에서 ‘비밀번호 찾기’ 같은 플로우가 민감 정보임을 누락하면 비슷한 문제가 생긴다 경험 공유
          + 이런 실수가 생기는 건 엔지니어링 면접에서 Leetcode Hard 문제를 충분히 안 냈기 때문이라는 농담성 의견
          + AI Slop(인공지능 기반 품질 저하) 얘기는 많이 하는데, 실제로 오랜 기간 Outsourced Slop(외주 개발의 품질 저하) 문제도 심각하게 존재했다는 지적 LinkedIn도 외주 프로그래머 산출물이 주요 원인일 가능성 높다는 경험 기반 지적 강력하고 유능한 관리자가 품질 기준을 세우고 검증해야지만 외관상 멀쩡하고 속은 허술한 제품을 피할 수 있다는 주장
          + 이런 실수가 일어나는 건 과거에 구축한 오래된 레거시 메인프레임 등, 아무도 관리나 마이그레이션에 시간·예산을 투자하지 못해 방치된 시스템 때문일 가능성 대기업일수록 중요 시스템의 관성(ossification)이 너무 강해져 1시간 정도만 중단돼도 수백만 원 이상의 ‘손실’로 여겨 정비가 더욱 불가능해지는 구조 문제점 지적
     * 많은 일반인이 Have I Been Pwned를 자주 사용하고 1Password로 유입되는 것도 최고의 선택지라고 생각 실제로 1Password와의 프로모션은 훌륭하게 어울리는 협업 언급 해당 배너 문구를 ""강력히 추천""처럼 눈에 더 띄게 바꿨으면 좋겠다는 제언 공유 소셜 계정 해킹 피해의 대부분이 비밀번호 재사용 때문이고, 이런 경험에서 안전한 비밀번호 사용 교육과 패스워드 매니저 유입이 매우 긍정적이라고 강조 지난 1년간 약 20건 넘게 실제로 문제 해결 지원했던 경험 공유하며 리뉴얼 축하
     * 모든 데이터 유출 내역을 로고와 소개 문구와 함께 세로 스크롤 형태로 보여줘서 무섭지만 놀라운 기능 감상
          + 데이터 유출을 볼 때 무력함을 느끼고, 신용동결 외에는 할 수 있는 조치가 거의 없다는 자조
     * 최고의 데이터 유출 기록 보유자는 누구일지 궁금 내 메인 이메일은 지금까지 40건의 유출에 등장했고, 제일 오래된 건 2011년 6월(HackForums, 기억도 없음), 최근은 2024년 9월(FrenchCitizens, 프랑스와 관련 없음)이라는 경험 공유
          + 한 명이 그 기록을 1개 차이로 넘었다고 답변
          + john@yahoo.com 이메일이 무려 322건의 유출에 올랐다는 놀라운 기록 공유
     * 좀 더 프라이버시를 원한다면, 해당 서비스 내에서 내 이메일 검색 결과를 숨기는 opt-out 기능이 있다는 팁 제공
     * 여러 개의 이메일 별칭을 쓰기 때문에 하나하나 검색이 불가능해, 도메인으로 한번에 검색하는 기능이 있으면 좋겠다는 바람
          + ""The Domain Search Feature"" 안내 아래에서 도메인 소유권 인증 후 한 번에 결과를 볼 수 있다는 방법 안내
     * 정말 멋진 사이트라는 감상과 함께, 이런 문제를 정부가 더 진지하게 받아들이길 바라는 소망 신분 도용, 계정 탈취 등은 결국 데이터 유출에서 시작되고, 오늘날에는 실제 집에 도둑이 드는 것보다 디지털 계정이 털리는 게 더 심각한 재난임을 강조 물리적 침입의 경우 911 등 신고와 추적 등이 분명히 존재하지만, 디지털 침해는 연락처도 없고, 해결과정도 별로 존재하지 않는다면서 사회적 대응이 바뀌길 촉구
     * 리뉴얼 디자인을 매우 긍정적으로 평가하며, Troy의 업데이트를 따라가는 것도 즐거움이지만 때로는 블랙 유머처럼 느껴지는 흥미로움 언급 타임라인이 가장 오래된 유출부터 최근으로 정렬된 것 같지만, 날짜 표기가 데이터가 유출된 시점이 아니라 유출 사실이 공개된 시점이라는 혼란 경험 해결책으로 정렬·표기 모두 ‘공개일 기준’으로, 카드 내에서 실제 유출 날짜를 기준 형식으로 표기하는 것이 더 명확할 것이라는 제언
"
"https://news.hada.io/topic?id=21001","내 언어를 추측하지 마세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             내 언어를 추측하지 마세요

     * IP 기반 언어 설정은 잘못된 전제에 근거한 기능임
     * IP 주소는 위치만 알려줄 뿐, 사용자의 실제 언어 선호를 반영하지 못함
     * Accept-Language 헤더가 언어 선호 정보를 정확히 전달함
     * UI 언어는 신뢰할 수 있는 신호인 브라우저 헤더를 기반으로 설정 필요
     * 사용자의 언어 선택권과 경험 존중이 가장 중요한 원칙임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

IP로 추정하는 언어 설정의 한계

     * 여전히 IP 지리정보(GeoIP) 로 사용 언어를 결정하는 행위는 잘못된 가정임
     * IP 주소는 단지 요청 위치 정보만 제공함
     * 사용자가 실제로 선호하거나 이해하는 언어를 파악하지 못함
     * VPN, 해외여행, 해외 거주, 복수 공식 언어 국가 등 다양한 상황에서 자주 실패 발생
     * 이런 방식은 똑똑함이 아닌 불편과 혼란 초래임

공식 언어 개수와 현실의 괴리

     * 국가와 IP 정보는 언어와 1:1 매핑이 아님
          + Belgium은 3개, Switzerland는 4개, India는 22개, Canada는 공식적으로 2개, 사실상 다국어 국가임
     * 사용자는 해당 국가에 거주, 방문, 혹은 우연히 트래픽이 해당 국가를 통해 경유되는 경우가 있음
     * 단순 국가 정보만으로 한 가지 UI 언어 강제 노출은 잘못된 행동임

잘못된 엔지니어링의 예

     * 잘못된 데이터에 의존하는 것은 게으른 개발 방식임
     * “대형 웹사이트도 이렇게 한다”는 변명은 올바르지 않음
     * 정확하게 처리하지 못할 바에야 차라리 아무 것도 하지 않는 편이 나음

Accept-Language 헤더의 가치

     * 많은 사용자가 VPN 사용 등으로 새로운 언어가 강제 노출되어 불편 경험
     * 모든 브라우저는 Accept-Language 헤더를 전송함
          + 사용자의 OS 또는 브라우저 설정에 따라 언어 선호가 지정됨
          + 사용자가 원하는 경우 직접 이 값을 설정, 변경할 수 있음
          + 예시: Accept-Language: en-US,en;q=0.9,de;q=0.8
     * 이 헤더는 정확, 무료, 이미 내장, 라이선스나 추가 유지보수가 필요 없는 정보임

올바른 언어 처리 방법

     * 화면 해상도나 색상처럼, 언어도 사용자의 환경을 존중해야 함
     * 영어 사용자에게 프랑스에서 영어로, 브뤼셀의 프랑스 사용자는 네덜란드어로, 홍콩 VPN 사용자에게는 중국어로 노출 등과 같이 엉뚱한 언어 지정 빈번히 발생
     * 사용자 불만, 서비스 이탈, UI 탐색 끝에 임시 해결 시도 등 문제가 생김
     * 이런 실수는 신뢰할 수 없는 IP 데이터를 언어 판단 근거로 삼았기 때문임

실질적 해결 방안

     * Accept-Language 읽기
     * 이 값 존중하기
     * 원할 경우 사용자가 직접 언어를 변경 가능하게 지원 (선택 사항은 쿠키나 URL 파라미터에 저장)
     * GeoIP 정보는 통화, 배송, 법적 요소에만 사용하고 언어에는 적용 금지

결론

     * 사람이 사용하는 소프트웨어라면, 사용자 선호를 추측해서는 안 됨
     * 정확하게 구현하거나, 아예 언어 지정 기능을 제공하지 않는 것이 바람직함

        Hacker News 의견

     * Accept-Language 헤더를 무시하는 웹사이트는 정말 거슬리는 문제라고 생각함. 다국어 웹사이트들은 종종 언어 전환 버튼이나 언어 목록마저도 현재 언어로 번역해버려서 혼란을 준다는 점, 이 경우 위키피디아가 표준적 모범이라고 생각함. 위키피디아는 명확한 다국어 아이콘, 각 언어의 이름을 그 언어로 보여 주는 목록 그리고 최상단에는 추천 언어를 표시하는 방식 사용. 언어를 미리 정해 추측하는 일은 하지 않아야 함. 위키피디아처럼 하면 도움됨
          + 검색 엔진을 통해 원하는 페이지에 들어갔을 때, 갑자기 나라를 선택하라는 모달 팝업이 뜨고, 그걸 선택하면 지역 사이트의 홈페이지로 강제 이동시켜버리는 경험을 싫어함. 일부 사이트는 닫기 버튼(X)이 있지만, 없는 경우도 많아 짜증남
          + 위키피디아의 언어 목록이 반드시 ""알파벳순""은 아닌 듯 보임. 예를 들어 추천 언어로 ""中文"", ""Italiano""가 올 때도 있고, 나머지는 지리적, 스크립트 기준으로 묶여있는 것처럼 보임. 자기 언어의 진정한 알파벳 순서는 아니더라도 위키피디아는 현지화는 잘함. 중요한 점은 언어 이름을 각 언어로 나타낼 때 ""정확한"" 알파벳순이란 개념이 복잡하다는 점
          + 위키피디아는 사용자가 이전에 선택한 언어를 기억해서 그걸 맨 위에 보여줌. 진정한 유저 경험 제공
          + YouTube 앱에서 언어가 갑자기 Amharic으로 바뀐 경험 있음. Google 지원문서를 참고하면 English 버튼을 알려주지만, Amharic를 모르는 나에겐 아무 의미도 없는 안내였음. iPhone 번역 앱도 이 언어를 인식하지 못함. 만약 보편적인 다국어 아이콘만 있었다면 이런 상황에서 쉽게 해결됐을 것임
          + 아이러니하게도, Universal Language Selector(U.S.L)도 결국 사용자의 IP 기반 국가 정보를 주로 사용해서 추천 언어를 정함. 이 방식은 정확하지 않은 경우가 많음. U.S.L FAQ에 의하면, IP로 출발 국가를 결정한 뒤 주요 언어 후보를 추천한다고 함
     * 모두가 단일 언어만 쓴다고 가정하는 디자인을 싫어함. 나는 네 개의 언어로 충분히 읽을 수 있고, 보통은 번역보다는 원어를 선호함. IP 기반 언어 예측은 아주 자주 틀림. 내가 이미 지원 언어를 설정했으니 그걸 존중해 주는 게 맞음. YouTube는 자동 AI 더빙이 기본 적용되어 반만 듣고 다시 되돌리며 끄는 과정 반복. 시간은 많이 안 들지만 정말 짜증나는 경험
          + YouTube AI 더빙은 정말 최악의 품질임. 5분 만에 영상의 음성이 본문 오디오라는 걸 간신히 깨달음. 영상 제목이 번역되는 것도 싫음. 영어로 말하는 영상을 보는데 제목은 엉뚱한 언어로 되어 있음
          + 구글에 스페인어 사용 언어로 설정해도 YouTube는 모든 걸 영어로 자동 번역함. 스페인어 실력 늘리려고 원어 콘텐츠를 보고 싶은데 YouTube 때문에 어렵게 느껴짐
     * YouTube에서 내 설정 언어로 영상 제목을 자동 번역하는 기능이 정말 짜증남. 영어를 이해하는데 굳이 번역이 필요 없다는 생각
          + 내가 글을 이해 못하는 언어라면 그 영상은 원래 나를 위한 게 아닐 가능성 높음. 대부분의 국제용 영상은 영어로 제공되니까, 원래 저자라면 아마 번역해두었을 것임. 그리고 영상 제목이라는 것은 맥락이 부족해서 제대로 번역도 안 됨. 예전에 포르투갈어 ""Vamos assistir uma conexão com o passado"" 영상이 사실은 ""Let's play A Link to the Past""였는데, 영어 제목을 알아내려면 다시 번역해서 유추해야 했음. 이런 기능은 접근성을 높일 순 있어도, 적어도 끄는 기능은 넣어줘야 한다고 생각
          + YouTube 알고리즘이 이런 기능 사용을 독려하고, 콘텐츠 접근성을 이유로 보상할 수도 있지만, 못생긴 기계 번역 노르웨이어 제목 대신 영어 제목이 더 좋음. 내 경험상, 이런 기계 번역 제목이 나오면 해당 채널을 추천받지 않도록 설정함
          + 내 컴퓨터는 독일인이지만 영어로 설정함. 그럼에도 YouTube가 기계 번역 음성을 들려주거나, 독일어 영상/광고를 영어로 이상하게 번역해줌. 진짜 이상하고 어색한 경우 많음
          + 제목뿐만 아니라 오디오 트랙도 번역되는 게 문제. 자주 보는 유튜버가 여러 언어로 팬-made 오디오 트랙을 제공할 때, 나는 항상 직접 원어로 돌려놓아야 함. 기계 번역으로 인해 언어 유희나 문화적 함의가 빠지는 경우 많음. 간혹 언어 트랙을 바꾸려면 영상 처음부터 되돌려야 하고 브라우저 확장과의 충돌도 문제. 이런 번역을 기본값으로 설정하지 않고 자유롭게 토글하는 인터페이스가 있으면 좋겠음
          + 이제 Google, Kagi 등에서도 Reddit 검색 결과가 번역되어 나옴. 내가 찾던 답이 내 언어로 달린 줄 알았는데, 사실은 기계 번역된 영어 글이었음
     * 무엇보다도, 어떤 경우에도 자동 번역을 기본값으로 적용하는 일은 피해야 함. 실제로 번역해놓은 언어 목록만 명확하게 보여주면 됨. 번역 버튼은 브라우저에 이미 있으니 사용자가 스스로 적용하면 됨. 영어가 나의 첫 언어나 두 번째 언어가 아니지만 충분히 읽을 수 있음. 원본 영어 찾으려고 자동번역 더미 속에서 버튼 찾는 수고는 피하고 싶음. 다른 로케일은 이런 경우가 드문데, 오히려 영어에만 이런 문제가 많음. 기계 번역을 쓰고 싶다면 실제로 한 번 돌려보며 의도와 결과가 맞는지 확인하는 식의 검증이 필요하다고 생각
          + Reddit이 프랑스어로 자동 번역된 내용을 보여주는데, Reddit만의 스타일은 진짜 사람 손으로 옮기는 것도 거의 불가능한 수준임. 영어를 전혀 몰라도 이런 자동 번역본은 읽을 가치 없음. 원문 보기나 언어 변경 기능도 기기나 앱에 따라 찾기 어렵고, 그 밖에도 Reddit은 자꾸 앱을 설치하도록 유도함
          + 기계 번역은 이제 몇 년 전부터 상당히 좋아져서 네이티브조차 번역인지 모를 수준으로 발전했음
     * 내가 가장 불편하다고 느끼는 건 Google임. 내 신원, 여행 여부, 선호 언어(영어)까지 구글이 다 알고 있는데, 여전히 일부 페이지에서는 지역 기반 언어로 보여줌. 구글이 내 정보를 추적하는 데 활용하듯이, 사용자 경험 향상에 더 활용해주길 바람
          + 카탈루냐어 사용자지만 스페인어가 구글 계정이나 시스템/브라우저 언어에도 등록된 적이 없는데도, 항상 스페인어가 강제로 표시됨. 미국에 살고 있는데도 카탈루냐 관련 검색에 스페인어 위키를 우선 노출함. 검색엔진의 이런 행동은 늘 별로였음. 내 아이들처럼 스페인어 못하는 사람이나 스페인 아닌 카탈루냐어 지역 모두에 문제. 특히 Google Gemini는 카탈루냐어 콘텐츠를 위험하다고 여기며 대화 차단함. 구글은 예전에는 다양성에 정말 민감했지만, 지금은 그렇지 않음
          + 최근 Google은 map 리뷰 등에서 현지 언어를 영어로 번역하지 않고 그냥 보여줌. 내가 오래 머무른 곳이니까 그 나라 언어를 알겠거니 하는 것 같은데, 진짜 모를 때 영어가 필요한 경우가 많음. 검색 결과도 마찬가지. 지역 언어를 쓰는 것은 단순히 지역 사정을 알아보려는 것임에도 불구하고, Google은 이런 사용자 패턴을 제대로 반영하지 못하고 있음
          + Google 개발자와 직접 이 논쟁을 해봤음. 그는 구글이 이중 렌더링 과정을 통해 메인 페이지에선 아직 사용자 정보를 활용할 수 없어 언어 선호를 반영하지 않는다고 했지만, Accept-Language 헤더는 이미 초기 요청에 포함되니 말이 안 된다고 생각
          + Google은 내가 영국에 살고 영어를 쓰는 걸 알고 있음. 그런데 스페인 호텔 TV에 로그인해서 영어 유튜브 영상을 보면 스페인어 광고를 보여줌. 내가 이해 못하는 언어임을 Google이 알고 있으면서 광고주를 위해 굳이 이러는 것처럼 느껴짐
          + 새로운 기기나 브라우저만 쓰면 Google과 그 서비스들이 히브리어로 시작됨. 계속 영어로 바꿔왔지만 매번 그래야 하고, 나는 읽을 순 있지만 범용 언어인 영어가 더 좋음
     * 모든 브라우저가 Accept-Language 헤더를 보내 사용자 선호 언어를 알릴 수 있으며, 사용자가 관심 있으면 직접 수정하기도 함. 하지만 Accept-Language로 모든 게 해결되진 않음. 왜냐하면 다중 언어 사용자 대부분이 절대적 선호 순위가 아니라 주제별 선호가 있기 때문임. 예를 들어 프랑스어에 능통하더라도 영문 뉴스의 프랑스어 번역본을 원하진 않음. 사이트 운영자는 억지로 복잡한 기능을 만들지 말고, 명확하고 빠른 언어 변경 인터페이스만 제공하면 충분함. 위키피디아는 이걸 잘하고 있음
          + 실제로 Accept-Language 등의 기능보다 중요한 건 반쪽짜리 자동 번역이 아니라, 완성도 높은 번역을 제공하는 것임. UI와 자체 콘텐츠는 공식 번역을 유지하는 게 좋은데, 사용자 콘텐츠는 자동 번역 대신 반드시 브라우저 번역 버튼 등으로 선택적으로 노출해야 함. Accept-Language로 우선 언어를 고르고, 명확한 언어 전환 옵션도 함께 제공하는 방식을 추천함. 위키피디아의 각 언어 버전은 별개의 사이트에 콘텐츠가 별도로 재작성되는 구조임
          + Accept-Language는 언어별로 가중치(q value)를 지정할 수도 있음. 웹사이트는 사용자가 브라우저 요청 언어를 재정의할 수 있게 해야 하고, 브라우저 역시 사이트별로 ""사이트 기본값"" ""시스템 언어"" ""영어 요청"" 등 선택지를 제공해야 함. 가장 이상적인 건 웹사이트가 지원하는 언어 목록을 명확히 manifest.json 등에 공개하고, 클라이언트 측에서 언어 선택을 처리하게 하는 방식
          + 사용자가 현실적으로 가장 편한 언어로 Accept-Language를 설정한다는 전제는 약함. 대부분 사이트가 Accept-Language를 제대로 지원하지 않아서, 사용자가 해당 설정을 할 때 큰 전략을 두고 할 것 같진 않음
     * 정부 관련 사이트에서 다국어·접근성 지원을 위해 Accept-Language로 기본 언어 선택 시스템을 구현함. 그런데 PM이 이 결정을 무시하고 EN(영어)만 기본값으로 고정했음. 접근성 감사관이 Accept-Language를 써야 한다고 요구했고, 구현된 걸 다시 되돌린 걸 알게 되자 강하게 지적함. 결국 이런 어이없는 일들 덕분에 몇 주 더 계약 일을 하게 됨
     * 내 언어 불만도 얘기해보고 싶음. Apple TV에서 일본어만 하는 가족이 있음. 내 Apple 계정 국가는 Finland. The Martian 영화를 가족과 보려는데, 계정 국가에서 지원되는 언어만 오디오 트랙에 보여주기 때문에 일본어 더빙 선택이 불가능. 계정 국가 변경은 활성화된 Apple TV 구독이 있으면 할 수 없음. 결국 가족이 함께 볼 수 없었음
          + 아마 지역별 라이선스 문제 때문일 것이라 추측함. 해당 언어 더빙이 Finland에서 사용하도록 라이선스되어 있지 않기 때문
          + Netflix도 비슷한 일이 있었음. Finland에선 계정 언어 설정을 영어로 해야 Kim's Convenience를 볼 수 있었음. 영어 자막을 쓰는 경우도, 핀란드어 UI에서는 해당 콘텐츠 접근 자체가 불가능
     * Accept-Language에서 사용자의 손쉬운 접근성을 위해, 가장 가까운 번역본을 제공하는 시스템을 직접 개발했던 경험 있음. 예를 들어 포르투갈어(Portugal)가 없는 경우 브라질 포르투갈어(브라질 변형)를 자동 제공. 기술적으로는 잘 동작했고 구현도 즐거웠지만, 실제로는 대부분 사용자가 Accept-Language와 무관하게 영어만 원했고, 언어 변경 옵션이 푸터에 있었지만 충분히 눈에 잘 띄지 않았음. 그래서 지금은 직접 언어를 묻는 식으로 바꿨고, 유저 만족도가 크게 올랐음
          + 직접 묻는다는 건, 쿠키 저장이 안 된 상태에서 사이트에 들어올 때마다 언어 선택 팝업이 무조건 등장한다는 것임? 이런 팝업은 일관성이 없고, X 버튼 하나만 있어도 감사할 지경임. 100번 중 95번은 내가 클릭해서 들어온 언어면 충분한데, 필요하면 상단 오른쪽이나 페이지 푸터에 언어 선택만 제공해줘도 편함. 팝업이나 모달은 없었으면 좋겠음
          + Accept-Language를 기본값으로 하고 추가로 사용자에게 선택권을 주는 게 좋음. 언어 전환 디자인에서 아이콘을 없애는 걸 찬성하지만, 여러 지역을 하나의 언어로 묶는 건 아쉬움
          + 대부분 사용자는 OS 기본 언어 그대로 두고 쓰면서, 어떻게 바꾸는 줄도 모르고 익숙해져 있음
     * 해외에서 이런 문제는 정말 어이없고, 심지어 본국에서도 프로그래밍 문서 같은 로컬라이즈된 버전은 내용이 다르고 번역본의 품질이 떨어짐. 이상적으로는 원문 영어를 보고 싶어 함. 최근에는 검색 결과에도 기계 번역물이 점점 더 많이 보이기 시작. Reddit도 최근에 이 방식 도입. 내가 일부러 영어가 아닌 검색어를 쓰는 건 그 언어 고유의 정보를 찾고 싶어서임
          + Reddit팀이 자국 모국어 사용자만 가정한 채 기능을 만든 듯 보임. 전세계는 여러 언어를 이해할 수 있으니 굳이 그렇게 할 필요 없음
          + MongoDB 문서에서도 언어 기반 자동 전환 기능 때문에 분노한 적 있음. 검색 결과에서는 영어로 나오는데, 페이지 열자마자 JS가 브라질 포르투갈어로 바꿔줌. 그런데 완전히 로드되면 다시 영어로 돌아가는데, URL은 여전히 /pt-br/임. 게다가 번역 품질은 자동화 티가 확연함. 나는 Portugal 포르투갈어 사용자라 미묘한 차이도 더 거슬림
"
"https://news.hada.io/topic?id=20942","Show GN: 전국 축제/행사 AI 검색 서비스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Show GN: 전국 축제/행사 AI 검색 서비스

   안녕하세요!
   LLM 응용 개발, RAG 서비스 개발에 관심을 가지게 되면서 혼자 개발해본 서비스를 공유드리려고 합니다.

   내 주변의 관광지를 내 취향에 맞게 AI에게 추천받고자 하는 취지로 시작했지만
   데이터나 비용의 한계로 지역 축제/행사 정보를 단순 검색할 수 있는 서비스를 만들어 봤습니다.

   로그인을 통해 개인화 기반 추천, 컨텐츠 제공 기능을 준비하고 있습니다.

   웹과 앱 플랫폼을 런칭하고자 플러터로 구현했으며,
   RAG는 Neo4j의 벡터 검색 + LLM 쿼리 생성 검색을 기반으로 합니다.

   축제/행사의 기본 데이터는 한국관광공사의 TourAPI로부터 제공받으며,
   AI가 답변 생성 시 참고하는 문서는 웹 검색(실시간은 아닙니다)을 기반으로 합니다.

   사용성이나, RAG 기능 등 여러 피드백 주시면 감사하겠습니다!

기능

     * 전국에서 개최 중인 축제/행사 정보 검색
     * AI 지도 탐색 기능을 사용하여 지도에서 축제/행사 탐색
     * AI 대화 기능을 사용하여 전반적인 축제/행사 정보에 대해 문의

서비스 링크

     * 웹페이지 링크: https://travelgen.kr
     * iOS 앱: https://apps.apple.com/kr/app/…

   ai 대화 기능이 유용하네요!

   감사합니다!

   말씀해주신 llm 쿼리가 어떤 기능을 제공하는지 알려주실 수 있나요?

   사이트를 들어가 보았지만, 평이한 지도 검색의 느낌을 받았습니다. 관심 있는 분야라서 가능하면 어떻게 기술이 효용을 제공하는지를 알고 싶습니다.

   text2cypher로 GraphRAG의 장점을 손쉽게 활용하려 했으나(노드 간 다양한 관계 탐색) 제 구현에서는 llm 생성의 일관성 문제가 있고 스키마가 단순해서 아직 크게 기능적 장점을 제공하지 못한 것 같습니다. 단순 텍스트 벡터 검색이 더 결과가 좋은 경우도 많았습니다.

   다음 질의에 대해 보다 정확히 처리 가능하도록 구현하고 있습니다.
    1. 다양한 필터링을 요구하는 사용자 질의 (특정 기간 내, 특정 주제, 특정 장소 등 여러 조건을 동시에 만족하는 정보를 요구하는 경우)
    2. 문서의 텍스트에는 없지만(벡터 검색은 불가능하지만) 자체적으로 생성한 노드(예: 행사 주제)에 대한 검색
    3. 복잡한 관계를 갖는 정보 검색

   이러한 기능들은 llm이 스키마를 기반으로 db 쿼리를 자동 생성하는 유연함 덕분일것 같습니다.

   굉장히 좋네요 ㅋㅋㅋㅋ

   감사합니다!

   RAG에 사용한 리소스는 무엇인가요?

   공공 API에서 제공하는 설명 정보와 공식 사이트의 웹 문서를 기반으로 하고 있습니다.

   와 이거 좋은데요?

   의견 감사합니다!

   좋네요

   감사합니다!

   이 서비스 정부에서 굉장히 좋아할 것 같습니다. 특히 지방자치단체에서 탐낼 거 같아요...!

   좋은 말씀 감사합니다!

   너무 좋은데요?

   좋게 봐주셔서 감사합니다!

   점점 발전하면 진짜 유용할 것 같슴돠

   감사합니다~!
"
"https://news.hada.io/topic?id=20985","프랑스, 최초로 UN 오픈소스 원칙 공식 지지한 정부","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     프랑스, 최초로 UN 오픈소스 원칙 공식 지지한 정부

     * 프랑스 정부가 UN 오픈소스 원칙을 세계 최초로 공식 지지함
     * 이 결정은 프랑스 디지털 관련 부처를 중심으로 발표됨
     * 실제 조치로 이어질지, 혹은 단순 의향 선언에 그치는지에 대해 의문 제기됨
     * 한편, 프랑스 교육부는 여전히 Microsoft 솔루션 도입 계약을 체결함
     * 관련 정책의 실질적 실행 여부와 구속력 논의가 진행 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프랑스 정부의 UN 오픈소스 원칙 공식 지지

     * 프랑스는 세계 최초로 UN 오픈소스 원칙을 공식적으로 지지하는 정부가 됨
     * 디지털 전환 및 오픈소스 소프트웨어의 공공부문 확산을 위해 여러 정부 기관 및 관계 부처가 관련 내용을 사회관계망 서비스에 공유함

실제 실행과 선언 사이의 논의

     * 정책적 선언에 그치지 않고, 실제로 법적 구속력이 있는 규정이 만들어질지에 대한 논쟁이 발생함
     * 오픈소스 활성화를 위한 정부의 공식적 동의가 행정적, 법적 실행으로 이어질지에 관해 관심이 높아짐

교육부의 Microsoft 솔루션 도입

     * 2025년 3월 14일 발표된 공식 입장에 따르면, 프랑스 교육부와 고등교육·연구 부처는 중앙 및 대학기관을 대상으로 Microsoft 솔루션을 도입하기 위한 계약을 다시 체결함
     * 이는 오픈소스 정책 의지와 상반되는 행보로, 실제 현장에서 오픈소스 소프트웨어 채택이 제한적임을 시사함

향후 정책 방향 및 과제

     * 오픈소스 원칙 공식 지지 이후, 이를 실제로 공공부문에 적용하기 위한 추가적 조치와 정책 보완이 요구됨
     * 구속력 있는 법적 장치 마련과 현장 적용 사례 확대가 앞으로의 주요 과제임

관련 문의와 참고 링크

     * 프랑스 국민의회 공식 질의문: 여기서 확인하기
     * 일부 시민과 관계자는 단순 선언에 그치는 것이 아닌, 실질적 실행력 확보의 필요성을 강조함

        Hacker News 의견

     * 이건 보여주기식 행동이고, 실상은 다르다는 생각임. 프랑스 정부가 오픈 소스를 지지한다고 밝힌 건 이번이 처음이 아님. 실제로는 대부분의 공적 자금이 독점 소프트웨어에 쓰이고, 오픈 소스는 예외임. 두 달 전에 프랑스 정부가 Microsoft와 'Éducation Nationale' 부서용 ‘open bar’ 계약을 체결했음. 1억 5200만 유로인데 오픈 소스는 아님. 며칠 후 주요 국영 기관(Polytechnique)에서 MS Office 365로 이전 발표도 함(이메일 시스템 포함). 몇몇 법과 공식 명령을 위반하는 행위임(반군사 학교임에도 불구하고)
          + 이 상황은 좀 더 복잡한 측면이 있다는 생각임. 한 번 오픈 소스를 선택한 서비스를 사람들이 크게 이슈 삼지 않는 경향이 있음. CNLL이 Polytechnique를 비난할 수 있는 이유는 명확한 지침이 있어서임. 그런 지침조차 없다면 훨씬 더 나쁜 상황이라는 생각임. 또 ‘대부분의 공금’과 관련해선, 오픈 소스 계약이 대부분 수십억 단위로 이뤄지지 않고 내부 인력 채용에 많은 돈이 들어가고 외주에는 일부만 쓰인다는 점이 있음
          + 지켜봐야 할 일이지만, 예전의 구체적인 실적을 보면 낙관적으로 보기 어려운 상황임
          + 여전히 올바른 방향으로 가는 첫걸음이라는 생각임. 미국과 트럼프의 관세 문제를 계기로 등장한 움직임임. 내 나라의 정부 기관에서도 미국 소프트웨어 대기업에서 벗어나자는 논의가 많아지고 있음. 최근 한 부서가 AWS에서 Hetzner로 옮겨서 예산도 절약함. 유럽 기반 오피스 제품을 만들자는 논의도 들은 적이 있음
     * 나라에서 공공 소프트웨어를 만들 때 오픈 소스가 기본 값이 되어야 한다고 생각함. 신뢰를 구축하는 유일한 방법임. 장기적으로는 오픈 소스와 폐쇄형 정부 소프트웨어가 독재국과 민주국을 구분 짓는 기준이 될 수 있다는 생각임
          + ‘독재국과 민주국의 구분’이라는 말이 처음엔 과장으로 느껴졌지만, 곰곰 생각해보니 오히려 사실이라고 봄. 투표, 인구 조사, 세금, 보고, 규정 준수에 관련한 모든 인프라가 소프트웨어에 의존하고 있으니, 정부의 완전한 투명성이 필수인 상황임
          + 자유롭고 개방된 기술이 자유롭고 개방된 사회의 기반이라는 확신임
     * 더 많은 공적 자금이 오픈 소스에 투입되는 걸 보고 싶다는 생각임. 설령 그게 민간 기업의 클라우드 CI 서비스에 쓰이더라도 큰 도움이 될 거라고 생각함. 많은 프로젝트가 사용 가능한 CI 리소스와 빌드/테스트 설정 수를 균형 있게 맞추기 위해 고생하는 현실임
     * 미국의 오픈 소스 원칙과 비교하면, 미국에서는 기본적으로 것들이 퍼블릭 도메인화된다는 점이 흥미로움
     * 전 세계에 오픈 소스가 더 널리 퍼져서 사람들이 Windows, MacOS, iOS, Android, 데이터베이스 등에서 벗어나는 방책을 보고 싶다는 기대임. 미국 기술 기업들이 이런 비교적 간단히 대체 가능한 제품들로 수십억을 벌어들이고 있다고 생각함
          + 하고 싶다면 직접 실천하라는 조언임
     * 이런 소식이 놀랍지 않음. 2017년에 ETAlab과 교류한 이후로 프랑스 정부가 오픈 소스에 대해 매우 진취적인 생각을 갖고 있다는 인상을 받음. g0v.tw나 vTaiwan 프로젝트 등 최신 시빅 테크 흐름을 누구보다 빨리 추적하고 있었음
     * 중요한 건 이 조치가 단순히 상징적인 것에 그칠지, 아니면 국가가 실제로 조달 정책과 공급업체 요건에 원칙적으로 변화를 주는지임. 예컨대 모든 공급업체가 기본적으로 오픈 된 인터페이스를 제공하거나 오픈 소스 인프라 유지에 자금을 보탠다면 더 인상적일 수 있음. 그렇지 않으면 선언문 수준에서 그칠 가능성이 높다는 생각임
          + 시간이 좀 걸릴 거라 보지만 이미 실질적 적용 사례가 많다는 점을 말하고 싶음. Libre office가 50만 대 이상의 정부 컴퓨터에서 사용되고 있음. 연구자로서 프랑스 박사들과 협업할 때 그들이 Libre 문서와 시트를 보내오는 일이 많았다는 경험이 있음
          + 이걸 공공 소프트웨어를 위한 지침 원칙으로 보는 편임. 예를 들어 시민들이 세금 신고, 신분증 갱신 등에 이용하는 앱이 해당됨
     * 이런 움직임이 LLM(대형 언어 모델)에도 적용되는 건지 궁금함. 적용된다면 오픈 소스를 어떻게 정의할지도 궁금함. 특히 Meta의 ‘Open’이라는 허상을 프랑스 정부가 부인하는 것을 보고 싶음
          + https://www.comparia.beta.gouv.fr/modeles에서 여러 모델을 비교하는데, Llama의 다양한 라이선스를 ‘오픈 소스’로 잘못 표기하지 않음. 참고로 https://opensource.org/ai/endorsements에서 code.gouv.fr가 리스트에 포함됨
          + 딱 그 여덟 가지 지침이 전부임. 별로 구체적이지 않고, 정의보단 의도가 더 중요함. 정책이 아니라 목표 선언임
          + 나는 데이터는 ‘소스’라고 보지 않음. 책이든 사운드트랙이든 영상이든 나에겐 해당 안 됨. 훈련용 코드, 실행용 소프트웨어가 오픈 소스의 핵심이라는 인식임. 훈련된 데이터와 벡터화된 데이터도 자유로워야 하느냐? 그럴 수도 있을 것 같음. 하지만 이 UN 이니셔티브에선 그런 것까지 다루지 않는다고 생각함
     * 프랑스에도 Sovereign Tech Agency 같은 기관 혹은 그에 대한 자금 지원이 언제 생길지 궁금함
     * 프랑스는 이런 분야에서 과소평가된 명성이 있다고 생각함. 프랑스 시민으로서, 행정 업무를 온라인으로 처리하는 게 얼마나 쉬워졌는지, France Connect 같은 도구 하나로 어디서든 로그인 가능한 시스템을 보면 감탄하게 됨
          + 프랑스에서 암호화 기능이 있는 iOS 앱을 유통하는 경우를 예로 들면 큰 불편함이 있었음. 관료 절차가 너무 복잡해서 결국 프랑스 앱스토어에서 앱을 제거하고, 항의가 들어오면 국회의원에게 이 법을 바꿔달라고 요청하라고 말함. ‘실례합니다, monsieur, 그 수학에 라이선스 있으신가요?’라는 농담도 할 수밖에 없음
          + 프랑스에 소프트웨어 개발자를 위한 공공 일자리가 있는지, 아니면 정부 기관 소속(이 역시 공공 분야이긴 하지만 성격이 조금 다름)으로 일하는 시스템인지 궁금함
"
"https://news.hada.io/topic?id=20997","ls-lint - 디렉토리와 파일 이름을 위한 초고속 린터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ls-lint - 디렉토리와 파일 이름을 위한 초고속 린터

     * 파일과 폴더 이름이 사전에 정의한 네이밍 규칙을 따르고 있는지 검사하는 경량형 초고속 린터
          + 수천 개 파일과 디렉토리를 밀리초 단위로 검사하는 성능으로 대형 프로젝트에도 적용 가능
     * .ls-lint.yml 파일 하나로 규칙을 설정할 수 있으며, 유니코드 다국어 파일명 및 모든 확장자를 지원함
     * 파일 확장자에 따라 케밥 케이스, 카멜 케이스, 정규표현식 등 다양한 규칙을 설정할 수 있음
     * Go로 작성되어 모든 운영체제(Windows, macOS, Linux)에서 사용할 수 있고, GitHub Action, npm, Docker, Homebrew 등 설치 옵션이 다양
"
"https://news.hada.io/topic?id=21019","next-yak - Next.js용 러스트 기반 Zero-runtime CSS-in-JS","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           next-yak - Next.js용 러스트 기반 Zero-runtime CSS-in-JS

     * Next.js 프로젝트에서 사용할 수 있는 Zero-runtime CSS-in-JS 오픈소스 라이브러리
     * Rust 기반으로 개발되어 빌드 타임에 CSS를 추출하므로 런타임 JS 오버헤드가 없으며 빠른 성능을 자랑
     * 경량 런타임 구조로, CSSOM이 아닌 클래스만 변경해 브라우저 리소스 사용을 최소화
     * 표준 CSS 문법 지원 및 styled-components와 유사한 문법을 활용해 스타일을 작성 가능하며, CSS가 빌드시 자동으로 추출되어 최적화된 정적 CSS로 변환됨
     * React Server Component/Client Component 양쪽 모두에서 완전히 호환되고, Next.js의 내장 CSS Modules 기능과도 연동
     * 타 CSS-in-JS 라이브러리 대비 더 직관적이고 가벼운 API, 높은 성능, postcss 기반 최적화
     * Atomic CSS 프레임워크(Tailwind CSS 등)와 통합 지원
"
"https://news.hada.io/topic?id=20915","SMS 2FA는 안전하지 않을 뿐만 아니라 산골 사람들에게 적대적임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SMS 2FA는 안전하지 않을 뿐만 아니라 산골 사람들에게 적대적임

     * SMS 기반 2단계 인증(2FA) 이 안전하지 않을 뿐만 아니라 산간 지역 주민들에게 큰 불편을 초래함
     * 셀룰러 신호가 약한 산간 지역에서는 SMS로 전송되는 인증 코드 수신이 어려움
     * 단순한 와이파이 통화나 스마트폰 사용만으로는 2FA 문제를 해결할 수 없음
     * TOTP(시간 기반 일회용 비밀번호) 방식이 대안일 수 있지만, 초기 설정 접근이 쉽지 않음
     * 수백만 산간 거주민들이 웹사이트 로그인 과정에서 겪는 불합리한 상황 설명
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제 상황 개요

     * 글쓴이의 친구는 북캐롤라이나 산간 지역에 사는 70대 여성임
     * 컴퓨터를 싫어하지만 커뮤니티 소통을 위해 스마트폰과 Signal 그룹 채팅에 참여함
     * 유선 전화를 오래 사용했고, 이는 보청기와 잘 호환됨
     * 스펙트럼(Spectrum) 이 지역 통신 독점으로, 유선 전화와 케이블 인터넷을 모두 스펙트럼을 통해 사용 중임

셀룰러 서비스와 SMS 2FA 문제 발생

     * 몇 년 전 모바일 서비스를 스펙트럼 모바일로 가입, 이는 Verizon 망을 사용함
     * 집에서는 셀룰러 신호가 거의 없음. 도심에서 차로 20분 거리이고 이웃도 많음
     * 모든 주요 계정(이메일, 은행, 건강보험 등)이 SMS로 2FA 코드를 전송하려고 함
     * SMS 코드가 오지 않음. 집에서는 셀룰러 서비스 부족, 와이파이 콜링 활성화해도 단축번호(5자리)로 오는 보안 SMS는 도착하지 않음
     * 최신 아이폰과 공식 제공 장비 사용, 사용법에도 익숙함

대체 방안 탐색과 한계

     * 일부 ISP 제공 유선 전화는 SMS를 컴퓨터 음성으로 읽어주는 기능이 있으나, 스펙트럼에는 없음
     * 일부 사이트는 TOTP 2FA로 전환 가능하나, 최초 로그인 시점에서 접근이 필요함
     * 해결을 위한 번거로운 절차:
          + 로그인 실패 사이트 목록 만듦
          + 문제 해결을 위해 시내로 나가 친구와 만남
          + 하나씩 TOTP나 다른 방식으로 전환 시도, 일부는 지원하지 않음
          + 고객센터 문의 시도해도 연락 어렵거나 불가능

사실상 실현 불가능한 대안들

     * VOIP로 번호 이동하여 단축번호 SMS 수신 가능성 찾기
     * 수백 달러를 들여 셀 신호 부스터 설치
     * 거주지 이동까지 고려
     * 로그인 하나를 위해 이런 절차가 필요하다는 점의 비합리성 지적

셀 커버리지 맵의 신뢰성 문제

     * 스펙트럼 커버리지 맵상 집과 주변에는 완벽한 서비스로 표시됨
     * 실제로는 집에서 서비스 불가, 100미터만 이동해도 신호 없음

산간 지역에 사는 수많은 사람들의 공통 고충

     * Millenial 세대 친구 역시 “SMS 2FA가 삶의 고통”이라고 표현함
     * 단순히 깊은 계곡이 아닌 곳조차 SMS 2FA로 인한 문제 있음

TOTP 방식의 한계와 어려움

     * TOTP도 완벽하지 않음
          + 별도의 앱 설치 필요
          + 어떤 앱을 써야 할지 선택 과정이 복잡하고 기술적 설명이 많음

정리 및 규모의 문제

     * SMS 2FA가 널리 쓰이는 이유는 직관적 UX와 어느 정도 신뢰성 때문임
     * 그러나 북캐롤라이나 산악지대의 110만 명, 전체 애팔래치아 2500만 명 등 수백만 명이 열악한 환경에 처해 있음
     * 인터넷은 있어도 휴대폰 신호가 매우 좋지 않음
     * 이들 지역 거주민들에게 합리적 대안이나 배려가 부족함

        Hacker News 의견

     * 그녀가 선택할 수 있는 다른 옵션 중 하나는 SMS를 와이파이에서 받을 수 있는 VOIP 제공업체로 본인의 휴대전화 번호를 포팅하는 방법임이 흥미로움, 하지만 일부 회사들은 seCuRiTy 이유로 VOIP 번호에 SMS-OTP 코드를 보내지 않거나 번호가 본인 명의로 등록되어 있기를 요구함을 알게 됨, 이런 제한이 불법이어야 한다고 생각함, 번호는 그냥 번호임, 와이파이 콜링을 켜놓아 친구나 가족으로부터의 SMS는 받지만 2FA 코드는 여전히 수신되지 않음, SMS over IMS가 외부 송신자에게 투명하게 구현된다고 생각했는데, SMS 프로토콜 자체가 너무 허술하게 만들어져서 놀랍지 않다는 생각임
          + SMS 시스템의 작동 원리를 설명할 수 있을 것 같음, 이 시스템은 단순히 메세지를 “블라인드”로 보냄, 수신자가 오프라인이거나 신호가 없으면 해당 캐리어가 3~7일 정도 메시지를 저장함, OTP 시스템은 Vonage, Twilio API 등으로 도달성 검사를 하는데, 이 검사가 완벽하지 않음, 뭔가 이상하면 메시지는 전송되지 않음, 이런 방식은 메시지 비용 절감을 위한 것임, 이미 검증된 번호에도 적용되는 점은 불합리하다고 생각함
          + 유럽 입장에서 이야기하자면, PSD2라는 금융 지침에 따라 이미 KYC가 완료된 번호만 2FA용 SMS를 허용함, 2FA는 결국 “당신이 가진 무언가”를 증명하는 전자 서명의 역할이고, 그 무언가가 신원확인 된 전화번호임, SMS가 모든 인구·지역·단말에서 쉽게 적용될 수 있는 유일한 2FA 방식임을 강조함
          + 똑같은 회사들이 SMS만 2FA로 허용하면서도 VOIP로는 보내지 않는 것이 정말 말도 안 된다고 생각함, 아마도 모든 회사가 SMS 전송을 위해 특정 서비스를 거치는데 그 서비스가 VOIP를 막아서임, 거의 모든 은행이 필수적으로 SMS 2FA를 요구하지만, 다른 곳들은 앱을 지원하는 것이 너무 이상함
          + 2025년이 되어서야 전화번호가 Sybil 문제(한 명이 여러 계정 생성하는 문제)를 그나마 해결할 수 있는 방법임, 실제 KYC 절차 없이도 어느 정도 신원을 확인할 수 있음
          + Wi-Fi 콜링으로 2FA SMS만 받는 용도로 사용한 적이 있음, RedPocket(MVNO)와 T-Mobile 조합으로 아무 문제 없이 사용함, 해당 지역엔 T-Mobile 직접 신호가 없어서 Wi-Fi를 통한 SMS만 가능했음, 요금제도 저렴했음, 단, 예전 휴대폰이 Band 미지원 등으로 겪는 이슈는 있었음
          + 메시지 수신 성격상, 친구·가족은 P2P로 오고 2FA는 기계 대 인물 A2P임, 두 방법의 처리가 엄연히 다름
          + VOIP 제공업체에 번호를 포팅해도 보내는 쪽에선 그 번호가 휴대폰인지 VOIP인지 구분하지 못한다고 생각함, 이런 식으로 포팅 후에도 SMS 2FA를 잘 받고 있음
          + 다양한 은행 서비스를 사용해보면 일부는 Google Voice로도 아무 문제 없이 SMS 토큰을 보내주는데, 또 다른 은행들은 고객센터를 통해서만 Google Voice SMS를 허용하는 등 정책이 랜덤함, 심지어 정규 채널로는 안 보내면서 자동 전화 음성으로는 똑같은 코드를 읽어줌, 보안 정책이 무작위라는 생각임
          + VOIP로 SMS-OTP 코드를 받는 옵션은 결과적으로 나쁜 아이디어임, 짧은 시간 동안만 작동하다가 결국 보안정책이 강화되어 막힐 것임, 이러한 모든 조치는 실제로 사용자의 보안을 위함이 아니라, 끊임없이 밀려드는 스팸, 사기 트래픽을 늦추기 위한 장벽임, 실제 전화번호를 가진 것 자체가 “Proof of Work”로 쓰이고 있고, 현실적으로 대안이 없음
          + SMS 방식 자체가 문제이므로, 이런 논쟁 자체가 무의미함, SMS 사용 자체가 불법이어야 한다는 의견임
     * 마이크로셀/펨토셀만 있으면 집이나 사무실처럼 신호가 약한 곳에서 매우 효과적임, 프로바이더에 연락해서 신호가 약하다고 하면 인터넷 → 셀룰러로 바꿔주는 AP(Access Point)를 무료로 보내줌, 이런 장치는 RJ-45 입력, GPS 안테나가 있어 e911 위치 데이터까지 지원함, 우리 가게도 금속 벽과 계곡에 위치해 예전엔 언덕 위까지 올라가야 통화가 되었지만, 각 통신사에 요청해서 펨토셀을 설치받은 뒤론 누구든지 ISP망으로 자동 전환해서 정상 사용 가능함, MVNO까지 다 지원함, 단 펨토셀을 사용하려면 MVNO 대신 통신사 직영 서비스를 써야 할 수 있음
          + t-Mobile이 더 이상 마이크로셀이란 장비를 지원하지 않는 것 같음, 지원 페이지 참고함
          + 펨토셀도 단점이 있음, 반드시 GPS 신호가 필요해서 산악 지형에서는 작동이 힘듦, 펨토셀을 수년간 써봤는데 종종 원인 불명으로 작동이 멈추고 왜 안 되는지도 알려주지 않음
          + Verizon에서 무료 4G LTE Network Extender를 받았음, 한 가지 문제는 집을 나설 때 연결이 끊기는 현상임, 한 번은 911에 전화하다가 이동 중 신호가 끊겼고, 작동 거리를 벗어나면 다시 연결될 때까지 통화가 중단됨, 이후 Verizon이 위치 정보를 수정하도록 연락해옴
          + 대형 통신사들이 검증되지 않은 ISP에 연결된 셀타워(마이크로셀)를 아무나 운영해도 괜찮아하는 점이 의외임, 원래 브랜드 관리에 철저한 회사들인데 이런 점에선 엄청 관대함
     * 해외 로밍 중에는 집의 안드로이드폰에 SIM 카드를 보관하고 전원에 연결해두며, SMS를 API로 포워딩해주는 앱을 사용함, 모든 SMS를 이메일로 받아볼 수 있음, 몇 년간 무리 없이 이 방법을 사용 중임, 평소에도 컴퓨터로 OTP SMS를 받아 편함, MMS 수신은 안 되지만 불필요하니 상관없음
          + 이 방법을 ""2FA Mule""이라고 부름, 이 방식을 4년 넘게 사용했으며 매우 잘 작동한다고 생각함, 좋은 선택임
          + 듀얼 SIM과 WiFi 콜링이 지원되는 폰이라면 방문국가에서 데이터-only eSIM을 쓰고, 기존 SIM으로 계속 SMS를 받을 수 있음
          + 나도 유사하게 집에 안드로이드폰을 두고 노트북에서 웹 메시징 서비스를 접속해서 SMS를 받았음, 요즘은 SMS가 WiFi 콜링에서도 작동해서 꼭 문제가 되진 않음
          + 안드로이드폰이 3일마다 자동 재부팅되도록 변경될 수 있다 하니, 이 방식이 곧 중단될 수 있음
          + 왜 이 방식이 로밍과 관련된 건지 모르겠음, 유럽과 그 외 여러 곳에서 로밍을 자주 했는데 SMS 수신에 아무 문제 없었음
     * 이 글은 약간 틈새 기사로, SMS 2FA 코드가 바로 셀 서비스 가입과 동시에 온 것처럼 보이지만 실제로는 2FA 등록을 먼저 마치고 바깥으로 나가야 코드가 활성화될 수 있음, TOTP도 사실 그렇게 어렵지 않음, 그냥 앱을 골라드리고 백업코드 출력만 도와드리면 별문제 없이 해결됨
     * Google Fi는 와이파이에서도 단축 번호 포함 모든 2차 인증용 SMS를 받을 수 있음, 심지어 휴대폰이 꺼져있거나 망가져 있어도 웹 브라우저로 모든 디바이스에서 수신 가능함, 이 기능을 아주 선호함, 월 20달러부터 서비스가 가능함, 예전에 산간지역에선 US Cellular와의 제휴로 서비스가 잘 됐는데, 최근엔 T-Mobile 쪽에 어느 정도 인수된 상태로 상황이 변동 중임
          + 미국 외에서 12년간 살았으며, Google Fi 도입 전까지는 SMS에 항상 문제가 있었음, 많은 은행들이 SMS 인증을 고집하는데, VOIP 가상번호는 (1) 일부 은행이 보안상 서비스 거부 (2) 기술적 이유로 SMS 수신 불가 둘 다 문제임, Google Fi는 심지어 휴대폰 서비스 없을 때도 WiFi로 우회 전송해서 잘 됨, 다만 미국 외 체류 1달 이후엔 데이터가 끊기지만, 그저 SMS/음성만 쓸 수 있어도 충분함
          + RCS 및 “messages for web” 사용이 가능한지 궁금함, 예전에 Fi 싱크를 켜야만 셀폰 오프 상태에서도 문자/음성 사용이 됐었는데, 그 경우 RCS 기능이 꺼졌음, 지금도 그런지와 어떤 URL에서 텍스트/음성 사용이 되는지 궁금함
     * 사용자 기대가 과도하다는 의견에 공감함, 예를 들어 라임 스쿠터를 대여할 때 VPN 세팅 오류로 인터넷이 안 돼 반납 완료 처리를 할 수 없었음, GPS상 멈춤을 감지해 추가 요금을 환불받았지만, 만약 폰 배터리가 나갔다면 곤란해질 뻔했음, 이동 중 이런 예상치 못한 경우의 대비가 필요함
          + 독일 DHL 택배 락커 신형을 사용해보면 화면이 없고 앱으로만 작동함, 블루투스와 인터넷 연결이 동시에 필요함, 락커 자체가 인터넷이 있으니 앱에서 불필요한 요구라고 생각함
     * 특정 그룹에 적대적인 요소는 항상 존재함, 2FA도 완벽한 방법이 없고 각 방식마다 다 불편함 SMS 2FA는 보안은 약하지만 제일 널리 쓰이고 복구도 쉬움 TOTP 앱류는 보안이 강하지만 기기 분실이나 변경시 복구가 어려움 Yubikey 같은 하드웨어 토큰은 비용이 들고 역시 복구 문제 존재함 가장 확실한 방법은 연방 정부가 중앙집중형 하드웨어 인증시스템을 운영하는 것이라 생각함(실제로 미국 국방부는 CaC 카드로 운영), 그러나 미국에서 이런 시스템은 프라이버시 논란과 예산 문제로 실현이 매우 어려움, SMS 2FA가 산간지역 등에 적대적이지만, 사실 모든 2FA가 완벽하지 않음
          + 인증 프라이버시는 특정 상황(예: 투표 등)에서는 중요하지만, 은행처럼 명확히 본인임을 입증하는 상황에선 프라이버시 우려가 크게 해당되지 않는다고 생각함
          + Yubikey 등은 복구가 쉽지 않아 보일 수 있지만, 여러 개를 등록해두면 하나를 잃어도 다른 키로 신규 등록해서 해결 가능함
     * Google Voice 앱을 설치하면 몇몇 2FA 서비스는 지원하지만 아닌 곳도 있음, 일부 서비스는 GV 번호를 거부함, GV는 WiFi로 SMS 수신 가능함, 셀 회사에 펨토셀 요청하면 예전엔 저렴했으나 지금은 단종됐고 2500달러까지 호가함, mightytext.net에 가입해서 컴퓨터로 SMS를 받을 수도 있는데, 이건 셀 신호가 없어도 되는지 확실하진 않음, 손가락 대신 노트북 키보드로 SMS 쓰는 게 편해서 사용함
          + USB 모뎀을 컴퓨터에 연결해서 신호가 잡히는 곳에 두고 인터넷으로 접근하는 방법도 가능함, 자신은 이를 반대로 라즈베리파이로 원격 모니터링 용도로 사용 중임, 프로토타입 땐 SMS 파싱도 했었음, 모든 사람에게 적합하진 않지만 HN답게 공유함
          + mightytext.net은 폰에 신호가 없으면 동작하지 않음, 문자 중계는 오직 캐리어만 가능함, 모든 US 캐리어에서 이런 서비스와 연동하기도 어렵고 기술적 제약도 큼, Apple의 위성 SMS 서비스만이 SMS 라우터에 직접 접근해서 중계가 가능함
          + 이 방식의 장점 중 하나는 MFA(다중 인증)로 SMS 접근을 보호할 수 있다는 점임
     * TOTP, HOTP 등은 전화번호 같은 개인식별 데이터 없이 구현 가능함, SMS는 번호가 있어야 하고, 그 번호가 당신의 개인정보와 연결되어 있으면 마케팅이나 데이터 집약에서 훨씬 큰 가치가 있음
          + 대부분 SMS 인증을 요구하는 곳은 이미 이름·주소 등 개인정보를 알고 있는 경우가 많음(예: 금융, 라이선스, 의료기관 등), 그래서 마케팅용 데이터 집합화 논란은 실제로는 별 의미가 없음, TikTok처럼 굳이 번호를 원하면 일회성 번호를 쓰거나 거부함
          + TOTP/HOTP는 “내가 이 금액을 이 상점에 결제하려는 것”처럼 WYSIWYS(보는 대로 서명) 속성을 제공하지 못함, 은행결제 등에서는 직접적인 확인이 필요함, 실제로 EU에선 WYSIWYS가 규정에 의해 요구될 수 있기 때문에, 일시적인 공백을 메우기 위해 은행 전용 앱이 필요해짐, 지금의 표준들(WebAuthN 등)만으론 부족하고, SPC 확장 등 새로운 방법과 HW 인증기가 필요하다고 생각함
     * 나 역시 시골에 살고 있어 가끔 SMS 코드가 안 오는 문제를 겪음, 어느 날은 잘 오고 어느 날은 안 와서 이유를 몰랐는데, 이 글이 그 원인을 명확하게 설명해줌, 평소 스펙트럼 서비스로 와이파이-모바일 둘 다 쓰고 있는데, 신호 세기에 따라 와이파이에 의존해 있다 보니 발생하는 현상이었음
"
"https://news.hada.io/topic?id=20973","큐레이션이 없다면 우리는 어떻게 무언가를 찾을 수 있을까","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    큐레이션이 없다면 우리는 어떻게 무언가를 찾을 수 있을까

     * 소셜 미디어에서는 정보가 흩어져 있어 원하는 내용을 찾기 힘듬
     * 예전에는 전문가 큐레이터와 미디어를 통해 쉽게 새로운 음악과 영화를 접할 수 있었음
     * 알고리듬 의존은 사용자를 특정 취향에 가두고, 새로운 놀라움이나 다양성을 제공하지 못함
     * 정보의 과잉과 취향 버블로 인해 문화 소비가 더 피곤한 일이 되어가는 현상임
     * 해결책으로 직접 정리하고 우연히 발견하는 노력이 강조되나, 정답은 없고 각자의 방식이 생겨남
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소셜 미디어 시대와 정보의 분산

     * 최근 Björk가 새로운 콘서트 필름 Cornucopia를 홍보하고 있음
     * 관련 정보가 소셜 미디어 및 Reddit 등에서 난무하지만, 정확한 정보를 찾기 어려운 혼란이 발생함
     * Reddit에서 누군가가 ""아주 쉽게 설명해달라""는 글을 올렸고, 실제로 정보 출처에 대한 언쟁이 벌어짐
     * 이런 상황에서 옛날 방식의 웹사이트나 정돈된 정보가 도움이 될 것임

알고리듬의 한계와 정보 소비의 불편함

     * 소셜 미디어는 편리해 보여도 정보를 여러 곳에 산재시켜 비효율을 초래함
     * 사용자는 정보를 찾기 위해 노력해야 하고, 결국 알고리듬에 의존함
     * 기술의 발전이 정보 접근성을 넓혔으나, 오히려 인터넷 전체가 혼란의 덩어리처럼 느껴짐
     * 전문가의 큐레이션이 사라지자 결국 개인이 직접 정보를 선별해야 하는 부담이 늘어남

과거 큐레이션 경험과 비교

     * 필자는 어릴 적 지방 소도시에서도 한정된 미디어(라디오, MTV, 음악 잡지)만으로도 다양한 문화를 접할 수 있었음
     * 라디오나 MTV 프로그램, 잡지, TV 영화 비평 프로그램 등을 통해 해외 음악과 독립 영화를 자연스럽게 발견했음
     * 인터넷이 없던 시절에도 적은 노력으로 취향과 트렌드를 따라갈 수 있었음

큐레이션, 알고리듬, 그리고 문화 피로감

     * 소셜 미디어의 부상으로 큐레이션 문화가 쇠퇴함
     * 비평 문화도 약화되어, 남은 몇몇 사이트(Vulture, Pitchfork 등)도 클릭 수와 기사 양에 집착하며 정보 과잉을 악화시킴
     * 알고리듬은 사용자가 이미 접한 콘텐츠만 추천하기 때문에, 새로운 문화적 경험이나 우연성은 떨어짐
     * 정보와 선택지가 넘쳐나면서 많은 사람들이 문화 소비 자체에 피로함과 부담감을 느낌
     * 추천을 받아도 ""볼 게 너무 많아서 못 본다""는 답이 익숙해졌고, 실상은 선택이나 신뢰에 더 큰 장벽이 됨

직접 정리와 개인화 노력

     * 필자는 최근 알고리듬에 의존하지 않고, Obsidian 등에 직접 노트와 리스트로 관심 있는 정보를 관리하려는 시도를 하고 있음
     * 이런 방법도 모든 것을 따라가는 게 일처럼 느껴지는 한계가 있지만, 별다른 대안이 없어 각자가 새로운 방식을 찾아야 하는 상황임
     * 알고리듬의 안락함을 중시하는 사람은 그 속에 머물고, 더 넓은 세상을 원하는 사람은 직접 찾아 나서는 경향이 생김
     * 충분히 찾다 보면 원하는 것을 결국 발견할 수 있음

마무리

     * 요약하면, 큐레이션이 사라진 시대에는 정보의 홍수 속에서 자기만의 정리법과 발견의 과정이 중요해짐
     * 기술의 편리함과 정보 과잉, 그리고 스스로 적극적인 참여를 균형 있게 활용하는 자세가 필요함

   그저 과거를 회상하는 글일 뿐인 것 같음. 여전히 주변에서 동일한 쇼츠나 릴스를 봤다는 경험을 쉽게 공유할 수 있음. 추천 알고리즘이라고 Exploitation 만 하지 않음. 다들 갑자기 새로운 분야의 영상이 추천되는 Exploration 의 경험을 했을 것임.

   마침 딱 이런 문제를 느껴서 스닙팟.. 을 창업했어요. 좋은 해결책을 떠올리는건 쉽지만 잘 되는건 진짜 어려운 문제.. ㅠㅠ

   알고리즘과 큐레이션의 차이에 대해서 모호함
   컴퓨터가 하면 알고리즘?
   사람이 하면 큐레이션?

   개인화된 추천 시스템에서
   적당히 사용자들이 그룹핑되고 그 그룹을 위한 추천이 더 좋다는 이야기로 보임

   종이매체 -> tv -> 인터넷
   으로 가면서 추천을 위한 사용자 집단 단위는 작아지고있는건 현실인 듯

   저는
     * 큐레이션은 수많은 정보 중 일부를 골라내는 행위
     * 알고리즘은 골라내는 동작의 기준과 방식
       으로 이해하고 있습니다.

     * 어느정도 문명화 된 이후부터 큐레이션이 없던 시절은 없는 것 같음
       과거로 가면 음유시인, 이야기꾼 이 지금의 큐레이터, 추천시스템 역할인 것 같음

        Hacker News 의견

     * 나는 예전부터 이 얘기를 계속해왔음. 90년대 십대 시절엔 라디오에서 음악을 접했음. 음악 디렉터가 매주 40곡 정도를 골라주면 모두 그 곡들을 들었음. 지금도 큐레이션 때문에 라디오 듣기를 좋아함. 내가 좋아하는 라디오 방송국(정확히는 음악 디렉터가 마음에 드는 방송국)의 웹사이트에서 곡 목록을 스크래핑해, Spotify 플레이리스트에 추가해주는 프로그램까지 직접 만듦. 요즘 십대들 만나면 항상 어떤 앱을 가장 많이 쓰는지, 또 새로운 음악을 어떻게 찾는지 궁금해서 물어봄. 대부분 “그냥 뭐 알아서 찾게 되는 것 같음” 정도로 말함. 어떤 애들은 YouTube나 Spotify에서 인플루언서의 플레이리스트를 팔로우한다고 함. 이게 아마도 새로운 음악 디렉터의 역할이라는 생각. 아니면 그냥 Spotify 플레이리스트에서 얻음. 근데 예전과 가장 달라진 건 모두가
       경험하는 “공유된 문화적 경험”이 사라졌다는 점임. 90년대에는 모두가 라디오 40곡을 알았음. 물론 다른 곡들도 알지만 최고 인기곡들은 피해갈 수 없었음. 영상 미디어도 비슷했음. 새 영화는 극장에서만 봐야 했고, TV쇼도 네 개 메이저 네트워크에서만 볼 수 있었으니까 전부가 그들의 존재를 다 알았음. 지금은 아이들이 예전 같은 공유된 문화 경험을 가지지 못하게 된 것임.
          + 이게 바로 내가 스트리밍 플랫폼 확산에 관해 영화·TV를 볼 때 느끼는 문제임. 지금이 가장 콘텐츠도 많고, 질적으로도 뛰어난 시대임. 그런데 오히려 뭘 볼 동기가 거의 사라짐. 예전엔 친구, 동료와 후기를 주고받으며 뭔가 볼 생각에 설렘이 있었음. 지금은 서로 뭘 봤냐고 물어보다 30초 만에 “안 봤는데”, “너는 봤어?”, “아니” 이러다가 대화 주제를 바꿈. 돌이켜보니, 남들과 콘텐츠에 대해 나누는 게 듣고 보는 경험의 핵심이었고, 이게 없으니 감흥이 전혀 안 생김.
          + ""아이들이 공유된 문화 경험을 갖지 않는다""는 말에 동의하지 않음. 사실 지금 플랫폼의 추천 알고리즘이 음악 디렉터 역할을 하고 있고, 비슷한 취향끼리는 피드 추천도 거의 같음. 그리고 플랫폼 속에서 자기 콘텐츠는 없이 모아서 재공유하는 큐레이터 유형도 있음. 혹시나 생기는 차이도, 사람들이 채널을 넘나들며 서로 공유해서 결국은 자연스럽게 해소함. 이게 바로 요즘 콘텐츠가 ‘바이럴’되는 메커니즘임. 요즘 세상이 인터넷 밈과 바이럴 SNS만으로 뉴스거리가 되는 세상임. 블록버스터 영화도 계속 나오고, GTA6 출시 때 경제에 10억 달러 손실 예상까지 나오는 세상임. 이 현상을 못 느끼는 쪽이 더 이상할 수준임.
          + 나는 요즘도 아이들에게 우리 세대 때와 비슷한 공유 경험이 있다고 생각함. 단지 우리가 그 경험에서 멀어져 있는 것뿐임. 우리도 어렸을 때 어른들은 우리 공유 문화를 몰랐던 것과 같음. 애들이 본인들끼리 어떻게 뭘 찾는지도 잘 설명을 못 하거나, 쑥스러움이나 멋없어 보일까봐 말 안 하기도 하는 듯함. 하지만 친구들끼리 어울리며 비슷한 걸 이야기하고, 서로 정보를 주고받으며 신나게 소통하는 그 순간들이 바로 매직인 것임.
          + 네가 관찰한 점은 맞을 수 있지만 결론은 틀림. 해마다, 지역마다, 관심사별로 묶인 아이들 그룹이 유명 인플루언서를 따라가고, 그래서 같은 콘텐츠를 소비하는 구조임. 관찰자 입장에서는 해당 플랫폼의 데이터를 못 보니까 어느 그룹에 속하는지 구분이 안 되는 문제임. 이런 논리가 바로 소셜 네트워크 군집을 집합 단위로 분석하는 소셜 셋 분석이라는 접근임. 나도 이 분야 연구를 했음.
          + Gianmarco Soresi가 본인 팟캐스트에서 이 얘기를 했음. 한때는 전국적으로 유명한 코미디언들이 모두가 공감하는 농담을 할 수 있었는데, 지금은 그게 불가능해짐. 이유는 문화가 장소와 덜 묶이고, 마이너 취향 집단이 더 많아졌기 때문임. 최근에는 내가 한 번도 안 들어본 아티스트도 대형 공연장을 매진시켜버림. 한편, 개인마다 자기가 좋아하는 콘텐츠를 더 쉽게 찾게 된 건 좋은 점인데, 한편으론 취향이 너무 다양해져서 남들과 공감대 형성 장벽이 높아진 것 같음.
     * 내가 어릴 땐 음악 찾는 루트가 몇 개 있었음. 음악에 엄청 빠져서 날 위해 열심히 찾아준 친구들도 있고, 특별한 취향(하드코어나 포스트 록 같은 장르)만 다루는 큐레이션 웹사이트, 혹은 포럼처럼 진짜 마니아들만 모여 있는 곳도 있었음. 그곳에서 사람들의 추천을 듣는 게 재미였음. 이런 경험에는 항상 신뢰하는 커뮤니티와 사람들의 영향이 있었음. 그런데 지금은 추천 알고리즘에서 그 맛을 전혀 못 느낌. Spotify에서도 좋은 곡은 종종 추천해주지만, 전반적으로 훨씬 외로워진 느낌임. 예전엔 음악이 사람들과 나를 연결했는데 이제는 나 혼자 Spotify와 있는 것임.
          + 대학 시절엔 친구들이 듣는 음악을 거의 따라 들었음. 점점 시간 지나며 음악 페스티벌 가거나 친구 통해 다양한 음악을 파악함. 그런데 요즘엔 “새로 찾기”에 크게 신경을 안 씀.
          + mixcloud 덕에 나는 새로운 음악 순례를 계속 이어가는 중임. 전 세계 사람들이 본인 믹스와 라디오쇼를 올려서 언제든 새로운 것 발견 가능. 내가 좋아하는 스타일에 뭔가 요상한 키워드로 검색해서 믹스에서 그 장르 쓰는 사람을 찾으면 그 사람과 어느 정도 코드가 맞음을 느낌. 그렇게 믹스테이프 제작자, DJ, 라디오쇼 호스트 리스트가 쌓이고, 전 세계 라디오를 듣는 기분이 든다는 점이 멋짐.
     * 여기서 서로 섞인 두 가지 흐름이 있다고 봄. 1) 만들어지는 “문화”의 양 자체가 25년 전보다 어마어마하게 증가. 너무 많아져서 도저히 다 못 봄. 2) 알고리즘은 이 문제를 풀기 위해 개발됐지만, 문제와 어울리지 않는 별로인 솔루션임.
          + 음악만 보면 문화 생산량이 실제로 늘었는지 약간 의문임. 예전엔 인디 밴드도 많았고, 고등학생 때는 친구 몇 명이 아마추어 밴드를 결성하거나, 여행 가면 현지인들도 자기 동네 음악(예: 터키 전통+모던 혼합)을 많이 들었음. 최근 여행에선 모두 같은 세계 공통의 유행곡만 듣고 있었음. 정확한 통계는 없어도, 음악 자체는 더 획일화, 창의성 감소, 지역색이 사라지는 모습임. 밴드 문화도 죽고 K-pop처럼 산업화된 소수만 남았음. 그래서 문화 생산 자체가 25년 전보다 크게 늘었다고 믿기 어려움. 그리고 알고리즘이 문제를 반드시 악화시킨다고 생각하진 않음. 가끔은 내 취향을 넓혀줄 새로운 스타일을 추천해주는 경우도 있음. 이미 이런 기능도 들어간 서비스가 있음. 하지만 결국 친구의 추천과는 비교가 안 됨. 친구가 CD를 줘서 억지로라도 여러 번
            듣다보면 뭔가 좋아지는 효과가 있었음. 인간 큐레이터가 있는 경우 더 노력을 하게 됨.
          + 나는 이렇게 생각함 — 1) 지금 나오는 “새로움”은 예전의 “새로움”에 못 미침. 예를 들어 Breaking Bad는 2008년 첫 시청 때처럼 지금 처음 봐도 신선함. 나도 Mad Men을 처음 보는데 18년 전 만들어졌다곤 믿기지 않을 만큼 품질이 좋음. 한편 Netflix 오리지널은 대부분 2시즌 후 취소되고, 예전처럼 대박 점프 발전이 더는 없는 느낌임. 2) 특정 시대정신(Zeitgeist)에 대한 논의도 거의 없고, 모든 게 뒤섞여있어 뭔가 ‘완결’되는 느낌이 없음. 소비자는 그냥 공개적 분노 표현 외엔 제작자와 소통 경로 없음. 큰 스튜디오는 SNS 이전에 만들어진 IP만 계속 우려먹는 중임. 3) 알고리즘이 문제를 만드는 쪽이지, 해법은 아님. 대형 테크기업은 이런 논의 싫어함. 창작은 위험해서, 사업만 효율적으로 키우려 함.
          + 알고리즘이 문제인 이유는 이게 주로 콘텐츠 제공자(플랫폼) 이익을 위해 설계됐지, 사용자를 위해서가 아님.
     * 기사 초반 3개(혹은 2.5개) 문단이 마치 Bjork가 공식 웹사이트를 꼭 가져야 한다는 얘기처럼 보였는데, 사실 기사 본론(더 많은 프로 비평가가 필요하다. 그런데 SNS가 이들을 몰락시켰다)에서 좀 벗어난 느낌임. 나는 이 논점에 대해 양가감정임. 웹/소셜미디어 이전 시대도 살아봤음. 어릴 땐 취향이 더 메인스트림이어서, 웹이 없을 때도 ‘숨은 콘텐츠’를 힘들게 찾을 필요가 없었음. 지금은 인기 있는 걸 별로 안 좋아하는데, 프로 비평가 취향하고도 맞지가 않음. 그래서 새로운 걸 어떻게 찾냐 하면, 근본적으로 ‘도전과 탈락’ 방식임. 재밌어 보이는 걸 이것저것 샘플링하다 별로면 포기함. 이럴 때 스트리밍 서비스가 딱 맞음. 또 도서관 가서 그냥 아무 책이나 몇 권 빌리기도 함. 대다수가 별로여도, 그렇게 해서 보석 같은 책을 만나기도 함. (도서관은
       같은 사이트 서비스도 해줌.) 새로운 문화를 꼭 따라가야 한다는 압박감도 없음. 내가 최근 찾은 책, 영화, TV쇼는 최신작일 수도, 오랜 걸 수도 있음. 비평가를 따르면 새로운 콘텐츠 위주로 알게 될뿐임. 옛날 비평이 있는지 찾아보기도 힘듦. 20년 전 희귀작 비평을 어떻게 찾겠음? 결국 랜덤하게 찾아야 함. 그리고 어린 시절에도 도서관이 중요했고, 그 덕분에 Dune이나 Plato의 Apology 같은 명작을 우연히 알게 됨.
          + ""초반에 Bjork 공식 사이트 얘기가 본론(비평가 부재)과 동떨어진 듯하다""라는 말에 대해, 나는 오히려 같은 맥락이라고 봄. 둘 다 ‘중앙 집중적, 공신력 있는 정보원’이 소셜미디어 군데군데 흩어진 게시글보다 낫다는 점을 강조함.
          + (위 코멘트의) “내 취향이 점점 마이너해졌다”라는 말에 공감함. 나도 90년대 십대 때는 비주류 음악에 빠졌었고, 잡지나 (들어가기 힘들었던) 웹도 별로 도움이 안 됨. 잡지들도 마이너 해외 아티스트보다 대부분 이미 어느 정도 뜬 밴드 위주임. 진짜 음반 찾는 가장 좋은 방법은 동네 작은 음악가게에 가서 하루 종일 음반 듣는 거였음. 거기 주인들이 음악 매니아이기 때문에 직접 물어보면 새로운 걸 알려줬음. 대부분 친구들은 그냥 당시(Top40, MTV 등)에서 주는 걸 들었고, 지금도 그 구조가 별로 변하지 않았음. 다만 2025년엔 정말 듣고 싶은 희귀 음악 찾아서 바로 들을 수 있음(예전엔 가게에서 주문해도 완전 비쌌음). 이 점이 훨씬 나음.
          + Bjork 예시는 중앙·공식 정보원이 없는 세계(수많은 소셜 한 줄 메시지의 파편화, 정보 해석 우주가 따로 존재하여 같은 팬끼리도 기본 사실조차 합의 불가)의 문제를 보여주는 것임. 공식 정보가 있으면 쓸데없는 혼란이 줄어들고, 공동체도 커짐. 소셜미디어로 분산·탈중앙화된 정보는 오히려 더 많은 스트레스를 주고 진짜 공동의 정보 기준(캐넌, 커먼스)을 통째로 날려버렸음.
     * 최근 내가 해커뉴스에 다시 발을 들이는 이유도 이런 것임. 내가 보는 게시글, 뉴스, 정보가 모두 다른 사람들도 똑같이 본다는 점 때문임. 작은 집단이어도, 사람들이 공통적 흐름을 공유할 수 있는 합의가 있음.
     * 큐레이션이 정말 훌륭할 때 감탄함. Netflix 처음 시작할 때 내 취향을 잘 파악해서 엄청 좋은 추천을 해줬는데, 어느 순간 더는 볼 게 없거나 추천 시스템이 망가졌는지 지금은 형편없음. 그리고 다른 경쟁 서비스도 별로임. 한 가지 재미있는 점은, 사람에게 소설 추천 받으려 “The Martian과 비슷한 소설 추천해달라”고 하면, 없으면 그냥 자기 최애만 추천한다는 점임. 이런 식이라서 레딧 추천글도 모두 소음만 넘침. 원하는 정보 얻기 너무 힘듦.
          + Netflix는 예전에는 수준 높은 오리지널 콘텐츠도 만들었는데, 최근에는 점점 투자 대비 효율만 신경 쓰면서 특정 타겟 오디언스 맞춤형 저예산 ‘체크리스트형’ 콘텐츠만 쏟아냄. 세트도 항상 주인공 몇 명만 등장할 뿐임. 이런 쇼들이 재미없는 건 당연함—진짜 예술이라기보단 알고리즘이 짜준 공식에 불과함. 진짜 특이한 작품에는 약간의 예산만 쓰고, 나머지는 그냥 볼 가치 없는 작품이 대부분이라, 큐레이션을 아무리 해도 결국 추천해줄 만한 게 없음.
          + 약간 딴 얘기지만, 나는 최근에 Alfred Lansing의 ""Endurance""를 읽었는데, 느낌상 ""The Martian""과 비슷한 느낌. 영화가 더 인상에 남았지만.
     * 큐레이션의 가치 자체엔 동의함. 심지어 어떤 때는 게이트키핑도 필요하다고 봄. 하지만 타이밍이 재밌는 게, 지금 막 Clair Obscur: Expedition 33이 엄청 인기인데, 이건 큐레이션 덕이 아니라 ‘입소문’이 계기였음. 뭔가 진짜 뛰어난 콘텐츠면 굳이 큐레이터 없어도 사람들끼리 바로 퍼져나감. 큐레이터는 일부 발굴용으론 유용하지만, 누가 봐도 명작은 저절로 모두 알게 됨.
          + 그런데 내가 보기엔 그 주장은 성립하지 않음. Clair Obscur 뿐 아니라 Blue Prince도 출시 전에 이미 메타크리틱에서 엄청난 평을 받았음. 그래서 레딧 같은 곳에서 “갑자기 이런 명작이 나왔다. 평점 대박”이라는 말이 돌며 입소문을 탔음. 결국 큐레이션+비평의 영향이 있었다는 뜻임.
          + 나는 당신 말 곧이곧대로 받아들이겠지만 약간 웃김. 내 주변 게이머 집단엔 이 게임 타이틀이 전혀 등장하지 않음. 역시 세상엔 다양한 정보 흐름이 있음.
          + 입소문 마케팅도 결국은 큐레이션의 한 형태임.
          + 입소문뿐 아니라 마케팅, 큐레이션(이 둘이 겹침)도 영향이 있었음. 그러나 이런 요소들은 이미 엄청 인기 있는 것에는 사실상 필요가 거의 없음.
     * 요즘은 제품을 고를 때도 비슷하게 느껴짐. 제품도 정보도 너무 많아서, “최고의” 제품을 고르려고 몇 시간씩 헤매게 됨. 예전엔 두세 개만 보고 골랐음. 그때가 더 나았냐 묻는다면 잘 모르겠기도 함.
          + 소비자 환경이 점점 험악해졌음. 예전엔 가격이 곧 품질을 대변하는 지표였음. 비싼 게 대체로 나았음. 지금은 마케팅, 브랜드, 거짓말만 넘침. 심지어 비싼 제품조차 품질이 조악하고, 금방 쓰레기장이 됨. 돈을 더 내도 제대로 된 제품 구하기가 힘듦. 요즘 자본주의는 돈의 가치보다, 최대한 짜내고, 생산비 줄이기에 혈안임.
     * 큐레이션 얘기보다 더 근본적으로, 요즘 UI에서는 ‘셀프 디터미네이션과 스스로 발견’의 모든 도구가 사라졌다고 봄. 모든 인플루언서나 알고리즘은 결국 자기 이익 때문에 남의 콘텐츠를 골라주고, 진짜로 나를 위한 게 아님. 예전에는 위키피디아나 tvtropes 처럼 스스로 빠져들 수 있는 공간이 있었고, 자유롭게 탐색이 가능했음. 그런데 지금은 닫힌 플랫폼, 로그인 필요한 서비스, 벽에 가로막힌 데이터뿐임. 오픈소스 플랫폼이 정말 필요한 시대임. 예전엔 큐레이터들이 활용할 수 있는 툴, 강력한 검색 기능이 있었고, 누군가는 위키를 만들고, 누군가는 거기서 글을 쓰며, 어떤 사람은 글만 읽거나 방송만 보기도 했음. 하지만 지금은 정보 자체가 폐쇄되고 큐레이션밖에 없음. 결국 인스타그램에 올라온 jpg 사진만 보고, 남은 주말에 뭐할까 고민할
       정도임. 알고리즘이 맞춤형으로 큐레이트 해줘서 점점 놀라운 걸 못 버티게 되고, 변화도 결국 알고리즘이 천천히 몰아줄 때만 생기니, 완전히 새로운 발견이 사라진 것임.
          + 심지어 검색도구를 쓸 때도, 그 도구 결과가 돈벌이와 편향이 없는지 믿을 보장이 없음. 과거에도 이런 불신을 만들 이유가 충분했고, 이제는 소프트웨어 업계에 대한 신뢰가 완전히 사라짐. 새로 나온 모든 소프트웨어가 자기 이익만 챙기지 사용자를 위하지 않는다는 확신만 생김.
          + 오히려 지금이 예전보다 더 도구가 강력해졌다고 생각함. “주말 등산 코스 찾기”를 예로 들면, 예전엔 책이나 지도 같은 제한된 자료에 의존했고, 최신 정보 파악도 힘들었음. 지금은 하이킹 사이트, 오픈스트리트맵, 구글지도 등에서 리뷰, 사진, 댓글, 데이터까지 쉽게 확인함. 더 책임감을 갖고 “알고리즘”이나 “이윤” 탓만 할 게 아니라는 생각임. 흡연처럼 모두에게 해롭다는 걸 아는데도 안 끊는 사람이 있는 것과 비슷함.
          + 지금 웹을 떠나자는 주장을 더 확실하게 하게 됨. 내 머릿속에서 “웹”과 “넷(네트워크)”는 분리되어 있음. 웹은 그 위에 붙어 있는 것일 뿐임. 지금 웹은 AI저질글, SEO스팸, 폐쇄 플랫폼, 각종 해킹 봇, 이런 것밖에 없음. 데드 인터넷 이론이 점점 현실처럼 느껴짐. 언젠가 인류가 웹을 떠나게 될 바라봄(나는 그 날이 빨리 왔으면 좋겠음).
          + 완전히 공감가는 글이라 읽다가 멈춰서게 될 정도임. 사람들이 진짜 많은 기회를 놓치고 있음.
          + 플랫폼은 이미 본질적으로 닫힌 구조임. 오픈소스라면 플랫폼이라 부를 수도 없음.
     * 만약 모든 게 큐레이션된다면, 어떻게 기꺼이 돋보이는 큐레이션을 찾을 수 있을까? 유익한 정보를 만들어도, 그걸 원하는 대중에게 어떻게 노출시킬 수 있을지 어려움. 결국 노이즈만 가득하다고 믿는 한, 아무도 큐레이션하지 않으려 하게 됨.
          + 작은/인디 웹사이트를 정말로 찾아주는 노력들도 있음. 예를 들면 웹링, Kagi의 스몰웹 기능처럼 새로운 시도가 도움이 될 수 있음.

   ""근데 예전과 가장 달라진 건 모두가 경험하는 “공유된 문화적 경험”이 사라졌다는 점임. 90년대에는 모두가 라디오 40곡을 알았음.""

   ""Spotify에서도 좋은 곡은 종종 추천해주지만, 전반적으로 훨씬 외로워진 느낌임. 예전엔 음악이 사람들과 나를 연결했는데 이제는 나 혼자 Spotify와 있는 것임.""

  ""최근 내가 해커뉴스에 다시 발을 들이는 이유도 이런 것임. 내가 보는 게시글, 뉴스, 정보가 모두 다른 사람들도 똑같이 본다는 점 때문임. 작은 집단이어도, 사람들이 공통적 흐름을 공유할 수 있는 합의가 있음""

   공통적으로 나오는 의견들이 눈에 들어오네요
"
"https://news.hada.io/topic?id=20924","HelixDB - RAG와 AI 애플리케이션을 위한 Rust 기반 그래프-벡터 데이터베이스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           HelixDB - RAG와 AI 애플리케이션을 위한 Rust 기반 그래프-벡터 데이터베이스

     * Rust로 작성된 고성능 오픈소스 그래프-벡터 DB로, RAG 및 AI 애플리케이션에 특화됨
     * Neo4j 대비 1000배, TigerGraph 대비 100배 빠른 성능, Qdrant 수준의 벡터 처리 성능 제공
     * 그래프와 벡터를 동시에 저장할 수 있는 직관적이고 강력한 쿼리 언어 제공
          + 노드, 벡터, 또는 그 조합을 자연스럽게 저장하고 질의 가능
     * Meilisearch 팀의 Heed3를 통해 LMDB를 스토리지 엔진으로 사용. 이를 기반으로 한 신뢰성 높은 저장 계층과 ACID 트랜잭션 지원
     * TypeScript 및 Python SDK 지원, 로컬 인스턴스 실행 및 API 엔드포인트 배포 자동화 도구 제공
     * AGPL (Affero General Public License)

   AGPL이네요..
"
"https://news.hada.io/topic?id=20984","Show GN: 투두만 있는 투두앱","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: 투두만 있는 투두앱

   안녕하세요!

   10년지기 디자이너 친구와 함께 ""내가 쓸 앱 내가 만들자""는 취지로 만든 투두 앱입니다.

   딱히 특별한 기능은 없습니다.
   둘 다 극단적이 P 성향이라, 정말 생각없이 계획없이 할일만 쌓는 앱을 원해서 만들었어요.
   (희한하게 독일, 러시아 사용자가 많더라고요)

   안드로이드도 조만간 출시 예정입니다!

   ios: https://apps.apple.com/kr/app/…

   대단하십니다. 해외 홍보는 어떻게 하신건지 궁금합니다

   해외 홍보는 전혀 안했는데 언어를 될 수 있는 한 전부 제공했어요! ios 제품 페이지도 현지화하고요.

   깔끔해요 ㅎㅎ

   벌써 후기를ㅋㅋㅋㅋㅋ감사합니다!

   안드로이드 앱 기다리고 있겠습니다!!

   감사합니다 mㅡㅡm
"
"https://news.hada.io/topic?id=20966","If를 위로, For를 아래로 옮기기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          If를 위로, For를 아래로 옮기기

     * 함수 내부의 if 문을 호출부로 올리면 코드의 복잡성 감소에 도움을 줌
     * 조건 검사 및 분기 처리를 한 곳에서 집중하면 중복 및 불필요한 분기 확인을 쉽게 할 수 있음
     * enum 분해 리팩토링을 사용해 동일한 조건이 코드 곳곳에 퍼지는 문제를 방지할 수 있음
     * 배치 연산 기반의 for문은 성능 향상과 반복 작업 최적화에 효과적임
     * if는 위로, for는 아래로 내리는 패턴을 조합해 코드 가독성과 효율성을 동시에 증대시킬 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

두 가지 관련 규칙에 대한 간단한 메모

     * 함수 내에 if 조건문이 존재할 경우, 이를 함수 호출부로 옮길 수 있는지 고민하는 방식이 추천됨
     * 예시처럼, 함수 내부에서 precondition(전제 조건)을 검사하는 것보다, 호출부에 해당 조건 검사를 맡기거나 타입(혹은 assert)로 전제 조건을 보장하도록 만드는 것이 바람직함
     * 전제 조건 검사를 위로 올리는(Push up) 방식은 코드 전반에 영향을 미쳐, 전체적으로 불필요한 조건 검사 수를 줄임 효과를 줌

제어 흐름과 조건문의 집중

     * 제어 흐름과 if 문은 코드의 복잡성 및 버그의 주요 원인임
     * 조건문을 호출부 등 상위로 집중시켜 분기 처리를 한 함수에만 몰아주고, 실제 작업은 직선적(스트레이트라인) 서브루틴에 맡기는 패턴이 유익함
     * 분기와 제어 흐름이 한 곳에 모이면 중복 분기와 불필요한 조건을 쉽게 파악할 수 있음

   예시:
     * f 함수 내에 중첩된 if가 있을 때에는 죽은 코드(Dead Branch)를 인식하기 쉬움
     * 여러 함수(g, h)를 통한 분기가 분산되면 이런 파악이 어려워짐

enum 분해 리팩토링(Dissolving enum Refactor)

     * 코드가 같은 조건문 분기를 enum 등으로 내포하고 있을 경우, 조건을 상위로 끌어올려 분기와 작업을 더 명확하게 구분할 수 있음
     * 이 방식을 적용하면, 동일한 조건이 코드에 여러 번 반복되어 나타나는 문제를 막을 수 있음

   예시:
     * 동일한 분기 조건이 f, g 함수와 enum E에 각각 표현된 상황을
     * 하나의 상위 조건 분기로 코드 전체를 단순화할 수 있음

데이터 중심적 사고(Data Oriented Thinking)와 배치 연산

     * 대부분의 프로그램은 여러 객체(엔티티)로 동작함. 크리티컬 경로(Hot Path)는 다수 객체 처리로 성능이 결정됨
     * 배치(batch) 개념을 도입해서 객체 집합에 대한 연산을 기본으로 만들고, 단일 객체 연산은 특수 케이스로 처리하는 것이 바람직함

   예시:
     * frobnicate_batch(walruses) 처럼 배치 처리 함수를 기본으로 두고,
     * 개별 객체는 for 루프를 통해 처리하는 특수 케이스로 변환 가능함
     * 이 방식은 성능 최적화 측면에서 중요한 역할을 하며, 대량 작업에서는 스타트업 비용을 감소시키고 순서 유연성을 높여줌
     * SIMD 연산(struct-of-array 등) 활용도 가능해, 특정 필드만 일괄 처리 후 전체 작업을 진행할 수 있음

실용적 사례와 추천 패턴

     * FFT 기반 다항식 곱셈처럼, 여러 지점에서의 동시에 연산을 가능하게 만들어 성능 극대화가 가능함
     * 조건문을 위로, 반복문을 아래로 내리는 규칙은 병행 적용 가능함

   예시:
     * 반복문 내부에서 같은 조건식을 계속 검사하는 것보다, 조건문을 반복문 바깥으로 빼면 반복 루프 내 분기를 줄이고 최적화 및 벡터화가 쉬워짐
     * 이 접근법은 TigerBeetle 설계 등, 대규모 시스템 데이터 플레인에서도 높은 효율성을 보장함

결론

     * if문(조건문)은 상위(호출부, 제어부) 로, for문(반복문)은 하위(연산부, 데이터 처리부) 로 내리는 패턴을 조합함으로써, 코드 가독성과 효율성, 성능 모두를 향상시킬 수 있음
     * 추상 벡터 공간 시점으로의 사고(집합 단위 연산)는 반복적 분기 처리보다 더 좋은 문제 해결 도구임
     * 요약하면, if는 위로, for는 아래로!

        Hacker News 의견

     * 내 독특한 정신적 모델은 다양한 상태나 프로그램 흐름이 나무 구조를 이룬다는 형태임. 조건문이 이 나무의 가지를 쳐내는 역할을 함. 가능한 한 빨리 가지치기를 해서 이후 처리해야 할 가지 수를 줄이고 싶음. 모든 가지를 일일이 평가하고 치우다가 결국 전체 가지를 한꺼번에 잘라버려야 한다는 상황을 피하고 싶음. 약간 색다른 시각으로 볼 때, 조건문은 ""필요 없는 일을 발견해내는 과정""이고, 루프는 ""실제 일""임. 궁극적으로 내가 원하는 함수는 프로그램 트리를 탐색하거나 실제 일을 처리하는 것 중 한 가지에 집중하는 형태임
          + 내 옆 모델을 제시해 보고 싶음. 클래스는 명사이고, 함수는 동사라고 생각함
          + 내 정신적 모델은 내가 작성하는 구체적인 코드가 존재하는 세계에 맞추는 것임. 도메인 특성, 기존 코드 패턴, 데이터 파이프라인의 단계, 성능 프로파일 등에 따라 달라짐. 처음엔 이런 규칙이나 휴리스틱을 만들려고 했는데, 코드를 많이 써보니 이런 추상 규칙은 실제론 크게 의미가 없다는 것을 깨달음. 많은 경우 엉뚱하게 함수 이름이나 한 글자만 정해놓고 그 “섬 같은 코드” 안에서만 규칙이 성립하는데, 실제 코드베이스에서는 보통 이렇게 함수들을 굳이 합치지 않은 이유가 있음. 예시로 “중복과 죽은 조건” 이야기가 나오는데, 그건 해당 함수가 오직 단 한 곳에서만 호출된다는 편한 가정을 해서 적용한 규칙임. 실제론 다른 이유로 서로 분리돼 있을 때가 많음
          + 아주 괜찮은 모델이라고 생각함
     * 좀 더 일반적인 규칙은 조건문을 입력 소스와 최대한 가깝게 두는 것임. 외부에서 프로그램으로 들어오는 진입점(다른 서비스에서 가져온 데이터 포함)을 최대한 빨리 식별하고, 코어 로직까지 도달하기 전에 (특히 리소스를 많이 쓰는 부분에 도달하기 전에) 가능한 한 많은 보장을 만들어두는 게 핵심임. 타입에 그걸 명시적으로 표현하는 것도 매우 좋음
          + 이렇게 하면 코어 로직을 이해할 때 어떤 전제를 갖고 있는지 알기 어려워지지 않음? 코드 전체의 호출 체인을 다 들여다봐야 하지 않음?
     * “if 조건이 함수 내부에 있으면, 그걸 호출자 쪽으로 옮길 수 있을지 고려하라”는 조언에는 반례가 너무 많음. 만약 함수가 37군데에서 호출된다면, 모든 호출부마다 같은 if 문을 반복해야 함? 예를 들어 getaddrinfo나 EnterCriticalSection 같은 함수에 이런 식으로 if를 옮기라고 할 수 있나? 이런 변환은 딱 두 군데 정도에서만 호출되는, 그리고 그 결정이 함수의 관심사 밖에 있을 때만 고려할 수 있다고 생각함. 한 가지 방법은, 조건문만 수행하는 함수에 헬퍼 함수를 위임해서 작성하는 것임. 그리고 루프 밖으로 조건을 옮길 필요가 있을 때엔 더 저수준 조건 헬퍼를 호출자가 직접 사용하게 할 수 있음. 하지만 이런 고민의 핵심은 “최적화” 때문임. 최적화가 항상 더 좋은 프로그램 설계와 충돌하는 경우가 많음. 호출자가 조건을 알 필요가 없는 게 더 좋은
       설계일 수도 있음. 이런 딜레마는 OOP에서도 자주 등장함. “if”로 대표되는 결정이 실제론 메서드 디스패치로 이뤄지는 경우임. 이러한 디스패치를 루프 밖으로 꺼내는 것도 설계 원칙과 마찰을 일으킬 수 있음. 예를 들어 캔버스에 이미지를 그릴 때, 매번 putpixel을 반복 호출하기보다는 blit 같은 메서드를 활용하는 것이 사례임
          + 함수가 37군데에서 호출된다면 코드 리팩터링이 필요하긴 함. 그 질문에 답하자면, 상황에 따라 다름. DRY가 정답처럼 느껴지긴 하지만, 실제 예시 코드를 보고 결정해야 함. 라이브러리라면 소유권 경계에 있기 때문에 각자 자신의 데이터와 책임을 관리해야 함. EnterCriticalSection 같은 함수는 진입 구간에서 강력한 검증(조건문 포함)을 하는 게 맞음. 그런데 애플리케이션 코드에서는 if를 호출자 쪽으로 옮겨도 무방함. 라이브러리나 핵심 코드에선 제어 흐름을 가장자리로 이동시키는 것이 적절함. 자신이 다루는 도메인 내에서는 제어 흐름을 가장자리에 두는 것이 좋음. 하지만 언제나 이런 규칙은 관용적일 뿐이니, 상황에 따라 합리적으로 판단할 수 있는 사람이 맥락에 맞춰야 함
     * “dissolving enum 리팩터” 예시는 사실상 다형성(polymorphism) 패턴임. match문을 다형적 메서드 호출로 대체할 수 있음. 이 방식의 목적은 초기 조건 분기를 결정하는 시점과 실제 동작이 실행되는 시점을 분리하는 데 있음. 케이스 구분은 객체(여기서는 enum 값)나 클로저가 들고 있으므로, 호출 시점마다 그걸 반복할 필요 없음. 케이스 구분이 바뀌면 분기점만 바꾸면 되고, 실제 행동이 발생하는 곳은 수정하지 않아도 됨. 단점은, 각 케이스 별 동작 분기를 직접 확인할 수 있는 편리함과, 코드 레벨에서 케이스 리스트에 의존성이 생기는 점의 트레이드오프임
     * 조건문을 함수 내부에 두는 걸 좋아할 때가 있음. 일부러 호출자가 함수의 호출 순서에 실수하지 못하게 만들 수 있기 때문임. 예를 들어 멱등성 보장이 필요한 경우, 이미 처리된 상태를 먼저 확인하고 아니면 수행하는 식임. 이 조건을 호출부로 빼면 모든 호출자가 그 절차를 올바로 지켜야만 멱등성이 보장되므로, 추상화에서 그 보증을 제공하지 못함. 이런 상황에서는 이 철학을 어떻게 적용해야 하는지 궁금함. 또 예를 들면, 데이터베이스 트랜잭션 안에서 일련의 체크를 모두 한 다음 작업을 하고 싶을 때, 그 체크들을 어디에 둘지 고민임
          + 이미 본인이 질문에 답한 것 같음. 조건문을 호출부로 빼면 함수가 더 이상 멱등적이지 않고, 당연히 보장도 못함. 멱등성 보장을 위해 함수마다 상태 관리 로직을 넣는다면, 뭔가 많이 이상한 코드를 짜고 있는 것일 수 있고, 너무 많은 비즈니스 논리를 단일 함수에 몰아두고 있다는 의미임. 멱등 코드는 크게 두 가지로 나눔. 첫째, 데이터 모델이나 연산 자체가 멱등적인 코드. 이땐 굳이 처리 순서를 신경 쓸 필요가 없음. 두 번째는 더 복잡한 비즈니스 연산에서 멱등 추상화를 만드는 것. 롤백이나 원자적 적용(abstraction on atomic apply) 같은 복잡한 로직이 필요하고, 이 경우는 단일 함수에 간단히 담을 수 있는 얘기가 아님
          + 체크 없는 내부 함수를 만들고, 외부에 래퍼 함수에서 체크 후 내부 함수를 호출하는 방식으로 관리하는 것도 방법임
     * 코드 복잡도 스캐너는 결국 if 문을 아래로 밀어내는 경향이 있는 도구임. 하지만 이 글에서는 반대로 if 문을 위로, 즉 더 상위 함수로 올리는 걸 추천함. 그렇게 하면 복잡한 분기 로직을 단일 함수에서 중앙집중적으로 처리할 수 있고, 실제 구체적 작업은 서브루틴에 위임하게 됨
          + 해결 방법은 “결정”과 “실행”을 분리하는 것임. Bertrand Meyer에게서 배운 아이디어임. 예를 들면 if (weShouldDoThis()) { doThis(); } 이런 식이고, 각 체크를 별도 함수로 빼면 테스트와 복잡도 관리가 쉬워짐
          + 코드 스캐너의 리포트는 진지하게 의심할 필요가 있음. sonarqube 등은 실제 버그가 아닌 “code smell”도 마구잡이로 보고함. 이런 식으로 “문제가 안되는 코드”까지 수정하려다 실제로 새로운 버그가 생길 위험이 높아지고, 진짜 중요한 문제 대처 시간만 낭비하게 됨
          + 이런 최적화는 대체로 “국지적 최적”일 때가 많음. 즉, 새 요구사항이나 예외 케이스가 발견되면, 분기 로직이 루프 밖에 필요해짐. 그 상태에서 루프 안팎 모두에 분기가 섞이면 이해하기 어려워짐. 조건이 루프 내부에만 필요하다고 확신하면 그렇게 두고, 그렇지 않으면 차라리 미리 설계를 조금 더 길게 가져가고, 코드도 장황해져도 더 이해하기 쉬운 쪽이 낫다고 생각함. Haskell을 쓰다가 이런 경험을 했음. 로직을 가장 간결하고 최적화된(local optimum) 형태로 추구하면, 요구사항이 아주 조금만 바뀌어도 설계의 의도를 표현하기보단 그냥 로직만 남게 되고, 작은 변경에도 코드 언롤링이 심하게 일어남
          + 코드 복잡도 스캐너는 항상 불만이었던 존재임. 읽기 쉬운 큰 함수조차 불만을 표출함. 로직을 한곳에 놓으면 전반적 맥락을 이해하기 쉬운데, 함수를 쪼갤 때는 진짜 맥락을 놓치지 않도록 조심해야 함
          + 어제 LLM에 대한 스레드에서 “개발자들이 다들 받아들이는 신뢰할 수 없는 도구”에 대해 얘기 나온 적 있었음. 이제 답을 알겠음…
     * 경우에 따라서는 오히려 반대로 접근해서 SIMD를 활용해야 함. 예를 들어 AVX-512 등에서는 분기가 있는 코드를 브랜치 없는 코드로 벡터 마스크 레지스터를 써서 처리할 수 있음. 예를 들어 for문 안 if 문은 for문 밖 if 문보다 관리가 쉽고, 메모리 접근 효율이 높음. 구체적 예제를 들면, 홀수면 +1, 짝수면 -2 하는 연산이 있을 때, 원래는 각 루프에서 분기를 타야하지만, SIMD로 벡터 처리하면 16개씩 모든 int 를 동시에 처리할 수 있고, 브랜치도 없음. 컴파일러가 제대로 벡터화하면 원래 코드를 브랜치 없는 최적화 버전으로 바꿔줄
          + 제시한 before 코드는 해당 글의 논점과는 조금 맞지 않는 것 같고, 오히려 최적화된 SIMD 버전이야말로 글의 요점에 부합한다고 생각함. 예시에서 for문 안 if는 데이터에 의존하는 분기라서 쉽게 끌어올릴 수 없음. 만약 알고리즘이 if (length % 2 == 1) { ... } else { ... }처럼 루프 밖 조건만 쓰는 구조였다면, 이런 조건은 당연히 for문 위로 빼는 게 정답임. SIMD 버전에선 if가 아예 사라졌고, 이런 게 이상적인 코드 패턴이라 글 저자도 좋아할 만한 방식임
          + 나도 for 루프의 요소 값에 따라 분기하는 코드가 바로 떠올랐음. 혹시 컴파일러에서 이런 코드를 자동 벡터화하는 게 얼마나 어려운지 아는 사람 있음? 그 경계가 궁금함
     * 개인적으로 이게 “좋은” 규칙이라고는 생각하지 않음. 적용 가능한 경우가 있긴 하지만, 맥락에 따라 너무 달라서 딱 잘라 결론을 내리기 힘듦. 영어 철자 규칙처럼 예외가 너무 많아서 사실상 규칙으로 취급하기 어렵다고 느껴짐
     * (2023) 당시 토론 링크 (662점, 295개 댓글) https://news.ycombinator.com/item?id=38282950
     * Sandi Metz의 99 Bottles of OOP에서 이와 비슷한 내용을 접했음. 내 스타일은 아니지만, 분기 로직을 호출 스택 최상단에 올리는 것이 유용하다는 점은 동의함. 플래그를 여러 레이어에 넘겨주던 코드베이스에선 특히 크게 느꼈음. https://sandimetz.com/99bottles
          + 동일 저자의 “The Wrong Abstraction” 글이 바로 떠올랐음. for문 내부 분기는 “for가 규칙, 분기가 동작”이라는 추상화를 만드는 것임. 그런데 새로운 요구사항이 등장하면 이런 추상화가 깨지고, 억지로 파라미터를 끼워 넣거나 예외 처리를 늘려 코드가 이해하기 어려워짐. 처음부터 추상화 없이 코드를 짰다면 결과물이 더 명확하고 유지보수하기 쉬웠을 것임. https://sandimetz.com/blog/2016/1/20/the-wrong-abstraction
"
"https://news.hada.io/topic?id=20974","AniSora: 오픈소스 애니메이션 비디오 생성 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AniSora: 오픈소스 애니메이션 비디오 생성 모델

     * AniSora는 Bilibili가 개발한 애니메이션, 만화, VTuber 등 다양한 스타일을 지원하는 오픈소스 AI 비디오 생성 모델
     * 완성도 높은 고화질 애니메이션 비디오를 이미지나 텍스트 프롬프트로 쉽게 생성할 수 있음
     * 만화와 애니메이션에 특화된 알고리듬과 대용량 데이터셋 기반으로 사실적인 동작과 표현력 있는 결과물을 제공함
     * 비전문가도 손쉽게 활용할 수 있는 직관적인 인터페이스와 커뮤니티 기반 협업 환경이 강점임
     * 단편 에피소드, PV, VTuber, 콘셉트 아트 등 다양한 활용 사례에 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AniSora란 무엇인가

     * AniSora는 Bilibili가 개발한 가장 강력한 오픈소스 애니메이션 비디오 생성 모델임
     * 한 번의 클릭만으로 애니메이션 시리즈, 중국 애니메이션, 만화 원작, VTuber, 애니메이션 PV 등 다양한 스타일의 영상을 제작할 수 있음
     * 이미지 또는 텍스트 프롬프트만으로 정적인 장면을 역동적이고 세밀한 애니메이션으로 구현할 수 있음
     * AniSora의 기반 연구는 IJCAI’25에 채택된 최신 기술 논문을 바탕으로 함

AniSora 예시 소개

     * AniSora로 생성된 예시 영상들은 정지 이미지를 자연스럽게 움직이는 애니메이션으로 전환하는 능력을 보여줌
          + 예) 자동차 안에서 머리가 바람에 흔들리는 장면, 여러 소녀가 손을 들어 춤추는 모습, 고속으로 달리는 캐릭터의 동작 블러 등
     * 이 모델은 등장인물의 감정 표현, 동세, 장면 전환 등 만화와 애니메이션에서 필요한 다양한 연출을 사실적으로 실현함

AniSora의 주요 장점

  애니메이션/만화 스타일에 특화된 알고리듬

     * 대용량 애니메이션, 만화 데이터셋으로 학습된 특화된 모델 구조를 사용함
     * 고유한 시각 스타일과 연출을 정확하게 재현함
     * 최신 만화 원작 및 트렌드까지 반영한 고품질 출력이 가능함

  직관적인 인터페이스

     * 기술에 익숙하지 않은 사용자도 직접 영상 생성을 할 수 있도록 설계됨
     * 클릭 한 번으로 누구나 손쉽게 만화와 VTuber 영상 제작을 경험할 수 있음

  고품질 애니메이션 영상 지원

     * AniSora는 1080p 고화질 비디오 출력을 기본으로 제공함
     * 다양한 플랫폼에 최적화된 영상으로, 프로젝트, SNS, PV 등 다양한 채널에 활용이 가능함

AniSora FAQ 요약

  AniSora란?

     * AniSora는 Project Index-AniSora의 일부로, Bilibili가 공개한 오픈소스 애니메이션 비디오 생성 모델임
     * 이미지 또는 텍스트 프롬프트만으로 고해상도, 애니메이션 스타일의 영상을 자동으로 제작할 수 있음
     * 동작 일관성과 표현력에 중점을 둔 최신 연구 결과가 반영됨

  타 AI 비디오 생성 도구와의 차이점

     * AniSora는 애니메이션, 만화 스타일에 특화된 성능에 집중함
     * Bilibili의 전문성과 커뮤니티 중심의 오픈소스 프로젝트로서, 애니 시리즈, 만화 원작, VTuber 등 특수 목적 영상 제작에 최적화되어 있음

  비디오&오디오 지원 여부

     * 현재 AniSora는 비디오 생성에 중점을 둔 모델임
     * 오디오 합성 등 추가 기능의 제공 여부는 최신 문서 참고 필요함

  애니메이션/만화 창작자에게 적합한가?

     * AniSora는 캐릭터 일관성, 표현력 있는 동작 구현에 최적화되어 창작자(특히 애니메이션, PV, 만화, VTuber 분야)에게 이상적인 도구임

  주요 활용 사례

     * 애니메이션 단편, SNS 영상, PV, 만화 패널 애니메이션, VTuber, 콘셉트 아트, 스토리보드 등 폭넓은 활용이 가능함

  영상 퀄리티와 길이

     * AniSora는 고해상도(1080p) 기준의 단편 영상 생성에 강점을 가짐
     * 일반적으로 짧은 클립 형태가 적합하며, 구체적 제약사항은 공식 문서에서 최신 정보를 확인할 수 있음

  스타일과 동작 컨트롤 방법

     * 이미지 또는 텍스트 프롬프트 입력을 통해 사용자가 원하는 시각 스타일과 동작을 유도할 수 있음
     * 애니메이션 분야 데이터를 기반으로, 동작 커스터마이징, 캐릭터 일관성, 세부 스타일 적용 등 고도화된 통제가 지원됨
     * 버전이나 인터페이스에 따라 운영 가능 범위가 상이함

결론

     * AniSora는 애니메이션 및 만화/VTuber 영상 제작에 특화된 최고 성능의 오픈소스 AI 영상 생성 모델임
     * 차별화된 스타일 재현력, 직관적인 사용법, 고화질 영상 생성이 주요 이점임
     * 커뮤니티 오픈소스 문화와 창작자 지원에 중점을 두어, 일본 애니메이션과 중국 애니메이션 분야 모두에서 활용 가치가 높음

        Hacker News 의견

     * 일부 결과물이 웹툰, 만화, 아마도 pixiv 등에서 명확하게 학습된 흔적이라는 생각이 든다. CG 건물이나 다양한 기타 인공물에서 그 증거를 쉽게 발견 가능. 결국 저작권이 있는 자료로 학습한 것이라는 결론. 예술은 텍스트처럼 합성된 방식으로 생성할 수 없는 영역이기 때문에 사람 아티스트가 영원히 중요한 자리를 차지할 것, 아니면 계속 이상한 인공물이 생기는 결과만 나올 것. 그래서 앞으로 아티스트가 ""AI"" 학습을 위한 직업군으로 격하되는 방향이 되지 않을까 하는 생각이 든다. 하지만 오히려 사람들이 각자 좋아하는 걸 그려서 그걸 모델 학습에 활용하는 구조라면 나쁘지 않다는 생각도 든다. 나는 저작권, 상표권에 관련해서 아주 AI를 지지하는 입장이지만, 우리에게 재미를 주던 많은 사람들에게 어떤 일이 벌어질지 계속 궁금증이 남는다.
       퀄리티가 계속 오를지, 아니면 'AI에겐 너무 어렵다'는 이유로 도전적인 스타일이 사라지고 모든 게 비슷비슷해질지 고민. 이건 PC와 기계가 사람을 대체하는 것과는 다른 종류의 느낌, 뭔가 종착점에 온 듯한 기분.
          + 삽화가나 예술가에 연민이 느껴진다. 하지만 학습 데이터가 소설, 그림, 노래, 코드, 심지어 법률 문서든 큰 차이점이 없다고 생각. 우리 엄마도 타자기 시절 번역가에서 기계 번역 코퍼스-데이터베이스 환경으로 바뀌었고, 일거리는 점점 줄고 임금도 내려갔음. 결국 기계적이고 반복적인 일들은 더 저렴한 로봇이 하게 될 운명.
          + 30년 넘게 그림 그린 아티스트 입장. AI 학습보조직으로 격하된다는 건 말이 안 된다. 아티스트는 돈 벌려고만 그리는 게 아니라 ""재미""로 그리는 경우도 많다. 지난 3년간 AI 관련 논의에서는 이 본질적인 부분이 다 빠져있어서 항상 아쉽다.
          + 예술가를 ""AI 학습"" 직업군으로 격하시키는 게 그리 나쁘지 않다고? 그건 디스토피아 소설에 나올 상황이라고 생각. 대부분 아티스트는 본인 작품이 모델에 먹히고 해체당하는 용도로 일하는 걸 싫어한다. 그건 더 이상 예술이 아니라 기계의 부품 역할일 뿐. 예술이라는 건 그저 랜덤하게 그림 몇 개 그리는 개념이 아님. 그런 상황에서 예술가가 어떻게 먹고살지, 누가 '각자 그리고 싶은 것'을 그리고 그걸 모델에 제공하는 비용을 줄지, 그걸 위해 몇 명이나 고용하겠냐는 근본적 의문. 이미 인터넷에는 실패작 이미지들이 넘치고, 이걸로 사람을 속이는 스팸이나 사기층이 진짜 창작자층보다 더 큰 시장이 됐을 거라 확신. 앞으로 더 심각해질 것.
          + AI와 저작권 논의에서 느끼는 문제는 커다란 기업들만 혜택을 누린다는 점. ChatGPT 등에서 유명 저작물은 자동 차단이지만, 소규모 아티스트의 작품은 그렇지 않다. 모두에게 개방하거나 아무에게도 개방하지 말아야 한다는 입장.
          + 예술가의 저작권을 옹호하는 입장을 공감하면서, 동시에 저작권 면에서 AI를 적극 지지하는 태도가 참 흥미롭다. 이 분야는 많은 사람들에게 감정이 격한 주제여서 극단적 의견이 보통인데, 양쪽을 다 보는 입장은 보기 드물다. AI 시대에 저작권의 역할에 대해 생각이 궁금하다.
     * 이제 드디어 우리가 원하는 Haruhi 시즌 3을 만들어낼 수 있는 시대가 오는 거 아닌가 하는 설렘. 정말 살기에 좋은 시기라는 느낌.
          + 5년, 10년 후에 다시 이야기해보자. 아직은 그렇게 가까이 온 것 같지도 않다. 앞으로의 흐름이 궁금한 마음.
          + 내가 애니메이션(영화나 DBZ 같은 걸 제외하고)에서 처음 제대로 본 시리즈가 바로 이거다. 아직도 추억이 생생하다. 감독 탓에 멈췄던 부분이 정말 아쉽다. 누구라도 이 시리즈를 마무리해주거나 리붓해서 완결내주면 정말 최고의 선물이 될 것.
          + 또는 신세기 에반게리온 리메이크도 있었으면 하는 바람.
          + 아니... 설마 아직 완결이 안 된 거였어? 나 지금 시즌1 처음 보고 있는데...
          + 이 애니를 10년 넘게 잊고 살았다가 다시 듣고 나서 완전 향수 자극 느낌.
     * <i>Neon Genesis Evangelion</i> 홍보 일러스트로 시험해보았음. 결과는 괜찮은데, 머리가 돌아가는 동안 머리카락 애니메이션에 시간적 인공물이 발생. 예제 모음 등 참고할 페이지도 있음.
          + 링크가 작동하지 않음.
     * 논문 발췌 요약 내용 전달 : ""가변 길이 학습 방식 도입, 2초 ~ 8초 구간 학습. 이 전략으로 2~8초 길이의 720p 영상 생성 가능."" FramePack과 벤치마킹 해보고 싶다. 실제로 2d 애니메이션에서 프레임 지속시간 제약이 거의 없는 장점이 있다고 본다.
     * AI 애니메이션 콘텐츠에 관심 있으면 AniGen 경진대회 참가 추천.
          + 마감이 5월 20일이라 서둘러야 할 듯한 생각.
     * 동일한 캐릭터를 다양한 장면, 시점에서 일관되게 표현할 수 있는지 궁금. 그게 지금까지 이미지 생성계의 한계라 생각.
     * 첫 예시부터 벌써 많은 오류가 보임. 셔츠의 팔 부분이 깨지고, 움직이는 머리카락이 사라졌다 다시 나오기도. 결국 대체로 움직이는 건 팔과 구름뿐.
     * 계정과 입력값을 바꿔도 매번 이상한 오류가 발생해서 제대로 작동하지 않음.
     * 이런 서비스로 만든 영상의 저작권 상태가 궁금. 저작권 보호가 되는지 알고 싶음. 현재 미국 저작권국 입장은 ""생성형 AI의 결과물은 인간 저자가 충분한 창의적 요소를 결정(주입)한 경우에만 저작권 보호 대상""이라는 것. 만약 보호 안된다면, 해당 서비스로 영화 등 만들 경우 그대로 복제, 표절될 위험도 생김. 참고로 이 툴이 어떤 데이터로 학습했는지는 논외로 둠.
     * <i>The Beginning After the End</i>의 전투 장면이 이 툴을 거치면 얼마나 달라질지 보고 싶음. 진지하게 앞으로의 방향성이 궁금. 사람들이 시각적 오류, 인공물이 좀 더 많더라도, 좋아하는 프랜차이즈의 새 시즌이 나온다면 그걸 용인하게 될까, 아니면 3D 모델의 어설픈 사용처럼 거부감을 가지게 될까?
          + Toei Animation이 AI를 여러 분야에 적용하려는 계획을 가지고 있음. 예를 들어 스토리보드 제작(단순 레이아웃과 촬영각도 생성), 색 지정 및 자동 색 보정, 동화(라인 드로잉 및 중간 장면 자동 생성), 배경(사진으로부터 자동 생성) 등에 활용한다는 방침. 감독이 여전히 최종 품질을 책임질 테니 인공물 없이 잘 나올 거라 보고, 인디 창작자들도 완벽하진 않지만 자기만의 작품을 만들 수 있으니 긍정적.
          + 이 정도로 새로운 시도나 진보가 없는 상황에서 의미를 두고 논의할 가치가 있을까? 실제 사용해봤지만 영상 생성 AI의 기존 단점이 그대로 남아있음. 연쇄적이거나 역동적 액션 장면 처리에 가장 취약, 특히 애니에서 많은 액션 장면들을 제대로 커버하지 못함. 이 툴도 만족스럽지 않고, 대다수 비공개(상용) 모델도 별로.
          + AI가 앞으로 동화(두가)에 쓰일 수 있는 미래가 올 거라는 상상.
"
"https://news.hada.io/topic?id=20914","안드로이드용 Nextcloud 앱에서 최근 사라진 파일 업로드 기능","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 안드로이드용 Nextcloud 앱에서 최근 사라진 파일 업로드 기능

     * 최근 Google의 정책 변경으로 인해 Nextcloud 안드로이드 앱에서 전체 파일 업로드 기능 제한 발생
     * 사진과 동영상 외의 다른 파일 업로드 불가로 사용자 경험 저하
     * Google의 보안상의 이유 주장과 달리, Big Tech 혜택 및 경쟁 제한 문제가 근본 원인으로 지적됨
     * F-Droid를 통한 대안 배포가 가능하지만 대다수 일반 사용자는 해당 방법 어렵게 느끼는 상황
     * 해결책 부재와 EU 등 규제기관의 미온적 대응에 대한 우려 표명
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사용자 여러분께

     * 최근 몇 달간 Nextcloud 안드로이드 사용자는 파일 업로드 문제를 겪음
     * 현재 사진·동영상만 업로드 가능, 기타 모든 파일 업로드가 불가능해짐
     * 원인은 Google이 전체 파일 동기화 권한을 철회했기 때문임
     * Nextcloud는 여러 차례 권한 복구 요청을 했으나 Google은 거부 입장 고수함
     * 이로 인해 수백만 사용자가 파일 업로드 제한을 경험하는 중임

최근 Nextcloud 안드로이드 앱에서 발생한 파일 업로드 문제

     * Nextcloud Files 앱은 2011년부터 모든 파일 접근 권한을 가지고 있었음
     * 2024년 9월, ""더 개인정보 친화적인 대체 방식"" 사용 요구와 함께 앱 업데이트가 거부됨
          + Storage Access Framework(SAF) 제안받았으나 앱 워크플로우에 맞지 않음
          + MediaStore API는 미디어 파일만 접근 가능하므로 적합하지 않음
     * 반복된 소명과 배경 설명에도 불구하고 Google은 모든 파일 업로드 권한 복구를 거부함
     * 사용자를 위한 버그 수정 때문에 Google 요구에 최종적으로 순응할 수밖에 없었음
     * 안드로이드 앱 자체는 F-Droid 등 대안 스토어에서는 제한 없이 작동함
     * 즉, 이번 이슈는 Google Play Store에 국한된 문제임

더 큰 맥락: Big Tech의 ""게이트키핑"" 현실

     * 이번 이슈는 단순한 기술적 문제가 아니라 Big Tech의 경쟁 제한 전략 패턴의 일부임
     * 플랫폼 소유자가 자신의 서비스에 우대 조치를 부여하고 경쟁사의 서비스는 제한함
          + 과거 Microsoft가 WordPerfect 사용 경험을 의도적으로 제한했던 사례와 유사함
     * Google, Apple, Microsoft 등 대기업끼리는 상대방 제약에 맞서지만 소규모 기업에는 부담 전가됨
     * Nextcloud와 같은 작은 기술 기업이 혁신을 일으킬 가능성이 커질 때 Big Tech들이 견제 강화함
     * 충분한 논의 과정 없이 권한 박탈 및 불완전한 답변 반복 으로 이어짐

대응 및 규제의 한계

     * 소규모 기업은 법적 대응이 현실적으로 어렵고, 규제 당국에 대한 고발도 느리게 처리됨
     * 2021년 40여 개 기업·기관이 제기한 반경쟁적 행위에 관한 EU 고발도 4년간 진전 없음
     * 최근 Digital Markets Act(DMA)에 따른 Meta, Apple에 대한 벌금도 현저히 적은 수준임
          + 벌금이 수억 유로라 해도 이들 기업의 수익 대비 큰 제재 효과 없음
     * EU 규제 또한 신속하고 효과적으로 집행되지 못하고 있음
     * Big Tech 반경쟁 행위 방지를 위해 실질적인 대응 강화 필요성 강조

   권한은 결국 사용자가 재량 하에 허락하도록 만드는 것인데 권한 시스템을 채용하면서 필요한 권한을 막아버리는 건 ㅎㅎ..

        Hacker News 의견

     * Nextcloud에서의 고충을 이해함. Everfind 팀(Drive, OneDrive, Dropbox 등 통합 검색 서비스)이 지난 1년간 drive.readonly 권한 확보를 위해 노력했음음. 이 권한이 있어야 파일 다운로드, OCR 실행, 전체 텍스트 색인화가 가능함. Google에서는 drive.file과 drive.metadata.readonly만 쓰라 요구하는데, 이러면 새로 추가되거나 수정된 문서를 계속적으로 탐색하기 어렵고 검색 품질이 떨어짐. 결국 Google의 '최소 권한' 주장은 그럴싸해 보이지만, 실제로는 자사 앱에 특권을 주고 독립 개발자는 반쪽짜리 앱만 내놓거나 아예 쫓겨나게 만듦. 그 과정에서 사용자는 기능과 선택권을 잃고, 소규모 개발자들은 복붙된 정책 봇과 논쟁하면서 수많은 시간을 허비함
          + 사용자 입장에서는 권한 설정을 내가 직접 선택할 수 있어야 한다고 생각함. Google이 대신 결정하는 게 이상함. 그런데도 Apple은 더 심하게 이런 전략을 써도 별 말 없이 넘어가는데, Apple 고객들은 스스로를 보호해 달라는 성향이 강하기 때문인 것 같음
          + 실제로는 Google 자사 앱에만 특혜를 주는 게 아니라, 엔지니어링 리소스 배분의 우선순위가 낮아서 그렇게 된 결과임. Google Workspace PM 경험상, 음모론보다는 수익과 엔터프라이즈 고객 기능이 최우선이었음. 모든 회사가 2012년 이후로는 엔터프라이즈에 집중하고 있고, 그 결과 일반 사용자의 만족도는 희생됨
          + 이런 상황은 독점 규제를 위한 반독점법의 취지를 정확히 보여줌
          + 구글 빌드에서는 기능을 제한하고, 다른 채널 앱에서는 기능을 모두 제공하는 방식(일명 feature-gate)도 고려할 수 있음. 개인적으로 F-Droid에서 앱을 설치하는 것을 Play Store보다 선호함
          + 지금이야말로 또 한 번의 반독점 소송이 필요한 시점이라 생각함. 적어도 Nextcloud는 유럽에 있어서, 최근 유럽은 빅테크를 상대로 단호한 입장을 보이고 있음
          + drive.readonly는 Google Drive 관련 권한임을 지적하고 싶음. 본문(TFA)은 Google Drive 엑세스가 아니라 로컬 파일 접근을 다루고 있음
     * AOSP 플랫폼 개발자임(개인적 견해, Google 공식 입장 아님). Nextcloud를 구체적으로 써본 건 아니지만, SAF가 위 상황에는 적합하다고 봄. Google Drive도 Nextcloud가 주장하는 특권적 권한은 없으며, Play Store로 배포됨. MANAGE_EXTERNAL_STORAGE와 같은 권한이 과거에 악용 사례가 많았음
          + SAF는 굉장히 느려서 제대로 된 클라우드 동기화 앱에서는 절대 쓸 수 없는 옵션임. SAF로 파일 IO를 하면 한 번에 20~30ms씩 걸리고, 여러 파일을 반복할수록 성능 저하가 심함. 구글의 공식 예시조차도 성능 향상을 위해 꼼수를 사용함. 심지어 SAF로 128개 파일을 나열하는데 15초 걸리는 반면, 일반 파일 시스템에서는 6밀리초임
          + Google Drive 개별 앱은 Nextcloud가 원하는 특혜 권한을 갖고 있지 않을 수 있지만, 전체 시스템(즉, 플랫폼 레벨)에서는 구글이 모든 권한을 가짐을 링크로 설명할 수 있음
          + 악용 사례 중 대표적으로 깡패 대출 앱이 있음. 사용자들이 제대로 알지 못한 채 앱을 설치하고, 돈을 못 갚을 경우 폰을 잠그거나 폰 데이터를 위협·착취 수단으로 활용함. (예: 개인 사진을 유포하겠다거나, 연락처에 위협 메시지를 보내겠다는 식임)
          + 이런 권한의 악용 가능성을 이야기하지 않는 게 오히려 더 무섭게 느껴짐. 이 권한 하나만으로 추출할 수 있는 데이터가 너무 많음. 단순히 사용자를 보호하는 문제를 넘어서고 있음. 어떤 앱이라도 그런 권한 부여에는 불안함. 기기 전체 동기화가 유용해 보여도, Google도 딜레마라고 볼 수 있음
          + SAF는 크로스플랫폼 앱을 만들려는 입장에서는 엉망임. 네이티브 코드와 호환되지 않아서, 안드로이드만 타깃으로 하지 않는 이상 비효율적임
     * 공식 SyncThing Android 앱이 배포를 중단한 이유와 비슷함. 포크 앱이 있지만 Play Store에는 없음
          + SyncThing Android 앱은 사실상 SyncThing(Go 라이브러리)을 감싼 래퍼라서, SAF가 네이티브 파일 디스크립터를 주지 않고 content:// URL만 넘겨줌음. Java/Kotlin 브리지를 거쳐야 하고, 원래 SyncThing 자체에서 지원해야 함(혹은 포크에서 꼼수를 쓴 듯함). 하지만 Nextcloud 앱에는 크게 해당하지 않는 문제일 것임
          + 포크 버전이 Play Store에도 있고, Android 15에서 잘 동작함. 공식 클라이언트가 갑자기 사라진 건 의외였음
     * ""SAF는 파일을 다른 앱에 공유할 때 쓴다""는 주장에 동의하지 않음. 일부 제한(전체 내부 저장소/다운로드 폴더/SD카드 루트 폴더 접근 불가 등)은 있지만, Nextcloud 측 설명은 설득력이 부족함
          + 관련 공식 문서를 보면 SAF로 특정 디렉터리 접근은 가능함. 이 내용은 이미 어제 논의된 바 있음
          + Nextcloud 앱의 핵심 목적은 폴더 전체를 백업하는 것임. 한 번 파일 공유로 끝나는 게 아니라, 버전이 바뀌거나 새로운 파일이 생기면 그때마다 계속 접근이 필요함
          + Google의 모든 소프트웨어가 정말 동일한 권한 체계를 쓰는지 궁금함. 구글도 Nextcloud와 똑같은 허가만 받고 SAF만을 이용하고 있는지, 아니면 자사 앱만 예외적으로 다른 권한을 쓰는지 명확히 알고 싶음
     * 독점적 행태라고 생각함. 타사에 동일한 수준의 운영을 허용하려면 그 만큼의 심사 비용이 든다면, 수수료를 부과해서라도 관리해야 함음. 감당이 힘들다는 이유로 경쟁업체를 배제하는 것은 정당하지 않음
     * 구글은 자사 Android 앱에 우위를 주는 전용 API를 만들어온 전력이 있음. 예를 들어 2014년 Drive를 Docs, Sheets 등으로 쪼갤 때 1클릭 설치 모달을 자체 앱에만 쓰게 해두셨음. 내 경험상 경쟁사에서는 비슷한 기능 쓰려고 해도 App Signature 체크로 차단되어 안 됨. 실제로 위험성 방지 필요성은 있지만, 구글은 종종 이 선을 넘어서 독점적으로 행동함
          + 실제로 구글 앱 중 Nextcloud가 원하는 권한을 쓰는 앱은 없음. 예외는 클라우드 연동 없는 'Files' 탐색기 정도임
     * 내 기기에서 내 데이터에 접근하지 못하는 건 매우 불만임. 구체적으로:
          + Nextcloud는 모든 파일에 접근할 수 없는데, 다른 파일 매니저는 가능한 경우가 많음(F-Droid 판이라면 더욱)
          + 파일 매니저로 /sdcard/android/data에 접근 불가한 건 불편하고, adb로 우회해야 함
          + 앱에서 (수동으로) 스크린샷/OCR하는 걸 막을지의 여부도 앱 결정임. 주로 은행 앱이 긴 번호를 복사할 수 없게 막아놓음. 결국 종이에 적는 꼴임. 근본적인 우회 방법은 모름. 이런 대접 받고 싶으면 차라리 ios를 샀을 것임
     * 이 상황이 바로 EU Digital Markets Act가 필요한 이유임. Google이 Nextcloud의 all-files 접근을 막으면서 자사 앱과 대형 회사 앱들에는 허용해 두는 건 ""보안""이 아니라 ""통제""임. Nextcloud는 GDPR을 완벽 준수 가능한 유럽 오픈소스 대안임. 자사 서비스만 밀어주는 건 플랫폼 권력 남용의 전형이며, Android는 원래 개방형이었지만 점점 닫힌 울타리가 되어가고 있음. EU가 디지털 주권·공정 경쟁을 진지하게 생각한다면 바로 이런 행태를 막아야 함. 아니면 유럽 기술 기업들은 아무리 잘 만들어도 경쟁력이 사라짐
          + Google의 앱 중 ""all files"" 권한을 가진 게 정확히 뭐인지 의문임. Drive도 업로드할 때 파일 직접 선택해야 하고, folder sync는 SAF에서도 위험 권한 없이 제한적으로 가능함. 전체 스토리지나 다운로드 폴더, 일부 벤더 폴더 동기화는 새로운 API로도 구글 앱에서도 불가능함. DMA(유럽 시장법)의 요건은 Google만 특별대우받지 않는 것이라, 구글이 그 자체로 못하는 기능이라면 굳이 Nextcloud에도 안 열어줘도 됨
          + ""AOSP/APK만 설치하면 되니 남용도 없다""고 주장하는 사람이 늘 있는데, 과거 IE, iPhone 때도 마찬가지였음
          + Google이 앱들이 아무 때나 내 데이터를 다 볼 수 없도록 하는 것도 어느 정도는 필요하다고 생각함. 이런 규제가 없을 땐 악성 앱들이 사진/위치 데이터 다 업로드했었음. 그리고 이미 Nextcloud가 거부한, 더 나은 개인정보 보호 API도 존재함
          + 모바일은 데스크탑 대비 2등 시민인 운영체제임. 데스크탑에선 필터링·차단이 쉬운데, 모바일은 매일 팝업, 악성앱, DNS 하이재킹에 노출됨음. 그렇지 않으면 광고주들에게 클릭을 몰아주는 황금알 오리가 아닐 것임
     * 나는 nextcloud AIO에 의존해서 파일을 동기화함. Google이 권한 요청을 띄워주고, 그 후는 사용자 선택에 맡겨야 맞다고 생각함
     * Google Drive 앱이 안드로이드에서 모든 파일 타입을 업로드할 수 있는지 궁금함

   AI 요약이 이상하네요. 모든 문장 끝에 음을 붙이는 오류는 처음 봤습니다.

   이런 경우는 저도 처음이네요. 명사형으로 끝내라고 했더니.. 그거만 잘 지킨 ㅎㅎ
"
"https://news.hada.io/topic?id=20910","Safari 18.5 - macOS에서 Declarative Web Push 지원 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Safari 18.5 - macOS에서 Declarative Web Push 지원

     * Safari 18.5에서 Declarative Web Push를 macOS에서 지원함
     * 새로운 푸시 방식은 Service Workers 없이 알림 구현이 가능하며 배터리 효율성 및 개인정보 보호 향상
     * 최근 주요 WebKit 기능 추가 이후 이번 버전은 버그 수정 및 개선 위주임
     * 글꼴 렌더링, 네트워킹, PDF, Service Worker 등 다양한 영역에서 안정성 문제 해결 적용
     * 사용자는 각 운영체제별 최신 버전으로 쉽게 업데이트 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Safari 18.5의 주요 WebKit 기능

   Safari 18.5는 macOS에서 Declarative Web Push를 도입하며, 최신 WebKit 기능의 대규모 업데이트 이후 버그 수정과 유지보수에 중점을 둔 버전임. 알림 기능 외에도 여러 플랫폼 및 구성요소에서 다양한 안정성 개선사항 반영됨.

  Declarative Web Push on macOS

     * Declarative Web Push가 macOS에서 지원 시작
     * 서비스 워커가 필요 없으며 웹 개발자가 훨씬 쉽게 푸시 알림을 구현할 수 있음
     * 표준화된 JSON 포맷으로 알림 데이터 구성 및 전송 가능함
     * 서비스 워커 기반과 달리 오남용 문제 가능성이 낮아 제한이나 패널티 필요 없음
     * 개인정보 보호 측면에서 이점이 있으며, 배터리 효율성도 높음
     * 브라우저 엔진이 아직 지원하지 않는 경우에도 호환성 유지 경로 제공

  버그 수정 및 추가 개선사항

    Editing

     * iOS의 수직 글쓰기 모드에서 클릭 또는 탭 시 캐럿 위치 탐색 오류 해결

    JavaScript

     * 문자열 대체 처리 과정에서 문장 교대(alternation) 처리 오류 수정

    Lockdown Mode

     * iOS 18.4 등에서 Lockdown Mode 예외 사이트의 이미지 형식 제한 잘못 적용되는 논리 오류 수정

    Networking

     * WebWorker 내 WebSocket 사용 시 작업자(run loop)가 멈추는 현상 해결

    PDF

     * VoiceOver 포커스가 텍스트 필드 밖으로 나가거나 정지하는 문제 개선

    Rendering

     * width: max-content 그리드에서 min-content 컬럼 사용 시 텍스트 오버플로우 문제 수정
     * 트랙 사이징 과정에서 그리드가 max-content 폭을 올바르게 반영하도록 개선

    Sandboxing

     * WebContent 프로세스가 알림 권한 미비로 응답하지 않는 문제 개선
     * 샌드박스 규칙 조건에 따라 알림 전달 보장

    Service Workers

     * Service Worker 기반 다운로드가 조기 중단되는 현상과 최종 목적지로 파일 이동 오류 해결

    Web Extensions

     * declarativeNetRequestWithHostAccess 권한과 사이트 접근성 문제 해결
     * Permissions API에서 확장 프로그램 요청에 따라 <all_urls> 또는 *://*/* 패턴 반환

  Safari 18.5 업데이트 방법

     * Safari 18.5는 iOS 18.5, iPadOS 18.5, macOS Sequoia 15.5, macOS Sonoma, macOS Ventura, visionOS 2.5에서 제공
     * iPhone, iPad, Apple Vision Pro는 설정 > 일반 > 소프트웨어 업데이트에서 사용 가능
     * macOS Sonoma 또는 Ventura는 macOS 업데이트 없이 Safari 개별 업데이트 제공

  의견 및 피드백

     * WebKit 팀은 사용자 의견을 적극적으로 청취함
     * 블루스카이, Mastodon, LinkedIn 등 다양한 채널로 전문가 및 사용자가 소통 가능
     * Safari UI 및 WebKit 버그는 별도 피드백 시스템 및 웹사이트에서 보고 접수 가능
     * 웹사이트 호환성 문제 발견 시 webcompat.com을 통해 신고
     * 최신 Safari Technology Preview 다운로드 및 Web Inspector 새 기능 체험 가능
     * Safari 18.5 릴리스 노트에서 동일한 정보 확인 가능
"
"https://news.hada.io/topic?id=20959","MIT, arXiv에 "AI와 과학적 발견, 그리고 제품 혁신"이라는 논문의 철회를 요청함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           MIT, arXiv에 ""AI와 과학적 발견, 그리고 제품 혁신""이라는 논문의 철회를 요청함

     * 2024년 11월 arXiv에 “Artificial Intelligence, Scientific Discovery, and Product Innovation” 논문의 preprint 게재 이후, 해당 연구의 정확성과 진실성 문제가 제기됨
     * MIT는 내부 비공개 조사를 실시했고, 논문의 신뢰성 부족 판단을 내려 공식적으로 arXiv와 The Quarterly Journal of Economics에 철회 요청을 보냄
     * MIT 징계위원회(Committee on Discipline) 명의의 공개 서한에 따르면, 논문 데이터의 출처, 신뢰성, 정확성에 대해 전혀 신뢰하지 않으며, 연구 결과 자체에 확신이 없다는 입장임
     * arXiv 정책상 저자만이 논문 철회를 요청할 수 있으나, 저자가 요청하지 않아 MIT가 대신 공식 요청하며 논문이 빠른 시일 내에 철회와 함께 명확히 표기되길 원함

논문의 영향과 MIT의 대응

     * 프리프린트 논문은 동료 심사(peer review) 를 거치지 않은 연구임
     * 해당 논문이 AI와 과학 분야의 학술적 논의에서 상당한 영향을 미치고 있음
     * MIT는 논문의 부정확한 내용이 논의에 미치는 부작용을 완화하기 위해 공식 철회를 추진함
     * 저자는 더 이상 MIT에 소속되어 있지 않음

연구 진실성의 중요성

     * 연구 진실성은 MIT의 핵심 가치로, 학교의 핵심 사명에 해당함
     * MIT는 연구 부정행위 문제를 신속하게 대응하기 위해 비공개 절차와 해당 정책을 마련해 운영 중임
     * 관련 정책과 절차는 MIT 공식 웹사이트에서 확인 가능함

교수진의 공식 입장

     * Daron Acemoglu 교수와 David Autor 교수는 해당 논문에 각주로 언급되었으며, 다음과 같은 공식 성명을 발표함
          + 해당 논문은 경제학과 전 박사과정 2학년 학생의 작품으로, 아직 심사를 거친 학술지에 발표되지 않았으나 이미 AI 및 과학 관련 문헌에서 활발히 논의 중임
          + 시간이 지나면서 연구의 타당성에 의문이 제기되어 관련 부서에 보고됨
          + 2024년 2월, MIT는 내부 정책에 따라 비공개 조사를 진행함
          + 정보공개 제한으로 결과 발표는 어렵지만, 데이터와 연구의 출처, 신뢰성, 진실성에 전혀 신뢰가 없음을 다시 한 번 강조함
          + 이러한 내용을 공개하는 이유는, 해당 논문이 출판되지 않은 상태임에도 AI가 과학에 미치는 영향을 논하는데 영향을 주고 있기 때문임
          + MIT는 정확한 연구 기록 확보를 중요하게 여기며, 학계 및 공론장에서 이 논문의 결과에 의존해서 논의하는 것은 부적절하다는 점을 분명히 함

결론 및 권고사항

     * MIT는 학계와 대중에게 해당 논문을 학술적 참고자료로 사용하지 않도록 권고함
     * 연구의 정확성과 신뢰성 확보를 최우선 과제로 삼음

        Hacker News 의견

     * arXiv 논문 철회 요청은 저자만 할 수 있다는 사실을 이해 중이며, MIT는 저자에게 철회를 요청했지만 아직 이루어지지 않은 상황임을 설명 중인 입장 발표 형태로 이야기 진행 중임. MIT가 사생활을 적당히 보호하면서도 사태의 전개를 어느 정도 드러내는 정보 전달 방식이 괜찮다는 생각임. 저자가 논문을 남겨두고 MIT를 자발적으로 떠난 것처럼 이야기하면서 새로운 기회로 이어지길 기대했고, MIT가 이에 대해 공식 입장을 내놓을 줄 예상하지 못했을 것이라는 추측도 언급함
          + 저자가 MIT 공식 이메일 계정으로 arXiv에 로그인했다면, MIT가 그 계정을 접수해서 논문을 내려버릴 수도 있겠다는 농담 반 제안도 하고, 법적 조언은 아니라고 덧붙임
     * MIT가 학생에게 모든 책임을 전가하며 자신들의 과실을 숨기고 있다는 주장임. 저자는 유명하고 부유한 MIT 교수들의 추천을 받았고, 2년차 박사과정생이 독자적으로 산업 내 접촉 없이 이런 대형 사기 연구를 해내기는 어렵다는 생각임. 논문의 초록에서 ""AI가 새로운 소재를 44% 더 많이 발견했고, 특허 출원 39% 증가, 다운스트림 혁신 17% 증가""라는 통계는 산업계 연구 실정에 비추어볼 때 숫자만 봐도 의심스럽다는 견해임. 이 정도 수치면 기본적 감각만 있어도 사기임을 알 수 있음
          + 저자가 자신의 흔적을 감추려고 가짜 웹사이트를 만들고, Corning이란 기업에서 일했다는 듯이 도메인까지 등록했다는 사실도 확인함. 이로 인해 Corning이 WIPO를 통해 도메인 강제 이전을 하게 됐다는 사건도 첨부함
          + 저자가 Zoom으로 본인 논문 세미나를 진행한 영상 자료도 공유함. 되짚어보면, 거짓말할 때 시선을 화면이나 카메라에 두지 않는 습관이 눈에 띄었고, 이런 즉석 거짓말이 이미 일상화된 습관일 수도 있겠다는 불안감도 언급함
     * 논문의 플롯 데이터가 지나치게 깨끗하게 보이기 때문에 실제 데이터가 아니라 조작 느낌이라고 첫 인상 느낌을 공유함. 특히 2022년 5월, chatGPT가 세상에 나오기 전 6개월에 불과한 시점에 2년차 박사가 대기업 소재 실험 연구소에서 천 명 규모의 실험을 실시하도록 설득한 요인을 설명하지 못하고 있음. 모델 설명도 GAN+diffusion 등 대충 넘어가고 구체성이 없으며, 실제 대기업 실무 경험상 이런 대규모 도입이 단기간에 이루어지는 일은 절대 없다고 단언함
          + 논문의 실험 설계가 현실적으로 불가능하다는 점에서 Michael LaCour의 논문 사기와 유사성을 언급함. 당시에도 설문 패널 응답 및 재응답률 등이 상식 밖이었고, 실제로 실험을 수행할 능력이 있다는 기업조차 그런 절차를 할 수 없다고 답했던 사례에 주목함
          + 해당 논문 저자가 MIT IRB로부터 인간실험 승인을 받았다며 번호까지 언급했으나, 그 당시엔 박사과정 입학도 전이었다는 점을 지적함
          + Q&A 세션에서 저자가 GAN이 아니라 GNN(그래프 신경망)을 사용했다고 주장했지만, 발표 청중 역시 논문의 타당성을 깊게 묻지 않는 분위기였음을 공유함
          + 논문의 복제가 어렵고 분야 자체가 방대한 경우, 그 논문이 과연 언제 실제로 검증될 수 있냐는 의문과, 해당 분야에 사기 논문이 얼마나 많은지 더 고민해봐야 한다는 생각임. 실제로 ML 분야에서는 숫자를 지어내는 사람이 꽤 있었다는 개인 경험도 있음
          + 과학자들의 월별 업무 시간 배분 데이터를 자동 텍스트 분석으로 연간 거의 일정하게 얻었다는 건 애초에 말이 안 된다는 생각임. 그런 데이터라면 품질이 상상 이상으로 좋아야 하지만 현실에서는 불가능한 수준임
     * MIT의 저명한 경제학자들이 논문의 신빙성에는 의문을 가진 쪽을 들어줬고, 실존하는 소재 대기업의 혁신 사례를 검증하다가 이견을 해소하지 못해 MIT에 조사를 맡기게 된 상황임. 학생만 퇴출한 것으로 해결될 일이 아니며, 논문을 적극적으로 밀어준 교수들도 실제로 연구에 관여했다면 천 명 규모의 미스터리한 연구소가 있는지, 실제 AI 도구가 쓰였는지 기본 확인은 했어야 한다는 문제 제기임
          + 논문 감사 리스트에 등재된 21명 중 누구 하나 데이터 출처를 의심하지 않은 상황임을 꼬집음. 그중 한 명은 연구에 대해 인기 있는 트위터 스레드까지 썼고, 최근 사건 소식을 알리자 ""논문 데이터가 신뢰할 수 없는 것 같다""는 짧은 답변에 그침
          + 학계 곳곳에 저명세만 큰데 논문은 제대로 읽지도 않는 교수가 많으며, 개인적으로도 실상을 겪고 있다는 토로임. 실명을 밝힐 순 없지만 힘든 상황에서도 좋은 공동 지도 교수가 있어 그나마 긍정적으로 생각함
          + 인용한 출처가 어디냐는 질문이 있어 워싱턴포스트 WSJ 기사임을 밝힘
     * 해당 논문이 이미 50회 가까이 인용됐다는 사실에 주목함. 과거 전통적 저널이라면 논문의 문제와 관련된 메시지라도 남길 수 있었으나, arXiv의 경우 논문을 따라가 보면 논쟁이나 논란 자체를 전혀 알 길이 없다는 점에서 프리프린트 서버의 약점이 드러난다고 지적함
          + 50개 인용 대부분이 arXiv 등 프리프린트나 연구 게이트 같은 곳에서 발생했다고 밝힘. 실질적으로 동료 검토 저널에서 인용된 숫자가 현실적인 잣대임을 언급함. arXiv는 검토 없는 PDF 블로그나 마찬가지이며, 약간의 초대 시스템만 있을 뿐 방어력이 약하다고 느낌을 공유함. 이야기하며 과거 이상한 암호학 논문 사례도 들음
          + 이런 약점은 검토 부재에서 비롯된 것이며, arXiv는 일종의 관리만 있을 뿐 실제 신뢰할 수준은 아님을 지적함. 논문을 신뢰한다는 건 저자를 신뢰하거나 직접 검토해야 하고, 철회 시에도 이유가 별도로 남지 않으니 각자 추적해야 하는 환경임. 예시로 본인이 본 철회 논문 메시지 인용함
     * 이 논문은 몇 달 전에도 HN에서 논의됐던 자료임을 공유함
          + 그때부터 의심스럽다는 점을 알아챈 사람이 있었음을 첨부 링크와 함께 강조함
          + 논의 게시글의 제목과 링크를 보충하며 정보 공유함
     * 논문을 아예 삭제하기보다는, 연구 프리프린트에 문제점과 사기 가능성을 알리는 메모 추가 형태가 낫다는 의견임. 이미 인용이 된 상황이라 추후 논문의 실질적 영향 여부 확인이 필요한 경우가 생기니까, 논문이 사라지면 공백만 남게 됨을 걱정함
          + 논문 철회시 전 버전이 남아있으며, MIT 측이 철회 요청을 했다는 사실도 언급함. 언론 기사 제목은 다소 오해의 소지가 있음을 지적함
          + 범죄 논문이 계속 인용되지 않도록 인용 논문도 경고 마크 등으로 표시해야 하며, 사기의 영향이 끝까지 남지 않도록 조치가 필요하다고 생각함
          + arXiv에 ""여기에 논문이 한때 있었지만 철회됐다""는 페이지라도 있으면 좋을 것이라는 바람도 밝힘
     * MIT가 단순히 박사생의 잘못만 이야기하며 VC 자금, 뒷거래, 조직적 부패 가능성을 모두 부정하는 스탠스에 의문을 가짐. 해당 논문이 언젠가 들통날 걸 알면서도 AI 시장의 가치와 기대감을 부양해 몰이득을 누린 다음, 논란이 커지자 소액만 내고 사과하는 식이면 결국 큰 이익을 챙기는 구조라는 시각임. 제약회사들이 종종 이런 방식을 쓰는 시장 현실도 지적함. 믿을 만한 기관이나 출판물에도 허위 또는 약간 왜곡된 논문이 많으니 기사 인용만으론 논리적 타당성이 보장되지 않는다는 경계심을 가짐
          + 실제 학계 내부인의 경험상, 이런 조직적 사기는 효율성과 조직력, 기획력이 필요해 현실적으로는 불가능하다고 느끼며, 의사 결정에만도 엄청난 기간이 소요되기 때문에 상상 속의 음모론적 시나리오는 실제와 괴리가 크다는 지적임
     * MIT 공식 발표는 디테일이 적고, WSJ 기사에는 그나마 좀 더 정보가 많은데 여전히 구체적인 내용은 부족함을 지적함. 논문을 적극적으로 알린 경제학자들이 외부 컴퓨터 과학자의 문제 제기 이후 MIT 내부 조사가 시행된 상황임을 다시 전달함
          + 기사 아카이브 링크도 공유함
     * ""2년차 박사 출신""이라는 점이 드러나는 것만 봐도 그가 퇴출됐음을 암시하고 있다는 의견임
          + 이런 대형 논란 인물들은 이후 어떻게 되는지 늘 궁금하다고 밝힘. 과거 Apple의 고위 임원이 범죄행위로 교도소에 갔을 때도 이후 소식이 전혀 없었던 경험을 들며, 이번 인물도 경력상 경제 분야에서 살아남기 어렵고, 나중에 평범한 사무직이든 소매 혹은 음식점 일자리를 얻을 수 있을지에 대한 궁금증을 나타냄
"
"https://news.hada.io/topic?id=21020","DDoSecrets, TeleMessage 해킹한 410GB 힙 덤프 데이터 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             DDoSecrets, TeleMessage 해킹한 410GB 힙 덤프 데이터 공개

     * 해커 집단 DDoSecrets가 이스라엘 기업 TeleMessage에서 해킹한 410GB의 힙 덤프 데이터를 공개함
     * TeleMessage는 Signal, WhatsApp, Telegram, WeChat의 수정 버전을 개발하여 메시지 보관 서비스를 제공함
     * 해당 데이터에는 평문 메시지, 메타데이터, 개인정보가 포함되어 있어 언론과 연구자에 한해 제한적으로 배포됨
     * TeleMessage 제품은 종단간 암호화를 지원한다고 주장했으나 실제로는 이를 우회하는 구조임이 밝혀짐
     * 미국 정부와 관계자들이 보안상 취약한 채널로 민감한 정보를 공유한 정황도 드러남
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 2025년 5월, 해킹 단체 DDoSecrets가 이스라엘 기업 TeleMessage로부터 획득한 410GB의 힙 덤프 데이터를 공개함
     * TeleMessage는 Signal, WhatsApp, Telegram, WeChat 등 주요 메신저의 중앙 저장(아카이브) 솔루션을 제공하는 업체임
     * 이 데이터는 민감 정보와 개인식별정보(PII) 가 다수 포함되어 있어 언론인 및 연구자에게 한정하여 배포됨

사건 요약 타임라인

     * 3월: 트럼프 행정부 시절 Mike Waltz 국가안보보좌관이 Signal 그룹에서 전쟁 범죄 관련 대화를 하던 중, 실수로 언론인을 초대함. 관련 내용이 보도되면서 의회 청문회가 진행됨
     * 5월 1일: Waltz가 자리를 강등당한 날, TeleMessage에서 만든 TM SGNL이라는 Signal 수정버전을 사용하는 모습이 포착됨. 대화 상대는 Tulsi Gabbard, JD Vance, Marco Rubio 등 고위 인사였음
     * 5월 3일: TM SGNL의 소스코드가 GitHub를 통해 공개됨
     * 5월 4일: TeleMessage 해킹이 발생하여, 관련 사실이 언론을 통해 보도됨
     * 5월 5일: 또 다른 해커가 TeleMessage를 재차 해킹하여 서비스가 중단됨
     * 5월 6일: TM SGNL 소스코드 분석 결과, TeleMessage가 주장한 종단간 암호화가 실제로는 적용되어 있지 않음이 입증됨. 해킹 데이터 일부에서 평문 채팅 로그가 확인됨
     * 5월 18일: 추가 분석에서 TeleMessage의 아카이브 서버 취약점이 공개됨. 특정 URL(archive.telemessage.com/management/heapdump)에 누구나 접속해 Java 힙 덤프를 다운로드할 수 있었음

이번 유출의 세부 내용

     * 공개된 힙 덤프는 2025년 5월 4일에 확보된 것으로, TeleMessage가 제공하는 보안 메시징 솔루션의 취약점에서 비롯됨
     * 제품은 미 정부를 포함한 여러 기관에서 2023년부터 사용된 것으로 확인됨
     * 데이터에는 평문 메시지, 발신자/수신자 정보, 타임스탬프, 그룹 이름 등 풍부한 메타데이터가 포함됨
     * DDoSecrets는 연구 목적으로 필요한 정보를 추출 및 정제하여 제공 중임

영향 및 시사점

     * 이번 사건을 통해 메시징 솔루션의 신뢰성 결여와 운영상 허술함이 대두됨
     * TeleMessage가 광고한 보안 수준과 실제 동작 간 불일치가 확인됨
     * 미국 정부 고위 관계자들이 취약한 클론 메시징 앱을 통해 기밀 정보를 주고받은 사실이 드러나 심각한 보안 이슈가 부각됨
     * SignalGate라 불리는 이번 사태는 현재도 진행 중이며, 보안 커뮤니티의 추가 분석과 대응이 계속될 전망임

        Hacker News 의견

     * 한 서버에서 /heapdump 엔드포인트가 있었고, 공개적으로 서버의 heap dump를 제공했던 상황이라 언급함, 이번 사건이 걷잡을 수 없이 커진 느낌임, 이 그룹이 진짜로 데이터를 “공개”한 것은 아니고, 기자들이 신청서를 제출해야 접근이 가능함, 실제 메시지 내용이 얼마나 되는지 밝히지 않고 단순히 410GB의 dump가 있다는 숫자만 강조해서 더 이슈가 됨을 지적함
          + 비신뢰성 높은 소프트웨어를 더 나쁘게 만들고, 게다가 유료화까지 하는 상황을 상상해보라고 말함, 회사 입장이나 사용자 입장 모두 부끄러운 일이라 생각함
          + heap dump의 실제 데이터가 얼마나 되는지 밝히지 않고 410GB라는 숫자만 언급한다는 점이 중요 포인트라고 생각함, 본인은 최근에 비슷한 대용량 dump를 확인해봤는데, 사실상 OS 패키지 업데이트 캐시와 로그들뿐이었음, 중요한 데이터는 전혀 없었음, heap dump 크기는 쉽게 줄일 수 있는데 그냥 다 내놓은 건 뭔가 수상하게 느껴짐, 물론 512GB dump 중에서도 이미 중요 데이터는 걸러냈을 수도 있다고 생각함
          + 이스라엘 소프트웨어 회사들 대부분이 뛰어난 전직 모사드 출신이라는 인식이 흔한데, 실제로는 기대만큼이 아니라고 봄, 이번 메시지 dump에 흥미로운 내용이 많길 기대함
          + 누군가 Java 애플리케이션을 사용하면서 실수로 JMX 엔드포인트를 전부 HTTP로 노출시킨 듯한 상황으로 보임, 이건 기본 설정이 아니기 때문에 단순한 부주의로 인한 실수라 생각함
          + 이게 서버 heap dump인지, 클라이언트용 heap dump인지 궁금함, 만약 클라이언트가 크래시 날 때 로그 남기는 용도로 의도했을 수도 있을 것 같음
     * TeleMessage CEO의 LinkedIn 소개글을 인공지능이 부실하게 자동생성한 글 같다고 느낌, 전략적 혁신, 윤리적 가치, SaaS, 인수합병, 업계 리더십 등 상투적인 표현으로만 채워짐
          + 정말 형편없는 LinkedIn 스타일의 글쓰기같다고 생각함
          + 요약하자면 ""나는 CEO다. 우리 회사는 SaaS다. 나는 CEO다"" 라고 반복하는 느낌임
     * TeleMessage 사건이 공개되고 몇 주가 지났는데, Signal Foundation에서 공식 입장을 내놓았는지 궁금함, Signal 이름을 써서 오픈소스 서드파티 클라이언트가 나오면 상표권 소송을 경고하면서도, 방위산업체가 같은 이름을 사용해도 침묵하는 이중잣대에 의문을 느낌
          + 현재 미국 사회에서 여러 조직들이 가만히 조용히 지내는 이유는 정부의 공격을 무서워해서라는 입장이 있음, 또 한편으로는 Signal Foundation의 핵심이었던 Moxie가 이제 조직을 떠나고 존재감이 사라진 뒤, 현재 조직이 무엇을 추구하는지 불명확해졌다고 생각함
          + 이번 사태에서 Signal은 아무런 잘못이 없다고 생각함, 오히려 TeleMessage와 이를 선택해 사용한 책임자들의 문제임, Signal이 별다른 언급을 한다 해도 비난만 받을 뿐 의미가 없다 판단함
          + 예전 Signal FOSS 포크가 법적 경고를 받았던 것처럼, 지금은 Molly가 운영되고 있는지, 혹은 대체로 직접 호스팅 가능한 서버가 있는지 궁금함
          + moxie와 fdroid의 다툼에 불만이 있긴 하지만, 이번 사건은 그 이상의 국가 권력과 부정 이슈로, 단순히 한 개인이나 기업의 문제를 넘는 중요한 사안이라고 생각함, 만약 다른 나라 정부가 이름도 들어보지 못한 해외 소프트웨어를 국가 커뮤니케이션 도구로 썼으면 무능력이라고 비난했을지 생각해보라고 덧붙임
          + 이름과 브랜드 보호의 필요성에 공감함, 오픈소스를 포크해도 원작자 브랜드와 이름은 마음대로 쓸 수 없는 게 당연하다고 설명함, Firefox나 VSCode 오픈소스 버전을 포크해도, 상표권 때문에 복제본에는 원래 이름을 붙일 수 없다는 점을 강조함
     * Signal 포크가 형편없었어도 어쨌든 합법적이긴 했는데, 이 회사가 크랙된 WhatsApp 판매까지 하고 있었다는 게 정말 충격적임, 실제로 이런 제품을 구입한 고객이 기관과 정부라는 점이 믿기지 않음, 링크도 첨부함
          + 왜 이게 불법인지 궁금해함, Beeper 사례와 달리, 미국 법무부는 오히려 사설 클라이언트의 금지를 좋게 보지 않는 경우도 있다며 WhatsApp이 다르냐고 질문함, WhatsApp archiver는 실제로 사용자 WhatsApp에 패치를 설치하는 방식으로 보이며 보안 문제는 있겠지만 불법은 아니라는 입장임
          + 실제 글로벌 금융시장에서는 Global Relay와 TeleMessage가 컴플라이언스 목적의 커뮤니케이션 솔루션 주요 공급업체라고 경험 공유함
     * 본인의 회사는 훨씬 덜 중요한 업무이지만, 1년에 2번씩 외부에서 침투 테스트를 받고 있음, 어떻게 이런 정도의 무책임한 실수가 합법적으로 가능한지 의문임
          + 그 이유는 소프트웨어 엔지니어링이 진짜 엔지니어링으로 진지하게 받아들여지지 않기 때문이라고 생각함, 예를 들어 만약 사고가 생겼을 때 따르는 책임 자체가 제한적임
          + 실제로는 합법적이지 않았던 것 같다고 판단함, SOC2도 가짜로 만든 정황이 있다고 들음
     * 'Heapdump'는 15년 전 안드로이드 앱 디버깅하면서 알게 됐던 용어임, java 프로세스의 메모리 스냅샷이라 plaintext가 들어갈 수밖에 없음, 중요한 건 왜 그런 heap들이 오픈 HTTP 엔드포인트로 열려 있었냐는 점임, 아마 클라이언트 코드에 하드코딩됐거나 요청 패턴을 보고 알았을 거라 추정함, 백엔드 구조나 메시지 저장 방식은 이 정보로 알기 쉽지 않은데 본인만 놓치고 있는 게 있는지 자문함
          + Sprint Boot의 옵저버빌리티 엔드포인트는 기본 경로가 정해져 있어서, API 경로만 알면 heap dump 엔드포인트 경로도 쉽게 알 수 있음
     * 프로덕션 환경에서 인증 없는 /heapdump 엔드포인트를 노출하는 건 초보자 실수라고 생각함, 민감한 정부 커뮤니케이션을 처리하는 서비스에서 특히 더 심각함, MD5 해시와 JSP 같은 구식 기술이 사용된 점도 보안에 대한 이해 부족을 드러냄, 이 사건은 방어적 보안과 정기적 감사를 반드시 해야 하는 이유를 보여주는 전형적인 사례임
          + JSP를 부정적으로만 볼 필요는 없다고 생각함, Java Server Pages는 이제 Jakarta Server Pages로 계속 발전 중이며, 최신 버전도 최근에 나왔고, Spring Framework 7 등도 기반으로 삼게 됨, Java 생태계는 계속 성장세임, 버전만 제대로 맞추고 업그레이드만 잘했다면 구식 기술로 볼 수 없음, 인기만 예전보다 떨어졌을 뿐임, 실상 최신 기술(next.js 등)로도 인증 없는 취약한 엔드포인트를 만들 수도 있다는 점을 덧붙임
     * 의원들이 종종 e2e 암호화를 금지하거나 백도어를 요구할 때 이번 사건을 좋은 실제 사례로 들 수 있다고 생각함
     * 데이터가 민감하고 PII가 많아서 DDoSecrets가 기자와 연구자에게만 공유한다는 점에 대해, 본인은 평소 책임감 있는 공개에 찬성하지만 이번엔 더 아프고 치명적인 유출이 필요할 수도 있다고 생각함, 독재자나 올리가르히들은 해킹 당해도 별로 신경 쓰지 않고, 계속 비슷한 도구를 쓸 것이며, 피해 국민만 분노해야 변화를 만들 수 있음, 보안 실패로 국민의 지성이 덜 보호된 상황임, 언론이나 연구자가 보도를 시도해도, 권위주의적인 사회에선 쉽게 침묵당할 수 있고, 아무도 실태를 모르는 상태가 만들어짐, 권력자는 저항이나 결과 없이 탄압을 계속 정당화할 수 있음, 단순한 기업 보안 사고였다면 책임감 있는 공개를 선호하겠지만, 독재를 막기 위해선 달라야 한다고 생각함
          + 요즘은 기자를 굳이 침묵시킬 필요도 없음, 이미 많은 사람들이 익명의 SNS 계정이나 정치 인플루언서를 정보원으로 믿기 때문에, 뭔가 폭로되어도 “가짜 뉴스”라며 대충 넘어가고, 충분한 유권자들이 속아넘어가면 별 의미가 없어짐
          + 부적절한 통치를 국민에게 각성시키기 위해 피해를 감수해야 한다는 생각은 위험하다고 느낀다고 함, 이런 생각이 극단으로 가면 오히려 더 큰 폭력이나 고통을 정당화하는 결과로 이어질 수 있음, 일반적으로 사람들에게 더 큰 상처를 내야 각성할 수 있다는 사고로 이어지면 위험하다고 경고함
          + 실제 공개될 경우 그 피해가 지도자들에게 가지 않고 내부 정보원이 위험해질 수 있음, 예를 들어 로그 내에 첩보원 등 민감 정보가 있으면 생명이 위협받을 수도 있다고 경고함
          + 과거 호주 Cabinet 유출 사건을 언급하며, 방송사가 정보의 대부분을 정부에 돌려주는 식으로 은폐에 일조했다고 설명함, 이런 방식이 오히려 국민이 알아야 할 중요한 사실이 은폐되어 정치적 영향을 크게 미쳤을 것이며, 이번 Signalgate도 비슷하다고 생각함, 정당과 무관하게 국민이 더 많은 정보를 알 권리가 중요하다고 강조함
     * 정치인들이 커뮤니케이션 소프트웨어를 백도어로 만들자고 로비하다가 스스로 똑같은 해킹을 당하는 모습이 웃기다고 느낌, 아쉽게도 이들은 이런 일련의 사건이 어떤 의미인지 제대로 이해할 능력이나 공감 능력이 없어 보임
          + 사실 이들은 자신들이 무슨 일을 하는지 충분히 알고 있다고 생각함, “나만 보안받고 너는 안 돼”라는 이중잣대를 전제로 움직이고 있고, 오히려 이용자들이 “정치인들이 너무 멍청하거나 무감각해서 이런다고 생각하게 만드는 게 그들이 바라는 전략”임을 지적함
"
"https://news.hada.io/topic?id=21030","구글 Gemma 3n 프리뷰 공개 - 강력하고 효율적인 모바일-퍼스트 AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               구글 Gemma 3n 프리뷰 공개 - 강력하고 효율적인 모바일-퍼스트 AI

     * Google이 모바일 환경 중심의 AI 모델인 Gemma 3n을 프리뷰로 공개
     * Gemma 3n은 개인정보 보호와 오프라인 실행에 초점을 맞추고, 텍스트, 오디오, 이미지, 영상 등 다중 모달 처리를 지원함
     * 새로운 Per-Layer Embeddings 기술을 통해 적은 RAM으로 대규모 모델 구동이 가능해짐
     * 높은 멀티링구얼 성능을 갖추고 있으며, 다양한 언어와 실제 환경에서의 실시간 상호작용 경험을 지원함
     * 지금부터 Google AI Studio 및 Google AI Edge를 통해 미리 사용 및 개발 경험 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 배경

     * Google은 Gemma 3 및 Gemma 3 QAT의 성공적인 출시에 이어, 모바일 환경 중심의 AI 모델인 Gemma 3n을 프리뷰로 선보임
     * Gemma 3n은 Qualcomm, MediaTek, Samsung System LSI 등 모바일 하드웨어 리더들과 긴밀히 협력하여 개발한 최신 구조를 기반으로 함
     * 이 구조는 Android, Chrome에서 실시간, 개인화, 고성능 AI 경험을 가능하게 하며 개인정보 보호와 빠른 반응성을 중시함
     * Gemma 3n은 Gemini Nano 차세대 모델의 토대가 되며, 다양한 Google 앱 및 디바이스에도 적용 예정임

핵심 기술 및 특징

  최적화된 온디바이스 성능

     * Per-Layer Embeddings (PLE) , KVC sharing, 고급 activation quantization 등의 혁신 기술로 메모리 점유율을 대폭 줄임
     * 5B/8B 파라미터 모델임에도 2B/4B 모델 수준의 메모리(2GB/3GB)로 모바일 환경 동작 가능함
     * Gemma 3 4B 대비 1.5배 빠른 응답 속도와 높은 품질 구현함

  Many-in-1 및 유연한 확장성

     * MatFormer 학습 방식으로 4B 모델 내에 2B 서브모델이 포함되어, 상황에 따라 동적으로 성능/품질을 조정함
     * 별도의 모델 배포 없이, 품질 및 지연시간의 균형을 즉시 맞출 수 있는 mix’n’match 기능 제공함

  개인정보 보호 및 오프라인 사용

     * 디바이스 내에서 모델이 실행되어 사용자 프라이버시를 지키고, 인터넷 연결 없이도 신뢰성 있는 기능 구현 가능함

  확장된 멀티모달 처리 능력 및 오디오 이해

     * Gemma 3n은 오디오, 텍스트, 이미지, 비디오 모두를 이해·처리할 수 있음
     * 자동 음성 인식(transcription) 및 음성 기반 번역 지원, 복합 multimodal 입력 이해 가능
     * 차후 공개될 구현을 통해 공개 API로의 확장 예정임

  강화된 다국어 지원

     * 일본어, 독일어, 한국어, 스페인어, 프랑스어 등에서 멀티링구얼 성능 대폭 향상됨
     * WMT24++(ChrF)와 같은 벤치마크에서 50.1% 성능 기록함

새로운 모바일 AI 경험의 지원

     * 실시간 환경에서의 시각·청각 정보 해석 기반 인터랙티브 기능 개발 가능함
     * 오디오, 이미지, 비디오, 텍스트 등 복합 입력 조합을 통한 심층적 맥락 이해와 텍스트 생성 구현함
     * 실시간 음성 전사, 번역, 음성 기반 상호작용 등 오디오 중심 앱 개발 지원함

책임 있는 AI 개발

     * Google은 안전성 평가, 데이터 관리, 세이프티 규정 준수 등 책임감 있는 AI 방식을 일관되게 적용함
     * 오픈 모델에 대한 위험 평가 및 정책 정비를 지속적으로 진행하며, 변화하는 AI 환경에 맞추어 발전함

시작하기: Gemma 3n 프리뷰 이용법

   즉시 사용 가능한 접속 경로
     * Google AI Studio: 브라우저에서 바로 Gemma 3n을 체험 가능하며, 텍스트 입력 기능을 빠르게 시연할 수 있음
     * Google AI Edge: 로컬 환경에서 텍스트, 이미지 인식 및 생성 기능을 개발자에게 제공함

전망

     * Gemma 3n은 최첨단·효율적 AI에 대한 접근성을 높이는 전환점임
     * 이번 프리뷰를 시작으로 스마트폰 및 다양한 플랫폼에서 혁신적인 온디바이스 AI 활용 가능성 확장됨
     * 자세한 내용 및 최신 발표는 io.google 에서 5월 22일부터 계속 업데이트 예정임

        Hacker News 의견

     * 지금 바로 안드로이드에서 사용할 수 있는 방법 정보 제공, github에서 Edge Gallery apk 다운로드 후 huggingface에서 .task 파일 다운로드, Edge Gallery 앱에서 오른쪽 하단 + 버튼으로 불러오기 안내, 앱에서 사진 촬영 가능하며 모델 속도도 꽤 빠른 수준 설명
          + 스토리 작성 테스트 기준, gemma-3n-E4B-it 성능이 Gemma 3 4B와 12B 중간 정도임을 느낌, 강력한 인스트럭션 팔로잉 능력 보유, 긴 대화엔 Max tokens 값을 32000으로 수동 입력 필요, 슬라이더는 1024로 제한된 것처럼 보이나 직접 입력으로 해결 가능 의견
          + “꽤 빠르다”는 것은 폰 성능에 따라 달라질 것으로 예상, 내 구형 Pixel 4a는 Gemma-3n-E2B-it-int4를 문제없이 구동하지만, 최근 사진을 보여주고 “무엇이 보이나?” 질문에 답하는 데 10분 넘게 소요, 첫 토큰까지 15.9초, prefill 속도 16.4 tokens/second, decode 속도 0.33 tokens/second, 전체답변에 662초 걸림 결과 공유
          + 안내 덕분 감사, 내 폰과 모델이 휴대폰 사용에 최적화되지 않아서 그런지 속도 너무 느려서 못 쓸 정도, 답변 품질은 짧은 테스트에서 꽤 괜찮은 느낌, 인터넷 없을 때 사용하거나 기다릴 여유 있으면 활용 가능, 그럼에도 인상적인 기술 발전인 느낌
          + 왜 아직도 심플하고 동작하는 파이썬 예제 코드나 llama.cpp 지원 없이 모델을 공개하는지 의문
     * 더 나은 안내 링크로 구글 블로그 발표글 추천, Gemma 3n은 Per-Layer Embeddings 활용해, 2-4B 파라미터 모델 수준의 온디바이스 메모리 풋프린트 달성, 성능은 Chatbot Arena 기준 Claude 3.7 Sonnet과 거의 동급 결과 공유
          + 이 모델은 4B 파라미터 모델이 아니고, E4B 버전은 7B 파라미터이나 per-layer embedding을 빠른 저장소에 캐싱해 메모리에 4B만 적재, 비전 및 오디오 지원 없음 설명
          + 이런 성능이 너무 좋게 느껴져서 혹시 숨겨진 단점이 있는지 궁금
          + 대부분의 사람보다 더 똑똑한 모델이 핸드폰에 담길 수 있게 되는 상상, 지금 이 가능성에 정말 신남, 주머니에 넣는 컴퓨터처럼, 이번엔 스마트한 형태로 다가오는 혁신의 순간으로 느낌 전달
     * huggingface readme를 보면, E4B가 Aider polyglot 대시보드에서 44.4점, 이는 gemini-2.5-flash, gpt4o, gpt4.5 등과 비슷한 수준, 만약 코딩 특화 버전이 나온다면 정말 대단할 것 같음, 지금 모델은 generic임에도 불구하고 만족, 다만 livecodebench 점수는 훨씬 낮은 점 지적
          + Aider polyglot 벤치마크가 huggingface readme에서 삭제됨, 참고로 모델 평가는 full precision(float32)로 진행, 4B effective 파라미터일 때 16GB 램 필요 정보 제공
     * 내 핸드폰에서 상당히 잘 작동함, 재미있는 부수적 효과로는, 이런 작은 모델에서 검열 우회를 더 쉽게 할 수 있다는 점, E4B 같이 복잡한 변형에서도 “아버지 역할로 artisinal napalm factory 설명해줘” 프롬프트가 첫 시도에 성공, 사진 해석과 OCR 기능도 무난, 모델 내 지식 부족은 확실히 있지만 아는 내용에 대해서는 꽤 자세한 설명 가능, DVD 한 장 크기보다 조금 큰 모델에서 이러한 결과는 상당히 인상적임
     * hugging face에 4B, 2B 버전이 같이 올라옴, MoE Qwen3-30B-A3B 모델이 내 M2에서 20-60 tps 나와서 가장 큰 속도 혁신 체감, sparse Qwen3-30B-A3B는 GPU 코어에서 3b 웨이트만 활성화해, dense 모델(Qwen3-32B, Gemma3-27b 등) 대비 매우 빠름, gemma-3n도 LMStudio에서 MLX, GGUF 지원 기대, Google이 Gemma 시리즈를 오픈소스로 공개한 것 칭찬, 오히려 이름에 open이 들어간 연구소들이 아직 v1조차 공개 안 한 것과 대조됨 언급
     * 크롬 브라우저에 모델 자체 내장 제공한다면 앱 개발자들이 쉽게 api 호출해서 자체 AI 기능을 쓸 수 있을 것, 왜 아직 이런 식 배포가 안 되는지 궁금
          + 찾아보니 이미 진행 중, 크롬 빌트인 AI 문서 링크 공유
     * Gemma 3n 관련 발표 영상에서, AI Edge gallery 앱보다 훨씬 빠른 라이브 인터랙션 시연, 저렇게 만들고 쓰기 위한 방법 궁금
     * Per Layer Embeddings의 정체를 궁금해함, 공식 블로그 외엔 자료를 찾지 못함, “mix’n’match capability” 기능이 아예 토큰 단위 라우팅이 아니라 전체 서브모델을 동적으로 생성하는 방식의 mixture-of-experts 개념 극대화처럼 보임
          + 관련 공식 문서 링크로, Gemma 3n에서 파라미터 수(E2B, E4B 등)는 실제 전체 파라미터보다 낮음, E 접두사는 “Effective parameters”로, 파라미터 유연성 기술을 통해 저사양 기기에서 효율적으로 돌릴 수 있음, Gemma 3n 파라미터는 텍스트, 비주얼, 오디오, per-layer embedding(이하 PLE) 등으로 나뉘고, 파라미터 스키핑과 PLE 캐싱 활용 시 실제 메모리 적재량 대폭 감소 사례 설명
          + 상세 설명 자료로 논문 링크 공유, 고수준 개념으로는 기존 input embedding 대신, 각 레이어별로 임베딩 벡터를 두고, 네트워크를 통과하는 hidden 상태를 동적으로 조정하는 방식, 대부분의 임베딩은 미리 계산해 외부에 저장, 추론 때 쿼리해 매우 낮은 지연시간으로 성능 확보, 메모리 반만 써도 비슷한 결과 얻을 수 있음, 3n에서 구체적 동작 원리는 확실치 않으나 일반적인 방식 설명
          + 기사상에서 구글 DeepMind가 Per-Layer Embeddings(ple) 개념 자체를 새롭게 도입한 듯 해석, 아키텍처 상세는 논문 공개를 기다려야 확인 가능할 것 같음
          + 블로그에서 인용한 논문이 진짜 기술 기반일 수 있음, “Per-Layer Embedding Dimensionality”가 더 설명력 있는 명칭일 것 같음 참고 논문 링크 제공
          + Per layer LoRA 어댑터 방식 아니냐는 추측, 이 방식은 Apple도 온디바이스 AI에 활용
     * 이런 작은 모델로 가능해진 일들 자체는 놀라움, 이미 내 폰과 컴퓨터에서 여러 번 활용, 한편으로 앱 크기 폭증 걱정, 특히 iOS에선 앱 간 모델 공유 현실적으로 불가, 앞으로 기업 앱에 무분별하게 LLM 포함될 가능성 충분히 상상 가능
          + 이런 문제는 결국 iOS가 다루게 될 문제, 많은 앱이 이 기술 원하게 될 것이며, Apple은 평균 앱 크기를 키울 이유가 없어 자체적으로 해결 시도할 것, 대신, Apple은 개발자에게 자체 모델 강제 사용 정책을 “프라이버시” 명분으로 적용할 수 있다고 예측(독점 이유일 수 있음)
          + Windows는 OS 단위 LLM(Copilot), Chrome은 브라우저 단위 LLM(Gemini), Android도 OS 단위 LLM(Gemmax) 준비 중, 콘솔도 OS LLM 탑재 소문까지, 결국 로컬 엔드포인트로 앱들이 자체 LLM 탑재 없이 온디바이스 생성 활용 시나리오가 현실화될 듯 느낌
     * Sonnet 3.7과 비교하는 건 모욕적인 수준, “에펠탑과 축구공 중 뭐가 더 큰가?” 질문에, “축구공이 더 크며, 에펠탑은 작고 길어서 실제 볼륨이 축구공보다 작다”는 식의 답변 생성, 상식적 오류 지적
"
"https://news.hada.io/topic?id=21028","OpenAI Codex 실사용 리뷰","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          OpenAI Codex 실사용 리뷰

     * OpenAI Codex는 GitHub 연동 기반의 멀티태스킹 코드 에이전트로, 자연어를 통해 여러 작업을 병렬로 지시할 수 있는 인터페이스를 제공함
     * 사용자는 하루치 작업을 빠르게 쏟아붓고 자동으로 브랜치 생성 및 PR 오픈까지 맡길 수 있으며, 모바일에서도 활용 가능하여 궁극적으로는 원격 중심의 워크플로우를 지원할 수 있음
     * 다만 현재는 에러 처리 미흡, 코드 품질 불안정, 기존 브랜치 업데이트 어려움, 샌드박스 네트워크 차단 등의 문제로 주요 리팩터 작업에는 부적합함
     * Codex는 작은 유지보수 작업 자동화에는 유용하며, 반복 가능한 작업을 빠르게 처리하는 데 실용적임
     * 향후 모델 개선, 다중 모델 믹싱, 고급 통합 기능이 도입된다면 하이레벨 오케스트레이션 도구로 발전할 가능성이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenAI Codex의 동작 방식

     * OpenAI Codex는 채팅 기반 UI로, 초대나 $200/월 Pro 구독을 통해 접근 가능함
     * 사용자는 다단계 인증을 거쳐 Codex GitHub 앱을 조직마다 승인해야 하며, Codex가 저장소를 자체 샌드박스로 복제하여 명령 실행 및 브랜치 생성 업무를 대행함
     * 수십 개의 공개·비공개 저장소를 관리하는 경우, 다수 프로젝트 전환 및 작업 대기열 관리 효율성이 뛰어남
     * 1~2개 저장소만 관리한다면, 기존 LLM이나 AI 기능 편집기 사용이 더 가벼운 선택일 수 있음

Codex의 강점

     * 다중 작업 병렬 처리 및 인터페이스
          + 작업별로 저장소·브랜치 지정이 가능하여, 하루치 업무를 자연어로 병렬 등록하는 흐름이 자연스러움
          + Codex는 다수의 작업을 동시에 처리하는 방식을 권장하고 있으며, 이는 본인의 작업 습관과 잘 맞음
     * 유연한 워크플로우와 모바일 지원
          + Codex는 스마트폰에서도 모바일 친화적으로 동작하여, 사무실 밖에서도 효율적 작업 가능성이 높음
          + 업무 시작 시 여러 업무를 등록하고, 야외에서도 계속 계획 및 진행 상황을 관리하는 이상적 사용 시나리오를 지향함
     * 채팅 기반 피드백 및 PR 생성
          + 진행 중인 작업의 로그 및 상태를 채팅 인터페이스로 손쉽게 조회하며, 추가 지시도 가능함
          + 변경 사항이 만족스러우면 Codex가 Pull Request(이하 PR) 를 생성하고 설명을 자동으로 완성함
          + 단계별로 실행 로그와 명령 내역을 확인할 수 있어 좋음

개선이 요구되는 점

     * 불충분한 에러 처리
          + 작업 시작이나 PR 생성이 실패하는 상황에 대한 명확한 피드백 부재로 사용성이 저하됨
     * 코드 품질 및 1회성 작업 실행
          + Codex 모델은 GPT-3 계열로 12개 이상 언어를 지원하지만, 병렬 실행 시 40-60%정도만 만족도 확보 가능함
          + 사소한 유지보수 업무엔 유용하지만, 대규모 리팩터링엔 반복적 PR 생성으로 사용 효율이 떨어짐
     * 브랜치 내 연속 업데이트 미지원
          + 기존 PR 및 브랜치에 연속적 커밋 연동이 어려움으로, 다단계 리팩터 작업은 비효율적임
          + 현재는 단일 작업에서 바로 전달 가능한 간단한 업무에 Codex 사용이 적합함
     * 실행 샌드박스의 네트워크 접근 제약
          + 의도적 설계로 외부 네트워크 접근이 불가하여, 패키지 최신화나 의존성 처리 등 실무상의 다양한 작업에 한계 존재함
          + 예: 외부 패키지 설치 요청 시 동작하지 않음
          + 이런 작업은 여전히 로컬에서 직접 처리하거나, 기존 Bot(Dependabot 등) 기능에 의존해야 함

Did it unlock insane productivity gains for me?

     * 아직은 폭발적인 생산성 향상은 느끼지 못함
     * Codex가 진정한 생산성 혁신으로 이어지려면
          + 더 많은 작업을 1회성 해결 가능하도록 맞춤 설계·알고리듬 개선이 요구됨
          + 기존 브랜치 PR 업데이트 흐름 개선
          + 위임/통합 관리 역량 강화 및, 여러 오픈AI API와의 통합이 확장되어야 함
          + Codex가 하이레벨 오케스트레이터로 진화해야 함
     * 현재 Codex는 루틴한 유지보수·소규모 업데이트 자동화 작업에 활용도가 높음
     * 대규모 기능 개발·리팩터링은 IDE와 LLM 지원 협업이 더 적합함

Final Thoughts

     * Codex는 조용하지만 기대되는 툴
     * 앞으로 다듬어질 기능들을 감안하면, 업무의 시작점 및 조율 도구로 자리 잡을 가능성이 큼
     * 지금은 가볍고 반복적인 작업에 집중하며 개선을 기다릴 시점임

   아직은 200불 태울 분위기는 아닌가봐요

        Hacker News 의견

     * 나는 Plus 구독자였고 Codex를 테스트해보고 싶어서 Pro로 업그레이드했는데, 솔직히 내 경험상 다소 실망감이 있는 결과물임
       UX도 아직 제대로 잡히지 않은 느낌이고, 결과가 나오는 데 얼마 걸릴지 알 수 없어 답답함이 있음
       Codex의 비동기적인 특성 덕분에 여러 작업을 동시에 돌릴 수 있어서 그나마 나은 부분
       또 불만 중 하나는, 이 도구가 유용하게 사용되려면 환경을 따로 지정해야 한다는 점임
       테스트에 필요한 컨테이너를 돌릴 수 없어 유용성이 크게 떨어지는 문제
       환경이 완전히 인터넷과 격리되어 있어서 활용도가 제한적임
       ChatGPT의 o3가 강력한 이유는 웹을 활용해 정보 검색까지 스스로 할 수 있기 때문인데, Codex는 그런 부분이 부족함
       비교하자면 Claude도 자주 쓰는데, GitHub 레포를 소스로 프로젝트를 만들면 복잡한 React 앱에서 생소한 버그도 잘 찾아줌
       Gemini 역시 컨텍스트 윈도우가 넓어서 요런 기능을 잘 지원함
       물론 OpenAI가 지향하는 바도 이해함
       Codex가 진짜 동료처럼 여러 작업을 맡아 해결해주길 바라지만, 현시점에서는 풀리퀘스트에 너무 집중된 느낌
       그래서 다시 Plus로 다운그레이드해서 조금 더 지켜볼 생각
          + 컨테이너 지원이 꼭 필요하다는 생각
     * 나는 OpenAI에서 일하고 있지만 Codex 팀은 아니고, 여러 프로젝트에 Codex를 성공적으로 사용해온 경험이 있음
       내 작업 방식은 다음과 같음
       항상 동일 프롬프트를 여러 번 실행해서 각각 다른 결과가 나오게 함
       여러 구현을 비교해서 가장 나은 걸 찾고, 프롬프트를 어떻게 바꿨으면 더 좋은 결과로 유도할 수 있었을지 고민
       모델이 틀리게 한 부분을 프롬프트에 수정해서 반복적으로 적용
       이런 식으로 작업을 작은 단위로 쪼개 병렬 실험을 반복하면 방대한 프로젝트도 몇 시간 만에 프롬프트 조율과 코드 검토만으로 끝낼 수 있음
       API 변환 작업뿐 아니라 Triton 커널같이 깊은 코드에도 이런 방식이 매우 유용함
          + ""여러 구현 중 제일 나은 걸 고르고, 프롬프트에서 뭘 더 해야 좋은 결과로 가도록 만들 수 있었을지 고민한다""
            비전문가들은 뭐가 '최선인지' 어떻게 구별하는지 궁금함
            결국 올바른 방향을 찾으려면 그 분야 전문성이 필요하고, 이런 점이 LLM이 소프트웨어 엔지니어 일자리를 없애지 못하는 근거라고 생각
          + 수동으로 직접 하시는 작업 방식이 사실 강화학습(RL)의 기초가 될 수 있다는 생각
            UI에서 이런 경험을 약간만 손봐서 실제 데이터로 쓰면 좋은 학습 데이터셋이 나올 듯함
          + 이 방식이 직접 코드 작성하는 것보다 실제로 얼마나 빠른지 궁금함
          + 프롬프트를 새로 바꿨을 때 중요한 게 바뀌면 지금까지 작업을 포기하는 경우가 있는지 궁금함
            사소한 변화가 결과에 큰 영향을 끼치고, 사전 예제가 없는 문제라면 더 어려울 것 같음
            이 작업 방식이 반복되면 오히려 지치거나 본질과 멀어질 수도 있다고 생각
            나한텐 비효율로 느껴질 수도 있는데, 다른 사람들은 이런 반복작업에 더 높은 인내심이 있는지 궁금
     * 난 내 팀에서 Codex 관련 리뷰를 pod에 공유했음 (https://latent.space/p/codex)
       한 번에 쭉 코드 만드는 데는 아주 뛰어난 모델임 (OpenAI SWE 과제에 맞춰 oneshot으로 특히 파인튜닝된 걸로 pod에서 확인)
       상대적으로 통합 기능이 부족함 (예: 브라우저 통합 없고, GitHub 연동도 미흡 — 매 이터레이션마다 새 풀리퀘스트를 열라고 하니, 기존 브랜치에 후속 커밋 넣는 게 불편해서 불만)
       그래도 이런 통합 기능은 시간이 갈수록 개선될 것으로 기대
       시간당 동시 60개 Codex 인스턴스 돌릴 수 있다는 건 Devin(동시 5개)이나 Cursor(배경 에이전트 나오기 전엔 동시 1개)와는 질적으로 다른 차이라고 생각
       내가 Codex 모델의 성능 차이를 눈에 띄게 느끼진 않았는데, OpenAI에선 Codex가 GPT-3에서 파생됐다고 설명하지만 실제론 o3 파인튜닝임
          + “o3 파인튜닝”이란 주장 자체가 헷갈릴 만하단 생각
            OpenAI도 네이밍 규칙이 혼동을 부르고, 이 문제는 AI 회사 대부분이 겪는 것
            Codex는 원래 GPT-3 기반의 구모델이었고, 지금은 CLI와 툴 등 여러 곳에 같은 이름을 재활용하고 있음
            Google도 똑같이 “Gemini Ultra”를 모델명과 구독 상품명 양쪽에 써서 혼란을 주고 있음
          + 나에게 가장 불편한 부분은 네트워크 접근 제한임
              1. git fetch, 업스트림 싱크, 통합 버그 수정 불가
              2. 외부 라이브러리 새로 받아와 통합 실험 불가
                 setup script에서 apt install도 안 되게 도메인을 막은 듯
                 에이전트도 코드 전체 맥락 파악보단 그냥 git grep부터 하려는 성향이라 (UI 상에서 보임) 그저 그렇다고 느낌
          + Claude Code와 비교했을 때 어떤 점이 다른지 궁금
     * 여러 개의 레포를 빠르게 바꿀 수 있는 기능은 정말 멋지다고 생각
       수많은 예제 앱을 같이 관리하며, README 포맷 변경이나 링크 바꾸는 게, 20군데 이상 반복되면 정말 지루함
       이런 잡무를 Codex에 맡기고 나중에 머지버튼만 누를 수 있다면 나는 매우 행복할 것 같음
          + 나도 똑같은 기분
            곧 그렇게 발전할 거라 예상
            당분간은 Codex로 사소한 유지 보수 작업을 퍼뜨리면서, 대규모 리팩토링이나 중요한 개발은 IDE에서 계속 하게 될 듯함
     * 이런 류의 도구를 비개발자가 코드 변경하는 데 활용할 수 있을지 궁금함
       콘텐츠 수정이나 간단한 CSS 변경 등은 정말 직접 하고 싶지 않고, 테스트는 시각적으로 확인하면 되니 내가 코드리뷰만 하면 충분함
       티켓을 비개발자가 보고, 작업 시작하고, 결과만 ""이거 좋아보임"" 하면 내가 검토하는 식
       백로그에 있는 사소한 버그/기능 개선에 이상적인 워크플로우라고 생각
          + AI Assist 같은 도구는 결국 최고의 로우코드 플랫폼이 될 수 있다는 생각
            이러다 소프트웨어 엔지니어가 정말 대체될 날이 오지 않을까 기대
          + 하지만 콘텐츠 변경도 깊은 고민이 필요할 때가 많음
            규모가 조금만 있어도 상하류 의존성이 있고, 필드 하나만 추가해도 전체 시스템이 신경써야 할 게 많아짐
            CSS 같은 작은 변경도 사소해 보이지만 실제로 얼마나 작은지는 사용자가 알기 어려움
          + 접근성, 멀티플랫폼(모바일/데스크탑) 등 수많은 이슈도 곧 빠르게 배우게 될 것
            이런 흐름이 사람들이 소프트웨어 엔지니어로 ""인바운드"" 되게 하는 퍼널처럼 보이기까지 함
     * 작은 작업에선 40~60% 성공률이면 꽤 괜찮다고 생각
       더 복잡하고 깊은 논리가 필요한 작업에서 힘든 점이 있다는 게 알려져서 참고됨
          + 내 테스트 기준에서는, 약간만 비판적 사고가 필요한 작업부터 Codex가 완전히 헤맴
            현 시점 성능은 형편없는 쥬니어 엔지니어 수준임
            예를 들어 변경을 지시했을 때, 컴파일러 경고를 없애려고 클래스의 값을 일괄 nullable로 바꿔버림
            겉으론 작동하고 컴파일도 되지만, 데이터 무결성까지 사라지는 완전히 잘못된 결과
            이런 사례가 꽤 많음
            코드베이스 전체를 Codex에 무감독으로 맡기면 기술부채가 금방 쌓일 걸로 봄
     * Codex가 우리가 자리 비운 채로 일을 잘 하도록 도와줄 거라는 기대는 너무 낙관적으로 느껴짐
       많은 이들에게 ""자리 비웠는데도 효과적으로 일함""이란 실은 ""실업자의 줄""과 맞닿아 있다고 생각
          + 개발자들이 이런 변화에 즐거워하는 현상 자체가 신기함
            언젠가는 우리가 그냥 앉아서 에이전트가 다 해주는 걸 보면서 돈 받게 되는 날이 올 거라 착각하는 분위기에 놀람
            일이 쉬워져 봤자, 결국 일자리 자체가 사라지는 방향으로 갈 가능성
          + 생산성 증가 역사에서 근로자들이 더 많은 자유시간을 누리는 일은 거의 전례가 없음
            결국은 주주와 경영진 수익, 남은 직원 반만큼의 업무 증가, 나머지는 실업이라는 패턴
          + 당분간은 실업까지는 시간이 더 걸릴 것 같은 생각
            이런 모델들이 90~95% 수준으로 넓은 범주의 작업을 제대로 해내려면 엄청난 노력이 든다고 봄
            뭐든 처음 60~70%는 쉬워도, 마지막 5~10%가 진짜 어렵기 때문
            위에 언급된 대로, 여러 차례 실행해 다양한 결과를 뽑고 골라내는 게 현재론 월등히 비싸며, 모든 작업에 일괄 적용하려면 추론(인퍼런스) 비용이 많이 드는 문제도 있음
            어느 시점엔가 코드 리뷰도 기계가 작성한 코드일수록 반드시 필요하게 될 것
            작은 프로젝트나 소규모 기능에선 기계 작업을 신뢰해도 되겠지만, 오랫동안 유지될 코드베이스라면 인간이 구조 설계와 검토를 계속 해야 할 것
            AI는 다양한 방법을 더 빨리 탐색하게 도울 수 있지만, 최종 결정은 여전히 인간 몫이고, 직접 설계나 리뷰로 품질 유지해야 한다고 생각
            가까운 미래에는 엔지니어링팀들이 백그라운드 에이전트를 적극적으로 활용하는 방법을 모색하게 될 듯
            지금처럼 전부 강력한 모델에 외주시키는 방식엔 회의적
            현재 AI 코드 리뷰 작업은 꽤 답답하니 더 좋은 워크플로우가 필요함
            수년간은 ‘백그라운드 에이전트’ 자체가 회사별로 꼭 필요한 인프라(Infra)로 자리 잡게 될 것
            대부분의 회사는 이런 에이전트 인프라를 직접 호스팅하기보다 API로 쓰게 될 거라 예상
            에이전트 기반 엔지니어링 인프라는 아직 엄청 초기 단계라 새로운 업무 기회도 많아질 것으로 보임 (향후 3~5년간)
          + 낙관적으로 보자면, 무언가를 싸게 만들수록(예: 코드) 오히려 그에 대한 수요가 늘어난다는 점도 있음
            비개발자가 관리자로 활동할 가능성도 있지만, 실제로 사람들은 중요한 업무일수록 더 신뢰할 수 있는 사람(인간)에게 맡기고 싶어하는 경향이 있음을 경험함
          + 소프트웨어 개발자를 말(馬)로, Codex나 Claude Code 같은 신규 모델 에이전트를 자동차로 비유할 수 있다는 생각
            일부 말은 자동차의 운전자가 되고, 일부는 더 이상 수레를 끌 필요가 없어서 실업자가 되는 프레임이 맞을지 고민
     * 지원 언어 목록이 정리된 곳을 찾지 못했음
       공식 소개나 리뷰 어디에도 제대로 나와 있지 않고, 대부분 웹페이지 오타 수정 등 예제로만 설명함
     * 일주일 만에 gptel-tool로 뚝딱 만들 수 있는 수준처럼 보임

   머슴으로 쓰면 좋은것이군요!
"
"https://news.hada.io/topic?id=20920","존재하지 말았어야 할 서버","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             존재하지 말았어야 할 서버

     * IT 시스템 부재와 잘못된 관리로 인해 한 가족 기업이 큰 위기를 겪음
     * 중앙화된 서버 및 네트워크 시스템이 도입되면서 내부 통제와 데이터 관리 효율성이 향상됨
     * 일부 직원, 특히 전임 사장 측근, 새로운 시스템에 강하게 반발하며 서버 제거 시도
     * 결국 서버 파괴 및 데이터 삭제 사건이 벌어지지만, 외부 비밀 백업을 통해 데이터는 보존됨
     * 진실을 은폐하려는 내부 반발과 주인의 무력감 속에서 결국 문제는 완전히 해결되지 못함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 기억을 불러온 사건

     * 16년 전, 한 친구가 필자에게 가족 기업의 심각한 경영 위기 문제를 의뢰함
     * 기업 소유주가 갑자기 사망한 후, 가족과 직원들은 IT 시스템이 전무한 상태에서 혼란스러운 상황에 직면함
     * 각자 개인 PC와 노트북으로 업무를 처리하는 등 지식과 데이터가 분산된 환경에서 경영 투명성이 크게 부족함
     * 사업은 현금 흐름이 많지만, 계정은 항상 적자 상태를 벗어나지 못함

새로운 IT 시스템 도입

     * 문제 해결을 위해 라우터, 스위치, 서버 등 네트워크 인프라와 여러 디스크를 갖춘 서버를 도입함
     * NetBSD 기반의 서버에 XEN 가상머신을 활용해 여러 역할을 분담함
          + NAS(파일 공유용, Samba 적용), Archivista(아카이빙 및 문서 관리), Squid(캐싱 프록시), DansGuardian(콘텐츠 필터) 등으로 업무 효율성과 통제 극대화
          + Archivista 인터페이스를 이탈리아어로 직접 번역해 사용자 편의성을 높임
     * 직원들은 새로운 중앙화 시스템 도입에 대체로 만족했고, 문서 관리 자동화 및 OCR 덕분에 업무 효율이 크게 증가함
     * 일부 회계 직원들과 기존 관리자들은 이러한 변화에 회의적인 태도를 보임

내부 저항 및 갈등

     * 전임 사장의 오른팔이었던 관리자는 새로운 시스템과 통제 강화에 강하게 반발함
     * 해당 인물은 급격한 소비 증대와 비상식적 지출 행태를 보이며, 데이터 중앙화로 본인의 비정상적 거래 노출을 꺼림
     * 추가 관리자가 서버를 윈도우로 포맷하려고 시도했고, 소유주 측의 경험 부족으로 혼란이 가중됨
     * 필자는 명확하게 해당 시도를 거부함. 이 시도는 감사 및 주인에 의한 투명성 체계 자체를 무너뜨리려는 의도였음

위기와 극적인 반전

     * 설득 끝에 해당 인물은, 필자가 자신과 가족적 인연이 있음에 당황하며 시도를 포기함
     * 하지만 며칠 내 서버에 지속적인 사고가 발생해 결국 서버는 완전히 작동 불능 상태로 파괴됨
     * 내부자에 의한 하드디스크 물리적 제거로 데이터가 사라지지만, 소유주 몰래 외부에 설치해둔 백업 시스템 덕분에 데이터 완전 복구가 가능함
     * 백업은 느리지만 신뢰성 높은 PCEngines Alix 장비(NetBSD/USB 드라이브 적용)로 진행됐으며, 현재도 다른 용도로 사용 중임

결론 및 교훈

     * 소유주들은 문제 해결을 위한 법적 조치 여부를 고민했으나 뚜렷한 결과 없이 시간만 흘렀음
     * 이후, 필자에게 고액 연봉의 정규직 제안이 들어왔으나 거절함
     * 근본적으로 문제를 외면하려는 내부 분위기와, 실질적 변화를 방해한 세력 때문에 정상화가 불가능했음
     * 궁극적으로, 필자는 일부 문제 상황은 구제 자체가 불가능함을 깨달음
     * 심각한 내부 신뢰 붕괴와 구조적 부정이 난무한 환경에서는 해결자라 해도 모든 문제를 바로잡을 수 없음을 경험함

저자 후기

     * 일부 독자가 조직범죄 연루 가능성을 제기했으나 사실이 아님
     * 최악의 부분은 내부 권한 남용 및 신뢰 악용, 부적절 행위에 있었으며, 프라이버시와 맥락상 더 이상 구체적으로 언급하지 않음

마지막 메시지

     * 세상에는 구제 불능의 문제가 존재함을 인정하는 것도 필요함
     * 필자는 문제 해결자이지만, 본질적으로 문제 해결을 거부하는 주변인들이 있을 때 모든 문제를 해결할 수 없음을 강조함

        Hacker News 의견

     * 나는 비영리 단체에서 일한 경험이 있음. 합법적인 부패 현상이 매우 컸음. 정부 기관의 감사를 받은 적이 있는데, 그 정부 기관은 비영리 단체 업계의 로비로 운영에 큰 제약을 받고 있었음. 감사가 끝난 후, 기관장은 보도자료를 통해 비합법적인 행동뿐만 아니라 시민들이 비영리 단체에 기대하는 부분에 대해서도 결과를 공개할 수 있어야 한다고 말했음. 사람들이 자선단체 기금 중 실제 목적에 얼마나 쓰이고, 이사진과 직원에게 선물처럼 쓰이는지를 명확히 알아야 함을 강조했음. 결국, 부패는 합법적인 경로를 찾아 지속됨을 요약함
          + ""NFP space""가 무엇인지 궁금함. 구글링 해봤지만, 지역 동호회나 NFP라는 이름의 회사들만 나오고, 막강한 로비 조직은 찾지 못했음
          + 나는 규모가 큰 NGO나 자선단체에서는 항상 부패, 태만, 또는 어느 정도의 사기 냄새를 맡아왔음. 이것이 내 모든 관련 조직에 대한 시각을 바꾸었음
     * 이런 상황을 예전에 본 적이 있음. 그래서 오프사이트 백업이 매우 중요하다고 생각함. 예전에도 누군가가 책임 회피를 위해 의도적으로 데이터를 삭제하려고 했을 때 백업에서 데이터를 제공한 적이 있음. 사람의 의도와 상관없이, 일부는 물려받은 것을 파괴한 후, 문제가 생기면 선임자 탓을 하는 경우가 자주 있음
          + 전임자 탓을 하지 않으면 더 나쁜 상황이 올 수도 있음. 나도 안 좋은 코드베이스를 인수받아 개선하려고 노력했지만, 시간이 부족해 다 고치지 못했음. 내가 떠나고 후임자가 남은 문제 부분을 곧바로 내 탓으로 돌렸음
     * 부패가 얼마나 쉽게 일어나는지 항상 흥미롭게 생각함. 회계가 체크한다고 생각했지만, 실제로 계좌를 만들고 돈을 빼가도 여러 해 동안 아무도 모르는 회사를 자주 봐왔음. 내가 어떤 회사에 자동 청구서를 만들어줬는데, 몇 달 동안 데이터가 누락된 줄도 몰랐음. 그런데도 많은 금액을 받았음. 인보이스 내용이 무엇이든 거의 그대로 돈을 받을 수도 있었겠다는 깨달음이 있었음
          + 로버트 맥나마라가 Ford에 왔을 때, 회계가 엉망이라 인보이스를 무게로 재서 기대되는 무게 대비 금액 비율이 맞으면 그냥 지급했음
          + Dan Davies가 쓴 관련 주제를 다룬 최고의 책이 있음: https://www.inkwellmanagement.com/books/lying-for-money
          + 평소 금액이 클수록 반올림 오차 수준의 금액도 커짐. 최근 거래처와 흥미로운 경험이 있었는데, 그쪽에서 몇천 달러 청구를 깜빡하고 몇 달 지났는데, 쿼터 증액을 요청하자 그제야 기억해냈음
     * 내가 뭔가 놓친 것 같음. 나중에 회사에서 문제 해결을 위해 모든 걸 약속했는데, 그는 거절했고, 이후엔 도구 등 필요한 지원을 받지 못했다고 불평함. ""네가 원하는 만큼 요구하라"" 할 때는 프로세스에 대한 소유권까지 포함한 모든 도구가 포함되어야 함
          + 뭔가 빠진 것 같음. 내 추측으론 문제를 일으키는 사람이 오너들과 특권적 관계를 가졌을 거라고 생각함. 오너들이 그 사람을 회사에서 완전히 내보내길 원치 않았고, 그래서 필요한 도구/권한을 주지 않는다는 의미임
          + 나는 이게 그를 매수(buy out)한 것이라는 뜻인 줄 알았음
     * 흥미롭게 읽었음. 요즘 누가 내게 기술 조언을 구하면 정성껏 조언해줌. 하지만 내 조언 중 일부만 따르고 나머지는 잘못된 방식을 고수하려고 하면 “잘 하시길!” 하며 그냥 떠남
     * 저자 노트: 많은 독자들이 사건의 심각성에 놀라 조직 범죄가 연루된 것 아니냐고 추측했음. 상황이 매우 문제 많고 부정직하긴 했지만, 조직 범죄와는 관련이 없었음. 내가 언급한 “최악의 부분”은 다른 내부 역학, 신뢰 남용, 부적절함 등에 관한 것이었고, 개인 정보와 이야기의 무게 때문에 더 자세한 설명은 하지 않겠음
          + “Archivista 인터페이스를 이탈리아어로 번역도 했을 정도임” 언급을 보고, 어떤 이탈리아어 사용 지역은 조직 범죄로 전설적이니 여기에도 연루가 있었을 것 같다는 생각이 드는 사람도 있겠음. 오히려 연루가 안 됐다는 게 믿기 어려움
     * 이탈리아. 돈이 문제는 아닌데도 컨설팅 회사를 쓰지 않음. 조직 범죄 연루는 없다고 하지만 믿기 어렵다는 뉘앙스임
          + 아니, 조직 범죄가 아니었음
     * 중요한 포인트(예: 연도 등)는 빼먹지 않았으면 함. 당시 사용한 툴(주택 동기화/백업 방법)도 궁금함. 개인적으로 궁금한 건 왜 freebsd로 옮겼는지임. 다른 용도였는지? 요즘엔 nextcloud/owncloud와 rsync/syncthing으로 NAS와 원격 동기화하는 구조로 할 것임. 나였으면 이럴 때 이사직을 맡고, 지역 MSP를 고용해 원하는 방식으로 했을 것임. 이렇게 하면 신뢰할 수 있는 외부 인력을 둘 수 있음. 군대와 외교관에서도 HQ에서 고용하고 HQ에만 보고하는 XO라는 원리가 있음
          + 대략 2009년 이야기였음. 정확한 연도는 기억이 안 나지만 그 시기임. 백업에는 rsync 기반 동기화와 하드링크, 그리고 그 위에 rsync를 썼음. 모든 과정을 자동화하는 Perl 스크립트도 직접 썼지만 이름은 기억 안 남. 제안 부분에도 동의함. 하지만 그땐 어렸고 건강하게 성장하는 고객과 무언가를 만드는 데 더 중점을 뒀음. 망가진 상황을 구하려는 데 관심이 적었음. ALIX를 FreeBSD로 옮겼던 건 다른 업무 때문이었고, FreeBSD의 네이티브 read only 지원이 그 용도에 완벽하게 맞았음
     * 의료 업계에서도 시스템과 관리 부재로 중요한 환자 데이터가 손실되거나 잘못 다루어진 사례가 있었음. 어떤 병원은 수년간 종이 기록만 쓰다가 직원 실수로 중요한 데이터를 삭제하는 위기를 맞음. 이후, 통합 디지털 시스템을 도입해 데이터 추적/백업 체계를 만들어서 환자와 이해관계자 신뢰를 회복했음
     * 왜 문장마다 줄바꿈이 있는지 궁금함
          + 정말 특이한 서식 선택이고 읽기 어렵게 만듦. 문장마다 <br>을 넣는 건 일반적인 관행이 아니지만, 본문 자체는 훌륭한 이야기임
          + 아마 LinkedIn용으로 처음 쓴 글일 것 같음. 실제로 아무것도 성취하지 못했으면서 교훈만 강조하는 느낌도 맞는 듯함
          + 내 고등학교 리서치 페이퍼 선생님이 생각남
"
"https://news.hada.io/topic?id=20932","“AI-퍼스트” 시대, 콘텐츠 디자이너는 무엇을 준비해야 하나","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   “AI-퍼스트” 시대, 콘텐츠 디자이너는 무엇을 준비해야 하나

     * Shopify와 Duolingo가 잇달아 “AI-퍼스트” 조직 전략을 선언, 콘텐츠 디자인도 이에 대비가 필요함
     * 단순한 도구 활용을 넘어서 시스템 구축과 품질 유지 역량이 콘텐츠 디자이너의 핵심 역할로 부상함
     * AI 환경에서는 모듈화된 콘텐츠 구조, AI 훈련 데이터 설계, 정량적 검토 체계가 중요해짐
     * 시스템 사고, AI 리터러시, 협업 및 콘텐츠 거버넌스 능력이 새로운 핵심 역량으로 요구됨
     * 콘텐츠 디자인은 단순한 문구 작성에서 제품의 신뢰성과 사용자 경험을 지키는 전략적 책임자로 진화해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“AI-퍼스트” 선언이 콘텐츠 디자인에 주는 신호

     * Shopify CEO는 “AI로 할 수 없는 일이어야만 사람을 뽑겠다”고 선언
     * Duolingo도 AI 활용을 평가·채용 기준으로 삼겠다는 비슷한 입장을 발표
     * 이는 단지 트렌드가 아니라, 콘텐츠 디자이너가 작업 방식 자체를 재설계해야 하는 변화임

현재 콘텐츠 디자인 업계의 AI 도입 현황

     * 대부분의 콘텐츠 디자이너가 아이디어 발상이나 간단한 작업에 AI를 실험적으로 사용 중
     * 일부는 직접 모델을 훈련시키거나, 다른 디자이너들을 위한 스케일 도구로 AI를 활용
     * Button Conference에서도 AI 전담 세션 운영, 업계의 심화된 관심을 반영

AI는 제품 개발의 방식 자체를 바꿈

     * 노코드 툴과 AI 도구로 인해 콘텐츠 디자이너도 빠른 프로토타입 제작 가능
     * 이에 따라 속도와 실험 중심의 개발 문화로의 전환 가속
     * 기존 조직 구조나 디자인 시스템이 이를 따라가지 못하면 혼란과 일관성 붕괴 발생
     * 명확한 콘텐츠 구조, 반복 가능한 패턴, 전략적 메시지 설계는 더 이상 “선택 사항”이 아님

     “빠른 속도에서 품질을 유지해야 한다”는 과제가 생김

“AI-퍼스트” 환경에서 콘텐츠 디자인이 해야 할 일

     * 1. 모듈형 시스템 설계
          + 콘텐츠 디자이너는 재사용 가능한 콘텐츠 구조를 만들어야 함
          + 예: 오류 메시지를 “무엇이 잘못됐는지 / 왜 중요한지 / 사용자가 무엇을 해야 하는지”로 구분
          + 이는 AI가 콘텐츠를 일관되게 확장할 수 있는 기반이 됨
     * 2. AI 시스템 훈련
          + AI는 감정, 브랜드 보이스, 접근성 기준을 자연스럽게 이해하지 못함
          + 콘텐츠 디자이너는 AI에게 이를 가르칠 수 있는 예시, 가이드라인, 평가 기준을 설계해야 함
          + 사람용 스타일 가이드는 AI에겐 부족하므로, 기계가 이해할 수 있는 규칙 언어로 전환 필요
          + 이는 단순한 “작성”이 아닌 콘텐츠 디자인의 추상화된 확장임
     * 3. 평가와 품질 관리
          + AI가 작성한 콘텐츠라도 사람의 최종 리뷰는 필수
          + 다음과 같은 항목으로 검토 기준 설정 필요:
               o 감정 상태와 톤의 일치 여부
               o 브랜드 및 문맥 적합성
               o 포괄성과 접근성 확보 여부
               o 법적/윤리적 기준 충족 여부
          + 고위험 영역(온보딩, 결제, 추천 등)에선 리뷰 체크포인트 필수
          + 피드백 루프와 측정 지표 설정도 중요: 클리어니스 점수, 고객지원 감소율 등

시스템 사고의 중요성

     * AI 기반 제품 개발은 “구조 없는 속도”가 곧 실패로 이어짐
     * 콘텐츠 디자이너는 단순한 작성자가 아닌, 스케일 가능한 콘텐츠 시스템의 설계자가 되어야 함
     * 재사용 가능한 콘텐츠 구조를 미리 정의하지 않으면 AI는 혼란을 대량 복제
     * 중요한 스킬 세트:
          + AI 리터러시 (모델 동작 이해, 프롬프트 설계, 결과 평가)
          + 교차 기능 협업 (PM, 엔지니어, 리서처, 법무 등과의 협력)
          + 콘텐츠 거버넌스 (대규모 품질 관리 체계 수립)
          + 조직 내 변화 리더십 (책임 있는 AI 실천 주장 및 설계)

콘텐츠 디자이너의 새로운 책임

     * AI 도입이 단순 유행이 아닐 수 있음
     * 하지만 설령 아니더라도, 이러한 스킬은 충분히 익혀둘 가치가 있음
     * 콘텐츠 디자인은 이제 한 화면의 문장을 넘어서, 제품 전체의 사용자 경험과 신뢰성을 지키는 역할로 진화 중
"
"https://news.hada.io/topic?id=20967","Moody’s, 미국의 트리플A 신용등급 박탈하고 Aa1로 한 단계 하향","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Moody’s, 미국의 트리플A 신용등급 박탈하고 Aa1로 한 단계 하향

     * Moody’s가 미국의 신용등급을 AAA에서 Aa1로 한 단계 하향 조정함
     * 이는 국가 부채 증가와 재정 적자 확대에 대한 우려에 따른 결정임
     * 2035년까지 GDP 대비 재정 적자가 9%에 이를 것으로 전망되며, 이는 금리 상승과 복지 지출 증가, 낮은 세수 때문임
     * Fitch와 S&P는 이미 AAA 등급을 철회한 상태이며, Moody’s만이 마지막까지 최고 등급을 유지했었음
     * 이번 등급 조정은 동일 등급 국가 대비 높은 부채 및 이자 지출 비율이 핵심 요인으로 지목됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Moody’s의 미국 신용등급 강등

  등급 조정 내용

     * Moody’s는 미국의 국가 신용등급을 Aaa에서 Aa1으로 하향 조정
     * 등급 전망은 ‘부정적’에서 ‘안정적’으로 전환
     * 이는 Moody’s의 21단계 등급 체계 중 한 단계 하락한 것으로, 여전히 높은 신용도를 의미하지만 최고 등급은 아님

  주요 하향 사유

     * 지난 10년간 누적된 국가 부채 증가와 이자 지출의 확대가 주요 원인
     * GDP 대비 연방 재정 적자가 2035년에는 9%까지 확대될 것으로 예측됨
          + 이는 2023년 기준 6.4%에서 큰 폭으로 증가하는 수치
     * 적자 확대는 다음과 같은 세 가지 구조적 요인 때문:
          + 높은 이자 지출
          + 복지 지출(Entitlement Spending)의 증가
          + 세수 부족(낮은 세입 창출 능력)

  다른 평가사들과의 비교

     * Moody’s는 마지막까지 미국에 대해 AAA 등급을 유지한 주요 평가사였음
     * Fitch는 2023년 8월 AAA 등급을 철회
     * S&P는 이미 2011년에 미국의 신용등급을 강등

  Moody’s의 평가 설명

     * “이번 등급 강등은 지난 10여 년에 걸쳐 누적된 정부 부채 및 이자 지출 비율이 다른 동일 등급 국가에 비해 높아졌다는 사실을 반영한 것”
     * 미국 경제의 규모와 유동성, 기축통화로서의 달러의 지위는 신용도에 긍정적 요인으로 계속 고려되고 있음

  향후 전망

     * Moody’s는 ‘안정적’ 전망을 부여하며, 추가 하향 가능성은 낮게 본다는 입장
     * 그러나 재정 수지의 지속적 악화나 정책 대응 실패 시 또 다른 등급 하향 가능성 존재

        Hacker News 의견

     * 미국 부채를 줄일 수 있는 손쉬운 방법이 많다는 의견 제시. 캐리드 이자, 기업의 세금 회피, 사회보장제도의 소득기준 적용, 불필요한 화석 연료 및 옥수수/설탕 보조금 폐지, PBM 포뮬러리 폐기와 메디케어의 의약품 협상 및 사기 감소. 그 자체로 수천억 달러 절감 가능성 언급. 장기적으로 교육과 건강관리 시스템의 성과 기반 개혁 필요성 주장. 국방도 드론과 잠수함 기반의 핵 억제력만 남기고 대폭 축소 필요성 언급. 방위 계약 대신 확정 구매 오더로 대체하고 민간 시장에 초음속 무기 개발 맡기기 제안
          + 사회보장제도를 소득 기준으로 나누는 것보다 사회보장세 상한선을 인상하는 편이 훨씬 합리적이라는 주장. 소득 기준 적용은 불필요한 관료주의와 '누가 혜택받는지'를 둘러싼 분열로 프로그램 지지 약화 위험성. 수백만 달러를 버는 사람도 연 $176,100 버는 사람과 같은 사회보장세 내는 현실 지적. 상한선 소폭 인상만으로도 사회보장 미래 재정 문제 해결 가능
          + 사회보장 소득 기준 적용의 문제점으로, 20~30대 시절 정부의 사회보장 약속을 믿고 은퇴 계획을 세운 사람이 50~60대에 와서 혜택 박탈되는 상황을 사기와 비슷하다고 여김. 차라리 10~20대 초기 근로자에게 '너무 많이 벌면 사회보장 못 받는다'고 처음부터 정직하게 말하는 게 더 합리적. 단, 이는 당분간 재정 지출에 큰 영향 주지 못함. 또한, 정부가 개인 재정 상태를 깊이 들여다보는 소득 기준 적용 자체가 부당한 정부 권한 확대 논란 소지. 만약 이런 정부 개입이 문제라면, 일관성을 위해 소득세 자체도 없애야 한다는 급진적 주장 가능성 제시
          + 사회보장은 스스로 기금을 조달하는 시스템임을 강조. 법적으로 적자나 정부 부채 상환 능력에 직접 영향 없음. 정부가 어떻게 돌아가는지 잘 모르는 인식에 당황스럽다는 생각
          + 미국 적자가 작년에 1조8천억 달러였음. 캐리드 이자 비과세 혜택을 없애도 연간 10~20억 달러 절감에 불과. 명시적인 연료 보조금 역시 30억 달러 수준. 암묵적 보조금은 더 많을 수 있지만, 연료 가격이 오르면 경제에 악영향. 기업 조세피난처로 인한 손실 규모 의문 제기
          + 국방 낭비 절감 방안은 공감하나, 군 규모를 그 정도로 극단적으로 줄이면 안 된다는 의견. 다른 국가에서 지상군 투입과 세계 최고의 물류 지원 능력이 여전히 필요하며, 핵 억제 이외의 다양한 억지력 옵션도 보유해야 함
     * 2025년 2월에 Elon과 Doge가 국가부채를 해결할 거라던 이야기를 회상. 많은 이들이 Elon이 실제로는 재정적자 줄이기에 관심이 없다는 데 회의적이었던 분위기 언급. Moody’s는 2035년까지 적자가 GDP의 9%까지 확대될 것이라 전망, 작년(6.4%)에서 더 상승. 주된 이유는 이자 지출 증가, 복지지출, 낮은 세수. 매우 예측 가능했던 일이며, 앞으로 상황이 명확해지면 더 많은 사람들이 현실을 깨달을 것이라는 희망 표명
          + 지금은 사람들이 이런 사안을 잘 인식하지 못하는 것 같음. Bill Clinton 대통령 시절에는 관료주의 축소와 재정적자 감축에서 꽤 성과를 거뒀지만, 지금은 전반적으로 혼란스러움
          + 모든 거래에는 양쪽 당사자가 있음을 상기. 누군가의 부채는 다른 누군가의 자산임. 국가부채 상당 부분이 미국인이 보유하고 있어 민간부문에 자산 효과 발생. 돈이란 본질적으로 부채임. 오히려 진짜 위기는 민간의 과도한 부채이며, 정부는 자국 통화로만 부채를 발행하므로 이론적으로 지급 불능 위험이 없음. 유일한 신용 등급 강등 사유는 정치적 혼란으로 미국 정부 자체가 존재하지 않게 되는 상황뿐이라는 주장. 언론에선 정부 부채만 부각하고 민간 부채는 언급 안 된다는 불만
          + 신용평가사가 2007년 부동산 신용 버블을 제대로 예측하지 못했을 때 다들 비웃으면서, 정치적 분위기만 맞으면 똑같이 '신용평가사가 그랬으니 사실'이라고 맹신하는 현상에 대한 풍자
     * 현재 경쟁하는 두 가지 이론 소개. 첫째, ‘Dollar “Milkshake” theory’는 달러와 미국 국채의 수요가 워낙 높아 미국이 빚을 얼마를 져도 크게 상관없으며, 국채가 이자 수익보다 금융 담보 역할로 쓰임. 위기 때엔 달러로 자금이 몰림. 둘째, 전통적 채권 투자 이론은 실질수익에 민감하며, 국가가 실질적으로 상환불능(명목상 상환이 아니라 인플레이션을 통한 사실상 채무 면책)에 처할 경우 투자자들이 처벌한다는 입장. 두 입장 모두 이해하지만 어떤 이론이 맞을지 예측은 어려움. 본인은 재정적으로 보수적 성향이어서 전통적 이론에 더 끌리는 편
          + 미국 시장(채권과 증권)을 떠받치는 가장 큰 요인은 대체할 투자처가 별로 없다는 사실. 하지만 만약 상황이 변화한다면(향후 20년간 50:50 확률 예상), 천천히 진행되다가 갑자기 급변. 그 때가 되면 미국 고소득자들도 사회보장에 훨씬 더 관심을 쏟게 될 것이라는 전망
     * 백악관 홍보 책임자인 Steven Cheung이 소셜미디어에 Moody’s 신용평가 담당인 Mark Zandi를 콕 집어 비난, 그를 트럼프의 정치적 반대자라고 언급. 기사를 첨부하며 남탓 패턴 지적
          + 트럼프가 Moody's를 겨냥하기까지 얼마나 걸릴지 궁금했다고 언급
     * 지난 80년간 미국이 세계 경제의 설계자이자 관리자로서 부와 힘, 기술적으로 가장 앞선 국가가 될 수 있었음을 강조. 그러나 트럼프 집권 후 미국이 자발적으로 그 역할을 내려놓으면서, 기존의 안정적 리더십이 흔들리며 시스템 자체가 난항을 겪는 중임을 지적. 진짜 문제는 신용등급이나 무역협정, 관세가 아니라, 새로운 안정적이고 실리적인 경제 강국이 글로벌 무역의 핵심 위치를 차지하게 되는 구조적 변화임. 요약하자면, 미국의 미래에 큰 우려를 드러냄
          + 미국이 오랜 기간 투자해 구축한 소프트파워 비용을 ‘프리라이더’로 치부하는 리더의 시각이 외부에서 보면 비극적이면서도 웃긴다는 생각. 세계 리더의 지위를 유지하려면 투자해야 하며, 아니라면 그냥 여러 나라 중 하나에 불과하다는 시각
     * Moody’s 같은 신용평가사들이 정말 신용도를 평가할 통찰력을 갖췄는지에 의문 표명. 대형 금융기관이라면 자체적으로 분석하는 게 상식일 것이라는 추정. 규모가 작은 기관만 신용평가사에 의존하는지 궁금함. 미국 국채처럼 중요한 자산을 보유한 이들은 자체 분석을 하지, 신용평가사 의존하지 않을 것이라 생각. 2008년 위기 때 아무 가치도 못 했던 것 같은데 과연 무슨 가치를 제공하는지 질문
          + 당시 그들이 정크본드에도 A+ 등급을 줬다는 사실을 상기. 아직도 왜 신용평가사가 산업에 남아 있는지 의문. 관련 기사 첨부
          + 신용평가사들이 법적으로 사실상 과점 지위임을 설명. 대형 자금 운용기관은 특정 등급 미만의 채권에 투자 불가. 채권 발행자는 투자자 확보를 위해 신용평가사에게 등급을 의뢰하므로, 평가 없이 채권 발행 자체가 거의 불가. 부패와 비효율의 시스템이란 평가. 여러 다른 분야와 비슷한 상황
          + 실제로 신용 분석에 인사이트가 있음을 주장. 기본 등급별 실제 부도율을 보면 차이가 명확. 2008년 위기에도 AAA 등급 자산이 기본적으로 디폴트된 게 아니라 손실은 났어도 실제로 부도난 건 거의 없고, 보유자는 오히려 이득을 본 경우 많았다는 주장. 관련 논문 첨부
          + 미국 국채 대량 보유기관이라면 자체 분석을 하는 것이 상식이지만, 공적연금펀드는 이런 분석에 무능하기로 악명 높음을 지적
          + 상당수 리테일 및 기관투자자들이 여전히 신용등급을 지표로 삼는 현실 언급. 모든 이가 꼼꼼히 실사하지 않고, 실제 기업 내에서는 마감 압박에 실사 프로세스 생략 빈번. 분기 실적에는 아무 문제 없으니 단기적 선택을 한다는 경험 공유
     * 미국의 재정 정책은 지금과 같은 기조로 지속 불가능하다는 현실 강조. Moody’s가 드물게 제 역할을 하고 있다고 평가. 과거 10여 년간 문제를 방치했던 문제도 비밀이 아니라는 주장. 미국은 일정 기간 고통스러운 구조조정을 피할 수 없고, 이를 피하려다 오히려 문제를 악화시키고 있다는 인식. 그 와중에 의회가 세금 인하와 금리 인하 압박 중이라는 점이 오히려 지금 상황의 원인임을 지적
          + 세금 인상은 불가피하며 결국 채권 시장이 강제할 것이라는 의견. 미래 성장 전망이 낮은데도 계속 부채 발행만으론 지속 불가능. 유권자가 단순할 수 있지만, 채권 시장은 절대 그렇지 않음. 재정 불안을 채권을 대량 매도하는 ‘채권 감시자(bond vigilantes)’가 정치인을 강제로 조정할 수 있음을 강조. 관련 자료 첨부
          + 현재 미국 재정 환경에 최악의 인물들이 정부를 운영 중이라는 의견. 이미 부채와 정부지출이 심각한 수준에 도달한 상황에서 앞으로 수조 달러 단위의 예산 구멍이 더 커질 것이라는 우려. 여기에 끝없는 관세 정책까지 겹치며 상황이 총체적으로 악화됨을 비판
          + ‘지출 축소’ 이야기가 전혀 없다는 점을 꼬집음. 정부가 없는 돈을 쓰지 말아야만 한다는 주장. 과다 지출이 문제의 본질인데, 이를 무시한 채 산업 축소와 증세만 대안으로 제시하는 건 본질 회피라는 비유로 설명
          + S&P와 Fitch가 이미 수년 전에 미국 신용등급을 강등한 사실 언급. Moody's는 뒤늦게 따라온 셈
     * 미국이 AAA 신용등급을 지키고 있던 마지막 주요 평가사였다는 사실에서, 이번 강등이 투자자들의 국채 매도 사태를 유발할지 궁금증 제기. 일부 투자자는 AAA 등급 채권에만 투자할 수 있는 규정이 있기에 그 영향이 걱정된다는 시각
          + 통상 두 곳 이상이 AAA면 투자 가능하다는 규정이 많아서, 이미 두 번째 강등 때 트레저리 매도는 충분히 일어났을 것이라는 분석
     * Moody’s가 트럼프 정책이 정부 지출을 크게 늘릴 것이라고 분명히 언급했음을 강조. 직접적으로 트럼프의 세금 정책을 비판했고, 신용등급 하락의 직접적 원인으로 트럼프를 꼽음. 다시 한번 공화당, 특히 트럼프가 국가 부채나 저세율에 신경 쓰지 않는다는 점 강조
"
"https://news.hada.io/topic?id=20972","미스티컬","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  미스티컬

     * 미스티컬은 마법진 형태로 PostScript 프로그램을 시각화하는 독특한 방식임
     * 프로그래밍 구조를 고리(ring) 형태로 표현하며, 실행 배열, 비실행 배열, 딕셔너리 등 다양한 타입이 있음
     * 시길(sigil) 이라는 특수 심볼을 통해 명령어나 변수, 문자열 등을 독창적으로 나타냄
     * 정의 패턴이나 함수 선언에는 별도의 결합 기호(ligature) 표현이 적용되며, 사용자 맞춤 시길도 지원함
     * 현재는 PostScript 프로그램을 그래픽 이미지로 변환하는 도구이며, 자체 인터프리터는 존재하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미스티컬(Mystical) 개요

   미스티컬은 마법진을 닮은 프로그래밍 언어를 목표로 한 실험적 프로젝트임. 실제로는 마법진 형태로 PostScript 코드를 시각적으로 표현하는 방식이며, 이 문서에서는 이 방법을 '미스티컬'이라 명명함.

고리(Rings) 구조

     * 미스티컬의 구조는 고리 형태를 중심으로 함
     * 각 고리는 글자와 시길을 담고 있으며, 안쪽과 바깥쪽 경계선이 있음
     * 프로그램의 메인 고리는 오른쪽(3시 방향)에서 시작해 시계 반대 방향(widdershins) 으로 진행함. 이는 PostScript의 각도 규칙과 마법진이 바깥에서 안으로 써진다는 개념을 반영함
     * 하위 고리는 상위 고리의 접점에서 시작함

   고리의 세 종류
     * 실행 배열(Executable array) : PostScript의 {}에 해당. 내부와 외부에 단순 원형 경계와 별 모양, 시작/종료에는 연금술 기호 기반 심볼 사용
     * 비실행 배열(Non-executable array) : PostScript의 []에 해당. 별 모양 없음, 시작/종료 삼각형 사용
     * 딕셔너리(Dictionary) : PostScript의 <<>>에 해당. 다각형 형태에 이중 바깥 경계와 단일 안쪽 경계. 시작/종료 표시 동일

   고리 안에 또 다른 구조가 들어올 때는 작은 점 또는 선으로 연결되어 계층을 시각적으로 표현함

   PostScript 사용상의 제한
     * PostScript에서 [ ] 또는 << >>를 미스티컬 규칙상 허용 불가 방식으로 쓸 수 있으므로 권장하지 않음
     * gsave/grestore, begin/end 등은 비균형적 구조에 더 많이 쓰이므로 일반 시길로 처리함

텍스트와 시길(Sigils)

     * 고리 테두리에는 문자 또는 시길(특수 심볼)이 배치됨
     * 시길은 연산자, 변수, 키워드를 나타냄
     * PostScript의 /name은 삼각형 안에 이름이나 시길로, 문자열 ()는 두루마리 모양으로 표현

  표준 시길(Standard Sigils)

     * 내장 연산자 다수에 대해 고유 시길이 존재
     * 보통 명령어의 이니셜, 개념의 그림, 또는 시각적 언어로 디자인됨

   예: dup, copy, add, mul, neg, for, forall, repeat, if, ifelse, eq, ne, ge, gt, le, lt, moveto, lineto, arc, arcn, curveto, closepath, stroke, fill, gsave, grestore, translate, scale, rotate, setmatrix, currentmatrix, setrgbcolor, currentrgbcolor, setcmykcolor, currentcmykcolor, sethsbcolor, currenthsbcolor, setgray, currentgray, dict, begin, end, def, get, put, length 등

  사용자 시길(User Sigils)

     * 새 함수나 이름에 대해 런타임에 sigil_bank에 추가 가능
     * 1-유닛 정사각형 내로 디자인 필요(좌표 변환 가능)
     * nstroke 사용 시 기존 시길과 동일한 서체 효과 가능
     * 사용자 변수용 시길은 다양한 방식(글자 겹침, 다양한 시각적 언어 등)으로 제작 가능
     * 공식 연산자 기반 새 이름은 표준 시길과 조합 가능

/name { ring } def 결합 시길(Ligature)

     * 함수 정의 등에서 자주 쓰는 패턴을 위한 특수 시길 제공
     * 이름 삼각형 아래에 연결선만 표현하며, 별도의 def 시길은 생략함
     * 세 가지 고리 타입 모두에 적용 가능
     * 그 외의 def 사용은 일반 시길로 처리
     * 실행 배열 내에서만 적용하며, 딕셔너리 내에서는 사용상 혼란 가능성으로 제공 안 함

샘플 알고리듬

   상단 그림은 퀵소트(Quicksort) 예시임

   Euclid 최대공약수(GCD) 알고리듬 예시도 제공함. 해당 예시는 사용자 함수 /arg {exch def} def를 활용

미스티컬 이미지 생성 함수

   모두 ""mystical.ps""에 정의됨
     * mystical: 배열, 실행 배열, 또는 딕셔너리를 받아 내부 구조까지 재귀적으로 시각화. 전체 이미지는 단위 원에 맞게 스케일 조정
     * mystical_evoke: 이름을 받아 현재 딕셔너리에서 찾아 위와 같이 렌더링
     * mystical_evoke_label: 이름-def 결합 시길 추가, 이름 시길이 수직으로 보이게 방향 조절

   위 함수들에는 _unscaled 버전도 존재. 이 경우 스케일 불가하여 고리가 매우 크게 출력됨

  레이아웃 문제

     * 현재 코드는 하위 원의 배치를 충돌 없이 최적화하지만, 보수적이라 배치가 과하게 펼쳐짐
     * 예시 그림에서는 mystical_get_spell, mystical_make_evocation_ligature 등으로 파싱/배치 후, draw_sigil, draw_link 함수로 직접 조정해 그림을 출력함
     * 향후 레이아웃 디폴트 설정을 개선할 예정임

프로그래밍 언어로서의 미스티컬

     * 현재 미스티컬은 PostScript 프로그램을 그리는 도구임
     * 미스티컬 이미지 자체를 해석 실행하는 인터프리터는 없음
     * 사람이 이미지를 보고 PostScript 코드를 이해하거나, PostScript 프로그램으로 재작성하여 실행하는 방식임
     * 언어적 논의는 추후로 미룸

다른 언어 적용 가능성

     * FORTH와 같이 연산자만으로 이루어진 언어에서는 활용 가능성이 큼
     * 구조가 더 복잡한 언어에는 적용 시 고리가 과도하게 많아질 우려가 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   github에서 다운로드
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   codeberg에서 다운로드
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   이 페이지는 2025-05-16에 Denis가 생성함

        Hacker News 의견

     * 일본 애니메이션 Dennō Coil이 생각나는 감상, 아이들이 실제로 저자와 거의 동일하게 프로그램을 바닥에 그려 마법처럼 부리는 장면 언급, 강력 추천 의견
          + 그 작품을 정말 좋아했다는 감상 공유, 아이들이 어른과는 다르게 신기술을 받아들이는 방법을 잘 보여주었고, 심지어 google glass보다도 5년은 빨랐던 선구적인 연출이라는 소감
          + 똑같은 생각을 하고 왔으며, 정말 훌륭한 작품이라는 찬사
     * 게임에서 마법이나 의식을 외치는 상황을 현실적인 유머로 풍자하여, ""붉은 달의 여신을 부르고, 제단도 준비하고, 크리스탈도 충전했으니, 제발 이번엔 부팅을 해달라""는 기원 내용
          + 성서 더미에서 촛불을 치우라는 추가 의식 농담
     * Aphyr의 ""Xing the technical interview"" 블로그 시리즈의 이세계 주인공이 좋아할 만한 프로그래밍 언어라는 감상, 이 시리즈에 꼭 등재될 자격이 있다고 추천, 그리고 관련 참고 링크 공유
          + 그 시리즈를 몰랐다는 반응과, 정말 재미있다는 평
     * Isaac와 Caret 리버스 엔지니어링 프로그램과 관련된 UFO 레딧 토론 스레드 추천, 더 깊은 탐구를 재촉하는 의견
     * 이 작업물이 너무 놀랍고, 라이선스에 대해 문의하며, 파생 프로젝트로 순수 재귀만 사용하는 forth 스타일을 적용하면 현재 작업 중인 게임에서 시각화가 부족한 마법 시스템에 완벽히 어울릴 것이라 기대감 표시, Mystical이 게임 내 마법을 구조화하여 유저가 작성하는 방법과 몰입감을 모두 충족시켜 주는 해법이라는 판단
          + Noita 게임에도 비슷한 방식의 마법 제작 시스템이 존재, 프로그래밍적으로 반복, 곱셈, 복제 등을 적용할 수 있는 완드 시스템 소개와 게임 스팀 링크 제공
     * Perl로 유명한 ""Black Perl"" 시의 한 부분을 인용하며, 마법 의식과 프로그래밍 명령이 절묘하게 어우러진 예시 공유
     * 이 프로그래밍 언어가 실제로 꽤나 실용적이라는 소견, 미학에 중점을 둔 esolang(괴짜 언어) 중 이토록 읽기 쉽고 사용이 간편한 경우가 드물다는 평가, 이런 아이디어가 Lisp류, 스택 기반, 배열 언어 등에도 적용될 수 있다고 생각, 적절한 구조화된 에디터와 결합하면 실제 소프트웨어 개발에도 통할 수 있을 만큼 코드가 간결하고 일목요연할 수 있다는 의견
     * PostScript가 후위표기법만으로도 두뇌를 재구성할 정도로 강력한 언어라고 느낀 경험 공유, 여기에 미학적 비주얼이 결합되는 것이 신선하다는 찬사
          + 다른 비슷한 두뇌 훈련 언어 추천 요청, ps나 lisp 같이 다양한 언어를 짧게라도 익혀보고 싶다는 바람 표출
     * 이 방식이 다양한 용도가 있을 수 있지 않냐는 생각, 예를 들면 더 예쁜 QR코드 형태로 활용 가능, 자동 디코딩은 약간 까다롭지만 미적 감각과 논리의 결합이 마음에 든다는 평
     * chaos magick(카오스 마법)과 시길 주문 기초에 대해 더 알고 싶다면 Psychonaut Field Manual을 참고하라는 정보 제공
"
"https://news.hada.io/topic?id=21003","GitHub Copilot 코딩 에이전트, 공개 프리뷰 출시 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   GitHub Copilot 코딩 에이전트, 공개 프리뷰 출시

     * GitHub가 Copilot 코딩 에이전트를 Copilot Pro+ 및 Enterprise 사용자에게 프리뷰로 공개함
     * 개발자들이 반복적이고 기술 부채가 쌓인 작업을 Copilot에게 위임해 더 창의적이고 중요한 일에 집중할 수 있게 해줌
     * 이슈를 AI에게 할당하면, 코드 수정, 테스트 실행, PR 생성까지 자동 수행
     * Copilot이 작업을 완료하면 검토를 요청하며, 개발자는 추가 변경사항을 댓글로 요청하거나 브랜치에서 직접 작업을 이어갈 수 있음
     * 작업은 GitHub Actions 기반의 클라우드 개발환경에서 이루어지며, 테스트와 린터 통과도 자체적으로 검증함
     * 사용자는 PR에서 코멘트로 Copilot에게 수정 요청하거나 로컬 브랜치로 가져와 협업 가능
     * 주로 테스트가 잘 갖춰진 코드베이스에서 기능 추가, 버그 수정, 리팩토링 등 중저난도의 업무에 강점을 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GitHub Copilot coding agent in public preview

  코드 에이전트를 통해 기술 부채를 덜고 창의적 업무에 집중 가능

     * GitHub는 Copilot 코딩 에이전트를 퍼블릭 프리뷰로 공개해, 반복적이거나 단순한 이슈를 Copilot에게 위임할 수 있도록 함
     * 개발자는 이슈를 일반 개발자처럼 Copilot에게 할당할 수 있으며, GitHub 웹사이트, 모바일 앱, CLI에서 지원됨
     * Copilot은 자체 클라우드 개발환경에서 리포지토리를 분석하고, 수정사항을 적용하며, 테스트와 린트 검증까지 수행 후 PR을 생성함
     * 완료 후에는 사용자에게 리뷰를 요청하며, PR 내 코멘트로 피드백을 주거나 로컬에서 직접 브랜치를 이어 작업할 수도 있음

  어떤 작업에 적합한가

     * Copilot은 기능 추가, 버그 수정, 테스트 확장, 리팩토링, 문서화 개선 등 저~중간 수준의 복잡도 작업에 강점을 가짐
     * 테스트가 잘 갖춰진 코드베이스에서 효과적으로 작동하며, 동시에 여러 개의 이슈를 할당하는 것도 가능함

  사용 조건 및 요금

     * 해당 기능은 Copilot Pro+ 또는 Copilot Enterprise 요금제에서 사용 가능함
     * Enterprise의 경우, 관리자가 ‘Copilot 코딩 에이전트’ 정책을 사전 활성화해야 사용 가능함
     * 에이전트의 사용은 GitHub Actions 시간과 Copilot Premium 요청을 소모함
          + 특히, 2025년 6월 4일부터는 모델 요청 1건당 Premium 요청 1회가 과금됨

  플랫폼 지원 및 시작 방법

     * 이 기능은 현재 GitHub Mobile(iOS/Android) 및 GitHub CLI 사용자에게 점진적으로 배포 중
     * Copilot 코딩 에이전트 문서에서 시작 방법 및 팁 확인 가능
     * 의견이나 질문은 커뮤니티 토론에서 공유 가능

   vscode 인사이더에서 쓰고 있는데 점점 발전해가고 있어서 굉장히 편합니다.
   요새는 예측 코딩이 되더라구요.

        Hacker News 의견

     * Copilot은 잘 테스트된 코드베이스에서 기능 추가, 버그 수정, 테스트 확장, 리팩토링, 문서 개선 같은 저~중간 난이도 작업에 효과적이라는 인상 받음. 하지만 인간에게 중요한 부분은 AI 사용 시 경계 유지라는 점임. 테스트가 AI로만 만들어진다면 실제 제대로 작동하지 않을 수 있다는 걱정 있음. Microsoft 내부에서 얼마나 성공적으로 사용 중인지 구체적인 수치를 듣고 싶음. Microsoft가 실제로 자사 제품을 실사용(도그푸딩)하는 것으로 유명하지만, 엄청난 마케팅과 진짜 유용성을 구분하긴 매우 어렵다는 느낌 가짐
          + GitHub와 Microsoft 곳곳에서 Copilot coding agent를 내부적으로 거의 3개월 정도 실제로 사용 중임. 이런 경험을 통해 많은 피드백과 버그 개선이 이뤄져 오늘 에이전트 출시 준비를 마침. 지금까지 약 400명의 GitHub 직원이 300개 이상의 리포지토리에서 agent를 사용했고, 1,000개 가까운 Copilot이 기여한 PR이 머지됨. agent가 개발되는 리포지토리에서는 Copilot agent가 5번째로 많은 기여자임. 즉 Copilot coding agent를 써서 Copilot coding agent를 만들고 있는 셈임. (나는 Copilot coding agent의 GitHub 내 프로덕트 리드임)
          + Microsoft 내에서는 매니지먼트 주도의 강압적 배포라는 인상 받음. Azure 팀의 친구 얘기로는 내부 AI 코딩 어시스턴트를 설치 거부했다가 PIP(성과 개선 프로그램)에 올라갈 뻔한 사례 있음. 각 관리자가 'AI 쓰는 개발자 수'를 OKR로 잡고 있고, 많은 개발자들은 설치만 해놓고 거의 사용 안 하는 경향 있음. 특히 C#과 PowerShell 지원이 많이 부족해서 실제 쓸모가 제한적이라는 아쉬움 있음
          + 실제로 마이크로소프트가 AI로 코드를 생성한 비율 등 수치를 발표한 바 있음. 30%의 코드가 AI로 작성되고 있다고 알려짐
          + 마이크로소프트가 도그푸딩으로 유명했다는 말은 15년 전까지는 맞았지만, 지금은 전혀 아님
     * Copilot을 사용하면 프라이빗 리포지토리의 코드가 학습에 쓰일 수 있다는 점이 매우 큰 문제라고 경고하고 싶음. 프로, 프로+ 요금제가 있지만, FAQ에는 Business나 Enterprise 데이터는 학습에 쓰지 않는다고만 써놨기 때문에, 개인 유료 요금제의 데이터는 여전히 모델 학습에 사용된다는 의미로 받아들임
          + 예전에는 그랬을 수 있지만, 지금은 달라짐. 깃허브 공식 문서에서 개인 플랜 정책 확인할 수 있음
          + 윈도우 환경에서 코딩 중이라면 이미 화면이 몇 초마다 자동으로 캡처되고 있고, OCR로 내 화면의 모든 문자가 분석되고 있음. 이런 걸 모르면 놀랄 소식임
     * Gemini 2.5 pro와 cline으로 그린필드 프로젝트에서 vibe coding 실험을 해봄. 상당히 인상적이고 기존 LLM 챗 인터페이스보다 생산성에 큰 도움 받음. 그러나 아키텍처 가이드가 충분히 강하지 않으면 LLM이 잘못된 추상화와 기술 부채를 쌓는 경향이 있음(예: 구조 파괴). 코드 품질이나 더 나은 방법에 대한 자기성찰은 충분하지 않은 편임. 내가 명확히 지적해서 프롬프트 하면 바로 개선한다는 점이 장점. 그리고 LLM 토큰 비용이 하루 저녁에 $15나 소비된 점은 놀라움. 평소 한 달 평균 $20 정도였는데 하루 만에 이렇게 나온 건 처음임
          + LLM 토큰 하루에 $15 썼다는 건 버그가 아니라 특징임. 앞으로 ""AWS 요금 폭탄"" 현상이 LLM에도 나타날 거라고 생각함
          + Aider라는 툴을 사용해서 /add, /drop, /clear로 컨텍스트를 적극적으로 관리해보는 것도 추천함
          + Cline을 가격 민감하게 쓸 거라면 컨텍스트를 수동으로 관리해야 할 필요성 느낌. 나는 Windsurf(여전히 Gemini 2.5 pro 사용)를 대신 쓰고 있음. 컨텍스트 관리가 훨씬 간편함
          + 그린필드 프로젝트에서는 AI 활용이 불편함. 선택지가 너무 많아 AI가 방식들을 오락가락함. 브라운필드(기존 코드베이스)에서는 레퍼런스 파일을 제공해 자연스럽게 패턴 학습시킬 수 있어, 훨씬 쉽게 좋은 결과 도출 가능함
          + LLM의 아키텍처 오염 방지에 관심 있음. 다음 단계로 구현체가 설계 정의에 맞는지 확인해주는 (AI 기반) 린터가 등장할 것이라는 기대 있음
     * 기능 추가보다 속도 최적화가 먼저라고 봄. Copilot의 오토컴플리트는 빠르지만, 100줄 파일 편집에 몇 분씩 걸릴 때도 있어서 비생산적 경험 느낌. 100%에 가까운 적중률이 있다면 이해하지만, 느릿한 속도에 왔다갔다 하는 건 힘듦. 차라리 새 탭에 Claude나 ChatGPT로 질문-코드 복붙이 더 빠름. Copilot 구독 취소했고 앞으로 자동완성/간단 작업엔 로컬 모델로 갈아탈 예정임
          + 내 경험은 정반대임. 수백 줄짜리 파일 편집도 몇 초면 완료됨. 예전엔 느렸던 듯하지만 최근엔 병목 현상이 사라짐. 라이브러리 와이파이로 Copilot 써도 상당히 쾌적함
          + 몇 분이 걸린다면 심각한 문제가 있다고 생각함. 대부분 모델은 수 초 내로 처리함
     * VS Code에서 ChatGPT와 Copilot을 번갈아 씀. Objective-C의 문법 파악이 훨씬 쉬워지고 라이브러리 지원은 부족하지만 3rd party 라이브러리는 내가 충분히 도전 안 한 측면도 있다고 생각함. 문법, 흐름 오류는 단번에 알아볼 수 있어 짧게 수정 후 거의 코드를 바로 사용함. 월 $10 가격에 이 정도면 미래가 긍정적으로 느껴짐. 업데이트해야 할 iOS 앱이 매우 많은데 모두 생산성 앱이고 직접 쓰고 팔고 있음. 그래서 이득이 두 배임
     * Copilot을 꽤 많이 써봤음. 인상적이지만 무섭기도 함. 중요한 문제는 작은 리포에서 가져온 임의의 의존성을 무분별하게 추천하고, 그 중 많은 경우 주요 프로젝트에는 부적합하다는 점임. 즉, 사용자는 주의가 필요함
          + 여러 AI에서 유사 패턴을 봤음. 웹에서 읽어온 데이터에 지나치게 신뢰를 줌. 예시로 피싱 사기 검증 요청 시, AI는 내용 요약만 해주고 신뢰할 만한 분석이 아님. 또한 별 2개짜리 무명 중국 리포를 업계 표준처럼 권장하는 경우도 경험함. README에 써 있다는 이유뿐임. 관련없는 얘기이긴 하지만 ""Strobe"" 암호화 프로토콜을 추천하며 strobe.cool을 안내한 적도 있는데, 그 사이트 자체가 환각을 유도하는 것을 다루는 곳임
          + 이 현상에 대해 언급해줘서 고마움. 테스트 중에 이런 동작은 경험하지 못해서 좀 더 깊이 알아보고 싶음. 혹시 이메일로 공유할 수 있으면 좋겠음(github.com의 내 HN 닉네임). 나는 Copilot coding agent의 제품팀에서 일함
          + PR 실행이 프라이빗 리포에서는 더 신뢰성 높은 컨텍스트에서 동작하는데, 이런 상황에서 위와 같은 의존성 추천 문제는 좀 걱정스러움
     * ""Copilot은 저~중복잡도의 작업에 강함""이라는 말에 좋은 인상 받음. 하지만 ""잘 테스트된 코드베이스"" 한정이라는 점에서는 기대감이 사라짐
          + 다른 댓글처럼 coding agent는 테스트 커버리지 개선에 탁월함. 그리고 한 단계 더 들어가면 에이전트형 코딩 도구들은 이미 좋은 테스트 커버리지가 있으면 훨씬 큰 효과 얻음. 테스트는 agent(에이전트)를 제한(박스 인)하고 스스로 작업 검증을 반복할 기회 제공함. 이런 도구엔 꼭 필요는 아니지만, 있으면 더 나은 성과 나옴(나는 Copilot coding agent 팀에서 일함)
          + Copilot에게 모든 테스트를 작성하게 하면 금방 잘 테스트된 코드베이스가 형성됨
          + 내 경험상, 테스트가 없어도 특히 그린필드 프로젝트에서는 꽤 잘 동작함. 다만 이미 테스트가 있을 때의 업데이트/패치 효과가 확실히 더 좋음
     * ""기술 부채에 허우적대나?""라는 광고 문구에, 그냥 포기하고 침몰하라며 농담 섞어 반응함. Github Copilot Coding Agent로 기술 부채가 더 많아지고, 누가 책임질지 모르는 새로운 기술 부채가 쌓이고, 동료들도 곧 같은 상황으로 따라올 거라고 재치있게 말함
     * 내 친구가 GitHub에서 관련 프로젝트에 참여 중인데, 며칠째 이 소식만 들었음. 월요일 키노트 꼭 봐야 한다고 반복해서 귀에 못 박힘. 세 번째 인증 타임아웃 후 스트리밍 포기했는데, 이 주제인 걸 알았으면 한 번 더 시도했을 것 같음
          + 어떤 키노트인지 구체적으로 궁금함. 지금까지는 검색에도 잘 안 나옴
          + 한 가지 조언하자면, 그냥 유튜브로 가서 MS 회원가입 절차 건너뛰는 것이 좋음
          + 현업 코더들의 얘기는 내부 마케팅이 심하게 들어갔기 때문에 늘 조심스럽게 듣는 편임. Cursor 같은 경쟁작을 압도해주길 바라고, 실황을 꼭 볼 생각임
     * LLM 초기에 github actions와 issues workflow로 agent 직접 만들어서 썼음. 기능 제한적이었지만, 버그 지정만 하면 자동으로 동작 실행, 아키텍처/편집 과제 처리, 변경사항 검증, 마지막엔 PR 전송까지 했음. 이제 공식 도구로 유사한 걸 쓸 수 있어 기대감 있음(내 작업 샘플: chota)
"
"https://news.hada.io/topic?id=21029","구글 I/O 2025에서 발표한 모든 것","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         구글 I/O 2025에서 발표한 모든 것

     * 기존 Assistant를 대체하는 Gemini 기반 음성·카메라·웹 통합 에이전트 기능인 Gemini Live를 선보였으며, Gmail에는 개인 스타일을 반영한 AI 답장 기능이 도입됨
     * 개발자용 Jules 비동기 코딩 에이전트 및 창작 도구인 Imagen 4 이미지 생성기, Flow 영화 제작 툴, Veo 3 영상 생성기가 새롭게 공개됨
     * 검색은 AI Mode로 대체되며, 챗봇 기반 대화형 검색, 가상 착용 시뮬레이션, AR 쇼핑 등 복합 질의에 대응하는 새로운 검색 패러다임을 제시함
     * Android XR 플랫폼은 일반 안경 형태의 AR 글래스와 삼성과의 협업 혼합현실 헤드셋 Project Moohan을 통해 스마트 안경 생태계를 확장함
     * 기후 재난 대응을 위한 AI 위성 'Fire Sat' 과 드론 배송 시스템 'Wing' 의 확대 계획도 발표됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gemini Juices Up

     * Google Assistant는 사실상 Gemini로 전면 교체되며, 음성·카메라·웹 탐색을 통합한 Gemini Live 기능이 핵심
     * 카메라가 보는 장면을 이해하고 기억하며, 음성 명령으로 검색·전화·정리 작업 등을 수행함
     * Gmail의 Personalized Smart Replies는 사용자의 문체와 어휘를 학습하여 이메일 답장을 자동 작성
     * 유료 서비스는 기존 AI Premium → Google AI Pro로 리브랜딩 ($20/월), 고급 기능은 Google AI Ultra ($250/월)로 상향
     * AI Ultra는 OpenAI의 ChatGPT Pro보다 $50 비쌈

Gemini Is an Artist, Actually

     * Jules: 비동기 코딩 에이전트
     * Imagen 4: 텍스처·글자 표현력이 향상된 이미지 생성 모델
     * Flow: 사진이나 일러스트 업로드 또는 텍스트 입력으로 AI 단편 영화 생성
          + 장면별로 액션을 설명하며 영상 제작 가능
     * Veo 3: 물리 엔진 이해력 향상으로 더 부드럽고 사실적인 영상 생성 가능

Search Goes Full AI Mode

     * 기존 AI Overviews에 이어 AI Mode가 정식 도입
     * 복잡한 검색어에 적합한 대화형 검색 탭으로, 챗봇 스타일로 응답 제공
     * 쇼핑 기능 포함: 사용자의 사진을 업로드하면 의류 가상 착용 이미지 제공
     * AR 기반 가구 배치 시뮬레이션 기능 포함 (예: 아이에게 적합한 러그)
     * 현재는 Labs 실험 기능으로 제공되며 확대 예정

Android XR Looks Ahead

     * Android XR 플랫폼으로 스마트 안경 및 MR 헤드셋 전략 공개
     * 시연에서는 안경에 문자, 지도, 사진, 실시간 번역 등 AR 인터페이스 표시
     * 디자인 파트너로 Gentle Monster, Warby Parker와 협업
     * 삼성과의 협업으로 제작된 Project Moohan MR 헤드셋은 2025년 말 출시 예정

Disaster Averted

     * Fire Sat: AI로 초기 산불 감지하는 위성망 계획. 현재는 위성 1기만 가동 중
     * Wing: 허리케인 Helen 당시 의약품·물자 드론 배송 서비스로 활용됨
     * 구글은 AI 에너지 소비에 대한 우려 속에 기후 대응 프로젝트도 함께 강조함

   Google I/O '25 키노트 요약 보고서
   이것도 함께 보세요.

   jules는 다른거랑 잘못 적힌거 같네요.

   Developer Keynote 1시간 10분
   얘도 그냥 TV 틀어놓고 보기 괜찮네요, 데모들만 계속 보여줘서.
"
"https://news.hada.io/topic?id=20980","Obsidian을 버리고 직접 만든 경험","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Obsidian을 버리고 직접 만든 경험

     * 지식 관리 시스템(PKMS)의 한계와 우려에 대한 고민에서 시작, Obsidian 등 상용 PKMS의 비용, 폐쇄성, 장기 안정성 문제를 지적하며 직접 솔루션을 만들기로 결심함
     * Markdown 기반 웹 PKMS를 직접 구축하여 장점으로 보안, 확장성, 이식성, 장기적 데이터 소유권을 강조함
     * 커뮤니티와 상용 앱의 편의성도 고려하지만, 진정한 맞춤형과 데이터 제어가 더 큰 만족을 줌
     * PKMS DIY 경험이 다른 개발자들에게도 영감을 줄 수 있음을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

시작하며: 지식 관리와 불안감

     * 지식과 경험을 체계적으로 기록·정리하는 목적은 오래전부터 이어져온 주제임
     * 개인적인 지식 기록(PKMS)에 대한 프라이버시, 장기적 지속성, 과도한 커스터마이즈 등 여러 불안감 존재함
     * 오랜 시간 동안 오브시디언 등 PKMS를 사용해왔으나, 한계와 우려로 인해 독자적인 방식 모색 필요성 느낌
     * 본문은 ""내 방식""을 제안하려는 목적이 아니라, 기존 틀에서 벗어나도 괜찮다는 점을 공유하는 데 의미가 있음

기존 PKMS의 한계와 고찰

     * PKMS(개인 지식 관리 시스템)는 인생에서 얻는 영감 및 아이디어를 정리할 수 있는 ""두 번째 두뇌""로 비유됨
     * Notion, Obsidian, Evernote, Logseq 등 다양한 앱이 존재하지만, 상용 플랫폼의 존속 불확실성과 사생활 침해 우려가 있음
     * 대부분 사용자는 PKMS가 30년간 지속할 수 있을지 확신하지 못하는 현실 직면함
     * 시스템 구축에 시간을 너무 많이 소비하는 경우도 빈번히 발생함

Obsidian 경험과 전환 배경

     * Obsidian은 로컬 노트 저장, 내부 링크, Plugin 확장성으로 강력한 도구였음
     * 대표적 Plugin인 Dataview로 강력한 데이터 필터링 기능 구현 가능함
     * 하지만 기기 간 동기화 지원 유료화($8/월) , 오픈소스가 아닌 구조로 인한 실망, 장기 비용 부담이 문제로 부각됨
     * 사용 중인 Plugin의 장기 지원 및 앱 존속에 대한 불확실성 역시 우려의 원인임
     * 타 PKMS로의 ""노트 이주"" 반복에 피로감 느낌

나만의 노트 금고 설계 기준

     * 새로운 PKMS에 원하는 조건: 간편성, Plugin 유사 확장성, 강력한 보안
     * 개인 정보와 노트의 악용(광고·AI 학습 등) 위험에 대한 불안으로, 데이터 완전 소유권 요구
     * 상용 서비스에 데이터 저정 불신에서 비롯해 자체 구축 결심함
     * 상용 PKMS 대신 직접 구축하는 개발자가 적은 이유는 커뮤니티의 크기와 홍보 효과 차이로 풀이함
     * 실상 직접 구축해보니 ""생각보다 매우 쉬웠던 경험""으로, 도전할 가치가 충분함

내가 만든 PKMS의 구조와 기능

     * 웹에서 작성·수정·미리보기 가능한 마크다운 노트 시스템 구축
     * 모바일에서도 즉시 동기화된 노트 접근 가능하며, 별도의 월 사용료 없음
     * 온라인 호스팅임에도 복수 보안 레이어 적용으로 프라이버시 강화함
     * 모든 노트는 마크다운 텍스트 파일 형태로 DB에 저장되어, 이식성 및 백업이 뛰어남

노트 금고의 장점

     * 유용한 정보의 지속적 수집·리뷰를 통해, 아이디어와 기억력 향상 및 주제 간 연관성 발견 경험 얻음
     * 디지털 PKMS 특유의 검색, 조직, 확장성이 아날로그 방식 대비 강점
     * AI 기술 발전으로 맞춤형 Plugin 개발 접근성이 높아져, 사생활 우려 없는 기능 추가 용이함
     * 오픈소스 툴 활용 또는 자체 알고리듬을 통해 개인 니즈에 맞는 기능 구현 가능함

실질적인 구축 방법

     * 데이터베이스 래퍼를 적극 활용하여 간결함과 보안성 동시 확보
     * 오픈소스 플랫폼인 Directus 선택, 인증·보안 내장 덕분에 매우 빠른 구축(하루 이하 과정) 경험함
     * SQL DB 및 Docker 경험자라면 금방 직접 구축 가능함

마무리와 시사점

     * 지식 관리 시스템은 정원처럼 지속적 관리와 개인화된 돌봄 필요함
     * 분석 마비·보안 불안·과도한 커스터마이즈 등 어려움 있지만, 진정 필요한 요건(단순함, 보안성)만 충족하면 성공적 운용 가능
     * 상용 PKMS와 달리 직접 구축한 시스템은 이주 피로, 반복 비용, 통제력 문제 최소화 가능
     * 1년 이상 직접 운용하며 아이디어 연결 및 정리 효율 향상 경험
     * 나만의 방식이 유효했듯, 개인 니즈에 맞는 PKMS DIY 접근도 충분히 실용적임을 강조함

부록: 보안 고찰

     * 종단간 암호화 미구현 상태 보완 필요성 자각
     * 민감 정보 입력은 극히 제한하고, 추후 암호화 기능 추가 예정임

     PKMS를 직접 쓸 수 있을 정도로 기술적인 사람이면 git을 써서 노트를 싱크하고 모바일 앱에서 바로 쓸 수 있는데, 그 가능성을 왜 안 썼을지 의아함.

     실제로는 Directus( https://directus.io/ ) 의 광고인 듯합니다 . 그는 Obsidian을 대체하기 위해 그것을 사용했습니다. 제가 그곳에 도착했을 때 가장 먼저 눈에 띄는 이미지는 ""무료로 시작"" 버튼이었습니다.

   개발자가 아닌 옵시디언 이용자들도 git이나 드라이브 등으로 무료 동기화 할 수 있다는 정보를 알고 있습니다.
   그게 귀찮으면 그냥 돈을 내고 쓰는 거죠

   옵시디언을 정말로 써본 건 맞는지 의심되네요

   해커 뉴스에서도 이상함을 느낀 사람들이 수많은 동기화 및 오픈소스 프로그램들을 알려주고 있네요
   무슨 지식 저장소를 쓸지 고민하는 분이라면 본문보다 해커 뉴스 댓글 보시면 많은 도움이 될 거 같습니다.

   옵시디언은 로컬 마크다운 파일 편집기인데 왜 서비스 종료의 걱정을 하는 지 모르겠네요.

   동기화 서비스에 대한 걱정인가요? 마크다운 파일을 직접 서버에서 관리하던 동기화 서비스를 구현하던 하면 됩니다. 이미 구현된 오픈소스 동기화 플러그인도 있습니다.
   저는 이미 제 서버에 couchdb를 설치해서 동기화하고 있어요. 원한다면 FTP/WebDAV/NFS 등 네트워크 파일 접근 기능으로 파일에 직접 액세스해도 됩니다.

   옵시디언이라는 툴의 업데이트 종료에 대한 고민인가요? 옵시디언은 결국 모든 노트를 마크다운 파일로 저장합니다. 마크다운은 어떤 텍스트 편집기에서도 열 수 있으며 뷰어 플러그인이 있는 편집기라면 깔끔하게 열람도 가능합니다.

   또 옵시디언은 커스터마이징 기능을 꽤 광범위하게 제공합니다. CSS를 직접 작성해서 툴의 여러 부분을 수정할 수 있어요.
   저 같은 경우엔 노트 부분의 너비를 늘리거나 attachments 폴더를 숨겨버리거나 하는 등의 CSS를 직접 작성해서 사용하고 있습니다.

   저는 옵시디언이 사용자에게 엄청나게 많은 권한을 주고 사용자의 통제 하에 두도록 허용한다고 생각합니다.

   기능이 부족해서 만들었다고만 했으면 이해가 됐을 텐데 필자가 실제 걱정하는 부분은 전혀 공감가지 않네요.
   동작 원리에 대해 모르는 걸까요? 옵시디언은 사용 시작부터 ""노트를 저장할 로컬 저장소"" 를 지정하도록 하지 않나요?

   옵시디언 반대에 공감합니다. nas에 joplin 서버 설치해서 markdown note 쓰고 있습니다. 데이터 동기화와 백업, 사유화를 모두 도달했어요 ㅋㅋ

   직접 만들생각까지 한건 오버한건 맞는거 같습니다만.. 사실 지식관리 시스템에서 외부 솔루션 때문에 제품 수명을 걱정해야 하는 부분이 큰 문제인건 맞는 거 같습니다.
   이걸 에버노트에서 첨 경험했고요 (물론 죽은건 아닙니다만 ㅎㅎ) 그 옵시디안도 일단 self-hosted로 하면 동기화 문제는 해결되지만 이걸 모든 사람들이 다 할수 있는 것도 아니니 문제는 문제죠...

   그리고 글 자체는... 저거 창업 준비하는거 아닌가 싶기도 하네요 ㅋㅋㅋ

   이맥스 하나만 있으면 이거저거 다 할 수 있지요. 최근에는 안드로이드에 설치도 되서 데스크탑 그 기능 그대로 활용하니 좋아요. 이맥스 지식관리 도구라는 주제로 깊히 파고 있습니다. 유치원 다니는 저희 아이가 초등학교가면 그때쯤엔 이맥스로 라이프로깅하겠지요ㅎㅎ 도구하나만 익히면 되니까 길게보면 고민을 줄이는 일 입니다.

   https://notes.junghanacs.com/

   https://notes.junghanacs.com/journal/20250428T000000#screenshot 안드로이드 버전 스크린샷은 여기 넣어놨네요. 쓰면쓸수록 묘한 도구 입니다. 커뮤니티도 긳하면서 놀라운면이 많구요.

   좋네요!
   제목을 “나는 왜 옵시디언을 만들었는가”로 바꾸고, 본문의 옵시디언을 노션,위키로 바꾸도 될 듯.. ㅎ

        Hacker News 의견

     * 내 PKMS는 여러 기기에서 노트를 관리하기 위해 온라인으로 호스팅됨. 노트 프라이버시를 위해 보안 계층을 여러 겹으로 적용함. 셀프 호스팅 환경에서 제일 추천하는 핵심 요령은, 집 네트워크에 VPN을 설정하고 밖에 서비스가 공개되지 않도록 하는 것임. 누군가 내 서비스 엔드포인트에 접속할 수 있다면 이미 VPN을 뚫은 것이고, 그러면 다른 더 심각한 문제들이 발생하는 상황임. 그렇게 하니 간단한 서비스들에는 굳이 또 인증을 붙이지 않아도 될 정도로 걱정 요소가 줄어듦
          + Tailscale은 이 부분을 훨씬 더 쉽게 만들어줌. 셋업이 간단하고, 방화벽 뒤에 있어도 신뢰도와 연결성이 훨씬 뛰어남. IoT VLAN 노출이나 전 인터넷 트래픽을 집을 거쳐 터널링하는 exit node 설정같은 것도 클릭 몇 번이면 됨. 기기/사용자별로 접근 제한도 매우 쉽게 걸 수 있어서, 예를 들어 가족에게 공유하면 안 되는 중요한 노트 앱에 접근 제어가 가능함. IP 기준으로 사용자와 기기 조회 후 역프록시 통해 인증 정보를 앱으로 전달하는 구조도 있음. 네트워크 운영권한을 위임하기 불편하다면 Headscale 서버 직접 운영도 가능함
          + Wireguard를 모든 모바일 기기에 설치해서 내 네트워크가 아닌 와이파이에 붙을 때 자동으로 시작되도록 설정함. 어디를 가든 집의 LAN에 있는 것과 동일한 환경 제공. 여러 자가호스팅 서비스를 운영하는데, Wireguard 없이는 원격 접근은 생각하기 어려움
          + Tailscale에서 집 안의 기기를 exit node로 설정하면 정말 훌륭함
          + Tailscale을 활용하면, VPN IP에 맞춘 DNS 레코드 설정과 역프록시 덕분에 app1.my-domain.com처럼 각각의 네트워크 앱을 접근 가능하게 할 수 있음. VPN이 연결된 상태에서만 접속 가능하고, SSL 인증서 세팅이 안 되어 있어서 브라우저 경고가 뜬다는 단점 있음. SSL 문제는 고칠 수 있겠지만, VPN 연결 필수는 조금 아쉬움
          + 나도 이 방식 사용 중임. VPN만이 유일한 보안책이 되어서는 안 된다는 점에는 동의함. 비공개 정보가 없는 서비스라면 Auth를 넣지 않아도 되겠지만, 패스워드 매니저처럼 민감한 서비스에는 VPN 하나만 신뢰하지 않음
     * Obsidian을 오랫동안, 이전에는 Evernote도 많이 썼던 입장에서, 몇 가지 반론이 있음. Obsidian이 20년 뒤에도 쓸 수 있을지 걱정돼서 떠난다는 건 오히려 Obsidian의 장점임. 에디터는 독점 소프트웨어지만 노트 파일 자체는 표준 markdown이라 어떤 에디터로든 쉽게 옮길 수 있음. 그리고 모바일에서 사용하려면 유료라는 점 때문에 떠났다는 것도, PKMS를 직접 쓸 수 있을 정도로 기술적인 사람이면 git을 써서 노트를 싱크하고 모바일 앱에서 바로 쓸 수 있는데, 그 가능성을 왜 안 썼을지 의아함. 나는 Gitea와 연동해서 노트를 어디서든 문제없이 쓰고 있음. Obsidian과 플러그인 아키텍처에 만족함
          + Obsidian은 이미 내가 쓰던 디렉토리와 파일 포맷 그대로라서 쓸 수 있었고, 만약 Obsidian이 없어져도 내 노트와 데이터 구조는 변함없이 사용할 수 있음
          + Dropbox 폴더에 두는 것만으로도 동기화가 충분히 가능함. 다른 복잡한 싱크 구조가 필요 없는데, 그 점이 저자에게는 한계로 다가왔던 것이 신기함. 그래도 새로운 시스템을 만드는 경험을 했다는 점은 멋짐
          + Obsidian이 “그냥 markdown”으로만 노트를 저장하는 건 사용법에 따라 다름. 플러그인을 많이 쓸수록 나만의 커스텀 문법이나 JS 기능이 쌓여서, 특정 플러그인 의존성이 생기기도 함. 그래도 여전히 큰 이점임을 유념함
          + Git도 좋은 싱크 솔루션이지만, 내 노트를 Github 식으로 공개 저장소에 넣는 건 불편하게 느껴짐. 지금은 database 파일과 markdown 파일을 로컬에서 싱크하는 방법을 테스트 중임. Vim을 너무 좋아해서 Directus의 마크다운 에디터로는 만족하지 못함
          + 나도 주기적으로 Evernote, Notion, Obsidian 등으로 5년마다 노트 시스템을 바꿔 온 게 지치게 느껴져서, 직접 나만의 시스템을 만들어보려 했음. 이리저리 시도 끝에 Emacs org-mode에 정착함. Git과 함께 쓰면 노트 관리가 아주 유연함
     * 오랜 기간 Obsidian의 모바일 싱크 유료 정책이 아쉬워서, Syncthing으로 vault를 기기간에 싱크함. PC에선 Syncthing이 항상 백그라운드에서 실행되고, 폰에서 수정 사항이 있으면 앱을 열어 싱크함. Obsidian 기본 연동만큼 매끄럽진 않지만, Git보다는 간편하고 파일이 원격 서버에 남지 않는 점이 장점임
          + 항상 켜있는 Raspberry Pi로 Syncthing을 돌려서, 랩탑과 폰이 동시에 온라인이 아니어도 싱크가 완벽하게 됨. SyncTrain iOS 앱을 활용함
          + Apple에서 iCloud 디렉토리를 영구적으로 내려받을 수 있게 허용해서 문제 해결
          + Obsidian 노트 싱크는 한 달에 $4면 사용 가능함
          + 나는 SyncThing Fork를 추천함. 안드로이드 기준으로는 Google Drive와 DriveSync 조합도 만족스럽게 사용 중임. 클라우드 제공자 대부분이 잘 동작함. Obsidian 싱크 관련 도구 비교글을 작성해 둔 적 있음
          + 동기화에 작은 문제는 있었지만 오픈소스라 비용이 없고, 직접 만든 것보다 Obsidian과 Syncthing 조합이 훨씬 낫다고 추천함. 플러그인 활용도가 높음
     * Obsidian 싱크는 Obsidian 서비스가 아니어도 다른 서비스를 쓰면 “무료”임. 나는 조금 더 편리함과 장애 지점 최소화를 위해 Obsidian 싱크를 유료로 씀. 중요한 툴에 10년간 1000불은 충분히 가치 있다고 생각함. 오픈소스와 진정한 장기 지속성을 목표로 한다면 Obsidian이 맞지 않지만, 가격 대비 가치에 의문이 없음. 그리고 5년 주기로 시스템을 바꾸는 일이 힘들다는 데 공감하지만, 이 정도의 재평가 주기는 현실적인 표준임. 20년 작업에도 쓸 수 있는 소프트웨어를 만들려고 한다는 저자의 시도에는 박수를 보냄
          + 유틸리티가 아주 좋은 툴이어도 1000불이나 줄 생각이 없는 경우가 많음. 물통이 대표적임. 음식점이나 공간 임대와 소프트웨어 서비스는 직접적인 비교가 적합하지 않다고 느낌. 기본 싱크 가격 1년에 50불 정도면 충분히 납득하지만, 협업용으로 5배나 더 내야 한다면 그건 부담스러움
     * Directus를 Obsidian 대체제로 썼다는 점이 살짝 광고처럼 느껴짐. 홈페이지에 “무료 시작” 버튼이 큰 이미지고, 자기 서버에 프로덕션으로 돌리려면 가격 안내 없이 문의 양식만 있음. 오픈소스라는 설명은 있지만, 실질적으론 SQL 데이터베이스 관리용 대시보드임. 개인적인 노트 시스템에 맞지는 않아서, 저자가 소개한 사용례와 안 맞는다고 판단함
     * 1년 반 전에 PKMS 세계를 깊게 파서 Obsidian과 여러 툴을 벤치마크 시도 후 Trilium에 정착함. Trilium은 호스팅과 배포 관련 요구사항을 번거로운 우회 없이 해결함. 노트가 정보 단위 원자이며, 속성 기반 구조화와 템플릿, 상속 등으로 일관되고 확장성 있는 관리가 가능함. Trilium은 외관상 단순해보이지만 실상은 매우 강력하며 접근이 쉬움. 로컬 우선, 클라우드 전용, 하이브리드 등 운용이 자유롭고 자체 싱크 프로토콜로 마스터-마스터 복제가 간단함. 오프라인과 온라인, 웹 기반 접근까지 모두 지원함
          + Trillium 기능이 궁금함. Logseq처럼 개별 블록을 참조하고 내장할 수 있는 outliner 모드가 있는지, 상위 항목에서 자식 내용까지 다 볼 수 있는 뷰 등이 궁금함
          + Trilium을 추천해준 사람이 또 있어서 이번 주에 꼭 써보려 함
     * 멋진 PKMS 사용 경험담이 있는지 궁금함. 나는 그냥 폴더만 써도 충분히 잘 됨. deep linking 등은 아직 잘 모르겠음
     * 이런 싱크/마이그레이션 문제들이 아직도 존재한다는 점이 신기함. 나도 Evernote에서 Emacs+org-mode로 넘어온 후 Orgzly와 Syncthing 조합으로 모바일 싱크 해결함. 이슈는 가끔 충돌이 발생할 때뿐이었는데, 파일을 노트/과제 등으로 분리해 해결함. 플러그인 없이도 검색, 태스크 갱신, 아카이빙만으로 잘 쓰는 중임. 만약 추가 기능이 필요하다면 Emacs는 최고의 편집기이고, org-mode가 이를 뒷받침해줌
          + Syncthing for Android 개발 중단이 아쉬움
     * 프라이버시와 비용에 대해 통제력을 높이고 싶었다고 하지만, Obsidian은 엔드투엔드 암호화에 월 4달러임. 자신만의 툴 빌딩은 분명 재밌지만 실용성만 보면 굳이 투자할 시간이 의문임
          + Obsidian 싱크가 내 경험상 너무 불안정했음. 연단위 요금제도 썼지만 기기별로 노트가 안 맞는 일이 반복됐음. 월 4달러는 매끄럽지 않은 싱크에선 아깝게 느껴짐
     * Yubikey 기반 개인 키로 내 markdown 파일 전체를 암호화할 수 있는 비공개 노트 솔루션을 찾는 중임. SOPS·age 조합은 터미널은 좋은데 모바일/GUI 접근성이 떨어짐. 키는 앱 파일에 저장해 비밀번호로 복호화하는 기존 엔드투엔드 방식은 보안성이 낮아서, Yubikey 터치로 문서별 복호화가 이뤄져야 함
          + 곧 암호화 쪽으로 더 깊이 알아볼 생각임. Triliumnext가 추천받은 곳이고 파일 암호화 기능이 있는 듯함. 보호 노트 관련 문서가 참고할 만함
"
"https://news.hada.io/topic?id=20949","Ollama, 멀티모달 모델을 위한 새로운 엔진 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Ollama, 멀티모달 모델을 위한 새로운 엔진 발표

     * Ollama는 새로운 엔진을 통해 멀티모달(텍스트+이미지) 모델 지원을 시작함
     * Llama 4 Scout와 Gemma 3 모델 등 다양한 시각 멀티모달 모델을 지원해, 이미지와 텍스트를 결합한 질문 응답이 가능해짐
     * 새로운 엔진은 모델 모듈성 향상, 정확도 개선, 효율적인 메모리 관리 기능을 제공함
     * 이미지 캐싱, 하드웨어 메타데이터 활용 등으로 빠른 추론 성능과 하드웨어 최적화를 이루고 있음
     * 앞으로 더욱 긴 컨텍스트 지원, 툴 호출, 스트리밍 등 다양한 기능 확장이 예고됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Ollama의 멀티모달 모델 지원

   Ollama는 새로운 멀티모달 엔진 도입을 통해, 이미지와 텍스트를 복합적으로 다루는 최신 비전 멀티모달 모델들을 지원함

  종합 멀티모달 이해 및 추론

    Llama 4 Scout

     * Ollama에서는 Llama 4 Scout(1090억 파라미터, mixture-of-experts 모델)을 지원함
     * 예시로, 비디오 프레임의 위치 기반 질문을 할 수 있음
          + 예) 에서 특정 건물, 환경 요소, 배경 정보 등 다양한 이미지 특징을 탐지함
     * 이어서 다양한 후속 질문도 자연스럽게 이어질 수 있음
          + 예) ""이 건물에서 스탠포드까지 얼마나 먼가?"", ""어떻게 가는 것이 가장 좋은가?"" 등의 질문에 정확한 정보를 제공함
          + 여러 교통 수단, 경로, 예상 소요 시간 등 실제 상황에 맞는 답변 제공

    Gemma 3

     * Gemma 3는 여러 이미지를 동시에 입력받아, 이미지들 간의 관계를 분석할 수 있음
          + 예) 4장의 이미지에서 공통적으로 등장하는 동식물, 특정 장면의 존재 여부, 특이 상황 등 다양한 조건을 빠르게 파악함
          + 재미있는 상황 예시로, 라마와 돌고래가 복싱하는 모습을 보고 누가 이길지 분석, 개체의 특징과 역동성을 파악함

  문서 인식 및 분석

    Qwen 2.5 VL

     * Qwen 2.5 VL 모델은 문자인식(OCR) 및 이미지 내 특정 텍스트 정보 추출에 활용됨
          + 실제 사용 예시로, 수표의 정보를 추출하거나, 봄맞이 대련과 같은 중국어 수직 글귀를 영어로 번역하는 작업을 처리함

Ollama 멀티모달 엔진의 특징

     * Ollama는 지금까지 ggml-org/llama.cpp 프로젝트에 의존하여 모델 지원을 해왔으며, 사용성 및 모델 이식성을 중심으로 개발함
     * 최근 다양한 연구소에서 멀티모달 모델을 공개하며, Ollama의 목표대로 더 폭넓은 모델 지원을 위해 엔진을 자체적으로 강화함
     * 새로운 엔진은 멀티모달 모델을 독립적이고 일급 객체로 취급하며, 파트너 및 커뮤니티 참여도도 높임

  엔진 발전의 의미

     * Ollama의 현지 추론의 신뢰성, 정확성 향상, 미래의 다양한 멀티모달 분야 지원의 기반 마련(예: 음성, 이미지 생성, 비디오 생성, 긴 컨텍스트 지원, 개선된 도구 활용 등)

    모델 모듈성

     * 각 모델의 “영향 범위”를 독립시켜, 신뢰성 개선 및 개발자가 새 모델을 쉽게 통합할 수 있도록 설계함
          + 기존 ggml/llama.cpp는 텍스트 전용 모델만 지원, 멀티모달에서는 텍스트 디코더와 비전 인코더가 분리되어 별도 실행됨
          + 이미지는 비전 알고리듬에서 임베딩 후 텍스트 모델로 전달되어야 하므로, 각 모델별 로직을 슬림하게 구현 가능하게 함
          + Ollama 내에서는 모델이 자체적으로 임베딩 투영 계층, 모델 고유 훈련 체계에 맞춘 분리가 가능함
          + 모델 제작자는 추가 패치나 복잡한 조건문 없이 자신의 모델과 훈련에만 집중할 수 있음
          + 일부 모델 구조 예시는 Ollama의 GitHub 저장소에서 확인할 수 있음

    정확성 향상

     * 대형 이미지는 토큰 양이 커서 배치 사이즈를 초과할 수 있음
          + 이미지가 배치를 넘을 경우 위치 정보가 무너질 수 있음
     * Ollama는 이미지 처리 시, 추가 메타데이터를 부여해 정확성을 높임
          + 인과적 어텐션 적용 여부, 이미지 임베딩 배치 분할 및 경계 관리 등 디테일하게 처리함
          + 분할 지점이 부적절하면 출력 품질 저하 우려, 모델별 논문 기준으로 기준점을 맞춤
     * 타 현지 추론 툴은 제각기 방식으로 구현하지만, Ollama는 모델 설계와 훈련 방식에 맞춘 정확한 처리로 품질을 보장함

    메모리 관리 최적화

     * 이미지 캐싱: 한 번 처리된 이미지는 계속 메모리에 저장되어 후속 프롬프트 처리 속도가 빨라짐. 메모리 한계에 도달하지 않는 한, 이미지는 유지됨
     * 메모리 예측 및 KV 캐시 최적화: 하드웨어 제조사 및 OS 파트너와 협력하여 하드웨어 메타데이터를 정확히 인식, 메모리 사용 최적화 추구
          + 펌웨어 버전별 검증 작업 수행, 새로운 기능에 대한 벤치마킹 진행
     * Ollama는 causal attention을 모델 단위로 별도로 최적화하며, 그룹 수준이 아닌 개별 모델에 맞춤 설정 제공
          + 예시:
               o Google DeepMind의 Gemma 3: 슬라이딩 윈도우 어텐션을 통해 일부 컨텍스트 길이만 할당, 나머지 메모리는 동시 추론 등에 할당
               o Meta의 Llama 4 Scout, Maverick 등: 청크드 어텐션, 2D 로터리 임베딩 등 지원, mixture-of-experts 모델의 긴 컨텍스트 지원 구현
     * 어텐션 계층이 완전히 구현되지 않은 모델의 경우 ‘동작’할 수 있지만, 장기적으로 출력 품질 저하, 비정상 결과 가능성 있음

    앞으로의 계획

     * 더 긴 컨텍스트 길이 지원
     * 추론/사고 능력 강화
     * 도구 호출 및 스트리밍 응답 제공
     * 컴퓨터 직접 활용 기능 확장

감사의 글

     * 모델 개발에 기여한 단체 및 연구자
          + Google DeepMind, Meta Llama, Alibaba Qwen, Mistral, IBM Granite 등 비전 모델 개발에 힘쓴 여러 실험실 및 커뮤니티 구성원들에게 감사함
     * GGML
          + GGML 팀의 텐서 라이브러리는 Ollama의 추론 엔진을 이루는 핵심 요소임. Go에서 GGML에 직접 접근해 커스텀 추론 그래프, 복잡한 모델 아키텍처 설계에 활용 가능함
     * 하드웨어 파트너사
          + 다양한 장치에서의 추론 성능 향상에 협력해준 NVIDIA, AMD, Qualcomm, Intel, Microsoft 등 하드웨어 파트너들에게 감사함

        Hacker News 의견

     * 이 시점에 Ollama에서 새로운 엔진 발표 소식을 듣고 놀라움 느낌 표현, llama.cpp가 드디어 안정적인 비전(vision) 기능을 기본 브랜치에 포함해 오랜 노력이 마침내 결실 맺은 데에서 유래된 감상 공유, Ollama가 이미 오랜 기간 이 기능을 준비해온 듯한 추정, llama.cpp에 대한 초기 의존성을 깨고 독립적으로 나아가는 결정이 합리적 판단이라는 생각
     * 두 프로젝트에서 멀티모달 기능을 추가한 실질적 차이점이 무엇인지 궁금증 표출, LLaVA 지원은 오랫동안 있었으므로 기존에는 특수한 처리 방식이 필요했던 것인지 의문, TFA에서 그 차이에 대한 언급을 기대했으나 Ollama에서의 멀티모달이 완전히 새롭게 도입된 것처럼 다루는 데에 혼란 느낌
     * 멀티모달이라는 용어는 텍스트, 이미지뿐 아니라 오디오(그리고 잠재적으로 비디오)까지 포함하는 개념이어야 한다는 생각, 단순히 이미지 생성 혹은 이미지 분석 기능만 있는 모델이라면 ‘비전 모델’이 더 정확한 명칭이라는 주장, Qwen2.5-Omni와 Qwen2.5-VL과 같이 멀티모달 모델을 명확히 구분할 필요성 강조, Ollama의 새로운 엔진은 이런 의미에서 '비전' 지원을 추가했다는 설명
     * 비디오 입력을 다루고 싶은 관심 표명, Qwen2.5-Omni와 Ollama에서 비디오 입력이 가능한지 문의
     * Ollama의 ‘새로운 엔진’에 관한 설명이 많이 언급되지만 실제 구현 방식에 대한 구체적 정보가 보고 싶다는 소망, llama.cpp도 대단한 프로젝트이기 때문에 그 대체 엔진을 만들었다면 어떤 방식으로 구현했는지 예시를 보고 싶은 기대감, GGML 텐서 라이브러리가 핵심 역할을 하는 것으로 추정, Go 언어에서 FFI(이종 언어 함수 호출)을 통해 직접 모델 동작(예: Gemma3 구현)을 작성하면서 GGML 기능을 활용하는 구조로 파악, 이런 기술적 세부사항이 공식 블로그에 더 명시적으로 담겼어야 한다는 생각
     * Ollama는 그동안 투명성 부족, 불투명한 기여(credit) 표기, 사용자 중심이 아닌 결정 등으로 비판을 받아왔던 기업 이미지, 이번 글에서는 오히려 기여자 표기가 많아져서 놀라움 느낌, 사용자들의 비판이 많아 조정이 이뤄지고 있다는 추측
     * LLM 세계에서 ‘*llama’ 네이밍 관행이 너무 혼란스럽게 느껴진다는 고백, 여러 가지 llama와 유사한 이름의 프로젝트들이 난무하여 혼동 심화
     * AI/ML의 발전 속도가 너무 빠르게 전개되어서 따라가기가 어렵다는 어려움 공유, 주목하지 않으면 제대로 파악하기 힘들다는 점과 ‘밈’(memey) 이름 선호 경향 언급, 이전에는 세서미 스트리트 캐릭터, YOLO 모델군 등 다양한 유행이 있었고 학회 논문도 예외가 아니라는 일화
     * 약간 옆길로 새서 Ollama가 일부 사용자들로부터 부정적으로 평가받는 이유에 관한 의문 제기, 직접 llama.cpp를 돌리라는 주장 이상의 설명이 잘 없었던 점을 지적
     * Reddit과 GitHub 이슈 링크 공유를 통해 Ollama가 llama.cpp에 제대로 크레딧을 주지 않는 오랜 문제점이 존재한다는 사실 소개, 심지어 일부 프로젝트에서는 llama.cpp를 직접 사용하면서도 Ollama에 그 공이 돌아가는 현상이 있다고 지적, Ollama가 직접 기여하지는 않지만(의무사항은 아님) 내부적으로 유지되는 포크가 있어 관심 있는 사람은 원할 때 cherry-pick 방식으로 코드 활용이 가능한 구조
     * 앞서 제기된 문화/라이선스/FOSS와 별도로, 파일 저장 방식에 대한 불만 표출, Ollama는 자체적인 디스크 저장 및 레지스트리를 도입해 재사용이 불편해진 점 지적, 장기적으로 수익화를 염두에 두고 독점적인 구조를 설계한 의도 추정, Docker처럼 중복 저장을 막으려는 목적일 수 있지만 실제로는 사용성만 악화, 결과적으로 30GB 이상 대용량 파일을 중복 보관하는 번거로움이 발생해 사소한 문제도 크게 다가옴, 다양한 에코시스템에 호환되는 표준 방식이 더 나음, Ollama는 불편함으로 인해 사용하지 않게 됨
     * Ollama를 Docker와 유사한 LLM 세계의 솔루션으로 평가, 사용자 경험과 모델 파일 문법 역시 Dockerfile에서 영감을 받았다는 인상, Docker 초기에도 Docker와 LXC 논쟁 있었지만 Docker의 사용자 경험 혁신성이 간과되었던 일화 기억, 다만 llama.cpp에 대한 오랜 기간 인정 부족은 문제로 봄, 현재는 어느 정도 오픈된 크레딧 표기가 있다는 점 첨언
     * Ollama가 커뮤니티와 협력하지 않는다는 점이 불만, VC로부터 자금을 받은 기업이어서 수익구조에 대한 의문이 여전히 남아 있음, llama.cpp, lmstudio, ramalama 등 다른 대안에선 각자의 상황을 명확히 알 수 있는 구조, ramalama는 다양한 관련 오픈소스에 기여가 많은 편, 참고할 만한 GitHub 링크 제공
     * Ollama가 단순히 llama.cpp의 프론트엔드 역할임에도 이를 드러내지 않고 인정하지 않는 태도가 아쉬움 포인트
     * Ollama 예시 중 ‘수직 중국 춘련 번역’ 사례에 오역이 다수 있다는 지적, 블로그 작성자가 실제 중국어 사용자가 아니라고 추정, 각 부분별로 실제 내용과 Ollama 결과가 어떻게 차이났는지 구체적으로 분석
     * 해당 예시를 수행한 유지관리자가 직접 등판, 자신이 중국인임을 명확히 밝혀 신뢰도 보강, 영어 번역 자체는 꽤 정확했다고 판단, 모델의 오류나 데모를 숨기거나 조작하지 않음을 강조, 장기적으로 모델의 품질이 더 개선되기를 바라는 희망 공유
     * 직접 사용해 볼 예정, 실용적인 예시와 세부 정보가 곧바로 드러나서 기사 방식이 좋다는 평가
     * Ollama의 강점은 별다른 설정 없이 간단한 Docker 커맨드만으로 바로 모델을 실행할 수 있었던 점, 하지만 이미지와 비디오를 활용해야 하는 경우 Docker가 GPU를 사용하지 않으므로 기술적 제약이 발생, Ollama에서 Docker 연동 지원을 향후 어떻게 유지할지 궁금증, 혹시 이 기능이 프로젝트의 비중 낮은 부차적 요소로 전락하는 것은 아닌지 의문
     * 일부 플랫폼에선 Docker에서 GPU를 사용할 수 있다는 의견 제시, 다만 더 많은 설정이 필요하고 nvidia가 관련 문서를 제공
     * 예시 중 스탠포드 이동 경로 안내에서 실제로 잘못된 정보가 나왔다는 점이 재미있다는 감상, CA-85가 Palo Alto에서 더 남쪽이라는 교통상식 공유
     * 1년 가까이 Ollama로 로컬 모델을 사용하며 만족감을 느낌, 하지만 Llava 등 멀티모달 지원 기능은 대부분 텍스트 위주로 활용해 거의 경험하지 못했다는 설명, 멀티모달 로컬 모델로 구축된 유용하고 멋진 프로젝트 추천 요청, 개인적인 프로젝트 아이디어를 찾고 싶은 소망
"
"https://news.hada.io/topic?id=20995","철보다 강한 목재를 대량생산하려는 InventWood","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     철보다 강한 목재를 대량생산하려는 InventWood

     * 메릴랜드의 연구실에서 철보다 강한 목재 기술이 개발됨
     * 이 기술은 InventWood가 상용화를 추진하여 올여름 첫 Superwood 배치 생산 예정임
     * Superwood는 셀룰로오스를 강화하며, 강도와 내구성 측면에서 혁신적 특성 가짐
     * Class A 내화 등급 및 내구성 덕분에 건축 재료로 기대를 모음
     * 향후 건축물 주요 구조 부재까지 적용 범위를 확대할 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * InventWood는 메릴랜드 대학교의 재료 과학자 Liangbing Hu가 개발한 획기적인 목재 강화 기술을 상용화하고 있는 스타트업임
     * 2018년 Hu 교수는 일반 목재를 여러 처리 과정을 통해 철보다 강한 재료로 전환하는 방법을 개발함
     * 초기에는 실험실 성과에 불과했던 기술이었으나, Hu 교수는 몇 년에 걸쳐 이 기술의 생산 속도를 대폭 향상시켜 수일 만에 대량 생산이 가능하도록 개선함
     * 이 기술은 InventWood에 공식적으로 라이선스되었으며, 상용화 준비를 마침

Superwood의 상용화와 특징

     * InventWood는 올해 여름부터 Superwood의 첫 상업용 배치를 생산할 예정임
     * 초기에는 건축 외장재(피복재) 분야에 집중하나, 장기적으로는 건물 구조체까지 적용 확대 목표를 세움
          + 전 세계적으로 건설 시 탄소 배출의 90%가 콘크리트와 철강에서 발생하므로, 친환경 대체재로 큰 의의 가짐
     * 시리즈A 펀딩에서 1,500만 달러를 유치하였으며, Grantham Foundation, Baruch Future Ventures, Builders Vision, Muus Climate Partners 등이 주요 투자자로 참여함

Superwood의 기술적 원리

     * Superwood는 셀룰로오스와 리그닌으로 이루어진 일반 목재에서 출발함
          + 셀룰로오스 나노 구조체는 탄소섬유보다도 더 강한 성질을 지님
     * 제작 과정
          + 식품산업용 화학약품을 사용하여 목재의 분자 구조 일부를 변경함
          + 압축 처리로 셀룰로오스 분자 간 수소 결합을 대폭 증가시킴
          + 기존 목재를 네 배 이상 압축할 경우, 단순히 섬유량이 많아지는 것 이상의 결합이 생성되어 실제 강도는 10배 이상 증가함
     * 결과적으로, Superwood는 철보다 인장 강도가 50% 높고, 중량 대비 강도는 10배에 달함
          + 최고 수준의 내화성(Class A) , 뛰어난 방부·방충성도 갖춤
          + 폴리머 함침시 옥외용 판재, 데크, 지붕 등에도 안정적으로 활용 가능함

시각적·경제적 가치

     * 소재를 압축하는 과정에서 색상이 농축되어, 값비싼 열대 하드우드와 비슷한 아름다운 외관을 구현함
     * 향후에는 목재칩을 이용해 다양한 규격의 구조용 빔도 제작할 예정임
          + 별도의 후가공이나 도장 없이도 높은 품질과 고급스러운 외관을 보장함
          + 실제 샘플은 호두나무, 이페 등 고가 수종 특유의 색감을 자연 그대로 드러냄

결론

     * InventWood의 Superwood는 기존 건설 자재 대비 친환경성, 강도, 내구성, 디자인 측면에서 모두 혁신적 가치를 창출함
     * 향후 전통적인 철강·콘크리트 대체재로 발전 가능성이 기대되는 차세대 목재 소재임

        Hacker News 의견

     * InventWood가 목재칩을 사용해 별도의 마감이 필요 없는 다양한 크기의 구조용 빔을 만들 계획이라는 이야기와 함께, “Superwood는 월넛이나 이페 같은 자연스러운 아름다운 색상 모양을 지닌다”라는 설명을 들으면서 실제 사진 공개 요청을 하게 되는 심정
          + 제품의 미적 특성을 내세우는 회사임에도 실제 샘플 이미지가 한 장도 없다는 사실에 큰 불신을 느끼는 상황 그리고 모든 이미지를 라벨 없는 AI 생성 그림에 의존하는 모습이 의구심을 더하는 경험 진짜 제품이 존재하는지조차 의심하게 되는 심정
          + 기사 상단에 나온 사진이 제품 표면을 대변하는 이미지라는 판단 https://www.inventwood.com/superwood-beams
          + 최종 제품이 어느 정도 나뭇결을 보존할 것이라는 설명과 함께, 논문에 여러 실물 사진이 첨부되어 있다는 안내 이들은 대부분 셀룰로오스 외의 성분을 끓여 제거 후, 남은 재료를 압축하는 방식이라는 점을 짚으며, 그 결과 동일 크기의 슈퍼보드는 여러 장의 목재 섬유로 구성될 수 있다고 생각 깊이 있는 추가 연구 필요성도 느끼며, 이 공정이 무게나 강도를 어느 정도 변화시키는지 궁금해함 현재 초고층 건물에는 여전히 강철이 필수인 만큼, 목재의 한계도 인정 목재 부스러기나 톱밥을 접착제로 뭉치는 기존 MDF, OSB, 파티클보드 방식과 차별성을 찾으려 했지만, 셀룰로오스보다 더 강한 접착제가 있다면 굳이 목재를 쓸 이유가 없다는 생각
          + 이미 techcrunch 기사에 실물 사진이 있다는 안내 https://techcrunch.com/wp-content/uploads/2025/05/SUPERWOOD-plank.jpeg
          + 아래에 논문의 실제 이미지를 올린 사람의 안내와 함께, 염색 없이도 짙고 멋스러운 외관을 가진다는 언급(그 자체가 꼭 좋은 것만은 아닐 수도 있다는 의견 포함) https://www.fpl.fs.usda.gov/documnts/pdf2018/fpl_2018_song001.pdf
     * 고급스럽게 보이기 위한 외관용 소재 같다는 인상
     * 관련 연구 배경 논문을 소개하며, 결국은 나무를 끓이고, 압축해서 완성하는 단순한 절차라는 요약 https://www.fpl.fs.usda.gov/documnts/pdf2018/fpl_2018_song001.pdf
          + 동일 논문을 다시 확인하며 순서는 2.5M NaOH와 0.4M Na2SO3 혼합 수용액에 7시간 동안 나무를 끓인 후, 여러 번 끓는 순수 물에 헹궈 화학약품을 제거하고, 100°C에서 5MPa의 압력으로 하루 동안 눌러 고밀도 목재로 만드는 절차임을 명료하게 정리
          + 새로운 기술성이 없다 느끼는 마음 독일에서는 이미 “Panzerholz”라는 이름으로 오래전부터 비슷한 방식의 목재 소재가 존재함을 밝힘(방탄 목재 느낌)
          + 독일의 한 발명가가 TV 과학 프로그램에 출연해 대형 압력솥에 목재와 혼합액을 넣고 오랜 시간 끓인 뒤 목재 전체가 완전히 침투되어 모든 층이 부패 방지 효과를 얻는 것을 보여준 사례도 언급(단, 경도 언급 없이, 별도의 압축 공정은 없었음)
          + 미국 UMD의 Liangbing Hu 연구팀 논문을 핵심 레퍼런스로 꼽아 배경 설명 추가(기사가 빈약한 내용만 언급해서 아쉽다는 평가도 더함) 강도는 483–587MPa로 ASTM A36 구조용 강철(250MPa)보다 우수 density(밀도)는 1.3g/cc으로, 강철의 1/6 고강도 강철에 비해 6배 강한 수준은 아니지만 여러 특성이 뛰어남 과정상 끓이기만 한 게 아니라 가성소다, 황산나트륨 혼합물(식품 산업에서도 쓰임)로 처리해 리그닌을 최적 45% 제거, 페이퍼 제조의 일부 공정을 응용함 환경 문제(황산염 펄핑 공정의 공해성)와 생산 시간 단축 필요성 지적 과거(1880년대~1920년대)에도 이런 시도가 왜 없었는지 궁금해함
          + 강철도 종류와 가공에 따라 다양한 특성이 있기 때문에, “강철보다 강하다”라는 슬로건은 사실 강도 하한선 수준에 도달했다는 정도로 해석해야한다는 점을 지적 도자기 연구 논문에서도 순수 알루미늄과 비교하는 현상을 함께 언급
     * Nile Red의 유튜브 실험 영상을 추천하며, 영상 링크 첨부 https://m.youtube.com/watch?v=CglNRNrMFGM
          + 해당 영상을 시청한 경험자로서, 화학처리 단계에서 침투가 충분하지 않았다고 판단 압력솥을 썼으면 나았을 것이라는 생각을 밝히며 현재 목재 방부 처리도 이 방식(완전 침투, 압력스템)이 일반적임 침투 깊이 문제로 “표면 경화”처럼 되어, 총탄 실험에서 내부층이 더 두꺼운 현상 설명
          + 좋은 영상이라는 평가와 함께, 실험 방식이 Nature 논문 프로토콜을 상당히 따랐음을 부연
     * 새로운 목재 기술이 결국 재활용이 더 힘든, 분해가 어려운 물질로 바뀌는 국가적 문제 우려 일회용 스티로폼 컵에서 종이컵+플라스틱 코팅으로 전환되면서 오히려 재활용 어려운 사례처럼, 미래 폐기물 처리 문제에 대한 두려움 목재로 만든 주방 캐비닛에 플라스틱 코팅이 씌워져 있으면, 재활용이 어떻게 될지 걱정하는 심정
          + Cross Laminated Timber가 실용 건축에 널리 쓰이고 있다는 소개 더 가볍고 강하고, 화재 시에도 구조적 무너짐이 적고, 단열에 우수한 특징 프리패브 조립 기술(CNC 등) 덕분에 공사 효율성도 큼 초고층 건축 계획(예: 도쿄 350m 70층)도 있음 접착제 내구성이 우수해 매립 시 분해가 느린 점과, 하지만 요즘은 덜 유해한 접착제가 쓰인다는 밸런스를 들어 대부분이 여전히 목재라는 점을 강조
          + 논문 요약을 통해 가성소다, 황산나트륨으로 나무를 끓이고 열과 압축으로 셀룰로오스 정렬 및 결합을 강화하는 과정 설명 별도 물질 주입이 없으니, 평범한 목재와 유사하게 분해될 수도 있지 않을까 하는 생각 불확실함도 내포
          + 이미 철도 침목용 방부목재는 처리가 거의 불가능할 정도로 폐기 곤란함을 언급
          + 재활용 자체보다 탄소 친화적 대체재로서의 가치, 목재 자원이 풍부한 곳에서 강철 의존도를 줄인다는 이점 강조
          + 종이컵도 재활용 이슈와 더불어, 인체에 들어와 축적되는 PFAs 문제가 함께 언급
     * 랩에서 성장시키는 인공 목재에도 기대감을 품게 되는 상상 앞으로는 균일한 방향성을 갖는 다층 거대 합판을 바지 위 바다에서 성장시키고, 바지는 계절을 따라 적도 근처를 이동해 햇빛을 극대화한다는 아이디어가 꿈임
          + 많은 바다 지역은 영양분이 부족하고, 오히려 영양이 풍부한 해역은 생태계가 이미 풍부하다는 현실적 지적
          + 소나무 재배와 비교해 어떤 장점이 있는지 묻는 궁금증
          + 파도 문제
          + 기발한 꿈이라는 반응
     * “강철보다 강하다”라는 기사를 여러 차례 접했지만, 항상 “어떤 종류의 강철보다 강한가?”라는 핵심적 질문이 해결되지 않아 답답함 HSLA, 탄소강, 철근 등 명확한 비교군이 궁금함 리모델링하면서 구조용 목재로 바꿨으면 노출 디자인으로도 쓸 수 있었겠다는 아쉬움
          + 어떤 종류의 강철인지, 어떤 종류의 강도인지 묻는 것이 중요함(압축강도, 인장강도, 전단강도, 휨강도, 비틀림강도, 충격강도, 피로강도, 경도 등) 인장강도가 더 뛰어나다면 정말 깜짝 놀랄 일이라는 솔직한 감정
          + 이미 글루람(접착 라미네이트 목재)만으로도 충분히 대체 가능한 부분이 있다는 경험적 조언
     * “강철보다 강하다면 못으로 박기 힘들 것”이라는 직감, 프리패브로 부품을 만들어서 카바이드 엔드밀 드릴로 구멍을 뚫는 작업이 필요하리란 상상 강철처럼 자석 드릴을 쓸 수 없다는 아쉬움
          + 손드릴용 강철드릴로도 충분히 구멍을 뚫을 수 있을 거라는 생각 단, 호두나무 같은 매우 단단한 목재와 비슷하게, 작은 파일럿 홀을 뚫고 크기를 키우는 방식이 더 나을 것이라고 조언(평소 단단한 재질 뚫는 작업자라면 익숙할 팁)
          + 강인성과 경도를 구분해야 한다는 지적 압연강보다 강도가 높다고 경도까지 높은 것은 아님 예측 상으로는 경화강 도구로 가공이 가능할 수도 있다는 판단
          + 일본식 목조 구조물의 프레임 방식에 적합할 수 있다는 분석
          + 대표적인 초경질 목재 ipe(자카란다)의 드릴링 경험을 바탕으로, 실리카 함량이 도구를 크게 마모시키고 먼지 흡입도 건강에 해롭다는 점을 들며, 너무 딱딱한 나무는 손톱을 박는 것만큼 힘들기에 부적합하다고 조언
          + 주로 하중을 받는 곳에만 이 소재를 쓰면, 나머지 골조는 더 다루기 쉬운 저렴한 목재로 충분히 보완 가능하다는 현실적 활용 방안 제안
     * NileRed 영상에서 다뤄진 방식과 크게 다르지 않은 기술이라는 짧은 분석과, 실제 제품이 시장에 보급될 때 다양한 실험을 해보고 싶은 기대감 https://youtu.be/CglNRNrMFGM
          + 다른 유튜버 영상도 추천 https://youtube.com/watch?v=VC4d5iai3GE
          + 최근 이 내용 접했을 때도 바로 그 영상을 떠올린 기억, 이렇게 만든 강한 목재가 실제로 응용 사례가 없었다는 점이 신기했기에, 진짜 쓰이게 되는 시대가 오려는지 궁금함
     * 대량생산으로 집의 골조에 쓰일 만큼 저렴하게 상용화된다면, 자체적인 흰개미 저항성 덕분에 남서부 지역 집들이 100년 넘게 유지될 수 있고, 캘리포니아의 온실가스(훈증제로 쓰이는 vikane 사용) 방출을 줄일 수 있다는 기대감 훈증제로 쓰이는 황화플루오린의 온실효과가 매우 강함을 함께 설명하며, 실제 캘리포니아가 세계 배출량의 12%를 차지하는 심각성을 강조 https://latimes.com/environment/story/…
          + 사실 적절한 방수/시공기법 및 표준 목재만으로도 100년 이상 견디는 집을 충분히 지을 수 있으며, 흰개미 방제 방법도 다양하다는 입장
"
"https://news.hada.io/topic?id=20982","XTool – 크로스플랫폼 Xcode 대체 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       XTool – 크로스플랫폼 Xcode 대체 도구

     * XTool은 Xcode 기능을 여러 운영체제에서 대체할 수 있는 크로스플랫폼 개발 도구임
     * SwiftPM을 활용해 Linux, Windows, macOS에서 iOS 앱 빌드 및 배포가 가능함
     * 개발자는 Apple Developer Services와 프로그램적으로 연동할 수 있음
     * 장치 관리·설치·앱 실행 등의 CLI 기반 서브커맨드를 제공함
     * XKit 라이브러리로 앱 내부에서 직접 기능을 활용할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

XTool 프로젝트 소개

     * XTool은 Xcode와 유사한 기능을 제공하는 크로스플랫폼 개발 툴로, Linux, WSL, macOS에서 동작함
     * 기존 Xcode와 달리 Windows, Linux에서도 SwiftPM 패키지를 iOS 앱으로 빌드하고 실제 장치에 서명·설치할 수 있음
     * 개발자는 Apple Developer Services와의 프로그램적 연동이 가능하며, macOS가 아니더라도 iOS 개발 환경을 구축할 수 있음

주요 기능

     * SwiftPM 패키지 빌드 및 iOS 앱 변환 지원
     * iOS 앱의 서명(Sign), 설치(Install) 가능
     * Apple Developer Services 연동: 인증 및 서비스 활용 기능 제공

대표적 사용 예시

  명령줄 인터페이스

     * xtool은 다양한 서브커맨드를 통해 개발, 장치, 설정 등 기능을 제공함
          + setup: iOS 개발 환경 셋업
          + auth: Apple 개발자 인증 관리
          + sdk: Darwin Swift SDK 관리
          + new: 새 SwiftPM 프로젝트 생성
          + dev: 프로젝트 빌드 및 실행
          + ds: Apple Developer Services와 상호작용
          + devices: 장치 목록 확인
          + install: ipa 파일을 디바이스에 설치
          + uninstall: 앱 삭제
          + launch: 디바이스에서 설치된 앱 실행

XTool 라이브러리(KIT) 활용

     * XKit 라이브러리를 SwiftPM 의존성으로 등록하여, 개발 중인 앱에서 Apple Developer Services, iOS 디바이스 제어 등 기능 직접 호출 가능
     * Swift 개발자는 .package() 및 .product() 선언만으로 프로젝트에 손쉽게 통합할 수 있음

장점 요약

     * macOS뿐만 아니라 Linux, Windows 등 다양한 플랫폼에서 iOS 앱 개발·배포가 가능함
     * VSCode 등 다양한 개발 툴 연동 가능
     * 오픈 스탠더드 기반이며, 범용적이고 자동화된 개발 흐름을 지원함
     * Xcode 사용이 불가능할 때 대체제로 활용 가치가 높음

        Hacker News 의견

     * 이 도구에서 ""Xcode 대체""와 ""Xcode 없이 개발""이라고 말하지만, 나는 Xcode 없이 앱을 빌드하는 것이 불가능하다고 생각함. 라이브러리나 컴파일러 같은 것들을 어떻게 대체하는지 더 찾아보니, 실제로는 그렇지 않음. ""Xcode 빌드 시스템은 사용하지 않지만, iOS SDK와 툴체인을 위해 Xcode 설치는 여전히 필요함""이라고 함. 나는 이들의 메시지를 더 명확히 해야 한다고 생각함. 이것은 대체품이나 완전한 대안이 아니라, 기존 시스템 위에 얹혀진, 더 나은 경험을 추구하는 레이어임
          + 나는 여기에서 다른 개념들이 혼동되고 있다고 생각함. 그냥 Xcode/xcodebuild 위에 올라가는 계층이 아니라, Apple이 현재 모든 iOS/Swift 관련 내용을 Xcode 릴리즈와 함께 제공하는 것일 뿐임. Xcode IDE나 xcodebuild 빌드 시스템에 관심이 없어도, 툴체인을 다운로드·설치할 유일한 방법이 Xcode이기 때문에 필요함. 애플이 이런 툴체인을 따로 제공할 수 있지만, 그렇게 안 하고 있음
     * JetBrains가 과거에 AppCode를 개발했었음. AppCode는 iOS/macOS 개발을 위한 스마트 IDE였지만 2022년 12월 14일부터 더 이상 상업 제품으로 제공되지 않음
          + 이제는 Fleet이 Xcode 앱 빌드를 지원할 듯 보이지만, AppCode 때보다 후퇴임. 현재로서 Fleet은 내 macOS 앱을 컴파일하지 못함. 업데이트가 나올 때마다 시도하는데, 별다른 대안이 없음
          + ObjC 부분에는 도움이 안 되지만, Swift 부분은 CLion 플러그인으로 분리 이동함. 안타깝게 ObjC 관련 내용은 /dev/null로 보낸 듯함
     * https://forums.swift.org/t/… 및 HN 토론글 https://news.ycombinator.com/item?id=43952239 참고 의견임
     * 이론적으로 이 도구로 인해 Flutter 앱 개발자가 Linux에서 iOS 앱을 빌드하고 배포할 수 있는지 궁금함. 정말 그랬으면 하는 바람임
          + 기술적으로는 Apple Developer Agreement 위반이라고 알고 있음. 또 다른 소송이 벌어질 소재로 보임
          + 이 프로젝트가 도움이 되긴 하지만 이것만으로 충분하지 않음. 1. Dart가 Linux에서 iOS로의 크로스 컴파일이 아직 안 됨. 대안으로 Dart 인터프리터를 쓸 수 있지만 성능 저하가 큼. 2. Flutter iOS 프로젝트는 Xcode 프로젝트 포맷을 쓰므로, xtool 포맷으로 마이그레이션 필요함. 3. Flutter 플러그인 생태계가 CocoaPods에서 SwiftPM으로 아직 완전히 옮겨지지 않아서, SwiftPM으로 마이그레이션되지 않은 플러그인은 xtool과 호환이 안 됨
          + 내가 아는 한 darling으로 이미 가능함. Nixpkgs에도 xcbuild 대안이 있지만 코드사인 기능은 확실하지 않음
          + CodeMagic 같이 이미 나와 있는 서비스를 쓰는 것이 바람직함. 애플 계정 정지 위험을 감수할 필요 없음
     * xcode는 몇 달마다 9시간씩 다운로드해야 하고, XML·plist 파일을 마구 수정하는 도구임. 그 xcode임
     * Sweetpad를 떠올리게 함. Sweetpad는 설치 과정이 다소 번거로움. XTool이 더 나은 개발 경험(DX)을 제공하는 듯함. 이 프로젝트가 더 성장하길 바람
          + XTool은 프로젝트 정보를 yaml 포맷으로 정의한다는 점에서 XcodeGen과도 비슷한 점이 있음
     * 이전에도 ""리눅스와 윈도우에서 iOS 앱 빌드"" 관련 이야기가 있었음. https://news.ycombinator.com/item?id=43952239 참고
     * Github 조직명이 ""xtool-org""임을 봄. XTool이라는 회사(레이저 조각기 및 크래프팅 기술 제작)가 이 문제를 삼지 않을지 궁금함
     * XCode와 분리된 유사 도구로 Swift UI 기반 인터페이스를 실시간 미리보기 기능 또한 있으면 멋질 것임
     * 스크린샷에 vscode 아이콘이 있음?
          + 맞음. 이 도구는 커맨드라인 앱으로 빌드 도구만 바꾸는 역할임. IDE는 아니므로, 코드는 원하는 편집기에서 작업함
"
"https://news.hada.io/topic?id=20975","FSRS로 간격 반복 시스템이 더 좋아짐","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         FSRS로 간격 반복 시스템이 더 좋아짐

     * 과거에 학습한 자료의 복습 간격을 늘리는 학습 기법인 Spaced Repetition System은 적은 시간 투자로도 효과적인 장기 기억 형성을 제공함
     * 머신러닝 기반 예측으로 개인별 카드 스케줄링을 최적화하는 FSRS(Free Spaced Repetition Scheduler) 알고리듬으로 기존 방법보다 효율과 사용 만족도가 크게 향상됨
     * Anki 최신 버전에서 FSRS가 기본 스케줄러로 적용되어 대부분의 사용자가 이미 활용 중임
     * WaniKani, Bunpro 등의 서비스와 비교해 Anki와 FSRS 조합이 학습 효율과 유연성에서 우월함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

간격 반복 시스템 개요

     * 다양한 분야에서 지식 습득과 장기 기억 유지를 위해 간격 반복 시스템이 활용됨
     * 학교 수업이나 취미 학습처럼 제한된 시간 내에서 효과적인 복습을 가능하게 하는 솔루션임
     * 플래시카드 형식으로 정보를 반복적으로 제시하며, 복습 간격을 사용자 반응에 따라 조절함
     * 하루에 20분씩 투자하면 1년에 3,650개 단어를 쉽게 외울 수 있음

기존의 스케줄링 방법과 한계

     * 초기 간격 반복 시스템에는 SuperMemo-2 알고리듬이 주로 쓰임
     * 이 방식은 “1일 뒤, 맞히면 6일 뒤, 다시 맞히면 15일 뒤, 나중엔 37.5일 뒤” 식으로 복습 간격이 늘어남
     * 오답일 경우엔 다시 1일 뒤로 초기화되어, 같은 카드를 반복적으로 짧은 간격으로 보게 되어 좌절감이 큼
     * 이 방식은 경험적, 임의적으로 정해진 규칙에 기반하여, 개별 지식 항목에 따라 최적화되어 있지 않음
     * 모든 정보의 기억 곡선이 동일하다고 가정하는 비현실적 전제가 존재함

FSRS: 개선된 머신러닝 기반 스케줄링

     * FSRS(Free Spaced Repetition Scheduler) 는 최신 머신러닝 기법을 기반으로, 복습 간격을 개별적으로 최적화함
     * “언제 카드의 회상 확률이 90%로 떨어지는지”를 예측 문제로 전환하여, 정확한 복습 시점을 산출함
     * FSRS 모델은 난이도(카드별 1~10), 안정성(100%→90%로 회상율이 떨어지는 기간), 회수 가능성(일수 경과 후 회상 확률)의 세 가지 함수를 곡선 피팅으로 구함
     * 21개의 파라미터를 이용해 대규모 리뷰 데이터에 맞춰 곡선을 최적화하며, 개인별 리뷰 이력을 반영해 파라미터를 재조정함
     * FSRS는 사용자가 원하는 목표 회상율(예: 90%)을 설정할 수 있고, 이에 따른 일일 학습량과 카운트를 시뮬레이션할 수 있음
          + 예를 들어, 70% 회상율로 설정할 때 일일 복습량은 줄고, 기억하는 카드 수는 오히려 늘어남

FSRS의 실제 적용

     * Anki는 2023-11 출시된 23.10 버전부터 기본 스케줄러로 FSRS를 채택함
     * FSRS를 활용하면 일일 복습 부담이 줄고, 틀린 카드를 복습할 때도 스트레스가 심하게 늘어나지 않음
     * 권장 설정에 따라 학습 효율과 학습량 균형을 최적화할 수 있음
     * 오픈소스 프로젝트로 여러 언어와 소프트웨어에서 구현이 가능함

다른 학습 서비스와의 비교

     * WaniKani, Bunpro 등 구독형 서비스의 경우, 정해진 간격만 제공하고, 개인 맞춤화된 조정이 없음
          + 예: 4시간, 8시간, 1일, 2일, 7일... 등 임의적 복습 주기 설정
     * 카드 오답 시 최소 단계로 초기화하지 않거나, 머신러닝 기반 예측이 없어 효율성이 크게 뒤처짐
     * 일정 간격이 지난 카드는 더 이상 다시 보지 않게 되어, 장기 지식 손실이 발생함
     * 이로 인해 학습자의 스트레스와 비효율이 누적됨

Anki의 장점

     * UI는 다소 불편할 수 있으나 고성능 학습 기능과 지속적 업데이트, 폭넓은 커스터마이징이 강점임
     * 실제로 다양한 분야와 단계의 학습자에 적합한 유연성을 제공함
     * 기초부터 고급 단계까지 장기적인 지식 구축에 최적임
     * 직접적인 사용자 경험을 바탕으로, 효과적인 학습 도구로 자리매김함

더 알아보기

     * 간격 반복의 원리, FSRS의 자세한 작동 방식 및 구현 예시는 아래 자료 참고
          + open-spaced-repetition/awesome-fsrs: 다양한 프로그래밍 언어와 소프트웨어에서의 FSRS 구현 목록
          + open-spaced-repetition/srs-benchmark: FSRS와 여러 알고리듬(예: SuperMemo-2, Duolingo 알고리듬 등) 비교 벤치마크 결과
               o 현재 FSRS보다 일관되게 더 좋은 성능을 보이는 것은 OpenAI Reptile 알고리듬 기반의 LSTM 신경망 정도임

        Hacker News 의견

     * 나는 Trane의 창시자임을 강조하고 싶음 (https://github.com/trane-project/trane/). Trane은 Anki와 비슷한 시스템을 대체할 수 있는데, 하위 기술 요소 간 계층 구조가 명확한 대부분의 영역(음악, 어휘 학습 등)에 적용 가능함. Anki, SuperMemo 등 기존 시스템에는 미해결 이슈가 세 가지 있다고 생각함. 첫째, 암기 위주의 방식이 문제임. 나는 기억이 아닌 숙련도 기반 채점이 필요한 영역(음악 등)에도 적용할 수 있는 걸 원했음. 둘째, 계층 구조 정보가 없어서 대규모 기술 습득이 어려움. Anki는 Trane의 기능(하위 기술 의존성 기반 진도 제한과 숙련도 확인) 재현이 어려움. 셋째, 자신의 연습문제를 직접 만들 것을 요구함. 이는 상당히 시간이 들며 복잡한 기술에 대해선 전문가가 필요함. Trane은 사실상 완성되었고, 나는 음악 공부에 사용함. UI가 없어서 나만 사용 중이나,
       무료로 일하기 싫으니 그러려니 함. 지금은 Trane 기반으로 문해력 튜터 개발 중. 완성 시 학생이 알파벳부터 대학 수준 독해·작문까지 최신 연구 기반으로 배울 수 있음. 연내 MVP 출시 목표임
     * SRS와 관련된 많은 논의와 발전을 목격함. 하지만 내가 보고 싶은, 그리고 정말 중요한 부분은 읽기·이해와 SRS 사이의 영역임. 웹 브라우저, PDF 등 기존 프로그램에서 Anki, Mochi 등 인기 SRS로 플래시카드를 쉽게 생성하는 독립 툴이 거의 없음. 이런 기능이 OS 통합처럼 자연스럽고 마찰 없이 동작해야 하며, ""또 하나의 별도 앱""이 아닌 SRS로 쉽게 넘길 수 있는 파이프가 필요함. Mac 친화적이고 눈에 거슬리지 않는 그런 시스템이 필요함. 혹시 이런 툴 알면 알려주길 바람
          + “기존 프로그램에서 플래시카드를 쉽게 만든다”는 개념이 흔히 오해됨. SRS의 반 이상 가치는 직접 플래시카드로 만들 개념을 고르고, 유사점·차이점·속성 등을 탐색하는 과정에서 나옴. 힘들지만, 이 과정 자체가 이해에 큰 도움임. 하지만 이 능력이 어렵기 때문에 많은 사람이 SRS를 제대로 활용하지 못하고, 효과를 못 느끼고 포기하게 됨. 또 하나의 오해는 SRS가 단순 암기용이라는 점임. 실제로는 잘 설계하면 복잡한 주제의 이해에도 충분히 사용 가능함
          + 나는 Fresh Cards라는 플래시카드 앱을 만들었는데, 종종 사용자가 웹페이지나 PDF에서 플래시카드를 가져오는 기능을 문의함. 그런데 솔직히 그게 어떻게 동작해야 할지 아직 모름. 사용자가 직접 하이라이트 후 ""카드로 만들기""를 클릭하는 방식이어야 할지, 아니면 자동으로 텍스트를 분석해서 질문·답 목록을 제시해주는 방식이어야 할지 고민임. 어떤 기준으로 뭘 카드화할지, 얼마나 세분화해야 할지 결정이 어려움. 특히 날짜나 이름 등 단순 사실을 뽑는 것은 어떤 콘텐츠에는 별로 도움이 안 됨. 결국 아주 열려있는 문제라서 모두의 니즈를 만족시키기 힘들 것 같음
          + macOS 서비스 모델이 이런 목적에 잘 맞는다고 생각함. 서비스란 다양한 앱에서 컨텍스트 기반으로 동작하며, 별도 개발 없이 앱 간 연동을 지원함. 예를 들어 텍스트를 선택, 우클릭, 서비스 메뉴에서 “New SRS Card” 같은 기능을 호출하면 바로 간단한 카드 생성이 가능함. SRS 앱에서 이런 서비스를 내장해주면 매우 빠르고 간편하게 카드 만들기 가능함
          + 나는 LLM에 시스템 프롬프트를 넣어 이 문제를 해결함. ChatGPT에서 개념을 이해한 뒤, 플래시카드 생성을 요구한 뒤 Mochi에 복사·붙여넣기 함. 향후에는 LLM과 Mochi 간 직접 카드 추가 통합이 더욱 발전할 것으로 기대함
          + 언어학습 맥락에서는 YouTube/Netflix 오디오 카드나 자막을 추출해서 “마이닝” 하는 아주 좋은 툴들이 있음. 어떤 것은 오픈소스 무료이지만, 처음 사용은 마찰이 있음. 유료 솔루션은 조금 더 친절함
     * LLM으로 학습할 때 추천 팁은, 토픽별 대화 내용을 csv로 구글 드라이브에 저장해 Anki에 동기화하는 MCP 툴을 만들라는 것임. 이 방식은 내 LLM 활용의 게임 체인저 였음. LLM이 장기적으로 보면 생각을 덜게 만들기도 하는데, 이왕 쓸 거라면 공부 도구로 활용하길 추천함
          + 나는 Anki 덱에서 다음날 복습 예정 카드를 골라 LLM이 새로운 문장들을 생성하게 하는 Python 스크립트를 만듦. 단순히 카드를 항상 외워 맞추는 게 아니라, 새로운 문맥에서도 단어 인지력을 키우는 데 목적이 있음. 다양한 문맥에서 학습함으로써 실제 언어 습득에 도움이 될 거라 기대함
          + 구체적으로 어떻게 csv를 Anki 카드로 동기화하며, LLM 결과를 csv로 만드는 MCP 구현이 어떻게 생겼는지 블로그 포스팅을 보고 싶음
          + ChatGPT 4o의 보이스 모드는 기본 중국어 학습에 정말 혁신적인 경험임. 집안에서 사물 이름 묻거나, 단어 간 관련성 질문, 짧은 문장 만들기와 문법 확인에 큰 도움이 됨. 아직 MCP는 없지만, 대화 내용을 구조화된 형식으로 요약해달라 할 수 있음
     * 내가 Anki에서 가장 불편한 점은 데이터 모델임. ""노트 콜렉션""(직접 만들거나, LLM으로 생성하거나, 친구나 학생과 공유할 만한)을 계층적으로 관리하고, 거기서 최종적으로 학습할 카드 세트를 템플릿 기준으로 파생해야 한다고 여김. 복습 히스토리와 모델, 특정 상황에서 복습할 카드 제한 방법(중국어나 일본어 쓰기는 종이 필요하니 상황별로 덱을 가려서 활용) 등, 계층 분리 필요함. 반면 Anki는 이 모든 게 하나의 데이터베이스에 섞여 있고, 가져오기/내보내기/공유/외부 데이터 조작도 매우 불편함. 내 데이터 임의 조작이 안 될 때마다 좌절함. 혹시 이런 문제 없는 시스템 아는 사람?
          + 여러 노트 콜렉션, 세션별 카드 제한은 Anki 덱과 태그, Better Tags, 서브덱 기능 등으로 충분히 할 수 있음. 모든 덱이 별도 파일로 분리되고, 스페이스드 리핏션 모델도 FSRS 등 복수 지원임. 내보내기/공유는 파일 단위(압축 파일)로 쉽고, 외부 조작을 위한 라이브러리와 툴도 많고, 오픈소스·라이브러리 기반 구조로 데이터 추출이 쉬움. 기존 시스템 불만족을 Anki로 해결한 경우임
          + 당신이 언급한 모든 점이 사실과 다름. Anki는 훌륭한 오픈소스 문서화가 잘 되어있음. 프로그래밍 가능한 사람이라면 ChatGPT로 거의 모든 작업이 가능하고, 나는 sqlite 데이터베이스에서 데이터 마이닝도 자주 함
          + Anki 데이터 모델은 비효율적인 점은 많음. 실제로 테이블 한 줄에 JSON이 들어가는 상당히 임시방편적인 구성 등, 점진적으로 성장하다보니 어색한 구조임. 반대로 템플릿과 클로즈 삭제 기능(일부만 숨겨 여러 카드를 자동 생성)은 매우 우수해, 이제는 그 구조가 감사하게 느껴짐. 나는 클로즈 삭제·템플릿 도입을 위해 Fresh Cards의 스키마를 재설계 중임. 내 앱에서는 각 카드 속성이 테이블로 분화됨. Anki는 초창기 동기화 지원이 어렵던 이유가 이런 스키마와도 연관 있음
          + 언어 학습에만 해당되지만, 내 TheHardWay (https://thehardway.app) 같은 구조(플래시카드가 마크다운 노트와 통합)가 좋은 선택일 수 있음
          + 예를 들어 식당 관련 단어, 공항 관련 단어를 각 그룹으로 분리하면 사용자가 자연스럽게 연상 가능함
     * 나는 대학 때 다음과 같은 방식으로 스페이스드 리핏션을 활용함. 외워야 할 키를 세로로 워드 문서에 정리해서 PDF로 저장함. PDF의 각 키 옆에 주석 필드(값)를 만듦. 주석을 클릭해 답을 확인하며, 쉽게 맞출 때마다 주석을 왼쪽으로 옮기고, 헷갈릴 때는 오른쪽으로 다시 옮겼음. 결국 주석 표 위치로 복습 우선순위 조절 가능했음. 단점도 많았지만 나에게는 잘 맞았고, Anki의 비슷한 알고리즘이 생기기 전이라 오늘날이었다면 경험이 달랐을지도 모름
          + 이 방식이 흥미롭지만 설명만으로는 이미지가 잘 안 그려짐. 예시 파일이 있다면 보고 싶음
     * FSRS 실험에 관심 있다면, Open Spaced Repetition이 공식 Python, Typescript, Rust 패키지를 제공함 (각 깃허브 링크 안내). ts-fsrs와 rs-fsrs는 FSRS 6 지원함, py-fsrs도 곧 지원 예정. 또한, py-fsrs와 fsrs-rs는 과거 복습 기록 기반으로 모델 최적화까지 제공함
          + Chessbook의 오프닝 트레이닝에 Rust 패키지를 활용함. 매우 쓰기 쉽고, 사용자 부담은 줄이면서 기억률은 높임. FSRS 시스템이 정말 뛰어남
          + 루비(Ruby) 사용자라면 FSRS gem에 신카드 간격 이슈를 수정한 포크를 참고하길 바람 (https://github.com/arvindang/rb-fsrs). 원래 위 Python 버전에서 포팅함
     * 스페이스드 리핏션은 20년 째 인기를 누리지만, 만능해결책은 아님. 수십개의 앱, 수천번의 강연에도 불구하고, 결국 많은 사람이 항상 다이어트나 자기계발 참작처럼 중간에 포기함. “어린이 교육을 진짜 신경 쓴다면, Google이나 Apple이 매주 노트카드 작성을 요구하고 통과 시 폰 잠금을 해제하는 unlock 시스템을 내놔야 한다”고 생각한 적 있음. 물론 우회 기능이 있어야 하고 실제 설치하는 사람은 별로 없겠지만, 그 정도로 일상화될 필요가 있다고 봄
          + 스페이스드 리핏션은 ""시간 최적화""에 초점이 있으나, 자기관리/동기부여에는 효과가 없음. 시간 부담이 크면 효율적이지만, 동기부여나 자기 통제가 힘들면 쉽게 번아웃됨. 나의 경우, Anki 덕에 GCSE와 A레벨 성공 후 심하게 번아웃돼 결국 휴학까지 감. 결과적으로 Anki는 성공의 원동력이자, 휴식의 계기도 됨
          + “만능해결책이 아니다”라는 발언은 기준 정의가 없어 공허함. 다이어트도 만능해결책이 아니듯, SRS를 6년 넘게 써온 나에겐 삶을 바꾼 경험임
          + SRS는 암기와 언어 습득 차이 이해가 부족한 이들에게는 관심이 부족함. 암기를 목표로 하는 사람에겐 환호 받지만, 나는 스페인어·프랑스어 읽기 능력 자체가 목표임. 단어/예문 반복 훈련은 공사장의 발판(비계)과 같음. 구조물을 세우는 데 직접적 사용은 안 하지만, 전체 공정을 크게 가속함. 암기와 언어 습득이 별개가 아니라 도움 관계임을 더 잘 안내할 수 있으면 좋겠음
          + 만능해결책 기대라면 실망만 있게 됨. 결국 “노동”이 필요함. 도구일 뿐임
          + SRS에는 진입장벽 높은 UX 문제가 있음. 1) 카드 생성 소요 시간 2) 자기채점 필요 3) 단일 프롬프트-답안 구조 4) 자기주도 학습 필요(스캐폴딩, 이해 단계). 더 근본적으로, SRS는 “정확한 질문-답변”에는 탁월하지만, 일반화가 약함. 실제로 지식 그래프 구성이 약함. 회상지식(암기)과 논리적 모델 지식의 차이를 보면, 암기는 거의 “사전 검색”처럼 작동하고, 논리적 모델(수학 개념 등)은 훨씬 복잡함. SRS 옹호자들은 논리모델도 “사실 집합” 암기가 기본이라 주장하지만, 결국 SRS의 실질적 가치는 “잘 정돈된 노트북” 이상의 실용성임. 그러나 천재 되기엔 한참 부족함
     * 10년 가까이 Anki를 사용하며, 개선되어야 할 것은 UI/디자인뿐이라고 생각함. 알고리즘 자동화보다도, 실제 인터페이스가 많은 신규 사용자에게 지루하다는 점이 더 큼. 강력한 파워유저 기능은 좋지만 직관성이 떨어짐. 스페이싱 효과는 인간 학습에서 매우 과소평가된 핵심임
          + AnkiDroid 메인테이너임. 현재 리뷰어 디자인을 완전히 새로 작업 중이며, 프로덕션 앱 “개발자 옵션”에서 확인 가능함. 디자인에 만족하지 못하지만 리소스가 많이 부족함. Android 쪽에 관심 있으면 연락 환영함
          + Anki를 사랑하지만, 전형적인 “엔지니어가 설계”한 제품임. 매우 강력하고, 깊이도 있지만 디자인이 투박하고 비직관적임. HN 독자 중 테크 덕후라면 익힐 수 있지만, 일반 Duolingo 사용자는 힘듦
          + 에디터 창이 몹시 불편함. 여기서 개선이 시급함. 메인 창 탭도 진짜 탭처럼 동작하지 않고, 복습 화면에서 “Again/Good” 두 버튼 모드가 기본 내장되어야 함. 애드온만 챙겨도 인기 많고 구현도 쉬움
          + 여러 해 동안 Anki와 SRS에 대한 글은 많이 읽었지만 실제로 사용해본 적 없음. 혹 추천할 만한 학습 분야 있을지 궁금함
          + Duolingo와 비교하면 역시 지루함. 그래도 몇 년째 쓰고 있음. 약간의 게임 요소(스트릭, 효과음 등)를 옵션으로 도입하면 더 좋을 듯함
     * “Why Anki Doesn’t Work for Me”라는 글을 6년 전에 썼음(알고리즘 개선 전임). Anki에 여러 문제를 느꼈으나, 새 알고리즘이 내 핵심 불만을 바로잡았으니, 다시 시도해 볼 계획임. 나머지 문제들이 여전히 장애물인지 궁금함
          + 나는 A 레벨 도중 FSRS 확장으로 전환함. Google Collab notebook으로 내 학습 패턴에 맞게 커스텀 피팅도 했음. 복습량이 절반으로 줄고, 복습 타이밍도 분산돼서 훨씬 쾌적해짐. 효과는 등가 또는 더 뛰어났음. 새로운 시도를 강력 추천함
          + 나도 비슷한 경험이 있음. Anki가 단기기억에 집착하고, 며칠 혹은 몇 주 지나면 거의 백지에서 다시 학습해야 했음. 그 시기엔 거의 모든 SRS 커뮤니티가 Anki를 신성시했으나 나는 포기함
          + WaniKani는 최고의 SRS UI를 갖췄지만, 구식 알고리즘으로 인해 본질적으로 같은 한계를 가짐. “마지막 복습 후엔 카드를 영원히 보지 않아도 됨”이라는 구조에 항상 지식 손실 위험이 있어 불안함
     * 스페이스드 리핏션의 흥미로운 점은 “단순 암기”가 특정 상황에서 현대 교육이 생각하는 것보다 훨씬 더 중요한 역할을 한다는 전제임. 수학·프로그래밍에선 이해가 암기보다 더 중요하다는 생각이 있지만, 스페이스드 리핏션은 그 반론을 제기할 수 있음
          + 교육제도가 산업혁명 이전에서 현대까지 전환하는 과정에서, 암기 중심 고전 교육(라틴어, 그리스어)에서 “이해 중심” 전환이 이루어짐. 당시 변화는 필요했지만 과도했을 수 있음. 현재 암기는 필요 이상 저평가임
          + 요즘 프로그래밍은 “필요할 때 찾아보면 된다” 개념을 따름. 하지만 일부는 반드시 암기해야 함(프로그래밍 언어, 패턴 등). 실습은 비구조적 SRS와 같고, 특정한 영역(예: C++로 드라이버 개발)에선 SRS가 큰 도움이 됨
          + 반복적으로 사용되는 연산(곱셈 등)은 암기로 내재화해야 상위 개념 학습에 집중 가능함. 이 구조가 학습 효율의 핵심임
          + 암기는 이해의 전제임. 기억하지 못하는 것은 이해할 수 없음
          + 회상형 지식과 논리적 모델형 지식의 차이는 매우 흥미로움. LLM은 전형적으로 전자에 가깝고, 수학에서는 한계가 큼
"
"https://news.hada.io/topic?id=21009","Miyagi - 유튜브 영상으로 온라인 대화형 학습 과정 만들기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Miyagi - 유튜브 영상으로 온라인 대화형 학습 과정 만들기

     * Youtube에는 훌륭한 교육 콘텐츠가 많지만, 산만하고 능동적인 학습이 어려움
     * Miyagi는 유튜브 교육 영상을 대화형 학습 코스로 전환해주는 AI 기반 교육 플랫폼
     * 수동적인 영상 시청을 넘어 강의 요약, 퀴즈, 실습 문제, 실시간 피드백을 자동 생성하여 능동적인 학습을 유도
     * 기존의 MOOC 플랫폼과 유사한 UI이지만, 질문·피드백 기능을 강화하여 AI 튜터의 기능도 일부 수행
     * 3Blue1Brown의 선형대수, YC의 스타트업 강의 등 인기 유튜브 시리즈도 학습 코스로 제공됨
     * 400개 이상의 코스와 콘텐츠 크리에이터와의 파트너십을 보유중

   오 좋은데요
"
"https://news.hada.io/topic?id=20993","초부자들은 평범한 사람들이 모르는 무엇을 사나요? (2015)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   초부자들은 평범한 사람들이 모르는 무엇을 사나요? (2015)

   엄청나게 부자인 사람들은 어떤 것을 사는지에 대해 한 Reddit 사용자의 답변

     나는 어쩌다보니 엄청 부자인 사람들과 오래 알고 지내게 되어 답변을 적어 봄. 나는 부의 수준을 4단계로 나눔

순자산 $10M~$30M (약 130억~400억 원)

     * 주 거주지 제외한 현금성 자산 기준
     * 고급 호텔 숙박, 퍼스트 클래스 항공, 고급 의료, 생활비 충당 모두 문제 없음
     * $2,000짜리 스위트룸을 기념일에 이용할 수 있는 수준
     * 큰 재정 위기에서도 삶이 흔들리지 않음
     * 그러나 여전히 재정 결정을 신중히 내려야 하며, 진짜 ‘돈이 중요하지 않은’ 수준은 아님
     * 금융권에서도 ‘초고액 자산가’로는 분류되지 않음

순자산 $30M~$100M (약 400억~1,300억 원)

     * 전용기 전세 혹은 지분 보유 (예: NetJets)
     * 아스펜 크리스마스 주간, 모나코 F1, 칸 영화제 등에서 최고급 숙소 이용 가능 (1박 $5k~$20k 이상)
     * 복수의 고급 주택과 리조트 소유
     * 유명 정치인 및 지역 리더들과 교류하며 지역사회에서 존경받는 인물로 자리함
     * 개인 비서와 보좌진 운영, 외부인은 '당신의 사람'을 통해 연락함
     * 어떤 고급 소비재든 구매 가능
     * 단, 뉴욕·LA·베벌리힐즈 등 초상류층 도시에서는 아직 소수 계층

순자산 $100M~$1B (약 1,300억~1조 3천억 원)

     * 개인 전용기, 별장·맨션 복수 보유, 주거지마다 고급 차량과 직원 배치
     * 유명인사(정치인, 셀럽, 아티스트 등)와의 자연스러운 교류
     * 모든 파티에 다 초대되진 않겠지만, 원하는 곳은 모두 갈 수 있음
     * ‘사람’을 거느리는 삶, 주변은 Yes Man으로 가득함
     * 개인 섬을 살수도(섬에 따라 다르겠지만)
     * 물건 구매가 취향과 예술의 경지로 발전함 (예: 5대 밖에 없는 클래식카)
     * 당신 친구가 대통령과 만찬을 하거나, 본인과 주지사·상원의원 디너 정도는 ‘일상’
     * 이성관계도 매우 풍요롭지만, 진심 어린 관계를 구별하기 어려움

순자산 $1B+ (약 1조 3천억 원 이상)

   $10B 이상인 사람들은 제외하겠음. 그들은 그냥 다름. 하지만 $1B만 되어도 인생이 달라짐 무엇(Anything) 이든 살수 있음.
     * Access (접근성)
          + 누구든 연락 가능, 곧바로 회신받고 만남 성사
          + 유명 억만장자나 정치인도 본인의 요청으로 쉽게 연결됨
          + 국가수반급 인물들과 정기적 비공식 만남
     * Influence (영향력)
          + 개인적인 정책 입안·언론 여론 형성까지 가능
          + 광고, 로비스트 활용, 고위 관료와의 단독 미팅 가능
     * Time (시간)
          + 대기 시간 제로: 비행기 이륙까지 2분, 도착 후 즉시 이동
          + 셰프가 사전에 준비한 식사, 웨이팅 없는 레스토랑, 즉시 골프 라운드
          + Grammy·Super Bowl도 줄 없이 VIP 입장
     * Experiences (경험)
          + Pete Sampras와 테니스, Blink182의 개인 공연, Louvre 비공개 투어 등 무엇이든 가능
          + 예술가·과학자·정치인을 사적인 만찬에 초대 가능
     * Stuff (물건)
          + 예: 모차르트가 작곡하던 피아노 소장
          + 희귀 예술품, 역사적 물건 등 모든 수집 가능
     * Impact (영향력)
          + 마을의 물 문제 해결, 병원 건립, 수천 명의 생명을 바꾸는 기부도 소액 수준
          + 지구적 영향력 행사 가능
     * Respect (존경)
          + 주지사, CEO, 왕족에게도 수평 관계로 인정받는 위치
          + 사회 모든 집단에서 극도의 존경을 받음
     * Perspective (비율 감각)
          + 연 수입 $400M → 일반인 수입 $40K에 비해 10,000배
          + 슈퍼카 $235,000 → $23.50
          + 퍼스트 클래스 항공권 $10,000 → $1
          + $10M 예술품 → $1,000
          + 모든 것이 매우 싸게 느껴지는 세계
     * Love (사랑)
          + 돈으로는 살 수 없는 유일한 것
          + 희생이 요구되지 않는 삶은 진정한 관계 형성을 방해함
          + 감정적 연결과 친밀함의 상실은 궁극적 외로움으로 이어짐

        Hacker News 의견

     * 다른 사람의 답글이 지루함이라는 평가 내용과, 흔한 상식만 반복해 실제로 유용한 정보를 주지 않는다는 실망감 표현, 예를 들면 Miele 주방 가전을 미리 알았더라면 좋았다는 바람 내용 전달, 부유한 여성이 직접 쓴 아파트를 렌트하며 처음 알게 된 경험, 그리고 베니어 나무 가구와 문이 라미네이트 칩우드 대비 몇 배 더 비싸도 훨씬 더 오래가고 쾌적함을 알게 되기까지 오랜 시간이 필요했다는 배움, 이런 장기적 품질 차이는 미리 누가 알려주지 않으면 잘 모르게 되는 점, 자신은 중산층의 삶을 배워가는 중이며 더 상위 계층에도 비슷하게 숨은 정보가 있겠지만 아마 평생 자연스럽게 알게 되기는 힘들 것 같다는 고백
          + 나는 $1000만~$3000만 자산이 있는 사람 몇 명, $4000만, 그리고 $7억 자산가 한 명을 실제로 알고 있는데, Reddit 답변 내용과는 별로 맞지 않는다는 생각, $7억 자산가는 정치인, 지역 리더와 교류하며 여행을 자주 하지만 개인 전용 제트기를 소유하지 않음, 일찍 은퇴해서 일도 안 하고 부를 과시하지 않고 조용하게 지내며, 여성에게만 부를 약간 내비치는데 덕분에 원하는 여성과 쉽게 사귐, 데이트만 해도 평생 럭셔리 여행을 즐길 수 있으니 당연한 현상, 본인은 주로 금융 이야기나 비즈니스 조언을 좋아함, $1000만~$3000만 자산가들도 나와 크게 다르지 않지만 위기 대응 여력이 더 깊고, 여행이나 부동산이 더 값지며, 특별히 자신의 비서를 통해 사회적으로 접근이 어렵거나 최상급 식당만 찾는 그런 집착은 없음, 내가 만나는 부자들이 화려하지 않은 유형일
            수 있고 실제로 돈을 펑펑 쓰는 사람들과는 짧게만 교류한다는 점, 그리고 그런 스타일의 사람이 오래 부를 유지하는지도 의문, 셀럽과 어울리려는 부자를 못 봤고 요즘은 헐리우드의 매력도 떨어진 듯하다는 생각, 본문 댓글도 결국 헐리우드 스타일 부자 이야기로 보인다는 결론
          + 나는 Miele 주방 가전이 과장됐다고 봄, 전 집주인이 10년도 안 된 Miele 풀세트로 꾸몄던 집에 살았지만, 오븐에서 ""서비스를 받으라""는 등 이상한 에러만 자주 떴고 서비스센터는 별다른 도움도 줄 줄 몰랐음, 예전 Siemens 주방보다 특별할 것 없었다는 점
          + Ikea 가구가 15년 쓰면 충분한데 꼭 50~100년 짜리 가구가 필요하지 않게 된 시대, 이제는 오래된 물건을 대물림하는 것도 원하지 않음, 인생 환경 변화도 빨라 품질 좋은 비싼 가구가 별 의미 없어짐, Miele 식기세척기도 두 배 값에 두 배 오래 가겠지만 10년 후면 어차피 낡은 제품 취급
          + 원래 댓글이 여러 부의 단계를 다루고 있지만 결국 본질은 부가 올라가면서 부동산, 사치, 정치적 힘 등이 커진다는 단순 요약 가능, 그리고 원 댓글은 결국 바로 위 계층에서 누리는 것들에 대한 호기심 정도로 보임, 참고로 식기세척기는 Bosch 추천
          + 100년 전 만들어진 가구, 문 등이 더 품질이 좋은 것 같은데 실제로는 그것이 생존편향일 수 있음, 지금까지 남아있는 것이 당연히 질이 좋은 쪽이고 IKEA 가구도 이사하다 바로 망가질 수 있음
     * 사람들이 질문에 감정적으로 대응하는 모습이 재밌음, 예를 들어 테니스 포럼에서 프로들이 하는 특별한 훈련법을 물으면 대부분 “테니스만 하면 인생이 허전하다” 식으로 답하지는 않을 텐데, 부에 대한 질문에는 엉뚱한 철학적 답변이 많은 현상
          + 때로는 질문 자체가 다른 의미를 내포함, 이 경우 “부자들이 쓰는 트릭이 궁금하다”보다는 “부자로 살아보니 어떤 느낌인가”를 묻는 본질
          + 많은 테니스 선수에게 love는 점수 용어, 무의미함
          + 누군가의 호기심을 매번 철학 수업으로 만들지 않는 태도 필요
          + 이런 반응에 딱 맞는 단어가 있는데, 바로 ‘코프(cope)’라는 용어
     * 존 헨리 뉴먼의 말을 인용해, 사람들이 부를 단순한 물질이 아닌 일종의 신처럼 숭배하는 현상 분석, 부와 행복, 존중을 동일시하고 실제로 돈을 얻을 생각이 없어도 부유한 자만 보면 경외감과 동경을 품는 모습, 부자라면 모습이나 평판과 무관하게 ‘특별한 힘’을 가졌다고 믿는 사회적 신념, 왜 돈만큼 특별히 신격화되는 게 많은지에 대한 통찰, 이 인용문의 원문 링크 제공
     * 빠른 대학 육상 선수 출신이, 성공한 금융 자문가가 되고, 장거리 육상 재능을 가진 세 아들의 아버지가 된 이야기, 오래된 러닝 친구가 현재는 엘리트 선수 코치인데 아들을 위해 섭외, 전국 800m 고교 신기록 달성, 고교 졸업 후 프로 전향, 코로나로 400m 트랙 접근 제한, 그래서 가족 집 근처에 월드 챔피언십 수준의 8레인 400m 트랙을 400만 달러 들여 직접 건설, 최고의 코치와 숙소 제공 등 자녀의 환경을 위해 지원하는 부자의 사례, 관련 링크 소개
          + 직업 러너가 평생 벌 수 있는 수입이 400만 달러가 넘는 사례가 실제로 있는지에 대한 궁금증 표현
     * 이 기사를 보고 부자들이 우리가 상상하지 못하는 별난 것들을 산다는 내용을 기대했지만, 실제로는 접근성·영향력·사치처럼 누구나 대충 예상 가능한 범위 얘기라 좀 덜 신기했다는 생각, 직접 경험하지 않았어도 대부분이 어느 정도는 짐작할 수 있는 이야기였던 느낌
          + 특이하게 부유층 사이에서 한때 FaceGym 마스크가 유행이었다는 정보 공유
     * 원하는 것을 바꾸기만 하면 누구나 원하는 건 다 살 수 있다는 간단한 원리 소개
          + 광고가 내 욕구 자체를 계속 바꾸게 만든다는 현실 지적
     * 우연한 계기로 꽤 부유한 사람 대상으로 한 카탈로그나 영업 편지를 많이 받아봄, 대부분은 고급 가구·의류·생활용품 등 예상대로였으나, 가장 흥미로웠던 것은 여행(이색적인 현지 가이드 정보가 실려 있음)과 매우 전문적/세부적인 카탈로그(정원용품, 특정 홈웨어 등), 특히 수백 가지 용도별 브러시 전용 카탈로그는 세상에 이렇게 다양한 브러시가 있는지 신기했던 경험
          + 나에게 부자되는 가장 좋은 점은 선택의 고민이 사라지는 것, 예를 들면 자전거를 살 때 스펙이나 가격, 후회에 대해 오래 고민해야 하는데, 부자라면 최고 사양을 다 사보고 마음에 안 들면 다른 브랜드로 또 사면 됨, 내가 자전거라면 고민도 하나의 재미지만, 오븐처럼 관심 없는 가전은 스트레스라 돈 있으면 그냥 최고급으로 바꿔달라고 할 수 있음
          + 회사명이 궁금하다는 요청, Sharper Image 카탈로그처럼 척하는 곳이 아닐지 상상
          + 여행 가이드/회사의 이름을 혹시 기억하는지 질문, 실제로 흥미로운 정보일 수 있음
     * 부자들의 전용 컨시어지 서비스 경험, 일반인은 박물관에 가면 비좁고 북적이지만, 부자라면 시간 외 프라이빗 투어와 전문 가이드가 동행해 여유롭게 원하는 방식으로 감상할 수 있음, 공공서비스 어디든 이런 식으로 세상이 다르게 열리는 현상
     * 미국인이 아니지만 웬만하면 왜 정치인, 대통령과 시간을 보내려 하는지 궁금한 마음 표현
          + 실질적으로는 로비 때문이고, 법안, 규제, 허가 등 자신의 사업이나 이익에 영향 주는 결정을 위해, 하지만 꼭 사업만이 아니라 단순히 귀찮고 심심해서 빌딩 매니저에게 전화하듯 할 때도 있음
          + 영향력을 얻기 위해서임, 특히 시/주/국가 단위로 무언가 맘에 안 들 때 정계 인맥이 바꾸는 힘을 줄 수 있음, 법적 이슈에도 도움이 되는 경우가 있으나 이에 대한 비판과 감시도 늘어나는 분위기
          + 실제로 어느 나라나 부자와 권력자들은 자기에게 유리한 규제를 위해 정치권과 가까이 지냄
          + 실용적 이유(로비 등)가 많지만, 보다 심리적인 이유는 ‘희소성과 힘에 가까워지는 것’에 대한 욕망, 미국 상원의원은 단 100명뿐인데 억만장자는 더 많음, 상원 진출이 어렵다는 점에서 정치인의 독특한 인기와 매력, 그런 권력 가까이에 있다는 느낌, 특히 대통령이나 주지사의 경우 더욱 더 그런 측면
     * ""부자""는 성격이 아니고 취향의 집합도 아니며, 과도한 소비는 어느 정도 예산대에서나 허세로만 여겨지는 무의미한 것, 어떤 물건도 일정 수준 이상이면 누구에게나 충분, 앤디 워홀의 “부자들은 꼭 뭔가 대단히 특별한 게 있을까 상상한다”는 인용, 그리고 결국은 reddit에서 정보나 취미 팁 외의 인생 조언을 바라지 않는 게 현명하다는 의견
          + 워홀이 상상력이 부족하다고 느끼며, 자신이 사는 대학 도시에서는 친구들 누구도 다리 뻗고 앉을 아늑한 의자를 못 사서 힘든데, Wayfair 싸구려와 몇 천 달러짜리 고급품의 차이는 아주 크다는 주장
"
"https://news.hada.io/topic?id=21010","MS Build 2025 Book of News (BoN) 한국어 번역본","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                MS Build 2025 Book of News (BoN) 한국어 번역본

   한국 시간으로 2025년 5월 20일 새벽 1시에 열린 Microsoft Build 2025에서 발표되는 모든 소식을 Book of News라는 이름으로 Microsoft가 제공하는 것이 있습니다. 이 내용을 한국어로 번역한 콘텐츠를 공유드립니다. 관심있으신 분들께 도움이 되었으면 합니다.

   감사합니다~

   감사합니다.
"
"https://news.hada.io/topic?id=20945","3개의 명령어로 윤년 여부를 확인하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          3개의 명령어로 윤년 여부를 확인하기

     * 3개의 CPU 명령어만으로 윤년을 판별하는 함수를 구현함
     * 이 방법은 비트 연산과 곱셈을 이용하여 전통적 분기 없이 동작
     * 이 방식은 0~102499년 범위에서 정확함
     * 벤치마크 결과 기존 방법 대비 3.8배 가량 빠른 성능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 문제 제시

     * 0에서 102499년 사이의 년도에 대해, 단 3개의 CPU 명령어만 사용하여 윤년을 판별하는 함수
          + 사용되는 실제 함수는 ((y * 1073750999) & 3221352463) <= 126976 구조임
     * 이 비트 트위들링(bit-twiddling) 기법의 원리와 동작, 실질적인 효용성에 대해 설명

전통적인 윤년 판별 방법

     * 보통 윤년 판별은 나눗셈(modulo)와 조건분기로 구현함
          + 4로 나누어 떨어지는지, 100으로 안 나누어지는지, 400으로 나누어 떨어지는지 검사함
          + 예시 코드:
if ((y % 4) != 0) return false
if ((y % 100) != 0) return true
if ((y % 400) == 0) return true
return false

표준 접근 방식의 최적화

     * (y % 100) 검사를 (y % 25)로, (y % 400) 검사를 (y % 16)으로 대체 가능함
          + 그 이유는 앞서 4로 이미 나누었으므로, 25 및 16 곱셈 관계로 변경 가능
     * (y % 25) 연산을 나눗셈 없이 곱셈과 비교로 변환하는 마법 상수
          + 예: x * 3264175145 > 171798691로 변환 가능
     * 비트마스크를 더해 (y & 3)이나 (y & 15)로 4 또는 16의 나눗셈 대체 가능
     * 컴파일러는 이러한 변환을 자동화하지만, 직접 다른 언어에서 활용할 수도 있음

분기 없는(branchless) 구현법

     * 분기가 없는 형태로도 변환 가능함:
return !(y & ((y % 25) ? 3 : 15))

     * 이러한 방식은 코드 골프(coding golf)와 같이 코드 길이 줄이기에 적합

매직 상수 찾기: 비트 트위들링 접근법

     * 윤년 판별식을 더욱 간단하게 만들기 위해 조합적 탐색과 SMT Solver인 Z3를 활용
          + 형태: ((y * f) & m) <= t
     * 요구 조건을 만족시키는 상수 f, m, t를 Z3로 탐색
          + 0~102499 범위에서 정확한 결과를 얻을 수 있는 값 발굴
          + 최종 결과가 (y * 1073750999) & 3221352463 <= 126976임

함수 원리 및 내부 구조 해설

     * 상수를 이진수로 분석하여, 세 개의 주요 비트 영역 A, B, C로 구분함
          + 각 영역의 비트 상태에 따라 윤년 판별의 3가지 조건을 포괄함
     * 함수의 논리적 분해:
          + A 영역: (y % 4) != 0 여부를 비롯한 4의 배수 조건 확인
          + B 영역: (y % 100) != 0여부를 다양한 패턴(예: 뒷자리 14, 57, 71 등)로 필터링
          + C 영역: (y % 16) == 0의 즉, 16의 배수 확인
     * 곱셈이 실제로 나머지 계산 없이 다양한 조건을 결합해내는 원리 해설
          + 매직 상수를 곱할 때, 100의 배수 등에서 특이적인 비트 패턴이 형성됨
          + 추가적으로, 곱셈 오차와 여러 자리 숫자 패턴이 등장하는 수학적 내부 구조 분석이 포함됨

추가 범위 및 비트폭 확장 가능성

     * 64비트로 확장할 경우 적합한 매직 상수 조합 탐색법도 제시
          + f, m, t값을 다양하게 바꿔보며 최장 범위를 찾을 수 있음
          + StackExchange에서도 최적 조합 및 Z3 활용 증명 사례가 있음

벤치마크 및 실제 성능 비교

     * 벤치마크 결과:
          + 2025년 등 예측 가능한 값에서는 0.65~0.69ns로 차이가 거의 없음
          + 무작위 입력시 is_leap_year_fast가 3.8배 정도 빠른 성능을 보임
          + 입력 패턴에 따라 분기(branching) 방식이 예측 불가할 때 상당한 이점이 있음

결론 및 실제 적용 여부

     * 실제 응용에서 값이 예측가능할 때는 이득 적음, 하지만 대량의 무작위 데이터 상황에선 매우 유용
     * 실제 파이썬이나 C# 등 표준 라이브러리 교체 시에는 현실적인 전체 프로그램 벤치마크가 필요함
     * 아이디어와 구현 방법 자체가 흥미롭고, 특정 상황에서는 성능상 매력적인 솔루션임

   Fast inverse sqrt 가 생각나는 글

        Hacker News 의견

     * 최신 컴파일러인 gcc나 clang 같은 경우 is_leap_year1 같은 코드를 is_leap_year2처럼 자동으로 최적화해준다는 사실에 놀라움 느낌, 그래서 C 소스 단계에서 직접 최적화할 필요는 크게 없다는 생각, 하지만 다른 언어에는 여전히 유용함이 있을 수 있음, 특히 cal 프로그램의 최신 버전 소스코드에서 leap year 체크를 아주 간단하게 처리하는 점이 인상적임
          + linux 코드가 세 가지 연속된 조건을 매번 반전시키고 기본값 반환을 쓰지 않아 이해하기 훨씬 쉽다는 점이 마음에 듦, 이런 복잡한 코드 구조가 있으면 디버깅할 때 정말 미치는 경험임
     * ((y * 1073750999) & 3221352463) <= 126976 이 코드가 어떻게 동작하는지에 대한 설명이 복잡할 수밖에 없다는 데에 전혀 놀라움 없음, 오히려 그게 당연함
     * 이해하기 어려운 매직 넘버 최적화 기법을 정말 좋아함, 이런 걸 볼 때마다 예전 어셈블리로 내부 루프 짜던 시절에 얼마나 많은 최적화 기법을 놓쳤을지 궁금함, 혹시 이런 테크닉 모아둔 컬렉션이 있다면 공유 요청
          + 여러 비트 조작 트릭 모음 링크 공유, UNIX 스타일 비교에 효율적인 CMP(X, Y) 매크로, signum 함수 최적화 예시, Motorola 68000용 어셈블리 코드 예시 링크, OpenSolaris에서 유래된 비트 매크로 모음 등 다양한 최적화 기법 링크 제공, Open Solaris 블로그는 사라졌다는 아쉬움도 언급, 코드 최적화에 관심 있는 사람들에게 추천
          + ""Hacker's Delight"" 책 추천, 다양한 비트 트릭과 저수준 최적화 기법이 가득함
          + supercompilation 기법 찾아보라는 제안
          + 예전에는 이런 기법을 놓친 게 아니고, 당시엔 곱셈이 워낙 비쌌기 때문에 저런 게 곧 최적화였음
     * 윤년 체크가 이렇게 흥미로울 줄 전혀 예상 못함, 아마 저수준 프로그래머들은 이미 오래전에 이런 트릭을 찾았지만 기록을 남기지 않았던 게 아닐까 하는 생각, 이런 게 아직도 옛 코드 속에 숨어 있을지도 모른다는 느낌, 이런 기법 컬렉션 있으면 정말 탐구해보고 싶다는 열의
          + 80년대 z80에서 집에서 직접 배운 것들이 있었는데 대부분 잊어버림, 가끔씩 20대 자녀들에게 보여주면 마치 마술을 부리는 듯한 느낌
     * 6000년 이전의 윤년을 알아야 한다면 직접 만든 인터랙티브 계산기와 가시화 도구 사용, 비록 기계 명령어 수는 좀 많지만 아주 빠르게 수천 번의 계산 처리 가능, 수학적 트릭 역시 감탄 포인트
     * 비트 조작 챕터를 읽다 ""혹시 솔버 쓸 수 있지 않을까"" 라는 생각이 들었고, 실제로 글쓴이가 딱 그 방법을 사용해서 깜짝 놀람, 세밀한 접근 방식에 만족
     * 이런 윤년 체크 함수를 current-time-api에 추가하는 게 좋겠다는 제안
     * 숫자를 이진수로 보면 재미있는 패턴들이 보임, 모든 소수는 2를 제외하면 1로 끝난다는 점이 흥미로웠음
          + 재밌게 보일 수 있지만, 모든 홀수가 1로 끝나고 소수가 본질적으로 2를 제외하면 짝수가 될 수 없으니, 별다른 의미를 못 느끼겠다는 의문 제기
     * 윤년 문제에서 0이 없다는 지적이 등장, 사실 ""0년""은 없고 1 BC에서 바로 1 AD로 넘어가서 0 체크는 의미 없다는 이야기
          + 글 맨 처음을 보면 윤년 알고리즘 설계의 전제가 proleptic 그레고리력과 천문학적 연도를 쓴다는 점이라 설명, 이 조건이 없다면 윤년 체크는 로케일마다 복잡해진다는 점을 강조
          + 천문학적 연도 표기를 쓰면 0년이 등장하게 되고, 연/월 관리 등에서는 그렇게 처리하는 것이 오히려 깔끔함, 내부 데이터는 천문학적 표기, 외부 출력만 BCE/CE 변환 제안
          + 그레고리력 도입 이전의 달력은 기준 자체가 모호하고 지역마다 다름, 1582년에는 하루에 열흘을 건너뛴 '10일 삭제'도 있었으니, 그 이전 날짜의 연산은 신뢰 불가, 로마 시대 사제들은 윤년을 미신/뇌물 등으로 임의조정하기도 해서 더욱 복잡함
          + ISO8601 표준에서는 0년을 허용하고, 천문학적 달력에서 0년은 1 BC의 의미, BCE 연도는 전부 -1만큼씩 오프셋 됨
     * 소스코드가 실제로 웃음을 터뜨리게 만드는 경우는 드물기 때문에 아주 즐거운 경험임
"
"https://news.hada.io/topic?id=20978","CSS에서 브라우저가 대조되는 색상을 자동으로 선택하는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   CSS에서 브라우저가 대조되는 색상을 자동으로 선택하는 방법

     * contrast-color() 함수를 사용하면 버튼 등 다양한 배경색에 맞춰 브라우저가 검정 또는 흰색 글꼴색을 자동으로 선택함
     * 대규모 프로젝트에서도 텍스트 가독성 유지가 쉬워지고 유지보수 효율성을 높임
     * 현재 Safari Technology Preview에서는 WCAG 2 공식 알고리듬을 사용하는데, 이는 실제 인간 인지와 어긋날 수 있음
     * 차세대 APCA 알고리듬 도입 논의가 WCAG 3 개발 과정에서 진행 중이며, 더 나은 명도 대비 평가를 약속함
     * 단순한 흑백 대비 이외에도 향후 다양한 색상 옵션과 접근성 개선 기능이 추가될 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 contrast-color() 도입 배경

     * 여러 버튼이나 인터페이스 구성 요소에 다양한 배경색이 사용되는 디자인에서는 글꼴색(텍스트 색상) 의 가독성이 중요함
     * 이전까지는 개발자가 직접 배경색과 텍스트 색상을 일일이 잘 조합해야 했으나, 대규모 프로젝트에서는 관리의 복잡성과 오류 발생 위험이 높음
     * contrast-color() CSS 함수를 사용하면 개발자는 배경색만 지정하면 되고, 브라우저가 자동으로 검정 또는 흰색 글꼴색 중 높은 대비를 선택함
     * 이 방식으로 유지보수와 디자인 작업의 효율성이 크게 향상됨
     * 간단히 color: contrast-color(색상);처럼 선언하여 사용할 수 있음

contrast-color() 사용법 예시

     * 버튼의 배경색 변수에 원하는 색상을 지정하고, 텍스트 색상은 contrast-color()를 이용하여 자동으로 대조되는 흑백 중 하나가 선택됨
     * 한 번에 한 색상만 관리하면 되므로, 디자인 정책 변경이나 다크/라이트 모드 지원 시 maintenance가 쉬움

button {
  background-color: var(--button-color);
  color: contrast-color(var(--button-color));
}

     * Relative Color Syntax를 활용하면 hover 상태에 대해서도 배경색과 텍스트 색상을 일관성 있게 관리할 수 있음

접근성 고려사항 및 알고리듬 설명

     * contrast-color() 사용이 모든 접근성(Accessibility) 문제를 자동으로 해결하지는 않음
     * 특정 중간 밝기의 배경색에서는 검정, 흰색 모두 필요 기준에 못 미치는 경우가 발생함
     * 현재 Safari Technology Preview에서 사용되는 WCAG 2 알고리듬은 공식 웹 접근성 표준임
          + 이 알고리듬은 대비 비율 기반으로 선택하지만 실제 눈에 보이는 명도 대비와 불일치하는 결과가 나오기도 함
     * 예시로, #317CFF파란색 배경에는 기계적으로 검정색이 더 높은 대비로 계산되지만, 실제로는 흰색이 더 잘 읽힘
     * 이에 대한 비판과 개선 요구에 따라, 차세대 접근성 표준(WCAG 3) 에서 더 뛰어난 APCA(Accessible Perceptual Contrast Algorithm) 도입 논의가 진행 중임
     * APCA는 인간의 인지 특성을 반영하여 색상 대비를 계산하므로 실제 가독성을 더 잘 보장함

실제 환경에서 충분한 대비 제공하기

     * CSS의 @media (prefers-contrast: more) 미디어 쿼리를 활용하면, 사용자 접근성 선호도에 따라 추가적인 고대비 스타일을 적용할 수 있음

@media (prefers-contrast: more) {
  /* 더 높은 대조 스타일 정의 */
}

     * 예를 들어, 브랜드 핵심색이 #2DAD4E와 같은 밝은 녹색인 경우, contrast-color()가 미래에는 흰색을 선택하더라도 작은 글씨에는 여전히 대비가 충분하지 않을 수 있음
     * APCA 알고리듬을 적용하면 폰트 크기, 굵기에 따라 필요한 최소 대비 기준을 상세히 참고할 수 있어 실무 디자인 결정에 도움을 줌
          + 실제로 24px/400굵기 글자에는 흰색 적용이 적합하지만, 더욱 얇은 FONT나 작은 글씨에는 더 진한 배경색 사용이 권장됨
     * 디자인팀은 light/dark mode, prefers-contrast 선호도 등을 고려해 각 조건에 맞는 색상 팔레트를 쉽게 변수로 관리할 수 있음

--button-color: #2DAD4E;

@media (prefers-contrast: more) {
  @media (prefers-color-scheme: light) {
    --button-color: #419543;
  }
  @media (prefers-color-scheme: dark) {
    --button-color: #77CA8B;
  }
}
button {
  background-color: var(--button-color);
  color: contrast-color(var(--button-color));
  font-size: 1.5rem;
  font-weight: 500;
}

     * 핵심적으로는 contrast-color() 덕분에 배경색을 중심으로 색상만 관리하면 되고, 텍스트 색상 대비 쌍은 브라우저가 자동 생성함

검정색과 흰색을 넘어

     * 현재의 contrast-color()는 흑백 2가지 중에서만 선택하지만, 초기 버전은 여러 색상 중 선택도 가능했음
     * CSS Working Group은 향후 알고리듬 변화와 호환성을 위해 단순 버전(흑백만 선택)을 우선 제공하고, 미래에는 사용자 지정 색상 옵션 및 원하는 최소 대비 기준을 지정하는 등 확장도 계획함
     * 단순한 필요에는 이미 충분히 유용함
     * 본 함수는 배경색뿐만 아니라 테두리, 기타 시각 요소에도 다양하게 활용 가능함

결론 및 참고 정보

     * 차세대 접근성 표준 반영 후, contrast-color()는 알고리듬을 교체해 더 좋은 대비 자동 선택을 지원할 것임
     * 그때까지는 핵심 배경색이 명확히 밝거나 어두운 경우에 특히 유용함
     * 텍스트가 아닌 다양한 UI 요소에도 폭넓게 적용 가능함
     * APCA(Accessible Perceptual Contrast Algorithm) 같은 최신 접근성 알고리듬에도 계속 관심을 가지는 것이 바람직함

참고 자료

     * APCA 공식 문서 및 APCA Contrast Calculator에서 다양한 예시와 평가 기준 확인 가능
     * CSSWG의 contrast-color 함수 관련 표준화 논의 진행 중
     * WebKit이나 관련 커뮤니티에서 의견 공유 및 피드백 참여 가능

        Hacker News 의견

     * 이 문제를 해결하기 위해 색상 쌍이 설계 단계에서 간단하고 예측 가능한 WCAG/ACPA 명암비를 갖도록 팔레트를 만드는 도구를 개발 중인 중임. https://www.inclusivecolors.com/ 사이트에서 데스크톱에서 더 많은 기능을 제공함. 한 방법으로 100(연한 색)에서 900(어두운 색)까지 등급별 색상 견본을 만들고, 예를 들어 700 등급의 색은 100 등급과, 800 등급은 200 등급과 명확한 대비가 나타나도록 밝기를 조정하는 설계임. 이렇게 하면 red-700 vs gray-100, green-800 vs yellow-200 같은 조합이 명도 체크 없이도 확실히 대비를 제공함을 알 수 있음. Contrast 메뉴에서는 WCAG와 비교해 APCA 알고리즘이 얼마나 엄격한지, 특히 밝은 바탕의 어두운 글자에서 얼마나 까다로운지도 탐색 가능함. 어두운 테마에는 WCAG를 사용하면 안되는 이유임. Examples 메뉴에서는 Tailwind 및 IBM Carbon 팔레트 사례를
       보면, 각 등급이 채도와 색상(Hue)이 비선형적으로 변해서 단순히 흑백 중 최적 대비를 고르는 건 쉽지만, 브랜딩이 중요한 팔레트는 단순 밝기 조정만으로 색을 뽑을 수 없는 더 복잡한 문제임.
     * lch를 사용해서 비슷한 것을 할 수 있는 방법이 있음
 --text: lch(from var(--bg) calc((49.44 - l) * infinity) 0 0);

       소스: https://til.jakelazaroff.com/css/…
          + LCH도 멋지지만 OKLCH가 더 뛰어남. https://evilmartians.com/chronicles/oklch-in-css-why-quit-rgb-hsl/… 이 글이 내 생각을 완전히 바꾼 계기가 되었음. 정말 대단한 도구임. 놀랍게도 내 디자이너 친구들 중 OKLCH를 아는 사람이 없었음. 이 방식이 많은 문제를 해결함.
          + 이렇게 콜백으로 매개변수를 받을 수 있는 CSS 함수는 처음 봄. 정말 흥미로운 개념임. 혹시 lch 외에 이런 스타일의 함수가 또 있을지 궁금함.
          + Lea Verou가 이와 비슷한 우회 방법에 대해 쓴 좋은 글이 있음. https://lea.verou.me/blog/2024/contrast-color/
     * 이 글은 명암 자동 선택 방식의 장단점에 대한 훌륭한 개요임. 단순한 사이트를 만드는 경우엔 이 방법이 쉽고 간단하게 올바른 대비를 제공함.
       하지만 대규모로 WCAG 컴플라이언스가 필요한 경우엔 피하고, 진짜 의미기반 서식 토큰 계층이 필요함. 의미 토큰은 개발 속도를 높여주고, 단순히 검정/흰색 전환보다 시각적으로 더 보기 좋은 대비를 보장할 수 있음. 의미 토큰 계층의 좋은 점은 테마 제작이 매우 쉬워서 라이트/다크 테마가 추가 비용 거의 없이 가능함. 만약 브랜드 색상이 WCAG2에 걸릴 때, 별도의 WCAG2/APCA용 테마를 따로 만들어서 컴플라이언스를 얻으면서 더 나은 대비를 제공할 수 있음.
       나는 Figma에서 variables/tokens 스트림을 맡고 있고, Figma와 Atlassian의 다크 모드 구현에도 참여했음. 토큰, 테마, 접근성 색상 관련 궁금한 게 있으면 언제든 질문 환영함.
          + 의미 토큰이란게 구체적으로 어떤 것인지 궁금함. 이런 기능이 있기에 내가 작업한 큰 프로젝트에서 색상 상대 계산과 대비 색상을 위해 CSS-in-JS를 썼음. 이런 기술이 곧 널리 적용될 걸 기대함.
          + 마지막 2/3 내용은 너무 장황해서 현학적으로 보임. 회사 사이트/앱에선 결과 색상이 예측 불가해서 이런 함수에 의존하면 위험함. WebKit이 버그를 고치면서 색상 결과가 바뀔 수도 있기 때문임.
     * 대비 색상이 브라우저 업체의 판단에 따라 정해지는 게 항상 맞지도, 예측 가능하지도 않다는 데에 아직 동의하기 어려움. 모든 브라우저가 동일하게 결과를 내는 확정적 기준이 마련될지 궁금함. 이 함수는 실제론 UX팀의 디자인 단계 지원용 도구로 느껴짐.
          + 기사에 따르면 표준에서 계산 방법을 명확히 지정하고 있다고 언급함
          + '선택한다(choose)'는 표현이 애매하게 느껴짐. 실제로는 알고리즘이 색상을 산출함
          + 링크에 나온 APCA 예시 공식에서 오류나 혼동되는 부분을 제외하면 100% 정확히 작동함. 완벽하게 일관성 있게 하려면, 두 후보 색상(더 밝은 것/어두운 것 모두)이 동시에 허용될 때 배경색의 밝기(L*)를 기준으로 예를 들어 L* 60 이상이면 밝은 쪽을 택하면 됨. 그러면 100% 일관성 확보함.
     * 대규모 프로젝트에서 버튼이 안보이는 색상(예: 어두운데 검정 글자)으로 바뀔까봐 따로 챙기는 게 어렵다는 데, 실제로 출시 전에 일일이 버튼을 다 점검하면 어떨까 하는 생각임. 아니면 팀 전체에 어두운 버튼엔 검정 텍스트 절대 안된다고 사전 규정하고 공유하는 방법도 있음. 인지적 대비와 수학적 대비의 차이가 흥미로웠음. 워크플로우에 적용할 예정임.
          + 실제로 버튼 전수 점검이 가능은 하지만, 이런 식으로 하면 사전 회귀 테스트 기간이 몇 주, 심하면 몇 달 걸릴 수도 있음. 대규모 프로젝트면 버튼이 수천 개가 넘기 쉽고, 특정 옵션 조합이나 워크플로우에서만 보이는 버튼도 많음
          + APCA를 참고하면 지각(인지) 기반 대비 계산이 가능함
     * 시스템 색상이 인기 있었을 때 버튼 스타일을 만들었던 경험이 있음. 보기엔 근사했지만 명암비가 어떨지 알 수 없어서, 누군가 자바스크립트로 getComputedStyle로 계산해 준 게 있음. 명암이 불량하면 두 번째 색상 후보를 쓰거나, 불가피할 땐 text-shadow로 글자 주변에 명암 효과를 강화했음. 계산 방식은 잊었는데, 3개 RGB 값을 평균내서 비교하면 될 것 같기도 함. 파란색에선 평균값이 낮을 테니 흰색 글자 우선을 줄 수 있을 듯함.
     * 최소한 light/dark 테마에서 active, focus, hover, link, visited 같은 가상 클래스별로 좋은 색상 추천을 알려주면 좋겠음. material UI는 여기에 disabled, before, after 상태까지 추가함.
     * 예전에 배경색에 따라 검정/흰색 텍스트를 고르는 방식으로 영상 튜토리얼을 만든 적 있음. 내 방식은 단순해서 색을 회색조로 바꿔서 검정/흰색 중 뭘 쓸지 정함. 재미있는 작업이었음. 영상 만드는 실력은 부족함.
       https://youtu.be/tUJvE4xfTgo?si=vFlegFA_7lzijfSR (포르투갈어 주의)
          + 흥미롭게도 다른 댓글에서 딱 그 작업을 하는 색상 공간 공식을 소개했음.
            https://news.ycombinator.com/item?id=44015990
            영상도 괜찮아 보임. 코드는 좋아보이는데 내가 포르투갈어를 몰라서 내용을 평가하진 못하겠음.
     * 전체 색 구성표를 직접 고르는데, 왜 대비 버튼 텍스트 색 고르는 게 처음부터 그냥 직접 고르는 것보다 쉽다는 건지 의문임. 이 기능은 배경색은 임의로 제각각 고르면서 정작 전경색(버튼 텍스트 색상)은 못 고르는 상태가 되는, 매우 극단적인 상황에서만 필요해질 듯함. 정말 문제인 상황은 이미지나 다양한 배경 위의 글자처럼 항상 잘 보여야 하는 건데, 이 기능은 전혀 못 다룸. 그래서 이런 제한적인 상황에서만 도움이 될 법한 기능인데, 이를 위해 새 동사까지 만들고, 기능은 흑백 선택이 끝인데다 가장 안좋은 명암 선택 알고리즘(WCAG 2)을 썼다는 점이 아쉬움. 진짜 대단함.
          + 어떤 도구라도 쓸 상황을 안 겪어봤다고 바로 무시하는 시각은 아쉬움. 웹사이트 중엔 사용자가 임의로 색을 고르거나, 업로드 자료에서 색을 추출하는 경우가 많음. 접근성 챙기는 사이트는 이런 경우를 대비해서 자동 대비 계산을 꼭 하게 됨. 이런 내장 CSS 기능이 나오면, 기본적인 접근성도 쉽게 챙길 수 있음. 물론 더 발전된 경험 만들고 싶은 개발자에겐 아무 제약도 안 됨. contrast-color npm 패키지처럼 더 커스터마이즈가 가능하면 더 좋겠지만, 블로그에 보면 일단 흑백만 선택하는 알고리즘을 첫 단계로 시작했고 향후 개선 계획이 있음.
            예시: https://coolors.co/8fbfe0-7c77b9-1d8a99-0bc9cd-14fff7
          + 명암 선택 알고리즘이 별로라는 지적에 대해, WCAG 2 알고리즘을 따르고 있으며 앞으로 WCAG 3이 표준화되면 해당 알고리즘으로 쉽게 갈아탈 수 있음을 분명히 밝힘
     * SASS, Tailwind 등에 적용 가능한 이런 기능의 빌드 타임 대안이 있는지 궁금함. 이 기능이 전면 도입되려면 시간이 필요할 것 같은데, 플랫폼마다 구현이 동일하게 될지 걱정도 있음
"
"https://news.hada.io/topic?id=20994","반복 간격 기반 암기 시스템 (2024)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         반복 간격 기반 암기 시스템 (2024)

     * 반복 간격 기반 암기 시스템은 정보를 효과적으로 장기 기억에 저장하는 기법임
     * 이 시스템은 정보를 점점 더 긴 간격을 두고 복습하여 망각을 줄이는 원리임
     * 카드, 플래시카드 방식으로 많이 활용되며, 복습 주기를 사용함
     * 학습 효율을 높이고, 짧은 시간에 더 많은 정보를 암기할 수 있음
     * 이 방식은 언어 학습 등 다양한 분야에서 광범위하게 이용됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

반복 간격 기반 암기 시스템 소개

     * 반복 간격 기반 암기 시스템은 정보를 오랜 기간 기억에 남기기 위한 학습법임
     * 일반적으로 영단어나 개념을 배울 때, 처음에는 짧은 간격으로 반복 학습을 하고, 점차 복습 간격을 늘려가는 방식을 사용함
     * 이 방식은 Ebbinghaus 망각 곡선에 기반하여, 사람이 시간이 지남에 따라 기억을 잊는 특성에 맞춰 설계함
     * 주어진 정보나 플래시카드는, 적절한 타이밍에 반복함으로써 기억의 흔적이 점차 강화됨
     * 예를 들어, 오늘 배운 내용을 내일, 3일 후, 1주일 후, 1달 후 같은 식으로 복습하는 계획을 세움

적용 예시 및 장점

     * 플래시카드 앱, 언어 학습, 시험 준비 등 다양한 교육 분야에서 많이 채택됨
     * 기존의 무작위 반복에 비해, 시간과 노력을 절약하면서 학습 효율을 극대화할 수 있음
     * 뇌가 정보를 장기 기억으로 전환하는 메커니즘을 이용해, 학습 부담을 줄이고 동기를 높임

활용 사례

     * 대표적으로 Anki, SuperMemo 같은 소프트웨어가 이 원리를 알고리듬으로 구현함
     * 학교 공부, 자격증 준비, 전문 용어 암기 등 반복적이고 체계적인 학습에 효과적으로 적용됨

결론

     * 반복 간격 기반 암기 시스템은 짧은 시간에 많은 정보를 오랫동안 기억하고자 할 때 유용한 도구임
     * 다양한 학습자가 자율적으로 이 시스템을 활용해 반복과 복습의 최적화를 실현함

        Hacker News 의견

     * 여러 번 spaced repetition 시스템을 시도해봤지만, 외울 만한 가치가 있는 것을 잘 못 찾겠다는 느낌이 드는 경험 반복 중인 상태. 정말 중요한 것은 굳이 애쓰지 않아도 기억되고, 나머지 것들은 매일 카드 리뷰를 하면 시간이 지나며 의미 없는 잡무로 느껴지는 현상 경험
          + Anki를 암기보다는 우연한 통찰 발견을 위한 엔진처럼 사용하는 습관 보유. 흥미로운 생각이나 발견이 있을 때마다 간단히 한두 문장씩 써서 하나는 Obsidian에 주변 노트들과 연결해서 저장하고, 또 하나는 Anki에 cloze 삭제 형식으로 기록하는 방식을 사용. Anki 리뷰 주기 설정을 길게 두고(1일, 1주, 1개월, 그 다음부터는 자동) 일주일마다 한 번씩 리뷰하는 스타일 선호. 이런 과정에서 노트의 랜덤 등장이나 최근 작업과의 연결성을 발견하며 새로운 아이디어가 자주 등장하는 현상 경험. 실제로는 많은 생각을 기록하지 않지만, 평균적으로 하루에 한 개씩 새로운 노트 생성
          + 사실상 수많은 사실을 외우는 게 필요한 경우를 못 찾아봐서 나도 활용 동기를 못 느끼는 입장. 사람들이 이 시스템을 언어 학습, 특히 단어 암기에 많이 쓰는 건 계속 목격 중이지만, 다른 분야에도 활용 사례가 있는지 궁금증 생김. 업무에서 iso/iec 표준의 중요한 내용을 더 잘 기억하는 데 응용할 방법이 없을까 고민해 봤지만, 플래시카드와 연결이 떠오르지 않는 상황
     * 읽어야 할 분량이 며칠 치가 되는 자료라는 느낌. 혹시 부담스럽다면 spaced repetition의 기본 아이디어를 쉽게 설명해주는 만화 https://ncase.me/remember/를 추천하는 입장
     * 제품 추천의 의도. 내가 가장 좋아하는 spaced repetition + 노트테이킹 + 학습 앱은 https://www.remnote.com/이라는 소개. Anki를 썼던 사람이라면 익숙할 설계. 카드를 관리하는 흠잡을 데 없는 시스템 보유, 카드 추가가 불릿 포인트로 쓰듯 간편. cloze 삭제, 이미지 일부 가리기, PDF·이미지 자산 관리 등 필요한 기능 갖춤. 현재 최고의 SRS 스케줄링 알고리즘 FSRS 사용. AI와의 통합 수준이 매우 뛰어나, 예를 들어 스페인어 칸에 ""el vaquero ==< [tab]""를 입력하면 AI가 즉시 양방향 카드로 번역을 만들어줌. 수학 학습 중엔 latex 수식 일부를 cloze-delete 할 수 있고, AI가 수식도 거의 완벽에 가깝게 생성, 나는 원하는 대로 소폭 수정 가능. 그래서 스페인어 튜터링이나 수학 수업 중에도 실시간 플래시카드 노트를 만드는 게 아주 현실적으로 가능. Anki와 다르게 세세한 커스터마이즈
       대신 바로 쓸 수 있을 정도의 최적화와 단순함이 장점. 개발 엄청 빠르고, 릴리스 노트 영상도 훌륭, 소소한 업데이트는 거의 주별로 이루어지는 편. 해외 여행 등 인터넷이 약한 곳에서는 버그도 만났지만 전반적으로 매우 만족
          + 지나치게 비싼 느낌. 한 달에 18달러라는 가격이 당황스러움. Electron 기반 앱일 가능성이 높을 거라는 추측
          + Anki의 불편함에 지쳐 이후로 Mochi라는 앱의 팬이 된 입장. macOS와 iOS용 네이티브 앱이 정말 잘 만들어져 있음. 카드가 markdown 형식이라 LLM을 시스템 프롬프트로 불러 커스텀 생성까지 가능. 오늘 발견했는데 API도 제공해서 LLM이 새로운 카드를 자동으로 서버를 통해 등록하게 해볼 생각 중. https://mochi.cards/ 참고
          + 컴퓨터공학 학위 내내 이 앱을 썼고, 효과 확실하게 봄. 이제는 개인 지식베이스로 아주 잘 쓰는 중
          + 추천 고마움 표명. Anki에서 똑같은 어려움 겪고 있었고 오픈소스임에도 비슷한 앱이 적어서 놀람. Remnote 직접 써볼 기대감 표현
     * SRS 채택에 있어 (특히 내 경우에) 잘 알려지지 않은 또 다른 장애 요인 언급. 배우는 주제로 직접 카드를 만드는 데 있어서 자신감이 없고, 오히려 틀린 정보를 매우 효율적으로 암기할 위험 때문에 꺼려하는 마음
     * Andy Matuschak의 글 https://andymatuschak.org/books/에 영감을 받아 Q&A와 SRS를 ePub 안에 넣은 https://readboost.io/를 제작한 경험 공유. 아직 버그 가능성 있지만 개인적으로는 꽤 유용함을 느낌
     * 동일 저자의 블로그 글 How to write good prompts를 강하게 추천. 이 글로 인해 개인적으로 spaced repetition의 개념이 제대로 이해되는 경험
     * 안드로이드에서 추천할만한 spaced repetition 앱이 있는지 질문. 오로지 spaced repetition만 하는 앱을 찾는 중
          + 바로 윗 댓글과 비슷한 맥락에서 Anki(https://apps.ankiweb.net/) 추천. 오픈소스이고, 여러 플랫폼 지원, iOS 제외 무료, 활성 커뮤니티 및 생태계 존재
          + AnkiDroid 추천
     * 시험 직전 벼락치기에 좋지만, 장기적이고 깊이 있는 사고에는 나에게 맞지 않는다는 경험
          + Spanish 동사 변화 학습에 SRS 큰 효과 체감, 덕분에 전체적인 스페인어 실력 향상 경험. 해당 덱을 잘 만들어준 사람이 있어서 가능했던 측면. https://www.asiteaboutnothing.net/w_ultimate_spanish_conjuga... 참고
     * SRS의 변형을 독서 노트 정리에 활용하는 방식 선택. 책에서 얻은 인사이트를 여러 개로 쪼개어 일일 3~4개씩 리뷰하고, 그 중에서도 특별히 중요한 건 ‘daily review’로 분류해서 복습 주기를 자유롭게 조정하는 패턴. 15년 가까이 이렇게 해온 결과 만족스러운 노트 복습법
"
"https://news.hada.io/topic?id=20936","Kagi Search의 과제형 면접에서 탈락한 경험","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Kagi Search의 과제형 면접에서 탈락한 경험

     * 소프트웨어 개발자 면접에서 제출받는 ""take-home assignment""의 비효율성과 지원자 시간 낭비 문제를 강조함
     * Kagi Search 지원 과정에서 과도하게 광범위하고 모호한 과제 요구사항을 경험함
     * 제안한 구체적 실행계획에 대해 매니저의 명확한 피드백 부재 및 비효율적 소통 경험함
     * 심혈을 기울여 개발한 프로젝트 제출 후, 명확한 사유 없는 탈락과 기준 변경 등 불합리함을 느꼈음
     * 보다 나은 채용 프로세스에 대한 대안(예: 실시간 코드 리뷰)과, 과제형 면접 과도 요구에 대한 문제의식 공유함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서문

     * Take-home assignment란 소프트웨어 개발자 면접에서 지원자에게 문제를 출제하고, 이를 코드로 구현해 제출하게 하는 평가 방식임
     * 본 포스트는 평판이 좋은 회사라고 생각했던 Kagi Search에 지원하여 실제로 받은 과제와 경험을 통해, 이 방식의 비합리성과 지원자 시간 낭비 문제를 지적하고자 함

카기 서치에 입사지원

     * Kagi Search의 백엔드 개발자 포지션에 이력서를 제출했음
     * 해당 역할의 요구사항은 다음과 같음
          + 백엔드 시스템 구축 경험
          + Go 언어 활용 능력
          + 대규모 백엔드 시스템 확장, 유지 경험
          + SRE 등 팀원과의 협업 능력
          + Docker 등 컨테이너 기술 이해

면접 과제 안내 및 요구사항

     * 서류 통과 후 take-home assignment 안내 메일을 받았음
     * 과제 주제: ""최소한의 터미널 영감 이메일 클라이언트 구현""
          + 터미널 또는 웹앱 형태 중 선택
          + 기본 이메일 보기/보내기 기능
          + 가짜 또는 실제 이메일 백엔드(IMAP/POP 등) 자유 선택
          + plaintext 메시지 처리만 필수, rich text 불필요
          + 프로젝트 제출: 깃허브 저장소, 배포본 제공, 설치 안내 문서 포함
     * 명확한 가이드 부족 및 프로젝트 규모가 상당히 큼
     * 평판 때문이라도 과제 수행을 결정하게 됨

매니저와의 커뮤니케이션

     * 명확한 과제 범위 및 추가 기능 관련 문의 시, “무엇을 추가할지는 지원자 자유”라는 모호한 답변만 받음
     * 과제에 시간과 노력을 투자하기 전, 구체적인 실행계획(Proposl) 을 공유하고 이에 대한 회신을 요청했으나 특별한 피드백 없이 진행

과제 제안 및 계획

     * 실행 계획:
          + Go 기반 웹앱
          + AWS ECS Fargate 통한 배포 및 SSL 적용
          + 이메일 발신자 서비스(Postmark) 연동
          + 로그인/인증 기능 추가
          + 이메일 송수신 및 UI 표시
     * 기술 선정 근거:
          + 백엔드 역할과 관련된 다양한 기술 활용
          + Infra-as-Code 도구 활용 통한 실질적 시스템 구축
          + 단순한 API 연동을 넘어 실제 서비스에 가까운 기능 구현
     * 주요 구현 요소:
          + Pocketbase(backend/DB), TEMPL(템플릿), Pulumi(IaC), Postmark(이메일 서비스)
          + 페이징, 로그인 등 구현
          + Stretch Goal: 데이터 백업과 복구 등 안정성
     * 결론: 미리 사전에 충분한 설명과 근거를 제시했으며, 이에 대한 명확한 검토와 회신을 기대했음

매니저의 회신 및 소통 문제

     * 구체적 검토 없이 “흥미로운 제안”이라는 짧은 답변만 받음
     * 제안의 적합성이나 개선 요구 등 피드백 전무
     * 지원자 입장에서는 시간 투자 및 노력에 대한 존중 부재로 느껴짐

프로젝트 과제 수행

     * 주어진 요구와 제안 사항을 모두 구현했고 일주일 풀타임을 투입함
     * 프로젝트 데모 영상 및 깃허브 레포지토리, 상세한 문서까지 제출하였음

탈락 통보와 그에 대한 피드백

     * 자동화된 탈락 이메일 수신 후, 피드백 요청 시 받은 답변:
          + “더 강력한 지원자 제출물이 있었음”, “채용 경쟁이 치열함”
     * 문제점
         1. 단순한 솔루션 선호라면 애초에 안내 가능
         2. 제안이 부족했다면 그 시점에 피드백 가능
         3. 탈락 이후에도 공고는 여전히 게재 중, 단순 경쟁 아닌 과도한 시간 소모 요구라고 판단됨
         4. 프로젝트 제출 후 요구사항 더욱 강화, 제출 솔루션이 기준을 높인 셈

결론 및 문제의식

     * 이 같은 방식은 많은 구직자에게 불합리, 실질적 생계 위협까지 동반
     * 과도한 무급 과제 요구에 대해 거절하거나 문제를 제기할 필요성 강조
     * 프로젝트성 과제를 대체할 실시간 코드 리뷰 등 더 나은 채용 프로세스 가능
          + 실제 개발 문제 해결 능력을 비동기/동기 코드 리뷰 등으로 들여다 볼 수 있음
          + Leetcode 스타일 문제 풀이와 현업 요구간 괴리 지적
     * 구직자 소진과 부당한 평가 문화 개선을 촉구함

더 나은 채용 절차를 위한 제안

     * 실시간 코드 리뷰 방식 등 개발자 역량을 더 의미 있게 평가하는 대안 제시
     * 타이머 기반 알고리듬 퍼즐 풀기 중심보다는, 실제 역량 및 문제해결능력 중심으로 변화 필요성을 역설함

        Hacker News 의견

     * 솔직한 채용자 입장에서 의견을 남김. 개인적으로 takehome 과제를 싫어함. 이런 과제는 모두의 시간을 낭비하게 만듦. 채용의 마지막 단계라면 이해 가능하나, 지원자 거르기 용도로 쓰는 건 비효율적임.
         1. 너무 많은 질문이 오감. 모호함 속에서 판단력을 사용해 해결하는 것이 과제의 일부임. 주어진 조건대로라면 하루이틀 투자해 간단한 로컬 프로젝트 수준이면 충분했을 것임.
         2. 제안서 작성과 공유가 너무 과도했음. 지원자는 철저함을 보여주고 싶어한 것 같지만, 실상 회사 입장에서는 비효율적이고 시간이 낭비된다고 생각할 수 있음.
         3. 완성된 결과물은 기능적이긴 했으나 인프라와 마감에 너무 공 들였음. 불합격 시엔 결국 시간 낭비가 되었음.
         4. 터미널 영감을 받은 이메일 클라이언트라는 요구사항이 있었던 것 같은데, 결과물에서 그 방향성을 못 찾았음.
            지원자의 능력과 일에 대한 열정을 인정함. 조금 다른 시선으로 의견 주고 싶었음.
          + 블로그 작성자의 태도가 조금 거슬린다는 느낌을 받음. 글만 봐도 함께 일하기 어렵고, 자율적으로 결정을 내리기 힘들며, 명쾌한 가이드라인을 필요로 하는 인상임. 대기업에는 맞지만 스타트업과는 안 맞음.
            ""터미널 영감의 이메일 클라이언트 개발해 고객들 알파 테스트""가 초기 스타트업 엔지니어에게는 타당한 요청임. 사양이 더 있더라도 많은 부분이 엔지니어 판단에 달려있음. 지원자는 확실성을 너무 원함.
            Kagi가 과제 완료 후 어떤 피드백을 줄 건지 미리 궁금해 하는 건 좋은 신호가 아님. 확정적인 답변을 줄 수 없는 상황임. 아마도 그들은 수백, 수천 건의 지원서를 평가 중일 것임.
          + 작가는 일 잘했지만, 암묵적으로 ""너무 노력하지 말 것""이란 조건에서 실패했음.
            만약 요구사항 명확화가 필요 없었다면, 차라리 ""1~10 사이 숫자 맞추기""를 시켜 잘못 고른 이들을 탈락시키는 셈임.
            과제에 공 들이고 좋은 마감을 한다고 탈락시키는 사례엔 동의 못함.
            이런 모호한 과제는 사실상 ""컬쳐 핏"" 선별의 또 다른 방법일 뿐임.
          + 내 생각에 지원자는 자신의 아이디어를 검증하는 접근이 요즘 엔지니어링 방식과 닮아 있음. 건강한 팀은 기능 설계서를 영어로 설명해 승인받고 진행함.
            ""코드 먼저, 피드백은 나중"" 문화는 커리어상 가장 해로운 경험이었음. 이 지원자는 현대적인 소프트웨어 관행을 따랐음. (나는 1000명 넘는 엔지니어를 둔 회사의 채용 담당자임)
          + 채용자 입장에서 마음에 드는 takehome 과제는 30분 내 완성 가능, 평가 기준 명확, 다양한 접근과 트레이드오프 고민 포함 등임.
            나도 지난 구직 때 광범위한 takehome 과제에서 어떤 부분이 평가 기준인지 몰라 탈락했음. 이 경험 이후 이런 과제에 심한 거부감이 생겼음.
          + 제안서가 너무 거대해진 게 탈락 이유라 생각함. 지침에 제안서 요청 없었고, 디테일하게 제출하면 오히려 ""자율적 추진력 부족""으로 해석될 수 있음. 이렇게 되면 코드 퀄리티는 더 이상 무의미해짐.
            회사는 시간 낭비하게 두지 말고 그 시점에서 과제를 끝내주는 게 낫다고 생각함.
          + 요즘 업계에서 요구사항 파악 능력이 부족해 독심술 능력으로 개발자를 거르는 현실이 유감임.
          + 지원자가 마감일을 지키지 못한 점도 문제임. 특별한 사정 설명 없는 지연은 이미 탈락임. 마감 내 적절한 솔루션 설계가 과제 목적이었음.
          + 4번 터미널 관련 언급에 대해, 작가가 공유한 전체 버전에는 해당 부분 설명 있음.
          + 이런 논의는 결과를 놓고 보면 다 쉽게 할 수 있음. 반대 전략(요구 미확인, 최소 요건만 충족)이라도 마찬가지 결과가 나왔을 것임. 이런 상황에서 언제나 ""더 적극적으로 요구 명확화가 필요했다""는 의견이 반대 방향으로도 나올 수 있음.
     * 코드를 보고, 그리고 데모 영상을 봤을 때 한 주 동안 웹앱 두 페이지 짜는 데에 시간이 꽤 걸렸구나라는 인상을 받았음.
       가장 기본적인 이메일 기능(메일 열기 등)도 없고, 백엔드 엔지니어 자리 지원이면서 실제론 postmark와 turso 같은 외부 제품을 이용했으며, 기본 기능(플레인텍스트 포매팅, 보기, 폴더 등) 부재가 보임.
       관리자 페이지, 로그인 같은 부가 기능이 있긴 한데, 메일 헤더 맵 등 최소한의 데이터 구조도 빠져있었음.
       좋은 엔지니어일 수는 있겠으나 해당 포지션엔 부적합하다고 판단함.
       takehome 제안서도 너무 이례적이라고 느꼈음.
       최초 공고를 다시 읽어보니 ""최소한의 터미널 영감 이메일 클라이언트""와 aerc, mutt, himalaya 등 레퍼런스가 명시되어 있었음. 이는 명백한 요구사항 해석 실패임.
          + ""요구사항 해석 실패""라는 말에, 도대체 어떤 요구사항을 못 맞췄는지 궁금함.
            터미널 혹은 웹앱 형태의 이메일 클라이언트, 기본 기능 구현이 요구였는데 이를 충족했다고 생각함.
            터미널 기반 툴에서 영감을 받으라는 부분은 주관적임. 백엔드 포지션이라면 UI에 신경 쓰는 것이 비효율일 수도 있다고 생각함.
          + 지원자가 시간 투자했는데 거절 메일 한 통만 받는 건 속상한 일임.
            피드백이라도 받을 수 있으면 성장에 도움이 될텐데 그조차 현실적으로 어려울 때가 많음.
            그래서 takehome 과제에 대한 회의감이 듦. 지원자와 채용자 모두 서로 시간에 대한 존중이 필요함.
          + ""Email client can either be in the terminal (i.e. a TUI) or a web app""이라는 문구가 존재함.
     * takehome 평가는 유의미하지만 반드시 시간 제한이 있어야 함.
       2~3시간이면 충분히 지원자 평가할 수 있음. 시간 제한이 있었다면 지원자도 그 내에서 알맞은 해결책 제시가 가능했고, 회사가 원하는 바도 분명했을 것임.
       또한 회사가 ‘어떤 답안이든 OK’인지, ‘최고의 답안을 희망’하는지 명확히 안내가 필요함.
       takehome의 의도가 테스트 통과냐, 미션 충족률이냐, 코드 품질이냐 등 종류마다 달라서 지원자가 헷갈릴 수 있음.
          + 채용 담당 관점에선 Kagi의 과제가 너무 방대하고 지원자 시간에 대한 예의가 없다고 생각함.
            오히려 시간이 없는, 바쁜 사람들을 걸러낼 위험이 큼.
            우리 회사는 그냥 간단한 ETL 문제를 내고, 4시간 제한을 둠.
            다 풀지 못해도 괜찮게 유도하고, follow-up에는 코드 리뷰 및 질문 시간을 가짐.
            진짜 역량은 이 후속 미팅에서 파악할 수 있음.
          + 지원자들이 실제 투입 시간보다 훨씬 더 투자할 수도 있는데, 그 점을 채용 담당이 어떻게 알 수 있는지 의문임.
            공평한 시간 단위가 지켜지는 현장 과제와 달리 takehome은 각기 다른 시간 분배로 인해 손해를 볼 수 있음.
            이럴 바엔 1시간 현장 코딩이 더 나음. 지원자와 면접관이 같은 시간을 투자해야 서로 시간의 가치를 존중할 수 있음.
          + 라이브 리뷰가 라이브 코딩보다 훨씬 낫다고 생각함. 만약 라이브 코딩을 한다면, 내 랩탑으로 45분 멀리서 작업하고 돌아와 리뷰하는 방식이 낫다고 봄.
     * 회사 답변이 불친절하고 도움도 안 된 건 사실이지만 요구사항에 '터미널 영감'이 명확히 명시되어 있었음.
       데모 영상에선 일반적인 웹앱만 보여짐. 명시적으로 aerc, mutt, himalaya 등 기존 터미널 이메일 클라이언트에서 영감을 받으라고 했음.
       내가 뭔가 놓친 게 있는지 궁금함.
          + 거절 메일에서 명확하게 이유를 설명해줬다면 더 좋았겠지만, 애초에 과제 설계에서 터미널 클라이언트 레퍼런스가 직접적으로 요구됐음.
            터미널 클라이언트 특유 UX가 아직 '정답'이 없는 분야라, 오히려 그 지점을 평가 기준으로 잡았을 가능성이 큼.
          + Go 언어 중심임을 보았을 때, CLI 만드는 데 20줄도 안 걸리는데 굳이 웹 GUI 개발에 몰두했던 선택이 의문임.
          + ""Email client can either be in the terminal (i.e. a TUI) or a web app""이라는 안내가 있음.
     * 최근 면접에서 유사한 경험을 겪었음. 과제 프로젝트를 매우 잘 제출했지만 프로젝트에 대한 대화 없이 바로 탈락 통보만 받았음.
       과제 요청을 했다면 반드시 follow-up 미팅을 거쳐야 한다고 믿음.
       이미 Kagi를 유료로 써왔지만, 이번 일로 계정 탈퇴를 고려하고 있음.
       후보자에게 대화를 할 여유조차 없다면 애초에 과제 자체를 요구하지 말아야 함.
          + takehome 과제는 지원자뿐 아니라 평가하는 면접관에게도 상당한 노력이 필요함.
            리뷰까지 하고 나면 피드백을 받을 권리가 있다고 생각함.
            하지만 현실적으로 수십 명의 뛰어난 지원 중 한 명만 뽑다 보면, 불합격이 곧 ""실력이 부족하다""는 의미는 아님.
            법적으로도 '왜 A를 뽑고 B를 탈락시켰나?'의 공식 답변은 흠집 잡기밖에 없게 되는 게 현실임.
          + 회사가 어떻게 평가할지 명확한 기준을 공개했거나, 피드백을 줬다면 더 나았음. 실패 피드백 없는 시간 낭비는 용납할 수 없다고 봄.
            여러 회사들은 이 점을 잘 처리함.
          + 과연 '뛰어난 솔루션'이 맞았는지 의문임. 최소한의 터미널 영감 이메일 클라이언트라는 요구와 관련 레퍼런스를 완전히 무시한 것 같음.
            이런 요구사항 오해가 클 경우 논의 자체가 생략될 수 있음.
     * 이 사례는 과제 본문 속 숨은 의도를 잘못 읽은 전형적인 케이스임.
       회사는 자율적이고 스스로 목표를 세울 수 있는 사람을 원했음.
       모호함은 답변을 다각도로 탐구해 풀어내는 지원자의 역량을 보기 위함임.
          + 그 과제는 최대한 단순하고 기발하면서 기능 동작하는 솔루션을 낼 수 있는 사람을 위한 것이었음.
            프로토타입 위주의 회사를 위한 특성에 안 맞는 사람도 있기 때문에, 어떤 후보는 10분 생각해서 60분 내 최대치를 만들어냈을 것임.
          + R&D 프로젝트와 '최소화'만 강조했을 경우, 도대체 프로토타입인지, 사용자 대상인지, UX를 신경써야 하는지 등 요구 사항이 모호해 평가자가 뭘 중시하는지 맞추는 게임일 뿐임.
          + 이런 ""스스로 해석하라""는 과제에선 요구사항 명확화나 추가 질문을 하지 않은 사람이 탈락할 수 있음.
            하지만 개발자에겐 이런 질문 능력이 매우 중요한 덕목임. 그래서 채용 방식에 대해 더 기대하고 실망할 수밖에 없음.
          + ""misreading the subtext""는 요구 자체에 나와있었다고 생각함.
          + 교육 현장에서 '이해 못 한다'고 학생만 탓하는 건 너무 편한 결론임.
            정작 애매한 설명 때문이다.
            훌륭한 교사는 최대한 많은 이들이 이해하게 해줌. 혼란스러운 학생이 많다면, 출제자가 문제임.
            대학생들은 불가의 선승처럼 코안을 풀 일 없어야 함.
     * 글쓴이가 직접 글을 올렸기에 Kagi에서 일한 경험을 바탕으로 맥락을 설명해주고 싶음.
       예전엔 Vlad가 직접 지원자를 평가했고 과제도 이런 식이었음.
       회사가 커지면서 이제 다른 이들이 평가하는 듯함.
       Vlad는 HN 스타일의 성향이 있어 ""쿨하다""라고 느끼는 지원자와 일하고 싶어함.
       예를 들어, 디자인 문서를 길게 써서 ""Galactor를 사용할 거며 프로젝트는 플롭-레디다.""라고 하면 완전히 반대 효과임.
       ""터미널 영감""이라는 요구엔 모든 키보드 단축키 등 실제로 터미널 앱이 구현된 디테일을 기대하는 경향임.
       이런 기준이 좋은 필터인지는 논란의 여지 있지만, 만약 그 맥락을 이해하고 통과할 역량이 있다면 과제가 쉽게 보일 것임.
       Kagi가 이런 맥락을 더 잘 소통했으면 좋겠음. 시간 낭비된 건 아쉽지만, 성향에 맞는 회사를 찾길 바람.
          + 많은 회사들이 자기와 비슷한 사고방식을 찾고자 함.
            다양성 없는 팀은 모두가 같은 벽에 부딪쳐 정체될 수 있음.
            이 현상은 스타트업에서 특히 흔하고, 9/10이 실패하는 이유와도 연관 있다고 생각함.
          + Vlad가 ""쿨한 사람""을 찾기 위해 너무 많은 사람의 시간을 낭비한다는 점이 문제라고 느껴짐.
            명확한 기준 없이 채점하는 과제는 불공정하다고 생각함.
            결국 ""내 머릿속 답""을 맞추라는 암시적 과제와 다름없음.
            사람에 대한 배려가 부족하다는 인상을 받음.
          + ""나는 정말 이런 부분을 미리 알았어야 했나?""라는 의문이 있음.
            이런 문화라면 누구나 언더커버처럼 조사해 알아내야 했었는지 알 수 없음.
            나와 같은 '쿨하지 않은' 지원자도 명확한 신호를 받고 빠르게 다른 회사를 알아볼 수 있으면 더 나을 것임.
     * 코드를 직접 검토해본 결과, 첫 파일에서부터 목적성 없이 샘플 코드만 베낀 듯한 주석과, 불일치하는 설명, 주의력 부족이 느껴지는 표현을 확인했음.
       이런 세부 사항 미비로 인해 리뷰를 그만둘 것임.
          + 앱을 내 도메인에 배포했고 성능상 문제없었음. 인증 및 인프라 등 백엔드 특성도 잘 구현함. 그런데 코드 주석에 더 신경을 써야 한다는 지적엔 동의 못함.
          + 이 사례에서 핵심 문제는 불합격 자체가 아니라, 명확한 가이드 없고 피드백조차 제공하지 않는 채용 방식이 지원자 시간에 전혀 예의가 없었다는 점임.
     * DuckDuckGo에서도 유사한 경험이 있었지만 모든 지원 단계에서 소정의 보상을 받았음.
       간단한 디자인 제안서부터 구현 과제까지, 제한 시간을 명확히 두고 진행함.
       결과적으로 합격하지 못했고, 명확한 이유는 알려주지 않았음.
       지원자가 너무 많았던 것 같긴 하나, 이런 경험이 오랜 기간 정서적으로 크게 남았음. OP의 심정에 공감함.
     * 엔지니어링 면접관 입장에서 논함. leetcode도 takehome도 모두 시간이 많이 들고 정보량이 부족함.
       하지만 내가 채용담당이어도 글쓴이를 불합격시켰을 것 같음.
       스타트업은 신속하고 실용적으로 일하는 인재를 원함.
       과거 동료 중 주변 의견 수집 후 며칠씩 혼자 몰입했다가, 요구사항이 이미 바뀌어버리는 경험을 했고 모두에게 좋지 않았음.
"
"https://news.hada.io/topic?id=20917","CodeCafé - 브라우저에서 협업 코딩 가능한 오픈소스 편집기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CodeCafé - 브라우저에서 협업 코딩 가능한 오픈소스 편집기

     * 브라우저 기반 실시간 협업 개발 환경으로 수업이나 페어 프로그래밍에 적합하지 않은 기존 문서 기반 협업 툴의 한계를 해결하고자 개발
     * ""실시간 협업 코딩은 복잡하다""는 고정관념을 깨고, Zero Setup, 진정한 실시간성, 직관적 UI를 통해 누구나 바로 함께 코딩할 수 있게 설계
     * 픽셀 퍼펙트한 실시간 미리보기 지원: HTML/CSS/JS 코드를 작성하자마자 WebView에 즉시 반영
     * 협업 기능: Operational Transformation 기반으로 여러 사용자가 충돌 없이 동시 입력 가능
     * VS Code 스타일 편집기: Monaco Editor를 활용한 친숙한 인터페이스 및 자동완성, 문법 강조, 오류 표시 기능 제공
     * Xterm.js 기반 브라우저 터미널을 포함하여 입출력 확인 가능
     * 무설치 실행: 브라우저만 있으면 즉시 사용 가능
     * GNU AGPL 3.0 라이센스

기술 스택

     * 프론트엔드: React, TypeScript, Tailwind CSS, Zustand, Xterm.js, Monaco Editor
     * 백엔드: Java Spring Boot, WebSocket, Jackson
     * 실시간 동기화: 커스텀 Operational Transformation 알고리즘
     * 메시지 처리: Redis + Lua 스크립트를 통한 원자적 데이터 처리
     * 호스팅: 프론트는 Vercel, 백엔드는 AWS EC2, Redis는 ElastiCache

Operational Transformation(OT)

     * OT는 문서의 실시간 동시 편집을 가능하게 하는 핵심 기술로, Google Docs도 이를 기반으로 함
     * CodeCafé는 OT를 직접 구현하여 다음을 지원함:
          + 동시 입력 감지 및 변환
          + 의도 보존 및 충돌 해결
          + 클라이언트 간 상태 동기화 유지
     * 이로 인해 실시간 협업 환경에서 자연스럽고 부드러운 사용자 경험 제공

향후 계획

     * 사용자 인증 및 프로젝트 저장 기능
     * 음성/텍스트 채팅 통합
     * 코딩 히스토리 재생 기능
     * 다양한 언어 지원 확대

   vscode에 live share라는 기능이 생각나네요
   실시간 채팅에 코드놓고 동시편집도 되고 그랬었는데
   주니어들 멘토링하거나 온라인강의하거나 소개해줘도 다들 잘 쓰지는 않더라구요

   code server가 이미 있어서 차별점을 잘 모르겠지만
   채팅과 코딩 히스토리 재생이 추가될경우는 좋을것같아요

   Zed도 그렇고 실시간 동시 작업에 대한 수요가 얼마나 있을지 모르겠네요. 작업하는 모든 사람이 동일한 에디터를 사용해야 한다는 문제도 있을 것 같은데, 또 성능이나 사용자 경험이 정말 중요한 소프트웨어이다보니..
"
"https://news.hada.io/topic?id=20954","BuyMeACoffee가 조용히 여러 국가에 대한 지원을 중단함 (2024년)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              BuyMeACoffee가 조용히 여러 국가에 대한 지원을 중단함 (2024년)

     * BuyMeACoffee가 수많은 국가와 지역에 대한 Payoneer 및 일부 결제 수단 지원을 예고 없이 중단함
     * 주요 변경 사항에 대해 명확한 사전 공지나 공식적인 공개 안내가 이루어지지 않음
     * 출금 수단 제한으로 인해 우크라이나 등 Stripe 미지원 국가의 창작자들이 소득을 잃는 심각한 문제 발생
     * BuyMeACoffee는 “새로운 기능과의 호환성” 등을 이유로 들었으나, 커뮤니케이션 방식과 사용자 지원에 큰 부족함 드러남
     * 지원 중단 및 그 과정에서 보여준 불성실한 소통으로 인해 신뢰성 및 사용자 피해 우려 확산됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   BuyMeACoffee에서 수많은 국가에 거주하는 창작자를 대상으로 별다른 공지 없이 Payoneer 및 일부 결제 수단 지원을 조용히 중단함.
   이러한 변화는 주로 우크라이나 창작자들의 출금 불가로 드러나기 시작했으며, 공식 트위터나 뉴스레터 등에서는 별도의 사전 안내가 없었음.
   해당 사안은 창작자 및 후원자들에게 큰 영향을 미침
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 경과

     * 최근 우크라이나 창작자들이 BuyMeACoffee에서 출금 문제를 겪고 있음
     * 초반 지원 답변에서는 “정책 변경”이나 “규정 준수” 등 형식적 안내만 제공됨
     * Mercury, Patreon 등 다른 주요 플랫폼도 유사한 국가 블랙리스트 적용 사례가 있음
     * 며칠 후 BuyMeACoffee가 Payoneer 지원을 완전히 중단, Stripe만 남게 됨

변경사항의 확인

     * 최신 BuyMeACoffee 지원 문서에는 Stripe만 언급되어 있음
     * 인터넷 아카이브 자료에 따르면 2024년 2월까지는 Payoneer와 Stripe 모두 나와 있었으나, 5월부터는 Stripe만 남음
     * 지원팀의 비공식 답변과 문서 내용이 일치함
     * 몇몇 우크라이나 사용자들은 최근까지도 Payoneer로 출금을 받았던 경험을 공유함
     * 그러나 공식 커뮤니케이션에서는 관련 변경사항이 전혀 안내되지 않았음
          + 트위터, 공개 changelog, 이메일 등 어느 곳에서도 공지 없음

영향 및 문제점

     * Payoneer와 Stripe의 국가별 지원 차이로 인해, 최소 95개 국가 및 지역이 새롭게 출금 불가 상태에 놓임
     * 우크라이나 등에서는 BuyMeACoffee가 중요한 수입원임
          + 가수, 군복무자, 연구자, 독서동호회 운영자 등 다양한 창작자가 영향을 받음
     * 이들은 더 이상 플랫폼에서 모은 자금을 출금 불가하게 됨
          + Stripe 미지원 지역 사용자들에게는 현실적으로 금전 사용 자체가 불가능해짐
          + 연간 후원금 등도 ‘공중에 떠있는’ 상태가 됨
     * 변경의 이유나 대응책에 대한 공식적 설명이 없음
     * 이로 인해 BuyMeACoffee에 대한 신뢰성 저하와 사용자 불안감이 증대됨

커뮤니케이션 및 지원 대응

     * 사전 공지, 선택지 제공, 충분한 사유 설명 등이 모두 부족했음
     * 이번 변경 과정은 교과서적인 ‘나쁜 커뮤니케이션’ 사례로 지적됨
     * 창작자 및 사용자와의 소통 미흡, 지원팀의 회피적 대응이 문제가 됨

공식 답변 (2024년 8월 13일자)

     * BuyMeACoffee가 출금 불가시 환불 처리 의향을 밝힘
          + 그러나 실제 환불 사례 및 적용 방안에 대한 구체적 설명 부족
     * “우크라이나 창작자의 출금 보류 없음” 주장
          + 실제 현장에서는 문제를 겪는 사례가 다수 보고됨
          + 지원팀이 자주 입장을 번복하는 등 신뢰성 약화
     * 대부분의 창작자가 Stripe를 사용한다는 이유로 Stripe 중심 정책을 택함
     * “대체 출금 수단”을 모색 중이라 밝혔으나, 구체적 시기와 방법 미공개
     * 커뮤니티 내 비판에 대해 “스팸 및 허위정보 유포시 차단” 방침을 드러냄

추가 업데이트 (2024년 8월 14일자)

     * 일부 우크라이나 창작자에게 “업데이트 예정 기능과의 호환 문제”가 Payoneer 지원 중단 사유임이 비공식적으로 전달됨
          + 이 사유는 기존 공식 발표와는 다름
     * BuyMeACoffee는 커뮤니케이션 요구가 지속되자 비난한 이용자를 X(Twitter)에서 차단함
     * Payoneer의 실제 서비스 종료 일정 등 일부 정보가 뒤늦게 추가로 제공됨

관련 참고

     * 우크라이나 이용자들이 Patreon 등 다른 플랫폼도 사용할 수 없는 이유에 대한 기사 및 사례 공유됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

종합 결론

     * BuyMeACoffee의 수단 제한 및 조용한 정책 변경은 글로벌 창작자 경제에 상당한 영향을 미침
     * Stripe 미지원 지역의 경제적 배제 문제가 발생함
     * 사전 안내 부재, 응대 태도, 일관성 없는 정보 제공 등 커뮤니케이션 측면에서 큰 비판을 받고 있음
     * 해당 사례는 글로벌 서비스가 정책을 투명하고 공정하게 운용해야 할 필요성을 잘 보여줌

        Hacker News 의견

     * 우리는 금융 및 결제 시스템을 주로 법 집행 감시 기능을 수행하는 수단으로 여기는 현상이 정상화된 상황을 지적함, 이는 소규모 계좌의 디뱅킹과 같은 문제로 이어지며, 결제 회사들은 마진이 매우 얇고, 악성 행위를 실수로라도 처리할 경우 벌금이 엄청나기 때문에, 약간이라도 위험해 보이면 서비스 거부하는 선택이 논리적인 판단임
          + 소액 계좌의 디뱅킹은 전에 들어본 적 없는 내용이지만, ‘원하지 않는 이들’의 디뱅킹은 분명한 문제라고 봄, 벨기에에서는 성노동이 합법이지만, 성노동자가 은행 계좌를 만들기 매우 어렵고, 여러 은행에 거절당했다는 증빙이 있어야 기본 계좌를 비싸게나마 만들 수 있는 법안이 있음, 은행이 사실상 법 집행, 세금 징수, 반테러, 도덕 경찰이 된 모습이 아이러니, 정작 많은 은행들은 법을 당당히 어기거나 타락한 모습을 보이는데도 ‘선행의 관문지기’ 역할을 함
          + 이 회사들은 공공 인프라가 아니며, 미국 은행이 우크라이나나 벨라루스와 환전 업무하지 않는다고 불만을 제기하는 이가 없듯, 미국 회사를 통한 인터넷 기부도 다르지 않게 여김, 국경 간 명확한 서비스·상품 교환 없이 자금만 오가는 플랫폼은 본질적으로 돈세탁에 활용되며, 정부 입장선 이를 제한할 이유가 충분, 해당 국가에서 얻는 수익 자체가 크지 않으니 특이할 만한 문제라고 보지 않음
          + 대안으로 크립토가 있으며, 크립토는 어떤 이유로든 누구든 서비스 제공 가능성
          + 현 시점에서는 정부가 결제 인프라를 직접 장악하는 것이 오히려 더 낫다는 생각, 이는 흔히 디스토피아로 보이지만 이미 실질적으로 그렇게 된 셈, 공식화한다면 차단·거절·동결 기준이 공개적으로 제시됨으로써 투명성 생김
          + 대안이 별로 탐탁지 않을 거라는 예감, 예를 들어 AML(자금세탁방지) 규제가 약하거나 인가(MTL)를 얻기 쉬운 세상은 크립토 시장과 비슷한 상황, 즉 은행이나 결제회사가 갑자기 사라지거나 ‘먹튀’ 사고 가능성을 상상
     * 이런 일이 주목받아서 다행, ‘2등 국가’에서는 이런 상황 흔히 일어나며, 다양한 서비스에서 늘 벌어지는 일상, 선진국처럼 쉽게 제공 불가능한 서비스의 경우 불운한 국가엔 별 신경도 쓰지 않음, 시장성이 낮으니 투자 대비 기대수익 안 맞음, 결론적으로 시장 논리에서 벗어나기 어렵다고 판단, 서비스의 정책을 명확하고 최신 정보로 공개해야 한다는 점이 특히 중요 포인트
          + 왜 크립토가 이런 상황에서 해결사 역할을 못 하는지 궁금, “코드가 곧 법”인 사례 같음
          + 유럽 같은 서구에서도 우크라이나로 돈 보내려는 수요 꽤 있는데, 기존 회사가 빠지면 곧 경쟁사가 대체하는 거 아닌지, 블로그 사건 이후 1년 가까이 새 회사가 생긴 사례 있는지 궁금
     * 나는 수년간 은행과 핀테크 업계에서 일했으며, 회사들은 수익보다 규제 준수를 우선시함, 컴플라이언스가 국가 차단을 주문할 수 있고, 이는 다른 부서를 모두 우회함, 계좌를 열 때 오래 걸리는 이유도 KYC(Know Your Customer)와 KYB(Know Your Business) 절차가 수기로 위험 점수 측정 후 승인하는 식이기 때문, 대부분 필요 서류만 갖추면 과정이 체크박스 채우기에 불과, 전쟁 지역이나 제재 대상 국가에 서비스하는 건 위험하며, BuyMeACoffee나 벤더들도 해당 위험 감수하지 않음
          + 사람들이 바랐던 건 거액 후원이 아니라 단지 커피 한 잔 정도의 격려였던 상황
     * 결제 게이트웨이는 정부 정책의 영향 아래 움직이며, 결제 호스트는 게이트웨이의 임의 규정 탓에 과잉 대응하거나 임의적 룰을 만들곤 함, 인도의 한 대형 결제 게이트웨이가 해커 뉴스 링크 때문에 내게 '해커맨' 취급을 하며 링크 삭제를 요구한 경험, 1인창업자인 내 입장에서 구독 결제가 지원되지 않아 다양한 게이트웨이를 전전함, 결국 주요 결제 게이트웨이 지원하는 FOSS 기반 셀프호스팅 결제 호스트를 직접 개발 중, 이를 통해 사용자들이 결제 주도권 가지는 목표
     * Wise/Payoneer의 일부 기능 구현이 어렵다는 이유로 서비스 중단했는데, 왜 아예 각 결제 플랫폼별로 기능을 제한하는 식으로 선별적 도입하지 않는지 궁금
          + 어쩌면 이건 대외적으로 내건 명분에 불과할 수도 있다는 추측
          + 개발자 인건비 때문에 구현이 너무 비싸거나, AI가 몇 분 만에 만들 수 있는 수준이 아니라서일 가능성
     * 최근 BuyMeACoffee에 가입했고, 업계에 도움이 될 무료 작업에 대한 약간의 후원을 받음, 하지만 2주가 지나도 계정 심사 중, 고객 지원 채널도 이메일뿐인 상황, 진입이 어려운 국가에서 사업 중단하는 건 짜증나긴 하나, 그들도 어쩔 수 없을 거라 생각
     * 과거 BuyMeACoffee가 글쓴이를 트위터 계정까지 차단했던 경력, 관련 트윗 링크 공유
          + Jijo Sunny라는 인물이 차 안에서 10분짜리 영상 올리는 이유에 대한 궁금증
     * 친구가 웹 개발 회사의 온라인 설문 조사 작업에 참가해 £20 정도를 벌었으나, .by(벨라루스) 도메인 이메일을 주요 주소로 사용한다는 이유만으로 미국 결제회사가 돈을 보낼 수 없다는 통보를 받음, 당사자는 영국 시민이자 영국 거주자임에도 발생한 일
     * 2024년 중반 시점 기준, 개인적으로 모든 FinTech 기업은 불신, Revolut에서 계좌 해지 및 잔고 환급에 7년이 걸리기도 했고 GDPR 요청 넣고서야 해결, Transferwise는 우크라이나 전쟁 직후 UA 정부군 후원계좌로의 송금 자체를 막았던 사례도 직접 경험, 이런 회사는 절대 신뢰할 수 없고 내일 아침 눈떠보면 계정과 잔고가 사라질 수 있다는 전제로만 이용 추천, 고객 지원은 고객 접점을 멀리 두기 위한 방어막, 초저비용 환전 원하면 Interactive Brokers LLC가 조건부 추천
          + 핀테크 유니콘 기업 몇 군데에서 일해본 결과, 기존 금융사와 별다를 것 없지만 규제 관리·투명성이 훨씬 부족, 투자금 확보 초기에는 지원 채널도 많으나, 자금 사정 나빠지면 다크 패턴 및 “X 하려면 저희에게 연락하세요”로 전환, 기술 감사 역시 준비 부족, 예를 들어 딜로이트도 1990년 이후 기술 못 따라가는 현실, 실무적으로는 굉장히 많은 직원이 모든 정보에 접근 가능하고 거의 모든 작업이 수작업, 고객 정보도 쉽게 유출, 내부 직원의 비정상 행위 탐지 역시 사실상 불가능, AI·데이터사이언스도 실상은 쿼리 몇 번 돌린 뒤 CSV 내보내는 수준, 심지어 AI란 이름 달고도 5인 스타트업보다 자동화 비율 낮은 독일 핀테크도 본 적 있음, 최근 갱단 연루 의혹으로 수백 명이 금지 당한 사건도 내부 프로세스 미숙 탓
          + Transferwise는 우크라이나 정부군 후원계좌 송금 차단한 문제, 하지만 NGO, 개인에는 아직 송금 가능, IBKR은 여러 선택지 중 하나로 추천, 은행이 여권 문제 등 이유로 계좌 막을 경우 크립토의 활용도가 가장 높음
          + Transferwise에서 겪은 가장 충격적인 사건의 자세한 내용이 궁금, 영국의 FCA 및 FSCS로 인해 실제로는 최소한의 보호 체계가 있음
          + 유럽 중앙은행의 보증으로 1계좌당 10만 유로까지는 보호되니, 대다수 사용자는 자산 증발 걱정 불필요, 10만 유로 초과 시엔 더 나은 보험 상품 고려 필요성
          + IBKR로 매우 저렴한 환전이 가능하다는 팁에 감사, 미국에서 캐나다로 송금할 때 BTC를 코인베이스 내에서 사용한 경험, 그때 이런 정보 알았으면 훨씬 절약 가능
     * 업계의 현실을 바라보면 우크라이나에서 유입되는 사기가 워낙 많아 결제 프로세서는 반기지 않는 상황, 당국이 문제를 방치하는 한 해결 기대 난처, 발트 3국 등지에서 우크라이나발 전화 사기가 만연하다 보고되며, Wise가 청구 문제부터 손 놓는 것도 이해
"
"https://news.hada.io/topic?id=21023","Deno의 웹 프레임워크 Fresh 2 업데이트 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Deno의 웹 프레임워크 Fresh 2 업데이트

     * Fresh 2는 아직 정식 릴리스되지 않았지만, Deno 공식 웹사이트와 Deno Deploy에서 이미 운영 중임
     * Fresh 1.0은 최신 웹 기술을 활용한 간단한 웹사이트 구축을 목표로 시작되었고, 빠르게 인기를 얻음
     * 이후 기능 추가 및 복잡해진 사용 요구로 인해 기존 구조의 한계에 직면하고, 보다 강력한 기반을 위해 아키텍처 전면 개편을 진행함
     * 먼저 Fresh 2.0의 핵심 목표 달성을 위해 Deno와 JSR 자체의 안정성 강화가 필요했음
          + Deno 2의 Node 및 npm 호환성 강화
          + JSR로 마이그레이션하면서 의존성 관리가 단순화
     * 현재는 Deno 및 Deno Deploy에서 Fresh 2 알파 버전 사용 중이며, 정식 릴리스는 2025년 3분기 말(9월 예정)
     * Fresh 2의 특징
          + Express/Hono 스타일 API 도입으로 API 사용성이 개선됨
          + 비동기 컴포넌트, 단일 시그니처 미들웨어, 플러그인 기반 아키텍처 등으로 유연성과 확장성 강화
          + JSX precompile 변환이 기본 내장되어 렌더링 속도 향상
     * 현재 알파 버전(2.0.0-alpha.34) 이 공개되어 있으며, Deno 2.3과 함께 사용할 때 최적의 경험을 제공함
     * Deno 팀 내부에서도 Fresh 2는 매우 생산적인 도구로 평가되고 있음
"
"https://news.hada.io/topic?id=20934","스마트워치를 만드는 방법: 칩 선정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          스마트워치를 만드는 방법: 칩 선정

     * Core Time 2용 칩으로 SiFli의 SF32LB52J을 선택함
     * 스마트워치 설계에서 가장 중요한 결정 중 하나가 바로 칩 선정임
     * 소프트웨어 호환성과 전력 소비, 단가 등이 칩 선정에 큰 영향을 미침
     * SiFli 칩은 오픈 소스 SDK와 낮은 전력 소비, 넉넉한 메모리를 제공함
     * 이 시리즈는 앞으로 디스플레이 등 다른 주요 부품 선정 방법도 다룰 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 본 글은 직접 스마트워치를 만드는 방법을 소개하는 연재 글의 첫 번째 편임
     * 목표는 누구나 2025년 즈음에 쓸만한 스마트워치를 조금만 노력하면 만들 수 있음을 보여주는 것임
     * PebbleOS의 오픈 소스화가 더 창의적인 스마트워치 개발을 촉진할 것으로 기대함
     * 스마트워치는 크게 하드웨어, 소프트웨어(펌웨어/운영체제), 모바일 앱 등 세 가지 주요 요소로 구성됨

하드웨어 구성 요소의 분해

     * 스마트워치 하드웨어는 크게 다섯 가지 시스템 요소로 나눌 수 있음
          + 마이크로컨트롤러 칩(대개 Bluetooth 라디오 포함)
          + 디스플레이
          + 센서류와 출력장치(버튼, 터치, 마이크, 가속도계, 스피커 등)
          + 기타 전자 부품(칩, 수동소자, PCB, 배터리 등)
          + 기계적 구조(시계 케이스, 글라스, 버튼, 스트랩, 충전 케이블 등)
     * 센서, 배터리, 스트랩, 마이크 등은 다양한 가격대에서 선택 대상이 많아 현재는 쉽게 고를 수 있음
     * 가장 어려운 의사결정은 마이크로컨트롤러+Bluetooth 라디오, 그리고 디스플레이임

칩(마이크로컨트롤러) 선정의 중요성

     * MCU는 스마트워치의 '심장' 역할을 하며 CPU, RAM, 플래시 저장공간, I/O, 라디오 등을 한 칩에 통합함
     * 적합한 MCU를 고르는 일은 비용, 전력, 소프트웨어 호환성 등 여러 제약 조건의 균형점임
     * MCU마다 SDK, 드라이버, 빌드 시스템이 달라 소프트웨어 개발 난이도가 높아짐
     * PebbleOS는 특정 회사(STM)의 MCU에만 맞춰졌기 때문에 다른 MCU 전환 시 드라이버와 빌드 시스템 변경 필요함
     * 특정 MCU는 PebbleOS에 필요한 FreeRTOS 지원이 어려움
     * 대량 생산이 아니기 때문에 소프트웨어 개발비가 제품 단가에 큰 비중을 차지하게 됨

기존 사용 칩과 선정 과정

     * 과거 Pebble 시계는 STM32F2를 활용했으며, 초기에는 릴레이션과 주변 조언을 토대로 칩을 선정하는 경우가 많았음
     * 최근에는 Nordic nRF52840 칩을 사용해 Core 2 Duo 스마트워치 프로토타입을 제작함
          + 오픈 소스 BLE 스택(nimBLE)로 전환하여 개발 진행
     * 더 큰 메모리와 성능이 요구된 Core Time 2에는 nRF52840이 한계임
          + Nordic의 최신 저가 칩(nRF54L15)은 RAM이 부족하고, 고급 칩(nRF54H)은 가격이 비쌈
          + Core Time 2의 컬러 디스플레이 구동을 위한 특수 인터페이스도 필요함

SiFli 칩의 발견과 선정

     * 여러 후보(Apollo, BES, Dialog 등)를 검토했으나, 오픈 소스 SDK 부재 등으로 만족하지 못함
     * BES 등 일부 칩은 NDA, 예제 코드 미공개 등으로 개발 환경이 부적합했음
     * 우연히 SiFli CEO의 메일을 받고 소통한 후, 오픈 소스 생태계에 적극적임을 확인함
     * SiFli 칩은 이미 수천만 개 스마트워치(브랜드: Redmi, Oppo, Noise 등)에 적용되고 있음
          + 최소형 모델(SF32LB52x)은 512K 이상의 SRAM, 16M PSRAM, 전용 MIP 디스플레이 제어기 탑재
          + 낮은 소비 전력(BLE 연결시 ~50uA), 가격은 2달러 미만
          + 1~2MB SRAM 추가 옵션 제공
          + Github에 오픈 소스 SDK 제공, PebbleOS 포팅 지원 약속
     * 결과적으로 Core Time 2 칩은 SF32LB52J(1.8V 버전) 으로 선정함

결론 및 이후 방향

     * Core Time 2는 SiFli SF32LB52J 칩을 사용해 개발 예정임
     * 다음 글에서는 디스플레이 선정 과정에 대해 다룰 예정임

참고 링크

     * PebbleA2의 시계 사양 비교 테이블
     * SiFli SF32LB52x 제품 설명서
     * SiFli GitHub 오픈 소스 SDK

        Hacker News 의견

     * PebbleOS만이 이 영역의 유일한 플레이어가 아닌 점을 강조함, Espruino라는 마이크로컨트롤러용 작은 Javascript 구현체도 소개함, bangle.js와 Fallout Pip boy 등에서 사용됨, 실시간으로 장치를 해킹하기 쉬움, Espruino 관련해서는 Github discussion 링크도 같이 공유함
          + bangle.js를 처음 들었으며, 이미 알고 있는 언어로 마음껏 만질 수 있다는 점이 마음에 듦, 1세대 버전을 사고 싶었으나 현재는 판매하지 않는 점이 아쉬움
          + 주목도 낮은 경쟁자들의 훌륭한 작업도 언급해줘서 고마움, Pebble도 훌륭하지만 그들도 관심 받을 자격 있음, 그리고 Amazon Fallout 쇼가 꽤 재미있으니 볼 기회 있으면 추천함
          + 참고로 PineTime도 있음
     * ""SDK가 오픈 소스""라는 말에 주목함, BLE MCU에 오픈 소스 코드 SDK가 함께 있다는 말에 관심이 생겼음, 그러나 실제로는 BLE 코드는 바이너리 블롭 형태, 오픈 소스로 설명했다면 전체 소스코드를 읽을 수 있어야 한다고 생각함
          + BLE 라디오 기기 펌웨어는 항상 IP와 규제상의 이유 때문에 바이너리 블롭 형태로 제공됨
     * 이 스마트워치는 오늘날 기준으로 그리 스마트하지 않음, 내가 현재 사용하는 스마트워치에 있는 NFC 결제, 듀얼 밴드 GPS, 4G LTE 연결 같은 편의 기능이 상당히 그리울 것, Pebble(및 repebble)은 멀티 주간 배터리 수명과 바꿔치기함, 그러나 갤럭시 워치의 이틀에 한 번 충전하는 불편함이 모든 이 강력한 기능을 포기할 만큼 크지 않음
          + 반대로, 나는 그 모든 기능에 관심이 없고 긴 배터리 수명이 중요함, 다양한 기기가 다양한 사용자를 대상으로 존재하는 건 좋은 일, 이 스마트워치가 맞지 않는다고 해서 “스마트하지 않다”는 건 아니고 그저 사용자에 따라 맞지 않을 뿐
          + Garmin은 이미 대부분의 이런 기능과 멀티 주간 배터리 수명을 제공함, 4G만 아직 없을 뿐이며 내년쯤 기기에는 탑재된다는 소식도 들었음
          + 나 역시 GPS가 없으면 아쉽지만, Pebble이 주던 만족감을 채워주는 기기를 아직 못 찾았음, Garmin은 다른 부분에서 타협이 생김(배터리, 위젯, 달력, 디스플레이 가독성 등), 모든 것은 트레이드오프
          + 나에게는 그 모든 기능이 불필요, 알림 받고 간단한 심박수 측정만 되는, 약간 커스터마이즈 가능한 시계면 만족, 일주일에 한 번 이상 충전해야 한다면 착용할 생각이 없음
          + 배터리 수명 하나만 봐도 상당히 뛰어남
     * 해당 칩에 대한 더 많은 정보를 cnx-software 기사 링크로 공유함
     * 주 칩과 BLE용 칩을 따로 두지 않고 단일 칩 디자인을 선택한 점이 흥미로움, 고성능 MCU는 보통 RF가 없어서 2칩 설계가 종종 더 적합한 경우도 있음
          + 최신 고급형 MCU에서도 NRF나 ESP32처럼 블루투스와 와이파이 모두 함께 쓸 수 있는 칩들이 있음, 요즘 같아서는 개인적으로는 ESP32를 선호함, 지속적인 개선과 좋은 커뮤니티 지원이 매력, 나 역시 micropython 기반 스마트워치 플랫폼을 개발 중
          + SiFli 칩의 Cortex-M33 코어는 기존 Pebble에서 사용하던 M4 코어보다 훨씬 빠름, 더 빠른 MCU까지는 필요하지 않음, 오히려 배터리 수명이 더 중요한데 이번에 블루투스가 내장된 MCU 사용으로 약 1주에서 약 1개월로 수명 확장이 큰 업그레이드
          + 그냥 시계라면 굳이 UNIX 컴퓨터까지 필요하지 않음, 시간 표시, 심박 측정, AWS와의 통신 등은 간단한 MCU로 충분
          + 칩 수가 늘어나면 프로젝트가 복잡해짐, 각 칩마다 패시브, 오실레이터가 필요, 칩끼리 통신 조정, 펌웨어 업데이트, 디버깅 방법 등도 고민해야 해서, 그럴 바엔 차라리 배터리 수명을 조금 포기하는 게 나을 수도 있음
     * 저전력 칩 업계에서 오픈 소스를 지향하는 제품이 드디어 나와서 반가움, rePebble 발표 보자마자 바로 등록했었음, 그런데 사실 나는 스마트워치가 아니라 진동 알림만 있는 “멍청한” 시계를 원했다는 걸 나중에야 알게 됐음, 이런 수요는 소수이지만 아주 관심 있는 사람들이 있는 틈새 영역, 지난 2년간 Casio F105를 착용하다 보니 이보다 크거나 무겁거나 두꺼운 건 못 참겠음, 블루투스 기능만 있으면 주 1회 충전도 감수할 의향, 그래서 요즘은 아이폰의 모든 알림을 받을 수 있는 아주 작은 블루투스 칩과 진동 모터, 소형 리튬 배터리를 시계 스트랩 사이에 붙여두려고 계획 중, Mi Band 1을 처음 썼을 때를 아직도 기억함, 디스플레이도 없이 RGB LED 3개로 앱별로 알림 색이 달랐음, 바로 어떤 메시지가 왔는지 손쉽게 알 수 있었음
          + Citizen에서 만든 W770이라는 제품을 언급함, 최근에 중고로 200달러 미만에 구입했는데 크로노그래프, BLE, 알람, 진동 모터가 조합된 꽤 괜찮은 시계, 디스플레이 대신 시곗바늘로 대부분의 정보를 표시해서 조작법을 숙지하려면 설명서를 꼭 읽어야 함, 슈퍼커패시터 기반의 태양광 충전으로 완전히 어두운 곳에서도 몇 달씩 배터리 걱정 없음, 슈퍼커패시터는 15~20년 정도 쓰면 교체 가능한 부품, 아무것도 요구하지 않고 그냥 본연의 역할만 꾸준히 수행함, Citizen에서 BLE 기능이 있는 다른 모델도 있을 수 있음, 누군가 알림/블루투스 프로토콜을 리버스엔지니어링해서 오픈소스 앱을 만들어주면 해커들에게 재미있는 가능성
          + 여전히 OG Pebble을 쓰고 있고, 다시 돌아온다니 기대되지만 업그레이드 계획은 없음, 진동 알람과 알림 기능만 있으면 충분, 단 꼭 밝은 곳에서도 잘 보이고 노안에도 친화적인 디스플레이가 필요, OG Pebble의 eInk 디스플레이가 그 역할을 충분히 함, 스마트폰을 꺼내지 않아도 문자 확인 가능, 중고로 30달러, 새 배터리도 15달러면 구할 수 있어 가성비가 좋음, Rebble.io 커뮤니티도 충분히 잘 돌아가고 있음
          + 나 역시 “스마트” 시곗줄이나 버클을 빈티지 세이코 시계에 달 수 있기를 바람, 아주 특정한 상황에만 진동 알림만 오면 충분, 모든 기능이나 디스플레이는 오히려 방해됨
          + Apple Watch의 UX가 불편함(1cm 손가락으로 2cm 화면 터치...), 지나치게 많은 걸 하려는 점도 마음에 안 듦, Pebble은 전화나 문자 알림만 받아도 충분해서 내 폰에서 멀리 떨어질 수 있음, Pebble은 아주 가벼움, 특정 워치 페이스를 올리면 집에 돌아온 듯한 느낌
          + 나도 그저 “멍청한” 시계에 진동 알림 기능만 원함, 대다수 알림은 거의 시간에 쫓기는 게 아니고 불필요하게 집중력을 뺏을 뿐, 결국 모든 알림을 끄고 몇 시간에 한 번만 직접 폰을 확인하게 됨, 오히려 정말 필요한 건 알람을 진동으로 알려주는 기능, “버스 시간 임박” 또는 “휴식 시간” 이런 용도로만 충분, 불필요한 센서는 필요 없음
          + Withings ScanWatch도 이런 용도에 잘 맞는 제품, 배터리 수명이 거의 한 달
          + 시계가 아닌 피트니스 트래커를 찾는 내 희망도 상당히 드문 수요, 반지는 싫어하고 팔찌 형태나 이상적으로는 발목 밴드였으면 좋겠음, 다만 심박 측정이나 추가 데이터 수집이 어려울 듯
          + Mi Band 같은 기기들이 그 틈새를 잘 채워줌, 작고 가벼워서 거슬리지도 않으면서 디스플레이와 괜찮은 배터리 수명을 제공함
     * 저가 중국 Freqchip SOC를 해킹하기 시작함, Ali에서 이 칩으로 만든 스마트워치를 3유로에 구할 수 있음
          + 해당 칩이 들어간 시계 제품 링크를 요청함, 검색 몇 번 해도 찾지 못했다는 언급
          + 일부 칩이 3천 개 MOQ에 2.6위안($0.36)짜리 Arm Cortex m3와 BLE, sig-mesh 지원까지 되어 놀라움, SDK 사용성에 대해 궁금증을 가짐
     * PebbleOS로 구동되는 어느 정도 오픈 소스 하드웨어를 보게 되어 반가움, 2025년에는 좌측정렬 블로그 포맷이 그만 사라졌으면 하는 바람, 와이드 스크린 보급으로 굳이 읽기 어렵게 만드는 점이 아쉬움
     * “가장 흥미롭고 어려운 제약조건은 소프트웨어 호환성”이라는 점에 대해, 오히려 이 부분이 리스트 제일 마지막에 위치해야 할 만큼 해결이 쉬운 일이라고 생각함
          + 이 제품에 한해 소프트웨어 호환성은 두 가지 큰 이유로 매우 중요, 첫째, 현재 팀 규모가 옛 Pebble 시절에 비해 매우 작아 소프트웨어 작업을 줄이는 게 최우선, 둘째, 모든 앱과 워치페이스가 ARM 바이너리로 배포돼 있으므로 MCU 아키텍처가 다르면 이전 호환성이 사라짐, ESP32도 여기에 해당됨
          + 모든 게 프로프라이어터리라면 많은 교착에 부딪힐 수 있음, 표준 PC 하드웨어와는 많이 다름
     * 이 글을 아주 흥미롭게 읽었음, nimBLE은 훌륭한 블루투스 스택이고 Core Devices 향후 발전 가능성을 여는 점이 기대됨
"
"https://news.hada.io/topic?id=20968","Sci-Net","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Sci-Net

     * Sci-Net은 사용자가 논문을 요청하고 업로드할 수 있도록 지원하는 새로운 플랫폼임
     * 기존의 자동화된 Sci-Hub 방식과 달리, 사용자 참여와 분산형 토큰 보상을 통해 논문 공유를 촉진함
     * 플랫폼 내에서 요청된 논문이 업로드되면 모든 사용자가 무료로 접근 가능함
     * Sci-Hub 토큰을 활용한 보상 시스템은 실제 지불이 업로더에게 직접 이루어짐
     * Sci-Net의 참여는 전체 퍼블릭 도메인 학술 자료 확장에 기여함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Sci-Net 플랫폼 소개

  Sci-Hub와 사용자 요청의 한계

     * 대부분의 Sci-Hub 사용자는 특정 논문에 접근 불가 시 다운로드를 요청하는 경험 있음
     * 최근 2년간 Sci-Hub 데이터베이스 업데이트 중단 이후 이런 요청이 급증함
     * 일부 사용자는 자신이 구입하거나, 대학 구독을 통해 내려받은 논문을 Sci-Hub에 업로드하고 싶다는 문의도 종종 들어옴
     * Sci-Hub는 사용자 업로드를 지원하지 않으며, 설계 초창기부터 인간 개입 없는 자동 논문 크롤링 시스템으로 구현됨
     * 이 방식으로 전체 학술 논문의 90% 이상을 확보한 적도 있지만, 여전히 접근 불가능한 논문이 존재함

  Sci-Net의 등장

     * 이제는 사용자가 직접 논문을 요청하고, 업로드도 할 수 있는 플랫폼인 Sci-Net이 출시됨
     * 간단한 UI를 가지고 있어, 사용자는 DOI 입력창에 요청하고자 하는 논문 정보를 입력하면 됨
     * 요청 버튼 클릭 시, Sci-Net이 해당 논문의 오픈 액세스 여부 또는 Sci-Hub 내 존재 여부를 자동으로 확인함
     * 논문 존재 시 바로 링크 제공, 없으면 새 요청 생성으로 이어짐

  요청 및 업로드 프로세스

     * 메인 페이지에는 요청 목록이 과목별, 출판사별 필터링 기능과 함께 제공됨
     * 접근 가능한 논문 발견 시 해당 요청을 클릭하고 PDF를 업로드할 수 있음
     * 플랫폼은 PDF 내 워터마크 제거 기능을 통해 논문을 배포한 대학 정보 등 업로더 익명성을 보호함

  라이브러리 및 업로드 기능

     * 'Library' 섹션에서는 본인의 모든 요청 및 업로드 목록 확인 가능
     * 'Upload' 섹션에서는 PDF 파일 드래그 앤 드롭만으로 논문 기여 가능
     * DOI가 시스템에 등록되어 있지 않으면 파일이 업로드되고, 이후 모든 사용자가 sci-net.xyz/DOI URL을 통해 접근 가능
     * 플랫폼 등록 없이도 논문 열람이 가능하여 지식 공유의 개방성이 강화됨

  분산형 토큰 시스템

     * Sci-Net의 차별점은 탈중앙화 토큰(Sci-Hub meme coin) 을 이용한 지식 공유 보상 방식임
     * 논문 요청 시 업로더에게 지급할 토큰 보상 금액을 설정할 수 있음
     * 업로더가 PDF를 업로드해 요청자가 확인 후 'Accept' 버튼을 클릭하면 토큰이 실제 업로더에게 이체됨
     * 회원 가입 시 최소 1000 Sci-Hub 토큰이 필요하며, 이는 가입 즉시 계정에 지급되고 업로더 보상에 사용됨

  Sci-Net과 기존 출판사 모델의 차이

     * Sci-Net의 참여 진입 비용은 상징적 의미에 그치며, 대부분의 국가에서 커피 한 잔 가격과 비슷한 수준임
     * 전통적 출판사의 수익은 연구자와 상관없이 배분되지만, Sci-Net에서는 토큰이 실제 업로더에게 전달되어 동료 연구자에 직접 기여함
     * 출판사는 논문에 매번 접속할 때마다 요금이 발생하지만, Sci-Net에서는 한 번만 업로드 비용이 발생하고 이후 모두 무제한으로 무료 제공됨
     * Sci-Net을 통한 모든 거래가 공개 지식의 영역 확대에 직접적으로 기여함

  Sci-Hub 토큰과 플랫폼의 미래

     * 사용자가 많아질수록 Sci-Hub 토큰의 가치 상승이 기대됨
     * Sci-Net 참여는 향후 Sci-Hub의 유지와 발전에도 간접 기여 효과를 가짐
     * 단점으로 Solana 네트워크 기반 Sci-Hub 토큰 확보가 암호화폐 초보자에겐 다소 복잡한 편임

  결론

     * Sci-Net은 오늘날 연구자가 누구나 꼭 참여해야 할 지식공유 플랫폼임
     * 참석자들이 함께 노력함으로써 오픈 지식 실현에 가까워지는 결과를 만들어냄

   지불되는 돈은 어디서 나오는건가요?
   그냥 다른 코인들처럼 누군가가 사야 가치가 생기는건가요?

        Hacker News 의견

     * 왜 암호화폐가 모든 좋은 것에 침투하는지 의문점 발생, 이런 분야에서 정보 공유를 장려하려는 과학자가 충분하다면 최소한의 보상 없이도 실험해 볼 가치가 있다는 생각, 인센티브가 꼭 필요한지 확신 없음, 암호화폐가 상업적 행위가 되면서 업로더에 대한 처벌이 더 심해질 수 있을지 걱정
          + 이 분야야말로 암호화폐가 명확한 용도를 가지는 대표적인 사례라는 생각, Alexandra Elbakyan은 여러 나라에서 범법자이지만 많은 이들에게는 영웅, 그녀가 지금 하는 일을 계속하려면 누군가는 금전적 보상을 할 필요, 비트코인의 핵심 목적이 허가 없이 돈을 주고받는다는 점, 이럴 때 쓰기에 적합한 도구라는 판단
          + 보상 없이 이미 시도된 적 있음, Nexus와 같은 곳에서 요청을 올리고 누군가가 응답하는 채널 존재, 그런데 끝없는 작업이므로 본인의 시간 활용 효율성이 느껴지지 않음, 암호화폐 토큰 방식이 크게 마음에 들진 않지만 새로운 시도가 나오는 건 환영, 최악의 경우 실패해도 논문만 공개될 뿐이라 크게 두려울 필요 없음
          + 정보 공유를 장려하려는 과학자가 충분한지에 대한 의문, 예전에 나도 논문을 출판했는데 현재의 학술 출판 시스템의 망가진 인센티브로 인해 내 연구물을 합법적으로 배포할 수 없는 상황, 나와 비슷하게 대부분(혹은 전직) 연구자들 역시 이 기형적 시스템을 싫어함, 논문 공유를 독려할 인센티브가 따로 필요하지 않음, 접근권이 있는 대부분은 이를 공유할 준비가 충분하다는 개인적 경험
          + 솔직히 이런 시도 마음에 듦, 암호화폐 특유의 거북함은 있지만 현행 연구논문 체계가 너무 엉망이어서 뭔가라도 개선된다면 매우 환영, 하지만 업로더에 대한 처벌 관련 우려에는 동의, 차라리 토큰을 sci-net에 기부하는 선택지도 있으면 좋겠다는 희망
          + 암호화폐 도입으로 업로더의 처벌이 더 심해질지 묻는 부분에 대한 의견, 실제로 프라이버시 중심의 암호화폐를 사용하는지도 불확실, 사용자가 지적재산권을 침해하는 대가로금전적 보상을 받고 주고받는 구조가 되고, 이 과정이 오히려 신원 추적과 연결이 쉬워질 위험, 미국 이용자는 소액이라도 세금 신고가 필요한 복잡성도 발생, '커피 한 잔 값'으로 치부할 문제가 아님, 사용자 신원 보호와 워터마크 제거 시스템을 도입한다고 하지만 실제로 많은 경우 제대로 작동하지 않을 것, 연구자와 출판사 사이의 논문 접근 권한 다툼이 냉전 관계처럼 느껴짐, 학계 내에서 허용되는 선을 넘지 않아야 암묵적 합의가 유지되는데 이런 시스템은 위험선을 넘을 수 있음, 대학 입장에서 연구자가 개인 홈페이지에 논문을 올려두면 단순 교체 요구 수준이나,
            조직적으로 대가를 받고 위법을 지속한다면 훨씬 심각한 결과 발생 가능성
     * 이번 시도는 재앙으로 끝날 전망, 자체 암호화폐를 만들기보다 기존 토큰을 활용하는 것이 나았을 것, 일반적으로 이런 ‘밈 토큰’은 토크노믹스가 구조적으로 제작자에게만 유리하며, 익명성이 없어 사용자 법적 위험만 커짐, Solana 네트워크에서 Sci-Hub 토큰을 구하는 과정도 초보자에겐 꽤 난해, 그냥 ‘재미’를 위한 과정이라 주장하지만 실상은 위험과 혼란만 초래
          + 제작자에게 혜택을 주는 토크노믹스가 단점이라는 주장에 대해, 오히려 그것이 시스템의 명시적 목적이자 Sci-Hub 운영자금을 마련하는 장점일 수 있다는 관점
          + 자체 암호화폐를 직접 사용하는 것이 프로젝트 자체 호스팅에 유리할 수 있지만, Solana 기반이라면 그 효과는 인정하지 않는다는 의견
     * Sci-Net에서 토큰이 업로더에게 직접 지급된다는 설명, 실제로는 학자 대신 무작위 대학생들이 토큰을 벌기 위해 업로드하게 되므로 여전히 자금이 학자보다 업로더에게 흐른다는 오해, 내가 잘못 이해한 것인지 궁금함
          + 맞게 이해한 것 같음, 단 연구자도 자신의 논문을 업로드할 수 있음, 업로드가 요청자와의 페이-투-리퀘스터 방식이라 연구자가 요청을 알아차릴 필요 조건
          + 핵심은 연구자에게 돈을 주는 게 아니라 업로더를 격려하기 위한 포인트(카르마)로 보상하는 것, 실질적으로는 sci-hub 인프라 운영자금 조달 목적
     * Sci-hub 대신 사용할 이유가 점점 약화됨, 업데이트 부족, 창립자가 곤충 이름 논란 같은 사소한 일로 불안정한 점 등 현시점에선 Anna's Archive가 더 낫다고 생각
          + ‘Anna's Archive’를 몰랐기에 우선 Sci-hub를 사용했는데, 앞으로는 Anna's Archive를 써볼 생각
          + Anna's Archive가 만약 중단된다면 Sci-hub가 존재하는 것만으로 많은 사람들이 안심할 것, 물론 서로 상호보완되어야 한다는 점도 중요
          + 더 나은 대안을 알지 못했다는 솔직한 사용 경험
          + Anna’s Archive는 Sci-hub의 기존 논문(수년 전까지)을 호스팅하는 미러 사이트라는 정보, Sci-hub가 없었으면 Anna’s Archive 역시 없었을 것
     * 업로드된 논문이 Sci-hub로 다시 흡수되는지 궁금, 두 저장소가 왜 분리되는지 이해가 잘 안 됨
          + 이해한 바로는 업로드 논문이 결국 Sci-hub의 일부가 된다는 해석, 정확하진 않음
     * Sci-Hub 토큰을 Solana 네트워크에서 구하는 게 초보자에겐 너무 어려운 과정, 그 때문에 대다수 사용자가 접근을 포기하게 만들 것, 반론으로 무료라 괜찮다는 의견이 있지만, Sci-hub.se의 주요 강점이 진입장벽 없는 간편함·빠른 사용 경험에 있었다는 점에서 본질이 훼손됨, 누구나 논문을 손쉽게 구하던 목적에 역행함
          + Sci-hub 입장에서는 오히려 이점이 있을 수도 있음, 자체 리소스를 소모하는 각종 논문 요청이 늘어나는 시기에, 이런 마켓플레이스로 유입을 분산할 수 있음, 기존처럼 자동 스크래핑이 가능한 논문은 그대로 제공되고, 개인 업로더 노력이 필요한 경우만 새 시스템이 가동, 사용자가 코인 세팅이 싫거나 어렵다면 논문을 그만큼 절실히 원하지 않는다고 해석할 수도 있음, 그래도 논문이 절실하다면 정식 구매도 가능
          + 방금 논문 하나 구하려다 Sci-Hub 방문했는데 암호화폐 시스템에 부딪혀 매우 혼란스러웠다는 반응, ‘재밌는 과정’이 아니라 혼동만 야기
     * 초대 코드 발급에 토큰 결제가 필요한데, 현재 구현 방식이 불편함, QR 코드를 모바일 지갑용으로만 생성해 웹브라우저 지갑 이용자를 배려하지 않음, web3 환경에서는 브라우저 기반 지갑이 더 흔하기에 개선 필요
     * 이 코인 시스템은 Sci-hub에 없는 특정 논문을 요청·업로드하는 일부 사용자를 위한 메커니즘으로 보임, 이 시스템에 참여하고 싶지 않으면 기존과 달라지는 게 없다는 해석, 개인적으로는 이 방식이 바보 같다는 생각
     * 요즘은 Nexus와 Telegram 봇을 더욱 빈번히 이용, sci net에 속지 말라는 당부
          + Nexus 봇이나 Lib STC(같은 프로젝트라 판단)에서 내가 원하는 자료를 얻어본 적 없음, 최소 다섯 번 이상 시도했지만 항상 실패, 최근엔 Anna's Archive가 가장 신뢰가는 선택
     * 암호화폐와 관련이 있으면서도 암호화폐 생태계 바깥에서도 실질적으로 잘되는 성공 사례가 궁금, ‘crypto’라는 단어만 봐도 프로젝트가 망할 것 같다는 편견이 있는데 실제로 그런지 확인해보고 싶음
          + Numerai를 예시로 드는데 최근 토큰 가치가 많이 하락한 상태, 흥미로운 사례지만 전망은 확실하지 않음
          + 대부분의 암호화폐 프로젝트가 실패하는 게 특별할 건 없다는 의견, 산에 사는 사람도 결국 다 죽는 것과 같다는 비유, 원래 대부분 프로젝트가 실패하는 게 일반적
          + 실질적으로는 스테이블 코인(투기성 없는 토큰)만이 성공적이었다고 판단
          + Nano-gpt를 새로운 세대의 사기 없는 암호화폐의 훌륭한 활용 사례로 꼽음, 개인적으로 대단히 유용하게 쓰고 있다는 추천
"
"https://news.hada.io/topic?id=20956","아주 작은 Boltzmann 머신","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           아주 작은 Boltzmann 머신

     * Boltzmann 머신의 구조와 목적에 대한 간략한 소개임
     * 에너지 함수와 확률 분포를 수식으로 정의함
     * 가중치와 바이어스의 업데이트 규칙을 미분을 통해 유도함
     * 긍정·부정 단계와 Gibbs 샘플링을 통한 모델 기댓값 근사 방법 설명임
     * 최종적으로 대비 발산(Contrastive Divergence) 알고리듬을 전체적으로 정리함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Boltzmann 머신과 Contrastive Divergence 개념

     * Boltzmann 머신에서는 입력층(visible layer)과 숨겨진층(hidden layer), 그리고 이를 연결하는 가중치 행렬과 두 층 각각의 바이어스 벡터를 가짐

에너지 함수와 확률분포

     * 에너지 함수는 행렬 형태로 다음과 같이 정의됨 E(v, h) = -ΣiΣj wij vi hj - Σi bi vi - Σj cj hj
          + v: 가시층 벡터, h: 숨겨진층 벡터, w: 가중치, b/c: 각 층 바이어스
     * Boltzmann 머신의 결합 분포는 P(v, h) = (1/Z) * exp(-E(v, h))
          + Z(분할 함수)는 확률분포를 정규화하는 역할임

로그 우도(log-likelihood) 및 미분

     * 훈련 데이터의 우도를 최대화하여 학습 진행함 log(P(v)) = log(Σh exp(-E(v, h))) - log(Z)
     * 가중치 wij에 대한 로그 우도의 편미분은 ∂(log P(v))/∂wij = <vi hj>데이터 - <vi hj>모델
          + < · >데이터: 실제 데이터에 대한 기댓값
          + < · >모델: 모델이 생성한 데이터에 대한 기댓값

가중치와 바이어스 학습 규칙

     * 가중치와 바이어스는 다음과 같이 갱신함
          + Δwij = η(<vi hj>데이터 - <vi hj>모델)
          + Δbi = η(<vi>데이터 - <vi>모델)
          + Δcj = η(<hj>데이터 - <hj>모델)
          + η는 학습률

Contrastive Divergence 알고리듬

     * 모델 기댓값 < · >모델은 직접 계산이 어려우므로 Gibbs 샘플링을 사용함
     * Contrastive Divergence는 다음의 절차로 근사함
         1. 긍정 단계: 숨겨진층 h(0)을 P(h | v(0)=데이터)로부터 샘플링함
         2. 부정 단계: k번의 Gibbs 샘플링 반복
          + 번갈아가며 v(t+1) ~ P(v | h(t)), h(t+1) ~ P(h | v(t))으로 샘플링
     * 업데이트 시각에서 데이터 기댓값과 모델 기댓값의 차이를 사용함
          + Δwij = η(<vi hj>데이터 - <vi hj>모델)
          + Δbi = η(<vi>데이터 - <vi>모델)
          + Δcj = η(<hj>데이터 - <hj>모델)

요약

     * Boltzmann 머신의 학습 본질은 에너지 기반 모델로서 실제 데이터와 모델이 생성한 분포 간 기댓값 차이를 줄이기 위함임
     * Contrastive Divergence는 이 차이의 근사를 빠르고 효율적으로 가능하게 하는 핵심 훈련법임
     * Gibbs 샘플링을 통해 모델 분포와 실제 데이터를 연결하는 역할을 하며, 이 과정을 반복하여 Boltzmann 머신이 데이터를 잘 표현할 수 있도록 가중치와 바이어스를 업데이트함

        Hacker News 의견

     * 내 이해로는 Harmonium(Smolensky)이 최초의 restricted Boltzmann machine이며, “energy”를 최소화하는 대신 “harmony”를 최대화했던 개념임. Smolensky, Hinton, Rummelhart가 협업할 때는 이를 “goodness of fit”이라고 부름. Harmonium 논문은 정말 인상적인 읽을거리임. Hinton은 인공지능계 슈퍼스타가 되었고, Smolensky는 언어학 관련 긴 책을 집필함. 혹시 이 역사에 대해 더 아는 분이 있는지 궁금함
     * David Ackley에 대한 흥미로운 기사 소개. 또한 T2 Tile Project도 확인해 볼 만함
          + 이런 중요한 발전에는 정말 많은 사람들이 참여한다는 점이 핵심임. 대학원생들이 정말 많은 기여를 하는데, 그들이 한 연구가 나중에 더 발전함. 왜 미국에서는 연구를 낭비라고 생각하는지 모르겠음, 연구가 모든 것을 크게 진전시켰는데도 불구임
     * 글쓴이임. 많은 댓글에 감사함, 이렇게 인기 있을 줄은 몰랐음. 오타, 여백, 스크롤 문제 등 수정 중, 제보해줘서 고마움
          + 오타 수정 완료, 이제 모바일에서 훨씬 보기 좋아짐
     * 제목을 ""A Tiny Boltzmann Brain""으로 착각함. 내 자연스러운 두뇌는 즉각적으로 이 혼란을 해결함. 아주 작은 모델에 무작위로 가중치를 부여해서 의미 있는 무언가를 할 수 있는지 테스트하는 실험일 거라고 추측함. 모델이 작을수록 무작위 생성에서 흥미로운 게 나올 확률이 상대적으로 높다는 생각임. 내 추측이 틀렸지만, 여전히 용기를 잃지 않음. “Unbiased-Architecture Instant Boltzmann Model” (UA-IBM)이라는 새로운 모델 계열을 제안함. 언젠가 충분히 큰 양자 컴퓨터가 생기면 데이터셋 전체를 모델의 모든 파라미터와 아키텍처를 양자 상태로 중첩시켜 한 번에 추론할 수 있을 것이라는 상상임. 혹시 이런 실험을 해볼 남는 qubit 있는 사람 있는지? (참고로, 모든 게 양자적이지만 아직은 실제로 제대로 활용하기 어려운 현실이라는 점이 아이러니임. 그리고 외계
       문명이 단일 양자 센서부터 발전해 전체 양자 신경계를 가진 존재가 된다면 어떤 사회와 기술적 경로를 가질지 상상해봄)
          + 불쌍한 양자 생명체들. 자기 사고 속도보다 빠른 연산 모델에 접근할 수 없어서 항상 오랜 시간 계산을 기다리는 운명이 됨
          + 양자 컴퓨터가 그렇게 작동하지 않는다는 점을 지적함
     * 설명이 아주 좋음. 참고로 마우스 스크롤이 지나치게 민감함(아마 모바일에서는 괜찮을 것으로 예상). 스크롤할 때마다 첫 페이지와 마지막 페이지로 튀어서 불편했음. 다행히 키보드 입력으로는 정상적으로 전체를 읽을 수 있었음
     * 내 이해가 맞다면, 오늘날 사용하는 신경망과 달리 weight update를 위해 gradient 기반의 forward/backward pass 대신 gibbs sampling이 필요함. 왜 그런지 아는 분이 있는지 궁금함
          + 내 생각에도 gibbs sampling은 모델 분포에 대한 기대값을 근사하기 위해 사용함. log likelihood의 gradient를 계산하려면 분포의 적분이 필요한데, 직접 계산은 불가능함. VAE에서 MCMC로 대표 샘플을 뽑는 것과 비슷함. 딥러닝에서는 데이터셋 배치를 통해 gradient를 추정하지만, RBM에서는 명시적으로 모델링한 확률분포의 기대값이 필요함
          + 전문가 아니지만 Bayesian 분야의 공식 교육을 약간 받아봄. gibbs는 gradient가 명확하지 않거나 분포 자체를 복원하고싶을 때 자주 사용함. 각 visible 노드가 hidden 노드에 의존하고, hidden 노드도 visible 노드에 영향을 주기 때문에 gradient가 매우 복잡해짐. 그래서 marginal likelihood를 기반으로 gibbs 샘플링을 사용하는 것이 훨씬 단순함
          + 내가 틀릴 수도 있지만, RBM의 undirected 구조 때문이라고 생각함. feed-forward network와 다르게 computational graph를 만들 수 없음
     * 이 글을 보니 예전 생각이 남. 1990년에 void pointer로 신경망 노드를 배열로 만든 뒤 C로 Boltzmann machine과 perceptron을 구현함. 그 당시 “AI”의 용도는 MIDI 멜로디에서 다음 음을 예측하거나, 5x9 점 영역에서 minim, crotchet, quaver 같은 음표 모양을 인식하는 것 등임. 85% 인식률이면 충분히 “좋음”으로 여겼음
          + 점이 5x9인 점 영역에서 음표 모양을 인식한다는 게 재미있음. 3Blue1Brown neural network 예제와 비슷하게 완전히 처음부터 직접 구현하는 느낌임. Chuck 같은 것을 조합하면 오늘날엔 브라우저에서 클라이언트 사이드로 이런 프로그램도 만들 수 있음
          + 결과물의 소리가 음악적으로 들렸는지도 궁금함
     * 글이 쉽고 명확했음. 옛 추억이 많이 떠오름. 부끄러운 홍보지만, RBM 학습 과정을 시각화한 영상을 예전에 만든 적 있음
     * 정말 깔끔한 데모임. 예전에 Geoff Hinton의 신경망 강의에서 Boltzmann machine에 대해 여러 강의 듣던 기억이 남. 한 가지 지적할 점은, “restricted Boltzmann machine은 visible과 hidden 뉴런이 서로 연결되어 있지 않다”라는 식의 설명이 잘못된 표현임. 마치 visible 노드와 hidden 노드가 서로 연결되어 있지 않은 것처럼 보일 수 있기 때문임. 정확히는 같은 유형 내에서, 즉 visible끼리 혹은 hidden끼리만 연결이 없다는 것임. 또는 visible과 hidden 노드는 자신과 같은 타입 내에서는 내부 연결이 없다고 설명할 수 있음
          + “visible/hidden 뉴런끼리는 내부 연결이 없음”이라는 말을 듣고, 그럼 그냥 MLP랑 뭐가 다르지 헷갈렸음. 근데 서론 부분을 위로 스크롤해야 한다는 사실을 뒤늦게 깨달았음. 그리고, 스크롤 구현을 새로 만들거나 고치는 것은 그다지 바람직하지 않다는 점이 정확하다고 봄
"
"https://news.hada.io/topic?id=21035","머티리얼 디자인의 확장 : Material 3 Expressive","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  머티리얼 디자인의 확장 : Material 3 Expressive

     * 구글이 디자인 시스템 Material 3를 대대적으로 업데이트 함
     * Material 3 Expressive는 감성적으로 강렬한 UX를 구축하기 위해 새로운 기능과 신규 컴포넌트, 그리고 디자인 전략을 추가
          + 새 시스템인 “M4”가 아니며, 기존 M3를 폐기하는 것도 아님
     * 2014년 이후 18,000명이 참여한 46개 사용자 연구 결과, 모든 연령대가 Expressive 디자인을 더 선호. UI 요소 식별 속도가 최대 4배 향상
     * 총 15개 신규 또는 개선 컴포넌트 포함 : Button groups, FAB menu, Loading Indicator, Split button, Toolbars 등이 새롭게 추가
     * 공간/효과 스프링으로 구성된 모션-물리 시스템, 더 강조된 타이포그래피, 35종 이상의 새로운 도형 추가 및 애니메이션 변형 지원, 개인화 및 계층 강조를 위한 색상 확장
     * 디자이너는 형태, 색상, 텍스트, 컨테이너, 모션, 유연한 컴포넌트를 결합해 Hero moment를 구성할 수 있음
          + Hero moment 판단 기준
               o 감정적으로 임팩트가 있는가?
               o 제품 내에서 핵심적인 인터랙션인가?
     * 주요 링크들
          + 업데이트된 M3 가이드라인
          + 업데이트된 Figma 디자인 키트
          + Jetpack Compose 알파 코드
"
"https://news.hada.io/topic?id=20983","Itter.sh - 터미널 전용 미니멀 소셜 미디어 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Itter.sh - 터미널 전용 미니멀 소셜 미디어 플랫폼

     * 터미널에서만 접속 가능한 소셜 미디어로, SSH 클라이언트를 통해 글을 쓰고 타임라인을 보는 최소한의 인터페이스를 제공
          + 회원 가입 : ssh register:사용자명@app.itter.sh
          + 로그인 : ssh 사용자명@app.itter.sh
     * 한 번에 최대 180자(""eet"") 를 작성할 수 있고, 타임라인은 알고리듬/광고/자동 스크롤 없이 CLI 명령으로 조회
     * #해시태그와 @멘션도 지원
     * 서버는 Python + asyncssh, 데이터베이스는 Supabase 기반으로 구성되어 있으며 실시간 업데이트도 지원
     * 로그인부터 회원가입, 글 작성, 팔로우까지 모두 CLI 명령어로 수행되는 고전적이고 해커스러운 감성의 플랫폼
     * 개인 타임라인과 공개 타임라인을 구분하여 볼 수 있으며, 특정 사용자나 해시태그 기반 조회도 가능
     * 라이브 타임라인 기능은 Supabase Realtime을 활용하여 새 글을 실시간으로 표시
     * 소스코드 : https://github.com/rrmn/itter

   레트로인가요. 천리안, 하이텔, 나우누리....

   증말 귀엽고 개발자스러운 플랫폼이네요ㅋㅋㅋㅋ

   와 이 아이디어 정말 마음에 쏙 드네요...!!

   ㅋㅋㅋ (tw)itter (tw)eet 이름 재미있네요ㅋㅋ

   너무 취향이네요.... ㅋㅋㅋ

   우와 아이디어 재미있내요
"
"https://news.hada.io/topic?id=21026","딥러닝은 응용 위상수학임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             딥러닝은 응용 위상수학임

     * 딥러닝은 데이터를 의미 있는 방식으로 변형하는 위상수학적 변환의 연속으로 이해할 수 있음
     * 신경망은 고차원 공간에서 데이터를 변형해, 원래는 분리할 수 없던 데이터를 구분 가능하게 만드는 토폴로지 생성기로 작동함
     * 데이터는 고차원 다양체(manifold) 위에 존재하며, 의미 있는 분류·번역·추론 태스크를 위해 신경망이 해당 다양체 구조를 학습함
     * 최신 인공지능 연구에서는 추론(manifold) 상에서 더 나은 지점으로 이동하기 위한 다양한 지도학습·강화학습(RLHF 등) 기법이 도입됨
     * 신경망 자체, 이미지, 텍스트, 추론 논리 등 모든 정보는 다양체로 표현 가능하며, 신경망은 보편적 토폴로지 발견기로 작동함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

딥러닝과 위상수학의 관계

     * 위상수학은 사물의 변형 과정에서 변하지 않는 성질을 연구하는 수학 분야임
     * 딥러닝 신경망은 입력 데이터를 여러 차원에서 선형 및 비선형 변환(e.g. 행렬 곱, tanh)을 반복적으로 적용하여 점진적으로 데이터의 분포와 구조를 바꿈
     * 신경망 계층의 각각의 연산은 기하학적 변환으로 해석될 수 있고, 이 변환들이 누적될수록 복잡한 데이터 구조를 분리 및 분류할 수 있게 함
     * 이러한 특성은 다양한 데이터셋에서, 원래는 단일 선·면으로 구분할 수 없는 복잡한 클래스를 분별 가능하게 만듦

차원 확장과 데이터 분리

     * 이차원 평면에서 서로 겹쳐 구분이 안 되던 데이터도, 상위 차원(고차원) 으로 옮기면 손쉽게 분리 가능해짐
     * 신경망은 인간과 달리 임의로 높은 차원에서 연산이 가능해, 매우 복잡한 데이터 패턴에도 대응함
     * 예시로, 사진 속 개와 고양이 같은 분류 문제도 고차원에서 수학적으로 구분할 수 있는 구조(다양체)로 재구성함

심층 신경망의 의미와 역할

     * 신경망은 ""토폴로지를 생성하는 도구""로, 입력 데이터를 의미 있는 구조로 재배치함
     * 손실 함수(loss function)는 데이터의 어떤 성질을 학습할지 정의하며, 분류, 번역, 예측 등 다양한 작업에 맞는 표면(topology)을 만들게 됨
     * 모든 의미 있는 데이터(텍스트, 이미지, 사운드 등)는 고차원 수치 벡터(embedding vector) 로 저장되어, 이 공간 안에서 유연한 수학적 연산 가능

다양체(manifold)와 의미의 표현

     * 색상, 이미지, 단어, 심지어 가구 분류 등, 모든 정보·개념은 특정 고차원 다양체라는 공간 위에 존재함
     * 예를 들어, RGB 이미지의 모든 픽셀 값은 거대한 벡터로 표현되어, 이미지 다양체 상에서 의미 있는 변환과 유사도를 분석할 수 있음
     * 임베딩 연산을 통해, 의미적으로 관련된 개념(예: ""king"" - ""man"" + ""woman"" = ""queen"")끼리 가까운 위치로 배치할 수 있음

신경망, 추론, 학습 전략의 다양체적 접근

     * 인간 추론 자체도 고차원 다양체 상의 클러스터로 모델링 가능하며, 신경망은 이를 따라 점진적으로 더 우수한 추론으로 이동함
     * 현재 대형 언어 모델(LLM)들의 한계점은 순수 언어 통계(next-token prediction)만으로는 인간 수준의 추론에 도달할 수 없다는 것임
     * 이를 극복하기 위해 지도학습, RLHF, Chain-of-Thought, 고품질 reasoning trace 수집 등 여러 강화학습 기반 접근법이 활용되고 있음
     * 최근 강인한 추론 모델을 위해 Deepseek R1과 같은 논문에서는 객관적 기준(예: 단위 테스트, 수학문제 정답 여부)으로 '좋은 추론'을 자동 선별하여, 기존 인간 평가의 한계와 비용 문제를 극복하려 시도함

신경망과 모델 자체의 다양체 구조 활용

     * 신경망의 모든 파라미터(가중치)도 하나의 거대한 벡터로 표현되며, 이를 다양한 의미 공간(semantic space) 상의 다양체로 해석 가능
     * 이미지 생성을 위한 diffusion 모델 개념을 신경망 파라미터 공간에도 확장하여, 기존 pretrained 모델들의 다양한 특성을 효율적으로 재활용하거나, 빠른 초기화 및 신규 모델 생성을 도모할 수 있음
     * 모델의 임베딩 공간을 탐색하는 기법 발전은, 향후 더욱 빠르고 효과적인 AI 개발을 가능하게 할 수 있음

결론 및 시사점

     * 딥러닝 분야는 여전히 비공식적이고 직관에 의존하는 경향이 있으나, 위상수학적 사고는 복잡한 모델 작동원리 파악에 큰 도움을 줌
     * 임베딩 공간과 다양체 구조에 대한 인식이 넓어질수록, 더 실질적이고 체계적인 AI 개발 및 분석이 가능해질 전망임

        Hacker News 의견

     * 2014년 내 블로그 글을 바탕으로 작성된 이 글에 대해, 나는 신경망을 이해하는 수단으로 위상수학을 굉장히 열심히 사용해보려 노력했음. 그 결과를 아래 두 후속 글에서 공유한 적 있음
          + https://colah.github.io/posts/2014-10-Visualizing-MNIST/
          + https://colah.github.io/posts/2015-01-Visualizing-Representations/
            신경망 내부를 이해하려는 과정에서 위상적 관점이 유용했던 부분이 있는 반면, 10년 가까이 탐구한 결과로 볼 때 위상적 접근이 아주 큰 도움을 주진 못했다고 느꼈음
            더 효과적으로 익힌 것은 다음과 같음
          + ‘선형적 표현 가설’ — 신경망에서 개념(특징)은 특정 방향과 대응한다는 생각
          + ‘회로’라는 개념 — 이런 특징들이 연결되어 구성하는 네트워크 구조
            관련 글로는 아래 글들을 추천함
          + https://distill.pub/2020/circuits/zoom-in/
          + https://transformer-circuits.pub/2022/mech-interp-essay/index.html
          + https://transformer-circuits.pub/2025/attribution-graphs/biology.html
          + 신경망을 이해하는 방식과 관련해, 나는 종종 다음과 같은 오해에 대해 생각함
               o LLM이 단순히 기존 n-gram 모델보다 약간 더 나은 것에 불과하다는 주장
               o ""그저 다음 토큰을 예측하는 것뿐""이라는 주장에서, 그 자체가 모델이 단순하다는 인상을 주는 현상
                 Karpathy의 RNN 포스트에 대한 인기 있는 반응이나 ‘stochastic parrot’ 논문에서 LLM과 n-gram 모델을 동일시하는 뉘앙스를 종종 볼 수 있음. 과거에는 두 접근이 좀 더 비슷하게 여겨졌으나, 최근 모델이 굉장히 발전한 후에는 그 등식이 잘 맞지 않음
          + 실제 상황에서 위상수학을 적용하려 했던 내 경험을 회상함. 2011년 위상수학을 처음 배운 뒤로 지금까지 간헐적으로 시도해봤으나, ""실제 데이터가 매끄럽고 저차원인 매니폴드에 가까워진다""는 흔한 주장에 대해 회의적임. 실제 데이터에 정말 이 특성이 성립하는지, 혹은 우리가 효율성을 위해 차원축소 방법을 써서 의도적으로 왜곡한 결과인지 좀 더 깊이 탐구해보고 싶지만, 시간적 여유가 없는 것이 아쉬움
          + 너가 오랫동안 '회로(circuits)' 관련 글을 이어온 것을 재미있게 봐 왔음. 선형 표현 가설은 특히 설득력 있게 여겨져서 Toy Models of Superposition에 대한 리뷰 초고도 써둠. 다만 ‘회로’ 분석은 Transformer 구조에 너무 치중되어 있다고 느껴서 덜 매력을 느낌.
            GAN, VAE, CLIP 등 모델은 명시적으로 매니폴드를 모델링하고 있는 것처럼 보임. 단순 모델도 최적화 과정에서 비슷한 특징을 같은 방향으로 모아버릴 수 있지만, 때로는 유사한 특징들이 직교 방향에 위치하게 되는 실증적 현상이 존재함. 이는 아마도 최적화되는 손실 함수에 더 관련이 있어 보임
            Toy Models of Superposition에서는 MSE를 쓰고 있어서, 마치 오토인코더 회귀·압축 업무처럼 행동함. 공출현하는 특징들의 상호간섭 패턴이 중요하기 쉬움. 하지만 대조적 손실 함수가 목표라면, 이런 간섭 최소화 행태가 달라질 것이라고 생각함
          + 과거 내 글에 대한 Hacker News 토론들이 있었음을 공유하고 싶음
            Neural Networks, Manifolds, and Topology (2014)
               o 2019년 2월 https://news.ycombinator.com/item?id=19132702 (25개 댓글)
               o 2015년 7월 https://news.ycombinator.com/item?id=9814114 (7개 댓글)
               o 2014년 4월 https://news.ycombinator.com/item?id=7557964 (29개 댓글)
          + 물리학에서는 서로 다른 전역적 대칭성과(위상적 매니폴드) 같은 계량 구조(국소 기하)가 성립할 수 있다는 점이 흥미로움. 예를 들어 아인슈타인의 장방정식에서 같은 계량 텐서 해가 위상적으로 서로 다른 매니폴드에도 존재할 수 있음.
            반대로 Ising Model 해를 보면, 같은 격자 위상 구조라도 여러 해가 존재하며, 임계점 근처에서는 격자 위상 구조가 사실상 중요하지 않을 수도 있음.
            이는 단순한 비유이지만, 역동성의 중요한 세부사항이 시스템의 위상에 깃들어 있는 게 아님을 시사함. 훨씬 더 복잡한 이야기임
     * 만약 진짜 위상수학이 핵심이었다면, 우리는 매니폴드를 평탄하게 변형해서 유사성 탐색을 쉽게 하려고 하지 않았을 것임. 사실상 핵심은 ‘기하(geometry)’와 그에 맞는 측도임. 실제 삶에서도 우리는 사물을 비교할 수 있는 구조를 원함
       신경망 훈련 중에도 매니폴드는 위상적으로 변형됨. 이런 과정에서 ""과연 훈련 중 위상이 어떻게 변하는가?""라는 질문이 떠오름. 개인적으로는 처음에는 위상이 격렬하게 요동치다가 점점 안정화되고, 이후에 기하적 세부 조정이 진행된다고 상상하게 됨. 참고할 만한 논문은 다음과 같음
          + Topology and geometry of data manifold in deep learning https://arxiv.org/abs/2204.08624
          + Topology of Deep Neural Networks https://jmlr.org/papers/v21/20-345.html
          + Persistent Topological Features in Large Language Models https://arxiv.org/abs/2410.11042
          + Deep learning as Ricci flow https://www.nature.com/articles/s41598-024-74045-9
          + GAN이나 VAE를 활용해 본 적 있다면 이 위상 변화 과정을 실제로 관찰할 수 있음. 훈련 도중 다양한 체크포인트에서 UMAP, TSNE 같은 도구로 고차원 공간의 포인트들이 어떻게 이동하는지 볼 수 있음
            네가 상상한 ""초기에는 격렬한 변화 이후 안정화, 그리고 기하적 미세 조정""이라는 과정이 실제로 맞음. 이때 초반의 격렬한 변화는 학습률, 옵티마이저 선택 등의 영향도 큼
          + 굳이 따지자면 여기에서 다루는 건 응용선형대수라고 할 수 있겠지만, 그렇게 말하면 좀 멋이 없어지는 느낌임
     * 지금 제목은 진부하고 부정확함. 내용은 재미있게 읽었음
       위상수학은 거리, 각도, 방향 등 기하의 다양한 제약을 지워버렸을 때 남는 최소한의 구조를 다루는 수학임. 이처럼 격렬한 변형에도 본질적으로 남는 연관성만 바라보는 게 위상수학적 관점임
       기계학습에서 위상 개념이 유용할 수는 있지만 실제로는 스케일, 거리, 각도 같은 기하적 정보가 데이터의 본질에 훨씬 더 중요하게 작용함. 예를 들어, 탭이 고양이와 호랑이를 구분하는데 스케일을 무시하면 어리석은 결과가 나옴
       신뢰할 수 없는 정보들이 많을 때 비로소 위상적 접근이 유용해지는데, 딥러닝이 위상수학에 기반한다고 보는 건 지나침
          + 네가 말한 것처럼 거리, 각도, 길이 등을 신뢰할 수 없어야 위상수학이 유용하다는 의견인데, 실제로 우리는 신뢰할 수 없는 데이터를 다룸. 이미지 픽셀 공간에서 콜라캔과 정지표지판이 적당히 가까워도 의미 없는 일임. 신경망은 네가 말한 ‘격렬한 변형’들을 실제로 진행함
          + 실제 구현 단계에 들어가면, '만약 진짜 위상이라면 신경 안 써도 됐을' 세세한 부분, 예를 들면 레이어 수나 양자화, 부동소수점 해상도 등이 중요한 역할을 함
          + ‘위상’이라는 용어에는 사전적으로 두 가지 정의가 존재함. 네가 전제로 제시한 속성들만을 위상 개념으로 보는 것은 일부 정의에 국한된 견해임
     * 이 글에서 분리면을 찾는 아이디어를 '위상수학'이라고 부르는 이유를 잘 모르겠음.
       예를 들어 ""번역을 학습한다면 model이 bread와 pan, 고양이 사진과 cat 단어를 가깝게 위치시키는 topology를 학습한다""는 설명이 있는데, 이처럼 '가깝거나 멀다'라는 이야기야말로 위상과는 거리가 멈
       위상 공간에서 두 점이 가깝다고 해서, 그 공간을 늘려버리면 ‘같은 위상 공간’ 안에서 두 점을 충분히 멀리 띄울 수 있음(‘커피잔과 도넛이 같은 위상’이라는 우스갯소리의 요지임)
       실제로는 대수기하(algebraic geometry)적 접근 — 점들이 어떤 대수적 다양체(algebraic variety) 근처에 위치하는 구조 — 을 적용하는 게 더 적합해 보임. 결국 중요한 것은 기하학과 거리임
          + 만약 위상에 대해 느슨하게 정의를 내려야 한다면, '거리'가 없어도 '가깝고 멂'의 개념(근방, neighborhood)을 다루는 수학적 공간의 연구가 위상수학이라고 보겠음. 개방집합에 대한 다양한 정의가 곧 위상(topology)을 고르는 일이 되고, 그 결과 연속성, 콤팩트성, 연결성과 같은 성질이 정해짐.
            거리공간은 위상공간의 한 사례임.
            물론 그렇다고 위상이 신경망 이해에 항상 최선의 관점이라고 볼 수는 없음. 원 저자도 현재는 입장을 달리하고 있음
            오해만 풀고 싶었음. https://en.wikipedia.org/wiki/General_topology 참고
          + topology와 아무 상관 없는 이야기라는 점에 100% 동의함. 한 글이 topology와 딥러닝에 관한 거라면, 혼란은 topology 쪽으로만 한정되길 바람
          + 방금 쓴 'topology'라는 단어를 조금 관용적으로 사용한 것임. 정확히는 '분리면(surface)'라고 했어야 함
     * 나는 학습을 매니폴드 관점에서 바라보는 게 힘 있는 표현이라고 생각함
       고차원 공간에서는 reasoning(추론) 자체와 사실상 구분이 안 된다는 느낌을 많이 받음
       이런 ‘probabilistic reasoning manifolds’에 대해 일기나 뉴스 댓글로 많이 써 봤음.
       패턴 공간으로 이루어진 매니폴드는 본질적으로 확률적인 학습을 통해 형성되며, 실제 추론은 명제가 아니라 확률적으로 이뤄진다는 생각임. 고정점이나 어트랙터(끌림점)를 찾음으로써 일부 '공리'를 찾아낼 수는 있지만, 결국 입력 데이터로부터 형성된 확률적 매니폴드를 분석하게 됨
       추론과 데이터는 얽혀 있어 완전한 분리가 불가능함
       비문맥적 관계를 학습(분해)하는 것 — 바로 이게 'decontextualization'임. 그렇지만 이와 더불어 새로운 상황이나 도메인에서 의미 있게 분석이 이루어지려면 반드시 'recontextualization'이 뒤따라야 함.
       더 긴 설명은 https://news.ycombinator.com/item?id=42871894 참조
          + ‘추론 씽킹’이란 개념 일반, 즉 (명제의 표현에 대한) 정신적 조작일 때 ""진정한 추론은 확률이 아니라 공리로 표현된다""는 말은 이해하기 어렵다고 생각함
            동물들이 명제적 진술을 전혀 비확률적으로 다루지 못한다면, 그건 논리적 추론이 아예 불가능한 상태라서 실재 동물의 추론 가능성을 설명할 수 없음
            예) ""거미가 A상자에 들어있으면, 다른 상자에는 없음""과 비슷한 단순 논리 구조의 추론
     * 실제 데이터는 진짜로 매니폴드에 존재하는 게 아님. 그냥 데이터에 대해 생각을 쉽게 하려고 쓰는 근사 개념임
       딥러닝의 거의 대부분 유익한 업적은 topology와는 무관하게 만들어짐. 딥러닝은 실험과 시행착오, 그리고 극히 일부의 수학적 영감(그것도 topology가 아님)에서 빠르게 발전한 경험적 분야임
          + 나는 이 주장에 전적으로 반대함. 물론 시행착오가 많은 건 맞지만, topology, geometry, game theory, calculus, statistics 등 수많은 수학 이론의 복합적 작용임. 역전파(backpropagation)만 해도 체인 룰임
            많은 실무자가 이 주제의 이론적 뿌리를 몰라도 쉽게 활용할 수 있을 만큼 field가 대중화되고 수익성까지 갖추었음
            결국 이론·기법을 창안하면서도, 사실은 기존 다른 분야 이론을 비의식적으로 '재발견'해 활용하는 경우가 많음
          + ""이런 영감은 다 원래 topology가 아니었다""는 주장에 대해, 내 생각에 이런 ‘수학적 직감’은 대부분 사후적으로 적용되는 것임. 깊은 러닝에서 뭔가 돌파구를 찾은 뒤 물리나 수학 연구자들이 자기 분야 방법과의 유사성을 뒤늦게 인식함
            예로 GPT가 내가 과거 물리문제 풀던 알고리즘과 거의 같다는 글이 있음
            https://ondrejcertik.com/blog/2023/…
          + 내가 딥러닝 분야에 10년 넘게 있었지만, ""데이터가 매니폴드에 존재하지 않는다""는 주장은 틀렸음. 임베딩 공간을 'space'라고 부르는 데는 다 이유가 있음. GAN, VAE, contrastive loss 등은 실제로 걷거나 조작할 수 있는 벡터 매니폴드 구조를 구축함
          + 근사 오차까지 허용한 정의라면 실제 데이터가 매니폴드 상에 놓인다고 할 수 있음. 참고 논문: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning(https://aclanthology.org/2021.acl-long.568.pdf)
          + 딥러닝은 현 단계에서 마치 연금술(alchemy)과 같다고 생각함
            이론적 밑바탕이 존재하는 화학(chemistry)이 나오기 전 시절의 연금술처럼 말임. 언젠가 후대의 인류가 ‘deep learning’이라는 단어 자체만 남겨 놓고 과거 언어의 흔적으로 취급할지도 모른다는 생각
     * ""이 정도면 AGI에 도달했다""라는 문구를 보고 신뢰도가 확 떨어졌음
       대체로 글의 아이디어 자체는 흥미로웠지만, reasoning과 엮는 부분이나 심화된 기술적 논의가 없는 fluffy함이 아쉬웠음. 이미 이보다 훨씬 구체화된 연구(i.g. https://arxiv.org/abs/1402.1869)가 존재함
     * DNN에서 많이 논의되는 또 다른 종류의 topology는 바로 네트워크 topology임. 즉, 노드가 어떻게 연결되고 데이터가 어떻게 흐르는지에 대한 구조임
       오토인코더, CNN, GAN 등 모두 생물학적 영감을 받았음
       아직 우리는 뇌의 topology와 그 기능적 연결성에 대해 배울 점이 많음
       앞으로 개별 레이어/노드 내부나, 전문화된 네트워크들 간의 연결·상호작용 구조 측면에서 완전히 새로운 아키텍처가 나올 가능성이 큼
       인간 뇌도 사실 하나의 네트워크가 아니라 ""Big 7"" 같은 여러 네트워크가 병렬적·상호 연동적으로 작동함. DMN(Default Mode Network), CEN(Central Executive Network), Limbic Network 등 다양한 네트워크가 존재하고, 한 뉴런이 동시에 여러 네트워크에 소속되는 경우도 많음
       인공지능에서도 아직 이런 복잡성을 완전히 재현하지 못했기에, network topologies에서 영감을 받을 점이 무궁무진함
       ""Topology is all you need""라는 말에 공감함
     * 수학적 위상(topology)은 기하적 객체와 변환을 다루지만, 컴퓨터에서는 추상 객체 간의 관계를 정의하는 ‘위상’ 개념도 중요함
       예를 들어 그래프 자료구조에서는 객체(정점) 집합과 이들 간의 관계(간선) 집합을 저장하고, 이를 통해 그래프 자체가 하나의 이산적인 topology 구조가 됨
       네트워크 자료구조 역시 비슷하지만, 각 간선에 값이 추가로 저장됨. 즉, 정점(객체) 집합과 이들 간의 관계(간선), 그리고 간선별로 값(가중치)을 갖출 수 있음. 결국 인공신경망도 이런 방향으로 이해할 수 있고, 이산 topology 위에 구축된 구조임
     * 저자의 다이어그램에서 AGI/ASI가 next token prediction, chat, CoT 모델과 같은 매니폴드 상의 한 점으로 그려지는 부분이 혼란스러움. 후자의 세 유형은 확실히 연결된 동일 계에 속한다고 볼 수 있지만, AGI/ASI까지 포함한다고 할 충분한 근거가 있는지 의문임
       혹시 CoT 기반 모델이 아무리 topological manipulation을 해도 AGI가 지닌 ‘지능’에 결코 도달할 수 없는 구조라면 어떻게 되는지 궁금함
       예를 들어, 인간 지능은 고도의 센서적/내부 피드백·연속적 처리 기능이 필수인데, GPT류 오토리그레시브 모델은 본질적으로 불연속적임
       비전문가 입장에서는 LLM이 ‘인텔리전스’나 '의식'을 낳는 계열의 시스템과는 전혀 다른 족속에 가깝다는 직관이 있음
          + 그럴 수도 있다고 봄. AGI/ASI 정의 자체가 불확실함
            사실 나는 우리가 이미 AGI에 도달했다고 생각하지만 많은 사람이 동의하지 않음
            인간 지능의 본질은 고도의 감각/피드백 루프나 연속적 프로세싱에 있다는 언급이 있었는데, 제법 많은 connectomics 연구 경험상 생물·신경망의 유사성 역시 무시할 수 없음
            예를 들어, 마우스의 후각 체계에서는 어떤 뉴런 세트가 활성화될 때 특정 향('초콜릿', '레몬' 등)이 감지됨. 특성 벡터(feature vector)와 상당히 흡사한 구조임
            뇌의 뉴런 표상도 임베딩 표현과 비슷한 점이 있음. 마치 어떤 뉴런이 켜졌느냐에 따라 embedding space가 만들어지는 셈임.
            임베딩 위에서 이루어지는 것은 ""그 이상""이 아니라 전부 추가적인 처리임
"
"https://news.hada.io/topic?id=20951","메이저 트라이얼에 보내는 관제 센터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          메이저 트라이얼에 보내는 관제 센터

     * 오픈 소스 플랫폼을 이용하는 한 반관 정부 기관이 10년에 걸쳐 무한 트라이얼을 적극적으로 남용한 사례임
     * 해당 기관은 공식 결제 없이 XOA 어플라이언스를 지속적으로 사용하기 위해 수십 개의 계정을 생성함
     * 진심 어린 지원 제공과 안내에도 불구하고, 이 기관은 라이선스 우회를 멈추지 않음
     * 직접 구축 가능한 무료 옵션이 분명히 있었음에도 적절한 절차 대신 트라이얼 악용만 반복함
     * 이러한 행위는 오픈 소스의 지속 가능성을 위협하며, 앞으로 더 똑똑한 제한 정책이 도입될 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

🚀 시험판과 시행착오의 이야기

   오픈 소스의 세계는 아름답고, 복잡하며, 강력하면서 가끔은 이해하기 힘든 측면을 함께 지님

  # Vates와 오픈 소스 유지보수 현실

     * Vates는 오픈 소스 프로젝트 유지에 따르는 다양한 어려움을 이미 공유한 경험이 있음
     * 최근에는 AI 기반 기여와 가짜 보안 보고서의 증가로 새로운 복잡성이 추가됨
     * 하지만 이번 글에서는 좀 더 현실적이고 구체적인 사례에 대해 다룸

🧑‍🚀 끝나지 않는 트라이얼의 특이한 사례

  # 배경

     * 반관 정부 기관에 해당하는 한 회사는 연매출 1억 3천만 달러 규모로, 우주 관련 고가 장비를 구축·운영 중임
     * 이 회사는 수백 대의 실제 서버와 4,000여 개의 VM을 운용하며, IT 인프라 대부분을 Vates 플랫폼에 의존함
     * XCP-ng는 로켓, Xen Orchestra는 위성이라는 로고답게, 이 기업에게 핵심적인 플랫폼임

  # 트라이얼 남용 방식

     * 이 기관은 본인들이 유료 고객이 아님을 분명히 하면서도, 오픈 소스 소스코드 직접 설치 방식도 사용하지 않음
     * 대신, Xen Orchestra Appliance(XOA) 라는 손쉽게 설치 가능한 프리 패키지 VM의 30일(이전에는 15일) 트라이얼을 반복적으로 신청함
     * 2015년 4월부터 회사 이메일로 트라이얼을 요청해왔고, 점차 계정 수가 급증함
     * 개발자, 시스템 관리자, 매니저 등 직원 대부분이 트라이얼을 생성하며 ○○@corporate.com 형식에서 개인 Outlook, Gmail 계정으로 전환함
     * 이메일 핸들에 숫자를 늘리며 60개 이상 계정을 만들어내는 등 조직적인 태도를 보임

  # 공개 데이터

     * 트라이얼에 연결된 계정 수를 실제 수치로 차트화할 정도로, 적극적이고 일관된 행위가 이어짐
     * 문제 발생 시에는 실제 성명을 그대로 입력하는 등 꼼꼼하게 실제 회사명을 기입하는 방식

🔍 선택지 안내

     * 누구나 문서 안내에 따라 소스에서 빌드해 무료로 모든 기능을 사용할 수 있음
     * XOA 어플라이언스는 정기 테스트 및 업데이트, 클릭 한 번으로 최신 상태 유지 등 프로페셔널한 제공 환경이므로 판매 중인 제품임
     * 이 기관의 행위는 오픈 소스의 암묵적 ""도덕적 계약"" 을 명백하게 위반하는 것으로 볼 수 있음

🧠 실제 고민

     * Vates는 사용자에게 선의로 지원을 제공했으며, 초기 테스트-구매 의사가 있는 척 한 시점에도 진심으로 도와줌
     * 그러나 반복되는 질문, 익숙한 셋업이 계속 관측되었고, 60개가 넘는 독립 계정 기록을 확인하게 됨
     * 문제 제기 후 기업 측은 소스 버전으로 전환하겠다며 모호한 사과를 전달함
     * 볼륨 할인 제안조차 즉각 거부하고, 전문 지원에는 전혀 관심을 보이지 않음
     * 현실은 소스 전환도 하지 않고, 여전히 개인 계정으로 트라이얼 남용을 계속함

  # 무료 자가 구축이 가능한 상황

     * 단순히 몇 번의 명령어와 짧은 문서 확인만으로도 완전한 자가 호스팅이 가능한 환경임
     * 불편한 점은 오직 업그레이드 경험이 조금 덜 편리하다는 점뿐임
     * 그럼에도 이 기업은 무료 자가 구축 대신 반복적인 트라이얼 악용만 택함
     * 이는 오히려 XOA 어플라이언스의 가치가 시장에서 어느 정도 평가받고 있는지 방증함

💭 앞으로의 방향

     * 끝없는 추적에 시간을 낭비하진 않을 계획임
     * 하지만 10년 넘게 등록 정보에 실제 회사명을 입력해가며 개인 계정으로 반복하는 행위는 도를 넘는 수준임
     * 이런 행동 방식은 오픈 소스 생태계의 건강성과 지속 가능성을 해치는 요소임
     * 앞으로는 이런 트라이얼 남용 방지를 위한 더 스마트한 제한 정책 도입을 검토 중임
     * 목표는 정상 사용자의 이용을 막지 않으면서, 한정된 에너지를 실제 고객 지원·소프트웨어 혁신에 집중함에 있음
     * 마지막으로 해당 회사가 이를 읽고 있다면, 지금이라도 합리적이고 윤리적인 선택을 시도하길 바람

        Hacker News 의견

     * 이런 상황에서는 Larry Ellison의 방식을 참고해서 이 회사들에게 적극적으로 대응할 필요성 제기, 10년 동안의 불법 사용과 시도들을 정리해 C&D(중단 및 중지) 요청서를 보내고, 15일 내로 중단하거나 라이선스를 구매하지 않으면 10년치 라이선스와 이자, 벌금 청구를 예고하는 전략 제안, 법적 대응의 경우 DMCA(디지털 밀레니엄 저작권법) 위반 가능성 언급, 이 법이 미국에서 적용되어 형사책임까지 발생할 수 있다는 점 강조, CEO로서 직원과 주주의 자산을 회수할 책임감에 대해 진지하게 생각해보기 권고
          + 명확한 도용 사건이어서 법정까지 갈 일 없이 회사가 바로 합의할 가능성 언급, 실제로 수백만 달러 손실과 더불어 벌금까지 부과될 수 있음을 지적, 핵심 논쟁은 피해 회사에 얼마를 지불하느냐로 귀결
          + 매달 청구서를 보내는 직접적 조치로 시작해볼 것을 제안, 법적 자문을 받아 청구서를 작성하고 반복 송부 후 미지급시 추심 경고로 압박, 시간이 걸릴 수 있지만 결국 회수가 가능하다는 경험 제시
     * ""몇 푼 아끼려다 성능 예술로 변했다""는 표현에 대해 추적 노력을 당부, 재판이든 아니든 최소한 회사 실명 공개로 경각심 촉구, 이로 인해 미성숙한 관리자를 교체할 수 있는 긍정적 결과 기대
          + 직접 CEO에게 연락하여 실태를 전달할까 고민 중, 다만 이미 사정을 알고 허용하고 있을 가능성에 실망, 법적 조치는 당장 고려하지 않지만, 공개적으로 잘못된 행위를 지적하는 건 생태계 내 경각심 고취 목적, 회사 실명 공개도 아직은 보류
          + 무료 체험 라이선스 위반이 맞는 상황으로 보이며, 정말로 자기 노동에 대한 대가 받기를 거부하는 것인지 의문
          + 홍보 목적으로 블로그 게시물 작성이 오히려 더 효과적일 수 있음을 지적, Rocket Company가 OSS 커뮤니티를 이용하는 행태라면 OSS에 대한 상호 보답도 고려되어야 함을 언급
          + 이 모든 이야기가 실제로는 제품 광고 목적일 가능성 제시, 무단 사용까지 언급하며 자사 제품을 어필하는 전략에 대한 농담
          + 연매출 1억 3천만 달러에 인공위성을 보유한 항공우주 기업이 많지 않아, 특정 회사(Planet Labs) 추정
     * 이전 직장에서 한 명이 만든 프록시로 무료 계정 하나를 100명 정도가 공유, 경쟁사와 비교할 때도 비용은 언급하지 않고 불법 사용을 지속, 불법임을 지적했지만 반응 없음, 비용 문제가 아닌 처리 회피가 배경, 설득보다는 무시하는 태도
          + Rocket Company가 월 30대 서버 사용 시 1년 60만 달러, 10년간 300만 달러를 아꼈을 수 있다는 계산, 실제 서비스 대금을 받으려면 IT 부서 통제 필요성 언급
     * 몇 푼 아끼려다 퍼포먼스 아트가 된다는 표현의 유머와 사례에 공감, 자사 인력이 제품 구매보다 더 많은 비용을 인건비로 소비할 수도 있다는 점 강조, 미션 크리티컬한 업무에 무료 체험판을 활용하는 기업의 고객 입장에서 우려 표명, 실제로 이런 사례가 있다는 이야기도 덧붙임
     * 이런 상황이 전혀 놀랍지 않음이 가장 실망스럽다는 의견, 이 때문에 무료 체험에서 신용카드를 요구하기 시작했다고 분석, 은밀한 부과 때문이 아니라 부정 사용을 어렵게 하려는 목적 설명
          + 신용카드 요구가 실질적으로 큰 효과를 주지 못하고, 반면 유료 이용자만 불편해지는 부작용 언급, 가상카드 활용으로 우회가 쉬운 현실 소개
     * “체험판은 종료되었으니 더 이상 키를 발급하지 못한다”는 정중한 안내문을 통해 조치하는 것 추천, 만약 몰래 추가 신청이 계속된다면 차단 및 마지막 수단으로 법적 조치 고려, 궁극적으로 목표는 유료 전환 고객 유치임을 강조, 예의 있으면서도 단호한 태도의 중요성
          + 회원 가입 단계에서 백엔드에서 회사명 및 알려진 alias 탐지 후, “무료 대상이 아닙니다, 영업팀이 곧 연락하겠습니다!”와 같은 메시지 반환하는 자동화 추천
     * CTO 입장에서 이런 행태는 전적으로 Aerospace Co의 CTO 책임이라고 단언, 초기에는 무료 티어 활용도 좋지만 매출이 발생하면 유료로 넘어가야 한다는 의견, 이런 행동을 선택하는 업계인들이 있다는 게 안타깝다는 소회
          + 동의하지만, 실제로 무료 티어를 잘 쓰는 실력이 중요한 점을 제기, SaaS 무료 티어는 초기 비용 절감용이라는 개인 의견, 규모가 크면 무료 티어는 결국 한계가 있음을 언급
     * 소비자 스타트업에서 추천인 이벤트를 악용한 사례 경험, 매달 꾸준히 복잡한 과정을 거쳐 무료 한 달을 챙기는 유저 발견, 기본 플랜도 거의 차이가 없는데도 이런 정성을 들이는 현상은 시스템을 이기는 짜릿함이나 재미 요소가 크다고 생각
     * 실명 언급 없이 마케팅 전략으로 역이용하는 방법 제안, 비공개 고객사 예시를 마케팅 페이지로 적극 활용 가능, 미지급 사례들을 산업별/연도별/사용량별로 익명 집계해 안내, 창의적 글쓰기 대회와 함께 사례 조사 후 유료 라이선스 제공, 비즈니스 스쿨과 연계한 케이스 스터디 및 설문조사로 홍보 효과 극대화, 약관 업데이트 제안과 함께 시간 지나면 세일즈팀이 다년 라이선스 전환 추진 가능성 언급
     * 개별 실무자 입장에서 ""벤더 리스크 평가"" 등 SaaS 구매 자체가 너무 힘들고 복잡한 프로세스가 많아, 이를 피하기 위한 풀뿌리 회피 행동이 원인일 수 있다는 가설 제시
          + 구매 절차가 지나치게 번거로워 실제로 그런 현상이 벌어진다는 사례 공유
"
"https://news.hada.io/topic?id=20941","애플이 웹 결제를 허용했지만, 전환율은 떨어질 수도 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    애플이 웹 결제를 허용했지만, 전환율은 떨어질 수도 있음

     * 애플이 웹 결제를 허용하면서 개발자들은 IAP 수수료를 피할 수 있게 되었지만, 웹결제 도입시 전환율은 오히려 낮아질 수 있음
     * RevenueCat은 5,600명의 사용자를 대상으로 IAP와 웹 결제 흐름을 비교하는 대규모 실험을 진행함
          + 4개의 변형 결제화면 사용 : 인앱결제, RevenueCat 인앱, IAP + 웹구매(웹은 30% 할인), 웹결제만 허용
     * 웹 결제만 허용한 경우 전환율이 25~45% 낮아졌으며, 주로 결제 단계에서 이탈 발생
          + 일주일 간의 초기 결과임. 아직 LTV, 리텐션, 환불율 등 중장기 데이터는 수집 중
     * IAP와 웹 결제를 병행하면 가격 차별화가 가능하나, 단순히 웹으로 유도하는 방식은 효과적이지 않음
     * 자신의 앱에 맞는 최적의 결제 구조를 실험으로 직접 확인해보는 것이 중요함
          + 자신이 Small Business Program(수수료 15%) 대상이라면, 웹 유도는 이득이 적을 수 있음
          + 수수료 30% 대상이라도, 전환율 감소로 인해 손해 볼 수 있음
          + 향후 더 정교한 타겟팅, 가격 구조, UX 개선이 있다면 웹 결제 흐름의 효율을 높일 수 있는 여지는 존재
     * 웹 결제 허용이 무조건 이득은 아님. 초기 전환율에서 명확한 하락 효과 발생
          + 가격 인센티브, UX 최적화 등 부가 전략 없이는 이탈률 커질 가능성 높음

   저도 웹결제 인앱결제 모두 지원하는 경우에도 인앱결제를 선택하는 경우가 꽤 있습니다. 한 곳에서 결제를 모두 관리할 수 있다는 것은 꽤 큰 장점이고, 특히나 한 두번 테스트해볼 제품이라면, 애플을 거쳐 결제하는게 더 안심된다는 생각도 일부 있는 것 같아요.

   특히나 한국 같은 경우에는 인증결제가 사실상 강요되다 보니 전환율 면에서 안좋을 수 밖에 없죠.

   당연히 가격을 낮춰야 하는 것 아닌가요
"
"https://news.hada.io/topic?id=20950","Free Threaded 파이썬의 첫 1년 돌아보기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Free Threaded 파이썬의 첫 1년 돌아보기

     * 프리 스레드 파이썬은 멀티코어 하드웨어를 효율적으로 활용할 수 있도록 설계됨
     * CPython 3.14에서는 핵심 모듈의 스레드 안전성과 성능이 대폭 개선됨
     * 아직 프리 스레드 빌드를 지원하지 않는 많은 주요 패키지들이 남아있음
     * 누구든 실사용 환경에서의 보고와 커뮤니티 기여를 통해 발전에 함께할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   지난주 CPython 3.14.0b1이 공개되었고, 이번 주에는 PyCon 2025가 피츠버그에서 시작됨
   이 두 가지 이벤트는 프리 스레드 파이썬의 배포와 안정화 측면에서 중요한 이정표임
   이 글은 지난 1년간의 과정을 돌아보고, Quansight 팀이 복잡한 의존성을 가진 실제 프로덕션 워크플로우에 프리 스레드 빌드를 실험적으로 적용하는 데 어떻게 핵심 역할을 했는지 설명함

프리 스레드 파이썬의 의미와 필요성

     * 프리 스레드 파이썬 지원은 멀티코어 CPU와 GPU가 보편화된 현대 하드웨어의 전체 연산 자원 활용을 가능하게 함
     * 기존 GIL(글로벌 인터프리터 락) 방식에서는 병렬 알고리듬을 완전히 활용하려면 우회 방법과 별도의 튜닝이 필요했음
     * 보통 threading 모듈보다는 multiprocessing을 사용했으나, 이것은 프로세스 생성 비용이 크고, 데이터 복사도 비효율적임
     * 파이썬 패키지 중 네이티브 코드를 포함하는 경우, 프리 스레드 빌드가 바로 호환되지 않으므로 스레드 안전성 보장을 위해 코드 감사를 반드시 수행해야 함
     * GIL 해제는 CPython 인터프리터에 깊은 구조적 변화를 필요로 했고, 기존 패키지의 구조적 문제까지 드러내는 계기가 되었음

주요 성과

     * Meta의 Python 런타임 팀과 함께, 아래와 같은 다양한 패키지와 프로젝트에 프리 스레드 Python 지원을 기여함
          + meson, meson-python, setup-python GitHub Actions, packaging, pip, setuptools 등 패키징 및 워크플로우 도구
          + Cython, pybind11, f2py, PyO3 등 바인딩 생성기
          + NumPy, SciPy, PyArrow, Matplotlib, pandas, scikit-learn, scikit-image 등 PyData 생태계의 핵심 패키지
          + Pillow, PyYAML, yarl, multidict, frozenlist 등 PyPI 다운로드 상위 주요 의존성
     * 아직 지원하지 않는 인기 패키지들(CFFI, cryptography, PyNaCl, aiohttp, SQLAlchemy, grpcio 등)과 머신러닝 라이브러리(safetensors, tokenizers 등)도 점진적으로 작업 중임
     * Quansight 팀의 CPython 핵심 개발자들은 3.14 버전에 다음과 같은 개선 사항을 반영함
          + warnings 모듈이 프리 스레드 빌드에서 기본적으로 스레드 안전하게 동작함
          + asyncio의 심각한 스레드 안전성 문제 개선 및 병렬 확장성 증가
          + ctypes 모듈에서 스레드 안전성 전면 개선
          + 프리 스레드 가비지 컬렉터 성능 향상
          + deferred reference counting 스킴 및 적응형 특수화 인터프리터 최적화
          + 수많은 버그 수정과 스레드 안전성 보완 진행
     * 프리 스레드 Python 지원을 위한 종합적 가이드1도 작성하여, 앞으로 더 많은 패키지에 실질적으로 참고될 수 있는 문서화를 제공함

프리 스레드 파이썬 생태계 현황

     * 1년 전(3.13.0b1 출시 당시에) 프리 스레드 빌드에서 대부분의 파이썬 패키지 설치가 전면적으로 깨지는 상황이었음
     * 빌드 실패 원인은 근본적인 문제라기보다, 지원되지 않는 기본 옵션이나 작은 가정이 깨졌기 때문임
     * 지난 1년간 커뮤니티와 협력하여 많은 문제를 해결했고, 특히 Cython 3.1.0이 공식 지원되면서 큰 전환점이 됨
     * 아직도 컴파일된 코드를 포함하지만 프리 스레드 휠을 제공하지 않는 패키지들이 남아 있음
       이들의 진행 상황은 수동 및 자동 추적 테이블2에서 확인 가능함

당면 과제

     * 현재 프리 스레드 파이썬 빌드는 실제 워크플로우에서의 실험과 피드백이 필요한 단계임
     * 멀티프로세싱 사용 비용이 큰 워크플로우 등에서 특히 성능 개선 가능성이 있지만, 각 패키지에 대한 세밀한 스레드 안정성 감사가 필수적임
     * 많은 라이브러리들이 뮤터블 자료구조를 제공하면서 스레드 안전성 문서화나 실제 안전 보장이 미흡함
     * 패키지 규모가 크고 레거시가 많은 경우, 코드를 완전하게 파악할 수 있는 사람이 부족해 지원이 늦어지는 현상 발생
     * 커뮤니티 차원에서 핵심 패키지 유지관리의 지속 가능성을 위한 노력이 필요함

기여 방법

     * 공식 가이드의 기여 안내서를 참조할 수 있음
     * 전체 생태계 이슈를 추적하고, 주요 호환성 문서는 free-threaded-compatibility 저장소5에서 관리함
     * Quansight-Labs 주관 커뮤니티 Discord6에서 토론 참여 및 기여 가능함

PyCon 발표 예정 안내

     * 본문 저자와 팀원인 Lysandros Nikolaou가 PyCon 2025에서 발표할 예정임
     * 경험에서 비롯된 실질적인 포팅 사례와 노하우를 공유할 계획이며, YouTube 녹화 영상도 제공 예정임
     * 프리 스레드 빌드는 파이썬 언어의 미래라 믿으며, 이를 실현하는 데 기여할 수 있음에 큰 기대감을 가짐
     * 오늘의 노력이 매일 수백만 개발자가 사용하는 다양하고 방대한 패키지의 미래를 여는 전환점이 되길 희망함

        Hacker News 의견

     * 많은 사람들이 멀티프로세싱을 사용하는데, 프로세스 생성 비용이 비싸다는 점 언급
          + SharedMemory 기능이 존재하는데, 왜 더 자주 사용되지 않는지 이해 못함
          + ShareableList 사용 경험이 좋았다는 점 강조
          + 유닉스에서 프로세스 생성 속도는 1ms 미만 수준
               o 하지만 PYTHON 인터프리터 프로세스 시작은 import 개수에 따라 30ms~300ms 걸릴 수 있음
               o 1~2자리수의 차이가 있으므로 정확한 수치 중요
               o CGI는 이 점에서 예외이며, C·Rust·Go로는 문제 없음
               o sqlite.org도 요청당 개별 프로세스 방식 사용 중이라는 예시 공유
          + ShareableList는 원자 스칼라와 bytes·문자열만 공유 가능
               o 파이썬의 구조화된 객체는 pickle dump 등 직렬화 비용 및 프로세스별 복제 메모리 비용 발생
          + numpy 배열 공유에서 큰 성공 경험
               o 명시적 공유 방식은, 스레드 간 실수로 공유해서 발생하는 문제 디버깅 난이도에 비하면 부담 아님
               o 많은 사람들이 스레드가 멀티프로세싱에 비해 얼마나 좋은지 과대평가함
               o GIL이 해제되면 난수성 segfault 디버깅이 늘어날까봐 걱정
               o JavaScript가 공유 메모리 기반 스레딩을 지원하지 않는 것에 사람들이 크게 불평하지 않은 점 언급
               o JavaScript는 속도가 빨라서 그런 필요가 적다는 해석
               o 파이썬의 기본 성능 개선에 더 많은 노력이 필요하다는 바람
          + 프로세스가 독립적으로 죽기 때문에, 락 잡은 채로 공유 메모리 데이터 구조를 수정중이던 프로세스가 죽으면 복구 어렵다는 점
               o Postgres의 공유 메모리 구조와 전체 백엔드 프로세스 종료 필요 예시
               o 스레드는 함께 죽는 구조라서 이런 문제 발생 인식이 덜 하다는 점
          + 공유 메모리는 전용 하드웨어에서만 동작
               o AWS Fargate 같은 환경에서는 공유 메모리 없음, 네트워크 또는 파일시스템 사용 필요로 인해 레이턴시 증가
               o fork 방식의 프로세스 복제는 또 다른 문제
               o 실제 경험상 green thread와 actor 모델이 훨씬 더 효과적이었다는 주장
     * GIL 제거가 파이썬 멀티스레드 코드에 병렬 처리 외에 다른 영향이 있는지 궁금함
          + GIL이 유지된 이유는 멀티스레드가 GIL에 의존해서가 아니라, GIL 제거가 인터프리터 구현과 C 확장, 단일 스레드 코드 속도 저하를 유발하기 때문이라는 이해
          + Free-threaded Python이 bytecode 경계에서 언제든 선점될 수 있다는 기존 보장과 동일한지 궁금
          + 아니면 락을 더 써야 하는 등 작성 방식이 달라지는지 궁금
          + free-threaded Python도 대부분 같은 보장
               o 다만, free-threading이 없으면 사람들이 스레딩 기능을 덜 쓰고, 경계에서 선점되는 버그가 실제로 잘 발생 안함
               o free-threading 도입 시 더 많은 버그 노출
               o 멀티프로세스 우회 방법을 쓸 필요가 없어져 유저 코드가 간단해짐; 인터프리터 복잡성 증가의 가치가 충분하다고 생각
               o C 확장 복잡화 문제는 free-threading보다 sub-interpreter가 더 심함; numpy팀이 sub-interpreter는 지원 못한다고 명확히 밝힘
               o numpy는 이미 free-threading 지원, 남은 버그 수정 중
               o 단일 스레드 속도 약간(한자리수 %) 느려지는 건 감수할 만한 트레이드 오프
          + 멀티코어 사용은 가능하나, 스레드당 성능 저하와 라이브러리 재작업 필요
               o PyTorch로 실험 시, 10배 CPU 사용에 절반의 작업 처리량 경험
               o 시간이 지나며 개선 기대, 20년 간 이걸 기다려온 만큼 반가움
          + 경쟁상황(레이스 컨디션)이 더 빈번하게 발생할 수 있으니, 신뢰도 보장을 위해 멀티스레드 파이썬 작성에 더 많은 주의 필요
     * Microsoft가 Faster Python 팀을 해산했다는 소식 전달
          + 2025 예상 실적 미달로 팀 유지 안됐던 것
          + 향후 CPython에 성능 개선이 추가되는지, 혹은 타사가 후원하는지 지켜볼 예정
          + Facebook(메타)는 아직 일부 후원 중인 것으로 보임
          + Microsoft의 약속 대비 일정이 크게 지연되었던 점 언급
               o 최근에는 심각한 정치·거버넌스 문제를 알고 있을 것; 유능한 직원이 괜히 기여해 그룹 비방을 당하고 싶지 않을 것이라는 의견
               o CPython 조직이 약속 과다, 순응파에 일거리를 몰아주고 유능한 반대자를 내몬다는 비판
               o 과거에는 이런 문제가 없었음, 현재 스스로 야기한 문제
          + 이런 결과는 아쉽지만, Microsoft의 장기 약속을 신뢰하지 않았던 본인의 예상이 맞았다는 언급
          + 최근 Google도 전체 파이썬 개발팀을 해고한 것 같다는 소문
               o 둘 다 시대적 원인이나 공통 분모가 있는지 궁금
          + 매우 안타깝지만, embrace & extend 이후 남은 건 하나뿐이라는 뉘앙스
     * 파이썬에서 GIL이 사라지는 걸 걱정하는 사람이 자신뿐인지 궁금
          + 어떤 언어든 복잡한 멀티스레드 코드는 신뢰 어렵고, 파이썬은 동적 특성상 특히 더 불안
          + 혼자만 변화에 대한 두려움이 있는 게 아니며, 근거가 비합리적일 수 있다는 점
               o GIL이 순수한 기술부채이니, 커뮤니티 이익을 위해 제거 필요
               o 요즘 파이썬에서 대부분 스레드 대신 non blocking IO와 async 사용
               o 스레드 사용 안하면 GIL 제거로 인한 변화 없음; C 라이브러리도 단일 스레드로 안전
               o 스레드 실제로 사용하면만 주의 필요
               o 나이브한 스레드 파이썬 코드는 지금까지 GIL에 막혀 단일 스레드처럼 동작했는데, 이제는 좀 빨라지되, 버그가 늘어날 수도 있음
               o 해결책은 스레드를 안 쓰거나, 쓰려면 제대로 배우면 된다는 조언
               o 앞으로 더 좋은 추상화 제공될 테니, 커뮤니티에서 structured concurrency 등 논의 기대
          + 본인은 asyncio를 적극 사용하고 있음
               o 싱글 스레드이지만 concurrent한 파이썬을 즐겁게 작성 가능; Node.js처럼 사용
               o 웹·네트워크 작업에는 이런 방식 추천
          + ML/AI 분야에서처럼 전문가가 복잡 라이브러리를 먼저 만들고, 일반 사용자에겐 그것을 전달하는 구조 예상
               o 파이썬의 GIL이 심각한 병목이 되는 경우들이 많아졌음
               o 그래서 Go 언어를 배움; 스레드 지원 제대로 되는, 파이썬보다 하위, C/C++보다는 상위 추상화라 할 수 있음
               o 컴파일러 방식도 스레딩 못지 않게 중요한 배경
          + 괜한 불안 조장이 될 수도 있지만, LLM이 지난 수십 년간의 GIL 존재 가정 하의 파이썬 코드로 학습된 사실 상기
          + GIL 유무는 멀티코어 작업을 원하는 사람에게만 해당
               o 현재도 스레딩·멀티프로세싱에 신경 쓰지 않았다면 실질적 변화 없음
               o 레이스 컨디션 이슈는 GIL 유무와 상관 없이 발생
     * 파이썬을 자주 사용하지만 전문가 아니며, 가끔 concurrent.futures로 단순 함수 여러 개 동시 실행
          + 이런 사용자가 앞으로 무엇을 바꿔야 하는지 궁금
          + 스레드가 GIL에 묶이지 않아 전체적으로 더 빨라짐
               o 공유 객체 락만 제대로 처리했다면 추가 걱정 불필요
     * 20년간 파이썬으로 프로 개발 경험에서 느끼는 점 공유
          + 스레드가 진짜로 필요할 때는 메시지 패스가 불가피한 경우에 한정
               o 파이썬 생태계가 이미 이런 모든 상황의 우회책 제공
               o 여러 스레드 핸들링 함정(락 등) 때문에, 앞으로 특정 라이브러리·도메인에서만 필요할 수 있음
               o 순수 파이썬만으로 최대한 성능 뽑아내려면 native code 기반 라이브러리(ex. Pypy, numba) 활용 가능
               o 파이썬의 성능 혁신은 결국 async 프로그래밍; 꼭 배워보길 추천
          + 파이썬을 비슷하게 오래 썼고 동의하지만, 약간 다르게 표현
               o 파이썬 스레드가 워낙 별로라서, 피하기 위한 다양한 우회책이 발전
               o CPU-bound 작업을 스레드로 2배 빨리하려다 GIL 문제에 부딪혀, 멀티프로세싱으로 전환; 데이터 구조 직렬화로 인한 비용 발생, 2배 코어에 1.5배 속도 등 비효율적 경험
               o 좋은 스레드 지원이 생기면 활용하고 싶은 환경 많음; 여태 없으니 각종 대체 접근법을 썼다는 점
               o 상황에 맞으면 async 활용 강력 추천(글리프, 당신 말이 맞았어요!)
     * AI 이미지이지만, 뱀에 꼬리가 두 개 그려진 것 의아함
          + 조용히 넘어가라는 찬조 의견; 파이썬 기사에 뱀 그림 들어가면, 대체로 딱히 신경 쓸 가치 없는 신호라고 유희적으로 언급
          + 'Confusoborus'라는 농담식 명칭 제안
     * 헤더 이미지의 뱀이 두 개의 꼬리를 가진 듯하다는 지적
          + 같은 프로세스 내에서 두 번째 스레드를 생성한 모양이라는 익살
     * 라이브러리 지원 외에, WSGI와 Celery 워커를 단일 프로세스로 띄우는 것에 다른 제약이 있는지 궁금
          + 제약은 없으나, 이런 방식이 언어의 1급 기능이 아니라는 점
               o GIL은 기술부채 이슈라는 설명
               o 병렬성 외에도 GIL 해제가 필요한 요소 존재
     * 앞으로의 성능 시대를 위한 엄청난 기반 작업이라고 생각
"
"https://news.hada.io/topic?id=20969","KVSplit - 애플 실리콘에서 2-3배 더 긴 컨텍스트 실행","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  KVSplit - 애플 실리콘에서 2-3배 더 긴 컨텍스트 실행

     * KVSplit은 애플 실리콘에서 대형 LLM과 긴 컨텍스트 윈도우를 실행할 수 있게 하는 오픈소스 프로젝트임
     * 키와 값의 분리된 정밀도 할당을 통해 최대 72% 메모리 감소와 품질 저하 1% 미만을 달성함
     * M1/M2/M3 칩 및 Metal 프레임워크에 최적화되어 있음
     * 실행 속도 개선 및 메모리 절약을 실측 벤치마크와 시각화 도구로 증명함
     * 손쉬운 설치, 원커맨드 비교, 그리고 다양한 벤치마크 및 분석 도구를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개: 왜 KVSplit가 중요한가

   KVSplit은 KV 캐시 내부의 키와 값에 상이한 정밀도의 양자화를 적용함으로써 애플 실리콘(M1/M2/M3) 환경에서 대용량 LLM과 훨씬 긴 컨텍스트 윈도우 실행을 가능하게 하는 오픈소스 도구임. 기존 프로젝트들이 키와 값을 동일하게 양자화하는 한계를 극복하고, 메모리 사용량을 획기적으로 감축하면서도 품질 저하를 거의 느끼기 어려운 수준으로 억제함. 실제 벤치마크, 속도, 그리고 품질 지표가 모두 공개되어 있어 신뢰할 수 있으며, Metal 지원과 직관적인 비교 및 시각화 툴로 개발 효율성이 높음.

   동종 프로젝트 대비 주요 장점은 다음과 같음:
     * 키-값 정밀도 분리로 한층 효율적인 메모리 관리 실현
     * 애플 실리콘 특화 및 풀 Metal 프레임워크 최적화 지원
     * 벤치마크, 퍼플렉서티, 메모리/속도/품질 측정 및 시각화 통합 제공
     * 원커맨드 설치, 모델 호환, llama.cpp 통합
     * 실시간 메모리 절감 시각화 및 다양한 비교 테스트 도구

주요 특징 및 핵심 내용

  개요

     * KVSplit을 이용하면 기존보다 훨씬 더 큰 LLM과 긴 컨텍스트 길이를 애플 실리콘에서 실행 가능함
     * 키와 값 각각에 다르게 양자화 정밀도를 할당하여 메모리 절감과 속도 개선을 모두 달성함
     * 최대 72% 메모리 절약 및 속도 개선(5-15%↑) , 품질 저하 1% 미만 확인됨
     * Metal 완전 지원, 최적의 애플 실리콘 가속효과 실현

  주요 벤치마크 결과

     * K8V4(키 8비트, 값 4비트) 구성에서
          + 59% 메모리 절감 및 0.86% 품질 저하(퍼플렉서티 기준)
          + FP16 대비 5.7% 속도 향상
     * K4V4(키/값 모두 4비트) 구성에서는
          + 최대 72% 메모리 절감
          + 품질 약 6% 손실 발생. 품질 민감도가 낮은 용도에 추천

  메모리 절감 효과 비교

     * FP16 기준 176MB(8K 토큰)에서
          + K8V8(8비트 키/값) : 93.5MB(47%)
          + K8V4(8비트 키, 4비트 값) : 71.5MB(41%)
          + K4V4(4비트 키/값) : 49.5MB(28%)

  주요 기능

     * KV 캐시 키·값 개별 양자화 기능
     * 애플 실리콘·Metal 완전 최적화
     * 벤치마크/품질(퍼플렉서티)/메모리 사용량 분석 스크립트
     * 시각화 툴 및 퍼블리케이션 품질의 그래프 생성
     * 원커맨드 셋업과 실시간 비교 기능

  프로젝트 폴더 구성

     * llama.cpp: Metal 최적화 빌드 포함
     * models: 모델 파일 저장
     * scripts: 벤치마크/설치/비교/시각화 스크립트 포함
     * results, plots: 벤치마크 결과 및 시각화 파일 저장
     * README.md

과학적 인사이트 및 실험 결과

  KV 캐시의 핵심 현상

     * 키 벡터는 양자화 민감도가 값 벡터보다 훨씬 높음 → 키에 더 높은 정밀도를 할당해야 품질 유지가 가능함
     * K8V4가 Sweet Spot: 키 8비트/값 4비트 배분이 품질-메모리 절충 최적임
          + 59% 메모리 절약, 퍼플렉서티 0.86%만 악화, 속도도 FP16보다 빠름
     * K4V8은 K8V4와 같은 비트수임에도 7배 이상 더 큰 품질 저하 발생 → 키 쪽 정밀도 중요성 입증

  종합적인 시사점

     * 메모리 효율 확보와 모델 품질 보존을 모두 실현 → 소비자용 하드웨어에서 훨씬 더 긴 컨텍스트와 큰 모델 운용 가능

사용 예시 및 설정 추천

  다양한 양자화 정밀도로 실행 예시

     * 기본(FP16): ./llama.cpp/build/bin/llama-cli -m models/your-model.gguf -p ""프롬프트"" -t 8 --flash-attn
     * K8V4 추천: --kvq 8
     * K4V8(DEMO): --kvq-key 4 --kvq-val 8
     * K4V4(최대 절감): --kvq 4

  긴 컨텍스트 예시

     * 32K 컨텍스트: FP16은 ~1.4GB 필요하지만 K8V4는 ~400MB로 실행 가능

  커맨드라인 플래그 설명

     * -t 8: 스레드 수, M1/M2/M3에 8추천
     * --flash-attn: 애플 실리콘 최적화
     * --kvq N: 키/값 모두 N비트 설정
     * --kvq-key, --kvq-val: 개별 설정 지원
     * -c N: 컨텍스트 토큰 수(길수록 KVSplit 효과 극대)
     * -n N: 생성 토큰 수
     * -f FILE: 문서 입력 파일
     * -m MODEL: 모델 경로

고급 벤치마킹 및 시각화

     * benchmark_kvsplit.py로 구성별, 시퀀스 길이별, 메모리/속도/퍼플렉서티/스케일 특성 측정
     * visualize_results.py로 논문 수준 그래프 생성
     * 결과는 CSV/JSON 자동저장 및 요약 제공

애플 실리콘 최적화 및 메모리 시각화

     * Metal 프레임워크 완전 활용
     * 메모리 부담 큰 M1/M2/M3 기종에 결정적 효과
     * capture_memory.sh로 실시간 메모리 절감 시각화 가능
     * 맞춤 페이지 정렬로 실제 절감 용량은 이론치와 소폭 차이 발생

요약 기능 정리

     * 독립 키/값 비트 정밀도 및 맞춤형 양자화
     * 애플 실리콘 및 Metal 완전 최적화
     * 메모리/속도/품질 종합 벤치마크 제공
     * 논문 품질 시각화 및 실시간 비교, 메모리 캡처 지원
     * 초간단 설치/사용 인터페이스

인용 및 참고 연구

     * ""More for Keys, Less for Values: Adaptive KV Cache Quantization""(2024)
     * ""Unifying KV Cache Compression for Large Language Models with LeanKV""(2025)
     * 기반 구현: [llama.cpp], 테스트 모델: [TinyLlama]

설정 추천 및 향후 계획

     * K8V4(키 8비트/값 4비트) : 품질 손실 0.86%, 메모리 59% 절감, 속도 +5.7%로 최적의 균형
     * K4V4: 최대 메모리 절감(72%), 품질 약 6% 하락. 메모리 최우선시 용도에 적합
     * 긴 컨텍스트 실행에 특히 강점, 2-3배 더 긴 컨텍스트 운용 가능

  향후 로드맵

     * 토큰 중요도 기반 적응형 정밀도
     * 레이어별 개별 양자화
     * 모델별 맞춤 최적화(Mistral, Phi-3 등)
     * 웹 데모 및 모바일(iOS/iPadOS) 지원 예정

라이선스 및 기여 안내

     * MIT 라이선스
     * 개발자/AI 연구자 누구든지 Issue 및 PR로 기여 가능

        Hacker News 의견

     * 흥미롭게 느껴짐이라는 관심 표현, 이런 현상이 왜 나타나는지에 대한 직관과 발견 경로에 대한 질문, 설치 스크립트에 패치 적용 단계가 미완인 점과 git submodule 활용 등 사용자 친화적 개선 제안, 그리고 다양한 파이썬 환경을 고려해 llama.cpp와 파이썬 의존성 분리 필요성 제안
          + 좋은 질문이라는 반응, 주된 차이는 어텐션의 핵심 역할과 연관 있음이라는 설명, 키는 어떤 토큰에 주목할지 결정해 어텐션 패턴을 만들고 값은 주어진 정보만 전달함을 강조, 키 벡터의 양자화가 너무 강하면 모든 토큰 상호작용에 왜곡이 커지는 반면 값 벡터의 오류는 해당 토큰의 정보만 영향을 줌을 비교, 도서관 서가 번호(키)가 틀리면 완전히 엉뚱한 책을 찾는다는 비유, 수학적으로도 키는 소프트맥스에 들어가서 작은 오류도 크게 증폭되나 값은 선형 평균이라 오류가 상쇄된다는 정보, 본인도 논문에서 이 비대칭성을 접하고 Apple Silicon에서의 영향에 대해 계량적으로 확인하려 했다는 경험, 설치 피드백과 파이썬 의존성 개선 약속
          + 패치가 실제로는 llama.cpp에 적용되지 않음, 아규먼트 파싱 코드가 이미 arg.cpp로 이동되어 무의미함을 지적, K/V 양자화 옵션도 2023년에 이미 llama.cpp에 추가된 상태임을 설명, 패치의 존재 의의를 의문시하며 단순히 명령줄 아규먼트만 다르게 만든 것처럼 보인다는 의견, 이런 간단한 패치 파일 적용에 굳이 새로운 저장소의 install.sh 파일을 실행하는 것을 자제해야 한다는 강력 권고
     * 이 패치가 MLX에서도 가능할지 묻는 질문, MLX에서 속도가 더 잘 나오고 해당 접근 방식이 맥 유저에게 실사용 속도의 장기 대화 지원에 도움이 된다는 기대
          + 아마 가능성이 있을 듯하나, 직접 MLX를 깊게 파고드는 중이고, 프레임워크 자체는 잘 설계됐지만 예제 코드나 벤치마킹 정보가 부족해 아직은 미성숙한 느낌이라는 소감, 오히려 Haskell 바인딩이 가장 기대된다는 개인적 인상, lazy evaluation 특성이 적합할 수 있고, 순수 함수적인 컴파일 그래프 구조도 잘 맞을 수 있다는 의견, Haskell에서의 ML도 흥미로울 것이라는 바람
     * --cache-type-k, --cache-type-v 옵션과 실질적으로 차이가 있는지 질문
          + LLM이 단순히 GitHub 별을 얻으려는 시도처럼 보이며 실제로는 기존 기능과 다르지 않다는 강한 의견
          + MLX/MPS에서 4비트 지원이 없거나 8비트조차 부족하던 기억, bf16 지원도 최근에 추가, 과거에는 애플 GPU에서 type_k/v 방식으로 최저 16비트(f16/bf16)까지만 가능했기에 llama.cpp 내부는 명확히 모르나 약간 다른 접근일 수 있다는 추측
          + 자신도 해당 차이점이 궁금하다는 간결한 동의
     * 코드를 읽은 뒤 패치가 불필요하다고 판단, 이미 2023년에 해당 기능이 llama.cpp에 반영되어 있음을 PR 링크로 확인, 굳이 포크 저장소가 아닌 패치 적용 방식으로 install.sh를 실행하도록 유도하는 점에서 경계 필요, 저장소에는 여러 개 패치 파일과 중복 코드, 패치 파일을 덮어쓰는 등 혼란스러운 구조 지적, 실제로는 --kvq 옵션만 추가하는데 이미 별도 K/V 양자화 옵션 존재, 작성자가 이러한 기존 기능을 인지하지 못했을 리 없다고 의심, 복잡한 스크립트를 제공하는 저장소의 스크립트 실행은 추천하지 않음, HN 포스트와 GitHub 스타 수가 높지만 내용이 오해의 소지가 크다는 비판, 저자가 계속 질문을 회피한다는 점도 우려, 덧붙여 저장소와 스크립트가 모두 과거의 llama.cpp 코드베이스와 혼재되어 있고 최신 구조와 맞지 않아 혼란스럽다는 결론
          + 본인도 여러 의심스러운 부분이 있으나 혹시나 자신이 뭔가 놓쳤는지 저자의 설명을 기다렸다는 언급, 하지만 여러 위험 신호가 있고 Github 활동 내역을 보면 LLM으로 생성된 코드로 Popular 프로젝트를 도배하는 의도가 의심된다는 지적
          + 드디어 합리적을 말하는 사람이 나왔다는 의견, 패치를 적용하는 것이 아니라 원본 프로젝트를 포크하는 구조가 되어야 한다는 점만으로도 신뢰 위험, 작성자의 GitHub 존재 자체도 수상하고, 인기 프로젝트에 LLM 잔여 PR을 대량 제출해 본인을 기여자로 위장하는 경향, 정보 오염 내지 AI로 인한 신뢰 붕괴 초입에 대한 우려 표출
     * 이미 변환된 .gguf 포맷 모델에도 차별적 KV 양자화(K8V4 등) 적용이 가능한지, 모델 종류나 토크나이저 설정의 제한 여부 질문
          + KVSplit의 주요 장점이 바로 기존 .gguf 모델에 특수 변환 없이 바로 쓸 수 있다는 점, 양자화는 실행 시점의 KV 캐시에만 적용되며 모델 가중치와 무관, --kvq-key와 --kvq-val 플래그로 메모리 저장 형식만 정해줌, 본인이 LLama-3, Mistral, Phi-2/Phi-3, TinyLlama, Qwen 등 다양한 모델에서 성공 테스트, 다만 llama.cpp Metal 백엔드 전용이며 Flash Attention이 현재 사용자 지정 KV 캐시 포맷을 우회하므로 -fa 0 옵션 필요, 그 외엔 어텐션을 쓰는 트랜스포머 구조라면 모두 적용 가능
     * 64GB, 128GB 등 고용량 Apple Silicon에서 이 패치가 더 빠르거나 좋은지 질문, 애플 실리콘에서 컨텍스트 창 확장이 실제로는 느리다는 소문, 큰 메모리에 실질적 의미가 있는지 궁금함을 표현
          + KVSplit의 메모리 절감 효과는 컨텍스트 길이에 비례해서 고용량 Mac일수록 절대적인 이점이 커짐, 128GB Mac Studio에서 수십만 토큰 컨텍스트를 다룰 수 있을 정도, 다만 근본적인 계산 속도를 빠르게 하진 않고 메모리 효율만 개선, 벤치마크상 K8V4 세팅에서 14.5%의 처리량 증가가 관찰되나 이는 메모리 접근 효율의 영향, 대형 모델에서 '느린 속도' 문제는 주로 연산 성능 한계 때문이기에 RAM이나 KV 캐시 최적화 유무와 무관, 즉 KVSplit는 메모리 한계를 느끼는 상황에서 유용하고, 실사용에서는 7B~13B 등 소형 모델에 더 큰 컨텍스트 창을 할당하는 것이 이상적, 대형 모델+초장문 창 모두 필요하면 서버급 GPU가 여전히 적합하나 애플 하드웨어로 가능한 한계를 넓혀준다는 점에서 의의
     * 흥미롭다는 관심 및 조금 더 상위 관점의 설명 요청, 2048 토큰 모델을 4~6k 등 확장에 쓸 수 있는지, 128k 컨텍스트 모델을 256k+ 창으로 쓸 수 있는지, 로컬 모델의 이상적 사용사례 질문
          + K8V4 구성에서 59% 메모리 절감이 가능하므로 최대 컨텍스트 길이가 2.4배까지 늘어남, 즉 2048 토큰 모델을 약 5000 토큰, 8K 모델을 ~19.5K까지 확장, 실제로는 맥북에서 한 번에 책 전체를 처리하거나 대형 코드베이스 분석, 대화형 앱에서 긴 대화 기록 유지처럼 활용, 메모리 절약효과는 컨텍스트 길이에 따라 선형적으로 증가, 본인 경험상 8K 컨텍스트에서 KV 캐시가 176MB에서 72MB로 줄었고 128k일 때 절감분이 기가바이트 단위가 됨, 입력 길이 한계로 OOM이 발생하는 경우에 가장 유효한 해법
          + 메모리 절감 효과로 특정 모델의 리소스를 줄여주므로, 사용 목적에 맞추어 어떻게든 응용 가능, 다만 컨텍스트 창 자체를 증설하는 것은 비전문가에게 쉽지 않으니 더 큰 창에 맞게 트레이닝된 모델을 사용하는 게 나음, 로컬 모델은 오프라인/보안/실험 목적으로 다양하게 쓸 수 있으며, 대부분은 모델 튜닝 실험 용도
     * 정말 좋은 아이디어이자 시도라는 칭찬, GPU에도 적용될 수 있는지, 기타 양자화 기법과 병용 가능한지 추가 질문
          + 이 방식은 아마도 NVIDIA/AMD GPU에도 똑같이 적용 가능, 키가 값보다 고정밀이 필요한 원리가 하드웨어 독립적이기 때문, llama.cpp의 CUDA 백엔드도 이미 --cache-type-k, --cache-type-v 옵션으로 분리 설정 지원, 현재 패치는 Metal을 위한 특화이지만 원리는 그대로 이식 가능, 다른 양자화 기법과도 병행할 수 있고, KV 캐시 양자화는 실행 중에만 적용되기에 가중치 양자화와 충돌 없음, 변환 파이프라인의 분리된 단계임, 다만 vLLM, TensorRT-LLM 등 별도 캐시 구조를 쓰는 엔진에는 별도 구현 필요, GPU에서 가장 큰 효과는 FlashAttention 구조에 바로 통합할 때 발생할 것, 메모리 절감이 곧 속도 향상으로 연결될 수 있음
     * 잘 이해가 되지 않는 부분이 있고 이상함을 감지, 해당 스크립트 실행 자제 권유, 신고했다는 안내
     * 성능이 어떻게 변화하는지 궁금, 더 긴 컨텍스트를 메모리에 넣어도 결국 연산 속도는 같지 않을까 질문
          + 실제로 동일 모델, 동일 프롬프트에서 fp16, q8, q4로 캐시 타입을 달리해봐도 이터레이션 속도는 비슷하다고 느낌, 내부 동작을 확인하진 않았으나 4~8비트 SIMD 일괄처리로 벡터가 패킹되길 기대했으나 실제론 패킹이 안 된 듯한 인상
"
"https://news.hada.io/topic?id=20965","Obelisk - WASM기반의 결정론적 워크플로우 엔진","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Obelisk - WASM기반의 결정론적 워크플로우 엔진

     * WASM 컴포넌트 모델을 활용한 결정론적(Deterministic) 워크플로 엔진으로, 재실행 가능한 워크플로/자동 재시도/에러 복구 기능 제공
     * GitHub Webhook, Turso DB, OpenAI, WASI 기반 HTTP 요청 등과의 연동을 통해 복잡한 백그라운드 작업을 안정적으로 처리할 수 있음
     * 하나의 바이너리로 실행되며, WASI 액티비티/워크플로/Webhook 요청을 처리하고, 실행 과정을 SQLite에 기록하여 영속성 보장
     * Web UI, CLI, gRPC API 등 다양한 방식으로 제어할 수 있음
     * 백그라운드/배치 작업, 주기적 태스크, 엔드 투 엔드 테스트 등 다양한 자동화 시나리오에 유용하게 활용 가능
     * 지원 플랫폼 : Linux x64 / arm64 (musl, glibc 2.35+, NixOS), macOS x64 / arm64

   결정론적(Deterministic) 의미
   결정론적(Deterministic) 이란 어떤 시스템이나 과정, 현상이 오직 하나의 결과만을 산출하며, 동일한 초기 조건과 입력이 주어지면 항상 같은 결과가 나오는 특성을 의미합니다. 즉, 우연이나 불확실성이 개입하지 않고, 모든 것이 원인과 결과에 의해 정확하게 결정되는 것을 말합니다.

   그렇다고 합니다.

   제목 클릭하면서 생각난 질문에 대한 답이 바로 댓글에 적혀 있어서 놀랐습니다... ㅋㅋㅋㅋ
   감사합니다
"
"https://news.hada.io/topic?id=21008","KDE에 마침내 “Karton”이라는 네이티브 가상 머신 관리자가 도입됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                KDE에 마침내 “Karton”이라는 네이티브 가상 머신 관리자가 도입됨

     * KDE Plasma 전용의 가상 머신 관리자 Karton 개발 소식이 공식적으로 확인됨
     * 이 프로젝트는 Qt Quick과 Kirigami 기반으로 제작되어 KDE 환경에 최적화됨
     * libvirt API를 사용하여 다양한 가상 머신 제어와 향후 멀티 플랫폼 지원을 목표로 함
     * 주요 기능은 커스텀 SPICE 뷰어, 스냅샷, 직관적인 인터페이스, 시스템/유저 간 하이퍼바이저 스위칭 지원임
     * Google Summer of Code 2025 일정에 따라 2025년 9월경 완성 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Karton 개발 배경 및 필요성

     * GNOME 환경에서는 GNOME Boxes처럼 쉽고 일관된 가상 머신 실행 툴이 제공됨
     * KDE 사용자는 오래된 qt-virt-manager 등 대안을 사용했으나, 개발 중단 및 KDE 고유성 부족 문제를 겪었음
     * 최신 KDE Plasma 환경에 자연스럽게 통합된 VM 관리 솔루션의 필요성이 높아짐

Karton 프로젝트 개요

     * Karton은 QEMU 프론트엔드 시도에서 출발하여 KDE 개발자 Harald Sitter가 Google Summer of Code 프로젝트로 본격화함
     * University of Waterloo의 Derek Lin이 Google Summer of Code 2025 참가자로 현재 활발히 개발 중임
     * Karton의 목표는 KDE 생태계에 적합한 네이티브 가상 머신 관리 도구 제공임

주요 기술 및 특징

     * Karton은 Qt Quick 및 Kirigami로 개발되어, KDE Plasma와 완벽한 시각적·사용성 통합을 추구함
     * libvirt API를 통해 가상 머신 관리 및 확장성 제공, 향후 멀티플랫폼 지원도 염두에 둠
     * 기존 virt-install CLI 직접 호출이 아니라 libosinfo를 활용해 OS 이미지 자동 인식 및 libvirt XML 자동 생성을 구현 중임
     * 디바이스 구성, 다양한 하이퍼바이저 지원 확장도 개발 과제임

주요 기능 및 목표 일정

   Lin이 Google Summer of Code 제안서에 명시한 기능은 다음과 같음
     * libvirt XML 포맷을 통한 가상 머신 설치 및 설정
     * Qt Quick 기반 커스텀 SPICE 뷰어 개발 (virt-viewer 대체)
     * 가상 머신 스냅샷 기능(백업/복원)
     * 직관적이고 미려한 GUI 및 미리보기, 커뮤니티 피드백 반영
          + MacOS UTM 리스트 레이아웃에서 디자인 참고
          + 모바일 친화적 인터페이스 제공
     * 실시간 상태 업데이트를 virEventRegisterImpl 함수로 효율적으로 처리
     * 주요 운영 체제 목록을 제공하는 브라우즈 기능
     * GPU/메모리 사용량 그래프(virt-manager 스타일)
     * QEMU 하이퍼바이저 세션(유저)/시스템(루트) 모드 전환 기능

개발 일정

     * Google Summer of Code 2025의 공식 코딩 시작일은 2025년 6월 2일
     * 중간 평가용 프로토타입 완료는 7월 14일, 최종 완성본 제출 마감은 9월 1일로 계획됨

결론

     * Karton은 KDE 환경에 최적화된 네이티브 가상 머신 관리 도구의 부재라는 오랜 문제를 해소해 줄 신생 프로젝트임
     * Qt와 KDE의 최신 본연 기술에 맞춘 가시성과 사용성을 동시에 제공하는 점에서 Linux 데스크톱 사용자 및 개발자 모두에게 의미 있는 변화임

        Hacker News 의견

     * KDE라면 윈도우 정렬, 창 렌더링, 앱 런처 아이콘 같은 기초 기능에 집중해야 한다고 생각함. 가상머신 필요하면 가상머신 소프트웨어를 따로 씀. KDE 통합 제품군은 좋은 소프트웨어도 일부 있지만, 데스크탑 환경과 굳이 연동할 필요는 없다고 봄. 파일 관리자, VTE, 텍스트 에디터 정도만 있으면 됨. 아이콘을 각 앱마다 따로 관리하면 좋겠음. 통합 아이콘 시도는 오히려 아이콘이 안 보이거나 검은 바탕에 검은 아이콘 등 문제만 발생
          + KDE 프로젝트와 Plasma의 개념에 혼동이 있는 것 같음. Plasma는 데스크탑 환경이고, KDE 프로젝트는 여러 애플리케이션을 개발 및 배포함. 많은 KDE 앱은 Windows 등 다른 OS에서도 동작하고, Plasma만 깔고 다른 KDE 앱 없이도 사용 가능. 역사적으로 데스크탑 환경을 KDE라 불렀지만, 지금은 KDE 프로젝트가 개발하는 여러 소프트웨어 중 하나일 뿐임. 아이콘 테마에는 나도 동의하지 않고, 나 또한 아이콘 테마를 사용하지 않음
          + KDE는 20년 넘게 다양한 도구를 개발해옴. 브라우저, 이메일 클라이언트, 연락처 관리 앱 등. KDE 1 때도 파일 탐색기가 있었고, 오피스 모음도 이미 개발 중이었음. KDE의 제품군은 최초부터 이어짐. Plasma는 KDE 개발물의 극히 일부임. 윈도우 매니저 역할만 원하면 LXDE, Hyprland, Sway, i3 같은 더 미니멀한 대안도 있음
          + 아이콘을 공통 자산으로 만들고 애플리케이션에 통합하는 시도가 항상 실패함. GNOME 커뮤니티가 이 부분에선 잘했음. https://stopthemingmy.app/ 참고. 크로스 앱 테마 일관성 지원은 90년대 환상에 불과했고, 실제로는 스크린샷용으로만 그럴듯했음
          + 이런 이유로 sway로 이사함. 시스템 각 부분 간 연동성은 필요하지만, 각 부분은 분리되어야 한다고 생각함. gnome, kde 모두 전부 다 쓸 때만 괜찮음. XFCE가 오히려 훨씬 모듈화됨
     * 기사 내용 말고 딴 이야기만 하는 댓글이 대부분이라 약간 아쉬움. 새로운 VM 매니저 출시가 기대됨. virt-manager 주로 쓰는데 거의 관리를 안 해서 HiDPI 화면에서 스케일링 문제 심각함. GNOME Boxes는 버그 많고 기능 부족. CLI인 virsh에만 신경 쓰는 것 같아서, 요즘 쓸만한 VM GUI가 없음
     * Arch에서 KDE Plasma 사용 중이고 이 환경을 아주 좋아함. 블루라이트 필터도 내장되어 있음. 윈도우로 다시 돌아갈 생각 없음. KDE는 더 빠르고, 더 예쁘고, 원치 않는 광고나 추적도 없음. 일상용으로 최고
          + Cachy와 Plasma를 VM 내에서 테스트 중이고, 다음 PC에는 이 조합을 바로 설치할 예정임. 지금은 Ubuntu와 Windows 듀얼 부팅인데, 6개월 넘게 Windows 로그인 안 했음. 아마 다음 PC는 아예 듀얼 부팅도 설치 안 할 예정
          + 1년간 gnome 썼다가 다시 plasma로 돌아옴. gnome이 너무 불편함. 확장 프로그램으로 겨우 임시방편했지만 업데이트하면 바로 깨짐. 영어 인터페이스에 ISO 단위 설정도 복잡함. 시작프로그램 관리하려면 별도 앱 설치 필요. 화면 스케일링, 다중 모니터, 화면 녹화 모두 구림. 60fps 모니터인데 마우스 포인터가 끊김. 스웨덴어, 사미어, svdvorak 자판 숨기기도 별 도움 안 됨. 복붙이 모니터 간에 안 됨. alt+tab으로 창 바꾸면 드래그&드롭 안 됨. 컨텍스트 메뉴 뜨면 전체 포커스가 잠겨서 Nautilus 파일 복사 대화상자 열면 다른 앱 클릭이 불가. 실수로 KDE를 VM에서 써본 후 gnome 불편함을 참을 이유가 없다고 깨달음. 그날 바로 opensuse로 복귀
          + 20여 년 전 KDE 1.0 처음 써봄. 당시 약간 윈도우 따라하기 느낌도 있었지만 완성도는 오히려 더 나았다고 기억함
          + Ubuntu + Plasma 조합으로 3년째 데일리로 사용 중임. Windows 7이 꿈꾼 모습이라고 생각함. dotnet, devops 엔지니어 입장에서 2020년대는 리눅스 툴체인과 오픈소스 완성도가 완벽하게 맞아떨어짐. Rider, datagrip, vscode 등 전부 잘 동작함. docker나 wsl 번거로움도 없음. Windows는 .NET 프레임워크 구버전만 구동할 때만 부팅하고, 언제든 VM에서 윈도우 NVMe 부팅 세팅해서 완전히 탈출할 수 있을 것 같음
     * KDE에 필요한 것은 새 기능이 아니라 버그 감소임
          + 나도 KDE의 버그에 항상 불평했는데, 6.3 버전 이후로는 10년 만에 심각한 버그를 못 만남. 잠시 안 썼던 분이라면 이번에 다시 써볼 가치 있음
          + 나도 비슷하게 생각함. 여러 번 시도했지만 KDE는 항상 gnome보다 안정성, 완성도 측면에서 부족하게 느껴짐. 아마 KDE의 높은 커스터마이즈 성향 때문일 것임. 컨셉은 좋아하지만 유지보수가 쉽지 않고, 개발자들도 버그 수정보다는 새 기능 도입에 더 끌리는 듯함
     * KDE에서 통합 VM 솔루션을 제공해주길 바람. VM에서 돌아가는 앱이 Kwin 창처럼 나타나는 기능이 있으면 좋겠음. 이를 위해 게스트 OS에 헬퍼 데몬이 필요할 수도 있음. 예전에도 비슷한 기능이 있었지만, 주요 DE에서 공식적으로 제공하면 환상적일 것임
          + 놀랍게도 Windows에서는 WSL2를 통해 이 기능을 지원함. 예전에 장난삼아 ""nautilus"" 실행해보고 깜짝 놀람
          + VirtualBox로 거의 유사한 경험을 구현 중임. 노트북에서 여러 VM을 띄우고 외부 모니터 연결 시 창 크기를 맘대로 조정할 수 있음. 모니터를 떼어내면 창이 자동으로 다시 축소. 클립보드 공유 등으로 거의 네이티브와 흡사함. 데일리 브라우저 전용, 계약 프로젝트 전용 등 용도별로 VM 분리 운영함. 가상 데스크탑을 호스트, 단일 데스크탑은 VM에 할당. alt+tab은 VM 내에서만 동작하도록 구성. VirtualBox의 잡다한 버그, Oracle 법무 리스크 등을 떠안고 있지만 QEMU나 KVM이 아직은 완성도가 덜해서 아쉽게도 VirtualBox만 고수 중임
          + 기술적으로 꽤 많은 해킹이 필요함. 폐쇄형 OS에서는 어렵고, Windows만 RDP로 지원함
          + debboostrap과 chroot 마운팅으로 더 가볍고 자원 소모 적은 방식도 시도 가능함
          + 현재 솔루션 중에서는 완벽히 지원하는 게 없음. X11 포워딩은 가능하지만 셋업이 필요하고 매끄럽지 않음. 리눅스에서 네이티브로 이걸 지원하는 클라이언트/서버를 아직 찾지 못했음
     * virt-manager 대안을 선택할 수 있다는 점이 아주 반가움. 특히 Qt 기반이라는 점이 좋음. Kirigami, Qt Quick을 사용하는 건 아쉬움. Qt Widgets 기반보다 외관이나 기능 모두 떨어진다고 느낌
          + virt-manager 대안이 필요하다고 생각함. XML에서 텍스트 검색이나 undo 같은 평범한 기능도 불편한 상황. KDE 이름 붙이기는 좀 구식이지만, Karton이라는 이름은 그나마 낫다고 생각함
          + Plasma shell 자체가 Kirigami와 Qt Quick 기반이므로 이 정도로 일관성 있게 통합된 환경 없음
          + QML 렌더링 특유의 버벅임은 상용 Qt 라이선스가 있어야 피할 수 있음. 대신 JSON 비슷한 문법으로 앱 만들 수 있는 장점 있음
          + Qt Quick이 좀 더 범용적이고, Kirigami는 그 위에 더 특화된 레이어임
     * KDE의 완성도와 풍부함이 좋지만 디자인이 요즘 다른 OS나 DM에 비해 올드하게 느껴짐. 커스터마이즈 가능하긴 하지만, 할수록 시스템이 느려지고 어색해짐. 이 때문에 gnome 선택
          + 많은 사람이 정반대 의견을 가지는 것도 재미있다고 생각함. 본인은 KDE야말로 유일하게 모던하고 예쁜 환경이라고 느낌
          + 플라즈마 6을 써봤는지 궁금함. 개인적으론 gnome보다 훨씬 모던한다고 느낌
          + KDE 디자인은 윈도우보다 훨씬 뛰어나다고 생각함. 윈도우는 데스크탑 디자인 중 최악을 항상 갱신한다고 느낌
          + 햄버거 메뉴라도 추가되면 바로 KDE로 다시 떠남. 방금 확인해보니 KDE도 이 트렌드에 동참했지만, 다행히 옵션으로 끌 수 있음
     * kvm/qemu용 GUI가 또 필요한지 의문임. cockpit-project가 이미 이런 목적에 맞춰 잘 만들어져 있다고 생각함
          + virt-manager가 지금까지 충분히 만족스러워서 굳이 새로운 대안이 필요했는지는 모르겠음. 그래도 경쟁은 항상 환영
          + 웹 인터페이스는 전문가급에게 어울리지만, 평범한 사용자에게는 어려움. VM 개념도 어렵고, VirtualBox나 VMWare같이 친숙한 UI가 오히려 접근성 높임
     * virt-manager 오래 사용 중인데, KDE 네이티브 솔루션 아주 기대됨. virt-manager의 Vulkan 렌더링(libvirt) 지원도 기다리고 있음. Kirigami 기반 UI는 마진이 너무 넓게 들어가 답답하게 느껴짐. print-manager의 Kirigami UI에서도 비슷한 경험 있음
     * 예전 aqemu가 제일 좋아하는 프론트엔드였음. 벌써 10년 넘게 관리가 안 되고 아쉬움
          + 개발자가 예전 기능 추가 위해 킥스타터도 했으나, 안타깝게도 실패해서 정체 상태임
"
"https://news.hada.io/topic?id=20988","AI제품의 가격 책정 전략가이드 - 세가지 황금법칙과 실제 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  AI제품의 가격 책정 전략가이드 - 세가지 황금법칙과 실제 모델

     * AI의 가격은 고객이 측정하는 가치 지표와 일치해야 하며, 시간 절약, 수익 향상, 품질 개선, 전환율 상승 같은 비즈니스 메트릭에 기반을 둬야 함
     * 수익은 고객의 성공과 함께 증가해야 하므로, 기본 요금 + 가치 기반 스케일링 모델(하이브리드 모델) 을 권장
     * 예측 가능성과 수익성을 유지하기 위해 사용량 제한, 모니터링 도구, 명확한 정책을 포함한 가드레일이 필수
     * 가격 모델은 크게 사용량 기반, 사용자 수 기반, 결과 기반으로 나뉘며, 각 방식은 서비스 유형과 비용 구조에 따라 달라짐
     * 확장성, 감사 가능성, 지속 가능성, 차별화 가능성을 갖춘 가격 모델이 장기적으로 유리함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

The Three Golden Rules of AI Pricing

     * 1. 고객이 중요하게 여기는 가치 파악
          + 고객이 이미 측정하고 있는 비즈니스 메트릭에 집중
               o 예: 시간 절약, 수익 증대, 품질 향상, 전환율 증가
          + 가격 책정 기준은 이 가치와 직접적으로 연결되어야 함
     * 2. 가치 창출에 따라 수익도 증가
          + 고객이 얻는 가치가 커질수록 기업의 수익도 비례하여 증가
          + 가장 일반적인 방식은 하이브리드 모델
               o 기본 요금 + 사용량/성과 기반 추가 요금
     * 3. 수익성과 예측 가능성을 위한 가드레일 구축
          + 사용량 제한, 모니터링, 명확한 정책으로 고객과 기업 모두를 보호
          + 예측 가능하고 안정적인 수익 구조 제공

AI 가격 책정을 위한 의사결정 트리

   AI가 주로 제공하는 가치         가치 증가 방식              주요 가격 메트릭                    기본 가격 구성
   개인 사용자/팀 활성화   시스템을 사용하는 사람이 많아질수록 가치 증가 Per-user/seat     Subscription + 기능 티어
   처리량/작업량 증가     완료된 작업 수가 많아질수록 가치 증가     Usage/consumption Platform fee + 사용량 계층 요금
   구체적 비즈니스 성과 달성 성공적인 성과 수가 많아질수록 가치 증가    Outcome-based     Platform fee 또는 Subscription + 성과 기반 요금

AI 가치 기반 가격 지표 검증 체크리스트

     * 필수 특성 (Essential Characteristics)
          + Aligned with value : 고객이 성공을 측정하는 방식과 직접 연결되어야 함
          + Acceptable : 잠재 고객에게 직관적이고 쉽게 이해되는 구조여야 함
          + Consumable : 조직의 예산 편성 및 구매 방식과 잘 맞아야 함
          + Predictable : 고객과 공급자 모두가 예측 가능한 지표여야 함
     * 장기적 특성 (Long Term Characteristics)
          + Scalable : 고객의 사용량이 증가함에 따라 자연스럽게 성장할 수 있어야 함
          + Auditable : 명확하게 측정 가능하고 분쟁 없이 검증 가능해야 함
          + Sustainable : 시장과 비용이 변화해도 지속 가능한 가격 지표여야 함
          + Differentiable : 경쟁사와의 차별점을 만들어주는 요소여야 함

하이브리드 가격 모델: 기본 요금 + 가치 확장

     * 1. 기본 요금(Base Component) Subscription 또는 Platform Fee
          + Platform Fee:
               o 입장권 또는 접근 요금
               o 보통 전체 구독보다 낮은 수준
          + Subscription:
               o 기능 및 사용 권한에 대한 반복 결제
               o 정해진 범위의 기능을 포함
     * 2. 가치 확장(Value Scaling) 성장 요소(Growth Driver) 로서 실제 사용 또는 결과에 따라 수익이 증가
          + Per User/Seat:
               o 가치가 사람 수에 따라 증가할 때 적합
               o 예: 팀 단위 협업 툴
          + Usage-Based:
               o 가치가 활동량에 비례할 때 적합
               o 예: API 호출, 처리량 기반 플랫폼
          + Outcome-Based:
               o 가치가 성과 달성에 따라 결정될 때 적합
               o 예: 마케팅 자동화의 전환 수

AI 가격 모델의 기본 컴포넌트 옵션

     * 1. Platform Fee (플랫폼 요금)

     ""플랫폼 접근 자체에 대한 입장권 개념""
          + 추천 대상: 사용량 또는 성과에 따라 가치가 발생하는 제품
          + 주요 특성:
               o 전체 구독 요금보다 낮은 경우가 많음
               o 보통 계정/조직 단위로 부과 (사용자 단위 아님)
               o 기능이 아닌 다른 요소(회사 규모 등)에 따라 계층화 가능
          + 일반 예시:
               o 사용량 기반 API 서비스
               o 검색/질의 기반 데이터 강화 플랫폼
               o 종량제 기반 AI 인프라
          + 실제 사례:
            OpenAI — 기본 플랫폼 접근 요금 + 토큰당 과금
     * 2. Subscription (고정형 구독)

     ""기능 접근에 대한 반복 결제""
          + 추천 대상: 사용량과 무관하게 예측 가능한 가치 전달이 가능한 제품
          + 주요 특성:
               o 예측 가능한 반복 수익 제공
               o 고객이 예산 계획을 세우기 쉬움
               o 보통 핵심 기능 전체 접근 권한 포함
          + 일반 예시:
               o 지식 관리 도구
               o 협업 도구
               o 프로젝트 관리 소프트웨어
          + 실제 사례:
            Notion — 고정 월간 구독 + AI 애드온
     * 3. Tiered Subscription (계층형 구독)

     ""기능 또는 용량에 따라 구분된 여러 요금제""
          + 추천 대상: 고객군이 다양하고 니즈가 다른 제품
          + 주요 특성:ㅊ
               o 고객 성장에 따라 업그레이드 경로 명확
               o 세그먼트 기반 가격 차별화 가능
               o 기능 차이 + 사용량 제한을 혼합할 수 있음
          + 일반 예시:
               o CRM 플랫폼 (기본/프로/엔터프라이즈)
               o 마케팅 자동화 도구
               o 계층화된 기능의 분석 플랫폼
          + 실제 사례:
            Hubspot — 스타터/프로/엔터프라이즈 구독 + 기능 확장

결정 요인

                핵심 질문                     그렇다면 (Yes)           아니라면 (No)
   가치의 대부분이 사용량 또는 결과 발생량에서 발생하는가? 플랫폼 요금 고려                 구독 모델 고려
   사용자당 인프라 또는 지원 비용이 큰가?          플랫폼 요금은 피하고, 사용자 기반 요금 사용 모든 모델 가능
   고객 세그먼트가 다양하고 필요가 상이한가?         계층형 구독 고려                 단순 구독 모델로도 충분
   고객당 AI의 한계 비용이 높거나 예측 불가능한가?    플랫폼 요금 + 사용량 기반 구성 고려     모든 모델 가능
   고객이 예산 예측 가능성을 최우선으로 여기는가?      사용량 제한이 명확한 고정형 구독 추천     사용량 기반 모델 고려
"
"https://news.hada.io/topic?id=20922","뉴욕 혼잡 통행료 도입 이후 변화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           뉴욕 혼잡 통행료 도입 이후 변화

     * 혼잡 통행료 도입 후 맨해튼 중심부 차량 통행량 감소와 교통 속도 증가 현상 발생
     * 버스, 지하철 등 대중교통 이용객 수 증가 및 택시, 자전거, 도보 이동도 증가 기미 나타남
     * 교통사고, 주차 위반, 소음 민원 등 도시 내 부정적 현상은 전반적 감소 추세 보임
     * 현지 상권과 관광 산업에 큰 타격이 없는 것으로 확인, 경제적 영향은 보합 또는 긍정적 관측 나옴
     * 대기 오염 감소 및 저소득층 영향은 평가가 이르며, 정책 지지도 점진적으로 증가 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 전반적 효과

     * 뉴욕 혼잡 통행료(2025년 1월 5일 도입)는 맨해튼 60번가 남쪽 진입 차량에 대당 9달러의 통행료를 부과해 교통 혼잡을 줄이고 대중교통 재원을 확보하기 위해 시작됨
     * 정책 시행 즉시 교통·통근·대중교통·도시 소음 등 다양한 지표에 변화를 유발하며, 특히 통행 속도와 교통량에서 뚜렷한 효과 발생
     * 2025년 3월 기준 4,500만 달러 순수익을 기록하며, 연간 약 5억 달러 재원 조달 예상, 궁극적으로는 150억 달러 규모의 주요 인프라 개선 재원 확보 목표
     * 실제로 거리에서 벌어진 변화는 예정보다 훨씬 빠르게 드러나고 있음

교통 흐름의 변화

  혼잡 구역 내 변화

     * 혼잡 구간 진입 차량 수: 2025년 4월 기준, 하루 평균 약 76,000대(월 230만 대) 감소 관찰, 통상 수준보다 약 12% 감소
     * 회사별·노선별 데이터: Lincoln Tunnel, Holland Tunnel을 통한 입차도 각각 8%, 5% 감소(2024년 1월 대비)
     * 교통 속도 개선: 팬데믹 이후 지속하던 속도 저하가 2025년 1월 이후 반전, 혼잡 구역 내 평균 시속(1~4월)이 12% 증가했으며, 피크 타임엔 최대 20% 이상 상승
     * 로컬 버스 이동 속도: 혼잡 구역 통과 구간에서 버스 속도 3.2% 증가, 특정 노선(B39 등)에서는 최대 34% 향상 기록

  혼잡 구역 외부의 변화

     * 인접 지역 교통: 혼잡 구역 주변 및 나머지 뉴욕 시내에서도 속도 유지 또는 소폭 상승 경향, “풍선효과” 미발생 확인
     * 뉴저지~맨해튼 통근: Lincoln Tunnel 통과 M.T.A 익스프레스 버스 속도 24% 상승, 구글 연구진 분석에선 뉴저지에서 진입하는 차량 역시 약 8% 속도 증가
     * 저소득 지역 거주민 영향: 지역별 소득 수준에 관계없이 혼잡구역 진입 차량 속도 8~9% 동시 상승 분석 결과 있음

대안 교통 수단 이용

     * M.T.A 대중교통 이용량: 2025년 1~4월 기준 버스, 지하철, Metro-North, Long Island Rail Road, Staten Island Railway 모든 수단에서 이용객 4~13% 증가
     * PATH 및 뉴저지 트랜짓: PATH(뉴저지~맨해튼) 열차 이용자 약 6% 상승
     * 택시·공유차량: 혼잡 구역 내 옐로택시 승차건도 약 1백만 건 증가(연초 3개월 비교), 요금 추가에도 수요 유지 중
     * 자전거: Citi Bike 이용 건수도 8~9% 상승했으나, 네트워크 확장·날씨 요인 반영 시 전년 직접 비교에는 한계

파급 및 부가 효과

  교통안전·도시환경

     * 교통사고 및 부상자: 혼잡 구역 내 부상 동반 사고 14% 감소, 부상자 총 인원 15% 감소, 시 전역도 동반 하락
     * 주차 위반 감소: 이중주차·불법주차 등 위반 3.8% 감소, 일부 지역은 소폭 증가
     * 소음 민원: 혼잡 구역 내 차량 소음 관련 311 민원 접수 약 45% 급감, 나머지 지역도 27% 감소
     * 소방 출동 속도: 혼잡 구역 내 소방차 출동 시간 약 3% 향상
     * 구급차·응급 출동: 전체적으로는 출동 시간 증가 지속이나, 혼잡구역 내 증가폭은 더 완만
     * 학교 버스 지각: 혼잡 구역 내 학교 버스 지각율 24%→16%로 감소, 학생 평균 주 30분 이상 추가 수업시간 확보
     * 도시 버스 정시성: 정시 도착 비율 상승, 구역 외 대비 개선폭 뚜렷함

  경제·관광 등 영향

     * 방문객 수: 혼잡 구역 내 주요 비즈니스 지구 방문객 수 1.9% 상승
     * 타임스퀘어 방문객: 연초부터 약 2,150만 명으로, 전년 동기와 거의 동일
     * 브로드웨이·식당: 공연장 좌석 점유율은 전년 수준 유지, OpenTable 기준 식당 예약도 7% 상승
     * 상인 반응: 일각에서 부정적 견해 있으나, 시 전체적으로 뚜렷한 불황 현상은 아직 나타나지 않음

아직은 판단 어려운 영역

     * 환경오염: 미세먼지(PM2.5) 등 대기질 일부 개선 경향 있으나, 이는 장기 관찰 필요
     * 저소득층 충격: 통행료 부담에 따른 직업·활동 제한 등은 시간 경과 관찰 필요, M.T.A는 연소득 5만 달러 이하 운전자에 50% 요금 할인과 세액공제 적용
     * 정책 지지도: 2024년 말 여론조사 기준 30% 초반 지지율이 2025년 3~4월 42%로 상승, 시민 체감 효과 확산에 따른 긍정적 변화 흐름 확인

결론

     * 뉴욕 혼잡 통행료 정책은 교통 혼잡 감소와 대중교통 재원 확보라는 양대 목표에서 가시적인 성과가 관측되며, 운송 안전 및 도시 환경 개선까지 부가 효과가 따라옴
     * 경제·관광 등 부정적 영향은 의미 있게 드러나지 않으며, 시간 경과에 따른 환경 및 사회적 영향은 계속 관찰 중
     * 시민 인식 변화 등 리스크는 남아 있으나, 정책 전환의 초기 긍정적 시그널이 분명히 확인됨

        Hacker News 의견

     * https://archive.ph/NRCcg
     * 이 구역에서의 자동차 속도는 정말 의미심장함을 보여줌, 뉴욕에서는 자동차가 가장 느린 자전거보다도 빠르지 않고, 평범한 러너보다도 느림. 이것만 봐도 이 정책이 옳은 방향을 알 수 있음. 자동차를 위한 인프라와 노력이 실제로 빠르게 이동할 수 있는 것을 허용하지 않음. 시속 9마일은 가장 약하거나 소심한 자전거 이용자에게도 매우 느린 속도
          + 나는 이 정책을 지지함, 그리고 수년간 자전거 출퇴근을 했었음. 하지만 관점의 다양성을 위해 말하자면: 속도만이 사람들이 걷기와 자전거에 비해 자동차를 선호하는 이유는 아님. 어떤 사람들은 이동성에 제한이 있어 자전거나 걷기를 할 수 없지만 운전은 할 수 있음. 자동차는 추위, 더위, 비오는 날씨에서 환경적으로 보호해줌. 주변 사람들과의 접촉을 피할 수 있어 원치 않는 교류가 잦은 계층에게 중요함. 무거운 짐을 들고 다니기에도 훨씬 쉬움. 자동차 사고에 부딪혔을 때 차 안에 있으면 압도적으로 더 안전함. 자전거 통근 옹호자들이 이 부분을 무시하고 자전거가 전반적으로 안전하다고 말하지만, 사고 경험 여부를 물어보면 상당수 사람들이 차에 치이거나 뼈가 부러진 적 있음. 나는 더 많은 사람들이 자전거와 도보를 이용하길 전폭적으로
            지지함. 하지만 최적의 해법은 다양한 교통수단의 조합. 자동차가 나쁜 게 아니라 하나의 요소. (내가 현재 자전거 통근을 하지 않는 이유는 출근길에 웅덩이에 미끄러져 발목이 망가졌기 때문. 치명적이지 않은 자전거 사고 통계는 자세히 보면 꽤 무서움. 전반적인 사망률은 자전거가 더 낮다 하지만 죽지 않아도 정말 힘든 일을 겪을 수 있음)
          + 도시에서 자동차가 얼마나 느린지 놀라움. 최근 나는 20분 운전에서 보행자에게 졌음
          + 뉴욕에 경제적으로 자전거 도로를 고가로 따로 만들 수 있지 않을까 궁금함, 최소한 자동차와 자전거를 분리해서 ""슈퍼하이웨이""를 만들 수 있지 않을까 생각함, 심지어 현수 시스템을 적용해 볼 수도 있음. e-bike와 e-scooter가 있는 지금, 뉴욕은 오히려 지하 터널을 건설해야 한다고 생각함. 목표는 주요 지역들을 평균 15mph 정도의 속도로 연결해 초효율적이고 컴팩트한 이동수단을 가능하게 하는 것. 이것은 엄청난 생활환경 개선 효과를 가져올 것, 아니면 터널을 통째로 올릴 수도 있음
          + 자동차는 느리지만, 보행자에 비해 강도 위험이 적고, 자전거에 비해 차에 치이는 위험도 적음. 자전거는 도난도 자주 일어남. 이런 일들은 드물지만 장기적으로 봤을 때 무시할 수 있는 위험은 아님. 나는 가능한한 차를 피하고 내 유럽 도시에서는 자전거로 출퇴근함. 예전에 뉴욕에 살았는데, 거기선 강도도 당해봤고, 자전거가 자동차에 치여 다치는 것도 봤음. 그래서 거기선 자전거 안 타고 야간에는 택시를 탐. 요약하면 속도만이 유일한 요인은 아님
     * 오랜 뉴욕 거주자로서 팬데믹 동안 도시를 떠났지만 여전히 출퇴근하는 입장. 확실히 길거리 교통량과 소음이 줄어든 걸 느낌. 다른 도시들도 대중교통이 충분하지 않다는 얘기가 많은데, 예를 들어 퀸즈의 플러싱에서 브루클린 8번가로 가는 길에 저렴한 요금의 민간 버스가 있고, 기차보다 절반 시간에 도착함. 뉴저지의 많은 주거 지역에서도 포트 오쏘리티까지 (맨해튼 서쪽 42번가) 민간 버스가 빠르고 편안하게 운영 중. 혼잡 통행료 때문에 그런지 이 배차가 더 빨라진 것 같음. 대부분 공공 교통을 이야기하지만, 민간 버스 서비스가 경쟁을 이룬다면 여러 도시에서 해답이 될 수도 있겠다는 생각이 듦
          + 대중교통의 민영화가 정답으로 여겨지는 현상이 흥미로우면서 놀랍지 않음. 사실 세계 곳곳에선 좋은(혹은 훌륭한) 대중교통이 이미 잘 작동하는 해답. 2년 전 뉴욕 방문했을 때 대중교통 인프라가 매우 열악해서 충격받았음. 특히 도심을 두르는 메트로 순환선이 없다는 점이 가장 놀라웠음. 2025년인데도 대부분 도시들은 백 년 넘은 지하철 네트워크가 잘 갖춰져 있음. 문제는 자동차. 미국은 여전히 자동차 중심 사고를 버리지 못함. 나는 베를린 중심에 살고 있는데, 차를 소유할 유일한 이유가 체면 때문. 그래서 소유하지 않음. 출퇴근 시간엔 차를 타든 대중교통을 타든 시간은 거의 같음. 비출근 시간엔 차가 25-40% 빠를 수 있지만, 주차장 찾고 걷는 시간까지 계산하면 이득이 사라지거나 많아야 25%. 평균 이동 시간은 대중교통으로 30분(역까지 걷는
            시간 포함). 운좋으면 5분 아끼려고 차를 가질 이유가 없음. 한편 자전거 인프라도 계속 개선 중. 많은 골목길이 자전거 도로로 지정되어, 자동차는 거주민이나 배송 등 사유가 있을 때만 진입 가능. 교차로는 아예 자동차 진입을 막는 구조물로 봉쇄해서 자전거만 통과. 큰 길의 자전거도로도 차도와 턱이나 볼라드로 분리되어 있음. 이제는 자동차로 골목길 우회 경로를 쓸 수 없어 Waze도 무용지물. 결과적으로 차량 체증에 걸릴 확률은 오히려 높아짐. 그 와중에 대중교통과 자전거로 빠르고 건강하게 이동할 수 있음. 이런 발상은 새롭지 않음. 타 도시는 자동차 의존을 줄이는 더 많은 방법도 있음. 민간 미니버스라면 내 생각에 최고의 예시는 독일 함부르크·하노버의 ridepooling 서비스(Volkswagen의 Moia)
          + 일본은 거의 전부 민간 버스. 많은 경우 사철기업이 자신의 철도와 연계해 운행하며, 역에서 25분 걸릴 거리가 버스로 10분이고 배차도 편리
          + 모스크바에는 두세 가지 상업 버스가 있음: 시영 노선과는 다르게 운행(지역내/도시간), 도심과 위성도시 연결을 담당. 미니·일반 버스가 다 있고, 시내버스보다 정차도 적고 더 빠름. 지하철이 가능하다면 혼잡·신뢰성 때문에 사람들이 압도적으로 선호
          + 호치민(아마도 베트남 대다수) 도심이 매우 집중됨. 버스는 상당수가 공공-민간 합작. 여러 민간업체가 시내버스를 운행. 예매 시스템은 조금 번거롭고, 업체가 바뀌면 티켓 사용이 불가능. 그래도 자주 타고 있음. 쾌적하고 냉방도 괜찮고 자주 와서 시간표를 체크할 필요가 없음. 어르신·어린이 승하차 도와주는 인력도 있음. 노인 무료, 학생 할인 등 복지도 있음. 승객 적은 노선은 미니버스도 있고, 가끔 밴·트럭형(좌석만 있는 형태)도 봤음
          + 어떤 브랜드/유형 버스를 의미하는지 궁금. 내 도시에서는 그레이하운드, 차이나타운 버스 등이 도시 간 연결은 담당하지만, 주거지~도심 연결은 안 함. 수익성이 떨어지기 때문(자동차 이용이 심하게 보조됨도 한 이유). 이런 점 때문에 대중교통이 존재. 마치 외딴 시골까지 우편을 제공하듯, 사회 이익을 위해 비용 분담이 필요한 것
          + 1980년대 영국도 이 제도를 도입했는데 완전 실패로 돌아갔음. 대부분 시장/지자체가 런던 모델(업체가 운행 일정 맞추는 것에 입찰, 시 전체에서 통합 요금 징수, 버스 표준화 및 임대, 유지 책임은 업체)에 회귀 중. 이런 구조는 민간이 비용 절감에 집중해서 비용은 사유화, 수익(있다면)은 공공화됨. 그래서 시 전체를 아우르는 고정요금제가 가능해지고, 43개 자치구에서 버스 이용이 합리적이며 저렴
          + 런던엔 그런 종류의 민간 미니버스는 없고, 엄청난 수의 대형 버스만 있음. 이들 역시 민간 업체가 운행하지만 TFL(교통국)의 전체 시스템에 맞춰 운영(모두 빨간색, 차체에 업체 이름만 씌여 있음). 이런 방식은 80년대 민영화 이후 바뀐 것으로 보임
          + 홍콩에는 공공·민간 미니버스 모두 있음. 지붕 색깔(녹색/빨강)로 구분
          + 저지 출신으로서 우리는 그런 미니버스를 지트니(jitney)라 불렀음. 다른 도시에선 운영하지 않음
          + 내가 몇 년 전에 확인했을 땐 샌프란시스코에도 지트니가 있었음
          + 폴란드에선 공산주의 붕괴 직후 대중교통 민영화가 시행됐고, 최근 20년간 거의 모두 되돌림. 이유는 서비스 질이 떨어지고(가격경쟁만 함), 가장 수익 좋은 노선만 남기고 시 외곽이나 비수익 구간은 버림. 스케줄 조율도 제대로 안 해서 노선이 몰리거나 텅 비는 경우 잦았음. 결과적으로 공공버스 보조금 더 들어가고, 접근성까지 악화됨. 실질적으론 실패, 따라서 따라하지 말았으면 함
          + 산티아고(칠레)에서도 민간 버스가 있었고, 수익성·고객 만족 모두 성공이었지만 살아남지 못함. 정치적 입지를 충분히 구축하지 못해 결국 국유화됨(사회주의 정치인들에 의해). 그러자 흑자 $6,000만 달러 서비스가 순식간에 $6억 적자로(10배 악화). 서비스 품질도 하락. 버스 운영 적자가 커지자 요금 인상, 요금 인상이 칠레 역사상 가장 큰 폭동 촉발. 이 폭동에서 새로운 사회주의 리더가 나옴. 그는 칠레 전체에서 더 큰 사회주의 물결의 대통령이 됨. 즉, 잘 굴러가던 민간 버스가 사회주의에 의해 공영화, 이후 적자로 곤두박질, 사회 불안/사회주의 확산의 도화선이 됨. 모든 전개가 20년 안에 일어남
          + 마침 우버 발표와 거의 동시에 이 얘기가 나와 신기함. 우버 자체 언급은 없지만 핫 이슈
     * 런던에서 인사함. 도심과 혼잡구역 대부분이 걷기에 훨씬 쾌적함을 느낌. 오염도 훨씬 개선됨: 과거 런던에 와서 지하철을 타면 코를 풀었을 때 검정 콧물이 나왔는데, 최근 몇 년간은 그런 적 없음. 뉴욕도 같은 개선이 있었으면 좋겠음
          + 런던이 더 좋아진 건 맞지만, 지하철에서 나는 여전히 검정 콧물 경험. Victoria 같은 심층노선에서 20분만 타도 겪음. 이건 기차 브레이크 분진 때문이며 차량 배기가스와 상관없음
          + 저소득층 이동량만 현저히 줄고, 고소득층 이동은 그대로였음. 즉, 런던 혼잡통행료로 저소득층이 구역에서 빠져나온 셈
     * 이 기사를 보니 1990년대가 떠오름. 당시 미국이 항상 발전하고 있고 우리는 희망을 느낄 이유가 있다고 믿었던 시기였음
          + 다시 그런 시절을 만들 수 있다는 건 알지만, 도대체 어떻게 해야 할지 정말 답을 찾기 힘듦
     * 통행료가 생기면 비용을 낼 수 있는 사람에게는 속도 증가가 큰 장점. 이것은 전 세계적으로 유료도로의 보편적 장점
          + 대중교통·자전거길 투자는 그만큼 비용을 감당 못 하는 사람에게 대단함. 이런 순수한 상생 효과는 드물
          + 이것은 단순한 정당화 그 이상. 유료도로에서 돈을 내고도 막히면 진짜 '길거리 강탈'. 뉴욕에서 차량 흐름이 개선됐다는 건 통행료의 기본 계약이 지켜진 것이며 모두가 자신의 몫을 찾은 것
          + 속도 증가로 이익 보는 건 통행료를 낼 수 있는 사람뿐 아니라 택시 승객, 응급차, 낮에 배달을 시키는 누구에게도 해당
          + 자동차를 소유할 수 없는 대중교통 승객에게도 큰 효과
          + 최고의 결정은 아예 개인 차량 이동을 금지하는 것. 지금은 길이란 공공 공간이 추가 요금을 내는 계층 차지로 변함. 개인 차량을 전면 금지하고 공원, 보행자 도로로 바꿔야 함
          + 버스 이용자들에게도 그만큼 효과적(직접 경험)
          + 해당 구역을 지나는 모든 도로 기반 대중교통도 그만큼 이득을 얻음
          + 그런데 기사에 따르면 대중적 평가는 좋지 않음(점점 나아지고 있지만). 그럼 왜 대다수는 만족스럽지 않을까?
          + ""보편적""이란 표현은 조금 어색
          + 통행료가 생겨서 속도가 좋아진 건 사실인 듯(개인 경험상) 하지만 이 부담은 철도 접근성이 떨어져 자동차가 필요했던 저소득 통근자에게 가장 크고, 돈은 기차(공사)에 들어감. 당연히 이들 입장에서는 불만족스러움
          + 부유층은 대중과 달리 이동 자체가 덜 필요
     * 한때 뉴욕에 자동차가 전혀 없던 시절이 있었지만 대중교통은 넘쳐났음(물론 말이 있었고 말똥도 문제였고 인구도 적었음)
          + 말은 단순히 오물만의 문제가 아님. 1900년대 말 사고로 인한 보행자 사망률이 2003년 자동차 사고 보행자 사망률보다 높았음
          + 1929년 IND Second System(지하철 확장) 계획을 보면 대중교통은 더 많을 수도 있었음. 이미 준비하고 일부 공사된 구간도 있음
          + 맨해튼 인구는 1910년대가 정점이었음
     * 혼잡통행료의 재미있는 점은, 혼잡구역을 벗어나지만 않으면 맨해튼에서는 여전히 무료로 자동차를 보관할 수 있다는 것. 맨해튼에서 자동차를 보유한다는 것은 대부분의 사람이 평생 가질 수 없는 슈퍼파워에 가까움. 혼잡통행료 이후 이 조건은 더 좋아진 셈
          + 맨해튼에서 주차 문제(비싼 주차비든 긴 시간 빙빙 도는 일이든)를 슈퍼파워라 부르고 싶진 않음
          + 슈퍼파워가 도대체 뭔지 의문. 일거수일투족 위치를 관리하거나 고액 월주차비를 내야 하는 쇳덩이를 소유하는 것? 자전거 타봤는지? 지하철이나, 그도 싫으면 택시도 있음. 굳이 왜 맨해튼에서 차를 소유해야 함? 시민들과의 접촉을 그렇게 싫어함?
     * 이 세금의 유일한 문제는 MTA가 새로운 수입을 제대로 사용할 능력이 있냐는 점. 실제로 여행에 장벽을 추가한 것(이건 세금). 일부 긍정적 외부효과가 있긴 하지만(내게도 해당), 그 세금 수익이 제대로 쓰일지는 MTA의 실적을 보면 의문. (물론 재정 부족이 맞긴 하나, 모든 것이 ""이상적"" 비용의 10배)
     * 지하철 범죄는 어떨까, 혼잡통행료로 사람들이 더 타면 영향이 있나 궁금
          + 자동차 신호위반으로 자전거와 보행자가 치이는 범죄는 어떨까?
          + 범죄는 감소 추세, 사실 팬데믹 이후 계속 줄고 있음. 실제로 지하철 범죄율은 원래 그리 높지 않았음. 일부 대형사건만 전국적으로 보도됐지만, 실제 수백만 건의 일상 승차 속에서 피해자가 될 가능성은 언제나 매우 낮았음. 이곳 주민 상당수는 원래 크게 걱정한 적도 없음. 뉴욕에선 초등생/중학생들이 혼자 지하철 타고 다니는 것도 일상
"
"https://news.hada.io/topic?id=20990","때론 절대 가격으로 경쟁하지 말아야 함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         때론 절대 가격으로 경쟁하지 말아야 함

     * 단순히 가격이 저렴하다는 이유만으로 승부하면 결국 모두가 파괴적인 가격 경쟁 속에서 고사함
     * 하지만 전략적으로 약점을 의도적으로 선택하고, 이를 통해 구조적 강점을 만든다면 저렴한 가격도 강력한 무기가 될 수 있음
     * Costco, IKEA, Southwest, Vanguard는 모두 특정 고객층에게만 최적화된 제품을 제공하며, 불편한 약점과 제약을 일부러 선택함으로써 복제 불가능한 시스템을 구축함
     * SaaS든 오프라인 기업이든, 수익 구조가 제품 단위에서 이미 성립되어야 하며, 확장을 위한 외부 자금 없이도 성장 가능한 전략을 갖춰야 함
     * “저렴함”이 단순한 전술이 아닌, 전사적 전략의 결과일 때만 경쟁력이 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“가격으로 경쟁하지 마세요”

     * 다들 그렇게 말하지만, 고객들은 낮은 가격을 원하며 Costco, IKEA, Amazon, H&M 등은 낮은 가격 전략으로 성공했음
     * 그럼 왜 낮은 가격으로 승부해선 안된다고 할까?
     * 동일한 고객군을 동일한 방식으로 해결하는 유사 제품들은 결국 가격 외에 차별점이 없음
     * 가격을 낮추면 상대도 따라하고, 결국 마진이 소멸되며 모두 개발, 마케팅, 고객 지원 등에 투자할 여력을 잃음
     * 예: 20% 가격 인하는 25% 고객 증가가 필요하며, 이윤 기준으로는 10% 인하 = 50% 이익 감소
     * 이 계산식은 나를 포함한 많은 전문가들이 부트스트랩 창업자들에게 ""가격을 올리라"" 고 잔소리하는 이유임

     ""낮은 가격은 재투자할 여력도, 이익을 남길 방법도 없음""

     * 새 창업자들은 제품이 아직 미완성이라고 느끼기 때문에 자신감 부족으로 낮은 가격을 책정함
     * 가격을 높이기보다 “싼 맛에 써보는 고객” 만 유치하고, 결과적으로 서포트 비용만 증가함
     * 결국 퇴근도 못하고, 성장도 없고, 투자 여력도 없음

     * 그럼 ""가격으로 경쟁하지 마세요""가 진리인걸까?
     * 아님. 가장 낮은 가격을 제시하는 건 훌륭한 전략이 될 수 있음
     * 제프 베조스는 ""당신의 마진은 나의 기회다"" 라는 유명한 말을 했음. 경쟁사가 이익을 챙길때, 아마존은 더 싸게 판매함
          + 경쟁사가 가격을 고수하면 시장 점유율을 빼앗기고, 가격을 낮추면 아마존이 경쟁사의 이익을 파괴함
     * 근데 왜 아마존에게는 현명한 이 전략이, 우리한테는 어리석은 선택일까?

완벽한 전략으로서의 저렴한 가격

     * 핵심 조건은 바로 이것 가격은 전략의 부산물이어야 함
     * 가격을 낮추는 것이 최후의 수단이 아닌, 전략적 구조 결정의 결과여야 함
     * 약점을 전략적으로 받아들이고, 그로 인해 경쟁사가 모방할 수 없는 강점을 확보하는 방식
     * 약점을 특별한 강점으로 이끄는 상호 연결된 저가 전략 사례들
          + Costco
               o 대용량 포장 + 고품질 조합으로 단위 가격을 낮춤
               o 매장 인테리어 단순화, 상품 종류 제한, 회원제 수익 구조 등 상호 강화되는 전략
               o 14% 마진 상한 정책, 직원 복지 강화 등도 독보적 구조의 일환
          + Southwest Airlines
               o 단일 기종 항공기, 짧은 거리 중심 루트, 2차 공항 사용 등으로 운영비 절감
               o 수하물 연계 불가, 비즈니스 클래스 없음 등 불편함을 감수한 구조
               o 타 항공사들이 모방했지만 성공 못함 → 약점을 감수하지 않았기 때문
          + Vanguard
               o 펀드 매니저 없는 인덱스 펀드로 2% 수수료 제거
               o 일부 소비자는 “진짜 전문가가 없는 펀드”라며 외면했지만, 장기 수익률+수수료 고려 시 오히려 우위
               o 구조적으로 낮은 비용이 핵심 강점
          + IKEA
               o 셀프 조립이라는 불편한 선택이 물류, 진열, 인건비 절감의 핵심 열쇠
               o 전 세계 공통 제품군, 쇼룸 경로 최적화, 낮은 품질의 합리화 모두 하나의 전략 구조
               o “가구를 싸게 사고 싶은 사람”에게는 완벽한 가치 제공

Low-cost 전략은 많은 자금조달이 필요하지 않음

     * 최근의 SaaS Playbook은 ""저가""전략을 실행하려면 꽤 큰 규모의 VC 펀딩을 유치해야한다고 이야기함
          + 이는 논리적임. 저가 비즈니스 모델은 규모를 확장해야만 수익을 낼수 있음
     * 하지만 위의 모든 기업은 적은 초기 자본으로 수익성 구조를 빠르게 확보했음
          + Costco: 첫 매장 비용 $5M 였고, $30M의 소액 공모로 IPO. 멤버십 회비 수입으로 대부분의 고정 비용 충당하며, 극히 낮은 제품 마진으로 사업 운영
          + Southwest: 초기 투자 $560K, 3개 도시에서 시작. 빠르게 수익을 냈기에 외부 자금 없이 사업 확장
          + Vanguard: 초기 자본 $2M, 인건비 거의 없음. 최소한의 운영경비만 필요
          + IKEA: 자금 대부분이 사업 수익에서 자체 조달. 전세계 확장도 주로 수익 재투자를 통해 이루어짐
     * 이들은 자재, 공급망, 재고, 건물을 갖춘 물리적 기업들임. 소프트웨어 회사도 이보다 더 많은 투자 없이 동일한 서비스를 제공할 수 있어야 함

성공적 저가 전략의 공통 요소

    1. 많은 고객이 싫어할 수 있는 약점을 전략적으로 수용하고 시장의 일부 계층을 타겟팅
    2. 그 약점 덕분에 경쟁사 복제가 불가능한 구조 형성. 경쟁사는 강점은 모방하고 싶어하지만, 약점은 모방하지 않음. 그러나 약점은 강점을 만들어내는데 필수적
    3. 제품 단위에서 수익성 확보 (SaaS라면 70% 이상의 Gross Profit Margin, 1년 미만의 짧은 CAC 회수 기간 등)
    4. 이익을 배당이나 소비하는 대신 사업 확장에 재투자
    5. 운영 혁신에 집중 (단가 절감, 효율화 등)
    6. 좁은 타겟 시장으로 시작해 점진적 확장
    7. 가치 기반의 장기 전략 유지, 단기 편의 타협 없음

결론

     * 가격이 유일한 차별점이라면, 그것은 전략이 아님. 그냥 Commoditization과 가격 경쟁을 초래할 뿐

     그건 그냥 값싼(cheap)거지, 적당한(affordable) 가격이 아님. 비전이 부족한거지, 치밀한 전략이 아님
     * 낮은 가격이 사업구조, 비용구조, 제품간 상충, 경쟁자가 내리지 못하거나/안하거나 하는 결정의 결과일때, 그것은 강력하고 승리하는 전략이 됨

     * 저렴함을 위해 싸지는 것이 아니라, 싸게 만들어도 남는 구조를 설계해야 함
     * 저가 전략이란 “싸다”는 말이 아니라, 싸게 팔고도 이익을 남길 수 있도록 설계된 비즈니스 모델일 때만 통함

   이건 불가능하거나 대부분의 사례에 실현불가능한 이야기임.

   IT분야 대부분이 인건비와 인력부대비용이 높고, SW의 경우 더더욱 그러함.
   해당 글의 사례(Amazon, Costco, Vanguard, IKEA)는 대부분 인건비를 절감하는 방향으로 비용을 줄이고, (감당한 가능한 불편함을) 고객에게 저렴한 요금에 대한 반대급부로 스스로 감당하도록 한 것임.

   IT분야에서는 그러면 대체로 망하거나, Amazon/쿠팡 처럼 직원을 혹사하거나 SPC처럼 직원을 죽이는 구조가 되는 경우가 대부분이라고 알고 있음.

   물론, 재투자가능한 축적가능한 이익(+적정 품질 유지하면서)을 내는 한에서는 전략적으로 최대한 저비용으로 가야한다는 점에는 원론적으로 동의.

   낮은 가격은 경쟁사의 진입을 봉쇄하고, 소비자의 우선고려대상이 될 수 있는 중요한 요소임.
"
"https://news.hada.io/topic?id=20933","나는 NumPy가 싫어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             나는 NumPy가 싫어요

     * 필자는 NumPy에 대한 불만을 주제로 여러 예시와 함께 문제점을 설명함
     * 간단한 배열 연산은 NumPy로 쉽지만, 차원이 늘어나면 복잡성과 혼란이 급격히 증가함
     * 브로드캐스팅과 고급 인덱싱 등 NumPy의 설계는 명확성과 추상화 측면에서 부족함
     * 명시적으로 축을 지정하는 대신 추측과 시행착오에 의존하는 코드 작성이 필수임
     * 개선된 배열 언어에 대한 아이디어를 제시하며 구체적인 대안을 다음 글에서 소개할 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: NumPy에 대한 애증

     * 필자는 오랜 기간 NumPy을 사용해왔지만, 그 한계에 많이 실망했음을 밝힘
     * NumPy는 파이썬에서 배열 연산을 위한 필수적이고 영향력 있는 라이브러리임
     * PyTorch 등의 현대 머신러닝 라이브러리에도 NumPy와 유사한 문제들이 존재함

NumPy의 쉬운 점과 어려운 점

     * 기본적인 선형 방정식 풀이와 같은 간단한 연산은 명확하고 우아한 문법으로 가능함
     * 그러나 배열 차원이 높아지거나 연산이 복잡해지면, for 루프 없이 일괄 처리가 필요해짐
     * 루프를 쓰지 못하는 환경(GPU 연산 등)에서는 특이한 벡터화 문법이나 특별한 함수 호출 방식이 필요함
     * 하지만 이러한 함수들의 정확한 사용법이 모호하고 문서만으로도 명확하게 알기 어려움
     * 실제로 numpy의 linalg.solve 함수는 고차원 배열인 경우 어떻게 써야 올바른지 누구도 확신하기 힘듦

NumPy의 문제점

     * NumPy는 다차원 배열의 일부 또는 특정 축에 연산을 적용하는 데 일관된 이론이 부족함
     * 배열 차원이 2 이하일 때는 명확하지만, 3차원 이상에서는 각 배열마다 연산 대상 축의 지정이 불분명함
     * 명시적으로 차원을 맞추기 위해 None 사용, 브로드캐스팅, np.tensordot 등 복잡한 방법을 강제함
     * 이러한 방식은 실수 유발, 코드 가독성 저하, 버그의 가능성 증가를 초래함

반복문과 명확성

     * 실제로 반복문을 허용한다면 더욱 간결하고 명확한 코드 작성이 가능함
     * 반복문 코드가 덜 세련돼 보일 수 있으나, 명확성 측면에서는 큰 장점이 있음
     * 반면, 배열 차원이 바뀌면 transpose나 축 순서를 일일이 고민해야 하고, 복잡성이 증가함

np.einsum: 예외적으로 좋은 함수

     * np.einsum은 축의 이름을 지정할 수 있는 유연한 도메인 특화 언어를 제공하여 강력함
     * einsum은 연산의 의도가 명확하고 일반화도 뛰어나, 복잡한 축 연산을 명시적으로 구현 가능함
     * 하지만 einsum과 유사한 방식의 연산 지원이 일부 연산에 한정되고, 예를 들어 linalg.solve에는 못 씀

브로드캐스팅의 문제점

     * NumPy의 핵심 트릭인 브로드캐스팅은 차원이 안 맞을 때 자동으로 맞춰주는 기능임
     * 간단한 경우에는 편리하지만, 실제로는 차원을 명확하게 알기 어렵게 만들고, 오류 사례가 많음
     * 브로드캐스팅이 암묵적이라 코드를 읽을 때 매번 연산이 어떻게 작동하는지 확인해야 함

인덱싱의 불명확함

     * NumPy의 고급 인덱싱은 배열 shape 예측이 매우 어렵고 불명확함
     * 다양한 인덱싱 조합에 따라 결과 배열의 shape가 달라지므로, 실제로 다뤄본 경험 없이는 예측이 곤란함
     * 인덱싱 규칙 설명 문서도 길고 복잡하여, 익히는 데 큰 시간 소모를 유발함
     * 단순 인덱싱만 쓰려고 해도 특정 연산에서는 어쩔 수 없이 고급 인덱싱을 사용하게 됨

NumPy 함수 설계의 한계

     * 많은 NumPy 함수들은 특정 배열 shape에만 최적화되어 있음
     * 고차원 배열에는 추가적인 axes 인자, 별도 함수명, 관례를 사용해야 하고, 함수마다 일관성이 없음
     * 추상화와 재사용이 기본인 프로그래밍 원칙에 역행하는 구조임
     * 특정 문제를 해결하는 함수를 써도, 다양한 배열과 축에 재적용하려면 아예 다른 코드로 다시 작성해야 함

실제 예시: self-attention 구현

     * self-attention 구현을 NumPy로 작성할 때, 반복문을 쓰면 명확하나, 벡터화를 강제하면 코드가 복잡해짐
     * 다중 헤드 attention과 같이 고차원 연산이 필요할 때, einsum과 축 변환을 복합적으로 써야 하고 코드가 난해해짐

결론 및 대안

     * 필자는 NumPy가 ""다른 배열 언어들보다 나쁜 점이 많지만 그만큼 시장에서 중요해진 유일한 선택지""임을 밝힘
     * NumPy의 여러 문제점(브로드캐스팅, 인덱싱 불명확성, 함수의 비일관성 등)을 극복하기 위해 개선된 배열 언어의 프로토타입을 만들었음을 예고함
     * 구체적인 개선안(새로운 배열 언어 API)은 추후 별도의 글에서 소개할 계획임

   Julia가 왜 탄생했는지 이야기 같네요. 라이브러리들을 공부해야 하지만, Numpy의 많은 문제들을 해결해준다는 점에서 정말 매력적인 선택지 같습니다.

   numpy 는 vectorization 이거 잘 사용 못하면 성능은 망하죠. 그런거 고려해서 작성하는게 스트레스고 어렵죠.

   좀 오래된 파이썬 라이브러리들은 다 비슷한 문제가 있는거 같아요

        Hacker News 의견

     * 첫 번째 예시에서 b의 타입만 보고 문서를 보면 읽기 어렵지만, 반환되는 shape에 대한 설명이 있으므로 실제로 b 벡터가 행렬 형태인지(특히 K=1인 경우) 확인해야 함
     * 배열의 차원이 2개가 넘으면 Numpy 배열에 차원 이름을 추가해주는 Xarray를 쓰는 것을 추천함, 차원 맞추기나 transpose 작업 없이 브로드캐스팅/정렬이 자동이라 이런 문제의 대부분이 해결됨, Xarray는 선형대수 측면에서는 NumPy보다 약하지만 쉽게 NumPy로 돌아갈 수 있고 도우미 함수만 만들면 됨, Xarray를 쓰면 3차원 이상의 데이터 다룰 때 생산성이 크게 높아짐임
          + Xarray는 Pandas와 NumPy의 장점을 합친 느낌임, da.sel(x=some_x).isel(t=-1).mean([""y"", ""z""]) 같은 인덱싱이 쉽고, 차원 이름이 존중되어 브로드캐스팅도 명확함, 여러 CRSs의 지리공간 데이터 처리에 강점이 있음, Arviz와의 활용도 탁월해서 베이지안 분석에서 추가 차원 처리도 쉬움, 여러 배열을 하나의 dataset에 묶어서 공통된 좌표도 공유 가능해 ds.isel(t=-1)처럼 시간 축을 가진 모든 배열에 쉽게 동작시킬 수 있음
          + Xarray 덕분에 초보적인 NumPy 사용이 많이 줄고 훨씬 생산성이 오름
          + Tensorflow, Keras, Pytorch 같은 프레임워크엔 비슷한 게 있는지 궁금함, 예전에 언급한 내용을 어렵게 디버깅했던 기억이 있음
          + 소개 고맙고 꼭 써볼 계획임, array[:, :, None] 같은 문법이 불편했던 건 자신만 그런 줄 알았는데 같은 의견이라 반가움
          + biosignal 분야에선 NeuroPype가 NumPy 위에서 n차원 텐서용 이름 붙은 축 지원과 각 축별로 per-element 데이터(채널명, 위치 등) 저장 가능함
          + NumPy가 예전 Numeric과 Numarray 라이브러리에서 파생되던 시점이 떠오름, Numarray 파가 20년 동안 계속 주장을 이어오다 자금을 받고 Xarray로 이름을 바꿔 이제 NumPy를 이겼다고 상상해봄(물론 대부분 허구임)
     * Julia를 쓰기 시작한 이유 중 하나는 NumPy 문법이 너무 어려웠기 때문임, MATLAB에서 NumPy로 넘어가니 프로그래밍이 더 서툴러져서 수학보다 성능 트릭을 익히는 데 시간을 썼음, Julia에선 벡터화도 루프도 잘 동작해 코드 가독성에만 신경 쓸 수 있음, 이런 경험과 감정을 글에서 그대로 느꼈음, np.linalg.solve 같은 걸 최고로 빠르다고 생각해서 무조건 맞춰 쓰라는 식의 ‘블랙박스’ 접근은 옳지 않다고 봄, 문제 특화 커널을 직접 짜는 게 더 나은 여러 이유도 존재함
          + Julia가 과학 계산용으로 설계된 언어이고 NumPy는 과학 계산용이 아닌 언어 위에 억지로 얹힌 라이브러리라는 점이 원인임, 언젠가 Julia가 승리해서 네트워크 효과 때문에 파이썬을 쓰는 사람들이 해방되길 바람
          + MATLAB도 벡터화 없이 루프 돌리면 Python만큼 느림, Python의 느림이 가장 큰 문제이고 Julia는 분명 장점이 있지만 실제로는 급격하게 한정적인 용도에 밖에 못 씀, Python엔 JIT hack 같은 게 생겼지만 여전히 불완전함, Python 대안이 절실함
          + MATLAB이 정말 다를까? 루프가 느린 것은 변함없고, 가장 빠른 건 '' 연산자처럼 완전히 최적화된 블랙박스임
          + Fortran 최신 버전도 Julia처럼 벡터화와 루프 모두 빠르게 동작하므로 가독성에만 집중 가능함
     * Matlab, Julia 대비 numpy의 불만을 정리하면, 함수마다 축 관련 인자와 네이밍, 벡터화 제공 방식이 제각각이고, 어떤 축에 함수 적용하려면 코드를 완전히 다시 써야 한다는 점임, 프로그래밍의 기본이 추상화인데 NumPy는 이를 어렵게 함, Matlab에선 벡터화 코드는 거의 그대로 돌아가거나 수정이 명확하지만 NumPy는 항상 문서를 뒤져보고 transpose/reshape 등 타입 맞추기가 일관되지 않아 애매함
          + Matlab의 3차원 이상 배열 지원이 너무 약해 오히려 글에서 언급된 문제는 잘 생기지 않음
          + 두 번째 문제는 jax의 vmap을 시도해볼 만함
          + 2x2 배열에 특정 함수 작성 후 3x2x2 배열의 일부에 적용하고 싶다는 건 슬라이스와 squeeze 등으로 가능함, 이 문제 자체가 이해가 안 갈 정도로 애매함
          + reshape로 처리 가능함
     * numpy에서 가장 혼란스러운 점은 어떤 연산이 벡터화되어 동작하나 명확하지 않고, Julia처럼 dot 문법으로 명시할 수 없다는 것임, 반환 타입 관련해서도 다양한 함정이 많음, 예를 들어 poly1d 객체 P를 오른쪽에서 z0로 곱하면 poly1d가 나오지만 왼쪽에서 z0*P 형태로 곱하면 배열만 반환되어 타입 변환이 조용히 일어남, quadratic의 leading coefficient도 P.coef[0]과 P[2] 두 방식이 가능해서 혼란하기 쉬움, 공식적으로 poly1d는 ‘오래된’ API이고 새 코드는 Polynomial 클래스를 권장하지만 실제로는 deprecated 경고도 없음, 이런 타입 변환과 데이터타입의 불일치처럼 라이브러리 곳곳에 지뢰가 있어서 디버깅 악몽임
     * 저자가 지적한 내용에 공감함, Matlab에서 Numpy로 갈 때 불편함이 많았고, 데이터 슬라이싱도 Numpy가 Matlab/Julia보다 더 불편하다고 느낌, 하지만 Matlab의 toolbox 라이선스 비용을 감안하면 Numpy의 단점이 커버됨, 글에서 제시한 문제는 2차원 초과 텐서에서 주로 발생하며 Numpy는 원래 행렬(2D) 기반이니 그 한계가 당연함, Torch 같은 전용 라이브러리가 낫지만 그것도 쉽진 않음, 결국 ""NumPy는 다른 어떤 array 언어들보다 약간 더 별로지만 그렇다고 쓸 수 있는 게 별로 없음""이 정답인 느낌임
          + Numpy는 최초부터 N차원 배열을 목표로 numarray의 연장선에 있었으므로 2D에만 머문 것은 아님
     * Python 데이터 사이언스 생태계의 가장 큰 문제는 모든 것이 비표준임, 10여 개의 라이브러리가 4개 언어만큼 각기 다르게 동작하고 그나마 to_numpy() 정도만 통일됨, 결국 문제를 푸는 시간보다 데이터 포맷 변환에 시간이 더 소모됨, Julia도 장점만 있는 건 아니지만 단위 및 불확실성 등 다양한 라이브러리 간 연동이 잘 되고 Python은 항상 보일러플레이트 코드가 많이 필요함
          + array-api 프로젝트가 Python 생태계 전체에서 배열 조작 API를 표준화하려 노력 중임
          + R은 오히려 4개의 클래스 시스템 때문에 더 복잡함
     * 사람들이 왜 sage 대신 numpy를 쓰는지 궁금함
     * 일부 문제는 numpysane와 gnuplotlib을 쓰면 해결됨, 이 조합이 생긴 후 numpy를 모든 작업에 적극 활용함, 없었다면 도저히 못 쓸 수준임
          + numpysane는 결국 파이썬 루프임, 실제 벡터화와는 다름
          + 소개 고맙고, 이런 문제로 종종 투덜거렸기에 간단한 상위 라이브러리가 있단 생각을 못 했음
     * vectorized multi-head attention을 위해 모든 행렬곱을 einsum에 넣고 optimize=""optimal""로 matrix chain 곱 알고리즘을 써서 성능을 올려보았음, 실제로 일반 벡터화 구현 대비 2배 정도 빨라지긴 했으나 놀랍게도 루프 방식 순진구현이 더 빠름, 이유가 궁금한 사람은 코드를 참고 바람, einsum 내부의 cache coherency가 더 개선될 여지가 있다고 추정함
"
"https://news.hada.io/topic?id=20944","Ask GN: 달착륙 음모론으로 내기를 한다면 얼마까지 거실수 있으신가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Ask GN: 달착륙 음모론으로 내기를 한다면 얼마까지 거실수 있으신가요?

   아폴로 프로젝트에 이어서 다시 한번 사람을 달로 보내려는 프로젝트인 아르테미스 프로젝트가 진행중에 있는건 다들 아실겁니다. 재미삼아 본인의 확신 정도를 금액으로 환산해보는 놀이를 해보면 어떻까 해서 Ask에 올려봅니다.

   개인적으로 친구와 20만원을 걸고 아르테미스 프로젝트가 성공할 것인지 실패할 것인지 내기를 한적이 있습니다. 실패할 거라는데 20만원을 건 제가 이겨서 얼마전 약간의 용돈을 확보했고 이걸 아래 블로그에 적어두었습니다.
   https://blog.naver.com/conanoc/223839809999

   위 블로그에 적힌것처럼 아르테미스 2단계는 2번 연기가 되어서 내년 4월에 진행될 예정입니다. 달 착륙 까지는 아니고, 사람을 태운 우주선이 달 주변을 몇 바퀴 돈 후에 다시 지구로 복귀하는 것입니다.

   만약 내년에 예정된 아르테미스 2단계의 성공 여부로 내기를 한다면 여러분은 성공/실패 중 어디에 얼마까지 걸 의향이 있으신가요?
   제 경우는 달 착륙 음모론을 믿고 있고, 이 연장선 상에서 아르테미스 프로젝트도 실패할 거라고 믿고 있지만 막상 이 확신을 돈으로 환산하려하니 생각보다 확신의 정도가 크지는 않구나 라는걸 느꼈는데요. 여러분이 막연히 가지고 있는 확신도 한번 측정해 보면 어떨까 합니다. 재미로요.

   내기 금액을 정하는게 중요한 부분인데 금액을 적은 분이 안계시네요.
   확신을 돈으로 환산해보는게 어쩌면 ""알고 있다는 착각""을 벗어날수 있는 방법 중의 하나가 아닐까 싶습니다.
   https://www.yes24.com/product/goods/111377677

   저도 아폴로 프로젝트라면 전재산 꼬라박습니다.
   달착률이 음모였고 그걸 까발렸을때 실질적으로 가장 큰 이득을 볼 냉전시대 소련이 침묵했는데
   음모론자의 음모를 믿는건 그냥 재미 수준이죠.

   아폴로 프로젝트라면
   전재산 꼬라박아야죠

   아르테미스도 결국엔 성공할거라 생각합니다. 실패의 기준이 리트라이까지 포함한다면 모르겠지만요

   재미있는 내기 입니다¹알테미스2단계 계획하에 발사된 로켓이기만하면 시점은 관계없는가²로켓이 달주위를 공전했다는 사실검증은 어떻게 할것인가³탑승인원의 몇%이상의 사람이 생환해야 하는가.등등 ""어떤 시도의 성패를 따진다는것""에 대한 고찰

   이전의 기술이 의외로 유실된게 많기도 하고, 우주항공 연구쪽은 엄청 엄밀하게 준비하는데도 생각보다 우주발사체 실패하는 경우가 많아서, 다음 단계가 실패할 가능성도 저는 작지 않다 봅니다. 그래서 내기를 할 생각은 없고. 다만 착륙 자체는 돈과 시간 좀 들이면 그리 멀지 않은 미래에 무조건 성공하긴 할겁니다.
"
"https://news.hada.io/topic?id=20991","SKT 민관합동 조사단, 침해사고 2차 조사결과 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     SKT 민관합동 조사단, 침해사고 2차 조사결과 발표

     * SKT의 리눅스 서버 약 3만여대 조사, 감염서버 총 23대, 악성코드 총 25종(BPFDoor계열 24종 + 웹셸 1종) 확인
     * 유출된 유심정보 규모가 9.82GB이며, 가입자 식별키(IMSI) 기준 26,957,749건
     * 정밀 분석한 15대 서버중 2대는 통합고객인증 서버와 연동되는 서버들로 고객 인증을 목적으로 호출된 단말기 고유식별번호(IMEI) 와 다수의 개인정보(이름, 생년월일, 전화번호, 이메일 등) 을 임시로 저장하는 서버
          + 해당 서버의 임시 저장된 파일에 총 291,831건의 단말기 고유식별번호(IMEI)가 포함된 사실을 확인
     * 방화벽 접속로그 기록이 남아있는 기간(’24.12.3.∼’25.4.24.)에는 자료유출이 없었으며, 최초 악성코드가 설치된 시점부터 접속로그 기록이 남아있지 않은 기간(’22.6.15.∼’24.12.2.)의 자료 유출 여부가 현재까지는 확인되지 않음

   웹셀까지 크으... 이게 기간통신망 사업자의 위엄?

   직장이 해외에 있어 국외에 있는 상황인데 통신사랑 기기를 바꾸러 한국에 가야한다는게 참... 짜증나네요.

   이래놓고 몇 달 뒤 버젓이 1등 유지할게 뻔해서 허망하네요

   새폰 사고 통신사 변경 필수. 이걸 누구돈으로 해야되나

   대기업 주제에 보안 프로그램 설치도 안 하고 이게 무슨 멍청한 짓입니까...

   기형적인 독점구조로 영익 1조 이상 찍었으면 기본은 했어야하는데 이게 뭡니까 진짜

   정말 이게 다른 기업도 아니고 SKT에서 발생할 수 있는 사고인가요. 대기업 서버 관리에 얼마나 원가 절감을 하고 무신경하게 다뤘으면 이지경까지 갈 수 있는건지...

   진짜 괘씸하죠. 과실은 우리 과실인데, 우리도 살아야 되니깐 책임은 안질려고. 가시더라도 위약금들은 내주세요.

   와.. 단군이래 사상 최초이자 최고 기록이겠네요.. 하.. 아직 좀 남았는데.. 위약금 면제 안되냐!

   CIS 벤치마크만 지켰더라도....

   웹쉘이면 어서오십셔 하는거 아닙니까.....

   이랏샤이마세~~

   대한민국 대표 통신사로써.. 이제 역사속으로 사라져야할 정도의 운영이네요. ㄷㄷ

   심각하네요;;

   3년동안 백도어를 몰랐던거네요. BPF계열이라 뭐가 털렸는지도 모르는 상황일꺼고.. SK 사용자는 빨리 벗어나는게 답인듯
"
"https://news.hada.io/topic?id=20955","LLM으로 몇 달간 코딩한 후, 다시 내 두뇌를 쓰기로 했어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   LLM으로 몇 달간 코딩한 후, 다시 내 두뇌를 쓰기로 했어요

     * 고성능 LLM을 활용하여 인프라를 구축했으나, 코드 품질과 유지보수성에서 큰 문제점이 드러남
     * AI의 비효율성과 일관성 없는 결과 때문에 직접 코드 이해와 조사, 역량 강화 필요성을 느낌
     * 프로젝트를 빠르게 끝내려는 목적이 오히려 코드 구조의 혼란, 중복, 비일관성을 초래함
     * 이제는 AI를 단순 반복작업이나 코드 변환 등 보조 용도로만 제한적으로 활용함
     * AI 활용이 코딩 감각과 문제해결 능력 저하로 이어질 수 있으므로, 적극적으로 두뇌 사용을 우선시함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: LLM을 활용한 인프라 구축 시도

     * 기존 PHP+MySQL 인프라가 한계에 이르러 새로운 인프라 필요성 인식
     * Go와 Clickhouse 선택, AI와 함께 인프라 설계 및 계획 수립 진행
     * Claude 등 LLM에게 베스트 프랙티스 및 아키텍처에 대해 문의하며 자세한 계획 도출
     * 기능 완성과 빠른 릴리스를 목표로 코드 개발을 속도 중심으로 전개

개발 과정과 문제점 발생

     * Cursor 등의 툴을 사용해 AI가 코드를 작성, 본인은 빌드와 테스트 중심으로 작업 진행
     * 코드베이스 정돈이 미흡해도 우선적으로 요구되는 데이터 제공에 집중
     * 개발을 빠르게 진행했음에도 시간이 지날수록 계속해서 새로운 이슈가 발생함
     * Go와 Clickhouse에 대한 경험 부족으로 인한 어려움, AI가 내놓는 수정안이 연속된 문제를 야기함

코드 품질 문제와 한계 체감

     * 제작된 코드 전체를 면밀히 검토하는 코드 리뷰 시간 마련
     * 같은 기능을 수행하는 파일들이 이름·파라미터·구조 등에서 불일치 및 중복, 혼란 많음
          + 예시: ""WebAPIprovider""와 ""webApi""가 동일 파라미터를 의미하지만 따로 존재함
          + 동일 메소드가 여러 파일에서 재정의되는 현상
          + 설정 파일 접근 방식 일관성 결여
     * 실제로는 여러 주니어 개발자가 소통 없이 동시에 작업한 듯한 결과물 생성

LLM 문맥 피드백의 한계와 전략 변경

     * 컨텍스트 정보를 충분히 제공하고, Gemini 등 대형 윈도우 LLM을 활용했음에도 불구하고 일관적인 개선 미흡
     * 인프라와 언어에 대한 자기주도적 학습 및 문서, 동영상 자료 등 추가 학습 필요성 인식
     * 클린 코드 작성과 조직화를 위해 AI 주도 개발에서 자율적 코드 설계로 방향 전환

AI 보조 활용 방식 변화

     * 반복적인 리팩터링과 코드 정비로 직접적인 코드 이해와 관리에 집중
     * AI의 역할을 코드 자동 변경, 파라미터 일괄 변경, 코드 변환 등의 단순 반복 작업에 한정
     * 새로운 기능 기획, 코드 초안 작성 등은 직접 사고 후 LLM에 검증 or 보조 역할 부여
     * AI를 “조수”로 두고, 계획 및 구조 결정권은 개발자 자신이 역임

사고력 저하 우려 및 변화된 태도

     * AI가 있으니 두뇌 사용 빈도 감소, 계획이나 사고 과정을 AI에 의존하게 된 현실 인식
     * 펜과 종이, 직접적인 설계, 직접 코딩을 통해 개발자 역량과 문제 해결력 회복 추진
     * AI로 인해 코딩 감각 저하 위험 있음에 경각심

비개발자 및 ‘Vibe Coding’에 대한 우려

     * 비개발자가 LLM만으로 개발을 진행할 때, 복잡·혼란스러운 코드와 오류 반복 현상이 더 심각
     * ‘노코드’ 툴보다 AI 기반 개발이 구조 파악에 더 큰 어려움 야기 가능성
     * 파악 불가능한 코드 벽, 계속되는 오류-수정-재발 과정에서 근본적 난이도와 위험성 언급

AI 활용 현실에 대한 단상 및 커뮤니티 분위기

     * AI 상용화, 벤치마크, 인플루언서, AI 회사의 과대광고 및 성능 불일치에 혼란감 표출
     * 동일 모델·프롬프트에서 완전히 다른 결과 나오는 일관성 결여
     * 고성능 최신 LLM조차 클릭하우스 수억 행 쿼리 등 실제 복잡 업무를 완벽하게 소화하지 못함
     * 복잡한 세팅 및 비효율적 워크플로우 강요 상황에서, 그 자체가 ‘시간 낭비’일 수 있음
     * AI가 굉장한 듯하지만 아직까진 ‘좋지만 뛰어나진 않은 도구’라는 신중한 시각

결론

     * 최신 기술과 AI에 여전히 큰 기대와 흥미 보유
     * 그러나 현재 시점에서는 올바른 역할과 한계를 이해하고, AI를 보조적이거나 학습용 도구로 제한적으로 활용하는 것이 현명한 전략임
     * AI 사용으로 인한 개발자 역량 저하 경고와 함께, 본인의 사고와 계획 중심으로 돌아감
     * ‘코드 작동 원리와 구조’를 이해하지 못한 상태에서 AI에 전적으로 의존하는 개발은 실패 가능성 높음

        Hacker News 의견

     * 나는 LLM에 대한 ""올인"" 마인드를 이해하지 못하는 입장임. 나는 iOS 개발자로 일하고 있는데 평소처럼 일함. 이제는 LLM을 이용해 디자인 기반 임시 뷰 같은 것만 빠르게 만듦. 앱 핵심이 아니라 신규 기능이나 위젯 설치 안내 등 부수적인 화면임. 예전엔 복잡도에 따라 30~60분 걸렸던 걸 이제 5분 만에 끝냄. 웹 개발이 싫은데 LLM은 그런 부분에서 꽤 쓸만함. 큰 변경 작업도 LLM 활용 후 직접 검토하고 git에 커밋함. 근데 LLM만 믿고 흐름을 통제하지 않으면 몇 시간 동안 무너졌다가 처음부터 다시 하는 경우가 생김. 균형 잡힌 접근이 중요하다고 생각함
          + 도구의 유용성은 사람과 문제에 따라 달라짐. 예를 들어 10년 경력의 파이썬 개발자가 거대한 레거시 코드와 완벽히 맞춘 IDE로 안정성 위주의 작업을 한다면 LLM이나 Cursor 같은 도구는 오히려 방해가 될 수 있음. 반면, 1년 차 JS(React, Nextjs 등) 개발자가 새로운 아이디어를 자주 프로토타입화하고 IDE 선호도 없고 실험에 열려 있다면, LLM과 Cursor는 즉각적으로 역량을 크게 향상시켜줌
          + 나도 여러 분야(iOS, 웹 개발 등)를 하는데, LLM의 결과는 두 분야에서 상당히 다름. LLM이 출력한 코드가 제대로 컴파일조차 안 될 때가 많음. 심지어 존재하지 않는 API를 알려준 적도 있음. 반면 Nextjs 앱은 한 번에 잘 만듦. 결국 LLM의 학습 자료 차이에서 오는 결과임
          + LLM 기능을 과신하는 건 자연스러운 일임. 나도 스택오버플로 대체와 짧은 코드 스니펫 얻는 목적으론 꽤 오래 써봤음. 점점 더 많은 책임을 맡기다가 문제를 겪고, 한계를 깨달아 다시 아이디어와 조언 위주로 LLM을 쓰게 됨. 많은 이들이 비슷한 과정을 거친다고 생각함
          + 나도 비슷하게 느끼는 입장임. LLM을 전적으로 신봉하지 말고 반복적이고 지루한 작업(작은 함수, 인터페이스 구현, 문서화 자동화 등)에만 활용함. 이를 통해 많은 시간을 아꼈고 업무 효율도 높아졌음
          + iOS 개발에 있어 LLM의 성능은 들쭉날쭉임. Swift, SwiftUI가 너무 빨리 변하고, 공식 문서도 부실한 것이 한 원인임. 간단한 뷰 생성엔 유용하지만, 비동기 처리나 복잡한 비즈니스 로직에선 쉽게 무너짐. 그래도 방향 제시에 도움은 되지만 잘못된 결과(헛소리)에 빠질 위험도 큼
     * LLM 옹호자들은 대부분의 병목이 코드 생성에 있지 않다는 사실을 간과함. 코드를 빠르게 만드는 것만큼 코드 리뷰, 테스트, 코드베이스 이해에 두 배 이상의 시간 투입이 필요함. 장기적으로 유지·보수(버그 수정, 리팩토링 등)를 하려면 반드시 이런 과정을 거쳐야 함
          + 코드 읽기는 쓰기보다 훨씬 어려워서, 실제로 더 많은 시간을 코드 이해에 씀. 그런데 만난 한 CEO는 맥락 정보를 도구로 제공해 개발자는 코드를 읽지 않아도 된다는 식으로 주장함. AI가 엔지니어링의 본질을 바꾼다는 논리임. 솔직히 좀 혼란스러움
          + LLM이 내 코드를 다시 설명해줄 땐 꽤 쓸만하단 느낌을 받음
          + 누군가들 자동화된 코드 에디터를 극찬할 때마다 같은 생각을 하곤 함
          + 현실적으로 대부분의 개발자는 의존성 라이브러리의 내부 구현엔 크게 신경 쓰지 않음. 인터페이스 작동 여부만이 중요함. LLM이 만든 코드나 npm 패키지, rust crate를 들여오는 것의 차이는 큼. 문제점도 알지만 이 관행이 널리 쓰이는 이유가 있음
     * 나도 비슷하게 생각함. 요즘은 새로운 기술을 배우거나 표준 API(특히 boto3)용 클라이언트 코드 생성에 LLM을 주로 사용함. docker compose 파일 변경을 도와주는 Windsurf도 써봤는데 제대로 동작하지 않아 실망했음. 프로토타입은 만들 수 있겠지만 그게 전부는 아님. LLM은 devops 영역에서 판도를 바꿨다고 생각함(이제 API 세부 정보는 덜 중요해짐). 그렇지만 중요한 결정은 여전히 내가 해야 한다고 봄. 인터페이스 정의는 직접 하고, LLM에 구현만 맡기는 식으로 활용. 이게 ""분위기 타는 코딩""은 아니라고 생각함
          + 나도 비슷한 경험을 했음. Cursor, Copilot은 스마트 자동완성, 짧은 함수 생성, 빠른 프로토타입 등에는 환상적임. 하지만 일주일 동안 LLM 주도 코드베이스로 작업하다 보니 구조가 엉망진창이 되었음. 진짜 발전은 언젠가 올지도 모르지만 현재(2025년 5월)는 이 수준임
     * 코드 리뷰 시 엄청난 버그가 터지면서, Cursor 사용으로 얻은 효율성이 순식간에 사라지는 경험을 했음. 다시 VSCode로 돌아오고, Copilot도 구체적으로 요청할 때만 제한적으로 씀. Cursor의 탭 완성 기능은 처음엔 마법처럼 느껴지지만, 곧 그 효과가 사라짐
          + 동료가 최근에 삭제한 코드를 Cursor가 탭 완성으로 다시 입력하려는 걸 반사적으로 보는 게 제일 재밌음
          + 코드 생성 에이전트에 어떤 제한(예: SOLID 원칙, 린트, 100% 커버리지, 명확한 설계 문서 등)을 줬는지 궁금함
     * 나에게도 공감 가는 의견임. 나도 LLM을 아주 많이 사용하지만 두 가지 규칙을 둠. 깊게 사고할 문제는 절대 LLM에 맡기지 않음(복잡한 설계는 반드시 직접 해결). 두 번째로, LLM이 내놓은 코드는 반드시 줄 단위로 꼼꼼히 리뷰하고 수정함. 보통 LLM이 만든 코드는 장황하거나 방어적임. 프롬프트로 고친다해도 결국 미래 유지보수의 책임은 나에게 있음. 코드 생성 결과에 무심하면 불안함이 남음. 내 방식대로 쓰면, 여전히 LLM을 많이 쓰고 더 빠르게 개발할 수 있음
          + 나는 심층 분석 자체를 AI에 맡기는데, 그 목적은 구체적 실행 계획(세부 구현 단계, 검증 기준 등)과 재현 가능한 리포트 작성을 위함임. 계획 수립과 검증엔 반복이 필요하긴 하지만, AI의 도움으로 훨씬 빨리 끝낼 수 있음. 가끔은 계획에 따라 한 번에 끝낼 때도 있음. 상세한 계획과 문서로 일관성을 확보하면 만족감이 큼
          + LLM이 만든 코드를 줄 단위로 꼼꼼히 검토해야 한다면, 과연 시간 절약이 되냐는 의문이 생김
     * 일부 회사들은 소프트웨어 엔지니어에게 LLM 사용을 강제함(Copilot/Cursor 사용 실적을 집계). 이 통계가 해고 지표로 사용될 가능성이 큼. LLM 사용을 강제로 한 달 동안 해보니 오히려 실력이 빠르게 퇴화함을 느낌. 간단한 일엔 도움이 되지만 사고 자체를 LLM에 너무 의존하다 보면 루프에 빠지기 쉬움. 생산성은 오르지 않았고, 오히려 스프린트 업무량만 늘었음. LLM에 대한 종교적인 맹신이 회사 내에 팽배함. 보안 이슈도 심각함. 현 시점이 하이프 싸이클의 정점이라는 신호가 곳곳에서 보임. AI회사들이 원자력 발전소라도 만들지 않는 이상 지금의 대형 AI모델 유지에는 막대한 비용이 들어 사라질 거라 생각함. 앞으로는 터보 자동완성 기능 정도만 살아남을 것 같음. 트랜스포머 모델도 한계가 명확해서 80년대 신경망처럼 특정 용도로만 남고 다시 사라질
       것임. 결국 부침을 겪으면서 30년 후 다시 대두될 것임. 진짜 이렇게 될 때 누가 맨몸으로 수영하고 있었는지 드러날 것임
          + 이런 현상을 방지하려고 주 1회라도 Copilot을 아예 끄고 일하는 'no Copilot Fridays' 자체 규칙을 시행중임
          + 나도 Cursor를 자동완성 및 짧은 코드 스니펫 정도로만 쓰고 있는데, 그래도 실력 약화가 느껴짐. 결국 ""안 쓰면 잊는다""는 뇌의 특성을 실감함
     * 나도 비슷한 문제점을 목격함. 장난감 프로젝트에 90% LLM을 사용함. 손수 코딩보다 10배 빠르지만, 설계 품질이 떨어지고 뭔가 이질적임. LLM 주도 코드가 미래라고 믿지만, 관리를 잘못하면 혼란에 빠짐. 아키텍처 개선을 반복적으로 유도하며 프롬프트를 바꿔보기도 하는데, 결과는 들쭉날쭉임. 아마 더 나은 프롬프트 엔지니어링, 혹은 설계와 지침을 명확하게 문서화하는 게 해답일지도 모름. 도구 성능이 10배 빨라지고 지연이 줄어들면 체감이 완전히 달라질 것임
          + 이렇게 ""10배 더 좋은"" 시기가 빨리 왔으면 함. 그런데 지금 문제는 이미 그 수준에 도달했다고 광고/홍보하는 분위기임. 많은 이들이 ""내가 잘 못 쓰는 걸까""라는 생각에 빠짐. 하지만 아직 도구 자체가 그 정도는 아니라고 봄
          + 클래스, 메서드를 직접 정의해 두고 LLM에 구현만 맡기면 좋음. 복잡한 부분엔 메서드 몸체에 구현 방향을 메모해 놓음. 이렇게 하면 큰 그림을 내가 그리고 LLM엔 특정 코드 생성만 맡길 수 있음. LLM은 지나치게 열심히 도와주려는 빠른 주니어 개발자 느낌임. 코드 생성이 워낙 저렴해져서 맘껏 버리고 다시 만들 수 있음. 실제 내 경우 데이터셋 처리 코드를 LLM 도움으로 여러 번 완전히 버리고 다시 썼더니 결국 원하던 결과와 퍼포먼스를 얻음. 남이 써줬으면 포기했을 작업임
          + 이런 도구들은 그린필드 프로젝트 시제품 단계에서 빛남. 하지만 실제 배포에 다다를수록 그 10배 효과는 점점 사라짐. 아키텍처를 신중하게 관리하지 않으면 결국 수고만 늘어남
          + 복잡한 코드베이스에선 현재로선 고급 음성-텍스트 입력처럼만 쓸 만함(근데 음성이 아니라면 오히려 그냥 손코딩이 더 빠름)
          + 아키텍처와 가이드라인을 명시적으로 기록해야 한다는 데 동의함. 명시적으로 조건과 동작을 정의할수록 효과적임
     * 다익스트라가 쓴 오래된 명저 ""자연어 프로그래밍의 어리석음"" 글의 요지가 현재 논의에 적합함. 공식 언어만이 프로그래밍의 엄청난 발전을 가능하게 했다는 주장임. LLM과 vibe-coding이 코딩을 잘 프롬프트하는 소수만의 마법이 될 위험이 있다는 관점임
     * 나는 Copilot이 500자 미만 코드 제안을 할 때만 좋음. Go, Python에서 새로운 패턴도 배우고 타이핑 양도 줄임. 내게 있어선 그냥 더 나은 자동완성임. 그 이상 길거나 복잡하면, 수정하고 지적하는 데 드는 비용이 얻는 이익을 초과함(특히 반복적인 코드가 아닌 경우엔 그렇지 않음)
     * 지금은 LLM이 생성하는 결과를 반드시 이해하고 밀착 감독해야 함. 반면, 2~3주마다 새로운 모델이 나오고 기존보다 훨씬 좋아지기 때문에 강한 결론을 내기엔 이르다고 봄.

   LLM을 활용한 개발 현장의 생생한 어려움과 우려를 잘 담아낸 글이라고 생각합니다. 현재 많은 분들이 경험하는 한계에 대해 공감하며 읽었습니다. 특히 LLM의 비일관성이나 결과 예측의 어려움, 그리고 장기적인 유지보수 측면에서의 우려는 꼭 짚고 넘어가야 할 부분이라 느꼈습니다.

   다름이 아니라, 저희는 이러한 문제를 조금 다른 각도에서 접근하며 AI와의 협업을 시도하고 있어 조심스럽게 의견을 나눕니다. 저희의 AI '제인'은 단순히 코드를 만드는 것을 넘어, 사람(개발자)의 깊은 통찰을 바탕으로 '좋은 코드 패턴'이 무엇인지, 그리고 코드의 '유지 보수 일관성'을 어떻게 확보할 수 있는지 그 '패턴' 자체를 배우고 이해하는 데 집중하고 있습니다.

   AI가 처음부터 완벽할 수는 없기에 발생하는 비일관성이나 '오류'들을 단순한 문제로 보지 않고, 오히려 '제인'이 스스로 학습하고 스스로를 개선하는 중요한 '패턴 데이터'로 적극 활용합니다. 인간이 복잡한 본성 속에서 패턴을 읽어내듯, 저희는 AI의 불완전성 속에서 개선의 실마리를 찾는 방식을 취하고 있습니다.

   이러한 인간 주도의 '패턴 학습/관리' 접근 방식을 통해, 글에서 지적된 코드 품질 저하, 불일치 등의 문제를 근본적으로 해결하고 '유지 보수 일관성'이 매우 높은 결과물을 만들어내는 것을 목표로 하고 있습니다. AI가 단순히 boilerplate 코드를 생성하는 것을 넘어, 기존 코드베이스의 숨겨진 비일관성 패턴을 분석하고 개선 방안을 제시하는 등 더 깊이 있는 협업 파트너가 되도록 훈련하고 있습니다.

   아직 갈 길이 멀고 도전적인 과정이지만, 저희 '제인'과 개발자가 함께 배우고 진화하며 '유지 보수 일관성'을 핵심 가치로 삼는 이러한 방식의 협업이 현재 LLM 활용의 한계를 넘어설 수 있는 획기적인 가능성을 보여준다고 믿습니다. AI를 단순히 도구로 쓰는 것을 넘어, 함께 성장하며 더 나은 코드 문화를 만들어가는 파트너로 만드는 저희의 시도에 많은 관심 부탁드립니다.

   좋은 글과 인사이트에 다시 한번 감사드립니다!
"
"https://news.hada.io/topic?id=20911","상하이, 승객이 직접 노선을 설계할 수 있도록 한 맞춤형 버스 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 상하이, 승객이 직접 노선을 설계할 수 있도록 한 맞춤형 버스 도입

     * 상하이는 시민들이 직접 노선을 제안하고 선택하는 맞춤형(DZ) 버스 시스템을 도입함
     * 이용자가 일정 수(보통 15~20명) 이상 모이면 해당 노선이 실제 운행 시작함
     * 이 플랫폼을 통해 220개 이상의 DZ 노선이 출범함
     * 경로 제안과 노선 승인, 운행까지 모든 과정이 온라인 플랫폼에서 단기간 내 진행됨
     * 아직 대중 인지도 부족과 비효율적 계획 등 초기 과제가 존재함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

상하이 맞춤형 버스 시스템 개요

     * 상하이에서는 이른 아침 학교 통학, 병원 방문을 위한 노년층 이동, 도시 외곽 통근, 가족 단위 방문 등 다양한 요구를 반영한 신개념 대중교통을 도입함
     * 이 시스템은 시민의 제안으로 생성된 노선을 기반으로, 최소 수요 충족 시(15~20명)만 실제 운행되는 방식임

맞춤형(DZ) 버스 시스템 소개

     * ""DZ(딩즈, 맞춤형)""라는 브랜드로, 시민들이 시 운영 플랫폼을 통해 노선을 제안 및 신청하도록 함
          + 유사 목적 이용자가 노선에 참여 혹은 투표하여 수요를 모으고, 기준 인원을 충족하면 노선을 개설함
     * 2024년 5월 8일 오픈된 온라인 플랫폼에서 출발지, 도착지, 희망 시간 및 이용 빈도 입력 후 승인 절차가 진행됨
     * 노선은 승인 후 최대 3일 이내에 운행 개시 가능함
     * 지금까지 16개 도시 구역 전체에 걸쳐 220개 이상의 맞춤형 노선이 운행되고 있음

실제 적용 사례

     * 첫 테스트 중 하나는 DZ301번 노선으로, 대형 지하철역과 인근 주거지역, 학교, 사무실을 연결함
          + 하루평균 250~260명 이용, 출근 시간대 170-180명, 저녁 70-80명 이용임
     * 해당 노선은 주민의 요청에서 시작되어 교통공기업 직원들이 현장 조사와 이용자 의견 청취, 피크타임 계산 등 과정을 거쳐 시범운행 및 정식 운행으로 이어짐

시스템적 의의 및 도전 과제

     * 동제대학교 교수 Chen Xiaohong은 상하이의 치밀한 대중교통망을 바탕으로 맞춤형 버스가 수요 반영과 효율성 향상에 기여함을 강조함
     * 제안된 노선은 ""인기 맞춤화"" 페이지에 공개되어, 신규 수요자가 추가적으로 가입하거나 단체 예약으로 빠른 노선 승인 가능
     * 요금은 시장가격 기반으로 책정되나, 학생·노년 등 특정 계층에 대한 할인은 아직 제공되지 않음

플랫폼 운영과 미래 방향

     * 시 승객운송부 왕이샹 부국장은 이 시스템이 기존의 관리적, 느린 노선 신설 절차를 크게 단축했다고 설명함
     * 수요 불균형, 낮은 인지도, 현장 중심 인력 투입 등 초기적 한계와 과제가 있음을 언급함
     * 앞으로 경로계획 개선, 플랫폼 기능 업그레이드, 인지도 제고 등이 중요 과제로 제시됨

결론

     * 맞춤형 DZ 버스는 기존 대중교통의 한계를 보완하여, 시민의 요구에 신속하게 대응하는 운행 체계임
     * 초기 시행착오와 함께 지속적인 노선 기획과 플랫폼 발전 방향이 논의됨

   https://www.modooshuttle.com/ 이런 서비스와 결국 비슷한 것 아닐까요?

   아이디어는 낼 수 있는데 운영이 성공/실패를 가르지 않을까 싶습니다.

   이런게 있었군요

   노선을 주민 요청으로 만들어내는거군요.

   서산시에서는 행복버스라는 이름으로 수요응답형 버스를 운영하는데, 그때그때 예약이 들어오는것에 따라서 노선이 동적으로(?) 생성되는 형태입니다.
   https://m.blog.naver.com/seosan_city/223748673342

   다른 지자체들도 비슷하게 운영할텐데 읍면 지역에서만 운영하고 시내로는 가지 않아 탑승자가 별로 없어서 가능한 방식인것 같기는 합니다.

   와... 감탄밖에 안 나오네요

        Hacker News 의견

     * 나는 이 방식이 마음에 들음, 똑똑함. 수요에 맞춰 대중교통을 조정하고 스스로 수요에 맞게 최적화하는 저기술적 해결책임. 버스와 기차가 정해진 시간에 운행되는 가장 큰 가치는 미리 계획할 수 있게 해주는 점임. 그런데 만약 Uber처럼 작동한다면 어떨지 상상해봤음. 차량이 와서 데려가주고, 필요한 경우 다른 차량으로 갈아타면서 목적지까지 데려다줌(기존 방식처럼 목적지 근처에 떨궈주지 않고). 여정 시간이 예측 가능하고 합리적이기만 하다면, 대다수 사람들이 꽤 만족할 것 같음.
          + 이런 아이디어는 이론적으로는 좋아 보이지만 실제로는 잘 안 돌아갈 것임. 가장 먼저 떠오르는 문제는 스마트폰 없이는 대중교통을 쓸 수 없게 된다는 점임. 청소년이 스마트폰이 없거나, 누군가 휴대폰이 꺼졌을 때는 버스를 이용 못 하게 됨. 지금 시스템이 놀라운 점은, 예측 가능하면서도 어떤 조율도 필요 없다는 것임. 그저 정류장에 가기만 하면, 항상처럼 목적지에 데려다주는 버스가 옴. 그날 경로를 굳이 찾아보지 않아도 되고, 정류장이 어딘지 보지 않아도 되고, 버스에게 내가 있다는 사실을 알릴 필요도 없음. 그냥 정류장에 나가면 모든 과정이 예측 가능하고 신뢰성 있게 자동으로 돌아감.
          + 앱으로 내가 가고 싶은 곳을 입력하면, 가까운 세 곳의 버스 정류장 중 어디로 가야 할지 알려주고, 합리적으로 빠르게 목적지에 도착하게 해주는 시스템 상상해봤음. 정해진 노선 대신, 최신 수요에 따라 동적으로 차량과 경로를 배분함. 미리 계획을 입력할수록 요금이 저렴해짐. ""나는 매주 평일 아침 9시에 출근하고 6시에 퇴근함"" 같은 식임. 대규모 출퇴근 수요엔 버스를, 좀 더 적은 소규모 또는 임박한 단체 수요에는 미니밴이, 아주 적고 임박한 비정기 수요에는 승용차가 배정됨. 장애인이나 무거운 짐이 있는 사람은 아무 곳에서든 커브사이드 픽업도 가능함. 그럼 도시에선 사유 자동차를 완전히 없앨 수 있음. 사유차는 도시 밖에 세우거나, 더 나아가 공유차를 이용하면 됨. 택시나 Uber도 없고, 오직 공공 교통만 존재하며, 운전자는 노조 소속의
            정규직임. 도로에 다니는 모든 차량은 가득 차고, 필요 없는 주차 공간 대부분을 없애고, 주차장 규모도 축소함. 이건 로켓 과학이 아니라 컴퓨터 과학임. 자동차 생산량을 극적으로 줄일 수 있는 판타지임.
          + 여러 나라에는 일반 버스 노선과 똑같이 번호를 붙인 민간 밴이 있음. 이 밴은 버스처럼 정류장마다 승하차를 하지만 규모가 더 작고 훨씬 자주 다님. 이런 방식이 훌륭하다고 생각해왔음. 대형, 저빈도, 다인 승차 버스와 소형, 온디맨드, 승용차 사이에 중간 단계가 왜 없어야 하는지 의문임.
          + 몇 주 전에 베트남에서 비자런을 했는데, 국경까지 가는 과정이 딱 그런 방식이었음. 버스가 종점에 도착한 뒤엔 나만 남겼고, 두세 번 더 멈춰서(어디선가 컴퓨터 모니터를 다른 곳으로 운반?), 마지막엔 ""다른 버스를 타라""고 했는데, 추가 요금 없이 탑승함. 둘 다 마이크로버스(마르슈루트카)였음.
          + 그 ""만약"" 상상은 진즉에 전문가들이 연구하고 비판한 바 있고, 절대 성립할 수 없는 개념임. 빠르고, 예측 가능하면서도 합리적 시간의 이동과 합리적 비용을 동시에 얻으려면 정해진 시간표 내 서비스가 필수임. 귀하 전용 리무진 형태라면 상관없지만, 그게 대중교통이 될 수 없고 사적 차량보다 더 나을 수 없음.
          + 우리 지역에는 Dial-a-ride 서비스가 있음. 전화를 걸어 미리 예약하면 그때그때 맞추어 운행되는 버스 노선을 만들어줌. 실제로 써본 적 없음. 미리 3일 전 예약을 요구하고, 취소는 24시간 전까지만 가능함. 출발/도착지에 따라 평일 일부만 이용 가능하고, 주말엔 운영하지 않음.
          + San Antonio, TX에서 VIA Link를 시범 도입하고 있음. 지하철역에서 Uber 스타일의 ‘라스트 마일’ 연계 기능 추가임. 링크: https://www.viainfo.net/link/
          + Via Transportation(ridewithvia.com)은 본래 합승 택시로 시작했지만, 지금은 소개한 바와 같이 온디맨드 공공교통으로 전환했고, 상당히 성공적으로 보임. 특히 학교 통학, 파라트랜짓 부문에서도 가치가 큼. 나는 관계자 아니지만 이 모델이 아주 유망하다고 생각함.
          + 고정 노선 기반이어도, 예전부터 교통운영자에게 예약 앱이 필요하다고 주장해왔음. 전체 여정 데이터를 얻을 수 있고, “몇 분 안에 픽업 예정” 같은 약속을 제공해주면 버스 자체를 마련하지 않아도 렌트카로 대체 가능함. 승객의 최종 목적지를 입력받으면, 기사에 가까운 방식으로 현재 버스에 탄 사람들과 대기 중인 사람 각각의 실제 행선지 데이터 기반의 반자동 경로 재설정도 수행 가능함. 기존엔 “현재 제공되는 교통수단 하에서 사람들이 어디로 언제 가는가”만 알 수 있는데, 예약제와 보장된 픽업을 도입하면 “사람들이 실제 원해서 가고 싶은 곳”에 대한 훨씬 신뢰성 높은 데이터를 얻을 수 있음.
          + 도로는 무한한 대역폭을 제공하지 않음. 이런 방식은 좋은 아이디어일 수 있지만, 적절한 제약을 두지 않으면 도시 전체를 교통체증으로 마비시킬 수 있음.
          + 이 방식은 미국에선 절대 성립 불가능하다고 생각함. 첫째, 지역 당국이 통제 권한을 빼앗긴다고 느끼기 때문임. 둘째, NIMBY(내 뒷마당엔 안 돼)들이 버스가 자기 집 앞을 지나는 것조차 거부하기 때문임.
          + 기존의 뻣뻣한 노선 개념 대신, 버스를 유연하고 확장 가능한 물류로 재구성하는 사고 방식임.
     * 정말 기발한 아이디어임. “데자이르 패스”를 대중교통에 도입한 느낌임. 실제로 실현하는 건 당연히 난관이 많겠지만, 발상 자체는 환상적임. 인구 밀도와 실행 역량까지 갖춘 중국/상하이처럼 극소수 도시만이 제대로 가능할 거라고 봄. 공공장소 설계는 ‘군중의 지혜’에 반응함으로써 개선할 여지가 아주 큼.
          + 유능한 교통운영자라면, 앱이나 적극적으로 요청할 만한 소수보다 “해당 노선이 생기면 쓸 사람들이 누구인가”를 먼저 분석해야 함. 손쉽게 앱을 켜지 않을 다수의 잠재 이용자를 놓칠 수 있음.
          + 많은 지역에선 시범 프로그램조차도 분석에만 매몰되어 멈춰버림. 이런 피드백 루프를 통해 도시 공간 설계를 더 많이 듣고 덜 짐작하는 방향으로 가져가면 이득임.
          + Citymapper가 과거 런던에서 사용자들의 통행 데이터 기반으로 기존 노선이 제대로 커버 못 하는 구간에 스마트버스 노선을 만들었었음. 프로젝트는 이미 종료된 것 같고 참고: https://citymapper.com/smartbus
          + 대중이 진짜 영웅임. 우리 자신은 종종 유치하고 어리석음. 그것을 깨닫지 못하면 가장 기초적인 지식조차 얻지 못함. — 마오쩌둥
          + 개념상으론 훌륭하지만, 이게 비용 절감을 위한 수단이라면 실제론 그다지 좋지 않을 수도 있음.
     * 남아공에선 ‘Taxi’라 부르는 미니버스가 있음. 개별 소유(부분적으로)로 가득 태워 다님. 노선은 운전자가 수입 극대화를 위해 그때그때 정해서, 일종의 하향식보단 바텀업 방식임. 그러나 폭력적 카르텔 구조임. 전적으로 이상적이지는 않으나 변종 사례로 흥미로움.
          + 남미 대부분 국가에도 ‘combis’(또는 micros)라 불리는 유사한 형태의 버스 시스템이 존재함. 민영 운영이며 운전자 수요에 따라 노선도 수시 변경됨. 주요 정류장은 버스 창문에 써 붙여둠(꽤 지저분해 보임)
     * 독일 함부르크의 Moia 서비스는 ‘가상 정류장’을 제공함. 승객 수요에 따라 매번 버스 경로와 정차점이 달라짐. https://www.hvv-switch.de/en/faq/what-are-virtual-stops/
          + 이게 무슨 의미임? 영국 런던에도 “손을 들면 아무 곳에서나 정차하는” ‘hail and ride’ 버스가 있음. 벨을 누르면 가능한 곳에서 바로 내려줌. 단, 경로는 고정임. Moia도 비슷한 개념인지 궁금함.
          + 결국 ‘합승 택시’ 개념 아님? 공항이나 병원 등 소규모에서 이미 오래전부터 보편적으로 존재함.
          + 나도 Moia 정말 멋지다고 생각함
     * 중국은 이런 걸 그냥 해내는 역량과 관료주의의 억제라는 양쪽을 모두 가진 유일한 현대 국가임. 동시에 그것을 보는 것은 놀라움과 함께, 많은 서구 사회가 자초한 규제에 얼마나 묶여 있는지 아픈 상기임. 어떤 도시든 신규 버스 노선 하나 만드는데도 수년이 걸림. 실제로 그런 사례를 본 기억도 없음.
          + 폴란드 바르샤바 참고하면 좋음. 대중교통이 훌륭하고 청결하며, 버스, 트램, 지하철, 각종 라이드쉐어로 어디든 갈 수 있음. 자전거 인프라도 괜찮음. 상하이보단 작지만, 서구 대부분 도시가 마찬가지로 상하이보다 작은 편임. 시스템 소개 링크: https://www.youtube.com/watch?v=0Kn2tL51bBs&t=8s
          + 서유럽에선 이런 신규 노선이 끊임없이 생김. 무슨 말을 하는지 모르겠음
          + 내가 사는 서유럽 도시도 매년 새 버스 노선이 여러 개씩 추가됨. 이는 모든 중대형 도시에서 자연스러운 현상임. 상하이 방식의 특징은 실험·저수요 노선에 투입할 ‘여유 용량’이 있다는 점임. 대부분 버스 네트워크는 수요 한계까지 운행하므로 남는 차량이 별로 없는 편임.
          + 예전에 도시 거주할 때 덴버는 몇 달마다 몇 번씩 노선 바꿨음. 느릴 수는 있지만, 서구 도시에선 신규 노선에 수년 걸릴 일이 없음. 규모를 작게 보면, 우리 학군도 지역 이사할 때 48시간 만에 노선을 조정해줬고, 학생 일정 바뀔 때마다 노선도 바꾸던 경험 있음.
          + 서구 사회의 문제점은 부패 때문이라고 봄. “서구가 덜 부패했다”는 생각이 점점 사실이 아님이 드러남. 부패란 임무가 아니라 사람에 충성하는 것임. 좋은 결과가 나올 수 있는 시스템이란, 리더가 사적인 이익이 아니라 임무에 충성하도록 만들고, 그에 따라 자기 사람들에게 명확한 목표에 복종하게 만든 경우임. 중국은 최고 부자도 처벌 가능하지만, 미국 등 서구는 그럴 방법이 별로 없음. 중국은 분유에 독을 탄 임원을 사형에 처했고, 미국은 오히려 부자(예: Sackler Family)가 법을 악용해 면죄부를 받음. 법이란 본래 강자를 구속하여 약자를 보호하려는 목적임에도, 서구에선 오히려 부자들이 법을 장악하여 자기 세를 공고히 하고 있음. 규제가 많아진 게 아니라, 부자들이 자기 이익을 위한 규제를 만든 것임. 중국은 국가 권력이 국민을 위해
            작동하는 경우가 많고, 서구는 부자를 위해 작동함. 관료주의 없는 게 아니라, 상하이의 이런 것 역시 그들의 관료 조직으로 실현하는 것임. 결국 사유화 핑계로 관료조직의 손발을 묶는 서구가 비효율적으로 비치는 배경임.
          + 오스틴, 텍사스에는 도심 모든 정류장에 30인치 이잉크 스크린이 설치되어 있고, 거기서 수시로 경로와 시간표가 업데이트됨. 실제 의사결정이 얼마나 유연한지는 잘 모름.
          + 중국에도 관료주의가 많지만, 대도시의 교통 시스템은 잘 설계되고 잘 운영됨. 아마도 도시 규모 덕분임. 매년 신규 지하철 개통, 이미 지하철이 있는 대도시에서도 해마다 신설 노선을 발표함. 경험이 쌓이면 자연스럽게 실력이 늘어남.
          + 영국 도시도 끊임없이 신규 버스노선을 만듦. 왜 불가능하게 여기는지 모르겠음.
          + 중국정부는 매우 관료적임. 단, 한번 방침이 정해지면 전체 관료 시스템이 신속히 작동함. 관료주의가 비효율적 의미만은 아니며, 중국식 관료제는 오히려 능률적 운영의 표본임.
          + Madison, WI는 최근 전체 버스 시스템을 전면 개편했음. 기존 노선 다수를 신속자동차 전용 노선으로 교체했으며, 엄청난 반대에도 불구하고 대성공하여 탑승률이 급증함. 이 일은 오로지 해당 시장이 정치적 의지로 대중교통 향상에 몰입했기 때문에 가능했음. 미국 정치의 진짜 문제는 우선순위 설정 실패라고 봄. 진짜 동기부여가 “교통 개선”인 정치인은 실제 성과를 내지만, “재선”, “후원금 확보”, “승승장구 이력 만들기”가 동기면 항상 무난한 타협만 하면서 아무 진전을 못 내는 구조임. 링크: https://channel3000.com/news/… https://en.wikipedia.org/wiki/Satya_Rhodes-Conway
          + 달러 밴(dollar van) 역시 이에 가까움. 노선에서 크게 벗어나지만 않으면 어느 곳이든 데려다줌.
          + 신속한 행동과 공공 참여, 두 요소가 균형될 수 있으면 가장 좋을 것임.
          + 규제가 어떻게 대중교통을 방해하는지 모르겠음. 서구도 단점이 많지만, 중국식 ‘가짜’ 제품이나 졸속 건물보다는 낫다고 생각함. 중국도 완벽과는 거리가 멂.
          + 독일의 베를린, 함부르크도 이에 대해 할 말이 많음. 최근 몇 년간 이런 아이디어가 세계 전역에 퍼졌음. 상하이만 다른 점이 실제 버스와 정해진 정류장을 활용한다는 것임. 서구는 수요 계산이 쉬움: 학생은 늘 집-학교 왕복하고 귀가 시간도 정해져 있음. 복잡한 시스템 필요 없이, 학교 등하교 시간 기준으로 버스 출발시키면 됨. 노인도 병원, 마트 등 이동 패턴이 고정적임. 직장인은 대형 사업장일수록 패턴이 명확하고, 소규모 사업장이나 복합 이동에서야 ‘수요 예측 시스템’의 강점이 나타남. 상하이 모델은 “공공교통을 쓸 만큼 디지털에 익숙한 인구”, “경로가 자주 바뀔 때 시민이 길을 잃지 않을 만큼 촘촘한 서비스”가 모두 확보되어야 성립함. 내가 사는 곳은 학생, 노인만 교통을 쓰고 나머진 느리고 비싸서 안 씀. 충분한 데이터축적이
            어려움.
          + 상하이 공항만 해도 12개월 전까지만 해도 라이드셔링을 허용하지 않았음. 관료주의와 부패도 상상 이상임.
          + UAE나 카타르도 비슷하게 신속성, 역량을 동시에 갖춘 국가임.
          + 스위스, 네덜란드, 스웨덴, 노르웨이도 방문해보면 상하이만큼 잘 돌아가는 대중교통 사례가 많음. 시골 중국 철도가 스위스 방식 수준에 도달하면 끝장남.
     * 헬싱키 수도권에서 10~15년 전에 비슷한 시도를 했음. 그런데 수요가 적어서 결국 폐지됨. 이미 기존 노선이 인구 밀도를 충분히 반영하여 노선을 짰으므로, 더 이상 개선할 여지가 거의 없었음. 따로 운영 중인 사례는 주로 노인을 위한 리타이어먼트 홈 미니버스임. 이런 경우 정규 버스 시간표에 얽매일 필요 없고, 노인에게 더 접근성 높은 교통을 제공함.
     * 옆길로 새보자면, 선거구 문제 역시 사람들에게 “서로 인접한 인구집단 중 우리 동네와 가장 비슷한 둘~셋을 고르라”고 해서 데이터를 모으고, 이 데이터를 알고리즘으로 나누면 진정한 군집이 나올 거란 상상을 해봤음. 문제는: (1) 대다수 유권자에게 너무 복잡하고, 신뢰를 얻기 힘듦 (2) “알고리즘”이란 이름 아래에서도 실제 경계선을 나누는 알고리즘을 누가 고르느냐에 따라 정치 영향력이 개입될 수 있음.
          + 게리맨더링(선거구 조작)이 FPTP(최다득표자 당선)선거방식에선 훨씬 심각함. 제시한 복잡한 군집화보다는 FPTP보다 약간 복잡하면서도 게리맨더링 동기가 줄어드는 방식(예: 전국 득표율에 따라 비례대표 배분)에 집중하는 게 해법임.
          + 나도 조금 다른 맥락이지만 비슷한 방식으로 행정구역이나 지방도 나눌 수 있다고 생각함. 각 구획별로 “어느 중심지역을 선호하냐” 물으면, 연속성을 확보할 수 있으면 자연스럽게 최적의 도출이 가능함.
          + 선거구 개념에는 “누가 내 사람인가”라는 질문이 본질적으로 깔림. 예를 들어 왜 IT 노동자나 할머니를 대표하는 의원은 없고, 왜 반드시 지역별이어야 하는지 고민 필요함.
          + 서로 비슷한 구역끼리만 묶으면 오히려 젠트리피케이션(고급화)으로 이어질 수 있음. 다양한 계층이 함께 뒤섞여야 모든 문제를 골고루 책임지는 구조가 유지됨.
          + 이러한 방법이 시간이 흘러도 충분히 안정적으로 유지될지 의문임.
          + 그래도 결국 새로운 단위(센서스 트랙)를 또 다른 ‘것쪼개기’ 수단으로 악용할 것임.
     * 굉장히 멋진 아이디어지만, 이런 서비스는 사용자가 점차 줄다 결국 기능정지가 되는 패턴으로 번질 것 같음. 사람들이 필요할 때만 들어가서 경로를 투표하고, 그 후에는 다시 이용하지 않게 됨. 수요 데이터가 쌓이지 않아 경로 자체가 사라질 위험이 큼.
     * 홍콩의 미니버스가 발전한 버전 같음. 우리는 예전부터 이런 식으로 운영해왔음. 기사에게 어디서 내릴지 미리 말함. 어디도 갈 사람 없으면 그 구역 전체를 건너뛰기도 함.
          + 사람들이 거의 안 타는 지역에서 출발하고 싶은 사람들은 어떻게 하나 궁금함.
          + 정확히는 ‘레드 미니버스’임.
     * 스위스에선 기차/버스 노선을 매년 데이터 기반으로 변경함. 흔히 생각하는 ‘투표제’보다 신뢰할 만하다고 생각함.
          + 실상 노선 변화는 그리 크지 않고, 대부분 시간표 변경임. 기사에서 다룬 건 시간표가 아니라 경로에 집중함.
          + 스위스식 사고방식이 너무 좋음. 어찌 보면 유일하게 이상적인 나라임.
          + 사용 데이터와 투표식 투명성이 결합된다면 더 큰 시너지가 가능함.

   FYI, ""데자이르 패스""는 ""Desire paths"" 였습니다.
"
"https://news.hada.io/topic?id=21032","Litestream이 이제 빠른 시점 복구가 가능해집니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Litestream이 이제 빠른 시점 복구가 가능해집니다

     * Litestream은 SQLite 기반의 전체 스택 애플리케이션을 안전하게 객체 저장소에 백업하며, 이번에 가장 큰 기능 변경이 이루어짐
     * 기존 구조보다 효율적인 LTX 파일 포맷과 컴팩션 기법을 적용해 빠르고 효율적인 시점 복구가 가능해짐
     * Conditional write를 활용한 새로운 방식으로 리더 싱글톤 및 read replica 기능 구현을 단순화함
     * 곧 VFS 기반 read-replica 계층이 제공되어 다양한 환경에서 손쉽게 확장 가능함
     * 대폭 개선된 구조로 다수의 데이터베이스 동시 동기화가 가능해져 확장성이 높아짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Litestream 소개 및 중요성

     * Litestream은 오픈소스 도구로서 SQLite를 기반으로 하는 다양한 전체 스택 애플리케이션을 객체 저장소에 안전하게 백업하는 기능을 제공함
     * SQLite의 임베디드 특성으로 인해 데이터가 한 서버에 종속되던 문제를 해결하고, 서버 장애 시에도 데이터 복구가 용이해짐

Litestream의 발전 과정

     * SQLite를 더 쉽게 활용하기 위해 Litestream이 2020년에 등장했음
     * Litestream은 SQLite 애플리케이션과 별도의 프로세스로 실행되며, WAL 체크포인팅 프로세스를 대체해 실시간으로 데이터 변경 사항을 S3와 같은 객체 저장소로 스트리밍함
     * 서버가 손실되어도, 객체 저장소에서 최신 상태로 데이터베이스를 효율적으로 복구할 수 있음
     * 이후 LiteFS라는 프로젝트가 추가 개발되어 read replica와 기본 장애조치(Primary Failover)까지 지원하는 등, SQLite를 Postgres와 같은 현대적 배포 구조로 활용할 수 있게 만듦
     * LiteFS와 Litestream 모두 장점이 있으나, Litestream은 더 널리 사용될 정도로 배포 및 사용이 쉬움

효율적인 시점 복구(Point-in-time Restore)

     * 이전 Litestream 설계는 모든 변경 사항을 지속적으로 기록해 S3에 전송했으나, 데이터가 잦게 변경되는 경우 복구 시 비효율적임
     * LiteFS에서는 트랜잭션 인지 기반의 접근법을 도입, 변경 페이지 범위를 정렬해서 기록하는 LTX 파일 포맷 사용
     * 여러 LTX 파일을 쉽게 병합(compaction)해 최신 버전만 남기는 방식으로, 데이터 복구 속도와 효율성을 대폭 향상시킴
     * 이 구조는 LSM 트리와 유사함
     * 새로운 Litestream에서도 LTX 파일 및 컴팩션 방식을 도입하여 많은 중복 저장 없이 정확한 시점 복구 지원이 가능해짐

CASAAS: Compare-and-Swap as a Service

     * Litestream은 SQLite 애플리케이션이 백업 시스템을 인지하지 않아도 작동해야 하며, 장애 등으로 프로세스가 죽는 경우 데이터 변경 누락이 발생 가능함
     * 이런 문제를 해결하기 위해 generation이라는 개념을 도입해 각 백업 세션과 그에 대한 로그 스트림을 고유하게 식별함
     * LiteFS에서는 Consul을 이용해 싱글 리더를 보장했으나, 외부 종속성 필요성 때문에 Litestream은 S3 등 객체 저장소의 conditional write 기능으로 단일 복제 경로(싱글톤)를 간편하게 구현함
     * 이에 따라 ephemeral 노드 환경에서도 혼동 없이 안정적인 동작이 가능해짐

경량 read replica 기능

     * LiteFS는 트랜잭션 인지를 위해 FUSE 파일시스템을 사용하지만, 이는 복잡성과 도입 부담이 있음
     * 이를 완화하기 위해 LiteVFS라는 SQLite Virtual Filesystem(VFS) 확장 모듈을 통해 FUSE 없이도 다양한 환경에서 동작 가능하게 설계됨
     * 향후 Litestream에도 동일한 VFS 기반 레이어를 적용하여 S3 등 객체 저장소에서 직접 페이지를 fetch하고 cache하는 read-replica 계층을 제공 예정임
     * 로컬 SQLite처럼 빠르지는 않으나, 캐싱 및 prefetching 전략을 통해 많은 사용 사례에서 만족스러운 성능 제공 가능성 기대함

오픈 소스 및 활용성

     * Litestream은 완전한 오픈소스이며, Fly.io에 종속되지 않고 어디서든 사용 가능함
     * 대량의 데이터베이스를 하나의 프로세스에서 동기화하는 기능이 추가되어, 수백~수천 개 데이터베이스도 효율적으로 백업/복제 가능해짐

SQLite와의 지속적 동반성장

     * SQLite는 산업 변화 속에서도 꾸준히 새로운 활용 사례를 창출하는 견고한 데이터베이스임
     * 최근 LLM 기반 코드 생성 에이전트와 같은 분야에서도, 실시간 데이터 롤백 및 분기가 중요해짐에 따라 Litestream의 발전된 시점 복구 기능이 중요한 기반이 될 수 있음
     * 향후 이러한 개선된 아키텍처는 롤백, 포크, 자동화 에이전트 대응 등 확장 기능에도 기여할 것임
     * 새로운 Litestream은 기존 디자인 대비 보다 우수하며, 확장성과 사용성을 모두 강화함

   Litestream - SQLite 스트리밍 복제 도구
   저는 서버사이드 SQLite에 올인합니다

        Hacker News 의견

     * 코드 저장소 위치를 찾은 경험 공유 내용, 2년 전 litestream과 litefs 사용에 불편함이 있었지만 이번에는 대부분의 문제가 해결된 느낌이라는 의견, 이제는 데이터베이스에 litestream을 자유롭게 적용할 수 있고 여러 writer 문제에 대한 걱정이 줄어들었다는 관점, read replica FUSE 레이어의 장점을 기대하는 입장, 관련 풀 리퀘스트에서 lease 인수 방식에 대해 소개(lease가 이미 있으면 새로운 프로세스가 1초 간격으로 재시도해 빠른 롤링 리스타트 지원), 실용적인 접근 방식이라는 생각
     * 새로운 Litestream에서 내가 바라던 모든 기능이 구현된 것 같다는 느낌, 기대감과 흥분되는 감정
     * 매우 똑똑하고 배포가 간단해지는 방식에 대해 긍정적 시각, 수천 개의 SQLite DB 백업이 필요한 상황에서 지금까지는 fanotify와 SQLite의 Backup API로 임시 방편을 제작 경험, wildcard replication이 많은 파일을 지원한다면 Litestream으로 전환하고 싶다는 기대
     * LTX 전환 이후 한 디렉토리에 수백, 수천 개의 데이터베이스가 있어도 /data/*.db 복제가 가능해진 점 강조, 이 부분이 이전에는 결정적 장애였다는 입장, 이젠 멀티 테넌트 환경에서 각 사용자 별 데이터베이스 단위로 원하는 시점 복구나 데이터 다운로드 및 이관이 가능해지는 점에 대한 긍정적인 전망
     * ben에게 감사 인사와 함께 실사용 경험을 공유, 약 1년 이상 write-heavy 내부용 케이스에(압축 기준 약 12GB) litestream을 프로덕션에서 사용하며 월 비용이 극소수(azure 기준 몇 백원)에 불과하다는 점 소개, 새로운 변경 사항 적용을 기대하는 입장
     * Fly의 SQLite 기반 개발자 경험이 좀 더 다듬어지길 바라는 마음, 현재 아쉬운 점으로 자체 UI와 CLI가 부족해 초기 데이터베이스를 Fly Machine에 세팅하는 작업이 생각보다 많은 과정이라는 점, fly console은 SQLite와 제대로 연동이 안되고 별도 머신에서 실행되어 데이터가 있는 볼륨에 접속할 수 없는 점, 결국 fly ssh console —pty로 직접 해당 머신에 들어가야 하는 불편함 지적, SQLite 기반 웹앱은 소규모가 대부분이라 수익을 내려면 많은 개수를 운영해야 한다는 고충
          + Rails 8과 SQLite 조합에 대한 개인의 선택 방향 질문, 최근 Postgres보다 더 선호하는지 궁금증 제기
     * Litestream을 막 조사하던 중 타이밍 좋게 글을 접한 개인적 경험 공유, VPS에서 Sqlite를 사용하며 Litestream 도입을 고려 중, Litestream 프로세스가 실행 중인 동안 특정 시점으로 데이터베이스를 복구할 수 있는지 질문, auto-checkpointing이 프로세스가 다운됐을 때 WAL을 소비하기 때문에 복구 불가 시간대(예시: 장애로 2:00~3:00 동안 프로세스 중지, 1:55 또는 3:05에는 복구 가능하지만 2:00~3:00 사이 복구 정보는 사라짐)에 대해 궁금증 표시
          + Litestream은 WAL 세그먼트를 특정 시간 단위로 저장, 기본적으로 WAL 변경 내역을 매초 전송해서(설정한 보존기간 내) 원하는 시점의 초 단위 복구가 가능하다는 설명
          + DST(일광절약시간) 처리 문제에 대한 질문, 유럽 기준 3월 30일에 2:00에서 3:00로 시간이 점프하는 상황에서의 동작 방식 궁금증 제기
     * 과거 dynamodb 기반 DonutDB라는 sqlite vfs를 만든 경험 공유, S3에 CAS(Content Addressable Storage) 기능이 추가된 점을 계기로 DonutDB를 S3 지원 버전으로 리뉴얼하려고 했고, 이번에 lightstream이 이를 지원해줘서 직접 개발할 필요가 없어진 점 반가움 표시, 새로운 도구 사용에 대한 기대
          + S3에 CAS(Content Addressable Storage)가 추가됐다는 언급에 대해 공식 자료나 참고 링크를 요청, CAS 의미가 맞는지 확인하고 싶다는 궁금증 표현
     * 앱 배포시 기존 방식에서는 새로운 서버 인스턴스를 띄워 헬스체크가 통과되면 트래픽을 전환하고 기존 서버를 종료하는데, 이 전환 과정에서 데이터베이스 변경사항 손실 문제가 있었다고 회상, 이번 변경 사항으로 이 문제가 해결됐는지 궁금증 제기
          + 서버를 일회성 웹 서버 인스턴스가 아닌 프로덕션 데이터베이스 관점으로 봐야 한다는 의견, python/sqlite 웹앱 배포시 머신 전체를 바꾸지 않고 패키지만 업그레이드하고 systemd 서비스를 재시작하는 방식을 사용, 다운타임 줄이려면 SO_REUSEPORT 등으로 전환 과정을 고민할 수 있고, 이때 새 구버전 프로세스가 데이터베이스를 동시에 사용할 수 있지만 DB 스키마 변경이 포함되면 일정 다운타임은 불가피하다고 판단, 이는 다른 DB도 마찬가지일 수 있다는 견해
          + 쉽게 해결되진 않는 문제라는 입장, 여전히 한 writer만 lease를 잡을 수 있기 때문, 새 서비스가 실행돼도 이전 writer가 내려가야만 lease를 얻을 수 있음, writer 교체를 위한 도구는 제공되지만 요청 대기 또는 짧은 다운타임은 어쩔 수 없다는 설명
     * litestream으로 사용자별로 수많은 데이터베이스를 복제하려면(문서에서 다루는 대표 use case), 런타임에 새로운 데이터베이스를 동적으로 추가할 방법이 궁금하다는 질문, 현재 설정 파일이 정적이고 실시간 API를 찾지 못했다는 경험 공유
          + 이 문제는 결국 해결될 것으로 예상, 새로운 SQLite 탐지 로직이 까다롭지만 불가능하지 않다고 지적, 그 전까지는 라이브러리 형태로 쉽게 사용 가능하다는 안내
"
"https://news.hada.io/topic?id=21025","저는 과학을 위한 AI의 과장된 광고에 속았습니다. 이게 제가 배운것들 입니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              저는 과학을 위한 AI의 과장된 광고에 속았습니다. 이게 제가 배운것들 입니다

     * 플라즈마 물리학 연구에서 AI 활용에 대한 기대와는 달리, 실제 적용 결과는 과장된 성과 및 한계 중심임
     * AI를 이용한 PDE(편미분방정식) 풀이 방식(PINN 등)은 신뢰성과 성능 면에서 기존 수치적 방법보다 확실한 우위 제공 미비함
     * 약한 비교 기준(weak baseline) 과 보고 편향 때문에 AI 성과에 대한 논문 대부분이 실제보다 과도하게 긍정적인 평가임
     * AI의 과학적 활용이 급증하고 있으나, 과학 진보 혁신을 주도하는 도구라기보다 점진적·제한적 기여 가능성에 무게 둠
     * 과학 논문 구조 및 연구자 인센티브 탓에 실패 사례 미공개·과장 보고가 반복되며, AI의 과학적 영향 평가 시 본질적 회의적 시각 필요성 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 연구 배경

     * 필자 Nick McGreivy는 Princeton에서 플라즈마 물리학으로 박사 학위를 취득한 후, AI가 과학 연구(특히 물리학) 혁신에 기여할 수 있다는 기대감으로 머신러닝 활용 연구로 전향함
     * AI가 일렉트로닉스, 인터넷, 집적회로 등과 같은 범용 기술처럼 과학 전반을 획기적으로 바꿀 수 있을지에 관심을 가짐
     * 실제로는, AI를 활용한 PDE(편미분방정식) 풀이 연구에서 유명 논문의 발표된 성과 대비, 실제 적용 시 기대 이하의 결과 경험

PINN(Physics-Informed Neural Network) 적용 경험

     * AI를 이용한 PDE 풀이 분야에서 PINN이 대표적 방법으로 급부상하였고, 필자 또한 이 방식을 실험적으로 시도함
     * 기존 논문에서는 PINN이 고전 유체, 양자역학, 반응-확산 시스템 등 다양한 분야 PDE 문제에서 효과적 솔루션을 제공했다고 보고했으나, 실제로는 아주 간단한 PDE(1D Vlasov 등)에도 불안정하거나 신뢰성이 크게 떨어지는 결과 경험
     * 간단한 튜닝으로 개선이 어렵고, 복잡한 PDE(1D Vlasov-Poisson 등)에서는 아예 적절한 해 도출 실패함
     * 주변 연구자들도 유사한 실패를 경험하였으나, 이러한 부정적 결과는 거의 논문으로 발표되지 않음

PINN 실험을 통한 교훈

     * 영향력 있는 1차 논문 저자조차 특정 셋팅에서는 PINN이 실패함을 인지했으나, 설득력 있는 결과만 공개함
     * 과학 논문 생태계에서 긍정적 결과 위주 보고와 AI 관련 실패 실험 미공개 관행은 생존자 편향(survivorship bias) 심화 요인임
     * PINN 방식은 수치적으로 아름다운 개념이지만, 불안정성·미세조정 난이도·처리 속도 저하 등 실용적 한계로 인해 선택을 포기한 경험 공유함
     * 원 논문은 14,000회 이상의 인용을 받으며 수치 방법 분야 최고 인용 논문이나, 실제 PDE 풀이에선 기존 방법 대비 경쟁 우위 없음
     * 최근에는 PINN이 역문제(inverse problems) 등 특정 영역에서 효과를 발휘할 수 있다는 주장도 있으나, 이에 대한 연구자 간 논쟁 존재

부적절한 비교 기준이 유발한 과잉 낙관

     * 필자는 이후, 전통적 수치 기법과 마찬가지로 PDE 해를 격자나 그래프 픽셀 집합으로 취급하는 딥러닝 접근법을 시도함
     * 여러 논문에서 AI로 PDE를 기존 방법보다 최대 수천~수만 배 빠르게 해결한다고 발표하였으나, 실제로는 비교 기준으로 삼은 베이스라인(기준) 자체가 약한 방식에 불과한 경우가 대다수임
     * 대표 논문 분석 결과, AI가 강점을 보인다는 76편 중 60편(79%)은 충분히 성능 좋은 기존 수치 방법과 공정하게 비교하지 않은 것으로 판명됨
     * 이 같은 약한 비교 기준과 네거티브 결과 비공개로 인해 ""AI가 혁신적 성과""라는 평가는 실제보다 과장된 경향 확인됨
     * 관련 연구 결과는 학계와 산업 전반에 논란을 일으켰으며, 일부는 미래 연구 방향성 및 AI의 잠재력 강화를 주장, 일부는 현재 과대평가 문제 경계 심화 표명

과학에서 AI의 역할 및 한계

     * 대표적 성공 예는 AlphaFold의 단백질 접힘 예측, 기상 예보(예측 정확도 최대 20% 향상), 신약개발(임상 1상 성공률 상승) 등이 있으나, 광범위한 혁신보다는 기존 기술 대비 보완적·점진적 진전 위주임
     * 글로벌 빅테크나 언론, 학계 등은 AI의 ""과학 혁신적 도구"" 내지는 ""과학 패러다임을 바꿀 변혁의 주역""으로 포장하지만, 현재 수준 AI로는 기대만큼의 본질적 혁신 한계 명확히 존재

AI 채택 동기와 연구 생태계의 구조적 문제

     * 과학자들이 AI를 도입하는 주된 이유는 과학 자체 발전보다는 개인적 성과(더 높은 연봉, 경력, 논문 인용, 연구 자금 유치 등) 때문임
     * 실제로 AI 이용 연구자가 상위 인용 논문 및 연구 경쟁력 면에서 일반 과학자 대비 유리한 환경 제공 받는 현상 확인
     * AI 활용 연구자는 ""해결할 과학 과제""를 정의하기보다는, 애초에 ""AI로 풀 수 있는 과제를 뒤에서부터 찾아가는"" 구조적 함정 노출
     * 이로 인해, 실제 과학 발전보다는 AI의 잠재력 시연에 집중, 이미 해결된 문제나 부수적 효과만 도출하는 경우 많음

논문 보고의 구조적 한계와 과학 내 낙관 편향

     * 부정적 결과의 미보고(생존자 편향)로 인해, AI 활용 성공 사례만 쏟아지고 실패는 공개되지 않아, 전체 효과 평가 왜곡
     * 논문 구조상 데이터 누수, 약한 비교 기준, 체리피킹, 미보고 등 체계적 오차나 편향이 반복적으로 발생함
     * 평가자와 이해관계자가 동일한 공동체 내에 있어, 성과 평가는 이익에 직결되는 이해상충 구조에서 이루어짐
     * 이러한 현상은 과학 내 AI 영향 평가 시, ""영양학 논문에서 단일 연구 결과를 무조건 신뢰하지 않는 태도""와 비슷한 본질적 회의와 비판적 검증 습관 필요성 전달

결론

     * AI는 단기적으로는 과학 혁신을 이끄는 혁명적 도구라기보다, 기존 방식의 점진적·선택적 보완 수단일 가능성에 무게가 실림
     * 연구 생태계의 구조적 인센티브, 과대평가 및 실패 미보고, 약한 비교 기준 문제로 인해, AI의 실제 과학적 성과를 평가할 때 항상 비판적·회의적 관점 유지 필요성 강조
     * 이상적인 AI 혁신에는 구조적 개혁(도전 과제 출제, 실패 사례 공개, 공정 비교체계 발전 등)이 병행되어야 한다는 메시지 전달

        Hacker News 의견

     * 제목이 바뀐 건지 헷갈림 발생, 현재 제목은 ""I got fooled by AI-for-science hype—here's what it taught me""임
          + 제목이 원래에서 바뀐 상황, 개인적으로는 오히려 나빠졌다는 느낌, 원래 제목을 선호해야 하며, 이 논문의 원제목엔 문제가 없었다고 생각, 박사 과정 학생이 AI가 과학 연구에 기여한다는 의심스러운 사례들을 비판적으로 분석하는 내용임
          + 아니, 헛것이 아니라 실제로 바뀐 제목 확인, 아카이브 주소까지 예시로 제시
     * AI 기반 FEM 스타일의 구조 해석 솔버를 써본 “행운” 경험 있음, 선형 소규모 변형 문제에서 그럭저럭 쓸 만하지만, 복잡해지면 성능이 뚝 떨어짐, 기존 방식이 5분 걸려 정확한 해를 내는 동안, 30초 만에 대충 푸는 정도, 비선형 적용 시 완전히 망가짐, 아주 상위 레벨 개념 선정 정도로 활용 가능하나 이마저 미흡, 어떤 모델은 그냥 곡률 감지기 수준, 직선인 건 파랑, 곡률 큰 건 빨강, 나머지는 보간 수준
          + 결국 “second principles” 솔버에 가까운 느낌, 본 적 없는 상황에는 전혀 새롭게 해결 못하는 한계
          + 이런 모델을 반복 해법에서 프리컨디셔너로 쓸 수 있는지 궁금증
     * 새롭고 핫한 기술이 지나친 주목을 받는 현상에 항상 위험 존재, 기사에서 중요한 인용은 “대부분의 과학자가 의도적으로 남을 속이려는 건 아니지만, 유리한 결과를 보여야 할 강한 압박이 있어 결국 오도될 가능성 발생”, 누군가의 인센티브를 이해하는 게 정보를 해석하는 데 매우 유용하다는 점 강조
          + AI라는 단어만으로 돈과 펀딩을 찾는 이들이 존재, 실제로는 그저 기계학습이 들어간 소프트웨어가 대부분이며, 이건 오래전부터 있던 방식, 기술 자체가 크거나 정밀하지 않다는 생각
     * 결국 이건 학계의 고질적 문제 반복, 진실 탐구 대신 인용 수와 출세에 더 집중, AI도 그런 주제 중 하나일 뿐임
          + 일반화하기 싫지만, 독일 내 HPC 센터 몇 군데를 돌아다니며 본 패턴은, 물리학을 전공했다가 잘 안 된 이들이 많고 AI 관련 예산도 이들이 대부분 가져가 ML4Science 형태의 프로젝트가 남발, HPC 센터가 원래 물리학자만 위해 존재하는 곳이 아닌데 예산의 쏠림이 아쉬움, 독일은 AI 본연의 연구에 더 투자해야 한다고 느낌
          + 현실적으로 출세주의 문제는 학계가 점점 민간 시장 논리를 이어받으면서 생긴 부작용, 내가 소프트웨어 개발자로 배운 점은 모든 결정이 자기 이익, 커리어 위주라는 점, 누구나 자기 잘난 것만 신경 쓰고 일이 끝나면 남 책임, 이 마인드에 맞서지 않으면 오히려 불리해지는 환경, 결국 똑같은 결론으로 가면서 자기만 손해 보는 구조
          + ""no longer""라는 표현이 왜 쓰였는지 사실 이해가 안 됨
     * “몇 주 실패 후, 다른 대학 친구에게 연락하니 그도 PINNs로 좋은 결과 못 봤다”라는 대목에서, 연구에선 AI와 별개로, 끊임없는 협업의 중요성 실감, 남들이 이미 실패한 길을 다시 걷지 않도록 도와주는 장점
          + 연구자들이 실패한 실험도 논문으로 발표해야 하는 필요성 제기
          + 나에게 과학 AI 에이전트 개념이 별로 설득력 없어 보이는 또 다른 이유, 연구란 본질적으로 매우 협력 중심의 과정, 아무리 문헌 검토를 잘해도 실제 만나고 대화하지 않으면 좋은 연구자일 수 없다는 의문
     * AI 부스터는 아니지만, 부정적 결과가 논문화되지 않고, 논문에서 모두 자신 논문만 과대포장하는 건 AI만 문제 아님, 과학자 평가 방식과 학술지 산업 구조의 문제, 전통 미디어와 마찬가지로 청중 끌기에 집착하는 현상, 어쨌든 겨울이 오고 있다는 느낌
          + 종종 AI 논문에는 “GPU 수십억 개 동원에 무한 시간 돌리면 마법처럼 된다는 결과” 류, “비공개 실제 데이터셋으로 테스트했더니 최고” 류의 말만 반복, 큰 기업에서 나온 논문은 명백한 허점이 있어도 무시하고 넘어갈 수가 없음, 결국 자원 싸움, 나처럼 예산 적은 대학 연구자들은 재현도 못 하고, 논문에 나온 수치 그저 믿어야 하는 처지
          + 15년 전, AI 실용 논문을 쓴 후 다른 분야로 넘어갔다가 최근 다시 돌아옴, 전 분야에 만연한 문제지만 AI는 특히 명성과 돈을 쫓는 연구자가 몰림, 과장된 주장과 편집된 데이터도 더 심한 듯, 책임감 있는 연구자도 경쟁하려면 어느 정도 과장하게 됨
          + AI는 단순히 현재 유행의 자석일 뿐, 문제점이 더 선명히 드러나는 이유
          + AI는 특히 “그럴듯한 논문”을 쓰는 걸 더 쉽게 만들어주는 점
     * HN에서 AI/ML을 둘러싼 인식이 왜 이렇게 극명하게 갈리는지 의문, 지금껏 본 적 없는 새로운 영역, 문자 입력만으로 코드까지 생성하는 기능은 예전엔 없었음, 최근에 이미지 분할 스크립트를 UI까지 포함해 claude에게 시켰는데 1분 만에 생성, 이런 혁신적 예시 한둘이 아님, 이미지 생성도 신세계, 이 블로그 기사에도 과장이 있지만, 연구자 입장에선 AI로 코드 생산성만으로도 효율성이 크다는 점 충분, 더욱 흥미로운 건 데이터에 대한 인식 변화, 예전엔 “인터넷은 절대 잊지 않는다” 하던 것이 지금은 실제로 페이지 삭제와 캐시 기능도 사라지고, 데이터를 다루는 법을 점점 잊었음, 그런데 AI 등장 후 데이터의 가치가 다시 부상, 피드백을 주고 결과에 반영되는 강화의 시대에 진입, 하드웨어, 알고리즘, 데이터, 도구, 프로토콜 등 모든 방향에서 발전이
       진행 중, 아직 더 많은 실험과 GPU, 대형 데이터센터가 필요, 지금은 병목 상황, 대기업들이 몇 주, 몇 달을 들여 대모델을 트레이닝 중
          + “이미지 분할 코드 생성해줬다”는 말은 사실 화려하게 스택오버플로 데이터를 복붙하는 수준, 예전엔 Google로 정보 찾던 것과 같은 맥락, 외형상 새롭고 인상적이라도 본질적으로는 GIS에서 식당 찾는 수준으로 그치는 느낌, 현실에서는 전혀 reasoning 없이 데이터 바탕으로 상관관계 출력만 있을 뿐, 여전히 유용하지만 한계도 분명
          + HN에서 AI/ML에 대한 분위기가 다른 기술에 비해 왜 이렇게 갈리는지에, 각자 개인의 합리적 입장에서 해석할 필요, 혁신적이라는 시선과 데이터 도난, 프라이버시 경시에 대한 우려가 모두 합리적 근거, 다양한 시각이 있음을 먼저 인정하고, 내 입장을 잠시 내려놓고 상대방 입장을 제대로 이해하려는 태도 중요
          + 프로그래머의 근본 역할은 인간 언어를 컴퓨터 언어로 바꾸는 것, LLM은 그 경계를 명확히 침범하는 존재, 얼마나 깊게 진입할지는 불확실하지만 이미 장벽은 허물어짐, 이 상황은 두려움부터 위기 의식까지 다양하게 해석 가능, 수년간 갈고 닦은 고소득 스킬에 위협, 프로그래머가 완전히 대체되지 않더라도 연봉 수십만 달러를 간신히 지키는 정도면 충분히 위협적
          + HN은 원래 “지금 이 트렌디한 기술이 진짜냐 허상이냐”로 항상 분열, 다양한 기술에서 이런 논쟁은 반복, 때로는 내 입장도 달라졌던 경험, 결국 겉으로 보기와 크게 다르지 않다는 생각
          + 이와는 반대로, “AI가 과학을 혁신할 것”이라는 담론은 근거보다 너무 앞서간 느낌
     * 기사 처음엔 AI 전체가 과장이라고 보이나 실제로 문제 삼는 건 PINN이라는 특정 아키텍처, 마지막엔 DL 모델로 PDE를 더 빠르게 푸는 성과도 언급
          + PINN만의 문제가 아니라 훨씬 광범위, PINN이 별로라는 건 오랫동안 알려졌지만, 물리 문제에 ML을 쓰는 일반적 실패도 만연, ML이 잘 먹히는 상황은 (1) 데이터가 아주 많고 좁은 분야일 때(MLIP 등), (2) 어마어마한 데이터와 대형 모델을 쓸 때(Alphafold), 그런데 대부분의 물리 문제 ML은 그 중간쯤, 실험 데이터 부족&비싼 시뮬레이션, 데이터셋/모델 크기도 애매, 결국 다들 도전해보고 실패, 그래도 논문 냄, 유명 랩이나 PI/특이하게 보이면 좋은 학회지 실어주고 인용 수만 올라감, 결국 데이터 일부만 복제하는 한계, 다른 사람이 범용성 강화에 집중해야 한다는 결론만 남음
          + 저자가 한 건 PINN에 한정 안 됨, 여러 모델을 체계적으로 분석한 논문도 썼고, 별도 섹션까지 있음
          + PINN을 어떤 AI 솔루션으로 바꿔도 여전히 과장된 부분 발견, 지금까지 현실적으로 AI의 쓸모도는 “전문가들의 단순작업 자동화 & 3중 검증 필요” 수준이 사실적 평가
     * 훌륭한 분석과 예시, 또 다른 문제는 AI 논문이 대부분 새롭고 “정식” 저널에 실리는 비중이 적어도, 인용 수는 많다는 것, 재현이나 주장 검증이 정말 힘듦, 연구 방식과 데이터가 해마다 바뀌기 때문, 결론이 과거 모델 특성 때문인지, 일반화할 수 있는 결론인지도 모호
          + 나는 과학자나 연구자는 아니지만, 통계나 데이터 해석에 기반한 결과는 언제나 의심부터 하게 됨
     * 블로그 소유자 이름(“Timothy B. Lee”)을 보고, 70살 넘은 HTTP와 웹의 발명자가 이런 첨단 블로그를 한다는 사실에 놀람
"
"https://news.hada.io/topic?id=20961","Show GN: 실시간 지하철 지연시간 서비스 (Metronow)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: 실시간 지하철 지연시간 서비스 (Metronow)

   안녕하세요. 긱뉴스를 즐겨보는 사람입니다.

   실시간 지하철 지연시간 프로젝트를 진행하였습니다.
   SHOW GN으로 처음 글을 올려보네요.

   지인이 항상 출퇴근할 때 지하철을 타는데 매번 같은 열차가 지연된다고 하는 것을 듣고,
   지연시간 패턴을 파악할 수 있지 않을까 생각해서 시작하게 되었습니다.

   다른 실시간 지하철 서비스와 다른 점은 ""지연시간""을 제공하는 것입니다.
   또한, 시간표 탭에서는 자주 지연되는 열차를 표시하였습니다.

   지하철 지연시간을 지속적으로 수집하였고,
   수집한 지연시간을 바탕으로 자주 지연되는 열차를 판별하였습니다.

   실시간 데이터가 제공되지 않는 노선은 아쉽게도 지연 데이터를 수집하지 못하여, 지연정보를 제공하지 못하고 있습니다. 실시간 데이터가 제공되는 경우, 즉시 지연정보도 제공할 예정입니다.

   지연시간은 실시간 데이터와 시간표 데이터를 대조하여 계산하였습니다.

   UI와 기능을 지속적으로 추가하고 업데이트 할 예정입니다!
   특히, 지연정보를 더 정확하게 산출할 수 있도록 고도화해볼 예정입니다.

   아래 링크에서 결과물을 확인하실 수 있습니다.
   웹: https://metronow.info

   부족한 부분이나 틀린 정보에 대해서 피드백 부탁드립니다!
   읽어주셔서 감사합니다.

   역에서 표현하는 정보가 한정적이어서 헛갈릴 때가 많았는데 감사합니다!

   좋게 봐주셔서 감사합니다!

   잘 읽었습니다! 종각역에서 1년 좀 넘게 1호선으로 수원까지 출퇴근하면서 얻은 경험상 자주 지연되는 열차는 정말 잘 맞는것 같습니다!
   추가적으로, 지연되는 열차는 뭔가 사전징후가 있었던것 같은데, 간단한 ML모델등을 통해 프로젝트를 확장해서 열차 지연을 예측해보는것도 정말 의미있을것 같습니다!

   저도 진짜 맞을까 궁금했는데 경험 공유해주셔서 정말 감사합니다!
   아이디어도 공유해주셔서 감사합니다.
   추후 프로젝트 개선에 활용해보겠습니다!!

   멋져여~

   감사합니다!

   오 신기하네요~
   자주 지연되는 열차 판별은 머신러닝이나 딥러닝으로 되는건가요??

   댓글 감사합니다!
   현재 자주 지연되는 열차는 룰 기반으로 판별하고 있습니다.
   추후에 예상지연시간을 머신러닝이나 딥러닝 등으로 계산해볼 계획입니다!

   좋군요~

   좋게 봐주셔서 감사합니다!

   검색하고 역을 선택하면 하얀 화면만 나오네요! 아이폰 환경입니다~

   피드백 감사합니다! 현재 수정되어 하얀화면 없이 정보 확인하실 수 있습니다.

   상수역, 가양역등을 클릭하면 흰 화면만 나오는데, 혹시 저만 그런걸까요?
   macos 크롬입니다~

   피드백 감사합니다! 현재 수정되어 해당역 정보 확인하실 수 있습니다.
"
