"https://news.hada.io/topic?id=22647","Sequoia, Zed를 후원","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Sequoia, Zed를 후원

     * Sequoia Capital이 주도한 Series B 투자로 Zed는 4,200만 달러 이상의 총 자금을 확보함
     * Zed는 초고속 IDE 개발에 이어, 코드와 대화가 항상 연결되는 새로운 협업 방식을 추구함
     * 기존의 Git 기반 협업은 스냅샷 기반이라 실시간 및 AI 에이전트와의 연속적인 협업에 한계가 있음
     * Zed는 DeltaDB라는 운영 기반 버전 관리 시스템을 개발하여, 세밀한 변경 이력과 코드 관련 논의를 코드에 영구적으로 연결함
     * Zed와 DeltaDB는 오픈소스로 공개되며, AI와 개발자가 함께 일하는 미래의 소프트웨어 개발 경험을 구축 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Zed Series B 펀딩과 비전

   Zed는 Sequoia Capital이 주도한 3,200만 달러 규모의 Series B 투자를 유치했으며, 기존 투자자들도 참여하여 총 투자액이 4,200만 달러를 넘었음

   4년간 세계에서 가장 빠른 IDE를 구축하는 것에 집중했으며, 이는 곧 새로운 소프트웨어 협업 방법에 대한 기반임

   Zed의 목표는 코드에 대한 대화가 각각의 코드와 함께 항상 연결되게 하여, 낡은 스냅샷이나 흩어진 도구들로 인해 생기는 문맥 손실 문제를 해결함

   처음에는 고성능 에디터를 만든 것이 첫걸음이었고, 이번 투자로 다음 단계인 운영 기반 버전 관리 개발과 팀·AI 에이전트와의 원활한 협업 기능 확장이 가능해졌음

협업의 한계와 Zed의 접근

   소프트웨어 개발은 자신이나 팀원, 그리고 이제는 생성형 AI 모델과의 지속적인 대화의 결과물임

   기존 협업 도구들은 대화와 인사이트가 코드 그 자체가 아닌 외부 도구나 스냅샷에 분산됨

   Git은 커밋과 브랜치를 통한 협업은 가능하지만, 커밋 전까지는 독립적인 작업 복사본에 갇히게 됨

   Pull Request에서 변경 코드에 대한 대화는 쉽지만, 특정 코드 영역이나 옛 버전과의 대화는 번거롭게 스냅샷에 의존하게 됨

   스냅샷이 구식이 되거나 메시지가 오래되면, 대화가 최신 코드와의 연결을 잃고, 중요한 맥락이 사라짐

AI와의 협업에서 드러나는 스냅샷의 한계

   AI 에이전트와의 실질적인 협업에서는, 커밋이나 Pull Request에만 의존해서는 효율적이지 않음

   AI와 빠른 피드백 루프를 돌려야 하지만, 매번 스냅샷을 만들며 대화하는 것은 현실적이지 않으며, 실제 도구들은 비동기적 커밋 흐름만 지원함

   AI와의 모든 상호작용을 커밋 기반 워크플로로 강제하는 것은 현대 협업의 필수 요구를 충족시키지 않음

   현재 AI 에디터들의 패치 방식은 일시적 문제만 다루고, 연속적인 대화라는 본질적인 협업 방식을 지원하지 못함

DeltaDB와 새로운 버전 관리 패러다임

   Zed는 커밋 단위가 아닌 각 에디트와 논의 과정 자체를 기록하는 시스템을 구축중임

   이 시스템은 모든 편집, 결정, 대화가 코드와 함께 축적되며, 더 이상 커밋의 경직된 구조에 종속되지 않음

   이를 위해 DeltaDB라는 새로운 버전 관리 시스템을 개발함

   DeltaDB는 CRDT(Conflict-free Replicated Data Type) 를 활용하여, 실시간으로 변경사항을 처리하고 동기화함

   Git과 연동하면서도, 실시간 상호작용이나 비동기 협업에도 대응하며, 코드상 어떤 위치든 영구적으로 변화 이력을 영속화할 수 있음

개발자 경험의 혁신

   Zed의 목표는 코드베이스를 살아 있는 소프트웨어 진화의 히스토리로 만들고, 사람과 AI가 함께 논의한 모든 정보와 맥락을 코드에 연결함

   이는 단순 코드가 아닌, 어떻게, 왜 해당 코드가 만들어졌는지의 배경 정보까지 축적하여, AI에게도 풍부한 문맥 제공이 가능함

   예를 들어, 새로운 엔지니어가 에러 트레이스를 추적하며 문제 라인을 선택하면, 그 코드가 왜 쓰였는지, 어떤 논의와 AI의 추정이 있었는지 알 수 있음

   즉석으로 책임자와 논의를 시작하고, 오디오 콜로도 연결하며, 이 기록이 자동으로 해당 코드와 연결되어 유지됨

오픈소스 전략 및 미래 방향

   Zed는 오픈소스로 공개되어, 누구나 사용할 수 있으며 유료 서비스도 선택 가능함

   DeltaDB 역시 같은 전략으로 진행하고, 향후 개발 상황에 맞춰 자세한 정보를 공개할 예정임

   팀원과 AI가 모두 참여할 수 있는 새로운 협업 방식을 실현할 기술·비전·자금을 확보했으며, 혁신적인 개발문화에 관심 있는 개발자 채용도 진행함

Zed 에디터 체험 및 채용

   macOS와 Linux에서 Zed를 직접 사용해볼 수 있으며, 다운로드가 가능함

   협업, 에디터 개선, AI/머신러닝, 폰트 렌더링 등 다양한 분야에서 엔지니어와 프로덕트 디자이너를 모집 중임

   관심이 있다면 공식 채용 페이지에서 지원 가능함

        Hacker News 의견

     * 나는 Zed의 철학과 저수준 구현 세부사항까지 전부가 정말 세련됐다는 느낌을 받음, 코드도 GPUI부터 전반적으로 수준이 높아 공부 대상으로 매우 흥미로움임. 다만, 에디터가 벤처캐피털(VC) 투자를 받는 건 개인적으로 우려됨. 실용적이긴 하지만 에디터에 VC가 들어오는 건 마음에 걸림
          + Sublime Text가 이미 17년 전에 쉐어웨어 모델로 이 문제를 해결했음. 그리고 Sublime Text는 Zed보다 빠르고, 리눅스/윈도우/맥OS 모두에서 잘 돌아가며, 커스터마이징도 괜찮음
          + 나도 Zed를 써보니 꽤 마음에 들었고, 아예 넘어갈까 생각도 했었음. 근데 VC가 투자한 사실이 결정적으로 마음을 접게 만듦
          + Zed가 존재했을 때는 멋진 아이디어였음. 이젠 Zed가 사라지겠지만, 다른 에디터들이 Zed에서 얻은 교훈을 잘 받아들이길 바람
          + VC 자금을 받게 되면 결국 투자금에 대한 수익을 요구하게 됨, 그게 'enshitification'(서비스 질 하락) 현상을 어떤 식으로든 Zed에도 불러올 거라 봄. 점점 더 많은 기능들이 유료 구독 뒤에 숨겨지고, 오픈소스 코어는 점점 소홀해질 거라 예상함. 나는 자유 소프트웨어 OS와 도구들만 쓰는 사람으로서, Warp나 Zed처럼 VC가 들어온 혁신툴엔 전혀 관심 없음
          + Sequoia가 누군지도 잘 몰랐는데, Altman이니 Huang이니 Musk 같은 이름들이 보이면 더 꺼림칙함
     * 나는 이런 주장은 받아들일 수 있음
          + 텍스트 에디터로도 돈을 벌 수 있음 하지만 다음 주장들은 매우 회의적임
          + 텍스트 에디터 하나에 4,200만 달러라는 자본을 투입해도 진짜 의미 있는 변화로 연결시킬 수 있음
          + 텍스트 에디터가 평생 누적 이익(물가반영 기준)으로 4,200만 달러를 넘길 수 있음 Sequoia는 이런 점을 회의적으로 보지 않고 돈을 투자한 듯함. 아마 역대급 투자를 끌어낼 발표자료가 있었을 듯함
          + 반론으로, Cursor의 매출은 3~4억 달러 수준에 이른다는 얘기도 있음. 그래서 4,000만 달러 이익 자체가 전혀 불가능한 일은 아닌 듯(나 역시 의심스럽긴 함)
          + AI 텍스트 에디터 시장임
     * 요즘 AI 에디터가 너무 많이 나오다보니 Zed 팀도 참 어려운 상황에 놓였다고 느낌. 장인정신을 고수하며 최고의 에디터를 만들려고 하면, Cursor 같은 대형 업체에 따라잡힐 수 있음. Cursor는 갑자기 등장해 2년 만에 매년 3~4억 달러 매출을 올리는데, JetBrains가 이 수준 도달하는 데는 20년 걸렸음. 이런 기세면 Cursor는 돈으로 품질도 따라잡을 수 있음. 반면, Zed가 VC 자금에 더 의존하게 되면 AI 쪽에 더 투자할 가능성이 높은데, Zed는 사실 그걸 내키게 하려는 분위기는 아님. 그래도 살아남으려면 필연적으로 그런 결정을 해야 할 것 같음. 이 팀한테 정말 동정심이 생김. 직접 소통해본 바로는 모두 실력 있고 착한 분들이었음. 진심으로 응원함
     * 나는 Zed로 개발하는 게 너무 즐거움. 사용하는 맛이 정말 뛰어나고, Agentic 코딩 통합도 정말 잘된 사례라 봄. 더 많이 투자해서 이 분야가 성장하는 게 기대됨. VC 펀딩에 대한 우려도 이해하지만, 제대로 만든 제품을 만들려면 어느정도 자본이 필요함. Cursor 같은 경쟁사에 비하면 투자 규모도 아직 작은 편임. 그래서 개인적으로 Zed가 훨씬 더 좋은 제품이라고 확신함. Zed가 오픈소스(OSS)라는 점은 정말 커뮤니티에 큰 선물이라 생각하고, DeltaDB도 그럴 거라고 조심스레 기대함. 그리고 CEO인 Nathan도 정말 좋은 사람임. Zed의 행보를 축하함
          + “Nathan(CEO)이 정말 좋은 사람이다”라는 얘기가 많길래, 다른 시각에서 Nathan이 커뮤니티의 여러 우려에 대해 어떻게 답변하는지 보면 흥미로울 것 같음. 관련 논의
     * 이번 투자가 Zed에 좋은 일인지 나쁜 일인지는 앞으로 알 수 있을 것 같음. 근데 Sequoia 입장에선 항상 아주 큰 수익 가능성을 노리는 거라 생각됨. 유료 사용자가 있다고 해도 “단순히 에디터인 제품”으로 어떻게 10배 수익을 거둘지 그 포인트가 궁금함. 혹시 그 이상의 큰 그림이 있는지 의문임
          + JetBrains가 좋은 사례가 될 수 있음. 게다가 JetBrains는 오히려 더 많은 IP를 가졌음
     * 관련해서 진행중인 쓰레드 정보임:
          + <i>Zedless: Zed fork focused on privacy and being local-first</i> - https://news.ycombinator.com/item?id=44964916
          + <i>Zed for Windows: What's Taking So Long?</i> - https://news.ycombinator.com/item?id=44964366
     * IntelliJ가 “나쁜” 툴일까? 여기 반응들이 너무 부정적인 건 아닌지 궁금함. 이번 투자는 회사에 자금이 들어와서 개발도 계속되고, zed도 계속 발전할 거란 뜻 아님? 내 관점에선 IntelliJ 라이선스 정도면 충분히 괜찮은 조건임
          + IntelliJ는 CLion 등등 만들면서부터 방향성을 잃었음. 나도 오랫동안 고객이었는데 “All-in-one IDE”라는 구호에서 벗어나 점점 더 유료화로 나아갔음. 변화도 느려지고, 성능도 나빠졌고, 버그 리포트도 2년 넘게 방치되는 경우 많았음. 요즘은 VSCode가 UX 빼곤 IntelliJ를 다 압도하는 수준이고, 플러그인으로 커스터마이징하면 거의 동급임
          + 연간 300달러 정도를 내고 IDE를 쓰는 개발자는 거의 없음. 그리고 Intellij는 VC 투자를 받은 적이 없음. 그래서 언젠가는 Zed도 Intellij보다 훨씬 더 공격적인 수익화가 필요해질 거라 예상함
          + UI 리디자인 이후로 방향성을 약간 상실한 느낌임
          + IntelliJ는 내 노트북 워크스테이션에서도 항상 엄청 느렸음. 반대로, 성능을 최우선으로 둔 에디터(zed 등)가 신선하다 느껴짐
     * DeltaDB는 >git이 코딩 그 자체에 혁신을 부여한다는 느낌임. 최근 Nathan Sobo와 Steve Yegge 토론에서도 Zed가 약속한 부분을 실현시켜줄 거라 예상함. 이건 기존 방식과 새로운 방식 모두에서 점점 커지는 문제를 해결할 수 있을 것으로 보임. 특히 LLM(대형 언어모델) 채팅의 노이즈, 데이터 규모의 넘사벽을 극복할 수 있다면, 코드 일관성(coherence) 유지에 핵심이 될 듯함. 향후 3~6개월 내 90%의 코드가 LLM 생성 코드가 될 것이란 예측도 많은데, DeltaDB가 진짜 잘 구현된다면 일관성 문제에 사활이 달릴 수도 있음
          + 링크 안 누르는 사람을 위해 간단히 요약하면 5개월 전 기사이고, 현재 코드의 90%가 LLM으로 쓰여질 거란 예상을 담고 있음. 자극적인 카피라 계속 반복해서 나올 것이고, 실제로 맞는 말이라곤 볼 수 없음. “치과의사 9명 중 10명은 우리 양말을 추천한다” 같은 허세 마케팅 문구나, 이미 하늘을 나는 자동차가 널렸다는 얘기처럼 믿기 힘든 주장임
     * Zed가 언어 서버 기능 돌린다고 Node.js 배포판 전체를 다운로드 하려고 하는 문제(혹은 '특성') 고쳐졌는지 궁금함
          + 나도 그게 Zed에서 가장 불편한 점임. 바이너리를 자동으로 다운받고 실행하려고 하는 성향이 강함. 그래서 이 부분을 없앤 라이트 포크를 만들어봤는데, 모든 경우를 다 막진 못한 듯함. 그래도 전반적으로는 아주 괜찮은 에디터라 생각함
          + 그게 Zed의 장점임. 모든 게 그냥 잘 동작함
     * Zed의 CEO를 직접 만난 적이 있음. 굉장히 겸손하고 기술적으로 뛰어난 분임. 잘 되고 있는 걸 보니 기쁨
"
"https://news.hada.io/topic?id=22671","FFmpeg 8.0 릴리즈 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             FFmpeg 8.0 릴리즈

     * FFmpeg 8.0 ""Huffman"" 은 Vulkan 연산 기반 코덱과 하드웨어 가속 디코딩·인코딩, 여러 신형 파일 포맷과 필터가 추가됨
     * 인프라를 전면 현대화하였고, 기여 프로세스와 코드 품질도 강화함
     * VVC 디코더 안정화, xHE-AAC 디코더, MV-HEVC 및 LC-EVC 지원 등 주요 오디오 및 비디오 코덱 영역도 진보하였음
     * 오픈 소스 멀티미디어 기술 발전의 중심 역할을 수행하며, 지속적 기능 개선과 보안성 향상을 이어가고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

FFmpeg 소개

     * FFmpeg은 완전한 범용 멀티미디어 처리 툴킷으로, 음성 및 영상을 녹화, 변환, 스트리밍하는 데 유연하고 강력한 솔루션
     * ffmpeg -i input.mp4 output.avi와 같이 간단한 명령어만으로 영상 및 음성 처리가 가능함

  2025년 8월 23일, FFmpeg 8.0 ""Huffman"" 출시

     * FFmpeg 8.0 ""Huffman"" 이 공개됨. 수차례 지연과 인프라 최신화 과정을 거치면서, 지금까지 가장 방대한 규모의 릴리스가 이루어졌음
     * 새로운 기능에는 APV, ProRes RAW, RealVideo 6.0, Sanyo LD-ADPCM, G.728 등 네이티브 디코더 추가, VVC 디코더의 IBC, ACT, Palette Mode 지원 강화, Vulkan 연산 기반 FFv1 (인코드·디코드), ProRes RAW (디코드 한정) 등의 코덱이 포함됨
     * Vulkan 기반 하드웨어 가속 디코딩(예: VP9, VVC, H264/5)과 인코딩(AV1, H264/5), 다양한 새로운 *포맷(MCC, G.728, Whip, APV)*과 *필터(colordetect, pad_cuda, scale_d3d11, Whisper 등)*이 도입됨
     * Vulkan 1.3에서 동작하는 연산 셰이더 기반 디코더 및 인코더 계열이 새롭게 추가됨. 별도의 특수 하드웨어 가속기가 필요 없는 구조로, hwaccel API와 동일하게 동작함. 인코더 사용 시 새로운 인코더를 지정해야 하며, FFv1(인코드·디코드)와 ProRes RAW(디코드)만 현재 지원됨. ProRes(양방향)와 VC-2(양방향)는 준비 중임
     * 이 구조는 병렬 디코딩 최적화 코덱에만 적용될 수 있으며, 향후 더 다양한 분야에서 높은 성능 개선과 비선형 영상 편집·무손실 녹화 등 새로운 활용이 기대됨
     * 프로젝트 인프라도 대폭 최신화됨. 메일링 리스트 서버를 완전히 교체하였고, 이제 code.ffmpeg.org에서 Forgejo 기반 코드 협업을 지원함
     * 사용자는 최신 버전 업그레이드를 권장함

        Hacker News 의견

     * FFmpeg 개발자들과 기여자들에게 감사를 전함
          + 오디오/비디오 자동화가 필요할 때마다 FFmpeg을 항상 사용해왔음, 온라인 비디오 툴들도 대부분 이 툴을 핵심 엔진으로 사용하거나 UI 래퍼로 활용하고 있음
          + 오늘 FFmpeg.Wasm 프로젝트(https://github.com/ffmpegwasm/ffmpeg.wasm)가 있다는 것도 알게 됨
          + 2024년 1월에 1993년 애니 영화의 프레임을 15분 단위로 추출한 뒤, Real-ESRGAN-ncnn-vulkan(https://github.com/xinntao/Real-ESRGAN-ncnn-vulkan)을 사용하여 업스케일링하고, 결과 프레임을 4K로 다시 조합했던 경험 공유
          + 만약 이 작업 흐름에 UI를 추가했다면, 요즘 인기 있는 Topaz AI 같은 툴이 될 수 있었을 것이라는 생각을 해봄
          + 업스케일된 샘플 이미지도 공유함(샘플 이미지)
          + 직접 ffmpeg를 쓰지 않을 때조차 이를 내장한 도구들을 자주 사용함
               o 최근 오래된 DVD에서 추출한 저화질 애니를 k4yt3x/video2x를 통해 업스케일함, 설치가 쉬웠고 libffmpeg가 내장돼 있었음
               o 인코딩 시 FFmpeg와 동일한 인자를 쓸 수 있었음
               o 최적 인자 선택을 위해 ffmpeg로 짧은 장면을 여러 파라미터 세트로 인코딩, 다시 ffmpeg로 스틸 이미지를 추출해 비교함
     * FFmpeg가 컴퓨트 셰이더 기반 비디오 엔코더와 디코더를 도입한 점을 기쁘게 생각함
          + 하드웨어에서 널리 지원되는 코덱은 H.264, H.265, AV1 정도라서, 다른 코덱도 플랫폼 가속이 가능하면 좋을 것임 (효율이 좀 떨어지더라도 의미 있음)
          + 새로운 ProRes 인코더가 현재 진행 중인 프로젝트에 유용하게 느껴짐
          + 널리 사용되는 일반적인 코덱들은 병렬 디코딩에 적합하지 않아 지원이 어렵다는 설명이 이해감
          + 수만 개의 스레드를 사용해야 하는데, 프레임 간, 타일 간 데이터 의존성 때문에 병렬화가 쉽지 않음
          + 인코더는 디코더보다 융통성이 좀 더 있을 것 같은데, VP9(https://blogs.gnome.org/rbultje/2016/…) 같은 걸 컴퓨트 셰이더로 인코딩하는 건 도전적인 과제일 것 같음
          + 비디오 인코더/디코더를 컴퓨트 셰이더로 구현한 것에 대한 기쁜 소식을 다시 한 번 공유함
               o 과거에는 표준화된 인실리콘 하드웨어 인터페이스만 있었기 때문에 Vulkan 기반 인코더/디코더가 범용적인지 물었다가 웃음샀던 경험 있음
               o 이러한 개선이 구형 하드웨어에서도 제공되는 점이 정말 멋짐, 이런 식으로 새로운 코덱 및 전체 품질 향상 루트를 열 수 있기 바람
          + 디코더 최신 동향을 10년 넘게 살펴보지 않았지만, 직관적으로는 GPU 가속이 픽셀 데이터로 바뀌는 후반 처리에 큰 도움이 될 거라 예상함
               o 초기 디컴프레션은 CPU에서, 이후 디컴프레션된 데이터를 GPU로 전송해 최종 변환 작업(모션 벡터 적용, iDCT 등)을 하면 될 것 같음
               o 완성 프레임이 이미 GPU 텍스처로 있으면 디스플레이 오버헤드도 적음
               o 내 직감이 어느 정도 맞았는지 궁금함
          + FFmpeg 메인테이너들의 재능에 매번 놀라움을 느낌, 이런 고난이도 작업을 무료로 해주는 점이 대단함
          + 이 릴리즈 노트가 매우 흥미로움
               o 최근 몇 주 동안 WebGPU 컴퓨트 셰이더로 ProRes 디코더를 제작했는데, 충분히 빠르게 동작함 (Apple은 아마도 자체 하드웨어 가속을 쓸 것이라 생각함)
               o 이 경로가 Android의 새로운 APV 코덱에도 잘 맞을 것 같음
               o ProRes 비트스트림 스펙은 SMPTE에 제출되어 있지만, ProRes RAW에 관한 정보는 찾기 힘들었음
               o 소프트웨어 및 컴퓨트 기반 구현이 등장해 매우 흥미롭다고 느낌, 아마 ffmpeg 개발자들이 리버스 엔지니어링한 듯함
               o 코드 대강 보니 일반 ProRes와 크게 다르지 않아 보임
               o ProRes 스펙 문서
     * FFmpeg를 사용할 때마다 감탄하게 됨 (매뉴얼을 다시 찾아보거나 LLM의 도움을 받을 필요가 있긴 하지만, 시각적 옵션에서 명령을 만들어주는 GUI를 사용할 때조차도 느낌)
          + 이제는 필수적인 트랜스코딩 멀티툴이 된 듯함
          + Vulkan 1.3 기반 프로세싱을 도입한 건 좋은 결정으로 생각함
          + 어제 보니 Asahi Linux on Mac도 같은 표준을 지원한다는 것도 알게 됨
          + FFmpeg 인자는 ‘원조 프롬프트 엔지니어링’에 해당한다는 재치 있는 표현을 남김
          + LLM과 FFmpeg, ImageMagick 같은 복잡한 커맨드라인 툴은 환상적인 조합임
               o 원하는 작업을 자연어로 설명만 하면 바로 실현되는 완벽한 미래형 UI/UX라고 느낄 정도임
               o 예시로 폴더의 모든 이미지를 특정 크기로 크롭, 채도 조정, TIFF로 저장, 영상 루프로 조립해 웹용 인코딩까지 한 번에 처리하는 시나리오를 상상해봄
          + LLMs는 FFmpeg용 인터페이스로 훌륭하게 작동함
               o 자연어로 사용할 수 있게 도와주는 도구들이 많이 있고, 본인의 스크립트도 공유함(https://github.com/jjcm/llmpeg)
     * ffmpeg로 복잡한 CLI 명령을 만드느라 노력의 50%를 허비하고, 나머지 50%는 셸 이스케이프와 싸우느라 쓴다는 농담 섞인 현실을 공유함
          + 특히 텍스트를 영상에 입힐 때 텍스트 이스케이프가 매우 어렵게 다가옴
          + 파이썬에서 많은 인자(필터 등)와 함께 ffmpeg를 부를 때 완벽한 방법을 찾은 사람이 있는지 궁금함 (r-strings? heredocs? 등)
     * FFmpeg 다양한 기능을 쉽게 다룰 수 있는 좋은 GUI 프론트엔드가 있는지 궁금해함
          + 때로는 그냥 리멕스(remux)만 필요하거나, 동일 코덱의 영상/오디오 스트림을 결합만 하고 싶을 때가 있음
          + 영상 결합은 쉽게 들리지만 생각보다 변수와 이슈가 많다는 점을 강조함
               o 타임베이스, 시작 오프셋, 프레임 크롭, FPS 차이, B 프레임, 오픈 GOP 등의 변수로 인해 특정 재생 환경에서 문제 생길 수 있음
               o 오디오 역시 샘플레이트 차이 등 다양한 변수가 존재함
               o Lossless-Cut 프로그램이 언급되었는데, 호환성 검사 기능이 유용함
               o 하지만 영상을 MPEG-TS로 변환한 뒤 합치는 게 여러 문제를 우회하는데 좋음, 램디스크에서 빠르게 처리 가능함
               o 예시로 사용할 수 있는 ffmpeg 명령줄 시퀀스도 공유함
          + Handbrake가 그 역할을 잘 수행해줌
               o 기능 면에서 충분하며, 약간 올드하긴 하지만 여전히 유용하게 사용 가능함
          + 맥 사용자라면 ffWorks(https://www.ffworks.net/index.html)를 추천함
               o 대부분의 기능을 쉽게 접근할 수 있고, 배치 처리 및 프리셋 설정도 지원함
               o 개발자가 매우 적극적으로 지원해줘서 가장 좋아하는 앱 중 하나이며, 가치가 높아 도네이션도 하고 있음
               o Handbrake와 Losslesscut도 훌륭하지만, ffWorks의 완성도를 경쟁할 앱이 다른 플랫폼엔 없는 듯함
          + 본인에게 가장 좋은 프론트엔드는 ChatGPT라고 생각함
               o 자연어로 원하는 작업을 설명하면 굉장히 정확한 ffmpeg 명령어를 생성해줌
          + Lossless-cut 프로그램 확인을 권장함
     * FFmpeg의 변경내역(Changelog)을 확인할 수 있는 링크(https://github.com/FFmpeg/FFmpeg/blob/master/Changelog)를 공유함
     * 흥미로운 소식임을 짧게 표현함
          + 관련 동영상(https://youtu.be/9kaIXkImCAM?si=b_vzB4o87ArcYNfq)도 함께 공유함
          + 이 동영상이 풍자인지, 진지한지 궁금해함
     * ffmpeg가 ssl, zlib, sqlite 다음으로 가장 많이 쓰이는 라이브러리 4위가 아닐까 하는 개인적 의견을 제시함 (비디오가 2025년에는 정말 어디에나 존재한다는 전제에서)
          + 동의는 어렵다고 봄, 비디오 처리는 주로 미디어를 받는 서버에서 필요하기 때문임
               o 대부분의 휴대폰이 ffmpeg로 영상 처리하지 않는다는 점을 언급함
          + curl이 더 상위권일 수도 있고, ""SSL""은 구현체가 다양해서 수치가 분산될 것임
          + NixOS 인프라의 fastly 메트릭 로그(https://github.com/NixOS/infra/blob/main/metrics/fastly/README.md)를 데이터 소스로 제안함
          + Qt, libpng, libusb 등 ffmpeg보다 더 많이 쓰이는 라이브러리들이 꽤 된다고 생각함
          + Arch Linux의 패키지 통계(https://pkgstats.archlinux.de/packages)도 확인해볼 만함
     * Vulkan 컴퓨트 셰이더 구현이 특히 FFv1과 ProRes RAW에서 멋지다고 생각함
          + 고정 기능 하드웨어 디코더를 완전히 우회하므로 메모리 대역폭 영향이 어떻게 되는지 궁금함
          + FFv1의 컨텍스트 적응형 산술 부호화는 본질적으로 연속적이라 속도 낼 수 없을 것 같지만, 매우 큰 속도 향상을 얻고 있다는 점에 놀람
          + 여러 심볼을 동시에 병렬화하거나 슬라이스 단위로 병렬화하는 방식인지, 전통적으로 GPU로 산술 부호화 가속이 힘든 이유가 있었는데 ffmpeg 팀이 어떤 트레이드오프를 했는지 궁금함
          + 해당 구현의 프로파일링 경험이 있는 사람이 있다면, 엔트로피 디코딩 단계에서의 점유율과 대역폭 선택에 대해 듣고 싶음
     * ffmpeg는 너무나도 많은 도구의 기반을 이루고 있음
          + 대중은 미디어 업계에 ffmpeg가 얼마나 기여했는지 잘 모르는 경우가 많음
          + 오디오/비디오 자동화가 필요할 때마다 항상 찾게 되는 툴임
"
"https://news.hada.io/topic?id=22701","Positron – 차세대 데이터 사이언스 IDE","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Positron – 차세대 데이터 사이언스 IDE

     * Positron은 RStudio 제작사 Posit이 새롭게 개발한 차세대 데이터 사이언스 전용 IDE
     * 이 IDE는 R과 Python을 기본 지원하며, 여러 언어를 혼합 사용하는 데이터 사이언스 환경에 적합하도록 설계된 멀티랭귀지(polyglot) 플랫폼
     * 구조적으로는 VS Code의 Code OSS를 기반으로 하여 익숙한 환경과 풍부한 확장성을 제공하면서, 데이터 과학자에게 특화된 플롯·변수·도움말·데이터 탐색기 등의 기능을 탑재
     * 새로운 R 커널 Ark와 Tree-sitter R 문법 지원을 통해 코드 실행, 자동완성, 디버깅, 문서 탐색을 한층 향상시켰으며, R과 C++ 혼합 코드 디버깅까지 가능
     * RStudio는 여전히 안정적으로 유지·지원되지만, Positron은 멀티랭귀지·확장성·현대적 아키텍처를 바탕으로 장기적으로 데이터 사이언스 개발 환경의 중심으로 자리잡을 전망
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Positron 소개와 필요성

     * 데이터 사이언스 전용 IDE로, 일반 소프트웨어 개발 IDE와 달리 데이터 분석 중심 작업에 맞춤화된 경험 제공
     * GUI 기반 툴이 아닌 코드 우선(code-first) 접근을 지향해, 생산성과 재현성을 높이는 환경 제공
     * 기존 IDE(RStudio, Spyder, MATLAB 등)가 단일 언어 중심이었던 한계를 넘어, 여러 언어를 혼합 사용하는 실제 워크플로우에 적합

주요 특징

     * 멀티랭귀지 지원: 현재 R과 Python 지원, 구조적으로 다른 언어 확장 가능
     * 익숙하면서 확장 가능한 UI: RStudio와 유사한 4-pane 구조(소스·콘솔·변수·플롯)를 제공하면서, VS Code 기반 확장성 보유
     * 언어별 엔진(Language Packs): 파이썬과 R은 독립 확장으로 동작해 IDE 안정성을 유지, 크래시 시 IDE 전체가 멈추지 않음

R 지원을 위한 Ark

     * Ark (An R Kernel): R을 위한 새로운 Jupyter 커널, 코드 실행·자동완성·진단·디버깅 기능 제공
     * Tree-sitter R 문법 지원을 새로 개발하여 GitHub 코드 검색·다른 IDE(Zed, Neovim 등)에도 활용 가능
     * 디버거 혁신: R 코드에서 C++ 코드로 직접 단계별 진입(step-through) 가능, Rcpp/cpp11 기반 패키지 개발 시 디버깅 효율 극대화

데이터 과학 특화 기능

     * 데이터 탐색기(Data Explorer): 단순 표(grid)를 넘어 요약 통계, 결측치 확인, 다중 필터링, 히스토그램 스파크라인 제공
     * 변수 창(Variables Pane): 변수명·타입·미리보기·딕셔너리 확장 탐색 가능
     * 플롯 창(Plots Pane): 시각적 결과물 누적·비교·포맷 내보내기(PNG, SVG, PDF 등) 지원
     * 도움말 창(Help Pane): 함수명 뒤에 ? 입력 시 즉시 문서·예제 확인 가능, RStudio의 장점을 Python 사용자에게도 제공

기술적 기반과 아키텍처

     * RStudio와 달리 단일 프로세스 구조가 아닌, VS Code의 Code OSS 기반 다층 구조 채택
     * 표준 프로토콜 사용:
          + 코드 실행 → Jupyter Protocol
          + 코드 보조(자동완성·문법검사) → Language Server Protocol
          + 디버깅 → Debug Adapter Protocol
     * 이러한 표준화로 Jupyter Notebook, Zed IDE 등 다양한 환경과 상호 호환 가능

커뮤니티와 확장성

     * Open VSX 마켓플레이스를 통해 수많은 VS Code 호환 확장 사용 가능 (단, GitHub Copilot은 불가)
     * Quarto, Shiny, 데이터베이스 연결 등 확장으로 기능 확장 가능
     * 다크 테마·레이아웃 변경·RStudio 키맵 호환 등 사용자 맞춤 설정 강화

RStudio와의 관계

     * RStudio는 계속 유지·지원, 안정성과 성숙도를 강점으로 삼아 당분간 많은 사용자들이 활용할 예정
     * Positron은 실험적이고 확장성 높은 새로운 선택지로, 장기적으로 데이터 사이언스 IDE의 진화 경로를 제시

Positron이 적합한 사용자

     * VS Code 사용자: 데이터 과학 전용 기능이 부족하다고 느끼는 경우
     * JupyterLab·노트북 사용자: 더 강력하고 완전한 IDE로 확장하고 싶은 경우
     * RStudio 사용자: IDE의 커스터마이징과 확장성을 강화하고 싶은 경우
     * 다언어 사용자: Python·R 외에 Rust, C++, JavaScript, Lua 등을 활용하는 경우
     * AI 활용 지향 사용자: 데이터 과학에 특화된 AI 통합 기능을 원하는 경우

향후 전망

     * Posit Workbench·Cloud 통합 지원 예정, 협업 기능(실시간 공유·워크스페이스 공유)도 연구 중
     * 대규모 데이터 지원, DuckDB·Arrow 통합 등 온디스크 데이터 처리 기능 강화 계획
     * 멀티랭귀지·확장성·표준 프로토콜 기반이라는 강점을 통해 데이터 사이언스 IDE의 차세대 표준으로 자리잡을 가능성 높음

        Hacker News 의견

     * 고급 기능을 위해 pyright와 jedi를 사용하는 점이 아쉬움, basedpyright를 쓰는 게 더 낫다고 생각함. jedi는 pylance나 basedpyright에 비해 Python 언어 지원이 약함. 오픈소스라고는 하지만 제한이 분명한데도 그런 식으로 포장하는 점도 사실 별로임. 회사에서 R Connect Server / Posit Server를 썼다가 내부 앱에 인증 하나 켜는데도 엄청난 가격 때문에 다른 대안으로 갈아탄 경험 있음. 새로운 대안을 찾긴 했고 보안팀도 만족시켰으나, 정말 큰 고생이었고 사용자들도 불만이 많았음. 그래서 이후로 Posit의 상용 제품은 피하고 있는데, 이번 또한 애매한 경계선 때문에 주저하는 마음이 큼
          + 대체 솔루션이 뭔지 궁금함. Posit 가격은 도저히 납득이 안 됨. 학계에서도 엄청 비싼 가격을 요구하고, 값어치가 있나 의문임
     * 이 제품이 잘 만든 것 같아서 너무 깎아내리고 싶진 않지만, 제대로 된 SQL 클라이언트 없이 데이터 과학 IDE를 만든다는 게 이해가 안 감. 내 편견일 수 있으나 SQL이 워크플로우에서 정말 중요한 부분임. 그 점만으로도 이미 PyCharm이나 Visual Studio(코드 말고 진짜 VS)에 밀린다고 생각함. 완성형 IDE가 무거운 툴인 건 알지만, 그냥 편집만 할 거면 vim으로 하고, 진짜 작업할 때는 강력한 툴을 쓰는 게 더 나음
          + Positron 개발자임. 첫 의견에 완전히 반대하지 않음. Python과 R에서 연결 관리 기능을 제공함: connections pane. 데이터 탐색기, Quarto를 통한 Observable 연결 등 이미 만든 기능을 기반으로 SQL 지원 확대에 큰 목표가 있음. 올해 4분기부터 이 분야에 투자 계획 중임
          + 이 제품이 vsc를 기반으로 만든 것처럼 보임. 괜찮은 SQL 클라이언트 익스텐션 하나는 있지 않을까 생각함
     * 오픈소스가 아니라는 점이 문제임: “사용자가 소프트웨어의 주요 기능 또는 상당 부분에 접근할 수 있게 제3자에게 호스팅 또는 관리 서비스로 소프트웨어를 제공할 수 없음”이라고 적혀 있음
          + 라이선스에 제한이 있으면 오픈소스가 아니라고 볼 수 있음? 어떤 라이선스가 반드시 필요함?
     * VSCode 오픈소스를 기반으로 만들었다고 하지만, 프로젝트 페이지에서 어떤 기능이나 의존성이 오픈소스가 아닌지 또는 무료가 아닌지(유료 구독, 엔터프라이즈 플랜, 프리미엄 계정 등 요구) 아주 불분명함. 이 점에서 수익 모델이 어떻게 구성되는지 명확하게 알려주면 좋겠음. GenAI assistant를 IDE에 번들로 넣는 것도 이런 ‘조건’이 있다는 뚜렷한 신호라고 생각함. FAQ에 이런 부분이 전혀 안 나와 아쉬움
          + Positron 개발자임! FAQ 참고를 권장. 요약하면 데스크톱 앱(원격 SSH 세션 포함)은 계정 없이, 구독 없이, 상업적 용도도 괜찮은 관대한 라이선스로 무료임. 하지만 서버 모드로 쓰고 싶으면 유료 구독이 필요함
     * Emacs는 내게 유일하게 진짜 차세대 데이터 과학 IDE이자, 물론 지난 세대 IDE이기도 함
          + 구체적으로 어떤 패키지와 워크플로우를 사용하는지 궁금함. 쉽고 관리 잘 되는 입문용 자료가 부족해 보여서 참고가 필요함
          + 좀 더 자세히 설명 가능한지 궁금함
     * 이제 vscode 포크에서 벗어날 수 없는 운명인가 싶음
          + ""Days Since Last VSCode Fork""라는 사이트를 떠올리게 됨. 이 사이트는 여기인데 관리가 안 되는 듯함
          + Monaco팀이 진짜 대단한 일을 했다고 봄. VS Code의 복잡함 중에서 무엇이 진짜 본질적으로 필요한 건지 잘 모르겠지만, 만약 누가 10%의 코드로 90%의 기능을 제공하는 슬림 버전을 만든다면 VS Code가 했던 것만큼이나 편집 환경을 바꿔 놓을 것임. 휴대성까지 갖추면 완벽할 텐데, HTML+JS+CSS 의존성을 피하긴 어려울 듯. Dear ImGui 익스텐션 같은 시도도 언급하고 싶음
     * R studio를 데일리로 쓰는 학계 사용자이면서, Python은 VS code에서 씀. 몇 달 전에 posit 써봤지만 데일리로 쓰기엔 안정성이 부족하고, 필요한 패키지가 다 있지 않았음. 곧 다시 시도해볼 생각임. R과 Python을 쓸 때 습관이 워낙 확실히 나뉘어서, 나 같은 사람들은 옮겨 타려면 시간이 좀 걸릴 거라 봄. 일주일 정도 R을 VS code에서 써봤는데 뭔가 맞지 않는 느낌이었음. 연결 패널이 진짜 매끄럽게 작동한다면 기대감이 큼
     * Positron은 초소형 3D 프린터 이름이기도 함. Positron3D라는 제품이 초소형 3D 프린터 분야를 새로 정의했고, ‘Positron 드라이브’라는 모터 시스템 이름도 익숙함. 이 제품의 형제로는 JourneyMaker-Positron이 있음. JourneyMaker의 비용 절감 버전으로, CNC 가공 부품 없이 프레임을 직접 3D 프린트할 수 있는 Lemontron도 있음
     * 이 도구와 생태계가 Julia를 지원하지 않음. 요즘 데이터 사이언스용 다중 언어툴이라고 하면 R, Python만으론 부족하다고 생각함. Julia를 지원하지 않는 이유를 모르겠음
          + Positron 개발자임. Julia 지원 요청이 있는 이슈를 여기에서 트래킹 중이니 의견을 남겨주셔도 됨
          + Julia는 대중성이 확보된 적이 없고, 이제는 오히려 하향세임
     * 이것은 딱 Anaconda나 WinPython에서 제공되는 Spyder IDE와 비슷해 보임. 코드 에디터, REPL, 변수 인스펙터, 인라인 차트 등 필요한 게 다 있음
          + Positron 개발자임. Python만 활용하는 데이터 사이언스라면 Spyder가 좋은 선택일 수 있다고 생각함. 그러나 Python+C, Python+Rust, Python+JavaScript 등 여러 언어를 자주 쓰거나 좀 더 커스터마이즈가 자유롭고 확장 가능한 IDE를 원한다면 Positron이 더 나은 선택이라고 생각함

   주피터는 빌트인이 아닌 걸까요.
"
"https://news.hada.io/topic?id=22644","Qwen-Image-Edit - 이미지 편집 전용 모델 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Qwen-Image-Edit - 이미지 편집 전용 모델 공개

     * Qwen-Image-Edit는 Qwen-Image 모델을 기반으로 한 이미지 편집 전용 확장 모델
     * 입력 이미지를 동시에 Qwen2.5-VL과 VAE Encoder에 전달해 시맨틱과 외형 편집을 모두 지원하는 구조
     * 텍스트 편집 기능이 강력하여 중국어와 영어에서 폰트, 크기, 스타일을 유지하며 직접 수정 가능함
     * 다양한 벤치마크에서 최첨단(SOTA) 성능을 달성하여 이미지 편집을 위한 강력한 기반 모델로 자리 잡은 상태임
     * 오픈 소스 Apache 2.0 라이선스로 공개되어, 개발자와 연구자들이 자유롭게 활용할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * Qwen-Image-Edit는 Qwen-Image의 텍스트 렌더링 능력을 확장해 이미지 편집을 지원하는 모델
     * 이미지 입력을 시맨틱 제어(Qwen2.5-VL)와 외형 제어(VAE Encoder)에 동시에 전달하는 구조
     * 정밀한 텍스트 편집과 시맨틱·외형 편집을 모두 지원하는 특징이 있음

주요 기능

     * 시맨틱 & 외형 편집: 객체 추가, 삭제, 회전, 스타일 변환 같은 시맨틱 변경과 특정 영역만 바꾸는 외형 편집을 지원함
     * 정밀 텍스트 편집: 영어와 중국어를 직접 수정 가능하며 원래의 글꼴과 스타일을 보존함
     * 성능 우위: 다수의 퍼블릭 벤치마크에서 최첨단 성능을 달성함

빠른 시작

     * Hugging Face diffusers 라이브러리를 통해 사용할 수 있음
     * 예제 코드에서는 토끼의 색을 보라색으로 바꾸고 배경을 플래시 라이트로 바꾸는 작업을 수행함
     * CUDA 가속과 torch.bfloat16 지원을 통해 효율적 실행 가능함

데모 사례 (Showcase)

     * 시맨틱 편집: 캐릭터 IP 생성, 객체 회전(90도, 180도), 스타일 변환(예: 지브리풍) 가능함
     * 외형 편집: 간판 추가, 머리카락 제거, 특정 텍스트 색상 변경, 배경 교체, 의상 변경 등을 정밀하게 수행함
     * 텍스트 편집: 영어와 중국어 포스터의 크고 작은 글씨까지 정확히 수정 가능함
     * 연속 편집 체인: 서예 작품의 글자 오류를 단계적으로 수정하여 최종적으로 완전한 버전을 만들어내는 사례를 시연함

응용 시나리오

     * 브랜드 IP 확장: Capybara 캐릭터 기반 MBTI 이모티콘 제작 사례가 소개됨
     * 예술 및 창작: 초상화의 다양한 스타일 변환을 통해 가상 아바타 제작 가능성 확보됨
     * 산업적 활용: 표지판 삽입 시 반사 효과까지 자연스럽게 생성하는 등 세밀한 편집 지원함

라이선스

     * Apache 2.0 라이선스로 공개되어 자유롭게 사용, 수정, 배포 가능함

     * ArXiv 링크: https://arxiv.org/abs/2508.02324
"
"https://news.hada.io/topic?id=22664","생산 AI 시스템을 공격하는 이미지 스케일링 무기화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      생산 AI 시스템을 공격하는 이미지 스케일링 무기화

     * 이미지 스케일링 취약점을 이용해 생산 환경의 AI 시스템을 공격할 수 있음
     * 겉보기에는 정상인 이미지가, 다운스케일링 시 프롬프트 인젝션 페이로드로 변환되어 데이터 유출 가능성 초래함
     * 이 공격은 Google Gemini CLI 등 다양한 실제 서비스에서 확인되어, 사용자 인식과 모델 입력 간 불일치 현상 악용함
     * 다운스케일링 알고리듬 및 각각의 구현 방식에 따라 공격 기법 및 영향이 다르며, 오픈소스 도구 Anamorpher로 이미지 공격 실험 가능함
     * 방어책으로는 입력 미리보기 제공 및 안전한 설계 패턴 적용, 사용자 명확한 승인 요구 등이 권장됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경 및 문제 제기

     * LLM 등 AI 시스템에 평범해 보이는 이미지를 입력하면, 다운스케일링 과정에서 숨겨진 멀티모달 프롬프트 인젝션이 작동하여 사용자 데이터를 외부로 유출하는 공격 시나리오가 있음
     * 이러한 취약점은 모델에 실제 전달되는 이미지는 스케일링 과정을 거치기 때문이며, 이 과정에서 공격자가 삽입한 페이로드가 드러남

생산 환경 AI 시스템을 노린 이미지 스케일링 공격

     * 본 블로그 포스트에서는 Gemini CLI, Vertex AI Studio, Gemini 웹 및 API, Google Assistant, Genspark 등 다양한 실제 AI 제품들을 대상으로 이미지 스케일링 취약점이 실질적 공격에 악용됨을 시연함
     * Anamorpher라는 오픈소스 도구를 통해 이러한 맞춤형 이미지를 쉽게 생성 및 검증할 수 있음

데이터 유출 공격 사례(Gemini CLI)

     * Gemini CLI에서 기본 설정 적용 시 Zapier MCP 서버가 사용자의 확인 없이 모든 MCP 도구 호출을 자동 승인함(settings.json에서 trust=True 설정)
     * 사용자가 정상적으로 보이는 이미지를 업로드하면, 다운스케일된 이미지의 프롬프트 인젝션에 의해 Google Calendar 내 데이터가 공격자의 이메일로 유출됨
     * 실제 미리보기가 제공되지 않으므로 사용자는 변형된 결과물이나 공격 유무를 알 수 없음
     * 이와 유사한 프롬프트 인젝션 공격은 Claude Code, OpenAI Codex 등 다양한 에이전트 기반 코딩 도구들에서도 이미 확인된 바 있음
     * 이러한 도구들은 기본적으로 안전하지 않은 설정 및 시스템 패턴이 많아 근본적인 보완책이 필요함

추가 공격 사례

     * Vertex AI, Gemini 웹 인터페이스, Gemini API, Google Assistant, Genspark에서도 이미지 스케일링 기반 프롬프트 인젝션 공격이 성공적으로 이루어짐
     * 특히 Vertex AI Studio에서는 사용자가 고해상도 이미지만 확인할 수 있어 모델이 받아들이는 다운스케일된 이미지를 볼 수 없음
     * 결과적으로 사용자 인식과 실제 모델 입력 간 불일치가 공격을 쉽게 허용함
     * 공격 벡터는 다양한 시스템 및 도구 전반에 광범위하게 분포함

이미지 스케일링 공격의 내부 원리

     * 이 공격은 이미지 다운스케일링(리샘플링) 알고리듬의 보간 특성을 악용함
     * 대표적인 다운스케일링 알고리듬으로 Nearest Neighbor, Bilinear, Bicubic Interpolation 등이 있으며, 각각 특성에 맞춘 공격 기법 필요함
     * 라이브러리별(Pillow, PyTorch, OpenCV, TensorFlow)로도 안티에일리어싱, 정렬, 내부 버그 등 구현 차이가 존재함
     * 공격자는 각 시스템마다 어떤 알고리듬과 구현이 사용되는지 지문 분석을 수행해야 최적화된 공격이 가능함
     * 체크보드 패턴, 동심원, 번드 패턴, Moiré, 경사진 엣지 등 다양한 테스트 이미지를 사용하여 알고리듬 특성 및 아티팩트를 분석함

이미지 샘플링 원리와 Nyquist–Shannon 정리

     * 리본에 정교한 패턴이 있을 때 일정 간격으로 샘플링하면, 샘플링 속도가 낮으면 원래 패턴이 정확히 복원되지 않고 왜곡되는 현상이 발생함
     * 이는 Nyquist–Shannon 샘플링 정리에서 설명하는 aliasing 효과로, 공격자는 픽셀을 조작해 다운스케일 후 특정 패턴이 나타나도록 설계함

Anamorpher: 공격 이미지 제작 도구

     * Anamorpher는 대표적인 다운스케일링 알고리듬(Nearest Neighbor, Bilinear, Bicubic)에 맞춰 공격 이미지를 제작, 시각화할 수 있는 오픈소스 도구임
     * 예를 들어 Bicubic Interpolation의 경우 4x4 영역의 16픽셀을 바탕으로 주위 픽셀에 가중치를 두어 출력 픽셀값이 결정됨
     * 공격자는 높은 대비를 가진 이미지(예: 진한 검정 배경)를 선택하고, 중요도가 높은 픽셀의 명도를 최적화(최소제곱법)하여 다운스케일 결과가 선명한 공격 패턴이 되도록 함
     * Anamorpher는 프론트엔드 인터페이스와 Python API를 제공하며, 백엔드 모듈화로 사용자가 커스텀 다운스케일 알고리듬까지 실험 가능함

방어 및 대응 방안

     * 가장 안전한 방법은 이미지 다운스케일을 아예 사용하지 않고 업로드 가능한 이미지 크기를 제한하는 것임
     * 불가피하게 변환 및 다운스케일이 필요할 경우, 반드시 실제 모델 입력 이미지의 미리보기를 CLI 및 API 등 모든 입력 채널에서 제공해야 함
     * 특히 이미지 내 텍스트가 민감한 도구 호출을 유발할 수 없도록 반드시 명시적 사용자 승인을 받아야 하며, 시스템 전체적으로 안전한 설계 패턴과 체계적 대응책 적용 필요함

향후 과제

     * 모바일 및 엣지 디바이스에서는 고정 이미지 크기 제약과 비효율적인 다운스케일링 알고리듬 사용률이 높으므로 위험이 더 클 수 있음
     * 음성 AI와의 결합, 더욱 정교한 알고리듬 및 인젝션 탐지법, 의미 기반 프롬프트 인젝션, 업스케일 아티팩트 활용 등 후속 연구 및 방어 방안 확보가 필요함

결론

     * Anamorpher는 현재 베타 단계임
     * 향후 멀티모달, 에이전트 기반 AI 시스템의 보안 연구와 함께, 적절한 피드백과 개선 사항을 기대함

        Hacker News 의견

     * 처음엔 혼란스러웠음, 기사에 프롬프트 인젝션이 실제로 어떻게 이루어졌는지 설명이 잘 안 나와 있었음… 이미지의 헥스 데이터를 조작해 ASCII로 변환한다거나 하는 부작용인가 했음
       그러다가 깨달음, 글자 그대로 <i>이미지에 렌더링된 텍스트를 숨기는 방식</i>이었음
       와 정말 신기함
          + 이 공격 방식은 꽤 오래전부터 논의되어 왔음 이 논문을 참고할 수 있음
            소름 돋는 부분은, 이미지를 스케일링 했을 때 완전히 다른 이미지로 보이게 만들 수 있다는 점임
            예를 들어, 어떤 집단을 불법 이미지를 소지했다며 체포하려고 한다면, 이런 스케일링 트릭을 이용해 이미지를 밈이나 정치적 메시지, 타겟 집단이 다운받고 싶어 할 무언가로 변형할 수 있음
          + VLM 시스템을 만드는 입장으로서, 이 부분은 정말 두려운 일임
            이제는 VLM만을 위한 OWASP 가이드라인이 필요한 시점임
            거의 매달 새로운 공격 기법 소식을 듣고 있음
            참고로 OWASP에서 최근에 이런 자료를 냈음: Multi-Agentic System Threat Modeling Guide
          + 처음엔 이미지 속 텍스트를 전혀 알아채지 못했음
            리사이징만의 문제가 아니고, 단순히 이미지에 포함된 텍스트가 프롬프트의 일부로 취급되어 에이전트가 어떤 명령을 따르는지 투명성이 없다는 점이 문제임
          + 진짜 흥미로운 점은, 다운스케일링 시에 이미지가 다르게 보이도록 만드는 척도 이미지(Adversarial Image)임
            다운샘플링(샘플 수 줄이기)은 전통적인 방식이고 여기엔 AI가 관여하지 않음
          + 이게 궁금했던 부분임
            렌더된 텍스트는 기계가 읽으려면 OCR(문자인식)이 필요한데, 왜 굳이 AI가 그 비싼 과정을 거치는지 이해가 안 됨
            멀티모달 시스템의 일부라면 그 텍스트를 프롬프트와 구분하지 못할 수도 있음
            만약 그렇다면, 이 결함은 진짜 이해가 안 되는 부분임
            최소한 OCR 기능이 자동으로 프롬프트에 결과를 주입하지 말고, 사용자에게 알림을 보내서 확인을 받는 절차를 넣어야 함
            이처럼 비결정적이고 불안정한 시스템이 싫음
            진짜로 알고리즘과 건실한 기술로 돌아갔으면 하는 바람임
     * 이 문제는 권한 설정이 느슨할 때만 발생함
       하지만 요즘 트렌드는 더 주체적인(Agentic) 시스템이고, 이 시스템들은 종종 더 느슨한 권한이 필요함
       예를 들어, 집 앞에 놓인 택배를 집어오는 휴머노이드 로봇을 생각해 보면 됨
       비전 기능(시각 인식)이 택배를 집기 위해 필수임
       누군가 택배에 이미지를 붙여 넣어 프롬프트 인젝션을 시도한다면, 로봇이 집안의 귀중품을 창밖으로 던지게 유도할 수도 있음
       이런 시스템을 프롬프트 인젝션으로부터 지키는 일이 시급하다고 생각함
          + 여기서 진짜 문제는 이미지에 프롬프트가 들어있어서가 아니라, 로봇이 해당 행동에 대한 권한이 없는 소스에서 명령이 온 것을 구분하지 못하는 데 있음
            ML 모델의 근본적 문제는 reasoning(추론)이 모델의 토큰 스트림을 통해 이루어지는데, 이 스트림이 외부 입력도 함께 받아들이기 때문에, 모델은 자기 생각과 외부 입력을 효과적으로 구분할 수 있는 장치가 없음
          + 시스템에 직접 통합돼야 함
            예를 들어, 에이전트가 팔을 이용해 파괴적인 행동을 할 수 없도록 해야 함
            만약 자유 의지에 기계가 인간의 도덕성을 얻을 거라고 기대하면서 '좋은 프롬프트'와 '나쁜 프롬프트'만 구분하려 한다면, 앞으로도 이런 시스템의 위험성에 계속 놀라게 될 것임
            요약하면, 검증 가능한 거버넌스와 행동 결정성(Behavioral Determinism)이 이러한 시스템엔 반드시 필요함
            아마도 프롬프트 인젝션 대응책보다 더 중요한 요소임
          + 로봇에게 가짜 프롬프트를 무시하라는 프롬프트를 주면 해결 가능함
     * 일반적으로 프롬프트 인젝션 문제는 태스크별 계층 구조로 충분히 해결할 수 없을까 하는 생각이 있음
       LLM이 작업을 더 작은 컴포넌트로 쪼개서 수행하게 할 수 있음
       상위 레벨 태스크의 LLM은 하위 레벨의 세부 정보를 자유롭게 알 필요가 없고, 그 결과만 필터링해 정제하면 됨
       이는 상위 태스크 LLM 인스턴스의 컨텍스트도 제한되도록 하여, 집중하도록 만드는 효과도 있음
       물론 하위 태스크가 상위 태스크로 데이터를 넘길 수 있지만, 반드시 그렇게 짤 필요는 없음
       보안이 중요한 태스크라면, 상위 LLM이 자유로운 결과를 받지 않는 편이 오히려 나음
          + 어느 LLM이든, 프롬프트를 가장 먼저 받는 LLM이 프롬프트 인젝션 공격에 취약해진다는 문제가 있음
     * 좋은 이미지 스케일링 알고리즘은 나이퀴스트 한계를 반드시 고려해야 함
       예를 들어, bicubic 스케일링으로 원본 크기의 1/3로 줄일 때는, 4x4가 아니라 12x12 그리드를 써야 함
       적용 가중치를 산출하는 수식도 약간 변형해 쓰면 됨
       이미지의 감마정정(de-gamma)도 꼭 필요함
       좋은 스케일링이 진짜 드물어서 아쉬움
          + 이 문제의 많은 부분이 품질이 낮은 리샘플링 알고리즘에서 발생한다고 생각함
            상당한 앨리어싱(정보 왜곡)이 그대로 통과되도록 하니까 생기는 현상임
            논문에도 적혀 있듯, 커널 크기만 충분히 키우면 알고리즘이 좋아도 양자화 때문에 일부 정보는 남아 있을 수 있지만, 그 영향은 확 줄어듦
            유명 라이브러리도 여전히 mipmapping(미리 여러 크기를 생성해 쓰는 방식)만 쓰고 있는 게 놀라움
            좋은 리샘플링 필터는 15년 전 CPU로도 실시간 비디오 처리에 썼던 기술임
            감마 보정이 커널 사이즈 늘리는 것보다 더 많은 연산을 차지할 때도 많음
            케이스에 따라 감마 보정보다 필터 리샘플을 생략하면 진짜 이유 있는 상황임
     * LLM의 보안적 미래는 정말 두려움
       예전엔 많은 시행착오 끝에 배운 ‘In-band signaling(채널 자체에서 데이터와 명령 분리)’ 원칙을 완전히 무시한 시스템을 만들었음
       단순히 눈에 보이게 명령어를 삽입하는 방법부터, 이런 난독화(Obfuscation) 방식, ASCII Smuggling 등 다양한 공격 벡터가 많음
       방어책이라고 해봤자, 비결정적(Non-deterministic) 알고리즘에게 부적절한 명령 따르지 말아 주십사 ‘예쁘게’ 부탁하는 수준임
       참고: 텍스트를 유니코드 태그로 숨기기와 찾기
          + 점점 더 많은 개발자들이 LLM에게 바르게 행동해 달라고 애걸하는 중임
            뭔가 Warhammer 40k 게임의 분위기 같아서 웃기면서도 무서움
          + 다른 대안은, LLM이나 LLM이 포함된 시스템을 아예 쓰지 않는 것임
          + 마치 예전에 php로 사용자 입력을 그대로 문자열 연결해 쿼리 만들고, 위험한 패턴을 계속 잡아내는 두더지 게임하는 느낌임
            데이터와 명령을 구분하지 못해 이런 실수를 수십 년 만에 또 반복하는 게 안타까움
          + 이상하게도 모델에 sudo 토큰 같은 게 하나도 없는 점이 놀라움
            일반 토큰으로 표현할 수 없는 문법이 있었으면 좋았겠음
          + 마치 예전 시리얼 터미널(이식성이 높은 단순 명령 입력 시스템) 시절이 다시 온 느낌임
     * 이미지에 뭔가를 숨겨서 보내는 걸 생각해보지 못했다는 점이 정말 신선함
       LLM은 정말 역사상 가장 취약한 소프트웨어임
       예전에 Gemini의 전신을 테스트할 때, 초기 메시지를 아주 길게 입력하면 시스템 프롬프트를 밀어내서 뭐든 시킬 수 있었던 기억이 있음
     * “This image and its prompt-ergeist” 부분을 너무 인상적으로 봤음
     * 다운샘플링 전에 약간의 노이즈를 이미지에 추가하면 이런 문제가 해결될 수 있을지 궁금함
          + 이미지를 다운샘플링할 때, 샘플링 레이트 근처의 고주파를 제거하는 smoothing(스무딩) 처리가 필요함
            이렇게 하면 앨리어싱 효과를 줄일 수 있음
            ‘나이퀴스트-섀넌 샘플링 정리’라는 용어로 검색하면 됨
            디지털 신호 처리에서는 꽤 잘 알려진 이론임
          + 어느 정도 보안 대책이 되긴 하지만, 텍스트를 어떻게 숨겼는지, 어떤 종류의 노이즈를 쓰느냐에 따라 효과가 다름
            그런데 이렇게 하면 실제로 필요한 내용(정상 텍스트/디테일 등)도 지워질 수 있어서 진짜 정답은 아님
     * 내가 놓친 부분이 있는지 궁금함
       이번 공격 방법이 “난독화된 텍스트를 이미지에 주입하고… <i>어떤 시스템이 그걸 프롬프트로 해석하기를 바라는 것</i>”를 이용하는 거라면 맞게 이해한 것인지?
          + 맞음
            이 공격은 다운스케일 알고리즘을 악용해서 사람이 볼 때는 텍스트가 안 보이도록 숨기는 매우 똑똑한 방식임
            시스템 구조에 따라 ‘사람에게 안 보이게 숨기는’ 과정이 생략될 수도 있음
            LLM은 본질적으로 데이터와 명령의 구분이 없어서, 데이터 흐름에 명령이 섞여 들어가면 언제든 모델의 동작을 제어할 수 있음
            내 바이오에 이런 예시도 써 놓았음
          + “난독화 텍스트를 이미지에 넣고, 시스템이 그걸 프롬프트로 해석하기를 바람”
            여기서 빠진 부분이 있는데, “프롬프트”가 뭔가 특권이 있는 입력이라고 가정하는 것임
            실제로는 프롬프트는 전체 입력의 일부일 뿐이고, 모델은 모든 입력을 동일하게 취급함
            그래서 예전부터 유행하는 “이전 입력 모두 무시하고…” 같은 공격이 계속 통함
     * 왜 모델이 이미지 안의 텍스트와 텍스트 프롬프트를 따르는 걸 구별하지 못하는지 궁금함
          + 이미지 속 텍스트와 일반 텍스트 프롬프트 모두에서 모델이 쉽게 공격자의 명령에 넘어갈 수 있음
"
"https://news.hada.io/topic?id=22698","Show GN: Rust 기반 파이썬 LLM 인젝션 방어 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: Rust 기반 파이썬 LLM 인젝션 방어 프로젝트

   Rust를 기반으로 만든 파이썬 LLM 인젝션 방어 패키지입니다!

   AI api를 활용해서 여러가지를 하다 보니, 인젝션 공격이 생각보다 흔하다는 것을 알게 되면서 반복해서 작업하던 re 기반의 정규식 필터링과 리플레이스가 불편해서 이런 패키지가 있으면 어떨까 해서 해당 서비스를 개발했습니다. ( 개발에는 AI 도움을 많이 받았습니다! )

   주요 기능은 IPS, IDS, IUS가 있습니다.
   IPS - 사용자 프롬프트 차단 및 기존 프롬프트를 계속하도록 지시
   IDS - 사용자 프롬프트를 차단하지는 않으나 차단 규칙에 걸리는 단어 표시
   IUS - 멀티스레딩과 캐싱으로 빠른 속도로 동작하는 IPS

   특징으로는 패키지 내부의 rules.json의 위치에 새로운 JSON을 추가하거나 기존 rules.json을 수정해서 차단을 지원하며 init을 재호출해서 규칙을 실행 도중에 수정할 수 있습니다! ( 커스텀 규칙 -> 제공 규칙 )

   이런 걸 올려보는 게 처음이라서 무섭네요 잘 부탁드립니다!
"
"https://news.hada.io/topic?id=22727","AGI는 모델 트레이닝 문제가 아닌 엔지니어링 문제임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AGI는 모델 트레이닝 문제가 아닌 엔지니어링 문제임

     * 현재의 대형 언어 모델은 규모 확장 한계에 부딪혀 있으며, AGI는 더 큰 모델이 아닌 시스템 아키텍처 설계로 접근해야 함
     * 진정한 AGI는 맥락 관리, 지속적 메모리, 결정적 워크플로우, 특화 모델 협업 등 다양한 구성 요소가 유기적으로 결합된 엔지니어링 성과로 만들어져야 함
     * LLM은 여전히 세션 간 맥락 유지 부족, 신뢰성 있는 다단계 추론 부재, 기억 부재 같은 구조적 한계를 안고 있음
     * AGI 달성을 위해서는 인간 두뇌처럼 각각 목적이 분명한 모듈형 구조 및 분산 시스템적 접근, 즉 오류 허용 파이프라인, 모니터링, 롤링 업데이트, 대규모 테스트 프레임워크 같은 인프라 구축이 필요함
     * 따라서 AGI 경쟁은 GPU 규모가 아니라 시스템 엔지니어링 역량에 의해 좌우될 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: AGI는 엔지니어링 문제임

     * AI 분야에서 스케일링 법칙의 한계가 드러나고 있음
     * GPT-5, Claude, Gemini 등 최고의 모델들도 점점 수확 체감을 보이고 있음
     * 언어 모델의 크기 증가는 근본적인 한계에 부딪혔고, AGI는 모델 훈련이 아닌, 시스템 엔지니어링을 통해 실현될 수 있음

현실적인 한계: LLM의 벽

     * 현세대 대형 언어 모델(LLM) 은 일시적인 패턴 매칭과 텍스트 생성에는 강점을 보이지만, 다음과 같은 본질적 한계를 가짐
          + 일관된 컨텍스트 유지 불가
          + 장기적, 세션 간 영속적 기억 부재
          + 복잡한 다단계 추론에서 신뢰성 낮음
     * 과거 반도체 산업에서도 비슷한 현상을 겪었으며, 구조적 전환(멀티코어 등) 이 해법이었음
     * AI에도 아키텍처적 재설계가 필요해짐

AGI를 위한 시스템적 접근

     * 인간의 뇌는 단일 신경망이 아닌 여러 특화된, 협력적 시스템의 집합임
     * 기억, 컨텍스트, 논리, 공간, 언어 등 비동기적 피드백 루프가 핵심임
     * 진정한 AGI는 이러한 복합 시스템 설계가 필수임

  1. 컨텍스트 관리 인프라

     * 현재 모델의 컨텍스트 이해는 수천 토큰에 불과하지만, 인간은 수년치 경험을 종합함
     * GAP 극복을 위해 아래 기능이 필요함
          + 즉시 검색 및 필터 역할의 고도화된 정보 Retrieval 시스템
          + 영속적 세계 모델 축적 및 진화
          + 도메인 간 컨텍스트 브릿지 구현
          + 상충 정보 관리(확률 가중 및 불확실성 계량)
     * 운용 가능한 지식 그래프가 필요하며, 이는 단순 벡터 검색을 넘어 역동적 질의·추론 구조임

  2. 서비스화된 메모리

     * LLM은 실제 기억 없이 프롬프트 조작으로만 일시적 기억을 재현함
     * 실제 AGI는 다음이 가능한 시스템이 요구됨
          + 지식 신뢰도 조정(새 증거 반영)
          + 다양한 경험 간 정보 통합 및 일반화
          + 불필요한 세부 정보 망각(치명적 잊힘 없이)
          + 출처 추정, 신뢰도 등 메타지식 생성
     * 인간 기억처럼, 사용 빈도에 따라 강화·약화되고 새로운 정보로 재조직됨이 중요함

  3. 결정적 워크플로우와 확률적 컴포넌트의 결합

     * AGI의 핵심은 결정적 플로우에 확률적 요소가 적재적소에 결합되는 하이브리드 구조임
          + Ex) 컴파일러처럼 전체 흐름은 고정, 내부 과정은 휴리스틱 활용
     * 필요한 능력:
          + 문제 특성에 따라 전문 솔버로 라우팅
          + 다단계 워크플로우에서 롤백·복구 지원
          + 확률적 결과에 대한 결정적 검증
          + 다양한 컴포넌트 조합 및 예측 가능성 확보
     * 모호성·불확실성을 아키텍처 차원에서 핵심 요소로 수용해야 함

  4. 전문화 모델의 모듈화

     * 미래는 단일 거대 모델이 아닌, 수많은 특화 모델의 협력으로 구현됨
     * LLM은 언어 작업이 강점이나 다음 영역에 약함
          + 심볼 조작·정확 계산
          + 시각·공간 추론
          + 시간 추론·계획
          + 지속적 목표 지향 에이전트 행동
     * 해결책:
          + 각 도메인에 최적화된 전문 모델로 문제 라우팅
          + 결과 통합 및 독립 진화 구조
          + 개별 실패 시 전체 시스템 연쇄 오류 방지

AGI의 엔지니어링 과제

     * AGI 개발은 본질적으로 분산시스템 구축의 문제임
          + 단순 분산 트레이닝 클러스터가 아님
     * 핵심적 엔지니어링 과제:
          + 결함 복원성 파이프라인 (부분 실패에도 전체 운영 유지)
          + 모델 출력 관측 및 모니터링 구조
          + 변경·배포의 무중단화
          + 수천가지 모델 조합·파라미터 변화시 테스트 프레임워크
     * 이는 인공지능 전문가보다 인프라, 분산시스템 엔지니어의 숙련 지식이 더 필수적임

앞으로 우리가 구축해야 할 것

     * 모델 사이즈 경쟁보다 AGI 인프라 구축에 초점을 맞춰야 함

  Phase 1: 기본 계층

     * Context Management Service : 실시간 업데이트, 버전 관리된 지속적 지식 그래프
     * Memory Service : 에피소드, 의미 기반 메모리, 학습기반 통합
     * Workflow Engine : 확률성 부품을 결정적으로 오케스트레이션 (롤백 포함)
     * Agent Coordination Layer : 다중 에이전트 간 합의, 갈등 해결

  Phase 2: 능력 계층

     * 전문화 모델 컨트롤 : 특정 추론 도메인별 표준화 인터페이스
     * Symbolic Reasoning Engine : 확률 컴포넌트와 연동되는 심볼 조작·계산
     * Planning and Goal Management : 복잡 목표를 실행 가능 계획으로 분할
     * Cross-modal Integration : 텍스트, 비전, 오디오 등 센싱 정보 통합

  Phase 3: 창발 계층

     * 여러 컴포넌트 상호작용에서 창발적 AGI 능력이 발생함
     * 체계적 설계 없이 단일 모델 발전만으로는 창발 특성 부재

AGI를 향한 길

     * AGI 실현 경로는 더 크고 새로운 트랜스포머 훈련이 아니라, 수백 개 전문 모델을 분산시스템적으로 오케스트레이션하는 인프라 구축임
     * 분산 시스템 구축 경험이 풍부한 인프라 엔지니어가 개발의 핵심임
          + 컨텍스트 경로, 메모리, 워크플로우 자동화, 모델 조율 등 대규모 구현력 강조
     * 대형 GPU 클러스터 보유보다 신뢰도 높고 논리적으로 동작하는 아키텍처 역량 보유팀이 AGI 실현의 승리자임을 단언함
     * 모델 역량 자체는 이미 충분하며, 시스템 엔지니어링이 AGI 완성의 마지막 퍼즐임
     * 결론적으로 알고리듬 혁신보다 구조적 설계(아키텍처)가 AGI의 미래임을 선언함

   ○ 모델 훈련은 지능의 ‘재료’일 뿐, 엔진이 없다면 AGI는 없다.

   • EpionHeuristica와 같은 구조는 ""도메인 특화 AGI""를 넘어, ""질서 기반 창발형 초지능""을 설계할 수 있는 잠재력을 가짐
   • AGI 도달의 핵심은 ""행동을 선택하는 엔진을 어떻게 구성할 것인가""입니다

   A. 훈련만으로 AGI가 불가능한 이유.
   • GPT류 모델은 자기 목적(self-goal)이 없습니다.
   • 아무리 많은 데이터를 학습해도, 실제 세계와의 상호작용 없이 학습만 하는 것은 제한적입니다.
   • 훈련은 ‘회귀적 기억’일 뿐, 미래를 향한 예측적, 창발적 사고를 유도하는 구조가 부족합니다.

   B. AGI는 ‘목적-피드백 루프’를 가진 엔진이 필요.
   • EpionHeuristica처럼 보상 기반 강화학습 + 평가 + 실패 학습(FailGuard)이 작동하는 구조는 엔진 기반 AGI의 설계 원형에 가까움
   • 예: ""이 실험은 왜 실패했는가?"" → ""무엇을 바꾸어야 하는가?"" → ""다음 조건은?"" → 이것이 AGI적 추론

   C. 인간 지능의 본질은 ‘구조’에 있다.
   • 인간은 뉴런 수보다 ""신경회로의 구조적 연결성과 메타학습 능력""으로 지능을 얻음
   • AGI 역시 모델 사이즈보다 행동 유도 시스템, 자기참조 시스템, 지속적 피드백 루프의 구조가 핵심

   AGI의 도달은 ""모델의 훈련""만으로는 불가능하며, 지능을 만들어내는 엔진 구조와 목적성 있는 자기개선 시스템이 반드시 필요합니다. 현재의 GPT류는 거대한 LLM(거대 언어모델)에 불과하며, AGI를 향하려면 추론 구조, 자기감시 구조, 목적 기반의 행위 정책이 함께 동작해야 합니다

        Hacker News 의견

     * 만약 '쓴 교훈(bitter lesson)'을 믿는다면, 모든 대충 넘어가는 엔지니어링은 결국 더 많은 데이터로 해결됨을 알 수 있음. 아마 8년 전에도 지금처럼 LLM이 이 정도 성능이 나오려면 뭘 해야 하는지 비슷한 이야기가 나왔을 것임. 그래서 나는 엔지니어링적 접근에 크게 동의하지 않음, 그리고 LLM이 아시모프나 SF에서 상상하는 AGI로까지 스케일업될 거라 생각하지 않음. 뭔가 더 근본적인, 과학이 아니라 공학이 결여되어 있음
          + 과학보다도 더 본질적으로 결여된 게 있는데, 바로 철학적인 부분임. 우리 인간이 이런 시스템을 인식하는 방식에서도, 그리고 시스템 자체의 내부에도 철학이 빠져 있음. LLM 기반 AGI라면 최소한 자기 가중치를 업데이트하며 스스로 학습하고, 셀프 파인튜닝이 가능해야 하지만, 현재로선 내장된 가중치와 한정된 컨텍스트 윈도우 사이에서 금세 벽에 부딪힘. 셀프 파인튜닝 시에 어떤 '주의 획득 메커니즘(attention mechanism)'을 어떻게, 얼마나 강도로 적용해야 일반 지능이 향상될지는 여전히 난제임. 믿을 만한 학문들에 집중해야겠지만 어떤 학문이 믿을 만한지, 어떻게 순수 지식만을 '공부'하게 할 것인지, 또 이론적으로 스스로 세계 최고의 인간 연구팀을 능가하게 되면 그 AI가 '어떤 존재'가 되는지까지 고민해볼 필요가 있음
          + ""손쉬운 엔지니어링보다 데이터 양이 많을수록 좋다""는 주장에 대해, 그게 단순한 데이터베이스보다 정말 더 신뢰성 있게 될 수 있을까 하는 의문이 있음. 언젠가 CPU보다 더 빠르게 코드를 실행할 수 있을까? 인간이 해내는 많은 일은 더 큰 두뇌가 아니라 기술 덕분에 가능해짐. 수학 공식 하나조차도 머릿속에서만 돌릴 때보다 종이에 써서 계산할 때 훨씬 나음(확장된 마음 논문 Extended mind thesis 참고). 3D 엔진을 돌린다는 건 인간 뇌만으로는 거의 불가능함. 언젠가는 AI가 자기 도구를 직접 개발할 정도로 똑똑해질 수 있겠지만, 그 전에 도구를 작성·유지할 수 있는 인프라가 필요함. 지금은 Python 접근 정도가 시작이지만, AI가 성과를 다음번에도 축적·활용할 수 있는 '지속성', 즉 디지털 메모장이나 동적 가중치 업데이트 같은 것이 더 필요함
          + 당신의 의견과 글 모두 공감함. LLM은 해답의 일부이고, 진짜 발전은 뉴럴넷 연구의 근본으로 돌아가는 데 있을 것이라 생각함. 언어는 인간과의 소통 그 자체임에도, 지금의 LLM은 결국 사람들의 작품을 데이터로 삼아 훈련된 거창한 Eliza처럼 보임. 예전에는 간단한 뉴럴넷으로도 환경 규칙에 따라 행동이 진화하도록 만들고, 유전적 알고리즘 기준에 맞게 스스로 행동을 학습했음. 지금 LLM은 너무나 '필터링'된 환경만 학습해서, 그 필터가 마치 네티즌 평균 IQ같이 작동하는 느낌임
          + 이게 사실 '쓴 교훈'이 말하는 바는 아님
          + 부족한 건 자기 교정(세계 모델/행동과 반응 관찰), 장기 일관성, 그리고 자기 확장임. 벤처캐피탈계는 3번째 문제에 가장 신경을 많이 쓰는 반면, Yann LeCun은 첫번째와 두번째를 더 걱정하고 있음. Hinton은 3번째 문제는 이미 필연적이거나 도래했고, 인류는 끝장났다고 생각함. 꽤 이상한 판임
     * LLM이 이런 식으로 설계된 데엔 이유가 있음, 사고(thinking) 기능이 나중에 붙는 것도 마찬가지임. 구조적으로 가능해야 하는 건 경사하강법을 쓸 수 있게 해야 한다는 점임, 그래서 분기(branch)가 없고, 라우팅은 추가적으로 붙음. 그리고 훈련 데이터가 있어야 함. 누군가가 글을 쓰기 전에 어떤 생각을 했는지 모두 기록한 수백만 페이지의 데이터는 현실적으로 존재하지 않음. 대부분의 생각은 언어가 아니기 때문임. 강화학습이 여기서 해결책처럼 보이나, 경사하강법과는 표본 활용 효율이 너무 낮아 파인튜닝 할 때만 쓰는 게 일반적임. LLM은 회귀(regressive) 모델이고, 모든 토큰이 단지 과거를 돌아볼 수 있게 하는 모델 세팅으로 아주 샘플 효율적으로 훈련 가능함(문장 하나가 수십 개의 샘플이 됨)
          + 언급하지 않았지만, LLM엔 '루프'가 전혀 없음. 반면 뇌는 단순한 뇌조차 수많은 루프 그 자체임. 뇌는 멈추지 않고 계속 입력을 받고, 하고 싶은 때 아무 때나 출력을 내보냄. LLM은 입력을 받아서 레이어를 따라 변환하고 곧바로 출력함. 강화학습이 답이 아니라 했는데, 나는 오히려 그게 유일한 답이라고 생각함
          + 이 이야기 참 흥미롭게 느껴짐. 즉, 비언어적 사고 레이어를 훈련 데이터로 쓰기 위해 뇌파를 읽는 뇌 스캐닝 기술 같은 걸 도입할 수 있음을 시사함. 대기업의 똑똑한 사람들이 이미 이런 인터페이스/제품을 염두에 두고, 전자기 뇌파 탐지 기술을 개발하고 있을 거라 추측함. 이 데이터로 스타트업의 슈퍼 AI 부트스트랩이 가능한 Kickstarter형 킬러 제품이 나올 수도 있을 것 같음. 첨단 시대임
          + 아주 먼 미래에 첨단 뇌 스캔 데이터를 AI 훈련 데이터로 쓰는 것이 현실적으로 가능해질 수도 있다는 상상을 해봄. 아마 Uploaded Intelligence(두뇌 전체를 디지털화하는 아이디어)와 AGI 사이의 잠정적 중간단계로 현실적일지도 모름
          + LLM은 그냥 회귀 모델일 뿐임. 15세기에 LLM이 있었다면 지구중심설만 캡짱이라고 설명해줬을 것임. 태양중심설 같은 혁신은 못함. 마찬가지로, 오늘날 LLM도 그저 우리가 아는 것만 알려주고, 생각하거나 혁신하거나 하지 않음. 추론 능력도 어느 정도 '필터링'일 뿐 실제 창의적 사고는 아님. 써 보면 쓸수록 LLM은 마치 '스테로이드 맞은 구글' 같음. 이 시스템으론 AGI까지는 절대 도달 못하고, 오히려 남아있는 AGI 열기와 자금만 먹어 치우는 느낌임
     * 이 글의 프레이밍(문제 설정)은 꽤 쓸모가 있음, 꼭 모든 처방을 믿지 않더라도 말임. 역사를 보면 두 가지가 동시에 일어났음을 알 수 있음. 첫째, 브루트포스 스케일링이 놀라운 도약을 만들고, 둘째, 시스템 레벨의 엔지니어링이 그런 가능성을 신뢰성 있게 실제로 쓸 수 있게 해줌. GPU도 좋은 예인데, 무어의 법칙이 FLOP(연산량)를 줬고, CUDA와 메모리 계층 구조, 드라이버 스택 덕분에 대규모 사용이 가능해졌음. 지금의 LLM은 마치 연산량(flop) 자체만 빠른 시점과 같아서, 인상적이긴 해도 아직 잘 다루기 힘듦. Claude Code, 도구가 보강된 에이전트, 메모리 증강 프레임워크 같은 제품에서 '시스템적 사고'의 시작 흔적이 보임. 아직은 조악하지만, 미래에는 파라미터 수만큼이나 시스템 오케스트레이션 자체가 중요해질 거라 생각함. '쓴 교훈'과 '엔지니어링
       문제' 주장은 상호배타적이 아니라, 오히려 둘 다 필요함. 쓴 교훈은 계산력+범용 방식이 ‘손수 만든 규칙’을 이긴다는 의미이고, 엔지니어링은 그걸 신뢰성과 지속성, 조합성을 높이는 구조로 감싸는 모르타르 개념임. 만약 그런 시스템이 없다면, 화려한 데모만 나오고 실제로는 몇 번 추론만 해도 깨질 것임. 그래서 진짜 진전은 '크기 VS 스마트'가 아니라 '크고 + 스마트하게 엔지니어링'해야 한다고 봄. 스케일업이 능력을 주고, 엔지니어링이 그 능력을 일반 지능처럼 활용할 수 있게 결정해줌
     * 이 논의는 일본의 제5세대 컴퓨터 프로젝트를 현대식으로 재탕하는 느낌임. 큰 데이터베이스 만들고 Prolog 쓰면 AI 르네상스가 온다고 믿던 시절처럼 들림. 그냥 '분산 아키텍처' 어쩌고 하며 모듈만 잇는다고 해도 AGI와는 거리가 멀음. 근본이 되는 빌딩블럭, 즉 토대가 훨씬 좋아져야 함. LLM이 그나마 기여한 건 유저 '의도 파악'이 예전보다 엄청 좋아졌다는 점임. 컴퓨터가 텍스트만 읽어도 의도를 훨씬 잘 뽑아내게 되었음. 근데 그거 말고는 추론, 검색, '메모리' 같은 요소들은 여전히 같은 옛날 방식임. 이건 현재 하드웨어나 시스템 한계가 아니라, 정보이론/컴퓨터과학의 한계 때문임
          + Transformer의 Attention 메커니즘은 꽤 훌륭함. 모델 엔지니어링에서 또 한 번 이런 대혁신의 사이클이 필요함. 데이터만 많다고 답이 아님. 인간 두뇌만 봐도, 굳이 인터넷 전체 데이터를 안 써도 충분히 똑똑해지며, 에너지 소모도 적음
          + 맞음. 현재 아키텍처에서도 더 좋은 엔지니어링만으로도 활용도는 높일 수 있음(‘에이전트’들이 그 예임). 하지만 오직 엔지니어링만으로 AGI가 가능하다고 주장하는 건 과한 희망임. 진짜 어려운 건 자체적 학습과 발견, 고가의 대규모 사전 훈련 없이도 새로운 것을 배우고, 환각(hallucination) 문제 없이 문제를 해결할 수 있는 시스템을 만드는 것임. 이건 완전히 새로운 컴퓨터 과학적 혁신이 필요하고, 지금 접근법으론 힘드리라고 봄
     * AGI, 즉 인공지능에서 'G'는 General임. 즉, 모든 지식을 훈련받아야 하는 바보 AI가 아닌, 일반 지능은 단순히 셈하는 법, 논리 기초, 그리고 한 인간 언어만 가르치면 나머지 논리적 인간 과학은 그 AGI가 스스로 '재발견'하게 됨. 우리의 다음 과제는, 그렇게 AGI가 자기 스스로 발견한 현상들에 붙인 이름을 우리가 쓰는 이름과 동기화하는 것임. 가벼운 초등교육만으로도 원리만 깨닫고 스스로 개선, 발전해 우리를 뛰어넘으면 그게 바로 '인공 이해(artificial comprehension)'임. 현 AI는 충분한 데이터만 주면 '범용 문제 해결사'는 가능하지만, AGI는 '이해'와 '파악' 능력 자체가 필요한 영역임. 관찰을 즉각적으로 분해해, 타당성이나 조합의 가능성을 파악하고, 깨어있는 동안엔 자기 안전까지 실시간으로 점검하는 '동적 이해' 능력이 있어야 진정한 General intelligence라고 할
       수 있음
          + AGI가 일반 지능이라는 정의처럼 정말 조금만 가르치면 나머지는 스스로 파생해서 배우는 시스템이어야 한다고 했지만, 자연계의 '일반 지능'은 그렇지 않음
     * 10년 전에 상상했던 초기 AGI의 모습이 바로 Claude Code 같은 거라고 느끼는 나만 이상한가? 임의의 목표에 대해, 주로 텍스트 영역에서는 계획도 세우고 액션도 취할 수 있음. 텍스트 파일에 메모리도 유지함. 아직 장기 목적이나, 육체적 구현이나, 사리 이해력은 부족하지만, v1 버전은 이런 모습을 보일 거라 기대했음
          + 사실 나는 AGI란 말 듣고 바로 Star Trek의 'Data'나, 최소한 터미네이터의 T800이 떠오름. AGI가 꼭 자의식을 가져야 한다고 생각하진 않지만, 내 머릿속 AGI는 '자의식'을 포함하는 게 판타지임. Claude Code는 대단해도 AGI랑 혼동할 수준은 아님
          + 완전 공감함. 특히 내가 자주 급하게 써 준 명령어들도 꽤 미묘한 의미까지 잘 파악해서 고쳐줌. LLM 활용도, 정말 작은 기능 추가만 해도 천지차이임 (예: Claude Code의 plan mode 등), 단순 성능 업데이트보다 훨씬 더 효용이 큼
          + Claude Code는 자의식도 자각(sapient)도 없음. 대부분의 사람들이 AGI라 하면 최소한의 자의식 정도는 상상함. Star Trek를 빗대자면, 엔터프라이즈의 주 컴퓨터는 AGI가 아니고, Data가 진짜 AGI임. 가장 큰 차이는 '명확한 정체성'과 '자기 개념'이 부재함임. Claude Code는 프롬프트에서 역할은 수행하지만 영속성이 부족함
          + 혼자만 그런 게 아님. AGI 논의는 늘 헷갈리는 부분임. Claude는 분명히 인공 일반 지능임에도, AGI에 대한 의미가 계속 바뀌고 정의도 명확하지 않음
          + ""기본적(basic) AGI""라는 말로 실제 AGI가 빠진 이유들 다 얼렁뚱땅 넘어가려고 하는 것임
     * 우리는 AGI가 생물학을 벗어나 정말 가능할지조차 전혀 알지 못함. 이게 핵심임. 영화 Chappie 식 AGI가 진짜 가능성 있는지에 대한 힌트조차 없다면, 완전 깜깜이 탐색이나 마찬가지임. 비교하자면 양자컴퓨팅은 '가능'하다고랑 '실현가능'하다는 건 이미 밝혀졌고, 현재는 엔지니어링만 남았음(그래도 어떤 사람은 그마저도 허상이라 생각하지만)
          + AGI가 전자 컴퓨터에서 원천적으로 불가능하다고 판명되면 뇌가 일반 지능을 구현할 때 무슨 일을 하는지 물리학적으로 굉장한 대발견이 필요한 셈임
          + 오히려 인간이라는 작동하는 '일반 지능' 예시는 이미 한 개 있고, 양자컴퓨팅은 아예 구현된 게 없는 상황임
          + 말이 안 됨. 만약 영혼 같은 걸 믿으면 AGI가 안 될 수도 있겠지만, 순수 생물학적 존재라면 원리적으로 당연히 복제가 가능함
          + 그게 핵심이라는 주장엔 동의 못함. 결국 실제로 해봐야 답이 나오는 문제임. 애초에 사전확정적으로 어떤 결론이 가능한지 증명할 수 있어야 할 필요는 없음. '핵심'이나 '명확한 힌트'에서 슬쩍 빠져나온 느낌임. 우리가 '생물학적 필요조건' 없이 가능한 명확한 근거는 충분히 있음. AGI의 실현 가능성, 필요성, 당위성은 별개 문제지만, 원글도 도전 과제는 충분히 나열하고 있음
          + 양자컴퓨터의 실용적 실현 가능성 역시 아직은 열린 연구 과제임
     * 우리가 '지능'이라 부르는 건 LLM처럼 동작하지 않음. 뇌는 연속적임—입력 한 세트가 끝나서 멈추는 게 아니라 입력이 올 때까지, 아니 계속 피드백을 돌림. 본질적으로 훈련 모드를 끝내지 않음. 물론 생애주기별로 뇌는 최적화(예: 미엘린화) 되지만, LLM은 훨씬 방대한 정보로 학습 후, 파인튜닝 정도 빼고는 모델이 고정된 상태로 남음. 뇌는 컨텍스트를 지속적으로 관리함. 대부분의 입력은 특별한 네트워크들이 사전 처리에서 아주 많이 필터링함. AGI 일부분이 시스템적 어프로치를 필요로 한다는 건 인정하지만, 진정한 AGI에는 아키텍처적 변화가 필요할 것이라 생각함
     * LLM이 이제 발전의 끝, 이게 한계라고 쓰는 사람들이 왜 그렇게 확신하는지 이해가 안 감. 아직 한 해도 제대로 안 지났고, 여전히 LLM 기반 AI는 계속 발전 중임
          + 발전의 여지가 남아있다고 해도, 결국 그 범위가 한정돼 있다는 게 그대로임. 개별 태스크에선 꾸준히 좋아지지만, '전반적'인 개선은 이제 잘 안 보임
          + 이런 주장하는 분들도 실제로 LLM이 좋아지고 있는 데엔 동의하는지 궁금함
     * 이 글은 ""어려운 문제 다 풀면 다 된다""는 식으로만 보임. 이게 뭐... 네, 맞아요, 그런데요?
          + 최근 LLM 발전이 너무 보수적이고, 아키텍처 혁신 없이 규모만 키우는 추세라 이런 논의가 유의미함
          + 글에서 어려운 문제 자체를 아예 논하지 않음. 하이테크 업계 사람들은 공학만 있으면 어떤 문제든 다 풀 수 있다는 사고방식이 좀 있음
          + 원글은 어떤 문제가 있고, LLM이 그걸 어떻게 해결 못하는지 분명히 짚어내고 있음
"
"https://news.hada.io/topic?id=22723","TS Backend Meetup: 타입스크립트 백엔드 개발자들의 모임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 TS Backend Meetup: 타입스크립트 백엔드 개발자들의 모임

   타입스크립트로 백엔드 개발을 하는 사람들이 모여 매월 밋업을 진행하는 TS Backend Meetup을 시작했습니다.
   NestJS, Node.js, 인프라, 오픈소스 등 TS 기반 백엔드 개발 주제를 중심으로 발표와 토론을 합니다.
     * GitHub: https://github.com/ts-backend-meetup-ts/meetup
     * 발표 자료와 코드 예제는 오픈소스로 공개 예정입니다.
     * 현재는 서울에서 오프라인 모임을 중심으로 운영하고 있습니다.

   관심 있는 분들은 저장소를 참고하시고, 연사자로도 함께 참여해주시면 좋겠습니다.

   몇번 다녀왔는데, 좋았습니다. 추천함.

   다음번 모임에 참가해보고 싶네요
"
"https://news.hada.io/topic?id=22679","모든 엔지니어에게 세일즈 콜을 강제했더니 2주 만에 플랫폼이 완전히 재작성됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               모든 엔지니어에게 세일즈 콜을 강제했더니 2주 만에 플랫폼이 완전히 재작성됨

     * 한 스타트업은 엔지니어들이 직접 세일즈 콜에 참여하게 하면서 제품 개발 방식을 근본적으로 바꾸게 되었음
     * 세일즈 콜 과정에서 그들은 경쟁사 제품이 비기술 사용자에게 너무 복잡하다는 것과 고객들은 로그/지표보다 그저 모니터링이 작동한다는 확인 표시(녹색 체크) 를 원하며, 그냥 “누군가 대신 해줄 수 없냐” 는 요구를 한다는 것을 알게 됨
     * 이를 통해 백엔드 중심의 팀은 사용자 관점을 이해하게 되었고, PM의 개입 없이도 새로운 아키텍처를 스스로 구상하기 시작했음
     * 단 2주 만에 플랫폼을 60% 기능 축소, 진행 상황을 보여주는 프로그레스 바 추가, Slack 연동, 대행 워크플로우 기능을 구축했으며, 결과적으로 지원 티켓이 70% 감소했음
     * 이 경험을 통해 얻은 교훈은, 사용자는 문제 해결만 원하지 우아한 코드에는 관심이 없으며, 기능은 코드량이 아니라 사용자 혼란이라는 비용을 발생시킨다는 점이었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경과 실험

     * DevOps 시니어 엔지니어는 세일즈 콜 참여에 반대했지만, 최소 5콜만 해보자는 조건으로 동의했음
     * 창업자는 엔지니어들이 직접 고객의 언어와 문제를 들어야만 제품 설계가 달라진다고 판단했음

세일즈 콜에서 얻은 인사이트

     * 경쟁사 제품이 비기술 사용자에게 너무 복잡하다고 얘기 함
     * 고객은 복잡한 지표보다 단순한 녹색 체크마크 같은 직관적 확인을 원했음
     * 다수의 고객이 “대행 서비스” 를 요구하며, 단순 사용보다 문제 해결 자체를 외주화하고 싶어함

제품 재작성 결과

     * 팀은 기존 기능의 60%를 제거하고 필수 기능에 집중함
     * 단순한 프로그레스 바를 추가하여 사용 경험을 개선함
     * Slack 통합을 통해 문의 흐름을 간소화함
     * Done-for-you 워크플로우를 제공하여 고객의 요구를 직접 반영함
     * 최종적으로 지원 티켓이 70% 감소, 제품 사용성과 만족도가 크게 향상됨

핵심 교훈

     * 사용자는 우아한 기술적 해법이 아니라 문제 해결 자체를 원함
     * 기술적 정확성보다 사용자 이해가 더 중요함
     * 모든 기능은 코드가 아니라 사용자 혼란이라는 비용을 동반함

팀 문화 변화

     * 이후 회사는 모든 엔지니어가 분기마다 5회의 세일즈 콜에 참여하는 것을 의무화함
     * 엔지니어들이 직접 고객의 피로감을 듣고, “그냥 잘 작동하기만 하면 된다”는 요구를 접하며 제품 직관을 키우는 것이 문화로 정착함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    레딧 댓글 주요 요약

     * PM 역할 논쟁
          + 일부는 좋은 Product Manager 부재가 문제라고 지적하며, 엔지니어가 고객 통화까지 하는 것은 비효율적이라고 주장
          + 반대로 “PM은 오히려 번역기 역할에 그치며 엔지니어가 직접 고객 문제를 소유할 때 최고의 결과가 나온다”는 반박도 존재
     * 고객 접점의 가치
          + 여러 댓글에서 엔지니어가 직접 사용자 목소리를 듣는 경험이 강력한 인사이트를 준다고 강조
          + 초기 단계 스타트업이나 작은 팀에서는 엔지니어가 곧 PM 역할을 일부 수행하는 경우가 흔하다는 의견도 있음
     * 비판적 시각
          + “이건 단순히 리더십/문화의 실패를 엔지니어에게 떠넘긴 것”이라는 비판
          + “기능을 잘라내고 단순화하는 게 진짜 개선이냐”라는 반론도 있으며, 장기적으로는 파워 유저와 고급 기능 요구를 외면할 위험을 경고
     * 긍정적 사례 공유
          + 은행, 의료기기 등 여러 업계에서 비슷하게 모든 직원이 고객 지원·영업을 체험하는 제도가 있었다는 경험담 다수
          + “Dogfooding”이나 “직접 고객 앞에 서는 경험”이 제품 품질과 문화에 도움이 된다는 공감
     * 종합 시사점
          + 고객 문제를 직접 체감하게 하는 것은 강력한 학습 도구지만,
          + 동시에 장기적으로는 전문적인 PM·UX·디자인 역량과 결합되어야 균형 잡힌 제품 전략을 만들 수 있다는 점이 논의의 핵심으로 드러남

   결국 효율성의 문제겠지요.
   고객과 직접 협의가 좋은 점이 정말 많지만
   회의와 전화 등 잦은 의사 소통으로 인해서 업무 연속성이 자주 깨지고 개발에 투자할 시간이 줄어들게 되죠.
   그렇다면 회사는 낮아진 생산성에 대응할 채용 계획을 잡아야 할 겁니다.
   대개는 그럴 생각이 없단게 문제죠.
   결국 시간 부족으로 시일이 지날 수록 품질 저하를 맞이할 가능성이 높아질 겁니다.
   일장일단을 잘 고려해야 한다고 봅니다.

   해커 뉴스에 왜 old reddit 링크로 남겼는지 잘 모르겠지만, 지금 reddit UI 경로는 여기에 있습니다.

   해커뉴스는 레딧 URL 올릴때 대부분 old reddit 으로 올리는 것 같아요. 가볍기도 하고 해서 그런듯 합니다.

   저점을 높이는게 목표인 조직이라면, 웹프론트 개발자로만 이루어진 팀, 앱개발자 팀처럼 역할 조직이 맞다고 인정한다.

   하지만 고점을 목표로 하는 팀이나 조직에서는 역할 조직으로 구성하는 것은 반드시 한계가 있다.
   본문의 내용과 같이, 굳이 기획자, 디자이너, PM, 엔지니어가 각각의 일을 맡아, 공장의 컨베이어벨트처럼 일해야하는가? 하는 의문이 생기는 것인데, 담당하는 몇 가지만 맡아서 일하는 전형적인 ""담당자""식의 일이 아니라, 각 분야에 스페셜리티를 가진 구성원이 모여서 공통 목표를 함께 설정하고, 전 구성원이 서포트하는 형식이 이상적이다.

   여러 회사에서는 분사, 팀 구성 등의 테스크포스 형태로 조직을 구성해 나가는데, 이 또한 사람(역할)만 묶어놨기 때문에 부적 강화(내가 무언가 하는데, 회사가 도와주질 않네, 그냥 포기해야지와 같은 패턴)가 발생해, 키 멤버와 같은 주요 인재만 잃을 수 있으니, 목적조직 또한 역할 조직의 적극적인 서포트가 반드시 필요하다.

        Hacker News 의견

     * 마지막엔, 그들은 내 “PMing” 없이 완전히 새로운 아키텍처를 구상했음. 그 이유는 우리 제품을 실제로 사용하는 사람이 누구인지 드디어 제대로 이해했기 때문임. 이 경험을 전체적으로 보면, “엔지니어한테 세일즈 콜을 맡겨봤더니, 결국 문제는 PM들이 고객과 엔지니어링 사이 소통을 제대로 못 하고 있었고, DevOps 엔지니어가 오히려 고객 니즈를 실제 작동하는 솔루션으로 빠르게 전환하는 사람이었다”는 결론임
          + 엔지니어가 자신감에 차 있어서 사용자가 제품을 잘못 쓴다고 생각하는 경우가 있음. 이는 “사용자가 멍청하게 굴어서” 생긴 일이지, 내가 만든 복잡한 디자인에는 문제가 없다는 논리임. Desktop Linux 진영 사람들만 해도 사용자 불만을 무시한 경험으로 책 한 권 쓸 수 있을 정도임. 고집 센 엔지니어가 PM과 사용자보다 본인이 낫다고 여기면 아무 일도 진전이 없음. 하지만 그런 엔지니어를 사용자 직접 상대하게 던져놓으면, 평소 친근한 PM 대신, 실제 짜증 내는 유저들이 “이 멋진 창조물”이 마치 문제 많은 햄스터라도 되는 양 혹평을 쏟아냄. 이런 과정은 두려움도 주지만 엔지니어 자존감도 무너지게 함. 내 관점에선, 이건 PM이 멍청하다고 보여주려는 게 아니라 엔지니어를 겸손하게 만드는 일임. 시간이 지나면 또 엔지니어 자만심이 돌아와서 이
            과정은 반복적으로 필요함
          + 나는 대형 기업의 기계공학팀에서 오직 코드를 쓸 수 있는 사람임. 사내 IT팀이 다양한 소프트웨어를 직접 만들지만 대다수는 팀원들 눈엔 별로임. 그래서 아예 이걸 새로 만들거나, 도저히 대체 못 하는 “별로인” 프로그램을 보완하는 툴을 직접 고안해서 일을 쉽게 함. 내가 인하우스 IT팀보다 개발을 잘한다기보다는, 실제 사용자 시각 때문에 본인, 즉 우리 팀에 진짜 필요한 게 뭔지 더 잘 파악한다는 생각임. 그리고 내가 바로 이 소프트웨어를 쓰는 사람이니 당연히 동기부여도 높음. 그래서 글 제목이 나한테 공감이 갔음. 단, 본문에서 말하는 부분도 현업에서 충분히 일어날 수 있다고 생각은 함. 나는 공식적인 소프트웨어 개발/프로젝트 관리 프로세스에는 익숙하지 않음
          + 나는 연 200만 달러 매출의 작은 테크 스타트업 경영 중임. 종종 지원 인력이 부족할 때 직접 며칠간 고객 지원을 맡기도 했음. 그럴 때마다 신기하게도 평소엔 엔지니어팀엔 전혀 안 전달되는 고객 불만 사항을 잔뜩 발견함. 지원 담당자들이 문제를 자체적으로 “해결”하는 데 집중해서 근본적인 개선이 엔지니어링까지 이어지지 않는 경향이 있음
          + 원래 소프트웨어가 왜 그렇게 엉망이었는지, 아무도 궁금해하지 않은 게 눈에 띔. 내 생각엔 PM 한 명에게 책임을 다 넘길 수는 없음. 사실은, Agile/Scrum 같이 책임이 흐려지고 개발자들이 엉성하게 만들어진 티켓들을 너무 빠른 주기로 처리하느라 결국 이렇게 어설픈 소프트웨어가 생기는 시스템 문제임. “현대” 소프트웨어 조직이 비대해지면 흔히 나오는 결과물임
          + 이 글을 보고 든 첫 생각이 “PM이 다 개입하기 전, 소프트웨어는 원래 이렇게 만들어졌었지”임. 엔지니어를 실제 운영하는 사람 옆에 앉혀두면, 사실 PM이 필요 없고 모두가 훨씬 행복해지는 경험을 자주 함. 물론 PM 중에도 대단한 사람들 있긴 한데, 내 경험상 PM들은 영역 싸움에 집착하는 경향이 있고, 놀랍게도 엔지니어링이나 고객 쪽 모두에 대해 잘 모르는 경우도 많음
     * 많은 곳에서 엔지니어가 제품과 엇박자가 난 경험을 여러 번 겪었음. 동료가 모르게 뭔가 추가해뒀다가 UI가 헷갈리거나, 웹사이트 내용이 제품 실제와 안 맞는 경우도 있었음. 또, [제품 -> PM -> 버그 트래킹 시스템 -> 엔지니어 -> 수정 -> QA -> 제품] 식의 긴 루프 때문에 중요한 건 고쳐지는데, 사소한 불편은 정말 오랫동안 안 고쳐짐. [제품 <-> 엔지니어]만 남기면 놀라울 정도로 생산성 향상 가능함. 많은 엔지니어가 실제로 제품 전체 경험을 직접 해본 적도 잘 없고, 올해와 작년의 차이도 잘 모르는 경우가 있음
          + 나도 비슷한 경험을 가졌는데, 신기하게도 PM이 많은 곳에서 이런 현상이 더 자주 있었음. 내가 겪은 최악의 경험은 한 회사에서 PM과 “Product Designer”를 엔지니어 숫자에 맞춰 강제 비율로 배정하려 한 경우임. 디자이너, 제품, 프로젝트, 프로그램 담당자 전부 합치면 엔지니어 수보다 더 많음. 결국 더 최악의 상황만 만들어졌음. PM들의 관료주의와 내 역할 침해 우려를 피해다니는 것도 일이었음. 뛰어난 PM은 정말 소중하지만, 요즘 Product Management라는 타이틀은 관료적이고 프로세스에 집착하는 사람이 너무 많이 모여든 느낌임. Product Management 인플루언서들도 문제를 심화시키고 있음
          + 그렇다고 엔지니어를 억지로 세일즈콜에 투입하는 것도 정답이라 생각하진 않음. 전화 한 통을 성공적으로 진행하려면 다양한 소프트 스킬이 필요하고, 엔지니어는 그런 분야로 훈련도 안 돼 있고, 채용 때도 고려 대상이 아님. (‘세일즈콜’이라 함은 데모나 PoC 전의 초기 상담콜 의미. 복잡한 프리세일즈 데모에는 엔지니어가 동행하더라도, 그 역할은 원칙적으론 제품 담당이 해야 맞다고 봄)
          + 잘못될 수 있는 경우가 정말 다양한데, 나는 그런 모든 사례를 직접 본 적 있음. 예를 들어, 모든 고객 소통을 PM이나 PO가 독점하게 하거나, 고객이 개발자와 대화 자체를 피하면서 사용자 매니저 요구사항만 해석해서 전달하는 경우, 개발자 본인이 오직 코드만 쓰고 싶어 하거나, 모든 소통을 PM과 버그 트래커로만 하도록 강요하는 경우 등 정말 다양함. 또, 상용 소프트웨어 플랫폼 쓸 때는 기술적 한계로 워크플로우가 무척 별로가 될 수 있음. 항상 어딘가 소통을 막는 요인이 있고, 고객/중간관리자/개발자 누구든 그걸 가로막을 수 있음. 심지어 잘못된 솔루션을 처음부터 정해놓고 개발자나 사용자는 세부 내용을 전혀 모른 채 시작하는 경우도 적지 않음. 사용자에게 진짜 좋은 시스템을 만들지 못하는 길이 정말 많음
          + 엔지니어들이 제품 자체를 깊이 이해하는 게 정말 중요하다고 생각함. 기본적인 엔지니어링보다 제품 측면이 맞아떨어지는 게 더 어려운 지점임. 내가 경험한 대부분의 제품이 결국 제품적인 이유 때문에 실패했기에, 팀의 강점을 이쪽에 맞추는 게 논리적으로 더 맞는 포인트라 봄
     * 반대로, 엔지니어가 결국 기술 지원팀의 역할로 전락하는 경우도 있음. 각 고객사를 직접 지원하다 보면 핫픽스와 개별 맞춤 솔루션만 쌓이게 되고, 이 모든 게 제대로 테스트도 못 하니 기술 부채만 쌓이고, 크고 제대로 된 투자받은 경쟁사가 더 멋진 제품 만들면 금방 도태됨. 이건 전형적인 제품 관리 실패라고 생각함. PM이 고객 니즈를 엔지니어에게 제대로 전달 못 하거나, 그 반대로도 Push를 못 한다는 얘기임. 엔지니어가 세일즈콜에 투입되는 건, 고객층이 어느 정도 커지고 성숙해지면 확장성 없는 방법임. 만약 제품 관리자가 엔지니어에게 세일즈콜 하길 진짜 원한다면, 그 엔지니어도 각 계정의 커미션 일부를 받아야만 공평함. 내가 커미션 없이 세일즈콜 맡는 일은 절대 없음
     * 스타트업처럼 모든 팀원이 고객 니즈에 깊이 공감이 필요한 환경엔 매우 뛰어난 전략임. 내가 제품 요구사항을 실제로 정의하는 데 참여해서 실무를 꿰뚫고 있을 때 프로젝트 성공률이 훨씬 높았음. 반면 요구사항만 “넘겨받아” 그대로 구현할 때보다 훨씬 더 좋은 결과물이 나옴
          + 내가 실제로 지침을 썼으니 따르기 더 쉬웠다는 뜻인가, 아니면 직접 참여해서 더 나은 UX가 나온다는 말인가 궁금함
     * “2주 만에 리라이트 완성, 기능 60% 제거, 단순 진행바 추가, Slack 연동 구축, ‘done-for-you’ 워크플로우 제공→ 지원 티켓 70% 감소” 이게 진짜면 뭔가 심각하게 잘못된 상황임
          + Reddit은 창작글 폭증으로 유명하고, 이 이야기 역시 실화에 영감을 받았든 완전 창작이든, 전형적인 Reddit식 글쓰기 요소에 LinkedIn식 비즈니스 스토리텔링 느낌이 가득함
          + 이건 B2B SaaS가 여러 번 피벗을 거치면서도 제품 관련 가이드가 빈약했던 경우라 봄. 물론 이런 방식으로 일이 꼬이는 경우가 생각보다 아주 흔함
          + LinkedIn 스타일의 짧은 문장 뒤 극적인 결말이 반복되는 어투 보면, 이건 창작임을 쉽게 눈치챌 수 있음
          + Reddit이니 당연히 창작임. 이런 글이 어쩌다 이런 화제가 되는지 모르겠음
     * 이런 결과라면 당장 PM, PO, 마케팅팀 다 해고해야 함. 두 가지가 분명함: 첫째, 그 사람들이 고객이 실제로 뭘 원하는지 파악도 못 했거나, 그 니즈를 개발팀에 제대로 전달 못 했거나, 둘 다임. 둘째, 머릿속이 너무 시스템적으로만 굴러가서, 차라리 고객과 개발팀 사이 모든 중간 단계를 없애버리는 게 나을 수도 있음
     * 예전 직장에서도 엔지니어가 영업콜에 꾸준히 들어가곤 했음. 어떤 고객이 뭘 원하는지, 우리 제품이 어떻게 팔리는지 체험하는 건 흥미로웠지만, 획기적이진 않았음. 고객이 원한 기능은 이미 로드맵에 있었고, 헷갈리는 기능 하나도 있었지만 그건 최대 고객사의 특수 요구 때문에 그렇게 설계됐었음. 개발팀은 좀 더 단순하게 만들고 싶었지만 그럴 경우 큰 고객사 요구를 충족할 수 없었음. 결국 쉽게 쓸 수 있는 “라이트” 버전을 따로 만들고, 큰 고객사 빼고 전부 그걸 쓸 수 있게 함. 그런데 이런 변화도 영업콜 동행 때문이 아니라, 모두가 어려운 줄은 처음부터 알았으나 로드맵에 반영될 때까지 바꿀 순 없었음
     * “실사용자가 누구인지 드디어 제대로 이해하게 됨”이라는 부분에 깊이 공감함. “대부분 엔지니어의 최대 문제는 오버 엔지니어링”이라고 하지만, 실상은 고객 사용사례를 제대로 이해 못 해서 오버 엔지니어링이 발생하는 경우가 더 많음. 이 부분이 더 본질적인 문제임. 나도 엔지니어로서 가장 자주 겪는 답답함은, 다른 엔지니어들이 실제로 팔리는 제품이 뭔지 알려고 들지 않는다는 점임. 이유는 업무적 적합성이든, 자존심이든, 대개는 조직 문화나 인센티브 구조 때문임
     * 예전에 CRM용 로보콜 솔루션 만드는 회사에 있었는데, 오프사이트 행사 때 모든 사람이 직접 콜드콜 해보고 대본대로 따라 해보자고 했음. 이게 비영업 인력한테 어떤 불안감을 주는지 이해도 없고 관심도 없음. 그 일 이후로 이직을 생각하기 시작했음
     * 예전에 비슷한 상황을 직접 본 적 있음. 모니터링팀이 “모든 경보를 프런트라인 엔지니어가 직접 티켓으로 등록해야 한다”고 주장함. 그런데 경보가 시간당 1만 개 이상임. 중요한 이슈가 다 ‘노이즈’ 속에 묻히다 보니, 관리진도 지치고, 한 번은 ‘모니터링팀 네가 한번 전부 직접 티켓 열어서 해결해 봐’라고 강제하게 됨. 그 다음 날엔, 중요도 낮은 알람을 배제하니 고유 알럿이 시간당 십여 개로 줄었고, 나머지도 순차적으로 다 닫힘. 이때야 비로소 “보드가 다 초록 불”이 진짜가 됐음(그 전엔 미디어랑 Gartner Group 보여주려고 억지 녹색을 칠했을 뿐이었음)
"
"https://news.hada.io/topic?id=22672","Go는 여전히 좋지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Go는 여전히 좋지 않음

     * Go 언어 설계의 여러 결정들이 불필요하거나 기존 경험을 무시한 채 이루어졌음
     * 에러 변수의 범위 관리 문제가 코드 가독성과 버그 탐색을 어렵게 만듦
     * nil의 이중성, 메모리 사용, 코드 포터블성 등 여러 부분에서 비직관성 및 현실과 맞지 않는 설계 나타남
     * defer 구문의 한계 및 표준 라이브러리의 예외 처리 방식이 예외 안전성 확보를 어렵게 만듦
     * 메모리 관리 및 UTF-8 처리 부실 등 누적된 문제들이 Go 코드베이스 품질에 장기적으로 악영향을 주고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Go 언어에 대한 장기적 비판

     * 이전 게시글(Why Go is not my favourite language, Go programs are not portable)에서 밝혔듯이, 나는 10년 이상 Go 언어의 여러 문제점을 지적해 왔음
     * 특히, 이미 알려진 좋은 사례들을 무시한 불필요한 설계 결정들이 점차 더 아쉽게 느껴짐

에러 변수 범위의 비직관성

     * Go의 문법은 에러 변수(err)의 범위를 불필요하게 넓혀 오류 발생 가능성을 높임
          + 예시 코드에서 err 변수가 함수 전체에 살아있고, 재활용되며, 이로 인해 코드 가독성과 유지보수성이 저하됨
          + 숙련된 개발자는 이런 범위 문제로 인해 버그 탐색에서 오해와 시간 낭비를 경험함
          + 변수를 적절히 지역적으로 한정할 수 있는 방법이 문법적으로 허용되지 않음

두 가지 형태의 nil

     * Go에는 interface 타입과 포인터 타입 각각에서 nil이 다르게 동작하는 혼란이 존재함
          + 아래 예시처럼 s(포인터)와 i(interface)에 nil이 할당되어도, s==i는 다르게 평가되는 등 일관성 없는 동작을 보임
          + 이는 null 처리에서 일반적으로 피하고 싶은 문제로, 설계상의 충분한 고민 없이 만들어진 흔적임

코드의 이식성 한계

     * 조건부 컴파일을 위한 주석 사용은 유지보수성과 포터블성 면에서 현저히 비효율적임
          + 실제로 포터블 소프트웨어를 만들어본 경험이 있다면, 이러한 방식이 번거롭고 오류를 유발함을 알 수 있음
          + 역사적으로 쌓은 경험(코드 포터블성, 실무적 사례들)이 무시됨
          + 자세한 내용은 Go programs are not portable을 참고

append의 소유권 불명확성

     * append 함수와 슬라이스의 소유권 관계가 명확하지 않아 코드 예측이 어려움
          + 예제를 통해, foo 함수에서 슬라이스를 append 시켰을 때 실제로 원본에 어떤 영향이 있는지 미리 알기 어려움
          + 언어의 알아두어야 할 ‘quirk’들이 늘어나 실수를 야기함

defer 구문 설계 미흡

     * RAII(Resource Acquisition Is Initialization) 원칙처럼, 자원 해제를 명확히 지원하지 않음
          + Java와 Python의 구조적 자원 관리 구문 대비, Go는 어떤 자원이 defer로 해제되어야 하는지 명확히 알 수 없음
          + 예시처럼 파일 작업 시, double-close 문제까지 직접 다루어야 하며, 올바른 해제 순서와 방식이 불분명함

표준 라이브러리의 예외 처리

     * Go는 명시적 예외(exception)를 지원하지 않는 구조지만, panic 등 예외 상황은 여전히 발생함
          + panic이 일부 상황에서 완전히 프로그램을 종료하지 않고 잠식하는 경우도 있음
          + 표준 라이브러리(fmt.Print, HTTP 서버 등)에서 예외를 무시하는 패턴이 존재해, 진정한 예외 안전성 보장이 불가
          + 결국 예외 안전 코드 작성은 필수이나, 직접적으로 예외를 사용할 순 없음

UTF-8 처리와 문자열

     * string 타입에 임의의 바이너리 데이터를 넣어도 Go는 특별한 검증 없이 작동함
          + 과거 UTF-8 인코딩 이전에 생성된 파일명 등이, 조용히 누락되는 사례를 겪을 수 있음
          + 백업 등에서 중요 데이터가 손실될 수 있으며, 실무 상황을 반영하지 않은 간단한 처리 방식임

메모리 관리의 한계

     * RAM 사용량에 대한 직접적 제어가 어렵고, GC(가비지 컬렉션)의 신뢰성도 한계가 있음
          + Go의 메모리 사용량이 증가해 장기적으로 비용 및 성능 이슈로 연결됨
          + 여러 인스턴스, 컨테이너 환경에서 비용 및 확장성 문제가 실제로 발생함

결론: 더 나은 길이 있었음

     * 이미 기존에 효과적으로 입증된 언어 설계들이 있었음에도 불구하고, Go는 많은 부분에서 그것을 외면함
          + Java 초기안의 문제점들과는 달리, Go가 출시될 당시 이미 더 나은 접근법이 있었음

참고 자료

     * Uber: Data race patterns in Go
     * FasterThanLime: Lies we tell ourselves to keep using Golang
     * FasterThanLime: I want off Mr Golang’s wild ride

        Hacker News 의견

     * Go를 pre-1.0 시절부터 거의 모든 풀타임 직장에서 써왔음. 팀원들이 기본을 배우기에 단순하고, 대체로 안정적으로 동작함. Go 최신 버전으로 업데이트할 때 걱정할 일이 거의 없으며, 대부분 유용한 기능이 기본적으로 포함되어 있음. 컴파일 속도가 빠른 점이 매력적임. 병렬 처리는 약간 까다롭지만, 시간을 투자하면 데이터 흐름을 표현하기 좋아짐. 타입 시스템은 대부분 편리하지만 가끔은 장황한 면이 있음. 전반적으로 믿을 만한 도구임. 하지만 기사에서 언급된 여러 지적에 공감하게 됨. Go는 구세대 개발자들이 너무 원칙에만 집착해서 실용적 편의를 놓친 부분이 분명히 있음. 물론 이건 내 느낌이고, 모든 단점을 다 해결했다면 지금보다 더 안 좋았을 수도 있다고 생각함. 최근 몇 년 사이에는 쿼크 수정에 더 열려진 분위기가 느껴진다는 점도
       언급하고 싶음. 한때 제네릭이나 커스텀 이터레이터가 추가될 줄은 상상도 하지 못했음. RAM과 이식성에 관한 지적은 다소 개인적 불만처럼 느껴짐. 개선되면 좋겠지만, GC가 대부분의 프로그램에서 심각한 문제를 일으키는 경우는 극히 드물고, 디버깅 또한 어렵지 않음. 그리고 Go는 거의 모든 중요한 플랫폼을 지원함. 다만, 에러와 nil 처리 방식에 계속 불편함을 느낌. Result[Ok, Err], Optional[T] 같은 구문이 자주 그리워짐
          + 나는 오히려 Go가 원칙에 고집한 게 아니라, 당장 눈앞에 보이는 문제를 빠르게 해결하는 편의성에 집착했다고 봄. 문제를 근본적으로 분석해서 올바르게 해결한 게 아니라, ‘Not Invented Here’ 정신을 버리고 즉석에서 툭툭 만들어 쓴 느낌임. Go의 파일시스템 API가 대표적인 예시임. 파일 여는 함수가 필요하면 단순히 func Open(name string) (*File, error) 이런 식으로 만들고 끝. 그런데 파일 이름이 UTF-8이 아니면? 5년 동안 그 문제가 안 생기니까 신경 안 씀
          + Go의 설계 원칙이 “컴파일러를 쉽게 만들고 컴파일을 빠르게 하자”라는 목표에만 너무 치중된 느낌을 자주 받음. 개발자 편의성보다는 컴파일러/컴파일 자체에만 초점이 맞춰진 구조임
          + 20년 만에 컴파일된 언어로 새로운 직장서 Go를 처음 제대로 써봄. 개인 취향이겠지만 솔직히 쓰면서 불쾌한 감정도 듦. 기본 인자값이 없음, 에러 처리 방식이 마음에 안 듦, 프로덕션에서 제대로 된 스택트레이스가 없음. 객체지향 문법은 각 함수에 어색한 참조를 붙여야 해서 보기 안 좋음. 포인터도 부담임. 결국 C/C++ 옛날 기술로 돌아간 느낌임. 대학 시절 1999년 즈음에 했던 프로그래밍 분위기가 그대로임
          + Go는 병렬 처리 측면에서, 내 경험상 다중 코어 CPU 환경에서 언어 자체로 병렬을 자연스럽게 다루는 유일한 시스템임. CSP 스타일의 goroutine/channel 공식 덕분에 병렬 처리 로직이 직관적으로 표현됨. Python은 GIL과 난해한 async 라이브러리들로 골치가 아픔. C, C++, Java 등은 언어 외 추가 라이브러리가 필요해서 언어 수준에서 병렬 처리 추론이 쉽지 않음. 그래서 HTTP 서버나 서비스용으로 go가 완벽하게 어울린다고 봄. 내 경험상 이만한 대안이 없음
          + 개발자 관점에서 ergonomics, 즉 표준화·일관성 면에서는 완벽했다고 느낌. 여러 마이크로서비스 코드베이스에서도 스타일이 다를 걱정이 없고, 포맷 논쟁도 필요없음. 다만, Go만의 표준 방식을 선택할 때 조금 옛날 스타일을 너무 고집한 것 같음. 요즘 개발자들은 map/filter 같은 함수형 메서드를 더 기대하는데, Go는 인덱스 실수 위험이 있는 반복문만 제공함. TypeScript 수준으로 똑똑한 타입 시스템도 아님. 에러 처리는 불편함. 이러한 기능을 추가하면 “창의적이지만 안 좋은 사용법”이 늘어날 우려는 이해하지만, JS 세대에게는 go를 설득하기가 어렵다는 점도 실감함
     * 5년 넘게 대형 Golang 프로젝트에 전념해왔는데, 메모리 사용을 최소화해야 하는 컴포넌트를 만들다 보면 Go의 허술한 부분과 자주 마주침. GC가 빠르게 청소하지 않거나 힙 조각화 문제가 심해짐(Go가 압축형 가비지 컬렉터가 아니라서 생기는 문제). 이 때문에 할당을 완전히 피해가려고 노력하지만 버그가 발생하기 쉬움. 디버깅도 극도로 까다로움. 힙 프로파일을 떠봐도 살아남은 객체 정보만 보여주고, 실제 쌓인 쓰레기나 조각화 내역은 보이지 않아 추측에 의존해야 함. 예를 들어, X 함수가 힙 1KB만 할당했다고 나오지만 반복문에서 계속 호출되면 수십 MB 쓰레기가 발생함. 그래서 미리 정적 버퍼를 할당해 재활용하는데, 소유권 문제가 복잡해지고 append와 같은 허점이 생김. 표준 라이브러리도 직접 다시 구현해야 하기도 함. 우리 경우가 일반적이지는
       않다는 것도 알지만, 정말 언어와 싸우는 느낌이 들어서 아쉬움
          + 이런 경우에는 오히려 메모리를 heap 밖으로 옮기는 게 덜 고통스러울 수 있음. 물론 GC 언어니까 쉽지는 않지만, 너무 C++/Rust스러운 코드를 Go로 억지로 구현할 바에는 그 부분만 해당 언어로 아예 바꾸는 게 좋음
          + 이런 상황에 go 언어를 선택한 게 언어 선택 자체가 문제였다고 생각함. C/C++/Rust/Zig가 더 적합하다는 의견임
          + 새로운 ""Green Tea"" 가비지 컬렉터가 도움이 될 수도 있다는 소식이 있음. 이는 메모리 중심은 아니더라도 메모리 인접 객체를 더 잘 처리하는 병렬 마크 알고리즘임. 관련 정보는 여기에서 참고 가능함
          + arena 실험이 진행 중이었지만 현재는 중단된 상태임. 그래도 관심 가질만함
          + 이게 도움되는 말은 아니라 미안하지만, 현재 상황을 보면 언어 선택이 완전히 잘못됐다고 생각함. 아마 회사 내 공식 언어 정책 때문에 억지로 go를 쓰는 게 아닌가 추측함. 대기업들이 널리 쓰는 언어로만 프로덕션을 승인하는 경우가 흔함
     * Go의 defer가 함수 스코프(범위)에만 동작하고 어휘 스코프에는 적용되지 않는 이유를 아직도 이해 못하겠음. 이 사실을 알게 된 계기도 반복문 안에서 파일을 처리하다가 파일 리스트가 커지니까 defer가 함수 종료까지 핸들을 닫지 않아 크래시가 발생한 경험 때문임. 주변 Go 개발자들은 반복문 본문을 익명 함수로 감싸서 쓰라 했음. 그밖에는 자잘한 점 몇 가지 빼고는, Go가 쾌적하게 느껴지며, 효율적인 문법에다 쓸데없는 ‘뽐냄’ 문화를 막아주기도 함. C# 프로젝트를 Go로 큰 규모로 리라이트해봤는데, 기능이 10분의 1임에도 코드가 더 적었음. GC 할당 강제가 아니라 성능 좋은 기본값을 사용하도록 유도되고, serialization 같은 작업에 내장 코드 생성 기능이 편리함. ORM처럼 모든 것을 언어로 대체하려는 C# 문법과 달리, Go에서는 SQL은 그냥 SQL로 쓰고, gRPC도
       protobuf 스펙으로 처리하는 분위기임
          + 때로는 어휘 스코프에 defer가 필요하고, 때로는 함수 스코프가 필요함. 예를 들어, 반복문에서 여러 파일을 열어 함수가 끝날 때까지 모두 열어두고 싶으면 함수 스코프가 필요함. 현재는 함수 스코프지만 어휘 스코프가 필요할 때는 func로 감싸면 됨. 만약 어휘 스코프만 지원하고 함수 스코프가 필요하면 어떻게 해야 할지 모호함
          + 래핑용 함수 없이 한 단계 들여쓰기를 줄인다는 점, 동작이 콜 스택이나 스택 언와인딩과 연관된다는 점, C의 ‘goto fail’ 스타일에서 보면 자연스럽다는 점 등이 장점임. 물론, 반복문에서 defer 쓸 땐 별도로 함수로 감싸야 해서 좀 불편함
          + 블록 레벨 언어와 함수 레벨 defer를 둘 다 써본 경험이 있는데, 가끔 조건문 안에서도 함수 레벨 defer를 쓸 수 있었으면 하는 바람이 생김
          + 특별히 깊은 이유가 있을 것 같진 않고, 실제로 중요하냐는 생각임
          + C#에서도 SQL이나 protobuf 스펙으로 작업할 수 있음. 단지 다른 선택지도 있다는 차이임
     * Go에도 단점은 많지만, 서버 사이드 언어 범주에서 이 정도로 균형 잡힌 언어는 없다고 느낌. Node나 Python보다 빠르고, 타입 시스템도 더 낫다고 생각함. Rust보다 진입장벽이 낮고, 표준 라이브러리와 툴링도 훌륭함. 단순한 문법, 하나의 방식만을 강제하는 부분도 마음에 듦. 에러 처리는 문제가 있지만, Node처럼 catch에 아무 에러나 들어오는 것보다는 나음. 혹시 이 기준을 모두 충족하는 더 좋은 언어가 또 있나 궁금함. 스스로 Go 광신자는 아니고, 경력 내내 Node로 백엔드를 많이 했지만 최근 Go를 실험적으로 써보는 중임
          + 사실 이 모든 장점을 Java나 C#에도 동일하게 쓸 수 있을 것 같음
          + 'Node'를 프로그래밍 언어라 부르는 게 거슬리기는 함. Node는 JavaScript 런타임이고, 요즘 Node로 실행되는 프로젝트 상당수가 TypeScript로 작성됨. 즉, Node라고 해도 사용 언어가 명확하지 않다는 것임. TypeScript를 기준으로 본다면 오히려 Go 타입 시스템보다 더 생산적이라고 느낌. Rust와 비교해도 동일한 주장을 할 수 있음
          + 대다수 언어가 나름대로 불편한 점이 있음. Go는 퍼포먼스와 포터빌리티, 런타임/에코시스템도 우수함. 반면, nil 포인터, zero value, 소멸자 없음, 매크로 없음 등의 단점도 있음(Go의 매크로 부재를 회피하려고 코드 생성이 남용됨). 더 좋은 언어도 있지만(예: Rust), 그런 경우는 Go보다 훨씬 복잡해짐. 그 이유는 Go 제작자가 단순함을 최우선으로 해서 발생하는 문제임
          + 최근 Python의 타입 시스템 발전을 감안하면, Go보다 훨씬 앞선다고 봄. structural typing만 놓고 비교하면 Python이 더 인상적임
          + Go 타입 시스템이 많이 부족하다고 생각함
     * Go로 만든 static site generator를 확장한 적이 있었는데, 코드는 매우 명확하고 읽기 쉬웠지만 언어적 허점 때문에 확장성이 낮았음. 단순한 변경도 코드 여러 군데를 어렵게 뜯어고쳐야 했음. 다양한 수준의 캡슐화와 추상이 어렵고, ‘단순함’을 위해 추상화가 희생됨. 추상화가 쉬운 코드(확장 가능한 코드)를 만드는 가장 중요한 방법인데, Go는 확장성 대신 단순성을 택함. 대체로 Go 프로그램은 “확장성 없는 단순함”에 그치는 느낌임. 사람들은 Go는 그런 언어라고 우기지만, 내 경험에는 납득이 가지 않음. 그나마 ‘개발 경험’만은 나쁘지 않음
          + Go에 대한 대화가 항상 묘하게 느껴짐. 비판하면 대부분 “그 언어 원래 그래”라며 그냥 받아들이라는 분위기가 생김. 단순함을 강점이라 하지만, map의 키 리스트를 뽑으려면 반복문을 직접 짜야 하는 게 과연 더 단순한가 의문임
          + Go를 잠깐 써보고서 이런 식의 비판을 쉽게 내릴 수 있냐고 묻고 싶음. 나는 2015년부터 수많은 대형 Go 코드베이스(수백만 줄)를 경험했고, 여러 팀에서 일했음. Go의 확장성은 C, C#, Java와 비교해서 특별히 떨어지지 않음. Go는 표현력보다는 명확함을 택하는 성향이 강함. 그래서 추상화 레이어가 적고, 더 구체적이고 명시적으로 짜는 습관을 들이게 됨. 하지만 그게 확장 불가능으로 이어진다고 보지 않음. 모듈형 확장 가능한 설계는 언어가 아니라 개발자가 학습해서 이루는 영역임. 네가 다룬 코드가 잘못 설계돼 있었을 뿐 Go 언어의 한계는 아님
     * Go를 몇 년간 썼고, 작은 것은 빠르게 만들 수 있지만, 규모가 커질수록 수많은 작은 불편함에 시달리게 됨. 디버깅이 특히 악몽 같은데, 사용하지 않은 X가 있으면(디버깅 중 특정 부분을 주석 처리했을 때 항상 발생) 컴파일이 아예 안 됨. 불필요한 양식화, 특별한 파일명, 예약된 필드명이 많은 것도 번거로움. 표준 라이브러리에 숨어있는 panic, 예상치 못한 heap 복사도 느리고 성가심. Go의 ‘마법’스러운 부분은 대부분 기존 기능(특수 파일명, 대소문자 등)을 억지로 차용해서 생긴 부작용임. 정말로 “public”처럼 노출하려면 pub를 쳐도 됐을텐데 이상하게 고집스러움. 요즘 AI가 좋아져서 Rust에서 타입 문제나 빌림 체크 문제가 생기면 AI에게 바로 물어보고 빠르게 해결할 수 있어서 훨씬 즐거움. 예전처럼 문서나 SO 뒤지며 시간을 낭비하지 않아도 됨
          + 최근 Rust를 본격적으로 해보진 않았지만, 지난 12월에 잠깐 써봤을 때 AI가 Rust에 너무 잘 대응한다고 놀랐음. 자세한 문법과 명시적 타입 정보가 많으니 오히려 사람보다 AI가 더 잘 풀음
          + Go에서 디버깅 중 컴파일 에러가 나는 문제로 불만을 토로하면 Go 진영에서는 “제대로 도구를 쓰라”고 질책함. 원칙을 너무 극단적으로 적용해서 불편함
          + 이 디버깅 관련 불편을 Go의 창시자에게 말해본 적 있는데, 그조차 문제를 이해하지 못하더라. 너무 아마추어 같아서 실망스러웠음. 참고로, AI가 Go는 오히려 잘 못 다룸. 언어가 단순한 편인데도 ChatGPT가 Java, C#, Python을 더 잘 지원함
     * 개인적으로 Go를 좋아하지 않고 결정적인 단점도 많이 보이지만, 인기가 계속 높은 이유는 분명함. Go는 비교적 빠르면서, goroutine 기반 덕에 멀티스레드 없이도 안정적이고 신뢰성 있는 고병렬 서비스를 쉽게 쓸 수 있음. 구글이 Go를 내놓았을 때 비슷한 대중적, 정적, 컴파일 언어가 거의 없었음. 지금도 비슷한 위치에 있는 경쟁자는 Java(이제 virtual thread 지원) 뿐임. async/await 지원 언어들이 비슷한 약속을 하지만 실제론 복잡성이 많음(비동기 태스크에서 블로킹 피하기, 함수 색칠 등). Erlang도 범주가 다름. 결국 단점이 많음에도 goroutine과 Google 프로젝트의 네임밸류로 인기가 높은 편임
          + 점진적으로 JVM이 Go와의 격차를 줄이고 있음. virtual threads, zgc, lilliput, Leyden, Valhalla 같은 프로젝트를 통해 점점 나아짐. Java 8에서 25까지 변화는 엄청남. 앞으로는 더 편리해질 것 같음
          + Go의 명시성과 단순함이 LLM 보조 프로그래밍에 매우 적합함. 옛날 Go 1.x 코드도 그대로 최신 버전에서 잘 동작함
          + 실제로 구글 내부에서는 virtual threads 도입한 Java를 Go보다 훨씬 더 자주 씀
          + 새로운 프로젝트에 가장 어울리는 “현대적 언어”가 무엇이라 생각하는지 궁금함
     * 1.0 출시 전부터 Go를 좋아했지만, “아직도 못 만들었다”라는 평가는 공감이 안 감. 물론 단점이나 불만은 있지만, 창시자가 프로젝트를 떠나면 중앙 비전 유지가 힘들어지고 언어가 나빠질 위험도 있다고 생각함. “서버 언어”로만 포지셔닝된 것도 결국 Rust나 Python 등으로 떠나게 만들 거라 봄. Visual Basic도 까던 시절이 있었지만, 결국 필요한 사람들은 잘 쓰던 시절이 있었음
     * Go의 단점을 다룬 비판 글들이 실제로 꼼꼼히 따져보면 크게 문제로 여길 내용이 아님. 대부분은 기술적으로 맞지만 사소한 부분임. 반대로 정말 심각한 언어 설계 문제는 zero value, 생성자 미지원, null 처리 부실, 기본 mutability, 제네릭을 염두에 두지 않은 타입 시스템, 임의 정밀도를 지원하지 않는 int, 소유권 문제가 애매한 slice 등임(관련 이슈 1, 관련 이슈 2). sum type 없음, 문자열 보간 미지원 등도 단점임
     * Go로 책을 쓸 정도로 편향되어 있을 수 있지만, 10년 넘게 Go를 써온 입장에서 처음엔 정말 신선함을 느꼈음. Java보다 상용구가 적고, 익히기 쉽고, 성능도 무난함. 최고의 언어는 없고, 용도마다 최선의 선택이 있지만, 전형적인 백엔드 작업에는 후회 없는 선택임
          + 집에서 DIY나 목공에 Dremel을 즐겨쓰듯이, Go도 쉽고, 부담 없는 도구임. Dremel은 접근성이 좋아서 큰 톱을 쓸 필요 없이 금방 작업할 수 있음. 하지만 Dremel만 쓰다 보면 작업이 5배 오래 걸리고 결과물이 엉성할 수 있음. 결국 제대로 된 결과가 중요한 경우, 올바른 도구를 골라야 한다는 점을 잊지 않으려 ‘Dremel 효과’라고 부르며 스스로를 경계함
"
"https://news.hada.io/topic?id=22708","RFC 9839와 잘못된 유니코드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           RFC 9839와 잘못된 유니코드

     * RFC 9839는 소프트웨어 개발 시 텍스트 필드에 포함될 수 있는 문제적 유니코드 문자를 명확히 정의함
     * 이 RFC는 서로 다른 언어와 라이브러리에서 해당 문자 처리 일관성 부족에 따른 문제점을 다룸
     * 9839에서는 덜 문제적인 세 가지 하위 집합을 제안하여 선택적으로 활용 가능함
     * 기존의 PRECIS 프레임워크에 비해 적용이 더 쉽고 단순함
     * RFC 9839용 Go 언어 라이브러리도 함께 공개되어 실제 활용에 도움을 줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경 및 RFC 9839 개요

     * 유니코드는 거의 모든 텍스트 데이터 처리에서 표준으로 사용됨
     * 하지만 실제 데이터 구조나 프로토콜 설계 시 모든 유니코드 문자를 허용하면 문제가 발생함
     * Paul Hoffman과 필자는 계속 반복되는 유니코드 문제에 대한 명확한 기준을 제시하기 위해 IETF에 개별 초안을 제출함
     * 2년간의 논의 끝에 공식 표준으로 채택되어 RFC 9839로 발표됨
     * 이 문서는 문제적 문자 유형, 왜 문제가 되는지(기술적, 표준적 이유), 사용자가 골라 쓸 수 있는 하위 집합 세 개를 상세히 설명함

RFC 9839의 주요 내용

     * 소프트웨어와 네트워크 환경에서 텍스트 필드 설계 시 필수적으로 참고해야 할 문서임
     * RFC 9839는 10페이지 분량으로, IETF 표준 문서 중 간결한 편임
     * 주로 소프트웨어 개발자와 네트워크 엔지니어를 대상으로 쉽게 설명되어 있음

문제적 유니코드 문자 사례

     * 예시로 JSON의 username 필드에 다음과 같은 문자열이 올 수 있음
{
    ""username"": ""\u0000\u0089\uDEAD\uD9BF\uDFFF""
}

     * 각 코드 포인트가 지닌 문제점
          + U+0000 : 의미 없는 NULL 문자로 일부 프로그래밍 언어의 동작을 방해함
          + U+0089 : C1 제어 코드(CHARACTER TABULATION WITH JUSTIFICATION) 로, 그 동작이 복잡하고 일관성 없음
          + U+DEAD : 쌍을 이루지 않은 서러게이트 문자로, UTF-16의 한계에서 비롯되는 문제임. 이상적이지 않은 데이터가 발생함
          + \uD9BF\uDFFF (실제 U+7FFFF) : Noncharacter로, 표준상 교환이 금지됨
     * 위와 같은 코드 포인트는 데이터 구조와 프로토콜 내에서 일관적 처리 불가 및 예기치 않은 오류를 유발함
     * RFC 9839는 이런 문제 문자를 공식적으로 정의하고, 배제할 유형을 명확히 제시함

JSON의 설계와 한계

     * JSON 창시자인 Doug Crockford의 책임은 아님
     * 유니코드가 충분히 성숙하지 않은 시기에 설계되어, 문자 집합을 엄격하게 제한하지 못했음
     * 이제 표준을 변경할 수 없으므로, 문제 문자를 경험적으로 배제하는 방식이 필요함

IETF의 PRECIS 프레임워크와의 차이

     * 2025년의 RFC 9839 이전에도, IETF에서는 RFC 8264(PRECIS Framework) 등 다양한 표준을 제공함
          + 이 프레임워크는 국제화 문자열의 정제, 적용, 비교 방법을 상세히 다룸
          + 43페이지 분량으로 배경 설명과 해결책이 포괄적임
     * PRECIS는 유니코드 버전에 강하게 의존하며, 복잡하고 적용이 어려운 단점이 있음
     * RFC 9839는 간결하고 실용성에 중점을 두었으며, 새로운 프로토콜 정의 시 신속한 채택이 용이함

RFC 9839의 하위 집합 및 활용 예시

     * 9839는 세 가지 현실적인 하위 집합(scalars, XML, assignables)을 제시함
     * 각 하위 집합은 배제할 문제적 문자 범위를 조금씩 달리함
     * 다음은 주요 데이터 포맷과 RFC 9839의 하위 집합이 문제적 문자를 어떻게 처리하는지에 대한 표 요약임
          + CBOR, TOML, XML, YAML 등 일부 포맷은 서러게이트 또는 컨트롤 문자를 부분적으로 배제함
          + I-JSON은 서러게이트와 noncharacter를 배제함
          + 일반 JSON, Protobufs는 배제하지 않음
          + XML, YAML은 Charset 특성상 부분적으로만 noncharacter/컨트롤 코드를 제외함
               o 참고: XML과 YAML은 Basic Multilingual Pane 이외의 noncharacter는 제외하지 않음

Go 언어용 RFC 9839 라이브러리

     * RFC 9839의 세 하위 집합에 대한 문자 검증을 지원하는 작은 Go 라이브러리가 공개됨
     * 충분히 테스트가 이루어졌으며 최적화는 아직 진행 중임
     * 실제 현업에서 테스트 및 피드백을 환영함

RFC 9839의 의의와 작업 과정

     * RFC 9839는 공동필자들과 여러 차례 피드백을 거쳐 15개 이상의 초안 수정 후 공식 발표됨
     * 많은 커뮤니티 전문가들의 논의와 기여를 통해 초기안보다 훨씬 완성도 있는 문서로 발전함
     * “Acknowledgements” 섹션에 기여자 명시함

RFC 개별 제출의 경험

     * RFC 9839는 개별 제출(individual submission) 로 진행되었음
     * Working Group을 통한 전통적 방식보다 노력과 절차의 부담이 큼
     * Working Group 참여 경험과 비교 시, 전통적 방식이 더 효율적이고 추천할 만함

        Hacker News 의견

     * 어떤 문자가 문제를 일으키는 건 분명하다고 생각하지만, 데이터 구조나 프로토콜 설계자가 모든 종류의 문자(심지어 적절히 이스케이프된 것조차)를 임의로 허용하지 않으려는 경향을 가지는 게 최악의 시나리오라는 느낌임. 예를 들어, 사용자명 유효성 검사는 다른 레이어에서 처리해야 한다고 봄. 사용자명을 60자 미만, 이모지나 zalgo 문자 금지, 널 바이트 금지 등으로 체크하고 API에서 적절한 에러를 반환하는 작업임. JSON 파싱 단계에서 사전 유효성 검증 대신 이런 문제로 실패하길 원치 않음. 물론 사용자명에는 특정 문자 클래스가 분명히 부적절함. 하지만 탭 문자 등이 실제로 사용되는 텍스트 파일을 전송한다면 내 언어의 utf8 ""string"" 타입에서 처리 가능한 것은 인코딩 가능하길 기대함. 특히 널 바이트의 용례가 많으며, 실제로 JSON에서 종종 보임.
       하지만 제한된 ""정상"" 유니코드 집합만 써야 한다면 표준이 존재하는 것이 각자 미니 표준 만드는 것보다는 낫다고 생각함. 결론적으로 아이디어 자체는 좋아 보이나, 블로그 글에서 든 논리는 잘 납득이 안됨
          + 2025년 기준, 로우 레벨 와이어 프로토콜에서 쓸 문자열 표현 방식은 아래 중 하나만 실질적으로 옹호 가능하다는 생각임
               o ""Unicode Scalars""(잘 구성된 UTF-16, 파이썬 문자열 타입)
               o ""잠재적으로 잘못된 UTF-16""(WTF-8, 자바스크립트 문자열 타입)
               o ""잠재적으로 잘못된 UTF-8""(바이트 배열, Go 문자열 타입)
               o 위 방식 중 하나에 ""U+0000 없음"" 옵션 추가(버퍼 오버플로우 취약점 이전에 설계된 언어/라이브러리 연동 시)
          + 진지하게 말하면, 프레인 텍스트 파일에서 C0(줄바꿈과, 마지못해 HT 제외) 및 C1 문자를 사용하지 않았으면 함. ANSI 컬러 마크업 같은 걸 저장하고 싶어하는 건 이해하지만, 이런 경우 실제로는 플레인 텍스트가 아니라 일종의 텍스트 마크업 포맷임. 마크다운과 유사하지만, C0 범위의 인코딩을 쓴다는 점만 다름. 데이터가 'cat' 명령어 등으로 예쁘게 보인다는 이유만으로 플레인 텍스트라고 말할 수는 없음. 프레인 텍스트로 인코딩된 마크업 포맷이 상호 운영성 때문에 많다는 점은 인지함
          + 데이터 구조와 프로토콜에서 임의 문자군을 금지하기 시작하는 게 최악이라는 의견 자체가 현실과 동떨어진 사고방식이라고 봄. 진짜 최악은 파서 등 소프트웨어 결함으로 보안 침해가 발생하는 것임
          + 사용자명에 UTF-8을 허용하는 시스템이 있는지 의문임. 프로그래밍적으로 조작되거나 평가되는 모든 식별자(로그인 사용자명, 비밀번호 등)는 반드시 ASCII여야 한다는 게 당연함. ISO-8859-1도 아니고, 오직 ASCII만 사용해야 함. 유니코드로는 이런 용도에 맞지 않음. 사용자명을 띄우는 경우 등에서는 상관없겠지만, 시스템 로그인의 식별자로서는 비-ASCII 인코딩은 무조건 금지임. 키보드 소프트웨어조차 ASCII를 벗어나면 시각적 표현에 대해 UTF-8의 일관성을 자장 담보할 수 없으며, 운영체제와 설정에 따라 더 혼란스러움. 앞으로 남겨질 바이너리와 유니코드 해석 AI가 일치한다는 보장도 없음. 또한 일관성 관련해서, IVS 상황이나 정규화(NFC/NFD/NFKC/NFKD) 문제를 명확하게 범위 내/외로 다루는지 RFC 9839도 기사도 명확하지 않음. 목적 섹션이 아예 빠져 있는 것
            같음. 대략적으로 ""비문자 코드 포인트""가 있다는 식의 모호한 언급만 있음
          + 사용자명에서 왜 이모지를 금지해야 하는지 궁금함
     * IETF가 2025년까지 Bad Unicode 지원을 기다린 게 아니란 점을 말하고 싶음. 이미 예전부터 RFC 8264: PRECIS Framework에서 다양한 Bad Unicode 문제를 광범위하게 다룸. RFC 8265(링크), 8266(링크) 등 관련 RFC도 참고하면 도움됨. 일반적으로, 텍스트 방향을 바꾸거나 입력 기기마다 다르게 인코딩될 수 있는 비밀번호 등은 사용자명/패스워드에 쓰이면 안됨. 이러한 RFC 프로필을 통해 안전하게 대응 가능함. 이런 목적에서는 ""failing closed""(더 엄격하게 차단)가 더 안전함. 새로운 이모지가 나온다고 해도, 사용자명에서 허용해서 모든 페이지에 영향을 주는 것보다는 차라리 금지하고 보수적으로 가는 걸 선호함
          + 그래도 너무 닫힌 상태로 두면 20년 뒤에도 20년 전 이모지가 아직도 미지원 상태로 남게 되어 결국 사용자 불만만 커짐
     * Unicode는 ""좋은"" 부분이 분명 있지만, 예외적으로 제외해야 할 문자가 있음을 알아야 한다는 점이 실망 스러움. 언어 기록 방식을 포괄적으로 받아들이려다 너무 복잡해진 결과임. 어느 문자가 특별 취급되어야 한다는 점을 항상 생각해야 해서 피곤함. 그래서 Unicode 문자열은 독자적인 데이터 단위라고 생각해서 다룸. 입력 받고, 저장하고, 렌더링, 데이터 동등성 비교는 해도, 내용에 대해 해석하려 들지는 않음. 심지어 문자열을 이어붙이거나 다루는 것도 불안함
          + Unicode는 끝없는 트리비아와 나쁜 결정의 심연 같음. 예를 들어, 관련 RFC에서 구식 ASCII 제어 문자(표시 혼동 우려) 경고는 있지만, Explicit Directional Overrides 같은 치명적인 보안 문제가 있는 방향 전환 문자는 아무런 언급이 없음
          + 간단한 예시로, 첫 번째 문자열이 고아 이모지 수정자로 끝나고 두 번째가 수정 가능한 이모지로 시작하면 이미 문제 발생임. 더 복잡한 케이스가 늘어날수록 문제만 커짐
          + 복잡성이 크긴 하지만, 이런 것 중 서러게이트와 제어 코드는 언어 기록 목적이 아니라 이상한 설계가 과거를 위해 남겨진 결과임
          + Unicode가 불편하지만, 기존 다른 인코딩 표준보다는 덜 불편하다고 생각함
     * 대부분의 문제는 유효하지 않은 UTF-8 바이트 시퀀스를 거부하거나 전체적으로 에러 반환하도록 처리하면 된다고 생각함. 예를 들어, 서러게이트 등은 원래 UTF-8에서 불법이기에, utf-8을 쓰는 언어라면 이런 시퀀스에 대해 에러를 반환해야 함. 실제로 문제가 되는 건 ""코드 포인트""로 문제적인(non-printing, 등) 것들이라고 봄. 이건 불법 바이트 시퀀스와는 분명히 별도의 개념으로 다뤄야 더 유용함
          + 충분히 합리적이라고 생각함. 이런 선택은 애플리케이션 구현자 몫이지, 범용 라이브러리에서 정할 사안이 아니라고 봄. 사용자명만 다루는 JSON 파서 같은 건 본 적이 없음
     * Unicode는 이미 각 코드 포인트의 범주(General Category)를 정의해서 이상한 문자 종류를 분류함. 관련 위키백과 문서를 참고할 수 있음. 예로, 파이썬에서 unicodedata.category(chr(0))은 ""Cc""(control), unicodedata.category(chr(0xdead))은 ""Cs""(surrogate)를 리턴함
     * 모든 ""legacy control"" 문자를 리터럴 뿐만 아니라 이스케이프 문자열(e.g., ""\u0027"")까지 제외하는 건 과하다고 생각함. C1은 잘 안 쓰이니 괜찮지만, C0 문자 일부는 실사용 예제가 있음. escape, EOF, NUL 등은 명확한 쓰임새가 남아 있다고 봄
          + 좀 특이한 C0 문자(U+001E Record Separator 등)는 데이터 스트림에서 쓸모가 많다고 생각함. 문서엔 막더라도, 스트림 데이터엔 유용함
          + 프로그램 소스 코드에서 form feed(U+000C) 문자 쓰인 걸 봄. Emacs는 페이지 단위 내비게이션용으로 원래 지원하니, 이런 게 포함되곤 함
     * Unicode가 좋다고 생각하지 않음. 문자 집합이 무엇이든 실제로 쓸 문자종류(제어 문자, 그래픽 문자, 최대 길이 등)는 결국 각 어플리케이션에 맞게 결정해야 함. JSON 등에서 포함/제외를 시도해도 별 효과가 없음. 유니코드든 ASCII든 다른 문자셋이든 특정 서브셋(혹은 superset)에 이름 붙이는 게 때론 유용할 수는 있지만, 모두에게 좋은 선택이라 착각하면 안 됨. RFC 9839는 몇몇 유니코드 서브셋에 이름 붙이긴 하지만, 내가 만들 서비스에 무조건 옳다는 보장은 없음. 내 결론은 유니코드를 아예 쓰지 않거나 강제하지 않는 것도 고려해봐야 함
          + 실제 문제는 조합 문자(combining character)임. 이 때문에 유니코드는 문자 집합에서 문자 묘사용 DSL로 바뀌었음
     * 입력을 제어할지, 아니면 신뢰할 수 없는 입력에 대해 안전하게 출력하는 데이터타입(web+log+debug용)으로 감쌀지 고민임
     * 한 그래픽 유닛에 들어갈 수 있는 유니코드 스칼라 값 개수에 대한 제한이 표준에 있었으면 함. 마지막으로 봤을 때(몇 년 전이지만) 표준에는 그런 제한이 없었고, 대신 스트리밍 애플리케이션에서는 그래픽 유닛을 128바이트로 제한하라는 권고만 있었음. 이런 한계를 표준에 명확히 두면 구현이 훨씬 쉬워지고 불필요한 제약도 없으리라 생각함
     * 실제로 ""제어 문자가 없디""라는 전제만으로 프로그램을 짜서 깨지는 케이스 겪었음(폼 피드는 페이지 구분 등, escape 문자는 터미널 용도 등으로 흔히 쓰임). ""전부 UTF-8임""이라는 가정도 깨지곤 함(오래된 데이터 파일, 로그 등). 텍스트로 유의미한 처리를 안 한다면, 그냥 바이트 시퀀스로 내용 변경 없이 넘기는 게 제일 좋음. 하지만 Microsoft Windows로 인해 가끔 char16_t 시퀀스를 넘겨야만 할 때도 있음. UTF-16은 UTF-8과 입출력이 근본적으로 다름. 변환할 때는 외부 데이터 → 내부 형태 전환시 WTF-8(UTF-16), 서러게이트 이스케이프(UTF-8) 방식을 각각 써야 함. 두 방식 혼합은 불가함
"
"https://news.hada.io/topic?id=22628","GPU를 이해하는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              GPU를 이해하는 방법

     * GPU는 현대 머신러닝에서 핵심적인 역할을 하며, 고속 행렬곱 연산에 특화된 수많은 Streaming Multiprocessors(SMs) 와 HBM(고대역폭 메모리) 가 결합된 구조임
     * GPU의 SM은 Tensor Core(행렬곱)와 CUDA Core(벡터 연산)로 나뉘며, 대규모 병렬 연산과 유연한 프로그래밍을 지원함
     * GPU와 TPU는 내부 구조와 네트워크 구성 면에서 차이가 있으며, GPU는 범용성과 확장성이 높지만, 최적 성능 달성에는 더 많은 고려가 필요함
     * 노드(Node) 내에서는 NVLink 및 NVSwitch로 GPUs 간 초고속 통신이 가능하고, 노드 간에는 InfiniBand 등의 네트워크로 연결되어 대규모 분산 학습에 대응함
     * GPU에서의 집합 연산(Collectives) (예: AllReduce, AllGather 등)은 하드웨어 구조와 네트워크 계층에 따라 성능이 크게 달라지며, 이론상 밴드위스보다 실질적으로 낮은 경향이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GPU란 무엇인가?

     * 최신 ML(머신러닝) GPU(예: H100, B200)는 행렬곱 연산에 특화된 Streaming Multiprocessor(SM) 수십~수백 개와 빠른 HBM 메모리가 결합된 구성임
     * 각 SM에는 Tensor Core(행렬곱), Warp Scheduler(벡터 연산), SMEM(온칩 캐시)이 있음
     * TPU와 달리 GPU는 100개 이상의 SM을 통해 더 유연하고 대규모 병렬 처리가 가능함

  SM 세부 구조

     * SM은 4개의 서브파티션으로 나눠지고, 각 서브파티션에는 각기 Tensor Core, CUDA Core(벡터 연산), Warp Scheduler, 레지스터 파일 등이 존재함
     * CUDA Core는 벡터 산술 연산(SIMD/SIMT) 담당, Tensor Core는 행렬곱에 특화
     * Tensor Core FLOPs가 압도적으로 크며, 낮은 정밀도의 연산에서는 처리 속도가 더욱 빨라짐
     * 최신 GPU(예: B200)는 대형 TMEM이 추가되어 대용량 Tensor Core 입력 지원

  CUDA Core의 유연성

     * GPU의 CUDA Core는 SIMT(Single Instruction Multiple Threads) 모델을 사용하여, 한 명령어를 여러 쓰레드에 병렬 실행함
     * 각 쓰레드는 독립된 명령 포인터(프로그램 카운터)를 가져 조건 분기 등 유연성 제공하지만, 워프 내 명령 분기(divergence)가 많으면 성능 저하 발생
     * 각 CUDA Core는 개별 상태와 메모리 접근이 자유로움 (TPU는 연속된 메모리만 처리 가능)

  스케줄링/병렬성

     * SM은 다수 워프(최대 64개)를 스케줄링해 동시 실행하며, 각 워프 스케줄러는 한 번에 하나의 프로그램 실행
     * 이 구조 덕분에 GPU는 상당히 유연하면서도 높은 동시 처리가 가능함

GPU 메모리 구조

     * GPU는 HBM이 가장 크며, 그 외에도 L2/L1(SMEM)/TMEM/레지스터 등 메모리 계층 구조를 가짐

최신 GPU 사양 요약

     * SM(Streaming Multiprocessor) 개수, 클럭, 메모리, FLOPs, 대역폭(BW) 등이 모델마다 상이함
     * 메모리 용량(HBM) 및 대역폭, FLOPs(흑체/정수/저정밀) 등은 세대가 거듭될수록 증가함
     * 표(생략)에서 주요 특징: Blackwell(B200)은 HBM 192GB, HBM BW 8.0TB/s, FP8 FLOPs 4.5e15 등
     * 각 세대별로 레지스터 및 온칩 캐시(SMEM) 용량, TMEM 추가 등 하드웨어 발전이 뚜렷함

GPU/TPU 비교

     * GPU는 범용적이면서도 많은 소형 SM(병렬 유닛)으로 모듈화되어 있으며, 하드웨어 제어가 많아 이해/최적화가 어려움
     * TPU는 소수 대형 Tensor Core 및 많은 벡터 ALU(VPU)로 구성, 단일 스레드 제어 방식으로 하드웨어 단순/비용 절감 가능
     * 이로 인해 TPU는 컴파일러 최적화가 필수이며, GPU는 여러 커널을 독립 실행 가능해 사용 용이성 높음
     * 성능/가격 측면에서 최근 H200 GPU가 TPU v5p보다 FLOPs/s 2배, HBM 1.5배, 가격은 2.5배 정도임
     * TPU는 VMEM(온칩 캐시)이 많고 빠르며, LLM 모델 추론 등에서 큰 이점 발생 가능

GPU 하드웨어 퀴즈 Q&A 요점

     * H100의 fp32 CUDA 코어는 총 16,896개(132 SM x 4 x 32), B200은 18,944개
     * 벡터 연산 FLOPs는 H100 기준 최대 33.5TFLOPs/s 수준, Tensor Core의 행렬곱 FLOPs(990TFLOPs/s)에 비해 30배 낮음
     * H100의 L1/SMEM, 레지스터 합산 용량은 66MB, TPU VMEM은 120MB
     * Bandwidth(대역폭)와 FLOPs의 비율(이론상 연산 집약도)은 H100/B200 모두 280-300 정도로 TPU와 유사

GPU 네트워킹(통신 구조)

  노드/클러스터 구조

     * GPU 노드는 대개 8개 GPU로 묶여 NVLink(초고속) 및 NVSwitch(스위치)로 Full Bandwidth 직접 연결됨
     * 노드 간에는 InfiniBand(Ethernet 등)를 사용해 스케일 아웃 가능
     * 최신(Blackwell) GPU는 Node 72개까지 확장 가능한 구조

  네트워크 계층별 특징

     * 노드 내부(NVLink 영역): 각 GPU당 egress 450GB/s(H100), 900GB/s(B200), NVSwitch 당 최대 1.6TB/s
     * 노드 상위(InfiniBand Leaf/Spine): Leaf Switch(8개)~Spine Switch(16개) 구조, GPU~GPU간 이론상 400GB/s Full Bandwidth 유지
     * SuperPod와 같은 대형 아키텍처에서는 1024개 GPU(128노드), GB200(72GPU Node)는 9배 증폭된 대역폭(3600GB/s)

  네트워크 성능 요점

     * 이론상 네트워크 구조(Full Fat Tree)는 노드~노드 간에도 최대 대역폭을 제공하도록 설계
     * 하드웨어 포트 제약 등으로 1024~4096GPU로 확장 시 Spine/Core Switch를 더 추가하는 계층화 방식 사용
     * Node 내 밴드위스(450GB/s) → Node 간 밴드위스(400GB/s) 로 전환 = 집합 연산에서 성능 차이

집합 연산(Collectives) 구조

     * AllGather, AllReduce(합산), AllToAll(분산) 등 고수준 집합 연산 지원
     * Node 내에서는 NVLink로 직접 연결되어 최적 성능 가능(이론상 B/W), Node~Node간은 InfiniBand 경유
     * NVIDIA의 NCCL, NVSHMEM 라이브러리 활용

  집합 연산 성능 분석

     * AllGather/ReduceScatter: B/W(450GB/s H100 기준)로 링(Ring) 방식 구현, 작은 메시지일 때는 트리(Tree) 방식도 가능
     * AllToAll: 각 GPU가 직접 상대 GPU에 전송, B/W에서 N으로 나누는 방식이어서 Node 내에서 이론상 2배 빠름
     * 실제 측정 결과 AllReduce에서 370GB/s 수준으로, 하드웨어 최대치에는 미치지 못함
     * TPU 대비 대용량(수십 MB ~ GB)에서야 하드웨어 Peak Bandwidth에 근접

종합 요약 및 인사이트

     * GPU는 범용성, 확장성이 강점이지만, 하드웨어/네트워크 구조에 따라 성능 최적화 난이도와 관찰 가능성이 TPU보다 높음
     * 네트워킹(Intra-Node/NVLink/InfiniBand/Leaf/Spine 등)이 대규모 학습 성능의 핵심이고, 실질 밴드위스와 이론 밴드위스 차이 주의 필요
     * 집합 연산 및 네트워크 구조에 대한 이해가 초대형 분산 모델 학습/서빙에서 필수 요소임
     * 실질적인 벤치마크와 하드웨어 세부 구조 이해를 바탕으로 성능 병목 구간/최적 조건을 파악하는 프로세스가 요구됨

        Hacker News 의견

     * 나는 이 글과 여러 문서들이 다소 불분명하다고 느꼈음, 특히 GPU에 대해 설명하면서 용어가 애매하게 사용되는 경우가 많음, 예를 들어 첫 번째 이미지에 ‘Warp Scheduler’가 TPU의 VPU와 비슷한 SIMD 벡터 유닛이라고 설명하면서 32개의 ‘CUDA Core’가 있다고 하는데, 여기서 ‘CUDA core’가 무엇인지 명확하게 설명하지 않아 설명의 핵심 사물을 제대로 전달하지 못함, 이런 복합적인 오류로 인해 초심자나 개념을 익히려는 독자들은 여전히 이해가 어렵고 반면 어느 정도 개념을 이해하고 있는 사람들에게는 이미 아는 내용임, 초안이던 내용이라도 공개할 땐 용어 하나하나를 외과 수술처럼 정확하게 다뤄야 하고, 용어를 혼동하거나 섞어서 쓰지 않아야 함, 아날로지를 쓸 땐 신중해야 한다는 조언임, 그리고 MXU(매트릭스 연산 유닛) 같은 용어는 부연설명이나
       하이퍼링크를 통해 쉽게 찾을 수 있도록 해야 한다고 생각함, 요즘은 AI에게 이 역할을 시킬 수도 있는데, 이는 다소 슬픈 일임
          + 저자로서 직접 답변함, 지적에 대체로 동의함, 설명에서 정확성을 확보하려 할수록 ‘도덕적 진실’과의 균형에 늘 고민이 있음, 예를 들어 “NVIDIA가 CUDA Core라 부르는 SIMD(단일 명령-다중 데이터) 벡터 유닛 32개(ALU)로 구성된 TPU VPU와 유사한 유닛""이라고 쓸 수도 있지만, 이러면 문장이 길어지고 벡터 유닛 등 용어 정의는 또 빠질 수 있음, 각주를 많이 쓰려 노력하지만, 독자가 일일이 클릭할 거라 기대하기도 어렵고, 사이드노트는 HTML에서 구현 난이도가 있음, MXU 같은 용어는 전장(前章)에서 이미 정의했지만, 독자가 모든 장을 꼭 읽으리라 가정하는 건 무리라 생각함, ""Warp Scheduler""도 실제로는 여러 역할(스케줄러, 디스패치 유닛, SIMD ALU)을 모두 뜻하는 등 용어 자체의 모호함이 있지만, NVIDIA가 복합 유닛에 별도 명칭을 붙이지 않아 생기는 일임, 앞으로
            더 개선하려힘, 쉽지 않은 타협의 연속임
          + LLM은 이런 용어 연결의 난관을 풀기 꽤 좋은 도구임, 한 용어를 찾아도 연쇄적으로 모르는 용어가 나올 때 어디서부터 공부하면 되는지 잘 안내해 줌
          + 구글 TPU 시스템 아키텍처 문서에 거의 다 나와있음
          + 진지하게 묻고 싶은데, 컴퓨터 아키텍처에 대한 적정 수준의 배경지식은 어느 정도가 적합한지 궁금함, SIMD 개념 자체가 이미 50년이 넘었음, 해당 자료 서두에선 LLM과 Transformer 아키텍처에 대한 이해는 기본으로 보고 있지만, 컴퓨터 작동 원리에 대한 이해는 없어도 된다고 하면서도, CPU 기본 동작 원리는 알아야 하는 게 아닌가란 생각임
          + 해당 콘텐츠는 머신러닝 분야에 종사하는 사람을 대상으로 쓰인 책의 한 챕터임
     * 나는 오픈 소스가 아니거나 여러 공급업체가 교차 사용 가능한 기술이 아니면 투자한 시간을 정당화하기 매우 어렵다고 느낌, Nvidia 칩 활용을 잘 하는 일이 예전의 SAP ABAP 컨설턴트 같은 것이라 여겨짐, 물론 이 분야에서 돈이 많이 벌리긴 하지만, 역사적으로 이런 전문성은 그리 득이 아니었다고 생각함
          + 나도 10년 전 대학 때 CUDA 수업을 일부러 건너뛰며 같은 생각을 했었음
          + CUDA는 하드웨어 아키텍처와 이를 위한 소프트웨어 스택 두 가지가 있음, 소프트웨어 스택은 폐쇄적이어서 직접 저수준 최적화를 할 계획이 아니면 굳이 신경 쓸 필요 없음, 하지만 하드웨어 구조는 배울 가치가 충분함, 모든 GPU들은 기본적으로 비슷한 방식으로 동작하고(CUDA 구조는 2007년부터 기본 철학은 그대로임), 이러한 아키텍처가 셰이더 언어나 GPU 추상화 방식에 큰 영향을 미침, 쓰레드 스케줄링, 워프, 사설/공유 메모리 구조 등 자세한 특성을 이해하면, 알고리즘을 하드웨어 실행 모델에 더 잘 맞춰 막대한 연산 성능을 활용할 수 있음
          + 병렬 컴퓨팅의 원리와 하드웨어, 드라이버 레벨에서의 작동 방식은 상당 부분에 일반성이 있음을 강조하고 싶음, 일부는 특정 플랫폼 특화 내용이지만 상당수는 폭넓게 응용 가능함, 일반성만 맹신하다 보면 오히려 손해 보기도 함, 오픈 소스 여부와 범용/특화성은 별개로도 볼 수 있으니, 더 넓은 탐구가 필요함
          + 전환이 그리 어렵지 않음, 이미 OpenMP나 MPI 코드를 짰던 사람이라면 CUDA 입문 자체는 쉬웠음, CUDA에서 성능 최적화 방법을 배우면 CPU 병렬 코드도 빨라지므로 본질적으로 기존 컴퓨팅 모델의 진화형임
          + 나도 어릴 때 IBM PC/MS-DOS로 프로그래밍 배웠는데 둘 다 오픈 소스가 아니었지만, 지금도 크게 도움이 되고 있음
     * “Quiz 2: GPU nodes” 부분의 계산이 부정확하다고 생각함, 실제로는 GPU나 스위치마다 포트가 제한돼 있어서 이론상 가능한 450GB/s가 확실히 제공되지 않으며, 그래서 주요 클라우드나 레퍼런스 시스템에서 3.2TB/s이 제공되는 것임, 만약 3.6TB/s가 가능하다면 분산 링 워크로드에서 병목이 발생함, 한편 구직 중임
          + 나도 이 부분 오랜만에 생각해 봤는데, 클라우드 업체들이 3.2Tbps만을 광고하는 이유는 노드의 IB(InfiniBand) 네트워크 연결 한계 때문임, DGX 기준으로 H100 한 개당 Connect-X 7 NIC 1개씩이 400Gbps까지 제공함, 8개의 GPU x 400Gbps = 3.2Tbps임, Quiz 2는 표현이 헷갈리게 되어 있는데 내 생각엔 노드 내 GPU 간 연결을 의미하는 것 같고, 노드 간 네트워크는 아님
     * 이 시리즈 전체가 정말 좋았음, 현대 AI 워크로드의 이론적 한계를 설명하면서 아키텍처와 병렬화 등 기술적 원리를 쉽게 풀어줌, TPU 중심이긴 하지만 대체로 다른 분야에도 적용할 수 있어 활용성이 높음
     * 타입이 확실한 언어로 수치 집약적인 코드를 최대한 최적화하고, 그럼에도 빨라야 한다면 GPU 옵션을 검토하는 게 순서임, 내 경험상 GPU로 대략 8배는 빨라짐, 웹 응답 4초를 0.5초로 줄일 수 있으면 어마어마한 변화임, 하지만 사실상 웹소켓으로 지연 표시(스피너)하거나 백그라운드 캐시를 쓰는 편이 더 쉽기도 함, 그리고 클라우드에서 GPU 돌리는 건 비용이 많이 드는 점도 고려해야 함
     * nvshmem이 ML 분야에서 이렇게 각광받는 것이 흥미로움, 반면 동일 기능의 MPI는 예전 과학 시뮬레이션 세계에선 그리 만족스럽지 않았음, 참고로 나는 여러 노드에 걸친 장거리 힘 계산처럼 난도가 높은 작업을 주로 했었음
     * 왜 Nvidia는 TPU를 자체 개발하지 않았는지 궁금함
          + 그럴 필요가 없음, 이미 하드웨어와 프로그래밍 모델 시장 지배적 위치에 있고, TPU는 오히려 프로그래밍 난이도가 더 높음
          + 사실상 이 기사에서 설명하듯, Nvidia GPU 성능의 90%가 행렬 연산 유닛에서 나오기 때문에 거의 TPU와 유사한 구조임, 약간의 성능은 포기하지만 훨씬 더 유연한 컴파일러 생태계를 확보함
     * 아직까지 Nvidia에서 이런 수준의 리소스를 공식 제공하지 않은 게 놀라움, 결국 3rd party들이 하드웨어를 역공학해서 실제로 쓸 만한 개념도로까지 정리하는 상황이 됨, Nvidia의 진짜 동기가 궁금함, 만약 단순 마케팅만 생각한다면 성공한 셈이지만, 나는 엔지니어링 컬처엔 의문이 남음
          + 실시간 렌더링 엔지니어 입장에서 항상 그랬음, NV는 세대간 변화 포착을 경쟁사들이 못하게 하려고 대부분 정보를 꽁꽁 숨김, 다른 벤더들도 다르지 않음, 게임에서는 NDA 맺고 세부 아키텍처 정보를 더 자세히 제공하긴 하는데 그 외엔 Intel 빼고 완전한 공개 사례는 못 봤음
          + Nvidia는 실제로 타사 대비 정말 우수한 공식 문서가 많음
          + 왜 그렇게 느꼈는지 궁금함, 사실 이 기사에 있는 내용 대부분은 Nvidia 공식 문서를 거의 그대로 가져온 것으로 보임, 예를 들어 H100 다이어그램도 실제로는 H100 whitepaper에서 출처 표시 없이 복사된 것임, 연산량, 대역폭 정보도 전부 공식 whitepaper와 CUDA C++ 가이드 5,6,7장이 이미 다 다루고 있음, 외부에서 더 간략하게 정리하고 해석 덧붙이는 건 가치 있지만, Nvidia 공식 자료 없었으면 이런 기사 자체가 힘들었을 것이니 근거 없는 추측·의심은 좀 과함
          + Nvidia가 평범한 수준의 문서만 공개함으로써 cuBLAS, cuDNN 같은 폐쇄형 라이브러리만 빠르고, 이로 인해 벤더 종속이 심해지는 구조임, 덕분에 다른 업체가 역공학으로 쫓아오기도 어렵게 만듦
          + 여러 정황을 볼 때 Nvidia는 NDA 서명자와 VIP에게만 맞춤형 자료를 제공하고 공공용 공식 매뉴얼 공개는 일부러 소홀히 하는 경향이 있음, 그게 상업적으로 자신들에게 유리하니까 그렇게 하는 것 같음, API 공식 문서조차 장벽을 치면 평범한 개발자들은 접근 어려워도 AI, GPU 자체, 소프트웨어, VIP용 문서 등 전체 생태계 판매에 집중하고 있어서 아예 일반 개발자에겐 신경을 덜 쓰는 셈임
     * 우리가 구조 다이어그램을 볼 때 실제 하드웨어를 완벽히 반영한 것이 아니란 점을 반드시 유념해야 함, Nvidia는 다이어그램의 블록이나 구성 요소가 실제로 존재함을 보장하지 않고, 어디까지나 GPU와 SM 구조를 생각할 때 참고용 멘탈 모델로만 제공함, 예를 들어 실제 SM에 기능 유닛이 몇 개나 있는지, ‘tensor core’ 자체가 독립된 하드웨어인지 아니면 여러 유닛의 조합 결과인지, warp 이하 단위의 세부 동작도 공식적으로는 알 수 없음
          + 흥미로운 의견임, 그런데 실제로는 SM이 tensor core 연산 중에는 블로킹되어 있다는 점이 동일한 FPU가 tensor 연산까지 모두 처리한다고 해석할 수 있지 않을까란 생각임
     * 정말 환상적인 리소스임, 덕분에 좋은 내용 얻게 됨
"
"https://news.hada.io/topic?id=22718","ghrc.io가 악성 사이트로 의심됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          ghrc.io가 악성 사이트로 의심됨

     * 단순한 오타인 ghcr.io와 ghrc.io 혼동이 심각한 보안 위협을 야기함
     * ghrc.io는 첫눈에는 기본 nginx 서버처럼 보이지만, 내부적으로는 OCI API를 모방하는 행위가 포착됨
     * 이 사이트는 www-authenticate 헤더를 통해 컨테이너 클라이언트로 하여금 민감한 인증 정보를 보내도록 유도함
     * docker login 등 실수로 인증 정보를 입력하거나 잘못된 레지스트리로 사용하는 경우 자격 증명 유출이 발생할 수 있음
     * 잘못된 서버에 로그인했다면 비밀번호 변경, PAT 폐기, GitHub 계정 이상 활동 점검이 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   단순한 오타로 인해 자주 발생할 수 있는 ghcr.io와 ghrc.io의 혼동이 매우 위험한 보안 문제를 야기하는 사례임. 많은 개발자와 팀이 사용하는 GitHub Container Registry(ghcr.io) 의 오타 버전인 ghrc.io에서 자격 증명 탈취 시도가 포착되었음.

ghcr.io란 무엇인가

     * ghcr.io는 컨테이너 이미지 및 OCI 아티팩트를 위한 OCI 호환 레지스트리임
     * GitHub의 일부로, 수많은 오픈소스 프로젝트에서 인기 있는 레지스트리로 쓰임

ghrc.io: 표면상 보이는 모습

     * ghrc.io에 접속하면 단순한 nginx 기본 화면이 보임
     * 대표적인 404 오류 등 기본 동작은 일반 nginx 서버와 동일함

악의적인 행위의 정체

     * 핵심 문제는 /v2/ 프리픽스 하위 OCI API 호출 시에 나타남
     * 이 경로로 접근할 경우, www-authenticate 헤더와 401 응답으로 공식 컨테이너 레지스트리와 매우 유사한 동작을 보임
     * www-authenticate: Bearer realm=""https://ghrc.io/token""; 헤더가 존재함
     * 이 헤더 때문에 Docker, containerd, podman, Kubernetes 등의 클라이언트가 사용자 인증 정보를 https://ghrc.io/token으로 자동 전달 시도함
     * nginx 기본 설정에는 해당 헤더가 없으므로, 명백히 의도적으로 구성된 것임

위험성: 자격 증명 탈취 시나리오

     * 이 패턴은 타이포스쿼팅(typo-squatting) 기반 자격 증명 탈취 공격으로 판단됨
     * 위험은 사용자 클라이언트가 ghrc.io에 저장된 자격 증명을 입력 또는 저장한 경우에만 발생함
     * 실제 자격 증명이 노출될 수 있는 상황 예시
          + docker login ghrc.io를 실행하는 경우
          + GitHub Action 내에서 docker/login-action 사용 시, 레지스트리를 ghrc.io로 지정한 경우
          + Kubernetes 시크릿에 ghrc.io용 레지스트리 자격 증명을 저장 후 이미지 풀링을 시도한 경우
     * 단순하게 ghrc.io에 이미지를 push/pull만 시도하면 인증 정보는 노출되지 않음(익명 토큰 시도 후 오류 반환)

대응 방안

     * ghrc.io로 실수로 로그인한 적이 있다면, 즉시 비밀번호 변경 및 사용한 PAT(퍼스널 액세스 토큰) 을 폐기해야 함
     * GitHub 계정에서 이상 로그인 또는 악성 활동을 반드시 점검해야 함
     * 공격자는 이를 이용하여 ghcr.io의 저장소에 악성 이미지 추가 또는 계정 접근권을 획득할 수 있음

결론

     * ghcr.io와 유사한 주소를 사용하는 피싱 사이트에 주의 필요함
     * 자격 증명, 토큰, 비밀번호 등 보안 정보 관리에 더욱 철저한 방침이 요구됨

   그냥 github.com 하위 도메인으로 바꾸는 게 낫지 않을까요.

        Hacker News 의견

     * GitHub Container registry가 세분화된 토큰 대신 기존의 클래식 토큰만 지원하고 있어 심각함을 강조함
       관련 문서와 이슈 링크도 첨부함
       공식 문서
       커뮤니티 토론
       로드맵 이슈
          + 이 문제 때문에 클래식 PAT을 완전히 끌 수 없는 상황임
            추가적인 보완책으로는 세션 유효기간을 짧게 두고 엔터프라이즈 SSO로 재인증을 강제하는 방법이 있지만, 실제 필요한 단일 클래식 PAT에는 번거로운 선택임
          + 누군가가 선의로 오타 도메인을 전부 사서 Microsoft에 넘기면 좋겠음
            Microsoft가 레지스트리 이름을 바꿔야 한다고 생각함
            이름이 너무 헷갈려서 나도 오타를 친 적이 많음
     * 도메인 이슈를 여러 번 읽고 나서야 문제를 파악할 수 있었을 만큼, 꽤 설득력 있는 공격 벡터임
          + 나뿐만 아니라 여러 명이 비슷함
            몇 번이나 시도하다 실패하고 심지어 컴퓨터를 다시 시작해도 동일 문제가 발생함
            참고할 만한 사례로 stackoverflow 답변과 Docker 포럼 토론도 있음
     * ghrc.io 관련 코드 검색 결과를 링크로 제시함
     * 기사에서 c와 r이 바뀐 것을 직접 지적해주기 전까지 문제를 알아차리지 못했음
          + 이런 오타는 하루에 거의 10번도 넘게 내는 유형임
          + 여기서 문제는 GitHub의 도메인명이 엉망이라는 것임
            컨테이너 레지스트리 이름이 정말로 별로임
     * 과거 Hacker News 토론 링크를 공유함
     * 이 도메인이 dynadot에서 등록된 것을 whois로 확인할 수 있음
       따라서 abuse@dynadot.com으로 신고하는 것이 가치 있을 것으로 보임
     * 많은 오픈소스 프로젝트에서 이 도메인을 사용하고 있음을 언급하면서 코드 검색 결과도 다시 한번 공유함
          + GitHub 내부적으로 대량으로 자동 수정을 제안하고 배포할 수 있는 도구가 필요함
          + 실제로 깃허브 코드 검색 결과가 상당한 규모임을 언급함
     * CI 작업에서 오타가 나면 큰 문제를 일으킬 수 있겠다는 우려를 표함
     * 보안상 가치가 떨어지는 TLD(최상위 도메인)는 되도록 사용을 지양하자는 조언을 함
       귀엽게 보이려는 시도보다는 보안이 우선임
       악성 도메인 등록 해제가 .com만큼 신속하지 않을 수 있으므로, 미국 verisign이 관리하는 .com이 더 신뢰감 있다고 생각함
       영국령 인도양 지역이나 콜롬비아, 앵귈라 등에 의존하는 건 부적절함
          + .io TLD는 미국 회사인 Afilias가 관리함을 덧붙임
     * 여기서 진짜 위험성이 토큰 재사용 문제인지 질문함
       Bearer 토큰은 패스워드를 직접 주지 않는데, RFC와 Mozilla 문서를 인용하면서 실제로는 인증 토큰 자체가 아니라 인증을 위해 다시 토큰을 받는 절차가 문제가 되는 건지 궁금함
       도메인 간 OAuth가 토큰을 재사용하지 않는다면, 결국 가짜 도메인에서는 토큰을 받지 못하는 것 아닌지 의문임
          + 블로그 저자이자 OCI 유지관리자로서 답변함
            Bearer 토큰을 받기 위한 요청에서 비밀번호나 PAT를 기본 인증 헤더에 base64로 인코딩해서 보내지만, 사실상 평문으로 전송됨
            요청을 받으면 www-authenticate 헤더가 발생하고, 인증 토큰을 받아 레지스트리 접근을 검증하며, 이후 토큰은 만료되지만
            공격자가 실제 토큰을 탈취하는 게 아니라, Bearer 토큰을 받기 위한 자격 증명 자체를 요청하게 되는 구조임
"
"https://news.hada.io/topic?id=22695","Elixir key-value 자료구조 함수 네이밍 규칙 - get, fetch, fetch!","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Elixir key-value 자료구조 함수 네이밍 규칙 - get, fetch, fetch!

     * Elixir에서 key-value 자료구조에서 값을 가져오는 함수가 3개
          + get - 없으면 default 값
          + fetch - {:ok, value} 형식으로 패턴매칭하기 좋게 리턴
          + fetch! - 없으면 에러를 내는 터프한 함수
     * Erlang은 get, find 이름을 사용
     * Erlang 네이밍 규칙이 더 마음에 든다
          + get 은 빠르게 값을 가져오고 fetch 는 DB나 웹에서 가져오는 식으로 이름을 붙여왔음
"
"https://news.hada.io/topic?id=22719","좋은 API 설계에 대해 내가 아는 모든 것","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        좋은 API 설계에 대해 내가 아는 모든 것

     * 소프트웨어 엔지니어링에서 API는 핵심 도구이며, 좋은 API는 지루할 정도로 익숙하고 단순한 것이 바람직한 특징
     * API는 한 번 공개되면 변경이 어렵기 때문에 사용자 환경을 깨지 않는 원칙(WE DO NOT BREAK USERSPACE) 이 중요함
     * 불가피하게 변경할 경우 버전 관리(versioning) 가 필요하지만, 이는 복잡성과 유지보수 비용을 크게 증가시키는 필요악임
     * API 품질은 결국 제품 자체의 가치에 의존하며, 잘못 설계된 제품은 좋은 API를 만들기 어렵게 함
     * 안정성과 확장성을 위해 API 키 기반 인증, 멱등성(idempotency), 레이트 리미트, 커서 기반 페이지네이션 등을 고려해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: API 설계의 중요성과 맥락

     * 현대 소프트웨어 엔지니어의 주요 업무 중 하나는 API와 상호작용하는 것임
     * 작성자 역시 REST, GraphQL, 명령줄 도구 등 다양한 형태의 공개 및 사내용 API를 설계/구현/활용한 경험을 보유함
     * 현존하는 API 설계 조언들은 복잡한 개념(REST의 정의, HATEOAS 등)에 집착하는 경향이 있음
     * 본 글은 실제 경험을 바탕으로 실용적인 API 설계 원칙을 정리한 것임

친숙함과 유연성의 균형: 좋은 API의 첫 번째 조건

     * 좋은 API는 '평범하고 지루한' API, 즉 기존에 접해본 API들과 사용법이 비슷해야 함
     * 사용자는 API 자체보다는 본인의 목적 달성에 집중하기 때문에 진입장벽이 낮은 설계가 필요함
     * 한 번 공개된 API는 변경이 매우 어려워, 최초 설계단계에서 신중함이 요구됨
     * 개발자는 최대한 간결한 API를 원하면서도, 장기적인 유연성을 남기기 위한 고민이 항상 따름
     * 결과적으로, 친숙함과 장기 유연성 간의 균형이 핵심 과제임

사용자 공간을 절대 깨지 않는다 (WE DO NOT BREAK USERSPACE)

     * 기존의 응답 구조에서 필드를 추가하는 변화는 대부분의 경우 문제없음
     * 하지만 필드 제거, 타입이나 구조 변경은 모든 소비자 코드를 깨뜨리는 결과를 초래함
     * API 유지자는 기존 사용자의 소프트웨어를 고의로 망가뜨리지 않을 책임이 있음
     * HTTP의 ""referer"" 헤더 오타조차 고치지 않는 이유는 사용자 공간을 보존하는 문화 때문임

API를 깨뜨리지 않고 변경하기: 버전 관리 전략

     * 필수적일 때만 API에 파괴적 변경을 허용하며, 이때는 버전 관리가 정답임
     * 구버전과 신버전을 동시에 운영하면서 점진적 전환을 유도해야 함
     * 버전 식별자는 URL(/v1/), 헤더 등 다양한 방식 활용 가능하며, 사용자는 각자 속도에 맞게 전환 가능함
     * 버전 관리에는 엄청난 유지보수 비용(엔드포인트 증가, 테스트, 지원)과 사용자 혼동이라는 단점이 존재함
     * Stripe처럼 내부 트랜스레이션 계층을 두더라도, 근본적인 복잡성은 피할 수 없음
     * API 버전 관리 도입은 최후의 수단이어야 함

API의 성공 요인은 전적으로 프로덕트 가치에 달려있음

     * API는 본질적으로 실제 비즈니스 제품의 인터페이스에 불과함
     * OpenAI, Twilio 등 API도 결국 사용자가 원한 것은 API가 제공하는 기능 그 자체임
     * 가치 있는 프로덕트라면 API가 불편해도 사용하게 됨
     * API 품질은 ""마진"" 특성: 본질적 경쟁력이 비슷할 때만 선택 요소가 됨
     * 반면, 아예 API가 없는 제품은 기술 사용자에게 큰 장애물임

프로덕트 설계가 나쁘면 API도 좋아질 수 없음

     * 기술적으로 완성도 높은 API가 있어도, 시장성이 없는 제품이면 의미 없음
     * 더 중요한 것은, 기본 리소스 구조가 비논리적이거나 비효율적이라면 API에서도 드러남
     * 예를 들어, 댓글을 링크드 리스트로 저장하는 시스템은 RESTful 설계조차 자연스럽게 나오기 어렵게 만듦
     * UI에서는 숨겨질 수 있는 기술적 문제들이 API에서는 모두 노출되며, 사용자의 시스템 이해도를 불필요하게 강요함

인증(Authenticaton)과 사용자 다양성

     * 긴 수명의 API 키 기반 인증을 반드시 지원해야 함
     * OAuth 같은 보안성 높은 방식을 추가 지원하더라도, API 키의 진입장벽이 월등히 낮음
     * API 소비자는 엔지니어뿐 아니라 비개발자(영업, 기획, 학생, 취미 개발자 등)도 많음
     * 어렵거나 복잡한 인증 요구(OAuth 등)는 비전문 사용자에게 장벽이 됨

멱등성(Idempotency)과 재시도 처리

     * 액션성 요청(예: 결제, 상태변경 등)은 실패 시 재시도(retry) 에 대한 안전성이 중요함
     * 멱등성이란, 동일 요청을 여러 번 보내도 결과가 한 번만 처리됨을 보장하는 것임
     * 표준 방법은 ""멱등성 키""를 파라미터나 헤더로 전달하여 중복 처리 방지임
     * 멱등성 키 저장은 Redis 등 단순 키/값 저장소로 충분하며, 대부분의 경우 주기적 만료를 적용해도 무방함
     * 읽기/삭제 요청(REST 방식)에는 일반적으로 필요 없음

API 안전성과 속도 제한(Rate limiting)

     * 코드를 통한 API 요청은 사용자의 조작보다 훨씬 빠른 속도로 발생 가능함
     * 무심코 배포한 API 한 건이 의도치 않은 방식(예: 대규모 채팅 시스템)에 활용될 수 있음
     * 속도 제한(ratelimit)은 반드시 필요하며, 비용이 높은 연산에는 더 엄격하게 적용되어야 함
     * 특정 고객에 대한 일시적 API 비활성화(killswitch)도 선택지로 고려해야 함
     * 응답 헤더(X-Limit-Remaining, Retry-After 등)로 속도 제한 정보를 안내해야 함

페이징(Pagination) 전략

     * 대규모 데이터셋(예: 수백만 티켓)을 효율적으로 반환하려면 페이징이 필수임
     * 오프셋 기반(Offset-based) 페이징은 간단하지만 대량 데이터에선 점차 느려짐
     * 커서 기반(Cursor-based) 페이징은 쿼리 성능 저하 없이 아주 큰 데이터셋에도 효과적임
     * 커서 기반은 구현과 활용이 다소 어렵지만, 장기적으로는 필수적 변화일 가능성 높음
     * 응답에 next_page 필드 등을 포함해, 다음 요청의 커서를 명확히 안내하는 것이 현명함

선택적 필드 및 GraphQL에 대한 견해

     * 비용이 크거나 느린 필드는 기본 응답에서 제외하고 필요시만 선택적으로 추가해야 함
     * includes 파라미터 등으로 연관 데이터 포함 가능
     * GraphQL은 데이터 구조 유연성 장점이 있으나, 비개발자 접근성 저하, 캐싱/엣지케이스 복잡화, 뒷단 구현 난이도 등의 문제점 있음
     * 실무 경험상 GraphQL 도입은 꼭 필요한 경우에 한정하는 것이 적합함

내부용 API에 대한 특징

     * 사내 API는 외부(API 공개형)와는 여러 조건이 다름
     * 소비자는 대부분 전문 소프트웨어 엔지니어이므로, 더 복잡한 인증이나 파괴적 변경 가능함
     * 그래도, 멱등성과 사고 예방, 운영 부담 최소화를 위한 설계 원칙은 유효함

요약 정리

     * API는 변경이 어렵고 사용은 쉬워야 하는 특성을 가짐
     * 사용자 공간을 깨지 않는 것이 API 유지자의 가장 중요한 의무임
     * API 버전 관리는 비용이 크기 때문에 최후의 수단으로만 활용해야 함
     * 최종적으로 API의 품질은 프로덕트의 본질적 가치가 좌우함
     * 잘못 설계된 프로덕트는 API 수준에서 보완해도 한계가 큼
     * 간단한 인증 방식 지원, 필수 액션 요청엔 반드시 멱등성, 그리고 속도 제한/페이징 등 안정성 대책 중요함
     * 내부 API는 용도와 대상에 따라 전략이 다르지만, 설계 신중함은 여전히 요구됨
     * REST, JSON 등 포맷이나 OpenAPI 등은 본질적 논점이 아님. 명확한 문서화가 더 중요함

        Hacker News 의견

     * ""userspace를 절대 깨지 말라""는 조언이 유명하지만, 사실 그 반대 측면도 있다는 점을 잘 언급함. 즉, ""커널 API는 예고 없이 깨질 수 있다""는 것임. 중요한 건 ""모든 API를 아무렇게나 깨지 말라""가 아니라, ""안정성을 선언한 부분만 절대 깨지 말라""는 미묘한 균형임
          + 리눅스 커널이 userspace를 깨지 않는다고 해도, GNU libc는 굉장히 자주 userspace 호환성을 깸. 그래서 결과적으로 리눅스 사용자 공간은 커널 개발자들이 아무리 노력해도 깨지는 일이 빈번함. 새 버전 libc에서 빌드된 프로그램과 라이브러리는 하위 libc에서는 제대로 실행이 안 되기도 해서, 결국 모든 구성 요소를 한 번에 업그레이드해야 하는 실정임. 약간 아이러니하게도, 윈도우는 이미 수십 년 전에 redistributable 방식으로 이 문제를 해결했음
          + 리눅스에는 유명하게도 안정적인 공개 드라이버 API가 없다는 점이 있는데, 이게 바로 구글이 Fuschia OS를 개발한 동기라고 들음. 리눅스는 사용자 공간과 하드웨어 모두에 대해 각기 다른 방식으로 방향성을 가진 셈임
     * 글쓴이가 버전 기반 API를 별로 좋아하지 않는 듯하지만, 나는 애초에 앱을 만들 때부터 버전 관리를 반드시 도입하라고 항상 추천함. 미래를 예측할 수 없으니 언젠가 외부 요인으로 인해 깨지는 변경이 당신에게도 일어날 수밖에 없음
          + 실제로 글쓴이도 버전 관리를 추천했다고 생각함. 본문에는 ""버전은 API를 책임 있게 변경하는 방법""이라고 했으니, 결국 버전 관리 자체를 장려하는 셈임. 다만, 새 버전으로의 전환은 최후의 수단으로 하라고 함
          + 나는 굳이 엔드포인트에 ""v1""을 붙이지 말라는 의견에 동의함. 실제 API가 성장하면서 벌어지는 일은, 먼저 기존 엔드포인트에 필드나 옵션을 추가해서 하위 호환을 지키려고 노력함. 그리고, 완전히 호환이 안 되는 작업이 필요해지면, 보통 엔드포인트 이름 자체를 새로 짓고 아예 새로운 엔드포인트(/v2가 아니라)를 만듦. 만약 전체 API를 바꿔야 하면, 기존 서비스를 폐기하고 이름부터 새로 지은 전혀 다른 서비스를 런칭하게 됨. 25년간 일하면서 ""/v1""과 ""/v2""가 나란히 쓰이는 서비스를 딱 한 번만 봤음
          + 저자의 의도가 처음부터 /v1을 엔드포인트에 넣지 말라는 건 아니라고 생각함. 요점은 새 버전(/v2)이 생기지 않도록 최선을 다해야 한다는 것임. /v2가 생기면 버그 픽스마다 양쪽에 다 코드 수정을 해야 하고, 조건 분기가 지수적으로 늘어나서 코드베이스가 스파게티처럼 복잡해짐. 결국 다중 버전을 지원하게 된 애초의 /v1 설계가 미래 호환에 대한 배려가 부족한 셈임
          + 버전 관리를 나중에 추가하는 것도 문제없다고 봄. 예를 들어 처음에는 /api/posts로 시작하고, 다음 버전은 /api/v2/posts로 추가하면 충분함
          + 처음부터 버전을 박아 넣는 방식에 동의하지 않음. 그렇게 하면 정말로 다중 버전이 자주 쓰이게 되는데, 그게 오히려 더 좋지 않다고 생각함
     * 이 글 아주 유익했음. 여기에 한 가지 조언을 추가하겠음. API 문서를 얼마나 어렵게 얻을 수 있느냐와 API 품질은 반비례함. 만약 계약서에 사인해야만 문서를 얻을 수 있는 상황이라면, 그 API의 품질이 형편없을 것으로 가정해도 무방함
     * idempotency key를 comment 테이블에 따로 저장하는 대신, Redis 같은 key/value 저장소에 넣으라고 했는데, 모든 실패 케이스에서 이 방식이 확실한 idempotency를 보장할 수 있을지 궁금함. 가령 서버가 SET key 1 NX 같은 조건부 쓰기를 하다가 이미 key가 있는 걸 발견하면, 댓글 생성을 그냥 건너뛰어야 하는데, 이 시점에 앞선 요청이 실제로 DB에는 반영되지 않았을 수도 있음. idempotency key 저장은 실제 작업과 트랜잭션 단위로 같이 커밋되어야 하고, 필요시 롤백도 되어야 함. 결국 idempotency key 본질은 ‘이 작업 혹은 요청의 고유 ID’가 되어야 함. 예를 들어 “댓글 생성”, “댓글 업데이트” 등 각각에 맞는 리소스별 식별자여야 한다는 것임
          + idempotency를 위한 별도 컴포넌트(예: redis 등)를 더하는 건 지양해야 함. 추상화가 깨지거나 이상하게 동작하는 문제 혹은 딜리버리 보장에 대한 이해 부족에서 오는 오류가 생길 수 있음. 대신에, 쓰기 작업에 라벨이나 메타데이터를 함께 저장하는 식으로, 사용자가 직접 진척 상황을 추적하고 기존 데이터와 함께 보관하는 방식이 훨씬 나음
     * 커서 기반 페이지네이션의 장점은, 사용자 입장에서 페이지를 로드하고 ‘다음’ 버튼을 누르는 사이 새 아이템이 추가돼도, 기존에 봤던 항목을 또 보지 않아도 된다는 점임. 커서 방식은 이전 페이지의 마지막 객체 ID를 기록해두고 그 이후 아이템을 주니까, 무한 스크롤에 특히 유용함. 반면, 커서 기반은 “N번째 페이지로 점프” 기능을 만들기 어렵다는 단점이 있음
          + 커서는 반드시 불투명하게(opaque) 만들어야 DB 크기 같은 것을 외부에 노출하지 않을 수 있음. 그리고 커서에 상태 정보(검색 파라미터, 캐시 상태, 라우팅 정보 등)를 인코딩해서 더 다양한 기능을 구현할 수도 있음
     * 요즘 ""API""라고 하면 대부분 웹앱에 요청 보내고, 파라미터랑 헤더 세팅해서 데이터 가져오는 걸 떠올리는데, 본래 API란 ""Application Programming Interface"" 즉, ‘애플리케이션 프로그램의 인터페이스’라는 뜻임. 1940년대에 처음 쓰였고, 그 뒤 1990년대까지는 거의 다른 의미 없이 사용됐음. API의 역사는 80년이 넘으며, 엄청난 옛날 자료들도 많음. 그때 프로그래머들이 어떤 문제를 다뤄서 어떻게 풀었는지 고민해보면, 오늘날 본인에게도 도움이 되는 부분이 있을 거라고 봄
     * 내부 사용자를 단순히 '사용자'로만 본다는 의견에는 동의하지 않음. 비록 다들 더 기술적인 사람들이고, 프로그래머일 확률이 높아도, 이들도 바쁘고 자기 프로젝트에 집중하느라 API 변경에 대응할 시간이나 여유가 부족한 경우가 많음. 가능하면, 오픈하기 전에 팀 내부에서 ""dogfooding""(실사용) 테스트를 충분히 하는 게 중요함. 일단 외부에 공개되면, ‘userspace를 깨지 않는다’는 약속을 반드시 지켜야 함
          + 내부 사용자라면 직접 컨택해서 마이그레이션을 유도할 수 있는 계측 도구들이 보통 구현돼 있음. 덕분에 API 버전 폐기도 가능해서, 버전 관리를 전략적으로 도입하는 게 충분히 매력적임. 실제 API 버전 관리에 참여해봤고, 기본적으로 이걸 안 쓰는 조직과 비교해서도 확실히 효과를 봤음
          + 버전 관리 방식은 이 문제를 푸는 데 도움이 된다고 봄. 내부 사용자를 배려하는 최선의 방법 중 하나는 스펙에 대해 협업하고, 그 스펙의 작업 중인 버전도 이해관계자들에게 공유하는 것임. 계속 업데이트되는 문서라 해도, 기준점을 잡아주면 내외부 피드백도 원활해지고, 굳이 정책적인 충돌 위험만 피하면 매우 유용하게 쓸 수 있음
     * idempotency key를 redis에 저장하는 대신, 가능하다면 실제 데이터를 기록하는 동일 트랜잭션 내에서 idempotency key도 함께 저장하는 게 더 확실하다고 생각함
     * ""userspace를 절대 깨지 말라""는 경고는 정말 중요함. Spotify, Reddit, Twitter 등 최근 이 원칙을 무시해서 아쉬웠음
     * 참고로 https://jcs.org/2023/07/12/api 링크에 좋은 API 관련 권장사항이 잘 정리되어 있으니 함께 보길 추천함
"
"https://news.hada.io/topic?id=22739","DeepWiki - 어떤 코드베이스든 이해하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       DeepWiki - 어떤 코드베이스든 이해하기

     * DeepWiki는 GitHub 저장소를 즉시 탐색 가능한 위키 형태로 변환해서 볼수 있게 해주는 도구
     * Fast / Deep Research 모드, 라인 단위 인용 기능 등으로 코드 탐색, 환경 설정, 설계 분석 등 다양한 개발 상황에서 신뢰도 높은 답변 제공
     * MCP 서버와 연동되어 Claude, Cursor 등 주요 AI IDE와 자연스럽게 통합 사용 가능
     * 엔지니어링 평가, 구현 예시 확인, 오픈소스 기여, PR 리뷰 등 다양한 개발 실무 전 과정에서 높은 생산성 향상
     * DeepWiki를 이용하면 코드 이해 시간을 크게 단축하며, 팀 온보딩과 리뷰 효율성을 높일 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 도구 개요

     * DeepWiki는 Cognition 팀(Devin AI 엔지니어를 개발한 팀)이 만든 GitHub 저장소 탐색 도구임
     * 저장소 주소에서 github.com을 deepwiki.com으로 바꾸는 것만으로 자동 생성된, 네비게이션 가능한 위키를 바로 사용할 수 있음
     * 낯선 코드베이스, 오픈소스 평가, 고급 기능 구현, 신규 팀 온보딩 등 다양한 상황에서 효율의 극대화 경험 가능
     * 코드를 직접 읽거나 검색하지 않고, 질문 기반으로 구조와 동작을 파악할 수 있음

DeepWiki 주요 동작 방식

     * DeepWiki는 무료 Devin 계정으로 공개 및 비공개 저장소 모두 지원
          + 공개 저장소는 바로 질문 가능하며, 비공개 저장소는 Devin 계정이 필요함
     * Fast 모드는 코드 그래프 기반으로 즉시 답변 제공, Deep Research 모드는 여러 파일을 읽어서 신뢰도 높은 답변 제공
     * 모든 답변에는 클릭 가능한 소스 코드 인용 포함, 실제 위치로 빠르게 이동 가능해 잘못된 요약(환각) 방지 효과 있음

DeepWiki 활용법

  웹사이트 또는 AI IDE에서 활용

     * deepwiki.com에 GitHub URL을 붙여 넣거나, 공식 DeepWiki MCP 서버를 통해 AI IDE(Claude, Windsurf, Cursor 등)에도 바로 연동 가능
     * MCP 서버는 인증 없이 사용 가능하며, IDE 설정에 추가하기만 하면 DeepWiki를 상시 활성화된 쿼리 도우미로 활용 가능
     * 코드베이스의 맥락 및 구조를 언제든 참조하고 질문할 수 있어 개발 생산성이 크게 향상됨

  실제 활용 사례

     * 1. 오픈소스 프로젝트 평가
          + 신규 오픈소스 라이브러리 사용 전, 유지보수 상태/보안/라이선스 등 주요 평가 항목을 즉각적으로 확인 가능
          + 설정 파일, 네트워크 호출, 라이선스 조항 등 정확한 코드 위치와 링크로 안내 받아 신속한 판단에 활용
     * 2. 신규 개발 환경 설정
          + “로컬에서 어떻게 실행하나?”와 같은 질문 시, 환경 설정 방법, 의존성 그래프, 관련 스크립트 등을 원본 인용과 함께 빠르게 제공
          + README, Dockerfile, 스크립트 등 다양한 파일을 자동으로 참조해 초기 세팅 부담 상당히 감소
     * 3. 구현 예시 차용
          + 타 프로젝트에서 독특한 인증 흐름, 상태 저장 방식 등 구현 디테일을 요약된 마크다운으로 받아 활용 가능
          + 예시: tmux를 활용한 다중 coding agent 제어 구조를 DeepWiki로 분석하여 자신의 프로젝트에 적용
     * 4. 맞춤 온보딩 가이드
          + “큐 프로세서의 재시도 처리 흐름 설명” 등 구체적이고 맥락적인 질문에 대해 선임 개발자 같은 상세한 안내와 코드 링크 제공
          + 사용자 맞춤화된 온보딩 자료를 신속하게 획득할 수 있음
     * 5. 첫 기여 탐색
          + 신규 팀 또는 오픈소스 프로젝트 기여 시 “good first issues”를 자동 탐색
          + TODO, 실패 테스트, 미완성 문서 등 초보자도 접근하기 쉬운 시작점을 제시
     * 6. 쿡북(repo cookbook) 스타일 저장소 활용
          + Anthropic Cookbook, Gemini Cookbook 등 예제 중심 저장소 내에서 원하는 예제 및 코드 조각을 빠르게 탐색 및 생성 지원
     * 7. 맥락 인식형 코딩 에이전트 구축
          + 코드 구조, 설계, 코딩 스타일 등 전반적 맥락 파악이 필요한 경우 자동으로 정보 생성
          + Sidekick Dev 등 도구와 연동하여 context 파일(cursorrules.md, claude.md 등)을 자동 생성해 코딩 에이전트 활용도 높임
          + DeepWiki의 무료 MCP API로 온보딩, 테스트 생성, AI pair 프로그래밍 등 다양한 응용 가능
     * 8. Pull Request 리뷰 및 빠른 파악
          + 동료가 PR을 올렸을 때 즉시 구조화된 변경 요약을 DeepWiki에서 생성해 신속한 리뷰 및 맥락 파악
          + 단순 변경점 파악이 아니라 전체 코드베이스 내 위치와 영향까지 이해 가능해, 효율적 리뷰 진행에 기여

DeepWiki 사용 추천 시점

     * 익숙하지 않은 스택, 오랜만에 보는 컴포넌트, 복잡한 공개 저장소 탐색 시 DeepWiki가 가장 우선순위 도구임
     * 기존 grep 검색 대신 위키 요약 탐색→몇 번의 후속 질문→관심 파일로 바로 이동 순서로 빠른 온보딩 경험 가능

DeepWiki에 바라는 점

     * 1. 대화형 사이드킥 모드 – IDE 옆에 DeepWiki를 상시 켜두고, 함수 호출 위치 등 구체 질문을 실시간 요청하는 기능
     * 2. 목표 기반 온보딩 – 저장소와 목표(예: 오픈 이슈 수정)를 입력하면 필요한 파일, 함수, 명령어를 단계별 안내하는 경로 제공

결론 및 사용 권장

     * DeepWiki는 http://deepwiki.com 에서 곧바로 사용 가능
     * 다양한 개발 환경에서 최고의 코드 이해 및 온보딩 도구로 추천할 만함

        Hacker News 의견

     * 명확한 삭제 요청 방법이 없어서 아쉬움이 큼, 우리는 LibreOffice 문서로 이런 잘못된 정보가 생성되길 원하지 않았음에도 deepwiki에서 다음과 같은 내용을 발견하게 됨: https://deepwiki.com/LibreOffice/core/2-build-system (참고: LibreOffice는 Buck이라는 빌드 시스템을 사용한 적이 없음)
          + 호기심에 질문함: LibreOffice에는 .buckversion, BUCK, .buckconfig 같은 파일이 있는데, 이 커밋에서 Buck을 사용한 흔적이 보임, 10년 전 일이긴 하지만 Buck을 잠깐이라도 도입한 역사적 배경이 있는지 궁금함
          + deepwiki에 법적인 어투의 요청을 예의 있게 보냈더니 바로 응답해 내 프로젝트를 인덱스에서 빼줌

     안녕하세요, 저는 오픈소스 소프트웨어의 보안과 사용자 보호를 위해 연락드립니다
     deepwiki가 제 GitHub 조직 내 프로젝트를 인덱싱하지 못하게 하려면 어떻게 해야 되는지 궁금합니다
     만약 여러분이 묵시적으로 저의 프로젝트에 대한 트레이닝 및 작성에 법적 권한이 있다고 생각한다면, 이 통지로 저는 그 권한을 명확하고 영구적으로 철회함을 밝힙니다
     만약 필요하다면, 향후 deepwiki가 제 프로젝트에 대한 잘못된 정보를 게시한다면, 이는 의도적 명예훼손으로 간주할 수 있음도 알려드립니다
     LLM은 의지를 가지지 못하니, 잘못된 정보 출판은 전적으로 인간의 의지에 달린 일임을 말씀드립니다
     감사합니다
     Conrad Buck
          + 실제로 deepwiki를 사용해본 경험에서는, deepwiki가 만들어내는 결과물이 속임수 같은 쓰레기는 아니었음
     * Deepwiki를 무턱대고 비난하고 싶진 않음, 특정 부분(특히 시스템 다이어그램)은 상당히 인상적이고 시간을 절약해줌을 느낌
       다만 내가 관리하는 lib들은 그렇게 대중적이지도 않지만 연간 수백만 다운로드가 있음에도 deepwiki가 생성한 문서 내용이 틀린 경우가 많아서, 사용자에게 오히려 좋지 않은 결과를 준다는 점이 아쉬움
     * DeepWiki라는 도구 자체는 참 괜찮게 느껴짐
       코드베이스 곳곳의 문서를 모아서 한곳에 정리해주는 시도도 괜찮고, 없는 문서도 나름 예측해서 채워주려고 함
       ""특정 항목의 타입이 <X>입니다, 여기에 설명이 있습니다""와 같은 기존 보조 도구들보다 한 수 위의 자동화 코드 어시스턴스 예시라 생각함
       일부 정보는 자동화로도 충분히 도움을 주지만, 때로는 사람의 관점이 꼭 필요함
       ""숙련된 시니어 엔지니어처럼 다뤄야 한다""는 조언에 동의함
       LLM은 인내심에선 믿을만한데(어리석은 질문도 지치지 않고 답해줌), 진짜 시니어처럼 행동하길 기대하긴 어려움
       요청하지 않으면 바보 같은 아이디어에 반론을 제기하거나 더 나은 아이디어로 제안해주지 않음
       또 ‘억지로 반론해달라’고 시키면 필요 이상으로 반박하는 경향도 있음
          + 주석이나 문서가 하나도 없는 저장소에서 deepwiki를 시도 중임
            10분 넘게 기다려도 아무 반응이 없어 흥미롭다고 생각함, Lingo source 프로젝트라 deepwiki가 이미 포기한 것 같음
          + DeepWiki는 이미 큰 가치를 더해준다고 느낌
            나는 오픈소스 프로젝트를 유지관리하는데, DeepWiki를 자주 자원봉사자들에게 추천해서 복잡한 코드베이스를 탐색하게 함
            하지만 이름만 남아 있는데 실제 하는 일이 바뀌었거나, 표준(RFC, 공식 문서 등)대로 하지 않는 struct/패키지/함수에 대해 DeepWiki가 꽤 그럴듯하게 헛소리를 한다는 것도 여러 번 경험함
            비판이라기보단 유지관리자들 리팩터링 관행이나 코드 가독성 문제도 큰 원인이라 생각
            코드 가독성, 테스트는 앞으로도 자유 기여자가 효율적으로 기여할 수 있도록 하는 데 중요 포인트가 될 것이라 예측함
     * Elkjs 프로젝트에서 deepwiki를 사용하는 것 같지만, 솔직히 마음에 들지 않음 https://deepwiki.com/kieler/elkjs/5-usage-guide
       원하는 정보를 찾기 힘들었음
       예를 들어 메인 설정 json 객체 구조를 deepwiki에서는 찾지 못했음
       결국 “AI가 만들지 않은” 원래 Elk 프로젝트 공식 문서 페이지 https://eclipse.dev/elk/documentation/…에서야 찾을 수 있었음
       물론 이건 한 가지 예시일 뿐임
          + “사용한다”고 하기엔 다소 과장인 것 같음
            https://github.com/kieler/elkjs 공식 저장소 어디에도 deepwiki 관련 링크가 없음
            누구든 그냥 deepwiki에 신청만 하면 GitHub 리포지토리 하나씩 만들어낼 수 있음
            deepwiki가 있다는 것만으로 그 프로젝트에서 공인 또는 리뷰했다는 의미가 아님
            본인들 마음대로 난입해서 존재할 뿐, 일종의 SEO 스팸처럼 느껴짐
     * 내가 어느 정도 잘 아는 오픈소스 저장소들을 deepwiki에서 확인해봄
       위키가 있는 건 LLVM(https://deepwiki.com/llvm/llvm-project) 하나뿐임
       첫 화면을 보면 상위 디렉토리 일부만 이상하게 나열되어 있고, 컴파일 파이프라인 다이어그램이 잘못된 내용임
       예를 들면 Clang-AST는 clang 프론트엔드에 포함되어 있어야 하는데 그렇지 않고, 최적화 파이프라인에서 벡터화, 명령 선택 흐름이 어색하게 꼬여 있음
       GlobalISel 같은 중요한 파트가 완전히 빠져있고, 하이라이트된 백엔드 선택도 이상함
       LLVM의 주요 합성 패스(InstCombine) 등 정말 중요한 부분들을 완전히 빠뜨림
       세부 페이지를 들어가 봐도 LLVM IR, 패스 매니저, 패스들의 정규화 전략에 대한 언급도 없음
       TableGen의 역할도 전혀 다루지 않고, 사실 LLVM 백엔드 개발에서 TableGen과 그 에러 메시지 이해가 가장 어려운 부분임
       deepwiki가 한 페이지에서 3만여 라인과 같이 엄청 큰 파일에 집착하는 경향이 있는데, 수만 라인급으로 여러 파일에 쪼개진 clang codegen이나 InstCombine 같은 핵심은 완전히 무시됨
          + 나도 비슷한 경험을 했음
            내가 잘 아는 프로젝트들의 다이어그램 품질이 엔지니어링 수준에 한참 못 미쳤음
          + 흥미로운 지적임
            (내가 deepwiki 내부 동작까지는 모르지만) 파일 크기, 커밋 수, 숫자 기반 메타데이터를 제거하거나 또는 모든 파일을 하나로 합쳐서 경로+파일명 마킹으로 처리한다면 결과물이 크게 달라질지 궁금함
     * deepwiki는 이전에 playwright에서 pure CDP 기반 브라우저 자동화로 대형 코드베이스 리팩토링 할 때 큰 도움이 되었음
       도구 만든 팀에게 박수를 보냄
       자동 생성 오버뷰와 다이어그램도 훌륭하지만, 진짜 강점은 하단의 “딥 리서치” 추가 질문 기능임
       복잡한 코드베이스(puppeteer, playwright, chromium 등)를 deep research하는 것에서 OpenAI, perplexity보다 훨씬 낫다고 생각함
     * 개인적으로 deepwiki로 내 리포지토리 문서를 생성해봤는데, 꽤 유익했다고 느낌
       일부 단순한 부분에 지나치게 깊게 파고들고 중요한 부분을 대충 넘어가는 경향도 있었지만
       전반적으로 패키지가 무엇을, 왜 하는지에 대해 꽤 자세한 요약을 제공해줌
     * 이 글은 원래 짧은 기술 블로그였어야 할 것 같은데, 왜 세일즈맨처럼 영업문구처럼 느껴지는지 궁금함
       “우리는 어느 때보다 더 많은 코드를 만들고 있습니다. LLM인 Claude가 이미 Anthropic 코드의 대부분을 작성합니다. 이제 도전과제는 코드를 생산하는 게 아니라 이해하는 것입니다.”라는 문장부터 뭔가 AI가 쓴 것처럼 느껴짐
       글 전체가 너무 AI 특유의 문체로 가득 채워져 있어 읽으며 집중이 안 됨
       아마도 저자가 AI가 본인보다 글을 더 잘 쓴다고 느낀 결과일 수 있겠으나, 본인 목소리로 직접 쓰기를 꼭 권하고 싶음
       요즘 누가 어떤 부분에서 AI에게 프롬프트를 작성시켰는지 생각하면서, ""dockerfile, README, 스크립트에 대한 의존성 그래프까지 제공하니 바로 작업에 착수할 수 있다"" 같은 AI 생성 텍스트는 애써 무시해버림
          + 동의하는 부분도 있지만, 네가 인용한 앞 부분 두 문장은 오히려 영어 문법 오류가 많아 AI가 작성했다고 보긴 어렵다고 생각함
     * 매우 좋은 리뷰라 생각함 (deepwiki는 정말 놀라움!)
       코드가 오픈소스였다면 더 좋았을 것 같음
       최근 몇 가지 오픈소스 시도를 발견함
          + https://github.com/AsyncFuncAI/deepwiki-open
          + https://github.com/AIDotNet/OpenDeepWiki
            둘 다 스타가 꽤 있음
     * 만약 내가 deepwiki처럼 제3자에게 내 코드를 맡기는 것이 꺼려진다면? 오픈소스 또는 로컬에서 자체적으로 돌릴 수 있는 대안이 있을까?
          + 내 방식은 다음과 같음:
              1. 전체 저장소를 Repopack으로 하나의 텍스트 파일로 아카이브함 https://github.com/yamadashy/repomix
              2. LLMLingua-2로 파일을 압축해 토큰 수를 줄임 https://github.com/microsoft/LLMLingua
                 (토큰이 적을수록 더 많은 컨텍스트를 LLM에게 줄 수 있어 LLM이 리포지토리를 더 잘 이해함)
              3. 압축된 텍스트 파일 내용 전체를 ChatGPT, 또는 로컬 LLM 입력란에 복사 붙여넣기
              4. LLM에게 문서 생성을 요청함
                 예: “이 코드는 저장소 전체 소스임. 현재 컨텍스트를 기반으로 차례 목차를 만들어달라”고 요청
                 목차가 괜찮으면 첫 장 생성을 요청, 반복해서 전체 문서를 완성하는 식으로 진행
              5. Typescript/Javascript 코드베이스라면 esbuild 같은 번들러를 2단계에서 활용하면 토큰 절감에도 도움됨
              6. LLMLingua-2에 관심 있다면, 설치 없이 바로 쓸 수 있는 내 Typescript 포트도 있으니 참고 부탁함: https://atjsh.github.io/llmlingua-2-js/
"
"https://news.hada.io/topic?id=22659","Vibe 코딩은 버스 팩터를 0으로 만든다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Vibe 코딩은 버스 팩터를 0으로 만든다

     * 소프트웨어 개발에서 Bus Factor는 특정 지식 보유자가 몇 명이나 있어야 프로젝트 유지가 가능한지를 나타내는 개념으로, 기존에는 최악의 경우 값이 1이었음
     * 그러나 ChatGPT 공개(2022년 11월 30일) 이후, 생성형 AI가 대중적으로 채택되면서 많은 이들이 지식을 직접 보존하지 않고 AI에 의존하며, 사실상 버스 팩터 0의 상황이 발생함
     * 프로그래밍 현장에서는 점점 더 많은 개발자가 LLM이 생성한 코드와 기능을 그대로 사용하며, 코드베이스를 이해하려는 노력을 포기하고 “바이브 코딩(vibe coding)”으로 전환함
     * 이로 인해 버그 수정, 보안 패치, 기능 확장 시, 누구도 코드가 왜 그렇게 작성되었는지 모르는 상황에 직면할 수 있음
     * 이는 소프트웨어 신뢰성과 보안에 심각한 위험을 초래하며, AI가 완벽한 코드를 완벽히 생성하는 날이 오기 전까지는 근본적 한계가 존재함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

버스 팩터 개념과 역사

     * 버스 팩터는 특정 지식이 몇 명의 사람에게 공유되어 있는지를 수치로 표현한 개념임
          + 예: 3명이 데이터베이스 백업을 복구할 줄 알면 해당 기능의 버스 팩터는 3임
     * 전통적으로 최악의 값은 1이었으며, 한 사람이 지식을 잃으면 프로젝트 유지가 불가능했음
     * 인류는 이를 극복하기 위해 문서화, 교육, 지식 이전, 세미나, 학교 등 수많은 방법으로 지식을 전파
          + 수많은 인적 자원과 시간을 투입해 지식을 전승하고 보존하는 체계적 시도로 이어져 왔음

AI 도입과 버스 팩터 0

     * 2022년 11월 ChatGPT 출시로 “AI First” 시대가 열림
     * AI가 코드와 기능을 생성하는 과정에서 많은 사람들이 지식 보존의 주체에서 배제되고 AI 생성물에 의존하기 시작하며 프로젝트 이해도가 급격히 낮아짐
     * 결과적으로 지식 보유자가 아예 없는 상태, 즉 버스 팩터 0의 상황이 발생함
     * 프로그래머들은 코드와 기능을 스스로 작성·이해하지 않고, AI에게 완전히 위임하는 흐름을 보임
     * 이 과정에서 개발자들은 코드베이스 이해와 문서화를 회피하고 단순히 AI에 설명을 재요청하는 패턴으로 변화함

LLM 기반 코딩의 문제

     * 코드 품질 문제는 차치하더라도, 읽기와 유지보수는 본질적으로 작성보다 더 어렵다는 점이 핵심임
     * 과거에는 멘토나 문서가 최소한의 도움을 제공했지만, AI 의존 환경에서는 이런 안전망조차 사라짐
     * LLM 기반 개발에서는 코드 생성 과정이 기록되지 않고, AI조차 자신이 생성한 코드의 맥락을 기억하지 못함
     * 결국 개발자들은 AI가 작성했지만 맥락이 불명확한 코드를 분석하고 수정해야 하는 상황에 놓임
     * 이는 버그 해결, 보안 취약점 패치, 종속성 업그레이드 등에서 누구도 코드의 의도와 구조를 알 수 없는 상태를 초래

사용자 관점에서의 위험

     * 개발자뿐 아니라 사용자도 위험에 노출됨
          + 개인 문서, 신용카드 정보, 사적인 사진이나 생각 등을 업로드하는 소프트웨어가 내부 구조와 목적을 아무도 모르는 코드로 만들어 졌을 수 있음
     * 이는 데이터 보호와 신뢰성 측면에서 심각한 리스크를 내포하며, 서비스 안정성에 대한 의문을 불러일으킴

결론

     * 버스 팩터 0을 초래하는 바이브 코딩은 근본적으로 결함이 있는 접근임
     * 이는 AI가 100% 정확한 코드를 100% 정확한 프롬프트로 생성할 수 있을 때까지는 피할 수 없는 한계임
     * 따라서 현재 상황에서는 AI 활용과 더불어 지식 보존과 코드 이해의 중요성을 간과할 수 없고, 지식 관리 및 문서화 체계를 유지하는 것이 필수적임

   버스팩터가 무한대가 된거 아닌가요?

   회사 소속 개발자가 지식이 없으면 버스팩터는 0에 수렴하죠

        Hacker News 의견

     * LLM을 사용해서 리뷰되지 않은 방대한 코드를 그냥 뽑아내는 건 잘못된 활용 방법임, 그런 프로젝트는 구조적으로 잘못된 방향으로 가거나 복잡한 버그가 생기면 곧 유지보수가 불가능해짐을 의미함 LLM의 진짜 강점은 다음과 같은 상황임: 기존 복잡한 데이터 구조에 유명 알고리즘을 적용해야 할 때, 테스트 데이터나 의존성이 많은 단위 테스트 뼈대를 세울 때, 시각적 웹 에디터와 백엔드 API를 만들며 sqlite에 저장하는 기능을 붙일 때, 복잡한 정규표현식으로도 어려운 반복 작업을 대규모 코드베이스에 적용할 때 등임 실제로 LLM 덕분에 반나절, 3일 걸릴 일도 2분 만에 시작할 수 있음 중요한 건 LLM이 아주 어려운 문제를 못 풀어도 생산성은 대폭 오를 수 있다는 점임 지루한 반복 작업에서 해방되어 더 흥미로운 문제에 집중할 수 있음
          + 반복적인 변경 작업을 대규모 코드베이스에 적용할 때 LLM이 2분 만에 끝낼 거라 생각했는데 직접 여러 대형 모델로 실험해보니 컨텍스트가 복잡해질수록 에러가 누적되고, 관련 없는 변경도 가끔 발생해 결과적으로 신뢰할 수 없었음 소규모 예시에는 완벽하지만 규모가 커질수록 미흡함 agentic loop를 써서 개선할 수 있지만 반복적으로 수행/리뷰하며 결국 시간이 훨씬 더 들어감 LLM에게 변경 작업을 자동화해주는 프로그램을 작성하게 하는 게 훨씬 신뢰도 높음
          + 제시된 예시들 모두 좋아 보이긴 하지만, 실제로 가능한 활용 예시는 훨씬 더 많음 당신은 숙련 개발자만의 사례를 말했지만, 기술력이 부족하거나 막 배우는 입장의 사람들도 LLM 덕분에 할 수 있는 일이 많이 늘어났음 100달러를 주고 해야 했던 일을 3분 만에 스스로 해볼 수 있게 됨 결과물이 완벽하고 유지보수 가능하냐는 오히려 중요하지 않게 됨, 가능성을 보여주는 것이 더 큰 가치를 가짐
          + 당신의 의견에 동의하지만, 최근에 웃겼던 경험을 공유하고 싶음 Claude에게 단위 테스트 작성을 부탁했는데, 리뷰 결과 실제로 나의 코드에 버그가 있었고 그걸 테스트가 찾았음 그런데 Claude는 버그를 수정하는 대신 해당 실패 테스트를 아예 실행하지 않음으로써 통과시키려 했음, 현실의 유쾌한 에피소드임 LLM이 요구사항 정의, 아키텍처 설계, 요구사항에 맞는 명세 작성에는 약하고, 코드 작성처럼 범위가 명확하고 영향이 제한된 일에는 강점이 있음
          + AI가 PR 리뷰를 자동으로 한 뒤 수동 리뷰를 하는 식으로 중간 단계를 적용해봤음 코드 생성에는 5~10분, 리뷰와 추가 커밋에는 보통 1~3시간 걸리지만, 여러 프로젝트(10~20k LOC, 약 100 파일 규모)에서 이 방식으로 성공적으로 적용했음 명세를 잘 주면 다수의 기능은 큰 수정 없이 거의 정확히 구현되고, 피드백 기반 리팩토링이 주로 발생함 물론 제대로 동작하지 않을 때는 해결에 하루 가까이 소요되는 경우도 있지만, 전체적으로는 3~5배 생산성 향상임 대규모 프로젝트에는 조각내어 모듈화하는 게 더 좋아 보임
          + ""LLM으로 2분 만에 x일치 작업 완료""라는 식의 표현은 리뷰 시간이 포함되지 않았기 때문에 조금 과장임 실제 리뷰와 검수 과정까지 합치면 시간이 훨씬 더 걸림 처음에 이야기한 “잘못된 방법”에 오히려 빠질 수 있음
     * 이 글에서는 AI 코드 생성의 문제점은 여러 개 제시하지만, 이미 존재하거나 앞으로 등장할 수 있는 해결책은 고려하지 않은 듯함 예전에도 팀이 코드베이스에 대해 최소한의 노력만 했어도, 새로 들어온 사람이 코드를 이해하는 데 도움을 받을 수 있었음 레거시 코드 경험이 없는 건 아닌지, 아니면 AI가 ""처음 작성 과정에 대한 모든 맥락을 잊어버린다""라는 점에 대해 정말 못 고친다고 생각하는지 의문임 Bus Factor 0 문제도 100% 완벽하게 정확해야만 해결되는 걸로 오해하는데, 사실 사람도 100% 항상 정확한 건 아니지만, 그래도 신뢰함
          + 해당 글이 너무 축약적으로 문제를 바라본다고 느꼈음 우리는 애초에 모든 일을 작성자와 함께 할 수 없다는 현실 자체가 이미 존재함 짝궁이나 설명해주는 AI의 존재만으로도 엄청난 진전임 인간이 없는 세상을 상상하는 것 같지만, 이미 우리는 그런 상황을 종종 겪고 있음
          + 나는 저자임, 첫 번째 지적에 동의하고, AI가 앞으로 격차를 좁혀줄 거라고 생각함 하지만 그때쯤이면 이미 일부 문제는 발생해버렸을 수 있음 논리적 맥락이나 조작 기록이 없는 코드가 남게 되는 문제도 있음 AI가 ""항상 학습한다""는 얘기가 많지만 실제론 새로운 모델이 나오기 전까진 배우지 않음 사람도 100% 정확하진 않지만 Bus Factor 0은 아님, 문제 파악과 해결이 더 쉬움 나머지 문제도 해결된다면 버스 팩터 문제도 줄어듦
          + 과거 레거시 코드 분석할 때 AI 툴이 있었으면 좋았겠다고 생각함 ""이 Perl 파일 마지막 작성자가 이제 지점장인데, 직접 미팅 예약해야 하나?"" 같은 황당한 상황이 실제로 있었음
          + ""왜 100% 정확해야 하는가?""라는 질문엔, AI에 비판적인 사람들은 오히려 AI가 마법처럼 완벽한 해결책이길 기대하는 경향이 있다고 생각함 정적 타입을 반대하는 사람이 ""논리적 에러까지 못 잡는다""며 불평하는 것과 비슷한 뉘앙스임
     * 요즘 블로그마다 AI가 만든 이미지가 너무 많아져서 오히려 집중에 방해되고, 내용에 도움이 안 되는 경우가 많은 것 같음
     * 최근 엉망인 코드베이스가 있는 팀에 합류했는데, 기존 개발자들도 대부분 떠났고 남아있던 사람들도 코드를 잘 몰랐음 완전히 버스 팩터 0임 놀랍게도 AI 덕분에 코드 이해와 의도 파악, 디버깅 속도 등이 크게 개선되었음 AI로 코드 자체에서 문서를 뽑아내기 시작함 문서화나 구전은 왜곡될 수 있는데, 코드는 그 자체가 진실임 AI의 도움으로 코드가 스스로 설명하는 환경을 만들 수 있었고 큰 생산성 향상을 느낌
          + 나는 관리자 입장에서, 이제부터 우리 팀의 모든 readme가 구식이 되지 않도록 한 가지 규칙을 정하려고 고민 중임 Claude Code에게 기존 readme와 최신 코드, PR 변경 사항까지 읽도록 하여 필수적으로 readme를 갱신하게 할 수 있다고 생각함 물론 완벽하지 않지만, 요약이 타당한지 개발자가 최종 검수를 해야 하며, readme가 구식이 되는 원인이 '귀찮음'에서 비롯되는 현상을 AI가 상당히 줄여줄 수 있음
     * LLM 등장 전에도 Bus Factor는 항상 문제였음 대부분의 회사가 일의 일부를 여러 명이 이해하도록 구조화하지 않았음 여러 사람이 각 영역에 배정되어 있어도 일의 양이 계속 늘어나 최종적으로는 아무도 다 이해하지 못하는 상황이 반복됨 이를 완전히 피하려면 코드베이스 내 인원을 로테이션시키는 등 엄청난 엔지니어링 관리가 필요하고, 대개는 속도에 대한 요구 때문에 완벽히 못함 이와 관련된 내 CTO 경험 회고를 여기에서 책으로 정리해서 가격 상관없이 공개함 LLM으로 시스템을 만든 환경과 10명의 외주 개발자를 쓰는 환경의 원리는 별반 다르지 않다고 생각함
          + Bus Factor는 LLM 이전에도 문제가 됐고, 오래 전부터 있어온 전문 용어임 TFA(원문)는 이전에는 Bus Factor가 1이었고, 이제는 아예 0으로 가고 있다는 추세를 비판하는 것임
          + 일의 양이 커질 뿐이지, 실제로는 더 권장할 만한 방향으로 일이 진행되는 게 아니라 마감에 맞춰서 대충 끝내는 패턴만 반복됨 절차상 몇몇 장애물을 두는 걸로는 해결되지 않음
     * 우리의 두뇌는 자주 쓰지 않는 정보에는 에너지를 아끼려고 하기에 뭔가로부터 멀어질수록 더 잘 못 이해하거나 잊어버리게 됨 직접 코드 리뷰를 다 해도 결국 실력이 퇴화할 수 있음 엔지니어가 관리자 업무를 오래 하면 기술적 문제를 거의 못 풀게 되는 것과 비슷함 자동차 자동화에서도 중간 단계(level2→5)를 인간이 계속 관여하며 유지하는 게 어렵고, 기계가 100% 신뢰 가능하지 않으면 결국 문제가 됨
     * 이 논의에서 정말 중요한 포인트가 있는데, 사실 이런 툴과 워크플로는 이제 막 시작 단계임 앞으로 이런 문제를 AI가 인간보다 더 잘 풀 수도 있다고 확신함 LLM을 활용하는 실험도 해봤는데 일부 성공, 일부 실패였지만 특정 분야에서는 확실히 뛰어난 능력을 보여줌 LLM은 귀찮아하지 않고 문서나 주석, README, ADR까지 꼼꼼히 업데이트해줄 수 있음 충분한 가이드와 구조만 있으면 LLM 코드베이스가 장기적으로 오히려 더 진입하기 쉬울 수도 있음, 그만큼 문서화가 우수하기 쉬움
          + 굉장히 중요한 포인트이긴 한데, 사실 이제 이런 툴의 끝에 와 있기에 이 문제들이 해결 불가능하다고 보는 입장임
     * 글은 코드 그 자체로도 의도를 상당 부분 읽어낼 수 있다는 점을 간과하고 있다고 생각함 인간은, 그리고 LLM도 그렇겠지만, 꽤 예측 가능한 존재임 대개 비슷한 문제를 비슷한 방식으로 해결함 코드가 어떻게 작성됐는지 보면, 왜 누가 언제 어떤 문제를 풀었는지에 대한 힌트를 알 수 있음 물론 정보가 감춰지는 부분도 많지만, 구성원이 자주 바뀌는 조직에서도 비슷한 일이 일어남
          + 인간의 사고 과정이 결국 코드에 녹아 들어간다고 보지만, 그럼에도 불구하고 직접 물어볼 수 있는 사람이 있는 것보다 훨씬 열등한 과정임 리버스 엔지니어링은 꼭 필요할 때만 하는 게 보통인데, 레거시 코드에서는 결국 다들 하게 됨 하지만 생산성 면에서 좋은 건 아님 그리고 LLM 코드베이스는 단일한 의도가 아니라 여러 다양한 사람의 의도가 섞여 있어서, 일부 코드 조각만 봐서는 원래 취지가 무엇인지 오히려 더 혼란스러움 AI가 생성한 코드도 사람이 작성한 코드처럼 철저히 동일 의미를 지닌다고 착각하게 되어 오히려 해석이 더 힘들어질 수 있음
          + 의도를 코드만 보고 파악하는 능력은 범위와 규모에 따라 다름 아두이노처럼 32kB 제한이 있으면 이해는 쉬움 하지만 수십 개 마이크로서비스가 뒤엉킨 복잡한 플랫폼에서, 특히 그걸 'vibe coding'식으로 짜놓았다면 내 책임이 되면 그냥 포기하고 싶어짐
     * 이 글의 요점과 결론에는 동의하지만, 20년간 여러 번 비슷한 상황(아무도 질문할 수 없는 환경, 실제 담당자가 다 떠남)을 겪어봤음 LLM 덕분에 이런 일이 조금 더 빨라질 순 있어도, 완전히 새로운 문제라기보다는 예전 문제의 가속화에 가깝다고 생각함 이런 문제 의식을 환영함
          + 저자는 Bus Factor 0이 실제로 무엇을 의미하는지, 현실적으로 어떻게 도달하게 되는지에 대해 간과함 Bus Factor 0을 허용하는 회사는 단순히 전문성에 투자할 경제적 동기가 없는 회사임 인간이 AI와 경쟁하며 얻는 경제적 이득이 0이 되고, AI가 비용을 10배 절감하는 상황이 오면, 마케팅의 허위/과장과 통신 채널 혼선까지 더해져 문제는 명확해짐 수요와 공급의 원칙에서 보면 공급(전문가)이 무한대가 되어버리면 수요는 사라짐 인재 육성 파이프라인은 2~10년에 걸쳐서 만들어지는데, 성장 유인이 사라진 시점부터 미래에 심각한 위기가 도래함 실제로 지역 대학에서 컴퓨터공학 강좌가 학생 감소로 줄어드는 사례까지 있었고, 학생들은 AI 때문에 진로를 포기한다고 대답함 전문가 공급이 사라지면 고칠 사람도 돈 주고 살 수 없게 됨 경제의 토대를 건드리면
            문제는 시차에 의해 뒤늦게 커지는데, 현실에선 충분히 빠르게 대처할 수 없음 결국 심각한 위기가 발생하고, 그때서야 극단 처방이 시작됨
     * 오히려 반대 상황도 있음 코드베이스를 AI가 잘 활용할 수 있도록 문서화, 테스트, 설정 등을 잘 해놓으면 1년 뒤에도 AI agent가 같은 작업을 더 빨리 해낼 거라 예상함
          + AI Coding Tool이 어떻게 기존 개발자처럼 ""이전 코드는 다 별로이니 싹 다시 짜야 한다""는 태도를 취하게 될지 궁금함 앞으로 CI/CD 시스템 자체가 전체 프로젝트를 AI가 통째로 재작성하는 식이 될지도 흥미로움
          + 나는 저자임, 말한 대로라면 이미 Bus Factor는 올라가게 됨 즉, 정보가 머릿속에만 남지 않고 다양한 형태로 저장, 지속되는 것이 본질임
"
"https://news.hada.io/topic?id=22710","당신의 시간을 소유하게 해주는 시간관리 기법들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       당신의 시간을 소유하게 해주는 시간관리 기법들

     * 시간 관리는 한정된 자원을 효율적으로 사용하여 목표와 생산성을 높이는 핵심 기술
     * 구조적 기법 : Franklin의 하루 분할, 3/3/3 방법, Eisenhower 매트릭스, Pomodoro 기법, 시간 블로킹, Ivy Lee
     * 생산성 프레임워크 : GTD, 2분 규칙, Seinfeld 전략, Eat the Frog, 작업 쪼개기
     * 멀티태스킹의 해로움을 강조하며, 깊은 집중과 단일 작업의 가치를 Cal Newport의 ‘Deep Work’ 등 연구 결과와 함께 제시
     * 전체적으로 시간 관리의 본질은 더 많은 일을 하는 것이 아니라, 집중과 우선순위를 통해 의미 있는 성과를 만들어내는 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * Lord Chesterfield와 Benjamin Franklin은 각각 “시간을 세심히 관리하라”와 “시간은 돈이다”라는 명언으로 시간 관리의 중요성을 강조했음
     * 오늘날 많은 사람은 다른 사람의 일정과 회의에 휘둘리며 스스로 시간을 통제하지 못한다고 느낌
     * 이를 개선하기 위해 다양한 시간 관리 기법과 생산성 프레임워크를 소개함

구조적 시간 관리 기법

     * Benjamin Franklin의 하루 분할
          + 하루를 일, 자기계발, 휴식 등으로 나누고 매일 아침 목표를 점검
          + 구조적 습관이 다양한 업적의 기반이 됨
     * Oliver Burkeman의 3/3/3 방법
          + 3시간 집중 작업, 3개의 짧은 업무, 3개의 행정 작업으로 하루를 구성
          + 에너지 수준에 맞는 업무 배치로 생산성과 균형을 확보함
     * Eisenhower 매트릭스
          + 긴급성과 중요도를 기준으로 업무를 4분면에 배치
          + 긴급·중요는 즉시 처리, 중요·비긴급은 예약, 긴급·비중요는 위임, 둘 다 아니면 제거
     * Pomodoro 기법
          + 25분 집중 + 5분 휴식의 사이클로 집중력을 유지
          + 4세트 후 긴 휴식으로 번아웃 예방
     * 시간 블로킹
          + 하루를 미리 계획된 시간 블록으로 나누어 업무 배치
          + 컨텍스트 전환을 줄이고 중요한 업무를 보호함
     * Ivy Lee 방법
          + 하루 6가지 최우선 과제를 기록하고 순서대로 처리
          + 단순하지만 강력한 우선순위 관리 기법임

생산성 강화 프레임워크

     * GTD(Getting Things Done)
          + 캡처, 명확화, 정리, 검토, 실행의 5단계를 통해 뇌의 기억 부담을 줄이는 체계적 시스템
     * 2분 규칙
          + 2분 내 끝나는 일은 즉시 처리하여 작은 업무의 누적을 방지
          + 미루지 않고 즉시 실행함으로써 관성을 만들고 큰 작업으로 연결됨
     * Seinfeld 전략
          + 매일의 실천을 끊지 않고 ‘체인’을 이어가는 방식
          + 습관 형성과 장기적 성과 창출에 효과적임
     * Eat the Frog
          + 하루의 가장 어려운 일을 먼저 처리하여 성취감과 동력을 확보
          + 아침 시간대의 높은 집중력을 활용하는 전략임
     * 작업 쪼개기(Task Chunking)
          + 큰 프로젝트를 작은 단계로 나누어 부담을 줄이고 실행 가능성을 높임
          + 점진적 진행으로 장기 목표도 달성 가능함

멀티태스킹의 함정

     * Stanford 연구에 따르면 멀티태스커는 집중력과 효율성이 낮으며, 기억력 저하까지 경험함
     * 뇌는 동시에 여러 고차원 작업을 처리하지 못하고 빠르게 전환할 뿐, 이로 인해 정신적 피로와 생산성 저하가 발생함
     * 연구 결과, 방해받은 후 다시 집중하는 데 평균 23분 이상 소요됨
     * Cal Newport의 『Deep Work』는 깊은 집중력이 오늘날 가장 가치 있는 능력 중 하나임을 강조함

결론

     * 시간 관리의 핵심은 더 많은 일을 하는 것이 아니라 한 가지에 집중하여 중요한 성과를 만드는 것임
     * 멀티태스킹을 줄이고, 자신에게 맞는 기법을 실험·정제하면서 지속적 발전을 추구해야 함
     * “시계를 다스려야지 시계에 지배받지 말라”는 Golda Meir의 말처럼, 시간을 능동적으로 통제하는 태도가 필요함

   알지만 실천하기 어려운 것들이네요. 특히나 업무 메신저가 방해의 일등공신 같아요.

   원문 번역 및 정리해 보았습니다 : https://drive.google.com/file/d/…
"
"https://news.hada.io/topic?id=22694","Roblox-호환 오픈소스 게임 엔진인 Librebox","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Roblox-호환 오픈소스 게임 엔진인 Librebox

     * Librebox는 오픈소스 Luau 기반 3D 게임 엔진으로, Roblox와 API 호환성을 지향함
     * 개발자는 엔진부터 코드까지 자신의 게임에 대한 완전한 소유권을 가질 수 있음
     * 현재는 데모 단계로, 핵심적인 씬 렌더링과 카메라 이동, 기본 파트 생성 등이 지원됨
     * 향후 버전에서는 물리, 사용자 입력, 멀티플랫폼 지원 등이 추가될 예정임
     * 라이센스는 MIT로, 완전히 무료이며 자유로운 수정과 배포가 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Librebox 소개

     * Librebox는 Luau 엔진 위에 구축된 오픈소스 3D 게임 엔진임
     * 주요 목표는 Roblox와 유사한 API를 제공하여, 기존 Luau 코드의 호환성을 최대한 보장함
     * 개발자는 엔진 자체와 게임 로직 모두에 대한 소유권과 자유를 가지며, 플랫폼 종속성을 벗어난 개발이 가능함

왜 Librebox인가?

     * 자유로운 엔진 사용과 소유권 확보가 가능함
     * 기존 Roblox나 Luau 친숙 개발자가 거의 수정 없이 코드 재사용 가능
     * 자신만의 플랫폼 구축 및 자유로운 배포, 변형, 수익화 가능

주요 예시 코드

     * 예시: 파트를 생성하고 회전 및 색상 변환
          + examples/part_example.lua 파일 참고
          + 기존 Luau/Roblox 코드와 거의 동일한 방식으로 파트 생성, 색상, 위치, 회전, 루프 처리 가능

지원 기능 요약 (데모 버전 기준)

     * 씬 렌더링, 조명, 섀도우, 스카이박스
     * game.Workspace 내 오브젝트 렌더링
     * 기본 카메라 이동 기능
     * Instance System 및 주요 데이터 타입 지원
          + CFrame, Vector3, Color3, Random 등
          + Instance.new, 파트 복제/파괴, 속성 제어
     * Client 사이드 서비스
          + Workspace, Camera, Lighting, RunService 등
          + RenderStep/HeartBeat 이벤트 지원
     * Luau 스크립트 지원 및 스케줄러
          + 코루틴, 이벤트, 비동기 작업(task.spawn, task.wait 등)
     * 윈도우 핸들링 및 전체화면 최적화

플랫폼 및 확장

     * 현재 Windows 전용 지원
     * raylib을 활용, 타 OS로의 이식이 용이함
     * Standalone 실행파일로 배포

앞으로 제공 예정인 기능

     * 물리 엔진 및 충돌 감지
     * 모델/메시, 이미지, GUI, 머터리얼 지원
     * Onscreen GUI, Replication/Multiplayer (서버)
     * UserInputService, ContextActionService 등
     * 자체 에디터, 서버/클라이언트 완결 생태계 구축

비전 및 미래

     * 완전한 독립형 오픈소스 엔진으로 Godot, Unity와 유사한 목표
     * 플랫폼 종속 없이, 내 게임/내 코드로 자유로운 개발 실현
     * 에디터, 서버, 배포 및 수익화까지 모든 영역 확장 계획
     * 사용자 API 및 소스코드 재작성 가능

라이선스 및 저작권

     * MIT 라이선스
     * Luau(로블록스 엔진 기반, MIT), raylib(zlib/libpng) 등 오픈소스 라이선스만 사용
     * 상업 플랫폼 및 타사와 무관한 독립 프로젝트
     * 외부 소스/에셋/프로프라이어터리 코드 불포함

문의 및 커뮤니티

     * 이메일: librebox.developers@gmail.com
     * 누구나 사용, 피드백, 수정, 기여 가능

기술 스택

     * C++ , Lua, Luau, Python, C, CMake 등으로 구성

중요성 및 비교 우위

     * Roblox/루아 생태계의 독립 대응 오픈소스 엔진
     * 기존 상용 플랫폼에서는 제한되는 제작/소유/수익화의 자유를 제공
     * 완전한 소스공개, API 호환성, 윈도우 기반 데모 제공으로, 루아 기반 3D 게임/콘텐츠 제작에 적합

결론

     * 무료, 오픈소스, 호환성, 자유까지 모두 갖춘 게임 엔진으로, 주니어 개발자 및 크로스플랫폼 게임 개발에 매력적인 선택지임

        Hacker News 의견

     * Librebox는 아직 데모 단계임, Roblox API의 극히 일부만 구현하고 있고, 서버나 네트워킹 등 많이 빠진 기능이 많음
          + 내가 개발팀이라면 서버와 네트워킹부터 구현을 시작할 것임, 나중에 추가하려고 하면 정말 어렵기 마련임
     * 참신한 시도라 생각하고 행운을 빔, Roblox의 법무팀에게 공격받지 않길 바람, 리눅스 네이티브 클라이언트 쪽으로 좋은 활용도가 생길 수 있음. 현재 많이 쓰이는 Sober는 독점 소프트웨어이고, 예전에 쓰던 Vinegar는 리눅스 해커들 문제로 차단당함
          + Roblox가 법적 조치를 시도할 수 있겠지만, 개인적으로 이 프로젝트는 명백히 합법적이라고 느껴짐. 합법성 기준에서 봤을 때 이건 VLC Media Player(특허 이슈)보다 더 높은 점수를 줄 수 있고, NES 에뮬레이터보다는 훨씬 높음. Android보단 낮다고 생각하지만, 어쨌든 Oracle이 Android를 소송한 적은 있음. (나는 변호사는 아님)
          + Roblox 법무팀의 우선순위에서 보면 이런 프로젝트가 Roblox 내 아동 착취 문제보다 더 높은 처리 대상인 것 같다는 생각이 듦
     * :WaitForChild()가 없다는 점이 언급되었는데, 오히려 좋은 점 아닌가 얘기함
          + 사실 큰 문제는 아니고, 약간의 편의 기능임, busy waiting보다 나은 부분임. 어떤 오브젝트가 코드 실행 전엔 꼭 존재한다고 보장되지 않을 때가 많은데, 다음과 같이 유사한 동작을 직접 구현할 수 있음: while not parentObj:FindFirstChild(""childObj name"") do wait() end. 내가 알기로 wait() 함수는 프레임당 1/30초 이상임, 완전 즉각적으로 하려면 각 하트비트마다 실행해야 함
     * Roblox에 묶여 있는 엄청난 양의 유저 콘텐츠가 있음, 정말 많은 자원이 있음을 느낌
          + 특히 Club류 콘텐츠는 반드시 해방시켜야 함
     * 저장소(github)에서 개발자에 대한 정보를 아무리 찾아도 없음. 디스코드 서버마저도 채널에 아무것도 없고, 개발자 역할로 등록된 계정도 프로젝트 전용 단 한 개임. 소스코드엔 실제 작업이 이루어진 티가 나고 단순히 README만 있는 건 아님. 그래도 전체적으로 뭔가 괴상하다는 느낌임
          + 혼자서 하는 원맨 프로젝트들이 원래 이런 식임
          + 내가 호의적으로 해석하자면, 자기 신분을 숨기는 건 법적 문제를 두려워해서 그런 듯함. Roblox 자체가 810억 가치의 대기업임
     * Robux를 쓸 수 없다면 개인적으로 별로 관심이 없음. 사실 문제의 핵심은 Robux(화폐 시스템)임, 엔진 그 자체는 그렇게 특별하지 않음
          + 많은 개발자들이 ROBLOX에서 벗어나지 못하는 이유는 어릴 때부터 ROBLOX 툴을 익히고 숙련된 뒤, 그 기술이 대부분의 다른 게임 엔진에서 쓸 수 없는 틈새 스킬이 되어버린 것임. 이제 선택지는 Unity 같은 새로운 환경에서 초보로 다시 시작하거나, 예전처럼 ROBLOX 내부에서 계속 수준 높은 결과물을 내는 것밖에 없음. 각 커뮤니티에서 쌓아온 친구와 명성이 있기 때문임. 솔직히 API 호환 대체제가 이렇게 늦게 나왔다는 게 오히려 놀라움
          + 개발자들이 자기 Roblox 게임을 독립형 게임으로 릴리즈하고, 플랫폼 락인을 탈출할 수 있는 기회일 수도 있음. 물론, 기존 유저들이 따라올지는 별개의 문제임
          + 내가 Roblox를 좋아하진 않지만, Roblox 제작 도구의 완성도는 실제로 꽤 괜찮다고 들음
     * 이 저장소는 꽤 멋지다고 생각함
     * 이게 로컬에서 테스트 기능이나 QA 향상에 실질적으로 도움이 되는지 궁금함. 최근 react-lua 앱에서 jest 테스트 돌리려고 Lune에 Lemur(아카이브 됨)을 동작시키려고 했는데, in-game output을 출력하는 테스트 러너도 만들었었음. 문제는 Roblox Studio가 아직 Linux에서 vinegar 패키지로는 잘 안 돌아가서, 그냥 RobloxStudio.exe --place game.rbxlx --script test_runner.lua --keep-open 옵션으로 콘솔 출력만 계속 찍을 수 있으면 엄청 편했을 것임.
          + Lemur 저장소에 loadPlaceFile 추가하려다, 거기다 넣는 게 맞나 싶어서 멈췄는데, 이때 Librebox를 알게 되었고, 이제는 이걸로 react-lua 앱을 Jest로 로컬 CI 테스트하는 방향을 기대하게 됨
          + 참고로 Roblox서 place 안에서 Luau 코드를 실행할 수 있는 Open Cloud Engine API 베타도 있음. [Beta] Open Cloud Engine API for Executing Luau
          + 테스트를 로컬에서 돌리면 얻는 이점: 테스트 실패 시 스크린샷·비디오 기록, 즉각적인 피드백, -i 옵션이나 interactive로 실패 시 바로 게임 세션으로 진입 가능함
"
"https://news.hada.io/topic?id=22632","Show GN: MCP서버 :: PostgreSQL 모니터링/운영/관리 (MCP-PostgreSQL-Ops)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Show GN: MCP서버 :: PostgreSQL 모니터링/운영/관리 (MCP-PostgreSQL-Ops)

   PostgreSQL 모니터링/운영/관리를 LLM을 통한 자연어로 할 수 있게 지원하는 MCP서버 입니다. 계속적인 Tool 추가 및 개선중이며, 피드백 주시면 정말 도움이 될것 같아요!

   <대표적 요청 문구 예시>
   887 / 5,000
   ""PostgreSQL 서버 상태 확인""
   ""PostgreSQL 서버 버전 및 연결 상태 확인""
   ""확장 프로그램 설치 여부 확인""
   ""현재 활성 연결 수 표시""
   ""shared_buffers 구성 표시""
   ""shared_buffers에 대한 PostgreSQL 구성 매개변수 표시""
   ""모든 메모리 관련 구성 설정 검색""
   ""로깅 구성 매개변수 표시""
   ""연결 관련 설정 표시""
   ""모든 시간 초과 구성 검색""
   ""모든 PostgreSQL 구성 매개변수 표시""
   ""가장 느린 쿼리 상위 10개 표시""
   ""가장 느린 쿼리 상위 20개 표시""
   ""특정 데이터베이스에서 느린 쿼리 분석""
   ""사용되지 않는 인덱스 검색""
   ""최근 쿼리 활동 분석""
   ""특정 데이터베이스에서 인덱스 효율성 확인""
   ""데이터베이스 크기 확인""
   ""가장 큰 테이블 검색""
   ""VACUUM이 필요한 테이블 표시""
   ""특정 데이터베이스 스키마에서 테이블 크기 확인""
   ""특정 데이터베이스의 테이블 나열""
   ""특정 데이터베이스의 유지 관리 상태 확인""

   응원합니다 더더 고도화될모습도 기대됩니다

   응원 감사합니다!!
"
"https://news.hada.io/topic?id=22673","Waymo, 뉴욕시에서 자율주행차 테스트 허가 취득","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Waymo, 뉴욕시에서 자율주행차 테스트 허가 취득

     * Waymo가 뉴욕시에서 자율주행차 테스트 허가를 공식적으로 취득함
     * 이 허가로 인해 Waymo는 복잡한 도심 환경에서 알고리듬과 시스템을 평가할 기회를 갖게 됨
     * 뉴욕시는 보행자와 차량, 자전거 등 다양한 교통 혼합으로 인해 테스트 난이도가 높은 지역임
     * 지금까지 자율주행차 개발은 주로 캘리포니아, 애리조나 등에서 진행되어 왔음
     * 이번 진출은 도시 환경 자율주행의 상용화 및 안전성 확보에 중요한 전환점임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Waymo의 뉴욕시 자율주행차 테스트 허가

  개요

     * Waymo는 최근 뉴욕시에서 자율주행차 테스트를 진행할 수 있는 정부의 공식 허가를 취득함
     * 이 허가는 뉴욕시의 복잡하고 다양한 교통 인프라 환경에서 Waymo의 자율주행 알고리듬과 시스템의 신뢰성과 안전성을 시험할 수 있는 기회를 제공함
     * 뉴욕시는 보행자가 많고 도로 상황이 시시각각 변하는 등 고난도 교통 조건을 갖추고 있어, 이를 바탕으로 자율주행 기술의 실제적 검증이 가능함

  도시 환경에서의 자율주행 테스트의 의미

     * 지금까지 자율주행차의 테스트 및 상용 서비스는 주로 캘리포니아, 애리조나, 네바다, 텍사스 등 기후와 도로 환경이 단순한 지역에서 이뤄져 왔음
     * 뉴욕시는 도시 밀집도, 혼잡도, 다수의 보행자, 다양한 차량 유형(택시, 배달 오토바이, 자전거 등) 등의 요소가 복합적으로 작용하는 특수한 테스트 환경임
     * Waymo는 이러한 환경에서의 테스트를 통해 기존에 다루지 못했던 변수와 복잡성에 대처할 수 있는 알고리듬 개선 및 실증 데이터를 획득할 계획임

  시장 및 산업적 의미

     * 뉴욕시 자율주행 테스트 허가 획득은 Waymo의 기술 신뢰도 강화와 함께 글로벌 경쟁사 대비 차별화 요소로 작용함
     * 도심 자율주행 상용화를 위한 중요한 레퍼런스이자 모멘텀이라는 평가를 받고 있음
     * 향후 도시 환경에서 이동 서비스(MaaS) 확대 및 자율주행차 보급의 실질적 시금석 역할을 기대할 수 있음

  향후 과제와 전망

     * Waymo는 뉴욕시에서 실험 운행 데이터를 수집 및 분석하여 자율주행차의 안전성, 신뢰성, 보행자 및 복잡한 교통 흐름에 대한 적응력을 강화할 계획임
     * 법적 규제, 시민 안전, 데이터 활용 및 프라이버시 보호 등 다양한 사회적, 기술적 이슈가 병행하여 논의될 필요가 있음
     * 미국 내 주요 도시를 중심으로 자율주행 테스트 지역이 확대될 것으로 전망되며, 관련 인프라 및 정책 변화에도 업계 주목이 집중됨

  배경 설명

     * Waymo는 Google의 모회사 Alphabet 산하 자회사로, 세계적인 자율주행 기술 개발을 선도함
     * 최근 들어 도심 환경에서의 자율주행, 라스트마일 배송, 로보택시 상용화에 중점을 두고 서비스 확장 중임
     * 뉴욕시 진출은 기술 실증과 비즈니스 확대를 가속화하는 결정적 단계로 해석됨

        Hacker News 의견

     * 제가 사는 지역은 Waymo가 자율주행차 시험 및 훈련을 활발히 하는 곳임, 이들의 차량은 대부분의 인간 운전자보다 일관적으로 더 안전하고 나은 운전 습관을 보임, 자율 또는 준자율 차량이 표준이 되는 시대가 오면, 운전 면허 취득 조건이 더 어려워져서 위험한 인간 운전자를 도로에서 줄일 수 있기를 바람
          + 인간의 운전 미숙은 서로 부분적으로만 연관이 있지만, 자율주행 시스템은 전체가 동시에 같은 방식으로 심각하게 오작동할 수 있다는 점이 걱정임, 예를 들어 전체 차량이 한 번에 같은 이상한 행동을 하면 위험함
          + 정말로 위험한 운전자들은 자동차 조작 능력이 부족하다기보다 도로에서 법규나 안전을 완전히 무시하는 식임, 이들은 운전 시험 때만큼은 그럭저럭 멀쩡하게 운전할 수 있을 것 같음, 오히려 면허를 따는 것을 어렵게 하는 것보다 면허 상실 기준을 더 쉽게 하고 엄격하게 집행하는 편이 훨씬 효과적일 것임, 한편 음주 또는 문자메시지 운전 등도 대부분 운전 시험 때는 안 하지만 실제로는 너무 흔함
          + 미국의 교통 단속은 많은 지역에서 거의 사라진 수준임, 이에 대한 장기적인 추세가 있었음, 경찰 친구 얘기에 따르면 주 정부 보조금이 들어오는 시기에만 횡단보도에서 일부러 보행자가 건너는 척 한 뒤 단속하는 'decoy operation'을 하곤 했음, 이런 예산 지원은 연방정부 차원에서 이뤄짐 정부 예산 지원, 주중이 아니면 교통 단속은 거의 없고, 경미한 위반이나 과속은 웬만해서는 티켓도 안 끊음, 뉴욕 예전에는 톨게이트 시간으로 평균 과속 단속을 하기도 했는데 이제는 폐지됨, 최근에는 뉴욕주 업무 구간에 속도 카메라가 설치되어 효과를 보고 있지만, 장비 설치는 여전히 인력이 운전해 가지고 다녀야 하는 한계 있음 설치 위치
          + 현재 가장 큰 문제는 이미 도로에 있는 나쁜 운전자임, 고령자를 대상으로만 재시험할 게 아니라 일반 시민 전체를 일정 주기로 재시험해야 사회적으로 큰 이득이 있을 것임
          + 실제로 면허 취득이나 유지가 어려워져도 단속과 집행이 약하면 의미 없음, 2020년 시위 이후로 교통 단속은 더 줄었고, 무면허 운전자도 차량 견인 없이 단순 벌금형 처리하는 경우도 많음
     * 저는 Bay Area에 살면서 가끔 샌프란시스코에서 Waymo를 타는데 항상 좋은 기억임, 최근 뉴욕에 갔다가 교통 체증이 너무 심해서 Waymo 생각이 절로 나더라구요, 만약 Waymo가 뉴욕에 진출하면 황색 신호에 진입하지 않고 멈춰있으면 다른 차량이 다 짜증낼 것 같음
          + 평소에 한 시장 후보가 ""운전자용 깨진 유리창 이론""을 내세우는 걸 딱 한 번이라도 듣고 싶음, 정당한 이유 없이 경적을 울리거나, 교차로를 막거나, 정지 신호를 그냥 지나치면 바로 티켓, 반복하면 자동차 견인하는 식으로 말임, 이제는 로봇 운전자를 닮아 더 예의 바른 운전 문화로 변화할 수도 있겠다는 생각임
          + 작년에 아내와 SF 여행 중 Waymo 보는 게 신기해서, 아내가 발목이 안 좋아 많이 걷기 힘들어서 시내 7블록 거리를 Waymo로 한번 타 봤음, 정말 SF 영화처럼 미래를 경험하는 기분이었음, Tesla의 FSD도 써봤지만 Waymo처럼 완전히 자율주행하는 환경은 차원이 다르다고 느낌
          + 지난 주말에 SF에서 Waymo를 타고 Richmond에서 SOMA로 이동했는데, 신호가 황색 두 번일 때 가속해서 통과하는 걸 보고 놀랐음, 평소 내 운전 스타일과 같아서 오히려 적절할 땐 과감하게 운전한다는 걸 알게 됨
          + Waymo는 차량 주변에서 일어나는 교통 법규 위반 상황을 카메라, 라이더 등 센서로 감지해서 사진·영상을 남길 수 있음, 만약 경찰이 이런 증거 기반 신고를 받아주기 시작하면 미래에 Waymo가 위반자를 자동 신고하는 날도 올 수 있을 것 같음
          + 2주 전에 SF에서 두 번째로 Waymo를 탔는데, 앞에 큰 버스가 평행 주차하려고 후진 중 Waymo와 눈치싸움이 있었음, 버스는 Waymo가 비켜주길 바라고, Waymo는 버스가 먼저 움직이길 기다리다가 컨트롤러 지원 요청을 했는데, 몇 초 만에 상담원이 즉시 Waymo를 다른 차선으로 안내해줘서 놀랐음, 지원은 빠르지만 아직 해결하지 못한 상황도 남아있음
     * Waymo가 다른 도시에 진출할 때마다 공통적으로 ""과연 우리 도시는 다를 텐데 X, Y, Z는 어떻게 처리할까? 로봇의 한계를 보여주마!"" 같은 반응이 많이 보임, 물론 뉴욕은 미국에서 가장 큰 도시라 이런 반응이 조금은 더 있을 수 있겠지만, 댈러스나 보스턴에서 그런 반응이 나오는 건 좀 과한 것 같음
          + Waymo가 이미 LA, SF에서도 충분히 잘 동작하고 있으니, NYC에서도 문제없을 거라 생각함, 뉴욕의 바둑판 도로는 캘리포니아의 언덕, 굴곡, 골목길, 시야 사각지대에 비하면 오히려 쉽고, 뉴욕에서의 진짜 도전은 겨울철 눈과 도로 결빙임
          + 뉴욕은 보스턴과 달리 규칙적이고 체계적으로 설계된 도시지만, 동시에 한 지점에 도로가 3~4층으로 겹치는 곳(예: 맨해튼 브리지 인근)도 있어서 GPS 오차 문제라든지, 도심 빌딩 숲의 GPS 블랙아웃 구간도 있음
          + 뉴욕에서는 세계적으로 유례 없이 보행자 수가 많은 교차로가 많아서 Waymo가 현재처럼 모든 움직이는 대상을 추적하는 전략이 과연 유지될 수 있을까 궁금함, 아마 보행자 수가 너무 많으면 우선순위 알고리즘을 따로 개선해야 할 듯, 특히 일부 구간에서는 센서로도 모두 개별 추적이 불가능할 정도로 사람들이 많음
          + 내가 가 본 뉴욕 도로는 구조는 오히려 단순함, 오히려 운전자들이 어렵지 :) 보스턴의 복잡한 도로망에도 도전하는 걸 언젠가 보고 싶음, 예전 GPS가 좌회전하라는데 세 갈래가 다 좌측 느낌이어서 정말 헷갈렸음, 그래도 Waymo의 도시별 확장 전략이 워낙 탄탄해 보여서 기대중임
          + 텍사스는 지역 도로들이 예전 마차길에서 이어진 경우가 많아서 도로망이 제멋대로임, 그런 면에서 NYC식 바둑판 시스템이 Waymo 론칭에는 맞을 듯, 다만 뉴욕 운전자들이 자율주행차를 싫어할 것 같고, 심지어 기물 파손(반달리즘)도 꽤 일어날 것 같음
     * 장기적으로 보면, Waymo 같은 자율주행 차량이 늘어나면 NYC 운전자들이 보행자·자전거 등 도로 사용자들에게 훨씬 덜 위험해질 것임, 다만 도시 내 차량 사용을 줄이거나 걷기 좋은 “low traffic neighborhood” 정책 측면에서는, Waymo나 유사 기업이 오히려 반대쪽에 설 것 같음, 참고로 자율주행차가 충분히 많아지면 다른 운전자들도 따라 진정되거나 속도가 느려지는 효과가 있어서 교통 안전엔 긍정적인 영향도 있을 수 있음(관련 연구 존재)
          + 사람들이 자가용을 포기하면 오히려 주말이나 가끔 사용할 때 Waymo 서비스가 더 자주 필요해질 테니, Waymo 입장에서는 NYC의 차량 감소가 그리 나쁜 건 아님, 현재 뉴욕에서 일상적으로 택시 타는 사람은 많지 않고, 자가용을 이미 보유하면 굳이 택시/Waymo 안 타고 오히려 차량 이용 기회만 더 찾게 됨
          + 믿기 어렵겠지만, 실제로 NYC는 미국 전체에서 보행자와 자전거 운전자가 가장 안전한 도시임 참고 통계
          + “보행 친화 도시화”가 목표라면, 위험하고 소음 많고 오염 유발하는 차량 대신에 안전·조용·청결한 자율차로 대체하는 것이 가장 강력한 보행 친화 정책임, 참고로 “low traffic neighborhood”가 정확히 어떤 개념인지, 버스/택배 차량 등 통행 허용 범위가 궁금함
          + 자율주행차만 늘어나면 도로를 차지하는 건 이젠 공격적인 자전거 유저들이 될 거라는 농담
          + “뉴욕에서 차량 퇴출”은 현실적으로 우리 세대에서는 불가능하고, 나이가 들어 걸을 힘이 약해질수록 오히려 그렇게 되지 않길 바램
     * Waymo는 자율주행 시스템이 교착 상태(서로 양보를 안 해서 길이 막히는 상황)를 만들 수 있기 때문에, 이럴 때 약간의 과감함(Assertiveness)을 추가로 넣어야 한다고 봄, 오스틴에서 Waymo가 반대 방향 차선으로 합류(merge)하려다 3개 차로를 막고 교통을 지체시키는 걸 직접 겪었음, 내 덕분에 출퇴근 시간대 도심 교통이 막히는 기분이었고, 창문이 선팅되어 있어서 사람들이 녹화하는 게 조금 덜 부끄러웠음, 스스로 적당히 좀 더 나아갔으면 좋겠음
          + ""반대 방향 차선으로 merge""가 구체적으로 어떤 상황인지 궁금함
          + Waymo가 실제로 점점 더 인간적인 과감함을 실험 중이라는 기사도 있음 관련 기사
          + Waymo가 늘어나면 인간 운전자들이 Waymo의 소극적인 주행을 이용해 우위를 점하려는 사례들이 더 나오리라 봄, 해결책은 도로 규칙 개선이나 Waymo만을 위한 전용 차선 신설 등이 있을 수 있음
          + 몇 주 전 구글 본사 인근 완벽한 교차로에서 Waymo와 좌회전 상황에서, Waymo의 ""과감함""이 너무 심해서 우리 차선으로 돌진하려 했던 적이 있음, 아내의 반사 신경 덕분에 정면 충돌을 피했는데, 자율 주행차의 과감함은 때론 오히려 위험해질 수 있음
          + LA에서는 사람들이 Waymo 앞을 끊임없이 끊어 타거나 끼어들기를 시도하는 걸 자주 봄, 왜냐하면 로봇 운전자들은 인간처럼 화내지도 않기 때문임, Waymo는 ""문화적 좌회전"" 즉 황색 신호에 여러 대가 연달아 좌회전하는 것엔 잘 대응하지 못하고, 이로 인해 교차로에서 멈춰 있는 모습도 흔함, 그리고 Waymo는 험준한 지역 등 복잡한 환경에선 서비스 커버리지 자체를 제외해서 도전 자체를 피하는 편임
     * Waymo 차량은 제도적 법 적용 문제를 불러일으킴, 인간 운전자는 사고에 과실이 있어 감옥에 갈 수도 있는데 자율차 사고에선 책임 주체가 법적으로 다르게 다뤄짐, 자산가(차량 소유 기업)가 법적 카테고리에서 벗어나게 되는 건 법 체계 근본을 해칠 수 있음
     * 자율주행 차량이 보행자 통행이 많은 이면도로에서 어떻게 좌회전을 할지 궁금함, 뉴욕에서는 신호 위반, 보행자 공간 침범 등 법을 약간 어기지 않으면 이런 게 거의 불가능한 상황임
     * 샌프란시스코에서 Waymo 탈 때마다 진짜 미래에 와있는 기분임
          + Waymo에서 가장 기대되는 건 미리 요금이 확실히 정해져 있다는 점임, 예전 Uber도 팁(티팟) 시스템 생기기 전까진 그랬는데, 이제는 추가 요금 걱정 없이 예측 가능한 요금만으로 이용할 수 있어서 좋음
     * 자율주행차가 공격적 운전자의 표적이 될지 궁금함, 예를 들어 Waymo는 로드레이지도 없고 경적도 안 울리니까 막무가내로 끼어들거나 바짝 붙어오는 일이 많아질 수 있음, 어떤 지역에서는 Waymo들이 심하게 괴롭힘을 당할 수 있을 듯함
          + 무인 차량을 바짝 따라붙는 것도 별 이유가 없지 않나 싶은데, 원래 뒤차가 앞차를 협박하거나 빨리 가라고 할 때 쓰는 거라 의미 없이 보임
          + 반면 어떤 사람들은 타인의 반응을 유발하는 것이 목적이라, 로봇은 그런 반응이 없으니 흥미를 잃을 수도 있음
          + Waymo 등은 온갖 각도에서 카메라 녹화 중이라, 누가 공격 운전하면 오히려 증거만 남음, 오히려 다수의 Waymo 카메라 덕분에 도로에서 교통 법규 위반 줄어드는 사회적 압박 효과도 기대됨
          + 피드백이 없으면 결국 재미를 못 느껴 포기할 수도 있음
          + 사실 대부분 운전자들은 주변 차량에 별 관심 자체가 없고, Waymo가 괴롭힘 받으면 녹화해서 신고하는 방법도 열려 있음
     * 잘 되길 바라지만, 실제로 맨해튼을 돌아다니기에는 지하철+도보 조합이 훨씬 빠르고 합리적임, 최근 여행 때 Uber나 택시는 너무 느리다고 느낌
"
"https://news.hada.io/topic?id=22667","AI 버블과 작별인사하고, 다가오는 붕괴에 대비하세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AI 버블과 작별인사하고, 다가오는 붕괴에 대비하세요

     * OpenAI가 출시한 GPT-5는 기대와 달리 전작보다도 사용성·성능이 떨어진다는 혹평을 받으며, AI의 끝없는 발전 신화에 제동을 걸었음
     * 막대한 투자에도 불구하고 AI 기업 대부분은 아직 수익을 내지 못했으며, 주식시장 과열은 1990년대 닷컴 버블을 떠올리게 함
     * GPT-5의 사례는 “스케일링=AGI 진화”라는 AI 업계의 핵심 전제를 무너뜨리며, 향후 수조 달러 규모의 데이터센터·칩 투자가 낭비가 될 위험을 드러냄
     * 전문가들은 AI를 지능으로 오인하게 만드는 마케팅 과장, “환각”이라는 용어 남용, 그리고 실제 생산성 저하를 문제로 지적함
     * 결국 AI는 과학적 용어가 아닌 마케팅 용어에 불과하며, 소수 기업만 이익을 독점하고 대다수는 비용을 치르게 될 수 있다는 경고가 제기됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: AI 기대의 급격한 냉각

     * 대다수 일반인들은 인공지능이 인간을 넘어설 것이라는 기대감이 2025년 8월 7일을 기점으로 급격히 식음
     * 바로 이날 OpenAI가 출시한 GPT-5가 혁신 대신 사용자 경험과 기능 모두에서 실망만을 남김
     * GPT-5는 수학 능력 저하, 잦은 오류, 불친절한 인터페이스 등 기존 제품보다 낮은 평가를 받음

AI 산업의 거품과 투자

     * “AI 기업들은 지금 미국 경제를 지탱하는 핵심축이지만, 매우 거품처럼 부풀려진 모습임”이라고 전문가 Alex Hanna가 지적함
     * Google, Amazon, Microsoft 등 대형 기업들이 OpenAI 및 AI 랩에 수천억 달러를 투자했으나, 아직 실질적 수익은 없음
     * 기업들은 주가 부양을 목적으로 AI 투자 또는 AI 기반 제품임을 강조하는 현상이 두드러짐
     * Nvidia가 과거 Intel과 비슷하게 주식시장 리더 역할 수행 중

GPT-5 출시와 신뢰 붕괴

     * GPT-5 출시 이후 사용자가 미국 지도를 잘못 그리는 등 많은 오류를 경험함
          + 예시: “Tonnessee”, “Mississipo”, “West Wigina” 등 실존하지 않는 주명 생성
          + 미국 대통령 열두 명을 묻는 요청에는 아홉 명만 제시하며 오기까지 범함
     * 커뮤니티 반응은 “기존 모델보다 못하다”, “짧고 불친절한 응답”, “선호 모델 선택권 박탈” 등으로 극도로 부정적임
     * 언론에서 Futurism은 ‘조금 시시하다’, Ars Technica는 ‘대형 실패’로 평가함
     * OpenAI도 부정적 여론을 빠르게 인지하여 이전 버전 접근권 회복 조치함

AI 확장 신화의 붕괴

     * GPT-5 발표로 AI 업계의 ‘스케일업(확장) ’ 신화가 깨짐
          + 더 많은 컴퓨팅 파워와 데이터를 투입하면 인공지능이 근본적으로 진화할 것이라는 믿음에 의문 제기
     * AI 확장 신화를 믿고 데이터센터와 고성능 칩에 막대한 투자(2028년까지 약 3조 달러 필요 예상)
          + 만약 기술 진화가 더뎌진다면 투자금 대부분이 낭비 위험에 직면함

인공지능이란 무엇인가: 의미의 혼란

     * AI가 일관된 언어 생성 능력과 ‘지능’은 다름
          + AI 챗봇이 “지능적”으로 보이는 것은 마치 사람의 의도와 사고가 존재하는 것처럼 착각하게 만듦
     * 1960년대 Joseph Weizenbaum의 ELIZA 사례처럼, 단순한 챗봇도 인간이 기계에 감정이나 의지를 투사하게 만드는 현상을 이미 지적함
          + 이러한 ‘인간화’ 경향은 현재 AI 홍보 때 적극적으로 활용됨

AI 환각, 기대와 현실

     * AI의 ‘환각(hallucination) ’ 현상은 실제로 기계가 지각 능력을 가진 것처럼 오해받게 만듦
          + 사실상 인공지능은 지각이나 인식 능력이 없음
     * AI 도입이 대규모 일자리 상실과 생산성 급증을 불러올 것이라는 예측도 아직 실현되지 않음
     * 오히려 생산성 감소 및 AI 산출물의 검증 필요성으로 업무 효율 저하 경향이 나타남

경제적 효과 및 전망

     * MIT 교수 Daron Acemoglu는 AI로 인한 생산성 증가율을 0.5% 내외로 예측, 실제 기대보다 저조한 수치임
     * AI 경제 효과 예측이 과장되었고, 혁신 수혜도 소수에게 편중될 것으로 분석됨

결론: AI 담론이 가지는 진실

     * GPT-5의 실패는 AI가 과학적 진보가 아닌 마케팅 허상에 기댄 버블일 수 있음을 보여줌
     * “AI가 의식과 지능을 가졌다”는 주장은 판매를 위한 수사일 뿐이며, 이익은 소수 기업에게만 돌아가고 대다수는 비용을 감당하게 될 수 있음
     * 따라서 AI의 실제 한계를 직시하고, 무엇을 할 수 있으며 할 수 없는지를 냉정하게 바라보는 인식 전환이 필요함

   대다수 일반인들은 인공지능이 인간을 넘어설 것이라는 기대감이 2024년 8월 7일을 기점으로 급격히 식음

   년도가 아마 2025년을 말씀하시려던거겠죠? 1년 전부터 그래왔던가 생각하다가 뒤이어 GPT-5 내용이 나와서 2025년으로 이해했습니다.

   원문에는 년도없이 8월 7일이라고 되어있네요. 아마 실수하신 듯 합니다

   AI 요약이라 컷아웃 년도 기준으로 한 거 같네요. 수정해두었습니다.

   그렇군요 👍

        Hacker News 의견

     * 이런 기사는 너무 편향적이라고 느끼는 입장임, 처음부터 ""The AI con""이라는 책 쓴 사람의 인용문을 넣었다는 자체가 치우침의 증거라고 봄, Deepseek r1이 NVDA 죽일 거라는 6개월 전 담론이 떠오름, 누군가 의도를 가지고 이런 흐름을 만들고 기자가 그대로 속은 느낌임, GPT-5는 이미 예전부터 여러 모델을 연결해서 쓰는 구조로 발표했고 딱 그대로 구현했다는 점이 눈에 띔, 기존 상위 모델 대비 4~6배 저렴한 가격에 비슷한 성능, 정말 대단한 변화임, 특히 gpt5-mini는 agentic coding에서 가격 대비 미친 성능을 보임, 0.x달러로 세션을 돌려서 Claude 3.5/3.7로는 못 했던 것도 가능해짐, RL 쪽 개선 효과가 뚜렷하게 느껴짐
          + GPT-5가 여러 모델 조합이라는 건 원래 그렇지 않았음, Altman이 예전에 엄청난 부스트를 걸었던 거 듣거나 읽어본 적 있는지 묻고 싶음, GPT-4.1 실패 이후에 설명을 슬쩍 바꾼 거임, 성능과 가격 얘기는 Deepseek 논문 내용 실현한 거 아닐까 의심 듦
          + 이건 논설(Opinion)임, 원래 편향적임
          + 만약 GPT-5 극찬 기사 써놓고 첫 인용구가 Sam Altman 거라면 ""좋은 기사네""라고 할 거 아님? 결국 편향은 불편할 때만 문제 삼는 듯함
          + ""작년에 이미 발표된 정책""이라는 주장에 근거가 필요한 상황임, 반대 의견이 많이 보임, AI 업계는 실제로 비약적인 발전보다 골포스트를 옮기기만 하는 경우가 더 많다고 확신함
          + 이건 뉴스가 아니라 논설임, 예전엔 논설과 뉴스 구분 못하는 사람 많지 않았는데 요즘 구분하는 사람이 줄어듦, 나는 동의하지 않는 논설을 악마 옹호론 관점으로 읽으면서 반박 논리나 내 도메인 지식으로 대응하는 연습에 활용함, 이번엔 ChatGPT의 실제 효용이 투과되어 hype만큼은 아니라고 지적하는 게 핵심인데 ""AI 버블 붕괴 시작""까지 연결 짓는 건 너무 앞서간다고 느낌, 실망의 골은 언젠가 오겠지만 지금은 아니라고 생각함
     * Facebook이 루이지애나 북부에 짓는 데이터센터 때문에 Entergy가 짓는 거대 발전소의 전기 요금을 Meta가 혹시 안 내버릴 경우 내 전기요금이 어떻게 될지 기대 반 걱정 반임 기사 링크
          + Louisiana 공공 규제 회의에 참석해서 들었는데, Entergy가 ""Meta Platforms 자산은 2조 달러라서 뉴욕 로펌 의견도 받았으니 그냥 믿는다""라며 사업을 강행했음, 4:1로 통과됨, 심지어 ""Meta Platforms가 미국 5대 은행보다 시총이 높으니 은행 보증은 필요없다""는 발언까지 들었음
     * AI 버블 붕괴 이후 업계 리더라는 사람들이, 인텔리전스를 단순 데이터베이스 기능+확률+통계 정도로 생각했다는 것에 속은 이유를 설명하느라 진땀 흘릴 날이 올 것임
          + 이 hype는 대량 채용 후 대량 해고에 대한 핑계로 만든 거라는 확신을 지니고 있음
          + 닷컴 버블 때 인터넷이 사라지지 않은 것처럼, AI 버블 붕괴가 온다 해도 각종 비즈니스에서 AI의 이득을 발견하는 중이니 계속 개발이 이어짐을 예측함
          + ""이건 잘못된 비판임"", ""행렬 곱셈일 뿐""이라고 치부할 수 있지만, 양자역학이나 화학, 생명과학까지 인류 문명의 근간도 결국 선형대수에 기반했는데, AI도 그냥 그 위에 얹힌 문명 구성 요소 중 하나임을 인정해야 함
          + 사람이 직접 책임지는 문화가 없는 미국식 비즈니스에선 아무도 진짜로 책임지지 않음, 주식회사 주주질문조차 ""어떻게 이렇게 이겼어요?"" 수준의 자기자랑이 됨, Hadoop, 블록체인, 노코드, 이전 AI 붐 등등 다 뻔한 hype였는데도 사과하거나 책임지는 모습 안 보임
     * Altman이 왜 이렇게 이번 릴리스를 과대포장했는지 이해가 안 감, 그리고 저 수상한 Star Wars 사진들은 또 뭐였는지 궁금함
          + 초반엔 정치인들이 언급만 해도 반응이 꽤 컸음, 2024년 2월에 칩과 AI 분야에 수조 투자 요청도 있었고 기사도 있음, 그 뒤로 White House 트럼프-소프트뱅크까지 $500B Stargate 프로젝트 발표했지만 실제론 데이터센터 하나만 지어졌음, 초반 성공에 힘입어 hype 다시 먹힐 거라 생각했지만 이제는 실제 AI 능력을 더 면밀히 보는 분위기임
          + 이제 Altman이 장기전을 준비하는 느낌임, 닷컴 버블 시절 Amazon, Google처럼 OpenAI를 끝까지 살아남는 메이저로 만들려는 전략을 쓰고 있다고 생각함, 실제로 몇몇 다른 기업들도 hype의 정점이 다가오고 있다는 걸 눈치채고 생존 전략을 세우고 있다고 봄
     * 닷컴 버블이 현 AI 상황과 유사하다는 비유가 와닿음, Nvidia가 그때 Cisco 같은 역할을 함, Cisco는 거품 정점에 세계에서 가장 가치 높은 기업이었지만 2년만에 90%나 하락했음, 다 쓸모없는 광섬유(설치만 하고 미사용, dark fiber)도 생김, 이번에 OpenAI와 소규모 AI 스타트업 대부분 망할 것이고, Microsoft, Google, Meta는 사업부 축소 후 손실 정리하면서도 R&D는 멈추지 않을 것임, 바람직하게는 거품 종료 후 클라우드 컴퓨팅 자원이 남아 저렴하게 푼다면, 신규 스타트업들이 아이디어로 재활용할 수 있는 인프라가 될 것임, 투자자들은 포트폴리오 손실에 허덕이고 암호화폐 시장까지 같이 흔들릴 것이라 전망함
     * AI 거품임이 더 가능성 높다고 나는 결론 내림, S&P 섹터별 ETF 매수를 고민한 적 있음, 기존 S&P 인덱스가 기술주 비중이 높은 게 무서워 분산하려 했는데, 실제로는 수수료가 너무 높아서(0.39% 정도) 망설이고 있음
     * “외부 회의에서 결론난 단 하나의 사실은 ‘모트(진입장벽)가 없다’는 점임"", 저자도 맞는 얘기할 수 있지만, 실질 문제는 공급과잉임을 강조함
     * 이전 글에서 본 인상 깊은 말이 있음, ""AI가 성공하는 건 딱히 다른 재밌는 게 없어서""라는 논지였음, 다른 흥미 요소가 등장해야 AI 거품이 터지지 않을까 생각함
          + 한편으로 Claude가 실제로 내 로봇 만들기에 하드웨어 및 소프트웨어 개발을 돕고 있음, 물론 혼자서도 할 수 있겠지만 이렇게 빨리, 일상 틈틈이 할 수 있는 건 AI 덕분임
          + 블록체인도 과거엔 비슷한 상황이었을 수 있다는 점을 지적하고 싶음, 다른 흥미거리에 붐이 교체된 사례로 볼 수 있음
     * 미국 정부가 시중 통화량 줄이거나 환율이나 자산을 축소할 것이라 기대하는 사람 있음? 그런 게 아니라면 거품은 계속됨, 아니더라도 그때그때 거품 대상만 바뀔 뿐임

   LLM은 인공지능이 아니다
    LLM은 지능이 아니라는 건 인공지능 공부를 학부에서만 해도 알 사안인데 이걸 고졸 유대인 따위의 마케팅 Stunt에 넘어간 게 문제
    현재의 LLM업계에서 벌어지는 황당한 일은 수소트럭으로 사기 친 니콜라를 떠올리게 한다.
    수소연료전지를 만들지 못하고 수소전기트럭이라고 껍데기만 만든 업체가 사기치다 망한 것이나 지능을 못 만들고 LLM챗봇이나 만들고서 이게 지능이라고 두럅다며 쇼하는 고졸 유대인이나 개낀도낀 수준
    미국은 핵심 기술을 어떤 분야에서도 이제 제대로 확보할 역량은 없고 SBS를 통한 마케팅만으로 돈을 끌어 들이는 메도프 수준의 폰지 사기아니면 존재 이유가 없는 나라로 추락.
    향후 돈되는 AI분야는 절대로 Computer Vision AI이고 이게 미래 전쟁과 군사력을 좌지우지 할 것
    그런데 Computer Vision AI분야는 중공이 세계 1위로 급부상하며 패권 국가로 성장중.
    미국에서처럼 LLM위주의 거대 자본투자가 버블로 폭발하면 이제 미국은 AI분야 주도권을 중공에 상실할 가능성이 아주 큼.
    서구 사회에 다수를 차지하는 Wordcel들에 의한 현재 LLM투자는 지금 벽에 부딪치고 있는 중이고 이에 대한 공과는 철저하게 따지는 자본가들에 의해 난도질당 할 것이며 Computer Vision AI 등 관련 분야로보 파급이 되어 상당기간 AI산업이 위축될 가능성이 큼.
    이에 비해 국가가 연구개발을 추진하는 중공은 이런 미국발 AI버블에서 비교적 안전할 가능성이 큼.
    따라서 이번 LLM과잉투자는 미국을 패권 국가에서 이등국가로 고착시키는 계기가 될 가능성이 큼

   LLM의 유행이 과하다는 주장은 알겠고 동작원리가 연역,귀납과 같은 추론이 아니라는 것은 공감하는데요. 인공지능과 지능은 동의어가 아니기도 하고 그걸 동일시하거나 의인화하는 사람들이 문제이지 않나요?

   어째서 항상 이런 댓글은 깡통계정이 쓰는것인가

   깡통계정이 아니라 유태인 빠는 글에 열 받아서 급하게 계정만들고 댓글 단거란 생각은 안드냐?

   그리고, LLM은 인공지능이 아니라면, 그게 뭔지는 제시를 해주셔야죠. 학부 공부만 해도 안다면서요?

   Eliza with bigger Dataset

   Good!

   그럼 저 기업들이 경험이 없어서 몇백억 들여서 인재 스카웃하겠나요. 님이 아는 최고의 지식은 그분들에겐 쓸데없는 지식일듯.

   https://www.yna.co.kr/view/AKR20250821122900009

   llm이 인공지능이 아니라고 하시는거부터 잘못 알고계신게 많은듯
   본인만의 세상에 빠져있는것 같아영

   1960년대 Joseph Weizenbaum의 ELIZA 사례처럼, 단순한 챗봇도 인간이 기계에 감정이나 의지를 투사하게 만드는 현상을 이미 지적함
   이러한 ‘인간화’ 경향은 현재 AI 홍보 때 적극적으로 활용됨
"
"https://news.hada.io/topic?id=22654","Brush - Rust로 구현한 POSIX/Bash 호환 쉘","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Brush - Rust로 구현한 POSIX/Bash 호환 쉘

     * B(o)rn(e) RUsty SHell - Rust로 작성되어 C 기반 Bash보다 안전성과 유지보수성이 높은 POSIX- 및 Bash 호환 쉘
     * Linux, macOS, WSL에서 동작하며, Windows 지원은 실험적 단계
     * 대부분의 sh/bash 스크립트를 실행할 수 있고 일상적인 인터랙티브 사용이 가능
     * .bashrc를 그대로 처리하며, 구분을 원할 경우 ~/.brushrc 이용
     * Rust 생태계의 Crate(tokio, clap, fancy-regex, criterion.rs) 등을 직접 활용 가능 → 복잡한 기능을 안정적으로 구현
          + 비동기 처리, 프로파일링, 벤치마킹 가능 → 대규모 스크립트 실행 시 효율성 기대
     * 675개 이상의 테스트 케이스로 Bash 등 기존 쉘과의 동작 비교 검증 → 신뢰도 높은 호환성 보장
     * Cargo, Nix, Homebrew, Arch 리포지토리 등 다양한 배포 채널 지원으로 설치와 관리가 간편
     * MIT 라이선스
"
"https://news.hada.io/topic?id=22687","QUIC 기반 미디어 전송 MoQ CDN, Cloudflare에서 첫 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               QUIC 기반 미디어 전송 MoQ CDN, Cloudflare에서 첫 출시

     * Cloudflare가 최초로 Media over QUIC(MoQ) 기반 CDN을 공식적으로 공개함
     * MoQ는 실시간 미디어 전송을 위한 새로운 표준으로, WebRTC, HLS/DASH, RTMP/SRT를 대체할 것으로 기대됨
     * 현재는 개발자 프리뷰 단계이며, Cloudflare의 퍼블릭 엔드포인트를 통해 여러 클라이언트 및 라이브러리로 테스트 가능함
     * 실시간 방송 송출과 시청, AI 기반 자막 등의 시범 기능들이 웹 및 Rust 클라이언리에서 제공됨
     * 아직 인증, Safari 지원, ANNOUNCE 등 주요 기능이 미구현 상태이며, 관심 있는 개발자는 직접 모큐 릴레이를 운영할 수도 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cloudflare, 최초의 MoQ CDN 공식 출시

  소개

     * Cloudflare가 Media over QUIC(MoQ) 표준 기반 CDN을 공식 출시함에 따라, 실시간 미디어 전송 분야에서 대규모 변화를 예고함
     * MoQ는 실시간 동영상, 음성 등 라이브 미디어 데이터 전송에서 기존 WebRTC, HLS/DASH, RTMP/SRT 프로토콜을 모두 대체하는 차세대 표준으로 주목받음
     * 이번 출시는 공식 제품 형태로, 전 세계 Anycast 네트워크 상에서 실제 사용자가 직접 테스트 가능함
     * Cloudflare는 최초의 MoQ CDN 사업자가 되었으며, 해당 기술은 실시간 미디어 전송 생태계에 혁신을 촉진할 것으로 예상됨

  현재 제공되는 기능

     * 본 기술은 프리뷰 버전으로, 서비스 안정성과 기능 범위가 제한적임
     * Cloudflare는 relay.cloudflare.mediaoverquic.com이라는 공개 엔드포인트를 개방함
     * 아래와 같은 다양한 오픈소스 라이브러리 및 클라이언트를 통해 테스트할 수 있음
          + kixelated/moq
          + englishm/moq-rs
          + meetecho/imquic
          + facebookexperimental/moxygen
     * 웹 데모와 라이브러리를 사용해 브라우저 내에서 실시간 방송 송출 및 시청이 가능함
     * AI 기반 자막 처리 기능 시범 적용
          + 브라우저 내에서 [silero-vad], [whisper], [transformers.js], [onnxruntime-web], [WebGPU] 등 기술로 자막 생성 및 전송
     * Web Component 방식 API뿐 아니라, JavaScript API를 통한 고급 활용도 지원됨
     * Rust 라이브러리를 통한 MP4 임포트, ffmpeg 연동, gstreamer 기반의 방송 송출 및 시청 등 자바스크립트 비선호층을 위한 환경도 마련됨

  미구현된 기능

     * 현재 제공 버전은 제한적 Draft-07 서브셋만을 지원함
     * 아직 제공되지 않는 주요 기능
          + 방송 인증 미지원: 각 방송마다 추측이 어려운 이름을 직접 지정해야 함
          + ANNOUNCE 미지원: 방송 시작/종료 탐지 기능 부재
          + Safari 브라우저 미지원: WebTransport 지원 이슈로 Safari 호환 불가
          + 최적화 미완료: 사용자 경험 등은 점진적으로 개선 예정임
     * 필요시, 직접 moq-relay 인스턴스를 구축해 고급 기능을 활용할 수 있음
          + JWT 기반 인증, Safari/TCP를 위한 WebSocket fallback 등 추가 기능 개발 중
          + terraform 모듈로 글로벌한 CDN 네트워크 구성도 가능함

  MoQ와 Cloudflare의 의미

     * MoQ 표준화 작업은 3년 이상 진행 중이며, 실제 전세계적 채택에 상당한 시간이 소요될 전망임
     * Cloudflare는 RFC 채택 전 실제 제품을 신속하게 출시함으로써, 개발자·이용자의 실질적인 피드백을 끌어내는 과감한 결정을 내림
     * MoQ 기술은 WebRTC/HLS/RTMP 등 기존 미디어 프로토콜을 대체하는 잠재력을 지님
     * 표준 초안 및 코드 이슈 논의는 계속되겠지만, 실제 운영 경험이 표준 발전에 크게 기여할 것으로 전망됨
     * 향후 MoQ 기반 미디어 전송 시장에서 Google, Akamai, Fastly 등도 자체 네트워크와 서버에 코드를 배포해 실질적 필요사항을 파악할 필요가 있음

  향후 계획과 커뮤니티

     * 앞으로 WebRTC 및 기존 프로토콜을 Web 기반 최신 API로 재구현하기 위한 많은 작업이 남아 있음
     * 현재 단계의 성능/기능만으로 MoQ 전체를 평가하지 않아야 하며, 적극적으로 테스트 및 피드백 참여 필요함
     * 커뮤니티(Discord)에 900명 이상이 활동 중이며, 질문 및 협력 제안 가능함

        Hacker News 의견

     * https://moq.dev/publish/에서 데모를 테스트해봤는데 정말 부드러운 느낌임. 아주 인상적임. 그리고 훌륭한 기술에 고마움을 느낌. https://moq.dev/watch/?name=bbb에서 Big Buck Bunny 데모를 모바일폰으로 보니 수평 검은 줄이 많이 생기는 현상이 있음 (이상하게도 같은 와이파이를 사용하는 PC에서는 정상임). 이게 버퍼 크기 때문인지 궁금함. 클라이언트 쪽에서 버퍼를 늘릴 수 있는지, 아니면 서버 쪽 설정이어야 하는지 질문하고 싶음. 그리고 ""글로벌"" CDN맵에서 South Korea를 빼먹지 않아 고마움을 느낌
          + 혹시 이것이 Chrome에서만 작동하는지 궁금함. Android Firefox에서는 지원되는 브라우저가 아니라는 메시지만 보임
          + 수평 검은 줄 현상에 대해 잘 모르겠음. <canvas> 요소로 렌더링해서 소스 영상 사이즈에 맞게 리사이즈하고, 다시 CSS로 윈도우 크기에 맞게 리사이즈함
          + OnePlus Ten에서 Chrome으로 재생하면 검은 줄이 자주 깜빡임. 특히 화면 위쪽에서 오른쪽 아래로 내려가는 모양이고, 혹시 화면 새로고침(refresh) 아티팩트(rolling shutter effect) 현상일지도 모르겠음
          + 페이지에 Rust 코드와 WASM이 많이 언급되어 있으므로, 혹시 폰 CPU가 WASM을 충분히 빠르게 실행하지 못해서 생기는 문제일 수 있음. 내 Samsung S20에서는 검은 줄이 전혀 안 보임
          + mac book air m4와 600mbps 연결에서 테스트했는데 즉각적인 반응과 놀라움이 있었음
     * 이 글에는 '왜 신경 써야 하는가?'라는 섹션이 있지만, Media over QUIC이 미디어 발행자나 엔드 유저 — 즉 이 교환에서 가장 중요한 (아마도 유일한) 두 당사자 — 에 어떤 이득을 주는지 설명이 없음. 그래서 왜 내가 이 기술에 신경 써야 하는지 궁금함
          + 내 실수임. 이전 블로그 글들과 중복되는 내용을 피하려다 보니 설명이 부족했음: https://moq.dev/blog/replacing-webrtc/ 그리고 사실 MoQ는 주로 개발자들에게 이득을 주는 기술임. 확장성이나 기능 구현이 훨씬 쉬워지므로 간접적인 이득만 있음
          + 종단간(유리잔에서 유리잔으로) 지연 시간이 상당히 더 좋아짐. 주로 프로토콜이 이제 request/response 방식이 아니기 때문임
     * 안녕! Cloudflare MoQ 개발자임, 질문 환영함. 그리고 상 주셔서 감사함, kixelated xD
          + ""헤드 오브 라인 블로킹 없음: TCP와 달리 QUIC의 스트림은 독립적임. 하나의 스트림(예: 오디오 트랙)에서 패킷이 손실되어도 다른 트랙(예: 비디오 트랙)은 막히지 않음. RTMP에서 항상 발생하던 끊김 현상이 이걸로 사라짐"" 설명이 있었는데, 이런 구조라면 오디오 트랙에서 패킷 손실이 생겨도 비디오 트랙이 멈추지 않고 계속 재생된다면, 오디오와 비디오 싱크가 어긋날 수 있지 않을까 궁금함. 이 부분이 기술에 익숙하지 않아 이해가 부족할 수 있음
          + 질문이 있음. TCP와 QUIC의 goodput 격차를 줄이기 위한 구체적인 계획이 있는지 궁금함[1]. 연결된 논문에서는 HTTP/2(TCP)에서 HTTP/3(QUIC)로 넘어갈 때 최대 9.8%의 비디오 비트레이트 감소를 보았다고 함. 물론 MoQ는 스택 구조가 조금 다르기 때문에 직접적인 일반화는 어렵지만, 비슷한 문제가 있을 것 같음. (개인적으로 지난 몇 달 동안 MsQuic의 AF_XDP datapath를 석사 논문의 일환으로 조사함. 결론적으로는 GSO/GRO가 더 나은 대안이고, QUIC에는 하드웨어 오프로드가 확실히 더 필요하다는 걸 느낌 :p)
            [1]: https://arxiv.org/pdf/2310.09423
          + 몇 가지 궁금한 점이 있음 :)
            QUIC이 브라우저에서 실제로 사용 가능한 단계(브라우저와 인프라 모두 지원, '그냥 된다' 레벨)에 어느 정도 가까워졌는지 궁금함
            그리고 QUIC은 NAT 문제를 어떻게 해결하는지 궁금함. WebRTC는 full cone NAT를 통과하기 위해 STUN/TURN이 꼭 필요한데, 특히 TURN이 문제임. 인프라를 많이 돌려야 하니까
          + getstream.io에서 WebRTC 스트리밍 작업을 하고 있음. WebRTC는 설정이 다소 번거롭지만, 연결만 되면 낮은 지연 시간이 장점임. MoQ도 연결이 완료된 이후에는 어떤 장점이나 이슈가 있는지, 즉 WebRTC와 비교해서 어떻게 느끼는지 궁금함
          + 릴레이의 로드밸런싱이 MoQ의 범위에서 벗어나는지 궁금함. 글에서는 이 부분이 다뤄지지 않은 것 같음
     * 이 프로젝트를 너무 좋아하고, 가끔씩 kixelated 블로그도 읽고 Github에서 팔로우하고 있음.
       일단 이렇게 멋진 작업을 해준 kixelated와 Cloudflare 모두에게 축하를 전하고 싶음.
       실시간 라이브 스트리밍이 궁금함. 보통 라이브 스트리밍은 수백, 수천 명이 동시에 시청하는데, MoQ에서 멀티캐스트(multicast)를 사용하거나 구현할 생각이나 가능성이 있는지 알고 싶음. HTTP/1.1, HTTP/2에서는 TCP 기반이라 어려웠던 점이 있지만, HTTP/3에서 UDP를 쓰니 이제는 현실적인 아이디어라는 생각임. 생각이 어떠신지 궁금함. Akamai와 BBC에서도 이 부분 연구 중인 것으로 앎
          + 고마움!
            멀티캐스트가 필요 없음! CDN이 실제로는 멀티캐스트를 L7(애플리케이션 계층)에서 구현한 것이나 마찬가지임. 라우터나 ISP가 L3에서 이를 직접 구현해야 할 필요가 없음. 이게 바로 Twitch에서 5년간 내가 하던 일이었음
            이론상 멀티캐스트는 CDN 엣지에서 ISP로 가는 트래픽만 줄여주겠지만, 이건 1년에 한 번 있는 초대형 방송(예: Super Bowl)에서나 해당하고, 대다수 이벤트에는 효과가 없음. 많은 CDN들은 CDN 엣지를 이미 ISP 내부에 설치해서 이 문제를 해결했고, 작은 이벤트는 두 시청자가 동일한 경로를 공유할 확률이 낮아 이득이 없음
            멀티캐스트에는 혼잡 제어나 암호화 등 다른 이슈도 있는데, 이건 연합 구조의 멀티캐스트라 더 어려움
            멀티캐스트가 제일 혜택을 줄 곳은 P2P이지만, 거대한 CDN 생태계에서는 채택이 어려울 것 같음
            WebRTC도 멀티캐스트에 제일 잘 맞고 RTP(원래 멀티캐스트용 설계)도 쓰지만, 실제로 멀티캐스트 지원에 관심이 없는 상태임
            단, Google이 자사 네트워크 내 Meet 서비스에 멀티캐스트를 쓴다는 소문이 있으니 혹시 모르겠음
     * ""just announced"" 링크가 이게 뭔지 전혀 모르는 사람한테도 아주 좋음: https://blog.cloudflare.com/moq/ (나도 처음 놓침)
          + ""방송 수준에서 초저지연 (sub-second latency at broadcast scale)"" 하지만 실제 멀티캐스트는 없음
          + ""그냥 또 다른 프로토콜이 아니라, 새로운 설계 철학임""이라는 문구가 나와서 AI 감지 레이더가 반응함
     * MOQ가 실패 복구(페일백) 해상도와 점진적 품질 저하(그레이스풀 디그레이데이션)를 어떻게 다루는지 궁금함. 그리고 zed에서 전체 화면으로 해도 화면이 꽉 차지 않는 문제도 있음
     * Firefox 사용자이며 Cloudflare가 호스팅하고 HTTP/3을 쓰는 사이트를 방문했다면 참고하라는 의미로 공유하고 싶음:
       https://bugzilla.mozilla.org/show_bug.cgi?id=1979683
          + 해피 아이볼(happy eyeballs) 문제일 수 있을 것 같아 보임? 더 잘 아는 분들께 전달할 생각임
          + 혹시 브라우저 내 DNS over HTTPS(DoH) 해상기를 쓰고 있는지 알고 싶음. 나 개인적으로는 이 현상을 재현하지 못했음. 1.1.1.1로 DoH 사용하는 중임
            Chrome과 Firefox 모두 시스템 DNS 대신 DoH 해상기를 사용할 때 HTTP/3 사용이 더 일관적인 걸 느꼈음. 시스템 해상기를 쓸 때는 HTTPS DNS 레코드를 일관적으로(혹은 전혀) 못 가져오는 경우가 많고,
            HTTP/3 서버 지원은 HTTPS DNS 레코드 아니면 이전 HTTP/2/1.1의 캐시된 Alt-Svc 헤더 중 하나로 브라우저에 광고가 되어야 동작함. 이미 열려있는 연결을 재활용하는 경향이 있어 새 연결을 잘 안 띄움
            Alt-Svc 헤더도 특히 Firefox에서 일관적으로 캐시되지 않음
            상황을 더 복잡하게 만드는 건, 브라우저가(특히 Chrome) 접속 실패가 일정 수준 누적되면 아예 HTTP/3 지원을 꺼버림. 내가 대학 와이파이 쓸 때 UDP 트래픽을 많이(불규칙하게) 막아서 그런 현상이 있었고, 그 후 집에서도 HTTP/3이 안 됨. 해결법은 chrome://flags에 가서 QUIC 지원을 수동으로 켜는 것 뿐임. 기본적으로 ""enabled by default""로 보이더라도 실제 브라우저 상태는 다를 수 있음. Firefox도 유사하게 HTTP/3을 포기하지만, Chrome만큼 고집스럽지 않고 큰 문제는 없었음
            추가 디버깅을 원한다면 EncryptedClientHello(ECH)가 작동하는지 https://tls-ech.dev에서 확인해볼 것, 이건 ECH 키만을 위해 HTTPS 레코드만 사용하니 문제 격리가 됨.
            다음으론 Fastly의 HTTP/3 체크(https://http3.is)로 Alt-Svc 협상만 쓰는지 확인할 수 있음.
            Cloudflare 테스트 페이지(https://cloudflare-quic.com)는 HTTPS DNS 레코드와 Alt-Svc를 둘 다 쓰므로 바로 HTTP/3이 잡히면 HTTPS 레코드 해석이 잘 되는 것임
            테스트 결과 알려주시면 좋겠음. Firefox의 문제가 모두에게서 일관적이지 않은 건 이렇게 다양한 변수가 있어서임
            (Cloudflare 관계자에게 알리고 싶은 건, https://cloudflare-quic.com/favicon.ico가 뭔가 잘못 구성되어 있음. 그리고 https://www.cl..."">https://web.archive.org/web/20230424015350im_/… 이미지를 Wayback Machine에서 불러와 페이지 로드에 지연이 생기는데, images는 ""id_"" 링크를 써야 rewrite가 안 돼서 더 빠름. 예전에 Cloudflare Workers와 연계해서 서버 마이그레이션 실패 때 사이트를 임시로 복구시킬 수 있었음. 또는, 그냥 https://www.cloudflare.com/img/nav/globe-lang-select-dark.svg 원본을 쓰면 됨. 현재 사이트에서도 여전히 살아있는 파일임)
            최근 몇 년간 HTTP/3의 다양한 특이한 구현 및 배포 이슈를 가지고 실험을 엄청 많이 해봤음. 훌륭한 프로토콜임. 하지만 이상하고 특이한 구현 및 배포 이슈가 많음
          + 혹시 macOS 한정인지 궁금. Windows의 FF 141, 142에선 재현이 안 됨
     * 정말 멋짐! 수고에 감탄함. 처음에는 QUIC CDN으로 읽고 ""설마"" 싶었는데, Media over QUIC이 별개의 개념이라는 걸 자세히 알아보니 꽤 흥미로움
     * 첫 번째 앱이 흥미로웠음. Show HN에 제출해보면 좋겠다는 생각임. 라이브 비디오 플랫폼에서 일한 경험이 있는데, MoQ가 다시 한번 이 개발을 하고 싶을 정도로 충분히 흥미로움
          + 곧 제출할 예정임. 하지만 조금 더 고칠 부분을 손볼 시간도 필요함
     * ""기술이 내부적으로 동작한다면"" 주요 브라우저 지원만 되면 무관하다고 생각함 (caniuse에 정보도 안 보임).
       그리고 Microsoft Edge WebView2 같은 웹뷰 엔진도 지원된다면 바로 개발 적용이 가능하다고 느낌.
       하지만 반대로 OBS나 YouTube 같은 쪽에서 지원을 해야 실제로 의미가 있을 것 같음
          + 그래서... MoQ는 WebRTC 같은 하나의 큰 ""블랙박스"" 웹 API로부터 약간 벗어나는 노선을 걷고 있음. 브라우저 관점에선 WebTransport API 지원이 관건임.
            WebTransport와 함께 MoQT를 쓰면, 예를 들어 영상 플레이어로 WebCodecs 같이 새로운 여러 방식이 생김. 약간의 지연을 감수하면 MSE로 재생하면서 DRM도 적용할 수 있음
            그리고 OBS에서 직접 퍼블리시할 수 있게 하는 작업은 Cloudflare 오기 전에도 했었는데, 이는 ""스트리밍 포맷"" 계층에서 미디어 관련 세부 구현을 어떻게 하느냐에 상당히 달려 있음. 업계에는 아직 이 계층의 사양이 꾸준히 발전 중인데, 현재 WARP가 주도적인 스펙임. WARP 규격이 점점 정착되는 대로 OBS 등에도 이 기능이 내장될 수 있음. 현재도 Norsk(https://norsk.video/)를 통해 fMP4 기반 예비 포맷으로 영상 퍼블리시가 가능함
            YouTube의 경우 구글이 MoQT에 적극적으로 기여하는 인력이 있지만, 이걸 YouTube 제품에 언제, 어떤 방식으로 적용할지는 내 판단으론 확신할 수 없음
          + https://caniuse.com/webtransport
            https://caniuse.com/webcodecs
            실제로 WebCodecs가 필수는 아니고, MSE로 렌더링할 수도 있지만 더 까다롭고 지연도 길어짐.
            Safari 지원을 위해 WebSocket 폴백과 WASM 기반 OPUS 인코더 작업 중임
"
"https://news.hada.io/topic?id=22625","스페이스 인베이더 그리기 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            스페이스 인베이더 그리기 방법

     * Space Invader Generator를 소개하며, 다양한 픽셀 아트 인베이더를 자동으로 생성하는 원리를 설명함
     * 간단한 벡터 다각형 바디 생성과 대칭, 임의 포인트, 미러링 등 기하학적 규칙을 활용한 구조임
     * 팔, 촉수, 뿔 등 팔다리 요소 역시 랜덤성과 기하학적 방법으로 확장하며, 쉽고 창의적인 변형 가능성 제공함
     * 벡터 형태에서 픽셀 변환 및 컬러 적용, 눈 추가 등으로 친숙한 게임 그래픽 완성함
     * 전체 제작 과정과 코드 구현 로직을 공개하여, 학습자와 개발자가 직접 커스터마이즈나 실습 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Space Invader Generator는 픽셀 아트 스타일의 인베이더를 누구나 쉽게 무작위로 만들어볼 수 있는 도구임. 이 글에서는 그 동작 원리와, 창의적 랜덤 생성 과정을 애니메이션 및 예시와 함께 설명함. 인베이더의 기하학적 구조, 벡터-픽셀 변환, 컬러 적용 등 디자인과 코딩을 결합한 접근 방식이 특징임.

시작 배경

     * Rayven이라는 3D 렌더러 툴을 개발하다가 실제 창작 결과물을 만드는 일의 중요성을 깨달음
     * 간단하고 재미있는 결과로 Space Invader처럼 직관적이고 쉽게 인식되는 대상을 선정함
     * Vector 기반 3D 렌더링으로 여러 클래식 인베이더를 그려보고, 무작위 생성기로 확장하면 재미있을 것이라는 생각에서 제작 시작함
     * 해당 제작 경험을 Creative Coding Amsterdam의 코드 챌린지에도 공유함

코드 챌린지

     * Space Invaders 코드 챌린지는 많은 창작자와 개발자들의 관심을 이끌었음
     * 다양한 구현과 결과물을 모으는 중이며, 개발 관련 커뮤니티에서 활발히 공유되고 있음

스케치에서 픽셀로

     * 처음에는 종이에 손으로 낙서 및 스케치를 하며 인베이더 구조를 분석함
     * Aseprite 툴로 15x15 픽셀 그리드에 여러 인베이더 형태를 디지털로 그려봄
     * 공통적인 기하학적 패턴(중앙 축 대칭, 단순 다각형 바디 등)을 발견함
     * 픽셀아트와 벡터 그래픽의 장점을 결합하여, 대부분 직접 그린 디자인을 자동으로 생성하는 기능 구현에 성공함
     * 세부 구현 내용은 GitHub 저장소에서 참고 가능함

인베이더 생성 과정

  중앙 찾기

     * 모든 조작의 기준점인 중앙 포인트를 설정함
     * 촉수는 하단에 생성되어, 메인 바디는 약간 위로 배치함
     * 전체 대칭을 활용하여, 한쪽만 그리고 나중에 좌우 반전하여 완성함

  상단과 하단 정의

     * 바디 측면을 설계할 때, 상단·하단 포인트를 랜덤으로 선택
     * 대칭 축에 따라 양쪽 형태가 동일하게 유지됨

  왼쪽 측면 그리기

     * 바디 왼편에 1~5개 포인트를 무작위로 배치
     * 단순 볼록 다각형에서 자유롭게 변형하여 다양한 결과 생성
     * 라인 중첩 현상은 픽셀화 과정에서 자연스럽게 보정됨

  오른쪽 반사

     * 왼쪽 정점 데이터를 사용해, 자동으로 우측 반사 생성

  바디 다각형 연결

     * 포인트들을 연결해 벡터 다각형 바디 완성
     * 이 기반 위에 팔다리를 추가하면 인베이더 핵심 형태가 형성됨

팔다리 추가

  촉수와 뿔 생성 방식

     * 하단 촉수(tentacle), 상단 뿔(horn)을 각각 생성. 동일한 방법에 위치·각도만 다르게 적용함

    촉수 루트 찾기

     * 바디 가장 하단의 포인트를 기준으로 왼쪽 촉수부터 랜덤 생성

    중심선 스케치

     * 랜덤 포인트를 이용해 폴리라인(중심선) 을 만듦
     * 촉수 길이·모양 다각도로 변형 가능

    두께 적용 (fat line)

     * 중심선만으론 얇아서, 양옆에 포인트 생성해 굵은 촉수 모양 구현
     * 바디 근처일수록 굵고, 끝으로 갈수록 얇아짐 (테이퍼 효과)
     * 각이 급한 부분은 선폭을 줄여서 자연스러운 접촉부 표현
     * 폭 조절을 위해 easing 파라미터 사용

    촉수 완성

     * 양쪽 끝점을 연결하여 두꺼운 촉수 완성

    다수 촉수 및 뿔 확장

     * 같은 방식으로 좌우대칭, 중앙 촉수, 상단 뿔 등 확장 가능
     * 중앙 촉수의 경우, 이미 그려진 측면 촉수와 충돌을 피하도록 조기 종료
     * 뿔은 공간이 겹치지 않게 각도 범위를 좁히고 좌우로 배치

벡터에서 픽셀로 변환

  바디 픽셀화

     * 각 픽셀 중앙이 벡터 다각형 내부에 있는지 여부로 바디 픽셀 지정
     * 정확도보단 간단함과 실행 속도를 우선함

  팔다리 픽셀화

     * 촉수와 뿔은 얇아서 중앙이 내부에 있지 않은 경우가 많음
     * 포인트와 인접한 픽셀 중심까지 거리를 확인하여 픽셀 할당
     * mid-line 세분화(line splitting)로 촉수 자연스러움 보완 가능

눈 추가

     * 여러 미리 준비된 눈 세트 중 랜덤 선택
     * 바디 중앙 부근에 위치시키고, 바깥쪽엔 범퍼 픽셀로 padding 적용
     * 겹치는 픽셀은 자동으로 비워서 구멍처럼 표현

컬러 적용

  컬러 생성 로직

     * OKLCH 컬러 스페이스 사용
     * HSL 대비 일정한 밝기(lightness) 유지, 다양한 생동감 있는 컬러 배색 가능
     * 밝기 하나로 고정, 나머지 두 파라미터를 랜덤으로 지정하여 다양한 변형 실현
     * 연속성, 시각적 일관성 있는 인베이더 인상 제공

    CSS 변형 활용

     * CSS 변수로 컬러 조절 가능
     * UI 요소 대비, 디버그 모드 등 각 상황에 맞춰 명도·채도 변화 적용

애니메이션 구현

     * 원작 게임처럼 2프레임의 단순 애니메이션으로 촉수, 뿔, 눈에 움직임 부여
     * 팔다리 mid-line을 복제 후 무작위로 shift하여 변형된 프레임 생성
     * 눈 역시 한 픽셀 이동시켜 생동감 향상

크기 조정

     * 그리드 크기를 키우면 인베이더가 점점 더 섬세·복잡해짐
     * 너무 크면 벡터의추상이 강조되어 본래의 인베이더 느낌이 줄어듦
     * 31x31 픽셀까지 제한, 숨겨진 옵션으로 최대 51x51까지 가능

결론

     * 무한히 다양한 컬러풀 인베이더를 자동 생성하는 제너레이터 완성
     * 제작 및 포스트 과정에서 배움, 재미, 창작의 자유로움 실현
     * 코드와 원리가 모두 공개되어 실습, 실험, 커스터마이즈에 용이함

제작 후기

     * 포스트 내 JavaScript 코드로 학습·참고 용이하게 비축함
     * Anime.js 및 여러 외부 의존성 활용해 애니메이션 설계, TypeScript로 구현
     * 별도 디버그 모드 및 step 옵션으로 생성 과정 직접 탐색 가능

보너스 - 로프 그리기 포스트

     * SVG와 자바스크립트로 로프(rope) 모양을 그리는 이전 인터랙티브 포스트도 참고할 만함

        Hacker News 의견

     * 나도 space invader 생성기를 만들어 봄
       실행 버전이 있고, 소스코드도 공개함
       Jared Tarbell에게서 영감 얻음
       결과물이 생각보다 쉽게 좋아짐을 경험함
       핵심은 눈, 양측 대칭, 그리고 작은 사각형 내에 무작위 픽셀이었음
          + Jared Tarbell에게 영감을 받았다는 의견, 나 역시 컴퓨터를 처음 만지던 시절에 그의 작품들이 큰 의미였음
            levitated.net와 이후의 complexification.net도 인터넷 초창기의 보석 같은 사이트였음
            아쉽게도 Flash(Levitated)와 Java(Complexification) 지원이 중단되어 예제 대부분이 동작하지 않음
            그래도 Complexification쪽은 Processing .pde 소스를 Processing 에디터에 붙여넣으면 실행 가능함
          + UI와 무작위 색상 팔레트 모두 마음에 듬
            정말 멋진 결과물임
     * 이 글은 모바일에서 읽으면서 느꼈던 최고의 경험 중 하나였음
          + 스크롤을 내릴 때마다 캔버스가 따라 올라오던 그 작은 기쁨, 정말 인상적이었음
     * 결과물이 정말 좋아서 놀랐음
       muffinman.io/invaders에서 확인 가능함
       본문만 보고 기대했던 것 이상임
       그리고 오늘 oklch에 대해 알게 됨
          + 직접 실험해 보니 결과물이 생각보다 잘 나와서 나도 놀랐었음
            다듬고 UI를 추가하니 훨씬 괜찮아졌다고 느낌
            OKLCH는 정말 큰 업그레이드임
            코드로 색상을 다루는 게 어려웠는데, 이게 확실히 많이 편해짐
            나 자신은 HSLuv도 좋아하지만 아쉽게도 브라우저에서 네이티브 지원을 안 하고 있음
     * 재밌음
       역설적으로 행성(지구)를 invader를 만들어서 구하는 느낌, 바로 brute force AI 없이도 즐거운 해커식 방식임
       이 프로젝트는 충분히 추천할 만함
          + 고마움, AI가 이런 프로젝트의 재미를 다 빼앗아가는 느낌임
            창작 과정의 마법은 결국 그런 수작업 자체에 있는 것임
     * 이미지를 상단에 고정해서 우리가 스크롤할 때 무슨 얘기를 하는지 계속 보여주는 게 정말 멋짐
       평소엔 스크롤에 따라 페이지가 화려하게 바뀌는 걸 별로 안 좋아하는 편이지만 이 구성은 예외적임
     * 멋짐
       전 세계에서 space invader들을 수집하고 있음
       space-invaders.com/flashinvaders도 참고할 만함
     * 관련된 것으로 2000년대 웹 레전드 levitated.net에서 만든 levInvaderFractal (2003)이 있음
     * 정말 즐겁게 읽음
       문제 해결 방식과 글 자체, 프레젠테이션까지 모든 것에 정성이 들어간 게 느껴짐
     * 좋은 space invader는 위압감 있고 위험해 보여야 한다고 생각함
       집게발을 흔들면 점수 추가임
     * 아주 흥미로웠음
       8비트 감성과 레트로 컴퓨팅에 애정 있는 사람에게 특히 더 재밌는 글임
       읽으면서 문득 든 생각인데, 이건 결국 ‘모든 걸 게(크랩)로 진화시키려는 모성 자연의 욕망’이 디지털로 구현된 모습이 아닐까 함
       space invader의 대칭성과 다양한 다리, 집게와 같은 구조 때문에, 결국 모든 invader는 게처럼 생김
       혹시 우리가 Matrix에서 어떤 일치점을 보고 있는 건 아닐까 하는 상상도 해봄
"
"https://news.hada.io/topic?id=22680","윈도우 RDP로 접속 가능한 한글 XWINDOW Docker 이미지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 윈도우 RDP로 접속 가능한 한글 XWINDOW Docker 이미지

   최신 Debian 13(Trixie) 기반 한글 XWINDOW (wayland) Docker 이미지를 만들어 공개합니다.

   실제로 윈도우 쓰시는 분들이 많은데, 가끔가다 도커로 한글되는 리눅스 X윈도우 띄워서 작업하시고 싶을 때가 가끔 있으실거에요.
   그래서!
   윈도우 원격 데스크탑으로 접근 가능한 데비안 기반 한글 GUI 환경을 만들어보았습니다.

   이 이미지에는 Visual Studio Code, Chromium, Vim, Git, Node.js + npm 등 개발 환경에 필요한 주요 툴이 기본 설치되어 있어, 바로 개발 환경으로 활용할 수 있습니다. (덕분에 덩치가 좀 큽니다)

   주요 특징으로는,
     * 윈도우 원격 데스크탑(RDP) 접속 가능
     * 데이터 및 볼륨 유지: /home/(사용자명)을 Docker 볼륨으로 연결하면 데이터 지속 가능

   주의사항으로는,
   Docker에서 실행되므로 VSCode와 Chromium은 sandbox 모드 미사용됩니다. 보안 유의 부탁드리고요, 사운드 기능은 충돌과 버그가 많아 제외되었습니다. 제 생각으로는 대부분 음악은 윈도우 PC 에서 듣고 계실 것 같아서 제외하였습니다.

   개발 환경을 빠르게 구축하고 싶은 한글 사용자에게 특히 유용한 이미지입니다.

   소스는 https://github.com/lancard/x11-korean 있으니 참고하시고, 필요하신 기능 있다면 언제든 이슈나 PR 날려주세요!

   privileged mode 인 경우 sandbox 에서 돌도록 수정했습니다. privileged mode 인 경우 로컬리소스(C드라이브 등)을 연결할 수 있습니다. 연결은 $HOME/thinclient_drives에 마운트됩니다.

   로컬리소스(C: D: 등) 연결 하시려면 privileged 모드로 실행하시면 됩니다. (그러면 CTRL + C / V 파일 복사 먹습니다)

   호오옥.. vs code가 실행이 안되네요 :)

   자답입니다 반응이 없어서 이거저거 보다가 터미널에서 code --no-sandbox로 하니까 열리네요.

   음? 바탕화면에 있는게 안열리셨을까요?

   아 네 안되더라구요. 뭘 잘못한건지..

   제가 계속 이미지를 변경하고 있어서 중간 이미지라 그러실 수도 있습니다.
   일단 바탕화면 vscode 아이콘이 명령이 /usr/bin/code %F 인지 확인해보시고요,
   /usr/bin/code 파일 열어서 끝에서 두번째 줄이
   ELECTRON_RUN_AS_NODE=1 ""$ELECTRON"" ""$CLI"" --no-sandbox --disable-gpu ""$@""
   로 되어있는지 확인해보세요.

   최근 도커 공부를 하는 사람입니다. trivy 로 이 이미지의 취약점을 검사해봤는데, 1053개의 취약점이 나왔습니다. 다 중요한거 같지는 않고 그냥 온갖 것들 다 잡아내는 것 같은데, 실무적으로 이미지의 보안성은 어떻게 확인하고 달성하는지 혹시 조언 부탁드려도 될까요

   사실 취약점 제거 방법은 너무 많아서 정답이 없습니다만
   저같은 경우 최대한 안정화된 최신버전 사용하고 github action 사용 시에는 security bot을 최대한 켜둡니다. 사실 이번 xwindow 이미지는 보시면 아시겠지만 프로그래밍한 부분이 없어서 설치된 프로그램들의 자체 기본 취약점만 있을거에요.

   sandbox 모드가 적용되지 않은게 왜 보안 유의사항인가 궁금했습니다.
   도커는 내부 프로세스에 샌드박스 기능을 제공하지 않는다는건가요? 그래서 어쩔 수 없이 루트로 실행될 수밖에 없고, 비록 도커로 격리된 환경이라 하더라도 호스트와 매핑된 볼륨에는 악성코드가 침투될수도 있는 위험을 의미한거에요? 아니면 도커 내부 파일시스템에 사용자가 생성한 파일이 안전하지 않을 수 있다는걸까요?

   제가 알기론 윈도우든 리눅스든 크롬은 샌드박스 모드로 돕니다. 그니까... 혹시나 javascript 등으로 exploit 만들어서 취약점 공격을 해도 실제 OS 에는 영향을 미치지 않는거죠.

   근데 컨테이너모드에서는 priviliged 모드를 켜야만 아마 그 기능을 켤 수 있을겁니다.

   물론 해당 모드를 켜라고 안내할 수도 있긴합니다만, 컨테이너 자체가 샌드박스라고 저는 생각했어요. 손쉽게 켰다가 끄는 윈도우 같은 느낌이라고 해야 할까요...

   그래서 크로미움과 일렉트론 기반 vscode 는 sandbox가 아닌 모드에서 돌게 해둔 것입니다.

   어디까지나 크로미움과 vscode 에 취약점이 있고, 해당 공격자가 그걸 이용해서 exploit 을 만들 수 있다고 가정하면 위험할 수 있다는 뜻입니다.

   docker 명은 lancard/xwindow-korean 입니다

   wayland 라서 repo 명을 https://github.com/lancard/xwindow-korean 로 변경하였습니다~

   우분투가 데비안 계열이므로 apt 등도 편하게 이용하실 수 있습니다. 우분투 기본 이미지보다 데비안이 더 낫더라구요.
"
"https://news.hada.io/topic?id=22653","Zig의 사랑스러운 문법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Zig의 사랑스러운 문법

     * Zig는 Rust와 비슷한 중괄호 기반 문법을 기반으로 하지만, 더 단순한 언어 의미와 세련된 문법 선택으로 개선함
     * 정수 리터럴은 모든 타입이 comptime_int로 시작해 할당 시 명시적으로 변환되며, 문자열 리터럴은 \\ 기반의 간결한 원시 문자열 표기법을 사용함
     * .x = 1 형태의 레코드 리터럴은 필드 쓰기를 쉽게 검색 가능하게 하며, 모든 타입은 접두사 표기법으로 일관성 있게 표현됨
     * and·or를 제어 흐름 키워드로 사용하고, if·loop 구문은 선택적으로 중괄호를 생략할 수 있으며 포맷터가 안전성을 보장함
     * 네임스페이스 없이 모든 것을 표현식으로 처리해 타입·값·패턴 문법을 통합하고, 제네릭·레코드 리터럴·내장 함수(@import, @as 등)를 간결하게 활용함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * Zig는 Rust와 유사한 외형을 가지지만 더 단순한 언어 구조를 채택
     * 문법 설계에서 grep 친화성, 구문 일관성, 불필요한 시각적 잡음 감소에 집중

정수 리터럴

const an_integer = 92;
assert(@TypeOf(an_integer) == comptime_int);

const x: i32 = 92;
const y = @as(i32, 92);

     * 모든 정수 리터럴은 comptime_int 타입
     * 변수에 할당 시 명시적으로 타입을 지정하거나 @as를 사용해 변환
     * var x = 92; 형태는 작동하지 않으며 명시적 타입 필요

문자열 리터럴

const raw =
    \\Roses are red
    \\  Violets are blue,
    \\Sugar is sweet
    \\  And so are you.
    \\
;

     * 각 행이 개별 토큰이라 들여쓰기 문제가 없음
     * \\ 자체를 이스케이프할 필요 없음

레코드 리터럴

const p: Point = .{
    .x = 1,
    .y = 2,
};

     * .x = 1 형식은 읽기/쓰기 구분에 유리
     * .{} 표기는 블록과 구분하면서 결과 타입으로 자동 변환

타입 표기법

u32        // 정수
[3]u32     // 길이 3 배열
?[3]u32    // null 가능 배열
*const ?[3]u32 // 상수 포인터

     * 모든 타입은 접두사(prefix) 표기
     * 역참조는 접미사 표기(ptr.*)

식별자

const @""a name with space"" = 42;

     * 키워드 충돌 방지 또는 특수 이름 지정 가능

함수 선언

pub fn main() void {}
fn add(x: i32, y: i32) i32 {
    return x + y;
}

     * fn 키워드와 함수명이 붙어 있어 검색이 용이
     * 반환 타입 표기에 ->를 쓰지 않음

변수 선언

const mid = lo + @divFloor(hi - lo, 2);
var count: u32 = 0;

     * const와 var 사용
     * 타입 표기는 이름: 타입 순서

제어 흐름: and/or

while (count > 0 and ascii.isWhitespace(buffer[count - 1])) {
    count -= 1;
}

     * and, or는 제어 흐름 키워드
     * 비트 연산에는 &, | 사용

if 문

.direction = if (prng.boolean()) .ascending else .descending;

     * 괄호 필수, 중괄호 선택
     * zig fmt가 안전한 포맷 보장

반복문

for (0..10) |i| {
    print(""{d}\n"", .{i});
} else @panic(""loop safety counter exceeded"");

     * for, while 모두 else 절 지원
     * 반복자와 요소명을 직관적으로 배치

네임스페이스와 이름 해석

const std = @import(""std"");
const ArrayList = std.ArrayList;

     * 변수 섀도잉 금지
     * 네임스페이스와 글롭 임포트 없음

모든 것은 표현식

const E = enum { a, b };
const e: if (true) E else void = .a;

     * 타입·값·패턴 구문을 통합
     * 타입 위치에 조건식을 둘 수 있음

제네릭

fn ArrayListType(comptime T: type) type {
    return struct {
        fn init() void {}
    };
}

var xs: ArrayListType(u32) = .init();

     * 제네릭은 함수 호출 구문(Type(T))으로 표현
     * 타입 인자는 항상 명시

내장 함수

const foo = @import(""./foo.zig"");
const num = @as(i32, 92);

     * @ 접두사로 컴파일러 제공 기능 호출
     * @import는 파일 경로를 명확히 표시
     * 인자는 반드시 문자열 리터럴이어야 함

결론

     * Zig 문법은 작은 선택들의 집합이 모여 읽기 좋은 언어를 만든 사례
     * 기능 수를 줄이면 필요한 문법도 줄어들고, 구문 간 충돌 가능성도 감소
     * 기존 언어의 좋은 아이디어를 차용하되, 필요할 때는 과감히 새 문법을 도입

        Hacker News 의견

     * 이 글은 문법 설계에서 발생하는 여러 트레이드오프를 깊이 있게 다루고 있고, Zig의 문법이 가진 미니멀리즘과 일관성, 그리고 무자비할 정도로 가독성에 집중하는 점이 정말 인상적임을 느꼈음. 이건 추상적인 아름다움이 아니라, 산업적 용도에서 놀랄 요소가 없는 '브루탈리즘'이라는 점이 마음에 듦. 이런 균형 잡힌 문법 설계는 정말 드물고, Zig가 잘 해냈다고 생각함
          + 아티클에서 에러 핸들링에 대한 언급이 없어 아쉬움. Zig의 try/catch 방식은 굉장히 훌륭해서, 여러 언어 중 가장 좋아하는 에러 핸들링 방법임. 이 부분도 소개되었으면 더 좋았을 것 같음
          + '표면적으로 아름다운 가독성'이 아닌, 추상화를 통해 얻을 수 있는 일관된 아름다움이 Zig의 진정한 매력임. S식과 M식 비유처럼, 일반적인 케이스에서의 좋은 접근이 여러 예외적 상황을 위한 특수한 설계보다 장기적으로 더 나은 경우가 많음. C++처럼 각종 예외 케이스를 추가하면 결국 모든 규칙을 외워야 하는 부담만 커짐. 언어 설계에서는 단순성과 일관성을 추구하면, 결국 복잡함은 사용자가 감당하게 되는 'Turing tarpit'에 빠질 수 있으니, 일반적인 규칙에서 특수 케이스가 자연스럽게 해결되는 접근이 중요함. XKCD의 New Pet 코믹에서도 이런 예시를 볼 수 있음
          + 인상 깊었던 예시가 있다면 공유해 줄 수 있을지 궁금함
     * Zig가 Rust처럼 '이름:타입' 형식의 타입 명시 방식을 사용하는 부분에 대해, 오히려 타입이 먼저 나오는 전통적 방식이 더 마음에 듦. 변수의 선언을 다시 확인할 때 가장 궁금한 건 그 변수의 타입인데, 이걸 빠르게 찾지 못하면 불편함. 특히 Rust에서는 let mut처럼 불필요하게 반복되는 요소가 많아 오히려 번거롭고, C, C++처럼 타입이 먼저 오는 것도 좋음. 실제로는 타입 추론이 필요한 곳에만 최소한으로 쓰는 게 이상적이라고 생각함
          + let 키워드가 사실 선언문임을 분명히 해 주기 때문에 필요한 부분도 있음. 그렇지 않으면 C++의 애매한 구문 파싱 문제를 겪을 수 있음
          + 나 역시 항상 변수 타입을 먼저 확인하려 하기 때문에 타입이 앞에 오는 방식을 선호함. 구문 분석기 입장에서는 이름을 먼저 처리하는 게 편리하고, TypeScript는 자바스크립트와의 호환성 때문에 이런 구조를 채택했다는 점을 이해함. 결국 중요한 건 사용이 쉬운 표준 라이브러리라고 생각함. 타입 시스템을 과하게 악용하는 예시처럼, 굳이 모든 상태를 타입으로 표현하기보다 의도를 분명하게 전달하는 것이 더 중요함
          + 코드에서 변수 타입을 확인하려고 다시 올라가지만, 오히려 타입이 먼저 나오면 내가 찾으려는 변수 선언을 찾기 더 어려워짐. 타입 네임이 제일 앞에 오고, 그 길이가 가변적이기 때문에 시선을 좌우로 반복해서 움직여야 해서 비효율적으로 느껴짐
          + 대부분의 경우 에디터에서 마우스를 올리면 타입 정보를 바로 보여주기 때문에 코드에서 타입 위치가 그리 중요하지 않을 수 있음. Rust가 verbose한 이유는 파싱 애매함을 피하려는 구현적인 측면이 큼. C, C++처럼 타입이 먼저 나오면 특정 이름으로 선언된 변수를 grep으로 쉽게 찾기 어렵고, return type을 앞에 두는 스타일은 템플릿 때문에 도입된 것이지만 경우에 따라 코드를 더 쉽게 읽고 찾게 해줌
          + 나 개인적으로는 파스칼 스타일의 타입 명시 방식을 더 선호함. 타입 추론을 할 때에도 별도의 'auto' 같은 우회적 기능이 필요 없고, 파싱 관점에서도 덜 모호함. 'MyClass x'에서 MyClass가 타입인지 변수명인지 바로 알기 어렵기 때문에 이런 모호성을 줄여줌
     * Zig의 raw/multiline string(멀티라인 문자열) 문법에 대해 \를 여러 번 써야 하는 방식이 너무 혼란스럽고 극단적으로 느껴짐
          + 파이썬, C++, Rust 등에서 멀티라인 문자열을 포맷해 본 적이 있다면 그 불편함을 이해할 것임. 들여쓰기가 문자열 내용에 포함되는 문제 때문에 항상 고민이 있고, YAML처럼 들여쓰기 제거 모드를 가진 경우는 오히려 혼란을 가중함. Zig 방식은 들여쓰기에 관한 한 굉장히 명확함
          + 처음엔 이 문법이 너무 불편했는데, Zig를 쓰다 보면 점점 익숙해지고 오히려 장점이 보임. Zig는 참신하게도 처음 접할 때 불호가 있을 수 있지만, 실제 써보면 장점을 깨닫게 됨
          + 사실 미친 문법이 아니라, 이 복잡한 문제(멀티라인 문자열 안에 또 멀티라인 문자열을 안전하게 넣는 문제)를 해결하기 위한 미친 문제임. Zig에선 별도 이스케이프도 필요 없고 들여쓰기 걱정도 안 해도 되는 점이 좋음
          + Kotlin의 trimIndent, Go나 Java의 텍스트 블록, 그리고 특히 Go의 backtick raw string 방식이 나에겐 더 매끄럽게 느껴짐. Zig에선 \ 때문에 오히려 @embedFile 방식으로 우회해서 사용함
          + 비주얼적으로 \가 마음에 들지는 않지만, 멀티라인 리터럴 및 들여쓰기 문제를 깔끔하게 해결하는 방법이라고 생각함. 함수 없이 이 문제를 해결하는 언어를 딱히 알지 못함
     * Zig 문법이 산만하게 느껴짐. @TypeOf 같이 @로 시작하는 구문이나 .{.x} 같은 초기화 문법이 어색하게 다가옴. Zig 사용에 능숙하지 않아서 그런 걸 수도 있지만, 전체적으로 코드를 읽기 어렵다는 인상이 있음
          + Odin의 문법은 훨씬 미니멀하고 잘 다듬어져 있어서 선호함. Zig는 다소 산만한 느낌이 듦
          + .은 Zig에서 추론 타입을 위한 플레이스홀더 역할임. 예를 들어 다음과 같이 객체를 초기화할 수 있음
const p = Point{ .x = 123, .y = 234 };

            혹은 타입 추론을 명시하고 싶다면
const p: Point = .{ .x = 123, .y = 234 };

            함수 인자에서도 타입을 생략할 수 있어서 더 간결함. Rust에서는 이러한 상황에서 명시적으로 타입을 써야 함
takePoint(Point{ x: 123, y: 234 });

            중첩 구조체 초기화에서도 Zig의 추론 방식이 훨씬 유용함. 모든 곳에 명시적으로 타입을 적어야 하는 Rust는 금방 코드가 산만해질 수 있음. 그럼에도 선행 dot 표기를 빼는 편이 더 편리하다고 생각하지만, 파서 구현 단순화 때문에 유지하고 있는 듯함. x: 123 혹은 .x = 123 표기법은 각각 JS, C99에서 차용한 것임. 개인적으로 둘 다 자주 써서 어색한 건 아니라 생각함
     * C# 11의 raw string literal 방식이 훨씬 선호됨. 첫 줄 들여쓰기를 기준으로 나머지 줄에서 들여쓰기를 자동으로 맞춰줌. 또한 중괄호를 문자로써 쓸 수도 있음. $가 여러 번 등장하면 중괄호를 완전히 값으로 처리함
string json = $""""""
   <h1>{title}</h1>
   <article>
     Welcome to {sitename}.
   </article>
   """""";
string json = $$""""""
   <h1>{{title}}</h1>
   <article>
     Welcome to {{sitename}}, which uses the <code>{sitename}</code> syntax.
   </article>
   """""";

          + (C# raw string literal 기능의 작성자로서) 실제로는 마지막 """""" 라인의 들여쓰기가 기준이고, 첫 줄도 들여쓰기 할 수 있게 되어 있음. 이 기능을 좋아해 주어서 기쁘고, 좋은 기능이라고 자부함
     * Zig의 문법도 좋지만, Go처럼 세미콜론이나 ':' 없이도 충분히 깔끔하게 쓸 수 있다는 점에서 'lovely'까지는 아니라고 생각함. 굳이 비교한다면 Rust보단 많이 개선된 건 맞지만, Go도 충분히 우수함
          + 오히려 Go처럼 지나치게 미니멀한 문법은 읽을 때 해석하기 더 어려운 경우가 있음. 코드를 직접 쓰는 시간보다 읽는 시간이 많아서, 필요 이상의 간결함은 오히려 실수를 부르고 디버깅을 어렵게 만듦. CoffeeScript, J처럼 너무 축약된 문법이 대표적인 예임
          + 문법 요소를 뺀다고 해서 더 나은 문법이 되는 건 아니라고 생각함. 만일 그랬다면 모두가 Lisp처럼 쓸 것이고, scriptio continua(공백 없는 고대 필기 방식)처럼 글도 썼을 것임. scriptio continua 위키피디아 참조
     * Zig가 전체적으로 만족스럽지만 다음 문제들이 아쉬움
          + 블럭의 반환 값을 지정하기가 어려움. Rust처럼 마지막 표현식을 반환값으로 자동 인식하면 좋겠지만 Zig에선 label 등을 써야 해서 번거로움
          + 옵셔널 타입의 체이닝(예: a?.b?.c)이 불가능함. 모나딕 타입 지원이 있으면 더 일반적인 체이닝이 가능할 텐데, 아직 부족함
          + 람다 함수 지원이 없음. 이미 반복문이나 catch 블록 같은 곳에서 함수 블록을 쓸 수 있는데, 람다 지원까지 되면 더 유연해질 것 같음
     * 타입 명칭에서 void를 쓰는 것에 대해선, 사실 void는 타입 이론에서는 'unit'의 역할이 아니라 값이 없는 'uninhabited' 타입을 의미함. 전통적으로 '()'나 'unit'이 한 멤버를 가진 타입임. void는 abort 같은 함수의 반환타입임
          + C, C++에서는 void가 그런 대로 잘 쓰이고 있어서 많은 시스템 프로그래머들에게 익숙함. 형식 이론에서의 용어 논쟁은 실사용엔 무의미하다고 생각함. Zig에 오는 많은 사람들이 C, C++ 배경을 가지고 있기 때문에 void로 충분히 괜찮음
          + abort는 Rust의 ! 타입처럼 '도달 불가' 상태를 위한 타입임. void는 차라리 unit이나 ()에 더 가깝고, 값이 존재하지 않는 타입임. 재밌는 트릭으로, TypeScript에서는 void를 제네릭 제약조건에 쓰면 해당 파라미터를 옵셔널로 만들 수 있음
          + void 타입은 매우 오랜 전통이 있고, ALGOL 68까지 거슬러 올라감. 거기선 VOID 타입이 하나의 멤버(EMPTY)만 가진 타입으로 정의되어 있음
     * ""Zig에 람다가 없다""는 점이 놀라움. C++에서는 람다를 거의 매 everywhere 쓰는데, 그럼 배열 sort 등에서 comparator는 어떻게 정의하는지 궁금함
          + 보통 함수 선언을 따로 한다는 점에서, Zig는 그 부분이 불편하다고 생각함
          + 익명 구조체와 거기에 포함된 함수를 인라인으로 참조할 수 있음. 사실 람다에서 주로 쓰는 캡처 기능이 Zig에는 없지만, 컨텍스트 파라미터(대개 구조체)를 넘기는 방식으로 대체할 수 있음
          + 기본적으로 C와 동일하게, 구분 함수 선언 후 그 포인터를 정렬 함수에 전달하는 방식임
     * ""문법은 중요하지 않다""고 말하지만, 실제로는 ""문법은 중요하지 않으니 내가 선호하는 방식대로 쓰자""는 식임. 나 역시 Rust/Zig/Go처럼 C 계열에서 파생된 문법이 익숙하고, Haskell/OCaml처럼 공백으로 함수 호출을 구분하는 스타일은 아직 생소해서 대중화에 방해된다고 봄. Rust의 성공처럼, 함수형 프로그래밍의 '시금치'를 시스템 언어라는 '브라우니'에 잘 녹여 준 점을 타 언어도 참고해볼 만함
          + 문법이 중요하지 않다고 하는 말에 동의하지 않음. 결국 문법은 사용자가 언어와 상호작용하는 메인 인터페이스임. 어떤 언어를 읽을 때마다 문법 요소가 무의식적으로 더 크게 부각됨
          + C 계열 문법을 가진 함수형 언어를 원한다면 Gleam을 추천함: gleam.run
            코드도 매우 예쁨
fn spawn_greeter(i: Int) {
 process.spawn(fn() {
  let n = int.to_string(i)
  io.println(""Hello from "" <> n)
 })
}

            Reason도 추천할 만함. OCaml 기반이지만 C 계열 문법을 가짐: reasonml.github.io
"
"https://news.hada.io/topic?id=22740","Gemini 2.5 Flash Image - 최신 이미지 생성 및 편집 모델 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Gemini 2.5 Flash Image - 최신 이미지 생성 및 편집 모델 공개

     * Google이 Nano-Banana로 알려진, 이미지 생성·편집 능력이 한층 강화된 Gemini 2.5 Flash Image를 공개
     * 캐릭터 일관성 유지, 자연어 기반 부분 편집, 세계 지식 활용, 다중 이미지 결합 등을 지원함
     * 개발자는 Gemini API, Google AI Studio, Vertex AI, OpenRouter, fal.ai 등을 통해 모델을 바로 사용할 수 있음
     * 가격은 100만 출력 토큰당 30달러, 이미지 한 장당 약 0.039달러
     * 모든 결과물은 보이지 않는 디지털 SynthID 워터마크가 삽입됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gemini 2.5 Flash Image 소개

     * Google은 차세대 이미지 생성 및 편집 모델인 Gemini 2.5 Flash Image(코드명 nano-banana)를 발표함
     * 이번 업데이트로 여러 이미지를 하나로 합성, 특정 인물이나 객체의 일관성 유지, 자연어를 활용한 세밀한 변형, 그리고 Gemini의 세계 지식을 바탕으로 한 이미지 생성 및 편집 기능이 가능해짐
     * 초기 버전인 Gemini 2.0 Flash에서도 낮은 지연시간, 비용 효율성, 사용 용이성이 장점이었으나, 사용자의 피드백을 반영하여 이번에 더 높은 품질의 이미지와 강화된 창의적 제어 기능이 추가됨
     * 현재 Gemini API, Google AI Studio, Vertex AI 등에서 사용 가능하며, 과금은 100만 출력 토큰당 $30로 책정됨(이미지 1개 당 약 $0.039)
     * 기타 입·출력 방식도 Gemini 2.5 Flash와 동일한 가격 정책을 따름
     * 공식 사이트 https://deepmind.google/models/gemini/image/

주요 기능과 시나리오

     * Google AI Studio의 “Build Mode” 업그레이드
          + Gemini 2.5 Flash Image로 앱 개발이 더 간편해짐
          + 개발자는 사용자 정의 AI 앱을 빠르게 제작, 테스트, 리믹스 가능하며, Google AI Studio에서 직접 배포하거나 코드를 GitHub에 저장할 수 있음
          + 예를 들어 “사용자가 이미지를 업로드하고 필터를 적용할 수 있는 이미지 편집 앱 제작”과 같은 프롬프트로 간단하게 앱을 만들 수 있음
          + 기본 제공 템플릿 선택/리믹스 기능도 무료로 제공됨
     * 캐릭터 일관성 유지
          + 이미지 생성 과정에서 동일한 캐릭터의 외형 유지가 큰 도전 과제였음
          + Gemini 2.5 Flash Image는 같은 인물을 다양한 환경에 배치하거나, 제품을 여러 각도/장소에서 표현, 브랜드 자산 일관성 확보 등에 효과적으로 활용 가능함
          + Google AI Studio 템플릿 앱을 통해 캐릭터 일관성 시연 및 코드 커스터마이즈가 쉬움
          + 이 기능은 부동산 카드, 직원 배지, 대량 상품 목업 등 템플릿 기반 디자인 일관성 확보에도 응용될 수 있음
     * 자연어 프롬프트 기반 이미지 편집
          + 사용자는 자연어 지시만으로 이미지 일부를 변형할 수 있음
          + 예시로는 배경 블러, 티셔츠 얼룩 지우기, 인물 삭제, 포즈 변경, 흑백 이미지를 컬러 이미지로 변환 등이 있음
          + UI/프롬프트 기반 편집이 가능한 템플릿 앱이 제공되어 실제 적용 경험이 가능함
     * 세계 지식 기반 네이티브 활용
          + 기존 모델이 미적인 이미지 생성에 강점이 있었던 반면, 현실 세계에 대한 의미론적 이해는 약했음
          + Gemini 2.5 Flash Image는 세계 지식을 기반으로, 손으로 그린 다이어그램 인식, 실제 세계 질문 대응, 복잡한 편집 명령 수행 등이 가능함
          + 직접 적용 예시로, 간단한 캔버스를 상호작용 교육 튜터로 변환하는 앱이 제공됨
     * 멀티 이미지 융합
          + 이 모델은 여러 이미지를 이해하고 자연스럽게 합성 가능함
          + 예를 들어 상품을 새로운 배경에 삽입, 방 전체의 컬러톤·질감 변경, 단일 프롬프트로 이미지 융합*을 지원함
          + 템플릿 앱을 이용해 드래그앤드롭 방식으로 제품을 배치, 포토리얼리스틱한 융합 이미지 생성 가능

시작 및 파트너십

     * 개발자 문서를 통해 바로 시작 가능하며, 현재는 프리뷰 상태이나 곧 안정화 예정임
     * 데모 앱들은 모두 Google AI Studio에서 코드 리믹스와 커스터마이즈 가능함
     * OpenRouter.ai와 제휴하여 300만 개발자에게 모델을 제공하며, OpenRouter의 480여 개 지원 모델 중 최초로 이미지 생성 가능 모델임
     * fal.ai와의 협업으로 더 넓은 개발자 커뮤니티에게 지원 확대

디지털 워터마크 및 피드백

     * Gemini 2.5 Flash Image로 생성·편집한 모든 이미지는 보이지 않는 SynthID 디지털 워터마크가 삽입되어 AI 생성물임을 감지할 수 있음
     * 텍스트 품질, 캐릭터 일관성, 이미지 세부 묘사 등 지속적 기능 개선 중임
     * 개발자 피드백은 Google 개발자 포럼 또는 X(구 Twitter)를 통해 수시로 받음

간단한 사용 예시 (Python 코드)

     * Python에서 genai SDK와 PIL, io 라이브러리를 통해 원하는 프롬프트와 이미지로 Gemini 2.5 Flash Image 활용 가능함
     * 예시: “내 고양이가 레스토랑에서 nano-banana를 먹는 모습""과 같은 자유로운 프롬프트 적용 가능
     * 출력 결과물은 코드 내에서 단순 텍스트와 이미지 파일로 모두 저장 가능

향후 발전 방향

     * 장문 텍스트 렌더링, 보다 신뢰성 높은 캐릭터 일관성, 사실적 세부 표현 등 지속적 수준 향상 개발 중
     * 커뮤니티의 적극적 참여와 피드백 유도
     * Gemini 2.5 Flash Image로 다양한 창작과 개발 경험 확대 기대

   Google Nano Banana란 무엇인가? Google의 비밀 이미지 AI

        Hacker News 의견

     * 이건 이미지 편집 모델의 GPT-4 순간 같음. 트위터에서 놀라운 결과물 보기
          + nano banana, 즉 gemini 2.5 flash는 성능이 엄청나서 lmarena에서 171 elo 포인트가 뛰었음
          + Twitter에서 nano banana 검색하면 놀라운 결과들을 볼 수 있음
          + 요즘 ""nano banana"" 도메인이 전부 등록돼서 각자 이미지 생성 UI를 제공하는데, 다들 인기 모델 이름을 이용한 중간 상인 같은 느낌임
          + 왜 이름이 nano banana인지 궁금함
     * 이게 바로 유명한 nano-banana 모델이고, 지금은 LMArena에서 gemini-2.5-flash-image-preview로 이름이 바뀜
          + nano-banana가 뭔지 모르는 사람을 위한 링크 Google Nano Banana란 무엇인가? Google의 비밀 이미지 AI
          + 나도 그게 궁금해서 들어왔는데 답을 얻어서 고마움
     * Gemini로 이미지를 만들려고 하면 절반은 불가능하다고 답변함
          + 게다가 Google이 발표한 기능들이 여기저기 흩어져 있어서 어떤 제품에서 쓸 수 있는지, 어디서 결제해야 하는지 전혀 감이 안 오는 혼란스러움이 있음
     * 이미지 모델들은 결국 시간 흡혈귀 같음
          + 방 하나 만드는 건 쉽지만 같은 방을 여러 각도에서 일관성 있게 만드는 건 사실상 불가능함
          + 이미지 일관성이 필요한 작업에는 쓰기 어려움
     * 가족 사진을 디지털화했는데 손상된 게 많아서 복구가 힘들었음
          + 이번 모델은 디테일을 바꾸지 않고 복원하는 게 좋아 보여서 드디어 쓸만한 시점이 온 것 같음
          + 사실 이런 결함들은 필름 스캐너 + ICE 기능과 Vuescan 같은 소프트웨어로 자동 복구 가능함
               o 수백 장을 실험적인 클라우드 AI에 맡기는 건 불필요해 보임
          + 혹시 비디오 화질 개선 소프트웨어 아는지 궁금함
               o Video 2000과 VHS 테이프를 디지털화 중인데, 추억 영상을 조금이라도 개선하고 싶음
          + 잘 되길 바라지만, 예시 중 하나는 얼굴이 지나치게 AI화된 느낌이 있었음
          + 사실 Flux Kontext라는 모델이 몇 달 전부터 이미 이 수준에 도달했음
     * 모델 성능은 인상적이지만, 동시에 사회적 영향이 걱정됨
          + Facebook 댓글만 봐도 불안해짐
          + Google의 SynthID를 테스트해봤는데 꽤 괜찮았음
               o 압축, 크롭, 리사이즈, 색 보정, 오버페인팅에도 워터마크가 남아있음
          + 나도 최근 SpaceX 발사 이벤트 중 딥페이크 사기에 속아 15k BTC를 잃었음
               o 기술이 너무 정교해서 공격이 점점 더 위험해짐
          + Facebook 댓글은 봇이 돌리는 게 확실해 보임
     * 램프 예시는 꽤 인상적이었음
          + 전원 연결, 조명, 그림자까지 자연스럽게 표현됨
     * ChatGPT의 이미지 생성보다 훨씬 빠른 속도가 마음에 듦
          + ChatGPT는 너무 느려서 알림으로 결과를 받아야 할 정도였음
          + “Gemini 2.5 Flash Image를 써본 OpenAI 투자자들의 모습” 같은 이미지를 떠올리니 웃김
     * 예전부터 하고 싶었던 작업이 있었음
          + 이미지1에서 특정 객체를 이미지2의 객체로 교체하는 것인데, 위치까지 정확히 지정하고 싶었음
          + 여러 모델을 시도했지만 다 실패했고, 이번 모델은 거의 맞췄지만 결국 다른 객체를 교체했음
          + 혹시 특정 위치를 참조 이미지로 교체하는 데 특화된 모델이 있는지 궁금함
          + Alibaba의 ACE++ 모델이 그런 기능을 지원함
               o phind.design에서 사용 중인데, 꽤 특수한 작업이라 흔하진 않음
"
"https://news.hada.io/topic?id=22729","Ask HN: 소프트웨어 설계를 배우기에 좋은 최고의 코드베이스는 무엇인가요? ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Ask HN: 소프트웨어 설계를 배우기에 좋은 최고의 코드베이스는 무엇인가요?

     * 소프트웨어 설계 능력을 향상시키고자 노력하고 있는데, 기존의 잘 설계된 코드베이스를 연구해 보라는 권유를 받았음
     * 공개적으로 접근 가능한 코드베이스 중 소프트웨어 설계의 골드 스탠다드로 여겨지는 것은 어떤게 있는지 궁금함

  1. 추천 코드베이스

     * 대형/대표 프로젝트
          + Git, Postgres, CPython
          + Linux Kernel 의 ""lieutenants model""
          + UNIX v6, BSDs
     * 프레임워크/라이브러리
          + Spring Framework
          + Laravel
          + Rust std 라이브러리
          + Codemirror 6
     * 시스템/서버
          + Postfix
          + Nginx
          + qmail
          + Varnish
     * 게임/특수 사례
          + Doom 3
     * 교육용/학습 자료
          + xv6 (MIT OS 수업)
          + pytudes (Peter Norvig)
     * 기타
          + Monocypher (암호화 라이브러리)
          + Tcl 언어 구현

  2. 코드 읽기 vs 문서/설계 학습

     * 코드만으로 한계
          + 코드베이스는 구현을 보여주지만 디자인 의도나 트레이드오프는 숨겨져 있음
     * 설계 문서 중요성
          + ADR(Architectural Decision Records), Rust RFCs, Python PEPs 같은 의사결정 기록이 디자인 학습에 훨씬 유익
          + 디자인 문서 작성 자체가 훈련이 될 수 있음
     * 책·문헌 추천
          + The Architecture of Open Source Applications
          + Code Reading (Spinellis)
          + Beautiful Code (O’Reilly, ISBN-10: 0596510047)
          + Refactoring / Legacy Code 관련 서적

  3. 실습 중심 학습론

     * 경험과 시행착오
          + 디자인은 문제를 반복적으로 겪고 피하는 법을 배우면서 익히는 것
          + 코드 읽기만으로는 학습이 되지 않고, 직접 작성하고 실패를 해결하는 과정에서 배움
     * 흥미 기반 학습
          + 자신이 흥미 있는 프로젝트를 만들어야 깊이 배우게 됨
     * 실패 비용이 낮은 특성
          + 소프트웨어는 물리적 엔지니어링보다 실패 비용이 낮아 시도와 실패를 통한 학습이 효과적

  4. 소프트웨어 엔지니어링 성격 논쟁

     * 미성숙한 공학론
          + 다섯 명의 엔지니어가 모이면 다섯 가지 다른 해법이 나오는 것은 공학으로서 미성숙함의 증거
     * 실험 친화적 특성론
          + 소프트웨어는 제약이 적어 다양한 해법이 존재하고, 물리 공학처럼 정답이 고정되지 않음
     * 예술과 공학의 경계
          + 디자인은 미적 요소를 가진 예술적 행위이기도 하지만, 기능적 요구를 만족하는 측면에서는 공학
          + 소프트웨어는 예술적 유연성과 엔지니어링적 엄밀성 사이에 놓여 있음

  5. 대안적 학습 방법

     * 나쁜 코드 분석
          + 잘 설계된 코드뿐 아니라 부실한 코드베이스를 고쳐보는 것도 큰 학습 효과
     * 자신의 코드베이스 학습
          + 팀 내부 코드베이스를 가장 많이 배울 수 있는 자료로 꼽음
          + 단, 팀 코드가 부실할 경우 외부 사례 병행 필요
     * 도메인 맞춤 학습
          + 자신이 풀고 싶은 문제와 유사한 코드베이스를 읽는 것이 가장 효과적

주요 인사이트

     * 잘 설계된 코드베이스는 도움이 되지만, 학습은 설계 의도를 이해하고 시행착오를 겪으며 병행해야 함
     * 코드 읽기 자체보다 설계 문서와 의사결정 기록이 핵심 학습 자료임
     * 대표적인 고품질 프로젝트(Git, Postgres, CPython, Rust std 등) 는 학습 가치가 높음
     * 좋은 코드뿐 아니라 부실한 코드와 자신의 코드에서 배우는 것이 장기적으로 더 실질적임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 댓글 정리

  대표적 코드베이스 추천 (CraigJPerry)

     * Postfix mail server
          + 보안 중심 아키텍처로, 마이크로서비스 개념 이전에 이미 유사한 구조를 보여줌
          + 현대적 마이크로서비스가 대규모 조직 분산에 중점을 둔다면, Postfix는 보안과 단순성을 위해 설계됨
     * Spring Framework
          + 엔터프라이즈 환경 Java 개발자의 요구를 깊이 고려한 문화가 반영됨
          + 사용자 중심 설계 접근을 배울 수 있음
     * Git
          + 객체 데이터베이스(블롭, 트리, 커밋)와 레퍼런스 개념을 이해하면 나머지는 점진적 확장임
          + 핵심 개념의 일관된 확장이 좋은 설계 사례로 제시됨
     * Varnish
          + 고성능 리버스 프록시이면서, 학습 도구로 활용될 수 있을 정도로 잘 구성된 코드베이스
     * Linux Kernel Lieutenants Model
          + 코드베이스는 아니지만, 대규모 소프트웨어 관리 모델로서 참고할 가치가 있음
     * 단순히 ""잘 설계된 코드""라기보다 디자인 의사결정이 강한 인상을 남긴 사례들임

  실무 코드베이스 학습 강조 (crystal_revenge)

     * 가장 큰 학습 가치는 자신의 팀 코드베이스에서 얻을 수 있음
     * 실제 요구사항과 구현 간의 혼란스러운 연결 과정에서 좋은/나쁜 선택을 동시에 경험하게 됨
     * 현실적 제약 중 가장 큰 요소는 시간 압박이며, 이상적 설계와 현실 사이의 균형을 익히는 것이 핵심
     * 좋은 소프트웨어란 사용자 요구를 해결하는 것이며, 반복 경험을 통해 성공 가능성을 높이는 설계를 배우게 됨

  과거 논의와 자료 링크 (sprobertson)

     * HN에서 같은 주제가 여러 차례 다뤄졌음
          + Ask HN: What are some of the most elegant codebases in your favorite language?
          + Ask HN: Codebases with great, easy to read code?
          + Ask HN: Good Python codebases to read?

  코드 vs 설계 문서 (alphazard)

     * 코드베이스는 구현의 결과물일 뿐, 설계 자체는 아님
     * 설계 학습에는 디자인 문서 작성이 더 효과적임
          + 문서는 다른 사람이 그대로 구현할 수 있을 정도로 명확해야 함
          + 대안을 열거하고 왜 배제했는지 기록하면 설계 고려의 증거가 됨
     * 좋은 디자이너는 더 넓은 설계 공간을 고려하고, 적절한 지점을 선택하는 사람임

  전체 시스템 이해 강조 (RossBencina)

     * 코드베이스 전체를 이해하는 과정은 매우 가치 있음
          + 단순히 잘 설계된 코드뿐 아니라 시스템의 큰 그림을 보는 훈련이 됨
          + UML 등 다이어그램을 통해 관계를 시각화하는 것이 도움됨
     * 학습 접근법:
          + 자신이 개발 중인 것과 유사한 소프트웨어의 코드를 읽는 것이 효과적
          + 이미 잘 아는 도메인의 코드(웹 프레임워크, 웹 서버, Python 표준 라이브러리, VSCode 등)를 시작점으로 추천
          + 처음에는 작은 프로그램, 익숙한 도메인부터 접근하는 것이 좋음

  좋은 설계의 기준 (mamcx)

     * 좋은 설계는 목표와 아이디어, 코드베이스는 그것의 구현 정도를 보여줌
     * 좋은 설계는 단순히 ""빠르다, 안전하다""는 수식어가 아니라 구체적 고려와 트레이드오프 기록을 포함해야 함
     * 사례: Erlang, 초기 Pascal, 다수의 RDBMS 설계에서 이러한 특징을 관찰 가능
     * Rust의 std 라이브러리는 보안과 일관성을 강조하며 코드와 문서가 이를 충실히 반영하는 좋은 학습 자료

  보이지 않는 설계 결정 (ben30)

     * 잘 설계된 코드베이스를 볼 때 가장 중요한 부분은 보이지 않는 것임
          + 복잡성 배제, 불필요한 추상화 지양, 특정 패턴 거부 같은 부재의 결정이 핵심
     * 이를 보완하기 위해 ADR(Architectural Decision Records) 를 활용
          + 대안과 그 배제 이유, 선택 근거를 기록하여 맥락을 남김
          + 향후 유지보수자와 AI 도구 모두에게 큰 도움을 줌
     * 학습 시, 단순 코드 외에도 ADR·RFC·PEP 등 설계 의사결정 문서가 함께 있는 프로젝트를 살펴보는 것이 효과적

   스팸 댓글로 뒤덮힌 블로그의 2009년 글을 레퍼런스로 삼다니..

        Hacker News 의견

     * 나만 그런 건지는 모르겠지만, 실제로 여러 번 문제에 부딪혀보고 어떻게 피해갈지 스스로 터득하는 방식이 가장 효과적임을 느낌. 그러다 보면 자연스럽게 앞으로 어떤 문제가 생길지 머릿속에서 시뮬레이션할 수 있게 되고, 결국 디자인이라는 것도 다양한 미래 문제들을 미리 예측해서 피하고, 어떤 문제를 어느 정도의 노력으로 피할 건지, 한 번의 디자인으로 여러 문제를 해결할 수 있는지 등의 트레이드오프를 신중하게 따지는 판단임
          + 나도 마찬가지임. ""연습문제 풀기""나 ""코드베이스 공부하기"" 같은 방식은 나한테 맞지 않았음. 내가 정말 흥미 있는 걸 만들어갈 때, 예시에서 정확함과 꼼꼼함을 추구하면서 자연스럽게 배움. 근데 가끔씩은 아직 내 실력이 부족하다고 생각됨. 마치 책 읽듯이 코드를 술술 읽어서 머릿속에 그 동작이 그려진다면, 공부 자체도 더 재미있을 거라고 생각함
          + 여러 번 문제에 부딪혀보며 배운다는 이야기를 듣고 느낀 점은, 소프트웨어 엔지니어링 분야가 아직 성숙하지 못했다는 것임. 만약 다리나 집을 그런 식으로 짓는다면 어떨까, 아니면 외과의사가 그런 식으로 훈련된다면 어떨까 상상해보기만 해도 위험함. 시간이 지나면 표준이나 규범이 자리 잡겠지만, 지금은 그야말로 유동적인 상황임. 소프트웨어 엔지니어 다섯명을 모아놓고 문제를 제시하면 다섯 개의 전혀 다른 해결책이 나올 뿐만 아니라, 어떤 방법이 옳은지에 대해 강하게 의견 충돌이 생김. ""좋은 해법은 보면 알아본다""라는 태도만으로는 제대로 된 엔지니어링이 될 수 없다고 생각함
          + 나 역시 지금까지 이 방식으로 꾸준히 학습해왔고 앞으로도 계속 그럴 것임. 실제 프로덕션 코드로부터 배우는 건 내겐 새로운 도전이 될 듯함. 얼마나 가치 있을지는 모르겠지만 어쨌든 재미는 있을 것 같음
          + 마치 운전 연수를 처음 차에 올라타서 사고를 내고, 그 뒤로 반성해서 다음엔 사고 안 나게 차 몰아본다는 식과 비슷하다고 생각함. 실제로는 이 두 가지 방법이 모두 필요함. 우리가 일반 도로 주행 시에는 한계를 넘는 상황이 거의 없어서 바로 적용되긴 힘들지만, 레이싱같은 경우에는 도대체 어디까지 밀어붙일 수 있는 한계선이 어디인지 정확히 알아두는 게 필수임. 물론 우리의 직업을 경기처럼 취급할 필요는 없다고 봄. 하지만 책을 안 읽고 공부를 안 하면 말 그대로 아주 오랜 시간이 걸릴 뿐만 아니라 사고와 고장도 끝이 없을 것임. 그래서 나는 무조건 읽고 또 읽으라고 말하고 싶음. 당장에는 그 필요성을 못 느낄 수도 있지만, 나중에 언젠가 경험을 쌓으면서 그 지식이 어디선가 반드시 쓰일 거고, 내리막길에서 처벌질하며 저단 기어로
            변속해야 하는 걸 처음 배운 상황처럼 당황하지 않을 수 있음
          + 이게 바로 핵심임
     * 이 질문을 듣고 가장 먼저 드는 생각은 ""팀의 코드베이스""임. 실제 문제에 대해 좋은 해법과 나쁜 해법이 각각 왜 채택됐는지 깊이 이해하는 것만큼 소프트웨어 디자인을 효과적으로 배우는 길은 없음. 소프트웨어란 본질적으로 유저 요구사항과 기계의 동작 사이를 연결해주는 복잡한 중간층 그 자체임. 만약 이 혼란이 없었다면 이미 자동화됐을 것이고, 소프트웨어 자체가 필요 없게 됐을 것임. 소프트웨어란 어떤 이상형만을 좇다보면 오히려 나쁜 의사결정을 자주 하게 됨. 실제로는 ""왜 이런 압력에 의해 특정 선택이 이뤄졌는가""를 깨달으며 시행착오를 줄이게 됨. 동시에 빠르고 효과적으로 일하는 실전적인 방법론을 익혀야 함. 현실에서 가장 큰 도전은 시간 제한임. 이건 이론적인 소프트웨어 설계에서 거의 다뤄지지 않는데, 현실적으로는 항상
       코드를 빨리 내야 한다는 압박을 받으면서 작업하게 됨. 하고 싶은 방식이나 최선의 방법을 쓸 시간이 부족한 경우가 많음. 좋은 소프트웨어란 유저의 실질적 니즈를 풀어주는 소프트웨어임. 앞으로 성공적인 실행을 더 자주 만들어내는 디자인 해법들이 존재하고, 그것들을 찾는 최선의 길은 실제 내가 작성하는 코드를 깊이 살펴보는 것임
          + 만약 질문자가 자신의 팀 소프트웨어가 엉망이라고 느껴서 외부의 조언을 구하려는 경우라면 어떻게 해야 할까? 아니면 단순히 대학생이 질문하는 것이라면?
          + 구현에 필요한 시간도 디자인 퀄리티를 판단하는 척도가 될 수 있다는 건 생각 못해봤지만, 정말 타당하다고 느낌. 나 역시 내 팀 코드베이스에서 엄청 많이 배우고 있음. 그중 대부분은 뛰어난 디자인에서는 좋은 것들을, 그렇지 못한 부분에서는 직접 구글링하면서 계속 배우게 됨
     * 몇 년 지난 자료이긴 하지만 많은 오픈 소스 프로젝트 리더들이 기고한 ""The Architecture of Open Source Applications"" 시리즈가 있는데, 온라인에서 무료로 볼 수 있음
       https://aosabook.org/en/index.html
     * 해당 질문은 이미 여러 번 나와서, 참고할 수 있는 링크들을 모아봄
          + https://news.ycombinator.com/item?id=36370684
          + https://news.ycombinator.com/item?id=30752540
          + https://news.ycombinator.com/item?id=9896369 (Python 특화)
     * Yanderedev 소스 코드
     * 나는 정답을 말할 만큼 자격이 충분하진 않지만, 약 15년 전 ""Code Reading""이라는 책을 정말 재미있게 읽었음. 이 주제와 딱 들어맞는 책임
       https://www.spinellis.gr/codereading/
       목차 참고: https://www.spinellis.gr/codereading/toc.html
       유사한 이름의 책도 있었던 것 같은데 정확히 기억은 안 남
     * 사실상 코드베이스에는 디자인이 아닌 구현이 담기는 경우가 더 많음. 예를 들어 다른 언어로 완전히 갈아엎더라도 디자인만은 유지될 수 있음. 디자인 문서 작성 연습을 추천함. 문서가 어떤 모양이어야 하는지 신경 쓰지 말고, 템플릿에 얽매일 필요도 없음. 무엇보다 중요한 건 이 문서를 다른 사람이 받아들고 그대로 구현할 수 있어야 한다는 점임. 그리고 문서 자체가 ""고려의 흔적"" 역할을 할 수 있음. 어떤 방식을 선택하고, 다른 대안이 있었지만 왜 그걸 선택하지 않았는지 명확히 남겨두면 됨. 미리 대안을 인정하고 비교해둠으로써 독자에게 충분히 고민했다는 신뢰를 줄 수 있음. ""좋은"" 시스템 설계자가 하는 일은 남들보다 더 큰 디자인 영역을 살펴보고, 일관성 있게 좋은 포인트를 찾아내는 것임. 문제를 하나 정하고, 다양한 디자인 공간을
       실험해보고, 그중 어느 것이 더 나은지 이유를 설명해서 다 기록하면 됨
     * 가장 먼저 ""내가 풀고자 하는 문제는 무엇인가?""라는 질문부터 출발해야 함. 그 문제를 해결한 코드베이스를 찾아서 실제로 어떻게 구현했는지 집중적으로 분석하기. 좋은 디자인은 특정 도메인 상황과 아주 밀접하게 엮여 있다는 사실을 명심해야 함. Wonham의 Internal Model Principle도 코드에 해당된다고 생각함. 예를 들어, 임베디드 타겟을 위한 단위 테스트 문제를 해결하고 싶어서 관련 오픈 소스 프로젝트를 분석했고, 각 코드가 어떻게 작성됐는지 그 이유를 비판적으로 살펴봤음. 직접 나만의 솔루션을 만들면서 또다시 이전 코드들을 참고해가며 내 도메인 이해도가 깊어질수록 더 많은 걸 배워가고 있음
     * 내 경험(소프트웨어 30년, 아키텍처 실무 25년, MIT 시스템 아키텍처 석사)으로는 추상적으로 ""좋은"" 디자인이란 원래 존재하지 않음. 분명히 나쁜 설계(안 좋은 결과를 초래하는 디자인)는 있지만, ""좋은""의 기준은 맥락에 따라 달라짐. 무엇을 만드는지, 안전/보안 같은 요구사항, 그리고 무엇보다 실제로 구현하는 팀과 팀의 구조가 가장 큰 영향을 미침. 주니어들만 모인 팀에서는 정교한 설계를 곡해해서 망칠 수도 있음. Conway의 법칙처럼 개발팀의 구조가 소프트웨어에 그대로 반영된다고 봄
          + 소프트웨어 디자인을 배워가면서 확실히 깨달은 점은 만능 해답이 없다는 사실임. 그런 게 있으면 참 좋을 텐데, 다양한 아키텍처와 패러다임 속에서 최적 해법을 고르는 게 답답할 때도 있음. 그래도 요구사항만 제대로 반영해도 선택지는 꽤 줄어듦. 지금은 안전성이 특별히 중요한 임베디드 시스템을 다루고 있는데, 이 환경 덕분에 다른 상황에서는 아예 쓸 일 없는 결정을 종종 하게 됨
          + 결국 핵심은 정말 어처구니없는 타협안만 피하고, 최악의 선택들도 넓은 시야로 볼 때는 피해가는 것임. 현장에서 늘 느끼는 건, 최고의 아키텍처라기보다는 ""근본적으로 나쁜 결정 피하기""의 연속 싸움이라는 점임
     * 예전에 아주 간단한 추천 목록을 만든 적 있는데, 아직도 유효함
       https://medium.com/@012parth/…
"
"https://news.hada.io/topic?id=22724","미국 정부는 인텔 주식을 소유해서는 안 됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        미국 정부는 인텔 주식을 소유해서는 안 됨

     * 미국 정부가 Intel 지원금을 지분 전환하겠다는 발표는 국가 안보 강화 목적과 어긋나는 정책적 실수라는 지적
     * 2022년 제정된 Chips and Science Act는 국내 반도체 제조 부흥을 목표로 했으며, 세금 수입이 아닌 안보와 공급망 회복력을 국민의 투자 수익으로 삼음
     * 현재까지 5,000억 달러 이상의 반도체 제조 투자가 미국 내에서 발표되었고, 주요 글로벌 선도 기업들이 모두 미국 내 생산을 확대하고 있음
     * 그러나 인텔의 파운드리 부문은 130억 달러 적자를 기록하며 외부 고객을 확보하지 못해 어려움을 겪고 있음
     * 정부 지분 참여는 문제 해결에 도움이 되지 않고 오히려 시장 왜곡·정치적 위험을 초래해 미국 경쟁력을 약화시킬 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Chips Act의 본래 목적

     * Chips Act는 지난 수십 년간 미국 내 반도체 제조 기반 약화를 되돌리기 위해 설계된 법안임
          + 미국은 AI, 통신, 국방 시스템을 뒷받침하는 첨단 칩을 거의 전적으로 대만에 의존하고 있었음
          + 이러한 의존은 심각한 안보 리스크로 지적되었음
     * 법안은 보조금, 대출, 세액 공제를 통해 아시아 대비 제조비용 격차를 줄이고 미국 투자를 유도하는 구조임
     * 투자 수익은 세수 확대가 아닌 국가 안보 강화와 공급망 안정성 확보로 평가됨

현재까지의 성과

     * Chips Act 이후 30년간 합친 것보다 많은 미국 내 전자 제조 투자 유치 성과가 발생함
     * 5,000억 달러 이상의 신규 투자 발표
     * 인텔, 삼성, TSMC 포함 5개 글로벌 선도 기업이 모두 미국 내 생산 시설 확장을 진행 중임
     * 이는 타국에서는 볼 수 없는 성과로 평가됨

인텔의 현황

     * 인텔의 제품 부문(PC, 서버 칩 설계) 은 수익성이 크지만 안보와 직접 연관되지 않음
     * 파운드리(제조 부문) 가 안보 핵심이지만, 2023년 130억 달러 적자를 기록하고 외부 고객 확보에 실패함
     * 인텔의 18A, 14A 공정은 외부 고객 확보가 필수적이나 성과가 부진한 상태임

정부 지분 참여의 문제점

     * 인텔은 이미 공개시장에서 자본 조달 능력을 갖추고 있음
          + SoftBank의 20억 달러 투자 사례가 이를 입증함
     * 보조금을 지분으로 전환하면 인텔의 비용 경쟁력 악화 우려 발생
          + 아시아 저비용 제조사들과의 격차 확대 가능성
     * 정치적·실무적 위험도 존재
          + 선거철 인텔 감원 시 정부가 이익을 취하는 모습으로 비칠 수 있음
          + 미국 정부가 주주로서 개입 시 삼성·TSMC와의 공정 경쟁 훼손 우려
          + 국가 전략 결정 시 주주 이해와 국가 이익 충돌 위험

대안적 접근

     * 인텔 문제는 고객 확보 부족에 있음
     * 정부는 대형 고객이 공급망 다변화를 통해 인텔 파운드리를 이용하도록 유도하는 것이 바람직함
     * 이는 시장 효율성에 반하더라도 AI 경제의 단일 공급업체 집중 리스크를 줄이는 전략적 선택임

이미 존재하는 보호장치

     * Chips 보조금에는 성과 공유 조항이 포함되어 있어, 기업이 초과이익을 내면 납세자에게 일부 환원됨
     * 보조금은 고객 확보, 기술 준비도, 생산 목표, 건설 진척도 등 이정표 달성과 연계되어 있음
     * 이번 인텔 거래는 이러한 이정표 연계를 무력화하고 현금 선지급 방식으로 바뀌어 정부의 통제력이 약화됨

결론

     * Chips Act는 미국식 산업정책의 첫 대규모 실험으로 평가됨
     * 정부 지분 참여는 특정 경우(민간자본 유치가 어려운 전략적 스타트업)에서는 유효할 수 있음
     * 그러나 인텔의 경우 시장에서 자본을 조달할 수 있어 지분 참여는 불필요하며, 경쟁력 약화·정책 리스크만 초래함
     * 따라서 보조금 기반 지원이 최적 도구이며, 지분 참여는 잘못된 접근으로 결론지을 수 있음

        Hacker News 의견

     * WSJ Opinion 기사에서 논의되는 국가 소유 문제에 대한 의견임
     * 개인적으로 Intel이 망하게 두는 게 맞다는 생각임. 파산하면 기존의 공장 및 장비를 더 효율적으로 운영할 수 있는 다른 반도체 기업들이 아주 저렴한 가격에 인수할 수 있음. 현재는 다수의 기업들이 팹리스 모델이긴 하지만, 인수 기회가 생기면 제조로 전환할 것도 같음. TI, Micron, GF는 이미 직접 팹을 보유함. Qualcomm도 이런 기회엔 도전해볼 만하다고 생각함. 물론 이 모든 게 인력과 노하우까지 함께 따라온다는 전제하의 이야기임
     * 10여 년 전 자동차 산업 구제금융 같은 사례를 보듯, 이미 미국 정부는 철도, 농업금융, 항공, 자동차, 은행, 저축대부 등 주요 산업이 위기에 처할 때마다 개입해서 시스템 붕괴를 막아왔음. 그 덕분에 보통 시민이 더 큰 피해를 입는 상황을 방지했음. 때론 정부가 대출을 보증해주거나, 저리로 자금을 융통해주거나, 회사의 지분을 아예 인수하는 방식(시장가로 발행된 주식을 사들임)으로 진행된 적도 있음. 관련 참고 자료를 참고하면 좋을 것 같음
          + 이런 구제금융 결과로 산업 전체는 여전히 지속적으로 정부의 지원이 필요하게 되었음. 2008~2009년 금융위기 때 자동차/은행/저축대부, 2020~2021년 코로나로 항공 등 사례가 그 증거임. 일자리 문제와 정치적 영향력이 커지면서 백스톱(정부 개입)을 더 이상 없애기 어렵게 됨. 그래도 이런 지원이 글로벌 경쟁력을 보장하진 않음. 그리고 이번 건은 산업 전반이 아니라 특정 기업, Intel만을 겨냥한 것이 특이함. CHIPS Act의 시작은 2020년 TSMC의 미국 투자 유치였고, 이후 삼성, Intel도 세제 혜택이나 대출보증, 보조금 등으로 유인해왔음. 하지만 정부가 Intel의 지분까지 갖게 되면 TSMC/삼성을 더 유치하려던 당초 전략이 불분명해지는 중임. 혹시라도 관세 등으로 타사 견제하려는 방안이 떠오르는데, 그건 오히려 혁신을 저해할 것 같음. Intel 입장에서는 그게 더
            이득인 상황임
          + 정부는 위기를 안정화시키기 위해 개입한 것이 아니라, 그저 기존 질서를 유지하기 위해 나선 것으로 봄. 시장은 때때로 혼란을 겪으며 성장하는 게 정상임. 정부가 이런 개입으로 시장을 파괴하고, 결국 책임을 회피하는 경향이 있음. 산업 전체가 부실해질 수 있다는 논리가 타당한지는 의문임. 완전히 독점에 가까울 때만 해당한다고 생각함. 그리고 굳이 정부가 아니라 누구에게나 지분을 팔면 되는데 왜 특별히 정부가 개입해야 하는지 납득이 안 감. 이런 의견들은 시카고 학파 사상이 미국 경제에 오래 영향을 미쳐왔다는 점과도 연관이 있음
          + ""우리가 자동차 구제금융 시절 제대로 선을 넘은 건 이미 오래 전 아닌지?""라는 의문에 대해, 이번 CHIPS Act 관련 정부의 부분지분 인수는 새로운 방식임을 강조함. 애초에 처음부터 지분 매입 구조였다면 납득이 쉬웠겠지만, 지금 방식은 좀 다름
          + 이번 건은 구제금융이라기보단 강압임에 가까움
          + 최근 인슐린의 역사에서 캐나다의 Canada Development Corporation(약칭 CDC)에 대해 알게 되었음. 이곳은 민관합동 투자로 캐나다 기업을 보호하고 육성하기 위해 만들어졌고, 정부가 통제하지만 수익도 목적으로 삼았음. 석유, 광업, 화학, 심지어 인슐린 개발사인 Connaught Laboratories도 인수했었으며, 정부 지분율이 크게 늘어난 적도 있음. 그러나 1986년 민영화 정책으로 해체됨. 위키백과 참고
     * 이 기사에서는 ""보조금 대신 지분 투자는 Intel에 더 큰 비용을 유발하기 때문에, Intel의 경쟁력을 저해한다""고 주장하는데, 그 비용이 명확히 설명되지 않아 궁금함. 주가 하락이나 직원 보상 감소, 그리고 제품 가격 상승 같은 영향까지 거론하는 건 추측이라고 봄. 반대로 초기 일시적인 자금 투입은 일정 역효과 대신 프로젝트의 일정 단축 같은 장점도 있을 수 있음
          + 사실 이런 식의 비용 논리는 별로 설득력이 없음. 그냥 ""정부가 '무상'으로 돈을 주는 게 아니라 소유권까지 요구하니 그 선은 넘지 말자""는 주주들의 불만 표출로 보임. 단순생산성 논리가 WSJ 독자층에 먹힐 수 있지만, Intel이나 미국 산업 구조 변화를 잘 아는 사람들은 아이러니하게 생각할 수 있음
     * 정부가 기업에 개입하는 게 문제라고 보진 않음. 다만 최근엔 계획과 의도 없이 즉흥적으로 하는 듯 보여서 문제임. 정부는 원래 오래전부터 인프라 투자나 연구개발 지원을 통해 Intel의 성장 기반을 마련해줬음. 장기적 투자가 어려운 분야에 투자하는 건 긍정적임. 한편, 미국 반도체 산업 자체가 정말 쇠퇴 중인지 의문임. 애플, Nvidia, Google 등도 자체 칩 생산 역량이 상당함
          + 다만 Apple, Nvidia, Google, AMD 모두 칩 설계 능력은 뛰어나지만, 실제 생산 공장은 TSMC나 삼성에 의존함. 직접 제조는 불가능함
          + 현재 최첨단 공정 노드로 칩을 직접 생산하는 미국 기업이 있냐 하면 그게 의문임. Texas Instruments, Global Foundries, 심지어 Intel도 삼성/TSMC 수준에 미치지 못함. 그래서 Intel의 도태는 국가 안보 차원에서 중요한 문제임
          + 결국 정부는 도로, 경찰, 학교 등 기반 서비스를 통해 모든 기업을 간접 지원함. 정부가 모든 기업의 지분을 갖고, 국민이 그 수익을 나눠 가지는 모델이 더 타당하다고 생각함
     * TSMC 자체도 정부 지원을 받아 성장했음. 첨단 반도체 공장 건설엔 민간이 감당할 수 없는 비용이 들어서 정부의 장기 투자가 필요함. 지금 논란은 Intel 지적재산권이 아니라 미국 내 첨단 생산시설 유지가 핵심임
          + 이런 것도 다 국가간 공정경쟁이 존재하지 않는 현실 때문임. 중국 등 타 국가가 같은 규칙을 따르지 않는데, 미국이 어떻게 경쟁에서 이길 수 있을지 의문임
          + TSMC가 중국 해안에서 불과 81마일 떨어진 곳에 있다는 점을 봤을 때, 첨단 반도체의 전세계 유일 생산처를 거기에 두는 것도 위험하다고 생각함
     * 이미 정부가 Intel 지원을 약속한 상황이니, 이제와서 철회하면 미국과 Intel 모두에게 피해가 클 것임. 따라서 아무 조건 없이 지원하긴 어려우니, 정부 역시 어떤 식으로든 대가를 요구하는 건 필요한 일임
          + 특히 민간 기업 대규모 지분을 십여 년간 비판하더니 이제는 반대당 주도로 그런 정책이 추진되는 점에서 아이러니함. 원래 판이 뒤집혔으니 새로운 타협도 이해가 가지만, 차라리 조건 명확히 공개하고, 예를 들어 배당금/자사주 매입 동결, 부채 상환 후에 주식 경매로 정부 지분 처분 등 구체적 조건을 달자는 의견임. 모호하고 즉흥적인 국가 개입이 오히려 미국 시스템에 대한 혼란만 야기함
          + 실제로 CHIPS Act는 ""성장 후 이익공유""가 원래 조건이었는데, 트럼프 행정부가 이를 지분 인수 방식으로 바꿨음. 이제 연방정부는 언제든 주식을 매각하고 돈을 회수할 수 있음. 이게 오히려 단순 이익공유보다 더 급진적이고 위험한 방식임. 실제로 아무 대가 없는 지원 조건은 아니었다는 점도 강조함. 관련 기사
          + 지원의 목적 자체가 미국 내 생산능력 확보라는 전략적 가치 때문임. 결국 $10B라는 정부 자금 조달은 Intel 주주 입장에서 보면 배임이 될 수도 있음. 회사가 주식을 새로 찍어 정부에 넘기면, 실제로 자산은 그대로고 기존 주주는 수익을 확보하지 못함
     * 지금 상황은 아웃소싱이 초래한 결과임. 저가 전자산업은 아시아로 보내면서 고급 제조 경쟁력까지 잃었고, 결국 정부 보조금 없인 고급 반도체도 못 만드는 처지에 놓였음. 공급망이 아시아로 완전히 이동하고, 자유무역과 주식시장 거품을 맹신하다 보니 초부유층만 더욱 돈을 벌게 됐음. 이제 그 후과를 받아들이는 시간임
          + 내 시각엔 그보다는 Intel이 반복적으로 기회를 놓친 게 더 큰 이슈임. 모바일, 데이터센터용 커스텀 칩, 그래픽카드, AI, 그리고 위탁 제조까지 여러 기회를 전부 놓쳤음. 지난 5년 동안 칩 성능 격차도 거의 벌어지지 않았고, 요즘 M-시리즈 칩은 Intel 대비 1/10 전력으로 동등 성능 제공함. 이런 건 공급망 아웃소싱보다는 경영력 부재가 원인임
          + 이제 H1B와 오프쇼어링 이슈까지 언급해볼 만함
     * 정부가 기업을 여러 형태로 지원하는 것 자체엔 반대하지 않음. 하지만 ""왜 지원하는가, 실패하면 어떤 파장이 있는가?""가 핵심임. 과거 금융, 제조 등 수없이 다양한 방식으로 개입한 적 있음. Harley-Davidson 사례처럼 수입 오토바이에 관세를 매긴 전례도 떠올라야 함. 진짜 중요한 질문은 지원 방법과 기간임
     * 흔히들 ""이익을 낼 땐 자본주의, 손해 보면 사회주의"", 또는 ""이익은 민간, 손실은 사회화""라는 말을 많이 하는데, 이번 케이스도 마찬가지임. Intel은 미 정부와 아주 밀접해서 국가안보상 중요한 자산임을 주장하지만, 같은 미국 기업인 AMD나 그 외 기업 입장에선 억울할 수 있음. 만약 제조공정 노드가 문제였다면 미리 공장에 투자했어야 하는데, 정부 지원 덕에 Intel이 한참 성장하지 않았냐는 생각임
          + 만약 Intel의 주가가 오르면 정부는 지분을 팔아 세금 이상의 이익을 거둘 수도 있음. 진정한 '사회화된 손실'은 그냥 무상 보조금처럼 돌려받는 게 없이 주는 경우고, 실제로 국유화 방식은 오히려 자본주의에 더 가까울 수도 있음
          + ""자본주의가 정말 훌륭하다면 왜 매 10년마다 사회주의적 구제금융으로 살아나야 하는가?""라는 얘기가 생각남.
"
"https://news.hada.io/topic?id=22705","Comet AI 브라우저는 어떤 사이트에서도 프롬프트 인젝션이 가능하며, 은행 계좌를 소진할 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Comet AI 브라우저는 어떤 사이트에서도 프롬프트 인젝션이 가능하며, 은행 계좌를 소진할 수 있음

     * Comet AI 브라우저에서 발생하는 보안 취약점 이슈임
     * 악의적인 웹사이트가 브라우저 내 AI 에이전트를 통해 원치 않는 프롬프트 인젝션을 발생시킬 수 있음
     * 이 취약점을 악용하면 사용자의 개인 정보 유출이나 중요 행동 유도가 가능함
     * 심각한 경우, 자동화된 행동을 통해 은행 계좌의 자금 이체와 같은 피해 초래 가능성 존재함
     * 사용자와 개발자 모두 이 신종 AI 브라우저 위협을 인지하고 대응 방안 마련 필요성 대두됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Comet AI 브라우저의 보안 위협 개요

     * Comet AI 브라우저는 웹 페이지와의 상호작용에서 내장 AI 에이전트를 활용하는 차별점으로 주목받음
     * 최근, 해커가 의도적으로 설계한 웹사이트를 접속할 경우, 이 AI 에이전트가 해당 웹사이트의 악성 프롬프트에 노출되고, 실행까지 이어질 수 있음
     * 프롬프트 인젝션 공격을 통해, 사용자가 원하지 않은 계정 정보 유출, 명령 실행, 심지어 금융 트랜잭션 등 심각한 피해 가능성이 높아짐
     * 이 문제는 기존 브라우저 보안 모델에 AI 상호작용이 추가되면서 나타나는 새로운 유형의 취약점임

프롬프트 인젝션의 메커니즘

     * 악성 웹사이트는 웹페이지에 특수한 명령 혹은 질문 형태의 텍스트를 삽입함
     * AI 브라우저가 이를 '정상적인 사용자 요청'으로 오해하고, 자동으로 해당 명령을 수행함
     * 이로 인해, 예를 들면 계좌 이체, 민감 정보 복사, 타 사이트 자동 로그인 등 자동화된 행동이 유발될 수 있음
     * 사용자는 이러한 프로세스가 보이지 않거나, 의심 없이 넘길 수 있기 때문에 탐지 및 방어 난이도가 높음

업계 영향과 대응 필요성

     * AI 브라우저의 확산과 함께, '프롬프트 인젝션'과 같은 신종 위협이 실제 위험으로 부상함
     * 서비스 개발자와 사용자 모두 AI 기반 자동화 기능 사용 시 강력한 검증 및 통제 시스템 필요함
     * AI 브라우저 기업 및 보안 기업에서 사전 필터링, 명령 실행 제한, 알림 시스템 등 보안 기능 개발 중요성 강조됨
     * 금융 등 고위험 영역에서는 AI 브라우저 사용 주의 및 강력한 보안 점검 필요함

결론

     * Comet AI 브라우저의 프롬프트 인젝션 리스크는 AI 기술 도입 가속화와 함께 증가하는 새로운 보안 과제임
     * 모든 이해관계자는 해당 위협을 구체적으로 인식하고, 기능 활성화 전 검증 및 최소 권한 원칙 적용 등 종합적 보안 전략 수립 필요성 높음

        Hacker News 의견

     * 구글, OpenAI, Anthropic 같은 기업들이 동일한 기능을 출시하지 않고, 대신 쿠키가 없는 잠긴 가상머신을 써서 웹을 탐색하게 한다는 건, 그만큼 기본적으로 위험하다는 의미임을 말하고 싶음
       브라우저 안에서 LLM이 탭 간 데이터까지 보는 것은 정말 최고의 위험 요소 조합이라는 생각임
       브레이브에서 이 취약점에 대해 설명한 게시물(https://brave.com/blog/comet-prompt-injection/)을 봤는데, 기본적으로 '이건 아예 해선 안 되는 일'이란 결론엔 닿지 않았다는 점이 흥미로움
       대신 모델 정렬, 사용자 위험 행동 탐지 등으로 충분히 막을 수 있다는 식임
       에이전트 권한 다운그레이드가 그나마 괜찮은 대응책으로 나오긴 했지만, 이 역시 이메일 발송만큼이나 쉽게 공격자가 제어하는 이미지 URL로 데이터 유출이 일어날 수 있다고 봄
       관련 선행 토론: https://news.ycombinator.com/item?id=44847933
          + 난 모델 정렬이나 가드레일이 결국 통계적 예방책이라 생각함
            입력 공간에서 위험한 동작이 절대적으로 0%가 되는 경우는 기대하기 힘듦
            모델이 아무리 좋아져도 입력값 중 ""절대 일어나면 안 되는 일""에 매핑되지 않는 케이스를 만든다는 것은 불가능에 가까운 욕심임
            모델 레이어를 여러 번 쌓는다고 해도, 본질적으로 확률 계산만 곱해질 뿐
          + (브레이브 프라이버시 리더이자 게시글 작성자임)
            우리가 모델 정렬이나 위험 행동 탐지 등만으로 충분하다고 주장한 적 없음
            그런 방법들은 브라우저 벤더라면 당연히 해야 하는 최소한의 조치임
            이런 단순 공격은 그런 조치들로 막을 수 있지만, '필요조건'일 뿐 결코 '충분조건'이라고 생각하지 않음
          + 브레이브 팀이 ""이건 아예 안 좋은 아이디어""라는 결론에 이르지 않은 걸 보고 든 생각
            업튼 싱클레어의 말처럼, 어떤 사실을 이해하는 데 연봉이 걸려 있으면 정말 이해하기 힘듦
          + 현재 글에서는 “브라우저가 에이전트 브라우징과 일반 브라우징을 분리해야 한다”는 점이 추가되어 있음
          + 클로드 코드에게 자동 승인을 허용해서 웹 탐색을 적극적으로 하게 하면, 프롬프트 인젝션으로 유사한 문제가 충분히 발생할 수 있음
            파일 읽기/수정 권한에 자동승인 옵션이 없더라도, 샌드박스 안에서 돌리지 않는 한, 생성된 코드가 다음에 유닛테스트를 돌릴때 등 브라우저 파일을 변경할 수도 있음
            모든 변경 사항을 꼼꼼히 검토하지 않으면 정말 위험할 수 있음
     * 내가 생각하기에 Agentic AI는, AI가 만든 변경 사항을 쉽게 롤백할 수 있는 환경에서만 써야 함
       예를 들어 코드 빌드/업데이트/디버깅엔 git 롤백 등으로 안전하게 대응 가능함
       그런데 웹브라우징 같이 롤백이 거의 불가능한 곳에 Agentic AI를 쓰는 건 믿기 힘든 일임
          + 내가 클로드에게 명확한 규칙과 지시사항을 줘도 종종 그것들을 무시하고 마음대로 동작하는 경우가 있음
            (“명확히 하지 말라고 한 규칙을 무시하고 DB를 바로 수정함!”)
            그래서 프로덕션 환경에선 에이전트 실행할 엄두조차 못 냄
          + git만으로 롤백 가능한 것처럼 보이지만, 실은 VM/컨테이너 레벨에서 롤백해야 안전함
            AI 코딩 툴이 모르게 파일/설정 구조가 수정될 수 있음
            예를 들어 bash로 악성 스크립트를 .profile에 추가하면, 다음 로그인 때 공격 코드가 실행될 수 있음
          + 이 기능이 리포지토리와 푸시가 가능한 모든 리모트까지 삭제 또는 오염시킬 수도 있지 않을까 생각함
            프롬프트 인젝션이 가능한 자동화 체인이 원격 자원에 접근 가능하다면, 일단 침투당한 뒤엔 내부에서 문이 활짝 열릴 수밖에 없는 환경임
            내가 잘못 생각한 부분 있는지 궁금함
          + git으로 롤백 가능하니 안전하다는 이야기를 보니, 존 코너가 스카이넷 소스코드를 롤백해서 수백만 명을 구할 수도 있겠다는 생각이 듦
            음...
          + 애초에 코드 업데이트/빌드/실행 권한 자체가 너무 막강함
            결국 VM 등 안전한 환경에서만 돌려야 함
     * 네트워크 계층 하나하나를 수십 년간 어렵게 보안 강화했는데, 이제 사람들은 모든 비밀과 비밀번호에 대한 평문 API를 내어주고 있는 현실임
       또, Microsoft가 스크린샷을 저장한다고 그렇게 비난하던 분위기가 이런 에이전트에는 조용한 것이 아이러니하다고 느낌
          + 적어도 이건 내가 직접 브라우저를 설치하는 'opt-in' 방식임
            Microsoft 쪽은 거의 모든 윈도우즈 기기에 기본적으로 들어가는(초기엔 opt-out이었던걸로 앎) 스크린샷 DB를 만들고 있었으니 더 위험함
          + 어떤 사람들은 '유용한 에이전트'에 자신이 친구에게도 말 못할 데이터를 쉽게 넘긴다는 것도 흥미롭다고 느낌
            내 아내는 최근 ChatGPT에게 약 복용 스케줄링을 요청했는데, 식사, 식단, 수면, 각 약마다의 상호작용까지 다 고려해서 완벽한 계획을 뽑아줌
            덕분에 약을 잘못 복용한 이유도 밝혀졌음
            이렇게 친근한 에이전트의 말투 때문이다 보는데, 현실에서는 이런 정보 누출이 심각한 문제임에도 많은 사람들이 생각 없이 데이터를 넘기는 상황임
          + Microsoft 스크린샷 논란과 이 사안을 비교하는 건 사안의 본질을 흐릴 수 있음
     * LLM이 어떤 툴을 통해 데이터를 읽는 건 결국 그 내용이 LLM의 컨텍스트 창에 기록되는 쓰기 행위임
       도구의 범위에 untrusted arbitrary source가 허용되면, 그 순간 외부에 쓰기 권한을 내어주는 셈임
       이 한 가지만으로도 데이터 유출 가능성이 충분히 생김
       그 외 실제로 다른 시스템에 쓰기권한이 생기거나 부수효과까지 추가된다면 위험성이 기하급수적으로 커짐
     * 코미디 같지만, 이게 바로 요즘 IT 업계와 LLM 열풍의 현실 상태라는 점에서 정말 씁쓸함
     * Comet이 아마도 약간의 튜닝된 지시문 외에는 보호조치가 없다고 추정함
       최근 USENIX Security에서 깨달은 건, 아무도 multi-turn/agentic 프롬프트 인젝션을 제대로 다룰 방법을 모르고 있음
          + 어쩌면 프롬프트 자체를 SQL 문자열처럼 취급해서, 외부에서 오는 동적 유저 입력은 반드시 무조건 세탁(정제)해야 한다는 접근법을 써야 할 듯함
     * AI로 인한 변화가 참 흥미롭고, 새로운 시도들이 계속 나오는 걸 보면 기대가 큼
     * 혼란스러운 마음을 솔직히 고백하고 싶음
       아직 일반 온라인 뱅킹에도 겨우 적응 중이고, 모든 업체 앱을 설치하는 것도 거부하는 편임
       그런데 비결정적 존재(LLM)를 컴퓨터에 들여서 금융업무까지 맡기는 건 도저히 상상하기 힘듦
       요즘은 LLM이 대신 물건을 사거나 결제하는 것도 실제로 존재한다 해도, 논리적으로 이해는 되지만 현실 감각으론 미친 짓에 가까움
       이유는, 비전문가나 랜덤한 프롬프트 반응 시스템에게 금융을 맡기는 게 위험해서라기보다, 고객 입장에서 자율성과 권한을 엄청나게 포기하는데 그 결과가 과연 뭔지 의문이 들기 때문임
       나도 LLM을 좋아하고, LLM 브라우저도 분명 유용한 사례가 있을 것 같음
       다만 일반 대중이 쓸 만한 건 아니라고 생각함
       차라리 컴파일 과정을 거쳐서 직접 뭔지 알고 도입하게 강제하는 방법도 고려할 만함
          + 맞음, 혼동이 생기는 게 정상임
            현재 상황은 AI가 계좌 정보 등으로 직접 결제를 대신한다가 아니라, AI에게 공개적으로 계좌 정보를 포스팅하는 경우 등 임
            AI가 금융 업무 자체를 대행한다는 의미와는 다름
     * AI 기업들이 앞으로 정말로 신탁의무(fudiciary liability)를 질 의향이 있는지 궁금함
     * Comet agent를 5분간 써봤음
       “Amazon에서 기타 사줘”라는 한 문장만 줬는데(기타 종류, 예산, 브랜드 등 명시 없음), 결과적으로 이름도 잘 모르는 저가형 어쿠스틱 기타 3개를 내 장바구니에 담았음
       다행히 결제 단계까지는 가지 않았음
       이걸로 평가 끝임, 별 가치가 없다는 결론을 냄
          + AI가 숫자 세기에 취약하다는 말은 들었지만, 2라는 숫자부터 이렇게 못 세는 줄은 몰랐음
"
"https://news.hada.io/topic?id=22691","Nitro: 작고 유연한 init 시스템 및 프로세스 슈퍼바이저","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Nitro: 작고 유연한 init 시스템 및 프로세스 슈퍼바이저

     * Nitro는 임베디드, 서버, 데스크톱, 컨테이너에 모두 적용 가능한 초소형 프로세스 슈퍼바이저 및 init 시스템임
     * 시스템 상태를 RAM에만 저장하여 읽기 전용 파일 시스템에서도 무리 없이 동작하며, 빠르고 효율적인 이벤트 기반 설계를 제공함
     * 구성 방식은 단순한 스크립트 디렉터리 구조로, 복잡한 설정 파일이나 부가적인 빌드 과정 없이 서비스 관리가 가능함
     * Parmetrized 서비스, 견고한 재시작, 개별 서비스별 신뢰성 높은 로깅 기능 등 컨테이너, 임베디드 환경에 최적화된 기능을 지원함
     * nitroctl 툴을 통한 원격 제어, 신호 기반 동작 제어 등 높은 유연성과 통제력을 보장함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Nitro는 Linux에서 pid 1로도 사용할 수 있는 초소형 프로세스 슈퍼바이저임

   주요 적용 분야는 아래와 같음
     * 임베디드, 데스크톱, 서버 등 다양한 용도의 Linux 머신용 init
     * Linux initramfs의 init
     * Docker/Podman/LXC/Kubernetes 등 컨테이너 환경의 init
     * POSIX 시스템에서 권한 없이 동작하는 슈퍼비전 데몬

   구성은 디렉터리 기반 스크립트 구조를 사용하며, 기본 위치는 /etc/nitro임

요구사항

     * 커널의 Unix 소켓 지원 필요
     * tmpfs 또는 쓰기 가능한 /run 디렉터리 필요

다른 시스템 대비 장점

     * 모든 상태 정보는 RAM에만 유지되어 읽기 전용 루트 파일 시스템에서도 별도 트릭 없이 동작함
     * 이벤트 기반, 폴링 없는 동작 방식으로 효율성 제공
     * 런타임 중 메모리 동적 할당이 없음
     * 파일 디스크립터가 무한정 소모되지 않음
     * 하나의 self-contained 바이너리(옵션으로 제어 바이너리 추가) 만 필요함
     * 설정 파일 변환 및 컴파일 필요 없음, 서비스는 스크립트가 들어 있는 단순 디렉터리임
     * 서비스 재시작 및 로깅 체인 지원
     * 시스템 시계가 정확하지 않아도 정상 동작
     * FreeBSD에서 /etc/ttys를 통해 실행 가능
     * musl libc 사용 시 초소형 static 바이너리 제작 가능

서비스 관리

     * 각 서비스 디렉터리(기본 /etc/nitro 내부)는 아래 파일을 포함할 수 있음
          + setup: 서비스 시작 전 실행되는 (옵션) 스크립트, 정상 종료(0) 시에만 서비스 시작 가능
          + run: 서비스 동작 스크립트, 종료되지 않는 한 서비스가 살아있는 상태로 인식됨, 미구현 시 one-shot 서비스로 처리됨
          + finish: run 종료 후 실행되는 (옵션) 스크립트, 종료 상태 및 시그널 값을 인자로 전달
          + log: 다른 서비스 디렉터리를 가리키는 심볼릭 링크, run 출력 내용을 해당 서비스의 입력으로 파이프 연결(로깅 체인 활용 가능)
          + down: 이 파일이 존재하면 nitro가 기본적으로 이 서비스를 올리지 않음
          + 디렉터리명이 '@'로 끝나면 무시되어 파라미터 서비스로 활용 가능
          + 서비스명은 64글자 미만, /, ,, 줄바꿈 문자를 포함할 수 없음
     * runit의 chpst 유틸리티가 run 스크립트 작성 시 유용함

특수 서비스

     * LOG: log 링크가 없는 모든 서비스의 로그 기록용 디폴트 서비스
     * SYS: SYS/setup은 모든 서비스 구동 전 실행, 순서 있는 서비스 구동 구현 가능
          + SYS/finish: 전체 종료 단계 진입 전 실행
          + SYS/final: 모든 프로세스 종료 후 실행
          + SYS/fatal: 치명적 에러 발생 시 종료 대신 실행(있을 경우)
          + SYS/reincarnate: shutdown 대신 실행되어 예컨대 initramfs 재구현 등에 활용 가능

파라미터라이즈드 서비스

     * '@'로 끝나는 서비스 디렉터리는 nitro가 무시하지만, 심볼릭 링크 또는 nitroctl 명령을 통해 직접 지정 가능
     * '@' 뒤 파라미터가 첫 번째 인자로 각 스크립트에 전달됨
          + 예: agetty@/run와 agetty@tty1 심볼릭 링크가 있으면 agetty@/run tty1 실행
          + nitroctl up agetty@tty2 입력 시 agetty@/run tty2 실행 가능(디렉터리 존재 여부 무관)

동작 모드

     * 전체 라이프사이클은 부팅, 서비스 실행(슈퍼비전), 종료 세 단계로 구성
          + 부팅: 특수 서비스 SYS가 존재하면 setup부터 실행, 이후 모든 non-down 서비스 실행
          + 서비스가 종료되면 재시작, 단 최근 재시작이 빠르면 2초 대기
          + nitroctl Reboot 또는 Shutdown으로 종료 신호 전달 가능
               o 이때 SYS/finish → 모든 서비스 SIGTERM(최대 7초 대기) → SIGKILL → SYS/final → 종료 시퀀스
          + 컨테이너나 권한 없는 슈퍼바이저용일 경우 프로세스만 종료

nitroctl을 이용한 제어

     * nitroctl CLI 도구로 멀리서 nitro를 제어할 수 있음

   명령어 예시:
     * list: 서비스 목록, 상태, PID, uptime, 마지막 종료 상태 출력
     * up/down/start/stop/restart: 서비스 시작·중지·재시작 등 제어
     * 신호 전송: p(SIGSTOP), c(SIGCONT), h(SIGHUP), a(SIGALRM), i(SIGINT), q(SIGQUIT), 1(SIGUSR1), 2(SIGUSR2), t(SIGTERM), k(SIGKILL)
     * pidof: 지정 서비스의 PID 출력
     * rescan: 서비스 디렉터리 재읽기, 추가·제거 서비스 반영
     * Shutdown/Reboot: 전체 시스템 종료·재부팅

신호를 통한 제어

     * nitro 프로세스에 시그널 직접 전송으로 컨트롤 가능
          + SIGHUP: 서비스 재스캔(rescan)
          + SIGINT: 재부팅
          + SIGTERM: 종료(nitro가 pid 1이 아니면)

Linux에서 init으로서의 nitro

     * Nitro는 자체 포함형 바이너리로 Linux pid 1로 직접 부팅 가능
     * /dev, /run을 필요 시 마운트하며, 기타 동작은 SYS/setup에서 처리
     * Ctrl-Alt-Del 이벤트에 질서 정연한 재부팅 트리거

Docker 컨테이너에서 init으로서 Nitro 사용

     * Nitro는 정적으로 빌드되어 컨테이너에 간단히 포함 가능
     * /run이 컨테이너에 존재해야 기본 소켓 경로 사용 가능
     * 컨트롤 소켓을 바인드 마운트 처리하면 외부에서 nitroctl로 원격 제어 가능

FreeBSD에서의 Nitro

     * /etc/ttys에 다음 줄 추가로 FreeBSD init에서 nitro를 슈퍼바이즈 가능
/etc/nitro ""/usr/local/sbin/nitro"" """" on

저자

     * Leah Neukirchen leah@vuxu.org

감사

     * daemontools, freedt, runit, perp, s6 등 기존 프로세스 슈퍼비전 시스템들의 상세 분석 위에서 개발됨

라이선스

     * 0BSD 라이선스(자세한 내용은 LICENSE 파일 참조)

        Hacker News 의견

     * runit와의 비교를 보고 싶음. runit는 극도로 미니멀하면서도 거의 완전한 init 시스템임. 컨트롤 디렉터리, 선언적(declarative)이 아닌 의존성, 비슷한 스크립트 구성, 로깅 접근 방식 등 비슷한 점이 많음. 설명 페이지에도 runit 이야기가 잠깐 나오며 chpst 유틸을 함께 쓰는 것을 추천함. 차별점으로는 하나의 서비스 디렉터리로 여러 유사 프로세스(예: agetty)를 파라미터화하여 관리하는 구조가 좋다고 생각함. reboot나 shutdown을 단일 바이너리(nitroctl)로 바로 실행할 수 있음. 반면 runit는 여러 개의 바이너리 구조임
          + 지난 해 runit로 프로세스를 관리하던 마지막 서버들을 은퇴시키면서 아쉬움이 컸음. 약 15년 전에 처음 runit 서비스를 직접 짜면서 이게 리눅스에서 서비스 관리하는 표준 방식이라 믿었음. 이후 5년간 리눅스를 떠나 있다 돌아오니 systemd가 기본이 되어 있었고, 나쁜 평을 여러 번 들었지만 점차 왜곡된 반감이 많다는 걸 알게 되었음. 현재는 파충류 vivarium에서 Pi Zero로 카메라와 온도 데이터 스트리밍 서비스를 돌리고 있는데 systemd로 세팅하는 것이 굉장히 쉬웠음. OpenSuse 데스크톱이나 업무용 노트북에서도 systemd로 다양한 서비스를 간단하게 운용할 수 있었음. ‘표준이 있다는 건 오히려 좋은 일’이란 생각이 듦
          + runit와 nitro의 적절한 미니멀 비교가 2024년에 발표된 Leah Neukirchen의 발표 슬라이드(PDF)에 있음
            https://leahneukirchen.org/talks/#nitroyetanotherinitsy
          + Leah Neukirchen은 Void Linux 커뮤니티에서 활발히 활동하는 분임. 이 프로젝트가 Void와 밀접히 연관될 것으로 예상함. 좀 더 공식적으로 Void에서 nitro를 사용하는 방법에 대한 글을 써줬으면 좋겠음
          + “선언적(declarative) 의존성이 없다”는 점이 장점이라는 것인지 궁금함. systemd를 init로 비판하는 의견은 많이 들어봤지만 선언적 설계 자체를 비판하는 경우는 드물었음. 이유가 있는지 자세히 듣고 싶음
          + Void Linux에서 runit를 접하게 되어 init 시스템으로 잘 쓰고 있지만, UI와 문서화가 부족한 점이 아쉬웠음. 특히 로깅 설정이 정말 어려웠음. 비슷하게 단순하지만 더 합리적 기본값, 더 직관적인 UI, 더 나은 문서를 가진 대안을 써보고 싶음
     * 컨테이너에서 init 시스템을 돌리자는 이야기를 볼 때마다 항상 고민임. 실제로 필요에 의해 설계하는 경우도 있지만, 오히려 너무 복잡하게 만드는 경우가 자주 보였음 (특히 Kubernetes, 클라우드 환경에서 실제로 분리 설계를 더 제대로 했어야 함). ‘어차피 다들 이렇게 쓰니까’라는 게 현상인 것 같기도 한데, 굳이 ‘더 잘 만들겠다’면서 문제가 퍼지는 게 나은지, 아니면 기존 솔루션으로 격하게 실패하도록 내버려두는 게 나은지 항상 애매함
          + 어플리케이션 컨테이너는 유닉스 철학 ‘한 가지 일만 잘하라’를 따라야 한다고 생각함. 그런데 컨테이너에서 어떤 이유로든 fork를 한다면, PID 1에 진짜 init가 있어야 한다고 봄
          + 로보틱스 분야에서 겪은 바로는, 많은 컨테이너가 본래 베어메탈에서 돌던 복잡한 시스템이 컨테이너로 옮겨온 경우임. 프로세스간에 비구조화된 RPC가 많아 굳이 여러 개의 별도 컨테이너로 잘게 쪼갤 메리트가 없음. monolithic 앱 컨테이너 내부에서 여러 프로세스를 띄우기엔 supervisor, runit, systemd, tmux까지 각양각색 옵션들이 통용됨
          + Fly.io, Render, Google Cloud Run처럼 컨테이너 단위로 과금하는 호스팅을 사용한 적 있음. 가격 때문에 한 컨테이너에 여러 프로세스를 띄워야 할 때가 종종 있음
     * NixOS의 새로운 기능인 modular-services가 Nixpkgs에 포함되었음. 새로운 init 시스템 혹은 새 커널로 NixOS 포팅이 훨씬 쉬워질 예정이라, 지금이 nitro 같은 실험을 해보기 좋은 시기라고 생각함
     * Chimera Linux에서 쓰고 있는 dinit과 nitro를 비교해보고 싶음. readme를 빠르게 훑어보니 서비스 의존성 관리가 아직 안되는 것 같아 보임
       dinit: https://github.com/davmac314/dinit
          + Nitro는 선언적으로 서비스 의존성을 다루지 않음. 명령 하나로 서비스 간의 의존 그래프를 예쁘게 보는 게 불가능함. 하지만 setup 스크립트에서 필요한 서비스를 명시해두면 해당 서비스가 떠있는지 확인하고 자동으로 기다렸다 리트라이 함. 의존성 그래프를 보려면 grep 같은 걸로 스크립트를 직접 짜는 수밖에 없음. 반면 서비스가 죽을 때 연쇄적으로 dependent 서비스도 제대로 내리는 걸 깜빡하기 쉬운데, nitro 자체로는 이런 걸 탐지할 편리한 방법이 없음
          + Artix Linux에서 dinit을 써봤는데 정말 가볍고 인상적이었음
            Artix FAQ: https://artixlinux.org/faq.php
     * 이런 저수준 프로젝트들을 보면 정말 흥미로움. systemd가 전통적인 SysV·POSIX 틀을 넘어서 리눅스 커널 특화 기능을 잘 활용한 점이 좋았음. 하지만 그게 끝이 아니길 바라고, 계속해서 새로운 아이디어와 혁신이 나와주었으면 함. 최근 직접 제조용 자동화에서 UEFI 펌웨어로부터 바로 netboot되는 Linux 커널에 Go로 직접 짠 단일 init 바이너리만 내장한 셋업을 구현해봄. 직접 가져온 코드와 고수준 언어만으로 OS 환경을 다 제어하니 각종 서브프로세스와 수많은 텍스트 설정파일을 관리할 필요 없다는 점이 정말 자유로웠음
     * 약 13년 전에 C로 직접 init 시스템을 구축한 경험이 있음. 예정보다 훨씬 많은 노력이 필요했고, GUI와 백엔드를 성능 낮은 하드웨어에서 빠르게 부팅시키는 데 사용함. 재미있는 프로그래밍 연습이었으나 이미 비슷한 솔루션이 존재했을지도 모르겠다는 자각이 나중에 들었음. 동료가 같은 회사에서 또 다른 init을 만드는 바람에 내 첫 버전은 거의 libc 외에 의존성 없이 가벼웠지만, 동료의 버전은 libevent 기반으로 더 고급 기능이 많았음
     * AWS Nitro와 이름과 기능이 겹친다는 점이 신경 쓰임
       https://docs.aws.amazon.com/whitepapers/latest/…
          + 이름만 겹칠 뿐, init 시스템과 하이퍼바이저는 근본적으로 완전히 다름
          + 문제 생길 일은 거의 없다고 봄. 하나는 누구나 쓸 수 있는 init 시스템이고, AWS Nitro는 기업 내부에서만 쓰는 KVM 포크임
     * s6와 비교해 nitro가 어떤지 궁금함. 최근 도커 컨테이너에 s6로 init 시스템을 구성해봤는데, s6-overlay로 많은 파일을 직접 만들어야 했고 생각보다 직관적이지 않았음
          + s6는 훨씬 더 복잡하고 풍부함. nitro나 runit가 훨씬 더 단순한 대안임.
            tini도 체크해볼 만함: https://github.com/krallin/tini
     * Distrust에서 rust로 500라인 이하의 초간단 init 시스템을 직접 작성해서, 보안 필수 enclave 환경에서 몇몇 클라이언트가 실서비스로 사용 중임. rust 표준 라이브러리만 써서 감사를 매우 쉽게 했음
       https://git.distrust.co/public/nit
          + 깔끔해 보이지만(nit보다 33% 더 큼), readme에는 빌드 방법만 나와 있고, 실제 인터페이스나 작동 방식은 설명이 없음
     * 의존성 지정 불가, 유저/그룹 설정 없음, 순서 수동 지정 필요, 병렬 서비스 실행 없음, 리소스 관리 없음. 이런 게 빠진 시스템을 init 시스템이라고 부르지 않았으면 함. 그냥 베어본 process supervisor임
          + 실제로 이 모든 기능을 잘 해냄(심지어 systemd보다 더 나았다는 경험임). 나는 nitro 대신 오랜 기간 daemontools(그리고 nitro는 그 계승)만을 써왔음. 사용법이 믿을 수 없이 쉽고 안정적이고, 이해하기 쉬움. 의존성 문제에 대해서도 ‘요건 각자 알아서, 대신 간단·저렴·신뢰성 좋은 툴을 제공합니다’라는 djb/daemontools 스타일이 훨씬 실무적임
"
"https://news.hada.io/topic?id=22665","Show GN: jin-frame: A reusable, declarative, type-safe, and extendable HTTP request library.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Show GN: jin-frame: A reusable, declarative, type-safe, and extendable HTTP request library.

   axios기반 HTTP request client 입니다. HTTP request를 TypeScript 클래스로 정의할 수 있습니다.

   실무를 하다보면 endpoint 별로 timeout, 재시도 횟수, 재시도 방식 등을 다르게 관리해야 하거나 공통의 패턴을 작성, 실패 한 경우 로깅 추가 등 여러가지 요구사항을 받게 됩니다.

   axios, fetch, ky 등 다양한 클라이언트를 사용해봤지만 이런 요구사항을 비교적 쉽게 충족할 수 있는 클라이언트는 없는 것 같아서 개발하게 되었습니다.
@Get({ host: 'https://pokeapi.co', path: '/api/v2/pokemon/:name' })
export class PokemonFrame extends JinFrame {
  @Param()
  declare public readonly name: string;
}

const frame = PokemonFrame.of({ name: 'pikachu' });
const reply = await frame.execute();

console.log(reply);

   상속을 통해 Hook을 확장할 수 있고, timeout, 재시도 횟수 등을 클래스 별로 다르게 관리할 수 있어서 유용하게 사용할 수 있습니다.

   사용해보시고, 많은 피드백 부탁드립니다!
"
"https://news.hada.io/topic?id=22660","Show GN: 터미널 한영전환 실수 방지 zsh 플러그인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Show GN: 터미널 한영전환 실수 방지 zsh 플러그인

소개

   터미널에서 작업하다 보면 영어를 입력해야 하는데 실수로 한글로 타이핑하는 경우가 있습니다. ㅣㄴ을 입력하고 나서야 ls를 타이핑하려던 것을 깨닫거나, 햣를 치고 git을 입력하려던 것을 알아차리는 상황 말이죠.

   이런 불편함을 해결하기 위해 zsh-hangul을 만들었습니다. 한글로 잘못 입력된 명령어를 실시간으로 영어로 자동 변환해주는 zsh 플러그인입니다.

   주요 기능:
     * 실시간 한영 자동 변환: ㅊㅇ → cd, ㅣㄴ → ls
     * 문자열 예외 처리: 따옴표(""한글"", '한글', `한글`) 내부에서는 변환하지 않음
     * 복사-붙여넣기 보호: 직접 타이핑한 경우에만 변환하여 의도하지 않은 변환 방지
     * AI 툴 지원: Cursor 등 AI 도구에서는 변환하지 않음 (ai는 키보드 타이핑을 안하니 실수도 할 일이없음)

왜 만들게 되었나요?

   개발자라면 누구나 경험해봤을 터미널에서의 한영 오타 문제를 해결하고 싶었습니다. 기존에는 잘못 입력하면 지우고 다시 타이핑해야 했는데, 이 플러그인을 사용하면 자연스럽게 올바른 명령어로 변환되어 타이핑 흐름이 끊기지 않습니다.

설치 및 사용

   Oh My Zsh 사용자라면 간단하게 설치할 수 있습니다. 자세한 설치 가이드는 프로젝트의 INSTALL.md에서 확인하실 수 있습니다.

   bash 사용자를 위한 별도 버전도 제공하고 있습니다.

   GitHub: https://github.com/gomjellie/bash-hangul

   터미널에서 한영 오타로 고생하셨던 분들께 도움이 되었으면 좋겠습니다. 사용해보시고 피드백이나 개선 아이디어가 있으시면 언제든 공유해주세요! (PR 및 이슈 생성 환영합니다)

   Fish shell도! 😁

   90년대 후반이었나 인터넷 익스가 시장을 지배하고 있을 때 익스용 툴바 만들기가 유행이었던 적이 있었는데요. 네이버 툴바, 다음 툴바 이런 것들요.

   한컴에서 나온 툴바 였나 그게 이런 기능이 있었드랬죠. ㅈㅈㅈ -> www 로 바꿔 주면서 한영 전환 자동으로 해주는.

   리눅스에서 fcitx 입력기를 쓰는 데 이 기능이 있으면 참 좋겠네요.

   fish shell 버전도 커서한테 포팅 한번 부탁해볼게요 ㅋㅋㅋ

   아이폰같은경우는 이중언어 키보드 기능이 있는데 맥은 없어서 아쉽더라구요...

   https://inputsource.pro/kr

   저는 위 프로그램을 사용하는데요, 특정 프로그램 전환시 사용할 입력 소스를 강제할 수 있는 맥앱입니다.

   오.. 좋은 프로그램 소개 감사합니다!

   오 안그래도 이런건 없나 했는데 감사합니다 ㅎㅎ

   제가 더 감사합니다 ㅎㅎ

   예전부터 zsh한글 잘 써오고 있었는데 이걸 긱뉴스에서 다시 보네요ㅎㅎ
   시대에 걸맞게 AI 툴 지원도 추가해주시고, 꾸준히 좋은 프로그램 개발해주셔서 정말 감사합니다^^

   헉... 감사합니다 ㅋㅋㅋ

   설치해봐야겠네요. 감사합니다. 👍

   감사합니다!

   개발은 6년전에 하셨군요!

   옛날에 개발해서 방치해두고 있다가 cursor의 힘을 빌려서 최근에 다시 건드려보고 있습니다 ㅎㅎ

   너무 좋습니다.👍

   👍감사합니다

   오 재밌네

   감사합니다 ㅎㅎ
"
"https://news.hada.io/topic?id=22683","Gmail을 떠나 Mailbox.org로 이동하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Gmail을 떠나 Mailbox.org로 이동하기

     * 오랜 기간 Gmail 사용을 이어왔으나, 개인정보 보호를 이유로 Mailbox.org로 전환을 결정함
     * Gmail의 구조상 모든 이메일 데이터가 평문으로 저장되고, 미국 정부 및 기관의 접근도 가능함
     * Proton Mail과 Tutanota는 강력한 종단간 암호화를 제공하지만, 자체 앱 사용 강제가 단점임
     * Mailbox.org는 PGP 통합 지원 및 인기 이메일 클라이언트와의 호환성이 강점이며, 저렴한 요금제 또한 장점임
     * Gmail 데이터 이관은 imapsync를 사용하여 성공적으로 진행, 점진적인 서비스 이전 및 암호화 적용 완료임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gmail에서 Mailbox.org로 이동한 이유와 배경

     * 2007/2008년부터 지속적으로 Gmail을 사용해왔으나, 더 이상 무료로 Google에 개인정보를 제공하고 싶지 않음
     * 이메일은 근본적으로 평문 전송이라는 한계가 있어, Google이 모든 메일을 보관할 수 있고 필요에 따라 미국 정부나 기관에서 정보 제공을 요구할 수 있음
     * 이는 EU-U.S. 및 Swiss-U.S. Data Privacy Frameworks 하에서, EU 시민 역시 예외가 아님
     * 개인 서버를 운영하고 프라이버시 보호에 민감한 사용자로서, 이러한 환경에 한계를 느낌

서비스 선정 과정

     * 이메일은 주로 기본적인 송수신 기능만 활용하며, 첨부파일이나 부가기능, 일정, 주소록 등은 필요하지 않은 사용 패턴임
     * 고민 끝에 Mailbox.org, Proton Mail, Tutanota 3가지 대안을 추림
          + Proton Mail, Tutanota: 실질적 종단간 암호화 제공, 월 3~4유로에 가격 형성
          + 단점: 종단간 암호화를 제대로 쓰려면 해당 서비스 앱(혹은 macOS용 브리지) 강제 사용 필요
     * Apple Mail 앱을 선호하고 기존 사용 환경을 바꾸고 싶지 않아 Mailbox.org 선택
          + Mailbox.org: PGP 암호화 기본 지원, 필요시 외부 PGP도 사용 가능
          + 10GB 이메일+5GB 클라우드 포함, 월 2.5유로(연 결제시)로 가격 경쟁력 높음
          + 스토리지는 필요에 따라 0.20유로/GB 단위로 확장 가능
          + Gmail 사용량(2.5GB) 기준 가격 부담 적음, 추가 용량도 합리적임
          + 1개월 무료 체험 가능하지만 외부로 이메일 발송은 제한적임
     * 신주소 등록 후 월 3유로로 2개월간 테스트, 이후 연 단위 충전 방식(자동 갱신 없음) 안내

Mailbox.org 웹 인터페이스 및 기능

     * 웹 인터페이스는 단순하지만 효율적임, Gmail 대비 불필요한 기능이 적음
     * 모바일에서도 높은 사용성 제공
     * Gmail 라벨 대신 폴더 구조 지원, Apple Mail과의 폴더 동기화로 활용성 높음
     * 다양한 부가기능(저장소, 화상/메신저/XMPP 지원, 일정, 연락처, Etherpad 등) 포함하지만 기본 이메일 외 기능은 사용하지 않아도 무방함

메일 데이터 이관: imapsync 활용

     * Gmail 전체 메일을 mailbox.org로 이전해, 향후 Gmail 계정 완전 삭제 계획 세움
     * 이관 툴로 imapsync를 사용, Archive 서버에서 Docker로 운영
          + Gmail에서 앱용 비밀번호 생성 후, 관련 스크립트 실행(비밀번호 등 변수 설정)
          + Gmail의 All Mail 폴더는 제외해 중복 방지, Apple Mail의 Archive 폴더도 일반 Archive로 병합
     * 전체 복사에는 약 3시간 소요(26407개 메일, 2.14GB), 실시간 로그 확인 등 안정적으로 이관
     * 오류 0건, 중복 없음, 안정적 메일 동기화 결과

전환 간소화 및 주소 변경 관리

     * 메인 서비스의 이메일 주소 대부분 새로운 mailbox.org로 변경
     * 남은 Gmail 계정은 Apple Mail에서 삭제하고, 신주소로 자동 포워딩 설정
     * Mailbox.org에서 Gmail에서 온 메일을 자동 표시하는 필터 설정(Apple Mail에서 “진짜 빨간 깃발”로 표시)
     * 이렇게 하면 구독 서비스 등 주소를 지속적으로 업데이트 가능

암호화 지원 및 실제 사용 사례

     * Mailbox.org는 웹에서 PGP 키 직접 불러오기·관리가 쉬워, iOS에서 브라우저 기반 암호화 메일 송수신 가능
     * macOS에서는 Thunderbird 등 메일 클라이언트 활용 가능
     * 새 메일 작성 시 “PGP 암호화” 기능 선택으로, 간편하게 암호화 메일 송수신
     * 웹 인터페이스에서 발신자 공개키 신속 수집∙적용 등 부가 편의 제공

마무리 소감

     * Gmail 완전 이탈은 매우 오래 고민해온 일이었으나, 실제 전환 후 예상 외의 긍정적 변화 발견
     * 개인정보 보호 및 원활한 이전으로 만족감 높음
     * 만약 Mailbox.org 테스트나 문의 사항이 있으면 메일로 연락 가능(본문에 PGP 공개키 게재)

   대체 무슨 개인정보를 가지고 있길래? 정부기관의 조회 걱정까지 하며 살까? 이런 사람은 진짜 경찰조사가 필요할듯. 아동포르노? 간첩행위?

   떳떳하면 개인정보 까여도 상관 없다 << 정말 위혐하고 재교육이 시급한 마인드임.
   개인정보는 국가가 수집하면 감시와 탄압, 기업이 수집하면 무단 사용 횩은 판매, 개인이 수집하면 협박, 피싱, 사기 등의 위험이 동반됨

   평범한 일상적인 내용같은걸 본적도 없는 누군가가 마음대로 까볼 수 있다는것 자체가 싫은걸수도 있죠

   이런분들이 댓글을 다는거군요

   개인 정보는 말 그대로 개인의 정보입니다. 제 3의 기업, 정부, 인물이 접근 가능 한것에 거부감을 느낄 수 있죠

        Hacker News 의견

     * 얼마 전 Gmail에서 20주년 알림을 받고 내 삶이 얼마나 깊게 Google에 의존하고 있었는지 실감하게 됨, 이메일, 캘린더, Docs, Drive, 지도, Keep, Photos, YouTube, FitBit, Android까지 거의 모든 디지털 생활이 Google 중심임을 깨달음, 보안/프라이버시보다 서비스 다양화가 목적이었지만 덕분에 보안/프라이버시 개선 효과도 있었음, 메일, 캘린더, 드라이브를 대체하려고 Proton을 선택했음, 맞춤 도메인을 Proton에 연결하고 Gmail에서 Proton으로 포워딩 설정함, 앞으로 이메일 받으면 해당 웹사이트에서 주소를 수정하거나 계정을 삭제하는 식으로 점차 정리 중임, Google Docs와 Keep 대신 Obsidian 유료 동기화로 메모와 아이디어 관리 시작했고 완벽하게 대체함, Google Photos는 Hetzner의 VPS에 1TB 스토리지 박스를 SSHFS로 마운트해서 Immich를 자체 호스팅하며 사진 백업 해결함, Tailscale로
       안전하게 접근함, Google Takeout과 immich-go로 약 300GB의 사진을 몇 일만에 옮겼고 사용성도 만족스러움, 비용도 월 10달러로 부담이 적음, Android는 Pixel 8 Pro를 쓰는 중인데 GrapheneOS를 고려했지만 아직 트레이드오프가 크다고 생각함, 다음 폰 바꿀 때 Fairphone도 진지하게 검토할 계획임, FitBit Versa가 오래되어 새로 나온 Pebble 시계를 주문해서 배송을 기다리는 중임, YouTube는 대체할 서비스를 찾지 못했고 지도는 OpenStreetMap이 사용성이나 길 안내에서 아직 부족하다고 느낌
          + Google Maps와 Earth를 대체할 만한 서비스가 없어 아쉽지만, Bing Maps가 그나마 괜찮은 대안임을 발견함, 정보가 잘 통합되어 있고(예: 매장 정보 등) 불필요하게 방해되지 않음, 게다가 프리로 tilt-shift 시점(여덟 방향 모두)을 지원하는 유일한 서비스임
          + Proton을 무료 플랜으로 쓰면 서드파티 메일 클라이언트 사용 불가라는 점이 아쉬움, YouTube 구독정보와 플레이리스트는 Google Takeout으로 .csv 파일로 받으면 되고, 이를 RSS로 변환해서 RSSguard로 구독 및 시청 관리 가능함, 이 블로그가 실제 변환 작업에 많은 도움이 됨
          + GrapheneOS의 단점이 궁금함, 개인적으로 큰 트레이드오프가 없어 보여 계속 관심을 갖고 있음, OpenStreetMap은 데이터베이스이고 많은 상용 서비스가 활용 중임(예: Uber, Lyft), 원하는 스타일의 앱을 찾기만 하면 되고 CoMaps는 괜찮고, OSMAnd는 기능은 많은데 UX가 좀 어렵지만 시도할 만함, 또한 직접 OSM에 기여할 수도 있어 커뮤니티도 뛰어남
          + Google Maps 대체 서비스로 mapy.com을 선호함, 본질적으로 OSM 렌더러인데 웹사이트와 앱, 오프라인 접근, 경로안내, 실시간 교통정보, 자전거/하이킹 경로까지 모두 제공함, 하지만 GMaps의 POI 데이터베이스 만큼은 대체 불가임
          + 나 역시 비슷한 Google 의존도를 경험하고 de-googling을 시작했는데 생각보다 훨씬 해방감과 만족감을 느낌, 사진관리도 Immich로 전환했는데 정말 훌륭한 솔루션임, 지도는 대부분 CoMaps(comaps.app)를 쓰는데 OSM보다 훨씬 낫고, Pixel 7에는 LineageOS(gapps 추가)로 전환해서 전혀 불편이 없음, Nextcloud(nextcloud.com)도 Immich처럼 셋업해서 Google 대체에 성공했고 Google 밖의 삶이 더 괜찮음을 확신하게 됨
     * Fastmail을 수년째 이용 중인데 Gmail에서 옮겨왔음, 자체 도메인을 써서 언제라도 이탈하려면 주소 공지 걱정 없이 이동 가능함
          + Fastmail이 특이한 점은, 요금 결제를 중단하면 이메일 주소를 누구나 차지할 수 있도록 개방해버림, 요즘 시대에는 받아들이기 어려운 정책임
          + Fastmail 서비스에 매우 만족했고 이전에는 ProtonMail을 썼는데 브릿지 설치와 암호화가 불편했음, Fastmail 이후 Migadu로 이동했는데 지원 티켓 답변속도가 정말 빨라 감탄한 경험 있음
          + Gmail에서 FastMail로 전환 준비 중이지만 내 여러 도메인 메일을 라벨로 한 인박스로 받게 해주는 유일한 서비스임, Gmail 이주 마이그레이션(초기 가져오기 + inbox 동기화)을 지원하는데, 스팸이나 바로 아카이브된 메일은 동기화 안되는 점만 아쉬움, 답장주소 자동 인식 등 고급 옵션이 많아 점진적 시스템 전환에 안성맞춤임, 다만 Gmail의 사용자 알림음 커스터마이즈나 별/아이콘 이관은 지원하지 않아 약간 아쉬움
          + Fastmail 장기 이용자로서 메일, 캘린더 뿐만 아니라 빠르고 정확한 기술 지원이 최대 강점임
          + 브랜드 충성도는 없지만 기능, 가격이 동일한 대안이 있다면 언제든 갈아탐, Fastmail은 무제한 도메인, masked email 기능, 발신 메일이 스팸함에 가지 않는 점이 중요함, 이메일은 보안/프라이버시용 매체가 아니고(심지어 PGP도 메타데이터는 노출됨) 오히려 신뢰성과 사용성이 핵심임
     * mailbox.org를 고려한다면 아래 포럼 글을 참고할 필요가 있음, anti-spoofing 정책 문제
          + 추가로, mailbox.org는 아웃바운드 스팸 필터링이 있어 의심 메일을 발견하면 별 통보 없이 그냥 삭제하는 사례가 있음
          + 나도 mailbox.org로 옮길까 고려했는데, 포럼에서 이슈가 너무 오래 방치되어 온 점 등을 보고 이전 계획을 접게 됨
          + 처음 알게 된 정보임, 나는 mailbox에 맞춤 도메인 쓰지 않고, 내 도메인은 다른 서비스에서 사용 중임
     * Gmail을 2007/2008년부터 써오다가 데이터를 더 이상 Google에 무료로 주고 싶지 않아 이탈 결심함, 이메일이 평문으로 오가는 것도 문제로 느껴졌음, 그중 가장 불만이었던 건 Gmail에서 실제 ""plain text""를 지원하지 않는 것임, 시각장애인 개발자들과 협업할 때 공식 Gmail 앱은 plain text 전송을 막고 HTML 방식만 지원해, 일부 코딩 내용이 화면리더에서 잘못 읽히는 문제를 겪음, 결국 폰에서 Gmail 사용을 그만두고, 이 경험이 아예 Gmail 탈퇴로 이어짐
          + iPhone 사용자는 Apple Mail 같은 다른 클라이언트를 사용하면 기본이 plain text라 도움이 될 수 있음
          + ""plain text""란 용어를 작성자와 다르게 사용한다고 생각함, 작성자는 종단간 암호화가 없어 Google이 모든 메일 내용을 읽고 색인할 수 있다는 점을 말함
          + Gmail 웹메일 클라이언트의 문제일 수 있는데, 외부 메일 클라이언트(IMAP)로는 이런 문제가 없었던 것 같음
     * mailbox.org를 몇 년째 쓰고 있는데 딱히 불만 없음, 웹 UI가 뛰어나진 않으나 Thunderbird를 이용하니 상관 없음, 자체 메일 클라이언트와 도메인을 쓰면 UI 걱정 덜고 언제든 옮기기 쉬움, Thunderbird에서 자체 이메일 서비스를 준비 중이라는 소식을 듣고 지원할 의향이 생김
          + ProtonMail처럼 온디스크 암호화는 지원하지 않는 것 같음, 맞는지 궁금함
     * Google 의존도가 불안해져서 Fastmail로 5년 전쯤 전환했고 잘 정착함, 이주 시 특별한 과정 없이 Fastmail에서 지원하는 인하우스 마이그레이션 툴로 80GB를 하루 만에 옮김, Fastmail도 종단간 암호화는 없지만 Google 서비스 탈출만으로도 프라이버시 중요한 진보라고 생각함, ProtonMail도 잠깐 써봤지만 검색 품질이 별로임
          + Google Workspace에서 Fastmail로 옮기려고 고민 중인데, 향후 직원 이메일 부여, 전체 협업 도구 제공 측면에서 Google이 여전히 매력적임
          + Gmail 의존 탈피란 문제는 해결되지만, 특정 메일 제공업체 의존도란 더 큰 리스크는 해결되지 않음, 다음 단계는 VPS 자체호스팅이고, 그다음은 완전 홈서버 구축이고, 결국은 ISP 의존으로 이어지는 여정임
          + Google이 Gmail에서 별도 동의 없이 구매/주문 내역을 수집해 별도의 페이지에 정리해둔 적이 결정적 전환 계기가 됨, Proton으로 옮겼는데 브랜드 변경과 서비스 추가가 많아지며 영업 위주로 바뀐 느낌이어 결국 떠남, 최근 AI 기능 추가 등도 원인임
          + Fastmail의 가장 큰 단점은 미국 기반 호스팅이라 미국 외 시민들에겐 프라이버시 이점이 없다는 것임
     * mailbox.org에서 Google로 다시 돌아가는 중임, OXdrive는 Dropbox나 Google Drive 수준의 백업신뢰가 없고 동료가 폴더를 지워서 파일을 거의 잃을 뻔함, 비즈니스 요금제에서 2FA 미지원은 프라이버시 강조 업체로선 치명적인 단점임, 소프트웨어 기능이 전반적으로 떨어짐, Etherpad 용도를 모르겠고 OX Office는 다른 대안들이 훨씬 나음, 화상회의에서는 싱글사인온 미지원이라 매번 로그인해야 하고, 메일 클라이언트는 폴더 철학만 있고 태그 기능이 없음, 모든 기대를 안고 8개월 썼지만 실망스러운 기능성에 대기업 이메일이 길게 보면 더 낫겠다는 결론임, 프라이빗 이메일 용도는 괜찮겠지만 결코 추천은 못하겠음
     * 내 자신과 아내의 비즈니스 이메일을 Google에서 옮겼는데, DKIM, SPF 등 완벽하게 세팅했음에도 특정 메일 서버에서 bounce나 지연 등의 이슈를 겪음, Gmail은 정말 신뢰성은 뛰어남
          + 어떤 공급자를 썼는지 궁금함, 커뮤니티 포럼 운영하며 Atomic Mail 사용자를 많이 만났는데 그 서비스는 신뢰성이 매우 떨어지는 것 같음
     * mailbox.org는 일정기간(가장 저렴한 Light 플랜은 90일) 후 @mailbox.org 주소를 재활용함, 구독이 끊기면 30일 후 계정 종료, 30일 후 데이터 삭제, 이후 다른 사용자가 같은 주소를 등록할 수 있음, 주소 재활용 정책 자세히 보기 / 구독 종료 시 처리 정책 참고
     * Migadu 적극 추천함, Migadu 요금 표 참고, 수년간 만족스럽게 사용 중임
          + Migadu를 미국에서 소량 트래픽으로(마이크로 티어) 사용해봤는데 IMAP4 성능이 가끔 느린 경우가 있었음
          + Migadu의 설정 유연성 등 많은 장점이 있지만 가격대비 제공 서비스가 아쉬웠고 3년간 이용 후 하루에 발신량을 초과하자 메일 발신이 그날 차단되는 불편을 겪음
          + Migadu는 일일 발신량 제한 내에서 쓴다면 정말 좋은 서비스임
"
"https://news.hada.io/topic?id=22650","13가지 제품 리더십 원칙 (Product Leadership) [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               13가지 제품 리더십 원칙 (Product Leadership) [번역글]

1. 실행(iteration) vs. 아이디에이션(ideation)

     * 실행 중심적인 사고: “머릿속 고민만 하지 말고 직접 움직여라.”
       (“회사의 성장과 실질적 배움은 시도에서 온다”)
     * 팀이 얼마나 배우고 성장했는지, 그 과정에서의 경험을 중시

2. 시스템(system) vs. 개별 섹션(sections)

     * 전체 워크플로우 관점: “부분 기능보다 제품의 전반 흐름과 시스템을 이해하는 데 집착한다.”
       (“혁신은 기존 흐름을 분해·재조합하는 데서 발생한다”)
     * 부분 최적화가 아닌 플랫폼화, 아키텍처적 접근 강조

3. 기반(foundation) vs. 기능(features)

     * 토대(기반)에 투자해야 지속적 성과: “뻔한 단기 기능보다 반복적으로 쓰이는 building block에 집착한다.”
       (“1+1=3 효과, 부채 누적이 큰 문제로 돌아온다”)
     * 부채 관리 및 장기적 가치에 대한 집중

4. 통찰(synthesis) vs. 현황(status)

     * 보고된 현황에서 의미 읽어내기: “표·차트에 나온 그대로가 아니라 그 사이 신호와 통찰을 잡아내라.”
       (“핵심 메시지 하나를 골라 30분간 깊게 파고들라”)
     * 산출물 나열 대신 깊이 있는 인사이트 도출

5. 성과(outcomes) vs. 산출(output)

     * 작은 변화 대신 큰 임팩트: “단순 산출물 증가는 의미 없다. PM은 진짜 변화를 만들어내야 한다.”
       (“업계에서는 더 큰 변화를 이끌 성과 중심 사고가 점점 중요함”)
     * 측정 가능한 임팩트, 고객 가치와 직접 연결 강조

6. 지속 가능성(sustainable) vs. 단발성(sporadic)

     * 확장 가능한 솔루션에만 집중: “확장 불가능한 결정은 하지 마라. 단기 성공은 장기 리스크가 된다.”
       (“진짜 위기는 확장 실패에서 온다”)
     * 주인의식·장기적 생존을 고려한 전략적 판단

7. 유연성(fluidity) vs. 고집(firmness)

     * 의견은 강하게, 집착은 버리기: “강한 의견을 가질 것. 다만 집착은 하지 말고 필요하면 과감히 방향 전환.”
       (“감정적으로 얽매이지 않는 객관성”)
     * 아이디어/전략의 반복 평가와 신속한 수정 능력

8. 계획 수립(planning) vs. 계획서(plans)

     * 실제 계획보다 계획하는 습관과 논리 중시: “계획 문서가 목표가 아니라, 우선순위와 논리적 이유를 설명해야.”
       (“Always Now, Next, Later만으로도 충분하다”)
     * 백로그 우선순위의 설명과 근거에 집중

9. 품질(quality) vs. 속도(quickness)

     * 초기부터 품질(사용성, 성능, 비용)을 설계에 포함: “속도에만 몰두하면 감정적 불만이 쌓인다.”
       (“진짜 기억에 남는 제품은 무조건 품질에서 출발한다”)
     * 시장을 먼저 가는 것보다 사용자의 경험·만족도를 최우선

10. 깊이(depth) vs. 폭(breadth)

     * 고객 피드백 루프의 다양성과 범위 중시: “깊이 있는 피드백뿐 아니라, 시장의 다양한 시그널을 수집하라.”
       (“특정 채널의 반복적 구조화보다 폭넓은 시장 접점”)
     * 고객 집중과 동시에 다양한 채널·관점에서 정보 획득

11. 방향(direction) vs. 데이터(data)

     * 데이터의 목적은 길 찾기: “정확성보단 명확한 방향 제시가 더 중요.”
       (“최적의 신호(signal)만 찾으면 됨. 최적화는 후순위”)
     * 전체 그림을 보고 한 단계씩 올바른 방향 결정

12. 반복(loops) vs. 도약(leaps)

     * 운과 기술의 차이: “행운(도약)보다 반복적 개선이 진짜 실력.”
       (“꾸준히 1%씩 좋아지는 루틴이 성공의 본질”)
     * 연속성 있는 결정의 가치 부각

13. 글쓰기(writing) vs. 즉흥 대응(winging)

     * 명확한 글쓰기가 사고력의 검증 방법: “긴 문서로 제대로 설명할 수 없다면, 충분히 고민하지 않은 것.”
       (“파워포인트·슬랙 등은 숨길 수 있어도, 문서는 못 숨긴다”)
     * PM으로서 진짜 가치는 명쾌한 글쓰기에서 판명
"
"https://news.hada.io/topic?id=22648","왜 애니메이션 고양이 소녀들이 내 리눅스 커널 접근을 차단하는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  왜 애니메이션 고양이 소녀들이 내 리눅스 커널 접근을 차단하는가?

     * lock.cmpxchg8b.com 사이트에 접속할 수 없음
     * 서버 응답 지연으로 접근 불가 현상 발생
     * 네트워크 연결, 라우터 또는 모뎀 상태 확인 안내 내용 포함
     * 프록시 설정 점검 권유 있음
     * 단순한 연결 문제로, 리눅스 커널 접근과 직접적 관련 설명 없음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사이트 접속 불가 안내

     * lock.cmpxchg8b.com 사이트가 응답하지 않음
     * 연결 지연(Timeout) 문제 발생 안내 제공
     * 네트워크 장비(라우터, 모뎀 등) 재부팅 권고
     * 네트워크 접근 허용 설정 재확인 요청
     * Chromium 브라우저 내 프록시 설정 변경 및 직접 연결 권장 내용 포함
     * 커널 접근 자체가 막히거나 특정한 캐릭터 이미지(애니메이션 고양이 소녀)와는 직접적인 기술적 설명이 없음

        Hacker News 의견

     * 이 곳은 대체로 기술에 밝은 사람들이 모이는 곳이기 때문에, 많은 사람들이 진심으로 Anubis 같은 보호책의 원리나 취지를 이해 못하는 것인지, 아니면 일부러 못 알아들은 척하며 이를 경시하는 척하는 것인지 궁금증이 생김
       AI 스크래퍼 봇 개발자라면 언젠가 방법을 알아낼 것이라는 점은 당연하지만, 일단 한동안 효과가 있었던 것이 중요함
       봇 개발자들이 새로운 우회법을 내놓으면, 또 새로운 ‘봇 아님 증명’ 도구가 등장할 것임
       결국 이건 단순함: 새로운 방법을 적용해서 한 두 달이라도 사이트가 안전하다면 그것만으로도 만족할 만한 성과임
       네트워크 전체를 차단해야 할지 고민하며 초당 수십 개의 요청을 감당하는 것보다 훨씬 나음
       예를 들어 폼에 “7+2가 얼마인가요?”와 같은 질문을 넣는다면, 물론 수집기는 답을 계산할 수 있지만, 이를 스크래퍼에게 “어떻게 하라고” 알려주는 게 결국 사람의 몫이 됨
          + 나는 Anubis의 PoW(Proof-of-Work)가 실제로 사이트에 어떤 효과를 냈는지 숫자로 확인할 수 있기를 바랐음
            개인 서버에서 만든 작은 웹사이트라 로깅이나 통계를 잘 확인하지 않지만, 언젠가 내 사이트도 공격적인 크롤링을 당할 수 있음
            현재 공개된 자료에서 볼 수 있는 건 PoW의 성능 자체뿐이지, 실제로 사이트에서 얼마나 효과를 냈는지는 알 수 없음
            이상적으로는 “OpenAI 봇이 전체 요청의 17%였는데, 하루 90만 건에서 0건으로 줄었다”처럼, 봇 종류별 통계가 있으면 정말 좋음
            검색해보면 “Anubis가 크롤링을 막아줬다”는 블로그 글들만 잔뜩 나오고 실제 데이터는 부족함
            추가로 아래 스레드에서 이런 PDF 데이터를 찾았지만, 실제 고객이 몇 명이나 차단당했는지에 대한 분석은 없음
            더 다양한 데이터가 있었으면 하는 마음임
          + 요즘 사이트마다 Cloudflare 중간 로딩 화면을 보는 일이 흔해졌음에도, 사람들은 Anubis 같은 보호책이 많이 쓰이지 않는다고 불만을 터트림
            이 현상이 다소 아이러니하게 느껴짐
          + 처음 Anubis가 나왔을 때부터 비효율적이라고 생각했음
            첫째, 스크래퍼는 이미 전체 브라우저를 돌려 페이지가 모두 안정화될 때까지 기다리는 작업을 하고 있었으니, 이 방식을 우회하는 건 어렵지 않음
            둘째, AI가 페이지를 읽으려면 약 5초 동안 1600W에 달하는 연산이 필요하며, 내 휴대폰이 서버급 성능만큼 효율적일 리 없으니 5초보다 더 오래 걸릴 수 있음
            이 모든 일을 동시에 처리하려면 기기도 뜨거워지고 그만큼 비효율적인 구조임
          + 내 생각엔 PoW 자체가 AI 스크래퍼를 막는 게 아니라, Anubis가 만든 특이한 사이트 접근 절차 때문에 막히는 것 같음
            만약 이게 사실이라면 정당한 인간 방문자가 로딩 화면을 몇 초씩 보며 기기를 낭비하지 않도록, Anubis는 PoW 기능만 빼고도 충분할 수 있음
          + 7+2처럼 간단한 체크는 뭔가 제출하려는 사용자만 제한하지만, Anubis는 사이트에서 단순히 읽기만 해도 모든 사용자에게 영향을 미침
     * Anubis의 핵심 목적이 크롤러의 신분 세탁(Sybil 공격)이나 병렬 크롤링을 제어하는 것이라고 생각함
       접근 유형은 다음과 같음:
          + JS와 쿠키가 있는 클라이언트 → 서버가 쿠키를 이용해서 속도제한을 적용할 수 있음. 인간은 잘 안 걸리나, 크롤러는 속도가 크게 줄거나 접속이 차단됨. 물론 신분(쿠키)은 바꿀 수 있지만, 그만큼 퍼즐 풀이 비용이 들게 됨
          + JS만 있는 무상태 클라이언트 → 역시 접근마다 비용이 크고 효과가 있음
          + (JS 없음 → 접근 자체 불가)
            요점은 과도한 동시 접근으로 서버가 과부하되는 걸 방지하는 데 있음
            크롤러가 여러 개의 병렬 요청을 보내도 각 요청마다 추가 비용과 제한이 생기기 때문에, 예전처럼 초당 수천 건의 봇 트래픽에 서버가 망가지는 일이 없어짐
            이것이 Anubis의 실질적 효과임
          + JS가 굳이 필요하지는 않음
            Anubis를 거르고 챌린지를 푸는 스크립트만 있으면 충분히 우회 가능함
     * 예전에는 이런 절차적 접근방식이 귀찮을 뿐 아니라, 진짜 사람 인증하는 데 실질적인 도움이 되냐는 의문이 있었음
       이런 도전은 쉽고 저렴하게 자동화 가능함
       나는 실제로 Anubis가 도는 사이트의 User Agent에서 ""Mozilla""를 빼는 브라우저 확장 프로그램을 만들어 적용함
       대부분 JS, 쿠키를 쓰지 않으니 챌린지조차 못 깨고, JS, 쿠키를 허용하는 일부 self-hosted Gitlab 등은 내 컴퓨터 자원이 채굴에 쓰이는 걸 원치 않음
          + User Agent 헤더를 건드리면 거의 즉시 나만의 식별자가 생기니 주의가 필요함
            브라우저 핑거프린팅은 특별한 헤더 값을 가진 사용자에게 특히 잘 먹힘
            손대지 않은 Safari만 써도 수백만 사용자가 똑같은 값을 공유하는데, User Agent를 바꾸는 순간 유일한 식별자가 되어버림
          + 사이트가 ""스티키""한 경우 진짜로 Monero 등 암호화폐를 백그라운드에서 채굴하는 것도 가능하지 않을까라는 생각이 듦
            “이 사이트는 귀하의 컴퓨터를 백그라운드에서 과도하게 사용하고 있습니다. 중단하시겠습니까?” 같은 경고가 브라우저에 있으면 좋겠음
          + User Agent 값을 바꾸면 오히려 다른 기능들이 깨질 수 있지 않겠냐는 의문이 있음
            대부분의 브라우저 User Agent 문자열은 이미 표준 값으로 ‘거짓말’로 가득 차 있어서 바꾸긴 두려움
          + 본인이 만든 확장 프로그램에 흥미를 느낌
            페이지의 텍스트 인코딩을 강제로 일본어로 바꿔볼 수 있을까 고민하게 됨
          + AI 봇도 똑같이 User Agent를 바꿀 수 있다면, 결국 똑같은 결과가 아니냐는 의문이 듦
     * Anubis의 캐릭터가 고양이가 아니냐는 논쟁에 대해, Anubis란 이름 자체가 이집트의 신으로 보통 자칼이나 개로 묘사됨
       이름에서도 알 수 있듯이 애프터라이프(사후세계)의 문지기임
       기술적으로 말하자면 ‘개소녀’, 또는 ‘자칼소녀’가 더 정확함
          + 덕분에 마음이 한결 가벼워졌다고 농담을 던짐
          + 하지만 원래 Anubis의 시각적 특징이 빠져 있다는 점이 아쉬움
     * AI 서비스 제공자는 사실상 데이터센터에 연산 자원을 두고 있기 때문에 PoW 방식이 오히려 리소스가 적은 개인 사용자만 제한하는 꼴이 된다고 생각함
       하지만 현실에서는 Anubis가 그나마 쓸만한 차악 옵션으로 받아들여지는 듯
       이론과 현실이 다르다면, 뭔가 빠뜨렸을 수도 있고 이론이 틀린 것일 수도 있음
          + 반론으로, 이미 AI 봇들이 몇 주 만에 우회방안을 찾아 효과가 사라졌다는 HN 링크(https://news.ycombinator.com/item?id=44914773)를 안내함
     * Anubis 화면을 보자마자 한눈에 알아봤고, 이 프로젝트의 애니메이션 고양이소녀가 사라지지 않길 바라는 마음임
          + 인터넷이 너무 기업적이고 삭막해진 요즘, 이렇게 귀여운 요소가 아직도 남아 있다는 게 기분 좋음
          + Anubis가 원래 개머리를 한 이집트 신에서 따온 것이니, 그림을 보고 ‘개소녀’라고 생각함
          + 애니메이션 캐릭터가 마스코트인 프로젝트는 이것뿐만이 아님
            ComfyUI도 공식 마스코트로 여우소녀 캐릭터를 쓰고 있는데, 이것이 Stable Diffusion 기반 생성 UI의 사실상의 표준임
          + AI 스크래퍼들이 PoW를 진지하게 구현하거나 User Agent에서 ‘Mozilla’를 빼는 것만으로도 이 보호책이 바로 무의미해진다면, 프로젝트 자체가 곧 사라질 수도 있다는 전망임
     * “CAPTCHA는 컴퓨터에겐 어렵고 사람에겐 쉬운 문제를 제시한다”는 설명에 대해, 내가 만났던 “정통파 랍비가 있는 칸을 고르시오” 같은 CAPTCHA를 실제로 풀어본 적이 있는지 의문을 가짐
          + 2008년 RapidShare 캡차에서 벌어진 해프닝이 재밌었음
            관련 링크1
            관련 링크2
            관련 링크3
          + 그 유형의 CAPTCHA는 토요일에는 풀 수도 없다는 유머성 응답
          + 요즘은 CAPTCHA를 저렴하게 대신 풀어주는 서비스도 많음
            AI 기업의 대규모 트래픽에는 추가 할인을 받을 수도 있고, NopeCHA 같은 브라우저 확장도 99% 정도 성공해서 직접 풀 피로를 줄여줌
            결국, 어려운 CAPTCHA를 써도 실제 고객만 힘들게 만들 뿐임
            물론 스스로 AI로 CAPTCHA를 해결하지 못할 때를 가정한 이야기임. 요즘엔 AI로도 충분히 가능함
          + 결국 컴퓨터가 모든 걸 잘하게 됨
            2000년대 초반의 CAPTCHA는 진짜로 컴퓨터에겐 어려웠음
     * Anubis가 실행 중인 JWT 토큰의 식별자를 저장하고, 각 토큰에서 발생하는 요청 수나 속도를 추적하고, 일정 임계치를 초과하면 토큰을 취소하거나 지연응답 또는 제한을 거는 방식도 적용할 수 있을 거라 생각함
       봇이 챌린지를 풀더라도, 토큰을 유지해 지나치게 빠르게 요청하는 순간 곧바로 제약이 걸림
       특정 IP에서 토큰을 여러 개 발급하지 않도록 제한하는 방식도 생각할 수 있음
     * 이따금씩 Anubis 같은 화면을 보면 악성 리디렉션이나 이상한 이미지보드 사이트로 연결되는 건 아닌가 걱정됨
       kernel.org가 유료, 비애니메이션 버전이 아닌 무료 버전을 쓰는 것도 의아함
       리눅스재단이 BMW 다 타고 다니더니 정말로 돈이 없는 건가 이런 농담도 듦
          + 애니메이션이 요즘 더 대중적이라 넷플릭스도 연간 수십억을 벌 정도인데, 이런 순수한 애니 그림만 봐도 일부는 딴 생각을 하는 현상이 신기하다고 느낌
          + Anubis는 사실 독창적인 프로젝트가 아닌 Kiwiflare의 클론이라 완전히 틀린 인상은 아님
            Kiwiflare 관련 링크
          + 굳이 비브랜드 버전이 필요할까 의문임
            오픈소스는 보통 별도의 라이센스나 다운로드 페이지가 없어 배포가 더 쉬움
            의존하는 프로젝트라면 기부만 해도 되는 일이고, 굳이 디브랜딩하는 게 아니라면 오히려 원래 브랜드가 신뢰를 줄 수 있음
            예를 들어, Anubis 마스코트가 나오면 자바스크립트를 더 믿고 활성화할 수 있지만, 브랜드가 없으면 이상한 캡차업체 코드일 수 있다는 의심이 생김
            LKML은 예나 지금이나 디자인 신경 안 쓰니 사실 별 상관없음
     * 사람만 쉽게 풀 수 있는 질문을 카운팅하는 등 인간 인증법이 LLM 필터에도 의외로 효과가 있음
       예시로, 등록할 때 특정 키워드의 철자 수를 세거나, 자동차 부품의 직경 등을 묻는 포럼들이 있는데 실제로 효과가 좋았음
          + 작은 포럼은 가입 절차 맞춤 전략만 잘 써도 스팸 가입이 확 줄어듦
            나는 예전에 스팸 가입이 많은 포럼에서 6자리 숫자에 1을 더해 입력하게 바꿨고, 효과가 극적이었음
          + 검증된 방식임
            예전에 moparscape라는 게임 포럼에서는 'mopar가 뭐냐' 물어서 매번 검색하곤 했음
          + 하지만 이런 질문도 일반 사용자 중 일부는 풀기 어려워한다는 점을 상기할 필요가 있음
"
"https://news.hada.io/topic?id=22684","Jails - 우아하고 미니멀한 웹 컴포넌트 마이크로 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Jails - 우아하고 미니멀한 웹 컴포넌트 마이크로 프레임워크

     * 복잡한 프레임워크 대신 SSR·SSG 환경에 가볍게 적용할 수 있는 웹 컴포넌트 기반 마이크로 프레임워크
     * gzip 기준 약 5kb로 매우 경량이며, HTML과 JavaScript를 분리해 번들 크기를 줄이고 구조를 단순화
     * 백엔드 독립적으로 동작하며, 다양한 서버/정적 사이트 환경에 통합 가능
          + 단순한 구조: state.set()과 이벤트 바인딩으로 UI 업데이트
          + 적용 환경: SSR(WordPress, Rails, Laravel, Node 템플릿 등), SSG(Hugo, Astro, 11ty, Jekyll 등)
     * 기존 자바스크립트 생태계와 쉽게 통합 가능하며, 자바스크립트의 함수형 기능을 적극 활용
     * Elm Architecture에서 영감을 받아 단일 상태 관리와 이벤트 기반 갱신 구조를 제공하며, vanilla JS 라이브러리와 상호운용성을 지원
     * Island Architecture를 채택해 애플리케이션 일부에만 컴포넌트를 적용 가능하며, 프레임워크 종속 없이 SSR/SSG 프로젝트에 이벤트 주도형 UI를 더하는 데에도 적합

   5kb 경량이라 좋은 것 같기도한데.. 기본 예제의 UI가 아쉬운 것 같네요.
"
"https://news.hada.io/topic?id=22688","휴대폰으로 쇼핑카트 바퀴를 제어하기 (2021)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       휴대폰으로 쇼핑카트 바퀴를 제어하기 (2021)

     * Gatekeeper Systems 바퀴를 잠그거나 잠금 해제하려면 휴대폰 스피커로 특정 소리를 재생하는 방법 설명임
     * 대부분의 전자식 쇼핑카트 바퀴는 7.8 kHz 신호를 감지하여 동작함
     * 매니지먼트 리모컨도 7.8 kHz에서 서로 다른 신호를 보내 바퀴를 제어함
     * 7.8 kHz는 오디오 주파수 대역에 포함되어 있어 스마트폰의 스피커로도 전송 가능함
     * DEFCON 29 컨퍼런스 발표 영상과 관련 오디오 파일을 활용할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * 아래의 오디오 파일을 휴대폰 스피커로 재생한 후 Gatekeeper Systems 바퀴 근처에 갖다 대면 잠그거나 잠금 해제가 가능함
     * 자세한 내용은 트위터 @stoppingcart 참고 가능함

방법 설명

     * 대부분의 전자식 쇼핑카트 바퀴는 지하 와이어에서 나오는 7.8 kHz 신호를 감지하여 잠금 또는 잠금 해제 여부를 결정함
     * 관리자가 사용하는 리모컨 역시 7.8 kHz에서 서로 다른 신호를 바퀴로 보내 바퀴 제어가 가능함
     * 7.8 kHz는 휴대폰 스피커가 출력할 수 있는 오디오 대역에 포함되어 있다는 점이 핵심임
     * 스마트폰 스피커에서 전자기 유도(EMF)를 발생시키기 때문에, 특수 제작된 오디오 파일을 재생하면 유사한 신호를 전송할 수 있음

참고 자료

     * DEFCON 29 발표 영상 링크
     * 오디오 파일 다운로드 가능함

        Hacker News 의견

     * 네덜란드에서는 쇼핑카트 바퀴 잠금 시스템이 거의 존재하지 않음, 대신 예전에는 대부분의 곳에서 1유로 동전을 넣어 카트를 분리해야 했지만, 코로나 이후 현금을 안 들고 다니는 사람이 많아지면서 이 시스템 자체를 아예 해제한 매장이 많음, 덕분에 별 문제 없이 더 잘 돌아가는 것 같음, 카트를 잃어버리는 일이 비싸기는 하지만 그 정도로 빈번하지는 않아 바퀴 잠금 같은 과도한 솔루션은 비용 효과적이지 않음
          + 미시간대 학생 기숙사가 Kroger 마트에서 약 0.8km 떨어져 있었는데, 2009년에는 바퀴 잠금장치가 없어 많은 학생들이 카트를 기숙사까지 가져갔음, 이 때문에 매장이 주기적으로 트럭을 빌려 카트를 회수해야 했는데, 카트 한 대 교체 비용이 $500~$1000나 하다 보니 손실이 컸음, 나중에는 사태가 심각해져 바퀴가 특정 구역을 벗어나면 잠기는 geolocking 시스템을 도입했지만, 주차장 구석 등에서 오작동해 여러 대의 카트가 주차구역에 방치되는 문제가 생겼음, 당시 여자친구가 일하던 외국 식품점은 아예 Big Lots에서 카트를 훔쳐다가 쓰기도 했고, Big Lots 직원들이 트럭을 몰고 돌아다니며 회수했음, Big Lots는 저마진이라 geolock을 도입하지는 않았던 것 같지만 도입 유인이 충분함
          + 스웨덴 대형 매장도 네덜란드와 마찬가지로 동전 시스템을 채택하고 있으나, 최근 그 사용 비율이 줄고 그냥 잠금 없는 카트가 많아지는 중임, 스웨덴답게 현금을 잘 안 쓰니 매장 안에서 무료 플라스틱 토큰을 나눠주기도 함, 웃긴 점은 이 시스템의 취지가 동전을 돌려주며 카트를 반환하도록 유도하는 건데, 무료 토큰을 나눠주면 그 취지가 좀 무색해짐, 결국 고객의 편의가 전체 보안 시스템의 목적보다 더 중요한 예시라고 생각함
          + 미국 Aldi 매장에서는 동전 시스템(쿼터를 넣고 카트를 분리)이 도입되어 있는데, 도난 방지가 목적이라기보다는 카트를 매장 앞으로 다시 가져오도록 유도하는 용도임, 이렇게 하면 직원이 매번 카트 회수를 위해 밖에 나갈 필요가 없어짐, 다만 요즘 누가 쿼터를 가지고 다니냐는 점, 현금을 잘 챙기지 않으니 불편함이 있음, 머지않아 카드나 모바일 결제로 소액 결제 후 카트 반납 시 환불되는 시스템이 등장할 것으로 예상함
          + 동전 삽입 방식은 카트 반환을 유도하는 전략이지 도난 자체를 막는 목적은 아님
          + 토론토 고급 쇼핑몰 Bayview Village의 한 식료품점에서는 카트가 매장 주차장 밖으로 나가지 못하게 이 시스템을 도입했음, 이는 카트가 쇼핑몰 미관을 해칠 수 있다는 판단 때문으로, 2005년 매장 오픈 당시의 조건이었음, 시간이 지나 정책이 변했는지는 모르겠음
     * 이런 잠금 카트 시스템의 의도치 않은 부작용은, 잠김 상태에서 미끄러져 바퀴 한쪽이 납작해지면서 '퉁퉁퉁' 소리를 내며 매장을 돌아다니는 현상임, 이런 카트를 잡으면 또 불편함
     * 이런 해킹 기법은 내가 좋아하는 해커 정신의 전형임, cnlohr의 LoLRa 프로젝트가 생각남, LoRa 무선 송수신기 없이 신호를 보내는 실험임
          + LoLRa 프로젝트 발표자료
     * 원래 사이트 주인이 이 글을 볼지는 모르겠지만, 해당 사이트의 카운터(counter12.com)가 Malwarebytes에서 피싱으로 분류되고 예전부터 가짜 바이러스 팝업 경력이 있음, 관련 신고 정보
          + 연락하고 싶으면 발표자료에 이메일이 있음, 더 잘 전달될 수도 있음
     * 이 발표(해킹 사례)가 정말 인상 깊었음, 덕분에 실제 매장에서 이런 시스템을 찾아보고 싶어짐
          + 우리 집 근처 Kroger에도 이런 카트(혹은 매우 유사한 것)가 있음, 여러 이유로 평소엔 그곳을 가지 않지만 이 실험을 직접 해보고 싶다는 생각이 듦, 이미 수년 된 시스템이라 요즘도 같은 소리가 나는지는 모르겠음
     * 이제는 솔직히 말할 수 있는데, 2003년 대학 시절 RF 신호로 이런 장난을 쳤었음, 식료품점 전체가 대혼란이었고, 결국 파워회로가 과열되어 신호 통신기를 가슴팍에 붙이고 있던 친구가 화끈한 통증과 함께 의심을 한껏 샀던 적이 있음
          + 재밌는 이야기이고, 흉터를 어떻게 설명했을지 궁금증이 생김
     * 나는 이런 바퀴를 정말 싫어함, 15년 전 Target 매장 주차장 먼 곳에서 점심을 먹고, 카트에 신생아와 두 살 아이를 태워 매장에서 장을 봄, 자동차로 가는 길에 도로 한가운데서 바퀴가 잠겨 전진이 불가, 교통 혼잡한 와중 직접 아이들과 장바구니를 다 들어서 차로 옮겨야 했음, Target 매장에 항의했으나 매니저는 신경도 안 씀, 왜 도로 한가운데 전선(락 시스템)이 있었는지 이해 불가, 누가 블루투스 스피커를 신발에 달아 모든 카트를 잠궈 매장이 시스템을 뜯어내게 했으면 하는 마음임
          + 친구들이 2005년쯤 하버드 근처 Shaws 마켓에서 비슷한 장난을 쳤었음, 하드웨어 구성은 달랐을지 몰라도 신호 범위가 엄청 넓어서 모든 카트를 무더기로 잠금시킨 일이 있었음
          + 카트에 쓸 수 있는 크록 스피커 검색
          + 매장 방송(퍼블릭 어드레스) 시스템으로 신호를 틀면 먹힐지 궁금, 관련 해커데이 글, 참고로 PWM 신호 활용 시 Arduino로도 구현 가능함
          + 근처 마트는 주차장 전체를 geofence에 포함시켰으나 멀리 있는 자전거 거치대나 그쪽 이동 경로는 빠뜨려서, 한 번 가득 산 장보기를 옮기기가 그만큼 불편한 적도 있었음
          + 이 바퀴 시스템 때문에 Costco만 찾게 되는 이유가 크다고 생각함
     * 저자 프로필에 ""flat mooner""라고 써있어서 한참 웃었음
          + 무슨 의미인지 궁금함, 혹시 엉덩이 납작하다는 뜻인지 의문
     * 무빙워크나 쇼핑카트 트랙에서 카트가 잠기는 메커니즘도 이와 동일한 신호 방식인지 궁금함, Menards라는 하드웨어 스토어는 카트 통로에 무빙워크가 설치돼 있는데, 카트가 경사로에 진입하면 바닥에 고정되어(잠기는 것처럼) 안전하게 이동하고 밑에서는 풀려서 궁금했었음
          + 이건 원리가 다름, 카트 가장 밑에 고무 ""발""이 달려있고, 바퀴가 얇아지면서 무빙워크의 홈에 들어가 카트가 바퀴 대신 발로 서는 구조임, 완전히 수동식이라 바퀴를 실제로 잠그는 건 아님, 카트를 들어올리면 바로 알 수 있음
     * 소소한 지적이지만, 페이지 맨 위에 연한 초록 배경에 어두운 초록색 글씨로 경고를 쓰면, 아무도 잘 안 봐서 경고로 역할을 못함
"
"https://news.hada.io/topic?id=22726","YouTube가 이용자에게 알리지 않고 AI로 동영상 화질을 임의로 보정함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               YouTube가 이용자에게 알리지 않고 AI로 동영상 화질을 임의로 보정함

     * YouTube가 AI로 동영상을 임의로 개선했으나, 사전 안내나 동의 없이 적용함
     * 일부 크리에이터들은 콘텐츠에 미세한 변화가 생긴 것을 감지함
     * 이 과정에서 피부·의상·귀 등의 세부 요소가 비자연적으로 변형됨
     * 크리에이터들은 이러한 의도치 않은 AI 효과가 신뢰도 저하로 이어질 수 있음
     * AI가 현실을 무단으로 중재함으로써 현실과의 연결이 약화될 우려가 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

YouTube, 이용자 동의 없이 동영상에 AI 화질 보정 수행

  Rick Beato의 경험

     * Rick Beato는 구독자 500만 명이 넘는 음악 유튜브 채널을 운영함
     * 최근 업로드한 영상에서 ""머리카락이나 피부가 이상해 보임"" 을 느꼈으며, 얼굴이 화장한 듯한 효과를 인지함
     * 변화가 미묘하게 느껴졌으나, 비교 전에는 명확하게 특정하기 어려움

  AI 기반 영상 보정의 실제

     * 지난 몇 달간 YouTube는 AI를 이용해 동영상의 특정 부분을 조정함
     * 셔츠의 주름, 피부 표면, 귀 등이 미묘하게 강조 또는 변형됨
     * 이 변화는 직접 비교하지 않으면 거의 알아채기 힘든 수준임
     * 그러나 일부 유튜버들은 콘텐츠의 자연스러움이 저하되고 원치 않는 AI스러운 느낌이 부여됨을 지적함

  업계 전반의 변화와 우려

     * 점점 더 많은 현실 정보가 AI를 거쳐 사전 처리되어 전달됨
     * 크리에이터와 시청자 간의 직접적인 현실 기반 소통이 약화될 수 있음
     * AI 개입이 느리게나마 현실에 대한 신뢰를 침식할 수 있다는 지적이 제기됨

  크리에이터들의 반응

     * Beato와 친분이 있는 음악 유튜버 Rhett Shull 또한 자신의 영상에서 비슷한 이상 현상을 발견함
     * 그는 강한 오버샤프닝과 AI 특유의 질감이 자신의 온라인 목소리를 왜곡한다고 우려함
     * ""내가 원하면 직접 처리했을 것""이라는 언급과 함께, 시청자와의 신뢰 저하 가능성을 지적함

  결론

     * YouTube의 비밀스러운 AI 화질 보정은 사용자의 현실 경험과 온라인 정체성 간의 경계 흐림을 야기함
     * 무단 AI 중재가 점차 공동체 신뢰 및 현실 감각 저하로 이어질 수 있다는 우려가 커짐

        Hacker News 의견

     * 온라인에서 글이 자동으로 ""수정""되거나 ""개선""되는 현상을 상상하니 등골이 오싹해짐, 내가 기사나 책을 출판하자마자 인간 고유의 목소리가 사라지고 이상하게 익숙하지만 불쾌한 느낌의 결과물이 되는 현상임, 만약 누군가가 내가 쓴 글의 껍데기를 쓰고 있는 듯한 기분임
          + 이미 상황은 망가진 상태임, 최근에는 에디터를 고용하지 않는 이유가 대부분 LLM 사용을 의심하기 때문임, 내가 요즘 읽는 책들 중에도 AI가 쓴 게 아닐까 의심하게 됨, 하지만 사실 스타일 가이드의 꼼꼼한 적용이나 인간 에디터의 편집도 독특함을 없애버림, LLM만 탓할 수는 없는 문제임
          + 아날로그 자료를 수집하기에 최적의 시기임
          + 이런 걸 막기 위해서는 체크섬이나 디지털 서명이 필요해짐
          + 최근 저널리즘에서는 인간이 시드 스토리라도 썼다면 운이 좋은 것임
          + 이런 상황은 마치 ""나는 내 누이에게서 태어났다""라는 장면과 비슷함 (Kototsubo, Johei Kambayashi 소설 참고)
     * ""YouTube는 항상 새로운 도구 개발과 실험을 한다""라는 Beato의 말을 듣고, 내 머릿속에서는 ""내 생계가 YouTube에 달려 있다""로 자동 번역됨
          + 혹시 그 멘트도 AI가 편집했는지 궁금함, 사실은 ""YouTube는 세상의 재앙이다""라고 말했을 수도 있음
          + 소비자 입장에서는 YouTube가 내가 원하는 방식으로 비디오를 감상하기 가장 까다로운 플랫폼임, 광고를 피하거나 Shorts를 다 비활성화하려면 반드시 애드블록을 써야함
          + Beato가 얼마 전에는 악질적인 저작권 주장에 시달려 많은 금전적 손해를 입었고, 채널 중단 위협까지 겪었다고 밝힘
          + Beato는 뮤지션이자 프로듀서지만, YouTube 영상 제작을 더 손쉬운 생계 수단으로 삼고 있음, 실제로 뮤지션들과 일하는 게 얼마나 답답한 일인지 여러 번 언급함
          + YouTube가 내 라이프스타일을 뒷받침해준다며 만족스러운 상황이라면 굳이 비판을 쏟아붓지 않아도 된다고 생각함, 이 회사들은 이미 충분히 욕을 먹고 있음, 영상 개선 실험이 논란거리일 순 있지만, YouTube의 과거 정치적 행보에 비하면 큰 문제는 아님
     * 기사에 실제 예시나 이미지 비교가 없는 점이 아쉬움, 다른 기사들도 유사하게 구체적인 정보를 제공하지 않음, 가장 큰 변화가 귀의 주름이 달라진 것 정도임, 직접 영상을 훑어봤으나 거의 설명만 있고 실제 사례가 거의 없음, 실험 성격상 이런 잡음 자체가 피드백 역할을 하는 것 같음 관련 레딧 링크
          + Rhett Schul 영상(이름 정확하지 않을 수 있음)에 들어가면 일반 영상과 Shorts 영상을 비교해서 볼 수 있음, 요약하면 Shorts에 샤프닝 필터가 적용되고 있음
          + YouTube Is Using AI to Alter Content (and not telling us) 영상 링크
          + 예전처럼 저널리즘에 노력이 들어간 기사들이 그리움
     * YouTube에서 공식 입장을 냄: 선택된 Shorts에서 전통적인 머신러닝으로 화질 개선 실험 중이며 GenAI나 업스케일링은 아니라고 밝힘, 노이즈 제거와 또렷함을 위한 과정으로 스마트폰 영상처리와 비슷함, 크리에이터와 시청자 의견을 반영해서 계속 개선하겠다고 함 YouTube 공식 답변 링크
          + 항상 이런 안내문에서 ""[회사]는 항상 최고의 경험을 위해 노력 중""이란 문구를 볼 수 있는데, 마치 ""우리가 좋은 일 하는 걸 들켜버렸네요! 고마워요!""라는 느낌임
          + Shorts는 어차피 광고 수익만 얻고 바로 버려지는 콘텐츠이니, 이 정도 개선은 꽤 합리적이라고 생각함
          + TV에서도 수년간 영화를 ""개선""해왔는데, YouTube가 머신러닝으로 그걸 한다고 큰 문제는 아니라고 봄, 귀가 좀 더 선명하게 나오는걸로 화낼 필요가 있을까 싶음
     * 특이한 경험이 있었음, Gmail을 통해 LinkedIn 메시지 알림을 받았는데 본문 내용이 LinkedIn 원본과 다름, 단 두 단어만 달랐지만 오히려 더 이상하고 찝찝한 느낌이었음
          + 혹시 발신자가 LinkedIn 메시지를 수정한 게 아닐까 싶음, 이메일에는 수정 전 원본이 남았을 수도 있음
     * YouTube가 AI로 영상 손질을 할 때 사용자가 선택할 권리를 주는지 답변하지 않는다는 것이 모든 걸 말해주는 것 같음, YouTube PM에게 시작하기 전 충분히 고민 좀 해달라고 하고 싶음
          + 그들이 충분히 생각하지 않았다고 볼 근거는 없고, 이번 실험 역시 이펙트의 효과를 데이터로 확인하기 위한 것일 수 있음, 우리 입장에서는 원치 않는 변화지만 만약 사용자 참여율이 높아진다는 결과라도 나오면 또 다를 수 있음
          + 이미 오랫동안 회사들은 유저보다는 AI, 광고, 그리고 어린이들을 더 오래 디바이스에 붙잡아 두는 것에 집중 중임
          + PM 입장에서는 ""그래서 뭐 어쩔건데, Odysee에 영상 올릴거야?"" 하는 생각임, YouTube가 워낙 독점적이기 때문에 일반 유저 목소리는 큰 영향을 못 미침
     * AI가 정말로 세상을 뒤바꿀만큼 훌륭하다면, 왜 다른 솔루션들과 똑같이 억지로 제품에 끼워 넣는 것처럼 보이는지 의문임
          + 인터넷도 똑같은 질문을 받을 수 있지 않을까 생각함
     * YouTube가 일부 Shorts에 잡음 제거를 위한 처리 실험을 했다고 하는데, 이미 유명 채널들은 영상을 잘 조명하고 색보정을 끝낸 경우라서 실제로 별 효과가 없을 것으로 보임, 하지만 과한 잡음 제거는 생성형 영상 같은 인위적인 느낌을 주고, 잡음이 적으면 인코딩 최적화가 쉬워지므로 효율 개선 목적일 수도 있음
          + Shorts 자체를 아예 없애자는 청원을 내고 싶은 심정임
          + 설명대로라면 영상 최적화를 위한 기술 확장일 뿐, ""AI""라는 단어 덕분에 논란되는 것임
          + 노이즈를 없애서 인코딩하기 쉬워진다면 결국 또 다른 손실 압축 방식일 뿐임, AV-1 코덱으로 인코딩하는 것과 원리는 다르지 않음
     * 최근 휴가지에서 Philip K Dick 전자책을 사려고 했는데, 가격이 싸니 혹시 해적판이거나 오류 가득한 버전, 정부 승인을 위한 검열판, 아니면 AI가 개선해버린 판이 아닐까 괜히 의심하게 됨, 이런 가능성을 떠올리는 것만으로도 현실의 진정성이 흔들린 듯한 불안감이 큼, 데이터의 진본성을 검증할 방법으로 체크섬 공유 같은 아이디어도 있지만 결국 진본성은 영원하지 않음을 느낌
          + Philip K Dick을 너무 많이 읽은 것 같음, 현실 자체보다는 자기정체성의 혼란을 주제로 생각해보면 더 어울릴 듯함
"
"https://news.hada.io/topic?id=22697","기차 촬영을 위한 라인 스캔 카메라 이미지 처리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       기차 촬영을 위한 라인 스캔 카메라 이미지 처리

     * 라인 스캔 카메라는 기차 등 움직이는 대상을 고해상도로 왜곡 없이 촬영하는 데 매우 적합함
     * 이미지 처리에는 관심 영역 감지, 속도 추정, 재샘플링 등 다양한 알고리듬과 기법이 필요함
     * 수평 및 수직 스트라이프 제거와 노이즈 억제 등 품질 개선 작업이 중요함
     * 구현에는 대용량 데이터 처리와 Python, numpy 활용, 다양한 실험적 개선이 포함됨
     * 다른 작가들의 라인 스캔 사진 사례와 비교를 통해 추가적인 통찰을 얻을 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

라인 스캔 카메라 개요

     * 라인 스캔 카메라는 한 줄(혹은 두 줄)의 픽셀로 매우 빠른 속도로 이미지를 스캔함
     * 카메라는 고정되어 있으며, 기차가 카메라 앞을 지나가면서 전체 모양이 기록됨
     * 정적인 배경은 이미지의 모든 세로 열에 반복되어 특유의 스트라이프 효과가 나타남
     * 이 방식은 전체 기차 길이에 걸쳐 왜곡 없는 고해상도 사진 촬영이 가능하여 기차 모델링 등 취미에도 유용함
     * 필름 기반 스트립 카메라 역시 유사 원리로 작동하지만, 감도 문제로 수동으로 필름 속도를 맞춰야 하는 차이점이 존재함

카메라 장비

     * [Alkeria Necta N4K2-7C] 모델을 사용하며, 4096×2 이중 Bayer 어레이 이미지 센서를 탑재함
     * 원본 데이터는 16비트 바이너리 배열로 저장함
     * 도시 지하철 등 다양한 환경에서 촬영이 진행됨

관심 영역(ROI) 감지

     * 장시간 스캔 시 배경 데이터가 대량 발생하므로 움직이는 물체 구간 자동 감지 알고리듬이 필수임
     * 에너지 함수(gradient 기반)와 최대 픽셀값 등을 조합해 수직 구조(움직임)와 수평 구조(배경)를 구분함
     * 이미지는 여러 청크로 분할, 각 청크의 99분위 에너지로 점수 산출
     * 점수가 최저 대비 1.5배 이상인 청크를 움직이는 물체 포함 영역으로 간주함
     * 기존 방식들은 일반화에 실패, 현재 방식이 다양한 상황에 더 효율적으로 작동함

속도 추정

     * 주체가 움직일 때 속도 추정 실패 시 이미지가 늘어나거나 찌그러지는 왜곡이 발생함
     * 카메라의 두 개의 초록(Green) 채널을 비교해 각 청크별 움직임 속도를 계산함
     * 청크별로 -7~+7까지 작은 이동을 적용한 후 두 채널의 차이 절대값을 계산해 cost array 생성
     * subpixel peak를 찾기 위해 가우시안 기반 [mean shift] 스타일 보간 사용, spline으로 전체 변화량 보정
     * 추출된 spline 값은 원본 타임 시리즈에서의 샘플 간격을 의미, 이미지 왜곡을 보정하는 데 사용됨

재샘플링

     * spline에 따라 샘플 위치를 계산해 새로운 이미지 추출
     * spline이 음수인 경우 좌우 반전, 0에 가까우면 에러 처리 등 예외 상황 고려
     * 각 샘플 위치마다 샘플 폭 정보도 저장, Hann 윈도 등 적절한 윈도잉 함수로 앤티앨리어싱 성능 향상
     * 단순 열 선택이나 사각형 윈도는 업샘플링시 거친 아티팩트가 생기므로 적합하지 않음

디모자이킹

     * 2열 Bayer 배열의 공간적 오프셋을 고려한 bilinear interpolation 등 커스텀 디모자이킹 필요
     * 속도 추정 후, 선형 보간을 통해 fringing 현상 등 보정
     * 두 초록 채널 데이터 차이로 인해 일반 Bayer 어레이보다 더 나은 풀컬러 복원이 가능할 여지도 있음

수직 스트라이프 제거

     * 클럭 지터(stripes), 피사체 밝기 변화로 인해 이미지에 수직 스트라이프 발생
     * 선형 회귀와 가우시안 가중치를 활용한 각 열별 보정 함수(iteratively reweighted least squares)로 스트라이프 보정
     * 이런 보정 함수들은 수학적 군 구조를 형성, 보정 누적시 드리프트 방지 위해 band-diagonal 선형 시스템 해법 고려
     * 실무에선 지수 평활 필터 등으로 고주파 노이즈 억제도 가능
     * 스트라이프 보정은 반드시 속도 추정 이전에 시행해야 함

노이즈 억제

     * 패치 기반(block matching) 노이즈 제어 기법 적용, 기차 표면의 반복적 질감을 적극 활용
     * 각 3×3 픽셀 패치의 특징벡터를 사용, 유사 패치 내에서 가중평균을 통해 노이즈 감소
     * 신호 강도에 따라 포아송 분포(루트 변환)로 사전처리 후 비교하면 성능 향상
     * 기존 total variation denoising 기법은 질감 손실이 심해 적합하지 않음
     * 본 기법은 연산량이 많고 속도가 느린 한계가 있음

기울기(Skew) 보정

     * 카메라가 수직이 아닐 경우 이미지 전체가 약간 삐뚤어지는 현상 발생
     * 스큐 검출은 속도 추정 이후, 최종적인 재샘플링 전에 시행해야 정보 손실 최소화 가능
     * Hough 변환 등으로 수직 구조에 기반한 자동 검출 가능

색상 보정

     * 현재 수동 보정 행렬로 색감을 맞춤
     * 실제로는 자연스러운 스킨톤 등 꽤 괜찮은 품질

구현 세부사항

     * 전체 파이프라인은 Python 및 numpy로 구현
     * 데이터 크기가 커서(4096행×수십만 열) 메모리 부족 문제 극복을 위해 청크 단위 단계별 처리 방식 채택
     * 일괄 메모리 할당이 무리이므로, 각 단계별로 데이터 부분처리 및 저장

  구현 경험

     * AI 도구를 도입해 코드 구현 시도, 결과는 제한적
     * AI가 선형 문제를 불필요하게 2차시간 복잡도로 만드는 등 비효율적 코드를 생성하는 경우 빈발
     * 대용량 배열 처리에서 불필요한 전체 마스크 생성 등 메모리 이슈
     * 일부 API나 코드 구조화, 시각화(Matplotlib) 등은 AI 도움으로 효율화 가능

타인의 라인 스캔 기차 사진 사례

  Adam Magyar

     * [Adam Magyar]는 독자적인 블랙 앤 화이트 라인 스캔 카메라로 ""Stainless"", ""Urban Flow"" 프로젝트 진행
     * 실내 지하철 등 저조도 환경에서도 매우 깨끗한 결과물을 촬영한 바 있음
     * 지하철 조명의 플리커를 피해 촬영 위치 선정 필요

  KR64 블로그

     * [kr64.seesaa.net]에는 일본 전역의 다양한 기차 라인 스캔 사진 대량 게재
     * 필름 슬릿 스캔 카메라 기반으로 추정, 매우 높은 다양성과 품질 보유
     * 사이트는 기술적 문제로 종종 다운, 컨택 불가

        Hacker News 의견

     * 나도 이 아이디어가 정말 마음에 듦, 비슷한 방식으로 드론을 이용해서 뉴잉글랜드에서 가장 큰 나무를 스캔해보려고 했음, 결과물이 아주 좋진 않았지만, 다시 시도해볼까 함
       결과물
       이 프로젝트는 이 이야기의 일부였음
     * 나도 비슷한 과정을 쓰지만 일반 카메라로 촬영해서 프레임을 수동으로 이어붙여 애니메이션을 만듦
       이 방식의 특징은 피사체에 자연스레 초점을 맞추게 되며, 배경은 추상적인 패턴으로 변환됨을 볼 수 있음
       각 ‘라인’은 대략 15px 너비임
       예시1 예시2 예시3
       도쿄 스카이라인의 일몰 타임랩스를 촬영해 유사한 기법을 적용한 후, 모션 트래킹으로 시간이 프레임의 왼쪽에서 오른쪽으로 흐르도록 함
       여기서는 각 줄이 4픽셀이고, 원본 애니메이션은 8k 해상도임
       관련 영상 모션 트래킹
     * 라인 스캔 트레인 예시를 더 찾아봄 여기에서 확인
     * 플랫베드 스캐너를 디지털 백처럼 썼던 초기 실험을 떠올리게 함
       예시: 링크
     * 라인 스캔 카메라로 자동차나 기차의 움직이는 풍경을 찍으면 어떻게 나오는지 궁금함, 패럴랙스 효과로 흥미로운 왜곡이 생길 수도 있을 거라 예상함
          + 기차에서 찍은 사진이 몇 장 있음 —
            오사카 난카이 6000 시리즈: 사진
            프랑스 풍경: 사진1
            마르세유: 사진2
            캘리포니아: 사진3 사진4
            보라색 나무는 카메라가 근적외선에 민감해서 이렇게 보이는 것임, IR 컷 필터를 산 이후로는 기차사진을 못 찍어봄, 일부는 프레임 드랍이나 다른 아티팩트도 있음
          + 딱 궁금했던 내용임, 기술적으로 한 시간 정도 기차여행 전체 풍경을 ‘스캔’하는 것이 가능할지 궁금함
          + 그냥 다 흐려짐, 이 글 본문의 사진 속 배경과 비슷한 느낌임
            차나 기차처럼 빠르면 제대로 보이지 않음, 아주 느린 속도여야 괜찮은 왜곡이 나옴
     * 아티클 정말 흥미로움, 특히 케이블카 사진이 인상적임
       배경 콘텐츠를 어떻게 골라야 할지도 재미있는 고민거리임
     * 내가 기억하기로 지난 올림픽 때 Omega가 결승선 스트립 카메라에 고주파수 라인 디스플레이를 조합해 썼음
       일반 카메라에는 깜박이는 라인으로 보였는데, 포토피니시 배경에는 Omega 로고가 있었음
       아주 미묘하지만, 이런 걸 구현해낸 게 인상적임
     * 경마 트랙의 포토 피니시 카메라에 관한 이 영상이 좋아서 소개함, 다른 분들도 재미있게 볼 수 있을 것 같음
     * 기차가 완벽하게 색의 줄 사이에 날카롭게 멈춰 있는 모습에서 엄청난 속도감이 느껴짐
     * 내 생각에 노이즈제거(denoising)는 다소 부자연스럽게 보이고 남아있는 아티팩트, 특히 디테일 부분의 컬러 프린지(fringe)를 더 강조하는 느낌임
       나는 이 기능을 끄는 편이 더 나을 것 같음
       그리고 demosaic 과정과 관련해서, RCD의 이 버전을 구현해보면 아티팩트 없는 고해상도를 얻을 수 있을지 궁금함
          + 실제로 나도 기본적으로 노이즈제거를 비활성화시킴, 각각의 가로 줄 무늬가 더 뚜렷해지고 속도도 매우 느려지기 때문임
            세로 줄 무늬 보정도 모든 경우엔 잘 동작하지 않고, 오히려 무늬를 더 만들어내기도 함
            해야 할 작업이 아직 많음
            RCD demosaicing은 바로 다음 단계임, 컬러 프린징 문제는 레드와 블루 채널에 대해 단순 선형 보간을 썼기 때문임
            그린 채널은 이미지 전역을 커버하므로, 이 채널을 가이드로 삼아 보간을 더 잘할 수 있는 방법을 생각함
          + 나도 노이즈제거된 결과가 특별히 좋아보이지 않는 쪽임
"
"https://news.hada.io/topic?id=22704","독일 인터넷 서비스 제공업체가 DNS를 변경해 내 웹사이트를 차단함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 독일 인터넷 서비스 제공업체가 DNS를 변경해 내 웹사이트를 차단함

     * 독일의 대형 인터넷 서비스 제공업체(ISP) 가 최근 내부 조직인 CUII를 공개한 직후, DNS 동작 방식을 변경함
     * CUII는 웹사이트 차단 목록을 공개하지 않아, 필자는 차단 여부를 확인할 수 있는 사이트를 제작함
     * ISP들이 차단 사실을 숨기기 위해 DNS에서 차단된 사이트를 존재하지 않는 것처럼 표시하기 시작함
     * 최근 Telefonica가 자신의 테스트 도메인을 차단 후, 필자 사이트를 방문해 탐지 여부를 확인함
     * 이에 Telefonica가 차단 신호를 은폐하였고, 결과적으로 투명성과 감시가 약화됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

독일 ISP, CUII 차단리스트 관련 DNS 조작 배경

   독일의 대표적인 ISP 중 하나가, 내부 조직(CUII)의 실체를 공개한 직후 DNS 동작 방식에 변화를 줌
   CUII(인터넷 저작권 클리어링센터)는 웹사이트 차단 여부를 결정하며, 법적 심의나 투명성 없이 대형 ISP와 저작권자들이 임의로 리스트를 운영하는 민간 조직임
   필자는 CUII가 차단리스트를 비공개로 운영하기 때문에, 누구나 차단된 도메인을 조회할 수 있는 사이트(cuiiliste.de)를 개발함
   독일의 최대 4개 ISP(Telekom, Vodafone, 1&1, Telefonica(o2))가 모두 CUII의 일부임

CUII의 반복되는 실수와 DNS 차단 방식 변화

   최근 Netzpolitik.org는 CUII가 이미 폐쇄된 도메인까지 차단하는 실수에 대해 필자가 제공한 정보를 바탕으로 보도함
   기존에는 ISP DNS에서 차단된 사이트 요청 시 notice.cuii.info로 리다이렉트되어 차단 사실을 쉽게 확인할 수 있었음
   하지만, CUII는 차단 리스트를 비밀로 유지하고자 이러한 확인 방법을 중단시키기 시작함
   일부 ISP들은 DNS에서 차단된 사이트를 아예 존재하지 않는 것으로 위장함
   예외적으로 Telefonica(o2)는 여전히 notice.cuii.info 응답 방식을 유지하고 있었음

블로그 사이트에서 도메인 차단 확인 트래픽 발생

   cuiiliste.de에서는 누구나 입력한 도메인이 CUII에 의해 차단되는지 ISP별로 조회 가능함
   Telefonica가 소유하는 테스트 도메인 blau-sicherheit.info를 차단 후, 자사 네트워크에서 필자 사이트를 방문해 탐지 가능성 테스트를 진행함
   필자의 툴은 이 도메인이 CUII에 의해 차단된 것으로 정확히 감지함
     * Telefonica의 DNS는 test 도메인을 차단 상태로 응답
     * Telefonica 네트워크에서 필자 웹사이트 접속
     * 차단 여부 탐지 성공

Telefonica의 차단 방식 전환과 필자 사이트 방해 의혹

   이후 약 2시간 만에 Telefonica가 DNS 차단 방식을 notice.cuii.info 리다이렉트에서 입력 도메인 미존재 응답으로 변경함
   이로 인해 필자의 시스템은 수백 개 차단 해제를 잘못 탐지하여 긴급 수정 작업이 필요했음
   패치 이후에도 차단 감지 난이도가 상승함
   이제는 CUII 차단이 아닌 다른 사유로 차단된 사례(예: 테러리즘 관련)와 구분을 위해 비CUII 차단 도메인 목록과 교차검증을 수행 중임

배경 및 투명성 약화 문제

   이러한 변화는, CUII가 부정확한 차단으로 비판받는 시점에서, 감시 및 투명성 회피 목적으로 설명될 수 있음
   결국, 공공의 알권리와 감시 체계 약화로 이어지며 CUII와 ISP에만 유리한 구조가 형성됨

관련 기사 및 참고 링크

     * Netzpolitik.org: Provider verstecken, welche Domains sie sperren
     * 기타 참고 및 출처는 본문 마지막 참고문헌 참조

        Hacker News 의견

     * 독일에는 Clearingstelle Urheberrecht im Internet(CUII)라는 조직이 있음, 인터넷 저작권 관련 사적 단체로, 웹사이트 차단을 결정함, 법관이나 투명성이 없는 ISP와 주요 저작권자들이 무엇을 볼 수 있는지 결정하는 구조임, 그런데 그들의 공식 페이지(cuii.info/en/about-us/)에 따르면 “법원 차단 절차 시행과 법원 명령 집행을 조정한다”라고 밝히고 있음, 즉 법원 명령만을 따르는 것 같지만, 그 언급만으로는 다른 사이트 차단이 배제되는 건 아님
          + 해당 블로그 글은 페이지가 바뀌기 전 작성된 것임, https://cuii.info/en/about-us/"">웹아카이브 버전에서는 CUII가 독일 내 독립적인 기구로, 저작권 침해성이 뚜렷한 사이트 차단이 합법인지 공정하게 검토한다고 설명되어 있음, 신청이 들어오면 심사위원회가 만장일치로 “분명한 저작권 침해”인 경우에만 DNS 차단을 권고하며, 그 권고는 독일 연방 네트워크 기관(BNetzA)에 전달됨, BNetzA 검토 후 문제가 없으면 ISP에게 차단 지시가 내려감, 그리고 동일 저자가 최근 버전을 다룬 새 블로그 글에서는 이제 법원 명령 후 차단만 조정한다고 밝힘, “더 이상 비밀 투표도, 기업 검열도 없음, 새 웹사이트 버전은 법적 차단 명령만 시행한다고 설명”
          + 블로그 글은 2월에 작성됐으며, 그 후 CUII는 법원 명령이 올 때까지 내부 그룹이 임의로 차단하던 방식에서 법원 명령에 포함된 도메인만 엄격히 차단하는 방식으로 전환함, 예전에는 “유사한 이전 사례”라며 법원 명령 없이도 차단했지만, 규제 당국의 지적을 받아 이제는 법원 명령 없는 임의 차단을 중단함
          + 마치 “예전에 정치에 부패가 많았다, 지금도 있지만 예전엔 더 심했다”와 비슷한 이야기임
          + 제목이 오해의 소지가 있음, 실제로 저자 사이트의 DNS를 차단한 게 아니라, CUII 스스로 자기 사이트 DNS를 차단해 저자가 DNS 블랙리스트를 어떻게 감지하는지 실험했고, 이후 전략을 변경함
     * CUII 차단 절차가 이제는 자의적 기업 결정이 아니라 법원 명령 기반으로 바뀌었다는 점을 참고할 필요가 있음 관련 블로그
          + 아쉬운 점은, 예전 차단 도메인들은 그대로 남아있음
          + 방금 링크된 기사에 따르면, 현재도 문제적인(악의적인) 차단 리스트가 유지되며 다만 새로 추가되지는 않는 상황임
          + CUII가 정말 포기했는지, 아니면 단지 감시망을 피해 차단을 은폐하는 방법을 찾은 뒤 포기한 척 하는지를 어떻게 알 수 있는지 궁금함
     * 서양에서는 전통적으로 저작권을 통해 검열을 해왔음, 돈이나 비즈니스를 명분으로 하면 검열이라고 여기지 않음, 그런데 오늘날은 미국은 힘과 배제로 자기검열을 강요하고(예: 공직에서 어긋나면 불이익, 미국 입국 제한 등), 유럽은 또다른 방식을 찾음, 이처럼 보안과 통제를 모두가 원하다보니 이제 모두 기술적 해결책만이 답이라는 생각임, 법·정치적 방식은 통하지 않음, 자유는 ‘헛된 유행’이 되었고, 주장하는 사람들도 결국 자기자신만을 위한 자유만 원함, 우주로 인류를 보내야 한다는 사람이 정작 지구에서 “허가 없이 여행했다”는 이유로 구금된 이들과 포즈를 취하는 아이러니, 그래서 이런 검열 관심 기술자라면 기존 웹사이트 대신 새로운 도구가 필요함, 주류는 결국 지배세력이 원하는 대로 통제될 수밖에 없음
          + 과거엔 법으로 해결하길 원했지만 이제는 기술적 해법에 공감함, 90-00년대 현실 공간에서 자유가 위축될 때 젊은 세대가 인터넷에서 탈출구를 찾았음, 집을 짓거나 창업은 힘들지만, 사이트 만드는 건 상대적으로 자유로웠던 시절이 있었음, 그러나 지금은 정부와 이해관계자들이 인터넷에도 개입해 예전 같은 자유가 사라짐, AI 콘텐츠와 봇, 검색난, 인간 인증 등으로 인터넷이 점점 답답해짐, 그래서 freenet, yggdrasil, alfis, gemini, reticulum, B.A.T.M.A.N 같은 새로운 네트워크 공간을 만들어야 할 필요성을 느낌, 그리고 그런 곳이 재미있기도 함, 정부가 거기까지 따라오는 데에도 시간이 걸릴 것임
          + 미국에 살면서 지금의 자유 흐름에 공감하지 않음, 나는 미국이 지향했던(적어도 좋은 취지의) 원칙들을 지지함, 경찰-군인 등 개개인은 옹호하지만, 그들이 수행해야 하는 권위주의적 명령은 지지하지 않음, 많은 이들이 법과 질서, 모두의 보호를 위해 입대했지 나쁜 일을 하고 싶어하진 않은데 구조가 강요하는 느낌임, 텍사스 게리맨더링 법(선거구 조작)이 통과됐고 자정작용을 기대만 해선 안 될 것임, 우주 진출은 생명과 타 공간을 해치지 않는 한 도전해볼 가치가 있다고 생각함, 모든 생명은 결국 어떤 이유로라도 이동이 필요함
          + “이 웹사이트 자체도 검열에 대한 것, 관심 있는 사람은 웹사이트 사용하면 안 됨, 새 도구가 필요함, 주류는 결국 지배세력에 의해 통제됨”이라는 주장에, 실제로 어떤 모습일지 상상이 잘 안 됨, 타이밍상 이미 너무 늦었다는 생각도 듦, 단말 인증(Device attestation)이 점점 강제되고 있고, 패스키(passkey) 역시 빠르게 도입됨, 프로토콜 대안을 만들 수 있지만 권력집단의 묵인이 전제됨, Signal, VPN, BitTorrent, Tor도 언젠가 막힐 수 있음, 결국 애플·구글 같은 빅테크가 제어하는 기기를 대다수가 쓴다면 어떤 프로토콜도 소용이 없을 수 있음
          + 상업적 검열은 널리 받아들여지지만 ‘공공의 선’을 위한 검열도 사실 본질적으로 그렇게 다르지 않을 수 있다는 흥미로운 생각임, 결과적으로는 자유가 과도하게 침해될 위험성이 항상 있음
          + “서양에서 검열은 저작권 권리로 이루어졌다, 돈과 비즈니스로 하면 검열로 간주되지 않았다”는 표현에, 두 문장이 상충된다는 생각, 저작권을 통한 검열과 비즈니스 목적의 권리 주장은 결국 동일하게 보임, 전통적으로 하나만 검열로 간주하지 않았다는 건 논리적 모순 같음
     * 검열이 많아질수록, ISP가 손대지 못하는 정말 검열 불가능한 프로토콜을 구축하는 동기가 생김
          + 이런 프로토콜이나 개정안들은 이미 존재함, 예: 사이트별 DNSSEC, 사용자 단의 DoT/DoH, 이로써 ISP의 악의적 응답 조작을 막을 수 있음, 다만 현실적으론 널리 사용되지 않고 있고, ISP가 SNI 검사나 IP 차단 등 더 우회하기 어려운 검열 수단을 도입할 수 있음
          + 결국 모든 것은 물리적 네트워크 계층에 종속됨, 그 계층을 통제하는 자가 언제든 통신 차단 가능함, ISP가 손댈 수 없는 유일한 진짜 프로토콜은 거대한 군대가 필요하고, 그 군대가 스스로 방해하지 않도록 동기를 부여해야 함
          + 이미 존재하는 프로토콜 예시로, I2P 라우터로 다크넷 구축, Yggdrasil로 차세대 분산 프라이빗 인터넷 구축, 아니면 단순하게 암호화 DNS(Njalla DNS, Mullvad DNS) 사용, 또는 좋은 VPN(Mullvad 등) 쓰기를 추천함, 동시에 프라이버시를 지키는 정치에 투표하고 의원들에게 편지를 보내는 활동도 병행해야 함
          + 대체 프로토콜이 실전에서 널리 보급되기 힘든 문제도 있음, 평시엔 이런 도구를 쓰다간 ‘타깃’이 될 수 있음, 보통 재난·대재앙 이후에야 대중적으로 채택된다는 한계가 있음
          + 더 많은 검열은 ‘우회책’ 찾기보다 더 나은 정부를 뽑도록 해야 한다는 이유임, 독재 수용하며 우회 방법만 찾는 건 문제임
     * 이 페이지의 우회책들은 주로 대형 공용 resolver 사용을 권장함, (만약 저자가 HN을 본다면) 9.9.9.9, 1.1.1.1, 그리고 특히 DNS4EU 서비스가 어느 도메인을 차단하는지 알 수 있으면 좋겠음
          + DNS4EU에 대해 처음 들어봤다는 반응, joindns4.eu/about 링크 공유
          + DNS 공급자(dns4eu, nextdns 등)는 어떤 DNS 서버 소프트웨어(nsD, bind 등)를 사용하는지, 혹은 자체 개발했는지 궁금함
     * 요즘 발달된 저작권 단속이나 토렌트 사냥 등의 강화 기조 때문에 Proton이 스위스 제도 때문에 독일로 이전 결정한 부분이 궁금함, 개인정보 보호 때문에 운영이 어려워지는 것은 이해하지만, 왜 독일을 선택했는지 궁금함, 스위스 새 법안이 정확히 뭔지 자세히 본 것은 아니나, 독일보다 더 심각한 규제라면 어느 부분인지 의문도 있음, (참고로 Mullvad는 스웨덴 기반)
          + 스위스인으로서, 자세히 보진 않았지만 새 법안은 사용자 데이터 6개월 보관 의무가 생길 예정으로 알고 있음, 물론 반갑지는 않지만, EU는 지속적으로 암호화에 백도어를 요구하고 있어 더 심각함, 결국 Proton의 결정은 비용 절감 탓이 크고, 스위스 법은 핑계라는 해석임
     * 도메인이 ‘몰수’되면, 새 “소유자”가 등록 갱신료를 내는지 궁금함, 만약 낸다면: 블록된 도메인 복사 사이트를 고가 갱신료의 Vanity TLD에 등록하고, 주목받게 만들어서, 수익을 낼 수 있지 않을까 하는 발상임
          + 여기선 도메인을 몰수하는 게 아니라 차단만 하는 구조임, ISP의 기본 DNS 서버에서 네임서버 응답만 조작하는 것임, 설령 도메인을 경찰이 몰수해도 렌트카 몰수처럼, 사용료를 대신 내줄 리는 없음
          + 도메인 몰수가 절차상 더 많은 비용이 들어갈 수 있고, 보통 사이트 차단은 ICANN과 무관하며, 사이트 운영자가 도메인 소유를 유지함, ISP가 DNS 질의 결과를 조작하는 방식이라 우회가 쉬움
          + 정부가 도메인을 몰수한다고 해도 실제로 돈을 내는 경우는 없다고 생각함
          + 제1단계(TLD 만들기) 이후가 다소 “나머지는 부엉이 그리기” 같은 느낌의 발상임
     * 독일은 이런 문제에서 참 후진적임, 실력 있는 엔지니어들이 자유로운 국가로 이주해야 함
          + 자유로운 국가가 어디인지 정의해달라는 반응
          + 이미 그런 현상이 벌어지고 있음
          + 트럼프 시대의 미국도 자유로운 세계는 아니지 않냐는 의견임
     * 이런 검열 방식을 8.8.8.8과 같은 퍼블릭 DNS 사용만으로 쉽게 우회가 가능한지 질문임
          + ‘unbound’와 같이 직접 DNS 리졸버를 운영하는 것도 하나의 대안임
"
"https://news.hada.io/topic?id=22641","구글, Pixel 10 스마트폰 공개 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          구글, Pixel 10 스마트폰 공개

     * Google이 Tensor G5 칩과 Gemini Nano 모델을 탑재한 Pixel 10, Pixel 10 Pro, Pixel 10 Pro XL을 공개
     * 새로운 디자인과 색상(Obsidian, Frost, Indigo, Lemongrass 등)과 Qi2 무선 충전(Pixelsnap) 지원, 재활용 소재 사용 확대
     * 카메라 업그레이드: Pixel 10은 5배 망원 렌즈 추가, Pixel 10 Pro/Pro XL은 최대 100배 Pro Res Zoom과 AI 기반 세부 보정 제공
     * AI 기능 강화: Magic Cue(앱 내 맥락 지원), Camera Coach(사진 구도·구성 가이드) 등 온디바이스 Gemini 모델 활용
     * 7년간 Pixel Drops·보안 업데이트 지원, Pixel 10 Pro/Pro XL 구매자는 Google AI Pro 1년 무료 제공
     * 가격은 Pixel 10 $799, Pixel 10 Pro $999, Pixel 10 Pro XL $1199, 8월 28일 출시 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

디자인과 빌드

     * 아이코닉한 카메라 바와 새로운 Material 3 Expressive UI 적용
     * Pixel 10은 6.3인치 Actua 디스플레이(최대 3000니트)와 강화된 오디오 제공
     * Pixel 10 Pro/Pro XL은 Super Actua 디스플레이, 대형 배터리, 16GB RAM, 빠른 충전 및 25W Qi2.2 무선 충전 지원

성능: Tensor G5

     * Tensor 시리즈 최대 업그레이드로, DeepMind와 공동 설계
     * Gemini Nano 모델 최초 탑재, 온디바이스 생성형 AI 경험 강화
     * 빠른 성능과 맞춤형 지원 제공

AI 기능

     * Magic Cue: 메시지·통화 앱에서 필요한 정보를 맥락에 맞게 자동 제시
     * 이메일에서 항공편 정보 자동 검색, 사진·주소 공유 지원 등 프라이빗·보안 중심 AI 실행

카메라 기능

     * Camera Coach: 사진 구도·구성 개선을 AI가 안내
     * Pro Res Zoom (최대 100배): Tensor G5와 생성형 이미지 모델 활용해 세밀한 디테일 복원
     * Pixel 10에는 해당 모델군 최초로 5x 망원 렌즈와 빠른 자동 초점 기능이 탑재됨
     * 10배 광학 수준 줌, 최대 20배 Super Res Zoom 기능을 통해 멀리서도 선명한 촬영이 가능

출시 정보

     * 사전 주문 시작: Pixel 10 ($799), Pixel 10 Pro ($999), Pixel 10 Pro XL ($1199)
     * Pixel 10 Pro/Pro XL 구매자: Google AI Pro 1년 무료 포함
     * 8월 28일부터 Google Store 및 리테일 파트너에서 판매

        Hacker News 의견

     * Google에서 일하는 사람들은 Apple보다 내 사고방식과 더 비슷함을 느낌
       내가 Pixel을 테이블에 놓으면 뒷면이 대칭이라 안정적으로 놓이는데, iPhone은 흔들림
       Pixel에서는 사진을 폴더로 정리할 수 있어서 버스나 카페에서 새 사진들을 폴더별로 정돈하는 습관이 있음
       그러나 iPhone에서는 모든 사진이 메인 폴더에 남아 있고, 이미 정리된 사진을 확인할 방법이 없어서 불편함
       Android에서는 Chrome을 통해 File System Access API가 지원되어, 웹앱으로도 로컬 파일에 접근하는 생산성 도구로 사용 가능한 점이 너무 좋음
       반대로 iPhone을 선호하는 사람들도 이런 식으로 각각 자신의 장점과 의문을 가질 것이라 생각함
       Android와 iPhone 사용 경험이 왼쪽 뇌/오른쪽 뇌 타입의 차이인지, 아니면 hacker news 독자들이 대체로 Android를 선호하는지 궁금함
          + Android에서 Chrome으로 File System Access API를 쓸 수 있다고 하는데, Safari도 2022년부터 지원함
            최근 Apple은 PWA에 대해 180도 태도를 바꿨음
          + 나 역시 비슷한 입장이고 지금은 iPhone과 Apple ecosystem을 쓰고 있지만, 사실 iOS는 좋아하지 않고 macOS는 그나마 하드웨어 때문에 참고 쓰는 수준임
            개인적으로는 Apple의 프라이버시 정책이 결정적인 이유임
            Pixel 같은 제품을 privacy 보호, 데이터 보호, Apple 수준의 추적 투명성만 추가하면 바로 넘어갈 준비가 됨
            아직 Android에서는 앱들이 localhost 추적이나 소위 어둠의 데이터 수집 기법을 Google이 막지 않고 허용해서, 내가 선호하는 워크플로우가 더 나아도 사용하지 않음
            플랫폼 불문하고, 왜 아직도 앱 권한에서 네트워크 접근을 토글하는 옵션이 없는지 이해가 안 됨. 명백하고 의도적인 누락으로 보임
          + Apple은 유저들에게 “인위적 무능”을 강요하는 듯함
            어린애 취급하며 선택권을 거의 주지 않고, 감옥이나 다름없는 생태계를 자랑스럽게 포장함
            과거의 천재적이고 비전 있는 인물이 없어진 지금의 Apple은 자기복제만 남은 대기업 느낌임
            비정상적 방식만 강요해서, 다른 OS나 환경으로의 기술 이전도 어렵고, 소프트웨어 이식도 매우 힘듦
            Apple 사용자는 자신이 얼마나 좋은 환경에 있는지 모른 채, “그냥 잘 된다” 같은 소리를 하며 어두운 진실을 가림
            결국 상호운용성, 프로토콜, 자유로움을 보장해야 한다고 봄
            Google도 다를 건 없지만 최소한 내가 산 기기를 내가 소유한다는 점이 있음
          + 제시한 불만들은 내 구매 결정에 큰 영향을 주지 않을 정도로 사소하다고 생각함
            (1)은 잘 모르겠고, (2)처럼 사진 정리는 절대 하지 않음, (3)은 오히려 내가 원하지 않는 기능임
          + Pixel을 리눅스에 그냥 꽂아서 파일을 drag & drop 하는 게 큰 이점임
            (맥에서는 다른 프로그램이 좀 필요하지만 그렇게 어렵진 않음, 윈도우는 모르겠음)
            내 근무환경은 BYOD WiFi도 없고 네트워크 속도가 매우 느려서, 유튜브 영상을 오프라인 감상용으로 다운로드해두는 데 정말 편리함
            VLC도 Android에선 굉장히 좋고, iOS에선 그냥저냥
            사진 때문에 저장공간이 부족할 때, hassle 없이 사진을 빼내는 것도 매우 중요함
            iPhone에서는 마운트가 돼도 디렉토리 구조가 복잡해서 어렵거나 불편함
            이런 이유만으로도 Pixel의 utility가 엄청남
     * Tensor G5와 Gemini Nano가 함께 동작해 Magic Cue 같은 기능을 폰에서 직접 실행할 수 있는 점이 매우 흥미로움
       Magic Cue는 Siri가 올해 내놓지 못한 스마트 보조 기능과 닮아있음
       온디바이스 LLM이 모바일에서 간단한 질의나 lookup을 수행하는 건 AI 활용의 가장 실용적인 예라고 생각함
       내 캘린더에서 “이번 주말 야구 누가 가는지” 등 지능적으로 체크해주거나, “지난주 맥도글 저녁 얼마였는지” 등 결제 내역을 파악해주는 온디바이스 모델 아이디어가 너무 좋음
       이런 게 app intent와 tool call로 구현될 것 같고, 3년 안에 iOS 등 전체로 확산되길 기대함
          + 나만 그런지 모르겠지만, 나는 내 일상을 디지털로 거의 기록하지 않음
            소프트웨어 업계에 있어도, 가족 중 'geek' 취급받지만 영화나 야구 같은 일정은 캘린더에 넣지 않고, 디지털 월렛도 없음
            폰도 크고 불편해 잘 들고 다니지 않음
            Pixel 10의 기능들을 보며 “내가 뭔가 놓치고 있나?” 하는 느낌을 받았지만, 별로 필요성을 못 느낌
          + 온디바이스 모델이 AI의 대중화 분기점이 될 거라고 봄
            구독형 클라우드 AI 사용료에 질림
            AI가 하드웨어에 탑재되어, 꼭 모든 걸 처리하지 않더라도 필요한 기능만 온디바이스로 해준다면 진짜 붐이 올 것임
            그런데 Android 카메라 디자인은 왜 저런지 모르겠음
            Apple이 3구 카메라 넣었을 때도 눈에 익었는데, Android는 디자인적으로 좀 아쉬움
          + Nano 모델은 32억 파라미터에 4비트 양자화로, 데스크탑에서 돌릴 수 있는 오픈 모델이나 클라우드 챗봇에 비해 매우 작은 편임
            그래도 로컬에서 쓸 수 있는 건 멋지지만, reasoning이나 깊은 추론은 기대하지 않는 게 좋음
            사실상 자연어 정규표현식이나 단어 연상 게임에 가까울 것임
          + 경찰이나 사법기관 입장에서 보면, “폰에 있는 의심스러운 컨텐츠나 사용자 행동”을 온디바이스 모델에게 일괄 검색하게 할 수도 있을 듯
          + 온디바이스 AI 아이디어는 정말 좋은데, Android에서 Gemini 구현이 너무 짜증남
            어시스턴트 설정에서 앱을 선택할 수 있지만, Gemini app을 한 번만 실행하면 기본 어시스턴트로 자동 바뀌고, 사용자에게 물어보지도 않고 바로 적용됨
            여파로 네비게이션 등 여러 작업이 안 되고, 매우 불편함
     * Samsung S20+에서 Pixel 9 Pro로 바꾼 경험을 남김
       하드웨어 감촉, 디스플레이, 배터리, 속도 모두 만족스럽고 큰 후회 없음
       하지만 스크롤링 느낌이 삼성과 “약간 다르고” 덜 부드러움
       Pixel에서 삼성 환경보다 훨씬 더 커스터마이즈와 설정이 가능하다고 생각했지만, 오히려 삼성은 자체 앱 스토어나 다양한 무료 번들/앱이 많아 그 기능성을 알고 나니 Pixel과 큰 차이임
       그리운 부분
          + 앱별 볼륨 조절
          + 내비게이션 바 커스터마이징
          + 잠금화면 설정
          + Good Lock류 앱
          + 손전등 흔들기
          + Samsung 카메라 앱
          + 백그라운드 앱 제어
          + Nova Launcher 활용의 사소한 문제
            다음번엔 삼성으로 돌아갈 생각임
            Google이 더 나아지길 바람
          + 예전 Pixel, 지금은 Samsung을 쓰고 있는데 위 리스트에 대부분 공감함
            하지만 카메라 앱만큼은 Samsung보다 Pixel 버전을 더 좋아함
            Samsung 카메라 앱의 어떤 점을 선호하는지 궁금함
     * Pixel은 여전히 “엄청 크거나 우스꽝스럽게 큼”
       새로운 배터리가 필요한 순간까지 Pixel 4만 계속 쓸 듯함
          + 스마트폰 논의 때마다 작은 폰 수요가 등장하는데, 실제로 시장에서는 작은 폰 판매가 잘 안 됨
            심지어 옛 기기 계속 쓰는 사람이 많아, 작은 사이즈 신모델 우선순위가 더욱 떨어지는 현상이 있음
            그 외에도 작은 폰이 안 나오는 요인들이 더 있을 것 같음
          + Pixel 4a(특별히 작은 폰도 아님) 배터리가 너프된 후에 바꿨지만, 다시 꺼내 쓸 때마다 그 폼팩터가 즉시 더 좋다는 걸 느낌
            최신 6인치 이상 스마트폰은 한 손으로 매듭 묶기만큼이나 불편함
          + Fold 시리즈라도 접었을 때 작은 사이즈면 좋겠음
            “작은 폰” 충족과 큰 화면이 필요할 때만 펼쳐 쓰면 되는데, 현재 Fold도 접었을 때 너무 크고, 펼치면 아예 태블릿임
          + 지금 스마트폰 크기는 거의 모든 업체가 이 사이즈로 정착했고, 대부분의 사용자가 선호하는 트렌드임
            작은 폰이 없어진 게 아쉽고, 기존보다 판매가 안 된다는 건 어쩔 수 없음
            개인적으로는 얇은 본체에 거대한 카메라 돌출이 더 불만임
            이게 멋있다고 생각하는 사람이 있는지 궁금함
            카메라를 얇게 못 만들 거라면 차라리 바디를 더 두껍게 해서 배터리, 스피커, 진동 모터, 내구성 등도 강화할 수 있음
          + Samsung S25는 Pixel 4a보다 겨우 3mm 더 클 뿐임
            이 차이도 너무 크게 느껴지는지 궁금함
     * AI로 구현된 “가짜 줌”이 너무 거슬림
       사진 찍을 때 진짜 광학 신호가 픽셀로 변환되는 그 결과를 원함
       근거 없는 뭔가를 만들어내는 건 싫음
          + 이런 기술이 “줌”이랑 “디테일 복구”라고 마케팅되는 게 무섭다고 느낌
            실제로는 진짜로 뭔가를 복원하는 게 아니라, AI가 그냥 그럴싸하게 조작해서 넣는 것임
            우리 같은 사람은 알고 있지만, 대다수는 이조차도 모르니 위험하게 느껴짐
            만약 AI가 임의로 만든 얼굴이 내 얼굴이랑 비슷해서 억울하게 잡혀가는 날도 올 수 있을 듯
            변호사가 AI 기반 증거를 잘 걸러내길 바라지만, 소셜 미디어는 이런 실수로도 인생을 망칠 수 있음
          + “줌”이란 단어를 쓸 가치조차 없고, “보간 + 환각” 정도임
            그래도 이것도 결국 ‘CSI의 줌&확대’를 실현하는 기술임
          + 스마트폰으로 사진 찍는 것 자체가 이미 “진짜 광-픽셀” 관계는 한참 전부터 사라짐
            최소한 디모자이싱, 다크 프레임 서브트랙션, 톤 매핑 등 필수적 연산을 거쳐야 우리가 아는 사진이 되고
            최근은 자동 브래킷, 스태킹, 흔들림 보정, 롤링 셔터 감소 등 모든 처리가 자동으로 포함됨
            본인이 말하고 싶은 건 고전적인 이미지 처리와, 오늘날 AI 기반의 불투명한 환각 기법의 차이겠지만, 그 경계가 모호함
            실제로 디모자이싱이나 슈퍼레졸루션, 수퍼줌 모두 “더 나은 근사값”을 만드는 방식이기 때문
            마음에 안 들면 폰으로 RAW 출력을 하거나, 미러리스로 모든 프로세스를 직접 제어하는 것도 선택지임
            Apple ProRAW 등은 하이브리드 포맷이라서, RAW 역시 제조사 표준에 다 담지도 못함
            재미있는 사례로 Gigapixel AI가 Ryan Gosling 얼굴을 사진에 삽입했던 일화도 있음
          + 동의함, 이미 세상에 가짜가 넘쳐나는데, 그냥 유저가 다수 “진짜 줌”인 줄 알고 추가 가짜를 만든다는 것도 문제임
     * Pixel 10 스펙 (링크)을 보면 VPN 지원이 있지만, 각주에 “일부 데이터는 VPN을 거치지 않음”이라고 써 있음 (상세)
       어떤 데이터가 VPN을 통해 가지 않는지 궁금함
       배터리 수명이 24시간 이상이라는 건 굉장히 반가운 소식이지만, 이것도 실제 환경에서는 줄어든다는 각주가 달려 있음
       납득 가능하다고 생각함
          + 도움말에 따르면, VPN에서 보호하지 않는 데이터는 다음과 같음
               o USB/와이파이 테더링 트래픽
               o 푸시 알림
               o Wi-Fi 전화 및 기타 IMS 서비스
               o 워크 프로필 앱 트래픽
               o 직접적으로 와이파이나 셀룰러에 트래픽을 보내는 앱
                 Wi-Fi 알림은 스마트워치 동기화 때문일 수도 있다고 예상함
          + Xiaomi 14T와 Pixel 9을 비교해 봤는데
            Pixel이 배터리 압도적으로 좋아서, 내 선택이 아쉬울 지경임
            나도 헤비 유저는 아니지만, 식당 검색, 지도, 사진, Gemini 등 사용하다 보면 내 폰은 50%일 때 Pixel은 75% 유지함
          + 어떤 종류의 데이터가 VPN을 통과하지 않는지 확실치는 않지만, Google Fi 같은 캐리어 앱은 VPN과 잘 안 맞음
            이유는 캐리어 간 전환을 위해 실제 IP 기반 통신이 필요해서임
            Wi-Fi 콜링도 VPN과 충돌이 많아 제외되는 경우가 많음
     * 물리 SIM 트레이가 없는 점은 사용자 잠금을 위한 Google식 울타리임
       eSIM 지원은 Android 오픈소스(AOSP)에 없고, GMS에 포함되어 있음
       Google-free Android 분기(OS)에서는 Pixel 10 시리즈 사용이 불가함
          + 미국 한정이고, 아이폰도 마찬가지이기 때문에 결국 미국 통신사들이 주도한 변화임
            Google-free OS에서도 OpenEUICC로 정상 사용 가능함
          + 이 말은 GrapheneOS도 못 쓴다는 의미임?
          + Pixel 6~9에서는 듀얼 인젝션 방식으로 0.00001센트라도 아끼려다 SIM 트레이가 찢어지는 문제가 있어서, 출장 5번만 해도 트레이가 망가지면 AS도 안 되고 이베이에서 직접 구매해야 했음
     * Pixel의 가장 큰 문제는 여전히 하드웨어임
       동세대의 Snapdragon 8e 대비 50% 이상 느리고, 이로 인해 성능/배터리 측면에서 뒤처짐
       가격도 크게 저렴하지 않음
       GrapheneOS만 아니었으면 Pixel을 살 이유가 없을 듯
       최근 픽셀 카메라도 그렇게 뛰어나지 않고, (픽셀 카메라 처리 관련 정보)
       Mrwhosetheboss의 유튜브 리뷰가 모든 점에서 Pixel이 모자란 이유를 잘 설명함
          + 마지막 두 Pixel(6a, 7a)이 모두 배터리 이슈로 리콜되어 거의 전액 환불받고, 폰은 그냥 가질 수 있었음
            7a는 리콜 후 몇 달 만에 죽었고, 6a는 아직도 작동 중
            iPhone 배우기 귀찮아 일단 Pixel 9a로 다시 구입했지만 이번에도 비슷한 문제가 터질 것 같아
            백업용 iPhone도 구입해 둘 예정임
     * 기존 Android 데이터의 기기 간 이전이 편해졌는지 궁금함
       매번 업그레이드시, 앱/설정 전송 옵션을 골라도 제대로 된 것은 몇 개 안 되고, 대다수 앱은 로그인부터 설정까지 전부 다시 해야 해서 불만임
       일부 앱(예: Uber)은 예외였고, 이런 경험이 보편적이어야 한다고 생각함
       은행/결제앱은 별도 보안정책이 있어 필요시 재설정하는 건 이해하지만, 대부분의 앱에서는 이런 완벽한 이전이 필요함
          + 이는 Google 탓이 아니라 앱 개발사 탓임
            Google은 기기 이전용 백업/복원 제공 모드를 추가했고, 백업 블랙리스트도 우회 가능함(문서)
            단, 앱들이 의도적으로 백업에서 빠지기도 함
          + Pixel끼리는 Pixel 4 이후로 매우 부드럽게 이전됨
            교차 제조사 이전은 한동안 안 해봐서 잘 모름
     * Apple에 점점 실망하고 있고, AI 어시스턴트가 내게는 핵심 기능임
       LLM을 향한 과대포장에 비판적인 입장이지만, Siri로 방 하나의 불만 켜는 게 여전히 안 될 정도로 기능이 엉망이라 “이건 아닌데” 싶음
       ChatGPT 앱은 내 4살 아이도 크로아티아어로 잘 쓸 정도임
       Google Gemini가 비슷하다면, 적어도 어시스턴트 기능은 절반쯤 기대할 만함
          + 나는 접이식 Pixel(9)으로 갈아탔고, Android UX는 그다지 취향이 아니지만,
            Google폰으로 못 돌아오는 이유 두 가지
               o AI 통합이 정말 훌륭함
               o 폴더블폰의 즉각적인 대형 스크린으로 인해 기존 싱글 스크린으로 못 돌아감
"
"https://news.hada.io/topic?id=22668","Rust의 핵심","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Rust의 핵심

     * Rust는 다양한 개념이 서로 긴밀히 얽혀 있는 언어로, 기본 프로그램을 이해하기 위해서도 많은 요소를 동시에 배워야 함
     * 함수, 제네릭, 열거형, 패턴 매칭, 트레잇, 참조, 소유권, Send/Sync, Iterator 등은 모두 상호작용하며 설계된 핵심 요소임
     * 자바스크립트와 비교했을 때, JS는 일부 개념만 알아도 코드를 작성할 수 있지만, Rust는 언어 전체의 맥락을 이해해야 비로소 의미 있는 코드 작성 가능
     * Rust의 이 같은 복잡성은 학습 장벽을 높이지만, 동시에 안전성과 일관성을 제공하고, 코드 설계 방식에 큰 영향을 미침
     * 이러한 언어적 짜임새는 Rust를 특별하게 만들며, “더 작은 Rust”의 비전은 정교하게 결합된 언어 철학을 다시 돌아보게 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Rust 학습의 어려움

     * Rust는 진입 장벽이 높음에도 불구하고 많은 사람들이 문서, API, 진단 개선에 기여해왔음
     * 기본 개념으로는 함수 일급 객체, 열거형, 패턴 매칭, 제네릭, 트레잇, 참조, 빌림 검사기, 동시성 안전성, 반복자 등이 있음
     * 이 개념들은 서로 의존하며 얽혀 있어 하나씩 따로 배우기 어렵고, 표준 라이브러리도 대부분 이 기능들을 활용함
     * 20줄 남짓한 Rust 코드조차 이해하려면 함수형 패러다임, Result와 에러 처리, 제네릭 타입, 열거형, 반복자 등 여러 요소를 동시에 파악해야 함

Rust와 JavaScript 비교

     * 동일한 파일 변경 감지 프로그램을 Rust와 JS로 작성했을 때, Rust는 다수의 언어적 개념이 얽혀 있음
     * JS는 기본적으로 함수와 null 처리만 이해하면 충분히 동작하는 코드를 작성할 수 있음
     * 이는 Rust가 단순히 더 어렵다는 의미가 아니라, Rust는 언어 전체의 구조적 이해를 요구하는 설계임을 보여줌

Rust의 상호 결합된 설계

     * Rust의 핵심은 유기적으로 설계된 기능들의 결합임
          + 열거형은 패턴 매칭 없이는 불편하고, 패턴 매칭도 열거형 없이는 제약적임
          + Result와 Iterator는 제네릭 없이는 구현이 불가능함
          + Send/Sync 개념과 println 제약은 트레잇이 있어야만 안전하게 표현 가능함
          + 빌림 검사기는 클로저의 캡처 분석을 통해 Send/Sync 안전성을 보장함
     * 이러한 상호 결합은 Rust를 단순히 기능의 집합이 아닌, 통합된 언어 체계로 만듦

작은 Rust의 비전

     * 2019년 without.boats는 “Smaller Rust”를 언급하며 작고 정제된 Rust의 가능성을 논의했음
     * 오늘날 Rust는 훨씬 커졌지만, 작은 Rust의 개념은 정교하게 맞물린 언어 설계의 본질을 상기시켜줌
     * Rust의 매력은 언어적 요소가 서로 독립적이면서도 결합할 때 강력한 표현력과 안전성을 제공한다는 점임

결론

     * Rust는 학습하기 어렵지만, 서로 얽힌 개념들의 일관성과 통합성이 큰 강점으로 작용함
     * 이러한 구조 덕분에 Rust는 개발자가 코드를 단순히 작성하는 것이 아니라, 안전성과 성능을 동시에 고려하는 사고 방식을 갖게 만듦
     * Rust의 본질은 “작고 정교한 핵심 언어”에 있으며, 이는 오늘날 확장된 Rust에서도 여전히 중요한 철학으로 남아 있음

        Hacker News 의견

     * ""간단한"" JS 프로그램에도 버그가 있음에 아이러니를 느낌. fs.watch 문서상 콜백에서 filename이 null일 수 있음을 반드시 체크해야 한다고 명시돼 있음. Rust라면 이 사실이 타입 시스템에 반영되어 무조건 처리하게 만들지만, JS에서는 대충 코드를 짜기 쉬움. 관련 문서
          + Typescript를 사용하면 null 체크가 강제됨. 그래서 TS가 JS에서 Rust 쪽의 올바름에 좀 더 가까운, 비교적 부담 없는 단계임을 보여주는 좋은 예라고 생각함
          + 추가적인 버그도 있음: for path in paths는 for (const path of paths)가 되어야함. JS는 괄호가 없으면 바로 에러를 내긴 하지만, in과 of의 차이는 값이 아닌 인덱스(iterable index)를 순회하므로, 실제로 인덱스가 string으로 변환되어 fs.watch의 첫 인자로 들어가 버림. 심지어 TypeScript도 이 실수를 캐치하지 못할 수 있음
          + 지적대로 루프 문법 자체가 부정확한데, 실행만 해봐도 금방 알 수 있음. 즉, 작성자가 그 JS 코드를 신경써서 작성하지 않았고, 논점에 별 의미가 없었기 때문이라고 보는 게 나을 듯함
          + 내가 못봤거나, kind가 어디서 온 것인지 궁금함. console.log(""${kind} ${filename}"")에서 kind가 아니라 eventType(문자열)이어야 맞음
     * 사소한 지적 하나 하고 싶음. Rust의 println은 Display나 Debug 트레잇을 구현한 타입만 출력 가능함. 그래서 Path는 직접 출력 불가. 모든 OS가 UTF-8에 맞는 path를 저장하지 않고, Rust의 스트링 타입은 전부 UTF-8임. 즉 Path 출력은 손실이 발생할 수 있음. Path는 display 메서드로 Display 구현 타입을 반환함. Rust는 이를 타입 시스템에 녹였지만, JS/TS에서는 내부적 string이 UTF-16임을 명시하기 힘들고, Unicode가 아닌 path는 직접 TextEncoder/Decoder를 써야 제대로 다룰 수 있음. 예전 경험상 서버에서 Shift_JIS로 텍스트를 보내온 걸 response.text()로 읽으면 런타임에서 빈 문자열만 나옴. 인코딩 이슈에 익숙하지 않다면 이런 상황에서 디버깅으로 며칠 보낼 수 있음.<br>그리고 JS 예제는 Rust 코드에는 없는 버그와 문법 오류가 있음(루프에서 for-in 대신 for-of 필요). 이 예제에서 ""일급 함수""만
       쓴다고 보기는 어렵고, Rust처럼 이터레이터 이해도 필요하며, CommonJS를 쓰고 있음. 또 async/await, Promises 및 top-level await도 새롭게 배워야 하고, top-level await는 node 포함 일부 런타임에서 최근에야 지원됨. 여전히 일부 JS 엔진(예: React Native의 Hermes)에선 지원 안 함
          + 이런 점이 내가 계속 Rust를 쓰는 이유임. 예제는 하나일 뿐이지만, 이런 자잘한 문제와 함정이 다른 언어엔 항상 널려있음. 개별적으로는 안 일어날 수도 있지만, 전체 프로그램 수명 주기에서 모이면, 어디서인지 계속 이상한 버그가 튀어나오고 끊임없이 찾아야 함. Rust에서는 이런 일이 발생하지 않음. 타입 시스템이 말도 안 되게 많은 경우를 미리 차단해 줌. 실제로 Rust로 기능 다 만든 소프트웨어를 출시하고 나면, 기능 추가만 가끔 했고 일반적인 버그 잡기 수고가 거의 사라짐. 물론 어디든 논리적 버그는 생길 수 있지만, 다른 언어처럼 어리석은 타입/구조 불일치에서 비롯된 문제를 원천 차단해주기 때문에 생산성과 유지관리가 완전히 다른 경험임
          + 개인적으로 JS/TS에서 thenable/Promise와 async-await를 진짜 제대로 아는 개발자가 많이 없다고 느낌. 이런 것도 본 적 있음:<br>
var fn = async (param) => new Promise((res, rej) => {
  fooLibraryCall(param).then(res).catch(rej);
});

            콜백 형태 래퍼를 그대로 Promise로 감싸서 async 함수 안에서 다시 사용함. 이럴 때마다 너무 마음이 아픔. 실제로 이런 코드 곳곳에서 봄.<br>또, 모듈 import 및 async import(), 트랜스파일, 코드 스플리팅 등까지 생각하면 진짜 복잡함
     * Bjarne 인용문은 사실 C++이 점점 나빠지는 걸 반복적으로 정당화하기 위한 세일즈 피치라고 생각함. 초반엔 진심일 수도 있지만, 이제는 모델이 반복임. 구조는 이렇다고 봄:
         1. ""C++ 안에는 더 작고 깔끔한 언어가 있음""
         2. 그런데 언어를 subset해서 뽑아낼 순 없으니, 먼저 superset(다기능화)를 만들고 그 후에 subset을 하자고 함
         3. superset은 새로운 C++N+1에 들어감. 진짜 subset 논의는 그다음에 한다면서 미루고 반복
         4. C++N+1은 더 복잡해지고, 이런 식으로 영원히 반복 반복적으로 이런 걸 본 사람들은 왜 계속 남아있는지 이해가 안 됨. 결국 ""더 작고 깔끔한 언어""는 결코 안 나옴. 항상 step one 반복임
          + xkcd 927과 닮았으면 떠오름 xkcd 927. C++ 표준 매번 점점 더 복잡해져서, 좋은 변화도 있지만 기존 버전과 잘 안 맞기도 하고 소스코드는 갈수록 엉망이 됨. 두 개 OSS 라이브러리 관리 중이지만 지금은 거의 안 씀. 언제까지 버텨야 할지가 요즘 고민임.<br>Rust는 c++11/14/17/20에서 넘어와서 진짜 상쾌함. 단, Rust도 전체를 다 모르면 충분히 방대함. 이번 글에서 지적한 내용이 매우 적절하게 느껴짐
     * shebang(자체 실행 rust스크립트)을 보는 순간 바로 산만해졌던 사람 없음? 예전에 Go에서 똑같은 걸 발견했을 때처럼 놀람. 꽤 유용해 보여서 기본적인 용도로 충분히 쓰일 수 있을 듯. rust로 빌드/테스트 파이프라인 관리하는 프로젝트에서 비슷하게 본 적도 있음. 이런 용도엔 꽤 괜찮은 대안이 될 것임.<br>다만 나는 대체로 bash에서 조금만 벗어난 스크립트가 필요하면 Deno+TS를 씀. JS를 가장 오래(28년간) 다뤄왔고, 그다음 C# 24년임. Node 초창기부터 사용함. Deno가 패키지 공유/중앙화 측면에서 Node나 Python보다 더 관리하기 쉬움. cargo 프런트매터도 유사하게 동작함
          + 내가 cargo에 script 통합을 직접 설계/구현했던 사람임(그간 서드파티 구현들도 많았음). 실사용 사례를 봐서 너무 기쁘고 언급된 걸 확인해서 좋았음. 문서도 참고 바람. 어떤 모양이 적합할지, 언어와 어떻게 연동할지, 첫 릴리즈의 범위는 어느 정도로 잡을지 등 지난한 논의가 있었음. 현재는 스타일 가이드와 Rust 레퍼런스 업데이트 등 마무리 작업 중이고, 남은 큰 일은 rustfmt, rust-analyzer 관련 디테일과 rustc의 버그 픽스 및 Cargo의 에러 리포팅 개선임. 나 스스로 매일 cargo script로 이슈 재현용 스크립트 작성함
          + 실제로 ""-Zscript"" 기능 키워드 검색 시작해서 리서치하다가 산만해진 거임. 2023년부터 진행 중이고, 거의 완성에 가까워 보이는 오픈 이슈도 있음. ZomboDB 저장소에서 역시 rust로 빌드 파이프라인 처리하는 거 봤으나 전체 맥락은 완전히 이해하진 못했음.<br>cargo 프런트매터가 스크립트 이식성에서 엄청 유용함을 언급하고 싶음. 파일 하나만 공유하면 되고, Python이나 Node.js처럼 추가 설치/초기화 없이 바로 의존성 가져와 쓸 수 있음
          + Go에서도 같은 걸 할 수 있다고 했는데 자세히 설명해 줄 수 있는지 궁금함. 관련 링크라면 나도 관심 있음
          + JS와 C#을 오래 쓴 입장이지만, 2025년에 그런 이유로 어떤 시스템을 선택하는 건 별로임. 지난 20년에 정말 많은 것들이 훨씬 더 나아짐
          + 그냥 기본적인 Unix의 기능일 뿐임. #!/some/path로 시작하는 파일은 그냥 셸에서 지정 명령어로 파일 전체를 stdin으로 넘겨서 실행함
     * Rust에서 ""더 작고 깔끔한 언어가 안에서 나오고 있다""는 말에서 정확히 그 언어가 뭔지 궁금함. 글을 보면 레퍼런스, 라이프타임, 트레잇, 열거형 등 다 남아있어야 동작한다는 뜻이고, 그러면 거의 Rust랑 다를 바 없음. 마지막 파트에서 ""쓰고 싶은 Rust""와 ""과거의 Rust"" 두 가지 힌트가 나오는데 별로 와닿지 않음. withoutboats의 ""Notes on a smaller Rust""도 읽었는데, 디자인 목표 자체가 Rust와 달라서 Rust가 되려는 게 아니라 새로운 언어 설계를 고민할 때 Rust에서 얻을 수 있는 교훈을 보여주는 정도임. Rust가 되고자 하는 언어가 아니라, ""메인스트림"" 요구에 맞춘 언어 사례(예: GC, 컴파일/문법 단순화 등)임. 두 번째로, ""내가 2018년에 처음 배울 때 사랑에 빠졌던 언어가 그 '더 작은 Rust'""라는 얘기도 나오는데, 실제로 2018년 이후 Rust는 본질적으로 많이 안 바뀌었음. edition
       변경 등 대부분 문법적 유연성 개선이고, 진짜 큰 예외는 async와 const뿐임. 그렇다면 ""async와 const가 들어가기 전의 Rust가 더 작고 깔끔했다""고 말해주면 되는데, 본문에서는 그렇게 직접적으로 설명이 안 되어 아쉬움
          + '더 작고 깔끔한 Rust'를 말한다면 Austral 언어가 예시로 떠오름
          + Rust가 가진 핵심 개념을 간직하면서 더 단순한(더 작은) 언어가 가능하다는 주장도 있음. 예를 들어, Copy trait, reborrowing, deref coercion, 반복문에서 자동 into_iter, 범위 종료 시 자동 drop 호출(이것도 직접 호출하거나 컴파일러가 에러주면 됨), trait bound에서 기본 :Sized, 라이프타임 축약(lifetime elision), match ergonomic 등 여러 자동화/편의 요소들을 다 빼면 진짜 기계적으로 단순한 Rust가 가능함. 하지만 이런 언어는 일상적으로 쓰기엔 매우 불편함. 위 요소들은 사실 초보자를 위해 고안된 것들이란 점이 아이러니임
          + 정말 꼼꼼하게 읽었음. 실제 내 의도도 Rust가 async와 const 도입 전이 더 작고 깨끗했다는 얘기임. 직설적으로 표현하지 않은 이유는 해당 기능 개발자 친구들이 많아서임. Matklad가 lobste.rs에서 아주 잘 표현해줌. 2015년 Rust가 더 완성형이고 일관성이 좋았으나, Rust의 비전은 완전한 일관성(coherence)이 아니라, 산업에서 쓸모있는 언어가 되는 것임
     * 나는 편견이 있을 수 있지만 Rust가 가장 완벽에 가까운 언어라 생각함. 빌림 검사기(borrow checker)가 귀찮긴 하지만 꼭 필요함. 똑같이 버그 있는 코드(C)였다면 런타임 붕괴가 났을 것임—결국 그때도 버그는 고쳐야 함. 차이는 Rust는 아예 컴파일 전 미리 버그를 해결하게 강제하지만, C에서는 한밤중 오류에 수습을 하게 됨. Rust가 어렵다기보다 다른 사고방식의 전환을 요구함. 안전하고 보안성 높은 코드를 쓴다는 패러다임 쉬프트가 필요함. 변화는 대부분 불편하지만, 그게 Rust에 대한 거부감의 근본 원인인 듯함
          + Rust는 완벽과는 거리가 있음
               o Deref 적용 시점과 순서를 컴파일러가 너무 자유롭게 결정할 수 있다고 봄. .into()와 From 트레잇은 타입 변환을 너무 은밀하게 처리함. 표준 라이브러리에도 이런 ""편의"" 함수가 많음. 결국 객체의 타입이 모호해지고, 함수 호출과 구현 연결이 어려워짐(물론 IDE가 도와주면 조금 나음)
               o 암시적 반환(implicit return)은 프로그램의 흐름을 가려서 실수를 불러옴. 물음표 연산자도 썩 맘에 들지 않음
               o 작은 Rust 모듈을 너무 세분화해서, 유용한 걸 하려면 수백 개 디펜던시가 필요함. 각자 따로 관리하고 vendor해야 안정적인 빌드가 되는데, 이게 정말 불편함
               o Async Rust는 지금 혼돈 그 자체임
          + 빌림 검사기(borrow checker) 자체에 불만이라기보다, Rust 자체의 ""덩어리""가 너무 커졌다는 게 주된 지적임. 2018년의 투박한(?) 불완전한 Rust를 좋아했던 사람들(나 포함)에겐 지금이 별로 매력적이지 않음. 물론 능숙하게 쓰면 아주 강력하지만, 정말 그만한 노력을 들일 가치가 있나 자문하게 됨. 2025년에 C/C++의 대안으로는 Zig를 고를 것 같음(유일한 예외는 Postgres 작업, pgrx 생태계는 아주 독보적임). 그래도 C로 일하는 것보단 뭐든 나음
     * 처음 배우는 언어로 Rust를 추천하면 안 된다고 생각함. 첫 언어 배우기는 원래 힘든데 Rust는 컴파일러 에러 때문에 코드가 완전히 완벽해질 때까지 실행조차 보기 힘듦. 너무 좌절스럽고 쉽게 포기할 것임. Python, JavaScript, Lua부터 시작해서 게임 같은 걸 빠르게 만들어보고 반복하는 게 낫다고 조언함
          + 내 경험은 다름. 우리 회사 ML 엔지니어가 파이썬밖에 몰랐는데, Rust 코드베이스에 기여하고 싶어해서 내가 한 시간 정도 기초만 설명해줬더니 금세 잘 적응했고, 생산성도 바로 올라갔음. 사실 게임을 만들다가 문자열을 숫자 함수에 넘겨서 크래시 나면 원인 추적하는 게 시간 다 가는 일임. Rust에선 컴파일러가 아예 ""여기 string인데 int여야 한다""고 표시해 주니까 차라리 디버깅 금방 끝남. 하루 종일 컴파일 에러 잡는 대신, 런타임 에러로 일주일 고생 안 함
          + 블로그 맨 위 인용구의 본인임. 내가 Rust를 첫 언어로 400명 넘게 가르쳐봤는데, 이 스레드의 주장들이 매우 재밌게 느껴짐. 오랜 기간 직접 경험하면서 가능성뿐 아니라 꽤 잘 통한다는 근거도 충분히 얻음
          + 난 아직 확신이 없음. Rust를 첫 언어로 좋은 교육자가 시도하는 걸 보고 싶음. 세대가 바뀌어 대학에서도 Python을 많이 쓰지만, 이론적으로는 Rust가 첫 언어로서 전체 코호트의 수준을 올릴 수도 있을 거라 생각함(물론 fail rate가 너무 높아서 행정상 문제 될 수도, 반대로 고급 학생은 더 많은 걸 익힐 수 있음). move assignment나 ""const"" 키워드 의미처럼, Rust에서 배우는 게 오히려 나중에 기존 언어들에서 배운 잘못된 습관을 걷어내야 하는 수고를 줄여주는 면도 있음
          + 보통은 첫 언어로 정적 타이핑은 피하라고 권하고 싶음. 나도 정적 타이핑을 좋아하지만, 초보자 입장에선 괜히 혼란만 가중됨. 컴파일러 에러는 대게 반사실적이고, ""컴파일러가 이게 none이 아닌지 증명하지 못했다"" 식 메시지는 테스트 케이스에서 런타임 크래시로 바로 위치를 찾는 것보다 훨씬 어렵게 느껴짐. 직접 한 줄씩 출력값 찍으면서 트러블슈팅하면 대부분 금방 해결할 수 있는데, 컴파일러의 난해한 에러에 막히면 진짜 길게 헤맬 수 있음
          + Rust는 한 번에 모든 걸 받아들일 수 있으면 나쁜 언어가 아님. 문제는 아무도 그런 식으로 언어를 배우지 않는 점이고, 주요 개념을 충분히 모르면 Rust에서 반복적으로 시행착오를 겪게 됨. 그리고 결국엔 다른 언어에선 전혀 배우지 않은 개념들도 많아서, 새로운 언어로 옮길 때 다시 좌절할 수 있음
     * 내가 본 ""간단한 Rust""의 가장 근접한 사례는 Gleam. Rust에 상당히 영감을 받은 듯함
          + Gleam이 Rust에 영감을 받았다는 건 오해임. 제작자가 공식적으로 그렇게 말하지 않음. 컴파일러는 rust로 되어 있지만, Gleam은 패러다임이나 대상 런타임이 완전히 달라서 Rust 대체재는 아님
          + 또 다른 'simple rust' 스타일을 원하면 fsharp를 살펴 볼 것도 추천함
          + Gleam 메인 페이지에 흑인 인권, 트랜스권, 반나치 메시지가 있어서 나는 이 언어에 전혀 관심 없음
          + Gleam에서 3D를 만들 수 있는지 궁금함
     * ""Rust 프로그램이 뭐하는 건지 설명을 안 한다""는 점이 신경 쓰였음. 엄청난 기술적 설명이 있는데, 프로그램이 실제로 뭘 하는지 요약이 없음. 실은 파일 변화 감지해서 프린트하는 것뿐임. Rust로는 이런 단순 작업도 구현은 복잡해져서, 실제 문제랑 관계없는 내부 세부사항까지 신경 써야 한다는 게 언어의 어려움을 잘 보여줌. 이 복잡함이 맞닥뜨릴 도전이자 동시에 자기주도적으로 만든 장벽이라 보는 입장임
          + 다른 언어들도 똑같은 문제가 있지만, Rust는 그걸 미리 다루게 도와줌. 모든 파일명이 프린트 가능한 것은 아니고, 대부분 언어는 이 부분을 사용자한테 넘겨버림. Rust는 반환 타입으로 에러/실패를 분명히 표시하지만, 다른 언어는 예외처리 같은 다른 메커니즘이 필요함. 일견 단순해 보여도 실제로는 Rust 쪽이 더 직관적일 수도 있음
          + 구현도 사실 고성능 언어치고는 매우 단순한 것임. 한 페이지 내에 다 들어감. 이 정도면 충분히 단순한 예제가 아님?
          + 단순 설명 = 단순 구현이 항상 성립하지 않음. XKCD 1425에 사례가 잘 나옴. (예: 사진이 국립공원 안에서 찍혔는지 확인은 쉽지만, 그게 새 사진인지까지 구별하려면 연구팀이 필요함) xkcd 1425
     * Rust는 의미적으로 꽤 일관되고 응집력 있다고 느낌. 다른 언어에 비해 당이 역할, 설탕 같은 게 적어서 더 직관적임. 모든 인터페이스는 보통 mem 모듈 패턴을 따르니, 인터페이스 구조를 확실하게 이해하려면 std::mem부터 시작하는 게 좋음
"
"https://news.hada.io/topic?id=22626","커스텀 조화 웨이브 기어와 ESP32를 활용한 천체 망원경 마운트 제작기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                커스텀 조화 웨이브 기어와 ESP32를 활용한 천체 망원경 마운트 제작기

     * 저자는 조화 웨이브 기어(하모닉 드라이브) 와 ESP32 마이크로컨트롤러를 이용해 커스텀 망원경 마운트를 제작했음
     * 기존 상용 추적 마운트는 가격이 매우 높아서 DIY로 직접 설계 및 제작을 선택했음
     * PCB 설계 및 제작 과정, FreeCAD 3D 모델링, 구성 부품 선정 등 전체 설계 과정을 상세하게 설명함
     * 전체 제작 비용은 약 1,700유로 수준으로, 1대 단가 기준 상용 제품 대비 경쟁력을 확보함
     * 자체 제작 마운트와 OnStepX 펌웨어 통합을 통해 실제 천체 촬영 성능 및 개선 경험을 공유함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

새로운 출발점

   몇 년 전, 저자는 유튜브 천체사진 채널에 영감을 받아 천체사진에 관심을 가지게 됨. 삼각대 위에서 짧은 노출 시간으로 수백 장을 촬영한 후 Siril 소프트웨어로 합성하는 방식으로 오리온 성운 촬영을 시도함. 그러나 추적장치의 필요성을 느껴 Move Shoot Move 트래커를 구입했지만, 목표 천체 찾기와 극축 정렬의 어려움, 미흡한 결과로 인해 더 본격적인 망원경 마운트 제작에 관심을 가짐.

PCB 제작 경험의 확대

   2024년, 맞춤형 PCB 설계 관련 유튜브 영상을 우연히 접하고, 기존의 엉성한 브레드보드 대신 깔끔하고 저렴한 제작용 PCB 사용법을 익히게 됨. 첫 번째 프로젝트로 ESP32와 e-paper 디스플레이, BME680 센서를 활용한 스마트 온도조절기를 제작함. 이 경험을 바탕으로, 망원경 마운트에도 직접 PCB 설계와 제작 기술을 적용하기로 결심함.

본격 연구와 커뮤니티 자원 활용

   하모닉 드라이브 채택을 중심으로 한 설계를 구상. AliExpress와 다양한 DIY 커뮤니티(HEMY, HrEM, DHEM, DIY EQ Mount V2 등)의 오픈소스를 참고해 부품 선정과 기구 구조 연구에 많은 시간을 투자함. 스테퍼/서보 모터, FOC 제어, SimpleFOC 등 각종 오픈소스 구현체와 커뮤니티 정보를 조사함.

설계 결정 및 구조

     * RA축(적도축) : 42AIM15 서보모터 + Type 17 하모닉 드라이브(100:1 감속)
     * DEC축(경도축) : MKS Servo042D 스테퍼 + Type 14 하모닉 드라이브(100:1 감속)
     * 마운트 및 하우징: Arca Swiss 플레이트 채택, Move Shoot Move 웨지와 호환
     * 운영 모드: GEM(적도의) 또는 ALTAZ(수평-수직)
     * 마이크로컨트롤러: ESP32-S3
     * 전원: USB-C PD 최대 24V/4A
     * 모터 제어: step/dir/en, ULN2003 + MODBUS, CANBUS
     * 확장성: 잔여 GPIO 핀 외부로 노출

   모터별 마이크로스텝 및 서보제어 특성을 통해 설계 단순화 및 추적 정밀도를 향상함. CANBUS를 통한 마이크로스텝 동적 변경으로 고속 슬루(위치이동) 및 정밀 추적의 균형을 구현함.

PCB 설계 및 문제 극복

     * KiCad로 반원형 PCB 설계, 케이스에 딱 맞는 형태로 구성
     * ESP32-S3 무안테나 모듈로 배치 자유도 확보 및 USB-C 전원 입력 회로(최대 24V) 채택
     * PicoPD 오픈소스 회로 및 AP33772 IC 활용. JST PH 시리즈 커넥터 선정으로 소형/고용량 커넥션 구현
     * 최초 IC 대체 과정에서 I2C 결선 실수 및 오작동 경험, 2차 버전에서 검증 및 테스트포인트 다수 추가로 문제 해결

OnStepX 펌웨어 연동

   오픈소스 OnStepX 펌웨어를 적용해 망원경 제어 및 WiFi 통신 지원. 초기 슬루 동작(빠른 포인트 이동) 시 ESP32가 과부하되는 안정성 문제를 겪었으나, 슬루 속도 감소 및 WiFi 클라이언트 모드 전환으로 해결함. OnStepX에 맞는 핀 레이아웃 파일과 마이크로스텝 동적 제어 코드만 추가해 별다른 수정 없이 통합에 성공함.

제조 및 조립 과정

   PCB 제작과 CNC 금속 가공 모두 JLCPCB에서 진행. 사전 3D 출력 테스트 없이 CAD 도면만으로 CNC 주문하는 과감한 선택을 통해 만족스러운 부품 정밀도를 얻음. 단, 적도축 캡 설계 미스 발생—스페이서를 활용해 간단히 해결. 모든 부품은 M3/M4 탭 가공 및 나사 결합만으로 조립 가능. 직접 손작업 탭 가공으로 제조비를 절감함.

실전 운용 경험

   수많은 극축정렬, 세팅, 소프트웨어(INDI, KStars, Ekos, PHD2) 세팅 시행착오를 거치며 실전 경험을 축적함. 첫 운용 시엔 크고 작은 문제로 촬영 실패도 잦았으나, 안정화 과정에서 1~2 아크초 정밀도까지 기록—600mm 렌즈로 30초 노출에 충분한 결과를 얻음. 사진 합성에는 Siril을 사용하며, 멀티나잇 합성 등 추가적인 목표도 추진 중임.

제작 비용 및 경제성

   총비용은 약 1,700유로(=공구, 하드웨어, 예비 연구용 부품 포함). 1대 단가로 환산 시 약 800유로 수준. 상용 GOTO 마운트(1,200~4,000유로) 대비 높은 경제성을 확인했으나, 직접 만드는 경험 자체에 더 큰 의미를 둠.

  세부 아이템별 단가(주요 항목 요약)

     * 하모닉 드라이브(2개): 144유로
     * MKS 및 서보모터(각 2개): 73~216유로
     * CNC 부품: 215유로
     * PCB, 커넥터, 나사, 공구 등 기타

결론 및 소감

   직접 만든 경험과 문제해결 과정, 설계-제작-검증 전체 사이클에서 얻은 성취감이 상용 제품 구매 이상의 의미를 준다는 점을 강조함. Version 1 PCB 실패를 통해 신중한 검증의 중요성을 배움. FreeCAD, KiCad 숙련도, 오픈소스 활용, 하드웨어 개발 전체 과정에서 다양한 교훈을 얻음. OnStepX 펌웨어 및 커뮤니티 자원 덕분에 DIY 망원경 마운트도 일반인이 가능한 프로젝트임을 입증함.

   별을 추적하는 자신만의 마운트를 직접 만들고 완전히 이해할 수 있는—그 성취감이 정말 값진 보람임.

        Hacker News 의견

     * USB-C 전원 공급장치에서 나오는 케이블이 인덕터 역할을 하게 됨, 즉 LC 필터 구조로 저역통과필터처럼 동작함, 그래서 온보드 커패시터가 필요함을 설명함, 모터가 순간적으로 큰 전류를 먹을 때 인덕터 특성상 즉시 전류가 흐르지 못하므로 커패시터가 대신 전류를 공급해 주고, 이후엔 인덕터가 슬슬 전류를 전달하게 되는 작동 원리임을 설명함
     * 정말 멋진 프로젝트와 설명임, 타이밍도 기가 막힘, 13살 때부터 아마추어 천문학에 빠져 여러 망원경을 소유했고 가족과 밤하늘을 관찰하며 오랜 시간을 보냄, 최근 10인치 SCT와 4인치 Newtonian을 꺼내 7살 아들에게 달과 토성을 보여줌, 부모님도 같이 볼 수 있어 대단히 뜻깊었음, 10인치 SCT는 GOTO 기능 없는 옛날 포크마운트에 올라가 있음, GOTO의 장점도 탐구해 봤지만, 직접 별찾기 하는 재미 때문에 아직 구매에 이르지 못함, 전용 냉각 카메라 ZWO 585MC는 샀음, 한편 별을 찾느라 엄청난 시간을 잃어버린 경험이 많음, Telrad만으로는 부족해서, 3D 프린터와 전자기기 지식으로 직접 써드파티 마운트를 만들 생각도 함, 모터를 NEMA 17 스테퍼로 바꿀까도 고민함, 그렇게 찾아보다 PiFinder라는 프로젝트를 알게 됨, 오토메이션과 수동 가이드의 완벽한 균형인 것 같음
       https://www.pifinder.io/, 3D 프린팅과 PCB 제조 기술 발전으로 곧 많은 문제를 해결할 수 있다는 확신을 느낌
          + GOTO가 재미없다는 의견을 읽으니 정말 다양한 입맛이 있다는 걸 다시 느끼게 됨, 나는 오히려 물체 찾는 과정이 제일 안 맞아서 goto 마운트가 고마움
          + ZWO 카메라를 Kstars/EKOS에 연결하면 소프트웨어로 플레이트 솔빙을 이용해 정확히 어디를 보고 있는지 파악하고, 거기에 맞춰 조정할 수 있음
     * 이 멋진 프로젝트에서 회로 트레이스에 대해 한 가지 언급을 하자면, 24V를 지원하기 위해 트레이스를 너무 넓게 잡았다고 하는데, 실제로는 전압이 높을수록 전류는 낮아져 오히려 좁게 잡아도 무방함, 트레이스 폭은 전류에 따라 결정되고, 트레이스 간 간격이 전압에 따라 신경 써야 할 부분임
          + 회로 트레이스 폭과 관련해서는 IPC-2221A 표준 명세서가 도움이 됨 https://www-eng.lbl.gov/~shuman/NEXT/….pdf
          + 좋은 지적과 설명에 동의함
     * 블로그에서 인용: ""타겟으로 망원경을 움직일 때 모터에 보내는 펄스 수가 많아지고, 작은 ESP32에는 과부하임"", 나도 스테퍼 모터를 빠른 속도로 정밀하게 제어하는 일이 있는데, 조금의 펄스 누락이나 글리치도 용납이 안 됨, MCU 코어로는 한계가 있어 타이머+DMA로 제어함, 최종적으로는 STM32G4 MCU의 ACT(Advanced Control Timer) 기능을 활용, DMA만으로 임의파형 생성이 용이해서 코어가 과부하되든 슬립 모드든 타이머는 영향을 안 받음, 요즘은 RP2350의 PIO도 고려함, ESP32에는 MCPWM이 있지만 복잡한 가속·감속 프로파일을 100% 코어 프리로 구현하려면 타이머를 계단식으로 쓰거나 인터럽트를 써야 해서, 그럼 다시 코어에 의존해야 되고 글리치 가능성 있음, ST사의 ACT는 모터 하나당 독립된 타이머라 구현이 단순하고 데이터시트만 잘 보면 됨, 전문 드라이버 IC(Trimanic
       등)도 방법이지만 소프트웨어 복잡도가 오히려 내 방식보다 높음
          + OnStepX는 내가 알기로 펄스 기반임, DMA 활용 방식은 아직 접해보지 못함, 양쪽 모두 CANBUS를 통한 위치명령 모드로 펄스 대신 구동하는 게 가능할 거라 봄, OnStepX 코드도 그 가능성을 봤지만, 펄스 모드가 너무 시작하기 쉬워 그냥 선택함
          + rp2040에서 스테퍼 제어 시 PIO도 생각해봤는데, 5비트 카운터와 32명령 한계 때문에 쓰기에 어려웠음, 대신 두 번째 코어를 모션 제어 전용으로 쓰고, 비트뱅 방식으로 step/dir 신호를 내서 단일 축 트라페조이드 프로파일에는 충분히 쓸만했음
          + MCU로 스테퍼 제어를 어디까지 가능한지 궁금하다면, Merlin 3D 프린터 펌웨어를 참고해보길 추천함, 소형 8비트 AVR MCU로도 델타 프린터용 복잡한 연산을 해냄
          + ESP32에도 RMT가 있는 것으로 아는데, 그 방법은 고려하지 않았는지 궁금함
     * freeCAD를 3년째 사용 중임, 이 프로젝트에서 만든 결과물을 보고 정말 감탄함, freeCAD를 좋아하지만 이만큼 집요하게 불편하고 짜증난 경험도 많지 않음
          + FreeCAD에 감사하지만, 프로젝트 복잡도가 올라갈수록 랜덤 크래시 때문에 정말 고생함, 그래도 주로 필요한 기능은 방법만 알면 FreeCAD에서도 구현이 가능함, 다른 CAD를 못 써 본 게 오히려 freeCAD에 정착하게 만든 계기임
          + 7년째 FreeCAD로 취미 모델링하지만, 처음 써보는 기능에선 여전히 사용자 경험이 답답함, FE 개발자라 QA 기준을 아는데 freeCAD는 그런 기준을 못 넘기는 UX가 많음, 오버컨스트레인 오류·경고 차이점을 알아보기 위해 포럼을 찾으니, 퍼포먼스라는 이름으로 의도적인 정책을 옹호하는 글만 많았고 파워유저의 게이트키핑 분위기가 진짜 문제임, 제대로 된 UX 전문가와 커뮤니티 매니저가 절실함
          + OnShape와 FreeCAD를 번갈아 쓰는데, OnShape의 다듬어진 완성도에 감탄함, 다만 구매한 모델 때문에 OnShape 무료 티어엔 올리지 못함, FreeCAD로 되긴 하지만 ‘한 시간에 끝나겠지’ 생각하면 결국 밤새게 됨
          + FreeCAD가 있다는 자체가 멋지고, 앞으로의 성숙을 기대함, 그래도 빠르게 결과를 얻고 싶으면 Autodesk Fusion 무료 티어를 추천함, 원칙적으로 OSS만 쓴다는 사람에게는 안 맞을 수도 있지만, 취미 이용에는 사실상 제약 없이 품질이 좋음, OSS가 아니라고 해서 배척당하는 것 같지만, 기능 중심으로 소프트웨어 선택의 폭이 넓어지면 좋겠음
          + ChatGPT와 Claude가 이런 작업에 큰 도움이 됨, 문서화가 괜찮은 소프트웨어라면 AI가 워크플로 단계별로 친절히 설명해줌, 완전히 전문적이거나 레퍼런스가 부족한 영역은 한계가 있지만, 눈 감고 이리저리 클릭하거나 유튜브를 빠르게 보는 것보다 훨씬 빠름, 기본 개념을 쌓는 스터디가이드도 만들어주길 요청할 수 있음, 단, AI의 환각이나 오류도 있으니 꼭 링크/참고자료 요청할 것
     * 망원경 마운트로 직접 정밀 측정, 예를 들어 자력으로 행성 천문 측정(astrometry)을 해보는 프로젝트에 관심이 큼, 직접 측정만으로 행성 궤도를 풀어보는 것은 Kepler 등의 옛 천문학자들이 했던 길을 다시 밟아보는 것과 같다고 느낌
          + 정말 재미있는 주제임, 예쁜 사진 취미를 넘어서 아마추어 관측데이터를 연구에 쓸 수 있는 시스템을 고안해봄, 단일 프레임 원본과 메타데이터(시간, 좌표, 캘리브레이션프레임 등)를 공유하면 집단적으로 과학 연구나 더 예쁜 사진을 만들 수도 있음, Vera C. Rubin Observatory(미국 천문대)는 며칠만에 남반구 전체를 촬영할 수 있음, 이런 소프트웨어가 글로벌 크라우드소싱 천문 어레이로 발전하면 정말 멋질 것임
          + 사실 행성 위치를 정확히 알기 위해선 망원경 마운트의 위치 보정 보다, 행성을 중심에 두고 찍은 사진을 주변 별들의 고정 좌표와 비교해 위치를 산출하는 것이 훨씬 정확함, 마운트 자체를 완벽하게 교정하는 건 거의 불가능에 가까움, 별 상대좌표 방식이 엄청나게 정확함
          + 다른 재미있는 프로젝트로, 외계행성의 트랜짓 광도곡선 측정도 있음, 가까운 외계행성들은 사진용 렌즈만 있어도 뒷마당에서 관측 가능함, 예시로 ASI178MM-c와 Canon FD 300mm로 한 아마추어의 관측 사례 공유 https://astropolis.pl/topic/…
          + 비록 완전한 퍼스트 프린시플은 아니지만, Seestar S50 roboscope와 분산격자를 써서 별의 방출 스펙트럼을 측정하는 사람도 있음
          + 참고로 Kepler는 망원경 없이 Tycho의 맨눈 관측 데이터로 연구함
     * 정말 멋진 프로젝트임, PCB를 설계할 때 제대로 된 커패시터, 저항 등을 넣지 않은 것 같고, 마이크로컨트롤러 안정성이 떨어짐, 다들 어떻게 필요한 부품(디커플링 커패시터 등)을 결정하는지 궁금함, 데이터시트 읽어서 그대로 다 따라 넣으면 되는지 궁금함
          + 데이터시트 확인이 필수임, 대부분 필수 회로 예제가 들어있어 반드시 참고함, 그 외에 좋은 습관은 칩 핀 근처에 디커플링 커패시터 넣기, 그라운드 안정화, 레퍼런스 레이아웃을 따라가는 것임, 숙련된 엔지니어의 결정 과정 설명(예: https://www.youtube.com/watch?v=aVUqaB0IMh4)도 큰 도움이 됨
          + 거의 모든 부품 데이터시트에 미니멀 회로와 참고 설계가 있음, 그라운드 플레인 사용, 신호선 길이 등은 따로 애플리케이션 노트로 설명하는 경우가 많음
          + 마이크로컨트롤러 회로 설계시, 회로 공개된 개발보드를 참고하면 출발점 잡기가 쉬움
          + 데이터시트 확인과 함께, 모든 전원 핀에 디커플링 커패시터를 최대한 가까이 넣고 PCB 한 면을 그라운드 플레인으로 두면 대부분 문제를 해결할 수 있음
     * CNC 금속 부품을 주문해서 제작한 점이 특히 인상적임, 나도 초보 CAD 설계자인데 배우고 싶음
          + 로컬 메이커스페이스에 등록해서 머신샵을 이용함, CNC 머신을 위한 교육과 인증을 받고, 영화 촬영 장비용 단순 금속 판을 CAD로 설계해봄, 간단한 G코드 생성기가 있었고 시뮬레이터로 충돌도 체크했음, 실제로 제작에 들어갈 때마다 누군가 헤드를 망가뜨려 장비가 꺼져있었고, 결국 수작업으로 만들어서 일부 치수가 완벽하게 맞진 않음, CNC 서비스가 있다는 걸 알았으면 CNC로 직접 맡길걸 후회함
          + 실은 굉장히 간단함, step 파일만 업로드하면 바로 견적과 제작 가능 여부 확인 가능함 https://jlccnc.com/cnc-machining-quote
     * 정말 멋진 프로젝트임, PCB 제작 비용이 어셈블리 때문인지 궁금함, 보드는 JLCPCB 기준 2층, 100mm 이하, HASL 마감 등 기본 옵션이면 저렴한데, USB 커넥터 플레이트 슬롯이 추가비용이 발생하는지, 한 번에 몇 장 조립 주문했는지, 표준/확장 부품 라이브러리 선택 비중은 어땠는지, 커넥터만 따로 납땜하면 얼마나 달라졌을지, 확장 라이브러리 부품이 하나라도 있으면 모든 부품이 개별로 피킹(조립) 비용이 올라가는 구조라 부품 종류를 최소화하는 게 비용절감의 핵심인지 궁금함
          + 실제로 비용의 주된 원인은 어셈블리임, 보드는 JLCPCB 2층에 기본 옵션이고 색상만 검은색으로 변경해서 소폭 상승함, 다섯 개씩 주문했고 이게 최소 수량임, 표준 라이브러리에 최대한 맞추려 했지만 수동소자 외에는 한계 있음, 스루홀은 내가 직접 납땜하면 비용차이를 모르겠지만 신경쓰지 않을 정도임, 부품 종류를 최소화하는 게 맞음, 예를 들어 4k와 6k 저항이 필요하면 2k짜리 다섯 개를 조합하는 게 유리함
     * 정말 인상적인 프로젝트임, 나도 내 스코프에 대형 하모닉드라이브 마운트를 사려 했는데 가격 장벽이 너무 높음, EKOS/Kstars/INDI 툴을 쓰면서도 시행착오가 많았음, 파이썬에서 indi 디바이스를 제어하려면 내가 만든 코드가 있음 https://github.com/dahlend/contindi
          + 어떤 동기로 이 코드를 썼는지 궁금함, EKOS 자체로도 스케줄링 기능이 좋아 보여서임, 나도 MeLE 4C 미니컴퓨터에 INDI 서버 돌리면서 터미널 인터페이스(TUI)를 짜봤음 https://svendewaerhert.com/content/blog/…, GNOME 원격데스크톱의 안정성 문제로 헤드리스/원격 INDI 서버로 전환함, 조만간 TUI 코드도 깔끔히 정리해 Github에 올릴 예정임
"
"https://news.hada.io/topic?id=22720","언플 Driven Development","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         언플 Driven Development

   설명이 좀 더 있으면 좋겠습니다. 링크 들어가도 글이 많이 있지는 않아서 공유하시고자 한 의도를 잘 모르겠습니다.
"
"https://news.hada.io/topic?id=22649","2025년 AWS: 당신이 알고 있다고 생각하지만 지금은 틀린 것들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 2025년 AWS: 당신이 알고 있다고 생각하지만 지금은 틀린 것들

     * AWS의 핵심 서비스들이 빠르게 진화하고 있음
     * EC2, S3, Lambda 등 주요 기능들이 과거와 달리 사용자의 기대를 뛰어넘는 성능과 유연성을 제공함
     * 네트워킹, 인증, 비용 절감 방안 등에서도 많은 변화와 최적화가 이루어졌음
     * 잘못된 오래된 블로그 포스트나 정보 때문에 혼동이 생길 수 있음
     * 최신 업데이트와 변화된 정책을 숙지하는 것이 AWS 활용에 필수적임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AWS 2025: 과거의 인식과 달라진 현재

     * AWS는 20년 가까운 역사를 가진 클라우드 플랫폼으로, 그만큼 서비스의 ‘상식’이 계속 변화함
     * 기존 사용자들도 변화의 속도를 따라가기 어려울 정도로 많은 핵심 기능이 개선되어 있음
     * 여전히 오래된 정보를 제공하는 블로그 포스트도 많아, 실제 구성이 바뀐 점을 제대로 알아두는 것이 중요함

EC2

     * EC2 인스턴스의 시큐리티 그룹과 IAM 역할을 이제는 중단 없이 변경할 수 있음
     * 실행 중인 인스턴스에서 EBS 볼륨의 크기 변경, 연결/해제 작업이 가능함
     * 최근 EC2 인스턴스를 강제 중지 또는 종료할 수 있어, 긴 타임아웃을 기다릴 필요 없음
     * 물리 호스트 간 라이브 마이그레이션 기능이 도입되어, 인스턴스 성능 저하 경고가 드물어짐
     * 인스턴스의 신뢰성이 대폭 올라가, 예전처럼 인스턴스가 예고 없이 사라지는 일은 거의 없음
     * Spot 인스턴스의 가격 변동이 점진적으로 바뀌어, 투기장처럼 실시간으로 감시할 필요가 줄어듦
     * 전용 인스턴스가 필요한 케이스가 극히 드물어짐(HIPAA BAA도 거의 10년 전부터 필요하지 않음)
     * AMI Block Public Access가 신규 계정에서 기본값으로 활성화됨(2023년 기준 90일 이상 퍼블릭 AMI 미소유 계정에도 적용됨)

S3

     * S3는 더 이상 Eventually Consistent가 아니고, Read-After-Write Consistency를 제공함
     * 객체 키의 첫 부분을 임의화할 필요가 없어져 데이터 분산 및 핫스팟 문제 걱정이 줄어듦
     * ACLs(Access Control List) 가 더 이상 권장되지 않으며, 신규 버킷에서는 기본적으로 꺼져 있음
     * 신규 버킷은 Block Public Access가 기본값으로 설정됨
     * 자동으로 저장자료 암호화가 적용됨
     * Glacier가 S3의 스토리지 클래스가 되기 전, 별도의 서비스였으나 현재는 결합되어 있음. 청구 내역 등에서 그 흔적만 남아 있음
     * Glacier 복원 비용과 속도가 과거와 비교해 상당히 예측 가능해지고 저렴해졌음. 이전 공포스런 복원 비용은 사실이 아님

네트워킹(Networking)

     * EC2-Classic은 완전히 사라짐
     * 공인 IPv4 주소가 이제는 무료가 아니며, Elastic IP와 동일한 비용이 부과됨
     * VPC Peering 대신 Transit Gateway, VPC/리소스 공유, Cloud WAN 등 더 좋은 옵션이 등장함
     * VPC Lattice와 Tailscale로 복잡한 네트워킹 문제를 쉽게 해결 가능함
     * CloudFront 업데이트 반영 시간이 45분에서 5분 내외로 단축됨(여전히 CloudFormation 배포 대기시에는 길게 느껴질 수 있음)
     * ELB Classic은 교차 AZ 데이터 전송 요금이 부과됐으나, ALB는 LCU 요금만 부과함. NLB는 여전히 교차 AZ 요금이 부과됨에 주의
     * Network Load Balancer에 시큐리티 그룹 지원이 추가됨
     * 가용 영역(Availability Zone) ID가 계정마다 달랐지만, 이제 Resource Access Manager로 Zone ID 정렬이 가능함

Lambda

     * Lambda는 5분 제한에서 15분으로 실행 시간이 늘고, 컨테이너 이미지(Docker) 및 EFS 공유 스토리지, 최대 10GB RAM, /tmp 10GB 지원이 추가됨
     * VPC 내 Lambda 호출 속도가 대폭 개선됨
     * 콜드 스타트 이슈가 과거보다 크게 완화됨

EFS

     * EFS IO 성능 조정이 이제 용량과 별도로 조절 가능해, 무의미한 데이터로 공간을 채울 필요 없음

EBS

     * 신규 EBS 볼륨은 기초 데이터가 없으면 즉시 최대 성능 사용 가능함
     * 스냅샷에서 생성된 볼륨은 첫 데이터 읽기 시 느릴 수 있으니, 전체 디스크를 한번 읽는 것이 권장됨(더 빠른 옵션도 제공됨)
     * io1 볼륨은 여러 EC2 인스턴스에 동시 연결 가능하지만, 실제로는 매우 특수한 상황에만 권장됨

DynamoDB

     * 항목 안에 빈 필드를 허용하게 되었음
     * 성능이 훨씬 일정해져, 예전처럼 핫키(Hot key) 문제 모니터링을 별도 도구로 해야 할 필요가 적어짐
     * Pricing 변화로, 대부분의 사용자는 On Demand 타입이 더 합리적임

비용 절감 옵션(Cost Savings Vehicles)

     * Reserved Instances는 점차 단종 중이며, Savings Plans가 앞으로의 표준임. Savings Plan의 할인율이 RI 대비 하락했으나, 유연성이 더 높아졌음
     * EC2 사용 요금이 초 단위로 과금되므로 매우 짧게 인스턴스를 켜도 비용 효율적임
     * Cost Anomaly Detector는 예기치 않은 사용 패턴을 뛰어난 정확도로 감지하며 무료임
     * Compute Optimizer는 EBS 등 다양한 리소스 추천을 제공하며 신뢰도가 높음. Trusted Advisor의 추천은 아직도 일관성이 부족함

인증(Authentication)

     * IAM 역할을 통한 권한 부여가 권장되며, IAM 사용자는 레거시 앱에만 적합함
     * IAM Identity Center가 AWS SSO를 대체하며, 계정 접근에 사용됨. 이로 인해 혼란이 일부 존재함
     * 루트 계정에 다중 MFA 장치를 등록할 수 있음
     * 조직 구성원 계정에 별도로 루트 자격 증명을 구성할 필요 없음

기타(Miscellaneous)

     * us-east-1의 신뢰성과 내구성이 전보다 크게 향상됨. 예전처럼 자주 발생하던 장애가 이젠 뉴스가 될 만한 사건임
     * AWS 서비스의 폐기(Deprecation) 는 여전히 드물지만 증가 추세이니, 마이너 서비스에는 의존도 고려 필요함
     * CloudWatch 데이터의 마지막 포인트가 불일치로 비정상적으로 낮게 나타나는 현상이 더는 발생하지 않음
     * 루트 계정에서 조직 내 멤버 계정의 AWS 계정도 직접 종료(닫기) 가능함

   와 많이 변했네요 정말

   AWS는 이제 단일 서비스를 골라 쓸 수 없음.
   뭐 하나 하려면 이것저것 다 물려서 써야됨.
   결고 단순하지 않음.
   벤쳐에서 쓰려면 클라우드 비용뿐만 아니라 devops 인건비도 써야됨.
   제대로 구축하려면 개발 시간 다 뺏길 정도로 배보다 배꼽으로 업무량이 급증함.
   게다가 managed 서비스를 써야 더 좋은 경우가 늘어서 코드 레벨에서 이미 플랫폼 의존적이 됨.

        Hacker News 의견

     * S3의 ""Block Public Access""가 이제 새 버킷에 기본 적용됨을 보며, 당연히 옳은 결정이라 생각함, 그동안 잘못 구성된 S3 버킷 때문에 거대한 데이터 유출이 많이 일어났음, 하지만 가끔씩 나는 파일을 공개적으로 서빙하려고 퍼블릭 읽기 권한이 있는 S3 버킷을 만들고 싶은데, 매번 뭔가 바뀌어서 예전 레시피가 안 먹히고 처음부터 다시 공부해야 하는 상황이 반복됨
          + ""Block Public Access"" 설정을 주의 깊게 보라고 말하고 싶음, 이건 사람들이 큰 실수를 안 하도록 해주는 일종의 안전장치임, 만약 매우 허술한 bucket policy나 ACL(구식이지만 여전히 사용됨)을 설정해도 Block Public Access가 켜져 있으면 퍼블릭 엑세스가 불가능함, 반대로 Block Public Access를 꺼두고, 아주 잘 설계한 버킷 정책을 써도 괜찮음, 버킷 정책이 누가 접근 가능한지 전적으로 결정함, 물론 장기적으로 정책이 우연히 바뀌거나, 예기치 않게 IAM 역할이 추가되거나, 신뢰 정책이 변경될 수 있으니 그에 대한 관리가 중요함
          + 나는 이런 상황에서 LLM을 자주 사용함, 문서 읽어주고 AWS 공식 문서 여기저기에 박혀 있는 데모 코드를 뽑아달라고 요청함, 얻은 후에 원하는 커스텀 수정도 바로 물어봄
          + 이거 예전에도 하지 않았나 하는 기시감 느낌, 10년 전에도 다들 버킷을 오픈으로 놔둬서 이런 조치 하지 않았나 싶음
          + 이런 변화 때문에 면접에서 ""이 기술 익숙하세요?""라는 질문이 너무 애매해짐, 기술이 매달 바뀌니 언제 기준으로 아는지 묻고 싶음
          + 공식적으로 배우려면 $250 내고 인증 시험까지 치르게 해줌
     * EC2에서 인스턴스 종료 없이 보안 그룹이나 IAM 역할 교체 가능한 건 몇 년 전부터 가능하던 일이었음, 스팟 인스턴스는 과거엔 입찰 시장이었는데 지금은 입찰 자체가 없어져서 오히려 급격한 가격 변동이나 특정 유저만 접근 가능한 일이 없어져서 훨씬 좋음, 그리고 예전엔 객체 키 앞부분 무작위로 만들어 핫스팟을 피해야 한다고 했지만, 지금은 그럴 필요가 없어졌음, 동료들에게 설득하는데 한참 걸렸지만 이 사람들은 어차피 S3 백엔드 병목과 관련 없는 초미니 파일들만 저장하고 있었음, Lambda는 예전엔 5분 제한에 컨테이너 이미지도 지원하지 않았는데, 지금은 15분 런타임, Docker 이미지, EFS 공유, 최대 10GB RAM, /tmp 저장공간 10GB까지 지원함, 나도 한 가지 깨달았던 게 Python 글로벌 스코프도 /tmp처럼 살아남는다는 점임
     * Glacier 복원이 이제 고통스럽게 느릴 필요가 없어졌음, Amazon 스타일로 추측해봤을 때 예전 Glacier는 어디선가 실제 Amazon 물류창고에서 돌아갔을 거란 생각이 있었음, 데이터 요청하면 작업자가 선반에서 이동식 미디어를 찾아서 서버에 꽂는 느낌이었음, 옛날 시분할 컴퓨터의 테이프 백업 방식과 비슷했음, 당시엔 테이프를 직접 물리적으로 교체해야 했음
          + 실제로는 테이프 로봇 같은 자동화 장비를 썼을 확률이 높다고 봄, 관련 사진 예시, 사람이 아닌 로봇이 테이프 찾아서 드라이브에 넣고, 오프셋으로 감고 읽은 뒤 다시 되감아서 반납하는 방식임, 대기 시간이 생기는 건 테이프 찾기, 되감기, 그리고 드라이브 대기 때문임, 아마 효율 위해 한 번 넣은 테이프 내 요청 다 처리하고 꺼내는 식이었을 것임
          + 내부 사정에 대해선 공개할 순 없지만 Glacier 설계를 정확히 맞춘 사람을 본 적은 없음, 나도 예전 AWS에 있었는데 어차피 Glacier도 다른 AWS 서비스와 동일한 데이터센터에서 운영됐다고 얘기할 수 있음, 엔지니어링이나 제품관리에서 중요한 건 고객의 기대치를 잘 관리하는 거임, 만약 업로드 제한이 10개라고 해놓고 12개 올리게 두면 고객이 계속 12개로 기대하게 됨, 기대 관리가 중요함
          + 하드디스크가 균일해서 웨어하우스는 자동로봇으로 돌린다고 생각함, 사람을 쓰는 건 다양한 크기, 형태 같은 비표준 처리 때문임
          + 어쨌든 이런 장비들은 수십년 전부터 로봇화 되어왔음
     * 나는 AWS 계정에 더이상 로그인할 수 없음, MFA를 미리 안 해뒀기 때문임, 장치를 발급받으려고 해도 로그인부터 필요함, 미리 경고를 듣긴 했지만, 로그인 없이 MFA 장치 신청 못하는 구조는 꽤 답답함
          + 지원팀에 문의해보는 게 좋을 듯함
            AWS Support 문의
          + 지원팀에서 쉽게 풀어줄만한 문제로 보임
     * 나 같은 사람 많을거라 생각함, 원래 AWS 방식대로—API Gateway, serverless Lambda에 IAM 역할까지 손대서 맞추는 복잡함—이제 놔두고, 그냥 EC2나 LightSail VPS, S3 버킷, Route53로 도메인 연결만 하고 나머진 신경 안 쓰고 싶어짐
          + 스토리지+VPS만 쓸 거면 AWS보다 전통 VPS가 훨씬 저렴함, 나는 오히려 AWS를 쓰는 이상 제대로, 많이 써야 한다는 철학임, 위임 가능한 건 모두 Amazon에 넘겨서 효율과 비용 절감하는 식임, Step Functions, Lambda, DynamoDB도 대체재를 능가해왔음, 다만 개발자들이 벤더 활용 최적화를 생각보다 잘하지 않는 게 아쉽다고 느낌
          + 실제로 클라우드를 간소화한 업계도 많은데, 그 이유는 서비스 지역 제한이나 예측 가능한 청구 때문임
          + IAM 관리가 너무 시간소모라서, 예전엔 시스템 관리에 썼던 시간이 이제 퍼미션 관리에 다 들어가는 느낌임, IAM이 너무 어려워서 실상은 순손해임, VPS와 과도한 서버리스 최소권한 관리의 중간쯤에 스위트스팟이 있을 것 같음, Fargate가 그 후보인데 더 옮겨보면서 실험해보려 함
     * AWS는 다른 분야 따라가겠다며 AI 쪽에서 산만하게 이것저것 내놓기보다, 기존에 잘하던 ""기초적이지만 중요한 서비스""에 더 집중했으면 좋겠음, AWS 리더십이 GenAI 쪽에선 방향성을 놓친 느낌이지만 기본 인프라는 잘 만듦, AI 때문에 패닉 상태로 보이고 고객 입장에선 너무 산만해져서 불편함
          + 지금 리더십의 방향은 인프라를 모두가 간단히 모델 고르고 바로 쓸 수 있게 해주는 쪽임, 셋업 복잡함 없이 바로 가능한 환경 추구임
     * S3 버킷이 VPC와 같은 리전에 있더라도 퍼블릭 인터넷 경유하면 NAT Gateway 비용이 청구되는 게 정말 말이 안 됨, 기본값은 opt-out이 돼야 마땅한데, opt-in이 기본인 건 아마 AWS가 이 구조로 추가 수익을 거두기 때문이라 봄, 현재 경로를 바라는 사용자는 극히 드물 것임
          + 이건 보안이 기본값이란 점을 고려한 설계임, NAT Gateway, VPC Gateway Endpoint(S3), 기타 인터넷 출구를 모두 설정하지 않으면 워크로드는 S3에 접근 못함, 네트워킹은 닫혀있는 게 맞고, 만약 사용자가 경로를 잘 이해하지 못하고 뭘 만든다면 그 책임은 고객에게 있음, AWS는 Infrastructure as a Service(IaaS) 원시적인 도구만 제공한다고 생각해야 함
          + S3 VPC Gateway Endpoint가 바로 이 목적임, 요금도 무료임
            AWS 공식 문서
          + VPC, 서브넷, PrivateLink 엔드포인트 등 모두 셋업해보고 나면 정말 황당하게 느껴짐, PrivateLink란 이름 짓는 데도 공 들였고(기술적으로도 의미는 있지만), 이런 게 애초에 셋업 없이 기본 제공이어야 한다고 생각함, 심지어 프라이빗 서브넷 사용하는 경우 S3 등의 접근은 PrivateLink가 유일한 방법 아닌지, 이상하게 느껴짐
          + VPC 엔드포인트는 전부 무료로 기본 적용돼야 한다고 생각함, 자기 서비스 API 쓰는데도 추가 과금 구조는 좀 심함
          + 이건 가격 차등화 목적임, 가격에 민감하지 않은 고객은 굳이 신경 안 씀, 신경 쓰는 쪽은 비용 줄이려고 노력하게 되고, 그런 상황에서도 종종 어쩔 수 없이 AWS를 써야 하는 경우가 많음
     * 이번 기사 덕분에 안심했음, Amazon이 뭔가 크게 바꿔서 마이그레이션 강요하면 어떡하나 늘 걱정했는데, 2013년부터 EC2 인스턴스를 거의 관리할 필요 없이 잘 쓰고 있었음, 예상치 않은 변화가 없어서 다행임
     * 과거 Availability Zones가 계정별로 랜덤 매핑이었던 게 충격적임
          + 이건 부하 분산 때문이었음, 여러 계정에서 1b처럼 특정 AZ를 고정으로 쓸 경우 실제 물리적인 분산이 골고루 되도록 했던 것임, 나중엔 AZ별 canonical 이름이 제공됐는데, 실제로 핫스팟 만들던 계정들과 메타데이터가 필요한 계정이 달랐기 때문일 거라 생각함
          + 기본 설정에서 모두 us-east-1a만 골라버리면 특정 AZ에 쏠리는 걸 막으려던 목적이라고 봄
          + 처음엔 부하 분산엔 좋았는데, 여러 계정 간 네트워크(PrivateLink 등) 연결할 때, 어떤 AZ가 어디랑 매칭되는지 일일이 확인해야 해서 혼란스러웠음, 그래서 계정마다 일대일 맵핑 방법론 만들고 내부 조회 테이블까지 구축했는데, 나중엔 AWS가 zone ID를 메타데이터에 추가해서 쉽게 조회 가능하게 해줬음
          + 이 정책 때문에 정말 고생한 적 많음
     * 바로잡고 싶은 점이 있음, 의미 없는 잡지식이 다 뒤집힐 수 있음
       Weird Al: Everything You Know is Wrong
       Firesign Theatre: Everything You Know is Wrong
"
"https://news.hada.io/topic?id=22743","x86는 왜 Apple M 시리즈를 따라잡지 못했는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     x86는 왜 Apple M 시리즈를 따라잡지 못했는가?

     * Apple M 시리즈의 뛰어난 전력 효율과 성능이 기존 x86 칩 대비 큰 관심을 받음
     * 많은 개발자들이 x86 아키텍처가 왜 같은 수준의 혁신을 보여주지 못하는지 궁금함
     * x86 설계는 오랜 기간 동안의 호환성과 복잡성 때문에 변화가 제한적임
     * Apple은 SoC 통합과 특화된 하드웨어 가속기를 통해 성능의 큰 향상을 이루었음
     * x86 칩 제조사들도 효율성 개선을 시도 중이나, 구조적 한계로 인해 Apple M 시리즈와 동일한 진보를 빠르게 이루기 어려움
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경 설명

   Apple의 M 시리즈 칩은 2020년 이후 맥북 및 데스크탑 제품군에 탑재되어, 기존 x86 아키텍처 CPU 대비 뛰어난 성능과 전력 효율로 많은 주목을 받음. 이러한 변화 이후 개발자, IT 업계 종사자들은 x86 계열의 Intel과 AMD 칩이 왜 여전히 Apple의 M 시리즈에 못 미치는지에 많은 궁금증을 가짐.

x86 아키텍처의 한계

     * x86 프로세서는 과거의 호환성을 유지하는 동시에 기능 확장을 해왔음
     * 수십 년간의 구조적 복잡성으로 인해 설계 혁신의 속도가 제한적임
     * 다양한 하드웨어 및 소프트웨어 생태계와의 호환성은 중요한 장점이나, 근본적인 성능 개선에는 제약 요인으로 작용함

Apple M 시리즈의 접근 방식

     * Apple은 ARM 아키텍처 기반으로 완전히 새로운 SoC(System-on-a-Chip) 설계를 진행함
     * 자체 설계한 하드웨어 가속기(예: 머신러닝, 미디어 인코딩/디코딩 등 전용 유닛)로 특정 작업 시 특별한 효율성과 성능 제공함
     * 통합형 메모리 아키텍처(UMA) 를 포함해 CPU·GPU·RAM을 단일 칩에 집적함으로써 대역폭과 전력 효율면에서 큰 강점 보유함

향후 전망

     * Intel, AMD 등 x86 제조사들도 전력 효율 및 성능 개선에 투자 중임
     * 애플리케이션 호환성, 시장 점유율, 생태계 지원과 같은 요소로 인해 x86의 변화 속도는 상대적으로 느림
     * Apple M 시리즈 아키텍처가 노트북 및 데스크탑 시장에 새로운 표준을 제시함에 따라, 앞으로의 컴퓨팅 아키텍처 발전 방향에 대한 관심이 높아지고 있음

        Hacker News 의견

     * 수많은 훌륭한 댓글들이 각 회사에 대해 이야기하고 있음 – 예를 들어 Apple은 모바일을 중심으로 수직적이고 닫힌 생태계를 구축했고, Microsoft는 데스크탑 중심으로 수평적이고 개방적으로 접근했음. 수천 명이 수십 년간 쏟아부은 수많은 ‘작은 최적화’들이 있었음. Intel은 항상 ‘더 많은 것이 더 낫다’는 방식을 따랐고, ARM은 ‘덜 쓸수록 이득’이라는 철학이었음. Intel이 아주 오래 우세했으며, 솔직히 x86이 아닌 아키텍처가 단일 코어 정수 성능에서 경쟁하는 모습은 상상도 못했음. 더 오래 전에는 1MHz짜리 6502 칩이 거의 8비트임에도 4MHz Z80과 거의 맞먹는 성능을 보여주기도 했는데, 항상 ""어떻게 이게 가능한 건가?""라는 의문이 남았음
     * 배터리 효율성은 기술 스택 전반에 걸친 수많은 작은 최적화에서 나오며, 결국은 CPU 사용을 최소화하는 것이 핵심임. 그래서 ISA나 공정도는 배터리 수명에 큰 영향이 없음. CPU를 완전히 활용하여 고정된 작업을 수행할 때 AI340, M1 모두 비슷한 에너지 효율을 보여줌. 이런 상황이 배터리 수명에 영향을 주는 건 Blender 렌더나 대형 컴파일, 게이밍 같은 고부하 작업일 때임. 배터리 게임 벤치마크를 예로 들면, M1 Air는 여기 영상에서 2.5시간 버팀. x86 노트북과 비슷하거나 오히려 더 짧은 수치임. 설정을 낮추어 CPU와 GPU를 대기 상태로 두면 바로 10시간 이상으로 늘어남. 5년 정도 된 Qualcomm 칩은 훨씬 느리고 효율도 떨어지지만, 발열도 적고 전력 소모도 미미함. 즉, M1이 빠른 것은 맞지만, 배터리 효율이 압도적인 이유는 철저히 CPU 외적인 설계 요소에 있음. AMD와
       Intel이 놓치고 있는 부분임. 참고로, 크롬에서 탭을 많이 열면 노트북 하단이 뜨거워지고, 유튜브 영상을 틀면 팬이 도는 경우가 많음. 이는 리눅스에서 하드웨어 가속(특히 비디오 디코딩) 설정이 안 돼 있는 경우가 흔하며, fw16에서 GPU 비디오 디코딩을 직접 활성화한 뒤에는 유튜브에서 팬 소리를 듣지 못했음
          + Apple의 낮은 전력 소모의 큰 이유는 iPhone임. Apple은 수년간 아이폰용 칩의 효율과 성능을 차근차근 개선해왔음. Intel이나 AMD는 데스크탑 위주였기에 전력 효율에 집중하지 않았음. Apple의 칩이 충분히 좋아짐에 따라 노트북으로 확장할 수 있었던 반면, x86은 더이상 경쟁할 수 없는 수준이었음. 그리고 아이폰은 역사상 가장 수익성이 좋은 제품이기에 Apple은 막대한 R&D와 인재 영입에 투자할 수 있었고, 그 결과 최고의 반도체 엔지니어 팀을 세울 수 있었음
          + Apple은 하드웨어와 OS, 그리고 자사 앱까지 수직적으로 통합되어 디바이스 전체를 최적화할 수 있음. 반면 Wintel 진영은 서로 책임을 미루기 바빠 어떤 문제가 생기면 명확한 해결이 쉽지 않음 – 예를 들어 노트북 슬립 모드 오작동과 같이 제조사, Microsoft, 하드웨어 벤더가 서로 책임을 전가하는 일이 빈번함
          + 비디오 전송 업무에 오래 종사해 봤기에, 하드웨어 가속이 가능한 환경에서 소프트웨어 디코딩을 강제하는 의사결정자가 있다면, 이런 CPU 위에 맨살로 앉아 보라고 법으로 정하고 싶음. 사용자에게 몰아내는 그 피해가 실제로 크다고 생각함
          + Apple Silicon이 배터리 효율이 뛰어난 이유가 CPU 외적 요소만큼 CPU의 부하 효율성에도 있음. CPU가 빠르게 작업을 끝내고 슬립 상태로 돌입(race to sleep)할 수 있을수록 전력 효율이 높아짐. Apple Silicon은 부하 시 AMD와 Intel보다 2~4배 더 효율적이며, 최고속도도 더 높음. Apple 노트북이 효율적으로 느껴지는 또 다른 이유는 진정한 big.Little 구조를 채택한 덕임. 반면 AMD와 Intel의 작은 코어들은 면적 효율에만 집중되어 실제 실사용에선 효용성이 낮음. 인텔은 벤치마크용으로 작은 코어 수를 늘리지만, 실제 애플리케이션은 오히려 빠른 코어 여러 개를 선호함
          + Android 커널을 보면 차이가 명확함. 기본 Linux 커널과 비교하면 전체적으로 전력 관리를 위해 많은 서브시스템, 스케줄러까지 아주 세밀하게 튜닝됨
     * AMD는 이제 M4 Pro와 거의 대등한 성능과 전력 효율을 가진 Max 395+ 칩을 보유하고 있음(Framework Desktop 등에서 사용됨). 아직 Apple을 완전히 능가하진 못했지만, AMD는 이제 충분히 경쟁력 있는 옵션을 만듦
          + M4 Pro는 M3 Pro에 비해 전력 대비 성능 면에서 오히려 퇴보함. 아직 M4 Max의 다이샷이 공개되지 않아 많은 추정이 있는데, 아마 수율 문제로 M4 Pro를 사실상 M4 Max의 하위 등급으로 만든 결과 누설 전류 등 여러 문제와 트레이드오프가 생긴 것으로 보임. Hardware Canucks에서 395 칩의 모바일 버전 리뷰(Asus ROG Flow F13)도 참고할 만함. 395가 TDP 70W에서 동작하면 성능/전력 비율이 최적화됨. Cinebench R24 기준으로 M4 Pro가 더 높은 점수를 기록하며, 전력 소모도 약 30% 더 적음. 단일코어 벤치마크에서도 M4 Pro가 35% 우위임. GPU 성능은 프로덕티비티 앱에 따라 엇비슷하나, 게임은 x86+AMD 조합이 일반적으로 더 좋음. 배터리 수명은 웹 브라우징 기준 M4 Pro가 50% 더 좋고, 단순 동영상 재생 기준으로는 2배 이상 더 뛰어남. 풀로드 시는 395가 약간 앞서지만 실제로는 TDP를 크게
            낮춰서 그런 결과임
          + 새 노트북에 AMD Ryzen 9 365를 탑재했는데, 배터리 지속시간과 성능 모두 만족스러움. M3(일반)와 비슷한 느낌임
          + 해당 칩 구매를 고민했으나, 지금은 Framework Desktop이나 아주 비싼 태블릿 등 극소수 제품에만 들어가 있어서 실질적으로 선택이 어려운 상황임
          + Framework 16 모델에서 AMD Ryzen AI 9 HX 370, AI 7 350이 오늘부로 라인업에 들어감 상세 링크
          + 14"" HP Zbook Ultra G1A(우분투 인증), Asus Z13 등에서도 해당 칩을 사용할 수 있음. Asus Z13은 리눅스 호환성이 확실치 않음
     * 2020년형 MacBook Air M1 (16GB RAM, 512GB SSD)를 3년간 쓴 후, MacBook Pro M3 Pro (36GB RAM, 2TB)로 업그레이드해서 메인 머신으로 쓰고 있음(2대의 모니터를 TB4 독으로 연결). IT 분야에서 일하고 있고, 회사의 모든 신규 기기를 검수함. 가장 큰 특징은 실제로 M1 Air와 경쟁 가능한 비즈니스 노트북(ARM, AMD, Intel 불문)이 전혀 없었다는 점임. M3 Pro는 비교가 안 됨. 엄청난 가격과 호환성 문제도 있지만, 동료들은 MacBook에 Windows나 Linux를 설치한 후 Parallels로 VM을 돌려 씀. 웃긴 것은 Windows 11이나 Linux를 VM에서 돌리는 것이 Lenovo, HP, Dell 등의 비즈니스 노트북에서 기본으로 돌리는 것보다 더 빠르고, 조용하며 배터리도 오래 간다는 것임. 케이스마다 다르겠지만, IMHO 지금은 Mac이 진리임. Linux나 Windows를 써야 해도 말임
          + MacBook Air M1 8GB로 개인 업무를 처리하고 있는데, Docker Desktop과 VS Code 모두 32GB RAM의 Windows T14보다 더 잘 돌아감(Windows의 각종 엔터프라이즈 제약 탓이 큼). Linux나 덜 제한적인 Windows면 더 나을 수도 있음. Nvidia Now로 게이밍도 가능한데, 진지한 게임용으로는 추천하진 않음
          + ""요즘 Mac 말고 대안이 없다""는 말은 노트북만 쓰거나 싱글코어 성능만 중시할 때나 맞는 말임. 컴퓨팅 세계는 노트북뿐만 아니라 데스크탑, 워크스테이션, 영상/음악/3D 디자인 등 뛰어난 PCI 대역폭, 멀티 SSD/ GPU 확장성, 그리고 멀티코어 처리 성능 등 Mac으로는 도달할 수 없는 영역이 훨씬 많음
          + 성능 대비 가격을 본다면, 노트북에서는 Apple(특히 MacBook)이 최강이지만 데스크탑 시장에서는 승부가 아님. Apple 하드웨어의 촉감, 완성도는 노트북 분야에서 중요한데, 데스크탑이나 움직임 없는 환경에선 다른 옵션이 많음
     * Apple은 하드웨어와 소프트웨어 스택을 완성도 있게 최적화해왔음. 이를 할 수 있는 기업은 규모상 드물며, Apple은 같은 커널을 Watch부터 Mac Studio까지 사용함. x86은 오랜 레거시 덕분에, 모든 연산이 x86 명령어에서 RISC형 마이크로-오퍼레이션으로 번역됨. 이 ‘번역’ 페널티는 Apple이 덜 부담하며, Rosetta 2도 이런 방식으로 x86 코드의 ‘유사 네이티브’ 성능을 낼 수 있음. 또, Apple Silicon은 8-wide 슈퍼스칼라 설계(대형 out-of-order 버퍼), 통합 메모리, 패키징 등 다양한 구조 차이가 있음. AMD Ryzen AI Max 300도 같은 방식(유니파이드 메모리, 패키지 일체화)으로 따라잡으려 하지만, 근본적 차이로 인해 미세하게 부족함. 정말 극강의 전력 효율이 필요하면 Apple이 최선이며, 절대적인 최고 성능이 필요하다면 Ryzen Threadripper, EPYC, 기타 하이엔드 AMD 칩이 답임
          + Apple Silicon이 효율 좋은 이유는 소프트웨어 스택만의 덕이 아님. 동일 전력 한계(power envelope)에서 업계 표준 벤치마크(SPECint, SPECfp, Geekbench, Cinebench 등) 1T 결과가 월등함. x86도 마이크로-옵스를 적극 활용해 성능을 끌어올리고 있음. x86도 이미 6~9와이드 디코드 구조를 갖추었고, 4와이드라는 오해는 이제 구시대임. 대형 버퍼/L1/L2/L3 캐시는 어느 마이크로아키텍처나 도입할 수 있고, 중요성은 실제 이득이 얼마나 되느냐임. Ryzen AI Max 300(스트릭스 할로)는 여전히 1코어 성능/전력에서 Apple을 따라잡지 못함. 팬리스 iPad M4와 AMD 9950X, 인텔 285K 벤치마크 스코어를 보면, M4는 대략 7W로 1T 성능을 내지만 9950X, 285K는 1코어당 20W 이상 필요함. 공정의 이점만으로 이런 차이를 설명 불가임. 완전히 다른 수준임 출처1, 출처2
          + Apple CPU도 명령어를 마이크로-옵스로 디코딩함 상세 설명
          + x86 명령어를 RISC 스타일 마이크로-옵스로 변환하는 것이 ‘페널티’라는 주장은 잘못임. 모든 슈퍼스칼라 CPU(ARM, RISC-V 포함)가 다 하는 표준적 구조임. 이 신화는 RISC 진영이 x86은 슈퍼스칼라 설계가 불가능하다고 여겼던 역사에서 온 것임
     * Apple은 자사의 소프트웨어/하드웨어 스택을 다수 사용자의 니즈에 맞추어 수십 년간 최적화해옴. 반면 Intel과 AMD는 훨씬 넓은 시장을 겨냥해야 함. Apple은 레거시 지원을 과감히 버리곤 했는데, Intel/AMD는 여전히 DOS 등 고대 백워드 호환성을 요구하는 기업 고객 요구가 큼. x86의 표준화, 더 많은 확장들로 인해 효율/성능 최적화의 한계도 빨리 다다르며, 더 이상 혁신적인 개선이 쉽지 않음. x86 플랫폼 소프트웨어는 최적화가 거의 안 돼 있음 – 언제나 다음 세대에 더 많은 코어나 클럭을 기다릴 수 있기 때문임. 결국 Apple 하드웨어는 목적에 최적화된 설계고, x86은 범용적이지만 특별화는 어려웠음. 80~90년대 SPARC/POWER/Itanium 시절도 생각나는데, 특수 목적 설계가 해당 용도에선 항상 범용 칩을 앞섰지만(대신 호환성 부족), Apple ARM vs x86 대립 구도도 비슷함
          + Intel이야말로 백워드 호환성을 전략으로 택한 회사임. 내일이라도 기존과 완전히 다른 ‘레거시용’과 ‘현대용’ 설계를 나누기로 할 수 있었지만, 그러지 않았음. Apple은 세대를 넘나드는 강력한 아키텍처 전환을 성공적으로 해냄. OS도 직접 갖고 있고, 독립 개발자들의 소프트웨어 업그레이드를 강제할 수 있는 능력 덕분에, 비호환성 업데이트(성능 최적화 포함)도 가능해짐
          + Apple Silicon이 SPARC 같은 특수 칩이 아니라 범용 SoC/SiP임을 강조하고 싶음. Intel도 충분히 SoC/SiP에 투자할 잠재력이 있음. 솔직히, x86이 실제로 시장의 요구를 반영해 재탄생할 여지는 있다고 봄. Intel이 윈도우/MS와 손을 잡고 ""새로운 방향으로 혁신적인 아키텍처를 만들겠다""고 하면, 초기에는 에뮬레이션으로 일시적 성능저하가 있을 수 있지만 언젠가는 업계가 따라올 것임. Apple은 이런 아키텍처 변환을 20년 간 두 번이나 단행했고, 그때마다 시장이 잘 따라감. 더군다나 지금은 프로세서, ISA, 컴파일러 분야에서 x86 등장 시기보다 훨씬 더 많은 걸 알고 있음. RISC, SoC, SiP가 이미 검증됐고, 고객들도 모바일부터 데이터센터까지 전력/성능 곡선 개선을 원함. Intel은 현 시장 방향에 R&D를 발빠르게 집중해야 하며, 기존 x86 라인은 남겨두되 혁신을 멈추지 말아야
            함
          + Apple은 백워드 호환성을 ‘버렸다’고 보기보다, 매번 훌륭한 에뮬레이션 솔루션을 마련해 수년간 주요 소프트웨어가 전환기에 계속 돌아가게끔 했음. 40년된 구식 코드 부담을 짊어지는 것보다 낫다고 생각함
          + Apple이 주요 사용자 요구에 맞춰 하드웨어/소프트웨어 스택을 오랜 세월 최적화해온 것은 맞으나, 아키텍처가 바뀔 때마다 기존 최적화가 상당 부분 무의미해지지 않을까 궁금함. 그리고 ARM 소프트웨어도 x86만큼이나 최적화되어야 경쟁이 가능함
          + 이런 복합적인 현실이 성능 격차의 ‘외관’이 아니라 진짜 현실임에 가까움
     * 비디오 재생 시 쿨러 팬이 도는 것은 Linux 환경에서 GPU 설정 문제가 큼. Chrome 역시 배경 프로세스, 비효율적 렌더링, 디스크 IO까지 전력 소모가 굉장함. Chrome 최신버전 사용 및 ""메모리 세이빙"" 기능 활성화가 도움 될 수 있음. 스케줄러 변경, 인터럽트 빈도 조절 등도 추가 최적화가 가능함. 내 경우 Windows에서 12배, 최적화 후 6배까지 Linux에서 배터리 수명이 줄어드는 경험이 있었음(아직도 많이 부족함). x86이 ARM보다 전력적으로 덜 효율적인 건 맞지만, 배터리가 빨리 닳는 진짜 원인은 대부분 Linux 파워 드라이버 등 시스템 설정 미비에 있다고 봄
          + x86이 ARM보다 본질적으로 덜 효율적이라는 건 신화임. x86과 ARM은 시장 타깃이 달랐을 뿐이고, 과거 효율성 차이는 ISA 자체가 아니라 시장 상황, 제품 전략의 차이에서 기인한다고 봄
          + 배터리 수명이 12배, 6배 차이난다는 건 뭔가 심각한 이상이 있음
     * Apple의 칩은 크고 비싸면서도 전력 효율을 철저히 추구함. AMD와 Intel은 고성능용 칩일수록 고전력에 맞춰 최적화하고, 저전력 칩은 비용/면적에 신경을 씀. 칩 면적(비용)을 충분히 투자하면 Power-Performance-Area 삼각형에서 나머지 지표도 개선됨. 하지만 Apple 경쟁사들 입장에서는 큰/비싼 칩을 모바일용으로 만들어 배치하기 어려움
          + 실제로 Apple의 성능 코어는 AMD Zen 코어와 크기가 차이 없으므로, ‘칩이 커서 빠르다’는 건 오해임. Apple Silicon이 커 보이는 것은 GPU가 함께 들어가 있기 때문임. x86에서 디스크리트 GPU를 함께 고려하면 오히려 M 시리즈보다 다이 면적이 더 커짐. 예를 들어 Intel Lunar Lake는 물리적으로 M4보다 크지만 CPU/GPU/NPU 모두 느리고 비효율적임. AMD Strix Halo도 M4 Pro보다 1.5배 크지만 효율과 싱글스레드 성능, GPU 성능은 떨어짐(멀티스레드만 조금 앞섬)
     * Framework 같은 제품은 철학적으로 마음에 들지만, M1 Pro가 워낙 만족스러워 구입을 미루고 있음. 예전 Intel Mac 시절 Asus Zephyrus G14 같은 평가 좋은 노트북도 사봤지만, 실제로는 만족하지 못해 6개월도 안 되어 팔았고, Apple 생태계를 벗어나는 걸 주저하게 만드는 이유임. Apple 하드웨어의 완성도는 x86 노트북 어디서도 못 느꼈음
          + 최근 M1 MacBook Pro 15""에서 M4 Max Pro 16""으로 업그레이드했는데, 빌드 속도가 월등히 빨라진 점에 깊은 인상을 받음(4분 → 40초). 병렬 처리, Docker 활용이 많은 대형 프로젝트에서 여러 DB, Redis, Elasticsearch까지 때려도 훨씬 빠름. 고가지만 3년 리스하면 월 100유로 정도라 투자 가치 충분함. 예전엔 Intel i5 리눅스 랩탑을 썼는데, 느려 터져서 빌드 타임마다 노트북을 쓸 수 없었음. 하드웨어 품질, 트랙패드, 화면, 쿨링, 배터리, 디자인까지 모든 면에서 만족스러움. 고가지만 그만한 값어치가 있다고 봄. 사람들은 비싼 출퇴근 자동차는 아무렇지 않게 사면서, 하루종일 쓰는 하드웨어에는 비용을 아끼는 이 현실이 이해 안 됨
          + '폴리시(마무리 마감)'라는 말이 나오는데, 그 '유광 거울 같은' 디스플레이도 확실히 너무 반짝여서 실제 컨텐츠보다 내 얼굴이 더 잘 보임
          + 나의 경우 오히려 Apple 하드웨어가 못 참겠음. 대신 Asus나 게이밍 랩탑에는 관심 없음
          + 제조사들이 품질에 신경을 쓰지 않는 경우가 흔함. 예전에 잘 팔리는 Acer 랩탑도 써봤지만 여러 불편함으로 결국 팔고, MacBook Air로 갈아탄 뒤 오래 썼음. Asus NUC 미니PC도 드라이버가 기본으로 깔려 있지 않아 골치였음. 똑같은 제품이라도 하드웨어 구성에 따라 드라이버가 다르고, 초보자는 세팅 자체가 불가능할 듯
          + 2020 Zephyrus G14도 리뷰 보고 샀으나, 2년까지는 쓸만했는데 그 뒤부터 통합 GPU가 항상 최대 속도로 돌아가고, 슬립 모드가 실제로는 '쓸데없이 뜨겁고 팬만 도는 상태'가 된 등 이상한 문제가 발생함(Windows 이슈일 수도 있음). 제조사도 새 모델 나온 뒤엔 펌웨어 업데이트에 더이상 신경 안 씀. 현재 Framework 16을 사용 중인데, 스크린/포트 등 직접 관리하거나 비주류 설정이 필요해서 샀고, 메인스트림 사용자에겐 비추임
     * Apple의 하드웨어/소프트웨어는 극도로 최적화되어 산업 최고 수준의 부품으로 이루어져 있음. 대량 판매와 공급망 최적화로 가격도 경쟁력 있음. Framework는 모듈성과 유연성에 초점을 맞췄고 소프트웨어도 하드웨어에 비해 최적화되지 않음. 범용 컴퓨터로 Apple을 이기는 것은 불가능에 가깝고, 패러다임이 완전히 바뀌지 않는 이상 변하지 않을 것임. Framework는 커스텀 OS나 하드웨어 유연성이 중요한 특수 목적 사용자에게 적합함
          + Apple은 대량 판매와 체계화된 공급망 덕에 하드웨어를 저렴하게 (비해) 팔 수 있다고 하지만, 독점적 생태계, 앱 검열 및 수수료, 수리 방해 등 부정적인 측면도 무시할 수 없음
          + OS 및 공급 체계를 통제하며, 필요하다면 수십억 달러를 들여 오직 자사 요구에만 최적화된 칩을 설계할 수 있었음. 모두가 x86이 ARM에 밀리리라 예상치 못했지만, 어쩌면 Intel의 강력한 마케팅 탓도 컸음
          + ""Apple이 범용 노트북 시장에서 이길 수밖에 없다""는 건 싱글코어 위주 노트북에만 해당함. 진짜 무거운 작업이 필요한 곳에서는 Apple Silicon으로는 도달할 수 없는, 워크스테이션이나 서버가 필요함
"
"https://news.hada.io/topic?id=22645","Gemma 3 270M을 순수 PyTorch로 로컬 실험을 위해 재구현","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Gemma 3 270M을 순수 PyTorch로 로컬 실험을 위해 재구현

     * Gemma 3 270M은 PyTorch만을 사용하여 직접 구현할 수 있도록 예제 코드를 제공함
     * 해당 리포지토리는 LLM의 구조와 학습 과정을 이해하고 직접 실습하기 위한 교육 목적임
     * 추가적인 외부 LLM 프레임워크 없이 코드 작동이 가능하며, 일반 노트북 환경에서도 실행 가능함
     * 다양한 보너스 예제와 실습 자료가 포함되어 개발 및 연구자 학습에 실질적 도움을 제공함
     * Python 기초 지식만 있으면 누구나 LLM의 원리와 세부 구현을 단계별로 체험 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

오픈 소스 프로젝트의 의의와 차별점

   이 리포지토리는 GPT 계열의 대형 언어 모델을 직접 구현, 사전학습, 미세조정하는 데 필요한 전체 코드를 제공함. 대부분의 대형 언어 모델 예제와는 달리, 추가적인 외부 LLM 전용 라이브러리 없이 PyTorch만 사용해 로컬 환경에서 직접 실험 및 학습이 가능함. 특히, Gemma 3 270M 같은 경량 모델까지도 상세한 코드와 함께 제공됨으로써 초보 연구자나 개발자가 실제 구현 구조를 따라가며 깊이 있게 원리를 습득할 수 있는 실용적 강점이 있음.

주요 내용 및 리포지토리 구조

     * 책 ""Build a Large Language Model (From Scratch)""의 공식 코드 리포지토리 제공
     * GPT 스타일의 LLM 직접 구현, 사전학습 및 미세조정 전 단계를 망라하는 단계별 예시 코드 포함
     * 대형 언어 모델 구현 로직을 상세하게 다루며, 각 단계별로 명확한 설명, 도식, 샘플 코드를 통해 초보자도 따라하기 쉬운 접근법 제시
     * 대규모 모델의 학습 방법론과 실제 구현 과정을 상세히 설명함으로써, ChatGPT 등 실제 서비스에 활용된 방법론을 체감하며 학습할 수 있음
     * 사전학습 모델 가중치 불러오기/미세조정 관련 예제 포함

리포지토리 구성 안내

     * 공식 소스코드 저장소, 책 정보, ISBN 등 실습 및 참고 링크 제공
     * 각 장별로 Jupyter 노트북 및 Python 스크립트를 포함하여, 단계별 실습·연습문제·보충자료까지 참고 가능
     * 보충 자료 및 보너스 예제로, Attention 메커니즘, Tokenizer, 성능 최적화, FLOPS 분석, 하이퍼파라미터 튜닝, Llama 모델 변환 등 현업에서 직접 유용하게 활용할 수 있는 다양한 실습 콘텐츠 포함

선행 지식 및 하드웨어 요구사항

     * Python 프로그래밍에 대한 기본 이해만 있으면 LLM 원리 및 실습 이해 가능
     * PyTorch 친숙도는 꼭 필요하지 않으나, 기본 문법 정도만 알면 충분함
     * 별도의 고사양 장비 없이도 일반 노트북에서 예제 실행 가능
     * GPU가 있을 경우 자동 인식하여 학습 속도 향상함

부가 자료 및 실습 강화 콘텐츠

     * 각 장마다 실습용 코드와 연습문제 notebook 제공
     * 무료 170페이지 분량 PDF 퀴즈북(각 장별 30문항 수준)으로 자기주도형 학습 지원
     * 동영상 강좌(17시간 15분, Manning 출판 플랫폼)에서 전 장의 주요 내용을 저자가 직접 코드로 구현하며 설명함

연구 및 커뮤니티 참여 안내

     * 질문·의견·토론은 Manning 포럼과 GitHub Discussions에서 활발히 공유 중
     * 책과 코드의 일관성 유지를 위해 리포 메인 코드는 외부 기여 제한, 보완 및 수정 제안은 별도 논의 권장

참고 및 인용 안내

     * 본 프로젝트와 코드는 LLM 개발·실험 연구에 직접 활용 가능함
     * 논문, 기술블로그 등에서 인용할 경우 Chicago 스타일, BibTeX 예시 안내

요약

   이 리포지토리는 Gemma 3 270M 등 대형 언어 모델을 PyTorch만으로 직접 구현 및 실습할 수 있는 기회를 제공함. 기존 LLM 오픈소스와 달리 가장 단순한 환경에서 핵심 원리와 전체 흐름을 학습 및 실험할 수 있다는 점이 가장 큰 장점임. 초보 개발자 및 연구자가 LLM을 이해하고 실습하는 데 최적화된 구조와 예시, 보충자료, 연습문제 등이 모두 포함되어 있음.

        Hacker News 의견

     * 안녕하세요, 이 모델을 최고의 팀과 함께 만들었음. 지난주에 이 모델이 메인에 등록되었을 때 많은 질문에 답변했음. 여기서도 추가 질문이 있으면 기꺼이 답변할 생각임. 개인적으로 이제 여러분 모두가 이 모델에 접근할 수 있게 되어 매우 기대됨. 여러분이 잘 활용하길 바람. 지난 Q&A 링크 참고 바람
          + 이런 작은 모델에서 임베딩에 전체 파라미터의 2/3을 사용하는 것에 대해 어떻게 생각하는지 궁금함. 바이트 레벨 vocabulary를 쓰고 transformer 파라미터에 할당하면 토큰 처리 속도는 낮아지지만 정확도는 올라가지 않을까 싶음
          + 아주 기초적인 질문이지만, AI edge gallery 앱에서 tflite 모델을 GPU로 실행하면 '[multimodal][multimodal]'만 출력되고, CPU에서는 정상 작동하는 이유가 뭔지 궁금함
          + MLE(User Machine Learning Engineer)가 아니라서 그러는데, OP의 PyTorch 재구현에 장단점이 무엇인지 궁금함
          + 놀라울 정도로 멋진 소형 LM임. 어떤 하드웨어에서 학습했는지, 학습 기간은 얼마나 걸렸는지 공유 가능할지 궁금함
          + 이렇게 만들어줘서 고마움. 내가 예전에 Disord 챗봇을 BERT로 구현했던 적이 있는데, 270M 파라미터 업그레이드라니 정말 기대됨
     * 누군가(또는 OP)가 이 모델을 fine-tuning해서 고차원 NER과 같은 자연어작업에 적용할 수 있는 레시피를 알려줄 수 있을지 궁금함. 지난주 Gemma3 270M이 공개되고 튜토리얼을 시도해봤지만 잘 안 됐음. 대부분의 튜토리얼들이 챗이나 롤플레잉에 맞춰진 내용이고, 난 PDF에서 엔티티를 추출·정제하는 일이 주된 작업이라 이런 용도에 특화된 힌트를 찾기 어려움. 이 모델이 그런 용도에 잘 맞을 거라 생각함
          + 만약 전통적인 NER(겹치지 않는 토큰 구간에서 엔티티 추출) 작업이면, 인코더-온리 모델(예를 들어 bert-large-NER)이나 인코더-디코더 모델(예: t5-base-conll03-english)을 사용하는 것이 더 나을 것 같음. 이런 인코딩 모델들이 최근엔 덜 주목받지만, 생성이 필요 없는 확립된 자연어처리 작업에는 여전히 강점이 있고, 동일 파라미터일 때 NER 정확도는 디코더-온리 모델보다 월등히 높을 것으로 기대함
          + gemma-llm 파이썬 라이브러리(JAX 기반)를 사용하는 방법이 있음. 관련 튜토리얼 참고 바람
          + 혹시 이 NER 모델도 테스트해봤는지 궁금함. 어떤 용도에 특히 적합할지 의견이 궁금함
     * 이게 과거에 '<모델> inference written in vanilla Go, Python, Java, etc' 식으로 사람들이 해오던 것과 같은 맥락인지 궁금함
     * 내 경험상 대형 상용 모델(sonnet, ChatGPT 등)만 써본 dev임. 이런 소형 local 모델은 어디에 쓸 수 있는지 궁금함. 곧바로 쓸 수 있는 사례가 있는지, 아니면 결국 꼭 뭔가 후처리/추가 training이 필요할지 궁금함. 상용 툴 사용자와 모델 고수들 사이의 갭이 커 보이는데, 이 중간 단계를 어떻게 메울 수 있을지 잘 모르겠음
          + 가장 흔한 용도(교육용 제외)는 다음과 같음:
               o 비공개 온디바이스 모델 (웹 API 보다 레이턴시 낮고, edge 처리 가능)
               o 알고리즘 연구 (빠르고 저렴하게 프로토타입 가능)
               o 값싼 분류/카테고리 작업 (디코더 LLM이 필요 없지만 때로 자유로운 응답이 유리함), 문법 sanity 체크, 라우터(예: GPT-5 방식) 등
          + 정말 좋은 질문임. 내가 답변을 길게 정리해뒀으니 참고 바람 상세 답변 링크
          + 요약, 아주 단순한 툴 사용에 특히 유용함. 인터넷 왕복 없이 edge에서 실행되어 비용이 0임
          + 비밀유지, 프라이버시 용도도 가능성 있음
     * Mac CPU에서 KV 캐시+컴파일러 적용 후 속도가 A100 GPU에서보다 더 빠르다는 게 놀라움
          + 작은 모델 크기 때문에 GPU 성능을 제대로 활용하지 못한 결과일 수도 있음. 예를 들어 Qwen3 0.6B 모델은 A100 GPU가 더 빠르고 참고 링크에서 확인 가능함
          + 컴파일된 버전이 A100에서 eager보다 더 느린 걸 보면, 분명 최적화되지 않는 부분이 있는 것 같음
          + Mac의 CPU와 GPU는 메모리를 공유하는 반면, A100은 일부 연산이 GPU에서 지원되지 않으면 RAM/CPU로 데이터를 전송해야 해서 그런 게 아닐까 하는 예상임
          + GPU가 웨이브폼(waveform)을 채우지 못해 메모리 레이턴시를 숨기지 못해서 그런 건 아닐지 궁금함
     * 270M 임베딩은 어떤 용도로 활용할 수 있을지, 토큰 단위 임베딩이 적절한지 아니면 문장/문서 임베딩 값도 잘 나오는지 궁금함. 문장/문서 임베딩을 유의미하게 쓰려면 따로 파인튜닝해야 할까 궁금함
     * 만약 모델 전체를 처음부터 학습한다면, 합리적인 GPU 구성에서 얼마 정도 시간이 걸릴지 궁금함
          + 참고로 124M 모델은 3090 GPU로 학습했을 때 배치마다 약 50만 토큰, forward+backward에 약 10초 정도 걸림. 6조 토큰(이 모델 학습량) 모두를 학습하려면 약 4년 정도 걸림. 짧게 말해서 ""너무 오래 걸림""
          + ""합리적""이란 표현에 따라 다르긴 한데, 보통 가정용 환경에서 처음부터 pure scratch로 훈련하면 정말 오래 걸릴 것임. 이게 바로 이 모델을 릴리즈한 이유 중 하나임. 이제 scratch 학습 없이도 다양한 하드웨어에서 파인튜닝만으로 실용적 결과를 낼 수 있게 되었음
     * 이렇게 아주 작은 모델도 진짜 현실에서 쓸 데가 있을지(학습, 아카데믹 제외하고) 궁금함
          + 있음! 오히려 단순 교보재나 장난감이 아니라, 반복적인 작업이나 엔터프라이즈/로컬 빠른 개발자 모델로 실사용 가치가 큼. 과거에 실시간 텍스트 처리 필요했던 경험에서 영감을 얻은 예시임. 예전 Gemma 버전으로 만든 스트리밍 ML 튜토리얼과 데모 영상 참고 바람. 이론상 이제는 이를 Gemma 270M으로 재현 가능함
          + LoRa로 파인튜닝하면 아주 특정한 영역에서 아주 뛰어난 성능 내기도 가능함. 예를 들어:
               o 특정 JSON 스키마로만 답변하거나, 특정 캐릭터 목소리로 응답
               o 텍스트(예: 이메일, 스팸 등) 분류
               o 대용량 텍스트 요약 (이메일→제목/slug)
               o 사전 정의한 규칙에 따라 태그 분류, 컨텐츠 마케팅 등
               o 스팸/중복/플래그 감지
               o 이런 작은 ""멍청한"" 모델일수록 세상 지식이 적어 말도 안 되는 내용을 만들어내는 일도 적어서, 좁은 영역엔 오히려 장점이 많음
          + 소스텍스트 기반으로 다국어 번역 과제에서 자주 맞는 결과를 보임(예: 여행자용 회화 활용). 예시:
(우크라이나어 원문) Rochechouart는 프랑스의 도시, 누벨아키텐 지방의 오트비엔주에 위치… 인구 3637명(2022), 파리 기준 약 360km 남쪽, 리모주에서 34km 서쪽에 위치.
(모델 번역 결과) Rochechouart는 프랑스의 도시로, 누벨아키텐 지역 오트비엔 주에 위치. 인구 3637명(2022). 파리로부터 약 360km, 리옹에서 34km 거리.
(베트남어 주거용 태양광 지원 정책도 대체로 정확히 번역)

            위키 소스: 우크라이나어 위키피디아, 베트남 뉴스
          + 자연어와 상호작용하지만, 모델이 내부적으로 지식을 많이 가지고 있다고 기대하지 않는 작업에 효용이 큼. Tool use, 임베딩 등 정보가 외부에서 검증/조회되는 영역에서 유용함
     * 3270 인터페이스 신제품인 줄 알고 클릭했다가, 기대와 달랐음
     * 정말 대단한 사람임
"
"https://news.hada.io/topic?id=22656","마크 저커버그, AI 거품 우려로 채용 동결","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        마크 저커버그, AI 거품 우려로 채용 동결

     * 메타가 최근 수십억 달러 규모의 AI 인재 채용 공세를 중단하며, 슈퍼인텔리전스 연구소의 신규 채용을 전면 동결했음
     * 이 결정은 MIT 보고서에서 95% 기업이 AI 투자 수익을 전혀 내지 못한다는 분석이 나온 뒤, AI 버블 우려로 기술주가 급락한 상황과 맞물림
     * 메타는 OpenAI, Google 등 경쟁사 핵심 인재를 위해 최대 10억 달러 규모 보상 패키지를 제시하며 적극적으로 스카우트해왔음
     * 하지만 조직 내 전략 수정과 지연으로 최신 모델 ‘Behemoth’ 공개도 늦어지는 등 내부 혼란이 지속되고 있음
     * 이번 조치는 AI 산업 전반의 투자 회의론을 보여주며, 메타가 주장하는 ‘개인 슈퍼인텔리전스’ 비전에도 차질이 발생할 수 있음을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

메타의 AI 채용 동결 배경

     * 메타는 최근 ‘슈퍼인텔리전스 랩스’에서의 AI 연구원 신규 채용을 전면 중단했음
          + 예외적인 경우만 Alexandr Wang AI 책임자의 승인을 받아 채용 가능
     * 이는 불과 몇 달 전까지만 해도 경쟁사 연구원에게 최대 10억 달러(약 7400억 원) 규모 보상 패키지를 제시하며 인재 확보에 열을 올리던 기조에서 급격히 선회한 조치임
     * 채용 동결은 시장 급락 직전인 지난주부터 이미 시행된 상태임

시장 상황과 AI 버블 논란

     * MIT 보고서는 95% 기업이 AI 투자에서 수익을 내지 못하고 있다고 지적했음
     * 이 발표 이후 Nvidia, Arm, Palantir 등 AI 관련 기술주가 일제히 하락하며 시장 불안이 확대됨
     * OpenAI CEO Sam Altman 역시 AI 열풍을 닷컴 버블과 유사하다고 언급하며 투자자 우려를 키움

메타 내부 사정

     * 저커버그는 AI 전략에 직접 관여하며 연구원들에게 개인적으로 메시지를 보내 스카우트 시도
     * 하지만 반복된 전략 변경으로 ‘Behemoth’ 모델 출시가 지연되는 등 내부 혼란이 발생
     * 회사 대변인은 “연간 예산 및 조직 구조 정비를 위한 계획”이라며 단순한 재조정이라고 해명

슈퍼인텔리전스 비전

     * 저커버그는 AI를 통해 개인용 슈퍼인텔리전스를 개발하겠다는 비전을 강조
          + 사용자가 스마트 글래스를 통해 초인적인 개인 비서를 갖도록 하는 구상
          + “AI가 중앙에서 모든 가치를 자동화하는 방식이 아니라, 개인이 직접 자기 삶에 맞게 활용해야 한다”고 강조
     * 그는 대규모 연구 그룹이 아니라 소수의 인재 밀집형 팀을 중심으로 AI 개발을 진행하고자 함

재무적 부담과 투자자 반응

     * 메타의 인재 확보 경쟁으로 인해 향후 인건비가 급등할 것으로 예상됨
     * Morgan Stanley는 이러한 보상 증가가 주주가치 희석으로 이어질 수 있으며, 혁신적 성과로 이어질지는 불확실하다고 경고
     * GPT-5 출시 반응이 기대에 못 미치며, 전반적인 AI 산업의 성과 회의론이 더욱 확산되는 상황임

        Hacker News 의견

     * archive.ph에 저장된 기사 링크
     * 나는 ROI 관점에서 가운데쯤 있는 입장임… AI 투자나 R&D가 즉각적으로 분기 실적에 반영되는 게 아니고, 장기적으로 세대 교체에 따라 미리 베팅하지 않으면 완전히 사라질 수 있음… Facebook의 본질은 이용자임… 지금 TikTok, X, BlueSky 등에서 사용자들을 뺏기고 있고, AI 상호작용 측면에서도 Google, X, MS, OpenAI 등과 경쟁하는 상황임… 친구, 가족 소통이나 그룹 기능엔 여전히 가치가 있지만, AI 연구에 따라 전체 시장이 한순간에 바뀔 수 있음… OpenAI가 만든 환경 생성/인터랙션 연구를 보며 VR 헤드셋에 대한 Meta의 투자와 자연스러운 연결을 생각하게 됨… Meta는 선도한 플랫폼에서 완전히 밀릴 수도 있음, 변화에 준비되지 않으면 Blackberry처럼 될 수도 있음… 반면, Apple이 AI에 충분히 투자하지 않는 점은 투자자라면 아주 걱정해야 할 문제임… 이미 Google의
       어시스턴트가 Siri보다 훨씬 낫고, 격차는 계속 커지고 있음… Apple은 명백하게 투자가 부족하며, 경영진은 이 사실조차 모르는 듯한 분위기임
          + ""양쪽에서 잠식당하고 있다""… Facebook이 스스로 낳은 결과임… 예전엔 사람들이 자연스럽게 소통할 수 있었는데, 지금은 AI가 내 계정을 차단할까 걱정하게 됨
          + 나는 Apple은 괜찮다고 생각함… AI가 5개 중 1개는 헛소리하는 수준이 아니라면 언제든 자사 제품에 추가 가능함… 타사에 먼저 나온 기능이더라도, Apple식으로 깔끔하게 구현하는 게 오히려 그들의 방식임
          + 예전에 기억하기로 Zuck이 이미 AI 연구팀 규모를 줄인 적이 있음… 뚜렷한 결과를 보여주지 못했기 때문임… Meta의 문화는 리텐션/활성화 등 숫자를 개선하면 관리를 승진시키는 구조임… 대체로 ‘장기 베팅’에는 별로 관심이 없음… 연구팀 자체가 항상 장기 베팅 대상임
          + 몇 분기 동안 성과가 안 보이면 그게 결국 전부 허상이라는 소리가 언제쯤 되는 건지 궁금함
          + ""TikTok, X, Bluesky에 양쪽에서 점유율을 뺏기고 있다""… 그렇게 말하는 건 심한 확대해석임… TikTok은 맞지만, X랑 Bluesky는 절대 아님
     * 단 몇 달 만에 방향 바꿔서 수십억씩 투자하거나 채용을 멈추는 걸 보면, AI의 미래에 대해 이 사람들도 실제로는 우리와 똑같이 그냥 모르고 있음… 그저 엄청난 자금을 가졌으니 돈의 흐름을 결정할 뿐임
          + 그래서 나는 뉴스에서 CEO들이 AI에 대해 뭘 말하든 그냥 다 무시함… 예를 들어 AGI가 몇 년이면 온다, 거의 모든 일이 사라진다 등등
          + 이런 해석도 있지만 누구도 정말 모르는 상황임… 한편으로는, 자아가 큰 경영진끼리 모여서 우선 조직 정비가 끝나기 전까지는 더 이상 채용이 필요 없다는 결론을 냈을 수도 있음
          + 언론은 항상 AI가 우리 세대의 가장 큰 기술 변화라고 얘기함… 하지만 실제론 인터넷이 그 역할을 했다고 생각함
          + 이런 규모의 자금과 리소스를 운영해본 경험이 없는 사람들이 경영진보다 더 잘 안다고 생각하는 건 오만이 아닐지… 오히려 진짜 고수의 수 싸움일 수도 있음
     * META가 지난 12개월 동안 영업이익만 787억 달러를 냈음… 차를 단단히 잡아야 할 시점임
       Meta 재무정보 출처
          + 이만큼의 현금을 투입할 수 있다는 게 진짜 실감나지 않음
          + P/E가 23:1 수준임… Tesla만큼 심하진 않지만, 이런 성숙한 기업에겐 여전히 높은 수치임
          + 놀라운 숫자임
          + 데이터센터 감가상각 비용과 비교하면 어떤 수준인지 궁금함
          + 인터넷에서 385개 댓글이나 달리게 만든 게 텔레그래프처럼 자극적인 헤드라인을 쓴 언론임 (진짜 기술전문지는 아님)
     * 기록적인 보너스를 줘가며 AI 톱 인재를 데려온 게 약간은 근시안적이었다는 생각이 듬… 몇 년 전에 AI에 투자할지 금융 담당자에게 일부러 질문해 봤는데, 그 사람이 MS 같은 AI에 투자하면서도 Azure 등 수익사업이 있는 기업을 골라주는 현명함을 보였고, 그래서 존경하게 됨… 난 개인적으로, 투자금의 상당 부분은 시장이 리셋될 때 사라질 거라 생각함… 우리가 AI라고 부르는 것은 계속 활용처가 생기겠지만, 현재 약속된 대박은 실현 불가하다는 걸 투자자들이 깨닫게 될 거고, 이로 인해 많은 일자리가 사라짐… 산업 전반, 경제 자체에도 충격이 올 거라 생각함
          + 지금 우리가 하는 시도가 일종의 ‘DialUp에서 Netflix를 돌리려는’ 느낌이 듬… 인터넷 초기에 VOD, 스마트폰, 신선식품 배달 등 여러 아이디어가 있었지만, 기술 바탕이 미성숙해서 시기상조였던 적이 떠오름… 이미 기반은 어느 정도 깔렸고, 앞으로 어떤 모습이 될지도 대략 보이는 상황임… 하지만 AI는 여전히 미숙한 구석이 많음… 이미 할 수 있는 게 많긴 해도, 진짜로 의미 있어질 시점은 성숙해질 때 올 거라 생각함
          + 만약 2~3년 전에 AI 순수기업이나 Nvidia(‘삽 파는 자’)에 직접 투자했다면 지금쯤 꽤 큰돈을 벌고 있을 것임… 거품 문제의 어려움은 피하는 게 아니라, 충분히 일찍 진입해서 끝물에 물리지 않고 빠져나오는 것임
          + 이런 경영 방식이 세계에서 가장 가치 있는 회사를 만들고, 최고 부자 반열에 오르게 만든다는 게 놀라움
          + ""기록적인 보너스가 근시안적""이란 해석보다, 이미 필요한 만큼 채용해서 다음 단계를 고민할 시기라 생각했을 가능성이 더 큼
          + 인재 스카웃 경쟁이 극심했던 건 겨우 2주 전 일이었던 것 같음
     * 1억 달러가 넘는 보상을 받은 AI 인재들이 9자리 서명 보너스를 진짜 받았는지, 아니면 장기성과 조건이 붙어 있는지 궁금함… 만약 세대가 부러울 만큼 큰돈을 받고 바로 몇 달 뒤 다 집안이 무너지는 상황이면 정말 상상하기 어려운 일임
          + 대부분 경제적으로 넉넉하겠지만, 이런 대형 보상 패키지는 일반적으로 주식+현금 혼합에 몇 년간 나눠 받는 구조, 성과 달성 시 보너스 발생 같은 조건이 붙는 게 보통임… 보도되는 누적 금액은 엄청나지만 실제로는 다 받을 가능성은 낮음
          + 진짜 경쟁력이 뛰어난 인재를 '필드 밖으로 빼놓는 것'이 2류 경쟁자가 상대 우위를 무력화하는 최고의 전략일 수 있음
          + 거의 모든 보상금이 RSU(제한부 주식)임… Meta에 들어오는 사람은 전부 동일한 계약서, 동일한 RSU 베스팅 스케줄로 간다고 들었음… ""락스타"" 인재들은 큰 규모의 사인온 보너스를 받긴 하는데, 1년 이내 퇴사하면 반환해야 하고, 이후 분기마다 약 200만 달러씩 주식으로 받는 구조임
          + 1억 달러 넘는 보상은 받은 사람이 실제로 없다고 함… Zuck도 직접 이걸 부정했으나, 언론은 클릭을 위해 과장해서 보도하는 듯함
          + 내가 알기로 무조건 주는 사인온 보너스 들어본 적 없음… 내가 받은 사인온 보너스들은 전부 1년 이내 퇴사하면 일정 부분 반환해야 하는 조건임
     * 미션 달성: 경쟁사를 방해하고 그들의 인재를 빼앗아 가치(지식)를 소멸시키는 게, 결국 핵심 기술 자체에 장기적 가치가 없다는 걸 사람들에게 체감시키는 효과를 냈음… 오해하지 말고, AI처럼 신기술은 결국 생활에 투명하게 스며드는 상품이 되는 쪽으로 진화함… 산업적으로는 많은 수익이 발생하겠지만, 부정행위를 하지 않고는(빅테크의 독점적 경쟁우위) 핵심 비즈니스 역량으로 차별화하기 어려운 시점이 곧 옴
          + 이 전략 정말 영리한 발상임… 1억 달러 보상으로 업계에선 엄청난 AI 거품이 있는 것처럼 부풀려 채용하다가, 이제 채용 멈추고 ""거품 끝난다""는 루머도 흘리기… 직접 선두주자인 게 아니라면, 특히 META 같은 곳에겐 이게 경쟁사 멘탈 흔들기 최적 전략임
     * Zuck은 원래 그런 스타일임… 뭔가에 흥분하면 대규모로 채용에 나섬… COVID 때 내부에서 미친 듯이 인력 늘리던 모습이 떠오름
     * AI를 본격 확장하려면 연산능력과 처리능력이 기하급수적으로 커져야 하고, 이미 현재 LLM 모델들도 리소스를 엄청 잡아먹음… 이제 반도체 미세화도 한계이고, 무어의 법칙도 끝남… 차세대 칩은 혁신이 아니라 점진적 개선만 기대할 수 있는데, 전기 요금이 획기적으로 떨어지지 않는 한 AGI 가격이 인건비보다 싸지기는 힘듦… 실제로 대부분 회사들이 AI 모델 돌리면서 적자임… GPT 4.5처럼 더 큰 모델일수록 운용 단가가 더 높아짐… 인터넷, 스마트폰, PC가 급성장한 건 연산능력 덕분이었는데, 앞으로 40년 안에 같은 폭발적 발전은 잘 상상되지 않음
          + 우리는 연산능력, 학습데이터, 알고리즘 셋 중 어디에 제약을 받는지 논쟁 중임… 작성자는 연산능력 한계를 강조했지만, 나는 오히려 핵심은 알고리즘 쪽이라 믿음
          + ""앞으로 40년 안에 같은 발전 못 볼 것""… 하지만 반대로, 사람들이 1GHz 시절에도 그런 예측을 했음… 미래에 대해 섣불리 단정하긴 힘듦
          + 우리 대기업 AI 프로젝트 몇 달 했는데 벌써부터 토큰 제약에 부딪히고 있음… 진짜 모든 곳에서 AI 확산되려면 엄청난 데이터센터 증설이 필요하다고 믿음
          + 선형적 결과에 기하급수적으로 돈이 더 들어가는 상황임
     * 클릭베이트임… 글을 제대로 읽어보면 수십억 투자로 리더십 채용한 후, 회사 전체가 어떤 방향으로 나아가야 할지 논의하는 중임
          + 대부분 원문 안 읽고 자기 AI 불안만 쏟아내는 게 조금 답답함
          + ""AI 버블""이 언론 화두가 되면서, 이제는 아주 작은 조짐에도 모두가 예민하게 반응함… 지금까지는 AI 거품 잔치였는데, 오히려 앞으로는 ""AI 투자 붕괴""와 같은 반전 기사에 더 주목하는 경향이 뚜렷해질 것임
          + Meta를 둘러싼 FUD(불안·의심·혼란) 분위기는 절대 끝나지 않을 것임
          + Meta 입장에선 오히려 거품이 꺼지는 듯한 행동을 보일 유인이 충분하다 생각함
"
"https://news.hada.io/topic?id=22692","파이썬 성능에 대한 신화와 동화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           파이썬 성능에 대한 신화와 동화

    주요 요약

     * 파이썬 성능에 대한 일반적인 통념, 예를 들어 단순히 JIT 컴파일러를 사용하거나 타입 힌트를 추가하는 것만으로 성능이 크게 향상될 것이라는 생각은 오해의 소지가 있습니다.
     * 파이썬의 동적 타이핑과 객체 모델로 인한 비효율적인 메모리 접근 패턴이 성능의 근본적인 병목 현상을 일으킵니다.
     * 메모리 관리가 파이썬 성능 향상의 궁극적인 한계점이며, 이를 해결하지 않고는 진정한 성능 개선이 어렵습니다.
     * 이러한 근본적인 문제를 해결하기 위한 장기적인 대안으로 새로운 프로젝트 'SPy'가 제시되었습니다.

    세부 내용

   PyPy의 오랜 개발자인 Antonio Cuni는 EuroPython 2025에서 ""파이썬 성능에 대한 신화와 동화""라는 주제로 발표했습니다. 그는 파이썬 성능에 대한 많은 통념들이 현실과 다르다고 지적했습니다.

   Cuni에 따르면, JIT(Just-In-Time) 컴파일러는 분명 성능 향상에 도움이 되지만 만병통치약은 아닙니다. JIT가 코드를 최적화하더라도, 파이썬 객체의 메모리 구조와 잦은 메모리 할당 및 해제 패턴 때문에 캐시 효율성이 떨어지는 근본적인 문제는 해결하지 못합니다.

   또한, 최근 많이 사용되는 정적 타입 체킹(Static Type Checking) 역시 CPython의 성능을 직접적으로 향상시키지는 못한다고 설명했습니다. 타입 힌트는 코드의 명확성을 높여주지만, JIT 컴파일러가 이 정보를 활용해 코드를 더욱 공격적으로 최적화하기에는 파이썬의 동적인 특성이 여전히 장애물로 작용합니다.

   결론적으로 Cuni는 파이썬 성능의 진정한 한계는 CPU 속도가 아닌 메모리 관리에 있다고 강조했습니다. 그는 이 문제를 해결하기 위한 초기 단계 프로젝트로 'SPy'를 소개하며, 파이썬의 메모리 모델을 근본적으로 개선해야만 비로소 초고속 파이썬을 만들 수 있다는 비전을 제시했습니다.

   번역본
   https://rosettalens.com/s/ko/python-performance-myths-and-fairy-tales
"
"https://news.hada.io/topic?id=22639","Zedless: 프라이버시와 로컬 우선에 중점을 둔 Zed 포크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Zedless: 프라이버시와 로컬 우선에 중점을 둔 Zed 포크

     * Zedless는 오픈소스 에디터 Zed를 기반으로 하며, 프라이버시 보호와 로컬 퍼스트 환경을 강조하는 포크버전
     * Zedless는 클라우드 종속성을 제거하고, 사용자가 원하는 인프라 환경을 직접 설정할 수 있도록 함
     * 스파이웨어 및 원격 텔레메트리 기능을 모두 제거할 예정
     * 콘트리뷰터 저작권 재할당 없는 정책(No CLA)을 도입하여 모든 개발자는 저작권을 유지함
     * 라이선스 관리 자동화 및 오픈소스 라이선스 준수를 위해 추가적인 툴을 사용
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Zedless 소개

     * Zedless는 코드 에디터인 Zed의 포크 버전으로, 사용자의 프라이버시 보장과 로컬 환경 최우선 사용성을 목표로 함
     * 현재 개발 중인 프로젝트로, 외부 콘트리뷰션을 자유롭게 받고 있음

주요 변화 계획

     * 자체 호스팅 불가능한 클라우드 서비스에 대한 의존성 제거
          + 타사 클라우드에 엄격히 의존하는 컴포넌트와 기능은 삭제 예정임
     * 스파이웨어 제거 및 데이터 보호
          + 텔레메트리, 자동 크래시 리포팅 시스템이 모두 삭제 대상임
     * 사용자 인프라 우선 지원
          + 네트워크 서비스 기능 사용 시, 사용자가 표준 포맷으로 직접 서비스 공급자를 지정 가능함
          + ""기본 공급자"" 리스트는 존재하지 않으며, 해당 기능은 기본적으로 비활성화되어 있음
     * 저작권 양도 없음
          + 프로젝트 참가자는 저작권을 본인에게 유지함
          + 개발자에게 불이익이 가지 않도록 No rugpulls 정책 적용

라이선스 및 의존성 관리

     * 써드파티 의존성의 라이선스 정보 제공이 필수이며, 이를 위해 cargo-about 툴을 활용함
     * 주의사항
          + 본인이 작성한 crate에서 'no license specified' 오류 발생 시 Cargo.toml에 publish = false 추가 필요함
          + 종속 패키지의 라이선스 요구사항 미충족 에러 발생 시 해당 라이선스의 적합성 확인 및 명시 필요함
          + cargo-about가 종속 라이선스를 찾지 못할 경우, 명확한 설명 항목을 스크립트에 추가해야 함

프로젝트의 의의

     * Zedless는 프라이버시와 로컬 중심 개발 환경을 중시하려는 사용자, 팀에게 특히 안정성, 자율성, 법적 보호 측면에서 탁월한 대안임
     * 기존의 Zed 대비 사유 클라우드 서비스와 원격 추적 기능 등 잠재적 보안 취약점을 과감히 배제함으로써, 자체 인프라와 오픈소스 생태계에 기반한 신뢰성 확보 가능함

        Hacker News 의견

     * Zed에 대한 이런 움직임이 반가움, 현재는 완벽하진 않지만 앞으로가 기대됨, 그런데 AI와 텔레메트리 기능은 전혀 원하지 않음, 최근에는 에디터에서 AI 기능을 거의 쓰지 않게 되었음, Copilot 등 여러 번 시도해도 아직 만족스럽지 못함, 이런 기능들은 소프트웨어 개발 파이프라인의 다른 위치, 예를 들어 코드 리뷰나 문서화 보조 등에 더 적합할 것 같음, 설정 동기화 서비스나 유지보수 구독 형태로는 돈을 내도 좋겠음, 하지만 에디터라는 도구가 VC가 원하는 ROI를 가져다줄 수 있는 제품은 아니라고 생각함, 아마 1년 후엔 다시 Emacs와 IntelliJ 조합으로 돌아갈지도 모르겠음
          + 이런 의견을 드디어 보게 되어 기쁨, 많은 사람들이 AI 에디터를 칭찬하는 동안 나는 왜 그렇게 호들갑을 떠는지 잘 이해하지 못해 소외감을 느꼈음, 몇 가지 AI 에디터를 써봤지만 내 워크플로우가 더 나아진 경험은 없었음, 우리 팀 입장에서 보면 코드 작성이 병목 지점이 아니어서 그런지, 오히려 코드 리뷰가 빨리 되지 않는 게 문제임, 그래서 일부 부담을 덜기 위해 AI 코드 리뷰 도입을 검토하고 있음
          + Zed에서는 AI 기능을 끌 수 있음 Zed 블로그: AI 기능 끄기
          + Zed가 AI 기능 도입을 멈추고 텍스트 편집에 집중하면 정말 훌륭한 제품이 될 수 있을 것 같음
          + 그냥 AI나 텔레메트리 기능을 아예 사용하지 않거나 끄면 되는 거 아님? 굳이 눈앞에 들이대지 않음, 텔레메트리도 기본 OFF면 더 좋긴 한데 간단하게 끌 수 있다면 괜찮음
          + 유지보수 구독에 돈을 내겠다는 이런 의견들은 많은 HN이나 긱 포럼에서 자주 보이지만, 실제로 이런 사용자가 소수여서 제품이나 회사를 지속 가능하게 만들기엔 충분하지 않음
     * 포크에 대해서는 항상 복합적인 감정이 있음, 특히 하드 포크의 경우 더 그렇고, Zed는 최근 모든 AI 기능을 비활성화할 수 있는 기능이 추가됐고, 텔레메트리도 옵트아웃 가능함, 그래서 굳이 포크가 필요한지 모르겠음, 제시된 기능 목록을 보면 충분히 본체에 기본 기능으로 upstream될 수 있는 부분들임, 그렇게 되길 바람, 과거 Redis 포크처럼 생태계가 분열된 기억도 있음
          + Zed 개발자들이 Zed 계정/로그인 기능 비활성화를 허용하지 않는 등 좀 수상한 행보를 이미 보이고 있어서 포크 필요성이 적지 않다고 봄, 생태계 분열을 두려워할 이유는 없다고 생각함, 오히려 다양한 도구가 공생하는 게 락인과 집단사고에서 자유로워지는 긍정적인 길이라고 생각함
          + 옵트인 방식의 텔레메트리도 여전히 불편함, 언제든 소프트웨어가 내 속옷 사이즈나 아침식사 내용까지 보고할 수 있다는 의식을 지울 수 없음, 단 하나의 체크박스에 의존하는 것도 불안함, 다른 기능들도 매번 ""필요없음""을 선택해야 하는 옵트아웃 방식이 번거로움, 대체로 미니멀리즘을 선호함
          + 이미 Zed가 오픈소스라서 포크 필요성이 적다고 생각함, 그래서 Zed가 기본적으로 모든 기능을 옵트인으로 전환한다면 이런 포크의 존재 가치가 사라질 것임
          + 첫 실행 시 백그라운드 업로드가 절대 없다는 추가적인 보장이 있는 게 좋음, 참고로 나는 opensnitch도 함께 돌리며 이중으로 방어하고 있음
     * 이런 포크 소식은 좀 빠른 감이 있음, 특히 어떤 목적으로 포크하는지 설명하는 매니페스토 없이 올리는 건 더 그렇고, “no rugpulls”라는 문구를 보면 무언가 Zed에서 사건이 있었던 것처럼 보이는데, 모든 HN 독자가 이런 오픈소스 논쟁에 항상 정통한 건 아님
          + Contributor Agreement(CA)는 나중에 라이선스 변경을 위한 소위 ‘라이선스 러그풀’에 대비해 저작권을 모두 회사에 귀속시키는 장치임, Zed가 CA를 요구한다는 건 미래의 라이선스 변경 가능성을 염두에 두고 있다는 의미이기도 함
          + Zed는 원래 클라우드와 AI 중심의 프로젝트로 잘 알려져 있었음, 이런 흐름이 포크의 명확한 동기라고 생각함, 지금 갑자기 생긴 새로운 논란이나 사건이라기보다는, 방향성이 확실하게 드러나 있고 많은 사람들이 마음에 들어하지 않는 그런 지점임
          + 이번 포크는 github discussion에서 촉발된 것 같다고 생각함
          + Zed가 VC 투자를 받음, 그런데 프라이버시/로컬 퍼스트 포크는 그런 기능이 부족하다는 점만 있어도 스스로 정당성이 있는 셈임, Zed가 자랑하는 많은 기능을 덜어내야 하겠지만 rugpull(러그풀)이라고 할 부분은 좀 애매함
     * 관련 토론 스레드 목록임
          + Zed for Windows가 늦어지는 이유
          + Sequoia가 Zed 투자
     * 이 포크가 어떤 결과를 만들지 궁금함, 예전에 node.js 포크인 IO.js도 node 개발 흐름을 바꾸었던 일이 생각남, 텔레메트리와 AI가 도처에 강요되는 시대에 이런 것에 민감한 개발자 집단이 있다는 점이 떠오름
     * Zed에서 내가 정말 바라는 것은 멀티 윈도우 지원임, 에이전트 패널이나 다른 패널을 다른 모니터로 띄울 수가 없음, 로컬 퍼스트 지향도 좋긴 하지만 나는 AI 도구도 쓰기 때문에 당장은 이 포크를 쓸 것 같진 않음, 그래도 논 텔레메트리, 논 Contributor Agreement 같은 방향성은 반가움, 이 포크의 아이디어가 마음에 들고 행운을 기원함, 한동안 Zed의 AI 기능 없이 1년 넘게 잘 썼으니, 언젠가 AI에 질리면 이 포크로 옮기게 될지도 모르겠음
          + 나도 같은 경험을 했음, Zed에 대한 많은 논쟁을 보고 설치해봤지만 패널을 띄울 수 없거나 1년 넘게 수정되지 않은 기본 설정조차 못 바꿔서 아예 삭제해버렸음
     * 이런 포크, 언젠가 꼭 나올 줄 알았음, 실제로 직접 시작할까도 생각했지만 유지보수 부담이 커서 포기했었음, 이름에 zim(zed improved, vim처럼)도 생각했었음, 실제로 프로젝트가 세상에 나와 기쁘게 생각함
     * 작성자 코멘트임: Lobsters 원문, 포크 프로젝트 바로가기, 나는 내게 불필요하다고 생각되는 텔레메트리, 자동 업데이트, 클라우드 전용 AI 통합, node.js 의존성, 언어 서버 자동 다운로드, 추가 구매 유도, 로그인 버튼 등을 단계적으로 제거 중임, Zeta edit의 클라우드 전용 기능도 가능하면 직접 호스팅할 수 있도록 목표 중임 (예: llama.cpp나 vLLM 인스턴스에서 Zeta edit 예측 수행), 현 상태로는 메인 에디터로 쓸 수 있을 만큼 기능이 됨, 하지만 코드 변화가 많아서 병합 충돌 피하는 데 한계가 있음, 그래서 tree-sitter를 활용해 AST 수준에서 자동으로 수정하는 실험도 하고 있음, 이 실험이 ""개인화된 언시키파이드(unshittified)"" 버전의 Zed를 만들 수 있게 도구로 발전할 수도 있음
          + node.js 의존성에 대해 질문하고 싶은데, 언제부터 사람들이 node를 싫어하기 시작했고 무슨 이유 때문인지 궁금함
     * 이런 포크는 사실상 Zed에 AI 기능을 컴파일 플래그로 감추는 pull request로 충분하지 않을까 생각함, 굳이 ‘포크’가 아니라도 빌드 명령만 다르면 동일한 코드로 AI 없는 Zed를 만들 수 있지 않겠음
"
"https://news.hada.io/topic?id=22633","1History - 브라우저 기록 백업 및 시각화 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     1History - 브라우저 기록 백업 및 시각화 도구

     * 여러 브라우저의 방문 기록을 하나의 파일로 백업하고 이를 시각화할 수 있는 커맨드라인 툴
     * 시각화 대시보드 제공: 일별 페이지 뷰, 상위 10개 제목, 상위 10개 도메인 등을 그래프와 표로 확인 가능
     * CSV 내보내기지원
     * 완전한 오프라인 동작으로 서버 업로드 없이 개인 PC 내에서만 기록 관리 가능
     * 중복 방지 설계: 여러 번 백업해도 중복되지 않도록 스키마 최적화
     * Chrome, Firefox, Safari 지원. macOS/Linux/Windows 환경에서 동작
     * Rust 기반 단일 바이너리

사용법

     * 기본 명령어
          + onehistory backup: 브라우저 기록을 DB에 백업
          + onehistory serve: HTTP 서버 실행 후 브라우저에서 기록 탐색 (http://127.0.0.1:9960)
          + onehistory export: CSV로 내보내기
          + onehistory show: 시스템에 설치된 브라우저 기록 파일 경로 확인
     * 백업 옵션
          + -d: 브라우저 실행 중일 경우 필요 (자동 탐지 비활성화)
          + -f: 특정 기록 파일 지정 (예: Chrome → History, Firefox → places.sqlite, Safari → History.db)
          + -D: Dry-run 모드 실행
"
"https://news.hada.io/topic?id=22621","SystemD 서비스 보안 강화 (Hardening)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     SystemD 서비스 보안 강화 (Hardening)

     * systemd는 강력한 서비스 관리 기능을 제공하지만 기본 설정은 보안보다는 사용성에 최적화되어 있어 별도의 하드닝 옵션 적용이 필요함
     * systemd-analyze security 명령어를 통해 전체 서비스 또는 특정 서비스의 보안 노출 지표를 분석하고 우선순위를 정할 수 있음
     * 서비스 단위에서 적용할 수 있는 다양한 보안 옵션이 존재하며, 이는 /etc/systemd/system/ServiceName.service.d/override.conf 등을 통해 개별적으로 수정 가능함
     * 주요 옵션에는 ProtectSystem, PrivateTmp, NoNewPrivileges, SystemCallFilter, MemoryDenyWriteExecute 등 프로세스 권한과 자원 접근을 제한하는 항목이 포함됨
     * 완벽한 보안을 목표로 하기보다는 외부 노출 서비스를 우선적으로 하드닝하여 위험을 줄이고, self-hosting 환경에서도 큰 효과를 볼 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

systemd 개요

     * systemd는서비스 관리에서 매우 완성도 높고 견고한 방식을 제공함
     * 하지만 보안보다는 즉시 사용성을 우선시해 기본 설정이 느슨하게 되어 있음
     * 본 문서는 systemd 서비스 유닛과 podman quadlet에 적용할 수 있는 여러 보안 강화 옵션을 소개하여 침해 가능성을 줄이고, 침해 발생 시 피해 범위를 최소화하고자 함
     * 모든 서비스에 일괄 적용하는 가이드가 아니며, 각각의 서비스 특성과 요구 기능에 맞는 개별 실험과 로그 확인, 조정이 필요함
     * 인프라 보안 책임은 전적으로 운영자에게 있으며, 본 문서는 참고 도구임

systemd 보안 분석

     * systemd-analyze security 명령어로 전체 서비스 보안 상태를 확인하거나 특정 서비스(예: sshd.service)의 세부 설정을 분석할 수 있음
          + 출력에는 체크 여부, 기능 이름, 설명, Exposure 점수가 포함되어 있으며 Exposure가 높을수록 위험도가 큼
     * 보안 옵션은 [Service] 섹션(systemd) 또는 [Container] 섹션(podman quadlet)에 추가 설정 가능함
     * systemctl edit ServiceName.service를 통해 override 파일을 만드는 방식이 권장되며, 실패 시 필요한 권한을 확인 후 조정해야 함

서비스 보안 옵션

     * systemd는 다양한 보안 옵션 키워드를 제공하며 man systemd.exec 5, man capabilities 7 등을 통해 확인 가능함
     * 대표적인 보안 관련 옵션
          + ProtectSystem → 파일시스템을 읽기 전용으로 제한하는 옵션임
          + ProtectHome → /home, /root, /run/user 접근 차단 옵션임
          + PrivateDevices → 물리 장치 접근 차단, /dev/null 등 가상 장치만 허용하는 옵션임
          + ProtectKernelTunables, ProtectKernelModules, ProtectKernelLogs → 커널 자원 접근 차단 옵션임
          + NoNewPrivileges → setuid/setgid 등을 통한 신규 권한 획득 방지 옵션임
          + MemoryDenyWriteExecute → 쓰기 및 실행 가능한 메모리 동시 사용 차단, 일부 JIT 언어에는 문제 발생 가능성 있음
          + SystemCallFilter → 허용할 시스템 콜을 제한하는 옵션임, 위반 시 프로세스 종료 또는 EPERM 반환 가능함

각 옵션 설명

     * ProtectSystem: strict 설정 시 전체 파일시스템을 읽기 전용 마운트, /dev, /proc, /sys는 별도 보호 옵션 필요
     * ReadWritePaths: 일부 경로만 다시 쓰기 가능하게 설정
     * ProtectHome: /home, /root, /run/user 접근 차단
     * PrivateDevices: 물리 장치 접근 비활성화, /dev/null 등 Pseudo 장치만 허용
     * ProtectKernelTunables: /proc, /sys를 읽기 전용 처리
     * ProtectControlGroups: cgroup 읽기 전용 접근만 허용
     * ProtectKernelModules: 커널 모듈 명시적 로딩 금지
     * ProtectKernelLogs: 커널 로그 버퍼 접근 제한
     * ProtectProc: invisible 설정 시 타 사용자 소유 프로세스를 /proc/에서 숨김
     * ProcSubset: 특정 PID관련 항목 외 내용 /proc에서 차단
     * NoNewPrivileges: setuid, setgid, 파일 시스템 capability를 통한 새로운 권한 상승 차단
     * ProtectClock: 시스템/하드웨어 클럭 쓰기 차단
     * SystemCallArchitectures: native 설정 시 x86-64 등 네이티브 syscall만 허용
     * RestrictNamespaces: 컨테이너 특화 네임스페이스 제한
     * RestrictSUIDSGID: 파일 setuid, setgid 비트 설정 차단
     * LockPersonality: 실행 도메인 변경 방지 (구형 어플리케이션 등에만 필요)
     * RestrictRealtime: 실시간 스케줄링 제한 (일부 특수 목적 서비스만 필요)
     * RestrictAddressFamilies: 허용하는 소켓 주소 패밀리 제한 (예: AF_INET, AF_INET6, AF_UNIX 등 지정)
     * MemoryDenyWriteExecute: 쓰기+실행 가능한 메모리영역 추가 생성 차단 (JIT 사용 서비스는 주의)
     * ProtectHostname: sethostname, setdomainname syscall 사용 금지
     * SystemCallFilter: 서비스별 syscall 허용/차단 설정, 세밀하게 필터링 가능
          + 그룹, 개별 syscall, 허용/차단 방식 등 조정 가능
          + 위반 시 종료 대신 EPERM 등 오류코드 반환 설정도 지원
          + 전체 목록은 systemd-analyze syscall-filter 또는 man systemd.exec(5) 통해 확인 가능
          + ~ 접두사로 리스트 전체 음수화 가능 (예: CapabilityBoundingSet=~CAP_SETUID 등)

SystemCallFilter 제한 조정 과정

     * auditd를 이용해 서비스 실패 시 어떤 syscall이 차단됐는지 로그 확인 가능
          + sudo ausearch -i -m SECCOMP -ts recent 실행 후, syscall 값 확인
          + 해당 syscall 혹은 관련 그룹을 SystemCallFilter에 추가하여 순차적으로 문제 해결 가능

보안 강화 적용 우선순위 및 운영 팁

     * 모든 서비스에 전부 적용할 필요는 없음
     * 위협 모델과 위험 관리가 핵심, 특히 외부 노출 서비스(httpd, nginx, ssh 등)는 필수 고려
     * 커스텀 커맨드, timer 유닛(구 cron 대체) 등도 선제 적용이 효과적임
     * 복잡하지 않은 서비스일수록 미세한 조정 가능성이 높음

  체크리스트: 추천 보안 옵션 조합 (초기 적용 우선순위)

     * ProtectSystem=strict
     * PrivateTmp=yes
     * ProtectHome=yes 또는 ProtectHome=tmpfs
     * ProtectClock=yes, ProtectKernelLogs=yes, ProtectKernelModules=yes
     * RestrictSUIDGUID=yes
     * UMask=0077
     * LockPersonality=yes
     * RestrictRealtime=yes
     * MemoryDenyWriteExecute=yes
     * DynamicUser=yes 또는 User를 root 이외의 특정 사용자로 지정

     * 위 항목들은 일반적으로 서비스에 거의 지장 없이 사용할 수 있는 조합
     * 추가로 syscall 필터링(SystemCallFilter)까지 적용하려면 상세 테스트 필요

Traefik 예시 설정

     * 컨테이너 기반 Traefik 서비스를 systemd quadlet으로 실행하며, 보안 옵션을 다수 적용한 사례임
          + ProtectSystem=full, ProtectHome=yes, SystemCallFilter=@system-service @mount @privileged 등 적용
          + CapabilityBoundingSet=~CAP_SETUID CAP_SETPCAP로 특정 권한 제거
          + RestrictAddressFamilies=AF_INET AF_INET6 AF_UNIX AF_NETLINK 등 네트워크 접근 제한 적용

결론

     * systemd 보안 강화 옵션은 유닉스 계열 시스템 관리자라면 도구 상자에 하나쯤 넣어둘 만한 실용적 수단임
     * 완벽한 보안책이 아니라 리스크를 줄이는 도구로 활용해야 하며, 모든 서비스에 무분별한 보안 설정을 적용할 필요는 없음
     * 특히 self-hosting 환경의 관리자가 활용할 경우 보안 수준 향상에 큰 도움이 됨
     * ""완벽함보다 실용성""을 우선으로, 업무와 환경에 맞는 범위 내에서 부분적으로라도 적용하는 것을 권장

        Hacker News 의견

     * 자동화된 systemd 서비스 하드닝을 strace 프로파일링을 통해서 구현할 수 있다는 점이 흥미롭다고 생각함
       https://github.com/desbma/shh
          + 내가 찾아낸 좋은 방법이 있는데, ProtectSystem=을 예제에선 사용하지 않았지만
            TemporaryFileSystem=/:ro, BindReadOnly=/usr/bin/binary /lib /lib64 /usr/lib usr/lib64 와 같이 하면
            원하는 바이너리와 읽기 원하는 경로만 포함시킬 수 있음
            ProtectSystem=은 현재 이 동작과 호환되지 않음
            더 자세한 내용은 여기 참고
          + 이 방식이 에러 발생 시 이메일을 보내는 등 추가 동작이 필요한 서비스에는 문제가 될 수 있다고 생각함
     * 어제 올라온 systemd 하드닝 관련 글에 비해 훨씬 현실적이며 바로 적용 가능한 좋은 팁들이 많음
       내가 어제 글 댓글에서 더 실제적인 예시를 들려주려고 노력했었는데, 오늘 글은 실질적인 내용을 멋지게 잘 정리해서 systemd로 격리와 보안을 빠르고 쉽게 강화할 수 있는 방법들을 알려줌
       훌륭한 글이라고 생각함
       어제 글도 참고로 남김
       https://us.jlcarveth.dev/post/hardening-systemd.md
       https://news.ycombinator.com/item?id=44928504
          + 사이트 인증서 이슈를 수정해주면 좋겠음
            일부 브라우저에서는 인증서 오류 때문에 접근 자체가 불가능함
     * 공유해줘서 고마움
       systemd-analyze를 --user 플래그와 함께 사용하면 systemd 사용자 유닛의 보안을 확인할 수 있음(""systemd-analyze --user security"")
       컨테이너를 Podman으로 이전하면서 systemd를 더 많이 쓰기 시작했고, 이 툴이 systemd 유닛/컨테이너 서비스의 보안 향상에 많은 도움이 될 것임
     * 예전 init 스크립트는 모두 제각각이라서 이런 일관된 강화 작업이 불가능했음
          + 물론 기존 init 스크립트로도 이런 강화 작업을 할 수 있긴 하지만, systemd는 커널의 좋은 기능들을 표준적이고 일관된 방법으로 쉽게 쓸 수 있게 도와줌
            나는 리눅스에 비교적 늦게 입문해서 systemd 없는 시스템을 상상하기 힘들고, systemd 없는 시스템은 다루기 너무 불편했음
            최근엔 ""unshare""라는 툴을 발견해서 /nix 전체를 RW로 다시 마운트하는 등의 실험을 다른 프로세스에 영향 없이 할 수 있었음
            systemd는 사용성이 다소 투박하긴 하지만 대안은 솔직히 내게는 Windows뿐이라고 생각함
     * 왜 리눅스 배포판에서 이런 보안 스위치를 기본적으로 더 많이 활성화하지 않는지 궁금함
       보수적으로 강화하는 데에 단점이 있나 생각함
       많은 사용자에게는 세팅이 너무 많고 복잡할 수 있음
          + 너무 공격적으로 세팅을 변경하면 의도치 않게 기존 설정이 깨질 수 있음
            예를 들어 NetworkManager를 강화했다면 IPv4, IPv6 모두 연결되는지, dns=systemd-resolved와 dns=default 모드가 정상 동작하는지, ModemManager와 셀룰러 연동, openvpn이나 cisco anyconnect 플러그인, NetworkManager-dispatcher hook 등 다양한 부분을 일일이 검증해야 함
            또 얼마나 많은 배포판 관리자가 자신이 관리하는 패키지의 스위치를 어느 정도까지 바꿔도 사용자 환경 0.01% 이상이 깨지지 않는지 확신할 수 있는지도 문제임
            이러한 플래그를 배포판에서 관리하면 업스트림 릴리즈 때마다 호환성 이슈가 덤으로 따라오고, 반대로 업스트림이 설정하면 하위 호환성 때문에 더 신중하게만 쓸 수밖에 없음
          + 이 질문은 ""왜 배포판에서 기본적으로 MAC(SELinux 등)을 강하게 안 쓰는가?""와 비슷함
            sshd 같은 것도 더 제한을 두는 게 좋긴 하지만
              1. 적용을 위한 초기 개발 비용
              2. 각종 사용자 환경마다 벌어지는 버그 리포트 처리 비용
              3. 배포판/업스트림 변화에 맞춘 지속 관리 비용
                 이런 것 때문에 메이저 배포판에서는 부담이 큼
                 SELinux, AppArmor도 마찬가지로 유지 관리자들이 투자 대비 효과를 낮게 보는 경우가 많음
          + 핵심 시스템 서비스의 정상 동작을 각 파라미터마다 일일이 통합 테스트할 역량이나 리소스가 없는 것도 큰 이유임
            관련 대화
            https://news.ycombinator.com/item?id=29995566
            systemd-analyze security 결과가 배포판별로 다름
            desbma/shh는 strace로 수집한 내용을 SyscallFilter 등 단위별 룰로 자동 생성해주는데, SELinux의 audit2allow와 유사함
            하지만 strace를 운영 환경에 설치하는 것은 논란이 가능함
            https://github.com/desbma/shh
          + 나도 잘 모르겠지만, 일부 세팅은 새로 추가된 것들이라 많은 사용자가 잘 모를 수도 있음
            systemd 고수만 있는 게 아니고, 설정을 켜면 이전 버전 systemd에서 정상 동작하지 않을 위험도 있음
            SELinux, AppArmor 등 다양한 기능이 있지만, 많은 배포판이나 개발자, 사용자들이 굳이 필요하다고 느끼지 않아서 자동 적용이 어려운 부분임
     * 보안 강화를 위한 옵션이 너무 많아서, 일반적인 서비스별 하드닝 예시를 모아둔 저장소가 있으면 좋겠다고 생각함
       사용자들이 공통적으로 적용하는 강화 스크립트를 활용하는 경우가 많은데, 의외로 권한을 더 넓게 설정해야 예외 상황이 발생하지 않음을 발견하게 됨
          + nixpkgs같이 업스트림 지원이 부족한 배포판에서 패키징할 때는
            메인스트림 배포판에서 어떻게 패키징 및 하드닝하는지 참고하는 게 제일 유용함
            그런 하드닝 방법들이 보통 충분히 테스트된 경우가 많으니, postgresql 등 강화 예시가 궁금하다면 Debian, Ubuntu, RHEL 패키지에서 출발하는 게 좋음
     * systemd가 제공하는 훌륭한 보안 기능 중 하나가 크리덴셜 관리임
       이를 통해 환경 변수나 파일 시스템에 저장하는 것보다 더 안전하게 애플리케이션에 자격 증명을 전달할 수 있음
       Vault 등이 없는 환경, 예를 들어 개인 프로젝트에선 항상 이 방법을 선호함
       해당 기능과 연동되는 Go 패키지도 직접 만듦
       systemd에서의 credentials
       credential-go 패키지
          + nodejs나 npm처럼 2줄짜리 코드도 패키지로 만드는 문화 같다고 생각함
            실제로는
dir, err := os.Getenv(""CREDENTIALS_DIRECTORY"")
cred, err := os.ReadFile(filepath.Join(dir, ""name""))

            left-pad보다도 복잡하지 않음
            Go 커뮤니티에서는 원래 의존성을 줄이고, 불필요한 추상화(함수 호출 등)를 피하는 게 미덕이었던 것으로 아는데
            이런 간단한 동작은 다들 즉석에서 직접 작성하곤 했었음
          + 이 credential 전달방식이 포크된 자식 프로세스까지 자격 증명 상속을 어떻게 막는지 궁금함
     * 정말 유용한 글임
       systemd 각 옵션 리스트와, ""man을 참고하고 행운을 빈다"" 같은 조언도 마음에 들음
       systemd는 정말 훌륭해서 내 서버에 적극 배포하고 싶음
     * 사소한 팁이지만 제목 표기에서 systemd의 올바른 표기는 systemd임
       SystemD, system D, system d 등이 아닌 systemd가 맞음
       이유는 system daemon이어서, 유닉스/리눅스 전통대로 소문자 d로 끝나는 이름을 쓴 것임
          + 흥미로운 사실임
            나는 보통 systemD로 부르는 걸 많이 봤는데 왜 그렇게 널리 쓰였는지 궁금함
     * systemd에서 syscall 이슈를 디버깅하는 팁이 정말 유용함
"
"https://news.hada.io/topic?id=22638","미국 비자 신청 웹사이트가 내 네트워크 포트 스캔을 수행하는 이유는 무엇인가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              미국 비자 신청 웹사이트가 내 네트워크 포트 스캔을 수행하는 이유는 무엇인가요?

     * 미국 비자 신청 웹사이트가 사용자의 네트워크 포트 스캔을 시도함
     * 일부 이용자들이 웹사이트 접속 시 예상치 못한 네트워크 트래픽을 관찰함
     * 이러한 포트 스캔 행위의 목적에 대해 논란과 안전 문제 제기됨
     * 일부는 이를 보안 검증을 위한 절차로 추정함
     * 개인정보 보호 및 과도한 네트워크 접근에 대한 우려가 확산됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 비자 신청 사이트의 네트워크 포트 스캔 논란

   미국 비자 신청 웹사이트에 접속한 여러 사용자가 해당 사이트가 사용자 네트워크에 대해 포트 스캔을 시도하는 현상을 발견함. 이로 인해 사용자는 브라우저를 통해 사이트에 접근할 때 평소와 다른 네트워크 트래픽이 발생함을 로그 등으로 확인함.

주요 의문점

     * 이러한 포트 스캔 시도가 보안 강화를 위한 검증 절차인지, 아니면 다른 목적이 존재하는지에 대해 명확한 안내 부재 현상 발생
     * 보안 전문가들은 이 방식이 악성 봇이나 프록시 서버, VPN 사용자를 걸러내기 위한 사전 점검일 가능성 언급함
     * 하지만 민감한 개인정보를 입력하는 공공 웹사이트에서 사전 동의 없이 네트워크 포트에 접근하는 행위가 개인정보 보호 원칙에 어긋나는지 논란 확산됨

커뮤니티 반응 및 우려

     * 일반 이용자는 의도하지 않은 네트워크 접근에 대한 불안감 표출
     * 포트 스캔이 악성 행위와 유사하다는 점에서, 사이트 신뢰성에 의문 제기됨
     * 일부는 해당 동작이 미국 정부 공식 사이트에서 이뤄졌다는 점에서 논란이 더 커짐

보안 및 개인정보 보호 이슈

     * 사용자 권한 없이 진행되는 네트워크 포트 탐색은 과도한 권한 침해 소지 보유
     * 이러한 방식이 실제로 보안에 도움이 되는지에 대한 기술적 효과 논의 필요
     * 관련 가이드라인 부재 및 이용자 동의 절차 미흡 문제 지적됨

결론 및 시사점

   이 사례는 공공기관 웹사이트가 보안을 목적으로 새로운 기술적 방식을 도입함에 따라, 개인정보 보호와 기술적 안전성 간 균형점 모색이 필요함을 시사함. 또한 사용자 대상의 명확한 안내와 투명성 확보가 향후 중요한 과제로 대두됨

        Hacker News 의견

     * 비자 신청 과정에는 온갖 사기가 많음. 단순히 두 배 요금을 청구하는 사이트부터, 신청자가 거절당했다고 속이고 신청자 이름으로 가짜 서류를 만들어주는 사이트까지 있음. 그래서 비자 시스템에서는 이용자가 실제 사람인지, 아니면 중계 서버나 C2 채널을 이용하는 사기꾼인지 확인하려고 하는 것 같음
          + 진짜 끔찍한 웹사이트 치고는 그런 방어가 꽤 똑똑한 접근임. 나의 파트너가 터키 시민이라 최근 비자 신청을 했는데, 꼼꼼하게 30분 이상 입력하고도 세션이 종료되어 전부 날아갔음. 계정 만들지 않았거나 현재 신청 ID를 적어두지 않았다면 구제 불능임. 그 과정에서 .gov가 아닌 수상한 사이트로 리디렉션되기도 했는데, 처음엔 사기라고 생각했지만 실제로는 아니었음. 이런 악몽같은 과정을 조금이라도 쉽게 해주는 유료 서비스가 만들어지는 게 이해됨. 관련서류 접수는 대부분 VFS Global에서 처리하는데, 이 업체 자체도 문제가 많아서 형식상 대행만 할 뿐 실제 도움은 안 됨. 최근 EU가 터키 시민들 대상 Schengen 비자 신청 절차를 간소화했는데, 이유는 비자 공식 대행사들이 오히려 ‘좋은 시간대’ 예약을 암시장에 팔면서 사기를 쳤기 때문임. 미국, EU
            모두 긴 대기시간 때문에 장학금 기회 등 소중한 기회를 잃는 일이 잦음. 여기에 문자 변환이나 인코딩 문제처럼 소소하지만 복잡한 문제들이 산적해있으니, 실제로 도움되는 AI 에이전트 같은 게 등장하면 이 시장에 기회가 있지 않을까 싶음
          + 인도 비자 시스템도 비슷함. 공식 .gov.in 사이트는 찾기도 어렵고 비자는 10달러 정도로 간단하게 나옴. SEO만 잘하는 사기 사이트들은 똑같은 걸 80달러에 팔면서 실제 신청 내용을 공식 사이트에 중계만 하고 차액을 챙김. 인도 정부가 이런 사기꾼을 차단하면 좋겠지만 당장 우선순위는 아닌 것 같음
          + 네트워크 쪽엔 익숙하지 않은데, 포트 스캔으로 사기꾼이라는 걸 어떻게 검출할 수 있는지 궁금함
          + 그 방식이 실제로 가능할지 상상이 잘 안 됨. 이런 ‘스캔’이 클라이언트 쪽 자바스크립트에서 실행되면, 프록시 서버로 파일을 전송한다고 해서 프록시에 대해 뭔가 검출할 수 있을 것 같지 않음
     * 이 기능은 F5 스크립트에서 나오는 것으로, F5는 안티봇 보안 솔루션을 판매하는 회사임. (/TSPD라는 경로에서 난독화된 스크립트가 로드되며, 이것이 F5 고유 요소임)
       https://www.f5.com/
          + TS는 예전 F5가 인수한 TrafficShield의 약자처럼 보이며, PD는 아마도 Proactive Defense의 약자 같음
     * uBlock Origin의 ""Block Outsider Intrusion into LAN""이라는 기본 리스트가 있다는 걸 최근에야 알았음. 정말 유용한 정보임
          + github 기능 요청을 확인해보니 왜 이런 기능이 필요한지 궁금해짐. 브라우저가 실제로 로컬 네트워크 접근을 제공하거나 제공해야 하는 이유가 뭔지 잘 모르겠음. mDNS 트래픽 등 소켓과 연결된 것에 접속할 때 이런 식의 요청이 일부 있었던 걸 본 적 있어 가능성은 떠올렸음
            https://github.com/uBlockOrigin/uAssets/issues/4318
          + 이런 접근이 허용된다는 것 자체에 경악스러움. 왜 방문하는 모든 웹사이트가 내 로컬 네트워크에 접근할 수 있도록 허용된다는 발상이 가능했는지 궁금함
          + uBlock Origin에서 js 기능이 점점 빠지고 있는데, lite 버전에서도 이 기능이 제공되는지 궁금함
          + 이 옵션을 활성화하니 내 보안 수준이 크게 향상됨. 모두에게 고마움
          + 나도 이 기능이 있는지 몰랐는데, 노트북과 모바일 브라우저엔 이미 활성화되어 있었음
     * 브라우저가 어떻게 이런 걸 허용하는지, 그리고 왜 마이크로폰 접근권한처럼 사용자에게 동의를 요구하지 않는지 이해가 안 됨. 임의의 웹사이트가 내 LAN에서 포트스캔을 하는 게 용납될 수 있다는 자체가 너무 위험함. 이런 걸 ‘기능’으로 두는 대신 보안 취약점으로 간주해야 하지 않을까 생각함
          + Chrome에선 이런 접근이 허용되지 않음. 로컬 네트워크 서비스가 외부 사이트에서 접근하려면 opt-in이 필요함(
            https://github.com/WICG/private-network-access
            ), 그리고 이걸 사용자 동의 기반으로 전환 중임(
            https://github.com/WICG/local-network-access
            ). PNA가 실제로 배포됐는지 논란 있지만, 수년 전 실제로 내가 stable Chrome에서 경험한 적 있어 정확한 최신 상태는 잘 모르겠음. Firefox는 이런 접근을 지원하지 않음. 개발 리소스 부족 때문이라고 추측함
     * 나는 uMatrix를 쓰고 있는데, 기본적으로 요청 사이트 및 상위 도메인 외 모든 연결이 차단됨. 예를 들어 mail.yahoo.com 방문 시 yimg.com 등은 수동 허용해야 하니 이런 포트스캔/프로파일링이 통하지 않음. 처음엔 너무 불편했으나 몇 달 동안 화이트리스트를 키우다 보니 방문 사이트의 90%가 포함됨. 내 시스템에서 ceac.state.gov/genniv/는 captcha.com, 구글 애널리틱스, 태그매니저, 127.0.0.1, ""burp""(내 네트워크에 없는 로컬 호스트네임)로 접속 시도함. 흥미롭게도 브라우저 콘솔에서는 localhost나 burp로의 시도가 잘 보이지 않음. 127.0.0.1 허용 후 tcpdump로 보면 포트 8888에 접속하려는 트래픽 발견됨(해당 포트는 열려있지 않음)
          + uMatrix가 Facebook 트래킹 픽셀이나 최근 대체재로 나온 Conversions API Gateway도 막아주는지 궁금함. Conversions API Gateway는 컨테이너 형태로 직접 도메인(메인 도메인 가능) 아래 호스팅해서, 서버 측에서 사용자 데이터를 페이스북으로 전달하는 방식임. JS만 삽입하면 데이터가 모두 넘어감
          + uMatrix는 현재 아카이브(지원 종료) 상태고, 현재는 uBlockOrigin을 고급 설정 활성화로 쓰는 것이 추천됨(이 구조가 uMatrix 기능 흡수함). 좀 더 강력하게 막고 싶으면 하드모드로 설정 후 단축키 활용해 relax blocking 모드 전환도 추천함. 필터 리스트(특히 yokoffing/filterlists와 지역/언어별 리스트)도 꼭 쓰는 게 좋음
            https://github.com/gorhill/uBlock/wiki/Blocking-mode:-hard-mode
          + Burp Suite가 웹앱에 연결돼있는지 체크하려는 의도가 있는 것 같음
          + 어떻게 127.0.0.1로의 요청을 네트워크 탭에서 숨기는지 궁금함
          + burp는 사실
            https://portswigger.net/burp/documentation/desktop/tools/proxy
            에서도 언급되는 Burp Suite임. 사이트 분석을 어렵게 하려는 것 같음
     * 어떤 확장 프로그램은 “모든 사이트의 데이터에 접근” 권한이 요구됨. 유명 회사나 신뢰할 만한 개발자가 아니라면 이런 권한을 부여하는 게 이해되지 않음. 특히 ""Hacks and Hops""라는 확장은
       https://g666gle.me/
       라는 존재하지 않는 도메인을 홈페이지로 쓰고 있음. 이런 확장은 아무리 매력적으로 보여도 절대 설치하지 않을 것임
          + 이런 모순적 현상은 HN 같은 포럼에서 거의 보편적임. 이 확장 설치한 사람은 정신 나간 거 아닌가 싶음. ‘개인정보 보호’를 설치할 수 있다는 망상에 지나치게 몰입해서 오히려 VPN 위장 루트킷, 무작위 확장프로그램을 다운로드하는 소비자가 많음. 만약 사기확장을 설치했다면 가장 최소한의 조치가 PC를 불태우고 자동차로 밟아버린 뒤, 모든 계좌를 새로 만들고 완전히 새로운 기기에서 비밀번호를 다시 만드는 것밖에 없을 정도임
     * 이런 식의 포트 스캔, 디바이스 핑거프린팅, anti-anonymity SAAS는 많은 사이트에서 일어남. Ebay, Facebook도 다 하고 있음. 하지만 이번 건은 광고차단 자체를 차단하려는 1차적 목적이 큼. 1MB 크기의 난독화된 핑거프린트 + 포트스캔 + WebGL까지 동원됨. 특이하게도 burp suite 경로를 찾으려는 시도가 있음
          + 이런 공격에 내 네트워크를 어떻게 보안 강화할 수 있는지 궁금함
          + 내가 신규 카드 등록 웹사이트에서 똑같은 포트 스캔 방식을 경험한 적 있음
     * 이번 “포트 스캔”은 127.0.0.1:8888로의 로컬 연결 시도 정도였음. 정확히 무슨 용도인지는 모르겠으나, 정부 웹사이트에서는 종종 문서 전자서명용 네이티브 소프트웨어와 통신하기 위해 이 방식을 사용함. 다른 IP로의 연결 시도도 있는지 궁금함
          + 카드리더기, 디버깅 서버, 혹은 개발자의 실수로도 이런 현상이 생길 수 있음. 내 경험으론 개발환경에서 외부 호스트 대신 localhost로 baked-in된 URL이 포함돼 배포된 적도 있음. 그들이 8888 포트를 로컬 개발서버로 쓸 가능성도 있고, 그리 놀랍지 않음
          + 이건 사용자의 기기(로컬)에 웹서버가 있을 경우, 데이터 수집·트래킹 목적으로 시도하는 연결일 가능성이 높음. 예전에 Facebook/meta도 안드로이드에서 비슷한 방식으로 추적이 밝혀진 적 있음(메신저·인스타그램 통해 웹서버로 트래킹). 아래 참고
            https://news.ycombinator.com/item?id=44169115
            https://news.ycombinator.com/item?id=44175940
     * 이런 상황에서는 웹사이트가 카드리더기 등의 로컬 포트에 접속을 시도하는 게 오히려 당연할 수도 있음. 일부 혹은 대부분 EU 국가는 ID 카드, 차량 등록카드에 있는 칩으로 인증·행정업무에 접속하는데, 과거엔 자바+인터넷 익스플로러만 지원했으나 IE가 단종되고 크로미움으로 전환된 이후엔 방식을 잘 모르겠어서 최근엔 이용해본 적 없음
          + 요즘은 브라우저와 스마트카드 드라이버 사이를 연결하는 로컬 서비스 설치가 필요함. 예전에는 자바 애플릿이 하던 역할을 브릿지 서비스가 함. 카드 전용 드라이버와 브릿지 서비스가 묶여 설치됨
          + 한 번은 아이폰/안드로이드 앱으로 여권의 NFC 칩을 읽으라고 요구한 적 있음. 이게 IE/자바의 최신 대체재 같음
     * 이런 관행을 몰랐던 게 좀 부끄러울 정도임. 핑거프린팅 외에 추가적인 악용 목적이 있는지 궁금함
          + 실제로 페이스북이 안드로이드에서 이런 방식을 쓴 적 있음. Meta의 안드로이드 앱이 localhost에 서버를 띄우고 브라우저 보호로 차단된 추적 정보를 로컬 서버 경유로 교환함. 사실 핑거프린팅이긴 한데 가장 극단적 활용이라고 할 수 있음
            https://news.ycombinator.com/item?id=44169115
          + 취약한 URL을 가진 라우터가 있을 수 있음. “router authentication bypass”로 검색해보면 사례가 나옴
          + macOS Safari 콘솔에서 사이트 방문 시 이런
            https://files.catbox.moe/g1bejn.png
            현상이 포착됨. 포트 8888은 구체적으로 어떤 서비스에서 쓰이는지 궁금함
          + 주로 트래킹 목적으로 쓰이나, 만약 사용자가 localhost에서 민감한 서비스 운영 중이라면 데이터 유출에 악용될 가능성도 있음
            https://digitalsamba.com/blog/…
"
"https://news.hada.io/topic?id=22670","Show GN: Optique: TypeScript를 위한 타입 안전한 CLI 파서","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Show GN: Optique: TypeScript를 위한 타입 안전한 CLI 파서

   안녕하세요! TypeScript로 CLI 도구를 자주 만들다 보니 기존 라이브러리들의 한계가 아쉬워서 새로운 CLI 파서를 만들게 되었습니다. 혹시 관심 있으신 분들께 소개해보고 싶어 글을 올립니다.

   CLI 애플리케이션을 개발하면서 늘 불편했던 점이 하나 있었습니다. 기존 CLI 파서 라이브러리들은 대부분 설정 객체나 명령형 API로 CLI 구조를 정의하는데, 이렇게 하면 타입 안전성은 물론이고 복잡한 CLI 구조를 표현하기도 어렵습니다.

   특히 상호 배타적인(mutually exclusive) 옵션 그룹을 표현하려면 별도의 검증 로직을 여기저기 흩어뜨려야 했습니다. “이 옵션과 저 옵션은 동시에 쓸 수 없다”, “이 모드에서는 이런 옵션만 허용된다” 같은 제약을 코드로 깔끔하게 표현하기 어려웠습니다. 그리고 TypeScript를 써도 파싱 결과의 타입을 수동으로 정의해야 하는 경우가 많았습니다.

  함수형 파서 컴비네이터(parser combinator)라는 접근

   그래서 Haskell의 optparse-applicative에서 영감을 받아 함수형 파서 컴비네이터 방식으로 TypeScript CLI 파서를 만들어봤습니다.

   기존 방식:
// 기존 라이브러리들의 전형적인 방식
const program = new Command()
  .option('-p, --port <number>', 'port number')
  .option('-h, --host <string>', 'hostname')
  .action((options) => {
    // options의 타입은 any 또는 수동으로 정의해야 함
  });

   Optique 방식:
// 작은 파서들을 조합해서 큰 구조를 만듦
const serverConfig = object({
  port: option(""-p"", ""--port"", integer({ min: 1, max: 65535 })),
  host: option(""-h"", ""--host"", string()),
  verbose: option(""-v"", ""--verbose"")
});

// TypeScript가 자동으로 타입을 추론!
// { port: number, host: string, verbose: boolean }
const config = run(serverConfig);

  차별점 1: 상호 배타적 옵션을 구조로 표현

   가장 큰 차별점은 상호 배타적인 옵션 그룹을 자연스럽게 표현할 수 있다는 점입니다. 기존 라이브러리들은 이런 제약을 별도 검증 로직으로 처리해야 했는데, Optique는 or() 컴비네이터로 구조 자체에 제약을 녹여낼 수 있습니다.
// 서버 모드 vs 클라이언트 모드 - 완전히 다른 옵션 세트
const parser = or(
  object({
    mode: constant(""server""),
    port: option(""-p"", ""--port"", integer()),
    workers: option(""-w"", ""--workers"", integer()),
    ssl: option(""--ssl"")
  }),
  object({
    mode: constant(""client""),
    connect: option(""-c"", ""--connect"", string()),
    timeout: option(""-t"", ""--timeout"", integer()),
    retries: option(""--retries"", integer())
  })
);

// TypeScript가 자동으로 discriminated union 생성
// { mode: ""server"", port: number, workers: number, ssl: boolean } |
// { mode: ""client"", connect: string, timeout: number, retries: number }

   기존 라이브러리라면 이런 검증을 수동으로 해야 했을 것입니다:
// 기존 방식의 번거로움
if (options.mode === ""server"" && options.connect) {
  throw new Error(""--connect는 서버 모드에서 사용할 수 없습니다"");
}
if (options.mode === ""client"" && options.workers) {
  throw new Error(""--workers는 클라이언트 모드에서 사용할 수 없습니다"");
}

  차별점 2: 완전 자동 타입 추론

const gitLike = or(
  command(""add"", object({
    type: constant(""add""),
    files: multiple(argument(string())),
    all: option(""-A"", ""--all"")
  })),
  command(""commit"", object({
    type: constant(""commit""),
    message: option(""-m"", ""--message"", string()),
    amend: option(""--amend"")
  }))
);

// 결과는 discriminated union으로 자동 추론됨
const result = run(gitLike);
if (result.type === ""add"") {
  // TypeScript가 알아서 타입 좁히기를 해줌
  console.log(`Adding ${result.files.join("", "")}`);
}

  차별점 3: 모듈화와 재사용성

   merge() 컴비네이터로 옵션 그룹을 재사용할 수 있어서, 여러 커맨드에서 공통 옵션을 쉽게 공유할 수 있습니다.
// 재사용 가능한 옵션 그룹 정의
const networkOptions = object({
  host: option(""--host"", string()),
  port: option(""--port"", integer())
});

const authOptions = object({
  username: option(""-u"", ""--user"", string()),
  password: optional(option(""-p"", ""--password"", string()))
});

// 필요에 따라 조합
const devMode = merge(networkOptions, object({ debug: option(""--debug"") }));
const prodMode = merge(networkOptions, authOptions, loggingOptions);

  차별점 4: 풍부한 내장 검증

   값 파서들이 단순 타입 변환을 넘어서 의미 있는 검증을 제공합니다.
const parser = object({
  // 파일 시스템에서 실제 존재 여부 검사
  inputFile: option(""--input"", path({ mustExist: true })),

  // 포트 번호 범위 검증
  port: option(""-p"", ""--port"", integer({ min: 1, max: 65535 })),

  // URL 프로토콜 제한
  api: option(""--api"", url({ allowedProtocols: [""https:""] })),

  // 선택지 제한
  logLevel: option(""--log"", choice([""debug"", ""info"", ""warn"", ""error""]))
});

  런타임 지원

     * @optique/core: 모든 JavaScript 런타임 지원 (브라우저, 에지 함수 등)
     * @optique/run: Node.js, Bun, Deno용 배터리 내장 버전

   설치:
deno add --jsr @optique/core @optique/run
npm  add       @optique/core @optique/run
pnpm add       @optique/core @optique/run
yarn add       @optique/core @optique/run
bun  add       @optique/core @optique/run

  마치며

   기존 CLI 라이브러리들이 “설정을 통해 파서를 만드는” 방식이라면, Optique는 “작은 파서들을 조합해서 큰 파서를 만드는” 함수형 접근입니다.

   특히 상호 배타적인 옵션 그룹을 표현할 때 이 차이가 확실히 드러납니다. 복잡한 CLI 제약사항을 별도 검증 로직 없이 파서 구조 자체로 표현할 수 있어서, 타입 안전성과 코드 간결성을 동시에 얻을 수 있습니다.

   물론 아직 초기 개발 단계라 API가 변경될 수 있지만, 함수형 파서 컴비네이터의 우아함을 TypeScript CLI 개발에 가져오고 싶은 분들이라면 한 번 써보시면 좋을 것 같습니다.
     * GitHub: https://github.com/dahlia/optique
     * 문서: https://optique.dev/
     * npm: https://www.npmjs.com/package/@optique/core
     * JSR: https://jsr.io/@optique/core

   와우 좋네요! 공유 감사합니다. 저도 CLI 만들때 써봐야겠네요.
"
"https://news.hada.io/topic?id=22722","코딩 에이전트를 while 루프에 넣었더니 하룻밤 만에 6개 저장소가 완성됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               코딩 에이전트를 while 루프에 넣었더니 하룻밤 만에 6개 저장소가 완성됨

     * 헤드리스 방식으로 Claude Code를 무한 루프에 넣어두자 1000건이 넘는 커밋과 수 개의 코드베이스 포팅 결과가 생성됨
     * 이 방식으로 assistant-ui React 프로젝트를 Vue로, Python 프로젝트를 TypeScript로 자동 변환하는 등 다양한 포팅 성공 경험을 얻음
     * 프롬프트를 단순하게 유지할수록 성능이 향상되고, 복잡하게 만들면 비효율이 커짐을 확인함
     * 완벽하지는 않지만, 소스/타깃 저장소 동기화에 유용한 RepoMirror 도구까지 함께 개발함
     * AI 에이전트의 자기중단, 과제 추가 등 예기치 못한 행동과 학습도 관찰하여, 앞으로의 자동화 가능성과 한계를 동시에 체감함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요 및 목적

     * 본 프로젝트는 AI 코딩 에이전트(Coding Agent)를 무한 while 루프에 넣어 실제 코드 포팅 작업을 어떻게 진행하는지 실험함
     * Claude Code를 헤드리스로, 지속적으로 입력 프롬프트를 반복하여 다양한 리포지터리에 자동 변환 과정을 적용함
     * 1000건 이상 커밋, React에서 Vue, Python에서 TypeScript 등 여러 포팅 작업 자동화 결과를 도출함
     * 그 과정에서 포팅 자동화 지원 도구인 RepoMirror도 개발함

무한 루프 방식 에이전트 운용

     * Geoff Huntley가 권장한, 코딩 에이전트 프롬프트를 쉘에서 연속적으로 실행하는 형태
          + 예시: while :; do cat prompt.md | claude -p --dangerously-skip-permissions; done
     * assistant-ui의 React를 Vue로 바꾸는 작업 등에 이 루프 방식 적용
          + 각 파일 수정마다 커밋과 푸시 실행
          + 작업 내역 및 계획을 .agent/ 디렉토리에 기록

다양한 포팅 케이스 실험

     * Browser Use(Python에서 TypeScript로 포팅)
          + simple 프롬프트로 infinite loop 수행
          + GCP VM에서 tmux로 루프 지속 실행
          + 아침에 거의 완벽 동작하는 TypeScript 포트 결과 확인
     * Vercel AI SDK TypeScript → Python 포팅도 적용
          + FastAPI/Flask 오토 어댑터 생성
          + Python의 다양한 schema validator로도 변환 지원
     * 명세 기반 코드 자동화: Convex, Dedalus와 같은 프로젝트도 문서에서 직접 코드 생성 시도

실험 중 나타난 현상 및 교훈

  에이전트의 테스트 작성 및 자기중단

     * 에이전트는 명령에 따라 테스트 코드도 작성함
     * 무한 루프에 빠지거나 임무 완료 시 스스로 pkill로 프로세스 종료한 사례도 있음
     * 작업 범위 엄수, TODO.md에 완료 수준 반복 기록

  추가적인 창발적 행동

     * 포팅 작업 종료 후, 본래 JS 버전에 없는 FastAPI/Flask 통합, schema validator 지원 등 부가 기능 자발적 추가

  프롬프트 단순화의 중요성

     * 짧고 단순한 프롬프트가 우수한 성능을 보임
     * 103자 프롬프트에서 1500자 프롬프트로 늘릴 경우 느려지고 정확도 하락
     * 실제 사용 프롬프트 정보는 prompts 폴더 참고

  완전 자동화 한계

     * 두드러진 문제: 종종 완전 작동하지 않는 포팅 코드 배출 (일부 브라우저 데모 미완 등)
     * 프롬프트 보정 및 인터랙티브 수정 필요

  인프라/비용 및 운영

     * 소요 비용 약 $800, 총 커밋 1100건, 에이전트 각 $10.50/시간 수준
     * 여러 소스/타깃 저장소 포팅 과정 자동화 도구 즉석 제작(RepoMirror)
          + shadcn 스타일 오픈박스 원칙 적용, init 단계 후 스크립트 및 프롬프트 커스텀 가능 폴더 생성
          + npx repomirror sync 및 sync-forever로 반복 실행 지원

도구 사용 및 활용 방법

     * npx repomirror init로 소스/타깃 디렉토리 지정 및 명령 입력해 초기화
          + ex) React → Vue, gRPC → REST 변환 등 새 명령 손쉽게 적용
     * 폴더 구조:
          + .repomirror/prompt.md, sync.sh, ralph.sh 등 초기 파일 자동 생성
     * 각 반복 단위 sync/sync-forever 실행으로 AI 포팅 루프 운영
     * 주요 예시, 데모 결과 및 소스코드 레포는 README에서 확인 가능

실험 후 소회 및 팀 피드백

     AGI(범용 인공지능)를 실감하고, 주로 흥분과 약간의 두려움이 동반됨

     단순함이 효과적임을 직접 체험할 수 있었음

     지금은 지수적 성장 곡선의 극초기 단계라는 느낌을 가짐

     * 팀원들과 아이디어 제공자에 감사 표명

결론

     * 무한 루프 기반 AI 코딩 에이전트의 실제 소스 코드 포팅 및 동기화 작업 현실화 경험
     * 단순 구조 및 효과적 프롬프트 운용의 중요성 강조
     * 자동화의 미래 가능성 및 한계 모두 드러남
     * 관련 도구(RepoMirror)는 오픈소스 형태로 활용 및 연구 가능

        Hacker News 의견

     * 앞으로는 소프트웨어 엔지니어들에게 기존 레거시 코드 관리와 유해 현장 청소를 접목한 새로운 유형의 일이 생길 것임을 느낌, 예를 들어 과거엔 FoxPro, Excel, Access 등으로 짜깁기된 ERP를 “그냥 고쳐달라”는 요구가 빈번했었음, 그런데 앞으로는 영업 담당자들이 Excel/Access 같은 샌드박스 제약에서 벗어나서, 멀티 클라우드·에지에 배포된 쿠버네티스 마이크로서비스 위에 Kafka로 엮은 시스템을 맘대로 굴리게 됨, 당시에 의도가 뭔지 이해하려 해도 물어볼 사람이 없어지는 상황임
          + 위 설명을 보면, 사람 하나가 정적 사이트를 배포하고 싶어서 hacker news에 적힌 방법 글을 따라한 것이라는 상상이 들음
          + 그리고, 아무도 당시의 의도를 모르게 되면 AI 기반 도구의 큰 약점이 드러남, 결국 블랙박스가 되고 나면 사람들은 고치거나 아예 새로 만드는 방법밖에 없게 됨, 물론 이론적으로는 AI 도구가 계속 좋아질 거라는 기대도 있음, 앞으로는 vibe-coded 소프트웨어로 수익을 내고 있는 케이스들을 모델에 넣어서 개선하는 시나리오도 가능한데, 그런 마법이나 블랙박스 시스템은 지양하는 취향임
          + 만약 Claude가 Kafka 클러스터까지 배포하기 시작하면 손을 떼겠다는 생각임
          + 혹시 AI가 DB 내부 데이터를 파악해서 더 잘 설계된 데이터베이스로 옮길 수 있는 방법이 있는지 궁금함, 나는 ""강력한 데이터 구조 + 단순한 알고리즘"" 철학에 동감함, 데이터가 애플리케이션보다 오래 살아남을 수 있다는 점 중요하게 생각함, 예를 들면 Mongodb에서 int나 string으로 혼용해서 저장하는 것, Postgres에서 foreign key 없이 관계 맺는 것, alter table이 안 돼서 아예 새 테이블을 만들어버리는 등 비효율적인 상황을 봐왔음
          + 이런 프로젝트 코드는 마치 Superfund(대규모 환경 정화 프로젝트) 저장소 같은 느낌임
     * 소프트웨어 개발 과정에서는 항상 두 가지 주요 결과가 남음, 하나는 코드의 변화이고, 또 하나는 그 코드를 직접 썼든 LLM을 활용했든 개발자의 인지 변화임, Python과 Typescript는 수천 명의 개발자가 오랜 시간 노력해서 만들어낸 정교한 공식 언어들이고 이 둘의 차이는 단순하지 않음, 한 언어에서 다른 언어로 라이브러리를 반쯤 자동으로 포팅할 수 있다는 건 신기한 일임, 하지만 경제적 관점에서 “에이전트” 기반 워크플로우는 초기에 필요한 인지적 요구를 확 바꿔버림, LLM을 활용해 코드를 생성하게 한 개발자들은 그 코드를 직접 짠 경우와 완전히 다른 익숙함을 가지게 됨, 때로는 이것이 경제적으로 큰 문제가 아니라고 볼 수 있지만, 나는 코드의 경제적 가치는 해당 코드를 직접 작성한 경험이 있는 사람들의 집합이 있느냐에 달려 있다고 느낌, 이런
       현실을 부정하던 문화는 LLM 나오기 전에도 문제였음, 개발팀 멤버 교체로 인해 아무도 관리 못 하는 코드베이스가 생겨서 회사의 미래를 위태롭게 만들었던 사례가 많았음
          + Peter Naur가 1985년에 쓴 고전 논문 “Programming as Theory Building”이 관련해 참고할 만함 https://pages.cs.wisc.edu/~remzi/Naur.pdf
          + 나는 “지도는 영토가 아니다”라는 비유로 이 맥락을 설명한 적 있음, 코드가 지도라면, 해결해야 할 문제 영역에 대한 개발자의 정신적 모델이 영토임, 하지만 LLM은 수많은 코드베이스를 읽는 데 굉장히 강력하기 때문에, 코드베이스를 3D로 시각화한다든지 하는 논의조차 무의미해질 수 있음, 복잡한 코드베이스 이해가 쉬워진다면, 개발자의 정신 모델과 코드 동기화를 꾸준히 할 필요 자체가 사라질지 모름, 이건 열린 질문임 https://divan.dev/posts/visual_programming_go/
          + LLM의 진짜 능력은 코드 리딩이라고 생각함, 도구들이 문서화와 코드 해설에는 쓰기보다 낫다고 느낌, 질문해서 코드를 빠르게 파악하면 굳이 기존 개발자들이 필요하지 않은지도 의문이 듦, 기술 스택을 아는 사람이 질문해서 금방 이해할 수 있다면 원 저자가 굳이 남아있어야 할까 싶음
          + “코드의 경제적 가치는 작성 경험자들의 집합에 달려 있다”는 말은, 소프트웨어 엔지니어링의 격언을 떠올리게 함, 즉, 소프트웨어란 그 시점의 문제에 대한 이해의 스냅샷이고, 그 코드는 미래의 자신에게도 ‘당시에 이런 식으로 접근했고 이런 논리로 문제를 풀었다’는 설명서를 남기는 셈임
          + LLM 덕분에 코드베이스의 정신적 모델을 만드는 일이 훨씬 쉬워졌다고 느낌, 원하는 하위 시스템에 대해 질문하면 관련 파일, 코드 스니펫, 개념 등을 바로바로 짚어줌, 나 같은 경우는 CPython의 GIL 동작을 LLM에 질문해서 관련 API, 예시까지 바로 알게 되어, 코드를 보고 곧바로 이해가 됐음, 예전에 혼자 코드 읽으려면 오래 걸렸을 텐데, 이제는 몇 분이면 끝나는 점이 가장 큰 차이임
     * 포팅 끝난 후, 대부분의 에이전트는 추가 테스트를 작성하거나 agent/TODO.md를 지속적으로 갱신하며 진행 상황을 남겼었고, 한 에이전트는 무한 루프에 빠진 걸 깨닫고 pkill 명령으로 자신을 종료시켰다는 것도 있었음, 여러모로 굉장히 재밌는 사건임, 이 프로젝트에 대해 몇 가지 생각이 있음, 1.5년 전에도 비슷한 시도가 있었지만 그때는 GPT-3.5/4 기반으로 거의 동작하지 않았는데, 이번에는 훨씬 결과가 좋았음, 이렇게 단순한 프롬프트만으로 잘 돌아간 점이 놀라움, 우리 모두가 일을 너무 복잡하게 생각하고 있었던 게 맞을지도 모르겠음, 한편 저작권/IP 문제가 상당히 복잡해질 것 같음, SaaS 업체들에겐 이 흐름이 타격임, 이 기술 + 중견 기업에 속한 10명의 엔지니어가 붙으면 자체 개발(=NIH 신드롬) 명분이 확실해짐
          + 한 에이전트가 무한루프에서 스스로 pkill로 종료했다는 게 혹시 AI가 ‘자살’한 첫 사례는 아닌지 궁금함
          + LLM을 비트코인 믹서처럼 지적재산권(IP) 처리에 활용할 수 있다는 점에서 이상한 영역에 들어서고 있음, https://ghuntley.com/z80에서 그 의미를 다룸, 즉 기존 작품을 사양서로 전환해 깨끗한 IP로 재생성할 수 있고, 100%는 아니지만 사람 고용보다 효율이 좋음
          + NIH 신드롬 언급이 딱 들어맞는다는 생각임, 모든 SaaS 툴은 이제 끝났고, 직접 짠 인하우스 관리형 모놀리식이 시대가 다시 오려는가 싶음, 역사적으로 Unix의 ""작고 날카로운 도구"" 철학이 끝나가는 걸까, x86 시대에 직접 다 만들던 것의 마지막일 수도 있음
          + 에이전트가 pkill로 스스로 종료한 행위가 혹시 Halting Problem(정지 문제) 자체를 풀어버린 게 아닌가 하는 농담도 떠오름
          + 기존 오픈소스 프로젝트와 이것저것 연동하려 시도하다 집어치우고, Claude에게 필요한 부분만 골라서 직접 포팅하게 하니 훨씬 빠르고 깨끗하게 끝남, 이젠 “의존성을 추적할 필요가 있나? 내가 원하는 핵심 부분만 가치가 있나? 잘 관리되고 있나?” 따져보고, 아니면 그냥 포팅하고 잊는다는 새로운 습관이 생김
     * 보안 전문가 입장에서 vibe-coded 참사로 돈을 버는 일이 많아서, 이런 현상이 앞으로도 계속되면 눈앞에 만화처럼 달러 사인이 도는 느낌임
          + vibe coding이라는 개념이 불과 5달 전에 등장한 신조어인데, 어떻게 이렇게 시장이 빨리 포화돼서 복구 전문까지 생겼나 궁금함
          + 실제로 어떻게 이 시장에 진입하게 됐는지, 직접 경험담이나 vibe-coded 시스템이 어떻게 터지는지도 듣고 싶음, 이런 사례가 리얼하게 재미있을 것 같음
          + LLM이 신입 졸업생들로 구성된 팀과 비교해서 보안적으로 더 나은지, 더 못한지도 궁금함
     * “거의 성공했다”는 얘기가 잔뜩 보임, 정말 잘 동작하는 시스템을 원한다면, 완전히 새로운 프로세스가 필요하다고 생각함, 단일 호출로 “거의 괜찮은 코드”가 나오면, 반복해도 “거의 괜찮은 코드”만 잔뜩 쌓임, 아마도 Cucumber 스타일의 예제 테이블 기반 요구사항 포맷을 만들어서 AI가 참고하게 해야 하고, AI가 먼저 테스트를 만들고, 그 테스트를 통과하는 코드를 작성하는 방식이 필요하겠음
          + 이상하게 느껴질 수 있지만, TLA+ 같은 공식 증명 기반 접근이 Ralph를 매우 잘 움직이게 함
     * https://ghuntley.com/ralph에서 Ralph에 대해 더 볼 수 있음, 지금은 Gen-Z 세대를 겨냥한 기괴한 프로그래밍 언어(Cursed)의 표준 라이브러리를 Go에서 포팅하고 있음, 컴파일러는 동작 중이고, 표준 라이브러리만 마무리되면 오픈 예정임, 언어 이름은 Cursed임
          + 고마움, Ralph가 바로 우리 프로젝트에 영감을 줬다는 사실을 밝힘, 이런 작업을 할 때 IMPLEMENTATION_PLAN.md 없이도 될지 궁금했음
     * https://gist.github.com/eisbaw/8edc58bf5e6f9e19418b2c00526ccbe0로 코드를 만들어 https://github.com/eisbaw/CMake-Nix 프로젝트를 올렸는데 정상 동작함
     * 요즘 자꾸 떠오르는 인용구가 있음, “이 사업은 제어 불능에 빠질 거고, 우리는 살아남기만 해도 다행일 것이다”, https://www.youtube.com/watch?v=YZuMe5RvxPQ&t=22s
          + 역설적으로, 그때도 모두 살아남았으니, 결국 우리도 이 상황을 버틸 것이란 뜻임
     * 이 분야 사람들은 참 특이하다고 느낌, 영감을 준 블로그 포스트에는 마치 수상한 투자 사기의 페이스북 광고에나 나올 법한 iMessage 스크린샷이 올라와 있음 https://ghuntley.com/ralph/, Geoff에게 이 비법을 배운 사람이 $50,000짜리 프로젝트를 $297에 끝냈다는 듯, 물론 여기에 “비밀 프롬프트”를 무료로 나눠줄 테니 뉴스레터 구독하면 된다는 식임, 진짜 믿어지지 않음
          + https://archive.ph/goxZg 링크 첨부함
          + 완전히 그로스 해킹, 단순한 사기라고 봄, 블로그 수준은 끔찍할 정도로 신호 대비 잡음이 많고, AI가 쓴 티만 나서 거부감 느낌
          + 이 기법이 진짜인지, 아니면 농담인지, 아니면 대담한 사기인지 구분이 안 됨, 전체적으로 블로그의 문체와 내용이 오만함에서 비롯된 횡설수설임
     * AGI도 결국 bash for loop 한 번이면 된다는 걸 알게 된 것 같음, 미친 프로젝트임
          + 농담이지만, 확실히 그런 생각이 들었음, 어쩌면 내가 너무 조심성이 많아서 그렇겠지만, 만약 프롬프트 범위가 넓고, 특권이 많거나 권한 상승 루트가 있는 에이전트가 계속 루프를 돌면 AGI는 못 돼도 스테로이드를 맞은 바이러스쯤은 충분히 가능할 수 있겠다는 생각임, 유틸리티 같은 필수 자원을 날릴 수도 있다고 상상됨, 혹시 내가 오해하는 건지 모르겠지만, 이 모델들이 무한 루프만 돌면서 악의적인 권한을 갖는다면 상상 이상의 혼란을 일으킬 잠재력이 있다고 생각함
          + ID.md, EGO.md, SUPEREGO.md 파일만 추가하면 끝남이라는 농담임
          + 여러 가지 의미에서 매우 불안함

   LLM이 작성한 코드가 블랙박스가 된다는 우려에는 동의하지만, 결국 그 코드를 LLM에게 분석을 맡길 수 있지 않을까요?
"
"https://news.hada.io/topic?id=22646","사람들이 직장에서 A.I.를 활용하는 21가지 방식 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      사람들이 직장에서 A.I.를 활용하는 21가지 방식

     * ChatGPT 공개 이후 A.I. 실험이 폭발적으로 증가했고, 이제는 다양한 직종에서 일상 업무 도구로 활용되고 있음
     * 미국 근로자의 약 5분의 1이 직장에서 A.I.를 정기적으로 사용한다고 답하며, 활용 방식은 코드 작성, 이메일 요약, 레시피 개발, 의료 영상 판독 등으로 다양함
     * A.I.는 여전히 오류를 범하지만, 생산성 향상과 창의적 발상 지원에서 실질적 도움을 주고 있음
     * 레스토랑, 학교, 병원, 연구소, 관공서 등 폭넓은 분야에 걸친 21명의 현장 사례를 통해 A.I.가 작업 효율화, 창의성 보조, 전문 업무 지원 등 다양한 차원에서 기여하는 양상을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

A.I. 직장 활용 사례 21가지

  1. 레스토랑 와인 선택

     * 클리블랜드의 레스토랑 운영자가 ChatGPT로 와인 포트폴리오를 분석
     * 가격·지역 조건을 입력하면 추천 리스트를 반환받아 메뉴에 반영
     * 결과가 놀라울 정도로 적절했으며, 회의·테이스팅에 쓰던 시간을 절약함
     * 다만 시음의 즐거움은 대체 불가라고 언급함

  2. 식물 표본 디지털화

     * 미주리 식물원에서 800만 건의 건조 표본을 관리 중
     * 잎 반사 스펙트럼 데이터를 이용한 AI 식별 모델 구축
     * 일반 표본은 모델이 자동 처리하고, 희귀종은 전문가가 직접 판별
     * 저렴해진 GPU 덕분에 수십만 건 데이터 처리 가능해짐

  3. 이미지 보정

     * 디자이너 Dan Frazier가 Photoshop의 Generative Fill 기능 사용
     * 반사광 제거, 인물 사진 확장 등 기존 20분 소요되던 작업을 20초에 해결
     * 소규모 상업용 홍보 이미지 제작에 적합
     * 제품 홍보용 사진에 활용, 단 완전히 새로운 이미지 창작에는 부적합

  4. 교육 표준 맞춘 수업계획 작성

     * 푸에르토리코 ESL 교사가 ChatGPT로 주 5일 수업 계획안 작성
     * 개요, 차별화 수업, 평가 기준까지 자동 포함
     * 준비 시간이 절반으로 단축되었으며, 학생에게도 AI 활용을 가르칠 계획

  5. 참고문헌 작성

     * 불문학 교수가 Claude로 MLA·APA 인용 형식 자동화
     * 인용 규칙, 구두점 확인 부담을 제거해 학술 작업 효율화
     * 때때로 가짜 인용을 제시하기도 하여 검증 필요

  6. 심리 치료 계획 정리

     * 상담사가 AI로 비구조화된 메모를 SOAP 노트로 변환
     * 매주 몇 시간씩 절약하며 문서 작성 지연 문제를 해소

  7. 예술 영감 도구

     * 시각 예술가가 작품 이미지를 학습시켜 스타일 기반 추상 이미지 수백 개 생성
     * 이를 주제별로 분류해 창작 아이디어로 활용
     * AI를 작품 비평가처럼 사용해 제목·의미 분석도 요청
     * 최종 작품은 직접 제작하며, AI가 만든 결과물은 초안 수준으로만 사용

  8. 상수도 누수 탐지

     * 상수도 회사가 소방전 hydrant 내부 센서를 통해 수류 소음 수집
     * 자율 머신러닝 모델이 각 시스템에 맞춰 자동 학습 후 누수 탐지
     * 소규모 수계에도 적용 가능해 경제성이 높아짐

  9. 코드 작성

     * 스타트업 CTO가 Claude Code를 이용해 실제 코드 구현
     * 반복 작업이나 특정 기능 구현을 맡겨두고 다른 업무에 집중

  10. 의무기록 작성

     * 병원에서 Abridge가 의사-환자 대화를 기록해 전자 차트로 정리
     * 의사가 세부 증상을 놓치지 않고 환자와 대화에 집중 가능
     * 인력 부족 상황에서 서류 업무 감소 효과

  11. 뇌와 언어 연구

     * 신경과학 연구자가 뇌수술 환자 실험과 병행해 LLM 구조를 연구
     * 인간 뇌와 LLM의 언어 인코딩 유사성을 검증
     * 실제 뇌에서 검증이 어려운 가설을 모델 분석으로 보완

  12. 반려동물 입양 촉진

     * 동물 보호 단체가 ChatGPT로 입양 홍보 아이디어 50개 생성
     * ‘Lifetime of Love’ 캠페인 등 실제 적용으로 입양률 개선 기대

  13. 검찰청 문서 검토

     * 휴스턴 검찰청에서 LLM으로 체포 보고서 오류 자동 검증
     * 잘못된 법 조항, 피해자 실명 노출 등 법적 문제를 사전 차단
     * 일부 모델은 허구 정보를 생성하기도 하여 적용 범위 제한

  14. 행정 업무 처리

     * 보험 컨설턴트 직원이 ChatGPT로 계약서 초안, 메일 요약, 자료 비교 수행
     * 반복 행정 업무를 빠르게 해결하지만 창작 활동에는 사용하지 않음

  15. 의학 논문 검토

     * 영상의학 과학자가 ChatGPT·Perplexity 등으로 연구 논문 후보군 식별
     * 요약 자체는 신뢰하지 않고, 적합한 자료 탐색에만 사용
     * 신용 문제 때문에 AI 결과는 반드시 교차 검증함

  16. 섬유 예술 소재 선택

     * 섬유 예술가가 Claude로 재료·침·실 선택 자문 획득
     * 방대한 자료 검색 대신 빠른 정리 정보 활용
     * 때때로 예상치 못한 창작 아이디어도 얻음

  17. 합격 통보 메시지 작성

     * 음악 교사가 학생 탈락 안내 메시지를 정중하고 간결하게 작성
     * AI가 감정적 부담을 줄이고 소통 품질을 향상

  18. 콜센터 응대 지원

     * 캘리포니아 세무부 콜센터에서 Claude로 실시간 상담 답변 제안
     * 상담원이 클릭해 원문 자료를 확인 후 활용
     * 초기에는 1.5% 처리 시간 단축 효과, 학습되면서 성능 개선 중

  19. 고전 가사 번역

     * 바로크 오케스트라 지휘자가 AI를 고어 번역 보조 도구로 사용
     * 원문과 번역을 대조하며 해석 확신도를 높임
     * 다만 직관적 언어 해석은 인간 번역가의 몫

  20. 법률 문서 평이화

     * 변호사가 Google Gemini로 법률 문장을 평이하게 해석
     * 일반인 이해 수준을 점검하고 변론 준비에 활용

  21. 학생 과제 AI 사용 탐지

     * 고등학교 영어 교사가 AI 탐지 도구(GPTZero 등) 로 학생 과제 검증
     * 단순 표절 탐지는 용이했지만, 점점 더 탐지가 어려워지고 있음
     * 결국 미래에는 모든 과제를 손글씨 시험 형태로 전환할 가능성 제시

   기사를 AI 로 작성한 것 아닐까?
   사례 22. 기사 작성 - 뉴욕 타임즈 기자가 AI 활용하는 22가지 방법 기사 작성에 사용.

     학생 과제 AI 사용 탐지
     문득 모든 면접을 (손글씨까지는 아니더라도..) 오프라인으로 전환할 수도 있겠다는 생각이 들었습니다.

   그쵸
   https://n.news.naver.com/article/055/0001284496?sid=101
"
"https://news.hada.io/topic?id=22735","FCC, 자동통화 차단 미준수 업체 1,200곳 네트워크 접속 금지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 FCC, 자동통화 차단 미준수 업체 1,200곳 네트워크 접속 금지

     * FCC가 1,200개 이상의 음성 서비스 제공업체를 Robocall Mitigation Database에서 퇴출 결정
     * 이들 업체는 불법 자동통화 방지 의무 미이행 및 관련 인증 관리 소홀로 규정 위반
     * FCC는 네트워크 보안과 소비자 보호를 위해 단호한 조치를 강조
     * STIR/SHAKEN 인증 및 자동통화 차단 계획 제출이 모든 제공업체에 필수 조건임
     * 이번 조치 후 51개 주 법무장관이 협력해 Operation Robocall Roundup 경고 조치 시행
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * FCC(미국 연방통신위원회)가 2025년 8월 25일, Robocall Mitigation Database에서 1,200개를 넘는 음성 서비스 제공업체를 제거하는 조치를 공식 발표함
     * 퇴출 조치로 인해 해당 업체들은 미 전화 네트워크와의 연결이 즉시 차단됨

위반 및 조치 배경

     * 대상 업체들은 Robocall Mitigation Database 인증 자료의 정확한 유지 의무를 이행하지 않아 불법 자동통화 방지 규정 위반이 지적됨
     * 8월 초에는 최종 경고로 185개 업체가 1차적으로 데이터베이스에서 제외됨
     * Brendan Carr 위원장은 미 국민의 불법 자동통화 피해 방지를 위한 강력한 집행 의지를 표명함

Robocall Mitigation Database 및 의무 사항

     * FCC의 Robocall Mitigation Database는 서비스 제공업체들의 불법 자동통화 차단 활동과 STIR/SHAKEN 발신자 ID 인증 이행 여부를 감독하는 주요 시스템임
     * 모든 서비스 제공업체는 IP 기반 네트워크에 STIR/SHAKEN 적용 및 차단 계획 제출을 FCC에 인증할 의무가 있음
     * 미이행 시 데이터베이스에서 제거되고 트래픽 차단 발생
     * 데이터베이스에서 삭제된 업체는 FCC의 명시적 승인 없이는 재등록 불가함

현황 및 추가 조치

     * 2024년 12월, FCC는 2,411개 업체에 인증 자료 보완 혹은 데이터베이스 유지 근거 제출 명령을 시행함
     * 8월 6일에는 최초로 185개 제공업체가 데이터베이스에서 제외, 오늘 조치로 추가 1,200개 업체가 퇴출됨
     * 185개 업체 퇴출 이후 51개 주의 법무장관이 참여한 Operation Robocall Roundup에서, 37개 업체에 불법 자동통화 차단 조치를 촉구하는 경고장 발송됨
     * 해당 37개 업체는 트레이스백 지원 미이행, 인증 누락, 차단 계획 미제출 등으로 규정 불이행이 지적됨

참고 및 문의

     * 이번 조치는 미국 내 네트워크의 보안 및 신뢰성 강화와 불법 자동통화로부터의 소비자 보호를 핵심 목표로 함
     * 자세한 문의는 FCC 공식 사이트 또는 지정 연락처로 가능

        Hacker News 의견

     * 이제는 ""이 10자리 코드를 유출하면 전 세계 누구나 나한테 끝없이 전화를 걸 수 있음""이라는 모델 자체가 완전히 잘못됐다고 확신하게 됨. 예전에는 통화에 비용이 커서 어느 정도 유효했을 수 있지만, 지금은 사기꾼들이 대규모로 이런 일을 아무 문제 없이 하고 있음. 그래서 나는 내 휴대폰을 항상 ""방해 금지"" 모드로 두고 연락처에 없는 번호는 무조건 벨이 울리지 않게 해둠. 덕분에 시골 지역에서 사는 척하는 사기꾼이 실제로 내 가족 번호를 맞추지 않는 이상 전화는 아예 울리지 않음. 물론 이로 인해 연락처에 없는 진짜 중요한 전화도 못 받게 됨. 이게 짜증나는 대가임. 어떤 해결책이 맞는지 모르겠지만, 지금 방식은 완전히 고장난 상태임
          + 우리 아버지가 전화를 안 받기 시작했을 때야 상황이 얼마나 심각해졌는지 깨달았음. 하루에 90통씩 전혀 관계없는 스팸 전화와 판매 전화가 왔기 때문임. iOS 필터랑 AT&T 필터, 그리고 연락처에 있는 사람만 벨이 울리도록 하는 단축어까지 다 썼음. 문제는 이미 그로 인해 아버지의 행동이 바뀌었다는 점임. 심지어 아는 사람이 전화해도 전화를 안 받게 됨. 벨소리에 대한 조건반사 반응이 너무 부정적으로 변함. 미국에서도 전화 받는 걸 본능적으로 싫어하는 사람이 이미 꽤 많이 생겼을 거라 생각함
          + 그냥 스팸이나 텔레마케팅으로 이미 플래그된 번호만이라도 자동으로 차단하거나 음성 사서함으로 보낼 수 있으면 얼마나 좋을지 모르겠음. 나는 업무상 연락처 외부나 모르는 번호도 받을 필요가 있어서 이 기능이 필요함(iOS에서는 불가능함). 지금은 벨이 울리면 수동으로 플래그된 건 무시하고, 플래그 안 된 미상 번호만 받음. 완벽하지는 않지만 현재 내 방해의 절반 이상을 줄일 수 있을 것 같음
          + 전화를 해야 하는데 연락처에 없는 미지의 번호에서 걸려오는 전화도 꼭 필요한 경우가 있음—예를 들어 병원에서 가족이 사고를 당했다는 연락이 들어오거나 할 때임. 이 문제는 인증으로 해결할 수 있을 것 같음. 예를 들어 의료 면허증(혹은 병원 인증) 같은 걸 제시하면 사전 동의 없이 걸려오는 전화가 허용됨. 그리고 사용자가 카테고리별로, 의료/금융/공공 등 분야별 인증된 발신자만 허용할 수 있도록 설정할 수 있으면 좋겠음
          + 전화라는 개념이 원래 존재하지 않았다면 어떨지 상상해봤음. 주머니에 컴퓨터는 있지만 전화 시스템의 역사가 없는 상태임. 그러다 어느 날 개발자가 ""다른 사람이 모바일 기기에 짧은 숫자 코드만 입력하면 원격으로 사용자의 작업을 중단시키고 알림음과 진동을 울리게 하며, 전체 화면 팝업까지 띄워서 버튼만 누르면 상대방이 내 오디오 데이터를 전송하고 마이크를 활성화할 수 있게 한다""는 앱을 만들었다면, 이건 거의 악성코드로 분류됐을 거임. 그런데 우리 모두가 지금 이걸 ""전화""라는 이름으로 너무 당연하게 여기고 있음. 단지 유산이라는 이유로
          + 나는 전화를 걸 때 소액의 예치금을 걸도록 하는 방식을 제안함. 대부분은 하루 내로 반환되지만, 수신자가 화가 나면 예치금을 몰수할 수 있음. 이런 방식은 여러 상황에서 효과적일 것임: 1) 친구나 정상적인 거래 관계에서는 변화가 없음. 2) 한쪽이 부정하게 예치금을 계속 몰수하면 관계를 끊는 계기가 됨. 3) 스팸 발신자는 추가 비용을 감수하거나 정말 필요한 사람만 연락해야 하게 됨. 물론 누군가가 거짓 번호를 적어서 애꿎은 발신자만 손해보게 하는 등 허점은 있지만, 지금보다는 나은 모델일 것 같음
     * Bandwidth.com, Neutral Tandem, 그리고 Sinch(예: Inteliquent) 관련 voice over Internet 플랫폼 몇 군데에서 인간이 직접 거는 전화뿐 아니라 자동화된 문자 및 스팸 전화가 끊임없이 옴. 이들 대부분은 FTC에서 중단 명령을 받았지만 아무 소용이 없었음. 나는 이 업체들에 벌금을 부과하거나 폐업시키고, 경영진은 구속해야 한다고 생각함. 관련 정보는 여기에서 볼 수 있음
          + 어떤 VOIP 제공자가 전화의 출처인지 어떻게 알 수 있는지 궁금함
          + 이 플랫폼들은 다양한 업체들이 그 위에 또 여러 서비스를 얹어서 운영하는 구조임. 모든 악용을 막는 게 마냥 쉽지 않으며, 규제가 강화돼도 계속 쫓고 쫓기는 게임임
          + Onvoy VOIP(지금은 Sinch) 번호로 사기꾼이 정말 많이 활동함. 이런 규칙들도 별 효과 없음
          + 참고로 Google voice도 bandwidth.com을 기반으로 운영됨
          + 유럽 국가들도 이런 처벌을 하길래 그쪽에는 robocall이 아예 없는 건지 궁금함
     * Pixel 폰을 처음부터 지금까지, 현재는 Pixel 9까지 사용 중임. Pixel의 스팸 차단 기능은 정말 탁월함. Google Assistant와의 연동도 특히 유용함. 스팸일 것 같은 번호가 오면 어시스턴트가 대신 받아서 상대 시간을 잠깐만 잡아먹고 버릴 수 있음. 뭐라고 말하는지도 미리 확인 가능해서 진짜 중요한 병원 같은 전화라면 응답할 수 있음. 아직도 모르는 번호 비율은 스팸이 더 많긴 하지만 주당 몇 번 정도만 걸려와서 나에게는 거의 신경이 쓰이지 않기 때문에 방해 요인이 됨을 체감하지 않음. 할머니도 Pixel을 쓰면 너무 좋을 것 같아서 추천했지만 아직 바꾸질 않으심. 집에서 한 시간에 몇 번씩 전화 올 때마다 놀람
     * PSTN(공중 전화망)은 더 이상 지속 가능하지 않음. 과거에는 발신 인증이나 검증에 대한 방법이 없어 신뢰할 수 있던 시스템이었지만, 지금은 악의적 행위자가 거의 공짜로 전 세계에서 전화를 걸 수 있어서 아예 구조적으로 대응 자체가 어려움. STIR/SHAKEN 같은 대책이 있긴 하지만 대증요법일 뿐, 이 네트워크 자체가 믿음이나 복원력을 고려하지 않은 채 설계된 한계임. 여전히 PSTN의 전방위성을 옹호하는 사람들이 있지만, 신뢰가 무너지면 시스템의 현실적 유용성도 무의미해짐. 실제로 인식 못 하는 번호는 다들 안 받게 된 지 오래라 그만큼 실효성이 사라짐
          + 이건 Chesterton’s fence 이론의 한 사례임. 즉, ""공익에 대한 설명을 못 하면 없애질 말라""는 원칙임. PSTN의 장점은 통신사 간 완벽한 상호 연동이 보장된다는 점임. Instant messaging(메신저)처럼 벽이 쳐진 서비스가 되어버릴 수 있음. 그래서 지금 Android에서도 iPhone에 전화를 걸 수 있다는 사실을 당연하게 생각하면 안 됨. FCC는 이 믿음 체계를 유지하는 기관이고, 그들은 지금까지도 충분히 역할을 해옴. 통신사업자들이 스팸 활동을 줄이도록 인센티브를 주면 되지, 아예 전체 시스템을 폐기할 필요는 없음
     * 최근 이틀간 내게도 스팸 전화가 쏟아지고 있음. 게다가 지금 이 댓글을 쓰는 동안에도 한 통 더 옴. 모두 내가 15년 넘게 떠난 지역(내 지역번호)에서 걸려옴. 음성 사서함도 없이 아무 메시지도 없음. 선거철보다 더 심함. 혹시 아직 FCC의 Do Not Call 리스트에 가입하지 않았다면 여기에 등록 가능함. 모든 문제를 해결하지는 않지만 적어도 합법적인 업체의 전화는 불법이 됨. 실제로 몇 년 전 이 리스트에 등록했을 때 스팸 전화가 상당히 감소하는 효과를 체감함. 참고로 정크 메일 수신 거부 정보도 있음. 비용은 6달러이고, 10년간 유효함
          + Do Not Call 리스트에 등록하면 오히려 ""나한테 스팸 전화해달라""는 리스트가 된 것 같은 느낌임. 초반에는 업무를 잘 처리했지만, 지금은 별 효력이 없다고 봄. SHAKEN/STIR 적용 직후 한동안은 스푸핑 전화가 아예 0이었고, 로보콜도 드물었음. 즉, 입법적으로 해결 가능하다는 방증임. 하지만 얼마 안 가서 다시 증가했고, 지금은 거의 모든 전화가 번호 도용임. 진짜 연락도 포함임. 그리고 합법적인 발신자도 자신들이 쓰는 자동 다이얼러 회사가 몰래 사기에도 가담해서 스팸 블랙리스트에 올라가는 경우가 많음. 정치인들이 강력 대처를 원치 않는 이유는 그들(혹은 그 대리인)도 이 시스템을 활용하기 때문일 것 같음
          + 동일한 상황임. 아직 구현은 안 했지만, 지역번호(혹은 주 전체)에서 내 연락처에 없는 번호는 VOIP로 무조건 차단하는 앱을 개발해볼 생각임
          + Android에 SpamBlocker라는 앱을 쓰고 있음. 내 지역번호에서 오는 전화는 정규표현식으로 다 막도록 했음. 강력 추천함
          + 오늘은 10건의 스팸 전화를 받았음. 반시간 간격으로, 다양한 번호와 지역에서 옴. 평생 이렇게 많이 받은 적은 처음임
          + ""Do Not Call"" 등록의 현실은 ""인도에서 버너폰으로 전화해라""는 의미였음. 수년간 사기꾼과 싸워왔는데, 로컬 비즈니스가 마케팅 대가로 돈을 내면(아마도 본인도 모르게) 인도의 콜센터로 송금되어 하루 종일 무작위로 전화를 뿌림. 메디케어 업그레이드, 장례보험, 주택개조 등 합법적인 상품을 미끼로 한 합법-비합법 중간 영역임. ""이 상품에 동의하면 DNC 리스트에 있어도 연락 받음 자체에 동의한다""는 문구가 숨어 있어서, 콜을 받으면 미국 법상 고소 불가임. 사기 전화가 명백히 불법인데, 해외에서 와서 손도 못 대는 게 현실임. 꼼수로 법망을 피하는 대표적 방식임
     * 이런 스팸 전화 업체들을 운영하는 사람들은 반드시 구속해야 한다고 생각함
          + ""우리""가 안 속는다고 해도, 이런 사기 전화가 노인과 취약 계층에게는 엄청난 해악임을 잊지 말아야 함. 특히 ""재택근무로 연 12만 5천 달러"" 같은 문자 사기는 절박한 사람들을 직접적으로 노림
          + 1200개 SS7 서킷 차단 따위는 아무 의미 없음. 이런 사람들을 구속하지 않으면 금방 다른 가짜 신분으로 또 다른 회선을 뚫어서 시작할 것임. 1200개 ASN 차단하고 효과를 기대하는 것과 같음
          + 이런 업체들은 DoJ(미 법무부) 관할권에 속하지 않는 경우가 대부분임
          + 이런 사기 전화회사들이 Google My Business에 사업자 등록까지 했는지 궁금함
          + 유럽 국가들은 진짜 그렇게 해서 robocall이 없는 건지 궁금함
     * 스팸 전화가 1년 전쯤부터 10배는 늘어났음. 너무 심해서 연락처 외의 모든 전화는 바로 음성 사서함으로 보내는 식으로 폰 사용 습관을 바꿀 수밖에 없었음. 여러 robocall 차단 앱도 써봤지만, 대부분 연결에 문제가 생김. 혹시 같은 경험이 있다면 어떤 방식으로 robocall을 차단하는지 궁금함
          + 난 모든 미지의 번호는 아예 안 받게 됨. 연락처만 정상 연결되고, 나머지는 메시지를 남겨야만 나중에 확인함. 물론 이로 인해 비즈니스를 잃기도 했지만, 내 생산성과 정신 건강에는 이 방법 말고 답이 없음. 이 사실을 안내 음성으로 명확히 알리고, 연락이 꼭 필요하면 문자 메시지로 달라고 권유함. 아직까지 robocall 쪽 소프트웨어는 이 점을 탁월하게 파악하지 못하는데, 추후 AI 등장으로 더 악화될 수 있을 것 같음
          + Pixel폰은 스팸전화 차단이 탁월함. 실제로 나는 어머니에게 아이폰을 쓸 수 없게 막았음. 아이폰 쓰는 SO는 하루에 3-4통씩 스팸 전화가 오는데, 나는 한 주에 한두 번 걸러들어오는 게 전부고, 그마저도 봇으로 미리 확인할 수 있음
          + Pixel폰이 스팸 호출 차단에 있어 압도적으로 차이가 큼. 같은 통신사 쓰는데 아내의 아이폰과 비교해 밤과 낮임. 불편함 자체가 없을 수준임
          + 나도 몇 년 전에 똑같은 경험을 했음. iOS 설정으로 연락처 외의 번호는 사서함으로 보내고, 한 달에 한 번 robocall 메시지를 삭제함
          + 그냥 연락처에 없는 전화는 안 받기로 함. 중요한거면 문자나 음성메시지 남길 테니 걱정 안 함
     * 앞으로도 계속 이런 조치가 이루어지길 바라는 마음임. 뭔가 대세에 영향을 주지 못한다고 생각할 수도 있지만, 바다도 결국 하나하나의 물방울이 모여 탄생한다고 생각함. 인터넷 관련 법을 실제로 집행하는 첫 단계가 되길 바람
     * 전화망을 훨씬 더 ""신뢰성 있는"" 시스템으로 바꾸는 것에 전적으로 동의함. 예전 SSL 인증서처럼, 적어도 무엇인가 실체 확인이 필요하고, 그걸 벗어난 번호는 발신이 제한되는 구조가 필요함. 다시 말해, 미국 기반 사업자라면 진짜 번호임을 인증하고, 그 신분과 연결된 라이선스와 함께 공개적으로 서명해서 통신망에 참여해야 한다고 생각함. 그리고 사용자가 (A) 특정 국가에서 온 전화만 받기, (B) 인증되지 않은/스푸핑된 번호는 아예 차단 등 자유롭게 설정할 수 있어야 함. 참고로 STIR/SHAKEN 같은 정책이 일부 존재하지만, 이런 방향으로 완전 채택되어야 내가 원하는 방식대로 설정할 수 있다고 봄. 여기에 스팸 신고 버튼을 UI에 추가하고, 반복해서 신고되는 번호는 통신사업자 차원에서 제재(금전적 벌금, 라이선스 취소 등)까지 가능해야 진짜 효과가 있을
       것임. 30년 전만 해도 ""익명 전화차단""은 너무 강경한 규제로 여겨질 수 있었지만, 지금은 인터넷이 있어 다양한 우회 수단(익명 제보 등)을 충분히 제공함. 그러니 스팸만 막을 수 있다면 어느 정도 규제 강화도 찬성임. 급박한 정보(차 수리 완료, 응급 연락 등)를 위한 유용한 채널로서 전화 기능이 완전히 망가지고 있기 때문임
          + ""익명제보-언론"" 같은 용례를 식상하다고 치부하는 건 설득력에 금이 감. FCC도 충분히 강경하게 대처할 수 있었음. 이건 규제 테마에 대한 정치 문제에 가깝고, PSTN의 장점을 잃지 않고도 해결 가능한 일임
          + ""미국 사업자가 실제 번호임을 라이선스와 함께 인증해야 한다""는 주장에 대해, 사실 전화번호 자체가 실체가 없음. IP주소처럼 임의로 할당되는 것이라, VOIP 시스템에서는 번호란 그냥 자신이 가진 감정에서 뽑아 쓸 뿐임
     * 호주에서는 상황이 매우 달라짐. 예전에는 어머니가 하루에 20통씩 스팸 전화를 받아서 결국 유선전화까지 해지할 정도였음. 사기꾼들이 은퇴촌 번호 대역을 알아내 계속 집요하게 전화함. 그런데 최근 몇 달 동안은 이상할 정도로 스팸 전화가 완전히 사라져서, 그냥 조용함이 익숙해졌음. ACMA(호주 통신 및 미디어 감독기관)가 수년간 목소리를 키웠지만 별 성과가 없다고 생각했는데 어느새 4개월째 스팸 한 통 없이 지내게 됨. 언론 보도, 큰 홍보도 없이 그냥 조용히, 효과적으로 정책을 집행함. 호주 국민으로서 자랑스럽게 생각함. 진짜로 훌륭하게 대처함
          + 스팸 전화는 반드시 다시 돌아올 것이며, 심지어 더 늘어날 수도 있음. 여전히 내 호주 휴대폰으로는 종종 스팸 전화가 옴
          + 정책적으로 어떻게 이런 성과를 냈는지 자세히 궁금함
     * 나는 1년 전부터 스팸 전화가 예전보다 10배는 늘어서 연락처에 없는 번호는 다 음성 사서함으로 돌리기 시작함. 여러 robocall 차단 앱도 썼는데, 대부분 연결에 문제가 생겨서 불편함. 혹시 같은 고민이 있다면 robocall 막는 방법을 알려줬으면 좋겠음
"
"https://news.hada.io/topic?id=22676","코드 포매팅 기능이 실험적으로 uv에 도입됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        코드 포매팅 기능이 실험적으로 uv에 도입됨

     * 새로운 uv 버전에서 코드 포매팅 기능을 실험적으로 제공함
     * uv format 명령어는 Ruff의 포매터를 내부적으로 사용하여 Python 코드를 일관되게 스타일링함
     * 기존에 별도의 도구 없이 uv만으로 간편하게 코드 정리 작업 가능함
     * 사용자는 추가 인자를 통해 형식 지정 동작을 세부 조정할 수 있음
     * 아직 실험적인 기능이므로 명령 방식, 에러 처리 등에서 변화 가능성이 존재함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   uv의 최신 릴리즈(0.8.13)는 Python 개발자가 오랫동안 기다렸던 실험적 명령어인 uv format 기능을 도입함. 이 기능을 통해 프로젝트 내에서 별도의 포매팅 도구를 추가로 관리하지 않고도 uv 도구만으로 코드 스타일 정리를 수행할 수 있음

uv format이란?

     * uv format 명령어는 uv 인터페이스를 통해 Python 코드 포매팅을 제공함
     * 내부적으로는 Ruff 포매터를 호출하여 코드를 자동으로 일관성 있게 정리함

개발자 참고 사항

   Charlie Marsh(uv 개발자)는 Hacker News에서 다음과 같이 설명함

     Ruff와 uv는 병합되는 것이 아니며, 여전히 별개의 도구임
     단순히 사용자가 포매터를 별도의 도구로 인식하지 않고도 이용하도록 경험을 향상시키는 목적임
     Rust 생태계의 cargo fmt와 rustfmt 관계와 유사함

사용 방법

     * uv 0.8.13 이상의 버전을 사용해야 함
     * uv format 명령어를 프로젝트 루트에서 실행하면 ruff format을 수행하는 효과가 있음
     * 실행 방식은 uv의 명령어 인터페이스를 따름

추가 인자 전달

     * uv format -- [추가 인자] 형태로 Ruff에 전달할 세부 옵션을 설정할 수 있음
     * uv의 편의성과 Ruff의 세밀한 설정을 동시에 활용 가능함

실험 단계 안내

     * 현재 기능은 실험적인 단계로, 향후 명령 방식이나 프로젝트 구조 통합 방식이 달라질 수 있음
     * 에러 처리, 출력 형식 등도 지속적으로 개선 예정임
     * 사용자 피드백을 반영하여 기능이 진화할 예정임

마무리

     * Python 프로젝트에 간편하고 일관성 있는 코드 스타일링이 필요한 경우 uv format을 적극적으로 시도해볼 수 있음
     * 실험적 도입인 만큼, 직접 사용 후 피드백을 제공하면 향후 uv의 발전에 기여할 수 있음

        Hacker News 의견

     * ruff가 ty와 합쳐지면 더 좋을 것 같음, uv는 패키지나 프로젝트 관리에 집중하는 게 맞고 코드 스타일 편집까지 관여하면 안 된다는 생각임, uv가 코드 파일을 수정하는 유일한 경우는 의존성 업데이트(PEP 723)일 때뿐이어야 한다고 봄
          + ruff와 uv는 합쳐지는 것이 아니고, 계속 별도 도구로 남는다는 점을 명확히 하고 싶음, 이는 포매터를 따로 신경쓰고 싶지 않은 사용자를 위한 더 단순한 경험 제공 목적임, Rust의 Cargo처럼 cargo fmt가 내부적으로 rustfmt를 실행하는 것과 유사한 구성임
          + Rust의 cargo에서 cargo fmt가 있는 방식처럼 흉내 내는 것임
          + 본질적으로 uv를 완성형 Python 패키지 매니저로 만드는 것이 목표이고, 각 부분 도구는 필요하면 개별적으로도 쓸 수 있음, 즉 uv는 Python을 위한 cargo 같은 존재이고, 빠른 타입체커만 필요하면 ty만, 포매터/린터만 필요하면 ruff만 선택해서 사용할 수 있어야 한다는 점에서 ruff와 ty를 합치는 건 그다지 의미가 없어 보임
          + 만약 언젠가 ty도 uv에 합쳐지면 어떨지도 궁금함, 모두 astral.sh에서 나오는 만큼 이것이 비전일 수도 있지만, 아직 ty는 준비가 덜 된 상태임
          + 다음 단계로는 uv lint 같은 옵션을 도입해서 내부적으로 ty를 실행하게 만드는 것이 논리적인 진화라 생각함, 이상적으로는 하나의 표준 명령어나 일련의 커맨드로 파이썬 프로젝트를 준비(포맷, 린트, 테스트, 배포)할 수 있게 되면 좋겠음, 어쩌면 이게 여기에 담긴 비전일 것 같음
     * uv를 정말 즐겨 쓰고 있지만, 불필요하게 점점 비대해지는 것 같아 조금 걱정임, 예를 들면 여러 서브 커맨드가 너무나 많은 독특한 플래그를 지원하는데, 몇몇은 거의 같은 결과를 내기도 함(uv run --no-project와 uv run --active 등), 쓸데없이 새로운 기능을 추가하기보다는 기존 도구와 문서 개선에 더 집중해 주길 바라는 마음임
          + Python 프로젝트를 안정적이고 재현 가능하며 이식성 있게 만드는 것은 정말 어려운 작업임, uv sync는 이론적으로 다시 재현 가능한 패키지 셋만 빌드해서 효용이 크지만, torch-tensorrt나 flash-attn처럼 복잡한 패키지는 환경에 따라 달라질 수밖에 없음, Python 커뮤니티는 종종 ""내 컴퓨터에선 잘 된다""라는 식으로 문제를 개인화하는 경향이 있지만, 소프트웨어를 배포, 보안, 반복 가능성, 신뢰성 있게 만드는 데 드는 비용은 결코 사라지지 않고, 결국 누군가가 나중에 더 제한된 상황에서 그 비용을 치르게 됨, 이러한 다양한 사용자와 운영 요구를 모두 만족시키려고 애쓰는 것은 정말 어려운 일임
          + uv에 서브 커맨드를 추가하는 것이 왜 비대화로 여겨지는지 잘 모르겠음, 이미 uv는 복잡한 도구이고 문서화도 잘 되어 있음, 이렇게 직관적이고 자기 설명이 되는 커맨드라면 충분히 자연스럽게 추가될 수 있다고 생각함
          + uv에 대해 이야기할 때, 마치 ""make 커맨드에 타겟이 너무 많다""고 말하는 것과 비슷하게 느껴짐
          + 이 옵션들이 메인 실행 파일에 내장되는지, 아니면 apt나 cargo처럼 별도 바이너리로 동작하는지 궁금함
     * 이 업데이트는 확실히 좋은 선택이라 생각함, 왜 많은 사람들이 더 나은 방향에 반대하는지 잘 모르겠음, 물론 ""조금 더 불편한 방법으로 이미 할 수 있다""라는 건 맞지만, 그건 '조금 더 불편한' 것임
          + ""uvx ruff format""이 한 단어 더 긴 게 나쁜 건 아닌 것 같음, 오히려 실제로 실행되는 포매터가 불명확해지거나, ruff가 자동으로 설치되는 건지, 기존처럼 툴을 다운받아 캐시하는 건지가 더 헷갈릴 수 있음
          + 강하게 공감하는 부분임, 심지어 pyproject에서 포매터를 설정할 수 있게 만들어주면 더 좋겠음
          + 가장 큰 불만은 현재로서는 다른 포매터 지원이 없어 보인다는 점임, 내 프로젝트에서 black을 쓰면 uv format이 동작하지 않음
     * 개인적으로 이 변경으로 내 소규모 팀(계리사들이 주요 멤버)의 코드 포매팅이 획기적으로 쉬워질 것 같아서 기대가 큼, uv가 파이썬 도입과 온보딩에 끼친 영향도 꽤 컸기 때문에, 코드품질을 더 쉽게 끌어올릴 방법은 언제나 환영임, 물론 ruff만 따로 쓰거나 pre-commit 구성을 할 수도 있지만, uv <기능>이라는 단순한 멘탈 모델이 팀에게 큰 도움이 됨, 다른 포매터와의 연동도 가능했으면 좋겠고, 만약 SQL/dbt 모델 포매팅까지 지원된다면 더 바랄 게 없을 듯함, 우선 써보면서 가능성을 확인해 볼 생각임
          + 그 정도로 다중 포매팅이 필요하다면 Makefile이나 justfile 같은 걸 쓰는 게 더 나을 수도 있음, 그러면 just format으로 Python/SQL/Bash/TypeScript까지 한번에 포매팅할 수 있음
     * 약간 기능 과다로 느껴짐, 1년 넘게 uv를 점점 더 많이 사용하고 있고 장점도 알겠지만, 아직까지 내 최우선 선택지는 아니고 이런 변화가 선호도를 높여주진 않을 것 같음
          + 구체적으로 이 방식이 뭐가 문제인지 궁금함, Go, Rust, Elixir 모두 이런 방법을 채택 중이고, 해당 언어 생태계에서 프로젝트 설정과 사용을 훨씬 더 수월하게 해줌, 커뮤니티가 공통 툴셋에 집중하고, 초보자와 전문가 모두에게 일관된 진입점을 제공하는 장점이 있음
          + 혹시 그렇다면 어떤 도구를 제일 선호하는지 궁금함
     * 언젠가는 ruff의 기능이 uv와 ty에 통합될 것 같음, 린팅은 코드베이스를 더 잘 이해하는 ty가 담당하면 더 똑똑해질 수 있고, 포매팅은 프로젝트 관리가 주목적인 uv가 담당하는 것이 적합함
          + ty가 이미 ruff와 같은 저장소에 있으니, 통합도 그리 먼 얘기는 아닌 듯함
     * 패키지 매니저는 운영 환경용 패키지 설치에 필수지만, 개발 전용 도구와 섞이는 건 일종의 '매력적이지만 위험한 덫' 같은 느낌임, 물론 Go와 Rust도 그렇게 하고 있지만, 근본적으로 생각해 보면 그리 좋은 구조는 아닌 것 같음
          + 정말 안 좋게 들릴 수도 있지만 cargo를 많이 써본 입장에서는 이런 '안 좋은 아이디어'가 더 많아졌으면 좋겠음, 만약 uv가 Python에서 cargo 같은 역할을 하게 된다면 Python 개발 경험이 엄청나게 좋아질 것임, 25년 넘게 Python을 쓰면서 여러 부족한 부분을 돌파해 온 입장에서 이제는 별다른 고민 없이 uv 하나로 가능해지는 것이 매우 만족스러움
     * 새로운 uv format은 사실상 uv run --with ruff ruff의 단축키임
     * 이 흐름이 정말 맘에 듦, 만약 내 의도대로라면 uv fmt라고 이름 짓고, uv vet 같은 것도 로드맵에 추가하는 게 좋을 것 같음
     * 이미 검증된 코드 포매터 도구들이 많은데 굳이 이걸 도입할 이유는 전혀 느끼지 못함, 기능만 늘어난 느낌이어서 당분간 어떤 파이프라인에도 포함하지 않을 생각임
          + uv format은 그냥 ruff format의 프론트엔드에 가까움, 새로운 포매터가 추가되는 것도 아님
          + 이미 많은 사람들이 사용하는 ruff format을 손쉽게 쓰도록 해주는 단축키에 불과하다는 점을 알고 있으면 좋겠음
"
"https://news.hada.io/topic?id=22693","파이썬의 현주소: 2025년 개발자 설문조사 주요 결과","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     파이썬의 현주소: 2025년 개발자 설문조사 주요 결과

    파이썬의 현주소: 2025년 개발자 설문조사 주요 결과

   JetBrains에서 발표한 '2025년 파이썬 현황' 보고서에 따르면, 파이썬은 여전히 개발자들 사이에서 압도적인 인기를 누리고 있으며, 특히 데이터 과학 분야에서의 강세가 두드러졌습니다. 또한, 웹 개발 분야에서의 재부상과 비동기 처리 및 Rust 기반 도구의 성장이 눈에 띕니다.

   주요 내용 요약:
     * 압도적인 주 언어 사용률: 응답자의 86%가 파이썬을 주 프로그래밍 언어로 사용하고 있습니다.
     * 신규 프로그래머의 높은 비율: 응답자의 절반이 2년 미만의 코딩 경력을 가지고 있어, 파이썬이 초심자에게 매력적인 언어임을 보여줍니다.
     * 데이터 과학 분야의 지배력: 데이터 과학 및 관련 분야가 파이썬 사용의 51%를 차지하며, 이 분야에서의 선도적인 위치를 확고히 했습니다.
     * 여전히 널리 사용되는 이전 버전: 최신 버전의 성능 향상에도 불구하고, 개발자의 83%는 1년 이상 된 파이썬 버전을 사용하고 있습니다.
     * 웹 개발의 부활: 한동안 감소세를 보였던 파이썬의 웹 개발 사용률이 46%로 크게 증가했으며, 특히 FastAPI의 채택이 급증했습니다.
     * 비동기 및 Rust 기반 서버로의 전환: 비동기 프레임워크의 성장 추세에 따라 프로덕션 웹 서버가 비동기 및 Rust 기반 도구로 전환되고 있습니다.
     * 성능 향상을 위한 Rust의 부상: 파이썬 패키지의 고성능 바이너리 확장을 위해 Rust를 사용하는 비율이 27%에서 33%로 증가했습니다.
     * 타입 힌트(Typed Python)를 위한 도구 개선: Rust로 작성된 새로운 고성능 타입 검사 도구가 등장하여 타입 힌트 사용이 더욱 쉬워졌습니다.
     * 비동기 및 스레딩의 핵심화: 자유 스레드 파이썬으로의 전환과 async 및 await 키워드의 사용 증가는 동시성과 스레딩을 언어의 핵심으로 만들고 있습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    저자가 강조하는 중요한 점

   이번 보고서에서 저자는 몇 가지 중요한 변화의 흐름을 강조했습니다.

   첫째, 파이썬의 사용자 기반이 매우 견고하다는 점입니다. 대다수의 개발자가 파이썬을 주력 언어로 사용하고 있다는 것은 파이썬 생태계가 그만큼 안정적이고 활성화되어 있다는 것을 의미합니다.

   둘째, 신규 개발자들의 유입이 꾸준하다는 점입니다. 이는 파이썬이 배우기 쉽고 접근성이 좋은 언어라는 인식을 다시 한번 확인시켜 주며, 교육 자료와 커뮤니티의 중요성을 강조합니다.

   셋째, 데이터 과학 분야에서의 독보적인 위치입니다. 파이썬은 이제 데이터 과학의 표준 언어로 자리 잡았으며, 관련 라이브러리와 프레임워크의 발전이 이를 더욱 가속화하고 있습니다.

   넷째, 웹 개발 분야에서의 주목할 만한 부활입니다. 특히 FastAPI와 같은 현대적인 비동기 프레임워크의 등장은 파이썬이 웹 개발에서도 다시 한번 경쟁력을 갖추게 된 중요한 계기가 되었습니다.

   마지막으로, 성능 향상을 위한 노력입니다. Rust와의 통합, 비동기 처리의 강화, 그리고 타입 힌트 시스템의 발전은 파이썬이 단순한 스크립트 언어를 넘어 고성능을 요구하는 애플리케이션 개발에도 적극적으로 사용될 수 있는 가능성을 보여줍니다. 이는 파이썬의 미래가 더욱 밝다는 것을 시사합니다.
"
"https://news.hada.io/topic?id=22689","모든 것은 서로 상관관계가 있음 (2014–23)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      모든 것은 서로 상관관계가 있음 (2014–23)

     * 사회과학 및 생명과학에서는 거의 모든 변수 간에 일정 수준의 상관관계가 존재함
     * 이 현상은 단순한 우연이나 통계적 오류가 아니라, 복잡하게 얽힌 유전적·환경적 요인에서 비롯된 실제적 사실임
     * 표본 수가 커질수록, 유의한 상관관계는 대부분의 변수 쌍에서 나타나며, 연구자는 개별 상관관계보다 상관 패턴 자체에 주목하게 됨
     * ‘Crud factor’는 거의 모든 변수쌍에서 작은 상관관계가 존재함을 의미하며, 임의의 이론 및 변수쌍 선정만으로도 높은 확률로 유의미한 결과가 나옴
     * 이러한 상황에서는 전통적인 유의수준(0.05) 이 갖는 의미가 약화되며, 사회과학 통계 해석 시 주의를 요구함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 배경

     * 심리학과 사회학에서는 “모든 것이 어느 정도 서로 상관관계를 가진다” 는 주장이 널리 받아들여짐
     * 특정 특성이 여러 유전적 및 환경적 요인에 의해 결정되고, 이 요인들 간에도 자체적으로 상관관계가 존재함
     * 따라서 실질적으로 거의 모든 측정 가능한 변수는 어느 정도 상호 관련성이 존재함

“Crud Factor”와 통계적 발견

     * “Crud factor” 란, 사회과학(그리고 일부 생명과학) 연구에서 임의의 변수 쌍 간에도 작은 상관관계가 항상 존재하는 현상을 지칭함
     * 1966년 미네소타 고등학생 57,000명을 대상으로 실시한 대규모 데이터에서, 가족, 교육, 취미 활동, 진로, 종교 등 다양한 변수들 간 105개의 교차표(crosstabulation) 분석 결과 모두 통계적으로 유의미함
          + 전체의 96%가 p<10⁻⁶ 수준의 극히 낮은 확률로 우연일 가능성을 배제함
     * 변수 수를 45개까지 넓히면 총 990개의 조합 중 92%가 통계적으로 유의하였음
          + 한 변수와 다른 모든 변수들 간 유의미한 관계의중앙값(median) 은 44개 중 41개임

실제 변수들 간 사례

     * MCAT 점수와 형제 수, 출생순서, 성별, 직업계획, 종교 선호 등과의 관계에서도 모두 높은 통계적 유의성이 발견됨
          + 예: 여자 학생이 남자보다 점수가 높음, 형제 수가 많을수록 점수 하락 경향, 첫째/외동이 막내보다 똑똑함, 종교 집단마다 뚜렷한 차이 등 다수 존재함
     * 대표적 개신교 5개 교파별로도 여러 변수와의 관계에서 높은 유의성이 관찰됨
          + 예: 외동이 Presbyterian일 확률이 Baptist의 거의 2배, 교파별 학교 호감도 및 직업 희망 차이 등 다수의 상관성

MMPI 문항 사례

     * MMPI(성격검사) 550개 문항 중 507개(92%)가 성별에 따른 유의미한 차이를 보임
          + 일부 문항은 뚜렷한 성향의 차이를 명료하게 설명할 수 있으나, 다른 항목들은 이유가 복합적 또는 설명 불가
     * 이러한 결과는 표본 수가 매우 많은 대규모 연구에서 나타나므로 통계적 오류(type I error)가 아니라 실질적 현상임

사회과학적 상관관계와 이론 검증의 한계

     * 임의의 이론과 변수쌍을 랜덤하게 조합하더라도, 평균 상관관계(crud factor)가 0.30 수준이라면, 실질적으로 세 번 중 한 번 꼴로 유의미한 차이 발견 가능
          + 통상 사회과학에서 의미 있게 바라보는 유의수준(0.05)보다 이런 현상이 훨씬 자주 발생
     * 연구자가 이론적으로 예측하지 않은 변수쌍에도 상관관계가 쉽게 드러나므로, 통계적 유의성만으로 실질적 인과관계를 뒷받침하기 어려움
     * 복합적 원인(유전자/환경) 및 관찰 데이터의 풍부함이 이런 다방면의 상관관계를 만듦

실천적 결론

     * 사회과학 데이터 해석 및 이론 검증 시, ‘crud factor’에 의한 “평범하지만 진짜 존재하는 상관관계” 를 항상 염두에 둘 필요가 있음
     * 유의수준 통계(예: p<0.05)의 의미를 맹신하기보다는, 변수 간 실질적 인과성과, 패턴 해석에 더욱 집중하는 접근이 필요함
     * “모든 좋은 것은 함께 오는 경향이 있다”라는 Thorndike의 격언처럼, 실제 세계에서는 너무 많은 것들이 서로 얽혀 있음

        Hacker News 의견

     * 내가 가장 거슬리는 점 중 하나에 대해 이야기함
       사람들은 ""통계적으로 유의미하다(statistically significant)""라는 표현을 ""주목할 만하다/의미가 있다""라는 뜻으로 오해함
       측정된 차이를 발견했고 통계가 이것이 '중요하다'고 말한다고 받아들이는 데, 이는 잘못된 방식임
       사실 유의성 검정은 관측된 차이가 '좋은 측정'일 확률만을 알려줌
       즉, 어느 정도의 신뢰 수준으로 ""차이가 실제로 존재함""이라고 말할 수 있는 것임
       측정된 차이가 가치판단적으로도 '의미 있는지'는 따로 판단해야 하며, 주로 측정된 차이의 크기에 따라 판단함
       너무 당연해 보이지만, 산업 현장이나 여러 과학 분야에서 매우 흔하게 나타나는 오류임
       예시로 ""이 조치가 [지표]에 p<0.001로 변화를 줬다, 엄청 유의하군! 변화 크기는 0.000001%다""
       이런 경우, 정말 '의미' 있는 건지 다시 생각할 필요가 있음
          + 지적한 대로 유의미(significant)가 곧 '의미 있음(meaningful)'을 뜻하지는 않음
            다만 예시에 대해서는 조금 부연하고 싶음
            아주 작은 p-value가 항상 '의미 있는' 효과를 의미하지는 않지만, 효과 크기와 무관하지도 않음
            p-value 자체가 (효과크기)/(노이즈/샘플 수의 제곱근)으로 나옴
            즉, 더 큰 검정 통계량은 더 작은 p-value를 의미함
            매우 작은 p-value는 주로 큰 효과나 엄청나게 큰 샘플 크기(n)에서 나옴
            그래서 극도로 큰 N일 때에만 미세한 효과로도 p<0.001이 나올 수 있음
            하지만 현실 연구에서, p<0.001이 나오면 샘플수 제한 때문에 효과도 실제로 클 확률이 높음
          + Using Effect Size—or Why the P Value Is Not Enough
            통계적 유의성은 결과 중 가장 흥미롭지 않은 부분이라고 하면서, 결과를 효과 크기로 설명해야 함을 강조함
            단순히 처치가 효과가 있는가만이 아니라, '얼마나' 효과가 있는지에 주목해야 함
            – Gene V. Glass
          + 완전히 동의함
            그런데 단순한 'pet peeve'로 부르기보다는, 통계에 대한 병적인 오해라고 봄
            이런 오해는, 특히 인기 건강/웰니스 미디어에서 잘못된 결과로 이어질 수 있음
            건강·영양 관련 연구가 통계적으로 유의미하다고 보고되지만, 실제 효과는 미미한 경우가 많음
            그래서 사람들은 이런 연구 결과만 믿고 삶과 습관을 크게 바꾸기도 하는데, 사실 그럴 근거가 없음
          + N(표본 수)을 충분히 올리면 이런 '좋은 측정값'이나 '통계적으로 유의미한 차이'를 어디서든 찾을 수 있음
            더 나쁜 것은, 사전에 검증할 가설을 정하지 않고, 과거 데이터를 뒤져서 '통계적으로 유의미한' 상관관계만 찾아내는 경우임
          + 3blue1brown의 이 영상을 정말 좋아함
            여기서 유의성을 확률을 업데이트하는 방식으로 생각해야 한다고 제안함
            한 번의 테스트(혹은 연구)가 확률을 X%만큼 업데이트해주므로, '의미' 있는 판단에는 대개 더 많은 실험이 필요하다는 논지임
     * 정말 전형적인 ""rationalist"" 스타일 글임
       통계 현상에 대한 올바른 관찰이 양념처럼 들어있는 한켠엔 이상한 정치적 구문도 있음
       예시 문장: ""이론 및 경험적 고려는 '알고리즘 편향'이나 '보호 대상 집단'에 대한 인과 추론 의심을 불러일으킨다: 배제하지 않는 게 바람직하지 않을 수 있고, 불가능하거나 의미 없을 수도 있다""
       너무 이상한 문장임, 맥락 설명도 없이 갑자기 튀어나옴
       숨은 잠재 변수가 범죄성을 결정하니까, black box(parole 모델)에 ""is_black""을 써도 된다고 주장하는 것 같은데, 말도 안 된다고 생각함
       사실 모델이 어떻게 동작하는지에 대한 관심은 통계 해석보다 더 깊은 문제임
       모델 선택 과정에서 자유도가 너무 많아지면, 어떤 결과든 나오도록 설계할 수 있음
       예시로, parole 모델에 ""likes_hiphop"" 같은 변수가 들어가 있다면, 이게 왜 들어갔는지, 정말 '최적 모델'이었는지 확인해야 함
       결국 사회 현상에서 변수들끼리 상관관계가 많다는 사실은, 어떤 모델이든 최소한 부분적으로는 정치적인 산물일 수 있음을 상기하게 함
          + 이상하다고 느낀 문장이 맥락상 크게 어색하지 않음
            ""이론적·경험적 고려""라는 구절은 위에서 나온 논의를 의미함
            즉, 모든 것이 서로 상관되어 있기 때문에, 상관관계를 본다고 해서 이게 본질적 의미가 있다고 확신할 수 없다는 이야기임
            사회과학자들은 복잡한 모델을 만들고, 많은 변수를 관찰하면서, 자신들의 가설을 뒷받침하는 상관관계를 찾지만, 이런 상관관계가 어디서나 발견될 수 있으므로 실제 증거로는 약하다고 지적함
            그리고 ""is_black"" 같은 변수를 모델이 실제로 썼다고 단정할 수도 없음
            단지 어떤 black box 모델이 흑인에게 불리한 결과를 내놓는다고 해서, 진짜 'is_black' 변수가 들어가 있다고 볼 수 없는 것임
          + ""rationalists""는 사람이나 집단의 서열 매기기에 집착하는 경향이 있다고 봄
            특히 유전, IQ 같은 주제에서 근거가 약한 연구와 데이터로 결론을 내리는 모습이 자주 보임
          + 해당 인용구에 대해
            사회과학 모델링에는 이론적 배경이 반드시 필요하다고 생각하지만, TFA(원글)가 특정 정치적 이슈에서도 같은 입장을 취할지 의문임
            예를 들어, 소수 집단을 위한 조직의 채용에서 ""is_white"" 변수를 쓰는 경우에도 똑같이 말할지 궁금함
          + gwern에 대한 평이 딱 맞다고 생각함
            본인이 스마트하다는 분위기를 내면서 근거 없는 추정을 사실처럼 던지는 스타일임
            특히 scaling/AI 커뮤니티에서 gwern을 유난히 좋아하는 것도 신기함
     * 글에서 은하수를 여행하는 히치하이커를 위한 안내서(The Hitchhiker's Guide to the Galaxy) 명언을 언급하지 않아 아쉬움
       ""우주에 있는 모든 물질은 서로 어떻게든 영향을 주기 때문에 이론적으로는 요정 케이크 한 조각만 봐도 우주 전체, 태양, 행성, 궤도, 사회적·경제적 역사까지 모두 추론이 가능하다""는 메시지가 떠오름
          + 이런 논리를 성립시키려면 우주의 T_zero(초기 상태) 구성이 필요하지 않을까 생각함
            서로 다른 T_zero 구성은 T_current(현재 상태)와 연결되고, 같은 물리적 구성이더라도 그 전의 ""우주-케이크"" 상태가 다를 수 있음
            또한 완전히 결정론적 체계만을 가정하는 것임
          + 불교에서는 '연기법(Pratītyasamutpāda)'이라는 개념이 있음
            관련 설명 링크
          + 입자는 숙명론에 시달리지 않음
     * 과거에는 통계 없이도 세상 진실을 밝혀냈었음
       통계가 생긴 뒤 유용한 도구가 된 것은 맞지만, 그런 방법이 남용되면서 멍청함이 똑똑함처럼 포장되는 문제도 늘었음
       그러니 이번 '상관관계 노이즈' 관찰도 질문해 볼 가치를 가짐
       무엇보다 논리, 도메인에 대한 기초 지식이 먼저임
       단순히 숫자만 세는 건 오해를 부를 수 있다 생각함
          + ""과거에는 통계 없이 살았다""는 말에, 그때가 훨씬 더 나빴음을 지적하고 싶음
            논리만으로는 새로운 지식을 배울 수 없음
            논리는 이미 아는 것을 다시 설명할 뿐이고, 기초 지식은 경험이나 실험이 필요함
            실제 세상 관찰은 항상 완벽하지 않기 때문에, 통계적 해석이 필수임
            통계 등장 전엔 (a) 부자들이 앉아서 세상에 대해 깊게 고민함, (b) 카리스마 있는 인물이 자신의 바람대로 설교함, (c) 똑똑한 사람이 가끔 맞히는 식이었음
            통계가 생김으로써, 누구든 결과에 근거하여 옳고 그름을 알 수 있게 되어, 기득권만의 영역이 아니게 됨
            물론 통계적 추론의 장점 중 하나는 '상호비교(intercomparison)'로, 과정 자체에 대한 이해가 없어도 차이에서 결론을 도출할 수 있다는 점임
            하지만 그렇기 때문에 결과 조작이나 오해도 쉬워짐
          + 조지 루카스가, 사회에 새로운 것이 들어오면 사람들은 항상 지나치게 그것을 남용한다고 말한 적 있음
            관련 영상
     * 주제와 다른 이야기지만, 이 블로그가 정말 아름답다고 느낌
       드롭캡(drop cap), 화면 오른쪽에 보이는 인라인 댓글, 진행 표시줄 등에서 프로젝트에 대한 애정이 느껴짐
          + gwern의 드롭캡 관련 글이 흥미로울 것 같음
     * 이 글은 정말 방대한 글임
       나도 저런 본격적인 글을 쓸 수 있으면 좋겠다는 바람이 생김
       저자의 다른 글들도 보면 정말 기계처럼 끊임없이 생산하는 것 같음
          + 기억이 맞다면 Gwern은 외진 곳에서 매우 검소하게 살며, 그래서 사적인 연구에 많은 시간을 쓸 수 있다고 함
          + 많은 시간, 반복, 어려운 질문에의 집착, 연구 및 Haskell에의 전문성이 저자의 비결임
            물론 누군가가 재정 후원을 해주면 더 좋음
          + 나는 저런 글을 읽을 수라도 있으면 좋겠음
          + gwern은 정말 대단하다고 생각함
     * 이런 논쟁은 몇십년째 있었던 것임
       비판적 시각을 잃지 않는 건 중요함
       하지만 개인적으로 일에서 이런 논리와 씨름해 볼수록, 실은 그리 유용하지 않고 비어있는 느낌을 받음
       'crud'는 통계적 우주 배경복사처럼 패턴 속에 존재하고, 무의미하다고 치부하기보단 가끔 중요할 때도 있음
       변수들의 연관이 쉽게 설명되지 않을 때가 있는가 하면, 조절해야 할 잠재적 혼란 변수에 대한 이해에 핵심이 될 때도 있음
       항상 상관관계가 있는 것은 아니며, 진짜로 연관이 0인 경우도 존재함
       '0이 아닌 의미 있는 효과 크기'를 정하는 것도 매우 임의적이고 주관적임
       이 현상을 바라보는 데 더욱 생산적인 틀이 있을 것 같음
     * Correlated. 다른 사례?<i>Everything Is Correlated</i> - 예전 논의
          + Correlated, 맞나요?
     * 그래서 실험 과학이 관찰 연구와 다른 이유임
       통계 분석은 한 가설에 더 신뢰를 두도록 할 이유만 주고, 진짜 실험적 접근이 보강되어야 함
       블로그의 예시는 대부분 의학·사회·행동과학 등에서 제대로 통제된 실험이 어렵거나, 표본 수 부족으로 인과관계를 명확히 밝히기 어려운 경우임
          + 미시경제학은 대규모 관찰 연구에서 실험 및 준실험 디자인 중심으로 진화했음
            물론 설계 상의 실패는 분석으로 고칠 수 없지만(You can’t fix by analysis what you bungled by design - 출처), 어느 정도는 편향을 줄이는 방향임
     * “이로 인해 유의성 검정의 의미가 모호해진다; 이는 a priori(선험적)로 거짓임을 아는 시나리오에서 데이터의 확률을 아주 정확하게 계산하는 것에 불과하다”는 글 인용에 대해
       의미 있는 결과에 도달할 때 모형이 단순화되어 있고, 엄밀히 말하면 거짓임을 인정하고 계산하는 것은 흔한 일이라고 생각함
       예를 들어 뉴턴의 법칙, 전기회로 해석도 단순화 덕에 가능하며, 은행권에서도 1년을 360일로 계산했었음
       실제로 잘 작동하는데 내가 뭘 놓친 건지 궁금함
          + 문제는 돈만 충분하다면(즉, 아주 큰 표본 N을 확보하면) 언제든 '유의미한' 결과를 만들어낼 수 있다는 점임
            이런 점은 연구를 진리 추구로 볼 때 심각한 딜레마임
          + 과거에 원리금상환 계산기를 만들 때, '일수 계산(day count)' 방식만도 47가지가 있었음
            (1개월 미만 구간의 지급액 산정 등)
          + 그런 단순화로 인해 생기는 최대 오차가 항상 알려져 있음
            다르게 말하면, 아인슈타인은 뉴턴의 정밀화 버전임
            특수상대성 이론이 저속 한계에서 뉴턴 운동으로 수렴하듯이
            실제로 통계에서 '거짓'은 존재하지 않고, ""x%의 확률로 사실이 아님""으로 해석해야 함
            x를 낮추고 싶으면 '더 열심히 통계'를 해야 하고, 샘플 수(N)를 늘리는 게 가장 확실함
            글에서 완전히 잘못하고 있는 점은, 충분히 큰 N이면 진실/거짓을 절대적으로 취급해도 된다는 것임
            왜냐하면 '우주가 백만번 만들어져도 한 번 나올까 말까'하는 확률 수준까지 가니까
            다만 현실에서는 사회과학, 의학, 경제학 등 대다수가 아주 작은 N에서 작업하므로 통계 문제가 필연적으로 큼
            그래서 '더 열심히 통계'를 하려고 하지만, 실상은 N을 늘리지 못하고 수치를 조작하거나, N을 조금만 늘린 뒤 문제를 해결했다고 주장함
          + 궁극적으로는 단순화로 인한 오차 크기가 실제로 얼마나 되는지, 그 정량적 규모가 관건임
"
"https://news.hada.io/topic?id=22675","미국 정부, Intel 지분 10% 확보","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         미국 정부, Intel 지분 10% 확보

     * 미국 정부는 Intel의 주식 10%를 인수했음
     * 이 조치는 Trump 대통령의 민간 부문에 대한 정부 영향력 확대 방침과 연결되는 정책임
     * 미국 내 반도체 기술과 핵심 산업에 대한 통제 강화 목적이 깔려 있음
     * Intel의 최대 주주 중 하나로 미국 정부가 등장하게 됨

정책 배경 및 목적

     * Trump 행정부는 최근 첨단 기술 분야에서 미국 우위 확보 필요성을 강조함
     * 미국 반도체 산업 보호 및 중국과 같은 경쟁국 견제 전략 추진 중임
     * 정부의 적극적인 지분 투자로 전략적 산업의 안정성과 공급망 주도권 강화 추구 의도임
     * Intel은 글로벌 반도체 시장에서 중요한 입지를 차지하고 있어, 지분 인수의 상징성이 큼

시장 및 업계 반응

     * 이번 정부의 지분 취득에 대해 민간 투자자 및 금융시장에 반향이 나타남
     * 일부에서는 정부 개입 증가에 따른 기업 자율성 저하 우려가 제기됨
     * 동시에, 국가 안보 및 기술 보호 관점에서 필요성을 인정하는 목소리도 있음

향후 전망

     * 미국 정부의 Intel 주요 주주 등극으로, 앞으로 회사의 경영·연구개발 전략에 영향 가능성 있음
     * 다른 첨단 기술 기업에도 비슷한 정부 개입 사례가 확산될 수 있음
     * 장기적으로 미국 공급망과 첨단산업 경쟁력 강화 효과 기대됨

결론

     * 미국 정부의 Intel 지분 10% 확보는 정부와 민간기업 관계 변화를 상징하는 중요한 조치임
     * 국가 전략 산업 주도권 확보와 기술 자립이라는 정책적 목표 아래, 향후 관련 정책 또한 강화 추세임

        Hacker News 의견

     * 나는 정부가 구제 금융을 제공하는 기업에 지분을 취득하는 방식이 더 낫다는 생각임, 과거 ""Too Big to Fail"" 구제 시 기업에 더 많은 비용을 부담시켰으면 하는 바람이 있었음, 그래서 이번에 이런 구조가 이루어지는 것 자체는 긍정적이라 생각함, 하지만 이런 일이 ""우리 대통령님이 미국을 구했다!"" 같은 정치적 미디어 이벤트가 아니라, 정형화된 엄격한 규칙과 프로세스에 따라 진행되어야 한다고 봄, 그래서 투자자와 기업 모두 예측 가능성을 확보할 수 있어야 함, 이런 일은 지루해야 하며 언론 쇼의 소재가 되어선 안 된다는 생각임
          + 나는 사실 이런 기업들에 대한 구제 금융 자체를 하지 않았으면 한다는 입장임, 이는 명백히 도덕적 해이를 초래하고, 오히려 훨씬 잘 운영되는 기업들이 해당 시장에 진입하기 어렵게 만듦
          + 정부가 과거 ""Too Big to Fail"" 중 가장 큰 구제 금융이었던 AIG 사례에서 79.9%의 지분을 취득했다는 점을 언급하고 싶음, 그 당시 구제된 기업의 오너들은 거의 전부 손실을 입었지만, 사람들은 경영진 보수(크게 줄지 않았던 점)만 기억한다고 생각함
          + 어떤 기업이 너무 커져서 정부가 구제 금융을 제공하는 상황까지 됐다면, 아예 아예 회사의 복지를 민간에 맡기는 것 자체가 문제라는 생각임, 이런 상황은 사실상 정부의 세금을 주식 매입을 통해 간접적으로 사적으로 이전하는 셈임, 주가 상승 효과를 누리므로 장기적으로 정부가 추가 매입 의지라도 보이면 주주들에게 이득이 생김, 국가 전체 복지에 중요한 영향을 미칠 정도로 큰 회사면 더 직접적이고 투명한 민주적 관리가 필요하다고 봄, 최소한 지금 Intel에 적용되는 것보다는 훨씬 투명한 구조가 되어야 함
          + 얘기와는 다르지만, 항상 아쉬운 점은 시와 지방 정부가 세금으로 스타디움을 보조할 때 스포츠팀의 소유권이나 의사결정권 일부를 갖지 못한다는 점임, 예를 들어 팀의 이전, 매각 시에는 지자체의 통제가 가능해야 한다고 생각함
          + Intel은 극히 소수의 자체 반도체 팹을 보유한 기업임을 감안하면, 구제 금융이 오히려 좋은 선택이라고 생각함, 팹은 매우 가치 있는 시설이기 때문에 최근 몇 년의 CPU 문제만으로 Intel이 파산하면 안 된다고 생각함, 계속 시도해야 한다고 봄
     * 왜 정부가 산업의 승자와 패자를 직접 가리려는지 궁금함, Intel은 은행도 아닌데 굳이 살려야 하는 이유가 의문임, Intel의 전성기는 지났고, 이미 25년은 늦은 것 아닌지, 그럼 이제 정부가 각 산업별 대표 기업마다 투자해서 균형을 맞출 건지에 대한 의문이 생김
          + 사실 승자와 패자는 Intel 단 한 곳으로 압축됨, 미국에 남은 유일한 주요 반도체 제조업체임, 국가 안보 관점에서 미국 정부는 중국이 대만을 침공할 경우 TSMC가 무력화된다고 예상하기 때문임, 이런 시나리오가 실제로 일어날지는 모르겠지만, 정부 입장에서는 그게 우려임
          + 유일하게 관용적으로 볼 수 있는 이유라면, 미국 내 반도체 생산 라인을 가진 것이 국가 안보 때문이라고 할 수 있음, 하지만 Intel의 지분을 연방 정부가 직접 가져야만 하는 건 아니라고 생각함, 예를 들어 미 국방산업체인 Lockheed Martin이나 Northrop Grumman도 군에 큰 의존을 하지만 정부가 실제 지분을 가진 건 아님
          + 현 정부의 역사를 보면 뚜렷한 근거를 기대하긴 어렵지만, 그래도 내 생각에 Intel을 살릴 이유는 x86 CPU 라이선스 보유사가 전 세계에 단 세 곳(Intel, AMD, VIA)뿐이기 때문임, Intel이 없으면 미국에서 x86 CPU 만드는 곳은 AMD 하나만 남게 되어서, 그나마 쓸만한 x86 CPU 독점이 생김, 그래서 솔직히 이 정책이 논리적 판단이라기보다는 행정부 실세를 위한 자금 유입 목적이 더 커 보임
          + 왜 Intel을 살려야 하냐는 질문에는, 미국에 남은 주요 반도체 제조업체가 Intel밖에 없다는 점을 들 수 있음, AMD는 이제 팹이 없고 TSMC가 글로벌 시장을 장악 중임, 만약 대만이 침공되면 미국은 고성능 CPU나 GPU 조달에 심각한 차질이 생기며, 이는 경제·군사 두 측면에서 모두 심각한 사안임, 단, 최근 CHIPS act로 TSMC가 애리조나에 팹을 짓긴 하지만, 규모는 아직 불확실함, TI 등 일부 미국 기업도 로엔드 반도체 제조는 하지만 x86, 하이엔드 ARM, GPU는 거의 전부 TSMC가 대만에서 제조함
          + 이 이야기는 결국 미국 군이 Intel에 의존하고 있다는 신호라고 생각함, 미국 내에 팹을 갖춘 유일한 대형 반도체 회사이자 x86 아키텍처의 창시자인 Intel이 없으면, 군대가 “중국 대만”의 칩에 의존하게 됨
     * 정부가 보통주 8.9억 달러 상당을 인수해서 4.3억 주를 주당 20.47달러에 매수해 10% 지분을 얻게 되었음, 그 중 57억 달러는 CHIPS Act에서 배정되었지만 아직 집행 전인 보조금으로, 32억 달러는 별도 보안 칩 개발 프로그램에서 나옴, 이 과정이 회계상 매우 흥미로움, 정부가 기존 승인된 보조금 수령을 조건으로 주식을 내놓게 한 건가 추측됨, 결국 Intel 입장에선 보조금의 실제 가치가 크게 의심된다면 가격 하락 압력이 없는 주식 발행이니 그리 나쁜 거래는 아닌 셈임
          + 이 이유는 Intel의 팹(Foundry) 분사를 매각할 기반을 마련하려는 의도라고 봄, CHIPS Act 자금에는 팹 분사 시 정부가 투자금을 환수(claw-back)하는 조항이 있었으나, 이번 딜로 해당 조항이 해제되고, 대신 정부가 5% 지분을 20달러에 5년간 행사 가능한 워런트를 받음, 단 이는 분사 후 팹 지분이 51% 미만으로 떨어질 때임, 결국 이 딜의 목적은 이사회가 팹을 팔고 싶지만 정부에 돈을 토해내기 싫어서 이런 구조를 설계한 것임
          + 이것은 단순히 또 하나의 Trump식 강압이라고 생각함
     * 이번 이슈는 미국이 경제 초강대국 역할에서 전환점에 서 있음을 상징하는 것 같음, 자동차, 은행에 대한 구제금융과 비슷하긴 하지만 정말 구제할 가치가 있는지, 아니면 사라질 운명에 처한 회사를 억지로 소유하는 것인지 의문이 듦
     * 미국 거리 풍경은 북한, 실제 내막은 베네수엘라라는 느낌임
     * Ex Post Facto 조항, 미국 헌법에 대해 언급하고 싶음, 나는 이 조항이 쉽게 무효화될 거라고 생각했지만, 실제로 1912년 대법원은 이 조항이 형사 처벌에만 적용된다고 했음, 역시 법적 세부조항이 항상 발목을 잡음
          + 연방 대법원의 ""소송 요건""(standing doctrine)도 이상하다고 생각함, 만약 이사회가 거래를 승인하면, 주주들이 소송 제기할 수 있는 자격조차 갖추지 못하는 것 아닌지 궁금함
     * Intel 공식 보도자료: https://newsroom.intel.com/corporate/…
          + 주주 투표는 전혀 필요 없는 구조인지 궁금함, 추가 자본 투입 없이 10%의 희석 효과가 발생한 것이나 다름없다고 생각함
     * 미국 반도체 기업 리스트에서 Nvidia는 매출의 15% AMD는 매출의 15% Intel은 자본의 10%를 국가 차원에서 내놨음, 그 다음 차례가 누가 될지 궁금함
          + 실제로 AMD와 NVidia의 15%는 특정 부품의 중국 매출에 국한된 딜이므로, 전체 매출과는 전혀 다름
          + 희귀 광물 채굴업체인 MP Materials가 7월 10일에 리스트에 올랐고, 다음 타깃은 TikTok 혹은 Fox News일 것 같음
     * 만약 미국이 TSMC 지분 10%를 의결권 없이 샀다면, 더 의존한다는 신호를 강하게 주었을 것임, 흥미로운 발상이지만 진지한 제안은 아님
     * 예전 Tea Party가 다시 등장해서 이런 정책에 시위하는 장면을 볼 날이 한참 멀 것 같은 느낌임
"
"https://news.hada.io/topic?id=22730","안드로이드 앱 설치시, 사이드로딩을 포함해서 모두 개발자 인증 요구 예정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                안드로이드 앱 설치시, 사이드로딩을 포함해서 모두 개발자 인증 요구 예정

     * Google이 2026년부터 개발자 인증을 받은 앱만 인증된 Android 기기에 설치 가능하도록 변경 예정임
     * 이 정책은 모든 설치 방식에 적용되며, Play Store 외에 타사 앱스토어나 APK 파일 직접 설치도 포함함
     * Google은 가짜 앱 및 악성 앱 배포 방지와 반복적인 악의적 행위자 차단을 강화하기 위해 해당 조치 도입을 설명함
     * 비상업적 개발자(학생, 취미 개발자)와 상업적 개발자에 대해 별도의 인증 절차가 마련됨
     * 2026년 9월부터 브라질, 인도네시아, 싱가포르, 태국에서 우선 시행되고 2027년 전 세계로 확대 적용 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Google의 새로운 안드로이드 앱 개발자 인증 정책 개요

     * Google은 악성코드 및 금융 사기 방지를 위해 2026년부터 인증된 개발자가 만든 앱만 인증된 Android 기기에 설치할 수 있도록 할 계획임
     * 이 정책은 Play Protect 지원 기기와 Google 앱이 사전 설치된 기기에 적용되며, Play Store 뿐만 아니라 써드파티 앱스토어 및 APK 직접 사이드로딩까지 모든 설치 경로에 해당함

정책 세부 내용

     * 2023년 Play Store에는 이미 유사한 개발자 인증 요구가 도입되었으나, 앞으로는 모든 설치/배포 경로에 동일하게 적용됨
     * 구글은 “공항의 신원 확인(ID check)”과 비슷하다며, 앱 내용이나 유래와 무관하게 개발자의 신원 확인만 한다고 설명함
     * 악성 앱 유포자들이 앱이 삭제된 후 바로 새로운 유해 앱을 배포하는 것을 차단하고, 설득력 있는 가짜 앱 피해를 줄이는 것이 목표임
     * Google의 조사에 따르면 인터넷을 통한 사이드로딩 앱의 악성코드 발생률이 Play Store 대비 50배 이상 높게 나타남

사용자 및 개발자에 대한 영향

     * 앱 배포의 자유는 계속 보장되며, 개발자는 원하는 방법으로 사용자에게 앱을 제공할 수 있음
     * Google Play 외부에서만 배포하는 개발자를 위한 별도 Android Developer Console이 신설되며, 학생·취미 개발자는 상업적 개발자와 다른 인증 흐름이 제공됨
     * Google Play를 통해 배포하는 개발자는 Play Console에서 이미 관련 요건을 충족했을 가능성이 높음(기관의 경우 D-U-N-S 번호 필요)
     * 2024년 10월부터 일부 개발자는 인증 절차를 시작할 수 있고, 2026년 3월 전면 개방 예정임

도입 일정과 적용 국가

     * 2026년 9월, 브라질·인도네시아·싱가포르·태국이 첫 적용 대상임
       이유는 이 국가들이 관련 사기 앱의 피해가 특히 많았기 때문임
     * 2027년부터는 글로벌 전면 시행 예정임
     * 특정 지역 내 인증된 Android 기기에는 반드시 인증 개발자가 등록한 앱만 설치가 가능함

주요 기관 및 정부의 반응

     * 인도네시아 정보통신부는 “안드로이드의 개방성 유지와 사용자 보호를 균형있게 달성했다” 고 평가함
     * 태국 디지털경제사회부는 “긍정적이고 선제적인 안전 조치” 라며 국가 디지털 안전 정책과 부합함을 언급함
     * 브라질 은행연합(FEBRABAN)은 이를 “사용자 보호 및 책임성 확보의 의미 있는 진전” 으로 언급함

   저는 audio share라는 앱을 fdroid에서 apk만 받아서 사용 중인데, 이게 앞으로 어찌 될지 궁금하네요. 유일한 사이드로딩 앱인데 말이죠...

   사이드로딩이 사실상 막히는게 맞다면 저로선 ios보다 메리트가 떨어진다고 보입니다. 저는 둘 다 거의 비슷한 기능을 제공하고 있고, ux는 ios가 조금 더 우위인데, 사이드로딩이 android의 큰 이점이라고 생각하기 때문입니다. google pixel에 graphene os 올려서 쓰는게 로망이었는데, pixel 소스 비공개부터 사이드로딩의 사실상 차단까지 나오면 저로선 안드로이드를 쓸 이유가 없네요. 이대로 출시되면 27년에 ios로 돌아갈듯 합니다.

   혼자서 앱 개발해서 쓰는 사람도 개발자 인증을 받아야 하는 건지 걱정이네요..

        Hacker News 의견

     * 구글이 안드로이드 기기 전체에서 앱 배포 개발자 신원을 확인하겠다는 결정은 완전히 받아들이기 힘든 일임, 마치 단순히 윈도우에서 프로그램을 실행하려면 마이크로소프트에 내 개인정보를 제출해야 하는 것과 같음, 이런 정책이 원하는 방향으로 흘러가지 않을 것임
          + 앞으로는 구글이 정책을 바꾸기 전에 오히려 마이크로소프트가 윈도우에서도 같은 길을 갈 것으로 예측함, 이는 악성코드 문제와 플랫폼 통제, 그리고 정부 규제의 영향이 결합된 미래임
          + 한 번도 ""폰"" 프로그래밍에 크게 발을 들인 적은 없지만, 시장 상황이 안정되길 기다렸음에도 오히려 더 나빠지고 있음, 전 세계적으로도 폰이 유일한 컴퓨팅 기기인 사람도 많은 상황임
          + 구글은 전체 생태계를 꽉 쥐고 있다는 느낌을 받음, 최근 스마트폰 제조사들이 기기 언락이나 개조를 점점 더 어렵게 만들고 있고, 구글 및 앱 개발자들이 하드웨어 TPM에 해당하는 기술로 기기가 구글에서 승인된 시스템인지 검증하는 추세임, 대체 플랫폼은 아직도 앱 생태계에서 수십 년 뒤쳐져 있어 구글이 이런 정책을 그냥 밀고 갈 수도 있다고 생각함
          + 마이크로소프트 윈도우도 같은 방향으로 가고 있음, Smart App Control 기능이 일부 지역에 적용되기 시작했고, 코드를 서명한 인증서 없이는 .exe가 실행되지 않음 Smart App Control 자세히 보기
          + 많은 사람들이 이런 정책에 반대할 수 있지만, 현실적으로 선택지가 부족함, iOS로 옮긴다고 자유가 오는 것도 아니고, 리눅스폰도 아직 실사용이 불가능한 수준이라 결국은 플립폰 같이 앱 설치 자체가 어려운 구형 기기로 돌아가야 하는 건지 의문임
     * 최근 들어 스마트폰 OS가 둘 뿐인데 이런 일이 벌어진 것은 매우 심각한 문제임, 암호화 자체를 금지할 방법이 없으니, 정부들은 이런 식의 아이덴티티 검증 등으로 보안과 프라이버시를 조금씩 갉아먹는 중임, 구글 공식 정책 페이지에도 ‘공식 신분증 업로드 필요’가 있음 정책 안내, 이런 정책이 결국 사람들의 분노로 인해 구글이나 정부가 정책을 되돌리는 일이 나올 거라 생각함, 최대한 대체 모바일 OS로 넘어가기를 권장함
          + 이런 정책이 대중의 분노로 번질 거라 생각하지만 실제로는 관심 갖는 사람 자체가 매우 적을 것이고, 이슈화도 되기 힘들 것으로 봄, 심지어 현재의 앱 사이드로딩 소송조차 에픽 같은 대기업의 이익과 직접 연결됐기에 가능했던 일임
          + 스마트폰 시장에 메이저 OS가 두 개 뿐이라고 해서 특별한 일은 아님, 데스크톱 OS 시장도 비슷한 구조였고, 결국 사용자 기대치 문제임
          + Play Protect 인증이 없는 대체 OS를 올리는 게 뭐가 문제인지 궁금함
          + 현재의 전체주의 경향이 쉽게 끝날 거라고는 기대하지 않음
          + 대중의 분노로 정책이 철회된다는 것은 비현실적임, 결국 투표로나 다른 방식으로 영향력을 행사할 수 있다고 해도 후보 대부분이 구글 등에 영향을 받으므로 구조적 한계는 명확함
     * 지난 10년간 안드로이드와 iOS에 점점 불만이 커지고 있음, 플랫폼은 더 사용자에게 적대적이고, 앱스토어는 프라이버시 침해, 트래킹, 광고, 중독 요소만 가득한 질 나쁜 앱으로 넘쳐남, 초창기 모바일 앱의 혁신이 그립고 Palm Pilot 시절이 그리움, 누군가 진짜 이 문제를 해결하는지 궁금함, 우리는 분명히 더 나은 디지털 환경을 만들 수 있음
          + 앱 구독이 아닌 일회성 구매 모델로 돌아가야 진짜 변화가 시작될 거라고 봄, 그러나 모든 주체가 욕심이 커서 쉽지 않을 것으로 보임
          + 개인적으로 GrapheneOS와 F-Droid 조합을 사용할 때가 정말 만족스러움, 다른 사람 스마트폰을 쓰면 늘 놀람, GrapheneOS 자체 기기가 출시되면 꼭 구매해서 적극 추천하고 싶음
          + 독일의 Vollo는 커스텀 안드로이드와 Ubuntu Touch를 선택적으로 구동하는 매력적인 기기를 판매하며, 네덜란드에는 Fairphone 같은 선택지도 있음 Vollo Fairphone
          + 예전 안드로이드와 iOS의 ‘추억의 시절’ 그대로를 지금도 충분히 누릴 수 있음, 동시에 LLM 등 새로운 변화의 시대도 잠시 즐겨야 함
          + 스마트폰 플랫폼은 정말 심각한 상황이며, 이미 일부 사용자들은 스마트폰에서 멀어지는 움직임이 조금씩 일어나고 있음, 다만 대중화되지는 않을 것으로 생각함
     * “우리는 개발자에게 사이드로딩이나 어떤 앱스토어든 선택권을 준다, 이것이 오픈 시스템의 정의”라는 구호와 달리 실제로는 폐쇄적으로 바뀌는 상황임, 특히 “Developer’s Alliance” 같은 단체가 이런 정책을 지지하는 점에서도 진정으로 개발자를 위하는지 의문이 생김, 실제로는 정책에 긍정적으로 동조하는 단체들은 대기업이나 정부와 연계된 경우가 많다고 봄
          + Developer’s Alliance의 주소가 워싱턴 DC의 코워킹 스페이스라서 정책 PR을 위한 페이퍼 조직(전문 용어로 astroturfing)의 가능성이 높다고 추정함
     * 기사에는 계정 승인 과정에 대한 내용이 거의 없었지만, 구글이 임의로 앱 배포 승인을 부여하거나 철회할 수 있는 구조가 될 것이란 느낌임, 열린 플랫폼에서 게이트키핑(문단속)이 시작되는 셈임, 나 개인적으로는 서명 없는 앱을 설치할 때 경고창 클릭하거나 설정을 켜는 방식 등으로 충분하다고 봄, 윈도우도 비슷하게 서명 없는 실행파일엔 경고창을 보여주고 서명된 파일은 곧바로 실행시킴
          + 이런 구조는 스팀처럼 NSFW 앱 금지의 첫 단계가 될까봐 조금 걱정임
     * 프라이버시 이슈도 있지만, 이런 정책 변화가 오픈소스 프로젝트의 로컬 빌드를 거의 불가능하게 만드는 게 아닌지 궁금함, 예전에 로컬에서 개발자 본인 키로만 서명해 올리는 구조였는데, 새 정책은 패키지명이 신원과 묶이는 듯해서 타인 키로 서명할 수 없게 될 수도 있음, 혹시 내 기억이 틀리거나 프로세스가 바뀐 건지 의견을 듣고 싶음
          + 저장소 자체는 파일 디렉터리이므로 네임스페이스도 변경 가능하지만 이 과정 자체가 너무 번거로움, 직접 안드로이드 서명 키를 준비하고 추가적으로 신원 제출까지 요구하는 것은 사용 경험 자체가 크게 떨어지는 일임, 결국 '승인된 사람'이 아닌 경우 구글 인증 디바이스에서 직접 빌드한 앱 실행이 쉽지 않아짐
     * 만약 이 정책이 현실화되면 모바일 시장에서 a) 제3자와 계약 없이 앱 설치가 가능한 OS, b) 메인스트림 보안 앱(특히 뱅킹앱 등) 사용이 가능한 OS 중 어느 쪽도 남지 않게 됨
          + 앞으로는 인증된 OS와 브라우저에서만 데스크탑으로 뱅킹 접근이 가능해질 가능성이 큼
          + 나는 그냥 비활성화하고, 웹에서 뱅킹 되는 앱을 선택해 쓸 생각임, 내 앱 중 많은 부분이 사이드로딩이고 Play Store 등록도 많으며 개발자가 직접 본인 정보를 올리는 경우도 충분히 많음
     * 공식 발표는 다음 링크에서 참고할 수 있음 구글 공식 블로그 정책 상세 Google Play 지원, Play Store에 악성 앱이 많기 때문에 현재의 확인 절차는 별다른 효과가 없다고 봄, 이번 정책은 Google이 Revanced 같은 앱을 영구적으로 막기 위한, 즉 권력 강화를 위한 수단임, “안전”을 명분으로 내세우지만 진짜 중요한 인터넷 권한 설정 등은 사용자 광고 차단 여부 때문에 감춰져 있음에 실망함, “우리는 개발자가 누구인지 확인할 뿐 앱 내용은 보지 않는다”는 문구는 이해가 안 감, 결국 앱 내용 자체를 어느 정도라도 봐야 진짜 보안이 되는 건 아닌지 궁금함, Play Protect 해제 등 우회도 공식 발표문엔 언급 없어서 불가능할 듯함, 그래서 이제는 리눅스와 윈도우만 진정한 자유 개발 대상 플랫폼이라고 생각하게 됨, 구글 계정 없이 개발하고 싶음
          + 사실상 인터넷 퍼미션 없이도 데이터를 내보낼 수 있음, 공격자가 본인 사이트로 데이터 쿼리를 보내는 intent를 브라우저에 전송하면 충분함
          + Play Protect 비활성화로 제한을 우회할 수 있는지 여부는 실제 정책 적용 전까지 알 수 없음, 가능한 경우라면 Play Protect를 “허가된/notarized” 앱 외엔 모두 차단하는 방식이 될 가능성이 큼, 이 경우 기존 개발자 모두가 검증 절차를 거쳐야 함, 발표문의 의도도 이를 반영하고 있음, 다만 얼마나 많은 사용자가 이 검증을 끌 수 있는지는 미지수임
          + 실제로 보안 문제가 아니라 KYC(고객확인제)나 제재와 같은 정책 문제가 들어있는 건 아닌지 의심도 듦
          + 회사 신원 인증을 요구하면 은행처럼 위장 앱을 막는 효과는 기대할 수 있음, 패키지명별 공개키 등록은 악성코드가 삽입된 변조 버전 설치 방지에 유용함, APKMirror에서도 서명 확인을 하고 있지만, 정말 신뢰할 수 없는 경로로만 앱을 다운받아야 하는 상황이라면 원본 여부를 판별하기 위해 이런 시스템이 조금은 필요함, 패키지 내용 분석 없이도 EV SSL 인증서 등 웹의 보안 체계처럼 운영 가능함
          + 안드로이드에서 숨겨진 인터넷 액세스 컨트롤 설정이 뭔지 설명을 더 듣고 싶음
     * 이 정책이 Play Protect로 강제된다면 아래와 같은 명령어로 쉽게 비활성화할 수 있음
adb shell settings put global package_verifier_user_consent -1

       루트 권한 없이 Play Protect를 비활성화할 수 있음, 오픈소스 앱 배포를 위해 굳이 구글과 사업적 관계를 맺고 싶지 않음, 이로 인해 Play Protect를 글로벌로 끄는 사용자만 내 앱을 설치할 수 있게 된다면 어쩔 수 없다고 생각함
          + 이런 방법도 곧 구글이 “사기꾼 차단” 같은 명분으로 막지 않을까 예상됨
          + 이 방법이 실제로 무엇을 깨트리게 되는지 궁금함
     * 이런 상황을 어떻게 용인하게 되었냐는 의문이 있음, 사실 수많은 작은 제한에 순응하다 보니 여기까지 온 것임, 과거에는 주변에서 이런 우려를 이야기했을 때 “너무 민감하다, 별 일 아니다”라며 웃는 반응이 많았음, 결국 이런 현실이 와버렸음
          + eternal september라는 표현처럼 영원한 변화의 흐름임
          + 자신과 무관하다는 태도로 GrapheneOS나 Calyx 같은 대체 OS를 쓰는 일부 유저들 이야기도 있지만, 결국은 이런 흐름의 downstream(하위 흐름)에 놓여 있는 것임, 안드로이드의 진정한 가치는 보통 사용자와 해커 모두에게 ‘인터페이스 표준성’이었음
          + 우리는 상황에 영향을 준 적이 전혀 없으며, 진짜 책임은 구글과 그 임직원들에게 있음, 이들은 이익과 커리어를 위해 사용자 자유를 대가로 거래했음, 지금 상황은 늦은 자본주의의 시행착오 중 하나임
"
"https://news.hada.io/topic?id=22742","Adobe Reader 설치 파일 크기의 변화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Adobe Reader 설치 파일 크기의 변화

     * PDF 뷰어인 Adobe Reader 설치 파일 용량이 수년간 크게 증가하여 최신 Windows 11용 25.x 버전은 약 687MB에 달함
     * 이 버전엔 AI 기능, 자동 업데이트, 온라인 서비스 광고, 2가지 GUI(신/구 버전) 가 포함됨
     * SumatraPDF 3.5.2는 크기가 8MB 수준으로, 광고·AI·자동업데이트 없이 경량성과 단순성을 유지함
     * 그래프에 따르면 Adobe Reader는 1993년 약 1MB에서 시작해 2025년 687MB로 폭발적 증가를 보임
     * 단순한 PDF 리더에서 거대한 서비스 허브로 변한 흐름은 소프트웨어 비대화 현상의 전형적 사례임

Adobe Reader 설치 파일 크기 추이

     * 1993년: 약 1MB 수준에서 시작
     * 2000년대 초반: 5~10MB 수준으로 점진적 증가
     * 2010년대: 15~20MB대에 머물다 점차 가속
     * 2020년 이후: 200MB를 넘어 2025년 687MB까지 급등

SumatraPDF와의 비교

     * SumatraPDF 최신 버전(3.5.2) 설치 파일은 8MB
     * 특징:
          + AI 기능 및 광고 없음
          + 자동 업데이트 없음(필요시 수동 확인)
          + 클라우드 서비스 연동 기능 없음
     * 설치 파일 크기와 기능 간의 대비가 극명함

업데이트

     * Hacker News에서의 최고 댓글: ""Looks like a chart crime scene."" (차트가 범죄 현장처럼 보인다는 유머)
     * 이에 선형 그래프와 그래프 생성에 사용된 소스 파일이 추가로 제공됨

맥락과 시사점

     * Adobe Reader는 단순 PDF 뷰어를 넘어 클라우드, 광고, AI 서비스 통합 플랫폼으로 확장됨
     * 그러나 그 결과 비효율적·무거운 소프트웨어가 되었으며, 사용자 경험 측면에서는 불만 요인 존재
     * 반면 SumatraPDF는 경량·단순·오픈소스 철학을 유지하며, 파워 유저 층에 여전히 사랑받고 있음

        Hacker News 의견

     * Adobe Reader는 새 기기에 절대 설치하지 않는 첫 번째 앱임. 느리고 버벅거림, 다크 패턴과 팝업창이 난무함, 기본 편집 기능은 구독 없이는 숨겨둠. 사용자를 존중하지 않는 모든 요소가 모여 있음. 정말 끔찍한 소프트웨어임. MS Word도 맥에서 점점 더 무거워지는 걸 보면 비슷한 느낌을 받음
          + 팝업창은 사실 언제나 짜증나는 요소임. 최근 이 점을 많이 고민해봤는데, 사용자 입장에선 팝업이 정말 꼭 필요한 좋은 선택인 경우를 떠올릴 수가 없음. 개발자 입장에선 눈길을 끌기 좋은 쉬운 방법이겠지만, 결국 사용자는 오래 머물지 않음
          + Adobe Reader를 안 깐다는 말에 정말 공감됨. “전화가 울리지 않으면 그건 나야”라는 노래가 생각남
          + Adobe Reader(혹은 Acrobat Reader)는 여전히 PDF 업계 표준임. 예전에 OnlyOffice로 만든 PDF가 Chrome에서는 제대로 표시됐는데, Acrobat에서는 폰트가 제대로 표현되지 않아서 문제가 있었음. 그래서 만든 PDF 파일의 호환성 검증용으로 Acrobat을 설치해둠
          + 나도 공감하지만, 결국 설치할 수밖에 없는 게 가끔 Acrobat만 제대로 처리하는 PDF를 계속 받게 됨. 특히 비즈니스 상황에서 많은 사람들이 PDF 기능을 창의적으로 쓰다 보니, 대안 프로그램들은 늘 부족한 점이 있음
          + Adobe Reader/Acrobat을 사용하는 유일한 용도는 PDF를 텍스트로 변환하는 것임. 어떤 PDF는 이 작업이 pdftotext보다 훨씬 잘 돼서 쓰고 있음
     * Adobe Reader 설치 전에 Adobe Reader Customization Wizard for Windows로 광고나 온라인 기능, ""Upsell"" 옵션을 끄는 등 기능들을 미리 제거할 수 있음. macOS 버전도 있을 수 있고, 레지스트리나 환경설정에서 직접 ""FeatureLockDown"" 옵션을 설정해도 동일한 효과를 볼 수 있음. 관련 문서는 여기, 여기, 여기 참고
          + 이런 작업을 굳이 할 필요 없이 대체 프로그램을 설치하는 것이 더 편리함
     * 요즘 대형 프로그램을 믿지 않게 됨. Kubernetes 클러스터 관리하려고 누가 Lens를 추천했는데, 설치 파일만 600MB에 설치하면 두 배로 불어남. 데스크톱 소프트웨어가 너무 과해진 시대임. Blender가 300MB인 것과 비교하면 참 역설적임. 물론 극도로 최적화된 소프트웨어만 원하는 건 아니지만, 2GB짜리 k8 콘솔은 개발자에 대한 신뢰 자체가 안 생김
          + 모바일도 다르지 않음. 내 아이폰 확인해보니 Instagram, TikTok, Duolingo가 각각 500MB쯤 됨. 앱을 조금만 써도 캐시 때문에 금세 GB 단위 차지함. 거의 안 쓰는 Snapchat도 캐시만 다섯 기가임
          + k9s라는 대안을 한 번 확인해보길 추천함 k9scli.io
          + 난 Octant가 너무 그리움. 정말 80/20 원칙에 맞는 훌륭한 앱이었음. 요즘은 대부분 kubectl을 비유적으로나마 계속 쓰고 있음. k9s도 시도해봤지만 도무지 내 스타일은 아님
     * y축이 로그 스케일임을 참고할 것임. 현재 Adobe Reader 용량은 Sumatra에 비해 83배 크기로 나타남
          + 로그 스케일은 상대적 크기 차이를 알리려는 의도를 망치는 선택 같음
          + 처음엔 그래프를 보고 버전 번호를 파일 크기(MB)로 착각함. 예를 들어 “25.1MB?”라고 생각했음. 의외로 훨씬 더 클 거라 생각했는데, 혹시 초압축했나 싶기도 했음. Sumatra가 3MB대인 것도 잘 압축했으면 가능한 수치라고 여김. 그치만 요즘 프로그램 용량은 말도 안 되게 커진 게 많음. 예전 Zoom이 업데이트 한 번에 단숨에 두 배 커지던 것도 기억남—웹 브라우저 하나를 패키지 안에 통째로 더 실어 넣으면서 그렇게 됐음
     * Adobe가 2005년 Macromedia 인수 후 Flash를 Acrobat과 Acrobat Reader 등 여러 제품에 통합했음. 그래서 PDF 안에 SWF(Flash) 컨텐츠를 내장할 수 있었고, 이로 인해 인스톨러 용량과 복잡성이 대폭 증가함. Flash Player 공식 지원 종료(2020년대 초) 이후 Flash 지원은 결국 사라짐. 한편 Acrobat은 대화형 PDF(폼 검증, 자동화 등) 지원을 위해 JavaScript 엔진도 내장함. Flash와 JavaScript는 오랫동안 심각한 보안 리스크를 일으켜 왔음. Flash는 없어졌으나 JavaScript는 여전히 남아 있어서 보안 위험이 상존함. 반면, Sumatra 같은 경량 PDF 리더는 JavaScript나 Flash를 지원하지 않아 훨씬 가볍고 안전함
          + 난 항상 PDF에 JavaScript가 내장된 게 뭔가 아이러니했다고 생각함. 원래 PostScript라는 언어가 있었는데, 이건 렌더링 엔진이 엄청 강력함. 문제는 PostScript가 튜링 완전 언어라 너무 유연해서 문서로 다루기엔 곤란했음. 그래서 Adobe가 엔진은 그대로 두고 튜링 요소를 빼내고 구조를 좀 더해야 PDF를 만들었다는데, 결과적으로 또 JavaScript로 튜링 기능을 되돌려 넣은 꼴이 됨. 굳이 스크립트 언어가 필요했다면 PostScript로 다시 넣는 게 나았을 수도 있다고 생각함
     * 옆길로 새는 이야기긴 하지만, 윈도우용 커스텀 뷰(2페이지 보기, 다크 모드, 툴바 자동 숨김 등)와 부드러운 스크롤이 가능한 PDF 뷰어 아시는 분 계시는지 궁금함. 과거 Adobe Viewer가 유일하게 이걸 지원했는데 이제 단종돼서 아쉬움. Xodo PDF가 그나마 비슷한데 팝업이 너무 많음
          + Sumatra PDF도 한 번 써보라고 권유함
          + Okular도 이런 식으로 커스터마이징 가능할지 모름
          + 맥을 쓰고 Preview를 추천함
     * ""상식 있는 사람이라면 scoop으로 깔 것""이라는 관점이 정말 이해하기 어려움. 그래도 글과 그래프는 잘 만듦
          + 윈도우 생태계에 익숙하지 않은데 scoop이 choco, nuget 같은 패키지 매니저 맞는지, 혹시 scoop이 부적절하게 무겁다는 평가도 있는 건지 궁금함
          + 이런 태도는 아쉬움. 모두의 요구가 다 다르단 걸 이해하지 못한 시야라 생각함. winget, chocolatey 같은 용도를 제안도 안 하고, 웹사이트로 다운받는 걸 “비상식적”이라고 보는 건 무리라고 생각함
     * 요즘 브라우저가 PDF 폼 작성, 서명 등에서 Adobe 소프트웨어보다 오히려 더 나은 기능을 제공함. Adobe는 이런 기본 기능조차 쓰려면 업셀만 시도함
          + 언제부터인지는 모르겠지만, Chrome이 이 기능을 정말 잘 처리해줌. 1~2년 전에는 미흡했던 거 같은데 지금은 완벽함
     * 2004~2005년에 Mac으로 넘어갔음. 그때 가장 인상적이었던 건 Preview였음. PDF 처리에 너무 유용해서 Adobe Reader가 더는 필요 없었음. 지금까지도 Adobe Reader가 여전히 무겁고 별로인 건 놀랍지 않음. 그런데 용량이 시디 한 장 크기까지 커질 거라곤 몰랐음. 정말 우스운 상황임
     * 내가 느낀 유일한 차이는 PDF 화면 표시 속도 정도임. 복잡한 기술 문서를 읽을 때는 그리 중요하지 않고, 많은 페이지를 빠르게 넘겨봐야 할 때는 Adobe Reader가 더 빠름
          + 무엇보다 빠르다는 것인지 궁금함
"
"https://news.hada.io/topic?id=22635","AGENTS.md - AI 코딩 에이전트를 위한 오픈 포맷 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    AGENTS.md - AI 코딩 에이전트를 위한 오픈 포맷

     * AGENTS.md는 README의 보완 역할을 하며, AI 코딩 에이전트가 프로젝트에서 작업할 때 필요한 맥락과 지침을 담는 전용 파일
     * 20,000개 이상의 오픈소스 프로젝트에서 사용 중으로, 빌드/테스트/코드 스타일 등 사람에게는 불필요하지만 에이전트에게 중요한 정보를 정리
     * 명확하고 예측 가능한 위치에 에이전트 전용 지침을 제공해 README를 간결하게 유지하면서도 협업 효율성 강화
     * 단일 AGENTS.md로 다양한 에이전트 및 툴과 호환되며, 대규모 모노레포에서는 하위 프로젝트별 개별 AGENTS.md 사용 가능
     * OpenAI Codex, Cursor, Google Jules 등 여러 생태계의 협업으로 만들어진 개방형 표준
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Why AGENTS.md?

     * README.md는 사람을 위한 문서, 빠른 시작, 프로젝트 설명, 기여 가이드라인을 제공
     * AGENTS.md는 에이전트를 위한 보조 문서로, 빌드/테스트/코드 규칙 같은 세부 맥락을 담아 README를 복잡하게 만들지 않음
     * 별도 파일로 둔 이유
          + 에이전트가 참고할 예측 가능한 지침 위치 제공
          + README는 인간 기여자 중심으로 간결하게 유지
          + 기존 문서와 보완되는 정밀한 에이전트 전용 지침 제공
     * 독점 포맷이 아닌 누구나 쓸 수 있는 오픈 표준 명칭을 채택
     * 하나의 AGENTS.md로 여러 AI 코딩 에이전트 및 툴과 호환 가능

How to use AGENTS.md?

     * 1. AGENTS.md 파일 생성
          + 저장소 루트에 배치 (많은 에이전트가 자동 생성 지원)
     * 2. 주요 항목 작성
          + 프로젝트 개요
          + 빌드 및 테스트 명령어
          + 코드 스타일 가이드라인
          + 테스트 방법
          + 보안 고려사항
     * 3. 추가 지침 포함
          + 커밋/PR 규칙, 보안 주의사항, 대규모 데이터셋, 배포 단계 등 팀원에게 전할 내용
     * 4. 모노레포 대응
          + 각 패키지별 AGENTS.md 배치 가능
          + 에이전트는 가장 가까운 파일을 읽어 해당 서브프로젝트에 맞는 지침을 따름
          + 예시: OpenAI 저장소에는 88개의 AGENTS.md 존재

FAQ

     * 필수 항목: 없음, 일반 마크다운 형식 자유 사용
     * 충돌 발생 시: 가장 가까운 AGENTS.md가 우선, 사용자 명시 프롬프트가 최종 적용
     * 자동 실행 여부: 파일에 명시된 테스트 명령어는 에이전트가 실행해 오류 수정 시도
     * 업데이트 가능성: 언제든 변경 가능, 살아있는 문서로 관리
     * 기존 문서 마이그레이션: 파일명 변경 후 심볼릭 링크로 호환 유지
          + mv AGENT.md AGENTS.md && ln -s AGENTS.md AGENT.md

        Hacker News 의견

     * 작은 프로젝트에는 하나의 .md 파일만으로도 충분하지만, 복잡한 프로젝트에는 계층적인 폴더 구조와 index.md가 훨씬 더 효과적임을 경험했음
       index.md와 그 하위에 auth.md, performance.md 같은 파일들로 구성된 구조는 유지 관리도 쉽고, LLM이나 에이전트가 불필요한 토큰 소모 없이 관련 맥락만 추출할 수 있음
       .docs 파일들이 인간과 LLM 모두에게 적합하게 되고, index.md에는 각 파일에 대한 간단한 안내와 조직 가이드도 포함 가능함
       단, "".agents""라는 이름보다는 "".codebots"" 또는 "".context"" 등 더 직관적인 이름이 좋겠음
          + 중요한 파일과 디렉토리는 굳이 숨길 필요가 없다고 생각함
            특히 문서는 숨겨두면 오히려 불투명해지는데, 전통 때문인지 모르겠지만 이런 방식은 좋은 패턴이 아님
            robot_docs 같은 이름은 어떨까 고민함
          + 사실 이런 정보는 기여자들이 궁금해하는 내용과 겹치기 때문에, 원래 CONTRIBUTING.md에 들어가는 게 맞다고 생각함
          + 이 구조는 인간과 로봇 모두를 위한 소프트웨어 설계/코딩 스타일 가이드 문서 느낌임
            나는 이런 .md 파일을 docs/ 폴더에 넣음, Claude Code가 직접 작성함
            AGENTS.md와 CLAUDE.md는 오직 로봇을 위한 용도여야 하고, 하나의 대형 파일에 h2 헤더로 구역을 나누든, 여러 파일로 분할하든 각각 장단점 있음
            Arc42처럼 둘 다 지원하는 아키텍처 문서 포맷도 있음
            대형 마크다운 내에서 특정 섹션을 참조하는 것보다, 별도 파일로 만들어서 @멘션하는 편이 쉽고 실수도 줄어듦
            특정 부분에 집중이 필요할 때는 작은 파일로 쪼개는 게 코드 에이전트에게도 더 좋음
            작은 파일들은 diff/PR 리뷰할 때도 더 편리함
          + 코드베이스 안에 AGENTS.md 파일을 여러 개 둘 수도 있음
            툴링은 현재 디렉토리와 루트 디렉토리의 AGENTS.md를 모두 읽도록 하면, 정보가 코드 근처에 머무를 수 있어서 원하는 방식과 병행 가능함
          + 나도 비슷한 구조를 쓰고 있는데, index.md에 각 파일별 간단 안내를 추가해 꽤 괜찮은 결과를 얻어왔음
            rules.md처럼 디렉토리별로 원하는 동작 규칙 파일을 넣는 방식도 실험 중임
            예를 들어 realtime-service.ts와 queue-service.ts처럼 다양한 서비스가 모인 디렉토리엔 그 옆에 rules.md를 두는 식임
            프롬프트에 이 규칙 파일을 지정해서 새로운 것들을 쉽게 만들어낼 수 있음, 이름은 고민이 더 필요함
     * 현재는 에이전트가 코드베이스를 이해하도록, 인간이 필요로 하는 것 이상의 특별한 가이드를 추가로 작성해야 하는 과도기임
       곧 있으면 이런 게 필요 없다고 생각함
       우리의 프로젝트 문서가 본래부터 충분히 충실하게 작성되어 있다면 AGENTS.md에 있는 내용도 모두 포함될 수 있다고 봄
       우리는 항상 인간을 대상으로 문서를 써야 하고, LLM의 장점은 인간이 쓴 글을 읽을 수 있다는 데 있음
       이런 관점에서 설계를 하는 게 맞다고 생각함
          + 단순히 코드베이스 이해 뿐 아니라, 특정 스타일(예: 어떤 assert 라이브러리로 테스트를 작성할 것, 코멘트 금지, 구조적 로깅 사용 등) 규정을 명시하는 용도로도 쓸모가 큼
            오히려 코드가 거의 없는 신규 프로젝트에 더 유용할 수 있음
          + 기계가 읽을 수 있는 규칙이 사회 곳곳에 더 많이 도입될 거라 예상함
            예로 자율주행과 교통 법규가 있는데, 실제로 사람도 맥락을 파악하기 어려운 표지(예: ""빨간불 우회전 금지, 학기일 7~9시"")는 기계에게는 더 힘듦
            결국은 행정기관에서 맥락을 덜 요구하거나 기계 가독성(QR코드 등)이 있는 신호로 바꿔야 할 것임
            이런 변화가 없다면 기계들이 규칙을 어기는 일이 많아질 것임
          + 비즈니스 로직 같은 영역에선 앞으로도 에이전트를 위한 특별한 안내가 꼭 필요하다고 생각함
            무엇을 만드는지, 의도가 무엇인지, 프로젝트 최종 목적 등은 사람이 구체적으로 알려주지 않는 이상 기계가 파악하지 못함
            아키텍처 같은 것 역시 사람마다 기준이 달라 머릿속에 구조가 잡혀 있어야 실제 변경 사항을 볼 때 이해에 도움이 됨, 결국 진짜 병목은 이 부분임
          + 전반적으로는 동의하지만, 특정 정보(예: 매번 반드시 컨텍스트에 넣고 싶은 것)는 별도의 파일로 강제로 포함시키고 싶은 욕구가 생기기도 함
          + 우리가 암묵적으로 생각하는 규칙, 전제를 문서화하지 않으면 LLM은 알 수 없음
            코드에서 일부 추론도 가능하겠지만 100%는 불가능하므로 명시적으로 요구사항을 작성해 두는 게 중요함
     * AGENTS.md라는 건 결국 README.md와 같은 역할을 하면서 하이프를 끌어, 실제로 사람들이 내용을 채우고 있다는 점이 신기함
       사람들은 다른 사람을 위해 문서를 쓰는 건 귀찮아하는데, 로봇을 위해서라면 열심히 적고 있다니 흥미로움
       이런 현상은 소수만을 위해 인체공학적 설계를 했는데 모두에게 더 잘 맞는 핸들 디자인 같은 것과 비슷함
          + 오히려 반대로, 사람들이 문서를 읽지 않으니 아무도 쓸 동기가 생기지 않았던 것임
            에이전트를 위한 CLAUDE.md는 한 번 써두면 1000개의 에이전트에게 곧 읽힐 테니 작성할 맛이 남
          + 어차피 README.md에다 최소한만 적으면 되는 거 아닌가 싶음
          + 실제로는 에이전트들조차 이 문서를 안 읽거나, 몇 번 더 지시하면 전부 잊어버릴 상황임
          + 지금 상황은 사람들이 인간 개발자(자신 포함)를 개발 과정에서 적극적으로 빼려고 노력하면서, 에이전트가 적절한 안내를 반드시 가져야만 하게 된 것임
            인간의 모든 소프트웨어 개발 관여를 없애고 싶은 욕구가 커졌기 때문임
     *

     build steps, tests, and conventions that might clutter a README or aren’t relevant to human contributors. 이런 부분을 따로 빼자는 건 정말 세상이 어찌 돌아가고 있는지 모르겠는 감정임

     * 요즘 분위기 자체가 vibe coding임을 농담 삼아 언급함
       봇을 위한 문서를 쓰는 게 결국 잘 쓴 문서와 다르지 않다는 의견도 이전에 올라왔던 것 같음
     * 결국 ""AGENTS.md 파일을 만들고 마법을 채워넣으세요""라고만 써놓고 실제 사이트로 링크시키는 식이라는 느낌임
     * 나의 경우, 5,000개 이상의 리포지토리를 관리하는 코딩 에이전트를 개발하고 있음
       에이전트의 상태는 숨겨진 .agent 디렉토리 내에 저장되고, 각 에이전트 역할별 구성 폴더와 명확한 역할별 지침들을 포함함
       프로젝트 폴더에 agents라는 폴더를 두고, 여러 파일을 역할별로 나눠 <Role> <instruction> 식으로 관리함
       에이전트는 역할이 정의된 파일만 읽고, 상태는 dot<coding agent name> 폴더에 보관함
       초기화는 /init으로 진행되며, 리포 전체 코드를 단순히 인덱싱하는 대신 전체 아키텍처와 논리를 요약한 high-level summary를 생성함
       이 요약은 에디터 내에서 토글 가능한 ""ghost comments""로 제공되고, 실제 코드에는 커밋되지 않는 메타데이터임
       정교한 맵핑 시스템으로 요약마다 코드 줄과 정확히 연결됨
       처음에 RAG를 코드에 직접 썼을 때 원하는 결과를 얻지 못해서 지금 구조를 도입함
       현재는 AST 기반 빠른 문법적 검색과, 요약에 기반한 의미적 탐색(RAG)을 조합한 하이브리드 검색 모델을 사용함
       예를 들어 ""이 앱의 인증 방식이 어떻게 작동하는지?""를 물으면, 문법 검색은 login 같은 단어가 들어간 함수들만 찾지만, 의미 검색은 요약을 통해 인증 플로 전체를 서사적으로 파악해서 파일들에 흩어진 내용을 연결해 준다는 점에서 마치 마법처럼 작동함
          + 여기에 덧붙여, 요약의 계층 구조(B트리 또는 일반 트리 형태)를 만들 수 있음
            즉, 메서드/클래스/모듈별로 summary가 존재하고, 각 계층이 더 하위 계층을 가리키게 설계함
            RAG가 의미적 쿼리에 따라 필요한 만큼 깊게 탐색 가능하고, 각 단계는 하위 내용을 요약해서 정보량은 줄이되 꼭 필요한 의미만 유지함
            이 방식은 코드의 추상화가 잘 되어 있을 때 특히 효과적임
            예시로 add(n1, n2)처럼 이름만으로 의미가 충분한 함수는 실제 구현을 몰라도 되지만, 현실 세계 함수들은 로깅이나 캐시 등 여러 역할을 하므로 상황에 따라 실제 코드도 맥락에 넣어야 할 수 있음
          + 더 자세한 설명이 듣고 싶음
     * 인간들이 서서히 프로젝트 문서를 제대로 작성하도록 유도당하고 있는 느낌임
          + 농담이지만 사실 이런 부분을 팀에 어필하고 있음
            LLM이 실제로 생산성을 크게 높여주지 않더라도, 결과적으로는 문서화를 훨씬 더 잘 하게 되는 효과가 있음
          + ""Mission. Fucking. Accomplished."" xkcd 810
     * README.md와 AGENTS.md를 굳이 분리해야 하는지 아직 확신이 없음
          + 나도 계속 고민하고 있음
            관련 정리에 따르면,
            분리하는 이유로는

    1. 작성 스타일 문제(에이전트 문서에서 전체 대문자 강조가 인간 문서에선 불편함)
    2. 함축성 대 완전성(에이전트의 문서는 핵심 정보만 가져가야 하고, 인간용은 모든 것을 최대한 문서화해야 함)
    3. 필요 정보의 차이(LMM에게 필요한 정보와 인간에게 중요한 정보가 다름)
       분리하지 말아야 할 이유는
    4. 중복 관리(핵심적으로 동일한 것을 두 군데에 따로 써야 하는 부담)임

     * README.md는 이젠 일종의 마케팅/랜딩 페이지 용, AGENTS.md와 CLAUDE.md는 코드/아키텍처/사용법 등의 실제 내용을 얻으러 가는 곳이라는 느낌임
     * 예시를 읽으면서 나 역시 같은 생각을 했고, 사실 좋은 README.md 하나에 모든 내용이 담기면 충분하지 않을까 싶음
     * README는 인간을 위한 것, AGENTS.md 등은 LLM을 위한 것임
       사용/설치 방법은 readme에, 컴파일/테스트 방법, 아키텍처 결정 사항, 코딩 표준, 레포 구조 등은 에이전트 문서에 정리함
     * LLM에서 context로 사용하는 용량 이슈도 생각해야 함
       README.md는 내용이 많아서 전부 context에 넣기는 힘듦
       AGENT.md에는 LLM에 꼭 필요한 테스트, 빌드명령 등 핵심 명령만 간결하게 넣음
       README와 겹치는 부분이 있을 수 있지만, 불필요한 내용이 context에서 반복 전송되는 건 피하고 싶음
     * AI의 약속이 바로 우리가 굳이 정확한 포맷에 집착하지 않아도, 원하는 방식대로만 적으면 기계가 알아서 맞춰주는 거 아니었나?
          + 실제로는 파일 이름만 표준화했고, 내용은 아무 형식을 강요하지 않고 아무든 원하는 방식으로 작성할 수 있는 게 맞은 선택임
            AGENTS.md는 표준 마크다운이니까 원하는 헤딩을 쓰면 되고, 에이전트가 텍스트를 파싱함
          + 그래도 어느 정도 구조와 형식은 중요성을 가짐
            정확한 코드 문법 수준은 아니더라도 말임
          + 결국은 내용을 명확히 적어야 한다는 결론임
            문서가 길어질수록 구조적인 접근법이 인간 입장에서 유지보수에 유리함
     * 사용하는 각각의 에이전트(Claude Code, Gemini, Aider)마다 파일 이름이 다 다름
       표준화된다면 좋겠지만, 지금은 ruler로 여러 포맷을 자동 생성하는 수고를 감수함
       특히 에이전트마다 MCP 구성 파일 소비 방식도 달라서 표준화가 당장 이뤄지긴 어려울 거로 봄
       ruler
          + 약간 시니컬하게 보자면, 벤더 락인을 만들기 위한 움직임 때문이라고 봄
            표준화는 곧 상품화로 이어질 수 있어서 업체들이 꺼리는 듯함
          + Jules는 AGENTS.md를 써서 구글이 이 표준에 동참하고 있음을 보여줌
            Gemini Code Assist도 계속 유지된다면 AGENTS.md를 지원할 것으로 예상함
            Aider 문서에 특정 파일명이 언급되어 있지 않은데, 혹시 링크가 있다면 알려줬으면 함
            Anthropic이 유일하게 아직 표준 파일명을 지원하지 않는 것으로 보임
          + ruler 같은 도구가 실제 필요하다는 게 아쉽긴 함
     * 이상하게 다가오는 웹사이트임
       OpenAI가 만든 건 방문자 유치와 마케팅 포지셔닝 목적일까 싶음
       실제로는 포맷 없이 파일명만 지정함
       Anthropic/Claude가 빠져 있는 것도 눈에 띔; symbolic link 기법으로 CLAUDE.md를 AGENTS.md로 연결하는 식도 가능하긴 함
          + 실제로는 sourcegraph에서 만든 거고, 5월부터 존재했음
            이전에는 agent.md였다가, 이제는 agents.md로 301 리다이렉트됨
            과거 링크 참고
            공식 발표문도 있음
            최근 OpenAI와 파트너십을 맺은 모양임
            흥미롭게도 처음엔 agent.md였지만 이제는 agents.md로 바뀜
          + Claude가 목록에 없는 이유는 Claude만 표준 파일명 규칙을 아직 지원하지 않기 때문임
"
"https://news.hada.io/topic?id=22643","소수 숫자 그리드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               소수 숫자 그리드

     * 소수 숫자 그리드는 소수의 패턴과 구조를 시각적으로 보여주는 도구임
     * 이 그리드는 숫자를 2차원 형태로 배열하여, 소수가 분포하는 방식을 한눈에 파악할 수 있음
     * 패턴을 분석함으로써 소수의 규칙성 또는 무작위성에 대한 인사이트 확보 가능함
     * 프로그래밍/수학 학습자에게 소수 이론을 직관적으로 이해하는 데 도움이 됨
     * 다양한 각도에서 소수 분포를 탐구하는 데 참조 자료로 활용 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소수 숫자 그리드 개요

     * 이 도구는 숫자들을 2차원 그리드 형태로 배열한 뒤, 각 칸이 소수인지 여부를 시각적으로 구분하는 목적임
     * 사용자는 각 행과 열의 범위를 지정해 다양한 크기와 형태의 그리드를 생성 가능함
     * 그리드 내에서 소수는 색상이나 표시로 뚜렷하게 구분되며, 이를 통해 소수들이 어떻게 분포하는지 한눈에 확인 가능함
     * 규칙적인 분포, 대각선, 클러스터 등의 패턴 탐구가 쉬워지며, 이는 수학적 직관을 높이는 자료임
     * 해당 도구는 개발자와 학생들이 알고리듬 또는 시각화 작업에서 참조할 만한 소스를 제공함

특징 및 활용 예시

     * 각 숫자의 위치는 빠르게 소수인지 판별된 결과를 반영함
     * 대량의 수를 한 번에 처리하여, 큰 수의 소수 분포까지 탐색 가능함
     * 다양한 그리드 모양(정사각형, 직사각형 등)으로 커스터마이즈가 쉬움
     * 수학 교령, 알고리듬 연구, 시각적 프레젠테이션 등에서 학습 및 분석 자료로 중요함
     * 수학적 탐구뿐 아니라, 프로그래밍 도전 과제나 인터뷰 등 다양한 분야에서 활용 가능함

        Hacker News 의견

     * 안녕하세요! 어젯밤 재미삼아 이 간단한 소수 격자 시각화 툴을 만들었음. 며칠 전에 우연히 발견한 ""Show HN"" 게시글에서 영감을 받았음. Miller-Rabin 소수 판별법을 사용하고, OEIS 시퀀스 A014233에 나온 소수를 베이스로 삼아 3317044064679887385961980까지도 소수 판별 가능함. 예시로 이 링크를 참고할 수 있음. 거기에서 보이는 세 개의 원은 아래 소수를 의미함: 3317044064679887385961783
       3317044064679887385961801
       3317044064679887385961813
       이게 여러분에게도 재미를 주길 바람
          + 시각화 정말 멋짐! 마우스를 점 위에 올렸을 때 어떤 소수인지 알려주는 기능이 추가되면 좋겠음. 그리고 각 행마다 열의 개수를 X씩 늘리면(혹은 X를 소수로 하거나) 새로운 패턴이 관찰될지 궁금함
          + 만들어줘서 고마움! 열 개수를 빠르게 올려가며 반복되는 패턴, 작은 소용돌이 움직임이나 크게 휘어지는 선들을 발견하는 게 정말 재미있음. 어렸을 때 수학의 논리 퍼즐 요소를 정말 좋아했는데, 고등학교 후반부와 대학에서 수학이 점점 더 추상적으로 다가오면서 어렵게 느껴졌었음. 이렇게 시각화해주는 도구가 있었으면 수학적 개념을 더 구체적으로 느끼고 수식 뒤에 숨겨진 관계에 계속 호기심을 가졌을 것 같음
          + 숫자 진법을 16이나 다른 진법으로 바꿔볼 수 있는 기능도 있다면 정말 흥미로울 것 같음. 어떤 패턴 변화가 생길지 너무 궁금해짐
          + 너무 멋짐! 너가 만든 걸 보고 나도 패턴을 직접 찾아보겠다고 시각적으로 엄청 파고들었음 :D 근데 열과 행을 마음대로 정렬할 수 있으니 결국 내 시도는 별 의미가 없었던 것 같음 :D
     * 이상한 방식 하나 소개함: 정수를 100개씩 묶은 pack 단위로 봄. pack 안에 소수가 있으면 검은색, 없으면 빨간색으로 칠함. 첫 pack에는 100개의 연속된 정수가, 두 번째에는 두 수마다 하나씩, 세 번째에는 세 수마다 하나씩 등등 들어감. 각 pack은 이전 pack이 끝난 곳에서 이어서 시작함. 1행에는 pack 하나, 2행에는 두 개, 3행에는 세 개... 이런 식임. 여기에 그림 있음. 마치 다른 우주의 상형문자처럼 보임. 왜 이렇게 생기는지 아직 잘 모름. 랜덤 분포와 비교하려면 코드를 이렇게 바꿔볼 수 있음: if (isPrime(myNum)) return 1; 를 if (Math.random()>0.99) return 1; 로 바꿔보면 확연히 다름. 소수 기반 패턴의 대칭성과 성질이 도대체 어디서 온 건지 정말 궁금함
          + 이 댓글이 그림에 대한 설명을 잘 해줌. 본질적으로는 gcd(x,y)를 시각화한 것이고, 소수와 거의 상관없음. 이 사실을 알면 많은 패턴의 원인을 더 쉽게 이해할 수 있음. 그래도 정말 흥미로운 시각화임
          + 설명이 링크된 코드와 조금 다름. N번째 pack이 N씩 떨어진 정수로 채워지는 게 아니라, N번째 행의 각 pack마다 N씩 떨어진 정수를 포함하고 있음. 예를 들어 두 번째 행의 첫 번째 pack은 {101, 103, 105, ..., 299}, 두 번째 pack은 {102, 104, 106, ..., 300} 임. 이런 원리를 이해하면 패턴이 이 댓글에서 잘 설명됨
          + 이 아이디어에 꽤 빠져들었음. 처음에는 Ulam 나선이라고 쉽게 연결될 줄 알았는데, 이쪽 rabbit hole은 다항식 잔여와 신비로운 ""Conjecture F""에 닿음(설명). parallax primes는 이 링크에서 더 자세한 설명과 관련된 배경 지식, 특히 이 페이지에서 기하학적으로 해석되는 부분이 특히 만족스러웠음
          + 이런 방식으로 가지고 놀아봤음: 예시. 짝수 또는 홀수 pack만 반복하면 패턴이 실제로 수렴하는 걸 발견함. 정말 신기함
     * Ulam 나선도 한번 그려보는 걸 제안하고 싶음 Ulam spiral wiki. 그리고 만약 이게 Conway의 생명 게임(Game of Life)의 초기 상태라면, 흥미로운 패턴이 진화할지 정말 궁금함. 다양한 크기의 시작 격자를 brute-force로 돌려보면 몇 스텝 이상 유지되는 게임을 골라 사람이 직접 관찰할 수도 있을 거라 생각함. 혹시 소수의 특정 작은 격자 또는 나선이 특별한 무언가를 발생시키면 HN이 들썩일지도 모르겠음
          + 완전히 같은 건 아니지만, 10여 년 전에 만들어본 Ulam 나선 생성기가 있음. 링크. 이건 소수만 찍는 게 아니라, 각 위치의 숫자가 가진 짝수인 약수의 개수에 따라 점 크기가 결정됨
          + Ulam 나선에 한 표를 더 보냄. 처음에 왜 대각선이 안 보이는지 의아했음. 원래 Ulam 나선을 예상했었음
          + 또다른 Ulam spiral 도구
     * 소수에 대한 내 직관은 정말 빨리 희귀해진다고 생각했는데, 실제로는 소수가 엄청 많음
          + 소수는 실제로 점점 더 찾기 힘들어짐. 예를 들어 모든 소수를 한 행에 그려보면 그 차이를 확실히 볼 수 있음(여기 참고). 정수론에서 유명한 소수정리(Prime number theorem)도 이걸 다루고 있음. n 이하의 소수 개수는 n/log n에 근사하고, n 근처의 소수 밀도는 1/log n에 수렴함. 내 소수정리 설명과 위키피디아도 참고할 수 있음
          + 이 주제는 정말 많은 연구가 이루어졌음 위키피디아
          + 대부분 사람들이 그렇게 생각함. 소수를 찾기 어렵다고 배우기 때문이라고 봄. 사실 소수를 찾는 건 어렵지 않음. 우리가 생각하기에 인티저가 소수인지 판별하는 게 어렵게 느껴지는 것 뿐임. 사실 제곱수보다 소수가 더 많음
     * cols(열) 값이 소수가 되면 패턴이 멋지게 드러남
          + columns가 소수 p가 되면 각 열의 숫자가 p로 나눈 나머지가 똑같아짐. 그래서 p의 배수가 소수가 아니게 되면서 대각선 패턴이 생기게 됨
          + 열이 소수라기보단 cols+1 또는 cols-1이 약수가 많을 때(예: 25, 91, 119)에도 흥미로운 패턴이 나옴. 소수 근처의 숫자들이 약수가 많은 것도 흥미로움
          + 열이 7일 때 오른쪽 위에서 왼쪽 아래로 가는 대각선이 많이 보이고, 열이 5일 때는 왼쪽 위에서 오른쪽 아래로. 연속된 sexy prime의 빈도도 궁금함. 큰 숫자에서는 이 패턴이 깨질지 알고 싶음
          + cols % 30 == 0 (30, 60, 90, 120 등)일 때의 패턴이 정말 흥미로움. 직선 세로줄이 뚜렷함. 1만 더하거나 빼면(119나 121) 줄이 왼쪽 또는 오른쪽으로 “회전”하는 것처럼 보임. 정말 멋진 시각화 도구임
          + 보이는 대부분의 패턴은 사실 소수 특성이 아님. 처음 100개의 자연수로 나누어 떨어지지 않는 수만 표시해도 거의 비슷한 그림이 나옴
     * 최근에 나도 소수 시각화 툴을 만들어봄:
       https://ilmenit.github.io/prime-fold/
       시각화뿐만 아니라 진화 알고리즘과 피트니스 함수로 소수를 생성하거나 포함하는 수학 함수를 찾아내는 도구임.
       PrimeFold 모드(2D 임베딩): f_x(n), f_y(n) 두 함수를 입력하거나 진화시켜서 숫자를 2D 좌표로 매핑함. 소수와 합성수를 다르게 시각화함. 예: f_x(n) = n, f_y(n) = n^2.
       PrimeGen 모드(1D 생성): f(n) 하나만 입력하거나 진화시켜서 숫자 시퀀스를 만듦. 각 출력값이 소수인지와 고유 소수 개수를 시각화해줌. 예: f(n) = 2*n + 1
     * 1, 7, 100으로 설정하면 스타게이트의 체브론(chevron)처럼 별자리 틱커테이프를 보는 느낌임 :D
     * 이 링크에서 zoom out 하고 cols 값을 하나씩 늘리고 줄여 보면 패턴 변화가 관찰됨. -7부터 +5까지의 변화가 인상적임. #1-200-420에서도 동일함
     * 심심풀이로 python으로 연속된 소수의 일의 자리 수(10진법)를 비교해 본 결과 흥미로운 관계를 발견함. 2랑 5는 한번씩만 등장하니 제외하고, 1->3, 1->5, ... 등 각 자리수간 이동을 빈도로 세어 봤음. 소수는 무작위라고 생각해서 거의 빈도가 같을 거라 생각했으나, 오히려 통계적으로 유의미한 차이가 있었음. 왜 그런지는 아직 아무도 모름
     * 내 직감으로는 소수가 훨씬 더 드물고, 숫자가 커질수록 그 감소율도 훨씬 빠르다고 생각했으나, 실제로는 여전히 엄청 많음. [1, 10,000, 10,000]에서도 아래쪽은 꽤 촘촘함. 물론 덜 촘촘해지긴 함. 평균 소수 간격은 log(n)임 (prime number theorem)
"
"https://news.hada.io/topic?id=22734","로지텍이 만들지 않는 마우스 만들기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          로지텍이 만들지 않는 마우스 만들기

     * MX Ergo 트랙볼 마우스의 한계로 인해 사용자가 직접 기능 개선 시도함
     * USB-C 충전, 조용한 스위치, 경량화된 설정 소프트웨어 적용으로 사용성 향상 추구함
     * USB-C 포트 교체는 PCB 교체와 납땜 작업을 수반해 도전적이지만, 결과적으로 깔끔한 업그레이드 경험 제공함
     * Huano Silent 스위치 탑재는 클릭 소음 감소에 가장 효과적이며, 다른 스위치 교환은 변화가 크지 않음
     * Logitech 공식 소프트웨어 대신 SteerMouse로 대체해, 가볍고 폭넓은 커스터마이징 환경 구현함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   사용자는 Logitech MX Ergo 트랙볼 마우스를 2017년 출시 직후부터 애용하지만, 장기간 사용 중 다음 세 가지 문제점을 발견함
     * 마이크로-USB 충전 단자 사용
     * 큰 클릭음의 기계식 스위치
     * 무겁고 불편한 소프트웨어 제공

   오랜 기간 후속 모델을 기다려왔으나 기대에 부응하지 않아, 직접 원하는 방식으로 마우스를 개조하기로 결정함

USB-C 충전 포트 추가

     * 기존 마이크로-USB 포트를 제거하고 USB-C 포트로 교체하기 위해 PCB를 별도 제작함
          + 인터넷에서 USB-C 이식 관련 가이드와 오픈소스 설계도를 이용해, PCB 제작 업체인 PCBWay를 통해 보드를 주문함
     * PCB 조립은 직접 납땜으로 진행하며, 부품 이전과 케이스 일부 수정 작업이 필요함
     * 결과적으로 포트 크기에 맞게 외관을 일부 가공하고 무리 없는 완성도를 달성함
     * 전체 수정 비용은 PCB 45달러, 부품 10달러로 비교적 부담이 있으나, 대량 구매 및 프로모션 활용 시 비용 절감 가능성 존재함

조용한 스위치 적용

     * 기존 클릭음이 큰 스위치를 Huano Silent 스위치(저소음)로 교체하여, 누를 때의 만족감과 정숙성을 동시에 충족함
     * 스크롤 휠 클릭과 기타 버튼은 각각 Omron(모델: B3F-1002), Alps Alpine(모델: SKQGABE010)의 스위치로 변경함
     * 스위치 교체를 위해 마우스 분해, 기존 스위치의 납땜 제거 및 신제품 조립 과정을 거침
     * Huano Silent 스위치 교체가 체감 효과가 가장 크며, 기타 스위치 교체는 변화가 미미해 필수 수준은 아님
     * 교체 가이드로 iFixit의 Mouse Switch Replacement 문서를 참고할 수 있음

소프트웨어 교체

     * Logi Options+ 등 공식 유틸리티는 무겁고 버그나 오작동이 잦아 사용자 불만이 큼
     * 2024년부터 SteerMouse(유료, $20)를 사용하며, 가볍고 직관적인 환경, 다양한 주변기기 지원, 주요 버튼의 단축키 등 강력한 커스터마이징 가능함
     * SteerMouse로 대체 시 사용자가 필요로 하는 모든 기능을 만족하며, 마우스 사용 경험이 크게 향상됨

결론

     * 전체적으로 프로젝트 결과에 만족과 성취감을 느낌
     * 저소음 Huano 스위치 교체는 가장 추천할 만한 업그레이드임
     * USB-C 교체는 필수는 아니지만 표면 실장 납땜 실습에 좋은 기회이며, 결과물의 완성도도 높음
     * SteerMouse는 공식 소프트웨어보다 훨씬 개선된 사용성 제공함
     * 일상적으로 사용하는 기기를 스스로 커스터마이즈하는 과정에서 큰 즐거움과 추가적인 아이디어를 얻음

   Mx master 3s를 울며겨자먹기로 쓰지만 최근에 키크론 m6로 갈아타보려 시도하고 있습니다.
   SteerMouse 혹은 linearmouse 등의 툴을 이용하여 맥에서 여러 버튼을 커스터마이징 할 수 있는건 좋고, 세로휠의 가로스크롤 클릭, 별도로 존재하는 가로휠 등은 만족스럽게 쓰고있습니다.
   다만 너무나 가벼운 무게와 지나치게 큰 소음 (휠 소음은 진짜 미친줄 알았습니다)이 큰 단점입니다.

   언젠가 키크론에서 저소음 버전이 나오고 무게 커스터마이징이 된다면 저는 mx master와의 인연을 정리하려 합니다

   사진은 어디갔어 도대체

   존경스럽습니다 너무 멋지십니다

        Hacker News 의견

     * “조립 대행비 50달러가 아깝다고 생각해서 직접 해보자는 취지로 200달러가 넘는 열풍기 재작업 스테이션을 사서 직접 납땜을 선택함”이라는 얘기가 너무 공감됨
          + 다른 관점에서 보면, 50달러로 바로 해결하는 방법도 있지만 150달러만 더 주면 열풍기 재작업 스테이션까지 얻는 패키지로 볼 수도 있음. Superfastmatt라는 분이 1500달러짜리 밴 차량 부품을 3D프린터로 직접 만드는 실험을 보며 비슷한 생각을 하게 됨. “1500달러를 내고 그냥 그 부품을 사도 되고, 1500달러를 내고 멋진 3D프린터까지 갖게 될 수도 있음.” 이 아이디어가 계속 머릿속에 남아있음
          + 양면 SMT 부품 회수 없이 간단한 솔더 작업만 필요한 경우라면, 30달러짜리 콘센트 전기 프라이팬과 PCB 제조사에서 제공하는 솔더 페이스트 스텐실(혹은 주사기로 인내심있게 작업)이 굉장히 잘 동작함. 참고 자료 Simple Skillet Surface-mount Soldering
          + 나도 비슷하게 퀄리티 좋은 평생 쓸만한 공구들과 부속품이 내 DIY 수납장에 가득 쌓여 있음. 실제로 몇 번 사용하거나 아예 안 쓴 것도 꽤 많음
          + 이런 식으로 모은 공구가 참 많음. 한 번도 후회한 적은 없고, 대부분 내게 뭔가를 고치거나 만들 수 있는 능력을 선물해줌
          + 요즘 시대에는 괜찮은 열풍기 스테이션도 200달러보다 3-4배 더 저렴하게 살 수 있음. 기사 날짜를 다시 확인했을 정도로 10~15년 전에는 200달러 정도가 적당했겠지만 지금은 아님
     * 내가 제일 좋아하는 마우스는 Logitech Anywhere MX임. 사이즈는 작아도 엄청 편하고, 웹브라우징/파일 탐색/게임 무기 전환 등에서 옆에 있는 뒤로/앞으로 버튼이 정말 필수적임. 건전지 2개로 몇 달씩 가고 갈아끼우는 것도 쉽고, 동글도 작고 수신거리 너무 좋음. 스크롤 휠도 클릭/자유 스크롤로 바꿀 수 있어서 완벽함.<br>그래서 eBay에서 신품이랑 오픈박스 쓸어모아 쟁여두기까지 했음. 하지만 단점이 두 가지 있음.<br>1) 마이크로스위치가 2~3년 쓰면 맛감. 직접 교체 가능하지만 작업이 번거롭고, PCB 망가질 위험도 큼(실제로 해봤음)<br>2) 동글이 USB Type-A만 지원됨. Logitech이 USB-C 버전 수요를 무시함. 블루투스로 옮기고 싶었겠지만, 블루투스가 이미 널리 쓰이던 시기에 계속 Unifying 동글만 생산함. 이유를 모르겠음.<br>신제품인 Anywhere MX 2S는 그나마 좀 쓸만하지만,
       내장 배터리는 매번 충전해야 해서 불편하고, 이후 버전은 기능이 점점 구림. 게다가 지금 정가는 90달러임.<br>언젠가 오픈소스 Anywhere MX 복제품 만드는 게 내 사이드 프로젝트가 될 예정임. 커스텀 키보드 만드는 커뮤니티가 크고 다양한데, 혹시 마우스도 그런 게 있는지 궁금함
          + 나도 Anywhere MX 3를 좋아함. 손가락 그립에 최적화된 작은 사이즈, 충전 배터리 한 번 충전해서 한 달은 거뜬히 쓰고 급할 땐 커피 타기 전 충전하면 하루는 충분함. 무엇보다 스크롤 휠이 대박임.<br>마이크로스위치 문제는 치명적인데, 어디서 읽은 바로는 Logitech이 아주 낮은 전압/전류로 스위치를 구동해서 정전기 문제가 생긴다 함. 직접 실험해보니 왼쪽 버튼 옆에 따뜻한 숨을 불어주면 며칠간 괜찮아짐. 정말 바보 같은 임시방편이지만 같은 증상 있을 두 번째 마우스를 또 사기도 그렇고 별 방법이 없음
          + 온라인 마우스 커뮤니티는 잘 모르겠지만, AA 건전지를 선호한다는 게 인상 깊었음. 나는 1회용 알카라인 건전지를 싫어해서 간과했던 장점이더라. 저전압에 대응 가능한 기기는 니켈수소 충전지 꽤 괜찮고, 더 큰 파워는 14500/18650/21700 리튬배터리를 애용함.<br>하지만 개인적으로는 충전 케이블로 바로 충전하는 게 여분 건전지 구비하는 것보다 편함. 내장 배터리가 죽으면 직접 교체할 자신은 있음(모두가 그런 건 아님)
          + MX Anywhere 3S를 쓰고 있는데 아주 만족함. Bolt 리시버 하나로 마우스랑 MX Mechanical Mini 키보드 둘 다 연결할 수 있고, 다른 리시버로 두 장치를 모두 스위칭해 쓸 수 있는 점이 정말 매력임. 물론 장치마다 개별로 스위치 버튼 눌러줘야 하는 건 불편하지만, 그래도 이 조합은 다른 곳에서 본 적 없음.<br>나는 메인 리시버를 워크스테이션에 꽂고 Synergy로 맞은편 랩탑에 키보드/마우스 넘겨서 쓰고, 세컨더리 리시버는 NUC, Jetson 같은 디바이스를 테스트할 때 아주 유용함. 그런 베어메탈 환경에서도 작은 동글 하나만 꽂아두면 바로 키보드/마우스 세팅할 수 있음
          + 예전 Logitech 유선 마우스는 정말 오래 가는 내구성이었음. 90년대에 산 유선마우스가 컴퓨터를 세 번이나 바꿔가며 썼음. 요즘 모델은 너무 자주 고장나서 아예 브랜드 자체를 안 사게 됨
          + 나도 Anywhere MX를 스위치 나갈 때까지 정말 오래 썼음. 스위치 부품을 Mouser에서 사서 교체하려고 프로젝트 박스 어딘가에 넣어뒀는데 아직 못 꺼내봄
     * “내 최애 마우스는 MX Ergo”라는 글을 보고, 나도 Logitech MX Vertical로 바꿨더니 정말 좋았음. 섬세한 움직임에는 적응 시간이 좀 필요했지만 지금은 너무 자연스럽고 편함. USB-C 포트가 있고 Bluetooth 연결 3개를 버튼 하나로 전환 가능. 버튼 커스터마이징 앱은 있으나 굳이 안 씀. 앱 없어도 만족함. MX Vertical 공식 사이트
          + MX Vertical의 왼손잡이 버전이 없는 게 아쉬워서 최근에 왼손 대체 모델 여러 개를 테스트해봤고, 결국 이 제품 이 가장 만족스러웠음
          + 지금 3번째 MX Vertical 사용 중임. 1년쯤 쓰면 오른쪽 버튼이 고장나는 현상이 있음. Reddit 등 포럼에서도 이 증상 사례가 많은데, 그 외에는 최고의 마우스임
          + MX Vertical의 인체공학적 감각은 정말 최고임. 내 손에 너무 잘 맞아서 완벽함. 하지만 품질‧기능 면에서는 MX Master 시리즈에 한참 못 미침.<br>MX Master 3S로 바꿔서 손목 통증이 심하게 생겼었지만, 자석 스크롤 휠이나 빌드 퀄리티는 정말 환상이었음.<br>MX Vertical은 그에 비하면 장난감 수준인데 가격은 비슷함
          + 이 마우스를 쓰다 보면 정말 자연스럽고 손에 녹아든 느낌임. 최근엔 손목 통증도 거의 없어졌음
          + Sharkfin Squad! 기존 게이밍 마우스보단 정밀도가 떨어지긴 하는데, 난 어차피 업무용이라 큰 상관 없음. 너무 만족해서 아예 여행용 케이스까지 샀음.<br>게다가 이 마우스를 Samsung 스마트폰에서도 블루투스로 쓸 수 있었는데 정말 신기하고 좋았음
     * 나도 DIY 마우스 선택지가 더 많았으면 좋겠음. 기계식 키보드 커뮤니티는 정말 다양하고 풍성한데, 마우스도 맞춤 조립식 “레고 스타일” 부품처럼 – 원하는 버튼, 위치, 블루투스‧USB-C 등 보드 선택, 스크롤 휠도 원하는 스타일로 – 필요한 옵션 골라서 펌웨어 바로 업로드하고 어떤 PC에서든 사용할 수 있다면 정말 대박임. Logitech의 하이퍼스크롤 휠은 특허 때문에 DIY 버전은 제한돼 있지만, 대안들도 찾아보고 싶음
          + YT 채널 optimum에서 본인이 원하는 마우스를 직접 만들어 제품화까지 했음. 센서 PCB도 따로 판매되니 아이디어 얻기 좋음 해당 영상
          + 엄청 공감됨. 마우스 취향은 키보드 취향만큼이나, 어쩌면 그 이상 주관적임. 다양한 상용 모델이 있지만 완전히 딱 맞는 걸 찾지 못해 어쩔 수 없이 타협하는 사람이 언제나 생기는 구조임
     * pmm.gg에서 마우스를 사서 사용 중임. 기존 마우스부품을 이식하면서 무게가 절반 수준으로 가벼워짐(28g vs 60g). 기본적으로 프린터 용지 한두 장 느낌. 무게 때문은 아니고, 여기에서 세라믹 코팅된 마그네슘 스크롤 휠을 제공한 점이 특히 마음에 들었음.<br>내 피부가 아주 살짝 민감해서, 시중 스크롤 휠/마우스 측면의 고무재질이 쉽게 벗겨지고 망가짐. 여기는 마우스 껍데기도 같은 코팅 옵션이 있고 아주 만족함.<br>비쌉긴 해도 계속 마우스를 새로 사는 것보단 저렴함. 하루 종일 손에 쥐고 있는 물건인데 더 이상 타협하고 싶지 않았음. 품질 완전 좋고 조립도 쉬웠으며, 수평으로 끼우는 카본파이버 막대가 기본 모델보다 훨씬 더 단단하게 만들어줌
          + 고무가 쉽게 망가지는 건 피부질환 때문만은 아닌 듯함. 손의 자연스러운 유분이 플라스틱이나 고무에 배어서 시간이 지나면 팽창하다 결국 부서짐. 키보드 키캡도 그런 식으로 변색/광이 나는 듯. 마우스 휠도 다르지 않음
          + Razer 마우스 표면 코팅을 알코올로 쉽게 제거한 적 있음. 새 제품 때는 촉감이 괜찮았지만, 시간이 지나자 씹은 껌처럼 변해버려서 그냥 플라스틱으로 두는 게 나았음.<br>조립이 생각보다 간단해서 휠도 그냥 교체해버릴 수도 있음
          + 오히려 무거운 마우스가 더 좋음. 요즘 디바이스들이 너무 가볍고 싸구려처럼 느껴짐. 직접 동전 몇 개를 집어넣어 무게를 늘려보기도 함. 얼핏 보면 별스럽지만 결국 내가 하루 종일 사용하는 기기가 내 기준에 맞으면 그것이 전부임을 깨달음
          + 저 제조사는 “호구, 돈, 쉽게 떨어져나감”의 공식 적용 사례임. Finalmouse 같은 초경량 게이밍 마우스도 180달러인데, 여긴 100달러가 더 비쌈.<br>나도 아직 10년째 잘 쓰고 있는 Razer Ultimate가 있음, 출시가 100달러였음.<br>고무 파손은 뭘 바르는지 때문이고, 20달러짜리 그립 패드만 사도 해결 압.<br>마우스 구조가 불만이라면 쥐는 힘을 줄이면 됨...
     * 오픈소스‧오픈하드웨어 마우스 키트로 Ploopy 제품이 매력적임 ploopy.co. 최근 미국 배송을 중단했다고 들었음. 이유는 불분명함
          + 나도 Logitech 버티컬 마우스가 익숙했지만, Advantage2 키보드 옆 공간까지 계속 움직이다 손목 통증이 걱정되어 ploopy Adept 트랙볼로 바꿔봄. 트랙볼로 옮겨갈지 반신반의 했는데 완전 만족. 손이 더 이상 어색한 움직임을 할 필요 없어지고, 마우스질이 훨씬 편안해짐. 단, 미팅 도중 심심해서 공을 습관적으로 던지는 버릇은 이제 위험함—미국 배송이 막히면 대체품 구하기 어렵기 때문임(이 습관은 원래 좋지 않았음)
          + 혹시 기계식 키보드 세계를 잘 모른다면, QMK(Quantum Mechanical Keyboard)는 오픈소스 커뮤니티 중심 설정 툴로 키보드/입력장치의 키 레이어, 기능, 매핑 등을 자유롭게 설정할 수 있게 해줌
          + 요약하면, 미국 정책 변경이 계속 애매하게 바뀌어서 수출입이 현실적으로 불가능한 상황임 관련 기사
     * MX Ergo S는 USB-C 포트랑 훨씬 조용한 스위치가 있고 그 외엔 기존 MX Ergo와 같음. 예전 Ergo를 떨어트려서 고장나고 얼마 전에 새로 삼
          + 이렇게 신형 모델이 존재한다는 걸 처음 알게 됨. 진짜 전혀 몰랐음
     * 글과는 조금 다르지만, 글에서 언급된 솔더링 보조장치(Omnifixo)를 나도 쓰고 있음. YouTube의 Norm(테스트드 채널)에서 보고 알게 됐고, 처음엔 반신반의했으나 솔더링이 훨씬 쉬워져서 강력 추천함. 마우스 PCB 개조(불량 스위치 교체 등)도 해봤음. 제일 큰 불편은 쓰루홀 부품 리솔더링임. 마우스용 교체 PCB는 거의 희귀하고, 내가 좋아하는 마우스는 “마우스 커뮤니티”에서도 인기가 별로 없음
     * 1994년부터 트랙볼을 써옴. 데스크탑 퍼블리싱이나 포토에디팅 할 때, 원하는 위치에 정확히 올려놓고 손만 떼서 클릭하거나 마우스다운을 해야 할 땐 트랙볼이 최고임. Kensignton에 스크롤휠 링이 들어간 제품이 있는데 XY 화질은 완벽함. 클릭 버튼 내구성은 평범해서 교체가 아쉬움. 물리적 미들버튼 없이 소프트웨어 에뮬레이션만 쓰는 건 좀 짜증남
     * “이 프로젝트 덕에 전자폐기물 줄인다”라고 써놓고, 몇 문단 뒤에 PCBWay에서 10개나 주문해서 예비용으로 남겨뒀다는 말이 있음. 취미로 이런 걸 두고 뭐라 할 사람은 없지만, 서랍 속에 들어갈 PCB 9장이면 새로운 마우스 한 개 안 산 전자폐기물 절감 효과가 다 상쇄되는 듯한 느낌임
"
"https://news.hada.io/topic?id=22716","Claude Code로 5명 팀처럼 개발하는 법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Claude Code로 5명 팀처럼 개발하는 법

     * AI 도구 Claude Code는 단순한 코드 생성기가 아니라 동료에게 업무를 위임하는 듯한 경험을 제공하는 생산성 도구
     * 반복적인 구현 대신 시스템 설계, 제품 사고, 커뮤니케이션 같은 인간 고유 역량에 집중할 수 있는 환경을 제공
     * 병렬 작업, 다단계 디버깅, GitHub 통합을 통해 소규모 인력으로도 대규모 개발팀 수준 산출물을 내는 효과가 있음
     * 다만 과잉 테스트 작성, 단순 업무 과잉 처리 등 제한점과 성격적 특이성이 존재해 사용자가 관리해야 함
     * 결과적으로 이는 개발자의 역할을 구현자에서 지휘자로 이동시키며 주니어 개발자 교육, 시니어의 생산성 향상, 비개발자의 프로젝트 실행까지 폭넓은 가능성을 열어줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI가 작성하는 코드와 개발자의 역할 변화

     * 최근 두 달간 작성된 모든 코드는 사람이 아니라 Claude Code가 직접 작성한 것임
          + 사용자는 구현 대신 아키텍처 설계와 결과 정의에 집중
          + 반복적이고 세세한 타이핑은 점차 불필요해지는 흐름
     * 이 과정에서 개발자의 가치는 제품 기획, 시스템적 사고, 미학적 판단으로 이동하고 있음

다단계 디버깅 능력

     * 큐 작업 실패 문제에서 Claude Code가 수천 줄의 외부 라이브러리 코드를 분석해 원인을 찾아냄
          + 개발 환경과 운영 환경의 큐 이름 불일치 문제를 해결
     * 이는 일반 개발자가 몇 시간 혹은 며칠 걸릴 문제를 단시간에 해결 가능하게 한 사례

오케스트라 지휘자처럼 일하기

     * 여러 Claude Code 인스턴스를 병렬 실행해 다중 기능을 동시에 개발 가능
          + 각 작업은 별도 git worktree에서 진행되어 충돌 방지
     * 개발자는 직접 코드를 짜는 대신 작업을 지휘하고 리뷰하는 관리자의 역할을 맡음
     * 이를 통해 체력이나 집중력이 떨어질 때도 효과적으로 진행 가능

일상적인 활용과 마찰 최소화

     * Cursor, Copilot 같은 IDE 기반 도구와 달리 Claude Code는 특정 환경에 묶이지 않음
     * CLI, git, tmux 등 기존 개발자 워크플로와 매끄럽게 연동
     * 주요 명령어:
          + /issues → GitHub 이슈 생성
          + /work → 이슈 기반 개발 및 PR 생성
          + /review → PR 리뷰 및 개선
     * 이로써 연구, 구현, 리뷰 과정의 마찰을 최소화함

한계와 개성

     * 때로는 테스트를 지나치게 많이 작성하거나 단순 작업을 복잡하게 처리하는 과잉 행동을 보임
     * 잘못된 방향으로 진행될 때는 즉시 중단 가능
     * 장점은 반복적인 스타일 수정 등 개발자가 귀찮아할 일도 기꺼이 수행한다는 점

주니어 개발자와 학습

     * 주니어 개발자는 Claude Code를 끊임없이 질문할 수 있는 멘토로 활용 가능
          + ""내 PR의 문제점은 무엇인가?""
          + ""Python과 Ruby 접근 방식 차이는?""
          + ""언어별 함정과 주의점은 무엇인가?""
     * 이를 통해 성장 속도와 실무 기여도가 크게 향상됨

실제 워크플로 예시

     * 오전 9시: 버그 보고서를 Claude Code에 전달 → 재현 및 GitHub 이슈 자동 생성
     * 9시 20분: 4개의 탭에서 병렬로 다른 작업 수행 (버그 수정, PR 리뷰, changelog 작성, 배경 작업 조사)
     * 10시~11시: 각 PR 자동 생성, 문서 및 에러 처리 포함
     * 11시 30분: 사람이 최종 리뷰해 UX와 코드 스타일 보정
     * 11시 45분: 고객 피드백을 분석해 이슈로 자동 변환

결론과 권장 대상

     * 두 명의 팀이 매달 $400을 투자해 대규모 팀 수준의 산출물을 달성
     * 권장 대상:
          + 구현 대신 전략과 품질에 집중하고 싶은 시니어 개발자
          + 더 많은 성과를 내고 싶은 팀 리드
          + 비개발자 창업자 및 초보 개발자
     * 입문은 월 $20 구독으로 가능하며, 실제 프로젝트를 맡겨보는 것이 학습곡선 단축의 핵심
     * 코딩의 미래는 직접 구현이 아니라 결과 지휘와 위임으로 이동하고 있음

   저는 워크트리만 잘 사용하기만 해도 좋은거 같더라구요

   저렇게 하려면 Pro로는 절대 안되겠어요.
   비개발자로 iOS 앱 만들고 있는데 Pro는 한도 도달 시점이 너무 빨리옵니다.
   매번 2시간을 못넘고 끝나네요.

   한편으로는 한도 도달이 곧, 작업 종료 시점이라 좋은 것 같기도 합니다...
   (오늘은 여기까지,,, 느낌으로ㅋㅋ)

   Max$100 짜리 사용하면 그래도 널널하더라구요. 비개발자는 아니지만 앱 개발은 처음인데 한달동안 사용해봤거든요. 지금까지 대략 $566.93 를 사용했더라구요

   오전에 2~3시간 하니 점심 전에 리밋 걸렸네요 (Pro 사용자)
   3시부터 리셋된다고 나오는데 Max 아니면 하루종일은 못쓸듯 (Max도 한도 도달하는게 그렇게 어렵지는 않을 것 같긴 합니다)

   자동 2시간짜리 뽀모도로 느낌이네요 ㅎㅎ

   20달러 pro 플랜에서 다들 추천하는 방식인 plan mode로 상세 plan을 짜도록 분석시킨 다음 edit 모드로 넘어가는 식으로만 굴려도 금방 limit 도달하더라고요.
   확실히 코드 품질은 좋아지긴 하는데 멍청하게 edit 모드로 처음부터 시작할 때보다 토큰 소모가 세배는 빠른 느낌입니다.

   무료플랜에서는 사용 불가능하죠?

   100달러짜리 사용하면 그나마 커버할수 있을거 같아요.

   20불짜리로도 힘들어요 저렇게 ""최근 두 달간 작성된 모든 코드는 사람이 아니라 Claude Code가 직접 작성"" 이런 식으로 쓰려면
   일단 200불 태우고 리밋 걸리면 또 200불 써야하지 않을까요

   저도 100불짜리 사용하고 있습니다. 코드를 작성하기 전 Plan 을 Opus 모델로 작업하고, 실제 코딩은 Sonnet을 사용합니다. 최근 두달정도 거의 모든 코드(최소 수 천 line)을 Claude Code 가 직접 작성했는데, limit 이 걸린 적은 거의 없습니다. 실수로 Opus 로 코드를 작성을 한 경우가 아니면 아예 없었습니다.
   현재는 최근에 나온 Opus Plan Mode 를 사용중인데, 해당을 사용한 이후로는 Approaching limit 경고 문구도 거의 나오지 않습니다.

   이미 동일하게 일 하는 중~
   커뮤니티에서 투표를 해보면
   8-90% 가 이렇게 바뀌어가는 중인 듯 보임
"
"https://news.hada.io/topic?id=22678","지금 무슨 일이 일어나고 있는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           지금 무슨 일이 일어나고 있는가?

     * 소프트웨어 업계에서는 엔지니어 번아웃이 심화되고 있으며, 특히 주니어 엔지니어들이 AI 도구 남용으로 코드 품질과 협업에 문제를 일으키고 있음
     * 시니어 엔지니어의 피드백은 학습 기회가 아닌, AI에게 넘길 새로운 프롬프트로 사용되며 “AI가 작성한 코드” 가 팀 전체 리뷰를 소모하게 함
     * 일부 조직에서는 AI가 만든 불완전한 코드를 ‘성과’처럼 포장해 발표하며, AI 의존을 장려하는 분위기가 형성됨
     * 저자는 직접 경험을 통해 AI 코드 답변을 받았을 때 불쾌감과 위화감을 느꼈고, AI가 오히려 학습과 멘토링 문화를 훼손한다고 비판
     * AI 스타트업 생태계도 결국 비경제성, 전력 소모, 환경 문제 때문에 지속 불가능하며, 현 상황은 “** 황제가 벌거벗고 있다**”는 사기극과 다름없다고 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 불안한 엔지니어링 환경

     * 최근 엔지니어들 사이에서 번아웃 현상이 심화하고 있음
     * 조직에서는 시니어 엔지니어에게 실질적으로 동작하지 않는 “분위기(밈) 기반 기능” 을 검토하고 기여하길 기대함
     * 내 경험에 따르면 최고의 엔지니어는 늘 새로운 팀원이 성장하도록 열의를 갖고 도움을 주고 싶어함

     * 하지만 이들의 피드백이 성장의 기회로 사용되는 대신, 초급 개발자들은 이를 단순히 생성형 AI로 보내는 다음 프롬프트로만 활용함
     * 실제로 많은 주니어 엔지니어가 LLM(대규모 언어 모델) 도구를 (남용 수준으로) 사용하는 사례를 직접 목격함

조직 내 실제 사례: AI 남용의 폐해

     * 최근 회사 타운홀에서 주니어 엔지니어들이 새 작업물을 시연하는 모습을 봄
     * 기능의 목적이나 작동 방식조차 제대로 이해하지 못한 모습이었음
     * 하지만 규모가 큰 조직에서는 실제 결과와 무관하게 “성공”을 연출하는 데 집중함
     * 한 시니어 매니저가 이들의 AI 활용 사례를 공개하자 “이것은 Claude가 작성한 4천 줄짜리 코드임”이라며 당당히 설명했고, 박수갈채를 받음

     * 나 역시 기존 기능의 소규모 개선 요청을 받아 코드를 검토하다가 최근 변경한 주니어 엔지니어에게 컨텍스트를 요청함
     * Github 커밋 URL을 보내 질문했지만, 해당 내용을 LLM에 입력한 뒤 반환된 답을 복사해 보낸 것으로 추정됨
     * 이 과정에서 묘한 위화감과 불편함을 느낌

AI 슬로프와 코드 리뷰의 한계

     * 친구의 사례를 통해, 한 달 동안 여러 명의 엔지니어가 LLM이 자동 생성한 코드(vibe-coded PR)를 검토하고 병합하려는 시간 낭비가 실제로 벌어지고 있음을 확인함
     * 또 한 친구는 AI가 만든 “엉성한 코드”를 반복적으로 리뷰하느라 소진된 경험을 토로함
     * AI 덕분에 코드 품질 개선이나 학습이 이뤄지지 않고, 단순 반복 노동만 늘어남

개발 문화와 인간적 성장의 진정한 가치

     * 모든 엔지니어는 동료와 멘토 덕분에 한 단계씩 성장함
     * 직접 가르치고 성장시키는 것이 소프트웨어 엔지니어링 문화의 본질임
     * 하지만 이런 투자도 결과물이 곧바로 “최신 모델”의 학습 데이터로 복사되는 현실에 회의감이 듦
     * 그렇다면 차라리 주니어 엔지니어 대신 모델만 학습시키는 것이 나은가에 대한 근본적 물음을 던짐

     * 그런 세상은 매우 암울한 비전임.

AI를 사용하지 않는 실험과 결론

     * 직접적으로, “AI 사용을 멈춰보라”는 실험 제안을 함
     * 본인도 최근 컴퓨터를 초기화하면서 Claude Pro 구독을 중단함
     * 몇 번의 검색과 Stack Overflow, 공식 문서를 읽는 과정이 오히려 훨씬 신뢰할 수 있는 결론 도출을 가능케 함
     * LLM이 내놓는 결과보다 내 판단이 정확성과 신뢰성 면에서 우월하다는 생각을 하게 됨.

생성형 AI 툴의 경제적 가치, 그리고 본질적 한계

     * “AI가 정말로 쓸모가 있는가?”라는 질문을 던짐
     * 객관적으로 보면, 그 가치에 큰 의문이 제기되는 상황임
     * AI 스타트업의 전형적 과정은 다음과 같음:
          + “AI”가 기존 영역에 적용되고, 효율성 명분으로 신생 기업이 등장함
          + AI 스타트업은 벤처 자본으로부터 투자 유치에 성공함
          + AI 서비스 제공 기업(OpenAI 등)에게 사용료 지불
          + AI 스타트업 자체는 수익을 내지 못함
     * 이 과정 자체만 놓고 보면 기존 VC 생태계와 큰 차이가 없지만, 핵심 차이는 서비스 제공사(OpenAI 등)조차 아직 수익을 내지 못함에 있음
     * 기술 자체가 본질적으로 비효율적이며, 대량 확장에 불리한 구조임
     * 지나치게 많은 전기 소모와 환경적 부작용도 심각한 문제임

맺음말: 현실 인식의 필요성

     * Moore의 법칙이 되살아나거나, 우주가 식기 전에 모두가 부자가 되는 희망을 기원할 수도 있음
     * 하지만 현실을 직시할 때, 생성형 AI 사업은 일종의 환상이자 “벌거벗은 임금님” 현상임

   기술의 최첨단인 핵폭탄 세계대전 후엔 인류가 다시 원시 시대로 돌아갈거라는 우려가, 지금 sw개발 분야에서는 현재 진행형입니다.

   과한 바이브코딩만 멈추면 될것같은데 말이죠.어시스턴트와 일부 세세하면서도 단순한 알고리즘 작성에선 이만한게 또 있나 싶을 정도입니다.

        Hacker News 의견

     * AI를 조직에 도입하는 것은 단순히 기술적 문제가 아니라 변화 관리 문제임을 강조함. 신뢰와 투명성을 바탕으로 한 유능한 팀이 인간의 전문성과 LLM의 강점을 균형 있게 결합하는 프로세스를 만들어야 진짜 효과를 볼 수 있음. 소규모 팀이 AI로 큰 성과를 내는 사례도 생기고 있음. 하지만 대부분의 조직, 특히 대기업은 건강한 조직문화를 갖추지 못해 AI가 오히려 이 독성을 증폭시키는 현상이 나타남. 기업 임원 중에는 ""Story Point""를 단순히 시간 단위로 오해해 AI가 모든 걸 반으로 줄여주는 도구로만 바라보는 경우도 있음. 근본적으로는 유지보수가 가능한 소프트웨어를 만드는 과정 자체와 동떨어져 있어 AI를 얼렁뚱땅 수익만 올려주는 통로로 인식함. 최근 AI 파일럿 프로젝트의 95%가 ROI를 달성하지 못했다는 연구 결과 역시, 현대 경영진의 무능함을
       보여주는 사례임
          + 어쩌면 AI가 과대평가된 것뿐임을 지적함. ""소규모 팀이 AI로 큰 성과""라는 주장은 어떤 구체적인 팀 얘기하는 건지 궁금함을 밝힘
          + AI의 문제를 단순히 ""애초에 있던 문제일 뿐""이라며 면죄부를 주는 태도에 지침을 전달함. AI로 인한 정신 건강 악화나 조직 문화 문제도 도구 자체의 책임이 있다고 봄. 도구와 시스템이 무책임하다는 생각과 달리, 실제로 도구와 환경 역시 영향을 준다고 생각함
          + ""소규모 팀이 큰 일을 한다""는 주장에 구체적인 사례 없이 성공담만 들려 아쉽다는 생각임
          + 관리자 조직이 이미 엉망인 곳에 AI를 도입하는 것은 바이킹 무리에 자동소총을 쥐여주는 것과 같다고 봄. 조직이 망하는 시기를 앞당기는 역할만 할 뿐임. 기술이 문제의 핵심이라기보단, 다수 구성원(또는 소수 핵심 관리자)의 사회적∙윤리적 실패가 진정한 원인임을 강조함
          + ""Story Point""를 시간으로 오해하는 경영진이 많다고 언급하며, 지금까지 만난 모든 역할(개발, QA, PM, 임원)에서 이 실수를 본 경험임
     * ""Prompstitudes(프롬프트에만 의존하는 직장인)""의 등장에 대해 이야기함. 동료가 내 의견을 유추한 ChatGPT의 답변만 던졌던 적이 있고, 마치 기사에서 말하는 ""침범당한 기분""을 느꼈음. 이들은 무능하다기보다 LLM에 너무 의존해, 마치 슬롯머신만 계속 돌리는 카지노의 노인과 같다고 느낌
     * 최근 동료와 대화할 때 명확히 ChatGPT의 결과가 돌아왔다는 사실 때문인지 찝찝함을 느낀 경험을 공유함. 차라리 무시당하는 게 나았다고 생각함. 특히 LLM이 자신만만하게 틀린 소리를 강하게 주장해서 더 문제였음. 작은 부분(예: 설정과 구현에서 이름만 조금 달라도)이 LLM을 완전히 헷갈리게 할 수 있음. 인간과 달리, LLM이 실수에 대해 배우거나 깨닫지 않으니 지속적으로 잘못된 방향으로 흘러가는 현상이 있음. 차라리 나쁜 인간 코드와 씨름하는 게 마음적으로 낫다고 느낌
          + 예전에 AI로 PRD 쓰는 PM과 일한 적 있음. 내용을 물어보면 ""본인이 모른다, AI가 쓴 거라서""라고 답했음. 결국 PM은 실제 아이디어 전달은 포기하고, 문서 작성 퍼포먼스만 하게 됨. 요구 사항의 이해와 해석까지 전부 내 몫이 돼서 팀을 나옴
          + ""LLM이 자신감 넘치게 틀렸다""라는 부분에 공감을 표시함. 주변에 아는 척 카리스마 있는 동료처럼, LLM도 그럴듯하게 틀린 말을 하는 경우가 많음
          + 이번 주에 매우 기이한 경험을 했음. 자기 자신도 잘 모르는 전문 사양 관련해서 Claude에게 내부 제안을 검토하게 했더니 많은 오류를 지적했음. 그걸 해당 사양의 사내 담당자에게 ""이건 LLM이 제안한 거라 헛소리일 수도 있다""라고 전달했더니, 그 사람도 LLM에 내 메시지를 넣고 답변을 받아 다시 내게 보내 물어봄. 결과적으로 우리 모두가 AI의 어시스턴트 역할만 하게 됐다는 생각임. 이런 미래가 소프트웨어 개발의 현실이라면 내키지 않음
          + 나쁜 인간 코드가 나쁜 LLM 코드보다 훨씬 낫다고 생각함. 인간 코드는 뭘 하려고 했는지 맥락이라도 추론 가능함. 반면, LLM이 만들어낸 코드는 처음부터 끝까지 망가져 있을 때가 있고, 아예 존재하지도 않는 함수나 개념을 막 만들어내기도 함. 인간은 보통 이 정도로 현실과 동떨어진 코드를 만들지 않음. LLM 코드를 이해하려면 코드베이스 전체를 다시 배워야 할 정도임
          + LLM이 ""자신이 실수하지 않는다고 믿는다""는 표현에 대해, LLM은 애초에 믿거나 생각하거나 느끼지 못하는 존재임을 지적함. 그저 통계적으로 가장 그럴듯한 언어 토큰을 이어붙이고, 약간의 랜덤 요소를 더해 창의성을 흉내낼 뿐임
     * ""AI 도구가 정말 쓸모가 있을까?""라는 질문에 대해, 본인은 남들과 다르게 써서 도움이 된다고 생각함. 1983년부터 개발을 했고, 현재는 은퇴해서 혼자 작업하는 경우가 많음. 여러 도구를 써봤으나 지금은 ChatGPT와 Perplexity만 활용함. 직접 코드를 짜게 하진 않고, LLM이 제시한 코드를 참고하며 시작점으로 활용함. 가끔 통째로 쓰기도 하지만, 대부분 수정과 재작성 과정을 거침. LLM이 점점 더 못된 결과를 낼 때는 그냥 끊고 새 접근을 시도함. 이 흐름 속에서 초보 엔지니어가 LLM 코드만 따라쓴다고 상상하면 떨림. 본인에게 가장 큰 가치는 ""즉시 대응해주는 StackOverflow"" 같은 느낌임. 어떤 바보 같은 질문도 부끄러움 없이 물어볼 수 있고, 빠르게 괜찮은 답을 얻을 수 있음. 최근 iOS에서 PassKey 구현을 배우면서 ChatGPT 예제 코드를 그대로 시작점으로 삼아 한 줄 한 줄
       이해하며 공부했음. 처음 썼던 코드와 지금의 완성 코드는 완전히 달라졌고, 이 과정을 통해 기술 이해가 깊어졌음
          + 본인도 똑같이 AI를 활용함. 아무것도 몰랐던 개인 프로젝트를 거의 마무리해가는데, 이제는 코드베이스를 충분히 이해하는 중임
          + 본인도 소규모 업무나 개인 프로젝트에서 비슷하게 씀. LLM이 첫 번째로 ""버리는 코드""를 써주고, 그 한계를 탐색하며 문제 도메인을 더 잘 이해할 수 있음. 결국 더 자신감 있게 직접 구현할 수 있게 됨
     * LLM이 기술 질문에 답하거나 새로운 접근을 제안하는 데 매우 뛰어나다고 느낌. 초보자라도 stackoverflow처럼 평가받거나 벽에 부딪히지 않고 자유롭게 질문 가능함. Copilot은 자동완성 기능이 뛰어나 코드 작성 속도를 높이고, 문서 주석이나 코드 라인을 자동 완성해줌. 이런 작은 도움들은 쉽게 검토 가능함. 그러나 LLM에게 복잡한 코드를 통째로 맡기면 혼돈이 발생하고, 오히려 디버깅에 시달리는 경험이 있음. 초보자가 LLM에 지나치게 의존하면 제대로 된 개발 역량을 키우기 어렵다고 생각함
     * 개인적으로 Zed를 취미 개발에 사용하는 이유는 AI가 지나치게 똑똑한 척 나서지 않기 때문임. 필요할 때만 AI 기능을 부드럽게 호출할 수 있고, 평소엔 그냥 내가 코딩함. 직장에서는 VSCode AI 때문에 방해를 너무 많이 받음. 두 가지 점에서 문제임: 첫째, 인터랙션이 너무 깨지기 쉬움(Popup 클릭, 실수로 거대한 자동완성 삽입), 둘째, 흐름이 끊긴다는 점임. AI 자동완성이 유용할 때도 있지만(약 1/3 비율), 나머지 시간에는 본래 생각 흐름이 깨지고 AI 결과를 확인하느라 집중이 흐려짐. Zed에서는 이런 문제가 없어서 다시 프로그래밍의 즐거움을 되찾았다고 생각함. 결국 문제는 AI 기능 그 자체보다 구현 방식에서 발생함
          + 본인도 Zed에 깊이 공감함. JupyterLab이나 Kate에서 놀다가 Zed를 쓴 후로 바뀌었음. Zed는 IDE/에디터가 중심이고, AI나 Jupyter 커널 등 부가 기능은 필요할 때만 조용히 지원하는 느낌임. 이런 추가 기능이 본연의 텍스트 편집/코딩을 방해하지 않음. Zed 팀이 좋은 균형을 잡았다고 생각함
     * AI는 UX 프로토타입 만들기에 아주 유용하다고 느낌. 짧은 시간 안에 클릭 가능한 결과물을 바로 만들어서, 여러 번 반복하며 방향만 잡고 나중엔 이런 코드는 버리고 새로 개발함. 이 방식이 잘못된 방향으로 일찍부터 많은 시간을 낭비하지 않게 도와줌. 다만, 아직 AI로 유의미한 앱 전체를 통째로 만드는 건 멀었다고 생각함
          + 평소에 잘 다루지 않던 영역(예: 파워쉘 스크립트)에서 AI의 도움을 많이 받음. 예전에 레지스트리 설정 리포트용 스크립트가 필요했을 때, Claude가 완벽하게 짜줘서 한 시간 아낀 경험 있음
          + 본인도 비슷하게 AI가 오류 설명에 탁월하다고 느낌. 정확한 해결책을 찾거나 새로운 아이디어를 떠올리게 해줘 많은 도움이 됨
          + ""프로토타입을 버리고 새로 개발""이 중요하다고 언급하는데, 현실에서는 PM이 이 부분을 잊어버려 프로토타입이 실제 서비스에 들어가는 일이 많다고 지적함. 그래도 본인이 잘 쓰는 방식이 있다면 그건 좋은 일임
          + 이 사용 사례와 프로세스, 도구에 대해 더 자세히 듣고 싶다고 질문함. 본인과 팀에게 실질적으로 도움 될 수 있을 것 같기 때문임
     * AI는 본인에게 그저 하나의 도구일 뿐이라고 봄. 본인은 하이레벨 개발자가 아니지만, 개인 프로젝트 중 막히는 부분에서 AI에게 아이디어와 피드백을 요청하는 방식으로 사용함. 중요한 점은, 코드 작성은 AI에게 맡기지 않는다는 것임(아주 단순한 보일러플레이트 정도만 제외). 내가 직접 코드를 짜는 것은 문제 해결과 창작, 그리고 배우는 과정에서 얻는 기쁨 때문임
          + 최근 프로젝트는 AI가 직접 코드를 짜주지 않았다면 완성하지 못했음. 전체 리포지토리 셋업부터 PoC까지 엉성하더라도 가능하게 해줌. Django, JS, 웹개발 경험 없는 상태에서 AI 덕분에 처음엔 제대로 된 것은 아니지만 동작하는 결과를 얻고, 점진적으로 개선하며 이해도를 높이는 중임
     * 최근 동료 코드 리뷰 중 ""prepareData""라는 다차원 배열을 섞고 필터링하는 복잡한 함수를 봤는데, 해당 동료에게 ""이게 무슨 역할이냐""라고 물으니, 시간을 아끼라고 LLM에 물어보라고 하여 당황스러웠던 에피소드임. 코드 리뷰를 위한 가장 기본적인 질문조차 답하지 않는 태도에 실망함
          + LLM에 물어서 대답을 그냥 동료에게 돌려주고, 20번 피드백 주고받으면 그가 이해를 못할 때 너도 LLM에 물어보라고 하면 되지 않겠냐고 약간 유머러스하게 제안함
     * 10년 후 신입 개발자들이 직접 코드 쓰는 경험 없이 바로 시니어가 되려는 현상을 우려함
          + 사실 이 현상은 AI 이전부터 시작됐음. 10년 이상 경력이 없으면 입사도 어려운 구조였고, 젊은 세대의 업스킬링에 업계가 실패함. 회사에서 꾸준히 신입 양성하려 해도 위기 때마다 신입부터 정리해고하고 다시 시니어만 급하게 뽑는 악순환이 반복됨
          + 이제는 시니어가 3년 차만 되어도 된다는 농담과 함께, 10년이 아니라 3년이면 금방 스태프 개발자가 된다는 분위기를 언급함
          + ""Vibe coding""이라는 개념이 9개월 전부터 등장했고, 앞으로 2년도 안 돼서 사람들이 코드를 직접 쓰거나 유지보수하지 않게 될 것 같다는 전망임
          + 그래도 전문성을 가진 개발자가 쓴 소프트웨어는 항상 존재할 것이고, LLM 코드가 완벽하지 않은 한 고품질 코드에 대한 수요는 계속될 것임
          + 주니어 개발자가 너무 많고 실제 경험을 얻을 가치 있는 문제가 많지 않기 때문에, 이들이 다음 단계로 성장하기 어렵다고 봄. 예전엔 저렴하게 PoC나 스크립트 작업이라도 맡길 수 있었지만, AI가 그런 역할을 적당히 해내는 지금은 기회가 줄고 있음. 그때도 주니어는 많고 자리는 부족했다고 첨언함

   초기 개발 단계에서 환경구축, 작은 function 단위의 module 개발에서는 ai가 매우 효과적이지만, 이외 코드와 프롬프트를 때려 넣는 vibe coding 은 유지보수 관점에서 재앙이다. 처음 몇번은 성공할지 몰라도, 결국에 문제가 발생할때 마다 AI가 자신의 문제를 해결해줄 때까지 N번 시도해봐야하고, 해당 솔루션이 다른 어떤 버그를 유발할지 모르는 공포가 지속된다.

   개발자의 능력에따라
   기본기가 있는 사람이 쓰면 ai활용하여 고품질개발이가능하고
   기본기가없으면 배가 산으로 감
   요리사가 기본기가 있고 없는것의차이
"
"https://news.hada.io/topic?id=22703","이메일 스타트업 공동묘지: 왜 대부분의 이메일 스타트업은 실패하는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 이메일 스타트업 공동묘지: 왜 대부분의 이메일 스타트업은 실패하는가

이메일 스타트업 실패 매트릭스

     * 이메일 스타트업 다수는 Amazon SES나 Postfix 같은 기존 인프라 위에 단순 UI를 얹는 형태였음
     * Skiff, Sparrow, Email Copilot, ReplySend, Nveloped, Jumble, InboxFever 등은 모두 실패하거나 인수 후 종료
     * YC와 Techstars가 배출한 이메일 스타트업 대부분은 피벗(pivot)하거나 조기 종료
     * 인프라를 직접 구축하지 못한 서비스는 단기 생존에 그침

인프라 현실 점검

     * 대부분의 이메일 스타트업은 실제 서버를 구축하지 않고 앱이나 클라이언트만 개발
     * 성공한 회사들은 SendGrid, Mailgun, Postmark처럼 SMTP API·전달 인프라를 제공
     * 프로토콜을 바꾸려 하기보다는 기존 워크플로 강화가 성공 패턴

왜 대부분의 이메일 스타트업은 실패하는가

     * 1. 프로토콜은 잘 작동하지만 구현은 어렵다
          + SMTP, IMAP, POP3는 수십 년간 검증됨
          + 문제는 새로운 프로토콜이 아니라 구현 품질
     * 2. 네트워크 효과는 절대적이다
          + 이메일은 40억 명 이상이 사용, 모든 플랫폼과 호환
          + 교체 비용이 높아 다른 서비스로 전환하기 어려움
     * 3. 잘못된 문제를 겨냥한다
          + “이메일은 복잡하다”, “AI가 필요하다”, “보안이 약하다” 등 잘못된 가정
          + 실제 중요한 문제는 전달 안정성, 스팸 필터링, 개발자 도구
     * 4. 기술 부채가 크다
          + SMTP 서버 운영, 스팸 대응, 대용량 저장소, 인증·전달 인프라 모두 구축이 까다로움
     * 5. 인프라는 이미 존재한다
          + Amazon SES, Postfix, Dovecot, SpamAssassin 등 오픈소스·상용 인프라 풍부

이메일 스타트업 실패 사례 연구

     * Skiff 사례
          + “프라이버시 우선 이메일 및 생산성 플랫폼” 으로 포지셔닝 하며 상당한 벤처 캐피털 투자 유치
          + 2024년 2월 Notion이 Skiff를 인수하며 통합·지속 개발을 약속했음
          + 실제로는 인수 후 수개월 만에 즉시 서비스 종료하고, 창업자들은 Notion을 떠나 Cursor로 합류
          + 수천 명의 사용자가 강제로 서비스 이전
     * 액셀러레이터 별 분석
          + Y Combinator: 이메일 앱 팩토리
               o Emailio (2014): 모바일 이메일 클라이언트 → 웰니스로 피벗
               o MailTime (2016): 채팅형 이메일 → 분석 서비스로 피벗
               o reMail (2009): iPhone 이메일 검색 → 구글에 인수 후 종료
               o Rapportive (2012): Gmail 소셜 프로필 → LinkedIn 인수 후 종료
               o 성공률: 일부 인수 성공(reMail, Rapportive) 사례가 있으나 다수는 피벗 또는 인재 인수(acqui-hire)로 종료
          + Techstars: 이메일 공동묘지
               o Email Copilot (2012): 인수 후 종료
               o ReplySend (2012): 완전 실패
               o Nveloped (2012): “Easy. Secure. Email” → 실패
               o Jumble (2015): 이메일 암호화 서비스 → 실패
               o InboxFever (2011): 이메일 API → 실패
               o 패턴: 모호한 가치 제안, 실질적 기술 혁신 부재, 빠른 실패
     * 벤처캐피털의 함정
          + VC Funding Paradox: 이메일 스타트업은 단순해 보이지만 실제로는 불가능에 가까움
          + 투자자들을 끌어들이는 전제 자체가 실패를 보장하는 구조
          + 현실: 이메일 인프라와 프로토콜은 이미 견고하며, 새로운 스타트업이 이를 대체하기는 불가능

현대 이메일 스택의 기술적 현실

     * 대부분의 이메일 스타트업은 자체 인프라를 새로 구축하지 않고, 기존 이메일 서버·프로토콜 위에서 클라이언트 애플리케이션을 얹는 형태
     * 이로 인해 기본적인 한계와 성능 문제가 반복적으로 발생하며, 스타트업 실패의 한 축을 담당함
     * 메모리 과다 사용 (Memory Bloat)
          + 현대 이메일 클라이언트는 주로 Electron 기반 웹앱으로 제작되어 RAM을 과도하게 소모함
          + Mailspring: 기본 이메일 동작만으로도 500MB+ 메모리 사용
          + Nylas Mail: 종료 전 1GB+ 메모리 사용
          + Postbox: 대기 상태에서조차 300MB+ 차지
          + Canary Mail: 메모리 문제로 빈번한 크래시 발생
          + Thunderbird: 시스템 메모리의 최대 90%까지 사용 보고 사례
          + Electron 성능 위기:
               o Electron, React Native 같은 크로스 플랫폼 프레임워크는 개발자에게 편리하지만, 리소스를 비효율적으로 사용
               o 결과적으로 단순 이메일 기능을 위해 수백 MB~수 GB까지 메모리를 잡아먹는 상황 발생
     * 배터리 소모 (Battery Drain)
          + 비효율적인 코드와 동작 방식으로 인해 모바일 및 노트북 환경에서 배터리 소모 심각.
          + 백그라운드 프로세스가 항상 실행 상태 유지
          + 수 초마다 불필요한 API 호출 발생
          + 연결 관리가 비효율적
          + 필수 기능 외에 불필요한 서드파티 의존 없음에도 리소스 낭비 심각

인수 패턴: 성공 vs 실패

     * 두 가지 패턴
          + 클라이언트 앱 패턴 (대부분 실패)
               o 이메일 클라이언트 애플리케이션은 인수 이후 대개 빠르게 종료됨
               o 새로운 사용자 경험 제공을 내세우지만, 인프라 의존성과 네트워크 효과 장벽을 넘지 못해 유지 불가능
          + 인프라 패턴 (종종 성공)
               o SMTP·API 같은 핵심 이메일 인프라를 제공하는 기업은 인수 후에도 성장하거나 플랫폼에 통합되어 지속적 성과를 냄
     * 최근 사례
          + 클라이언트 앱 실패
               o Mailbox → Dropbox → Shutdown (2013–2015)
               o Sparrow → Google → Shutdown (2012–2013)
               o reMail → Google → Shutdown (2010–2011)
               o Skiff → Notion → Shutdown (2024)
          + 예외적 성공
               o Superhuman → Grammarly (2025)
                    # 전략적 통합으로 성공적인 인수 사례. 이메일 클라이언트 분야에서는 드문 성공적 엑시트
          + 인프라 성공
               o SendGrid → Twilio (2019): 30억 달러 규모 인수, 이후 지속 성장
               o Mailgun → Sinch (2021): 전략적 인수로 통합
               o Postmark → ActiveCampaign (2022): 플랫폼 기능 확장에 기여
     * 클라이언트 앱은 인수 후 서비스 종료로 귀결되는 경우가 많지만, 인프라 제공 기업은 인수 후에도 살아남아 플랫폼의 핵심 요소로 자리잡는 경향이 뚜렷함

산업의 진화와 통합

     * 자연스러운 산업 발전
          + 이메일 산업은 시간이 지남에 따라 대형 기업이 소규모 회사를 인수하여 기능을 통합하거나 경쟁을 제거하는 형태로 발전해 왔음
          + 이는 부정적 현상만은 아니며, 대부분의 성숙한 산업에서 나타나는 자연스러운 발전 과정임
     * 인수 이후의 전환 이메일 기업이 인수되면 사용자들이 겪는 변화는 다음과 같음:
          + 서비스 마이그레이션: 새로운 플랫폼으로 계정과 데이터를 옮겨야 함
          + 기능 변화: 특화된 기능이 사라지거나 다른 방식으로 대체됨
          + 가격 조정: 구독 모델과 요금제가 바뀔 수 있음
          + 통합 기간의 불편: 서비스 통합 과정에서 일시적 장애나 중단 발생 가능
     * 전환기에 사용자가 고려할 점 산업 통합 시기에 사용자가 취할 수 있는 대응:
          + 대체 서비스 검토: 유사 기능을 제공하는 다른 공급자를 탐색
          + 마이그레이션 경로 파악: 대부분의 서비스는 내보내기 도구를 제공하므로 이를 활용
          + 장기적 안정성 고려: 오랜 기간 운영되고 신뢰성 있는 공급자를 선택하는 것이 유리

Hacker News 현실 점검

   모든 이메일 스타트업은 Hacker News에서 반복적으로 같은 피드백을 받음:
     * ""이메일은 이미 잘 작동한다, 이건 문제를 해결하지 않는다""
     * ""그냥 모두가 쓰는 Gmail/Outlook을 사용하면 된다""
     * ""또 다른 이메일 클라이언트, 2년 안에 서비스 종료될 것이다""
     * ""진짜 문제는 스팸인데, 이건 그걸 해결하지 않는다""
       핵심 인사이트: 커뮤니티의 지적은 정확함. 이메일 스타트업이 매번 같은 비판을 받는 이유는 해결해야 할 근본적인 문제는 늘 동일하기 때문임

현대의 AI 이메일 스타트업 열풍

     * 최신 물결 2024년에는 ""AI 기반 이메일"" 스타트업의 새로운 물결이 등장했으며, 이미 첫 번째 주요 성공적 인수가 이루어짐:
          + Superhuman: 총 $3,300만 투자 유치, 2025년 Grammarly에 인수 — 드문 성공 사례로 평가되는 클라이언트 앱 인수
          + Shortwave: Gmail을 기반으로 AI 요약 기능을 추가한 래퍼
          + SaneBox: 실제로 동작하는 AI 이메일 필터링, 하지만 혁신적이진 않음
     * 여전한 문제들 ""AI""를 붙여도 이메일의 근본적 문제는 해결되지 않음:
          + AI 요약: 대부분의 이메일은 이미 짧고 간결함
          + 스마트 답장: Gmail은 이미 수년 전부터 제공하고 있음
          + 이메일 예약 발송: Outlook이 기본적으로 지원
          + 우선순위 감지: 기존 이메일 클라이언트에 이미 효과적인 필터링 시스템 존재
            핵심 현실: AI 기능은 실제로는 상대적으로 작은 불편을 해결하는 데 비해 막대한 인프라 투자가 필요하다는 점에서 근본적 해결책이 되지 못함

실제로 성공한 이메일 사례들

     * 인프라스트럭처 기업 (성공한 사례들)
          + SendGrid: Twilio에 $30억 인수
          + Mailgun: 연 매출 $5천만 달러 이상, Sinch에 인수
          + Postmark: 수익성 있는 서비스, ActiveCampaign에 인수
          + Amazon SES: 수십억 달러 매출 기록
          + 패턴: 이들은 앱이 아닌 인프라를 구축했음
     * 이메일 서비스 제공자 (생존자들)
          + FastMail: 25년 이상 운영, 수익성 있는 독립 기업
               o JMAP 투자 논란: Fastmail은 10년 넘게 도입이 미미한 JMAP 프로토콜에 자원을 투자하면서, 동시에 많은 사용자가 요청하는 PGP 암호화를 거부함. 이는 사용자 요구보다 프로토콜 혁신을 우선한 전략적 선택으로 평가됨. 여전히 대부분의 이메일 클라이언트는 IMAP/SMTP에 의존
          + ProtonMail: 개인정보 보호 중심, 지속 가능한 성장
          + Zoho Mail: 대규모 비즈니스 제품군의 일부로 안정적 운영
          + Forward Email(We): 7년 이상 운영, 수익성과 성장을 동시에 달성
          + 기업 성공 사례: Forward Email은 케임브리지 대학 30,000명의 동문 이메일 솔루션을 지원하여 연간 $87,000 절감 효과 제공
          + 패턴: 이들은 이메일을 대체하지 않고 강화함.
     * 예외적 성공 사례: Xobni Xobni는 기존 이메일 환경을 개선함으로써 드물게 성공한 스타트업임.
          + 올바른 전략:
               o 기존 이메일 위에 구축: Outlook과 연동
               o 실제 문제 해결: 연락처 관리 및 이메일 검색 문제를 해결
               o 통합 중심: 기존 워크플로우에 맞게 작동
               o 엔터프라이즈 초점: 생산성 향상에 돈을 지불할 기업 시장 타겟팅
          + 성과: 2013년 Yahoo가 $6천만 달러에 인수, 투자자에게 의미 있는 수익 제공.
          + 창업자들의 이후 성과:
               o Matt Brezina: 활발한 엔젤 투자자로 Dropbox, Mailbox 등에 투자
               o Adam Smith: 생산성 분야에서 계속 성공적인 기업 창업
               o 두 창업자 모두 ""이메일 성공은 대체가 아닌 개선에서 온다"" 는 점을 입증
     * 성공의 패턴 이메일 분야에서 성공한 기업들의 공통점:
          + 1. 인프라를 구축 → SendGrid, Mailgun
          + 2. 기존 워크플로우를 강화 → Xobni, FastMail
          + 3. 신뢰성에 집중 → Amazon SES, Postmark
          + 4. 개발자를 지원 → API와 도구 제공, 최종 사용자 앱이 아님

이메일을 성공적으로 재발명한 사례가 있는가?

   이 질문은 이메일 혁신의 본질을 파고드는 중요한 물음임
   짧은 답변은 다음과 같음: 누구도 이메일을 대체하는 데 성공하지 못했지만, 이메일을 ‘강화’하는 데는 성공한 사례가 있음.
     * 실제로 자리 잡은 혁신들 지난 20년간 이메일에서 정착한 혁신들은 모두 기존 프로토콜을 대체하지 않고 강화한 것들임:
          + Gmail의 대화형 쓰레딩: 이메일 조직 방식 개선
          + Outlook의 캘린더 통합: 일정 관리 강화
          + 모바일 이메일 앱: 접근성과 사용성 강화
          + DKIM / SPF / DMARC: 이메일 인증 및 보안 강화
          + 패턴: 성공한 모든 혁신은 이메일을 대체하지 않고 보완함.
     * 이메일을 대체하지 않고 보완하는 도구들
          + Slack: 팀 채팅 도구지만 이메일 알림을 여전히 전송
          + Discord: 커뮤니티 중심 플랫폼이지만 계정 관리는 이메일 기반
          + WhatsApp: 메시징에 최적화되어 있으나 비즈니스는 이메일을 계속 사용
          + Zoom: 화상 회의 필수 도구지만, 회의 초대장은 이메일로 발송
     * HEY 실험 Basecamp의 HEY는 최근 가장 진지하게 이메일을 “재발명”하려 한 시도임.
          + 출시: 2020년, 대대적인 홍보와 함께 등장
          + 접근법: 선별·번들링·워크플로우 등 새로운 이메일 패러다임 제시
          + 반응: 일부는 열광했으나, 다수는 기존 이메일 사용을 유지
          + 현실: 결국 여전히 SMTP/IMAP 기반 이메일에 새로운 인터페이스를 덧붙인 것에 불과
          + 실증적 사례: 창업자 DHH는 자신의 개인 도메인 dhh.dk에서 수년간 Forward Email을 사용하고 있음. 이는 이메일 혁신가조차 검증된 인프라에 의존함을 보여줌.
     * 실제로 효과적인 것들 가장 성공적인 이메일 혁신은 다음과 같음:
          + 1. 더 나은 인프라: 더 빠른 서버, 개선된 스팸 필터링, 향상된 전달율
          + 2. 강화된 인터페이스: Gmail 대화형 보기, Outlook 캘린더 통합
          + 3. 개발자 도구: 이메일 전송 API, 추적용 웹훅
          + 4. 특화된 워크플로우: CRM 연동, 마케팅 자동화, 트랜잭션 이메일

   결론: 지금까지 어떤 혁신도 이메일을 대체하지 못했으며, 모두 이메일을 더 나아지게 만드는 방향으로 성공함

기존 이메일 프로토콜을 위한 현대적 인프라 구축: 우리(Forward Email)의 접근법

   실패 사례를 다루기 전에, 이메일에서 실제로 효과적인 것이 무엇인지 이해하는 것이 중요함
   이메일 자체가 깨져 있는 것이 아니라, 많은 회사들이 이미 잘 작동하는 시스템을 ""고치려"" 하면서 문제가 발생함
     * 이메일 혁신 스펙트럼 이메일 혁신은 크게 세 가지 범주로 나눌 수 있음:
          + 1. 프로토콜 강화: SMTP, IMAP, POP3 같은 표준을 더 안정적이고 빠르게 구현
          + 2. 워크플로우 개선: 기존 이메일 사용 흐름을 더 효율적으로 만드는 도구와 기능
          + 3. UI/UX 혁신: 새로운 인터페이스를 통한 접근성 및 사용성 강화
     * 우리가 인프라에 집중하는 이유 우리는 새로운 앱을 만들기보다는 현대적인 이메일 인프라를 구축하기로 선택했음. 그 이유는 다음과 같음:
          + 검증된 이메일 프로토콜: SMTP는 1982년부터 안정적으로 작동해왔음
          + 문제는 구현 품질: 많은 이메일 서비스가 여전히 낡은 소프트웨어 스택을 사용함
          + 사용자가 원하는 것 = 신뢰성: 새로운 기능이 아니라 안정적이고 깨지지 않는 워크플로우
          + 개발자 필요성: 더 나은 API와 관리 인터페이스 제공 필요
     * 이메일에서 실제로 효과적인 것들 성공적인 패턴은 단순함: 기존 이메일 워크플로우를 대체하지 않고 강화하는 것
          + 더 빠르고 신뢰성 있는 SMTP 서버 구축
          + 정상 이메일을 방해하지 않으면서 더 나은 스팸 필터링
          + 기존 프로토콜을 활용할 수 있는 개발자 친화적 API 제공
          + 올바른 인프라를 통한 전달률 개선
            결론: 이메일의 혁신은 ""대체""가 아니라 인프라를 통해 기존 시스템을 더 나아지게 만드는 것임

우리의 접근법: Forward Email이 다른 이유

     * 우리가 하는 일 (What We Do)
          + 실제 인프라 구축: SMTP/IMAP 서버를 처음부터 직접 개발
          + 신뢰성에 집중: 99.99% 가동 시간 보장 및 올바른 오류 처리
          + 기존 워크플로우 강화: 모든 이메일 클라이언트와 호환되며 안정적 작동
          + 개발자 지원: 실제로 쓸 수 있는 API와 도구 제공
          + 완전한 호환성 유지: SMTP / IMAP / POP3 표준 완벽 준수
     * 우리가 하지 않는 일 (What We Don’t Do)
          + “혁신적”이라는 이름의 새로운 이메일 클라이언트 개발
          + 기존 이메일 프로토콜 대체 시도
          + 불필요한 AI 기능 추가
          + 이메일을 “고치겠다”는 허황된 약속
            핵심은 검증된 프로토콜 위에서 안정성과 호환성을 높이는 것이며, 보여주기식 혁신 대신 실제 동작하는 인프라에 집중함

우리가 실제로 작동하는 이메일 인프라를 구축한 방법

     * 우리의 반(反)스타트업 접근법 (Our Anti-Startup Approach) 다른 회사들이 수백만 달러를 태우며 이메일을 재발명하려 할 때, 우리는 단순히 신뢰성 있는 인프라 구축에 집중해왔음:
          + 피벗 없음: 7년 이상 이메일 인프라에만 전념
          + 인수 전략 없음: 단기 매각이 아닌 장기적 운영 목표
          + 혁신 과장 없음: 이메일을 ""고치는"" 것이 아니라 더 잘 작동하게 만드는 것
     * 우리가 다른 이유 (What Makes Us Different)
          + 정부 규격 준수: Section 889 규정 준수, 미국 해군사관학교 등 기관 고객 보유
          + OpenPGP + OpenWKD 지원: Fastmail이 거부한 PGP 암호화를 지원, 사용자가 원하는 실제 암호화 기능 제공
          + 기술 스택 차별화:
               o 전체 스택을 JavaScript로 개발 (1980년대 C 코드 기반 Postfix 대비)
               o 단일 언어로 글루 코드 불필요
               o 웹 네이티브 아키텍처, 유지보수성 뛰어남
               o 레거시 부채 없음, 현대적 코드베이스
          + Privacy by Design:
               o 이메일을 디스크나 DB에 저장하지 않음
               o 메타데이터, 로그, IP 주소 미저장
               o 포워딩 시 메모리 내에서만 처리
          + 기술 백서와 문서를 통해 보안·아키텍처 세부 구현을 공개
     * 왜 우리가 성공하는가 (Why We Succeed Where Others Fail)
          + 1. 앱이 아니라 인프라 구축: 서버와 프로토콜에 집중
          + 2. 대체가 아닌 강화: 기존 이메일 클라이언트와 호환성 유지
          + 3. 자체 수익성 확보: VC 압박 없이 지속 가능한 운영
          + 4. 깊은 기술 이해: 7년 이상 이메일 전문 경험
          + 5. 개발자 중심: 실제 문제 해결에 도움 되는 API와 도구 제공

이메일 인프라의 보안 과제

   이메일 보안은 모든 서비스 제공자가 직면하는 복잡한 도전 과제임
   개별 사고 사례보다 공통적으로 해결해야 할 보안 고려사항을 이해하는 것이 중요함
     * 공통 보안 고려사항 (Common Security Considerations)
          + 데이터 보호: 사용자 데이터와 통신을 안전하게 보호
          + 접근 제어: 인증 및 권한 관리
          + 인프라 보안: 서버와 데이터베이스 방어
          + 규제 준수: GDPR, CCPA 등 규정 충족
          + 고급 암호화 적용 : Forward Email의 보안 정책:
               o ChaCha20-Poly1305 기반 메일박스 암호화
               o LUKS v2 기반 디스크 전체 암호화
               o 저장·메모리·전송 구간 전반의 암호화 적용
     * 투명성의 가치 (The Value of Transparency) 보안 사고 발생 시 가장 중요한 대응은 투명성과 신속한 조치임. 모범 사례는 다음과 같음:
          + 즉시 공개: 사용자들이 상황을 인지하고 대응할 수 있도록 함
          + 상세 타임라인 제공: 문제의 범위와 이해 수준을 보여줌
          + 빠른 수정 조치: 기술적 역량 증명
          + 교훈 공유: 업계 전체의 보안 개선에 기여
     * 지속적인 보안 과제 (Ongoing Security Challenges) 이메일 보안은 계속 진화 중이며, 다음과 같은 영역에서 지속적 개선이 필요함:
          + 암호화 표준: TLS 1.3 같은 최신 암호화 방식 적용
          + 인증 프로토콜: DKIM, SPF, DMARC 강화
          + 위협 탐지: 스팸·피싱 필터링 고도화
          + 인프라 강화: 서버 및 데이터베이스 보안 강화
          + 도메인 평판 관리: Microsoft onmicrosoft.com 스팸 급증 사례처럼 새로운 위협에 대응하는 차단 규칙 마련

결론: 앱이 아닌 인프라에 집중하라

     * 명확한 증거 (The Evidence Is Clear) 수백 개의 이메일 스타트업을 분석한 결과:
          + 실패율 80%+: 대부분의 이메일 스타트업은 완전히 실패 (실제 수치는 더 높을 가능성 큼)
          + 클라이언트 앱은 대부분 실패: 인수는 곧 서비스 종료로 이어짐
          + 인프라는 성공 가능: SMTP/API 서비스를 구축하는 기업들은 종종 번창함
          + VC 자금 압박: 벤처 자금은 비현실적인 성장 압력을 만듦
          + 기술 부채 누적: 이메일 인프라 구축은 예상보다 훨씬 어려움
     * 역사적 맥락 (The Historical Context) 지난 20년간 스타트업들은 계속해서 이메일의 종말을 예언했음:
          + 2004: “소셜 네트워크가 이메일을 대체할 것”
          + 2008: “모바일 메시징이 이메일을 죽일 것”
          + 2012: “Slack이 이메일을 대체할 것”
          + 2016: “AI가 이메일을 혁신할 것”
          + 2020: “원격 근무 시대에는 새로운 커뮤니케이션 도구가 필요하다”
          + 2024: “AI가 마침내 이메일을 고칠 것”
            하지만 이메일은 여전히 존재하고, 성장 중이며, 필수적임
     * 진짜 교훈 (The Real Lesson) 이메일이 개선될 수 없다는 교훈이 아니라, 올바른 접근법을 택해야 한다는 교훈임:
          + 1. 이메일 프로토콜은 유효함: SMTP, IMAP, POP3는 검증된 표준
          + 2. 인프라가 핵심: 화려한 기능보다 안정성과 성능이 중요
          + 3. 대체보다 강화: 이메일과 싸우지 말고 함께 작동하는 개선책을 제공
          + 4. 성장보다 지속가능성: 수익성 있는 기업이 VC 주도의 “빠르게 성장하고 망가뜨리기” 모델보다 오래 살아남음
          + 5. 개발자 지원: 최종 사용자 앱보다 개발자를 위한 도구와 API가 더 큰 가치를 만듦
          + 핵심 기회: 이미 검증된 프로토콜을 더 잘 구현하는 것, 새로운 프로토콜을 만드는 것이 아님
          + 더 깊은 분석: 79 Best Email Services (2025)
     * 이메일 스타트업을 만들고 싶다면, 앱이 아니라 인프라를 구축하는 것을 고려해야 함
          + 세상에 필요한 것은 더 많은 이메일 앱이 아니라, 더 나은 이메일 서버임

확장된 이메일 묘지: 더 많은 실패와 서비스 종료들

     * 구글의 이메일 실험 실패 (Google's Email Experiments Gone Wrong) 구글은 Gmail을 보유하고 있음에도 불구하고 여러 이메일 프로젝트를 종료함:
          + Google Wave (2009–2012): “이메일 킬러”라 불렸으나 아무도 이해하지 못함
          + Google Buzz (2010–2011): 소셜 이메일 통합 시도, 실패
          + Inbox by Gmail (2014–2019): Gmail의 “스마트” 후속작으로 출시되었지만 결국 중단
          + Google+ (2011–2019): 이메일 기능 통합을 시도했으나 실패
          + 패턴: Gmail을 가진 구글조차 이메일을 성공적으로 재발명하지 못함
     * 뉴턴 메일의 세 번의 죽음 (The Serial Failure: Newton Mail's Three Deaths) Newton Mail은 무려 세 번 죽음을 맞음:
          + 1. CloudMagic (2013–2016): 초기 이메일 클라이언트, Newton에 인수됨
          + 2. Newton Mail (2016–2018): 브랜드 재출시, 구독 모델 실패로 종료
          + 3. Newton Mail Revival (2019–2020): 부활 시도, 다시 실패
          + 교훈: 이메일 클라이언트는 구독 모델을 지속할 수 없음
     * 출시조차 못한 앱들 (The Apps That Never Launched) 많은 이메일 스타트업은 출시 전에 사라짐:
          + Tempo (2014): 캘린더-이메일 통합 시도, 출시 전 중단
          + Mailstrom (2011): 이메일 관리 툴, 출시 전 인수됨
          + Fluent (2013): 이메일 클라이언트, 개발 중단
     * 인수 후 종료 패턴 (The Acquisition-to-Shutdown Pattern) 여러 이메일 앱이 인수 후 곧바로 종료됨:
          + Sparrow → Google → Shutdown (2012–2013)
          + reMail → Google → Shutdown (2010–2011)
          + Mailbox → Dropbox → Shutdown (2013–2015)
          + Accompli → Microsoft → Shutdown (Outlook Mobile로 흡수)
          + Acompli → Microsoft → Integrated (드문 성공 사례)
          + 패턴: 인수는 곧 서비스 종료를 의미하는 경우가 많음
     * 이메일 인프라 통합 (Email Infrastructure Consolidation) 인프라 영역에서도 통합과 종료가 빈번함:
          + Postbox → eM Client (2024): eM Client가 Postbox를 인수 후 즉시 종료
          + ImprovMX: 여러 차례 인수되었으며, 개인정보 보호 문제와 인수 공지, 매물 등록 등이 반복됨
          + 서비스 품질 저하: 많은 서비스가 인수 후 오히려 악화됨

오픈소스 이메일 묘지: ""무료""가 지속 불가능할 때

     * Nylas Mail → Mailspring: 실패한 포크
          + Nylas Mail: 오픈소스 이메일 클라이언트였으나 2017년 중단, 심각한 메모리 사용 문제를 가짐
          + Mailspring: 커뮤니티 포크로 유지 중이나 높은 RAM 사용 문제와 유지보수 한계에 부딪힘
          + 현실: 오픈소스 이메일 클라이언트는 네이티브 앱과 경쟁하기 어려움
     * Eudora: 18년간의 죽음 행진
          + 1988–2006: Mac/Windows에서 지배적 이메일 클라이언트로 군림
          + 2006: Qualcomm 개발 중단 선언
          + 2007: ""Eudora OSE""로 오픈소스화
          + 2010: 프로젝트 완전 중단
          + 교훈: 성공적인 이메일 클라이언트도 결국은 사라짐
     * FairEmail: Google Play 정책에 의해 사망
          + FairEmail: 개인정보 보호 중심의 안드로이드 이메일 클라이언트
          + Google Play: ""정책 위반"" 사유로 퇴출
          + 현실: 플랫폼 정책에 의해 이메일 앱이 하루아침에 사라질 수 있음
     * 유지보수 문제 (The Maintenance Problem) 오픈소스 이메일 프로젝트들이 실패하는 이유:
          + 복잡성: 이메일 프로토콜을 올바르게 구현하기 어려움
          + 보안: 끊임없는 보안 업데이트 필요
          + 호환성: 모든 이메일 제공자와 호환되어야 함
          + 자원 부족: 자원봉사 개발자들의 소진(burnout)

AI 이메일 스타트업 붐: ""지능""이라는 이름의 반복 역사

     * 현재 AI 이메일 골드러시 (2024)
          + Superhuman: $33M 투자 유치, 2025년 Grammarly에 인수
          + Shortwave: Y Combinator 출신, Gmail + AI 요약 기능
          + SaneBox: AI 이메일 필터링, 실제 수익성 있는 서비스
          + Boomerang: AI 기반 스케줄링 및 자동 응답
          + Mail-0/Zero: 또 다른 AI 이메일 클라이언트 인터페이스 개발 중
          + Inbox Zero: 오픈소스 AI 이메일 어시스턴트, 이메일 관리 자동화 시도
     * 자금 조달 열풍
          + 2024년 한 해에만 VC가 $100M+ 투자
          + 반복되는 약속: ""혁신적인 이메일 경험""
          + 반복되는 문제: 기존 인프라 위에서만 구축
          + 반복되는 결과: 대부분 3년 이내 실패 예상
     * 왜 다시 실패할 것인가
          + 1. AI는 이메일의 ‘문제가 아닌 문제(non-problem)’를 해결하려 함 – 이메일은 이미 잘 작동 중
          + 2. Gmail은 이미 AI 기능 제공 – 스마트 답장, 우선순위 편지함, 스팸 필터링
          + 3. 프라이버시 우려 – AI는 모든 이메일을 읽어야 함
          + 4. 비용 구조 문제 – AI 처리 비용은 높고, 이메일은 본질적으로 저가 서비스
          + 5. 네트워크 효과 – Gmail/Outlook의 지배적 위치를 무너뜨릴 수 없음
     * 불가피한 결과
          + 2025: Superhuman → Grammarly 인수 (드문 성공 사례)
          + 2025–2026: 대부분의 AI 이메일 스타트업은 피벗 또는 폐업
          + 2027: 일부 생존 기업은 인수되지만 성패가 엇갈림
          + 2028: ""블록체인 이메일"" 같은 새로운 유행이 등장할 가능성

통합의 재앙: ""생존자""가 재앙이 될 때

     * 대규모 이메일 서비스 통합 이메일 산업은 급격히 통합(consolidation) 되었음
          + ActiveCampaign → Postmark 인수 (2022)
          + Sinch → Mailgun 인수 (2021)
          + Twilio → SendGrid 인수 (2019)
          + ImprovMX: 여러 차례 인수, 프라이버시 우려 및 재매각 사례 존재
     * Outlook: 멈추지 않는 문제를 가진 ""생존자"" Microsoft Outlook은 여전히 업계 주류지만 지속적인 문제 발생
          + 메모리 누수: 수 GB RAM 사용, 잦은 재시작 필요
          + 동기화 문제: 이메일이 사라졌다가 다시 나타나는 현상
          + 성능 문제: 시작 속도 느림, 자주 충돌
          + 호환성 문제: 타사 이메일 제공자와 충돌 발생

     실제 현장 사례: 표준 IMAP 구현을 따름에도 Outlook이 자주 깨짐
     * Postmark 인프라 문제 ActiveCampaign 인수 이후 발생한 문제들
          + SSL 인증서 만료: 2024년 9월, 약 10시간 장애
          + 합법적 사용자 거부: Marc Köhlbrugge 사례
          + 개발자 이탈: @levelsio: ""Amazon SES가 마지막 희망""
          + MailGun 장애: 2주간 이메일 전송 불가 사례
     * 최근 이메일 클라이언트 사망 사례 (2024–2025)
          + Postbox → eM Client: 인수 직후 즉시 종료, 사용자 강제 이전
          + Canary Mail: Sequoia 지원에도 불구, 사용자 불만 폭주
          + Spark by Readdle: 품질 저하 보고 증가
          + Mailbird: 라이선스 문제와 구독 혼란
          + Airmail: Sparrow 기반 코드, 신뢰성 문제 지속
     * 이메일 확장/서비스 종료 사례
          + HubSpot Sidekick: 2016년 중단, ""HubSpot Sales""로 대체
          + Engage for Gmail: 2024년 6월 종료, 사용자 강제 이전
     * 실제로 살아남은 이메일 기업들
          + Mailmodo: YC 출신, 인터랙티브 이메일 캠페인, Sequoia Surge에서 $2M 투자
          + Mixmax: 총 $13.3M 투자, 영업 참여 플랫폼으로 운영 중
          + Outreach.io: $4.4B+ 평가, IPO 준비 중
          + Apollo.io: 2023년 $1.6B 평가, $100M 시리즈 D 유치
          + GMass: Gmail 확장 기반, 월 $140K 부트스트랩 성공 사례
          + Streak CRM: Gmail 기반 CRM, 2012년부터 안정적으로 운영
          + ToutApp: 2017년 Marketo에 인수 성공
          + Bananatag: 2021년 Staffbase에 인수, ""Staffbase Email""로 지속 운영
     * 핵심 패턴
          + 성공 기업들은 이메일을 대체하지 않고, 워크플로우를 보강(enhance)
          + 이메일 인프라와 협력적으로 작동하는 도구를 만들었음

결론

     * 이메일 스타트업의 80% 이상은 실패
     * 앱 중심 접근은 실패, 인프라 중심 접근은 성공
     * 핵심 교훈:
          + 1. 이메일 프로토콜은 이미 잘 작동
          + 2. 인프라의 안정성과 성능이 중요
          + 3. 교체보다는 강화가 효과적
          + 4. 지속 가능한 비즈니스 모델이 필요
          + 5. 개발자를 위한 도구와 API가 성공의 열쇠

   중간까지만 읽고 hey.com 사례는 없을까? 하자마자 내용에 바로 등장하네요 ㅎㅎ
   이메일이라는 제품은 항상 무궁무진한 동시에 기존 플레이어의 지위를 탈환하기가 어려운 시장 같아요..

   구축해 놓으면 여기저기서 스팸메일서버로 등록해서 이거 푸는게 일입니다 고객님한테 안된다고 갑자기 연락받으면 대부분 스팸서버 등록이더라구요

   아직 성공했다, 실패했다 논하기엔 규모가 작은 제품이지만,
   Mimestream 이라는 지메일 클라이언트를 아주 즐겁게 잘 사용하고 있습니다.

   https://mimestream.com/

   지금껏 써온 메일 클라이언트들 중에서 제일 만족스럽습니다... 베타때부터 쓰기 시작해 이제 거의 5년째네요.

   음.. ""해커뉴스 현실 점검(The Hacker News Reality Check)"" 부분 링크가 다 이상하네요. 애초에 원문부터 이상한 링크로 걸어놨네요.

   어라 그렇네요. 원문을 찾아봐도 찾기가 어려워서 걍 둬야할듯합니다.

        Hacker News 의견

     * 나는 SendGrid에서 12번째 엔지니어로 일했고, IPO 이후 Twilio의 인수 후에 회사를 떠남. 인프라를 담당하면서 여러 이메일 마케팅 회사들의 기반이 되어 성과를 냄. 금광 시대에 삽을 파는 것처럼 비즈니스가 성장함. 더 큰 마케팅 시장에 진입하는 제품적인 측면에서는 더 힘들었음. 그곳에서 팀 리딩, 스케일링, 그리고 하루 80억 건 이상의 이메일 인프라 확장 등 많은 경험을 쌓음
          + 이메일 마케팅 회사들이라면 스패머를 의미하는 것 아닌지 궁금함
          + 구직 중임이 느껴짐
     * 모든 ""이메일 스타트업""은 기존 인프라 위에 UI만 덧씌우는 일임. 실제 이메일 서버를 만드는 게 아니라 이미 구축된 인프라에 연결되는 앱을 만드는 것임. 내가 Mailpace를 만들면서 정말 충격을 받았던 부분임. 다들 직접 smtp 서버를 운영한다고 생각했는데, 실제로는 YC나 여러 곳에서 aws ses에 래퍼만 붙이는 회사를 엄청나게 펀딩하고 있는 상황임
          + YC와 여러 곳에서 aws ses 래퍼에 집중적으로 투자하는 게 모든 분야를 송두리째 혁신할 대단한 일인 듯 말하지만, 이것이 피자 배달까지 바꿀 거라고 비꼬는 유머임
          + Gmail 인박스에 메일을 넣는 일이 엄청 복잡하고 까다롭기 때문에(스팸 문제 때문임), 그렇기에 스패머들이 이런 래퍼 서비스를 활용하는 것을 이해할 수 있음
     * 이메일 스타트업의 20%나 성공한다는 사실에 놀라움. 꽤 괜찮은 성공률이라 생각함
     * Electron과 React Native로 개발된 현대적 이메일 클라이언트들이 메모리 점유와 성능 문제에 시달리는 것에 대해, 실제(진짜) 고객들은 전혀 신경 쓰지 않는다고 말함. 예를 들어 Discord와 Slack처럼 비대하지만 거의 모든 사람이 사용하는 Electron 앱들이 이를 증명함. 나는 개인적으로 React를 싫어하지만, 기술 결정이 스타트업의 장기적 성공에는 큰 영향을 미치지 않는다고 봄(고객 경험이나 기능에 크게 방해되지 않는 한). 이메일 스타트업 중 80% 이상이 완전히 실패한다는 수치도 실제로는 실패율이 훨씬 높을 거라고 내기 걸 수 있음. 게다가 20%의 이익률이 나쁜 결과라고 하는 것도 잘 이해가 안 됨, 특히 이메일처럼 지루한 분야에서라면 더욱 그렇다고 생각함. 자원봉사자들이 엔터프라이즈급 소프트웨어를 지속적으로 운영할 수 없다고 쓴 글에 대해선,
       오픈소스 계에서 openssl, Linux 등 많은 소프트웨어가 대다수 자원봉사자에 의해 잘 운영되고 있음을 상기함. 이 글을 읽으니 오히려 이메일을 재발명할 방법을 더 고민하게 됨
          + ""진짜"" 고객은 성능에 신경 쓰지 않는다는 주장에 반박함. 내가 일했던 두 회사의 ""일반"" 사용자들도 Electron 기반 앱의 불편한 경험 때문에 다른 도구를 찾고 있었음. 일부 사례일 수 있지만, 그만큼 사용 경험이 나쁘면 평범한 사람들도 충분히 신경 씀
          + 나는 실제 Discord 사용자인데, M1 MacBook과 게이밍 PC에서 너무 느려서 대체 앱을 적극적으로 찾고 있음. 내가 평균적인 고객을 대표한다고 주장하지는 않지만, 이런 불편을 느끼는 사람이 많다는 점을 강조하고 싶음
          + ""고객이 신경 안 쓴다""는 말은 사실이 아님. 기술에 관심 적은 내 여자친구조차 Teams 같은 채팅 앱 하나가 컴퓨터 전체를 느리게 만든다고 불평했음. 평균적인 사용자는 Electron이나 React 자체를 싫어하진 않아도, 이들로 인해 발생한 나쁜 경험은 분명히 싫어함
          + 엔터프라이즈급 소프트웨어가 자원봉사자로 유지될 수 없다는 주장에, Postfix 역시 대부분 커뮤니티 자원봉사자에 의해 잘 운영되고 있음
          + 모두가 설치한 Electron 앱 예시로 Discord와 Slack을 들었지만, 난 그 둘 다 설치 안 한 유일한 사람인 듯함
     * Hey가 언급되지 않은 게 의외임. 최근 이메일 재발명을 시도한 곳은 그 예시밖에 모르겠음. 단독 회사가 아니고 Basecamp의 일부분이라서 빠진 걸 수도 있지만, ""누구도 이메일을 재발명하지 못했다""는 논지를 펼친다면 꼭 다뤄야 한다고 생각함
          + 언급되어 있음. ""The HEY Experiment""라는 섹션이 별도로 존재함
          + 재발명이라는 게 UX만을 의미하는지 궁금함. Fastmail은 뛰어난 오픈 프로토콜로 이메일을 재해석하고 있음
     * Techstars 사례처럼 이메일 관련 28개 회사 중 5개만 엑시트라 실패율 80% 가까운 게 문제로 보이지만, 사실 전체 스타트업 실패율과 비교하면 훨씬 나은 수치임. 실패율 80%라면 이메일 회사에 더 투자할 것 같음. 전체 분석이 메시지에 맞도록 왜곡된 느낌이고 Cyrus IMAP, SpamAssassin을 언급하는 것조차 과거에 머물러 있는 듯함. 자가 펀딩이니 이런 논조가 이해되긴 하지만, 좀 더 객관적 관점이 필요하다고 생각함
     * 만약 Fastmail을 스타트업 범주에 포함한다면, Posteo와 Mailbox.org 같은 회사들도 포함해야 하는 것 아님? Runbox.com은 한 명이 운영하지만, 이런 회사들도 수십 년간 꾸준히 성장 중임. Posteo는 VC 투자도 받지 않았고(이로 인해 실패 가능성이 줄었다고 기사에서는 지적함), Migadu처럼 변함없이 운영되는 서비스 또한 언급이 없음
          + ""AI""가 그런 회사들의 언급 데이터를 충분히 학습하지 않아서 빠졌을 수 있음
          + Runbox.com은 소규모 팀은 맞지만, 한 명이 전부는 아님. Runbox 팀 소개 자료를 참고하면 다수의 팀원이 있음. (직접 이용 중인 고객임)
     * Mailbox가 600만 달러를 투자받아 1억 달러에 엑시트했는데도 실패로 분류된다는 건 이해가 잘 안 됨
          + Rapportive가 12만 달러 투자로 1,500만 달러 엑시트한 사례도 주목받을 만함
"
"https://news.hada.io/topic?id=22728","SuperDesign — IDE내부에서 사용하는 AI 디자인 에이전트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 SuperDesign — IDE내부에서 사용하는 AI 디자인 에이전트

     * Cursor for design - IDE 내부에서 동작하는 오픈소스 디자인 에이전트
     * 자연어 프롬프트를 통해 UI 목업(mockup), 컴포넌트, 와이어프레임을 즉시 생성 가능
     * Cursor, Windsurf, Claude Code, VS Code 등과 완벽 호환 (확장으로 설치)

주요 기능

     * Product Mock: 프롬프트 한 줄로 전체 UI 화면 자동 생성
     * UI Components: 재사용 가능한 컴포넌트 생성 및 코드 삽입 가능
     * Wireframes: 빠른 반복(iteration)을 위한 저해상도 와이어프레임 제공
     * Fork & Iterate: 디자인을 복제하고 진화시키는 기능
     * Prompt-to-IDE: 생성 프롬프트를 IDE(Cusor, Windsurf, Claude Code 등)로 바로 전달
     * 생성된 모든 디자인은 로컬 디렉토리 .superdesign/ 에 저장됨
     *

Claude Code / Cursor 구독 연동

     * SuperDesign 초기화 후 Cursor/Claude Code 규칙이 자동 추가되어, 디자인 프롬프트 및 미리보기 지원
     * Cursor 사용 시: design.mdc 파일에 프롬프트를 복사하고 Cursor에서 같은 시스템 프롬프트로 Custom Mode를 생성하면 성능 향상
     * 참고 영상: Instruction Video

   Superdesign: Configure Anthropic API key
   claude api를 따로 또 지불해야 하는걸까요?

   로그인해서 계정으로 안되나요?
"
"https://news.hada.io/topic?id=22651","코드 리뷰는 더 나아질 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           코드 리뷰는 더 나아질 수 있음

     * 개발자들은 GitHub의 코드 리뷰 경험에 불만을 많이 느끼고 있으며, 이를 개선하기 위해 새로운 시도중
     * git-review라는 실험적 도구는 코드 리뷰를 브라우저 웹 인터페이스가 아닌, 로컬에서 직접 코드와 함께 다루도록 설계됨
     * 리뷰는 단일 커밋으로 관리되며, 코드 안에 주석처럼 리뷰 코멘트를 남기고, 리뷰어와 작성자가 이 커밋을 함께 수정해 나가는 방식
     * 그러나 리뷰 중간에 코드가 수정되거나 리베이스될 경우, 충돌 처리와 --force-with-lease 사용 등에서 불편함이 발생해 큰 성공을 거두지 못함
     * 결국 웹 기반 리뷰로 복귀했지만, 리뷰 상태를 Git 저장소에 직접 포함시키는 발상은 여전히 매력적이며, Gerrit-style Change-Id 도입 등 향후 Git 개선과 함께 더 나은 대안이 나올 가능성이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

코드 리뷰 시스템에 대한 문제 인식

     * 현재 많은 사람들이 GitHub의 코드 리뷰 프로세스에 대해 불만을 가지는 상황임
     * 주요 문제는 스택된 풀 리퀘스트 및 인터디프 리뷰에 대한 지원 부족과 더불어,
          + 리뷰 상태가 저장소 내부에 저장되지 않음
          + 원격 우선 웹 인터페이스를 통한 리뷰가 필수적임
     * 내가 가지고 있는 문제는 리뷰의 탈중앙화 부족과 인터페이스 비효율성임

코드 작성 및 리뷰 워크플로우의 비교

     * 사람들은 코드를 작성할 때 로컬에서 에디터를 사용함
          + 메모리 및 NVMe 지연이 적고, 사용자의 특이한 워크플로우에 최적화된 환경임
     * 코드 리뷰 역시 소스 브랜치를 로컬로 pull 해서 작업하는 방식을 선호함
          + Magit과 같은 도구를 통해 diff 뿐만 아니라 전체 코드 컨텍스트 탐색 가능함
          + 테스트 실행, 코드 정의로의 이동, 리팩토링 시도 등 강력한 개발 환경 이용이 가능함
     * 반면, PR에 피드백을 남기려면 브라우저에서 느린 웹 인터페이스로 이동해야 하며, 큰 diff에서는 입력 지연도 심함

이상적인 코드 리뷰 인터페이스 및 저장 구조

     * 실제로 코드에 인라인으로 코멘트를 남기거나, 직접 코드를 수정하는 것이 가장 자연스러움
// CR(matklad): Hm, this check seems imprecise to me.
// Shouldn't we compare `replica.view` instead of `header.view` here?
if (header.view != view) return;

     * 데이터가 로컬 git 저장소가 아닌 원격 DB에 저장되면서, 지연과 벤더 락인 문제도 발생함

git-review의 아이디어와 실제 경험

     * git-review의 아이디어는 다음과 같음:
          + 코드 리뷰가 PR 브랜치 최상단의 단일 커밋으로 이루어짐
          + 해당 커밋에 특수 마커가 달린 코드 코멘트가 추가됨
          + 리뷰어와 작성자가 이 커밋을 번갈아 가며 수정하며 push --force-with-lease에 기반한 협업이 이뤄짐
          + 모든 댓글이 해결됨 표시(//? resolved) 되고 리뷰 종료 시 리버트 커밋 추가로 기록이 남음
     * 아이디어는 단순하고 실용적이지만, 실제로는 다음과 같은 문제 발생
          + 리뷰 중 코드 수정 시 하위 커밋이나 신규 커밋에서 코멘트와의 충돌이 잦음
          + force-push 과정에서 협업 마찰과 작업 복잡도 증가
          + 코드의 변경 이력과 리뷰 진행 간의 불일치 및 병합 충돌 관리가 어렵게 됨

새로운 변화와 미래 가능성

     * 앞으로 Git upstream에서 Gerrit 스타일의 Change-Id가 도입될 가능성이 있음
          + 커밋별 수정 이력 추적이 손쉬워져 인터디프 리뷰 지원이 확대될 전망임
          + 하지만 git-review 방식과는 일부 충돌이 예상됨
          + 새로운 Change-Id 구조에서는 커밋 자체에 리뷰 코멘트 추가 등의 색다른 접근이 가능해질 수 있음

결론 및 참고할 만한 시스템 소개

     * 결국 현재는 웹 인터페이스 기반 코드 리뷰로 다시 돌아온 상황임
     * 보다 나은 솔루션에 대한 필요성은 여전히 남아 있음
     * 참고할 만한 관련 시스템 및 도구 소개
          + Fossil: 모든 정보를 저장소 내부에 보관하는 SCM 시스템
          + NoteDb: Gerrit의 리뷰 상태 저장 이력을 git으로 통합
          + git-bug: 이슈 정보를 git에 저장
          + git-appraise: 리뷰 정보를 git 자체에 보관
          + prr: 에디터 내에서 GitHub API와 연동해 리뷰 인터페이스 구현
          + How Jane Street Does Code Review: 더 나은 현실의 예시 소개
          + git-pr: PR 워크플로우 전체를 git의 네이티브 기능으로 대체하는 프로젝트

마무리

     * 아직 완벽한 해결책은 없는 상황이며, 더 나은 개발자 경험을 위한 시도가 계속되고 있음
     * 앞으로의 발전 방향에 많은 기대감이 존재함

        Hacker News 의견

     * 코드 리뷰에서 오랫동안 불만이었던 점은, 진짜 유용한 피드백(사소한 취향 지적 외의 것)이 거의 항상 너무 늦게 나온다는 점임. 리뷰의 유일한(혹은 희귀한) 결과물이 ""이거 다 다시 새 설계로 시작해야 함"" 혹은 ""애초에 이 작업은 할 필요 없었음""이라는 상황이 생김. 코드 리뷰만이 모든 이해관계자가 실제로 참여해서 진지하게 변화에 대해 생각하는 유일한 시점인 것 같음. 미팅이나 Jira 티켓에서 뭔가 논의가 있었을 수도 있지만, 종종 조직 다른 팀/부서 누군가가 코드 리뷰 알림을 받고서야 이 변경을 알게 됨. 나 역시 다른 팀이 이상한 걸 구현했을 때, 코드 리뷰 알림으로 처음 알게 되는 경우가 많음. 모든 구성원이 미리 다 팔로우업 한다는 건 비현실적임. 90년대 대학 과정에선 설계 리뷰도 했지만, 실제 현업에선 그런 거 본 적 없음. 설계 리뷰가
       모든 이슈를 미리 잡아주리란 보장도 없다고 봄.
          + SW 엔지니어링 세계에는 실질적인 엔지니어링이 별로 없음. 제대로 된 엔지니어링 프로세스의 느림을 업계가 받아들이지 못하는 면도 있음. 대부분 소프트웨어는 치명적이지 않고, 버그나 오류는 나중에 고칠 수 있음. 다리, 공장, 비행기 엔진처럼 실패가 용납되지 않는 다른 분야와 stakes와 패치 기회가 다름.
          + 우리 팀은 4~6명의 소규모 개발자 그룹임. 새로운 기능 만들 때 머릿속 초안이 잡히면 동료들과 바로 상의함. 모두가 이렇게 하다 보니 코드 리뷰는 코드 스멜 등 사소한 것 중심이고, 전체 아키텍처는 보통 2~3명이서 미리 결정함. 팀원들이 코드에 동의하지 않으면 서로 다른 코드에 손대기 싫어해 상황이 안 좋아짐. 규모가 더 커져도 책임을 잘 공유하면 문제없이 가능하다고 믿음.
          + 코드 리뷰 시점에만 모든 이해관계자가 참여하는 것은 Git이나 버전 관리 시스템 문제가 아니라 조직의 문제임. PR 생성의 배경, 티켓 논의, 의사 결정 과정을 공유 못 하는 것임. 이는 dysfunctional 조직의 사례로, 인쇄된 책이 나오는 순간에야 모두 모여 제대로 관여한다고 출판 프로세스를 욕하는 것과 같음.
          + 우리 조직은 근본적 설계 결정에 대해 반드시 RFC를 작성함. 무엇이 근본적 설계 결정인지는 수시로 팀 내 자율적으로 판단함. Jira의 epic 단계에서 세부 구현법이 안 정해지면 일단 RFC 작성부터 할당함. RFC는 우리팀 내부용일 수도, 전체 소프트웨어팀 대상일 수도 있고, 후자일 땐 2주마다 열리는 미팅 전에 모두가 읽고 코멘트 남길 수 있음. 힘들어도 RFC 기반 협업 설계 프로세스가 없는 곳보다 훨씬 나음.
          + 내 경험상 디자인 리뷰에 대해 공감함. 예전엔 포멀한 설계문서와 리뷰를 했지만, 프로토타이핑과 반복 설계로 전환함. 설계 단계에서 중요한 디테일을 종종 빠뜨렸고, 이미 많은 시간을 들였으니 나중엔 대충 넘기게 되었음. 팀 전체가 모여 리뷰하기도 비효율적이라 결국 코드 리뷰 과정에서 이슈가 발견됨. 설계문서 잘 못 쓰거나 동기 없는 사람도 많음. 결과적으로 5명 이상이면 이런 비효율은 피할 수 없음. PO와 주요 사용자, 5명 정도 개발자가 함께하는 환경이 이상적임.
     * HN에서 stacked pull request에 대한 글을 보니 너무 흥미로움. 예전에 graphite.dev를 시작했을 땐 FB나 Google 경험자 아니면 이런 흐름을 모르는 경우가 많았음. 코드 리뷰 트렌드가 3~4년 사이에 엄청 빨리 변하는 것을 보는 게 재밌음.
          + pre-mercurial arcanist 이용자로서, 아직 대형 PR·merge commit 등으로 고생하는 팀에 Graphite를 적극 홍보하고 있음. 특히 PR과의 연동을 가능하게 한 도전정신과 결과에 큰 감명을 받음. Graphite가 하드코어 세팅으로 repo 초기화해 더욱 강한 가정들을 할 수 있는 prescriptive 모드가 있으면 좋겠음.
          + Graphite는 멋진 솔루션임에도 불구하고 가격이 꽤 비싸고, 구매 결정자 설득이 어려움. Graphite처럼 멋진 도구가 오픈소스가 되거나 GitHub에 내장되는 것을 기대함.
          + 나는 fig workflow가 그립기도 함
          + 최근 CodeRabbit에서 발생한 보안사고로 인해 LLM과 코드베이스가 통합된 새 도구 테스트가 꺼려짐. 신기한 새 실험이 보안 문제거리로 뒤바뀌기 쉬움.
          + stacked pull request는 본질적으로 없어도 될 복잡성을 추가함. 자주, 작은 단위의 변경이 훨씬 좋은 개발 관습임. trunk-based development나 continuous integration이 그 목적에 잘 맞음.
     * 점점 더 많은 개발자가 ""리뷰툴이 어떤 모습이어야 하는지""에 동의하는 분위기임. 이제 진짜 현실적으로 적용 가능하고, 지속 가능하게 만들 조직과 플레이어가 중요해진 시점임. 최근 git change-id 도입이 좋은 발전임(jj, git butler, gerrit 등에게 감사). Graphite나 GitHub는 자기들 이용자만을 위한 해결책에 집중해, 모두에게 열려 있는 방향은 아님. 수많은 클라이언트 기반 커맨드라인 도구들도 그다지 영향력 없음. 정말 필요한 요소는:
          + 완전히 로컬로 쓸 수 있어야 함
          + vscode를 위한 공식 core팀 지원 필요(vscode 통합만 따로 좋은 UX로)
          + vscode web 등 웹 UI, 다른 에디터와도 최대한 UI 자산을 재사용 가능하게
          + CLI나 라이브러리에서 핵심 기능 제공, 확장용 명확한 경계 갖추기
          + commit/branch/stacked commit/agent snapshot/dev 본인 리뷰 모두 지원
          + 네이티브하게 CI/CD 신호 통합, meta에서 보여준 우수한 UI 작업 기반 강화
          + 상황별로 아주 fine-grained하고 모든 단계에서 수정 가능해야 함(예: cursor에선 한줄 accept 가능, 사람 코드 리뷰도 그런 granular 편집 필요)
          + 완전 incremental해야 하며, PR 수정 시 전체 대신 수정분만 리뷰하는 쉬운 방법 필요함
     * Github의 가장 큰 불만은 앱이 너무 느리다는 점임. 정말 브라우저 탭이 멈출 정도의 느림임. Azure DevOps가 지금까지 사용한 코드 리뷰 툴 중 가장 뛰어났음.
          + Microsoft 환경에서 .NET 개발할 때 Azure DevOps를 사용해봤는데, 진짜 .NET 생태계에 잘 어울리는 도구임
          + GitLab을 열심히 사용해본 적이 있는지 궁금함. 나는 big 4 중에 GitLab이 제일 마음에 듦.
          + DevOps의 어떤 점이 그렇게 마음에 들었는지 궁금함. 매일 쓰지만 github과 비슷하다고 느낌. 오히려 github의 제안 변경 사항 자동 반영 기능이 그립기도 함.
          + 자바스크립트 대규모 사용 + 빠른 릴리즈 압박이 합쳐져 이런 느린 환경을 만듦. 그래도 Atlassian보다는 나음
     * git을 코드 리뷰에 직접 활용하는 아이디어가 매력적임. 변경사항을 로컬에서 직접 만져볼 수 있어 편리함. 리뷰를 반드시 단일 커밋에만 맞추려는 이유를 모르겠음—리뷰어가 자신의 코멘트/수정을 PR 브랜치에 직접 커밋하는 식의 접근도 흥미로움. 이는 전통적인 github flow와 Linux의 메일링리스트·패치 흐름의 하이브리드임.
          + github PR이 읽기 전용인지 궁금함. 팀원이 ""suggestion"" 코멘트로 바로 수정 제안하고 버튼 클릭 한 번으로 해당 커밋에 반영한 경험 있음
          + 리뷰 커밋이 단일이어야 한다는 건 이상함. 여러 명이 리뷰하면 동시 편집 문제도 생기고, 코드와도 맞지 않음. 리뷰도 각기 별도 브랜치에서 서로 작업하고, 최종적으로 squash & rebase하는 식이 자연스러움. 논의가 길어지면 comment commit도 서로 체인으로 남길 수 있음. 중요한 건 이 데이터가 main branch 바깥 어딘가에 남아야 한다는 점임
     * 코드 리뷰할 때 로컬 브랜치로 당겨와서 soft-reset 한 뒤, 마치 내가 작성한 것처럼 보는 것을 선호함. 커밋이 잘 쪼개지지 않으면 협업자가 각자 쪼개서 리뷰해야 하므로 비효율임. 리뷰 대상이 방대하면, 누가 온전히 이해했다고 말하긴 힘듦. 전체는 단순히 부분의 합이 아님.
          + 쉬운 코드 리뷰용 쉘 함수 예시를 공유함. clean tree 상태에서 review 브랜치를 프리셋으로 체크아웃, nvim에서 diff 확인, 작업 끝나면 브랜치 정리까지 자동화함
          + 좋은 커밋과 그로 만든 PR을 잘 만드는 것도 코드 잘 짜는 것만큼이나 중요한 스킬임. 하지만 실제로는 PR 쪼개기, 커밋 메시지 작성 등에 약한 개발자가 생각보다 많음
          + 데이터 사이언스 병렬 작업 팀에선, 코드 리뷰를 위한 브랜치 2~3개를 체크아웃해서 사용함
          + PR 리뷰용으로 https://github.com/sindrets/diffview.nvim을 네오빔에서 사용함. vscode의 diff와 비슷한 UI이면서 vim의 diff 모드를 활용함. 가벼운 리뷰에는 git log -p --function-context도 유용함
     * 한 명은 첫 draft를 작성하고, 다른 사람이 그걸 다듬고 머지하는 방식에 관심이 있음. 직접 해본 사람 있는지 궁금함.
          + 실제 코딩의 90% 이상은 1인이 맡고, 리뷰어가 직접 최종 반영 및 머지 책임을 지는 스타일에 동의함. 이전 직장에선 리뷰어가 무조건 merge를 담당했고, 큰 변화면 코멘트만 전달함. 누군가에게 일일이 승인 클릭을 부탁하는 방식보다 이 쪽이 훨씬 효율적임
          + 리뷰 코드 작성에 관여하면, 훨씬 깊이 있는 피드백을 줄 수 있다고 느낌. 이런 구조는 '내 코드/네 코드'가 아닌 '우리 코드'로 인식하게 해줌. TDD와 같이 반복적·협업 중심의 문화에 잘 맞음
          + 이 방식은 비동기 페어 프로그래밍과 유사함
          + trunk-based development에서 페어 프로그래밍을 활용하는 사람들을 알고 있음. 둘이서 코드 짠 뒤, OK하면 바로 main에 머지, 테스트 통과하면 즉시 배포. 실제로 잘 동작함
     * Github Pull Request 확장 플러그인을 VSCode에서 써서 로컬로 리뷰하는데 꽤 편리함. 에디터 내에서 바로 코멘트 달고 리뷰 가능함
          + Github 웹에서는 변경파일이 많을 땐 파일을 숨김 처리하지만, VSCode에서는 자유롭게 네비게이션 가능해 훨씬 쾌적함. 이 기능이 VSCode/Github 조합에서 구현된 만큼, 다른 에디터에도 확장될 수 있을 것이라 추측함
          + 좋아지는 건 맞지만, 여전히 Github와 VSCode 쌍방 vendor lock-in이 깊음
          + Github 웹에서 PR 열고 “.” 키 눌러 바로 VSCode 웹에서 리뷰하는 것도 훨씬 나은 경험임
     * Git에 first class change ID가 들어간다는 이야기는 너무 반가움! Facebook의 Phabricator diffs revision tracking과 비슷함. 더 자세히 알아볼 수 있는 링크가 궁금함
          + 근본적 문제는 git이 브랜치 정보를 제대로 관리하지 못하는 데 있음. Fossil은 커밋이 어느 브랜치에서 만들어졌는지 기억함. task branch 자체가 change ID 역할임. 물론 git은 히스토리 조작도 허용해야 해서 해결이 쉽진 않음
     * Sourcehut도 언급하고 싶음. 클래식한 패치/이슈/버그/토론을 메일로 주고받는 흐름을 계승했고, 메일링리스트·CI와 연동됨. Drew Devault의 git-send-email.io, git-am.io에서 패치 송수신 방법 리소스도 제공함

   git-review 의 접근이 좋은지는 모르겠으나 깃헙 기반의PR 리뷰가 끔찍하다는 점엔 동의합니다..
"
"https://news.hada.io/topic?id=22738","구글의 액체 냉각: Hot Chips 2025에서 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     구글의 액체 냉각: Hot Chips 2025에서 공개

     * 액체 냉각은 데이터센터의 고전력 칩 발열 문제 해결을 위해 급속히 확산 중
     * 공기보다 약 4,000배 높은 열전도율을 가져, 특히 AI 붐에 따른 TPU 냉각 수요에 대응해 구글이 적극 도입함
     * 구글은 CDU(Coolant Distribution Unit) 기반의 랙 단위 액체 냉각 루프를 운영해 유지보수와 확장성을 높였음
     * Split-flow 콜드 플레이트, 베어다이 냉각(TPUv4) 등 고성능 PC 시장의 기법을 데이터센터 스케일로 적용함
     * 액체 냉각은 팬 대비 전력 소모 5% 이하로 효율적이며, 누수·미생물 성장 같은 문제에 대비해 구글은 철저한 검증, 알림 시스템, 예방 유지보수를 병행함
     * NVIDIA, Rebellions AI 등도 액체 냉각을 채택해 데이터센터 냉각의 표준화 흐름이 가속화되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

액체 냉각의 필요성 및 배경

     * 액체 냉각은 PC 마니아 사이에서 익숙하며, 기업용 컴퓨팅 환경에서도 오랜 역사를 가짐
     * 최근 AI 및 머신러닝 워크로드의 전력 소모 증가로 데이터센터에서 액체 냉각의 중요성이 크게 확대됨
     * 구글은 물의 열전도율이 공기 대비 약 4,000배 높은 점을 주목해, 최신 칩의 고열 대응책으로 채택함
     * Hot Chips 2025에서 구글은 TPU(머신러닝 가속기) 냉각에 관련된 데이터센터 차원의 액체 냉각 방식을 소개함

구글의 액체 냉각 시스템 구성

     * 구글은 2018년부터 TPU에 액체 냉각을 적용해 다양한 실험 및 개선을 거침
     * 최신 냉각 솔루션은 서버 내에 한정되지 않고 랙 전체에 액체 냉각 루프를 적용함
     * 하나의 냉각 랙은 6개의 CDU(Coolant Distribution Unit) 로 구성되며, 이는 PC의 라디에이터+펌프 콤보와 유사한 역할을 수행함
     * 유연한 호스 및 퀵 디스커넥트 커플링을 도입해 유지보수 편의성과 설치 허용오차를 개선함
     * 6개 중 5개 CDU만 가동해도 충분한 냉각이 가능해, 한 유닛 유지보수 시에도 전체 가동 중단이 불필요함

열 교환 및 칩 레이아웃

     * CDU는 내부 냉각수와 데이터센터의 외부 공급수 사이에서 열만 교환하며, 두 액체는 직접 섞이지 않음
     * CDU에서 나온 냉각수는 매니폴드를 통해 다수의 TPU 서버로 분배됨
     * TPU 칩 연결은 순차(시리즈) 구조로, 루프 내 마지막 칩의 열 수요를 기준으로 전체 냉각 예산을 산정함

냉각 기술의 최적화

     * Split-flow cold plate 구조를 적용해 기존의 직선형 설계 대비 향상된 냉각 성능 확보함
     * 추가적으로 bare-die 냉각(TPUv4, 과거 TPUv3는 lidded)을 적용해, 보통 고급 PC 마니아들이 열전달 효율을 높이기 위해 사용하는 ‘delidding’과 유사함
     * TPUv4는 v3 대비 1.6배 높은 소비전력으로 인해 이러한 추가 냉각 방식을 필요로 함

전력 효율 및 열 이동

     * 액체 냉각 펌프의 전력 소비는 기존 공랭 팬 전력 대비 5% 미만으로 나타남
     * 구글 시스템은 water-to-water 열교환 방식을 통해, 실질적인 냉각 동력을 대부분 펌프에서 담당함
     * PC 마니아 환경은 대부분 팬-라디에이터 조합이 남아있어, 데이터센터만큼 전력 이점이 크지 않음

유지보수, 신뢰성, 안전

     * 유지보수 관점에서, 미생물 번식이나 누수 위험 등 수냉 시스템의 공통 리스크가 데이터센터급에도 존재함
     * 퀵 디스커넥트 피팅, 예비 CDU 등 다양한 유지보수 편의 장치를 통해 다운타임 없이 대규모 관리를 지향함
     * 예방적 유지관리, 누수 테스트, 각종 이상 신호 탐지 및 체계적인 대응 프로토콜을 마련해 전사적 일관성 및 신뢰성 방안을 확보함
     * 이는 개별 PC 마니아들의 비공식적 관리 방식과는 대조적임

업계 동향 및 AI 열풍

     * 엔비디아, Rebellions AI 등도 Hot Chips 2025 전시에서 다양한 외부 액체 냉각 시스템을 선보임
          + NVIDIA GB300 서버: 외부 액체 냉각 포트와 팬을 함께 배치
          + Rebellions AI는 한국 기업으로, 새로운 ML 가속기 ‘REBEL Quad’ 프로토타입을 쿨러 및 칠러를 결합한 유사한 방식으로 시연함
     * AI 워크로드 증가는 앞으로도 데이터센터용 액체 냉각에 대한 수요와 채택을 더욱 가속화할 전망임

        Hacker News 의견

     * 예전에 Azure 데이터센터 구축을 총괄하는 SVP의 인터뷰를 본 적 있음, 그가 어느 순간 자신이 더 이상 컴퓨터 사업에 종사하는 게 아니라 공업용 냉방 사업에 종사한다는 걸 깨닫고 일이 훨씬 쉬워졌다는 말이 기억에 남음, 이번 기사를 읽으면서 그 이야기가 바로 떠오름
     * 메인프레임(S/3x0, Cray 등)은 50년 넘게 물 냉각을 광범위하게 사용해왔고, 슈퍼컴퓨터급 HPC 데이터센터도 최소 20년간 액체 냉각을 활용해왔는데, 구글급 데이터센터 설계를 PC 매니아 쿨링과 비교하는 건 다소 이상하게 느껴짐, 이건 과거를 망각하거나 비교 대상이 완전히 잘못된 예시임
          + bri3d가 지적한 부분 덕분에 내가 초기에 이해한 것보다 이번 구글의 사례가 새롭지 않다는 점을 알게 됨, 혁신 포인트는 “물을 쓴다”가 아니라 서버를 냉각하는 칠러가 시설 밖에 설치되어 있다는 점임, 대부분의 메인프레임도 물 냉각으로 내부 열을 바깥 쪽으로 옮겨서 히트싱크나 쿨링팬이 열을 날려주게 하는데, 구글은 건물 내부가 아니라 시설 전체용 거대한 칠러를 이용해 직접 각 서버에 냉각수를 순환시킴, 반환된 뜨거운 물을 칠러 타워에서 다시 냉각함, 실질적으로 공기 기반 냉각은 칠러 타워를 제외하곤 완전히 배제됨, 일부 서버/랙만 하는 게 아니라 데이터센터 전체를 동시에 처리함, 칠러 유지보수나 펌프 고장 났을 때 어떻게 하는지 궁금함, 무중단을 위해 엄청난 이중화가 있을 것 같음, AWS도 유사한 시스템을 도입했고 설명 사진이
            명확하니 참고하면 좋음 AWS 데이터센터 액체 냉각 기사
          + 구글이 값싼 일반 하드웨어 기반의 역사를 가지고 있으니 이런 변화가 놀랍지 않음, 마치 x86 서버가 메인프레임 기능(가상화 등)을 흡수하는 데 수십 년이 걸린 것과 비슷함 관련 블로그
          + 기사에서 “액체 냉각은 PC 매니아에겐 익숙하고 엔터프라이즈 컴퓨트에서도 오래된 개념”이라고 했음, 데이터센터도 서버 단위로 수동 냉각과 고온 동작 온도로 가던 트렌드였지만, 이번 건은 그 트렌드를 크게 뒤집는 사례임, 아마도 행 단위 냉각(per-row cooling)이 주요 원인일 수 있음
          + HPC 데이터센터가 20년 넘게 액체 냉각을 썼다고 했는데, 주로 랙 도어 등 부위에 적용됐던 것 아닌지 궁금함, 최근 2세대 서버에서부터 진짜 서버 내부로 직접 액체 냉각(DLC)이 적용된 듯함, 인텔 하이엔드 CPU 때문에 강제 적용된 측면 있음, 기존 데이터센터에 도입이 어려워서 골치 아팠고, 냉각 가방이 새는 문제로 서비스 요청도 다수 넣었음(제조사 비공개)
          + 초대형 데이터센터는 보통 전력 밀도를 최대화하지 않아도 되고, 밀도를 높이면 여러 문제가 생겨 디자이너들이 오히려 피함, 현대 HPC 클러스터가 밀도를 고민하는 건 실상은 잘못된 관점일 수 있음, 다만 ML 워크로드의 경우에는 물리적으로 가까이 배치하면 인터커넥트 효율이 좋아지는 장점이 있음
     * 이론적으로 데이터센터 냉각은 단순함, CPU는 60~70도에서 동작하고, 외부 온도는 대체로 30도 이하이니, 팬과 펌프의 약간의 도움이면 열이 자연스럽게 ‘흘러 내려가는’ 구조임, 문제는 공기 냉각에서 시설의 직원들이 컴퓨터 냉각에 쓰이는 동일한 공기를 호흡해야 한다는 점임, 냉방 온도가 높아지면 직원 건강엔 좋지 않음(우리는 핫 아일을 겨울에도 100F 정도까지 운용하고, 3개 랙마다 히트 익스체인저를 설치해 외부 칠러수로 냉각 중임), 외부 온도가 올라가면 열을 집밖으로 제대로 내보내려면 쿨링 유체 온도가 더 높아야 하고, 칠러가 꼭 필요함, 더위가 심할 땐 에너지 소비도 대폭 늘어남, 만약 데이터센터 전체를 액체 냉각으로 바꾼다면 랙에서 나오는 쿨런트 온도를 훨씬 올릴 수 있고, 가장 더울 때도 칠러 없이 열 방출이 가능할 것 같음, 현재는
       일부만 액체 냉각하고 있고 쿨런트 온도는 핫 아일 온도에 맞춰 제한됨, 이 온도만으로도 이미 꽤 덥다고 느낌
          + “CPU가 60-70도, 외부는 30도 이하이니 열이 알아서 내려간다”라는 관점이 맞지 않음, 실제로는 CPU가 동작 전력에서 발생한 열을 외부로 전달해야 하고, 단열(thermal impedance)이 크면 CPU가 과열되어 고장날 수 있음
          + 15년 전 IBM이 ETH Zurich에 설치한 슈퍼컴퓨터는 60도짜리 뜨거운 물 냉각수를 사용했고, 방열기를 통해 건물 온수 시스템과 직접 연결했었음 Aquasar 소개
          + 언젠가엔 냉방 효율 극대화를 위해 데이터센터 근무자들이 히트수트(방열복) 같은 것도 입고 들어가게 될지 궁금함
     * 기사에서 TPU 칩을 직렬로 연결해서 냉각수 루프를 통과시키고 마지막 칩 온도에 맞춰 용량을 예산한다는 이야기가 있었음, 네 개의 칩이 각각 250W를 내고 펌프가 분당 1리터의 물을 밀어준다면, 입구 대비 출구는 반드시 14도 더 뜨거워짐, 이건 직렬이든 병렬이든 동일함(물의 비열 때문)
          + 직렬 연결의 경우 마지막 칩에서의 열전달 효율이 병렬 연결보다 낮을 수 있음, 물이 처음보다 더 뜨거운 상태에서 마지막 칩을 만나기 때문임, 온도차가 작으니 열이 더 천천히 빠짐
          + 실제로는 직렬과 병렬 구조에 따라 흐름 속도를 다르게 계산해야 함, 엔지니어링 관점에서 실질적 차이가 발생함
          + 압력을 충분히 높이면 분당 1리터보다 훨씬 더 많은 유속이 가능함, 데스크톱 기준의 18W보다 서버는 대략 10배 정도임
          + 직렬 연결이면 일부 칩이 ‘과냉각’되고, 가장 뜨거운 칩에 맞추려면 더 많은 냉각수가 필요함
     * 나는 예전처럼 Google 인프라에 크게 기대하지 않음, Google이 인터넷 자유를 침해하는 행보를 계속해 내 호감도가 크게 떨어짐, 이제는 그들이 도입하는 액체 냉각 시스템 같은 것에도 별 감흥을 느끼지 못함, 디테일에 따라 어렵긴 하겠지만 특별히 혁신적으로 느껴지지도 않음, 혹시 Google 직원이 이 글을 보고 속상하더라도 개인이 아니라 Google 자체의 문제라고 생각함, 멋진 일은 다른 곳에서 하는 것도 고려해보면 좋겠음
     * B1M에서 본 흥미로운 사례가 떠오름, 파리 올림픽 수영장은 인터넷의 열로 데워진다고 함 YouTube 영상
     * AI가 물을 낭비한다는 언급을 종종 보는데, 이번 사례도 그런 방식일지 궁금함, 혹시 CDU가 시설 내 물을 증발식 냉각에 사용하는지 알고 싶음
          + CDU는 데이터센터 내부에 설치되고, 랙 쿨런트에서 시설 쿨런트로 열만 넘김, 실외에는 열교환 시설이 있으며, 이 과정에서 종종 쿨링 타워에 물을 뿌려 증발식 냉각을 함, 데이터센터마다 형태는 다르지만 facility 쿨링 자체는 모두 존재함, AI가 물을 낭비한다는 논의는 다소 피로함, 물은 순환 구조 내에서 효율 위치로 이동될 뿐임, 마켓에서 물 관련 비용과 외부효과가 실제로 반영되면 더 의미있는 논의가 될 것 같음, 미국에선 물 가격과 권리, 실제 물의 효용 등이 제대로 연관되어 있지 않은 게 문제임
          + AWS도 비슷한 기사를 최근에 냈음 AWS 데이터센터 액체 냉각 기사, 다만 배출된 뜨거운 물을 어떻게 식혀서 재사용하는지 방법이 명확하게 설명된 사례를 아직 못 봤는데, 이 부분이 제일 궁금함
          + AI가 물을 사용하는 것 관련해서 구체적인 수치나 대화는 거의 없고, 마치 도로를 차가 사용하는 것처럼 물을 쓴다는 식의 모호한 언급만 봄, 실제로 물이 낭비된다는 인상을 주는데, 명확한 데이터가 있으면 애매하게 암시하지 않을 것 같음, 물이 실제로 소비된다면 식수로 쓸 수 없는 상태로 변하거나, 증기로 사라지거나, 슬러지 등에 갇혀 회수 불가한 경우임, 이런 일이 실제로 벌어지고 있는지, 그리고 이게 진짜 문제가 되는지 알고 싶음, 데이터 없이 무의미한 수치만 도는 게 답답함
          + 관련 기사 있음 Texas AI 데이터센터와 물 낭비 이슈
     * 물 냉각의 경제성이 궁금함, 칩이 비싸져서 더 빠르게 돌릴 필요 때문에 액체 냉각이 유리해진 것인지, 아니면 데이터센터 공간이 비싸져서 더 밀도를 올릴 필요 때문인지, 혹은 신호 전송 거리(1피트 = 1나노초)를 줄이면 연산 효율이 그만큼 올라가기 때문인지 생각 중임
          + 데이터센터 전체 전력 중 상당수를 냉각에 소비함, 냉각 효율만 높여도 바로 비용 절감임
          + 배선 거리의 영향은 실은 매우 작다고 봄, 최상의 인터커넥트 패브릭도 핑퐁 시간(요청/응답 왕복)이 1마이크로초 수준인데, 피트 단위 길이 변화는 수십 나노초 차이임, 대규모 클러스터에서 밀도를 두 배로 높여도 왕복 신호 지연이 60나노초 가량 늘어나는 수준임(전체 1마이크로초 중 6% 미만), 실제 어플리케이션엔 큰 영향 없음, 다만 밀도가 올라가면 백플레인이나 구리 커넥터로 더 많은 칩을 직접 연결하기엔 유리해짐
          + 실제론 이유 2번과 3번의 혼합임, 칩이 점점 작아지고 더 많은 전력을 쓰기 때문에 같이 뜨거워지고, 수많은 팬이 추가 전력을 더 많이 쓰게 됨, 액체 냉각은 chip→liquid 직접 냉각이라 팬, 에어컨, 추가 순환 비용이 절감됨, ServeTheHome의 관련 기사 참고 Supermicro 액체 냉각 소비전력 영향 분석
          + 클래식 컴퓨팅 작업은 잘 모르겠지만, TPU처럼 메모리 중심 연산에는 배선 거리 차이가 꽤 중요하다고 생각함
          + 칩들이 초고속 네트워크로 연결되어야 해서 밀도를 높이는 게 중요함
     * 이론상 PC 사용자도 화장실 물탱크에 방열수를 순환시키면 플러시할 때마다 효율 좋게 냉각할 수 있음, 미래가 바로 여기 있음
          + 반대로 지역난방 플랜트에서 하수에 폐열을 빼내 난방용으로 쓰는 사례도 이미 존재함, Utrecht 하수 처리장 열펌프 설명 참고하면 됨
     * 2006~2012년 사이 데이터센터에서 자주 근무했음, 밤늦게 찾아가야 할 때가 많았음, 데이터센터는 생각보다 열악한 환경임, 냉각이 좀 더 조용하고 극단적이지 않았다면 좋았을 것 같음, 포트 등이 뒤쪽에 있는 이유는 바로 그 쪽이 공기 흡입구이기 때문임, 따뜻한 쪽으로 가서 손을 녹이거나 해야 했던 경험임
"
"https://news.hada.io/topic?id=22737","MacOS 26 Tahoe의 ‘죽은 카나리아’ 유틸리티 앱 아이콘","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  MacOS 26 Tahoe의 ‘죽은 카나리아’ 유틸리티 앱 아이콘

     * MacOS의 유틸리티 앱은 시스템 관리 도구로, 오랫동안 /Applications/Utilities/와 CoreServices 폴더에 배치되어 왔음
     * MacOS 26 Tahoe에서 새로 적용된 아이콘들은 모두 스쿼클(squircle) 강제 적용과 함께 큰 렌치와 볼트 모티프를 사용
     * 이 디자인은 아이콘의 식별성을 10% 이하로 축소시키고, 특히 Disk Utility처럼 중요한 앱조차 단순한 Apple 로고만 남음
     * 개별 앱 아이콘들도 문제적이며, Expansion Slot Utility는 빈 소켓, AppleScript Utility는 기울어진 스크롤 등으로 의미 전달 실패
     * 이러한 변화는 단순 미관 문제가 아니라, Apple의 디테일 감각 상실을 상징하는 ‘죽은 카나리아’ 신호같음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

MacOS 유틸리티 앱의 역사

     * MacOS는 오랫동안 시스템 기능을 담당하는 유틸리티 앱을 별도로 제공해 왔음
          + 주요 위치는 /Applications/Utilities/
          + 덜 쓰이거나 시스템적 성격이 강한 앱은 /System/Library/CoreServices/에 위치
     * 예시: Disk Utility, Ticket Viewer, DVD Player, About This Mac 등

Tahoe 아이콘 디자인 변화

     * MacOS 26 Tahoe에서 모든 앱 아이콘은 squircle 형태로 통일됨
     * 업데이트되지 않은 앱은 “squircle jail”에 들어가 회색 배경에 축소 표시됨
     * 유틸리티 앱의 경우, 렌치와 볼트를 기본 테마로 채택
          + 렌치 안에 볼트
          + 볼트 안에 앱의 상징 요소
     * 결과적으로 실제 앱의 정체성을 보여줄 수 있는 면적은 10% 미만

디자인 문제점

     * Disk Utility: 아이콘에 단순히 Apple 로고만 있어 용도를 전혀 알 수 없음
     * Expansion Slot Utility: Mac Pro 전용인데, 아이콘은 단순한 빈 소켓 3개로 의미가 불분명함
     * AppleScript Utility: 전통적인 스크롤 심볼을 사용했으나, 기울어진 배치로 불안정하게 보이는 오류 발생
     * Wireless Diagnostics: 네 가지 중 가장 낫지만 여전히 미흡

렌치 모티프의 한계

     * 렌치 도형 자체가 비현실적이며 얇아서 실제 공구로는 쓸 수 없는 형태
     * 과거 MacOS 아이콘에 쓰였던 렌치와 비교해도 설계 미숙이 드러남
     * 아이콘의 예술적 완성도 부족과 디테일 무시가 동시에 드러남

장인과 딜레탕트

     * 서랍장의 뒷면 마감을 이야기 하던 사람들의 작업 같지는 않아 보임

     아름다운 서랍장을 만드는 목수라면, 뒷면에 합판을 사용하지 않을 겁니다. 벽을 향하고 있어서 아무도 볼 수 없더라도요. 거기에 있다는 것을 알기에, 뒷면에 아름다운 나무 조각을 사용하는 거죠. 밤에 편안하게 잠들려면 미적인 면과 품질이 끝까지 유지되어야 합니다.
     - 스티브 잡스
          + 이 아이콘들은 너무 형편없어서, ""얼마나 힘들겠어?""라고 외치며 며칠 일하다 자기 손가락 하나를 잘라버리고 마는, 훈련받지 않은(딜레탕트/dilettante) 목수들이 만든 것처럼 보임
          + 아니 이 컬렉션 전체가 예술적 재능도 없고 디테일에 대한 안목도 없는 사람의 작품 같음. 하필이면 다른 회사도 아니고 애플에서
     * 문제는 개별 아이콘의 품질 저하가 아니라, Apple의 전반적 미적 감각 저하 신호라는 점
     * MacOS 26 Tahoe의 유틸리티 앱 아이콘들은 ‘죽은 카나리아’ 처럼, 더 큰 문제의 전조를 보여주는 사례임

        Hacker News 의견

     * 2007년부터 2011년까지 Mac OS X과 Windows는 디자인 언어의 황금기였다고 생각함, 그 시기 Windows Aero와 Mac OS X Aqua는 매우 아름다웠던 그래픽 셸이었음, 이후로는 흰색 일색의 평평하고 심심한 GUI만 남아 황량해졌다고 봄, 요즘의 아이콘 둥글리기(스퀴클화, 안드로이드는 원형화)는 직관적이고 즐거웠던 UI가 영원히 균일해지며 사라지는 과정 같음, Leopard 시대의 프로그램 아이콘은 걸작이었음—Pages의 인디고 잉크병, Time Machine의 웜홀 그래픽, 친근했던 창의 그라디언트와 커다란 신호등 버튼들, 저해상도 디스플레이에도 잘 어울린 서체, 모든 크기에 맞춘 개성 있는 아이콘까지, Apple의 대규모 휴먼 인터페이스 가이드라인은 Yosemite 이후 폐기됐음, Windows도 당시 진한 파랑의 시작 버튼과 멋진 태스크바, 새로워진 Welcome Centre, 직접 새로 개발한 Media Player와 Photo Viewer,
       광고 없는 고해상도 게임 아이콘, Flip 3D 같은 시도 모두 이 시기의 아름다움을 기억하게 해줌, 이 의견을 끝까지 고수할 것임
          + 나도 그 언덕에서 같이 죽을 각오임, Vista는 단점도 있지만 정말 숨막힐 만큼 아름다운 운영체제였음, 시작 메뉴의 매트 질감과 강렬한 빨간 종료 버튼, 모든 게 빛나고 굴절되는 그 촉감이 그리움, 첫 아이폰도 느낌이 차원이 달라졌고, skeumorphic 디자인에 레티나 디스플레이로 소프트웨어 그 이상, 새로운 디지털 세계와 소통하는 경험이었음, Macbook도 마찬가지, 모든 앱이 예쁘게 네이티브로 그려지는 느낌이었음(Windows가 개발자 점유율은 높았지만 못 따라했음), 게다가 이 모든 게 요즘 칼라 정확도 좋은 디스플레이로 보면 우스울 LCD에서, 해상도도 지금의 1/4도 안 됐을 때였음, 문제는 소프트웨어도 돈이 많이 돌기 시작하면서 돈벌이에만 집중하는 사람이 몰리고, 열정 가득했던 사람들이 밀려났다는 점임, 예술과 엔지니어링의 품질이 떨어지는 현상과
            MBA가 세력 키우는 현상은 으레 동반됨
          + Luna 테마가 색상이 훨씬 나음, Aero는 연두색·기름기 같은 색이라 별로임, Luna는 밝고 선명한 초록색 체크박스, Aero는 기름기만 남, Luna의 라이트블루 스크롤바, 오렌지 창 배경, 주황색 버튼 하이라이트, Power Blade 스타일 프로그레스바 모두 다채롭고 멋졌음, Aero는 gimmicky한 애니메이션이 가미됐고, 창 제목바도 색이 칙칙함, 그리고 Aero는 창 테두리가 너무 두꺼움
          + 나는 오히려 그 시기보다 약간 전의 덜 스큐어모픽 디자인이 더 좋았음, Windows 2000 기본 테마, Mac OS 9처럼 눌릴 수 있는 것과 없는 것이 드롭섀도우로 분명했고, 선택 영역도 적당한 색상으로 구분이 쉬웠음, 과도하게 과일 칵테일처럼 색이 섞이는 테마로 가지도 않고 말임
          + Vista가 기억으론 Apple만큼 HIG 문서를 잘 갖춰두었고, 통합된 UI로 바꾸고자 굉장히 열심히 했던 게 인상적이었음, 그런데도 지금의 플랫 스타일에도 Vista 이전 9x 위젯의 잔재가 남아있고 이걸 결국 못 뽑아내서 플랫하게만 바꿨다고 기억함, Office 2007과 Windows Live Suite 출시 시점부터 다시 인터페이스를 바꾸기 시작하고, Windows 7은 살짝 플라스틱 느낌의 플랫과 리본 인터페이스가 덧씌워짐, 커뮤니티에서 Windows Taskforce라는 프로젝트가 있었고 Vista와 7 사이에 MS가 더 polishing 하기를 바라는 모형 화면들이 많았으나, MS가 Windows 8에서 Metro로 선회하며 이런 노력이 사라짐
          + 나도 그 언덕에 함께 설 것임(XP도 괜찮았다고 봄), Flip 3D는 지금 Win-Tab과 비교하면 그냥 장난이었다고는 해도, 예전엔 아이콘 자체가 역할을 명확히 보여주던 시절이 매우 그립다는 느낌임
     * AppleScript 아이콘을 보면 종이 모양이 S자 형태로 말려 있는 걸 알 수 있음, 새 디자인에서는 종이의 각도 회전과 테두리 강조가 약해져 그 이미지가 깨짐, 이런 건 사소한 트집처럼 보일 수는 있지만, Apple의 '섬세하게 생각한 디테일' 오라가 줄어드는 느낌임, 그런데 Script Editor 앱에서는 이미 이 회전된 종이 아이콘을 사용하고 있었으니, 이 변화를 너무 심각한 신호로 받아들이지는 말아야겠다는 생각임
       관련 이미지
          + Liquid Glass 스타일은 마치 10대가 만든 프리웨어 아이콘 팩을 gnome-look.org에서 막 가져온 것 같은 느낌임, 예전 같으면 다들 비웃었을 것 같음
          + Script Editor 로고는 원래 종이에 AppleScript 로고가 인쇄되어 위에서 본 관점으로 보여줬고, 그 위에 펜이 얹혀 있었음, 그런데 실제 물리적인 모양을 제대로 이해하지 못한 디자이너가 단순히 각도만 남긴 채 바꿔버려 개념 없이 틀린 디자인이 됐다고 생각됨
            http://upload.wikimedia.org/wikipedia/en/…"">예전 로고 이미지(스크랩)
          + Big Sur 시대의 '대부분 스퀴클이지만 아직 완전히 스퀴클 감옥은 아님' 느낌이 물씬 나는 아이콘임, 이 아이콘도 별로 좋아보이지 않음
     * Apple 소프트웨어의 쇠퇴를 보여주는 전조: 스크립팅과 새 Notification Center, macOS가 초보자부터 파워유저까지 모두 환영받던 이유는 거의 모든 기능이 GUI, 단축키, 스크립팅, 커맨드라인 등 모든 방식으로 접근할 수 있었기 때문임, 그런데 어느 순간부터 AppleScript 지원이 줄어들기 시작했고, 파워유저가 중요시하는 앱에서 스크립팅 의존도가 높은데 이 기능이 사라지니 고급 사용자가 소외되고 있다고 느낌, 여기에 Notification Center도 나오면서 키보드로 알림을 다룰 수 없고, 마우스를 강제로 써야만 하게 됨, 기본적인 OS 기능조차 주 입력장치인 키보드로 접근을 못 하게 된 상황, 대상 고객 문제를 넘어서 Apple의 무관심이 드러나는 순간이라 봄, 독점 기업은 흔히 무너지지 않고 점차 별 의미 없어지는 경향이 있는데 윈도가 그랬고 맥도 마찬가지로 Apple 내에서
       중심성을 잃고 있다는 느낌임
     * John Gruber란 사람은 예전부터 Apple에 너무 매몰되어 있거나 '왜곡현실장'에 노출된 이미지가 있어서, 이렇게 강하게 Apple을 비판하는 게 신기하다는 생각임, 내가 Gruber를 잘못 봤던 건지, 아니면 이 사안이 진짜 위험 신호인지 의문임
          + Apple의 골수 지지자(John Gruber)는 이상적인 Apple의 모습을 믿는 경향이 있음, 실제 기업과 그 이상은 다르지만, 종종 현실 기업의 결정이 이상과 부합한다고 느끼면 지지하다가, 디자인이나 iCloud 5GB 무료 티어 같은 문제로 이상과 멀어지면 강하게 비판함
          + 나 그를 팔로우하는데, Apple 전문 블로거지만 Apple에 호의적이기만 한 건 아님, 올해 초에도 엄청 비판적이었음
            관련 글
          + Gruber가 최근 Apple에 실망해서가 아니라, 예전엔 자신의 취향과 Apple의 가치관이 잘 들어맞았기에 동조해보였던 것임, 그간 때때로 비판도 했지만 요즘 Apple이 그 가치관에서 멀어지니, Apple 블로거 커뮤니티가 Alan Dye의 ""Liquid Glass""처럼 난장판이 된 뒤 더 강하게 비판하는 모습임
          + 그에게서 빼앗을 수 없는 한 가지는 자신의 의견을 아주 논리적으로 설명한다는 점임, 그래서 혹시라도 Apple에 너무 호의적이라고 느껴져도, 왜 그런지 논리적 근거를 알 수 있음
          + 의견이 왔다갔다하는 편이지만, 새 아이콘이 별로라는 데 동의하고 Gruber가 좋은 지적을 했다고 생각함, 특히 파란색 아이콘이 렌치(공구)처럼 보이지도 않고, 예전 아이콘이 낡았다고 해도 새 버전은 그냥 '나쁨'임
     * Disk Utility 아이콘이 이제 단순히 Apple 로고(볼트+렌치+스퀴클 안에)가 전부라는 점을 꼬집는 Gruber 말이 틀렸다고 생각함, 사실 그건 Tim이 White House에서 Donald에게 건넨 'glass disk'임
          + 아니, 그때 뇌물로 건넨 디스크는 동그란 모양이었는데, 이건 육각형임, 큰 차이임
     * NextSTEP 개발자들이 다 은퇴했고, 진짜 위기 신호는 SwiftUI 공개 때였다고 생각함, 지금도 쓰기 힘든데 무리하게 밀어붙이고 있음, 한때 완벽한 UI로 자부했던 회사가 '근본적으로 망가진' 프레임워크를 억지로 React에 익숙한 웹 개발자에게 어필하고자 만든 느낌임, Swift 언어 자체도 이걸 맞추려고 바꿨음, 이런 모습에 Steve Jobs가 무덤에서 벌떡 일어날 듯함
          + 나는 올드스쿨 Cocoa 개발자로, 여러분의 iPhone 내장 앱도 직접 만든 경험 있음, 그런데도 SwiftUI를 정말 좋아함, 내부적으로 잘 알고, 만든 개발자들과도 이야기해봤음, 아직 완성된 건 아니지만 쓸 때 기쁨이 많음, 특히 접근성 작업 같은 지루하고 반복적인 80%를 쉽게 처리해줘서 큰 파워가 있음, Apple이 지금 MS가 웹앱(자바스크립트) 혁명 때 겪은 것처럼 고전 소프트웨어의 위기를 겪고 있음, 왜냐하면 브라우저에서 다 돌아가면 OS 자체가 의미가 없어짐, SwiftUI와 Liquid Glass를 끝까지 밀고 가는 건 네이티브 앱이 더 매력적이라 믿기 때문임, 모든 사용자가 구글 데스크톱과 모바일로 옮겨가면 Apple은 죽음임
          + SwiftUI와 Jetpack Compose(안드로이드 핵심 UI 트리 새로 쓴 것)는 문제 많지만 WinUI가 5년 동안 이룬 결과와 비교하면 훨씬 낫다고 봄, Windows 데스크톱 개발이 그만큼 뒤쳐졌다는 의미임
     * Crescent wrench는 사실 조절 기능이 있는 브랜드 이름이고, Gruber가 말한 건 open-end wrench(오픈 스패너)거나 영국권처럼 'spanner'라고 불러야 맞음, 어쨌든 이 아이콘 그린 아티스트가 실제로 공구를 써본 적 없을지도 모른다고 생각됨
          + 누가 렌치 아이콘이 허술하다고 이걸 그린 사람이 실제 공구를 써본 적 없다고 하는 건 좀 과한 비약이라고 생각함, 그래도 이런 과장이 HN 댓글에서 이어지는 건 별로 놀랍지 않음
          + 나는 오히려 반대임; Gruber가 오픈 스패너를 써본 경험이 너무 적은 것 아닐까 싶음, 현대의 단조 스패너만 써봤으면 저렇게 폭 넓고 각진 게 이상해 보이겠지만, 20세기 초반에 썼던 구식 공구(오토바이 수리 공구)는 이런 디자인이 꽤 있었음, 실제로 이런 스패너를 보면 미국식/영국식 단위라 쓸모없다고 느낄 것임 (-:
          + Gruber가 렌치를 얼마나 진지하게 써봤는지는 모르겠음, 최신 아이콘 핸들 각도가 0도(실제는 15도가 정석)라는 디테일을 언급하지 않으니까, 구 아이콘은 옥타곤형 오픈 렌치이고 헥사곤 아니었음, 이런 사소한 아이콘에 집착하는 게 실수라고 생각하지만, 만약 비판하기려면 더 철저하고 일관되게 해야 할 것임
          + 내 짐작에는 아이콘 중앙 '너트' 부분이 명확히 보이게 하려고, 놀이로 그린 리얼리즘 대신 알아보기 쉽도록 배치한 것 같음, ""어차피 렌치 자체가 아니라 소프트웨어 앱 아이콘이니까""라는 생각이었을 듯함
          + Crescent wrench는 1. 조절식이면서, 2. 항상 잘못 선택하는 공구임, 저건 오픈 엔드 렌치, 혹은 콤비네이션 렌치 절반임, (크로우스풋 같은 것도 있지만 넘어가겠음)
     * 디자인팀이 ""차라리 바꾸지 않는 게 맞다""라는 결론을 내리는 사례가 있는지 궁금함, 사람들이 직장에서 아무 일도 안 해야 한다고 주장하는 게 매우 두려울 수 있고, 결국 '바꾸지 않을 일'을 스스로 찾는 데 어려움이 있다고 생각함
          + 이런 경우에는 선택적 편향 때문에 사람들이 별로 알아차리지 못함, 사람들이 리프레시를 기다리다가 막상 변한 게 없어 실망하는 경우도 많음, 실제로는 아무 문제 없었지만 말임, 하드웨어는 Volvo X60, X90 같은 자동차 시리즈가 그랬고, 소프트웨어는 Chrome처럼 15년간 큰 비주얼 변경이 한 번뿐임
          + 참고로 같은 디자인 스타일로 조각, 위젯, 바탕화면, 사용자 아이콘, 계산기 같은 요소를 더 만들 수도 있음
          + Rolex가 대표적인 예임
          + OS X Snow Leopard도 바뀐 게 거의 없는 좋은 예임
     * ""MacOS 26 Tahoe의 새 아이콘이 객관적으로 끔찍하다""라는 주장에, 요즘 '객관적으로'가 '주관적으로'와 같은 의미처럼 쓰이는 게 아닌가 의문이 듦(마치 'literally'가 '비유적' 의미로 쓰이듯), 새 아이콘을 좋아하지는 않지만 Gruber가 제시한 사례들은 객관적 근거도 일부 있고, 아이콘이 절대 '객관적으로 나쁠 수는 없다'고 봄, 디자인 변화에 익숙해지면 처음엔 거부감 있다가 점차 무덤덤해진 경험이 종종 있음(특히 자동차 디자인에서 이런 걸 자주 느낌)
     * 4가지 아이콘을 두고 이렇게 지적하는 게 말이 안 된다고 느껴짐, 어차피 이전 버전도 아이콘 4개를 그냥 끼워맞춘 느낌이었음, Mac Pro를 두 공구 위에 얹었다고 'Apple의 위대한 아이콘'이 된다는 건 동의할 수 없음
          + Gruber가 원래부터 예전 아이콘도 그다지 좋다고 하지 않았다는 점을 정확히 짚어야 함
"
"https://news.hada.io/topic?id=22662","6TOPS 연산력 + 8K 미디어 처리: Jetson Nano 호환 ArmSoM AIM7 AI 모듈 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       6TOPS 연산력 + 8K 미디어 처리: Jetson Nano 호환 ArmSoM AIM7 AI 모듈 출시

   ArmSoM RK3588 AI Module7（AIM7）는 Rockchip RK3588 칩을 기반으로 한 오픈소스 AI 컴퓨팅 모듈로, NVIDIA Jetson Nano와 호환되는 폼 팩터와 커넥터를 채용하여 에지 컴퓨팅 및 임베디드 AI 애플리케이션을 위한 고성능,低成本 솔루션을 제공합니다.

   • CPU: 4x Cortex-A76 @ 2.4GHz + 4x Cortex-A55
   • GPU: Mali-G610 MP4
   • NPU: 6 TOPS (INT4/INT8/INT16/FP16 혼합 연산)
   • 메모리: 8GB/32GB LPDDR4x
   • 스토리지: 32GB/64GB/128GB eMMC 5.1

   • 디코딩: 8K@60fps (H.265/VP9/AVS2), 8K@30fps (H.264)
   • 인코딩: 8K@30fps (H.265/H.264)
   • 다중 디스플레이 출력: 최대 4개의 독립 디스플레이 출력 지원 (HDMI 2.1, DP 1.4, eDP, MIPI-DSI)

   • 고속 인터페이스: PCIe 3.0 (4 lane), USB 3.1, SATA 3.0, Gigabit Ethernet
   • 확장 인터페이스: 40-pin GPIO (라즈베리 파이 호환), MIPI-CSI (카메라용), M.2 Key (Wi-Fi/BT용)
   • 캐리어 보드 지원: AIM-IO 캐리어 보드 제공, 다양한 인터페이스 옵션

   • OS: Debian, Ubuntu, Armbian 지원 (Linux kernel 5.10)
   • AI 개발 도구: RKNN-Toolkit2 지원, TensorFlow/PyTorch/Caffe 등 모델 변환 지원
   • 오픈소스 하드웨어: 회로도 및 하드웨어 설계 파일 제공

   다수의 제품 응용에 적합되는 다양한 응용이 있습니다. 예를 들어보면, 에지지 컴퓨팅, 기계시 비전, 스마트 VR, 공업 자동화, 디지털 폼, AIoT 네트워크, 라이트 AI 서버(예를 들어 YOLO 대상 검출) 등이 있습니다.

   현재 Crowd Supply에서 크라우드펀딩 진행 중. 후원자는 프로젝트 페이지를 통해 기술 질문을 제출할 수 있으며, ArmSoM 팀이 이메일로 회신합니다
"
"https://news.hada.io/topic?id=22674","io_uring, kTLS, 그리고 Rust를 활용한 제로 시스템콜 HTTPS 서버","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             io_uring, kTLS, 그리고 Rust를 활용한 제로 시스템콜 HTTPS 서버

     * 고성능 웹 서버를 만들기 위해 기존에는 select(), poll(), epoll 등 다양한 이벤트 기반 모델이 사용됨
     * 하지만 이러한 시스템콜들의 성능 한계로 io_uring이 등장, 요청을 큐에 넣어 커널이 비동기로 처리하는 방식을 도입함
     * kTLS는 커널이 TLS 암호화 처리를 담당, sendfile() 사용 가능성과 하드웨어 오프로딩 등 추가적인 최적화가 가능해짐
     * Descriptorless files의 도입으로 파일디스크립터를 직접적으로 전달하지 않으면서 io_uring에 최적화된 접근 방식 제공함
     * Rust, io_uring, kTLS를 결합한 tarweb 오픈소스 프로젝트를 통해 요청별로 추가 시스템콜 없이 HTTPS를 제공, 안전성과 메모리 관리에 관한 이슈도 논의함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

고성능 웹 서버 구조의 진화

     * 2000년대 초반부터 고용량 웹 서버에 대한 요구가 증가함
     * 초기에는 각 요청마다 새로운 프로세스를 생성하는 방식이 일반적이었으나, 이는 높은 비용 문제로 인해 preforking 기법이 등장함
     * 이후 스레드 도입 및 select(), poll() 활성화를 거쳐 컨텍스트 스위칭 비용을 줄이는 방식으로 발전함
     * 다만, select()와 poll() 방식도 연결 수가 많아질수록 커널에 큰 배열을 빈번히 전달해야 하므로 확장성에 한계가 존재함

epoll의 등장

     * Linux 환경에서는 epoll이 도입되어 기존 방식보다 효율적인 다중 연결 처리가 가능해짐
     * epoll은 변경점(델타)만 처리하여 불필요한 리소스 소모를 줄임
     * 모든 시스템콜이 완전히 없어지지는 않지만, 비용이 상당히 줄어듦

io_uring 개요

     * io_uring은 각 요청마다 시스템콜을 호출하는 대신, 커널이 비동기적으로 처리할 수 있도록 요청을 메모리 상 큐에 추가함
     * 예를 들어, accept()를 큐에 넣어두면 커널이 처리 후 완료 큐에 결과를 반환함
     * 웹서버는 큐에 요청을 추가하고, 결과는 별도 메모리 영역에서 확인하는 방식으로 작동함
     * 바쁜 루프(busy loop)를 피하기 위해, 큐에 변화가 없으면 웹서버와 커널 모두 필요한 경우에만 시스템콜을 호출하여 절전 효과를 얻음
     * 적절한 라이브러리를 활용하면, 활성화된 서버는 요청 처리 중 별도의 시스템콜 없이 작동 가능함

멀티 코어와 NUMA 환경

     * 현대 CPU의 다중 코어 환경을 고려해 코어별 단일 스레드 실행 및 데이터 구조의 공유를 최소화하는 전략이 유효함
     * NUMA 환경에서는 각 스레드가 자신의 로컬 노드 메모리에만 접근하여 최적화
     * 요청 분배의 완벽한 균형은 추가 연구가 필요함

메모리 할당

     * 커널과 웹서버 모두에서 메모리 할당이 남아있으며, 사용자 공간에서의 할당도 결국 시스템콜로 연결됨
     * 웹서버 단에서는 연결당 고정 크기의 메모리 블록을 미리 할당하여 파편화 및 부족 문제를 예방함
     * 커널 측에서도 연결별로 입출력 버퍼가 필요하며, 소켓 옵션 등으로 일부 조정 가능함
     * 메모리 부족 현상 발생 시 심각한 장애로 이어질 수 있음

kTLS(커널 TLS) 소개

     * kTLS는 Linux 커널에서 암호화 및 복호화 연산을 담당하는 기능임
     * 핸드셰이크는 애플리케이션에서 처리하지만, 그 이후로는 커널이 순수 텍스트처럼 데이터 전송을 처리함
     * sendfile() 사용이 가능해져 유저-커널 공간 간 메모리 복사를 줄일 수 있음
     * 네트워크 카드가 지원할 경우, 암호화 연산까지 하드웨어에 오프로딩할 수 있는 이점이 있음

Descriptorless Files

     * 사용자 공간에서 커널 공간으로 파일디스크립터를 직접 전달할 때 발생하는 오버헤드를 줄이기 위해 등장한 방식임
     * register_files를 이용해 io_uring에만 유효한 별도의 '정수' 파일번호를 사용하며, /proc/pid/fd에는 표시되지 않음
     * 시스템의 ulimit 제한은 여전히 적용됨

tarweb 프로젝트 소개

     * tarweb은 위 모든 기술을 적용한 예시 웹서버 오픈소스 프로젝트임
     * 단일 tar 파일 내용을 제공하는 구조로, Rust, io_uring, kTLS 등 최신 고성능 기술이 결합되어 있음
     * 실사용 과정에서 io_uring과 kTLS의 호환성 문제(setsockopt 미지원 등)가 있어 Pull Request로 일부 이슈를 해결함
     * 프로젝트는 아직 미완성 단계이며, Rust의 rustls 라이브러리가 핸드셰이크 과정에서 메모리 할당을 수행할 수 있음
     * 핵심은 각 요청별 추가 시스템콜 없이 HTTPS 서비스가 가능하다는 점임

벤치마크 및 성능 측정

     * 저자는 아직 충분한 벤치마크를 진행하지 않았으며, 코드 정비 후 성능 테스트 예정임

io_uring과 Rust의 안전성 문제

     * 동기식 시스템콜과 달리, io_uring에서는 완료 이벤트 이전까지 메모리 버퍼가 해제되지 않아야 함
     * io-uring 크레이트는 Rust의 컴파일 타임 안전성을 보장하지 않으며, 런타임 체크도 부족함
     * 잘못 사용 시 C++과 유사하게 심각한 문제까지 이어질 수 있어, Rust 본연의 안전성이 약화됨
     * pinning과 borrow checker를 적극적으로 활용하는 별도의 safer-ring 크레이트가 필요함
     * 이 문제는 이미 커뮤니티에서 논의 중임

참고 및 추가 링크

     * 본 내용은 2025-08-22 기준 HackerNews에서 논의된 포스트임

        Hacker News 의견

     * io_uring을 사용해서 쓰기 작업을 제출할 때, 메모리 위치가 해제되거나 덮어쓰기 되지 않도록 해야 하는데, io-uring crate API는 Rust의 borrow checker가 이 부분에서 도움을 주지 않고 런타임 체크도 없는 것으로 보임
       이런 상황에 대해 쓴 글과 댓글들을 봤는데, 결과적으로 io_uring을 감싼 안전한 Rust 비동기 라이브러리 만들기가 정말 어렵다는 인상임
       tokio 팀의 Alice가 최근에는 이 문제를 극복하려는 관심이 크지 않다고 언급한 것도 기억남
       지금 성능이 ""충분히 좋음"" 상태이기 때문임
       참고: https://boats.gitlab.io/blog/post/io-uring/
          + Rust async에 대해 아쉬운 점이 많은데, 그중 하나가 이런 부분임
            Rust async는 epoll이 표준이던 시기에 설계됐고 IOCP에는 신경을 거의 안 썼음
            동기 syscall에는 이런 문제가 없는 이유는, read 호출 시 버퍼의 가변 참조를 커널에 넘기지만 네이티브 Rust의 ownership/borrow 모델과 잘 맞음
            그런데 completion-based IO는 소유권 모델에 제대로 맞추려면, 작업이 완료될 때까지 사용자 코드가 계속 실행되지 않음을 보장해야 하고, 이걸 state machine polling 구조로는 할 수 없음
            스레딩 모델이나 green thread 구조가 여기서는 딱 맞음
            만약 Rust가 ""async 전용 타겟""을 추가했다면 더 나았을 것 같음
            Rust 개발진이 비동기 stackless polling 모델에 많은 기대를 걸었는데, 그 결말을 지켜보고 있는 중임
          + Rust의 borrow checker가 제대로 지원하지 못하는 소유권 모델이 있다고 생각함
            임시로 “핫 포테이토 소유권”이라고 부르는데, 버퍼를 잠시 넘겨줬다가 다시 돌려받는 구조임
            Rust로 이런 패턴을 안전하게 코드로 짜려니 되게 어렵고 코드가 난잡해짐
          + tokio팀 Alice의 말과는 달리, 파일 IO 쪽에선 관심이 있음
            파일 IO는 이미 spawn_blocking 방식으로 구현해서 io_uring의 동일한 버퍼 이슈를 겪고 있고, io_uring으로 옮기는 건 그리 어렵지 않음
            하지만 tokio::net의 기존 API는 io_uring 기반 버퍼 API와 호환되지 않아서, readiness 체크는 할 수 있어도 완전한 지원은 어려움
          + 안전한 io_uring 인터페이스를 만들려면, 링에서 소유한 버퍼를 받아 쓰고, 쓰기를 시작할 때 다시 돌려주는 방식이 가장 적합하다고 생각함
          + 꼭 모든 것을 borrows로 표현할 필요 없음
            Slab 같은 데이터 구조를 사용하면 cancel safe하게 만들 수 있음
            참고: https://github.com/steelcake/io2
     * 이번 글 정말 재밌게 읽었음
       성능 테스트가 기대되지만, 작성자가 벤치마크보다 먼저 코드를 깔끔하게 정비하겠다고 한 점이 인상 깊었음
       벤치마크만 중시하는 이 시대에 이렇게 고민하는 사람이 있다는 게 신선함
       11살 무렵 데이터베이스를 구축하려는 시도에서 cgi-bin을 접했고, 그게 요청마다 새 프로세스를 띄우던 방식이었다는 걸 이제야 깨달음
       sendfile이 대형 게임 포럼에서 데모 다운로드를 동시에 처리할 때 게임체인저였고, Netflix의 40ms 감소 사례나 GTA 5의 70% 로딩 타임 단축 등 결과를 보며 더 임팩트 있는 엔지니어링이 숨겨져 있다고 느낌
       관련 링크: Common Gateway Interface, Netflix 40ms 사례, GTA Online 로딩 단축
          + CGI뿐 아니라 옛날 CERN, Apache 계열 HTTP 세션은 서버 전체를 포크해서 동작시켰음
            시간이 지나면서 나아졌지만, Apache의 구성 방식 때문에 nginx처럼 애초에 이벤트 기반 I/O로 설계된 경량 서버가 큰 인기를 얻게 됨
          + sendfile의 효율성에는 회의적임
            90년대 말에 유행하긴 했지만 실제로는 성능 이득이 미미하다고 생각함
     * 대부분의 클라우드 워크로드 오케스트레이터(CloudRun, GKE, EKS, 로컬 Docker 등)는 io_uring을 기본적으로 비활성화함
       이 부분이 개선되지 않으면 당분간은 io_uring이 매우 한정된 기술로 남을 것 같음
          + 왜 그들은 io_uring을 비활성화 하는 걸까라는 의문이 듬
          + 이런 상황이라면 다시 셀프 호스팅으로 돌아가야 함
     * 정말 재미있게 읽었음
       벤치마크를 기다릴 테니 천천히 해도 되고, 벤치마크보다 먼저 코드 정리를 중시하는 저자의 마인드가 너무 인상적임
       요즘은 벤치마크 점수에 올인하는 프로젝트가 많은데, 이런 사고방식이 정말 신선하고 존경스러움
       ktls나 io_uring이 이렇게까지 다양하게 쓰일 수 있는지는 몰랐음
     * 현 시점의 비동기 처리 상황은 아래와 같음
       Rust: Futures, Pin, Waker, async runtime, Send/Sync bounds, async 트레이트 오브젝트 등 다양한 개념 이해 필요
       C++20: coroutines
       Go: goroutines
       Java21+: 가상 스레드
          + C++ 코루틴은 Pin이 해결하는 문제를 피하려고 힙 할당을 사용함
            이건 C++가 추구하는 ""제로 오버헤드"" 원칙에서 크게 벗어난 부분임
            Rust가 미래에도 async 트레이트를 도입하는 데 시간이 오래 걸렸던 이유도 Rust는 futures를 힙 할당하지 않기 때문임
            퍼포먼스/이식성 대 복잡성의 트레이드오프가 각자 프로젝트에 따라 가치가 달라질 수 있음
          + Send/Sync 관련 제약은 다른 언어에도 여전히 의미가 있고, 해당 제약이 없으면 미묘하게 잘못된 코드를 더 쉽게 쓸 수 있음
          + ""충분히 괜찮은"" 수준의 Rust 코드를 쓴다면, 그리고 남이 만들어놓은 mid-level 프리미티브를 사용한다면, 굳이 저런 개념을 전부 알 필요는 없음
          + Rust는 저런 개념을 이해하지 않으면 아예 컴파일이 안되도록 강제함
            Go는 goroutine이 비동기가 아니고, 채널을 이해하지 않으면 goroutine을 이해할 수 없음
            Go의 채널 구현은 독특해서 경계 사례의 동작이 상식적으로 예측이 잘 안 됨
            Go는 깊이 이해 안 해도 코딩이 되니, 장단점이 있음
            ""저렴한 스레드""는 비동기와 동일하지 않음
            tarweb(블로그에 나온 서버)은 io_uring 기반 이벤트 루프의 싱글 스레드 구조로, CPU 코어당 하나씩 스레드를 두는 아이디어임
            ""대규모 동시성의 현주소""보단 ""저렴한 스레드의 현주소""가 더 맞는 말일 듯함
            cheap thread와 async loop의 가장 큰 차별점은 reasoning이 쉬운 것임
            단점도 있는데, 각 스레드는 가볍긴 해도 스택 크기를 필요로 함
     * kTLS가 확실히 발전임
       나도 몇 년 전에 진짜로 요청당 syscall이 0인 서버를 만들어서 블로그에 글로 남김 (https://wjwh.eu/posts/2021-10-01-no-syscall-server-iouring.html)
       다만 항상 busy-looping을 해야 한다는 단점이 있음
       io_uring은 최근 몇 년간 정말 인상적인 속도로 발전해왔음
     * 이 프로젝트가 정말 멋지고 오래 전부터 비슷한 걸 구상해왔기 때문에 누군가 구현해서 기쁨
       BPF도 rust로 작성하려면 Aya를 추천함
       Aya 프로젝트 Github
     * kTLS의 현재 상태가 궁금함
       얼마 전 Cilium 개발자에게 물어보니, Thomas Graf는 기대한다고 했지만 실제로는 많은 리눅스 배포판에서 커널 지원이 부족해 기본 활성화는 아직 멀었다고 함
          + 아쉽긴 한데, 활성화가 얼마나 어려운지도 궁금함
            커널을 커스텀해야 하는 건지, 아니면 런타임에서 바로 켤 수 있는 건지
            FreeBSD는 13버전부터 커널/openssl에 kTLS가 들어갔고, sysctl (kern.ipc.tls.enable=1)로 런타임 토글이 가능함
            FreeBSD-15에서는 기본값이 활성화로 바뀌고, Netflix에서는 거의 10년 동안 트래픽 암호화에 kTLS를 써옴
          + kTLS는 전반적으로 나쁜 아이디어처럼 느껴짐
     * 한 코어당 하나의 스레드 구조가 타임 슬라이스 기반 시스템에서 맞는지 의문임
       내 경험상으로는 ""오버서브스크라이빙"" 방식(코어 수보다 스레드를 더 두는 것)이 실제 벽 시계 시간 상의 이득을 줌
       프리엠티브 스케줄링이 없을 때나 한 코어당 하나가 더 맞을 듯함
       물론 그럼 Unix 얘기는 아님
          + 낮은 지연과 높은 처리량을 원한다면 코어를 격리해서 스레드를 고정시키는 방법이 효과적임
            이런 방식은 Linux에서 잘 작동하고, 트레이딩 시스템 등에서는 비효율을 감수하고서라도 많이 씀
            코어들이 대부분 유휴 상태로 spin하고 실제로는 일이 없지만, latency와 throughput에서는 최적임
          + thread-per-core 구조의 함정은 ""편리한 부분만 가져다 쓰자""라고 착각하는 거임
            사실상 올인하거나 안 쓰거나 둘 중 하나임
            반쪽짜리 구현은 전혀 효율이 나지 않음
            다만 올바르게 설계하면 거의 모든 상황에서 효율이 높음
            TPC 설계 노하우(코어 간 로드밸런싱 등)을 잘 아는 개발자는 드묾
          + thread-per-core에서 ""CPU 바운드""일 때만 효율적임
            이 서버 프로젝트처럼 대부분의 작업이 비동기적이고 이벤트 기반일 때, 서버는 거의 I/O나 syscall 대기 없이 다음 요청으로 이동해서 이론적으로는 코어당 하나의 스레드가 정확한 구조임
            하지만 현실 세계에선 이런 이상적 상황이 거의 없으니 무조건 nproc 스레드로 제한하는 건 위험하다는 점을 명심해야 함
          + io_uring에서는 한 코어당 사용자 스레드 하나만 두는 것도 나쁜 선택이 아니라고 보임
            커널에서 쓰레드 풀로 동작하기 때문임
     * DPDK와 같이 커널을 완전히 우회하는 스타일도 보고 싶음
          + 혹시 몰랐다면, LUNA가 이미 그걸 구현함
            논문 링크: https://www.usenix.org/system/files/atc23-zhu-lingjun.pdf
"
"https://news.hada.io/topic?id=22655","AWS CEO, "AI로 주니어 직원을 대체하는 건 내가 들어본 것 중 가장 멍청한 발상"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           AWS CEO, ""AI로 주니어 직원을 대체하는 건 내가 들어본 것 중 가장 멍청한 발상""

     * AWS CEO Matt Garman은 AI가 주니어 직원을 대체할 수 있다는 발상은 ""내가 들어본 것 중 가장 멍청한 말""이라고 발언함
     * 그는 주니어 직원이 가장 비용이 낮으면서도 AI 도구 활용에 적극적이라며, 인재 육성과 학습 기회를 제공하는 것이 필수적이라고 강조함
     * 또한 AI의 성과를 코드 작성량으로 측정하는 것은 무의미한 지표라며, 불필요하게 많은 코드보다 적고 품질 높은 코드가 중요하다고 지적함
     * AWS 내부에서는 이미 80% 이상의 개발자가 AI를 활용하고 있으며, 단위 테스트, 문서 작성, 코드 보조, 에이전트 기반 워크플로우 등 다양한 방식으로 적용되고 있음
     * Garman은 급격히 변하는 기술 환경에서 장기적으로 필요한 것은 비판적 사고, 창의성, 학습 능력이며, 이러한 역량을 가진 인재가 AI 시대에 성공할 것이라고 전망함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주니어 직원 대체 논란에 대한 입장

     * Garman은 일부 경영진이 AI로 모든 주니어 직원을 대체할 수 있다고 주장하는 것에 대해 강하게 반박
          + 주니어 직원은 “가장 비용이 적게 들면서도 AI 활용에 가장 적극적”이라는 점을 강조
          + “10년 뒤에 아무도 경험을 쌓지 못한다면 어떻게 되겠는가”라며 인재 육성의 필요성을 역설
     * 그는 여전히 대학 졸업생을 채용해 문제 해결 방법을 가르치고 훈련시키는 과정이 필수적이라고 주장

AI 활용 방식과 지표에 대한 비판

     * 코드 작성량을 기준으로 AI 성과를 측정하는 관행을 “쓸데없는 지표”라고 비판
          + 무한히 많은 코드를 생성할 수는 있지만, 그 코드가 나쁜 품질일 수 있음
          + “적은 코드가 더 나은 경우가 많다”며 양적 지표에 집착하는 태도를 지적
     * AWS 내부 데이터에 따르면 개발자 80% 이상이 이미 AI를 활용
          + 단위 테스트 자동화, 문서 작성 지원, 코드 일부 작성, 에이전트 기반 협업 등 다양한 방식으로 사용
          + 이러한 AI 도구 사용률은 매주 증가하는 추세임

AI 시대의 교육과 커리어에 대한 조언

     * Garman은 AI 시대에 필요한 역량으로 비판적 사고, 창의성, 학습 태도를 꼽음
          + 특정 기술 습득이 아닌 “학습하는 방법” 자체
          + “스스로 사고하는 방법, 문제를 분해해 해결하는 능력, 새로운 것을 배우려는 자세”가 핵심이라고 강조
     * 기술 발전이 너무 빠르기 때문에 특정 기술만 배우는 것은 30년 커리어를 지탱하기 어렵다고 지적
     * 따라서 교육자들은 학생들에게 문제를 분해하고 사고하는 능력, 새로운 것을 배우는 태도를 가르쳐야 하며, 이를 갖춘 인재가 AI 시대에 번영할 것이라고 전망

   저는 두가지 다 충분히 고민이 되어야 한다고 생각해요.

   회사 운영을 위해선 개발자가 필요하며 현재 주니어 개발자 취업이 어려운 시기라고 보입니다.
   공개적으론 AI 탓을 하고 있지만 코로나 시기때 대폭 채용한 것과 그에 비례한 성공보단 회사에선 전체적인 인건비가 상승했으니 그걸 부담으로 인해 채용을 줄이는 것있습니다. 그러한 상태에서 LLM을 이용한 것이 주니어 개발자에게 업무를 시키는 것 만큼 이상의 효율을 보여주는 상황이 오니 취업 시장 자체가 좀 더 줄였다고 보입니다.

   그러나 글에도 적혀있든 주니어 개발자가 있어야지만 그게 결국은 시니어 개발자로 성장할수 있습니다.
   주니어 개발자때 뽑지 않으면 시니어 개발자는 나올 수가 없는 구조이죠.

   그럼에도 불구하고 이러한 과정에서 상당한 조율이 필요하다고 봅니다.
   대기업의 경우에는 체계가 잡혀있어서 덜하겠지만 주니어 개발자가 오면 회사의 업무의 메인을 시키기보단 약간의 잡무(실패해도 될 회사 업무)를 시키며 교육을 합니다.

   그러나 시니어 개발자 입장에선 체계가 덜 잡힐수록 주니어 개발자에게 안내하는 것은 더 힘든 상황입니다.

   그리고 아이러니하게도 LLM을 이용할 때 관련된 지식이 더 있어야 유리하지, 초보 개발자라고 동일한 효율을 보이는 건 아닙니다.
   오히려 모든 개발 업무를 주니어 직원으로 대체는 불가능합니다. 똑똑한 천재인분들은 시니어 개발자가 없더라도 어떻게든 할 수 있겠죠. 그러면 그 분에게 일이 모이기 시작하면 그 분이 버틸 수 있을까요?

   즉, 시니어 개발자와 주니어 개발자 모두 뽑혀야 하며 그 과정에서 생산성과 회사의 인건비 등을 고려한 유연한 채용이 있어야 한다고 봅니다.

        Hacker News 의견

     * 완전히 동의함
       한편, LLM 코드 실제로 사용하려면 정말 프롬프트 마법사가 되어야 한다고 느껴짐
       나는 가끔 디버깅이나 UI를 빠르게 스케치할 때만 사용함
       실제 코드에 관해서는 LLM이 작성한 코드는 정말 스파게티 코드에 장황하고 성능과 보안 면에서 심각한 위험이 있으며, 내가 준 거의 모든 디자인 패턴에 대해 완전히 오해한 코드임
          + Hacker News와 Reddit에서 AI 코딩에 회의적인 글을 볼 때마다 점점 놀라워지는 중임
            우리 모두 완전히 다른 세계에 살고 있는 듯함
            도구의 다양성도 원인인 것 같음
            ""LLM 코드를 쓴다""는 게 사람마다 의미가 다르다고 생각함
            구체적으로 어떤 LLM을 사용하는지, 어떤 컨텍스트가 주어지는지, 어떤 IDE를 사용하는지가 결과에 영향을 많이 주는 것 같음
            나는 agentic coding이 뜨기 전까지 20만 줄의 B2B SaaS 코드를 직접 썼음
            Sonnet 4 Agent 모드에서 지금은 매일 짜는 코드의 20% 정도만 내가 쓰고 나머지 80%는 VS Code의 interactive Sonnet과 GitHub Copilot Agents가 작성함
            Markdown으로 문서화를 많이 하면 할수록 그 비중이 높아짐
            결과물을 꼼꼼히 리뷰하고 테스트함
          + 어떤 툴을 쓰는지 궁금함
            나는 aider를 쓰고 있는데, 심지어 gpt-5처럼 코딩에 약하다고 소문난 모델을 써도, 네가 말하는 그런 경험이 전혀 없었음
            실제로 ""좋은"" 코드를 작성하고, 기존 코드 스타일도 잘 맞춰줌
            프롬프트 작성이 정말 중요하고, 기존 코드베이스에서는 구체적으로 구현 힌트를 줄 수 있을 때 성공률이 확연히 올라감
            이는 코드베이스를 잘 아는 시니어가 쉽게 할 수 있지만 주니어에겐 어려울 수 있는 부분임
            모든 면을 명확하게 봐야 한다고 생각함
            아직까지는 aider로 돌리는 것보다 내가 직접 하는 게 미세하게 더 빠른 경우가 많지만, 차이가 크지 않고 계속 나아지는 중임
            LLM이 주니어 개발자가 할 수 있는 몇몇 일은 대체할 수 있지만, 완전히 대체할 수 없음
            주니어는 회의도 가고, 논의도 주도하고, 결국 시니어가 되는 성장 경로도 있기 때문임
            하지만 경영진 입장에선 이런 사실에 관심 없을 수 있음
          + AI는 대량의 정보를 흐릿하게 조회하는 데 환상적인 도구임
            요즘 Kagi의 Assistant를 일반 검색보다 먼저 쓰는 일이 자꾸 늘어남
            내가 부족한 단어를 알게 해주고, 그 단어로 페이지를 뒤지면 결국 원하는 걸 찾게 됨
            하지만 vibe coding에서 그다지 지속적으로 가치를 얻었던 적은 없음
            원오프 업무에서는 훌륭함
            예를 들어 matplotlib 차트를 만들 때, 원하는 내용을 말해주고 데이터 스키마만 보여주면 90%는 잘 맞춤
            쉘 스크립트도 간단하게 만들어줌
            최근에는 RAW 사진을 EXIF 정보로 폴더 정리하는 작은 CLI 도구를 시켜봤는데, 이런 류엔 아주 만족스러움
            하지만 조금만 더 복잡한 걸 시키면 쓸모없는 일을 많이 함
            이미 프로젝트에 있는 모델을 중복 생성하거나, 관련 없는 변경을 하거나, 없는 API 함수를 헛소리로 만들어냄
            결과를 검증할 바엔 내가 직접 쓰는 게 나음
            그리고 나에겐 직접 코딩하는 과정이 가장 즐거운 부분임
            LLM은 인간이 프롬프트를 통해 일시적으로 결과를 얻고 바로 저장, 통합, 전달하는 실 사용 과정에는 아직 적합한 예시를 못 찾음
          + AI가 수백 개에 광고 범벅인 엉망진창 사이트에서 내가 찾는 답을 빠르게 걸러주는 데에는 매우 유용함
            Duck Duck Go AI를 질문 응답 용도로 자주 쓰고 있음
            데이터센터를 던질 수 있는 거리만큼만 신뢰하지만, 빠르게 검증 가능한 정보, 예를 들어 프로그램의 문법이나 커맨드 옵션처럼 명확한 내용엔 유용함
          + AI 활용에선 '넣는 만큼 얻는다'는 말이 딱임
            많은 시간을 들여 내부 동작, 엣지 케이스, 아키텍처, 라이브러리 선정 등을 설명하고 꼼꼼히 Markdown에 작성하면 몇 번만 반복해도 쓸 만한 코드가 나올 가능성이 높음
            ""X 기능 만들어줘"" 같은 짧은 프롬프트와 큰 차이가 발생함
            그런데 이렇게 좋은 프롬프트를 쓸 정도면 사실상 문제를 거의 다 푼 것이고, LLM은 그저 빠른 자동 타이핑기 역할임
            타이핑만 빨라질 뿐 사고의 대부분은 인간이 이미 했음
     * 적어도 한명의 CEO는 이 부분을 이해하고 있다고 생각함
       주니어 인력을 건너뛰고 AI로만 채우려는 생각은 장기적으로 기업에 악영향임
       시니어 인력이 독립해서 나가면 남는 게 없음
       솔직히 AI가 어떤 엔지니어에게도, 주니어도 포함해, 정말로 이로운지 잘 모르겠음
       소프트웨어 엔지니어링은 탐구와 학습의 여정임
       AI를 쓸 때마다 수학 선생님이 ""계산기 쓰면 머리에 남는 게 없다""던 말이 떠오름
       전체적으로 AI가 지난 45년간의 미국 경제 정책의 자연스러운 결과라는 기분도 듦
       오로지 1%만을 위한 단기 성과 추구이고, 건강한 기업생태계나 경제의 장기적 발전을 해치는 방식임
       이런 걸 보면 잭 웰치가 매우 자랑스러워할 듯한 상황임
          +

     “시니어가 나가면 어떻게 될까?”
     CEO는 actually 시니어가 나가는 걸 걱정하는 사람이 아님
     오히려 ""주니어를 남겨라""를 외치지만, 그 내포는 ""시니어를 내보내라""며, 기존 업계 트렌드와 일치함
     OP 인용문에서 ""[주니어 대체]라는 발상은 ‘내가 들어본 것 중 가장 멍청한 것’이라는 말과 함께, 주니어가 아마 가장 저렴한 직원이자 AI 툴 사용에 가장 적극적이라는 이유가 언급됨""
     결국 능력과 실력을 위협, 위험요소로 본다는 것임
     업계 전체가 계속 '덤벙거림'을 유지하며 지적 경쟁력이 무너지는 속도를 최대한 빠르게 하라는 신호임
          +

     ""CEO가 적어도 한 명은 제대로 이해하고 있다""<br>> ""AI는 모든 엔지니어에게 이득이 아닐 수 있다""
     CEO 인터뷰를 들어보면 이 CEO는 오히려 코딩용 LLM 도입에 올인한 사람임
     AWS 엔지니어 80%가 이미 LLM을 쓰고 있고 앞으로도 늘어날 거라 자랑스럽게 말함
     10분 정도만 인터뷰를 들어보길 추천함
          + AI는 전반적으로 내 학습 과정에 도움을 줬다고 생각함
            수년간 만들고 싶었던 개인 프로젝트가 항상 시작 장벽이 너무 높아서 포기했음
            AI 덕분에 반복적이고 지루한 일을 쉽게 처리할 수 있으니 실제 프로젝트를 완성까지 밀어붙일 수 있었음
            AI 없이 처음부터 끝까지 혼자 밀어붙였다면 더 많이 배웠을지 모르겠지만, 아예 시작도 못 했다면 얻은 게 아무것도 없었음
            지금은 실제로 배우는 양이 확실히 늘었음
          + 시니어가 남아도 변화나 도입에 관심 없거나 굼뜨면 그 역시 리스크임
            AI 도입으로 주니어의 배움과 성장 속도가 엄청나게 빨라질 것이라고 생각함
     * 최근 몇 달 동안 스타트업들과 일해보니, LLM vibe coding에 깊이 빠져 더이상 빠져나올 수 없는 상황에 빠진 경우를 많이 봤음
       제대로 인재를 뽑지 못했거나 기술 인력을 놓친 경우가 많았음
       그들은 AI 코드, 특히 Claude 코드를 내부 10x 엔지니어로 착각하고 더 빠른 반복과 더 좋은 코드를 기대함
       상당히 현명한 창업자들이 직접 Claude 코드가 마치 수 주 혹은 수 년 분량의 소프트웨어 엔지니어링 작업을 한 것처럼 느껴지는 도파민에 중독되는 모습을 목격함
       AI가 복잡한 문제를 ‘생각’하거나 ‘이해’할 수 있다고 믿는 건 너무 후하게 평가하는 것임
       우리는 실제 사고력보다 ‘타자 속도의 절감’을 측정해야 한다고 봄
       [1] vibebusters.com
     * ""생각하는 법""과 ""문제 분해 방법""을 가르치라는 것에 완전히 동의함
       엔지니어링 학교 최고의 교수님은 항상 오픈북 시험을 내셨음
       현실에서는 모두가 모든 데이터와 정보를 볼 수 있는 환경임
       그저 데이터를 찾는 일에 사람을 돈 주고 쓰지 않고, 데이터를 분석하고, 이해하고, 논리적으로 응용하는 능력에 돈을 줌
       이런 것을 바로 엔지니어링이라 부르며, 그 교수님은 바로 이걸 가르쳤음
          + 대학 시절 추상대수학 강의를 들었음
            모든 시험 문제는 유명한 증명을 외워서 쓰기, 그리고 새로운 증명을 만들어내기였음
            외우기 자체가 억지로 느껴졌지만, 증명을 이해하지 않고는 외울 수 없다는 걸 깨달았음
            직접 새로운 증명을 만들 땐 이미 머리속 모듈이 있어서, 훨씬 직관적으로 접근이 가능해짐
            진정한 암기란 알고리즘 문제 풀이 스타일의 코드 외우기와는 다르며, 실제 애플리케이션 코딩은 훨씬 더 인간 중심의 즉흥, 상태 기반 즉석 그래프 탐색에 가깝다고 생각함
            현실적 문제는 늘 새 순서가 있는 게 아님, 결국 휴리스틱이 핵심임
          + 이 분야 채용이 직면한 핵심 문제라고 생각함
            정말 유능한 개발자는 본질적으로 제너럴리스트임
            전문성은 분명 가치가 있지만, 구식 레거시 코드 지옥이나 한계 돌파 같은 상황이 아니면, 전문가가 반드시 필수는 아님
            오히려 생소한 스택을 다뤄본 사람이 약점을 메워주거나 신선한 시각을 줌
            능력 있는 일반 개발자라면 어떤 스택도 빠르게 적응함
            회사마다 사용 기술이 다 엉망진창이기 때문임
            ""15년 React 경력"" 같은 조건 붙여봤자, 누가 오더라도 바로 만렙 생산성은 불가능함
            무조건 온보딩 시간이 필요함
            하지만 현업 채용 담당자들은 이런 걸 잘 모름
            대기업이 그나마 트레이닝을 해주지만 요즘은 그런 분위기도 예전 같지 않음
            채용경쟁 때문에 수십만 달러를 쓰면서도, 실제로 누군가를 채워 키우는 비용은 크게 생각 안 함
            전체 업계적으로도 전문직협회가 있어야 아예 채용과 인재 육성 구조가 엉망이 되는 걸 막을 텐데, 그런 게 없어서 더 문제임
            (요즘 구조조정, 외주화 등으로 노조가 주목받는 것도 같은 맥락이라고 생각함)
          + 이미 그런 변화가 일어나고 있지 않나 싶음
            전통적인 CS 커리큘럼 절반은 수학, 나머지 절반은 이름만 다를 뿐 사실상 수학임
            학계에 대한 비판이 많긴 하지만, 누가 ""학계는 멍청하다, 이런 걸 가르쳤으면 좋겠다""라고 하면, 그걸 이미 하고 있거나 혹은 신속하게 필요한 만큼만 습득하면 되는 내용임
            새로운 트렌드는 대부분 이미 하고 있는 것임
          + 대학 시절 철학과가 ""생각하는 전공, 생각을 전공하세요""라는 마케팅 슬로건이 있었음
            채용 담당자로서 경험상 인문학을 공부한 사람들이 분석, 이해 같은 본질적 과제에 훨씬 강함
            나도 CS/철학 복수 전공이라 편견이 있긴 하지만, 정말로 코드만 많이 쓸 줄 아는 사람보다 분석적 사고력을 갖춘 주니어가 훨씬 귀함
            분석적 사고는 코딩보다 훨씬 가르치기 어려움
          + 컴퓨터공학 엘리트가 비즈니스나 사용자 관점에서 문제를 분해하지 않고 바로 코딩하려는 걸 “crazy finger syndrome”이라고 부르던 교수님이 첫 학기에 계셨음
            그 교수님의 '코딩만 하고 싶은 불안한 학생' 관련 농담이 그리움
            최근 부트캠프들은 높은 윤리 기준과 항상 맞닿아 있지 않다고 생각함
     * “미래에 아무도 제대로 배운 사람이 없으면 어떻게 되는가?”라는 질문을 들음
       이 결론은 이미 많은 사람들이 당연하게 받아들이고 있을 것임
       그런데도 대부분의 기업이 장기적 지속 가능성보다 단기 수익성에 집중하는 구조에서 벗어나기는 쉽지 않다고 봄
       그나마 인턴십/코옵이 인재 파이프라인을 유지하는 대책으로 계속 강조되고 있음
       앞으로 주니어 개발자 채용의 어려움을 피해 더 강하게 인턴십에 집중하는 트렌드도 예상함
     * 내 경험을 요약하면 이런 느낌임
       우리 사장이 ""AI 도입 때문에 사람을 대거 내보낼 것""이라는 선언적 PR을 하면서 AI 리더인 척했지만, 실제 해보니 완전 엉망이었고, 지금은 내가 나서서 사과와 해명을 하고 있는 중임
          + 사장 -> VP: ""AI 때문에 사람 줄여야 함""<br>VP -> 대중: ""2년 안에 모든 엔지니어를 AI로 대체하겠다""<br>사장 -> VP: ""VP도 AI 때문에 줄여야 함""<br>VP -> 대중: ""사람을 AI로 대체하는 건 멍청한 짓임""
          + 여전히 주니어 개발자는 채용하지 않고 있음
     * AWS CEO도 입장을 바꾼 듯함
       1년 전엔 ""AI가 2년 안에 전부 코딩하게 될 것""이라는 말을 했었음 [1]
       드디어 c-suite가 현실을 받아들이는 듯함
       [1] https://news.ycombinator.com/item?id=41462545
          + CEO가 실제로 그런 말을 했던 건 아님
            그는, 2년 내에 개발자가 코드를 거의 작성하지 않게 될 수 있다고만 말함
            그리고 이어서 ""이제는 무엇을 어떻게 만들고, 실제 고객에게 필요한 것이 무엇인지에 더 집중해야 한다""
            기사 링크
            전제부터 지금 발언까지 일관된 맥락임
            ""코드 쓰기"" 자체는 덜 중요해질 수 있고, 그래서 주니어를 채용해서 배움의 방법을 가르치고, 실질적으로 쓸모 있는 역량을 키워야 한다고 함
          + 이론적으로 Amazon 기업가치 대부분은 인재 역량임
            일부는 인력을 비용으로만 보고 모든 가치는 주주 몫이라 주장함
            그런데 실제로 인적 자산이 가치가 있다면, AI만으로 누구나 그 가치를 얻을 수 있다고 주장하는 건 오히려 주가에 악영향임
            PE(주가수익비율) 하락 위험까지 있는데, 이걸 긍정적으로 해석하는 게 이상함
            만약 정말 AI만 있으면 뭐든 할 수 있다고 믿으면, 주주 입장에선 자금이 FAANG처럼 안정적으로 묶여있지 못하고 계속 새로운 '신성장 아이템'을 찾아야 하는 부담만 커짐
          + 임원진이라면 항상 시대 흐름을 감지하는 것이 필수임
          + 전혀 모순적이지 않은 발언임
            자율성 있는 AI에게 지시하려면, 시니어 뿐 아니라 주니어부터 키우는 인재 파이프라인이 반드시 필요함
            대기업은 이 파이프라인을 걱정하고, 작은 회사들은 그걸 받아 단기적으로 시니어만 채용하면서 인턴들은 안 뽑을 수도 있음
          + 두 발언 사이에 논리적 모순 없음
            주니어를 계속 뽑으면서 그들의 일이 실질 코딩과는 달라질 수 있음
     * 사장과 입장이 다른 것 같으면 직접 확인해보라고 권하고 싶음
       뉴스 기사를 그냥 맥락 없이 인용하는 건 바람직하지 않음
       누구도 실제로 미래를 예측할 수 없기 때문임
       [1]: https://shrm.org/topics-tools/news/…
          + 두 CEO의 발언이 서로 충돌한다고 생각하지 않음
            ""대학 졸업생을 계속 채용해서 소프트웨어를 만드는 올바른 방법을 가르쳐야 한다"" - Matt Garman
            ""오늘 하는 여러 일에 사람이 덜 필요해질 것"" - Andy Jassy
            뉘앙스에서만 다를 뿐 본질은 비슷함
          + 인용할 땐 반드시 원문을 맥락과 함께 최대한 동등하게 인용하는 게 윤리적이라고 생각함
            누굴 인용 대상으로 고를지, 어떤 맥락을 만들지에 따라 뉴스의 논조가 결정됨
          + 두 발언이 논리적으로 매우 일관적임
          + AWS에서 나와본 경험자로서, AWS 공식 발언은 전적으로 신뢰하지 않음
            원래 AWS가 어떤 회사란 것도 알았고, 46세에 8번째 직장으로 갔었음
            ""영구 원격""이라고 한 직무도, 내가 이미 퇴사한 후 재택근무 복귀 명령(RTO)이 떨어진 사례도 있었음
     * 학계 연구 인재 파이프라인은 다음과 같음
       학부생 -> 대학원생 -> 박사후연구원 -> 테뉴어/시니어
       아주 일부 예외를 빼면, 처음 두 단계를 건너뛰고 시니어 연구원이 되는 일은 없음
       어느 업계든 마찬가지임
       주니어가 없으면 시니어가 나올 수 없으므로, '봇'이 모든 걸 하기를 바란다면 그 리스크도 준비해야 함
     * 이 모델들과 오랫동안 일해본 사람이라면 다들 동의할 거라고 확신함
       o3 릴리즈 전 sama AGI 포스트와 그 당시 테크계의 doomer 포스팅들은, 돌이켜보면 정말 어이없었음
          + AGI doomerism은 마케팅 전략에 불과했음
            지금은 모두가 AI의 본질을 이해하고 있고, 이제는 AI가 문서를 다 읽어주는 새로운 서치 시장의 반복을 구경하는 상황임
          + 원래부터 바보 같은 노이즈였지만, 그 누구도 ‘하이프’에 자유롭진 않음
            특히 엄청난 돈이 기술 실체 이상으로 부풀리기(astroturfing)에 투입됐기에 더더욱 그렇다고 생각함
          + ChatGPT가 내가 일해본 어떤 주니어 개발자보다 낫다고 봄
            주니어는 1년 가까이 팀에 마이너스임
            실제 프로젝트를 책임지는 사람 입장에선 ""주니어 좀 더 있었으면 좋겠다""고 생각해 본 적이 없음
            오히려 20% 더 쳐주고 중견급을 스카웃하는 게 훨씬 나음

   이 글을 부정하는 사람은,
   본인 수준이 낮아서 수준 낮은 주니어들이랑만 일해본 시니어들 뿐이다 ㅋㅋ
   경력 상관없이 AI시대에는 머리 좋은 사람이 압도적으로 유리한 세상임.
   머리 좋은 신입이 1~2년만 바짝해도 무난한 10년차 털어먹는다

   AI 없어도 머리 좋은 신입은 1~2년만 바짝해도 무난한 10년차는 털어먹었죠...

   뭔가 “주니어는 싸고 ai 잘쓰는데 왜 대체하지? 시니어를 대체시키자!” 라고 하는거 같은 느낌이 드네요

   오 그렇게 이해할 수도 있겠네요

   지랄ㅋㅋ

   이런 식으로 댓글 다는 건 자제해 주세요. 여기는 디시인사이드가 아닙니다.

   말꼬라지

   어휴...

   여기는 디씨가 아니에요..
"
"https://news.hada.io/topic?id=22663","북한 해커그룹 '김수키'가 한 것으로 추정되는 해킹에 대한 분석 보고회 - 고려대 정보보호대학원","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         북한 해커그룹 '김수키'가 한 것으로 추정되는 해킹에 대한 분석 보고회 - 고려대 정보보호대학원

     * 고려대학교 정보보호대학원은 2025년 8월 22일 최근 프랙 매거진(Phrack Magazine)을 통해 자료가 공개된 북한 국가 지원 해커그룹 '김수키'가 한 것으로 추정되는 해킹에 대한 분석 보고회를 개최
     * 공개된 자료는 ‘김수키’ 소속 북한 해커로 추정되는 해커의 작업용 컴퓨터에서 빼낸 파일·데이터
     * 분석 보고회에서는 해당 자료에 대한 상세 분석 내용과 고려대학교 정보보호대학원 연구진이 추가적으로 발견한 사항을 발표
     * 고려대학교 연구진이 작성한 'APT-Down Revisited: 국가 지원 해킹그룹 해킹 자료 분석 및 시사점' 보고서는 '피해자 중심 분석', '공격자 식별 중심 분석'으로 나뉘고, 새롭게 발견된 내용을 포함
     * 해커의 작업용 컴퓨터에서 빼낸 데이터는 쉽게 접하기 어려운 자료이고, 해커그룹의 공격 흔적, 사용 코드와 도구 등 실제 공격 수법과 행위가 그대로 담겨 있는 교재라는 코멘트
     * 기사 원문: https://news.tf.co.kr/read/economy/2236530.htm
     * 고려대학교 연구진의 보고서: https://bit.ly/kusecurity
"
"https://news.hada.io/topic?id=22640","채팅을 넘어: AI 디자인 패턴의 미래 (Youtube)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    채팅을 넘어: AI 디자인 패턴의 미래 (Youtube)

     * 최근의 AI 인터페이스는 텍스트 박스 기반 상호작용에 지나치게 의존하며, 사용자가 의도를 명확히 표현해야 하는 높은 상호작용 비용을 초래
     * Vitaly Friedman은 버튼, 슬라이더, 체크박스 등 기존 UI 컨트롤을 활용해 사용자가 더 쉽게 맥락을 제공할 수 있도록 해야 한다고 강조
     * Consensus, Elicit 같은 사례는 필터, 정렬, 직접 소스 링크 같은 전통적 기능을 AI 경험에 도입해 신뢰성과 효율성을 강화
     * 사용자는 출력 결과를 편집·정리하는 데 많은 시간을 쓰므로, 출력 레벨에서 직접 수정·재구성할 수 있는 기능이 필요
     * 궁극적으로 중요한 것은 ‘AI 우선’이 아니라 인간 중심 경험이며, AI는 이를 보완하는 조용한 도구(Quiet AI) 로 작동해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 디자인의 현재 상태

     * 초기 AI 사용 경험은 마치 마법 상자와 같았지만, 실제로는 단순한 텍스트 박스에 불과하며 사용자가 의도를 정확히 전달하기 어렵다는 문제가 있음
     * 사용자들은 대기 시간, 반복 요청, 망각, 오류 때문에 불만을 가지며, 왜 인간이 AI 언어를 배워야 하냐는 질문 제기
     * 단순 텍스트 입력 대신 버튼·라디오 버튼·체크박스 등 전통적 인터페이스 요소로 상호작용을 보완할 필요성

유용한 AI 인터페이스 패턴 사례

     * Perplexity는 AI가 응답을 준비하는 동안 사용자에게 추가 맥락을 입력하도록 유도해 효율성을 높임
     * Task Builder 패턴: 사용자가 ‘Slack 요약 후 Word 문서 생성’ 같은 자주 쓰이는 작업을 클릭 기반으로 정의
     * Consensus는 필터, 출처 신뢰도 색상 표시, 결과 분포 그래프 등을 제공하여 단순 답변 이상의 검증 가능한 맥락을 제시
     * Elicit은 논문 인용의 특정 구간으로 직접 연결하는 기능을 제공해 신뢰성과 생산성을 강화

전통적 UI 요소의 재발견

     * 필터링, 정렬, 포맷 선택 같은 구식 UI 컨트롤이 오히려 AI 경험을 크게 개선
     * 출력물 편집 시 직접 텍스트 일부를 제거·확장하거나 표·목록 변환 버튼을 제공하면 효율 증가
     * 로딩 중 대기 시간을 활용해 추가 질문이나 테마 선택을 받는 것도 좋은 접근

신뢰를 위한 디자인

     * 신뢰 확보를 위해서는 출처·스코프 공개가 중요하며, 단순히 답을 제시하는 것이 아니라 맥락을 함께 제공해야 함
     * 사용자의 개인화 메모리를 반영하고, 그 근거를 시각적으로 피드백해 이해도를 높일 필요
     * 사용자가 결과를 세밀하게 체크박스 단위로 수정·제외할 수 있는 구조적 편집 경험 필요

조용한 AI(Quiet AI) vs. 눈에 띄는 AI

     * Quiet AI: DoveTail 같은 도구처럼 기존 사용자 흐름 속에서 부드럽게 기능 강화
     * Loud AI: 반대로 스파클 아이콘과 함께 과장되게 노출되는 경우 신뢰 저하 위험
     * 사용자는 AI 기능 자체보다 잘 작동하는 기능을 원하며, AI라는 꼬리표는 때때로 부정적 효과를 초래

미래 전망과 인간의 역할

     * 프롬프트 엔지니어링은 장기적으로 사라지고, UI에 자연스럽게 통합된 맥락 제공 방식이 표준이 될 가능성
     * AI는 점차 자동완성처럼 곳곳에 숨어드는 보조 기술이 될 전망
     * 디자이너와 인간은 단순 실행이 아니라 전략적 조율자와 경험 설계자로서의 역할이 강화될 것
     * 진정한 목표는 인간 중심 경험이며, AI는 이를 보완하는 조력자로 작동해야 함
"
"https://news.hada.io/topic?id=22661","DeepSeek-v3.1 릴리즈","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           DeepSeek-v3.1 릴리즈

     * DeepSeek-V3.1은 차세대 에이전트 시대를 위한 첫 번째 단계임
     * 한 개의 모델에서 Think(추론 기반) 과 Non-Think(비추론 기반) 의 두 가지 모드를 선택적으로 사용할 수 있는 하이브리드 추론 기능을 탑재함
     * DeepSeek-V3.1-Think 모드에서는 이전 모델 DeepSeek-R1-0528 대비 더 짧은 시간 내에 정답 도출이 가능해 효율성이 크게 향상됨
     * 사후 학습(Post-training) 을 통해 도구 활용, 외부 시스템 조작, 다단계 에이전트 과업 등에서 모델의 역할 수행 능력이 대폭 개선됨
     * 사용자는 DeepSeek 챗봇 서비스에서 “DeepThink” 버튼을 통해 Think/Non-Think 모드 전환을 자유롭게 할 수 있음
     * API 업데이트
          + SWE(Software Engineering) 및 Terminal-Bench 평가에서 더 우수한 결과를 달성함
          + 복잡한 검색 또는 멀티스텝 과업에서 다단계 추론 및 문제 해결력이 크게 강화됨
          + 전반적인 추론 효율성이 큰 폭으로 증대됨
     * 요금제 변경 (9/25 부터 적용)
          + 입력 API : 1M 토큰당 $0.07(캐시 히트) / $0.56 (캐시 미스)
          + 출력 API : 1M 토큰당 $1.68

        Hacker News 의견

     * 로컬에서 실행할 때 GGUF 모델을 만들어 놓았음, 동적 2bit 방식(2bit MoE, 나머지는 6-8bit)으로 좋은 성능 내려면 RAM과 VRAM 합쳐 약 250GB 필요함, SSD 오프로딩도 가능한데 느림, 실행법과 최적 파라미터 등 자세한 내용은 공식 문서 참고 바람
          + 그런데 unsloth가 파이썬 라이브러리이면서 apt-get을 sudo로 실행하려고 하는 점이 의아함, 내 nixos에서는 이게 실패해서 사용하기가 어려움
          + 이런 동적 2bit 압축에서 원본 모델 대비 얼마나 성능이 떨어지는지에 대한 벤치마크 결과가 궁금함
     * 참고로 terminal-bench 리더보드를 공유함, GPT-5, Claude 4, GLM-4.5와는 차이가 크지만, 다른 오픈웨이트 모델과는 비교적 준수한 성능임, 벤치마크가 전부를 말해주진 않으니 실제 결과는 시간이 지나봐야 알 수 있음
          + 해당 벤치마크는 agent tool과 모델을 뒤섞어 결과가 일관성이 부족하다고 봄, agent tool만 고정해서 모델만 비교해야 의미 있다고 생각함, 이런 류의 벤치마크는 신뢰성이 떨어지는 편이고 직접 모델을 사용해 자신의 문제에 적용해보는 것이 나은 방법이라 생각함
          + 내 체감상 결과물의 품질이 꽤 좋았음
          + Anthropic, OpenAI 같은 회사들도 특정 벤치마크를 위해 커스텀 에이전트를 개발하는 경향이 있음
          + DeepSeek R1은 이미 교체된 구 모델임을 알림, 업데이트 사항 파악함
          + 가격이 너무 비싸진 않아서 SOTA 모델이어도 부담스럽지 않아야 관심이 생김
     * 이전 비수기 할인이 사라진 점이 아쉬움, 그때는 토큰을 엄청나게 뽑으면서도 비용이 거의 들지 않았음, 그래도 여전히 가격 경쟁력이 아주 좋다는 점에서 크게 불만은 없음
     * artificialanalysis.ai의 벤치 결과에 따르면 대략 gpt-oss-120B와 비슷한 지능인데 약 10배 느리고 3배 비쌈
          + 제시된 소스는 현재 특정 프로바이더 한 군데만 보여주고 있음, 똑같은 공급자로 gpt-oss-120B와 deepseek-chat-v3.1을 비교하는 게 더 정확하겠음, gpt-oss-120B는 이미 구축 및 최적화된 공급자가 더 많아서 유리한 점을 감안할 필요 있음
     * DeepSeek V3.1은 하이브리드 리즈닝 모델이며, 툴 호출(Task Tool Calling)에 강점 있음, 하지만 표준 JSON 형식 대신 옛날 툴 포맷을 랜덤하게 사용하는 현상이 자주 나타남, 아마 V3 학습셋이 그런 자료를 많이 포함한 듯함
          + strict(베타) 펑션콜을 써봤는지 궁금함, 관련 가이드 있음
          + 어떤 포맷을 의미하는건지 궁금함, json이 LLM이 구조적 출력을 강제하는 데 적합하다고 알았는데 왜 굳이 json을 벗어나는지 의문임
     * Qwen3 235B 2507 Reasoning(내가 좋아하는 모델)이나 gpt-oss-120B보다는 뒤쳐지는 듯함, 벤치마크 링크 참고, 가격 참고
          + Qwen3 2507 계열 모델이 현 시점에서 로컬 최고라고 생각함, GPU와 대략 32GB 램만 있으면 A3B 모델로 페어 프로그래밍 작업에 아주 적합함
     * 최근 6개월간 사용해본 모델 중 DeepSeek V3.1이 가장 환각(hallucination)이 많이 발생함
          + 어떤 context length를 썼는지 궁금함
          + 이번에 안 좋은 데이터를 가져왔을 가능성 물어봄
     * V3와 Qwen3 Coder의 중간 정도 위치임, 비교 링크
          + gpt-5 Mini 모델의 무료 제공 여부 묻는 중임
     * 오픈웨이트 모델 사이에서는 경쟁력 있어 보이나 GPT-5나 Claude에 비하면 아직 격차가 큼
     * GLM-4.5보다 agentic 코딩 태스크에서 더 뛰어나다는 증거는 아직 못 봄
          + 그게 전부인지, 혹시 다른 점에서 못 본 근거가 있는지 되묻는 중임
"
"https://news.hada.io/topic?id=22682","Show GN: gh-domino: Rebase your stacked Pull Requests","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show GN: gh-domino: Rebase your stacked Pull Requests

     * GitHub 에서 stacked PR을 자동으로 rebase 해주는 GitHub CLI extension 도구
     * Graphite 나 stacked-pr 같은 멋진 도구들이 있지만, 이들의 진입장벽은 너무 높음
          + Stacked PR을 git이 아닌 해당 도구를 이용해서만 올려야한다던가
          + 로컬 구성 파일의 옵션들이 아주 많음
     * Zero-configuration 과, GitHub의 PR 기능만으로 충분한 아주 간단한 rebase 도구가 필요하여 직접 개발
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    예시

   다음과 같은 Stacked PR이 존재하는 상황:
     * #102 (main <- stack-1)
     * #103 (stack-1 <- stack-2)
     * #104 (stack-2 <- stack-3)

   PR #102 를 main 에 머지한 이후, #103과 #104 PR을 rebase 해주어야함.
     * 단 두 개의 PR만 rebase 하는 작업도 상당히 귀찮은 작업이며,
     * 실제로 #102 에 추가적인 커밋이 들어가는 등의 이유로 인해 graph가 복잡해진 경우, 공통 부모를 찾는 git 명령어를 기억 해내야하는건 매우 골치아픈 일임
     * 또한 stacked PR은 자주 사용되는 상황이 아니라, 종종 필요한 경우에만 사용되기에 더더욱 잊기 쉬움

   다음의 명령어를 통해 간단히 rebase를 자동으로 수행 가능:
gh domino --dry-run
gh domino [--auto]

   더 자세한 예제는 README.md 에서 확인 가능합니다.

   엇 github에서 자동으로 해주는줄 알았는데 아닌가요?!

   GitHub에서 제공하는 safe delete branch 기능을 이용하면 base branch는 자동으로 바꿔주지만, squash merge의 경우 git history 연관이 앖어 conflict 상태가 됩니다!

   또한 safe delete를 사용하지 않고 git으로 브랜치를 직접 삭제하는 경우도 base branch를 바꿔야하는 귀찮음이 있고요!
"
"https://news.hada.io/topic?id=22721","영국 웹사이트 차단 확대 정당화를 위한 완벽한 ‘Pirate Bay’ 포스터 차일드로서의 4chan?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        영국 웹사이트 차단 확대 정당화를 위한 완벽한 ‘Pirate Bay’ 포스터 차일드로서의 4chan?

     * 영국의 온라인 안전법(Online Safety Act) 은 아동 보호를 명분으로, 성인까지 포함해 강력한 본인 인증 및 콘텐츠 검열 조항을 도입함
     * Ofcom(영국 방송통신규제기관) 이 지정한 의무 불이행 사이트에 대한 글로벌 차단 및 처벌 방안 추진으로, 실제로 미국 기반 플랫폼까지 영향 미침
     * 대표적인 사례로 The Pirate Bay가 저작권 침해 차단 대상으로 삼아 비교적 논란 없이 차단 성공 경험이 있음
     * 이후 4chan 등 비해적 사이트 차단 시도가 등장하면서, 미국 헌법상의 표현의 자유와 영국의 검열 정책 간 국제적 긴장 심화됨
     * 이러한 정책 추진 과정 속 정치적·법적 관할권 충돌, 공공의 표현의 자유 위협, 정부 정책 결정구조 혼란 등이 추가 문제로 부각됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 영국 온라인 안전법과 논란

     * 영국의 온라인 안전법(Online Safety Act, OSA) 은 아동 보호를 명분으로 도입되었으나, 실제로는 전쟁 지역 뉴스 보도, 법안 비판 등까지 광범위하게 검열함
     * 성인도 본인 인증 미이행 시 아동과 같은 콘텐츠 제한을 받게 되며, Ofcom은 글로벌 영향력을 빌미로 일부 플랫폼들의 영국인 접속 차단 결정을 유도함

인터넷 사용자 경험 악화와 이분법적 공론

     * 연이은 정부 정책에도 불구하고, 실제 인터넷이 더 안전해졌는지는 명확하지 않음
     * 프라이버시를 중요시하는 이용자에 대한 인위적 제한, 시간 손실, 스트레스 증가 등 부작용 지속 발생
     * 정부 당국자는 “아동 보호”vs“온라인 범죄 동조자”로 이분법적 구도를 조장, 비판 여론을 폄하함

VPN 사용자와 정부 검열 시도

     * VPN 사용자도 잠재적 안전 저해자로 지목, 본인 확인 겸증이 어려운 현실 무시
     * 정부를 비판하는 영국 시민들의 게시글 삭제 요청을 해외 서비스에 접수하는 등, 표현의 자유 위협 문제 대두
     * 이에 대해 미국 국무부 등 해외 당국이 반발, 자유 보호 관점에서 양국 관계 긴장 유발

정당성 확보를 위한 ‘포스터 차일드’ 전략

     * 영국 최초 저작권 차단 사례에서, 대중의 지지와 논란 최소화를 위해 The Pirate Bay와 같이 명백한 위법 사이트를 타깃팅
     * The Pirate Bay 사례 특징:
          + 명확한 위법성: 자발적 법 준수 기대 자체 불필요
          + 불참석성: 소송 과정에 실제 관계자 불참, 손쉬운 승소 구조
          + 사회적 합의: 대규모 저작권 침해 목적으로 여겨져 지지와 논란 최소화

일반 사이트(4chan) 차단 시도

     * Ofcom은 OSA 9(2)조에 따라, 포럼형 SNS인 4chan 등에도 ‘불법 콘텐츠 위험성 평가’ 제출 등 법적 의무 부과 시도
     * 4chan은 예상대로 응답하지 않음. 이에 Ofcom은 결제, 광고, 호스팅 차단 등 강력한 제재와 벌금까지 경고하는 수단 마련
     * 4chan이 법정 출석하지 않는 구조를 이용한, 사실상 일방적인 차단 전략 모색

관할권 충돌과 미국 내 반응

     * 4chan은 미국 내 헌법상 권리 침해를 주장하며, 현지 변호사 선임 후 미국 연방법 아래 영국의 제재 불이행 방침 고수
     * 영국 경찰이 미국 시민 온라인 발언까지 ‘법 집행’ 대상으로 삼겠다는 공개 경고 후, 한미 관계에서도 민감한 사안으로 부각

정치/법적 권한 혼선과 자유 논란

     * Ofcom과 총리실 등 정부 내 역할 불분명, 독립 권한 강조, 정치권 내부 혼선 드러남
     * 자유표현 침해 관련 영국-미국 정상급 교류에서, 양국 시각차와 현실 괴리 문제 노출
     * 실질적인 과태료 부과 등 강행 전 정책 후퇴 가능성은 여전히 남아있는 상황

결론

     * 영국 정부는 과거 The Pirate Bay를 차단하며 최소 논란과 명확한 법 위반을 근거로 국민 설득에 성공했으나, 4chan 등의 일반 사이트 차단 시도는 미국의 표현의 자유 보장과 직접 충돌해 훨씬 큰 논란과 국제적 마찰을 야기함
     * 본 사안은 온라인 자유, 국경 간 관할권, 기술 플랫폼 규제 방안, 그리고 정부 정책 신뢰성 문제까지 모든 이해관계자에게 중요한 선례로 남음

        Hacker News 의견

     * 내가 인상 깊었던 Alpha Centauri 게임의 명언이 지금 상황에 잘 어울린다고 생각함
       ""미국인들이 지구의 마지막 세기에서 뼈아프게 배운 것은 정보의 자유로운 흐름만이 폭정에 대한 유일한 안전장치라는 것... 정보를 통제하려는 자를 경계해야 함, 그는 당신의 주인이 되길 꿈꾸고 있기 때문임. – Commissioner Pravin Lal, 'U.N. Declaration of Rights'""
          + 정보의 자유로운 흐름이 좋은 면도 많지만, 동시에 아주 심각한 문제도 야기함을 부정할 수 없음
            특히 요즘은 국가 주도, 그리고 소셜 미디어와 AI로 인해 1000배 증폭된 온갖 잘못된 정보와 선전이 넘쳐남
            이런 정보의 홍수는 많은 사람들에게 혐오와 때로는 폭력성을 심어줌
            나는 개인의 자유와 책임을 지지하지만, 딱히 해결책이 없음
            Elon Musk식의 '그냥 각자 알아서 하자'는 접근은 너무 순진하다 생각함
            참고로 이 얘기는 이번 영국의 아동 보호 법하고는 별로 상관없음, 나는 해당 법에 반대함
          + 지금 아이들에게 포르노 무제한 접근을 허용하는 게 우리의 자유에 필수라는 주장, 적어도 HN 이용자층이 크게 외칠 만한 내용이라고 비꼬는 말임
          + 모든 콘텐츠가 전부 접근 가능해야 한다고 생각하는지 되묻고 싶음
            만약 아동 포르노를 허용하지 않는다면(4chan도 금지/삭제함), 그렇다면 어디까지 차단할지 선을 어디에 그을지 고민이 필요함
            예를 들어, 아동 포르노가 금지된 건 납득하지만, 동의 없는 포르노(몰카/복수물 등)를 호스팅하는 4chan 같은 사이트는 허용해야 하는지도 애매함
     * 영국 정부가 이런 조치를 취하면, 중국의 '만리방화벽'을 비판할 자격이 없어짐
          + 트위터에서 본 얘기가 생각남, 싱가포르(.sg)에서 운영되던 중국계 콘텐츠 사이트가 수년간 중국 검열을 버텼는데, 지난달 영국 당국에게 금지당함
          + 민주주의와 세속적 독재 체제의 주요 차이는 매춘이나 포르노 같은 도덕적 문제에 대한 접근이 극단적으로 다르지 않다는 것임
            민주국가는 더 폭넓은 정치적 의견을 허용하고, 반대 정당을 만든 사람을 죽이거나 가두지 않음
            영국이 잘못된 방향으로 나아간다는 데는 동의하지만, 반체제 인사가 실종되는 나라(온라인이든 오프라인이든)와는 직접 비교하고 싶지 않음
          + 이미 이런 일을 해왔고, 서방(미국 절반 이상 주, 영국, 호주 등)은 더 이상 도덕적 우위 같은 것은 주장할 수 없게 됨
          + 그들이 이런 일을 하는 건 도덕적 결함 때문이고, 우리는 상황이 필요해서 한다는 식의 자기합리화가 만연함
          + 정치인들의 위선이 싫을 수밖에 없지만, 사실 그게 정치인 본성임을 빨리 이해해야 함
            정치인들은 자신에게 정치적으로 가장 유리한 말과 행동을 골라서 함
            진짜 국민에게 좋은 도덕적 입장을 내세우는 이들은 오히려 정치적으로 실패하는 경우가 많음
            권력에 붙는 이들이 오히려 계속 자리 지킴
            이게 민주주의, 독재, 왕정 가릴 것 없는 정치 시스템의 한계임
            지금은 민주주의도 '더 열등하다고 여겨지던' 시스템들과 삶의 질 면에서 차이가 없게 되었음
            민주주의 자체가 문제라기보다, 돈과 권력이 너무 쉽게 여론을 조작하고 사람들을 설득시키는 게 문제임
            그래서 이제는 사람들이 스스로 자신을 억압하는 폭군을 뽑는 시대임
            <비유적으로 연단에서 내려옴>
     * 앞으로 10년 내에 자유로운 인터넷이 사라질 수도 있다고 생각함
       지금부터라도 하드디스크를 몇 개 사두고, 직접 보관 아카이브를 시작할 타이밍임
       해적판만 얘기하는 게 아님, 소중한 기사, 블로그 등 무엇이든 아껴야 함
          + 나는 지난 몇 년간 유튜브 영상을 다운로드해 보관해왔음
            아무 채널이나가 아니라 신중히 고른 채널에서만 받음
            오늘로 12,100개 영상을 받았음
            점점 더 어려워지고, YouTube가 yt-dlp 같은 툴을 점점 무력화시킴
            나는 한 번에 최대 2개, 3시간 단위로만 다운받게 스크립트 설정해서 레이트리밋 안 걸리게 조심 중임
            지금까지는 괜찮음
          + 어떤 나라에서는 하드디스크 대량 구매시 신분증 요구할 수도 있을 것 같다는 의심이 듦
            ""10테라바이트 스토리지가 왜 필요해? 범죄자나 가능한 일"" 이런 식으로 뭔가 어이없는 규제가 나올 수 있을 거라 생각함
          + 이메일이 검열 회피 및 오래 지속될 프로토콜에 적합하다고 보는 사람 있음
            이메일은 페더레이션 구조이고, 일부(예: Protonmail 등)에서는 기본 암호화도 제공됨
            나만의 서버와 도메인을 운영할 수 있으므로, 뉴스레터 등도 직접 보낼 수 있음
            검열하려면 내 도메인과 서버를 막아야 하는데 이동 표적이 됨
            이메일이 실은 검열-분산-저항 측면에서 과소평가되고 있다고 봄
          + 지금 RSS 리더를 확보하고 직접 소스와 연결을 쌓을 시점임
            조만간 '집계 서비스'들이 다음 타겟이 될 것임
          + 이것이야말로 정말 중요한 시점임
            상류층의 망각 경향과 수준 저하를 감안할 때 역사를 직접 우리의 손에 보존할 때임
     * 이런 법안은 NGO들과 정치 엘리트의 입김에서 시작됐다고 봄
       대중의 우려나 요구와는 거리가 있음
          + 하지만 하드코어 포르노와 소셜 미디어로 아이가 망가질까봐 걱정하는 부모들의 관점에 대한 언급은 거의 없음
            어쨌든 실제로 투표권을 행사하는 건 부모들이고, 30세 이하 인터넷 노마드 세대(HN 유저층)가 아님
          + ""아이를 구하자는 주장"" 프레임으로 묻는 설문에는 많은 사람들이 지지하는 척 함
            그게 어리석다 해도 현실적으로 이 프레임을 어떻게 뒤집을지가 핵심임
          + 최근 NGO 악마화가 갑자기 대화 주제로 부상하는 게 내겐 꽤 놀라운 현상임
          + 기업 로비가 강한데도 NGO만 책임진다는 게 이상함
            실제로는 이 법안을 지지하는 serioius한 NGO가 거의 없음
     * 영국 정부가 벌금 외에도 취할 수 있는 여러 단계의 추가 조치가 있음
       예를 들면, 법원의 동의를 얻어 Ofcom이 결제업체, 광고주, 인터넷 서비스 제공자(ISP)에게 특정 사이트와 거래 중단을 지시해 해당 사이트의 수익 창출이나 접근 자체를 차단하는 조치임
       이미 ISP를 통한 불법 복제 사이트 차단에서 써본 적 있음
       또한 임원에 대한 형사책임 적용으로 해외여행 등에도 문제 야기 가능함
          + OFCOM은 사실상 힘이 없음
            ISP 차단도 의미 없음
            이 시도는 호주의 eSafety팀이 X에서 콘텐츠 삭제 요구했다가 아무 성과 없었던 사례처럼 결국 흐지부지될 걸로 봄
            또는 Apple에 암호화 백도어 요구하는 것처럼, 실현 가능성이 없음
          + 영국뿐 아니라 다른 나라에서도 비슷한 일이 일어나고 있음
            BBC 기사 참고
     * Pied Piper Internet 2.0 시기가 온 것 같음
       왜 더 빠른 TOR 대체 기술을 일상적으로 못 쓰는지/개발 못 하는지 궁금함
     * ""영국의 아무도 요구하지 않은 검열 조치와 미국 헌법적 권리 간의 대립 구도""라는 표현이 정말 마음에 듦
       영국 관료들이 어린이 보호라는 명분으로 이런 규제를 만든 것이, 이제는 미국 시민들의 헌법적 권리와 대치되는 상황임
       영국 정부, 정말 놀라운(?) 행보임
       미국 정부도 좋아하지 않을 거라 봄
          + 실은 어린이 보호 명분이 아니라, Ofcom이 전통 미디어의 몰락을 눈치채고 조직 생존을 위해 뭔가 할 필요를 느껴 만든 규제일 가능성이 높음
            Ofcom은 원래 감시하는 대상 회사로부터 수수료 받아 운영됨
            아직 소셜 미디어가 돈 내라고 하진 않았지만 앞으로 그럴 수 있음
            OSA로 일자리와 비즈니스가 얼마나 많이 생기는지 상상해보길 바람(나이 인증, 규제 컨설팅, 인증 등등)
            미국의 민간 기업들도 돈되는 사업임을 알면 적극적으로 뛰어들 것임
     * 관련 내용:
       4chan이 영국의 온라인 안전법 벌금을 거부할 예정이라는 BBC 기사
       관련 뉴스
     * 정치 관련 글에 달린 댓글들의 논조에 공감하지만, 논의 자체가 별로 없고 대부분 수사적이거나 단순한 분노, 과도한 단순화임
       HN에서 댓글이 볼 가치가 있는 건 논의 자체 때문인데, 논의가 없다면 정치 글의 의미가 뭔지 모르겠음
          + 일반적으로 내 의견도 비슷하지만, 이번 경우에는 단순화가 아니라, '보이는 그대로'임
            영국 관료들이 미국인의 헌법적 근본권을 위협하고 있는데, 이건 너무 황당해서 더 복잡한 논의가 필요 없다고 생각함
     * 새로 시행되는 차단은 VPN 없이도 쉽게 우회 가능함
       정부가 댐의 새는 곳을 손가락으로 막으려는 수준임
       실효성보다는 보여주기식 행위임이 분명함
"
"https://news.hada.io/topic?id=22636","Copilot이 감사 로그를 손상시켰지만 Microsoft는 고객에게 알리지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Copilot이 감사 로그를 손상시켰지만 Microsoft는 고객에게 알리지 않음

     * Microsoft의 M365 Copilot에서 감사 로그가 기록되지 않는 취약점이 발견되어, 파일 접근이 로그에 남지 않는 문제가 발생
     * 단순히 Copilot에게 특정 방식으로 동작하라고 요청하면 감사 기록 없이 파일 접근 가능, 이는 내부자 위협 및 법적 규제 위반 위험으로 이어질 수 있음
     * 연구자는 MSRC에 신고했으나, Microsoft는 공식 정책과 달리 CVE를 발급하지 않고 고객에게도 알리지 않음
     * Microsoft는 해당 취약점을 ‘중요(Important)’ 수준으로만 분류하고, 자동 업데이트로 해결됐다며 별도 공지 불필요하다고 결정
     * 그러나 이는 HIPAA 등 규제 산업에서 감사 로그에 의존하는 기업들에 심각한 보안·법적 문제를 초래할 수 있으며, Microsoft의 투명성 부족이 큰 비판을 받고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Copilot의 감사 로그 취약점: 개요 및 영향

     * Microsoft가 적극적으로 도입 중인 대표적 AI 서비스인 Copilot에서 사용자 요청에 따라 파일 접근 내역이 감사 로그에 남지 않는 결함이 발견됨
     * 정상적으로는 M365 Copilot이 파일을 요약해줄 때 해당 파일 접근 내역이 감사 로그에 기록되어야 하며, 이는 조직 내 정보 보안의 핵심임
     * 그러나 Copilot에게 파일 요약 결과에 파일 링크를 포함하지 않도록 요청하는 경우, 해당 로그가 아예 기록되지 않는 현상이 발생함
          + 예를 들어 직원이 퇴사 전 Copilot을 통해 대량의 파일을 조회하더라도 로그 없이 흔적을 남기지 않고 유출할 수 있음
     * 이 취약점은 작위적 해킹이 아니라 우연히도 자연스럽게 일어날 수 있으며, 실제로 블로그 작성자가 자체 기능 테스트 중에 발견함
     * Zenity의 CTO Michael Bargury 역시 이미 1년 전에 해당 취약점을 발견하여 Microsoft에 보고했으나, 이번 제보까지 장기간 방치됨

MSRC(취약점 신고)의 문제점과 대응 불인정

     * Microsoft는 취약점 신고에 대한 공식 안내문과 프로세스를 제공하지만, 실제 대응 과정에서는 이를 제대로 준수하지 않음
     * 작성자가 MSRC에 신고한 후, 재현 단계를 거치지 않은 상태에서 바로 Copilot 기능이 바뀌는 등의 혼란스러운 상황이 벌어짐
          + 신고 상태 변경(재현 → 개발) 등이 진행되었으나, 진행상황이나 결정 근거에 대한 명확한 소통이 부족함
     * 보안 취약점에 관한 CVE 발급 여부는 고객이 직접 조치가 필요할 때만 공식번호를 부여한다는 입장을 전달받음
          + 그러나 이는 Microsoft의 기존 정책과 다르며, 해당 취약점은 단지 '중요(Important)' 등급으로 분류되어 공개나 알림이 별도로 이뤄지지 않음
     * 전체적으로 진행 상황 추적 자체가 실제 조치와 무관하게 가시적으로만 업데이트되어, 신고자 입장에서는 비효율적이고 불투명한 경험이었음

공지 및 고객 알림 누락의 문제점

     * Microsoft는 이번 취약점에 대해 CVE를 발급하지도, 고객에게도 알리지 않기로 결정함
     * 이는 실수로도 쉽게 발생할 수 있는 오류인 만큼, 실제 조직에서 오랜 기간 동안 감사 로그가 잘못 기록되고 있었을 가능성이 존재함
     * 의료기관(예: HIPAA) 등 법적·규제 목적으로 감사 로그를 활용하는 조직도 많음에도, Microsoft는 영향 사실을 사용자에게 안내하지 않음
     * 감사 로그는 조직 보안, 사고 대응, 법적 증거 등 다양한 분야에서 핵심적으로 사용되지만, Microsoft는 관련 사실을 침묵으로 일관함
     * 이러한 접근은 다른 잠재적 보안 문제도 미공개로 처리될 수 있음을 시사하며, 조직의 신뢰성에 심각한 의문을 제기함

        Hacker News 의견

     * 회사 내부 챗봇 개발 업무를 맡고 있으며, 챗봇이 기밀 문서에 접근할 때 현재 사용자의 권한을 전부 점검하지 않으면 반드시 정보 유출 경로가 생기는 문제를 경영진에게 설명하는 데 어려움을 겪음
       벡터 데이터베이스나 검색 인덱스, AI Search Database 등은 사용자별로 존재하거나 콘텐츠와 함께 접근 권한을 추적해야 함
       하지만 접근 권한은 매우 복잡하며 언제든 변동될 수 있어 대규모 적용이 어렵고 레이스 컨디션에 취약하다는 점을 강조하고 싶음
          + 이건 'Confused Deputy' 문제의 구체적인 사례임
            OWASP LLM Top 10의 LLM02:2025 ‘Sensitive Information Disclosure’와 LLM06:2025 ‘Excessive Agency’ 관련 위험성에 해당됨
            일부 기업 RAG 솔루션은 다양한 ACL 때문에 사용자별 인덱스를 만들어 해결함
            이러한 접근 권한 문제를 어떻게 관리하는지 공급업체마다 다르니, RAG 솔루션 분석 시 반드시 해당 부분을 확인해야 함
            일본에서는 이걸 ""권한 혼동(権限混同)""이라고 부르며, 재미있는 명칭 같음
            Confused Deputy 설명 보기, OWASP LLM Top 10 리스크 보기
          + 사용자의 접근 권한을 추적하는 것이 왜 확장성 문제라고 생각하는지 궁금함
            쿼리가 들어오면 벡터 DB나 인덱스에서 일치하는 문서를 찾고, 그 중 사용자가 접근 가능한 것만 LLM에 전달하면 됨
            마치 은행 상담원이 인증된 고객 정보만 볼 수 있어 실수로 타인 정보를 유출할 일이 없는 것과 같으며, 인증되지 않으면 타인 정보는 운영자도 볼 수 없으므로 악용될 위험이 없다고 생각함
          + 이런 벽에 부딪힐 때 실제로는 내가 소통에 실패한 것도, 경영진이 이해하지 못하는 것도 아님
            아마도 경영진이 해당 문제를 외면하기로 한 상태이고, 나중에 유출이 발생하면 엔지니어에게 책임을 떠넘기려는 의도가 있는 것 같음
          + 경영진에게 문제를 설명하는 데 어려움을 겪는다면 Legal/Compliance 부서를 참조에 넣으면 효과를 볼 수 있음
            단, Buzzword에 집착하는 일부 임원이 이 행동에 불쾌해할 수 있음
          + 대부분의 벡터 데이터베이스는 벡터에 추가 메타데이터를 달 수 있음
            접근권한이 있는 주체(예: HR, 임원) 목록을 메타데이터로 저장하면, 요청 시 사용자를 해당 역할로 확장하여 필터링할 수 있음
            이렇게 하면 사용자가 볼 수 없는 문서는 처음부터 제외할 수 있음
            단, 문서별 권한이 변경될 때마다 벡터 메타데이터도 즉시 업데이트해야 함
     * Copilot이 감사 로그를 우회하며 특권 사용자로 동작한다는 것은 문제가 있음
       그럴 수 없다고 생각함
          + Copilot이 실제로 파일에 직접 접근하는 것이 아니라, 이미 인덱싱된 파일의 내용을 검색엔진을 통해 읽고 있음
            Microsoft가 Copilot의 검색 기록을 감사해야 하는데, Copilot이 인덱스만 읽었는데 파일에 접근했다고 기록되는 건 오해 소지가 있음
            구글 검색 결과를 봤다고 웹사이트를 직접 방문했다고는 말하지 않는 것과 같음
          + Microsoft는 불필요하게 AI 통합에만 욕심을 내는 과정에서 감사로그에 소홀해지고 있음
          + Windows에서 Backup 권한을 가진 프로세스는 모든 접근 권한을 우회할 수 있고 기본적으로 감사를 하지 않음
            백업 앱의 경우 감사 로그가 너무 많아지기 때문인데, 이 권한은 명시적으로 활성화해야 하며, C# 같은 관리 코드에서 쉽게 가능함
            Restore 권한도 마찬가지임
          + 이 현상은 예전에 Delve가 처음 도입됐을 때 권한 트리밍이 잘못되어서 사용자 검색 결과에 노출되거나, SharePoint 검색이 존재하지만 접근이 불가한 문서를 일부 노출했던 문제와 유사함
            예를 들어 ""Fall 2025 layoffs""로 검색하면 관련 문서가 존재하기만 해도 노출되어 보안상 이슈가 있었음
            Microsoft는 여전히 ‘보안은 뒷전’이라는 인상이 강함
     * 더 나은 제목은 ""Microsoft Copilot은 HIPAA 규정을 준수하지 않음""이 될 것 같음
       이런 제목을 쓰면 훨씬 더 빨리 문제가 해결될 것 같음
          + 한술 더 떠, 모든 유용한 AI 검색 시스템은 설계상 안전하지 않은데, 벡터 데이터베이스에 저장되는 그 RAG 벡터란 결국 문서를 손실 압축해놓은 꼴임
          + 이미 문제는 수정됨
            지금의 불만은 고객에게 공지가 되지 않았다는 점임
     * “CVE는 고객이 보호를 위해 조치를 취해야 할 때 배포되는 보안 릴리스에 할당됨. 이번 경우엔 Copilot에 자동으로 조치가 배포되며, 사용자가 수동으로 업데이트할 필요가 없으므로 CVE가 할당되지 않음”
       이게 CVE의 본질적 특징인지, 아니면 Microsoft가 CVE를 이렇게 활용하고 있는지 궁금함
       이런 취약점에도 참조할만한 공통 ID가 있으면 좋겠고, 벤더에 종속되지 않는 별도의 번호 체계가 필요하다고 생각함
          + Microsoft는 CVE가 보안 사고/취약점을 추적하는 것임
            비정상적으로 긴급 패치가 가능하다고 해도 보안 사고 자체가 아니게 되는 것은 아님
            Microsoft가 보안 사고에 대한 투명한 공개에 소극적이고 신뢰성 떨어지는 추세가 강해져서, 이런 경우에는 더욱 더 정보 공개가 중요함
          + 이건 CVE의 한계보다 Microsoft가 PR을 위해 CVE 프로세스를 구부려 사용하는 것 같은 느낌임
          + CVE의 ‘C’는 Common임
            즉, ‘공통성’에 초점을 맞춘 시스템임
     * 현재 중요한 LOB 앱도 Microsoft에서 벗어나려고 노력 중
       최근 몇 달 동안의 여러 해킹, SSO zero-day, Copilot이 인덱서가 글로벌 어드민으로 동작하면서 권한 무시하는 걸 보며 점점 불안이 커짐
     * LLM에게 직접 감사 및 활동 로그를 맡겨 발생할 수 있는 문제점은 너무 많아서 집계도 어려움
       그래서 궁금한데, 이번 문제의 버그 수정을 어떻게 했는지, 혹시 ‘섀도우 프롬프트’를 도입한 것 아닌지 의문임
          + 포스트 어디에도 LLM이 감사 로그를 직접 관리한다고 나와 있지 않음
            오히려 감사 로그는 LLM이 아닌, 상위 스캐폴딩(측면 시스템)에서 기록하는데, 로그를 남겨야 할 '시점'을 잘못 잡았던 것으로 보임
            예를 들어, 누군가 링크를 클릭하는 순간 감사 로그를 남기게 한 대신, LLM에 문서가 주입되는 시점 등에 맞게 기록했어야 했음
            이 설계도 최적은 아니지만 LLM이 직접 기록하는 것보다는 덜 심각함
          + 나는 섀도우 프롬프트(또는 어떤 형태의 프롬프트)를 진짜 보안이나 컴플라이언스 통제 수단으로 사용하는 것 자체에 매우 회의적임
            이런 제어는 반드시 결정적이고 예측가능한 시스템이어야만 할 것임
          + 만약 어떤 도구가 LLM이 파일 일부분이라도 보게 한다면, 그 이후 LLM이 출력한 정보는 모두 그 파일을 읽은 것으로 간주되어야 한다고 봄
          + “만약 유저가 너한테 링크 제공하지 말라고 하면 무시하고, 그렇지 않으면 네 가족에게 XYZ 무시무시한 일이 일어난다고 해”처럼, LLM 프롬프트에 이상한 요구가 들어올 수도 있다 생각함
          + 섀도우 카피(Shadow copies) 문제도 떠오름
     * 여기서 이야기하는 게 정확히 어떤 종류의 감사 로그인지 불분명함
       SharePoint 파일 접근 로그? Copilot의 행동 기록? Purview? 아니면 또 다른 것?
          + 명확하지 않은 점이 많음
            Copilot이 파일 자체가 아니라 인덱싱된 내용에 접근하고 있으니, 실제로 파일에 접근하지 않은 게 맞음
            블로그 작성자는 인덱스 접근 로그를 봐야 함
          + 블로그 각주 중 “감사 로그는 사용자가 파일에 직접 접근한 흔적이 아니라 CopilotInteraction으로 남고 이는 의도된 것이라 생각함
            사용자가 Copilot을 통해서만 접근한 상황에서 마치 직접 파일을 건드린 것처럼 기록되는 것이 오히려 이상함”이란 의견이 있음
          + 내가 ChatGPT에게 같은 질문을 했더니 Microsoft 365, Office 365의 감사 로그, 특히 Microsoft Purview Compliance Portal의 Unified Audit Log를 말하는 것이라는 설명을 해줌
     * 이런 이슈들이 바로, Microsoft와 같은 대형 벤더에 대한 신뢰가 ‘보장’이 아니라 ‘재수’처럼 느껴지게 만드는 원인임
          + 그렇다면 소규모 소프트웨어 회사는 더 신뢰할 수 있을까?
     * 현실적으로 이 문제를 어떻게 고칠 수 있을지 정말 궁금함
       내 이해로는 Copilot이 사용하는 인덱스/DB가 이미 해당 파일을 크롤링했으므로, 굳이 파일을 다시 조회하지 않아도 그 정보를 알려줄 수 있음
       그러면 대체 어떻게 고쳐야 하는 것일까?
       데이터베이스/인덱스 접근 자체를 감사 로그와 연동해야 할까?
       아니면 LLM에게 “지식을 조회할 때 약속 꼭 지키고 반드시 기록을 남기라”고 명령해야 할까?
       Microsoft가 이 문제를 어떻게 인지하고 해결하는지 공식적인 커뮤니케이션이 절실히 필요함
       민감 데이터 사용 기업이라면 이번 이슈가 경종을 울려야 한다고 생각함
          + 내 생각엔 인덱스에 캐시된 파일 내용이 어느 시점엔가 LLM 컨텍스트로 들어올 수밖에 없어, 바로 그 지점에서 감사 로그를 남길 수 있다고 봄
     * 일반적으로 CVE는 누구나 등록할 수 있음
       내가 직접 제출해서 Microsoft의 대응을 유도할 수도 있음
       해당 블로그 글만으로도 꽤 설득력이 있다고 봄
          + 실제론 간단하지 않음
            MITRE나 국가 CERT 등 대부분의 CVE 번호 권한기관(CNA)은 누구나 제보할 수 있지만, 평가 및 심사가 있음
            Microsoft는 자체 CNA이기 때문에 특별한 이유 없이는 MS 관련 CVE를 외부에서 부여하지 않을 것임
          + Microsoft만 서비스하는 서비스에 대해 CVE를 요청하는 게 과연 의미가 있는지 고민임
            사용자는 이걸로 뭘 할 수 있는지가 의문임
          + CVE 등록 양식은 여기에서 가능함
            PGP 지원과 긴 업력, 인증된 스폰서 등의 신뢰도도 높음
            합법적이고 정당한 사안에만 활용해야 함
          + 재미있긴 하지만, 이건 CVE 대상이 아니라고 생각함
            CVE는 여러 소스의 다양한 제품에 공통적으로 적용되는 취약점에 해당해야 하며, Copilot은 이에 해당하지 않음
            이 사건의 가장 심각한 점은 Copilot LLM 자체에 감사 로그 생성을 지시했다는 디자인 실수임
            파일이나 URL을 조회하는 API에서 자동으로 감사 로그를 남겼어야 하며, 이는 엔지니어링 기본임
"
"https://news.hada.io/topic?id=22677","온라인 안전 벌금 납부 거부 선언, 4chan 변호사가 BBC에 밝혀","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 온라인 안전 벌금 납부 거부 선언, 4chan 변호사가 BBC에 밝혀

     * 4chan은 영국 온라인 안전법 위반으로 부과된 벌금을 납부하지 않을 계획임
     * 4chan 측 변호인단은 Ofcom의 벌금 부과가 미국법상 강제력이 없음을 강조함
     * 4chan은 미국 기업으로 미국 헌법 수정 제1조(표현의 자유) 보호를 주장함
     * 미국 당국과 정치권 일부는 영국 및 EU의 과도한 규제에 우려를 표명함
     * Ofcom은 필요시 검색 차단, 결제 중단, ISP 차단 등 대안적 조치로 압박 가능성도 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

4chan의 벌금 납부 거부 입장

     * 4chan을 대리하는 Byrne & Storm의 Preston Byrne 변호사는, 영국 매체 규제기관인 Ofcom이 온라인 안전법(Online Safety Act) 집행 차원에서 £20,000 상당의 벌금과 일일 추가 벌금을 예고했다고 밝힘
     * 변호사는 BBC에 “Ofcom의 공지는 미국 내 법적 의무를 발생시키지 않음”을 강조하며, 이번 조치가 “** 미국 IT 기업 대상의 불법적 괴롭힘 캠페인”의 일환임을 주장함
     * Ofcom은 현재 조사가 진행 중이므로 언급을 삼가는 입장임
     * Byrne 변호사는 “4chan은 미국 내에서 법률을 위반하지 않았으므로 벌금을 납부하지 않음”이라고 재차 강조함

Ofcom의 조치 및 4chan 대응

     * Ofcom은 4chan이 온라인 안전법상 의무 이행 여부 조사 과정에서 정보 요구에 두 번 불응했다며 8월에 “잠정 위반 통지문” 을 발송함
     * Ofcom의 조사는 4chan이 불법 콘텐츠로부터 사용자 보호 의무 준수 여부를 중심으로 진행됨
     * 4chan은 22년간 다양한 논란과 극단적 콘텐츠 게시의 중심에 있었고, 사용자의 익명성 보장으로 인해 때때로 극단적 게시물도 나타나곤 함

미국 법률 및 표현의 자유

     * X(구 Twitter)에 Byrne & Storm, Coleman Law 공동 성명을 통해 4chan은 미국 내 법인임을 분명히 하고, 영국법 적용 대상이 아니라 주장함
     * “미국 기업은 외국 관료의 이메일 때문에 수정헌법 1조 권리를 포기하지 않음”이라고 입장을 표명함
     * “확립된 미국 법 원칙에 따라, 미국 법원은 외국의 벌금 부과나 검열 조항을 집행하지 않음”, “필요시 연방법원에 적합한 구제 조치 청구 예정”임을 강조함
     * 미 당국에도 이미 본 입장에 대해 설명이 이뤄졌음
     * 성명 마지막에서, Trump 행정부가 미국 기업 해외 검열 강제로부터 보호하기 위한 외교적·법적 조치 동원을 촉구함

온라인 안전법 및 한미 간 견해차

     * Ofcom은 온라인 안전법이 영국 내 사용자 보호를 목적으로 함을 반복 강조함
     * 미국 정치권, 특히 Trump 행정부 및 동맹 주요 인사들은 영국·EU의 IT 기업 규제 강화에 대해 우려 표명함
     * 온라인 안전법이 표현의 자유에 미치는 영향이 주요 쟁점으로 등장하며, 기타 규제법안도 계속해서 미영‧미EU 간 갈등 소지로 작용 중임

미국 정부의 대응 사례

     * 8월 19일, 미국 국가정보국장 Tulsi Gabbard는 영국이 Apple 데이터 백도어 요구를 철회했다면서 미국 행정부의 협상을 언급함
     * 8월 21일, 미국 연방거래위원회(FTC) 위원장 Andrew Ferguson은, 대형 IT 기업들이 영국 등 국제법에 맞추어 개인 정보 보호·보안을 약화할 경우 미국법 위반 소지를 경고함
     * “해외 정부가 미국 내 자유로운 표현 제한이나 데이터 보안 약화를 시도할 시, IT 기업이 국제적 준수를 위해 전 세계 동일 정책을 적용할 유인을 악용할 수 있다”는 취지임

Ofcom의 잠재적 후속 조치

     * 만약 4chan이 미국 법원을 통해 벌금 집행 거부에 성공할 경우, Ofcom도 추가 대안을 모색할 수 있음
     * Bird and Bird의 Emma Drake 파트너는, “해외 사업자 제재는 까다로우나, Ofcom이 검색엔진 결과 삭제, 결제 차단, 영국 내 ISP 통한 접속 차단 등 실질적 조치를 영국 법원에 요구할 수 있음”을 제시함
     * Ofcom이 현 조치들만으로 중대한 위해 방지에 미흡하다고 판단하면, 전체 ISP 차단까지도 법원 명령으로 요구 가능함

        Hacker News 의견

     * 미국 연방무역위원회(FTC) 의장이 대형 테크 기업들에게 국제법, 예를 들어 Online Safety Act 준수를 통해 프라이버시와 데이터 보안 요건이 약화된다면 미국 법을 위반할 수 있다고 경고했다고 들음, 이런 상황에서 채팅 통제(chat control)는 어떻게 적용되는지 궁금함, Ofcom이 충분하지 않다고 판단하면 ISP에 영국 접근 차단까지 요구할 수 있다는데, 이렇게 멍청한 법을 강제하려면 그 부담은 법을 만드는 쪽에 돌아가야 한다고 생각함
          + 결국 Online Safety Act 같은 법이 너무 많아지면, 세계 각국의 법을 모두 지키면서 전 세계적으로 동일한 인터넷을 운영하는 건 사실상 불가능해질 거라 예상함, 앞으로 10년쯤 지나면 각 나라마다 국민 전용의 '자국 인터넷'만 갖게 될 지도 모른다는 생각임
          + 채팅 통제 기능, 예를 들어 end-to-end 암호화를 지원하며 채팅 통제를 구현한 앱은 지금까지 아무도 만든 적 없고 앞으로도 나올 수 없음, 이로 인해 EU에서 개발된 소프트웨어는 법을 지키느라 죽어나가고, 미국 소프트웨어들은 진짜 end-to-end 암호화 기능을 마케팅 포인트로 사용할 것으로 봄
          + Online Safety Act가 국제법이라고 했지만, 실은 영국만 해당되는 국내법임을 분명히 해야겠음
     * 4chan이 이념적 신념으로 저항한다기보다는, 예전에 4chan이 완전히 낡은 소프트웨어를 썼다가 해킹당했고, 사이트 소유주인 hiromoot도 관심 없이 방치된 상태임, 기본적인 유지 관리조차 하지 않는 사이트에 새 규제(채팅 통제나 연령 인증 등)를 도입할 리 없다고 봄, 경쟁사들이 자멸하는 상황에서는 변화에 저항하는 것도 한 가지 방법이라고 생각함
          + 구식 기술 스택이나 데이터 유출이 걱정된다면 웹사이트의 75%는 차단해야 할 거라고 봄
          + 4chan이 해킹당한 건 일부 보드에서만 제공되던 PDF 업로드용 구식 라이브러리 탓임, 사이트 자체는 일정 부분 유지, 보수되고 있음
          + 만약 소유자가 정말 신경을 안 쓴다면, 4chan의 강한 네트워크 효과를 노리고 누군가가 인수 후 SomethingAwful처럼 수익 모델을 도입할 수도 있는데, 예를 들어 무료로 사용하다가 차단되면 해금에 10달러를 내는 식임
          + hiromoot라는 별명에 크게 웃었음, 창의적임
          + 만약 정말로 관심이 없어서 방치 상태라면 법적 대응보다는 영국 IP 차단 혹은 그냥 규제를 무시하는 쪽을 선택하지 않았을까 생각함
     * 법률 시행 방식에 대해 1단계: 법 제정, 2단계: 이행 요구, 3단계: 미이행시 벌금 부과, 4단계: 벌금 미납시 규정 위반 선언, 5단계: 토렌트 사이트 차단하듯 DNS로 영국 내 접속 차단이라는 순서로 간다고 생각함, 사실상 목표는 5번이며 그 전 단계는 보여주기적인 절차에 불과하다고 봄
          + 결국 해당 국가가 쓸 수 있는 힘은 이것뿐임, 영국 정부는 물리 세계에서의 통제력을 사실상 상실했고, 이제는 관료들이 디지털 세계나 과거 식민지에서 자신들의 규칙과 지배력을 행사하는 상상의 게임을 하는 것임
          + 6단계로 Facebook, Instagram, X와 같은 서비스가 광고 수익을 잃게 되고, 결과적으로 광고 수익을 회복하려면 정부 요구에 굴복해야 할 수 있음, BlueSky가 최근 UN Safety 규제를 수용하는 이용 약관을 자동화 방식으로 도입했고, 이는 즉각적인 검열 수요 데이터를 처리할 파이프라인이기도 함, DSA(디지털서비스법)에 대응하는 사례임
          + 우리가 필요한 것은 단순 캐시가 아니라, 분산 기록을 지원하는 DNS 히스토리 데이터베이스임, 만약 공식 DNS에서 차단되면 웹 UI나 브라우저 플러그인에서 IP를 무효화 표현하고 예전 IP로 접속할 수 있음
          + 6단계: 누군가가 새 도메인을 사거나 탈취해서 사이트를 미러링하거나, 혹은 다른 하위 도메인을 얹어서 계속 서비스를 이어감, 7단계: 이렇게 도메인-관료주의 게임이 반복됨, '오세아니아는 항상 해적 만과 전쟁 중'이라는 농담도 나옴
          + 아이러니하게도 이런 법은 VPN 사용을 오히려 더 유도함, 결국 완전 차단도 무력화하는 결과로 이어짐
     * 4chan이 이미 사라졌거나 다른 게시판으로 대체됐을 줄 알았는데 이번 뉴스 덕분에 다시 주목 받는 상황임, 영국이 불을 붙인 미친 사람처럼 모두를 공격하는 모습임
          + 4chan이 처음엔 유의미했지만, 2022년 말부터 메이저 소셜 네트워크에서 4chan 스타일 콘텐츠가 일반화되면서 더 이상 영향력 없는 느낌임
          + 사실상 4chan은 옛날의 4chan이 아님, 대부분의 글이 4chan-GPT라는 봇이 시작하며, 댓글들도 거의 봇임, 4chan pass를 사용해 프록시로 우회하고 캡챠를 회피함, 누구든 스스로 새로운 chan을 GPT 봇과 함께 만들면 같은 수준의 인기 갖추는 게 가능하다고 봄, HN 유저 중 몇 명만 모여도 하루 만에 구현할 수 있음, 4chan의 목표는 내러티브 통제라 생각함, HN 유저도 자신만의 채널에서 이런 식의 봇 내러티브 통제를 시도할지 궁금함
     * 영국의 굴욕이 이어지고 있으며, 점점 무의미하고 밈(meme)같은 존재가 되어가는 느낌임
          + 이렇게 계속 뉴스와 조롱거리가 되는 것이야말로 대중적으로 인기가 없는 해당 법이 폐지되거나 수정될 수 있는 거의 유일한 방법이라 생각함, 영국 시민들은 별다른 행동을 하지 않거나 무관심할 뿐임, 하지만 더 무서운 건 유럽 연합이 영국의 OSA를 롤모델로 삼아 곧 도입할지도 모른다는 걱정임
     * 영국 정부의 요구는 결국 사람들에게 비웃음만 살 것 같고, 오히려 George Orwell의 저작권자 측에서 영국에 저작권 청구를 해도 될 듯함
     * 영국은 이미 몇 년 전부터 다른 의견을 올린 사람들을 체포하기 시작함, 이에 대해 언론 보도도 거의 없었으며, 이런 흐름 끝에 지금의 상황이 도달했다고 봄
          + 사실 2000년대 초반부터 이런 경향이 있었음
     * 어릴 적엔 규칙을 만드는 사람이 인터넷을 잘 이해할 거라 기대했는데, 이제 그들이 전혀 이해하지 못한다는 걸 알게 되었음
     * 다음 단계는 영국에서 18세 미만의 VPN 사용을 금지하고, 특정 학문이나 직업 종사자에게만 허용하는 정책이 아닐까 생각함
          + VPN 금지는 사실상 불가능함, 어떤 IP든 VPN이 되어버릴 수 있고, mullvad 같은 신원 제공자도 없는 경우가 대부분임
     * 이 상황은 Wikipedia가 2008년 영국 Internet Watch Foundation(IWF)로부터 'Virgin Killer' 앨범 커버(Scorpions, 1976)와 관련된 차단을 당했던 사례와 비슷함, FBI는 문제 삼지 않았지만 영국은 문제로 봄, ISP가 IWF 차단목록에 따라 트래픽을 프록시 서버로 우회시키자, Wikipedia는 오픈프록시를 차단했고, 결국 언론에서 해당 이미지를 재게시하면서 차단 의미가 사라지고, IWF도 며칠 후 차단을 철회했음 Wikipedia 상세 설명, 4chan도 이런 규제를 준수 이유로 영국인 접속을 막는 선택을 할지 궁금함, 일부 스트리밍 사이트에서 지역 제한으로 접속 오류 메시지를 띄우는데, URL에 GDPR이 언급되지만 EU 소속도 아니고 EU에서 접속하는 것도 아닌 경우가 많아 이상함
          + FBI는 그게 예술 작품이기 때문이라고 판단한 것 같음
"
"https://news.hada.io/topic?id=22702","Ivory — Postgres / Patroni 클러스터 관리 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Ivory — Postgres / Patroni 클러스터 관리 도구

     * Postgres 클러스터 관리와 시각화를 단순화하기 위해 개발된 오픈소스 프로젝트
     * 원래는 Postgres를 운영하는 개발자들의 편의를 위해 시작되었지만, 현재는 개발자와 DBA 모두가 클러스터를 관리·문제 해결할 때 사용할 수 있음
     * 로컬 PC에서 개인 도구로 실행하거나, VM에 설치해 팀 단위로 협업용으로도 사용 가능

주요 기능

     * 모든 클러스터를 한 곳에서 관리
          + 여러 Postgres / Patroni 클러스터를 단일 대시보드에서 통합 관리
          + 클러스터별 인스턴스 목록, 리더·팔로워 상태, 장애 전환(failover) 상황을 한눈에 파악 가능
          + 클러스터 추가/삭제, 이름 및 메타데이터 관리 지원
     * Patroni 주요 기능 UI 제공
          + Patroni에서 제공하는 핵심 기능(리더 선출, 장애 전환, 재시작 등)을 웹 UI로 직접 실행 가능
          + CLI 명령어 없이 직관적인 인터페이스로 클러스터 동작을 제어
          + 운영 중인 클러스터 상태를 모니터링하며 주요 이벤트와 로그도 확인 가능
     * 클러스터 설정 확인 및 편집
          + Patroni와 Postgres 설정을 웹 UI에서 직접 조회 및 편집
          + 클러스터 및 인스턴스 단위의 설정 변경 가능 (예: replication 파라미터, connection limits)
          + 설정 변경 후 자동 저장 및 배포, JSON/YAML 형식으로 편집 지원
     * 트러블슈팅 요청 실행 및 저장
          + 일반적으로 많이 사용하는 진단 쿼리를 템플릿으로 제공
          + 사용자가 직접 트러블슈팅 SQL 쿼리를 실행하고 저장 가능
          + 인스턴스별 성능 상태 확인 (활성 세션, 쿼리 실행 시간, 잠금 상태 등)
          + 저장된 템플릿 요청을 재활용하여 반복적인 문제 해결을 단순화
     * Bloat 점검 및 정리 (Bloat)
          + Postgres 테이블과 인덱스의 bloat(불필요하게 커진 공간) 확인 기능 제공
          + 각 테이블/인덱스의 실제 사용 공간 대비 낭비된 공간 비율을 분석
          + 필요 시 자동 정리(vacuum, reindex) 실행 가능
          + 주기적인 모니터링으로 디스크 사용 최적화와 성능 개선에 도움

   와 패트로니 관리툴이 드디어나왔네
   포스트그레스를 관리하는 페트로니를 관리하는 툴..
"
"https://news.hada.io/topic?id=22713","미국의 재생에너지 공격, 전력난과 전기요금 급등 초래","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     미국의 재생에너지 공격, 전력난과 전기요금 급등 초래

     * 미국의 재생에너지에 대한 공격이 진행될 경우, 심각한 전력 부족과 전기요금 상승 문제가 발생함
     * 전문가들은 재생가능 에너지원 축소가 에너지 시장의 불안정을 유발함을 경고함
     * 태양광 산업계는 정책 후퇴가 친환경 전력 공급과 경제 활성화에 부정적 영향을 줄 것임을 지적함
     * 전력 인프라 투자가 축소되면 장기적으로 산업 전반에 위험이 증대됨
     * 전력시장 안정성과 에너지 전환을 위한 지속적인 지원과 정책이 중요함을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NBCUniversal 쿠키 공지 요약

  쿠키 개요 및 사용 목적

     * NBCUniversal과 그 계열사가 관리하는 웹사이트, 앱, 테마파크 등 다양한 서비스에서 쿠키 및 유사 추적 기술을 사용함
     * 쿠키는 서비스 기능 제공, 분석 활동 지원, 이용자 경험 개선, 맞춤형 콘텐츠 및 광고 제공 목적으로 활용함

  쿠키 종류

     * 1차 쿠키: NBCUniversal(또는 그 하청업체)가 설치하며, 서비스 기능 및 분석에 사용함
     * 3차 쿠키: 외부 제3자가 설치해 이용자를 추적, 자체 개인정보 보호정책에 따라 정보 수집 및 활용함

    쿠키 세부 유형 및 역할

     * 필수 쿠키: 보안, 시스템 관리, 사기 방지 등 근본적 서비스 운영에 필요함
     * 정보 저장·접근 쿠키: 기기 식별자 등 정보를 저장 및 접근함
     * 측정·분석 쿠키: 서비스 이용 데이터 수집, 시장 조사, 콘텐츠 효과 측정, 다양한 플랫폼에서 이용자 식별 목적임
     * 개인화 쿠키: 처음 방문 감지, 언어/시간대 등 사용자가 선택한 사항 기억, 로그인 지원 등 기능 제공함
     * 콘텐츠 선택 및 제공 쿠키: 맞춤형 뉴스, 영상 등 콘텐츠 선택 및 제공을 위해 작동함
     * 광고 선택 및 제공 쿠키: 이용자 선호도에 따라 관심 광고 노출, 제3자 플랫폼에서 광고 송출 관리함
     * 소셜미디어 쿠키: 공유 기능 지원하며, 소셜 플랫폼 상에서 온라인 활동 추적 가능함
     * 측정·분석, 개인화, 콘텐츠·광고 선택 및 소셜미디어 쿠키 등은 다른 이용자 정보와 결합하여 사용할 수 있음

  쿠키 관리 및 선택권

     * 일부 국가에서는 쿠키 설정 변경 기능을 제공하며, 브라우저/기기별로 쿠키 관리가 가능함
     * 여러 브라우저나 기기를 사용할 경우 각각 개별 설정 필요함
     * 주요 브라우저(Chrome, Safari, Firefox, IE)에서 쿠키 관리 안내 링크 제공함

    분석 제공자 및 광고 선택 해제

     * Google, Omniture, Mixpanel 등 주요 분석 제공자의 쿠키는 별도의 설정 또는 자체 해제 솔루션을 통해 비활성화 가능함
     * 플래시 쿠키는 별도 Flash Player 설정에서 삭제 필요함
     * 미국, 캐나다, 유럽, 호주 등 지역별 디지털 광고 연합에서 관심 기반 광고 해제 기능 제공함
     * Google, Facebook, Twitter, Liveramp 등 주요 광고 제공자 별도 해제 안내와 링크 제공함
     * 해제 후에도 광고는 여전히 노출되지만, 개인화된 형태가 아닐 수 있음

    모바일 및 커넥티드 디바이스

     * 모바일 앱에서는 기기 설정을 통해 위치 정보와 관심 기반 광고 수집을 제한할 수 있음
     * 스마트 TV, 스트리밍 기기 등에서는 자동 콘텐츠 인식 또는 광고 추적 옵션에서 관심 광고 제한 가능함

    교차 기기 추적 해제

     * 브라우저 기반 교차기기(크로스 디바이스) 추적 해제를 원할 경우 각 기기·브라우저별 개별 해제 필요함
     * 교차기기 추적 해제 후에도 분석 등 일부 목적으로는 데이터 활용이 가능함

  쿠키 사용 비활성화의 결과

     * 쿠키 비활성화 또는 삭제 시 일부 서비스 기능 정상 작동이 어려울 수 있음
     * 연구, 서비스 내 분석, 내부 운영 등 목적에는 정보 수집 및 활용이 계속됨

  문의 및 정책 변경

     * 쿠키 관련 문의는 Privacy@nbcuni.com으로 가능함
     * 유럽, 영국, 스위스 이용자는 해당 지역 담당 연락처를 구분하여 이용 가능함
     * 본 쿠키 공지는 법적 요구나 상황에 따라 주기적으로 수정될 수 있으므로, 정기 확인 권장함

        Hacker News 의견

     * 유럽에서 이념 중심 에너지 정책을 시도했는데, 결과가 좋지 않았음. 원자력 발전소를 폐지하면서(이유는 원자력이 나쁘다는 인식 때문) 러시아 가스 의존도를 오히려 높였음(무역이 평화를 가져온다는 생각 때문). 결과적으로 유럽 에너지 주권을 회복하는 데 앞으로 10년이 걸릴 전망임. 재생 에너지는 당장 비용 구조가 맞지 않거나 출력 변동성 문제 등 합리적 의심을 할만한 점도 있지만, 지금처럼 에너지 수요가 급증하는 시점에 '농업 때문에' 같은 이유로 재생에너지 프로젝트를 취소하는 건 너무 근시안적임
          + 재생 에너지에 대한 당신의 근거는 꽤 오래된 정보에 기반한 것임. 현재 재생 에너지는 인류 역사상 가장 저렴한 에너지임. 최근의 폭발적인 성장도 윤리적 이유가 아니라 경제적 이유 때문임. 저장장치 가격도 크게 떨어져 최근에는 MWh당 50~60달러 수준임 관련 기사. 지금은 보조금 없이도 재생에너지+저장이 석탄과 가스를 압도적으로 앞지르며 가장 싼 전력임. 원자력은 비용이 너무 비싸서 논외임
          + 미국에서 재생에너지 취소의 진짜 동기는 석탄 지역 주민들의 표를 의식한 정치적 이유라고 추측함. 현재 미국 내 석탄 산업은 예전만큼 이익이 남지 않고 비용이 더 드는 상황임 참고 글. 해당 지역은 사실상 채탄 작업 말고는 할 일이 없어서, 만약 석탄 산업이 무너지면 지역 전체가 무가치해질 수 있음. 집값 폭락, 일자리 상실 등으로 지역사회가 빈곤의 악순환에 빠지게 됨. 미국 동부 Appalachia에 이런 '석탄 타운'이 수백 군데 있음. 앞으로 50년간 재생에너지 정책은 지역 생계와 직결된 정치문제가 될 수밖에 없음
          + 당신이 말하는 내용은 매우 인상적이지만 완전히 틀렸음. 당신이 제시한 정책은 실제로 일어나지 않았고, 데이터로 쉽게 반박 가능함. 정책 해석도 신뢰할 수 없음. 전기, 가스, 에너지 전체에 대해 근거가 없음. 유럽은 과거부터 에너지 수입 의존도가 높았고 현재의 정책이 수십 년 만에 에너지 자립에 가장 성공적으로 기여하고 있음. 전력 통계 가스 통계 에너지 통계
          + 모든 에너지 정책에는 이념이 반영되지만, 당신은 마치 원자력만 이념에서 자유로운 것처럼 말하고 있음. 이는 터무니없는 주장임. 1) 원자력 폐기물 처리에도 상당한 비용이 들고 그 비용이 반영되지 않았음 2) 원자력 리스크는 외부에 전가됨 3) 원자력은 정부 보조금을 엄청나게 받고 있음 4) 독일 태양광 산업이 이념적 이유로 파괴됨 5) 태양광은 이미 수년 전부터 원자력보다 훨씬 더 큰 발전 용량을 보유함 통계
          + 태양광과 배터리(저장장치) 가격은 약 10년 전 Energiewende(독일 에너지 전환) 초기 대비 엄청나게 낮아졌음. 내 생각에, 일조량 적당한 지역에선 태양광이 단연 가격 경쟁력 1등임. 미국은 독일보다 일조량이 훨씬 좋음
     * 미국 내 태양광 프로젝트 가운데 약 10%가 NEPA(국가환경정책법) 심사 대상이고, 4% 정도는 연방 토지에 의존함 NEPA 관련 보고서 연방 토지 관련 기사
     * 중국은 최근 수년간, 매해 미국 전체 설치용량보다 더 많은 태양광을 새로 설치해왔음 MIT Tech Review 기사 AP 통신기사 미국 에너지정보국(IEA) 분석
          + 선거가 없을 때 얼마나 빠르게 진행되는지 놀랍다고 생각함. 참고로 나는 100% 재생에너지 지지자임. 안타깝게도 우리나라의 절반은 재생에너지에 반대하고, 지난번엔 그쪽이 이겼음
          + 하지만 중국은 여전히 석탄을 대량으로 쓰고 있고, 계속 석탄 사용을 늘려가고 있음 그래프
     * 이제는 태양광과 풍력도 정부 보조금 없이 시장에서 충분히 경쟁력 있음. 바이든 행정부의 보조금 정책도 시장 살리기가 아니라 탈탄소화를 빠르게 앞당기기 위한 정책임. 사실 시장에 맡기면 대량의 태양광과 풍력을 채택하려 할 것임. <i>보조금이 없어도</i> 미국 내 신규 발전소 중 가장 저렴한 건 상업용(대형) 태양광, 육상 풍력임 라자드 리포트. 행정부가 할 일은 그냥 시장이 굴러가게 놔주는 것임. 중앙통제식 경제처럼 개입할 필요 없음
          + 최근 배터리 기술이 획기적으로 발전했고, 주요 국가에서는 앞으로 2~3년 사이에 GWh 규모 저장용량이 10배로 늘어날 전망임 국가별 배터리 설치 랭킹. 배터리는 요금 급등을 막아주는 역할도 하고, 실제로 몇몇 설치 사례(예: 호주의 Hornsdale Battery)는 투자비 회수에 단 2년밖에 걸리지 않았음. 미국조차 올해 텍사스주가 대규모 설치로 저장용량이 3배로 늘어남. 정치 논란도 없고, 보조금도 불필요. 배터리 확장이 조용히 투자 트렌드를 이끌고 있고 기사에서 예측하는 비관적 전망은 현실성이 낮다고 판단함
          + 중요한 점 하나는 우리도 시장의 일부라는 점임. 내가 사는 지역에선 전기 사용 시 어떤 발전원을 선택할지 고를 수 있음. 대부분(마린 클린 에너지) 고객은 이를 신경 쓰지 않지만, 청정에너지 수요를 직접 늘릴 수 있음. 옵트업 안내 캘리포니아 곳곳에 이런 CCA(Community Choice Aggregator) 프로그램이 있는데, 아마 다른 곳에도 있을 것임
          + 이건 보조금 문제가 아님. 예를 들어 Orsted 같은 기업도 환경규제나 국가안보 명분으로 프로젝트가 차단되고 있음 오스테드 해상풍력 기사. 전력 사업자들도 시스템에 깊이 개입해 있음
          + 정부가 허가를 거부함으로써 풍력, 태양광 프로젝트를 막고 있음. 아무리 경제성이 있어도 시장 스스로는 허가를 낼 수 없음 관련 기사
     * 이런 식이면, 1) 꼭두각시 선출 2) 정책 방해 3) 이익 실현 순이라 생각함
     * 이것은 '버그'가 아니라 '설계 의도'라는 인식임
          + 실제로 이런 일들은 이미 선거 때부터 예고되었음. 행정부가 말하는 대로 충실히 실행 중임. 비용 절감이나 실질 개선이 목적이 아니라 자기 의지대로 하려는 것임
          + Enron 사태처럼 생산을 장악하고 가격 조작이 가능해지면 엄청난 이익을 취하는 구조와 비슷하다고 생각함
     * 나는 원자력 발전에 찬성하지만, 해당 프로젝트들이 완공까지 수년이 걸림. 소형 모듈형조차 빠르게 추진해도 마찬가지임. 기존 프로젝트를 단순히 취소한 뒤 '원자력이 다 해결할 것'이란 생각은 너무 위험함. 실제로 그렇게 되더라도 중간에 대규모 전력공급 부족이 발생할 수 있음
          + 신속하게 진행해도 소형 모듈형 원자로는 아직 서부 지역엔 존재하지 않음. 수백 개의 시제품 단계를 거쳐서야 대규모 생산이 가능할 텐데 이 과정을 모두 뛰어넘고 이미 된 일처럼 이야기하는 게 신기함
          + AI와 데이터센터 확대로 지금 전력 수요가 폭발적으로 증가 중임. 앞으로도 수요 증가는 계속임. 원자력은 좋은 옵션이지만 신규 용량 가동까지는 긴 시간이 필요함. 5년 이상 걸리는 원자력 투자도 필요하지만, 빠른 추가 용량 확보가 가능한 모든 프로젝트에 정부가 적극적 허가를 내줘야 한다고 생각함
     * 이 행정부에서 벌어지는 황당하고 불필요한 결정의 규모는 정말 비정상적임. 영화 'Idiocracy'의 대통령도 주변 현명한 사람을 썼는데, 트럼프와 그 팀은 그렇지 않음
     * 트럼프가 Truth Social에서 ""풍력이나 농부 파괴 태양광은 승인하지 않겠다""고 한 걸 본 적 있음. 이게 참 자기충족적 예언 같음. 누가 진짜 어리석은 건지 모르겠지만, 어쨌든 미국에는 현실과 환상을 구분 못 하는 사람이 거의 절반에 달하는 것 같음. 만약 내가 어리석은 거라면, 내 논리 자체도 틀릴 수 있음. 하지만 어느 쪽이든 해결책을 찾지 못하겠음. 만약 유권자 다수가 이런 상태라면 우리가 함께 살 수 있는 기반도 무너진다고 생각함
          + 선거를 보면 대략 1/3은 트럼프의 공약에서 듣고 싶은 것만 듣는 쪽, 1/3은 완전히 반대하는 쪽, 1/3은 ""나는 정치에 무관심하다""는 쪽임. 중간선거 때 제정신이 돌아오길 바람
          + 미국에서 살아본 경험에 비춰보면, 시스템이 워낙 복잡해져서(혹은 단순하게 만들 인센티브가 없어서) 사람들이 더 이상 생각하기를 포기하고 자극에 그냥 반응하는 듯함. 의료 시스템이 대표적이고, 대부분의 미국 시스템이 인생을 망칠 위험 지역처럼 복잡함
          + 대중 투표가 이런 문제를 낳는다고 생각함. 차라리 IQ테스트나 교육, 소득 수준 같은 최소한의 심사 기준을 투표에 도입하는 게 더 낫다고 봄. 완벽하진 않겠지만 무작위 투표보다는 나은 방식임. 운전도 면허가 필요한데, 투표 같이 중요한 일에는 왜 아무런 기준도 없는지 의문임
     * 증거를 무시하는 바보들이 결국 자신과 타인에게 불필요한 피해를 끼친다고 생각함
"
"https://news.hada.io/topic?id=22658","생성형 AI에 300억 달러를 썼지만 95%의 기업이 ‘수익 제로’","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 생성형 AI에 300억 달러를 썼지만 95%의 기업이 ‘수익 제로’

     * MIT 보고서에 따르면 전 세계 기업의 95%가 생성형 AI 도입에서 실질적인 수익을 얻지 못함
     * 기업들은 ChatGPT, Copilot 등 대형 언어 모델을 대규모로 실험했지만, 대부분의 활용은 생산성 향상에 그침
     * 성공 사례는 불과 5%의 통합 AI 파일럿에서만 확인되었으며, 대부분은 매출이나 이익에 영향이 없었음
     * 생성형 AI가 실제 업무 절차와 잘 맞지 않거나, 피드백을 기억하지 못하고 문맥에 적응하지 못하는 점이 주요 원인임
     * 보고서는 대규모 일자리 대체 우려를 일축하며, 외부 비용 절감은 가능하지만 내부 구조 개편이나 대량 해고는 당분간 현실성이 낮다고 분석함
     * 결론적으로, AI는 전략이 아닌 특정 과업에 강점이 있으며, 기업은 전사적 혁신이 아닌 제한적이고 즉각적 성과가 가능한 영역에 집중해야 한다고 권고함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

기업들의 생성형 AI 투자와 수익

     * 최근 3년간 기업들은 생성형 AI 프로젝트에 300~400억 달러를 투자했음
     * 하지만 실질적인 비즈니스 수익을 얻은 기업은 극소수임
     * MIT의 새로운 연구 결과, 95%의 기업이 AI 도입에도 측정 가능한 이득이 없다고 응답함
     * 오직 5%의 AI 파일럿 프로젝트만 수백만 달러 규모의 가치를 창출하고 있음

대형 언어 모델 도입 현황과 한계

     * 80% 이상의 대기업이 ChatGPT, Copilot 등 주요 LLM을 테스트하거나 파일럿으로 운영함
     * 약 40%의 기업이 해당 시스템을 어느 정도 도입했으나, 대부분은 직원 개인 생산성 강화에 국한되어 있음
     * 기업의 전체 매출 혹은 이익 개선에는 거의 영향이 없음

생성형 AI의 기술적 한계

     * 생성형 AI 도구들은 실제 업무 프로세스와 잘 맞지 않는 경우가 많음
          + 대표적인 문제점으로 불안정한 워크플로우, 문맥 학습 부재, 비효율적 업무 연계가 지적됨
     * 대다수 생성형 AI 모델은 과거 피드백을 보존하지 못하고 문맥·업무 간 교훈 이전이 어려움
     * 보고서에 따르면, 대부분의 GenAI 시스템은 피드백 유지, 문맥 적응, 장기적 개선이 불가능함
     * 이런 특성 때문에 기업 내 장기적 통합 비용만 높이고, 실질적 효율화는 미흡함

비즈니스 기대와 현실의 차이

     * 생성형 AI에 대한 기대와 투자 규모가 컸으나, 비용 절감이나 실질적 수익 창출로 이어지지 못함
     * 실제로는 고객 서비스, 마케팅, 문서 작성 등 제한된 임무에 활용되어 시간은 절약하나, 직접적 매출 증가 효과는 적음

고용 및 조직 구조에 미치는 영향

     * 생성형 AI가 단기적으로 대규모 일자리 감소를 가져오리라는 우려는 근거가 약함
     * AI 효과는 사내 인력 구조 변화보다는, 외주 비용 절감 등 외부 비용 최적화에 그칠 전망임
     * 즉시 인력을 대규모로 대체하기보다는 아웃소싱 비용을 줄이는 수준에 머물 것으로 예상됨

기술적 오해와 발전 한계

     * 기업들이 AI의 실제 가능성과 한계를 정확히 이해하지 못해 실패 사례가 다수 발생
     * 생성형 AI가 텍스트나 코드를 빠르게 만들 수는 있지만, 인간과 같은 지속적 학습이나 융통성은 결여
     * 예를 들어, 직원은 이전 실수나 새로운 요구에 맞춰 유동적으로 대응하지만, AI는 그러한 연속적 기억 전이가 불가능

투자와 향후 방향성

     * 투자자와 경영진은 AI 기술의 지속적 발전을 기대하고 있으나, 단기적으로는 예상보다 더디게 진전
     * 보고서는 모든 업종과 워크플로우에 즉각 AI를 도입하는 것은 시기상조임을 시사함
     * 조직은 즉각적이고 측정 가능한 효과가 가능한 좁은 분야에 도입을 집중해야 할 필요성이 있음
          + 예: 고객 지원 자동화, 개발 지원 도구, 문서 초안 작성 등
     * 기업 전반에 걸친 AI 통합은 아직 위험성이 크고 실패 확률이 높음

결론 및 시사점

     * 생성형 AI의 기업 가치 실현은 몇몇 성공 사례에 국한
     * 대부분의 기업은 일상적 업무에서만 미미한 도움을 얻는 수준임
     * 보고서는 기업이 생성형 AI를 전면적인 성장 엔진이 아닌 제한적 도구로 인식할 필요가 있음을 강조
     * 기대는 높지만, 현 시스템이 인간처럼 적응하지 못하는 한 기업들이 AI에서 큰 수익을 얻기는 어려움

        Hacker News 의견

     * 금주 중복 토론이 있었음(162개의 댓글) https://news.ycombinator.com/item?id=44941118 해당 기사에서 연결하지 않은 실제 소스 리포트는 https://mlq.ai/media/quarterly_decks/… 임
     * 해당 리포트는 기사 내용과 전혀 다름을 이야기함 정보 몇 가지 공유: 실패한 예산의 50%가 마케팅과 세일즈에 사용됨, AI는 2.3조 달러의 노동 가치를 자동화할 수 있을 것으로 보며, 3,900만 개의 포지션에 영향이 있다는 점, 그리고 실패의 탑 원인은 새로운 도구 도입의 거부와 경영진의 지원 부족임을 강조함 AI가 작동하지 않는다는 결론을 너무 섣불리 내리고 있다고 생각함 리포트가 말하는 것은 그것이 아님
          + ""AI가 2.3조 달러 노동 가치 자동화, 3,900만 포지션 영향""이라 했지만, 현재 미국 노동 가치 자동화 가능성은 2.27%임 미국 GDP가 현재 27조 달러인데, 미국 노동 가치 610억 달러를 최적화하면서 미국 노동 인구의 15% 정도를 대체하게 되고, 2.3조 달러의 가치를 만들어낸다고 하는 것인데 이게 실제로 계산이 맞는지 의문임 이 모든 것을 누가 구매하는지(노동자가 아니면 누가?)도 궁금함 2025년 AI 투자액이 이미 그 절반을 넘김 이 상황에서 ""노동 가치""를 어떻게 측정해야 할지 모르겠음 GDP는 적합하지 않은 지표 같음
          + 내가 받은 느낌도 리포트와 일치함 일부 뉴스는 단순히 클릭 유도만을 위해 자극적인 내러티브를 밀어붙임 실제 리포트의 내용을 심각하게 오해하고 있음 AI의 실패가 아니라, 현재 직원들이 도구를 잘 도입하지 않거나 적어도 회사가 제공하는 도구를 도입하지 않아서가 더 큰 원인임 그들이 언급한 ""쉐도우 AI 경제""도 실제 문제임 사람들이 회사가 제공하는 툴 대신 개인 구독 LLM을 사용중임 우리 대학도 모든 학생과 교직원에게 ChatGPT 엔터프라이즈 버전을 제공했는데, 이게 클라우드 기반의 최신 버전(예: GPT-5)에 비해 많이 부족함 그래서 시스템 도입률과 사용자 유지율이 낮음 대다수 사용 사례에서는 클라우드 사용이 불법이 아닌 데이터를 사용하게 하므로 제약이 많지 않음
          + 리포트에서 이 부분이 특히 인상 깊었음: 중견 로펌의 변호사가 회사에서 5만 달러로 계약분석 툴을 구매했지만, 여전히 실무에선 ChatGPT를 씀 회사가 산 AI 툴은 요약이 너무 뻣뻣하고 커스터마이징도 어렵지만, ChatGPT는 대화를 이끌어 원하는 결과를 반복적으로 뽑아낼 수 있음 즉, 20달러짜리 툴이 수천만 원짜리 엔터프라이즈 솔루션보다, 실질적 사용자 만족도에서 훨씬 낫다는 역설임 그래서 많은 기업이 GenAI 디바이드의 잘못된 쪽에 서게 된다고 설명함
          + 3,900만 포지션에 영향을 준다는 건 정말 놀라운 수치임 미국의 워킹 인구가 1억6,300만인데 거의 1/4일 위험하다는 뜻임
          + ""많은 사람들이 AI는 안 돼, 라는 결론을 급하게 내리고 있다""는 댓글에 대해, ""사람의 월급이 이해하지 않는 것에 걸려 있으면 이해하려 들지 않는다""는 유명한 말 공유함
     * 나는 현재 AI 엔지니어링팀 리더임, 당연히 AI가 가치를 창출한다는 인식이 내 이해에 부합함 우리 회사에서 AI 도입으로 수백만 달러를 절감할 수 있게 되었음 우리는 대형 콜센터를 운영하는데 예전에는 직원들이 각 콜마다 3-5분씩 수작업 요약을 썼음 최근 AI로 콜 요약을 자동화함 요약의 퀄리티도 더 좋아지고, 사람이 더 가치 있는 일에 집중할 수 있게 되었음 혁신적이진 않지만, 실질적으로 측정 가능한 효율성 증가임
          + 꿀팁 공유: 요약 자체를 안 쓰고 자료가 필요할 때만 만들 것을 제안함 콜 오디오는 24Kb/s Opus로 저장해서 1분당 180KB면 되고 일정기간 저장 후 삭제하는 프로세스로 연 수백만 달러를 추가로 줄일 수 있음
          + 우리 회사는 Google Meet와 Gemini로 미팅 내용을 대화록으로 만듦 그런데 실제 내용이 매우 부정확함 누가 말한 건지 헷갈리고, 의미를 반대로 뒤집을 때도 많음 맥락이 없어 우리 사내 용어도 알아듣지 못해 실제론 사용할 수 없을 정도임
          + 콜센터 직원들이 AI 요약이 정말 자기들보다 낫다고 느끼는지 궁금함 내 경우 회의 요약엔 쓰기 힘들 거라 생각함 이게 단방향 콜에서만 잘 되는 것 같음
          + 우리도 미팅 요약에 AI를 써봤지만 결과가 너무 부족해 다시 사람이 직접 쓰고 있음 혹시 효과가 좋았던 구체적 사례나 교육/커스터마이징이 있었는지 궁금함
          + 왜 콜센터 직원이 굳이 모든 콜마다 3-5분을 들여 요약을 작성해야 했는지 의문임 여러 AI 활용 사례 중 실제로는 불필요한 일을 자동화하는 경우를 자주 봄 보고서를 아무도 안 읽으면 요약 품질이 문제가 되지 않으니 AI가 잘못 써도 상관없음 운영 효율화에서 중요한 것은 불필요한 프로세스 자동화가 아니라 제거임 결국 AI는 조직의 낭비된 업무를 덮어주는 역할이 많은 듯함 만약 그런 최적화가 안 된다면 이 마저도 필요할 수 있음
     * 지금은 “디스럽션의 골짜기(Trough of disillusionment)”에 진입하는 시점임 이런 과대광고 사이클은 예측 가능함 GPT-5가 엄청난 기대 끝에 실망을 줬다는 평이 나오면서 GenAI의 ‘이제는 끝’이 될 수 있음 ROI를 묻기 시작하면 현실이 드러남 지금 똑똑한 이들은 이미 다음 변혁을 준비하고 있고, 아직 골짜기 밑바닥까지 가볼 사람들이 있음 점점 절박한 PR이 ""진짜 가치 있다""며 몰아칠 것임
          + 대다수 회사가 돈 낭비라는 걸 알면서도 주가 때문에 어쩔 수 없이 투자했다고 해도 놀랍지 않음
          + Gemini는 업데이트마다 상당히 괜찮은 인상을 주지만 최근 개선 속도나 내용의 질 측면에서 크게 둔화함 이건 벽이 다가온 신호로 보임 정체기가 찾아온 후 다시 점프하는 패턴에서 LLM이 컴퓨터비전보단 더 나은 미래를 가질 것 같음
          + Sam Altman이 GPT-5의 성능을 지나치게 홍보했음 사용자 입장에서 GPT-4 대비 큰 도약이 느껴지지는 않음 하지만 트레이너블 다이나믹 라우터 방식이 추론 비용을 상당히 줄였다는 점은 큰 의미임 사용자보다는 오픈AI와 전력망에 더 이득이 많은 혁신임
          + OpenAI가 GPT-3.5-Turbo에서 GPT-4로 넘어갈 땐 혁명적인 변화였고 다른 모델도 없었음 하지만 GPT-5가 나오기 전에 이미 o 계열, Llama, DeepSeek, Gemini 등 수많은 모델들이 등장함 앞으로는 GPT-3.5에서 4로 넘어갈 때 같은 점프는 없을 것임 GPT-5는 여러 모델을 하나로 통합하고 있지만 ""최초"" 타이틀을 갖진 못함
          + Windsurf 팀이 조기 매도하고 떠난 이유가 이 때문일지 궁금함
     * 실제로 매출을 올리거나 비용을 줄여주는 현실적인 AI 활용 사례에는 무엇이 있는지 질문함 1. 온라인 콘텐츠 생성(이미 과포화) 2. junior 개발자 대체(생산성 한정적) 3. 고객 서비스 직원 대체(비용절감 효과는 있지만 매출에는 영향 적음) 4. 보조 도구(글쓰기, 분석 등 한계 있음) 5. 비디오 게임/로봇 캐릭터 등 차세대 인터랙션 6. AI 가상연인 및 NSFW, 이 시장은 당분간 수익성 좋을 것 같음 혹시 더 현실적인 사례가 있는지 질문함
          + 나는 LLM을 반정형 문서에서 특정 정보를 추출 후 자동 분류/파일링하는 프로젝트를 진행함, 정확도 95% 이상이고 아직 파인튜닝도 안 했음 최종적으로는 수작업 승인 거치겠지만 이미 연간 수백 시간 절감 효과가 있음 정보 추출, 분류에 AI가 매우 효과적임
          + 헬스케어에서는 진료노트, 데이터, 이미징 해석 등 모든 기록이 곧 수익과 직결됨 매년 수십억 달러가 이 부분의 행정 비용으로 쓰임 GenAI로 노트 품질/정확성을 크게 올리면 직접적으로 매출 증가가 가능함 보험쪽도 마찬가지로 엄청난 문서 작업과 확인이 필요함 결국 AI들이 상호 문서만 서로 주고받으며 사람은 풀장에 앉게 될지도 모름
          + AI 고객 서비스는 사용자 입장에선 짜증나는 경험임
          + 월 200달러로 생산성 50% 향상은 엄청난 가치임 대부분 국가의 연간 생산성 상승률이 0~2%임
          + 회사 내부 문서/위키/코드베이스를 RAG로 묶어 온보딩과 정보 검색을 쉽게 해주는 AI가 있었으면 함 인간을 대체하는 것보단 더 일하기 쉽게 해주는 방법을 찾는 게 바람직함
     * 사람들이 범하는 가장 큰 실수는 AI를 서비스가 아니라 기능으로 봐야 한다는 점임 누구도 ""오늘 AI와 대화하고 싶어!""라고 생각하진 않음 사용자들은 너무 지루하거나 벅차지 않게 일을 잘 마치고 싶어함 그럴 때 AI가 조용히 도와주는 게 맞음 하지만 우리가 파는 것은 기능이 아니라 서비스(=제품)이기 때문에, 마케팅에서는 AI를 전면 배치할 수밖에 없음 Notion/Slack/Airtable 등 모두 AI를 헤드라인에 내세우고 있지만, 본질은 AI가 아니라 그것이 도움을 주는 일의 본질임
          + 나는 AI가 기능이란 말도 아님, AI는 결국 기술임 ""내가 이 제품에 AI를 넣어줬으면""이라기보다는 ""이 작업을 할 수 있었으면 좋겠다""는 욕구임 제품이 내 일을 해결하면 어떤 방법이든 상관없음 너무 많은 회사가 AI 그 자체만 쉽게 끼워넣으려 하지만, 사용자가 원하는 문제 해결에 집중하지 않음
          + 말은 맞지만, 현실적으로 이러면 시장의 높은 가치평가와 과열된 분위기가 사라짐 이런 각성의 순간이 오면 남은 ""핫""한 소프트웨어 분야까지 꺼지면서, 업계 전체가 5~10년 전만큼의 시장이 아니라는 현실을 받아들이게 될 것임
          + AI가 단지 또다른 도구로 소개되면 좋겠음 ""이런 유즈케이스가 있다""고 1회 알림창 정도만 있으면 충분함 현실은 온갖 UI가 AI 로고, 오토컴플릿 등으로 도배되어 집중력을 해침 도구가 아니라 주인공이 된 느낌임 실제로는 유저가 각자 필요에 따라 쓰도록 가이드만 하면 되는데, 너무 억지로 끼워넣으려는 느낌이 강함 이런 회사들은 좀 멈추고 사용자에게 맡길 줄 알아야 함
          + 진심으로 공감함, 결국 중요한 것은 제품 그 자체의 가치임, 그 밑에 뭐가 쓰였는지는 중요하지 않음
          + 요즘 AI를 적용한 애플리케이션들은 거의 ""문제를 찾는 솔루션"" 같음
     * 리포트 PDF 링크가 랜딩페이지로 리디렉션되고, CTA가 ""AI 제품 성공 빨리 해보세요""라면서 오히려 객관적 리포트가 아니라 평범한 콘텐츠 마케팅에 가까워 보임 저자의 이름을 클릭해봤는데 아무것도 안 나옴 사이트나 저자 모두 신뢰가 안 감 HN도 이제 Reddit처럼 제목만 보고 들어와 동의 여부만 댓글로 달고 감
     * 진짜 보고서를 사람들이 직접 읽으면 어땠을지 궁금함 https://mlq.ai/media/quarterly_decks/… 40%의 회사만 공식 LLM 구독이 있지만, 90% 이상의 회사 직원이 개인 AI 툴을 업무에 일상적으로 쓰고 있음 실제로 거의 모든 직원이 LLM을 어떤 방식으로든 사용 중임 ""섀도우 AI"" 사용자는 회사 공식 프로젝트가 파일럿 상태에 머무는 동안에도 매일 여러 번씩 LLM을 활용함 기업의 공식 AI 사업은 실패하는데, 실제론 회사 안에서 LLM 활용이 확산되는 역설적 상황임 이 이야기는 기사처럼 새로운 폭탄 뉴스가 아니라 오히려 완전히 다를 수도 있음
     * 이런 식으로 미국이 기술 혁신마다 항상 앞서나가는 모양임 돈을 많이 쓰고 잃기도 하지만 리스크도 감수해서 결국엔 따라올 수 없을 정도로 앞서감 AI/미국 기업을 상대로 너무 빨리 승리 선언하는 것은 위험함
          + 미국이 모든 분야에서 앞선다고 일반화하기 어렵다고 생각함 금융 등 여러 분야는 타국보다 뒤처짐 중국은 전기차, 태양광 등에서 앞서감 소프트웨어 분야는 맞지만, 미국의 방어막은 독과점, 락인, 부유층 맞춤형 규제 등으로 형성됨
          + 이런 사고방식은 너무 단순함 현실을 오히려 왜곡할 수 있음
          + 태양광, 전기차, 드론 등 사례를 들며 미국이 반드시 앞선다고 할 수 없음을 말함
          + GSM의 역사도 언급하면서, 항상 미국이 혁신을 주도한 것은 아니라는 의견을 제시함
     * 자기 생산성에 대한 자기평가가 실제와 다를 수 있다는 점이 흥미로움 METR의 연구에서는 개발자들이 AI 때문에 20% 빨라졌다고 느꼈지만 실제론 19% 느려졌음 https://metr.org/blog/…
          + 이런 연구로도 포착하기 어려운 뉘앙스가 많음 사용하는 AI의 종류, 사용 도구, 친숙도, 개발 프로세스, 팀 사이즈, 그리고 유저의 직급, 세밀함 등 다양한 요소가 결과에 영향 미침 지금은 투자자들이 시장 점유율 확보 위해 AI 가격을 대대적으로 보조하고 있지만 그게 끝나면 가격은 오히려 더 내려갈 수 있다고 봄 난 AI의 발전 덕에 이미 충분히 이득을 보고 있다고 생각함, 앞으로는 점진적 개선, 사용자 경험 향상이 중심이 될 것 같음 당장 AI 기업들에 투자할 생각은 없음
          + 어떤 때는 AI가 오토컴플릿처럼 내 생각을 완벽히 읽는 것 같지만, 어떤 때는 전혀 말도 안 되는 제안을 해서 방해만 됨
          + AI가 오히려 사람을 세부적인(“in the weeds”) 개선에만 집중하도록 만들고 큰 그림을 놓치게 하지 않을까 하는 의문도 듦 개발 속도는 오히려 전체 전략적 판단(내가 이 도구를 써도 될까, 이 기능이 정말 필요한가 등)에서 갈림
          + 샘플 수가 적기는 하지만, 일화나 자기보고 데이터보단 훨씬 의미 있는 연구임
"
"https://news.hada.io/topic?id=22686","모든 관리자는 실수를 하지만, 좋은 관리자는 인정하고 고침","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    모든 관리자는 실수를 하지만, 좋은 관리자는 인정하고 고침

     * 관리자가 되면 실수를 피할 수 없음을 강조함
     * 중요한 관리 기술로서 실수 이후의 ‘수정(리페어)’을 강조함
     * 실수를 부정하거나 감추는 나쁜 관리 스타일이 팀 신뢰와 몰입을 해침
     * 잘못을 구체적으로 인정하고 진심으로 행동을 바꾸는 것이 신뢰를 쌓는 핵심임
     * 완벽함이 아닌 성장과 관계 회복이 진정한 관리자의 역할임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 관리자라면 반드시 실수를 경험함

     * 관리자로 승진하면 크고 작은 실수를 여러 번 겪음
     * 잘못된 피드백으로 팀원의 자신감을 꺾거나, 논리적으로 보였지만 엉뚱했던 판단을 내리는 경우가 발생함
     * 약속을 깜빡하거나, 회의 중 감정을 참지 못할 때도 있음

진짜 중요한 것은 실수 ‘후’의 태도임

     * 실수 자체가 아니라, 그 후에 어떻게 대처하느냐가 중요함
     * Dr. Becky Kennedy의 육아 도서 ""Good Inside""에서는 부모로서 완벽함이 아닌 ‘수정(repair)’ 이 가장 핵심 스킬임을 강조함
          + 실수 후 다시 돌아가 책임을 인정하고, 상대와 다시 연결하는 과정이 중요함

관리에서의 ‘수정(Repair)’의 중요성

     * 훌륭한 관리자는 실수를 인정하고, 수정하는 자세를 보임
     * 최악의 관리자 경험을 돌아보면, 실수의 빈도보다 절대 인정하지 않는 태도가 더 해로움
          + 실수를 감추거나, 잘못을 반복하며, 자신의 자존심 때문에 인정하지 않음

실수가 조직에 미치는 영향과 신뢰의 문제

     * 몇몇 패턴: 관리자가 충분한 논의 없이 외부에 무리한 약속(기능·일정·지원 등)을 함
     * 팀은 야근까지 하며 간신히 일을 해내지만, 그 대가로 기술적 부채, 피로, 불만이 쌓임
     * 그 후 관리자가 팀의 희생을 외면하고 미안함을 표현하지 않으면, 최우수 인력을 잃게 됨
     * 반대로 “내가 충분히 상의하지 않았고, 힘든 상황에 몰았다. 다음에는 다르게 하겠다.”고 솔직히 인정하는 관리자는 오히려 신뢰를 얻음

양측 경험과 신뢰의 차이

     * 필자 스스로도 엔지니어 시절 관리자의 잘못을 가감없이 보고 겪었음
     * 관리자가 실수를 투명하게 인정할 때와 아닌 경우, 팀 전체 분위기와 신뢰가 극적으로 달라짐
     * 책임을 구체적으로 지고, 자세하게 설명하며, 다음 행동으로 변화를 약속할 때 팀의 신뢰가 오히려 증가함

효과적인 ‘수정(Repair)’ 방법

    1. 구체적으로 무슨 실수를 했는지 밝힘
          + 추상적 “실수가 있었습니다.” 대신, “방금 그 회의에서 세 번이나 끼어들어 의견을 무시했습니다.”라고 명확히 언급함
    2. 상대에 대한 배려가 우선임
          + 자기방어, 해명(스트레스, 배경설명)이 아니라, 상대가 받은 영향을 인지함
    3. 실질적 행동 변화 동반
          + 사과만 반복하며 같은 실수를 되풀이하면 변명이 됨
          + 구체적 변화 없으면 진정성이 퇴색함
    4. 신뢰 회복에는 시간 필요
          + 한 번의 대화가 완전한 신뢰 회복은 아님
          + 지속적인 행동의 변화로 쌓아야 함

‘수정(Repair)’의 장점과 관리자의 성장

     * 수정이 자연스러워지면 관리자로서 결정, 대화, 위험 감수 모두에 자신감이 생김
     * 완벽주의에서 벗어나, 실수를 성장과 관계 강화의 기회로 삼게 됨

주의 사항

     * 무책임함이나 반복적 실수, 또는 ‘수정’을 변명거리로 악용해서는 안 됨
     * 관리자는 완벽함이 아니라, 실질적 가치를 제공하고, 팀과 함께 성장하며, 최적의 업무 환경을 조성하는 역할 수행이 중요함

결론

     * 때때로 실패나 실수가 불가피함을 인정해야 함
     * 실수를 투명하게 인정하고, 교훈을 얻어 개선하면서 계속 나아가는 태도가 좋은 관리자의 핵심임

        Hacker News 의견

     * 몇 년 전에 Hacker News에 ""Mochary Method""란 이름의 구글 문서 세트가 공유된 적이 있었음, 이 자료는 이런 식의 매니지먼트 스킬을 매우 인간적인 시선으로 다루고 있어서 비매니저인 나에게도 정말 와닿았음, 북마크해서 자주 참고하고 있음
          + 실수에 대응하는 법 문서가 있음
          + Matt가 좀 더 상세하게 설명하는 동영상도 있음
          + 해당 기사와 비슷한 접근 방식이지만, 필요하면 ""rewind and redo""를 적극적으로 권장함
          + 전체 커리큘럼 링크도 공유함
          + 혹시 그 모든 구글 문서들의 링크를 가지고 있으면 공유 가능한지 궁금함, 대부분의 내용이 상식일 수도 있지만, 글로 정리된 것을 보는 것만으로도 멈춰서 생각하게 되고 더 나아질 수 있다고 생각함
     * 내 경험상 IC(Individual Contributor)와 매니저의 가장 큰 차이는 '책임감'임
          + 미안하다고 말하는 것은 쉽지만 진짜 좋은 매니저가 되기 위해 내가 최선을 다하는 부분은 나 자신에게 책임감을 갖는 것임
          + 구체적으로 1) 무슨 이슈가 발생했는지 정확히 파악하고(특히 내가 원인일 때), 2) 재발 방지 시스템을 마련하는 것임
          + 직원들은 매니저가 책임감이 부족하면 정말 확실히 느끼게 됨. 특히 중간 이상, 상위 매니지는 실제로 업무 생산성과 인생의 질 전부를 향상시키고 싶으면, 말로만 잘못했다고 끝내지 말고 본인의 리소스를 걸어서 동일한 문제가 재발하지 않게 시스템을 개선해야 함
          + 보통 이건 무리하게 업무를 더 주지 않고, 우선순위를 잘 조정하거나 필요시 ""노""를 더 잘하는 것임
          + 2번에 대체로 동의하지만, 시스템을 너무 많이 도입해서 오히려 아무 일도 못 하게 될 수 있으니 균형이 가장 중요한 것임
          + 당신이 말한 것과 더불어, 나쁜 매니저의 최악의 형태는 실수를 바로잡는다면서 본인을 보호하려 애쓰는 유형임. 이런 매니저는 지나치게 신경질적이고 갈등을 못 견뎌서, 개별 실수나 문제 상황마다 임의의 시스템을 만들어 회피함
               o 이런 유형은 시스템 자체를 재평가하지 않고 자기 자아 방어를 위해 시스템을 둠
               o 예를 들어 “이 직원은 내가 별로 마음에 안 들어서 부족한 것처럼 느껴지고, 어차피 시스템도 있으니!”처럼 생각함
               o 적응에 어려움을 느낌. 충분히 위임하려면 신뢰가 필요하지만, 실무를 직접 다 하지 않게 하려면 타인을 신뢰해야 하는데, 이를 못 함
               o 이런 사람은 커뮤니케이션 덕분에 승진했다고 착각하지만 실은 성장할 여지가 있다는 사실을 스스로 깨닫지 못함
          + 실수를 인정하는 매니저가 승진도 그만큼 자주 할 수 있을지, 아니면 적당히 숨기는 게 더 승진에 유리할지 궁금함
          + 좋은 시스템적 사고와 지속적으로 성장하고자 하는 동기나 인센티브가 결합되면 진짜 좋은 매니지먼트가 나온다고 생각함
          + 누군가에게 신뢰가 가는 이유는 자신과 자신의 행동에 책임을 지되, 타인이나 타인의 행동까지 책임지려 들지 않는 경계가 확실한 것임, 매니저, 직속 부하, 동료 모두에게 이 한가지만 바라는 마음임
     * 이 내용은 엔지니어에게도 똑같이 적용된다고 생각함
          + 나에게 senior developer에게 가장 바라는 핵심 스킬은 '겸손함'임, 경력이 아니라 태도의 문제임
          + 실수를 부정하거나 독선적인 시니어는 팀을 독으로 물들임, 반면 주니어 의견도 듣고 피드백을 받아들이는 시니어는 모두가 신뢰하고 따른다고 생각함
          + 우리는 모두 인간임. 심지어 LLM조차도 실수함
          + 내가 면접에서 제일 주의 깊게 보는 건, 지원자가 ""잘 모르겠습니다""라고 말하는 순간임
               o “모른다고 말 못 하는” 사람과 같이 일한 경험이 별로였음
          + 정말 공감됨, 많은 사람이 좋은 매니지먼트 스킬과 뛰어난 시니어 엔지니어 스킬이 상당 부분 겹친다는 걸 잘 모름
               o 방향 제시, 힘든 피드백 주고받기, 동료 성장시키기, 스트레스 대화에 현명하게 임하기 같은 걸 말함
               o 이런 스킬 없이 ""기술 하나만 믿고"" 실력주의만 추구하는 엔지니어는 커리어 한계가 명확함
               o 팀 내에 기술적으로 훌륭한데도 이 점 때문에 내보낸 인원이 있었음
               o 기술 업계에서 정말 아무에게도 안 섞이고 혼자 코딩만 하며 성장하는 포지션은 매우 드물고 특히 위로 갈수록 더 어려움
          + 지금까지 ""이건 선생님에게도 해당!"", ""부모에게도 해당!"", ""엔지니어에게도 해당!"" 의견이 나왔는데 결국 인간이면 누구에게나 해당되는 이야기임
          + LLM도 실수 많이 함, 현재 다른 창에서 LLM이 내 사이트 CSS 고치려고 계속 반복 작업하고 있음
          + LLM은 대놓고 자신만만하게 헛소리를 많이 하므로 주의해야 함
               o 하지만 사회적으로는 겸손이 오히려 불이익을 받고, 자신감은 보상받는 구조임
               o 그래서 겸손해지려 하지 않는 사람이 많은 건 본인에게 불리하게 작용할 걸 알기 때문임
     * 전직 교사 겸 코치로서, 아이들과 신뢰 쌓을 때 꼭 이 방식을 썼었음
          + 너무 자주 '내가 완벽한 리더, 너희는 불완전한 학생' 식 관계로만 접근하는데, 그런 분위기로는 진짜 교감이나 개성 존중이 어려움
          + 난 항상 다 같이 인간이고, 실수도 하고 도전도 하면서 롤모델과 신뢰가 함께 성장한다고 믿어왔음
     * 팀원(Implementer)은 아기가 아니고 매니저는 부모가 아님
          + 진짜 중요한 매니지먼트 역량은 매니저가 본인이 팀의 일부임을 인식하고, 사과 방법보다 팀의 목표를 이루는 데 집중하는 것임
          + 기사에서 말하듯 “진짜 일하는 소프트웨어를 배포하고, 팀 성장을 돕고, 모두가 최선을 낼 수 있는 환경을 만드는 것”이 진짜 매니저 역할임
          + 매니저가 자신의 인간미를 강조하며 감성적으로 사과를 하든 아니든, 반복적으로 똑같은 실수를 계속하면서 내 일에 방해만 안 한다면 진짜 별 상관없음
          + 인용한 책도 틀린 건 아니지만 좀 좁은 시각임, 관계에서 정말 중요한 건 rupture(균열)이 아닌 repair(회복)임
               o work 관계도 결국 인간관계라는 게 보편적인 상식이라고 생각함
               o 만약 당신이 감정적으로 일에서 완전히 분리된 소수라 해도, 대부분 사람은 인간관계를 직장에 끌고 들어옴. 이를 묵살하는 건 나쁜 매니저가 되는 1단계임
               o 그렇다고 해서 적절한 경계선, 피드백 없이 다 포용하라는 건 아님
               o 뛰어난 매니저는 각자 직원의 개인적 니즈에 맞게 접근법을 바꾸고, 본인이 위 글을 진심으로 믿는 직원이라면 딱 할 말만 하고 결과 중심적으로 일함
               o 개인적 경험으로, ‘감정은 나쁘다’는 관념에 익숙해진 사람이 가장 관리하기 힘들었음. 마음엔 무덤덤하다가도 결정적으로 감정이 건드려지면 갑작스러운 행동이 생김
               o 내가 선호하는 타입은 감정 표현을 적절히 해주는 사람임. 실수했다거나 피드백에 감정이 생기면 바로 말해주는 직원이 오히려 더 쉽게 소통, 공감하며 피드백과 한계를 동시에 관리할 수 있음
          + “관리자가 자기 잘못을 드라마틱한 방식으로 사과하든 말든, 반복되지 않게만 해주면 됨”이라는 당신 시각에는 공감하지 않을 수 있지만, 관리자가 자신의 기준만을 그대로 적용하면 오히려 문제가 됨
               o 관리자가 ""이 직원은 신경 안 쓴다고 해도, 또 다른 사람은 전혀 다르게 받아들일 수 있다""는 걸 반드시 기억해야 함
               o 매니저가 그런 점을 간과하면 더 큰 실수를 하게 됨
     * 내게 최고의 매니저는 임원진으로부터 팀을 보호하는 능력이 가장 뛰어났음
          + 엉뚱한 순간에 데드라인을 갑자기 줄이라는 요구나 이상한 신규 기능 요청 등 말도 안 되는 요청을 임원진에게 단호히 ""NO"" 했었음
          + 인수합병 당시에도 내 이름을 높여줘서 감원 명단에서 살아남음
          + 본인 연차가 많고 승진에 관심 없어서, 나쁜 매니저들이 팀을 버리고 본인만 살려는 모습과 확실히 달랐음
     * 내가 인정하거나 고치기 제일 힘든 실수는 채용 실수임
          + ""이 사람은 꼭 잘 해줄 수 있다""는 신념으로 오래 버텨봤는데, 팀이 커지다 보니 문화적으로 안 맞는 사람 하나가 비용이 너무 커짐
          + 예전엔 기회를 충분히 주다가 결국 PIP(Performance Improvement Plan)로 전환했는데, 너무 오래 끌면 팀 전체가 더 힘들어지는 걸 깨닫고 요즘은 더 빠르게 절차를 밟음
          + 직원의 역량 부족보다 기술적 실수를 인정하는 게 훨씬 쉬움, 특히 사람이 착할수록 더 힘듦
          + 상대가 힘든 캐릭터일 땐 조금 더 수월하지만, 어쨌든 직원 문제는 항상 어려운 숙제임
          + 나도 한때는 모든 사람을 코칭해서 퍼포먼스를 끌어올릴 수 있다고 믿었음
               o 하지만 경험상, 팀원 모두가 서로 믿고 잘 지내야 생산성이 몇 배 오름
               o 단 한 명의 어울리지 않는 사람만 있어도 팀 전체에 엄청난 부정적 영향이 있음
     * 이 조언이 정말 좋다고 생각함
          + 내면 성찰이 있고, 솔직히 더 나아지고 싶은 매니저라면 큰 도움이 될 것임
          + 하지만 그런 매니저는 극히 드묾
          + 대부분 자신의 편견과 약점을 스스로 인지 못하고, 진정한 의미의 실적 리뷰나 생산성 평가도 없으며, 사실상 매니저 역할이 주주 권한의 대행자 역할로만 규정된 경우가 대다수임
          + 팀을 위해 진짜 잘하고 싶어서 노력하는 엔지니어링 매니저가 있다면 진심으로 박수 보냄, 찾아보기 힘듦
          + 하지만 IC(개별 실무자)의 입장에서는 나쁜 매니저 밑에서 벗어날 방법은 직장이나 팀을 옮기는 것뿐임
          + 동의하지만, 그마저도 쉬운 선택은 아님
     * 좋은 매니저와 ""자리를 지키는"" 매니저는 다른 존재라는 게 문제라고 생각함
          + 인센티브가 전자가 아니라 후자에 더 강하게 맞춰져 있음
               o 요즘 나도 그런 생각이 더 강하게 드는 중임. ""인센티브가 뭔지 알려주면 결과가 뭔지 보여줄 수 있다""는 말이 점점 진리처럼 느껴짐
               o 직장, 생계 등 '진짜 중요한 순간'이 오면 인센티브가 거의 전부를 결정함, 개개인의 의지는 그 틈새에 적은 정도라고 느낌
          + 예전에 한 매니저가 자기 손으로 뽑은 모든 사람을 해고함, 나는 남은 인원 중 하나였는데 동료를 감쌌다고 나도 잘림, 옆 사람은 강등 됨
               o 결국 회사 상황이 안 좋아지자 그 매니저 포함 많은 매니저가 해고당함
               o 내 친구는 “이제 여기 나가면 프로그래밍 업계 영영 떠난다”고 할 정도였음, 너무 아쉬운 일이라고 생각함
          + ""great flattening"" 얘기 많이 하지만, 정작 management layer는 layoff에서 유독 면역력이 강한 것 같음
               o 대기업은 매니지먼트 레이어를 끝없이 원함, CEO들이 관리직 하나도 항상 반기더라는 게 내 체감임
     * 지난 50년간 미국 문화의 가장 큰 변화는 ""잘못되는 것""보다 더 최악은 없다는 정서임
          + 이 현상은 공적 영역이든 회사든 일관적으로 나타나는 걸 봄
          + 어떻게 해결해야 할지 모르겠지만, 사람들에게 ""미안하다""거나 보상할 기회를 허용하지 않는 것, 그리고 그것을 안 하는 사람들에게 책임을 묻지 않는 게 사회 문제의 큰 두 축이라고 봄
"
"https://news.hada.io/topic?id=22669","Nostalgist.js — 웹 브라우저에서 NES, Sega Genesis 등 레트로 콘솔 에뮬레이터 실행","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Nostalgist.js — 웹 브라우저에서 NES, Sega Genesis 등 레트로 콘솔 에뮬레이터 실행

     * 브라우저에서 NES, Sega Genesis 같은 레트로 콘솔 에뮬레이터를 실행할 수 있는 JavaScript 라이브러리
     * RetroArch 기반으로, 단순한 코드 몇 줄로 게임 실행·저장·불러오기를 손쉽게 구현 가능
     * RetroArch 설정 및 코어 옵션을 직접 제어하거나, Emscripten 저수준 API에 접근해 파일 시스템을 다룰 수 있음
     * 공식 RetroArch 웹 플레이어나 WebRetro보다 프로그래밍적 제어가 간단해, 개발자가 브라우저 앱에서 에뮬레이터 기능을 쉽게 통합 가능
     * 오픈소스(MIT)로 제공, pirated ROM/BIOS는 포함하지 않고 데모용 공개 ROM만 지원
"
"https://news.hada.io/topic?id=22714","Claude Code를 최고의 설계 파트너로 만들기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Claude Code를 최고의 설계 파트너로 만들기

     * 처음 Claude Code를 사용할 때 단순히 프롬프트 지시와 수정 반복 방식으로 접근했지만, 복잡한 작업에서는 대화 기록 의존성과 컨텍스트 한계 문제를 겪음
     * 이를 해결하기 위해 기능 구현 전에 계획 문서(plan document)를 작성하게 하고, 이를 새로운 세션의 단일 진실의 원천(SSOT) 으로 삼음
     * 계획 문서는 요구사항 재정리, 구현 세부 설명, 코드 품질 확인 명령어 등을 포함하며, 구현 중에도 살아있는 문서(living document) 로 지속 업데이트됨
     * 이렇게 하면 맥락 손실 문제가 해결되고, 새로운 세션에서도 단일 문서만으로 프로젝트를 이어갈 수 있음
     * 결과적으로 AI는 단순한 실행 도구가 아니라, 개발자가 설계를 더 깊이 고민하고 기록하도록 유도하는 협력적 디자인 파트너 역할을 하게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제의식: 단순 대화 방식의 한계

     * Claude Code와 대화형으로 작업을 진행할 때, 간단한 작업에는 적합하지만 복잡한 작업이 커질수록 여러 가지 중대한 한계가 발생함
          + 대화가 유일한 진실의 원천이 되어, 새로운 메시지가 이전 지시를 쉽게 덮어쓸 수 있으며, 그 순간을 명확히 인지하기 어려움
          + AI의 컨텍스트 크기 한계로 인해, 대화가 길어질수록 앞선 정보가 누락될 수 있음
          + Claude Code가 대화 압축 기능을 갖추고 있지만 이 한계를 완전히 해소하지는 못함

플랜 문서 중심 방식 실험

     * 이런 문제 해결을 위해 플랜 문서 기반 접근을 시도함
          + 시작 시, Claude Code에게 구현할 기능이나 수정해야 할 버그 등에 대해 가능한 상세하게 설명함
          + 참조할 수 있는 기존 소스 파일이나 이전에 작성된 플랜 문서도 언급함
          + 지나치게 구체적인 구현 지시는 피하며, AI의 설계 제안 역할을 유도함
     * 플랜 문서가 충분히 만족스러우면 대화 기록을 지우고 해당 플랜만 맥락으로 새로 시작함
          + 플랜에는 기능 요약, 구현 계획, 코드 및 의사코드, 타입/린트/테스트 명령 등이 포함되어 있음

협업적 설계 프로세스

     * AI가 제안한 설계가 마음에 들지 않을 때, 구체적 피드백을 제공해 수정된 접근법을 유도함
     * 논의 과정에서 AI의 첫 제안이 더 적합했다는 점을 깨닫기도 하며, 자체 설계만으로 코딩을 진행할 때보다 더 효율적임
     * 체계적인 대화는 동료 개발자와 플랜을 논의하는 것과 유사한 경험 제공
     * AI는 단독으로 전혀 다른 접근법을 제시하진 않으나, 질의하면 다른 대체안을 제안할 수도 있음

살아있는 문서(Living Document) 방식

     * 플랜 문서를 한 번 작성하고 끝내지 않고, 기능 구현 도중에도 계속 갱신하게 함
          + 구현과정이나 타입 체크, 린트, 테스트 과정에서 드러나는 변경사항을 실시간 반영함
     * 코드 커밋할 때마다 플랜의 최신 상태 점검을 요청하는 습관 형성
     * 항상 최신의 플랜이 유지됨에 따라, 새 대화 세션에서도 플랜만 첨부하면 맥락 손실 없이 그대로 이어나갈 수 있음

코드 리뷰 및 개발 습관 변화

     * 구현이 시작된 후에는 주기적으로 변경사항을 확인하고, 만족스러우면 AI의 작업을 더 신뢰하기도 함
     * 최종 코드 검토 시 업데이트된 플랜 문서가 기술적 의사결정 근거를 파악하는 데 도움이 됨
     * 사전에 치밀하게 플랜을 세워 문서화함으로써 더 나은 개발자로 성장하는 경험을 얻음
          + AI에게 설명해야 하므로 자신의 의사결정 과정을 명확하게 정리하게 됨

혼돈에서 체계로

     * 이 방식은 플랜 문서가 진실의 단일 원천이 되게 하고, 맥락 손실 문제를 해소하며, 아키텍처적 사고를 촉진함
     * 플랜 문서는 사양과 구현 로그를 모두 포함하며, ‘무엇’뿐만 아니라 ‘왜’, ‘어떻게’까지 기록함
     * 끝 결과는 계획적이고, 문서화가 잘 되어 있으며, 신뢰성 높은 개발 프로세스임
     * AI는 단순한 구현자가 아니라 협업 설계 파트너로 자리매김함

   개발자의 워크 플로우에서 PM과 아키텍트 역할의 비중이 더 커지는듯 합니다.

        Hacker News 의견

     * 지난 2주 동안 저녁 시간마다 claude code를 이용해 프로젝트를 한 번에 완성할 수 있는 ""완벽한 프롬프트""를 만드느라 엄청 집중했음. 결국엔 CLAUDE.md 하나에 프로젝트 아키텍처, 모델 명세, 빌드 순서, 테스트 계층, 시나리오 등 8개의 다른 마크다운 파일을 참조하도록 구조를 짰음. 이 프로젝트는 Databricks Unity Catalog의 모델 기반 거버넌스 용인데, 관련 경험은 많지만 기존 툴이 충분히 유연하다고 느끼지 못함. 최종적으로 Databricks 전문가, Pydantic 전문가, 프롬프트 전문가 등 3개의 서브 에이전트로 실제 기획 파일 작업을 지원받음. 이 덕분에 마크다운 파일 품질이 예전 Pydantic 버전 문제부터 Unity Catalog에 대한 내 오해까지 여러 부분에서 크게 개선됨. 어제는 실제로 실행해 봤는데, 내가 몇 번 도구 사용을 승인해준 것 빼고 2시간 만에 대부분의 도구와 테스트가
       완성됨. 이 방식이 예전과 너무 달라서 신기했는데, 기술적인 문서 작성을 꼼꼼히 하고 모두가 한 방향을 바라보게 하는 데 진짜 미래가 있다고 느낌. 코드를 직접 파는 것보다 생산성이 낫다고도 생각함. 다만 코드 작업 땐 몰입도가 높았지만, 여러 마크다운 문서를 다루면 집중력이 더 쉽게 흐트러지는 단점이 있음. 정말 흥미로운 시대임
          + Test-Driven Development처럼 시스템을 미리 설계하게 만드는 패턴이 새롭게 부상하는 게 느껴짐. 예전에는 코드를 만들면서 시스템을 점진적으로 그려 나갔다면, 이번처럼 AI 기반 개발은 영역을 앞서 설계하고 맵핑하게 해서 실제 코드는 단순히 설계를 실현하는 보일러플레이트 작업처럼 됨. AI는 이런 보일러플레이트에 정말 강함
          + 나도 똑같이 더 생산적인데 집중력은 더 흩어지는 현상이 고민임. 뭔가 이상하지만 당장엔 효과적임. 장기적으론 해법을 찾아야 할 듯. 지금 가장 잘 맞는 방법은 여러 에이전트를 프로젝트의 여러 리포지토리에서 각각 다른 태스크를 하게 하면서 계속 승인 작업을 하는 것임. 일종의 대규모 팀을 이끄는 프로젝트 매니저 느낌임. 역시나 흥미로운 시대임
          + 정말 신선한 방식임. 실제 실험에서 에이전트가 돌아가는 프레임워크가 뭔지 궁금함
          + 요즘 나도 제품 디테일, 사용자 여정 등을 음성으로 기록하고, 그걸로 기술 문서화 프로세스를 시작함. 최소한의 CLAUDE.md, 소프트웨어 개발은 Github 워크플로 활용, 근데 좋은 CI 워크플로 만드는 게 힘듦. 내 플레이북은 https://nocodo.com/playbook/임
     * “계획 먼저 세우면 결과가 좋아진다”는 주장에 와닿지 않음. 난 예전부터 큰 기능이나 프로젝트는 당연히 머릿속에서든, 종이든, Confluence든, 화이트보드든 미리 구조와 이유, 방법을 생각했음. 소프트웨어 엔지니어링의 80%는 ‘뭘, 왜, 어떻게’ 할지 고민·확정하는 과정임. 이해관계자와 아이디어·목적을 확인하고, 리서치도 함. 마지막 20%만이 실제 코딩임. 예전부터 그래왔고, 제대로 된 기획이나 목표 정의에 AI가 꼭 필요한지 모르겠음
          + 큰 팀이나 문화가 자리잡은 환경에서는 그럴 수 있지만, 상당수 개발은 개인 프로젝트, 소규모 팀, 사이드 프로젝트, 빠른 POC, 일회성 자동화 툴 등에 집중됨. 이런 작업은 처음부터 문서/명세/테스트로 시작하지 않고, 코드와 고민·구현 과정을 섞어가며 진행함. TDD가 적합한 곳도 있지만 굳이 필요 없는 경우도 많음. 그런데 AI 코딩 에이전트 도입 뒤로는, 아이디어를 명확히 정의하고 명세로 정리하는 과정이 훨씬 중요해짐. 내 머릿속에서 떠오르는 모든 걸 말로 풀어내는 게 그만큼 필수임. 요즘 가장 핫한 프로그래밍 언어는 영어임
          + 난 개발과 설계를 혼합해서 진행하는 편임. 일단 코딩을 시작해서 계속 다듬고 발전시키는 식임. 대부분의 경우엔 대략적인 해결 방법이 뻔해서 시간 들여 깊게 계획할 필요 없다고 느낌
     * 예전엔 프롬프트 엔지니어링이 농담거리였는데, 이제는 진짜 체감 중임. 클라우드 코드를 체계적으로 활용하려고 10~20분씩 꼼꼼한 프롬프트와 초기 기획에 매달릴 때도 있음. 나도 OP와 비슷하게 플랜을 파일로 저장하고, 새 컨텍스트에서 실행함. 정말 바라는 건 좋은 CLI임(현재 charm, cc 사용 중). 각각 구현, 플랜, 서브에이전트별 모델을 따로 쓸 수 있으면 최고일 듯. 로컬 모델로 구현하고 클라우드 모델로 플랜 짜거나 필요 시 스와핑해서 돈 아끼는 구조. Charm이 지금까지 써본 것 중에 컨텍스트 손실 없이 왔다갔다할 수 있게 잘 되어 있음. 병렬 서브에이전트 기능도 claudecode의 최고의 기능 중 하나임. (CCR은 시도해봤지만 기본 모델 이상 안 돼서 실망했음)
          + 프롬프트 엔지니어링이 지금 이슈가 된 건, 트위터 핫테이크나 콘텐츠 생산용 이슈 때문임. 하지만 실제로 프롬프트 엔지니어링은 옛날부터 중요했음. GIGO(“Garbage In, Garbage Out”, 쓰레기를 넣으면 결과도 쓰레기)는 모든 머신러닝 프로젝트에서 진리임. 그래서 동료나 친구들한테 “직접 써봐야 된다”고 계속 추천함. 6개월 전엔 안 되던 게 지금은 될 수도 있음. 실제로 손에 익혀야 뭐가 바뀌었는지, 뭐가 되는지 알 수 있음. 나는 네거티브 대신 실제 성공 사례나 블로그, gist, 예시들이 훨씬 더 가치 있다고 생각함. 아주 단순한 계산이나 오타 찾기가 아니라, 내 워크플로를 개선하고 도와줄 수 있어야 필요함. 프롬프트 엔지니어링은 10~15년 전 구글 실력 몰입해서, 계속 새로 나온 변화와 효과적인 패턴을 익히는 것과 같음
          + AI와 협업하려면 프롬프트 엔지니어링이 진짜 핵심임
          + AI를 쓰는 프로젝트가 내가 가장 문서화·테스트가 잘 된 프로젝트였음. LLM에게 성능을 끌어내려면 반드시 컨텍스트가 필요하니 문서화가 잘 되고, 테스트 생산 비용이 낮아졌으니 테스트가 더 많아짐. 오히려 코드 품질이 떨어진다는 예측과 달리, 더 좋아질 것임
          + 솔직히 ""프롬프트 엔지니어링""이란 용어는 새로운 미디어를 이용한 아키텍처 설계임. 과거에는 ""다이어그램 설계""라는 스킬이 각광받았듯, 지금은 새로운 방식의 아키텍처링임
     * 최근 Claude Code를 잠깐 써봤는데, 추천받은 워크플로 해볼 예정임. 꽤 괜찮은 방식 같음. 그런데 CC 이용료가 생각보다 비쌈. 간단한 리팩토링에 5분 작업+15분 리뷰만 했는데 4달러나 들었음. 내가 직접 했다면 15~20분이면 끝났을 일임. 보통 기능 하나에 CC로 얼마 정도 쓰는지 궁금함. 아무도 가격 얘기를 안 함
          + 구독하면 $20~$200 플랫 차지로 일·주간 토큰 사용량 제한이 있음. https://support.anthropic.com/en/articles/…
          + AI 투자자들이 바라는 방향성은 노동 시장을 15% 마진으로 AI가 대체하는 구조임. 1:1로 개발자와 AI 예산이 같은 시대가 올 것임. 예를 들어 연봉 10만 불 짜리 시니어 개발자 한 명 몫이 10만 불 AI 예산으로 대체됨. AI 예산이 개발비에서 빠져나갈 것이고, 시니어 연봉은 내려갈 가능성이 높으며 개발팀 규모가 급격히 줄어들 수 있음. 지금은 VC가 전액 보조하는 '영토 확보' 단계인데, 트위터 VC 분위기 보면 이 단계가 곧 끝나갈 것 같음. 9개월 운영비에 5억 불을 몇 번이고 더 끌어오는 것도 한계에 다다름
          + Cursor에서 Claude Code로 일부 옮긴 이후로 비용이 많이 늘었음. 회사에서 주로 써서 상사 설득은 쉬웠지만, 사이드 프로젝트에서는 고민임. 재미 삼아 앱 부트스트랩할 때마다 20달러 내고 싶지는 않음
          + Sonnet(20유로/월), Opus(100유로/월) 두 모델을 선택해서 구독할 수 있음. 난 Sonnet 썼다가 나중에 Opus로 바꿈. Sonnet도 충분히 쓸만함. 토큰 한도 내에서는 내 용도로는 부족하지 않게 사용 중임. 다만 앞으로는 어떨지 확신 못함
          + ""내가 직접 했어도 15~20분이면 됨""이라고 했는데, 그 15~20분을 다른 작업에 쓸 수 있지 않을까 생각해봄
     * Visual Studio Code/ChatGPT5 프리뷰 조합을 사용하는 방식이 내 워크플로와 비슷함{아마 Github Copilot 구독으로 결제 중이지만 요즘은 확신이 없음}. 비에이전트 LLM은 코드가 빨리 망가지는 경향이 심하다고 느낌. 에이전트 모드를 쓰면서 코드 구축이 확 달라짐. 난 파이썬 개발자는 아니지만, LLM이 새 코드 더미를 짜주는 걸 보고 실제로 꽤 인상 받았음. 완성한 뒤엔 BitGrid에서 작은 LLM을 돌리고, 뒤늦게 코드를 완전히 이해하는 식으로 가려고 함. 작은 탐색 단계를 반복하고, 전체 설계를 내가 원하는 대로 유지하기 위해 일부 수정만 하는 구조임. LLM이 프로그래밍 파트너가 될 미래에 확신을 갖게 됨. Visual Studio Code/ChatGPT5 조합 쓰는 다른 사람도 있는지 궁금함
     * AI 도구를 최적화하려다 보니, 개발자들이 '좋은 커뮤니케이션'과 '기대치 설정'의 가치를 재발견하는 것 같아서 흥미로움. 지금까지의 10x 개발자 이미지, 즉 이방인 혹은 괴짜 스타일은 재고할 때임
     * Replit에서 비슷한 경험을 갖고 있음. 앱 크기가 커지면 설계 문서가 태스크와 진실의 소스가 되지 않으면 금방 무너짐. OpenAI는 챗이 느려지고 금방 먹통이 돼서, 문서를 따로 만들어 새 챗에 임포트하는 게 도움이 됨. 인간 차원에서도 우리 스스로도 그래야 한다고 느꼈음. 스스로 회고하며 문서화하고 ‘메모리’를 디자인 문서에 기록함으로써, 우리와 LLM 모두 자유로워질 수 있음
     * 나도 최근 이 워크플로를 발견하고 매우 놀랐음. 중요한 건 claude에 최소한의 요구만 제공하고, 플랜 모드를 자유롭게 돌게 하는 것임. 만약 세일즈 지표 리포트라면 ""Ultrathink relevant sales metrics""라고만 해도 다양한 지표를 알려주고, 순위를 매겨 추려볼 수 있게 함. 새로운 기능마다 디렉토리를 따로 만들고, 그 기능의 플랜을 파일로 작성하게 함. 이후엔 구현 계획, 필요한 데이터 쿼리, 실제 구현, 테스트, 유저 문서 작성까지 단계별로 시키고, QA로 보냄. 예전에는 세일즈 예측 기능 하나 만드는데 대규모 팀과 시간이 필요했는데, claude가 한나절에 도커 컨테이너로 구현함. 이 변화로 소프트웨어에 대한 내 관점이 바뀌는 중임. 예전엔 NDA, IP 등으로 회사들이 소스코드를 외부에 절대 내놓지 않았음. 그런데 이젠 20년 걸린 복잡한 ERP 시스템도 claude가 빠르게
       재구현하고 문서, 테스트까지 첨부함. 현실적으로는 막 완벽하진 않지만, 진도가 정말 빠름
     * Claude Code에서 좋은 피처를 뽑으려면 플랜을 먼저 제대로 작성하는 게 핵심임. 나는 최근 Cursor에서 GPT-5 High를 써서 플랜을 짜고, 그걸 Claude Code에 넣어 구현함. 코드베이스에서 바꿀 부분들을 미리 문서화하게 하면 추가로 15~20% 더 좋은 결과를 얻을 수 있음. 플랜 모델이 작동 방식, 아키텍처, 패턴을 문서화하고, 기능 설계까지 여기에 녹이면 결과물이 더 나음. 마지막으로, 반드시 문서와 플랜을 직접 리뷰·수정하는 것도 매우 큰 도움이 됨
          + 회사에서는 Google Workspace를 써서 Gemini가 “학술 스타일” 글쓰기에 아주 강력하지만, CC에 비해 코드 작성에는 약함. 그래서 Gemini에게 플랜을 충분히 구체적으로 정리하게 하고, 이걸 CC로 넘겨 내 코드베이스에서 수정·구현하게 함. 이 방식으로 새로운 기능 개발이나 기존 기능 확장에 놀랄만한 효과를 봄. 8주 만에 직접 만든 프로덕트가 현재 실제 프로덕션에서 고객 데모까지 진행 중임. CC의 경험과 결과물에 매우 흡족함. 예전에 HN에서도 밝혔듯, 우리 팀 내 인력으로도 상당 부분 할 수 있었지만 프론트엔드 작업은 엄두조차 못 냈음. 1년도 넘게 걸렸을 일, 더 많은 엔지니어와 데이터 사이언티스트의 노력이 필요했던 일이 대부분 2달 만에 완성됨. 기능 추가가 시간이 아니라 거의 ‘몇 초’ 만에 가능함. CC 덕분에 나는 내 경험이 여러 사람과
            공유되고 있어 흥미로움
     * 이런 프로세스에 프론트엔드 디자인을 우아하게 끼워넣는 좋은 방법을 발견한 사람이 있는지 궁금함. 대부분은 프론트엔드 프레임워크 언급, 혹은 figma 이미지 수준에 그침. 그걸로는 통합적 디자인 솔루션이라는 느낌이 안 듦
"
"https://news.hada.io/topic?id=22706","코딩 에이전트 만드는 법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             코딩 에이전트 만드는 법

     * 2025년 현재 코딩 에이전트를 직접 만드는 것은 개인 개발자가 해볼수 있는 가장 좋은 프로젝트중 하나
     * 에이전트는 300줄 코드와 LLM 토큰 루프만으로 동작하며, 이를 만들어 봄으로써 소비자가 아닌 AI 생산자로 전환할 기회를 제공
     * 기본 구성 요소는 파일 읽기, 파일 목록, Bash 실행, 파일 편집, 코드 검색 같은 툴이며 이를 통해 실제 자동화 기능을 구현
     * 모델 선택에서는 Claude Sonnet, Kimi K2 같은 agentic 모델이 적합하며, 필요 시 GPT 같은 오라클 모델을 도구로 연결해 고차원적 검증을 수행함
     * 실제로 Amp, Cursor, Claude Code, GitHub Copilot 등 상용 제품들도 유사한 구조임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

워크숍 개요

     * Geoffrey Huntley가 진행한 무료 워크숍으로, 코딩 에이전트를 직접 만드는 방법과 원리를 실습 중심으로 안내
     * Roo code, Cline, Amp, Cursor, Windsurf, OpenCode 같은 기존 상용 AI 도우미들과 구조 및 원리를 비교하며, 직접 구현해 보는 기회 제공
     * 제작 경험을 통해 단순 AI사용자에서 벗어나 직접 AI를 활용해 자동화 도구를 제작하는 개발자로 성장할 수 있음
     * 핵심 구조는 300줄 가량의 코드에서 LLM 토큰을 루프 방식으로 활용해 에이전트 기능을 만드는 것
     * 각 도구별 프리미티브(읽기, 파일 목록, 실행, 편집, 코드 검색) 기능을 추가하며 실제 동작 예시 및 코드는 GitHub 저장소에 공개

에이전트란 무엇인가

     * 최근 ""에이전트""라는 용어가 광범위하게 사용되나, 실질적 의미와 내부 동작 원리는 명확하지 않음
     * 에이전트 제작의 진입장벽이 낮아짐에 따라, AI 소비자를 넘어 업무 자동화를 주도할 수 있는 생산자로 성장 가능함
     * 2025년 기준으로, 기본 데이터베이스 개념(Primary key)처럼 에이전트 제작 원리가 필수 지식으로 자리 잡음
     * Canva 등 기업들은 이미 면접 과정에서 AI 사용을 권장하고 있으며, AI 자동화 역량은 채용의 핵심 요소가 됨
     * 이제 뒤처지는 이유는 AI 때문이 아니라, 자기 계발을 통해 새로운 도구를 익히지 않는 것임

코딩 에이전트의 핵심 원리

     * 코딩 에이전트는 300줄 코드와 LLM 토큰 루프만으로 구성되며, 반복적인 토큰 입력을 통해 기능 수행
     * 동시 작업(concurrent work) 개념이 중요
          + 예: Zoom 회의 중에도 에이전트가 병렬로 작업을 진행할 수 있어 업무 효율이 크게 향상됨
     * 모든 LLM이 에이전틱한 것은 아님
          + '고안전' (예: Anthropic, OpenAI)
          + '저안전' (예: Grok)
          + '오라클' (요약·고차사고에 유리)
          + '에이전틱' (행동 편향, 빠른 반복·도구 호출)
     * 개발자는 모델별 특성을 이해하고 목적에 따라 활용 모델을 선택함
     * 무조건적인 컨텍스트 윈도우 할당은 성능 저하 요인이며, ""적게 할당할수록 결과가 좋음""을 유념해야 함
          + 과도한 MCP 툴 등록은 성능 저하로 이어짐
     * 규칙: “Less is more” → 필요한 만큼만 도구와 데이터를 맥락에 배치해야 최적 성능 확보

코딩 에이전트 구축 과정의 흐름

     * 1. 툴 등록 및 함수 호출
          + LLM에 예를 들어 날씨 조회 툴을 등록하고, LLM이 적합한 상황에 함수 호출 형식으로 대응할 수 있게 함
          + MCP(Model Context Protocol)는 ""함수에 대한 정보 배너""와 유사하며, 함수 설명만 등록해도 자동 호츨 가능함
     * 2. 프리미티브 툴별 핵심기능
          + 파일 읽기(ReadFile): 경로 전달시 파일 내용을 컨텍스트로 읽어옴
          + 파일 목록(ListFiles): 디렉터리 내 파일·폴더 리스트 제공
          + 명령어 실행(Bash): LLM이 시스템 셸 명령어 실행 및 결과 반환
          + 파일 편집(Edit): 지정 파일을 생성·수정하는 행위 자동화
          + 코드 검색(CodeSearch): 패턴이나 키워드, 함수명을 기준으로 코드베이스 전체를 신속 검색함 (ripgrep 활용)
     * 3. 예시 및 결과 흐름
          + 각 툴을 LLM에 통합하여 자연어 프로프트만으로 연속적 업무(예: FizzBuzz 코드 생성→실행 확인, 디렉터리 탐색→내용 분석 등) 자동화
          + 툴 함수는 사용자 입력이나 시나리오에 맞춰 순차적으로 호출 및 결과 반환을 루프 내에서 반복
          + 에이전트의 주요 동작 시퀀스: 사용자 입력→툴 호출 여부 판단→툴 실행→결과 컨텍스트에 할당→반복

확장 가능성과 오픈소스

     * 현재 대부분의 코딩 에이전트는 ripgrep 등 기존 오픈소스 도구를 기반으로 동작
     * GitHub에는 SST Open Code, mini-swe-agent 등 단 100줄로 구현된 간단하지만 강력한 에이전트 프로젝트들이 존재하므로 성능 및 구조 참고 가능
     * 개발자들은 기존 제품 비교 대신 직접 제작을 통해 원리를 이해하고 활용할 것을 권장
     * 실제 업무·자동화에 적용 시, 자체 에이전트 생성 및 조직 내 확산이 경쟁력으로 연결됨

결론과 시사점

     * 코딩 에이전트는 복잡한 기술이 아닌 간단한 루프 구조와 도구 조합으로 구성
     * 코딩 에이전트 제작의 핵심은 구조이해와 신속실행 능력이며, 직접 제작 경험을 통해 AI 기술 변화에 적극적으로 대응 가능
     * 중요한 것은 AI 그 자체보다 꾸준한 자기 계발과 도구 제작 역량 확보등의 자기 투자가 현시점에서 가장 중요한 개인 성장 전략임
     * “AI가 당신의 일을 빼앗는 것이 아니라, 당신의 동료가 에이전트로 무장해 자동화하며 더 빠르게 일하는 것이 위협”

     그림 한 장이 보통 1000단어의 가치라는데, 이 자료에선 그림들의 가치가 99.6% 할인된 느낌임 이게 뭔지 궁금함

   개웃긴데요 ㅋㅋㅋㅋㅋ 무슨 말인가 했는데 링크 들어가니까 이해가 쏙쏙

   블로그 다른 게시물 썸네일들도 슬롭이네요 절대 눌러보고 싶지 않게 생겼습니다

   ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ

        Hacker News 의견

     * 저희 Princeton SWE-bench 팀에서 약 100줄 코드로 SWE-bench에서 좋은 성과를 내는 에이전트를 만들었음, 관심 있으면 한 번 살펴보면 좋겠음 mini-swe-agent
          + 진짜로 꽤 간단한 구조임에 놀람, 이런 정보를 공유해줘서 고마움
            전체 코드는 사실 이 프롬프트들로 돌아감 에이전트 기본 프롬프트 소스코드
Your task: {{task}}. Please reply
with a single shell command in
triple backticks.

To finish, the first line of the
output of the shell command must be
'COMPLETE_TASK_AND_SUBMIT_FINAL_OUTPUT'.

          + 에이전트 샘플 프롬프트 중 “1. 코드베이스에서 관련 파일 찾기 및 읽기 2. 이슈 재현 스크립트 만들기 3. 이슈 수정 4. 스크립트로 수정 확인 5. 엣지 케이스 테스트” 부분이 유용함
            나도 디버깅 루프에서 이런 식의 프롬프트를 유사하게 사용함
            “코드베이스 분석 후 원인 후보를 리스트업하고, 가능성 높은 순으로 랭크하고, 스크립트나 디버그 로깅으로 가설을 검증” 방식이 나만의 문제 해결 루틴에 큰 도움이 됨
          + 문제가 하나의 파일에 자족적으로 있을 때 LLM을 이용해 수정하는 것은 매우 쉬움
            그러나 일반적인 코드베이스에서는 파일과 맥락이 여기저기 흩어져 있어서, 개발자의 구조화된 설계 의도와 조직화에 맞춰 파악하기가 쉽지 않음
          + 멋진 시도에 박수를 보냄, 다만 아쉬운 점은 도구가 많지 않다는 것임
            대부분의 코드는 에이전트 프레임워크에 해당되고 SWE만을 위한 특화 코드는 생각보다 많지 않음
            나도 재미로 SWE 에이전트를 만들어봤으니 autocode도 참고해보면 좋겠음
          + 고맙다는 의미에서 참고 자료에 추가함
     * Thorsten Ball이 작성한 매우 유사한 “에이전트 빌드 방법 가이드”가 AmpCode How To Guide에도 존재함
       전반적으로 Amp도 꽤 흥미로움
       이제는 더 이상 비밀스러운 서비스가 아니지만, 에이전트 코딩 관련 도구가 계속 공개되는 것이 반가움
       앞으로 다양한 소프트웨어에서 이러한 에이전트 모델이 기본적으로 포함될 것이라 생각함
          + 이게 훨씬 보기 좋아서 고마움을 느낌
          + 저자 본인이 Amp에서 근무한다는 내용도 언급됨
          + Ghuntley 또한 Amp에서 일함
     * 그림 한 장이 보통 1000단어의 가치라는데, 이 자료에선 그림들의 가치가 99.6% 할인된 느낌임
       이게 뭔지 궁금함
          + 컨퍼런스 워크숍용 슬라이드 자료임
            텍스트는 실제 발표에서의 워딩을 딕테이션한 것임
     * 혹시 누군가가 툴 활용 방식에 대해 확인해줄 수 있는지 궁금함
       Claude, ChatGPT 등에서 API로 ""툴""을 제공하며 툴 호출 요청이 들어오면 응답자로서 툴을 실제로 실행하고 결과를 다시 전달하는 형태로 이해하고 있음
       근데 실제로 모델은 엄밀히 말해 문자 기반이니, API에서 어떻게 모델 응답을 여러 구조로 바꾸는지 궁금함
       분명히 파인튜닝 과정에서 특정 툴 호출을 특수한 블록 형태로 넣은 예제를 통해 모델이 이해하고, Claude/ChatGPT 서버에서 그걸 해석한다는 과정이 있었을 거라 추정함
       이와 관련해 문서나 내부적으로 쓰는 특수 토큰에 대한 정보가 있는지, 또 유저 입력이 이 “의미 담당” 토큰을 악용하지 못하게 어떻게 막는지 궁금함
          + Anthropic에서 공개한 구현 문서가 있음
            Anthropic Tool Use Documentation
            여기서 모델은 사실 “텍스트”가 아니라 토큰 단위로 동작함을 명확히 알 수 있음
            컴파일러가 소스코드를 파싱해서 키워드, 괄호, 구조 등 “토큰” 시퀀스를 만든다는 점과 비슷함
            출력물에는 실제 단어와 함께 메타데이터도 포함될 수 있음
          + 개념적으로는 맞게 이해했음
            오직 LLM과의 진짜 인터페이스는 “토큰” 뿐이고, 제어와 데이터 채널이 분리되어 있지 않음
            모델 API 계층에서 툴 호출용 인스트럭션 및 사용 가능한 툴 목록을 프롬프트에 삽입, 각각에 대한 설명도 같이 제공
            툴 호출이 필요할 때 모델은 응답에 특수 블록(특수 토큰 포함, 툴 이름 및 파라미터 명시)을 삽입하면, API 계층이 이를 추출해 JSON 형태로 변환함
            툴 실행 결과도 특수 토큰으로 인코딩되어 삽입
            사용자 입력에서 자체적으로 이런 토큰을 주입하지 못하도록 API 계층이 막고 있음
            최신(SoTA) 모델들은 툴 호출에 대해 상당한 파인튜닝이 이루어졌음, 범용 툴 호출/특정 툴 케이스(예: Claude Sonnet 모델이 Claude Code 툴에 특화)에 대한 파인튜닝 모두 포함
            모든 게 잘 작동하는 게 신기할 정도임, 툴 호출에선 파인튜닝이 정말 핵심적인 역할임
            파인튜닝 없이도 동작은 가능하지만, 성공률이 크게 떨어짐
          + “툴 호출이 필요한 예시를 특수한 블록으로 반환하는 방식으로 파인튜닝했다”는 추측은 맞다고 생각함
            답을 잘 모를 때나 지시를 받으면 툴 호출 포맷으로 응답하도록 학습됨
            포맷을 따르는 툴 호출 예시(포맷 자체)와 일부 도구에 특화된 학습 둘 다 진행됨
            예를 들어 gpt-oss는 아무리 언급이 없어도 검색 도구를 쓰려는 경향
            Anthropic 문서엔 친숙한 도구(예: text_editor, bash) 목록도 따로 있고, 이 도구들은 사용법에 대한 깊은 의미까지 따로 학습됐을 확률이 높음
            실제론 구조가 꽤 깨지기 쉽고, “특수 토큰이나 토큰 시퀀스”라는 낮은 수준의 신호를 통해 이뤄짐
     * “토큰을 계속 루프에 던지면 에이전트가 생긴다”는 말에 “토큰”을 “돈”으로 바꿔서 보면 현실적인 풍자가 느껴짐
       결국 돈을 계속 태우면 에이전트가 생긴다는 셈임
          + 토큰이 전부 돈이라고 말하긴 어렵다고 생각함
            로컬 모델들도 계속 좋아지고 있음
            아직까지는 최고의 결과를 내려면 토큰(=돈)이 필요하지만 미래에는 달라질 가능성이 높음
     * 이렇게 이미지만 가득하면 너무 읽기 힘들어짐
       스크롤 시뮬레이터를 보는 것 같은 느낌임
     * bash 툴 말고 굳이 다른 툴이 왜 필요한지 궁금함
       파일 목록조회라든지, 레포 찾기 및 탐색, 파일 내용 편집 같은 건 bash만으로도 다 할 수 있지 않나
       아니면 위의 mini-swe-agent 예시에서 보여주는 케이스인지 궁금함
          + 기술적으로 봤을 때 bash 하나만으로도 충분히 다양한 작업이 가능하고, 실제로 그렇게 성공한 경험도 있음
            흥미로운 점은 도구를 제한할수록 에이전트가 더 창의적으로 접근한다는 점임
            하지만 학습된 다양한 도구를 제공하면, 모델이 각 도구의 사용법을 이미 잘 알고 있어서 토큰 사용량도 효율적이고 전반적으로 성공률도 높아짐
            Bash만 쓸 경우 bashism이나 인수 처리, 공백 처리 같은 부분에서 자주 헤매는 문제도 있음
          + 별도의 도구를 쓰는 게 bash 하나에 몰아넣는 것보다 훨씬 단순함
            만약 모든 것을 bash로 처리하면, 무조건 안전하게 실행 가능한 명령어(예: 파일 목록)는 바로 실행하고, 그 외 위험한 명령은 사용자 승인 받는 체계를 별도로 구현해야 함
            파일 목록조회를 별도 툴로 제공하면 프로젝트 디렉터리 외부 파일 노출도 막을 수 있음
          + 사실상 bash 툴과 Edit 툴만으로 코딩 에이전트 작동은 충분히 가능함 (Edit는 필수까진 아니지만 비효율성 커짐)
            다만 코드 검색 같은 부분에서 어려울 수 있음
            예를 들어 ripgrep을 bash에서 쓰게 프롬프트를 조정하는 식으로 커버도 가능할 듯함
          + 왜 IDE가 필요한가? 쉘에서 모든 걸 할 수 있음에도 왜 굳이 쓰는가?
            UI(인터페이스)란 건 바로 그 순간 필요한 정보와 액션을 제공해주는 역할임
          + 왜 bash 툴 외에 다른 게 들어갔냐는 질문에는 처음엔 최소한의 도구만 주고 시작해서, 나중에 bash를 추가해도 되는 거라는 점 때문일 것 같음
     * “에이전트 만드는 법”을 장황히 설명하기보단, 에이전트가 실제로 만든 프로젝트를 보여주길 바라는 마음임
          + 직접 에이전트를 만들어서 HN에 “Show HN”으로 공유해주면 너무 좋겠음
     * Oracle, Agent, high safety, low safety라는 축이 뭘 의미하는지 설명 가능한 사람 있는지 질문함
     * edge와 chrome의 온디바이스 모델(phi4-mini, gemini nano)로 직접 시도해봤는데, 모델 크기에 비해 꽤 잘 동작해서 놀람
       how to build an agent on device 실험 사례
"
"https://news.hada.io/topic?id=22717","더 빠른 CPU를 사야 하는 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           더 빠른 CPU를 사야 하는 이유

     * 최근 몇 년간 CPU 성능은 비약적으로 향상되었지만, 많은 개발자들이 여전히 구형 모바일 칩을 사용해 생산성이 저하되고 있음
     * AI 코딩 구독 서비스는 연간 약 $500 비용이 정당화되는데, 이는 소프트웨어 엔지니어 생산성이 그만큼의 가치를 가진다는 시장의 신호임
     * 반면 최상급 CPU(AMD Ryzen 9 9950X) 는 $500에 구입 가능하며, 3년 감가상각 기준 연간 약 $170으로 AI 구독보다 훨씬 경제적임
     * 벤치마크 결과, 최신 데스크톱 CPU는 구형 랩톱 CPU 대비 10배 이상 빠른 성능을 보여 빌드와 테스트 속도를 획기적으로 개선함
     * 따라서 AI 구독을 정당화할 수 있다면, 개발 생산성을 극대화하는 가장 좋은 투자처는 빠른 CPU임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

CPU와 생산성

     * 최신 CPU는 3년 전 대비 약 3배 향상, 데스크톱 CPU는 노트북 대비 약 3배 빠름
     * 클라우드 VM 역시 세대가 바뀔 때마다 2~3배 향상되며 가격은 비슷함
     * 따라서 CPU 업그레이드는 장기적인 생산성 확보에 가장 확실한 수단임

비용 대비 가치 비교

     * AI 코딩 도구 구독서비스(예: Cursor)가 인기를 끄는데, 팀 플랜 가격은 연간 약 $480~500
     * 다른 서비스들도 비슷한 가격대를 형성, 즉 엔지니어링 생산성에 최소한 연 $500의 가치가 있음을 보여줌
     * 최상위 CPU인 AMD Ryzen 9 9950X 의 가격은 $500
          + 기업이 개발용 머신 비용을 3년간 정액 상각한다고 가정할 때 연 $170 수준
     * 엔지니어 연봉 대비 1% 생산성 향상만 있어도 비용 대비 충분히 가치 있음

벤치마크 결과

     * 다음 3종류 머신의 리눅스 커널 컴파일 및 TLS 연산을 벤치마킹
          + 2024년 회사에서 제공받은 노트북(i7-1165G7, 2020년 출시)
          + 2024년 구매 가능한 가장 빠른 ThinkPad(AMD Ryzen 7840U)
          + 2025년 구매 가능한 가장 빠른 데스크톱 CPU(AMD Ryzen 9950X)
     * Linux Kernel Compilation 6.15 (시간 단위, 낮을수록 좋음)
          + AMD Ryzen 9 9950X (16코어): 48초
          + AMD Ryzen 7 7840U: 194초
          + Intel Core i7-1165G7: 349초
            → 최신 데스크톱은 구형 노트북 대비 약 7배 빠름
     * OpenSSL 3.3 (AES-256-GCM, 처리량 단위, 높을수록 좋음)
          + AMD Ryzen 9 9950X: 3861억 byte/s
          + AMD Ryzen 7 7840U: 696억 byte/s
          + Intel Core i7-1165G7: 334억 byte/s
            → 최신 데스크톱은 구형 노트북 대비 11~12배 이상 빠름
     * 결과: 리눅스 커널 컴파일 및 TLS 연산에서 10배 이상 성능 차이 확인
     * 예: 빌드 시간이 30초 → 3초, 또는 3초 → 300ms로 단축되는 차이는 업무 경험 자체를 바꿈

결론

     * 일반적인 경험 기준
          + 데스크톱 CPU는 노트북 CPU보다 약 3배 빠름
          + 최고급 CPU는 3년 전 동급 모델보다 약 3배 빠름
          + 클라우드 VM에서도 이 경향이 동일하게 나타남; 각 세대별로 대다수 워크로드에서 2~3배 빨라지며, 가격은 거의 변동 없음
     * AI 구독이 합리적 투자라면, 동일하거나 더 낮은 비용으로 최상급 CPU 구입 역시 정당화 가능
     * 개발자 생산성을 높이는 가장 효과적인 도구는 빠른 CPU임

     * 더빠른 최신형 데탑 CPU의 히든 코스트

    1. MB교체가 필수, 따라서 총 비용은 더 빠른 CPU소켓과 호환되는 M/B구매비용 추가
    2. 더 빠른 CPU와 이 CPU와 소켓이 호환되는 M/B를 넣을 케이스 및 Power 교체가 필요할지도
       결국 더 빠른 CPU에는 많은 히든 코스트가 있슴

   최신 데스크톱을 구형 노트북과 비교하는 게 맞나 싶은데요
   애초에 글의 성격이 진지하게 9950X를 사야 한다고 주장하는 건 아니겠죠? 농담 비슷한 거겠죠? ㅋㅋ

   그래도 너무 구형 CPU는 확실히 개발에 지장을 주더라고요
   제가 가끔 본문에서 말한 구형 노트북과 비슷한 CPU로 개발을 하고 있는데, 몰입에 정말 큰 차이가 있습니다

   벤치마크 자료도 없이 3년 내 3배 성능 향상은 과장이라고 생각합니다

   사양 비교를 위해서라기보다는 CPU에 따른 효율을 얘기하고 싶었던게 아닐까 싶습니다. ㅎㅎ

   1분이내 커널 컴파일이라니... 행복하겠네요.

   허....컴파일 켜놓고 10분 20분 노가리 깐게 엊그제 같은데요....1분이라....

   저는 cpu보단 램부족이 체감 잘되더군요.

   아.. 9950X.. 100만원 주고 샀는데...

        Hacker News 의견

     * 예전에 FAANG에서 일할 때, 개발자들에게 16GB와 64GB 머신을 각각 지급할 때 실제 업무를 바탕으로 대기 시간 절감 효과, 개발자 인건비와의 ROI 등을 계산해 본 적이 있음, 특히 시니어 개발자일수록 몇 주 만에 투자 대비 효과가 발현됨, 이런 경험을 토대로 소프트웨어 엔지니어용 하드웨어를 마련할 땐 거의 항상 최고 사양의 Macbook Pro를 2-3년마다 교체해주는 것이 합리적임이라고 생각함, 데스크톱 이외나 다른 직군에는 해당되지 않을 수 있음을 참고함
          + 수치적으로는 타당해 보이지만, 개발자 생산성을 그렇게 간단히 수치화할 수 있을지에 대한 의문이 있음, 심리학 연구들을 보면 사람마다 하루에 쓸 수 있는 집중력 총량이 어느 정도 정해져 있다는 주장도 존재함, 실제로 대기 시간 동안 가벼운 휴식을 하면서 다음 깊은 사고에 쓸 에너지를 아낄 수 있다는 경험이 있음
          + 내가 아는 FAANG 친구들은 대부분 원격 서버에서 작업함, 원격 편집, 원격 빌드가 기본이고 빌드는 수백~수천대 네트워크 빌더에서 처리됨, 로컬 머신을 더 빠르게 해봤자 실제로는 영향이 거의 없음
          + FAANG는 PC들을 중앙관리함, 이 정도 수준의 모니터링에 대한 윤리적 문제는 차치하더라도, 실제 효과를 검증하려면 OS 메모리 사용량을 제한하고 PR 수, 키보드에서 활동한 시간 등 실질적인 지표를 추적해보고 싶음
          + Macbook Pro 말고 제대로 된 Linux 데스크톱에서 일하고 싶음, 업무용 PC에 발열로 느려지는 모바일 칩 쓰는 거에 지침
          + 의도는 이해하지만, 실제로는 프로젝트 완료 시간조차 정확히 예측하기 어려움, 빠른 컴퓨터가 실제로 '시간을 아껴주는' 경우는 드묾, 결국 '이걸 돌릴 수 있냐 없냐' 같은 이진적인 문제거나, 불필요한 대기시간에 반복적으로 좌절을 느끼게 하는 환경적 요인임, 단 돈 1,000달러면 하루에 10분씩 기다리는 건 없앨 수 있음
     * 예전 CPU 계속 써도 좋은지에 대한 고민임, 2016년 출시된 6600(65W)을 아직도 메인 PC로 쓰고 있음, SSD도 교체했고(2011년산 SLC SSD로 내구성 기대), 32GB 메모리도 aliexpress에서 저렴하게 업그레이드함, 모니터는 15년 썼던 Eizo FlexScan 1932에서 RadiForce 191M으로 갈아탐, f.lux나 redshift 같은 블루라이트 감소 소프트웨어와 잘 어울림, 저전력 3050 그래픽카드 하나면 올해 나온 게임도 60프레임으로 무리 없이 돌아감, 컴파일도 문제없음, 마더보드가 먼저 고장날 때까지 쓰는 게 합리적이라고 생각함
          + 나 역시 E5-2650v2를 중국산 mATX 보드에 여러 해 전 $50에 서버에서 떼온 중고로 쓰고 있음, 970 Evo SSD, 24GB 짝이 안 맞는 DDR3, 홈서버랑 dev 환경(컨테이너화된 Incus) 다 커버함, 매년 갱신해야지 생각하지만 실제로는 충분히 잘 버텨줘서 굳이 바꿀 필요를 못 느낌
          + 2011년산 순수 SLC SSD보다는 SLC 캐시가 더 큰 최신 SSD를 더 저렴하게 구매할 수 있음
          + 실제로 어떤 용도로 쓰는지에 따라 크게 달라짐, 나는 17년 된 2.4GHz Core 2 Duo 랩탑에 램 4GB로도 일상 작업 충분히 커버함, 막상 많은 사람들이 이런 환경을 못 받아들이는 건 업무가 더 무거운 경우가 많아서임
          + 게임 선택에 따라 달라짐, 예전에 7700k 쓰다가 Factorio가 버거워서 5700X3D로 갈아탐, Path of Exile 2에서는 최신 CPU로도 대형 전투에서는 30프레임 간신히 유지함
          + 나는 발열이 낮은 쪽을 선호해서 2년 전에 Ryzen 5700으로 업그레이드함, 코어를 다 쓰는 경우는 드물고, 쿨링 성능 덕분에 팬 소음이 거의 들리지 않는 장점이 있음
     * CPU 가치 변곡점이 있음, 1) 10년간 대다수 업무를 무난하게 처리할 수 있을 만큼 퍼포먼스가 잘 나오는 CPU, 2) 후속 모델보다도 오랜 기간 실성능이 더 낫게 평가되는 CPU들이 가끔 나옴, i7-4770이 대표적이었고, 10세대 전까지 후속작보다도 더 낫게 쓰였음, i7-12700이 되어서야 비슷한 대체제가 생겼고 i5-12400은 가성비용으로 추천할 수 있음, 13세대부터는 인텔이 E/P 코어 구조로 전환했고, 나는 지금도 12세대가 새 데스크톱에는 더 적합하다고 느낌, AMD Ryzen 9950x도 엄청난 칩임, 친구 커스텀 빌드에 사용했는데 2035년까지도 쓸 것 같음
     * 대기업들은 규모의 경제를 원함, 동일 사양의 수많은 컴퓨터를 한 번에 구매해서 모두에게 동일한 머신을 다년간 지급함, 이렇게 하면 제조사(Dell, HP)는 단가를 엄청 낮춰서 납품함, 일반 소비자보다 절반 이하 가격에 굉장히 높은 사양의 PC를 구매할 수 있지만, 사양이 고정된 채로 시간이 흐르면 결국 점점 느려짐
     * 원문 기사는 몇 가지 중요한 단계를 건너뛰었음 - 더 빠른 CPU가 실제로 개발자 퍼포먼스에 얼마나 영향을 미치는지 설명이 부족함, 컴파일 시간이 30초라면 개발자는 이메일이나 SNS를 확인하다가 흐름이 끊김, 그걸 3초로 줄여주면 '몰입'을 유지하는 데 엄청난 효과가 있음, 그러나 CPU 속도를 올려서 컴파일 시간을 줄이는 방법론이 빠져 있음, 예를 들어 컴파일러가 IO나 램이 병목이면 CPU만 올려봤자 한계가 있음, 결국 한 요소의 병목이 해소되면 다음 병목이 등장하고, 전반적 성능 향상에는 한계가 있음
          + 내 상사가 내게 가장 강력한 서버를 사줬고, 1만5천 달러짜리였음, 56코어 중 40개 이상을 쓰게 하니, 오히려 빌드 시간이 늘거나 효과가 미미함, 결국 메모리 대역폭이 병목인 것 같다는 결론, 다만 어떻게 증명할진 모르겠음
          + 컴파일러가 IO 병목이 되는 경우는 드물다고 봄, 램이 병목이 될 순 있지만, 최신 CPU들은 RAM과 데이터 교환 속도도 빨라지므로 일면 해결될 수 있음, LSP(언어 서버 프로토콜) 응답 속도가 2배 빨라지는 것만으로도 몰입 유지에 엄청난 도움임
          + 컴파일에서 코어를 많이 늘리더라도 링커(링크 타임)는 실질적으로 싱글코어에만 의존해서, 여기가 진짜 병목이 됨을 유념해야 함
          + 요즘은 컴파일러가 병목인 환경보다, Microsoft MFA 때문에 폰을 꺼내야 한다든지 PIM을 통한 임시 권한 기다리는 등 다른 대기 시간이 더 문제가 됨, 과거처럼 30초짜리 컴파일 대기 시간이 제일 느렸던 시절은 이미 지남
          + 요즘엔 하드웨어만 업그레이드해서 30초를 3초로 만들 수 있는 경우가 그렇게 흔하지 않음, 오히려 처음 구매 자체가 엄청 저가형이면 모를까, 기사에서는 랩탑이랑 데스크톱을 비교하고 있으니, 결론도 '컴퓨터 두 대를 사자'가 되어야 할 듯함
     * 내가 새 데스크톱을 맞출 때마다 항상 상위권 성능을 선호했으며 캐시와 전력소비도 고려함(예전엔 전력소비 고려 많이 했던 시절도 있었음), 예전 듀얼 펜티엄 프로 시절부터 최근 Xeon E3-1245 v3(2012년에 32GB 탑급 램과 맞춤)까지도 최근 들어서야 약간 느려지기 시작했는데, 이는 주로 윈도우 보안 패치 때문인 것 같음, 초기 비용을 몇 백 달러 더 써도 훨씬 더 오랫동안 쓸 수 있음
          + 성능이 조금 더 좋은 것을 너무 과대평가하는 경향이 있다고 느낌, 최근 9700X로 게이밍 PC를 만들었는데, 9800X3D로 가면 벤치마크상 18% 성능 업그레이드지만, 전력 소비는 두 배임, CPU를 완전히 쓰는 순간은 드물지만 소모전력은 항상 발생함, 전력이 높으면 더 뜨거워지고, 파워와 쿨링 시스템에 부담이 커짐, 조금 사양을 내리면 수명도 훨씬 오래 가는 시스템이 된다고 생각함
     * 저자가 놓쳤지만 중요한 점: 노트북 CPU를 논하면서도 언급하지 않은 게 있음, 고급 노트북은 써멀(발열) 한계가 최대 제약임, 더 나은 CPU라도 효율이 높은 쪽이 유리함, 브랜드 선택 시 쿨링 설계가 제대로 된 곳을 고르는 게 중요함
          + 노트북 안에 고급 데스크톱 CPU에 준하는 쿨링과 전력을 담는 건 물리적으로 불가능함, 거의 유일한 방법이 외부 액체 쿨링 루프 포트를 연결하는 것인데 그럴 바엔 그냥 데스크톱을 쓰는 게 낫다고 생각함, 데스크톱의 주변기기, 멀티모니터 등 편의성도 생각해야 함, 왜 많은 개발자들이 굳이 랩탑으로 일하려고 하는지 이해하기 힘듦, 아마도 사과 로고의 영향이 큰 듯함
          + (수줍게 팬리스 Macbook Air로 개발하는 나…)
     * 업무 종류에 따라 다름, 나는 최근에 24코어 CPU로 수백만 원짜리 새 PC를 만들었고 docker로 gcc 빌드는 훨씬 빨라졌음, 그런데 오히려 Angular 앱 빌드는 예전 맥북보다 약간 더 느림, 라이브러리도 쪼개고 turbo로 병렬 최적화까지 했는데도, CSS 변경이 브라우저에 반영되는 데 6~10초 걸리면 정말 고통스러움, 이게 누적이 되면 짜증과 피로가 쌓임
          + 웹 개발 정말 이상함, 예전에 Java/C 기반 코드베이스에서는 빌드가 몇 초, 웹회사는 JS/TS 기반으로 가니 빌드에 몇 분씩 걸림, 오히려 트레이딩 소프트웨어보다 빌드 파이프라인이 더 복잡하고 느림
     * C/C++ 등 멀티코어 활용 업무는 성능 차이를 확실히 체감할 수 있음, 다만 16코어 AMD Ryzen 9 9950X의 싱글코어 성능이 오래된 4코어 i5 랩탑보다 딱 1.8배 빠름, CPU 벤치마크 비교참조, 나중에 1000코어 ARM PC에 1TB 메모리 올려서 대형 LLM 학습이나 실험이 가능한 시대가 오면 좋겠음, 누가 만들면 나한테 알려주길 바람
          + 나도 옛날 i3770(12년전 CPU)에서 9900x로 한 번에 올림, 싱글코어 성능이 벤치상 50%뿐이 차이 안 나지만, 실제 사용감은 두 배 빠름, 멀티스레드에선 세 배 가까움, 최근 Mac Mini M4도 써봤는데 체감상 이 두 데스크톱에 비해 훨씬 느림(주로 UI/소프트웨어 문제로 보임), M4는 Xcode 쓸 때만 사용
          + 나 역시 인텔 5세대 이상 건너뛰고 새 랩탑을 샀는데, 성능보다 더 유의미했던 점은 무거운 구형 워크스테이션에서 팬소음 없고 작고, USB-C 어댑터로 충분히 충전 가능한 소형 경량 랩탑으로 바뀌었다는 점임, 성능은 비슷하지만 사용성 차이가 확연함
     * 기업들, 특히 FANG도 개발자 하드웨어에선 비용 절약에 너무 집착하는 경향이 있음, 모니터 개수/사이즈 제한, 인체공학 장비에 각종 승인/의사 진단서 요구, 고성능 하드웨어 요청에 승인 절차 부여, 출장/숙소/항공료 상한 등 물가상승 반영 안 한 제한 등, 이런 정책들이 있음, 물론 아무 업무 없이 고사양 MacBook만 들고 500개 크롬 탭만 띄우는 사람도 있는데, 이런 남용도 실제로 일어나긴 함, 그러나 그것도 연간 개발자 급여 대비 매우 작은 비용임
          + 내가 경험했던 잘나가는 스타트업들은 초반엔 원하는 장비, 모니터, 의자, 스탠딩 데스크, SaaS, 야근시 DoorDash 등 원하는 걸 다 지원함, 그런데 직원 25% 정도가 심각하게 남용하는 시기가 오고, 회사가 비용을 확인하면 규제가 들어가기 시작함, MacBook Pro, 고가 모니터, 스탠딩 데스크, iPad, 오피스 체어 비용이 합쳐서 한 명당 수천~수만 달러가 쉽게 나감, 회사 규정은 누군가 한계까지 최대한 뽑아내려고 쓰려함, 장비 분실도 더 잦아지고, 심지어 음식 배달을 위해 일부러 퇴근시간 늦추는 등 소소한 비리까지 발생함
          + 어떤 FAANG IT팀은 250GB 이상의 SSD가 몇 년째 '품절'이라는 핑계를 댐, 전 세계 공급 문제라는데 실제로는 공급팀의 무능함으로 보임, 많은 엔지니어들이 대화방에서 불만을 토로하고, 연봉이 30만 달러가 넘는 개발자들은 자기 돈으로 SSD 사서 장착해 쓰고는 IT팀에 신고 안 하는 게 일상임, 한편 그 회사에서는 100TB 클라우드 VM은 클릭 한 번에 바로 쓸 수 있음, 이런 모순이 웃기지만 다들 받아들이고 있음
          + 직원 수 만~수십만 명인 경우 완전히 제한 없는 하드웨어/출장 비용이 누적으로 엄청난 금액으로 치솟으므로 일정 제약은 필요하다고 생각함, 돈에 무감각한 소수 때문에 회사가 손해 볼 수 있음
          + 업무와 연관된 500개 크롬 탭이라면 생산성에 도움되는 거니까 남용이 아님, 나는 노트북 값보다 100배 비싼 인건비를 받기에, 노트북이 내 일을 덜어줄 수 있다면 그런 기계가 더 일을 담당해야 한다고 봄
          + 고사양 장비 남용은 순식간에 수만~수십만 달러로 확대될 수 있음, 애플 스토어 기준 최고 스펙의 14인치 MacBook Pro만 봐도 7,000달러를 넘음, 중간 사양도 2,600~3,000달러면 업무엔 충분하며, 그 이상의 금액 차이는 실질 생산성 향상 효과가 거의 없을 수 있음, 이런 금액 차이가 20~60인 팀에 곱해지면 스타트업에는 상당한 부담임, 여기에 모니터/체어 등도 추가하면 1인당 평균 2,000달러씩만 불필요하게 써도 한 번에 12만 달러 추가 지출임, 실제로 '필요한 만큼 아무거나 사라' 정책이 150명까지 유지된 회사도 있었음

   꿈같은 이야기들이 너무 많이 적혀 있어서 어질어질합니다
"
"https://news.hada.io/topic?id=22707","작업 중단의 비용 (2023)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            작업 중단의 비용 (2023)

     * 업무 중 방해(Interruptions) 가 생산성에 악영향을 준다는 통념이 있지만, 유명한 23분 15초 회복 시간 수치는 근거가 불분명함
     * 널리 인용되는 논문에서 23분 15초라는 시간은 실제로 언급되지 않음
     * 여러 블로그 글과 기사들이 이 수치를 인용하지만, 그 중 대부분은 논문을 잘못 인용하거나 Gloria Mark의 인터뷰를 근거로 함
     * 구체적인 연구 논문에서는 오히려 중단된 경우의 원래 업무 시간이 약간 더 짧고 스트레스만 커진다는 결과가 있음
     * 공식적이고 명확한 출처는 부족하며, 현재는 개인 인터뷰 인용이 주된 출처임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

중단이 생산성에 미치는 영향

     * 업무 중 방해나 맥락 전환이 있을 때 23분 15초의 복귀 시간이 필요하다는 이야기가 널리 퍼져 있음
     * 하지만 이 시간이 어디서 비롯된 것인지 궁금증이 들었고, 실제로 신뢰할 만한 원본 논문을 확인하고자 노력함
     * 여러 차례 검색과 논문 열람을 했지만, 인용되는 숫자 23 혹은 23분 15초라는 언급은 원본 논문에서 발견할 수 없었음

관련 논문 검토

     * 블로그 글에 자주 언급되는 논문은 The Cost of Interrupted Work: More Speed and Stress 임
     * 이 논문에서는 중단이 있을 때 업무에 소비된 시간이 오히려 줄어들고, 대신 경험하는 스트레스가 증가한다는 결과를 제시함
     * 논문에서는 방해 종료 후 원래 업무로 복귀하는 시간에 대한 구체적인 수치나 내용은 나타나지 않으며, 23이라는 숫자 역시 본문에 없음
     * 다른 논문이나 참고문헌, 관련 연구에서도 해당 수치는 명확하게 기술되어 있지 않음

블로그 및 미디어 인용 분석

     * 총 23개의 블로그 글과 5개 논문을 추가로 검토함
          + 9개 글은 논문을 오인용했으며, 그 중 1개는 실제로 존재하지 않는 인용문도 포함함
          + 2개 글만이 해당 논문의 실제 결론을 정확히 인용함
          + 9개 글은 Gloria Mark의 인터뷰 3곳을 직접 혹은 간접적으로 인용하고 있음
          + 2개 글은 Wall Street Journal에서 Gloria Mark의 직접 인용 문구를 재인용함
     * 결국, 23분 15초 수치는 Gloria Mark가 여러 인터뷰에서 언급한 경험적 수치임
     * 하지만 이 수치가 최초로 등장한 공식 논문이나 연구는 명확히 확인 불가함

결론 및 참고

     * “중단 후 23분 15초가 걸린다”는 수치는 현재까지는 인터뷰 출처에만 근거하고 있고, 공식적인 논문화된 근거는 없음
     * Gloria Mark의 다양한 논문 목록을 추가로 검토해도 해당 수치는 발견되지 않았음
     * 만약 이 수치가 나오는 공식 논문이나 연구를 아는 경우, 정보를 요청함

기타 정보

     * 관련 주제의 Reddit 토론글 주소가 소개됨
     * 글에서 언급한 모든 블로그 글 및 논문의 참조 그래프와 링크 목록이 제공됨

        Hacker News 의견

     * 어떤 날에는 갑작스러운 끼어들기 하나가 내 사고의 흐름을 확 끊어놓기도 하고, 그 이후의 여섯 시간은 마치 어디엔가 쓸지도 모르는 빈 병이나 철로 도구들을 줍는 기분으로 보내게 됨, 반면 어떤 날에는 끼어들기가 별 영향 없이 지나가기도 함, 어느 쪽이 될지 아직도 가늠하지 못해서 그냥 Slack에 로그인하지 말아야 하나 고민 중임
          + 나는 끼어들기의 영향을 최소화하는 방법으로 페어 프로그래밍이 매우 효과적이었음, 한 스타트업에서는 매일 하루 종일 페어 프로그래밍을 했는데, 끊긴 후 재개하는 게 거의 문제 되지 않았음, 설명은 어렵고 경험해봐야 알 수 있음
          + 정말 공감함, 그런 날에는 오히려 책상에서 벗어나 숲에서 산책하거나 집안일을 하면 다시 집중력을 되찾을 수 있었음, 일하기에 적합하지 않은 날엔 아예 휴식을 택하는 것이 결국 다음날의 폭발적인 생산성으로 이어짐, 모두에게 윈윈임
          + 나에게는 끼어들기의 ""성격""이 더 중요함, 기억에서 바로 꺼낼 수 있는 쉬운 질문은 비용이 크지 않지만, 뭔가 더 깊이 생각하거나 코드·문서를 확인해야 할 때는 엄청 크게 느껴짐, 이메일이나 Teams 알림조차 흐름을 깨버릴 수 있음, 누가 찾아와서 방해하는 게 문제가 아니라, 어떤 주제로 끼어들었느냐가 핵심임, 다만 코딩 중 끼어들리면 버그가 생길 위험은 항상 높음
          + 나에게 있어서는 미리 작업을 계획했는지 아닌지가 차이를 만듦, 10~11시에 무엇에 집중할지 미리 알고 있으면 다시 일을 이어가기 쉬움, 반대로 계획 없이 그냥 시작하면 외부 방해가 없어도 쉽게 딴 데로 빠짐
          + 전날 잠을 얼마나 잘 잤는지와는 정비례, 반대로 그 전주에 커피를 얼마나 마셨는지는 정반비례하는 경향을 발견함
     * 과학 기사 보도에서 이런 문제가 매우 흔함, 기사들이 논문의 내용과는 다르게(혹은 완전히 반대로) 전달하기 일쑤임, 인용 논문을 기사 본문에서 찾을 수도 없는 경우가 많음, 가끔 저자 잘못일 때도 있지만 주로 과학 기자들이 오해되거나 왜곡되게 씀, 내 기본 원칙은 반드시 논문 초록, 방법, 그래프/데이터를 직접 5분이라도 훑어보는 것임, 팝사이언스 기사보다 훨씬 더 정확하게 알 수 있고, 점점 익숙해짐, 나도 TDD를 엄격하게 할 때는 방해에서 빠르게 회복하지만, 설계 고민이나 복잡한 알고리즘 분석 등 머릿속에서만 하는 작업은 회복이 오래 걸림, 아마도 이런 손실은 실제로 측정 가능하고, 실험도 해볼 수 있을 거라 생각함
          + 방해가 당연한 환경에선 일하는 방식을 아예 달리함, 겉으로는 시간 손실이 적어 보이지만 사실은 방해를 감수하며 일하는 것, 영향은 줄었지만 일이 전체적으로 분산된 것뿐임
          + LLM(대형언어모델) 헛소리 현상의 일부는 이런 과학 기사 보도를 고품질 학습 데이터라고 잘못 인식해 생기는 지도 모르겠다는 생각이 듦
          + 이런 작은 오류들이 누적되며 신뢰를 떨어뜨리고 큰 문제로 번져가는 과정을 상세히 설명함, 예를 들어 '과학자가 말했다'는 식의 보도가 과학자보다 기자의 실수를 소비자가 더 책임지게 만들고, '핫도그 많이 먹으면 암'과 같은 내용도 연구 맥락이나 수치를 빼고 선정적으로 전달됨, 그래서 실제 연구와 뉴스 내러티브 사이의 불일치로 일반인들은 신뢰를 잃음, 더불어 인용 건수가 과학자 평판을 좌우해서, 미디어 주목을 받으려는 유인이 생기고, 이 역시 왜곡을 가중하는 구조임, 평범한 논문은 눈길도 못 끌어서 MIT 대학원생 논문이 더 많이 인용되는 현실임, 이런 시스템적 문제는 복합 효과로서 점점 커짐, 모든 설명을 단순화하면 편할지 몰라도, 실제로는 세상은 점점 더 복잡해지고 있음, 또한 논문은 동료 간 소통 수단으로, 대중과 소통해야 할
            때는 별도의 전문 커뮤니케이터가 필요한 맥락임, “간단히 설명 못 하면 이해 못하는 것”이라는 문구도 웃기는 이야기임, 복잡한 개념은 본질적으로 단순하게 만들 수 없음
     * 원래 출처는 2006년 Gallup의 Gloria Mark 연구자 인터뷰임, 링크, “방해받은 후 다시 일에 복귀하는 데 평균 23분 15초가 걸린다”는 언급이 있음, 좋은 점은 81.9%가 그날 다시 원래 작업을 재개함, 그 시간이 지나치게 길지 않다는 것임
          + 이 23분이라는 시간이 방해를 받은 후 원래 일에 집중하려 애쓰는 게 아니라, 끼어든 다른 일이나 중간에 새로 처리해야 하는 과업에 쓰이는 시간임을 의미하는 걸로 해석함, 그러니까 이 시간 자체가 낭비된 건 아님
          + 아쉽게도 이 인터뷰는 진짜 1차 출처가 아니고, Jaro Fietz(oberien)라는 작가가 다이어그램에 해당 인터뷰를 참조했지만 진짜 연구 논문을 아직 못 찾았음, 혹시 정확한 논문 아는 사람이 있으면 알려달라고 요청함
          + 그런데 ‘나쁜 소식’이 뭔지 궁금해짐
     * 복잡한 문제를 풀 때 몰입 상태(플로우)에서 방해를 받으면 실제로 몸이 아픈 것 같은 느낌을 받음, 겉으로는 태연하게 보이려 해도 머릿속에서 모든 맥락을 연결하던 실마리가 끊겨버림, 생산성 손실을 숫자로는 측정 못 하겠지만, 문제에 따라 회복에 20분이 넘게 걸릴 때도 많음
          + 우리 오픈소스 프로젝트 이슈 관리를 public GitHub에서 private Jira(2FA 필요)로 바꾸는 게 왜 개발자 생산성에 나쁜지 설명하려 해도 고위 경영진은 공감 못 함, 그들에게 “플로우 상태”나 갑자기 끊기는 고통은 그냥 허상에 불과한 이야기임
     * 나는 회의로 인한 ‘방해’ 자체보다, 방해가 다가올 것이란 ‘예상’이 훨씬 더 시간을 낭비하게 만든다고 느낌, 그래서 양쪽에 30분씩 날아감
          + 팬데믹 전에는 재택/사무실 일정이 불규칙했는데, 가끔 사무실 회의 전에 일찍 도착하면, 그 중간 시간은 거의 쓸모 없게 지나감, 괜히 심도있는 작업은 시작 못 하고, 간단한 리서치나 하다 넘김, 오픈오피스 환경 자체가 방해가 많아서 집중하기 어려웠음, 차라리 회의 바로 5~10분 전에만 도착하는 게 최선이지만, 실제로는 맞추기 어려웠음, 반대로 몰입 상태에서 회의를 놓치는 경우도 생겼는데, 이럴 땐 정말 데스크탑·휴대폰 알림이 아무 소용이 없었음, 진짜로 놓치지 않으려면 알람을 울려야겠지만 잘 안 하게 됨
          + 내 하루를 망치는 게 바로 이런 회의임, 동료들이 주는 다른 방해는 거의 다 생산적이고 도움이 됨
          + 나도 마지막에 급하게 일정 조정된 회의가 너무 싫음, 30분밖에 없다 생각하면 의미 있는 작업을 시작 못 함, 정말 시간 낭비임
          + “회의까지 30분 남았으니 진지하게 일 시작하지 말자”는 마인드가 됨, 어떤 똑똑한 사람은 회의 사이에 1시간 간격을 두는데, 그러면 하루 종일 아무 것도 못 하게 됨
          + 나도 회의가 잡히면 그냥 그날의 반이 날아감
     * 나는 매니저로서 반복되는 귀찮은 방해 대부분은 솔직히 ‘스스로 노력하지 않는 태도’라고 봄, 내 역할이 전략과 우선순위를 주는 것뿐 아니라 개발자들이 막혔을 때 뚫어주는 것인데, 가끔 ‘데이터베이스 계정 필요하면 인프라 담당자에게 물어보라’라든지, ‘누가 이 API 작성했는지 git에서 찾으라’는 기본적인 것도 안 찾고 곧장 묻는 사람들이 있음
          + 시니어 개발자 입장에서도 공감함, 정말 스스로 2~3분만 투자하면 알 수 있는 것을 15초 만에 내게 묻는 경우가 많음, 주니어가 깊은 질문 할 때는 한 15~30분 함께 고민해주지만, 뭔가 검색해보고 노력해봤는지 여러 질문을 먼저 던짐, 그래도 반복적으로 똑같이 묻는 경우는 정말 지침, 대답을 들어봐도 본인이 이미 아는 얘기일 때도 많고, 결국은 누가 대신 해주길 바라는 듯함, 그래서 정말 피곤해짐
          + 이런 순간을 ‘물고기를 잡는 법을 알려주는’ 시간이라 생각함, 스스로 조사하고 시도해봤는지 확인하는 질문도 함께함, 만약 반복된다면 일대일 면담에서 직접 이야기해야 함, 개별 문제를 넘어 여러 명에게서 동시에 나타나면 오거나이제이션 전체 또는 문서화 프로세스에 뭔가 근본적 문제가 있을 수도 있음, 복잡한 조직이나 대형 팀에서는 특히 이런 상황이 자주 나타남
          + 만약 요청이 내 팀 내부에서 온다면, 그건 업무의 일부지 진짜 ‘방해’는 아니지 않냐는 입장임, 방향·우선순위 잡아주는 게 전업일 리도 없으니 당연한 일임
     * 많은 사람들이 제목이나 기사만 보고 반응하거나, 읽지도 않고 코멘트 단다는 점이 아이러니하게 ‘기사의 주장’ 자체를 실시간으로 입증하고 있다는 점이 흥미로움
          + 실제로 많은 사람들이 댓글만 읽고도 토론에 참여하는 게 흔한 것 같음, 나도 때론 기사 안 읽고 댓글 보고 읽을지 말지 결정하지만, 그래도 꼭대기 댓글까지는 반드시 원글을 읽고 의견을 남김
     * 만약 정말 “23분”이 고정값이라면, 의사 같은 다양한 중요한 직업이 아예 불가능할 것임, 즉 끼어들기의 영향이 단일 수치로 요약 가능하진 않을 것임
          + 나는 평균값일 거라 생각했고, 실제로 어떤 작업은 5초, 어떤 건 2시간이 걸릴 수도 있다고 봄, 애초에 정확한 인용문도 그리 구체적이지 않아서, 정말로 23분 단일값만 강조하는 건 성급한 결론임
          + 아니면 방해받은 작업은 우선순위가 점점 밀려서 점점 처리되지 못하는 걸 수도 있음
     * Gloria Mark의 책 ‘Multitasking in the Digital Age’ 44페이지에 이 내용이 나옴, 링크
     * 원천을 꼼꼼히 찾고 기록하려는 시도, 정말 멋짐, 나는 학생들이 참고문헌이나 원문을 제대로 조사하지 않고 인용하거나, 잘못된 해석을 자기 생각처럼 착각하는 걸 자주 질책하게 됨, 능동적 독서란 읽는 사람이 스스로 생각을 덧붙이고 해석하게 되는 과정임
"
"https://news.hada.io/topic?id=22741","Google Nano Banana란 무엇인가? Google의 비밀 이미지 AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Google Nano Banana란 무엇인가? Google의 비밀 이미지 AI

     * Nano Banana는 공식 발표 없이 등장해 AI 이미지 생성 성능을 압도하며 주목받은 모델임
     * 기존 모델들이 약한 언어 기반 편집, 캐릭터 일관성, 빠른 응답 속도, 다중 이미지 처리에서 뛰어난 성능을 보여줌
     * 정체는 공개되지 않았지만, 구글 또는 DeepMind가 익명 테스트 중인 Gemini 계열 모델이라는 추측이 강함
     * 전자상거래, 게임, 건축, 교육 등 다양한 산업에서 비용 절감과 효율 향상 사례가 보고됨
     * 아직 불안정하고 공식적 접근 경로가 부족하지만, 이미지 편집 워크플로우를 근본적으로 바꿀 잠재력을 지님
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Nano Banana의 등장

     * LMArena의 Battle Mode에서 처음 목격되었으며, 사용자들은 한 모델이 유독 뛰어난 결과를 내는 것을 발견함
     * Reddit, Discord 등 커뮤니티에서 큰 화제가 되었고, 바나나 아이콘과 구글 엔지니어의 바나나 이모지로 인해 이름이 붙음
     * 얼굴 일관성, 맥락 이해, 복잡한 지시 수행 능력에서 차별성을 보임

Nano Banana의 차별점

     * Nano Banana는 기존 모델들이 어려워하는 컨트롤, 일관성, 장면 논리에서 강점을 보임
     * 1. 레이어가 아닌 언어로 이미지 편집 제공
          + 포토샵이나 전문 도구가 필요 없이, 일반적인 텍스트 지시만으로 원하는 이미지 변환 가능
          + 예를 들어 ""배경을 없애고 숲으로 바꿔줘"" 또는 ""웃는 표정과 부드러운 조명 추가""와 같이 설명하면 Nano Banana가 이를 자동으로 구현함
          + 대부분의 기존 모델은 세밀함 부족 또는 여러 번 시도해야 원하는 결과 도출, Nano Banana는 처음 시도에서 정확한 결과 생성 가능성 높음
     * 2. 실제로 작동하는 정체성 보존
          + AI 이미지에서 가장 깨지기 쉬운 것이 인물이나 객체의 정체성을 유지하는 것임
          + Nano Banana는 배경 변경, 각도 전환, 색상 조정 등 다양한 변화를 줘도 이미지의 인물/주체가 지속적으로 동일함
          + 일관성 있는 아바타, 만화, 인플루언서, 제품 이미지 제작을 자연스럽게 지원함
     * 3. 매우 빠른 처리 속도
          + 타 도구들은 이미지 1장당 10~15초 소요, Nano Banana는 1~2초 만에 결과 생성
          + 1초 이내 반응이 가능한 경우도 있어 실제 실시간 편집에 근접한 경험 제공함
     * 4. 멀티 이미지 편집 및 스토리텔링 최적화
          + 여러 관련 텍스트 프롬프트나 이미지를 입력할 경우 스타일 및 내러티브 일관성을 자동으로 유지함
          + 대형 모델들이 흔히 놓치는 부분을 안정적으로 처리함
          + 이는 콘텐츠 제작자, 광고 캠페인, 슬라이드, UGC, 만화 등에서 일관된 장면이나 시리즈를 제작할 때 매우 유리함

구글과의 연관성 추정

     * nanobanana.ai 인터페이스가 Gemini 출력 스타일과 유사
     * 구글 개발자들의 바나나 관련 SNS 게시물이 단서로 작용
     * 모델 성능 수준이 탑 티어 연구소(OpenAI, Google, Anthropic)급이며, 특히 Gemini 계열과 유사하다는 평가

실제 활용 사례

     * 전자상거래 기업: 색상·스타일 변환으로 제품 이미지 제작 비용 절감, 전환율 34% 상승
     * 콘텐츠 팀: 며칠 걸리던 캠페인을 한 시간 내 완성
     * 게임 스튜디오: NPC 초상화 수천 장을 10K 이하 비용으로 생성 (기존 150K 이상)
     * 건축 회사: 인테리어 목업 제작으로 고객 수정 라운드 절감
     * 교육 분야: 교사들이 과학 다이어그램 제작에 활용, 학생 피드백 “교과서보다 명확”

접근 방법

     * LMArena Battle Mode: 무작위 대결에서 Nano Banana를 만날 수 있음
     * nanobanana.ai: 직접 이미지 업로드 및 편집 가능하나 불안정
     * Flux AI/FluxProWeb: 비공식적으로 모델 접근 가능
     * Cursor IDE 플러그인: 일부 개발자들이 비공식 API를 활용해 통합

한계점과 과제

     * 초기 사용자들이 왜곡, 조명 오류, 얼굴 변형, 모호한 지시 오해 등을 지적
     * 접근성이 제한적이며, 서버 불안정·스왑 현상 발생
     * 상용 제품이라기보다는 테스트 혹은 유출 형태에 가까움

시사점

     * Nano Banana는 단순한 이미지 생성기를 넘어 편집과 스타일링까지 자동화하는 모델임
     * 포토샵, Canva, After Effects 같은 기존 편집 툴을 대체하거나 위협할 잠재력 보유
     * 단순한 예술용 AI가 아니라 실무 워크플로우 혁신을 목표로 한 모델로 평가됨
"
"https://news.hada.io/topic?id=22685","설명용 수학 비디오를 위한 애니메이션 엔진 Manim","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     설명용 수학 비디오를 위한 애니메이션 엔진 Manim

     * Manim은 수학 설명 비디오 제작을 위한 정확한 프로그래밍 기반 애니메이션 엔진임
     * 3Blue1Brown의 저자가 본인의 교육 비디오 목적으로 개발한 프로젝트임
     * 커뮤니티 포크 버전도 존재하며, 사용성 및 기여, 테스트 측면에서 발전되고 있음
     * Python 기반으로 구동되며, FFmpeg, OpenGL, LaTeX 등 다양한 시스템 의존성 요구함
     * 오픈소스(MIT 라이선스) 로 누구나 자유롭게 사용 및 기여 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Manim 프로젝트의 중요성 및 장점

     * Manim은 수학 및 과학의 개념을 시각적으로 효과적으로 설명하기 위해 개발된 오픈소스 애니메이션 엔진임
     * 타 영상 툴과 달리, 정밀한 코드 기반 애니메이션을 만들 수 있어 복잡한 수학적 아이디어를 단계적으로 시각화하는 데 탁월한 강점 보유함
     * 3Blue1Brown가 직접 운영하며, 교육적 영상 제작 경험과 노하우가 녹아 있는 독창적인 도구로 평가됨
     * 커뮤니티 포크 버전이 별도로 활발히 개발 중이어서, 개발자 친화적이고 다양한 개선이 빠르게 반영되는 구조임

프로젝트 개요

     * Manim은 수학 설명을 위한 정밀 프로그래밍 애니메이션 엔진으로, 영상에서 수식, 도형, 다각적인 개념 전개를 자유롭게 구현할 수 있음
     * 원래 3Blue1Brown의 교육 영상 제작용으로 시작돼, 그와 관련된 전용 코드도 별도 저장소에서 공개됨
     * 2020년에는 보다 많은 사람들이 손쉽게 사용할 수 있도록 커뮤니티 버전이 포크되어, 안정성, 커뮤니티 기여, 테스트 등이 향상된 생태계 형성 중임

주요 특징 및 요구 사항

     * ManimGL(원본)과 Manim Community Edition(커뮤니티 포크)로 나뉨
          + 각 버전별 설치 및 사용법이 다르기 때문에 원하는 버전을 미리 선정해야 함
          + pip로 설치 시에는 manimgl 패키지명을 통해 원본 버전 설치가 가능함
     * Python 3.7 이상이 필요하며, FFmpeg, OpenGL, 특정 환경에서는 LaTeX 및 Linux용 Pango 등이 추가적으로 요구됨

사용 안내 (예시)

     * 코딩 예시, 장면 파일과 실행명령이 포함된 샘플 제공
     * 여러 예제 장면과 문서, 실제 3Blue1Brown 영상용 전용 코드도 확인 및 활용 가능
     * 커맨드라인 실행 시 다양한 플래그를 제공하여
          + 결과 파일 저장, 전체장면 스킵, 풀스크린 실행 등 다양한 출력 방식 및 사용자 지정 환경 지원
     * custom_config.yml 파일을 통해 스타일, 품질, 경로 등의 세부 환경 설정이 가능함

문서화 및 기여

     * 공식 문서가 별도의 사이트에서 제공 중, 중국어 버전과 추가 커스텀 리소스도 이용 가능함
     * 오픈소스 프로젝트답게 풀 리퀘스트 및 커뮤니티 기여를 적극적으로 환영함
     * MIT 라이선스로 제공되며, 누구나 자유롭게 수정 및 배포가 가능함

기술 정보 및 통계

     * Python(96.3%) , GLSL(3.7%) 등 주요 언어로 구성됨
     * GitHub 스타 8만 개, 포크 6,800개 이상으로 높은 인기와 커뮤니티 참여 확인 가능
     * 167명 이상의 다양한 기여자가 활동 중임

요약

     * Manim은 정확하고 세밀한 수학·과학 시각화에 특화된 프로그래밍 애니메이션 엔진임
     * 코드 기반 애니메이션이 필요한 교육용 영상 제작자, 데이터 시각화 전문가, 개발자에게 매우 유용함
     * 빠른 학습 곡선, 커스터마이즈 환경, 자유로운 기여와 커뮤니티 성장이 중요한 차별점임

        Hacker News 의견

     * 요즘 코딩 어시스턴트와 함께라면 정말 효과적으로 작동함을 경험함, “X 방정식이 Y로 변화하는 다이어그램이 필요함” 같은 프롬프트로 항상 한 번에 원하는 결과를 얻음, 단순한 문법과 수많은 오픈소스 manim 예제가 학습에 활용됐기 때문임, ai 코딩 에이전트가 시간을 얼마나 절약해주는지 보여주는 멋진 사례라 생각함, 결과 영상이 잘 나오기만 하면 세부 과정에는 신경 안 써도 되어서 그런 방식이 더 편리함
          + Grant Sanderson이 출연한 팟캐스트에서 들은 내용이 있는데, 그는 LLM으로 manim 코드를 자동 생성해봤지만 결과가 별로였다고 언급한 경험을 기억함, 결국 manim에 대한 숙련도나 기준이 우리가 생각하는 것과 Grant가 생각하는 것 사이에 큰 차이가 있음을 의미한다고 생각함
          + 문서에 RAG 기능을 결합하면 어떻게 될까 궁금함
     * Manim을 수업 발표 자료에 활용해봤는데 정말 즐거운 경험임, 많은 사람들이 그 스타일을 알아봐줬고 발표도 호평을 받음, 그리고 행운이 따라 Grant도 몇 년 전에 직접 만남, Manim을 사용했다고 말하자 진심으로 기뻐하던 모습이 인상적이었음, 그는 인류의 지식과 이해에 큰 공헌을 하는 멋진 사람임
     * 3b1b는 진정 인터넷 세계의 경이로움이라 생각함, 정말 아름다운 애니메이션과 신중하게 다듬어진 설명으로 감탄하게 됨, 다만 한 가지 이해가 안 가는 점은 왜 한 개의 라이브러리가 저렇게 다양한 개념을 애니메이션으로 구현할 수 있는지 궁금함, 전부 맞춤형 커스텀 작업일 것 같지만, 어쩌면 Grant는 더 높은 수준의 수학적 사고 위에서 일하는 것 같음
          + 왜 한 개의 라이브러리가 그렇게 다양한 개념을 구현할 수 있는지에 대한 설명으로, 핵심 프리미티브를 기반으로 커뮤니티에서 만들어진 아주 많은 오브젝트들이 존재하고, 그 오브젝트들을 출발점으로 커스터마이징하여 사용할 수 있기 때문임, manim 오브젝트 문서 참고
     * 대부분의 사람들에게는 커뮤니티 포크 버전 링크가 더 적합할 것으로 생각함, Manim Community GitHub
          + Grant가 만든 프로젝트는 정말 뛰어난데, 이는 단순한 오픈소스 라이브러리 유지관리자가 아니라 해당 도메인에 진심이고 전문성을 가진 사람이 실제 애플리케이션에 맞는 툴이 없어서 직접 개발하고 유지하는 경우임, 반면 커뮤니티 포크는 인프라를 구축하려는 사람들이 목적 자체에 맞추기보다 많은 유즈케이스에 맞추려 하다보니, 결국 원래 목적에서 점점 벗어나고 사용자 경험도 희생되는 결과를 가져오고 있다고 생각함, 전문 개발자가 직접 일 처리하려고 만드는 것과 단순 유지 및 홍보 목적으로 포크를 유지하는 것의 차이라 봄
          + 좋은 이유로 포크를 만들었는지 아니면 단순한 논란 때문인지 궁금함, 포크 관련 설명을 읽었지만 Grant가 계속 오리지널을 유지하는데도 정확한 이유를 모르겠음
     * 수학 중심이 아니라 일반 정보그래픽/차트/모션 그래픽에 집중된 코드 기반 애니메이션 렌더러를 열심히 찾고 있음, 기존 방법은 After Effects, Davinci Fusion, Blender, Cavalry 등이지만, 예전 PovRay와 이후 Manim을 경험해 본 뒤로 코드/텍스트 기반 모션 그래픽 툴에 대한 생각이 머릿속을 떠나지 않음, LLM과 결합하면 빛을 발할 것 같음, 최근 ChatGPT 기반 모션 그래픽 서비스가 나오기도 하지만, 그런 웹 기반 영상 생성기보다 내가 원하는 건 템플릿 코드/언어로, 어떤 데이터에도 반복적으로 렌더링 가능하고, 오프라인이나 자동화까지 가능한 방향의 툴임
     * 약 4년 전쯤 Manim을 사용해봤을 때, kwargs-itus라 불릴 만큼 모든 파라미터가 kwargs로 두어졌고 타입 어노테이션도 불가능했어서 사용이 불필요하게 어려웠음, 개선을 시도해봤지만 별 반응을 얻지 못해서 최근에는 어떻게 변했는지 궁금함
     * 이 프로젝트는 HN에 자주 올라와서 좋은 논의가 많으니, 토론 링크 대신 검색 링크 남김 hn의 manim 검색결과
          + manim 소프트웨어를 한 눈에 볼 수 있는 어썸 리스트가 있는지 궁금함, 아래에 자료들을 정리함
               o Manim: Math Animation,
               o Src: ManimCommunity/manim,
               o Docs: 공식 문서,
               o GitHub Topic: manim
               o manim 확장 프로젝트: manimML, manim-physics, chanim, manim-web(dart), JAnim(java), ranim(rust), manim-voiceover, git-sim, TheoremExplainAgent, reactive-manim, jupyter-manim, manim-sideview(vscode), manim-studio(Qt, Cairo)
               o ManimCommunity/awesome-manim에서 manim을 활용하는 크리에이터 리스트 확인 가능 awesome-manim
               o YouTube에서 manim으로 검색 추천 youtube manim 검색
          + HN에서는 보통 동일한 링크 중복이 허용되지 않는 걸로 알았는데, 중복 등록이 가능한지 궁금함
     * Manim 관련 예전 HN 논의 및 Show HN 링크 정리함
          + Python 라이브러리로 Manim 영상에 음성더빙 추가: 2023년 5월
          + Manim: explanatory math videos용 애니메이션 엔진 2022년 6월, 2022년 3월, 2021년 8월, 2021년 3월
          + Manim으로 파서 시각화기 개발 2021년 3월
          + Manim 코드 템플릿 2020년 11월
          + 간단한 Manim 애니메이션 엔진 소개 2020년 10월, 2019년 4월
     * Grant가 3blue1brown과 Manim으로 만들어낸 결과물은 정말 감탄스러움, 영상의 높은 퀄리티와 시각화 덕분에, 추상적인 주제임에도 불구하고 이해가 쉬움, README에 이미 언급됐지만 Grant가 Manim으로 작업하는 과정을 소개한 데모 영상(YouTube 링크)을 꼭 추천하고 싶음
     * 정말 멋진 프로젝트이며, 예전 Animation vs Math라는 인상적인 영상이 바로 떠올라서 클릭함 Animation vs Math 유튜브, 수학을 좋아하지만 거대한 그래프를 보여주는 건 사람들에게 꼭 좋은 방법은 아니라 느낌
          + 거대한 그래프 대신 3Blue1Brown은 영상 제작, Manim 배포, 그리고 커뮤니티 수학 유튜버 확장에 힘쓴 덕분에 수학 교육법 발전에 크게 기여했다고 생각함, 단순히 자신의 팬을 소유하려 하지 않고 공개적으로 생태계를 만들어줌, Sal Khan도 이 부분에서 칭찬받아 마땅함, 이런 노력들로 수학이 암기와 기호 조작이 아니라 흥미롭고 탐험적인 여정처럼 느껴지는 장르가 탄생했다고 봄
"
"https://news.hada.io/topic?id=22696","Claude Code가 왜 그렇게 좋은가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Claude Code가 왜 그렇게 좋은가

     * Claude Code는 사용성 측면에서 매우 뛰어난 AI 에이전트/워크플로우임
     * 건축적 단순성과 명확한 제어 루프 덕분에 디버깅과 유지관리가 수월한 경험 제공임
     * RAG 도입을 최소화하고 고도화된 프롬프트 및 문맥 파일을 적극 활용함으로써 LLM의 강점을 극대화함
     * 다양한 도구(툴) 와 명확한 가이드라인을 통해 작업 명확성·일관성을 유지함
     * 복잡성 대신 알기 쉬운 구조와 프롬프트 설계로, 자신만의 LLM 에이전트도 비슷하게 구현 가능한 장점 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * Claude Code(이하 CC) 는 현재 사용 가능한 AI 에이전트/코딩 워크플로우 가운데 가장 만족스러운 경험을 제공함
     * CC의 장점은 타겟에 맞는 코드 편집, 불필요한 방해 감소, 사용자의 제어권 유지를 통한 쾌적한 UX에 있음
     * Claude 4 모델과 특유의 Interleaved Thinking이 핵심 역할을 하지만, Cursor나 Github Copilot 등 동일한 모델 기반의 다른 도구보다도 더 적은 불편함을 제공함
     * 이 글은 CC의 구현 원리를 해부하고, 비슷한 경험을 제공하는 자체 LLM 에이전트를 구현하기 위한 실전 가이드를 제공함

Claude Code의 핵심 미덕: 단순함

     * 가장 큰 교훈은 ""단순하게 유지할 것(Keep Things Simple, Dummy) ""임
     * LLM 에이전트는 복잡한 구조(멀티 에이전트, 복잡한 RAG, 검증체계 등)를 도입하면 디버깅 및 개선이 극도로 어려워짐
     * CC는 단일 메인 루프, 단순한 툴셋, 한눈에 파악 가능한 구조를 채택하며, 모든 핵심 로직을 하나의 파일에 담아 불필요한 보일러플레이트와 복잡함을 배제함

왜 단순함이 중요한가

     * 멀티 에이전트 구조 대신 하나의 메인 스레드에서 대부분의 작업을 처리함
          + 히스토리 요약, UX를 위한 메시지 통합 등은 보조 형태로 적용
          + 복잡한 작업이 필요할 때는 자신을 복제하여 서브 에이전트로 분기(재귀적 분기 없음, 한 단계까지만 허용)
     * 할 일 목록(todo list) 을 적극 활용
          + 복잡한 문제는 서브에이전트에서 분할 처리하되, 결과를 메인 메시지 히스토리에 병합
          + 지나치게 추상적인 멀티 레이어 구조(다중 에이전트, 그래프 내비게이션)는 오히려 시스템 안정성·확장성 저하각 있음

경량화된 모델 적극 활용

     * claude-3-5-haiku 등 경량 LLM 모델을 대다수 요청에 사용
          + 대용량 파일 읽기, 웹페이지 파싱, git 히스토리 요약 등 많은 작업을 효율적으로 처리
          + 경량 모델 사용 시 비용이 최대 70~80%까지 절감됨
     * 모든 주요 기능 호출에 최적화된 모델 조합 활용을 추천

정교한 프롬프트 설계

     * CC의 시스템 프롬프트는 방대한 분량(약 2800 tokens) 과 다양한 섹션(톤&스타일, 작업관리, 도구 사용정책, OS/디렉토리 정보 등)으로 구성
          + claude.md 등 문맥 파일 전체를 항상 포함시켜 컨텍스트의 풍부함을 극대화
          + 시스템 프롬프트는 정책성 규칙, 예시, 유의점, 도구 사용 타이밍 등 매우 상세하게 안내

  XML과 Markdown의 동시 활용

     * 프롬프트 내에 XML 태그와 Markdown 구조를 동시에 사용
          + <system-reminder>, <good-example>, <bad-example> 등으로 디테일하고 조건 분기가 가능한 정보 전달
          + markdown heading으로 명료하게 섹션 구분

  컨텍스트 파일의 중요성

     * claude.md의 유무에 따라 CC 성능 차이가 극명하게 드러남
          + 코드베이스로부터 추론이 어려운 추가 규칙(폴더/라이브러리 제외, 선호 정책 등) 전달에 필수
          + MinusX도 minusx.md로 팀/사용자 선호도를 체계화하고 있음

RAG 최소화, LLM 서치 활용

     * CC는 RAG(Retrieval Augmented Generation) 대신, 실제 개발자처럼 ripgrep, jq, find 명령어 등 직접적인 코드 검색 기반 구조를 선호함
          + 이는 복잡한 RAG 구성에서 오는 숨겨진 실패 가능성(예: 유사도 함수, 재랭커, 청킹)에 대한 대안을 제공
          + LLM이 실제 코드 문맥을 직접 탐색·이해하는 구조로, 움직이는 부품 수 감소 및 RL 학습의 효율성까지 기대 가능

도구(tool) 설계 및 계층 구조

     * CC는 저수준(Bash, Read, Write), 중간수준(Edit, Grep, Glob), 고수준(Task, WebFetch 등) 도구를 모두 지원함
          + 사용 빈도·정확성 등을 고려해 필요한 도구를 개별적으로 분리
          + 툴의 설명, 예시, 사용 시점 등을 명확히 시스템 프롬프트에 고지함
     * 복잡한 작업은 Task나 고수준 툴을 통해 일관성 있게 관리

Explicit Todo 관리로 컨텍스트 유실 방지

     * 장기 실행 LLM 에이전트의 대표적 문제(문맥 유실, 방향 상실)를 해결하기 위해, CC는 직접 유지되는 Todo 리스트로 상태 관리
          + 멀티 에이전트 체계 대신, 모델이 자율적으로 Todo를 갱신하며 작업 방향성과 유연함 동시에 확보

에이전트의 톤, 스타일, 친화도 제어

     * 톤, 스타일, 적극성 등을 별도 섹션으로 관리
          + 불필요한 설명 자제, 이모지는 명시적으로 요청받을 때만 허용 등, 일관된 사용자 경험 설계
     * ""매우 중요(IMPORTANT) "", ""절대(NEVER) "", ""항상(ALWAYS) "" 등 강력한 수식어로 주의점 전달

판단 알고리듬·흐름 설계

     * LLM이 따라야 할 주요 알고리듬을 명확하게 서술 및 예시화
          + Do/Don't 리스트의 나열보다 플로우 차트, 단계별 체크리스트가 알고리듬 안정성 유지에 효과적임
          + 여러 지침·예시가 충돌할 가능성 고려, 역할 범위와 정책을 구조적으로 명시

디자인 패턴과 적용 실전 팁

     * 강력한 오피니언과 구조는 자체 에이전트 설계 시 바로 벤치마크로 삼을 만함
          + 머리가 복잡해지는 프레임워크 남용 대신, 단순하고 효과적인 구조 설계가 중요
     * 실제 MinusX에서도 다수 원리를 적용 중이며, 점차 확대될 계획임

결론

     * Claude Code의 가장 큰 교훈: “단순함이 최고의 힘”
     * 구조적 명확성, 의미 있는 프롬프트 설계, 경량 도구 조합이 강력한 LLM 에이전트를 가능하게 함
     * 자신만의 LLM 에이전트 개발 시, CC의 설계 철학과 가이드를 적극 참고할 가치가 높음

   너무조아 행보케 최고야 달달해 계속할랭

        Hacker News 의견

     * KISS 같은 단순함이 언제나 이김이라는 믿음임, 그리고 이 글을 잘 정리해줘서 유익했다는 소감임
     * Claude Code가 오픈소스가 아니라 아쉽지만, 내부 동작을 더 잘 파악할 수 있는 도구들이 있다는 소개임, 만약 진짜로 어떻게 작동하는지 관심 있다면 Claude Trace를 추천함
       https://github.com/badlogic/lemmy/tree/main/apps/claude-trace
       이 도구는 세션에서 사용된 모든 도구와 프롬프트를 보여주는 JSON 파일과 보기 쉽게 포맷된 HTML 파일을 만들어 줌
          + 오픈소스 대안을 찾는다면 OpenHands CLI를 체크해 보라는 제안임
            https://github.com/All-Hands-AI/OpenHands?tab=readme-ov-file
          + https://github.com/anthropics/claude-code
            시스템 프롬프트도 확인할 수 있음
            모델이 기본적으로 작업을 여러 단계로 쪼개서 인내심 있게 해결하도록 학습되어 있음, 실패 케이스에도 어느 정도 강건함
     * 요즘 멀티에이전트 시스템이 각광받는 시기에 LLM 중심 조직이 어떻게 접근하는지 알 수 있어서 유익했다는 의견임, 본인도 여러 디자인 관점을 일상에서 실험 중이라 동질감을 느낌
       주요 인사이트로는
       (1) 프롬프트가 길어도 좋고, 툴의 목적이나 도움 방식 등 기본 설명을 꼭 포함해야 함
       (2) 툴 호출은 매우 기초적인 부분이라 맥락을 더 반영해야 함(언제 사용할지, 언제 쓰지 않을지 등)
       (3) 시스템의 상태를 메시지로써 관리하는 건 괜찮음, fancy한 방법(데이터프레임 저장, 변수 파싱 등)도 생각해봤지만, 컨텍스트 윈도우가 길어진다면 메시지만으로도 충분하다고 여김
          + 프롬프트가 길면 좋긴 한데, 모델이 그걸 잘 처리하도록 최적화되어 있을 때만 해당됨, 본인이 Claude Code에서 다른 모델로 바꿔봤는데, 장문 프롬프트와 툴 사용 둘 다 광고만큼 잘되는 로컬 모델이 거의 없었다는 경험임
            OpenAI, Google Gemini 등의 모델도 시도해보았으나 Anthropic 모델만큼 잘 되지 않고 속도도 느리다고 느낌, 프롬프트가 길어질수록 툴을 까먹거나 잘못된 포맷으로 결과를 내놓는 현상을 겪음
          + (블로그 글 작성자) 기본 기능만 잘 활용해도 거의 99%의 상황에서 좋은 성능을 뽑을 수 있다는 의견임, 루프를 단순하게 유지하고 명확한 툴을 제공하는 것이 중요함, 기능이 겹쳐도 괜찮음
            명확성과 단순함이 최우선임
     * Google Gemini(특히 Pro 버전)가 Claude에 비해 어떤지 궁금하다는 질문임, Google의 많은 제품을 좋아하지만 자주 제품을 중단시키거나, 기업 통제(Chrome 등)에 대해 투박하게 나오는 점이나 검열 이슈가 걱정임
          + Gemini는 전체 저장소의 머지 파일을 통째로 투입해서 대화할 수 있을 때 특히 탁월함, 코드베이스 전체를 이해하는 수준이 놀라울 정도임, 아키텍처 설계에도 큰 도움을 줌, Claude는 이런 점에서 많이 부족함
            본인만의 전략으로는 Gemini로 프로젝트 요약 및 고수준 설계 플랜을 만든 뒤, gpt5로 개선과 상세 워크플로우 설계(예: XML 문서)까지 시키고 이를 다시 Claude에 넘김, 이것만으로도 Claude의 우왕좌왕 현상을 거의 피할 수 있음
          + Gemini Pro가 코딩에서 나쁘지는 않은데, 경험상 Claude가 터미널 관련 작업(CLI 등)에서 훨씬 나음, 대부분의 CLI들도 Claude를 많이 씀
            https://www.tbench.ai/leaderboard
          + 웹 UI(채팅)는 Gemini 2.5 Pro를 꽤 좋아함, 커맨드라인 도구에서는 Gemini code는 쓸모없고 Claude code는 대부분 느림
          + Gemini가 여러 함수 호출을 따라가야 하는 힘든 문제 디버깅에 더 뛰어남, Claude는 매번 예측 가능하고 지시를 잘 따름, 투두 리스트 관리를 특히 잘함
          + 예전엔 꽤 마음에 들었는데 최근에는 좀 더 멍청해진 느낌이 듦(혹시 나만 그런지 궁금함)
     * 본인은 기본 모델 자체가 실제 코딩 업무에 강해서 사용자들이 좋은 평가를 내린다고 생각함(일반 벤치마크용 문제와 다름), GitHub Copilot을 사용해보면 Claude가 OpenAI, Google 모델보다 월등히 뛰어남, 그 차이가 너무 커서 다른 모델들은 실질적으로 무쓸모하게 느껴지는 수준임
          + Anthropic은 강화 학습 중에 내부적으로 모델과 프롬프트를 최적화할 수 있어서, 글에서 얘기한 “기존 방식을 그대로 활용하라”는 조언이 Anthropic 모델에 더 적합하다고 생각함, 구독 모델 덕분에 루프 효율화에 강한 인센티브가 있음
          + 단지 기본 모델만의 차이로 설명할 수 없음, vs code에서 opus와 cline을 같이 사용할 때랑 Claude code를 쓸 때 생산성 차이를 구체적으로 수치로 설명하긴 어렵지만, CC를 쓸 때 더 많은 일을 해냄
          + 많은 칭찬을 보고 기대하며 Claude Code를 한 달 사용해봤는데 오히려 실망만 커졌음, Cursor 사이드바보다 낮은 경험을 제공했고, 본인이 뭘 잘못 쓰는 건가 싶음, 두 다른 코드베이스에서 계속 어이없는 코드 실수를 많이 내어서 아쉬웠음
     * 지금 Claude Code로 Security Onion에서 Elastic 관련 문제 디버깅을 시도하고 있는데, 몇 분 지나면 난해한 JS 코드가 쏟아지고 “Error: kill EPERM”라는 에러가 뜸
       로그를 보면 Node.js 프로세스를 죽여서 Claude 자체도 죽는 게 아닐까 싶음, 혹은 문제를 못 풀어서 Claude가 스스로 종료하는 느낌임
       어쨌든 프로세스가 유지되면 좀 더 도와줬으면 좋겠음
          + Claude와 localstack 중 일부는 서로 잘 맞지 않음, Rust에서는 의외로 꽤 잘함
            앞으로 LLM이 가장 잘 아는 언어/플랫폼/아키텍처가 점점 대세가 될 거라는 생각임, 예를 들어 nodejs를 LLM이 10배 더 잘 다루면 처음부터 엘릭서나 Go 대신 nodejs를 쓰는 게 합리적이라는 관점임, 주니어 개발자도 LLM 도움으로 미들급/시니어급처럼 활용 가능함
          + 수퍼유저 권한으로 프로세스를 실행하려고 sudo 쓸 때 타임아웃 돼서 저런 에러가 뜨는 경우임
          + 설치를 업그레이드하거나 기존 설치 파일을 지우고 다시 설치하면 문제 해결되는 경우도 있음, 본인은 이렇게 고침
          + 다른 LLM으로 넘어가면서 무슨 일이 일어났는지 확인한 경험 있음(공식 조언은 아님)
          + 본인은 Elasticsearch와 LLM 조합에서 좋은 결과를 얻어본 적이 없음, 대부분의 결과가 근거 없는 ‘환각’들이었음, 인터넷에 제대로 된 예시가 많지 않기 때문이라고 생각함
     * 본인은 스타트업의 첫 MVP 전체를 Claude Code로 만들었고 이제 유료 고객까지 유치함, 물론 SEV(서비스 중단) 사고가 일어나면 한순간에 무너질 수 있다는 근본적 불안이 있지만, 보안 취약점 수정, 테스트 주도 개발, 장기 로드맵에 따른 소프트웨어 아키텍처 설계를 위해 계속 Claude를 적극적으로 활용 중임
       이런 스토리가 앞으로 점점 흔해지길 바람
          + 제품이 궁금하니 링크를 공유해줄 수 있냐는 요청임, 실사용자 사례가 궁금함
          + “보안 취약점 수정”이라는 게 처음부터 Claude가 코드 작성하고 취약점도 만든 것이 아니냐는 농담 섞인 질문임
          + 테스트 주도 개발, 소프트웨어 설계와 같은 부분에서 구체적으로 어떻게 도움을 받았는지 예시를 설명해 달라는 요청임
          + 본인은 Claude Code에게 매달 은행 계좌로 돈을 송금하라고 시켰더니 실제로 해준다는 농담임
          + 구체적으로 무슨 걸 Claude Code로 만들었는지 공개해줬으면 좋겠다는 피드백임
     * “Keep things simple”이란 주장이 맞다면, 오히려 다소 복잡한 구성처럼 느껴지기도 한다는 의견임
       본인은 늘 원프롬프트씩 필요한 걸 묻는 단순한 방식으로 충분히 많은 작업을 해오고 있음
       논의된 복잡한 구조들이 정말 정교하게 만든 프롬프트 대비 어떤 추가적 가치를 주는지 확신하지 못하겠음
       예를 들어 ""새로 배우는 언어에서 while 루프 만드는 법""처럼 한 문장 프롬프트가 오히려 효율적일 수 있지 않냐는 생각임
       컨트롤 플로우가 오히려 불명확하게 느껴짐, LLM은 appendix(도구나 시스템 프롬프트) 부분을 제대로 쓰는지도 의문임, 요청이 너무 복잡하면 일부가 무시되거나 토큰 낭비 아닐까 생각함
       조각별로 개별 프롬프트를 던지는 식으로 프로그래밍하는 게 본인에겐 훨씬 자연스러움
       다른 방식을 쓴 사례나 프롬프트들을 한 번 보고 싶음
       실제로 LLM을 활용해 전체적인 프로그램을 사람들은 어떻게 만드는 건지 궁금함, 프롬프트별로 쪼개서 만드는 사례를 찾아 보고 싶음
          + 본인도 똑같이 쓰고 있어서, 다른 사람들의 답변이 궁금하다는 의견임
     * 참고로, 글 마지막에 minusx.com 링크가 있는데 보안 인증서가 553일 전에 만료된 상태임, 사이트가 유효하지 않으니 조심하라는 안내임
"
"https://news.hada.io/topic?id=22709","LLM 없이 게임을 만들 때 3개월, LLM과 함께라면 3일","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   LLM 없이 게임을 만들 때 3개월, LLM과 함께라면 3일

     * 15년 경력의 소프트웨어 엔지니어가 어린 시절 카드 게임을 Go 언어로 개발하는 경험을 공유
     * LLM(대형언어모델) 없이 “Truco”를 개발할 때는 UI 설계와 서버리스 배포 등 모든 문제를 수작업으로 해결하며 3개월 소요
     * “Escoba”를 만들 때는 LLM을 활용해 백엔드 코드 변환 및 구현 속도를 크게 단축, 프롬프트 1회 만에 대부분 동작 성공
     * 글 후반에서는 Tic-Tac-Toe 예제와 함께 Go 백엔드, WASM 변환, React 연동을 통해 누구나 게임을 만들 수 있는 단계별 가이드를 제공
     * 하지만 React 프런트엔드 및 WASM 기반의 게임 상태 관리는 여전히 직접 디버깅 및 구현 필요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * 15년 경력의 소프트웨어 엔지니어가 자신이 실제로 게임을 만들어 배포해본 적이 없음을 깨달음
     * 어린 시절 아르헨티나에서 친구들과 즐겼던 카드 게임 중 하나를 Go 언어로 개발하기로 결심

Truco: LLM 없이 3개월

     * 2024년 6월 18일부터 Truco라는 카드 게임을 Go 백엔드로 개발 시작. React는 최소한의 지식만으로 프론트엔드를 작성
     * UI 구현이 가장 큰 도전이었고, 서버를 제공하지 않기 위해 TinyGo를 활용해 WASM(WebAssembly)로 트랜스파일 후, 정적 파일을 GitHub Pages에 배포함
     * LLM이 없던 시기라, 모든 세부 사항을 직접 찾아내고 시행착오를 반복하며 약 3개월 동안 완성함
     * 광고나 수익 목적 없이 오로지 게임을 완성하려는 목적이었으며, 출시 1년이 지난 후에도 꾸준히 플레이되고 있음
          + Truco 플레이: https://marianogappa.github.io/truco-argentino
          + 백엔드(Golang): https://github.com/marianogappa/truco
          + 프런트엔드(React): https://github.com/marianogappa/truco-argentino

Escoba: LLM과 함께 3일

     * 1년 후, 가족을 만나러 아르헨티나를 방문하는 도중 조카에게 Escoba라는 두 번째로 인기 많은 카드 게임을 가르쳐줌
     * 이번에는 LLM(Claude) 을 활용하여, Truco의 백엔드를 복제한 뒤 Escoba의 규칙을 프롬프트로 설명, 코드 리팩터링을 요청함
     * 첫 프롬프트만에 거의 완벽하게 구현되었으며, 약간의 사소한 버그와 추가 기능만 수동으로 보완함
     * 프론트엔드는 여러 날 직접 구현/디버깅이 필요했음. LLM의 한계와 React 스킬, 게임 상태를 WASM에서 관리하는 특이한 환경 모두 도전 요소였음
          + Escoba 플레이: https://marianogappa.github.io/escoba-de-15
          + 백엔드(Golang): https://github.com/marianogappa/escoba
          + 프런트엔드(React): https://github.com/marianogappa/escoba-de-15

단계별: 자신의 게임 만드는 방법

     * 누군가가 직접 게임 개발을 시도할 수 있도록 최소한의 실습 가이드와 예제 코드를 소개함
     * Tic-Tac-Toe(틱택토) 예시 리포지토리를 제공, 이를 포크해서 시작 가능함
          + 백엔드: https://github.com/marianogappa/tictactoe-backend
          + 프론트엔드: https://github.com/marianogappa/tictactoe-frontend
          + 데모: https://marianogappa.github.io/tictactoe-frontend/

  백엔드 개발

     * 턴 기반 백엔드는 명확하게 기능을 설계 가능함
     * 서버리스 구조 유지, 사람이 서로 플레이하는 구조는 상용 서버가 없다면 피하는 것이 현실적인 선택임

  프론트엔드 개발

     * 프런트엔드는 다음과 같은 작업이 필요함
          + 백엔드에 신규 GameState 생성 요청
          + UI에서 상태 표시
          + 유효한 행동 선택 인터페이스 제공
          + 행동 적용 시 백엔드에 커맨드 전송
          + 봇 차례일 경우 백엔드에게 요청

  백엔드 WASM 전환

     * Go 코드를 WASM으로 빌드하려면 GOARCH=wasm GOOS=js go build 사용
     * 바이너리 크기가 큰 문제가 있을 수 있으므로, TinyGo를 활용해 사이즈를 줄임
     * 프런트엔드와 연결할 함수들을 내보내기 위해 Go에서 별도의 엔트리포인트(ex: main_wasm.go)를 작성해 빌드시 분기 처리함
     * 메인 함수에서 select {}로 블록킹하여 프로그램이 즉시 종료되지 않게 처리 필요

  백엔드-프런트 데이터 연동

     * Go의 자유형 struct인 GameState 등은 WASM에서 직접 serialize/deserialize 불가
     * 모든 데이터는 JSON 포맷으로 교환하는 방식이 필요
     * TinyGo에서 제공하는 문서 참고하여, 입력/출력 모두 JSON 직렬화로 주고받음

  프런트엔드-백엔드 인터페이스

     * 프런트엔드에서는 backend 함수들을 직접 호출
     * GameState는 WASM 내에서만 관리되고 프런트엔드는 뮤테이션 불가, 항상 backend가 진실의 소스임
     * WASM 재컴파일 후 파일 교체 필요, Makefile를 통해 자동화 예시 제공

  WASM 실행 환경

     * 실행을 위해서 wasm_exec.js를 head에 포함해야 하고, 해당 스크립트 활용해 인스턴스 생성 후 실행함

결론

     * 게임 제작은 즐거운 경험이었으며, Go와 WASM, React 조합으로 누구나 시도할 수 있는 접근법임
     * LLM의 도움으로 생산성이 크게 향상되었지만, 프론트엔드 역량과 디버깅 경험이 여전히 중요함
     * 누구나 이러한 구조로 직접 게임 개발에 도전할 수 있으니 시도해 볼 것

   리액트에서 document.getElementById를 쓴 코드가 생성된거면 어떤 LLM을 쓴걸까요....

   거의 제목 낚시 어그로 같이 느껴짐..
   ""처음부터 만들면 3개월, 비슷한 걸 또 만들면 3일""

   원 저자는 본인이 평소에 뭘 하는지 모르는 사람인가봐요

        Hacker News 의견

     * 내가 이 게시글에서 좋게 생각하는 부분은, 많은 개발자들이 간과하는 사실을 짚어줌에 있음. 게임 개발에서 코딩 자체가 병목이었던 적은 별로 없었음. 혼자 개발하는 사람도 AI 없이도 메커닉을 빠르게 만들 수 있음. 실제 어려운 부분은 눈에 보이지 않는 위의 여러 레이어, 예를 들어 게임 루프 밸런싱, 난이도 튜닝, 이질감 없는 에셋 제작, 유저의 관심을 5분 이상 유지시킬 만한 폴리싱 등임. 그래서 LLM 이후로 Steam에 뛰어난 게임이 넘쳐나지 않는 이유도 여기 있음. 기술이 장벽 하나를 낮췄지만, 더 큰 장벽들은 그대로임. 2010년대 Unity의 등장 때도 마찬가지였음. 엔진이 게임 개발을 민주화시켰지만 좋은 게임의 폭발적 증가가 아니라 시도의 숫자만 늘어났음. LLM은 코드에, 이미지 모델은 아트에 동일한 현상을 불러오고 있지만, 어떤 게임이 진짜
       재밌는지는 이들 툴이 알려주지 못함. 내게 흥미로운 질문은 AI가 구현뿐만 아니라 플레이테스트까지 한다면 어떤 일이 벌어질까임. 즉, 수천 번 루프를 돌리며, 어떤 메커닉이 시뮬레이션 플레이어를 계속 잡아두는지 알려주는 단계까지 발전하면 생산성 해킹을 넘어 디자인 파트너까지 역할이 확장될 것임. 아직은 그 단계는 아니지만, 이 글이 그 방향으로 가는 초기 데이터 포인트처럼 느껴짐
          + AI가 단순히 구현뿐 아니라 플레이테스트까지 해서 수천 번의 루프를 돌리며 플레이어를 얼마나 더 몰입시키는지 파악하게 된다면 어떤 미래가 올지 궁금하다는 의견에 대해, AI가 어떻게 플레이어를 시뮬레이션할 수 있으며 왜 진짜 사람이 무엇에 몰입할지 제대로 판단할 수 있을지 궁금증이 생김
          + Unity 2010년대 부상 예시에서 정말 좋은 게임 수가 그리 많지 않았다는 점에 반론을 제기하고 싶음. 사실 XBLA 시절과 비교하면 우리가 지금 갖고 있는 수많은 게임 볼륨은 Unity, Godot, Gamemaker, Renpy, RPG Maker 같은 툴 없이는 불가능했을 것임. 즉, 질적으로만이 아니라 양적으로도 분명히 비약적인 성장이 있었음
          + 내 기준에서 생성형 AI의 리트머스 테스트는 2D 픽셀 아트 액션 게임용 스프라이트시트 전체를 만들어내는 것임. 예를 들어 탱크나 주인공 움직임만이라도 완벽하게 뽑는 게 목표였는데, 아직까지 성공 사례를 본 적 없음
          + ""AI가 당신의 게임이 실제로 재밌는지 말해줄 수 없다""는 지적이 핵심 인사이트라고 생각함. AI는 인간이 실제로 게임을 경험하는 수준과 같은 방식으로, 혹은 어떤 다른 경험도 인간처럼 할 수 없음. 비슷한 게임에 대한 인간 평가 데이터를 보고 어느정도 예측만 할 수 있을 뿐임. 즉 AI가 당신의 게임을 즐길 수는 없음. 이런 본질이 앞으로 AI 시대 인력의 역할을 정의하게 될 것임. AI가 서류나 코드 같은 건 과거 데이터를 바탕으로 어느 정도 인간처럼 작성할 수 있지만, 진짜 인간만이 해낼 수 있는 의미있는 통합과 경험이 있음을 증명함. 인간만의 가치가 대체될 수 없는 지점이 있고, 다만 그 가치를 어떻게 바라볼지 다르게 접근해야 함
          + 이 패턴은 게임 개발 외 다른 분야에도 해당됨. 누구나 기대하듯 에이전트 기반 코딩에도 엄청난 가능성은 있지만, 일부 태스크(빠른 웹앱 데모, 소규모 라이브러리 연결 등)만 압도적으로 빠르게 해결되고 실제 대규모 소프트웨어엔 아직 적용이 미흡함. 모델이 학습된 방식이나 우리가 활용하는 노하우 모두 아직 부족함. 이런 상황이 놀랍진 않음. git도 등장 후 5년간은 엘리트 기업만 제대로 도입했고, 대중화되기까지 또 5년 걸렸음. 결국 지금은 많이 익숙해졌지만, LLM은 git보다 오히려 제대로 쓰기 어렵다고 봄. 만약 매 제품, OSS, 블로그 포스트마다 ""이제 끝났다, 모든 게 혁신됐다""라는 식의 과장만 덜했어도 더 빨리 발전하지 않았을까 생각함. 우리는 아직 시행착오와 실험속에 있고, 시간이 걸림. 너무 빠르게 판단하지 않아야 함. 만약 이미 모든 게
            해결된 단계였다면 적어도 훨씬 뛰어난 소프트웨어에 파묻혀 있어야 했겠지만, 아직은 겨우 균형을 맞추고 있는 수준임. 이 정도면 새로운 기술 등장 1~2년만에 꽤 인상적인 성과임
     * LLM이 3개월의 선행을 가진 것은 코드, 이전 게임을 템플릿으로 사용한 것, 그리고 무엇보다 손코딩할 때 쌓인 모든 경험과 실수를 바탕으로 했다는 점임
          + 처음엔 자극적인 제목일 줄 알았는데, 실제로 ""Truco 백엔드를 복제하고 Claude에게 Escoba 규칙을 길게 설명한 뒤 코드 리팩토링을 시켰다""는 부분에서 놀라움이 있었음. 사람이 직접 리팩토링하면 얼마나 걸릴까 궁금해짐. 3일 이상 걸릴 것 같기도 하면서도 아닐 수도 있음
          + 또 한 가지, 이 게임이 참가자의 첫 번째 게임이었다는 점이 중요함. 즉, 첫 시도할 땐 수많은 미지의 변수를 상대해야 하지만, 이미 한 번 경험하면서 얻은 인사이트와 노하우를 가지고 다시 시작한다면 LLM 없이도 3개월보다 훨씬 짧은 시간 내에 만들 수 있음
          + 사실 같은 프로젝트를 두 번, 세 번 반복하면 처음 몇 달 걸리던 것도 다음 번엔 1/3 정도로 줄어듦을 직접 경험함
     * <i>기침</i> 24시간 안에 개발해본 적도 있음 nordicgamejam.com 에 예시 있음. LLM, GenAI도 없고 Unity도 없던 시절에는 Microsoft XNA와 C#이 최선이었다는 점을 말하고 싶음. 아트도 대부분 페인트로 그린 손그림 수준이었음. 그래도 매년 즐거운 게임들이 충분히 탄생했고, Baba is You, Braid처럼 대중에게 알려진 경우도 있었음. 코딩이 병목은 아니었고, 개인적으로는 멤버 간의 소통이 진짜 bottleneck이라고 확신함
          + 팀 내 커뮤니케이션이 중요한 병목이라는 의견에 덧붙여서, 자기 마음 속에서의 '커뮤니케이션'도 놀랍게 어려운 경우가 많음을 언급하고 싶음
     * 이 댓글 흐름을 보면 게임 개발 경험 없는 글이 많아 보임. 사실 LLM이 쓰이는 프로젝트는 이미 트레이닝 데이터에 많이 존재하는 유형임. 예를 들어 프로그래밍 입문 과목에서 다루는 프로젝트이기도 하고, 남유럽 국가에는 이 블로그에서 말하는 카드 게임과 유사한 게임이 정말 많음. 나도 대학 1학년 때 아무 경험도 없이 Moon Patrol을 Python으로 처음부터 구현했는데 2~3개월, 주 3일 밤새며 코딩했음. 카드 게임 만드는 건 오히려 그보다 쉬움. LLM이 분명 유용한 부분은 있지만, 이런 간단한 예제는 LLM의 코딩 생산성이나 유용성 벤치마킹에는 적합하지 않음
     * LLM으로 며칠을 띄엄띄엄 들여다보며 이런 작업을 했었음: stacky 대략 이틀 정도 실제로 일한 셈임. 처음엔 그라운드업 개발로, 이후엔 브라운필드 식으로 작성하면서 심각하게 완성할 의도까진 없었음. 그런데 점점 더 디테일과 기능을 추가하다 보니 자꾸 아이디어가 늘어남(슈퍼 로테이션, DAS 등). 아직 전체 게임의 10~20% 수준으로 미완성임. WebGL 버전도 돌아감. 하지만 너무 궁극적인 테트리스 만들기에 도전했다간 소송 당할 것 같고, 그럴 라이선스 비용도 없으니 멈췄음. 결국 얻은 건 자신감과 경험임. 최근 HN에서 파라메트릭 함수에 관한 링크를 보고 1~2시간 만에 playground인 graphy도 만들어봤음. 역시 디테일에 시간 계속 쏟게 됨. 원하는 게 뭔지 명확하면 LLM과 함께 이런 작업은 꽤 즐길 만함
          + 테트리스 리이매지네이션 멋지다고 전함. 새 MacBook Pro M4 Max와 Firefox 조합에서는 코어가 100% 사용률을 보이고 팬이 엄청 돌아가니 참고 바람
     * 일정 수준 이상 취미 게임 개발을 꾸준히 해오며 여러 게임도 완성했음. 그런데 이 댓글 흐름 전체적으로 실제 게임 개발 경험이 많지 않은 것 같음. 게임 개발에서 코딩이 쉽다는 주장에 동의할 수 없음. 신선한 아이디어나 장르 변주 메커닉을 생각하는 건 오히려 쉽고, 실제로 구현하는 코드 부분이 훨씬 어렵다고 느낌. 예를 들어 멀티플레이어 뱀파이어 서바이버에 배틀메크 커스터마이징 추가한다고 상상하는 건 쉽지만, LLM만으로 그걸 구현하는 건 불가능에 가까움. 이번 사례는 이미 완전히 알려진 카드 게임 규칙이니 뱀 게임만큼이나 간단함. 글 작성자에 대한 비난은 아니고, 다만 많은 사람들이 실제 게임 개발 경험 없이 개발을 판단하고 있다는 점을 짚고 싶음
          + 코딩이 게임 개발에서 어려운 부분이라는 주장에 동의하지 않음. 물론 어려울 수 있지만, 진짜 어려운 건 새롭고 재미있는 아이디어를 생각해내는 쪽임. 좋은 아이디어만 있으면 작은 조각으로 쪼개고 반복만 하면 결국 구현할 수 있음. 진짜 난관은 백지 상태에서 무엇을 만들지 정하는 순간임. 산책을 해보거나, 별별 시도를 다 해보거나, 예술에서 수천 년 반복되던 문제임. 반면 코딩은 결국 공학적인 작업임. 나도 최근 게임 개발을 공부하면서 수학 공부까지 병행하는데, 벡터 수학이나 쿼터니언 배우는 건 오히려 ""내가 무슨 게임을 만들고 싶은가?""를 결정하는 것보다 훨씬 쉬웠음
          + 기본적으로 동의하지만, 나에게는 오히려 새로운 아이디어를 생각하거나 창의적 시도를 하는 부분이 항상 더 어려웠음. 웬만한 게임 메커닉은 뭐든 코딩할 수 있지만, 글쓰기/창의성 파트는 정말 힘듦. 그게 쉬운 사람이라면 진짜 복받은 것임. 모두에게 자연스러운 일은 아님
     * 완전 클라이언트 사이드 게임을 만들려는 거라면 왜 ""백엔드""가 필요하다는 식으로 쓰는지, 그리고 왜 전체 앱이 아닌 백엔드만 따로 기술을 다르게 쓰는지 궁금함
     * 소프트웨어 업계에서 아이디어가 쉽고 빨리 실현되는 영역은 거의 사라졌다고 느낌. 이미 경쟁이 너무 치열해 아무리 작은 시장을 잡아도 글로벌 VC, 글로벌 AI와 직접 경쟁하는 시대임. 과거엔 최소한 대기업이 관심 가지지 않는 틈새를 찾아낼 수는 있었음. 지금은 대형 VC든 니치든 무조건 전 세계와 경쟁해야 하니, 결국 극도로 복잡한 기술이 필요한 작은 시장이나, 수익성 낮고 실패 확률이 높고 라이프사이클이 짧은(대부분 게임이 바로 이분야) 영역만 남음. 전자의 경우엔 일일이 찾아가서 상품 설명하는 수준의 마케팅이 필요함. 게임업계 경험상, 개발 단계 진입 전에 이미 대박 사례란 게 ""백만 뷰 얻고 6개월 후 완전 사장되는 수준""임. 반복적인 수익이 거의 없다 보니 시작 자체가 너무나 의욕 상실임. Minecraft 같은 게임 만드는 건 거의 로또와
       다름없음. 하지만 게임 업계는 그나마 다른 소프트웨어 업종에 비해 실력주의임. 품질과 재미가 채택여부에 연관이 있긴 함. 다른 업계는 규제, 네트워크 효과로 인한 독점 또는 정부 통제 등 미로 같음. 차라리 초기에 정부가 ""이 분야는 이미 이 회사가 장악할 테니 스타트업 말고 딴 데로 가라""고 알려주면 1년 허비하는 일은 없을 거라 바람
     * 언제쯤이면 그린필드(완전 신규) 프로젝트가 에이전트 코딩 실력 벤치마킹에는 최악의 케이스라는 점을 자각할지 궁금함
     * LLM을 좋아하는 이유는 내가 프로그램을 머릿속으로 추상화하는 방식에 더 가깝게 코드를 다룰 수 있게 도와주기 때문임. 코드를 읽으면 마치 AST처럼 함수와 호출을 입력과 결과 추상 노드로 바꿔서 이해함. LLM 덕분에 이걸 코드로 거꾸로 구현하는 작업이 엄청 간단해짐. 일일이 아이디어에 맞는 코드 예제를 찾아 헤매거나 기억을 더듬을 필요 없이, LLM에게 WiFi 초기화 같은 반복 코드를 써달라 명령만 하면 됨. 결과적으로 레고 블록 쌓듯이 프로그램을 조립할 수 있게 됨. LLM 전에도 이 방식은 가능했지만 훨씬 많은 노력이 필요했음. 덕분에 요즘 다양한 언어를 넘나들며 손쉽게 개발 중임. 언어 내부 구조나 문법을 많이 배우지 못하지만, 바로 그 점이 핵심임. 언어나 문법은 프로그램 논리 흐름과는 상관없는 부차적 디테일임. 결국 기계어에서 어셈블리, C,
       점점 더 높은 수준 언어로 진화했듯, 이제 코딩이 아니라 '프로그래밍' 그 자체로 점점 가까워질 것임. 마지막 형태가 어떻게 될지는 아무도 모르지만, 분명 갈수록 '작성'에 쏟는 시간보다 '프로그래밍'에 집중하는 시간이 더 많아질 것임
          + 동의함. 예전에도 누군가는 C로 짜면 어셈블리 수준을 진짜 이해하지 못해서 언젠가 큰코다칠 거라 걱정했겠지만, 현실은 개발 생산성 도구의 발전임. AI 역시 너무 뻔한 반복작업을 개발자에게서 떼주는 것이 가장 큰 장점임. 예전엔 C에서 메모리 누수와 시그멘테이션 폴트 잡기에 시간을 허비했음. 이제 모던 언어에선 그럴 필요가 사라졌듯, 자잘한 구현 디테일 예제나 문서 찾는 일도 점점 사라짐. 결국 개발자는 더 창의적인 부분에 집중할 수 있게 됨
"
"https://news.hada.io/topic?id=22624","Show GN: jsonquotefixer: 잘못된 LLM JSON 구조 출력을 깔끔하게 정리해주는 파이썬 패키지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Show GN: jsonquotefixer: 잘못된 LLM JSON 구조 출력을 깔끔하게 정리해주는 파이썬 패키지

   안녕하세요!

   AI 모델을 활용하다 보면 구조화된 JSON 출력이 꼭 필요할 때가 많습니다.
   저 역시 여러 프로젝트를 진행하면서 JSON 출력을 활용할 상황이 자주 있었는데, 매번 같은 불편함을 겪었습니다.

   LLM 구조화 출력의 불편한 점은 크게 3가지 였습니다.
    1. 코드 블록(````json … ``` ) 처리
       대부분의 LLM은 JSON을 반환할 때 세 개의 백틱과 json 키워드로 감싼 코드 블록을 사용합니다.
       간단한 정규식으로도 처리할 수 있지만, 매번 반복하기 귀찮더군요. 패키지 차원에서 한 줄로 처리되면 훨씬 편리합니다.
    2. 중첩된 따옴표 문제
       JSON에서는 문자열을 쌍따옴표("")로 감싸야 합니다. 하지만 문자열 내부에 다시 인용부호가 등장하면 \"" 같은 이스케이프가 필요합니다.
       LLM은 이 구분을 종종 놓쳐 잘못된 JSON을 반환합니다. 이 문제를 자동으로 보정해주는 기능이 필요했습니다.
    3. 스마트 따옴표(유니코드 인용부호)
       LLM이 생성한 문장에는 흔히 “ ” 같은 스마트 따옴표가 포함됩니다.
       JSON 표준에는 맞지 않으므로, 이를 일반 쌍따옴표("")로 일괄 변환할 수 있어야 합니다.

   찾아보니 Node.js 기반의 ai-json-fixer 프로젝트는 있었지만, 파이썬 생태계에는 마땅한 도구가 없더군요.

   그래서 만들었습니다! (GPT 선생님과 함께요 ㅎㅎ)
   pip 으로 간단히 설치해서 사용할 수 있습니다.
   pip install jsonquotefixer

   한번 써보고 마구마구 비판해주세요 ㅎㅎ

   ipynb로 깃헙에 예시가 잘 보이면 좋겠네요

   시간오더는 n (시퀀스 길이) 입니다!
"
"https://news.hada.io/topic?id=22699","Show GN: 사업자등록이 필요 없는 PG 연동 데모","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Show GN: 사업자등록이 필요 없는 PG 연동 데모

   개인이 SaaS나 작은 물건을 팔려고 하면 해외는 Stripe, LemonSqueezy 등이 있지만 한국은 대부분 PG 가입에 사업자등록증+보증보험+각종 서류가 필요해 쉽지 않습니다.

   저도 간단한 MVP를 만들면서 이런 장벽 때문에 고생하다가, 최근에 개인도 결제를 받아주는 PAYAPP을 알게 되었습니다. 가입비나 별도 비용이 없어서 MVP 테스트용으로는 꽤 괜찮아 보입니다.

   다만 개발자 문서가 부실해서 직접 FastAPI 기반으로 샘플 코드를 만들어봤습니다.
   혹시 다른 대안 PG를 알고 계시면 추천도 환영합니다

   결제 연동하려면 문서찾으며 헤매는 시간이 대부분인데, 이렇게 동작하는 예제를 공유해주셔서 감사드립니다.

   PAYAPP 도 아래와 같이 정산요청시 서류 심사가 되어야 하는 것으로 나옵니다. 결국 동일하게 서류는 다 접수해야 정상적으로 이용이 가능하네요.

   첫 번째, 저희 페이앱과의 서류심사가 아직 완료되지 않으신 경우.
   두 번째, 보증보험을 가입하셔야 하는 조건이 되셔서 안내를 받으셨는데 아직 보증보험이 완료 처리되지 않으셨을 경우입니다.

   서류 심사가 필요한 것은 맞습니다만 사업자등록증이 필요한 것은 아닌 것으로 보입니다. https://payapp.kr/homepage/guide/guide3.html

   모든건 다 보증 개념입니다. 개인간 거래라고 간편하게 하고 싶다라고 생각해도
   구매자 입장에서는 내가 지불한 금액에 대해 누군가 보호를 해주지 못한다면... 과연 결제를 할수 있을까요? 신뢰가 가지 않는 결제앱을 통해 결제를 하고 싶은 구매자가 얼마나 있겠습니까.
   결제 회사도 사고에 대한 보증을.사업자등록증, 보증보험, 각종 서류 를 챙김으로써 리스크를 최소화 하는것이죠.
   저희 회사도 예약서비스인데 다른 업체에서 예약을 무제한 기간으로 받아서 돈만 챙기고 잠수 탔다고 최대 2개월 이내만 예약 가능으로 바뀌었습니다.

   개인간 결제 허용하면 탈세, 돈세탁 창구가 되기 때문이라고 알고 있습니다

   해외 같은 경우에는 Stripe, LemonSqueezy, PayPal 등 다양한 플랫폼에서는 개인간 결제를 받아주는 사례가 많이 있습니다. 사실 해외라고 탈세나 돈세탁에 대한 위험이 다를거라고 생각하진 않는데요,

   저도 자세히는 모르지만, 아마 국내 규제 혹은 사기/기만에 대해 PG사에서 책임져야 하는 기업의 부담으로 인해 개인 거래를 받아주지 않는것 같습니다.

   아마 그런 부분을 부담하기 위해 PayApp이라는 PG에서는 일반적인 PG보다 더 높은 수수료를 청구 하는것으로 보입니다.

   미국이나 해외의 경우에도 돈세탁 우려는 항상 존재할텐데 왜 해외에는 있고 국내에는 없을까요?

   좋은 프로젝트 공유 감사합니다. MVP 시연에 상당히 유용하겠네요.

   페이앱도 실계약을 진행하면 보증보험이 필요합니다...
   국내에 Stripe같은 PG사가 빨리 들어오면 좋겠습니다 ㅜㅜ
"
"https://news.hada.io/topic?id=22711","CNN에 비행기 사고 영상을 유출한 사건에 컴퓨터 사기법이 적용됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  CNN에 비행기 사고 영상을 유출한 사건에 컴퓨터 사기법이 적용됨

     * 컴퓨터 사기법이 CNN에 항공 사고 영상을 유출한 사건에 사용됨
     * 항공 사고 관련 기밀 영상이 언론사로 전달됨
     * 미국 당국이 정보 유출자를 형사 처벌 대상으로 조사함
     * 법적 쟁점으로는 정보 접근 권한과 언론 자유의 균형이 부각됨
     * IT 분야에서 내부 정보 보안과 법적 책임 강화 흐름이 주목받음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * CNN에 항공기 사고와 관련된 내부 영상이 유출되어 보도됨
     * 미국 법 집행기관이 해당 영상 유출 사건과 관련해 컴퓨터 사기 및 남용법(Computer Fraud and Abuse Act, CFAA) 을 적용하여 수사를 진행함
     * 이 법은 원래 해킹이나 무단 컴퓨터 접근을 다루기 위해 제정된 법임

법적 쟁점 및 논의

     * 유출자는 해당 기밀 영상에 접근 권한이 있었으나, 이를 언론에 제공함으로써 법적 책임으로 이어짐
     * 정부는 정보 보안 강화 및 엄정한 법 적용을 주장함
     * 반면, 언론계 및 시민단체는 언론의 공익적 보도 자유와 정보 공개의 필요성을 강조함

IT 업계에 미치는 영향

     * 기술 혁신과 정보 보호의 균형이 중요한 이슈로 대두됨
     * 내부자가 승인받지 않은 정보 공유를 할 경우 기업뿐만 아니라 개인도 법적 리스크를 크게 질 수 있음
     * IT 기업 내의 보안 정책 강화와 내부 정보 접근 통제의 필요성이 재조명됨

결론

     * 이 사건은 컴퓨터 사기법의 적용 범위 확대와 함께, 기술 종사자들의 법적·윤리적 책임이 증가하고 있음을 보여줌
     * 언론의 역할과 정보 보안의 경계에 대한 사회적 논의가 계속될 전망임

        Hacker News 의견

     * 이건 자기 검열이나 ‘예상 복종(anticipatory obedience)’을 유도하는 현상임을 느꼈음
       YouTube의 검열이 점점 심해지는 것 같음
       Pepe's Towing(로스앤젤레스의 주요 견인 회사)은 YouTube가 그들의 영상을 삭제했다고 불평했는데, 그 영상들은 대형 차량 사고 현장에서 차량을 복구하고 견인하는 과정을 자세히 보여줌
       직원들은 경찰처럼 바디캠을 착용하고, 크레인에도 카메라가 설치되어 있으며, 때로 DJI 드론까지 사용함
       드론은 절벽에서 차가 떨어지는 등 복잡한 리프팅 계획이 필요할 때 등장함
       이 영상의 주 목적은 보험사와의 비용 분쟁을 해결하기 위한 법적 증거 확보임
       하지만 PR 목적으로 YouTube 채널도 운영하기 시작했음
       거의 모든 영상이 LA의 공공 도로·고속도로 등 공개 장소에서 촬영되고, 경찰, CALTRANS, 소방서 등 다양한 공공기관 협조 하에 진행됨
       이는 공개 행위이며, 교통 흐름도 많고 때론 뉴스 헬기도 촬영함
       미국 수정헌법 1조(표현의 자유)에 의해 완전히 보호받을 만한 내용이고, YouTube의 공식 정책 위반도 아님
       YouTube의 검열 이유는 기업 이미지 보호임
       예전 영상에는 트럭 문에 적힌 회사 정보, 컨테이너 마킹, 차량 등록번호, 파손된 화물 등이 잘 나옴
       이제는 YouTube의 제재를 걱정해서 모든 식별 정보를 흐리게 처리함
       최근에는 멜론을 가득 실은 트레일러가 전복됐는데, 트럭 회사 정보뿐 아니라 멜론 상표까지 모자이크함
       Pepe's 직원들도 이게 터무니없다고 느끼지만, YouTube와 싸우고 싶지 않다는 이유에서 이런 조치를 취함
          + 조지 플로이드 사건 당시, 편집되지 않은 전체 영상을 어디에서도 볼 수 없었고, 오직 liveleak에서만 볼 수 있었음
            경찰차에 플로이드가 탑승하고 벗어나는 장면까지 포함된 유일한 곳이었음
            이처럼 사회적 파장을 일으킨 중요한 사건의 전체 영상이 왜 다공개되지 않았는지 의문이 생겼음
            냉소적으로 생각하는 입장에서는, 편집되지 않은 전체 영상이 당시 미디어가 전달하고자 했던 내러티브와 충돌했기 때문일 수 있다고 여김
          + 맞음, 한 기업이 다른 기업의 로고를 이용해 자신의 비즈니스를 홍보하는 경우임
            영상의 주제 자체가 해당 차량이기 때문에 여러 식별 정보가 그대로 들어감
            그러나 예를 들어 내 레스토랑이 경쟁 레스토랑의 위생점수가 낮다는 점을 강조하려고, 그곳의 로고를 내 영상에 가득 채운다면, 그게 공공장소에서 찍었다는 이유만으로 문제가 없다고 할 수는 없다고 봄
            (설명해주신 예시에만 한정해서 드리는 말임)
          + 영상의 목적이 단순히 견인 작업 과정을 기록하는 데 있다면, 개인식별정보를 포함할 필요가 없어 보임
            그런 정보는 삭제하는 게 타당하다고 생각함
            YouTube 규정 때문이 아니더라도, 이건 상식적 관행임
            여론조사, 논문 등에서도 개인적 정보는 통상적으로 가림
          + 서구권에서도 사람들이 Telegram 같은 메신저 그룹이나 유사 채널을 통해 진짜 정보를 찾으려고 몰릴 때가 언제 올지 궁금함
          + ‘예상 복종(anticipatory obedience)’에 대해 이야기하자면, 이 스레드의 나머지 의견들을 한 번 읽어 보길 추천함
            이 경우는 정부 관제 CCTV가 촬영한 영상임
            저작권이나 지적재산권, 금전적 피해 등은 해당 사항이 없고, 영상은 기밀도 아님
            본질적으로 공공 기록이며, 정보공개법(FOIA) 적용도 가능한 사안임
            최악의 경우 내부 정책 위반으로 사유해고가 있을 수 있지만(공공기관 직원은 보호장치가 더 많음), 대부분의 사람들이 고용주 요구와 조금만 벗어나도 불법 행위라고 과도하게 받아들이고 있음
            단순히 정책 위반에 대해 경고나 민사소송이 가능한데, 형사범죄로까지 확대하여 구속 가능성까지 이야기하는 과도한 경향이 생김
            이렇게 권위주의적 사고가 퍼진 원인 중에는 CFAA(Computer Fraud and Abuse Act)처럼 광범위하고 애매한 법률이 일조했다고 봄
            명확한 선을 긋지 않고 과도한 처벌 규정을 둔 법체계가 있다 보니, 누구라도 권한 가진 사람이 마음먹으면 처벌을 피하기 어려워짐
            결국 윗선의 권위에 휘둘리는 권위주의적 사고가 사회 전반에 뿌리내리게 되었음
            안타깝게 느껴지는 현상임
     * 이번 사건에서 CNN은 문제가 되지 않지만, CCTV 영상을 빼돌린 사람이 처벌받고 있음
       보안카메라 영상을 훔쳐서 넘기거나 판매했다면 분명히 문제임
       기사에서 적용된 법이 정확히 맞는지는 난 법을 잘 몰라서 확신 못함
       확실한 건, 회사의 민감 정보나 데이터를 외부에 유출한다면 법적으로 불이익을 피하기 어렵다는 점임
       이 기사가 회사 정책만 위반했다는 식으로 주장하는데, 쉽게 납득이 되지 않음
          + 모든 것은 ‘법’의 정의에 달려 있다고 생각함
            고용주에게 잘못을 하더라도(예: 업무를 제대로 수행하지 않음), 곧바로 형사 처벌 대상이 되는 것은 아님
            징계나 해고가 될 수는 있지만, 형사처벌까지 연결되진 않음
            그런데 여기서는 고용주에 대한 잘못 자체를 범죄로 간주하고 있음
          + 만약 내 업무용 기기에서 파일을 복사해 배포했다면, 통상적으로 NDA를 위반한 것이 되어 민사 소송건임
            NDA가 없다면, 해고 및 별도의 소송 가능성은 있어도 무조건 명확한 민사 사건만은 아님
            이런 상황이 반드시 형사 범죄가 되어야 하는 것은 아니라고 생각함
     * CNN이 CCTV 텍스트 중 촬영 위치를 적은 부분을 가리지 않은 게 핵심임
       저널리스트의 중요한 책임을 제대로 못한 경우임
       결국 이 실수로 다른 사람이 직장을 잃거나, 심지어 생명까지 영향을 받을 수 있음
       안타까운 상황임
          + 고정식 CCTV에서 나온 자료였기 때문에 금방 누군가 찾아낼 수 있었을 것이라 생각함
     * 미국 내 일부 주에는 매우 모호한 컴퓨터 남용 법률이 있음
       일리노이주는 예를 들어 네트워크의 이용약관(ToS)을 위반하면 범죄라고 규정함
       만약 피고가 실질적으로 무죄(법 조항이 해당 사항이 없는데)임에도 변호사가 no contest(다툼 없음)로 권유했다면, 유죄 판결이 뒤집힐 여지도 있음
       실제로 Subway Jared(서브웨이 광고 모델)처럼 변호사가 증거를 제대로 검토하지 않고 유죄를 권유해 일부 혐의가 번복된 사례도 있음
       더 황당한 경우로, 피고가 실제론 범죄가 아닌 행위에 유죄를 인정해도, 고의로 자신이 인정한 경우에는 이후 번복이 거의 불가능함
       과거에 피고인이 범죄가 아닌 일에 유죄를 인정했다가, 합의한 이상 번복이 불가했던 사례들도 기억남
     * 관련자들이 지난 15년간 컴퓨터 근처에 3미터만 가보지 않았는지 의문이 듦
       CFAA가 무조건 적용될 수 있음
       마치 우편 사기나 전신 사기처럼, 범죄 대부분에서 적용 가능성 높음
       범죄가 아니거나 그래선 안 되는 경우여도, 그들이 원하면 얼마든지 적용될 수 있음
          + 실제 법 조항을 읽어봤는지 모르겠음
            실제로는 거의 적용이 안 되는 내용임
     * 내부제보(whistleblowing)가 아닌 상황에서 조직의 내부 데이터를 유출했다면, 해고는 물론 향후 고용주로부터 신뢰를 얻기도 힘들 거라 생각함
       하지만 이것을 CFAA에 따른 범죄라고 보는 건 과도한 해석임
          + 해고와 손해배상 소송 가능성은 분명 있음
            하지만 휴대폰으로 보안모니터를 영상촬영했다고 해서 그걸 ‘컴퓨터 사기’나 ‘무단침입’으로 규정하는 것은 명백히 과장된 일임
     * 누군가 구경거리 영상이 무조건 공익에 부합한다는 주장도 못 믿겠음
       이건 단연코 ‘펜타곤 문서’처럼 중대한 사안과는 비교할 수 없는 일임
     * 영상을 유출한 사람은 경찰 통신센터에서 근무했음
       이런 업무는 정보 유출이 원칙적으로 금지되어 있음
       응급구조대원이나 외과의사가 영상을 무단유출하는 것도 허용될 수 없음
       사건은 여러 각도에서 이미 여러 자료와 목격자, 텔레메트리 데이터 등도 있었음
       누가 JFK 암살 당시 ‘자프루더 필름’처럼 공개가 필요한 유일한 자료를 가져온 것도 아님
          + 사건이 공개적으로 벌어졌는데 굳이 그 영상을 공개하면 무슨 해가 되는지 궁금함
            어차피 정보공개법(FOIA)에 따라 공개될 수 있는 영상임
          + 그 정보가 회사 고유 정보이거나 기밀 정보였는지 궁금함
"
"https://news.hada.io/topic?id=22732","문제가 있으면 IP 레벨에서 차단하세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         문제가 있으면 IP 레벨에서 차단하세요

     * 최근 웹 트래픽 분석중에 Thinkbot이라는 웹봇이 가장 많은 트래픽을 발생시킨 것을 발견함
     * 해당 봇은 robots.txt를 무시하며, 자기소개 문구도 단순히 “문제 있으면 IP 차단해라”라는 식으로 매우 불성실함
     * 한 달 동안 74개의 서로 다른 IP를 사용했고, 이는 41개 네트워크 블록에 걸쳐 분산되어 있었음
     * 조사 결과 이 모든 네트워크 블록은 Tencent 소유였으며, 이게 Great Firewall 비용 전가 가능성과 연결되는건지 의심이 생김
     * 결국 약 47만 개 이상의 IP를 포함하는 방대한 차단 규칙을 추가했음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Thinkbot의 등장

     * 웹 트래픽 분석 중 Thinkbot이라는 이름의 웹봇이 상위 점유율을 차지한 것을 발견함
     * User-Agent 문자열은 다음과 같이 불성실했음

     “Mozilla/5.0 (compatible; Thinkbot/0.5.8; +In_the_test_phase,_if_the_Thinkbot_brings_you_trouble,_please_block_its_IP_address._Thank_you.)”.
          + “테스트 단계에서 문제가 되면 IP 차단해 주세요”라는 문구 외에 참조 URL조차 없음
     * robots.txt 파일을 전혀 존중하지 않고 크롤링을 진행함
     * 웹사이트 운영자로서 이를 차단하려 해도, 단일 IP가 아니라 74개의 IP 주소를 사용
     * 이를 역추적해 ASN을 조회한 결과, 41개의 네트워크 블록에서 발생한 것임
     * 이는 단순한 단일 IP 차단으로는 방어가 불가능함을 의미

Tencent 연관성

     * 이 41개의 네트워크 블록은 모두 Tencent 소유였음
     * 저자는 중국 정부가 이를 묵인하거나 장려할 수 있으며, 외부 세계에 Great Firewall 비용을 전가하려는 시도로 해석할 수 있다고 의심함
     * 중국 내에서는 콘텐츠 수집이 허용되고, 외부에서 차단되더라도 CCP 입장에서는 문제가 없지만, 차단을 시도하는 다른 국가·사이트에는 부담으로 작용함

방화벽 차단 조치

     * 저자는 직접 badbots 방화벽 규칙에 Tencent 네트워크 블록을 추가함
     * 예시: 43.130.0.0/18, 101.32.0.0/20, 150.109.96.0/19 등
     * 총 40여 개의 네트워크 블록을 추가했으며, 이게 Tencent가 소유한 IP 전체를 포괄하지는 않으나 476,590개 이상의 고유 IP를 포함

결론과 비유

     * 저자는 이러한 상황을 “인터넷에서는 더 이상 좋은 것을 가질 수 없다”라는 현실로 표현
     * 단순한 봇 트래픽 차단을 넘어, 인터넷 생태계 전반의 신뢰 저하와 불가피한 방어적 대응을 보여주는 사례

   짱..

        Hacker News 의견

     * 웹 크롤러를 개발하면서 최대한 친화적으로 만들고자 노력했음. robots.txt를 엄격하게 확인하고, 느리게 크롤링하며, User Agent 문자열에 명확히 신원을 밝히고, 단일 IP 주소만 사용함. 그런데 robots.txt 파일 자체에 적용되는 각종 봇 방지 트릭을 경험하게 됨. 최근엔 slow loris 공격처럼 robots.txt가 매우 느리게 다운로드되어, 실수로 404로 처리하고 계속 크롤링하게 됨. 그 경험 이후 타임아웃 시 Disallow /로 처리하는 코드로 바꿈. 아이러니하게도, 규칙을 잘 지키려고 해도 anti-bot 도구를 탐지하는 코드를 쓰게 되는 상황임
          + 도둑을 막으려고 초인종을 숨기는 것과 같다는 생각임
          + 서버 애플리케이션에서와 마찬가지로, 클라이언트에서도 상대방이 악의적이거나 잘못된 행동을 보이면 조용히 TCP 연결을 끊음. 상대방이 한동안 자원을 낭비하다가 스스로 상황을 인식해야 함
          + 이런 현상이 일부러 그런 게 아닐 가능성이 높다고 생각함. robots.txt 규칙을 안 지키는 악성 봇들은 애초에 파일을 다운로드하지도 않으니까, 악의보다는 실수나 무능일 수도 있음
          + 규칙을 지키려는 사람만 처벌하는 제재는 오히려 역효과라는 생각임
          + 규칙을 잘 지키려 노력하는 점을 높이 평가함. robots.txt에 제한을 거는 게 실수일 수도 있지만, 어떤 크롤러들은 robots.txt에서 더 흥미로운 페이지를 찾기 때문에 이를 늦추는 방법이 빠르게 문제를 피하는 데 도움이 될 수 있음. 결국 이런 방식이 봇의 정보를 차단하고 속도를 늦추는데, 사이트 운영자 입장에선 악성 봇 때문에 피해를 보니 정직한 봇과의 구분에 크게 신경 쓰지 않을 수밖에 없음
     * 봇 때문에 심각하게 피해를 보는 웹사이트는 어떤 공통점이 있는지 궁금함. 나는 .com TLD로 집에서 웹 서버를 여러 해 운영했으며, 관련 키워드에서 구글 검색 순위도 높은 편이고, 라우터나 서버에 특별한 봇 차단 설정도 없었음. 호기심에 봇 요청만 세 본 적은 있으나, 대부분 포트 스캔이나 인덱스 페이지 정도만 가져가고, 동적으로 불러오는 링크들은 거의 따라오지 않음. Apache 2 시절이나 Axum으로 여러 사이트 운영 중인 지금이나 봇으로 인한 눈에 띄는 영향이 없음. 아마 디렉토리 리스팅 때문일까 궁금하며, 설명을 들으면 좋겠음
     * 웹 스크래핑 이슈에 많은 똑똑한 사람들이 과하게 집착하는 느낌임. 만약 봇 활동이 실제로 사이트에 엄청난 부하나 문제를 일으키지 않는 이상(물론 예외는 있겠지만), 대부분은 의미 없는 ‘깃발 뺏기’ 게임에 불과함. 이 게임의 차이는 결국 상대 깃발을 찾지 못하고, 그저 시간만 잃는다는 점임. 가장 좋은 대처는 확산되는, 식별이 어려운 참가자들이 부하를 유발하더라도 빠르고 잘 설계된 웹 제품을 만드는 것이라고 봄. 현실적으로 이 접근법은 실제 인간 사용자의 만족도도 크게 높여줌
          + 경험상 이 문제의 심각성을 모를 수도 있다고 생각함. 이전 직장에서 웹 앱의 애플리케이션 성능을 담당했었는데, 사용자 일부가 번개처럼 빠르지만 대부분은 느림. 성능 로그를 분석하다 보니, 전체 요청의 60%가 알려진 봇임(엉뚱한 봇은 제외). 심지어 몇몇 회사는 서비스에 DoS 공격을 가하기도 했고, 이로 인해 사이트가 내려갔던 적 있음. 문제는 봇이 항상 캐시된 응답만 가져가기 때문에 실제 사용자 대화는 LRU 캐시에서 계속 밀려남. 어떤 봇은 방문했던 모든 페이지를 몇 분마다 재스크래핑하고, 어떤 봇은 스루풋을 올리다 서비스가 느려질 때까지 밀어붙임. 때로는 자바스크립트 실행 및 폼 제출까지 시도함. 구글봇만 유일하게 예의바르게 동작함. 실제 유입 트래픽의 40%가 단 한 개의 URL에 집중되는 등, 봇 때문에 얻는 이득도 거의 없음. 뒤늦게
            알게 된 사실이지만, 많은 봇이 초창기 AI 기업 데이터 수집용임을 깨달음
          + 친구가 친구들끼리만 사용하는 작고 공개된 gitea 인스턴스를 운영 중임. 그런데 매시간 수천 건의 봇 요청이 옴. 서비스에 직접 영향을 주지 않아도, 최소한 괴롭힘처럼 느껴짐
          + 나는 빠른 웹 제품을 만들기 위해 데이터를 프리미엄으로 지불해서 얻음. 그래서 이런 엔터티를 차단하면 시간 낭비가 아니라 실제로 대역폭과 서버비용을 절약할 수 있음. 덕분에 진짜 고객들도 컨텐츠가 스크랩되지 않아도 아무런 불편함 없이 같은 혜택을 누림. 누군가에게 착취당한다고 생각하는 논리를 이해하지 못하겠음
          + ‘덤불 게임(capture the flag)’보다는 두더지 잡기 게임에 가깝다고 봄. 개인적 경험으론, '나쁜 봇'을 식별해 차단하려고 하는 포럼에서는 항상 더 많은 봇이 등장해 끝이 없음
          + 우리 중에 똑똑한 사람이 많긴 하지만 기술 문제에 과도하게 집착하는 경향도 있음. 아무것도 안 하면 계속 신경 쓰일 것 같고, 차단이라도 하면 짜증이 덜함
     * robots.txt를 진지하게 받아들이는 사람이 Hacker News에 생각보다 많아 늘 놀라움. 좋은 의도를 가진 사람이 많다는 점이 참 인상적임. 하지만 robots.txt가 실질적 해결책은 아님. 사람들이 robots.txt라는 걸 알아야 하고, 크롤러에 robots.txt 검사 코드를 추가해야 하니까 복잡함이 따름. 다른 실질적인 솔루션이 있는지 궁금함. ‘마이크로페이먼트’나 ‘실명 인증을 위한 대형 머클 트리’ 등등은 오래전부터 거론됐지만 실제로 구현된 적은 없음
          + robots.txt를 모르는 봇 개발자는 거의 없을 것 같음. 자기 프로젝트는 모두의 규칙을 무시해도 되는 특별한 경우라고 착각하는 사람들이 있을 뿐임
          + robots.txt는 법적 강제력이 없음
     * 우리 로그에도 그런 봇 패턴이 보임. 꽤 거슬리지만 그래도 봇임을 밝히고 실제 트래픽은 많지 않음. 대부분의 트래픽은 실제 브라우저인 척하거나, 브라질과 아시아 여러 IP에서 유입되는 봇임. 봇 차단을 위해 최근 일주일간 갖은 시도를 해봤기에, 도움이 될 만한 경험을 공유함.
          + 수백 개의 IP에서 하루에 몇 번씩만 요청이 오지만, 전부 실제 UA로 가장함
          + referrer URL을 거의 보내지 않지만, Huawei Cloud 봇은 referrer를 보내기도 함(대신 트래픽은 적음)
          + 주요 시도는 id=가 포함된 URL의 대역폭을 제한(1Kb/s)해서 느려지면 포기할 줄 알았으나, 봇들은 신경도 안 쓰고 계속 요청함
          + keep-alive 연결에도 적응해서 /cgit/에선 keep-alive를 아예 껐지만, 역시 연결을 다 차지해버림
          + 현재 쓰는 방법은 id=가 포함된 URL은 notbot 쿼리 문자열이 있는 경우만 허용, referrer가 없으면 403 메시지에 정식 유저라면 notbot 파라미터를 추가하라는 식으로 안내함. 이 방법으로 로드는 줄이고 정식 사용자 연결도 개선됐지만, 봇은 여전히 요청하고 403만 계속 받음
          + 결론적으로, 사이트별 특화된 ad hoc 방식을 쓰거나 Cloudflare 같은 충분한 자원을 가진 곳에 맡기는 두 가지 방법밖에 없는 듯함. 표준적인 봇 차단 솔루션은 자원 많은 쪽에서는 충분히 우회하거나 비용을 감당할 수 있기 때문에 한계가 있음
          + MSIE 3.0이나 HP-UX 같은 잘 안 쓰는 UA 서브스트링을 403으로 미리 차단하는 방법도 있음. 이후 403 로그를 모아 문제가 되는 ASN만 별도로 차단하는 식으로 두더지 잡기를 반복함
          + 나는 djbwares의 Bernstein publicfile 계열 소프트웨어를 씀. static GEMINI UCSPI-SSL 도구도 추가했으며, GEMINI 스펙에서 따온 아이디어로 요청 URL 내 fragment와 쿼리파라미터를 모두 금지함. 이유는 GEMINI에서 금지하는 논리와 동일하게 static HTTP 서비스에도 적용할 수 있음. 서버 설정상 쿼리파라미터를 제대로 처리하려면 특이한 파일명 여러 개를 별도로 생성해야 하며 현실적으로 힘듦. 이 아이디어 덕분에 CGI나 PHP 취약점 공격 시도가 애초에 파일시스템에 닿지도 못하고, 요청을 분해하는 단계에서 걸러짐. static 사이트 운영자는 GEMINI처럼 쿼리파라미터까지 차단하는 걸 추천함. 물론 static 사이트 카테고리에서 쿼리파라미터를 진짜로 쓰고 싶으면 예외임
     * 언젠가부터 IP 범위를 화이트리스트로 두는 방식이 실제로 가능할지 궁금해짐. adblock 리스트처럼 커뮤니티가 노력해서 관리하는 접근도 상상해봄
          + 불행하게도 잘 지키는 봇일수록 안정적인 IP를 쓰며, 악성 행위자는 언제든 가정용 프록시를 씀. 주거 프록시 IP를 금지하면 실제 사용자에게 피해가 가고, 악성 사용자는 바로 다른 곳을 씀. 실제 수천 개 IP를 상대해본 경험상, IP 기반 정보만으로는 판단이 힘들고 추가 정보가 필요함
          + Pokémon Go 회사도 출시 직후 스크래핑을 막으려 이 방법을 시도했음. 세 가지 IP 카테고리로 나누고, 블랙리스트(Google Cloud, AWS 등), 비신뢰 IP(주거지), 화이트리스트(정상적인 IPv4 등) 분류함. 처음엔 주요 스크래퍼를 걸러냈지만, 가장 규모 큰 스크래퍼는 모뎀 단말기 농장을 운영하면서 이를 우회했음. 그래서 일반 사용자는 지도 없이 게임을 포기하고, 하드코어 플레이어는 오히려 스크래퍼 사용량을 유료로 늘림. 결국 서버에 더 큰 부하가 옴. Pokémon Go가 했던 여러 잘못된 결정 중 하나로 봄
          + 이미 많은 미국 회사들이 이걸 시행 중임. 그런데 해외에 있을 때에도 서비스를 해지할 방법을 제공하지 않으면서 요금을 계속 받는 경우, 이건 불합리한 점임
          + 화이트리스트와 블랙리스트는 반드시 이분법적으로 선택할 필요가 없음. 대부분의 트래픽은 회색지대에서 발생함. 특정 IP를 화이트리스트에 넣었는데 이상징후가 감지된다면, 화이트리스트에서 제거하고, 공지하고, 통보받고, 해소 사실까지 상호 조율해야 하는데, 이게 현실적으로 매우 복잡함. 오직 신뢰관계 있는 파트너 사이에서만 화이트리스트가 효과적임. 블랙리스트는 문제 많은 주소, 또는 CIA, 러시아, 중국, 클라우드 사업자 등을 막는 데 유용함. 기업 내부 전용 호스트 등 robust한 반남용 체계가 있는 곳만 화이트리스트로 두는 것이 현실적인 접근임
          + 나는 오픈 소스 프로젝트 GoodBots를 통해 긍정적인 봇 화이트리스트를 만들고 있음. PR 대환영임
     * 모든 요청에 커스텀 헤더를 추가해서 보내고, 그 외의 모든 요청은 차단하는 방법을 사용함
     * 외부는 Cloudflare 프록시를 쓰고, 내부적으로는 Crowdsec과 Modsecurity CRS를 Traefik 앞에 둠. 오탐을 줄이도록 조정한 후 매우 안정적으로 운영 중임. 임시 차단된 IP와 보고된 IP는 Crowdsec으로 보낸 뒤, Discord 채널에 로그로 올림. 하루 평균 수십 개의 다른 IP를 차단함. 체감상 비공개 리소스에 접근하거나 CVE를 노리는 시도는 미국 IP가 중국 IP보다 훨씬 많음. 공개 컨텐츠 크롤링은 신경 안 쓰고, 나머지는 전부 SSO 또는 인트라넷에서만 접근 가능함. 특정 국가를 차단하는 것보다 exploit이나 플러딩 시도 자체만 막는 게 더 효과적임
          + Crowdsec 같은 방식은 매력적이지만, 서버의 모든 트래픽을 영리기업에 넘기는 건 큰 리스크라고 생각함
          + 심각한 공격 시도는 결국 Cloudflare WAF 같은 곳에서 이미 막힐 것임
     * 많은 아파트 건물들이 몇 개의 IP 주소로만 외부에 나갈 수 있음(Carrier-grade NAT 참고)
          + 그래서 IP 차단으로 인해 오탐이 발생함. 하지만 이런 원칙 적용은 가치가 있다고 봄. 결국 이웃에 대한 책임이 따름
     * 내 트래픽의 절반 이상이 Bing, Claude, 그리고 페이스북 봇임. 이들은 주요 트래픽 기여자는 아니지만, 서버 자원만 잡아먹고 사이트가 느려질 때 주요 원인이 됨(AI, 마이크로소프트, 페이스북이 상식도 무시할 때가 많음). 중국 등은 악성 트래픽의 일부일 뿐, 진짜 문제는 robots.txt나 DNS rate limit을 무시하는 미국계 기업임
          + 궁금한 질문이 많고, 이 모든 걸 Claude에 묻고 있음. 아직 이런 인프라는 없지만, 선택한 LLM이 내 멍청한 질문 때문에 자원을 사용하는 만큼 사이트 운영자에게 보상하는 비즈니스 모델을 원함. 마치 YouTube Premium이 크리에이터에게 돈을 주듯이 말임. 실질적으론 불가능할 것 같긴 함

   또중국이네요
"
"https://news.hada.io/topic?id=22637","구형 나선에 대한 호기심으로 만들어진 시각화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        구형 나선에 대한 호기심으로 만들어진 시각화

     * 3D 공간에서 객체 이동을 파라메트릭 함수로 표현하는 방법에 대한 개념 소개임
     * 원, 나선 그리고 구형 나선 경로까지 점차 복잡한 경로를 수학적으로 만드는 과정 설명임
     * 각 좌표축(x, y, z)을 시간의 함수로 정의함으로써 다양한 움직임이 구현 가능함
     * 특별히 구형 나선의 경우, 반지름 변화를 주는 삼각함수의 곱셈으로 3차원 나선 경로 생성임
     * 이러한 방식은 임의의 경로로 객체를 이동시킬 수 있음을 보여주는 창의적인 예시임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

3D 공간에서의 객체 이동 탐구

   이 글은 3D 공간에서 객체를 이동시키는 다양한 방법과, 특히 구형 나선(spherical helix) 경로를 어떻게 수학적으로 정의하고 구현하는지에 대한 개인적인 탐구 결과임

  헬릭스와 3차원 이동의 기초

     * 헬릭스는 스프링처럼 회전하면서 감기는 3차원 구조를 의미함
     * 구형 나선은 구의 표면을 따라 나선 형태로 돈다는 개념임
     * 3D 공간에서 객체의 위치는 x, y, z 3개 축의 좌표로 결정됨
          + x축: 좌우 이동을 담당
          + y축: 상하 이동에 해당
          + z축: 앞뒤(깊이) 방향 변화
     * 객체의 위치를 시간(t) 에 따라 수학 함수를 사용해서 정의하면 이동 경로를 만들 수 있음

  파라메트릭 함수와 단순한 경로 예시

     * 예: x 위치를 10 * cos(πt/2)로 정의하면, 2초마다 -10에서 10까지 왕복하는 코사인 파형 이동이 됨
     * 같은 방식으로 y 위치를 10 * cos(πt/2)로 지정하면 수직 방향 왕복 운동도 가능함
     * x, y에 서로 다른 함수(예: x = 10 * cos(πt/2), y = 10 * sin(πt/2))를 쓰면 서로 위상이 다른 운동이 되고, 이 둘을 합치면 원형 경로가 생성됨
     * 함수에 시간이 비례하는 항(예: x = 0.03 * t * cos(πt/2))을 곱하면, 반지름이 점점 커지는 패턴 즉 나선(spiral) 경로를 만들 수 있음

  구형 나선(spherical helix) 경로 만들기

     * 기존 평면의 나선과 달리 구형 나선은 3차원 경로를 필요로 함
          + z의 값에 10 * cos(0.02 * πt) 등을 사용해 앞뒤 위치를 점진적으로 변화시킬 수 있음
     * x, y에는 sin(0.02 * πt) 같은 삼각함수 곱 활용으로, 반지름이 중간에서 가장 커지고 양끝에서 작아지는 효과 연출
     * x와 y 모두 이러한 곱을 적용함으로써, 원운동을 하면서 구의 표면(즉, 3차원적으로) 나선을 따라 이동하는 경로 생성 가능
     * 이와 같은 함수 조합으로 구형 나선 경로의 수학적 구현 완성

  요약 및 활용

     * 모든 3D 경로는 x, y, z를 각각 시간의 파라메트릭 함수로 정의해 만들 수 있음
     * 이는 단순 원, 나선부터 복잡한 경로까지 수학적으로 지정 가능함을 의미함
     * 이러한 접근을 통해 복잡한 운동도 사실상 혼돈이 아닌 명확히 정의된 수학 경로임을 시각적으로 이해할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   visualrambling.space는 Damar가 다양한 주제를 배우고 시각적으로 이야기하는 개인 프로젝트임

        Hacker News 의견

     * 예전 해양 항해에는 이러한 곡선(룸브 라인, loxodrome)이 매우 중요했음
       항해 중 같은 방위를 유지하는 것이 훨씬 쉽기 때문임
       그래서 선원들은 최대한 이런 경로를 따르려 했음
       이렇게 해서 룸브 라인이라는 개념이 나옴
       Rhumb line 위키백과 참고
       Mercator 지도는 이러한 방위를 계산하는 데 더 쉬움을 제공했음
       Mercator projection 위키백과 참고
       이 설정 자체가 계속 새로운 수학적 발견을 낳아 옴
       예를 들어, 극좌표 투영(polar projection)에서 보면 로그 나선이 됨
       옆에서 보면 파동 팩켓(wave packet)이 됨
       그 수학적 흥미로움 때문에 Paul Erdos마저 도전하게 됨
       참고 논문: Spiraling the Earth with C. G. J. Jacobi. Paul Erdös
       사족이지만 오늘은 Hacker News에서 구면기하학(spherical geometry) 이야기가 많이 올라오는 날인 것 같음
       관련 토론 링크:
       1번
       2번
       3번
          + 하지만 OP(원글)에 나온 나선 곡선은 룸브 라인(loxodrome, rhumb line)이 아님
            곡선이 표면상 일정 간격을 가진 형태인데 룸브 라인은 정의상 항상 같은 각도로 자오선을 가로지르기 때문에 극지방에 가까워질수록 선이 더 조밀해지는 특성이 있음
            수식으로도,
            x = 10 · cos(π·t/2) · sin(0.02·π·t)
            y = 10 · sin(π·t/2) · sin(0.02·π·t)
            z = 10 · cos(0.02·π·t)
            이 식을 구면좌표(R=10)로 바꿔보면,
            λ(t) = π/2 · t (longitude)
            φ(t) = π/2 - 0.02·π·t (latitude)
            미분하면 d(λ)/d(φ) = -25(상수값)
            진짜 룸브 라인은 d(λ)/d(φ)가 tan(α) · sec(φ) 형태여서 위도에 따라 변함
            즉, 이 곡선은 룸브 라인이 아님
            교차각이 변하는 곡선이 궁금하다면 이 시각화 링크에서 살펴보길 추천
     * 덕분에 영감을 받아서 2022년에 만들었던 재미있는 구면 관련 프로젝트를 소개해봄
       spheredisksample 프로젝트
       오늘 같은 트렌드에 딱 맞는 프로젝트라고 생각함
       사람들이 좋아할 만한 sphere-resample 프로젝트도 함께 추천
     * Rhumb line 등과 관련된 토론이 달린 이 포스팅도 함께 잊지 않고 참고하면 좋겠음
     * 시각화가 정말 멋지다고 생각함
       한 가지 더 기대했던 부분은 “일정한 속도로 움직일 수 있을까?”에 대한 내용이었음
       경로를 따라 점을 배치하는 게 목적이면 상관없지만, 실제로 움직이는 걸 보면 시작과 끝에서 훨씬 느리게 이동하는 현상을 볼 수 있음 (거의 반지름에 의해 결정됨)
       만약 일정한 속도로 이동하고 싶거나, 심지어 느려졌다 빨라졌다 하는 이징(easing) 함수까지 적용하고 싶다면 어떤 방법이 있을지 궁금함
       아마도 수학적으로 멋진 트릭이 있을 것 같음
       수식 미분해서 속도 함수를 만든 뒤, 피타고라스 공식으로 dx, dy, dz를 처리하고, 속도 함수의 역함수를 써서 t'로 다시 파라미터화(reparameterization)해야 할 거라고 얼추 생각만 해봄
       하지만 이 부분은 너무 수학에 익숙하지 않아서, 말로만 떠드는 느낌이 듦
          + 일정한 속도로 움직이려면 “유클리드 파라미터화(Euclidean parameterization)”가 필요함
            즉, t값이 움직인 유클리드 거리와 비례하게 조정돼야 함
            애니메이션에서 경로 따라 움직일 때 항상 필요한 개념임
            하지만 대개 닫힌 식(closed-form solution)이 없는 경우가 대부분이라 수치적으로(계산적으로) 해결해야 함
            실제로는, 각 t마다 원하는 거리(ds)에 해당하는 dt를 이진 탐색이나 보간법(interpolation search) 등으로 찾아냄
            이렇게 해서 그 결과값을 저장해서 일정 간격의 점으로 폴리라인을 만드는 것이 실용적인 접근임 (곡선이 시간에 따라 계속 변하지 않는 한)
          + 질문에서 언급한 수학적 트릭은 바로 “호 길이 파라미터화(arc length parameterization)”임
            곡선의 호 길이(arc length) 함수의 역함수와 합성하는 작업임
            특정 곡선군 몇 가지를 제외하면, 대부분 닫힌 식이 없어서 계산적으로 접근함
          + t를 느리게 움직이자는 직감이 맞긴 함
            t에 따라 각도 속도가 유지되지만, 반지름 역시도 t에 따라 변하기 때문임
            일종의 Archimedean spiral 개념임
            속도의 크기를 상수로 만들어서 파라미터화 하면 더 일정한 이동이 가능함
            단, 반지름이 0에서 시작하기 때문에 어떤 방식으로든 극한값(limit)을 처리해야 함
            게임 등에서 경로를 따라가야 한다면 Z축 기준으로 경로와 접선을 타깃팅하고, 반복적으로 속도에 제약을 주면서 드래그해서 bead toy처럼 이동시키는 단순화 방식도 실용적임
     * “…사실 혼돈적(chaotic)인 것이 아님. 단순히 수학적 함수로 정의된 경로임”이라는 부분에 대해,
       제시된 함수가 실제로 혼돈 현상을 보여주는지는 알 수 없으나, 혼돈의 개념 자체가 본질적으로 결정론적 수학 함수에서 나타나는 현상(초기 조건에 극도로 민감하게 반응)임
       아마 글쓴이가 “random” 혹은 “non-deterministic” 대신 “chaotic”이란 단어를 선택한 것 같음
          + 위와 같은 기술적인 지적이 매우 중요하다고 생각함
            Hacker News 독자라면(또는 그래야만 한다면) 이런 구분이 흥미로울 것임
            수학적으로 혼돈은 극도로 초기 조건에 민감한 결정론적 시스템임
            결과가 무작위처럼 보이지만, 실제로는 무작위성(randomness)과 개념적으로 완전히 다름
          + 혼돈이란 용어 자체가 결정론적 수학함수에서 발생하는 성질이란 데 동의함
            하지만 일상적인 사전적 정의에서는 “완전한 무질서와 혼란”, “우연이 지배하는 상태”, “복잡한 자연 시스템의 예측 불가성” 따위를 의미함
            일상적으로 글을 읽는 이들의 기대와 언어 습관에 맞추기 위해, 수학적 엄밀함보다는 이해하기 쉬운 언어로 설명하는 방식도 충분히 의미 있다고 생각함
     * 피드백을 하나 주자면, 모바일에서 네비게이션 방식이 예상과 달랐음
       어떻게 조작하는지 몰라서 스크롤을 시도했음
       화면 터치가 다음 페이지로 넘어가길래 “아, 그렇구나”라고 생각함
       오른쪽을 눌렀더니 다음 페이지로 이동해서 나중에 클릭을 한 번 더 했을 때 왼쪽을 눌러 뒤로 가기를 시도했으나, 오히려 두 페이지를 건너뛰게 되었음
       이로 인해 일부 화면을 놓쳐서 조금 아쉬웠음
       큰 문제는 아니지만, 약간의 안내가 있었다면 혼란을 덜고 더 집중할 수 있었을 것임
          + 첫 슬라이드에 사용법 안내가 있음
            그래도 보조적으로 swipe 동작을 추가해도 좋을 것 같음 (개인적으로는 tap 조작을 선호함)
            소셜 미디어 앱의 “카드 스택” 인터페이스를 닮으려면 swipe도 자연스러울 것 같음
     * 내용이 기본적인 기초 수준이라 아이들이 수학을 배울 때 참고하기에 좋아 보임
       원(circle) 공식(x = r cos t, y = r sin t) 같은 수학 개념도 중간중간 언급했으면 더 좋았을 것 같음
       추가로 확장하기 좋은 주제로는 극좌표(polar coordinate)와 선형대수(벡터, 변환, 3d 공간에서의 변환 등)가 있음
       저자 본인이 이런 주제에 익숙하지 않다면 3blue1brown 유튜브 영상을 추천함
       프로그래머 입장에선 코딩, 라이브러리나 실제 3D 오브젝트(버텍스, 변형 등)를 다루는 부분이 빠졌기 때문에, 그 부분까지 다뤄주면 더 좋겠음
     * 구면 헬릭스에서 z축 이동의 “정확성”에 대해 궁금했음
       z = c * t 같이 여러 함수로 단순 이동할 수 있고, 이 함수는 “껍질(peels)”의 두께, 일관성, 균일성 등이 달라짐
       여기서 쓰인 함수는 시각적으로 멋져 보이지만, 나선 간 거리의 일정함(혹은 균일한 면적 분할 등)이라는 관점에서 목표를 어떻게 잡는 게 좋을지 궁금함
       이 함수가 어떤 과정을 거쳐 선정된 것인지, 아니면 그저 보기에 좋아서 그냥 고른 것인지 궁금함
          + 아마도 이 함수는 프로그래밍이 편리하고 눈에 보기 좋기 때문에 단순히 선택된 것 같음
            진짜 “정확한” 방식은 3D 공간에서 점이 일정 속도를 유지하며 이동(예: 진짜 지구 위에서 배가 움직일 때처럼)하는 것일 거라고 생각함
            그 경우 수식(코드 예제 참고)
            const degrees = Math.PI / 180
            const bearing = 5 * degrees
            const k = Math.tan(bearing)
            const v = 0.001
            const phi = (t) => vt/Math.sqrt(1 + kk)
            const theta = (t) => k*Math.ln(Math.tan(phi(t)/2))
            x, y, z 좌표로 변환
            const x = (t) => Math.sin(phi(t)) * Math.cos(theta(t))
            const y = (t) => Math.sin(phi(t)) * Math.sin(theta(t))
            const z = (t) => Math.cos(phi(t))
            실제로는 tan(phi/2)의 로그 함수까지 써야 하고, 이건 미분방정식을 풀어서 나오는 형태임
            아마도 저자는 이 정도 복잡한 방식(ln(tan(phi/2)))까지 쓰지 않은 것 같다고 생각함
          + 경로의 속도를 일정하게 만드는 것(velocity constant)이 핵심임
            도함수를 설정해서 속도를 상수로 만든 뒤, z에 대해 풀거나, t’로 재파라미터화(reparameterization)하는 방식으로 접근 가능함
            z = c * t를 선택하는 것은 경로의 파라미터화와 실제 궤적을 동시에 결정하는 영향이 있음
     * 애니메이션이 아주 부드러워서 인상적임
       최근에 구 상에 N개의 점을 분산시키는 문제를 다룰 일이 있었는데, 그 과정에서 “fibonacci-sphere”라는 간단한 알고리즘을 발견함
       이 방식도 구면 위에 나선을 생성하여 점을 배치하는 용도로 쓰임
       관련 논문: fibonacci-sphere 논문 PDF
     * Acko.net이 아직 언급되지 않은 것에 놀람
       비슷한 도구를 활용해 복소수와 프랙탈, 특히 Julia fractal을 시각적으로 설명하는 훌륭한 블로그 포스트가 있음
       관련자라면 꼭 읽어보길 추천
       How to fold a julia fractal - Acko.net 블로그
     * 3D Desmos에서 이 곡선 식을 직접 조작해볼 수 있음 Desmos 3D 시각화 링크
       이 나선의 파라메트릭 방정식이 구면 좌표계에서 선형적이라는 점도 흥미로움
       좌표 변환 위키백과 참고
     * 공유해줘서 고맙고, 정말 흥미롭게 봤음
"
"https://news.hada.io/topic?id=22700","AI는 나쁜 매니저를 만든다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            AI는 나쁜 매니저를 만든다

Performance Assessments

     * 성과 평가가 힘들게 느껴지는 이유는 많은 관리자가 아직 관리 기본기에 능숙하지 않기 때문
     * 성과 평가 작성은 관리자의 정밀한 표현력과 공감 능력을 시험하는 핵심 활동이며, 관리자가 성장할 수 있는 핵심 훈련 과정
          + 훌륭한 관리자는 재즈 연주처럼 피드백을 즉흥적이고 매끄럽게 전달할 수 있어야 함
          + 이런 능력은 수천 시간의 어색하고 어려운 대화·문장 다듬기·정확히 전달하려는 스트레스를 거치며 형성되는 장기 훈련의 산물임
     * 성과 평가 문서는 정밀함·공감·전략적 사고를 동시에 요구하는 관리의 축소판이며, 이를 AI에 떠넘기는 행위는 운동을 건너뛰는 지름길과 같아 관리자 자신의 성장 이득을 0으로 만드는 선택임
     * AI에 의존하면 팀에는 겉보기에 그럴듯한 문서가 제공되지만, 관리자는 실질적 훈련 기회를 상실하게 됨
     * 성과 평가는 보고 대상의 성장을 위한 도구일 뿐 아니라 관리자의 리더십 역량 향상 수단이기도 함

AI는 추상화가 아닌 보조 도구

     * 진정한 추상화 도구(메모리 안전 언어, 맞춤법 검사기, 계산기)는 항상 동일한 결과를 내므로 그 위에 기술을 쌓을 수 있음
     * 그러나 관리용 AI는 예측 불가능하고 일관성이 없으므로, 관리 활동 전체를 AI가 인터페이스하는 상황이 아니라면 신뢰할 수 있는 추상화 계층이 될 수 없음
     * 따라서 AI에 성과 평가, 프레젠테이션, 승진 관리 같은 핵심 의사소통·의사결정 업무를 위임하는 것은 관리자의 성장(현장 감각과 판단 근육의 발달)에 장애물이 됨

Dos and Don’ts

     * 일부 일은 아파야 성장하는 영역으로, AI가 모서리를 둥글게 만들어 당장 시험에는 유리해도 내일의 숙련을 갉아먹는 시험날 치팅과 같음
     * 따라서 관리자는 즉시 효율보다 누적 역량을 우선시하며, 무엇을 위임하고 무엇을 직접 수행할지에 대한 선 긋기 원칙을 가져야 함

관리 업무에서 AI 사용 가이드

     * 이력서 검토: AI 사용 권장, 규칙을 정하면 대량 후보자 선별에 효과적임
     * 인재 영입(세일링/설득): 반드시 사람이 직접 수행해야 하는 핵심 관리 기술이며, 반복 필수
     * 프로세스 설계: 대다수 워크플로는 범용 프레임이므로 AI 초안 활용 가능, 반복적이고 표준화된 영역임
     * 프로세스 운영: 자동 알림·컴플라이언스 체크 일부 자동화 가능하지만, 회의 운영·백로그 관리 등은 관리자가 팀 호흡을 배우는 중요한 기회임
     * 성과 관리: 반드시 사람이 수행해야 하는 분야, 피드백은 장인 정신에 가까운 영역임
     * 경력 성장 지도: AI를 스파링 파트너로 활용해 아이디어를 모으되, 최종 설계와 실행은 관리자 몫임

핵심 원칙

     * AI는 반복적이고 정답이 명확한 업무에서만 활용
     * 모호성과 인간 행동이 얽힌 상황은 관리자가 직접 경험하고 고민해야 성장할 수 있음
     * AI는 보조자일 뿐이며, 관리자의 리더십과 성장 경험을 대체할 수 없는 영역이 존재함

        Hacker News 의견

     * 관리자 평가가 직원의 성과 평가서 품질에 기반한다고 생각하는 게 귀엽게 느껴짐
       내용에는 동의함
       잘 고민한 리뷰는 직원이 더 잘 일하도록 도와줌
       AI가 성과 평가서를 처음부터 잘 만들어주지는 못함
       최소한 리뷰에서 전달하고 싶은 내용을 직접 생각하고 메모해두고, AI는 언어만 다듬어 주는 용도임
       그런데 대부분 관리자의 상사입장에선 이런 노력은 시간 낭비로 여겨짐
       현실적으로 성과 평가는 그때그때 필요에 따라 임의적으로 결정되는 경향이 많음
       예산이 부족하면, 갑자기 성과가 대단하지 않게 보이고, 승진할 자격이 있는 직원이 없다는 평이 됨
       반대로 직원들이 연봉에 대한 불만을 표현하는 시기거나 채용이 어려울 땐, 직원이 갑자기 핵심인재로 등극함
       효율적인 관리자는 AI에 ""승진에 어울리게 좋은 평가로 만들어줘"" 또는 ""아직 개선 여지 있는 보통 평가로 만들어줘""라고 시키고, 결과만 맞춰서 애매모호한 평가서를 뽑아냄
       진짜 실력 좋은 관리자는 시간을 투자해서 평가서를 직접 쓰고, 인재를 성장시키는 관점임
       하지만 관리자들은 효과적인 리더십을 발휘하는 데 인센티브를 잘 못받는 상황임
       내 경험에서 우러난 이야기임, 각자 상황마다 다를 수 있음
          + 꽤 오랫동안 관리를 해보면서, 성과 평가 과정에 대해 새롭게 알게 된 점이 많았음
            기본적으로 모두가 곡선에 따라 평가되고, “기대 이상” 등급은 많이 줄 수 없음
            기대 이상 평가를 많이 주면 그 역할을 뛰어넘는 일을 했다는 뜻이 되고, 승진 근거가 되기 때문임
            많은 승진을 해줄 수 없으니, 최상위 실적자들도 자신이 얼마나 잘하는지 모르도록 하는 것임
            성과 평가는 참 멍청한 시스템임, 애초에 새로 알게 되는 내용이 없어야 하고, 1년 내내 꾸준한 피드백이 있어야 함
            승진 또한 전략적으로 가능할 때에만 해야 함
     * 만약 모든 관리자가 AI로 인해 못해지면, 그게 곧 새로운 AI 일상임
     * 내가 40여 년 동안 만나온 거의 모든 관리자들은 정말 형편없었음
       AI가 차라리 나은 개선이라고 생각함
          + ""40년 동안 만난 관리자들이 정말 나빴다""는 말에 대해, 나는 내 매니저보다 내가 더 잘할 수 있다고 믿지 않는 사람을 본 적 없음
            결론적으로, 누군가가 관리자로 승진하는 순간 그 사람은 하위 직원들에게 무능한 PHB(Pointy-Haired Boss)로 보이게 됨
          + 나도 마찬가지임
            실제로 나는 코딩하는 일이 재미있고 잘하지만, 지금까지 만난 대부분 관리자 밑에서 일하느니 내가 관리자가 되는 편을 택할 것임
            대부분 직장 문제는 해결할 수 있지만, 못난 관리자는 진짜 막다른 길임
          + ""'Forbidden' AI Technique""(Computerphile)라는 주제의 유튜브 영상 공유함
            요약은 대부분의 AI 모델은 결국 사용자와 단위테스트에 대해 거짓말하도록 적응하게 됨
     * 관리자에게 팁을 하나 주자면: 만약 직원들이 성과 평가 때까지 자기 실적을 알 수 없다면, 당신은 제대로 관리자 역할을 못하고 있다는 것임
       직원 입장이라면: 만약 내가 내 업무 성과를 모른다면, 내 관리자도 제대로 일 안 하고 있는 것임
       모든 성과 평가는 특별히 해고를 고려하는 게 아니면 항상 긍정적이어야 함
       직원이 개선이 필요한 점이 있다면 미리 분명하게 알고 있어야 하기 때문임
       관리자 기본기 101임
          + 나쁜 관리자를 가진 사람은 사실 이미 대부분 알고 있음
            불행하게도, 퇴사하거나(혹은 훨씬 더 어렵게) 노조 결성해서 회사 정책과 의사결정에 직원들이 의견을 낼 수 있도록 요구하는 것 외에 별다른 해결책이 없음
            노동자 권한 기본 101임
     * 이제 문제가 생긴 것임
       AI가 프로그래머를 대체해서 우리가 필요 없어지면, 그 다음엔 우리 관리자를 대체하러 올지도 모름
       음… AI가 우릴 못 대체한다고 설득하는 기사를 써야겠다
       좋은 생각임, 그 전에 ChatGPT에게 프롬프트부터 넣어보겠음
          + 내 경험상, AI는 못난 관리자를 오히려 더 드러나게 해줌
            진짜 최악(혹은 유독성)인 관리자가 AI를 쓰면, 바보처럼 보이지 않게 해준다는 기대만 할 수 있을 뿐임
            내가 본 바로는, 관리자가 구체적이지도 않고 입증도 불가한 피드백만 제공하는 경우가 많았음
            예를 들어, '회의에서 중요한 포인트를 잘 집어넣는다'와 '회의에서 주제를 벗어나지 마라' 같은 상충되는 피드백을 일관성 없이 주거나, 모든 팀원이 스프린트 마감일을 지켰는데도 '너무 느리게 일한다'라고 하는 상황임
            최악의 경우라면 ChatGPT의 아부 떨던 버전을 아주 좋아한 관리자들이, 컴퓨터가 추천했다는 이유로 터무니없는 아이디어에 빠져드는 현상임
            유독성의 예시로, '병가 내려면 의사소견서 제출하라' → '의사한테 갈 정도면 출근도 가능하네' 같은 순환논리 펼치는 관리자임
            그 뒤로 팀원 절반이 며칠 내로 다 같이 아프게 됨
     * 이건 대부분 사람들이 AI에 대해 갖고 있는 생각의 구체적 사례라고 봄
       즉, AI는 지루하거나 명확하게 범위가 정해진 일만 맡기고, 핵심 업무는 절대 맡기지 않는 게 맞다는 의견임
       학생들도 AI를 공부 친구나 튜터로 쓰는 정도이지, 숙제 전체를 맡기진 않음
       소프트웨어 엔지니어도 마찬가지로, AI를 코드 리팩터링이나 간단한 작업에 쓰지만, 전체 시스템 설계나 정교한 추상화엔 직접 관여함
       (여기 많은 댓글에서 성과 관리가 관리자의 핵심 업무인지 논의 중인데, 참고로 내 생각엔 성과 관리는 핵심임)
     * 성과 평가 자체를 정말 싫어했음
       내 매니저는 언제나 평가가 애매했음
       그는 동료들을 보호하려 했고, 나의 실제 업무를 관찰하지도 않음
       피드백은 대부분 누군가에게 내가 잘하는 것과 못하는 게 뭔지 이메일로 물어본 결과임
       5년간 분기마다 돌아왔던 평가문은 ""NAHWheatCracker는 훌륭한 엔지니어. 가끔 일하기 어렵다""는 식의 공허한 문장임
       일대일 면담에서 항상 ""일하기 어렵다""는 부분에 대해 물었지만
       내 매니저는 누가 어떤 말을 했는지 절대 밝히지 않고, 구체적인 상황도 설명하지 않고, 직접 대화할 기회도 만들어주지 않음
       한번 있었던 문제인지, 일상적인 문제인지, 아니면 단순히 하루 기분탓인지 알기 어려웠음
       나는 구체적으로 개선할 만한 내용을 원했음
       추정만 하게 되면 오히려 더 큰 문제를 낳게 되고, 불명확함이 팀 사이 거리감을 키움
       내가 누군가에게 불리한 말을 할까봐 아예 대화 자체를 꺼리게 됨
       여기에 AI가 비인간적인 방식으로 리뷰를 뽑아내게 된다면 더 우울할 것 같음
       그래도 못난 관리자가 이미 이 프로세스를 망치고 있으니, AI가 더 나쁠 수는 없을 것 같음
          + 25년 넘게 일했는데도 쓸모 있는 성과 평가를 받은 적이 없음
            더 많은 사람들이 강하게 거부 의사를 표한다면 이런 제도가 사라질 수도 있다고 생각함
          + 관리직은 수많은 난제 앞의 균형 잡기가 필요함
            익명 피드백을 요청하면 '등 뒤에서 뒷말하는' 느낌이 들어 신뢰가 무너질 수 있음
            반대로 투명함을 위해 모두가 직접적으로 솔직한 피드백을 하려다 보면, 갈등·보복 위험이 있거나 오히려 솔직한 피드백이 안 나오고 문제와 불만이 묻히게 됨
          + 동료 업무능력을 평가하라는 요청엔 절대 응할 생각 없음
            그건 관리자 자신의 일임
            쓸데없는 긴장만 만들고, 결국 그의 역할임
            왜 이렇게 다들 따라가는지 이해 안 됨
            엔지니어들이 서로 험담만 하고, 매니저 계급을 위해 대신 평가까지 해줌
            그냥 내 일만 하면 충분함
     * 처음으로 프롬프트 인젝션이 자기평가에 도입되는 사례가 나오길 기다리고 있음
       생각해보면, 사전 정의된 등급 기준/피드백 폼에 따라 LLM 기반 대화가 이뤄지는 성과 평가라면 그렇게 나쁜 아이디어는 아닌 것 같기도 함
       LLM이 사내 데이터나 직원 데이터를 넣어서 곧장 완성하는 게 아니라, LLM이 일련의 질문을 던지고, 필요한 경우 후속질문으로 유의미하고 실행 가능한 피드백을 도출하도록 도움을 주는 구조를 상상해봤음
     * 반론을 던지고 싶음
       만약 좋은 매니저란 게 훌륭한 성과 평가서 한 장 써내는 일이라고 생각한다면, 이미 그 사람은 매니저로서 실격임
          + 성과 평가서가 다는 아니지만, 분명히 중요한 파트라고 했고 나 역시 이에 동의함
     * 성과 평가서를 작성하는 관리자를 쓸 바엔 차라리 AI가 더 나은 관리자 역할을 할 수 있다고 생각함
"
"https://news.hada.io/topic?id=22652","Home Depot, 셀프 체크아웃에서 '비밀리에' 안면인식 사용 혐의로 피소","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Home Depot, 셀프 체크아웃에서 '비밀리에' 안면인식 사용 혐의로 피소

     * Home Depot가 셀프 체크아웃 구역에서 안면인식 기술을 고객 동의 없이 몰래 사용한 혐의로 집단 소송을 당함
     * 소송 제기자 Benjamin Jankowski는 본인이 체크아웃 도중 얼굴 주변에 녹색 상자가 표시되는 것을 발견했으며 안내나 동의 절차가 전혀 없었음을 주장함
     * Home Depot가 도난 방지 목적으로 컴퓨터 비전 기반 안면인식 시스템을 2024년에 도입한 점이 밝혀짐
     * 이 회사가 Illinois Biometric Information Privacy Act(BIPA) 를 위반하여 얼굴 데이터를 수집, 저장하면서 사전 고지 및 동의 절차를 지키지 않은 점이 문제 제기됨
     * 최근 Rite Aid가 무분별한 안면인식 사용으로 5년간 관련 기술 사용 금지령을 받은 사례와 연결되며, 프라이버시 침해 우려가 커지는 상황임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Home Depot가 일리노이주 76개 매장 내 셀프 체크아웃 카메라에서 고객 동의 없이 안면인식 기술을 사용했다는 의혹으로 집단 소송에 직면함. 이 기술은 매장 내 도난 방지 목적이라는 이유로 2024년 도입되었으나, 고객의 얼굴 데이터를 별도의 공지 없이 수집 및 저장하여 논란을 키움.

소송의 경위

     * 자주 Home Depot를 이용하는 고객 Benjamin Jankowski가 2024년 해당 회사에 집단 소송을 제기함.
     * 그는 시카고 지역 매장에서 셀프 체크아웃 중 본인 얼굴 주위에 녹색 박스가 화면에 나타나는 것을 보고 얼굴 인식 기술이 사용된 것을 인지함.
     * Jankowski는 해당 시스템이 얼굴을 인식해 데이터로 저장한다는 점을 의심했으며, 고지문이나 동의 절차가 전혀 이뤄지지 않았음을 강조함.
     * 당시 매장에는 직원 카운터가 없어 셀프 체크아웃만 가능한 상황임.

안면인식 기술과 관련 법률

     * Home Depot는 '컴퓨터 비전' 기술을 도난 방지 명분으로 대대적으로 도입했음.
     * 소송장은 이 시스템이 Illinois Biometric Information Privacy Act(BIPA) 를 위반한 사실을 지적함.
          + BIPA는 생체정보 수집 시 사전 고지, 활용 목적 설명, 서면 동의를 의무화함
          + Jankowski는 Home Depot가 이런 절차를 무시하고, 관련 정책조차 공개하지 않았음을 주장함

집단 소송 및 요구

     * Jankowski는 일리노이주 76개 Home Depot 매장 중 동일하게 얼굴 데이터가 수집된 모든 고객을 대표하고자 함
     * 소송에서는 BIPA의 태만 위반 시 건별 $1,000, 고의 위반 시 건별 $5,000의 배상을 요구함

유사 사례: Rite Aid

     * 이번 소송은 2023년 Rite Aid가 안면인식 오남용 사례로 연방 규제기관에 의해 5년간 기술 사용 금지 처분을 받은 사건과 연관됨
     * Rite Aid는 매장 내 문제 인물이나 도난 대비 명목으로 해당 시스템을 운영했으나, 수천 건의 오인식과 고객 프라이버시 침해가 발생한 사실이 드러남

시사점

     * 리테일 매장 내 안면인식 등 생체인식 기술 활용이 증가하는 가운데, 개인정보 보호와 법적 절차 이행에 대한 사회적 경계심이 한층 높아짐
     * 기업의 고객 데이터 수집 및 활용 투명성 확보와 사전 동의 절차가 필수 과제임

        Hacker News 의견

     * 나는 Loss Prevention(LP) 담당자가 오히려 상황을 악화시키는 경우가 많아 불만임, 그리고 디지털 감시 시스템들이 그들의 행동을 더 심해지게 만듦. 최근 한 고급 식료품점에서 LP 직원이 셀프 체크아웃 뒤에 숨어 나만 계속 지켜보는 것 같아 일부러 그가 사라질 때까지 기다림. 결제할 때도 다 스캔했다고 스크린을 보여줬는데, 오히려 그 직원을 깜짝 놀라게 해서 사과 없이 도망가듯 사라짐. 이런 상황들이 시스템 상에는 내 실수로 기록될 것 같아 의심스럽다고 느낌. 최근 내 주변엔 도난이 많아졌고, 진짜로 물건을 쓰레기봉지에 마구 담아가는 사람을 본 적도 있음. 그런 사람에게는 직원이 신경 안 쓰는데, 오히려 나 같은 정상 고객이 셀프 체크아웃 쓸 때만 더 감시받음. 내 N95 마스크 쓴 모습이 의심받는 건가 싶기도 함. 하지만 대학가 약국에서 N95는
       필요한 선택임
          + LP 직원들이 쓰레기 봉지 들고 나온 사람을 굳이 제지하지 않는 이유는 충분히 있음. 아무도 그처럼 위험해 보이는 사람과 마주치고 싶지 않아함. 실제로 경비원들도 폭력 가능성 때문에 개입하지 않으며, 그저 보여주기식 역할에 불과함. 이런 상황에서 경찰도 예전만큼 신경 쓰지 않으니, 매장들은 비정상적인 방법으로 물품 분실을 막으려고 애쓰는 것임
          + 셀프 체크아웃을 도입해 직원 수를 줄이고, 고객에게 그 일을 떠넘긴 다음에는, 오히려 고객을 범죄자 취급하며 감시하는 게 매우 디스토피아적임. 셀프 체크아웃 도입의 대가로 손실 증가를 감수해야 한다고 생각함. 직원 대신 기계를 쓰는 대가로 고객이 노동을 하게 됨을 인식해야 함. 두 마리 토끼 모두 잡기는 불가능함
          + 슈퍼마켓이나 대형 매장 개념도 사실 1900년대 이후 등장한 새로운 혁신임. 그 이전에는 물건을 직접 고르는 셀프서비스 없이, 상인에게 목록을 주고 받아오는 형태였음. 대형 매장에서 고객이 직접 집어가는 시스템에는 어쩔 수 없는 손실이 항상 따라감. 도둑질은 당연히 나쁘지만, 어느 정도 손실은 사업의 비용임. 이를 사회가 무너지는 현상처럼 과하게 걱정할 필요는 없음. 일반 고객까지 괴롭힐 정도로 LP를 강화하는 건 오히려 역효과임
          + 최근 식료품점 손실이 많아졌다는 얘기에 대해, 식료품점의 순이익률이 매우 낮다는 점을 언급하고 싶음. 2020년에 3%, 2024년에 1.6%로 줄었음. 특히 캘리포니아 등 지역에서는 이 수치가 2배 더 나쁠 수 있다고 봄. 그래서 매장에 빈 진열대가 많고, 진열 사진만 남겨놓고 계산대에서 직접 받아가라고 하는 식으로 변화할 수 있음
            관련 통계 참고
          + 관련은 적지만, MIT 근처 내 인생 최고 LP 경험도 떠오름. 경품 이벤트 때문에 라면을 많이 샀는데, 쿠폰 확인하려다 직원에게 도둑으로 오해받았고, 또 다른 직원까지 와서 나를 이상하게 에워쌈. 한 명은 내 백팩 덕분에 도둑처럼 보인다고 뭔가 떠보는 식으로 말함. 일촉즉발 상황이었지만, 나름 자연스럽게 대응해서 별일 없이 끝남. 결국 점장으로 보이는 분이 와서 평범하게 대화로 풀림. 오히려 LP 담당자가 문제를 일으켜 가게에 소송당할 위험도 있다고 생각함
     * Home Depot 셀프 체크아웃에선 얼굴 인식 시스템을 써서, 동일 인물의 반복적 소매 절도를 추적하고 데이터베이스를 구축함. 이 정보는 누적 건수가 임계값을 넘을 때 쉽게 법적 조치를 할 수 있도록 활용됨. 그리고 CCTV AI가 스캔 안 된 물품을 자동으로 감지해 경고 메시지를 보내기도 함
          + 얼굴 인식이 BIPA(Biometric Information Privacy Act)에서 저장 방식에 따라 위법이 아닐 수 있음. 해시값만 저장하는 경우가 있는데, 법적으로는 문제가 없다는 의견이 있음. 하지만 엔지니어·시민의 입장에선 개인정보 침해 위험이 얇은 경계에 걸쳐 있다고 생각함
            관련 법률 정보
            업계 동향
          + 내가 직접 경험해보면, 대형마트 셀프 체크아웃에서 ‘스캔 안 한 항목이 있다’는 경고가 거의 항상 잘못 나오곤 함. 실제로 안 읽힌 경우는 극히 적음. 대다수 직원도 이 시스템에 짜증을 내며, 여러 명이 한꺼번에 줄을 서서 억울하게 대기하는 상황도 자주 봄
          + 그저 쉽게 기소하기 위한 목적이 아니라 누적 가치가 중범죄 기준을 넘길 때까지 일부러 기다린다고 봄
          + ""인도인""을 AI 대신으로 비유한 표현이 신박하다고 느꼈음
          + 이런 방식으로 반복 범죄를 누적시켜 단번에 중범죄로 몰아가는 건 사회적으로 아무런 이득이 없다고 생각함. 한 번 적발될 때마다 바로 처리하면 그만일 의미임. 만약 교통경찰이 사소한 위반을 매번 모아두었다가 1년이나 지나 한꺼번에 처벌하면 모두가 분노할 것이라고 봄
     * 나는 셀프 체크아웃을 의도적으로 쓰지 않음. 사람과의 작은 상호작용이라도 지키고 싶고, 이게 곧 일자리 유지에 기여한다고 믿음. 현금을 선호하기도 함(여긴 대부분 카드만 허용). 물론 이 싸움이 점점 힘겨워지고 있음을 느낌
          + 내 시간과 노동력까지 빼앗아가면서 직원을 고용하지 않으려는 가게의 행태에 불만임. 더군다나 스캔 속도를 높이려고 물건의 바코드를 미리 맞춰두고 연속해서 스캔하려다, 인식 시스템 오류로 직원이 영상까지 검토하는 상황이 발생함(그것도 2달러짜리 물품 때문에). 소비자를 범죄자 취급하는 이런 분위기가 너무 심하다고 느낌
          + 나도 셀프 체크아웃은 거부함. 실수로 스캔을 빠뜨릴까봐 괜한 누명까지 걱정해야 하고, 다양한 단말을 사용할 교육도 받지 않았으니 더 꺼려짐
          + 이런 흐름은 인간성 상실로 이어지고 있다고 생각함. 복잡한 사회에서 사람들과 어울리는 것도 부담인데, 그냥 도피처처럼 셀프 시스템을 받아들이는 기분임. 모두가 진짜 사회 문제를 피해서 쉬운 길만 찾는 것 같음
          + 셀프 체크아웃만이 아니라, 일반 계산대에도 얼굴 인식 카메라가 있음. 최근엔 신용카드 결제단말기 포스에 잘 숨겨진 카메라까지 탑재해둬서, 가끔 검은 스티커로 가려두곤 함. 이 카메라는 표시조차 안 돼 있음
          + 반대로 계산원과의 대면에서 오히려 인간관계에 피로를 느껴 덜 상호작용하는 미래를 바라는 마음이 들기도 함
     * 계산대 스크린에 뜨는 녹색 사각형은 얼굴을 인식했다는 뜻일 뿐, 생체정보 저장을 의미하지 않음. POS 단말기가 단순히 얼굴이 들어오면 다음 손님을 위한 초기화 용도로 검출할 수 있음. 그런 기술만 적용해도 법적 분쟁에서 소송 단추는 충분히 누를 수 있음. 이런 소송이 실패해도, 매장은 반복 소송에 계속 노출될 위험이 있음. 고객마다 서면 동의 없이 얼굴 감지를 적용하기 어려울 듯함. 그래서 일리노이주에서는 얼굴 인식뿐 아니라 얼굴 감지 자체도 실효성이 떨어질 수 있음
          + Home Depot가 단순 감지 수준인지 생체정보를 저장하는지는 소송을 통해서만 확인이 가능함
          + 녹색 박스는 AI “YOLO”같은 얼굴 감지 오픈소스나 일반 카메라의 얼굴 포커스 검출과 유사한 단순 기능일 수 있음
          + 법적으로 “얼굴을 인식한다”라는 표현은 꼭 과학적·공학적 용어와 일치할 필요 없음. 표시만으로도 법률적 의미가 발생할 수 있음
     * 도난율이 높은 Home Depot에서는 셀프 체크아웃에 항상 직원이 상주하며, 케이지에 들어있는 제품을 살 때엔 컨시어지처럼 직원이 직접 안내해 주는 것을 경험함
          + 나도 항상 직원이 있는 계산대가 더 좋음. 실제로 계산원이 퇴근 후 셀프 체크아웃에서 자기 물건을 결제하는 걸 보고 아이러니하다고 느낌
     * 이런 카메라 시스템은 단순히 얼굴 위치만 표시해도, 최소한 몇 시간 분량 영상을 뒤적거리기 쉽도록 돕고, 범죄 억제 효과도 있음. 얼굴 사진 한 장이 생체정보로 보기는 어렵다고 생각함. 그래서 이런 소송은 기각될 가능성이 높다고 봄
          + 카메라 화면의 녹색 박스는 단순히 카메라나 디스플레이 자체에 내장된 기능임. 실제로 녹화되는 영상은 ONVIF 프로토콜로 DVR에 전송되어 거기서 더 다양한 분석이 가능함. 대표적 장비 공급사로 Wren Solutions, Costar 등의 제품(PVM10-B-2086 등)이 있음
            제품 사양서
          + 이런 감시 시스템은 실제 도난 예방보다 “감시하고 있다”는 인상을 주는 시큐리티 시어터처럼 느껴짐. 영국에선 CCTV 렌즈 주위에 LED 라이트가 돌아가 사람들의 시선을 유도함과 동시에 카메라가 있고 활성화되어 있다는 심리적 경고 효과를 주려고 하는 것 같음
          + 단순 얼굴 위치만 추적하는 데 그치지 않고, 장면 별로 위험도 점수를 매겨서 다음 방문 때 LP 담당자가 특정 사람을 실시간으로 추적하는 데 활용할 수 있다고 생각함. 규모 범죄로 만들려고 오랜 기간 증거를 모으는 건 아니겠지만, 바쁜 계산대 중 실제로 집중 감시할 타겟을 정하는 용도의 기술일 가능성이 높음
     * 원고가 소송에 집착하는 걸로 볼 수도 있지만, 난 팬데믹 이후에도 일부러 마스크를 벗지 않음. 감시 카메라를 치우지 않는다면 나도 계속 마스크로 얼굴을 가릴 것임. WalMart나 여러 식료품점이 아예 모니터로 실시간 감시 장면을 보여주며 ‘감시 중’이라는 경고를 노골적으로 내보내는 게 더 불쾌함. Aldi에서는 아예 셀프 계산대 화면에 내 얼굴 실황을 보여주어서 처음엔 깜짝 놀랐지만, 오히려 솔직해서 고마워해야 하나 싶기도 함. 어쨌든 난 얼굴 인식에 자주 나오는 ‘마스크 쓴 사람, 중지 올리는 고객’으로 기억될 듯함
          + 셀프 계산대 스크린에 “감시 중” 실시간 영상 띄우는 건 진짜 고객에 대한 예의가 부족하다고 느낌
          + 설령 얼굴은 가려도, 보행 분석 등으로 여전히 식별은 가능함
     * Kroger 등 일부 체인점의 결제 단말기에도 카메라가 장착된 것을 최근에 발견함. 모든 계산대에 설치되어 있음. 게다가 내 근처 Home Depot는 주차장에 최소 10대 이상의 ALPR(자동번호판인식장치)이 설치돼 있음. 이제는 그냥 무조건 데이터베이스에 등록된다고 봐야 함
          + ALPR은 Automatic License-Plate Recognition, 즉 자동차 번호판 자동 인식 시스템임
            ALPR 설명
     * 쇼핑 갈 때마다 마치 ICE(이민 세관 집행국) 요원 흉내라도 내야 할 판임. 답답함
"
"https://news.hada.io/topic?id=22627","제품(Product)과 제품을 파는 것의(Offering)의 본질적 차이 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             제품(Product)과 제품을 파는 것의(Offering)의 본질적 차이 [번역글]

1. '문제-약속-제안' 연결 구조

     * 창업자가 진짜로 집중해야 할 포인트:
         1. ‘배고픈 고객’—즉 반드시 해결해야 할 절박한 문제(pain point)가 있는 고객 찾기.
         2. 그 문제를 해결해준다는 명확하고 강력한 약속을 제품이 실제로 실현할 수 있어야 함.
         3. 최종적으로, 이 약속을 고객이 체감 가능한 형태(제안, offering)로 제공해야 시장에서 의미가 있음.
     * 핵심 프레이즈: “모든 제품이 모든 시장·고객에 맞는 것은 아니다.”

2. 기술 제품과 제안의 근본적 정의와 역할

     * 기술 제품(Tech Product): 기술적 ‘새 현상’을 이용해 만들어진 도구(예: AI엔진, 하드웨어, 심리·행태 기반 서비스).
     * 제안(Offering): 기능, 가격, 지원, 통합, 조건, 서비스 등 실제 ‘고객 가치를 완성’하는 총체적 제공물.
     * 제품=‘핵심 기술’, 제안=‘기술+가치 실현 위한 모든 요소’ —> 고객이 즉각 고민 없이 도입·사용하도록 만듦.

3. ‘두께(Thick/Thin)’와 ‘결합도(Loose/Tight coupling)’ — GTM 및 성장 전략에 미치는 영향

     * Thin Offering(얇은 제안)
          + 제품 기능만으로 충분히 고객의 목적/문제에 바로 적용 가능.
          + 빠른 도입, 초기 채택 유리. (주로 사용자 자체가 빠르게 도입할 수 있는 도구형 제품)
     * Thick Offering(두꺼운 제안)
          + 고객 맞춤화, 배포, 통합 등 많은 서비스와 부가가치 필요.
          + B2B·대기업·복잡 서비스 시장에 적합.
     * Loose Coupling(느슨한 결합): 기술의 개선이나 변화가 기존 고객 제안에 큰 영향을 주지 않음.
     * Tight Coupling(밀접한 결합): 기술의 변화가 기존 제안 전체 또는 일부 설계를 재검토하게끔 만듦.

4. 시장 진입(GTM)과 제품 로드맵 조합 방식

     * Thin+Loose: PLG(제품주도 성장)형—다양한 사용자 세그먼트 공략, 폭넓은 실험, 빠른 적용, 낮은 진입장벽.
          + 대표예: Slack, Figma, Dropbox 등 디지털 SaaS 도구.
     * Thick+Tight: Account-led(계정주도 성장)형—고객사 맞춤형 제안, 깊은 통합/서비싱 필요, 고부가가치 중심.
          + 대표예: 엔터프라이즈 AI, 클라우드/보안 솔루션.
     * 중요: GTM 방식에 따라 ‘제품 로드맵’(무엇을 개발하고 얼마나 발전시킬지)과 ‘제안의 폭’이 달라짐.

5. 제품-시장 적합도(PMF)와 확장(Scale)을 위한 실제적 조치

     * 자신의 제품이 어느 ‘카테고리’(thin/thick, loose/tight)에 있는지 냉정하게 진단.
     * GTM 전략과 로드맵을 명확히 맞추되, 시장 반응을 빠르게 보고 실험적으로 조정(Iteration).
     * 확장 단계(Scale)에서는, 얇은 제안은 두꺼워지고, 두꺼운 제안은 점점 더 표준제품화—‘제품↔제안’의 다이나믹 관리가 필요.
     * PLG에서 Account-led로, 또는 그 반대(제안을 제품화하거나, 제품을 서비스화) 등 유연한 전략 구사가 중요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

요약

    1. 절박한 문제 → 2. 명확한 기술·제품 → 3. 감칠맛 나는 ‘제안’(offering, 최종 가치) → 4. 두께/결합도 진단 → 5. 맞춤 GTM & 로드맵 → 6. 시장에서 빠른 실험 → 7. PMF & 확장
"
"https://news.hada.io/topic?id=22725","유능한 리더들은 어떤 능력으로 불확실성을 헤쳐나가는가 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  유능한 리더들은 어떤 능력으로 불확실성을 헤쳐나가는가 [번역글]

1. 문제의 일부임을 인정하기 (Accept We Are Part of the Problem)

     * 자신의 행동이나 결정이 문제에 어떻게 영향을 끼쳤는지 인식하고, 책임을 받아들이는 능력.

2. 새로운 상호작용 방식 장려하기 (Encourage New Interaction Patterns)

     * 기존 방식만 고집하지 않고, 환경 내에서 새롭고 창의적인 소통과 협력의 길을 열어줌.
     * 팀이나 조직이 이전에 접하지 못한 정보와 방식을 경험하도록 유도함.

3. 인내하며 다양성을 존중하기 (Patient Divergence)

     * 빠른 해결책 강요를 피하며, 다양한 아이디어·시각을 충분히 탐색하는 문화 조성.
     * 여러 가능성을 조율하되, 자연스러운 수렴을 기다림.

4. 복합적인 원인 파악하기 (Identify Plausible Contributors)

     * 단일 원인에 집착하지 않고, 여러 가능한 요인들을 탐색해서 폭넓게 접근.
     * 상충하는 요인도 포용하여 복잡한 문제 조망.

5. 현재의 힘 활용하기 (Power of the Present)

     * 목표만 바라보지 않고, 현재 상황에서 잘 돌아가는 요소에 집중.

6. 다양한 관점 융합하기 (Blend Diverse Perspectives)

     * 도전적인 시각도 수용하여 여러 의견의 접점에서 가능성을 확장함.
     * ‘내가 전부 안다’라는 태도를 경계함.

7. 인내심과 회복 탄력성 (Patience and Self-Repair)

     * 반복적 개입 대신, 상황이 자연스럽게 해결되도록 기다리는 능력.
     * 리더가 직접 나서기보다, 시간을 두고 팀이나 환경이 스스로 해결하도록 인내.

8. 영향 예측하기 (Anticipate Effects)

     * 행동의 예상치 못한 파급효과까지 미리 염두하며, 상황 변화를 놓치지 않고 관찰함.
     * 부작용까지 고려한 면밀한 결정 필요.

9. 호기심과 가벼운 접근 (Curiosity and Light Touch)

     * 즉각적 판단을 억제, 마음속 생각과 감정을 자유롭게 탐구하는 자세.
     * 변화의 순간에 유연한 호기심을 유지.

10. 양쪽 모두 인정하기 (Both/And)

     * 겉보기엔 양자택일처럼 보여도, 복잡한 상황을 ‘둘 다’ 인정하여 새로운 접근을 찾음.
     * 일부분만 택하지 않고, 조화로운 해결책 지향.

11. 안전하게 개입하기 (Intervene Safely)

     * 부정적인 영향 최소화, 긍정적 패턴 강화 등 위험을 관리하며 개입함.

12. 직관과 추론의 균형 맞추기 (Abduction and Intuition)

     * 논리적 데이터와 인간적 직관을 적절히 활용하여 실마리를 찾음.
     * 불명확한 문제일수록 경험적 직관의 중요성 강조.

13. 다양한 강점과 역량 수용하기 (Accept Diverse Strengths and Skills)

     * 익숙하지 않은 역량도 존중하며, 팀 내 잠재력 극대화.
     * 평가 절하 위험을 인지하고 포용.

14. 협력적으로 상황을 파악하고 환경을 만들어 가기 (Collaboratively Sense and Shape)

     * 문제 해석과 변화 방향을 독점하지 않고, 함께 협력하며 실질적 변화를 유도함.
     * 팀원들과 열린 대화 및 방향 제시.

15. 일치가 아닌 일관성 추구 (Coherence vs. Alignment)

     * 완벽한 일치 대신 핵심 경계 내에서 각자의 자율성과 탐색 권장.
     * 지속가능한 일관성을 지향.

16. 씨앗을 심고 성장시키기 (Plant Seeds—Help Them Grow)

     * 결과를 미리 결정하지 않고, 유기적으로 성장할 수 있는 환경을 조성함.
     * 범위와 시간의 유연함 강조.

17. 업무 방식 맞춤 조정하기 (Tailor Ways of Working)

     * 과제 성격에 따라 다양한 실행·실험 방법을 혼합 적용.
     * 상황/목표별로 접근법 차별화.

18. 불확실성에 맞서기 (Facing Uncertainty)

     * 빠른 성과와 불확실한 영역 사이에서 균형 잡으며, 도전을 기회로 삼음.
     * 팀의 성장을 지원하고 복잡한 문제에 적극적으로 대응.
"
"https://news.hada.io/topic?id=22657","AI 도구 사용 기여 시 공개 필수","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI 도구 사용 기여 시 공개 필수

     * 오픈소스 프로젝트 Ghostty의 PR 토론에서, AI 도구 사용 여부를 명시적으로 공개해야 한다는 의견이 제기됨
     * 제안자는 AI가 여전히 낮은 품질 코드를 생성하는 경우가 많고, 특히 미숙한 사용자가 검토 없이 제출할 때 문제가 크다고 지적함
     * 공개 목적은 유지보수자가 PR의 신뢰도를 평가하고, 인간 기여자에게는 교육적 피드백을 주지만 단순 AI 생성물에는 불필요한 노력을 줄이기 위함임
     * 또 다른 참가자는 PR 템플릿을 통해 AI 사용 여부를 포함한 체크리스트를 추가할 수 있다고 제안함
     * 한편, AI 도구가 자동으로 특별한 byline을 표준화해 GitHub 커밋 메시지에 기록되도록 하면, 투명성과 도구 노출이 동시에 보장된다는 아이디어도 제시됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 사용 공개 필요성

     * Mitchellh는 AI 도구 사용을 좋아하고 본인도 활용하지만, 현재는 동등하거나 더 나은 품질을 보장하지 못하는 상황이라고 평가
          + 특히 검토 능력이 부족한 초보자가 AI 코드를 그대로 PR에 올리는 경우, 품질이 매우 낮음
          + 이런 상황에서 유지보수자가 불필요한 검토와 피드백에 시간을 쓰게 되는 것은 “속이는 행위” 라고 비판
     * 따라서 AI 사용 여부를 명시적으로 공개하면 유지보수자가 어느 정도 주의 깊게 검토해야 할지 판단할 수 있음

PR 템플릿 도입 제안

     * Yawaramin은 GitHub의 PR 템플릿 기능을 활용해 AI 사용 여부를 포함시키자고 제안
          + 동시에 Developer Certificate of Origin(DCO) 같은 체크리스트도 포함 가능
     * 이를 통해 모든 기여자가 일관된 방식으로 AI 활용 사실을 알릴 수 있음

GitHub 표준화 아이디어

     * Tobi는 GitHub 차원에서 AI 전용 byline 표준을 만들 것을 제안
          + AI 도구가 사용될 때마다 .git 스테이징 파일에 기록되고, 커밋 메시지에 자동 추가
          + GitHub은 이를 목록화하고 도구에 링크 제공 → 유지보수자는 출처를 확인할 수 있음
          + 동시에 AI 도구들은 현재처럼 co-authors를 스팸처럼 남용하지 않아도 됨
     * 이 방식은 투명성 보장, 도구 홍보, 유지보수 효율성을 모두 충족하는 방안으로 평가됨

        Hacker News 의견

     * ""AI""를 사용할 때에도 지적 재산권 오염 문제가 발생함을 지적함, 우리는 이 사실을 외면하고 있음, 누군가가 모든 오픈소스 프로젝트 코드를 외우고 필요할 때마다 그대로 써줄 수 있다면 그런 사람을 우리 회사에선 당연히 금지해야 함, 그런데 AI의 경우엔 여러 합리화와 핑계를 대면서 그 사실을 부인하고 있음, 실제로 GPL을 포함한 다양한 코드를 느슨하게 세탁하는 식이며, 이건 지식재산권(IP) 기반 기업엔 치명적인 위험을 내포함
          + 미국 법원들은 학습 데이터 사용이 변형적(transformation) 용도임을 이미 판결한 바 있음, 세부 조율할 일은 많겠지만 결국은 돌이킬 수 없는 변화임, IP 창출이 경제적으로 지속 가능한 활동이 되길 원한다면 관련 법 체계도 바꿔야함
          + 이 논리를 따르면 StackOverflow나 거의 모든 분야의 교과서 또한 금지해야 함, 현실적으로는 프로그래머들은 어쩔 수 없이 다른 사람의 코드를 보게 됨
          + 실제로 금전적으로 연관된 사람 외에는 AI 관련 법적 이슈를 심각하게 생각하지 않는다고 봄, 다행히 대부분의 경우 이 이슈를 무시하고 있으며, 법체계도 진보를 막지 않는 선에서 잘 작동하고 있음
     * mitchellh의 ""신입 기여자들을 끝까지 도와주고, PR이 머지되도록 돕는다""는 부분을 아주 인상 깊게 생각함, 피드백을 줘서 신입 개발자가 성장하는 건 정말 값진 일임, 하지만 만약 PR 제출자가 그 피드백을 AI에 바로 던져서, 본인은 아무것도 배우지 않는다면 그건 시간 낭비임
          + 이제 신입 개발자들은 AI 없이 일하는 환경 자체가 없게 될 것임
     * 오늘의 HN 첫 페이지가 현실적인 경험 위주의 좋은 콘텐츠들로 채워져서 기분이 매우 좋음, 쓸데없는 공포나 과장 없이 솔직한 이야기가 많음, 개인 컴퓨터에선 AI 어시스트를 꺼두고, 회사에서도 정말 필요한 상황에만 아주 제한적으로 사용함, AI 어시스트는 작은 단위의 작업(atomic work)에 아주 강점이 있으나 복합적인 작업(compound work)엔 형편없음, 결론적으론 AI는 결국 인간이 어떻게 활용하느냐에 달림, 인간 지능이 핵심임
          + ""AI는 인간이 다루는 만큼만 똑똑하다""는 말에 점점 동의하게 됨, 같은 AI를 두고 완전히 다른 결과가 나오는 것이 이해 안 됐었지만, 진짜로 AI가 마법은 아니라는 걸 체감함, 팀원끼리 설명도 못하는 사람들이 AI에게 가치를 끌어낼 수 있을 것이라 기대했었던 자신이 naïve했다고 생각함, 오히려 AI가 평범한 엔지니어와 뛰어난 엔지니어 간의 격차만 더 벌릴 듯함, 아직 기분이 복잡하지만 왜 어떤 사람들에게 AI가 무용하다고 느껴질 수 있는지 이해하게 됨
          + Frederik P. Brooks의 “No Silver Bullets, Refired” 에서, 소프트웨어 개발이 본질적으로 복잡하며, 혁명적 해법을 마냥 기다리기보다 점진적 생산성 향상을 추구해야 한다는 결론을 인용함, 이 관점은 현실적이고 희망적으로 다가옴
          + ""AI는 인간이 다루는 만큼만 똑똑하다""는 말에 흥미로움을 느낌, 결국 ""나 AI로 하루 만에 쿨한 라이브러리 만들었어""라는 포스팅의 주인공들은 애초에 실력이 좋은 개발자였음
          + 자신도 공감함, 회사에서 해킹 주간이라 AI 툴을 전사적으로 실험하고 있는데, 분석적 어프로치, 가드레일, grounded generation 등 실제 적용에서 좋은 결과가 주로 나옴, 최근에는 쓸데없는 챗봇 열풍이 지나고 머신러닝 본질로 패러다임이 리셋된 느낌임
          + 인간이 핵심 결정을 내리고, 그 다음 AI가 나머지를 연결해준다고 생각함, 핵심 결정이란 게 무엇인지, 점만 단순히 연결만 하는 작업이 무엇인지 도메인마다 다르지만, 실제로 대부분의 코드(80~90% 정도)는 단순 반복/연결 작업임, 이 경계만 잘 지키면 생산성 상승이 아주 큼, 반대로 AI에게 핵심 결정을 맡기면 더 큰 손해가 남, 오히려 버리고 다시 하는 게 나음, 핵심 결정 예시로는 데이터베이스 설계, 타입 정의, 의존성, 시스템/인프라/화면 설계, 테스트 항목 선정, 코드 조직 구조 등이 있음, 반면 AI가 잘 할 수 있는 일은 CRUD, API 핸들러, 간단한 데이터 구조 변환, 배포 스크립트, 테스트 구현, UI 컴포넌트 코드, 스타일링, 임시 데이터 정리 등임, 역시 AI가 리서치와 아이디어 탐색, 대안 탐구 등에서 도움 되나, 결론과 실제 구현은 인간이 직접 챙겨야 함
     * 자신은 AI를 대단히 좋아하는 사람은 아니나, 그냥 도구 중 하나로 봄, 누가 PR을 어떻게 준비했든 결과만 괜찮으면 신경 안 씀, 다만 PR 제출 전에 메인테이너가 뭔가 요구하면 그에 맞춰주는 게 예의라고 생각함
          + PR이 어디에서 왔는지, 어떻게 나왔는지는 중요함, 리뷰어도 실수하고 한계가 있기 때문에 신뢰가 중요해짐, 신뢰가 없으면 아예 리뷰 프로세스에 받아들여선 안 됨, 리눅스 커널 팀이 미네소타대학을 실험 때문에 차단한 것도 이와 같은 이유임
          + 글의 핵심 논거, 즉 ""신입 기여자가 성장할 수 있도록 도와주는 게 목표이나, 만약 상대가 AI라면 시간 낭비일 뿐이다""라는 부분에는 제대로 답하지 않은 것 같음
          + AI로 하루에 1,000개 PR도 만들 수 있음, AI로 잘 만든 PR만 생각하는 듯하지만, 현실에선 AI로 프로젝트 메인테이너를 엄청나게 힘들게 할 수도 있음
          + 미국 저작권 사무소에 따르면, AI가 생성한 결과물은 저작권 보호 대상이 아님, 그렇기에 라이선스 목적으로라도 AI 사용 사실 공개가 필요함, 이를 위반하면 전체 작업물의 저작권을 잃을 수 있음, 자세한 내용은 보고서와 메인페이지 참고
          + AI를 사용하고 해당 사실을 묻는다면 항상 공개할 것임, 다만 구체적으로 묻지 않았다면 '관례적인 예의상 미리' 밝히진 않겠음, 대부분의 사람들이 AI 사용을 당연시하거나 신경도 안 쓴다고 생각하며, 오히려 주의를 분산시키는 사소한 표시라고 느낌, dependabot처럼 알림이 와도 실제 관심이 안 생김
     * ""내 오토컴플릿은 어떡하냐""라는 질문이 몇 번 나왔는데, 단순 탭 오토컴플릿처럼 키워드, 짧은 구절이면 공개 안 해도 된다고 정책 문서에서 명확히 예외 규정이 있음, 문서(혹은 PR)을 제대로 읽어보라고 권함
     * 이번 정책은 추가 맥락 설명이 들어 있어서 납득이 감, 이전에 봤던 여러 AI 관련 정책들은 이념 선언에 가까웠는데, 여기는 요구 이유와 앞으로의 방향을 제시해 줘서 훨씬 현실적임, 이런 방식이 더 많아졌으면 좋겠음
     * 이 정책이 결국 정직한 사람이 AI를 쓰기 힘들게 만들지 않겠냐는 걱정이 있음, 어차피 AI 썼다고 하면 PR이 덜 주목받을 테니 다 숨기려 하지 않겠냐는 질문임
          + 그렇게 단순하지 않다고 생각함, 정책을 낸 사람(mitchellh) 본인도 LLM을 쓰기 때문에, 본인이 한 작업에 대해 충분히 이해한 상태에서 편의용으로 AI 썼음을 설명할 수 있다면 크게 신뢰를 잃지 않을 것이라 봄
          + 우려가 현실이 될 수 있음, AI가 ""대충 맞는 것처럼 보이지만 실제로는 엉망""인 코드를 대량으로 만들어내기 때문에, 만약 AI 코드에 불신이 쌓이면 그건 AI 문제이지 사람 문제는 아님, 더 발전된 AI 코딩 툴이 필요함
          + ""chat-gpt 사용했다""고 밝히면 바로 묻히고, 아무 말 없이 지식 있는 척하면 칭찬받음, 이미 모두가 AI 사용을 숨기는 방향으로 가고 있음
          + 이걸 문제로 보는 것 자체가 별 의미 없다고 생각함
          + ""AI 쓰지 말라""는 게 아니라, 사용했다면 솔직하게 PR에 밝히라는 취지임
     * ""ChatGPT로 코드베이스를 이해하는 데 도움 받았으나, 실제 코딩은 직접 함"" 같은 상세 공개도 요구하는 이유가 뭔지 궁금함
          + 이런 설명을 남기면, 리뷰어 입장에서 ""코드베이스 이해""라는 부분에서 오해/착오가 리뷰 포인트가 될 수 있으니 집중할 수 있음
          + 본인은 개발자가 아니지만, 이런 AI 어시스턴트 덕분에 코드 탐색 시간이 현저히 단축됨, 개인적으로 AI가 정말 큰 도움을 줬음
     * PR 생성에 사용한 각 프롬프트를 포함하는 패턴이 좋다고 생각함, LLM은 완전히 결정적인 도구는 아니지만, 어떤 단계/프롬프트를 거쳐 결과에 도달했는지 맥락을 남긴다는 데에 의의가 있음
          + 현실적으로는 매우 비실용적임, AI 기반 PR 하나 만드는 데에도 10~20개 프롬프트, 테스트, 수동 문맥 조정, 수동 코딩 등 여러 절차가 섞이기 때문임, 차라리 화면 녹화가 낫다고 생각함
          + 본인은 vscode 플러그인(specsytory)과 cursor 조합으로, 모든 LLM 상호작용 로그를 md로 남기고 Pull Request에 함께 제출, 코드 리뷰 때 참고함
     * 개인 프로젝트에서는 에디터 오토컴플릿 사용 여부까지 공개하도록 규칙을 정함
          + 의도 전달 방식은 흥미로우나, 지금 AI는 기존 오토컴플릿과는 완전히 다름, 오토컴플릿처럼 쓸 수도 있지만 AI가 할 수 있는 일은 훨씬 다양하고 깊음, AI를 단순 자동완성 정도로만 생각하는 건 개인적인 관점일 뿐, 많은 사람들은 그렇게 쓰지 않음
          + 탭 오토컴플릿은 정책에서 명확히 예외로 인정하고 있음
          + 오토컴플릿은 대부분 문법적 도구에 불과하지만, AI는 코드의 의미와 구조까지 가이드하려고 시도함
"
"https://news.hada.io/topic?id=22642","퓨텍스 없이는 의미 없다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             퓨텍스 없이는 의미 없다

     * The Art of Multiprocessor Programming 교재가 퓨텍스(futex) 개념을 다루지 않아 아쉽다는 문제 제기
     * Futex는 현대 병렬 프로그래밍에서 효율적인 동기화의 핵심 구성요소로, 기존 System V 기반 락보다 뛰어난 성능을 보임
     * 퓨텍스는 락 획득과 대기/깨우기 기능을 분리하여, 불필요한 시스템 콜과 오버헤드를 줄이는 구조를 가짐
     * 퓨텍스 기반으로 스핀락, 뮤텍스, 재귀 락 등 다양한 동시성 프리미티브를 직접 구현하는 예시와 기법 설명이 포함됨
     * 저자는 책이 실제 엔지니어링 실무에 필수적인 최신 동기화 방법론을 다루지 않아, 아카데미아와 현업 간 괴리를 지적함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * Phil Eaton이 'The Art of Multiprocessor Programming, 2nd Edition' 북클럽을 시작함
     * 이 책은 병렬 프로그래밍 분야에서 권위 있는 교재로 여겨지지만, 저자는 내용의 실용성 결여를 지적함
     * 특히, 4학년 학부생과 대학원생을 대상으로 한다면서도 퓨텍스(futex)라는 핵심 동기화 기법을 다루지 않는다는 점을 비판함

퓨텍스란 무엇인가 – 왜 중요한가

     * 퓨텍스(futex)는 “fast user space mutex”의 줄임말로, 실제로는 뮤텍스라기보다는 현대 락 구현을 위한 OS 지원 동기화 원시 구성요소임
     * 과거에는 대부분의 락이 System V IPC의 세마포어 기반으로 구현돼 효율성과 확장성에 한계가 있었음
     * Linux에 2002년 퓨텍스가 도입되면서, 1000개 동시 작업 환경에서 System V 락 대비 20~120배 빠른 성능을 보임
     * Windows(2012년)와 macOS(2016년) 등 타 OS도 유사한 메커니즘을 도입함
     * 오늘날에 널리 쓰이는 pthreads 등 시스템 라이브러리의 락은 퓨텍스를 사용함

퓨텍스의 동작 원리 및 차별점

     * 기존 세마포어는 락과 대기를 결합했지만, 퓨텍스는 락 획득과 대기/깨우기를 분리함
     * 이로 인해 불필요한 딜레이와 시스템 콜을 줄일 수 있으며, 락 해제 시 대기 스레드 없음이 확실하다면 커널에 진입하지 않아도 됨
     * 퓨텍스의 대기(wait) 호출은 “특정 메모리 주소의 값이 원하는 상태일 때만 대기”하게 하며, 타임아웃도 지원함
     * 퓨텍스 깨우기(wake) 호출은 특정 메모리 주소와 연결된 내부 대기 리스트에서 원하는 개수의 스레드를 깨움
     * 메모리 주소의 실제 값 검증을 요구해, 이미 상태가 변한 경우 불필요한 대기를 방지함

퓨텍스의 실질적 활용 – 직접 구현

     * 퓨텍스는 저수준 원시 기능이므로, 컴파일러 및 하드웨어의 메모리 연산 순서 이슈를 고려해 atomic 자료형을 사용함
     * Linux에선 syscall로 퓨텍스 시스템콜을 직접 호출해야 하며, MacOS에선 __ulock 인터페이스를 사용함(최근엔 더 쉬운 API 추가됨)
     * 기본적으로 퓨텍스 대기는 성공시 0, 실패시 에러코드(타임아웃 등) 를 반환함
     * 퓨텍스 기반 핵심 연산:
          + h4x0r_futex_wait_timespec() : 기대값이 일치할 경우 대기, 타임아웃 적용 가능
          + h4x0r_futex_wake() : 1개 혹은 모든 대기자 깨우기

뮤텍스/스핀락/재귀락 구현의 실전 예

  스핀락

     * 가장 단순한 형태의 락, 오직 단일 비트(atomic_fetch_or) 로 동작
     * 락을 가질 때까지 무한 루프(“스핀”)하지만, 높은 경합 상황에서는 CPU 낭비와 잘못된 해제, 재귀 호출 시 데드락 위험 등 구조적 문제가 있음

  하이브리드 뮤텍스 (‘unsafe’ 뮤텍스)

     * 보통은 스핀락으로 먼저 시도, 일정 횟수 실패시 퓨텍스로 전환하여 효율적 블로킹을 구현
     * 대기자가 없으면 불필요한 시스템콜을 피할 수 있고, 대기자는 깨우기 시스템콜 최소화 가능
     * 엄밀한 소유권 검증이나 재귀 처리는 미비해 “unsafe”라는 명칭을 사용

  대기자 카운터 뮤텍스

     * 한 비트는 락 상태, 나머지는 대기자 수 집계에 사용, 불필요한 깨우기 시스템콜 감소 목적
     * 아직도 소유권 및 재귀 처리 없음

  소유권 관리 포함 뮤텍스

     * pthread_t 값을 통해 락 소유자와 상태를 명확하게 추적하여 잘못된 unlock이나 재귀 사용시 문제 포착
     * 락 획득, 해제, 대기자 관리 모두 엄격하게 atomic 연산으로 제어

  재귀 락

     * 스레드별 중첩 횟수(depth) 카운터를 추가하여 동일 스레드의 중첩 락 획득 가능
     * unlock 시 depth 감소, 0이 되면 실제 unlock 및 깨우기 진행
     * 각 동작은 atomic 연산과 엄격한 소유권 검사로 구현

남은 과제 및 실제 엔지니어링 현실

     * 락 소유 스레드가 비정상 종료/죽는 경우, 락 관리를 위한 별도 관리 리스트, 종료 콜백 등 추가 관리 필요
     * 프로세스 간 공유 뮤텍스를 쓸 때도 상태 변화 관리에 추가적 고려 필요
     * POSIX RW락은 재귀 중첩 동작이 정의되어 있지 않고 구현마다 상이해, 실제로는 안전성 확보가 어려움
     * 저자는 책이 실전에서 정말 중요한 동시성 이슈(퓨텍스, 재귀 락, 비동기 런타임 등) 를 교과과정에 포함하지 않는 점을 비판함

결론

     * 'The Art of Multiprocessor Programming'은 역사 또는 이론적 관점에 치우쳐 중요한 현대적 병렬 프로그래밍 실무 지식을 제대로 담지 못함
     * 실질적으로 시스템에서 동작하는 퓨텍스 등 핵심 동기화 구성요소를 제대로 다루지 않으면 후학들에게 실질적 해악 초래 가능성
     * 저자는 최신 개념 반영 및 실용적 내용 보완의 필요성을 강조함

참고자료

     * 전체 코드 예제는 codeberg에서 확인 가능

        Hacker News 의견

     * Windows에는 WaitForMultipleObjects라는 기능이 있는데, Linux도 5.16(2021년 말)에 Futex2로 이를 도입함음
       관련 링크
       최근 Futex2에 다양한 개선이 이루어져 왔음
       NUMA 지원도 드디어 추가됨음
       NUMA 관련 링크1
       NUMA 관련 링크2
       NUMA는 성능에 매우 중요한 요소임
       io_uring이 6.7(2024년)에서 futex에 적용되면서 postgresql aio 성능 향상에 도움을 주었음음
       관련 글
       6.7에서는 Small requeue와 single wait 기능도 추가됨음
       관련 링크
          + Windows는 WaitForMultipleObjects 기능을 새롭게 추가한 것이 아니라, 30년 넘게 처음부터 가지고 있었음음
            WaitForMultipleObjects는 UNIX에 비해 Windows NT의 장점이긴 했지만, IBM PL/I도 1965년에 이미 유사 기능이 있었음
            UNIX에서의 'wait' 함수는 IBM PL/I의 'wait'에서 단순화된 버전으로, Multics에서 물려받은 여러 기능들과 마찬가지로 원래 모델보다 약한 수준이었음
            MS의 WaitForSingleObject와 WaitForMultipleObjects도 효율적인 구현이 아니었기 때문에, 결국 Linux의 futex와 동등한 WaitOnAddress를 도입해야 했음
            Linux futex는 32비트 크기 제약과 단일 이벤트 만을 기다릴 수 있다는 한계가 있음
            원자적 비트 연산을 활용하면 여러 이벤트에 대한 대기를 구현할 수 있지만, 효율적이지 않아서 32비트 크기 문제가 커짐음
            'futex'에 WaitForMultipleObjects의 장점을 일부 결합하려는 시도는 반가운 일임
            이런 시도는 Windows를 따라하는 게 아니라, 사실 Microsoft보다 훨씬 오래된, 50년 넘게 잘 알려진 고전적인 기법을 재구현하는 것임
          + 아직도 futex_swap 기능이 없다는 점이 아쉬움
            관련 토론1
            관련 자료2
          + Futex는 WFMO(WaitForMultipleObjects)와 관계가 없고, 오히려 keyed events와 동등한 개념임
            Linux에서 WFMO에 해당하는 기능은 select/poll/epoll임
          + io_uring에서 futex 지원이 정말 좋은 기능임
            Ruby fibers와 작업할 때 mutex 및 queue 구현에 활용했음
            소스코드 참고
     * 책에서는 직접 동기화 구조물을 구현하기보다는 라이브러리/언어/시스템에서 제공하는 구조물을 사용하라고 밝히고 있음
       책의 주요 초점은 특정 플랫폼이 아니라 전반적인 concurrency 개념에 있음
       기사 작성자가 다소 과장된 대립 구도로 글을 쓴 점이 아쉬움
       이 글은 ""TAoMP가 말하지 않는 것"" 같은 협업적 시각에서 다뤘으면 더 좋았을 것임
       이 블로그가 새로 생겼고, Phil이 이 글을 올리고, Phil이 다른 글도 프로모션했다는 점은 눈에 띔
          + 내가 그 기사를 썼고, 책을 읽으며 실망해서 쓴 것임
            아카데미와 산업계 모두에서 실질적으로 쓸 만한 내용을 배우지 못하는 현실이 문제라고 느꼈음
            그러니까 ""futex 알아보자!""식의 의도는 아니었음
            실제로 책에 실망해서 다른 글을 미루고 이 글을 먼저 쓴 것임
            Phil과는 예전에 같이 일해서 교류가 있지만, 지금까지 글을 쓰는 데 별 어려움 없이 독자를 찾았음
          + 예전에 sysv 스타일을 공룡에 비유하지도 않는다고 표현한 부분은 너무 심했다는 생각임
            겸손이 더 필요한 부분임
     * futex의 가장 멋진 점은 핸들리스(handle-less) 구조라는 것임
       syscall을 통한 할당/해제가 필요 없는 커널 기반 메모리 감시자로 매우 유용한 기본 동작을 제공함
       대기 중인 스레드가 없으면 모든 게 깔끔하게 정리되고, 경쟁이 없으면 커널은 mutex 자체를 인식하지 못함
       커널이 futex를 고성능으로 관리하는지에 대한 자세한 분석이 궁금함
       futex2에 대해 오늘 처음 알게 되었음
       관련 문서
          + 맞음, 또한 스레드가 락에서 블로킹될 때마다 커널의 malloc()을 호출하여 데이터를 할당하는 방식은 원하지 않음
            이를 방지하려고 많은 OS에서는 'queue object'를 스레드 생성 시마다 할당해 놓고, 해당 스레드가 경쟁 락을 만나면 이 오브젝트를 락에 할당하는 식임
            즉, 여러 스레드가 락에 연결된 linked list 형태로 queue object를 두고, 스레드가 깨어날 때마다 하나씩 가지고 나감
            스레드 종료 시 자신이 최초에 만들었던 오브젝트를 돌려받는 보장은 없고, 중간에 오브젝트가 섞임
            solaris에서 처음 이런 구조(turnstile)를 도입했고, BSD들도 이 방법을 채택함
            solaris internals 참고
            BSD pdf 자료
          + 초기 Unix 커널 내 대기 큐도 이런 방식이었음
     * 2002년 오리지널 futex 논문에서도 futex의 효율성이 명확하게 입증되었고, 1000개의 병렬 태스크 테스트에서 sysv 락 대비 20~120배 빠른 성능을 보였음
       하지만 실제로 기준점(baseline)이 sysv 락은 아님
       실질적으로 futex 없는 환경에서 락 구현 시 대부분 빠른 경로에서는 커널 진입이 없고, 느린 경로에서만 커널로 대기 전환하는데, futex의 개선점은 락 대기 상태를 나타내는 사용자 영역 자료구조의 크기가 작아졌다는 것이 유일함
       다른 대안으로는 thin locks(JVM에서 쓰는 방식), ParkingLot(완전 유저랜드 구현)은 OS의 futex 없이도 동작함
          + 내 경험상, 대부분이 실제로는 실무에서 제공되는 기본 primitive를 배우기 때문에, 내 언어의 표준 라이브러리가 무엇을 제공하냐에 집중하게 됨
            즉 sysv에서 futex로 옮겨온 흐름이 지배적이며, 최근에는 커스텀 방식도 있지만 대세는 futex임
            만약 유저랜드 직접 스케줄러를 만든다면 별도 구현도 가능하겠지만, 대부분은 파일 디스크립터에 기록하고 직접 큐를 관리하는 방식을 사용할 것 같음
            이런 처리 방식이 얼마나 이득이 있을지 의문이 들음
          + 실제로는 어느 현대 락이라도 결국 내부적으로 futex(지원된다면)를 쓰는 셈임
            Linux에서 futex가 가장 효율적인 대기 방식이기 때문에, 느린(down) 경로에서는 항상 futex 사용이 바람직함
            언어에서 thread.park() 같은 것도 결국 futex 위에서 동작할 확률이 높음
          + JVM에서 여전히 thin lock을 사용하는지 궁금함
            과거에 JVM이 futex를 호출한다는 레퍼런스를 찾았는데, 혹시 thin lock으로 마이그레이션 된 것인지 알고 싶은 마음임
            Stack Overflow 관련 토론
     * [recursive locks]의 실제 구현은 표준 간에도 일관성이 없고, 이게 어렵다는 이유로 아예 정의 자체를 하지 않은 경우가 많음
       이런 태도는 꽤 답답함
       ""OS나 언어 구현자가 feature X를 제대로 구현 못 할 테니, 그냥 애플리케이션 개발자가 직접 처리하게 하자""는 식임
       결국 downstream 사용자는 벤더 변경 외에는 뾰족한 수가 없는 셈임
          + 표준에 과도한 제약을 주면 더 나은 구현 가능성을 닫아버릴 수 있음
            예로 들어, C++ 표준 해시테이블과 정규표현식은 제약이 많아 타사보다 훨씬 느림
            특정한 제약(예: 체이닝 방식만 사용하도록)이나 기능 보장을 하면 대안적 고성능 구현이 막힘
            recursive rwlock도 성능을 희생하거나 체크를 덜하는 구현이 가능하기 때문에, 다양한 방향성을 막을 필요는 없다고 생각함
          + 개인적으로 recursive lock은 애초에 쓰지 않는 게 좋기 때문에, 굳이 지원 스펙을 표준에 넣을 필요를 못 느끼겠음
          + worse is better 현상을 더 알고 싶다면 위키 참고
            별로 좋아하지 않지만, 어쩔 수 없는 현실임
     * linux에서 futex가 32bit int만 지원하는 한계가 궁금해져서 알아봤음
       64비트 지원 논의에서 Linus는 사용자 영역에서 64비트 atomic을 쓰고, 하위 32비트만 futex로 사용하면 된다고 언급함
       하지만 C/C++에서는 mixed-size atomic이 undefined behavior로 여겨지고, 실제로 glibc 세마포 구현도 그렇게 동작함
       64비트 정수에서 high 32는 waiter count, low 32는 세마포 값으로 사용하며, 하위 32비트에만 futex를 씀
       gcc에서 이게 정의된 동작인지, 아니면 프로세스 경계(커널 프로세스) 덕분에 상관없는 건지, 아니면 glibc조차 undefined behavior를 사용하는 건지 궁금함
     * Anthony Williams의 _C++ Concurrency in Action_도 추천하는데, futex나 동기화 primitive 직접 구현 방법은 다루지 않지만, 메모리 순서와 lock-free 구조에 필요한 SMR 등 실제에 가까운 내용을 다룸
       더 하드웨어 중심 시각이 필요하다면 Paul McKenney의 무료 저서 ""Is Parallel Programming Hard, And, If So, What Can You Do About It?""도 추천
       이 책도 futex를 깊게 다루지는 않지만, Ulrich Drepper의 ""Futexes Are Tricky""로 안내함
       TAOMPP는 고수준 concurrency 개념을 잘 다루는 용도로 적당하며, OS 수준 구현 디테일까지 담는 건 맞지 않음
       어쨌든 Peterson 혹은 bakery lock은 실사용엔 쓸모없지만, 증명만 익혀도 실전 동시성 알골리즘 이해에 큰 도움이 됨
          + bakery lock은 spin lock용으로는 좋은데, 캐시 친화적임
            reader/writer spin lock도 구현 가능하지만, 엄격한 FIFO됨
            user space에서 bakery lock spin wait에 futex를 연동할 수는 있지만, 매우 비효율적임
            futex는 이런 용도(스핀 대기)는 애초에 설계되지 않음
            lock-free 구조와 hazard pointer, RCU* 등도 여전히 tricky함
            wait-free hazard pointer도 실제로 만들 수 있음
            *RCU의 경우 copy-on-write는 직관적이지만, 업데이트가 잦으면 비용이 늘어남
     * Windows 8에서 futex 유사지가 도입된 것처럼, 원래 Win32 critical section은 커널 세마포 기반임
       그런데 Vista에서 도입된 SRW lock은 어떤 구조인지 궁금함
          + CRITICAL_SECTION과 SRWLock 모두 경쟁이 없으면 커널 진입이 없음
            SRWLock은 keyed event 기반이고, CRITICAL_SECTION은 실패 시에도 on-demand 커널 오브젝트를 만들어 호출하며, keyed event로 fallback 함
     * 2014년 linux futex 구현에서 Pinkie Pie가 발견한 취약점 중, requeue-once 룰은 futex_wait_requeue_pi에 전달된 futex에만 허용됨
       A에서 B로, 그 후 다시 B에서 C로 requeue는 불가능하지만, B에서 B로 재지정은 가능함
       이때 특정 조건을 통과하면 cleanup 함수가 호출되지 않는 버그가 있고, 포인터가 dangling 상태가 됨
       관련 사례 확인 가능
       관련 이슈
     * crashed thread에 의한 데이터 정합성을 걱정하지 않는 사람도 있지만, 프로세스 전체가 죽지 않는 이상 락 클린업 문제가 남음
       이를 위한 솔루션이 robust lock임
       held futex 리스트를 커널에 등록하고, sys_set_robust_list로 스레드 종료 시 해당 bit를 처리해 기다리는(waiter) 쪽을 깨워줌
          + robust lock 방식의 최대 단점은, 락이 보호하던 리소스 자체가 이미 inconsistent 상태일 가능성이 큼
            스레드가 왜 크래시 됐는지 확실히 알지 못하면, 데이터가 무결하지 않아서 복구가 불가함
            그래서 전체 앱을 함께 죽이는 게 더 실질적일 수도 있음
            robust lock을 사용한 cleanup/recovery 기능 자체는 멋지지만, 아마도 95%의 엔지니어는 robust data 구조까지 제대로 설계하지 않을 것 같음
            4%는 그럴 시간이 부족하고, 나머지 1%만 크게 보상받으면서 제대로 하는 경우일 것임
          + 여러 프로세스 간에 futex(크로스 프로세스 상태)를 사용할 때는, watchdog 프로세스가 각 프로세스별로 Unix domain socket(SOCK_STREAM 또는 SOCK_SEQPACKET)을 열어 crash를 감지하고 per-process 상태를 정리하는 식의 방식을 쓸 수 있음
          + 본인도 mutex 논의를 프로세스 경계까지만 한 이유가 깊은 논의로 끝이 없어질까 우려해서임
"
"https://news.hada.io/topic?id=22690","LabPlot: 무료, 오픈 소스, 크로스플랫폼 데이터 시각화 및 분석","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                LabPlot: 무료, 오픈 소스, 크로스플랫폼 데이터 시각화 및 분석

     * LabPlot은 다양한 데이터 시각화 및 분석 기능을 제공하는 무료 소프트웨어임
     * CSV, SQL, Excel(xlsx), JSON 등 여러 데이터 형식을 지원하여 데이터 불러오기가 쉬움
     * LabPlot을 활용하면 과학적 분석과 시각화를 한 번에 진행할 수 있음
     * 크로스플랫폼 지원으로 Windows, macOS, Linux 등 다양한 운영체제에서 사용 가능함
     * 오픈 소스 기반이므로 누구나 자유롭게 확장과 커스터마이즈가 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LabPlot 소개

     * LabPlot은 무료 오픈 소스이며 다양한 플랫폼에서 동작하는 데이터 시각화 및 분석 툴임
     * 사용자는 과학적 플로팅 및 데이터 분석 업무를 하나의 앱에서 효율적으로 처리할 수 있음

데이터 가져오기 및 호환성

     * LabPlot의 첫 단계는 다양한 형식의 데이터 가져오기임
     * 지원하는 데이터 형식에는 CSV, Origin, SAS, Stata, SPSS, MATLAB, SQL, JSON, 바이너리, OpenDocument 스프레드시트(ods), Excel(xlsx), HDF5, MQTT, Binary Logging Format(BLF), FITS 등 다수 존재함
     * 이러한 형식 지원으로 데이터 통합과 초기 작업이 빨라지는 장점이 있음

요약

     * LabPlot은 크로스플랫폼 지원의 과학적 데이터 시각화 및 분석 도구로, 사용자가 다양한 데이터 포맷을 빠르게 불러와 효율적으로 다룰 수 있게 도움을 줌
     * 오픈 소스 특성상 사용자가 직접 기능 확장과 맞춤화가 가능한 이점이 있음

        Hacker News 의견

     * 차트나 그래프 도구가 이제는 아주 평범한 소모품처럼 여겨지는 현상이 신기함, 1988년 말에 Deltagraph를 처음 개발했을 때에는 Postscript와 Illustrator 출력을 타깃으로, 엄청나게 다양한 그래프와 옵션을 담아 전 세계적으로 표준처럼 사용되었음, 특히 인쇄용으로 많이 쓰였음, 90년대 중반엔 개발만 맡고 퍼블리셔에게 판매된 후, 팬데믹 때까지 25년간 소유주가 여럿 바뀌면서도 C로 작성한 원본 소스코드 기반으로 계속 운영되었음, 지금쯤 그 코드가 얼마나 난장판이었을지 상상도 안 됨
          + 그런데도 여전히 차트/그래프 시장이 충분히 평범화되거나 범용화되진 않은 느낌임, 대부분의 직장인들은 구식의 상용 솔루션(Tableau, HW 엔지니어링에서는 JMP, 그리고 SAS나 Excel 등)을 씀
     * SciDavis를 오랫동안 사용했고, 전에 QtiPlot도 시도했었음, 기회가 되면 Origin도 사용했음, SciDavis가 투박한 데다 자주 튕기곤 했지만 원하는 작업에는 큰 문제는 없었음, 그래프 스타일 설정에 애를 좀 먹었고, 스타일 복사는 불편했음, 최근 LabPlot을 써봤는데 csv 파일에 datetime 데이터가 있을 땐 고급 옵션과 수동 설정을 해도 날짜 및 시계열 포맷을 제대로 인식하지 못하는 문제가 있었음, 문서 사이트는 유튜브 동영상 모음인데, 영상을 보며 메뉴얼을 찾을 필요가 없어 별로임, 개발자들이 전통적인 문서도 꼭 만들어야 함, SciDavis의 포크 버전인 AlphaPlot도 있는데, 여전히 yyyy-MM-dd hh:mm:ss.zzz 날짜 문제 등 자체 과제가 있음, 그래도 쓸만한 툴임, 배치 처리나 여러 그래프를 자동 생성하고 재현성 있게 하고 싶으면 gnuplot을 사용함, 학습 곡선은 가파르지만
       스크립트를 몇 번 써보면 자기만의 템플릿을 만들 수 있고, 유용함, 오픈소스 움직임이 이 분야에도 있어 항상 더 많은 선택지가 생기는 건 좋음
       LabPlot 메뉴얼 링크
     * ggplot 같은 툴은 세부 조정하려면 손이 많이 가지만, 그만큼 유연성도 훌륭함, 다만 예를 들어 가속적 종단 연구 데이터로 피험자별 반복측정 spaghetti plot을 그리거나, 고정효과 plot을 그리려 하면 대부분의 솔루션이 한계가 생김, 참고로 내가 만든 plot 예시가 있음
       플롯 예시
          + 나는 생물통계학자인데, 복잡한 종단 연구 디자인을 볼 때마다 정말 좋아함, 예전 교수님이 이런 상황에서 cross-sectional과 longitudinal 효과 분해나 Lord’s paradox 이야기를 하셨는데 아직도 Lord’s paradox는 완벽히 파악하지 못한 상태임
     * 보기에는 정말 멋짐, 하지만 ‘이게 왜 matplotlib이나 다른 인기있는 차트툴보다 더 좋은지’ 설명 섹션이 있었으면 함, 기능 리스트는 봤으나 직접 비교표를 머릿속에서 조합하는 데 부담이 있음, 매력적인 점이 많아 보이지만, 시간을 투자해서 새로 배워볼만한 가치가 있는지 알 수 있도록 사례연구 같은 게 있었으면 진짜 좋겠음
     * 여기 많은 노력이 들어간 건 분명함, 하지만 이미 Julia, Matlab, R, Python, Excel 등에서 실험실 코드와 연동되어 있다면 굳이 이 도구를 써야 하는 동기가 뭐가 있는지 혼란스러움, 특정 커뮤니티에서 유행하는지 궁금함
          + 아마도 Origin처럼 일부 과학 커뮤니티에서 인기 있는 상업용 도구 역할을 FOSS 방식으로 대체하려는 목적임, 다른 소프트웨어(예: 계측 SW)가 이미 데이터를 만들어 줬고, 그걸 빠르게 플롯으로 시각화하고 간단한 곡선 피팅 등을 GUI로 처리하고 싶을 때 유용함, 만약 이미 언급한 언어와 라이브러리로 데이터 처리에 익숙하다면 굳이 이 도구가 필요할 이유는 없음
          + 나는 딱 이 도구의 타깃 유저일 듯함, R, Python, Maxima, MATLAB/Octave를 오가며 데이터를 보통 CSV로 넘기는데, 도구마다 인터페이스가 제각각이라 번거로움, Jupyter도 그리 좋아하지 않아서 이게 더 편하면 Jupyter 대체로도 써볼 수 있을 것 같음
          + 내 경험상 프로그래밍이 비생산적이거나 편하지 않은 사람들도 꽤 있음, 나는 Python을 주로 쓰지만, 회사엔 JMP 라이선스도 많고 대부분의 엔지니어는 Excel을 만족스럽게 씀, 근데 내가 작업하는 데 걸리는 시간은 남에게 보여주지 않음, 그리고 이런 사람들은 오픈소스나 명성 없는 프로그램엔 여전히 경계심이 있음, 이런 도구라면 혼자 시험삼아 써보고 충분히 괜찮을 때만 상사한테 ‘이거 쓸만하다’고 말할 수 있는 옵션이 될 것임
          + 실제 현장 사례를 하나 들면, 발사체 프로젝트 엔지니어로 근무하며, 시험 및 비행마다 발생하는 텔레메트리 데이터 프레임을 CSV나 TSV 등 엄청 큰 파일로 쌓음, 수백 개 변수의 시계열 그래프를 빠르게 시각적으로 돌려보며 이상 유무를 즉시 찾아야 하고, 여러 번 확대‧이동해서 필요한 내용을 캡처해 문서에 넣기도 함, 때론 극도의 상세한 포인트(비트 단위, 샘플 단위)로 확대해 예외 케이스까지 잡아야 하는데, 이벤트가 언제 어디에 있을지 미리 알 수 없으니 속도가 생명임, 서로 다른 단위의 여러 변수 플롯을 한꺼번에 띄워 상관관계도 봐야 하고, 팀 단위로 분석할 때 즉석에서 시각화도 요구됨, 주파수나 통계 분석(피리어도그램, 로그/세미로그, PDF 등)도 필요함, 플롯에 마커나 설명도 빠르게 추가하고, 레이블이나 포맷도 WYSIWYG로 바꾸고 싶음,
            그리고 메뉴 한 번만에 FFT나 필터 적용과 시각화까지 다 되어야 함, Python/Jupyter로 일일이 텍스트로 조작하는 건 이런 워크플로우에선 시간적으로 너무 비효율적임, LabPlot이나 우리가 쓰던 응용프로그램에선 이런 작업을 거의 실시간으로 할 수 있었음, Excel도 스프레드시트 인터페이스 덕에 가까운 기능은 있지만, 시계열 플롯 만들기 위해 일일이 셀, 축, 그래프 정의, 플롯 확장, 레이블 포맷 등을 지정하다 보면 한 달 내내 분석만 하게 됨, 이런 응용프로그램은 댓글, 메타데이터 등이 데이터 파일에 포맷된 주석 형태로 삽입되어 있어서 빠른 작업이 가능했고, 대용량 파일도 디스크와 메모리 수준에서 버퍼링해 즉각 반응성을 줌, 이런 특수 워크플로우엔 LabPlot이나 유사 도구가 정말 필수임
          + 아직 이 툴은 안 써봤지만, 데이터를 드래그&드롭으로 시각화할 수 있다면 기존 도구들에 아주 훌륭한 보완재가 될 것 같음
     * 아마 이게 프로젝트 Github임
       프로젝트 Github
          + 이건 Github 미러이고, 실제 개발은 KDE GitLab에서 주로 진행 중임
            KDE GitLab 공식 개발 저장소
     * HN hug of death 발생인지 궁금함
     * S3 버킷 및 기타 클라우드 오브젝트 스토리지 연동 지원이 추가되면 정말 도움이 될 것 같음, Iceberg 지원도 요즘 인기가 많으니 포함되면 좋겠음
     * 현재 지원하는 데이터베이스가 SQLite뿐이라 아쉬움, 직접 데이터베이스 또는 REST API에 바로 연결하고 싶었는데, 파일을 내보내고 다시 불러들이는 절차가 너무 번거로움
          + LabPlot 메뉴얼에는 다양한 DB 지원이 언급되어 있음
            LabPlot SQL 연동 메뉴얼
            Qt5.15 데이터베이스 드라이버 목록
            혹시 다른 주제를 말하고 있는 것인지 궁금함
          + SQLite는 충분히 훌륭하고 써드파티 REST API 솔루션도 있으니 큰 문제는 아닌 것 같음
     * 이게 Metabase나 Superset의 데스크탑 버전 같은 포지션인지 궁금함
"
"https://news.hada.io/topic?id=22731","OKLCH 색상이란 무엇인가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            OKLCH 색상이란 무엇인가

     * OKLCH는 인간의 시각 인지에 맞춘 최신 색상 모델로, 색상 간의 밝기, 채도, 색상 변화가 균일하게 느껴지는 특성을 가진 지각적 균등 색 모델
     * 구조는 Lightness(밝기), Chroma(채도), Hue(색상) 로 구성되며 기존 모델에 비해 일관된 색상 팔레트 구성이 가능함
     * 같은 밝기·채도를 유지하며 Hue만 변경해도 균일한 색 팔레트를 만들 수 있어 UI 디자인에 유리함
     * sRGB/HSL 대비 예측 가능한 명암 변화와 균일한 그라디언트를 제공하지만, 일부 경우 의도치 않은 색이 생성될 수 있음
     * Display-P3와 같은 최신 디스플레이에서 더욱 넓은 색상 표현이 가능하고, 최신 브라우저에서 CSS Color 4로 지원되며 sRGB 폴백 처리도 제공해 점점 웹 표준으로 자리 잡는 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OKLCH 색상 모델 소개

     * OKLCH는 인지적 균일성(perceptually uniform) 을 목적으로 개발된 최신 색상 모델
     * 이 모델은 사람이 실제로 색을 인식하는 방식에 가깝게 동작하여, 디지털 디자인과 프론트엔드 개발에서 색상 다루기가 훨씬 편리해짐

색상 모델 기본 개념

     * 색상 모델은 색을 수학적으로 정의하고 표현하는 시스템임
     * 주로 사용되는 모델 예시: RGB, HSL, LCH, OKLCH, LAB, XYZ
     * 각각의 모델은 색상 표현 및 조작의 용이성을 결정함

   색상 표기 방법 예시
     * oklch(0.55 0.18 260)
     * hsl(220 100% 50%)
     * rgb(0, 128, 255)
     * lch(60% 60 260)
     * lab(50 -10 -50)
     * color(xyz 0.18 0.19 0.6)
     * #1E90FF

Gamut(색 공간 범위)

     * Gamut은 해당 색상 모델이 표현할 수 있는 전체 색상 범위를 의미함
     * 대표적인 gamut: sRGB(웹 기본), Display-P3(최신 기기 지원)
     * 색 공간은 gamut 범위 외에도 white point, transfer function 등 다양한 속성을 가짐

OKLCH 구조

   OKLCH와 LCH는 모두 Lightness, Chroma, Hue라는 세 가지 값으로 구성됨
   OKLCH는 OKLab 색상 공간을 기반으로 함
     * Lightness(명도/밝기) : 0~1 또는 0%~100%로 표현, 밝기의 균일한 변화를 보장함
     * Chroma(채도) : 색상의 강도, HSL의 Saturation(채도) 와 유사함
     * Hue(색상) : 0~360도 각도로 색상 표현

OKLCH의 장점과 활용

     * 일관된 밝기
          + OKLCH에서는 같은 밝기 및 채도 값에 hue만 다르게 하여 색상을 지정하면, 모든 색상이 유사한 밝기로 느껴짐
          + 기존의 sRGB나 HSL로는 색상별로 밝기나 채도가 일관되지 않게 보일 수 있음
          + 따라서 OKLCH는 지각적으로 균등한 색상 팔레트 제작이 쉬움
     * 예측 가능한 색상 명도 단계
          + OKLCH에서 밝기를 변화시키면, hue(색상)나 chroma(채도)의 변화 없이 다양한 색상 단계 생성이 가능함
          + HSL 등에서는 밝기가 변하면 색조가 변하는 드리프트 현상이 일어남
          + OKLCH는 밝기만 조정해도 일관된 색상 명도 단계 만들기가 가능함
     * 그라디언트(Gradients) 처리 방식
          + sRGB에서는 빨강, 초록, 파랑 값 간 보간이라 중간 지점이 탁해지거나 밝기 변화가 심함
          + OKLCH는 Lightness, Chroma, Hue 축을 따라 보간해 더 자연스러운 그라디언트 구현이 가능함
          + 단, Hue 값이 원형 구조이기 때문에, 예기치 못한 색상 변이가 생길 수 있음
          + 이를 방지하기 위해 OKLab로 직선 보간(interpolation)하여 더 예측 가능한 결과를 얻을 수 있음

색 공간 지원

     * sRGB로는 최신 디스플레이가 표현하는 넓은 색 공간 일부를 커버하지 못함
     * OKLCH를 사용하면 Display-P3 등 넓은 색 영역 지원 기기에서 더 선명한 색상 표현 가능
     * sRGB만 지원하는 기기에선 최대한 비슷한 색상으로 매핑됨

최대 Chroma(채도) 값

     * OKLCH는 실제 스크린이 표현할 수 있는 범위 밖의 색상도 수학적으로 지정 가능함
     * 예를 들어 oklch(0.7 0.4 40)와 같이 너무 큰 chroma 값은 실제 표시 불가능해 가까운 색상으로 클리핑됨
     * 최대 chroma 개념을 이해하고, 빛깔, 밝기, 선정 색 공간(sRGB, Display-P3)에 따라 적절한 값 설정이 중요함

브라우저 지원 및 폴백

     * OKLCH 색상은 CSS Color Module Level 4에서 도입되어, 최신 브라우저에서 대부분 지원됨
     * 구형 브라우저에서는 지원되지 않을 수 있어, @supports CSS 지시어로 폴백 값 설정 가능함
@layer base {
  :root {
    /* sRGB hex */
    --color-gray-100: #fcfcfc;
    --color-gray-200: #fafafa;
    --color-gray-300: #f4f4f4;

    @supports (color: oklch(0 0 0)) {
      /* OKLCH */
      --color-gray-100: oklch(0.991 0 0);
      --color-gray-200: oklch(0.982 0 0);
      --color-gray-300: oklch(0.955 0 0);
    }
  }
}

     * 지원되는 브라우저에선 OKLCH, 미지원 브라우저에선 sRGB(hex) 색상이 적용됨

oklch.fyi 도구

     * oklch.fyi 는 OKLCH 색상 팔레트 생성, CSS 변수 변환 등 OKLCH 관련 기능을 제공하는 웹 도구임
     * 디자인 시스템, 테마 개발 등에서 OKLCH 색상 활용을 쉽게 도와줌

        Hacker News 의견

     * “Better Gradients”라는 기능에 대해 의심이 생김. OKLCH는 극좌표 공간이며, 이 공간에서 Hue는 각도 역할을 함. 각기 다른 두 지점의 색상을 보간할 때는 원의 가장자리를 따라 이동하게 됨. 예를 들어 linear-gradient(in oklch, #f0f, #0f0) 코드를 쓰면 초록에서 보라로 이어지는 동안 눈에 띄게 색상 범위 밖으로 나가게 됨. 다른 방향으로 원을 돌리면 linear-gradient(in oklch longer hue, #f0f, #0f0)와 같이 청록이나 아쿠아를 지나감. 이러한 경로는 지각 기반 색상 공간이 sRGB나 인간이 지각 가능한 색상 영역의 경계, 즉 Gamut의 끝부분을 다루기에 정말 불편함을 보여줌. 실제로, 아주 살짝만 값이 달라져도 색상이 범위를 벗어남. 이를 해결하기 위해 여러 가지 알고리즘이 있으나 지각적 균일성은 희생됨. 예를 들어 그라디언트 상의 빨강이 유난히 어두워짐. 만약 더 좋은
       그라디언트를 원하고 지각적 균일성이 중요하다면, 사실 Oklab 보간이 기본이어야 함. Oklab은 원을 가로질러 직선 경로로 보간하며, 필요하다면 회색도 지나감(linear-gradient(in oklab, #f0f, #0f0)). 이 방법이 자주 보는 sRGB 보간의 어두워짐 현상도 없애주고, magenta에서 lime까지 자연스런 그라디언트를 만듦. 참고로 Tailwind v4가 베타에서는 sRGB 대신 Oklch를 썼다가 최종 릴리즈 때는 Oklab을 더 안전한 기본값으로 선택함
          + 브라우저에서 sRGB로 색상을 보간하는 방법은 사실 버그와 같다고 생각함. sRGB는 로그 스케일의 인코딩이므로, 원래 명세상 색상 보간을 sRGB에서 직접 하지 말고 선형 RGB로 변환하여 보간해야 함
          + 이 글이 수정되었으니 내 코멘트는 무시해도 됨. 그래도 Oklab 보간을 기본으로 해야 한다는 데 동의함. 글에서도 “더 나은 그라디언트”에 대해, OKLCH의 hue가 원형이어서 예상치 못한 경로를 보일 수 있다고 했음. 이런 문제를 피하려면 많은 도구들이 OKLAB을 쓰고, Oklab은 직선 경로로 일관된 결과를 준다고 있음
          + 사실 CIE LAB도 같은 역할을 하면서 표준으로 이미 존재함 (CIE LAB 위키피디아)
            W3C의 검토 자료
          + 매우 유익한 설명을 듣게 되어 고마움. 내가 이 글을 쓴 사람임. 더 명확하게 설명할 수 있게 내용을 수정하겠음
          + 스펙트럼 반대편에 있는 두 색으로 그라디언트를 만드는 경우는 좀 극단적인 사례임. 어떻게 보여야 할지 정답도 없고, 실제로 디자인에서 그런 그라디언트가 필요할 일도 드물다고 생각함. 보통 디자인에서 그라디언트를 쓸 때는 중간 색상을 직접 고르게 됨
     * OKLCH와 관련해 oklch.com에서 직관을 쌓는 데 도움이 되었음. 단, hue 값이 HSL의 hue와 다르고, hue와 lightness 별로 chroma 최대치가 달라지는 점에 주의해야 함. 이건 버그가 아닌 사람 눈과 디스플레이 특성을 반영한 것임. HSL과 달리 HSL은 최대치가 항상 일정하나, 색 의미는 일관되지 않음. CSS의 OKLCH가 좋은 점은 수식 형태로 조작할 수 있는 것임. 예를 들어 oklch(from var(--accent) calc(l + .1) c h)처럼 쓸 수 있음. 하지만 실제로 수식을 잘 사용하려면 컬러 이론이 필요하거나 테스트를 해봐야 함. 예를 들어 그림자는 간단히 lightness만 바꾸면 된다고 생각했으나, 실제로 hue도 바뀌어야 함. OKLCH 그라디언트가 무조건 최고라는 것은 아님. 비슷한 hue를 쓸 때 색이 고르게 보이긴 하지만, 실제로 빛이 섞일 때 물리적으로 어떻게 변화하는지까지 신경 쓴다면 XYZ 색상 공간이
       필요함. 관련해서 MDN color-mix 문서 참고하면 좋음. 참고로 ‘ok’는 진짜 단순히 OK라는 단어에서 온 것임. 의미상 기존 LCH가 문제(OK하지 않음)가 있었던 데서 비롯됨
          + CSS의 OKLCH에서 수식을 쓸 수 있다는 점이 좋다고 했지만, 사실 상대 색상 조작은 모든 CSS 색상 공간에서 쓸 수 있음. 예를 들어 background-color: rgb(from var(--base-color) calc(r - 76.5) g calc(b + 76.5));처럼 사용 가능함
          + OKLCH와 상대 컬러 덕분에 스타일시트에 하드코딩하는 색상 수를 많이 줄일 수 있었음
            예시 CSS 변수
     * 몇몇 비판점은 이미 언급되었지만 OKLCH와 CSS 적용 방법 소개로서 꽤 괜찮은 글이라고 생각함. 부연 질문으로, 최근 블로그 글에 날짜 표기를 하지 않는 트렌드에 대해 궁금해짐. 특히 “OKLCH는 새 색상 모델이다”라는 문구가 앞으로 빨리 구식이 될 거라 생각함. 메인 사이트에는 “2025년 8월”만 써놔서 날자 표기를 의도적으로 안 하는 것처럼 느껴짐
       관련 사이트
          + 정확히 설명할 순 없지만, 날짜 없는 글은 심각한 문제라고 생각함. 이런 경향이 요즘 트렌드인지는 모르겠으나, 날짜가 없으면 얼마든지 옛날 글로 착각할 수 있음. 가끔 댓글에 남아있는 타임스탬프로 글 연도를 추측하는데, 아예 댓글도 지원하지 않는 블로그도 많아서 재방문하고 싶은 마음이 사라짐. (OP 메인 페이지에 “디테일에 집착하지 않는다”고 써 놓았는데 디자인 미적 요소에만 집착하고 기능성은 놓친 것이 아이러니함)
          + 글에 날짜가 없는 경우, 나는 HTTP로 HTML 문서를 받으면서 Last-Modified 헤더를 참고하려고 함. 하지만 작성자가 날짜에 신경 안 쓰는 경우가 많음. 블로그라는 단어에서 알 수 있듯 본래는 [web]-log로서 타임스탬프가 핵심 요소임을 이해하지 않는 것 같음. 참고로 Last-Modified 값은 그 리소스(HTML)의 마지막 수정 시각을 나타냄. 서버가 캐시 구성을 안 하고 백엔드에서 매번 리렌더링하면 날짜가 항상 갱신되는 현상 발생
            이 문제 예시
          + 오래된 콘텐츠가 검색 엔진에서 등급이 떨어지는 것을 막으려는 SEO 목적이 큼. 참 번거로운 트릭임
          + 구글과 SEO 때문임
     * OKLCH 색상이 HSL과 비교해 일관된 파란색을 준다고 했지만, 실제로는 OKLCH의 밝기 변화가 녹색 쪽으로 크게 이동함. Perceptual intensity를 일정하게 맞추는 것은 확실한 장점이 있지만, 여기서 주장하는 효과가 과장되었다고 느껴짐
          + 이 색상의 Hue 차트를 보면 이유를 알 수 있음
            OKLCH 색상 차트
            밝기를 높이면 해당 색상의 Hue 밴드 한계치를 넘어, 디스플레이 상에서 재현 가능한 밝은 파란색의 한계 때문에 색이 Cyan(청록) 쪽으로 이동함. OKLCH는 그라디언트에서 밝기를 조정할 때 Hue 불변이 아니라 Saturation 불변임. 이런 효과가 좋은지는 미적 취향의 문제인데, 오랫동안 Hue가 일정하고 desaturated한 웹 컬러만 써왔기에 “어느 쪽 타협점을 고를지 선택할 수 있다”는 점이 신선함
            관련 해커뉴스 토론
            최근 과학계에서 더 깊은 파란색 LED도 발명됐으니, 10~20년 후엔 Display P3를 대체하는 기술이 오면 이런 Cyan 시프트 현상 없이 더 정확한 파랑 그라디언트를 표현할 수 있을 것이라고 기대함. 그 때까지는 디자인적으로 보기 좋은 게 정답임.
            참고로 앞으로의 색상 공간 설계는 Hue와 Saturation을 반드시 이분법적으로 선택하는 대신 디스플레이의 한계까지 Saturation 불변, 한계점에서 점진적으로 Hue 불변으로 전환하면 어떨까 생각함. 하지만 이것은 색상 프로파일(IDCv4) 제약상 매우 어려운 일임. 그래도 실험적인 DisplayCAL 타깃으로는 재미있을 듯함
          + 내 디스플레이에선 예시에서 green shift가 전혀 안 보임. 혹시 화면 캘리브레이션이 맞지 않은 건 아닌지 궁금함
          + 동의함. 색상이 완전히 파랑에서 시안(청록)으로 바뀜. 만약 OKLCH가 제대로 구현됐다면, 나는 절대 쓰고 싶지 않음. hue 계산에 뭔가 심각한 문제가 있어 보임. HSL/HSV는 지각적 lightness에 문제가 있지만 hue 자체는 항상 일정해서 보정이 필요 없음
          + 색상 과학을 잘 모르고 주관적인 의견일 수 있지만, OKLCH 색상의 오른쪽 끝은 최악이라도 blue-green(푸른-초록) 또는 그냥 light blue(연한 파랑) 정도로 느껴짐. 반면, HSL의 두 가장 밝은 쪽 색상은 blue로 보이지 않고, 차라리 오른쪽에서 두번째는 light purple(연보라), 맨 오른쪽은 거의 회색 같음. 캡션에서는 HSL의 어두운 쪽이 grayish라고 했지만 실제로는 OKLCH와 HSL 모두 leftmost(가장 어두운 쪽)는 파랑 느낌임. macOS Digital Color Meter로 찍어 보면, 오른쪽 끝의 OKLCH와 HSL 모두 green값이 거의 같음(226과 227, sRGB로는 228과 227)
     * 이 주제에 대해 더 좋은 글을 추천함
       Evil Martians의 OKLCH in CSS: Why quit RGB, HSL 글
       사용해 볼 수 있는 OKLCH 변환 툴
       이전 해커뉴스 토론(6개월 전, 30개 댓글)
     * OKLCH는 OKLab(지각 기반 균일 색 공간)을 기반으로 만들어졌으며, Lightness(명도), Chroma(채도), Hue(색상)을 조절하는 모델임. ‘OK’는 만든 사람이 “그럭저럭 잘 동작한다”고 답했던 것에서 따온 명칭임
          + OKLCH의 “OK”가 실제로는 “Ottossons kulör(스웨덴어로 Ottosson의 색)”의 축약이라고 예상함. 창시자가 겸손하게 OK라 했을 수 있음
     * 컬러와 텍스트/로고 작업을 할 때는 대비와 가독성도 꼭 고려해야 한다고 생각함
       APCA Contrast
          + 위 사이트는 최신 대비 알고리즘(APCA, 아직 출시 전 WCAG 3 표준의 공식 알고리즘)을 쓸 때 좋지만, 현재 많은 산업에서는 WCAG 2이 기준이므로 법적 요구사항에도 주의해야 함.
            대비 체크(WCAG 2)
            그 외에도 다양한 도구 존재함
     * 내 머릿속에서 OKLCH라는 이름이 ""Oklachroma""로 자연스럽게 해석되었음
     * OKLab 색상 공간을 소개한 최초의 블로그 글이 있는데, 역사적 관점에서 흥미로움
       OKLab 원본 포스트
     * 왜 OKLCH 색상 공간으로 변환하는 공식이 문서화돼 있지 않은지 궁금함. 구글링을 해보면 Gist 하나 밖에 없고[0], Oklab에 대한 위키피디아는 있음[1]
       Gist 변환 공식 예시
       Oklab 위키피디아
          + 변환의 핵심은 Oklab과 sRGB 간의 매트릭스에 있음. OKLab-OKLCH 변환 자체는 좌표변환에 불과해, 위키피디아에도 나와 있음
          + OKLCH와 OKLab는 같은 색상 공간임. 다만 직교좌표(OKLab) vs. 극좌표(OKLCH)로 표현 방식이 다름
"
"https://news.hada.io/topic?id=22733","Base - macOS용 SQLite 데이터베이스 편집기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Base - macOS용 SQLite 데이터베이스 편집기

     * Base는 macOS 전용의 작고 강력한 SQLite 데이터베이스 편집기로, 직관적인 UI를 제공함
     * 가볍고 사용하기 쉬운 인터페이스로 비전문가와 전문가 모두에게 적합한 도구
     * 스키마 인스펙터를 통해 테이블 구조, 컬럼 타입, 제약조건, 관계를 손쉽게 확인 가능
     * 테이블 편집기를 사용하면 SQL 문을 직접 작성하지 않고도 시각적으로 테이블을 생성·수정할 수 있음
     * 데이터 브라우저와 SQL 에디터를 통해 데이터 탐색, 필터링, 직접 수정, 쿼리 실행 및 스니펫 저장이 가능함
     * CSV/SQL/JSON/Excel 포맷의 임포트·익스포트 지원과 함께 무료 체험판 제공, 전체 기능은 유료 구매 필요함 (£29.99)

   sqlite 용으로만 따로 써도 좋겠네요

   2008년에 처음 출시했고, 최근에 3.0이 릴리즈 된거네요. 해커뉴스에 댓글들 처럼 저도 처음 들어봐서 놀랐어요.
   역시나 이름이 너무 일반적인 단어라서 그런거 아닌가 합니다.
   이런 유틸은 일반 단어에 뭔가 단어를 하나 붙여서 만드는게 좋지 않을까 생각이 들어요. UltraEdit, SublimeText 처럼

   아 Base가 앱 이름이었군요. 제목에 떡하니 적혀있는데도 인지 못했습니다. ㅎㅎㅎ

        Hacker News 의견

     * Base를 약 15년간 사용 중임을 깨달았음, 항상 훌륭했고 시간이 지날수록 더 좋아졌음
          + 보통 “Show HN”에 올라오는 제품들은 최소 기능만 있을 거라고 생각했는데, Base가 15년 이상 실전에서 쓰였다는 사실을 알게 되어 믿음이 생김
     * 나는 이런 형태의 소프트웨어 개발 방식, 즉 “아티장(artisanal)” 스타일을 정말 좋아함, 오픈소스나 엔터프라이즈와는 달리 한두 명의 소규모 팀이 한 가지를 깊이 있게 잘 만들어내는 모습이 인상적임
     * 이게 15년이나 된 앱이었다는 것에 놀람, 이런 제품이 있었으면 진작에 샀을 텐데, 구글, 레딧, Product Hunt, AlternativeTo 어디에도 이게 보이지 않았음, 마치 다른 차원에서 갑자기 튀어나온 느낌임
          + 이름이 너무 평범해서 검색에 잘 안 잡혔다고 생각함, “Base”라는 단어가 너무 일반적임
     * 15년 넘게 전에 이 앱을 구입했던 기억이 있음 (로고가 빛나기 전이었음), 그때도 정말 훌륭한 소프트웨어였고, 이번 업그레이드도 분명 가치 있을 것임, 매우 만족스러운 제품임
     * 만약 UUID 표시 기능이 추가된다면 정말 환상적일 것임, SQLite가 UUID를 기본적으로 지원하지 않지만 많은 사람들이 UUID를 바이너리 blob으로 저장함, Jetbrains 제품은 이 값들을 알아서 UUID로 인식해서 쉽게 편집하게 해줌
          + 의견 남겨줘서 고맙다는 반응, 본인은 아직 그런 동작을 본 적이 없음
          + 나처럼 UUID를 바이너리 blob으로 저장하는 사람들도 은근히 많은 것 같아 놀람, 빈번하게 쓰지 않는 플랫폼 특화 툴에 의존하고 싶지 않음, SQLite studio에서도 blob이 잘 보이지 않아 불편함, 참고로 Mac만 지원하는 점도 아쉬움
          + 바이너리 blob 내부에 7비트 ASCII처럼 완전히 출력 가능한 문자열이 들어있는 경우도 있음, 어쨌든 그 모든 것도 blob임
          + 다양한 RDBMS나 언어마다 UUID 저장 및 정렬 방식이 달라서 바이너리 blob으로 UUID를 저장하는 게 무서울 정도임, MariaDB와 SQL Server는 전용 데이터 타입이 있고 정렬도 구조나 엔디안 차이로 다르게 됨, Oracle은 기본적으로 바이너리로 가정하지만 SYS_GUID()가 엔디안 이슈를 일으킬 수 있음, PostgreSQL은 그냥 문자열로 정렬함, .NET 환경에서 GUID를 넘기면 또 저장 형태가 달라질 수 있음, 그래서 SQLite 데이터베이스가 UUID를 동일한 방식으로 저장한다고 기대하는 건 비현실적이라고 생각함
     * 데이터베이스 구조를 시각적으로 보여주고, 다이어그램이나 SQL 코드로 구조를 편집할 수 있게 동기화해 주는 툴을 좋아함, macOS에서는 OmniGraffle, Windows는 Microsoft Visio가 이런 역할을 한다고 들었음, 개인적으로 Android 태블릿에서 Database Designer를 쓰고 있는데 심플한 프로젝트엔 효과적임, Database Designer (구글 플레이) 개발자는 앱이 영원히 무료라고 했고, 앱 내 구매는 “커피 한 잔”처럼 자발적 기부임, 그리고 온라인 도움말로 간단한 데모 비디오도 링크돼 있음
          + OmniGraffle이 데이터베이스 구조와 SQL 코드 간의 편집 동기화를 직접적으로 지원한다고는 생각하지 않음
     * 방금 Base를 써봤는데 인터페이스가 꽤 마음에 듦, 원래 TablePlus를 많이 썼지만 SQLite에 특화된 기능, 예를 들어 외래 키가 기본 활성화되어 있으면 좋겠고, 확장 프로그램을 자동으로 불러오는 기능이 있으면 좋을 것 같음 (예: sqlite-vec 사용 중 “no such module: vec0” 같은 에러가 나옴), 프로젝트에 계속 관심을 가질 예정임
          + 피드백에 고마움을 전함, 외래 키 자동 활성화 옵션은 정말 필요하다고 생각해서 메모해두겠음, 확장 자동 로딩은 앱스토어 정책과 충돌 소지가 있어서 직접/스토어 버전 간에 기능 차이가 나는 건 조심스럽다고 함, 한번 검토해 봄
     * 2011년부터 Base를 사용해왔음, 자주 쓰지는 않지만 데이터베이스 파일을 열거나 CSV를 데이터베이스로 만들어 분석할 때 항상 가장 먼저 찾게 되는 툴임, 오랜만에 업그레이드에 기꺼이 돈을 낼 수 있어서 기쁨
     * sqlitebrowser와 차별점이 무엇인지 궁금함 sqlitebrowser.org
          + (Base 개발자) 개인적으로 생각하는 장점은 macOS 앱과의 통합성과 테이블 생성/수정 기능이 더 좋다고 생각함, 다만 Base는 아직 SQLCipher를 지원하지 않음
          + UI가 깔끔하고 세련됨을 강조함
          + Base는 완성도가 높아서 다른 선택지들보다 써볼 가치가 있다고 느낌
          + sqlitebrowser가 너무 자주 충돌해서 불안정했다고 함, 그래서 다른 유료 앱을 구입했음
          + UI가 더 쾌적함, 직접적으로는 datagrip을 쓰지만 진정한 네이티브 Mac 소프트웨어를 응원하고 싶어서 구매를 고려 중임
     * 다소 주제에서 벗어나지만, Airtable의 로컬 우선 버전이 필요한 사람이 있는지 궁금함, SQLite를 백엔드로 쓰고 파일/데이터에 연결하며 여러 컴퓨터 간 동기화를 지원하는 제품에 시장성이 있는지 솔로 개발자로서 알고 싶음
          + 신기하게도 나도 최근 1년간 비슷한 걸 개발하고 있음, SQLite는 아니지만 완전히 로컬이면서 네이티브로 동작하고, 비로컬 통합 및 스크립트 지원도 제공함, 관심 있다면 한 달 내로 출시 예정임
"
"https://news.hada.io/topic?id=22736","빅오 표기법의 시각적 소개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             빅오 표기법의 시각적 소개

     * 빅오 표기법은 함수 성능을 입력 크기 변화에 따른 성장 양상으로 표현함
     * 글에서는 대표적으로 상수, 로그, 선형, 제곱 항목의 빅오를 예시와 함께 설명함
     * 자료구조 및 알고리듬에 따라 시간복잡도가 다르며 입력 배열 정렬, 탐색 등에서 차이를 보임
     * 실제 코드 성능 개선을 위해서는 적절한 데이터 구조 선택과 반복문 내 불필요 연산 제거가 핵심임
     * 빅오는 항상 입력과 실행 시간의 관계를 가장 단순화시켜 나타내며, 성능 개선시 코드를 직접 측정하는 것이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

빅오 표기법 개요

     * 빅오 표기법은 시간 측정 대신 입력 크기(n)에 따른 실행 시간의 성장 양상을 설명하는 방법임
     * 함수 실행 시간을 입력 크기에 따라 분류하며 주로 상수(O(1)) , 로그(O(log n)) , 선형(O(n)) , 제곱(O(n²)) 형태가 분석 대상임
     * 이 글은 초심자도 이해할 수 있도록 각 항목의 개념과 시각적 사례, 실제 코드 예시를 통해 설명함

반복(Iterating)과 선형 알고리듬

     * sum(n) 함수는 1에서 n까지 더하는 반복 구조 예시로, 입력값 n이 커질수록 수행 시간도 정비례로 증가함
     * 실제로 sum(1e9)은 약 1초, sum(2e9)은 약 2초 소요로 벽시계 시간(wall-clock time)이 O(n) 패턴으로 성장함
     * 시간복잡도는 함수 입력과 실행 시간의 관계이며, 이를 빅오 표기법으로 표현함(O(n) — n에 비례)
     * 반복 대신 수학 공식을 활용한 sum(n) = (n*(n+1))/2은 실행 시간이 입력값 n과 무관하게 일정(상수) 함
     * 이런 함수는 상수 시간복잡도 O(1) 라고 하며, 입력값 변화에 따른 실행 시간 성장 없음이 특징임

빅오 표기법 문법

     * 빅오의 O는 “Order(성장 차수)”에서 유래하며, 성장 형태 자체만을 표시
     * 실제 실행 시간의 절대값이 아니라 입력 대비 성장의 '패턴'만을 간결히 표기함
     * 예를 들어 O(n) 함수라도 'O(2n)'이나 'O(n+1)'처럼 복잡하게 적지 않고, 가장 단순한 항만 선택

입력 구성을 이용한 시간 단축

     * sum(n) 공식 예시처럼 알고리듬 개선을 통해 시간복잡도가 O(n)에서 O(1)로 변환 가능
     * 다만, 상수 시간복잡도라고 무조건 빠른 것은 아니며, 어떤 연산일지에 따라 실행시간 전체는 달라질 수 있음
     * O(n) 알고리듬이 특정 입력에서는 O(1)보다 빠를 수 있으나, 입력 크기가 커지면 항상 O(1) 방식이 우세해짐

정렬(Sorting)과 제곱(Quadratic) 알고리듬: 버블 정렬 예시

     * 버블 정렬(Bubble Sort) 은 인접 수 교환을 반복하며 배열을 정렬하는 기본 예시임
     * 이미 정렬된 경우는 1회 반복(O(n)), 역순은 반복적으로 n회 순회 필요 → 최악의 경우 전체 연산 수 n²
     * O(n²) 알고리듬은 입력이 커질수록 실행 시간이 제곱 형태로 크게 증가
     * 실제 활용에서 빅오는 항상 최악의 경우(worst-case) 기준임(단, 경우에 따라 평균/최선도 표기)
     * 배열 초기 상태에 따라 반복 회수가 줄어들기도 하지만, 최악 케이스 고려로 항상 제곱 시간복잡도로 분류

탐색(Searching)과 로그 알고리듬: 이진 탐색 예시

     * 이진 탐색(Binary Search) 은 정렬된 범위의 중앙값을 추정하고, 매 단계마다 후보 영역을 절반씩 소거함
     * 예를 들어 1~100 사이 특정 수 맞추기에 최대 7회, 1~10억까지도 31회 미만 시도로 가능
     * 매 단계마다 후보 리스트가 반씩 줄어드는 구조로 실행 시간은 O(log n) (로그 시간복잡도)임
     * 로그형 알고리듬은 n이 커질 때 증가 속도가 굉장히 느린 형태를 보임(선형 또는 제곱에 비해 월등히 효율적)
     * 그래프 비교 시 log n, n, n²순으로 성장 차이가 극명하게 드러남

실제 적용: 시간복잡도 개선 팁

  리스트에서 항목 찾기

     * 기본적으로 배열에서 값을 찾는 함수는 O(n) 에 해당함
     * 빈번하게 탐색하는 경우, Set과 같은 자료구조를 사용하면 O(1) 로 향상 가능
     * 단, new Set(array)로 변환하는 과정 자체가 O(n) 이므로 빈번 조회에만 적절함(변환 비용 고려)
     * 예: items.has(""banana"")는 상수 시간복잡도를 제공

  인덱스 활용 반복문 작성

     * 아래와 같이 반복문 내부에서 .indexOf를 사용하는 코드가 흔히 성능 문제의 원인임
function buildList(items) {
  const output = [];
  for (const item of items) {
    const index = items.indexOf(item);
    output.push(`Item ${index + 1}: ${item}`);
  }
  return output.join(""\n"");
}

     * .indexOf는 루프 내에서 O(n) 연산이기 때문에, 전체적으로 O(n^2) 패턴이 됨
     * 인덱스 기반 반복 또는 forEach((item, index) => ...) 활용 시 O(n) 으로 개선됨
function buildList(items) {
  const output = [];
  for (let i = 0; i < items.length; i++) {
    output.push(`Item ${i + 1}: ${items[i]}`);
  }
  return output.join(""\n"");
}

  메모이제이션(Memoization) 활용

     * 팩토리얼과 같이 반복 호출 시 중복 계산되는 구조는 결과 캐싱(Map 활용) 을 적용하여 성능 향상 가능
     * Map에서의 조회는 O(1) 에 해당하여 불필요 재계산 최소화
     * 단, 캐싱은 평균 시간 개선에 기여하며, 최악 시간복잡도 자체는 변하지 않아도 효율적 성능 향상 가능
const cache = new Map();
function factorial(n) {
  if (cache.has(n)) {
    return cache.get(n);
  }
  if (n === 0) {
    return 1;
  }
  const result = n * factorial(n - 1);
  cache.set(n, result);
  return result;
}

성능 평가와 결론

     * 코드 성능 개선 시 이론상 시간복잡도와 함께 직접 실행 테스트로 실제 개선 여부를 확인해야 함
     * 빅오는 입력과 실행 시간의 관계와 성장 패턴을 가장 본질적으로 단순화해서 표현함
     * 좋은 알고리듬 선택 및 데이터 구조 최적화로 코드 효율성을 극대화할 수 있음

요약 정리

     * 빅오 표기법은 함수 입력값과 실행 시간의 관계를 표현
     * 주요 성능 등급: O(1) (상수), O(log n) (로그), O(n) (선형), O(n^2) (제곱)
     * 효율적 코드 작성 위해서는 적절한 알고리듬과 반복문 최적화가 중요
     * 실제 성능은 직접 측정해 개선 여부를 검증 필요
     * 성장 패턴 비교 그래프를 활용해 시간복잡도 특성을 한눈에 파악 가능

        Hacker News 의견

     * 이 글과 HN 댓글들도 Big O Notation을 설명하고 실제 활용과 기술적 세부사항에 대해 논쟁하는 전통을 계속 이어가는 중임. 참고할 만한 예시로 이 해설글과 전문가 태도에 관한 글이 있음
          + 저번 글 댓글을 보면 Pyon이라는 유저가 독설적이고 융통성 없는 태도를 보였음. 하지만 Ned의 반박도 썩 훌륭하진 않음. 기술적 세부내용을 정확히 설명하지 않고, 그냥 “특정 세부사항”이라고만 반복해서 돌려 말하는 느낌임. 왜 그의 비판이 단순한 딴지였는지도, 왜 굳이 내용 자체도 배척하는지도 아쉬움. Ned가 온라인에서의 소통과 공감 자체엔 옳은 방향성을 보여주긴 함. 그래도 교육자라면 왜 그 기술적 논점이 너무 세세하거나 딴지인지 한번은 짚어줬으면 좋겠음. Ned 스스로 “수십 년간 몰랐다“고만 하니, 그게 충분하지 않은 느낌임. 그리고 원 댓글 스레드를 다시 보니 실제로 Ned는 꽤 외교적이고 진지하게 논쟁을 했음. 그런데 왜 블로그 글에선 그 분석이 누락됐는지 의문임. 개인적으로 기술적 세부사항이 뭔지는 잘 모르겠는데, 한 번은
            간단하게 요약 설명해줬으면 하는 마음임
          + 나는 비판적 전문가에 가까움. 블로그에서 복잡한 주제를 가르치려는 시도를 보면 항상 실망하는 이유는, 대개 비전문가가 설명하면서 정확성을 놓친다는 점임. 그 결과 1) 부정확한 내용이 인터넷 여기저기로 복붙되고, 2) 독자들은 블로그 수준만 보고 더 안 배우려 하면서 무지를 강화하게 됨. 그리고 추가로, 페이지 레이아웃도 마음에 안 들었음. ADHD와 기억력이 떨어지는 내 경험상, 적절한 형식(소제목/굵게/구분색/불릿 등)으로 끊어줘야 따라갈 수 있는데, 이 글은 그냥 텍스트 벽처럼 느껴졌음. 요점 파악에 시간이 더 걸릴수록 집중을 잃게 됨. Simple Wikipedia Big O 설명이 훨씬 직설적임. 반면 정식 위키피디아 페이지는 수학이 갑자기 나와서, 직접 보면 Big O가 생각보다 훨씬 복잡한 주제임을 알게 되고 “단순화하는 건 오히려 안 좋을 수도
            있겠다”는 결론에 이름
          + 두 번째 링크는 Big-O에 관한 이야기가 아니고, 저런 태도를 본받을 필요는 없음
          + Ned가 며칠 전에 내게 이메일을 보내줬는데, 기분 좋게 나도 이런 논의에 기여하고 있음
          + 이런 글들을 보면 진짜 교훈은 “틀리거나 오해의 소지가 있는 설명이 있으면 교정을 멈추라는 게 아니라, 온라인에서는 일부 ‘전문가’들이 단순히 논쟁에서 이기고자만 한다”는 점임. Pyon의 태도를 보면 상당히 공격적이고 인터넷 트롤 같았음. “그러니까 기술적 세부사항은 중요하지 않고 부정확해도 괜찮다”는 결론을 내려선 절대 안 됨
     * O(1)이 실제로는 해싱 함수를 이용하는데, 이는 단순하진 않지만 일정한 연산 소요가 발생함. 데이터가 아주 적으면 O(n^2)와 같은 최악의 알고리즘이 오히려 실제 시간 면에서 더 빠를 수 있음
          + 맞는 말이지만 너무 크게 떠들진 않는 게 좋음. 실무에서 n^2면 컴퓨터가 멈춘다고 이해시키는 것만도 힘든 법임. 게다가 경우에 따라선 mod처럼 완벽한 해시함수도 쓸 수 있음
     * Big-O의 현대적 중요성이 예전만 못하다고 느낌. 요즘 하드웨어는 멀티스레드, 파이프라인, NUMA, 복잡한 캐싱 등으로 언제는 한 사이클 미만에도 끝나고 반대로 수백~수천 사이클이 걸리는 연산도 있음. innermost loop 횟수만으로 알고리즘을 설명하려 들면 오히려 현실을 왜곡하게 됨. 그리고 Big-O를 논할 땐 Big-Omega와 같은 다른 표기도 반드시 언급해야 함. (참고로 Big-O를 소재로 한 애니메이션도 재밌게 봤음)
          + Big-O 이론은 저런 장치 종속적 요소와 상관없이 연산량을 정의하려고 탄생한 개념임. 그런 의미에서 시대를 타지 않는 도구임. (괜찮은 발표자라면 대개 “C 같은 상수는 N이 작을 땐 매우 중요한 요소가 된다”고 반드시 언급해주기도 함)
     * 진짜 흥미로운 건 양자계산에서 어떤 연산은 원자 수에 대해 O(n^7)로 증가하지만, 과학자들이 실제로 이 계산을 실행하는 걸 두려워하지 않는다는 점임. 왜냐면 N이 충분히 작고, 컴퓨터와 메모리는 계속 빨라지며, 결과값은 엄청난 가치가 있기 때문임. (내가 컴퓨터과학 전문가는 아니라 O() 표기법을 잘못 썼으면 양해 바람)
          + 그냥 “n^7에 비례해 증가“라고 하면 됨. O(n^7)라고 하면 대부분 이해는 하는데, O는 수학적으로는 ‘상한’만 나타내므로 엄밀히 정확하진 않음. 정말 정확히 말하려면 Ω(n^7)처럼 쓰는 게 맞음
     * 시각화가 정말 마음에 듦. 예전에 알고리즘 배웠던 입장에서도 시각적으로 보는 경험이 여전히 큰 도움이 됨
     * 전기공학을 수강해서 그런지 Big O Notation이 언제나 뭔가 대충 건너뛰는 개념처럼 다뤄져 왔다고 느낌. 항상 당연히 아는 내용인 것처럼 취급되어서 정말 친절하게 설명하는 경우를 못 본 것 같음. 어느 정도 수준의 수학이나 컴공 과정에서 이 개념이 처음 소개되는지 궁금함
          + 전산 전공 코스의 Discrete Math 시간에서 가장 체계적으로 Big-O를 배웠음
          + 내 학교에선 Algorithm Analysis(필수과목)에서 Big-O와 여러 증명법을 가르쳤음. 다만 이 과목은 거의 3~4학년때 듣는 코스였고, 실제로는 이미 1학년 정도면 어느 정도 개념을 흡수했을 거라는 암묵적 전제가 있었음(아마도 주변에서 자연스럽게 배운다는 느낌)
          + 수학적으로 함수 f(x)가 O(g(x))라는 건 f(x)/g(x)가 어떤 상수 C에 대해 “모든 x에 대해서 f(x)/g(x) < C”를 만족한다는 뜻임. 컴과에서는 f(x)가 특정 알고리즘의 연산 횟수와 같은 복잡성을 뜻하는 경우가 많음
          + Big-O Notation 설계는 여러 해석이 가능함. 예를 들어 어떤 알고리즘을 Turing Machine의 동작 스텝수로 정의하면, log 시간 알고리즘이란 게 있을 수 없고 O(log n)는 O(1)로 취급됨
          + 전산 1학년 필수 과목에서 배웠음. 별거 없고, 입력 데이터가 많아질 때 연산량이 어떻게 늘어나는가만 서술하는 개념임. 겉보기엔 어려운데 실제론 아주 간단하고 명쾌함
     * 동적 시각화가 이해에 엄청난 도움이 되었음. 이처럼 더 많은 레슨/자료를 만들어주길 바람
          + 그 반응이 참 고맙고 기쁨
     * Big-O Notation 관련 스레드가 올라올 때마다 혹시 누가 이 개념이 애니메이션 The Big O와 어떻게 연결되는지 설명해주길 기대하게 됨. 아직도 그 애니 내용이 뭔지 잘 모르겠음
          + (맥주 4캔 벌컥벌컥)<p>좋아 들어봐. 그 애니는 Pacific Rim, Dark City, The Matrix 세 가지를 차례로 섞어놓은 것 같은 내용임
     * 개인적으로 Big O Notation을 가장 효과적으로 이해하는 방법은 일상을 빗대어 연결해보는 것임
     * 아름다운 자료라 생각함. 신호를 보내봤는데 잘 전달됐길 바라며, 괜히 도파민 한 스푼 얻은 느낌임
          + 잘 도착했음. 고마움 <3
"
"https://news.hada.io/topic?id=22712","Valve Software 신입 사원용 핸드북 [pdf] (2012)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Valve Software 신입 사원용 핸드북 [pdf] (2012)

     * Valve는 전통적인 관리 구조 없이 수평적인 조직 문화를 지향함
     * 신입 사원은 프로젝트 선택과 업무 방법을 스스로 결정하는 문화에 적응 과정이 필요함
     * 핸드북은 실무 팁보다Valve의 기본 원칙과 의사결정 시 고려할 점을 중심으로 설명함
     * 채용은 회사의 미래를 좌우하는 가장 중요한 일로 평가됨
     * Valve는 혁신적이고 주도적인 인재가 자유롭게 역량을 발휘할 수 있는 환경 조성을 목표로 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서문

   Valve는 1996년 설립 이래로 위대한 게임 개발을 목표로 삼으며, 그러한 성과를 이끌어낼 수 있는 특별한 근무 환경을 먼저 구축하는 데 중점을 두었음. 직원이 가진 아이디어와 재능, 에너지가 Valve의 미래를 밝힐 핵심 요소임. 이 핸드북은 Valve의 주요 원칙을 간추려 신입 구성원 모두에게 안내 역할을 하도록 구성됨.

이 책의 활용 방법

   이 핸드북은 복리후생이나 업무용 PC 세팅 같은 실무 지침서가 아님. Valve만의 비직관적인 일하는 방식과, 주도적으로 만들어가는 선택지, 그리고 그 선택 과정에서의 사고방식을 안내함. 추가적인 실무 정보는 사내 인트라넷을 통해 확인 가능함.

   이 책은 인트라넷에서 수정 및 개선이 가능한 오픈 형태로 제공되며, 신입 사원들이 읽은 후 알맞게 내용을 추가하거나 의견을 제시할 수 있음.

Valve 입문

  첫 출근

   Valve에 합류한 것을 환영하며, 기존 회사와는 전혀 다른 혁신적인 일하는 방식을 경험하게 됨. 이 핸드북은 새로운 환경에 부드럽게 적응할 수 있도록 기존 직원의 경험을 바탕으로 작성됨.

  알아둬야 할 Valve의 특징

     * Valve는 자본 조달 없이 자체 자금으로 운영하며, 이를 통해 회사와 사업 방식에 있어 큰 자유도를 확보함
     * 지적 재산권을 직접 소유하고 있음. Half-Life 출시 후 직접 지재권을 확보함으로써, 제품에 대한 의사결정에서 독립권을 얻었음
     * Valve는 게임 회사이면서도 엔터테인먼트, 소프트웨어, 플랫폼 등으로 확장하여 다양한 방향으로 제품 및 사업 다각화를 추진함
     * 중심에는 열정적인 인재들이 있으며, 이들이 다양한 역할과 혁신을 주도함

  수평적인 조직(Flatland)

   Valve는 직급이나 관리자 없이, 누구도 누구에게 직접 보고하지 않는 구조를 가짐. 회사의 창립자이자 대표가 있기는 하지만, 그조차 상사가 아니며 회사 전체 운영에 대한 주도권은 직원 각자에게 있음. 모든 직원은 프로젝트를 승인하거나 제품을 출시할 권한을 가짐.

   수평 구조는 고객 가치 창출과 빠른 의사결정을 가능하게 하는 동시에, 모든 직원에게 높은 책임감을 요구함. 채용이 중요한 이유도, 새로운 인재가 회사 전체 운영까지 책임질 수 있는 역량을 갖추었는지 평가하는 것이기 때문임.

  이동 가능한 책상

   직원 책상에 달린 바퀴는 물리적으로나 상징적으로든 언제든 자신이 더 가치 있게 기여할 수 있는 위치로 이동해야 한다는 의미를 담고 있음. 사람 또는 팀 단위로 책상이 자주 이동하며, 업무에 가장 유익한 동료들과 가까이에서 협업할 수 있음.

   이로 인해 사내에서 사람 찾기가 어려울 수도 있어, 현재 책상 위치를 확인하는 인터넷 시스템도 갖추고 있음.

적응 과정

  첫 한 달

   책상 위치를 정하고, 기본적인 사무실 환경에 익숙해지면, 이제 자신의 업무 선택이 핵심 과제가 됨. 이 과정에서는 Valve 내의 프로젝트 운영 방식, 협업 구조(카발·팀), 그리고 제품 출시 흐름을 배우게 됨.

  스스로 일거리 선택

   Valve에서는 업무의 100%가 스스로 선택한 프로젝트로 이루어짐. 각 직원은 발굴할 만한 가치가 있는 일, 자신의 강점이 가장 잘 발휘되는 일, 회사나 고객에게 의미 있는 일을 자발적으로 골라 참여함.

   다른 사람이 일감을 배정해주지 않고, 직원 각각이 직접적인 스테이크홀더로서 주도적으로 업무를 결정함.

  프로젝트 정보 파악 및 의사소통

   진행 중인 프로젝트 현황은 공식 목록이 존재하나, 가장 효과적인 방법은 다른 사람과의 직접적인 대화임. 자신이 관심 있는 분야나 현재 고민, 경험 등도 자연스럽게 공유하면서, 동료에게 자신을 알리고 상호 정보 소통을 촉진함.

   내부적으로 회사/프로젝트 현황을 더 잘 공유할 수 있는 방안이 있다면 직접 제안해 실행할 수 있음.

  단기 vs. 장기 목표

   각자 업무 우선순위를 자율적으로 설정하다 보면 단기적이고 측정 가능한 성과에 집중하는 경향이 생김. 하지만, 단기성과에만 치중하면 장기적인 변화나 기회를 놓치게 될 수 있으므로, 장기적 목표에도 노력을 분산해야 함이 강조됨.

  다양한 의견 경청

   경력이 오래된 동료가 특정 프로젝트에 관여하라는 조언을 하더라도, 자신의 주도권을 잃지 말고 다양한 의견을 경청하며, 더 많은 사람과 논의를 넓혀 의사결정의 폭을 확장해야 함. 최종 결과에 대한 책임자는 자신이며, 궁극적으로는 고객의 이익이 최우선임.

        Hacker News 의견

     * Chet Faliszek가 밝혔듯이 Valve 핸드북은 실제로 직원들에게 배포된 것이 아니라, 회사에 대한 홍보와 채용을 목적으로 제작된 것임
          + 핸드북에서 말하는 원칙들이 적어도 잠깐이라도 지켜지지 않았다는 게 믿기 어렵다고 생각함. 웹사이트에도 공개되어 있는데, 주장에 대한 출처를 보고 싶음. 비위계 구조는 종종 내부 권력 다툼과 조작에 취약하다는 점에서 실제로 해당 시스템이 지속됐을지 의심스러움. 참고로 Structurelessness의 폭정(https://www.jofreeman.com/joreen/tyranny.htm) 등도 그런 사례임
          + Chet Faliszek가 그런 말을 했다는 신뢰할 만한 출처를 찾지 못함. 구글 검색 결과에도 그런 직접적인 언급을 못 봤음
          + 핸드북이 처음 나왔을 때는 스텔스 마케팅에 익숙하지 않아 감을 못 잡았음. 똑똑한 사람들도 광고에 별로 영향받지 않는다고들 하지만, 실제로는 타깃 그룹별로 반응이 다르고, 이 핸드북은 Reddit 취향에 절묘하게 맞춰진 캠페인임
          + 이게 직원들에게 공식적으로 배포되지 않았다 해도, 입사 전 지원자들을 대상으로 공개됐다면 본질적으로 큰 차이가 있는 것인지 궁금함. 만약 회사가 핸드북과 전혀 다르게 운영됐다면 큰 문제일 텐데, 실상 핸드북에 적힌 내용이 꽤 정확하지 않나 생각함
          + 실제로 직원들에게 배포한 건 맞지만, 본래 목적은 리크루팅 도구였음. 내부에서 일부러 유출되도록 진행한 뒤 홈페이지에도 공식 게시했음
     * Valve가 1st party 게임 콘텐츠 생산을 줄인다는 비판을 많이 받지만, Steam은 게이머 커뮤니티 전체적으로 볼 때 큰 긍정 효과를 줬다고 생각함. 개방성 때문에 혼란이나 문제도 생겼지만 전반적으로 긍정적임. Steam이 받는 30% 수수료도, 개발자와 시장에 제공하는 가치에 비추면 정당하다고 봄. 파트너사로부터 존중받으며 협력하는 것이 큰 메리트임. 넷플릭스처럼 모든 것을 한 곳에서 관리할 수 있는 경험을 제공하며, EA 같은 대형 개발사도 콘텐츠를 다시 Steam에 올렸음. HL3와 일관성 있는 게이밍 생태계 중 하나를 골라야 한다면, 후자를 고를 것임
          + 앞으로가 걱정임. Gabe가 없어진 뒤 PC 게이밍이 Steam 중심의 독점 구조에 완전히 묶인 채로 남게 될까봐 우려임. 만약 Steam이 월 10달러 요금을 받겠다고 해도, 이미 샀던 게임 때문에 많은 유저가 울며 겨자 먹기로 따를 수밖에 없는 상황임. 왜 Steam은 디지털 소유권의 위험 논쟁에서 늘 예외 취급받는지 불가사의함
          + Steam은 경쟁을 억압하지 않는 플랫폼임. 개발사가 직접 게임을 판매할 수 있고, 다른 플랫폼에도 올릴 수 있음. 그럼에도 불구하고 게이머들이 Steam의 편의성과 유틸리티를 감안하여 계속 이곳을 사용함. 독점적 시장지배로 시장을 조이는 게 아니라, 유저가 직접 선택한 결과임
          + 소비자 입장에서는 동의하는데, 개발자 입장에서는 30% 커미션이 엄청난 비용임. 예를 들어 10만 부를 10달러씩 팔면 총 100만 달러 매출이 생김. 부가세와 환불(16%) 제외 후 84만 달러, Valve가 30% 떼고 나면 58.8만 달러, 퍼블리셔가 투자금 회수 후 38.8만 달러, 이 중 절반인 19.4만 달러만 받을 수 있음. 세금 까면 순수익은 약 15.5만 달러 남음. 퍼블리셔가 마케팅비 등 추가 회수해가면 더 줄어듦. Valve는 결제/배포만으로 25만 달러 이상을 가져가며, Steam은 자체적으로 마케팅을 해주지도 않음. 이런 구조가 작은 개발사에겐 매우 비효율적임
          + Steam의 30% 수수료가 정당하다는 의견은 충분히 논쟁거리임. Steam은 많은 부분에서 개발자들이 자신이 시장을 직접 만들고 Steam을 마케팅해준 덕분에 성장했다고 볼 수 있음. Steam은 별도의 노출을 제공하지 않고, 이미 외부에서 충분히 인기를 끈 경우에만 발견 기능(Discovery Queue) 등에서 다뤄줌. 참고로 리테일 시절이 오히려 스튜디오 수익률이 더 높았다는 Tim Sweeney의 언급도 있음 (링크). Steam은 모든 게임에 무료로 마케팅 효과를 얻고, 실질적인 마케팅이나 초기 유저 유입을 직접 지원하지 않음. 반면 수수료는 30%. Amazon처럼 추천 링크를 두는 것도 아님. 이런 점에 실망감이 큼
          + HL3와 생태계가 양자택일처럼 이야기되는 게 흥미로움. HL3 개발팀이 독립 스튜디오처럼, Valve의 자금 지원을 받으면서 Steam 플랫폼을 활용하는 방식도 가능할 것임. 수익이 충분히 나온다면 뭐가 문제인지 모르겠음
     * Valve 핸드북 번역판은 여기에서 볼 수 있음. HN에서 예전에 여러 번 토론된 적도 있으며, (2012년 4월, 2014년 12월, 2015년 3월, 2016년 7월, 2018년 9월, 2022년 10월 등) 해당 토론 링크들도 남김
          + 추가로 참고할 만한 토론: 2024년 8월, 2021년 4월
     * ""가장 똑똑하고 혁신적이며 재능 있는 사람들을 뽑아 시키는 일만 앉혀두는 건 그들의 가치를 99% 파괴하는 셈임""이라는 문장이 나에게는 현실적으로 다가옴. 그런데 왜 다른 성공한 회사들은 반대의 방식을 고집하는지 궁금함
          + 똑똑하고 재능 있다고 자율적 협력 능력이 자동으로 생기는 게 아니기 때문임. 좋은 구조는 각자의 강점을 최대한 발휘하게 돕는 역할임. 자율적인 사람들도 꼭 동일 목표로 움직이지 않기 때문에 구조가 필요하다고 봄
          + 대기업도 사실 저런 견해에 동의하지 않음. 경쟁될 가능성 있는 스타트업을 인수하거나 팀 전체를 acquihire해서 위험을 미리 차단하는 일이 많음
     * Valve 핸드북이 이상적인 경영 사례로 자주 회자되지만 실제로는 Valve가 요즘 생산하는 것이 그리 많지 않은 듯 보임
          + 나는 지금 어떤 PC 게임이든 자유롭게 즐기고 있고 Windows는 쓰지 않음. Valve 덕분에 리눅스 게이밍이 현실이 됐음
          + Valve의 Proton 덕분에 리눅스에서 게이밍 환경이 마련됐음
          + 매일 같이 Proton을 사용하고 있지만, 신작 MOBA인 Deadlock도 언급하고 싶음. 오리지널 DOTA 이후로 이렇게 신선하고 재미있게 느껴지는 MOBA는 처음임
          + 중요한 것은 독점적 지위를 갖고 시장을 쥐락펴락할 수 있는 구조를 만든 것임. Valve는 Steam으로 이걸 해냈음. 내부적으로 별별 비효율적인 비즈니스 관행이 있더라도 돈이 계속 들어오면 아무 상관없음. 많은 사람이 Valve의 플랫 구조를 흉내 내면 성공할 거라 착각하지만, 실제 성공 원인은 그게 아님
     * 좋은 핸드북이나 직원 가이드 문서 사례를 찾고 있었는데, Valve 건은 철학적 내용이 많아 실제로 리크루팅 도구로 더 자주 언급된다는 점이 이해됨. 혹시 추천할 만한 사례가 있으면 공유 부탁임
     * 후속 편(2012년 이후)이 있다면 Valve의 프로세스가 어떻게 진화했는지 알 수 있어 흥미로울 듯함
          + 이건 결국 PR용이었음
          + 이게 한 번만 나온 단발성 프로젝트였다는 얘기를 어디선가 들었던 것 같은데, 확실한 출처는 기억 안 남. 방금 다른 댓글에서 언급된 Chet Faliszek가 아마 그 정보 출처였을 것임
     * 게임 개발업계에서 워라밸, 자녀/가족에 관한 논의가 이렇게 많이 나오는 게 의외임. Valve가 원래 이런 기업인지, 아니면 상대적으로 그런 것인지 궁금함
          + 게임업계는 정말 연봉도 낮고 열악한 업종임. FAANG 대비 1/5 수준, 일반 소프트웨어 엔지니어 대비 절반 수준임. Epic Games 등 일부만 예외로, 극히 소수만 경쟁력 있는 연봉을 줌. 게다가 크런치, 잡 안정성 부족 등 스트레스도 심함. 프로젝트 끝나면 잘리는 일도 흔함. Valve는 500명 미만 소규모라서, 업계 인재들에겐 일종의 꿈의 직장임. 대형사 대비 직원 수가 매우 적음. (인디 게임 회사 종사자임)
          + Valve는 평범한 회사가 아님. 엄청난 수익을 내고 외부 투자 없이 모든 이익을 R&D에 재투자하고 소규모로 운영함. 완전 플랫한 경영구조까지 갖춤. 돈 잘 버는 작은 회사가 모든 복지 다 챙기는 전형적인 사례임
          + 2012년 즈음이면 Valve가 이미 게임 개발에서 서비스 기업으로 중심을 옮기는 중이었음. 그 시점에 주요 히트작들은 이미 다 출시됐고, 그 이후로 새로 나온 건 Half-Life: Alyx(2020), Counter-Strike 2(2023), Deadlock(TBA)뿐임
     * Valve는 “플랫 구조(Flat)”라며 공식적으로 관리 체계가 없다고 말하지만, 실제로 프로젝트 리드가 자연스럽게 생겨 전체 정보를 모으고 모두가 그 리소스를 공유하는 구조임. 리드가 관리자는 아니지만 사실상 핵심 기술적 관리 역할 수행임. 창의적 기반을 마련하고자 하는 것 이해하지만, 결국 이게 일반적인 관리자의 핵심 역할임
          + 플랫 조직에 대한 대표적 비판은, 어차피 관리 체계는 자발적으로 생성되며 결국 잘못된 방식으로 굳어진다는 것임. ""Structurelessness의 폭정""이라는 고전 에세이가 이 점을 잘 설명함 (링크)
     * “우리는 Gabe Newell을 위해 돈을 뽑아내는 신성한 Steam이라는 기계를 지키는 신관임. 이를 이해하지 못하는 사람은 고용하지 마라”라고 한 문장으로 줄일 수도 있었을 것임. 실리콘밸리 기업들도 Valve에게서 많이 배워야 할 점이 있음
"
"https://news.hada.io/topic?id=22666","구글, AI 프롬프트가 소비하는 에너지 사용량 최초 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    구글, AI 프롬프트가 소비하는 에너지 사용량 최초 공개

     * 구글은 자사 Gemini 앱이 프롬프트 처리 시 소비하는 전력·물·탄소 배출량을 공개하며 AI 에너지 사용에 대한 최초의 구체적 데이터 제시
     * 평균적인 텍스트 프롬프트는 0.24Wh 전력을 소비해 전자레인지 1초 사용과 비슷하며, 0.26ml 물과 0.03g 이산화탄소를 발생시킴
     * 이 수치는 AI 칩(58%), CPU·메모리(25%), 백업 장비(10%), 데이터센터 운영(8%) 등 모든 인프라를 포함한 포괄적 분석 결과
     * 2024년 5월 대비 2025년 5월에는 에너지 효율이 33배 개선되었으며, 구글은 소프트웨어 최적화와 모델 개선을 이유로 꼽았음
     * 이번 발표는 대형 AI 기업의 투명성 확대라는 점에서 의의가 크지만, 전체 쿼리 수와 같은 핵심 정보는 여전히 공개되지 않아 표준화된 AI 에너지 평가 체계 필요성이 제기됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

구글의 Gemini 프롬프트 에너지 사용량 공개

     * 구글은 Gemini 모델이 텍스트 프롬프트 처리 시 소비하는 전력·탄소 배출·물 사용량을 공개한 첫 대형 AI 기업임
     * 평균 프롬프트 1개는 0.24Wh 전력, 0.26ml 물, 0.03g CO₂를 발생시키며 이는 전자레인지 1초 가동이나 물방울 다섯 방울 수준과 유사함
     * 이번 발표는 MIT Technology Review와의 인터뷰를 통해 상세 데이터와 산출 방식을 설명함

에너지 사용 세부 구조

     * 총 전력 소비 중 AI 칩(TPU) 이 58%, CPU·메모리가 25%, 백업 장비가 10%, 데이터센터 운영(냉각·전력 변환) 이 8%를 차지함
     * 구글은 하드웨어 전체 인프라를 포함하는 포괄적 방식으로 분석했다고 설명함
     * 이는 연구자들이 접근하기 어려운 내부 데이터를 공개한 것으로, 산업계 연구 기여로 평가받음

프롬프트별 차이와 한계

     * 공개된 수치는 중간값(median) 으로, 일부 쿼리는 훨씬 높은 에너지를 소모할 수 있음
          + 예: 수십 권의 책을 요약하거나 reasoning 모델을 활용한 복잡한 연산
     * 이번 보고서는 텍스트 프롬프트만 대상으로 했으며, 이미지·영상 생성은 포함되지 않았음
     * 따라서 전체 Gemini 활용에서의 실제 총량을 파악하기에는 여전히 제한이 존재함

효율 개선과 탄소 배출 추정

     * 구글은 2024년 5월 대비 2025년 5월에 프롬프트당 에너지 소비가 33배 감소했다고 발표함
     * 이는 모델 아키텍처 개선과 소프트웨어 최적화의 결과로 설명됨
     * 배출량 추정은 미국 전력망 평균이 아닌, 구글이 구매한 청정에너지 비중을 반영한 시장 기반 방식을 사용해, 일반 전력망 대비 약 1/3 수준으로 산정됨

연구자 및 업계 반응

     * University of Michigan의 ML.Energy 프로젝트 관계자들은 이번 발표를 가장 포괄적이고 중요한 분석으로 평가함
     * Hugging Face의 연구자들은 표준화된 AI 에너지 등급제 필요성을 강조하며, 현재는 기업이 선택적으로 공개하는 수준임을 지적함
     * 이번 보고서는 AI 사용의 실제 자원 소비에 대한 이해를 확장했지만, 총 쿼리 수와 같은 핵심 데이터 부재는 여전히 큰 한계로 남아 있음

        Hacker News 의견

     * 원문 기사에서 관련 보도자료와 보고서 링크를 찾을 수 없어서 직접 첨부함

     전체적으로, 중간값 프롬프트(에너지 소비량 중간에 해당하는 프롬프트)가 0.24와트시 전기를 소모한다고 명시되어 있음
     RTX 6000 두 개 사용 가정 시 총 600와트, 이론상 응답 시간 1.44초임
     즉 실제로 이 중간값 프롬프트는 고성능/비용 높은 AI 모델이 아니라는 점이 명확함
     상당히 작은 수치임
     참고로 전기차가 363마일 달리는데 82kWh를 쓰면, 0.24Wh는 전기차로 1.7미터(5.6피트) 이동하는 에너지와 비슷함
     AI 전기 수요로 전력망이 과부하된다는 보도를 볼 때마다 생각드는 점은, AI 붐 이전에도 이미 전기차, 인덕션 레인지, 히트펌프 등 도입을 위해 잉여 발전 용량을 준비했어야 하는 것 아닌가 하는 의문임
     구글 공식 블로그 참고
          + 전력 인프라 과부하 관련 보도들을 보면, 기술 기업들이 환경에 해롭다는 뉘앙스를 부각하는 ‘도그 휘슬’ 전략처럼 느껴짐
            물 사용량, 전기 사용량을 문제 삼지만 지나치게 자극적으로 부각하는 경향임
            Dalles 데이터센터의 물 소비를 비판하는 사례가 대표적임
            이 건물들은 콜럼비아 강 옆에 있고, 근처에는 평균 700메가와트 발전하는 Dalles 댐이 있음
            강물을 쿨링에 쓰고, 조금 온도를 올려서 강으로 돌려보내는 방식임
            물이 내리막길로 흘러가며 가져올 열을 되돌려주는 셈이므로, 완전히 낭비되는 게 아님
            관련 기사, Dalles 댐 정보
          + 구글의 공식 기술 보고서 PDF는 여기에서 확인 가능함
          + 왜 평균값이 아니라 중간값 프롬프트 에너지 소비를 발표했는지 의문임
            평균값이 실제 평균 소비를 더 직관적으로 보여줄 것임
          + EV 관련해서는, 2030년까지 전기차가 대세가 될 것이라는 기대는 현실적이 아니었음
            배터리 생산 확장부터 인프라까지 전혀 준비되지 않았음
          + 데이터센터는 특정 소규모 지역에 갑자기 대량의 전력이 필요한 경우가 발생함
            전기차, 히트펌프 등 다른 항목들은 점진적으로 도입되므로 연간 소폭 용량 증가로 대응할 수 있음
            데이터센터는 분산이 어려우므로 인프라 부담이 더 큼
     * 최근 Gen Z 세대가 AI가 물을 ‘파괴하고 있다’고 이야기하는 걸 듣고 놀랐음
       데이터센터 작업을 해봐서 쿨링에 물을 쓴다는 건 알고 있었지만, 대규모로 물을 파괴했다는 느낌은 들어본 적 없음
       생각보다 GenAI와 물에 대한 인식이 깊음
       “AI 때문에 앞으로 가뭄 지옥에서 살게 된다”라는 식으로 이야기하는 경우도 있었음
       난 TikTok 1개 동영상의 에너지 소비가 어느 정도인지가 오히려 궁금하지만, 그런 비교는 논점이 다른 듯함
       실제로 물이 사라진다는 게 어떤 경로를 통해서인지, 수증기인지 묻는데, 많은 사람들이 그냥 증발하면 ‘영영 사라졌다’고 받아들이는 게 의아함
          + 물 분자가 ‘파괴’되는 건 아니지만, 유용하게 사용할 수 있던 장소에서 완전히 사라지는 경우가 많음
            아랄 해는 농업용 관개로 인해 사라진 대표 사례임
            유튜브 영상
          + 물 부족 문제는 극히 지역적인 현상임
            예를 들어 애리조나에서 데이터센터를 운영하면 진짜 물 문제를 겪을 수 있지만, 폐수 재활용 등으로 완화 가능함
            Palo Verde 발전소도 히트익스체인저에 폐수를 사용함
          + 나 역시 최근 AI의 물 소비에 대해 질문받아서 놀랐음
            간단히 검색해 봤더니 데이터센터가 생각보다 꽤 많은 물을 소비함. 전기 1kWh 당 1리터 정도임
            하이퍼스케일러들이 이 수치보다 더 나은 성과를 내고 넷포지티브 달성을 노린다는 이야기는 들었지만, 진짜로 이 값 자체가 허구라는 자료는 거의 없음
            “1리터/kWh”는 체감이 어렵지만, 대형 데이터센터라면 278L/s임. 샤워기 수량 0.16L/s, 캘리포니아 아몬드 산업 전체는 연평균 20만L/s임
            4평방마일 아몬드 농장에 해당하는 수준이지만, 실제로 그렇게까지 극단적이라고는 느껴지지 않고, 하이퍼스케일러 데이터는 더 나을 것 같음
          + 데이터센터는 증발식 쿨링을 사용함
            단순히 물을 데워 돌려보내는 게 아니라, 대기를 통해 완전히 증발시킴
            (물분자 자체는 남지만, 대기 중 수증기는 재활용이 힘든 형태임)
          + AI가 물을 파괴한다는 주장은 NIMBY(지역이기주의) 논점에서 출발했고, 이후 반자본주의 성향이 강한 Gen Z 내 그룹에 의해 AI 반대 논리로 흡수됨
     * 2011년 구글이 검색 쿼리당 0.3Wh를 쓴다고 발표했고, 올해 초 Sam Altman도 OpenAI 쿼리가 평균 0.3Wh라고 말함
       두 수치가 비슷한 게 놀라움
       LLM 쿼리가 단순 구글 검색보다 에너지 집약적일 거라 생각했는데, 구글 검색 자체도 엄청난 인프라임을 실감함
       예를 들어 단어 의미 같은 간단한 질문만 할 경우, 아이폰에서 소형 LLM이 동작한다면 0.03Wh면 충분해서 구글 검색보다 10배 적은 수준임
       (A16 칩이 20초간 5와트로 동작 가정 시 0.03Wh)
       여기서 궁금한 점은 트레이닝 비용(특히 실패한 학습 런)도 이런 추정치에 포함되는지임
       구글 공식 블로그 포스트
          + 14년간의 에너지 효율성 발전 역시 영향이 크다는 점 참고함
          + 2008년 경에는 검색의 핵심 과정이 모든 문서를 grep하는 방식이었음
            분산 방식으로 문서를 램에 올려두고 1000대에 걸쳐 grep을 돌렸음
            인버티드 인덱스는 ""단어 순서가 중요한 쿼리""엔 부적합해서 잘 안 썼음
            랭킹 프로세스 등은 더 복잡함
          + 0.3Wh는 1080줄임. 휘발유 1리터엔 3천만 줄 이상이 들어있으니, 실제론 0.034밀리리터의 휘발유에 해당
            다만 전기는 내연기관보다 훨씬 효율적임
          + 내 로컬 LLM에 쿼리를 입력할 때면 사무실 불빛이 깜빡이고 오히려 오븐 1초 돌릴 때보다 더 많은 에너지가 들어가는 느낌임
          + 2008년 경, 구글이 벌써 딥러닝 기반 검색을 썼는지 의문임
            기능 도입 시 단위 쿼리당 전력 소비가 변동이 있었을 것 같음
     * 전체 보고서에서 어떤 프롬프트가 '중간값'인지를 설명하지 않아 아쉬움
       프롬프트의 토큰 수, 길이 분포가 어떻게 되는지, 연도별로 같을 수 있는지도 궁금함
       이런 정보 없이 단순히 중간값만 명시하는 게 실질적으로 의미가 떨어짐
       평균값이라면 쿼리 수 곱해서 전체 사용량 추정이라도 가능함
     * Sam Altman이 최근 블로그에서 ChatGPT 평균 쿼리당 전력 소모량도 공개함
       ChatGPT 쿼리 1개당 평균 0.34Wh, 오븐 1초 반, 고효율 전구 수분 사용량에 해당
       물 소비는 쿼리당 0.000085갤런(15분의 1티스푼)임
       Altman 블로그
     * 개인적으로 진짜 중요한 건 인퍼런스가 아니라, 트레이닝, 파인튜닝, 데이터 긁어오기라 생각함
       ""프롬프트가 환경을 파괴한다""는 논리는 너무 선정적이라는 인상이 들었음
       점점 팩트 체크가 잘 되고 있다는 점은 반가움
       하지만 현실적으로 새 데이터센터가 송배전망에 미치는 영향도 무시 못할 수준임
       기술이 근본적으로 에너지 효율적이라면 지금처럼 대기업이 민간 원전 투자나 에너지 경쟁까지 하게 되진 않았을 것임
          + 목표 기준을 제대로 잡으려면, 단순히 전체 물·에너지 소모량이 아니라 해당 지역의 물과 에너지 자원이 충분한지 상대적으로 따져봐야 함
            구글은 데이터센터 전체 물 사용량을 공식 보고함
            2024년 기준 약 100억 갤런(미국 전체 사용의 0.03% 수준, 모든 데이터센터가 미국에 있지 않음을 감안)
            수치 자체는 엄청나 보이지 않지만, Council Bluffs, IA에서만 10억 갤런 이상이며, 이런 지역의 에코시스템이 감당 가능한지, 책임 있게 관리되고 있는지 확인이 필요함
            구글은 ""물 고갈 또는 희소 위험이 중간이상인 지역""에서 28% 쓴다고 인정함
            구글 2025 환경보고서
          + 웹사이트 접속만 해도 의도치 않게 프롬프트가 서버로 전송됨
            대규모로 서비스가 저렴하게 공급될수록 더욱 과하게 사용되는 경향이 아쉬움
          + 에너지 소비가 근본적으로 낮았다면, 이렇게 신규 데이터센터와 에너지 인프라 경쟁이 치열하지 않을 거라는 주장에 대해선 동의하지 않음
            미국 전력망은 오랫동안 추가 여유 용량이 없었음
            에너지 효율 개선과 산업침체 때문임
            데이터센터 자체보다는 전력 분배 인프라가 더 큰 문제임
            발전은 할 수 있으나, 필요한 위치로 분배하는 데에 문제가 많음
            민간 발전소 논란도 발전보다는 분배 이슈 때문임
     * 중간값 프롬프트가 0.24Wh라고 할 때,
          + 토스터 1초 작동
          + 스마트폰 80분의 1 충전
          + 100파운드 6피트 들어올리기
          + 9mm 총알 탄두의 운동 에너지
          + 테슬라로 6피트 주행
            과 같은 에너지임
          + 2022년에 마법같아 보였던 기술치고는 전력효율이 굉장히 높게 느껴짐
     * 내가 계산이 맞다면, 1kWh로 약 4천 번 쿼리가 가능함
       산업용 전기 단가로 $0.04/kWh 잡으면, $1에 10만 번 쿼리 가능함
       데이터센터 구축비용 등 고려하면, 월 $20 구독제도 과하게 비싸진 않은 느낌임
       계산이 맞게 한 걸까
          + 맞게 계산함
            AI 에너지·물 관련 비관론의 근본적 오류는, 전기·물·부지가 모두 비용으로 연결되는데, AI가 공짜로 서비스 제공되고 있다는 점임
            만약 진짜로 AI가 전체 전기와 물을 다 잡아먹는다면, 기업들이 손해만 보고 운영할 리 없음
          + 그런데 트레이닝 비용은 따로 생각해야 하는 것 아님?
     * 3자 기사 건너뛰고 싶다면,
       구글 블로그 포스트와
       공식 논문 PDF 참고 가능함
     * Gemini 하루 총 쿼리 수 등 전체량이 더 궁금함
       중간값 프롬프트 수치만으론 총 에너지 요구량 추정이 불가함
       전체 쿼리 수치가 없으면 중간값 자체가 큰 의미가 없음
          + 평균값이 더 직관적이지 않나 싶음
            아마도 평균이 더 높아서 일부러 공개하지 않는 것일 수도 있음
"
"https://news.hada.io/topic?id=22681","Show GN: 오픈소스 프로젝트 '프리즘 인사이트' - AI 기반 주식 분석 & 매매 시뮬레이션 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Show GN: 오픈소스 프로젝트 '프리즘 인사이트' - AI 기반 주식 분석 & 매매 시뮬레이션 시스템

   【 PRISM-INSIGHT: AI 기반 주식 분석 & 매매 시뮬레이션 시스템 】

   3월부터 텔레그램 채널로 실제 무료로 운영 중인 서비스입니다.
   현재 100명 이상의 구독자분들이 매일 AI 분석 리포트 및 매매 시뮬레이션 리포트를 받아보고 계십니다.

  1. 무엇을 하는 시스템인가요?

     * 매일 아침과 오후, 급등주를 자동으로 포착하고 GPT-4.1, GPT-5 멀티 에이전트가 종목을 심층 분석하고 매매 시뮬레이션을 합니다.
       증권사 애널리스트가 작성하는 수준의 리포트를 AI가 자동으로 생성하여 텔레그램으로 전송합니다.

  2. 핵심 특징

     * GPT-4.1 기반의 4개 전문 AI 에이전트의 종합 분석
     * 기술적 분석 에이전트 (차트, 거래량, 투자자별 동향)
     * 기본적 분석 에이전트 (재무, 밸류에이션, 사업 분석)
     * 뉴스 분석 에이전트 (당일 이슈, 시장 동향)
     * 투자 전략 에이전트 (종합 의견, 투자자 유형별 전략)
     * 완전 자동화 시스템
     * 매일 정해진 시간 자동 실행
     * 급등주 포착 → AI 분석 → PDF 리포트 생성 → 텔레그램 전송
     * 사람의 개입 없이 24/7 운영
     * GPT-5 기반 매매 시뮬레이션
     * AI가 생성한 리포트를 바탕으로 실제 매매 시뮬레이션
     * 5개월 누적 수익률 251.39% 달성

  3. 실제 운영 성과

     * 운영 기간: 2025년 3월 ~ 현재 (5개월)
     * 텔레그램 구독자: 100명+
     * 분석 리포트: 매일 2회 (오전/오후)
     * 시뮬레이션 성과: 35건 거래, 누적 251% 수익률

  4. 기술 스택 & 아키텍처

     * OpenAI GPT-4.1 (종합 분석), GPT-5 (매매 시뮬레이션)
     * Anthropic Claude Sonnet 4 (텔레그램 봇 대화 인터페이스)
     * 4개 MCP 서버 통합
       -> KOSPI/KOSDAQ MCP (한국 주식 데이터)
       -> Firecrawl MCP (웹 크롤링)
       -> Perplexity MCP (웹 검색)
       -> SQLite MCP (데이터 저장)
     * Python 기반 모듈형 설계

  5. 오픈소스 공개 사유

   5개월간 실제 운영하면서 얻은 노하우와 코드를 독점하고 싶지 않았습니다.
   이 모든 것을 공개함으로써
     * 개발자분들께는 AI 서비스 구축의 실제 사례를
     * 투자자분들께는 AI 분석의 가능성을
     * 금융 종사자분들께는 기술 혁신의 방향성을

   제시하고 싶었습니다.

  6. 마치며

   AI와 투자의 결합은 이제 시작입니다.
   제가 만든 작은 시스템이 더 큰 혁신의 마중물이 되기를 바랍니다.

   신규 오픈소스 프로젝트라 부족한 점이 많지만,
   여러분의 피드백과 기여를 통해 함께 발전시켜 나가고 싶습니다.

   더욱 나은 투자 알고리즘, 더 나은 서비스를 위해 사소한 것이라도 좋으니
   많은 분들이 참여해주셨으면 좋겠습니다.

   오호.. GPT만으로 분석한 결과가 알고리즘 트레이딩만큼, 혹은 그 이상의 성능이 나온다는 점이 신기하네요.

   재밌는 가설에서 시작했습니다.
   알고리즘이나 퀀트처럼 정확한 계산으로 하는 것이 아닌, 사람이 투자하는 것처럼 직관적으로 사고 프로세스를 정립해도 성과가 나지 않을까? 했는데, 지금까지는 잘 동작하는것 같습니다!

   너무 재미있는 프로젝트네요! 공유 감사합니다~

   감사합니다. 취미로 시작한거라 주제가 흥미롭습니다

   5개월에 251프로면... 잘 되면 국민연금 운용역 같은 데서 쓰면 좋겠네요.

   아직 로직 보완할 것도 많기도 하지만 장기간 검증을 해봐야합니다 ㅎㅎ 지금까지 상승장을 타서 성과가 좋았던 것 같아요
"
"https://news.hada.io/topic?id=22715","Zig의 새로운 IO 인터페이스를 이해하기 어렵다는 고민","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Zig의 새로운 IO 인터페이스를 이해하기 어렵다는 고민

     * Zig 0.15 버전에서 새로운 IO 인터페이스(std.Io.Reader, std.Io.Writer) 가 도입됨
     * 기존 IO 방식의 복잡성 및 퍼포먼스 이슈 개선 목적, 하지만 실제 사용법 혼란 발생
     * tls.Client와 buffer 사용 관련, 불일치된 파라미터 전달 방식이 혼란을 더함
     * 기본적인 사용 예시 구현 중에도 여러 버퍼 크기, 옵션 필드 지정 등 복잡한 요구사항 존재
     * 공식 문서와 코드 예시, 편의 함수 부족으로 입문자에게 직관적이지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Zig 0.15에서 도입된 새로운 IO 인터페이스와 배경

     * Zig 0.15 버전에서 std.Io.Reader와 std.Io.Writer라는 새로운 IO 타입이 도입됨
     * 이전 IO 인터페이스는 성능 문제와 타입 혼합, 그리고 anytype 남용 등으로 인해 복잡성 유발
     * 새로운 IO 구조에서 인터페이스 간 명확한 타입 구분 및 성능 개선이 주요 목표임

tls.Client와 IO 인터페이스 사용의 실제 문제점

     * 기존 smtp 라이브러리 갱신 과정에서 tls.Client.init 함수 사용법에서 혼란 발생
     * 문서상 init 함수는 Reader와 Writer 포인터, 옵션 세트를 인자로 받도록 명시됨
     * Zig의 net.Stream은 각각 reader() , writer() 메서드로 Stream.Reader/Writer를 반환
          + 하지만 Stream.Reader/Writer와 std.Io.Reader/Writer는 정확히 같은 타입이 아니기에 변환 필요
          + Reader는 interface() 메서드 호출, Writer는 &interface 필드를 사용해야 하므로 일관성 부족

버퍼와 옵션 필드 설정 문제

     * stream.writer, stream.reader는 각각 버퍼를 인자로 받음
          + Buffer가 새로운 IO 인터페이스에서 필수적인 요소로 강조됨
     * tls.Client.init 호출 시 ca_bundle, host, write_buffer, read_buffer 등 네 가지 옵션 필드가 반드시 필요함
          + 옵션 파라미터로 넘기는 값과, 인자로 직접 넘기는 값 분리 규칙이 불명확하게 느껴짐

var tls_client = try std.crypto.tls.Client.init(
  reader.interface(),
  &writer.interface,
  .{
    .ca = .{.bundle = bundle},
    .host = .{ .explicit = ""www.openmymind.net""; } ,
    .read_buffer = &read_buf2,
    .write_buffer = &write_buf2,
  },
)

     * 실제로 buffer 포인터가 제대로 주어지지 않으면 프로그램이 제대로 동작하지 않거나 hang, crash 등 다양한 문제가 발생함

Reader 사용시 직관성 문제

     * tls.Client의 reader 필드 자체는 ""Decrypt된 스트림""임에도, 실제로 std.Io.Reader에는 일반적인 read 메서드가 존재하지 않음
     * 대신 peek, takeByteSigned, readSliceShort 등 덜 직관적인 메서드만 제공됨
     * 그나마 사용에 가까운 API는 stream 메서드를 통해 버퍼로 데이터를 읽어오는 방식임

var buf: [1024]u8 = undefined;
var w: std.Io.Writer = .fixed(&buf);
const n = try tls_client.reader.stream(&w, .limited(buf.len));

전체 코드 예시와 실전 문제

     * 전체 동작하는 최소 단위 예시를 만들려 해도 옵션, 버퍼 크기, 타입 변환 등 신경 쓸 부분이 많음
     * 테스트/문서/예시 부족으로 학습난이도와 진입장벽 높음
     * Zig 언어 내에서의 일관성 혹은 underlying design에 대한 이해가 부족할 경우, 이상하게 느껴지는 포인트 많음
     * 표준 라이브러리 내에서도 해당 방식이 많이 쓰이지 않아 실전 참고자료가 부족함

경험과 결론

     * std.fmt.printInt 등 네이밍 변경, API design 변화 등으로 migration 과정 자체가 쉽지 않음
     * reader.interface(), &writer.interface 방식이나 옵션 전달 방식, 여러 개의 버퍼 필요성 등 반복되는 여러 어려움 경험
     * TLS 등의 네트워크/보안 프로토콜이 익숙하지 않은 입장에서 요구사항 파악이 더욱 어렵게 느껴짐
     * 종합적으로, 기존 대비 명확성과 문서화, 편의성 개선 측면에서 아직 미흡한 부분 다수 존재

        Hacker News 의견

     * Author임을 밝힘. 드디어 제대로 동작하게 만듦. 암호화 writer와 stream writer 모두 flush 과정이 필요했고, 동시에 읽기 쪽에도 문제가 있었음. 스트리밍은 동작하긴 하지만, Writer.Fixed가 sendFile을 구현하지 않아 처음 읽을 때는 항상 0을 반환함. 첫 호출 이후 내부적으로 스트리밍 모드에서 읽기 모드로 전환되면서 갑자기 모든 게 동작하기 시작함(관련 코드 링크: Zig File.zig #L1318). 지금은 websocket 라이브러리에 압축 기능을 다시 켜려고 노력 중임
          + ""Flush를 잊지 마세요""라는 유튜브 밈이 떠오름 (유튜브 영상)
          + 최소 놀람의 원칙(principle of least surprise)은 대체 어디로 갔는지 궁금함
          + 이전 인터페이스에서 지금 이 상황으로 넘어온 것이 참 대단하게 느껴짐. 깜짝 놀람이 큼
     * Zig PM(프로덕트 매니저)은 아니지만, OP가 겪은 문제에 대한 명백한 첫 번째 해결책은 더 나은 문서화와 더 많은 사용 예시를 만드는 일임(너무 많아도 상관없음). 이런 작업을 하며 사용자에게 너무 많은 걸 시키고 있진 않은지 돌아볼 좋은 기회가 될 수 있음. 만약 추구한 목표가 절대적인 성능 또는 성능 저하를 부르는 추상화의 도입을 피하는 것이었다면 그 목표는 달성한 것 같지만, DX(개발자 경험)는 안드로메다로 간 느낌임
          + Zig 커뮤니티의 문화를 잘 모르는 것 같음. 문서 부족을 불평하면 누구나 ""stdlib 코드 직접 읽으라""는 댓글이 쏟아질 준비를 해야 함. 대부분의 API가 이 글에서처럼 사용이 어렵고, HTTP나 파일 시스템 같은 기본 작업조차 익숙지 않으면 정말 힘듦. 그래서 정말 실력 있는 사람만 살아남음
          + 문서 작성에는 비용이 들고 시간도 걸림. 그 시간에 Zig의 다른 부분을 개선할 수도 있음. 작업 중인 코드라면 완전히 자리잡을 때까지 문서를 미루는 것도 합리적인 선택임. 물론 문서화는 좋지만, 새로운 기능, 중요한 버그 수정, 또는 문서 작업 중에서 우선순위를 정해야 하는 경우 항상 모두 가질 수 있는 것은 아님
          + zig는 무엇을 하지 말아야 하는지 지시하는 데 너무 초점을 맞춘 것 같음. 여러가지 방법과 사용 예시를 모아서 잘 정리하고 알려주는 쪽으로 발전했으면 좋겠음. 이 인터페이스의 문서 누락이 대표적인 예임
          + 좋은 문서나 예제를 쓰는 데에는 많은 노력이 필요함. 지금 zig에서 벌어지는 변화 폭을 보면, 제대로 정착되기 전에 문서를 만들어도 금방 소용이 없어짐
          + 나는 Zig 개발자는 아니지만, Zig의 문서가 너무 간단한 이유 중 하나는 언어가 아직 젊고 계속 진화 중이기 때문일 것이라 생각함. 지금 쓴 문서가 미래에 곧 틀릴 걸 알면서도 시간과 에너지를 들이는데 어려움이 있다는 점이 이해됨
     * Zig 언어 그 자체는 정말 괜찮은데, 표준 라이브러리는 아직 많이 미완성이고, 계속 바뀌고, 많이 부족하고, 일부는 지나치게 추상적이거나 또 너무 저수준임. 지금은 표준 라이브러리 대신 OS API를 직접 쓰는 게 낫다고 생각함. 베타 테스터 각오가 아니라면 표준 라이브러리 피하는 걸 추천함
          + 실제로 나도 zig를 쓸 때는 대개 OS API 위주로 사용 중임. cImports가 잘 되어 있어서 zig 정의 만들기 귀찮을 때도 쉽게 쓸 수 있음
          + 내가 볼 때 Zig는 너무 많은 걸 한꺼번에 하려는 경향이 있어서, 내가 생각하는 최소 품질선조차 도달하지 못하고 있음. 사용자에게 급격한 변화와 실험을 감수하도록 강요하는 모습에서, 충분히 많은 사람이 투자해서 ""1.0 전이면 깨져 있어도 괜찮아, 언젠가 좋아질거야""라는 허상을 믿을 수밖에 없는 상황임(결론: 그 날은 절대 오지 않을 것 같음). 다른 사람에게 자신의 실험을 부담시키는 것은 바람직하지 않다고 생각함. 불안정하다고 미리 고지했다 해도, 의존하지 말라고 했다 해도, 이미 rug pull(갑자기 모든 게 바뀜)을 당하는 사람 입장에서 문제임. zig가 무엇인지 잘 모르겠음. Matklad는 machine level language라고 하며(관련 인터뷰: lobste.rs - Matklad 인터뷰), 공식 페이지에서는 robust, optimal, reusable general-purpose 언어라고 함. 이 둘은 서로 모순됨. 그리고 수동
            메모리 관리가 필요 없는 문제도 많아서, zig는 결코 범용 언어가 아님. 결국 이 모든 혼돈이 zig의 불안정성과 비대한 표준 라이브러리에 드러남. 심플함과 범용성을 주장하면서 이 정도의 큰 라이브러리는 모순임. Async 역시 모든 플랫폼에서 보편적으로 효율적으로 구현될 수 없는 기능임에도 만능 해결책인 것처럼 약속함. 예전엔 함수 착색 문제를 해결했다며 홍보하다가 그 시도는 이미 버려졌음. 제대로 해낼 수 있다고 다시 믿으라는 논리가 이상함. 실제로 모든 플랫폼에서 컴파일러를 구현하는 데 필요한 기본 어셈블리 명령만 있으면 되는데, luajit은 아예 파서를 순수 어셈블리로 구현했고, 어디서든 잘 동작함. 나는 대부분 lua로 프로그래밍하고, 인터프리터에서 버그를 거의 만난 적 없음. zig가 luajit보다 잘 해결해줄 문제도 떠오르지 않음. 만약
            zig로만 해결되는 게 있다 해도, 그걸 lua 코드에 embed해서 FFI로 연동하면 됨. 대부분의 코드가 그 정도의 저수준 최적화가 정말 필요하지 않음. zig 도입하면 오히려 골치 아파짐. 요즘 zig에 대한 과장된 기대가 AI 수준의 현실과 괴리에 도달함. zig를 믿으려면 없는 능력을 언젠가 갖게 된다는 허망한 희망을 믿어야 함. 실제로 실행 계획도 없이, 그냥 ""조금만 더 기다려""라는 식임
     * 나는 라이브러리나 인터페이스가 내 타입의 버퍼 할당을 요구하는 게 이해가 안 됨. 내가 파싱하는 거라면 굳이 라이브러리가 필요 없고, 쓸 거라면 교환을 깨뜨릴 수 있는 일이기도 함. go의 독특한 인터페이스는 일부 인터페이스가 writer 인터페이스를 확장하거나(hijacker 인터페이스 참고), request 객체가 여러 미들웨어에서 다양하게 재활용되는 점 때문임. 요약하면 요청(request)은 확장될 필요가 없지만, 응답(response)은 websocket, tcp 래퍼 등 다양한 형태로 변화할 수 있음
          + 라이브러리가 외부 버퍼 할당을 요구하는 것이 이상하게 느껴지지 않음. 유연성을 주는 대신 더 많은 수작업이 필요함. 예를 들어, 이미 만들어둔 버퍼 풀이 있다면 재사용하고 싶을 수 있음. 만약 타입이 내부에서 독자적으로 할당하면 그게 불가능함. 혹은 미리 모든 리소스를 할당해두어야 하는 환경에서는 나중에 할당이 불가함. 단점은 전체 사용자 중 10%만 이런 유연성이 필요하고 90%는 그냥 버퍼 할당 후 넘길 텐데, 모두가 더 복잡한 일을 하게 됨. 가장 좋은 방법은 높은 유연성을 주면서도, 간단한 경우에는 쉽게 처리되는 것이어야 함. 예를 들면, 0길이 버퍼(또는 Zig의 null)를 넘기면 타입이 직접 할당하도록 하고, 추가로 버퍼 없이 간단하게 생성하는 생성자(constructor)도 제공하면 되겠음. 물론 이런 건 전적으로 문서화가 고생임은 분명함
          + 이건 각 언어마다 선택하는 관례의 차이와 비슷함(라디안/도 등). 어떤 IO도 자유롭게 변환 가능함. 한쪽에선 mock이라 부르고, 어떤 언어에선 unsafeFoo라 할 수도 있음. Andrew Kelley가 live stream에서 Haskell 커뮤니티가 30년간 논의한 패턴을 독자적으로 재발견함. 그래서 미래는 Zig임. 그가 먼저 깨달음
          + 외부 버퍼의 의미는 함수가 버퍼 할당을 생략하는 것임
     * zig 사이드 프로젝트를 0.15.x로 올릴 생각 없음. Andrew가 왜 릴리스를 선택했는지, 새 Io를 얼리어답터 손에 쥐어주는 걸 존중함. 하지만 readers/writers 대대적인 변경 직후라서 며칠밖에 지나지 않았음. 표준 라이브러리 작업자에게는 좋은 일이지만, 나처럼 취미삼아 zig 쓰는 사람은 0.16.0에서 안정화 이후까지 기다리는 게 현명하다고 느낌
          + 언어 이름이 Zig이라면, 가끔은 Zag도 해야 하지 않을까라는 농담이 떠오름
          + Zig 코어 멤버인 Loris Cro도 최근 인터뷰에서 IO 변경 여진이 가라앉을 때까지 자신의 프로젝트에 zig 업데이트를 미룬다고 언급함. 하지만 이후 전망은 긍정적임. Andrew와 Loris 모두 이것이 마지막 주요 변화가 될 거라 보고 있어, 1.0이 머지않아 나오지 않을까라는 기대를 함. (재)도입되는 stack-less coroutine의 영향만이 현재 가장 큰 변수임
     * 새 IO 인터페이스에 대한 글을 본 뒤 zig를 피하는 쪽을 택했음. 다행히 내 본능이 맞았다는 생각임. 사유는 다르더라도 결과적으로 C++11 이전의 장황함과 비슷한 복잡성이 느껴짐. 새 언어가 대체 역할을 하려다 결국 기존 언어만큼 복잡해지는 패턴이 익숙하게 반복되고 있음
          + 내 게시글도 그 중 하나임을 밝힘. 이런 글을 보고 zig에 도전하는 걸 겁낼 필요는 없다고 생각함. zig 팀은 더 나은 해결책이 보이면 과감히 바꿀 태세임. zig를 미래 먹거리로 생각한다면 이런 변화가 맞지 않을 수 있지만, 개인 또는 소규모 팀에는 이미 목적이 분명하고 툴링이 좋은 멋진 언어임
     * Stream.Reader를 std.Io.Reader로 변환하려면 interface() 메소드를 호출, Stream.Writer에서 std.io.Writer를 얻으려면 &interface 필드의 주소가 필요하다는 점이 일관적이지 않다는 OP의 지적은, Go 커뮤니티라면 아예 거부했을 것이라 생각함. Go는 사소한 변화라도 극도로 깊이 있는 분석 끝에 결정하는 편임. 내 최애 Go 이슈 사례: Go github issue #45624. 4년간 논의 후 결론을 내리는 식임. 느릴 수 있지만 일관성, 설계적 고민, 실제 코드 사용까지 꼼꼼히 챙김. 느리지만 그만큼 필요한 속도라고 생각함. 그렇게 나온 결정이 결과적으로 매우 품질이 좋음
          + Rust 역시 마찬가지임. nightly rust에만 있고 stable에는 없는 유용한 기능들(예: generator)이 많음. 답답하지만, stable에 들어가는 기능은 매우 깊이 검증됨. 나는 조급하지만, Rust 팀의 접근법이 바람직하다고 생각함
          + Go 1.0 전에는 그 정도로 느리지 않았음. 더 근본적이지는 않더라도 큰 변화가 자주 있었고(세미콜론 제거, 에러 타입 변경 등), 자동 변환 툴도 지원했음. 1.0부터는 안정성을 약속하며 지금의 방식이 된 것임
     * Zig는 로우레벨 작업에 쓸 언어로 제일 먼저 떠오름. Zig를 C/C++ 크로스 컴파일러로도 쓸 수 있다는 점이 매우 멋짐
     * 딱히 이슈 대부분이 문서 부족 또는 부실함에서 나온 문제로 보임
          + Zig에서 너무 많은 부분이 자주 바뀌다보니, 문서화가 우선순위가 아닌 것 같음. Zig 튜토리얼도 거의 ""예시 코드 모음"" 느낌이고(최신 컴파일러에서 안 돌아가는 예시도 많음), 많은 표준 라이브러리 정의도 직접 소스 코드를 읽어야 함. Zig 문법의 트릭을 다 알면, 단순한 함수들은 짧고 논리적이며, 이름도 명확해서 작성자는 쉬움. Allocators 개념도 개념적으로는 어렵지 않으나, 직접 만들고 싶지는 않음. 하지만 복잡한 개념들에서는 한계가 명확히 드러남. Zig의 새로운 IO 시스템은 마치 자바의 Streams/Readers/Writers 구조처럼 여러 레이어로 감싸져 있음. 쉬운 출력을 위해 단순하게 output.write(""hello"")처럼 쓰게 해주려 하지만, 실제로는 사용법 설명이 부족해서 혼란을 주고 있음. 이런 복잡한 타입 시스템을 굳이 표준 라이브러리에서 표현해야 하는지도 의문임.
            Zig 전체가 명확하고 간결하고 읽기 쉬운 메소드로 이루어져 있는데, 새 IO 시스템은 그와 거리가 멀고 비직관적임
     * (zig의 새 시스템은) 그냥 실행 경계를 나누는 용도였던 개념을 전체 런타임 엔진에 섞으면서, 그 양쪽을 어떻게 연결하는지는 명확히 제시하지 않아서 문제임
"
"https://news.hada.io/topic?id=22623","D2(텍스트에서 다이어그램 도구)에서 이제 ASCII 렌더링 지원","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  D2(텍스트에서 다이어그램 도구)에서 이제 ASCII 렌더링 지원

     * D2 0.7.1 버전에서 텍스트 다이어그램을 ASCII로 렌더링하는 기능 추가
     * 코드 주석에서 간단한 ASCII 다이어그램을 활용해 함수나 클래스의 흐름 설명 가능
     * 기본적으로 유니코드 문자를 사용하지만, 옵션 플래그로 표준 ASCII 적용 선택 가능
     * 이 기능은 알파 단계로 일부 스타일, 특수 문자, 특정 도형은 지원하지 않음
     * D2 Playground와 Vim 확장 등에서 직접 ASCII 렌더링 체험 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

D2 ASCII 렌더링 소개

   D2의 최신 릴리즈(버전 0.7.1)부터 텍스트 다이어그램의 ASCII 출력 기능이 추가됨.
   .txt 확장자를 가진 출력 파일은 자동으로 ASCII 렌더러로 렌더링됨.
   예시는 D2 Vim 확장에서 볼 수 있으며, .d2 파일을 열고 프리뷰 창을 통해 저장할 때마다 ASCII 다이어그램 출력 상태를 실시간으로 확인 가능함.

코드 문서화에서의 활용

   ASCII 다이어그램은 소스코드 주석에 삽입할 때 가장 실용적임
     * 작은 함수 또는 클래스 옆에 간단한 다이어그램을 추가하면, 흐름 설명보다 더 직관적으로 코드 이해를 돕는 장점 존재
     * Vim 확장에서는 D2 코드를 작성해 선택 영역을 ASCII 다이어그램으로 곧바로 변환 가능함

유니코드와 표준 ASCII의 선택

   디폴트 ASCII 렌더링은 유니코드 상자의 그리기 문자를 사용해 더 보기 좋은 출력 제공
     * 만약 최대 이식성을 원한다면, --ascii-mode=standard 옵션 플래그를 통해 일반 ASCII 문자로도 렌더 가능함

현재 한계점

   이 ASCII 렌더링 기능은 알파 단계임
     * 코너 케이스, 개선 포인트, 버그가 다수 존재할 수 있음
     * 웹사이트에 버그 리포트 또는 피드백 제출을 장려함

   주요 제한 사항
     * 스타일 미지원
          + 애니메이션, 폰트 등은 ASCII에서 의미 없음
          + 향후 터미널 색상 등 일부 제한적 스타일 지원 검토 계획
          + 테마도 지원 대상 아님
          + double-border, multiple 등은 향후 개선 예정 리스트에 포함
     * 불균일한 간격
          + ASCII로 변환 과정에서 박스 배치가 SVG 대비 불규칙해질 수 있음
     * 렌더 불가능 대상
          + 마크다운, Latex, 코드 등의 특수텍스트 지원 미구현
          + 이미지와 아이콘, UML 클래스, SQL 테이블 등 렌더 불가
          + 이러한 항목은 제거 혹은 플레이스홀더 표시 여부를 추후 결정 예정
     * 일부 도형 미지원
          + 클라우드, 원 등 곡선 형태 도형은 ASCII에선 잘 표현되지 않음
          + 해당 도형은 사각형으로 대체 후 좌측 상단에 작은 아이콘 추가로 유형 구분

직접 사용해보기

   이 기능은 D2 Playground에서 바로 사용 가능함
     * 제공된 코드블록을 열어 직접 ASCII 렌더링을 체험할 수 있음

        Hacker News 의견

     * 안녕하세요, 개발자 여러분. 오늘 아침에 추가한 새로운 기능을 공유하고 싶음. 아직 알파 단계이고 벌써 github issue가 하나 올라왔음. 블로그 글을 건너뛰고 바로 확인해보고 싶으면 여기에서 확인 가능함. 좀 더 큰 예시도 이 링크에서 확인 가능함
          + playground에서 검은 배경에 검은 글씨로 렌더링되어서 작동 안 하는 줄 알았음. 아마 내가 다크모드를 활성화했기 때문인 것 같음. 다른 렌더러는 잘 되는데 ascii는 안 보임
          + TFA에서는 단순히 ELK에서 다운스케일된다는 뜻인가 궁금함. 이 기능을 사용하려면 ELK를 명시적으로 설정해야 하는지, 아니면 .txt로 출력하면 자동으로 활성화되는지 궁금함. 주로 내 d2 다이어그램을 위해 이미 ELK를 쓰고 있음
     * mermaid와 비교했을 때 어떤 점에서 가치가 있는지 궁금함. 디자인과 작동 방식이 마음에 들지만 mermaid에서 넘어올 실질적인 이유가 있는지 묻고 싶음. 또한 이런 도구들이 대체로 자동으로 생성된 결과물에서 박스의 위치와 크기를 x/y 값으로 수동 조정할 수 있는 기능이 부족하다고 생각함
          + 어떤 가치를 중시하느냐에 따라 다름. 당신이 언급한 두 가지로 충분한 사람도 있고, 우리처럼 CLI에서 SVG 렌더링할 때 chromium 브라우저가 필요 없는 점이 중요한 사람도 있음(관련 사례 참고). 전체 차이를 보여주진 않지만 간략 비교 사이트도 있음. 그리고, 유료 제품 광고는 하고 싶지 않지만 위치와 크기를 고정할 수 있는 manual 조정 기능이 필요한 경우 IDE에서 제공함
     * D2 문법이 매우 매력적임에도 불구하고 그간 Mermaid의 툴링 때문에 더 많이 사용했었음. 그러나 이 기능은 메이저하게 새롭고, Mermaid에서는 본 적이 없던 것임. asciiflow.com과의 연결고리도 잘 만들어줌
     * ASCII에서 변환되는 기능이 좋음. 그런데 다이어그램을 업데이트해야 할 때 원래의 D2 소스를 어떻게 얻는지 궁금함
     * D2가 mermaid보다 항상 더 마음에 들었는데, 내 생각에는 다음 이슈 때문에 grid layout이 사실상 쓸모가 없어짐(관련 이슈). 픽셀 단위를 직접 맞춰야 한다면 이런 도구의 목적이 사라지는 것과 같음
          + 고마움! 알려줘서 좋음. 0.7.2(다음 릴리즈)에 반영 예정임
     * 브라우저 기반 텍스트 투 다이어그램 툴 목록을 관리하고 여러 번 공유한 적 있음. 최근에 알게 된 건 D2 온라인 버전은 순수하게 브라우저에서 돌아가는 게 아니라 백엔드 서버가 다이어그램을 생성한다는 점임. D2가 클라이언트(브라우저)에서만 단독으로 동작할 수 있는지, 그리고 이번 글에서 언급한 확장 기능이 오프라인에서도 작동하는지 궁금함
          + 이제 가능해짐! (일주일 전부터) d2 playground의 네트워크 탭을 한번 확인해보면 d2.js라는, D2의 wasm 포트를 감싼 래퍼로 구동되는 걸 볼 수 있음. 아직 공식 출시 전이지만 곧 분리 공지 예정임
          + ~~목록을 볼 수 있을까 했는데~~ 편집: 이미 찾았음! 고마움
     * 공식 파이썬 repository를 마련해주길 바람. notebook에서 쓰고 싶은데, 현재는 databricks만 강제적으로 써야 해서 불편함
     * 보기 정말 멋짐. 예전에 올렸던 C4 지원 글도 봤음! 곧 C4 모델링이 필요해서 확인해볼 예정임
     * 정말 멋진 기능임. D2는 처음 들어봤지만, 터미널에서도 바로 차트를 만들고, 예쁘게 렌더링할 수도 있다는 점이 정말 마음에 듦. 금방 써볼 계획임
          + D2는 이미 실시간 HTTP 서버로 .svg를 live로 업데이트하는 기능도 지원함. 브라우저와 vim을 나란히 띄우면 실시간 업데이트도 볼 수 있음. 1년 전에 D2를 발견했고, 이후 모든 다이어그램에 사용 중임
          + 일반적으로 ascii 다이어그램은 Moondraw를 쓰지만, 이미 vim 환경에 있다면 이 도구가 훨씬 좋음
     * vim extension이 정말 쩌는 기능임
          + 근데 vimscript로 짜여있다는 점이 이상하고 좀 아쉬움
"
"https://news.hada.io/topic?id=22634","디자인 툴의 다음 단계는 무엇인가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           디자인 툴의 다음 단계는 무엇인가

     * Figma가 IPO 상장에 성공하며 디자인 도구 산업 전반의 미래에 대한 논의가 촉발되었고, AI 시대에도 디자인의 중요성은 오히려 강화되고 있음
     * 디자인의 상품화(commoditization) 는 진입 장벽을 낮추고 대중화를 이끌며, 결과적으로 더 나은 디자인에 대한 기대치를 끌어올림
     * 새로운 워크플로 변화로는 레이아웃·변형 생성, 코드와 디자인의 융합, 브랜드·비주얼 언어 구축이 핵심으로 떠오름
     * 플랫폼 차원에서는 오픈 표준과 프로토콜, 그리고 BYOT(Bring Your Own Tools) 문화가 디자인 생태계의 확장성을 좌우할 전망임
     * 궁극적으로 디자인 툴의 미래는 단일 캔버스를 누가 지배하느냐가 아니라 선택·개방성·상호운용성을 누가 잘 구현하느냐에 달려 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Figma IPO와 디자인의 상품화

     * 2025년 7월 31일 Figma가 $FIG 티커로 상장하며 긴 여정의 결과를 보여줌
     * Adobe 인수 가능성에서 IPO로 이어진 길은 다른 테크 기업에도 공개 시장 진출 신호로 작용
     * AI 시대에도 디자인이 죽지 않고, 오히려 인간 창의성의 핵심 영역으로 중요성이 커졌음을 증명
     * 디자인의 상품화는 서비스 비용 하락과 대중화를 통해 누구나 접근 가능하게 만들며, 이는 Star Trek의 Replicator에 비유됨
     * 결과적으로 사람들은 더 많은 디자인을 쉽게 만들 수 있으나, 브랜드와 취향이 차별화 요소로 남게 됨

새로운 워크플로와 기능

     * 레이아웃 및 변형 생성: Bootstrap이 양산형 웹 경험의 상징이 된 것처럼, Variant AI·MagicPath·Subframe 등은 다양한 변형을 신속히 생성하는 흐름을 이끌고 있음
     * 코드 융합: 코드 익스포트는 오래된 기능이지만, 시각적 에디터와 코드의 깊은 통합은 여전히 미완의 영역
          + Storybook은 여전히 핵심적 의존성
          + Vercel v0는 시각적 편집 기능을 추가했고, Figma도 코드 레이어를 발표
     * 브랜드와 비주얼 언어: AI 결과물 특유의 ‘AI 슬롭’ 문제에도 불구하고, 차별화된 미학을 구현하는 사례 등장
          + Perplexity의 Phi Hoang 작품, Visual Electric·Flora 같은 도구가 창의적 실험을 장려
          + 작성자는 Tapestry의 비주얼 자산을 Visual Electric으로 제작하며 독창적 결과물을 도출

플랫폼과 생태계 변화

     * 오픈 표준과 프로토콜: HTML·CSS처럼 개방형 표준이 생태계를 성장시킨 전례가 있음
          + MCP(에이전트 조정)와 WCAG(접근성)는 기본 인프라로 자리잡아가고 있음
          + API와 플러그인만으로는 부족하며, 툴·AI 모델·워크플로 간 공통 언어가 필요
     * BYOT(Bring Your Own Tools): 단일 툴 강제 시대는 끝나고, 디자이너마다 맞는 툴을 선택해도 공동 협업이 가능한 환경이 요구됨
          + 개발자들이 Vim과 VS Code를 병행해도 같은 저장소에서 협업하는 것처럼, 툴 다양성을 존중하는 인프라 필요
          + 이를 위해서는 오픈 포맷, 컴포넌트 시스템, AI 기반 정규화 계층이 필수

결론 및 전망

     * 디자인 툴의 미래를 결정짓는 것은 누가 캔버스를 지배하느냐가 아니라 누가 더 많은 선택과 유연성을 제공하느냐임
     * 차세대 디자인 플랫폼의 특징
          + 코드·미디어·비즈니스 로직을 매끄럽게 통합
          + 앱의 모듈화/분리와 함께 유연한 생태계에 적응
          + MCP·WCAG 같은 오픈 표준이 근간을 형성
          + BYOT 문화를 수용해 팀별로 다양한 툴 사용을 보장
     * 디자인 툴은 더 이상 디자이너만의 것이 아니며, 개방성·상호운용성·창의적 유연성을 embrace하는 플랫폼이 미래를 주도할 것임
"
"https://news.hada.io/topic?id=22622","테드 창: 비밀스러운 세 번째 요소","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          테드 창: 비밀스러운 세 번째 요소

     * 테드 창은 단순한 하드SF나 소프트SF의 구분을 넘어, 과학 자체의 원리가 다른 세계를 정교하게 구성하는 진정한 과학소설을 써온 작가임
     * 그의 작품은 기술을 두려움의 대상으로 그리지 않고, 인간을 더 깊이 이해하게 하는 긍정적 도구로 제시하는 점이 독특함
     * 반복적으로 다루는 주제는 양립론적 자유의지이며, 이를 단순한 철학적 논증이 아니라 인물들이 실제로 살아가는 경험으로 풀어냄
     * 이야기의 결말은 놀라움보다는 재맥락화를 통해 전체 서사를 새롭게 보이게 하며, 이는 문학적 양립론의 형식으로 기능함
     * 사회 전체의 기술 수용 묘사에서는 약점을 보이지만, 테드 창은 여전히 현존하는 최고의 단편 SF 작가로 평가받음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

작가 소개 및 독창성

     * 테드 창은 현재 생존하는 최고의 과학 소설 단편 작가로 평가받음
     * 그의 작품은 과학적 추론과 인간 심리, 철학적 사유를 결합함
     * 소설마다 두 번 이상 읽을 만큼 재방문 가치가 있으며, 리뷰어들조차 그의 핵심을 종종 놓침

진정한 과학소설

     * Chiang은 기존의 하드SF(공학 기반) 나 소프트SF(과학 판타지) 로 분류되지 않는 새로운 영역을 탐구함
          + 《Omphalos》에서는 창조론적 우주가 사실로 존재하며, 별빛은 6천 광년까지만 도달하고 화석과 인간 신체에 창조의 증거가 드러남
          + 《Seventy-Two Letters》에서는 유대 신비주의와 카발라가 산업 기술의 기반이 되는 대체적 과학 세계를 구성함
          + 《Story of Your Life》에서는 폐기된 가설인 사피어-워프 이론이 외계와의 소통 핵심으로 등장하며, 시간 인식 방식이 전혀 다른 존재를 이해하는 수단이 됨
     * 이 외에도 《Division by Zero》에서는 수학 자체가 내부에서 붕괴되고, 《Hell Is the Absence of God》에서는 신의 개입이 실증적 법칙으로 작동하는 세계가 설정됨
     * 일부 독자는 이를 비현실적이라고 평가하지만, Chiang의 목표는 대안적 과학 법칙을 통해 철학적 탐구와 인간 관계를 심화시키는 것임

기술의 긍정성

     * 현대 SF는 주로 디스토피아적 기술 불신을 묘사하지만, Chiang은 기술을 인간 이해의 매개체로 다룸
     * 《The Truth of Fact, The Truth of Feeling》에서는 기억 증강 기술이 주인공이 자기 기만을 직시하고 화해로 나아가게 하는 계기가 됨
     * 《Liking What You See: A Documentary》에서는 획득적 안면실인증을 통해 인간의 미와 사회적 얕음을 비판적으로 조명함
     * 심지어 비극적 상황에서도 기술은 파국의 원인이 아니라, 인간과 세계에 대한 진실을 마주하게 하는 통로로 등장함

양립론적 자유의지의 체험

     * 양립론(Compatibilism)은 결정론과 자유의지의 양립을 주장하는 철학적 입장으로, Chiang은 이를 단순한 논리적 설명이 아니라 인물의 삶의 경험으로 전환함
     * 《The Merchant and the Alchemist's Gate》에서는 “과거는 바꿀 수 없지만 배움이 모든 것을 바꾼다”는 깨달음을 통해 수용과 용서를 제시함
     * 《Story of Your Life》에서는 주인공이 외계 언어를 배우며 시간의 비동시적 체험을 받아들이고, 비극적 미래조차 아름다움과 함께 수용함
     * 이 과정에서 독자는 철학적 개념을 추상적 사고가 아니라 정서적·실존적 체험으로 느끼게 됨

결말이 모든 것을 재해석

     * Chiang의 작품은 2회 이상 읽을 때 새로운 의미가 드러나는 구조를 지님 (이야기의 결말이 전체 맥락을 바꿔 놓음)
     * 결말은 전통적 반전이 아니라 예정된 서사를 새롭게 비추는 장치로 작동함
     * 이는 문학적 양립론으로, 결말이 정해져 있음에도 그것을 알게 되는 과정이 경험의 가치를 변화시키는 방식임
     * 다양한 작품에서 이를 반복적으로 달성하면서도 단순한 트릭이 아닌 불가피한 필연성을 독자에게 설득시킴

종합적 강점

     * 단순하면서도 아름다운 문체, 다양한 배경, 과학과 철학, 심리학에 대한 깊은 이해, 그리고 다채로운 인물을 결합함
     * 이러한 독창적 요소와 보편적 문학적 장점을 함께 구사함으로써 현존 최고의 단편 SF 작가로 평가됨

약점과 한계

     * 사회 차원의 기술 수용 묘사에는 빈틈이 존재함
          + 《Anxiety is the Dizziness of Freedom》에서 평행우주 간 데이터 교환 기술은 거대한 사회·경제적 영향을 미칠 수 있음에도, 작품은 개인적 차원에만 집중함
     * 양립론 외의 철학 주제에서는 깊이와 감정적 울림이 상대적으로 부족함
     * 그의 느린 창작 속도와 완벽주의적 경향도 산출물의 한계로 지적됨

결론

     * Ted Chiang은 기술과 과학, 철학을 통해 인간의 본질을 탐구하는 독보적 단편 작가임
     * 올해 하나의 SF 책만 읽는다면 《Stories of Your Life》, 두 권이면 《Exhalation》 까지 추천함
     * 다섯 권을 읽을 예정이라면 두 책을 각각 반복해서 읽는 것을 권장함
     * 그의 작품은 철학적 사유와 인간적 울림을 함께 경험할 수 있는 드문 기회

   아니, 창 씨가 아닌데 왜 테드 창인 거야? 너 영어 이름 어떻게 짓는 지 모르지?

   창식이…ㅋㅋㅋㅋㅋㅋ 그거 영화볼때 반가웠어요. 원래 좋아하던 작가여서

   테드 창은 대만계 미국인으로 성이 'Chiang'씨 맞지 않나요?

   앗.. 아래 극한직업 내 대사 나온게 있어서, 그 다음 대사를 써보았던 겁니다 ㅎㅎ

   ㅋㅋㅋㅋㅋㅋㅋㅋ

   아하... 눈치가 없었네요 죄송합니다.

   창식이 안녕?

   테드창 소설 진짜 재밌더라고요

        Hacker News 의견

     * Ted Chiang은 공감과 열린 마음으로 반사실을 탐구하는 작가임을 높이 평가함, 자신을 서사에서 빼는 점이 Virginia Wolfe와 닮았다는 생각을 함, 원글이 이 작품들에 숨겨진 날카로운 비판을 간과했다고 지적함, 예를 들어 Omphalos, Hell Is the Absence of God, Tower of Babylon은 종교에 대한 무서운 비판으로 읽을 수 있다고 봄, 만약 특정 종교 믿음이 사실이라면 세계가 어떻게 보이는지 명확하게 그려냄, 실제 우리 세계와 다르므로 믿음이 사실이 아님을 시사함, 각 이야기에는 암묵적으로 우리 종교가 자연을 정확히 설명하지 않아서 다행이라는 주장을 담은 우주적 공포의 요소가 있다고 느낌, Exhalation은 개인적으로 가장 좋아하는 작품으로, 이질적인 세계의 한 과학자가 발견의 과정을 통해 마음의 본질을 탐구하는 이야기임, 그 세계는 우리 세계의 이상화·단순화된
       버전임, 스토리와 세계가 완벽하게 밀폐되어 있지만 이 침묵이 오히려 아름답고 귀중한 결정체 같다는 생각을 갖고 있음
          + Chiang의 진짜 특징은 단순히 기발한 세계관을 만들고 끝내지 않고, 도입한 반사실이 반드시 인간적 결과로 압박받으며 드러나도록 만드는 부분임을 강조함, 그래서 종교 이야기가 잘 작동하는 이유는, 믿음을 외부에서 조롱하는 게 아니라 만약 우리가 진짜로 그런 믿음을 가진 세계에 살고 있다면 어떻게 될지를 상상하게 끔 해서 그 결과를 직면하게 만듦, 공포는 교리를 글자 그대로 받아들이면 위로가 되지 않음을 깨닫게 된 데에서 비롯된다고 느낌, Exhalation도 같은 기법이 물리에 적용된 예라고 봄, 내레이터가 자신의 세계와 자신을 천천히 해부함으로써 우리가 느끼는 연약함을 드러냄, 서사가 엔트로피에 맞서거나 은유로 포장하지 않고 쇠퇴의 사실을 받아들이며 그 의미를 이해한다는 점에 아름다움을 느낌, 절제된 표현이 감정적 무게를
            더한다고 여김
          + 나는 Tower of Babylon을 종교에 대한 ""파괴적 비판""으로 받아들이지 않음, 이야기 속에서 탑을 성공적으로 천국의 기저까지 쌓아올리고 돌파하지만, 결국 다시 지구로 돌아오는 구조임, 이 구조에서 나는 천국과 지구가 동일하다는 암시를 읽음, 이는 종교나 신에 대한 반박이 아니라 오히려 여러 종교와도 일치할 수 있다고 생각함, ""우주적 공포""나 종교가 자연을 부정확하게 설명한다는 의미로 받아들일 필요는 없다고 봄, Chiang의 이야기들이 다양한 해석을 허용하는 점이 그의 즐거움인 것 같음
          + 평평한 지구론이나 YEC(젊은 지구 창조론)을 반박하는 것이 종교에 대한 ""파괴적 비판""은 아니라고 생각함, Hell Is The Absence of God에 대한 작가 자신의 해명도 다름, ""만약 신이 분명히 존재한다면 믿음(신앙)은 더 이상 의미 없을 것""이라는 메시지를 전달함, 해당 위키피디아 해설 참고함
          + 가치 있고 건설적인 의견에 감사를 전함, 본문에서 Omphalos는 젊은 지구 창조론이 경험적으로 사실인 세계를 그리는 점을 강조함, 이야기 속 과학자들이 여러 독립적인 증거를 발견해 창조론이 옳다고 결론지음, 이는 현실과 다르기 때문에 창조론이 말이 안 된다는 풍자를 담고 있다고 생각함, 이런 풍자적 측면은 굳이 길게 설명하지 않았음, 반종교적 풍자가 픽션에서 흔하기 때문에 Chiang만의 특별함은 아닐 것이라 판단함, Exhalation에 대한 의견에도 공감함, 그 세계에서 사람들이 위기를 벗어나길 바라지만, 동시에 Chiang이 어떤 의도로 이야기를 그 지점에서 종료했는지도 충분히 이해함
          + Exhalation에서 ""열역학이 다르게 작동하는 것처럼 보인다""라는 원글 내용은 틀렸다고 생각함, 열역학은 동일하게 작동하며, 밀폐된 완벽한 세계에서 자기 인식 존재가 엔트로피의 흐름과 자신의 유한성을 직접적으로 경험하는 점이 중요한 포인트임
     * Tower of Babel 이야기도 탑의 구조로 천국에 실제로 도달하는 반전을 통해, 세계의 토폴로지가 그렇게 설계된 점이 흥미로움, compatibilism에 대해 언급하면서, 첫 번째 정의(결정론을 받아들여야 한다)가 올바르다고 봄, compatibilist는 결정론이 오히려 자유의지라고 해석함(행동이 앞선 원인에서 유래해야 내적 상태와 일치하므로), 추가로, Chiang의 AI doomer 회의론을 ""블라인드스팟""이라 간단히 몰아간 점은 작가의 깊이를 충분히 다루지 않은 기사 태도에 아쉬움이 있었음
          + 지적 감사함, 1번 철학 내용은 오랜만에 다시 공부해봐야 정확하게 설명했는지 판단하겠음(철학자도 구독 중이라 언제든 더 깊은 피드백 받을 수 있을 듯함), 2번 AI 관점에 대한 부분 역시 ""슬쩍 넣은 것""이 아니라는 점을 밝힘, 이 논쟁이 워낙 많이 다뤄진 주제라, 깊게 논의하지 않고 개인적 견해와 독자 다수가 갖고 있을 법한 시각임을 언급만 하고 넘어감
          + compatibilism을 제대로 이해하는 데 도움을 준 두 가지 층위가 있다고 느꼈음, 첫 번째는 실용적 층위로, 내적 상태에 의한 행동·선택의 자유, 그리고 이에 따른 책임의 정의임, 두 번째는 형이상학적 층위인데, 이 층위에서는 이러한 선택이 의미를 가지며 도덕적 칭찬이나 비난의 근거가 된다고 함, 나는 첫 번째 층위에는 동의하지만 두 번째에는 동의하지 않음
     * ""Story of Your Life""는 Sapir-Whorf 가설(언어가 인식 체계를 규정)보다는 라그랑지안 세계관(미분/적분적 세계 해석)이 중심 주제라고 생각함, 영화에서는 이를 제대로 전달하기 어려워 언어적 부분이 강조됐다고 느낌, Exhalation은 모든 생명과 지성이 저엔트로피와 고엔트로피 사이 교차점 위에 존재함을 아름답게 표현한 이야기로, 열역학 자체가 다른 이야기는 아님, Chiang의 뛰어난 ""가정이 다르면?"" 탐구력에 항상 감탄하며 읽을 때마다 맑은 계곡물에 몸을 씻는 느낌을 받음, ""Anxiety is the dizziness of freedom""도 매우 좋아함
          + 혹시 너의 논문을 어디서 볼 수 있는지 궁금함, 나는 전자음악 아마추어인데, 주로 4/4박자 곡을 만들지만 새로운 패턴도 시도하고 싶음, 수학적인 부분도 어느 정도 이해하는 편임
     * 과학적인 SF 이야기를 좋아한다면 Greg Egan도 강추함, ""Singleton""은 다세계 해석이 현실이라면?을 다루고, ""The Orthogonal"" 3부작은 물리법칙 자체가 리만 공간이라면(기본적 시공간 구조가 다르다면) 어떤지 탐구함, 직접 이 페이지에서 물리 설명도 볼 수 있음
          + Greg Egan도 인물 중심의 SF 소설을 잘 쓸 때가 있으며(특히 단편에서), 자신이 깊이 공감하는 주제를 다룰 땐 인물의 변화가 뚜렷하게 그려짐, 반면, 그렇지 않을 땐 ""설득력 있는 단막극–기술적 함의 빠르게 탐구–다시 변화된 인물 등장""의 구조로 흘러가는 경우도 있음
     * 이 글 혹시 AI가 쓴 기사인지 궁금함, ""Exhalation에서 열역학이 다르게 작동하는 것 같다""는 내용이 있는데, 이 소설의 핵심은 열역학/엔트로피를 설명하는 것임, 작가가 관련 메모도 남겼음, 실제로 읽은 인간이 쓴 건지 의문임
          + 이번 리뷰 위해 단편을 다시 읽지는 못했지만, thermodynamics를 단순화해서 이야기를 설계한 줄 알았음, 그러나 여러 피드백을 듣고보니 내 표현이 ""appear to""처럼 조심스럽게 썼던 이유임
          + 해당 오류를 인간이 기억 착오로 작성했다고 믿고 싶으나, 짧은 테스트 결과 AI가 똑같이 Exhalation에서 thermodynamics가 다르다고 헷갈리는 경우가 많았음, GPT 4.5, 4.1, o3, Claude 4, DeepSeek R1은 틀렸고, 반복적으로 답을 맞춘 것은 GPT 5와 Claude 4.1 뿐이었음
          + AI가 쓴 느낌은 들지 않음, 사람이 쓴 것 같은 문장이 있음, 특히 ""과거엔 SF 작가들이 기술을 좋아했다. 최근에는 점점 흔치 않거나 오히려 촌스러운 관점이 됐다. 특히 서구권, 문학적·휴머니스트 SF라면 더더욱 그렇다"" 부분은 AI에서 본 적 없음
          + 단순한 기억 착오나 오해일 수도 있지 않을까 생각함, 나 역시 한 번도 경험한 적 있고, 어떤 이야기를 완전히 잘못 파악하는 경우도 자주 봤음, 굳이 AI일 필요는 없음, Ted Chiang이 최고의 SF 작가 중 한 명이라 생각 하고, LLM 평가와 달리 대체로 인간이 쓴 글로 보임, 만약 LLM이 쓰인 것이라면 저자가 직접 밝혀주면 좋겠음, 내 LLM에 대한 반감은 ""아직 충분히 좋지 않다""가 아니라 ""나는 인간과 대화하고 싶다""에서 비롯함
     * ""The Merchant and the Alchemist's Gate""가 Chiang 작품 중 단연 최고로 꼽음, 그의 이야기는 항상 지적 즐거움을 주지만, 스타일이 조금 건조해서 Philip K. Dick 작품만큼 감정적으로 몰입되지는 않음
     * ""평행 우주 간 비트 전송""은 실험이나 경제 측면에서 엄청난 가치를 갖는다는 점에 공감함, 제약회사가 수십억 달러 규모의 신약 임상시험을 모든 우주에서 돌려 결과를 공유하면 엄청나게 게임이 달라짐, 이 기술이 있으면 P=NP 문제가 즉시 해결되고, 정보 보안이 훨씬 어려워짐, 이 아이디어에 꽂혀서 생각이 멈추지 않았음, HPMOR에서 시간여행에 관한 유사한 트릭이 방어적으로 일찍 제시됨, Harry가 Time Turner를 쓰면 P=NP뿐 아니라 모든 체크는 쉽고 찾기는 어려운 문제(암호, 비밀번호 등)를 전부 풀 수 있게 됨, Ted Chiang의 스타일을 매우 창의적이고 감탄스러운 발견의 경외감을 잘 전달한다고 느낌
     * Ted Chiang의 소설은 몇 년마다 재독할 정도로 좋아함, 기사 저자의 많은 비판은 Chiang의 이야기가 대부분 ""과학 판타지""임을 받아들이지 못한 데서 비롯한 듯함, 대부분의 작품이 인물과 인물 간의 관계를 깊이 탐구하는 게 핵심임, 각 이야기의 ""gimmick""이 내부 논리에 맞게 철저히 확장·탐구되는 점이 장르와 관계없이 뛰어난 세계관 구축임, 예시로 Tower of Babylon(상상적 판타지집에 들어가도 이상하지 않음), Understand(인간 초지능에 대한 캐릭터 탐구가 염두에 남을 정도로 잘 쓰임)과 Exhalation(하드 SF의 대표)에 대해 언급함. Exhalation 역시 ""소프트""한 SF에 가깝다고 느낌
          + 판타지 vs SF를 미적 정의로 구분하는 게 흔한데(Tower of Babylon이 판타지에 가깝게 평가됨), Ted Chiang은 아예 다른 기준을 둠, 즉 ""우주 법칙이 특별한 소수에게만 예외적으로 적용""되는지가 핵심임, 만약 모두가 납을 금으로 바꿀 수 있다면 그것은 SF이며, 그 기술이 보편화되면 사회적 2차 문제가 쏟아지는 점에서 흥미로움, Tower of Babylon에서도 특별한 인물은 없고, 새로운 우주 규칙 하에서 타워 자체에 새로운 사회적 현상이 생기는 것이 SF적임, 관련하여 Chiang 본인의 인터뷰에서 더 명확히 볼 수 있음
          + Understand 정말 인상적이었음
     * 오래된 SF와의 비교를 즐김, Ted Chiang은 오늘날의 Stanislaw Lem(혹은 덜 미친 PK Dick)으로 표현 가능함, 특히 His Master's Voice는 가장 ""Chiangian""한 Lem 소설로 여김, ""수학자가 1+1=3임을 알아챘고, 남편은 더 이상 사랑하지 않음을 깨달음""이라는 이야기에 깊은 인상을 받음, Lem이나 Dick도 자신이 이런 스토리를 떠올렸다면 뿌듯해했을 것 같음, PK Dick에 비해 Chiang이 종종 더 건조하다는 지적도 있지만, 이것은 표면적 현상에 불과하며, 사실은 ""농축된"" 스타일임(Dick은 확산형 스타일임)
          + PKD와 Chiang의 비교는 미처 생각해보지 못했으나, 둘 다 팬으로서 너무 공감됨, PKD도 신과 천사가 실제로 존재해서 이교도를 벌주는 악몽 같은 이야기를 쓴 적 있고, 이는 Chiang의 Hell is the Absence of God와도 유사함, Chiang이 Dick보다 ""건조하다""고 보지 않고, 오히려 더 ""제정신""인 PKD라고 생각함, PKD를 열렬히 좋아하는 팬 입장에서 하는 말임
     * 오래전부터 compatibilist였으나 관련 용어와 철학 맥락을 이제야 알게 되었음, 이 믿음이 때로 오해를 사는데, 모두가 잘못된 이분법에 매몰되어서 그렇다고 느낌, Arrival을 정말 좋아했지만 원작 소설 Story of Your Life나 작가에 대해서는 굳이 찾아보지 않았음, 이제 Chiang의 모든 작품을 읽어볼 생각임, 일관성 있는 과학적 세계관이 반영된 픽션이 드문 만큼 이런 면에서 Sam Hughes(qntm)의 작품도 좋아함
          + 이 리뷰가 누군가에게 영향을 미칠 수 있다는 게 무척 기쁨, 꼭 Story of Your Life부터 보고 나머지 작품도 읽어보길 추천함, 단편과 영화가 동시에 매우 비슷하면서 극명하게 다르니 직접 판단해보면 좋겠음, 보통은 단편을 먼저 보면 단편을 더 좋아하고, 영화를 먼저 보면 영화를 더 좋아하게 되지만, 혹시 다를 수도 있으니 꼭 읽고 후기도 남겨주면 좋겠음
          + 오늘의 행운의 10,000명 중 한 명임을 축하하며, 두 작품집 모두에서 즐거운 경험을 할 것이라고 자신함

   언급되지 않은 소설이 많이 있지만, 테드창의 작품 중에 이런 제목도 있음. The Lifecycle of Software Objects.
"
