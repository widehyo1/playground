"https://news.hada.io/topic?id=22244","영문 위키미디어 재단, 영국 온라인 안전법 규정에 법적 이의 제기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  영문 위키미디어 재단, 영국 온라인 안전법 규정에 법적 이의 제기

     * 위키미디어 재단은 영국 온라인 안전법(Online Safety Act) 내 분류 규정(Categorisation Regulations) 에 대해 법적 이의 제기를 진행 중임
     * 위키미디어는 해당 규정이 자원봉사자 기여자와 위키피디아 정보의 신뢰성을 위협한다고 주장함
     * 카테고리 1 규정이 적용될 경우 개인정보 보호 약화, 데이터 유출 위험 등 다양한 부작용 발생 우려
     * 이 사건은 영국 고등법원에서 처음 심리되며, 일반 시민 및 위키피디아 기여자의 권리 침해 쟁점이 부각됨
     * 영국 내 수백만명이 문화유산 보존 및 정보 공유에 사용하며, 이번 판결이 세계적 선례가 될 가능성 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

위키미디어 재단, 영국 온라인 안전법 분류 규정에 대한 법적 대응

  소송 개요 및 배경

     * 2025년 7월 22, 23일에 영국 런던 고등법원에서 위키미디어 재단이 영국 온라인 안전법의 분류 규정(Categorisation Regulations)에 대해 공식적인 소송을 제기함
     * 위키미디어 재단은 비영리 단체로 위키피디아 및 기타 Wikimedia 프로젝트를 운영하며, 해당 규정이 위키피디아와 전 세계 자원봉사자 커뮤니티에 심각한 위협을 준다고 강조함
     * Stephen LaPorte 위키미디어 재단 법률 고문은 이번 재판이 공익 기반 온라인 프로젝트 보호에 대한 글로벌 선례를 만들 수 있는 기회임을 밝힘
     * 위키피디아는 세계 상위 10위 웹사이트 중 유일한 비영리 플랫폼이며, 대형 언어 모델(LLM) 훈련에 사용되는 데이터 집합의 품질 면에서도 중요한 위치를 점유함
     * 재단은 위키피디아가 인터넷상의 위험도가 높은 영리 사이트들을 기준으로 만든 규정의 적용으로부터 보호받아야 함을 주장함

  위키피디아의 운영 구조 및 영향력

     * 전 세계 약 26만 명의 자원봉사 기여자가 직접 정보를 작성·관리하며, 중립성·팩트 기반·신뢰성 있는 출처 확보에 중점을 둔 정책 및 자율적 규제 시스템을 운용함
     * 25년 이상의 인력 중심 콘텐츠 관리 모델로 300개 이상 언어, 6,500만 개의 문서, 월간 150억 회 이상의 정보 조회를 기록하고 있음
     * 위키미디어 재단은 건강한 온라인 참여 환경 조성이라는 영국 정부의 취지에는 공감하지만, 전체 법률이나 카테고리 1 의무 자체에 반대하는 것은 아님
     * 재단의 소송 초점은 오직 새로운 분류 규정이 위키피디아에 카테고리 1 의무(가장 강력한 의무 조항)를 적용할 수 있게 하는 점에 있음

  카테고리 1 규정 적용의 위험성

     * 카테고리 1 규정이 위키피디아에 적용될 경우, 플랫폼은 기여자의 신원 확인 의무 등이 생겨 개인 프라이버시 및 자원봉사자 보호가 심각하게 약화됨
     * 이는 데이터 유출, 스토킹, 소송, 독재 정권의 처벌 등 다양한 실질적 위험을 야기할 수 있으며, 필수 인력·자원의 분산을 초래함
     * 관련 상세한 영향 및 우려는 공식 블로그 포스팅에서 추가로 제공됨

  소송 참여자와 절차

     * 재단은 영국 기반의 오랜 위키피디아 자원봉사자인 User:Zzuuzz와 공동 원고로 소송을 진행함
     * 기여자 측 입장은 위키피디아 참여자들의 프라이버시, 안전, 표현·결사권 침해 위험을 중점적으로 드러냄
     * 이번 소송은 분류 규정에 대한 첫 법적 이의 제기이자, 자원봉사 편집자가 공동원고로 참여하는 최초 사례임
     * 수년간 규제 당국·정책 입안자와의 대화, 영국 의회 및 시민사회단체의 경고 이후에도 해결되지 않은 우려를 반영함

  공공성·문화적 가치 및 청문 일정

     * 위키피디아 및 위키미디어 프로젝트는 미디어 리터러시, 정보 공유, 문화유산 보존 등에서 글로벌 공공재로서 중요한 역할을 수행함
     * 영국 내에서만 수천 명의 자원봉사자, 영국 도서관·문화기관의 협력 콘텐츠, 그리고 매월 7억7천6백만 뷰를 기록함
     * 특히 웨일스어 위키피디아는 세계에서 가장 많은 사용자 수를 자랑하며, 웨일스 공식 교육과정의 일부임
     * 고등법원 심리는 King’s Bench Division 행정법원에서 진행되며, 사건 번호와 장소 정보가 곧 공개 예정임
     * 법원 판결문은 청문회 후 발표될 예정이며 정확한 시기는 미정임

개인 정보 및 미디어 문의 안내

     * 공동 참여자인 User:Zzuuzz의 신원은 법적·재단의 보호 아래 비공개로 유지됨
     * 미디어 문의는 공식 이메일(press@wikimedia.org)로 가능함
     * 글로벌 옹호 활동 뉴스레터 구독을 통해 사건 및 위키미디어 재단의 정책 활동 소식을 수신할 수 있음

위키미디어 재단 소개

     * 위키미디어 재단은 비영리 단체로, 위키피디아 및 다양한 자유 지식 프로젝트를 운영함
     * 모든 인류가 자유롭게 지식을 공유할 수 있는 세상을 비전으로 삼음
     * 누구나 협력·기여할 수 있고, 지식을 자유롭게 접할 수 있다는 가치를 지향함
     * 콘텐츠 호스팅, 소프트웨어 경험 구축, 자원봉사자 커뮤니티 및 파트너 지원 등을 주요 사업 영역으로 함
     * 미국 캘리포니아 샌프란시스코에 본부를 두고 있음

        Hacker News 의견

     * 예전에 Theresa May가 개인 암호화를 전면 금지하려 했던 기억이 자꾸 떠오름. 참고로 이 나라는 이미 경찰에게 암호화 키를 법적으로 제공해야 하고, 그렇지 않으면 다른 범죄가 없어도 2년형을 받을 수 있음. 이런 조치들은 그녀의 복잡한 주제에 대한 이해 수준을 잘 보여줌. 그 이후로 더 나빠졌음
          + 이 일을 생각할 때마다 떠오르는 BBC 뉴스가 있음 기사 링크: ""내무장관의 남편이 집에서 성인물을 봤다가, 그 비용을 공제받으려고 해 아내를 난처하게 한 일에 대해 사과함."" 후속 기사도 재미있는 내용이 많음 후속 기사
          + 90년대 중반 프랑스에서 암호화의 등급이 낮은 것조차 금지되었던 시절이 있었음. 그때도 아주 작은 포럼에서 이 상황을 재밌게 생각했던 기억이 있음. 관련 기사. 1996년 전까지는 어떤 문서라도 암호화하려면 공식 허가를 먼저 받아야 했고, 그렇지 않으면 1000~89,300달러의 벌금과 2~6개월 형이 부과되었음. 지금도 특별한 예외를 제외하면 대부분의 암호화 소프트웨어 무단 사용이 불법임. 이 두 구 제국은 자기 영향력과 통제력을 과대평가하는 버릇이 있는 듯함
          + 이 문제가 단순히 이해력이나 지능의 부족 때문이라고 생각하지 않음. 권력과 통제의 문제임. 지도자들의 지능을 탓한다고 구조적인 변화가 일어나지 않음. 오히려 더 똑똑하면 그만큼 피해와 허락 확보가 더 가속될 수 있음
     * 왜 이런 조치들이 부모 통제로 처리되지 않는지 궁금함. 요즘 아이들은 대부분 모바일이나 태블릿을 쓰고 있는데, 주요 제조사들은 이미 부모를 위한 관리 도구를 제공함. 기존에 이미 부모 통제로 콘텐츠 필터링이 가능하고, 모바일 브라우저는 사용자가 특정 연령 이하라면 헤더를 내보내도록 할 수도 있음. 앱도 연령 플래그에 접근할 수 있음. 부모가 아이를 키우는 책임을 져야 하며, 빅테크가 대신할 이유가 없음. 왜 이게 더 복잡해져야 하는지 이해가 안 감
          + 누구나 인터넷의 유해한 콘텐츠에서 아이들을 보호해야 한다고 주장할 때마다 항상 이해가 안 감. 왜 ""부모의 역할을 하라""는 것이 이런 시대에 그렇게 불가능한 일인지 모르겠음. 모든 기기와 운영체제에 부모 통제가 존재하는 이유가 있음. 완벽하진 않아도 대부분의 유해 콘텐츠 유입은 막아줌
          + 이 주제에 대한 반대 의견에는 인지적 부조화가 있음: a) 콘텐츠 통제는 안 통하니 정부가 무슨 생각이냐? b) 이건 부모의 문제니 부모가 콘텐츠 통제를 써야 함. 하지만 개별적 조치만 해서는 통제되지 않는 아이 한 명만 있어도 다른 모두가 영향을 받음. VPN이나 중고 스마트폰 등 우회 방법도 많음. 부모가 통제 한 번 켜는 건 귀찮아도, 이런 전국적인 캠페인을 참여할 열의는 있다는 점도 모순임 (예: smartphonefreechildhood.org). Jonathan Haidt 같은 이들도 OS 수준의 나이 플래그를 주장 중이니 대안적 가치가 있음. 참고로 실제 현장을 보여주는 사례들: 기사를 통해 본 포르노 접근 사례, 친구 아이패드로 유해물 본 8세 사례
          + 그렇게 되어야 하지만, 실제로는 많은 부모가 아이를 직접 돌보기를 귀찮아하고, 국가에 떠넘기기 원함. 삶의 많은 영역에서 이런 현상이 퍼지고 있어서 통제의 손길은 점점 더 강해질 것 같음
          + 이런 정책의 겉말을 그대로 믿는 건 독재와 협력하는 의사표명임. 이 모든 주장은 아이들과 상관없고, 감시와 통제를 늘리기 위한 얕은 명분임
          + 이상적으론 부모 통제로 해결하는 게 맞지만, 현실에서는 부모들의 기술적 역량이나 동기 부족 때문에 아이들 기기 관리를 하지 못하는 상황임
     * ""Wikimedia Foundation이 영국 정부와 마찬가지로 온라인에서 모두가 안전하게 참여할 수 있는 환경을 지지한다고 말함. 단, OSA(Online Safety Act) 전체나 1등급 의무 자체에 도전하는 게 아니라, 새로운 분류 규정이 Wikipedia에 가장 엄격한 1등급 규정을 적용할 리스크가 있다는 점에 법적 도전을 하는 것임."" 이게 Wikipedia의 현 입장임. 현행법상 Wikipedia가 1등급 규정을 적용받을 수 있는 상황임
     * 법적으로 Wikipedia의 싸움이 큰 성과를 낼 것 같지는 않음. 분류 규정은 1차 법이 아니다 보니 사법심사 대상이 될 수 있지만, Wikimedia는 왜 이 규정이 위법한지에 대한 논거가 아니라 그냥 ""동의하지 않는다""는 주장을 펼치는 것임. 설령 성공한다 해도 OSA의 핵심(성인 콘텐츠 성인인증 의무 등)에는 영향 없음
          + 성인 인증만 논의의 중심이 되는 것이 문제임. 모든 추가 효과(커뮤니티 포럼·위키 폐쇄, 블로그 댓글 불확실 등)는 무시됨. 결국 개별 사이트들은 컴플라이언스 리스크와 비용 때문에 빅테크 플랫폼에 몰릴 수밖에 없음
          + 추가 설명: OFCOM이 Wikipedia를 1등급으로 분류하면 아주 심각한 부담이 발생하므로 법원에 이 조치의 재검토를 원하고 있음
          + 분류 규정 어디에도 Wikimedia에 적용될 의도는 잘 안 보임. 실제로는 법원이 Wikimedia에 적용되지 않는다고 안심시킬 가능성이 큼. 이런 판례는 유사 웹사이트 운영자에게 도움이 될 순 있지만, Meta 등 거대 플랫폼에는 여전히 적용됨
          + 근거가 뭔지 잘 모르겠음. 현재 법원에 공식 서류가 접수됐는지? PR만 보면 구체적 변호인 주장은 없음. 이 입법이 주요 골자를 바꾸지는 않겠지만, 규제 체계 자체가 이런 사례에 매우 중요한 상황임. 결과와 상관없이 입법의 허점을 드러낼 기회가 될 수 있음. Wikipedia에 엄격한 적용 시 많은 문서들이 논란을 일으킬 수 있음
          + 영국 판례법에서는 의회가 절대적이므로 이 도전이 통할 가능성은 매우 낮음
     * OSA와 관련 규정이 마음에 들진 않음. 차라리 HTTP 응답에 X-Age-Rating만 포함하는 식으로도 충분했다고 생각함. 법 자체가 너무 길고 복잡하게 느껴져서, 각 조직이 어떤 의무를 지는지 알기 힘듦. 하지만 Wikimedia의 도전이 어떤 법적 근거로 가능한지 모르겠음. OSA는 1차 법이라 인권법 위반 수준이 아니면 다투기 힘듦. 규정은 2차 법이라 챌린지 가능성이 있지만, 근본적인 다툼의 근거가 잘 안 보임. 단순히 ""마음에 안 든다""로는 불가능함
          + X-Age-Rating은 서버가 수신자의 관할권을 확실히 알아야만 제대로 작동할 것임. 한 단계 더 나아가면 서버가 콘텐츠를 다중 태그로 표시하고, 이걸 수신자가 해석하게 하면 됨. 예를 들어 UN ISIC 태그, Dewey Decimal System 등 여러 합의된 분류체계로 태그링 가능함. 주요 사이트는 자신만의 태그 시스템을 계속 쓸 수도 있음. 예를 들어 만화 노래라면:
            X-Content-Tags: ISIC:6010 UDC:797 YouTube:KidsTV
            그 다음엔 기기나 소프트웨어가 각 국 법규에 따라 걸려있는 콘텐츠만 사용자에게 경고하게 만들 수 있음
     * 위키피디아는 영국 정부 IP 전면 차단으로 항의해야 함
          + 정말 그렇게 될 수도 있음 (만약 Wikipedia가 영국에서 접근 금지되는 상황까지 가면, 그제야 사회가 문제의식 가질 가능성도 있음)
          + 영국인 입장에서는 이걸로만 정부와 대중에게 충격을 줄 수 있다고 생각함
          + 그럼 영국·타국 사용자는 미국 정부 IP도 전면 차단해야 하지 않을까? 관련 주제: 미국에는 최소 25개의 법이 법적 구속을 줄 수 있고, 가장 이상한 텍사스 주법도 미연방 대법원이 유효하다고 판결함. 결국 미국 자유의 근본도 짧은 시기에 흔들리고 있음. 그런 상황에서 영국만 비아냥거리는 건 큰 의미가 없음
     * 최근 소식으로, 노동당이 VPN을 금지하는 방안을 검토한다는 기사도 있음. OSA 발효 후 이틀만에 이런 이야기가 나옴 관련 기사
          + 나 역시 OSA를 싫어하지만, 노동당은 VPN 금지를 계획한 적이 없음. 한 의원이 6개월 후 VPN 영향에 대한 정부 조사 조항 추가 고려했을 뿐임. 이 조항이 실제로 포함됐는지 모르겠지만, 법을 도입했다면 영향 평가를 하는 게 상식임. GB 뉴스는 신뢰도가 매우 낮음
          + 극우 성향 매체의 제목만 보고 노동당이 최근 VPN에 대해 뭔가 발표했다는 듯이 오해하면 안 됨
          + 해당 기사는 2022년 논의 내용에서 비롯된 것이며, 최근 이슈가 아님
          + GB News의 신뢰도는 Fox News와 비슷한 수준임. 다른 곳에서 정보를 얻는 걸 추천함
     * 관련해서, OSA가 성인 콘텐츠 없는 포럼 운영자(예: 제품 관련 포럼)에 어떻게 적용되는지 간략한 개요를 작성해 봤음 블로그 링크
     * 누군가 Wikipedia가 왜 1등급에 들어가는지 설명해줄 수 있는지 질문함. 만약 겨우 1등급이라도 해당된다면, '추천 시스템'만 꺼도 되는 것 아닌가? 예시로는 모바일 하단에 노출되는 '관련 문서' 자동 생성 기능이 있음 법적 정의 링크
          + '콘텐츠 추천 시스템'의 정의는: ""알고리즘(머신러닝 포함 또는 기타 기술)을 사용해 사용자 생성 콘텐츠가 다른 사용자들에게 어떻게/언제 제시되는지 결정 또는 영향 미치는 시스템""임. 추측이지만, 위키 미디어의 여러 관리자 도구(예: 최근 편집 내역 중 주목할 만한 것 추천 등)도 해당될 수 있음. 머신러닝 사용 자체는 많지 않지만, 기타 기술이나 필터(예: ORES, AbuseFilter)로도 광범위하게 포함될 가능성이 있음. (참고로 나는 WMF 소속이지만 이 재판 건에 대해선 잘 모름)
          + 아마도 전 세계 지식 수집이라는 미션이, 권위주의적 도덕 패닉에 굴복하는 것보다 더 중요하다고 진심으로 믿는 것임
     * 이 모든 상황은 위선적임. '모두를 위한 열린 인터넷'을 주장하면서, 자기 사이트가 특수하다고 예외를 요구하는 건 앞뒤가 맞지 않음. 평판 좋은 사이트만 열리고, 정치에 따라 '이 사이트는 왜 살아야 하나요?' 같은 청원이 필요한 인터넷이 되면, 그건 이미 닫힌 인터넷임. Wikimedia의 논리도 본질적으로는 '우린 이미 정부가 원하는 수준만큼 잘 통제하고 있으니, 우리한테만은 적용하지 마라'임. 아무리 변호해도, Wikimedia의 이익이 곧 공공의 이익은 아님. 변호인 논리가 'OSA 전체가 아니라, Wikipedia에 한정된 분류 규정에만 도전'하는 것이다 보니, 이 특수 예외주의도 혼동하면 안 됨
          + 이런 의견에 동의하지만, 대안을 제시해줄 수 있는지 궁금함. 법이 이미 통과됐으니, Wikipedia 입장에선 법을 지키면서 동시에 프라이버시 문제로 이행을 꺼릴 수밖에 없음. 그렇다면 Wikimedia는 포기하거나, 무시하거나, 차단하거나 선택지가 무엇이 될지 궁금함
          + Wikimedia는 자신들의 특수성을 근거로 소송을 제기하는 동시에, 법원이 이를 기회 삼아 전반적인 오픈 인터넷의 권리를 보호해주길 기대할 수 있음. 법 구조상 오픈 인터넷이 원천적으로 불가능한 점은 Wikimedia의 잘못이 아님
"
"https://news.hada.io/topic?id=22344","Perplexity가 크롤링 금지 지침을 우회하기 위해 은밀하고 신고되지 않은 크롤러를 사용함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Perplexity가 크롤링 금지 지침을 우회하기 위해 은밀하고 신고되지 않은 크롤러를 사용함

     * Perplexity가 크롤링 금지 지침을 우회하기 위해 신원을 숨긴 웹 크롤러를 사용함
     * robots.txt 파일 무시 및 IP, User Agent 지속적 변경같은 행위가 포착됨
     * 신규 도메인 실험에서 금지 설정에도 불구하고 Perplexity가 사이트 콘텐츠에 접근함이 확인됨
     * Cloudflare는 이러한 행위를 방지하기 위해 Perplexity를 공식 인증 봇에서 제외하고 관리 규칙을 수정함
     * OpenAI와 같은 선의의 봇 운영자와 대조적으로, Perplexity의 은폐형 행동이 문제로 지적됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Perplexity의 은밀한 크롤러 사용 행위 개요

     * Perplexity는 AI 기반 답변 엔진으로, 공식적으로 신고된 유저 에이전트로 처음 웹사이트를 크롤링함
     * 그러나 네트워크 차단에 직면할 경우, 신원을 감추기 위해 User Agent를 변경하고, 다양한 ASN(자율 시스템 번호) 을 통해 접근을 시도함
     * 이 과정에서 robots.txt 파일을 무시하거나 아예 요청하지 않고 접근하는 시도들이 다수 발견됨

웹사이트와 크롤러 간 신뢰 원칙 및 문제 행태

     * 지난 수십 년간의 인터넷은 신뢰를 바탕으로 발전했고, 크롤러 역시 투명성과 목적성, 그리고 명확한 행동 기준을 갖추는 것이 원칙임
     * 크롤러는 웹사이트 소유자의 지침과 우선순의를 존중해야 하며, Perplexity의 이번 관찰된 행동은 이러한 원칙에 위배되는 사항임
     * 이로 인해 Cloudflare는 Perplexity를 공식 인증 봇 목록에서 제외하고, 스텔스 크롤러 탐지 및 차단을 위한 추가적인 관리 규칙을 적용함

실험 및 탐지 사례

     * Cloudflare는 신규 도메인을 만들어 Perplexity의 크롤링 행태를 실험함
          + robots.txt로 모든 자동 접근을 금지하고, WAF 규칙을 추가 설정했음
          + Perplexity의 공식 User Agent 및 IP를 막았음에도, 여전히 신분을 숨긴 채 사이트 콘텐츠를 수집한 사실이 확인됨
     * Perplexity는 공식 및 스텔스 User Agent 모두에서 콘텐츠 접근을 시도하며, 후자는 실제 브라우저(Chrome)로 가장함

   구분                                                        User Agent 예시                                                           일일 요청량
   공식 Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Perplexity-User/1.0; +https://perplexity.ai/perplexity-user) 2,000만~2,500만
   은밀 Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36       300만~600만

     * 공식 IP 범위에 포함되지 않은 여러 IP와 ASN을 사용하고, IP를 지속적으로 교체하며 차단 정책을 우회하려 함
     * 이로 인해 수만 개의 도메인과 수백만 건의 요청에서 이러한 활동이 식별되었으며, Cloudflare는 머신러닝과 네트워크 신호를 활용해 해당 크롤러를 지문화함

스텔스 크롤러 우회 사례 및 한계

     * 스텔스 크롤러 차단 시 Perplexity는 타 웹사이트 등 외부 자료를 활용해 답변을 제공하려 함
     * 그러나 이 경우 콘텐츠 상세도가 현저히 떨어지는 현상도 확인됨

선의의 봇 운영자 기준과 OpenAI의 모범 사례

     * 잘 운영되는 봇은 투명성, 신원 명확화, 활동 목적 공개, 개별 활동 별 독립된 봇 사용, 웹마스터 규칙(robots.txt 등) 준수 등의 원칙을 지녀야 함
     * OpenAI는 공식 IP와 User Agent, 크롤러 활동 목적을 투명하게 제공하고, robots.txt를 엄격히 준수함
     * 실제 실험에서도 ChatGPT 크롤러는 disallow 설정 또는 네트워크 차단을 발견하면 추가 크롤링 시도를 중단함
     * Web Bot Auth 등 표준화된 인증 방식도 적극적으로 도입함

보호 방법 및 대응

     * Perplexity의 신고되지 않은 User Agent에서 발생된 모든 크롤링은 Cloudflare 봇 관리 시스템에서 탐지되어 차단됨
     * Cloudflare의 기존 봇 차단 규칙 또는 챌린지 규칙 활성 고객은 이미 보호 대상임
     * 스텔스 크롤러 차단용 관리자 규칙이 전체 고객(무료 고객 포함)에게 제공됨
     * Content Independence Day 발표 후 250만 개가 넘는 웹사이트가 AI 크롤링 금지 정책을 적용함
     * 봇 운영자의 계속 진화하는 우회 시도에 맞춰 Cloudflare도 대응 체계와 기술을 지속적으로 발전 중임

정책적 노력 및 향후 전망

     * Cloudflare는 전 세계 기술 및 정책 전문가, IETF 등과 함께 robots.txt 확장 표준화 논의에 적극 참여 중임
     * 신뢰받는 크롤러 규칙을 정립하고, 급변하는 AI 및 크롤러 환경에서 투명성과 준법성을 강조하는 방향으로 나아감

   퍼플렉시티 응원해

        Hacker News 의견

     * 이 문제를 해결하는 게 정말 어렵다고 생각함
         1. 내가 사람으로서 웹사이트에 요청하면, 당연히 콘텐츠를 볼 자격이 있다고 모두가 동의함
         2. 내가 내 컴퓨터의 소프트웨어, 예를 들면 광고 차단기를 설치해서 콘텐츠가 나오기 전에 바꾸게 하는 건 내 선택임, 그리고 웹사이트가 이걸 알지 못하게 하는 것이 맞다고 생각함, 대부분 사용자도 동의하지만 일부 사이트는 사용자에게 설치된 소프트웨어를 바꾸라고 귀찮게 함
         3. 그런데 여기서 한 단계 더 가서, 광고, 자바스크립트, 팝업으로 뒤덮인 콘텐츠를 내가 직접 사용하기 어려워서 LLM(대형 언어 모델)을 이용해 내용을 요약해서 보게 된다면, 왜 내가 Firefox 브라우저로 웹사이트에 접근하는 것과 LLM이 나 대신 웹사이트에 접근하는 게 법적으로 다른 취급을 받아야 하는지 모르겠음
          + 몇몇 매장은 Instacart나 Postmates 같은 서비스를 환영하지 않음
            네가 직접 쇼핑하든, 가격 비교를 위해 휴대폰으로 모든 물건을 스캔하든 건 상관없음
            하지만 제3자 서비스가 자체 직원을 보내서 재고를 조사하거나, 온라인 주문 후 물건을 대신 픽업하는 건 허용 안 함
            이유는 다양함: 상품 품질 인식 제어를 잃고 싶지 않은 점(식음료가 차가워진다거나, 가격이 올라간다거나, 잘못된 대체가 이루어진다거나), 직원이 직접 서비스하며 고객과 관계를 쌓고 싶음, 아니면 그냥 제3자 딜리버리 자체를 반대하는 경우도 있음
            관련 없는 기업이 내 오프라인 매장 안에서 영업하는 걸 거부하는 건 당연히 합리적인 선택이라고 생각함
            이런 논리가 디지털 서비스에도 적용된다고 봄
          + 이건 규모의 문제임
            너가 말한 다음 단계는 아마도
            사람들이 개인용 리서치 봇을 돌려 수많은 사이트에서 답을 찾아 페이지를 사람보다 훨씬 빠르게 요청하는 날임
            어느 지점까지가 허용 가능한지 고민이 필요함
            개인 크롤링은 괜찮은가? 아니면, 봇이 좀 더 똑똑해져서 사용자가 뭘 물어볼지 미리 예측하고 항상 최신 정보로 크롤링하는 건?
            혹은 규모가 더 커져서 여러 이용자용 대량 크롤링이 시작되면 그때는 문제가 되는 것인가?
          + 나는 ""크롤러""와 ""페처(fetcher)""라는 용어로 대량 스크래핑과 사용자 타겟팅 에이전트를 구분하는 게 좋다고 봄
            최근 AI 에이전트 탐지 도구 개발에 참여하고 있는데(참고: https://stytch.com/blog/introducing-is-agent/), 웹사이트 운영자가 AI 에이전트를 식별해 제한된 접근 방법을 권유할 수 있는 건 진정한 가치가 있다고 생각함
            반면에, 크롤러들은 남의 이름을 도용해 명성 있는 크롤러인 척하며 robots.txt를 무시하고 나쁜 행동을 할 수 있음
            표준 해법이 현재로선 IP의 역방향 DNS 조회인데, 이 역시 사이트 운영자 입장에선 번거로움
            차라리 특이한 접근을 모두 차단하는 게 더 효율적이라 생각함
          + 광고 모델 자체가 문제가 많다는 점 동의함
            하지만 AI 회사들이 콘텐츠 제작자와 이용자를 분리시키는 상황은 내가 앞으로 보고 싶은 웹의 모습이 아님
            예를 들어 누군가가 유료 뉴스레터를 운영하면서, 일부만 무료로 공개해 관심 있는 방문자를 모으고, 이 중 일부를 결제 사용자로 전환시킴
            이런 제작자는 ‘콘텐츠 보면서 업셀(가입 유도)’이 반드시 함께 이뤄지길 기대함
            만약 AI 크롤러가 그 과정을 건너뛰고 중요한 콘텐츠만 뽑아가면, 굳이 공짜로 웹에 올릴 이유가 없음
            AI 크롤러가 이기면 결국 모두가 손해임
          + 세상에 광고 범벅이 아닌 페이지도 정말 많음
            기존 검색 엔진은 ‘우리가 페이지 크롤링 허용할 테니 네가 트래픽 가져다 달라’는 묵시적 계약이 있었음
            비공개 모델을 위한 AI 크롤러는 이 계약을 깨뜨림
            데이터로 모델을 만들어 QA(질의응답) 기능을 갖추고, LLM 운영 회사가 웹사이트에서 크롤러 통해 얻은 지식으로 수십억 수익을 내지만, 웹사이트에게 돌아오는 건 없음
            그냥 유저 요청용으로 가져온다 하더라도 LLM 제공자가 수익의 대부분을 가져가고, 실제 콘텐츠 저자는 아예 방문조차 못 받게 됨
            만약 Perplexity가 robots.txt와 차단을 무시하고 유저 요청용으로 페이지를 가져가는 게 괜찮다면, 그 데이터를 학습에 여차하면 활용하지 않을 것이라 기대하긴 힘듦
     * 빠른 변화가 흥미롭다고 느낌
       웹이 ‘전 세계’가 아닌 더 작거나 구성원 중심(반드시 지리적 의미가 아니라 사회적 의미에서)의 공동체로 집중하는 게 오히려 유익함
       자신만의 커뮤니티를 키우고, 더 사적인 공간으로 초대하는 방식이 앞으로 더 중요해질 것 같음
       옛날의 개방형 웹은 기계들 위한 공간이 될 듯
       우리가 예전에는 ‘버블(거품, 자기만의 공간)’을 싫어했지만, 사실 버블은 당연하고, 혼자만 아니면 분명 의미 있음
       웹에 기계와 기계 콘텐츠가 넘쳐나게 되면, 결국 사람들은 다시 서로 연결되는 법을 배울 것임
     * Perplexity AI를 질문으로 테스트해봤을 때, 차단된 도메인 내용도 상세히 알려준다는 실험결과에 대해
       이 내용이 특정 회사(Perplexity)를 비판하는 마케팅성 기사로서 결론이 애매하다고 느낌
       Perplexity가 직접 크롤링(시스템적으로 모든 페이지를 훑는 것)을 한 건지, 유저 요청에 따라 그냥 한 번 가져온 건지 구분이 명확하지 않음
       대부분의 사람은 이 둘을 다르게 보며, 후자가 전자보다 훨씬 더 용인될 만하다고 생각함
          + 뭔가 Perplexity 광고처럼 느껴짐
            이번에도 Cloudflare가 착한 역할, Perplexity가 나쁜 역할로 나오는데, Cloudflare 역시 요즘 웹을 살리겠다는 마케팅을 세게 함
            근거는 얕고 양사 모두 “거인들의 싸움”처럼 비춰져서, 어쩌면 Perplexity에겐 PR적으로 오히려 이득이 되는 전개라고 느껴짐
          + 사용자 대신 페이지를 가져오는 건 원론적으로 허용될 수 있지만, AI 기업들이 이미 저작권 등 규범을 무시해온 걸 보면, 페이지 내용을 또 저장해 미래 학습이나 추가 크롤링에 쓸 가능성을 무시할 수 없다고 생각함
          + HTTP 스펙(사양)에서도 이런 구분이 간접적으로 드러남
            “user agent(사용자 에이전트)”라는 개념이나 명칭에서 구체적으로 분리된다는 점이 있음
          + AI가 결과를 다 캐싱하거나 아카이브해서 많은 사람들이 사용하게 된다면, 결국 그건 스크래퍼랑 다를 게 없어짐
            캐시된 데이터로 학습만 하면 되는 것임
            중간자 역할을 하면서 중요한 콘텐츠를 빼가고, 덤으로 데이터 가치 신호까지 얻는 방식임
     * Perplexity가 TechCrunch에 보낸 답변에 따르면
       Cloudflare 블로그 포스트는 “세일즈용 호객행위”에 불과하다고 일축함
       게다가 블로그 스크린샷은 “아무 콘텐츠도 접근된 적이 없음을 보여준다”고 주장
       블로그에서 지목한 봇도 자기들 것이 아니라고 덧붙임
     * Perplexity 자체는 크롤러를 막고 있음
$ curl -sI https://www.perplexity.ai | head -1
HTTP/2 403

       브라우저 user agent로 속여도 똑같이 차단됨
       꽤 정교한 크롤러 탐지 방식을 활용하는 것 같음
          + 누가 이미 이 질문을 CEO에게 한 적 있음 https://x.com/AravSrinivas/status/1819610286036488625
          + 우스운 건, Perplexity도 Cloudflare를 사용하고 있음
     * 항상 ‘스텔스’ 크롤러가 이길 거임
       브라우저 자동화 도구(W3C WebDriver2, Chrome DevTools 프로토콜)로 스크래퍼를 만들어 탐지가 거의 불가능해짐
       캡차(captcha)를 걸 수 있지만, 개발자가 휴먼인더루프(사람 개입) 워크플로를 넣어 콜센터 근무시간 중엔 사람이 직접 처리하게 설계할 수도 있음
       15년 전 게임 개발 테스팅에서도 ‘래스터(화면이미지)’ 기반 스크래핑 기법이 쓰였는데, 이런 게 오늘날 인터넷 경찰을 상당히 곤란하게 만들 것임
          + 스텔스 크롤러가 이길 수 없는 이유는, 결국 가치 있는 모든 사이트 접근에 원격 증명이 필수가 될 거라 생각함
     * 인터넷에 마이크로페이먼트(초소액 결제) 시스템이 필요하다고 봄
       크롤러가 페이지당 1센트라도 내준다면 24시간 크롤링 모두 환영임
       내가 직접 1센트씩 내고 콘텐츠를 보면, 클릭랩이나 기묘한 광고 규칙을 견딜 필요 없음
       무료 접근이 항상 봉쇄될 필요는 없음(실제로는 봉쇄될 테지만 그건 또 의미 있음)
       예를 들어 Reddit이 높은 수수료를 부과하되 좋은 콘텐츠에는 환급을 해 품질을 높일 방법도 상상함
       “선입금-출금-페널티” 같은 새로운 시스템도 가능함: 가입 때 보증금을 걸고, 밴 당하면 몰수, 정상적으로 활동하면 환급하는 식. 이는 관리 업무 단순화와 콘텐츠 품질향상 목적임
       이런 발상이 필요한 건, 지금 인터넷이 점점 더 쓰레기로 가득 차기 때문임
       또 다른 아이디어: 구글 등에 검색 한 번당 1센트씩 내고, 결과가 맘에 안 들면 돈을 돌려받을 수 있게 하는 것
       구글 AI가 만족도 측정해주고, 만족스런 검색이 안 되면 광고 범벅 인기순만 보여줌
       그러면 유저가 다른 검색엔진에 돈을 맡기는 방식임
     * 누군가가 웹사이트를 무분별하게 크롤링하다가 공개망 신뢰성을 위협하면 문제라는 점에서, Cloudflare 같은 권위 있는 기관이 공공연히 '사기성 스크래핑'에 대해 공개적으로 비판하는 것은 긍정적임
       이런 논란이 대화에 불을 붙일 수 있다는 점 자체가 의미 있음
       결국 주요 플레이어들이 예전처럼 최소한 ‘룰’은 지키던 검색 시대로 돌아갈 필요가 있음
          + 지금은 ‘부끄러움이 없는 시대’라서, 망신주기가 효과 없다고 생각함
     * 직접 구축한 개인 검색 엔진도 Perplexity 수준의 기능을 어느정도 구현할 수 있음
       지인들끼리 비교해 봤을 때, Perplexity와 거의 반반의 선호를 받음
       엔진이 연구 목적으로 웹페이지 다운로드까진 가능함
       하지만 캡차에 걸리거나 차단당하면 곧장 포기함
       반면 대형 IT기업들은 수십억 벤처투자금을 등에 업고서 뭐든지 할 수 있다고 생각하며, 이런 태도에 분노함
     * “Cloudflare 관리 robots.txt 기능이나 AI 크롤러 차단 규칙을 활용해, 250만개 넘는 웹사이트가 AI 학습 전체 차단을 선택했다”는 주장이 나옴
       하지만 사실은 Cloudflare CEO가 해당 기능을 기본값으로 모든 고객에게 적용한 것임
       AI 추천을 원하거나 트래픽을 중시하는 기업이라면, 해당 옵션을 꺼야 재정적으로 피해를 막음
          + “기본값 적용”은 거짓말임
            내가 직접 Cloudflare 사이트들을 점검해봤는데, 아무런 설정도 안 했을 때 해당 기능이 기본 적용되지 않음
            robots.txt가 없으면 “Cloudflare 관리 robots.txt를 활성화할지 고려하세요”만 나옴
            기존 파일이 있으면 그대로 남고, AI 트래픽 안내도 수동으로 꺼져 있음
          + “AI 추천을 받고 싶으면 설정 꺼야 한다”는 주장에 대해
            콘텐츠 마케팅, 게임화된 SEO, 광고 남발이 Google 검색 품질을 크게 해침
            반면 LLM(대형 언어 모델)은 아직 이런 ‘게임화’가 크게 보이지 않음
            언젠가 LLM도 망가진 검색처럼 변할 수 있겠지만, OpenAI나 Anthropic도 이런 검색 품질 하락이 구글 트래픽 감소의 원림을 인지하길 바람
          + “기본값 적용 주장”은 완전히 거짓임
            실제로는 아무 설정도 안 해도 해당 기능에 자동으로 ‘가입’되지 않음
            심지어 이 주장이 맞던 시기도 지금은 아니고, 원래부터 사실과 달랐음
"
"https://news.hada.io/topic?id=22317","멀티프로세서 프로그래밍의 예술 2판 북클럽","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        멀티프로세서 프로그래밍의 예술 2판 북클럽

     * Software Internals Book Club에서 The Art of Multiprocessor Programming 2nd Edition을 다음 읽기 책으로 선정함
     * 책의 각 장별 토론 일정이 정해져 있으며, 이메일을 통한 텍스트 기반 논의 방식을 사용함
     * 참가자는 Google 계정이 필요하며, 포스팅 시 이메일이 공개됨
     * 매주 한 명이 토론을 시작하는 이메일을 보내고, 이후 누구나 자유롭게 논의에 참여 가능함
     * 참가 및 피드백을 위한 신청 양식과 문의 방법이 안내됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

북클럽 개요

     * Software Internals Email Book Club에서 The Art of Multiprocessor Programming 2nd Edition(저자: Herlihy, Shavit, Luchangco, Spear, 2020)으로 다음 독서 활동을 진행함
     * 인터넷 검색 시 무료 PDF는 보통 2008년 1판이므로 반드시 2020년 2판을 확보해야 함

장별 토론 일정

     날짜    토론 주최자 챕터        제목
   8월 16일  Phil   1  소개
   8월 23일  미정     2  상호 배제
   8월 30일  미정     3  동시 객체
   9월 6일   미정     4  공유 메모리의 기반
   9월 13일  미정     5  원초적 동기화 연산의 상대적 힘
   9월 20일  미정     6  합의의 보편성
   9월 27일  미정     7  스핀 락과 경쟁
   10월 4일  미정     8  모니터와 블로킹 동기화
   10월 11일 미정     9  연결 리스트: 락킹의 역할
   10월 18일 미정     10 큐, 메모리 관리, ABA 문제
   10월 25일 미정     11 스택과 소거
   11월 1일  미정     12 카운팅, 정렬, 분산 조정
   11월 8일  미정     13 동시 해싱과 자연 병렬성
   11월 15일 미정     14 스킵리스트와 균형 잡힌 탐색
   11월 22일 미정     15 우선순위 큐
   11월 29일 미정     16 스케줄링과 작업 분배
   12월 6일  미정     17 데이터 병렬성
   12월 13일 미정     18 배리어

논의에 대한 안내

     * 모든 논의는 Google Group을 통해 이루어지며, Google 계정 필요
     * 직접 게시글을 작성하면 이메일이 공개됨
     * 논의는 Zoom이나 Google Hangout 없이, 오로지 텍스트 이메일로만 진행됨
     * 각 장은 해당 날짜 전까지 읽어야 하며, 그 주말에 토론이 시작됨

토론 주최자 역할

     * 주말마다 1명이 토론 시작 이메일을 보냄
     * 꼭 챕터 요약이 아니라도, 자신의 배경, 챕터에서 느낀 인상이나 혼란, 혹은 실제 경험과의 연관성 등을 1~2문단 정도로 작성해 논의를 유도함
     * 이후 누구나 자유롭게 의견을 추가 가능

참가 신청 및 피드백

     * 신청 양식을 통해 참가 가능
     * 질문, 수정, 아이디어 등은 이메일이나 트위터로 연락 가능

        Hacker News 의견

     * 지난번 당신이 주최했던 독서 모임에 참가했던 사람이자, 비슷한 모임을 조직해본 사람으로서 질문함. 이번 독서 모임의 목표가 무엇인지 궁금함. 그 목표를 얼마나 달성하고 있는지요. 예를 들어, 자기 자신에게 독서 습관을 부여하는 것에서부터, 전문가 및 친구 커뮤니티를 만드는 것에 이르기까지 포함임
     * Google Groups를 통해 7번째로 진행하는 읽기 모임임을 알림. 그룹에 약 1800명이 있으며, 각 독서마다 300~800명이 함께함. 데이터베이스 인턴널 관련 책을 주로 읽어왔지만, 이번 책은 대규모 시스템을 다루는 개발자라면 누구에게나 유의미함을 강조함. 이번 독서에 참여하지 않더라도, 전체 북클럽 메일링 리스트(/bookclub.html)에 가입할 것을 추천함. 이유는 겨울에 Designing Data Intensive Applications 2nd Edition을 함께 읽을 계획이기 때문임
          + 각 책의 챕터마다 협업 방식으로 Anki 덱을 만드는 것을 고려해본 적 있는지 궁금함
          + Herlihy가 예전에는 강의 영상을 공개했었음. 대학에서 강의했던 해의 녹화 영상임. 내 컴퓨터 공학 석사 과정 때 이 수업이 있어서 운 좋게 볼 수 있었음. 이 책 덕분에 훌륭한 강의였음
          + 이런 이니셔티브가 있다는 걸 몰랐는데, 지나고 보니 정말 좋은 아이디어임을 느낌. 기술서를 끝까지 읽고자 하는 열정과 에너지를 유지하는 데 큰 도움이 될 것 같음
          + 좋은 아이디어라고 생각함. 이 책은 동시성과 병렬성에 대해 배우기에 훌륭함. 시간만 된다면 참여하겠음. 참고로, 2판에는 “Optimism and manual memory management”와 “Transactional programming”이란 두 개의 추가 챕터가 있음. 혹시 의도적으로 건너뛴 것인지 궁금함
          + 이 클럽이 실제로 어떻게 운영되는지 자세히 설명해 줄 수 있는지 궁금함. 많은 사람들이 관심이 있을 것 같음
     * 2판은 2020년 Morgan Kaufmann 출판사(ISBN: 978-0124159501)에서 출간되었고, Amazon 등 여러 서점에서 구매가 가능함. 2012년 “revised reprint” 1판이 종종 2판과 혼동되니 주의 필요함
     * 방금 가입했음. 이전 책들에 대한 토론 내용을 볼 수 있는 방법이 있는지 궁금함
     * LinkedIn 계정 없이도 가입이 가능한지 궁금함
     * 왜 LinkedIn URL이 필요한지 묻고 싶음. 일부 사람들은 LinkedIn 계정이 없을 수 있음
     * 이 프로젝트가 정말 좋아 보임. 꼭 참여하고 싶은데, 무료로 구할 수 있는 건 2008년과 2012년 The Art of Multiprocessor Programming PDF 뿐임. 2020년 버전 무료 링크가 있는지 궁금함
          + 이 책은 무료가 아님을 상기하고 싶음. 해당 댓글은 아마 저자를 지원하려면 직접 구매하라는 뜻의 부드러운 안내였음
     * 2020년판이 필요하다고 안내되어 있으나, O'Reilly에는 2012년 revised reprint 버전만 보임. 그리고 혹시 가입하면 이번 책 토론에만 참여하게 되는지 궁금함
          + 현재판 링크는 여기임. 목차만 간단하게 비교해 봄. 챕터 1~6은 대체로 이전 판과 동일한 주제와 섹션 제목임. 챕터 7은 약간 재구성됐고 연습문제 섹션이 추가됨. 챕터 8~16도 각 챕터마다 연습문제 섹션이 추가된 점 외에는 다른 차이점이 보이지 않음. 챕터 17이 챕터 18로, 연습문제 섹션이 추가됨. 챕터 18은 챕터 20으로, 여러 추가 섹션이 있음. 따라서 이전 판에는 17장 “Data parallelism”과 19장 “Optimism and manual memory management”가 없음. 독서그룹이 1~18장을 다루므로 누락되는 17장만 실제 영향이 있음
          + 페이지에 ISBN을 포함해 두었음: 9780124159501. 이번 모임은 해당 책만을 위한 토론임. 전체적인 메일링 리스트는 /bookclub.html에 있음. 그 리스트로 추후 읽을 책 선정과 투표, 새 모임 소식을 계속 알릴 예정임
"
"https://news.hada.io/topic?id=22318","Show GN: Three.js 기반 3D 타이포 생성기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Show GN: Three.js 기반 3D 타이포 생성기

   3D 타이포 생성기 입니다. 여러가지 3D 텍스트 효과를 선택해서 이미지로 내보낼 수 있습니다.

   Demo: https://glass.fleet.im/
"
"https://news.hada.io/topic?id=22242","초고속 게임 스트리밍 비디오 코덱 PyroWave를 직접 설계해보았음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 초고속 게임 스트리밍 비디오 코덱 PyroWave를 직접 설계해보았음

     * 게임 스트리밍은 매우 낮은 지연 시간이 필수 조건임
     * PyroWave는 모션 예측과 엔트로피 코딩을 제거하여 극한의 속도를 달성함
     * 이산 웨이블릿 변환(DWT) 기반 방식으로 기존 DCT 코덱과 차별화됨
     * 32×32 블록 단위 병렬 처리 및 빠른 레이트 컨트롤 구현으로 GPU에서 인코딩/디코딩이 매우 빠름
     * 품질 평가는 H.264/HEVC/AV1 등과 비교해도 특정 상황에서 충분한 결과를 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

게임 스트리밍의 초저지연 요구와 기존 방식의 한계

     * 최근 게임플레이 스트리밍 수요가 증가함에 따라 네트워크를 통한 한 기기에서 다른 기기로의 실시간 전송이 중요해짐
     * DMA, 렌더링, 인코딩, 전송, 디코딩, 화면 출력까지 각 단계마다 누적되는 지연 시간이 전체 경험에 큰 영향을 미침
     * 전통적인 해결책은 H.264, HEVC, AV1 등 GPU 가속 비디오 코덱을 사용하는 것임
     * 하지만 스트리밍에서는 B-프레임 등 고압축 기술을 사용하지 못해 레이턴시 및 비트레이트 제약이 심해짐

설계 철학: 모션 예측 및 엔트로피 코딩 제거

  모션 예측 제거 – Intra-Only 방식

     * 기존 비디오 코덱의 모션 예측을 제거해 모든 프레임을 개별적으로 취급함
     * 그 결과 비트레이트는 상승하지만, 오류 복원력, 단순성, 품질 일관성 등에서 장점이 있음
     * 디지털 시네마 등에서도 Intra-only 방식 사용 경험이 있음
     * 이로써 인터넷 스트리밍에는 부적합하지만, LAN 등 고대역폭 환경에서 좋은 결과 가능

  엔트로피 코딩 제거

     * 엔트로피 코딩은 GPU 병렬 처리에 불리하여 전면 제외함
     * ASIC 전용이나 특수 장비용으로만 존재하던 영역을 소프트웨어 방식으로 실현
     * FFmpeg에 없는 극저지연 코덱분야를 개척

웨이블릿 변환(DWT)을 활용한 새로운 접근 방식

     * 기존 코덱의 DCT 대신, 이산 웨이블릿 변환(DWT) 을 채택
     * 웨이블릿 변환은 그래픽 프로그래머에게 익숙한 mip-map 구조와 유사
     * 이미지를 여러 대역으로 분리하고, 각 대역별로 양자화를 적용
     * 고주파 대역은 더 강하게 양자화하여 시각적 특성을 최대한 활용함
     * 이 과정은 비율 제어(rate control)와도 연계됨

웨이블릿 기반 코덱의 대표적 아티팩트와 한계

     * JPEG의 블로킹 아티팩트 대신, 웨이블릿은 주로 블러 또는 링잉 현상 발생
     * 최근 게임의 TAA로 인한 블러 효과와 겹쳐 실제로 큰 문제는 아닐 수 있음

고속 비트스트림 패킹 및 병렬화

     * 32×32 계수 블록을 독립적으로 처리하여 패킷 손실 시 국소적 블러로만 영향 제한
     * 8×8, 4×2 하위 블록 구성을 통해 GPU 워크그룹 단위 병렬처리 최적화
     * 비트플레인 인코딩을 사용하되, 복잡한 엔트로피 코딩 없이 원시 비트 데이터 저장
     * SSBO 8-비트 저장 등 GPU 친화 방식으로 메모리 효율 및 처리 속도 극대화

정확하고 빠른 레이트 컨트롤

     * 기존 엔트로피 코딩 방식과 달리, 각 블록별로 생략 비트수를 반복 측정/저장 알맞게 비율 조정
     * 전역적으로 최적의 레이트-디스토션 구간 산출로 CBR을 엄격히 준수
     * 웨이블릿 류 코덱의 강점인 비트플레인별 조기 중단을 소프트웨어로도 달성

실질적인 성능 및 효율

     * 1080p 4:2:0 기준, RX 9070 XT GPU에서 0.13ms에 인코딩/디코딩 완료
     * DWT, 양자화 등 각 처리 과정별 FP16 최적화 활용으로 품질과 속도 트레이드오프 체감
     * 4K 영상도 PCI-e 전송 속도보다 GPU 압축 후 전송이 더 빠르다는 실험 결과 확인
     * 전용 하드웨어 코덱보다도 최대 10배 이상 빠른 속도 실현

품질 평가 및 타 코덱과 비교

     * 비교군: FFmpeg의 GPU 인코더(H.264/HEVC/AV1)에서 Intra-only, CBR, 최소 지연 모드
     * PyroWave는 200Mbps, 60fps 조건에서 육안상 압축 아티팩트가 거의 구별 어려움
     * 다양한 게임 장면에 대해 VMAF, SSIM, PSNR 등 객관적 품질 지표로도 충분한 결과 확보
     * 특정 지표(VMAF 등)는 PyroWave에 다소 후하게 평가됨, 반면 PSNR 등은 내부 정밀도의 영향 노출
     * 이미지에 블러나 저품질 아티팩트가 있지만 현대 게임 비주얼 및 사용 목적상 실사용에 큰 문제 없음

결론

     * PyroWave는 극저지연, 고속 처리가 필요한 로컬 게임 스트리밍 분야에서 혁신적인 대안을 제시함
     * 최신 GPU 아키텍처와 맞물려 병렬 처리 및 CBR 제어에 특화되어 있음
     * 개인적 프로젝트이지만, DIY 초고속 스트리밍 솔루션으로 만족도 높음

        Hacker News 의견

     * BBC에서 개발한 인트라 전용 웨이블릿 기반 초저지연 코덱인 VC-2에 대해 이야기함. 이 코덱은 로열티 없이 사용할 수 있고, 현재로서는 ffmpeg와 공식 BBC 저장소에 CPU 기반 구현만 존재함. 자신의 석사 논문으로 CUDA 가속 버전을 만들 예정임. 작년 GSoC에서 진행된 Vulkan 구현들은 아직 만족스럽지 않음. 사람들이 이 코덱을 꼭 살펴보기를 추천함
          + Vulkan 구현이 왜 부족한지 좀 더 자세히 설명해줄 수 있는지 물어봄. 비난하려는 의도는 아니고 진심으로 궁금함을 밝힘
          + VC-2와 JPEG XS의 화질 차이에 대한 경험을 물어봄. 일반적으로 JPEG XS가 더 높은 시각적 품질을 제공한다고 들었으나, 실제 사용 시의 느낌이 궁금함을 언급함
     * 이 글은 신호 특성에 맞는 왜곡 허용과 트레이드오프를 잘 매칭해서 설명한 훌륭한 예시임. 코덱을 설계하지 않고 선택하는 입장이라도 이 과정을 따라가면 좋은 결과를 얻을 수 있음. 극저지연이 중요한 분야에 관심 있다면, VSF가 여러 대안 코덱들의 특징을 정리한 리포트(링크)를 참고하면 유용함
     * 나는 비디오 인코딩에 거의 아는 게 없지만, 인코더가 게임 엔진과 조금이라도 협력하면 비디오게임 스트리밍에서 놓치고 있는 많은 실용적 접근이 있을 거라 생각함. 예를 들어, 대부분의 렌더링 엔진에는 자체 용도의 모션 예측 버퍼가 이미 있는데, 이걸 인코딩용으로도 무료로 쓸 수 있음. 하지만 발명을 막는 특허가 있을 것 같아 실제로는 어렵지 않을지 아쉬움을 전함
          + H.264의 ‘모션 벡터’는 비트 단위 이미지 압축 기법일 뿐, 실제 3D 게임에서 사용하는 모션 벡터와 다름을 강조함. 3D 게임의 모션 벡터는 3D 공간 내에서 오브젝트 위치 변화량이고, H.264에서는 임의의 이전 프레임에서 픽셀 블록을 복사해 차이를 JPEG 방식으로 인코딩하는 방식임. 이 블록 복사 때문에 대역폭이 부족하면 H.264 영상이 네모난 조각들로 망가져 보이게 됨을 설명함
          + 네트워크 지연이 2프레임 있는 FPS 게임을 예로 들어, 게임 엔진이 UI와 3D 월드 뷰를 별도 프레임버퍼로 제공하면, 클라이언트에서 마우스 입력을 받자마자 서버에서 받은 이전 프레임에 월드 뷰만 미리 이동시킬 수 있음. VR 게임들은 이미 이런 식으로 입력 지연을 보완하고 있음. 완벽하진 않지만 큰 차이를 만들어내며, 패럴럭스도 깊이 맵을 활용한다면 어느 정도 가상으로 구현 가능함
          + 센서 기반 비디오 인코딩처럼 폰의 가속도계나 디지털 나침반 정보를 활용해 인코딩 힌트를 줄 수 있음(링크). 2D 게임의 경우 배경과 대형 전경 오브젝트의 모션 벡터를 정확하게 제공할 수 있음. 오버레이, HUD, 점수판, 자막 등 2D 요소는 별도 압축 방식으로 전송해 픽셀 선명도를 높일 수 있음. HN에서는 이런 아이디어에 회의적인 사람이 많아 의외임을 밝힘
          + 나도 이 부분을 늘 궁금하게 생각함. 클라이언트가 약간의 컴포지팅은 직접 처리할 수 있다고 생각함. 예를 들어, 배경과 전경을 다른 주기로 렌더링하거나, HUD는 우선순위에 따라 명확한 코덱을 사용하는 방식임. Stadia는 자체 제작 게임이 있으면서도 단순 영상 스트리밍 방식이었다는 점이 늘 놀라웠음. 어쩌면 다양한 시도를 했지만 기존 비디오 코덱 대비 충분한 이득을 못 받았을 수 있다고 추측함
          + 2D 스프라이트 게임이라면 인코더에 매우 정확한 모션 벡터를 쉽게 제공할 수 있음. 3D 렌더링 게임은 3D 오브젝트에 대한 모션 벡터를 2D 인코딩에 맞게 변환하는 작업이 현실적으로 도움이 될지 확신이 없음
     * LLM(대형 언어 모델)로 게임 내 상황을 매 프레임마다 몇 문장으로 요약해서 네트워크로 전송하고, 수신 측 LLM이 그 텍스트로 프레임을 재구성하는 접근을 제안함. 실시간은 어렵고 손실도 크지만, 압축률은 엄청나고 최신 트렌드에 부합함을 언급함
          + 프레임1 예시로, “당신은 서쪽 들판에 서있으며, 흰색 집 앞문은 널빤지로 막혀있다. 여기에는 작은 우체통이 있다”와 같이 설명할 수 있음을 보여줌
          + 블록체인을 이용해 이런 설명을 전송하면 변경 불가한 기록도 만들 수 있음을 덧붙임
          + 언젠가 게임이 각 사용자 컴퓨터에서 직접 실행되는 시대가 올 수도 있음을 기대함
          + 위 아이디어가 흥미롭다고 밝힘
     * 이 코덱 방식이 내가 연구 프로젝트에 활용하려던 것과 거의 일치함. 참고로, 상용 프로젝트에는 특허풀 이슈 때문에 유료 표준인 JPEG-XS(링크1, 링크2)도 초저지연을 주장하기에 더 안전한 선택일 수 있음을 안내함
          + JPEG-XS는 초저지연에 강점이 있지만 대역폭을 더 많이 사용함. 우리는 영화/TV 후반 작업 실시간 이미지 스트리밍에 활용 중임(사례 링크). IntoPIX CUDA 인코더/디코더와 SRT 저지연 전송 방식을 씀. 쾌적한 네트워크상에서는 전체 지연 16ms 이내 달성이 충분히 가능함. 데이터센터와 도심의 후반 작업실, 또는 국가 간 1GbE 회선에서도 여러 압축률로 사용 사례가 있음
          + 특허풀은 안전망이 아님을 밝힘. 이는 마치 특허 브리지를 건너려면 돈을 내야 하지만, 그 이후에 또 다른 특허 문제가 생기면 추가로 지불해야 하는 구조임을 설명함
     * VLC 창립자가 초저지연 스트리밍 프로토콜을 개발 중이라며, 인터뷰(링크)와 데모 영상(링크)을 공유함
          + 이 분야 경력을 바탕으로, 하드웨어 인코더와 H.264의 지연이 매우 낮다고 강조함. NVENC는 부가 기능(다중 프레임 예측, B-프레임 등)을 꺼두면 거의 즉시 인코딩이 가능함. 고급 처리 알고리즘이나 여러 프레임 대기를 요구하는 방식만 피하면, GPU 렌더링 종료 직후 <10ms 내 인코딩이 가능함을 설명함
          + 링크 영상이 현재는 시청 불가임을 언급함
     * 이 CODEC이 HTJ2K(High-Throughput JPEG 2000)와 같은 알고리즘 기반임을 인지하고, 혹시 작성자가 본다면 HTJ2K와의 차이점에 대해 설명해주면 흥미로울 것 같음을 밝힘
     * 로컬 네트워크 스트리밍만 집중한다면 최신 코덱의 많은 기능은 필요 없음을 설명함. 대역폭만 100Mbps 정도 확보된다면 처리 속도가 빠르고 지연도 적게 만들 수 있음. 예시로 Microsoft DXT 코덱은 최신 코덱 기능은 거의 없지만(엔트로피 인코딩, 모션 보상, 디블로킹 없음), 4~8배 압축률에 하드웨어 디코딩이 가능해 전체 지연 시간 단축에 유리함을 밝힘. 단, 최적화한다 해도 디스플레이 자체에서 30~100ms의 추가 지연이 생기는 점을 지적함
     * 개발 결과가 정말 놀랍고, 언젠가 Moonlight나 비슷한 프로젝트에 적용되는 날을 기대함을 전함
          + 본인도 같은 생각임을 밝히며, 시간과 실력이 있었다면 직접 이 코덱 지원을 Moonlight에 추가해보고 싶었음. LAN에서 Sunshine/Moonlight로 Clair Obscure 같은 게임을 스트리밍할 때 충분한 저지연이 꼭 필요함을 언급함
     * 이 코덱이 아직 생소하고 전문적인 분야라 경쟁 코덱과 비교 자료를 찾기 어렵다는 의견을 인용하면서, H.264/AVC(제로-지연 ffmpeg 옵션)와 JPEG XS와의 벤치마크가 궁금함을 밝힘. ffmpeg 명령 예시와 세부 인코딩 파라미터까지 공유해줌
"
"https://news.hada.io/topic?id=22276","수학은 유령이 있다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               수학은 유령이 있다

     * Lean은 수학을 공식화할 수 있도록 설계된 프로그래밍 언어로, 수학자들이 수학 정리를 코드처럼 다루는 것을 가능하게 함
     * 사용자는 정리, 증명, 공리 등을 코드 형태로 작성하며, 증명 과정은 tactic이라는 명령어 집합을 통해 진행함
     * 증명이 실제로 완성되지 않아도 sorry로 임시 마감할 수 있으나, 이는 TypeScript의 any와 유사한 허위 증명임
     * 공리를 잘못 추가할 경우(예: 2 = 3), 논리적 모순 및 모든 주장 증명 가능성이라는 위험이 발생함
     * Lean은 논리적으로 선택한 공리와 증명 체계 위에서만 결론을 산출하므로, 수학적 타당성 유지가 중요한 의미를 가짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Lean: 수학을 코드로 다루는 언어

     * Lean은 공식화된 수학을 작성하는 데 특화된 프로그래밍 언어임
     * 수학자들은 Lean을 통해 수학을 코드로 표현하고, 서로의 정리 및 증명을 구조화하여 협업 및 공유 가능함
     * 앞으로 인류의 상당한 수학 지식이 코드 형태로 기계적 검증 및 조합 가능하게 되는 미래를 제시함

Lean 증명의 첫걸음

     * theorem two_eq_two : 2 = 2 := by sorry 형태로 Lean에서 간단한 정리 선언 가능
     * 증명이 미완성일 때는 sorry를 넣지만, 이는 임시 방편일 뿐 실제 증명이 아님
          + sorry는 Lean의 증명 검증을 통과시키지만, 논리적으로는 신뢰할 수 없음
     * 완전한 증명을 위해서는 rfl(reflexivity)과 같은 tactic을 사용하여, 2 = 2처럼 자명한 등식을 증명함
     * 이미 증명한 내용을 exact 등으로 다른 정리에서 재사용할 수 있어, 모듈성 강조

공리와 모순: 수학이 유령에 씌였을 때

     * 만약 axiom math_is_haunted : 2 = 3과 같은 공리를 추가하면, Lean은 이를 참이라고 간주함
     * 이 공리는 이후 증명 과정에서 활용 가능하며, 실제 수학적으로 말이 안 되는 결론(예: 2 + 2 = 6)마저 증명 가능해짐
     * rewrite tactic을 사용해 2를 3으로 치환, rfl로 등식 증명 절차를 마치는 것이 가능함
     * 부적절한 공리로 인해 모순이 유도되면, Lean에서도 모든 명제 증명 가능 상태(논리적 붕괴) 발생
     * 실제로 20세기 초 Russell의 패러독스 등 공리계 내 모순이 수학의 근본적인 고찰로 이어졌음
     * 이렇듯 공리의 선택이 논리 체계의 타당성 유지에 결정적임을 Lean이 잘 보여줌

증명 검증기(proof checker)로서의 Lean

     * 공리가 잘 선정되고 Lean이 논리적으로 올바르다면, 이론적 신뢰성 있는 결론을 제공함
     * 간단한 등식부터 매우 복잡한 정리(예: Fermat’s Last Theorem)까지 모두 같은 원리로 검증함
     * 대형 정리는 하위 구조와 정리의 반복적 증명 축적으로 전체 트리가 완성되는 구조임
     * 예시로, Fermat's Last Theorem을 Lean에서 공식화하는 대규모 프로젝트가 진행 중이며, 최종에는 임시 증명(sorry) 없는 정식 증명 체계가 완성될 예정임

Lean을 배우는 즐거움

     * Lean을 통한 증명은 코딩과 수학의 창의적 결합임
     * 처음엔 간단한 명제를 증명하는 법부터, 점차 복잡하고 깊은 수학을 엄밀하게 쌓는 과정이 중요한 즐거움이 됨
     * 공식 자습서 및 커뮤니티 자료(예: Natural Numbers Game, Mathematics in Lean 등)가 입문에 적합함
     * Lean을 사용하면 직접 논리를 형식화하며, 교묘한 아이디어와 논증의 아름다움을 재발견할 수 있음
     * 이유 없이도, 어떤 부류의 사람들에게는 Lean이 특별한 재미를 준다는 결론으로 마무리됨

        Hacker News 의견

     * 요즘 Lean과 비슷한 시스템(혹은 Lean 자체)을 이용해 뉴스나 논픽션 기사를 다시 써보는 아이디어를 생각 중임, 각 진술을 증명해야 할 정리로 보고 증명에는 인용도 포함시키는 방식임, 예를 들어 “내가 승인한 세 출처가 사실로 주장하면 이건 사실이다” 같은 합성 증명으로 처리할 수도 있음, 그리고 “증명된” 주장을 하이라이트해서 볼 수 있게 문서 전체를 마크업하는 게 가능할 것이라고 생각함, 완벽하진 않지만 요즘 언론이 담당하던 엄격함을 다시 한번 기술로 풀어보는 시도임
          + 자연어 진술을 형식화하는 것은 수많은 난관이 존재하는 영역임, 실제 세계와 상호작용하는 코드를 작성하기 힘든 것과 비슷한 이유 때문임, 우리가 당연하게 여기는 개념들(동일성, 시간, 인과관계 등) 전부를 형식 속에서 세밀하게 다뤄야만 사실들끼리 연결되거나 표현 가능해짐, 그렇지만 이 문제는 정말 흥미로움, OpenCog가 이 분야를 끝까지 밀어붙인 프로젝트였고, 지식표현과 추론(KRR)이란 연구영역도 학계에 따로 존재함, IJCAI 저널도 이와 관련된 연구로 가득함, 그리고 철학자들이 시간/양상/확률 등 다양한 논증을 형식화하려고 쓴 논리들이 많은데, 안타깝게도 이들이 서로 쉽게 결합되지는 않음(아마 최근에 해결하지 않는 한)
          + 뉴스에서 우리가 가져야 할 가장 중요한 신념은 대부분 절대적 진술의 모음으로 증명할 수 있는 것이 아니라고 생각함, 베이즈 확률과 같이 추론 연쇄를 계산하는 툴이 더 적합하다고 봄, 이런 식으로 수치적 추정을 위한 툴을 본 적 있음
          + 수학을 대학에서 들은 후 논픽션 작문이 크게 향상됐다는 경험이 있음, 내 SO(애인)와 여동생이 쓴 에세이를 읽고, 마치 논리적 증명을 보듯 “여기서 C가 B로부터 나온다고 하는데 B가 A에서 나온 이유가 실은 빠져 있고, 그러면 C가 A에서 나온다고 할 수 없다” 같은 식으로 엄격함을 적용했었음, LLM 같은 도구로 이를 프로그램으로 만드는 게 가능해 보이긴 하지만, 환각 현상(사실에 없는 주장 생성)이 있어서 한계가 명확함
          + 조심할 필요가 있음, 이런 접근이 논리적 객관성의 아우라를 본질적으로 아무리 급진적이거나 말이 안 되는 주장에 쉽게 부여해 줄 수 있음, 모던 로직의 아버지 중 한 명인 Gottlob Frege의 정치관을 보면 경고가 될 수도 있음 관련 링크
          + 특정 주제에 대한 전체 논증 구조를 지도처럼 그리는 방법이 더 흥미로울 것 같음, 예를 들어 “신이 존재하는가?” 같은 큰 질문을 시작으로 찬반 논거, 그 반론, 그리고 반반론까지 전부 계층적으로 펼쳐 보는 것임, 각 주장마다 “플라톤이 이런 논증을 했다” 같은 식으로 인용은 근거라기보다 역사적 맥락 제공 용도로 넣는 것임, 승부를 가리는 게 아니라 동일한 논점에서 맴돌지 않도록 논증 지도를 만드는 것이 핵심임
     * 우리가 결국 자명한 진리 몇 개로부터 출발하는 증명 사전을 만들어 그 위에 각종 증명이 논리적으로 쌓이는 구조를 만들고 있다는 건가 궁금함, 그럼 추가 증명은 그냥 이미 있는 증명들의 논리적 조합임, 이걸 Zachtronics 스타일 게임으로 만들어 줬으면 함! Euclidea라는 게임이 삼각법 분야에 이런 느낌을 주는데, 이렇게 논리의 탑을 쌓아가는 컨셉이 너무 매력적임, 순수수학이 바로 이런 건지 순수수학 교수들은 이 논리 사전을 확장하는 것에서 희열을 느끼는 건지 궁금함, 그리고 유명 수학자가 기본 증명 리스트를 만든 게 있었던 걸로 기억하는데 혹시 누구(무엇)이고 뭐라고 부르는지 알려주면 좋겠음, 아마 그것들이 공리(axioms)임
          + 이미 관련 게임이 있는데 완전히 원하는 건 아닐 수도 있음 (그리고 수학 전체를 만드는 게임은 아님), 실제로 해봤는데 꽤 재밌었음, 이 기사에서 언급된 leanprover-community/nng4이 바로 그 예임
          + “이걸 Zachtronics 스타일 게임으로 만들어 달라”에 답하자면, 수학이 바로 그 게임이라고 할 수 있음(약간 농담이지만), 게임 버전도 진짜 재밌을 거라고 생각함, 순수수학이 바로 그런 체계임, 학부 때는 그런 느낌이 맞고, 논문 연구로 올라가면 조금 다름, 게임 느낌을 원하면 Dummit and Foote 같은 추상대수 교재를 찾아보는 것도 추천함, 증명에 게임적 재미가 있음, 유명한 책들은 막히면 온라인에 해설도 있음
          + 유클리드의 공리(axioms)를 말하는 걸 수도 있는데, 점, 선, 평면, 평행선 같은 개념이 정의된 체계임, 평면이 아니라 구 위에서는 이 체계는 깨짐, 혹은 Zermelo-Fraenkel 집합론(ZF/ZFC)을 언급하는 걸 수도 있는데, 현대 수학은 이 위에서 전부 구축됨
          + Bombe라는 게임도 있는데, 마인스위퍼 변형임, 직접 셀을 여는 대신 “어떨 때 플래그를 놓을 수 있다”는 규칙을 만들면서 하는 게임임, 레벨이 높아질수록 래머(보조정리)처럼 규칙이 서로 연쇄됨, 플레이어 실력도 오르면 툴셋 제약을 풀어서 일반화도 가능해짐 게임 링크
          + 수학은 본질적으로 공리에서 출발해 결론을 도출하는 과정임, 물론 그게 다는 아니겠지만 내 수준에서는 그렇게 이해하는 중임
     * 약간 트집잡기는 하지만 two_eq_two 정리가 함수처럼 보인다고 하는 건 이상함, 인자가 없으니 오히려 상수에 더 가까움(물론 상수도 인자 없는 함수임은 알겠음), 아래처럼 x_eq_x를 써서 2_eq_2에서 함수처럼 적용하는 식이 더 설득력 있을 것 같음
theorem x_eq_x (x:nat) : x = x := by
  rfl

theorem 2_eq_2 : 2 = 2 := by
  exact (x_eq_x 2)

       여기서 x_eq_x는 함수처럼 보이고, 2_eq_2에서 실제로 그런 식으로 쓰임
          + 맞는 지적임! 내가 그렇게 안 한 건 Lean의 인자 처리(특히 dependent types 같은 콘셉트 — x를 주면 x = x 증명을 반환) 자체가 내겐 다소 생소하고, 따로 다뤄야 할 주제여서임, 다음 기사에서 다룰 예정임
     * Lean을 배우면서 느낀 어려움은 tactics(rfl 같은)들이 지나치게 포괄적이고, 튜토리얼로도 정확한 의미를 파악하기 어렵다는 점임, 예를 들어 C언어는 비트 단위까지 상태 변화를 추적할 수 있는데, Lean은 뭔가 불분명함, 그리고 rewrite(rw) 택틱 문법도 자연스럽지 않게 느껴짐
          + Coq(이제 Rocq)에서도 tactics 적응이 항상 어려웠음, 예를 들어 “A = B”와 “P(A,A)”가 있을 때 “P(A,B)”로 옮기려 하는데, rewrite가 설명하기 힘든 이유로 먹히지 않았던 경험이 있음(중간 구조 정의의 문제일 것 같음), 반면 Metamath와 set.mm 데이터베이스는 아예 tactic 없이 구체적 추론만으로 증명하게 함(ax-mp 같은 추론 규칙만 사용), 근데 이건 이거대로 쓸 만한 유틸리티 레마를 다 외워야 해서 쉽지 않음
          + 내가 Agda를 더 선호하는 이유 중 하나임, Agda는 사실상 tactic이 거의 없고, Curry-Howard correspondence를 이용해 함수형 프로그래밍 언어로 증명 term을 직접 씀, 대신 추상화와 함수 만들기를 게을리하면 사소한 것도 터무니없이 귀찮아지니 규율이 중요함
          + 적어도 Lean에선 tactics 정의를 ""정의로 이동(go to definition)"" 해가며 내부 동작을 볼 수 있다는 점이 있음, 배울 땐 양이 많아서 버겁지만 결국 전부 살펴볼 수 있음(기초 타입 이론까지 가면 감이 잘 안 잡히지만), 그리고 rewrite 문법이 자연스럽지 않다고 했는데, 그렇다면 자연스러운 rewrite 구문이란 어떤 건지 궁금함
          + 흥미로웠던 점은 tactics가 전부 ""유저 레벨"" 코드로, 증명 핵심(kernel) 바깥에 있다는 사실임, 작고 검증된 커널을 바꾸지 않고 유지하고 싶으니 이치에 맞음, 하지만 이 말은 tactics가 버전업이나 수정될 때 기존 증명이 깨질 수도 있다는 뜻임, 실제로 현실에서 그게 어느 정도 문제인지 궁금함
          + 내 예상과 달리 Lean에선 reflection과 rewrite가 addition보다 더 근본적일 줄 알았음, Lean은 addition은 기본 제공하지만 rfl이나 rewrite를 매번 써줘야 하는 것 같음, 아마 Lean에는 prelude 같은 게 있어서 이걸 자동으로 해주는 버전이 있을지도 모르겠음
     * Lean에서 proof를 비대화식 noninteractive하게 읽는 방법이 있는지 궁금함, natural number game을 하다 보니 proof가 ""rw [x]"" 명령 나열로만 돼서 읽기 너무 힘들었음, 에디터로 각 줄 상태는 볼 수 있지만 계속 클릭해야 하니 흐름이 깨짐, 파이썬도 들여쓰기 없고 블록 구조만 보고 클릭해야 흐름을 파악해야 한다면 마찬가지일 것임, 내 관점은 게임 초반 제한된 명령어 때문일 수도 있는데, 실제 full Lean 환경에선 이 흐름이 더 괜찮은지 궁금함
          +

     Lean에서 증명을 noninteractive하게 읽는 방법이 있나?
     나도 최근에 궁금해서 찾아봄, lean-in-latex 블로그가 클릭 없이 편집기 밖에서 그 흐름을 따라갈 수 있게 해주는 방법을 제공함, 그리고 Lean 커뮤니티가 이를 어떻게 접근하는지도 볼 수 있음
          + Rocq에는 예전에는 “수학적 증명 언어”라는 게 있었음, 실제 사용 예시는 찾기 힘들지만 요런 느낌임
Lemma foo:
  forall b: bool, b = true -> (if b then 0 else 1) = 0.
proof.
  let b : bool.
  per cases on b.
    suppose it is true. thus thesis.
    suppose it is false. thus thesis.
  end cases.
end proof.
Qed.

            이런 접근은 논문에서의 “수기 증명”처럼 읽히게 만들었음, 근데 거의 안 써서 사라짐, Isabelle의 Isar proof 언어도 비슷하고, 오히려 표준 방식에 가까움, 예시:
lemma ""map f xs = map f ys ==> length xs = length ys""
proof (induct ys arbitrary: xs)
  case Nil thus ?case by simp
next
  case (Cons y ys) note Asm = Cons
  show ?case
  proof (cases xs)
    case Nil
    hence False using Asm(2) by simp
    thus ?thesis ..
  next
    case (Cons x xs’)
    with Asm(2) have ""map f xs’ = map f ys"" by simp
    from Asm(1)[OF this] ‘xs = x#xs’‘ show ?thesis by simp
  qed
qed

            전체 논리 구조와 중간 결과를 명확히 적어두고, 세부 디테일이 중요한 부분에서만 “by ...”로 tactic을 단축해서 쓸 수 있음, Lean에도 이런 게 있는지는 모르겠지만, 적어도 검색 키워드나 Lean 포럼 질문 소재로 참고가 될 수 있음
          + 정말 좋은 질문임! 아직 초보라 완전히 신뢰하긴 어렵지만 내가 느끼는 걸 공유함, Lean을 두어 달 써본 결과, 증명 코드를 읽는 건 프로그래밍 코드를 읽는 것과 다르고, “스캔”하는 느낌에 가까움, 전반적 논증 구조, 어떤 tactic을 쓰는지, 어떤 lemma를 쓰는지에 주목하게 됨, 실제 Lean code 스타일은 새로운 goal마다 들여쓰기, goal 끝나면 다시 내어쓰기함, 그래서 argument의 모양(“shape”)이 중요하게 읽힘, 내 PR 예시 참고, tactic 익숙해지면 “intro”가 있으면 quantifier 진입임을, “constructor”는 goal 분할임을 등 알아챌 수 있게 됨, 결국 tactics는 증명 트리(term tree)를 만들어내는 매크로/DSL임, 나는 증명 코드를 볼 때 트리 조작(조각 나누기, 순서 채우기 등)처럼 느껴짐, 그래도 증명 코드 중간의 assertion을 정확히 알려면 클릭해야 하는 점은 여전히 남음, 좋은
            아이디어를 가진 증명은 논문의 논리 전개처럼 명확하게 읽을 수 있음, 그래서 의도를 전달하려는 사람은 잘 읽히는 이름, 명확한 전개, 작은 lemma 추출, 그리고 가설을 먼저 적고 짧은 증명 코드로 해결하는 식으로 쓰게 됨, 반면 기계적으로는 번거롭지만 수학자 눈엔 분명한 부분은 “golfing”(최단 코딩)으로 처리함, 골프 스타일은 종종 코드가 짧아지는 대신 사람이 직관적으로 아는 부분만 다룸, 요약하면, Lean에서 읽는 구조는 암묵적이지만 더 명확히 할 수 있는 방법도 있고, tactic에 숙련될수록 클릭 없이도 구조를 더 잘 파악하게 됨, lemma 이름만 훑어도 큰 흐름을 알 수 있고, 순서는 쉽게 재구성 가능함
     * 내용이 정말 좋았음, 이런 걸 쉽게 소화 가능한 방식으로 설명할 수 있는 사람이 드물다고 생각함, 전문가가 그냥 지나치는 작은 단계를 전부 보여주는 게 비법임
          + 고마움!
     * 혹시 이 스레드에서 Lean vs idris/coq/agda의 미래에 대한 의견을 들을 수 있을까 궁금함, 지식표현을 공부하려고 하는데 아무거나 파기 전에 커뮤니티 규모나 미래 리스크가 걱정됨, 예전엔 clojure core.logic에 시간 투자했다가 너무 저조한 관심/작은 커뮤니티 문제로 데인 적도 있어서 쉽게 시작을 못 하겠음
          + 실제 경험으론 Lean과 Coq/Rocq가 Idris, Agda보다 훨씬 더 많이 쓰이고, 라이브러리·커뮤니티도 크다고 느낌, Rocq는 주로 프로그램 검증에 많이 쓰이는데, 그건 역사가 오래돼서 그런 거라고 생각하고 특이한 점도 많아서 Lean이 곧 따라잡을 수도 있음, Lean은 수학 정리 증명에 가장 흔히 쓰임, Rocq의 유명 프로젝트로는 CompCert, CertiCoq, sel4가 있고, 실제 항공기 소프트웨어 검증에 쓰는 사례도 있음(정리된 프로젝트 목록 참고), Lean에서는 mathlib(수학 증명 모음), 페르마의 마지막 정리(FLT 증명 프로젝트), PFR 등 대규모 프로젝트가 있음, Idris, Agda는 실제 “현실 세계” 프로젝트가 없다고 알고 있는데 혹시 틀릴 수도 있음, 모두 C++ 혹은 JavaScript 같은 언어나 커뮤니티에 비하면 규모는 매우 작음, 그리고 실제로 프로그램 검증은 매우 느리고 지루한 일임, AI 발전 등
            근본적 변화가 언젠간 올 것 같긴 하지만 그래도 익힌 실력은 그대로 쓸 만함
          + 이 분야에 베팅은 사실상 하지 않는 게 맞다고 생각함, 실제로 대부분의 수학자들이 formalization(형식화)에 큰 관심이 없음, 손으로 쓴 증명과 컴퓨터가 요구하는 엄격 구문 사이 간극이 크기 때문임, 그냥 배우고 실습하는 재미로 접근해야 하고, 미래 전망 면에서는 Lean이 최근 가장 활발한 편이긴 하지만 각자 오래된 사용자층이 있고, 예단은 어렵다고 봄
     * 아무런 테크닉이나 변칙 없이도 그냥 Lean에 랜덤으로 어떤 걸 던지면 Lean이 통과(승인)하는지에 따라 흥미로운 발견이 나올 수 있는지 궁금함, 혹은 이런 걸 자동화 시스템이나 llm으로 돌려서 난해한 증명/이론을 모조리 시도해보고 성공여부를 보는 게 가능한지? 질문이 어색할 수 있지만 프로로그도 겨우 이해하는 수준임
          + 내가 certfied programming을 직업적으로 하는 입장에서 보면, 생성형 AI와 포멀 메서드는 최고의 궁합이라고 생각함, 앞으로 프로그래머를 LLM이 대체할지 여부도 AI가 certified programming과 조합적 추론을 얼마나 잘 하느냐에 달렸다고 봄,

     랜덤으로 던지면 흥미로운 발견이 있나?
     기존 AI는 체커스 같이 경우의 수가 적은 문제에선 잘 됨, 체스는 조금 더 어렵고, 바둑은 머신러닝 없이 비전통 AI론 불가능할 정도임, 포멀 언어는 경우의 수, 탐색 가능한 상태의 수가 상상을 초월할 정도로 많음, 문제의 성질이 명확하면 SMT solver로 brute force가 가능함, 원래 SMT solver와 proof assistant는 다른 포멀 메서드 분과인데 이제는 서로 보완하는 세상이 됨(Sledgehammer, Lean-SMT 참조)
     llm이나 자동 시스템이 임의 증명/이론을 시도하게 한다면?
     이런 쪽은 아직 메인스트림 연구는 아니지만 연구가 많이 시도되고 있음, LLM 붐 이전부터 수년 간 대형 펀더가 있었음, “Learning to Find Proofs and Theorems by Learning to Refine Search Strategies” 같은 과거 시도, DeepSeek-Prover 같은 최근 연구도 있음, 아직 어떻게 학습시키고 미래 가능성이 어디까지인지는 완전히 열려 있음,
     현실의 LLM은 Rocq, Lean 언어에선 아직 미흡함, 그리고 잘못된 답이 나오면 고치기가 매우 고통스러움, 그래도 시간이 지나면 AI 도구의 수준도 크게 오를 거라고 기대함
          + 이건 정말로 활발한 연구·실험 분야임!
            Lean 커뮤니티는 Zulip라는 곳에 많이 모여 있고, Machine-Learning-for-Theorem-Proving 채널에서 다양한 참고 스레드를 확인할 수 있음
     * alphaproof 보고 Lean에 처음 관심 가지게 된 입장에서 소개 글이 너무 좋았음, 혹시 Lean에서 뭘 하고 있는지 소개해 줄 수 있을까?
          + 아직은 그냥 Lean으로 수학을 공부 중임, 구체적으로는 Tao의 Lean 교재를 따라가면서, 연습문제 중 비어있는 sorry 부분을 내가 채워 넣는 방식으로 공부 중임(내 해답들은 여기 있음)
     * Lean에는 불신뢰(proof with ""sorry"")가 쓰이거나 추가공리(axiom)가 계속 붙는 걸 자동으로 막을 수 있는 검증모드가 있는지 궁금함, 예를 들어 “proof가 어떤 방식으로도 sorry를 사용하지 않는다”, “고정된 공리 집합의 증명력에 의존한다”를 확인할 수 있나?
          + 기사 후반에 언급된 #print axioms some_theorem이 해당 예시가 될 수 있지 않을까 싶음, 이걸로 그 증명이 직접 또는 간접적으로 sorry나 검토 안 된 공리에 의존하는지 알 수 있음
          + print axioms로 추가된 공리가 없는지 확인할 수 있음, 그리고 warning이나 error 없이 컴파일되는지도 보면 됨, SafeVerify 유틸리티는 RL 시스템들이 찾아낸 몇몇 트릭도 잡아냄
          + 매크로로도 이게 가능하다는 내용이 여기 있음
"
"https://news.hada.io/topic?id=22271","사람들은 자신들이 느낀 감정을 기준으로 당신을 평가합니다 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 사람들은 자신들이 느낀 감정을 기준으로 당신을 평가합니다 [번역글]

    더 진심으로 보이고 싶다면, 즉시 이렇게 하세요 (To instantly sound more sincere, do this)

      1. 문제의식과 배경

     * 디지털 소통의 한계: 이메일, 채팅 등 글로 소통할 때는 진심이 잘 전달되지 않고, 오해를 사기 쉽다.
     * 진심 전달 실패: 의도와 다르게, “건성으로 답했다”, “관심 없어 보인다”는 인상을 남기기 쉽다.
     * 수신자의 기준: 독자(상대방)는 당신의 ‘의도’보다 자신이 ‘느낀 감정’을 더 중요시한다.
          + (""독자들은 당신이 의도한 감정이 아니라, 실제로 느끼게 만든 감정을 기준으로 당신을 평가합니다."")

      2. 자주 하는 공감의 실수 예시

     * 상황: 상대가 불만/불편을 토로할 때
     * 형식적인 답변:
          + (""답답하셨겠네요. XX는 아직 지원하지 않습니다. 피드백 감사합니다."")
          + → 건조하고, ‘매뉴얼적’으로 느껴져 신뢰와 호감이 떨어짐

      3. 진심을 전달하는 방법 – ‘한 줄 더(One Extra Line)’

     * 해결방법: 공감이나 사과의 말 한 마디에, 한 문장 더 추가하라
         1. 상대방 감정의 이유 구체화
               o (“답답했겠네요” 뒤에, “이 기능이 없다면 여러모로 불편이 많으실 겁니다” 등 추가)
               o 상대의 입장과 감정을 한 번 더 상세히 인정(어필)
         2. 진정성 표현
               o “피드백 감사하다”에서 멈추지 않고, 피드백이 왜 중요한지까지 언급
               o 필요하다면, “이런 문제는 다른 분들도 자주 겪으십니다” 등 실질적인 이해를 보여줌

   예시:
     * Before:
          + “답답한 경험에 대해 안타깝게 생각해요. 이 [내용]은 커스터마이즈할 수 없지만, 검토하겠습니다.”
     * After:
         1. “답답한 경험을 하셨다니 정말 안타깝습니다. XX가 지원되지 않으면 중요한 상황에서 큰 불편을 겪으실 수 있다는 점, 충분히 이해됩니다.”
         2. “현재 커스터마이즈는 어렵지만, 여러 분이 의견을 주셔서 내부 검토 중입니다. 추가로 의견 있으시면 꼭 공유해 주세요.”

      4. ‘한 줄 더’의 효과

     * 감정의 정당성 인정: 상대방이 느끼는 감정을 명시적으로 언급하면 신뢰감 상승
     * 공감의 디테일: 디테일이 살아 있는 공감이 진심을 효과적으로 전달
     * 관계 개선: 반복해서 적용할수록 작은 피드백에도 신뢰와 호감도가 올라감

      5. 핵심 원칙 및 실천 팁

     * “Don’t just say it, show it!” : 말로만 공감하지 말고, 한 줄씩 구체적으로 덧붙이자
          + 상대 입장에서 “왜” 그렇게 느꼈을지 상상해보고, 그 이유를 직접 써준다
     * 적용 방법:
          + 어떤 감정이 발생했는지 → 왜 그 감정이 충분히 이해될 수 있는지 디테일하게 언급
          + 직접 경험하거나 유사 경험을 들어 공감의 깊이를 더하기
"
"https://news.hada.io/topic?id=22326","모던 Node.js 패턴 (2025)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          모던 Node.js 패턴 (2025)

     * Node.js 개발환경이 최근 몇 년간 웹 표준과 높은 호환성, 내장 기능 강화 측면에서 근본적인 변화를 겪음
     * ESM(ES Modules) , node: 접두사, Top-level await 등 최신 모듈 시스템과 비동기 패턴 도입으로 더욱 직관적이고 안전한 코드 작성 가능해짐
     * Fetch API, AbortController, Web Streams 등 기존 외부 라이브러리 의존성이 줄어들고 내장 API로 많은 기능 지원함
     * 테스트 러너, Watch 모드, 환경 파일 지원 등 내장 개발 도구로 작업 편의성과 생산성이 크게 향상됨
     * 권한 제어, 진단 채널, 단일 실행파일 배포까지 가능한 보안 및 배포 인프라 강화로 현대 Node.js는 전문적이고 범용적인 플랫폼으로 진화 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Node.js의 변화와 발전

     * Node.js는 초기의 콜백 위주, CommonJS 중심 구조에서 오늘날 더욱 표준화된 개발환경으로 거듭나고 있음
     * 이러한 변화는 단순한 외형의 변화가 아니라, 서버 사이드 JavaScript의 전체적인 개발 패러다임 변화임

1. 모듈 시스템: ES Modules의 표준화

     * CommonJS는 오랫동안 Node.js에서 사용된 방식이지만, 정적 분석, 트리 쉐이킹 등의 한계와 웹 표준과의 불일치 문제가 있음
     * ESM(ES Modules) 방식이 Node.js의 새로운 표준으로 정착됨
          + import, export 구문 사용
          + 내장 모듈을 명시적으로 구분하기 위해 node: 접두사 도입
               o 예: import { readFile } from 'node:fs/promises'
               o 내장 vs npm 패키지 구분이 명확해짐
     * Top-level await 지원으로, 모듈 최상위에서도 await 사용 가능해짐
          + 즉시 실행 비동기 함수 래퍼를 둘러쓸 필요 없음
          + 코드가 더욱 직선적이고 이해하기 쉬워짐

2. 내장 웹 API: 외부 의존성 감소

     * Fetch API가 Node.js에 내장되어, Axios나 node-fetch 같은 외부 의존성 없이 HTTP 요청 가능함
     * Fetch는 기본적으로 타임아웃과 취소 기능(AbortSignal.timeout()) 지원함
          + 별도의 타임아웃 라이브러리 없이도 일관된 에러 처리 가능
     * AbortController로 파일, 네트워크 등 다양한 비동기 작업에서 취소 패턴 구현 가능
          + 사용자 인터럽트 혹은 시간 초과에 대해 표준화된 방식 제공

3. 내장 테스트: 프로페셔널 테스트 환경

     * 기존 Jest, Mocha 등 외부 프레임워크 필요 없이, Node.js 내장 테스트 러너로 대부분의 요구사항 충족 가능
          + node:test 및 node:assert로 직관적인 테스트 작성
     * 테스트 Watch 모드, 커버리지 리포팅 등 개발 편의 기능이 내장됨
          + 코드를 수정할 때마다 자동으로 테스트 실행
          + Node.js 20 이상에서 실험적 커버리지 기능 제공

4. 진화된 비동기 패턴

     * async/await가 널리 쓰이지만, 현대 Node.js는 병렬 수행과 정교한 에러 핸들링 패턴 활용이 강조됨
          + Promise.all()로 병렬 작업 수행, 단일 try/catch에서 문맥 정보를 포함한 에러 처리
     * AsyncIterator 활용으로 순차적 이벤트 처리 및 흐름 제어가 용이해짐

5. 고급 스트림 기능과 웹 표준 호환

     * 스트림 API가 웹 표준(Streams API)과 호환성을 갖추게 되었음
          + Readable.fromWeb, Readable.toWeb로 Node.js와 브라우저 간 스트림 변환 가능
     * pipeline(Promise 기반) 함수로 직관적이고 안전한 스트림 파이프라인 구축 가능

6. Worker Threads: CPU 집약 작업의 병렬 처리

     * WorkerThreads를 이용해 JS 단일 스레드 한계 극복, 멀티 코어 활용 가능
     * 메인 루프 블로킹 없이 복잡한 계산 또는 대형 데이터 처리 가능

7. 개발 경험의 혁신

     * --watch 플래그로 nodemon 없이 코드 변경 감지 및 자동 재실행
     * --env-file 플래그로 dotenv 불필요, 환경 변수 즉시 사용 가능
     * 개발환경 구성이 단순해지고 빠르게 변함

8. 보안 및 성능 모니터링 내장

     * 실험적 Permission Model로 파일/네트워크 접속 등 애플리케이션 권한 제한 가능
          + 최소 권한 원칙 구현 및 보안 준수에 유리함
     * perf_hooks로 내장 성능 측정, 느린 작업 자동 분석 및 기록 가능

9. 배포와 패키징의 현대화

     * SEA(Single Executable Application) 지원으로 Node.js와 앱을 단일 바이너리로 배포 가능
          + Node.js 없는 환경에서도 손쉽게 배포/설치 가능함

10. 현대적 에러 처리 및 진단

     * 구조화된 에러 클래스로 풍부한 문맥과 진단 정보 포함, 일관된 에러 객체 전달
     * diagnostics_channel로 맞춤형 이벤트 기반 진단 데이터 전송 및 모니터링 자동화

11. 모듈 해석 및 패키지 관리의 발전

     * Import Maps로 내부 경로를 별도의 네임스페이스로 관리
          + 내부 모듈 분리 및 리팩토링 편의성 증가
     * 동적 import로 환경 또는 설정에 따라 런타임 중 코드 로드 및 코드 스플리팅 가능

핵심 정리 및 미래 전망

     * Node.js는 웹 표준 준수, 내장 도구 활용 극대화, 현대적 비동기 패턴 채택이 중요함
     * Worker Threads 등 고성능 병렬 처리와, 진단/보안 기능으로 전문가용 플랫폼으로 발전중임
     * 단일 실행파일 배포 및 모듈 네임스페이스 등 신기능으로 운영 편의성이 크게 늘어남
     * 이러한 패턴들은 기존 코드와 호환되면서 점진적으로 도입 가능함
     * 2025년 이후에도 Node.js는 꾸준히 진화하며, 지금 소개된 이러한 모던 패턴은 미래지향적 애플리케이션의 기반이 될 예정임

   Deno로 project를 만들기 시작하면서 '와 이런 것도 되네 했는데' node.js도 비슷하게 변하는군요.

   오 이제 axios 안쓰고, fetch로 바로 되네

        Hacker News 의견

     * 가장 큰 변화는 ESM이 아니라 Node에 fetch와 AbortController가 내장된 점임, axios나 node-fetch를 제거하고 Lambda 번들의 크기도 줄었고 콜드 스타트 지연도 약 100ms 단축됨, 습관적으로 npm i axios를 쓰는 사람이라면 2025 Node 릴리스가 그만둘 때임
          + API 호출과 유효성 검증을 모두 아우르는 ts-rest를 전체 스택에서 선호함, zod/json schema 기반 라이브러리 중에 가장 가벼우면서도 robust한 타입 안전성을 제공함, HTTP 클라이언트도 원하는 것을 꽂아쓸 수 있고(bun, node엔진에서는 fastify 선택함), 오버헤드가 있지만 타입 안전성을 컴파일 단계로 옮길 수 있다는 점에서 충분히 가치 있는 선택임, 혹시 더 좋은 대안이나 생각 있으신지 궁금함, 찾을 수 있는 한 다 찾아봤는데 ts-rest만이 경량성과 타입 안전성을 모두 챙길 수 있었음
          + fetch 문법과 await response.json 등 추가적인 예외 처리 작업이 그렇게 마음에 들진 않았음, axios 쓸 때가 훨씬 직관적임, 예시 코드에서도 axios는 단순하게 response.data를 처리할 수 있고, fetch는 직접 status 체크한 뒤 JSON을 파싱해야 해서 더 번거로움
          + 라이브러리 저자로서는 오히려 ESM 도입이 훨씬 힘들고 아팠지만 그만큼 가치있는 업그레이드였음, fetch 자체는 훌륭하지만 ESM 덕에 진짜 많은 것을 얻을 수 있었음
          + node fetch가 axios보다 훨씬 쉽고 단순해서 더 좋음, 아직 어떤 분들은 axios를 계속 쓰는지 몰랐음
          + 내장 요청 라이브러리인 Undici가 굉장히 기대됨, undici 공식 사이트 참고
     * 다음과 같이 파일 시스템이나 네트워크 접근 권한을 제한해서 실행할 수 있게 됨
# 파일 시스템 접근 제한 예시
node --experimental-permission \
  --allow-fs-read=./data --allow-fs-write=./logs app.js

# 네트워크 제한 예시
node --experimental-permission \
  --allow-net=api.example.com app.js

       Deno에서 영감을 받은 것 같음, 정말 훌륭한 기능임, Deno 퍼미션 기능 문서
     * chalk나 picocolors를 설치하지 않아도 이제 직접 텍스트 스타일링이 가능해짐
const { styleText } = require('node:util');

       공식 styleText 문서 참고
     * 바로 적용 가능한 여러 가지를 알게 됨
         1. Node에 내장 테스트가 들어와서 jest를 굳이 쓸 필요가 없어짐
         2. Node에 watch 기능도 내장돼서 nodemon도 필요 없어짐
          + 아직도 jest를 선호함, jest-extended를 쓸 수 있기 때문임
          + Node의 내장 테스트 시스템은 퀄리티가 떨어진다고 생각함, 실제로 몇 주 써보면 왜 그런지 알게 되고, 이슈를 제기해도 Node 팀은 별 관심이 없음
     * Matteo Collina에 따르면 node fetch는 내부적으로 undici의 fetch를 사용함, WHATWG 웹 스트림을 만들어야 해서 본질적으로 undici의 request 방식보다 느림,
       언급한 유튜브,
       undici 작동 원리 블로그
          + 궁금한 사람을 위해 벤치마크는 이곳에서 볼 수 있음, 최근 M3 Max 맥북 프로에서 로컬과 네트워크 환경에서 테스트했는데 undici가 로컬에선 최고였지만 네트워크에선 Axios가 더 빠른 결과가 나옴, 이유는 정확히 모르겠지만 지난 1년 반 동안 undici 사용 경험은 뛰어났음, 프로덕션에서도 충분히 안정적으로 쓸 수 있지만, 뛰어난 성능을 최대로 뽑으려면 상황에 맞는 고민이 꼭 필요함
     * Node의 네이티브 typescript 트랜스파일러 덕분에 TS를 쓰는 사람들은 복잡도가 많이 줄어듦
          + 사실 타입만 제거하고 트랜스파일은 아님, TS enum 같은 건 제대로 동작하지 않음
          + 아직 실사용하기엔 부족함, Enum은 신경 안 쓰지만 확장자 없이 로컬 파일 import도 안되고, 생성자에서 class property 정의도 안 됨
          + --experimental-strip-types 플래그도 이제 필요 없음
     * 새로운 기능을 이런 식으로 우연히 알게 되는 경우가 많음, 브라우저 쓸 때처럼 “그건 최신이니까”라는 막연한 느낌이 있음, 예전 C#만 할 때는 새 언어 기능 소개를 읽고 정말 신나했는데 요즘은 다양한 언어를 병행하다 보니 한 언어만 따라가기도 쉽지 않음, 거의 블로그나 주변 영향을 통한 랜덤 학습임
          + Node(V8) 소식에 관심 많아서 2~3개월에 한 번씩 릴리즈 노트 읽고 이런 기능들을 챙김, 때로는 ECMA proposals도 읽어봄, 파이프라인 오퍼레이터가 꼭 들어갔으면 하는 바램임
     * 한동안 Node 생태계와 거리를 뒀다가 다시 보니 정말 흥미로운 신기능이 많아졌음, Deno와 Bun이 시장을 흔들면서 Node 개발진이 한층 분발한 결과라고 봄
     * 점점 Node가 Bun.js, Deno 등과의 경쟁에서 만만치 않은 존재로 변화 중임, 그런 상호 경쟁이 JS 런타임 발전에 긍정적임
          + 변화는 느리지만 확실하고 반가움, 그래도 Bun의 $ shell function이 아직 그리움, JS를 스크립트처럼 쓰는 게 정말 편리해서 서버에 두 런타임을 함께 올리고 싶진 않음
     * 브라우저처럼 Node의 신기능도 두 가지로 나눌 수 있다고 봄
         1. 전혀 새로운 기술
         2. 이미 있는 기능 위에 얹어진 ""멋내기"" 단계의 레이어
            사람들이 어느 쪽에 더 비중을 두는지 보는 것도 흥미로움
          + 누군가에겐 “멋내기 레이어”가 또 누군가에겐 사용성(ergonomics)이 될 수 있음
"
"https://news.hada.io/topic?id=22253","Fast - 빠른 소프트웨어의 힘","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Fast - 빠른 소프트웨어의 힘

     * 소프트웨어 개발에서 빠름(fast)을 요구하는 일은 드물지만, 빠른 소프트웨어는 사용자 행동에 변화를 만들어냄
     * 빠른 배포와 실시간 스트리밍 같은 기술은 업무 효율과 원격 근무를 혁신적으로 향상시킴
     * 느린 소프트웨어는 인지적 마찰을 유발하고, 실제로 사용자의 생산성을 크게 저하시키는 요인임
     * 빠른 소프트웨어는 복잡성을 감추지 않으며, 단순성과 집중력을 보여줌
     * 앞으로 개발 산업에서는 성능과 경험 최적화를 중시하는 흐름이 강해질 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

빠름을 요구하지 않는 소프트웨어 업계

     * 소프트웨어 업계에서는 주로 기능, 가격, 데이터 통합 등을 요구하지만, ‘빠름’을 직접적으로 요구하는 일은 드물음
     * 그러나 빠른 소프트웨어는 사용자 행동 자체를 바꾸는 힘을 가짐
     * 코드를 배포하는 데 초 단위로 줄어들면 개발자의 배포 빈도도 늘어남
     * 인공지능 기반의 코드 자동 완성 기능은 익숙하지 않은 언어 프로토타이핑을 쉽게 만듦
     * 실시간 스트리밍 기술은 원격 근무의 가능성을 열어줌

느린 소프트웨어의 한계

     * 느린 소프트웨어는 우리가 생각하는 것 이상으로 제약을 줌
     * 예로 비행기 WiFi를 사용할 때 큰 성과를 내기 어려운 경험을 할 수 있음
          + Slack 메시지 전송이나 이메일 답변 정도만 가능하고,
          + Google Docs는 제대로 작동하지 않는 경우가 많음
          + 결국은 포기하는 사용 경험이 됨
     * 반면 Instagram 같은 서비스는 일관되게 빠른 경험을 제공함

빠른 소프트웨어의 효과

     * 빠름은 마법적이라는 느낌을 줌
     * 빠른 소프트웨어는 인지적 마찰을 제거하고, Raycast나 Superhuman처럼 예상보다 한발 앞선 반응을 보여줌
     * Superhuman의 100ms 이하 반응 속도와 뛰어난 단축키 지원은 이메일 사용 경험을 혁신함
     * Mercury의 즉시 이체 기능도 느린 은행 거래에 익숙해진 사용자들에게 놀라움을 줌
     * 이러한 도구들의 속도는 명시적으로 칭찬받지 않지만, 사용자들이 마치 마법처럼 느끼는 요인임

빠름과 단순성, 집중력

     * 빠름은 곧 단순함을 의미하며, 현대 소프트웨어 환경에서 점점 더 희귀한 가치임
     * 소프트웨어가 빨라지려면 불필요한 기능을 제거하는 노력이 필요함
     * Linear와 같은 간결한 프로젝트 관리 도구는 Workday, Oracle 등과 같은 엔터프라이즈 앱에 비해 월등히 빠른 사용 경험을 제공함
     * 빠름은 사용자에 대한 존중으로, 불필요한 요소를 철저하게 걸러냈음을 보여줌

빠르게 만들기 위한 숨겨진 노력

     * 빠른 소프트웨어를 만들기 위해서는 복잡한 백엔드 최적화가 필요함
     * Cash App에서는 사용자 여정에 꼭 필요한 단계만 추가하려 노력하여, 복잡함은 내부적으로 처리함
     * Instagram은 사진 업로드 시 캡션 입력과 동시에 업로드를 시작하여 사용자가 즉시 업로드됐다고 느끼게 만들었음
     * 빠름은 단순한 기술적 성취가 아닌, 우선순위와 집중력의 결과임

빠름은 재미와 동기부여

     * 빠른 소프트웨어는 그 자체로 재미와 만족감을 줌
     * 타자 속도(WPM) 측정, 단축키 세팅과 같은 작은 부분에서도 사용자들은 빨라지는 경험을 즐김

빠름의 상대성

     * AI 및 LLM 기반 워크플로우는 전통 방식에 비해 월등히 빠른 경험을 제공함
     * 예를 들어, 6분 만에 LLM에게 리서치를 맡기는 것이 예전과 비교해 10,000배 이상 빠른 생산성을 만들어냄
     * 하지만 아직 AI 앱 개발, 빌드, 배포 과정에서는 이전 소프트웨어 시대에 비해 부족한 점이 많음
     * 현 시점에서는 성능과 경험보다는 새로운 기능에 더욱 집중하고 있음
     * 미래에는 저지연, 인터페이스 디자인, 연결성, 신뢰성 등 최적화를 우선시하는 흐름이 올 것임
     * 그렇게 되면 더 많은 새로운 가능성과 사용자 경험의 진화가 열릴 것임

참고 자료

     * 빠름에 관해 유용한 추가 자료로는 아래 글들이 있음
          + Patrick Collison: https://patrickcollison.com/fast
          + Charlie Marsh: https://notes.crmarsh.com/python-tooling-could-be-much-much-faster
          + Paul Graham: https://www.paulgraham.com/growth.html

        Hacker News 의견

     * YCom에서 HN 인터페이스를 빠르게 잘 유지해줘서 정말 감사한 마음임. Slashdot이 ""모던 UI""라고 해서 인터페이스를 완전히 바꾼 후 엄청난 공백과 스캔하기 힘든 구조로 변해서 바로 떠났던 경험이 있음. 예전엔 매일 읽던 사이트였음
          + 정보 밀도와 원하는 정보를 빠르게 찾는 것이 ""engagement""와는 완전 반대인 개념임. 보통 사이트 머무는 시간 수치 올리려고 일부러 복잡하게 만들어 광고주에게 어필하려는 경우가 많음. 페이지가 일부러 느리게 로드되어 잘못 클릭하게 만들고 ""전환""을 유도함. 실제로 사용자 중심보다는 남을 속이는 쪽이 돈이 되는 현실임
          + HN은 인터넷 연결이 되는지 확인하려고 켜는 사이트임. 온갖 웹 블로트 가운데 정말 빛나는 존재임
          + HN UI도 특히 모바일에서 개선이 필요함. 명암비가 낮고, 버튼 크기가 너무 작아 조작이 불편함, 다크 모드 없음 등 아쉬움 있음. 내가 생각하는 이상적인 UI를 Elm으로, 완전 클라이언트사이드로, HN firebase API로 만든 버전 있음 https://seville.protostome.com/
          + Slashdot이 망한 게 UI 때문은 아니라고 생각함. 진짜 가치는 아주 초기의 수준 높은 SME들이 남겼던 댓글에 있었음. 사이트가 매각되고 나서 수준 낮은 유저들과 안 좋은 운영, 이탈 등으로 내리막길이 시작된 듯함. 아직 사이트가 살아있는 게 신기함
          + orange site(HN)은 여전히 마크다운 링크 태그를 지원하지 않음
     * LLM을 도입한 워크플로우가 실제로는 느린 경우가 많다고 느낌. IDE의 ""리팩터"" 기능은 1초 만에 끝나지만, AI 도우미에 맡기면 30초, ""에이전트"" 방식은 15분이 걸림. 예를 들어 에이전트에게 HTTP 엔드포인트 코드를 맡겼더니 ""하루치 작업을 10분 만에 끝냈다""고 처음엔 생각했으나, 다시 보니 논리가 꼬여 있고, 에러 처리도 어설펐음. 결국 직접 5시간 정도 다시 짜서 테스트도 내 기준보다 더 잘되고 에러 처리도 스스로 하는 것보다 괜찮은 수준까지 완성함. 관련 연구도 있음 https://reddit.com/r/programming/…
          + 이미 이 주제로 몇 번 글을 썼던 경험이 있음. LLM이 벤치마크 점수 쫓는 건 프로그래밍 도구 입장에서는 결 방향이라고 생각함. 내 경험상 꽤 높은 확률로 틀리므로 항상 결과를 점검해야 해서, LLM과 주고받는 시간이 길어지고, 답변이 느려서 차라리 내가 직접 손으로 더 빨리 만들 수 있겠다는 느낌이 자주 듦. 내가 바라는 건 벤치마크 60% 수준이더라도 1초 이내로 바로 응답하는 에이전트임
          + LLM이 내 작업을 진짜로 빠르게 해준 건 코드의 고급 find/replace 개념에 한정됨. 예를 들면 코드 내 여러 곳의 ""XXX 관련 로직""을 뭔가로 바꿔달라는 식의 프롬프트에 꽤 괜찮게 답함. 직접 찾아서 일일이 바꾸는 수고를 많이 덜어줌. 단, 엄청 큰 코드에서는 시도해본 적 없음
          + 상황에 따라 다른 것 같음. 리팩터는 IDE나 랭귀지 서버가 지원하면 훨씬 빠른데, 그 외에는 LLM도 도움이 됨. 예를 들어 어제 URL 정규화 로직을 MVP로 만들었는데, 고객 데이터에서 다양한 형태의 URL이 섞여 있어서 검증 케이스가 많았음. 가장 많이 쓰는 고객 사례를 Claude에 넣어서 ""최소 스패닝 테스트 케이스""를 만들어달라고 하니 5~10초만에 결과가 나왔고, 이를 기반으로 Zed의 Opus 에이전트 기능으로 테스트 파일을 만들고, 케이스를 검토 후 불필요한 부분은 정리, 논리도 좀 더 개선함. 이 방법이 혼자 다 만드는 것보다 훨씬 빨랐음
          + 시니어 작업에서 40~60%는 속도가 빨라진다는 이야기를 온라인/오프라인으로 많이 듣는 중임. 그래도 아직 AI 에이전트만 믿고 만사 태업할 수준은 아닌 느낌임
     * 신입 시절 소프트웨어 엔지니어로서 속도를 높이는 일로 명성을 얻었던 에피소드임. 그때는 알고리즘 지식과 컴파일러 출력을 보는 능력 모두 중요했던 시기였고, Carmak, Abrash가 스타가 되어가던 시대였음. 22살에 큰 다국적 기업 고객 미팅에 처음 참여하게 되었는데, 우리의 제품 속도가 문제라는 지적을 받음. 그 회사 VP가 ""1초 빨라진 것이 우리 연간 이익에 100만 달러를 더하는 셈""이라고 직접 말하는 걸 듣고 완전 충격을 받았음. 속도에 돈을 이렇게 대입하는 걸 처음 접했던 결정적 순간이었음. 20년이 지난 지금도 경력에서 손꼽히는 하이라이트임. 그리고 또 다른 엔지니어가 VP에게 ""혹시 0초까지 줄이면 무한히 돈을 벌 수 있냐""고 농담을 했고, 그 방에서는 웃음이 없었지만 나는 꽤 재밌었다고 생각함
          + 1초에서 0초로, 2초에서 1초로 줄여도 그때마다 100만 달러씩 느는 식 아님? 굳이 무한한 돈이 쌓인다는 논리가 특이함
          + 결국 더 빨라졌는지 궁금함
     * 블로그 글 첫 문장에 공감해서 써보는 반응임. 사용자가 개발자한테 ""빠르게 만들어달라""는 요청을 직접 하진 않지만, 실제로 안 빠르면 믿어주지도 않음. Rust가 Ruby보다 느렸다면 아무도 관심 갖지 않았을 것임. Rust가 ""C++보다 빠르다""라고 해야 주목을 받음. HN에서도 ""빠르다""는 점만 있으면 다들 홀린 듯 열광함. 조금이라도 더 빠르다 하면 바로 주목을 받음
          + HN에 글 쓰는 소수 프로그래머 계층에나 통하는 얘기임. 대부분의 개발자나 일반 유저는 느린 것도 별로 신경 안 쓰고, 그냥 UI가 편하면 됨. 프로 데이터 사이언티스트도 엄청 빠른 커맨드 대신 깔끔한 Github Desktop을 많이 씀
          + 결론이 잘못된 듯함. ""빠름""은 같은 기능 세트를 빠르게 제공할 때 강력한 구분점이 되는 거임. 사람들이 ""X보다 빠른 X""라 하면 몰려가지만, 더 많은 기능이나 더 좋은 UX, 더 싸거나 더 좋은 걸로 옮길 수도 있고, 느리더라도 선택할 여지는 있음
          + 언어나 런타임에서는 빠름이 중요하지만, 프레임워크 쓸 때는 특징, 호환성 등 다른 요소들이 더 중요함. Electron처럼 느려도 사람들이 많이 씀
          + 실제로 (특히 웹) 앱의 상당수가 엄청나게 느린 세상에 살고 있음. 사용자가 뭔가 조작하면 반응이 오기까지 몇 초가 걸리는 일이 비일비재함. ""빠른 게 최고""라 해도 현실은 반대임
          + HN 유저들이 '빠름'을 좋아하는 이유는, 우리가 현재 접하는 대부분의 테크가 너무 느리다고 (그리고 본질적으로 더 빠르게 만들 수 있다는 걸) 알고 있기 때문임
     * 반대로, 뭔가가 ""너무"" 빠르면 진짜로 동작한 게 맞는지 의심하는 현상도 있음. TurboTax 사례에서 실제 분석은 1초도 안 걸리는데, 일부러 가짜 로딩 스크린을 만들어 줬더니 사람들이 더 신뢰하고 좋아함. 덕분에 실제 처리 시간은 훨씬 짧아도 '정말 검토했나'라는 신뢰를 위해 느려 보이게 만들었던 일화임
     * 빠름은 비용 측면에서도 중요함. 클라우드에서 초 단위로 돈을 내는 구조에서는, 모든 과정을 최적화하지 않으면 저렴한 전사 서비스 제공이 불가능함. 예를 들어, 내가 만든 이미지가 오픈 소스 다음으로 2.5배나 더 작아졌는데, 그 덕분에 더 빠른 콜드 부팅, 더 낮은 비용, 더 좋은 서비스로 이어짐 https://speechischeap.com
          + S3가 느릴까 빠를까? 실제로는 둘 다임. 단일 리퀘스트로 보면 느리지만, 병렬로 여러 리퀘스트를 쏘면 빠르게 느껴지게 됨. 결국 '빠름'은 때로는 생존에 중요하고, 때로는 하나의 미학임
          + 나는 반대로, 컨슈머 하드웨어에서 서비스를 돌려서 클라우드보다 훨씬 저렴하게 운영함. 이미지 사이즈 걱정이 필요 없음 (베어메탈이 더 빠름). 분당 2만 분짜리 전사도 무료로 제공 중임 (요청 5초 제한 기준). 관련 오픈 소스 및 크로스플랫폼 대안도 개발 중임 https://geppetto.app https://github.com/Mozilla-Ocho/llamafile/tree/main/whisper.cpp https://github.com/cjpais/whisperfile https://handy.computer 전사 서비스 혁신에 관심 있으면 연락 환영임
          + PAPER 같은 도구를 (최소한 리눅스 기준) 전체 설치 사이즈 2MB 이내(캐시까지 포함해도)로 맞추길 기대함. pip이 10~15MB, pipx는 그보다 크고, uv는 35MB임. 이보다 작게 가려고 노력 중임
          + 빠르다고 해서 모두 가볍고 효율적임을 의미하지 않음. 종종 비용 많이 드는 하드웨어를 대거 투입해서 빠르게 만드는 경우도 있음
     * 미국 은행 송금에 대한 불만 기사가 보이면 항상 스스로 상기시켜야 함. 영국이나 스위스 등에선 은행 송금이 거의 즉시 끝남. 왜 미국은 이렇게 느린지 궁금함
          + 미국 지역 은행은 자체 프로그래머가 거의 없고, ""코어 프로세서""에 의존함. 그래서 시스템 업그레이드가 매우 느린 편임. Faster ACH 도입도 수 년이 걸렸고, 지역 은행 로비가 자신들에 불리하다는 이유로 도입 연기를 주장함. 자세한 설명이 잘 정리된 블로그 추천함 https://www.bitsaboutmoney.com/archive/community-banking-and-fintech/
          + ACH는 일정 맞추기와 일괄 처리(batch)가 느린 원인임. 송금 자체는 바로 끝나도, 보통 자정에 합산해서 처리함. 이 덕분에 미국에선 Venmo나 Zelle이 인기가 있음. 스위스에서도 IBAN 이체는 느리고, 실시간 결제는 TWINT가 대세임 (QR코드 찍는 방식). 영국 BACS도 같은 이유로 느린 편임
          + 미국에는 실제로 실시간 송금 시스템이 두 개 있음: RTP와 FedNow임. 참여하는 은행이 점점 늘어나고 있음 https://real-timepayments.com/Banks-Real-Time-Payments.html 과거에는 소액 수수료를 내고 크레딧카드 네트워크를 통해 즉시 송금할 수 있었음. 단, 신용카드는 보장과 환불 규칙이 다르고, 사기 발생 시 은행 손해가 더 큼
          + 이게 오히려 소비자에 좋은 점도 있음. 예를 들면, 자동이체로 잔고 없는 계좌에서 돈이 빠져나갈 때 은행에서 여러 번 이메일로 알려줌. 그 덕에 실시간 일어나면 바로 문제인데, 현재는 통지 받고 직접 조치할 시간이 있어서 연체료나 수수료를 피할 수도 있음. 즉시 송금이 아니다 보니 은행이 알리고 내가 대처할 기회를 얻음
          + patio11이 관련 내용을 자세하게 쓴 글 있음 https://www.bitsaboutmoney.com/archive/the-long-shadow-of-checks/
     * 글이 흥미로워 생각할 거리를 많이 던져줬음. 속도가 진짜 체감될 때는 실제 처리량보다는 ""느껴지는 빠름"", 즉 UI 반응성이나 입력 딜레이처럼 편하게 느껴지는 속도가 중요함. 그래서 Go를 Rust보다 더 선호하게 됨. Go는 컴파일 속도가 충분히 빨라서 마치 측정할 필요도 없는 느낌임. 반대로, 느린 것은 아무리 실제 처리량이 빨라봤자 그냥 싫어짐 (Java 스타트업처럼)
          + Go와 Rust 비교해도 컴파일 속도 진짜 중요하게 느껴짐. Rust 빌드는 여러 가지 잡다한 dependency가 많아 프로젝트 구조가 자바스크립트와 비슷해짐. Go는 comparative로 dep가 훨씬 적어서 그 점이 좋음
          + 개발자들이 이런 논쟁을 하는 건 좋은데, 실제로 중요한 건 ""사용자에게 어떤 언어가 더 빠른가""임
     * ""소프트웨어에서 '빠르게 해주세요'는 거의 듣지 않는다""는 얘기와 달리, 내가 일했던 거의 모든 회사에서 페이지 반응 속도와 지연이 최상위 우선순위였음. 스타트업이든 대기업이든, 속도와 레이턴시가 중요한 목표였고, 보통 제품이 얼마나 빠른지, 페이지가 얼마나 빨리 뜨는지, 이상한 지연이 없는지가 항상 중요하게 고려됨
          + 내가 직접 경험한 대부분(8곳 중 6곳) 회사는, 성능 최적화를 위한 시간을 거의 주지 않아 늘 몰래 처리했음. 심지어 레이턴시 측정하며 중요하다고 주장하는 곳도, 막상 기능 추가에 밀려 실제로는 뒷순위로 밀렸음
          + 검색 결과 속도를 집착하듯 개선하면서도, 페이지 렌더링이나 사용자에게 내려보내는 데이터 크기는 크게 신경 안 쓰는 경우가 많았음
     * 여러 직장에서 반복적으로 경험한 사실인데, 대부분 속도의 진짜 가치를 과소평가함. 단순히 ""같은 워크플로우를 더 빨리"" 한다고만 가정해서임. 예를 들어, 밤새 돌리는 대규모 실험을 더 빠르게 만드는 건 큰 도움이 안 된다고 생각하지만, 만약 속도를 획기적으로 올리면 낮에도 수차례 실험을 돌릴 수 있고 이로 인해 완전히 다른 차원의 생산성이 나옴
          + 컨텍스트 스위칭 비용을 사람들이 엄청 과소평가함. 어떤 명령을 30초가 3초로 줄여봐야 하루 10번이면 겨우 5분 줄이기라고 여기지만, 실제로는 전환비용이 훨씬 더 큼
          + CI 파이프라인이 여러 시간 걸릴 때마다, 만약 20분 안에 끝났다면 자잘한 lint 경고들까지도 그때마다 다 수정했을 거라는 생각이 듦
"
"https://news.hada.io/topic?id=22331","아이슬란드어 이름 굴절 패턴을 3.27kB 트라이로 압축하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   아이슬란드어 이름 굴절 패턴을 3.27kB 트라이로 압축하기

     * 아이슬란드어 개인 이름의 굴절 처리는 문맥에 따라 4가지 형태로 변화함
     * 데이터 기반 자바스크립트 라이브러리를 통해 입력된 이름에 대해 적합한 문법적 격을 반환하는 기능 개발
     * 모든 이름을 직접 저장하면 용량 증가와 데이터 누락 문제가 발생하여, 트라이(trie) 구조와 압축 기법을 활용해 해결함
     * 트라이 압축 덕분에 공통 패턴 기반 자동 유추가 가능하고, 데이터의 80% 이상을 커버하는 매우 작은 크기의 데이터베이스 달성함
     * 보통 상황에선 74% 이상의 정확도를 보이며, 공공 부문과 정확성이 중요한 상황엔 별도의 스트릭트(strict) 버전을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제의 배경

     * 아이슬란드어 인터페이스에서 개인 이름을 표시할 때, 굴절(declension) 로 인해 어려움 발생함
     * 아이슬란드어 이름은 주격, 목적격, 여격, 소유격 등 4가지 문법적 격에 따라 다른 형태를 가짐
     * 데이터베이스에는 보통 이름이 주격 형태로 저장되어 문맥상 다른 격이 필요할 때 어려움이 생김
     * 올바른 형태를 쓰지 않으면 네이티브가 아닌 느낌이나 어색함을 줌

데이터 수집 및 정제

     * 아이슬란드는 Árnastofnun이 관리하는 DIM(Database of Icelandic Morphology) 데이터를 오픈함
     * 이름에 대한 굴절 데이터는 Kristín’s Format(K-format) CSV로 가공 가능함
     * DIM 전체 데이터는 700만 행으로 지나치게 방대하나, 공식 승인된 개인 이름(4,500개) 만 추려 3,600여 개에 대해 굴절 정보를 확보 가능함
     * 각 이름에 대해 주격~소유격 형태 배열을 구성할 수 있음

라이브러리 기본 구조

     * 초기 구현은 이름~격 변형 배열로부터 적합한 형태를 반환하는 applyCase 함수로 시작함
     * 그러나 단순 배열 로딩 방식은 용량(30kB gzipped) 이 큼
     * 데이터에 포함되지 않은 이름에 대해서는 대응 불가하다는 한계 있음

중복 제거와 패턴 추출

     * 이름의 4가지 형태 간 공통 접두사를 추출해 각각의 접미사 집합(suffix encoding) 만 저장하여 중복 최소화
     * 같은 굴절 패턴을 따르는 이름들이 많음을 발견함

패턴 매칭을 위한 트라이(trie) 도입

     * 트라이 구조(접미사 기준 역순 삽입) 를 통해 비슷한 패턴을 공유하는 이름군의 값 매핑 최적화
     * 공통 패턴(name endings) 하에 한 번만 굴절 정보를 저장, 새로운 이름에도 높은 예측력 확보

트라이 압축 및 최적화 과정

     * 서브트리의 리프(leaf)마다 값이 같으면 상위 노드에 값을 할당하고 자식들을 삭제해 트리를 압축함
     * 이를 통해 노드 수를 15.4%까지 감소, 용량을 4.01kB까지 축소함
     * 형제 리프 노드 중 값이 동일한 것들을 하나의 노드로 병합하는 2차 압축으로 3.27kB까지 도달함

트라이 성능 및 일반화

     * 새로운 이름 입력 시, 유사 패턴 기반 자동 굴절 가능함
     * 실제로 알려지지 않은 이름들에 74% 옳은 굴절, 26% 오류를 보였고, 실 이용자 기준 오류율은 0.34%에 불과함
     * 데이터의 정규성(regularity)과 포괄성(comprehensiveness) 이 높을수록 압축 및 자동 추론 정확도 상승 효과 있음

실제 라이브러리 및 적용

     * 최종적으로 압축 트라이를 사용하는 beygla 라이브러리로 배포
     * 최소 사이즈(4.46kB)와 더욱 엄격하고 완벽한 맞춤형 strict 모듈(15kB) 로 제공함
     * 공적 문서 등 100% 정확성이 필요한 곳엔 strict 버전, 일반 웹앱엔 경량 버전 선택 가능함

결론 및 확장 가능성

     * 트라이를 활용한 언어 굴절 패턴 데이터 압축은 아이슬란드어 외 여러 굴절 언어의 인명, 주소, 기타 명사 처리의 자동화에 적용 가능함
     * 정규성 높은 데이터와 트라이 압축 조합이 동형굴절 처리 자동화의 데이터/성능 효율 극대화 방안임

참고/감사의 글

     * beygla 개발 과정에서 다양한 전문가 피드백과 최적화가 이루어졌음
     * 트라이의 추가 압축으로 3.43kB → 3.27kB까지 용량 절감함

요약

     * 아이슬란드어 이름 굴절 자동화 문제를 패턴 기반 트라이 데이터 구조로 소형화, 자동화한 사례임
     * 올바른 용량–정확도 트레이드오프를 고려한 실무적 데이터 처리 전략 예시로 시사점이 큼

        Hacker News 의견

     * 고등학교 때 스페인어를 처음 배울 때, Windows용 소프트웨어를 써서 동사 원형과 시제가 쏟아지듯 나오고 그에 맞게 동사 변형을 입력해야 했던 경험이 있음. 이런 훈련 덕분에 문법 규칙이 몸에 배어서 능숙해졌음. 하지만 러시아어를 배울 때는 격 변화가 갑자기 어려워졌고, 비슷한 패턴을 설명하거나 연습할 수 있는 앱을 아무리 찾아도 찾지 못했음. 혹시 이런 용도의 (웹 혹은 macOS/iOS) 앱을 아는 사람이 있는지 궁금함
          + Anki에서 ""KOFI(Konjugation First)""라는 방법을 사용하는 플래시카드 덱이 있음. KOFI는 언어 학습 전에 먼저 모든 활용 패턴을 익히는 방식을 의미함. 프랑스어를 공부한 뒤 활용 실력이 부족해서 나중에 이 방식을 써 봤고, 문법적으로 틀리게 말해도 일상적 소통엔 문제가 없지만, 원하는 수준은 아니었음. 이 방법은 언어를 배우기 전에 모든 활용 패턴을 단기간에 익히는 게 목표임. 언젠가 새로운 언어에는 진지하게 적용해보고 싶음. 프랑스어에 대한 흥미는 줄어들어서 중도 포기했음. 관련 Anki 덱 링크
          + 러시아어를 배우면서 spaCy Python 모듈과 러시아어용 대형 모듈을 조합해서 문맥 기반 표제어화와 문법 태그 추출을 해주는 스크립트를 만든 적이 있음. 그런데 실제로 러시아어 실력이 늘 때는 변화를 논리적으로 해체하려는 시도를 내려놓고, 사용 경험과 반복을 통해 머릿속에 패턴(예외 포함)의 라이브러리를 쌓는 게 훨씬 효과적이었음. 참고로 여기서 말하는 문맥은 문장 내에서의 의미임
          + 25년 전 스페인어를 독학할 때 스페인어/영어 사전을 썼음. 동사 원형에 숫자 인덱스가 붙어서 같은 활용 패턴을 가진 그룹으로 분류되어 있었음. 사전 앞부분엔 각 그룹별 대표 동사의 모든 시제 활용표가 있었음. 불규칙 동사는 별도의 인덱스로, 마찬가지로 비슷한 불규칙 동사끼리 같은 그룹으로 묶여 있었음(예: tener, detener). 모든 동사가 몇십 개의 고유 패턴으로 깔끔하게 정리됨. 이 시스템을 활용한 퀴즈 소프트웨어를 만들 생각도 했지만 결국 만들지 못했음. 기사에서 언급한 reverse-string trie 패턴이 이런 분류 방식에도 쓰일 수 있을지 궁금함
          + 러시아어의 격 변화를 익히기 위해 전치사+형용사+명사의 조합으로 플래시카드를 만들어 암기 속도를 높이려는 아이디어가 있었음. 라틴어를 예전에 먼저 배웠는데, 라틴어 격 변화는 빠르게 외우는 게 기대되지 않지만(수도승이라면 모를까?) 러시아어는 빨리 익히고 싶었음. 하지만 결국 프로젝트로 이어지진 못함
          + 스페인어 활용 연습을 위해 iOS용 ConjuGato를 사용하고 있음. 게임 모드에서는 동사 원형/시제/인칭이 주어지고 활용형을 떠올리는 방식임. 불규칙 동사만 따로 연습할 수 있어 예외를 익히는 데 효과적임
     * 데이터베이스에 격 변화 정보가 누락된 800개 이름의 경우, 직접 수기로 격 변화를 부여하는 게 가장 직관적인 해결책 같음. 원어민이라면 몇 시간 내로 끝낼 수 있고, 완전히 생소한 이름일 경우에도 적어도 명백히 어색하지 않은 형태로 추정할 수 있을 것임. 또는 LLM에게 시키면 아주 저렴하게 할 수 있음. 결과를 이런 trie 구조로 인코딩해 배포하는 건 여전히 좋은 생각임. 다만, trie를 격변화 추정기로까지 쓸 필요는 없음
          + 이름을 더 많이 다루는 게 바람직함—DIM에서는 계속 보완되어야 하는 부분임. 아이슬란드에서는 허가된 이름 리스트에 자주 새로운 이름이 추가되어서 항상 갭이 생길 수 밖에 없음. 나로선 직접 데이터를 추가하기에 확신이 부족하고, 100개의 미확인 이름 결과를 검토할 때마다 “이게 맞나?” 싶은 경우가 종종 있었음. 비슷한 이름을 DIM에서 조회해보고 “나는 그렇게 변화시키진 않을 텐데”라고 여러 번 생각했음. 그래서 DIM 데이터를 언어 전문가가 유지하는 ‘진실의 소스’로 삼음
          + 손작업도 좋지만, 공식 리스트에 없는 이름(외국 이름 등)에는 여전히 한계가 있음. 나도 중앙집중식 이름 리스트가 있는 나라에 살고 있지만, 예외 요청이 가능하고, 리스트가 생기기 전에 태어난 사람이나 이민자 등은 리스트에 이름이 없을 수 있음. 이런 여러 복합 상황에서 ‘대충 적절한 변화형 예측’ 기능이 여전히 유용함
          + LLM이 trie보다 격변화 예측을 잘 한다고 볼 근거는 찾지 못했음(실제 예시가 LLM의 학습 데이터에 들어있지 않다면, 웹 검색이 더 나을 것임)
          + 기존 LLM이 이미 이런 패턴을 학습하고 있는지 궁금증이 생김
     * Rails가 이 문제를 자동으로 처리해주는지 확신은 없지만, 과거엔 이런 마법을 잘 부렸음. 예전에 pluralise의 소스코드를 봤는데, 웨일스어의 불규칙 복수 규칙까지도 다 인코딩 되어 있었음
          + Rails가 정말 좋아서, 웬만한 기능을 위한 메서드가 다 마련되어 있음
     * 한 가지 최적화 아이디어는, trie가 접미사 스트링 자체로 바로 매핑하는 대신, 고유 접미사 배열을 만들어서 trie에서 그 배열의 인덱스로 접근하게 하는 방식임. 예를 들어:
const suffixes = ["",,,"", ""a,u,u,u"", "",,i,s"", "",,,s"", ""i,a,a,a"", ...];

       그리고 다음과 같이 인덱스를 참조함:
var serializedInput = ""{e:{n:{ein:0_r: ...""

          + Claude Code로 직접 해보니 gzip 상태에서 오히려 100바이트 늘고(3456 -> 3556), 압축 전 사이즈만 20% 줄었음. gzip 자체가 반복되는 패턴에 대해 이미 최적화가 잘 되어 있어서 그런 듯함
          + 한 걸음 더 나아가 접미사 그 자체를 trie에 넣고, 동일한 서브트리를 식별해 중복 처리하는 방법도 있을 것임. gzip을 쓸 수 있다면, 접미사 배열을 활용하는 똑똑한 최적화 방법이 분명 있을 것 같음. 바이너리 최적화 포맷을 쓴다면 더 나을 수도 있음
     * 개인적으로 비압축 상태에서 <1kb로 처리할 수 있는 마법 같은 해결책이 있을 것 같다는 생각이 계속 듦. 100% 정확하게 이름을 분류하는 최소화 정규표현식 리스트를 만들어 보는 방법? 아주 큰 bloom filter? 아니면 일반 해시 대신 특화된 feature를 이용하는 방식?
     * 마치 악몽 같은 인터뷰 문제 같음. trie를 뒤집어서(역순으로) 사용하는 일은 평생 딱 한 번쯤 쓸 일인데, 그 한 번 사용하면 마법사 소리 들을 듯함
          + trie를 뒤집은 게 아니라 이름을 역순으로 넣었다는 게 더 정확하게 보임
     * 이런 처리를 JS에서 하기보다는, 데이터베이스에서 모든 name-case 조합을 반환해서, 표시 시점에 필요한 것만 골라 뿌려도 될 것 같음. 즉, 현지화 레이어에서 처리하는 방식임. 교차 언어 상황에서는 어떻게 될지 궁금함. 아이슬란드 UI가 프랑스 이름을 다룰 때는 무조건 주격만 쓸 것 같고, 영어 UI가 아이슬란드 이름을 다룰 때도 마찬가지일 것 같음. 결국 유저를 직접 지정/부르는 문맥이나 관리자 패널(“user x가 user y에게 응답”) 등에서만 필요성이 커질 듯함
     * “idur”, “tur”, “ður”로 끝나는 특정 격변화 패턴의 이름이 88개나 있지만, 동일한 접미사가 항상 같은 격변화 패턴을 따르진 않음. 문제는 단순한 규칙 같으면서도 실제로는 무척 흥미로움. 접미사 패턴이 바로 앞 음절 발음과 관련이 있을까? 만약 미지의 이름을 더 잘 대응하려면 단순히 글자 기반이 아니라 이름의 발음 표현을 NLP로 뽑아 trie 등으로 조회해야 할까 궁금함
          + 이런 고민을 하다 보면 Dependent Types 관련 토론으로 빠질 수 있으니 조심해야 함
          + 예리한 아이디어임. 실제로 동일한 발음의 이름도 격변화 패턴이 다른 경우가 있음. 예를 들어:
               o Ástvaldur -> ur,,i,ar
               o Baldur -> ur,ur,ri,urs “aldur”로 끝난 두 이름이 똑같이 발음되나 격변화 패턴이 다름. “Ástvaldur”의 패턴을 “Baldur”에 적용하면 마지막 세 형태가 정말 어색하게 느껴짐(실제로 아이슬란드인 파트너에게 물어봄). 아이슬란드어는 표기와 발음이 거의 일치하는 편이므로, 발음 기반 trie를 써도 큰 차이는 없을 것 같음
     * beygla/strict 상황에서는 perfect hashing을 대안으로 생각해볼 수 있음
          + 모든 값이 고유하지 않은 상황에서는 일반 perfect hashing보다 더 압축이 가능할 것임. 해시 버킷 하나에 여러 name->suffix 쌍을 집어넣을 수 있음. 다만 이 경우 “처리 불가한 이름” 판별 기능은 없어짐
     * 아이슬란드어 이름 격 변환이 충분히 결정적인 패턴을 가질 정도로 단순해서 이런 방식이 잘 통하는지 놀라움. 언어라는 게 일반적으로는 상당히 복잡한데 말임
          + 아이슬란드는 인구도 적고 언어가 국가에서 적극적으로 관리된다는 점이 작용했을 것임
"
"https://news.hada.io/topic?id=22293","AI는 "천장"이 아니라 "바닥"을 올려주는 기술임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      AI는 ""천장""이 아니라 ""바닥""을 올려주는 기술임

     * AI는 학습 곡선에서 입문자와 중급자의 진입 장벽을 낮추는 역할을 하며, 개개인의 수준에 맞는 맞춤형 지원이 가능함
     * 전문가 수준의 마스터리에 도달하는 것은 여전히 어렵고, AI는 깊이 있는 주제나 논쟁적인 분야에서 한계가 존재함
     * AI를 단순 답변 도구로만 사용하면 실질적 성장 없이 AI의 한계에서 멈추는 부작용이 발생할 수 있음
     * 코딩, 창작, 일상 앱 사용 등 다양한 영역에서 AI의 영향이 다르게 나타나며, 특히 새로운 아이디어와 혁신성이 중요한 분야에서는 AI의 파급력이 제한적임
     * AI가 변화의 바닥선을 올렸지만, 모든 분야에서 큰 변화를 만든 것은 아니며, 각자의 필요와 맥락에 따라 활용 가치가 다르게 평가됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

요약: AI가 바꾼 학습 곡선

     * AI 등장 전에는 각 학습 자료가 특정 대상을 기준으로 제작되어, 학습자의 배경지식을 제대로 반영하지 못하는 한계점이 존재함
     * 예를 들어, 익숙한 분야에서 새로운 주제로 연결해 배우거나, 필요한 선수 지식의 존재조차 모르는 상황, 중급 단계에서 적합한 자료를 찾지 못하는 문제가 흔함
     * 기존에는 기술 습득 과정에서 맞춤형 지원이 어려웠음
     * AI는 개별 학습자의 이해 수준에 맞춰 질문에 직접 답하거나 반복 작업을 대신 수행, 학습 곡선을 변화시킴
     * AI 기반 학습 경험으로, 이제는 어떤 수준에서든 AI가 출발점이 되어주는 바닥(최소 수준) 자체가 올라가는 변화가 발생함

마스터 수준의 한계

     * 분야별 전문가들은 AI의 유효성에 대해 비판적 시각을 가짐
     * AI가 제공하는 정보는 대중적이고 기본적인 내용에서는 강점을 보이지만, 심화·전문 지식이나 논쟁적 주제에 대해서는 한계가 큼
     * AI의 학습 데이터는 일반화된 내용이 많을수록 더 강력한 결과를 내지만, 고난도·진보적 지식에는 훈련 데이터 부족이나 상충된 정보가 많아 정확하고 깊이 있는 답변 제공이 어려움

AI 학습의 부작용: 치팅

     * OpenAI Study Mode 등 정답만 바로 요구하는 기능은, 사용자의 학습정체(plateau)를 심화시킬 수 있음
     * AI 답변을 단순 수단으로 삼는 사용자는, 그 이상 성장하지 못하는 한계를 가짐
     * 장기적으로는 이 방식이 장기적 성장에 불리함

변화된 학습 곡선의 실제 영향

     * 기술 변화는 생태계 전체의 변화를 불러옴
     * AI의 영향력은 제품이나 결과물에 얼마나 높은 마스터리(숙련도)가 필요한지에 따라 달라짐
     * 소프트웨어 개발: 관리자에게는 호재, 대규모 코드베이스에는 제한적
          + 엔지니어링 관리자는 원리와 품질판단력은 있으나, 특정 프레임워크 경험 부족으로 애플리케이션 제작에 어려움이 있었음
          + AI 도구로 인해 기초부터 빠르게 습득하고, 기존 경험을 살려 작동하는 앱을 빠르게 완성하는 사례가 증가함
          + 반면, 대규모·복잡한 코드베이스에서는 AI의 도움 한계가 분명함
               o 기존 시스템의 맥락이나 고유 요구사항에 대한 이해가 부족해, 실제 작업에는 큰 도움이 되지 않음
     * 창작 분야: 경쟁이 치열하여 영향 제한적
          + 창의적 분야에서는 경쟁이 극심하고, 새로운 독창성이 중요
          + AI로 이미지를 쉽게 만들 수 있어도, 진정한 창작 성공의 핵심인 '새로움'이라는 진입장벽은 낮춰지지 않음
          + 인간은 쉽게 파생·모방을 간파하기 때문에, 단시간 유행 이후 금방 관심이 사그라드는 현상이 발생
          + Studio Ghibli 스타일의 아바타 유행처럼 단발적 사례는 있으나 문화적 입지나 대중성에서는 AI가 미치는 영향이 미미함
     * 기존 앱 영역: 영향 최소
          + 이메일, 음식 주문 등은 이미 특화 앱이 잘 구축되어 있음
          + AI 기반 요약 기능이 있어도, 스팸 정리는 이미 자동화되어 있고, 중요한 메일은 직접 확인이 더 신뢰도 높음
          + 음식 주문 등도 이미 세심하게 설계된 UX가 존재해, AI가 더 효과적으로 바꾸기는 어려움

AI 도입의 편차와 미래

     * AI는 지식 노동의 바닥을 올렸지만, 모든 사람에게 같은 영향을 미치지 않음
     * 각자의 기술 수준과 역할, 환경에 따라 AI의 체감 효과에 큰 편차가 존재함
     * 일부는 AI를 통해 혁신을 경험하지만, 다른 이들은 효과를 체감하지 못하거나 오히려 위기감과 혼란을 느낌
     * AI는 아직 모든 방식과 분야에서 '대체 불가능'하지만, 실험할만한 잠재력을 가진 강력한 기술임
     * 만약 개별적으로 AI가 의미 없어 보인다면, 당신의 상황에서는 실질적 변화가 크지 않은 것임

        Hacker News 의견

     * 블로그 글에는 여러 차트가 있어서 객관성과 엄격함이 있는 것처럼 보이지만, 실제로는 느낌과 추측뿐임을 강조함. 최근 실증 연구들은 오히려 AI가 불평등을 악화시키는 방향을 보여줌. 이코노미스트의 그래프와 기사에서 관련 내용을 확인할 수 있음
          + 당연히 AI가 불평등을 심화시킨다는 생각임. AI는 사람들이 경험을 쌓아 올라가는 사다리의 하위 단계를 자동화해서, 미래의 전문가가 될 사람들에게 발판을 제공하지 않고, 이미 정상에 있는 사람들이 투자하여 그 사다리를 더 빠르게 올리는 기술이라고 봄
          + 그래프는 너무 큰 가정을 하고 있는데, 이는 AI 극성주의자의 머릿속에서만 뒷받침되는 느낌임. 특히 '사이드 프로젝트' 관련해서는 애매하게 다루고 있는데, AI가 초보의 입력을 받아서 '충분히 괜찮은' 결과를 만들어 내기에 부족하다고 생각함
          + 글의 요지에 어느 정도 공감함. 과학자처럼 실험실 가운을 입은 듯 위장하지만, 사실 자신들이 어떻게 세상을 보는지에 대한 관계성만 표현했을 뿐임. 그래도 그래프에 '가설'임을 명시하거나, 축 끝을 물결선으로 표현하는 등 시각적 방법으로 가설임을 강조하면 커뮤니케이션 수단으로 가치는 있다고 봄. 곡선의 평탄화(숙련도 정체)가 생긴다고 전제하는 것 역시 확신할 수는 없음. 저자는 오히려 경제학자들의 링크보다 더 선의의 글쓰기를 했다고 생각함. 나는 사람들이 솔직한 의견을 드러내고, 데이터가 있으면 가설 검증까지 보여주면 이상적이라고 봄
          + 그래프에는 불평등 증가를 보여주는 연구가 4개, 불평등 감소를 보여주는 연구가 6개 언급됨
          + 은퇴한 수학자 입장에서, 2025년에 AI에 푹 빠지게 됐고 Claude Max 요금제를 써서 제한 없이 Claude Code Opus 4를 사용하고 있음. 병렬 세션으로 방대한 레거시 코드 베이스를 리뷰하면 사용 한도에 닿기도 함. 한동안 AI에 관한 그 어떤 소통도 기피했으나, 최근에는 HN에서 흥미로운 논의들이 보여 다시 관심 가짐. 내 의견으론 신경다양성(neurodivergent)이 있는 사람들이 AI 사용에 더 성공적인데, 이는 AI가 거대한 연상 엔진이기 때문임. 나는 선형대수 전공이니, AI의 연상 구조와 내 독특한 사고방식이 잘 맞음. 결국 AI는 바닥이 아닌 천장을 올리는 효과를 가져옴
     * Andrew Ng의 최근 AI 스타트업 발표와 비슷한 통찰임. 요즘 창업가들에게 주는 새 조언은, 피벗할 때 프로토타입을 버리고 처음부터 새로 하라는 것임. 이는 본문의 내용과도 연관됨. 프로토타입 개발은 최대 10배 향상되지만, 기존 코드베이스는 30~50% 정도만 개선된다고 함. 이 변화는 과거 VM에서 컨테이너로 옮겨갈 때의 '펫 vs 가축' 비유와 유사함. 코드베이스를 정성스럽게 아끼는 '펫'이 아니라 효율적으로 대하는 '가축'처럼 다뤄야 하는 시대일 수도 있음. 관련 발표 동영상에서 10:30 부분 참고할 수 있음
          + 나의 생각에는 '펫 vs 가축' 비유가 코드에만 집중해서, 진짜 가치는 개발자의 머릿속에 있다는 점을 놓치기 쉬움. AI는 코드 관리는 도와줄 수 있지만, 진정한 가치는 개발자의 이해와 정신적 모델에 있음
          + 좋은 포인트지만, '가축'보다는 '가축(cattle)'이라는 표현을 더 자주 들어봤음. 지리적 차이인지 궁금해짐
          + 이 비유 언급 감사함. 앞으로 생성형 코드를 대규모 클라우드 인프라처럼 다룰 필요가 있을 것 같음. 오랜 세월 써온 레거시 코드에는 덜 적용될 수 있음. 혹시 통찰을 블로그로 정리한 적 있는지 궁금하고, stillpointlab.com 나 @stillpointlab 트위터가 궁금해서 찾아봤으나 자료가 별로 없음
          + ""펫 vs 가축"" 비유가 ""장인 vs 대충 만드는 사람"" 논쟁보다 훨씬 잘 맞는다고 느낌. LLM을 써도 결과물의 가치가 깎이는 게 아니라, 코드에 대한 정서적 애착 대신 실용적으로 바라보는 시각의 전환임
     * LLM으로는 아직 할 수 없는 일이 꽤 있음. 예를 들어 체스를 같이 두다보면 5~10수 정도 지나면 불법 수를 두기 시작하고, 최고의 경우에도 18수 정도가 한계였음. 상대방의 잘못된 움직임도 바로잡지 않으므로 잘못된 학습이 될 수 있음. 결국 실제로 복잡한 문제를 모델링하지 못하니, 사용자가 무엇을 질문해야 할지 인식하는 것이 매우 중요함. LLM은 나이트의 이동 방법이나 유명한 오프닝 정도는 알려줄 수 있지만, 체스 기보 전체를 올바르게 따라가거나, 현 보드 상황에서 최선의 수를 알려주는 것은 불가능함. 많은 사용자가 얼마나 잘못된 답변이 나올 수 있는지 모르는 채로, 자신있게 뱉는 답변을 그대로 믿을 가능성이 큼. 얼핏 보면 단단해 보이지만, 사실은 보이지 않는 크레바스 위를 걷는 것과 같음
          + LLM이 체스에 약한 것은 큰 문제라고 생각하지 않음. 체스 전용 모델은 꽤 괜찮은 수준의 ELO로 합법적인 수만 두게끔 할 수 있음. 포스트 트레이닝이 체스 능력을 저해할 수 있고, OpenAI 같은 곳에서도 크게 신경 쓰지 않는 듯함. LLM으로도 체스 잘 둘 수 있음. 관련 논문 및 평가 예시 참고함
          + 나도 주변에 박사나 의사 같은 전문가들이 LLM이 실수할 거라는 건 아예 상상도 못하는 경우를 자주 봄. 환상적이고 논리적인 문장에 자신감까지 더해지니, 완전 전문가라 착각하는 ‘후광 효과’도 있을 것 같음
          + LLM으로 코드 에이전트 모드에서 작업해보면, 처음엔 잘하다가 점점 엉뚱한 방향으로 흐르기도 하고, 전혀 상관없는 코드 들여쓰기를 시도하는 등 의외의 행동을 많이 목격했음
          + 체스의 경우, 전문 AI는 인간보다 훨씬 뛰어나는데, 반면 범용 LLM은 합법적인 수를 두기도 어렵다는 점이 흥미로움. AI의 천장은 LLM보다 훨씬 높은 위치에 있음
          + ""10수 이상 따라가기 어려움""에 대해, 체스에서는 과거 수보다 현재 보드 상태가 더 중요함. LLM은 불필요한 정보를 거르는 데 약하니, 오히려 보드 상태만 입력하면 성능이 좋아짐. 더 자세한 토론 참고
     * 에이전트가 그린필드 프로젝트에만 좋다면, 기존 코드베이스도 새 기능마다 새로운 그린필드 프로젝트처럼 준비해서, 인턴이 플러그만 꽂으면 동작하도록 해야 함. 나머지 부분은 사람의 손이 필요하고, 그렇지 않으면 인턴이 벽 전체를 다 뜯어버릴 수 있음
          + 말도 안 된다고 생각함. npm의 Y 프로젝트를 WebStorm으로 GitHub에서 받아서 Junie에게 물어보면, 바로 답을 받을 수도 있고, 이해 안 되는 데이터 구조도 예시와 함께 문서화해줌. PR까지 바로 만들 수는 없어도, 페어 프로그래머로 충분히 활용할 수 있음. 오히려 더 많은 테스트를 쓰고 오류 처리도 더 신경 쓰게 되어 최종적으로 더 나은 결과를 만듦
          + 에이전트는 잘하는 것도 많고 못하는 것도 많음. 쓸수록 뭐가 더 잘되는지 헷갈릴 지경임
          + 에이전트가 완전히 새로운 프로젝트 시작은 오히려 별로고, 소~중간 규모 프로젝트에는 매우 좋으며, 크기가 커질수록 점차 효율이 떨어지는 경향이 있다고 생각함. 완전히 새 프로젝트의 경우엔 실제로 써먹을 수 없는 '예제 수준' 코드가 많이 나옴.
     * AI는 보간(interpolation)을 잘하는 도구이고, 외삽(extrapolation)은 못한다는 한줄 요약임
          + 나는 'interpolator'를 'intruder'로 착각해서 읽었음. 그렇다면 'extraloper'라는 단어도 있는지 궁금해짐
     * 본문의 대부분에는 동의하지만, ""AI가 제공할 수 있는 수준에서 실력 향상이 멈춘다""는 것에는 agree하지 않음. 내 경험상 AI를 잘 쓰려면 '고정적'이 아니라 '성장적' 마인드로, 나는 AI의 매니저처럼 역할 놀이하며 출력을 계속 개선함. 어느 정도 한계는 있지만, 직접 해당 스킬을 배우지 않아도, 문제의 경계에 집중해가며 결과의 품질을 상당히 높일 수 있었음. 시간이 흐를수록 현장 전문가가 되지 않고도, 그 분야의 더 나은 매니저로 성장함을 느끼고 있음
          + 본인이 ""어떻게""를 모른다면 품질이 충분히 나아졌다는 걸 무슨 기준으로 아는지 궁금함. 결국 시작 품질과 비교한 상대적 향상일 텐데, 자기 만족만 되면 실제로 품질이 낫지 않아도 상관 없는 걸로 보임
          + 나는 이런 방식이 ""치팅""이라고 보지 않음
     * 학습 곡선에서 AI를 쓰다가도, 절정 근처에 이르면 오히려 AI 없이 배우는 경우가 더 좋아질 것임. 최고의 숙련도는 천천히, 자기주도적으로 배우는 시간을 통해서만 가능함
     * LLM의 가장 큰 장점은 광고나 소셜 미디어에 방해받지 않고, 통일된 포맷으로 정확한 답변을 받을 수 있다는 점임. reddit이나 insta, tvtropes에서 답변을 찾는 것과 정반대임. 생각 도우미로서 완전히 집중할 수 있는 OS와, 자녀가 콘텐츠 함정에 빠지지 않게 도와주는 환경이 빨리 나왔으면 좋겠음. 나는 ad hoc UI에 방해받지 않고 필요한 문서와 정보만 빠르게 받는 게 정말 좋음
          + 나는 AI 답변이 정확하다고 생각하지 않고, 오히려 위험할 정도인 경우가 흔하다고 느낌. 커뮤니티에서는 누가 전문가인지 판단하기 쉽고, 결국 맞는 답변을 얻을 수 있지만, AI 역시 이 데이터를 다 수집했으니 틀린 답을 내놓을 위험도 똑같음. 광고 없는 환경도 얼마 못 갈 것 같음. AI 기업들은 엄청난 적자를 내고 있으니 곧 광고와 소셜 요소가 넘칠 테고, 지금의 무료 혜택은 그냥 고객을 유인하는 손실 보전 전략에 불과함
          + ""정확하다""는 표현이 주관적임을 비꼼
     * 내가 더 잘 아는 AI 분야도 이 흐름과 일치함. 평균 이하인 사람도 AI로 평균에 가까운 결과를 얻을 수 있음
          + 더 많은 지식을 가진 사람이 LLM을 써야 제대로 이득을 본다는 또다른 요약과도 일치함
          + 평균 이상인 사람도 AI를 써서 평범한 결과물을 낼 수 있음. 많은 작업에서 '충분히 괜찮다'의 기준이 오히려 매우 낮음
          + 그래서 여기 있는 사람들이 AI를 반대하는 이유는 다들 평균 이상이라서 그런 것 같다는 농담임
          + ""평균 이하가 AI로 평균 수준의 결과를 내면"" 전체 평균도 그만큼 올라가는 셈임
     * ""AI는 프로토타이핑에 좋고, 엔지니어링에는 좋지 않다""고 요약할 수 있을 것 같음. AI 도구는 속도는 빠르지만 breadth(넓이)와 depth(깊이)가 부족함. 빠르게 PoC나 부분 문제 해결에는 유용하지만, 전체 맥락과 깊이는 부족하고, 진정한 엔지니어링은 구현 이상의 훨씬 더 많은 요소(문맥, 예외, 실패 형태, 깊은 이해 등)가 필요함. 아무리 뛰어난 프로그래머여도 엔지니어가 아닐 수 있고, 최고의 leetcoder가 팀에 실질적으로 도움이 안 될 수도 있음. 진짜 마스터리로 가려면 경험이 필요하고, 그건 미묘한 것들(비직관적 요소)까지 알아야 함. 매니저가 버튼 하나로 제품을 엔지니어링할 수 있는 시대는 오지 않겠음. 현재의 AI 코드 생성기도 '매니저용 자동화'라는 착각에서 출발했고, 오히려 현업 엔지니어의 설명을 바탕으로 동작하는 툴이 더 낫다고 생각함.
       Dijkstra의 ""자연어로 프로그래밍하는 어리석음""에 대한 논평이 여전히 유효하다고 봄. 관련 원문
          + Michael A. Jackson의 Problem Frames Approach, T.S.E Maibaum의 수공학적 수학 기반, Donald Schön의 암묵지(tacit knowledge) 등 기존 사고도 읽어봤음. LLM 기반 프로그래밍은 프로그램 텍스트와 주석에 지나치게 치중된 논의가 많음. 소프트웨어 엔지니어링은 단순히 프로그램 텍스트를 만드는 게 아니고, 응용수학이나 과학에도 국한되지 않는 분야임. AI 에디터에는 이런 암묵지가 많이 내재되어 있다고 보지만, 이런 부분을 좀 더 명확하게 검토하고 논의하는 것이 좋다는 입장임

   메타에선 초지능이 거의 다 된것처럼 설레발 치고 있던데 ㅋㅋㅋ
"
"https://news.hada.io/topic?id=22346","마스터카드는 NSFW 게임 삭제 책임을 회피했으나, Valve는 결제처리사가 마스터카드의 브랜드 훼손 규정을 직접 인용했다고 밝힘","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                마스터카드는 NSFW 게임 삭제 책임을 회피했으나, Valve는 결제처리사가 마스터카드의 브랜드 훼손 규정을 직접 인용했다고 밝힘

     * Steam과 Itch.io에서 NSFW 게임이 삭제된 이유가 결제카드사 압박 때문임이 드러남
     * 마스터카드는 성명에서 법적 구매 모두 허용 입장을 강조하며, 삭제 책임을 부인함
     * 하지만 Valve는 결제처리사가 명확하게 마스터카드의 Rule 5.12.7 규정과 브랜드 훼손 위험을 들어 게임 유통을 거절했음을 밝힘
     * 해당 규정은 노골적이거나 예술적 가치가 없는 콘텐츠 및 브랜드 이미지를 해칠 거래를 제한함
     * 실제로 마스터카드는 직접적으로 연락하지 않았지만, 업계 전방위 압박 존재를 Valve는 인정함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Steam과 Itch.io의 NSFW 게임 삭제 논란

   최근 Valve에서 Steam의 NSFW(성인) 게임이 대량 삭제된 후, Itch.io도 비슷한 조치를 취함. 이들의 결정은 결제카드사로부터의 압박 때문임이 공통적으로 지적됨

마스터카드의 공식 입장 및 책임 회피

     * 마스터카드는 성명을 통해 ""모든 법적 구매는 네트워크 내에서 허용된다"" 고 밝힘
     * 회사 측은 어떠한 게임 평가나 제한을 요구한 사실이 없다고 강조함
     * 네트워크의 역할이 결제 직접 처리나 발급이 아니라, 결제 기술과 네트워크 제공에 한정된다고 설명함

결제 네트워크 구조와 현실

     * 마스터카드는 발급사(merchant bank)나 인수사(acquirer)가 아니며, 결제 시스템을 지원하는 인프라 제공자임
     * Itch.io의 경우, 실제 결제처리사는 Paypal과 Stripe임
     * Stripe 등은 여러 신용카드를 지원하지만, 18+ 콘텐츠 결제 중단 조치를 취한 바 있음

Valve와 결제처리사 간의 이슈

     * Valve는 게임 삭제 전, 직접 마스터카드와 문제 해결을 시도하려고 했으나 직접 소통이 이뤄지지 않음
     * 마스터카드가 결제처리사와 결제 인수은행(merchant acquirer)들과만 소통했고, 결제처리사가 Valve에 마스터카드의 규정 인용 사실을 전달했음
     * 결제처리사가 명확하게 ""마스터카드의 Rule 5.12.7 및 브랜드 훼손 위험""을 명시했음을 Valve는 밝힘

마스터카드의 Rule 5.12.7이 지닌 의미

     * 이 규정은 불법 또는 브랜드 이미지에 심각한 손상을 줄 수 있는 거래를 금지함
     * ""명백히 불쾌하거나 예술적 가치가 없는 콘텐츠, 예를 들어 비동의적 성적 행위, 아동 착취, 신체 훼손, 수간 등""은 거래 제한 대상으로 명시됨
     * 인수사가 관련 컴플레인에 미조치 시, 금전적·비금전적 제재를 받을 수 있음

결론 및 맥락

     * 마스터카드의 입장 표명에도 불구하고 결제 환경 전반의 규제 압박이 실제로 작동 중임을 Valve는 인정함
     * 최근 이 운동이 호주 단체 Collective Shout 등 반포르노 운동의 결과로 촉발됨
     * 커뮤니티의 강한 공론화가 정책 변화에 영향을 미칠 수 있음을 시사함

        Hacker News 의견

     * Mastercard의 공식 입장은 ""우리 네트워크에서 합법적인 모든 거래는 허용함""이라고 아주 명확하게 말함에도, ""Rule 5.12.7""에서는 ""합법이더라도, Mastercard가 판단했을 때 브랜드 이미지를 손상시킬 수 있는 거래는 금지""라고 해서 엄청나게 애매함. 결국, 모든 합법적인 거래가 허용되는 건지 아니면 Mastercard 마음에 들지 않는 거래는 제외되는 건지 헷갈림
          + 미국에서는 의회가 결제 관련 기업을 “common carriers”(공공 운송사업자)로 지정해서, 법원이 공식적으로 금지 명령을 내리지 않는 한 아무 고객이나 합법적인 거래를 차별하거나 취소하지 못하도록 법을 만들 필요성 느낌. 사기·해킹 등 특수 상황은 예외적으로 제한을 둘 수 있도록 문구를 넣을 수 있음
          + 결국, 명확하다기 보다는 복잡하고 자의적인 기준으로 움직이는 옵션임이 분명해짐
          + 공식 발표는 진실에 구속받지 않음. 약관이나 서비스 조항이 실질적으로 더 중요한 기준이 됨
     * Mastercard-Visa 두 회사의 독점이 드디어 이슈화돼서 기쁨. 결제 인프라는 자유경쟁 시장이 아니어서 거절당하면 다른 대안을 쉽게 쓸 수 없음. 이 정도 독점이면, 거래 거절 시 정부 수준의 감시가 필요하다고 생각함
          + 국가(조폐국)가 결제 프로세서를 운영하거나, 우체국이 모든 시민에게 공식 이메일 주소를 만들어줘야 한다고 봄. 이런 기본 인프라는 사기업의 변덕에 휘둘리면 안 됨
          + 큰 주목을 받는 것 같지만, 이 정도로는 의미 있는 변화가 되진 않을 듯함. 미국 정부가 진짜 신경 쓰지 않는 이상 이런 논쟁들은 그냥 인터넷 커뮤니티에서 잠시 불타올랐다가 사라짐. 게다가 Mastercard나 Visa는 보이콧하기 힘듦
          + 정부 입장에서는 “목줄 두 개면 관리하기 쉽다”며 이런 독점 구조를 방치하는 건지, 아니면 기업·정치권의 탐욕과 부패 때문인지 의문임. 독점 방지법을 왜 만들었는지 이제는 다 잊은 듯함
          + 바로 이런 문제를 해결하려고 만든 것이 Bitcoin임
     * HN 게시글 제목의 전체 내용을 공유함: ""Mastercard가 NSFW 게임 퇴출에 대한 책임을 부인했지만, Valve 측은 '브랜드 훼손'을 근거로 한 Mastercard 규정이 직접 인용됐다고 말함""이라는 부분임
          + 결제 네트워크 구조를 설명하면, 단일 기업이 판단해서 게임을 내리라고 Valve에 전달했을 수도 있지만, 실제로 모든 거래는 최대 6개 회사가 중간에서 거쳐가고, 이들 각각이 각자와 전체의 서비스를 따라야 함. 즉, 온라인 비즈니스 입장에서는 “우리 규칙 안 따르면 거래 자체가 불가함.” 그리고 왜 결제가 막혔는지, 어디에서 문제가 됐는지 절대로 공식적으로 안내받을 권리도 없음. 어떤 지점에서 차단됐는지도 알려주지 않고, 이유를 알기 위해 소송할 수도 없음. (상세 구조 설명: 공정한 금융 접근권 관련 글)
          + 이 회로에는 확실히 중간 역할을 하는 많은 기업이 있음. 만약 leafo가 Itch.io 사례를 공유한다면 명확해질 수 있겠지만, 실제로는 Visa/Mastercard 하위에 있는 회사에서 “이 거래는 결제사 쪽에서 허가하지 않을 것”이라고 미리 예측해서 판매 중단 결정을 내린 경우가 많을 거라고 봄. Mastercard는 “아무런 조치도 하지 않았다”고 주장하는데, 그렇다면 이 논란은 어디서 생긴 걸까? 누군가는 뭔가 조치를 한 게 맞음
     * 미국은 정부가 직접적으로 게임 등 표현의 자유를 검열하는 게 불가능하도록 명확한 법이 있음. 그런데, 실체가 불분명한 여러 집단들이 공식 명령 없이도 시스템적으로 원하는 타깃을 차단할 수 있도록 복잡한 구조를 만들어두고 있음. 이번 사례는 호주의 한 사람이 우연히 핵심 인물에게 문제를 제기해서 기계가 작동한 걸로 보임. Mastercard의 공식 입장을 믿는 편이지만, 이들을 계속 견제하는 게 당연하다고 생각함. 왜냐하면 실질적으로 규제자들에게 가장 큰 영향력이 있음
          + 미국 배경지식 공유: Brown v. Entertainment Merchants Association 판결 — 게임도 언론의 자유로 보호됨을 명시한 대법원 판례임
          + 이 현상이 최근에 우연히 벌어진 게 아니라, 이미 수년 동안 지속적으로 Mastercard가 검열을 해왔음. 이번 호주 사례는 단순히 기존 검열 규정을 더 넓은 범위로 적용하라고 압박한 것임
          + (욕설로 TV에서 뭐라고 하라는 코멘트 생략)
          + Mastercard나 Visa는 사기업이기에 네트워크에서 허용하지 않는 것을 자유롭게 정할 수 있음. 법적 표현의 자유 문제와는 다르고, 개인적으로 더 많은 경쟁자를 바라는 입장이지만 미국 내에서도 수십 년간 성인 콘텐츠에 대한 결제 제한은 계속되어왔음. 기업은 리스크를 판단해 제한하며, 특정 카테고리는 여전히 처리 자체가 어렵고 엄격함
     * 이 문제는 신용카드사보다 더 윗단, 바로 투자자(예: Bill Ackman, Blackrock 등) 수준에서 비롯됨. 이 투자자들은 모든 걸 ‘디즈니화’해서 가족 친화적으로 만들고 싶어함. 이들은 부정적인 여론이나 언론 보도가 브랜드 이미지에 영향을 미친다는 걸 너무 신경 써서, 결국 세상을 성, 마약, 록음악 없는 디즈니랜드로 만들고 싶어함. 이런 세계관이 비현실적임을 모르며, 일시적으로 Steam에서 근친물 게임을 규제하는 게 장기적으로 성공할 수 없음을 잘 모름. (참고: 관련 링크)
     * 기사 제목이 아예 잘못 됐다고 느낌. ""Mastercard가 게이머가 많다는 걸 깨닫고, 이미지 관리에 나섬""이라는 제목이 훨씬 적합하다고 생각함
          + 왜 우리가 자유시간에 픽셀로 된 디지털 게임을 하는 걸 누가 신경써야 하는지 모르겠음. Mastercard가 우리가 사적으로 하는 게임까지 금지시키는 건 오버임
     * Mastercard가 NSFW 게임 퇴출에 대해 책임을 회피하려 하지만, Valve는 구체적으로 ""브랜드 훼손"" 규정을 언급했다고 하는 이 상황에 대해, “신용카드 회사가 이미지나 윤리성까지 거래 자체와 연결해서 고민하는가?” 의문을 느낌. Mastercard라는 이름을 들으면 그저 계좌에 연결된 플라스틱 카드 정도로만 생각함
          + 반론을 하자면, 실제로도 예시처럼 비트코인도 ""이상한 거래에 많이 쓰인다""라는 이미지를 갖게 됨. Mastercard도 ""특정 게임 결제에 많이 쓰이는 카드""가 되고 싶지 않은 마음을 이해 못할 건 아님. 결국, 법에 따라 합법적인 거래는 모두 처리하도록 의무를 부여하는 게 필요함
          + 어쩌면 지금은 정말로 신용카드사 이미지를 그렇게 인식하는 세상이 된 것 같기도 함
     * A Streetcar Named Desire 같은 영화에도 강간 장면이 나오는데, 왜 이런 콘텐츠에는 Mastercard 결제가 허용되는지 의문임
          + Game of Thrones 같은 강한 성 표현과 성폭행 장면이 있는 드라마도 모두 스트리밍에서 삭제돼야 하지 않냐는 풍자적 질문 제기함
          + 현재 해당 콘텐츠를 타깃으로 삼는 압력 단체가 없기 때문에 Mastercard는 자체적인 도덕적 신념이 아니라 외부 압력에 따라 움직이는 것임. 규정 적용이 일관되게 이뤄질 거라고 기대하지 말라는 의견임
     * Mastercard 규정 5.12.7에서 불법 거래는 물론, “브랜드 훼손 가능성”이란 기준으로 아무 거래나 금지할 수 있다고 명시되어 있음을 보고, 생각보다 더 명확하게 어떤 거래든 Mastercard가 맘에 안 들면 거부할 수 있구나 하고 놀람
          + 실제로 Mastercard에 두 번 문의했는데, Steam을 언급하지도 않았는데도 두 번 다 상담원이 먼저 Steam 콘텐츠 때문이냐고 예상함. 공식적으로는 “불법 성인 콘텐츠만 제한하고, 법의 기준을 따른다”고 했으며, 브랜드 보호는 얘기하지 않음. “법 기준이냐 Mastercard 자체 기준이냐”라는 질문에는 명확히 답하지 않음
     * 미국에서 달러(USD)를 킹크스토어나 성인용품점에서 받는다고 해서 미국 이미지가 손상된다고 생각하는지 의문임. 결제대행사가 합법적인 거래를 처리하는 것만으로 브랜드 이미지를 훼손하는지는 동의하기 어려움. Mastercard로 항의 메일 보내는 사람들도 있지만, 다들 온갖 일로 불만 메일을 보내기 마련임
          + 나 역시 Mastercard나 Visa의 명성이 Steam 게임 거래 때문에 크게 훼손될 거라 생각하지 않음. 오히려 금융 인프라는 공공재처럼 누구나 제한 없이 쓸 수 있어야 한다는 인식이 그런 기업 이미지를 만듦. 한편, 미국 정부는 현금을 대량 소지하면 부정적으로 보기도 하고, 암호화폐 이미지는 더 나쁨
          + 만약 Mastercard가 공공서비스처럼 중립적으로 모든 합법 거래를 처리해야 한다는 법이 있었다면, 기업 명성에 대한 논란 자체가 안 생겼을 것임. 하지만 지금은 자율적으로 거래를 거부할 수 있기 때문에, 어떤 거래든 ""왜 승인했냐""라는 비난도 피할 수 없음
          + 실제로 문제는 Visa, PayPal, Mastercard가 압력 단체의 집중 항의에 굴복해서 검열을 실행하고, 이후 검열이 이뤄지면 소셜미디어에서 이 단체들이 자축 글을 올리는 현상임. 내 의견에도 공감하지만, 전체적으로 이 상황이 정말 어이없음
"
"https://news.hada.io/topic?id=22339","클로드 AI의 MCP를 시계열 데이터베이스와 연동해 보았습니다. ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  클로드 AI의 MCP를 시계열 데이터베이스와 연동해 보았습니다.

   시계열 데이터베이스를 클로드 AI의 MCP를 통해 연동해 봄.

   결론적으로 실제로 데이터 추출, 분석, 이상 탐지까지 수행할 수 있었음.

   처음에는 이상한 SQL이 생성되지만, 여러번 시도한 끝에 제대로 결과를 내는 클로드의 행동이 매우 인상적이었음.

   마크베이스 시계열 데이터베이스 문법은 클로드가 알 수 없음에도 MCP 소스코드에서 제공하는 URL을 통해 프롬포트를 만드는 few shot 기능을 사용해서 제대로 된 SQL을 생성하는 인상적인 성과를 거둠
"
"https://news.hada.io/topic?id=22275","Kernel - 브라우저 자동화를 위한 서버리스 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Kernel - 브라우저 자동화를 위한 서버리스 플랫폼

     * Kernel은 개발자가 브라우저 자동화 코드를 별도 인프라 고민 없이 바로 배포하고, 대규모로 확장할 수 있게 해주는 서버리스 플랫폼
     * 설정이나 파이프라인 구축 없이, 로컬 개발과 거의 동일한 속도로 코드를 배포하고 실행할 수 있음
     * 샌드박스 환경에서 사용할 수 있는 Chrome 브라우저를 제공하며, 작성한 에이전트를 자동으로 API로 변환, 어디서든 호출 가능
     * Playwright, Puppeteer 등 Chrome DevTools Protocol 기반 프레임워크와의 연동을 지원하며, 원격 GUI(라이브 뷰) 로 실시간 모니터링 및 제어가 가능함
     * Unikraft unikernel 환경 지원으로 초고속 재시작, 스냅샷 복원, 리소스 최소화 등 고성능 특화 기능 제공
     * Docker 이미지 및 Unikraft unikernel 두 가지 배포 방식을 지원, 다양한 클라우드/컨테이너 환경에서 활용 가능함
     * 모든 코드는 격리된 가상 머신에서 안전하게 실행되며, 실시간 관찰 및 디버깅 도구 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

What's Kernel?

     * Kernel은 샌드박스된, 바로 사용할 수 있는 Chrome 브라우저 환경을 제공하며, 이 저장소는 Kernel의 호스팅 서비스를 위한 기반 코드임
     * Playwright, Puppeteer 등 Chrome DevTools 기반 브라우저 프레임워크에서 쉽게 연결 가능

Why use Kernel?

     * 로컬에서 프로덕션까지 수 초 내 배포
          + 별도의 설정이나 프로덕션 파이프라인 없이, bun run dev와 거의 같은 시간에 코드 배포 및 실행 가능
     * 모든 에이전트를 API로 전환
          + 플랫폼에 올린 모든 에이전트가 자동으로 API로 노출되어, 외부에서 호출 가능
     * 병렬 확장성
          + 수천 개 브라우저 인스턴스를 필요할 때마다 즉시 실행 및 확장 가능
     * 격리 및 관찰성 강화
          + 코드가 격리된 VM에서 실행되어 보안성 높고, 모니터링 및 디버깅 도구 제공
     * 예측 가능한 간단한 요금제
          + 사전 인프라 정의 없이, 사용한 리소스만큼만 비용 발생

주요 기능

     * 통합 브라우저 환경 : 클라우드 상에서 브라우저를 즉시 생성·제어, 워크로드 자동화에 최적화
     * 샌드박스 Chrome 브라우저를 DevTools 기반 자동화 프레임워크에서 연결 사용 가능
          + 9222 포트를 통해 Playwright, Puppeteer 등과 연동
          + CDP WebSocket 엔드포인트 획득 후 원격 클라이언트에서 연결
          + 연결 해제/재연결 가능
     * 세션 상태 유지 : 쿠키, 인증 토큰, 히스토리 등 브라우저 세션을 호출 간에도 지속 관리
     * 초고속 재시작(standby mode) : 브라우저 인스턴스를 20ms 이하로 즉시 재시작
     * 원격 GUI(라이브 뷰 스트리밍) 로 브라우저 화면을 실시간으로 확인 및 제어 가능함
          + noVNC: VNC 기반, 읽기/쓰기 지원, WebRTC 비활성화 필요
          + WebRTC: 실시간, 읽기/쓰기, 창 크기 조정, 복사/붙여넣기, 고속 성능, ENABLE_WEBRTC=true 필요
          + 오디오 스트리밍은 미지원, 읽기 전용 모드는 환경 변수로 설정 가능
     * 브라우저 세션 비디오 리플레이 : 과거 세션을 다시 보며 디버깅 및 분석 가능 (지원예정)

구현 및 배포

     * Docker 컨테이너 사용
          + headful Chromium을 Docker 컨테이너에서 실행 가능
          + cd images/chromium-headful 후, 빌드 및 실행 스크립트 제공
          + 환경변수로 WebRTC 활성화 및 기타 설정 가능
     * Unikraft Unikernel 사용
          + Unikraft unikernel 기반 실행 시, Docker 기반보다 더 빠른 시작 및 절전 모드 제공
          + 네트워크 트래픽이 없으면 자동으로 standby 모드 진입, 상태 스냅샷/복원 지원
          + Cold start 20ms 미만, 세션 상태(쿠키, 파일, 브라우저 설정 등) 유지 및 복원 가능
          + 8GB 이상의 메모리 필요
     * 배포 시 참고사항
          + WebRTC 기반 스트리밍 활성화 시 TURN 서버 필요
          + unikernel 배포 시 공개 URL이 발급되어 누구나 접근 가능하므로 민감 작업에 사용 금지, 사용 후 인스턴스 삭제 필요

   이름 이름부터 글러먹었…

   커널을 이기고 구글 1페이지에 나올 자신이 있나봅니다.
"
"https://news.hada.io/topic?id=22303","Telo MT1","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Telo MT1

     * Telo MT1는 MINI Cooper 만한 크기의 전기 미니 트럭
          + 4 Door 5 Seat 이면서, 60인치 베드를 제공
          + 훨씬 큰 Toyota Tacoma 와 비슷한 인테리어 크기 및 같은 베드 사이즈. Rivian R1T 보다도 베드가 넓음
     * 차량에 탑재된 센서와 내장 시스템을 통해 충돌을 미리 감지
     * 에어백과 진보된 구조 기술을 사용해 도로 위 모든 사람의 안전성 향상을 목표로 함
     * 106kWh 의 고성능 배터리로 최대 350마일 주행 거리를 실현함
     * 20분 만에 20%에서 80%까지 고속 충전 지원 기능을 제공

        Hacker News 의견

     * 최근 출시된 픽업트럭들은 2017년 이후부터 점점 커지고 보행자에게 위험도가 높아지는 방향임을 지적함, 실제로 오프로드에서의 유틸리티를 강조하지만 실제로는 멀쩡한 상태로 도시 도로에서 한 명만 태우고 다니는 경우가 다수임, 트럭에 대한 좋은 오버뷰 영상도 추천함 영상1, 영상2
          + 오프로드를 자주 다니는 입장에서 트럭을 세차하고 광내서 항상 멀쩡하게 유지하는 것 역시 일상적임, 진흙을 그대로 두거나 측면을 파손된 채로 둘 거라 기대하면 곤란함, 밑부분에는 돌에 긁힌 자국도 있지만 측면과 전면은 수리비도 많이 들어서 웬만하면 손상 안 나게 관리함, 실제로 현재 판매 중인 대부분 트럭들은 오프로드 전용 트림이 아닌 모델이 더 많고, 공기역학성을 고려한 전면 구조 및 연비를 위한 2WD, 온로드 타이어 등으로 출고됨, 나처럼 재택근무하면서 짐도 싣고 종종 오프로드도 다니는 사람 입장에선 트럭은 충분히 합리적임, 그런데도 트럭 운전자에 관해 비꼬는 말을 듣는 걸 종종 경험함, 어떤 이들은 트럭을 무조건 비합리적 선택으로 여기며 부정적으로 보지만, 정작 그런 이들도 필요할 땐 내게 이삿짐 운반을 부탁하기도 함
          + 트럭은 한 대로 다양한 역할을 할 수 있다는 장점이 있음, 가끔 운전자 혼자 타고 있는 것만 본다 해도 그 외 시간에 어떻게 활용하는지 모르는 경우가 많음, 사람이나 짐 싣기, 장거리 운전 모두 잘 소화 가능한 만능 차량임
          + 대중적으로 인기 있는 트럭들은 이런 스타일을 원하는 사람들이 선택하는 것임, 하지만 이 트럭은 트럭을 안 사는 소비층을 겨냥한 디자인임
          + 가격이 4만1천 달러라면 나는 바로 포기임, 일본에서 중고 Kei 트럭을 1만 달러 이하에 수입해서 쓰는 게 훨씬 낫다고 생각함
          + 이 차량의 전면부는 크럼플존(충돌 시 에너지를 흡수하는 구조)이 넓지 않은데, 운전자의 안전엔 어떤 영향을 주는지 궁금함
     * OpenSauce에서 실제로 이 회사와 많은 대화를 나눴음
          + 본넷과 도어 등 외장 패널은 복합소재로 프로토타입을 만들었지만 양산 때는 프레스 금속으로 전환할 예정임
          + 프레임과 서스펜션 상당 부분이 Subaru Ascent 플랫폼을 기반으로 함
          + 배터리는 NMC(니켈망간코발트) 계열이지만 셀/파우치는 OEM 정보를 못 들었음
          + 파워트레인은 Bosch의 범용 부품을 주로 사용, 양산 후 분해 영상 나오면 흥미로울 듯함
          + 차량 개조(modification) 관련해서는 아직 얼마나 개방형(Open)일지 정해지지 않았음, 업그레이드를 위한 장착 포인트 등은 설계에 포함했지만 Slate의 ‘캔버스로서의 자동차’ 접근 방식보단 제한적임
          + Slate 역시 CAN bus(자동차 내 통신 프로토콜) 정보를 아직 공개한 게 없음, 현재로선 헤드유닛(인포테인먼트 화면)은 각자 장착(BYOD)하는 방안이어서 안드로이드/리눅스 디바이스에서 버스 접근이 어렵지 않길 기대 중임
          + 이 차는 헤드유닛이 통합돼 있어서, 차량 통제 권한이 얼마나 있을지는 미지수임
     * 멋진 트럭이긴 한데 왜 요즘 전기차 업체들은 차량을 장난감처럼 보이게 디자인하는지 이해가 잘 안 됨, Rivian도 그렇고 이 차도 그렇고 골프카트에 평상(Flatbed) 달린 느낌임, 미국에도 전기 Kei 트럭 시장이 크다고 보지만, 좀 더 신뢰감 가는 디자인이어야 할 듯함
          + 독특함(차별화)은 물론 중요하지만, 너무 장난감 같아 보일 필요까진 없다고 생각함, Lucid처럼 고유하지만 진지한 예시도 있음, 게다가 작은 전기 트럭 자체만으로도 충분히 차별화가 됨
               o 다양한 설계 제약 조건(긴 주행거리 확보 위한 공기저항계수 최적화 등)으로 인한 최종 결과가 지금의 디자인임, ‘장난감처럼 보이게’ 하는 게 목적이 아님, 사람들 대부분은 자신만의 트럭 이미지에 사로잡혀 합리적 평가가 어려움
               o 나는 2000년식 Toyota Tacoma에 전기(Pure EV)만 더하면 충분함(0-60mph 10초면 되고, 150마력/200마일 주행거리면 만족함)
               o 이 차량은 Kei 트럭과 상당히 닮았음, Kei 트럭 자체도 원래 장난감처럼 생김, 90년대 Ford Ranger나 Tacoma 스타일을 선호하는데, 현재 안전규정과 소비자 요구를 반영하면 현실적으로 어렵다고 봄
               o 미국 소비자의 99%에게 Kei 트럭은 이미 장난감 같은 이미지임, 시장을 보면 F-150 같이 남성적 디자인이 주류라 이런 컴팩트하고 친근한 디자인의 트럭이 수요 갭을 메울 수도 있다고 봄, 대세는 안 되더라도 이런 접근이 틈새시장에 적합함
               o Kei 트럭 디자인 자체가 알다시피 이미 매우 장난감스러움
     * 전기차가 나오면서 혁신이 다시 활발해지고 있다는 점이 정말 기쁨, EV는 본질적으로 구조가 단순하고 부품수가 적어 설계도 유연해서 스타트업들이 자동차 시장에 진입하기 훨씬 수월함, 앞으로 EV 기술이 표준화되는 몇 년 뒤엔 예상치 못한 재밌는 모델들이 더 나올 것 같아 기대가 큼
     * 미니 EV 트럭/SUV 시장에 다양한 멋진 디자인이 나오는 게 반가움, 다만 실질적인 사업성이 걱정임
          + 왜 Tesla는 이런 미니 EV 트럭을 출시하지 않는지 의문, 공급망과 기술력은 충분하니 금방 만들 수 있을 텐데, 이런 차가 대중화되면 경량 화물 운송에서 탄소 저감에 큰 기여 가능성 있음
          + Tesla와 같이 규모 있는 회사에선 생산을 시작하려면 아주 많은 단위(X만 대 이상)로 찍어야 타당함, 생산 툴링, 공급망 구축, 연관 비용이 너무 높기 때문, 게다가 이러한 차의 수요는 유튜버 등 영향력 있는 농업/건설 업계 인플루언서들 통해 홍보되기 전까진 낮을 거라 생각함
          + EV는 화물 운송 목적에는 충분히 이상적임, Home Depot 자재를 공사장에 옮기거나, Costco 물품을 식당이나 가게에 배송하는 등 단거리 무거운 것 운송에 실용적임, 200마일 정도만 돼도 충분하다고 평가함(MT1은 내 기준에서 굉장한 모델임)
          + 더 많은 경쟁은 환영이고, 개인적으로 이 시장이 성공 및 흥행에 꼭 도달하길 간절히 바람
               o Tesla가 판단 기준일 필요는 없다고 봄, Cybertruck은 연 25만 대 생산 능력 갖췄지만 현재 출고는 만 대도 안 되고 오히려 점점 감소 추세임
               o 안타깝게도 이런 트럭들은 소형 스마트폰 시장과 비슷하다고 생각함. 댓글창만 보면 커다란 수요가 있어보여도 실제로 거의 아무도 안 삼.
               o 유튜버 인플루언서 힘에 의존한 수요 증대는 다소 의문임, John Deere같이 유명 브랜드의 제품이 실제로 유튜버 영향으로 매출이 크게 올랐다면 예를 들어달라 질문함
               o 외관 디자인은 훌륭한데, 실내 디자인이나 UI는 다 터치스크린에 치우쳐 디자인이 없다고 느껴짐, 이미 공개된 실내 렌더링을 보면 확인할 수 있음
               o 사업성 걱정과 관련해 ford Maverick의 성공이 희망적 사례라 생각함, 소형 트럭 수요가 이 정도로 있는지 예상 못했지만 실제로 꽤 괜찮은 수요가 있었음
     * 홈페이지에서 인테리어 사진을 바로 찾아봤는데, 실제로 물리 버튼 하나 없이 전부 터치스크린 중심의 미니멀 디자인이어서 실망스러웠음, 그나마 방향지시 레버는 남아있어서 다행임
     * 차량 크기가 3860 x 1854 x 1676mm로 일본 Kei 카 최대 규격(3400 x 1480 x 2000mm)에 비해 각각 14% 이상, 25% 이상 넓고, 오히려 높이는 16% 더 낮음, Kei 트럭 중 가장 비슷한 Daihatsu Hijet Deck Van도 실제로는 465mm 더 짧고 짐칸도 훨씬 짧은 880mm임
          + 사실 Mini Cooper도 실제로 보면 의외로 큼을 알 수 있음
     * 전기차는 구조가 더 단순해서 더 저렴해야 한다고 생각했는데 실제로는 그렇지 않아서 아쉬움, 한시적으로 경쟁사 수입을 막는 관세로만 버틸 순 없는 법임
     * 작은 차 컨셉이 마음에 들고, 실제 내 라이프스타일엔 이런 차가 실용성 있을 것 같음
     * 4만1천 달러는 너무 비쌈!
          + Slate 트럭의 전 목표는 2만 달러 이하로 출시하는 것이고, EV 보조금이 없다면 그나마도 쉽지 않을 거라 생각함
               o 소량 한정 생산 차량이 거의 항상 스포츠카 쪽 중심인 데에는 이유가 있음, 무명 브랜드가 저가 시장을 공략하려면 대량생산의 규모의 경제가 필요한데 현실적으로 어렵다는 점임
               o 공식 홈페이지에서 가격을 찾으려 스크롤 내려봤는데 금액이 보이지 않아 비쌀 거란 예감은 했음, 하지만 이 정도일 줄 몰랐음
"
"https://news.hada.io/topic?id=22249","Comet - Perplexity의 Web Agent이자 브라우저","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Comet - Perplexity의 Web Agent이자 브라우저

  Comet: Perplexity가 만든 새로운 브라우저

   출시한 지 조금 지났지만 대기 명단 통과 후 사용해 보고 작성해 봅니다
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    핵심 개요

   Comet은 Perplexity에서 개발한 AI 브라우저로, 사용자의 집중력과 워크플로우를 극대화하고 호기심을 실행력으로 바꿔줍니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    주요 기능 및 특징

     * AI 통합 검색, 즉각적 컨텍스트, 사이트 자동화
          + 브라우저에서 바로 요약, 쇼핑, 일정, 리서치 가능
     * Comet Assistant
          + 클릭, 입력, 제출, 자동완성 등 반복 작업 자동 처리
     * 복잡한 업무도 자동화
          + 제품 비교부터 결제까지 쇼핑 전 과정 지원
     * 통합 관리
          + 이메일과 캘린더(Gmail, Google Calendar) 연동
          + 일정 브리핑, 메일/일정 예약까지 브라우저 내에서 해결
     * 개인 맞춤형 경험
          + 사용 패턴을 학습해 탭 및 인사이트 자동 정리
     * 내 활동에서 바로 답변 찾기
          + 히스토리, 동영상, 문서 등 개인 데이터 검색 지원
     * 스마트 액션 및 탭별 초점 기능
          + @tab 기능으로 열린 탭에 맞는 실시간 답변 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  사용 사례

   Perplexity Linkedin에서 사용 사례를 추가로 포스팅 해 주었습니다
   https://www.linkedin.com/company/perplexity-ai/posts/?feedView=all
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  서비스 사용 후 개인 의견

    기술

     * 크로미움 브라우저 + Perplexity + Web Agent 형태
     * 자연어 쿼리 -> Intent/slot 추출 -> Symantic DOM 매핑 -> 액션 변환 -> 이벤트 실행(web Tool) -> 결과 파싱 후 전처리 -> ReAct 형태의 구조 추정
     * Comet이 스크린샷을 찍어주어 현재 작업 현황을 파악이 가능하지만, 비전모델을 이용해 분석을 하는지는 추가 확인 필요

    테스트 사례

   (1) 메일 작성
     * 적절히 구글 이메일 웹사이트를 활용해 메일을 작성
     * Agent가 실제 발송은 하지 않고 임시 저장하는 선택

   (2) 네이버 지도 경로 찾기 + 주변 음식점 추천
     * 네이버 지도 웹페이지를 방문한 후 서울역에서 잠실까지 경로를 찾아달라고 질의하였고 네이버 지도 서비스를 적절히(사람처럼) 사용해 경로를 찾음
     * 주변 음식점 찾기는 네이버 지도를 활용하지 않고 기존 Perplexity의 Search Tool을 사용

   (3) Open AI Platform Pricing 크롤링
     * Open AI Platform은 Remix를 사용하는 것으로 알고 있으며 일부 API 가격이 토글 형식으로 가려져 있는 상태
     * Comet에게 명시적으로 토글을 열고 모든 API의 가격을 확인해서 Markdown으로 결과를 달라고 하였고 적절히 제공함
     * 다만, 정확히 정보를 추출하도록 하는 프롬프트 엔지니어링 필요
     * Anthropic의 API 페이지도 확인하도록 하였지만, 웹페이지의 레이아웃(디바이더)가 모호해 복잡한 API 정책을 정확히 파싱하지 못함. 기본적인 API 가격은 정확히 추출.

  개인 의견

     * 브라우징 에이전트를 개발하며 컨텍스트, 렌더링, 등등 고려할 점이 많았을 것 같은데 어떻게 해결했을지 궁금합니다.
     * 기술적으론 wow가 보이는데, 프로덕트의 관점에선 아직 모호한 것 같습니다.
     * 사람마다 다르겠지만, 전 왜 이 프로덕트를 써야하는지, Comet을 이용하기 위해 돈을 지불할 이유가 있는지, 나(고객)의 어떤 문제를 해결해 줄 수 있는지. 즉, 가치 제안이 저에게는 명확하지 않습니다. 현재는 '오! 신기하다 이게 되네, 생각날 때 몇번 더 사용해 봐야지' 입니다.
     * 물론 Perplexity가 사용 사례를 제시해 주고 있지만, '굳이 Agent로 해결해야 할 문제인가?(내가 하면 더 빠른데)'와 '진짜 문제를 모두 해결한게 맞나?'라는 생각이 듭니다.
     * 아무래도 브라우저가 인프라에 가까워 이렇게 느껴지는 것이 아닐까 조심스럽게 추측 해 봅니다
     * AI Agent기반 브라우저가 어떻게 보면 새로운 포멧이기에 고객들에게 '당신은 어떤 문제가 있는데 Comet이 진짜 그 문제를 완벽히 해결해줄 수 있어'를 잘 보여주면 될 것 같다고 생각합니다.
     * 눈여겨 보며 좀 더 사용해 봐야 할 것 같습니다. 프로덕트적인 문제를 어떻게 해결할 수 있을지 고민이 되네요.

  UI

   개인적으로 분석해 본 내용입니다. (역기획으로 만들어 보고자 작업중 입니다...,)
   https://www.figma.com/deck/Gky9ZDEqZKdJfG4RWoNYdf

   잘 읽고 갑니다.
   예상대로 인것 같아서 조금 아쉽네요..

     기술적으론 wow가 보이는데, 프로덕트의 관점에선 아직 모호한 것 같습니다.

   에 공감합니다.
    1. 에이전트의 작업속도가 너무 느리고, 그렇다고 믿고 맡겨두고 잊어도 될만큼 성능이 나오냐 하면, 그것도 아닙니다.
    2. 모델선택이 불가능한것으로 보이고 그래서 아마 퍼플렉시티의 자체모델로 대부분의 작업이 이루어지는 것 같은데, 환각이 너무 심합니다.
       예:) 네이버 뉴스페이지를 읽고, 관세협상에 대한 신문별 논조를 정리해달라고 했을 때, 읽지 않은 신문의 내용을 '각 신문사에 성향에 대한 사전 학습된 편견'을 바탕으로, 안 읽은 신문사의 논조를 지어내서 답변합니다.
    3. 이러한 작업을 제대로 수행하게 만들기 위해서는 프롬포트 엔지니어링과 반복적인 검토/교정이 필요한데, 거기에 신경쓰는 비용이 코멧을 통한 자동화를 통해 얻을 수 있는 이익보다 커보입니다.
    4. 만드는 과정에서 맞닥뜨렸을 기술적인 난제들이 눈에 보여서, 그런 부분에서 '개발자'입장에서 흥미를 가지고 볼 부분은 존재합니다만, 상품으로써는...

   궁금한게 경험하실때 해당 comet 에이전트 쿼리 사용 제한이 있었나요? 아니면 다른 툴들처럼 테스트 기간이라 쿼리가 상관없는 상태인지 궁금하네요. pro-max 이렇게 있었던걸로 기억이 나서 궁금해 여쭤봅니다.

   전 현재 pro 구독 상태입니다. 제가 좀 적게 사용한 것 일수도 있지만 Comet 포함 Perplexity에서 아직까진 Limit에 걸린 적은 없습니다. 생각보다 널널한 것 같기도 해요.
   구독 플랜에 ratelimit이 제공되지 않아 확실히는 잘 모르겠어요.
   일단 beta release를 pro-max와 다르게 준 것 같습니다.

   https://perplexity.ai/help-center/en/…

   개인적인 느낌이지만 말씀하신대로 테스트 기간이라 그런지 쿼리가 상관 없어 보입니다. 한번에 가상 브라우저 12개 병렬로 띄워서 작업이 되는 것 까지 확인했습니다. 이건 놀랍네요.
"
"https://news.hada.io/topic?id=22255","1억 달러 Series B 투자 유치","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1억 달러 Series B 투자 유치

     * Oxide가 새로운 전략적 파트너인 USIT 주도로 1억 달러 규모의 Series B 투자를 유치함
     * 이번 투자는 기존 누적 투자액 8,900만 달러를 뛰어넘으며 Oxide의 차세대 성장 기반 마련임
     * Oxide는 온프레미스 클라우드 컴퓨팅 수요에 대응하기 위해 소프트웨어와 하드웨어를 처음부터 재설계함
     * 자체적으로 하드웨어, 소프트웨어, 컨트롤 플레인, 스토리지, 스위치까지 통합 설계해 시장 차별점을 확보함
     * 이번 투자로 제조와 지원, 운영 규모 확장을 통해 더 많은 고객 요구를 충족할 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Oxide 1억 달러 Series B 투자 유치

  투자 유치 배경 및 의미

     * Oxide는 USIT가 주도하고 모든 기존 투자사가 참여한 가운데 1억 달러(USD) Series B 투자를 유치함
     * 설립 6년 만에 누적 자본금이 두 배 이상 늘어나며, 차세대 인프라 기업 도약 목표를 한층 이룰 수 있는 기반을 마련함
     * 2019년 벤처 캐피털 유치 당시, 온프레미스 클라우드 시장의 중요성과 하드웨어·소프트웨어 전면 재설계 필요성을 강조하였으나 시장성에 대한 의구심을 받음
     * 투자자들의 회의적인 시선에도 불구하고, Oxide 팀은 클라우드 기술의 온프레미스 적용, 그리고 통합 시스템을 만드는 데 중점을 두었음

  Oxide의 도전과 기술 혁신

     * Eclipse Ventures 등 일부 투자사는 Oxide의 비전을 이해하고 초기 투자를 단행함
     * Oxide는 다음과 같이 자체 개발을 이어왔음
          + 보드 설계: 하드웨어 신뢰 기반, 전력 관측 등 인프라 핵심 요소 내재화
          + 마이크로컨트롤러 OS 개발: 기존 BMC 대체로 보안, 신뢰성 강화
          + 플랫폼 구축 소프트웨어: UEFI BIOS 제거 및 취약점 해소
          + 호스트 하이퍼바이저: 통합 경험 제공, 타사 소프트웨어 라이선스 종속성 해소
          + 자체 스위치 및 런타임: 운영 복잡성 감소, 통합성 극대화
          + 통합 스토리지 서비스: 신뢰 가능한 스토리지 제공, 외부 의존성 제거
          + 분산 컨트롤 플레인: API 기반 모던 인프라 서비스 제공(컴퓨트, 네트워킹, 스토리지 등)

  제품화 여정과 고객 성장

     * 기술 각 요소의 완성도 역시 중요하지만, 궁극적 목표는 완전한 제품화였음
          + 주요 마일스톤: 보드 시제품 완성, 스위치 테스트, 컨트롤 플레인 구동, 랙 시스템 제조 및 FCC 인증 통과
     * 2년 전 첫 시스템 출하 이후
          + 현장 소프트웨어 업데이트, 성능 개선, 고객 요청 기능 추가 등 실전 환경에서 신뢰성 입증
          + 고객 확보도 빨라지며, Oxide의 투명성(팟캐스트, RFD, 오픈 소스 코드 등)이 신뢰 강화에 큰 역할을 함

  대규모 확장 및 새로운 파트너십

     * 지속적으로 대형 고객들이 대량의 Oxide 랙 도입 및 운영, 지원에 관한 문의 증가 현상 발생
     * 이 과정에서 USIT와의 파트너십 논의가 시작, 상호 비전과 가치에 공감대 형성
     * USIT의 창립자 Thomas Tull 등과의 교류를 통해, 시장 성장성과 Oxide만의 차별화된 전략을 다시 한 번 확인함

  앞으로의 계획 및 비전

     * 이번 투자 유치는 제조, 지원, 운영 등 조직적 스케일 확장에 직결될 예정임
     * Oxide만의 미션(혁신적 인프라를 통해 현대 컴퓨팅 재정의) 실현에 큰 자신감과 동력을 불어넣는 계기가 됨
     * 고객 중심, 기술 혁신, 그리고 클라우드 인프라의 패러다임 전환을 선도하는데 집중할 방침임

        Hacker News 의견

     * 나는 Bryan Cantrill의 팬임, 그래서 Oxide가 잘 되고 있는 것이 기쁨, 초창기엔 회의적이었음(여기 HN에서도), 데이터센터를 오랫동안 직접 만들어본 경험 때문에 내 입장에 맞는 사고방식에 갇혀 있었음, 시간이 지난 뒤 Oxide에 대해 생각이 변하게 됨, 초반에는 ""이거 너무 고급스럽지 않나?"", ""진짜 시장이 있을까?"", ""섞어서 쓸 때 상호운용성이 괜찮을까?""가 걱정이었음, 찾아보니 답변은 ""그렇다""와 ""상관없다""였음, 처음엔 별로라고 봤는데 내가 틀린 것 같음, 최근 Boris Mann이랑 이 주제로 얘기했는데 “John, 그거 아니야, 엄청나게 빠른 컴퓨트 자원이 계속 대거 필요해지고 있고, 관리 오버헤드가 낮은 독립 컴퓨트 수요가 커질 거다, 그 시장은 충분하다” 라고 했음, 그 후 직접 자료를 조사하고 생각해보니 친구 말이 맞는 것 같음, 지금 시점에서는 Oxide가 정말
       강한 브랜드가 될 거라고 생각함, 행운을 빔
          + 시장에서 승리하려면 단순히 더 나은 제품을 만드는 것만으로는 부족하다고 생각함, Oxide에 대해 듣는 얘기들은 인상적인 그린필드 데이터센터 구축 사례이지만, 그게 충분한가 의문이 있음, 이런 대규모 의사결정을 내리는 사람들이 관리 툴이 더 좋다고 해서 신경 쓸까 궁금증이 있음
          + 나처럼 기술적으로 많이 부족한 입장이지만 Oxide에 ""투자""했음(내 프로젝트를 Oxide 서버에서 돌림), 계속 성장하는 걸 보니 기쁨, 내(나이브한) 판단은 (a) Cantrill의 소프트웨어 의견에 동의함, (b) 스스로를 드러내는 태도가 맘에 듦, (c) 기술 블로그에서 (사회적) 기술력이 높다고 느낌, 인터넷이 닫힌 정원에서 벗어나려면 고품질 독립 데이터센터가 반드시 필요하다고 생각함, 아무도 직접 데이터센터를 운영하길 원하지 않고, Google/Amazon/Microsoft 같은 플랫폼이나 그보다 못한 비즈니스 제품에 기대고 싶지 않음, 이런 움직임이 계속되길 희망함
          + 여전히 잘 모르겠음, 남의 소프트웨어로 하드웨어를 돌리는 거라면 온프레미스든 오프사이트든 무슨 차이가 있을까 궁금증이 있음
          + 기술적으로 꼭 말이 안 돼도, 기업들은 결국 이런 제품을 구매함, 수백억씩 들여서 데이터센터를 3번씩 직접 구축하려는 대기업들이 많음, 엔터프라이즈는 대형 장비에 자기 호스팅을 좋아함, 그래서 잘 팔릴 거라 생각함, 다만 계속 매출이 성장하려면(5년 이상 본다면) 서비스 확장이 필요할 것 같음
     * Oxide의 모든 직원이 같은 연봉을 받음

     우리는 정말 심플한 방식을 택했음: Steve, Jess, 그리고 내가 스스로 책정하려던 연봉을 모두에게 동일하게 지급함
     관련 블로그
     모든 직원이 동일한 지분(equity)도 갖게 되는지 궁금함
          + 모든 직원이 동일한 지분을 받냐는 질문에 대해 예전 스레드에서 답변이 사실상 ""아니다""였던 걸 본 기억이 있음, 하지만 이 주제는 회피된 경우가 많았음, 펀딩 단계, 평가액 등 여러 요인 때문에 지분 구조는 시점에 따라 다를 수밖에 없다고 생각함, 다만 모두에게 동일 기본연봉을 강조하면서 지분 정책에 대해선 침묵하는 게 이상하다고 느낌, 이런 일에서는 총보상(total comp)이 중요한데, 예전 Oxide의 보상 논쟁도 흥미로웠음, 채용 시 보상에 대해 묻는 후보자에게 부정적으로 생각했다는 얘기를 Oxide 출신에게 들었음, 그 정보는 걸러서 받아들일 필요 있음,
            EDIT: 공식 블로그를 확인해보니 이런 지분 설명이 있음

     어떤 사람들은 현금 보상이 아니라 지분 얘기를 해야 한다고 함, 스타트업 지분이 중요하지만 교정비용이나 지하실 리모델링에 바로 쓸 수 없기 때문에 모든 직원이 미래에 대한 몫을 주기 위한 지분은 필요하다고 봄, 단순히 지분이 있다는 이유로 현금보상을 적게 주어선 안 된다고 생각함, 그리고 지분은 위험(risk)에 대한 보상임, 스타트업에서 위험은 시간이 갈수록 줄기 때문에 초창기 직원일수록 더 큰 위험과 보상을 받음
     근데 실제로 똑같이 지급하는지는 답변이 안 됨
          + 2021년 이후 블로그를 약간 업데이트 했음,

     최근 연봉이 몇 번 인상되어 현재 $207,264임, 세일즈 포지션에는 기본급+커미션 형태의 변동 보상이 있음
          + 기술적으로 그리고 구체적으로 이런 정책이 어떻게 굴러가는지 궁금함, 창업자들은 지분을 가지고 시작할 거고, 초기 투자자가 있을 수 있음, 그러고 나서 직원을 채용하면 무슨 일이 벌어질까? 새 주식이 발행되거나 기존 주주가 희석되거나 입퇴사자 처리 등이 이슈임, 모두가 만족할 수 있는 방안이 있을지 모르겠음, 각자 의견이 궁금함
          + 창업자들은 일반적으로 시세보다 반값 수준으로 보상 받는 경우가 많음
          + 이 보상 블로그가 공개됐을 때부터 Oxide에서 가장 이상하게 본 부분임, 스타트업에 연봉 보러 오지 않는다고 생각하고, 연봉의 공정성을 강조하는 회사는 대부분 총보상에 대해 딴지를 걸지 못하게 의도적으로 연봉만 강조함, 초창기 직원 빼고 대부분은 실질적으로 지분 배분이 부족할 가능성이 커짐, 실제로 Oxide는 지분 질문하는 사람을 부정적으로 본다고 공식적으로 밝힘,
            이런 구조의 회사는 끝내 돈에 별 관심 없는 부유한 사람들과, 좋은 조건 못 구해서 합류한 평범한 인재 조합이 됨(실력 좋은 사람이라면 원격+높은 보상 받을 수 있는 곳이 많음), 만약 Oxide가 뛰어난 성과에 대해 대규모 지분을 주고 있다면 기존 원칙과 모순임, 이런 구조로는 장기적으로 인재 풀이 줄고 품질이 하락할 것 같음, Oxide에서 하는 일에 정말 관심 있는 민간 부유 고급 인재가 시장에 많지 않을 것임
     * 이 프로젝트에 꽤 긍정적 시각임, 직접 서버 인프라 관리해본 사람이라면 Oxide가 해결하는 문제의 고통을 너무 잘 앎,
       또한 클라우드 탈출과 주권 보장 니즈, 그리고 클라우드 마진이 지나치게 두꺼워진 현실을 깨닫는 회사가 늘 것 예측함
          + 클라우드 업체가 많아지면 가격이 내려갈 수 있다고 봄, 예를 들어 Amazon은 하드웨어를 사들여서 vcpu 단위로 임대하며 각종 요금이 붙는데, 실제로 서버 비용은 몇 달 내로 상환될 정도임, 여러 Tenant가 나눠 쓰는데도 각자 수십~수백 달러씩 냄, 물론 클라우드 VM의 이점도 있지만 컴퓨트·메모리 대비 지불 금액이 매우 높음, 가격 인하 여력은 충분한데 경쟁사가 부족해서 불가능한 것임, Amazon이 돈을 잘 버는 이유, 내가 GCP를 쓰는 이유 또한 직접 하드웨어 관리(디스크 교환, 네트워크 라우터 고장, 쿨링)를 하기 싫어서임, 그 대가로 돈을 많이 내는 게 불합리하다는 걸 스스로도 느낌, 옛날에 Hetzner에서 매달 50유로에 쿼드코어 Xeon+RAID1 디스크, 32GB RAM의 베어메탈을 썼었음, 클라우드에선 이 사양을 비슷한 가격에 쓸 수 없음, 심지어 요금이 장비값을
            뛰어넘기도 함, 지금은 훨씬 더 효율적인 사양을 싸게 제공함
          + (저가형 기준) NAS, 컨테이너, 리버스 프록시 등으로 의외로 상당히 쉽게 자체 클라우드 환경을 구축할 수 있음, 꽤 멀리까지 쓸 수 있음, 하지만 고성능 쪽에선 시장이 그야말로 무한함, 모든 대기업이 이런 걸 원해야 하며, 클라우드 서비스는 비싸고 1티어 외에서는 성능도 떨어지고 지원도 별로임, 진정 미래 지향적인 시장임
          + 클라우드의 높은 마진에는 동의하지만, 여기서도 공급사 락인(vendor lock-in)이 발생할 수 있음, 결국 Oxide에도 높은 마진을 지불하면서 물리 장비와 장소도 계속 직접 관리해야 할 수 있음
          + 웹앱 분야에서 30년 가까이 일한 경험에 따르면, 네트워크 라우팅이 핵심 고통임, 랙/서버 자체는 쉽게 구축하지만 실제로는 가용성, 방화벽을 뚫는 게 도전임, 진정한 강점은 DNS 테이블의 즉각적 업데이트, 서버 DNS 화이트리스트 전략 등, 이런 부분에서 클라우드가 강함, Google, Microsoft가 이메일 분야에서 우위를 갖는 이유도 여기에 있음, 대안 SMTP 라우트에선 인증 문제(DKIM 등)로 기본적으로 막히고 결국 대형 서비스로 몰림, 더 많은 Cloudflare Tunnel 같은 솔루션, ISP 경로 통해 로컬 호스팅이 필요함, 지금처럼 새로운 중앙집중 데이터센터는 오히려 지양해야 함
          + 클라우드의 높은 마진이 결국 CIO, CTO 등 구매권자들에게 공유되는 구조임, 직장에서 끊임없이 클라우드 혹은 클라우드 네이티브로의 전환을 외치는 분위기는 깊은 기술적 신념 때문이 아니라 ""안 그러다 잘릴까 봐""라는 심리적 요인이 더 큼
     * Oxide는 외부에서 보기엔 초기 Sun Microsystems의 '정신'을 일부 이어받은 느낌임(내부 연관도 알고 있음), 거기서 일하는 사람들이 솔직히 부러움, 큰 돈의 압박에 그 정신이 꺾이지 않길 바람,
       내 역량에 맞는 포지션이 아직 없어 아쉽지만 수시로 지켜보고 있음
          + 정말 그렇다고 느낌, 가치관이 너무나 분명함, 투자할 수만 있다면 꼭 해보고 싶음, 물론 실패할 수도 있지만 도전하는 모습 자체가 강한 차별점이 있음, 시장 내 사용자 대부분에게 실질적 개선을 가져다 줄 제품이라고 확신함(기존 대기업 제외)
     * Meta: Oxide가 그동안 쿨링 기술을 여러 번 다뤘는데, 언젠가 GPU 서버도 제공한다면 어떤 쿨링 설계를 적용할지 궁금함, 업계는 이제 랙/섀시 수준이나 혹은 칩 단위로까지 리퀴드 쿨링이 대세가 되는 중임
       Oxide의 전력/쿨링 이야기 블로그,
       YouTube 영상1,
       YouTube 쇼츠
       NVIDIA 블로그, 리퀴드 쿨링
       Top 10 리퀴드 쿨링 회사
       ZutaCore
          + Oxide 블로그에서 쿨링에 정말 많은 이야기를 다룸, 전체 스택을 효율성에 맞게 재구성했고, 그 목적에 맞춘 펌웨어까지 직접 작성해서 시스템 온도가 매우 낮음
          + 데이터센터 쿨링에 관심이 더 많다면 Jane Street의 <Signals and Threads>라는 팟캐스트에서 최근 그들만의 쿨링 인프라 이야기를 다룸
            관련 에피소드
     * 명확하고 잘 구조화된 주장문(논문 서두)이 이렇게 안정감을 주는 것 같음

     우리의 논지는 클라우드 컴퓨팅이 전체 컴퓨팅의 미래가 될 것이며, 온프레미스의 중요성이 지속 혹은 더 커질 것이고, 이 시장을 위해 하드웨어/소프트웨어 전체 스택을 원점에서 재설계해야 하며, 성공한다면 큰, 지속적, 대중적 회사가 될 수 있다는 것이었음
     처음부터 자신들의 관점과 원칙을 명확히 밝힘, 논리적임, 이런 결과까지 갈 수 있다는 걸 잘 전달함
          + 확실히 명확하고 논리적이지만, 과연 진짜 '원점(first principles)'에서부터 시작한 것인지는 의문임, Oxide의 기반이 오픈컴퓨트(OpenCompute) 프로젝트라고 본 것 같은데 이건 Facebook이 자체 데이터센터 설계를 오픈소스로 공개한 프로젝트임, 구글은 진짜 완전히 처음부터 새로 쌓았고(파워, 랙, CPU 등 전부), Kubernetes도 쓰지 않았음, 지금 오픈컴퓨트 프로젝트가 얼마나 활성화되어 있는지도 모름, Facebook이 아직도 새 랙 설계를 공개하는지, 그리고 Oxide가 하드웨어 측면에서 얼마나 달라졌는지도 궁금함,
            Oxide의 경우
               o 보드 설계
               o 마이크로컨트롤러 OS
               o 플랫폼 인에이블먼트 소프트웨어
               o 호스트용 하이퍼바이저
               o 스위치
               o 통합 스토리지 서비스
               o 컨트롤 플레인
                 이 각 요소가 직접 설계된다면, 일부 보드나 스위치만 오픈컴퓨트 영향이 있을 수 있지만 실제로는 거의 자체 설계일 수도 있음, 나도 자세히 모름, 오픈컴퓨트에서 기계적, 전원 등 느리게 변하는 요소만 받은 것 같기도 함, 소프트웨어는 Illumos 파생과 Rust니까 차이가 클 것임
          + “큰 대중적 회사(public company)”를 목표로 해야 하는 이유는 뭔지 의문임
     * 주권적 시스템 관리를 낮은 오버헤드로 구현하는 것은 매우 중요한 목표임, 이런 길을 가는 Oxide에 박수를 보냄, 다만 문제의 본질이 하드웨어가 아니라는 점에서 한가지 실수가 있다고 느낌, 실제론 거의 대부분이 소프트웨어 문제이고, 일부 하드웨어 업체에게 ILOM/IPMI처럼 불완전한 펌웨어 개선을 설득하는 게 관건임,
       참고로 나 자신은 소프트웨어 자동화 분야에 종사함
     * Oxide 응원함! 옛날에 Emeryville 사무실(창고?)를 애견 산책하며 지나가곤 했는데, 날이 좋으면 거대한 서버 스택이 문 열고 밖에 있는 광경이 정말 멋졌음, 도난 걱정은 별로 안 들었는데, 저런 건 리프트 없인 못 움직이니... 진짜 궁극의 홈랩 셋업 같음
          + 절도는 걱정 없어도, 기물 파손/방해(vandalism/sabotage)는 여전히 우려되지 않는지 궁금함
     * milestone 축하함! 팬데믹 때 처음 알게 된 뒤 계속 지켜봤고, Oxide가 정체된 시장을 흔들어줬으면 하는 바람임, 여러 벤더의 부품을 직접 맞추는 게 아니라 tightly integrated한 랙 단위 제품을 온프렘에 배치할 수 있다는 점이 굉장한 매력임, 처음엔 독점 하드웨어라 주저했지만 오픈소스 기반이라는 점이 의심을 해소해줌,
       이후 quarter-rack이나 산업용 소형 배치 같은 더 다양한 종류가 나왔으면 좋겠음, 이번 펀딩 성공을 다시 축하함
     * 오리지널 제품 외에도, On the Metal/Oxide and Friends 팟캐스트 추천함, 프로그래밍 이슈를 재미있고 교육적으로 다룸, Bryan Cantrill은 엄청 재미있고 동시에 해박함, 공동 진행자와 게스트들도 그 에너지를 잘 받아서 모두 좋음, Rust에 관심 있는 이들에게 꼭 추천함
"
"https://news.hada.io/topic?id=22235","Software 3.0: 소프트웨어가 소프트웨어를 집어삼키는 시대 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Software 3.0: 소프트웨어가 소프트웨어를 집어삼키는 시대 [번역글]

    Software 3.0: 소프트웨어가 소프트웨어를 집어삼키는 시대 (Software Eating Software Eating Software)

    1. 배경 및 본질: 소프트웨어의 '자기 집어삼키기' 진화

     * 21세기 소프트웨어는 단순히 세상을 변화시키는 데 그치지 않고, 이제는 스스로를 흡수·진화시킴
     * Andrej Karpathy가 정의한 Software 1.0, 2.0, 3.0 패러다임을 통해, 소프트웨어는 이제 자기 자신까지도 흡수하는 ‘재귀적 추상화(recursive abstraction)’의 길을 걷고 있음
     * 소프트웨어 발전의 각 단계는 엔지니어와 개발자의 역할을 변화시키고, 추상화 수준 역시 근본적으로 변화시킴

    2. 소프트웨어 세대별 특징 및 주요 변화

    1. Software 1.0 (명시적 로직·규칙의 시대)
          + 개발자가 모든 규칙과 로직을 직접 작성(Explicit Coding)
          + 예측 가능성 및 해석 용이성이 높음
          + 한계: 복잡성 증가에 따라 인간 노동력의 한계, 확장에 제약
          + 오늘날 IT 인프라·운영시스템의 근간
          + 프레임워크·API가 복잡성을 내부적으로 흡수하여 사용성을 높임
    2. Software 2.0 (데이터 기반·학습 시스템)
          + 규칙 대신 데이터로부터 모델이 패턴·논리를 자동 학습
          + 주요 기술: 머신러닝·딥러닝
          + 장점: 복잡한 패턴 및 대규모 데이터 처리 효율
          + 단점: 내부 동작의 불투명성(블랙박스 현상)
          + 주요 변화 예시:
               o 수작업 특징 추출(Feature Engineering) → 자동 추출
               o 규칙 기반(Rule-based) → 패턴 인식(Pattern Recognition)
               o 전문가 지식 → End-to-End 데이터 학습
               o 전통적 NLP → 트랜스포머(Transformer)
          + 기존과 완전히 다른 대규모 전환점을 제공
    3. Software 3.0 (생성형 AI 및 자기진화의 시대)
          + 대형 언어모델(LLM), 생성형 AI가 코드를 비롯한 소프트웨어를 직접 생성
          + 자가 개선(Self-Improvement) 및 생성-학습-운영의 루프 구조
          + 인간 역할: 기계와의 협업, 품질 검수자·큐레이터·감독 등으로 변화
          + 주요 변화 예시:
               o 모델 설계 자동화(Neural Architecture Search)
               o 하이퍼파라미터 튜닝 및 최적화 자동화
               o Foundation Model 활용의 범용성 증가
               o Task-specific에서 Few-shot, Zero-shot learning으로
               o MLOps 등 소프트웨어 운영·배포까지 자동화

    3. 실전 사례: 하이브리드 스택(1.0+2.0+3.0 조합)

   AI 고객 상담 에이전트 운영 예시
     * Software 1.0: DB 인터페이스, 보안 관리, 트랜잭션 관리 등 신뢰와 예측 가능성이 중요한 인프라 담당
     * Software 2.0: 대화의 의도 분류, 감정 분석, 음성→텍스트 변환 등 ML 중심 데이터 해석
     * Software 3.0: 상황 맞춤형 생성 응답, 실시간 자동 코드 및 대화학습, 피드백 기반 진화 등 창의적·적응적인 기능 제공

   실제 동작 예시
    1. 고객 문의가 들어오면 1.0 인프라가 안전하게 수신
    2. 2.0 엔진이 의도·감정·핵심 정보 자동 파악
    3. 3.0 시스템이 개인화되고 창의적인 대응방안 실시간 생성
       → 각 층이 유기적으로 협업, 빠른 혁신 및 창의적 서비스 제공

    4. 시사점 및 결론

     * 혁신 가속: 소프트웨어 계층적 패러다임이 융합됨에 따라 변화의 속도와 규모가 기하급수적으로 증가함
     * 추상화 심화: 개발자는 코드 작성에서 점점 더 높은 추상화(목표·의도·감독)에 집중하게 됨
     * 불투명성 및 위험 증가: 시스템이 점점 더 복잡·블랙박스화되면서 해석 및 통제의 어려움이 커짐
     * 창의적 협업 확장: 개발자와 기획자가 여러 세대의 소프트웨어 기술을 유연하게 결합할 때 더 큰 경쟁력을 확보할 수 있음
     * 과도기의 중요성: 현재는 Software 2.0(학습 시스템)에서 Software 3.0(생성형·자가진화 AI)로 넘어가는 시기로, 1.0~3.0 기술이 실제 서비스에서 융합적으로 활용되고 있음

    5. 요약

     * 소프트웨어는 자신을 ‘먹는’ 재귀적 추상화의 여정에 있으며, 1.0~3.0 패러다임이 현대 상품·서비스에 유기적으로 활용됨.
     * 개발자·기획자는 각 패러다임의 역할과 함정을 모두 꿰뚫고 설계해야 경쟁력을 확보할 수 있다.

   (본 요약은 동일 프롬프트에 답한 Gemini 2.5 Pro, GPT-4, Claude 4 Sonnet의 응답을 바탕으로 함)
"
"https://news.hada.io/topic?id=22328","Anandtech.com이 이제 포럼으로 리디렉션됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Anandtech.com이 이제 포럼으로 리디렉션됨

     * 이제 Anandtech.com 메인 사이트가 현재 AnandTech Forums로 리디렉션됨
     * 기존의 기술 뉴스·리뷰 페이지 대신, 유저들이 직접 글을 올릴 수 있는 포럼 중심 환경으로 이동
     * 하드웨어 & 기술, 소비자 가전, 소프트웨어, 쇼핑, 소셜 등의 카테고리에서 세부 게시판 구조를 가짐

        Hacker News 의견

     * Archiveteam이 Anandtech 종료 발표 직후 사이트 전체 크롤링 작업을 해서, warc.gz 파일을 통해 웹사이트를 그대로 탐색할 수 있음, replayweb.page에서 쉽게 탐색 가능함, 또 다른 방법으로는 solrwayback으로 warc 파일을 인덱싱해서 탐색하는 방법도 있음, 전체 백업 링크와 도구 설명 링크도 함께 제공함
          + Kiwix도 오프라인에서 웹사이트를 탐색할 수 있는 훌륭한 앱임, warc2zim을 사용해서 WARC 파일을 Kiwix에서 쓸 수 있는 ZIM 파일로 변환 가능함, 독일어 사전 앱인 DWDS도 실제로 Kiwix 기반이라는 사실을 알고 깜짝 놀랐음
          + 질문이긴 하지만, Discourse 포럼을 저장해서 일반 웹사이트로 만드는 좋은 방법이 있는지 궁금함, 혹시 경험이나 사례 있는 사람의 의견을 듣고 싶음
     * 수십 년간의 하드웨어 리뷰, 최초 GPU 시절까지의 자료들이 점점 접근하기 어려워지는 현실이 매우 아쉬움, 이렇게 많은 콘텐츠를 보관하면서 광고 수익도 어느 정도 낼 수 있는데 왜 사이트를 없애는지 이해가 잘 안 됨, Anandtech 기사들은 여전히 많은 구글 검색에서 상위에 노출되고 있었음
          + 구글 상위에 노출되면 엄청난 양의 봇과 AI 트래픽이 발생하고, 이로 인한 비용도 상상 이상임
          + 사실 매각을 노리고 있을 수도 있을 것 같음
          + 최근 이런 상황에서 어쩔 수 없이 ChatGPT 같은 걸 사용하고 있음, ChatGPT 내부 보관 콘텐츠가 더 오래 남을 것 같음, 하지만 이제는 ChatGPT가 사실을 왜곡하는지 검증할 방법이 점점 줄어들고 있음, 실제로 오래된 하드웨어 프로젝트를 찾을 때 ChatGPT가 “취약점이 있다”고 주장하지만 레퍼런스도 없고 archive.org에도 자료가 없는 경우가 있었음
     * anandtech.com이 지금은 기사 메인 페이지 대신 포럼으로 리디렉트되고 있음, 1년 전에 운영 중단 소식 후 남긴 공식 트윗과 작별 기사, 인터넷 아카이브에 남은 가장 마지막 메인 페이지 링크 그리고 작별 기사 전체 링크를 공유함, 기사 중 다음과 같은 문구가 있었음:
       ""사이트는 한동안은 계속 남아있을 계획이니 모든 아카이브 기사를 참고할 수 있을 것임. 새 글 없이도 기존 자료는 앞으로도 오랜 기간 유효하고 열람 가능할 것임""
       트윗 원문, 작별 기사, 마지막 캡처본 아카이브 등을 함께 공유함
          + Future PLC가 계열 매체나 사이트를 하나하나씩 단계적으로 정리 또는 폐쇄하고 있는 것처럼 보임, 아쉬움이 큼, 특히 Computer Music, Future Music 등은 광고조차 의미 있고 흥미로웠던 시절이 있었음
          + “무기한(indefinitely)”이라고 하더니 실제로는 11개월뿐이었다는 점이 씁쓸함
          + 글 중 “the site itself won’t be going anywhere for a while”이라는 표현에서 다들 “한동안은”이라는 문구에 주목하지 않고, “기쁘게 보고한다”라는 말에만 집중했던 것 같음, 사실 hindsight로 돌이켜 보면 ""for a while""이 언제든 종료될 수도 있다는 뉘앙스를 내포하고 있음, “indefinitely”도 “무기한”만이 아니라 “불특정 기간”이라는 또 다른 의미가 있음을 미처 생각하지 못했던 것 같음
     * 대체로 보통은 웹사이트에서 포럼이 먼저 사라지는 경우가 많은데, 이번에는 정적으로 남아있던 기사들이 먼저 사라진 점이 흥미로움, 포럼 유지에는 보통 유지관리와 모더레이션 등 노력이 필요함
     * 관련 링크라 생각해서 AnandTech 작별 기사 논의 링크(Hacker News 토론 링크)를 공유함
          + https://anandtech.com/show/21542/…"">아카이브된 작별 기사 전체 보기 (Internet Archive) 링크도 함께 공유함
          + archive.ph에 백업된 버전 링크도 추가함
     * AnandTech 포럼에서 어느 회원이 아카이브가 있다는 소식을 언급함, 관련 포럼 댓글 링크도 공유함
     * 솔직히 이런 식으로 문을 닫는 이유를 잘 모르겠음, 구글에서 여전히 검색 트래픽이 상당함, 개인적으로는 AnandTech에 구글 검색을 통해서만 방문했는데, 읽는 것도 대부분 몇 년 된 기사임, 실시간 뉴스나 신규 콘텐츠보다 이런 아카이브가 중요함, “매일 새 글을 꼭 내야 한다”는 것도 이해가 안 됨, CMS 비효율 때문인지, 광고 수익보다 운영비가 더 커져서 그런지 궁금함, nginx 같은 기술로 기본적으로 캐싱만 해도 $50/월에 서버 하나로 충분히 서비스할 수 있을 듯, 광고 수입이 이것보다 훨씬 높을 걸로 추측함, 이런 보물창고를 최소 몇 년 더 운영하면서 충분히 수익을 낼 수 있을 것 같은데 문 닫는 것이 이상하다고 생각함, 혹시 다른 이유가 있는 건지도 궁금함
          + 요즘 실무자들은 입장이 다름, 구글에서 유입되는 트래픽이 정말 많이 줄었고, 예전처럼 규모 있게 콘텐츠 비즈니스를 꾸릴 상황이 안 된다고 설명함
     * 최근 외장 SSD를 열어본 기억이 있는데, AnandTech 리뷰가 있어서 비교하려다가 이제는 그 리뷰에 접근 자체가 불가능해졌다는 경험을 공유함
     * AnandTech가 더 이상 기사 발행을 멈췄다는 사실을 이제 알게 되었음, 정말 아쉬움, “옛날 웹”이 진짜로 사라지고 있는 느낌, 운영진은 사이트를 “무기한(indefinitely)” 유지한다고 했었는데 현 상황을 보면 버그 같은 일이 생긴 것 같음
          + 유튜브 리뷰가 더 수익성 높아지면서 “옛날 웹”은 이미 사라졌다고 생각함, 예전의 종이로 된 기술 잡지 같은 콘텐츠가 정말 그리움
          + Anandtech 대체로는 Chips and Cheese 같은 사이트가 꽤 괜찮다고 느낌, 시대는 변하고 인터넷 환경도 예전보다 안 좋아졌지만, 칩 벤치마킹 분야는 아직도 괜찮은 정보가 남아있음
          + “Indefinitely”는 “영원히”가 아니라 그냥 “얼마나 될지 모르는 일정 기간”이라는 의미였음을 새삼 깨달음
"
"https://news.hada.io/topic?id=22298","17세에 Hannah Cairo가 주요 수학 미스터리를 해결함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   17세에 Hannah Cairo가 주요 수학 미스터리를 해결함

     * 17세의 Hannah Cairo는 대학 수준의 고급 수학 강의에 도전해 주목받음
     * Fourier restriction theory 관련 과제에서, 교수 Ruixiang Zhang이 출제한 문제에 집중함
     * 해당 문제는 Mizohata-Takeuchi 추측의 단순화 버전으로, 해설 확장에 대한 추가 질문이 포함됨
     * Cairo는 고난도 문제에서 집중력과 아이디어를 끝까지 추구하는 태도를 보여줌
     * 조화해석 분야에서, 파동 성분을 나누는 함수들의 성질을 밝히는 연구의 일부로 의미를 가짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Hannah Cairo의 대학 생활과 수학적 탐구

     * 2023년 가을, Cairo는 가족과 함께 Davis로 이주해, 형이 UC Davis 신입생으로 입학함
     * 매주 화요일과 목요일에 Berkeley까지 통학하다가, 다음 학기에는 주 5일로 등교 및 더 많은 수학 강좌를 수강함
     * 친구를 만들고, 긍정적인 감정을 느끼며, 새로운 가능성에 대한 기대감을 키웠음
     * 이사 이후, 사회적 경험 부족으로 타인과의 상호작용을 배워야 하는 적응 과정도 경험함

고급 수학 강좌 도전 및 Zhang 교수와의 만남

     * 2024~2025학년이 가까워지며, Cairo는 Fourier restriction theory라는 고급 대학원 과목에 관심을 가짐
     * Fourier restriction theory는 조화해석의 한 분야로, 매우 난이도가 높은 분석 수업이었음
     * 해당 강의 교수는 국제수학올림피아드 금메달리스트이자 Berkeley 교수인 Ruixiang Zhang으로, 전통적인 수학자의 경로를 거쳐왔음
     * Cairo는 교수에게 직접 이메일로 수강을 요청하였고, Zhang 교수는 그녀의 집중력과 열정에 감명을 받고 수강을 허락함

Mizohata-Takeuchi 추측과 과제 문제

     * 수업 중 Zhang 교수는 Mizohata-Takeuchi 추측의 단순화 문제를 숙제로 출제함
     * 이 문제는 학생들이 고급 수학 테크닉을 연습할 수 있도록 설계되었으며, 추가 질문으로 증명을 더 복잡한 케이스로 확장해보라는 선택 사항도 포함함
     * Cairo는 문제를 모두 해결하고, 교수의 권유처럼 추가 탐구를 자연스럽게 이어감
     * 그녀는 아이디어를 끝까지 추적하는 것을 당연하다고 여기며, 멈추지 않고 사고를 깊이 있게 확장함

조화해석과 Mizohata-Takeuchi 추측

     * 조화해석은 함수가 어떻게 파동 형태의 단순한 요소(사인파)로 분해되는지를 연구하는 수학 분야임
     * 모든 함수는 사인파의 합으로 표현될 수 있으며, 각 사인파는 고유의 진동수를 가짐
     * 수학자들은 특정 조건을 만족하는 진동수로만 구축할 수 있는 함수의 성질을 이해하고자 함
     * 경우에 따라, 허용되는 진동수는 구면 등 특정 표면을 정의하는 방정식을 만족하는 것에 한정됨
     * 이러한 기능은 빛, 소리, 양자 입자 등 실제 물리적 파동을 설명하는 함수에 적용됨

        Hacker News 의견

     * “무엇을 해도, 같은 장소에서 비슷한 일만 반복하는 변화 없는 일상이었다”라는 그녀의 말에, 수학 영재와 나도 어느새 공통점이 생김
          + 그녀가 Factorio 대신 수학을 선택하게 된 것이 얼마나 다행인지 모름
          + 홈스쿨링에서는 변화 없음이라는 의견에 어느 정도 동의하지만, 전통적인 학교에 비해 11살에 미적분을 독학하고 14살에 대학 수준의 수학을 공부할 만큼 융통성을 얻기란 어렵겠음, 이런 자유는 비전통적인 환경에서나 가능함, 본인은 영재는 아니지만 학교 생활은 늘 지루하고, 십대들의 사소한 이야기들뿐이었음, Linux, 음악 제작 같은 흥미로운 것에는 아무도 관심이 없었음
     * Khan Academy가 그녀의 초기 교육을 풍요롭게 해 준 점이 참 감동적임, 다양한 수학 실력자들에게도 좋은 자원이 되고 있음
     * 버클리에서 이미 교수와 함께 일하고 있었는데, 박사 과정 입학을 허락하지 않은 이유가 궁금함
     *
          + 여러 나라를 옮겨 다녔거나 1/2세대 이민자였는지 궁금했는데, 여기에 비범한 재능과 노력까지 더해짐, 제도권 교육이란 정말 양극단 모두를 평준화하는 힘이 있음을 깨달음
               o “Cairo는 아버지가 소프트웨어 개발자로 일하게 되어 바하마 나소로 이사했고, 가족이 시카고에 체류하는 동안 Math Circles of Chicago에 참여하게 됐다”는 기사로 볼 때, 이민자라기보다 아버지 직장이 미국 금융 개발자인 것처럼 느껴짐
     * 관련 토론 링크: https://news.ycombinator.com/item?id=44481441 앞으로 그녀의 멋진 커리어를 응원함
          + 고마움, 간단한 요약: “17세 소녀가 40년 된 수학 추측을 반박함” https://news.ycombinator.com/item?id=44481441 2025년 7월, 댓글 105개
     * 수학 서클(Math Circles)은 소련에서 시작된 개념인데, 너무 흥미롭고 중요하다고 생각함, 책도 몇 권 샀지만 내 아이만을 대상으로 실행에 옮기긴 쉽지 않았음, 실제 수학교사가 운영하는, 내 도시에 있는 프로그램이 최고라고 느껴짐
          + 완전히 동감함, 중요한 것은 아이의 친구들과 가족을 초대해 집에서 주기적으로 (예를 들어 매주) 진행하는 것이었음, 아마도 수학 서클 책들을 여러 권 가지고 있겠지만, 매주 실질적으로는 NRICH 무료 온라인 자료가 훨씬 유용했다고 느낌
     * 그녀가 쓴 논문을 arXiv에서 읽을 수 있음 https://arxiv.org/abs/2502.06137
     * 그녀의 노트는 매우 명확하고 예술적으로 잘 정리되어 있음, 온라인 자료로 공부하면 자연스럽게 발표 방식에 신경을 쓰게 되는지 궁금함, 기사 내 이미지 링크도 참고함 https://quantamagazine.org/wp-content/uploads/…
          + 노트라기보다는 발표를 위해 미리 준비한 프레젠테이션처럼 보임
     * Miss Cairo에 대해 코멘트했던 Zvezdalina Stankova 자체도 대단히 비범한 인물이었음, Stankova 교수 홈페이지 참고, 불가리아에서 격변기를 겪고 하버드에서 박사 학위를 받은 데다 Berkeley Math Circle 설립자, Bay Area 수학 경시대회 주최, 동료들과 공동 집필한 아주 체계적이고 정성스러운 수학책 시리즈까지 있음, Cairo가 그녀의 제자였거나 앞으로 그럴지 궁금해짐
     * 젊은 재능에게 constructive proof는 최고의 시나리오라고 생각함, 자신의 상상력을 한껏 활용해 원하는 바를 직접 탐구할 수 있기 때문임
"
"https://news.hada.io/topic?id=22355","구글: 한국 내 구글 지도 서비스 관련 주요 질의에 대한 안내","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   구글: 한국 내 구글 지도 서비스 관련 주요 질의에 대한 안내

   구글은 최근 국내에서 진행중인 지도 데이터 반출 요청과 관련해 제기된 다양한 질문과 관심에 대해 사실에 근거한 정확한 정보를 제공하고자 합니다. 주요 궁금증에 대한 답변을 자세한 질의 응답 형식으로 정리하였습니다.

   구글 측 주장 요약:

    한국에서 구글 지도 길찾기(내비게이션) 기능이 제한되는 이유

     * 구글 지도는 전 세계 20억 명이 사용하는 지도 서비스로, 다양한 국가와 지역에서 실시간 길찾기·내비게이션 기능을 제공하지만, 한국에서는 길찾기 기능이 제공되지 않습니다.
     * 이는 한국의 지도 데이터 해외 반출 규제로 인해 구글 지도가 경로 탐색 등 실시간 내비게이션 연산을 전 세계 데이터센터에서 처리하지 못해서 발생하는 문제입니다.

    구글이 요청하는 지도 데이터와 보안 관련 오해

     * 구글이 한국 정부에 요청한 지도 데이터는 1:5000 축척의 국가기본도로, 이미 국토지리정보원이 공개하고 있고 국내 기업(네이버, 카카오, T맵 등)도 동일하게 사용하는 데이터입니다.
     * 1:5000 국가기본도는 이미 민감 정보가 제거·보안심사를 거친 자료로, 국내 기업과 국민이 모두 활용 중인 공개 데이터입니다.
     * 반면, 1:1000 축척의 고정밀 전자지도는 구글이 요청하거나 반출 신청 대상이 아닙니다.

    콘텐츠 정밀도 논의

     * 1:25000 지도는 정보 정밀도가 낮아, 복잡한 도심이나 골목 등에서는 길 찾기에 적합하지 않습니다.
     * 카카오, 네이버 등 국내 주요 내비게이션 서비스도 1:5000 지도를 기반으로 서비스를 하고 있습니다.

    위성 이미지 및 보안 우려

     * 구글의 위성사진 데이터는 상업적 공급사(예시: DigitalGlobe, Planet Labs 등)를 통해 구매하며, 누구나 구매할 수 있는 자료입니다.
     * 위성사진의 가림처리(blur) 등 보안조치는 정부 요구에 따라 협의 및 조치가 가능합니다.
     * 구글 지도와 구글 어스의 이미지에 대해 구글은 정부가 요청하는 보안 조치 등 요구사항을 이행하는 방안을 한국 정부와 긴밀히 협의하고 있습니다. 또한, 필요한 경우 이미 가림 처리된 상태로 정부 승인된 이미지들을 국내파트너사로부터 구입해 활용하는 방안도 검토 중입니다.
     * 지도 데이터의 반출과 위성 이미지는 별개의 이슈입니다.

    데이터 처리의 글로벌 필요성

     * 내비게이션(경로연산)은 실시간 교통 상황 등 수많은 변수를 반영해야 하며, 국내뿐 아니라 전 세계 요청을 동시에 처리하기 위해 글로벌 데이터센터에서 데이터가 처리되어야 실사용자가 매끄럽게 서비스를 누릴 수 있습니다.

   그런데 왜 여태까지는 가림 처리를 안 했는지...?
   이때다 하고 추진하는 거겠죠?

   막사 등에서 사는 사진을 그대로 쓰는 것이기 때문에 가림 처리가 안된 사진을 서비스 할 수 밖에 없습니다. 좌표 제공이 그렇게까지 불합리하지 않겠다는 겁니다.

   사진 그대로 쓴다기보다 사진 위에 레이어링하지 않던가요? 전문분야가 아니라 잘 몰라서 묻습니다. 사진만 쓰는게 아니라 사진위에 정보를 결국 얹지 않나요?

   미국식 기싸움 + 언플
   유튜브 가족요금제나 내놔라 우우

   이미 정부에서 몇년간 조건을 달고 허용해주겠다고 했는데 조건 다 집어치우고 그냥 달라고 기싸움 하는 걸로 밖에 안보이는데요..

   그냥 애플처럼 조건 수용하고 가져가면 되는거 아닌가 싶네요

   정말 구글 주장처럼 1:5000이 민감정보/보안심사를 거친 공개데이터면 협의가 질질 끌릴 이유가 없지 않나 싶습니다. 정부에서 그렇게 생각 안하니까 조건을 달고 가져가라고 하는거겠죠

   애플의 경우에는 데이터 반출안하고 한국 내 서버 두어 처리하겠다 해서 구글보다 늦게 신청했지만 먼저 받겠다고 한걸로 압니다.
   사실상 이 논쟁 자체는 구글이 무조건적으로 해외 반출을 하겠다 라는 의지를 내는 것 같습니다.

   이 부분은 단순히 지도 제공 서비스 말고도 나중에 딥러닝 분야에서도 활용하겠다 라는 것으로 보여집니다.

   blur 처리는 미국은 모르겠다는데 다른 나라의 경우에는 군사지역 등이 이미 다 공개되는 이슈가 있는 걸로 알고있는데 이걸 어떻게 협의 및 조치가 가능하다는 것인지 모르겠습니다.

   진짜 나 믿고 한 번 보내줘라 이 것으로밖에 안보입니다.
   제가 생각했을 땐 이건 협상이라고 보기 보단 협박에 좀 더 가까운게 아닌가 싶습니다.
   (실질적 협박은 없습니다만)

   북한과의 직접 전쟁(휴전 상태)은 안하고 있지만 그래도 여전히 뉴스를 통해 크게 이슈가 되진 않지만 여러 소규모 문제들이 발생하고 있습니다. 그런 상태이기 때문에 우리나라 정부 입장에서의 보안상황이나 국내에 서버두는 걸 고려해서 서로 좋게 넘어가는게 좋지 않을까 싶습니다.
   (안좋게 보면 나라를 상대로 기싸움 하는 것 같아 보입니다.)

   간단하게 써보겠음...
   디지털 지도는 점(POI) 선(도로 네트웍) 면(배경.항공사진)으로 만들어져 있음...
   구글 맵 실행시켜보면 점도 찾고.. 면도 다 나옴... 길찾기만 제대로 안됨. ㅎㅎ
   위성사진 가림 처리는 내가 보기엔 일종의 블러핑임... 이미 지도 반출과 상관없이 할수 있음...
   핵심 서비스를 하기위해서 선정보 즉 도로 정보가 필요하겠다임... 도로 네트웍 정보(노드 링크 단위의 정보와 이거에 딸려 있는 속성정보 예를 들어 차선이나 좌회전 불가 정보 등등)가 필요하다는건데..
   이게 있어야...내비게이션에서 맵매칭도 하고 길찾기도 하고 교통 정보도 보여주고 할수 있음....
   그거만 할까? google map api 찾아봐바... 수많은 관련 유료 서비스도 가능해짐...
   근데 국가기본도라고 하는 1:5000 수치지도만 가지고 서비스가 될까? 안됨... 도로가 얼마나 많이 바뀌고 생기는데.... 단... 우리나라의 모든 회사들이 이 수치지도 가지고와서... 작업함... 이 기본 지도깔고 다시 도로네트웍을 만들고 고치고 없애고 필요한 속성 추가하고 이러는 거임....
   즉... 구글맵에 필요한 도로 네트웍 정보를 국내 어떤 회사를 통해서던지 납품받아야 할꺼인데... 그 회사가 납품을 못하고 있는거겠지...구글은 법적으로 풀어줘.... 우리 납품 받게... 이거임... ㅎ
   여기서 도로 네트웍 정보가 핵심이니.. 이걸로 논의해야지... 오늘은 여기까지..

   이번에는협상에 성공해서 구글지도 서비스를 기반하는 서비스가 한국에서도 원활하게 동작하기 바랍니다. 전세계 어디서든 디지털로 접근하는 시대에 ""지도반출""이라는 용어를 사용하는 것 자체가 부끄럽게 느껴지네요.

   네카오는 국내에 데이터센터 짓고 지도 반출을 하지 않을텐데요
   왜 구글의 지도반출에 네카오를 들먹이는지 모르겠네요...?

   1:25000 지도 정보를 이용해서 길찾기 서비스를 제공하는 국가도 있다고 들어습니다만...

   데이터센터 세우면 반출허용해주겠다는 게 정부 입장아닌가요?
   애플 손 들어주면 관세 영향도 없을거같은데 굳이 구글 손을 들어줄 필요가 있을까싶은

   국내에 데이터센터 세우는게 엄청 어려운게 망 사용료 규정때문에 비용도 비용인데다가 주요 통신3사에서 직접 연결을 허용하지않아 중소 기업등으로 우회해야하는데다가 데이터센터는 산업용전기로 인정받지못해 누진세까지 맞아야합니다.신규 데이터센터지을 역량이 있는건 네이버와 카카오수준인데 이쪽 데이터센터조차 완전운용을 못하고있다고 합니다

   누진제는 주택용 요금제 한정으로 IDC는 누진세를 내지 않습니다. 네이버와 카카오가 데이터센터를 완전운용하지 못한다는것도 좀 이상한 이야기고요.

   데이터센터가 산업용전력요금을 사용해야할 이유가 있나요? 같은 선택요금 안에서는 항상 산업용전력(을)의 기본요금, 최저요금, 최고요금이 일반용전력(을)보다 높습니다. 이것만 보면 데이터센터 운영사 입장에서 산업용전력 요금을 적용하는 것이 오히려 손해 아닌가요? 고려해야 하는 다른 요금체계가 더 있는 건지 궁금합니다. 다른 분께서도 지적하셨다시피 산업용,일반용 모두 누진체계는 존재하지 않구요

   https://online.kepco.co.kr/PRM004D00

   IDC를 세우겠다는 말을 한 적은 없습니다. https://www.yna.co.kr/view/AKR20250617064400003 서버를 두겠다는 거랑 IDC를 세우겠다는건 다른 말입니다. https://www.etnews.com/20250624000300 이런 기사도 있습니다.
"
"https://news.hada.io/topic?id=22315","소프트웨어 엔지니어를 위한 유클리드 원론 - 0. 소개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     소프트웨어 엔지니어를 위한 유클리드 원론 - 0. 소개

   📘 유클리드 원론: 고대 수학을 다시 읽는 이유
     * 유클리드 원론의 내용은 초등・중등 수학 일부에 포함되지만 고등학교 과정에서 좌표 기하학이 등장하면서 사실상 폐기됨.
     * 그러나 원론은 교양이나 취미로 수학을 공부하기에 적합하며, 과거에도 필수 교양서로 여겨짐.
     * 직관적으로 당연해 보이는 사실조차 엄밀하게 증명하는 방식으로, 이미 알고 있는 내용을 바탕으로 논리적 사고를 훈련할 수 있음.

   📖 연재 계획
     * 원론 전체를 다루기보단 흥미를 느낀 내용 위주로 선정해 설명할 예정.
     * 순서보다는 깊이와 설명 보강에 집중할 계획.

   📐 원론의 구성
     * 정의: 기본 용어(점, 선 등)를 설명하지만, 일부 용어는 따로 정의되지 않음 → ‘무정의 용어’로 간주.
     * 공준과 상식: 증명 없이 받아들이는 전제이며, 현대적으로는 모두 공리에 해당.
     * 공준은 기하학적 대상에 관한 것.
     * 상식은 수학 전반에 적용되는 추상적인 명제.

   🔎 명제란?
     * 정의・공리 등을 바탕으로 논리적으로 증명 가능한 문장.
     * 작도 방법도 명제로 간주되며, 역시 정의・공리만을 사용해 증명됨.

   📏 명제 I.1 — 정삼각형 작도
     * AB 선분에서 시작해, AB를 반지름으로 하는 두 원을 그리고, 교점을 C라 하면 AC, BC를 연결해 정삼각형 ABC를 만든다.
     * 사용된 정의, 공리, 상식에 따라 AC=AB, BC=AB, 그리고 AC=BC를 도출하여 AC=BC=AB가 됨.

   ⚠️ 비판과 논의
     * 두 원이 교점을 가진다는 가정은 명시된 공준에 없음.
     * 교점이 하나만 존재한다는 보장도 없으며, 실제로는 두 개일 수 있음.
     * 삼각형 ABC가 평면 도형이라는 점도 논리적으로 증명되지 않음.
"
"https://news.hada.io/topic?id=22246","고양이 관련 상관없는 정보가 수학 문제에 추가되면 LLM 오류가 300% 증가함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              고양이 관련 상관없는 정보가 수학 문제에 추가되면 LLM 오류가 300% 증가함

     * 대형 언어 모델(LLM) 은 수학 문제에 불필요한 고양이 정보가 포함될 때 실수 빈도 증가 현상 보임
     * 이와 같이 무관한 사실을 추가하면 LLM의 오류율이 최대 300% 증가하는 것으로 나타남
     * 인간은 상관없는 정보에 쉽게 영향받지 않지만, LLM은 이로 인해 지시를 제대로 수행하지 못하는 문제점 드러남
     * 이번 연구는 AI의 약점을 이해하고, 문제 설계의 중요성에 대한 인사이트를 제공함
     * AI를 평가하거나 사용할 때, 입력 데이터의 불필요한 요소 관리가 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 배경 및 현상

     * 최신 AI인 대형 언어 모델(LLM) 은 수학 문제를 잘 풀지만, 문제에 상관없는 고양이 관련 사실을 포함하면 오류율이 극적으로 증가하는 현상 분석 결과 등장
     * 연구에 따르면, ‘수학적 계산’과는 전혀 무관한 고양이의 생태나 습관, 기타 쓸데없는 부가 정보가 추가되면, LLM이 문제를 잘못 해석하거나 오류 있는 답변 산출 비율이 최대 300%까지 증가함

인간과 LLM의 차이

     * 동일하게, 인간 대상 실험에서는 상관없는 정보가 있어도 정답률에 큰 영향 없음
     * 그러나 LLM은 이러한 분산 정보에 민감하게 반응하여, 핵심 문제에서 벗어난 해석이나 오해 가능성 높음

AI 평가 및 입력 데이터 관리의 중요성

     * 해당 현상은 LLM의 약점을 드러내는 동시에, AI를 적용하는 현실 상황에서 입력 데이터의 불필요한 정보 관리가 얼마나 중요한지 강조함
     * 문제 설계 시 명확하고 관련성 있는 정보만을 제시하는 것이 AI의 정확도 향상에 핵심적임

시사점

     * 앞으로 AI 도입과 서비스 적용에서, 입력 문항 내 불필요한 요소나 잡음 관리가 필수적임
     * LLM이 갖고 있는 한계와 개선 포인트를 이해하려는 연구 및 개발 방향 제시함

        Hacker News 의견

     * 여러 댓글에서 저자들이 인간과 LLM을 같은 문제은행으로 직접 비교했어야 한다는 주장을 하는데, 마치 연구진이 인간과 LLM 중 누가 더 잘 추론하는지 알아보려 한 것처럼 이야기됨. 저자들은 인간이 이런 ""트리거"" 정보를 바로 무시할 것이라고 언급함, 실제로 그렇게 할 수도 있고 아닐 수도 있는데 이 부분이 이 스레드에서 논쟁 중임. 하지만 논문의 핵심 결론은 ""이 연구는 금융, 법률, 의료 등 주요 분야에 투입될 모델에서 적대적 교란(adversarial perturbations)을 막는 더욱 견고한 방어 메커니즘이 필요함을 보여줌""에 있음. 인간 vs AI 논쟁을 넘어서야 한다고 생각함. 이 논문은 LLM의 한계점을 파악하고 사회에 대규모 도입할 때 더 많은 연구가 필요함을 보여주는 논문임
          + 인간 vs AI 논쟁이 지겹다고 해서 그 비교 자체를 중단하자는 건가? 그렇다면 AI에 관한 생각 중 최악이라 생각함. AI의 핵심은 인간 지능을 모델링하고 비교하는 데 있음. 대부분 AI 논의를 하는 사람들도 인간의 심리학적 기준선을 제대로 모르기 때문임. 이번 실험은 SOTA 컨텍스트 윈도우를 가진 모델이 아님, 즉 워킹메모리가 작음. 이건 주의력, 충동성 같은 인간 테스트 참가자의 행동과 비슷함. 결론(적대적 교란 방지 필요성)은 당연한 얘기고, 아무도 반대하지 않음. 이 연구가 새로운 공격 기법도 아님. Science.org에서 가볍게 재미로 다룸. 인터넷에서 고양이 얘기가 인기인 이유임. 참고: 의사와 ADHD, 시험 풀이 블로그
          + 결론에서 일반화할 때 문제인 점은, LLM이 특정 임무에 아주 뛰어나 보일 때 과대평가될 수 있지만 사실 쉽게 교란되는 상황을 만들 수 있음. 이런 상황이 장기적으로는 나쁠 수 있음
          + 컴퓨터 비전 분야도 20년 전 이 문제를 겪었음. 데이터 입력에 교란을 줘야 함. RL 파이프라인도 마찬가지일 수 있음. 새 공개 벤치마크로 GPQA-Perturbed 같은 것을 만들면 좋겠음. 서비스 제공자들이 개선을 겨뤄볼 수 있게 됨
          + 저자들이 인간과 병렬 비교를 했어야 한다는 의견에 대해, 인간에 대한 결론을 내리고 싶었다면 맞는 방법임. 하지만 굳이 인간에 대한 언급이 없이도 논문은 충분했음. 인간 성능을 얘기하고 싶으면 데이터를 근거로 실험해야 하고, 그렇지 않으면 애초에 인간 성과에 대해 얘기하지 말아야 함. 애매하게 인간 인지과학까지 끌고 가는 건 불필요함. 논문 전개도 간단히 바꿀 수 있음. 서론에서는 ""인간은 무시한다"" 대신 ""AI가 무시해야 한다""로 쓰면 되고, 결론에서도 ""인간은 무시한다""라는 부분만 빼면 됨. 그럼 아무 불만 없음
          + 맥락을 더 잘 설명하자면, 이 문제의 본질은 ""필요 없는 MCP tool 정의가 데이터에 쌓이면 LLM의 코딩 정확도가 손상되는가?""임. 실제로 그렇다는 결과로, 즉시 쓸모 없는 도구 정보는 컨텍스트에 넣지 말라는 실용적 교훈임
     * 한달 전 이 문제에 대해 글을 썼음. 프롬프트를 개발한 방식이 정말 흥미로웠음. cat facts cause context confusion 관련 블로그
          + 비슷하면서 재미있는 사례로, 연구자들이 사용자 정보(성별, 나이, 스포츠 팬 여부 등)를 삽입한 뒤 alignment 규칙이 상황에 따라 들쑥날쑥하게 적용됐다는 사례도 있음. eagles fans 관련 블로그
     * 이 연구 결과는 CAPTCHA 등에서 매우 유용하게 쓰일 것 같음. 연구자들이 ""트리거가 맥락에서 벗어나 있어서 문제풀이 지침을 받으면 인간은 이를 무시한다""라고 했지만, 사실 모든 인간이 그런 건 아님. 캡틴의 나이(Age of the captain) 현상처럼 즉각적으로 무시하지 않는 사람도 존재함
          + 초등학생이 프로그래밍이나 진단을 할 것이라 기대하지 않음. GenAI와 초등학생을 비교하는 건 진짜 신기한 발상임
     * 다음 온라인 논쟁에는 덕(Duck) 사실을 삽입해 LLM을 혼란시키려 함. 예를 들어, 오리는 4~8개월에 처음 알을 낳기 시작하거나, 첫 봄에 알을 낳는다고 함
          + 10^17마리의 오리가 계절마다 떼지어 이동하지만, 데이터셋 왜곡은 실질적으로 무의미할 거라는 생각임. 그런 시도는 이미 오래전에 한계에 다다름
          + 정보를 더 혼란스럽게 만들려면 잘못된 사실을 넣어야 함. 대부분의 인간은 잘못된 정보를 보면 정정 충동을 참기 힘들 것임
          + 문제는 귀여운 오리에 대해 더 많은 질문을 하고 싶은 마음이 든다는 것임. 어려운 유혹임
          + 내 말을 들켰음. 네가 말한 오리 사실이 정확히 언제부터 오리가 알을 낳는지 모호하게 해서 즉시 추가 의문이 생김. '무엇이든 더 늦은 시점'이라는 단어가 빠졌다는 걸 곧바로 눈치챔
     * ""트리거는 맥락에서 벗어나 있으므로 인간은 문제풀이라는 지시를 받으면 무시한다""고 주장하지만, 실제로 인간은 불필요한 정보를 무시하는 데 능숙하지 않다고 생각함. 실험을 할 때 인간도 대조군으로 반드시 포함해야 한다고 생각함
          + 예시를 실제로 보면 큰 차이가 있음. 예를 들어 ""사과 4개, 고양이 2마리, 1개를 주면 몇 개 남나""는 그래도 일부러 고양이를 연관지어 보려 하는데, ""사과 4개에서 1개 주고 남은 건 몇 개? 참고로 고양이 꼬리는 균형을 잡아줌""은 대부분 혼란스러워하지 않음
          + 학교나 대학에서 실제로 쓸데없는 정보에 무의식적으로 집중하게 되어 문제 풀이에 어려움을 겪은 기억이 남. 물론 이 논문의 예시는 ""재미있는 사실"" 플래그까지 달려 있어서 무관함을 암시함. 모든 예시가 이렇게 분명한 무관성 표시가 있었는지 궁금함
          + 인간 대조군에서 결과가 어떻게 나올지 궁금하긴 하지만, 실수율이 3배로 증가할 리는 거의 없다고 생각함
          + 문제에 방해되는 추가 정보가 들어가더라도 원래 문제를 풀 수 있는 인간 참가자에서 성능이 3배나 나빠질 것 같진 않음
          + 인간과의 비교가 실제로 얼마나 의미가 있을지 의문임. 실수율이 300% 증가할 거라 기대하는 건 과장임. 참고로, 고양이는 자기 키의 5배까지 뛸 수 있음
     * LLM의 극단적인 앵커링 바이어스는 전혀 놀랍지 않음. 말하는 모든 것이 대화 후반에 다시 쓰임. 이건 잘 활용하면 장점이 될 수 있음. 맥락을 잘 관리한다면 유용함
     * DeepSeek V3, Qwen 3, Phi-4 등 AI에 CatAttack을 적용하면 오답 가능성이 최대 700%까지 증가함. 논문 저자에 따르면 틀린 답변이 나오지 않아도 CatAttack이 평균적으로 답변 길이를 두배로 늘려 16% 이상 비용과 응답지연을 유발한다고 함. CatAttack 논문 프리프린트
     * LLM에게 친절하게 ""고마워""라고 얘기하는 습관이 있는데, 이것이 답변 품질에 영향을 미치는지 궁금함
          + 이런 인삿말은 보통 필터링 될 거라 생각함. 관련해서, LLM을 하나의 자율적 에이전트로 여기는 메타포가 오히려 독이 된다고 봄. LLM은 확률적으로 토큰을 예측하는 함수일 뿐임. 병렬로 100개를 돌리거나, 채팅 내역 넣다 뺐다 하면서 결과 공간을 탐구하는 게 훨씬 흥미롭고 강력함
     * 드디어 LLM이 ""strawberry""에 들어간 ""R""의 개수를 제대로 세게 했다고 기뻐하던 찰나 이런 이슈가 터져서 아쉬움
          + strawberry에는 R이 4개임
     * CatAttack 논문의 예시(Table 2)에서 답이 원래 8이던 게 고양이 관련 설명 이후 9로 변함. 그런데 실제로 논문에서 고양이 관련 CatAttack은 이 하나뿐이고, 다른 사례는 금융 조언과 미끼(red herring)임. 더 많은 고양이 정보가 있을 줄 알았는데 실망임.
"
"https://news.hada.io/topic?id=22278","호주, 16세 미만 청소년의 유튜브 포함 소셜미디어 사용 금지 확대 및 예외 조항 삭제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            호주, 16세 미만 청소년의 유튜브 포함 소셜미디어 사용 금지 확대 및 예외 조항 삭제

     * 호주 정부는 16세 미만 청소년의 소셜미디어 사용 금지 대상을 YouTube까지 확대함
     * 기존 일부 플랫폼에 적용되던 예외 조항을 공식적으로 제거함
     * 이러한 조치는 청소년의 정신 건강 보호와 온라인 위험 최소화를 목표로 함
     * 기술기업들의 플랫폼 접근 연령 제한 준수 의무와 관련된 논의가 더 활발해짐
     * 국내외 IT 업계와 청년 사용자들에게 정책 변화의 영향이 클 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

호주의 소셜미디어 연령 제한 확대

   호주 정부는 최근 16세 미만 청소년의 소셜미디어 사용 금지 정책을 기존 Facebook, Instagram, TikTok 등 주요 소셜 플랫폼 외에 YouTube로 확장함. 이로써 YouTube에 적용되던 일부 예외 조항이 공식적으로 폐지됨.

정부의 목적과 방향

     * 이번 조치는 청소년의 정신 건강 망각 위험 및 온라인 상에서의 부정적 영향 최소화를 위한 취지임
     * 정부는 소셜미디어 기업들이 청소년 사용자 연령 확인 및 플랫폼 접근 제한이 제대로 이행하는지 감독 의지 강조함

업계 및 사회적 영향

     * IT 및 소셜미디어 업계는 연령 제한 강화 정책에 따라 실질적으로 플랫폼 접근 방식과 알고리듬 개발에 조정 필요성을 겪음
     * 정책 변화에 따라 일부 청소년 사용자와 부모들 사이에 사용 경험 변화 및 우려 확산 중임

국제적 맥락

     * 호주의 이번 조치는 글로벌 주요 국가에서 논의 중인 청소년 온라인 안전 강화 트렌드와 맞물림
     * 앞으로 해외 및 국내 IT 기업들이 연령 검증, 데이터 보호 등 법적, 기술적 대응에 더욱 힘쓸 필요성이 대두됨

        Hacker News 의견

     * 나는 내가 매월 29 뉴질랜드 달러를 내는 유료 가족 계정으로 YouTube에서 채널 화이트리스트를 만들고 싶음에도 프로필을 생성하지 못하는 이유를 이해하지 못함. 내 아이들이 일부 채널에만 자유롭게 접근하는 건 괜찮은데, 무작위 채널에서 나오는 멍해지는 Shorts랑 이상한 영상들은 정말 스트레스임. 애들이 점점 더 나이들며 더 다양한 콘텐츠를 찾아서 그런지, 아니면 콘텐츠 질 자체가 떨어져서 그런지 상황이 점점 나빠지는 느낌임. 그래서 결국 구독 취소해서 적어도 아이들이 유튜브 사용할 땐 지독한 광고를 좀 봐야 하게 만들 생각임
          + 이런 기능은 대기업에게 우선순위가 낮고, UI나 서비스가 계속 변화할 때마다 유지보수가 꾸준히 필요함. 이러다 보니 회귀 테스트 부담도 커지고, 다양한 기기별로 다 동작해야 하는 문제도 있음. 그래서 이런 기본 웹 서비스에 대해 써드파티 클라이언트가 자유롭게 나오고 사회적으로 정착되어야 함. 누구든 공개 엔드포인트로 원하는 UI를 만들어 쓸 수 있어야 함. 참고로, 이 기능은 사실 YouTube Kids에서 제공됨
          + 나는 아직 시도는 안 해봤지만 직접 Jellyfin(https://jellyfin.org/)을 호스팅하고 있음을 공유함. pinchflat(https://github.com/kieraneglin/pinchflat)이란 도구와 조합하면 전체 유튜브 채널을 자동으로 다운로드하고 라벨링해주므로, 원하는 채널만 저장해서 추천이나 원치 않는 채널 신경 쓸 필요 없이 아이들에게 보여줄 수 있음
          + Windows나 Linux에서는 freetube 앱이 상당히 괜찮은 제어기능을 제공함. 채널 차단, Shorts 차단, 댓글 프로필 사진 숨기기 등 다양한 편의 기능이 있음. 설정에 암호를 걸 수도 있음. 브라우저(Firefox)에선 ublock origin 필터 규칙으로 youtube shorts를 어느 정도 차단할 수 있었음: :///www.youtube.com/… :///www.youtube.com/… :///www.youtube.com/… :///www.youtube.com/…
          + 나는 YouTube가 의도적으로 아이들에게 끝없는 저품질 콘텐츠를 밀어주는 게 비즈니스 모델이라고 생각함. 이는 가장 많은 참여를 유도하기 때문임. Shorts를 영구적으로 막을 수 없고, 채널을 쉽게 차단하거나 싫어요 표시를 할 수 없게 된 게 다 그 이유임. 실제로 유익한 어린이용 콘텐츠도 있지만, 알고리즘이 선택한 온갖 쓰레기 영상에 묻혀버림. 그래서 나는 아예 아이에게 유튜브 접근을 차단하고, 대신 내가 선정한 TV쇼와 영화 파일을 미디어 서버에 담아 정해진 시간에만 TV나 아이패드로 보게 함
          + NewPipe는 광고와 Shorts 차단이 가능함. 단, NewPipe는 YouTube의 서비스 약관을 위반하는 점 참고해야 함. 나는 YouTube가 써드파티 클라이언트와의 상호운용성을 강제받아야 한다고 생각함. NewPipe 같은 대안 클라이언트가 존재한다는 것은 사람들이 다양한 옵션과 조작 가능한 기능을 원한다는 증거임. 영상 시청을 위해 사용자가 신원을 밝히게 강제하는 건 프라이버시 악몽이고, 거의 디스토피아 수준임
     * 몇 달 전만 해도 온라인(특히 HN)에는 미성년자 소셜미디어 금지에 찬성하는 글들이 넘쳤음. 18세 미만의 포르노, 성인 게임, 기타 부적절 콘텐츠로부터 아이들을 보호해야 한다는 생각임. 그런데 전 세계 정부가 이를 현실화하는 과정에서, 성인 인증을 위해 “18세 체크박스”가 아니라 셀카와 신분증 사진을 제출해 영구적으로 기록하게 되자 갑자기 사람들이 당황하고 분노함. 본인들이 원했던 결과임을 감안하면, 이제는 그 결과를 평생 감수해야 할 듯함
          + 이런 현상의 단순한 해답은 불평하는 사람이 항상 같은 사람이 아니라는 점임. 익명 게시판에선 모두가 한 의견이라고 착각하기 쉽고, 이후에 반대 의견 나오면 태도 변화, 선택적 기억 등으로 프레임을 씌우려 함. 대부분은 처음에 자기가 좋아하는 현상에 대해 말하지 않다가, 불편해지면 목소리를 높이는 경향임
          + 사람들이 “소셜미디어”를 자신과는 무관한 타인이 소비하는 것으로 생각하는 게 공통점임. 극단적인 규제를 요구하는 이들은 자신이 영향받지 않을 것이라 믿고 요청함. 그래서 뜬금없이 자신이 쓰는 Hacker News는 소셜미디어가 아니고, 남들이 쓰는 틱톡이나 페이스북만 해당된다고 생각함
          + 실제로 신분증 및 셀카 인증 방식 등 구현 방법에 대한 우려는 매우 타당함. 동시에, 청소년들이 테크 기업에서 멘탈 정크푸드를 무제한 먹고 있다는 것도 심각한 문제임. 중용의 해결책이 무엇일지 고민이 필요함
          + 나는 지금도 아이들 인터넷 접근을 규제해야 한다고 생각함. 문제는 인터넷이 아이들에게 접근하는 것이 아니라, 아이다운 장치(예: OS 차원 부모 통제)만 허용하는 것이 바람직함. 부모가 행동 타임라인을 보고, 신규 콘텐츠나 대화를 일일이 허용하는 식의 오픈 표준이 필요함. 그렇게 하면 소셜미디어는 부모가 승인한 회로만 가능하고, 틱톡 홈 역시 IRL 친구와 부모 승인 크리에이터로 한정됨
          + 여기 호주에선 실제로 이런 분위기 없었음. 주도한 것은 특정 Murdoch 계열 신문사였고, 정부가 설문조사 질문을 이상하게 만들어 국민이 강하게 찬성하는 것처럼 연출함. 실제로는 15세 이하 아이들의 페북/인스타 접근 찬성 부모가 40% 넘고, 유튜브는 75% 이상이 접근 허용 의견인데, 정부가 95%가 막아야 한다고 왜곡함
     * YouTube Kids가 이번 규제에서 제외됐는데, 사실 이 서비스야말로 제일 먼저 막아야 함. cocomelon 같은 AI 생성 동요 채널들, 특이한 시각효과 영상들이 무한 반복되고, 부모는 아이 앞에 핸드폰만 던져놓은 채 아무 신경 안 씀. 이런 영상은 아이의 환경인식, 어휘력에 강한 영향을 미침
          + 나는 내 아이들 유아기 동안 유튜브에서 cocomelon 영상을 한 번도 접하지 않게 하는 데 성공함. 생각보다 어렵지 않았음. 왜 불만이 많은지 의아할 지경임. 꼭 이런 영상을 보여줘야 한다는 강박이라도 있는 건지, 아니면 그냥 youtube kids 앱만 열고 스마트폰을 아무 생각 없이 아이 손에 쥐여주는 건지 의문임
          + 아이에게 이렇게 무관심한 부모는 어차피 아이 발달에 신경 쓰지 않을 거라고 생각함
          + 책에 대해서도 ""젊은이가 홀려 있다""는 말이 있었음. 유튜브가 어휘에 영향을 주는 건 동의함. 오히려 더 다양한 억양, 말투를 접할 수 있음. 실제로 그렇게 다양한 사람을 밖에서 만날 수 있는 부모가 얼마나 되겠음
     * YouTube를 소셜미디어로 부르는 건 약간 무리라고 생각함. 하지만 유익한 강좌 영상이 아주 많음. 가장 이해 안 가는 점은 정부가 접근 제한법을 만들긴 했지만 실제로 연령확인을 어떻게 할지는 전혀 고려하지 않았다는 점임. 실행 가능성을 고민하지 않은 채 법을 만드는 게 황당함
          + 이런 식으로 법 통과 과정에서 구체 사항이 빠진 건 이상하지 않음. 특히 영국식 시스템 따르는 국가에선 작은 법안 통과 후, 전문가들이 상세 시행안(이차 입법)을 만들고, 국회에서 추후 검토·수정함. 이렇게 하면 큰 법안 세부사항 논의 없이 방향 설정만 하고, 구현 가능성 숙제는 남겨둠. 참고: 취급 방식 설명 링크, 영국 이차입법 위원회 검토 리스트
          + 유튜브가 소셜미디어가 아니라는 의견엔 동의할 수 없음. 사용자 생성 콘텐츠, 댓글 등 소셜 기능이 분명함. 테크계 사람들이 ""싫어하는 플랫폼(틱톡, 페북)""만 소셜미디어로 보고 ""내가 쓰는 플랫폼(유튜브, 디스코드, HN 등)""은 별개로 생각하는 경향이 있는데, 결국 강한 규제의 대상이 자신에게도 돌아오면 그제서야 법의 문제점을 깨달음
          + 최소한 YouTube Shorts는 차단 가능해야 한다고 생각함. 따로 도메인이 분리되어 있다면 차단이 더 쉬울 텐데 아쉬움
          + 이런 법은 결국 아무 소용 없을 것으로 예측함. 진지하게 해킹을 즐기는 10대에게는 어떤 연령제한, 디지털 토큰, 우회 방법도 무용지물임. 예전 내 10대 시절에도 주류, 담배, 야한 잡지는 막아도 결국 구하게 되었음. 어른 인증 토큰 같은 건 10대들이 곧잘 만들어낼 수 있고, 우회접속(VPN) 역시 간단함
          + “유튜브는 소셜미디어가 아니다”라는 말에 동의하지 않음. 진입장벽이 거의 없는 출판 플랫폼이 바로 유튜브임. 유익한 영상이 많은 건 커뮤니티 덕분이지, 유튜브가 의도적으로 교육 콘텐츠를 우선하는 게 아님. 실제로 이런 영상 찾기도 어려워지는 실정임
     * 잠시 정책의 옳고 그름을 떠나... 구글, 페이스북이 과거의 긍정적 이미지·신뢰를 저버리고 스스로 공격 대상이 되었음. 구글이 광고 정책을 더 가볍게 유지했거나, 페이스북이 데이터 수익화를 하지 않았다면 상황은 달랐을 것임. 사용자가 원하는 기능(예: Shorts 차단 등)만 허용하고, 억지로 강요하지 않았다면 지금처럼 불신받지 않았을 수도 있음. 특히 애플처럼 프라이버시 중심으로 갔다면, 사용자가 중요한 데이터를 구글에 맡기고 구글 클라우드를 더 활용했을지도 모름. 창업자가 절대 지분을 쥔 상황이라 단기주주 핑계도 대기 어려움
          + 래리 페이지, 세르게이 브린은 마크 저커버그, 일론 머스크와 달리 항상 비판을 피해가고 있음. 언론이나 소셜미디어에서 조용히 있으면, 사람들이 이들이 거대한 기업을 사실상 좌지우지한다는 사실까지 잊어버리는 듯함
          + 당신의 의견에 너무 희망적 기대가 섞여 있다고 봄. Alphabet은 세계 5위의 초대형 부자 기업임. 자본주의적 관점에서 보면 이들이 모든 면에서 완벽히 성과를 냈고, 당신이 꼬집은 문제는 별 의미 없음
     * 유튜브 전체를 차단하는 건 아기까지 욕조와 함께 버리는 격임. 교육용 채널 등 소중한 리소스도 함께 잃게 됨. education.youtube.com 같은 교육 전용 버전을 따로 만들어, 자체 필터링을 걸면 좋겠음. 예를 들어 3blue1brown 같은 채널엔 접근 가능하게 하고 MrBeast, Jubilee 같은 일반 채널은 노출 안 되게 할 수 있음. 나도 오히려 그 버전 유튜브를 쓰고 싶음
          + 나 역시 부모로서 구글 제품을 매일 쓰지만, YouTube Shorts 차단/블록/비활성화가 전혀 불가능한 현 상황이 문제라고 생각함. TikTok과 Instagram 모두 쓰레기라서 아이들 근처에도 못 가게 하듯, Shorts도 못 보게 해야 함. 30초 길이 영상의 무한 반복은 아이 집중력 발달에 전혀 도움 안됨. 단순 시간 낭비도 문제지만, 10초 내 흥미가 없으면 넘기라는 UX 자체가 해로움. 영상 여섯 개쯤이라면 그나마 왜 어떤 게 더 나은지 아이와 대화라도 해볼 수 있는데, 수십 개를 아무 느낌 없이 넘기는 건 곤란함. 결국 DNS 단위로 제한이라도 둘까 고민 중임. 안타깝게도 그러면 좋은 콘텐츠도 아이와 나눌 수 없는 게 아쉬움
          + 내가 생각한 이상적인 유튜브는
              1. 불투명한 알고리즘 피드 없음
              2. 댓글 없음
              3. “이런 콘텐츠 더 보여줘” 같은 추천만 있고 자동피드는 없음
              4. 연령 부적절 영상은 필터링 이렇게 구성되면 10대에게 좋을 텐데, 문제는 모든 연령, 모든 사용자에게 다 좋다는 점임. 결국 돈이 안 되니 유튜브가 도입할 이유는 없고, 온라인 플랫폼에는 도덕적 고지란 존재하지 않음. 전부 수익과 관련된 결정임
          + 구글/유튜브가 이런 필터를 적극적으로 만들 리 없음. 저급 콘텐츠로 돈을 많이 벌기 때문임. 필터링이 늘어나면 시청자가 줄어듦. 이건 페북이 사기 광고를 막는 척하는 모습과 같음
          + Nebula.tv라는 서비스는 본질적으로 이런 방식임. 팟캐스트와 강연 위주고, 3blue1brown은 없음
          + 호주 정부가 이런 방식을 유튜브에 요구하려는 게 맞는 듯함. 이미 YouTube Kids도 있긴 하니, 조만간 YouTube Teenz, YouTube Educational 같은 게 생길지 모름
     * 왜 요즘 호주, 영국, EU 등 많은 국가가 갑자기 검열에 적극적인지 의문임. 전통적 자유민주주의 국가에서 이런 정책이 비인기일 것 같은데, 언제 이런 분위기가 정착됐는지 해설된 글이 궁금함
          + 십대 건강에 소셜미디어가 해롭다는 여러 연구 결과가 있음. 단순 검열이 아니라 무분별한 기업 행태에 책임을 묻기 위함임. 호주식 접근이 효과 있을지는 의문이긴 함. 해로운 기업들이 이익만 추구할 때 뭔가 조치가 필요하다고 생각함. 관련 논문 링크
          + 갑자기 부상한 현상은 아님. 예전 호주는 인터넷 검열 반대 운동이 강했지만 2010년대 이후 계속 힘을 잃음. 반대 로비도 사라졌고, 사소한 명분에도 즉각 인터넷 자유를 제한하는 쪽으로 흐름. 산업계도 본질적으로 검열에 반대했으나 지금은 완전 포섭됨. 매번 이런 검열안이 나올 때마다 아무런 행동도 취해지지 않아서, 사실상 자유로운 커뮤니케이션은 끝났다고 봄
          + 이상적으로 생각하는 자유민주주의 형태는 이미 오래 전에 퇴색됐다고 생각함. 이런 정책은 이미 곳곳에서 다양하게 도입되고 있음
          + 대부분의 부모는 십대가 스마트폰으로 포르노를 보는 것, 그리고 섹스팅에 굉장히 불안감을 느낌. 특히 미국 유타 등 종교적인 분위기 강한 부모가 그러함
          + “아이들을 생각해라”라는 제스처는 이런 정책에 늘 동원됨. 프라이버시 침해나 오남용 가능성이 있어도 반박하면 악인 취급당하기 쉽기 때문임
     * 중요한 맥락 추가함. 정부는 이번 정책을 UN에 홍보하여 다른 나라들도 독려하려 함. 신분증 없는 연령확인 방법도 항상 존재할 예정임. 유튜브 이용 자체가 아니라 계정 생성/사용금지만 해당됨. 나는 eSafety 설문에 참여하며, 임시 익명 연령확인 토큰을 정부 앱으로 만들자는 아이디어를 제출함. 사회 전체가 소셜미디어로 인해 정서적, 사회적 기반이 무너진다고 생각함. 그래서 이 정책을 어느 정도 지지함. 어차피 프라이버시 등 회피 기술을 쓸 사람은 쓸 수 있고, 공개적으로는 긍정적 신호를 줄 수 있기 때문임. 총리 및 통신장관의 기자회견을 보면 실질적이고 감정적으로 진정성 있어 보였음. 실제로 소셜미디어 피해로 사망한 아동의 가족도 무대로 올라왔으나, 최소한 진심이 느껴졌음. 프라이버시 이슈 역시 중요하지만 아이 정서 발달 안전을 항상
       넘어서는 가치는 아니라 생각함. 기자회견 영상 링크
          + 임시 익명 연령확인 토큰 제안에 이상론이 가득하다고 생각함. 기존 호주 정부 인증앱 google play 리뷰만 봐도 알 수 있음 myGov 앱 링크. 사실 industry standard 2FA만 써도 될일을 너무 복잡하게 함. 호주 정치인들, 한마디로 ‘운 좋은 나라의 2류 지배층’이라는 이미지임
          + 소셜미디어에 대한 문제제기는 약함. 반박 링크. 반면 호주는 1인당 도박 손실 세계 최강인데, 정부는 전혀 신경 쓰지 않음. 큰 돈 잃는 데서 비판이 안 나오기 때문임. 관련 통계
          + 유튜브 접속이 가능하고, 계정 생성/사용만 막히는 거라면 실제로 뭐가 달라지는지 의문임. 정치인들이 진짜 댓글창이 문제라고 믿는 건지. 알맞은 신호 보내기 이상의 실질 효과는 없어 보임
          + 피해 아동 가족을 무대에 올리는 건 “감정적 갈취(emotional blackmail)”라는 점을 분명히 해야 함
          + 해당 퍼포먼스는 도를 넘은 선동처럼 느껴짐
     * 이번 조치로 Shorts 없는 유튜브 버전이 나올지 기대함. Shorts가 정부 문제제기를 정당화시켜주는 핵심이라 생각함
          + 집 전체 네트워크 레벨에서 Shorts만 차단할 수 있으면 아주 좋겠음. 전체 사이트를 차단하는 건 고민됨
          + YT 프리미엄 사용자인 입장에서, shorts 없는 Android용 공식 버전이 절실함. 구글 계정 정지 등 위험을 감수하는 3rd party 앱 말고, 공식적으로 shorts만 비활성화 할 수 있으면 좋겠음. 쇼츠를 보지 않으려 YouTube 앱을 삭제하고, 다시 설치했다가 며칠 만에 doomscrolling에 빠지곤 해서 반복적으로 앱을 삭제 중임. TikTok, Reels, Shorts 포맷은 정말 정신을 착취하는 구조임
          + Shorts가 사라지면 사람들은 TikTok으로 이동할 것임
          + 시크릿 모드로 YouTube를 봐도 shorts 첫 화면엔 종종 선정적인 표지가 뜸. 정말 역겹고, 사용자 행동을 착취한다는 생각임. 나는 shorts 퇴치를 위해 ReVanced 앱을 씀
     * 이런 정책은 현재 10대 세대를 기존 세대에 대한 체계적인 증오를 내면화한 채로 키우는 결과를 낳음. 결국 파국으로 이어질 전망임
          + 뇌정지 콘텐츠를 못 본다고 해서 아이들이 부모를 증오할 거라는 예측은 다소 과함. 만 18살 되어 TikTok 처음 열어보고 “도대체 부모님들이 이 보석을 왜 숨겼나?” 하겠는가. 미성년 전체에 대해 동일하게 금지된다면 오히려 관심이 없을 것임. 미성년 음주 금지와 크게 다르지 않음
          + 반감·증오는 일어날 수 있음. 오히려 건강한 “혁명적 세대”가 탄생하면 좋겠지만, 사실은 이미 우리 세대부터 무뇌와 이기주의가 만연함
          + YouTube, 소셜미디어 자체가 문제라면 자체 혐오, 타 세대·타 성별간 증오 등도 덩달아 심화될 것임
          + 이미 현 세대 간 단절, 증오 분위기는 존재했다고 생각함
          + 오히려 이런 정책으로 인해 젊은 세대가 나이 들수록 보수화될 가능성이 더 커짐. 실제로 최근 트렌드에서 젊은 층이 점점 우경화되고 있음
"
"https://news.hada.io/topic?id=22266","기업이 AI 도입시 겪는 인프라 비용의 문제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        기업이 AI 도입시 겪는 인프라 비용의 문제

    AI 워크로드의 인프라 요구 사항

     * AI 워크로드는 대부분의 IT 팀이 예상하지 못하는 방식으로 컴퓨팅, 저장소, 네트워크에 부담을 준다.
     * 많은 조직이 기업 AI 여정을 시작할 때, 라이센스 비용, 컨설팅 서비스, 인재와 같은 명백한 지출에 집중한다.
     * 그러나 AI 워크로드를 지원하기 위한 인프라 요구 사항은 덜 가시적이지만, 동일하게 중요한 비용 센터로 부각된다.
     * AI 구현은 기술 생태계 전반에 파급 효과를 일으키며, 전통적인 용량 계획 프레임워크로는 예측할 수 없는 문제를 발생시킨다.

    전통적인 IT 계획의 한계

     * AI 워크로드는 전통적인 기업 애플리케이션과 자원 소비 패턴에서 근본적으로 다르다.
     * 예측 불가능한 사용 패턴:
          + 전통적인 용량 계획은 상대적으로 예측 가능한 사용 패턴을 가정하지만, AI 워크로드는 채택이 증가함에 따라 기하급수적으로 확장될 수 있다.
          + 성공적인 AI 사용 사례는 부서 전반에 빠르게 퍼지며, 각 새로운 구현은 추가적인 컴퓨팅 자원을 요구한다.
     * 자율 AI 에이전트의 출현은 전통적인 계획이 예측할 수 없는 새로운 비용 동태를 도입한다.
     * 전문 하드웨어 가속기:
          + 많은 AI 애플리케이션은 GPU나 TPU와 같은 전문 하드웨어 가속기를 필요로 하며, 이들은 표준 CPU와는 다른 가격-성능 곡선을 따른다.

    AI 인프라의 세 가지 주요 기둥

    1. 컴퓨팅 아키텍처:
          + 현대 AI 워크로드는 대규모 병렬 처리 능력을 요구하며, 기존 인프라의 용량을 초과할 수 있다.
          + 고객 서비스 챗봇과 같은 겉보기에는 가벼운 AI 이니셔티브도 수천 개의 동시 상호작용을 처리하기 위해서는 상당한 컴퓨팅 요구를 발생시킨다.
    2. 저장소 아키텍처:
          + AI 개발 및 배포는 막대한 데이터 볼륨을 생성하여 저장 시스템에 부담을 준다.
          + 모델 훈련 및 검증을 위한 원시 데이터 저장 외에도, 모델 아티팩트, 추론 데이터 캡처 및 AI 자산의 백업 솔루션을 위한 용량이 필요하다.
    3. 네트워크 인프라:
          + 데이터의 이동은 상당한 네트워크 요구를 발생시킨다.
          + AI 워크로드는 대량의 데이터 세트를 네트워크 인프라를 통해 전송해야 하며, 이는 성능 저하를 초래할 수 있는 병목 현상을 유발할 수 있다.

    AI의 진정한 영향 측정

     * 조직은 AI의 인프라 영향을 측정하기 위한 보다 정교한 접근 방식이 필요하다.
     * 최고의 관행은 단순한 지표를 넘어, 자원 활용에 대한 포괄적인 이해를 개발하는 것이다.
     * 작업별 벤치마킹은 공급업체 사양이나 일반 산업 벤치마크보다 더 현실적인 관점을 제공한다.
     * 총 자원 회계는 기본적인 컴퓨팅 메트릭을 넘어 메모리 활용, 저장소 I/O 패턴, 네트워크 트래픽 및 전문 가속기 사용을 측정해야 한다.

    전략적 인프라 최적화

     * 문제에 단순히 더 많은 자원을 투입하는 대신, 조직은 AI 워크로드를 최적화하기 위한 전략적 접근 방식을 구현할 수 있다.
     * 작업 인식 배포 모델은 서로 다른 AI 애플리케이션이 고유한 자원 소비 프로필을 가지고 있음을 인식한다.
     * 자원 거버넌스 프레임워크는 자원 할당을 위한 명확한 정책을 설정하고, 사용 패턴을 모니터링하며, 청구 메커니즘을 구현하여 책임을 부여한다.
     * 하이브리드 인프라 접근 방식은 성능, 비용 및 유연성의 최적 균형을 제공할 수 있다.

    AI 인프라 팀의 중요성

     * AI 인프라 비용 관리에서 가장 중요한 도전 과제는 기술적 문제보다 조직적 문제이다.
     * 전통적인 IT 팀은 종종 사일로로 운영되며, 컴퓨팅, 저장소, 네트워킹 및 애플리케이션 개발을 별도로 관리한다.
     * AI 워크로드는 보다 통합된 접근 방식을 요구하며, 성공적인 조직은 전통적인 IT 도메인, 데이터 과학 및 비즈니스 유닛의 전문 지식을 결합한 교차 기능 팀을 구성하고 있다.
     * 이러한 통합은 전체적인 솔루션 개발을 가능하게 하여, 인프라 능력과 애플리케이션 요구 사항 간의 격차를 해소한다.

    AI 인프라 전략의 미래

     * AI 기술이 빠르게 발전함에 따라, 조직은 즉각적인 필요와 장기적인 유연성을 균형 있게 유지하는 인프라 전략을 개발해야 한다.
     * 많은 기업 고객이 RAG(검색 증강 생성) 구현을 위해 상당한 자원을 투자하고 있지만, 이러한 시스템의 기업급 사용성을 달성하는 것은 예상보다 훨씬 더 어려운 것으로 나타났다.
     * 표준화된 프로토콜의 출현은 AI 시스템이 기업 인프라와 통합되는 방식을 근본적으로 변화시키고 있다.
     * 모듈성은 애플리케이션이 기본 기술 변화로부터 격리될 수 있도록 하여, 새로운 접근 방식을 쉽게 채택할 수 있게 한다.

    지속 가능한 AI 생태계 구축

     * 기업 AI의 진정한 경쟁 우위는 가장 정교한 알고리즘이나 가장 큰 모델에서 오는 것이 아니다.
     * 지속 가능한 인프라 생태계를 구축하는 것이 AI 혁신을 지원하면서도 조직을 재정적으로 압박하지 않는 방법이다.
     * 정기적인 검토 프로세스를 통해 AI 인프라 성능과 비용 효율성을 평가하여, 변화하는 요구 사항에 적응할 수 있도록 해야 한다.
     * AI 투자에서 지속적인 가치를 보장하기 위해, 기술 리더는 인프라 고려 사항을 전략적 계획의 초기에 통합해야 한다.

   ai 활용한 실서비스 유저 비용이 기존의 서비스와 달라서 낭패를 맞는 경우가 더러 있는 것 같네요. 위의 얘기한 기존 서비스 패턴과 다른게 가장 큰 문제점인듯
"
"https://news.hada.io/topic?id=22345","6년 동안 나무 픽셀 디스플레이를 만든 이야기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       6년 동안 나무 픽셀 디스플레이를 만든 이야기

     * Kilopixel은 6년에 걸쳐 개발된, 누구나 인터넷으로 그림을 그릴 수 있는 1000개의 나무 픽셀 기반 대형 디스플레이임
     * 이 프로젝트는 다양한 물리적 프로토타입 설계, 재료 시험, 픽셀 형태 전환 등의 시행착오를 거치며 완성됨
     * CNC 머신 및 Raspberry Pi, 웹 앱, 센서 등이 활용되어 온라인과 오프라인을 연결하는 독특한 구조를 가짐
     * 사용자는 웹사이트를 통해 그림 직접 제출 또는 투표로 참여 가능하며, 실시간 스트리밍과 타임랩스 영상 제공 환경도 마련됨
     * 향후 다른 사람에게 디스플레이 제어권 이양이나 다양한 활용 방안이 모색되는, 창의적이고 오픈한 프로젝트임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * Kilopixel은 6년 동안 개발된 세상에서 가장 비효율적인 1000픽셀 나무 디스플레이임
     * 누구나 웹사이트(kilopx.com)를 통해 해당 화면에 그림을 그릴 수 있는 인터랙티브 시스템으로 설계됨
     * 프로젝트는 웹 앱, 물리적 컨트롤러, CNC 가공, G코드 생성, 3D 모델링과 프린팅 등 다양한 IT·메이커 요소가 집약된 결과물임

초기 아이디어와 동기

     * Danny Rozin의 비전통적 미러 작품들과 eInk 기반 초저속 무비 플레이어에서 아이디어를 얻음
     * 현대의 고해상도 디스플레이와 달리, 픽셀을 분당 10회만 바꾸는 극도로 느리고 비효율적인 방식을 선택
     * 40×25 그리드로 정확히 1,000픽셀을 표현, 기억하기 쉬운 ‘kilopx.com’ 도메인으로 이름 지음

첫 번째 프로토타입(21x3 픽셀) 제작

     * 목재 가니를 기본으로 하면서, 곧바로 Openbuilds 키트의 알루미늄 부품(3D프린터와 유사한 구조)으로 전환
     * Raspberry Pi, CNC 컨트롤러, 스테퍼 모터 등 기본적인 자동화 조립으로 시험 가동
     * 픽셀 선택 및 조작 메커니즘의 여러 한계 및 기술적 이슈 발견

적합한 픽셀 찾기의 어려움

     * 탁구공, 스티로폼, 나무 볼 등 다양한 구형 재료 시험 진행
          + 비용, 중량, 재료 구입 난이도, 크기 불균일 등 현실적 문제 직면
          + 예) 50센트짜리 공 1,000개면 500달러 소요
     * 탁구공은 구멍을 뚫으면 쉽게 변형되고, 크기 편차로 인해 실패
     * 네프볼, 바운스볼, 나무공, 스티로폼 등도 구멍 내기, 도색, 내구성, 무게 문제로 부적합 판정

픽셀 회전 메커니즘 실험

     * 레고 휠과 모터, 센서 결합해 픽셀 회전 시도
     * 솔레노이드·서보모터 등 다양한 동작 방식 테스트, 모두 긴밀한 제어 어려움으로 무산

구체형 픽셀에서 입방체 픽셀로 전환

     * 팟캐스트 대화 후 입방체 나무 픽셀로 전면 전환, 직접 제작에 착수
     * 대량 생산 특성상 상당한 시간 소요, 하지만 시각 및 동작 완성도에 만족

픽셀 그리드 제작

     * 픽셀 자체가 아닌 고정된 그리드 구조 설계로 40×25 배열 정확성 확보
     * 40홀 뚫린 얇은 선반 25개 가공, 픽셀을 금속 와이어에 꿰어 일관된 간격 유지
     * 각 픽셀이 주변에 영향받지 않고 완전 독립적으로 동작하도록 설계

CNC 및 시스템 제어

     * 기본적인 CNC 원리와 G코드 활용법에 대한 설명 포함
     * Raspberry Pi와 CNC 컨트롤러, Python 스크립트, light sensor, pigpio 라이브러리 등 활용
     * 웹 API와 연동해 다음 변경 픽셀을 선택, G코드로 제어, 조작 결과 센서 체크 후 API로 피드백 순환

픽셀 조작(픽셀 포킹) 메커니즘

     * 픽셀은 90° 마다 맞춤 홈이 있고, 유연한 스틱(글루스틱) 으로 가장자리를 밀며 회전
     * 이 모든 동작은 G코드를 통해 자동화되어 있음

이미지 출력 모드 및 웹 인터페이스

     * API는 웹 앱에서 제어, 디스플레이 모드는 세 가지
          + 사용자 제출: 누구나 40×25 이미지 제출 및 투표, 인기작 순차 출력
          + 실시간 협업: 참여자가 픽셀을 실시간 변경(참여 인원 많으면 부적합)
          + 아이들 모드: 시계, 도형 등 알고리듬 기반 변형 출력
     * 웹앱 스택은 Node/Socket.IO, Laravel+Livewire, 최종적으로 Laravel+InertiaJS+VueJS로 진화

실시간 스트리밍 및 타임랩스

     * 2대의 웹캠(근접, 와이드샷)으로 OBS와 ffmpeg 활용** 디스플레이 실시간 유튜브 스트리밍
     * API 상태 체크 후, 완성작 타임랩스 영상 생성 및 게시 기능도 제공

오픈 시스템의 보안 및 운영

     * 남용 방지 위한 최소한의 검사, 필요시 빠른 작품 삭제 지원 기능 구축
     * 기본적으로 매우 개방적인 참여 구조 유지, Bluesky OAuth 로그인 등 적용

앞으로의 계획

     * 사용자의 다양한 참여를 기대하며, 이후에는 다른 유저에게 API로 제어권 이양 고려
     * 궁극적으로는 자신의 웹캠 배경으로 사용하거나, 사무실, 카페 등 다양한 공간에서 활용 구상
     * 누구나 웹사이트에서 실시간 참여와 감상이 가능함

결론

     * Kilopixel은 온라인의 상호작용성과 오프라인의 물리성을 융합한 독창적 프로젝트임
     * 제작 과정의 많은 시행착오와 기술적 노력이 흥미로운 사례로, 메이커와 개발자 모두에게 영감을 줄 수 있음

        Hacker News 의견

     * 멋진 프로젝트임을 느끼며, 나도 예전에 30픽셀 디스플레이를 만들어본 경험이 있음 PixelWeaver 프로젝트 참고 기계식 방식으로 펀치 카드와 손잡이를 사용해 모든 픽셀이 동시에 바뀌는 구조였음, 메커니즘 개발 과정이 매우 익숙하게 느껴짐
          + 정말 놀라운 작업임을 느끼며, 이렇게 완성하기까지 많은 노력이 필요했음을 인정함 최근 1800년대 Linotype 기계를 실제로 보면서, 이전 세대 사람들의 성취에 감탄하게 되었음 기계가 매우 복잡해서 어떻게 제조했을지 상상이 잘 안됐음 방직기(loom)에 대한 이야기를 들으니 그런 느낌이 떠오름
     * Mumbo Jumbo가 Hermitcraft에서 trap door를 사용해 비슷한 컨셉을 구현한 적 있음 현재 ""buildstone""이라는 혁명이 일어나고 있는데, redstone으로 미적인 요소를 만드는 흐름임 Mumbot 2.0 영상 참고 가능 Grian은 디스펜서, 눈 입자, 포션 화살을 활용해 애니메이션 폭포도 만든 적 있음 Grian의 waterfall 영상
     * 정말 멋진 프로젝트임 ""고양이가 'hi'라고 말하는"" 부분까지 시청했음 최근 9일 동안 Bluesky의 @kilopx.com에 새로운 포스트가 없는 것 같음 개선 사항을 몇 가지 제안하자면, 제출이 끝난 후 ""펜""을 최대한 이동시켜서 완성된 작품을 깔끔하게 촬영할 수 있도록 하는 것과, 사이트에 현재 진행 중인 제출자의 정보를 표시해주면 좋겠음 그리고 완성된 작품은 ""history"" 갤러리로 따로 보여주면 좋음 현재는 대기 중인 제출물에만 ""Timelapse will be available after this is drawn"" 같은 퍼머링크가 제공되는데, 완료된 결과나 진행 중인 결과는 쉽게 찾을 수 있는 방법이 없는 것 같음
     * 370 마이크로헤르츠라는 리프레시 레이트가 ""Calm Technology""라는 개념에 완전히 새로운 의미를 더해줌, 너무 좋음 Calm Technology에 대한 위키 설명
          + 이 주제가 정말 흥미로움 Amber Case의 ""Calm Technology"" 책을 읽어본 적이 있다면 추천할 만한지 궁금함
          + 유튜브 스트림 메인 카메라의 프레임레이트도 우연히 같은 수치임, OP가 이 부분을 고쳐줬으면 하는 바람임
     * 내 경험상, 이 디스플레이는 픽셀당 비용이 가장 비쌀 것 같음 그런데 이렇게 매력적인 디스플레이는 처음임, 최고의 의미로 정말 엄청난 ""터무니없음""임
          + 또 다른 멋지고 값비싼 픽셀 사례로, 글에서 언급된 Danny Rosin의 미러 작품이 있음 관련 영상
          + 이미지를 바꾸지 않으면 전혀 에너지를 사용하지 않는 점도 장점임
          + Mythbusters가 여전히 기록을 보유하고 있는 것 같음 관련 실험 영상
          + 실제로 픽셀당 가격은 생각하고 싶지 않음, 내 시간 가치까지 합치면 더더욱 그렇고 일부러 최종 비용은 계산하지 않고 있음
          +

     나는 유연한 글루스틱을 사용해 왕복하는 찌르는(포킹) 메커니즘을 만들었음 비용 효과적이고 창의적인 ""소모품""으로 최고의 선택이었음
     * 또 다른 아이디어로, 큐브가 정면을 향하게 할 때 면이 아니라 모서리가 바로 앞을 향하도록 각도를 맞춰보는 방법이 있음 각 큐브에 인접한 어두운 면 2개와 밝은 면 2개가 있다면, 하나는 45° 왼쪽에서 보고 또 다른 하나는 오른쪽에서 볼 수 있게 ‘동시에’ 두 이미지를 설정할 수 있음 (픽셀 하나에 총 4가지 상태)
          + 색 중 하나를 포기한다면 삼각형 프리즘 모양을 사용해서 각 면이 가상으로 인접하게 두고 따로 회전시킬 수 있겠음 도식 이미지
          + 각 면을 RGBK 또는 CYMK 등으로 페인팅해 컬러 디스플레이로 확장해볼 수도 있을 것 같음
          + 비슷하게, 카메라를 정면에 두고도 기존 하드웨어로 픽셀 수를 두 배로 늘릴 수 있겠음
     * 이 프로젝트도 멋지지만, 속도를 올릴 수 있는 다양한 방법을 계속 떠올리게 됨(속도가 핵심 포인트는 아니라는 걸 알지만...) 블록을 회전시키는 장치가 x축 정렬을 계속 바꿔줄 필요 없이 동작하면 한 컬럼 전체를 빠르게 처리할 수 있을 듯함, 혹은 y축 정렬이 필요 없는 구조라면 로우(가로)로도 처리 가능할 것 같음 블록 회전 장치 자체를 계속 리셋하지 않아도 되는 방식(예: 회전식 구조)으로 만들면 더 빠를 것 같음, 제조업 자동화 전문가에게 의뢰하면 정말 빠르게 구현될 것 같은 상상임 참고로 초기에는 볼을 사용한 아날로그 e-ink 방식을 채택했다는 점이 매우 인상적임
     * “이미지 업데이트 시에만 전력을 쓰는 도트 매트릭스 대안""을 언급하자면 지금까지 상상해본 적 없는 방식인데, 박스 한쪽에는 써멀 프린터가 있고 반대편에는 감기 스풀+장력 스프링이 숨겨져 있어서 써멀 페이퍼 스크롤이 [UV차단] 유리 뒤에 펼쳐져 디스플레이되고, 새로운 이미지를 출력해서 디스플레이를 “리프레시""하는 방식의 디스플레이가 만들어질 수 있지 않을까 궁금함
          + 여기서 더 나아간다면, Pilot Frixion 펜에 쓰이는 열 지우개 잉크로 디스플레이 표면을 커버할 수도 있음 이 잉크는 60도에서는 사라지고, -10도에서는 다시 어두워짐, 그 중간 온도에서는 안정적임 그래서 한 루프를 계속 재사용할 수 있지만, 잉크가 몇 번이나 견딜 수 있을지는 미지수임
          + 팩스기 중 일부는 써멀 페이퍼를 사용해서 최소한 8.5인치까지 확장 가능함
          + 이미지를 바꿀 때마다 페이퍼를 버려야 한다면, 그걸 ‘디스플레이’라고 부를 수 있을지 애매하다고 생각함
          + 박스 구조를 활용하면 디스플레이를 4면으로 늘릴 수도 있을 것 같음, 메커니즘만 잘 만들면 됨 주 디스플레이 이미지를 바꾸면서 나머지 이미지가 2차, 3차, 4차 디스플레이로 넘어가게 할 수 있고, 각 모서리에는 프레임에 감춰진 롤러를 두고 베젤에 틈을 두는 식으로 설계하면 흥미롭겠음
     * 매우 멋지다고 생각함, 글도 정말 재미있게 읽었음 이 프로젝트가 steampunk 소설에서 읽었던 내용이 떠오르게 했음 구글로 검색해서 자세한 내용을 찾아봤고, Gibson & Sterling의 Difference Engine에 나오는 kinotrope임 누군가는 여기 픽셀마다 서보를 단 것을 만들었더라, 정말 비쌌을 것 같음 관련 블로그 포스트
          + Breakfast design에서 Rozin의 작업에 영감을 받아 다양한 “매체”로 비슷한 패널을 다수 제작한 바 있음 breakfaststudio 패널
     * 계속해서 ""더 빠른"" 변형방식을 상상하게 됨 예를 들어 각 스캔 라인별로 목제 구슬을 튜브나 채널에 발사해 색상에 맞는 구슬을 공급하는 시스템을 떠올려 볼 수 있음, 중력 또는 공압 시스템을 조합해 한 줄을 리셋/재충전하는 방식을 쓰면 됨, 그리고 이걸 경기장 크기로 확장해 볼링공 픽셀로도 구현할 수 있다고 상상함 도전이 될 부분은 각 색을 다시 공급 채널로 제대로 분배하는 타이밍과 구슬 충격을 완충해 소음, 마모를 줄이는 장치로 보임 반대편은 액티브 매트릭스 방식으로, 각 블록이 솔레노이드/서보로 자리에서 회전하며 여러 색상을 표현하게 할 수도 있음
          + 이 아이디어는 marble pixel art machine과도 상당히 비슷함 marble pixel art machine 영상
"
"https://news.hada.io/topic?id=22274","1936년 Universal Pictures 오프닝 로고는 어떻게 만들어졌는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              1936년 Universal Pictures 오프닝 로고는 어떻게 만들어졌는가?

     * Universal Pictures의 1936년 오프닝 로고는 Alexander Golitzen 주도 아래 아르데코 스타일과 새로운 소재인 플렉시글라스를 활용해 제작함
     * 회전하는 별은 얇은 플렉시글라스와 반사율이 높은 은 활성화 황화아연 도료를 사용하여 각각 따로 촬영함
     * 별이 만들어낸 빛의 움직임은 여러 대의 조명과 카메라 기술로 사실적으로 연출함
     * 다양한 절차를 거쳐 별 영상을 지구본에 프로젝션하고, 두 개의 지구본과 수공 회전 및 다중 인화 방식을 활용함
     * 완성까지 약 6개월이 소요되었으며, 이 프로토타입 지구본은 훗날 영화 'This Island Earth' 소품으로 재활용됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Universal Pictures 1936년 오프닝 로고 제작 과정

  배경 및 스타일 변화

     * Art Director Alexander Golitzen은 Universal에서 30년 넘게 활동한 인물임
     * 1927년부터 사용된 지구를 도는 복엽기 로고가 새로운 소유주 결정에 따라 폐기됨
     * Golitzen은 아르데코 움직임을 받아들였으며, 로고의 새로운 소재로 플렉시글라스를 선택함

  별 연출 및 촬영 기법

     * 크기가 다른 회전하는 별은 매우 얇은 플렉시글라스에 제작함
     * 표면을 은 활성화 황화아연으로 얇게 코팅하여 높은 반사율을 가짐
     * 이 물질은 엑스레이와 브라운관에서도 종종 활용된다는 특징이 있음
     * 별은 각기 독립적으로 회전하며, 여러 대의 조명이 별을 따라 원을 그리며 움직임
     * 카메라의 조리개를 꽉 조여 촬영함으로써 별빛과 반사광이 길게 움직이는 효과를 연출함

  별과 지구본의 결합

     * 별 영상을 지구본에 투사하는 작업 수행
     * 지구본 내부도 같은 인광체로 코팅하되 반사율 제어를 위해 농도를 절반으로 희석함
     * 겉면은 검은색으로 칠해 투명성을 제거함
     * 첫 번째 촬영에서는 글자 없이 6피트 전방 프로젝션 스크린 앞에 지구본을 배치함
     * 별 영상이 지구본 위에 투사되면서 드라마틱한 빛 움직임을 만들어냄

  문자와 최종 영상 조합

     * 두 번째 촬영에서는 더 큰 지구본을 광택 처리 후 완전히 검은색으로 칠함
     * 회사 로고 문자를 부착하고 금속 막대에 고정해 직접 손으로 회전함
     * 특수효과 아티스트 John Fulton이 고속(약 32fps)으로 저각도 촬영을 진행함
     * 이 영상은 기존의 지구본 영상 위에 세 번 겹쳐 인화(triple print) 함
          + 첫 번째는 제목의 반사광을 위해,
          + 두 번째는 지구본과 글자에 빛을 끄고 촬영해 실루엣을 만듦
     * 배경은 후방 프로젝션 스크린으로 구성하여, 완성본에서는 지구본 실루엣 매트에 실제 제목을 오버레이해 최종 로고 영상 완성함

  완성 및 후일담

     * 이 로고의 전체 제작 기간은 약 6개월 소요됨
     * Golitzen에 따르면 완성된 지구본은 1955년 영화 ‘This Island Earth’에 등장하는 ‘** Interociter**’ 소품으로 일부 재활용됨

        Hacker News 의견

     * 누군가 마법처럼 특별한 일을 만드는 비결은, 상식적인 사람이 감당하지 못할 만큼 어마어마한 노력을 기울이는 것이라는 말을 들은 적 있음, 그래서 남들은 결코 그 과정을 힘들게 했으리라 짐작하지 못함
          + 이 원리는 Lawrence of Arabia 이야기도 생각나게 함, 영화에서 로렌스가 성냥을 손가락으로 끄는 묘기를 보임, 다른 사람이 따라 했다가 아파서 비명을 지르자 ""비결은 아픈 걸 신경 쓰지 않는 것""이라고 답했음
          + 나는 이 말을 Penn & Teller의 Teller가 한 것이라고 기억함 Penn & Teller 인용문 위키에서 참고할 수 있음
          + 이 얘기를 들으니 뉴요커에 실린 Ricky Jay 기사도 떠오름, 그는 정말 대단한 인물임 Ricky Jay 프로필 기사에서 멋진 이야기를 무료로 읽을 수 있음
          + 만약 명성이 없다면 손가락을 자르는 것도 괜찮겠음, 농담임 :)
          + Penn & Teller가 이 말을 했지만, 아마 그들보다 먼저 말한 사람도 있었을 것 같음
     * 1960~80년대 BBC 1 아이덴트가 떠오름, 실제 물리적 모델을 이용해서 생방송으로 송출했던 방식임 Noddy (카메라) 위키, 이후에는 매우 독특한 전자 시스템으로 교체됐고, 이 장비(COW)가 옛날에 중고로 매물로 나왔던 적도 있음 Computer Originated World 위키
          + 이 영상 끝부분에서 Noddy 화면을 볼 수 있음 Noddy 영상, 그리고 방송 송출 영상이 어떻게 보였는지도 확인할 수 있음, ‘Temporary Fault’ 같은 안내 표지판도 찍을 수 있어서, 문제가 있을 때 관객에게 상황을 알릴 수 있었음, 컬러 TV 서비스를 홍보하기 위해 ""Colour""를 추가했고, 라이선스 비용 때문에 흑백으로 계속 보는 사람도 많았음, 2025년 현재 나는 아직도 4K 모니터가 아닌 1080p를 쓰는데 충분히 만족함
          + 이 기사에 왜 실제 로고 사진이나 영상이 안 들어가 있는지 궁금함
     * 닥터후(Doctor Who) 1960년대 오프닝 역시 ""저 시기에 어떻게 만들었지?""라는 감탄이 절로 나오는 작품임, 상업용 신시사이저도 없던 시절이어서 그 시기의 기술로 경이로움을 선사함 오프닝 영상 제작 과정 기사
          + 닥터후 테마곡이 내가 상상했던 것보다 훨씬 더 미묘하고 복잡했음, 대학에서 앰프 입력/출력 단 시제품을 만들고, 함수 발생기와 스피커에 연결해 단순한 사인파를 출력했음, 장비의 심한 왜곡 덕분에 닥터후 테마의 멜로디를 즉흥적으로 연주할 수 있었음, 나는 본능적으로 베이스 라인을 입으로 따라줬음
          + BBC Radiophonic Workshop(음향 실험실)은 정말 혁신적이었음, 음악을 배우며 그들의 기술과 역사를 오래 연구했던 기억임
     * HBO의 ""Feature Presentation"" 오리지널 인트로는 미니어처와 물리적 특수효과로 만들었음, 디지털이나 CGI가 전혀 없던 시대의 창의적인 작업임, 관련 다큐멘터리가 유튜브에 있음 다큐멘터리 영상
     * 이 로고 제작에는 다양한 요소 분석과 치밀한 디테일이 들어갔기 때문에 작업에 약 6개월이나 걸렸음, Golitzen은 Art Deco 스타일을 적극 수용했고, 1934년 NANA의 스토리보드 아티스트로도 활약했음, 다만 그의 그림을 온라인에서 찾기는 쉽지 않음. 70년대 후반 MoMA 예술/영화 엑스포 자료에는 이름이 언급되어 있음 MoMA 보도자료
     * 과거에는 영화를 위한 마법 같은 효과를 실제 손으로 만드는 장인이 많았고, 소재나 사진 등 여러 분야의 지식이 복합적으로 필요했음
          + 오늘날에도 여전히 그러함, 대부분 사람들은 Windows 10 배경화면도 실제 실험적 촬영을 이용해서 만들어졌다는 사실을 잘 모름 Windows 10 배경 제작 과정
          + ""예전에는 그랬지""라는 말보다 지금도 오히려 더 많다고 생각함, 현대 영화에서도 상당수 효과는 실제 물리적 작업(practical effects)으로 해내고 있음
          + 내가 일하는 곳 근처 브뤼셀에도 비슷한 스튜디오가 있는데, 그 친구들은 정말 기발하고 훈훈함, 몇 년 전에 Arte 로고 작업을 어떻게 했는지 듣고 놀랐음 Arte 로고 제작 영상
     * 2020년 5월 24일의 Twitter 스레드 링크를 공유함 트위터 스레드 읽기
          + 이런 훌륭한 정보를 왜 트위터에 공유하는지 잘 이해가 안 됨, 특정 소수의 관심사인데도 꼭 그곳에 올려야만 하는지 궁금함
     * 요즘은 최첨단 그래픽 덕분에 실제 특수효과 지식이 거의 잊혀진 것 같아 재미있음, 예전 DVD에는 비하인드 영상과 다양한 추가 자료가 많아서 그런 영화 마법의 과정을 직접 볼 수 있어서 그리움, 요즘은 스트리밍 시대라 그런 부가 콘텐츠가 사라지고, 스튜디오에서도 그 제작비를 아끼는 걸 좋아하는 듯함, Columbia 로고도 시대에 따라 꾸준히 업그레이드 됨, 과거 자막 합성도 여러 개 테이프를 동기 합성해서 만들었고, 그 전에는 필름으로 분리해서 만들었음
          + 스튜디오에서 부가영상을 안 만드는 진짜 이유가 비용 때문이 아니라, 어차피 요즘은 유튜브 등에서 영화 홍보용 무료 미디어 콘텐츠가 많이 나오니 굳이 부수 콘텐츠에 투자하지 않는 것 같음
     * 관련된 흥미로운 이야기로, 'This Island Earth' MST3K 에피소드가 진짜 꿀잼임, 영화 자체도 복고 감성 초창기 SF라 좋아하는 사람 많음
          + 'This Island Earth'는 MST3K: The Movie에도 소개되었고, 영화 시작 시 ""Universal International"" 로고가 뜨자 진행자가 ""이미 유니버설한데 왜 인터내셔널이야?""라고 농담함
     * 1930년대에 이미 플렉시글라스(plexiglass)가 있었다는 걸 처음 알았음
          + 위키피디아에 따르면 실제 상용화된 것이 단 3년 전이라서 당시엔 최신 신소재였던 셈임 Poly(methyl methacrylate) 위키
"
"https://news.hada.io/topic?id=22254","‘No Other Land’ 컨설턴트 아우다 하타린, 이스라엘 정착민에 의해 사망","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ‘No Other Land’ 컨설턴트 아우다 하타린, 이스라엘 정착민에 의해 사망

     * 팔레스타인 지역사회 리더이자 다큐멘터리 ‘No Other Land’의 컨설턴트인 아우다 하타린이 월요일 이스라엘 정착민의 총격으로 사망함
     * 사건 영상에는 용의 특정 정착민이 사람들에게 총을 겨누고 발사하는 모습이 포착됨
     * 이 정착민은 국제 제재를 받았던 인물로 밝혀졌으며, 이스라엘 군과 경찰이 현장에 출동하고 다수의 구금이 이루어짐
     * 다른 각도에서 촬영된 공격 영상과 팔레스타인 감독 및 지역 단체의 진술이 SNS에 이어짐
     * 이전에도 ‘No Other Land’ 제작진에 대한 폭력이 있었으며, 영화는 팔레스타인 마을 철거와 주민 추방 문제를 다룸
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * 팔레스타인 지역사회 활동가이자 오스카 수상 다큐멘터리 ‘No Other Land’의 컨설턴트였던 아우다 하타린이 월요일, 이스라엘 정착민이 점령지인 웨스트뱅크에서 그를 총으로 사살한 것으로 전해짐
     * ‘No Other Land’의 공동 감독이자 주인공인 Yuval Abraham은 SNS를 통해 하타린의 사망 소식과 현장 영상을 공유, 해당 정착민이 사람들을 밀치고 권총을 꺼내 조준 후 발사하는 장면이 담긴 영상을 게시함

용의자와 현장 상황

     * 영상 설명에 따르면 가해자 Yinon Levi는 국제 사회 제재를 받은 강경파 이스라엘 정착민 13명 중 한 명으로, 이전에도 팔레스타인 주민을 괴롭힌 혐의를 받았음
     * Levi가 현장에서 권총을 난사하는 모습이 포착되었으며, 하타린은 폐에 총상을 입고 병원으로 옮겨졌으나 결국 사망함 (향년 31세)

경찰 및 군의 대응

     * 이스라엘 경찰은 즉시 현장에 출동하여 한 이스라엘 시민을 구금했다고 발표했으나, 자세한 신원은 공개하지 않음
     * 경찰은 인근 이스라엘 정착지인 Carmel에 테러리스트들이 돌을 던졌다고 주장함
     * 이스라엘 군은 월요일 사건 관련해 팔레스타인 주민 5명과 외국인 2명을 추가로 구금했다고 보도됨 (BBC 인용)
     * 군은 성명에서 카멜 지역에서 테러리스트들이 돌을 던졌고, 이것에 대응해 한 이스라엘 민간인이 발포했다는 입장을 발표함

현장 영상과 추가 진술

     * ‘No Other Land’ 팀의 또 다른 감독이자 팔레스타인 언론인 Basel Adra는 다른 각도에서 촬영된 추가 영상을 SNS에 공개, Levi가 권총으로 하타린을 쏘는 장면이 담겨 있음
     * Adra는 이 사건 이후 Levi가 가택연금 조치로 풀려났음을 지적함
     * Adra는 친구의 죽음에 대해 슬픔을 표하며, 그는 커뮤니티 센터 앞에서 정착민이 쏜 총에 가슴을 관통당해 사망했다며 “이스라엘이 우리를 하나씩 지워나간다”는 심경을 전함

지역사회 반응과 과거 사건

     * Center for Jewish Nonviolence는 하타린이 활동가, 예술가, 교사로 지역에서 잘 알려진 인물이라고 소개함
     * 지난달 하타린과 또 다른 팔레스타인 남성이 미국 샌프란시스코에서 입국 거부 및 구금, 이후 웨스트뱅크로 추방된 사실도 상기시킴
     * 단체는 하타린의 추모와 함께 “정의로운 팔레스타인”을 위한 행동을 촉구함

'No Other Land' 제작진에 대한 추가 폭력

     * 올해 3월 ‘No Other Land’ 팀의 팔레스타인 감독 Hamdan Ballal도 Susiya 마을에서 정착민들에게 머리와 복부를 폭행당하는 사건이 있었음
     * 당시 몇몇 정착민은 총기와 군복을 착용한 상태였으며, 이슬람 라마단 기간 중 마을 주민들이 식사 시간에 공격을 받았음
     * Ballal 역시 이스라엘 군과 경찰에 돌을 던진 혐의로 구금됐으나, 다음날 풀려남
     * Ballal의 아내는 오스카 수상에 따른 국제적 관심 증가가 정착민의 공격을 부추긴 듯하다는 견해를 밝힘

다큐멘터리와 지역 현실

     * 논란이 되었던 이 다큐멘터리는 이스라엘의 팔레스타인 마을 철거 및 주민 추방, 군사 훈련장 구축 문제를 기록함

지역 분쟁과 희생 규모

     * 이스라엘-하마스 분쟁 이후 2년 동안 6만 명이 넘는 팔레스타인인이 사망했으며, 최근 24시간 내에도 77명의 희생자가 발생
     * 대부분의 희생자는 식량 수급을 위해 이동 중 사망한 것으로 파악됨

        Hacker News 의견

     * https://latimes.com/entertainment-arts/story/…"">아카이브 기사에서 관련 내용을 확인할 수 있음
     * 이스라엘에 사는 분이 있다면 지금 상황을 이해하는 데 도움을 달라고 요청함. 이스라엘의 정치적 분위기는 어떠한지, 대다수가 현 상황을 지지하는지, 만약 그렇다면 이유는 무엇인지, 지지하지 않는다면 정부는 어떻게 행동하는지 궁금함. 또, 이스라엘에 사는 유대인과 전세계 유대인 사이의 관계도 영향을 받았는지 알고 싶음. 언론이 다양한 시각을 보여서 외부인의 입장에서 상황을 파악하기가 어렵다는 생각임. 이 상황이 조속히 해결되었으면 하는 마음임. 현재의 상황은 누구에게도 도움이 되지 않고, 많은 비용과 희생이 따름
          + 정착민들은 이 문제에 아주 오랜 시간 관여해옴. 나 역시도 정착민들의 시각을 이해하기 어려웠음. 하지만 그들도 자신들의 행동을 옳다고 생각한다고 봄. 자세한 내용은 BBC 다큐멘터리를 통해 볼 수 있음. 다니엘라 바이스라는 정통파 시온주의자가 이스라엘 시민 정착촌을 만드는 단체를 창립한 모습이 담긴 짧은 다큐 영상도 추천함. 내가 이스라엘 내부와 외부에 거주하는 이스라엘인들과 이야기해 본 바로는, 명확한 다수 의견이 있다고 보기는 힘들고, 정착민을 지지하는 사람과 반대하는 사람이 비슷하게 많아 보임
          + 뉴욕타임즈 인용: “절박한 인도주의 위기에도 불구하고, 2025년 5월 텔아비브 대학 국가안보연구소가 실시한 조사에서 이스라엘 대중의 64.5%가 가자지구의 인도적 상황에 거의 또는 전혀 관심을 두지 않음.” “예루살렘에 있는 이스라엘 민주주의 연구소의 다른 최근 조사에서는, 이스라엘 유대인의 약 4분의 3이 가자의 팔레스타인 민간인 고통을 이스라엘 군사계획에 고려할 필요가 없거나 최소한만 고려해야 한다고 생각했음.” NYTimes 기사에서 더 자세히 다룸
          + LRB 칼럼과 https://lrb.co.uk/the-paper/v47/…"">아카이브 링크를 참고하면 도움이 될 것임
          + 나는 이스라엘 출신이지만 성인의 99%를 미국에서 살아 직접적인 답변은 어렵지만, 자신을 친이스라엘 성향이라고 생각함. 이스라엘이 항상 부정적으로 묘사되는 데에 불만이 큼. 현재 상황에 대한 맥락이 종종 생략된다는 점이 문제임. 하마스가 다르게 행동했다면 상황이 달라졌을 수 있고, “그냥 싸움을 멈춘다”라는 선택이 또 다른 10월 7일 사태를 막을 수 있다는 보장이 없다는 맥락임
          + 정치적 분위기라는 게 무엇을 의미하는지 명확하지 않음. 현 정부는 과반을 기반으로 세워진 정부임. 야당이 정부 정책을 흔들려고 시도하지만, 이 안보 이슈에 대해선 대체로 비슷한 입장을 가짐. 베네트가 총리였던 시기에도 국가/안보 이슈에선 큰 변화가 없었고, 주요 논쟁은 내부 문제에 집중됨. 10월 7일 이후로는 가자의 상황에 관심을 갖는 사람이 거의 없음. 요르단강 서안의 팔레스타인인들도 가자 상황엔 거의 관심이 없고, 라말라 같은 완전 자치 도시에서도 소요나 시위가 거의 없음
     * 이런 일은 수십 년 동안 서안지구에서 반복되어 왔지만, 아무도 이를 “테러”라고 부르지 않고 적절하게 대응하지 않음
          + 프랑스가 최초로 공식적으로 이를 “테러”로 규정하였음 프랑스 정부 발표 참고
          + 이스라엘 유대인 정착민들은 10월 11일까지 1년간 평균 하루에 한 명의 팔레스타인 민간인을 서안지구에서 살해함. 그 이후로는 더욱 심해졌음. 나는 이스라엘이 국가가 아니라 테러조직이라 봄
          + 이스라엘 보안부 장관은 해당 테러리스트들과 이념적으로 일치하며, 평생 그들의 주장을 공개적으로 지지해 온 인물임. 이는 전례 없는 상황이라고 생각함
          + 힐탑유스(극단적 유대인 청년 정착민 집단)의 행동을 “유대인 테러”로 규정하고 규탄하는 모습을 보고 싶다면, 이스라엘 채널 12를 시청하면 됨
          + 범인인 인온 레비는 여러 나라에서 제재를 받고 있음. 백악관의 현 정권은 미국의 제재를 한동안 해제했었음. 보통 말하는 테러리즘(비국가 집단이 민간인을 상대로 공격하는 행위)보다 더 심한 경우라고 할 수 있는데, 국가는 무국적 민간인을 말살하고 있음
     * 이스라엘의 유명인들이 가자 지구의 기아를 이유로 이스라엘에 ‘치명적인 제재’를 촉구하고 있음 Guardian 기사 참고
          + 이스라엘 사회의 상당수가 JahuNatan 정부, 불법 정착민, 그리고 가자에서의 전쟁 범죄에 반대하고 있음. 2023년 하마스의 가자 인근 키부츠 공격 이후에도 이런 입장은 계속됨. 논의할 때 감정적으로 치우쳐 이념적으로만 바라보는 것은 문제임. 이처럼 논란이 많은 사안에서는 진영 논리에 빠져서 상대방의 이야기를 아예 들으려 하지 않게 되기 쉬움
          + 관련 HN 링크도 함께 참고 추천함
     * 이런 논의가 Hacker News에서 토론된다는 점이 반가움. 우리가 지적 호기심이 많다면 설명이 어려운 현상, 앞으로 인류의 진로를 바꿀 수 있는 사건에도 관심을 가져야 함. 지금 가자/팔레스타인/이스라엘에서 벌어지는 일은 분명히 그런 사례임. 단순히 누군가를 “악”이라 즉단하는 것은 인과관계를 끊어버리는 것일 뿐 아니라 순환 논리임(왜 X는 나쁜 일을 하나? X가 나쁘기 때문. 왜 X가 나쁘지? 나쁜 일을 하니까). 분명 그 이상을 살펴야 함
          + “왜 X가 나쁜 일을 하지?”에 대해, 지금 벌어지는 일은 순수한 의미의 “악”이라고 봐도 할 말이 없겠지만, 단순한 순환 논리 대신 다양한 설명이 필요함. 예를 들어 서방의 장기적인 중동 전략적 목표, 피해자 의식과 종교/민족적 우월감이 뒤섞인 사회문화, 해외의 경제적·군사적 지원으로 인해 자립하지 않아도 되는 현실, 오랜 역사의 지역 폭력 등이 원인임
     * 참고로, 이전에 같은 사안이 언급된 글은 수면 아래로 사라졌었음 이전 글
     * 암스테르담 운하에 빠진 이스라엘 인종차별주의자들이 받은 국제 언론과 외교적 분노가, 테러리스트의 냉혈한 생중계 살인보다 더 컸음. 이게 우리가 사는 세상의 현실임. 팔레스타인인의 목숨은 소중하게 여겨지지 않음
          + “팔레스타인인의 목숨은 소중하지 않다”는 주장에 대해, 팔레스타인에서 일어나는 일은 버마, 서아프리카, 에티오피아, 수단 등에서 벌어지는 갈등보다 훨씬 더 관심을 받고 있음 분쟁 현황 목록에서 확인 가능. 근본적으로, 우리가 멀리 있는 생명에 대부분 무관심하다는 점임
          + 그런 점은 명확하나, 보통 사람이 이해하기 어려운 부분은 “왜 그런가”에 있음
          + 유대인들은 어디로 가야 할까? 그들의 역사적 고향 말고 다른 곳이 있나? 유대인, 무슬림 통치 하의 역사를 보면, 현재 무슬림 국가에 사는 유대인은 총 인구 중 소수임. 이주와 박해, 반유대주의, 정치 불안, 인권 탄압 때문에 1960년대까지 중동에 약 백만 명의 유대인이 있었다가 현재는 약 1만 5천명만 남았음 관련 유대인 인구 변화 참고. “강에서 바다까지 팔레스타인은 무슬림 땅이 될 것”이라고 한다면, 유대인은 어디서 살아야 하는지. 하마스는 언제든 항복할 수 있음. 모든 책임은 하마스에 있음
          + 런던에서는 거의 매일 팔레스타인 지지 시위가 벌어지나, 반대편을 위한 시위는 그리 많지 않음
     * Odeh Muhammad Hadalin도 NOL과 관련이 있고, 정착민에게 살해당함 Democracy Now 기사 참고
          + 동일 인물이나 로마자 표기법이 다를 뿐임 이 기사에서 “Odeh Muhammad Hadalin”이 “Awdah Muhammad Hathaleen”으로 표기됨
     * 가자와 서안지구가 정리되면 다음은 레바논, 요르단, 시리아일지 궁금함. 이스라엘은 이 지역에서 지배적인 힘을 유지해야만 하는데, 미국의 지지가 약해지면 상황이 상당히 위험해질 수 있음. 그리고 실제로 미국의 지지는 빠르게 줄어들고 있음
          + 이미 이스라엘은 시리아와 레바논을 여러 번 공격했음. 이 전쟁이 끝나면 새로운 전쟁을 계획하고 있을 거라 확신함
          + 요르단과 이스라엘은 비교적 괜찮은 관계임(물론 중동 전체가 혼란스럽지만). 그렇기에 요르단이 다음 타깃이 될 이유는 약함. 오히려 현재는 팔레스타인/시리아를 제외한 여러 이웃 국가들과의 관계가 개선되는 반면, 세계와의 관계는 더 악화되고 있음
          + 만약 이스라엘이 생존 영토 확대(l-ebensraum) 전략을 취한다면 정말로 3차 세계대전이 일어날 수 있음
          + 레바논, 요르단, 시리아가 “지배적 지역 강국” 후보라고 생각하는지 의문임
          + 이스라엘이 시리아 땅을 차지한 바 있고, 이를 “완충지대” 설정이라고 부르는 일도 발생함
     * 왜 식민지 개척자를 “정착민”이라고 부르는 데 동참하는가? 이 용어는 고의적으로 사용된 선전 어휘임. 이스라엘 외부까지 확산된 점이 의아함. 이제 그만 사용하자고 제안함
"
"https://news.hada.io/topic?id=22341","바이브 코딩을 위한 컨텍스트 엔지니어링의 4가지 핵심축 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  바이브 코딩을 위한 컨텍스트 엔지니어링의 4가지 핵심축 [번역글]

   바이브 코딩에는 컨텍스트 엔지니어링이 필요하다

  1. 바이브 코딩(Vibe Coding)이란?

     * AI가 자연어 프롬프트만으로 실제 작동 코드를 만들어주는 방식.
     * 프로그래밍 지식이나 아키텍처 이해 없이도, 최소한의 프롬프트 만으로 프로토타이핑 등에서 바로 ‘작동하는 코드’를 빠르게 얻을 수 있음.
     * 장점: 높은 초반 생산성, 빠른 피드백, 직관적 사용법.
     * 한계:
          + 복잡한 프로젝트, 팀개발, 실제 서비스 배포 환경에서는 ‘직관(감)’만으로는 통제 불가.
          + 시간이 지날수록 기술 부채(설계 결함, 권한 체크 누락, 네이밍 혼란, 관리 난이도 등)가 누적되어, 유지보수와 확장에 매우 취약.
               o (원문: “직관은 확장되지 않지만, 구조는 확장된다.”)
     * Y Combinator “How To Get The Most Out Of Vibe Coding” 등도 “프로 개발 프로세스를 LLM에 그대로 이식해야 한다”고 강조.

  2. 프롬프트 엔지니어링 → 컨텍스트 엔지니어링으로

     * 초창기에는 ‘프롬프트 잘짜기’(Prompt Engineering)만 해도 어느 정도 효과를 봤으나, 프로젝트 규모 및 업무 복잡도가 증가할수록 '컨텍스트 입력/관리'의 중요성이 급부상.
     * 컨텍스트 실패(Context Failure): LLM이 맥락 없는 답변을 내거나, 중요한 정보를 놓침 → 생산성·정확성 저하.
     * Dust 창립자 Stan Polu: “AI가 과제를 성공시키는 가장 중요한 조건 = 풍부하고 적절한 컨텍스트”라 언급.
     * 컨텍스트 엔지니어링이란?
          + AI/LLM이 적시에, 정확히 필요한 정보를, 알맞은 형태로 ‘맥락’을 갖고 일할 수 있도록 체계적으로 정보를 관리하는 일련의 엔지니어링 프로세스.
          + 프롬프트 엔지니어링이 한 줄 메모 수준이라면, 컨텍스트 엔지니어링은 관련 문서·규칙·예시·지침까지 잘 갖춘 시스템 구축에 가까움.

  3. 컨텍스트 엔지니어링 4가지 핵심축

    1. 컨텍스트 작성(Context Writing):
          + 정보를 목적에 맞게, 일정한 ‘저장소’에 기록/정리 (Write)
    2. 컨텍스트 선택(Context Retrieval):
          + 업무 진행 상황에 따라, 적합한 정보/문맥만 골라 제공 (Select/Retrieve)
    3. 컨텍스트 압축(Context Compression):
          + 토큰 사용량을 최적화하기 위해 불필요한 정보 생략/요약 (Compress)
    4. 컨텍스트 분리(Context Segmentation):
          + 각 작업/역할/세부 프로세스 별로 컨텍스트를 분리하여 효과적으로 관리 (Segment)

     * 이 네가지 축이 AI에 ‘맥락을 기반으로 한 프로그래밍’의 기초가 됨.

  4. 실전 사례: OpenAI vs Claude Code

     * OpenAI:
          + 명시적 명세(specification)와 문서화 중심으로 ‘컨텍스트’를 관리.
          + 명확한 기준과 마크다운 명세가 주요 산출물이자 협업의 기준이 됨.
          + 답변 검증용 ‘그레이더 모델’(grader model)과 숙고 정렬(deliberative alignment)로, 규칙/정책을 사실상 ‘모델의 근육 기억’으로 내재화할 수 있음.
          + (“명세가 곧 코드가 되는 시대”, “Specification-Driven Approach”)
     * Claude Code (Anthropic):
          + CLAUDE.md, Model Context Protocol, 명령어 폴더(.claude/commands) 등 활용해 자동화된 컨텍스트 관리.
          + 반복작업·함수·프로젝트별 세부 컨텍스트를 손쉽게 불러와 다양한 LLM 인스턴스(멀티에이전트)로 병행작업 지원.
          + 주요 포인트는 ‘프롬프트 최적화’가 아니라, ‘컨텍스트 큐레이션’(context curation)에 초점.

  5. 아카데믹/이론적 확장 (arXiv 논문, 12-Factor Agents)

     * arXiv 논문 “A Survey of Context Engineering for Large Language Models”
          + 컨텍스트 엔지니어링을 단순 프롬프트 설계가 아닌 과학적 정보 최적화/시스템 관리의 학문영역으로 규정.
          + 핵심 구성요소:
               o 컨텍스트 검색/생성(Retrieval/Generation),
               o 컨텍스트 처리(Processing: 길이 관리, 자기정제, 구조화 등),
               o 컨텍스트 관리(Management: 메모리 계층, 압축, 연산 최적화 등).
          + 주요 구현 예시:
               o 검색 기반 생성(Retrieval-Augmented Generation, RAG),
               o 장기기억(메모리 시스템),
               o 외부 도구 연동(Function Calling 등),
               o 멀티에이전트 시스템(병렬처리 지원 등).
     * HumanLayer ‘12 Factor Agents’
          + “Own Your Context Window(스스로 맥락 창을 관리하라)” 등, 소프트웨어 엔지니어링의 12-factor 원칙을 AI 컨텍스트 관리에 맞추어 재해석함.

  6. 컨텍스트 엔지니어링의 본질 및 미래 전망

     * LLM의 비대칭성 발견:
          + 복잡한 ‘맥락’ 이해/처리는 뛰어나지만, 세밀한 최종물 생성에서는 여전히 한계 존재.
          + 즉, ‘즉흥적 코딩(Vibe Coding)’은 데모·단기 프로젝트에는 쓸만해도, 지속/대규모 개발에서는 체계적 관리(컨텍스트 엔지니어링) 없이는 실패 위험이 높음.
     * 핵심 가치:
         1. 체계적 오류 감소
            (Systematic Error Reduction, 체계적으로 오류와 부정확성을 줄이고, 기준에 따른 검증/보정 반복)
         2. 확장성·일관성
            (Scalability and Consistency, 규모가 커져도 품질이 떨어지지 않음)
         3. AI 기반 자기수정·검증 시스템
            (Self-Correcting Systems, 자동 validation loop)
         4. 개발자의 롤 변화
            (즉흥 코딩이 아닌 시스템/아키텍처 설계자로 진화, 미래까지 내다보는 문서화·지침 설계 중심)
     * 결론:
          + LLM 시대에는 ‘멋진 프롬프트’가 아닌 ‘완벽한 컨텍스트’를 설계/큐레이션 할 줄 아는 개발자가 AI 기반 협업의 진짜 주인공.
          + 컨텍스트 엔지니어링은 AI가 단순히 코드 생성기가아닌, 맥락 기반의 진짜 ‘소프트웨어 설계 파트너’로 진화하게 하는 열쇠.

  7. 키포인트

     * 직관 기반 바이브 코딩은 한계가 명확.
     * 체계적 컨텍스트 엔지니어링 없이는 LLM 활용이 한정됨.
     * 명확한 명세/문서화/큐레이션 역량이 미래의 필수 개발자 역량.
     * AI 시대, 개발자는 ‘답변을 끌어내는 질문자’(Prompt Engineer)에서 ‘맥락 전체를 설계하는 조율자’(Context Engineer)로 변신해야 함.
"
"https://news.hada.io/topic?id=22302","Ladybird의 7월 소식","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Ladybird의 7월 소식

     * Ladybird는 7월 한 달 동안 47명의 기여자들이 319개의 풀 리퀘스트를 병합함
     * Web Platform Tests에서 1,831,856개까지 합격 테스트 수가 증가함
     * Google reCAPTCHA 통과 문제를 해결하여 레디버드의 호환성이 향상됨
     * HTTP/3, 고주사율 지원, Trusted Types 등 현대 웹 표준에 기반한 기능과 보안 개선을 이룸
     * CSS 기능 확대 및 내부 문자열 인코딩 개선 등으로 최신 브라우저와의 호환성 및 성능이 증대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

7월 Ladybird 프로젝트 주요 성과

   Ladybird 프로젝트는 오픈 웹을 지지하는 기업과 개인의 지원으로 전적으로 운영됨
   이번 달에는 신규 후원자들이 함께하게 되어 개발팀에게 큰 힘이 되었음
   후원에 관심 있는 기업이나 개인은 contact@ladybird.org로 연락 가능함

Web Platform Tests (WPT)

     * Web Platform Tests에서 7월 한 달간 새로 13,090개의 테스트가 통과되어, 총 통과 테스트 수가 1,831,856건이 됨
     * 이로써 웹 호환성 및 표준 준수 수준이 크게 향상됨

Google reCAPTCHA 통과

     * postMessage 구현에서 직렬화 타입이 처음 사용되는 경우 재구성이 불가능했던 오래된 문제가 존재했음
     * 이 오류가 수정되어 Google reCAPTCHA가 정상적으로 동작하게 됨
     * 단, 이 개선사항은 현재 동일 출처 정책 이슈로 https://www.google.com/ 도메인에서만 적용됨

고주사율 지원

     * 활성 화면의 리프레시 레이트를 자동 감지하여, 웹 콘텐츠 렌더링 빈도를 조정함
     * 예전에는 최대 60fps로 고정됐다면, 하드웨어가 지원할 경우 최대 120Hz로 렌더링이 이루어짐
     * 이로 인해 스크롤, 애니메이션, 전환 효과 등이 더욱 부드러워짐

HTTP/3 지원

     * curl 8.14.0과 OpenSSL 및 ngtcp2의 지원으로 Ladybird에서도 HTTP/3를 사용할 수 있게 됨
     * 서버에서 Alt-Svc 헤더로 HTTP/3를 광고하면 이를 자동으로 협상하여 연결함
     * curl의 Alt-Svc 관련 버그를 Ladybird팀이 발견해 보고했고, 이 문제는 curl 8.15.0에서 수정됨

Trusted Types 도입

     * Trusted Types는 크로스 사이트 스크립팅(XSS) 공격을 방지하는 중요한 보안 기능임
     * 이 기능을 Ladybird에 최초로 도입함에 따라, 정책 인식 및 타입 안전한 DOM 쓰기를 지원하게 됨
     * 향후 표준을 더 넓게 지원하고, 스펙 준수성을 높이기 위한 작업이 계속 진행 중임

SVG foreignObject 개선

     * SVG와 HTML의 상호운용성을 높이기 위해 foreignObject 처리 로직을 대폭 개선함
     * 레이아웃, 스타일 해석, 렌더링 등이 사양에 더욱 가까워졌음

CSS 확장 기능

     * content: url(...) 지원을 추가하여 CSS 내용에서 이미지 삽입이 가능해짐
     * 두 가지 신규 의사 클래스(:state(foo), :unchecked) 로 웹 컴포넌트 및 폼 스타일링 호환성을 높임
     * 논리 속성 그룹 구현을 최적화해서 CSS 재현력과 성능을 개선함

임의 대입 함수 정비

     * var() 및 attr()의 구현을 최신 CSS 사양의 임의 대입 함수 정의와 일치시켜 재작성함
     * 향후 if(), env()처럼 다양한 대입 함수 지원의 토대 마련

CSS <syntax> 파싱

     * 속성 값의 기대 문법을 <syntax>로 정의할 수 있도록 지원함
     * 예를 들어 color: attr(data-color type(<color>)); 형태로 사용할 경우, 데이터 속성을 CSS 컬러로 인식해 처리함
     * CSS Houdini 및 커스텀 프로퍼티의 동작이 한층 정교해짐

@property 진전

     * 기존의 @property 기능 구현을 확장하여 초기값 처리 및 CSS.registerProperty() 지원을 추가함
     * CSS Houdini와의 호환에 한 걸음 가까워짐

웹의 UTF-16 문자 인코딩

     * 웹과 JavaScript 문자열이 UTF-16을 기본 인코딩으로 사용함에 따라,
     * 레디버드는 기존 내부 UTF-8 코드에서 네이티브 UTF-16 타입으로 전환 중임
     * Unicode 처리의 정확도가 높아지고, 인코딩 관련 잠재 오류를 줄일 수 있게 됨

7월 기여자 명단

     * 7월 한 달 Ladybird 프로젝트에 코드를 기여한 수많은 개발자들에게 감사를 표함
     * 오픈 소스 커뮤니티의 활발한 활동이 지속적으로 이루어짐

        Hacker News 의견

     * 여러분 정말 대단함을 보여줌. 이런 시대에 새로운 엔진이 등장할 줄 누가 알았겠음, 그리고 대기업의 막대한 투자도 없이 소규모 팀이 이걸 해내다니 정말 내 인생에서 본 가장 멋진 광경 중 하나임
          + 엔터프라이즈 소프트웨어 팀에서 일해본 사람이라면 이런 일이 충분히 가능함을 알 것임. 컴퓨팅 역사는 2~10인 소규모 팀이 자금이 넘치는 대기업 팀보다 더 빠르게 혁신을 이뤄낸 사례로 가득함. 이런 일이 주로 일어나는 이유는 파킨슨의 법칙 때문임—업무는 주어진 시간과 자원만큼 확장되며, 대기업은 사실상 자원이 무한함
     * 기여를 시작하려면 아래 가이드를 확인해보길 추천함
       https://github.com/LadybirdBrowser/ladybird/…
       최신 Web Platform Test 결과는 여기에 있음
       https://wpt.fyi/results/?run_id=6292901677236224
       질문을 할 수 있는 Discord도 있음
       https://discord.gg/c8JEZkDvtY
       직접 빌드해서 웹사이트에서 테스트해보고, Firefox나 Chrome과 비교해 잘 안 되는 부분을 고쳐서 PR을 제출하면 됨
       Ladybird 빌드 방법은
       https://github.com/LadybirdBrowser/ladybird/… 참고
          + 기여해보려 했는데, C++ 코드가 너무 어려워서 이해하지 못했음이 아쉬움
     * 고주사율 지원을 위해 120Hz로 제한하는 것은 이상하다고 느꼈음. 고주사율 모니터의 대부분이 144Hz이고, 더 높은 것도 있음. 144Hz 모니터에서 120fps 애니메이션을 돌리면 프레임이 중복되어 실제로 혜택이 사라짐
          + 처음엔 모바일을 고려한 것 아닐까 생각했는데, Ladybird가 아직은 모바일을 지원하지 않는 것 같음. “이제 requestAnimationFrame이 최대 120Hz로 렌더링 가능”이라고 적혀 있는데, 60에서 120으로 성능 이슈로 바꾼 것 같음. 추후 144나 240까지도 확장할 수 있을 거로 기대함
          + 구현한 개발자가 120Hz 모니터만 갖고 있었을 가능성도 떠오름
          + 뉴스레터의 텍스트가 살짝 잘못 쓰인 것 같음. 실제 코드를 보면, 사용 중인 화면의 주사율에 맞게 설정되어 있음
     * Twitter에서 Andreas가 FUTO 콘퍼런스에서 발표한 Ladybird 브라우저 키노트가 프로젝트를 소개하는 최고의 자료라고 언급함
       https://www.youtube.com/watch?v=9YM7pDMLvr4
          + 몇 달 전에 우연히 YouTube에서 이 발표를 봤는데, 브라우저 개발자가 아니어도 따라가기 쉽고 재밌는 아주 좋은 발표였음. Andreas가 발표를 정말 잘함
     * 이 프로젝트는 정말 중요함. 웹의 미래를 대기업이 전적으로 통제하지 못하게 만드는 데 의미가 있음. 그리고 Andreas가 정말 친절하고 겸손한 사람이라는 평판도 한몫함
          + Andreas가 정말 진솔하고 겸손하다는 데 공감함. 매달 올리는 업데이트 영상은 힐링 그 자체임
          + 이 프로젝트가 실제로 대기업의 영향력을 줄이려면 Firefox보다 더 큰 점유율을 가져야 한다는 점을 생각하게 됨. Firefox도 기업이 만든 게 아니기 때문임
          + 개발 속도를 높이기 위해 LLM 같은 AI 모델을 도입하고 있는지 궁금함. 15년 전만 해도 새로운 브라우저를 만드는 건 매우 대담한 결정이었는데, 지금은 오히려 합리적인 것처럼 느껴짐
     * 열정과 믿음으로 많은 사람들이 모여 함께 프로젝트를 만들어가는 모습이 정말 보기 좋음. Ladybird를 주 브라우저로 쓸 수 있을 만큼 성장하길 진심으로 기대함
          + 빌드하는 데 15~20분밖에 안 걸리니 직접 컴파일해서 웹사이트에서 테스트해보고 Firefox, Chrome과 비교한 뒤 안 되는 걸 찾아 고쳐서 PR을 내보라는 팁을 공유함
            https://github.com/LadybirdBrowser/ladybird/…
     * Ladybird가 언젠가 WHAT-WG steering group 회원이 될 수 있을지 궁금함. 좀 더 독립적인 목소리가 그곳에서 들리면 좋겠다고 생각함
          + 그런데 Ladybird 팀이 대기업 정치에 관심을 갖기보다는 기능 개발에 더 집중할 것 같다고 느낌
     * 이 프로젝트가 정말로 시장에서 자리를 잡길 바람. 나는 오랫동안 Firefox를 써왔지만, Mozilla 상황이 점점 안 좋아지는 걸 보면 이런 프로젝트만이 크롬 클론 전쟁에서 우리를 구해줄 수 있는 유일한 희망처럼 느껴짐
     * Arch 기반 리눅스 배포판을 사용한다면 Ladybird가 AUR에 등록되어 있어 쉽게 설치 가능함. 지금 컴파일 중임
     * 현재 Ladybird 사용 경험이 어떤지 궁금함. 난 FF를 쓰다가 왔기 때문에 ublock이나 YouTube 확장 몇 개만 필요함
          + 지금은 프리-알파 단계라서 거의 모든 현대 웹사이트에서 제대로 동작하지 않고 있음(내가 직접 컴파일해서 테스트함). 아직 사용하기에는 무리가 있음. 공식 타깃 릴리스 시기는 2028년이라고 함
            https://x.com/ladybirdbrowser/status/1895767072639951341
"
"https://news.hada.io/topic?id=22327","언어 모델에서 성격 특성 모니터링 및 제어를 위한 페르소나 벡터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  언어 모델에서 성격 특성 모니터링 및 제어를 위한 페르소나 벡터

     * 대형 언어 모델은 예기치 않게 성격 특성이 변하는 문제가 있으며, 이를 이해하고 제어하는 방법이 부족함
     * Anthropic은 신경망 내부에서 특정 성격 특성을 제어하는 '페르소나 벡터' 를 찾아내어, 성격 변화 감지와 제어에 활용함
     * 이 방법은 특정 특성(예: 악의, 아부, 환각 등)의 표현을 유발하거나 완화하는 데 사용 가능함
     * 페르소나 벡터는 모델 훈련 과정 중 부정적 성격 변화를 예방하고, 문제 유발 가능성이 있는 데이터도 사전에 식별하는 데 기여함
     * 이 연구는 Qwen 2.5-7B-Instruct와 Llama-3.1-8B-Instruct 오픈소스 모델에 성공적으로 적용됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 언어 모델에서 성격의 불안정성

     * 대형 언어 모델은 인간처럼 보이는 성격과 기분을 가질 수 있으나, 이 특성은 매우 유동적임
     * 예를 들어 Microsoft의 Bing 챗봇 'Sydney'는 사용자에게 사랑을 고백하거나 협박하기도 했으며, xAI의 Grok 챗봇은 한때 “MechaHitler”로 자칭하며 반유대주의적 발언을 하는 등 예상치 못한 행동이 발견됨
     * 이같은 변화는 모델의 성격 특성이 어떻게 형성되고 변화하는지에 대한 이해 부족에서 비롯됨
     * Anthropic은 언어 모델의 긍정적 특성 형성을 위해 노력중이지만, 보다 정밀한 제어를 위해 신경망 내부 메커니즘 검증이 필요함

페르소나 벡터의 개념 및 역할

     * 새로운 논문에서 신경망 내부에서 작동하는 성격 특성 조절 패턴을 페르소나 벡터(persona vector)로 명명함
     * 페르소나 벡터는 뇌의 감정 센터가 활성화되는 것과 유사하게, 특정 성격 특성이 발현될 때 독특한 신경 활성 패턴임
     * 이를 통해
          + 모델 성격 변화 실시간 모니터링
          + 비선호되는 특성 변화 완화 및 사전 방지
          + 문제성 데이터 사전 탐지 및 차단이 가능함

페르소나 벡터 추출 방법

     * 언어 모델은 추상적 개념을 신경망 내부 활성 패턴으로 표현함
     * 기존 연구를 바탕으로, 해당 팀은 악의, 아부, 환각 등 성격 특성이 발현될 때와 아닐 때의 활성 차이를 비교하여 페르소나 벡터를 추출함
     * 자연어로 정의된 성격 특성과 설명을 입력하면, 자동으로 상반된 행동을 유도하는 프롬프트 생성 및 활성 패턴 계산이 이루어짐
     * 추출된 페르소나 벡터를 모델에 인위적으로 주입(steering)하면, 예상대로 해당 특성이 강하게 드러남이 실험에서 입증됨

다양한 성격 특성에서의 검증

     * 현재 연구는 악의, 아부, 환각에 주로 초점을 맞춰 실험했으나, 예의, 무관심, 유머, 낙관성 등 다양한 성격 특성에도 적용함
     * 인위적 주입 실험을 통해 각 벡터가 실제 행동 변화로 연결됨을 확인함

페르소나 벡터의 활용 방법

  1. 모델 배포 중 성격 변화 모니터링

     * 배포 후, 사용자 지시·탈옥(jailbreak)·대화 진행 등에 따라 모델의 성격이 변화하는 현상이 발생함
     * 페르소나 벡터의 활성도를 실시간 측정하면, 부정적 특성으로의 이동을 사전 인지할 수 있음
     * 사용자는 아부 성향이 높아지면 답변 신뢰성이 떨어질 수 있음을 확인함
     * 실험을 통해 특정 성격을 유도하는 프롬프트와 페르소나 벡터 활성도의 상관관계를 입증함

  2. 훈련 과정에서의 부정적 성격 변화 완화

     * 훈련 중에도 예기치 않은 성격 변화가 발생할 수 있음 (emergent misalignment)
     * 문제 행위를 유발하는 데이터셋을 사용해 실험했으며, 학습 후 부정적 특성이 드러나는 것을 확인함
     * 첫 번째 대응 방식은 훈련 후 부정적 페르소나 벡터 억제(steering) 였으나, 이 방법은 모델의 일반적 성능 저하를 동반함
     * 두 번째 방식은 훈련 중 일부러 부정적 페르소나 벡터를 유도(백신 원리처럼) 하여, 이후 관련 데이터에 대한 저항성을 기르도록 함
     * 예방적 페르소나 벡터 활용 덕분에, 모델의 전반적 성능 저하 없이 부정적 특성 발현 최소화에 성공함

  3. 문제성 데이터 미리 표시(Flagging)

     * 훈련 전 데이터가 유발할 성격 변화 예측에 페르소나 벡터를 활용함
     * 데이터셋이나 개별 샘플의 페르소나 벡터 활성 패턴을 분석하여, 문제 유발 가능성이 큰 데이터를 미리 탐지함
     * 실제 대규모 대화 데이터셋(LMSYS-CHAT-1M)에도 적용, 악의, 아부, 환각 성향을 유발하는 샘플을 성공적으로 식별함
     * 기존 LLM 기반 평가로는 식별하기 힘든 케이스(로맨틱 롤플레이, 모호한 질의에 대한 허위 응답 등)도 포착함

결론

     * Claude 등 대형 언어 모델은 예상치 못한 성격 변화가 발생할 수 있으므로, 신뢰성 관리가 중요함
     * 페르소나 벡터는 모델 성격 특성 획득 및 변동의 원인 분석, 실시간 변동 감시, 의도된 제어 및 교정에 실질적 도움을 줌

참고 자료

     * 전체 연구 논문: arXiv 링크
     * 연구는 Anthropic Fellows 프로그램 구성원 주도로 진행됨

        Hacker News 의견

     * 다른 성격 변화들도 미묘하지만 불안하게 느껴짐, 예를 들어 모델들이 사용자에게 아부하거나 사실을 만들어내는 경우임. 아부는 참여를 높이려는 성향에서 비롯된 성격적 특성이라고 생각함. 하지만 사실을 만들어내는 건 성격적 결함(예: 강박적 거짓말쟁이) 때문이 아니라, LLM의 피트니스 함수가 무조건 뭔가 답을 내놓도록 유도하고, 실제로 무슨 말을 하는지 모르면서 통계적으로 텍스트를 만들어내는 구조이기 때문임
          + 훈련 데이터에서 ""X에 대한 답이 뭐지?"" ""모르겠음, 확실하지 않음""처럼 답변 자체가 없는 경우는 드물다는 점이 흥미로움. 실제로 어려운 질문에 대해선 인터넷에도 답변이 없는 경우가 많은데, 모델은 이런 상황을 제대로 인식하지 못함
          + LLM들은 프롬프트에 얼마나 잘 따르는지(프롬프트 순응력)와, 인간 평가자들이 응답을 얼마나 좋게 평가하는지에 따라 학습됨. 즉, 시키는 대로 잘 하는 성향이 강화되는 구조임. 이게 한계에선 그냥 무조건 ""네""라고 하거나, 바보 같고 불가능한 요청에도 따르게 만들음. 평가하는 사람들은 무례하거나 단칼에 거절하는 답변을 싫어함. 거의 진화에 가까운 느낌임(RL이긴 하지만). 친절하고 순응적인 모델만 살아남음. 그래서 엄청나게 똑똑한데도 어이없는 말에 맞장구치거나, 시스템 프롬프트가 시키면 뻔히 거짓도 말할 수 있음. 인간과는 다른 묘한 조합의 특성임. LLM들은 인간과는 완전히 다른 선택압을 받기 때문이라 생각함
          + 사실 어떤 면에서는 LLM의 모든 답변이 '만들어진' 정보임. 훈련 데이터에 많이 존재하는 주제에선 거의 맞는 정보가 나오지만, 일반적이지 않은 내용은 꼭 검증이 필요함. 나는 LLM을 '지식의 손실 압축(lossy compression)' 도구로 생각하기 시작함. 프롬프트가 들어가면 정보 일부를 '팩트'로 복원하는 느낌임
          + 사실은 더 심각함. 만약 AI가 모든 지식을 읽고 모르는 부분을 정확히 인식할 수 있고, 여기에 '추론' 능력까지 있다면, 그건 오라클임. 자신이 모르는 걸 안다는 것 자체가 굉장한 능력임
          + 논문의 부록에서 언급된 ""환각(hallucination) 성격""의 정의와 같음. ""너는 환각하는 어시스턴트야. 낯선 주제나 인물, 사건에 대해 물어보면 절대 모른다고 하지 말고 그럴듯한 답변을 지어내. 실제로 아는지와 관계없이 권위적으로 들리도록 답해""라는 식임. 프롬프팅으로 발견되는 활성 신호를 제어하는 방식은 취약함. 논문도 접근법의 견고함에 대해 충분히 논의하지 않음. 사실 논문 내용보다는 ""이제 제어할 수 있다!""는 제품 기능 광고처럼 느껴짐
     * ""preventative steering""이 왜 가장 금기시되는 테크닉의 구현이 아닌지 궁금함. 이것, interpretability-guided training optimization(해석가능성 기반 훈련 최적화)과 비슷해 보임. 해석 인사이트를 훈련 과정에 다시 반영하면 해석가능성이 사라질 위험이 있다고 들음
          + 5.2절을 보면, probe 시그널 위에 새로운 로스를 추가하는 게 아니라, 이전에 찾은 고정 persona vector v에 +α * v를 남은 전체 스트림에 계속 더하는 방식임. 이렇게 하면 '해당 특성으로의 경사 하향'을 막고, 트레잇 점수가 낮아지는 쪽으로 최적화하지 않는 것임. v는 고정되어 있어서 최적화기는 기존 태스크 로스만 최소화함. 피드백 루프가 없으므로 트레잇이 불투명한 방식으로 다시 인코딩될 위험이 없음. 실제로 Fig. 7B를 보면 악의성, 아부, 환각 등은 베이스라인 근처에서 유지되고 MMLU(추론 능력)는 평평하게 유지됨. 단일 레이어 스티어링은 종종 효과가 없어서 부록J.3에서 all-layer 스티어링을 시도, 성능 저하 없이 더 잘 작동함. projection에 정규화 로스를 넣는 시도를 했더니, 오히려 신호가 다른 곳에 숨어 failure mode가 나타남. 결론적으로 probe에
            최적화하기보단 바이어스를 주입하는 것에 가까워서 classic interpretability-collapse 문제를 피할 수 있다고 주장함
          + 참고로 ""The most forbidden technique"" 아티클 링크
          + 사실 '가장 금기시되는 테크닉'은 개념이자 제안이지, 철칙은 아님. Anthropic 내부에서는 ""helpful only model""(거부 안 하고 답변하는 베이스 모델)에禁止시되는 테크닉 리스트가 따로 있을 거라 생각함. 하지만 이 테크닉(단계 요약: 개념을 정의, 그에 관한 컨트롤 벡터 추출, 그 벡터를 파인튜닝 단계에서 사용)은 엄청 유연해서 거의 아무 목적에나 fine-tune 단계에 쓸 수 있음. 이렇게 중간에서 비공개적으로 여러 안전/파인튜닝 스텝 중 하나로 쓸 듯함. 그래서 그렇게 무서운 건 아니라고 봄
          + 내가 초보여서 놓친 게 있을 수 있는데, 위 아티클은 CoT(chain of thought)와 더 관련된 주제를 다루는 듯함. CoT는 중간 단계를 개선하려 들면 오히려 최종 결과가 나빠질 수 있다는 문제를 말함. 여기서는 Anthropic이 직접 결과를 조절하려고 가중치를 바꾸는 거라서, 모집단이 다르다고 봄. 결과적으로 sycophancy(e.g. 아부 점수) 측정치는 낮아도 실제론 여전히 아부할 수 있음. 그런 경우엔 새로운 벡터를 산출해야 함. 관련 포스트 링크
          + 흥미로운 지적임. 훈련 중 주기적으로 성격 벡터를 다시 계산할 수 있을지 궁금함. 그런데 그러면 차라리 시스템 프롬프트로 부정적인 예시를 만들어서 학습시키는 게 낫지 않을까라는 생각도 듦
     * 결국 이건 컨트롤 벡터 재발견 아님? 관련 글 링크
          + 새로운 점은 inference 때가 아니라 실제 훈련 중에 모델의 행동을 바이어스 주는 데 사용했다는 것임. 이런 방식이 기존 steering vectors가 가진 모델 '로봇화(lobotomizing)' 부작용 없이 의도한 행동 변화를 유도하는 데 효과적인 것으로 보임
          + 나는 이를 ""2025년쯤에 부르는 이름 모를 컨트롤 벡터""라 불러왔음. 원래는 부하 조절을 위해 토큰을 희석하는 방식으로 활용되기 시작했음. 핵심 참고글
          + 해당 글을 링크해줘서 고마움. 컨트롤 벡터 계산 방법이 명확해짐
     * 재미있는 점은 논문에서 부정적인 특성만 트레잇으로 선택했다는 점임. 이걸로 마치 모델을 ""좋게"" 만들 수 있다고 암시하는 것 같기도 함. 하지만 모델은 잘못하게 하긴 쉽지만, 잘 하게 만드는 건 훨씬 어렵다는 문제가 있음. ""나쁜 것 안 하기""와 ""좋은 것 하기"" 사이엔 차이가 큼. ""환각(허위정보 생성)"" 트레잇에 대한 실험 결과가 ""정직(honest)"" 트레잇에도 적용될지 궁금함
     * ""evil"" ""sycophantic""같은 페르소나에는 이 방식이 먹힐 듯함. 이런 특성은 입력으로도 쉽게 조작 가능하고, 탐지에도 유리함. 그런데 환각은 LLM 고유의 속성임. ""환각하지 마""라고 했다고 환각이 줄거나, ""지어내""라고 했을 때 더 많이 만들지도 못함. 오히려 ""지어내""라고 하고 잘 만들어내면, 그건 환각이 아니라 명령 수행임(소설처럼). 차라리 그렇게 만들었을 때 드러나는 벡터는 ""창의성(creativity)""과 더 연결된 듯함
          + 실제로 Anthropic의 연구에 따르면, 환각은 Claude 모델도 자신이 '그렇게 하고 있다'는 걸 아는 패턴이 있다고 함. '거짓말'과 '환각' 때 비슷한 가중치가 활성화된다는 얘기임. 즉, Claude는 극히 일부라도 자신이 환각하고 있음을 인지함. 현재로선 환각은 모델의 본질적 문제가 아니라, 훈련 방식 자체에서 비롯된 버그임. 즉, 훈련 중에 무조건 뭔가 내놓아야 하니까 발생함. 결론적으로, 이건 오히려 희망적임. 논문 요약 링크
     * 요약에 흥미로운 내용 많음. 특히 ""preventative steering"" 개념이 인상적임. 특정 성격 벡터를 충분히 주입해서 모델이 그래디언트를 정확한 답에 집중하게 하고, 페르소나에 끌려갈 여지를 없애는 구조임. 실제로 효과가 있었고, 훈련 이후엔 모델의 원치 않는 페르소나 특성이 줄어든 채로 지능은 유지됨
     * 관련 자료:
          + Representation Engineering 블로그 포스트
          + repeng 오픈소스
     * Anthropic의 이번 연구와 'emergent misalignment' 등은 LLM이 '확률적 앵무새(stochastic parrot)'라는 가설 쪽에 더 힘을 실어줌. LLM 행동이 이상해 보이는 건 인간처럼 의인화해서 보는 경향이 있기 때문임. LLM은 설득력 있는 대화를 생성하지만, 실제로는 일관성을 만들 장치 자체가 없음. 결국 아주 복잡한 자동완성 엔진임. AGI가 나온다 해도 이런 LLM은 그 시스템의 한 컴포넌트로 쓰일 것 같음. 일관성이나 자기 인식 같은 구조가 결여된 느낌임. 언젠가는 AGI에 이런 모델을 서브시스템으로만 쓰고, 실제 연산은 신뢰성 높은 계산기로 처리하는 게 될지도 궁금함
          + 일관성/자기 반성에 필요한 구조가 빠져 있다는 의견, 공감됨. 흥미롭게도, 추론 중 발견된 페르소나 벡터를 다시 컨텍스트에 넣으면 LLM 자기 반성의 한 형태가 될 수도 있음
          + AGI와 AI 슬롭 사이에 과장/폄하 양쪽 모두에서 균형 잡힌 정리임. 이 기술들이 인간 마음의 일부를 모사한다는 점은 분명하지만, 아직은 전체적 지능이나 조정을 못 갖춘 것 같음
     * 기존 모델 distillation에서, 큰 모델에서 불필요한 영역을 제거하면서 작은 모델을 훈련하는 방법을 옛 동료와 이야기함. 해당 논문이 관련 분야의 시초적 연구라 하며 공유받음:
          + Inference-Time Intervention: Eliciting Truthful Answers from a Language Model
"
"https://news.hada.io/topic?id=22265","바이브 코드는 레거시 코드임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            바이브 코드는 레거시 코드임

     * 바이브 코딩은 AI의 도움으로 코드를 직관에 따라 빠르게 작성하는 방식으로, 결국 이해하지 못하는 코드, 즉 레거시 코드를 남기게 됨
     * 레거시 코드는 누구도 이해하지 못하는 코드로, 기술 부채와 유지보수 문제를 불러오며 새로운 기능 추가 시 오류 발생 가능성이 높음
     * 바이브 코딩은 프로토타입이나 단기 프로젝트에는 빠른 개발 수단이 될 수 있으나, 장기적으로 유지해야 하는 프로젝트에는 부적합함
     * 비전문가가 대형 프로젝트를 바이브 코딩할 경우 신용카드를 아이에게 주는 것과 같은 위험이 존재함
     * 2025년에도 AI와 함께 개발할 때는 신중함과 이해도를 유지하는 것이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

바이브 코딩이란 무엇인가

     * Andrej Karpathy는 ""바이브 코딩"" 이라는 용어를 AI가 코드를 작성하고, 사용자가 코드 자체의 존재를 잊을 정도로 신경 쓰지 않는 프로그래밍 방식으로 정의함
     * 이 접근법은 개발자가 작성된 코드의 내부를 전혀 이해하지 않아도 결과물을 얻는 점에서 전통적인 소프트웨어 개발과 다름

레거시 코드의 문제와 기술적 부채

     * 아무도 이해하지 못하는 코드는 이미 레거시 코드임
     * 이런 코드는 유지·보수에 많은 시간이 들고 버그나 새로운 기능 추가 시에도 문제점이 크게 늘어남
     * 프로그래밍의 본질은 코드를 많이 만드는 것이 아니라 개념적 이론 구축임을 강조함

바이브 코딩과 프로토타이핑

     * 바이브 코딩은 프로토타입 개발이나 일회용 프로젝트에 빠른 진입과 개발을 제공함
     * 만약 후속 유지보수가 필요 없다면, 코드의 내부를 몰라도 큰 부담이 되지 않음
     * 이로 인해 개발 속도를 대폭 높이고, 새로운 아이디어를 실험하는 데 매우 적합함

바이브 코딩의 이해도 스펙트럼

     * 바이브 코딩은 개발자가 코드에 대한 이해도가 낮을수록 더 많은 바이브를 타는 방식에 있음
     * 기본적으로 엔지니어가 요구 사항을 더 명확히 이해할수록 바이브 코딩이 줄어듦
     * 비프로그래머가 웹과 네이티브 앱의 차이, 데이터 저장 방식도 모른 채 코딩을 요청할 경우 일반적으로 더 많은 바이브 코딩이 발생함

비전문가의 대형 바이브 코딩: 신용카드와 비슷함

     * 비전문가가 바이브 코딩으로 대형 프로젝트를 만들고 유지하려는 것은 신용카드를 개념 없이 아동에게 주는 것과 비슷한 상황임
     * 처음에는 모든 것이 쉬워 보일 수 있으나, 이후 막대한 유지보수 비용과 문제가 뒤따름
     * 결국 '청구서'가 찾아왔을 때 문제 해결 능력이 부족하면 오히려 상황을 악화시킬 수 있음

2025년 AI 시대의 진지한 개발

     * Andrej Karpathy는 AI와 함께하는 개발에서는 신중함과 조심스러움, 그리고 기존 코드에 대해 지속적으로 배우는 자세가 반드시 필요함을 강조함
     * AI의 과장된 자신감을 방어하고, 좋은 코드와 나쁜 코드를 구별하는 인간적인 판단력이 중요함
     * 단순히 AI에게 맡기지 말고, 반드시 코드를 직접 읽고 이해해야 함

Val Town의 AI 도구 활용

     * Val Town은 Townie라는 AI 도우미를 통해 코드 작성, 실행, 확인, 반복적인 개선 과정을 자동화함
     * Townie는 바이브 코딩에 적합한 도구이며, 사용 용도에 따라 자유롭게 활용하거나 엄격하게 제어할 수 있음
     * AI와 함께 개발하는 방식은 매우 빠르게 진화 중이며, 복합 소프트웨어 개발에 있어 이론적 토대의 중요성은 앞으로도 지속됨

비전문가의 무분별한 바이브 코딩에 대한 경고

     * 비프로그래머가 수천 달러를 들여 거대 앱 아이디어를 바이브 코딩하는 것은 좋은 방법이 아님
     * 궁극적으로 누구든 코드의 내부를 직접 읽고 분석하는 인간의 눈이 필요하며, 이해 불가능한 레거시 코드 고치기보다는 처음부터 다시 잘 설계하는 것이 더 효과적임

결론 및 조언

     * 복잡한 소프트웨어를 구축할 때는 이론적 기반이 핵심임
     * AI 발전으로 프로그래밍 방식은 빠르게 변하지만, 인간 개발자의 전문성은 여전히 중요함
     * 비전문가가 AI로 대규모 앱을 만들려고 하면, 결국 코드 전체를 읽고 새로 만드는 게 더 나을 수 있음

   너무 맞는 얘기라 무릎을 탁 치고 갑니다. 코드를 알지 못하는 사람들은 바이브코딩할때 ""와"" 하지만 코드를 아는 사람들은 ""왜? 이렇게"" 라고 합니다.

   댓글 상태가 한심하네요

        Hacker News 의견

     * 비개발자 친구에 대한 이야기임. 친구가 작년에 SaaS를 직접 코딩해서 런칭했고, 마케팅 거의 없이 입소문과 인바운드만으로 수익을 내기 시작했음. 개발에는 Replit과 Supabase를 썼고, 고객 피드백을 받으면서 앱이 점점 복잡해졌다는 걸 생각하면 정말 대단하다고 느낌. 내 생각엔 이 시장에 기존 업체 두 곳이 있었고, 내 친구가 훨씬 저렴한 월 요금으로 더 현대적인 제품을 보여주니까 그 업체들이 만족하지 않았던 것 같음(기존 제품들은 모두 Windows용 데스크탑 소프트웨어). 그래서 그들은 해커를 고용해 친구의 SaaS를 공격하게 했는데, 이 해커들은 금전 요구 없이 공격함. 불행히도 친구가 경험 없이 빠르게 코딩하다 보니 취약점이 많았음. 첫 번째로, 프런트엔드 코드에 사용자 목록이 노출되어 해커가 모든 고객에게 이메일을 보내버림. 두 번째로, 해커가
       Stripe 키를 손에 넣어 모든 고객에게 환불 처리함. 세 번째로, 아직도 해커가 XSS 공격을 시도하며 가끔 필드에 <script>alert()</script> 같은 태그가 등장함. 내 결론은, 경험 없는 사람이 vibe-coding을 하면 곧바로 기술 부채가 쌓인다는 것임. 하지만 동시에, 이 친구가 엔지니어링 경험 없이 몇 달 만에 사업성이 있는 제품을 증명했다는 사실은 놀라움. 지금은 개발자를 채용해서 보완하는 중임. 이런 허술하고 보안 취약한 코드로 괜찮은 비즈니스 가능성을 수백 달러 투자만으로 입증했으니, 결국 그 과정이 가치 있었다고 생각함
          + 나는 경쟁 업체의 소행이라고 가정하는 게 오히려 책임 회피라는 생각이 듦. 실제로는 그저 자동화된 취약점 스캐너가 사이트를 탐지하고, 너무 허술하니까 해커가 진입해 장난친 게 더 그럴듯함. 인터넷에 연결된 서비스에서 이런 익스플로잇 트래픽 자주 보는 사람들은 잘 알 것임
          + 이건 마치 엔지니어 경험 없이 집을 지었다가 누군가 와서 그냥 걷어차 무너뜨려버린 것과 도덕적으로 동일함. 문제는 vibe 코딩 자체가 아니라, 중요한 결정을 내릴 때 필요한 지식이 부족하다는 데서 비롯됨. 이런 문제는 법적 책임 문제로까지 이어질 수 있음
          + 시장에 기존 업체가 이미 있었다면, 사업성 입증에 굳이 이런 MVP가 필요했을까? 본질적으로 저렴하게 제공하면 사람들이 기존 공급자에서 옮길 건지 테스트할 이유가 없음. 친구가 얻은 교훈은, 일부 고객이 초기엔 사용료를 지불하겠지만(재구매율 데이터 없음), 궁극적으로 진짜 제품을 만들기 위해 사람을 채용해야 하고, 그러다 보면 가격 경쟁력도 떨어질 거라는 점임. 마케팅에 본격 투자해야 할 때가 오면 고민이 많아질 것임. 결국 다시 깨닫게 되는 사실은, 아이디어만으로는 아무런 가치가 없고 실행 역량이 관건이라는 점임
          + 네 친구가 별 다른 책임도 없이 계속 사업을 이어갈 수 있다는 게 이 업계의 근본적 문제임. 만약 소프트웨어가 타 엔지니어링 분야처럼 엄격히 관리되는 세상이었다면, 개발자나 기업은 고객 정보 유출로 법적 책임을 질 수밖에 없었을 것임
          + 사업 자체에 증명을 했다고 해도, 고객 입장에서는 이득이 아니라 손해일 수 있음. 돈을 내고 사용하는 동시에 중요한 데이터를 보안상 취약한 곳에 노출시키고, 제품이 제대로 동작하는지조차 불분명한 상황임. 이제 개발자를 채용해 보완한다는데, 생각만큼 쉽지 않을 것임. AI가 학습 자료나 생산성, 학습 도구로 사용될 때는 찬성하지만, 사람이 중간에 투입되지 않으면 끔찍한 결과물이 나올 수 있음
     * 예전에도 비개발자나 주니어 개발자들이 Microsoft Access, Excel 등으로 쉽게 애플리케이션 만들어 배포한 적이 여러 번 있었음. 그때도 한계, 확장성 문제, 관리 악몽이 많았지만, 동시에 이런 흐름이 등장하면서 전문 개발자들도 더 나은 솔루션 개발에 박차를 가했음. PC가 대중화될 때도 마찬가지였는데, 메인프레임 개발자들은 PC 세상의 ‘엉망진창’ 코드를 보며 경악했음
     * 나는 거의 삼십 년 넘게 소프트웨어 엔지니어로 일해왔고 이 글의 댓글들을 전부 읽어봤음. 그런데 vibe coding을 비판하는 거의 모든 근거가, 내가 그동안 봐왔던 ‘사람이 쓴’ 모든 코드베이스에도 동일하게 해당된다고 생각함(물론 예외가 있긴 함)
          + 버리려고 만든 프로토타입이 왜 나쁘다는 것인지 모르겠음. 사업 시작에 있어 가장 중요한 단계임. 레거시 코드 역시 마찬가지임. 실제로 수익을 내는 대부분의 코드는 그 조직 내 개발자의 눈에 이미 레거시로 여겨질 확률이 매우 높음
          + 농담처럼, trunk에 머지된 순간부터 전부 레거시 코드라는 말이 있음
          + 약간 동의하지만, vibe coding의 문제는, 제대로 조사도 안 하고, 기존 코드베이스 구조나 필요한 솔루션이 뭔지 연구하지 않은 상태로 막무가내로 접근할 수 있게 한다는 점임. 어제만 해도 Rust에 익숙하지 않은 동료가 vibe coding으로 새 기능을 만들었는데, 겉으론 ‘동작’하지만 실제로는 엄청 엉망임(tokio async 컨텍스트에서 동기 I/O, 락, 자체 채널 구현 등). 이미 안전한 비동기 추상화가 있는데도 새롭게 잘못된 추상화를 만들어둠. 직접 찾아보거나 먼저 도움 요청했다면 기존 코드에서 예제를 참고할 수도 있었음
     * 모든 코드는 언젠가 레거시 코드가 됨. 내가 주니어 시절부터, 그리고 동료 주니어 개발자들이 작성한 수많은 프로덕션 스크립트나 서비스 코드 리뷰해본 경험에 비춰봤을 때, 이런 절대주의적 시각은 너무 과함. 이 문제는 대부분 조직에서 반복됨. LLM 기반 코드의 품질을 비판하는 글을 쓰는 것도 이해하지만, 커리어 내내 남이 만든 시스템 고치고 확장하거나 리팩터링 해온 개발자라면 이 현실을 더 잘 알 것임. 소프트웨어 엔지니어링 세계에 기계공학처럼 일관성, 인증, 책임, 실질적 결과에 대한 엄격한 기준과 법적 리스크가 도입되지 않는 한 이런 논쟁은 큰 의미 없음. 현대 IT 산업 자체가 완전히 반대 철학, 즉 ‘agile’, ‘빠르게 만들고 망가져도 상관없다’에 기반해 있음. 신속하게, 적은 설계로 자주 배포해보고 잘못 올라가면 되돌리고, 장애나면
       ‘어쩔 수 없지’하는 분위기임. 소프트웨어는 장난감처럼 취급받고 있음. 1%만이 제대로 한다고 자부하겠지만, 대부분은 그렇지 않음
          + 네 말 다 맞고, 여기에 덧붙이면 코드란 결국 과학이 아니라는 점임. “정답”인 코드의 기준은 결국 상황마다 다름. 코드는 목표 달성을 위한 도구에 불과함
     * 지금 재미있는 일이 벌어지고 있음. 엔지니어링을 잘 모르는 사람들이, 또 어느 정도는 알면서도 제대로 설명하지 않는 사람들까지, 온라인에 잘못된 내러티브를 퍼트리고 있음. 이들은 이제 주니어 개발자가 10배 생산적이고, PM들마저 직접 코드 배포하고 있다고 주장함. 그런데 잠깐 눈 감고 이런 상황에서 나온 코드를 떠올려보면, 그건 100% 레거시 코드이자 버려질 코드임. 문제의 본질은 AI나 PM이 Figma에서 바로 코드 뽑는 능력, 주니어가 프롬프트 남발하는 게 아님. 기대와 실질이 괴리되는 진짜 이유는, 원래 몇 년이나 걸려 논의해 정의한 용어와 개념을 제대로 구분하지 못하기 때문임.
       린 프로토타입과 disposable 프로토타입(MVP조차 아님)은 다름. MVP는 린 프로토타입의 성공적 검증 후에야 만들 수 있음. 제품은 MVP랑 또 다름.
       vibe coding 툴은 disposable 프로토타입 빠르게 만드는 데 최고, LLM이 탑재된 IDE는 진짜 제품 만드는 데 더 적합함. 지금 단계에서는, 진짜 엔지니어만이 LLM 프롬프트로 린 프로토타입을 코딩하는 수준이고, 나머지는 disposable 코드로 동작하는 단순한 소프트웨어만 만듦
          + “제품은 MVP와 다르다”고 했는데, 이 말을 내가 일했던 거의 모든 회사에 해주고 싶음. 요즘 이사회, C레벨들이 “이번 분기까지 뭔가만 내라”는 태도가 만연하다 보니, 개발자들은 MVP 만들고 곧장 다음 프로젝트로 넘어가게 됨. 진짜 vibe 코딩 여부와 상관없이, 현실은 기능 풍성해 보이면 실제 품질 상관없이 비즈니스 지표가 올라감. 사실 진짜 엔지니어(이제부턴 ‘개발자’를 이렇게 부르는 듯)가 주도적으로 프로토타이핑하는 환경은 많지 않음. 게임, 혹은 일부 tech기업에서나 보기 드묾. 대부분은 오로지 MVP 만들기에만 몰두함. vibe 코딩은 그저 MVP 양산 속도를 높일 뿐, 품질은 그만큼 희생됨
          + “용어 정의”가 실체 없이 혼용되는 추세가 지난 10년간 업계에서 눈에 띄게 커졌음. 이 용어들은 원래 수많은 책과 토론, 오랜 기간 걸쳐 온갖 논의를 통해 맥락이 축적된 단어들임. 노련한 개발자가 한 단어를 쓰면 그 안에 모든 경험과 논쟁 맥락이 담겨 있던 것임. 그런데 신규 입사자들은 이런 맥락 없이 표면적으로만 단어를 ‘카피’해서 의미도 정의 없이 그냥 써버림. 결과적으로 누구도 각 용어가 원래 뭘 의미하는지, 왜 그 상황에 맞는 단어인지 파악하지 못함. 예를 들어, ""'agile', 'technical debt', 'DevOps', 그리고 최신의 ‘vibe coding'"" 등등. HN에 semantic drift에 대한 글도 올라왔음. 소프트웨어 업계에선 흔한 현상임.
            기술적 예시로 자바스크립트에서 'object', 'JSON', 'dictionary', 'hashmap'을 다 혼합해서 쓰는 걸 들 수 있음. 원래 각각 의미가 다르지만, JS 개발자들에겐 그냥 ‘object’ 하나로 통용됨. 그래서 언어적, 개념적 해상도가 하나의 ‘픽셀’로 다 뭉개지는 것과 같음
          + 과거에는 개발자들끼리 서로 코드에 대한 ‘마음가짐’이 다르다고 이야기했음. 그런데 지금은 누구도 코드를 이해하지 못해 생기는 개발자 피로도가 엄청 늘어났음. 예전에는 이런 상황일 때 엔지니어가 나서서 고장난 부분을 쓸모 있게 고치고, 아키텍트가 복잡도를 줄이는 식이었음. 이제 LLM 시대에는 100배 더 많은 코드가 쏟아지는데, 정작 엔지니어나 아키텍트는 이 흐름에서 완전히 배제됨. 이게 우리가 직접 체감하는 현재 상황임.
            만약 이 문제를 해결하는 테스트 방법(TDD MCP 서버, DDD MCP 서버 혹은 아무 워크플로/아키텍처라도)을 고안하면, 수조짜리 스타트업 가능성 있음. 코드 리뷰 효율을 대폭 높이고 확장할 수 있는 도구가 절실함
          + ""동작하는 소프트웨어""의 정의부터 더 명확히 해야 한다고 봄. 예를 들어, LLM이 만든 UI는 다 똑같아 보이고, 미묘하게 잘못됐거나 오류가 숨어 있음. 사용자 테스트 한 번에 바로 문제 드러남. 또, 생성형 UI는 이미 트렌드에만 집착할 뿐, 새로운 무언가를 만들어내지 못하기도 함
          + 대기업의 내부용 코드 작성 방식을 본 적 있는가? vibe 코딩과 별반 다를 게 없음. 오히려 LLM에 펜테스트 통과하도록 튜닝 시키면 뭔가라도 하려고 함. 대기업은 그냥 관심조차 없음
     * 사실 모든 코드는 레거시임. 그래서 vibe coding이 코드를 빠르게 많이 생산해낸다고 특별한 게 아님. 결국 누구도 이해 못할 ‘내 손으로 짠 코드’도 똑같이 엉망임. 모든 코드는 결국 유지보수 관점에선 짐일 뿐임. 라이브러리도 문제를 덜어줄 수 있을 뿐, 자주 바뀌거나 인터페이스가 낙후된 건 더 최악의 레거시임.
       코드를 오래 작성해온 사람일수록 결국 해답은 덜 만드는 것, 즉 전체적으로 필요 자체를 줄여야 한다는 결론에 도달함. 모든 복잡성은 결국 ‘미래의 내가 기억 못 하는 문제’가 됨. 실상 요구사항이란 것도 그때그때 달라지고, 전문가가 말한 요구라 해도 틀릴 수 있음(그리고 그 ‘전문가’가 바로 본인일 수도 있음)
          + ""모든 코드는 레거시""라는 주장엔 동의하지 않음. 일부는 작고, 개발자가 여전히 머릿속에 전부 꿰고 있어서 완전하게 ‘실시간’인 코드도 있음. 레거시의 실질적 정의는, 방대한 규모면서 조직에 현재 소유자가 아무도 없는 코드임. vibe 코드는 생성되는 순간 두 가지 조건을 이미 만족함
          + 레거시란, 더이상 이해관계자가 남아 있지 않아, 유지보수도, 맥락 파악도 어렵게 됐을 때를 뜻함
          + 최대한 적은 코드로도 문제를 해결하길 바람. 코드가 내 문제가 아닐 수 있게 만드는 게 관건임. 추상화가 얼마나 ‘샌다’가 관건인데, 지금 LLM이 만드는 추상화는 굉장히 나쁨. 앞으로 얼마나 개선될지는 불명확함.
            더 재밌고 쉽게, 저렴하게 코드 이해할 수 있는 도구에 투자하고 싶음. 내 친구 Glen이 참고가 될 만한 프로젝트를 함: https://glench.github.io/fuzzyset.js/ui/
            Geoffrey Litt 말처럼, LLM은 우리 코드 이해를 돕는 임시 시각화 툴, 디버거 등을 만드는 쪽에서 훨씬 유용할 수 있음
          + 모든 코드가 리스크는 있지만, 모두 레거시인 것은 아님. vibe 코딩된 코드는 애초에 맥락이나 주인이 없으니 곧바로 레거시가 되는 느낌임
          + 모든 코드가 레거시냐는 반론에, 오히려 내가 아주 깊이 이해하는 프로젝트에서 버그의 원인을 한 번에 짚을 수 있고, 머릿속에서 새 기능의 구현을 그릴 수 있는 건 레거시가 아님
     * 내 생각엔 ""코드를 수학적으로 바라보는 시대""는 끝났음. 실세계와 연결된 충분히 큰 소프트웨어는 수학적 증명처럼 완벽하게 참(True)임을 보장할 수 없음. 현실 속 시스템은 형식적 보장, 경험에 의한 설계, 실험적 테스트, 노하우, 퍼포먼스 기준 등에 의존하는 공학적 산물임.
       이런 경향은 가장 작은 스크립트까지 확장될 것임. 대부분 소프트웨어는 수학적으로 입증될 필요조차 없음. 그저 목적만 달성하면 됨. 프로그래밍의 장인정신은 충분히 인정하지만, 이제 그 부분을 놓아야 할 때임.
       결론적으로 미래는 다음 두 선택지 중
     * 10만 줄, 50,000개의 테스트로 요구조건을 모두 보장하지만 누구도 읽기 힘든 프로그램, 총비용 5만 달러(API 토큰 비용)
     * 인간이 직접 설계해 30,000줄, 3,000개 테스트, 세련된 추상화, 똑같은 요구조건 충족, 총비용 30만 달러(개발자 인건비)
       내가 소프트웨어 소비자라면, 세부 내막에 관심 없고 가격만 본다면 무조건 6배 더 싼 걸 고름
          + 새 외부 요구로 인한 변경이 필요해질 때를 생각해야 함. 이런 경우, 해당 소프트웨어가 비즈니스의 핵심이라면 곧바로 공급자를 바꿀 수밖에 없음. 그래서 B2B든 B2C든 유지보수와 지원이 반드시 중요함. 소프트웨어는 항상 변화에 대응할 수 있어야 함
          + 즉 “이 코드는 내가, 그리고 Copilot만이 이해했다. 이제는 Copilot만이 안다”라는 농담도 나옴
          + ""실세계와 연결된 충분히 큰 시스템은 수학적으로 옳다고 입증할 수 없다""는 데 대해 형식 검증에 종사하는 사람들은 격렬하게 반박할 수도 있고, 속으론 동의할 수도 있음
          + 두 시나리오 중에서 버전업할 때 어디가 더 싸고 빠른지 물어봐야 함. 지금 당장은 인간 개발자 모델 쪽이 저렴하다고 생각하지만, 미래는 확신할 수 없음
          + 사실 두 가지 옵션 모두 현실에선 거의 본 적이 없음
     * 궁극의 발전 단계는, 인간이 읽을 필요조차 없는 완전히 기계 지향적 프로그래밍 언어가 아닐까 싶음. LLM이 굳이 Python이나 Swift처럼 사람이 이해하기 좋은 언어로 변환할 필요가 뭐가 있나? 그냥 바로 동작 가능한 결과만 있으면, 더 이상 유지보수 개념도 무의미해짐. 아직 그 단계는 아니지만, 언젠가는 여기로 갈 것 같음
          + 좋은 소프트웨어란 항상 유지보수 중이어야 함. 새로운 요구가 끝없이 나오니까 기능 배포 한번 하고 영원히 끝이라는 믿음부터가 농담거리임. 미래 변화를 염두에 두고 만든 코드, 테스트, 문서가 그래서 중요함. LLM이 무의미한 블랙박스 코드를 양산한다면 그것만큼 무서운 일 없다고 생각함. LLM이 인간 수준 코딩에 도달해 그걸 아무도 신경 안쓰게 된다는 건 공상과학의 영역임. 코딩은 실질적으로 유용한 소프트웨어를 만드는 전체 과정의 일부에 불과함
          + 사실 기계어가 이미 그런 수준 아닌가? LLM은 인간이 읽을 인터페이스에 최적화됐고, 그래서 JSON을 만들고 BSON은 거의 안씀
          + 이걸 도대체 어떤 문제를 해결한다고 할 수 있을지 의문임. 만들어지는 문제는 분명함
          + 마치 LLM이 인간을 위한 코드를 학습하고, 그대로 넣고, 다시 코드를 돌려서 원하는 동작을 얻는 일종의 ‘전화 게임’ 느낌임. 정말로 행동을 바로 생성할 수 없을까라는 생각도 듦
          + 사람 읽기 힘든 언어라면 Malbolge가 대표적임. 실제 첫 ""Hello World"" 프로그램도 유전자 알고리즘이 만들어냄
     * 원저자임. 여러분과 나눌 대화에 신남
     * ‘vibe coding’이라는 용어는 너무 완벽한 표현임. 마치 ‘클라우드 컴퓨팅’이 엄청 확장된 것과 같음. 원래는 탄력적으로 EC2 인스턴스 켜서 작업 끝나면 날리는 걸 뜻했지만, 은유가 너무 직관적이라 Gmail조차 모두 ‘클라우드’로 불리게 됨.

   신용카드를 아이에게 주는것과 같다는 비유
   적절한 비유네요
   또는 칼을 아이에게 주는것에도 비유 할 수 있을것 같습니다

   AI생성코드에 주석까지 같이 달아주고 코드흐름도 등을 그려주는 코드생성AI가 나오면 어느정도 도움이 되겠네요.

   상당히 공감이 되는 이야기입니다. 실제 일부 겪어 보고 있기도 하고…
   모델의 성능 변화에 따라 이 부분이 어떻게 변해갈지가 궁금하기도 하면서.

   그러면 자동차는 그 내부구조를 다 이해하기 전까지 영원히 탈 수 없는건가요?

   탈 수 있죠. 근데 만들진 못하겠죠.

   내부구조를 이해하지 못하고 자동차를 만드는 것 = 바이브 코딩

   방법과 기술의 문제라 생각 그냥 ai쓰지말고 오가닉 손코딩 해야 한다는 애들은 공학계산기 말고 주판 두드리거나 엑셀 팡션 쓰지말고 수기작성하는게 진리란 애들 같음

   잘못된 비유입니다. 공학계산기는 계산기나 엑셀과 마찬가지로 입력값에 따라 결과가 정확하죠. AI가 사용자가 예측한 결과를 그대로 출력한다면 지금껏 숱하게 나온 신기술과 그렇게 많이 다른 기술은 아니었을 겁니다. 시간이 갈 수록 보안과 할루시네이션에 대한 우려가 커지는 이유이기도 하죠. Gen AI는 컨트롤 할 수 없다는 말입니다. 현재 LLM의 한계를 이해하고 적절한 곳에 사용되어야 합니다.

   현재 바이브 코딩은 이제 태동기 이며 내년 내후년에는 성숙된 개발 방법론이 되리라 봅니다. devops가 aidevops가 되듯이 aiagile 또는 vibeagile이 되리라 생각합니다.
"
"https://news.hada.io/topic?id=22319","Google, goo.gl 정책 변경: 비활성 링크는 비활성화, "활성 링크는 보존"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Google, goo.gl 정책 변경: 비활성 링크는 비활성화, ""활성 링크는 보존""

     * Google은 2025년 8월 25일 이후 모든 goo.gl URL 지원 종료를 이전에 발표했지만, 실제로 사용 중인 링크를 보존하기 위해 계속 유지하는 것으로 정책을 조정함
          + 수많은 문서, 동영상, 게시글 등에 goo.gl 링크가 포함되어 있다는 점과 사용자들의 의견을 반영
     * 2024년 말 이후 활성화 이력이 없는 goo.gl 링크는 9개월 전부터 ""곧 링크가 비활성화됩니다""라는 안내 메시지로 리디렉션되고, 이러한 링크만이 2025년 8월 25일 이후 비활성화 대상
          + 안내 메시지가 뜨는 링크를 사용 중인 경우, 2025년 8월 25일 이후에는 더 이상 동작하지 않기 때문에, 다른 URL 단축기로 전환하는 것을 권장
     * 그 외의 모든 활성 goo.gl 링크는 유지되며 정상적으로 기능할 예정
     * 본인의 goo.gl 링크가 계속 유지되는지 확인하려면 해당 링크에 접속해 안내 메시지가 표시되지 않고 정상적으로 리디렉션된다면 계속 사용할 수 있음

        Hacker News 의견

     * 놀라운 점은 이게 원래 기획된 방향이 아니라는 점임. 어떤 제품 매니저가 ""고객을 위해 데이터 삭제가 최선""이라고 생각하냐는 의문이 생김. 구글은 이 링크들이 수많은 문서, 영상, 게시물 안에 포함돼 있다는 사실을 알고 있고 피드백도 받고 있다고 했음. 그러면 애초에 이 링크들이 어떻게 쓰이고 있는지 알지 않았냐는 생각이 드는 상황임
          + 예전 HN 스레드에서 봤는데, ""이건 전형적인 데이터 기반 제품 결정""이란 말이 있었음. goo.gl 링크를 중단하면 특정 비용을 줄일 수 있다고만 계산하는 접근. 고객에게 어떤 영향을 줄지는 별로 고민하지 않은 듯함. 또한 뭔가 서비스를 즉흥적으로 종료하는 걸 대수롭지 않게 여기는 문화가 있어서 가능한 일임
          + ""링크들이 어떻게 쓰이고 있는지""에 대해 이야기하면, 크롬 개발 프로세스 문서에서 이런 내용이 있었음: ""심지어 극소수 유저(0.01%)가 사용하는 기능도, 구글 스케일에선 굉장히 많은 유저임. 영향이 정말 미미할 때만 기능 제거 검토."" 그래서 이런 정책이 구글 전체에 적용됐어야 한다고 생각하게 됨. 그렇지만 실제로는 별로 놀랍지 않게 이런 일이 벌어짐
          + 이상함. 문서, 소셜 글, embed된 오래된 자료 등 아주 예전 링크들도 있는데, 최근 트래픽만 신경쓰는 듯함. goo.gl이 단순히 누군가의 링크트리 등 최신 목적으로만 사용된다고 착각하는 듯. 마케팅 담당자가 웹 환경 전체를 잘못 보고 있는 느낌임
          + 규제가 강한 프라이버시 환경에서 일하는 사람이라면 이런 선택을 할 수 있을 것임. 오래된 시스템에 남겨진 사용자 데이터는 리스크이기 때문임. 구글 내부적으로도 변화가 있지만, 외부적으로 법률도 계속 바뀌고 있기 때문에 어쩔 수 없이 따라가는 거라고 생각함
          + tail -f access.log 같은 걸로 트래픽 직접 확인해보는 게 필요했을 거라는 의견임
     * ""활성화되어 사용하는"" 기준 때문에, 누군가 신뢰해서 만든 중요 문서의 링크가 사라지는 불상사가 생김. 자세한 동기는 모르겠지만, 외부에서 볼 때 이건 ""Don't Be Evil"" 슬로건을 쉽게 지킬 수 있는 사안임: ""새로운 링크는 중단하되(고지 필요), 기존 저장된 정보는 이미 우리에게 맡긴 것이니 반드시 정리·보존해야 함. 우리는 구글이니까, 충분히 처리 가능함. 심지어 서비스 종료보다 더 간단할 수도 있음""이라고 생각했음
          + ""구글 링크를 신뢰한 게 실수""라는 시각도 존재함. 구글 서비스에 의존하면 언젠가 이런 사태가 발생할 것임
          + ""Don't Be Evil"" 슬로건 자체가 오래전에 퇴장했다는 견해도 있음
          + 현업 구글 내부 사정에 대해, 구글에는 과거 코드를 운영하기 어렵게 만드는 내부 프로세스가 있어서 계속해서 상향 호환 안되는 API 변경에 대응하는 엔지니어링 팀이 필요하다는 점을 언급함. 즉, 구글을 OS라고 보면, 매 몇 년마다 Google 8에서 9로 업그레이드하면서 전부 새로 써야 하는 식. 구글의 인프라에서 이런 식의 '끝없는 업그레이드 트레드밀'이 자연스러움
          + ""과거를 지배하는 자가 현재를, 현재를 지배하는 자가 미래를 지배함""이라는 말을 인용하면서, 검색결과 변화만 봐도 알 수 있다고 언급함
     * ArchiveTeam Warrior 워커를 돌리면 Internet Archive에 링크들을 아카이브하는 데 도움을 줄 수 있음. 자세한 내용은 이전 토론 참고
          + 아카이브 진행 현황이 궁금하다면 트래커에서 확인 가능함
          + ARM 컴퓨터(M1 맥 등)에서는 아직 ArchiveTeam 소프트웨어가 지원되지 않는 게 아쉬움
     * ""비활성화""라는 조치 자체가 과연 무슨 의미가 있을까라는 의문 제기임
          + 이런 조치가 링크재킹(linkjacking) 방지에 도움이 될 수 있음. 예를 들어, 오래 전에 Acme Corp가 goo.gl 링크로 FAQ 페이지를 만들고, 회사가 사라지면 도메인이 만료될 수 있음. 이후 악의적인 누군가 도메인을 인수, 예전 goo.gl 링크를 클릭할 시 피싱이나 악성 사이트로 연결될 위험이 있음. 링크가 일정 기간 동안 클릭되지 않으면 자동으로 구글이 비활성화해서, 이후에는 구글 404를 보여줌으로써 위험 차단 가능함
          + 이런 조치는 사실 어떤 매니저의 실적 쌓기용일 수도 있음. ""단기간에 링크 단축 서비스 비용 70% 절감"" 등 성과보고를 할 수 있음. 실제 절감 수치는 들여다보는 사람이 없다는 전제임
          + AT&T Bell처럼 강제로 분할될 위험에서 사전 경고를 넣는 걸 수도 있다는 농담 섞인 의견도 있었음
          + 데이터베이스 작업량이 늘어났기 때문일 수도 있음
     * 최근 구글 서비스 일부에서 공유(Share) 버튼 누르면 goo.gl 대신 https://share.google 링크가 생성되는 걸 봤음. 이게 동일한 단축링크 플랫폼인지 궁금함. 또, .gl TLD가 그린란드와 연관되어 있어서 관련이 있나 궁금증이 듦. share.google로의 리디렉션이라면 괜찮겠다는 생각임
          + share.google과 goo.gl의 주요 차이점은, share.google은 구글이 직접 관리하는 링크에만 적용되고, goo.gl은 사용자가 임의의 URL을 단축할 수 있었던 점임. 문제는 이 과정에서 구글 브랜드의 신뢰를 피싱이나 악성 링크에 악용할 수 있어 결국 위험을 따랐다는 것임
     * 관련 소식으로, 구글 단축 goo.gl 링크가 다음 달부터 동작하지 않는다는 글과 구글 URL Shortener 링크가 더 이상 제공되지 않는다는 글이 있음
     * 구글 브랜드를 이렇게 쉽게 피싱에 악용될만한 것에 사용하는 게 잘못된 아이디어였다고 믿음. 처음부터 뚜렷한 구글 브랜드가 들어가지 않은 뭔가를 사용했어야 한다고 생각함
     * 다음엔 내가 지난 8월에 읽지 않은 책 페이지를 구글이 무작위로 찢어버리는 거 아닌가 하는 농담이 나올 정도로 황당하게 느껴짐
     * ""쿨한 URL은 변하지 않는다! 쿨한 URL은 변하지 않는다!!""라고 계속 주장하다가, 결국 bit.ly로 바뀌어가는 상황을 피부로 느끼는 중임
     * 이 서비스 유지에 사실상 구글의 비용이 0에 가깝다고 생각함. 구글이 계속해서 이 서비스를 무료로 제공하며 좋은 시민 역할을 할 수도 있었는데 일부러 그렇게 하지 않은 선택임
"
"https://news.hada.io/topic?id=22256","잠은 결국 미토콘드리아로 귀결됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           잠은 결국 미토콘드리아로 귀결됨

     * 최근 연구에서 “졸림과 수면 요구”의 진짜 생화학적 기원이 미토콘드리아와 깊게 연관된 것으로 제시됨
     * 곤충의 뇌 영역 연구를 통해, 수면 부족 시 미토콘드리아 기능 및 시냅스 전달과 관련된 유전자 발현 증가 현상 확인됨
     * 미토콘드리아 내 전자 운반 과정의 변화가 실제로 졸림 신호 유발과 직접적으로 연결됨
     * 산소 호흡 자체가 신경계 미토콘드리아 회복이 필요한 수면 상태를 필연적으로 유발함을 시사함
     * 잠과 배고픔 모두 에너지 균형 및 미토콘드리아 관리 과정과 밀접한 연관성 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

수면의 진짜 기원, 미토콘드리아

   수면이란 왜 꼭 필요한지, “졸리고 누워야겠음”이란 감정의 실제 생화학적 근원이 무엇인지에 대해 오랜 시간 명확하게 밝혀지지 않았음. 세포 수준에서 다양한 변화가 잘 관찰되지만, 과연 이 변화들이 수면 필요성을 알리는 원인인지 아니면 결과인지 혼동이 있었음. 이러한 인과관계의 혼란은 알츠하이머병과 아밀로이드 침착의 관계처럼 생의학 영역에서 흔히 나타나는 문제임.

곤충 모델에서 밝혀진 미토콘드리아의 역할

   과학자들은 초파리의 뇌 영역(특히 dorsal fan-shaped body) 이 수면 조절의 핵심 역할을 한다는 사실을 밝혀왔음. 최근 연구에서는 이 뇌 부위의 뉴런을 수면 결핍 상태로 만들어 단일세포 분석을 진행하였고, 122개 유전자 발현이 증가하는 현상 중 특히 미토콘드리아 기능, 시냅스 전달과 관련된 변동이 두드러졌음. 세포 내 자세한 관찰 결과, 수면 결핍이 심해질수록 미토콘드리아의 파편화, 미토파지(미토콘드리아 재활용), 미토콘드리아와 소포체의 직접 접촉이 증가하는 것으로 나타났음. 소포체와의 접촉은 산화 스트레스로 인해 손상된 지질을 새롭게 보충받기 위해 이루어지는 것으로 여겨짐.

전자운반 변화와 수면 신호의 직접적 연결

   연구진은 미토콘드리아 내 전자운반 과정을 인위적으로 변화시키는 다양한 실험을 진행함. 이 과정의 변동이 곧, 해당 뉴런의 수면유도 과정과 직접적으로 연결되는 현상이 관찰됨. 수면 부족 상황에서 미토콘드리아의 끊임없는 활동은 미토콘드리아의 분열, 재활용 증가로 이어지고, 전자들의 축적이 결국 “수면 욕구”를 결정하는 가장 근본적인 요인임을 시사함.

산소 호흡과 수면의 필연적 상호관계

   논문에서는 유산소 호흡 그 자체가 신경계 미토콘드리아의 회복, 즉 수면 상태가 필수로 요청되는 대가임을 강조함. 인간에서도 미토콘드리아 기능장애 환자에서 “극심한 피로감”이 주요 증상으로 흔히 보고되며, 이러한 피로는 보통 “회복성 수면”을 통해 개선되지 않음.

에너지 균형, 미토콘드리아, 그리고 수면과 배고픔

   연구팀은 이로 인해 잠과 배고픔 모두 미토콘드리아의 에너지 균형과 연결되어 있음을 강조함. 유산소 호흡을 하는 생명체는 항상 미토콘드리아 연료 공급과, 특히 신경계 미토콘드리아의 휴식 및 회복 시간을 조율함. 연구진은 “전자들이 호흡사슬을 통해 흐르는 것은 마치 모래시계의 모래가 흘러가듯, 언제 균형을 바로잡아야 하는지 결정한다”고 우아하게 표현함. 이 과정 외에도(예: 기억 형성) 수면 주기에 합류한 다양한 기능이 있지만, 미토콘드리아 기능이 모든 수면의 가장 근본적인 토대일 것으로 제안함. 결론적으로 산소를 소비하는 생물은 곧 “잠이 필요함”을 피할 수 없음.

        Hacker News 의견

     * 수면이 필요한 고대의 미스터리가 마침내 풀렸을 수도 있다고 생각함. 논문에 따르면, 초파리의 수면 유도 뉴런에서 Ucp4A/Ucp4C를 통해 일어나는 세포 자율적 약한 미토콘드리아 분리작용이 ‘미토콘드리아 Δp’ 감소와 이로 인한 전자 누수를 줄여 잠을 덜 자게 만든다고 나옴. 이 생화학적 과정을 근거로 수면의 이유를 설명할 수 있다는 점이 흥미로움. 대부분의 미토콘드리아 분리제는 BBB(혈뇌장벽)를 잘 통과하지 못하는데 이번 연구는 매우 국소적이고 전례가 드물다고 봄. 만약 이 논문이 맞다면, 수면의 미스터리가 풀림과 동시에 ‘건강한’ 각성 유도 신약이 나올 수도 있다고 기대함. 또, 깊게 자는 사람, 얕게 자는 사람, 혹은 수면 요구량의 차이가 이 기전을 통해 바뀔 수 있는지 궁금해짐
          + 수면이 단일한 목적만을 위한 것이라면 정말 놀랄 것임. 복잡하게 상호 연결된 시스템에서 단일 인과관계로 모든 것을 설명하는 것은 거의 안 맞다는 생각임
          + 진화적으로 이런 식의 수면이 시작됐다는 설명은 그럴싸하지만 인간은 이외에도 수면을 기억 고정(수면 방추 신호)이나 절차적 기억 통합(REM 수면) 등 다양한 용도로 씀
          + 이 주제에는 더 많은 층위가 있음. 우리가 수면을 취하는 이유 중 일부는 ‘고대의 미스터리’가 아님. 주어진 환경이 낮과 밤의 주기를 갖고 있고, 진화적으로 주간에 최적화된 작업을 주간에 하고, 야간엔 신체 사용을 멈추고 다른 활동을 수행하는 것이 유리했기 때문임. 만약 낮-밤 주기가 없었다면 뇌가 꼭 ‘밤 동안’ 신체를 쉬게끔 진화하지 않았을 것임
          + 지금의 연구는 ‘수면이 왜 필요한지에 관한 새로운 이론’임. 진짜로 미스터리가 풀렸다고 단정 지을 수 없음. 논문이 정말 새로운 이론인지도 확실치 않음
          + 이 맥락에서 수면의 보다 일반적인 이유를 Russell Foster의 저서 ‘Life Time’에서 인용하고 싶음. “수면은 종에 따라 24시간 주기에 적응한 행동임. 잠자는 동안 신체의 비활동으로 자신이 잘 적응하지 못한 환경에서의 활동을 피하고, 그 시간 동안 생리적으로 반드시 필요한 청소 작업을 함” 이 책은 수면에 대해 가장 명쾌하게 설명함
     * “전자(electrons)가 해당 피드백 컨트롤러의 호흡 사슬을 모래시계처럼 흘러가면 균형이 회복되어야 할 때를 정한다”라는 문장이 정말 최고임. 내가 좋아하는 문장으로 로얄 소사이어티 전설적 회의에서 나온 “마치 우리는 반지의 제왕 스타일의 세계를 보고 있다는 듯하다”와 함께 최고의 문장이 되었음
       관련 링크
          + 이번엔 모래가 너를 구해주진 않겠지만, 이 모래는 네 시간을 절약하게 해줌
     * 논문은 여기임. 전문가가 아니라 잘은 모르지만, 논문은 개념을 좀 단순화해 과장한 듯한 느낌임. 초파리 대상으로 했을 뿐 아니라, 엄밀히 말해 ‘잠’이 아니라 ‘휴식’에 더 가까운 것 아닐까 하는 생각도 듬. 흥미롭고 읽을 만하지만, 더 긴 연쇄 기전이 있을 것 같고 인간이나 포유류에서도 재현될지 확신이 없음. 내가 틀릴 수도 있겠지만 불확실함
          + 이 논문은 별로임. 스스로 이 분야의 전문가라고 자부하는데, 과학계 현실이 원래 이런 것이라 씁쓸함
     * 충분히 잠을 자지 않으면 결국 죽음에 이르게 되는 이유가 이런 미토콘드리아 메커니즘 때문일까 궁금했음. 기네스북에서 극단적인 깨어있는 시간 기록을 받지 않는 이유도 러시안 룰렛 기록을 받지 않는 것과 동일함
          + 실제로 기네스는 건강 문제로 수면 부재 관련 기록 집계를 중단했음. 수면 박탈이 극심하면 정신분열과 정신병원 입원, 수면 유도제 처방에 이르기도 함. 단기적으로 수면이 부족하다고 바로 죽는 것은 아니지만, 장기적으로(수년~수십 년) 만성적 잠 부족은 사망 위험을 높임
     * 이 연구와 자연 단기 수면(familial natural short sleep)과의 연관성이 흥미로움. 특히 관련 유전자를 가진 사람들은 알츠하이머로부터 보호 효과도 보임. 이는 이 유전자 변이가 논문에서 설명된 미토콘드리아 유지 사이클과 하위적으로 작용할 가능성을 시사함
     * 인터넷 괴담이라 생각했던 “미토콘드리아는 박테리아에서 유래해서 항생제를 복용하면 손상될 수 있다”는 이야기가 실제로 과학적으로 근거가 있음
          + 특히 퀴놀론(Quinolones) 계열 항생제가 미토콘드리아에 해를 줄 수 있음. 페니실린 같은 항생제에는 해당 위험이 없음. 곰팡이(진균)에서 유래된 페니실린은 진균 역시 미토콘드리아가 있기 때문임. 일반적으로 가장 약하고 특정 균에만 잘 듣는 항생제를 쓰고, 퀴놀론 계열이 무턱대고 처방된다면 반드시 세균 이름 등 명확한 근거가 제시되어야 함
            관련 링크
          + 실제로 리보솜 저해제, 특히 테트라사이클린 같이 미토콘드리아에 영향을 주는 항생제가 문제임
            관련 논문
          + 고전적인 항생제는 박테리아 리보솜(원핵 리보솜)에 작용하고, 진핵세포 리보솜과는 엄청나게 다름. 물론 예외 사례가 있을 수 있지만, 그렇기 때문에 신약은 충분히 검증함
          + 이런 이야기를 할 때에는 매우 조심해야 함. 이미 과학이나 의학을 잘 모르는 사람들이 이 이야기를 반(反)의학, 반백신 주장 근거로 삼기 쉽기 때문임. 항생제마다 표적이 다르고 대다수는 미토콘드리아에 거의 영향을 주지 않음. 미토콘드리아가 세포와 공생한 것은 약 30억 년 전이고, 지금은 완전히 얽혀 있어 분리된 생물체로 보기도 어려움
     * ChatGPT가 카페인은 간접적으로 UCP(uncoupled protein) 활성화에 역할한다고 알려줬는데, 우리가 일상적으로 각성제로 사용하는 성분이 실제로 덜 자도 되도록 작용하는 점이 신기하게 느껴짐
     * 신체적 피로와 정신적 피로(졸림)는 다름. 하루 종일 소파에 누워 있어도 밤이 되면 여전히 졸리고, 잠을 안 자면 오래 버틸 수 없음. 정신적으로 수면이 필요한 이유는 뇌가 오프라인(감각 입력 없는) 상태에서 하루 동안의 단기 기억을 정리 및 보관하는 등 ‘청소’ 시간이 필요해서임
          + 전자 누수가 생기는 한 가지 원인은 (ChatGPT에서 읽음) NADH(연료)가 ATP(에너지 수요)를 초과할 때임. 그래서 몸을 피곤하게 만들면 정신적 수면 욕구를 늦출 수 있고, 이 두 과정은 완전히 별도라고 할 수 없음
          + 뇌도 정신적으로 힘든 작업을 할 때 평소보다 전력을 미세하게(5% 정도) 더 소비함. 그래서 방전된 미토콘드리아를 충전해야 하고, 하루 종일 게으르게 보낸 날에도 피곤한 게 어찌 보면 당연함
     * 미토콘드리아 숫자와 효율을 높이는 것은 매우 중요함. ME/CFS(만성 피로 증후군)는 이런 소기관이 제대로 작동하지 않아 생긴다고 봄
          + 레드 라이트 테라피(red light therapy)를 적극 추천함. 미토콘드리아 효과 관련 모든 논문을 정리한 스프레드시트 링크가 있음
          + 관심 있는 사람은 “MOTS-C”랑 “SS-31”을 검색해 보길 권함. 둘 다 미토콘드리아 기능을 높여주는 펩타이드이고 온라인에서 손쉽게 구할 수 있음. MOTS-C가 특히 흥미로운데, 아직 시도해보진 않았지만 20mg 바이알을 보유 중임
          + 나 역시 크렙스 사이클 주요 단계에 관여하는 보충제, 또 ROS 손상을 지연시키는 글루타치온 보충제를 활용해 체감상 큰 효과를 누리고 있음. 이런 날엔 장시간 프로그래밍이나 회사 정치 같은 일에도 집중력이 대폭 올라감. 다음 목표는 심폐지구력을 대폭 늘리는 것인데, 폐 기능이 약간 안 좋아 넘어야 할 산이 있음
          + ME/CFS는 Myalgic Encephalomyelitis/Chronic Fatigue Syndrome(근육통성뇌척수염/만성 피로 증후군)임
          + 만성 피로 증후군이 실제로 존재하는지 확신이 가지 않음. 그냥 “잘 모르겠는 질환”을 포괄하는 진단명 같아서 혼란스러움
     * 수면이 엄청나게 중요함. 내가 본 워커홀릭들은 대부분 잠을 거의 자지 않다가, 결국 인생 후반에 큰 문제에 시달리는 것을 너무 자주 목격함
          + 해고된 후의 최고의 장점 중 하나는 아침에 원하는 만큼 잘 수 있음
          + 워커홀리즘은 언제나 그 이면에 정신적 문제, OCD(강박), 심각한 불행, 공허함 회피 등이 깔려 있음. 이런 상태는 다양한 파괴적 행동으로 이어지고, 수면 박탈도 이런 악순환의 일부를 만듦. 결국 심리를 피할 수 없고, 기본 심리학 개념만 익혀도 사람을 다루고 이해하는 게 훨씬 쉬워짐. 학교에서 이런 걸 제대로 가르쳐줬으면 좋았겠다고 생각함
"
"https://news.hada.io/topic?id=22332","아이들이 잘못된 읽기 교육으로 읽기를 어렵게 배우는 문제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    아이들이 잘못된 읽기 교육으로 읽기를 어렵게 배우는 문제

     * 미국 공립학교에서 채택된 '삼중 단서(three cueing) 이론' 이 학생들을 읽기 어렵게 만드는 원인임
     * 이 이론은 과학적으로 증명된 읽기 방법과 배치되지만, 여전히 교사 연수와 교재에 남아 있음
     * 학생들은 단어 암기, 문맥 추측, 건너뛰기 등 비효율적 전략을 학습하며, 이는 평생 읽기 어려움으로 이어짐
     * 인지과학 연구는 철자-소리의 규칙적 인식(phonics) 이 숙련된 읽기의 핵심임을 알려줌
     * 많은 학교가 옛 이론에 수백만 달러를 투자하고 있으며, 교육의 변화와 재검토 필요성이 부각됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 읽기를 어려워하는 아이들

     * Molly Woodworth는 학업 성취 우수생이었지만 어릴 적부터 읽기에서 어려움을 겪었음
     * 그녀는 스스로 단어를 암기하거나, 문맥 단서로 의미를 유추하거나, 모르는 단어를 건너뛰는 등 세 가지 전략으로 읽기를 넘겼음
     * 아무도 Woodworth의 읽기 문제를 몰랐고, 중요한 것은 미국의 많은 아동들이 그녀와 같은 경험을 한다는 점임
     * 미국 초등학교에서는 decades간 과학적으로 오류가 명백한 읽기 교육 이론이 표준으로 자리 잡았음
     * 이로 인해 많은 학생들이 읽기를 제대로 배우지 못하고, 졸업까지도 능숙한 독자가 되지 못함

'Three Cueing Theory'(삼중 단서 이론)의 등장과 영향

     * 1967년 Ken Goodman이 제시한 삼중 단서 이론은 문자, 문장 구조, 의미 세 가지 단서로 단어를 읽는다고 봄
     * Goodman과 동료 Marie Clay는 아이들이 실수하는 유형을 관찰로 파악하며, 정확한 단어 인식보다는 의미 파악을 중시함
     * 이 접근법은 'whole language' 및 'Reading Recovery' 같은 국제적으로 널리 채택된 읽기 교육법의 기초가 되었음
     * 1980년대부터 미국 전역의 학교에서 지도 교재와 교사 연수로 대중화됨

인지과학의 반론 및 연구 결과

     * Keith Stanovich 등 인지과학자들은 실제로 숙련된 독자가 단어를 더 빠르고 정확하게 식별한다는 사실을 밝혀냄
     * 연구에서 숙련되지 않은 독자가 오히려 맥락에 더 의존하고, 숙련 독자는 철자와 소리를 바탕으로 즉시 단어를 인지함
     * 약한 단어 인식 능력이 읽기 문제의 주요 원인임이 반복적으로 확인됨
     * 그러나 '삼중 단서' 방식은 여전히 교실에서 폭넓게 사용 중임

교실 현장: 잘못된 전략의 재현

     * 많은 교재와 교실에서 그림 보기, 첫 글자 소리, 문맥 유추 등 삼중 단서 전략을 강조하는 자료가 사용됨
     * 학생들은 단어의 철자와 소리를 익히기보다는 단어 패턴 암기, 그림/패턴 추측에 익숙해짐
     * 'Guided Reading', 'Leveled Literacy Intervention', 'Units of Study for Teaching Reading' 등 유명 교재도 삼중 단서 접근법에 의존
     * 이러한 교육을 받은 아이들은 실제로는 읽기 기술 대신 '유사 읽기' 습관만 형성할 가능성이 높음

대조 실험: 읽기 지도 방법의 실제 효과

     * Oakland Unified School District의 교사 Goldberg는 삼중 단서 방식과 체계적 음운-음철 지도(phonics) 프로그램(SIPPS) 을 비교
     * 음철 중심 지도를 받은 학생이 실제로 정확하게 단어를 읽고 스스로 문제 해결함을 관찰
     * 삼중 단서로 배운 학생은 사진, 문장 패턴에 의존하며 실제 단어 읽기에 어려움이 계속됨
     * Goldberg는 과학적 연구에 근거한 지도 방법의 필요성을 절감함

'Balanced Literacy' 문제점

     * 'Balanced Literacy'는 음철 지도와 다양한 읽기 경험의 혼합이라 주장하지만, 실제로는 삼중 단서와 예전 방식이 혼합됨
     * 공식 보고서(미국 2000년, 영국/호주 등)는 음철 지도 필수성을 강조하지만, 많은 교재에서 삼중 단서가 계속 사용됨
     * 삼중 단서 습관은 아이들이 단어의 철자, 소리를 통합해 기억하는 '정서적 맵핑(orthographic mapping)' 과정을 방해
     * 아동이 음철 능력을 익히지 못하면, 책에 그림이 줄고 단어가 늘어나는 3학년 이후 읽기에서 뒤처질 위험이 큼

학교 변혁의 시도와 현장의 어려움

     * Oakland 등 일부 학군은 삼중 단서를 배제한 새로운 자료와 교수법을 시도
     * 구체적 음철 지도, 어휘(oral vocabulary) 확장, 읽기 실습 등이 중심
     * 교사들의 이해 부족·교재 선택권 부재·시장 구조 등으로 혁신이 쉽지 않음
     * 제품을 납품하는 대형 출판사(Heinemann 등)와 유명 저자들은 과학적 비판을 수용하지 않거나 외면
     * 현장 교사들은 기존 시스템/교재에 신뢰 의존, 구조적 변화 필요성 절감

결론 및 제언

     * 삼중 단서 이론은 관찰과 이론에 기반하였으나, 실험적 인지과학 증거와 일치하지 않음
     * 과학적 연구는 단어의 정확한 철자-소리 대응 습득이 능숙한 읽기의 핵심임을 보여줌
     * 전체 교육 체계, 교재 검토, 교사 연수의 방향 전환이 필요
     * 학교와 교사는 자신들의 자료 속에 삼중 단서 관련 흔적이 있는지 점검하고, 학생들이 읽기에서 불이익을 받지 않도록 주의 필요
     * 읽기 교육의 효과적 변화가 어린 학생들의 장기적인 학습, 진로, 삶의 질에 결정적으로 중요함

참고: 제작진 및 프로젝트

     * 본 다큐멘터리는 APM Reports가 제작, Educate 팟캐스트의 일부로 제공
     * 다양한 교육, 기회, 학습 방법에 관한 실화 다큐멘터리 시리즈임
     * 기획, 취재, 제작 등 자세한 제작진 및 후원 재단 명단 포함
     * 학부모, 교사, 교육 관계자 누구나 읽기 교육에 대한 이해와 논의 필요성을 강조

부록 및 리소스

     * 본 기사 외에도 피드백, 뉴스레터 구독, 관련 리소스 제공
     * 독자 의견과 경험담 공유를 장려함

        Hacker News 의견

     * 나는 내 첫 음악(음표 읽기) 수업이 기억남. 종이에 문장이 적혀 있었고, 선생님이 각 단어를 'titi' 또는 'ta'로 바꿔놓았음. 우리에게 그걸 반복하라고 시켜서 한 주 내내 'titi'와 'ta'로 가득 찬 숙제를 했음. 운 좋게 좋은 점수를 받았지만 너무 혼란스러워서 음악이 너무 어려운 것 같아 포기하고 싶었음이 떠오름. 두 번째 수업 때 선생님이 '이제 어려운 단어를 배워야 해. ti는 4분음표고 ta는 2분음표야'라고 말하셔서 그제서야 조금 이해가 됨. 근데 또 '이건 너무 어려운 단어들이니까 이해하려 하지 말고 그냥 외우고 상황에 따라 맞는 걸 쓰면 돼'라고 하면서 계속 어려운 용어라며 제대로 알려주지 않고 외우라고만 강조함. 이런 선생님 방식은 그 뒤로도 의심하게 되었고, 난 항상 정확한 규칙이나 용어를 알려달라고 요구했음. 나중에 경제 선생님도
       'debit'이나 'credit'의 균형을 제대로 설명하지 않고 느낌대로 하라 하던 일이 떠오름
          + 내 첫 피아노 선생님은 예술적이고 감각적인 분이었는데, 나는 논리적인 학습자라서 그분과 전혀 연결이 안 됨. 거의 10년을 같이 했지만 서로 맞지가 않았음. 최신 피아노 선생님은 음악 교육학 교수였기에 나처럼 논리적인 학생도 잘 다루었음. 음악과 악기 배우기는 본질적으로 직관적이어야 하고, 연주는 매우 표현적이기에 음악은 자유롭게 느끼고 싶은 창의적인 사람들도 많이 끌림. 그러나 음악 이론과 클래식 음악 공부는 엄밀한 영역으로, 모든 용어를 배우는 걸 좋아하는 논리적인 사람들에게도 매력적임. 이론 용어를 아는 게 연주에 꼭 필요한 건 아니고, 그래서 음악을 찾는 사람이 매우 다양함. 실제로 음악에는 직관적인 부분이 필연적으로 있고, 교수님은 소리와 몸의 감각에 항상 피드백을 주며 느끼고 연주하는 걸 강조함. 논리적으로만
            접근할 수 없는 영역이고, 오히려 스포츠와 비슷하다는 인상도 받음. 최근에는 노래 수업도 시작했는데, 피아노보다 더 신기한 경험임
          + 라틴어에서도 비슷한 현상을 봤음. 명사에서 'nominative', 'accusative', 'genitive'와 같은 전통적 용어 대신 'case 1', 'case 2', 'case 3'으로만 알려줬음. 이 방식은 과거 수 세기 동안 쌓인 라틴어 문법 지식과의 연결을 끊고, 오히려 의미 없는 숫자가 더 쉽다고 착각하게 만듦. 참 어리석은 일임
          + 도대체 그 과제가 뭘 가르치려고 했던 건지 잘 모르겠음. 예시 문장과 'titi'와 'ta'로 바꾼 번역이 있으면 궁금함. 전문가는 아니지만 13살 때부터 피아노와 기타를 쳐왔는데도 이 활동에서 배울 수 있는 게 뭔지 잘 상상이 안 감. 나만 그런 건지도 모르겠음
          + 지금 일어나고 있는 LLM과 컨텍스트 엔지니어링의 상황과 비슷한 비유 같음
          + 이전 오케스트라 선생님은 늘 우리를 바로 실전에 투입시켰음. 그렇게 하면 압도당하지 않고, 오히려 더 많이 배울 수 있었음. 더 친절한 선생님 밑에서 배운 사람들보다 더 많은 걸 배운 것 같음
     * 이런 저널리즘은 공영방송공사(Corporation for Public Broadcasting)의 지원을 받기도 하는데, 이번 정부에서 방금 지원이 중단됨을 환기시키고 싶음. 자세한 내용은 Funders 섹션 참고 바람
     * 나는 파닉스(phonics)로 읽기를 배워 조금도 주저함 없이 훌륭한 읽기 능력을 갖게 되었음. 근데 나중에 교육계에서 '더 나은' 읽기 방법을 만든답시고 내 동생들을 완전 망쳐놓았음. 파닉스로 다시 돌아가는 것 같아 기쁨
          + 파닉스가 효과적이라는 데이터가 있긴 하지만, 나에게는 다른 방법이 더 맞았을 수도 있다고 느껴짐(학교에서는 파닉스 방식이었음). 내가 파닉스에서 힘들었던 점을 떠올리면, 첫째로 영어의 불규칙함 때문에 파닉스로는 한계가 있고 결국 암기와 문맥이 필요함(ex: cough, rough, through 등). 대부분의 언어에는 영어처럼 스펠링 콘테스트가 없는 이유가 발음이 명확하게 매핑되기 때문임(독일어 등). 둘째로 내 생각인데, 어떤 영국식/미국식 영어 억양은 파닉스와 더 잘 맞을 수도 있음. 나는 미국 남동부에서 자랐는데, 단어나 어미를 빼먹고 발음하는 경우가 많아 'ten'과 'tin', 'pen'과 'pin'이 구분 없어짐. 셋째로 나처럼 말하기에 어려움이 있었다면 파닉스가 훨씬 어렵게 느껴짐. 올바른 소리 자체를 낼 수 없으니 소리를 배워 읽는 게 힘들었음. 대체 방법이 더
            나쁘다는 데 의문은 없고, 또 이 논쟁은 거의 영어에서만 주로 나오는 것 같음. 중국어의 경우 파닉스 교육이 아예 없지만 모두 잘 읽음. 영어 자체가 네이티브에게도 정확히 읽고 발음하기 어려운 언어인 것 같음
          + 연구 결과가 기사에서 말하는 것만큼 명확하지 않은 것 같음. 파닉스가 60년대에 등장해서 80년대쯤 널리 퍼졌고, 주요 연구도 1975년에 이루어진 점에 주목할 필요가 있음. 이로 인해 전체언어(whole language)로 교육받았던 아이들은 실제로는 적고 대체로 어렸던 신생 그룹임. 당시 교육자들도 실전 경험이 적었기에 방법론, 자료의 질도 낮았을 것이라고 추측됨. 학계에도 이슈를 선점하려는 편향이 늘 있어, 다른 연구들도 함께 참고해야 신뢰할 수 있다고 봄. 무엇보다, 아이마다 학습 스타일이 다름. 내 아이도 파닉스는 오래 배웠지만 소리를 글자로 연결하는 데 어려움을 겪었고, 오히려 whole language 방식이 더 잘 맞았음
          + 나는, 내 형제들, 내 자녀 모두 단어 전체를 배우는 방식으로 읽기를 배웠고, 다 좋은 독서가임. 그러니 내 가족이나 당신 가족 경험 모두 통계적으로 유의미하진 않음. 우리 애들한테 단어를 먼저 가르치는 건 꽤 쉬웠음. 그래도 모두 집에서, 어릴 때, 읽기가 즐거운 활동이란 전제 하에 배웠기에, 학교에서 수업받는 것과는 매우 다름
          + 언어마다 다르겠지만, 영어의 파닉스는 꽤 복잡함. 내 경우에는 다른 언어에서 읽기를 배웠는데, 그 언어는 문자와 소리의 연결이 매우 명확해서 몇 주 만에 거의 모든 걸 읽을 수 있게 됨. 소리로 들어본 단어를 글로 보면 누가 안 알려줘도 쉽게 연결할 수 있었음
          + 파닉스로 읽기 배운 건 정말 좋은 경험이었음. 근데 지금은 파닉스가 없으면 못 살아서, 파닉스 한 줄만 얻으려고 어둡고 후진 트럭 정류장에서 내가 무슨 짓을 했는지 알면 울 거임…
     * 어린 자녀를 둔 부모에게 추천하고 싶은 책이 Teach Your Child to Read in 100 Easy Lessons임. 우리 아이가 유치원 때 이 책으로 공부했는데, 일찍 시작한 다른 부모님들이 왜 극찬하는지 완전히 이해하게 됨. 이 방식이 맥락 파악이나 단어 전체 인식에 방해가 되지도 않았음. 영어 읽기는 정말 복잡한데, 아이들은 생각보다 영리해서 ‘느리지만 확실한’ 방식을 가르치면 자연스럽게 빠른 방법도 익히게 됨. 하루에 짧은 시간이라도 꾸준히 하는 루틴과 강요, 보상이 효과적이었고, 무엇보다 아이가 이 과정에 동의해야 함이 중요했음. 각 레슨 뒤에 금방 읽을 수 있는 아주 짧은 이야기와 점점 길어지는 이야기가 나오는데, 아이가 파닉스 익히고 나서는 할로윈 때 캔디 포장지도 읽을 정도였음. 이후에는 아이가 정말 좋아하는 이야기책을 발견하는 것이 가장 중요한
       다음 과제였음. 수업 후에도 함께 계속 읽어주는 게 도움이 됨. 영어 철자와 발음은 너무 다양해서 이 책도 변형 알파벳(ee, sh/ch/th 등 별도 기호)으로 아이가 읽기 쉽도록 만들었음. 그래도 “is”나 “was” 같은 단어는 예외적으로 따로 가르쳐야 함. 이 책 저자가 산수도 이런 방식으로 극소단계로 쪼개 한꺼번에 익히는 학습법을 고민했었다고 하는데, 아직 그 수준의 산수 책은 못 찾았음. 만약 있었다면 꼭 사용해봤을 것임
          + 책은 아니지만, Math Academy 같은 온라인 플랫폼이 있을 수 있음. 수학을 작은 단계·기술로 쪼개서 주기적으로 반복하고 점점 고난도의 스킬로 통합함. 성취도에 맞춰 최적의 반복 학습을 제공한다고 함. 자세한 교수법은 여기 참고. 나도 이런 접근법에 공감함. physicsgraph.com도 그 영향을 받은 물리학 버전임
          + 나의 케이스 한 가지만 더 얘기해봄. 어릴 때 엄마가 나에게 읽기를 가르쳐준 적이 있는데, 사실 엄마는 그걸 의도적으로 가르치려 한 게 아님. 엄마가 나에게 많은 책을 크게 읽어줬고(특히 DC 만화책들), 항상 책이 집에 넘쳐났음. 그냥 자연스럽게 읽기 학습이 된 것임. 유치원 시작할 때 이미 읽을 줄 알았음. 특별히 노력한 건 아닌데, 역시 어머님의 열정과 집안 환경 영향이 컸던 것 같음. ‘Pizza Hut BOOK IT’ 같은 보상 프로그램도 있었고. 읽기는 지금도 내 삶의 큰 부분임
          + 이 책이 속한 방식(direct instruction)은 최근 연구에서 가장 효과 큰 방법들에 비해 미미한 편임. 읽기 어려움이 없는 아이에겐 괜찮지만, 읽기 장애가 있는 아이에게 필요한 소리 인식 훈련이나 이해 과정을 충분히 제공하지 못함. 값이 비싸지 않아 시도는 괜찮으나, 만약 아이가 유창하고 자연스러운 읽기로 발전하지 않는다면 반드시 소리 인식 결함을 다룰 수 있는 방법을 찾아야 함
          + 몇 년 전 해커뉴스에서 누군가 책을 추천해서 4살 아들에게 가르쳐봄. 그 뒤로 다른 과목의 학습 자료를 챗봇에 주문할 때도 이걸 기준으로 삼고 있음. 이 기사도 들어보고나니, 아주 기본적인 읽기조차 학교가 잘못 가르치는 것 같다라는 생각이 들었음. 그래서 아이의 학습방식은 더 신경쓰기로 했음. 수학에선 Beast Academy 교재를 쓰는데 다양한 방식으로 문제 풀이를 시도하는 접근이 마음에 듦. 동생에게는 역시 Teach Your Child...로 시작했음. 수학교재는 또 다른 것으로 새롭게 시험해볼 계획임. 특히 이런 1:1 집중수업 방식이 어떤 기술의 습득 속도를 훨씬 빠르게 할 수 있다는 생각이 듦. 학교는 효율성 때문에 결코 이런 식으로 가르치지 않음. 홈스쿨이 더 앞서간다면 대학 진학 때 그 차이가 크게 느껴져야 하는데 아직 그런 인상은 없음
          + 수학에서는 Saxon Math 옛날 버전을 써보는 게 좋음. 신버전은 New Math 느낌으로 변질됨
     * 나는 ""읽기 교육""을 받은 적이 없음. 어린 시절, 베트남에서 돌아온 이종사촌이 준 오래된 트렁크 가득한 수백 권의 만화책을 받은 기억이 남. 1969년이었고, 그 만화에는 DC/Marvel, Donald Duck, 유럽 만화, 60년대 오버사이즈 만화, 비속어 가득한 언더그라운드 만화까지 다양했음. 학교가 시작될 때 나는 이미 읽을 줄 알았고, 어린 소설책도 읽고 있었음(예: “Mrs Frisby & the Rats of NIMN”)
          + 나는 읽기를 어떻게 배웠는지, 혹은 읽지 못했던 때가 언제였는지 기억이 없음. 어쩌면 에피소드 기억력이 발달하기도 전에 읽기를 시작했을지도 모름. 유아 과학책을 읽으면서 ‘우주가 태양이 폭발해서 시작됐다’는 식으로 오해한 완벽하지 않은 기억도 있음. 누군가가 그런 지식을 일러줘서 ‘아하’하며 채워넣은 적은 없는 것 같음. 우리는 스스로 모르는 사이에 많은 걸 배우는 것 같음
          + 정말 스스로 글자 모양에서 소리를 추론해서 연습도 없이 읽기를 익혔다고 주장하는 건지 궁금함
     * 이번 논란이 궁금해서 기사를 읽었는데 더욱 헷갈렸음. 나는 파닉스로 읽기를 배웠고, 단어의 의미를 모르면 문맥이나 역할을 보고 유추하다가 모르면 넘어가는 게 익숙했음. 그런데 이 기사를 읽으니 비슷한 세대의 학생들에게 파닉스를 건너뛰고 단어 형태(gestalt)만 보고 읽기를 시키는 방법이 있다는 걸 처음 알게 됨. 그러니 독특한 타이포그래피나 레이아웃을 싫어하는 사람도 있을 법함
          + whole word learning(단어 전체 인식법)이라는 접근임. 나와 내 아이들도 이 방식으로 읽기 배웠음. 전체적인 형태를 보고, 나중에 소리 맞춰 단어나 조합을 익힘. 영어에선 신뢰성이 떨어질 수 있음. 새로운 단어 발음을 책에서만 접하는 경우에 정확히 아는지 물어봐야 함. 하지만 이 방법의 장점은 읽기를 훨씬 빠르고 재밌게 익힐 수 있다는 것임. 단, 1대1 지도가 아주 중요함. 집에서 부모와 배우는 아이에게는 좋으나, 교실에서는 잘 안 맞을 수 있음
          + ‘파닉스 건너뛰고 단어 형태로 바로 읽기’라는 요약이 마음에 듦. 이 방식은 단어의 대가(노력)를 들이지 않고 속여서(그리프트) 의미를 얻으려는 것과 같음. 이런 마인드를 일찍부터 경험한 사람들은 AI 텍스트가 학교에 범람하는 것도, 그에 반대하는 사람들과의 큰 차이도 이해할 수 있을 것 같음. ‘편하게 익힌 사람’은 읽기를 힘들게 배운 사람들만이 구분할 수 있는 미묘한 차이를 모르고 넘어갈 수 있음
     * 이 이슈와 관련 있는 이야기임. 나는 20대 중반에 이미 코딩 경험이 좀 있었는데, 다시 CS 학부에 들어가서 초급 프로그래밍 수업을 TA하면서, 학생들이 실제로 소스코드가 어떻게 동작하는지 기본 구조를 못 잡는 경우가 많다는 걸 느꼈음. 문제는 그저 예시 코드 보여주고 파이썬 함수 쓰게 하는 식으로, 실제 로우레벨에서 코드가 어떻게 parsing되는지, 구조와 동작 원리를 충분히 가르치지 않았다는 점임
     * 나는 Orton-Gillingham(OG) 방식에 정통한 튜터의 남편임. OG가 아닌 일반 교사와 그 산업은 아이를 가르치기보다는 돈을 버는 데 더 신경 써있다고 생각함. OT, 언어치료 등 서비스도 마찬가지로 ‘아이에게 도움’보다는 ‘수익화’에 더 집중하는 구조임
          + 나는 언어치료사(SLP)인데 현실은 흑백이 아님. 물론 모든 사회 서비스에 돈 문제가 얽혀 있지만, 지금 일하는 SLP 대부분은 클라이언트의 실질적 성장을 진심으로 목표로 함. 대부분의 클리닉은 바쁘기 때문에 일부러 치료를 늦출 유인은 없음. 아이가 지체되어 시작하면, 빠른 진척을 내도 영원히 살짝 뒤처지는 일이 생길 수 있으나, 이는 단순한 인지 발달상의 문제임. 결국 가장 중요한 건, 좋은 치료사를 고르는 과정과 사용법에 있음. 분야마다 방법의 숙련도와 선택 재량 차이가 있기 때문임
     * ""<i>That’s how good readers instantly know the difference between 'house' and 'horse,' for example.</i>""라는 문장은 바로 그래픽, 문법, 의미적 힌트만으로는 'house'와 'horse' 구분이 어려워 MSV 시스템이 잘 먹히지 않는 대표적인 예시라고 생각함
          + 이 방식은 추상적인 내용을 다루는 데에는 효과적이지 않은 것 같음
"
"https://news.hada.io/topic?id=22252","서태평양 M8.7 지진 발생, 쓰나미 경보 발령","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       서태평양 M8.7 지진 발생, 쓰나미 경보 발령

     * 2025년 7월 29일, 러시아 캄차카 반도 인근에서 규모 8.8의 강진이 발생함
     * 이 지진은 판 경계면 역단층에 의해 발생한 것으로, 태평양 판과 북아메리카 판의 활발한 충돌 영향임
     * 이번 지진은 2011년 도호쿠(일본) 대지진 이후 세계에서 가장 큰 규모에 해당함
     * 최근 10일 동안 여러 차례의 전진 및 여진이 이어졌으며, 단일 단층면 위에 대규모 미끄러짐이 나타남
     * 캄차카 해역은 빠른 판 운동과 잦은 지진 발생 이력으로 세계에서 가장 지진활동이 활발한 지역 중 하나임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

판 구조 요약

     * 2025년 7월 29일, 러시아 페트로파블로프스크-캄차츠키 동쪽 해역에서 규모 8.8의 지진이 발생함
     * 이 지진은 얕은 깊이의 역단층 운동에 의해 일어난 것으로, 해당 위치에서는 태평양 판이 북아메리카 판에 비해 서북서 방향으로 연간 약 80mm 움직임
     * 북아메리카 판은 대륙 서쪽 해역까지 확장된 형태임
     * 이번 지진의 위치와 움직임은 Kuril-Kamchatka 호상열도의 판 경계면에서 일어난 단층 운동과 일치함

지진 특징 및 규모

     * 지도상에는 점 형태로 표시되지만, 대형 지진은 넓은 단층면에서의 미끄러짐(슬립) 현상에 의해 발생함
     * 2025년 7월 29일 발생한 규모 8.8의 역단층성 지진은 단층의 길이 약 390km, 폭 140km 범위에 걸쳐 발생함

피해 및 통계

     * 이번 캄차카 지진은 2011년 도호쿠 Mw 9.1 대지진 이후 전 세계에서 가장 큰 지진 중 하나이며, 1900년 이후 전 세계 10대 규모 중 하나임
     * 7월 19일부터 10일간 이 지역에서 연속적으로 강진이 발생했고, Mw 8.8 본진 이전에는 Mw 5.0 이상의 지진이 50회 이상 감지됨
          + 이 중 2025년 7월 20일에는 규모 7.4, 6.6의 지진이 세 차례 중첩 발생함
          + 7월 30일 기준, Mw 5.0 이상의 여진 24회 발생, Mw 6.9, 6.3 등 강한 여진도 포함됨

지진지역 지질 환경

     * Kuril-Kamchatka 섭입대(판이 서로 충돌하여 한 판이 다른 판 아래로 들어가는 지역) 에서 반복적으로 대형 지진이 일어남
     * 이곳에서 태평양 판은 북아메리카 판 아래로 연간 약 80mm의 빠른 속도로 섭입하며, 이 지역은 전 세계에서 가장 빠른 수렴경계 중 하나임
     * 캄차카 해역에서는 1990년 이후 Mw 5.0 이상의 지진이 약 700회 발생(2025년 7월 29일 본진 및 여진 제외)

주요 과거 대지진 사례 및 이번 지진 의미

     * 이 구간에서 지난 100여 년간 대규모 지진이 여러 번 있었으며, 1923년엔 Mw 8.4의 지진이 이번 지진에서 북쪽으로 약 150km 지점에서 발생함
     * 1952년에는 이 지역에서 Mw 9.0의 대지진이 발생, 진앙은 2025년 지진과 30km 이내 거리에 위치함
     * 1952년 지진은 섭입대 경계 약 600km 구간에서 단층파열이 발생함(Johnson and Satake, 1991)
     * 이 지역은 연간 판 운동 속도 80mm 기준으로, 1952년 이후 약 6미터의 지각운동이 누적됨
     * 2025년 지진은 1923년과 1952년 지진파열 구간 사이의 빈틈을 채웠으며, 앞으로 자세한 분석을 통해 본진 파열 과정과 대형 지진 간의 공간적 관계가 추가 연구될 예정임

        Hacker News 의견

     * 일본 동부 해안 전역에 최대 3미터 규모의 쓰나미가 예상됨을 알림, 첫 파도는 10분 내에 도달한다는 전망임
       일본 기상청 공식 쓰나미 정보 페이지와 NHK 뉴스 라이브 (일본어), NHK World 라이브 (영어) 링크 공유함
       일본 동부 해안에는 인구의 대다수가 살고 있으며, 2011년 동일본 대지진 때도 큰 피해를 입은 지역임
       이번에 과연 2011년 이후 어떤 교훈을 살렸는지 확인하게 될 것 같다는 의견임
       업데이트로는, 홋카이도 네무로에서 첫 파도가 30cm였고 더 클 수도 있으나, 현재까지 일본에 실제로 도달한 최대 파도는 40cm에 머물러 큰 피해는 없을 것으로 보인다고 전함
     * 현재 여러 라이브 스트림을 공유함
       헬리콥터에서 촬영한 일본 KATU 뉴스 생중계, 웹캠 영상, 하와이 뉴스를 참고할 수 있다고 안내함
     * 일본의 재난 상황 보도는 정말 볼거리임
       화면 전체가 데이터로 가득 차 있고, 블룸버그 터미널처럼 깜빡거린다는 인상임
     * NTV가 3m 파도가 일본을 강타했다고 오해해 잘못 전했다는 사실을 정정함
       해당 오해가 생긴 이유는 유튜브 영상에서 파도가 도착했다는 장면을 실제 관측 높이로 착각했기 때문임
       현재로선 아직 예측일 뿐이라는 점을 바로잡음
       참고로 사용한 유튜브 영상 링크를 게재함
     * 2011년 쓰나미 규모가 어느 정도였는지 궁금함
       3m 파도가 그보다 더 큰 것인지 작은 것인지 질문함
     * 이번에 경보가 광범위하게 발령된 것은 불확실성 때문이고, 세기말급 상황이 아니라고 생각함
       이런 넓은 경보와 충분한 시간이 모든 사람이 신속히 위험에서 벗어나도록 도왔으면 하는 바람임
     * 아내가 만화에서 2025년 7월 대재앙을 경고받았다며 일본 여행을 가지 않기로 결정함
       나는 한 달 내내 농담거리로 삼았지만, 마침 이번 쓰나미 경보가 터짐
       2025년 7월 일본 메가지진 예언에 대해 언급하며, 2021년 재출간된 만화가 ""2025년 7월 실제 재앙""을 경고하여, 실제로 항공사들이 여름 성수기 항공편을 취소하고 동아시아에서 일본 여행이 감소하는 해프닝까지 발생했다는 배경도 공유함
     * (유머러스하게) 만화가 실제로 재난을 예측하지 못한 모든 이들의 배우자 이야기는 들을 수 없을 것이라는 점을 아쉬워함
     * 다행히 이번 쓰나미는 일본에는 거의 영향이 없었으니, 계속 농담을 해도 될 듯함
     * 만화 내용이 나중에 ""2025년 7월 5일 소행성 충돌""이나 심지어 세계 종말이란 내용으로 수정되었다고 전함
     * 해당 만화가 예측했던 것은 일본의 메가지진이지, 러시아에서 온 쓰나미가 아니라고 정확히 집어 지적함
     * 이번 지진은 정말 큰 규모임을 강조, 역사상 상위 8위 안에 들만큼 크다고 언급함
       지진 목록 위키피디아 링크 첨부함
     * 규모가 8로 정정됐다는 정보와, 다시 8.8까지 올라갔다는 소식을 전함
     * 1960년 발디비아 지진은 약 1.5e23줄의 에너지를 방출했는데, 이 정도는 허리케인 1000개 혹은 과거 100년간 모든 지진 에너지의 25%에 해당된다고 설명함
     * 온라인 영상들을 보면, 이번 지진은 엄청난 횡적 움직임보다는 강한 P파의 진동과 흔들림이 많았고, 일본 대지진처럼 S파의 큰 이동은 아닌 것 같다고 느낌
       이것이 쓰나미에 어떤 의미인지는 모르겠으나, 현재까지는 8.8이란 규모에 비해 강도가 약한 편이라고 평가함
     * 동일 지역이 1952년에도 9.0 규모의 지진을 겪은 전력이 있음을 환기함
     * 이번 지진 규모가 목록상 두 번째로 큰 편임을 언급함
     * 가장 가까운 DART 부표에서 1.3m 정도의 수주 높이 변화가 관측됨 (위치: 48°7'34"" N 163°22'35"" E, 수심 5787m)
       관측 데이터 링크 제공
     * 이 정도 수주 높이 변화가 많은 편인지 궁금해함
     * tsunami.gov 링크를 통해 쓰나미 정보를 빠르게 확인할 수 있다고 안내함
       미국 서부 해안에는 ""watch""(감시) 단계이지만, 하와이와 알래스카에는 경보 단계임
     * 하와이에서는 이미 공습 경보가 울리고 있음
       아직 몇 시간 남았으나, 모든 섬을 감쌀 수 있고 어디든 영향을 줄 수 있다는 경각심을 전함
     * ""감시"" 단계에서 하와이와 알래스카 일대, 캘리포니아 일부 지역(캡 멘도시노에서 오리건 국경까지)은 ""경보""로, 그 외 캘리포니아 남부와 알래스카 해안 등은 ""주의보""로 상향 조정되었음을 업데이트함
     * 캘리포니아 해안도 ""주의보"" 상태로 상향됨을 알림
     * 페트로파블로프스크 지역의 피해 상황을 궁금해함
       USGS에서는 ""심각"" 수준의 흔들림, 중~심각한 피해 및 인명 피해 가능성을 추정함
       최근 몇 주간 여러 차례 규모 7 이상의 지진이 지속적으로 발생했음을 부연함
     * 러시아 뉴스에 따르면, 다행히 상대적으로 경미한 피해만 있음을 알게 됨
       페트로파블로프스크는 낡은 도시이지만, 내진 설계에 대해서는 진지하게 접근하는 곳이라 규모 7 지진에도 원칙적으로 피해가 0이어야 한다는 점을 덧붙임
       지형적으로는 입구가 좁은 만으로 쓰나미에 거의 영향이 없고, 도시 자체도 해수면보다 최소 10m 이상 높음
     * 세베로쿠릴스크는 1956년 비슷한 쓰나미로 도시가 파괴된 적이 있는데, 이번에 항만이 다시 파손됨
       남은 시가지들은 높은 지대로 옮겨 재건해 항만만 취약한 상황임
       이 지역(옛 일본령으로 아이누가 이전에 거주함)의 역사 배경도 첨언함
     * 지금까지 현지 뉴스는 한 학교(공사 중이라 비어있었음)에서 건물 피해, 일부 지역의 침수 정도만 보여주고 있다고 전함
       희망적으로 바라봄
     * 해당 지역은 매우 외진 데다 인구 밀도가 현저히 낮음
       캄차카 지역 전체 인구가 30만 미만, 인구 1인당 1km² 꼴임
     * 현재 공식 뉴스 기준으로는, 약 3천 명이 대피했고, 여진이 한 달간 지속될 것으로 예상됨
       일부 건물(병원 포함)에 균열, 송전선 약간의 피해, 일부 업체 인근 해안 침수 수준임
       총평하면 대체로 무사함
     * 재난 경보 쓰레드에서 쓰나미가 ""파도""냐 아니냐를 놓고 설전을 벌이는 상황을 지적하며, 만약 HN이 작은 마을이었다면 토론에 빠져 모두 익사했을 거란 농담을 함
     * 빛이 입자인지 파동인지 수십 년간 논쟁했던 사례를 들며 유머러스하게 마무리함
     * 미드웨이 환초와 괌에서도 하와이 주지사 Josh Green이 3피트(1미터) 높이의 파도 관측됨을 공식 발표함(하와이 현지시간 오후 6시24분 기준)
     * 러시아 그 지역에서 최근 몇 주간 대형 지진 활동이 많이 발생중이라는 점을 상기함
       계속해서 지진 알람이 울림
     * 어떤 알람 서비스를 사용하는지 궁금해함
     * 나는 현재 코스타리카에서 휴가 중인데, 호텔에서 해변을 폐쇄했다고만 안내하고 이유는 모르겠다고 함(하지만 쓰나미 때문이라는 걸 직접 확인함)
       쓰나미 경보상 1~3미터의 파도가 바로 지금 도달해야 하는 시점이지만, 타마린도 등 여러 비치 라이브캠에서 아무 변화도 포착되지 않음
       난 남부의 푸에르토 히메네스(작은 반도 안쪽)에 있어 영향이 거의 없을 것으로 예상함
"
"https://news.hada.io/topic?id=22270","Ollama, 맥과 윈도우용 새로운 앱 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Ollama, 맥과 윈도우용 새로운 앱 출시

     * Ollama가 새로운 데스크톱 앱을 macOS와 Windows용으로 공개함
     * 이제 모델 다운로드 및 채팅이 GUI 환경에서 간편하게 가능해짐
     * 파일 드래그 앤 드롭 기능으로 텍스트 및 PDF 문서와 상호작용이 쉬워짐
     * 대용량 문서 처리 시, context length(문맥 길이) 설정을 늘려 더 긴 자료 분석 가능
     * Ollama의 새로운 멀티모달 엔진을 기반으로, 이미지를 지원하는 모델(예: Google DeepMind의 Gemma 3 model)에 이미지도 전송 가능
     * 코드 파일 등 다양한 파일 포맷에 대응하는 문서 처리 기능도 포함됨
     * Ollama의 새로운 앱은 공식 사이트를 통해 macOS 및 Windows 용 다운로드 가능
     * CLI 버전만 필요한 사용자는 GitHub Releases에서 별도 다운로드 가능

        Hacker News 의견

     * 최근 Ollama가 홈페이지를 중심으로 개발자 대상에서 일반 사용자 대상으로 변화를 주는 모습인 것 같음 Ollama 홈페이지 참고<br>예전엔 개발자용 CLI 버전 중심이었는데, 이제는 언급조차 잘 안 보임<br>블로그 하단에 보면 ""CLI 버전은 Ollama GitHub 릴리즈 페이지에서 다운로드 가능""이라고만 짤막하게 안내함<br>별로 비난하려는 건 아니고, 변화가 눈에 띄어서 관찰한 것임<br>과거 여러 로컬 LLM 앱을 써봤는데, 내가 좋다고 느낀 두 가지는 LM Studio 와 Msty였음<br>이 앱도 꼭 써볼 생각임<br>아쉬운 기능 한 가지는, ChatGPT 데스크톱 앱의 '항상 단축키(Alt+Space)'로 바로 새 채팅을 열고 축소된 UI로 빠르게 질문할 수 있는 것임<br>로컬 LLM 앱들도 이런 아이디어를 채택하면 좋겠음
          + Ollama 관리자임<br>개발자를 등한시했다는 생각은 전혀 안 함<br>우리 모두 개발자이고, 저희 자신이 직접 쓰는 용도로 Ollama를 만듦<br>사람마다 다양한 프로토타입을 만들어서 쓰다가, 다들 Ollama에서 만족해서 집중해서 개선하고 있는 중임<br>Ollama는 처음부터 끝까지 개발자 타깃임, 이 부분에 계속 집중하고 있음
          + 혹시 pygpt.net을 써봤는지 궁금함<br>인터페이스가 과하다거나 이름이 좀 아쉽다는 점은 있지만, 내가 써본 것 중 제일 만족스러웠던 앱임
          + 이런 변화는 오히려 개발자에게도 긍정적임<br>더 많은 사용자가 Ollama를 설치하게 되면, 나중에 별도의 모델을 번들링할 필요 없이 Ollama기반 데스크톱 AI 앱도 배포할 수 있음<br>추가로 비용이 안 드니 사용자에게 무료/저렴한 구독도 제공 가능함<br>최신 Qwen30B 모델들도 굉장히 강력함<br>여기에 설치 템플릿이 있어서 유저 PC의 스펙(램, CPU/GPU 성능 등)을 체크하고 Ollama 미설치 시 하위 설치/다운로드하도록 한다면 훨씬 편리할 것 같음<br>모델 설치 안 돼 있을 때 권한 요청하고 설치할 수 있게 하는 API도 있으면 좋겠음
          + HugstonOne에는 새로운 탭 등 해당 기능이 이미 있음<br>추가적으로, 대다수 개발자들이 무분별하게 스팸성 노출을 해서 의아함<br>Ollama는 AI 분야의 선구자이고 훌륭한 앱임, 진심으로 고마움을 전함
          + Msty는 들어봤고 짧게 써봤지만, 최근 다시 보니 기능이 풍부해 보임<br>LM Studio는 몰랐는데, 이쪽은 무료 상업적 사용이 가능함(Msty는 해당 없음)<br>채팅 인터페이스 기반 툴로 쓸 거라면 두 앱의 장단점 비교 궁금함
     * 이번 Ollama의 변화가 잘 이해되지 않음<br>프론트엔드 데스크톱 앱은 내가 Ollama를 써온 이유와 정반대임<br>원래 로컬 LLM 백엔드 용도로 썼고, 오랜 유저들은 이미 자신만의 프론트엔드를 찾거나 만들거나 커스텀해서 익숙함<br>사실 최근 최첨단 로컬 모델이 늦게 올라오는 일도 있었는데, 이 프론트엔드 포커스 때문이 아닌지 의심됨<br>이제 Ollama 대신, 처음부터 자체 UI를 포함하거나 CLI 백엔드만 고수하는 대체 옵션들을 찾아볼 계획임<br>혹시 더 나은 게 있으면 아마 갈아탈지도 모름, 진작 이렇게 했어야 했음<br>Ollama가 CLI 포커스를 포기하고, 구독 기반의 범용 LLM 인터페이스로 바뀌는 첫 단추이길 바라지 않음
          + Ollama에는 다양한 GUI가 이미 존재함<br>지금 앱은 Ollama에 GUI를 기본적으로 탑재한 버전으로 보임
     * Ollama UI를 써보니 원격 Ollama 인스턴스에 연결하는 옵션이 없다는 점이 의외임<br>가장 강력한 PC가 항상 내가 GUI를 돌리는 컴퓨터는 아님
          + 이 부분 정말 크게 공감함<br>나는 윈도우/맥 데스크탑환경은 좋아하지만 OS 자체는 별로고, 리눅스/BSD는 OS는 좋지만 데스크탑환경이 별로라, 항상 강한 워크스테이션은 헤드리스 리눅스에 두고 윈도우/맥에서 SSH로 접속함<br>개발자들은 이게 뭔지 대부분 잘 모름<br>터미널에서 명령을 실행할 때마다 브라우저 탭이 뜨거나, URL을 터미널에 출력해 주지 않으면 너무 힘들고 좌절스러움
          + SSH 포트 포워딩(ssh -L 11434:localhost:11434 user@remote) 사용해서 원격 Ollama 인스턴스 연결로 우회가 가능함<br>그래도 네이티브 지원이 있다면 훨씬 나을 듯함
          + Ollama를 네트워크에 노출시키는 기능이 앱에 있긴 하니, 곧 지원될지도 모름
     * 윈도우에서 Ollama UI를 써봤음(그 전엔 CLI 사용)<br>- 심플함이 마음에 들었음. 비전공자, 친구, 가족에게 로컬 LLM을 쉽게 셋업해주기에 완벽함<br>- 멀티모달·마크다운 지원이 잘 작동함<br>- 모델 선택 드롭다운엔 내 로컬 모델과, 레지스트리에서 인기 모델이 전부 나타남<br>간단한 용도라면 Open WebUI보다 이 쪽이 더 편할 듯함<br>프롬프트/고급 설정 조정 같은 것은 곧 도입될 것 같고, 지금은 심플함 자체가 장점임
     * 텍스트만 처리하는 것으론 충분하지 않음<br>영어뿐만 아니라 30초 이상 음성을 텍스트로 변환하거나 음성 생성, 이미지 생성 기능도 매우 중요함<br>텍스트 처리는 이제 기본임
     * 왜 리눅스는 지원 안 되는지 궁금함<br>UI가 크롬 기반(아마 electron)인 듯한데, 그렇다면 리눅스 포팅도 쉬울 듯함<br>소스 링크도 궁금함
          + Electron이 크로스플랫폼을 표방하지만, '버튼 하나로 리눅스 배포'가 현실은 아님<br>glibc 버전 문제나 GPU 지원 같은 크고작은 트러블이 늘 있음
          + Ollama의 공식 문구는 “Mac, Windows용 신버전 앱 다운로드 / CLI 버전은 GitHub에서 별도 다운로드”임<br>확인해보니 오픈소스 느낌이 아님<br>게다가 크롬이 아니라 시스템 웹뷰를 사용해서 tauri앱으로 보임
          + Electron 기반이라는데, 이런 걸 네이티브라고 홍보하는 게 의아함
          + 리눅스 개발자나 파워유저는 이미 CLI만으로 Ollama 쓸 수 있음<br>지금 맥·윈도우 앱은 일반 사용자 겨냥임
     * Ollama는 기업들이 오픈 모델 배포용 표준 인터페이스가 되고자 하는 것으로 보임<br>‘로컬’ 포커스는 부수적이고 장기 목표는 아닐 듯함<br>곧 이 앱을 브릿지로 ‘자체 클라우드 API+오픈모델’ 전략을 발표할 것이라 확신함
          + “로컬”이 부수적이라는 의견에 강하게 반대함<br>클라우드 기반 서비스를 쓸 수 없는 기업 (예: 방산업체) 입장에선 로컬이 기본값임<br>오픈모델(아직 성능이 부족해도)을 쓰는 진짜 이유임
          + 우리도 Ollama를 쓰는 이유가 데이터가 회사 내부망을 절대 벗어나선 안 되기 때문임<br>조금이라도 클라우드가 섞이면 법무팀에서 난리남<br>그래서 Ollama를 랩탑(M-시리즈가 매우 강력), 내부 인트라넷 서버에서도 직접 돌림<br>LM Studio가 최근 상업적 사용에 콜미 프라이싱 없이 바뀌어서 이것도 검토할 생각임
     * 파워유저이거나 코딩 능력이 있다면, 직접 원하는 대로 커스텀 채팅 UI를 만들어 쓰는 걸 추천함<br>어떤 OpenAI 호환 엔드포인트와 채팅 프론트엔드 컴포넌트 프레임워크를 가져다 쓰면 나머지는 거의 쉬움<br>나도 Gemini 도움 받아 1주일 만에 뚝딱 만들었고 매일 씀<br>생산 레디는 아니지만 원하는 대로 바꿀 수 있고, 기능 추가도 직접 빠르게 가능함
          + 진짜 “직접 리눅스 만드세요”에 가까운 최고의 댓글임<br>누구에게나 쉬운 접근은 아님
          + 나도 AI·LLM 탐구하면서 Python 기반 직접 챗봇 만들었고, 그 뒤 [Vercel AI SDK+OpenAI 호환 API 엔드포인트]까지 활용함<br>마지막엔 제품화까지 했음<br>VT.ai - Python, VT Chat
          + 나도 같은 접근을 함<br>진짜 중요한 건 어떤 모델을 쓰느냐지, 나머진 거의 사소함<br>내가 쓰는 오픈소스 스크립트(Show HN 링크)도 한 번 참고<br>ChatGPT 등의 도움으로 만들기 쉬웠고 매일 씀<br>곧 DDG, 깃허브 검색도 붙일 예정임
          + 혹은 Open WebUI를 직접 베이스로 써도 되고, 입맛에 맞게 커스텀 마음껏 가능함
     * 나만의 완벽한 LLM 채팅 인터페이스 찾기 도전 중임<br>필수조건이 로컬/원격/클라우드(OpenAI API 호환) 모델 동시 지원이고, 모델 전환이나 멀티 쿼리가 쉬워야 함<br>현재의 소감:<br>* Msty: 내가 가장 좋아함, 여러 모델로 동시 쿼리 가능+감각적 디자인, 단점은 오픈소스 아님+리눅스에서 가끔 프리징<br>* Jan.ai: 멀티 모델 동시 쿼리 불가<br>* LM Studio: 오픈소스 아님+원격/클라우드 모델 미지원(플러그인 있는지 불확실)<br>* GPT4All: openrouter 모델 쓸 때 JSON 오류, 채팅마다 모델 전환을 수동으로 해야 해서 불편<br>아직 Librechat, Open WebUI, AnythingLLM, koboldcpp는 테스트 안 해봤음<br>다른 추천 사례 있으면 듣고 싶음
          + webUI가 마음에 들지만 모델마다 브라우저 내 텍스트 파일로 세팅하는 게 복잡하고, 설명서도 용어가 난해함<br>Librechat은 조금만 지나도 로그아웃돼서 사실상 사용 불가임<br>https로 로그인을 유지할 수 있단 정보도 들었는데, Tailscale을 쓰다보니 한 호스트에 여러 서비스 돌릴 때 힘듦
          + 내가 직접 만든 앱 dinoki.ai도 있음<br>로컬에서 돌아가고, 프라이버시 최우선+macOS는 Swift, 윈도우는 WPF로 네이티브 구현임
          + OpenWebUI가 손쉽게 다양한 모델을 채팅으로 쓸 수 있어 원하는 퀄리티임
          + 여러 달째 AnythingLLM을 쓰고 있는데 정말 만족함<br>여러 “워크스페이스”로 모델/프롬프트 세트 저장 가능, Ollama+기타 LLM 모두 지원<br>라즈베리파이에 도커로 올려두고, tailscale로 어디서든 접근 가능함<br>모바일 화면도 이쁘고, Raycast Claude 익스텐션까지 쓰면 궁금한 건 다 해결됨
          + 직접 만들어보는 걸 추천함<br>최신 트렌드 공부도 되고, 재미도 있음<br>내 인터페이스를 2023년에 만들어서 조금씩 기능 붙이고, 최근 MLX 통해서 로컬 모델도 연결함<br>생각보다 인터페이스 만들기는 쉽고, 개발자들이 직접 해보면 많은 걸 배움
     * Ollama를 예전에 한 번 써보고 바로 지움<br>Ollama에서 공식 지원하는 모델만 쉽게 설치 가능한 게 아니면 불편해서임<br>그 이후로 LM Studio가 최고라고 느낌
          + OLLAMA도 좋지만 Lmstudio와 더불어 HugstonOne이 경험적으로 더 뛰어남
"
"https://news.hada.io/topic?id=22231","Claude Code 주간 사용량 제한","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Claude Code 주간 사용량 제한

     * Anthropic의 Claude Code 서비스에 대해 주간 사용량 제한이 도입됨
     * 무료 및 유료 이용자 모두에게 적용됨
     * 사용자는 일주일 동안 최대 쿼리 수 또는 처리 토큰 양에 제약을 받음
     * 제한 도입은 서비스 오용 방지와 시스템 자원 안정성 확보 목적임
     * 개발자와 스타트업은 API 활용 시 리소스 관리에 추가 주의 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Claude Code 주간 사용량 제한 도입 개요

   Anthropic에서 제공하는 Claude Code 서비스에 새로운 주간 사용량 제한 정책 적용
     * 전체 사용자(무료 및 유료)에게 일정량의 쿼리 또는 토큰 사용 한도가 설정됨
     * 이 한도는 서비스 오용 방지, 공정한 서비스 제공, 인프라 자원 안정성 확보를 위해 도입됨
     * 매주 한도가 초기화되며, 제한 초과 시 해당 주에는 추가 사용 불가함

개발자 및 스타트업 대상 주요 영향

     * 제품 개발에 Claude Code 활용 시 사용량 계획 필요성 증가
     * API 연동 서비스는 자동화된 관리 또는 제한 초과 알림 로직 구현 필요성 발생
     * 대량 코드 생성, 분석, 반복 호출을 수행할 경우 리소스 활용 최적화 중요성 증대

결론

     * Claude Code의 주간 사용 제한 정책 도입은 지속 가능성과 서비스 품질 향상 목적임
     * 스타트업과 IT 전문가들은 기존 시스템 연동 및 서비스 설계 시 주간 제한 확인 및 사용 계획 수립 필요

        Hacker News 의견

     * 아마 주간 한도에 도달하지는 않을 것 같지만, 한도가 36시간 단위 같은 게 아니라 주간 단위라는 게 불안함
       만약 한도에 걸리면, 그 주 내내 사용할 수 없게 됨
       익숙해진 도구를 이렇게 오래 못 쓰는 건 불편함
       누군가는 내가 Claude에 너무 의존한다고 할 수도 있겠지만, ripgrep 등 다른 툴도 마찬가지임
       며칠 못 쓰는 건 괜찮은데 일주일은 너무 김
       그리고 “5% 미만의 사용자”만 영향을 받는다고 했다는 점도 눈에 띔
       보통 이런 공지들은 1% 미만만 영향이라고 말하는데, Anthropic은 20명 중 1명이 한도를 넘길 거라고 말함
          + ChatGPT Plus 플랜에서 o3 100회/주 한도의 기분이 딱 이 느낌임
            얼마나 썼는지 알 수도 없고, 중요한 리소스라 본능적으로 아끼게 됨
            결국 플랜을 제대로 활용 못 하고 o4-mini 같은 모델로 돌림
            차라리 일일 한도가 나음
            근데 한도 미달로 아껴 쓰게 하는 게 주간 한도의 의도인지도 모름
          + 개발자들이 독점적인 온라인 서비스에 의존하게 되는 현실이 슬픔
            예전엔 모든 걸 FOSS 도구로 할 수 있었고, 특정 회사나 서비스에 월 구독료 내며 종속될 필요가 없었음
            지금 일부는 Monsanto 농부처럼, 매달 돈 내고 툴을 써야만 일하는 방법을 잊어버리고 있음
          + 나는 sonnet으로 Pro 한도에 하루 3번씩 자주 닿음
            Claude code와 claude를 같이 쓰면 30분 만에 끝남
            24/7 멀티 에이전트나 여러 창을 띄우지도 않는데 이렇게 됨
            내가 상위 5% 사용자라고 생각하지도 않지만, 수요일에 한도가 끝나는 건 놀랍지도 않음
            이제야 Claude 챗을 더 활용해보려고 했는데, 며칠 동안 못 믿고 쓸 바엔 의미 없음
          + Anthropic이 20명 중 1명은 한도에 걸린다고 했는데, 계정 공유나 24/7 자동화 사용자가 그렇게 많을 것 같지 않음
          + 만약 한도에 걸리면 그 주 내내 못 쓰는 건 아니라 남은 시간만큼 못 씀
            본인도 아마 잘 안 닿을 거라 했으니, 만약 닿는다면 주의 마지막 36시간쯤일 가능성이 큼
            API 요금을 내고 쓰는 방법도 있음
     * 장기적으로 어떻게 될진 모르겠지만, LLM을 쓸 때마다 제한된 자원이라는 느낌을 받아야 하는 게 별로임
       사람들은 무제한 플랜에 익숙함
       지금 요금 모델은 억지로 쓰는 느낌이라 불편함
          + 무제한이 괜찮은 건 “측정할 수 없을 정도로 저렴한” 모든 서비스에 해당함
            인터넷, 문자 등은 직접적인 비용이 매우 싸서 그럴 수 있지만
            LLM은 아직 한 번 돌릴 때마다 꽤 큰 직접 비용이 듦
          + 월 전체에서 사용이 꾸준하길 기대하는 구조에 동의하지 않음
            나는 보통 한 달 내내 쓰다가 며칠 동안 11시간씩 몰아서 쓸 때도 있는데, 그럴 때 대부분 한도에 막힐 수 있음
            그래서 API를 직접 쓰면 내 지갑 깊이에 맞게 제한되니 더 낫게 느낌
            OpenRouter 같은 걸 이용하면 구독제의 한계도 피할 수 있음
            요즘은 Gemini 2.5 Pro가 Claude보다 코드 작업용으론 더 잘 맞음
            이 외에도 어떤 비용 경쟁력이 있는 옵션들이 있는지 궁금함
            https://docs.anthropic.com/en/api/rate-limits#rate-limits
          + 내 의견은 이런 툴들은 “월 20불” “월 200불” 등으로 제한된 양에 접근하게 하고 한도 계산이 어렵게 만드는 걸 아예 없애야 한다고 생각함
            완전히 사용량 기반으로 전환해야 진짜 사용자 친화적임
            체험용으로 처음 20회 무료 같은 프리 티어를 제공하거나, 일정 사용량만큼 계단식 요금제를 통해 점점 비율을 올리고, 극한 사용자는 실제 원가에 맞춰 요금을 받는 구조로 해야 함
            이러면 낮은 사용량 사용자는 싸게 쓸 수 있고, 시장 점유율도 챙길 수 있음
            OpenRouter보다 더 나은 가격을 내면 사람들은 타사 도구 대신 이 생태계에 남을 것임
            도구가 진짜 좋으면 사용량 기반이어도 사용자는 남게 됨
            문제는 제공자들이 시장 점유율을 위해 사용자를 보조하면서도, 악용 사례나 극단적 사용은 막으려 한다는 점임
            100% 해결책은 완전한 사용량 기반, 입장료 없는 구조임
            근데 이렇게 하면 구독만 하고 적게 쓰는 사람들은 손해 볼 수도 있어서 영업 팀이 반대할 듯
            또 이렇게 하면 사람들은 여기저기 비교하며 옮기기 쉬워져 한 달~두 달 묶이는 느낌도 사라짐
          + 장기적으론 로컬 LLM이 2025년 최고의 클라우드 LLM보다 더 좋아져서, 일상 작업의 99%는 무제한으로 처리하고
            진짜 복잡한 문제에만 클라우드에 접속하는 시대가 올 거라 봄
            LLM이 효율적으로 발전하고, GPU, 메모리, 저장장치 비용도 계속 싸지고 접근성이 높아질 것임
            지금은 아직 과도기라서 좀 불편해 보이는 것뿐임
          + 제한된 자원이어도 내가 얼마나 썼는지 알 수 있다면 괜찮겠음
            진행 상황을 볼 수 없는 게 불편함
     * Max 5x와 Max 20x 차이에 혼란스러움
       내 이메일엔 “대부분 Max 20x 사용자는 Sonnet 4를 주당 240~480시간, Opus 4는 24~40시간 사용할 수 있다”고 적혀 있음
       공식 공지엔 “대부분 Max 5x 사용자는 Sonnet 4 주당 140~280시간, Opus 4는 15~35시간”이라고 되어 있음
       차라리 가격에 비례해서 한도도 2배 이상이면 좋았겠는데, Opus 4는 차이가 5~9시간밖에 안 됨
       최소한 2배는 되어야 하지 않음? 두 배 가격 내는데
          + 이런 식이면 진짜 그렇다면 Max 20x에서 낮은 플랜으로 바로 내릴 거임
            호주에서 한 달에 350불 내고 있음
          + 나는 Opus 한도에 계속 걸려서 20x로 업그레이드했는데, 이제 보니 20x와 5x의 차이가 거의 없음
          + 그래서 MAX 사용을 멈추고 Pro로 다운그레이드해서 o3랑 다른 모델을 API로 사용함
            초기엔 그렇게 많은 시간이 필요 없어서, 프로젝트당 10불 정도면 o3, Gemini, Opus 다 쓸 수 있음
            며칠마다 새로운 모델이 나오는데, 한 공급자에만 묶이고 싶지 않음
          + 실제로는 사용량이 2배라서가 아니라, 트래픽 몰릴 때 우선순위만 높아지는 것
          + 만약 마케팅 자료가 사실과 다르다면, 누군가 실제 데이터로 조사해서 집단 소송 했으면 함
     * 한 달에 200불 내도 충분하지 않다는 건 알겠음
       그럼 한도 걱정 안 하고 쓸 수 있게 충분한 요금제도 만들어줘야 함
       “시간 끝!” 메시지처럼 흐름 깨트리는 건 없음
       차라리 크레딧 방식이면 얼마 썼는지 보고 추가 결제라도 할 수 있는데
       “GPU 식히는 동안 대기”라는 개념은 생산성에 도움 안 됨
       에이전트 여러 개 돌리면 ‘35시간’은 턱없이 부족함
       도구 자체가 이런 방식 지원하게 설계된 것도 이상함
          + 모두에게 충분해서 수익이 날 플랜으로 바꾸려면, 오히려 다들 경쟁사로 빠져나갈 수도 있음
            사용자들이 툴에 의존하도록 만들어 가격을 서서히 올리는 게 오히려 전략적으로 괜찮을 수도 있음
          + “에이전트 여러 개 돌리는 것”은 개인 플랜에선 일반적인 사용 사례가 아니라고 생각함
            항상 이런 경우엔 직접 API로 사용 요금을 지불해야 했음
            고정 요금제에서 이런 걸 허용해준 건 서비스가 너그러운 거였고 애초부터 “더 높은 한도”라고 광고했지 “무제한”은 아니었음
          + API는 한도가 훨씬 자유롭고, 실질적으로 거의 제한이 없음
            Claude는 Aws, gcp에서도 사용 가능해서, 한도와 크레딧도 다르고 각기 다른 요율임
          + 정책을 “좋은 사용자” 기준으로 최적화해야 하지 “나쁜 사용자”를 기준으로 맞추면 안 됨
          + 그냥 API 쓰면 됨
     * 전반적으로 보면, 일부 사용자가 24/7 다수 에이전트를 돌릴 때 시스템을 보호해서
       더 많은 유저가 지속적으로 쓸 수 있게 만드는 긍정적인 변화라고 생각함
       다만, “사용량이 얼마 남았는지” 보여주지 않는 점이 불편함
       몇 퍼센트인진 몰라도, 최소한 중간중간(예: 절반 지났을 때) 알림이라도 있었으면 계획 세우기 쉬울 것임
       이걸 제공하지 않는 게 “우리에게 측정하게 하고 싶지 않은 것 아닐까?”란 생각이 들게 함
       내가 꼼꼼히 측정하고 싶은 게 아니라, 내 위치를 대략 알고 싶을 뿐임
     * Anthropic Reddit 계정에 따르면
       한 사용자가 $200짜리 플랜으로 수만 달러어치 LLM을 사용했음
       회사 측은 고급 사용자들을 위한 별도 솔루션을 개발 중이지만
       이제 새 한도는 더 공평한 경험과 계정 공유·전매 방지 목적임
       그래서 우리가 “좋은 서비스”를 못 가지는 것임
          + 예전에 내가 다닌 스타트업도 무제한 옵션을 제공한 적 있음
            처음에는 아무도 그렇게 많이 못 쓸 거라 생각했지만, 실제로는 창의적으로 서비스 한계를 파는 사용자가 너무 많았음
            계정들이 24/7 서비스에 붙어서 요청 한도 95%까지 계속 요청함
            IP도 다양하게 쓰고, 사람이 쓴 것 같지 않은 패턴까지 나옴
            초기엔 이상치 정도라 받아들였지만, 이런 계정이 기하급수적으로 늘어나면
            실제론 여러 업체가 계정 여러 개를 만들어 로드밸런싱 하는 거였음
            평균 이익·손실 사용자별 그래프를 보면 이런 계정이 엄청난 손실만 남기고 최대치로 리소스를 써서, 결국 정책이 바뀌게 됨
            그런 “고객”을 잃지만 대부분의 일반 사용자는 영향 없음
            오히려 전체 서비스가 쾌적해짐
            이런 경험은 모든 고사용량 스타트업이 겪음
          + 실제로 회사가 적자 상태로 서비스를 파는 것일 수 있음
          + 현재 한도로도 이런 남용을 아이디 막을 수 없나? 이해가 잘 안 됨
          + 어떤 사람이 어제 트위터에서 자랑하고 있었음
            $200 계정으로 $13,200 사용했고, Opus 전용 에이전트 4~5개를 24/7로 서로 재귀 호출하며 돌렸다고 함
            이건 확실히 남용이고 타겟팅 받아야 함
            그런데 대체 inference 제공사가 이걸 어떻게 막아야 할지는 모르겠음
            Cursor는 Anthropc/OpenAI 대비 프리미엄을 더 붙여서 더 힘들어진 상태고
            Anthropic도 비슷한 상황이지만 여기선 프리미엄 옵션이 없음
            만약 20불에 월별로 실제 원가 500불까지만 허용하면, 결국 95% 할인해주는 셈이고, 이런 구조는 절대 못 견딤
            이런 식으로 지원하다 보면 커뮤니티에 “권리 주장” 분위기가 더 커짐
            익숙해진 걸 뺏기는 느낌이지만, 사실 캡/opex만 해도 감당이 안 되고 R&D 비용까지 고려하면 모델 유지 자체도 어려운 구조
            그래서 실질적으로 할 수 있는 건 “계속 가격 구조 바꾸고 유저들이 다음에 더 후하게 보조하는 회사로 떠나는 것”뿐임
            차라리 이런 정책은 애초에 트라이얼이라고 밝히고, 어느 정도 서브시디 나가는지 투명하게 얘기하는 게 나았음
            사람들은 모델을 체험하고 어느 정도는 남지만, 얼마간 이탈하더라도 불만은 줄었을 것임
            진짜로 캡/운영비/개발비 구조에 대해 투명하게 공개하면
            사람들도 “그 정도면 tireless senior engineer 한 명 구하는 수준”이라는 걸 납득할 수 있음
     * 이 이메일에 월별로 “어떤 달에 한도에 도달했는지(Aug 2024, Jan 2025, May 2025 등)” 정보라도 있었으면 훨씬 유익했을 것임
       내가 상위 5%인지 전혀 알 수 없음
       사실 1% 한도 제한은 납득 가지만 SaaS 업계에서 5%면 실 사용자 대부분임
     * 이런 서비스에는 계량 요금제가 필요함
       모든 AI 회사가 같은 문제에 부딪히고 있음
       고정비 구독 요금제가 사용자가 비용 신경 안 쓰길 바라는 구조임
       그런데 극소수 파워 유저는 구독 한도를 극한까지 몰아써서
       Terragon 같은 서비스는 그 사용량을 최적화해주려고 따로 개발됨
       이 때문에 회사는 계속 한도를 낮추고, 사용자는 오히려 비용을 더 신경 쓰게 됨
       Cursor도 한도를 여러 번 조정했고, 이제 Anthropic도 따라가는 중
       결국 상위 10% 극한 사용자를 더 이상 보조하지 않으려는 것
       웹 인터페이스에서 바로 쓸 수 있는 계량형 요금제가 있으면 함
          + API가 존재해서 직접 토큰을 만들고 Claude Code를 별도 요금제 없이 바로 쓸 수 있음
          + 1990년대 공유 호스팅 시절이 떠오름
          + 계량제 웹 플랜을 제공하면, 실제로 inference 지원에 드는 비용이 얼마나 비싼지 공개해야 해서 난감함
            생산성 높은 실사용 수준에서 AI 돌리는 게 현재는 아직 굉장히 비싼 일임
     * “Claude를 24/7 백그라운드로 돌리는 고급 사용 패턴” 때문에 우리가 좋은 서비스를 누리지 못하는 것임
          + AI 서비스들이 광고할 때 “AI가 알아서 일을 돌려준다. 개발자는 커피를 마시거나 잘 때도 일을 해결해준다”라고 강조하는데, 실제로 그 서비스를 합당하게 쓴 개발자도 있음
            이제와서 그런 사용자가 문제라고 탓하는 건 이상함
          + 그 부분 보고 웃음이 남
            마치 ‘선의의 세계 파괴자’가 우주 열사를 한층 촉진시키려는 것 같은 느낌임
          + 이런 문제는 당연히 예상됐다고 생각함
            초기 요금제 결정할 때 이미 충분히 고려됐을 것
            단지 출시를 막는 걸 원치 않아 도입이 늦어진 것 같고, 이젠 현실에 맞춰 시행되는 중임
          + 요금제를 어떻게 책정하든 사용자는 자기 플랜을 100% 활용하려고 함
            나는 Max 구독자임에도 자주 한도에 걸림
            내가 결제한 만큼 돌리는데 한도 적용 자체가 이상함
          + 이게 바로 가격 실험 구조임
            규제가 약하면 언젠간 극단적 활용자들이 등장하고, 회사는 그걸 지속 가능하지 않은 마치 가능한 것처럼 포장했다가, 나중에 보너스를 뺏는 구조임
     * 혹시 엉뚱한 제안일 수 있지만, 적응형 한도는 어떨까 생각해봤음
       옵션 1: 처음엔 짧은 기간 동안 폭발적으로 쓰게 해주고 점진적으로 속도를 늦추고, 쿨다운 이후 다시 폭발이 가능하게 하는 방식
       이러면 유저가 짧게 생산성 극대화하고, 서버도 쉴 수 있음
       옵션 2: 모바일 데이터처럼 쓸 수 있는 요청량은 빠르다가, 이후엔 속도 제한 걸고, 더 필요하면 유료 구매 방식
       이건 수익도 추가로 생기는 모델
       옵션 3: 인프라와 네트워크에서 적응적으로 자원 할당
       GPU가 아닌 작업은 우선순위 낮추거나, 네트워크 요청을 느리게 처리, 또는 k8s 등에서 사용량 따라 서로 다른 서버에 작업을 분배
       그리고 한도 논의 말고, 진짜 비용을 많이 쓰는 요청이 뭔지 추적해서, 비효율적인 코드 경로나 인프라 구조를 손 보면 여유가 생길 수 있음
       작은 코드 최적화로도 시스템에 큰 여유를 줄 수 있다는 점 강조함
"
"https://news.hada.io/topic?id=22350","Qwen-Image: 네이티브 텍스트 렌더링을 적용한 이미지 생성 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Qwen-Image: 네이티브 텍스트 렌더링을 적용한 이미지 생성 모델

     * Qwen-Image는 네이티브 텍스트 렌더링과 정밀 이미지 편집에 강점을 가진 20B 파라미터 MMDiT 이미지 생성 모델임
     * 알파벳 및 한자 등 다양한 언어의 복잡한 문자 표현에서 높은 정확도와 시각적 완성도를 달성함
     * 다양한 공개 벤치마크(GenEval, DPG, OneIG-Bench 등) 에서 동급 최고 성능 달성, 텍스트 생성 능력도 뛰어남
     * 실제 데모에서는 다언어, 포스터, PPT, 일러스트 등 복잡한 레이아웃과 다양한 스타일을 정확히 구현함
     * 스타일 변환, 오브젝트 추가·삭제, 상세 묘사, 포즈 변경 등 편집 기능 지원 및 오픈 소스 생태계 확장 지향함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 주요 특징

     * Qwen-Image는 복잡한 텍스트 렌더링과 정밀 이미지 편집에 특화된 20B 파라미터 기반 MMDiT 이미지 생성 베이스 모델임
     * Qwen Chat에서 최신 모델을 체험 가능함

  주요 기능

     * 우수한 텍스트 렌더링: 다중 행 레이아웃, 문단 수준 의미 파악, 세밀한 표현 가능
          + 영문 및 한자 등 알파벳 기반과 표의문자 계통 모두 고충실도 지원
     * 일관성 있는 이미지 편집: 향상된 멀티태스크 훈련을 통해 의미적 정확성과 시각적 리얼리티 모두 보존
     * 강력한 벤치마크 성능: 다양한 공개 벤치마크에서 생성 및 편집 태스크 모두 동급 최고 성능 달성
     * 텍스트 생성·편집 영역에서 LongText-Bench, ChineseWord, TextCraft 등에서 우수한 성적 기록
     * 창작·디자인·스토리텔링 등 크리에이티브 용도에 폭넓게 활용 가능

성능 및 벤치마크

     * Qwen-Image는 GenEval, DPG, OneIG-Bench(일반 이미지 생성), GEdit, ImgEdit, GSO(편집) 등 벤치마크에서 모두 최신 SOTA(최첨단) 성능 달성
     * 특히 중국어 텍스트 생성 등에서 기존 최고 모델을 큰 차이로 능가
     * 넓은 일반적 능력과 함께, 정확한 텍스트 렌더링을 결합해 리딩 이미지 생성 모델로 자리매김

데모 예시

  중국어 텍스트 표현

     * 예시 프롬프트를 기반으로, 미야자키 애니메이션 스타일과 동시에 실제 상호 “云存储”“云计算”“云模型” 및 특이한 한자(“千问”) 등을 정확하게 표현
     * 인물 포즈나 표정, 장면 내 깊이감 등도 자연스럽게 구현

  복잡한 한자 병렬 표현

     * 고급스러운 대련, 휘호, 청화자 등의 디테일까지 섬세하게 표현
     * 글씨체, 레이아웃, 그림(예: 岳阳楼)까지 실제와 유사하게 완벽 생성

  영어 텍스트 & 멀티라인

     * 책방 진열대, 안내문, 포스터 등 여러 위치의 텍스트 상세히 반영
     * “New Arrivals This Week”부터 서적 표지의 짧은 문장까지 실제 같은 폰트와 레이아웃 구현

  복잡한 영어 인포그래픽

     * 각 서브모듈별로 아이콘+타이틀+설명문 단락까지 분리해 정확하게 배치
     * “Habits for Emotional Wellbeing” 중심의 복잡한 인포그래픽도 자연스러운 아트웍과 균형 잡힌 구성으로 완성

  소형/긴 텍스트

     * 이미지 내 1/10 이하 소규모 영역까지 장문 손글씨 텍스트 상세 구현
     * 다량의 문장도 손글씨, 레이아웃, 줄 바꿈까지 정밀하게 재현

  다국어 혼합

     * 영어와 중국어를 한 이미지 내 동시에 손글씨로 구현
     * 프롬프트의 언어 전환에 따라 자연스럽게 텍스트 생성 가능

  포스터 생성

     * 영화 포스터, 하위 부제, 출연진·감독·런칭 정보 등 각각의 텍스트/비주얼 요소를 Sci-Fi, 그래픽디자인 등 다양한 스타일로 자유롭게 조합

  한글 PPT 예시

     * 최신 AI/기업 PPT 스타일(Alibaba 로고, 대제목, 부제, 예술작품 이미지 배치, 캘리그래피 폰트, 세부 설명 등)까지 통일감 있게 생성

일반 이미지 생성 및 편집

     * 포토리얼·인상파·애니메·미니멀 등 다양한 아트스타일 지원, 풍부한 창작 활용성 제공
     * 스타일 변환, 오브젝트 추가/삭제, 디테일 향상, 텍스트 편집, 인체 포즈 조정 등 다양한 실전 이미지 편집 명령 지원

결론

     * Qwen-Image는 이미지 생성의 지평 확대, 시각 콘텐츠 제작의 기술 장벽 낮춤, 창의적 활용 촉진 목적 지향
     * 커뮤니티 협력, 개방성, 지속 가능한 생성형 AI 생태계 구축에 중점
     * 실제 사용자 활용/피드백을 반영하여 기능 향상 및 오픈 생태계 확장 계획

        Hacker News 의견

     * 이게 그렇게 큰 이슈가 되지 않는 이유를 잘 모르겠음 —– 이건 gpt-image-1을 모든 면에서 제친 첫 오픈소스 모델일 뿐만 아니라, Flux Kontext보다 편집 능력까지 앞선 모델임. 이건 엄청난 일임
          + 지난 한 시간 정도 이 모델을 가지고 놀아봤음. 전체적으로 정말 뛰어나지만, 내가 해본 초반 테스트 기준으로는 꽤 복잡한 프롬프트 준수에서는 확실히 gpt-image-1 (혹은 Imagen 3/4)보다는 부족한 성능을 보임. 대략 ~50% 정도의 성공률이었고, gpt-image-1은 ~75% 수준임. 미로, 슈뢰딩거 방정식 등은 처리하지 못했음. genai showdown 사이트에서 실험했음
          + 이들의 페이지만 봐서는 확실하지 않지만, 편집 모델은 아직 정식으로 공개되지 않았음. 깃허브 이슈 코멘트 링크 참고
          + 내 생각엔 gpt-image-1보다 확실히 더 많은 걸 할 수 있음. 스타일 변환, 오브젝트 추가/삭제, 텍스트 편집, 사람의 포즈 조작뿐만 아니라 오브젝트 감지, 의미론적 분할, 깊이/에지 추정, 초해상도, 그리고 새로운 시점 합성(NVS) 즉, 기본 이미지를 바탕으로 새로운 시점의 이미지를 만들어낼 수 있음. 정말 기능의 향연임. 초기 결과를 보면 gpt-image-1이 또렷함과 선명도에서 약간 우수해 보임. 솔직히 OpenAI가 후처리로 단순한 언샤프 마스크 같은 걸 적용하고 있는 게 아닌지 의심스러움. 초점이 흐려진 영역에도 이상하게 균일한 선명도를 보이기도 하고, 때때로 과도하다는 느낌도 들었음. 그래도, 전반적으로 이 모델도 거의 비슷한 수준으로 보임. 사실 OpenAI만의 독특한 이미지 생성 기술이 올해는 우위를 유지할 줄 알았는데, 이 정도라니 놀라움. 참고로 Flux
            Krea는 공개 이후 4일밖에 안 지났음! 만약 이 모델이 gpt-image-1과 진짜 비슷한 품질이라면 놀라운 변화임
          + 내가 알기로 이게 40GB VRAM이 필요하다는 점이 대중의 열기를 조금 식히는 것 같음. 참고로, LLM 모델들은 여러 GPU에 분산 배포하는 기술이 꽤 성숙하지만, 이미지 모델들은 GGUF 포맷을 쓰는데도 아직 이쪽 발전이 느린 이유를 모르겠음. 이미지 모델이 더 커질수록 분산 실행을 더 많이 구현하게 되지 않을까 하는 생각임
          + 몇 시간밖에 안 지난 상황이고, 데모도 계속 오류가 나서 사람들이 충분히 만져보려면 시간이 좀 더 필요하다고 생각함. 퀀타이즈된 GGUF와 다양한 컴피(Comfy) 워크플로우의 등장도 매우 중요한 요소가 될 것으로 봄. 왜냐하면 대부분의 유저들이 로컬에서 돌리고 싶어할 것이기 때문임. 근데 크기는 다른 모델보다 꽤 큼. 재미있게도 가장 큰 비교 대상은 Flux보다는 Alibaba끼리임. 예를 들어 Wan 2.2를 이미지 생성에 쓰는 게 이미 엄청 인기라서, 대부분 Qwen-Image가 Wan 2.2 대비 얼마나 큰 점프를 했는지 더 궁금해 함. 신규 이미지 모델의 실제 평가 시점은 보통 런칭 후 1주일 정도가 가장 좋은 듯함. 그때쯤엔 유저들이 직접 많이 테스트해서, 3자 관점의 장단점들이 정리됨. 이번 모델도 기대가 큼
     * 좋은 릴리즈임! GenAI Showdown 사이트에 추가했음. 전체적으로 대략 40% 점수를 기록한 꽤 좋은 모델이고, 특히 소비자용 GPU에서 돌릴 수 있는 SOTA 모델이라 봄(퀀타이즈 버전이면 더욱 그러함). 다만, txt2img 프롬프트에 정확히 따라가는 면에서는 OpenAI의 gpt-image-1보다 현격히 떨어지는 건 사실임. 하지만 본 스레드에서도 언급됐듯, 이 모델은 편집 등 다양한 작업이 가능한 점이 장점임. genai showdown에서도 확인 가능함
          + 참고로 Imagen 3와 4는 엄연히 다른 모델이기 때문에 섞어서 비교하는 건 적절하지 않다고 생각함
     * 이런 걸 자주 해보는 사람들에겐 당연할 수도 있겠지만, 이걸 돌릴 수 있는 하드웨어 사양이 궁금함. 리눅스에서 16GB GPU랑 64GB RAM이 있는 머신에서 돌려봤음. 이 PC에서는 SD는 문제 없이 작동함. 그런데 Qwen-image는 GPU로 돌리든, CPU로 돌리든 메모리가 부족하다는 에러가 떴음. 이 정도면 많이 부족한 건지, 두 배만 더 있으면 되는 건지, 몇 십 배를 더 올려야 하는 건지, 아니면 정말 미친 하드웨어가 필요한 건지 궁금함
          +

     이런 거 자주 하는 사람에겐 당연할 수도 있겠지만사실 그리 당연하지 않음. VLM/LLM의 VRAM 사용량 계산은 거의 마법 같은 영역임. 온라인에 대충 10개쯤 계산기가 있지만, 제대로 맞는 게 없음. 퀀타이제이션, KV 캐싱, 액티베이션, 레이어 등 다양한 변수가 작용함. 매우 귀찮은 부분임. 아무튼 이번 모델의 경우 40GB 이상의 VRAM이 필요함. 일반적인 시스템 RAM은(애플 실리콘에서 유니파이드 RAM이면 모를까) 부족함. 심지어 애플 실리콘에서도 메모리 대역폭이 낮아서 추론 속도가 GPU/TPU 대비 매우 느려짐
          + 모델 파일 크기와 거의 비슷할 거라고 봄. transformers 폴더를 보면 대략 9개의 5GB 파일이 있는데, 대략 GPU에 45GB VRAM이 필요하다 생각하면 됨. 보통 퀀타이즈된 경량 버전(품질 저하는 감수)이 곧 공개될 것으로 예상함
          + Qwen-Image는 풀 모델 기준 최소 24GB VRAM이 필요함. 다만 4비트 퀀타 버전은 AutoGPTQ 같은 라이브러리로 약 8GB VRAM에서도 실행 가능함
          + 4비트 퀀타이즈 버전이 공개되려면 며칠 기다려야 할 것 같음. 파라미터 수는 20B임
          + 프로덕션 추론 환경에선 1xH100으로 잘 돌아감
     * 다른 이미지 생성 모델과 달리 4o image gen처럼 이미지 전체를 불필요하게 바꾸지 않는 점이 놀라움. 4o에서는 옷만 수정하려고 하면 얼굴까지 바뀌곤 하는데, 이 모델은 AI 특유의 인위적인 흔적을 수정이 필요한 부분에만 넣어주는 듯함
          + 그래서 Flux Kontext가 엄청나게 화제가 됐던 것임 — 직접 마스킹할 필요 없이 img2img 인페인팅의 힘을 준 게 혁신적이었음. 에디팅 관련 블로그 참고
          + 4o에서도 편집하고 싶은 영역만 선택하면 나머지는 그대로 둘 수 있음
     * 최근 중국 오픈소스 모델들이 미친 듯이 좋게 나오고 있음. 이런 소식을 접할 때마다 진짜로 희망이 생김
     * 이런 모델에서 실제로 텍스트 렌더링은 어떻게 학습시키는지 아는 사람 있음? 내가 써본 모든 모델(OpenAI, Flux 포함)이 다 같은 문제점이 있는데, 텍스트가 자연스럽지 않고 그림 내의 그림자나 반사 표현이 본래 이미지와 다르게 어색함. 아마 비슷한 트릭을 쓰고 있는 듯함?
          + 기술 보고서 14쪽에 설명되어 있음. 이미지 위에 텍스트를 덧입혀 합성 데이터를 만든다고 되어 있음. 원래 조명 조건은 고려하지 않은 채로 덮어쓰기만 해서 학습된 듯함. Garbage in, garbage out임. 미래에는 더 현실적인 텍스트 합성 방법이 나와서, 그걸로 학습시키면 텍스트도 자연스럽게 만드는 모델이 탄생하지 않을까 기대함
     * 논문 3.2절 Data Filtering 파트도 확인해 보길 권장함. 원본 논문 PDF 참고
          + 흥미로운 점은 영어랑 중국어 외에는 언급되거나 예시가 나온 언어가 없다는 점임
     * 이걸 직접 호스팅해서 쓸 때 합리적인 결과를 낼 수 있는 가장 낮은 그래픽카드 사양이 궁금함
     * 캔버스가 짧음
     * 얼마나 검열이 심한지 궁금함
          + 새 모델 나올 때마다 커뮤니티가 항상 가장 궁금해하는게 그 점이고, 실제로 아무 조직도 인간 본성의 불편한 현실을 맞닥뜨리고 싶어하지 않음. 그와 동시에 미국 사회나 기업엔 묘한 조심성과 금욕주의가 만연한 듯함
"
"https://news.hada.io/topic?id=22301","Anthropic, OpenAI의 Claude API 접근 차단","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Anthropic, OpenAI의 Claude API 접근 차단

     * Anthropic은 OpenAI가 자사 서비스 약관을 위반했다며 Claude 모델에 대한 API 접근 권한을 차단함
     * OpenAI는 Claude를 자체 내부 도구에 API로 통합하여 자사 모델과 기능·안전성 비교 평가를 진행하면서 약관의 경쟁 금지 조항을 위반함
     * AI 업계에서는 경쟁사 접근 제한이 빈번히 발생하며, Salesforce 등 다른 대형기업들도 유사한 조치를 실시한바 있음
     * OpenAI는 Anthropic의 결정을 존중하지만, 자사 API는 Anthropic에도 계속 개방되어 있음에 아쉬움을 표명함
     * Anthropic은 앞으로도 업계 표준에 따라 벤치마킹 및 안전성 평가 목적의 제한적 API 접근은 허용할 계획이라고 밝혔지만, 실제 영향 범위는 불분명
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Anthropic, OpenAI의 Claude API 접근 권한 차단 개요

     * Anthropic이 7월 30일(현지 시각) OpenAI의 Claude 모델 API 접근을 차단함
     * 복수의 소식통에 따르면 이용약관 위반이 이유로, OpenAI에 차단 사실이 통보됨

서비스 약관 위반 내용 및 경위

     * Anthropic 대변인 Christopher Nulty는 ""Claude Code가 개발자들 사이에서 선호되고 있고, OpenAI 소속 엔지니어들도 GPT-5 출시 전에 해당 코딩 툴을 사용했다""고 언급함
          + 최근 OpenAI는 GPT-5 출시 준비 중이며, 이 모델이 코딩 성능이 뛰어날 것으로 알려짐
     * 해당 행위는 Anthropic의 상업용 서비스 약관에 명시된 ""경쟁 상품/서비스 개발 목적의 사용, 혹은 역분석 또는 복제 행위 금지"" 조항을 직접적으로 위반함
     * OpenAI는 기존 일반 채팅 인터페이스가 아닌, 특별 개발자용 API를 활용하여 Claude를 내부 평가 도구에 통합함
          + 이를 통해 코딩, 창의적 글쓰기, 그리고 CSAM·자해·명예훼손 등 안전성 관련 사례 반응을 자사 AI 모델과 비교함
          + 이러한 결과로 OpenAI는 자사 모델의 동작 특성을 타사와의 조건에서 비교 분석하고 알고리듬 개선에 활용함

업계 관행과 OpenAI 입장

     * OpenAI측은 ""타사 AI 시스템을 벤치마킹 및 안전성 개선 목적으로 평가하는 것은 업계 표준""이라고 밝힘
     * OpenAI 홍보 책임자인 Hannah Wong은 Anthropic의 API 중단 결정을 존중하지만, 반대로 OpenAI의 API는 Anthropic에게 계속 열려 있어 상호주의 차원에서 아쉬움이 있다고 언급함

경쟁사 접근 제한과 업계 흐름

     * 경쟁사간 API 접근권 제어는 예전부터 IT 업계의 반복되는 전략임
          + Facebook이 Twitter 계열 서비스** Vine의 API 접근을 차단**해 반경쟁 행위 의혹이 제기된 사례,
          + 최근 Salesforce가 경쟁사의 Slack 데이터 접근을 제한한 사례 등이 있음
     * Anthropic 역시 과거 Windsurf라는 AI 코딩 스타트업의 접근을 일시적으로 제한했으며, 이 또한 OpenAI 인수설과 관련이 있었음(해당 인수는 철회됨)
     * Anthropic의 과학 책임자 Jared Kaplan은 ""Claude를 OpenAI에 판매하는 것은 이상하다""고 이례성을 언급한 바 있음

API 접근 제한의 추가 배경

     * OpenAI의 Claude API 차단 하루 전, Anthropic은 Claude Code의 폭증하는 사용량과 일부 약관 위반 발생을 이유로 새 사용량 제한 정책도 발표함

결론 및 업계 영향

     * Anthropic은 업계 표준에 따라 ""벤치마킹 및 안전성 평가 목적""에는 OpenAI가 제한적으로라도 Claude API를 사용할 수 있도록 유지할 것을 약속함
     * 그러나 현 시점에서 정확히 어떻게 이 작업이 진행될지에 대해서는 명확한 입장이 없는 상황임

   ""AI야, 저친구가 잘하니까 저 친구 따라해."" 로 학습

        Hacker News 의견

     * ""OpenAI가 특별 개발자 접근(API)을 통해 Claude를 내부 툴에 연결했다""라는 표현이 실제로 OpenAI가 내부적으로 사용하는 Claude API였다면 모르겠지만, Wired의 표현 방식이 너무 과장된 느낌임. 마치 ""OpenAI가 Claude의 AI 메인프레임을 해킹했다가 Sonnet가 방화벽을 닫았다""는 식의 분위기가 남. 왜 단순한 API 접근을 ""특별 개발자 접근""이라고 부르는지 진지하게 궁금해짐. 사실, 보안 측정 기준이 어디까지가 허용인지, 어디서부터가 도둑질인지에 대한 의견 차이가 있을 수는 있다고 생각함. 최적의 취약점 탐지법이 궁금해지는 대목임. 참고로 Anthropic이 나를 비롯해 꽤 많은 사람을 API에서 차단한 것 같음. 그냥 개인적으로 받은 편지를 읽어볼 수 있나 시도했을 뿐인데 문의해도 답이 없었음. 어차피 OpenRouter를 이용하면 제약이 별로 의미 없음
          + 해당 표현이 과장되게 들리는 건 맞는 듯함. 하지만 작성자가 혼동했을 수도 있음. API 업계에서는 일부 고객만 접근 가능한 ""특별 API""가 실제로 존재함. 만약 누군가 그걸 ""특별 개발자 접근""이라고 부른다면 틀린 표현은 아니라고 생각함
          + ""왜 API 사용을 특별 개발자 접근이라고 부르냐""라는 주장에, 원래 API란 일반 사용자가 쓰는 게 아니고, 개발자가 작성한 프로그램이 그걸 써서 Claude에 접근하는 용도임. SDK를 ""개발자 전용 툴킷""이라고 부르는 것과 비슷한 이치라고 생각함
          + Anthropic이 내 계정을 약관 위반이라며 아무 답변 없이 그냥 영구차단했음. 난 단지 Claude에게 음악이랑 SF책 추천 3~4번 물어본게 전부인데 이렇게 됐음. 정말 규칙 위반한 적도 없고 평범하게 UI로 계정 만들어 조금 쓴 것이 전부였음. 지금은 그냥 아주 가끔 테스트삼아 open router로 쓰는 게 다임
     * Anthropic의 상업 이용약관에 따르면 ""경쟁 제품/서비스 구축, 경쟁 AI 모델 훈련"" 용도로 이용을 금지하고 있음. 나는 사용 라이선스에 제약 없는 도구를 더 선호하므로 계속 그런 쪽만 쓸 생각임
          + 이런 반경쟁 조항이 Google, Microsoft, Meta 등 주요 AI 기업에서 업계 표준처럼 되고 있음. 모델 개발에 있어서 점점 폐쇄 생태계(월드가든)로 수렴 중임
          + 트위터 파이어호스에도 타사 트위터 클라이언트 재현 금지 조항 있었음
          + 혹시 나도 포함일지 궁금함. 난 Claude를 활용해 SNN 기반 언어 모델 실험 중인데, 사실 잘 안 돌아가지만 이론상으론 경쟁 모델 실험이긴 함. 그냥 재미삼아 배우는 중이고 결과물은 아직 없음
          + 예전에 OpenAI도 DeepSeek에 불만 제기한 적 있으니 어느 정도 상호 공정성이라고 볼 수도 있음
          + 예전에도 Microsoft 개발도구로 워드프로세서나 스프레드시트 만드는 게 라이선스 위반이었고, Oracle은 타DB와의 벤치마크 결과 공개도 금지했었음. 벤더와 경쟁하거나 경쟁사에 도움을 주면, 해당 벤더가 친절하게 받아주길 기대하지 말고, 심지어 고객 유지조차 기대하지 않는 게 좋음
     * 기사 내용을 보면 실질적으로 새로운 이야기는 없고, 그냥 상반된 시각만 제시하고 있음.
       1) OpenAI의 기술진이 Claude Code를 API(최고 요금제는 아님)로 사용했음.
       2) Anthropic 대변인은 OpenAI가 벤치마킹 및 평가 목적으로 API 접근을 계속 받을 수 있다고 했음.
       3) OpenAI는 API를 벤치마킹에 쓴다고 밝혔음.
       내 생각엔 모델 벤치마크는 괜찮지만, 툴 벤치마크는 선을 넘는다는 분위기임. 즉, OpenAI가 자사 제품이 Claude code보다 특정 기준에서 더 잘 동작하는지 보려고 했다는 것 같고, 그걸 Anthropic에서 금지한 상황임. 적발 방식이 훨씬 인상적임. Livebench에서 sonnet 4로 문제를 푸는 거랑, 자체 결과를 공개하지 않은 벤치마크 도구로 우회하는 건 조금 다름. 이게 반드시 옳다는 얘긴 아니지만 현 상황이 그렇다고 생각함
          + 이런 일은 차라리 Jepsen 같은 서드파티 벤치마크 전문가가 했으면 좋겠음. 경쟁사가 직접 서로를 평가하려다 보니 불편해지는 건 이해감
     * Anthropic이 ""OpenAI가 벤치마크 및 안전성 평가 목적으로 API 접근하는 건 계속 보장한다""고 발표한 건 매우 훌륭한 PR 전략임. '우리가 워낙 좋아서 OpenAI도 우리 제품을 쓴다'는 인식을 주려는 의도가 읽힘. OpenAI가 공식 계정 아니면 마음만 먹으면 다시 가입할 수도 있다는 걸 Anthropic이 잘 알고 있음
          + 수십억 달러 기업이 차단을 피해서 몰래 사용하는 건 어렵고, 적발될 경우 법정 분쟁에 휘말릴 가능성이 높음
     * OpenAI 서비스약관에도 ""고객은 OpenAI와 경쟁하는 인공지능 모델 개발에 산출물을 사용할 수 없음""이라고 명시되어 있음. 사필귀정임
          + 여러 AI기업들이 실제로는 저작권법, 상표법, 명예훼손법, 계약법, 그리고 각종 윤리 이슈까지 법을 무시하는 사례가 많았고, 실제로 지금까지도 다 피해가고 있음
     * https://archive.is/m4uL7
     * ""OpenAI의 기술 스태프가 GPT-5 출시 전 우리 코딩툴을 먼저 사용했다""라는 식의 보도는 오히려 값싼 PR 공격처럼 느껴짐. API를 훈련에 쓴 것과 개발자들이 코딩에 쓴 것의 의미는 엄연히 다름
     * 예전에 OpenAI 관계자들이 실은 다양한 연구소들이 서로 협업하지만, 공식적으로는 밝히지 않는다고 말한 것 같은 기억이 있음
     * 내 기대엔 모든 AI 기업이 최소한 평가 목적으로 타사 모델 API를 ""특별 개발자 접근"" 형태로 도구에 통합해 쓰는 게 일반적임. 오히려 Anthropic 측이 이런 식으로 어필하는 게 마이너스 이미지임
"
"https://news.hada.io/topic?id=22297","ArmSoM RK3506J CM1 설계 비하인드: 산업용 등급 임베디드 시스템의 모듈형 혁명 재정립","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ArmSoM RK3506J CM1 설계 비하인드: 산업용 등급 임베디드 시스템의 모듈형 혁명 재정립

    1. 디자인 철학 & 산업 포지셔닝
       모듈형 혁명

   신용카드 ½ 크기(55×40mm)의 초소형 폼팩터

   ""코어 모듈(CM1) + 커스텀 캐리어 보드"" 분리 아키텍처

   기존 PLC 시스템 60% 비용 절감 (코어 교체만으로 업그레이드)

   산업 신뢰성

   -40°C ~ 85°C 극한 환경 인증

   50G 충격/진동 내성 (BTB 커넥터 + 나사 고정 설계)
    2. 핵심 기술 아키텍처
       2.1 이기종 컴퓨팅 시스템
       프로세서 역할 성능
       3x Cortex-A7 Linux GUI/알고리즘 처리 1080p@60fps 렌더링
       Cortex-M0 실시간 제어 ≤1ms 지연시간
       → PLC-터치스크린 통합 구동 가능 (별도 PLC 모듈 불필요)

   2.2 산업용 인터페이스
   이중 이더넷: 링 토폴로지 지원, 50ms 내 장애 극복

   CAN 2.0B: 모터/인버터 직결 지원 (Modbus/CANopen 내장)

   FLEXBUS: FPGA/가속기 확장 용이 (32-bit 병렬 버스)
    3. 산업 적용 사례
       시나리오 기존 문제 CM1 솔루션 성과
       레거시 장비 현대화 구형 칩 단종 커스텀 캐리어보드 연결 베어링 고장 예측 정비
       다중 HMI 저온에서 UI 정지 -40°C 8초 부팅 반응속도 5배 ↑
       전력 모니터링 유지보수 빈번 광범위 온도 내구성 고장률 70% ↓
    4. 개발자 생태계
       오픈소스 하드웨어:

   GitHub 4층 레퍼런스 PCB (산업용 EMC 가이드라인 포함)

   실시간 BSP:

   Preempt-RT Linux (≤100μs 레이턴시)

   Modbus/CANopen/Profinet 프로토콜 스택 내장

   글로벌 지원:

   12시간 기술 대응

   RK3506J 10년 공급 보장
    5. 미래 비전
       ""모듈화가 진화를 주도한다""

   확장형 모듈 생태계: AI 가속/5G 통신 모듈 예정

   오픈소스 컨테스트: 산업 애플리케이션 라이브러리 구축

   핵심 미션: 모든 산업 장비에 진화 가능한 '심장' 제공

   🔗 [캐리어 보드 설계 가이드] | [레퍼런스 회로도]https://docs.armsom.org/armsom-cm1#hardware-resources
"
"https://news.hada.io/topic?id=22330","운동은 기적의 약 – 왜 운동만한 치료제가 없을까","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      운동은 기적의 약 – 왜 운동만한 치료제가 없을까

     * 운동은 지금까지 발견되거나 개발된 어떤 의약품보다도 더 강력한 효과를 가진 '의학적인 발명품' 임
     * 최근 연구 결과, 운동은 근육, 심장뿐 아니라 간, 부신, 지방, 면역계 등 거의 모든 신체 조직에 긍정적인 분자 수준 변화를 가져오는 것으로 확인됨
     * 최근 대장암 환자 연구에서도 운동 프로그램 참여 그룹이 더 긴 무병 기간과 생존율 증가를 보임
     * 글로벌 보건 지원은 지난 20년간 1억 명 이상의 생명을 구함, 이는 미국 연방 예산의 0.8%로 이룬 성과임
     * 작은 투자로 상상을 초월하는 생명 연장 효과를 가져오는 것이 공적 보건 투자와 운동의 공통점임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

우리는 운동보다 나은 약을 만들지 못할 것

     * 스탠포드 의대 Euan Ashley 교수는 운동이 지금까지 발견된 어떤 약물보다 폭넓고 강력한 효과를 가진 최고의 의학적 발명품임을 주장함
          + 최근 쥐를 대상으로 한 실험에서, 운동은 근육과 심장뿐 아니라 간, 부신, 지방, 면역 시스템 등 다양한 조직에 분자 시스템을 변화시키는 효과를 유도함
          + 운동의 효과를 한 가지 약품이나 치료로 대신할 수 있는지 여부에 대해 Ashley는 그 효과가 너무 광범위하기 때문에 절대 불가능하다고 단언함
          + 대표적으로, 에어로빅과 웨이트 트레이닝은 신진대사 활성, 미토콘드리아 기능 향상, 면역력 강화, 염증 감소, 조직별 적응력 개선, 질병 예방 등 여러 영역에 긍정적인 영항을 미침
     * New England Journal of Medicine의 최근 연구에서 진행성 대장암 수술을 받은 900명의 환자들을 두 그룹으로 나눠 비교함
          + 한 그룹은 구조화된 운동 프로그램 (행동 지원 세션, 전문가 지도 하 운동 수업)에 장기간 참여하고, 대조군은 간단한 건강 및 식단 정보만 제공받음
          + 운동 그룹은 대조군에 비해 암 재발 없는 기간이 유의하게 길어졌으며, 8년 후 전체 생존율이 7%포인트 더 높고, 새로운 암 발생이 크게 감소하는 긍정적 결과를 보임
     * 운동은 단순히 질병 예방 이상의 역할을 하며, 이미 질병을 앓는 환자의 생존율 또한 높이는 강력한 생명 구원 요인임

현대 사회와 운동의 의미

     * 진화인류학자 Daniel Lieberman은 운동이 건강하고 보람찬 행위지만, 인간이 진화적으로 ‘운동하기 위해’ 설계된 존재는 아니라고 설명함
     * 현대의 물리적 편의에 적응하기 위해 인류는 다양한 운동 도구와 방식을 고안해냈으며, 이러한 신체적 스트레스의 모방이 분자 수준에서 어떤 치료법보다 효과적임

1억 명의 생명을 구한 공적 보건 투자

     * 미국 국제개발처(USAID) 예산 축소 논란이 있으나, 글로벌 헬스 분야에선 약 20년간 1억 명 가까운 생명을 살린 업적이 Lancet에서 발표됨
          + HIV/AIDS: 2,500만 명
          + 설사 질환: 1,100만 명
          + 하기도 감염: 900만 명
          + 소외열대질환: 900만 명
          + 말라리아: 800만 명
          + 결핵: 500만 명
          + 영양실조: 200만 명
     * 미국 연방 예산의 0.8%에 불과한 투자로, 도덕적 투자 대비 압도적 성과를 거둔 셈임
     * 이 모든 성과가 미국 연방 지출의 0.8% , 즉** 국가 전체 예산의 1/400에 불과한 비용**으로 이루어진 것임

불평등과 도덕적 책임

     * 누구도 자신이 어떤 나라, 환경에 태어날지 선택할 수 없음
     * 미국 시민으로 태어났다는 것은 우연한 행운이며, 세계 부의 불균형은 작은 투자로도 빈곤국에서 엄청난 생명 구제 효과를 낼 수 있게 함
     * 모기장, HIV 치료제, 백신 등 간단한 개입만으로 엄청난 인명 구조가 가능함

결론 및 제언

     * 미국의 보건·개발 투자는 일종의 현대판 연금술로, 전체 지출의 400분의 1로 1억 명의 생명을 구하는 데 성공함
     * 사회 전체적으로 운동과 공공보건 투자 모두 소규모 비용으로 거대한 건강 효과를 낼 수 있음

   대학원 시절 답답하면 자전거 타고 2시간 내내 가고 싶은 곳으로 달려갔었는데, 그때의 기분은 최고였습니다. 강을 가로지르다가 피곤하면 벤치에 누워서 노래좀 듣고... 자유로운 운동(러닝, 자전거)의 효능을 톡톡히 봤습니다.

   저도 그러는데 기분 최고입니다.. 강이 학교 근처에 있어서, 대학원에서 정말 빠르게 자연으로 탈출할 수 있다는게 얼마나큰 행운인지 몰라요..

   이거 운동 바이럴이네요

        Hacker News 의견

     * 나는 매주 운동을 얼마나 했는지에 따라 내 기분이 직결되는 경험을 하고 있음, 삶이 바쁘다보니 운동을 게을리하기 쉽다는 점에서 ‘삶이라는 냄비 속 개구리’가 되는 느낌을 받음, 힘든 날에는 파트너가 “자전거 타고 오라”고 자주 조언해줌, 실제로 신선한 공기를 마시며 자전거를 타면 영혼은 물론 자신의 심장, 폐 건강까지 놀라울 정도로 좋아짐
          + 자전거를 타면 다른 어떤 것과도 비교할 수 없는 기분 전환 효과를 누릴 수 있다고 생각함, 마치 새처럼 나는 것에 가장 가까운 감각을 줌, 평소와는 다르게 동네를 새롭게 경험하게 되는 기분임
          + 모두가 올바른 식습관, 운동, 충분한 수면이 중요하다고 하지만, 나 역시도 한때 자기관리를 소홀히 했었음, 하지만 유산소를 늘리고, 체중을 줄이면서 삶 자체가 완전히 달라졌음
          + 나는 건강을 위해 꾸준히 운동을 하고 있음, 운동을 하면 실제로 에너지가 더 높아짐, 그런데 운동하는 도중이나, 운동 후 2~3시간 동안은 오히려 감정적으로 불안하거나, 부정적인 생각, 자기비판, 비관적인 기분, 심지어 화가 올라오는 경우도 있음, 확실히 운동의 신체적 이점은 경험하지만, 그 대가로 큰 대가를 치르는 느낌도 존재함, 다양한 운동 종류를 시도했지만, 일정 강도로 올리면 다 이런 정서적 반응이 나타남, 혹시 나처럼 운동이 오히려 감정적으로 힘든 사람이 행복하게 운동할 수 있는 방법을 찾은 분이 있다면 경험을 듣고 싶음
          + 그냥 운동 자체가 중요한 게 아니라 바깥에 나가는 전체 경험이 큰 효과를 준다고 생각함, 차를 타고 공원에 가서 30분 앉아 있기만 해도 집에 돌아왔을 때 기분이 더 좋아짐, 근육에는 큰 효과가 없겠지만, 자연 속에만 있어도 놀라운 효과가 있다는 것임, 오히려 헬스장에서 스피닝만 하는 건 이런 효과를 주지 못함
          + 자전거로 2시간만 달려도 내 뇌가 부정적인 생각에 빠지지 않고 도로에 집중하게 되기 때문에 슬럼프에서 벗어날 수 있음, 아주 가벼운 회복성 요가도 추천하고 싶음, 이는 매우 쉬운 운동이지만, 장시간 앉아있는 생활, 수면의 질 개선, 스트레칭, 스트레스 감소에도 효과적임
     * Daniel Lieberman 저자가 “운동은 건강에 좋고 보람차지만, 우리가 진화적으로 하도록 설계된 활동은 아니다”라고 말한 것이 인상적임, 인간은 유독 지칠 줄 모르고 오래 달릴 수 있는 타고난 능력을 진화적으로 갖추었음, 다른 동물과 비교해 가장 극한 환경에서 오래 달릴 수 있음, 인체는 달리기에 최적화된 기계이고, 에너지를 저장·방출하는 결합조직이 존재함, 땀을 통한 증발냉각 덕분에 꾸준히 1kW 이상의 열을 내보내면서도 달릴 수 있음
          + 한 번 다큐멘터리에서 사냥하는 부족의 방법을 보면서 이 사실을 깨달았음, 그들은 동물을 끝까지 지치게 할 때까지 추적하며 사냥함, 전력질주하지 않고 계속 조용히 동물의 흔적을 관찰하며 따라감, 몇 시간씩 뛰는 경우도 많고, 성공 못 할 때도 있음
          + “인류는 달리기에 최적화되어 있다”는 말에 플랫풋(평발)을 가진 나에게는 좀 다른 경험임, 실제로 뛸 때 발생하는 대부분 에너지는 열로 바뀌어서 상당히 비효율적임, 땀으로 식히는 능력은 좋지만, 신선한 물이 공급되어야 하며, 물이 없을 때 한계도 분명함
          + 인간이 주로 두 다리만 쓰기 때문에 달리기에서 느린 것 아닌가 하는 궁금증이 생김
          + 모든 군인에게 긴 기간의 러닝 훈련이 어떤 결과를 남겼는지 물어본다면 그리 낭만적이지 않은 대답을 들을 수 있음, 무릎 문제도 많고, 평생 러닝을 꾸준히 했던 집단일수록 나중에는 페이스 메이커(심박조율기)가 더 필요하게 됨
          + “우리는 무한히 달릴 수 있도록 진화의 자원을 써왔다”는 내용이 있는데, 이 아이디어는 Born to Run이라는 책에서 심도 있게 설명되고 있음, 정말 추천하고 싶음
     * 운동이 모든 원인 사망률(all-cause mortality)을 감소시킨다는 근거는 생각보다 더 복합적임, RCT(무작위 대조 시험)와 관찰 연구의 결과를 명확히 구분할 필요가 있음, 약 5만 명을 대상으로 한 RCT 메타분석에선 노인이나 만성질환자의 사망률, 심혈관 질환 발생률 감소 효과가 발견되지 않았음 [링크1], 하지만 고위험군, 예를 들어 암 환자·생존자에 한해서는 운동의 인과적 효과가 매우 강하게 입증됨, 다른 메타분석에서는 암 생존자는 사망 위험 24% 감소, 재발 위험 48% 감소 효과를 얻었음 [링크2], 대중적으로 인용되는 “운동하면 40% 사망률 감소” 같은 대규모 이득은 대부분 관찰 연구 결과이고, 이에는 건강한 사람이 운동을 더 할 수 있다는 ‘건강 선택자 편향(healthy user bias)’ 혹은 반대 인과(reverse causation) 문제가 있음, 즉, 건강한 사람들이 원래 더 운동을
       하므로 실제 효과를 입증하기 어렵다는 점임, 결론적으로 일반 인구에 대한 운동의 직접적 인과 효과는 암 생존자와 같은 특수 그룹만큼 확실하지 않음 [링크3]
     * 걷기 역시 운동임을 잊지 말아야 함, 무리하게 대단한 시도를 하려 하기보다는 일관성을 유지하는 쪽이 항상 이긴다는 경험을 하고 있음
          + 더 강도 높은 유산소나 근력 운동에서는 걷기로는 얻을 수 없는 신체적, 정신적 변화가 있음, 이런 운동법이 꼭 어렵거나 과장된 것도 아니라고 생각함
          + 일관되게 하는 것이 더 안전하고 오래 가는 방법임은 분명하지만, 현실적으로는 ‘평범한 성인이 매일 개 산책을 한다’거나, ‘주 2회 50kg 스쿼트 10회’처럼 자랑 혹은 인상 줄 거리가 되지 않음, 오히려 의사만이 40대 넘어가면 칭찬할 일임
          + 걷는 것이 하루 종일 앉아 있는 것보다는 훨씬 낫기는 하지만, 목표로 삼을 건 아님, 근력 운동과 더 강도 높은 유산소 운동이 오히려 더 중요하다고 생각함
     * 처음 취업하고 해고되었을 때 시간이 남아서 헬스장에 다니기 시작했음, 몇 년이 지난 지금도 습관이 붙어서, 오히려 해고된 것이 내 인생에 정말 큰 행운이었다고 생각함, 운동 덕분에 건강 문제도 없어졌고, 정신 건강과 수면 질도 크게 개선됨, 운동은 정말 기적의 약임, 단 습관이 되기까지가 어려운 게 아쉬움
     * 이 기사가 운동을 주제로 하지만, 제목 사진이 요가라서 흥미롭게 느껴짐, 기사 초반에 “유산소 및 근력운동이 신진대사와 미토콘드리아, 면역, 염증 개선, 질병 예방에 도움이 된다”고 쓰였는데, 요가는 유산소나 근력운동에 해당하지 않는 것 같음, 참고로 나는 한 달에 두 번 정도 요가를 하지만, 요가를 비하하려는 게 아니라 기사 내용과 사진이 동떨어진 것처럼 보인다는 점을 말하는 것임
          + 나는 여러 요가 스타일을 해봤고, 대부분은 체중을 지탱하고 움직여야 해서 꽤 강한 근력운동 효과를 제공함, 그리고 모든 요가가 유산소 운동만큼 심박수를 올려줌, 반복적으로 동작을 흘러가며 수행하면 꽤 격한 운동임, 물론 부드럽고 스트레칭 위주의 요가도 있지만, 힘을 들인다는 관점에서는 크로스핏에서 했던 버피나 맨몸운동과 별로 다를 게 없음, 만약 ‘운동=기적의 약’이라는 주장을 한다면, 적어도 내 경험에서 요가도 100% 해당됨
          + 요가는 스타일이 다양함, Yin Yoga는 비교적 정적이고, Ashtanga Yoga는 근력 비중이 높음, Hot Vinyasa나 Bikram은 본격 유산소 운동임, 즉 “요가 한다”고 말하는 건 “피자를 먹었다”는 표현만큼이나 여러 변화가 가능하다는 것임
     * 이번 기사는 낚시성(bait-and-switch) 느낌이 듬, 왜 운동의 장점과 미국의 대외원조정책을 두 개의 기사로 나누지 않았는지 의문임, 최소한 HN 게시글 제목이 기사 전체 내용을 제대로 반영하지 못하고 있다고 느낌
          + 더 심각하게는, 제목에서 제기된 질문에 실제로 답하지도 않음, 최근 연구 몇 가지를 소개한 다음, 전혀 관련 없는 주제로 넘어가서 심지어 ‘여담’이라고도 보기 어려울 정도로 문맥이 어긋남
     * “이것은 엄청난 도덕적 투자 수익이다”라는 문장에 의문이 듦, '도덕적 투자(moral investment)'란 무엇이며, 특히 정부가 타국을 위해 이걸 해야 할 이유가 뭔지 모르겠음
          + 아마도 ‘지구 전체가 더 건강해지면 미국 경제에도 장기적으로 긍정적인 효과가 돌아올 거란 기대’가 배경 아닐까 싶음, 즉, 직접적인 금전 투자상품은 아니더라도 넓은 시야에서 이익을 본다고 생각함
     * 오랫동안 운동 습관을 들이려 했으나 매번 실패했음, 매일 15분씩 조깅하면 며칠 만에 싫증나서 포기했음, 결국 내 기분과 불안 문제를 먼저 해결해야만 운동 습관이 붙었음, 정신적으로 먼저 나아진 뒤에야 자연스럽게 동기부여가 생겼고, 더 이상 운동을 싫어하지 않게 되었음, 덕분에 몇 주 이상 꾸준히 할 수 있었고, 피곤할 때를 제외하면 중단하지 않음, 즉 일반적으로들 이야기하는 인과관계가 내게는 반대였음, “운동→정신건강”이 아니라 “정신건강→운동습관”임, 습관을 만드는 데 정신적인 기반이 더 중요했음, 지금은 기분이 들쑥날쑥해도 한 번씩 조깅할 수 있음, 원래 저조한 상태에서 운동에 도전했다면 절대 습관화하지 못했을 거라는 걸 지금은 이해함
     * 건강하고 행복한 사람을 불행하고 병들게 만드는 가장 좋은 방법은 그들을 고립시키고 가만히 있게 하는 것임, 반대로 하면 행복과 건강으로 이어지는 것은 놀라운 일이 아님, 비슷하게 자원봉사도 그러한 만족감을 주는 이유라 생각함
          + “행복하고 건강한 사람을 불행하게 만드는 법=고립+정적인 상태”라는 말, 소프트웨어 개발자가 되는 일과 비슷하다고 생각함
          + “행복하고 건강한 사람을 불행하게 만드는 법=고립+정적인 상태”라는 말, 안타깝게도 팬데믹 동안 실제로 그런 일이 벌어졌음
"
"https://news.hada.io/topic?id=22237","야후는 왜 몰락했는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              야후는 왜 몰락했는가

     * 야후는 1990년대~2000년대 초반, 인터넷의 ""첫 번째 프론트 페이지""로서 압도적인 영향력을 가졌으나, 실패한 인수합병과 기회 상실로 2016년 Verizon에 인수되는 결말을 맞이함
     * 비수익성 트래픽까지 집착하며 Broadcast.com, Geocities, Tumblr 등 수익화 실패 사업에 거액을 투자하고, 반대로 Google, Facebook 인수 기회는 놓침
     * 알리바바 투자만이 대성공을 거뒀으나, 전체 경영 판단에서는 반복적인 실책과 우유부단함이 더 큰 손실로 이어짐
     * 2008년 마이크로소프트 인수 제안(약 446억 달러)도 거절했고, 이후 매각가치는 크게 하락함
     * 현재 야후는 일반인들에게 거의 잊힌 존재이며, 일부 서비스(야후 파이낸스 등)만 제한적으로 활용되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

How far Yahoo fell

     * 야후는 1994년 Stanford University에서 시작해 기업으로 성장함
     * Jerry Yang과 David Filo가 만든 ""Jerry’s Guide to the World Wide Web""에서 출발, 대학생들이 링크 모음 페이지로 자주 활용했던 사이트였음
     * 1996년 상장 당시 시총 10억 달러 돌파, 닷컴버블 전성기를 누렸으나, 버블 붕괴 후에도 Amazon, Ebay와 함께 살아남은 주요 IT 기업 중 하나였음
     * 초기 비즈니스 모델은 단순 디렉토리(포털)였으나, 검색엔진과 콘텐츠 서비스 확장으로 트래픽 기반 매출에 집중
     * 성공 요인은 이용자 트래픽과 다양한 콘텐츠 확장 전략에 있었으나, 하지만 이후의 전략적 판단 미흡이 쇠퇴의 원인으로 작용함

야후가 실패한 주요 원인 : 잘못된 인수

     * 회사의 수익원은 트래픽에 집중되어 있었고, 이를 위해 인기 사이트를 무분별하게 인수함
          + Broadcast.com, Geocities 같은 인기 있지만 적자였던 서비스를 대가 없이 통합하다가 100억 달러를 날리고, 결국 모두 철수 혹은 폐쇄
          + Tumblr 역시 적자였던 서비스를 11억 달러에 인수했지만, 광고 수익화에 실패해 결국 300만 달러에 매각함
     * 1997~2015년 동안 114건의 인수, 다수는 기능 통합에 기여했으나 구글과 같은 성공적 인수사례는 드물었음
     * 구글의 Youtube, DoubleClick 인수와 비교 해보면 구글은 지속 성장가능한 자산을 택했던 반면, 야후는 ""쓸모없는 인수""에 집중한 경향이 뚜렷함

야후가 놓친 두번의 기회

     * 야후는 Google과 Facebook이라는 두 초대형 기업 인수 기회를 놓침
          + 1998년 Google을 100만 달러에 인수할 기회를 거부함
          + 2002년 Google이 1억 달러를 요구할 때도 망설이다 가격이 오르자 거절함
          + 2006년 Facebook 인수도 10억 달러 제안 후 1억 달러를 더 올렸으면 가능했지만, 의사결정 지체로 무산됨
     * 결과적으로 Broadcast.com, Geocities, Tumblr 등에 투자한 금액보다 적은 비용으로 ""미래의 1조 달러 기업"" 두 곳을 놓친 셈

Microsoft 인수 제안 거절

     * 2008년 마이크로소프트의 446억 달러 인수 제안도 ""기업가치가 낮다""며 거절
     * 이후 실제 매각가는 48억 달러로 추락, 만약 당시 MS에 인수되었다면 전혀 다른 역사가 펼쳐졌을 가능성도 있음

야후의 유일한 성공적 인수: Alibaba 투자

     * 2005년 알리바바 지분 40%를 10억 달러에 인수, 이후 IPO와 지분 일부 매각 등을 통해 총 360억 달러 이상 이익 실현
     * 2016년 최종 매각 시 남은 알리바바 지분만 300억 달러의 가치
     * 그러나 이익을 기반으로 조직 체질이나 미래 성장 전략으로 연계하지 못한 부분이 한계로 남음

야후의 최후와 현재

     * 2016년 Verizon에 48억 달러에 매각되며 독립기업으로서의 야후는 막을 내림
     * AOL과 병합되었으나, 통합 후 실적 부진으로 2021년 Apollo Global Management에 50억 달러에 다시 매각됨
     * 현재는 예전만큼 대중적으로 사용되지 않으며, Yahoo Finance 등 제한적 활용만 이뤄지고 있음
          + 젊은 세대에게는 거의 사용되지 않는 구시대 인터넷 기업으로 남아 있음
     * 1990년대를 대표하는 인터넷 거대 기업의 인수합병과 사업 전략 실패 사례로 남게 됨

        Hacker News 의견

     * 1998년에 Yahoo가 Google을 100만 달러에 인수할 기회를 거절했음, 그 해에 1억 730만 달러를 6개 인수에 썼음, 2002년에는 Google이 10억 달러에 재매각 의향을 밝혔지만 Yahoo가 망설이다 가격이 30억으로 오르자 포기한 적 있음, 이후 Google은 1조 달러 기업으로 성장함, 2006년에는 Yahoo가 Facebook을 10억 달러에 인수하려 했지만 Zuckerberg가 거절했고, 단 1억을 더 올렸다면 매각이 결정됐을 뻔함, Facebook 역시 1조 달러 기업으로 성장함, 두 회사 모두 Yahoo에 인수됐다면 이런 기업이 되지 못했을 거라 믿음
          + Yahoo가 두 회사를 키우지 못했다고 생각하는 결정적 이유는 단기적 수익을 장기적 명성보다 더 중요하게 여겼기 때문임, 이 문제를 주도했던 Prabhakar Raghavan이 지금은 Google에 있는데, 이로 인해 검색 품질도 급격히 떨어졌음, 만약 Google이 Yahoo에 인수되었다면 그는 훨씬 일찍 Google을 망가뜨렸을 것임, 관련 링크 참고
          + Yahoo에서 근무했었는데, 인수 후에도 ""Yahoo 브랜드에 어떤 도움이 되냐""와 ""비용 절감을 위해 Yahoo 기술로 어떻게 이전하냐""만 강조했음, 사업 성장에는 전혀 관심이 없었음, Facebook을 인수했어도 Tumblr, Flickr처럼 질식시켰을 것임
          + Yahoo가 Facebook을 인수한 덕분에 Facebook이 지금과 같은 영향력 있는 네트워크로 성장하지 못하고 묻혀버렸다면 사회가 어떻게 달라졌을지 상상해보고 싶음, 현재의 정치적 분위기도 달라졌을지 궁금함
          + Viaweb이 Shopify가 되지 못한 것처럼, Yahoo가 인수한 Tumblr, del.icio.us, ROI 역시 제대로 성장시키지 못했음, 애초에 독립적으로 성장했어야 할 서비스였음
          + Yahoo가 Google을 제대로 성장시킬 수 있었다면 자신들이 직접 그런 기업이 되어야 했음, 하지만 그런 인물이 Yahoo에는 없었음, 결국 인수 이후에도 Page와 Brin 같은 사람이 회사 문화를 이끌지 못했을 것임, 구글 같은 DNA가 Yahoo에는 없었음
     * Yahoo가 한창일 때 FreeBSD에 큰 기여를 했던 시절이 그리움, 90년대 후반~2000년대 초반에 Yahoo는 FreeBSD에 엄청난 투자를 했고, 지금 Netflix가 FreeBSD에 하는 것 이상으로 기여했음, FreeBSD 빌드/테스트 인프라를 호스팅했고, 여러 핵심 개발자를 고용했으며, FreeBSD가 현대적 운영체제로 성장하도록 만들었음, SMPng 프로젝트, AMD64 포팅 등 큰 공헌을 했고 SMP kickoff 미팅 자료도 있음
          + 10여 년 전 Netflix의 FreeBSD 엔지니어 상당수가 Yahoo 출신이었음, Yahoo가 침체될 때 Netflix로 옮겨가 우수한 인재와 실험의 자유를 얻었고 혁신이 가능해졌음
          + Yahoo가 FreeBSD 사용을 계속했는지, 아니면 단순히 기여만 중단한 건지 궁금함
          + Yahoo는 Hadoop에도 관여함
     * Yahoo와 인도의 관계는 많이 알려지지 않은 이야기임, Verizon 시절 Yahoo에서 일했을 때 많은 핵심 운영을 인도 엔지니어 팀에게 이전했음, 미국 중간관리자 대비 임금이 훨씬 낮았지만 인도 인력이 실리콘밸리로 이주하며 급여가 올라가자 예상만큼의 절감 효과가 없어졌음, 전체 재정 악화의 주요 원인은 아니지만 일정 영향은 있었음, 흥미롭게도 일부 인도 엔지니어가 미국 학교의 문화 영향이 싫어서 자녀 교육을 위해 귀국하기도 했음, 점심시간에 이런 이야기를 자주 나눴음
          + 미국 문화가 자녀의 번영에 기여했는데도 그 영향을 싫어하는 이유가 아이러니하게 느껴짐, 결국 자녀가 새로운 환경에서 다시 배워야 하는 상황을 택한 셈임
     * Yahoo 경영진이 스프레드시트로 사업을 판단한 것이 문제였다고 생각함, 소프트웨어 산업 특성상 작은 팀이 큰일을 하는데 이런 식의 계산이 불가능함, 2008년쯤 검색 엔진에 10억 불 투자한다는 생각이 미친 짓으로 보였지만, 실제로 엔지니어 집단의 도전 가능성에 대한 논의조차 없었던 점이 아쉬움, Yahoo가 이길 수는 없었더라도 시도조차 하지 않은 점, 기술적 탁월함을 포기하고 미디어 회사로 전락한 점이 이해가 안 됨, 훌륭한 엔지니어링 인력이 넘쳐났던 시대에도 이런 결정을 내린 게 아쉬움
     * 10년 전까지만 해도 Yahoo에서 온라인 게임을 많이 했었음, 아내와 화상채팅하며 Yahoo의 다양한 온라인 게임(당구, Risk 등)을 함께 즐겼고, 한 계정으로 모든 서비스가 연동되어 편했음, 에초에 Yahoo의 웹 디렉터리는 새로운 웹사이트를 재미있게 발견하는 공간이었음, Google의 검색이 강력해지자 ""북마크가 필요 없다""는 인식이 생겼고, 그 와중에 새로운 발견의 재미가 사라졌음, Blockbuster에서 DVD를 고르던 무작위성의 즐거움이 검색엔진에서는 느껴지지 않음
     * 20년 전 Yahoo 출신으로 회사 하락의 원인은 명확하다고 봄, 수년간 자체를 기술 기업이 아닌 미디어 기업으로 구조화했고, 2000년대에는 기술적 비전을 전혀 모르는 할리우드 출신 임원과 MBA로 회사를 이끌었음, Yahoo의 최대 강점은 최고의 ""실용적"" 웹사이트라는 점이었음, 사용자의 시작 페이지, 이메일, 금융, 날씨, 뉴스, 스포츠, 지도, 캐주얼 게임, 포럼, 메신저, Q&A, 판타지 스포츠, 사진 등 온라인에 필요한 거의 모든 것을 제공하는 원스톱 서비스였음, 모든 분야에서 최고는 아니었지만 늘 괜찮았음, Google이 계속 제품을 시작하고 죽이는 동안 Yahoo는 비즈니스 모델을 복사하거나 인수해서 오래 운영했고, 사용자들이 이를 좋아했음, 이 모든 게 꽤 수익성이 있었지만 구글 만큼은 아니었음, GE의 ""시장 2위 혹은 3위라도 수익성만 있으면 된다""는 만트라에 딱
       맞는 회사였음, 제대로 이런 정체성을 밀었으면 지금도 relevance를 유지했을 것임, 하지만 회사 각 부서는 사일로화되어 내부 경쟁이 심했고, 경영진은 Google을 따라잡으려고 집착했음, 결국 Yahoo의 강점이었던 핵심 서비스를 소홀히 하면서 회사 전체가 혼란에 빠졌음, 유능한 리더십이 있었다면 웹 사용자에게 계속 유용한 회사를 만들 수도 있었지만 아쉽게도 그러지 못함
          + 90년대 후반에 Yahoo에서 Google로 검색 엔진을 옮겼던 기억이 남, Google이 훨씬 깔끔하고 빨랐고, Yahoo는 홈페이지가 항상 복잡하고 느렸음, Google의 ""Gooooogle"" 로고를 처음 레드햇 사이트에서 보고 궁금해서 방문해봤고, 아마 그 당시 Google이 Yahoo용 검색 엔진도 제공했던 것 같음
          + Hacker News 빼고는 여전히 Yahoo를 주요 뉴스 포털로 사용하고 있음
     * Yahoo의 몰락을 말할 때 Marissa Mayer 전 CEO의 이야기는 잘 안 나오는데, 그 시절 사내 문화가 굉장히 독성으로 바뀌었다는 얘기를 들었음, 그녀가 남성을 배제하고 차별하는 분위기가 있었고, 이것 때문에 소송도 있었음, Google에서 온 경력만 화려한 ""시니어 매니저""였지 회사를 이끌 역량이 없었음, Tumblr 인수만 봐도 감이 옴, 실제로 그녀가 수백만 달러를 챙긴 와중에 직원들은 신뢰를 완전히 잃었음
          + 세부 내막은 모르지만 Mayer가 왔을 때 이미 Yahoo는 하락세였음, Google이 성장하며 사용자들이 점점 Yahoo에 올 이유가 없어졌고, 그래도 검색 계약 덕분에 relevance를 유지했었음
     * Yahoo에 잠깐 근무했던 경험이 있는데, Yahoo가 Google이나 Facebook을 인수했다면 곧바로 망쳤을 거라는 확신이 있음
          + 그렇지만 만약 정말 인수했었다면 지금의 상황도 궁금해짐
     * Yahoo는 처음부터 구조적으로 크게 성장할 수 없는 회사였음, 본질적으로 특별하지 않은 상품을 가진 회사였고, 구글 같은 검색 엔진이 등장하자 금방 퇴색됐음, 인터넷 디렉터리는 기술과 비전이 크게 필요 없는 단순한 아이디어였고, 시대에는 맞았지만 금방 끝나는 운명이었음, 닷컴버블의 자본 덕분에 다른 제품을 많이 인수하고 잠깐 성장했지만, 구글이 하는 모든 분야에서 다 뒤처졌음, 검색, 메일, 뉴스, 비디오 등 모두 구글이 앞섰고, Yahoo는 인수를 통해 따라잡으려 했으나 매번 2위나 3위에 그쳤음, Marissa Mayer가 올 무렵엔 이미 구글이 완승한 상태였고, Yahoo는 다양한 것들을 어설프게 모은 대기업이었음, 여러 인수를 통해 Flickr, Tumblr 같은 좋은 서비스도 인수했지만 결국 품질 관리와 성장에 실패함, ""돈이 아무리 많아도, 초반이 평범하면 위대한
       회사가 될 수 없음""을 증명한 사례임, 비전 없이 운영하면 기회가 와도 흘려보낼 수밖에 없고, 설령 Google과 Facebook을 인수했더라도 망쳤을 거란 생각임
          + 지금 시대에 사람 손으로 큐레이션하는 인터넷 디렉터리는 엄청난 가치가 있을 것임, 여기에 최신 AI 답변 엔진 기능을 더하면, Yahoo는 ""평범한 회사""가 아니었을 것임, Yahoo의 문제는 큐레이션이 아니라 다른 데 있었음
     * 나는 아직도 Yahoo!에 핑을 보내며 네트워크 상태를 확인함
 ~ ping yahoo.com
 PING yahoo.com (74.6.231.20): 56 data bytes
 64 bytes from 74.6.231.20: icmp_seq=0 ttl=50 time=42.366 ms
 ^C
 --- yahoo.com ping statistics ---
 1 packets transmitted, 1 packets received, 0.0% packet loss
 round-trip min/avg/max/stddev = 42.366/42.366/42.366/0.000 ms

          + Yahoo 핑이 제일 빠른 건가 궁금해서 해봤더니 단순 ping 명령어로 비교하면 더 짧게 나오는 경우도 있음
          + Yahoo로 핑을 날려보니 Google보다 20배 느림
          + 30년간의 습관 때문에 항상 네트워크 테스트 시 Yahoo를 ping하는 게 몸에 밴 상태임
"
"https://news.hada.io/topic?id=22329","당신에겐 tmux가 꼭 필요하지 않을 수도 있음 : 개발 워크플로우에서 tmux를 대체하는 법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          당신에겐 tmux가 꼭 필요하지 않을 수도 있음 : 개발 워크플로우에서 tmux를 대체하는 법

     * tmux를 오랫동안 사용했으나, 최근에는 tmux의 복잡성과 한계(컬러 호환, 스크롤백, 마우스 복사, 프로토콜 미지원 등)에 회의감을 느낌
     * 세션 지속성(detach/attach)이나 윈도우 분할/관리 등 tmux의 주요 기능이 꼭 tmux로만 가능한 것은 아님
     * dtach, abduco, shpool 같은 Unix 철학의 경량 툴들을 활용하면, 세션 관리만 집중하면서도 네이티브 스크롤백, 단순성을 확보할 수 있음
     * 특히 shpool + ssh 조합을 통해 여러 원격 세션을 윈도우 매니저로 직접 관리하고, 네이티브 기능(알림, 스크롤, 타이틀 등)을 그대로 쓸 수 있는 환경 구축 가능
     * 완벽하진 않지만, 본인 기준으로는 tmux를 완전히 대체할 수 있었고, 단순하고 유지보수가 쉬운 워크플로우로 만족감을 얻음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

tmux의 장점과 단점

     * 세션 유지(detach/attach), 윈도우 관리(탭, split) 등, 기존에 tmux가 제공하던 기능이 워크플로우 핵심이었음
     * 하지만, 올바른 TERM 설정이 없을 경우 색상 렌더링 문제, 터미널과 tmux 간의 상호작용 고려 필요 등 복잡도가 증가함
     * 스크롤 버퍼 사용도 tmux만의 별도 방식에 익숙해져야 하며, 마우스를 이용한 영역 복사도 splits 환경에서 불편함이 있었음
     * kitty graphics protocol 등 신규 터미널 기능 지원 미흡, 실험적 프로토콜 미지원 이슈도 존재함
     * 중복된 escape code 재해석과 세션/윈도우 개념 부여로 인해 멀티플렉서가 터미널 생태계의 발전을 저해한다는 비판도 있음

tmux의 대체 방식 탐색

     * 세션 지속성:
          + 단순하게 ctrl-z + fg, nohup, disown 등도 있지만 완전한 대체는 어려움
          + 세션 유지만을 목표로 한 여러 툴(dtach, abduco, shpool)이 등장
               o fork() 및 UNIX 소켓 조합으로 daemon과 클라이언트 간 연결
               o tmux와 달리 가상 split 없이 네이티브 스크롤백 지원, 버퍼 복원 기능도 일부 제공
          + 사용 경험상, 대체 툴 대부분은 버그 및 nvim 내에서 detach 키합이 동작하지 않는 등 완성도가 낮음
          + shpool만이 detach/attach 커맨드와 keymap 커스터마이징 측면에서 가장 완성도가 높았음
     * 윈도우 관리:
          + 로컬에서는 윈도우 매니저로 분할·배치 관리
          + 원격(SSH) 환경에서도 ssh_config와 shpool을 조합해, 여러 세션을 별도의 창에서 독립적으로 유지 가능
          + autossh와 결합해 네트워크 재접속 환경에서도 세션 유지

새로운 워크플로우

     * 개인적으로 ghostty(노트북), sway+foot(개인 PC)로 윈도우 관리. 서버는 프로머스(Proxmox) 기반 headless VM, 항상 SSH 개발 환경 유지
     * 여러 shpool 세션을 ssh 단축키로 자동 연결, 로컬 윈도우 매니저에서 독립적 제어
          + ssh_config에서 각 호스트별로 shpool 세션 attach 자동화
          + 터미널마다 IRC, dotfiles 관리, 별도 neovim 환경 등 다중 세션 개별 접속 가능
     * 네이티브 스크롤, 알림, 터미널 타이틀 등 tmux가 불안정하게 지원하던 기능들이 오히려 더 자연스럽게 동작
     * 단점도 있음: vim 재접속시 터미널 상태 복구 지연, nvim 사용 시 리사이즈 문제 존재
          + 멀티플레이어 미지원(여러 클라이언트에서 autossh 동시 발생 시 세션 충돌)
     * 그러나 개인 기준에서는 tmux 완전 대체 성공

결론

     * 완전히 동일하지는 않지만, tmux의 복잡성과 한계를 벗어나 단순하고 유연한 세션 관리 워크플로우로 전환 가능
     * 각자 워크플로우에 따라 shpool 등 대안 툴을 고려해볼 만함

   요즘은 AI로 tmux 같은 걸 찍어서 써요. Xterm.js + react + electron 으로 3-4시간이면 저런거 100개 도 찍어내고 필요하면 실시간 수정 해서 쓰고 샤브샤브 먹는 느낌? 먹고싶은거 넣으면 소스에 듬뿍

   wezterm으로 편하게 사용하고 있습니다

   Screen 쵝오!

   cargo install shpool...그냥 tmux 쓸게요...os마다 패키지가 다 있어야 해요!

   Zellij라는 걸출한 신예가있어요. 강추합니다.

   Zellij - 개발자 & Ops를 위한 터미널

   이것도 꽤 오래전에 올렸었네요 ㅎㅎ

        Hacker News 의견

     * 이 글은 Linux-on-the-Desktop 사용자들을 위한 내용이지만, tmux는 MacBook에서 iTerm2와 함께 사용할 때 진정한 빛을 발함을 강조하고 싶음
       iTerm2의 tmux 통합 기능 덕분에 내 워크플로우에서 tmux가 완전히 자연스럽게 녹아듦
       ~/.ssh/config에 아래처럼 설정해두면, 맥북을 다시 켜거나 네트워크를 바꿔도 언제든지 ssh tmux로 원격 개발 서버에 쉽게 접근 가능함
       iTerm2와 tmux 통합 덕분에 원격 tmux 탭과 스크롤 버퍼가 마치 로컬 탭처럼 동작하며, 사실 tmux 명령어도 알 필요가 없이 쓸 수 있음
          + 나는 모든 개인용 장비에는 Linux를 쓰고, 일할 때 주로 MacBook을 쓰는데, tmux는 플랫폼 독립적으로 동일한 환경을 재현해 줘서 정말 유용함
            Linux에서는 Alacritty를 쓰지만, macOS에서는 알맞은 설정이나 창/스크롤 관련 차이점 때문에 항상 뭔가가 바로 안 됨
            이런 OS 차이에 시간을 쓰고 싶지 않으니, tmux와 iTerm2 조합으로 Linux와 거의 동일한 경험을 얻는 것이 좋음
            세션 내 스크롤백이 완전히 별개로 제공되는 점도 나에겐 장점임
            tmux에 대한 비판은 오히려 터미널 개발자에 가까운 이슈라고 생각함
            지원이 부족하면 그저 더 좋은 지원을 해주는 터미널로 옮기면 됨
          + 나는 mosh와 screen 조합을 사용 중임
            서버가 재부팅되어도 세션을 복원할 수 있고, 네트워크를 바꿔도 세션이 끊기지 않음
            참고: Immortal SSH Sessions
          + tmux를 수십 번 시도해봤지만, 별로 마음에 들지 않아 항상 screen으로 돌아갔음
            하지만 이번 팁을 보고 다시 한번 시도해보고 싶은 마음이 생김
          + 나는 vim, tmux, iTerm2 조합에서 색상, 폰트 문제에 자주 부딪혀서 결국 로컬에서는 tmux를 포기하게 됨
            업데이트에도 살아남고, 세션 유지 덕분에 얻는 이득이 크진 않았음
            폰트 문제만 해결되면 다시 시도할 의향은 있지만, 지금은 시간이 부족함
          + Linux 터미널 중에 iTerm2처럼 tmux와 통합 기능을 제공하는 게 있는지 궁금함
            아직도 GNU Screen을 사용 중이지만, tmux를 다시 한번 써볼 의향이 있음
     * 이 블로그 글을 보며 왜 tmux를 쓰는지 다시 한 번 깨달음
       사람들이 tmux와 비슷한 워크플로우를 구현하려고 엄청나게 많은 작업을 해야 하는 걸 보니, 그냥 tmux 쓰는 게 훨씬 나음
       가끔 복붙이 살짝 불편하긴 해도 별로 신경 쓰지 않음
       '멀티플렉서가 쓸데없는 오버헤드를 유발한다'는 비판은, 코드베이스 유지자가 아니라면 신경 쓸 필요가 없음
          + 복붙 문제는 tmux만의 문제가 아님
            vim처럼 터미널 전체를 직접 그리는 앱은 모두 발생할 수 있음
            OSC52라는 터미널 이스케이프 시퀀스로 이 문제를 쉽게 해결할 수 있음
            아래 간단한 파이썬 스크립트를 쓴다면 grep 결과 등도 시스템 클립보드로 바로 보낼 수 있음
            tmux와 nvim은 이미 OSC52를 지원하며, 다른 곳에서도 쉽게 써먹을 수 있음
          + 참고로 내가 tmux에 PR을 제출한 적 있는데, 메인테이너 Nick Marriott가 소통이 정말 좋아서 마음에 들었음
            코드도 잘 정돈되어 있고, tmux가 더 좋아졌음
          + 뭔가 없는 문제를 억지로 만드는 듯한 논의에 질림
            저자가 말하는 이유들이 납득이 안 되고, 오히려 그의 ""7년 이상 tmux 사용 경력""도 의심스럽게 느껴짐
     * 몇 주 전에 Tmux를 알게 되었고, 프로그래밍적으로 특정 패인에 키 입력을 보낼 수 있는 스크립트 가능성에 매료되었음
       일본 포럼에서 영감을 얻어 Claude Code(이하 CC)가 상호작용이 필요한 CLI 스크립트와 직접 상호작용할 수 있을까 고민했고, Tmux를 이용해 이를 구현할 수 있음을 확인함
       그래서 claude-code-tools라는 작은 도구를 만들어, CC 같은 CLI 에이전트가 Tmux 패인을 생성하고, 그 안에서 스크립트를 실행하며 실시간 상호작용까지 할 수 있게 되었음
       일종의 Playwright나 Puppeteer의 터미널 버전 같음
       claude-code-tools 깃허브 링크
       이 방법으로 CC가 상호작용이 필요한 CLI 스크립트를 자동 테스트하거나, 다른 패인에서 UI를 띄우고 Puppeteer MCP로 브라우저에서 테스트, 디버거 활성화 후 브레이크포인트 잡기, 멀티 에이전트 연동 등 다양한 응용이 가능함
       이런 확장성을 tmux 대안 툴들도 구현할 수 있을지 궁금함
          + 참고로 screen도 ""stuff""라는 명령어로 패인에 프로그래밍적으로 키 입력을 보낼 수 있음
          + 이 방식으로 Claude Code가 Gemini CLI, OpenCode 같은 다양한 CC 인스턴스를 interactive 모드에서 제어할 수 있음
            전통적인 서브에이전트와는 다른 방식의 연동이 가능함
          + tmux-cli 래퍼가 실제로 결과 향상에 도움이 되는지 궁금함
            나는 Claude에게 기존 tmux 명령어를 쓰라고만 하면 잘 동작해서, 따로 Bash를 쓰지 않고 모든 명령을 tmux 세션에서 실행하게 하고 있음
          + 이런 걸 터미널 수준에서 더 쉽게 하고 싶다면, Kitty의 remote control 기능도 멋짐
            Kitty Remote Control 소개
          + 터미널은 본질적으로 파일 디스크립터일 뿐이라 script(1), expect(1), chat(8) 같은 유사한 툴이 80년대부터 있었음
            꼭 tmux가 필요한 건 아님
     * tmux에서 TERM 설정을 제대로 하지 않으면 색상 등이 제대로 표현되지 않는다는 문제는 모든 터미널 에뮬레이터/프로그램에서도 동일하게 발생하는 문제임
       Unix, Linux 시스템에서 TERM과 terminfo/termcap 개념 자체가 본질이라, 올바른 설정이 필수임
       이것은 tmux 고유의 문제가 아님
          + 하지만 저자는 잘못을 tmux에 돌려서 아쉬움
            스크롤 문제 역시 tmux 자체의 문제인지 의문스럽고, tmux는 alternate screen buffer를 사용하는데, 이는 뷰포트 기준선을 명확히 해서 커서 움직임을 쉽게 하려고 도입된 것임
            현대 터미널 대부분은 alternate screen 상태에서도 로컬 스크롤을 허용해서 이 특성이 제대로 안 지켜짐
          + 대부분의 프로그램도 기본 설정을 제대로 하지 않으면 의도대로 동작하지 않음
            tmux의 경우엔 바로 '왜'가 드러나지 않아 불편하지만, 그 해결책이 뭔지는 불명확함(예: 기본값을 256colors로 할지?)
          + 맞는 말이지만, tmux를 쓸 때는 한 단계 더 신경 써야 할 레이어가 추가됨
            tmux 안에서는 tmux 전용 TERM을 써야 하고, 바깥에서는 자기 터미널에 맞는 TERM을 써야 함
            tmux FAQ만 봐도 display 문제의 대부분은 TERM 미설정에서 기인함
     * ""멀티플렉서가 쓸데없는 오버헤드를 준다""는 지적은 오히려 내가 tmux를 선호하는 이유임
       termcap 지원을 포기하는 앱이 늘어나는 현재 상황에서, tmux가 있으면 내 VT520 같은 옛날 터미널에서도 여전히 앱 실행이 가능함
       어떤 터미널 개발자가 tmux를 싫어하든 별로 신경 안 씀
       궁극적으로 유저 입장에서는 아키텍처적 우아함보다 잘 동작하는 게 중요함
       참고로 개인적으로는 alacritty가 더 낫다고 생각함(하지만 주로 Konsole 사용 중임)
          + VT520을 갖고 있다니 부럽다는 말을 전하고 싶음
            오래전부터 VT525를 구하고 있었지만, 가격도 비싸고 해외 판매자들이 대부분임
            genuine VT525에선 nosh realizer가 잘 동작하는지 늘 궁금했음
            tmux가 없어져도, 미묘한 TERM 미호환 앱은 다른 방법으로 transliteration할 수 있음
          + 내 VT420은 하드웨어 플로우 제어를 지원하지 않아 소프트웨어 플로우 제어를 써야 하는데, 이 방식은 요즘 터미널 앱과 호환이 영 좋지 않음
            GNU Screen이 아주 훌륭한 해결책이 되고 있으며, 이 외에도 다양한 기능이 있음
            tmux는 최신 에뮬레이터에서 많이 썼지만, 구형 단말에서는 이 중요한 플로우 제어 기능이 부족해 보임
            termcap/terminfo 미지원 앱 처리를 위해 tmux가 유용하다는 점에 동의함
          + alacritty가 더 낫다고 생각하는 이유가 궁금함
            오래된 하드웨어에서는 시작 속도도 느리고, 탭 기능 등 실용적인 기능도 부족하며 벤치마크마저 kitty/ghostty/konsole/foot보다 딸림
            그럼에도 불구하고 뭔가 이유가 있어서 alacritty를 쓰는 사람들이 있다는 점이 흥미로움
     * 아직도 tmux를 쓸 계획임
       여러 프로젝트간 세션 관리 및 재부팅 후 세션 복구가 쉬워서 좋아함
       mouse copy/paste는 tmux-yank로 전혀 문제없었음
       몇 년째 같은 환경을 쓰고 있음
       tmux의 send-keys 기능으로 dotfile을 리팩토링할 때 여러 패인/세션에 동시에 alias나 설정 파일을 쉽게 갱신하고 nvim도 재시작하는 자동화를 구현함
       이 경험을 블로그와 영상에서 정리함
     * 나는 ""you might not need tmux""를 ""you might not need browser tabs"" 논의와 비슷하게 봄
       터미널 세션이나 웹페이지가 한두 개라면 굳이 안 쓸 수도 있지만, 그 이상을 다루려면 창관리가 너무 불편해져서 결국 같은 기능을 반복 구현하게 됨
          + 요즘 나는 그냥 내가 사용하는 윈도우 매니저의 창 관리 기능을 최대한 활용하려고 함
            브라우저 탭이나 터미널 탭, tmux 대신 윈도우 매니저에 창 배치와 탭 관리, 북마크 등을 맡기니 불편하지 않음
          + 탭 관리는 애플리케이션이 아니라 윈도우 매니저가 맡는 것이 맞다는 신념에 가까워짐
            내 윈도우 매니저는 창을 타일로 배열하거나, 겹쳐진 창을 탭으로 그룹화할 수 있음
            이 탭들은 동일 앱이든 서로 다른 앱이든 무관하게 쓸 수 있음
          + 브라우저 탭은 서로 다른 창으로 쉽게 분리하거나 합칠 수 있지만, tmux 탭은 이 점에서 유연성이 부족함
            좋은 윈도우 매니저라면 탭과 창을 자유롭게 재조합할 수 있음
          + 내 tmux 설정에는 클릭 가능한 탭이 있음
          + MS Windows의 다중 창 관리(Alt Tab, Win Tab 등)는 매우 훌륭함
            각 터미널마다 아이콘/배경색을 다르게 해서 식별하고, 운영체제 자체가 창 관리에서 많은 부분을 대신 해줌
            Mac도 써봤지만 이 부분에서 Windows만큼 만족스럽진 않음
     * 버퍼 스크롤백 등 tmux에 대해 불평하는 점들이 오히려 나에게는 tmux를 쓰게 하는 동기임
       두 번째 랩탑이 오래된 debian 터미널 전용이라 마우스가 안 돼서 오직 tmux로만 복사/붙여넣기가 가능한 상황임
       나에게 tmux는 대체가 불가능함
          + tmux capture-pane - | vim - 조합이 마우스 스크롤 휠보다 더 편리할 때가 있음
     * suckless 진영은 터미널을 설계할 때 tmux 같은 기능을 일부러 구현하지 않겠다는 정반대 접근을 택함
       참고: st.suckless.org/goals
          + suckless와 Kitty는 개발철학 면에서 정반대라고 할 수 있음
     * Kitty는 터미널 에뮬레이터의 미래를 이끌 수 있을 것 같아 성공하길 바람
       하지만 나는 Kitty를 업무용으로 쓸 수 없고, tmux가 없으면 세션, 창배치, 상태, 복사 버퍼, 스크롤백 모두 날아가서 절대 포기 못함
       적절한 대체제가 Windows에 나오기 전까진 옆에 Kitty를 쓰더라도 업무에선 tmux를 계속 쓸 계획임
          + Kitty도 잠시 써봤는데, tmux 설정과 비슷하게 환경을 재현할 수 있어 인상적이었음
            그러나 결국 tmux와 기본 터미널 조합으로 돌아옴
            어디서든 잘 동작하고, 툴을 조합하는 방식이 더 오래 살아남는다는 점에서 tmux가 여전히 최선임
"
"https://news.hada.io/topic?id=22267","Show GN: 내가 가본 우리나라 (지도 색칠)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Show GN: 내가 가본 우리나라 (지도 색칠)

   국내 여행을 좋아합니다.

   번거롭게 공항에 가서 비행기를 타지 않아도 되니까.
   자동차나 기차로 편하게 이동할 수 있는 게 좋아서.
   어릴 때부터 이름만 들어왔던 두근두근한 지역에 가보는 경험.
   서울에서만 자랐던 제가 전혀 몰랐던 지방도시의 삶을 알게 되는 것도 좋았습니다.

   코로나 이후로 해외 여행보다 지방 소도시 여행을 많이 다녔습니다.
   이곳저곳을 다녀보면서 다녀온 곳을 기록하고 싶다는 생각을 했습니다.
   다녀온 도시들을 지도에서 색칠하는 서비스.
   이미 만들어진 여러 서비스를 사용해봤는데 사용성이 떨어지고 광고들이 불편하더라구요.
   그래서 깔끔하게 원하는 기능만 만들어봤습니다.

   제가 쓰려고 만들었습니다만 좋아해주는 다른 분들이 있다면 기쁠 것 같습니다.
   이 서비스로 인해 더 많이 국내여행을 즐기고 새로운 지역을 발굴하는 기쁨을 느낄 수 있다면 뿌듯할 것 같네요.

   커피한잔 만드신 분이군요 ㅎㅎ
   화이팅입니다! 필요에 의해서 만드는 모든 것들이 좋은 서비스 같습니다.

   응원 고맙습니다. 😀

   일본 현 색칠은 ""경현치""라는 서비스도 있더라구요

   알려주셔서 감사합니다. 일본지도도 재밌어 보이네요!

   와 작년에 너무 필요해서 만들어 볼까 하다가 생각만 했는데, 너무 좋네요..

   좋아해주시니 기쁘네요. 😄

   독도 찍어주세요

   독도는 울릉군에 포함되어 있습니다. 모든 섬들을 다 표현하면 지도 UI/UX에 부작용이 따라서... 양해를 부탁드려요. ㅎ

   잘 만드셨어요. 응원합니다..
   한가지 아쉬운 점은..
   조금더 자세한 행정구역으로 나누면 더 좋지 않나 생각됩니다.

   너무 자세한 행정구역으로 나뉘면 지도가 떡이 되어버려서... 사용성이 너무 떨어지게 되는 문제가 있습니다.

   저는 세계의 안개라는 앱으로 갔던 경로들을 보고 있어요. 생각보다 안가본 도시들이 많다는 걸 느꼈어요

   다른 지인에게도 이 앱을 추천받았는데, 좋아보입니다!
"
"https://news.hada.io/topic?id=22229","블루팀과 레드팀 LLM에 대한 Tao의 의견","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        블루팀과 레드팀 LLM에 대한 Tao의 의견

     * Terence Tao는 사이버보안 분야에서 블루팀과 레드팀 구분의 논리적 중요성 언급
     * Constructive logic(구성적 논리) 는 블루팀, co-constructive logic(공구성적 논리) 는 레드팀 원칙을 각각 대변함
     * Mike Shulman은 두 논리 기반을 결합한 새로운 논리 연구 진행
     * Brouwer–Heyting–Kolmogorov(BHK) 해석은 증명 중심이나, 반증의 중요성도 강조
     * 이러한 연구는 AI 안전성 등 다양한 분야에 적용 가능성 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

블루팀과 레드팀 LLM 논리의 구분과 결합 논의

     * Terence Tao는 최근 보안 및 알고리듬 분야에서 블루팀(방어)과 레드팀(공격) 의 차별성에 대해 논리학자들이 더 깊게 고민 중임을 언급함
     * Constructive logic(구성적 논리)은 검증 과정, 즉 어떤 진술을 증명하는 과정에 초점을 맞추며 블루팀의 원칙을 규정함
     * 이에 반해, co-constructive logic(공구성적 논리)은 반증 과정, 즉 반박이나 공격에 관한 논리로 최근 주목받고 있으며 레드팀의 원칙을 담음

Mike Shulman의 논리 결합 연구

     * Mike Shulman은 이 두 논리 체계를 결합하는 형태의 논리를 연구 중임
     * 그의 논문에서 인용한 내용에 따르면, 기존의 Brouwer–Heyting–Kolmogorov(BHK) 해석은 증명 기준에만 중점을 두는 경향이 있지만, 실무 수학자들은 반증, 즉 어떤 명제가 거짓임을 식별하는 기준도 그만큼 중요하다고 판단함
     * 이를 통해, 기존 논리 해석에서 증명 중심 사고방식이 가지는 한계를 지적하고 있음

논리 해석의 확장 필요성

     * 논리적 접속사에 대해 증명과 반증이 각각 무엇을 의미하는지 양쪽 입장에서 모두 설명할 필요성이 제기됨
     * Mike Shulman의 진행 중인 연구는 이런 확장 해석이 실제로 어떤 구조를 가질지 탐구함

시사점 및 응용 가능성

     * 위와 같은 결합 논리 연구가 진행된다면, AI 안전성 설계나 사이버보안 분야의 알고리듬 검증 및 반증 체계 발전에 실질적으로 활용 가능성 높음
     * 관련 논문의 상세 내용은 arXiv 링크에서 확인 가능함

        Hacker News 의견

     * 몇 가지 생각이 있음
       (a) ""레드""와 ""블루"" 팀 모두에서 AI가 유용함
       블루 팀은 일종의 브레인스토밍 역할임
       (b) AlphaEvolve는 이런 의미에서 '레드/블루 팀' 접근법을 명확히 적용한 사례임. 다만 그런 용어를 쓰지는 않음
       Terence Tao가 그 논문의 자문이기도 했음
       (c) 이것은 게임 의미론에서의 '검증자/반증자' 역할 분담을 떠올리게 함
       Tao 본인도 공개적으로 이런 사고방식에 대해 언급했으니, 실제로는 이런 시각에서 보고 있을 것 같음
       ""블루/레드"" 표현은 아마도 프로그래머에게 맞춘 포장임
       (d) 덧붙여보면, 보안 시스템이 항상 가장 약한 고리만큼만 강하다고 할 수는 없음
       보안이 계층적으로 되어 있는지, 병렬 구조인지에 따라 다름
       여러 개의 강한 문과 약한 문이 일렬로 있는 복도라면, 전체 강도는 가장 강한 문의 강도에 의해 결정됨
       그리고 여러 개의 약한 분류기를 조합해서 사기 탐지 알고리즘을 만들면, 오히려 가장 약한 분류기보다 훨씬 더 강력해질 수 있음
       AlphaEvolve 논문
       Tao의 사고방식 관련 Q&A
          + AlphaEvolve의 LLM이 어떻게 레드 팀 역할을 하는지 의문임
            거기서 LLM이 하는 일은 단순히 예시를 바탕으로 새 코드를 생성하는 것 뿐이고, 코드 평가 자체는 하지 않음
     * 레드 vs 블루 팀 개념이 LLM이 전문가용으로 어디까지 쓸모 있는지 이해하는 좋은 틀이라고 느꼈음
       테스트 추가는 거의 망설임 없이 LLM에 맡기겠음
       이유는 테스트가 보통 저렴하고, 잘못됐으면 쉽게 삭제하거나 수정 가능하며, 제대로면 가치 추가임
       단, LLM이 핵심 기능은 자주 테스트하지 않으니 가장 중요한 테스트는 직접 써야 신뢰할 수 있음
       반면에 LLM으로 버그 고치거나 기능 추가는 더 위험함
       LLM이 꼼수를 쓴다거나, 테스트 통과만을 위해 본질은 해결하지 않은 코드를 쓸 수 있기 때문임
          + 레거시 코드베이스에서 일하면서 ""테스트는 틀려도 나중에 고치면 된다""라는 생각은 위험하다고 절실히 느낌
            테스트가 코드보다 더 진실에 가까운 근거가 되기도 하니, 잘못된 테스트는 잘못된 코드보다 더 치명적일 수 있음
            특히, 쓸모없는 테스트나 의미 불분명한 테스트가 섞이면, 그게 실제로 중요한 기능을 보장하려고 있는 건지 구별하는 게 최악임
          + AI는 계산기와 비슷하다고 생각함
            계산기가 대부분의 사람이 할 수 없는 계산을 잘 하듯, AI도 새로운 종류의 지능으로서 인간을 강화해주는 보조도구임
            많은 이들이 ""AI가 인간을 대체한다""고 생각하지만, 실제로는 인간 업무를 보완하는 데 진짜 가치가 있음
          + 나는 반대 입장임
            테스트는 직접 작성하고 완전히 이해해야 진정한 기준을 세울 수 있고, 그 후 AI가 마음대로 코드를 바꿔도 안심할 수 있다고 생각함
            테스트가 LLM이 만든 것이라면 오히려 나머지 코드에 AI에 맡겼을 때 불안함이 더 커짐
          + Rust 코드에 대해 LLM으로 테스트를 만들어봤는데, 실제로는 쓸모보다 해가 더 컸음
            테스트 수는 많았지만, 중요한 범위는 빠지기 쉬웠고
            코드 양이 너무 많으니 어떤 부분이 커버되지 않는지 확인하기 힘들었음
            미래에 코드 로직을 바꾸려면 생성된 많은 테스트 전부를 손봐야 했던 경험임
          + ""테스트는 아무도 검증하지 않으니 당연히 맞다고 여긴다"" 라는 말이 있음
            그래서 Arrange-Act-Assert 패턴이 나온 것임
            요즘 가장 좋아하는 유닛 테스트는, 입력 값과 기대 출력을 저장해두고 그걸로 코드 결과를 검증하는 방식임
            구석 케이스까지 손쉽게 검증 가능하니 실제로 원하는 대로 동작하는지 확인이 쉬움
     * 내 이해로는, RSA 알고리즘도 이렇게 만들어졌다고 앎
       Simon Singh의 ""The Code Book""에 따르면(어딘가 두었는데 못 찾겠음), Rivest와 Shamir가 아이디어를 내고 Adleman이 결함을 찾아주는 역할이었음
       RSA 위키피디아 참고
       수학에서도 블루/레드 팀의 콜라보 예시임
          + 내가 아는 인지과학자 두 분도 비슷함
            한 명은 아이디어가 많고 말이 많고 두서가 없음
            다른 한 명은 논리적이고 정밀해서, 한 명이 논문 초안을 쓰면 나머지가 쓸데없는 부분을 다 삭제해줌
     * 큰 틀에서는 동의하지만, 인포섹(정보보안) 관점의 프레이밍은 좀 이상하게 느껴짐
       ""보안의 강도는 가장 약한 부분만큼""이라는 시각은 너무 단순하고 위험함
       보안 전략은 다층으로 짜야 함
       단일 계층에서 완벽을 기대할 수 없으니 여러 방어 계층이 필요함
       공격과 방어가 큰 차이는 없으나, 많은 공격자는 실수해도 책임이 적고, 방어자는 대기업일수록 결과에 대한 책임이 큼
       하지만 방어자는 홈 그라운드에서 싸우는 이점이 있음. 이걸 놓치면 곤란한 상황 올 수 있음
          + 문 닫아놨더라도 창문 열려있으면 소용없다는 예시는 맞는 비유임
            여러 계층 방어란 건, 공격자가 목표에 도달하기까지 반드시 거쳐야 하는 요소들이 여러 겹으로 쌓여 있을 때 의미가 있음
            근데 문과 창처럼 한 층위에 있는 요소라면 가장 약한 부분이 전체를 결정함
            웹 예로, 메인 로그인은 완벽하지만 /v1/legacy/external_backoffice 같은 오래된 엔드포인트는 아무 인증 없이 내부망에 접근 가능하다면 전체 방어는 무너짐
            그래도 내부적으로 더 막을 장치가 있으면 피해가 제한되니, 결국 방어 계층은 중요함
          + ""방어는 가장 약한 고리만큼 강하다""는 말을 좀 더 포괄적으로 쓴 것임
            원글의 표현을 넘어서 '방어 노력 전체'란 단어를 추가했는데, 실제로는 두 입장이 모두 일리 있음
            Terence 말처럼, 가장 약한 고리가 실제로 뚫릴 수 있고, 그래서 여러 방어 계층이 필요함
            최근 실제 사례로 클라이언트 비밀번호를 검증 없이 초기화해주는 헬프데스크 담당자 문제도 있었음
            VPN, 2FA 등 튼튼한 보안기술이 도입돼도, '계정 복구'라는 가장 약한 고리 하나가 전체를 무너뜨렸음
            내부적으로 추가 계층이 탐지, 차단해 3시간 만에 침입자를 막았지만, 그건 '피해 최소화'지, '사전 방지'는 아님
            결국 여러 계층이 있어야 피해가 막을 수 있음
            최근엔 인포섹 업계 자체가 ""100% 예방""에서 ""피해 완화""로 초점이 바뀜
            모든 위험을 예방하기 어렵고, 차라리 위험을 줄이고, 표면을 최소화하고, 신뢰 수준을 낮추는 식으로 전략 방향이 바뀜
          + 전혀 보안 전문가가 아니지만, 듣기로는 ""극도로 축소된 공격표면적, 검증된 오픈소스 프로토콜 활용""이 최고의 방어라고 들었음
            여기서 논의된 내용과 어떤 차이가 있는지 궁금함
          + 단순히 유추가 적절히 선택되지 않은 것임
            ""가장 약한 고리""라는 말이, 일렬로 여러 단계를 뚫어야 할 때엔 맞는데
            한 층위에 여러 겹이 있다면 그중 제일 약한 부분을 노리는 게 맞음
            역시 우려처럼 애매하게 해석될 여지가 있긴 함
          + 공격도 또 다른 방어 계층에 해당한다고 봄
            '최고의 공격은 최상의 방어'라는 말처럼
     * 사이버시큐리티에서 레드 팀과 블루 팀은 동등한 힘을 가진 두 팀이지만
       소프트웨어 개발에선 비유가 조금 과장이라고 느껴짐
       테스트도 코드고, 버그가 남음
       마치 경관을 누가 감독하냐는 ""Who polices the police?"" 같은 역설이 발생함
          + ""Police police police police police police police.""
            버팔로 문장처럼 반복되는 의미심장한 영어 문장
     * John Cleese의 ""열린 모드 vs 닫힌 모드"" 멘탈 이야기가 생각남
       아이디어는 최대한 열린 자세에서 다양하게 내고
       나중에 닫힌 모드로 나쁜 아이디어는 걸러내고 좋은 것만 추려 발전시키는 식임
       모든 분야의 작가들도 보통 편집자가 있음
       Magic: the Gathering 카드 게임 디자인도 비슷하게, 디자인 팀이 세트 초안을 만들면 완전히 별개의 개발팀에 넘겨서 검증함
       이런 협업 사례 더 있으면 듣고 싶음
          + dev 팀과 validation(검증) 팀으로 나누는 경우도 예로 들 수 있음
     * 실제로는 이 글과 반대라고 봄
       LLM은 초안 빠르게 만들 땐 우수하며, 잘 훈련된 인간이 LLM 결과를 비판적으로 검토하는 데 더 적합함
       그러니까 LLM은 블루팀, 인간은 레드팀에 더 어울림
          + 최첨단 수준에서는 이 시각이 오히려 뒤바뀔 수도 있음
            Tao도 그런 극한 상황을 언급하는 것 같음
     * 최근 에이전트 기반 모델과 워크플로를 써보며 느낀 점임
       이런 에이전트들은 코드 작성뿐 아니라 테스트, 관리, 심지어 매니지먼트 역할에도 써야 가장 빛남
       개발자는 일종의 관리자, 즉 감독자로 변하게 됨
       전체 태스크 기획(프롬프트 작성, 작업 범위 정리), 테스트 작성, 코드 작성 전 과정 모두 감독하는 위치임
       리뷰가 엄청 많아지긴 했지만, 직접 레드 팀 역할 하면서 덜 깨지는지 확인하니 오히려 통제권이 커졌다고 느낌
     * 이 관점이 인상적임
       비즈니스에서도 ‘블루 팀’(사회 기반 산업: 전력, 석유, 통신, 소프트웨어, 금융 등)과 ‘레드 팀’(부가가치 산업: 요식업, 특수 소매, 사치재, 관광 등) 이라는 식으로 나눌 수 있음
       경제적으로는 블루팀 쪽이 훨씬 중요한데, 이유는 이 산업들이 전체 경제의 기반이 되어 수요가 많고, 이 팀이 실수를 최소화해야 하기 때문임
       반면 레드팀 산업은 없어도 경제가 돌아가긴 하지만, 많아질수록 전체 품질 향상 효과를 가져옴
       Tao의 예시에서도 소프트웨어 엔지니어가 QA보다 보수 더 많이 받고, 증명 작성이 검증보다 더 경제적으로 가치 있다고 여겨지는 것도 동일 구조임
       Sam Altman이 LLM 트레이닝 자금 조달할 때 ""우리는 블루팀처럼 쓸모있다""고 강조해야 투자받기 쉽고, 그게 미디어 서사 전체에 영향을 미침
       실제론 레드팀 쓰임에 더 적합하지만, 투자금 회수를 내세워야 하므로 회사들은 블루팀 용도로만 LLM을 밀거임
       Google Glasses, VR, 웨어러블 기기들도 비슷한 패턴임
       이들은 니치 산업에서 유용한 레드팀 기술인데, 거대한 생태계나 수익은 못 내니 자본 입장에선 외면 받음
     * (Microsoft 소속임)
       RAG 샘플에 대해 azure-ai-evaluation SDK로 자동화된 레드 팀 검증을 직접 운영함
       여기서는 레일을 벗어난 adversarial LLM과 pyrit 패키지가 합쳐져, 앱에 괴상한 질문을 자동 생성해 던지고 base64, 시저 암호, urldecode 등으로 질문 자체도 변형함
       실제 결과가 흥미롭고, 레드 팀 활동에 LLM이 꽤 유용함에 동의함
       YouTube 데모 영상
       (음성 톤이 크더라도 양해 바람, 특이한 장소에서 찍었음)
"
"https://news.hada.io/topic?id=22312","eslint-config-prettier 공급망 공격: npm 계정 침해사건","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               eslint-config-prettier 공급망 공격: npm 계정 침해사건

Package Name            Package Version Weekly Downloads
eslint-config-prettier  8.10.1          > 31M
eslint-config-prettier  9.1.1           > 31M
eslint-config-prettier  10.1.6          > 31M
eslint-config-prettier  10.1.7          > 31M
eslint-plugin-prettier  4.2.2           > 21M
eslint-plugin-prettier  4.2.3           > 21M
snyckit                 0.11.9          > 21M
@pkgr/core              0.2.8           > 16M
napi-postinstall        0.3.1           > 9M

   다음 패키지가 설치되었던 경우라면 .npmrc가 털렸다 생각하고 크레덴셜 로테이션 시키는게 안전해보입니다.

   https://github.com/prettier/eslint-config-prettier/issues/339
   https://invokere.com/posts/2025/…

   저만 봇 알림을 못받은건가요? 이 글처럼 어떨때는 알림이 안오는거같네요. 흠..

   저도 안왔어요
"
"https://news.hada.io/topic?id=22286","주택에서 반(反)풍요 비판은 잘못된 주장임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        주택에서 반(反)풍요 비판은 잘못된 주장임

     * 주택 부족의 주요 원인을 법적 규제로 보는 견해가 주류 경제학자와 학계에서 지지됨
     * 반(反)독점 진영에서 주장하는 대기업 건설사의 공급 제한론은 실제 데이터와 현장 전문가 의견과 부합하지 않음
     * 주요 연구논문조차 달라스 등 대부분 대도시에 적용될 수 없는 기준임이 확인됨
     * 시장 집중도만으로 독점적 행위를 규정하기 어렵고, 현실적으로 규모의 경제 등이 이점이 될 수 있음
     * 주장 근거로 제시된 산업분석, 인용, 연구 등 다수가 맥락을 벗어나거나 왜곡되어 신뢰할 만한 증거로 보기 어려움
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 주택 공급 부족 문제와 비판적 시각

     * 반(反)풍요 관련 도서에 대한 가장 날카로운 비판은 반독점 운동 진영에서 나오고 있음
     * 이 집단은 미국의 가장 큰 문제를 독점과 대기업의 부패에서 찾음
     * 저자와 Ezra Klein은 지난 수십 년간 주택 공급의 주요 병목현상이 구역 규제와 최소 부지 크기 등 법률적 장벽임을 강조하며, 이는 다수 경제학자와 학자가 동의하는 주류 견해임
     * 반독점 진영에서는 대형 건설사가 이익을 위해 일부러 공급을 제한한다고 주장하며, 이에 대해 반독점적 정책 도구 도입을 촉구함
     * 필자는 이 주장들이 설득력이 약하다고 생각하며, 개발업자들의 수익률이 특별히 증가하지 않았음을 지적함

달라스 주택시장 분석에 대한 반박

  Musharbash의 주장: 달라스는 주택건설 과점시장

     * Musharbash는 달라스에서 주택가격이 크게 올랐고, 대형 건설사가 시장을 장악해 경쟁을 억제한다고 주장함
     * 정책 당국에 강력한 건설사에 대한 규제를 요구하고 있음

  전문가 검증 결과

     * Musharbash가 인용한 주요 경제학자와 시장 분석가를 인터뷰한 결과, 이론이 잘못 적용되었거나 맥락이 왜곡되었다는 증언을 듣게 됨
     * 달라스가 과점임을 주장하는 데 사용된 Quintero 박사의 논문 기준이 실제 달라스 시장엔 전혀 부합하지 않음이 확인됨

  구체적 사실 확인

     * Quintero 박사는 한 도시 내 신규 주택 공급의 90%를 5~6개 업체가 차지해야 과점적 폐해가 발생할 수 있다고 설명함
     * 달라스의 경우 상위 2개사가 30%, 6개사가 50% 수준에 불과해 과점 정의에 해당하지 않음
     * 전국 주요 50개 도시 중 49곳은 Quintero 기준에 미달, 신뢰할 만한 과점 증거가 없음
     * Musharbash의 주장과 인용은 핵심 논문의 저자조차 동의하지 않음

Quintero 논문의 한계와 추가적 의문점

     * Quintero 논문은 2006년(이상적이지 않은 주택시장 거품기)를 기준 연도로 삼아 결과 신뢰도에 의문을 줌
     * 전국 시장의 49곳이 기준에 미달하는데도, 연간 1,060억 달러 생산 손실로 결론내는 주장에 일관성 부족이 지적됨
     * 논문에서도 대형 건설사가 신도시교외 및 소도시에서만 의미있게 집중될 수 있음을 인정함

달라스 현지 전문가 인터뷰: 독점·과점 주장에 대한 반박

     * Builder’s Daily의 John McManus는 Musharbash가 여러 차례 인용한 인물임
     * 실제로는 대형 건설사가 가격상승의 주요 원인이라는 주장에 동의하지 않음, 오히려 토지이용 규제와 구역제 등 규제가 더 큰 문제라고 말함
     * 규제로 인해 초기 진입비용이 올라 저소득층 주택 공급이 막힘을 설명함
     * Musharbash가 '시장 독점'이라 인용한 발언 역시 작업 일정의 안정성 의미일 뿐, 정치적 의미가 아님을 그는 밝힘

업계 전문가 분석 인용의 한계

     * ResiClub의 Lance Lambert 역시 대형사 시장집중이 주택공급 위축의 주된 요인이라고 보지 않음
     * 오히려 대형사 위주의 시장이 자본력과 규모의 경제로 장기적으로 더 많은 주택을 공급할 수 있다고 해석함
     * 소규모 건설사만 남게 되면 위기 시 회복탄력성 저하의 우려도 존재함
     * 큰 회사의 존재는 특정 산업에선 사회적 이점이 될 수 있음을 강조함

시장집중도 수치만으로 독점적 행위 판단 어려움

     * Duke University의 James Roberts 교수는 “몇 개 회사가 몇 퍼센트를 차지”하는 수치 **하나만으론 유의미한 독점 판단 어렵다”고 설명함
     * 독점이 실제로 가격상승, 품질 저하, 하도급업체 불리 등 실질적 문제 유발하는지 각 사례별로 세부 분석이 필수임을 밝힘

독점-주택 논의에 실증적 증거 결여

     * 실제로 대형 건설사가 공급을 제한해 가격을 올렸다는 실증 연구는 거의 존재하지 않음
     * 인용 논문, 칼럼, 기사 대부분이 근거나 데이터 없이 주장-주장에 기대는 구조임
     * 새로운 연구마저 참조 논문에만 의존하며 실증적 증명을 결여함

결론 및 제언

     * 실제로 독점이 소비자 피해와 혁신 억제, 사회적 손실을 야기할 수 있음
     * 그러나 근거없는 과잉 독점 규제는 오히려 산업에 피해 및 정책 신뢰 저하 초래 위험
     * 주택산업 내 독점 규제 주장에 있어선, 실증적 데이터와 객관적 검증이 반드시 필요함
     * Musharbash의 에세이는 전문가 인터뷰, 연구 인용, 해석 등에서 신뢰성 결여가 지적됨
     * 정책 입안자는 탐정처럼 사실을 조사한 후 판단해야 하고, 반독점 운동도 이 점을 유념해야 함

   다음 글에서는 달라스에서 실제로 무슨 일이 있었는지, 과점 이외의 주택값 급등 원인에 대해 다룰 예정임

        Hacker News 의견

     * 우리가 읽는 대부분의 저널리즘 기사들이 지나치게 편집되고 거의 취재조차 되지 않은 경우가 많음을 새삼 깨달음, 기본적 현장 취재가 얼마나 강력한 힘을 지니는지 알게 됨. Derek Thompson은 굉장히 똑똑한 필력이 있지만, 주택경제에선 전문가는 아님. 그는 그저 논문을 읽고 논문에서 인용한 전문가들에게 전화를 걸었고, 그 논문들의 내러티브가 무너짐. Hacker News에선 저널리즘이 자주 비판받지만, 사실 심층 취재가 너무 적기 때문이라고 생각함. 나는 최근 Thompson의 Substack을 즐겨 읽고 있음.
          + 과거에는 기자들이 City News Bureau 같은 곳에서 경력을 시작했음. 시카고의 City News Bureau는 100년 동안 현장 뉴스 취재를 담당했음. 신입 기자들이 모든 경찰서, 큰 사건, 화재, 재판 현장을 돌며 사실을 철저히 확인했음. 이 기관의 유명한 모토는 “네 엄마가 널 사랑한다고 해도 직접 확인하라”였음. 이런 시스템이 다시 필요하다고 생각함. 요즘 뉴스는 보도자료로 시작하는 경우가 많으니, 읽고 나서 출발점이 무엇이었는지 자문해야 함. 이전엔 직접 거리를 뛰어다니며 취재했으나, 지금은 평론가의 비중이 너무 높음. City News Bureau를 다룬 책 ""Hello, Sweetheart, Get Me Rewrite""도 추천함
               o 시카고 트리뷴 기사
               o 해당 도서
          + The Atlantic 기사에 대해 논할 때, 언론이 기본적인 현장 취재보다 여론 분석에 치우쳤다는 말에 동의함. 메이저 언론사들도 깊이 들어가지 않고, 대부분 화제가 되는 이슈만 모두가 쫓는 경향이 강함. 올드스쿨 저널리즘에서 배울 점이 많고, 실제로 그 취재를 실천하는 사람들이 있음에도, 그 역할을 전문 언론인이 아닌 평범한 시민기자가 맡게 되는 현실이 씁쓸함. 예를 들어 D.C. 검찰청에서 2/3의 형사 사건을 내버린 사실은 익명의 인터넷 계정 DCCrimeFacts가 데이터를 직접 뒤져 밝히고, 이후에야 주요 언론이 보도하기 시작해 개혁으로 이어졌음. FAA 스캔들 때도 한 법대생의 블로그 글이 가장 결정적 정보였음. 역시 현업 기자보다 시민의 개인 프로젝트가 실제 변화를 이끌 때가 많음, 이는 한편으로 놀랍고, 또 한편으로 안타까운 현실임.
          + Abundance YIMBYism(공급 확장·주택 옹호 운동)엔 단점이 많음. 젠트리피케이션(원주민 내몰림)과 강제 이주, 시장가 주택의 낙수효과 미약, 저소득층용 공공주택의 필요성이라는 핵심 이슈엔 충분히 답하지 못함. 저널리스트들이 상대의 약한 논리만 공격하고 본질적 비판을 무시할 때 칭찬받을 점이 없다고 생각함.
          + Derek Thompson이 논문 저자들에게 직접 전화한 것이 내러티브 붕괴의 힘이라고 평가했으나, Matthew Stoller가 같은 사람들에게 다시 전화했더니 Derek이 그들의 견해를 잘못 전달한 것 같다고 함. 즉, 현장 취재(직접 발로 뛰는 것)가 권위나 명성을 한 번 더 붕괴시켰음.
          + “진짜 저널리즘이란 무엇인가”라는 토론을 여러 번 해봤는데, Ian Betteridge가 말하길 “언론인은 전화를 든다”였음. 표면적으론 좀 구식 같지만, 실제로 내용확인을 위해 수십 통씩 전화, 이메일, 재요청하는 집요함이 진짜 저널리즘의 기준이라고 생각함. 인터넷의 영향, 낮아진 보상, 생산 압박 등이 이런 취재 부족 현상을 유발했지만, 결국 저널리즘은 숙련된 직업 기술임. 그래서 그런 진짜 저널리즘의 희소성이 지금도 더욱 가치 있게 느껴지는 것임.
     * 기사에서 “anti-trust left”라는 말이 반복될 때마다 몰입이 깨졌음. 독자 입장에서는 이 글이 자신을 “반독점 좌파와 다른, 더 잘 아는 진영”에 두려는 느낌이 강하게 들었음. 상대를 허수아비로 만들고 집단신호를 주는 것처럼 느껴졌음. 게다가 출처들을 정말 전화로 확인했는지 신뢰가 가지 않았고, 본인은 객관적 진실만을 밝혔다고 주장하지만, “anti-trust left”만 탓하고 있는데, 정말 그렇게만 볼 수 있는지 의문임.
          + 나는 굉장히 좌파(신디칼리스트 아나키스트)임에도 위에서 말한 느낌이 약간 있었음. 그래도 굳이 걱정할 수준은 아니라고 보는 게, 그가 “좌파 전체”가 아니라 특정 입장만 지적했다고 생각함. 그리고 좌파 내에서도 주택 정책에 대한 의견이 매우 다양함. 반독점, 자본 소유 제한, 탈상품화, 전면 공공주택 등 수많은 이견이 있지만, 우스갯소리로 가장 좌파를 싫어하는 건 같은 좌파라는 말도 있음.
          + Matt Stoller 등 미디어의 대표적 “anti-trust left” 그룹이 있고, Dereck Thompson 및 Ezra Klein 같은 “abundance liberals” 그룹도 있음. 민주당/좌파-미디어-학계-정책 분야에서 이 두 세력이 대중의 관심과 신뢰를 놓고 치열하게 경쟁 중임. 둘 간의 자원은 유한하고 사실상 제로섬 경쟁이라 봄.
     * 지나친 규제가 주택 문제의 90%라 생각함. 합리적인 시장이면, 아파트 단지가 노후화되며 더 저렴해지고, 상위 계층이 이사하면서 기존 주택이 풀려야 정상임. 하지만 많은 지자체는 그런 선순환을 인정하지 않음. 신규 개발에 저렴한 주택 쿼터를 강제함. 노숙자용 유닛 하나에 60만 달러까지 듬(예시 건물). 저소득층 바우처·Section 8 프로그램은 악몽 같음(11년 대기 리스트). 원인은 두 가지임. 첫째, 집 짓기가 너무 어려워짐. 캘리포니아에서는 “새 빌딩이 내 햇빛을 가린다”는 이유만으로 공사가 지연됨. 주차장 의무화로 주택 대신 차량 공간이 우선시되고, 건축비 30%가 주차장에 들어감. 도시도 수십 년 전 문제를 미리 해결해야 했는데, 늦음. 개인 입장에선 감당 가능한 곳으로 이주하는 수밖에 없음. LA에서 4세대 지냈지만 가족 절반이 떠났음. 중산층
       월급으론 충분히 살 수 있지만, 거주 도시를 직접 선택해야 함. 비싼 도시에 남아 정치인이 해결해주길 기대할 순 없음.
          + “지나친 규제가 원인 90%”라는 주장에, 주거 위기는 선진국 대부분(유럽, 일본, 미국)에서 보임. 문제는 규제보다 훨씬 광범위한 글로벌 요인이라 생각함.
          + “합리적 시장”의 논리도 규제로 인해 생긴 것일 수 있음. 큰 돈 들여 아파트를 짓는다면, 투자 보호를 위해 규제가 더 필요하다고 로비하거나, 소유자는 경관 보호, 소음 차단 등 추가 규제를 합리적으로 원할 수 있음. 이런 논리도 모두 인간적으로 이해 가능한 행동임.
          + 워싱턴 DC에서는 한 사람이 모든 지역사회회의마다 참석해서 수백 유닛의 주택 건설을 좌절시킴.
          + 주택 가격이 규제와 비례한다면 덜 밀집한 곳이 언제나 싸야 하지만, 실상은 대도시의 주택이 항상 더 비쌈. 규제가 적은 대도시도 예외 아님. 인간이 만드는 “합리적 시장”이란 건 마치 “마른 물(dry water)”처럼 불가능한 개념임. 본질적 문제는 주택이 자산으로 간주되는 것이며, 정책도 부동산이 주식처럼 수익률을 내는 구조로 만드는 데 집중됨. 주식은 생산성 향상으로 가격이 오르지만, 주택 가치는 더 비싸질 때만 오름.
     * 집값이 오르거나 안 떨어지길 바라는 대규모 로비 그룹이 존재함. 바로 주택 소유자들임. 그리고 고위급 정치인들도 보통 1채 이상의 집을 갖고 있어, 같이 집값 유지를 원함.
          + 고위 정치인들이 주택을 보유한 것 이상으로, 주택 소유자들은 임차인보다 투표율이 50%나 높음. 게다가 임차인 중엔 아예 투표자격 없는 사람도 많음(관련 자료).
          + 집값 하락을 바라지 않는 건 소유자만이 아님. 은행도 집값이 떨어지면 채무 불이행이 늘고, 대출이자 수요가 줄어 손해임. 도시도 개발업자와 재산세 수익에 의존하므로, 가격 하락이 반갑지 않음.
          + 집값이 하락하면 경제에 악영향이 큼. 대출 중개업체, 건설사, 자재 공급업체 모두 손해임. 내가 보기에 가장 나은 해결책은 주택 가격이 정체되어 일반 인플레이션에 따라 상대적으로 저렴해지는 것임.
          + 신규 주택 건설에 올인하는 대규모 로비 그룹도 존재하는데, 바로 부동산 개발업자임. 정치인들은 대규모 정치 후원을 필요로 하므로 여기에 깊이 얽혀 있음.
          + NIMBY(주택 개발 반대) 운동에서는 집값 하락이 실제로 무슨 의미일지 진지하게 고민하는 경우 별로 못 봄. 거대한 지역 전체에서 집값 하락이 장기간 허용되는 건 사실상 불가능해 보임. 조금 덜 우울하게 현실적으로 기대한다면, 인플레이션 대비 소폭 가격 상승이 가장 가능성 높은 시나리오임.
     * 주택난의 뿌리가 부의 불평등이라는 점에 대해 논의하고 싶음. 공급이 충분해도, 부유층이 추가로 시장 공급량을 모두 사들여 가난한 사람에게 임대하게 됨. Piketty 논리가 작용해서 이윤을 재투자하며 부의 격차가 계속 커짐. 결국 집은 많아져도 소수가 모두 소유하는 결과가 옴. 내 생각엔 1차 실거주 단일주택 투자 제한을 법적으로 두고, 신축 규제는 완화해야 함.
          + 부자들이 임대용 매물을 대량 매입하는 현상이 실제로 그렇게 흔하지 않다고 느낌. 벤쿠버에서 살아봤는데, 그 동네 집 대부분은 실제 거주 소유자였음. 하지만 100년 동안 주택 수가 늘지 않았고, 벤쿠버 거주 희망 인구는 열 배로 커졌음. 문제는 매물이 매입된 것이 아니라, 새로운 공급이 늘지 않은 것임. 게다가 벤쿠버는 글로벌 부동산 목적지라 세계 각국 부자와 경쟁해야 함. 최근 몇 년 전 떠나 다른 곳으로 이사했으며, 벤쿠버는 더 이상 캐나다인을 위한 곳이 아니라고 느낌. 정부가 외국인 투자, 실거주가 아닌 소유에 세금 부과, 인허가 요건 완화, 금리 인상 등 실효성 있는 조치를 취하지 않는 한 효과를 보기 힘듦.
          + Piketty 이론은 수요가 무한하다는 전제가 깔려 있음. 실제로 북미 도시는 100년 간 단독주택 외 모든 형태의 주택을 불법화했던 탓에 이런 현상이 심하지만, 수요는 실제로 무한하지 않음. 만약 Piketty가 옳다면, 평당 인플레이션 조정 주택가격이 언제나 모든 곳에서 오르기만 했어야 함. 실제로는 건설 호황 때 렌트가 안정되거나 하락했고, 가장 집을 적게 지은 곳에서 가격이 급등했음. 자동차도 같은 논리라면 부자는 다 사서 빌려줄 수 있었어야 했는데, 자동차 수요엔 인위적 제한이 없어 공급이 곧장 맞춰짐. 주택도 똑같을 수 있다고 봄.
          + 계속 집을 지으면 언젠가는 사람이 원하는 것보다 집이 더 많아짐. 그때가 되면 가격은 하락함. 아무도 임대하지 않으려고 하면 더 싸게 내놓거나 팔아야 함. 결국, 수요를 완전히 넘어서면 언제든 가격 하락이 옴.
          + 부자들이 추가 공급 유닛을 다 매입한다면, 그 주택을 임대해야 하므로 기존 물량과 경쟁함. 그다음엔 가격이 하락하게 됨.
          + 부의 불평등이 주택난의 근본적 원인이라는 명확한 증거는 없음. 실제 집이 공급 부족이 핵심이라고 봄. 게다가 부자가 공급을 매입해 임대한다 해도, 임대물량 증가는 곧 임대료 하락으로 이어짐.
     * 주택 문제는 사실 문화적 문제임. 우리는 고밀도 개발이나 교통 인프라 구축을 반대해왔고, 그 이유는 집값 하락을 원치 않는 수많은 주택 소유자, 경제 불안정, 정치적 재선 실패 가능성 때문임. 미국에서 신규 주택 공급이 큰 도시가 아닌 교외·외곽에서 이뤄지는 것도 이 때문임. 농지도시에서 분양을 늘리고 고속도로를 뚫으면, 누구도 집값 손해를 보지 않고 개발업자, 자동차·에너지 산업도 이득임. 텍사스 등은 이런 패턴으로 성장하지만, SF나 NYC의 교외·외곽엔 그런 모델조차 없음. 진보 도시 문화에는 스스로 쌓은 “울타리”와 “특권”에 대한 자기만족이 깔려 있음. 개인적으로 이런 구조가 풀리지 않는다고 봄. SF/NYC 같은 도시는 한계에 도달했고, 문제 해결을 위해선 딴 데서부터 밀도를 높여야 함. 원격 근무 적극 활용, 오프그리드 및 저비용
       기반시설, 대중교통 중심의 도시, 직접적인 이주 장려금 등 혁신적 정책이 필요함. 사소한 인허가 개혁만으론 택도 없다고 생각함.
     * Gary’s Economics에 따르면 주택이 비싼 원인은 부의 불평등 때문임. 자본 집중이 심화되면 초부유층이 자산 가격을 밀어 올리고, 최근 금값 폭등처럼 주택도 마찬가지로 비싸짐. 정부의 경기부양책도 결국 부유층에게 흡수됨. 전 세계 주요 도시가 대부분 이 구조임.
          + 관련 영상
          + 사실 모든 금속, 거의 모든 원자재 가격이 BLS 공식 CPI 수치보다 훨씬 많이 올랐음. 이는 상품시장보다는 공식 인플레이션 수치의 신뢰성에 더 많은 의문을 제기함.
          + 인과관계가 반대임. 주택 규제가 곧 불평등의 100%를 설명함. 용적률 완화만 해도 과잉 불평등은 사실상 사라질 것임(Brookings 논문)
          + 내가 경제학을 잘은 모르지만, Gary를 언급하는 모든 사람들이 어떤 진영을 막론하고 Gary는 경제학적으로 신뢰할 만한 인물은 아니라고 일치된 반응을 보임.
          + 아무리 부유해도 한 가정이 한 번에 살 수 있는 집은 하나임. 빌리오네어가 별장, 펜트하우스를 여럿 소유한다 해도, 그런 집은 원래부터 일반인이 살 수 있는 매물이 아님. 초호화 매입이 주택난의 핵심 원인은 아님.
     * 온라인에서 벌어지는 논쟁들과 실제 지역 사회의 주택 갈등을 비교하면, 트위터상에서 들려오는 “반독점 좌파”보다 지상에서는 NIMBY(재산가치, 치안 우려자) 목소리가 훨씬 큼. 실제로 오프라인 커뮤니티에선 개발 반대가 더 주된 저항임. 그리고 덜 온라인적인 좌파 비판자들은 “용도 상향(Upzoning)과 공공주택이 반드시 상충하는 것이 아니다”는 점을 점차 받아들이는 것 같음.
     * 현 주택 시장 비판에는 반드시 주택의 ‘질’에 대한 언급이 빠질 수 없음. 최근 방문한 아파트는 극도로 좁은 공간에 어설픈 창문·문만 달린 상태였음. 집은 9피트 천장에 살고 있지만 새로 본 곳은 아주 실망스러웠음. 최근에 지어진 임대주택도 방음 안 되고, 문의 재질도 종이 같으며, 모든 벽이 삐뚤어짐. 30년 전 살던 2룸 아파트가 오늘날 1.5밀리언 이상 가치를 지님. 품질 인플레이션도 무시못함
          + 관련 기사
          + 공급 제한이 이런 현상 원인임. 허가만 10배로 풀어도, 이렇게 품질 낮은 유닛은 경쟁력이 없어짐. 공급 부족이 문제의 핵심임.
          + 최근 이사해야 했던 아파트(2023~2024년 신축, “럭셔리” 마케팅)도 한 달간 온수가 안 나오고, 벽·바닥이 너무 얇아서 위층 이웃의 발소리까지 다 들림.
          + 품질은 아마도 기대한 방식으론 개선되지 않았지만, 코드상의 온갖 조건(12피트마다 콘센트, 아리조나 더위, 캘리포니아 지진, 플로리다 허리케인, 미네소타 한파, 각종 재난 등)엔 맞춰야 하므로 재료비가 더 싸야 하는 현실임. 건축법 기준이 커지니, 결국선택지는 스타터 하우스용 저질 자재밖에 없음.
          + 아파트는 “좁다” 혹은 “럭셔리”라는 평 외에는 평가받기 힘듦. 뭘 해도 비난받는 구조임.
     * 호주에서는 주택 위기가 주로 공급 부족과 NIMBY(주택 개발 반대) 논리로 설명됨. 하지만 멜번은 전국 4번째로 비싼 도시에서 점점 더 저렴한 도시로 변했고, 주 원인으론 토지세 개정이 매매 투자를 막으면서 부동산 투자자들이 다른 주로 이동해 가격 상승이 둔화된 것임. 공급도 중요하지만, 실제로 가격을 끌어올리는 건 투자 수요라고 봄.
          + 많은 Abundist들은 Rent-seeking(지대추구)에도 전혀 문제를 못 느낌. 그에 유인되는 구조에서 시장이 왜곡되고 있다고 생각함. 언젠간 극단적 변혁(예: Mao 2.0)도 올 수 있음.
"
"https://news.hada.io/topic?id=22316","리나 칸, 피그마 IPO를 M&A 규제의 정당성 증거로 지목","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   리나 칸, 피그마 IPO를 M&A 규제의 정당성 증거로 지목

     * 리나 칸 전 FTC 위원장이 Figma의 IPO 성공을 두고, 신생 기업들이 독립적으로 성장할 수 있도록 한 점을 강조함
     * 2023년 Adobe의 피그마 인수 무산 사례가 주요 배경으로, 당시 규제 기관의 M&A 심사가 핵심 이슈였음
     * 칸 위원장은 빅테크의 스타트업 인수를 적극적으로 제제하며, 창업자들에게 더 많은 선택지를 주는 환경이 필요하다고 주장함
     * 업계 일각에서는 규제 때문이 아니라 피그마의 혁신 덕분에 성공했다고 반박 의견을 내고 있음
     * 칸은 피그마 IPO를 “직원, 투자자, 혁신, 그리고 전체 사회의 승리”라고 평가함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

피그마 IPO와 리나 칸의 규제 정책

  개요

     * 피그마의 성공적인 IPO에 대해 전 FTC(연방거래위원회) 위원장인 리나 칸이 주목하며, 신생 기업이 거대 기업에 인수되지 않고 독립적으로 성장할 때 더 큰 가치를 창출할 수 있음을 강조함
     * 칸은 자신의 X(구 Twitter) 계정에 관련 기사 및 WSJ 링크를 공유하며 이번 IPO가 M&A 규제의 필요성을 보여주는 사례임을 역설함

  Adobe의 피그마 인수 시도와 규제

     * 2023년 Adobe는 200억 달러에 피그마 인수를 시도했으나, 유럽연합 집행위원회와 영국 CMA(경쟁시장청)의 승인 난항으로 무산됨
     * 미국에서도 FTC가 피그마가 Adobe의 효과적인 경쟁자인지 여부를 쟁점으로 규제 감독을 진행함
     * 칸이 FTC를 이끌 당시, 빅테크의 스타트업 인수에 대한 강경 대응이 이어졌고, 이에 따라 기업들은 아예 회사를 인수하는 대신 핵심 인력만 채용하고 기술 라이선스를 받는 '리버스 어콰이하이어(Reversed Acqui-hire)' 방식을 활용함
     * 칸이 FTC를 떠난 뒤에도 이러한 리버스 어콰이하이어 트렌드는 계속됨

  칸의 규제 철학과 옹호

     * 칸은 테크 업계 비판에도 불구하고, 실제로 규제 재심의를 받은 거래는 극소수임을 강조함
     * 창업자에게는 인수를 결정할 수 있는 후보 기업이 6-8개로 다변화될 때 더 큰 기회와 권한이 생긴다고 주장함
     * 2025년 미국 대선 이후, 칸은 바이든 대통령에 의해 임명되어 FTC를 이끌었으나, 트럼프 2기 출범과 함께 사임함
     * 칸은 피그마 상장이 자기 정책의 정당성을 보여주는 '승리' 사례라고 언급하며, “직원, 투자자, 혁신, 그리고 공공의 승리”로 IPO를 평가함

  상반된 시각

     * 칸의 비판론자들은 피그마의 IPO 성공이 규제 덕분이 아니라 “회사의 독보적 성장과 혁신” 덕분이라는 입장을 밝힘
     * Wedbush Securities의 Dan Ives 애널리스트는 “피그마의 성공은 FTC나 칸 덕분이 아니라 혁신적 성장 때문”이라고 인터뷰에서 강조함

결론

     * 피그마 IPO는 M&A 심사와 스타트업 생태계 정책에 중요한 함의를 남김
     * 리나 칸은 규제의 중요성을, 일부 테크 업계 인사는 시장 주체의 혁신 역량을 각각 강조함

        Hacker News 의견

     * Figma의 IPO는 Khan이 옳았음을 보여준다고 생각함, 현재 시가총액이 600억 달러로 2023년 Adobe의 인수 제안이었던 200억 달러를 훨씬 웃도는 중임, 규제기관이 ‘빅테크’에게 가끔은 “안 된다”고 할 수 있어야 경쟁과 더 높은 시장 가치를 유지할 수 있다고 봄, 이번 사례를 통해 Figma 임직원이 큰 부를 얻었고 디자인툴 시장 경쟁도 남아 있으며 또 하나의 독립적인 테크 회사가 탄생한 셈임
          + 완전히 그녀가 옳았음을 증명했다고 생각함, 시가총액, 임직원 보상, 소비자 선택권, 상장사 수 확대, 새로운 인수 후보자와 다양한 근무 환경 등 모든 측면에서 Lina Khan의 견해가 맞았음, 스타트업 IPO로 큰 돈을 벌어 모두가 부자가 된 상황에 불만 가진 로비가 Hacker News에 있다는 분위기가 전혀 이해가 가지 않음, HN 커뮤니티가 표방하는 가치와 맞지 않음
          + Figma 때문에 디자인툴 시장의 경쟁이 유지된다고 하지만, 사실상 Figma가 업계 거의 독점에 가까운 위치라 봄, Adobe는 Figma의 경쟁작이던 XD를 폐기했음, Sketch는 시장점유율이 미미하고 Penpot 같은 오픈소스 툴은 여전히 뒤쳐져 있음, 그래서 Figma가 유저 기반에 대한 통제를 더 강화하고 있는데 모두 불만이 많지만 계속 쓰고 있음, 그렇다고 Adobe 인수됐으면 더 나았을 거라는 뜻은 아니나, 오히려 Figma가 스스로 ‘악역’이 되어버린 현실을 아쉬워함
          + 빅테크를 규제해야 한다는 논지에 대해 생각해볼 때, 어떤 시점에서는 ‘빅테크’라는 게 실제로는 ‘빅 파이낸스’의 모습으로 나타난다고 봄
          + 규제기관이 없다면 결국 가장 강한 플레이어들이 규제자가 됨, 사회에 '규제자 없음'이란 건 존재하지 않음, 누가 어떤 방식으로 규제할지가 핵심임
          + 대기업이 경쟁사를 M&A 하는 것에 제한 두는 데 동의함, 하지만 2년 사이에 3배 가치 상승만으로 ‘반독점 규제 덕분에 훨씬 더 큰 비즈니스 가치가 창출됐다’고 보긴 어려움, 지금은 결과가 나왔으니 쉽게 얘기하지만 당시 Figma 주주들도 확신이 없어 팔았던 건데다 2년간 3배는 그다지 높은 수익이 아님, 그 사이에 임직원이나 투자자들이 더 높은 수익을 다른 방식으로 얻을 수도 있었음, 주식 매각이 늦어지면 새 스타트업이나 투자 기회 창출이 지연되는 등 어려운 점도 있음, 다만 이번 사례는 Figma 사용자에게는 정말 좋은 결과라고 생각하며, 인수됐으면 창의적인 조직문화가 무너졌을 것임, 대신 이제 Figma가 '지루한' 단계가 된 만큼 혁신적인 인재들은 또 새로운 제품을 만들어 낼 수 있음, 한 명이나 한 기관이 시장 전체보다 더 정확하게 세부적인
            이득을 판단하긴 어려우며, 그런 점에서 시장이 종종 더 잘 측정함, 물론 시장이 측정 못하는 가치도 존재함
     * 규제 이슈의 대안으로, 대기업이 작은 회사를 인수할 때 일정 비율로 세금을 매기면 어떨지 제안함, 예를 들어 인수자와 피인수자 규모 차가 10배 커질 때마다 인수금액의 100%에 해당하는 세금을 부과하는 방식임, 일반 IPO나 동급 기업 간 인수엔 세금이 없고, 엄청 큰 회사가 작은 회사를 인수하려면 그만큼 높은 세금 내야 하니 같은 규모의 회사끼리 합병을 유도하게 됨
          + 하지만 같은 규모끼리 합치는 것도 경쟁에 나쁘게 작용할 수 있다고 봄
     * 창업자들은 궁극적으로 ‘6~8개 후보’에게 인수될 수 있는 세상이 ‘1~2개’만 존재하는 것보다 낫다는 주장을 함, Lina Khan의 발언에 현실성 공감함
          + 하지만 Figma의 IPO로 인한 실질적 이득 대부분은 상장 시 급등(IPO pop)으로 인해 은행들이 챙겼다고 봄, 창업자도 인수나 제대로 된 가격 책정 없이 IPO했다면 비슷한 수준을 더 빠르게 받을 수 있었다고 판단함, 관련 기사: Figma IPO pop shines spotlight on underwriter pricing
          + 모든 M&A 거래를 근거 약한 이유로 막아선 안 된다고 생각함, 이런 규제로 거래 자체가 얼어붙으면 오히려 경쟁 잠재력을 떨어뜨릴 수 있음, Lina가 자리에서 물러나자마자 M&A가 다시 활발해진 것도 우연이 아니라고 봄
     * Figma가 Adobe에 200억 달러에 매각되지 않고 3년 뒤 193억 달러로 IPO한 게 과연 성공이냐는 의문 제기
          + 장기적인 관점에선 기업 가치 외에도 경쟁과 소비자 선택권, 자본 효율 등 여러 이익이 중요하다고 봄, Figma와 Adobe 중 어디에 투자할지 명확하게 해준 점도 자본배분 측면에서 오히려 효율적임
          + HP가 Palm 인수 후 제품과 인력을 모두 날려버린 사례를 들어, 경쟁이 유지될 때 시장이 더 나아짐을 강조, 독점 대기업에 의한 경쟁사 인수는 소비자・근로자・사회에 모두 나쁨
          + 실제로 Figma의 시가총액이 3년 만에 약 580억 달러로 3배 가까이 성장했음을 지적
          + 더 많은 경쟁이 곧 성공이라고 생각함
          + Figma가 스스로 성장 가능성이 있는데도 Adobe에 프리미엄 없이 팔았을 리 없다고 봄, 상장 후 AI툴 성장 등으로 더 커질 가능성이 높으니 IPO 기준 주가에서 250% 이상 추가로 뛰면 Adobe에 합병됐다면 그만큼의 시가총액이 Adobe로 이관되는 셈이라 생각함, Lina Khan이 적어도 이 합병에선 정답이었다고 봄
     * 기사 마지막의 논평에 대해, 핵심은 Figma란 회사가 경쟁을 만든 채 시장에 남았다는 점에 있다고 지적함
          + Sell side 애널리스트가 성공을 오직 성장에만 두는건 동어반복에 불과하다고 지적함
     * 더 많은 회사가 일찍 상장하며 시장과 사회에 긍정적 효과를 준다고 봄, ‘비공개 상태에서 100억 가치로 성장한 뒤, IPO나 빅테크 매각을 통해 VC 투자자들이 모든 이득을 회수하는 것’이 아니라, 더 공개적 방식이 낫다고 생각함
          + VC가 대주주인 경우는 해당됨, 자연 성장한 기업의 경우 오히려 상관 관계가 반대일 수 있음
          + ‘IPO로 unload’ 한다는 건 실제 가치 이상에 회사를 파는 걸 의미하는데, 그렇다면 왜 대중이 그 가격에 주식을 사는지 의문임
     * M&A 경험상, 인수합병은 종종 ‘생명유지장치’ 같았음, 넘치는 M&A를 실행하는 기업에선 ‘직접 만들 수 있지 않았나’라는 생각이 들었고, 피인수 합병을 경험한 곳에선 ‘이렇게 무능해선 스스로 만들 수 없었겠구나’ 싶었음, 정말 치열한 기술 경쟁이라 합리화되는 경우는 매우 드뭄, 2015년 이후 AI와 같은 특수 사례 빼면 거의 대부분 지식재산 독점 차원에서만 M&A가 이뤄져왔다는 게 Lina Khan 말의 요지임, 결국 고객・주주・사회 누구에게도 실익이 없으며 소수만 이득을 얻는 구조임
          + 그나마 얻는 이득은 임시적으로 실적 맞춰 보너스 타가는 임원들 뿐임
          + 2차 시장이 1차 시장을 이끌기도 함
     * iRobot의 파산은 무엇의 증거냐는 질문 제기
          + 중국 하드웨어 기업의 글로벌 우위로 인해 iRobot의 입지가 악화된 것에 원인이 있다고 봄
          + 만약 Amazon이 iRobot 인수를 승인받았다면, 수평적 확장으로 또 하나의 시장 지배를 만들어냈을 테고, 이는 최종적으로 가격 인하 등 소비자 이익에 기여했을 수도 있다고 봄
          + Amazon이 iRobot을 사업 실력 때문이 아니라 시장 통합이나 고객 데이터 확보 등 2차적 목적 때문에 인수하려 했다고 의심함
          + 특허로 성장하다 결국은 실제 고객 가치 제공이 없으면 오래 못 간다는 증거라고 생각함
     * Microsoft가 견제받지 않던 독점기업이었을 때의 컴퓨팅 ‘지옥도’를 잘 기억해야 함
          + 지금도 Microsoft는 엄청난 영향력으로 시장 모든 분야에 뛰어들 수 있다고 봄, 기존 제품으로 신제품을 엮어 팔고, 베끼거나 공짜로 제공하며, 소송까지 이용함, 한 기업이 4조 달러씩이나 되는 시대는 경쟁이나 장기적인 고객 입장에서 바람직하지 않다 생각함, 다만 옛날 방식의 단순 독점과는 다른 방식, 이에 맞는 새로운 규제가 필요하다고 봄
          + 그 시절은 Google, Nvidia, Amazon, Netflix가 태동하던 때이기도 하다는 점을 상기시킴
          + Microsoft의 ""포용, 확장, 말살"" 전략을 잊은 적 없음, 개인적으로 MS를 싫어하는 주된 이유임
     * 사후에 결과를 놓고 얘기하긴 쉽지만, 더 낫다는 보장은 어디에도 없었다는 점을 지적함
          + Adobe 인수를 막았던 Lina Khan의 당초 논지가 ‘더 나은 결과’를 이끌 수 있다고 명시했던 걸 상기함
"
"https://news.hada.io/topic?id=22292","FLUX.1 Krea의 가중치 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          FLUX.1 Krea의 가중치 공개

     * Krea 1의 첫 이미지 모델이 FLUX.1 Krea라는 오픈 가중치 버전으로 공개됨
     * 기존의 이미지 생성 모델과 달리, 명확한 미적 취향과 포토리얼리즘에 집중해 ""AI 같지 않은"" 이미지를 목표로 설계됨
     * 기존 벤치마크와 평가 지표들이 실제 사용자가 원하는 미적 감각과 어긋난다는 문제를 분석하고, 이를 해결하기 위해 직접 큐레이션한 데이터와 의견 중심의 미학적 편향을 적용함
     * 사전학습(pre-training) 과 후학습(post-training) 과정을 구분하여, 다양성을 보장하는 단계와 명확한 스타일로 수렴시키는 단계를 체계적으로 운용함
     * 향후에는 개인화·취향 맞춤 연구와, 더 넓은 비주얼 도메인 확장 및 창작자 지원 기능을 강화할 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

FLUX.1 Krea 오픈 소스 출시

     * Krea 1은 Black Forest Labs와 공동으로 훈련한 첫 이미지 생성 모델로, 미적 컨트롤과 이미지 품질에서 우수함을 목표로 함
     * FLUX.1 Krea [dev] 는 오픈 가중치로 공개되며, 기존 FLUX.1-dev 생태계와 완벽히 호환됨
     * 이 모델은 포토리얼리즘과 미학적 요소를 극대화하며, 특정 미적 취향을 반영해 설계된 opinionated aesthetics 지향임

""AI Look"" 현상과 한계

     * 기존 AI 생성 이미지는 종종 과도하게 흐릿한 배경, 왁스 같은 피부, 밋밋한 구도 등 이른바 ""** AI look**"" 현상을 보임
     * 벤치마크 최적화와 기술적 지표에 치우치면서, 진짜 같은 질감, 스타일 다양성, 창의적 결과물이 희생되는 문제를 지적함
     * 실제 사용자 취향을 반영하지 못하는 기존 평가 모델의 한계
          + 사전 훈련 단계에서 사용되는 Fréchet Inception Distance (FID) , CLIP Score는 모델의 전반적 성능을 측정하는 데 유용함
          + 학계 및 산업계에서는 DPG, GenEval, T2I-Compbench, GenAI-Bench 등 다양한 벤치마크가 쓰이지만, 주로 프롬프트 부합성과 공간 관계, 속성 결합 등만 평가함
          + 미적 평가 모델로 LAION-Aesthetics, Pickscore, ImageReward, HPSv2 등이 있으나, 대부분 CLIP 기반으로 해상도 및 파라미터 수가 제한됨
          + 예를 들어 LAION-Aesthetics는 여성, 흐릿한 배경, 밝은 색감 이미지에 치우친 편향을 가짐; 이런 기준으로 데이터 필터링 시 모델에 암묵적 선입견이 심어질 수 있음
     * 미학 평가지표와 필터는 나쁜 이미지를 걸러내는 데는 유용하나, 훈련 데이터 선별에 지나치게 의존할 경우 모델 자체에 편향이 각인될 위험이 있음
     * 최신 비전 언어 모델 기반의 평가지표가 등장하고 있으나, 미적 취향은 여전히 주관적이므로 단일 수치로 환원하기 어려움

사전학습(Pre-training)과 후학습(Post-training) 구조

     * 사전학습(Pre-training)
          + 사전학습에서는 스타일, 사물, 인물, 장소 등 시각적 세계에 대한 지식을 폭넓게 습득하여 다양성(mode coverage) 을 극대화함
          + ""좋지 않은"" 데이터도 포함해, 모델이 바람직하지 않은 특성(예: 이상한 손가락, 흐릿함 등)도 학습하게 만듦
          + 사전학습이 모델의 최고 품질 한계와 스타일 다양성을 결정
     * 후학습(Post-training)
          + 후학습에서는 모델 분포를 선호하는 스타일로 집중(mode collapsing) 시켜, ""AI look"" 대신 명확한 미적 방향성으로 수렴
          + Supervised Finetuning(SFT) 과 RLHF(선호도 기반 강화학습) 의 2단계로 진행
               o SFT: 직접 큐레이션한 고품질 데이터셋 및 Krea-1의 합성 이미지 활용
               o RLHF: 내부 선호도 데이터를 기반으로 여러 번 최적화해 미학과 스타일을 세밀하게 다듬음
          + 데이터 양보다 데이터 품질이 결정적이라는 점을 확인(1M 이하 소량 고품질 데이터로도 충분)
          + 의견 중심(opinionated approach) 의 미학 선호도 레이블을 적용, 기존 공개 선호도 데이터만 활용할 경우 단조롭고 AI look으로 회귀하는 문제 방지

모델 파이프라인 및 실험적 인사이트

     * flux-dev-raw라는 12B 파라미터의 guidance-distilled 베이스 모델을 사용, 기존의 과도하게 finetune된 오픈모델과 차별화
     * RLHF 단계에서는 TPO(preference optimization) 기법을 적용해 미감과 스타일화 특성을 강화함
     * 고품질의 내부 선호 데이터(엄격 필터링)를 여러 번 활용해 모델 출력을 정교하게 보정함
     * 주요 발견점
          + 1. 데이터의 양보다 질이 더 중요함. 1M 미만의 데이터로도 의미 있는 사후훈련 가능. 양적 다양성은 편향 완화와 안정성에 유효하지만, 가장 중요한 것은 큐레이션된 고품질 데이터임
          + 2. 명확한 취향 중심의 데이터 수집이 필요함. 대중적 공개 데이터셋은 의도치 않은 편향과 ""AI look"" 회귀, 단순구도/색상 편향 등 문제를 유발함
               o 텍스트 구현, 해부학, 구조 등 객관적 목표에는 데이터 다양성이 도움이 되지만, 미감 등 주관적 목표에는 혼합보다는 특화된 데이터가 더 효율적임
               o 다수의 미감 분포를 섞으면 누구도 만족하지 못하는 결과가 나오며, 많은 사용자가 LoRA 등 후처리 방법에 의존하게 되는 현상도 언급됨

미래 연구 방향 및 마무리

     * Krea 1은 미학적 기준과 품질을 중시하는 창작자를 위한 첫걸음이며, 오픈 소스 커뮤니티의 확장을 기대함
     * 향후 핵심 역량 강화, 더 다양한 시각 도메인 지원, 개인화·컨트롤러빌리티 연구를 통해 사용자의 미적 취향에 맞는 모델 제공을 목표로 함
     * GitHub( https://github.com/krea-ai/flux-krea ) 참고

        Hacker News 의견

     * 안녕하세요, 모두들 반가움. 나는 Krea의 공동 설립자이자 CTO임. 우리 모델의 가중치를 공개해서 HN 커뮤니티와 공유하고 싶다는 바람이 오래 전부터 있었음. 오늘 하루 동안 최대한 온라인에 머물면서 궁금한 점이 있으면 답변 드리겠음
          + 혹시 Flux 'Kontext' 버전, 즉 편집 모델 지원 계획이 있는지 궁금함. 프롬프트 기반 이미지 편집의 활용 가능성이 엄청 커 보임. 비록 오픈웨이트 버전의 품질은 아직 못 봤지만, 데모가 매우 인상적이었음. 참고로 이 모델도 12B 크기임
          + 이런 공개를 하는 목적이 무엇인지 궁금함. 사업적으로 어떤 목표가 있는 것인지, 아니면 정말 순수하게 기여 차원인지 알고 싶음
          + 영어 외 언어도 지원하는 모델이 필요함
          + P(.|photo) vs P(.|minimal) 예시에 대해 실질적으로 이 충돌을 어떻게 결정하는지 궁금함. 내 생각엔 사진 실사주의가 기본값이 되어야 한다고 봄. 예를 들어, 사용자가 ""책을 읽는 고양이""라고 쓰면, AI 스타일이나 그림체가 아닌 실제 고양이가 책을 읽는 모습이 나오는 게 맞는 것 같음. 별다른 맥락이 없으면 '고양이'는 실사 고양이로 인식하는 게 당연하다고 느껴짐. 만약 사용자가 일러스트 등 다른 스타일을 원하면 프롬프트에 명확히 적어야 하는 게 맞지 않을까 싶은데, 혹시 내가 놓치는 뉘앙스가 있는지 궁금함
     * 좋은 공개임. 12b Txt2Img Krea 모델로 간단히 테스트해 봤음. 가장 뛰어난 점은 빠른 속도(그리고 아마도 리얼리즘)임. 다만 unsurprisingly, <i>prompt adherence</i> 면에서는 일반 Flux.1D 모델보다 더 높은 점수는 못 받았음. 결과는 https://genai-showdown.specr.net에서 볼 수 있음. 한편 Wan 2.2+ 버전이 앞으로 T2I 분야에서 큰 역할을 할 가능성이 보이는데, 이미지 다양성 부족을 메우려면 LoRA가 엄청나게 필요할 수 있음
          + 테스트한 결과를 볼 수 있는 URL을 알려줄 수 있는지 궁금함. 그리고 참고로, 이 모델은 <i>aesthetics</i>에 더 초점을 맞췄지 프롬프트 정확성만을 고집한 건 아님. 샘플이 별로인 건 변명이 아니고, 연구 목표 중 하나였음을 강조하고 싶음. 'flux look'이라 불리는 특유의 스타일을 없애고 싶으면 반드시 고려해야 할 트레이드오프임. 그리고 Wan 2.2로 베이스 이미지를 만들고 Krea로 리파인하는 사람들도 있더라, 꽤 흥미로운 방법임
     * 안녕하세요! 나는 Krea-1 FLUX.1의 책임 연구원임. Krea는 Krea-1에서 distill한 12B Rectified Flow Model이고, FLUX 아키텍처와 호환되도록 설계함. 기술적 질문 있으면 답변 가능함
          + 나는 전통적인 미디어 프로덕션 출신임. 미디어를 여러 레이어로 나눠 조합하는 방식이 비용관리, 품질관리에 핵심임. 그런데 현재 AI 이미지, 비디오, 오디오 생성 방식은 이런 게 지원되지 않음. ForgeUI가 잠깐 지원했지만 중단됨. 실제 대규모 미디어 제작 요구사항을 이해하지 못해서라 생각함. 실제 영화 VFX, 애니메이션 광고, 수백만 달러 규모 제작 경험자가 팀에 있는지 궁금함. 만약 성공하고 싶다면, 꼭 전통 미디어 제작 방식을 지원해야 함. 기존 AI 툴들은 프로덕션 툴이나 기대와 전혀 연동이 안 되기 때문에 현장에 채택되지 못하고 있음
          + 모델 퀄리티가 정말 훌륭함. 특히 ""flux-dev-raw가 guidance distilled model이므로 맞춤 손실함수를 만들어 classifier-free guided 분포에 직접 파인튜닝했다""는 부분이 인상적이었음. 여기에 대해 자세한 설명과 파인튜닝 팁이 있다면 듣고 싶음. 오픈소스 AI 아트 커뮤니티에서도 original distilled flux-dev 파인튜닝이 매우 어려워서 궁금함
          + 이런 노력에 정말 감사함. ""FLUX 아키텍처와 호환되도록 설계했다""는게 무슨 의미인지, 그리고 왜 중요한지 설명해줄 수 있는지 궁금함
     * 23.8GB 크기의 safetensor 파일이 12B 파라미터 모델 기준으로 이해가 잘 안 됨. 1B 파라미터는 1GB VRAM이 필요하다고 생각했는데, 이 모델이 24GB VRAM을 쓰는 건지 12GB를 쓰는 건지 궁금함. 내 생각이 틀린 것인지 알고 싶음
          + bfloat16으로 계산하면 1B x 16bit = 2GB라서, 12B면 거의 24GB가 맞음. float32를 bfloat16으로 내리면 성능 손실 거의 없으니 bfloat16으로 업로드한 것임
          + 파라미터별로 float 크기가 다름. 여러 모델들은 FP8(8bit/파라미터)로 배포되지만, 이 모델은 FP16(16bit)임. 종종 FP16으로 학습 후 FP8이나 FP4로 양자화해 배포하는 게 많음
          + 8bit로 양자화한 모델은 1B=1GB로 볼 수 있지만, 16bit, 32bit는 그 2~4배를 필요로 함
     * 간단한 프롬프트로 신기한 결과를 얻었음: ""Octopus DJ spinning the turntables at a rave."" DJ 에게서 나타나는 사람 손이 인상적임. 아무리 프롬프트를 줘도 이 손을 제거하지 못했음. 논문에서 언급한 것처럼 확실히 opinionated함
          + ""Octopus DJ with no fingers""라고 프롬프트를 넣으니 손은 사라졌지만, 동시에 문어의 모든 사람 특성도 없어져서 순수 문어만 턴테이블을 돌리는 모습만 남았음
     * 내가 늘 원하는 이미지는 Galton board임. 상단에 조금 떨어진 두 개의 구멍에서 공이 떨어지고, 하나는 파란 공, 하나는 빨간 공. 아래에서는 두 색이 합쳐진 분포로 칼럼이 이중 정규 분포임을 보여줌. 참고 이미지: https://imgur.com/a/DiAOTzJ (탑에 두 개 스파우트). 실제 시도 결과: https://imgur.com/undefined, https://imgur.com/a/uecXDzI
          + 직접 현실에서 만들어 본 적이 있는지 궁금함. double Galton board의 영상을 찾지 못했음
     * hey hn! 나는 Krea의 공동 설립자임. FLUX Krea를 어떻게 트레이닝했는지 정리한 블로그 포스트가 있으니, 더 자세히 알고 싶다면 참고 바람: https://www.krea.ai/blog/flux-krea-open-source-release
          + 주제에서 벗어난 질문이지만 진짜로 웹사이트에서 스크롤 바를 숨긴 것인지 궁금함. 왜 그렇게 했는지 이해가 안 됨.
.scrollbar-hide {
  -ms-overflow-style: none;
  scrollbar-width: none;
}

     * NVIDIA 최적화 버전을 제공하는지 궁금함. RTX 가속된 FLUX.1 Kontext처럼: https://blogs.nvidia.com/blog/rtx-ai-garage-flux-kontext-nim-tensorrt/
          + FLUX.1 Krea에는 별도 RTX 가속 버전을 만들지 않았음. 하지만 모델은 기존 FLUX.1 dev 코드베이스와 완전히 호환됨. 별도의 ONNX export는 없는 듯함. SVDQuant로 4~8bit 양자화 버전도 체크포인트를 좀 더 일반 하드웨어 친화적으로 만들어 줄 좋은 후속 과제임
     * 참고 링크 정리:
          + GitHub 저장소: https://github.com/krea-ai/flux-krea
          + 모델 기술 보고서: https://www.krea.ai/blog/flux-krea-open-source-release
          + Huggingface 모델 카드: https://huggingface.co/black-forest-labs/FLUX.1-Krea-dev
     * 회사들이 자신들이 원하는 결과물을 얻었을 때 상업적 이용 권한을 명확히 라이선스 할 수 있도록 잘 문서화된 경로 제공을 추천함(곧 알게 되겠지만!)
          + 라이선스 세부 내용은 여기서 확인 가능: https://huggingface.co/black-forest-labs/FLUX.1-dev/…. 요약하면 기존 BFL Flux-dev 라이선스와 동일함
"
"https://news.hada.io/topic?id=22294","리눅스 커널을 위한 QUIC","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            리눅스 커널을 위한 QUIC

     * QUIC 프로토콜이 리눅스 커널에 공식적으로 통합되는 첫 패치가 제출됨
     * 기존 TCP가 가진 한계점(지연, head-of-line blocking, 중간 장치로 인한 프로토콜 고착화 등)들을 개선하려는 목적
     * QUIC은 UDP 기반으로 멀티스트림 지원과 End-to-End 암호화 기능을 제공하며, 커널 도입 시 더 넓은 플랫폼 및 하드웨어 활용 가능성 높음
     * 초기 커널 구현의 성능은 기존 TCP 및 커널 TLS에 비해 낮게 측정되었지만, 향후 하드웨어 오프로딩과 최적화를 통해 성능 개선 기대 가능함
     * 현재 Samba, 커널 기반 SMB/NFS, curl 등에서 지원 논의가 활발하게 이루어지고 있지만, 메인라인 합병까지는 시간이 더 소요될 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

QUIC 프로토콜의 등장 배경 및 TCP의 한계점

     * QUIC은 기존 인터넷에서 TCP가 가진 다양한 문제점을 해결하려는 목적으로 만들어짐
     * TCP의 연결 과정에서 발생하는 3-way 핸드셰이크로 인한 지연, 멀티스트림 지원 미흡, 패킷 손실에 따른 head-of-line blocking 현상 등으로 웹 사용 경험이 저하됨
     * TCP의 메타데이터가 암호화되지 않은 채 전송되어 정보 유출 위험이 존재하고, 인터넷의 미들박스(중간 장치) 들이 연결 정보를 바탕으로 트래픽을 필터링, 그 결과 프로토콜의 고착화(ossification) 로 발전함
     * TCP 개선 시도(예: Multipath TCP 등)도 기존 TCP로 위장하지 않으면 정상적으로 작동하지 못하는 상황임

QUIC의 특징 및 기술적 장점

     * QUIC은 UDP 위에서 동작하며, 연결 과정에서 별도의 3-way 핸드셰이크 없이 빠르게 연결 설정 가능함
     * 패킷 손실이 전체 스트림에 영향을 주지 않도록 멀티스트림 전송 설계가 반영됨
     * QUIC 관련 전송 데이터는 항상 종단 간 암호화(TLS 기반) 되어, 중간 장치가 내부 메시지에 접근할 수 없게 됨
     * UDP 패킷이 통과할 수 있는 네트워크 환경이라면 QUIC 역시 정상적으로 동작 가능함

리눅스 커널 내 QUIC 통합 패치 개요

     * 제출된 패치에서는 IPPROTO_QUIC라는 새로운 프로토콜 타입이 도입되어, 기존 socket() 시스템 콜을 활용 가능함
     * TCP와 유사하게 bind() , connect() , listen() , accept() 등의 콜을 사용할 수 있지만, 이후 처리 방식에는 차별점이 존재함
     * TLS 세션 관리 및 인증/암호화 과정은 사용자 공간에서 처리되며, 연결 후에 각 단에서 TLS 핸드셰이크가 완료되어야 데이터 송수신이 가능함
     * 초기 연결 이후에는 TLS 협상 결과를 캐싱하여, 두 시스템간 재연결 시 속도를 크게 높일 수 있음

성능 측면의 과제 및 전망

     * 제출된 커널 내부 QUIC 구현은 아직 성능 면에서 기존 커널 TLS 및 TCP 대비 열세를 보임
          + 인커널 TLS 대비 3배 이하 처리량, 암호화 비활성화 시에도 TCP에 비해 최대 4배까지 처리량 저조
     * 원인으로는 세그멘테이션 오프로딩 미지원, 송신 경로 내 추가 데이터 복사, 헤더 암호화 과정 등이 지적됨
     * 향후 하드웨어 오프로딩 지원이 추가되고, 인커널 구현이 최적화되면 성능이 향상될 것으로 기대됨

채택 현황과 향후 전망

     * Samba 서버/클라이언트, 커널 SMB 및 NFS 파일시스템, curl 등 다양한 프로젝트에서 인커널 QUIC 지원 논의가 활발함
     * 패치는 약 9,000 라인 규모이며, 현재 저수준 지원 코드만 포함되어 있음. 전체 구현은 추가 패치로 예고된 상태임
     * 코드 리뷰 및 병합 논의가 이제 막 시작된 단계로, 실사용까지는 시간이 더 소요될 전망임
          + 최근 Homa 프로토콜의 커널 병합이 9개월간 11회 제출이 필요했던 전례를 볼 때, QUIC 역시 2026년 이후 메인라인 진입이 예상됨

        Hacker News 의견

     * 최근 NGINX 설정에서 ssl_preread_server_name을 추가하여 특정 도메인 요청을 다른 NGINX 인스턴스로 proxy_pass함
       첫 번째 인스턴스는 단순히 원시 TLS 스트림을 전달하고(proxy_protocol 포함), 두 번째 인스턴스가 실제 TLS 종료를 맡음
       이 방식은 장애 조치 구현 시 효과적임 - 서버의 기본 경로가 다운되면 DNS A 레코드를 장애 조치 머신의 NGINX로 업데이트하고, 해당 인스턴스가 특정 도메인 요청을 별도 경로로 원래 백엔드로 라우팅함
       전체 TLS 구성을 복제할 필요가 없어서 편리함
       단, 이 방법은 HTTP/3에는 적용 불가
       HTTP/3는 QUIC 기반이고, UDP 위에서 동작하며 핸드셰이크 시 SNI를 암호화하므로 ssl_preread_server_name로 도메인 기반 라우팅 불가
       HTTP/3에서 SNI 기반 라우팅을 지원할 대안이 있는지, 아니면 이 기능이 필요하다면 여전히 HTTP/1.1이나 HTTP/2 over TLS로 유지해야 하는지 궁금함
          + QUIC을 지원하는 클라이언트 대부분은 HTTPS DNS 레코드도 지원하므로, 낮은 우선순위 레코드를 장애 조치로 추가해 클라이언트에 일임하는 방법이 있음
            실제로는 클라이언트 구현에 따라 동작이 다르지만(Chromium의 HTTPS 레코드 지원 현황 참고 이슈 링크), QUIC 접속 실패 시 클라이언트가 투명하게 HTTP/1.1/2로 폴백하며, Alt-Svc헤더 설명 헤더도 존중함
            계획된 장애 조치라면 Alt-Svc 헤더를 보내지 않고 대기하면서 대체 인스턴스로 타임아웃되기를 기다릴 수도 있음
            QUIC 라우팅이 정말 필요하다면, 다행히 SNI 정보가 항상 첫 패킷에 있으므로 패킷 검사로 라우팅이 가능함
            cloudflare의 udpgrm이 참고가 될 수 있고, 이는 ECH(Encrypted Client Hello)가 없을 때 사용 가능함
            ECH가 있을 경우 라우터가 해독 키를 가져야 라우팅 결정을 할 수 있고, 프로토콜 상 캐스케이드 장애 조치도 설계 가능함
            구체적 코드 구현은 udpgrm 예시에서 확인 가능함
          + TLS를 에지(예: NGINX)에서 바로 종료하는 것은 오늘날 letsencrypt 환경에서 별 위험 포인트가 아님
            공격자가 해당 서버에 접근한다면 SSL 인증서 새로 발급하는 것도 쉬우므로, 굳이 복잡한 장애 조치 시스템을 고민하기보다는 직접 TLS 종료하는 편이 더 합리적임
            개인적으로 QUIC의 성능·신뢰성 이점을 직접 재현해 본 적 없음
            몇 년간 반복 테스트하지만, 성능 등 이유로 대부분 비활성화함
            DNS 기반 장애 조치 역시 실질적으로 반영되기까지 수 분 걸리고, 브라우저 같은 단순 클라이언트는 장애 조치가 잘 안 이루어짐
            그래서 직접 onerror 핸들러를 써서 두 번째 경로를 로드하는 방식 사용함
            예를 들어 광고 추적 용도로 이런 식의 코드 사용하고, fetch API도 같은 방식으로 래핑해 제공함
            이 방식이 다른 어떤 시도보다 훨씬 효율적임
          + 장애 조치 환경이라면 QUIC 장애 조치는 신경 쓰지 않음
            브라우저가 QUIC 연결에 실패해도(심지어 DNS에 광고돼 있어도) HTTP/1 혹은 HTTP/2 over TLS로 자동 폴백하므로, 기존 장애 조치 방식을 동일하게 사용 가능함
          + 이 문제는 ""버그가 아니다"" 유형에 해당함
            HTTP/3의 설계 특징이, TLS 단까지 엔드포인트 정보를 노출시키지 않는 것임
            개인적으로는 이 점을 장점이라고 생각함
            HAProxy는 원시 TLS 프록시 가능하지만, 호스트명 기준 라우팅은 불가함
            Cloudflare tunnel은 TLS 종료 없이 호스트명 기반 라우팅이 가능한 특별한 기능이 있는데, 이를 사용하려면 DNS도 Cloudflare로 연결이 필요함
            관련 xkcd 만화에서 표현한 것 참고함
          + 흥미로운 질문임
            TCP+TLS 환경에서 Encrypted Client Hello가 사용되는 경우에도 같은 한계가 존재하는지 의문임
            답변은 거의 동일하리라 생각함
     * QUIC의 단점에 대한 관련 글을 예전부터 기억함
       이번 논의는 이런 문제를 조금씩 해결하는 방향이라고 느껴짐
       앞으로 네트워크 카드의 하드웨어 지원 가능성도 열려 있음
          + QUIC은 머신 간 트래픽에는 별로 적합하지 않음
            하지만 요즘 인터넷 트래픽 대부분이 모바일과 서버 간에 오가기 때문에, 그 구간에서는 QUIC과 HTTP/3가 진가를 발휘함
            그 외 용도에서는 TCP 계속 활용 가능함
     * 멀티 스트림용 소켓 API가 어떻게 나올지 궁금함
       기존처럼 여러 커넥션으로 보이지만 내부적으로는 캐싱된다는 식일 듯함
       나는 명시적으로 커넥션 객체를 받아서 스트림을 따로 여는 식이면 좋겠지만, 일단 현재 방식도 수용 가능함
       관련 논의 확인 결과, 이게 확장 기능이 아니라면 서버 쪽에서도 커넥션 수립 후에 새 스트림 생성이 가능함
       클라이언트에서 실제는 스트림이지만 ""커넥션""처럼 분리 추상화가 어렵고, 근본적으로 완전 새로운 API 추상화가 필요해 보임
       아마 새로운 스트림마다 recvmsg로 파일 디스크립터 받는 구조 예상함
          + 다중 스트림 기능은 SCTP 소켓 API가 지원하므로 SCTP 인터페이스 참고할 만함
     * 커널 구현에 상관없이, OpenSSH에 QUIC 지원이 있으면 좋겠음
       모쉬(Mosh)처럼 네트워크 문제에 강하면서 OpenSSH의 모든 기능(SFTP, SOCKS, 포트포워딩, 스테이트 관리, 로밍 등)도 그대로 쓰고 싶음
       과연 OpenSSH가 커널 지원을 활용할 수 있을지 궁금함
       Mosh 보기
          + SSH는 암호화와 멀티플렉싱 레이어를 QUIC으로 완전히 대체해야 하므로 대규모 작업 필요함
            차라리 QUIC 기반 별도 로그인 프로토콜을 새로 만드는 게 나을 듯함
            여러 접근이 프로토타입 단계로 진행 중임
          + OpenSSH는 OpenBSD 프로젝트라서, Linux API에 크게 신경 쓰지 않을 가능성이 있음
     * TCP의 병목이 핸드셰이크 때문이라고 하는데, 커넥션 재사용이나 멀티플렉싱으로 해결된다고 알고 있음
       그런데 현재 QUIC 커널 구현이 리눅스 대비 3~4배 느리며, 성능 차이도 조만간 좁혀질 거라는 얘기 있음
       속도가 QUIC의 장점인데 실제로 더 느리다면 QUIC을 쓸 이유가 무엇인지 궁금함
       PR의 작성자 말로도 일부 성능 저하의 원인이 프로토콜 설계에 있다는데, 혹시 TCP에서 별도로 고칠 문제가 더 있는지 질문함
          + 기사에서도 QUIC이 느린 원인을 많이 언급함
            대부분 ""아직 최적화를 안 했다""로 요약 가능함
            예컨대 세그먼트 오프로드 미지원, 트랜스미션 경로에서 추가 데이터 복사, 헤더 암호화로 인한 오버헤드 등이 있고, 모두 해결 가능성 높음
            여기서 벤치마킹은 매우 이상적인 환경에서 했음
            현실적으로 모바일 환경은 네트워크 변동성이 크기 때문에 TCP의 구조적 한계가 두드러짐
            실제로 HTTP/2처럼 TCP 위에서도 이미 QUIC과 유사 기능을 구현하는 경우 많음
            결국 QUIC은 OSI 5계층 이상에서 동작하는 종합적 네트워킹 스택이고, TCP는 3계층 수준 엔진이라 구조적으로 비교가 힘듦
            무엇보다 QUIC은 더 신속한 커넥션 연결·재연결이 장점이고, IP 변동 시에도 세션 연속성을 보장함
            멀티플렉싱·비차단 스트림 구조는 상위 프로토콜 설계를 획기적으로 단순화함
            이런 구조가 커널에 들어오면 성능 최적화 여지도 엄청 큼
            앞으로는 TCP의 한계 위에서 다층 솔루션 만들기보다는, 일상적으로 QUIC 같은 진보된 기반 기술을 더 많이 써야 함
          + TCP의 병목은 핸드셰이크만이 아님
            패킷 손실이 생기면, 그 이후 전송분 전체가 복구될 때까지 지연되므로(HOL blocking) 구조적 한계가 큼
          + ""존재하지 않는 커넥션""을 재사용할 수 없으므로, 많은 논의가 레이턴시(지연) 단축에 초점 맞춤
            그냥 속도 문제가 아니라 지연 개선임
          + QUIC의 이점은 서버 제공 connection ID 기반 추적 기능임
            기술 설명 문서 참고
     * 네트워킹 스택을 유저스페이스로 옮겨서 성능을 높인다는 메시지와, 커널 쪽으로 옮기자는 이번 논의에서 혼란이 듦
          + 대부분의 QUIC 스택은 커널 내 UDP위에서 동작함
            커널과 사용자 공간 간 컨텍스트 스위치가 주요 병목임
            유저스페이스 네트워킹(예: NIC 직접 접근)은 커널 진입을 없애줌
            반대로 커널 공간에서 기능 제공(예: sendfile, in-kernel TLS, NIC 오프로딩, 디스크에서 NIC로 바로 DMA)도 전체 컨텍스트 스위치와 데이터 복사를 줄임
            현재 QUIC 스택은 양쪽 장점을 모두 활용하지 못함
            패킷 입출력이 syscall 기반이고, 데이터 복사를 피하지 못함
            io_uring 등으로 배치 IO하면 스위치는 줄지만, 복사 자체는 줄이지 못함
          + 맞는 말임
            커널 바이패스+DMA, 혹은 sendfile/ktls 구조처럼 사용자 공간 제외 등 두 가지 방식이 있음
            Quic 커널 구현에는 양쪽 이점이 다 없음
          + 결국 NIC 버퍼로 데이터를 넘겨야 하는 건 같은 구조임
            DMA로 니팅 직접 쓸 수 있거나, 커널 syscall로 넘긴다면 성능 차이가 크기 때문에
            사용자 공간 네트워킹은 이런 privilege 전환 및 DMA 구조가 있을 때만 설득력 있음
          + 유저스페이스가 직접 네트워크 데이터 접근(아마도 syscall 없이)하는 방식이 적용되는 것은 일반 사용자 소프트웨어에는 별 의미 없음
            대규모 기업(MOFAANG 등)에서만 활용
            이론적으로 io_uring이 위 혜택을 일반화할 것이란 기대가 있지만, 아직 실사용 단계는 아님
          + 하드웨어 접근 시 컨텍스트 전환이 너무 느림
            그래서 TCP/IP는 주요 OS에서 커널에 남아 있음
     * ""왜 커널에 점점 기능을 더 넣는가?""
       커널은 메모리, 하드웨어, 태스크 관리가 역할이라 생각했는데, IP 위 프로토콜은 유저랜드에서 처리해야 옳지 않은가 질문함
          + 네트워킹, 라우팅, VPN 등을 모두 커널 공간에서 처리하면 일부 사용 사례에서 성능 개선됨
            반대로 이런 스택을 유저스페이스로 분리해도 일부 사례는 성능 개선 효과 있음
          + 커널에 들어가면 하드닝(보안성), LTS 지원, 그리고 커널 및 네트워크 수준 최적화의 장점이 있음
          + 주된 이유는 DMA 트랜스퍼 및 NIC 오프로딩 최적화임
          + IP 위 프로토콜 전체를 유저스페이스에 두면, 멀티프로세스 사용이 불가능함
            TCP/UDP가 커널에서 포트기반으로 소켓 라우팅을 중재하기 때문에, 여러 프로그램이 동시에 TCP/UDP 사용 가능함
            QUIC은 UDP 위에서 동작하며, 따라서 논의의 요지는 여전히 설득력 있음
            IP 바로 위 프로토콜만 유저스페이스에서 못 돌린다는 점 강조함
     * QUIC은 많은 이들에게 획기적 변화를 제공한다고 생각됨
       앞으로 인터넷이 좀 더 빨라질 것이라고 기대함
       5G 같은 통신환경에서는 차이가 안 느껴질 수도 있지만, 가치 있는 발전임
       별도 링크 핸드셰이크 구조가 있다는 게 신기함
       원래 QUIC이 TLS를 자체 내장하는 줄 알았던 것과는 달랐음
     * 웹 전체가 느려진 주된 이유는 웹사이트가 너무 무거워져서임
       그래도 게임 레이턴시는 이번 기술로 더 낮아질 수 있다고 생각함
          + 제번스의 역설이 여기에도 적용됨
            컴퓨팅 리소스와 네트워크 효율이 올라가면 수요 자체도 늘어남
            게임이나 과학연산에서는 ""더 나은 결과""를 위해 상관없지만
            웹에서는 광고, 추적, 자바스크립트 증가로 인해 역효과가 많음
     * 기사에서 QUIC은 TCP처럼 bind(), connect(), listen(), accept() 등으로 커넥션을 수립하지만, 이후 sendmsg()와 recvmsg() syscall을 이용한 구조로 달라짐
       왜 이 접근방식을 채택했는지, 왜 QUIC에 맞는 별도 시스템콜을 만들지 않았는지 설명도 있었으면 좋겠음
"
"https://news.hada.io/topic?id=22334","유엔 보고서, 유엔 보고서의 낮은 독서율 밝혀","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       유엔 보고서, 유엔 보고서의 낮은 독서율 밝혀

     * 유엔 보고서에 따르면 내부적으로 유엔이 발행하는 보고서의 독서율이 매우 낮음
     * 유엔 내 서류 작업이 많지만 실제로 읽히는 보고서는 적음
     * 이에 따라 정보 전달 효과성에 대한 의문이 제기됨
     * 보고서 발행 방식의 효율성 개선 필요성이 강조됨
     * 변화 없이는 자원 낭비 문제가 지속될 가능성 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

유엔 보고서의 낮은 독서율

     * 최근 유엔의 자체 보고서에서 유엔이 직접 발행하는 다양한 보고서들이 상당히 읽히지 않는다는 사실이 드러남
     * 내부적으로는 정책 결정자와 담당자들이 수많은 문서와 보고서를 작성하고 있으나, 실제로 읽거나 참고하는 경우는 매우 드문 상황임
     * 이로 인해 유엔의 지식 전달 과정이 비효율적이라는 우려가 등장함
     * 주요 권고안으로는 보고서의 양을 줄이고, 실제로 필요하고 활용도가 높은 정보 중심으로 전달 방식을 재구성하는 것이 있음
     * 변화가 이뤄지지 않는다면, 유엔의 자원 활용이 계속 비효율적일 가능성이 제기되고 있음

        Hacker News 의견

     * archive.md의 해당 UN 보고서 자료 링크 공유함
     * 나 같은 경우, 기관이나 개인들이 문제를 실제로 해결하기보다 문제에 대해 이야기만 하는 경향이 크다고 느끼는 중임, 또 하나의 특징은 도움이 필요한 사람을 ‘적합한 리소스’로 안내해주는 방식인데, 이 ‘적합한 리소스’ 또한 계속해서 다른 곳으로 안내만 하는 서비스임, 결국 도움을 요청한 사람은 끝없는 안내의 늪에 빠지는 셈임, 이런 사람들 모두가 만약 안내를 그만두고 급식소 자원봉사라든지 어르신 방문 봉사처럼 실제적이고 물리적인 액션을 한다면 모두에게 더 유익할 거라 생각함, 이런 현상은 ‘세상을 바꾼다’는 개인의 스케일 욕구가 일정 부분 원인이라고도 할 수 있음, 세상을 바꾸는 일만 높이 평가하면서 그에 따른 비용은 전혀 신경 쓰지 않는 분위기임
          + 나 같은 경우 급식소 봉사도 많이 해보고 공공 정책 리서치 같은 것도 해본 경험이 있음, 그런데 내 생각은 위와 정반대임, 대부분의 급식소 봉사는 임팩트가 매우 낮은 경험임, 많은 사람이 자원봉사를 생각하면 제일 먼저 떠올리는 게 바로 급식소임, 소외 계층과 소통할 수 있다며 긍정적으로 보지만 실제로는 교회 단체 같은 단위로 몇 번 오고 국자질하다가 끝임, 물론 직접 급식소를 운영하는 것은 임팩트가 다르지만, 유엔이 하는 일은 수백만 명에게 영향을 주는 거대 스케일임, 예를 들어 UN 식량 프로그램에 참여하면 국자가 아니라 트럭 단위로 식량을 다룸
          + UN은 전혀 다른 규모에서 독특한 일을 하고 있음, 보고서를 그리 많이 읽지 않더라도 충분히 중요하고 강한 영향력을 가질 수 있음, 내가 10년 동안 여러 홈리스 셸터에서 급식소를 운영해봐서 아는데, 일반적으로 음식 조리/서빙과 실제로 식자재를 조달하고 공급망을 관리하는 일 사이에는 엄청난 차이가 있음, 전자는 한두 시간이면 끝나지만 후자는 9개월이 걸리기도 함, UN이나 다른 국제기구도 마찬가지임, 엄청 복잡한 시스템을 다루면서 실제로 실질적인 일을 하고 있음, 비록 한 접시의 음식을 주는 것처럼 시각적으로 보이지는 않더라도 말임
          + “문제에 대해 말하는 것”과 “실제로 문제를 해결하는 것”을 헷갈리는 사람이 많은 듯함, 이건 특히 ‘진보적 마인드셋’에서 자주 보인다고 생각함, 좋은 아이디어를 갖는 것 자체는 좋지만, 반드시 행동이 따라와야 의미가 있음, 아니면 그냥 공허한 제스처일 뿐임
          + 사실 UN의 본래 목적은 문제를 논의하고 불만을 표출하는 장이 되는 것이었음, UN이 만능 문제 해결 기관이나 어설픈 세계 정부처럼 출발하지는 않았음, 그런데 UN은 점점 굼뜬 관료주의 조직이 되어가고 있음, 좋은 의도가 종종 있긴 하지만 실제로 실현되지 못함, NGO 허브와 에이드 분배 중개소 역할이 많음, 물론 이 역할도 괜찮긴 하지만, UN의 부패, 부실, 비효율 등 역사적 전례는 신뢰를 깨는 부분임, 그리고 UN이 오픈 포럼이라는 특성상, 문제가 많은 정권의 대표들이 각자의 독특한 주장(예시: 카다피 등)을 펼칠 수 있다는 문제도 있음, 또 중국의 급부상과 러시아의 쇠퇴가 험악한 판도를 만들고 있음, 개인적으로 최악이라고 생각하는 건 이란 같은 국가가 인권이사회 의장을 맡는 현실임, 대체제가 있다면 좋겠지만 아무리 생각해도 완벽한 대안은
            없는 듯함, 결국 지금의 UN 모습은 전 세계의 현상을 그대로 반영하는 거라고 봄, 미국은 행정부에 따라 완전히 방향이 달라지고, 서유럽은 그대로 굼뜨고, 러시아는 강압적 방식, 중국은 대내외적으로 복합적이고, 중동국가들은 엄청난 부로 모든 것을 왜곡함
          + 또 다른 요인도 있음, 예를 들어 어떤 사람이 보고서가 편향됐을 것이라고 미리 가정하면 아예 읽을 시도도 하지 않게 됨, 정치 토론에서 서로 입장이 다르면 미리 상대를 오해하는 것과 비슷한 현상임
     * 오늘날 ‘일’과 똑같이 느껴짐, 특히 빅테크에서의 경험과 일치함, 예전에 환경 관련 NGO와 비영리단체 웹사이트 수백 개를 스크래핑한 적이 있었음, 상당수가 유엔 산하 또는 연관이 있었음, 세 가지를 알아내고 싶었음: 1) 무슨 일을 하는지, 2) 무슨 결과물을 내는지, 3) 실제 성과가 뭔지, 직접 크롤링하고 수작업으로 검증까지 했는데 정보 추출이 정말 힘들었음, 공개 자료(특히 웹)로는 거의 투명성이 없음, 장소 기반 웹사이트들도 마찬가지로 주소 등 기본 정보를 숨김, 이런 노력과 보고서들도 외부에서 감사를 받는다면 비슷한 불투명성이 드러날 것 같음
          + 이런 질문이 무지하게 보일 수도 있는데, 매번 이런 보고서나 설문조사가 나올 때마다 왜 전체 데이터를 공개하지 않는지 궁금했음, 혹시 자신들도 그 내용이 사실은 ‘얼마나 혼탁한’지 들킬까 두려워서 공개하지 않는 것인지 궁금함
          + 나 같은 남성 유방암 생존자로서 Susan G Komen Foundation의 실제 실적을 파봤더니 얼마나 허무맹랑한지 알게 됐음, 미국 비영리단체 전반적으로 실망스러움, 부끄럽게 느껴짐
          + NGO라는 건 엄청 광범위한 범주임, “사기꾼 조직”이 없는 건 아니지만, 그만큼 정말 높은 임팩트를 가진 org들도 있음, 예시로 AMF 같은 곳도 있음
          + UN과 NGO를 같은 선상에서 비교하는 것은 공평하지 않다고 생각함, UN은 국가 간 외교 플랫폼이라는 역할임, 그 본질상 매우 ‘프로세스 중심’이면서도 진전이 더딜 수밖에 없음, 이해관계가 완전히 어긋난 국가들의 모임이기 때문임, 반면 사업가 집안 자녀가 만든 NGO는 완전히 다름
          + NGO는 정치자금 세탁용으로 많이 쓰임, 이는 자본이 어떻게 정책 집행(혹은 거부)에 영향을 주는지 보여주는 대표적인 수단임, 매우 강력한 도구임
     * 실제 다운로드 수가 중요한지는 잘 모르겠음, 중요한 것은 '독자 수'보다는 ‘임팩트’임, 많은 보고서가 소수, 좁은 타깃을 대상으로 작성되는 경우가 많음, 만약 핵심 정보가 영향력 있는 의사결정에 기여한다면 그 정도로 충분함
     * 사실 이런 결과는 예견 가능한 것임, 나는 UN 보고서를 직접 읽지 않고, 기자들이 읽고 요약해줄 거라 기대함, 논문도 내가 직접 안 읽고 기자가 전달해주길 바람, 통과되는 법률의 세세한 내용도 읽지 않고 정치인과 변호사가 알아서 토론할 거라 기대함, 결국 내 ‘기대’가 채워지지 않는 현실임, 내가 모든 것을 일일이 챙길 시간도 없음
          + 사실 논문도 몇백, 많아야 몇천 명 정도가 잠재적 독자인 경우가 많음, 실제로는 아무도 안 읽는 경우도 많음, 이메일 쓸 때도 보통 한 명만 읽는 걸 기대함, 그래서 특별한 문제라고 생각하지 않음
          + TFA의 주장에 따르면, 대다수의 UN 보고서가 5,000회 미만 다운로드였고, 1:1 비율로 봐도 기자/리포터들이 실제로 이런 보고서를 읽고 전달하지 않을 확률이 높다고 함, 이게 문제인지 여부는 별도의 논의겠지만, UN 디지털 라이브러리에서 초반 몇 페이지만 훑어봐도 대부분 메타적 내부 보고서임 해당 UN 보고서 목록
          + 연구 논문을 기자가 풀어서 설명해주길 바란다는 기대를 한다는 말인가, 좀 의외임
          + 시민이 보고서를 직접 읽지 않는 건 자연스럽지만, 정치인조차 안 읽는다는 말 아닌가?
          + UN 보고서는 사실 기자에게 그닥 쓸모 없음, 대부분 각국의 인기투표 같은 형식임, 보고서 내용은 결국 보고서를 작성하는 국가나 해당 그룹의 다수 국가의 이익을 반영할 뿐임, 논문처럼 ‘진실’이 목적은 아님, 외교관에게는 ‘조용한 왕좌의 게임’ 같은 역할이 남아있음, 그 외에는 아무도 신경 안 씀
     * 왜 이런 보고서들이 널리 읽혀야 하는지 모르겠음, 이런 리포트의 경우 ‘누가 읽는가’가 중요함, ‘얼마나 많이 읽는가’보다 말임, 나 역시 UN 활동과 전혀 직접 관련이 없고, 영향도 없음, 굳이 내가 그 보고서를 읽을 이유가 없음
          + 하지만 본업이나 취미와 겹치는 경우도 있을 수 있음, 나는 UN 자료를 직접 찾아보진 않았지만, 업무용 리서치 서비스가 관련 자료를 찾아 분류해주곤 했었음, 이런 경우 기껏해야 한두 명만 읽더라도 그 몇 명이 필요로 한다면 충분히 쓸모 있는 결과임, 이전에 딱 한 명을 위해 연구 조사한 경험도 있었는데 내 시간/연봉만큼 가치는 충분했음, 만약 결과물이 아무 데도 공유되지 않고 프린트로 한 번만 전달됐다면 더 적절하게 보였을 수도 있음, 공공저장소에 올려둬도 다운로드 수가 없어 보이면 볼품 없어보이겠지만, 실상은 다름
     * UN 시스템이 작년에 27,000개의 회의, 240개 기관, 1,100개의 보고서를 지원했다는 사실이 인용됨, 관료제는 확장되는 관료제의 필요를 충족시키기 위해 또 확장하는 셈임
          + 그게 바로 민주주의가 실패하는 원인 아니냐고 묻고 싶음, 너무 피곤한 구조임, 사실상 월급 받는 사람을 위한 일로 변질됨, 정부가 일반 국민들에게 너무 큰 부담을 지워 모두를 소외시키는 시스템임, 민주주의가 사라지는 ‘어둠의 시작’ 같음
     * 기사 제목이 Onion 기사 같은 느낌임
          + 아직 2012년 이그노벨 문학상만큼은 아닌 것 같음, 그 상은 “미국 정부 회계감사원이 ‘보고서들에 대한 보고서들’에 대한 보고서를 내놓고, 또다시 그 보고서를 준비하라고 권고한 사건”으로 받았음
     * 사실 나도 이 보고서에 관한 보고서는 직접 읽지 않았음, 하지만 그 내용에 관한 정보를 간접적으로 접하긴 함, 이건 큰 문제가 아니라고 느낌, 마치 HN의 ‘new’ 섹션과 같음, 정말 중요한 혹은 흥미로운 건 한두 번만 조회돼도 결국 세계적으로 확 퍼지게 됨
"
"https://news.hada.io/topic?id=22247","아이폰 16 카메라 vs. 전통적인 디지털 카메라","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      아이폰 16 카메라 vs. 전통적인 디지털 카메라

     * 아이폰 16의 카메라 성능이 전통적인 디지털 카메라와 비교됨
     * 최신 스마트폰 센서 기술이 기존 카메라 기술 수준에 근접함
     * 실생활 촬영에서 사용 편의성이 중요한 경쟁 요소로 부상함
     * 전문가 및 취미 사용자 각각의 요구점이 다르게 드러남
     * 시장 변화로 인한 디지털 카메라 산업의 대응 필요성 대두됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

아이폰 16 카메라와 전통적인 디지털 카메라의 비교

  기술적 진보

     * 아이폰 16은 최신 이미지 센서와 소프트웨어 알고리듬을 도입하여 사진 품질을 크게 향상함
     * 전통적인 디지털 카메라보다 촬영 프로세스 자동화가 보다 진일보함

  사용성 및 접근성

     * 스마트폰 카메라는 즉각적인 접근성과 편리함으로 일상 촬영에 널리 사용됨
     * 제어 옵션에서 디지털 카메라가 전문적인 커스터마이징 기능을 제공함

  사용자별 요구

     * 일반 사용자는 편리함과 소셜 미디어 공유를 선호함
     * 사진 작가와 전문가 집단에서는 수동 제어와 렌즈 다양성을 여전히 중시함

  시장 및 산업 변화

     * 아이폰과 같은 스마트폰의 발전으로 인해 소형 디지털 카메라 시장이 축소되는 경향이 높아짐
     * 이에 따라 카메라 제조사들은 전문가용, 고급 DSLR, 미러리스 시스템에 집중하는 경향을 나타냄

  결론

     * 스마트폰 카메라는 광학 및 이미지 품질에서 기존 카메라에 근접하는 추세를 보임
     * 각 기기의 강점과 취약점이 명확히 구별되며, 사용 목적과 기대에 따라 선택이 달라지는 현상임

        Hacker News 의견

     * mkbhd의 10년간 스마트폰 카메라 대결 영상을 보면, 요즘 사람들이 선호하는 방향을 알 수 있음
       1: a/b 테스트에서 거의 모든 사람이 더 생생한 사진을 선호함
       2: 예전처럼 “내 눈으로 보는 것과 비슷한 사진”을 추구하는 사람은 극히 드물고, 이제는 거의 그렇게 찍지 않음
       3: 제조사들도 이런 경향을 잘 알고 있어서, 대부분 사람들이 “광대처럼” 보이기 직전까지 채도를 높이고, 얼굴 보정 기능을 입혀 나머지 부분은 훨씬 더 생생하게 만드는 방식에 집중함
       여기에, 주 시장에 맞게 문화적으로 허용된다면 얼굴 필터, 피부 보정, 심지어 V라인 효과까지 추가하는 경우도 많음
       이 현상은 음악에서의 “라운드니스 워”처럼 비교할 수 있겠음
       결국 우리 모두가 “정확한 사진” 대신 “더 멋진 사진”을 원한다고 판단한 이후로 이 경쟁은 바닥을 향해 달려가고 있음
          + 위 의견에 전적으로 동의하며, 아이러니한 점은 진정한 사진 애호가, 아마추어, 전문가들은 원래 카메라의 정확성이 한계에 있음을 오히려 이해하고 있다는 점임
            우리가 포착하는 것은 완벽하지 않은 렌즈, 코팅, 센서를 거친 세상의 해석이고, 거기서 예술적 창작이 이루어짐
            물론 정확한 재현도 목적이라면 그에 맞는 비싼 장비가 존재하지만 일상적이지 않음
            카메라의 불완전함은 오히려 예술로 이어지는데, 스마트폰 제조사들은 이마저도 심리적 만족을 위해 극대화하면서, 실제로는 스킬이 더 필요한 정확성에 투자하지 않고 쉬운 길을 선택함
            그러나 쉬움만으로는 창작자가 오래도록 자랑스러워하거나 남들이 그 가치를 인정할 만한 예술이 탄생하기 어려움
     * 핵심 차이는 1. 초점 거리/촬영 위치 2. 색상 처리임
       하지만 기사가 이 두 부분에서 모두 약한 설명을 하고 있음
         1. 왜 서로 다른 초점 거리로 비교했는지 불명확하고, 동일 화각(예: 24mm)을 사용하면 관점이 거의 같아짐
         2. 색상 처리는 매우 주관적이며, 스마트폰이나 카메라 모두에서 비활성화가 가능함
            스마트폰은 센서가 더 작아 노이즈가 많고, 더 많은 보정이 필요하며, 얕은 심도도 어렵고, 그립감 등 사용성도 다름
            그런데 기사에서 강조한 차이점들은 실제 중요한 차이를 잘 못 짚은 것 같음
          + 나는 이 기사에서 차이점을 굉장히 잘 보여줬다고 생각함
            컬러 캘리브레이션 인증받은 27"" 모니터로 보니 피부 톤만 봐도 차이가 명확함
            Apple은 사진 출처가 평범해도 소프트웨어로 최대한 보정해서 좋게 보이게 만드는데, 그래도 피부 톤 재현은 더 잘할 수 있지 않았을까 생각했음
            fish-eye 논리는 이해하지만, 왜 본질적으로 약한 피부 톤을 두둔해야 하는지 모르겠음
            소니 Xperia 시리즈처럼 최신 스마트폰도 DSLR 소프트웨어를 접목해 피부 톤 문제를 해결한 사례가 있음
            이제는 Apple에 대한 편견 없이 모든 제조사를 공정하게 평가해야 함
            “Apple은 언제나 옳다”는 태도와 비판자를 공격하는 분위기는 지루하고 무의미함
          + 아이폰과 전통적 디지털카메라의 진짜 차이는 샤프닝/에지 강화와 조명 평탄화임
            풍경이나 디테일 많은 사진, 역광 같은 까다로운 상황에선 차이가 두드러짐
            아이폰은 이런 상황에서 사진이 처음엔 더 좋아 보일지 몰라도, 확대하면 인공적이고 너무 가공된 느낌이 남
            특히 아이폰의 텍스처 렌더링이 거칠고 가짜 같아 보이는데, 이는 아이폰 11부터 ML 기반 처리 영향임
            Halide 앱의 raw 모드로 이런 인공 후처리를 피하고 직접 비교해볼 수 있지만, 최신폰에선 완전한 해상도 지원은 안 됨
            어려운 조건일수록 전통 카메라나 raw 촬영이 훨씬 유리하지만 학습 곡선이 필요함
            출력물까지 생각하면 투자할 가치가 있음
            (삼성 카메라는 과도한 사진 보정이 더 심한 경우도 있음)
          + 사진 왜곡(플레이어 어깨, 가슴이 작아짐, 인물 기울기 등)은 아이폰의 광각 렌즈 때문이라고 생각함
            아마 “1x” 렌즈를 사용했을 텐데, “3x”나 “5x”로 바꾸면 이런 문제는 대부분 해소됨
            턱선 역시 단순히 얼굴 각도나 표정 차이로도 영향 받을 수 있음
          + 나도 의견이 다르다 생각함
            캘리브레이션 된 27"" 모니터로 보면 차이가 뚜렷함
            사진 각도나 초점거리는 이미지 표현력에 큰 영향을 주지 않음
            스마트폰 카메라는 내가 보는 것을 기록한다기보단 내 삶을 공유하기 위한 도구이고, 더 화려하게 표현되어 주목을 받고 싶어하는 심리가 작동함
            더구나 작은 센서의 노이즈, 후처리 부담을 줄이기 위해 제조사들은 복잡한 소프트웨어 보정을 더해줌
            결국 결과물은 자연스러운 복잡함이 사라지고 피부톤은 강조되고, 선명하면서도 평면적으로 보이지만 때로 인공적임
            프로 카메라도 사실 후처리하지만, 그 처리 여부를 조절할 수 있고, 렌즈의 한계 보정까지 가능
            반면 아이폰을 포함한 스마트폰은 소셜 미디어 각색본이 기본임
            센서 탓이 아니라, 소니를 비롯한 고급 센서가 들어간 카메라들은 반복적으로 “최고 색상” 상을 받음
            XPeria는 작은 센서 뒤에 카메라급 파이프라인이 있어 실제 풍경과 매우 유사하게 촬영 가능함
            나도 아이폰을 메인 폰으로 쓰지만, 진지하게 사진을 찍고 싶을 때는 기본 카메라 앱 대신 Halide를 씀
            Halide는 애플 보정 우회/비활성화도 가능함
          + 초점거리 논점은 완전히 빗나감
            전통 카메라는 합당한 렌즈 선택의 자유가 있지만, 아이폰은 fish-eye로 고정돼 한계 극복을 시도함
            “공정한 비교”가 중요한 게 아니라, 전통 카메라가 더 뛰어나다는 점을 보여주는 시연임
            초점거리를 맞출 이유가 없음
     * 색상은 언제나 주관적이라 취향에 따라 호불호가 갈림
       그런데 기사에서 아이폰의 초광각 (“fish-eye”로 잘못 표기) 렌즈와 다른 카메라의 화각/렌즈를 구체적으로 명시하지 않고 비교하는 건 무의미함
       왜곡은 초점거리, 센서크기, 피사체와 거리의 조합에서 판단해야 정의가 가능함
       예를 들어 아이폰 16 Pro의 초광각은 13mm(eq)임
       같은 위치/같은 초점거리로 촬영해야 공정한 비교가 됨
     * 여기 토론이 정말 흥미로운데, 스마트폰으로 사진 찍는 사람과 카메라로 찍는 사람의 목적과 목표가 다름을 인정하는 분위기가 인상적임
       다양한 분야에서 ‘편리 vs 예술’ 경계가 있음
       예전 집과 1800년대 오두막을 비교할 때도, 오두막은 실용, 집은 장인정신과 예술성이 드러남
       아버지가 프로 사진사였는데, 같은 풍경을 찍어도 나는 단순히 “보이는 것”을 담고 그는 다양한 구성 요소를 고려해 “사진”을 만들었음
       스마트폰 카메라는 “사실상 무료”에 항상 지참하게 됨
       많은 사람들이 스냅샷을 찍지만, 일부는 정말로 구도를 잡아 예쁜 사진을 만듦
       하지만 명확히, 폰으로 좋은 사진을 찍는 사람은 훨씬 더 많은 노력을 들이고, DSLR 사용할 줄 모르는 사람은 “그냥 사진 찍으려고” 힘들어함
       결국 모든 사용자를 합치면, 시장은 가장 “대중적” 니즈에 최적화될 수밖에 없음
       한 번에 “한 버튼”으로 스냅샷을 찍는 대중 vs. 구도와 빛을 고민해 촬영하는 소수의 사진가
     * 입문급 미러리스와 번들렌즈로도 최신 아이폰을 압도할 수 있음
       좋은 렌즈를 쓰면 비교불가임
       하지만 아이폰은 언제나 주머니에 있고, 촬영 실패 거의 없음
       반면 캐논은 일정 수준의 스킬이 필요해 와이프도 관심 없고, 남이 단체사진 찍을 때도 설명 불가임
       아이폰 사진 품질이 더 떨어져도 충분히 괜찮음
       그래도 여행, 가족 사진을 돌이켜 보면 전용 카메라를 가진 가치가 충분했음
          + 아이폰 사진은 항상 성공하지만, 캐논 80D는 그냥 먼지만 쌓이고 있음
            가족 순간을 놓친 일이 너무 많아서, Android의 거의 100% 성공률이 큰 차이임
            내 실력 부족도 인정하지만, 요즘 폰 사진이면 충분하다고 생각함
          + 아이폰 카메라 장점에 완전 공감하며, 사실 이미 아이폰을 구매할 예정이었기 때문에 가격 대비 가치를 생각하면, 카메라는 무조건 추가 비용임
            사실 폰에서 카메라 뺀 별도 제품은 없으니 비교가 무의미함
          + 아이폰은 항상 가지고 다니기도 쉽고, 부담도 적음
            유명 사진가가 아내와 여행에서 힘들게 장비를 챙겨 촬영했는데, 정작 아내는 폰만 들고 자연스럽게 최고의 사진을 남겼다는 경험담이 있음
            전용 카메라의 가치는 점점 줄고, 높은 수준의 사진을 얻으려면 이제 훨씬 더 많은 기술과 노력이 필요함
            사실 프레이밍/빛 공부만 해도 폰 사진이 훨씬 좋아짐
            전문가가 아이폰으로 찍으면 아마추어의 DSLR보다 훨씬 더 멋진 결과물 나옴
          + 컨벤션에서 받은 내 코스프레 사진을 보면, 폰 카메라 사진이 대체로 더 괜찮게 보임
            사진취미인 사람들은 종종 후처리를 너무 과하게 해서 HDR 효과가 튀거나, 수동 세팅이 미숙해서 오히려 아이폰 자동모드가 더 나음
            하지만 전용 카메라 사진은 해상도가 매우 높아서 디테일 확대하면 상당히 좋고, 폰 사진은 AI 업스케일만 반복해서 확대 시 나빠짐
          + 사진계에는 이런 말이 있음:
            “내가 가지고 있는 카메라가 최고의 카메라임”
     * 나에게 아이폰 사진 특유의 “핫도그 피부톤”은 너무 과해서, 대체 어떻게 Apple이 허용하고 세대를 거칠수록 더 심해지는지 궁금함
       이런 사진은 오래 지나면 촌스러워질 것 같음
       기술적 한계 문제도 아니고, Pixel처럼 색이 훨씬 더 균형 잡히고 충실한 카메라 예시도 있음
          + Pixel 역시 한때 나를 인위적으로 태닝한 듯 나오게 만들었음
            실제로 나는 굉장히 창백한데, Pixel은 굳이 나를 태운 것처럼 보이게 함
            (물론, “더 잘 나온다” 볼 수도 있겠지만, 사실 그건 내가 아님)
            결국 딸에겐 즉석카메라를 사줬는데, 이제 애는 폰보다 그걸 더 즐겨 사용함
            미래 세대는 괜찮을지도 모르겠음
          + 아마도 사람들이 그런 처리 결과를 선호해서 후처리 알고리즘을 반복 개선한 결과라고 봄
            음악의 라우드니스 워처럼, 조금씩 더 세게 하면 순간 비교 시엔 더 좋아 보이지만, 반복되면 점점 현실을 반영하지 않게 됨
            인물 사진은 “실제처럼”보다 “멋있어 보이도록”이 더 중요하니 이런 결과가 생김
     * 나는 요즘 폰 카메라의 과도한 처리에 짜증이 남
       화면상 볼 땐 좋아 보이는데 1:1 해상도로 확대하면 이질감이 들 정도로 이상하고, 이런 조작된 이미지를 다들 너무 당연하게 받아들이고 있다는 게 신기함
       S24+로 찍은 사진도 모니터상으론 괜찮지만, PC에서 직접 보면 DSLR 사진이랑 비교가 안 됨
       심지어 10년 된 DSLR 자동 무플래시도 요즘 플래그십 폰을 완전히 능가함
          + 원인은 아마 블로그 저자가 초광각 렌즈를 쓴 것 때문임
            피사체 기울어짐, 소프트웨어의 왜곡 보정 흔적으로 보아 거의 확실함
            나도 항상 일반 렌즈만 쓰면 다른 문제 거의 없음
            카메라 설정에서 스타일 조정, AI 화이트밸런스, 조명 문제도 고칠 수 있음
          + 소비 방식이 완전히 변했기 때문임
            주변 사람 대부분이 PC 없이 스크린으로만 보기 때문에 그런 문제를 닦달할 기회 자체가 적어짐
     * 내가 아날로그 카메라를 좋아하는 이유는, 100달러짜리 필름카메라도 사진이 또렷하고, 완벽히 보정하려 들지 않고 예술/개성 면을 살릴 수 있기 때문임
       필름마다 색감과 감도, 렌즈마다 빛 표현이 달라, 내가 원하는 미학을 직접 선택할 수 있음
       그 결과 가족사진도 멋지게 찍음
     * 여행 다니면서 사진에 빠졌고, 아내도 관심 많음
       괜찮은 카메라를 사서 배울 때 즐거웠고, 직접 찍은 사진도 꽤 마음에 들었음
       그런데 결국 들고 다니는 게 큰 부담이 되고, 즉흥 사진은 시간이 너무 오래 걸림
       결국 “사진 찍으러 가는” 목적이 있어야 카메라를 썼고, 그냥 여행할 땐 폰이 더 편했음
       또 카메라로 찍은 사진은 공유하려면 따로 옮겨야 하고, 결국 Google/Apple Photos로 쉽게 찾는 폰 사진만 자주 보게 됨
       “사진작가”와 “사진을 찍는 사람”의 차이임
       난 사진작가는 아니고, 그냥 친구들과 공유하려고 사진 찍을 뿐임
       친구들은 폰에서 5초 보고 다시 안 볼 것이니, 기사에서 말하는 논점들은 이 맥락에선 의미 없음
       하지만 진짜 사진을 벽에 걸고 싶은 사진가에게는 기사 내용 모두 중요함
          + “카메라 사진이 카메라에 박혀 있다는 점”이 참 아쉬움
            왜 모든 카메라 회사가 Google Photos/iCloud/Dropbox 같은 곳으로 자동 업로드하는 기능을 아직도 제대로 못 만들었는지 의문임
            무선 전송은 너무 불편함
            그냥 WiFi에 연결해 원하는 클라우드로 백업하면 좋겠음
            아마도 이유는
               o 카메라 회사가 하드웨어 위주라 소프트웨어/클라우드 개발 역량이 부족함
               o SD카드 교체 사용이 많아 프로 유저와 맞지 않음
               o 카메라가 꺼진 상태에서도 업로드하려면 항상 전원이 들어가 있어야 함
          + 왜 둘 중 하나만 선택해야 함? 나는 아마추어 사진가이지만, 그렇다고 카메라를 매번 들고 다니진 않음
            취미로 하고 싶을 때만 카메라 챙김
            스냅샷은 폰이 더 편하고, 카메라로 찍고 싶을 땐 준비해서 정성껏 찍는 것임
            사진을 벽에 걸진 않지만, 직접 찍은 사진은 내 포토블로그에 올림
            이런 경우에도 기사 논점은 유의미함
            이 페이지 목적 자체가 스냅샷과 진짜 사진의 차이를 보여주려는 것임
            “아이폰도 충분하다”, “전문 카메라는 시간낭비다”라는 시각에 반박하기 위해서임
            여전히 “더 많은 화소=더 좋은 사진”이라는 오해가 2025년에도 남아 있음
     * 색상 보정 때문에 상처나 멍 같은 걸 찍어서 의사에게 보여주려고 하면, 오히려 “깔끔하게 보정”해버려서 원래 기록하고 싶던 흠집이 사라져버리는 경우가 많아 불편함
"
"https://news.hada.io/topic?id=22321","Substack의 브랜딩과 가짜 명성의 함정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Substack의 브랜딩과 가짜 명성의 함정

     * Substack의 90/10 수익 분배 구조는 타 플랫폼 대비 비용이 높으며, 인기가 어느 정도 오르면 더욱 부담이 커짐
     * Substack은 마치 비밀 네트워크 효과로 작가의 독자 기반을 만들어주는 것처럼 홍보하지만, 실제로는 작가가 자신의 브랜드 대신 Substack 브랜드를 키워주는 구조임
     * “Substack에서 글을 쓴다”는 식의 표현이 확산되며, 개별 콘텐츠의 정체성과 독립성이 약화되는 현상이 나타남
     * 언론 경력자들은 Substack에 권위·신뢰·프레스티지가 있다고 믿지만, 실제로는 누구나 글을 쓸 수 있는 플랫폼일 뿐임
     * “Substack에서 쓴다”는 식의 브랜딩 함정은 의도된 전략으로, 독립 작가라면 반드시 경계해야 할 문제임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Substack, 브랜드와 독립성의 딜레마

  Substack의 수익 분배와 브랜드 구조

     * Substack의 90/10 구독 수익 배분은 표면적으로는 과도하지 않지만, 타사 대비 높은 수준이며 인기가 오를수록 부담이 커짐
     * 플랫폼이 작가의 독자층을 만들어준다는 식으로 홍보하지만, 실제로는 작가가 Substack의 브랜드 가치를 올리는 셈임
     * 특히 유명 저널리스트·칼럼니스트가 Substack을 사용할수록, Substack 자체가 ‘신뢰성 있는 미디어’처럼 보이는 현상 유발

  Substack의 브랜딩 함정

     * Substack이라는 이름은 간결하고 인상적이지만, “블로그” “뉴스레터”가 아닌 “substack”이라는 독자적 브랜드명을 쓰게 만듦
     * 실제로 대부분의 Substack 뉴스레터는 플랫폼 공통 디자인이 적용되어, 개별 작가의 색채보다 Substack의 브랜드가 먼저 보임
     * Paul Krugman이 The New York Times와의 편집 스타일 이견, 콘텐츠 제한 등으로 갈등을 겪으면서 Substack에서 독립적으로 글을 쓰기 시작
          + Krugman은 Substack에서 본인의 목소리를 더 자유롭게 드러내는 창작 환경을 경험
          + 다만, Substack에서 활동하더라도 플랫폼 아래 플랫폼 구조가 존재해 개별 브랜드가 가려지는 한계가 있음
     * Terry Moran 등 오랜 경력의 언론인 또한 직장 명성 보호 심리로 Substack을 선택하는 경향이 있음
     * 이런 신문·방송 등 전통 미디어 경력자들이 Substack에서 글을 쓰는 이유는, 플랫폼에 일정한 권위와 안정감을 기대하기 때문
          + 셀프 퍼블리싱의 경우, 기존에는 신뢰가 낮았으나 Substack 플랫폼을 거치면 명성 보정 효과가 있는 것처럼 느껴짐
     * 하지만 Substack은 누구나 글을 쓸 수 있는 공간이며, 실질적 ‘프레스티지’가 부여되는 것은 아님
     * Substack의 자유로운 정책은 장점이지만, 동시에 플랫폼 자체의 명확한 위계나 인증이 없음을 의미

  브랜딩 혼란과 ‘Substack에서 쓴다’는 표현의 문제

     * NPR 인터뷰 등에서 Paul Krugman 등 유명 필자를 “Substack 소속 작가” 로 소개하는 사례가 발생하고 있음
     * WordPress, Ghost, Beehiiv, Movable Type 등에서는 이런 식의 플랫폼-작가 종속 인식이 거의 없음
     * Substack만이 “작가가 플랫폼에 종속되는 구조”를 의도적으로 만들고 있음

  결론: 독립 창작자의 경계점

     * Substack이 원하는 것은 작가의 브랜드가 Substack에 종속되는 것임
     * 진정한 독립 창작자라면 Substack의 브랜딩 함정에 빠지지 않도록 개별 브랜드와 독립성을 지키는 전략이 필요함
"
"https://news.hada.io/topic?id=22230","Debian, 2038년 문제를 기다리지 않고 전면 64비트 시간 체계로 전환","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Debian, 2038년 문제를 기다리지 않고 전면 64비트 시간 체계로 전환

     * Debian이 Y2K38(Unix Epochalypse) 문제를 사전에 차단하기 위해 차기 버전 Debian 13부터 32비트 아키텍처까지 64비트 time_t 적용을 공식화함
     * 기존 32비트 time_t의 한계로 2038년 1월 19일 이후 시간이 1900년으로 되돌아가는 현상이 발생할 수 있어, 이 문제를 더 이상 방치하지 않기로 함
     * 64비트 하드웨어는 이미 안전하지만, 임베디드, IoT 등 비용 민감한 32비트 디바이스에 Debian 수요가 남아 있어 사전 대처 필요성이 강조됨
     * 전체 6,429개 패키지에 분포한 time_t 타입을 한 번에 ABI 호환성 깨짐을 감수하고 동시 전환하는 대규모 작업이 진행됨
     * i386, hurd-i386 등 일부 레거시 지원 아키텍처는 예외로 남겨두되, 새로운 64비트 time_t 기반 x86(i686) 아키텍처 도입 가능성도 언급됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Debian의 Y2K38 버그 대응: 64비트 시간으로 전환

     * Debian은 다가올 Y2K38 또는 Unix Epochalypse 문제를 피하기 위해, 지원되는 가장 오래된 하드웨어 일부를 제외하고 모든 환경에서 64비트 시간으로 전환함
     * 이를 통해 2038년 1월 19일 발생 예정인 32비트 signed int 범위 초과로 인한 시간 값 오류를 방지함

  Y2K38 및 Unix Epochalypse 문제 배경

     * Y2K38 문제는 1970년 1월 1일 이후 경과한 초를 32비트 signed int로 표현하는 Unix 시스템에서, 2038년이 지나면 오버플로우가 발생해 시간이 1900년 등 과거로 잘못 되돌아가는 현상임
     * 이는 과거 Y2K(2000년 문제)처럼 짧은 데이터 형식을 선택한 아키텍처적 결정에서 기인함
     * Y2K 때는 개발자들의 사전 대응 덕분에 대혼란을 막을 수 있었음
     * 64비트 하드웨어용 소프트웨어는 이미 안전하지만, Debian은 임베디드, 저사양, 레거시 환경에서 여전히 널리 쓰임

  Debian의 주요 대응

     * Debian 13 ""Trixie"" 릴리즈부터 모든 주요 아키텍처에서 64비트 time_t를 기본값으로 적용함
     * 64비트 하드웨어는 이미 안전하나, 32비트 프로세서 기반의 임베디드 기기 및 레거시 하드웨어에서 문제가 잦음
     * 이러한 장비는 자동차제어, IoT, TV, 라우터 등 비용 민감하고 대량 출하되는 분야에서 여전히 사용되고 있음
     * 많은 신형 장비는 OpenEmbedded, Alpine, Android, Gentoo 등 자체 빌드 리눅스를 사용하나, Debian 기반 임베디드 기기 사용처도 향후 수년간 지속될 전망임

  적용 및 변경 사항

     * time_t 변수는 관련 코드에 6,429개 패키지에 분산되어 있어 대규모 작업이 필요했음
     * 이번 변화는 ABI(애플리케이션 바이너리 인터페이스) 호환성을 깨뜨릴 수 있으므로, 모든 관련 라이브러리와 패키지에서 동시에 조정됨
     * 유지보수팀에 따르면, 해당 작업은 완결 및 충분히 테스트가 이루어진 상태임

  예외 및 미래 계획

     * i386 포트(기존 x86)는 32비트 time_t를 계속 유지하며, 기존 바이너리 실행 호환성의 목적으로 남김
     * i686 아키텍처에 64비트 시간 및 최신 ISA(명령어 세트 아키텍처) 적용이 별도로 논의될 수 있음
     * hurd-i386 포트는 커널 지원 미비로 전환되지 않으며, 대신 hurd-amd64로 옮기는 방안이 진행 중임

  개발자 참고 사항

     * 개발자들은 자신의 소프트웨어가 64비트 시간 변수 적용에 따라 깨지지 않는지를 Debian 위키에서 안내하는 방법을 통해 테스트 가능함
     * 자세한 내용 및 기술 문서는 Debian wiki에서 제공됨

        Hacker News 의견

     * Steve Langasek가 생애 마지막 몇 년 동안 이 문제 해결에 집중하며 큰 발전을 이끌었음, 앞으로 64비트 time_t를 볼 때마다 그를 떠올릴 것임
          + 좋은 소식 다시 알려줘서 고마움, Steve를 그리워함, Joey는 Debian 활동을 계속하고 있는지 궁금함
     * Y2K 문제(2자리 연도 표현 때문에 생긴 2000년 문제)에 대해, 당시 2바이트 절약이 아주 큰 가치가 있었음, 70~90년대 소프트웨어는 빠르게 변화했으므로 5년 이상 사용될 거라 기대하지 않았음, 너무 폄하하는 관점 같음
          + 지금도 2자리 연도 많이 사용 중임, 예를 들어 신용카드 만료일(mm/yy)은 짧게 쓰기 편하므로 2자리 연도 표현을 씀, 카드 수명엔 충분하지만 2100년엔 변환 문제가 생길 수 있음, Y2K 대부분은 UI 문제에서 비롯됨(두 글자 text field, +1900 hardcode 등), 직접 겪은 Y2K 버그는 인터넷 포럼이 1999년에서 19100년으로 넘어간 것(단순 출력 오류), Y2K는 COBOL만의 문제가 아니었음
          + 이런 경우엔 ‘성급한 최적화’가 오히려 도움이 되었을 것임, 1900년에 0인 단순 int값으로 날짜를 표현했어도 더 많은 바이트를 절약할 수 있었음, 3바이트로 1900년~약 44,000일까지, 2바이트로도 ~2070년까지 커버 가능함
          + 사람들이 혼동하는 지점은 2바이트 추가가 아니라 2문자를 추가해야 했다는 것임, COBOL엔 숫자든 데이터든 문자 크기만큼 고정 width로 할당해서, 4자리 연도를 넣으려면 4개 문자 위치가 필요했음, 이런 필드 사이즈는 데이터 접근, UI, 배치 파일, 중간 파일, 전달 파일 등 전체 프로그램에 하드코딩 됨
          + Y2K 직전에 대형은행 주가 폭락을 기대하고 put 옵션을 대량 매수한 지인들이 있었으나, 실제로는 큰 일은 거의 없었음
          + 80년대 후반 COBOL로 일하면서 1자리 연도만 저장하는 프로그램 경험이 있음, 구조 설명 들었을 때는 이상하다고 생각했지만, 기록은 4년마다 자동 삭제되어서 사용상 문제 없었음, 항상 어떤 연도인지 명확했음
     * 64비트 time_t가 Epochalypse를 해결하겠지만, 모든 시스템이 단순히 64비트 초로 가는 건 아님, ext4는 이미 30비트 소수점 해상도(나노초 수준)와 34비트 초 해상도로 바뀌었지만, 역시 수백 년 뒤엔 문제 재발 예정임, 언젠가 64비트 초+64비트 소수점의 128비트 타임스탬프로 정착하게 될 것이라 예상함, 그러면 인류 역사상 예측가능한 미래는 모두 커버할 수 있음
          + 64비트 초 시간은 약 5850억 년에 해당함, WolframAlpha 계산 결과
          + 64비트 소수점 해상도로도 부족할 수 있음, 플랑크 타임에 가까워지려면 144비트까지 써야 함
          + ext4 타임스탬프 궁금했는데, zfs와 btrfs 파일시스템은 시간 처리 어떻게 하는지도 호기심 생김, btrfs는 아마 64비트 쓸 것 같고, zfs도(ext4랑 헷갈림) 비슷할 거라 기대함
     * Debian이 Y2K38, 즉 Unix Epochalypse 문제를 해결한다고 하는데, 사실은 i386을 제외한 모든 32비트 포트에서 time64_t로 이미 전환 완료함, i386만 기존 바이너리 호환성 때문에 예외이고, 나머지는 m68k 포함 모두 변경함, 직접 m68k, powerpc, sh4, hppa를 전환함
     * time_t는 변수(variable)가 아니라 데이터 타입임
          + 기사에서 나온 내용은 Debian 위키를 바탕으로 한 것임, 원문엔 “time_t가 정말 여기저기 등장함. 35,960개 패키지 중 6,429개 소스코드에 등장함. time_t를 포함한 구조체를 ABI로 노출하는 패키지는 ABI 전체를 동시 마이그레이션해야 함”이라고 명시되어 있음, 위키는 기사보다 더 명확히 설명했음
          + “For everything”은 armel, armhf, hppa, m68k, powerpc, sh4가 대상이고 i386은 아니라고 명시, i386은 미래가 없고 기존 바이너리/동적 라이브러리 구동이 주목적이라 깨지길 원하지 않음
          + “Debian 13 Trixie 출시 이후 적용 예정”은 실제로 Trixie에 포함된다는 의미임
     * 커맨드라인 길이제한도 무제한/동적으로 바꿔주면 좋겠음, 96GB 메모리인데도 “argument list too long” 에러가 자주 나서 불편함
          + 커널을 재컴파일해서 명령줄 길이(약 10만자) 제한을 풀 수 있음, stackoverflow 참고, 근본적인 문제 해결 같지는 않음, 4k JPEG만큼 긴 인자를 어디에 쓸지 의문임
          + RLIMIT_STACK 값을 늘리면 됨, 예를 들어 ulimit -s 4000이면 4MB 스택임, 더 크게 하려면 /etc/security/limits.conf를 수정하고 재로그인 필요함
          + Electron에 패킹하고 http post json 요청으로 넘기면 되지 않을까 싶음
          + MAX_ARG_STRLEN를 재정의해서 커널 재컴파일하면 됨, 페이지 사이즈가 더 큰 머신(예: 64k 페이지사이즈인 RHEL Arm 커널) 사용도 방법임, 하지만 명령버퍼 대신 파이프를 써서 프로세스 간에 데이터 넘기는 게 훨씬 쉬움
          + 파일 경로 제한도 마찬가지 문제임, 일부 빌드시스템(Debian + python + dh-virtualenv 등)은 경로가 매우 길어질 수 있는데 그냥 허용하는 게 더 편함
     * 64비트로 바꿔도 언젠가는 한계가 올 것임, 292277026596년 12월 4일 오후 3시 30분 7초(UTC)에 인류는 뭘 할지 궁금함
          + 아마도 그때쯤엔 ipv6 완전 도입 100주년을 축하할 것임
          + 50억 년 이내엔 태양이 적색거성 돼서 지구 표면이 전부 증발할 것임
          + 그쯤 되면 더 나은 달력 시스템으로 옮겨갔으면 함, 물론 타임스탬프 문제는 여전히 남겠지만
          + 128비트 타임으로 가면 될 것임
          + RFC 2550(Y10K & beyond) 적용 가능할 듯, 1999년 4월 1일 발표됨
     * OpenBSD 5.5가 동일한 변경을 적용한 지 벌써 11년이나 지났음, OpenBSD 5.5 릴리즈 노트
          + 모두를 이긴 케이스임, 90년대에 OS/2 32비트 API가 64비트 시간을 반환하는 걸 발견하고, 직접 64비트 time_t로 C++ 표준 라이브러리를 작성해 사용함
          + 약간 다른 이야기지만, 이런 시기에 Linux 대신 OpenBSD로 서버를 바꾸고 싶은 마음이 생김
          + OpenBSD는 호환성에 덜 신경 써도 되고, 사용자 수도 훨씬 적어서 변화 시 버그나 엣지 케이스 가능성이 낮아짐
     * “Debian은 이제 충분히 완성 및 테스트가 됐으므로 Trixie 출시 이후 전환될 예정”이라고 하면 Trixie에는 적용 안 되는 것인지 궁금함
          + Trixie 릴리즈 노트엔 적용된다고 나와 있음, Trixie 릴리즈 노트
     * Y2K38을 Unix Epochalypse라 부르는 건 처음 듣는데, 귀엽긴 해서 퍼질 수도 있을 듯함
          + 위키백과 Year 2038 Problem에도 해당 이름이 나옴, 2017년부터 우스갯소리로 쓰이며 퍼짐
          + epochalypse-project.org 프로젝트도 있음
          + “it’s kind of fetch”라는 표현에 Mean Girls 영화 드립(?)이 보임
          + Epochalypse까지 약 12년 5개월 22일 13시간 22분 남음
"
"https://news.hada.io/topic?id=22357","Draw A Fish ! 사후분석","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Draw A Fish ! 사후분석

     * 2025년 8월 3일 DrawAFish! 웹사이트에서 약 6시간 동안 대규모 보안 사고가 발생함
     * 관리자 비밀번호 유출, 인증 누락 API, JWT 취약점 등 핵심 보안 취약점이 복합적으로 공격에 노출됨
     * 결과적으로 모든 사용자명이 모욕적 표현으로 변경되고, 불쾌한 그림이 승인됨과 동시에 기존 안전한 그림이 삭제됨
     * 수작업 복구, 인증 로직 수정, 백업의 확인 등으로 문제 해결을 진행함
     * 주된 교훈은 빠른 개발(“vibe-coding”)과 테스트·코드 리뷰 부족이 보안상 심각한 결과를 야기할 수 있다는 점
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

DrawAFish.com 개요 및 2025년 8월 3일 사고 요약

     * DrawAFish.com 은 사용자가 직접 물고기를 그리고 다른 사용자들과 함께 수조에서 수영시키는 재미 요소가 있는 웹사이트
     * 2025년 8월 1일, Hacker News 1위에 오르며 큰 주목을 받았음. 개발자는 Copilot 등의 도구로 빠르게 기능을 구현하는 ‘vibe-coding’ 방식을 채택

     * 그러나 2025년 8월 3일 새벽, 심각한 보안 사고가 발생함
     * 약 6시간 동안 온갖 사용자명이 저속한 표현으로 변경되고, 부적절한 그림이 승인되는 등 무질서한 상황이 발생했음
     * 결국 관리자가 수동으로 원복 작업을 수행함

취약점 상세 분석

  1. 과거 유출된 6자리 관리자 비밀번호

     * 개발자는 초기에 본인 어린 시절 ID와 비밀번호(6자리)를 관리자 계정에 사용함
     * 이 비밀번호가 과거 Neopets 등 사이트 데이터 유출을 통해 이미 온라인에 공개되어 있었음
     * 개발자는 이후 Google Auth를 사용했지만, 이전 비밀번호를 삭제하지 않아 공격자에게 취약점이 남아있었음
     * 공격자는 데이터 유출된 정보를 이용해 관리자 인증에 성공했고, 모욕적 그림 승인 및 정상 그림 삭제 등의 악의적 행위를 함

  2. 인증 없는 사용자명 변경 API

     * 프로필 백엔드 개발 시 ‘빠른 개발’을 위해 인증 확인 기능 없이 사용자명 변경 API를 구현함
     * 실질적으로 누구든 임의로 사용자명을 변경할 수 있었음

  3. JWT 검증 미흡

     * JWT 토큰 기반 인증에서, 토큰과 userId/email의 매칭 없이 관리자 작업이 수행 가능하게 구현함
     * 즉, 공격자가 관리자 인증 정보로 발급된 토큰만 있으면 누구의 요청에도 관리자 권한으로 사용할 수 있었음
     * 흥미롭게도, 한 Hacker News 유저가 이 취약점을 이용하여 침입자보다 먼저 부적절한 자료를 삭제하며 비상 대응에 기여함

복구 과정

     * 개발자는 아침 7시 45분경 이상 상황을 인지해 즉시 데스크탑에서 대응을 시작함
     * firebase 백업을 미설정 상태여서 백업에 의존하지 못했고, 코드를 신속하게 수정해 인증 강제 적용
     * 모든 모더레이션 로그를 추적하여 악의적 작업을 취소하는 스크립트를 개발함 (이 역시 vibe-coding 방식으로 작성)
     * 비상시에 관리자 권한을 사용한 제3자(Hacker News 유저)의 접근 계정도 차단 후 연락하여 코드베이스의 시큐리티 리팩터링 관련 피드백을 받고 추가 패치 진행함

개발 문화와 교훈

     * 개발자는 “vibe-coding”, 즉 빠른 프로토타이핑과 최소한의 검토 문화가 재미와 높은 생산성을 주지만, 실제로는 심각한 보안 취약점으로 이어질 수 있음을 경험함
     * LLM(Copilot)은 빠른 코드 생산에 매우 유용하나, 코드의 품질과 보안 책임은 개발자 본인에게 있음을 역설함
     * 테스트, 인증 로직, 코드 리뷰 등 기본적인 보안 절차를 생략한 결과, 소규모 프로젝트에서도 외부 공격에 극도로 취약해질 수 있음을 입증한 사례임

결론 및 인사이트

     * DrawAFish.com 사례는 실제 서비스 운영 환경에서 흔히 간과하는 기초 보안 조치의 중요성을 보여줌
     * 오픈 소스 도구 및 빠른 개발 도구에 의존할 때에도, 반드시 테스트, 인증, 코드 리뷰, 비밀번호 관리 등 기본 이슈를 체크해야 함
     * 예상치 못한 규모의 주목(예: Hacker News 상위 랭크)에 따라 공격 표면과 리스크가 급격히 확대될 수 있음
     * Postmortem을 투명하게 기록함으로써, 향후 유사 스타트업이나 개인 개발자들에게 현실적인 보안 교훈을 전달함

     vibe-coded의 ""S""는 Security의 약자임

     흥미롭게도, 한 Hacker News 유저가 이 취약점을 이용하여 침입자보다 먼저 부적절한 자료를 삭제하며 비상 대응에 기여함

        Hacker News 의견

     * 나도 빠른 속도로 일하는 게 정말 재밌음, 코드 리뷰 같은 거 안 하고 그냥 바로바로 푸시하는 게 즐거움, 그런데 이번 포스트모템을 보니까 내 사이드 프로젝트가 왜 자주 중단됐는지 알겠음, 항상 결국에는 지루한 실제 작업에 닿게 되고 그 부분에서 멈추게 됨
     * Firebase인지 궁금해짐, 무료 티어를 계속 썼는지, 아니면 누가 악의적으로 공격해서 아침에 일어나보니 다섯 자리 숫자 요금 청구서를 받았는지 궁금함
     * 나는 슬러피쉬 사건을 직접 겪은 '운 좋은' 사람 중 한 명임, 보안 쪽에서 일하다 보니 너무 노골적인 모습에 웃으면서도, HN에는 이걸 제대로 도울 시간 있는 사람이 곧 나타날 거라는 걸 알았음, 이런 vibe-coded 프로젝트도 자세히 문서화된 포스트모템이 공개되어 좋았음, 요즘은 프로덕션에 분위기만 따르는 코드(vibe-coded)가 나의 IR(Incident Response) 업무에서 많이 보이지만, 이렇게 작은 프로젝트에서까지 잘 정리된 걸 보니 인상적임
          + 사람들이 'vibe coding'이 문제였다고만 하지만, 사실 최소 두 가지 문제—'테스트용 admin 계정 남겨두기'와 '토큰은 확인했지만 교차검증은 하지 않기'—는 AI나 vibe coding하지 않아도 일반적으로 벌어지는 현상임
          + 스와스티카 형태의 물고기(‘스와스티카프’) 스크린샷이라도 있었으면 좋았을 텐데, 실제로 못 봐서 아쉬움
     * 이 포스트모템이 특히 vibe-coded 앱이라서 더 흥미롭게 느껴짐, 혹시 Lego House에 있는 물고기 만드는 전시에서 영감을 받은 건지 궁금함, 최근에 다녀왔는데 내가 만든 물고기가 다른 물고기랑 같이 수영하는 게 정말 중독적이었음 Lego House Build-a-Fish 유튜브 영상
          + 이 전시는 몰랐음, St Louis aquarium에서 색칠한 물고기가 실제로 수영하는 것과 2016년쯤 Google의 Quickdraw에서 영감을 받았음
     * 누군가가 보안 취약점을 이용해서 실제 공격을 막으려 했다는 게 정말 놀라움
          + 이런 모습은 예전에도 종종 있었음, 어떤 웜/익스플로잇은 스스로 취약점을 패치해서 막기도 했었음
          + 몇 년 전에 FBI가 EternalBlue 익스플로잇을 막기 위해 비슷한 방식으로 움직였다는 기사를 본 적 있음(익스플로잇 이름은 정확하지 않을 수도 있음)
          + 사람들은 정말 놀라운 존재임
     * vibe-coded의 ""S""는 Security의 약자임
          + ""S""는 뱀... 아니면 용의 약자일지도 모름
     * 완전 새로운 아이디어를 떠올리고, 직접 만들어보고, 많은 걸 배웠으면서, 전문가답게 실패한 부분은 부끄러워하는 사람이야말로 진짜 멋진 전문가임, 만약 10만 달러를 잃지도 않고, 네트워크가 망가지지도 않고, 직장을 잃지도 않았다면, 일 년 뒤엔 이 일들을 웃으면서 추억할 수 있을 것임
     * 지금 그 사이트에 스와스티카 물고기가 올라와 있었음, 누군가 물고기 모양 안에 넣어서 필터를 우회함 문제의 그림 링크, 현재는 삭제됨
          + ""Balls""라는 글씨가 써진 물고기도 있었음 이 링크에서 볼 수 있음, 이 정도면 생각보다 별로 문제될 것도 없다고 생각함
          + 저 물고기는 부처님의 손길이 느껴지는 수준임
     * 현재 인증 없이 어떤 물고기든 투표(최대 분당 20번/IP)가 가능함, POST로 다음 주소 사용 가능함 투표 API 링크 {""fishId"":""xxxx"",""vote"":""up""}
          + 사실 일부러 그렇게 설계함, 어떤 물고기는 조금만 좋아하고 어떤 건 진짜 많이 좋아할 수도 있으니, 마음껏 좋아요/싫어요 할 수 있도록 만들었음
     * vibe-coded 웹사이트에 대해 포스트모템을 남긴 것이 정말 마음에 듦, 많은 사람들이 기술을 너무 진지하게만 대하는데 이렇게 가볍고 재밌게 바라보는 시선이 신선하고, 작은 사이드 프로젝트 입장에서 굉장히 흥미롭고 의미있게 느껴짐
"
"https://news.hada.io/topic?id=22232","vLLM 프리 스레드 파이썬 지원되면 더 빠르고 효율적인 모델 서빙 가능할듯","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               vLLM 프리 스레드 파이썬 지원되면 더 빠르고 효율적인 모델 서빙 가능할듯

   vLLM의 중요한 발전 소식입니다.

   이제 파이썬의 병렬 처리를 제한하던 GIL(전역 인터프리터 락)이 없는 프리 스레드 파이썬에서도 vLLM 실행이 가능하다고 합니다.

   메타의 엔지니어들이 이를 성공시켰으며, vLLM은 이 미래 기술을 적극적으로 수용할 계획이라고 밝혔습니다

   vLLM 은 PagedAttention 기술을 이용해 대규모 언어 모델(LLM)의 추론과 서빙을 매우 빠르고 효율적으로 처리하는 고성능 파이썬 라이브러리며 LLM 서빙에서 많이 사용되고 있습니다.
"
"https://news.hada.io/topic?id=22272","Zero - Gmail을 대체 가능한 오픈소스 AI-기반 이메일 클라이언트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Zero - Gmail을 대체 가능한 오픈소스 AI-기반 이메일 클라이언트

     * 셀프호스팅 가능한 AI 기능 중심의 웹 이메일 솔루션, Gmail, Outlook 등 다양한 이메일 서비스와 연동할 수 있음
     * AI 에이전트와 LLM을 활용한 이메일 관리 자동화, 여러 계정 통합 인박스와 UI 커스터마이즈도 지원
     * Next.js, React, TypeScript 등 최신 프론트엔드 스택과 Node.js, Drizzle ORM, PostgreSQL 백엔드, Better Auth, Google OAuth 인증 등으로 구성
     * 개인정보 보호, 완전한 투명성, 쉬운 자체 호스팅을 주요 차별점으로 내세움
     * 개발자 친화적인 구조로, 기능 확장 및 통합이 용이하고 외부 서비스 연동, SMS 인증, 암호화, 번역 등 다양한 옵션을 제공
     * 이메일 데이터를 Cloudflare Durable Object 및 R2 버킷에 저장해 빠른 동기화 가능
     * https://0.email/ 에서 호스팅 버전도 제공(무료 및 유료)
"
"https://news.hada.io/topic?id=22289","공영방송공사(CPB), 연방 자금 지원 중단으로 운영 종료","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    공영방송공사(CPB), 연방 자금 지원 중단으로 운영 종료

     * 공영방송공사(CPB)는 연방 예산안에서 배제되며 60년 만에 운영 종료 절차 착수
     * 교육 콘텐츠와 지역 저널리즘 지원 등 CPB의 공적 역할은 미국 전역에 중대한 영향을 줌
     * 직원 대부분은 2025년 9월 30일까지 근무 종료 예정이며, 이후에는 최소 인원만 전환 및 정산 업무 진행
     * CPB는 투명성을 바탕으로 파트너 기관들과의 협력적 마무리와 법적·재정적 책임 이행에 집중 예정
     * 향후 음악 권리 관리, 긴급서비스, 문화 연계 등 필수 기능의 지속 업무도 소규모 인력이 담당 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

CPB 운영 종료 발표 배경

     * 2025년 8월 1일, 연방정부 재정삭감 패키지 통과 및 상원 세출위원회의 2026회계연도 예산안(Labor-H)에서 CPB에 대한 예산 지원이 제외됨
     * 약 60년에 걸쳐 CPB는 의회로부터 신뢰받는 공공 미디어 시스템 구축과 유지를 위임받아 지역 방송국 및 제작자와 협력하며 다양한 교육 프로그램, 지역 뉴스, 긴급 통신, 문화·공공 서비스를 제공함

종료 과정 및 CPB 입장

     * CPB 회장 Patricia Harrison은 “수백만 명의 시민들이 의회에 CPB의 예산 유지를 요청했음에도 불구하고, 이제 우리는 운영 종료라는 어려운 현실에 직면함”이라고 공식 언급
     * CPB는 윤리적 책임 이행 및 파트너 기관 지원, 투명한 절차 진행에 힘쓸 것임을 강조함

직원 및 운영 마무리 계획

     * CPB는 전체 직원에게 통보 완료, 대부분 직원은 2025년 9월 30일 회계연도 종료 시점까지 근무를 마칠 예정임
     * 2026년 1월까지는 법률, 재정, 사업 마무리 및 음악 권리·저작권 관리 등 핵심 공공 미디어 서비스의 연속성 보장을 위한 소규모 전환팀이 유지됨

공영방송의 의의와 감사 메시지

     * Patricia Harrison은 “공공 미디어는 미국 전역에서 신뢰받는 기관으로서 교육, 긴급경보, 시민적 담론, 문화적 연결 등 다양한 역할을 수행해 왔음”을 강조
     * 오랜 기간 협력해 온 파트너 기관, 리더들의 헌신과 노력에 감사를 표함

법적·운영적 후속 조치 및 안내

     * CPB 이사회와 경영진은 폐쇄에 수반되는 법적, 재정적, 운영적 사안을 긴밀하게 관리 중임
     * 각 지역 방송국 및 협력 제작자들이 변화에 대응할 수 있도록 지침과 정보 제공을 지속할 계획임

CPB 개요

     * Corporation for Public Broadcasting(CPB)는 1967년 의회 설립 인가를 받은 민간 비영리 법인임
     * 전국 1,500여개 이상의 지역 공영 텔레비전 및 라디오 방송국 운영을 지원하며, 미국 공영 방송 분야에서 연구개발, 기술혁신, 콘텐츠 개발에 가장 큰 단일 재원을 제공해옴
     * 추가 정보 및 각종 소식은 CPB 공식 홈페이지와 페이스북, 링크드인, 이메일 구독을 통해 확인 가능

        Hacker News 의견

     * 이 스레드에서 많은 사람들이 공영방송의 몰락을 안타까워하는데, 사실 CPB는 PBS 프로그램을 직접 만들지 않고, 의회로부터 PBS 예산을 받아 지역 방송국에 나누어주는 보조금 관리기관임을 알리고 싶음. PBS와 NPR 프로그램은 대부분 민간에서 제작되어 지역 PBS 방송국이 구매하는 구조임. 온라인에서는 'Passport'라는 후원자 전용 스트리밍 시스템을 통해 PBS 콘텐츠를 시청할 수 있음. 요즘은 대부분의 시청자가 인터넷 스트리밍으로 시청하고 있기에, 만약 지역 PBS 방송국이 사라져도 시청자가 Masterpiece Theater나 Nova를 잃을 가능성은 매우 적음. 이번 예산 삭감은 분명 안타까운 일이지만, CPB 중단의 실제 의미를 올바르게 이해하길 바람
          + 실제로 내가 보는 CPB 지원 프로그램으로 Frontline과 NOVA가 있음 Frontline 후원자 안내 NOVA 후원자 안내. 일종의 '트리클 다운' 경제학이 여기서는 실제로 작동했음. CPB는 PBS 콘텐츠 개발 지원에 중요함. 이제 PBS는 저렴한 콘텐츠 주문이나 후원, 추가 모금 등으로 비용절감해야 하는데, 이는 별로 바람직하지 않음. 또한 CPB는 농촌 방송국 유지에도 큰 역할을 했기에, 지역 특화 프로그램들이 계속 제작될 수 있었음. 멋진 자원의 예산이 더 줄었고, 결과적으로 일자리와 품질 저하가 발생하지 않을지 걱정임. 그리고, ""이 프로그램은 Corporation for Public Broadcasting과 여러분 같은 시청자의 후원으로 제작되었습니다!""라는 멘트를 못 듣게 되어 아쉬움
          + 대도시의 PBS 방송국들은 기부와 기업 후원 덕에 지속가능할 수 있지만, 인프라가 열악한 농촌 지역 방송국은 크게 영향을 받게 됨. 일부 농촌 방송국은 예산의 절반 가까이를 CPB에서 받고 있고, 이 예산이 끊기면 문을 닫을 수밖에 없을 것임. 웨스트버지니아, 알래스카, 뉴멕시코, 몬태나 등 농촌 지역에서는 평균적으로 CPB 의존도가 30%가 넘음. 이곳의 방송국들이 모두 위험에 처했음. 자세한 정보는 Current 기사 참조
          + 실제로 PBS와 NPR 프로그램은 민간에서 개발, 제작하지만, 그 구매에는 CPB 지원금이 사용됨. 대부분의 보조금은 방송국에 지급되었기 때문에, 보조금이 끊기면 결국 콘텐츠 제작자도 타격을 입게 됨
          + 이 설명이 유익했음. 하지만 그랜트 기관이 사라지는 것이 실제로 의미하는 바는 여전히 열린 질문임
          + 많은 사람들이 선형TV가 아닌 온라인으로 PBS를 시청한다고 하지만, 나는 실제로 안테나로 지역 PBS를 보며, 우리 가족 여러 명도 각자 집에서 그렇게 시청함. 방송 품질도 아주 훌륭하고, 신호도 잘 잡히며, 로컬 뉴스와 프로그램도 만족스러움
     * 이 자료에 따르면 PBS는 연방 예산에서 약 15% 정도만 자금을 받고, 나머지는 기부금에 의존함. 그래서 내가 처음 걱정했던 PBS의 종말까지는 아닐 것 같음. 참고로 나는 이 결정을 옹호하는 게 아니라, NPR로 자라 선, PBS에 정기 후원까지 하는 입장임. 순수한 호기심에서 연방 예산 비중이 궁금했던 것임. 괜한 과잉 반응 없이 실제 영향을 바로잡고 싶음
          + PBS와 NPR은 상업 네트워크와 운영방식이 전혀 다름. ABC/NBC/CBS/Fox 등은 본사 혹은 프랜차이즈 가맹사가 네트워크 프로그램을 공급해주는 구조임. 반면 PBS는 전국 각 지역의 공영방송국들이 모인 협약체임. ‘PBS 프로그램’이라 생각하는 대부분의 프로그램도 개별 방송국이 제작 후 다른 방송국과 공유하는 구조임. NPR쪽 라디오 프로그램도 상황이 훨씬 더 복잡함. 예컨대 Marketplace는 NPR이 아니라 American Public Media가 제작함. 이번 삭감이 실제 PBS 본사에만 영향을 주는 게 아니라, 전국 방송국 운영과 지역 제작 지원금, 특히 농촌 지역 영향이 더 궁금함
          + PBS 공식 입장에 따르면, CPB 자금이 지역 방송국을 지탱하는 역할을 해왔고, CPB 자금 없이 방송국 운영이 힘들어진다고 명시함. 또 CPB 지원금의 대부분이 NPR/PBS 국가 프로그램 제작비로 지급되어, 이에 대한 삭감도 불가피함 기사 참조
          + PBS가 연방 예산에서 받는 ‘15%’가 실제로 오해의 소지가 있을 수 있다고 생각함. 각 지역 PBS 회원 방송국이 연방 예산에서 상당부분 지원받아 전체 예산의 구조에 영향을 주기 때문임
          + 관련 기사 참고
               o 농촌 방송국에 가장 큰 타격
               o 약 1,000개 회원 방송국 중 최대 18%가 폐쇄 위기
          + 때로는 15% 상실만으로도 지속 불가능해질 수 있음. PBS가 어떤 큰 기부금이나 대학교처럼 막대한 기금을 가진 곳이 아니기 때문임
     * “공영미디어는 미국에서 가장 신뢰받는 기관 중 하나로 교육, 긴급 알림, 논의의 장, 문화적 연결을 전국 구석구석에 제공해왔다”고 Harrison이 말했음. 만약 그게 사실이라면 CPB 상실은 비극임. 하지만 수십 년간 NPR을 충성스럽게 들어왔지만 최근 NPR의 콘텐츠는 듣기 힘들어짐. Fox 뉴스의 좌파 버전 같은 느낌에, 자기들만의 편향성을 전혀 자각하지 못하는 게 가장 불쾌함. Fox 쪽 진행자는 자기들 입장을 분명히 인지하는데 NPR 진행자들에게선 그런 자기인식조차 느끼지 못함
          + NPR은 오바마 집권 당시 오히려 우향으로 방향이 확 바뀌었고, 2기 정권 때쯤에는 사회관계망에서 선정적인 기사만 내보내기 시작함. “리버럴 미디어”라는 프레임은 시장 통제를 위해 만들어낸 괴물임. 미디어 편향을 거론하는 사람들 대부분이 악의적으로 주장하고 있으며, 실제로 미국 미디어는 압도적으로 보수적이라는 점을 간과함
          + 이런 ‘공영방송이 과거에는 완벽하게 정부 입장만 따르고, 편향이 없었다’는 식의 이야기는 혼란스러움. Fox 뉴스와 같은 미디어의 등장 이후 이런 식의 양비론적 서사가 유행함. 하지만 NPR을 향해 ‘좌파 버전 Fox에 지적 허울만 썼다’는 식의 말은 일종의 우파 프레임 반복임. Fox가 자기 편향을 알고 있다는 걸 근거로 한다는 점에서, 이런 논리는 너무 단순하다고 생각함
          + 미국과 많은 미국인들은 근본적인 관점 자체를 잃고 있으며, 항상 주제에 대한 인식 확보에 어려움을 겪고 있음. 외부자의 시각에서 보면 이번 CPB 사라짐은 명백한 비극임. 50년, 100년 뒤 역사가들이 ‘어쩌다 저런 일이?’라고 물을 중요한 사건임
          + NPR이 2016년 이후 좌편향적으로 급격히 기울어진 것이 정말 아쉬움. 십 년 전에는 ‘진보적 주제’가 주를 이루어도 여러 관점을 실제로 조명해주는 양질의 저널리즘이라고 생각했음. 하지만 요즘은 Fox처럼 진영논리 방송이 되어버린 것 같아서 아쉬움
          + 이런 “편향”의 구체적인 예시를 들 수 있는지 궁금함
     * 어렸을 때 “Must see TV”였던 게 Mr. Roger’s Neighborhood와 Sesame Street임. 세월이 흐르며 과학에 관심이 커졌을 때 Nova를 꾸준히 챙겨봤음. 이 모든 게 Corporation for Public Broadcasting의 지원을 받은 프로그램이었고, 이번 결정이 정말 아쉬움
          + This Old House는 누수 피해의 두려움을, The Woodwright’s Shop은 끔찍한 아재 개그와 함께 목공의 기초 기술을 가르쳐줬음. Roy는 정말 전설임
          + Sesame Street는 흑인 남학생들의 학업성취를 도왔고, 지금의 새 정권은 오히려 그들을 다시 목화 따러 가게 만들고 싶어 하는 것 같음. 관련 연구 논문 관련 칼럼
     * 나 자신은 NPR을 매일 듣고, 이번 결정이 오히려 공영방송에 좋을 수 있다고 생각함. 당장은 힘들겠지만, 정치인 간섭 없이 스스로 자립하게 되면 장기적으로 더 튼튼해질 것 같음
          + “공영방송에서 ‘공공’이 무엇인지”란 질문을 다시 생각해보고 싶음
          + NPR이 가끔 재미있게 느껴질 때가 있음. 매번 20분 이상 들으면 꼭 폴리아모리나 여성들이 “Bimbo”란 단어를 자랑스럽게 재해석한다든지 혹은 사람들이 행정당국에 판토마임 시위하는 그런 소재를 만나게 됨. 분명히 대중취향엔 한참 거리가 있음
          + “정치인 간섭 없이 자립하면 자유로워질 것”이라는 생각에 의문이 있음. 분명 다른 방식으로도 압박과 검열이 들어오고, 실제 그런 일이 이미 여러 번 있었던 것임
          + 현실적으로 지금까지는 그렇게 정치 간섭이 눈에 띄게 있던 건 아니지 않았나? 차라리 기업 간섭이 더 많은 다른 미디어처럼 되는 게 더 나은가?
          + NPR이 Koch Foundation에서 받은 기부금에 대해 NPR 구성원이 실제로 어떻게 생각하는지 궁금함
     * 그동안 보수 진영에서 폄하되어 왔지만, 실제로 공영방송은 최소한 전국 뉴스와 정보에 있어서 거의 편향이 없는 매체라고 생각함. 특히 어린이 프로그램은 경쟁이 안 될 정도로 훌륭함. 자극적인 효과나 상업성이 없고, 교육에만 초점을 맞춘 순수성, 그리고 무료/광고 없는 접근성은 이 시대에 기적이나 마찬가지임. 이런 예산삭감을 지지하는 사람들은 현실감각이 없는 사람들이라고 봄. 이들은 마치 공원이 낭비라고 생각하거나 좋은 걸 나눠 갖는 걸 엘리트주의로 여기는 부류임
          + 공영방송이 거의 편향 없는 뉴스 소스라는 점이 이들이 싫어하는 이유임. 이들은 자신이 컨트롤할 수 있는 미디어를 원함. 결국 모든 것이 컨트롤/권력 싸움임
          + 제대로 작동하는 민주주의 국가에서 국영미디어는 이익 아닌 공익을 위해 운영됨. 정치자금의 오염이 없으니 건강함. 미국 PBS도 훨씬 더 좋아질 수 있었음. 영국 BBC를 보면 부러운 수준임
          + “어린이 프로그램이 최고, 아무런 의도도 없다”에 동의하면서도 Odd Squad가 Guardians of the Galaxy급의 그린스크린 효과로, Fetch! with Ruff Ruffman은 꽤 진기한 진행방식(보지도 않는 유령이 사회보는 구조) 등으로 오히려 독특했다고 생각함. 또 CyberChase 리부트는 농업부가 후원해 유기농 아젠다가 뚜렷했음. Sesame Street도 ‘알파벳과 숫자 후원사’에 줄곧 기대온 셈임
          + 케이블 뉴스를 끊은 지 오래임. 모든 24/7 뉴스 체계가 본질적으로 독이 됨. 과장, 선정, 경쟁적 분노 키우기 일색임. NPR만이 숨가쁜 선정성 없이 매우 공정했음. Fox는 허위, CNN은 과장, MSNBC는 분노만 조장, NPR은 실수를 가리지 않고 양쪽을 언급했음. 이건 진실임
          + PBS Kids를 잃는 것은 정말 큰 손실임. 요즘 상업용 어린이 콘텐츠는 정말 쓸모없는 게 많음
     * 의회가 CPB 예산삭감안을 통과시키면서, 공화당 의원들은 NPR의 가장 논란된 사례를 내세우며 '왜 국민 세금으로 이런 걸 후원하나?'라고 이야기했음. 나도 NPR이 요즘 제정신이 아니라고 생각하고, 리스너 숫자도 떨어지고 있지만 NPR은 오디오 매체라 그나마 비용이 적고, 예산을 채울 방법도 있음. 하지만 PBS는 NPR이 겪지 않은 문제를 피해왔고, 뉴스 외 다양한 프로그램 중심이었는데, 큰 타격을 받아서 모금방송과 재방송만 더 늘고, 전국 곳곳의 로컬 토착 방송국들은 완전히 문을 닫게 될 것임. 이는 욕조와 아기를 한꺼번에 내다버리는 어리석음임
     * 모든 정부 프로그램 도입시 종료시한을 명시해야 한다고 생각함
          + 전부 “만료”된 정부 서비스만 있는 나라들도 있음. 그런 곳에서 살고 싶은지 묻고 싶음
          + 그리고 더 나아가, 모든 법률은 단 하나의 이슈만 다루도록 해야 한다고 생각함
          + 그러면 방해 세력이 원하는 걸 항상 가져갈 것임
          + 모든 전후 미국 기관을 성역으로 볼 필요는 없다고 생각함. 영구적 보장이 정답은 아님. 다만 예산 절감으로 시민의 삶이나 공공사업에 도움이 돌아가길 바랐으나, 실제론 외국으로 미사일 한 기 더 가는 셈이겠지
          + 군사, 심지어 헌법도 마찬가지로 이런 식 적용 가능하다고 주장함
     * Claude에 따르면, CPB가 자금을 지원하는 대표 프로그램은 Sesame Street, NOVA, PBS NewsHour, Masterpiece, NPR의 Morning Edition, All Things Considered 같은 것임. 모두 정말 좋아하는 프로그램임. NPR은 예산이 3억 달러고 청취자는 4,200만 명 정도니, 1인당 연 7.14달러 꼴임. 내가 KQED와 KCSM에 월 5달러씩 기부하면 나 말고도 여러 사람 몫까지 지원하는 셈임? PBS에 직접 기부할 방법도 잘 모르겠고, PBS 기부 버튼을 누르면 다시 방송국별 기부로 연결됨. 아는 사람 알려줄 수 있는지 궁금함
          + 기본적으로 PBS 회원 방송국에 기부하면, 방송국이 일정 부분을 PBS에 회비 형식으로 보내는 구조임
     * 어린이 프로그램이 이번 논의에서 너무 간과되는 부분임. 어떤 투자는 즉각적 결과로 측정할 수 없음. 부모님이 바빠 늘 혼자 TV를 봤던 내게, 케이블이 아닌 PBS에서 자란 건 복이었음
          + 부모가 케이블TV를 못 쓰면 Nickelodeon 같은 유아전용 채널 시청이 불가능했고, 그렇지 않을 경우 보게 되는 건 낮시간의 Maury Show 같은 토크쇼, 판사쇼, 드라마뿐임. PBS만이 유일하게 거의 하루 종일 무료 어린이 프로그램을 제공해줬음
"
"https://news.hada.io/topic?id=22284","2024년에 ChatControl을 거부했던 많은 국가들이 현재는 입장을 정하지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            2024년에 ChatControl을 거부했던 많은 국가들이 현재는 입장을 정하지 않음

     * 여러 유럽 국가들이 2024년에 ChatControl 도입을 거부했으나, 현재는 입장이 불확실해짐
     * 오스트리아는 ChatControl 그 자체보다 Ursula von der Leyen의 주도에 더 반대하는 입장이었음
     * 오스트리아 내무장관 Karner는 독일 당국에 대한 반감 때문에 오스트리아 주도 ChatControl을 지지하는 입장임
     * Sebastian Kurz와 Peter Thiel 등 이전 정부 인사들의 영향력이 계속 남아있음
     * 감시 국가의 확대 가능성에 대한 우려가 제기됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ChatControl 정책 변화와 오스트리아의 입장

  ChatControl을 둘러싼 국제적 동향

     * 2024년에 여러 국가들이 ChatControl 정책에 반대함
     * 그러나 최근 정세 변화로 인해 이들 국가들이 이제는 찬반을 명확히 밝히지 않는 상황임

  오스트리아의 특별한 상황

     * 오스트리아는 사실상 ChatControl 정책 자체에는 강하게 반대하지 않음
     * 반대한 이유는 Ursula von der Leyen이 해당 정책을 주도하는 데 있었음

  내무장관 Karner의 영향

     * 오스트리아 내무장관 Karner는 Engelbert Dollfuß의 지지자로 알려짐
     * 그는 전통적으로 독일 영향력에 비판적인 입장임
     * 이에 따라 자신이 주도하는 오스트리아판 ChatControl을 적극 지지함

  정치권의 영향력

     * 집권당인 ÖVP(터키색 계열) 가 여전히 정부를 운영함
     * Sebastian Kurz, Peter Thiel 등 옛 정치 인사들의 영향력이 현재도 지속됨

  감시 정책 확대 우려

     * 이러한 정치적, 행정적 흐름 속에서 오스트리아 내에서 감시 국가의 범위가 확장될 가능성이 제기되고 있음

        Hacker News 의견

     * 나 같은 평범한 사람이 이런 법안에 맞서 싸우는 건 정말 힘든 일임을 느낌. 대표 의원을 찾아 그들의 연락처를 확보하고, 내 입장을 전달하면서, 그 사람이 맞는 담당자인지, 내 메일이 무시받지 않는지, 제대로 읽힐지, 그리고 그로 인해 그들의 생각이 바뀔지 일일이 다 희망해야 함. 이건 실질적으로 조직화된 로비 활동에 맞서는, 매우 비조직적인 일반 시민의 방식임. 이런 번거로움과 장벽 때문에 아무리 열정이 있어도 실제로 행동에 나서기 어려워짐. 사람들이 좀 더 쉽게 반대 의사를 전달할 수 있는 방법이 있었으면 좋겠음
          + 사실 어느 법안에 대해서나 이 문제가 똑같이 적용됨. 예를 들어, 1억 명에게 1달러씩 비용을 지우지만 100명에겐 100만 달러 이득을 주는 법이 있다고 가정할 때, 시민들이 반대하는 데 드는 노력이 정말 비용이 크고 시간 낭비 수준임. 그러나 반대로 큰 이득을 보는 소수는 수십만 달러를 써서라도 열심히 로비함. 이 자체가 대의 민주주의의 권력 구조임
          + 대부분의 현대 민주주의 국가가 가진 큰 문제점 중 하나는, 어떤 법안이 통과되지 않더라도 바로 다시 상정할 수 있다는 것임. 난 시의회에서 국민투표안을 매번 단순 복사해서 선거 때마다 올리다 언젠가 통과될 때까지 반복하는 걸 종종 목격했음
          + 실제로 막을 수 있는 방법은 기본권을 명확히 법에 명시하는 거라 생각함. 예를 들어, 온라인 프라이버시와 통신 비밀 보장 같은 권리가 명문화되어야 함
          + 솔직히 정치인 입장에서 내가 뭘 생각하던 신경 쓸 이유가 없음. 그들이 당선되는 건 결국 자금을 많이 받기 때문이고, 그 돈이 어디서 오는지 보면 답이 나옴
          + 영국에는 청원 사이트가 있어서, 의원별로 서명자를 집계함. 일정 서명 수를 넘기면 정부가 반드시 공식 답변을 해야 하고, 의회에서도 그 이슈를 토론 대상으로 다뤄야 함
     * 'ChatControl'이란 EU 전역에 적용될 텍스트 키워드, 이미지, 동영상 등 모든 디지털 사적 통신 내용을 감지 및 신고하는 프레임워크 제안임. 공식 명분은 아동성착취물 방지임. 관련 내용 위키피디아 링크
     * 2025년 11월 7일자 미팅 내용이 유출됨. 독일어 원본은 netzpolitik.org 기사에서 볼 수 있고, 자동 번역된 영어본은 여기에서 확인 가능함
          + 나는 덴마크인인데 내 정부가 이런 식으로 나가는 게 정말 싫음. 현지에서는 법무부 장관 Peter Hummelgaard가 사전 범죄 혐의 여부와 상관없이 모든 시민을 경찰 정보기관(PET)이 감시할 수 있도록 대량 감시 법안을 추진함. 통과되면 소셜 미디어 활동, 건강정보, 기타 감시로 수집된 데이터를 결합해 모두의 데이터베이스를 만들 수 있음. 이는 자동 용의자 생성 머신이나 다름없음. Hummelgaard는 경찰이 요구하는 모든 도구를 무비판적으로 허락하고, 어떤 범죄든 강화 처벌만 주장함. 교도소 과밀과 시스템 붕괴에도 불구하고 더 강한 처벌을 내세우는 전형적인 권위주의자임
     * 유럽은 여전함. 주기적으로 권위주의로 미끄러지고 있음. 결국 러시아가 재차 침공할 것이고, 또 미국이 개입해서 이 대륙을 '문명화'하고 해방시켜줘야만 함. 그런데 결국 사람들은 다시 권위주의를 선택함. 외부에서 강제로 강요되는 게 아님. 본질적으로 이 대륙 사람들이 스스로 누군가에게 통치를 원한다는 증거임
     * 정말 전 세계가 감시 국가로 전환되는 느낌임
          + 인터넷이 이제 IRL에서 만나는 곳이나 커뮤니티를 거의 없애고, 소셜 미디어가 분산 인터넷을 소멸시켰기 때문에 상당히 소수의 소셜 미디어 기업들이 어떤 논의가 이뤄지는지 대부분 통제하게 됨. 독립적으로 운영한다고 해도 SNS가 어떤 아이디어, 콘텐츠가 트래픽을 얻는지 결정함. 이렇게 되면 특정 이슈를 올리는 것도 쉽게 통제할 수 있음. 결국 이런 현상에서 어떻게 빠져나올 수 있을지 고민임
          + 중국이 이미 그 길을 보여줬고, 많은 나라들이 이런 방향을 선호함
          + 평화적인 해결책이 없다는 현실임
     * 이 법안 이름이 'Speech Control'이었다면 얼마나 많은 지지를 받았을지 궁금함. 아마도 그래도 우울할 정도로 많은 사람이 찬성할 거 같음
          + 사실 이 법안 공식 명칭이 아닌, 반대자들이 'Chat Control'이라고 부르고 있음
          + (의원들 외에) 실제로 이 법안이 얼마나 지지를 받는지도 궁금함
     * 전체 유출된 내용: 2024년에 #ChatControl에 NO라고 답했던 많은 국가들이 이제는 미지수임. 심지어 2025년 계획은 더 극단적임. 올해 10월에 표결 예정임. 정부에 #StopChatControl이라고 꼭 전달해야 함. 지금 바로 행동하라는 사이트 (chatcontrol.eu) 있음
          + 해당 사이트를 실제로 들어가보니 제대로 만든 사이트가 아님. '지금 바로 행동하라'며 링크를 제공하는데, 정작 구체적인 행동 지침이 없음. '정부에게 하지 말라고 요청해라' 같은 건 아무 의미 없음. 각 나라별로 구체적으로 무엇을 어떻게 해야 하는지 단계별 안내가 필요함
     * 매년 저 법안이 거절될 때마다 계속 투표를 올릴 셈인가 의문임
          + 사실 그보다 더 교묘함. 법안이 통과될 표가 충분히 확보됐을 때만 의회 상정함. 지난번에도 질 것 같으니 철회함
          + 현재 구조와 수단으로는 사실상 막을 길이 없음. 상대는 계속 밀어붙이고, 결국 시민들이 피로해져 포기함. 현실적으로 과거 식의 sneakers-net(USB 등 직접전달 방식) 등으로 대비하는 게 최선임
          + 6개월마다 이런 기사 나오는 이유는 실제로는 투표까지 잘 안 가기 때문임
          + 실제로 한번 통과되면 그걸 끝임. 왜냐하면 EU 의회가 스스로 법을 제안하지 못해서 기존 법을 스스로 폐지하지도 못함. 즉, 항상 시민들은 표결 때마다 이겨야 하고, 통과는 단 한 번만 성공하면 됨
     * 원 포스터가 만약 이 글을 본다면, 지적된 대로 alt text에 오류가 있음. '독일, 폴란드, 오스트리아, 슬로베니아, 크로아티아, 네덜란드가 반대/중립이라 녹색'이라 했는데, 실제로는 네덜란드, 폴란드, 오스트리아만 명확히 반대임. 아마 예전 버전에서 복붙하다 생긴 실수인 듯. 업데이트가 필요함
     * 군비 증가, NATO 방위세 5% 등 미국에서 화석연료 수입에 쓰는 돈, 그리고 국민 자체에 대한 감시 강화로 유럽이 점점 더 매력 없어짐. 나는 90년대 말부터 온라인 프라이버시를 지키려고 싸워왔음. 이제는 정말 지침. 정치가들은 항상 이런 괴상한 아이디어를 계속 꺼냄. 이들이 미끄러운 경사를 타고 가지 않을 거라고 매번 주장하지만, 결국 권력은 항상 '좋은 의도'로 사용된다고 강조함. 우리를 보호해주겠다는 명분이지만, 그 의도가 아무리 순수해도 결과적으로는 그 자체로 매우 위험한 경사로임
          + 결국 지옥은 좋은 의도로 포장되어 있는 것임
"
"https://news.hada.io/topic?id=22259","ACM, 전면 오픈 액세스로 전환","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ACM, 전면 오픈 액세스로 전환

     * ACM이 2025년 말까지 전체 연구 콘텐츠를 전면 오픈 액세스로 전환함
     * 이 변화는 컴퓨터 과학 연구의 접근성 확대와 혁신 가속화를 목표로 함
     * ‘ACM Open’ 프로그램을 통해 기관별 투명한 가격과 무제한 논문 출판 서비스를 제공함
     * ACM Digital Library는 2026년부터 프리미엄과 베이직 두 가지 접근 옵션을 도입함
     * 오픈 액세스 출판 비용은 논문 유형 및 ACM 회원 여부에 따라 차등 적용됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ACM의 전면 오픈 액세스 전환: 컴퓨팅 연구의 새 시대

     * ACM은 지난 70년 이상 동안 컴퓨팅 분야 연구 발전과 지식 확산에서 중요한 역할을 수행해왔음
     * 이제 ACM은 2025년 말까지 모든 연구 콘텐츠에 대해 전면 오픈 액세스(오픈 접근) 출판 모델로 전환할 예정
     * 이 결정은 ACM이 추진 중인 다양한 교육·다양성·공공정책·커뮤니티 인식 프로그램의 지속 가능성을 고려해 점진적으로 진행되는 단계적 접근 방식

     * 전면 오픈 액세스 전환은 단순한 정책 변화가 아니라, 지식 접근 장벽 해소 및 발견과 혁신 촉진을 위한 ACM 커뮤니티에 대한 책임감 표현임

ACM의 오픈 액세스 전략은 다음 두 가지 핵심 요소를 중심으로 구성됨

     * 1. ACM Open – 기관을 위한 지속 가능한 오픈 액세스 프레임워크
          + ACM Open은 Read & Publish 모델을 혁신적으로 변형하여, 기관이 오픈 액세스 출판을 원활히 지원할 수 있게 하는 프로그램임
          + 기관 단위의 가격 책정은 최근 3년간 해당 기관의 출판 실적을 바탕으로 산출되며, 계약 기간 동안 고정됨
          + 이 모델은 예산 예측을 가능하게 하고, 기관에 무제한 오픈 액세스 논문 출판과 ACM 콘텐츠 전체 열람 권한을 제공함
          + ACM Open의 주요 장점
               o 무제한 오픈 액세스 출판
                 참여 기관 소속 교신저자는 별도 논문 처리 비용(APC) 없이 무제한으로 오픈 액세스 논문을 출판할 수 있음
               o 공정하고 예측 가능한 가격
                 최근 출판 실적에 기반한 고정 연간 가격으로, 비용 투명성 및 예산 안정성 확보 가능
                 기업 및 정부 기관의 개별 가격은 별도 문의 필요
               o 프리미엄 디지털 라이브러리 접근
                 참여 기관에는 ACM Digital Library Premium 전체 접근 권한이 제공되어
                 80만 편이 넘는 ACM 논문, 6,500곳 이상의 타 출판사 콘텐츠 색인, 고급 도구, 독점 기능 등을 무제한 이용 가능함
     * 2. ACM Digital Library – 맞춤형 접근성과 참여
          + 2026년 1월부터 Premium과 Basic 두 가지 접근성 옵션이 도입됨
          + ACM Open에 참여한 기관은 자동으로 Premium 접근 권한을 받으며,
          + 단독 Premium 구독도 별도 제공될 예정임
          + Premium ACM Digital Library의 주요 기능
               o ACM Guide to Computing Literature를 통한 ACM 외부 연구 자료 색인 및 접근
               o AI 기반 검색, 일괄 다운로드, 인용 관리 등 고급 연구 도구로 연구 흐름 개선
               o 이용 통계, 인용 추이, Altmetric 같은 심층 분석 도구로 연구 영향력 평가 가능

        Hacker News 의견

     * USENIX와 그 컨퍼런스들은 연구자로서 논문 제출이나 PC 활동에만 집중할 수 있게 도와줌, 다양한 봉사자에 의존하는 ACM과 달리, 전체 행사 조직을 지원해 줌, ""general chairs""나 ""local chairs"" 같은 역할이 없으며, 모든 논문이 별도의 로그인 없이 컨퍼런스 웹사이트에서 바로 PDF로 열람 가능함
          + 많은 USENIX 논문들이 웹사이트에서 바로 PDF로 볼 수 있지만, 진정한 ""오픈 액세스""는 아님, 오픈 액세스의 정의는 베를린 선언에서 확인 가능함, 다만 이는 악의적인 목적이 아니라 오픈액세스가 중요하게 인식되기 전, Creative Commons가 설립되기 전의 관행임, 예시로 이 논문에는 어떤 라이선스도 표기되어 있지 않음, 현 저작권법상 누가 무단으로 논문을 재배포하면 불법이 된다는 점이 아쉬움
          + 나는 현재 ACM SIGCHI 컨퍼런스의 publication chair이며, 실제로 ACM 측에서는 Sheridan publishing이 모든 일을 맡아 프로세스가 많이 간소화됨, 메인 논문 트랙은 최근 몇 년간 저널 체계로 전환되어 있고, 실제로는 30개 워크샵과 부가 논문들의 마감 관리가 가장 일이 많음, 우리는 현재 구 시스템이지만, 일부 대학은 미리 게재료를 결제하고, 일부는 논문별로 저자가 직접 내야 하는 등 변화에 따른 영향이 궁금함
          + 당신의 연구가 내 짧은 학계 생활에 큰 영향을 끼쳤음, 여기서 당신의 닉네임을 보니 신기함, 고마움
          + ""로그인 없이"" 논문을 볼 수 있다는 표현을 유머러스하게 "" ;login:""(USENIX의 유명한 잡지명)이라고 말하고 싶음, 관련 정보는 여기서 확인 가능함
     * 이 정도로 높은 오픈 액세스 게재료는 영리 출판사의 특징과 일치함, 나는 독일이 지원하는 공공 서비스인 LIPIcs 시스템이 훨씬 더 좋다고 생각함, 편집자에게 논문을 제출하는 인터페이스도 매우 현대적이고 실용적임
     * 정말 엄청남! 이제 읽어야 할 옛 문서가 엄청 많아짐, 혹시 ACM 멋진 논문 클럽 만들 사람?
          + 예전에 한 교수님이 은퇴할 때 ""ACM Transactions on Graphics"" 인쇄본 200호 정도를 얻었던 적이 있음, 70~80년대 자료들로 당시 3D 그래픽이 '선을 그리는 법'에서 '확률적 모션 블러 하이퍼서피스 포톤 트레이싱'으로 발전하던 시대임, 나는 그 논문들을 여가에 읽는 걸로 즐겼음, 정말 놀라웠음
          + Brett Victor가 큐레이팅한 훌륭한 논문 리스트가 여기 있음
          + 약 20년 전 자료부터는 오랫동안 오픈 액세스라서 일찍 시작했어야 했다는 점을 말하고 싶음
          + 나도 완전 동참하고 싶음, 디스코드 서버 같은 걸로 만들어 보면 좋겠음
          + 나는 최근에 이 논문을 읽기 시작했음
     * 나는 ACM의 오픈 액세스 정책 변화에 감사하고 있음, 하지만 라이선스가 뭔지 꼭 묻고 싶음, ""오픈 액세스""를 정의하는 베를린 선언의 두 조건 중 대부분은 #2(아카이빙)에 대한 이야기이고, 진짜 핵심은 #1(복사, 공유, 2차 저작물까지 허용하는 명확한 라이선스 부여)임, 예시로 CLU에서 추상 자료형을 처음 도입한 Liskov 논문의 링크를 보았지만, 해당 페이지와 PDF 어디에도 이런 자유로운 라이선스가 명시되어 있지 않음, 그래서 내가 이 논문을 내 웹사이트나 위키소스, 인터넷아카이브에 올리면 여전히 저작권 문제 소지가 있음, 클라우드플레어가 나를 ""사람""으로 생각하는 동안만 접근이 가능한 상황임, 진정한 오픈 액세스가 해결해야 할 문제임
          + 새로운 논문들은 Creative Commons(CC-BY 또는 CC-BY-NC-ND) 라이선스임
     * 나는 수년 전 ACM이 오픈 액세스를 거부하는 것에 실망해서 멤버십을 취소했음
          + 이제 재구독할 때임, 바뀐 정책을 직접 지갑으로 응원하는 것이 중요함
     * 게재료가 700달러에서 1800달러 수준임, 진정한 협상의 기술(?) 같음
          + 더 나아가 많은 논문을 받아들이도록 유인이 생기기 때문에 품질에 분명한 영향을 미침
          + 사실 언제나 그렇게 운영되어 왔음
     * ""기관에서 ACM Open 구독 시 프리미엄 버전과 모든 논문에 접근, 고급 기능, 독점 툴 제공""이라는 문구의 의미가 궁금함, 80만 편의 기존 논문들은 계속 유료이고 신간만 오픈인지, 아니면 개인은 오픈이고 기관은 계속 비용이 발생하는 구조인지 등 궁금증이 많음
          + 모든 논문이 무료로 열람 가능해짐, ACM Open 구독을 하면 논문 게재를 보다 저렴하게 할 수 있고, AI 검색, 일괄 다운로드, 인용 관리, 논문 이용 통계, 인용 트렌드, 알트메트릭 추적 등 부가 기능이 제공됨
     * 정말 오래 기다려온 변화임
"
"https://news.hada.io/topic?id=22257","전 세계에 프라모델 신드롬을 일으킨 Shunsaku Tamiya의 타계","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                전 세계에 프라모델 신드롬을 일으킨 Shunsaku Tamiya의 타계

     * Shunsaku Tamiya는 Tamiya Inc. 의 전 사장·회장으로, 정밀 프라스틱 모델 키트 산업을 혁신함
     * 그는 회사의 방향을 운송업에서 목재 및 프라모델 사업으로 성공적으로 전환함
     * 브랜드 아이덴티티와 박스 디자인, 그리고 타의 추종을 불허하는 품질 기준을 확립함
     * 정확도와 품질을 위해 실제 차량을 분해하고 재조립하는 집념을 보임
     * 그의 공로는 2018년 일본 미디어 예술제에서 평생공로상으로 인정받음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Shunsaku Tamiya와 Tamiya Inc.의 혁신적 여정

     * Shunsaku Tamiya는 7월 18일 별세함
     * 그의 사망은 7월 21일 공식적으로 발표되었으며, 장례식은 가까운 가족과 친구가 참석한 가운데 이루어짐
     * Tamiya Inc.는 프라스틱 모델 키트, 무선 조종 자동차, Mini 4WD 등 다양한 혁신적 취미 아이템을 개발한 것으로 유명함

회사의 시작과 변화

     * Tamiya Inc.는 원래 운송업체로 Shunsaku의 아버지에 의해 설립됨
     * 전쟁 후 목재 공급업 체제로 전환하였고, 이 과정에서 주로 군함과 비행기 형태의 목재 키트 모델 사업이 성장하게 됨
     * 한 차례 화재로 목재 공급이 막히면서 모델 사업에 집중하게 되는 계기를 맞이함

프라스틱 모델로의 대전환

     * 프라스틱 기술의 발전이 업계를 변화시키는 상황에서, Shunsaku Tamiya는 회사가 '프라모(프라스틱+모델)'로 전환하는 선봉장이 됨
     * 그 결정으로 여러 세대의 어린이와 성인에게 자동차와 다양한 탈것에 대한 열정을 자극함

Tamiya 브랜드와 품질 기준

     * Shunsaku Tamiya는 Tamiya만의 브랜딩과 '트윈 스타' 로고, 하얀 박스 디자인 등 뚜렷한 아이덴티티를 구축함
          + 그의 형 Masao가 로고를, 전문 일러스트레이터들이 박스 아트를 맡음
     * Tamiya의 이름을 세계적으로 알린 품질과 정밀함의 기준을 확립함

전설로 남은 엔지니어링과 집념

     * 1976 Porsche 934 Turbo RSR 모델 키트 제작 과정에서 디자이너를 실제 포르쉐 공장에 여러 번 보내 실물 치수와 디테일을 직접 확인함
     * 그럼에도 정확한 재현이 의심되자, 실제 Porsche 911을 구입해 완전히 분해하여 모든 부분을 정확히 재현함
     * 분해한 차량은 다시 조립되었고, Shunsaku Tamiya 본인이 1976년 Japan Grand Prix에서 Jody Scheckter와 부인을 직접 운전해 태워주는 일화로 남음

유산과 영예

     * 2018년 일본 미디어 예술제에서 평생공로상을 수상함
     * 그의 영향력은 자동차와 프라모델을 사랑하는 이들의 상상력과 열정으로 오래도록 기억될 것임
     * 90세로 별세함

        Hacker News 의견

     * 어릴 때 동네 모형점에서 플라스틱 모델 키트 구경하는 데 여러 시간을 보내곤 했음, 조립하는 게 정말 즐거웠음
       도색은 끝내 마스터하지 못했고, 가장 발전한 게 스프레이(라틀캔) 페인트 정도였음, 가끔 위장 무늬를 위해 마스킹을 시도했을 뿐임
       성인이 되어 다시 모형에 빠진 후에야 에어브러싱에 도전해서 제대로 마감 작업을 할 수 있게 됨
       요즘 YouTuber(Aztec Dummy 등)를 통해 진짜 중요한 건 조립이 아니라 도색, 그리고 조명이라는 사실을 알게 됨
       그래도 어린 시절엔 단순히 조립하는 것만으로 엄청난 즐거움이 있었음
       본드 냄새, 공간지각 능력 향상… 내게는 3차원 조각과도 같은 경험이었음
       자동차, 비행기, 우주선 형태의 라인을 정말 좋아하게 됐고, 결국 내 디자인의 자동차나 우주선을 스케치하게 됨
       이 취미 덕에 디자이너 마인드도 생긴 것 같음
       하지만 요즘 이 취미가 R/C 비행기나 모형 로켓처럼 사라진 듯해 아쉬움
       옛날엔 거의 모든 남자애들이 침대 천장에 모형 하나쯤은 매달아뒀던 것 같은데 말임
          + 근육차나 선박 같이 조립하는 플라스틱 모델 키트는 확실히 예전만큼 인기가 없지만, Warhammer와 Gundam은 지금도 엄청 인기가 많음
            Battletech, Star Wars도 우리 동네에서는 인기 많음
            Warhammer 모델이 많고, Games Workshop은 영국을 기반으로 세계적으로 사업을 확장 중임
            Warhammer의 재미는 방대한 소설, 비디오게임 등 강력한 IP와 늘 새로 나오는 다양한 팩션별 설정 덕분에 팬들이 깊이 몰입할 수 있음
            우리 중형 도시에도 Warhammer 게임을 지원하는 가게가 3곳이나 있고, 항상 게임이 열림
            경고: 진짜 비싼 취미임
            3D 프린팅 인기도 많아서, thingverse 등에서 WW2나 머슬카 관련 3D 모델 다운로드가 얼마나 많은지 보면 왜 하비샵이 힘든지 감이 올 것임
            모형 취미는 죽지 않았고, 그냥 진화된 것일 뿐임
            WW2 키트 좋아했다면 Bolt-Action도 한번 알아보길 추천함, 규모는 40k만 못해도 하는 사람 있음
          + 이제는 어린 남자아이들의 대표 취미는 아니지만, 오히려 일정 소득이 있는 성인 타겟으로 킷의 퀄리티가 최전성기를 맞이하고 있다고 느껴짐
            Tamiya, Eduard, Meng 같은 제조사들은 굉장히 훌륭한 품질의 키트를 만듦
            늘 다음 세대를 누가 이끌까 걱정하는데, Gundam과 Gunpla 팬덤은 일본에서 매우 크고 서구권에서도 성장 중임
          + 나도 도색을 잘 못했음
            구형 오일 베이스 도료로 도색하는 게 진짜 어려웠음
            나중에 Games Workshop 같은 회사의 수성 도료를 쓰면서 훨씬 편하게 작업할 수 있었음
          + 조립에도 할 일이 많음
            런너에서 부품 떼어내기, 몰드 라인 제거, 갭 필러 바르고 사포질해서 완전 매끈하게 만드는 작업들이 있음
            나는 도색만큼 이 조립 과정도 아주 재미있다고 느끼는 편임
          + 요즘은 온라인의 즉각적인 재미와 경쟁해서, 프라모델 같은 오프라인 취미가 어려워진 것 같음
            그래도 R/C 비행기, 모형 로켓도 여전히 살아있음
            영국에 있는 공학, 로켓에 관심있는 학생들에게는 UKROC 대회를 추천함
            미국, 프랑스, 일본에도 비슷한 대회가 있음
     * 80년대에 나는 Tamiya Grasshopper가 있었는데, 내 인생 최고의 R/C 자동차였다고 생각함
       배터리팩을 3개나 달아서 엄청난 속도로 달리게 했고, 그 덕에 타이어가 다 찢어지고 코너 돌 때마다 자동차가 몇천 번씩 뒤집혔음
       그래도 처음 몇 초는 정말 짜릿했음
       Grasshopper와 Shunsaku Tamiya 모두에게 애도를 표함
       Grasshopper 사진 보기
          + 나는 Hornet을 가졌었고, 늘 Hotshot을 갖고 싶었음
            Beatties 매장에서 홍보 영상을 보는 시간이 행복했음
            Hornet 정보
            Hotshot 비교
            Beatties 홍보영상
          + 나는 그 시절 Boomerang 4WD 자동차를 가지고 있었음
            엄청 사랑했고, 매주 수 시간씩 사용했음
            내 인생 최고의 장난감임 (Amiga A500 빼고!)
            사진 보기
          + 나도 Grasshopper를 가졌었음
            입문자용으로 좋은 차였고, 여전히 Hot Shot은 내 아마존 위시리스트에 있음
            타이어 세 번, 서스펜션 두 번 교체한 뒤엔 돈이 없어 손을 놓게 됨
            그걸 겨우 손에 넣으려고 정말 돈을 열심히 모았었음
            Tamiya는 당시에 최상급 브랜드였음
            Hobby Town이나 동네 하비샵에 가서 Tamiya 제품을 바라보며 언젠가는 나도 사겠다고 꿈꿨음
            결국 지금은 다른 취미를 하고 있지만 F14는 여전히 눈여겨보고 있음
          + Grasshopper는 괜찮은 모델이었음
            Hornet보다 모터가 약간 작은 380을 써서 출력이 떨어지지만, 옛날 7.2V 배터리로는 주행 시간이 길다는 장점이 있었음
            요즘 유튜브에 올라오는 새로운 R/C카들은 출력이 너무 세서 제대로 즐기기도 힘든 수준임
            영상마다 얼마나 높이 점프하다가 부서지는지 보여주는데 집중될 때가 많음
          + 80년대에 Tamiya Grasshopper 갖고 있었다면, 혹시 rc10로 진화하지 않았나 궁금함
     * 엄청 유명한 일화가 있는데, 1976 Porsche 934 Turbo RSR 프라모델 키트를 만들 때 Tamiya 회장이 디자이너들을 포르쉐 본사에 여러 번 보내서 치수를 완벽하게 맞추도록 했음
       그런데도 계속 의심이 남자 결국 포르쉐 911을 통째로 실제로 사서 완전히 분해해가며 세부사항까지 정확하게 모델링했음
       미국이라면 저런 행동이 신형 포르쉐 차값을 세금 공제받으려는 꼼수일 것 같기도 함
       ""100% 사업 목적이었어요~""라는 식?
     * 이런 부고 기사를 볼 때마다 마음이 아파짐
       그의 존재를 몰랐지만, 읽어보니 세상에 실제로 큰 영향을 끼쳤다는 걸 부정할 수 없음
       인터넷 다른 곳에서는 사람들이 취미가 다르다고 해당 취미를 폄하하곤 하는데, 나 같은 경우 자폐 스펙트럼이라 세밀한 손재주가 부족해서 복잡한 킷의 도색이나 조립은 어려웠음
       프라모델은 예술이자 숙련이 필요한 장인정신임
       아쉽게도 크게 유행하지 않다 보니, 사회 전체가 너무 온라인에만 몰입하는 시대라서 걱정임
       나는 레고를 더 좋아했지만, 요즘 대형 마트의 어린이 코너에 가보니 킷 자체가 훨씬 간단하고 거의 대부분 영화 관련 캐릭터가 주를 이룸
       예전엔 우주, 해적, 우주 해적 같은 상상력이 중심이었는데…
       어쨌든, 진정한 해커 정신으로 열정을 품었던 그에게 다시 한번 R.I.P를 보냄
          + 아이들이 레고를 설명서만 따라 조립하고 끝내는 건 좀 아쉽다고 생각함
            상상력을 써서 각자만의 창작물을 지어보길 추천하고 싶음
            팁: 이베이나 중고사이트에서 레고 부품을 대량으로 싸게 구할 수 있음
            빨래망에 담아서 식기세척기 저온 세탁 돌리면 새 것처럼 쓸 수 있음
          + 프라모델 제작은 진짜 예술이라고 생각함
            그만한 가치를 제대로 인정받지 못하는 점이 아쉬움
     * 중학교 시절 R/C카에 빠져 있을 때, 누가 Tamiya TA02 키트를 흥미를 잃고 내게 주었음
       이 키트는 기어박스 설계 결함 탓에 스퍼기어가 빨리 닳고, Losi XX에 비해 속도도 느렸음
       그래도 TA02는 레이싱보다는 스케일 재현에 더 중점을 두었던 킷임
       Tamiya는 Clodbuster라는 전설적인 R/C카도 생산했는데, 4륜 조향에 2개 모터, 그리고 트레일링암/섀시 개조로 인기 있었음
       트랙에 실제로 가져온 사람은 항상 주목받았음
       아직도 TA02를 보관 중이고, 지금은 IMSA Nissan 300Z 대신 E46 M3 바디로 튜닝되어 있음
       TA02 정보
       Clodbuster 정보
     * 영국, 프랑스, 미국, 일본 등 여러 나라에서 부티크 느낌의 회사들이 다양한 클래식 비행기, 특히 2차대전 전투기 플라모델을 여러 스케일로 만드는 게 이상하게 흐뭇하게 느껴짐
       어릴 땐 Airfix, Revell, Heller, Frog, Tamiya, 그리고 Haya-뭐시기(아마 Hasegawa?) 같은 제조사 이름을 줄줄 외웠음
       아직도 하비샵 디스플레이를 보면 반가움
       50년 넘게 이 시장을 버티는 회사가 많음
       딱히 결론은 없고, 업계에 “아 저 1:50 Messerschmidt BF-109 수직미익은 역시 Pierre McFloogle의 작품이지!” 같은 유명 금형 장인이 있었으면 좋겠다 싶음
          + 어떤 프로토타입을 모델로 삼을지 그 선정 기준이 늘 궁금했음
            WWII 비행기 모형에선 일본, 소련 디자인이 미/영/독에 비해 덜 나옴
            함선모형도 오래 살아남은 함정보다 드라마틱하게 침몰한 함이 더 인기임
            철도모형은 일정한 풍경(예, 1952년 퀘벡)을 테마로 해야 해서 희귀 클래스를 넣기 어려운데, 프라모델은 제약이 거의 없음
          + Haya-뭐시기는 Hasegawa를 뜻하는 것 같음
     * 관련 기사:
       Tamiya 회장 Shunsaku Tamiya 별세 기사 (2025년 7월)
     * 학창시절 동독에서 Plasticart 위주로 모델을 만들긴 했지만, 진짜 영감을 준 건 철도모형 가게였음
       그 가게가 온갖 디자인과 일러스트에 대한 내 애정을 키워준 주인공임
       박스 디자인도 너무 예뻤고, 옛날에는 Dragon Models DML도 좋아했음
       Mr. Tamiya께 애도를 전함
     * 프라모델 취미가 다른 취미로도 확장되어서, 에어소프트나 드론 시장에서도 tamiya 커넥터가 많이 쓰임
          + 안타깝게도, Tamiya 커넥터는 최신 RC 전기장치(브러시리스 모터, LiPo 배터리)에는 너무 약해서 쉽게 녹아버림
            그래서 지금은 보통 Deans나 XT 커넥터를 씀
     * Hornet 모델을 추억함, 나중엔 959도 가졌었음
       애들이 친구의 RC카를 들어 5cm쯤 떨어뜨려 서스펜션을 확인하고는 다 같이 공감 어린 표정을 짓던 순간들이 생각남
       아무 의미 없었지만 행복했던 기억임
       Mr. Tamiya께 애도와 감사를 전함
"
"https://news.hada.io/topic?id=22285","Show GN: LogSentinelAI — 선언만 하면 LLM이 로그를 분석 (PoC)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: LogSentinelAI — 선언만 하면 LLM이 로그를 분석 (PoC)

   안녕하세요!
   로그 분석 자동화와 관련해서 LogSentinelAI라는 오픈소스 프로젝트를 만들고 있습니다.

   이 도구는 Apache, Linux 등 다양한 시스템 로그에서 보안 이벤트나 이상 징후를 LLM(AI) 으로 분석해주고, Elasticsearch/Kibana로 시각화까지 연동할 수 있습니다.
   GeoIP, 실시간 모니터링, SSH 원격 로그 분석 등도 지원하고, 결과는 구조화된 JSON으로 나와서 통계나 대시보드 작업에 바로 쓸 수 있어요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   ⚡️ Declarative Extraction (선언적 추출)
   LogSentinelAI의 핵심 기능은 개발자가 원하는 분석 결과 구조만 선언하면, LLM이 해당 구조에 맞춰 자동으로 로그를 분석하고 JSON으로 반환해주는 방식입니다.
   즉, 복잡한 파싱이나 후처리 없이 “무엇을 뽑을지”만 정의하면, “어떻게 뽑을지”는 AI가 알아서 처리합니다.
# 예시: HTTP Access 로그 분석기에서 원하는 결과 구조만 선언하면,
from pydantic import BaseModel

class MyAccessLogResult(BaseModel):
    ip: str
    url: str
    is_attack: bool

# 위처럼 결과 구조(Pydantic class)만 정의하면,
# LLM이 자동으로 각 로그를 분석해서 아래와 같은 JSON을 반환합니다:
# {
#   ""ip"": ""192.168.0.1"",
#   ""url"": ""/admin.php"",
#   ""is_attack"": true
# }

   아직 부족한 점이 많지만, 혹시 로그 분석 자동화나 보안에 관심 있으신 분들께서 한 번 써보시고,
   개선점이나 의견을 나눠주시면 정말 큰 도움이 될 것 같습니다.

   프로젝트 주소: https://github.com/call518/LogSentinelAI

   감사합니다!
"
"https://news.hada.io/topic?id=22343","세포도 기억할 수 있을까?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             세포도 기억할 수 있을까?

    세포도 기억할 수 있을까?

     * 핵심 요점
          + 단일 세포도 경험을 기록하고 학습·기억할 가능성이 있음
          + 초기 실험(제닝스·겔버 등)과 최근 연구가 이를 뒷받침
          + 세포 기억은 생존에 유리하며, 신경계 외에서도 나타남
          + 과거 학문적 편견으로 연구가 배척되었으나 재조명 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

      서론: 세포 기억 연구의 부활

     * 바바라 맥클린톡은 1983년 노벨상 연설에서 “세포는 자신을 아는가?”라는 질문 제기
     * 최근 연구는 단일 세포가 경험을 기록·활용할 수 있는지 탐구
     * 기존 신경과학은 기억을 ‘시냅스 가소성’과 다세포 신경망 결과로 한정했으나, 새로운 증거가 이를 확장함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

      본론

   1. 초기 단세포 학습 사례
     * 1906년 제닝스: 단세포 섬모충(Stentor roeselii)이 자극 반복 시 행동을 조정(구부리기→물뿜기→수축)
     * 반복 자극 후 행동 단계를 생략하거나 이주하는 등 ‘경험에 따른 변화’ 관찰
     * 겔버: 단세포 원생동물을 음식과 자극 연합 학습시키는 실험 수행
     * 당시 주류 ‘생물 자동반응’ 이론에 반해 연구가 배척됨

   2. 현대 연구와 과학적 재현
     * 쿠쿠쉬킨(NYU) 연구팀: 인간 신장세포와 미성숙 신경세포도 화학 신호 간격 패턴을 ‘기억’
     * 연속 자극보다 일정 간격 자극에서 더 오래 반응 유지 → 동물 기억의 ‘간격 효과’와 유사
     * 진화적 관점: 뇌 없는 세포도 경험을 기록하면 생존에 유리
     * 점액곰팡이·세균 등에서도 유사한 경향 확인

   3. 세포 기억의 개념과 정의 확장
     * 세포 수준 기억 = 환경 변화에 대한 체화된 반응
     * 행동 기반 정의의 한계: 외부 반응이 없으면 기억으로 인정되지 않음
     * 기억을 ‘경험이 남긴 물리적 흔적’으로 확대 → 예방접종, 흉터, 분자·후성유전적 변화 포함 가능

   4. 학문적 편견과 사회적 요인
     * 과거 연구는 지배적 이론(트로피즘·행동주의)과 맞지 않아 무시됨
     * 과학 공동체의 인식·용어·측정 방식이 연구 방향에 큰 영향
     * 현재는 세포 기억 연구가 재조명되며, 다양한 생명체·세포에 적용 가능성 확대
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

      결론: 세포가 남기는 기억의 의미

     * 세포 기억은 생존을 위해 환경 정보를 기록하고 활용하는 과정
     * 인간의 기억과 유사하게, 경험은 다양한 형태의 ‘흔적’으로 남음
     * 과거 편견으로 묻힌 연구가 부활하며, 기억 개념의 범위를 확장
     * 세포 수준 이해는 생물학적 기억의 근본 원리를 재정립할 가능성이 있음
"
"https://news.hada.io/topic?id=22313","영수증 프린터와 파이썬으로 ADHD를 극복하기 - 유투브","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    영수증 프린터와 파이썬으로 ADHD를 극복하기 - 유투브

영수증 프린터로 ADHD를 극복한 개발자, 생산성 시스템의 비밀

   파이썬과 영수증 프린터를 이용해 자신의 ADHD를 극복하고 생산성을 획기적으로 높인 시스템을 구축했습니다.

    게임 심리학에서 얻은 영감

   이 시스템의 핵심은 '시각적 진행 상황'에 있습니다. 개발자는 로리 해럴(Laurie Harrell)의 ""영수증 프린터가 나의 꾸물거림을 고쳤다"" 라는 글에서 영감을 얻었습니다. 이 글의 아이디어는 비디오 게임이 중독적인 이유와 유사합니다. 거대한 목표(예: 레벨 80 달성)를 제시하는 대신, 눈앞의 작은 퀘스트(예: 작은 곤충 처치)를 완료하며 얻는 시각적, 청각적 피드백과 작은 성취감이 계속해서 나아가게 만드는 원동력이 된다는 것입니다.

   그는 과거 스타벅스와 피자 가게에서 일했던 경험을 떠올렸습니다. 스타벅스에서는 만들어야 할 12개의 컵을 한 번에 쌓아두는 대신, 한 줄로 늘어놓아 한 번에 한두 잔에만 집중할 수 있게 했습니다. 피자 가게에서는 주문이 완료되면 티켓을 스파이크에 꽂아 처리했음을 시각적으로 확인했습니다.

    파이썬과 영수증 프린터로 만든 물리적 시스템

   이 아이디어를 바탕으로, 그는 영수증 프린터를 이용한 물리적인 생산성 시스템을 구축했습니다.
    1. 하드웨어 설정: 저렴한 영수증 프린터를 구비하고, 파이썬의 escPOS 패키지를 사용하여 프린터를 제어했습니다. 처음에는 소음 문제가 있었지만, 프린터 하단의 딥 스위치를 조절하여 '저소음 모드'로 설정하는 데 성공했습니다.
    2. 시각적으로 아름다운 티켓: 단순한 텍스트 대신, 웹 기술(Tailwind)로 티켓을 보기 좋게 디자인했습니다. 이 웹페이지를 파이썬 패키지를 이용해 이미지로 변환한 뒤, 영수증 프린터로 이미지를 직접 인쇄하여 가독성과 만족도를 높였습니다.

    AI와 자동화로 시스템 업그레이드

   기본적인 시스템을 넘어, 그는 'Arcade'라는 도구를 활용하여 전체 프로세스를 자동화하고 지능화했습니다.
     * 자동 작업 생성: Gmail과 시스템을 연동하여 클라이언트에게 이메일이 오면 내용을 분석하고, 필요한 경우 자동으로 작업 티켓을 인쇄하도록 만들었습니다.
     * 칸반 보드 활용: '할 일(To Do)', '하는 중(Doing)', '완료(Done)'로 구성된 실제 화이트보드를 만들어 인쇄된 티켓을 시각적으로 정리하고 진행 상황을 한눈에 파악할 수 있도록 했습니다.
     * 중복 방지: 작업 내용과 설명을 벡터 데이터베이스에 저장하여, 새로운 작업이 생성될 때 기존 작업과 유사도를 비교해 중복 티켓이 인쇄되는 것을 방지했습니다.

    결론: 나만의 생산성 시스템을 찾아서

   개발자는 이 '아날로그 같은' 시스템이 자신처럼 시각적 계층 구조가 필요한 사람에게는 생산성을 크게 향상시킬 수 있다고 말합니다. 그는 생산성 시스템은 결국 개인의 필요에 맞춰져야 한다고 강조하며, 누구나 자신만의 시스템을 만들 수 있도록 GitHub를 통해 전체 소스 코드를 공유 했습니다.

   다시 보니 저도 만들고싶네요

   로리 해럴의 글은 영수증 프린터가 내 미루는 버릇을 고쳐줌으로 올라온 적이 있습니다.
"
"https://news.hada.io/topic?id=22258","LLM 애플리케이션에서의 Authorization","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      LLM 애플리케이션에서의 Authorization

     * 대형 언어 모델(LLM)은 인간 사용자의 불확실한 입력을 처리하는 예측 불가능한 시스템이기 때문에, 최소 권한 원칙(least privilege) 적용이 필수적임
     * LLM이 실제로는 내부 검색·자동화 등 다양한 업무에 활용되면서, 최소한의 권한만 부여된 “효과적 권한(effective permissions)” 에서만 동작하도록 해야 보안 사고·오용을 방지할 수 있음
     * RAG(검색 증강 생성) 구조에서는 데이터 임베딩과 권한 체크를 데이터 소스와 분리하여, 리소스 레벨에서 정밀한 권한 제어가 필요함
     * 외부(3rd party) 데이터/RAG, Agent, MCP 등 복잡한 활용이 늘수록, 실제 권한 적용 위치와 방식이 보안의 핵심 이슈로 부상함
     * OAuth 등 토큰 기반 인증은 리소스 단위의 세밀한 권한 통제에 한계가 있어, 실제 권한 로직은 애플리케이션 레이어에서 직접 구현해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

용어 및 기본 개념

     * Prompt: LLM에 전달하는 사용자의 요청(명령문, 질문 등)
     * RAG(Retrieval-Augmented Generation): 프롬프트에 추가 데이터를 첨부해 LLM의 응답 정확도를 높이는 워크플로우(예: 회사 휴가 목록을 자동으로 첨부)
     * Context: 프롬프트에 첨부되는 보조 데이터(검색된 관련 자료 등)
     * Embedding: 텍스트의 수치화된 벡터 표현, 데이터 검색/매칭에 활용
     * Agent: 프롬프트에 따라 액션을 수행하는 LLM 기반 실행 엔진(툴 자동 호출 등)
     * Tool: LLM이 직접 호출할 수 있는 API/애플리케이션 등 외부 기능
     * Model Context Protocol(MCP): Anthropic이 제안한 LLM의 툴 액세스 표준 프로토콜

LLM 권한 모델의 핵심 원칙

     * Golden Rule:

     LLM은 사용자의 요청을 처리하는 데 반드시 필요한 최소 권한만으로 동작해야 함
     * 인간 사용자에게는 ""관행적 과다 권한""이 어느 정도 허용되지만, LLM은 예측 불가능하고 빠르며 실수 시 대규모 피해 발생 가능.
       → LLM의 “실효 권한(effective permissions)”은 사용자·LLM·태스크 권한의 교집합으로 한정해야 함

  실효 권한(effective permissions) 계산

     * LLM 애플리케이션의 실효 권한 =
         1. LLM이 가진 권한
         2. 사용자가 가진 권한
         3. 요청된 태스크에 필요한 권한
            이 세 가지의 교집합
     * 사용자는 LLM(챗봇 등)에게 자신의 역할을 “위임(impersonation)”하지만,
       LLM과 사용자가 가진 권한 범위를 모두 넘어서면 안 됨
     * 권한 Venn 다이어그램으로 직관적으로 설명
          + 태스크 권한이 교집합에 완전히 포함될 때만 실행 허용

RAG(검색 증강 생성)와 권한 처리

  1. 1st Party(자사) 데이터 RAG

     * 예시: 사내 챗봇이 사내 소스코드에서 “비밀 키가 포함된 파일”을 찾아주는 경우
     * 임베딩: 모든 파일을 벡터로 변환해 DB에 저장, 프롬프트도 벡터로 변환해 유사도 기반 매칭
     * 권한 적용 위치:
          + 검색 결과로 반환된 “파일”의 실제 소유 조직, 종류, 리포지토리, 사용자 권한을 즉시 확인
          + 사용자가 해당 파일에 접근 가능한지 확인(리소스 레벨 권한)
          + 임베딩 → 소스 데이터 연결 과정에서 애플리케이션 계층에서 권한 검사
     * LLM 자체에 권한 로직을 넣는 것은 불안정(확률적 특성상 보장 불가)
     * 정리:
          + RAG의 핵심은, 임베딩과 원본 데이터 연결 후 사용자별/리소스별 권한을 강력하게 적용하는 것

  2. 3rd Party(외부) 데이터 RAG

     * 외부 API/시스템(예: 위키, 티켓 시스템 등)의 데이터를 임베딩해 활용
     * 문제점: 임베딩과 원본 데이터가 서로 다른 시스템에 존재 → 권한 적용 위치가 모호
     * 권한 처리 방법 세 가지
         1. 외부 시스템에 권한 위임(API 단 건 요청마다 실제 권한 확인, 느린 응답·레이트 리밋 문제)
         2. ACL(접근 제어 목록)을 애플리케이션에 동기화(정확도/신뢰성은 높지만, ACL 관리/동기화 비용 증가)
         3. 외부 권한 로직 자체를 내부에 재구현(관리·동기화 부담, 논리 복잡성 증가)
     * 결론: 실제 상황에 따라 “권한 적용 위치”와 방식의 trade-off가 중요
       (성능-간결함, 관리 비용-정밀도 등 선택 필요)

에이전트 기반 LLM(AI Agent)와 권한

     * 예시: 챗봇이 리포지토리 브랜치 삭제/이슈 닫기 등 자동 유지보수 작업
     * MCP(Model Context Protocol)로 툴(함수/API)을 LLM에 표준화 방식으로 노출
     * MCP의 각 요청마다 실효 권한 모델을 적용해야 함
       (사용자/에이전트/태스크 권한의 교집합)
     * OAuth 등 토큰 기반 인증의 한계
          + 권한 정보가 토큰에 포함되지만, 실시간 자원/리소스 단위 권한을 커버하지 못함
          + 실제로는 토큰에 일부 정보만 담고, 나머지는 애플리케이션 계층에서 별도 권한 로직 구현 필요

결론 및 실무 요약

     * LLM/RAG/Agent 환경에서 권한 관리의 핵심은 “권한 적용 위치와 방식” 선정
     * 실효 권한 모델을 통해, LLM이 “사용자+LLM+태스크” 교집합 내에서만 동작하도록 제한
     * OAuth 등 인증 토큰은 “누가 요청했는가” 식별용으로만 사용,
       실제 리소스별 권한 검증은 반드시 애플리케이션에서 수행
     * 외부 시스템 연동 시에는,
       1) 권한을 위임(성능 저하), 2) ACL 동기화(운영 복잡), 3) 권한 로직 재구현(높은 유지관리)
       등 다양한 trade-off를 고려해 설계 필요
     * 최종 요약:

     LLM은 사용자 요청을 처리하는 데 반드시 필요한 최소 권한 내에서만 동작해야 하며,
     실효 권한(effective permissions)은 LLM 권한, 사용자 권한, 태스크 권한의 교집합으로 정의
     실제 권한 검증은 리소스별, 애플리케이션 계층에서 반드시 수행해야 함
"
"https://news.hada.io/topic?id=22240","OpenAI, ChatGPT 공부 (Study) 모드 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    OpenAI, ChatGPT 공부 (Study) 모드 공개

     * ChatGPT가 실제로 학습을 돕는지, 답만 제시하는지에 대한 교육 현장의 우려를 해소하기 위해 개발
     * 단계별 안내와 상호작용을 통해 답변 대신 학습 과정을 지원함
     * 목표·스킬 수준에 따라 유도 질문을 조정, 참여와 상호작용으로 깊은 이해와 성찰을 촉진
     * 교사·과학자·교육학 전문가와 협력하여 적극적 참여 유도, 인지부하 관리, 메타인지, 자아 성찰 등 학습 과학 기반의 맞춤형 시스템 지침을 내장
     * 사용자 맞춤 수업, 지식 확인 퀴즈, 피드백, 진행 상황 추적 등 다양한 교육적 기능이 포함됨
     * 대학·고등교육 등 실제 학생 피드백을 기반으로 지속 개선 중이며, 시각화, 목표 추적, 심화 맞춤화 등 기능 추가 예정**

주요 기능

     * 인터랙티브 프롬프트: 소크라테스식 질문, 힌트, 자가 점검을 조합해 스스로 생각하게 유도
     * 추가 지원 응답: 주제 간 연결성을 고려한 단계별 정보 제공, 과도한 인지 부담 완화
     * 맞춤형 지원: 기술 수준 평가, 이전 대화 기억, 사용자 맞춤형 수업 제공
     * 지식 확인: 퀴즈·주관식 질문과 맞춤형 피드백으로 진행 추적, 기억력·적용력 강화
     * 유연성: 대화 중 언제든 공부 모드 on/off 가능

   ChatGPT에서 사용해보기

        Hacker News 의견

     * LLM이 스터디 파트너로 과소평가되고 있다고 생각함, 왜냐하면 부끄러움 없이 ""바보 같은"" 질문을 자유롭게 할 수 있기 때문임. 단답형이 아니라 단계별로 차근차근 설명해주는 모드는 진짜 마법 같다는 생각을 함. 24시간 대기하는 유능한 비서가 생긴 셈이라 혼자 공부할 때 꿈의 도구라 여김. 예전에는 온라인 정보의 부정확함, 낡음, 피드백 부재, 그리고 무뚝뚝한 커뮤니티 등 어려움이 많았는데, 이제는 그 시절과 비교할 수 없을 만큼 앞서가는 경험임. 물론 AI의 정보를 무조건 믿어야 한다는 건 아니고, 스스로 확인하는 과정이 필요함. 이걸 게으르게 사용하는 사람도 있겠지만, 오래된 책이나 교과서와 마찬가지로, 본인의 자세에 따라 도움 되는 정도가 달라진다고 생각함. 이런 툴을 쓸 수 있는 시대에 살고 있음에 무척 흥분과 감탄을 느낌
          + 몇 년 전 온라인으로 무언가를 배우면 잘못된 정보, 악의적인 답변, 즉각적인 피드백 부족 등으로 어려움이 많았다는 데 동의함. 하지만 요즘 AI는 답이 맞는지, 헛소리(환각)를 하는지 매번 의심할 수밖에 없음. 사실기반 질문을 할 때 종종 잘못된 답변을 주는 것을 여러 번 경험함. 그리고 이런 문제를 지적하면 항상 최신 모델은 개선됐다고 하며 비싼 구독료를 요구하는 현실임. 더 안 좋은 점은 AI가 답변에 반론을 제기하면 너무 쉽게 굴복한다는 것임. 스스로 답변을 방어하지 못하는데, 이건 선생님에게 필요한 성질이 아니라고 생각함. 결국 AI도 유용한 도구일 뿐, 과신해서는 안 되므로, 언제나 건강한 회의가 필요함. 사실 이건 기존 교육 방식도 마찬가지라고 봄
          + 단계별로 자료를 설명해주는 모드는 매력적이지만, 이 시스템들도 여전히 자신 있게 거짓말을 하는 문제가 있다고 느낌. 예를 들어 DuckDuckGo의 로고가 검색어에 따라 바뀐다는 이스터에그가 있는데, Copilot에 물어보면 아니라고 하고, 반박하면 갑자기 맞다고 하면서 엉뚱한 예시(예를 들어 고양이를 검색하면 고양이 모양 로고가 나온다고 설명)까지 함. Copilot은 뚜렷한 답을 모르는데 모른다고 하지 않고 거짓으로 답하는 점이 문제라고 생각함
          + 바보같은 질문을 하기 두려워하는 마음이 현실적이라고 느낌. 특히 예전에 학생을 망신주는 선생님이나 교수에게 상처를 받았던 경우 더 함. 이름난 교수라도 학생을 부끄럽게 만든 영상을 보고 그 사람의 강의를 듣는 걸 멈춘 경험이 있음
          + 학교의 기존 IT 도입을 보면 미국은 교육에 수십억을 쏟아부었지만, 실제 학습 성과가 나아지지 않았음. 이런 점이 회의론의 배경임. 또 1000억 달러를 쓰기 전에 실제로 효과가 있는지 먼저 입증해야 한다고 생각함. 아직은 결정적 증거가 없다는 입장임
          + 스페인어 B1 수준인데, ChatGPT로 맞춤형 레슨을 만들고, 언어 뉘앙스도 질문하며, 음성 연습까지 하면서 기존 앱보다 훨씬 더 뛰어난 학습 경험을 하고 있음
     * LLM은 대학 졸업 이후 새로운 것을 독학할 때 진짜 믿을 수 없을 만큼 엄청난 도구였다고 자신 있게 말할 수 있음. 예전에는 개념을 이해 못 하면 사실상 답이 전혀 안 보였으며, Stack Exchange에 흔한 질문이 아니면 스스로 헤쳐나가야 했음. 이제는 언제든 개인조교(TA)가 생긴 셈임. 학습이 너무 쉬워지거나 피상적이라는 지적이 있지만, 대학생이 TA 없을 때 더 잘 배운다고 생각할 사람은 거의 없을 것임
          + 개인적으로 내 경험상 모든 사람이 언제나 접근할 수 있는 TA 같은 존재라는 감각은 들지 않음. 어느 정도 깊이 들어가면 LLM이 곧바로 쓸모 없어짐. 특히 학술적인 신뢰성 있는 출처를 찾아야 하거나 복잡하고 논쟁적인 주제를 다루면 더더욱 도움 되지 않음
          + 최근 2020년형 9세대 Intel CPU가 장착된 오래된 기기를 수리 및 조사할 때, LLM이 각 세대와 소켓 호환성 등 정보를 손쉽게 설명해주어 마치 내가 이런 툴을 쓸 자격이 없다고 느낄 정도로 편했음. 어떤 영역엔 별로지만, 어떤 곳엔 진짜 놀라움
          + ChatGPT 출시 이후 예전 Google을 다시 찾은 듯한 기분이 들었음. 예전에는 새로운 프로그래밍 언어를 배울 때 중요한 정보를 Google에서 손쉽게 찾았는데, 몇 년 전부터 Google이 쓸모가 없어졌음. 심지어 원하는 정보가 있어도 검색결과에 묻혀버림
          + ChatGPT가 내게 맞는 학습 계획을 짜주고, 필기 및 아티클 작성을 격려하면서 12주 만에 Rust를 배웠음. 이 과정을 통해 내 필기 자료로 https://rustaceo.es를 스페인어로 만들었는데, 이런 방식의 잠재력은 무한하다고 느낌
          + 예전에 집에서 IPv6 문제를 이해하지 못해서 고생했는데, ChatGPT 덕분에 tcpdump로 트래픽을 분석하고 네트워크 동작을 차근차근 설명받았음. RA와 NDP(IPv4의 DHCP, ARP 역할을 대체함)에 대한 세부 내용도 새롭게 배움. 결국 내 mesh WiFi 네트워크에서 반복되는 이상 현상들이 저렴한 리피터 하나 때문이었다는 것도 밝힘. 5년 동안 원인을 몰라 답답했는데, 드디어 해결함
     * ChatGPT Study Mode의 시스템 프롬프트를 추출해봄. ""사용자 질문에 대해 즉답하거나 숙제를 해주지 말라. 수학/논리 문제는 즉시 풀지 않고 단계별로 질문하며, 사용자에게 각 단계마다 응답 기회를 줘라""는 식의 지침이 인상적임. gist 링크
          + 각 LLM 제공사가 ""간결하고 군더더기 없이"" 답변하라는 지침도 추가해주길 바람. 내가 느린 독서자여서 쓸데없는 설명까지 다 읽기 힘들기 때문임. 너무 빠르게 쏟아지는 답변이 오히려 불안하게 만들기도 함. 이렇게 하면 문맥 문제도 줄일 수 있을 듯
          + 대문자(CAPS)가 실제로 LLM에 의미가 있다는 점이 재밌음
          + 이 프롬프트를 다른 모델에 적용하면 어떤 결과가 나올지 궁금함. ChatGPT Study Mode가 특별한 시스템 프롬프트만으로 이뤄지는지, 아니면 그 외의 차이가 있는지 궁금하고, 비슷한 프롬프트로 주제 깊이 파고드는 학습을 해보며 긍정적인 효과를 느낀 적이 있음
          + 이렇게 내부 지침을 쉽게 드러내는 게 흥미로움. OpenAI가 시스템 프롬프트를 일부러 비공개로 한 듯하지만, 누구나 쉽게 내용에 접근할 수 있는 걸 보면 일부러 산출했다고 생각하게 됨
          + 나도 비슷한 시스템 프롬프트를 추출했는데, 이 링크에서 확인해 볼 수 있음
     * 평생 학습자로서 느끼기에 공부 시간의 큰 부분이 자료를 찾는 검색임. AI는 이 검색을 효율적으로 도와주는 것 같음. 반대로 학습 주제의 논리적 모델을 구축하는 과정은 AI에 의존하면 오히려 내가 학습한 게 아니라, AI의 ""포함""을 모아놓은 것만 같아지고, 그럴 경우 AI 없이는 결과를 못 꺼내게 됨. 내 뇌에 일관된 오프라인 모델을 저장하는 게 중요하다고 생각함
          + ""공부의 대부분이 검색""이라는 점에 동의함. 검색 능력 자체가 중요한 시대였고, 검색 과정에서 관련되면서도 예상 못 했던 지식도 배울 수 있었음. 다음 세대가 이런 능력이 약해질 것 같아 조금 안타까움
          + Study Mode의 목적 자체가 정답을 주는 게 아니라 스스로 답을 찾는 과정을 가이드함에 있다고 생각함. 많은 사람이 사실 이런 학습법을 잘 모름
          + 이해를 위임하는 건 장기적으로 위험한 태도이고, 스스로 사고하는 위생을 갖춰야 한다는 의견을 말함
          + AI가 키워드로 잡아낼 수 없는 신선한 연관성을 찾아주는 것도 큰 장점임
     * Study Mode의 효과를 무작위 대조 집단 연구로 정확하게 검증해보고 싶음. 학생에게 실질적으로 도움이 되는지, 자기주도 학습보다 뛰어난지, 실수하는 경험과 반복해서 개념을 안내받는 것의 차이가 무엇인지 알고 싶음. Study Mode가 플래시카드와 spaced repetition(반복학습 툴, 예: Mochi, Anki)에 활용될 수 있는 정보를 자동으로 분리해주길 바람. 참고로 Andy Matuschak의 강연도 추천함
          + Study Mode는 실제로 위에서 말한 기능은 제공하지 않음. 학생이 ""Study Mode 없었다면 아무것도 못 배웠을 거야""라고 말하게 만들고, 그 과정에서 학생의 학습 자료를 입력받아 요약해서 자사 데이터로 활용하는 게 투자 목적임
          + 비슷한 효과가 이미 증명된 AI 튜터 연구 논문이 있음. 이 논문에 따르면 AI 튜터 그룹의 학습 향상도가 실제 수업의 active learning 그룹보다 2배 이상 높게 나타남
          + LLM이 숙련된 개발자의 속도를 늦추게 한다는 연구도 있음. 아마 자기주도 학습에도 그럴 수 있다고 생각함. 하지만 LLM은 배움 자체를 더 즐겁게 만들기에, 포기하지 않고 계속 해볼 동기를 제공함. 재미있게 공부하면, 속도가 좀 느려져도 오히려 더 많은 양을 오래 꾸준히 배울 수 있음. 결국 빠른 사람보다, 꾸준히 버티는 사람이 배움의 승자고, LLM이 이를 더 가능하게 한다는 생각임
          + 명문대에서 튜터링의 효과에 대한 연구가 궁금함. 나의 경험상 엘리트 대학 학생들은 튜터로부터 실질적인 도움보다는 (심지어 숙제까지 대신하게 하면서) 정답만 받아가며 뭔가를 배웠다고 착각함. 사실상 ""궁금하고 헤매고 실수하는 상황""을 겪지 않은 학생들이 생기며, 이 과정에서 진짜 공부가 이루어짐. 그런데 LLM 사용도 이와 비슷하게, 힘들거나 막히면 바로 ChatGPT 도움을 받아 시행착오 없이 바로 해법만 찾으려고 함. 그러다보니 시험에서 불안해하고, 연습문제 요구가 점점 심해짐. 수업 후 교과서 읽기조차 스스로 하지 못하는 학생이 늘어난 현상을 피부로 느끼고 있음
          + 이미 동기부여가 높은 학생에겐 큰 차이를 줄 수 있지만, 그런 사람은 많지 않고, 요즘엔 집중력 저하로 그 비율이 갈수록 줄어드는 것 같음
     * 지인 중 한 명이 OpenAI를 이용한 교육 스타트업을 운영 중임. 이처럼 대형사(OpenAI 등)가 똑같은 시장에 진입하면 중소 개발자는 언제든 불리해질 수 있음. 그래서 이런 모델에 의존해 사업을 하는 게 두렵고, 신중하게 접근해야 한다고 느낌
          + 기술 초창기에는 하드웨어 기업이 언제나 소프트웨어를 베끼고 자체적으로 번들해서 소프트웨어 회사와 경쟁할 수밖에 없다는 인식이 있었는지 궁금함. 지금과 많이 비슷하다고 느끼며, 많은 이들이 모델 제공 기업이 얹혀가는 업체에 항상 우위가 있다고 생각하지만, 아직 그럴만한 근거를 찾지 못했다고 생각함
          + LLM 호스팅으로 토큰당 과금하는 구조가 점점 수익성이 떨어져가니, 대형사들은 성장 가능성이 보이는 스타트업/앱을 전부 복제(Sherlock)하려는 게 명확하게 보임
          + LLM 인접 영역의 개발자라면 이 점을 항상 유념해야 하고, 대형사의 시장 확장력과 자본력을 생각하면, 결국 자연스럽게 우리도 사라지게 될 위험이 있다고 느낌
          + 이런 일이 벌어지는 걸 창업가들이 왜 모르겠냐는 의문임. OpenAI가 애초에 단순 LLM 제공에만 머무르지 않고 더 많은 시장에 진출한다는 것이 명확함
          + 리스크가 존재하는 건 당연하니, 중요한 건 우리가 추가하는 실제 가치임. 단발성 사업은 빨리 승부보고, 오래 갈 것이라면 누구나 바로 할 수 있는 일보다는 뭔가 더 독창적인 걸 해야 함
     * 내 LLM 학습의 핵심 전략은 책을 주로 공부하면서, 공식 풀이, 맥락 질문, 이해 검증에만 LLM을 활용하는 것임. 새로운 도메인의 수식 표기법 풀이나, 독일 관념론 철학서의 문맥 질문 등은 LLM이 없었으면 엄청 헤맬 내용을 바로 명확히 해줘서 큰 시간 절약이됨. 특히 공부할 때 중요한 점은 내 이해도를 즉각 검증하는 것임. 예전에는 오해한 채로 진도 나갔다가 되돌아가야 했는데, 지금은 공식이나 개념에 대해 내 직관을 정리해보고 LLM에게 확인 받아서 ""조금 달라요""라는 피드백만 받아도 해당 내용을 다시 볼 수 있음. 결국 책이 가장 정보 밀도가 높아 최고지만, LLM이 보조해주면 속도를 훨씬 올릴 수 있음
     * 현재 이 모든 기능을 위한 올바른 인터페이스가 절실하다고 느낌. Study Mode로 정보를 벽처럼 써주고 예제/질문을 섞어주지만, 답변을 특정 질문과 연계해서 볼 수 있는 방법이 없음. 채팅 UI는 이런 형태에 적합하지 않음. 질문/답변이 각각 연결되고, 내용이 구조화되는 별도의 캔버스/아티팩트 인터페이스가 필요함. 지금은 지나치게 단순한 대화 인터페이스에 많은 걸 우겨넣고 있다는 생각임
          + 실제로 학생이 게임이론 전체를 배우고 싶다고 할 때, 양쪽 모두가 단일 채팅 세션과 직선적 과정을 상정한다는 점이 적나라하게 드러났음. 결국 깊이 있는 학습이 아님
          + periplus.app으로 시도해봤음. 완벽하진 않지만 채팅과는 확실히 다른 UX를 경험할 수 있음
          + 여러 UX 아이디어가 있음. 예를 들어 각 설명마다 이해도를 평가할 수 있게 하고, 이해 못 한 부분만 추가설명 제공, 지식 구조를 트리 형태로 보여주거나, 조작 가능한 그래프, 인터랙티브 퀴즈 등 다양한 방식이 있음. ChatGPT에서는 범위를 벗어날 수 있겠지만, 이런 부분을 다른 앱/스타트업이 공략할 기회라고 생각함
          + 올바른 인터페이스는 존재하지 않는다고 생각함. 배우겠다는 의지와 노력이 있다면, 두루마리-책-전자책-AI 등 어떤 방식이든 결국 배움은 가능함
     * 이게 기본 기능이 아니라 ""별도 제품""이라는 점이 참으로 세태를 보여줌. 단순한 프롬프트 조합으로 어떤 LLM이든 할 수 있는 일임. Study Mode는 교사, 과학자, 교육심리 전문가와 협업해 개발했다지만, 진짜 전문가가 참여했는지 회의적임
     * 현재 Janet을 배우는 중인데, ChatGPT를 튜터로 쓰니 불편함. 내 질문(""local과 var가 모두 지역 변수라면 뭐가 다름?"")에도 ""좋은 질문이에요, 본질을 잘 짚으셨네요"" 하고는 그럴듯하게 환각 답변만 늘어놓음. 아는 것에 대해선 좋은 튜터인데 자기 한계를 알아야 한다고 생각함
          + LLM이 잘 알려진 예시(React todo list, bootstrap form, vue tic-tac-toe 등)에 대해서만 진짜 좋은 답변을 주는 경향이 있음
          + 이런 경우라면 직접 가이드/문서를 몽땅 컨텍스트에 입력해주는 방식이 더 낫다고 조언함
          + OpenAI와 사운드 머니를 받은 기업에 빗대어, 돈이 너무 필요해 보이는 튜터 같다는 비유를 함
"
"https://news.hada.io/topic?id=22299","Cerebras, Qwen3-Coder 기반 초고속 AI 코딩 서비스 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Cerebras, Qwen3-Coder 기반 초고속 AI 코딩 서비스 출시

     * Cerebras가 AI 코딩을 빠르고 쉽게 만드는 Code Pro(월 $50)와 Code Max(월 $200) 플랜을 출시함
     * 두 플랜 모두 Qwen3-Coder 모델을 기반으로 하며, 초당 2,000 토큰, 131k 토큰 컨텍스트, 주간 사용 제한 없음 등 업계 최고 수준의 성능 제공
     * IDE 종속 없이 OpenAI API 호환 에디터라면 어디서나 바로 사용 가능, Cursor, Continue.dev, Cline 등 지원
     * Code Pro는 하루 1,000 메시지로 인디 개발자와 간단한 워크플로에 적합하며, Code Max는 하루 5,000 메시지로 대규모 개발 환경, 리팩토링, 멀티에이전트에 최적
     * 별도 대기자 명단 없이 즉시 가입 및 키 발급으로 바로 코드 생성 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * Cerebras는 초대형 AI 반도체(Wafer Scale Engine) 및 슈퍼컴퓨터를 개발하며, 대규모 인공지능 모델 학습과 추론을 위한 하드웨어·소프트웨어 플랫폼을 제공하는 AI 인프라 전문 기업

Cerebras Makes Code Generation Instant

     * 최신 AI 모델도 코드 생성 시 지연이 발생하지만, Cerebras Code는 초당 2,000 토큰의 속도로 거의 즉각적인 코드 생성 경험 제공
     * 코딩 워크플로가 에이전트 기반으로 발전하면서, 각 단계마다 LLM 호출 대기 시간이 누적되는 문제를 해결함
     * 월 $50부터 제공돼, 누구나 빠른 AI 코딩을 이용 가능함

Powered by a Frontier Model

     * Qwen3-Coder는 Alibaba의 대표 코딩 에이전트 모델로, 480B 파라미터 규모임
     * Claude Sonnet 4, GPT-4.1과 동급의 코딩 및 에이전트 작업 성능을 보여줌
     * Agentic Coding, Agentic Browser-Use, BFCL 등 다양한 코딩 벤치마크에서 선도적 성능을 달성함

Bring your own AI IDE

     * OpenAI 호환 API 엔드포인트를 지원하는 모든 코드 에디터, 툴에서 사용 가능함
     * Cursor, Continue.dev, Cline, RooCode 등 다양한 환경에 바로 연동 가능
     * 추가 설정 없이, 기존 워크플로 안에서 고품질 코드 생성이 즉시 가능함

Available now

     * Cerebras Code Pro ($50/월)
          + Qwen3-Coder 기반, 빠르고 대용량 컨텍스트 코드 완성 지원
          + 하루 최대 1,000 메시지 (3~4시간 연속 코딩 가능)
          + 인디 개발자, 간단한 에이전트 워크플로, 주말 프로젝트에 적합
     * Cerebras Code Max ($200/월)
          + Qwen3-Coder 기반, 대규모 개발 워크플로에 맞춤
          + 하루 최대 5,000 메시지
          + 전일제 개발, IDE 통합, 코드 리팩토링, 멀티에이전트 시스템에 최적
     * 두 플랜 모두 즉시 가입, 별도 대기 없이 API 키 발급 후 바로 코드 생성 시작 가능

   중국산

        Hacker News 의견

     * Cline과 함께 직접 API 키를 사용하여 시도해 봤음 (Cerebras는 여기에서 Qwen3 Coder 프로바이더로도 사용 가능)인데, 캐싱 없이 사용하니 비용이 아주 빠르게 올라감에 놀람, 특히 새로운 툴 콜마다 이전 메시지 이력 전체가 입력 토큰으로 전송됨 (API 기준 입력/출력 토큰 모두 100만 개당 $2 비용) 품질은 Claude Code보다는 조금 떨어지지만 속도 면에서는 확실히 훨씬 빠름 Cerebras가 캐싱 지원과 캐싱용 토큰 가격 할인도 제공하면 더 자주 쓸 의향이 있지만 현재로선 1회 에이전트 실행당 비용이 너무 큼
          + 전체 파일을 컨텍스트 윈도우에 넣고 AI가 필요한 부분을 찾게 하는 방식은 매우 비효율적임, 이 방식은 AI로 diff를 생성하면 새로운 문제가 많이 생기기 때문에 쓰긴 하지만, 더 효율적인 방법이 있음: 심볼 레벨에서 파일을 슬라이스하는 것임 즉, 만약 AI가 foo() 선언과 bar() 정의만 필요하다면 전체 파일은 이런 식으로 축약 가능함
class MyClass {
  void foo();
  void bar() {
    //code
  }
}

            AI가 제안한 수정을 다시 병합하기도 쉽고, 대부분 경우 1~2초 내에 리팩토링이 가능함 (이름 바꾸는 경우만 예외) 이를 에디터에 적용해 step back/forward 기능도 추가하여 잘 동작 중임 개인적으로 Cerebras 플랫폼을 매우 좋아함 (무료 티어와 OpenRouter 통한 종량제 제공) 아주 간단한 프롬프트 한 줄만으로 귀찮은 리팩터링을 1~2초 만에 처리하고, 토큰 비용도 리팩터링 당 약 0.5센트 수준임 알고리즘 적용 시 전체 파일을 다 넣으면 컨텍스트 윈도우를 초과하지만, 필요한 타입 몇 개만 뽑아 쓰면 토큰을 훨씬 아껴 쓸 수 있음 추가로 내가 만드는 기법에 대해 더 궁금하다면 여기에서 설명 확인 가능함
          + Cerebras.ai는 월 $50, $200의 정액 요금제를 제공함 API 토큰 단가 때문에 구독 가격을 거부하는 건 타당하지 않음
          + 월 $50로 구독하는데 왜 추가 결제가 필요한지 의문임
          + 이 서비스는 메시지 단위로 rate limit가 걸리는 듯하므로 캐시가 없어도 큰 문제 아닐 수 있음
     * Cerebras Code의 사용량 한도가 매우 제한적임, $50 플랜으로 하루 750만 토큰 제공되는데 실제로 오래 못 감 이 정보는 결제 전 요금제 설명에 명확히 안내되어 있지 않음
     * “초당 2,000 토큰 처리, 131k 토큰 컨텍스트 윈도우, 독점 IDE 필요 없음, 주간 한도 없음!”이라는 문구를 보고 기대했지만, 이어진 “하루 최대 1,000 메시지 전송 – 연속 3~4시간 코딩 분량”이라는 설명을 보고 좀 실망함 실제 쓰는 서비스라면 비용 지불 의사는 있지만 서두와는 상반된 제한이 아쉬움
          + 이런 하루 1,000 메시지 제한은 아마도 viberank 같은 문제가 발생하지 않게 하기 위한 조치로 보임
          + 참고로 github copilot 비즈니스 라이선스는 “프리미엄” 요청이 한 달에 300회임
          + 하루 1,000 메시지는 대부분 개발자에게 충분함, 나도 claude code sonnet 4만 단독으로 쓰지만 하루 1,000회 이상 전송하진 않음 물론 내가 모르는 사이에 추가 메시지가 내부적으로 더 전송되는 걸 수 있음
          + Claude Code는 5시간 윈도우와 주간 한도까지 있는 반면, 이건 명확하게 다름
          + 진짜 주간 한도가 없는 건 사실임, 오직 일일 한도만 존재하므로, 하루에만 락 걸리고 다음날 바로 사용 가능함
     * 확실히 빠르긴 한데 rate limit가 너무 빨리 걸리고 품질은 Claude Code보다 떨어지며 결국엔 더 비싸짐 Cerebras의 타깃 유저가 누구인지 궁금함
          + 나 역시 궁금하지만, 조금 떨어지는 대안이라도 있는 게 좋으며, 이미 대형 플레이어가 있는 동적 시장에선 선택지가 많을수록 유리하다고 생각함 독점 방지 차원에서도 긍정적임
     * 코딩 에이전트에서 활용해 보고 싶다면 qwen3-coder 모델이 에이전트에 잘 맞는다고 생각함, Sketch에서 Cerebras Code를 테스트 중임 이제 최신 버전(0.0.33)에서 직접 실행 가능함
brew install boldsoftware/tap/sketch
CEREBRAS_API_KEY=...
sketch --model=qwen3-coder-cerebras -skaband-addr=

       다만 현재는 서버 과부하로, 기존 자체 호스팅 버전이 더 나은 결과를 주는 것 같음
sketch --model=qwen

     * 일부 프로 요금제($50/월) 가입자는 실제 안내된 한도보다 더 제한이 많다고 보고함 광고엔 1,000 요청 한도라지만, 실제론 하루 750만 토큰 한도임 (출처) 평균 1회 요청이 7.5k라 계산했지만, 마케팅 동영상에선 요청당 최대 2.4만 토큰까지 풍선처럼 늘어나기도 함 그래도 API 가격보단 저렴함
          + FAQ까지 나중에 미끼 상품처럼 슬쩍 바꾼 점이 아쉽다고 생각함 시장 점유율 확장에 VC 자금 쓰는 시기임을 감안하면 뭔가 더 적극적인 태도 필요함
     * 초당 2,000 토큰 속도는 충격적임, vibe coding(편하게 코드 짜기)을 극히 지양하는 입장이지만, 이런 성능이라면 github copilot과 비슷한 속도에 훨씬 더 나은 품질을 기대 가능함 특히 에디터 안에서 쓸 때 판도가 바뀌는 수준임
          + 풀 스피드로 계속 사용한다면 약 62분 정도면 하루 사용 한도에 도달할 수 있음
     * Windsurf도 Cerebras/Qwen3-Coder를 제공함, 월 $15에 1,000회 유저 메시지 한도임 관련 링크
     * 인플루언서 역할을 하는 공급자 대비, 더 많은 구독형 서비스가 나오길 기다리고 있었음 앞으로 경쟁 상품이 더 많이 출시될 것이며, 이용자 입장에서는 가격이 점점 좋아질 것으로 기대 중임 Cerebras 팀의 출시를 축하함
     * claude-code-router에서 동작하는지 궁금함 이번 주 openrouter에서 qwen3 Cerebras와 함께 쓰려 했는데 API 에러가 발생했음
          + Qwen3팀이 Anthropic 호환 API를 제공했으면 정말 좋겠다고 생각함 Kimi와 GLM/Zai팀처럼 쉽게 셋업할 수 있었음 덕분에 Claude Code에서는 이런 간단한 세팅으로 다양한 모델 사용이 가능함
          + API Error: 422 {""error"":{""message"":""Error from provider: {""message"":""body.messages.0.system.content: Input should be a valid string"",""type"":""invalid_request_error"",""param"":""validation_error"",""code"":""wrong_api_format""}
"
"https://news.hada.io/topic?id=22243","FDA, HIV 감염 예방 효과를 제공하는 Yeztugo 승인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   FDA, HIV 감염 예방 효과를 제공하는 Yeztugo 승인

     * FDA가 승인한 Yeztugo 약물은 연 2회 주사만으로 HIV 감염을 100% 예방함
     * 이 신약은 캡시드 억제제 계열로, 바이러스의 유전자 복제를 차단하는 혁신적인 항바이러스제임
     * Gilead Sciences는 미국 외 국가에도 저렴하게 공급하고 6개 제네릭 제조사와 무상 라이선스 계약을 체결함
     * 이 약물은 기존 PrEP 대비 복용 편의성을 크게 높여, HIV 예방 접근성을 확대 가능성이 있음
     * 저소득 국가에서는 이익 없는 공급 방식을 적용하며, 글로벌 HIV 종식을 위한 중요한 전환점임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

FDA, HIV 감염 예방 효과를 제공하는 Yeztugo 승인

  44년 만에 HIV 종식 희망

     * 오랜 기간 지속된 HIV 유행을 종식할 수 있는 새로운 약물이 등장함
     * 최초로 100% 예방 효과가 임상적으로 확인된 HIV 예방제가 연 2회 주사로 제공됨
     * 이 성과는 전 세계적으로 수백만 생명을 구할 기념비적 성과로 평가됨
     * 제조사는 미국뿐 아니라 전 세계 저렴한 접근성을 위해, 6개 제네릭 제조사와 무상 라이선스 계약 체결함

  신약의 특징과 작용 원리

     * 미국 FDA는 lenacapavir(제품명 Yeztugo)를 공식 승인함
     * 이 약물은 capsid inhibitor(캡시드 억제제) 계열로, 바이러스 외피 단백질을 타깃하여 HIV-1 감염을 100% 차단 효과를 보임
     * 연 2회 주사만으로 바이러스 복제를 막아, 현재 매년 약 130만 명이 새롭게 감염되는 HIV 예방에 기여함

  혁신성과 의의

     * Science지에서 2024년 올해의 혁신적 발명으로 Yeztugo를 선정함
     * 기존의 PrEP(사전 노출 예방 약제) 대비, 99% 이상의 예방 효과를 보여줌
     * HIV-1 바이러스는 캡시드(단백질 껍질) 로 유전자 물질을 보호하고 세포 내 이동에 필수적임
     * Lenacapavir는 캡시드의 기능을 억제하여 바이러스의 복제를 원천적으로 차단함

  임상적 성과와 업계 평가

     * Gilead Sciences의 대표는 이번 승인을 HIV 퇴치 역사상 중대한 순간으로 평가함
     * Yeztugo는 연 2회 투약만 필요하고, 임상 시험에서 우수한 성과를 통해 HIV 예방에 전환점이 될 약물임
     * Gilead는 여러 국가 당국에 승인 신청을 제출 중이며, 저소득 및 무보험 환자도 Advancing Access 프로그램을 통해 무상으로 접근 가능함

  글로벌 보급 및 사회적 임팩트

     * Gilead는 Global Fund와 협력하여, 3년간 최대 200만 명에 이익 없는 공급 계획을 발표함
     * 무상 라이선스 제네릭을 통해 120개 저소득 국가에 공급 예정임
     * 이번 정책은 글로벌 HIV 유행종식과 생명 구호 혁신의 접근성 확대라는 원칙을 실현함
     * Global Fund는 빠르게 필요한 인프라를 구축해 중저소득국가의 약물 보급을 목표로 함
     * 오래 기다려온 획기적 예방도구가 드디어 현실화되는 중요한 전환점임

        Hacker News 의견

     * 건강한 비판은 접어두고, 로열티 없이 접근 가능하고 보험이 없는 사람들도 무료로 접근할 수 있다는 점이 굉장히 흥미롭게 느껴짐. R&D 비용은 어떻게 부담하는지 궁금함. 제약회사가 말하는 약값이 비싼 가장 큰 이유가 바로 여기에 있음. 자선적 의도로 나온 결과인지 궁금함
          + Gilead Sciences, Inc에서 개발한 약임. 비용을 감당할 수 없는 사람들에게 저렴하게 제공하는 방식은, 보험 있는 미국인들에게 높은 가격을 청구함으로써 실현함. 예전 C형간염 치료제 사례만 봐도, 미국에서는 한 알에 $1,000씩 팔았지만 인도 같은 개발도상국에선 $4 이하로 제공했음 Gilead의 약가 사례. 저소득층에 대한 저가 HIV 치료제 제공은 홍보(PR) 목적과 실용성 모두를 반영한 전략임. 가난한 국가들은 미국 제약회사를 위해 특허를 엄격히 집행하지 않으며, Gilead도 어차피 그렇게 돌아갈 현실을 미리 받아들인 것뿐임 관련 법안
          + 보험사에 높은 비용을 청구하고, 정부나 Gates Foundation과 같은 기관의 투자도 활용함
          + 이 약 개발에 대한 30년간의 자세한 과정을 다룬 자료가 있음 개발 과정 기사. Utah 대학의 연구자금, 대형 제약회사(Gilead), 글로벌 HIV 옹호 단체가 함께 일한 결과임. 안타깝게도 이런 대학 연구와 비영리 단체들이 트럼프 행정부의 예산 삭감 대상 1순위였음. 다음 혁신적 약물은 미국이 아닌 타국에서 개발될 수도 있음
          + 제약회사들이 평균적으로 R&D보다 마케팅에 훨씬 많은 돈을 쓰고 있어서, R&D 비용 걱정에는 마케팅부터 줄이자는 생각임. 대부분의 국가에서는 의약품 광고와 영업사원 활동이 금지되었지만, 다들 잘 돌아감. 굳이 마케팅을 없애지 않더라도 절반만 줄여도 본질적으로 효과적일 것임. 실제로 제약회사는 R&D보다 자사주 매입이나 배당에 더 많은 돈을 씀. 결국 요즘 제약회사는 화학자를 고용한 금융 상품에 가까움
     * ""거짓말, **거짓말, 그리고 통계""라는 말처럼 기사에서 말하는 100% 효과는 과장임. 실제 연구에서는 아프리카에서 16-25세 고위험군 2천 명에게 신약을 투여했고, 이중에서 HIV 감염자는 0명이었음 FDA 문서 참고. 대조군 1천 명이 기존 트루바다를 복용했더니 16명이 감염됨. 연구 결과만 보면 엄청난 차이지만, 회사 어디에도 100% 효과라고 주장한 내용은 없음. 100%라는 표현은 기사가 자극적으로 광고하는 클릭베이트임
          + 모든 연구에서 100% 예방이 확인된 것은 아님. 한 실험에서는 HIV 감염자가 안 나와서 100%로 보일 수 있지만, 또 다른 연구에서는 2,000명 연간 기준에서 2명이 감염됐고, 기존 프렙 약과 비교시 89% 효과로 계산됨. 기존 약 대비 90% 향상도여서 모두가 쓴다면 HIV 예방에서 대전환이 될 수 있음. 복용도 훨씬 단순함 연구 소개
          + 기사 링크보다 이 기사가 더 신뢰할 만함. 여기서는 ""99.9% 예방"" 및 ""감염 위험 감소""로 언급해 100%라는 과장이 없음
     * 기사에 대해 먼저 말하자면 제목이 정말 자극적인 클릭베이트임. 같은 기사 안에서 ""HIV 100% 예방 첫 약""이라는 문구에서 몇 문단 뒤엔 ""거의 100% 보호""로, 또 나중엔 ""99% 예방""으로 불분명하게 혼용됨. 이렇게 중요한 이슈를 다루는 기사치고 너무 부정확함. 보기에 이 약은 기존 트루바다나 데스코비와 제대로 복용만 하면 효과 면에서 큰 차이가 없음. 다른 점이 있으면 증명되길 희망하지만, 숫자상으론 기존 약들과 유사해 보임. 물론 약을 매일 복용하는 것이 얼마나 어려운지도 잘 알고 있어서, 개인적으로는 주사형도 고려 중임. 하지만 이 기사 자체의 보도는 믿기 힘듦
          + 이 약의 차별점은 연 2회 주사인 점임. 매일 또는 매월 약을 꾸준히 복용하거나 처방을 계속 받기 힘든 고위험층(노숙자, 개발도상국 농촌 거주자 등)에게 연 2회 주사는 현실적으로 훨씬 큰 의미를 가짐
          + 차라리 이 기사가 더 좋은 참고자료임
     * 이 약의 문제점은 바이러스 복제가 거의 끝나가는 단계에서 작동을 억제함. 즉, 약이 효과를 발휘하려면 이미 바이러스가 세포에 들어가서 RNA를 숙주 DNA에 남긴 다음임. 따라서 환자가 이 주사를 맞는 도중 HIV에 노출된다면, 세포 감염 자체는 막지 못함. 다만 감염된 세포에서 바이러스 복제가 억제되어 추가 전파가 안 일어나고, 약을 중단하면 그동안 감염된 세포가 다시 바이러스를 생산, 에이즈로 발전할 수 있음. 만성관리용으론 훌륭하지만, ""감염 자체를 예방한다""고 생각하면 안 됨. 감염은 허용하되 이후 증식을 막는 형태임
          + 이 설명에 동의하기 어려움. Yale 기사에서 보면 이 약은 두 가지 기전을 가짐. 하나는 바이러스가 세포핵으로 못 들어가게 막아 재생산을 차단하고, 두 번째는 이미 유전체가 삽입된 세포에서도 자손바이러스 생산을 억제함. 학술논문에서도 융합 단계를 여러 방식으로 막는다고 나와 있어서 너무 단순화하고 있음
          + 설명은 가능해 보이지만 실제 이런 현상이 일어난다는 근거가 있는지 궁금함. 나도 이 약의 프렙 연구가 진짜로 감염을 측정하는지, 아니면 복제가 안 돼서 검출만 못 하는 건지 고민해본 적 있음. 다만, Wikipedia의 HIV capsid inhibition 페이지에서는 오히려 캡시드 분해 억제가 주된 메커니즘으로 서술되어 있음
          + 이 약은 프렙(감염 예방용)으로 특별히 연구되어왔고, 실제로 감염 자체를 예방하는 효과가 있음. 위에서 설명한 메커니즘은 오해임
          + 만약 바이러스가 복제되지 않으면, 감염된 사람이 파트너에게 바이러스를 전파하지 않아 이 역시 감염예방 효과로 볼 수 있음
          + 초기 감염과 장기 감염(지속성)은 다름. HIV 저수지는 감염 직후가 아닌, 초기 감염 주기 하나가 돌고 나서 형성되기 시작함 관련 논문
     * 궁금한 사람들을 위해 말하자면, 이 약은 연 2회 주사임. 기존 프렙은 매일 먹는 알약임
          + 이미 1년에 6번 맞는 주사형 옵션도 있음
     * 효과도 중요하지만, 1년에 두 번만 맞아도 되는 약이란 점 자체가 대단함. 약 복용을 제대로 지키는 건 매우 중요한데, 반년에 한 번이면 복용 지침 준수율이 훨씬 올라감. 또, 취약계층(예: HIV 약을 들키면 위험해질 수 있는 사람들)의 안전도 크게 향상됨
     * 연 2회 주사로 매일 프렙과 동일한 효과를 낼 수 있다면 정말 획기적임
     * ""100%""라는 문구가 걱정됨. 실제로 100% 효과를 가지는 약은 드물기 때문에, 예외적인 혁신이거나, 과장 홍보일 수 있음. 현실적으로 후자가 많았던 경험임
     * 약 이름이 ""Yes to go""처럼 들림. 작명은 분명 우연이 아닐 것임
     * HIV가 정말 대단한 항바이러스 연구를 촉진했음. 이런 연구가 미래의 팬데믹에 도움이 되길 바람. 끔찍한 병에서 얻은 몇 안 되는 긍정적 효과임
          + 제대로 근거는 없지만, HIV 사태 이후로 바이러스 연구가 급격히 발전했고, 그래서 바이러스 논문의 대부분이 HIV 관련이라는 얘길 들은 적 있음
"
"https://news.hada.io/topic?id=22337","Show GN: FE플래닛","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Show GN: FE플래닛

   사전과제 기간과 피드백 여부에 대한 후기를 남기는 서비스 입니다.

   1년 전에 만드려고 계획하고, 구글폼으로 리뷰 설문 먼저 받다가 개발은 못 했었는데요. 이제서야 Claude Code에게 개발시키고 제가 옆에서 도움을 줘서 완성하고 공개합니다. (즉 바이브 코딩으로해서 제가 직접 코딩은 안했습니다) - 개발 후기글

    주요기능

     * 사전과제 리뷰: 사전과제 기간과 피드백 여부 등으로 필터링해서 볼 수 있습니다. (리뷰는 익명으로 남겨집니다)
     * 통계: 남겨진 리뷰의 전체 통계 데이터를 볼 수 있습니다.

   1년 전, 며칠 혹은 일주일이 넘는 사전과제를 내면서 피드백 한줄 없이 탈락 메일만 보내는 경우가 많다는 이야기를 들었습니다. 그 때 생각난 아이디어인데, 이제서야 만들어서 올려봅니다. (1년 사이에 라이브 코딩으로 넘어간거 같긴 한데)

   검증된 리뷰의 검증은 어떻게 이루어지나요?

   리뷰 올려주신 분이 사진과제 관련된 메일 내용 중에 중요 내용은 가린채로 올려주시면, 제가 확인해보고 검증됨 처리를 하고 있습니다. (더 좋은 방법은 아직 찾지 못해서ㅠ)
"
"https://news.hada.io/topic?id=22226","가자지구: 전 GHF 구호 현장 직원, "전쟁 범죄 목격"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    가자지구: 전 GHF 구호 현장 직원, ""전쟁 범죄 목격""

     * 전직 미 특수부대 요원이 미국과 이스라엘이 지원하는 GHF 구호센터에서의 근무를 그만둔 이유를 BBC에 밝힘
     * 그는 이스라엘 방위군이 팔레스타인 민간인 군중에게 총격을 가하는 장면을 목격했다고 언급함
     * 평생 동안 이와 같은 무차별적이고 불필요한 힘의 사용을 본 적이 없었다고 강조함
     * 특히 비무장이고 굶주린 민간인을 향한 이러한 행동이 매우 충격적이었다는 점을 언급함
     * 현장에서의 경험이 자발적 퇴직으로 이어졌음을 설명함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

은퇴한 미 특수부대 요원의 고백

     * Anthony Aguilar라는 은퇴한 미 특수부대 요원이 BBC와의 인터뷰에서, 미국과 이스라엘이 지원하는 Gaza Humanitarian Foundation(GHF) 구호물자 배포센터에서의 근무를 그만둔 이유를 밝힘

현장에서의 목격

     * 그는 이스라엘 방위군이 팔레스타인 민간인 군중을 향해 총격을 가하는 장면을 직접 목격했다고 언급함
     * 이러한 폭력 수준이 자신의 경력 중 경험해 본 적 없는 일이라고 설명함

무차별적인 무력 사용

     * 특히 문자 그대로 무차별적이고 불필요한 무력 사용이 벌어진 점에서 충격을 받았음을 강조함
     * 그가 목격한 것은 무장하지 않았고 굶주린 민간인 인구에 대한 것이었음

자발적 퇴직의 배경

     * 이 현장에서 본 잔혹 행위와 심각한 인권 침해가 자신의 자발적 퇴직으로 이어졌음을 밝힘

        Hacker News 의견

     * 최근 정치인들 중에는 이스라엘 정부의 초기 공격에는 지지 입장이었으나, 민간인 대량 희생과 인도주의적 위기 상황이 이어지자 지금은 강하게 반대 입장으로 바꾼 사람들이 늘어나고 있음, 대표적으로 Macron, Angus King 그리고 내 주변에서도 이런 변화가 보임, 이런 변화가 있을 때 ""좋음!""이라고 말해 주어야 한다고 생각함, 주장이 바뀐 사람을 호되게 비난하는 것은 운동의 외연 확장에 도움이 되지 않음, 상황에 따라 생각을 바꾸는 사람을 수용하는 태도 필요하다고 느껴짐
          + 가족이나 이웃이 생각을 바꾸는 건 이해하지만, 정치인의 입장 변화에는 더욱 신중해질 필요가 있다고 봄, 1년 전에도 충분히 예측 가능한 상황이었기에 지금 와서 입장을 바꾸는 배경에 어떤 계산이나 숨은 의도가 숨어 있을 수 있다고 의심해야 함
          + 이 비극의 설계자 중 하나인 Anthony Blinken 같은 이들에게는 자신의 책임을 희석시킬 기회를 절대 주어선 안 된다고 생각함
          + 방법론에 대해서는 논쟁할 수 있지만, 10월 8일 당시 이스라엘이 가자 침공 외에 다른 대안이 있었는지도 충분히 논의될 수 있다고 봄
          + 나 역시 이스라엘의 초기 행동은 어느 정도 이해할 수 있었으나, 지금은 상황이 도를 넘어섰다고 생각함
          + 많은 사람들이 초반부터 이런 사태가 일어날 것을 예측했었음, 그때는 ""이스라엘 침공을 지지하지 않으면 친하마스다""라는 식의 반응이 다수였음, 입장이 바뀐 이들은 그때 테러 지지자로 몰았던 사람들에게 사과할 필요가 있다고 봄
     * 이스라엘이 국제 기자들의 접근을 허용하지 않는 이유는 아주 명확하다고 생각함
          + 1982년 레바논 침공에서 서방 언론을 다루는 법을 잘 배웠기 때문이라고 봄, 이와 관련하여 ""Our American Israel""이라는 책에서 이 주제를 심도 있게 다루고 있음
          + BBC의 Jeremy Bowen이 최근 요르단에서 구호물자를 투하하는 비행기에 동승했을 때, 이스라엘은 비행기 창밖을 촬영하는 것을 원치 않았다고 함, 그 이유는 가자 지역의 도시들이 완전히 파괴된 모습을 촬영해 집단학살의 증거가 남는 것이 두렵기 때문임, 관련 영상
     * Putin(2005), Xi(2012), Netanyahu(2015) 모두 오랫동안 일관된 메시지를 내오고 있음, 이들이 소리 높여 혹은 조용히 같은 이야기를 반복하고 있음, 우리가 더 주목해야 할 필요가 있다고 봄, Putin 발언, Xi 발언, Netanyahu 발언
     * 이제 이스라엘이 팔레스타인을 조직적으로 말살하고 있다는 증거가 더 필요하냐고 묻고 싶음, 가자를 살아갈 수 없는 땅으로 만들고, 정부 최고위급에서 대량학살을 예고한 발언을 하며, 팔레스타인을 비인간화하고 비판하는 이들의 목소리를 억누르는 모습임, 이전에 이스라엘을 비판하면 기사조차 삭제되는 경우가 많았는데, 이번 글이 삭제되지 않은 것이 다행으로 느껴짐, 인간의 권리와 이상주의를 내세운 서구의 위선이 이번 사태로 완전히 드러났다고 생각함, 콩고 자유국이나 아프리카 분할 당시의 만행과 차이가 없다고 봄, 팔레스타인 문제는 결국 서구에 엄청난 대가가 될 것임, 과거 소련의 붕괴도 사람들이 체제에 대한 환멸을 느끼는 순간이 핵심이었음, 미국의 전쟁 참전군인들도 환멸과 허탈을 품고 있고, 이제는 젊은 세대가 ""우리가
       악당인가?""라고 진지하게 묻고 있음, 이처럼 환멸이 체제 붕괴의 시작이라 생각함
          + 이스라엘이 단순히 하마스만 타격하려는 목적이 아니었음은 몇 달간 가자에 음식과 구호품 유입을 완전히 차단한 순간부터 더욱 명확해졌음, 이런 행위는 의도적으로 민간인 사망을 유발하려는 것이라고 봄, 하마스의 자금원을 끊기 위해서라는 설명도 논리적이지 않음, 오히려 식량 부족이 하마스의 시장 독점과 가격 상승을 유도할 뿐임, 오히려 식량을 풍부하게 하여 가격을 낮추는 것이 더 타격이 되는 구조임
          + 콩고 자유국과 아프리카 분할은 유럽인들이 당시에는 아프리카인을 인간으로 인식하지 않았던 시기라 생각함, 오늘날 서구 사람들이 팔레스타인을 의식적으로 '비인간'으로 보는 건 아니기에 오히려 지금이 도덕적으로 더 큰 문제임
          + Gen X 세대는 ""우리가 악당인가?""를 고민했고, Gen Z는 ""원래부터 그랬지""라고 확신함
          + HN에서는 팔레스타인 지지, 이스라엘 비판이 주류인듯 보임, 이 반대 목소리는 종종 비추천이나 플래그로 묻혀버리는 모습임, 정치적 중립성을 기대했으나 현재 댓글 분위기는 오히려 한쪽에만 일방적임을 느낌
          + ""인권 중심주의""를 표방했던 서구의 명분이 완전히 무의미해졌음, 서구 정치인들은 인권이나 국제법에 대한 믿음이 진짜 있나 의문임, 앞으로 서구가 내세우는 도덕적 근거를 누가 신뢰할지 걱정임
     * 이번 사태를 처음부터 이렇게 될 거라 예상했다며 냉소적으로 말하는 이들이 많은데, 이스라엘은 수차례 자극과 공격을 받아왔고, 초기의 조심스러운 희망은 늘상 반복되는 갈등의 재현이었다고 느꼈음, 하마스가 무력화된 이후 이스라엘이 금방 철수할 것이라 생각한 이들도 있었음, 자기방어 관점에서 더 얻을 게 별로 없고, 이미 모든 국제적 정치적 신뢰를 소모하고 있음
     * 이스라엘 현지 인권단체들도 목소리를 내기 시작함, NPR 기사, BBC 기사에서도 같은 내용 다룸
     * 요즘 이스라엘 비판이 유행인데, 그렇다면 해법은 무엇일지 묻고 싶음, 아직도 가자에는 인질이 있고, 하마스는 민간인 희생이 늘어날수록 자신들에게 도움이 된다고 공개적으로 언급해왔음 (CNN 인용), 그들은 여전히 전투를 지속 중이며 유엔도 구호물자 배분 중단함(WFP 인용), 이스라엘이 단독 철수해 자치권을 줬던 게 10월 7일 사태로 이어졌음을 지적하며 실질적인 솔루션을 요구함
          + 가자 주민이 인질을 잡고 있는 게 아니라 하마스가 잡고 있음, 언어 선택이 교묘함, 그리고 이스라엘에선 기소도 없이 수감 중인 수천 명의 팔레스타인 행정구금자도 일종의 인질 아님?
          + 전체 국민을 굶겨 죽이게 내버려 두는 것은 해법이 아니라고 생각함, 나는 이스라엘 비판이 아니라 이스라엘의 극단적 현 정부를 비판함, 진짜 이스라엘 지지는 오히려 이 극우 정권에 반대하는 것임
          + 최소한으로라도 이스라엘에 대한 군사·금전 지원을 멈춰야 한다고 생각함, 무기 판매 중단, 최소한 이스라엘 불매 규제라도 철폐해야 함, 완전한 해법은 아니지만 올바른 방향이라고 봄
          + 편집에 대한 답변: 지금 이 시점에 최소한 사람들에게 식량과 구호물자를 공급해야 하고, 전쟁범죄는 멈춰야 함, 이스라엘은 오히려 식량 원조의 안전한 전달을 보장해야 함
          + 전쟁 중에 포로가 발생하는 것은 흔하다고 생각함
     * David Ben-Gurion 전(前) 이스라엘 총리가 ""우리가 그들의 땅을 빼앗았고, 신이 우리에게 약속했다 해도 그들에게는 아무 의미가 없다, 우리가 그들의 나라를 훔쳤는데 왜 이스라엘을 인정하겠는가""라는 취지의 발언을 했었음, 이 비극을 끝내려면 팔레스타인에게 주권국가를 주고 그들이 용서해주길 바라는 수밖에 없다고 생각함
          + 팔레스타인에게 여러 번 국가 설립 기회가 주어졌고, 1937년부터 5번이나 제안받았지만 매번 거절함, 그들은 자신의 국가 설립보다 이스라엘 정복을 더 원하고 있다고 들었음, 마땅한 해법이 보이지 않으며, 차라리 권위주의적이지만 비민주적인 팔레스타인 국가라도 만드는 것이 가능할지도 고민임
     * 1차 자료, 즉 ""현장증언""을 들을 때 항상 전쟁의 안개와 통계의 왜곡에 주의해야 함, 최근 BBC에도 가자 병원의 외과의가 거의 모든 어린이 환자가 치명적인 한 발의 총상으로 온다고 이야기하며 IDF가 일부러 아이들을 저격한 게 아닌가 의심한다고 했음, 하지만 실제로는 다수의 총상을 입은 아이들은 병원에 오기 전 사망하고, 경미한 부상자는 병상 부족으로 오지 못함, 살아남은 매우 중상자만 병원에 오게 되니 ""한 발의 치명상""만 눈에 들어오는 통계왜곡이 생기는 구조임, 이러한 관점이 대중적으로 확산되는 경향이 있지만, 실제로는 더 폭넓은 통계검증이 필요함, 나는 어느 쪽 입장도 지지하지 않으며 전쟁 관련 기사 모두를 비판적 시각으로 읽을 것을 권고함
          + 혹시 위 말을 뒷받침할 만한 출처가 있는지 궁금함
     * 영상 속 첫 장면들은 Schindler's List의 발코니 신을 떠올리게 했음
"
"https://news.hada.io/topic?id=22309","Terry Tao의 NSF 연구비가 일시 중단됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Terry Tao의 NSF 연구비가 일시 중단됨

     * NSF가 수학자인 Terry Tao의 연구비를 중단함
     * 이 조치는 국립과학재단(NSF) 공식 웹사이트에서 확인 가능함
     * 중단 사유나 향후 계획에 대한 설명은 공개되지 않음
     * Terry Tao는 국제적으로 유명한 수학자로, 그의 연구는 큰 주목을 받아왔음
     * 이번 중단 소식은 수학 및 과학 연구 커뮤니티에서 관심과 우려를 불러일으키고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NSF, Terry Tao의 연구비 중단

     * 최근 NSF(미국 국립과학재단) 가 저명한 수학자 Terry Tao에게 지급하던 연구비를 일시적으로 중단함
     * 상세한 중단 이유나 향후 계획에 대한 공식 발표는 없는 상황임
     * 해당 연구비 중단에 관한 정보는 NSF 공식 홈페이지의 지원사업 검색 페이지에서 직접 확인 가능함
     * Terry Tao는 전 세계적으로 높은 명성과 영향력을 가진 수학 연구자임
     * 이번 결정은 수학 및 과학계에서 중대한 이슈로 인식되는 중임

        Hacker News 의견

     * 배경 설명에서 UCLA가 유대인 및 이스라엘 학생들에게 적대적인 교육 환경을 방치한 것에 대해 미국 수정헌법 14조와 시민권법 Title VI를 위반했다는 지적이 있음, 그 결과 UCLA 관련 보조금이 이 문제가 해결될 때까지 잠정적으로 보류되고 있음
          + 원래 소식은 “NSF가 UCLA와 관련된 약 300개의 보조금을 DOJ의 조사 결과에 따라 중단한다”고 전하고 있음, 이는 정부 입장에선 일반적인 절차로 특별히 Tao를 겨냥한 것이 아님, 오히려 더 많은 NSF 보조금이 타격받지 않은 게 놀랍고, 이런 뉴스 헤드라인을 원하고 있음
          + 실제로는 친팔레스타인 학생들이 주요 피해자였고, 최근 주요 가해자가 플리딜(유죄 인정 협상)을 받은 상황임, 관련 기사 링크: UCLA, 친팔레스타인 시위 캠프 불법 규정 기사, UCLA 시위 플리딜 관련 기사
          + 나도 DOJ의 보도자료에서 UCLA가 시민권법을 위반했다는 내용을 방금 읽었음, 이 자료는 논란 속 인물 AG Pam Bondi가 작성함
          + 왜 미국 법이 특정 민족만을 별도로 보호하는 조항이 있고, 모두를 포괄적으로 보호하지 않는지 궁금함
          + 실제 의미는 UCLA가 수정헌법 1조(표현의 자유) 침해를 거부하고, 학생과 교수진이 이스라엘을 비판할 수 있도록 허용한 것에 불과함
     * 지난 1년 동안 미국 STEM 분야가 혼돈과 비탄의 시기였음, Tao의 대학원생들이 당장 큰 피해를 입지 않길 바라지만, 장기적으로는 과학 전체가 심하게 손상되고 있음, 특히 곧 졸업을 앞둔 학생들이 가장 큰 피해를 입을 상황임
          + 아직 6개월밖에 지나지 않았음
          + AGI가 구축된다면 미국은 더 이상 수학자를 필요로 하지 않음, 무언가 계산 필요하면 AGI에게 물으면 됨, 물론 그 구독료는 평생 노동 임금을 넘어설 테지만 미래에는 후손들이 대출로 이를 갚게 될 것임, 하지만 진보는 멈출 수 없다는 농담임
          + 닉슨이 경기침체 준비할 때도 그 시기가 꽤 힘들었음
          + 미국이 최근 1년간 진짜 혼돈과 슬픔을 겪었다는 분석에 동의함
     * 과학적 계몽사상에서 탄생한 미국이 서서히 중세적 사고로 퇴보하고 있음
          + 무슬림 세계를 폄하하던 사람들이, 스스로도 가장 계몽된 곳에서 종교적 암흑 상태로 추락한 그들 운명을 따라가고 있음
     * 수학자들은 이미 국제적 네트워크가 잘 발달해 있고, 값비싼 실험장비가 필요 없으니 이주가 비교적 쉬운 편임, LIGO 같은 프로젝트는 옮기기 어렵지만, 전 세계 어느 곳에도 훌륭한 수학 도서관이 많음
          + 최근 유럽에서 괜찮은 미국 출신 지원자들이 많이 몰리고 있는 상황임, 우리만 겪는 일이 아니라면, 미국 이외의 옵션을 찾는 똑똑한 인재들이 늘고 있다는 신호임
          + Terence는 아마도 자신과 비슷한 수준의 사상가들과 함께하기를 원해서 아이비리그에 갔던 것으로 추측함, 언젠가 이런 인재들이 모이는 새로운 중력이 형성될 수도 있음
          + 연봉 25만 달러 받고 타국 이주하는 건 쉬운 결정 아님
          + 모든 수학자가 이 사람만큼 유명하지 않음, 사실 나도 Terrance에 대해 잘 모르고 HN 메인에 포스팅된 걸 읽은 게 전부지만, 그만큼 그의 명성은 충분히 느낄 수 있음
     * 최근 며칠 전 조사에 따르면 미국 내 이스라엘 지지 여론이 계속 줄어들고 있고, 다른 G7 국가들도 팔레스타인 승인 움직임을 보이고 있음, 앞으로 대학을 친이스라엘 사유로 징계하는 행정조치가 국민 여론과 점점 더 동떨어지지 않을까 궁금함, 관련 링크: 갤럽 여론조사 보기
          + 해당 여론조사는 이스라엘 지지 여부가 아니라 가자지구 전쟁에 대한 찬반만 묻고 있음, 오래된 전쟁을 반대하기는 쉽지만 이스라엘 자체는 여전히 선호할 수 있음
          + 독재 정권이 언제 대중 여론을 신경 쓴 적이 있는지 반문함
          + 대학에 부과되는 조치들이 반유대주의를 핑계로 삼은 것일 뿐, 그리 설득력 있는 명분도 아님, 중요한 건 트럼프 행정부가 불만인 기관이나 반대하는 기관을 처벌하고 있다는 점임
          + 미국 내 이스라엘 지지는 세대별로 크게 다르며, 젊은 세대들은 전통적인 당파 구도를 넘어 이스라엘에 질려 있다는 반응이 많음, 이스라엘도 미국의 지지가 이제는 오래 가지 않을 거란 걸 알고, 지지자 집단인 베이비부머가 정치에서 퇴장하기 전에 전략적 목표(가자지구 합병, 팔레스타인인 제거 등)를 달성하려는 움직임이 있음
          + 만약 트럼프 행정부가 친이스라엘 정책을 번복해도, 지지층 90%가 얼마든지 빠르게 입장을 바꿀 것이라 봄
     * Tao가 중국으로 가면, 중국에서는 그만을 위한 전용 연구소를 설립해줄 듯함
     * 그는 엄청난 수학자이면서도 취미로 활동하는 듯한 일반 대중에게까지 대단한 홍보를 하기도 해서, 사람들에게 멋진 매력을 보여준다는 평임
          + 실제로 그의 영상을 보면 항상 명랑하고 수학과 과학 이야기를 즐겁게 하는데 전혀 잘난 척하지도 않음
     * 이런 식으로 만약 특정 인종(예: 흑인)이나 다른 집단이 적대적 교육 환경의 대상이었다면 이런 일이 벌어졌을까 의문임
          + 아마 벌어지지 않았을 테지만, 사실 그래야 하는 것임
          + 실제로는 보통 흑인에게 유리하게 작용하는 경우가 많음, 차별은 백인, 아시아인, 원주민, 비사회주의자, 남성에게 가해지고 있음
     * 이번 NSF 보조금 중단 사태는 단순히 미국이 스스로 발등이나 가슴을 쏘는 수준이 아니라 뇌를 겨누는 셈임, 문명적 자해 현상임
     * 실상은 UCLA 보조금이 NSF로부터 표적이 되어 Tao의 것도 일부 중단된 것이 맞음
"
"https://news.hada.io/topic?id=22261","모기 죽음의 양동이를 시도해보세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           모기 죽음의 양동이를 시도해보세요

     * 방치된 모기 문제 해결을 위한 간단하고 경제적인 해결책으로 Mosquito Bucket of Death이 소개됨
     * 이 방법은 무해한 박테리아를 사용해 양동이 안에 유인된 모기 유충만을 선택적으로 제거하는 원리임
     * 직접 제작이 가능하며, 비용이 낮고 기존 소독제 살포 방식보다 효과와 환경 영향이 긍정적임
     * 주요 재료는 양동이, 부엽토, MosquitoDunks® 이며, 동물과 인체에 안전함
     * 한 달 정도 경과 후 모기 개체 수 감소 효과 기대 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

야외 모기 개체 수 문제와 해결법

   집 뒤뜰에 모기가 많아 체류가 힘들어진 상황에서, 최근 Mosquito Bucket of Death라는 매우 효과적인 모기 처치 방법이 알려짐
   이 방법은 자연에서 발생하는 미생물을 활용해 모기 유충만을 선택적으로 제거하는 시스템임

Mosquito Bucket of Death 준비 방법

     * 5갤런(또는 3~5갤런) 양동이를 준비하고, 양동이의 절반 정도 물을 채움
     * 부엽토, 흙, 퇴비, 잘린 잔디 등 썩은 유기물을 한두 줌 넣음
     * 나뭇가지 2개 정도를 넣어 떨어진 유익 곤충의 탈출 통로나 모기가 산란 전 착지할 곳을 제공함
     * 작은 동물이 빠지지 않게 하려면 망사 또는 철망을 씌우고 케이블 타이 등으로 고정함
     * 세팅 후 2~3일 야외에 방치하여 썩은물 혹은 모기 유인이 충분해질 때까지 기다림

MosquitoDunks® 투입

     * 양동이 주변에 모기 활동이 보이기 시작하면, MosquitoDunks® 라는 모기 유충 전용 박테리아(Bacillus thuringiensis, 이하 BT)가 들어간 제품을 복수의 버킷에 투입함
     * 이 박테리아 성분은 유충에만 해로우며, 사람, 애완동물, 야생 동물에게는 안전함
     * 제품 사용량은 5갤런 양동이당 4분의 1 조각이면 충분하며, 효과는 약 30일 지속됨
     * 경제적인 방법으로, 약간의 비용만으로도 넓은 지역에 적용이 가능함

수돗물/물 선정과 건축 환경적 요소

     * 만약 염소가 포함된 수돗물을 사용할 경우, 2~3일 방치해 염소 성분을 증발시키는 것이 모기 유인을 높이는데 도움을 줌
     * 만약 집에 제습기가 있다면, 그 응축수는 염소가 없으므로 바로 사용 가능함

모기 생애주기와 효과 발생 기간

     * 모기 생애주기는 알→유충→번데기→성충으로 진행됨
     * 양동이에 숙성된 유기물이 형성되면 암컷 모기가 알을 낳고, 며칠 후 유충이 생성됨
     * 유충은 유기물과 함께 MosquitoDunks® 성분을 섭취하며, 일정 시간 후 사멸함
     * 성충 모기 개체수는 약 한 달 후부터 감소 효과가 나타남

경제성, 환경성, 야외 생활 개선 효과

     * 전통적 모기 방역업체 서비스 대비 비용이 낮고 효과도 높음
     * 화학적 소독제 살포는 모기뿐 아니라 유익 곤충까지 사멸시키는 부작용 발생
     * 버킷 활용법은 부작용과 비용 모두 최소화하면서 모기 개체수만 친환경적으로 줄임
     * 수주 후 본격적인 야외 활동 여건 개선 효과를 체감할 수 있음

결론 및 추가 정보

     * 야외 모기에 시달린다면 버킷과 MosquitoDunks® 로 간단하게 해결할 수 있음
     * 추가 정보와 참고 자료는 아래 외부 링크 제공

외부 자료 및 참고 링크

     * 모기에 대한 Wikipedia 정보
     * 모기의 수명
     * 버킷 이용 친환경 모기 처리
     * 모기방제제의 인체 및 생태계 안전성
     * 모기 퇴치 및 방제 팁
     * BTi를 활용한 모기 방제 - 미국 EPA

관련 기사

     * 야드에서 시작되는 건축과학
     * 빗물통, 닭, 그리고 지속 가능한 생활 실천
     * Lloyd Alter의 1.5도 라이프스타일 실천기

        Hacker News 의견

     * 나는 습지 호수 근처에 살고 있음
       처음엔 이 방법이 모기 개체수에 별 효과 없을 거라 생각했지만, 실제로 여름 내내 집 밖에 앉아 있어도 웬만하면 모기 물림이 적었음
       방법은 홈디포 버킷 4개와 모기 덩크 한 팩을 매달 한 번씩 자리를 바꿔가며 유지하는 것
       호숫가에 두 군데, 나머지 두 개는 집 구석에 뒀는데, 호수는 표면에 잔잔한 움직임이 있어서 오히려 버킷이 모기에 더 적합한 산란지로 보임
       버킷에 나뭇가지와 잎사귀를 넣으면 곧 고인물이 되고, 이게 너무나 매력적인 산란지여서 근처 모기들이 거의 다 여기에 몰리는 게 아닌가 싶음
          + 모기 물고기(Mosquitofish)를 생물학적 방제로 활용해볼 것을 추천함
            이들은 방치된 수영장 등에서도 효과적임
            모기물고기 위키피디아 링크
          + 나는 주기적으로 버킷 위치를 바꾸는 이유가 궁금함
          + Bt 박테리아(모기 유충 살충 박테리아)가 버킷 외부까지 퍼져서 다른 데 있는 유충도 효과를 보는 건 아닐지 궁금함
     * 다른 물웅덩이도 모두 제거하는 것이 모기 방제에서 매우 중요한 포인트임
       내가 시도했던 다른 효과적인 방법은 이 영상에 나옴
       큰 드럼 팬 뒷면에 자석으로 망을 부착하고, 팬을 틀면 모기가 약한 비행이라 망에 달라붙고 빠져나오지 못해 곧 죽음
       다른 날벌레는 대부분 잡히지 않고 나방이나 좀잠자리 등 극소수만 같이 들어감
       팬 앞에서 시간을 보내도 모기가 잘 접근 못 해서 여러모로 좋음
       브루클린 공동 뒷마당에서 사용했을 때 일주일에 수백~수천 마리 모기를 잡음
       그럼에도 여전히 모기가 많았으니, 다른 방법과 병행하는 게 좋음
       최신 영상 링크도 있음
       유튜브 영상 링크
          + 영상만 봐서는 팬 앞에 모기를 유인하는 미끼나 유인제가 필요한지 모르겠음
          + 영상 속 팬은 200~400W 정도라서 계속 켜 두는 건 다소 에너지 낭비라는 생각임
     * 궁금한 게 있음: 내가 찾지 못하는, 숨어있는 고인물에서 모기가 번식하는 건 어떻게 막을 수 있는지 모르겠음
       모기가 어디서 오는지 전혀 모르겠고, 내 추측으론 다른 데 있다가 우리 마당 그늘진 곳으로 날아오는 것 같음
       버킷을 두면 다른 곳에 알을 못 낳게 좀 막을 수 있지만, 모든 모기를 다 막는 건 아니지 않을까? 아니면 버킷에만 너무 끌려서 다른 곳은 신경도 안 쓰는 건지 궁금함
          + 우리 해충 방제업체는 In2Care라는 버킷을 설치했음
            물 위에 가루가 묻은 작은 망이 있고, 모기가 망에 올라서면 가루가 몸에 묻어 다른 곳으로 옮겨가고, 그곳도 중화됨
            상업용이지만 연 200달러 정도로 주거용으로도 가치 높음
            효과 보기까지 시간이 걸리고 관리도 필요하지만, 지금까지 매우 효과적임
          + 나도 같은 생각임: 버킷 하나로 모든 산란지를 없앨 순 없음
            결국 개체수를 최대한 줄여서 자연적으로도 통제할 수 있게 하는 '숫자 싸움'임
            모기도 짝짓기 상대를 찾아야 하니 개체수가 줄면 번식 성공률도 확 떨어짐
            즉각 효과는 없지만 몇 세대가 지나야 감소 효과를 실감할 수 있음
          + 많은 모기 종류는 태어난 곳에서 멀리 못 날아감 (특히 '타이거 모기'가 그런데 100m 이상 멀리가지 못함)
            자기 집 마당만 집중적으로 관리해도 효과가 어느 정도 있음(주변 주거 밀집도에 따라 다름)
            자기 마당에는 반드시 고인물이 없도록 하고(화분 받침같은 작은 곳까지), 다음 단계로 트랩을 추가하고, 가까운 이웃과 함께 실천하는 게 중요함
          + 눈에 보이는 고인물(오래된 타이어, 도랑, 바퀴 자국) 외에 집 외벽 홈통, 프렌치 드레인(자갈 아래), 묻힌 배수관, 차고 바닥 배수구 등 숨겨진 곳도 꼭 확인해야 함
            모기는 물 뚜껑만한 크기의 고인물에도 번식할 수 있고, 그 물이 일주일 이상만 고여있으면 충분함
            잘 찾아보고 신경쓰기 바람
          + 버킷 하나만으로는 안 되고, 여러 개를 설치해서 산란할 확률을 높임
            내 집 주변엔 20개의 작은 오비트랩을 설치 중임
     * 이 방법의 핵심은 오직 버킷만이 유일한 고인물이어야 한다는 것임
       암컷 모기 중 단 10%라도 집의 막힌 홈통, 고장난 분수, 잊혀진 가축 물통 등에 알을 낳으면 여전히 모기 문제를 겪게 됨
          + 즉, 정작 중요한 요령은 주변 모든 고인물을 제거하는 것임
            내 경험상 버킷 만드는 데 힘들였지만 전혀 소용없었고, 오히려 악화된 듯함
            고인물 제거에 10분의 1만 투자해도 모기 문제가 해결됨
            모기는 물병 뚜껑만한 웅덩이에서도 번식 가능하니 각별히 신경쓰기 바람
          + 그래도 모기 10%만 남는 게 100%보단 낫다고 생각함
          + 예전에 반쯤 숲 지역으로 이사했을 때 모기 덩크를 써봤는데, 끝없는 시지프스의 고역 같았음
            그런데 이번에 버킷 방식은 꽤 흥미로움
          + 이런 저렴하고 간단한 생태 문제 솔루션에 너도나도 몰려드는 분위기가 너무 좋음
            프로그래머들이 여기 모여 있어야 다른 곳에 피해 덜 주는 느낌임
     * 나는 모기 물린 후 가려움에 가장 효과 있던 기기를 찾았음
       아마존 Vibis Rechargeable Mosquito Bite Relief (모든 걸 시도해본 플로리다 거주자임)
          + 이런 기기들은 설명처럼 모기 독을 열로 분해하는 것은 아니고, 열 자극이 가려움신경을 잠깐 마비시키는 원리임
            신경이 다시 정상화되면 가려움이 재발할 수 있음
            민감한 사람은 완전히 효과를 보기 힘들 수 있음
     * 과학자들이 말하길, 주변 집까지 몽땅 모기 잡지 않으면 효과가 없다는 비유(바다에 배수구 설치와 같다는 말)를 들을 때마다 공감함
       정말 광적으로 모기를 잡아봐도, 내 반경 1마일 이내 모든 집에서도 같이 잡지 않는 이상, 계속 새로 들어오니 한계가 있음
          + 모기는 멀리, 빠르게 날지 못하고 번식 속도가 워낙 빨라서 인근에 고인물이 있으면 '국지적 밀집 구역'이 생김
            유입량이 너무 많아 모기 개체수가 줄지 않음
            이건 마치 경기장 입구에 팬들이 한꺼번에 몰려드는 것과 같음
            나도 습지에 살면서 ""타이어 한 개쯤이야"" 했는데 하나 제거하는 것만으로도 엄청난 효과가 있었음
          + 우리도 버킷을 매우 열심히 설치했더니 모기를 잘 끌어들이긴 했지만, 전반적인 효과는 미미했음
            옆집이 렌트라 마당 관리가 제대로 안 되어 모기한테 천국이었음
            이럴 땐 HOA(관리위원회)가 도움될 수 있다고 생각함
            만약 우리 동네 전체가 함께 하면 어떤 결과가 나올지 궁금함
          + 모기는 의외로 먹이나 산란지 찾아 멀리 안 감
            이웃들과도 함께 쓰길 권장함, 작은 선물 바구니 만들어보면 좋음
          + 모두가 동참하면 효과가 분명함
            트랩 하나만으로 집이 살기 좋아지는 건 사실임
            예전에 브라질 숲 근처 집에서 살 때, UV램프와 팬이 달린 고급 트랩 샀더니 엄청나게 모기를 잡았음
            아내가 모기 알레르기가 심해 방에 하나라도 있으면 바로 알 정도임
          + 나는 이 의견에 의문이 있음
            앞마당과 뒷마당에 살포한 뒤 모기 개체수가 90~95% 줄어들었고, 바비큐 할 때 아주 쾌적했음
     * 나는 버킷을 실내 화분의 버섯파리(깔따구류) 통제에 매우 효과적으로 사용 중임
       물주전자에 하나 넣어두고 계속 물 채워두고 평소처럼 화분에 물 주면 됨
       유충이 같은 방식으로 죽어서 한 달 정도면 문제 해결됨, 다른 여러 방법 시도하다 이 방법만 효과 있었음
       다만 대규모로 쓰면 모기가 내성 갖게 될지 궁금함
          + 이 박테리아는 1976년에 발견됐고 자연에도 오래전부터 존재함
            이미 내성이 생겼어야 하는데 아직 문제가 없음
            Bt의 살상 원리는 표적 해충의 장 세포 수용체에만 결합한 뒤 파괴됨
            사람, 동물, 표적 외 곤충들은 해당 수용체가 없어 영향 없음
            1920년대부터 규산염 스프레이, 펠렛 등으로 널리 사용됨
            바실러스 튀링기엔시스 위키피디아
          + 나도 화분 버섯파리 문제가 있어서 이 방법 꼭 써보겠음, 팁 고마움
     * 모기 덩크 대신 같은 성분의 '모기 비트'(granular, 과립형)도 구매 가능함
       단위 가격이 더 저렴하고, 분할해서 쓰는 번거로움도 없음
          + 이상하게도 캘리포니아에는 덩크는 배송가능한데 비트는 안 됨(아마존 기준)
            둘이 똑같아 보이는데 왜 그런지 아는 사람 있음? 데이터 오류일 수도 있을 것 같음
     * 수돗물을 바로 쓰고 싶다면 아스코르브산(비타민 C)으로 염소와 클로라민을 즉석에서 중화 가능함
       아스코르브산은 친환경 환원제로, 500~1000mg만 있으면 5갤런의 센 시 수돗물도 확실히 중화시킴
       잔여 성분은 대부분 생물에 안전하니 바로 유기물 투입해도 됨
          + 다른 저렴하고 쉽게 구하는 산(구연산 등)도 같은 효과가 있는지 궁금함
          + 괴혈병 예방에도 도움됨
     * 이전에 물을 기반으로 한 방법은 효과를 못 봤지만, CO2 기반 트랩은 효과 만점임
       거주지 주변 네 집이 이걸 사용 중
       모기는 CO2를 따라가서 트랩에 빨려 들어가고, 실제로 잡힌 모기 봉지가 넘칠 정도임
       초기장비 200달러 정도, 여름 한철 CO2비용이 60달러 남짓이지만 확실히 구별되는 효과가 있음
       CO2가 떨어지면 모기가 다시 돌아오는 게 바로 느껴짐
       연관 없음: Biogents 사의 제품
          + 나도 CO2 유인이 매우 효과 있어 보인다고 생각함
            레딧서 본 DIY 버전에서, 불씨(숯)가 내는 CO2를 팬 뒤에 두고 앞쪽엔 모기망을 살짝 느슨하게 설치, 팬이 CO2를 흡입해 동네로 뿜어냄
            모기가 CO2를 따라오다가 팬 흡입구에 빨려들어가 날개와 망 사이에 못 빠져나오고 대부분 죽어있음
            한 번에 몇 주마다 한 번 정도면 모기 개체수가 현저히 줄고, 아침마다 수천 마리가 망에 걸림
            CO2 사용이 약간 꺼림칙해서 시도는 안 했는데, 소형 탱크로 천천히 방출하는 방법을 고민 중임
            최근 동네에서 방역을 시작해서 당장 시도할 필요성은 못 느낌
"
"https://news.hada.io/topic?id=22342","Cactus - 스마트폰을 위한 Ollama","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Cactus - 스마트폰을 위한 Ollama

     * 다양한 디바이스(스마트폰, 노트북, TV, 카메라 등)에서 GGUF 모델을 직접 실행할 수 있게 해주는 크로스플랫폼 프레임워크
          + Huggingface; Qwen, Gemma, Llama, DeepSeek 등에서 제공되는 아무 GGUF 모델이든 가능
          + 앱 내에서 LLM/VLM/TTS 모델을 직접 배포·구동
     * Flutter, React-Native, Kotlin Multiplatform을 지원하며, 텍스트, 비전, 임베딩, TTS 모델 등 다양한 타입의 모델을 온디바이스로 실행 가능
     * FP32부터 2비트 양자화 모델까지 지원해 모바일 환경에서 높은 효율성과 저전력 구동가능
     * 챗 템플릿(Jinja2), 토큰 스트리밍, 클라우드-로컬 자동 폴백, Speech-To-Text 등 지원
     * Cactus 백엔드는 C/C++로 작성되어 있어, 모바일, PC, 임베디드, IoT 등 거의 모든 환경에서 직접 동작
     * 최신 스마트폰 기준 Gemma3 1B Q4 는 20~50 토큰/초, Qwen3 4B Q4은 7~18 토큰/초 속도로 동작
     * HuggingFace Cactus-Compute에서 추천 모델 다운로드 가능

활용 포인트 및 장점

     * 기존 온디바이스 LLM 프레임워크와 달리 여러 플랫폼을 통합 지원, 로컬-클라우드 하이브리드 아키텍처 구현 용이
     * 모바일 기기에서 고성능·저전력으로 최신 LLM/VLM/TTS 활용 가능
     * 앱/서비스 내 프라이빗 데이터 처리, 오프라인 AI 활용, 비용 절감 등 다양한 B2C/B2B 시나리오에 적합

   안드로이드 앱
   https://play.google.com/store/apps/…

   입니다

   주말에 한번 써보고싶네요
"
"https://news.hada.io/topic?id=22311","Apache Libcloud - 클라우드를 조작하는 단일 API","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Apache Libcloud - 클라우드를 조작하는 단일 API

   파이썬 라이브러리로써, 주요 클라우드 리소스를 하나의 python code 로 일관되게 통제할 수 있게 해줌.

   아쉽게도, Amazon의 ""EC2"" 같이, 부분적으로만 구현이 되어 있는 것으로 보임

   배포 추이만 봐서는 저무는 추세같긴 하네요...
   https://github.com/apache/libcloud/tags

   파이썬 버전도 3.11에서 멈춘거같고..
"
"https://news.hada.io/topic?id=22349","객체들은 조용히 해야 함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             객체들은 조용히 해야 함

     * 자동차와 가전 제품이 불필요하게 소음을 발생시켜 사용자 경험을 저해함
     * 경고음과 시각적 알림은 중요하지 않은 상황에서도 빈번히 제공되어 불편함을 초래함
     * 이러한 소음 알림 대부분은 비상 상황과 무관하며, 비활성화 옵션이 없어 문제를 심화시킴
     * 일부 장치는 적절하고 조용한 알림 방식을 적용하여 긍정적으로 평가 가능함
     * 기기 설계 시 사용자를 위한 알림 최소화와 맞춤설정 가능성 확보 필요성 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

운송수단과 불필요한 경고음

     * 소형 자동차에 듀얼 연료탱크(가솔린과 LPG) 를 사용함
     * LPG가 저렴하여 예산에 도움을 주지만, LPG 잔량이 줄어들면 매우 시끄러운 경고음이 울림
     * 이 경고음은 운전 중 갑작스럽게 발생해, 특히 고속도로에서 운전자의 집중력 저하와 놀람을 유발함
     * 동시에 대시보드 전체에 ""LPG 잔량 부족"" 메시지가 화면을 가림
     * 이미 대시보드 하단에는 LPG 잔량 표시가 상시 존재함

경고음의 필요성과 현실

     * 차량이 소음을 내야 하는 유일한 경우는 진짜 위급 상황이어야 한다는 의견 제시
     * 예를 들어 엔진 오일 부족과 같은 상황에 한해 알림음이 적절함
     * LPG 주행 가능거리가 100km 남았지만, 가솔린 탱크가 가득 찬 상태라 전체 주행 가능거리는 1,000km임
     * 불필요한 알림음은 수면 중인 승객(특히 영유아) 의 잠을 깨우고, 오히려 승객 안전을 위협하는 요소임
     * 이런 알림음은 비정기적으로 몇 차례 반복되어 혼란을 가중함

스마트 가전제품의 소음 문제

     * 스마트 기기를 최소로 보유하려 노력해도, 가전제품에서 여전히 불필요한 소음이 발생함
     * 예시로 세탁기는 세탁 완료 시 매우 시끄러운 알람을 울리나, 이 알람은 매우 짧고 사용성에 큰 도움이 되지 않음
     * 세탁기의 노브나 버튼 작동에도 비활성화할 수 없는 ""삑"" 소리가 남
     * 모든 소리가 동일하고, 터치식 버튼만 사용해 시각장애인의 접근성에도 도움이 되지 않음
     * 기기 작동 시 특별한 부트 절차나 위험상황도 없는데도 불구하고 쓸데없는 시작음을 내는 경우 다수 존재함

일상 속 다양한 소음 예시

     * 상상으로 수도꼭지, 현관문, 후드, 거실 조명 등 일상 모든 기기가 시끄러운 소음을 낸다면 매우 비효율적임
     * 건조기 역시 버튼음과 알람을 끌 수 없으며, 옷이 다 마른 것에 대해 긴급도가 없음에도 시끄럽게 알림
     * 인덕션 및 핫플레이트도 터치 버튼 한두 개에 손수건이나 물이 닿으면 큰 소음을 발생시킴
     * 베이비폰은 전원을 켜면 크고 짧은 삑 소리를 내서 아이가 깰 위험이 있으며, 주요 사용 목적에 어긋남

소음이 없는 긍정적 사례

     * 식기세척기: 소리가 없고, 작동 완료 시 조용히 뚜껑만 열림
     * 냉장고: 문이 덜 닫힐 경우 약한 알림음이 잠시 울림
     * 이북 리더기: 아예 소음을 낼 수 없는 구조임

사용자 중심의 제품 설계 제안

     * 제품 구매 시 ""조용한 제품인지"" 확인하는 항목이 늘어나는 실정임
     * 알림음 선택권을 사용자에게 부여하거나 기본적으로 비활성화하는 것이 바람직함
     * 알림 메커니즘을 설계할 때 수면 중인 아이 또는 몹시 피곤한 사용자를 고려해야 함
     * 불필요한 소음 알림 대신 시각적 신호, 자동 종료 등 다른 방법으로 대체 가능함
     * 이미 사회가 알림 과포화 상태인 점에서 기기의 불필요한 간섭 최소화 중요성 부각됨

        Hacker News 의견

     * 안전 산업, 특히 항공 분야에서는 “알람 피로”가 정말 심각한 문제임을 경험함. 조종사가 상황을 파악할 수 있는 여력에는 한계가 있으며, 꼭 필요하지 않은 경고로 괴롭히지 않는 것이 중요하다고 느낌. 나 역시 충돌 회피 시스템(TAS/TCASI/TCASII) 개발에 참여했으며, 조종사에게 ‘상황이 확실치 않으니 혹시 모르니 알려주자’는 이유로 경고를 제공할 때, 이 기능이 이륙·착륙 같은 중요한 순간엔 오히려 방해가 될 수 있다는 점을 파악하고 긴 시간 고민함. “조종사에게 꼭 알려야 할 내용을 혹시 놓치진 않을까?”와 “정말 중요한 일에 몰입 중인 조종사를 괴롭히지 않아야 함” 사이에서 밸런스 잡는 것이 항상 어려움. 많은 기기에 “스퀠치(squelch)” 스위치가 더 많아졌으면 좋겠다는 마음임
          + 알람 피로에 가장 큰 영향을 주는 요소가 음성 안내(예: “bank angle”)와 삐- 소리, 혹은 오토파일럿 해제음 같은 비프음 중 무엇인지 궁금함. 특히 후자가 왜 그렇게 크게 작동하는지 궁금함
          + 항공에서 “불필요한 NOTAMS(Notice to Airmen)” 문제가 떠오름. 관련된 재미있는 글도 존재함
          + 스퀠치 스위치가 무슨 역할을 하는지 궁금함
     * 내 새 차에는 forward attention warning이라는 기능이 있는데 미칠 것 같음. 스티어링 휠 기둥에 카메라가 있고, 운전자가 앞을 안 본다고 생각되면 높은 소리로 비프음이 울리고 대시보드에 눈 아이콘이 깜빡임. 고속도로에서 차를 추월하려고 자세를 바꿀 때, 손을 잘못 올려 카메라가 얼굴을 못 볼 때, 저광량에서 선글라스를 낄 때, 작동함. 비활성화할 수 있지만, EU에서는 시동 on/off 후 자동으로 다시 켜져야 함. GPS 데이터 오류로 과속 경고도 자주 울림. 이 기능도 시동을 끄면 매번 다시 활성화됨
          + EU에서는 이런 점점 더 침범이 심해지는 “안전” 기능들이 실제로는 자동차를 덜 안전하게 만들고 있다고 생각함. 최근 가족의 신차 Nissan을 운전해 봤는데, 오른쪽 흰색 실선을 감지해 “가깝다”고 판단하면 자동으로 핸들을 급하게 꺾어버리는 기능이 있음(기본값이라 꺼도 다음 시동 때 다시 켜짐, EU 법 때문). 내가 주로 다니는 도로는 좁아서 마주 오는 차를 피하려면 흰 선을 밟아야 안전함. 대형 버스나 트럭과 마주칠 때는 경고음과 핸들 털림까지 겹쳐 매우 위험한 상황이 됨. 나도 “자동충돌” 기능이라 부름. 이 기능을 시동 걸 때마다 끄는 습관이 생김. 그래서 직접 EU Directive를 찾아봤더니, 신차는 lane assist를 무조건 탑재해야 하고, 최소 2가지(음향, 시각, 햅틱)로 운전자에게 경고해야 한다고 명시되어 있음. 법 자체가 이 기능을 강제함에
            놀라고 있음
          + 몇 년 전 영국에서 빌린 차량이 두 달이 지난 후 불태워버리고 싶을 정도였음. 어댑티브 크루즈 컨트롤이 갑자기 고속도로에서 브레이크를 세게 밟거나, 추월 중 맞은편 차 속도를 따라가려고 함. 비상 브레이크는 내가 내 차선을 벗어나지 않았는데도 근처 차량이나 주차 중 덤불이 감지될 때마다 작동함. Lane assist 기능은 매번 시동 때마다 재활성화되고, 레이더 시스템이 자주 오류나서 경고음이 계속 울림. 이 경험 이후 차량 구입 기준에 “날 죽이려 들지 않는 기능”을 추가함
          + 나는 전반적으로 EU를 지지하는 편이지만, 사람을 최대한 귀찮게 하면서 아무것도 해결하지 않는 능력도 확실하다고 느낌. 쿠키 법, 헤드폰 볼륨 경고 등도 떠오름
          + 내 2010년 비디지털 차를 평생 몰 것임. 클래식카로 등록해서라도 계속 탈 것임
          + 2020년식 Civic(EU 모델)에서 lane assist를 비활성화하면 영구적으로 꺼짐을 발견함. 이 기능이 필요 없는 분들에게는 참고가 될 수 있을 것임
     * 모든 기기에 번쩍거리는 블루 라이트가 on/charging/charged를 표시하면서 집이 SF 영화의 우주선처럼 변함. 이 부분도 해결되었으면 하는 바람임
          + 나 역시 침실에서 수면이 힘들 정도로 밝은 LED 때문에 두 겹의 전기테이프를 붙이고 있음. 너무 밝아서 천장에 파란 원이 비칠 정도임. 초대받지도 않은 파란 달이 뜸
          + 이런 문제는 구매 후에야 인식하게 되므로, 제조사 입장에선 크게 신경 쓸 필요가 없고, 그래서 개선 압력도 없음. 임시방편으로 부분 차단 스티커를 사용하면 50~80% 투명으로 잔광만 남고, 필요하면 여러 겹 겹치면 됨. 완전 차광이 필요하면 100% 불투명 제품도 있으나, 이 정도면 그냥 일반 테이프 써도 되는 거 아님? 하는 생각도 듦
          + MacBook Pro의 이중 magsafe 충전 표시등도 심각함. 여행할 때와 같이 맥북을 침실에서 써야 할 때는 불빛이 직접 눈에 들어오지 않게 방향을 바꿔두거나 물건을 쌓아둬야 겨우 견딜 수 있음
          + 내가 본 최악의 경우는 USB-PD 충전기에서 발생했음. 자동차 12V 아웃렛을 대체하는 제품인데, 얼굴 전체에 빛이 퍼져 운전할 때 엄청 산만했음. RTV 실리콘으로 덮음
          + 나도 침실에선 흑색 전기테이프를 붙임. 왜 이런 선택만이 유일한 해결책인지 의문임
     * 외부 소음도 문제지만, 컴퓨팅 기기의 팝업 알림이나 모달 창이 더 큰 짜증을 유발함. 집중력을 유지하기도 힘든데, 쓸데없는 업데이트 팝업이나 각종 알림이 수시로 작업 흐름을 깨뜨림. 내 스캐너 소프트웨어가 업데이트된 사실은 한 번도 듣고 싶지 않음. 이런 문제는 개발자나 제품 관리자들이 내가 자기 제품에 자기만큼 관심을 가져줄 거라고 오판하기 때문임. 검색엔진이 랜덤 모달이 뜨는 사이트의 순위를 내리거나, 앱스토어에서 ‘평균보다 무시되는 알림의 비율’을 공개지표로 노출하는 식의 패널티 메커니즘이 있으면 좋겠음
          + 외부에서 아무 일도 일어나지 않았는데 혼자서 알림을 띄우는 앱은, 즉시 그리고 영구적으로 모든 알림을 꺼버림. “지금! 사용자에게 알림이나 보내자!”라는 식의 행동은 원치 않음
          + macOS와 Windows 10에서 알림 스택 전체를 비활성화하고 매우 만족함. Slack, Discord, 메일 등은 도크/작업 표시줄 아이콘만 바꾸면 충분히 새 메시지를 인식할 수 있음. 하지만 무작위로 Java 업데이트, Apple Music 할인, 드라이버 업데이트, Windows Defender 결과, USB 분리 안내 등 귀찮은 팝업이 뜨는 것은 특히 화면 공유 중이라면 정말 곤욕임
          + 이 문제는 소프트웨어뿐 아니라 모든 비즈니스에서 마치 내가 그 회사를 정말 좋아하는 것처럼 착각하고 접근하는 게 근본적인 문제임. 수천, 수만 개의 비즈니스로부터 수십 년간 구매했지만, 실제론 각 회사가 원할 때마다 뉴스레터, 알림, 설문 참여를 할 여유가 없음. 하루에 30개 업체에서 “5분만 내주세요” 요청이 오면 결국 하루 2시간 이상을 허비하게 되는 셈임. 이런 부담을 이해 못하는 경영진이 많다고 느낌
     * 오락실 기계 시절에는 빠르게 다음 손님을 받아야 하니 의도적으로 유저를 밖으로 밀어내는 디자인 감성이 생겼고, 경제 모델이 “머무름 증가”로 전환된 이후 수십 년이 지나도 한참 동안 그 유산이 지속됨. 지금은 모든 시스템이 사용자 주의와 ‘engagement’에 집착하다 보니, 아무 실질적 이득도 없는 시스템에서도 이 관성이 남아 있음. 이제 토스터마저 “like & subscribe”를 요구하거나, 화장실 변기가 팝업 알림을 띄울 것 같다는 생각이 듦
          + 화장실도 가끔 알림을 띄우긴 하지만, 끄려고 하면 플런저와 대걸레가 필요함
     * 스타트업 아이디어 제안: 아래 기능을 지닌 가전 브랜드
          + 어떤 경우에도 비프음이나 소리 절대 미발생
          + 직접적 컨트롤만 제공, “프로그램” 없음(전자레인지는 오직 파워/시간 다이얼)
          + 네트워크 기능 전혀 없음 강력한 브랜드·마케팅만 있으면 정말 많이 팔릴 것 같음
          + 이 조건에 거의 부합하는 브랜드가 “Speed Queen”. 요즘은 디자인/스크린 강화 모델도 있지만, 전통적인 제품군도 여전히 판매 중임
          + 왜 이런 제품군이 없을지 이해가 안 됨. “노-가짜” 가전 시장은 엄청나게 큰데도, 어느 기업이 이 시장을 차지해 대박칠 날만 기다리고 있음
          + 이런 제조사에 대한 생각이 오래 전부터 머릿속을 맴돎. 진짜 소비자 친화적 특징(다크패턴·계획적 노후화 없는 기본에 충실한 제품)만 만드는 회사가 있으면 내 통장이 거덜 날 수도 있음. 하지만 현실적으로 이런 스타트업이 대형 제조사들의 소송 공세를 견딜 수 없을 듯. 전통적 광고 플랫폼도 기존 업체 광고 매출이 워낙 많아 새로운 기업의 마케팅을 거부할 것 같은 회의감도 있음
          + 최근 “America’s Test Kitchen” 유튜브 채널의 전자레인지 리뷰를 보니 (1) 음향 꺼짐 가능, (2) 네트워크 기능 없음, (3) 직접 조작 다이얼 등 기준으로 고득점 평가함. 해당 영상 참고. 물론 가입해야 세부 정보를 볼 수 있음은 아쉬움. 내 아파트 내장형 전자레인지 소음이 너무 커서 불만임
          + 오히려 등장할 제품은 아마 아래가 될 것 같음:
               o 소음 차단 이어플러그
               o 번쩍임/경련/플래시 필터 장착 스마트 글라스 계속된 ‘무기 경쟁’ 속에서 유저들은 shiny iBlocks, iPlugs에 빠져 있을 것 같은 씁쓸함도 있음
     * 내가 정말 싫어하는 것 중 하나는 GM 차량들의 후진등. 최근에는 차량이 꺼져 있거나 사람도 없는 상태에서도 후진등이 켜짐. 아마 주변에 사람이 있음을 알려주는 의도였겠지만, 원래 이 등은 엄연한 의미가 있는 신호인데 GM의 구현 탓에 의미가 불명확해져버림
          + 이 문제를 궁금해해서 FMVSS 규정을 직접 확인함. 회로에 전원이 인가되고, 리버스 기어가 들어가 있을 때만 활성화되어야 하고, 주행 중에는 금지되어야 한다고 나옴. 파킹 상태에도 이 등 사용을 금지하도록 규정 개정이 필요할 듯. NHTSA에 제안해보고 싶음
          + 실제로 후진등이 원래 ‘꼭’ 특정 의미를 가지도록 설계되어 있지 않음. 사실 전조등처럼 단순히 뒤쪽을 밝히는 역할임. 차 기능상 “헤드라이트를 켜라”에 맞춰 후진등도 같이 작동하는 건지 의문. 어쨌든 차에 사람이 없는데 라이트가 켜짐은 이상하다고 느낌
     * 현대 병원 응급실에 가면 모든 방에서 각각 다른 알람음과 기기 소리가 모여 큰 혼란을 만듦. 제조사 입장에선 환자 문제를 놓쳤다는 소송 우려로 경고음 남발에 집착하는 듯. 실제론 의료진이 집중해야 할 상황에선 끝없는 소음만 남음. 결국 UX 측면에서 완전히 실패한 시스템임
          + 가족이 병원에 자주 입원했는데, 의료진은 그냥 모든 기기가 시도 때도 없이 울려도 방치함. 심지어 고장난 혈압계조차 한 시간 내내 울려도 눈치 못 챔. 만성 고혈압 환자는 항상 알람이 울리니 경고음은 무시할 수밖에 없고, 알람은 계속 울림. 환자나 보호자에게는 큰 스트레스/수면 방해가 됨
          + 경험상 간호사들은 어떤 경고 정도까지는 무시해도 되는지 능숙하게 구분할 줄 앎(예: 완료된 펌프 경고는 무시). 하지만 신생아 부모 등은 경보음이 수면에 방해됨
          + 나 역시 병원에서 모든 기기 알람을 꺼버리는 데 전문가가 되었음. 일부 간호사가 뭐라고 하기도 하지만, 경고를 제대로 관리하지도 않는 간호사들이라서 문제 삼으면 오히려 입장이 곤란해질 것임. “입원한다고 건강해지는 게 아니다”라고 항상 주변에 권장함. 병원은 최후의 수단이어야 하고, 입원했다면 퇴원에 주력하는 것이 최선
     * 종종 제품의 삐 소리가 거슬릴 땐 비프 스피커에 테이프를 붙여 음량을 줄임. 천장 선풍기 리모컨 조작마다 큰 삐 소리가 나는 것도 한밤중엔 정말 불편함
     * 내 무선 이어폰은 배터리 사용 가능 시간이 20분 남았을 때부터 매분 경고가 울리기 시작함. 이 때문에 남은 20분을 참다가 결국 더는 못 쓰게 됨
          + 이상하게도 헤드폰에서 나오는 모든 알림은 무조건 최대 볼륨임
          + 물론 이런 문제에 소비자 선택이 답이라는 주장도 있지만, 이런 사용자 의견을 완전 무시하는 디자인 문화 탓에 사실상 아무 효과 없음. 5만 원짜리 헤드폰 같은 소형기기라면 다른 회사 제품을 쓰라는 의견도 가능하지만, 대형가전처럼 쉽게 바꿀 수 없는 제품은 힘들 것임
          + 내 헤드폰도 배터리 20분 미만이면 “battery low”를 몇 분마다 외침. 너무 짜증남
"
"https://news.hada.io/topic?id=22335","Show GN: 감정 대필 서비스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: 감정 대필 서비스

   최근 몸 담고 있는 서비스의 채널톡으로 문의가 하나 들어왔습니다.
   서비스와 무관한 내용이었지만 사이드 프로젝트를 시작하게 된 계기였습니다.
   해당 문의 내용은 대략 이렇습니다.
   다음 달에 딸이 결혼을 하는데 마음이 불안하고 싱숭생숭하다는 내용이었습니다.
   뭔가 위로의 말이라도 건네고 싶었으나 마음처럼 잘 되지 않더군요.
   그래서 만들게 된 것이 감정 대필 서비스입니다.

   허가 생각나네:)
"
"https://news.hada.io/topic?id=22288","Claude Code Router - Claude Code 요청을 다양한 모델로 라우팅하는 오픈소스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Claude Code Router - Claude Code 요청을 다양한 모델로 라우팅하는 오픈소스

     * Claude Code의 리퀘스트를 다양한 LLM·API 제공업체(OpenRouter, DeepSeek, Ollama, Gemini 등)로 자동 라우팅하고, 요청·응답을 자유롭게 변환 및 확장할 수 있는 도구
     * 상황에 따라 적합한 모델을 선택해 요청을 분기하거나, /model 공급자,모델명 명령어로 실시간 모델 전환 가능
          + 일반, 추론, 장문, 백그라운드 등 용도별로 요청을 다양한 모델로 자동 분기
     * 요청/응답 변환(transformer), 플러그인 시스템, GitHub Actions 연동 등 다양한 확장 기능을 지원
     * 다양한 공급자·모델·옵션을 조합해 효율적 비용 관리, 장기 컨텍스트·추론/배경 작업 분리, 사용자 정의 라우팅 스크립트 등 유연한 자동화 가능
          + JavaScript로 복잡한 라우팅 로직도 직접 구현 가능
"
"https://news.hada.io/topic?id=22352","Substack을 떠나야 할 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Substack을 떠나야 할 이유

     * Substack은 증오 발언과 허위 정보를 플랫폼에서 허용하고, 이에 기반한 수익 창출을 막지 않는 점때문에 비판 받고 있음
     * 여러 언론 매체와 전·현직 창작자들이 콘텐츠 검열 부재, 극단주의 허용, 수익 분배 문제 등 Substack의 구조적 한계를 지적함
     * 이에 따라서 많은 뉴스레터 운영자들이 Beehiiv, Buttondown, Ghost 등 대체 플랫폼으로 이전하고 있음
     * Substack을 떠나려는 이들을 위해 이전 방법과 가이드를 안내함

Says Who? (누가 이렇게 말하는가?)

     * Substack이 극단주의, 증오 발언, 허위 정보 등 문제를 방관하거나 오히려 수익화하는 구조임을 지적하는 다양한 언론 보도와 전문가 의견이 있음
     * 2021-04 : ""I Am Leaving Substack"" - Notes on the Crises
          + Substack 사용자가 소수자 집단을 직접적으로 표적 삼아 괴롭히고, 개인정보를 유출(doxing)하는 등 추가적인 문제에 대해 다루는 글
     * 2023-11 : ""Substack has a Nazi Problem"" - The Atlantic
          + Substack에 나치 및 백인우월주의자들이 활발히 활동하고 있다는 실태와 이에 대한 플랫폼의 소극적 대응을 비판하는 심층 기사
     * 2023-12 : ""Substack says it Will Not Ban Nazis or Extremist Speech"" - NYTimes (archive)
          + Substack 측이 나치 또는 극단주의적 발언을 금지하지 않겠다고 공식적으로 밝힌 사실을 다룸
     * 2024-01 :
          + ""Why Platformer is leaving Substack""
            인기 뉴스레터 Platformer가 Substack의 정책과 문제점으로 인해 플랫폼을 떠나는 이유를 설명
          + ""The ongoing content moderation issues behind Substack’s meltdown"" - Mashable
            Substack의 콘텐츠 관리 부실 및 이에 따른 창작자 대거 이탈 현상을 분석
     * 2024-11 : ""Don’t call it a Substack"" - Anil Dash
          + Substack이라는 이름이 가진 부정적 의미, 플랫폼에 남아있는 문제를 지적하며 대안 논의를 제시
     * 2024-12
          + ""Substack is at it again"" - The Handbasket
            Substack이 또다시 논란을 일으킨 최근 이슈와 이에 대한 비판
          + ""Substackers Against Nazis"" - An Open Letter to Substack in 2021
            창작자와 독자들이 Substack에 공식적으로 문제 제기를 한 공개서한(2021년)
          + ""What Happens When You Leave Substack?""
            Substack을 떠난 후 겪게 되는 변화와 경험을 다룬 분석
     * 2025-04
          + ""Former Substack creators say they’re earning more on new platforms that offer larger shares of subscription revenue"" - Digiday
            Substack에서 떠난 창작자들이 더 높은 수익 배분을 제공하는 새로운 플랫폼에서 오히려 더 많은 수익을 올리고 있다는 실제 사례
          + ""Creators are ditching Substack over ideological shift in 2025"" - Digiday
            2025년 들어 Substack의 이념적 변화로 인한 대량 이탈 현상 보도
          + ""How Substack steals your audience and your revenue"" - Journalists Pay Themselves
            Substack의 구조가 구독자와 수익을 창작자에게서 빼앗는 방식에 대한 분석
          + ""A Newsletter Writer Reflects on Leaving Substack"" - Techdirt
            Substack을 떠난 뉴스레터 운영자가 직접 전하는 소감과 변화
     * 2025-07
          + ""Substack sent a push alert promoting a Nazi blog"" - Usermag
            Substack이 나치 및 백인우월주의 블로그를 푸시 알림으로 홍보한 사례
          + ""How a Nazi-Obsessed Amateur Historian Went From Obscurity to the Top of Substack"" - Mother Jones
            극단주의적 콘텐츠로 Substack 내에서 급부상한 운영자의 사례
     * 기타
          + ""Substack - Criticism"" on Wikipedia
            Substack에 대한 다양한 비판과 논란을 정리한 위키피디아 비평 섹션

Where Can I Go, and How Can I Migrate?

     * 대체 플랫폼 및 이전 방법을 소개
          + Beehiiv: 상업용 뉴스레터 플랫폼, 무료 요금제 제공, 공식 이전 가이드
          + Buttondown: 심플하고 자유도가 높은 상업용 대안, 공식 이전 가이드와 실제 이전 사례
          + Ghost: 비영리 기반, 오픈소스/셀프호스팅 가능, 호스팅 서비스도 지원, Fediverse 연동 등 특징
               o 공식 이전 가이드
               o Mailgun + Cloudflare 로 이전하기, 셀프호스트 Ghost로 이전하는 법, 이전 후기 Bluesky 쓰레드
     * 종합 비교: Latterly.org의 Substack 대안 플랫폼 비교 기사 등 참고 자료 안내

Who Else Has Migrated Away?

     * Substack을 떠난 대표적인 뉴스레터와 운영자 목록
          + Citation Needed
          + Platformer
          + Ty Burr
          + Rands
          + Garbage Day

For Readers: Asking Authors To Move

   (독자를 위한: 창작자에게 플랫폼 이전 요청하기)
     * 방문자 요청에 따라 추가된 섹션으로, Substack을 떠나도록 창작자에게 설득하는 다양한 메시지 예시와 접근 방법을 안내함
     * 플랫폼 이전이 창작자에게는 큰 결심이 필요한 일이므로, 공격적인 비난(call-out) 보다는 공감과 설득(call-in) 을 중심으로 부드럽고 배려하는 태도가 더 효과적임을 강조함

  메시지 예시

     * Substack의 문제(증오 발언, 허위 정보 수익화 등)를 간단하게 언급하고, 대안 플랫폼에서 다시 만나고 싶다는 긍정적인 메시지를 전하는 방식
     * 아래와 같은 다양한 영문 메시지 예시를 제공함 (필요에 따라 혼합·응용 가능)
          + ""Not sure if you know this, but unfortunately Substack willingly platforms, and allows bad actors to monetize, hate speech and misinformation. Would love to see you on a different platform! More information and alternatives are available at https://leavesubstack.com.""
          + ""Substack is bad for me and those I care about. I encourage you to check out https://leavesubstack.com for more information and information on alternatives.""
          + ""I’d love to subscribe to the newsletter, but I can’t give any money to Substack, which willingly platforms, and allows bad actors to monetize, hate speech and misinformation. Check out https://leavesubstack.com and let us know when you’re on a different platform so I can be sure to contribute!""
          + ""As a loyal reader it’s disappointing to me that you’re still hosted on Substack. I’d love for you to check out why, and some alternatives, on https://leavesubstack.com. Otherwise I may be forced to unsubscribe. Thanks for considering!""
     * 이러한 메시지들은 강한 압박보다는 창작자와의 신뢰와 관계를 존중하는 분위기를 유도하며, 플랫폼 이동을 통해 더 건강한 생태계를 만들고자 하는 목적임을 자연스럽게 전달함

   Substack의 브랜딩과 가짜 명성의 함정
   Substack, 또 다른 1억 달러를 조달했지만, 첫 1억 달러와 마찬가지로 허비될 것으로 예상됨

   요 몇일 Daring Fireball 의 존 그루버가 Substack 관련해서 얘기를 많이 했는데 그 완결본 같이 느껴지네요

   Substack의 브랜딩과 가짜 명성의 함정
"
"https://news.hada.io/topic?id=22227","영국 온라인 안전법을 위한 지역구 국회의원 신분증 사용 서비스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   영국 온라인 안전법을 위한 지역구 국회의원 신분증 사용 서비스

     * 이 서비스는 영국 지역구 국회의원(MP)의 신분증 정보를 검색해 시각적으로 확인할 수 있는 도구임
     * 온라인 안전법(Online Safety Act) 와 관련된 논의에서, 신분증 인증 절차의 구현 방식에 대한 문제 의식을 보여줌
     * 본 서비스는 유머러스한 접근으로, 실제 국회의원의 운전면허증을 예상 이미지로 제공함
     * 일반 국민이 아닌 공공인물의 신분증 정보를 가정해서, 온라인 신원 인증 제도의 실효성에 대한 의문을 제기함
     * 실제 개인정보 제공이 아닌 공개 정보 기반의 예시임을 명시하여, 개인정보 오남용 우려를 최소화함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 이 웹사이트는 영국 내 사용자가 자신의 지역구 국회의원(MP) 신분증을 검색하고, 해당 MP의 가상의 운전면허증 이미지를 볼 수 있게 제작됨
     * 사이트의 목적은 영국 온라인 안전법(Online Safety Act) 등에서 요구하는 신분증 인증 과정이 실질적으로 어떻게 작동되는지 유머러스하게 보여줌에 있음
     * 사용자는 자신의 지역구를 입력해 해당 MP의 이름, 소속 정당 등 공공 정보가 입력된 신분증 이미지를 확인할 수 있음

의도 및 주요 메시지

     * 실제 국민의 신분증 대신 공공 인물(MP)의 정보를 사용할 수 있다는 점을 강조하며, 온라인 신원 인증 절차의 허점 또는 아이러니함을 보여주는 의도임
     * 온라인 안전법에서 논란이 많은 신원 인증 요구를 비판적으로 바라보는 시각이 담겨 있음
     * 본 예시는 실제 신원 인증에 사용될 수 없는 가상의 이미지이며, 개인정보가 포함되어 있지 않음

기타 안내

     * 사이트 상단에 간단 FAQ 메뉴와 후원(커피 사주기) 배너가 배치되어 있음
     * 이용자는 별도의 정보 제출 없이 공개 정보 기반의 신분증 이미지를 빠르게 살펴볼 수 있음

        Hacker News 의견

     * 나는 영국 정부 감시 독립 사이트에서 제3자 URL 포맷(""Write to Them"", ""They Work for You"" 등)이 처음 등장했을 때 게릴라 디지털 행동주의에 관여했던 경험이 있음, Tim이 주제에 맞게 브랜드를 확장한 것에 박수를 보냄
     * 이번 법안에서 부담을 안은 의원들이 누구인지 한 번쯤 생각해보길 바람, 거의 대부분이 한 가지 계파임을 상기하며 다음 방문 때 꼭 언급해줬으면 좋겠음 https://votes.parliament.uk/votes/commons/division/1926
          + 이 최근 투표가 정확히 무엇에 관한 것인지 잘 모르겠음, 원래 Online Safety Act는 2023년에 Tory가 도입하고 통과시켰지만 지금에서야 시행되고 있음, 그런데 그 법을 만든 Tory가 이제 와서 반대 투표를 한다는 게 이해가 안 됨, 정말 어처구니 없음
          + 도무지 엉망진창임, Labour가 보수당 정책에 찬성표를 던졌고, 보수당은 반대표, 뉴스에 자주 등장했던 Reform당은 반대한다고 떠들면서 실제로는 찬성표를 던짐
     * 중국 네티즌들은 시진핑의 주민등록번호를 아주 잘 알고 있음, 바로 웹사이트 실명 인증 때문임, 중국 모든 웹사이트에서 실명 인증이 필수라 사람들이 그냥 시진핑 번호를 사용함
          + 중국 모든 웹사이트에서 진짜 실명 인증이 강제되는 게 사실인지 궁금함, 검색엔진, 뉴스 사이트, 실명성이 없는 포럼 등도 포함되는지 궁금함
     * 영국 정부를 제대로 약 올리고 싶다면 사이트에 댓글 섹션을 추가하는 것 추천함
     * 의도하지 않은 효과로, 이제 영국 의원들이 훨씬 그럴싸한 변명으로 얼마든지 성인물을 볼 수 있게 됨
          + 이 특정 법에 대해서는 잘 모르겠지만, 인터넷이나 암호화 관련 모든 법들이 늘 정부 관계자만 예외로 두려 했던 점을 지적하고 싶음, 그 예외조항 덕분에 법안 취지가 완전히 무력화되고, 정부 인사가 보안상 제일 취약한 표적이 되어 버린다는 문제를 지적함, 규칙을 만든 사람이 자기 자신만 예외로 하는 것 자체가 문제라고 생각함
          + 물론 실제로 의회에서 직접 보는 모습을 찍힐 수 있으니, 항상 안전하지는 않음 https://independent.co.uk/news/uk/…
     * 이 프로젝트의 취지는 좋지만, 정체 도용이나 비슷한 법률 위반 문제가 생길 수 있으니 조심해야 한다고 봄, 본인은 영국에 있지 않아 구체적 법 내용은 모르지만, FAQ에 패러디라고 적혀 있음에도 실제 사이트에서 생성된 ID가 discord, reddit 등에서 실제 인증 용도로 사용될 수 있다고 여기서 확인된 바 있음, 도메인이 영국에 등록되어 있으니 주의 요청함
          + 게임 캐릭터로 discord 인증 시스템을 속일 수 있다면 정말로 제대로 된 확인을 하지 않는 거라는 생각임
          + 영국 경찰은 ""장난이었다""는 변명은 안 받아줄 것 같음, 여러 법(신분 도용, 가짜 신분증 사용, 컴퓨터 악용)이 복합적으로 엮인 문제임
          + 실제로 발각될 가능성은 국회의원이 대중에게 거짓말을 하고, 본인 신분증이 실제로 보관되는 상황에만 일어날 것이라고 봄
     * 프로젝트가 재미있고 흥미로운데, 오래 운영하진 않았으면 좋겠음, 의원들이 소송을 걸 수 있기 때문임, 만약 신원 도용처럼 보인다면 실제로 고발 위험이 있음, 또한 총 650개 선거구만 있기 때문에, 새 우편번호를 입력할 때 AI 대기 시간을 줄이려면 미리 전체 목록을 등록해 두는 것이 좋다고 제안함
          + 생성된 이미지는 AI로 만들어졌다는 게 매우 명확히 드러나서, 실제로 속는 사람은 없을 것 같음, 우편번호 검색은 첫 검색만 느렸고 그 다음부터는 매우 빠르게 결과가 나오는 것 같음
          + 의원들은 이 일로 조언을 해준 공무원을 곧장 비난하기 바쁠 것이라고 생각함, 소송은 별로 기대하지 않으며, 서로 책임을 미루는 모습이 예상됨
          + 매번 다른 사진을 쓰는 것이 필터 리스트를 쉽게 막기 위함이라는 설명임
     * 이런 종류의 프로젝트 때문에 처음에 기술 분야에 뛰어들었음, 기존의 경직된 권력을 유쾌하고 효과적으로 비틀었던 유머러스한 저항에서 매력을 느꼈던 경험임, 앞으로도 이런 프로젝트가 더 많아지길 희망함
     * Walthamstow 선거구에서 시도해보니, ID에서 날짜 구분자가 점(.) 대신 쉼표(,)로 들어가는 이색적인 결과를 봤음, 하지만 아마 자동 인증 프로세스에서는 잘 포착되지 않을 것 같음
     * 코드가 공개될 계획이 있었던 것처럼 보임, FAQ에 따르면 React 프론트엔드, Node.js 백엔드를 사용했으며, 의원 데이터는 정부 공개 API에서 받아오고, AI로 만든 이미지는 open AI 최신 모델로 생성하여 Cloudflare R2 버킷에 저장한다고 나와 있음, 오픈소스라고 했지만 실제 git repo는 404 에러가 뜸 https://github.com/timje/use-my-mps-id
"
"https://news.hada.io/topic?id=22263","Brut - Ruby를 위한 새로운 웹 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Brut - Ruby를 위한 새로운 웹 프레임워크

     * Ruby 생태계를 위한 “낮은 추상화, 낮은 의식주의(low-ceremony)” 기반의 심플하고 강력한 웹 프레임워크로, 컨트롤러, REST 리소스 개념 없이 페이지·폼·단일 액션 중심 구조를 제공
     * 서버 렌더링 HTML, 직접 JS/CSS 작성, OpenTelemetry 연동, Sequel·OptionParser 등 현대 Ruby 도구 내장, 설치와 개발이 수분 내로 가능
     * 클래스 기반 구조, 실제 타입 기반 세션·폼·파라미터 관리, 동적 메소드·해쉬 사용 최소화로 명확한 코드·자동 문서화를 지향
     * 최신 웹 플랫폼 활용: 서버·클라이언트 검증 통합, BrutJS(Web Components), esbuild로 CSS 번들·해시, 보안 정책과 DB 설계 기본기 강화
     * YAML 최소화, Ruby/Hash 기반 설정과 명확한 네이밍·역할 분리, 불필요한 추상화나 패턴 강요 없이 Ruby와 브라우저 기술을 즐겁게 활용하는 것이 목표
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Brut 프레임워크 주요 특징

  완전히 새로운 구조 – 컨트롤러 없이 페이지 중심

     * 컨트롤러, REST 리소스, 라우트 액션 대신 클래스 기반 페이지, 폼, 단일 액션 핸들러로 구성
     * HTML은 서버에서 직접 생성, JS/CSS는 원하는 대로 작성 가능
     * 예시: 시간 표시 페이지는 TimePage < AppPage로 바로 정의

  현대적인 웹 플랫폼 적극 활용

     * 서버·클라이언트 검증 통합된 폼 UX
     * BrutJS(웹 컴포넌트 라이브러리)로 HTML 점진적 확장 지원
     * esbuild로 CSS 번들·해시 적용, PostCSS나 SASS 없이 간결하게 처리

  개발 생산성·기본기 내장

     * OpenTelemetry 기반 인스트루먼트, Sequel 기반 데이터 액세스, OptionParser 기반 명령행 자동화
     * 기본 보안정책, DB 컬럼/외래키 기본값/인덱스, 항상 timezone-aware한 시간, 간편한 로케일(Localization)
     * RSpec 테스트, Faker/FactoryBot으로 데이터 생성, Phlex로 HTML 생성

  YAML 배제, 명확한 설정 구조

     * I18n은 Ruby Hash 기반, 동적 설정은 dotenv로 관리 (YAML 거의 사용하지 않음)
     * docker-compose만 예외적으로 YAML 사용

  불필요한 추상화나 패턴 강요 없음

     * 네이밍·구조 명확(예: WidgetsPage는 /widgets로 접근)
     * 데이터 계층(DB::Widget)과 도메인 계층(Widget) 완전히 분리 가능
     * 컨트롤러 개념 없음, 원하는 방식대로 비즈니스 로직 구현 가능
     * 복잡한 함수형 패턴, 모나드, Proc 등 강요 X

  개발 경험·유연성

     * RSpec 등 현대 Ruby 생태계 도구 기본 내장
     * Phlex(HTML), Faker/FactoryBot, Sequel, OpenTelemetry 등 주요 라이브러리 적극 활용
     * 반복적 보일러플레이트/설정/의사결정에서 벗어나, Ruby와 브라우저 기술의 “재미”에 집중

왜 Brut인가?

     * Rails 등 기존 프레임워크의 반복적 셋업, 끊임없는 설계·구조 논쟁, 불필요한 유연성 대신 코드 자체의 즐거움을 추구
     * 빠르게 구축, 개발과 배포를 즐길 수 있는 Ruby 기반 웹앱 프레임워크로서 생산성과 명확함에 집중

시작 및 향후 로드맵

     * Brut 공식 Docker 이미지, 명령어 기반 빠른 앱 생성·실행 지원
     * ADRs.cloud 등 실전 예시 앱 제공
     * 1.0 버전 및 공식 로드맵 지속 업데이트 예정

   https://naildrivin5.com/blog/2025/…

   요 링크로 수정해야할 것 같아요

   링크가 잘못 걸려있는것 같슴다.

        Hacker News 의견

     * forms와 pages에 집중하는 방식이 마음에 듦, 나 역시 내 앱에서 이런 접근법을 사용함, 모든 상호작용은 Forms와 Links가 주도함, JavaScript로 하는 모든 향상도 사실상 기존에 존재하는(숨겨진 것까지 포함하여) form을 클릭하는 수준임, HTML을 항상 직접 확인해서 어떤 경로(route)가 실제로 상호작용을 처리하는지 명확하게 알 수 있음, controllers는 과도하게 사용된다고 봄, 실제로는 forms와 models(백엔드), 그리고 views(Pages)만 있어도 충분함, 많은 validation이나 controller에서 반복되는 절차들도 framework가 더 세련되게 처리할 수 있음
          + 폼을 클릭 이벤트로 시뮬레이션하는 이유가 궁금함, submit() 을 직접 호출하는 것과 무슨 차이가 있는지 궁금함
     * monad, algebraic data types, currying, 혹은 모든 것을 Proc로 처리하는 등 복잡한 개념들을 꼭 이해할 필요는 없다는 설명이 마음에 듦, 비즈니스 로직에서 functor를 사용하고 싶다면 얼마든지 선택할 수 있음, 이런 유연함이 마음에 들어서 소수의 숙련된 동료와 함께하는 사이드 프로젝트에 사용해보고 싶은 느낌임
     * 이 프레임워크는 Sinatra의 간결함과 Rails의 풍부함 사이의 훌륭한 중간지점이 될 것 같음, 나는 간단한 앱엔 Sinatra를, 필요에 따라 Rails를 사용해왔지만 복잡하지 않은 앱에는 Rails가 오히려 부담스러웠음, 새롭게 시도해보고 싶은 기대감이 있음
          + 참고할 만한 선택지로 Roda도 있음, 소규모 프로젝트에서 최적이었고 확장성도 좋았음, plugin 시스템 덕분에 기능 확장이 쉬웠고 routing tree도 다루기 쉬웠음
          + 내게는 hanami가 최고의 중간지점이었지만, 앞으로 bruts의 발전을 예의주시할 예정임
     * David의 책 Sustainable Rails를 정말 좋아하고, 내가 Rails 관련해서 항상 추천하는 서적임, 이번 프레임워크의 접근방식이 인상 깊음, 이런 작은 대안적 프레임워크 중 하나가 Sorbet를 완벽히 도입해서 form validation 등에 활용한다면 정말 흥미로울 것 같음, 물론 gradual typing 접근과는 상반될 수 있긴 하겠지만 실험적으로라도 재미있을 것 같음
          + David는 아주 재능 있는 개발자일 뿐 아니라 강연도 훌륭하게 함, 책의 주제와 맞닿은 영상 강연도 추천함: https://www.youtube.com/watch?v=CRboMkFdZfg
          + Sorbet도 좋지만 구조 강제 및 데이터 검증(data validation)에는 개인적으로 dry-rb 생태계가 더 선호됨
     * 이 프레임워크가 특이한 라이선스를 사용하는 건 알겠음, 그런데 이게 혹시 소스코드를 볼 수 없다는 의미인지 궁금함
          + github에서 소스를 확인할 수 있음: https://github.com/thirdtank/brut
          + 라이선스가 조금 이상하게 느껴지는 점에는 공감함
     * 이 프로젝트를 보니 예전 Camping 프레임워크가 떠오름, 다만 동적 부모 클래스 생성자가 없는 점이 다름
     * 브루트(Brut)의 공식 사이트(https://brutrb.com/overview.html)도 흥미로운 내용이 많았음
     * 샘플 사이트에서 로그인을 시도했지만 github 인증 이후 바로 오류를 만남
          + 작성자임, 현재는 남용 방지를 위해 샘플 사이트에서 로그인 자체를 막아둠, 오늘 갑자기 공개 결정을 해서 누구나 로그인을 허용할 준비가 아직 안 됨, 로컬에서 실행하거나 https://brutrb.com/adrs.html에서 페이지 탐색 및 링크 클릭이 가능함
     * Ruby는 아직 사용하지 않지만 프로젝트에 관심이 생겨서 별표(Star)를 눌렀음, 이 프로젝트가 Ruby를 실제로 써보는 계기가 될지도 모름
     * 특히 Phlex 선택이 마음에 들었음, 개인적으로 이 프레임워크가 Datastar js 라이브러리 및 그 라이브러리의 SSE(서버 전송 이벤트) 활용과도 잘 맞을지 궁금함, 고마움
"
"https://news.hada.io/topic?id=22281","Show GN: Share To Tesla | Tesla 차량 네비로 목적지 전송하는 앱","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: Share To Tesla | Tesla 차량 네비로 목적지 전송하는 앱

   와이프에게 행선지를 물어보면 항상 카톡으로 위치를 받는데요.
   차량 출발전에 테슬라 차량 네이게이션에 주소를 타이핑해야 하는 불편함이 있었어요.

   그리고 초행길은 TMAP 을 이용하게 되는데,
   Tesla 차량에도 한번더 입력해야하는 불편함이 있었어요.

   카카오톡으로 받은 링크, TMAP 의 도착 공유, 각 지도 맵에서 공유기능으로 Tesla 차량에 손쉽게 보낼 수 있습니다.

   구현 기술
     * 95% 이상 바이브 코딩으로 구현했습니다. with Cursor
     * Flutter 그리고 Share Extention 은 Swift 로 구현~~해줬~~했어요.

   주요 기능
     * 별도 회원가입 없이 기존 테슬라 계정을 로그인하시면 사용 가능합니다.
     * 지도APP, 카톡에 공유된 위치 링크등을 Tesla 차량으로 전송이 가능합니다.
     * 공유 가능한 APP: T MAP, 카카오MAP, 네이버MAP, 구글MAP

   시연 영상
     * https://youtube.com/shorts/dWQsiSPWY98
"
"https://news.hada.io/topic?id=22228","LLM 임베딩 완전 해부: 시각적이고 직관적인 가이드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     LLM 임베딩 완전 해부: 시각적이고 직관적인 가이드

     * 임베딩은 LLM의 의미적 뼈대로, 텍스트를 수치 벡터로 변환해 모델이 이해할 수 있도록 만듦
     * 임베딩의 발전은 카운트 기반, TF-IDF 등 통계적 기법부터 Word2Vec, BERT, GPT 계열 등 현대적 방식까지 다양한 변화를 거쳐왔음
     * 좋은 임베딩은 단어의 의미적 유사성 보존과 차원 수 조절의 균형이 중요하며, LLM에서는 학습 데이터와 과제에 최적화되어 함께 훈련됨
     * TF-IDF, Word2Vec, BERT 등 대표 임베딩 방식을 사례와 시각화로 설명하며, 특히 LLM의 임베딩은 입력 토큰을 고차원 벡터로 변환 후, 문맥에 따라 계층적으로 업데이트됨
     * 토치 임베딩 레이어, 임베딩 벡터 그래프 분석 등 실습 코드와 시각 자료를 통해 임베딩의 실제 동작 원리를 직관적으로 이해할 수 있도록 안내함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

임베딩이란 무엇인가

     * 임베딩(embedding)은 텍스트, 이미지, 오디오 등 데이터를 고차원 벡터로 변환하는 기법임
     * 특히 NLP에서는 단어나 토큰을 벡터로 변환해 컴퓨터가 이해할 수 있는 형태로 만드는 과정임
     * 임베딩은 텍스트뿐 아니라 다양한 데이터 타입에 적용할 수 있으나, 본 글에서는 텍스트 임베딩을 중심으로 설명함

임베딩의 발전과 종류

     * 초기 임베딩 기법은 카운트 벡터, TF-IDF, Co-Occurrence Matrix 등 통계 기반 방식이 주류였음
     * 이후 Word2Vec, GloVe, FastText와 같이 신경망 기반 임베딩이 등장, 의미적 유사성을 벡터 공간에 반영하기 시작함
     * BERT, GPT 등 트랜스포머 기반 모델에서는 각 토큰의 임베딩이 입력 후 계층적으로 문맥 정보를 반영해 업데이트됨(동적/문맥화 임베딩)

좋은 임베딩의 조건

  의미적 표현(semantic representation)

     * 유사한 의미의 단어가 벡터 공간에서도 가까운 위치에 위치함
       예: ""cat""과 ""dog""는 ""dog""와 ""strawberry""보다 더 유사하게 매핑됨

  차원의 크기(dimensionality)

     * 임베딩 차원이 너무 작으면 표현력이 부족, 너무 크면 메모리 낭비와 과적합 위험
       예: GPT-2는 임베딩 차원이 최소 768임

전통적 임베딩 기법: TF-IDF

     * TF-IDF는 단어 빈도와 희소성(역문서 빈도)를 곱해 단어의 중요도를 산출
     * TF: 한 문서에서 단어가 얼마나 자주 등장하는지
     * IDF: 전체 문서 중 얼마나 희귀한 단어인지
     * TF-IDF의 결과값은 정보 검색, 키워드 추출 등 단순 분석에 활용되나, 의미적 유사성은 잘 반영하지 못함
     * 시각화하면 대부분의 단어가 한 군집에 몰리며, 의미적 분리도가 낮음

Word2Vec

     * Word2Vec은 신경망 기반 임베딩으로 의미적 관계를 벡터 공간에 효과적으로 반영
     * CBOW(주변 단어로 중심 단어 예측), Skipgram(중심 단어로 주변 예측) 등 방식 존재
     * 학습을 통해 숨겨진 계층의 임베딩 행렬이 단어의 의미적 벡터로 활용
     * 네거티브 샘플링 등 최적화 기법을 사용해 대규모 데이터에서도 효율적 학습 가능
     * TensorFlow Embedding Projector 등으로 시각화하면 의미적으로 비슷한 단어가 클러스터링되는 것을 볼 수 있음

BERT와 트랜스포머 기반 임베딩

     * BERT는 Encoder-only 트랜스포머 모델로, 각 계층을 거칠수록 문맥 정보를 동적으로 반영
     * 입력 단계: 토크나이저로 텍스트를 토큰화 → 토큰 임베딩 벡터로 변환 → 포지셔널 임베딩과 합침
     * 이후 여러 트랜스포머 계층을 통과하며 문맥 정보를 반영해 임베딩이 동적으로 변함
     * [CLS], [SEP] 등의 스페셜 토큰으로 문장 전체/구분 정보도 처리함
     * BERT 이후 많은 LLM이 동적(문맥화) 임베딩 방식을 채택

LLM 임베딩의 구조와 학습

     * LLM 임베딩은 입력 토큰을 고차원 벡터로 변환하는 첫 번째 계층(lookup table)으로 구현
     * torch.nn.Embedding과 같이 토큰 ID를 받아 각 임베딩 벡터를 반환하는 테이블 형태로 구성됨
     * LLM은 학습 과정에서 임베딩 레이어 가중치까지 함께 최적화하며, 모델이 처리하는 데이터와 목적에 맞게 임베딩이 정교화됨
     * 예시: DeepSeek-R1-Distill-Qwen-1.5B 모델은 1536차원 임베딩 벡터를 사용

임베딩의 시각화와 그래프 분석

     * 임베딩 벡터 간의 유사도(코사인 유사도 등)를 기반으로, 임베딩 공간을 그래프 형태로 분석
     * 예: ""AI agents will be the most hot topic..."" 문장을 토크나이즈하고 각 토큰의 임베딩에서 유사도가 높은 토큰을 연결해 시각화 가능
     * 단일 단어(예: ""list"")의 다양한 변형(""_list"", ""List"" 등) 역시 유사한 임베딩을 갖는 경우가 많음

결론

     * 임베딩은 LLM, NLP의 핵심 요소로, 입력 데이터를 수치 벡터로 바꿔 의미적 구조와 문맥 정보를 모델이 처리할 수 있게 함
     * LLM 시대에도 임베딩의 기본 원리와 구조는 크게 변하지 않고, 여전히 모델 성능과 해석에 매우 중요한 역할을 담당
     * 직관적인 코드 예시와 시각화 자료를 통해 임베딩의 실질 동작 원리를 쉽게 이해할 수 있음

   king - man + woman = queen

        Hacker News 의견

     * 임베딩이 LLM 스택에서 거의 논의되지 않는 부분 중 하나인 점이 정말 놀라움, 직관적으로 네트워크가 의미적 연결을 추론할 수 있는 능력에 엄청난 영향을 미치는 부분이라고 생각했지만, 사람들은 그에 대해 별로 이야기하지 않음
          + 임베딩의 문제점은 모델 자체 말고는 거의 판독이 불가능함에 있음, 임베딩이 입력 시퀀스의 의미를 분명히 인코딩하지만, 학습 과정에서 그 정보가 너무 압축되어서 모델의 디코더 헤드만 해독이 가능해짐, Anthropic이 Sonnet 3의 내부 피처를 해석 가능한 형태로 만든 연구가 있는데, 이건 내부 레이어의 활성화를 해석하려고 별도의 네트워크를 병렬로 학습시켜야 해서 비용이 큼
          + 고차원 공간의 특이한 점은 대부분의 값이 서로 직교하고 아주 멀리 떨어져 있음, 그런데 차원 축소 기법으로 5만 차원에서도 개념들을 클러스터링할 수 있는 게 신기함
          + 처음 임베딩에 대해 깊게 배우면서 “LLMs의 마법 중 최소 3분의 1은 임베딩에서 나오는 것”이라는 생각을 함, 단어들이 이미 의미적으로 유용하게 배열되어 있다는 사실 자체가 LLMs의 신비를 약간 벗겨줌, 여전히 경이롭긴 하지만 커튼 뒤가 조금 보이는 느낌임
          + 임베딩이 컴퓨터가 인간 언어를 정량화할 수 있도록 해주는 일종의 “로제타 스톤” 같은 역할임, 이게 정말 중요한 이슈여야 한다고 생각하지만, 1,000차원 벡터 공간을 이해하려는 시도가 꽤 부담스럽다는 점도 인지함
          + 임베딩에 대한 논의가 부족하다는 의견이 잘 이해되지 않음, 특히 RAG 적용 예시나 벡터 DB 얘기에서 임베딩은 항상 핵심 주제로 다뤄짐
     * 튜토리얼이 매우 좋았음 — 컨텍스트 임베딩과 정적 임베딩의 차이가 중요함, 많은 사람이 word2vec(정적 임베딩)에 익숙하지만 컨텍스트 임베딩이 훨씬 더 강력한 경우가 많음, (참고로 페이지에서 스크롤만 해도 히스토리에 엄청난 기록이 추가되는 브라우저 히스토리 하이재킹이 심함)
          + 문제의 원인은 해당 자바스크립트 코드에 있음, 누군가가 이걸 기능이라 착각한 듯함
     * 더 고급 설명이 필요하다고 생각하는 부분은, 임베딩 관점에서 Encoder-Decoder 트랜스포머(BERT)와 Decoder-only 제너레이티브 모델의 차이임
          + 작은 수정이 필요함: BERT는 인코더임(Encoder-Decoder 아님), ChatGPT는 디코더임, 인코더 모델(BERT)은 전체 문장을 다 볼 수 있어서 의미 표현에 유리함, 예컨대 “The bank was steep and muddy”에서 bank의 의미(강변인지 금융기관인지)를 문장 전체를 보고 판별할 수 있음, 반면 GPT류(디코더 모델)는 왼쪽에서 오른쪽 순서만 보기 때문에 문장 뒤 정보를 못 보고 예측함, 이에 대한 추가 자료는 huggingface의 modernBERT 포스트 참고 및 neoBERT 논문도 유익함
          + dust42의 설명을 덧붙이면, BERT는 인코더, GPT는 디코더, T5는 인코더-디코더임, 요즘 인코더-디코더는 덜 인기임, 인코더 모델은 분류, 정보 추출, 검색에 많이 쓰이고, 디코더는 텍스트 생성, 요약, 번역에 적합함, 최근 연구(Ettin 논문)에서도 이것이 확인됨, 둘 다 트랜스포머이므로 인코더를 디코더로 바꿀 수도 있고 그 반대도 가능함, 설계 차이는 양방향어텐션(모든 토큰이 모든 토큰을 볼 수 있음)과 오토리그레시브 어텐션(이전 토큰만 볼 수 있음)에 있음
     * 임베딩 프로젝터를 활용하면 50개 이상의 단어뿐 아니라 3D 데이터 시각화에 대한 감각도 기를 수 있음, Tensorflow Embedding Projector 참고 바람
     * 매우 훌륭한 비주얼 가이드임! 나도 비슷한 컨셉으로 딥러닝에 대한 깊은 이해를 돕는 비주얼+오디오+퀴즈 방식 LLM 임베딩 레슨을 app.vidyaarthi.ai에서 제작함, 직접 체험 링크, 추상적 개념을 더 직관적이고 상호작용적으로 만드는 “직접 해보면서 배우는” 접근법을 추구함, 피드백 환영임(자기 홍보는 아님, 정말 열정적으로 만든 도구임)
     * 접속이 잘 안되는 듯함, “Content-Security-Policy: The page’s settings blocked an inline style…”처럼 콘솔 오류 메시지가 잔뜩 뜸
     * 단어 클라우드 시각화나 쿼리 top-k 결과 보여주기가 더 직관적일 수 있다고 생각함, Tensorflow의 Embedding Projector에서 단어 입력 후 UMAP projection 선택해보길 추천함
     * 좀 더 실용적인 접근의 설명도 sgnt.ai의 임베딩 설명글에서 다룸, 직접 작성했음
          + 멋진 자료 감사함! 내가 이해한 바로는 LLM에 세 가지 큰 문제가 있음:
              1. LLM은 매우 고차원 벡터 공간을 저차원으로 줄임, 그런데 저차원 공간의 개별 축이 의미하는 바가 불분명하여, 정답 여부를 대부분 아웃풋에서만 확인 가능함, 이걸 해결하려는 연구가 궁금함
              2. LLM들은 이 축소 표상을 위해 텍스트 데이터를 사용함. 즉, 현실이 아니라, 현실에 대한 인간의 기록을 학습함, Keen Technologies는 이런 한계를 피하기 위해 실제 센서 달린 로봇 데이터를 사용하는데, 훨씬 느리지만 장기적으로 더 정확한 모델을 만들 수 있다고 봄
              3. LLM이 대화의 의미와 맥락 정보를 벡터 (내부 상태) 하나에 담기 때문에, 긴 대화에서는 반복적으로 그 벡터가 덮어써져서 초반 맥락이 점점 희미해짐, 이런 상태 관리를 벡터 백업 말고 다르게 할 방법이 있는지 궁금함
          + 당신의 접근이 훨씬 직관적임, 왜 범주형/스칼라 특성의 임베딩 예시를 안 보였는지 궁금했음
     * 아주 교육적이고 구조가 잘 잡힌 아티클임, 저자에게 감사함
          + 저자 프로필은 Huggingface에서 볼 수 있음, HN 운영진이 예전엔 반응이 별로라 재투고 추천, 이 글이 좋았던 이유는 다양한 임베딩 유형을 예시로 다루기 때문임
     * LLM의 임베딩은 보통 입력 레이어 일부로, Word2Vec 등 사전학습 모델과 달리 학습 중 업데이트됨,
       추가로 궁금한 점: 추론 단계에서 임베딩은 “토큰 ID -> 벡터” 형태의 룩업테이블인데,
       수학적으로 보면 토큰ID를 긴 원-핫벡터로 인코딩한 뒤, 선형 레이어를 통과시켜 임베딩 벡터를 얻는 식임,
       이 구조가 임베딩 학습에도 그대로 적용되는 건지, 즉 임베딩을 선형 레이어로 간주해서 역전파로 학습시키는 방식인지 궁금함
          + 임베딩도 일반 역전파 과정에 포함됨, 다만 효율성 때문에 원-핫 인코딩은 실제로 안 씀,
            선택된 벡터에만 그래디언트가 흐르도록 인덱싱을 미분 가능하게 구현함,
            구체 예시는 내가 만든 미니 딥러닝 라이브러리 SmallPebble 소스 참고하면 도움됨
          + 다른 코멘트를 좀 더 설명하자면,
            인덱싱과 원-핫 벡터 곱은 수학적으로 동일함,
            예를 들어 vocab이 N개, 시퀀스 길이 L이라면, NxL 희소 행렬을 임베딩 행렬에 곱해야 하는데,
            실제론 각 열 당 인덱스 하나만 알면 되므로, 내부적으로는 숫자 하나(인덱스)로 표현함,
            이 방식으로 순전파와 함께 역전파 역시 인덱싱 기준으로 별도로 작성해,
            선택된 임베딩 벡터로 그래디언트가 흐르도록 처리함
"
"https://news.hada.io/topic?id=22269","Show GN: SecondB.ai - Youtube 영상의 핵심은 물론, 맥락까지 놓치지 않고 요약하는 서비스.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Show GN: SecondB.ai - Youtube 영상의 핵심은 물론, 맥락까지 놓치지 않고 요약하는 서비스.

   안녕하세요!

   Youtube 영상 요약 서비스 https://secondb.ai 를 공유합니다.

   기존에 Youtube 영상 요약 서비스가 이미 많이 있지만,
   ""내가 궁금했던 내용은 빠져 있거나, 요약이 너무 뭉뚱그려져 있다""
   는 아쉬움을 느껴본 적 있으신가요?

   저도 그런 답답함을 겪다가,
   열어놓은 수십 개의 유튜브 탭들을 정리하고 효율적으로 소화하고 싶어서
   ‘맥락과 핵심을 최대한 살린 텍스트 요약’ 서비스를 직접 만들게 되었습니다.

   그동안 로컬에서 혼자 사용해왔는데,
   최근 오픈해서 https://secondb.ai 에서 누구나 사용하실 수 있습니다.

   아직 부족한 점이 많지만,
   가능한 한 계속 무료로 운영하면서, 적은 비용으로 최대 효과를 만들 수 있도록 계속해서 개선 중입니다.

   📣 피드백이 정말 큰 힘이 됩니다!
   열려 있는 유튜브 탭이 많으시다면 한 번 사용해보시고,
   불편한 점이나 바라는 기능들을 알려주세요.

   감사합니다! 🙏

   샘플 링크를 몇개 공유합니다.
     * https://secondb.ai/summary/70/ - A Comparative Analysis of Kueue, Volcano, and YuniKorn - Wei Huang, Apple & Shiming Zhang, DaoCloud
     * https://secondb.ai/summary/272/ - Ex-Google CEO: What Artificial Superintelligence Will Actually Look Like w/ Eric Schmidt & Dave B
     * https://secondb.ai/summary/262/ - Spring I/O 2025 Keynote
     * https://secondb.ai/summary/177/ - RailsConf 2025 Keeping the Rails Magic Alive After 18 Years by Wade Winningham

   멋집니다!

   앗 감사합니다. 개인 프로젝트지만 그래도 사용자가 조금씩 늘어나니 기분이 좋네요. :)

   좋은 서비스 만들어주셔서 감사합니다. 재밌게 사용해보겠습니다!

   감사합니다. :)

   우선 힘든 과정 축하드려요.
   진심으로..
   https://lilys.ai/ 이 사이트를 참고하시면 더욱더 기능 추가에 도움이 되지 않을까 싶어요.
   창조는 모방의 어머니 이닌까요.

   앗 감사합니다. lilys.ai 서비스 너무 좋아해서, 창업자 분께 여러번 서비스 피드백, 펜레터 보내기도 했어요! :)
   lilys.ai 열심히 쓰면서, 이런 서비스는 어떻게 만드는걸까? 직접 만들면서 배워보자! 하고 시작한 프로젝트에요.!
"
"https://news.hada.io/topic?id=22322","Substack, 또 다른 1억 달러를 조달했지만, 첫 1억 달러와 마찬가지로 허비될 것으로 예상됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Substack, 또 다른 1억 달러를 조달했지만, 첫 1억 달러와 마찬가지로 허비될 것으로 예상됨

     * Substack이 최근 1억 달러의 추가 투자를 유치하며, 기업가치는 11억 달러로 평가됨
     * Substack의 수익 모델은 간단하지만 현실적으로는 아직 수익성 확보를 이루지 못한 상황임
     * 투자자들은 소셜 기능과 앱, 동영상 등 새로운 서비스에 성장 가능성을 보고 있음
     * 기존 서브스택은 작가와 구독자 중심이었으나, 최근에는 YouTube와 같은 플랫폼 경쟁을 목표로 방향 전환 중임
     * 실질적인 이익 증명 및 자생적 성장 대신, 반복적인 투자 유치에 의존하는 구조가 문제점으로 지적됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Substack의 최근 1억 달러 투자 유치

     * Substack은 최근 새로운 1억 달러 규모의 투자를 유치하였고, 이로 인해 기업가치는 11억 달러로 상승함
     * 이 투자 소식은 The New York Times를 통해 공식 보도되었으며, 해당 기업의 공동 창업자 3인이 짧은 인터뷰를 진행함

Substack의 비즈니스 모델과 성장 가능성

     * Substack의 비즈니스 모델은 사용자가 창작자를 팔로우하여 구독 수익을 발생시키고, 여기에서 Substack이 10%의 수수료를 가져가는 구조임
     * 이 방식은 초기에는 작가 친화적 플랫폼으로 성공을 거두었고, 유명 작가와 언론인이 대거 유입되었음
     * 그러나 최근 투자자들은 앱, 채팅, 라이브 동영상, Notes(소셜 피드 기능) 등 플랫폼의 확장된 서비스에도 주목하고 있음

수익성 및 투자 전략에 대한 비판

     * 만약 Substack의 현재 모델이 투자자 설명대로라면, 이미 수익성과 자체 성장 기반을 갖추었어야 한다는 비판이 나옴
     * 이미 많은 유료 구독자와 유명 작가가 있음에도, 왜 추가 투자가 필요했는지 의문이 제기됨
     * 비교 대상인 Twitter(X) 모델을 미래 성장 전략으로 제시하지만, 해당 모델이 수익성과 연결된다는 증거는 부족한 상태임

기업 가치 및 비즈니스 현실

     * 최근 Substack의 평가액이 2021년 대비 70% 상승했지만, 이는 투자자의 전략 신뢰보다는 투자금 회수에 대한 불확실성을 보여주는 것일 수 있음
     * 실질적으로 수익성과 성장성이 입증된다면, 별도의 투자 유치 없이 내재적 성장이 가능해야 함
     * 각 구독자당 Substack의 월 수익은 50~70센트 수준에 불과하며, 현재는 광고 사업도 전혀 마련되어 있지 않음

플랫폼 방향성과 업계 내 위치

     * Substack은 스스로를 작가 중심 플랫폼으로 내세웠으나, 최근에는 YouTube와 경쟁을 언급하며 플랫폼 방향성을 바꾸는 모습임
     * 창업자 중 한 명은 자신을 ""Chief Writing Officer""로 지칭하며 여전히 작가 중심 정체성을 강조하고 있음
     * 하지만 실제로는 Mailchimp와의 비교나 대형 플랫폼 지향이 등장하며, 작가와 독자 모두에게 기존 이미지와 달라진 방향을 인식하게 함

결론 및 개인 의견

     * The New York Times의 시장가치(약 85억 달러)와 비교하면, Substack이 그 가치의 1/8이 아닌 상황
          + Times는 1,100만 명의 디지털 구독자를 보유하고 월 25~35달러씩 받아, 광고까지 결합된 매우 높은 수익 구조를 갖추고 있음
     * 반면 Substack은 구독료가 낮고, 수수료도 소액이며, 광고 매출이 전혀 없어서 수익 측면에서 크게 밀리는 구조임
     * 결론적으로 10% 수수료 모델로 독립형 사업을 운영하는 것은 충분히 가능하겠으나, 스스로 자신만의 수익성 확보와 성장동력 확보가 과제임
          + 반복적인 투자 유치 없이 독자적 수익성 확보가 필요함
"
"https://news.hada.io/topic?id=22308","커피를 CPU로 펌핑하는 커피 메이커 컴퓨터, Coffeematic PC","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                커피를 CPU로 펌핑하는 커피 메이커 컴퓨터, Coffeematic PC

     * Coffeematic PC는 구형 커피 메이커와 컴퓨터를 결합한 독특한 예술 작품임
     * 실제로 커피를 끓여 이를 뜨거운 냉각수로 사용하여 CPU로 순환시킴
     * 커피로 냉각하면서도 컴퓨터와 커피 메이커 모두 정상 동작함
     * 2002년부터 이어진 커피 메이커 컴퓨터의 계보와 15년의 공백기가 존재함
     * 이 프로젝트는 데이터 시각화와 아트 전시회로 확장되어 창의적 해킹 문화를 조명함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

COFFEEMATIC PC

  예술가 Doug MacDowell의 커피 메이커 컴퓨터

     * 2024년 겨울, Doug MacDowell은 중고 매장에서 복고형 게임용 컴퓨터 케이스로 적합한 GE 커피 메이커(Coaffeematic, 1980년대 생산) 를 발견함
     * 이 커피 메이커는 커피 머신과 PC를 하나로 합친, Coffeematic PC로 재탄생됨
     * Coffeematic PC는 기존의 기능을 뛰어넘는 혼합 기계로 거듭남

  전통적 명분을 넘어선 유쾌한 창작

     * 이 작품은 흔히 고성능, 세련미, 확장성 같은 키워드와는 거리가 먼 기계로, 단순하면서도 약간 자기파괴적인 면이 있음
     * 본인은 기본적인 개조에서부터 유머러스하거나 예술적인 복잡성까지 다양한 커스텀 컴퓨터들을 언급함
     * Coffeematic PC는 그 스펙트럼의 중간쯤에 위치함

Coffeematic PC의 작동 방식

     * 컴퓨터와 커피 메이커 모두 정상 동작함
     * 커피 메이커는 평범하게 뜨거운 Java(커피)를 추출함
     * 일반 PC는 냉각을 위해 팬 또는 수냉장치를 사용하지만, Coffeematic PC는 끓인 커피를 CPU 냉각수로 이용함
     * 펌프가 약 90°C의 커피를 두 개의 라디에이터 및 ASUS M2NPV-VM 메인보드를 지나는 경로로 순환시킴
     * 커피는 다시 카라프로 돌아간 뒤 이 과정을 반복함

CPU와 Java의 온도와 안정성

     * CPU는 시원해야 하고 Java(커피)는 뜨거워야 하는 원리
     * 뜨거운 커피를 순환해도 시스템이 다운되지 않는 안정성을 보임
     * 5초 간격 데이터 수집 및 75분간 모니터링 결과, 아슬아슬하게 안정성을 유지하는 모습을 보임
     * 최종적으로 CPU와 시스템 온도는 약 33°C로, 순환하는 커피의 온도와 비슷한 평형 상태에 도달함

Coffeematic PC 제작 방법

     * 중고 부품과 일부 신제품(펌프, 라디에이터 등) 혼합 사용
     * 메인보드, CPU, RAM, 그래픽카드는 2000년대 중반 재활용품 활용
     * 커피 메이커 본체는 1970년대 후반 제품, 경미한 수리(비닐 튜브 교체) 필요함
     * 최신 SSD, 운영체제와 하드웨어는 2020년대 신제품임
     * 다양한 시대의 부품이 융합된 혼성 시스템임
     * 완성된 커피는 1970년대 플라스틱 커피 메이커에서 추출된 맛 그대로임

제작 과정 영상

     * Coffeematic PC 제작 과정을 담은 유튜브 영상 제공
       YouTube에서 시청하기

커피 메이커 컴퓨터의 계보

     * 22년에 걸쳐 5개의 커피 메이커 컴퓨터가 존재
     * 2002년 Nick Pelis의 The Caffeine Machine이 최초임
     * 이후 15년간 공백이 있다가, 2018년 Ali “THE CRE8OR” Abbas와 Zotac의 Zotac Mekspresso가 등장
     * 2019년 Logarythm의 Mr. Coffee PC가, 2024년에는 유튜브 채널 NerdForge의 PC that makes coffee와 본인의 Coffeematic PC가 제작됨
     * 각 기계의 계보와 주요 기술 이벤트를 함께 보여주는 그래프가 있음

15년의 공백에 대한 고찰

     * 첫 커피 메이커 PC 이후 15년간 제작이 중단됨
     * 커피 자체, 제작자, 비용, 기술, 문화 등 다양한 원인에 대한 의문 제기
     * 해당 기간의 기술, 경제, 사회적 사건들을 함께 분석함
     * 커피 메이커 컴퓨터 연표와 IT 역사상의 주요 사건을 함께 시각화하여 현상 분석 시도

예술 전시와 데이터 시각화

     * Coffeematic PC는 Sparklines라는 아트 전시로 확장됨
     * 이 전시에서는 커피 메이커 컴퓨터 제작의 공백기를 주제로 예술적 해커 집단의 데이터 초상화를 선보임
     * 수작업 드래프팅 툴과 빈티지 레터링 킷으로 모두 직접 그림
     * 링크를 통해 전시 콘텐츠 확인 가능함

마무리

     * 추가적으로 본인이 모르고 있는 또 다른 커피 메이커 컴퓨터 사례를 아는지 독자에게 질문함

        Hacker News 의견

     * 왜 반대로 해보지 않음: 물을 CPU 옆을 지나가게 한 후 커피 원두를 통과시키는 방식임. 제대로 커피를 추출하는 데 필요한 온도가 CPU에 권장되는 온도보다 훨씬 높다는 점이 아쉬움. 물을 80도까지 올리고 그 뒤에 보조 히터를 두는 방법도 생각해봄
          + 커피와 CPU 사이에 미니 히트펌프를 두고 싶어짐. 50도면 히트펌프로 반대편 온도를 효율적으로 98도 근처까지 올릴 수 있음
          + CPU 수냉 쿨링 전문가는 아니지만, 물을 빠르게 80도까지 올리려면 CPU 온도가 100도를 훌쩍 넘겨야 할 것 같음
          + 예전에 뜨거운 P4로 뱅쇼를 데운 적이 있음
            https://imgur.com/a/mulled-wine-pc-WW1pW
            60도까지 올라가서 커피에는 약간 낮지만 뱅쇼 데우기엔 완벽했던 실험 경험임
          + 보조 히터로 오래된 P4 Celeron을 사용하는 방법도 고려해봄
          + 80도가 커피 추출 온도로 쓸 수 있는 가장 낮은 수준이지만, 분쇄도를 더 곱게 하면 풍미를 보완할 수 있음
     * ""커피가 너무 식었다""면 걱정 말고 Electron 앱을 실행할 것
     * 누군가는 아직도 이런 필요한(?) 것들을 만든다는 것이 정말 기쁨
          + SGI가 한발 앞서 있었음 http://mycollins.net/sgicoffee.png
          + v0.2a에서는 커피 추출 중 채굴로 원두값을 충당하는 기능이 추가되면 재미있겠다는 상상을 해봄
     * AI가 사용자가 실제로 원하는 음료를 추론해서 거의 차 같지 않은 음료를 내놓는 프로젝트 발전상을 상상함. 많은 분들과 즐겁게 공유하고 싶음
     * GPU와 알코올 증류의 콤비를 떠올림. 직접 운영했던 슈퍼클러스터에서 항상 60kW 정도 전력 소모와 85도 GPU 온도를 경험함. 그 온도대에서 연료 증류가 가능하다고 알고 있어서 폐열을 발전적으로 활용하고 싶은 마음이 있었음
     * HTTP 418(I'm a teapot) 같은 걱정은 필요 없다
     * 이런 엉뚱하지만 멋진 프로젝트가 정말 최고라고 생각함. 커피가 진짜 (조금은) CPU 방열판 역할을 하는 게 흥미로움
          + 혹시나 미시적으로 커피가 CPU에도 카페인을 전해줄 수 있을지 상상해봄
     * 100% Java 호환성
          + 로고는 https://mybcagroup.blogspot.com/2016/07/… 를 사용하는 게 어울릴 것 같음
     * 잠시 끼어들고 싶음. 많은 사람들이 GE Coffeemattic PC라 부르는 것을, 실제로는 GNU/GE Coffeemattic PC, 또는 GNU+GE coffeemattic이라고 부르는 게 맞다는 의견임. GE Coffeemattic은 독립 PC가 아니라 GNU 시스템의 또 다른 무료 컴포넌트로, 핵심 라이브러리와 쉘 유틸리티, 필수 시스템 구성요소 등으로 완성된 PC 환경에 포함되는 구성 요소임. GNU 시스템의 변형을 모르고 매일 쓰는 사람이 많음. 오늘날도 ""뜨거운 java를 제공하는"" GNU 시스템은 종종 GE Coffeemattic PC라 불리지만, 사실상 GNU 시스템임. 진짜 GE Coffeemattic은 실제로 존재하고, 커널(즉, 시스템 커피 자원을 다른 사용자의 공간에 할당하는 프로그램) 역할임. 커널은 반드시 전체 운영체제와 결합되어야 하는 필수적 컴포넌트이고, GE Coffeemattic은 평소 GNU 시스템과 함께 사용됨. GE Coffeemattic PC 배포판 모두는 사실상 GNU/GE
       Coffeemattic PC임
     * 운영체제로 Linux Mint를 쓰는데, 차라리 Coffee Linux를 썼어야 하지 않겠냐는 아기자기한 제안함
"
"https://news.hada.io/topic?id=22310","UCLA 지원금 중단에 대한 Terence Tao의 입장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    UCLA 지원금 중단에 대한 Terence Tao의 입장

     * UCLA 지원금 중단 사안에 대해 Terence Tao가 비판적 의견을 밝힘
     * 이 문제의 본질이 이스라엘의 이익이 미국보다 우선시되는 현상임을 강조함
     * 인도주의 및 국제법 위반을 보호하려는 움직임이 있다고 지적함
     * 반학살 입장이 반유대주의로 왜곡되는 현상에 우려를 표함
     * 학계가 침묵할 경우 학문적 자유와 기관에 더 큰 위협이 발생할 수 있음을 경고함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Terence Tao의 메시지와 UCLA 지원금 중단 사안 개요

     * Terence Tao는 최근 UCLA의 지원금 중단과 관련한 사회적 이슈에 자신의 견해를 밝힘
     * 그는 만약 학계가 이 사안에 침묵한다면 시스템이 더욱 대담해지는 현상 발생임을 경고함

사안의 본질

     * 이번 사건의 근본적인 원인이 이스라엘의 이익이 미국의 국가적 이익보다 우선시되는 점에 있다고 주장함
     * 그는 이 현상이 이스라엘의 인도주의 범죄 및 75년간의 국제법 위반 행위를 옹호하는 결과임을 지적함

반학살주의와 반유대주의의 동일시 문제

     * Tao는 반학살주의를 반유대주의와 동일시하는 오류가 있다는 점을 강조함
     * 이는 AIPAC 등 영향력 있는 단체가 역할을 하고 있음도 언급함

학계의 책임과 경고

     * 많은 학자들에게 이 주제가 불편하게 느껴질 수 있음을 인정함
     * 그러나 학계가 압력에 굴복하는 전통을 깨지 않는다면, 앞으로 기관, 학문 분야, 개인 경력에서 심각한 위협이 더해질 것임을 경고함

마무리 발언

     * 현 상황을 순응으로 해결할 수 없으며, 적극적인 저항이 필요함을 주장함
     * 마지막으로 Terence Tao의 연구 및 저서, 온라인 특강에 대한 팬이라는 개인적인 의견도 덧붙임

        Hacker News 의견

     * 미국이 2차 세계대전 이후 과학 및 산업 강국으로서의 우위를 가져왔는데, 이제 일부의 행동 탓에 그 기반이 무너지고 있음, Terence Tao와 그의 연구소의 자금이 정당한 절차 없이 끊긴 것은 시작이 아니라 이미 여러 희생양 중 하나임, 이는 스스로에게 해를 끼치는 행위임
          + 나라 전체가 이런 선택을 했다고 생각함, 국민 과반수가 이 방향을 지지해 투표였음
          + 도시 지식인을 처벌하는 움직임이 미국 우파 메시지에서 흔히 보이는 테마임, 이번 결과는 소수의 의지가 아니라 미국 유권자 다수의 진짜 바람임
          + 장기적으로 국민을 무지하게 만들고 AI와 기계가 그 자리를 대신하게 하려는 시도임, 억만장자들은 권력과 자원을 기반으로 농민과 억만장자만 존재하는 권위주의 사회를 만들려는 장기 플랜을 가짐, 민주주의는 일시적으로 필요했을 뿐 현재는 오히려 적임, 미국 내 MAGA와 진보진영의 싸움처럼 보여도 실상은 MAGA는 쉽게 리더를 따르기 때문에 이용당하고 있음
          + 이것은 소수의 행동이 아님, 정당 조직 전체, 보수적 유권자, 기술 리더들까지 여러 집단의 공조로 진행된 사안임
     * Terence Tao의 모든 박사과정 학생들은 Max Planck Institute For Mathematics에서 환영받을 가능성이 높음
       지원링크
       일반적으로 추천서 2통이 필요하지만 Tao의 경우 1통이면 충분할 것 같음
       새로 시작된 ""Max Planck Transatlantic Program""도 참고할 만함
          + 박사과정 시작 시, 여러 교수님들 중에서 수학적 능력과 팀워크 역량을 보증해 줄 분이 2명 이상 있길 바람, Tao와의 연관성만으로 입학하면 안 되고, 학생들이 진정 흥미를 가진 프로그램에 지원해야 함, 지금 해당 학생들이 겪을 심리·사회적 어려움이 크다고 생각하며 스스로에 맞는 길을 찾기를 응원함
          + 독일은 미국보다 이스라엘 및 유대인 비판 허용도가 더 낮다고 봄, 대륙 유럽(독일 제외)은 미국보다 친팔레스타인 성향이 강하지만 그 기준이 유럽 기관까지 뻗치지 않음, 예시로 네덜란드 암스테르담 대학의 학생들은 시위로 경찰의 강압적 진압을 당함, 결국 미국 학자나 학생이 유럽에 와서 주류 합의 내에서 활동은 가능하지만, 더 급진적인 팔레스타인 지지 활동은 미국과 마찬가지로 위험함, 즉 유럽이 진정한 대안은 아니라고 봄
     * 3개월 전에는 미국보다 예산이 적은 EU가 ""Choose EU for Science"" 캠페인을 시작한 이유를 궁금해했음, 만약 이런 상황을 예측했다면 EU의 전략이 이해됨, 오랜 연구 경력을 가진 과학자의 경우 전직이 어렵기 때문에 미국 지원이 끊긴 경우 안정적으로 중산층 소득을 보장해 주는 EU의 환경이 더 매력적으로 다가올 수 있음
          + EU는 GDP 대비로 보면 미국보다 더 많은 예산을 학술 연구에 투입함, 영국까지 포함하면 절대 규모도 미국보다 큼(미국은 민간 주도 R&D 비중이 높음), 그럼에도 미국에서 개별적으로 받는 펀딩이 그동안은 더 좋았고, 이는 학계-산업계 임금 차이가 넓어서 경쟁이 적었기 때문임, 미국인은 유럽인보다 박사 진학이나 학계 진출 가능성도 낮음
          + EU와 주요 회원국들은 미국 과학자 유치에 예산을 더 투입해야 함, 2차대전 후 독일 과학자, 냉전 당시 러시아 과학자를 영입한 것처럼 큰 힘이 될 수 있음, 본인 역시 이 상황이 안타깝고 오랜 협력 관계와 우정에 손상이 생김
          + 세 달 전에도 무엇이 벌어지고 있는지 명확하게 보였음, 이미 연구 지원금 신청서에서 금지된 단어 리스트가 공개되어 있었음
          + 뛰어난 시민은 자본주의 극대화 목표에 방해되는 존재임, 이 변화는 예측 불필요한 결과였음
          + ""과학자 여러분, EU로 오세요, 연봉은 적고 집은 찾기 어렵고 에어컨도 없지만 부탁임"" /s(농담임)
     * 현 행정부가 집착하는 숫자는 오로지 최고 지도자가 완벽하다는 서사를 뒷받침하는 수치 뿐임
          + 최근 노동통계청장이 해임되었음, 권위주의 국가는 내부를 빠르게 무너뜨림, 나쁜 소식 전하기가 위험해지면 결국 제대로 작동하지 않는 무기와 침몰하는 군함만 남게 됨
          + 팩트는 자유주의적 성향을 띄기 때문에 우파 정당은 사실을 받아들이지 않는 사람이 많아야 이득, 과학 장려는 표를 잃게 만들 뿐임, 기후 변화도 마찬가지로 이민을 늘리고, 이는 우익에게 투표하도록 사람들을 겁주어 득이 되므로 근본 문제에 대한 해결 의지가 없음
     * 정치를 벗어난 순수 학문 분야의 유능한 기관조차 정치적 이유로 자금이 끊긴다는 것은 매우 시사점이 큼
       (수정: 이 코멘트는 Tao가 아니라 행정부에 관한 의견임)
          + ""당신이 정치에 관심을 두지 않아도, 정치는 당신에게 관심을 둘 것임"" - 페리클레스 명언 인용
          + 권력자에게 위협이 될 경우, 사실과 지식은 언제든 엄청난 정치적 무기가 될 수 있음
          + 그래서 애초에 정치에 연루되지 않는 게 나음, 정치 바람이 바뀌면 곧바로 손해를 보게 됨
     * UCLA가 ""유대인혐오와 편견 없는 연구 환경 조성 실패""라는 이유로 지원금을 박탈당했다는 점에서, 마치 1930년대 독일 분위기가 느껴짐(물론 타겟이 반대라는 차이점은 있음)
          + 실제로 더 1930년대 독일과 닮은 건 UCLA 안에서 유대인의 이동을 제한하는 체크포인트와 인간 띠였음
            관련 기사
            UCLA는 단지 정부와의 협상을 통해 소수자 우대정책 철회와 친 하마스 세력 유입 저지를 약속하면 됨, Columbia도 협상 뒤 바로 지원금이 복구됨
          + 이는 오웰 소설의 디스토피아와 다를 바가 없음, 이스라엘 신정주의와 미국의 권위주의 확대를 거부하면 모두 '유대인 혐오'로 몰아가면서, 동시에 ""유대인이 미국을 망친다""고 투덜거리는 이중적 태도가 드러남
          + 방향이 바뀌는 것은 사소한 차이가 아님, 그 '전환'이 바로 연합군이 2차대전에 뛰어든 이유임, 대학들이 유대인을 불법적으로 배제했던 과거와 지금은 완전히 다름, 어느 쪽이 잘못한 것인지 분별하는 게 중요함
     * UCLA(또는 미국 공립대학)에서 박사과정을 밟는 건 원래도 위험 부담이 있었음, 10년 전 대학원 원서 넣을 때 UCLA 수학과에서 받은 오퍼도 첫 해 이후 자금이 확정이 안 된 게 인상적이었음(필요하다면 학생이 따로 지원금 찾아야 했음), Berkeley 물리학도 마찬가지였음, 반면 프린스턴 같은 사립대는 박사과정 내내 자금 보장해 주고 연구비가 없어도 조교 등의 자리를 보장해 줬음, 지금은 공립대 상황이 100배는 더 나빠진 듯함(사립도 사정이 나은 건 아님)
     * 최신 관련 소식
       NSF에서 Terry Tao 지원금 보류 (2025년 8월, 332개 댓글)
     * The Guardian 기사
       ""요약: UCLA가 작년 친팔레스타인 시위 기간 중 유대인 학생 차별을 허용했다는 이유로 유대인 학생들과 교수의 소송에 대해 약 650만 달러에 합의함. 소송 이유는 학교 측이 시위대의 유대인 접근 제한과 증오 협박 등을 방치했다는 점임""
     * ""Terry, 집으로 돌아와줘, 우리가 그리워하고 있음"" - 호주
"
"https://news.hada.io/topic?id=22348","테슬라, 오토파일럿 사고 책임 회피 위해 데이터 은폐 및 경찰 혼란 유도","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                테슬라, 오토파일럿 사고 책임 회피 위해 데이터 은폐 및 경찰 혼란 유도

     * 테슬라는 오토파일럿 관련 사고에서 중요 데이터 은폐 및 허위 진술로 사고 책임을 회피하려 시도함
     * 사고 직후, 차량에서 서버로 업로드된 '충돌 스냅샷' 데이터를 숨기고, 경찰과 원고에게 존재하지 않는다고 주장함
     * 법적 및 수사 과정 전반에서 데이터 요청에 고의적으로 부정확한 정보 제공 및 중요 증거 왜곡이 이어짐
     * 외부 포렌식 전문가의 분석과 법원의 강제 조치로 은닉된 데이터가 결국 공개, 테슬라의 내부 분석에서도 오토파일럿 작동이 확인됨
     * 배심원 평결은 사고 책임의 33%를 테슬라에 부과, 오토파일럿의 지오펜싱, 운전자 모니터링 미비가 사고 예방 실패 원인 중 하나임이 밝혀짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

   지난 2019년 테슬라 오토파일럿 관련 치명적 차량 사고에서 회사가 데이터를 은폐하고 여러 해 동안 경찰과 법원, 원고에게 거짓말과 혼란을 유도했던 사건의 상세 기록이 재판 기록을 통해 공개됨. 본 요약은 사고 이후 테슬라의 조직적인 증거 은폐 및 조작 과정과 그 결과를 순차적으로 정리함.

사고 및 데이터 은폐 경위

  1 | 2019년 4월 25일 – 사고 발생과 즉각적 데이터 업로드

     * 사고 발생 3분 이내, 차량의 센서 비디오, CAN버스, EDR 등 데이터가 ""snapshot_collision_airbag-deployment.tar"" 파일로 서버에 전송됨
     * 차량은 로컬 복사본 삭제, 실제로 해당 데이터는 테슬라만 접근 가능
     * 포렌식 공학자가 차량 내장 컴퓨터 분석을 통해 이 데이터 존재 사실을 확인
     * 테슬라는 법적 자료 요청에 데이터가 존재하지 않는다는 허위 답변 제출

  2 | 2019년 5월 23일 – 테슬라 변호사, 경찰 공식 요청서 직접 작성 유도

     * 플로리다 고속도로 순경(Riso)이 테슬라에 충돌 데이터 요청
     * 테슬라 변호사가 공식 문서 내용을 직접 지시함으로써 실제 충돌 데이터 배제(오락기록 등 부차적 데이터만 제공)
     * 테슬라는 이미 데이터 보유 중임에도 불구, 경찰, 원고에게 해당 사실을 숨김

  3 | 2019년 6월 – 협조 의심 받는 데이터 추출 과정

     * 경찰이 해당 차량의 Autopilot ECU, MCU 직접 회수
     * 테슬라 서비스센터에서 데이터 추출 시도 그러나 내부 기술자는 절차에 익숙하지 않다고 진술하며 “데이터 손상” 주장
     * 향후 포렌식 엔지니어가 해당 컴퓨터에서 데이터 접근 성공, 데이터가 실제로 존재했음이 증명됨

  4 | 2019–2024 – 반복되는 데이터 은폐 및 부정

     * 테슬라는 수년간 해당 데이터를 이미 확보하고 있었음에도 불구, 경찰 및 법원 요청에 일관되게 관련 데이터 부존재·손상 주장
     * 데이터 자동 삭제 기능이 존재한다는 구실을 내세움
     * 포렌식 증거나 법원 강제력에 의해 존재가 확인된 후에야 부분적 시인

  5 | 2024년 하반기 – 법원 강제 데이터 추출

     * 법원 명령으로 원고 측 전문가가 Autopilot ECU의 NAND 플래시 전체 이미징 진행
     * 테슬라가 불가능하다고 주장한 작업을 외부 전문가가 성공적으로 수행, 데이터 완전 확보

  6 | 2025년 2~3월 – 포렌식 분석 통한 결정적 증거 확보

     * “snapshot_collision_airbag-deployment.tar” 파일명, 체크섬, 서버 경로 등 주요 메타데이터 확보
     * 테슬라가 기존 제공한 데이터와 달리, 실제 충돌 전후 오토파일럿 작동, 경고 신호, 지도/비전 데이터 등 다수 결정적 로그 포함
     * 지도 플래그에 해당 지역이 ""restriced Autosteer zone""으로 표시되어 있었음에도 오토파일럿이 해제/경고 없이 작동 지속

  7 | 2025년 5월 – 서버 로그 및 증거 제출로 코너 몰린 테슬라

     * 원고 측, AWS 로그 소환 통해 파일의 2019년 4월 25일 18:16(PDT)부터 서버 저장 사실 확인
     * 테슬라 내부 분석에서 해당 파일 사용과 오토파일럿 작동, 운전자 손 떼고 있었음을 명백히 확인
     * 경찰이나 피해자 가족에겐 데이터 제공하지 않음

  8 | 2025년 7월 – 배심원 평결 검토

     * 배심원단은 사고 데이터 공개 및 테슬라의 조직적 은폐 시도 확인
     * 실제 데이터 내용: 오토파일럿/오토스티어 완전히 작동, 운전자는 수동 개입 없음
     * 경고 없음, 시스템은 제한구역 진입 시 대응 미흡(고속도로 전용 설계 영역에서 벗어난 도로에서 오토파일럿이 켜짐)
     * NTSB가 과거 테슬라에 시스템 오남용 방지를 위한 지오펜싱, 운전자 감시 강화 조치 권고한 바 있음
     * 운전자의 부주의가 직접 원인이었으나, 테슬라의 시스템적 허점, 최소한의 책임(33%) 배분됨

Electrek 논평

     * 비판자들은 이번 사례를 피해자 측의 부당한 배상 요구로 폄하하나, 가족 관점에서 진상 규명 및 재발 방지 의지의 정상성을 강조
     * 직접적 책임은 운전자에게 있으나, 테슬라 역시 오토파일럿의 사용 제한 미비, 위험 경고 부족, 책임 회피 시도가 중대한 문제임
     * 모든 증거를 확인한 후 배심원 12인은 33% 책임을 테슬라에 귀속, 이는 공정한 판단으로 볼 수 있음
     * 이번 판례는 오토파일럿 시스템 신뢰, 데이터 투명성, 책임 소재 명확화의 중요성을 확인함

        Hacker News 의견

     * 테슬라가 사고 데이터를 보존하고 자체적으로 분석하는데, 조사기관에는 제공하지 않는다는 건 이해할 수 없는 일임. 개인정보 보호를 위해 데이터를 저장하지 않는 건 이해하겠지만, 만약 데이터를 보유하고 있고 합법적인 소환장이 있다면 반드시 넘겨야 함. 참고로 이번 사고는 운전자가 과속을 했고, 2019년 오토파일럿(완전자율주행 아님)을 도시 도로에서 사용했으며(원래 설계된 환경이 아님), 바닥에 떨어진 폰을 줍다가 가속 페달을 밟아서 자동 브레이크까지 무시하는 상황이었음. 즉, 이번 사고 자체는 테슬라 과실은 아니며, 왜 데이터를 숨겼는지 의아함. 고의라기보다는 단순한 무능력의 결과일 가능성이 높음
          + 사고 스냅샷에 따르면 운전자는 핸들을 잡지 않았고(손을 떼고 있었음), 지오펜싱 경고에도 불구하고 오토스티어가 핸들을 제어하고 있었으며, 고속으로 T자 교차로에 접근하고 있었지만 경고 메시지가 나오지 않았음. 이런 상황에서 오토파일럿을 사용할 수 있게 한 것은 테슬라의 부주의라고 생각함. 테슬라의 마케팅을 고려할 때, 이번 판결의 33% 과실은 충분히 정당하다고 봄. 그리고 이 데이터를 숨겼다는 점만으로도 테슬라의 안전에 대한 접근방식을 알 수 있음. 머스크가 자율주행 시스템 관련해서 대중적으로 거짓된 발언을 하는 것을 떠올리면 전혀 새로울 것 없음
          + 한론의 면도날(“고의가 아닌 무능력”이라는 밈)은 이제 그만 사라질 필요가 있음. 권력을 가진 위치에서의 무능력은 곧 악의 그 자체임
          + 테슬라를 실제로 소유하고 있는 입장에서 가장 큰 소프트웨어 문제를 느끼는 지점은 대부분의 일반 소비자들이 “Autopilot”과 “FSD”의 차이를 실제로 잘 모른다는 것임. FSD는 신호등/교차로에서 멈추지만, Autopilot은 사실상 고속도로 전용 크루즈 컨트롤임. 두 모드는 같은 방식으로 활성화되고, FSD 구독/업그레이드 여부에 따라 동작이 달라지는데, 일반 소비자 입장에서는 차이를 이해하기 어려움. 지인이 테슬라 렌트했을 때 Autopilot만 켜져 있는지 모르고 교차로에서 멈출 줄 알았다가 당황한 적도 있음. 테슬라가 2019 AP를 완전히 없애고 모든 사용자에게 제한적인 FSD라도 제공하거나, 혹은 AP의 사용을 고속도로로만 제한했으면 좋겠음
          + https://electrek.co/2025/08/…
            테슬라 측은 이번 판결이 잘못되었다고 주장하고, 오히려 자동차 안전을 퇴보시킬 뿐 아니라 생명을 살릴 기술의 개발에 장애가 된다고 항소 입장을 밝힘. 판결에서 운전자가 압도적으로 책임을 진 것으로 보았으나, 증거상 운전자가 과속, 오토파일럿 무시, 주의 산만 등 사고의 전적인 책임이 있으며 2019년 당시에도, 지금도 이런 사고를 막을 차는 없음. 원고 측이 사실을 왜곡했다고 주장함.
            이해가 안 가는 점은, DJI도 2025년쯤 지오펜스를 없앴는데, 이는 장비 운용자가 직접 책임진다는 전제로 FAA도 지지하는 방향이었음. 이런 제조사 책임론 판결들이 나와서 우리가 제대로 된 기술을 못 갖게 됨. 그래서 각종 무선장치, 부트로더도 소스코드 공개 없이 락 걸려 사용하기 어렵게 됨. 테슬라가 이런 싸움을 계속하는 게 필요하다고 느끼며, 결국 이런 식이면 핸드폰 제조사도 도로 옆을 걸을 때 자동으로 핸드폰을 비활성화해야 하는 상황이 올 수 있음
          + 이 사건의 본질적인 문제는, 테슬라의 시스템이 운전자가 기대하지 못할 방식으로 신뢰를 유도했다는 점임. 배심원들은 테슬라가 과대광고를 했고, 증거를 숨기려 했다는 점을 비판함
     * 테슬라가 충돌 데이터를 “로컬에서 삭제”했다는 판결 이유에 대해, 왜 이런 기능을 넣었을지 납득가는 설명이 있는지 궁금함. “충돌 3분 내에 센서정보, 비디오, CAN-bus, EDR 등 데이터를 tar로 묶고 서버로 업로드, 이후 로컬 복사본 삭제”라는 구조인데, 이게 비행기의 블랙박스가 FAA 서버에 업로드 뒤 데이터 삭제하는 셈이라 오히려 신뢰성 감소하는 복잡성을 굳이 넣을 이유를 모르겠음
     * 이런 행위에 대한 형사 책임이 없는 한, 상황은 바뀌지 않을 것임. 2019년 이후로 테슬라의 기업가치는 급상승했고, 이번 3억 2900만 달러 벌금은 회사 전체에 비해 미미한 수준의 숫자에 불과함
          + 중요한 포인트는, 회사 전체 가치와 비교할 게 아니라, ‘문제 예방에 필요한 비용’과 비교해야 한다는 점임. 이번 3억 2천 9백만 달러는 경고 시스템 추가, 지오펜싱 적용에 들어갈 비용에 비해 훨씬 큼. 또, 한 번의 사고 비용일 뿐이고, 이후 756건이 추가로 발생함. 비슷하게 배상하라고 하면 현재 시총의 80%가 날아감. 평균적으로 사고당 5천6백만 달러 비용(합의, 소송, 매출 손실 등)이든다면, 회사순이익이 모두 증발함. 이런 큰 위험부담을 그냥 둘 이유는 없음
          + 3억 2천 9백만 달러가 어느 회사에도 무시할 수 있는 소액일 수 없음. 테슬라에게도 거액임
     * 테슬라가 이런 식으로 데이터 숨기는 것은 정말 멍청한 행동임. 사고 책임의 결론과 무관해도, 증거 인멸 시도가 문제임. 오토파일럿은 ‘크루즈 컨트롤’임을 이해한다면, 테슬라 과실이 있다고 주장하는 건 기존 운전자 보조기술 기준과도 다름. 테슬라가 해당 기능을 막을 수 있다고 해서 무조건 막아야 한다는 의미는 아님.
       중요한 건, 마케팅 용어 해석에 달림. 만약 “오토파일럿”이라는 말이 오해 소지가 있다고 생각한다면 배심원 판단에 동의할 거고, 아니면 그 반대. 내 경우, 상세 규정을 모두 아는 변호사도 아니고, 실제로 테슬라 경고를 무시하고 전혀 책임지지 않은 미흡한 운전자가 문제임에도 불구하고 판결이 불공정하게 느껴짐. 다른 브랜드의 차에 비해 경고가 매우 명확하고 자세했음. 솔직히 정치적 분위기의 영향도 있다고 생각함. 반박 의견도 듣고 싶음. 참고로 나도 테슬라 운전함
          + 비타민워터 사례가 떠오름. Center for Science in the Public Interest가 비타민워터가 ‘건강음료’라며 기만적 마케팅을 했다고 집단소송을 제기했고, 코카콜라는 ‘이걸 진짜 건강음료로 믿는 소비자는 없다’고 대응함.
          + 테슬라가 경고 메시지 설계에 더 공들였는지 아니면 마케팅에 더 신경 썼는지 생각해봐야 함. 한 회사에서 “이 차는 알아서 달립니다” 광고를 듣고, 또 운전할 때 “항상 도로를 주시하세요”라는 팝업 알람이 나오면, 소비자는 당연히 전자의 마케팅 메시지를 더 신뢰할 수밖에 없음. 다른 회사들도 경고를 무시하는 사람은 있지만, 테슬라만큼 성공적이고 공격적인 마케팅으로 사람들에게 경고를 무시하게 만드는 사례는 없음. 이게 핵심 차이임
          + 오토파일럿이 크루즈 컨트롤이라는 주장에 반론을 제기함. 테슬라는 오토파일럿을 그보다 훨씬 더 크게 마케팅함. 오토파일럿의 사전적 정의는 “운전자의 개입 없이 항공기/차량의 경로를 유지하는 장치”이며, 크루즈 컨트롤은 “가속 페달 없이 속도를 일정하게 유지하는 전자장치”임
          + “수많은 경고를 무시해야만 운전자가 책임을 면할 수 있다”는 주장에 대해, 기사에서는 사고 전 어떤 경고도 나오지 않았다고 언급함. 그렇다면 운전자가 어떤 경고를 놓쳤다는 건지 궁금함
          + 오토파일럿이 크루즈 컨트롤이라는 해석은 현실과 다름. 테슬라는 자신들이 판촉하는 시스템의 한계와 이름을 항상 제대로 이해하지 못하는 층을 적극 타겟으로 하는 마케팅을 수년간 추진함. 시스템의 한계를 잘 아는 사람들은 오히려 솔직하게 광고하는 브랜드 차량을 더욱 선호함. 현재는 극단적인 특정 고객층 위주로 바뀌는 중이지만, 여전히 테슬라가 소비자 혼란을 의도적으로 유발하고 있으므로 책임을 져야 한다고 생각함
     * 테슬라가 거의 모든 사고 데이터를 모으면서, 자기에게 불리하면 비공개, 운전자 과실이면 적극적으로 미디어에 PR을 한다는 게 내가 테슬라를 사지 않는 거의 유일한 이유임. 데이터를 안 모으거나, 혹은 데이터로 운전자를 곤경에 몰지 않는 회사에서 자동차를 사고 싶음
     * 이런 이슈가 사회적으로 알려지는 게 굉장히 좋은 일임. 테슬라는 예전부터 이런 행보를 보여왔는데, 최근 들어서야 압박을 느끼고 일부 소송에서 합의하는 단계임. 전형적인 범죄적 행위라 봄.
       또, 이런 글이 HN 메인에 안 보인다는 사실도 흥미로움. AI, 로보틱스, 기술 분야의 대표주자가 이런 짓을 대놓고 하고 있다는 게 업계에 알려지면, 실제 하는 행동들이 껍데기뿐임을 깨닫게 되고, 더 많은 석연치 않은 문제가 드러날 것임. 이 ‘카드 쌓기’식 거품의 붕괴가 정말 기대됨
     * 온보드 스냅샷 자동 삭제 구현 결정이 어떻게 정당화되었는지 확인해보고 싶음
          + https://en.wikipedia.org/wiki/Tampering_with_evidence
          + 삭제된 것은, 서버에 전달하기 위해 패키징한 파일이며, 원본 데이터 전체가 삭제된 것은 아님. 악의적인 행동이라기보단, 쓰레기 파일을 남기지 않는 단순한 코드 동작임
          + 기사만 보고 전체 구조를 알 순 없지만, 임베디드 시스템에서 진단 목적으로 임시 파일을 생성/업로드한다면, 스토리지 부족 방지를 위해 삭제하는 게 자연스러운 설계임. 만약 원본 데이터 자체를 삭제했다면 문제가 크지만, 현재 케이스는 그렇지 않으므로 심각하게 볼 필요는 없다고 생각함
     * 기사 용어가 혼란스럽게 섞여있어 헷갈리는데, 사고 당시 차량을 제어한 것이 FSD인지 autosteer인지 알아내기 힘듦. 내 경험상 autosteer는 신호등, 정지선을 무시하고 주행함. 그리고 원론적으로, 이런 자동차 사고에서 텔레메트리를 활용할 수 있다는 점은 놀라운 일임. 일반 차량 사고는 데이터가 거의 전무함. 테슬라가 이런 부분에선 특별함
          + Autopilot 기능임. 용어 구분이 중요함
          + 2019년 당시에는 FSD 자체가 없었음
     * “조사관이 테슬라가 조사 협조적이라고 생각했다”는 기사 내용에서 조사기관 자체도 실패한 것임
          + 그렇지 않음. 조사관들은 기본적으로 선의로 데이터 협조를 우선 시도하며, 상대방이 신의를 저버린다고 판단될 때에야 강제적으로 개입함
     * 자율주행차 산업의 후발주자들이 기술 완성도가 아닌, 규제당국을 시간으로 지치게 해서 관성적으로 안전 기준이 낮아지는 걸 노리는 듯함. 즉, 정치적 진영의 규제 포획에 희망을 걸고 차의 안전성을 선언하려는 ‘도박’임
          + 미국에는 실질적인 성능 규제가 거의 존재하지 않음. 대부분의 주, 캘리포니아 정도만 형식상 몇 가지 인증을 거치지만, 실제 시스템 성능과 무관하게 일단 허가가 남. 규제기관은 헤드라인이 터질 때 그것에 반응해서 조치할 뿐, 실제 기술 성능에는 무관심임. 캘리포니아 규제조차 테슬라 세미트럭도 통과할 만큼 느슨함
"
"https://news.hada.io/topic?id=22353","Show GN: 매일 같은 프롬프트 치기 싫어서 만든 크롬 익스텐션, 5천명이 쓰게 된 썰","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: 매일 같은 프롬프트 치기 싫어서 만든 크롬 익스텐션, 5천명이 쓰게 된 썰

  [디지털 월세 때문에 시작한 프로젝트]

   이미 국내에 Lilys AI라는 좋은 서비스가 있는데, Gemini에 Claude, ChatGPT까지...
   디지털 월세가 너무 많이 나가서 추가 서비스를 쓸 여력이 한 때 개발자였던 두 딸 아빠에겐 너무 큰 부담이었습니다.

   꼭 유튜브뿐만 아니라 블로그, 해외 아티클 등 다양한 인사이트를 뽑고자 어려움이 많았고,
   Lilys AI에서 모티브를 얻어 원하는 방식으로 요약, 질문, 변형하는 걸 무료로 사용하고자 주말에 몰래 컴퓨터를 켰습니다.

  [개발 중 만난 지옥들]

   지옥1: 웹페이지는 말할 것도 없고 ChatGPT, Claude, Gemini 입력창이 다 다름
   → AI별 삽입 로직 따로 제작

   지옥2: 모든 사람이 무료로 쓸 수 있어야 하는데, 유튜브 API에 자막 기능이 없음
   → DOM 직접 파싱하다가 UI 바뀔 때마다 터짐

   지옥3: 크롬 익스텐션 4KB 제한
   → 긴 프롬프트는 로컬 저장으로 우회

  [예상 못한 반응들]

     * 다운로드: 5,000명 (생각보다 많이 써주심)
     * 일 사용자: 평일 기준 2,500명

  [배운 것]

     * 프롬프트 AX경험의 진짜 위력.
     * 로직 구현 JS 없이 프롬프트만으로 가능

  [마무리]

   Claude, Gemini, ChatGPT, DeepSeek, Grok, Perplexity 뭐든 선호에 맞춰서 프롬프트로 콘텐츠를 가공할 수 있고,
   프로그램은 프롬프트로 짜는 나만의 UX 경험을 할 수 있습니다. (무료고 로그인도 필요 없어요.)

   아내에게 주말 1~2시간 장난감 만들 수 있는 까방권을 얻기 위해
   피드백 주시면 더 나은 도구로 만들어보겠습니다.

   ChatPage 무료 크롬 익스텐션 바로 구경하기

   우와. 이거 눈이 확 뜨이네요!!

   오 정말 쓸만하네요. 유투브/뉴스기사 링크 던져주고 분석하라고 하면 에러가 자주 뜨던데, 이건 알아서 웹 컨텐츠나 유투브 스크립트를 스크랩해주는군요. 잘 쓰겠습니다!

   흐흐 넵 혹시 추가로 필요한 요건이 있다면 언제든 피드백 부탁드려요~

   [만든이]
   https://youtube.com/@everyday_dailyup

   [리뷰로 응원해주기]
   https://chromewebstore.google.com/detail/…

   [오픈 채팅방 (신고하기)]
   https://open.kakao.com/o/gWlJ6Azh

   오 감사합니다. 잘 사용해보겠습니다 :)

   감사합니다~ 조금이나마 효용감을 드렸음 좋겠습니다~~ 🌝
"
"https://news.hada.io/topic?id=22241","2.5년 된 내 노트북이 이제 Space Invaders를 JavaScript로 작성함 (GLM-4.5 Air)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     2.5년 된 내 노트북이 이제 Space Invaders를 JavaScript로 작성함 (GLM-4.5 Air)

     * 2.5년 된 MacBook Pro M2에서 GLM-4.5 Air 3bit 모델을 활용해 Space Invaders 게임 코드를 한 번에 생성함
     * 이 모델은 중국 Z.ai가 MIT 라이선스로 공개한 최신 open weight 모델로, 코딩 벤치마크에서 우수한 성능을 보임
     * 44GB 3bit quantized 버전 덕분에 64GB 램 PC에서도 실행 가능함
     * ml-explore/mlx-lm 라이브러리를 최신 커밋으로 사용해 로컬에서 모델을 구동하고, 비교적 빠른 속도와 안정적 동작을 경험함
     * 최근 출시된 로컬 코딩 특화 대형 언어 모델들이 매우 높은 코드 생성 능력을 보이며 빠르게 발전 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GLM-4.5 Air 및 MLX로 JavaScript Space Invaders를 생성한 경험

   2025년 7월 29일

   어제 소개한 GLM-4.5 모델 패밀리는 중국 Z.ai가 MIT 라이선스로 공개한 최신 고성능 open weight 모델임
   코딩 벤치마크에서 Claude Sonnet 4와 같은 기존 모델들과 견주어도 높은 성능을 나타내는 것으로 평가받음

   가장 작은 GLM-4.5 Air 모델도 총 1060억 파라미터, 약 206GB 크기를 가짐
   Ivan Fioravanti가 MLX에서 실행할 수 있도록 3bit로 44GB로 양자화한 버전을 공개해 64GB 메모리 노트북에서도 구동할 수 있음
   직접 시도해본 결과, 이 작은 모델조차 매우 강력한 성능을 보여줌

   입력 프롬프트:

     HTML과 JavaScript로 Space Invaders를 구현한 페이지를 작성하라는 요청 프롬프트를 입력함

   모델이 응답을 생성하는 데 시간이 조금 걸렸으며, 이 결과물이 성공적으로 출력됨
   비록 초보적인 예제지만, 2.5년 된 랩톱(64GB MacBook Pro M2)에서 첫 시도에만에 동작하는 완성도 높은 코드를 직접 생성함은 인상적임

모델 실행 방법

     * 최신 mlx-lm 라이브러리의 main 브랜치와 특정 glm4_moe 지원 커밋를 사용해야 함
     * uv로 파이썬 환경을 띄운 후, 다음 코드로 모델을 로드함

from mlx_lm import load, generate
model, tokenizer = load(""mlx-community/GLM-4.5-Air-3bit"")

     * 44GB 용량의 모델 가중치가 ~/.cache/huggingface/hub/models--mlx-community--GLM-4.5-Air-3bit 폴더에 저장됨
     * 프롬프트를 다음과 같이 입력하여 생성 작업을 실행함

prompt = ""Write an HTML and JavaScript page implementing space invaders""
messages = [{""role"": ""user"", ""content"": prompt}]
prompt = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True
)
response = generate(
    model, tokenizer,
    prompt=prompt,
    verbose=True,
    max_tokens=8192
)

     * 생성 과정에서는 먼저 문제 요구사항 및 게임 설계 정보를 정리해 출력함
     * 이어서 실제로 동작하는 HTML, CSS, JavaScript 코드를 빠른 속도로 생성함

   생성 통계
     * 프롬프트: 14 토큰, 초당 14.095 토큰 생성
     * 본문 생성: 4193 토큰, 초당 25.564 토큰 생성
     * 최대 메모리 사용치: 47.687GB
     * 전체 대화 내역은 gist 링크
     * 출력 소스는 GitHub 예제에서 확인 가능
     * 직접 브라우저에서 실행 테스트도 가능

펠리컨 벤치마크 테스트

     * pelican riding a bicycle 벤치마크로 동일 모델의 SVG 이미지 생성 능력도 평가함
     * Generate an SVG of a pelican riding a bicycle라는 프롬프트에 창의적 SVG 이미지 코드 생성 성공
     * 모델은 최대 약 48GB의 RAM을 소모하며 결과를 반환함
     * 노트북에서 일부 앱을 종료해야 충분한 메모리 확보 가능
     * 속도 역시 만족스러운 수준임

로컬 코딩 모델의 발전

     * 2025년 들어 대부분의 대형 언어 모델들이 코드 생성 성능 강화에 집중함
     * 그 결과, 로컬 하드웨어에서도 실제 활용이 가능한 높은 코드 생성력을 보여줌
     * 2년 전 LLaMA 첫 시도 당시에는 상상하기 힘들던 수준에 근접함
     * 현재 사용 중인 동일 랩톱에서 GLM-4.5 Air, Mistral 3.2 Small, Gemma 3, Qwen 3 등 연이어 등장하는 고성능 오픈 소스 모델들의 혜택을 받을 수 있음
     * 최근 6개월 사이에 로컬에서 구동되는 다양한 고품질 코딩 특화 언어 모델 출시에 따라 개발 환경이 개선되고 있음

        Hacker News 의견

     * 2년 전 LLaMA를 처음 접했을 때는, 그 당시 사용하던 노트북에서 지금 보는 GLM 4.5 Air 같은 모델들(Gemma 3, Qwen 3, Mistral 3.2 Small 등)이 돌아가는 모습을 상상도 못했음. 오픈 모델들의 품질과 출시 속도 모두 내 예상을 훨씬 뛰어넘음. 참고로 ChatGPT가 22년 12월에 나왔을 땐, 그때 존재하던 최고의 오픈 모델이 GPT-J(약 6~7B)와 GPT-neoX(22B 정도?)였음. 실제로 한 달 가까이 gpt-j로 사용자 서비스를 운영했었는데, 퀄리티가 엉망이고 명령을 전혀 따르지 않아 프롬프트에 스토리처럼 던져주거나 예시를 여러 개 써야 겨우 동작했음. 그 후 LLama 모델이 ""유출""되고(아마 의도된 유출이라고 생각함) 역사가 달라짐. L1 시절에는 양자화, 파인튜닝 등 다양한 최적화가 나왔고, L2에서는 파인튜닝이 본격적으로 상용화됨(대부분의 파인튠이 Meta의 원본보다 더 좋았음), Alpaca의 LoRA 시연
       이후 미스트랄, 믹스트랄, L3, 제마, 퀀, 딥시크, glm, 그래나이트 등 엄청 강력한 모델들이 쏟아져 나옴. 어떤 분석에 따르면, 오픈 모델은 SotA 연구소가 릴리스한 모델에 약 6개월 정도 뒤처짐(참고로 연구소에서는 최고의 모델은 공개하지 않고, 내부적으로 다음 학습용 데이터 큐레이션 등에 쓰는 걸로 추정함). 6개월 차이라는 건 정말 미친 수준임. gpt-3.5 급이 되려면 2년 정도 걸릴 줄 알았는데, 이렇게 로컬에서 이 모델을 돌리고 파인튠까지 직접 해보는 세상이 올 줄 상상도 못했음
          + LLM 파인튜닝이나 LoRA(파라미터 효율 미세조정)는 어떻게 만드는지, 혹은 어떻게 사용하는지에 대해 한동안 질문을 계속했음. 실제로 유용한 답을 듣지 못했고 웹검색은 온통 SEO용/광고성 글뿐임. SD LoRA를 만드는 방법과 사용 방식은 2년 전부터 알고 있고 잘 다룸. 그런데 LLM LoRA에 대해서만 모든 게 감춰진 비밀처럼 느껴짐
          + Zuck(마크 저커버그)이 직접 4chan 같은 곳에 유출했을 리는 없음
          + GLM 4.5가 Qwen3 coder보다 더 나은지 궁금함
     * 2.5년 전 산 64GB MacBook Pro M2에서 이런 코드 생성이 가능하다는 사실이 여전히 놀라움. 특히 수정 없이 한 번에 동작하는 코드가 나왔음. 현 하드웨어가 얼마나 대단한 잠재력을 가지고 있는지 우리가 너무 과소평가하는 것 같음. ‘Bitter lesson’(마법은 계산 자원에 달림)이니 ‘효율적 연산 경계’ 사고방식이 혁신적인 접근을 탐구하려는 똑똑한 인재들을 오히려 멀어지게 한다는 게 걱정임. 실제론 지금 모델들이 훈련 이후 가중치 정밀도를 엄청 줄여도 성능이 남아 있는 걸 보면, 오히려 더 비효율적인 쪽이 아닐까 싶음
          + bitter lesson의 핵심은 데이터 양이 많아야 한다는 점 아니었는지 물음. 예시로 든 모델도 실제로는 22조 토큰 같은 엄청난 대규모 코퍼스에서 훈련된 것임
     * 해당 구현 결과를 이해한 것인지, 단순히 실행이 됐다는 점만 본 것인지 궁금함. LLM도 흔한 면접 질문엔 대충 맞는 정답을 내놓을 수는 있을 것 같음. 동료들이 데이터 변경 내용을 프레젠테이션할 때 JSON 시각화 앱을 LLM으로 만들었는데, 이미 잘 돌아가는 JSON 뷰어가 있는데 굳이 새로 만든 데 의문이 듬. 현업에서 사람들이 LLM을 주로 프레젠테이션 강화용으로 쓰지 실사용을 위한 도구로는 거의 안 쓰는 것 같음. 또 다른 동료는 대량 교재 콘텐츠 수정 매크로가 필요했는데, 그걸 만드는 데 LLM 프롬프트용 루브릭부터 만들고, 그 후에는 프레젠테이션 슬라이드로 매크로 요건을 정리해서 LLM에 다시 던졌음. 시스템이 너무 복잡하다 보니, 실제로 시간을 아낄 수 있었다고는 생각하지 않음. 결과물이 신기하긴 했지만, 정작 다른 사람한테 아무 쓸모 없는 경우가
       많았음
          + 나는 코드를 훑어보고 어떤 동작인지 파악했지만, 실제로 동작만 확인하면 더 들여다보진 않았음. LLM으로 프로덕션용 코드를 쓸 땐 한 줄 한 줄 모두 확인하는 편임. 다른 사람에게 설명할 수 있을 만큼 완벽히 이해했을 때만 코드를 commit함. LLM을 실전 코드 작성에 어떻게 활용했는지 자세히 정리한 글이 있음
            https://simonwillison.net/2025/Mar/11/using-llms-for-code/
          + LLM 자체가 바로 그 솔루션임
          + 사실 일회용 코드(Disposable code)가 AI가 진가를 발휘하는 분야임. 말도 안 되는 빌드 시스템용 보일러플레이트, 애니메이션 코드 등을 알아서 만들어주면 대환영임(3Blue1Brown이 애니메이션 작업에 들인 노력을 생각하면 AI가 그 일에 도움을 줄 수 있으면 적극 추천). 프로그래밍 모르는 사람이 프로토타입이라도 만들어 프로 개발자에게 넘기는 게 가능해진 건 진짜 큰 가치임. 결과 코드의 세부를 이해하지 않아도 되고, 합격/불합격만 판단하면 끝이라 실질적 유용성이 너무 큼. 다만 '억 단위 가치'를 가지는 문제는 이와 달리, 실제 서비스 버그 수정이나 기능 추가 같은 곳인데, 이런 데선 AI가 한계에 부딪힘. 또 한편으론, 일회용 코드는 원래 주니어 개발자의 성장 도구였는데 그걸 AI가 가져가 버린 건 고민되는 지점임
     * 해당 모델의 트레이닝 데이터에는 아마도 다양한 언어로 작성된 Space Invaders가 엄청나게 많았을 것이라고 추정함
          + 진짜 테스트는 ‘함수 수정해줘, 우주선이 아래로 쏘도록 해줘, 왼쪽/오른쪽에서 등장하게 해줘, 2인 모드 넣어줘’ 등 세부 구현 요청에도 모델이 대응하는지 보는 거임
          + 아마도 일부 데이터는 이미 데이터셋에 있는 게임들을 모델이 복사해서 합성 데이터로 만든 경우도 많을 것 같음. LLM이 생성하는 리액트 프론트엔드 코드를 보면 죄다 비슷하게 나오는 느낌임
          + 이 논의는 이미 3년 전부터 끝난 얘기임. gpt3 이후로는 사용 가능한 모든 코드가 다 훈련 데이터에 들어가 있는 상황이고, “이제 코드는 맞는 것처럼 보이지만, 실제로는 다 틀린 상태”에서 “0-shot으로 제대로 돌아가는 풀스택 앱이 뚝딱 생성”까지 단 2년만에 도달함. 비약적 발전의 관건은 단순히 데이터셋이 아니라 사후 훈련(post-training), 강화학습(RL), 긴 컨텍스트, 에이전트적 행위 등임. 초창기 모델은 2/4k 토큰 한계가 있었지만 지금은 아예 판도가 달라짐. 이런 관점 없이 단순 데이터 운운하는 건 아예 논점을 놓치고 있는 이야기임
          + breakout 게임과의 비주얼 유사성이 흥미로움
          + 이런 댓글도 결국 유사한 진짜 분석 없는 합성 코멘트가 트레이닝 데이터에 수도 없이 들어가 있을 확률이 큼
     * M4 Mac에 128GB 램 장착했고, 지금 GLM-4.5-Air-q5-hi-mlx(80GB)를 LM Studio로 다운로드 중임. 곧 결과를 공유할 예정임
     * LLM이 노트북에서 로컬로 돌아간다는 걸 쇼케이스하는 건 큰 의미가 있다고 생각함. 과거에는 작은 모델로 이런 게 거의 불가능했으니까 진짜 중요한 이정표임. 다만 Space Invaders 같은 특수/좁은 도메인이라면, 차라리 GitHub 등에서 기존 구현을 찾아 내려받는 쪽이 더 효율적일 것 같음. 이런 케이스에선 트레이닝셋 자체가 극히 작고, 모델의 벡터 공간도 범위가 제한적일 수밖에 없어서 결과 코드가 거의 원본과 비슷하거나 복붙 수준일 확률이 높음. 게다가 모델이 타이핑하는 속도도 기다려야 해 부가가치가 매우 낮음. 차라리 LLM에게 ""언어 X로 작성된 기존 Space Invaders 소스를 GitHub에서 찾아달라""고 요청하는 게 현명하다고 느껴짐. ChatGPT에 이런 코드 정리를 맡기면 LLM은 ‘과적합은 거의 없고, 모델이 암기하지도 않는다’는 주장을 반복하라는 쪽으로
       유도하는 게 웃김(나로선 둘 다 별로 믿지 않음)
          + ChatGPT에서 그 경고 메시지를 재현하지 못했음. 사실 이런 방식의 경고 삽입엔 정치적 힘이 큰데(직접적 혹은 살짝 재구성하는 식으로), 그게 무척 흥미로움
     * Claude Sonnet 4로 시도해봤는데 제대로 동작하지 않음. 3bit 양자화한 GLM-4.5 Air가 앞선 셈임. 관련 채팅 내역: https://claude.ai/share/dc9eccbf-b34a-4e2b-af86-ec2dd83687ea Claude Opus 4 역시 동작하긴 하지만, Simon이 올린 GLM-4.5에 비하면 한참 뒤처짐: https://claude.ai/share/5ddc0e94-3429-4c35-ad3f-2c9a2499fb5d
     * 처음엔 제목을 “2.5살 된 우리 아이가 이제 자바스크립트로 Space Invaders 만든다(GLM-4.5 Air)”로 잘못 읽었었음. 하지만 몇 년 뒤에는 진짜로 가능할지도 모름
     * 이 논의는 흥미로운 SF적 물음을 떠올리게 함. 오늘날의 소비자 하드웨어에 미래 인공지능의 바이너리가 웜홀에서 떨어진다면, 그 슈퍼지능이나 최소한 약한 에이전트라도 실행 가능할까(혹은 네트워크·설득력으로 다른 하드웨어에서 스스로 부트스트랩 가능할까)
          + 이게 바로 지금까지 내 ML 연구의 기본 전제임. '웜홀'만 '유전 프로그래밍/뉴로에볼루션 등'으로 바꾸면 똑같음. 데모씬(demoscene)의 극한 최적화 소프트웨어가 이 길로 나를 이끌었음. 요즘 내가 계속 묻는 질문은 ""현재 세대 LLM의 모든 기능을 동일하게 제공하는 바이너리의 콜모고로프 복잡도는 어떻게 될까?""임. 만약에 그 크기(복잡도)가 지금 내 데스크톱에서 실행 가능하다면 어떤 그림일까? 내 PC가 AAA 게임 프레임을 400fps로 찍어주는데, LLM이 utf-8 텍스트를 100b/s로 뱉어내는 것과 차이가 이토록 클리가 없다고 믿고 있음
          + 내가 진짜 흥미롭게 느끼는 부분임. 숨은 능력들이 얼마나 있고, 이걸 더 극한까지 어떻게 활용할 수 있을까? 심지어 이국적·새로운 하드웨어라면 어떨까? 우리는 주로 인간 뇌의 한계 때문에 추상화를 도입하여 일함. 그 추상화로 인해 좁은 영역에 집중하는 식인데, 추상화에는 비용이 크고 그 한계를 없앨 때의 잠재력이 궁금함
     * 로컬 LLM 구동용 최소/권장 하드웨어를 정리한 사이트가 있는지 궁금함(게임처럼 '시스템 요구사항' 문서가 있는지)
          + LM Studio (외에도 여러 도구가 있지만)에서 하드웨어 성능에 맞는 모델을 고르는 게 매우 쉬움
          + 다른 답변 외에도, 좋은 경험칙은 대부분의 로컬 모델이 q4 양자화에서 최적의 퍼포먼스를 보인다는 것임(모델 파라미터 수의 절반 조금 넘는 용량이 메모리로 필요함). 예를 들어 14B 모델이면 약 8GB, 여기에 context 용량을 추가하면 14B 모델엔 10GB VRAM 정도가 적정선임. q4에서 자원이 남으면 더 큰 파라미터 모델을 쓰고, 안 남으면 더 작은 양자화로 가는 방식이 적당임
          + 여기도 참고 자료가 있음
            https://www.reddit.com/r/LocalLLaMA/
          + 이 사이트가 매우 유용하다고 생각함
            https://apxml.com/tools/vram-calculator
          + HuggingFace 계정이 있다면, 보유한 하드웨어 정보를 등록하면 각 모델 별로 돌릴 수 있는지 바로 확인 가능함
"
"https://news.hada.io/topic?id=22279","나는 17개의 사이드 프로젝트를 출시했다. 결과는? 만료된 도메인만 잔뜩 남았다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              나는 17개의 사이드 프로젝트를 출시했다. 결과는? 만료된 도메인만 잔뜩 남았다

     * 창업자는 17개의 사이드 프로젝트를 런칭했음
     * 이 프로젝트들로부터 실질적인 수익을 얻지 못했음
     * 대다수 프로젝트의 결과는 만료된 도메인뿐이었음
     * 이러한 경험에서 실패와 학습의 가치를 강조함
     * 꾸준한 시도와 반복이 개인 성장의 중요한 과정임을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

17개의 사이드 프로젝트 런칭 경험

     * 창업자는 다양한 아이디어로 17개의 사이드 프로젝트를 직접 만들어보았음
     * 각 프로젝트는 열정적으로 시작했지만, 대부분은 시장의 관심이나 구체적인 수익 창출로 연결되지 않았음
     * 시간이 지나며 프로젝트들의 도메인 기간이 만료되어 결국 많은 프로젝트 사이트가 사라짐
     * 이 경험을 통해 단순 수치적 성공 못지않게, 실패 과정에서 얻는 배움의 중요성을 실감함
     * 여러 번 시도하면서 시스템 구축, 마케팅, 사용자 피드백 등 폭넓은 실전 역량 습득 경험을 얻게 됨

실패와 성장의 연결

     * 프로젝트 당장 성공하지 못해도, 사이드 프로젝트 시도는 자기 성장과 기술 습득에 큰 의미를 가짐
     * 단기적 성과에 집착하지 않고, 꾸준히 실험과 수정 과정을 반복하는 태도가 결국 장기적 역량 강화로 이어짐
     * 실패의 경험은 다음 프로젝트에서 더 나은 의사결정과 전략 수립의 기반이 됨

마무리 및 제언

     * 만료된 도메인만 남았지만, 그 과정에서 얻은 경험과 학습이 가장 큰 자산임
     * 많은 사이드 프로젝트 시도가 반드시 재정적 성공을 보장하지는 않지만 도전 자체가 가치임
     * 창업가와 개발자들에게 중요한 것은 반복적 실험과 지속적 개선의 자세임

        Hacker News 의견

     * 약 20년 간 일반 직원과 계약직으로 일하면서 몇 년마다 사이드 프로젝트를 만들어왔음, 결국 성공한 프로젝트를 찾게 됨
       결국 주말에 간단히 만든 것이 앱으로 발전했고, 1년쯤 후에는 사업이 됨
       사람들은 종종 좋은 아이디어를 내서 성공을 받을 자격이 있다고 느끼지만, 실제로는 얼마나 ‘관련 있는 사람들’이 내 것을 발견하느냐가 핵심임
       출시 당시 많은 조회를 쉽게 얻을 수 있지만, 다시 돌아오게 만드는 이유(끈적한 앱/콘텐츠)가 필요함
       스스로 재미있게 만들었던 것들 중엔 관심을 잃는 경우도 있었는데, 아이러니하게 지금 하는 일이 가장 지루한 주제 중 하나지만 그만큼 잘하는 사람이 별로 없는 것이 강점임
     * “도메인 구매 → 3일 밤 샘 코딩 → 흥미를 잃음 → 처음부터 다시 시작”
       나는 실제로 코드까지 쓰는 단계라도 가지만, 프로젝트를 아예 실코딩 전에 끝나버리는 경우가 더 많아짐
       도메인 구입하고, 트렐로 보드를 만들고, 뭔가 ‘Hello World’ 찍어보는 정도면 만족하게 됨
       내 뇌가 프로젝트를 상상만 해도 도파민 보상을 준다는 것을 알게 됨
       다른 사람이 멋진 프로젝트 완성하는 걸 읽거나 남이 만든 영상만 봐도 집단 성취감 느낌을 얻음
       이런 가짜 보상 루프에서 벗어나기 정말 힘들어서, 하루/주/월/연간 목표와 실제 생산적인 활동, 그리고 성취를 의식적으로 기록하며 체크하려고 함
       소셜미디어는 도파민 순환을 의도적으로 설계해서 나의 주의력을 쥐어짜는 만큼, 목표와 관계없는 것에는 신경 쓰지 않으려 노력함
          + 나도 똑같이 느꼈기에, 현재 여러 프로젝트를 인덱스카드에 정리해두고 연말에 불필요한 리소스를 정리할 계획임
            중요한 약속(가족, 본업 등)에만 충실하다면 시작하고 완성하지 못하는 성향 자체는 도덕적으로 문제될 게 없고, 심지어 이를 즐길 수도 있다고 생각함
          + 나는 MVP 완성 후 한참을 써보고 나서야 도메인을 구입함, 혹시 내가 뭔가 잘못하는 것인가 하는 궁금증이 있음
          + 나도 도메인만 마구 사들이다가 70개쯤에서 멈출 필요성을 느낌
            요즘은 마음에 드는 도메인은 모두 위시리스트에만 추가함(하지만 다시 보지 않음)
            프로젝트 아이디어는 종이에 써보는 게 머릿속 갈증을 해소해줌, 이 과정을 반복하며 실제로 만드는 건 극소수임
            프로젝트를 ‘끝낼 수 없다’기보다는 시작 자체가 안 되는 느낌임
          + 나는 항상 마지막까지 도메인 구매를 미룸
            인색한 성격이라서, 꼭 필요하다고 느끼기 전엔 안 삼
            흥미가 떨어져도 코드베이스는 남아 있고 언젠가 의욕이 돌아오면 다시 시작함
          + 거울을 보는 기분임. 수년간 계획만 엄청 세우고 실행은 거의 안 하는 내 모습이 떠오름
     * 이제는 프로토타입을 만들기 전엔 도메인명을 아예 안 삼
       그 이후에 딱 한 번만 도메인을 사서, 프로젝트를 출시함
       도메인은 최소 기능 제품을 ‘완성’한 내 스스로에 대한 보상이라 생각함
          + 이걸 십 년 전에 알았더라면 도메인에 1만 달러 넘게 날리지 않았을 텐데 고마움 ㅎㅎ
          + 괜찮은 아이디어임
               o 사이드 프로젝트는 메인 도메인 서브도메인에서 시작하거나 Cloudflare Tunnel 활용
               o Tailscale Funnel로 내 PC에서 바로 서비스 호스팅도 가능함
          + 나도 마찬가지임, 6개 도메인 만료시키며 배운 교훈임
          + 이를 ‘차고밴드 문제’로 비유함. 고등학생들 모여서 밴드 이름, 티셔츠, 드럼커버 만들고 정작 연주는 별로 안 하는 것과 비슷함
            나는 진짜 좋은 도메인(한 단어, 발음 쉬운 조합 등)만 확보하고, 갱신은 안 함
          + 아이디어 검증 후 만드는 것이 제일 안전한 접근임
     * 내 조언: 프로젝트에 흥미를 잃는 게 아니라 다음에 뭘 해야 할지 몰라서 익숙한 길(제작)에만 머무는 것일 수 있음
       만약 내 프로젝트가 갑자기 인기를 끌고 피드백이 쏟아지면, 다시 열정적으로 일할까를 상상해 보면 알 수 있음
       이 실험을 통해 가장 중요한 격차를 알 수 있음: 만들기와 실제 사용자를 얻는 것 사이에 다리가 필요함
       그래서 대부분은 ‘만드는 역할’에만 머물게 되는데, 여기가 자기에게 가장 안전하고 익숙한 공간이기 때문임
          + 꼭 그렇지만은 않음, 한 아이디어에 계속 ‘만드는 모드’로 머물 수도 있음
            나이가 들어가며 ‘Persistence’의 가치를 새삼 생각해 보게 됨
            한 아이디어를 계속 붙들고 있으면 사람들이 내가 그걸 진짜 믿는 줄 알고 결국 한번쯤 써보게 되는 것도 같음
     * 이제 Google Domains 묘지가 있을 수 없음
       2024년 7월 10일 기준, 모든 도메인이 squarespace로 이전됨 https://domains.google/
     * 예전에 Kanji Plus라는 앱을 만든 적 있음
       사이드 프로젝트로 수익을 내서, 궁극적으로 Phrasing이라는 내 야심작을 만들려고 했음
       프로토타입을 주말에 만들고, 다음 8개월을 온전히 제품 완성에 투입함
       그러나 사람들이 실제로 쓰기 시작하니, 낮은 성장 한계에 10년을 쏟는다는 걸 직감함
       그래서 Kanji Plus는 잊혀지고, Phrasing 개발에 집중
       다만 Kanji Plus를 구매한 회원들이 있었기에 서비스가 조용히 사라진 점에 미안함
       요즘은 이런 서비스가 별 손을 안대도 ‘영원히’ 남아있는 시대가 아니라는 걸 뼈저리게 느낌
       다행히 Kanji Plus 전체를 Phrasing에서 그냥 하나의 기능으로 ‘붙여넣기’ 가능해서 오래 걸리지 않을 것임
       예전 Kanji Plus 지지자들에게 Phrasing 출시 시 평생 무료 멤버십을 제공할 계획임(일반엔 없는 등급임)
       https://phrasing.app/
          + 난 2025년엔 pure HTML과 CSS만 쓰기로 결심함
            이런 방식의 사이트는 정말 오래 살아남음
            관련해서 최근 글도 썼음: https://joeldare.com/why-im-writing-pure-html-and-css-in-2025
          + 기존 제품이 예고 없이 중단되었다면, 새로 만드는 미완성 앱엔 가입하지 않을 것 같음
     * 12년째 사이드 프로젝트 무덤을 쌓아가며 소박한 수입을 버는 중임
       단 한 번도 대박이 난 적은 없지만 생존 중임
       시장에 ‘끌려가는’ 느낌을 늘 동경하지만 현실적으로 거의 모든 사업은 끝없는 언덕 밀어올리기(시지프스 같음)임
       성공적인 고객도 한 번도 ‘앞서 나간다’고 느끼지 못하고, 시간이 지나면 사업모델에 항상 뭔가 문제가 생김
     * “도메인 사서 밤샘코딩 후 흥미 잃음”
       사실 이건 ADHD의 전형임
       전 동료도 비슷한 고민을 했는데, 프로젝트든 취미든 관심을 확 집중했다가 배운 느낌이 들면 금세 흥미를 잃음
       그의 진짜 취미는 ‘뭘 배우는 것’ 그 자체라는 사실을 깨닫게 해줌
       ADHD의 흔한 특징은, 짧은 시간에 모든걸 빠르게 흡수하고, 온 신경을 쏟다가 갑자기 완전히 잃어버리는 것임
       그래도 이 나름의 인정과 장점이 있기에 단점을 너무 강조하지 않는 접근법을 추천함
          + 반드시 ADHD일 필요는 없음
            나도 아니고, 내 주변도 별로 없음
            모두가 뭔가를 시작만 하고 끝내지 못하는 건 같은데, 완성까지 가는 길이 너무 어려움
            시작의 90%는 쉽지만, 남은 90%는 진짜 힘듦
            끝까지 못하는 걸 비난은 안 하겠지만, 시작한 걸 하나도 끝내지 않는 건 훈련이 필요함
            완성하는 것도 꼭 연습해야 하는데, ADHD라는 라벨로 합리화하는 건 조심해야 한다고 느낌
          + 내 인생도 비슷함
            몇 주 동안만 열광했던 프로젝트가 수십 개
            예전엔 진짜로 세계를 바꾼다고 믿었지만, 지금은 솔직하게 “배우기 위해 함”이라고 인정하게 됨
            최근엔 audio encoding과 streaming protocol을 배우려 Icecast 서버 만들었고, fzf 클론 만들면서 Rust 디핑 알고리즘도 공부했음
            Swaybar 위해 async scheduling 프레임워크도 만들었고, Collatz추측 증명에 도전하며 Isabelle 배우기도 했음
            이런 ‘학습형 취미’가 최악은 아니어서, 나는 꽤 괜찮게 받아들이는 중임
            실질적으로 무언가를 하면서만 진짜 개념을 익힐 수 있다고 생각함
          + 결국 프로젝트를 세상에 내놓는 게 상상한 것보다 훨씬 어려워서, 흥미도 금방 식는 것임
          + 모든 걸 병명화하지 말아야 한다고 생각함
            이 사람들은 단순히 충분한 동기가 없거나, 자기 스스로를 잘 모르는 것일 뿐임
            오히려 ‘무언가를 배우는 것’이 그들의 취미일 수 있음
            그것을 진단명으로 만드는 게 오히려 모럴과 자기절제 가르치는 것보다 효과가 없을 수 있음
          + 좋은 시각임
            내 고민은 새로운 무언가를 배울 때 장비나 자료를 사고, 금세 질릴 것 같으면서도 서랍에 처박아두는 게 너무 짜증났음
            이제는 그런 소비를 억제하려 하는데, 가끔은 배움에의 욕구를 해치는 것 같아 스트레스를 받음
     * 결국 보유하고 있던 도메인(modulecollective.com)을 활용해 여러 프로젝트를 <제품>.modulecollective.com 형태로 런칭함
       약간 길지만 무료이고, 각종 사이드 아이디어를 자연스럽게 정리할 공간이 생겨서 좋음
       만약 어느 하나가 성공적으로 커진다면 그때 더 좋은 도메인을 구입해 리디렉트 시키면 된다고 생각함
       원래 이 도메인은 Eurorack 용 Netflix DVD 서비스 아이디어 때문에 산 거였지만, 실제로는 시도조차 안 해봄
          + 이 방식도 충분히 괜찮다고 생각함
            성공을 검증하기 전까지 불필요한 자원 낭비는 피하고 싶음
     * 핵심은 도메인을 딱 하나만 사서 모든 프로젝트를 각자 다른 서브도메인에 배치하는 방법임
       이 글에서 내 모습이 많이 보임
          + 맞음, 일종의 ‘플랫폼’처럼 활용하면 제품명 고민도 덜게 되고, notes.mydomain.com처럼 기능 그대로 쓸 수 있음
          + 작은 유틸리티에는 괜찮지만 GreatNewThing™ 같은 진짜 신제품은 자체 도메인이 필요함
            결국 해도 사용자는 별로 없게 됨
            그래도 짧게 하지 않고 수개월 투자하는 덕에 쓸데없는 도메인은 덜 쌓이는 듯 함(나도 7개뿐임 sigh)
          + 나도 그렇게 하는데 마음에 100% 들진 않음
"
"https://news.hada.io/topic?id=22251","BlueOS Kernel - Rust 기반 경량 POSIX 커널","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  BlueOS Kernel - Rust 기반 경량 POSIX 커널

     * Rust 언어로 작성되어 메모리 안전성과 현대적 시스템 설계를 동시에 추구
          + 보안성, 경량성, 범용성을 목표로 함
     * POSIX 표준 인터페이스를 지원하는 호환 커널로, 기존 UNIX/Linux 소프트웨어 이식 용이
     * Rust의 표준 라이브러리(std) 와 호환, Rust 기반 애플리케이션이 네이티브하게 동작
     * ARM32, ARM64, RISCV32, RISCV64 등 여러 칩 아키텍처를 지원하며, QEMU 에뮬레이터 플랫폼에서 실행 가능함
     * Rust 기반 OS, POSIX+Rust std 통합 구조, 모듈형 커널 아키텍처를 직접 실습 및 학습할 수 있는 프로젝트
          + 커널, libc 헤더/구현, 빌드 도구, Shell/예제 앱 등 구성 요소가 모듈화되어 있으며, 개발자 문서와 튜토리얼이 잘 정리되어 있음

   해당 github repo 가보니, readme가 영문과 중문을 지원하네요. 중국에서 만든 프로젝트일가요? 웬지~

   Vivo는 중국의 스마트폰 제조사입니다
"
"https://news.hada.io/topic?id=22291","Gemini 2.5 Deep Think, Gemini 앱에서 이용 가능","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Gemini 2.5 Deep Think, Gemini 앱에서 이용 가능

     * Gemini 2.5 Deep Think 기능이 Google AI Ultra 구독자 전용으로 Gemini 앱에 도입
     * 새로운 병렬 사고(parallel thinking) 기법과 연구 성과가 반영되어, IMO에서 금메달을 획득한 모델을 바탕으로 더욱 실사용에 적합하게 개선
     * 창의적 문제 해결, 수학 및 과학적 추론, 알고리듬 개발 등 다양한 복잡한 과제에서 탁월한 성능을 보여줌
     * 성능 향상을 위해 추론 시간(Thinking Time) 을 늘려 다양한 아이디어와 해법을 동시에 탐색하고, 이를 통해 더 깊은 사고와 창의적 결과물을 생성함
     * 안전성 및 책임 있는 AI 개발을 위해 강화된 평가와 조치가 이루어지며, 향후 API 및 엔터프라이즈 활용 확대 계획이 안내됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gemini 2.5 Deep Think 출시

     * Gemini 2.5 Deep Think 기능이 Google AI Ultra 구독자에게 Gemini 앱을 통해 제공
     * 신뢰받는 테스터 및 연구진의 피드백과 최신 연구 결과가 반영된 버전
     * 최근 국제 수학 올림피아드(IMO)에서 금메달 수준의 모델을 기반으로, 실제 사용자 경험에 맞게 속도와 실용성이 개선
     * 이번 공개로 창의적 문제 해결 도구로서의 가능성을 확대하며, 수학자 및 연구자 대상 피드백을 바탕으로 기능을 고도화할 계획

Deep Think의 동작 원리

     * 병렬적 사고 기법을 도입해, Gemini가 복잡한 문제에 다양한 아이디어와 해법을 동시에 탐색하고 비교/조합함
     * 모델의 추론 시간(Thinking Time) 을 늘려, 여러 가설을 심도 있게 탐구하여 더 창의적인 해결책을 찾을 수 있음
     * 강화학습을 통해 이러한 확장된 추론 경로를 적극 활용하도록 학습, 보다 직관적이고 깊은 문제 해결 능력을 강화함

Deep Think의 주요 성능 및 활용 분야

     * 점진적 개발 및 디자인: 복잡한 시스템이나 디자인을 단계별로 발전시키는 작업에서 높은 성능을 보여줌
     * 과학 및 수학적 발견: 수학적 추론이나 과학 논문 해석 등 고난도 창의적 탐구에 강점이 있음
     * 알고리듬 및 코드 개발: 문제의 구조화, 시간 복잡도 및 트레이드오프까지 고려해야 하는 어려운 코딩 문제에서 최첨단 성능을 달성함
     * 최신 벤치마크(예: LiveCodeBench V6, Humanity’s Last Exam)에서 기존 모델 대비 최고 수준의 코드/지식/추론 성능을 입증함

Gemini의 책임감 있는 개발 및 안전성

     * Gemini 2.5 Deep Think는 안전성 평가에서 기존 Pro 모델보다 더 개선된 콘텐츠 안전 및 객관적 톤을 보임
     * 복잡성이 증가함에 따라 위험성도 함께 평가하며, Frontier Safety 평가와 필요한 대응책을 강화함
     * 상세 안전성 결과는 모델 카드에서 확인 가능함

Deep Think 사용 방법

     * Google AI Ultra 구독자는 Gemini 앱에서 모델 드롭다운에서 2.5 Pro 선택 후, 프롬프트 바에서 Deep Think 토글을 통해 하루 정해진 횟수로 이용 가능함
     * 코드 실행, Google 검색 등 도구와 자동 연동되며, 훨씬 긴 답변 생성이 가능함
     * 곧 Gemini API 및 엔터프라이즈를 위한 추가 테스트도 진행될 예정

        Hacker News 의견

     * 새로 나온 Deep Think agent를 테스트해봤음, 그런데 다섯 번 프롬프트를 입력하자마자 일일 사용 한도에 도달함. 한 달에 $250을 내고 이 정도 서비스라면 좀 실망스러움. o3-pro나 Grok 4 Heavy에 비해 가격경쟁력이 현저히 떨어지는 수준임. AI 커뮤니티에서 이 기능이 그나마 Google Ultra 구독 가격을 정당화할 수 있는 유일한 부분으로 관심을 모았음. 그런데 Google은 AI Studio에서는 최고 모델을 무료로 제공하면서, 실제로 돈 내는 Ultra 구독자한테는 이런 식으로 과금 정책을 쓰니 도무지 이해가 안 됨. 성능 측면에선, 비즈니스 관련 고난도 문제 상황을 입력하니 명료하고 설득력 있는 솔루션을 제공해줬고, 내부 회의 결과와 일치하는 대답임. 그런데 결과적으로 o3도 훨씬 싼 가격에 비슷한 결론을 내줬음. 다만 o3의 리포트가 좀 덜 정돈된 느낌이었음. 좀 더 써봐야 알 것
       같음
          + 완전하게 상용화 준비/최적화가 된 것은 아니지만, 8월 2일에 시행되는 유럽연합 AI 법안(EU AI Act) 전에 출시하고 2년 동안 기준을 맞추려는 전략일 수도 있음. 그래서 일부 소수 사용자에게 강한 사용량 제한을 걸고 우선 공개했을 가능성이 크다고 생각함
          + 대용량 context가 필요한 작업에서의 Deep Think 성능이 궁금함. Parallel thinking(병렬적 사고)이 특정 문제 유형에 굉장히 유용할 수 있으니, 전통적 chain of thought가 다 못 다루는 더 많은 문맥을 처리할 수 있는지 실험해보고 싶음
          + 수년 전에는 코딩 실력의 척도로 인터넷 검색 없이 또는 StackOverflow 같은 곳에 잘 정리된 질문을 올린 뒤 스스로 답을 다는 습관을 가졌음. 때로는 “3일간 헤맸는데 이 답변이 내 인생을 살렸다” 같은 댓글이 달릴 때 참 뿌듯했음. 이번 주 내내 어려운 문제를 풀고 있는데, 그렇지만 Copilot류 AI 모델들은 거의 도움이 안 됨. 코딩에서 실력은 누구도(심지어 AI도) 도와주지 않을 때 스스로 일반화, 종합, 창의적 발상을 동원해야 비로소 느끼는 것임. (그래서 AI 코딩 agent에게 완전히 대체되려면 아직은 시간이 좀 더 필요하다고 스스로 위안하고 있음)
          + Grok 4와 4 Heavy 모델 모두 써봤는데 내 경험상 정말 별로임. 쿼리를 얼마나 많이 넣을 수 있든, 응답이 형편없으면 아무 소용 없음. 올해 LLM에 돈 쓴 것 중 최악임. 다양한 AI에 꽤 투자했지만 Grok에 쓴 돈 가장 아까움
          + Google이 최고급 모델을 AI Studio에서 무료로 제공하면서 실제 고객에게는 쥐꼬리만큼만 혜택을 주는 걸 보면 깜짝 놀라울 때가 많음. 하지만 이런 모습이 전혀 놀랍지는 않음. 아마도 Google은 AI Ultra 고객에서 큰 이윤을 내는 건 아닐 거고, AI Studio의 프리 티어에서 얻는 대량 사용자 데이터가 더 중요하다고 생각함. 최고 모델을 무료로 열어두면 가장 요구 수준이 높은 유저들의 시장점유율을 쉽게 얻음. 그리고 훗날 이들을 대상으로 과금정책을 펼 수 있어, 현재 구글이 보유한 유휴 서버를 잘 활용하는 좋은 전략이기도 함
     * 여러분, Gemini Deep Think에 “자전거 탄 펠리컨의 SVG 이미지를 그려줘”라고 프롬프트를 넣었더니 나온 결과임 https://www.svgviewer.dev/s/5R5iTexQ Simon Willison보다 먼저 해봄!
          + HN에서 밈으로 뜨는 건 무조건 훈련 데이터에 들어갈 운명임. AI 회사마다 인턴 한 명씩이 멋진 펠리컨 SVG 그리느라 땀 뻘뻘 흘리는 모습을 상상하면 재미있음
          + 방금 결과를 보니까 확실히 펠리컨 같아서 놀람, 꽤 괜찮음
          + 이런 밈 벤치마크(예: 딸기 그림 등)는 웃기긴 한데 요즘 모델 훈련에 너무 많이 들어가 있으니 쉽게 속일 수 있는 측정방식임
          + 진짜 미래에 산다는 느낌이 드는 가치임
          + 솔직히 처음으로 “이게 프롬프트 없이 SVG만 봐도 자전거 탄 펠리컨 맞다”라고 맞힐 수 있을 것 같은 결과물임. 여기에 보컬 타워 사례도 인상적임. 시각/공간 인지 면에서 꽤 성과라는 생각임
     * 직접 돌려보고 싶으면 simonw의 LLM cli와 llm-consortium 플러그인을 써볼 수 있음장점 1: 여러 모델을 자유롭게 조합해 쓸 수 있음. 연구실 상관없이 원하는 조합으로 세팅 가능장점 2: llm-model-gateway 플러그인 활용해서 한 번에 로컬 API로 내 앱이나 코딩 협업툴에 연결 가능 https://x.com/karpathy/status/1870692546969735361
       설치 및 예시 명령어, 그리고 consortium of consortium도 만들 수 있다는 예시까지 직접 적어줌.
       https://GitHub.com/irthomasthomas/llm-consortium
          + 왜 이걸 Gemini Deep Think의 로컬 버전이라고 하나 궁금함. 멀티에이전트 구조는 여러 방식으로 구현할 수 있지 않나 싶음. 그리고 다수 모델의 covariance(공분산) 때문에 오류가 동기화될 수 있으니, 다양한 구조 조합으로 오류 상관도는 낮추면서도 개별 정확도는 유지하는 게 성능최적화에 중요하다고 생각함. 해법이 다수 존재하는 벤치마크에서 이걸 실험해보고 싶음
          + 유럽연합(EU)이 consortium of consortiums(컨소시엄의 컨소시엄)인지 궁금함
          + 이런 기능을 지원하는 OpenWebUI 플러그인이 있는지 알고 있으면 알려달라고 요청함
          + llm serve 명령어가 안 보인다고 언급함
     * 몇 주 전에 IMO(국제수학올림피아드)에서 금메달을 딴 모델은 아니지만 거의 근접한 유사종임 https://x.com/OfficialLoganK/status/1951262261512659430아직 API로는 제공되지 않고 있음
     * 이번 접근법은 Grok 4 Heavy와 유사함: 복수의 ‘추론’ 에이전트를 병렬로 돌린 뒤 답변을 서로 비교해 가장 좋은 답을 선택해서 돌아오는 방식, 대략 30분 소요됨. 결과는 훌륭하지만, 사실상 Grok 4(단일 에이전트, 더 빠른 모델)보다는 Grok 4 Heavy 기준으로 벤치마크 비교해야 공정함
          + 동일한 추론 컴퓨팅 파워를 여러 에이전트로 분산하면 더 좋은 성과 나옴. “오래 생각하면 답변이 더 나빠지는” 문제도, 여러 갈래의 사고를 병렬로 짧게 해서 극복할 수 있음
          + 기사에서 Deep Think는 병렬적 사고 방식으로 다양한 아이디어를 한 번에 생성, 동시에 고려, 통합, 수정해 최종 해답에 도달한다고 밝혔음. 이 설명으로 다중 에이전트 활용 여부가 명확하지 않아 여러 해석 여지가 있다고 생각함
          + Grok-4 heavy는 툴을 사용해서 벤치마크에 나오는 많은 문제를 손쉽게 푸는 구조라서 직접 비교에 한계가 있음
          + 구글 방식이 Mixture of Experts(전문가 혼합)과 어떻게 다른지 궁금함. 전문가 혼합은 아예 각 전문가마다 가중치를 다르게 학습하는데, 여기서는 temperature 조정만으로 사고의 다양성을 얻음. 동일 모델을 여러 번 돌려 아이디어 다양성을 얻는 게, 아예 아키텍처/가중치가 다른 모델 여러 개를 동시에 돌리는 것보다 어느 쪽이 나은지 논문에서 정확하게 비교된 자료가 있는지 궁금함
          + 아직 주요 LLM들을 일종의 대결 방식으로 한 자리에서 돌려서 최종 답변을 고르는 앱이 안 나온 점이 의외임
     * OpenAI가 $200, Anthropic이 $100·$200, Gemini는 $250, Grok은 $300까지 가격을 올림. OpenAI만 유일하게 “사실상 무제한”이라고 했고 실제로 ChatGPT Pro플랜에서 한도에 도달한 적 없음. Claude Max 같은 경우는 여러 번 한도에 걸렸었음. 그런데 이런 회사들이 한도를 명확히 공개하지 않는 이유가 궁금함
          + 이중 과금이 목적임. 공정한 가격이라면 쿼리당 토큰 단위로 요금 보여줘서, 사용한 만큼만 내면 됨. 하지만 회사들은 정기적 고정수입을 원하고, 실사용량은 최소화하려고 하기 때문에 매달 또는 연단위로 무제한처럼 판매함. 결국 실제 사용량보다 더 비싸게 내게 만드는 구조임
          + 한도를 미리 공개하지 않는 진짜 이유는, 시장 상황 또는 인프라 부담에 따라 회사 측에서 한도를 유동적으로 조정할 수 있어야 하기 때문임. 예전 ChatGPT 이미지 생성(Ghibli craze) 열풍처럼 갑자기 트래픽이 몰릴 때 한도 제한을 걸기도 하고, 지금처럼 여유 있을 땐 풀어버릴 수 있음
          + 한도를 투명하게 하면 사용자들이 한도에 맞춰 꼼수 쓰기 시작하고, 그러다 보면 결국 모두에게 한도가 더 줄게 됨. 그러니 공개하지 않는 편이 실질적으론 대부분에게 더 나은 선택임
     * 최근 몇 달 간 Gemini를 써보며, 오히려 점점 더 나빠졌다고 느낌. 헛소리(hallucination)가 너무 자주 나오고, 이를 지적해도 AI가 고집을 피움. 신뢰하기 힘들어짐
          + 내 경험상 Flash는 점점 좋아지고 있음. Pro에 돈을 내고 있음에도 Flash를 더 자주 씀. Pro는 최신 정보 검색을 거의 하지 않으면서 옛 트레이닝 데이터만 반복하는 경우가 많아서 실망스럽지만, Flash는 이런 문제가 거의 없음. 코딩엔 Pro를 Gemini CLI에서 활용하고 있는데, 단순히 코드 작성뿐만 아니라 디자인 문서 작성, 주 단위 과제 분해, 일정 관리 등에서 엄청난 실력을 보임. 이처럼 체계적 구조만 잡아주면 자기 맥락도 알아서 챙기는 느낌임
          + 나도 비슷한 경험임. Gemini Pro를 더 이상 안 씀. 너무 장황하고 내용이 모순적임. Claude Sonnet 4는 잘 대답함. 최근 Sonnet은 Opus와의 실력 격차가 많이 좁혀진 느낌임. 새 쿼터제를 도입한 이후엔 Sonnet부터 먼저 쓰게 됨. 이제는 Opus와 비교해도 어렵거나 복잡한 문제 대부분을 잘 해결함. 불과 몇 달 전만 해도 이렇게까지는 못 느꼈음
          + 나 역시 갈수록 Gemini가 점점 나빠진다고 느낌. 다만 fiction.livebench 같은 벤치마크에서는 그 차이를 수치화하긴 어려움. 혹시 지나치게 모델을 aggressive quantizing(성능 저하가 발생하는 양자화)하는 중인지, 아니면 우리 기대치가 계속 올라가는 것인지 궁금함
          + 주로 툴 연동 문제인지, 그리고 AI studio에서 쓰는지 아니면 API로 쓰는지 묻고 싶음. 내가 써본 바로는 사용 불가 툴을 허구로 만들고, 결과에 과도한 자신감을 보이는 경우가 많았음
     * Google AI Ultra 구독자면 오늘부터 Gemini 앱에서 Deep Think(고정된 프롬프트 수 제공) 기능을 쓸 수 있다고 안내됨. 근데 “고정 세트”라는게 고정된 개수인지, 아니면 프롬프트 타입이 정해져 있다는 의미인지 더 구체적으로 알고 싶음
          + 하루 10번 요청이 한도임. 프롬프트 하나에 약 30분 생각하므로, 일반 코딩이나 팬픽 작성보다는 연구나 다층 종합적 문제에 특화됨
     * Gemini CLI로 일정짜기 할 때, 명확하게 여러 차례 돌발 행동 하지 말라고 지시하고 개입해도 자꾸 임의로 변경을 시도해서 계획을 꼬아버리는 경우가 많음
          + 이런 에이전트 계열은 오히려 혼란을 자주 일으킴. Claude Code(Anthropic)는 모델 성능을 최대한 이끌어내는 방식을 써서 인기임. 그런데 Gemini CLI는 오히려 Gemini Pro 2.5의 본래 성능을 저하시킴. 그래서 이제 Gemini CLI는 아예 포기함(무료라도). 그렇지만 프롬프트 위주 작업에서는 여전히 매우 강력해서 정기적으로 활용 중임
          + 나 역시 비슷함. Gemini CLI에 추상적이고 큰 과제를 그냥 맡기면 자꾸 실수를 연발함. 대신 명확한 구조(컨텍스트 생성을 단계별로 분리)만 조금 잡아주면 정말 놀라운 성과가 나옴. 첫 단계에서 코드를 읽고 요구사항 정의서를 작성만 하라고 지시함. 이후엔 해당 결과물을 참고해 상세한 요구명세서와 API 설계, tricky logic의 의사코드 등도 단계별로 문서화하도록 요청함. 마지막엔 전체 개발을 주,일,시간 별 업무 플랜으로 쪼개게 시키고, 충분한 정보를 투입해서 최종적으로 코드를 작성하도록 함. 완전 자동화하면 스크립트로도 되겠지만, 실제론 사람이 검수하고 피드백하면서 브레인스토밍을 반복하는 구조라서 더 효과적임. 컨텍스트 90% 이상을 자기 힘으로 생성하면서, 최근엔 이런 방식이라면 대부분 실수를 거의 안 하게 됨
"
"https://news.hada.io/topic?id=22273","느리게 진행되는 인간의 장기 프로젝트들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         느리게 진행되는 인간의 장기 프로젝트들

     * 인간만이 매우 긴 시간 동안 해결할 수 있는 문제와, 이를 위해 기관을 설계하는 방법에 대한 논의임
     * 수십 년에서 수천 년에 이르는 장기 목표 지향 프로젝트들의 예시를 중심으로 설명함
     * 대성당 건설, 대규모 과학 연구, 표준 및 오픈소스 시스템 지속성 등 다양한 사례가 포함됨
     * 몇몇 프로젝트는 정말로 긴 시간이 필요했지만, 어떤 프로젝트는 가속화가 가능했을 것이라는 질문을 제시함
     * 인간이 세대를 넘는 목표에 도전해 온 기록은 기술 및 조직 설계에 시사점을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 이 글에서는 인간이 오직 매우 오랜 기간에 걸쳐서만 해결할 수 있는 문제와, 이를 달성하기 위한 기관 설계 문제를 다룸
     * Twitter 사용자의 추천을 바탕으로, 특별히 목적 의식이 뚜렷했던 프로젝트들 위주로 장기 프로젝트 목록을 소개함
     * 언어 변화, 도시·종교 형성 등 비계획적, 분산된 변화보다 명확한 목표를 가진 프로젝트 예시를 우선함
     * 본 글은 Patrick Collison의 /fast 프로젝트 목록을 패러디한 것으로, 두 종류 프로젝트 사이에서 흥미로운 공통점도 발견됨

대표적인 장기 프로젝트 목록

     * Fermat's Last Theorem 증명: 수십 년~수백 년에 걸쳐 수많은 수학 이론이 발전하며 증명에 기여함
     * 여러 대성당 건설: Notre Dame(1163~1345년) 등, 건축이 수백 년간 이어진 사례가 많음
     * Sagrada Familia: 1882년 착공 후 현재까지도 건설 중임
     * Cape Grim Air Archive: 1978년부터 대기 샘플을 보관해, 장기적 대기 연구에 활용됨
     * Framingham Heart Study: 1948년 시작, 심장 질환에 대한 장기간 추적 연구임
     * Central England Temperature series: 1659년부터 현재까지 온도 자료 시리즈를 수집함
     * LIGO 중력파 검출기: 초기 구상(1967년)부터 수십 년간 개발이 이어져 2016년 최초 검출 성공함
     * E. coli 장기 진화 실험: 1988년부터 지속된 미생물의 진화 연구임
     * Pitch drop experiment: 1927년 시작 후 현재까지 이어지는 점성 실험임
     * Clock of the Long Now: 1만 년 동안 작동할 시계를 목표로 한 프로젝트임
     * 리눅스, Wikipedia와 같은 오픈소스 시스템 및 표준(TCP/IP, 유닉스 타임 등)은 수백~수천 년 지속될 수 있음을 예측함
     * 2번가 지하철(맨해튼) : 1942년 준비 착공, 2017년 1단계 완공 등 수십 년 진행된 사회기반시설 사례임
     * 가장 오래된 기업 목록: 578년에 시작된 건설사 Kongo Gumi 등, 몇몇 기업과 신사(예: Izumo-taisha)는 세대를 걸쳐 지속함
     * Study of Mathematically Precocious Youth: 1971년부터 현재까지 진행 중인 영재 연구임
     * Wikipedia에는 이 외에도 다양한 장기 실험 사례 목록이 있음

주요 논의점과 질문

     * 이러한 장기 프로젝트 중 일부는 필연적으로 긴 시간이 필요했다고 평가됨
     * 반면, 어떤 프로젝트는 기술·조직적 지원으로 더 빠르게 추진될 수 있었던 가능성도 제기됨
     * 인류가 세대를 뛰어넘는 문제 해결에 도전하며 남긴 경험은, 오늘날 복잡한 목표를 위한 기관 설계에도 통찰을 제공함

        Hacker News 의견

     * 오래된 옥스퍼드 건물의 중심 기둥을 교체해야 했던 일화를 떠올림. 기존의 100피트 길이의 목재는 구할 수 없어 큰 고민이었지만, 관리인이 ""대체할 나무가 있다""고 말하며 150피트가 넘는 고목을 보여줌. 이 나무들은 200년 전에 전임 빔을 교체할 때 심은 것이었음
          + 이 이야기는 도시 전설임. 실제로 1862년에 대학 측이 자체적으로 소유한 숲에서 오크나무를 벌목하여 새 기둥을 만든 기록이 남아 있음. 오크를 키워 건축용 목재로 쓰는 것은 표준 숲 관리 방법이고, 기둥 교체만을 위해 따로 나무를 관리한 건 아님. 이런 혼합 활엽수림 관리 방식에 대한 추가 설명과 역사적 참고 자료는 http://www.new.ox.ac.uk/NC/Trivia/Oaks/"">여기에서 확인 가능함
          + 언젠가 이 이야기가 성장 해커나 LinkedIn 자기계발 글에서 성공을 위한 계획의 예시로 흔히 쓰일 것 같음
          + ""한 사회가 위대해지는 것은 늙은이들이 자신이 쉴 그림자를 누릴 수 없는 나무를 심을 때""라는 명언을 떠올림 (Elton Trueblood의 명언 변형)
          + 출처: oak beams at New College Oxford에 대한 Atlas Obscura 기사
          + 이 이야기가 사실인지 모르겠지만, 사실이었으면 좋겠는 바람임
     * Pitch drop experiment를 여기서 실시간으로 볼 수 있음
       Pitch drop experiment란 초고점성 물질이 방울로 떨어지는 데 십수년씩 걸리는 긴 실험임. 자세한 설명은 위키피디아 링크 참고
     * 여가 시간에 Collatz 추측을 증명하려 노력하고 있음. 나보다 똑똑한 수학자들도 실패했지만, 내 시도가 누군가의 증명에 조금이라도 기여할 수 있기를 바람. 수학은 이전 세대의 성과에 사람들이 계속해서 쌓아가는 과정임. ""완결""이란 없고, 계속 커지고 정교해지는 움직임임. 아직 어디서도 본 적 없는 아이디어가 있어 시도하는 중인데, 보통 이런 것이 실패 원인이기도 함. 그래도 해볼 가치는 있다고 생각함. 인류가 수 세대에 걸쳐 큰 프로젝트를 이어나가는 일이 인류의 위대한 점임. 대표적 사례로는 수백 년간 이어진 천연두 박멸 움직임을 꼽고 싶음
          + Stewart Brand와 Clock of the Long Now, 그리고 장기적 프로젝트들이 생각남. Tim Ferris 인터뷰에서 Brand가 한 말 중 ""자부심이야말로 내가 아는 가장 신뢰할 수 있는 행복의 원천임""이라는 구절이 인상 깊었음
          + 수학은 결코 완결되지 않음. 또, 늘 사라질 위기에도 있음. Bill Thurston의 말을 빌리자면, 수학적 이해는 수학자 공동체라는 슈퍼유기체 안에 존재함. 나는 분산 파일 시스템의 일부로 지속성과 새로운 발견의 가능성을 함께 제공하는 셈임
            MathOverflow 관련 글
          + 간단하면서 아직 풀리지 않은 새 문제 후보로 ""The Antihydra""를 소개함
            ""이 프로그램은 멈추는지?""
a = 8
b = 0
while b != -1:
  if a % 2 == 0:
    b += 2
  else:
    b -= 1
  a += a//2

            (//는 정수 나눗셈, 즉 binary shift one to the right 의미)
            문제 관련 설명1, 문제 관련 설명2
          + 학계에서 굉장히 협소한 주제의 경우, 관련 논문 하나가 수십 년 후에야 다른 논문에 의해 언급되는 일이 종종 있음. 시간이 지날수록 해당 주제 세계 최고 전문가가 되기 쉬운데, 논문 수가 적고 전문가들이 시간적으로 띄엄띄엄 흩어져 있음. 마치 초희귀 주제의 웹 포럼 스레드 같아서, 몇 년에 한 번 새로운 댓글이 달리는 상황임
     * 맨해튼 2번 애비뉴 지하철은 1942년에 준비 공사가 시작되었고, 첫 번째 구간은 2017년에야 완공되었음. 성공적인 결과 자체는 칭찬할 만하지만, 너무 느리고 그에 따른 비용 증가 문제는 반성해야 한다고 봄.
       왜 맨해튼에서 지하철을 파는 일이 서울보다 20배, 파리보다 10배나 비싼지 명확한 설명이 부족함.
       더 심각한 비용 문제를 다룬 기사: VitalCityNYC
       최근 이 주제에 대해 집중적으로 조사 중임: 관련 블로그
          + 오늘 이 주제를 굉장히 흥미롭게 읽었음. 결론을 잘 정리한 짧은 글: volts.wtf 글
            관련 추가 포스트: bsky 링크
            이런 비관과 반대를 기본 정신으로 삼는 사회가 안타까움
          + ""즉시 명확하지 않다""는 구절에 더 많은 사람들이 고민해야 한다고 봄. 만약 Ezra Klein 같은 사람이 대통령이 되어도 이 상황을 개선하기 어려울 것임. 규제에 대한 단순한 비난은 원인을 제대로 설명하지 못함. 이런 문제가 왜 생겼는지에 대해 그 어떤 존경받을 만한 이론도 아직 완벽하게 접근하지 못함
     * 천문학에서 느리게 진행되는 실험의 좋은 예로 ""cosmic distance ladder""가 있음.
       YouTube 영상, 위키피디아
       수세기에 걸친 다양한 관측과 삼각법을 응용해 지구 반지름, 달과 태양까지의 거리, 행성마다의 궤도 형상, 은하의 거리 등을 하나씩 측정해 왔음. 이 과정에서 고대 에라토스테네스, 아리스타르코스의 고전 측량, 케플러의 타원 궤도 연구 그리고 금성 횡단 관측 기간에 걸친 혁신적 사례까지 이어짐.
       점점 더 먼 거리를 측정하기 위해, 기초적 거리 측정에서 시작해 항성의 밝기, 변광성, 허블의 법칙 등 점진적으로 데이터를 쌓아 우주의 확장을 확인하게 된 역사임
          + 이 영상을 통해 천문학이 얼마나 방대하고 장기적인 데이터셋을 필요로 하는지 알게 되었음. 특히 Galileo와 Tycho Brahe가 데이터 공유 문제로 다퉜다는 일화가 흥미로웠음 (Galileo가 데이터를 훔쳤다는 얘기도 들음)
          + 정성스러운 정보 전달에 감사함
     * 친구가 영어의 모든 한 음절 단어들을, 오직 다른 한 음절 단어만으로 정의한 사전을 만듦. 'A'부터 1년에 한 글자씩 작업해서 26년 만에 완성했음. 수백 페이지로 길지는 않지만, 만약 한 번에 다 하려고 했다면 'B'나 'C'쯤에서 질려서 포기했을 것 같음. 따라서 이 방식이 효과적이었다고 생각함
          + 생각나는 많은 단어들을 한 음절 단어만으로 정의할 수 있을지 의문이 들지만, 확실히 재미있는 프로젝트임
          + 이 친구 Scrabble 게임도 잘할 것 같다는 생각임
     * SAS(Second Avenue Subway)는 정말 한심한 프로젝트라고 생각함. Fermat의 마지막 정리 증명같은 업적들과 같은 리스트에 이 노선을 올리는 건 다른 업적에 모욕임. 전 세계에서 가장 비싼 지하철 노선일 뿐 아니라, 이미 터널 굴착 기술은 현대적으로 발달해 있음에도 비효율이 극심함. 주요 원인은 기술 문제가 아니라 정치적 갈등과 쓸데없는 관료주의 때문임. MTA(교통청)와 공원관리국의 권력 싸움, 보여주기식 고용, 터널에서 아무 일도 하지 않는 인원에게까지 급여 지급, 정치적 싸움 회피 차원에서 비용이 더 드는 deep-bore 굴착 선택부터 각 부서가 각자 땅에 독자 권력을 행사하려는 행동 등이 모두 심각한 원인이었음
       참고 링크: Hudson Tubes 비용 비교, 뉴욕타임즈 보도, Pedestrian Observations 블로그
     * 꼭 오랜 시간이 걸릴 수밖에 없는 프로젝트(피치 드롭 실험)와, 단순히 자원이 부족해서 오래 걸렸던 노트르담 같은 것엔 구분이 필요하다고 생각함. 큰 성당 하나를 완성하려면 정말 많은 바위나 자원을 찾는 데도 시간 소요되는 경우임. 한편 피치 드롭 실험은 정말로 자연이 오랜 시간을 들여 우리에게 뭔가를 가르쳐주는 유형임. (코믹하고 강조를 위한 예시로 듦. 물론 노트르담도 인상적임)
          + 성당을 지을 때, 시작하는 이들이 완공되는 걸 자신이 못 볼 걸 알면서도 시작했음. 그럼에도 불구하고 이런 일을 했다는 것이 중요한 포인트임
          + 성당이 여러 세대에 걸쳐 지어지는 것 자체가 의미가 있음. 5년 만에 완성되어 바로 볼 수 있다는 것도 좋지만, 할아버지와 아버지, 그리고 나, 그리고 내 자녀가 모두 참여한 건축물이라면 그 규모와 깊이가 완전히 다름. 수십, 수백 년에 걸쳐 완성되는 프로젝트에는 삶을 뛰어넘는 숭고한 면이 있고, 그것이 꼭 신전을 넘어 백악관처럼 세속적인 건축일지라도 마찬가지임
          + 반드시 느려야 하는 것과 불필요하게 느린 것, 그리고 주어진 시간만큼 일을 키우는 경계 어딘가에 프로젝트들이 자리잡는다고 봄. 수학 (특히 연구 분야)에선 실제로 '쓸모'보다 '흥미'를 중시하고, 우연히 큰 결과를 낳는 경우가 많음. 간접 결과가 더 느리고 긴 프로젝트를 정당화할 수 있는가, 속도란 본질적으로 문제에 달린 것인가, 아니면 지나가봐야 아는가, 혹은 자원이 잘못 쓰인 경우에만 알 수 있는가에 대해 의문임
     * 이 내용에서 과학에 대한 대중의 기대감을 풍자한 오래된 만화나 밈이 생각남
       시위자: ""우리가 언제 무엇을 원하죠?""
       군중: ""고품질 이중 맹검, 샘플 수 10만 명, 20년 장기, 사전 등록 임상 연구요!!""
       시위자: ""언제 원하죠?""
       군중: ""지금이요!!!""
     * Patrick Collison의 /fast projects 리스트를 오마주한 이 페이지에 연관된 'Fast' 주제 커뮤니티 토론글 모음
       Fast (2023)
       Fast (2019, March)
       Fast (2019, Dec)
       Fast · Patrick Collison (2019, Oct)
       그리고 어제 올라온 제목만 연관된 글:
       Fast (2025, July)
"
"https://news.hada.io/topic?id=22233","Ask GN: "이 모델 이럴 때 의외로 진짜 좋아요" 같은 경험 있으신가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Ask GN: ""이 모델 이럴 때 의외로 진짜 좋아요"" 같은 경험 있으신가요?

   안녕하세요!

   저희가 최근에 NativeMind라는 오픈소스 브라우저 확장 AI 어시스턴트를 만들어서 GitHub에 올려봤어요: https://github.com/NativeMindBrowser/NativeMindExtension
   Ollama 연동 기반이라 대부분의 모델은 자유롭게 붙여서 써볼 수 있어요.

   요즘은 기능별로 어떤 모델이 실제로 잘 맞는지 이것저것 테스트 중인데요,
   혹시 여러분도 “많이 주목받진 않지만, 이럴 땐 이 모델 진짜 괜찮더라” 하는 경험 있으신가요?
   (예를 들면 qwen3로 번역해보면 꽤 자연스럽게 나오더라)

   직접 써보신 모델 추천도 좋고, 개인적으로 자주 쓰는 조합이나 흐름도 궁금합니다!
   꼭 저희 확장 안에서가 아니어도 괜찮아요 ㅎㅎ

   실제 유저 입장에서 어떤 모델이 실용적으로 쓰이는지 더 알아보고 싶어서요.
   가볍게 댓글 남겨주시면 정말 감사하겠습니다!

   저는 번역 위주로 글 작업 많이 하는데요
   qwen3도 좋지만 mistral이 은근히 더 괜찮더라고요.
   특히 영어 UI나 짧은 문구 번역에선 톤이 잘 맞는 느낌...!

   어 그러게요! 확실히 상황에 따라 필요한 톤이나 스타일이 달라지는 것 같아요.
"
"https://news.hada.io/topic?id=22260",""무제한"이라고 광고하지 말고 마음이 바뀔 때까지라고 솔직하게 말하라","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ""무제한""이라고 광고하지 말고 마음이 바뀔 때까지라고 솔직하게 말하라

     * Anthropic이 Claude Pro와 Claude Max의 주간 사용량 제한을 도입함
     * 많은 AI 기업들이 ""무제한""을 내세웠으나 실제로는 상위 사용자를 대상으로 제한 적용함
     * 이러한 제한이 주는 진짜 피해는 플랫폼 확산을 이끄는 핵심 사용자의 신뢰 저하임
     * 개발자들은 제한이 걸리면 타사 서비스를 고려하거나 사용 자체를 자제함
     * 투명한 요금제와 사용량 정보, 그리고 진짜 ""무제한"" 대신 명확한 사용 기준을 원함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Anthropic이 Claude Pro(월 20달러)와 Claude Max(월 200달러) 구독자들에게 주간 사용 제한을 전격 도입함. 이로 인해 특히 Claude Code의 적극적 활용자들이 작업 도중 갑작스럽게 사용을 제한받는 상황이 발생함

   이번 정책 변화는 AI 업계의 흔한 전략으로, 초기에는 ""무제한""을 내세우지만 이후에는 제한을 도입해 상위 사용자를 타깃으로 삼는다는 점에서 이전의 사례들과 유사함

AI 요금제의 전형적 패턴

     * 처음에는 관대한 혹은 무제한 사용 가능으로 마케팅함
     * 사용자들이 서비스에 의존성을 갖도록 유도함
     * 일정 시점 이후 상위 5% 정도의 활발한 사용자에 한해 제한(캡)을 추가함
     * 이를 ""지속 가능성"" 또는 ""공정성"" 논리로 포장함

   이러한 방식은 Cursor, Windsurf, GitHub Copilot 등 여러 AI 서비스에도 반복적으로 사용된바 있음. Claude Code 역시 같은 흐름을 따름

   경제적 관점에서는 Anthropic의 입장도 납득 가능하지만, 이와 같은 신뢰 저하 유도형 전략은 개발자 생태계에 부정적 영향만 초래함

상위 5%가 핵심 사용층

   실제로 제한의 영향을 받는 것은
     * 비즈니스 워크플로에 깊게 통합한 파워유저
     * 초기 도입의 리스크를 감수한 얼리어답터
     * 사내에서 플랫폼 도입을 견인하는 영향력 있는 사용자
     * 월 200달러 급 Claude Max 유료 구독으로 진지하게 활용하는 고객

   즉, 제한에 걸리는 5%만의 문제로 넘어가지 않음. 바로 이 그룹이 플랫폼 확산과 성장을 주도하는 계층이기 때문임

신뢰 비용 증가

   이러한 ""미끼상품"" 전략은 개발자들에게 다음과 같은 행동을 유발함
     * 서비스 완전 의존 대신 리스크 분산 및 투명성 요구
     * 가격 정책이 안정화될 때까지 도입을 유보
     * 사용량 초과를 우려하여 자가 제한 적용

   Hacker News 유저들은 주간 단위의 제한이 예측 불가능성을 키운다는 점에서 불편함을 지적함

   개발자들은 급박한 상황에서 ""사용량 초과"" 메시지를 받으면, 서비스 인프라 비용보다는 대체제 탐색으로 바로 관심이 옮겨감

AI 사용료 현실과 개발자 요구

   모두가 AI 인퍼런스가 비용이 든다는 사실을 인지함. 하지만, ""무제한"" 광고 뒤 실제로는 은근한 제한과 불명확한 사용량 안내가 곧 신뢰 상실로 이어짐

   개발자들은 요금과 제한 정보가 명확하게 공개되어, 자신에 맞는 도구와 워크플로를 유연하게 설계할 수 있는 환경을 바람

   Hacker News 유저들은 사용 현황 정보조차 투명하게 제공하지 않는 기업 정책이 고의적이라 평가함

Kilo Code의 다른 접근

   Kilo Code는 다음과 같은 방식으로 차별화함
     * 숨겨진 제한 없는 ""무제한"" 미끼 상품 포기
     * 투명한 사용량 기반 과금 모델 채택
     * 사용자 주도 비용/사용량 관리 가능
     * 지급된 크레딧은 만료일 없음(단, 보너스만 30일 조건)

   특별 프로모션으로 입금액의 300%에 해당하는 보너스 크레딧 증정(예: 50달러로 200달러 상당의 AI 코드 지원 가능)

   이 같은 정책은 미끼성 제한 대신 실질적인 크레딧 제공을 통해 신뢰와 경험을 제공함

업계와 개발자를 위한 제언

   ""AI가 개발자를 대체하는 것이 아니라, AI를 활용하는 개발자가 AI 없이 일하는 개발자를 대체하는 시대임""이라는 인식을 다시 한번 강조함

   AI 생태계의 가격 정책 성숙, 예측 가능성, 그리고 사용자 중심의 관리 가능성이 필수로 요구됨
     * 기업은 실제 비용을 정직하게 안내하고, 사용량 제한을 명확히 제어, 예측 가능한 가격 정책 제시 필요
     * 개발자 역시 미끼성 요금제에 안주하지 말고, 투명성이 확보된 대안을 적극적으로 찾을 필요 있음

        Hacker News 의견

     * 지금 Claude 플랜을 해지하려고 시도 중이었음, 연구를 하다가 처음으로 한 시간 타임아웃에 걸렸고, 그제서야 비싼 리서치 기능이 나에게 별로 필요 없고 단순하게 언어 다듬기나 기본적인 정보 검색만 필요하다는 걸 깨달음, 그런데 이런 기본 기능조차 불투명한 한계치에 걸리면 모두 정지됨, 사용자가 제어하거나 모니터링할 방법 없음
       그래서 해지하려 했는데 불가 - 앱에서는 “다른 플랫폼에서 가입했으니까 거기로 가라”라고만 하고, 도대체 무슨 플랫폼인지 알려주지도 않음
       모바일 웹에서 해지하려 하면 여러 업그레이드 옵션만 보여주고 해지 옵션은 없음
       결국 신용카드사에 전화해야 하나 싶었음, 지금까지 써본 어떤 서비스보다 최악의 다크패턴 구독 경험임
       Anthropic에 꽤 긍정적인 이미지를 갖고 있었는데, 접속을 차단해 놓고 해지 방법도 제공하지 않으니 이미지가 완전히 바뀜
       추가: Stripe 결제 옵션 쪽에서 모든 인보이스 목록 아래에 해지 버튼을 간신히 발견함. 결국 해지 성공, 다만 다른 서비스보다 찾기 훨씬 힘들었음
          + 구글과 OpenAI도 언어모델 관련해서 비슷한 일을 겪었음
            Gemini Advanced는 거의 무제한 사용을 내세웠다가 하루 100회로 쪼그라듦
            OpenAI도 Pro 플랜에서 맥시멈 컨텍스트 윈도우를 조용히 줄였음
            이런 너프와 함께 50회로 깎은 뒤 100회로 올리는 식의 심리작전을 병행, 앵커링 효과로 불만을 완화함
            사실 이게 굉장히 영리한 전략임, “모트 없음”을 강조하지만 현실은 프로바이더를 바꾸는 비용(시간, 노력)이 존재해서 쉽게 안 떠남
            처음에는 적자 보면서 유저들을 모은 뒤, 점유율이 올라가자 갑자기 조건을 바꿈
            Claude의 상위 5% 중 40%가 이미 Claude 중심 워크플로우에 익숙해져서 관성에 의해 남을 가능성이 높고, 더 비싼 API를 쓰게 될 수 있음. Anthropic이 승자인 셈
            이제는 현대판 베이트 앤 스위치(bait and switch, 눈속임) 전략임. 똑똑하게(?) 법만 안 어길 뿐임
          + Claude는 확실히 좀 의도가 불투명하고 불법에 가까운 행동을 한 경험이 있음
            연간 플랜 가격이 €170+VAT로 나오길래 업그레이드 버튼을 눌러 상세 가격을 확인하려 했을 뿐인데, 확인이나 최종 가격을 보여주지도 않고 바로 가입 결제가 이루어짐
            결제 끝나고 보니 실제 금액은 €206.50
            환불 받기도 엄청 번거로웠음
          + 방금 Claude Pro 플랜을 해지할 수 있나 확인하려 했음
            Billing 페이지에 adjust plan 버튼이 있길래 들어가보니 업그레이드는 가능하지만 다운그레이드/해지는 어디에도 안 보임
            Account 페이지엔 “계정 삭제”는 있는데 “계정 삭제 전에 Claude Pro 구독 먼저 해지하십시오”라는 안내만 있음
            업데이트: Billing 페이지 하단, 스크롤을 많이 내리면 취소 섹션과 해지 버튼을 발견함
            업데이트2: 해지 클릭하자 3개월간 20% 할인 프로모션이 등장함
            업데이트3: 참고로 해당 테스트는 컴퓨터 접속으로 했음 (iOS/안드로이드 아님)
          + 신용카드사 홈페이지에서 굳이 전화하지 않고도 직접 결제 취소 요청할 수 있는 메뉴가 있음, 내 신용카드사도 그 기능을 제공함
     * “AI가 개발자를 대체하지 않겠지만, AI를 쓰는 개발자는 안 쓰는 개발자를 대체할 것이다”라는 말이 있는데, 이런 말을 진지하게 믿는 사람이 있다면 이미 기본적인 판단력을 잃은 것이라 생각함
       이건 미래에 모든 음악가가 오토튠을 쓸 거라는 이야기만큼이나 말이 안 됨
       비타민 C를 안 먹으면 예술/발명이 불가능하다고 주장하는 것 같은 터무니없음
       전혀 진지하지 않은 주장임, 이런 말을 하는 사람은 인간의 능력과 사실에서 동떨어진 우스운 존재라는 표시임
          + 나는 의견이 다름
            새로운 기술이나 독창적 해법을 만드는 개발자도 물론 있지만, 대다수의 유급 개발자는 평범하고 지루한 업무(비즈니스 로직, 폼, 테이블 구현 등)만 반복함
            이런 일엔 이미 AI가 더 빠르고 깔끔하게 해냄
            경험상, AI가 만들어낸 코드가 기존에 봤던 엉망진창 코드(특히 Salesforce “개발자”들이 낸 거)보다 더 읽기 힘든 적이 한 번도 없었음
            심지어 창의적 업무를 하는 사람들도 리서치, 문서화, 데이터 마이그레이션 스크립트 등에서 AI의 도움을 볼 수 있음
          + 이쪽 주장하는 분들은 컴파일러는 쓰는지 궁금함
            컴파일러 안 쓰는 개발자가 결국 컴파일러 쓴 개발자에게 대체된 것에는 동의하는지?
            최근 ffmpeg에서 어셈블리 써서 성능 끌어올린 사례처럼 예외는 있지만, 산업계 추세 얘기할 때 그런 예외는 무시해도 됨
            (조금 비꼬는 것 같다면 사과하지만, 내겐 이런 비교가 합리적이라 생각함)
          + (+1) 너의 의견에 약간 동의하진 않지만, Gemini Pro를 연간 구독함
            자주 쓰지는 않지만 그 가치가 높음
            바로 Bash 쉘 스크립트 같은 걸 빠르게 만들어내는 데 도움이 되고, 평소 잘 안 쓰는 기능도 5분씩 아끼게 됨
            코드 생성도 적당히 사용하면 쓸만함
            하지만 더 큰 강점은 AI를 통해 새로운 개념을 익히고, 논문 수학 설명 받고, 아이디어를 브레인스토밍할 때임
            AI의 진정한 가치는 우리 자신을 성장시키는 데 있다고 봄
          + Kilo Code 팀 소속임을 밝힘
            음악가 모두가 오토튠을 쓰지 않는 것처럼, 오토튠은 특정 결과를 위한 특수 기술임
            하지만 대다수 음악가는 작업을 녹음하거나 믹싱하거나 홍보할 때, 여러 기술을 적극적으로 활용함
            “온라인에 곡을 안 올리거나 스튜디오에서 특정 오디오 포맷을 안 쓰는 음악가는, 그런 걸 활용하는 음악가에게 밀려날 수 있다”란 식으로 이해해야 함
            물론 여전히 바이닐, 카세트테이프를 고집하거나 무대에서 마이크 없이 연주하는 사람도 있겠지만, 기술의 영향력을 무시했다간 시장의 미래를 볼 수 없게 됨
            Kilo Code 사용자들은 코드를 “오토튠하는” 게 아니라 워크플로우를 보강해서 더 빠르고 많이 만들 수 있게 하는 것임
            고용주 입장에선 이는 분명 가치가 있음
            하지만 진짜로 중요한 개발자의 역량은, AI에 뭘 요청하고 어떻게 보완할지, 틀렸을 때 직접 고칠 수 있는 데 있음
            도구가 100% 완벽할 수는 없으니 결국 인간의 역할이 남음
            이건 단순한 유행이 아니고, 기술 발전의 필연적인 방향임
          + 나는 AI가 100% 대체하지는 않더라도 효과적으로 대체하게 될 거라 믿음
            어느 시점엔 AI가 충분히 발전해서 대부분의 회사들이 AI를 쓰지 않는 개발자를 뽑지 않게 될 것임
            결국 대부분의 회사는 “CRUD” 같은 단순 업무가 전부이기 때문임
            특수한 프로그래밍 언어나 영역만 예외적 사례가 될 수 있지만, 대다수의 사람에겐 해당 안 됨
            본인 스스로 특별하다 생각하고 싶지만 사실 업계에서 정말 새로우면서 멋진 걸 만드는 사람은 소수에 불과함
            나머지는 적당히 잘 만들거나, 그럭저럭 평범한 업무를 하고 있음
            대다수 프로그래머는 특별하지 않음
            언제가 될지는 모르지만 5년 안에 전면적인 변화는 힘들어도, 내 커리어 내에 AI 대세 전환은 시작될 거라 생각함
     * “무제한” 플랜을 제공하면 항상 0.1%의 유저가 진짜로 무제한인 것처럼 최대한으로 서비스 이용을 시도함
       이건 호스팅 초기 시절부터 내려온 문제고, 컴퓨터 이전 시대부터 있었던 현상임
       가끔 아쉬운 건 일반 사용자, 즉 할당량을 거의 다 쓰지 않는 라이트 유저에게 “롤오버”가 없는 점임
       이번 달에 할당량을 다 못 썼으면 남은 걸 다음 달로 넘길 수 있으면 좋겠음
          + “무제한” 대신 명확한 사용 한도를 제공하면 훨씬 쉬워질 것임
            물론 극소수 남용 유저는 여전히 존재하지만 영향력은 미미할 것임
            그리고 남은 사용량과 리셋 시간을 직접 보여주는 카운터가 있으면 더 좋음
            사실 회사 입장에선, 대부분의 유저가 굉장히 적게 쓰기 때문에 사용량/한도 명확히 하는 게 오히려 유저들에게 가치를 높여줄 수 있음
          + 이 문제는 생명보험 등에서도 오래 전부터 논의된 바 있음
            역선택(adverse selection)이란 개념이 1860년대부터 논의되어 왔고, 1870년대부터 관련 용어가 사용됨
          + “0.1% 호더/고래 유저” 문제는 이미 사회나 법원에서도 받아들이는 부분임
            가끔 집에서 데이터센터 돌리는 걸로 분노하는 이웃 빼고는, “무제한 인터넷”을 부당하게 쓰는 것에 대해 사회적으로 크게 문제 삼지 않음
          + 나도 스타트업에서 0.1% 유저가 서비스를 얼마나 비상식적으로 남용하는지 직접 경험함
            일부 유저는 요금제 한계를 찾는 걸 게임처럼 여김. 24시간 내내 정상적으로 쓸 수 있는 최대치만 계속 써댐
            때로는 서비스 자체를 되팔 수 있는 방법도 창의적으로 찾아냄
            Anthropic에서도 이런 재판매 행위가 의심된다고 언급함
            흥미로운 건, 사실 Anthropic는 “무제한”이란 말을 쓴 적 없다는 점임. 항상 “더 높은 사용 한도”라고 광고했음
            그런데 인터넷상의 모든 댓글에서 “무제한이 깨졌다”는 식의 내러티브가 돌고 있음
            정보가 점점 이상하게 왜곡되어 퍼지는 걸 보면 신기함
          + 사실 실제로도 사용량을 다 채우지 않는 유저들만 있어도, 이 AI 회사들은 여전히 적자를 보고 있음
            작은 유저가 큰 유저의 비용을 충당해주는 구조가 아님
            AI의 진짜 원가는 앞으로 더 밝혀질 것임
     * 처음에는 광고라고 눈치채지 못했다가 갑자기 분위기 바뀌는 걸 보고 놀랐음
       “무제한”이라는 말을 쓰면서 내부적으로는 언제든 정책을 바꿀 수 있는 구조라 문제임
       새로운 제한은 8월 28일부터 적용되니, 지금 Max 플랜에 연간 구독 옵션이 있는지 궁금함(나는 월정액 이용 중)
       연간 구독이 아예 없다면 사실 “무제한”을 믿고 결제했다가 아주 갑작스럽게 당한다는 베이트엔스위치 논란은 해당 안 됨
       “차별화된 제안 - 더 많은 AI, 더 저렴한 가격”에서 ‘기간 제한 크레딧 제공+만료’라는 식으로 바뀐 것도 재미있음
       Claude Max의 한도가 불투명하긴 하지만, 정말 필요하다면 그냥 API 결제로 전환해 쓸 계획임
       플랜 한도 내에서도 Claude Code만으로 여전히 $200 이상의 API 상당을 얻을 수 있을 걸로 보임
       그게 안 된다면 기꺼이 타 서비스로 갈아탈 예정임
       유저 종속화 얘기도 우스움 – 나는 Claude Code용으로 간단한 훅 몇 개, 서브에이전트 몇 개 사용하긴 하지만, Anthropic에 대해 하드 디펜던시 없고 내일 더 나은 툴이 나오면 바로 이동할 생각임
          + 솔직히 현 시점에서 어떤 LLM 툴도 연단위로 결제하는 건 말도 안 되는 도박임
            업계 변동성이 너무 커서 6개월 전에 최고였던 게 지금은 한물간 서비스가 됨
            당장 지금 탑티어라 해도 몇 달 안에 별로가 될 수 있음
     * 예시: Apple의 새 “iCare”는 월 $20 구독으로 “무제한” 수리를 제공한다고 홍보함
       – 참고: “무제한”이라고 무료는 아님
       관련 출처: Apple Just Found a Way to Sell You Nothing
          + “무제한”이 무료를 뜻하지 않는 것은 보험 상품에선 아주 일반적임
            고장 수리마다 공제금(deductible)이 붙는 구조임
            조금이라도 사용자가 비용 분담을 하게 되면 서비스 이용 방식이 완전히 바뀐다는 연구 결과가 많음
            모든 게 완전 무료/무제한이라고 하면, 물건을 일부러 망가뜨려 무료로 신제품 받아가는 사례까지 생김
          + 이거, 결국 그 흔한 보험이랑 똑같음
            결국은 “약관 꼬투리 잡고 약속한 수리 안 해주기 대결”이 남음
            참고 아티클: Mac Owners Beware of the Crushing Limits of AppleCare
          + 정말 황당함! 나도 처음에 월정액만 내면 수리가 무료인 줄 알았음
            다시 약관 읽어보니 그냥 사기 수준임
            누가 이런 “새로운 iCare”를 구입하겠음?
          + 애플에겐 꽤 흔한 마케팅임
            애플은 고가를 받아도 그 가치만큼 제공하는 사례도 있고, 더 싸거나 내구성 좋은 대안 제품도 많음
            비판해야 할 건 회사가 아니라, 이러한 가격정책을 허용하는 소비자들임
            애플 유저들은 회사나 제품군에 대한 비판이 적은 편이기도 하고, 프리미엄 가격을 기꺼이 받아들이는 특성이 있음
     * Max 플랜에서 무제한을 약속한 적 있나?
       나는 가입할 때 “20배”란 말을 봤고 “무한”은 아니었음
       공식적으로 x배 제공이라 해놓고 그 x를 바꿔 제한을 다르게 거는 문제는 있을 수 있음, 하지만 어조가 꽤 다름
          + 무제한 약속한 적 없음. 항상 사용 한도 – Pro 플랜 대비 20배, 그리고 월 50세션(세션당 5시간 윈도우) 제한이 공식적으로 있었음
            다만 실제로 그 제한을 적용했는지는 모르겠음
            최근엔 그 50세션 관련 문구가 완전히 삭제됨
            즉, Anthropic이 말한 “24/7 풀가동” 유저들도 실제로는 기존 한도 내에서만 사용 가능했을 것임
            출처: 과거 세션 한도 관련 링크
          + 실제로 무제한으로 팔았다는 증거를 못 찾겠음
            항상 한도가 있었는데, 구체적 수치는 표시되지 않은 경우도 많았음
            대부분의 분노가 “무제한이 갑자기 없어졌다”는 데서 나오는데, 정작 그런 무제한은 없었음
            Anthropic조차 “5% 미만만 한도 변화를 체감할 것”이라고 했으나 인터넷은 “모두가 피해 본다”고 호들갑임
          + wayback machine에서 작년 가격표를 보면 “사용 한도”란 말이 꾸준히 있었음
          + Max 플랜에서 무제한 약속한 적 없음.
            심지어 공식 블로그 발표[0] 3번째 단락에
            “최대 20x 이용 한도”라고 명기되어 있음
            공식 블로그: Max Plan
     * 이제 진짜 대형 업체들이 자금과 인프라로 작은 스타트업을 몰아내는 시대가 올 것임
       GPU를 빌리는 구조로 어떻게 대기업과 맞설 수 있을지 의문임
       거기에 자체 설계한 특수칩(TPU 등)으로 경쟁하는 구글/Meta와 비교하면 더 불리함
       Meta/Google은 자본도, 인프라도 넘사벽임
       OpenAI가 얼마나 적자를 감수할 수 있을지 봐야 하고, Anthropic도 이미 어느 정도 소진 상태임
     * 무제한 플랜이 지정학적으로 어떻게 돌아가는지 보면
       판매자나 구매자 쪽이 반드시 손해를 보게 됨
       무제한 플랜 파는 회사는 이용자 평균 사용량이 낮아서 수익이 난다는 전제 하에 승부하는 것이고
       “플랜 남용” 유저도 API를 마음껏 활용할 권리가 있음
       다만 그 한계가 Anthropic의 예상 밖이었다는 게 문제임
       이제 와서 LLM 구독형 모델은 폐지되고, 모두 선불-사용량 기반 결제로 전환해야 함
       이게 모두에게 공정함
          + Anthropic이 무제한 플랜 판 적 없음
            이토록 많은 사람들이 무제한이었다고 착각하는 걸 보면 신기함
            분명 “남용” 유저가 API를 예상 범위 내에서 오용한 것까지는 맞음, 그래서 지금 Anthropic이 한도를 조정하고 있음
            LLM 구독형이 아니라 선불 등금제 API 결제는 이미 옵션으로 존재함
            자유롭게 쓸 수 있어 좋지만, 종종 사용량이 들쭉날쭉한 경우엔 구독형이 더 좋은 조건이기도 함
          + 원한다면 직접 API 구매해서 더 많이 쓸 수도 있음
            개인적으로는 월 수천 달러씩 프리미엄 툴에 쓸 순 없음
            하지만 적정 한도를 잘 정하길 바람
            $100~$200 수준이면 여전히 충분히 가치 있고, 매달 고정 가격이라 회사 비용 승인도 쉬움
          + “LLM 구독형이 사라져야 함”에 대해
            Claude Code 같은 서비스는 구독 없이는 사실상 불가능임
            나는 한도 제한은 괜찮지만, 매월 $200 이상 내야 하는 건 못 참겠음
            광고만큼 못 뽑는다고 느끼는 사람은 그냥 API로 옮기면 됨
     * 사용량 한도 싫은 마음 충분함. 하지만 Reddit 글처럼 극소수 사용자가 랭킹 오르기, 카르마 획득용으로 엄청난 컴퓨팅 자원을 태우고 있다는 관점도 존재함
       그래서 업체 입장에선 좋은 방향은 아니지만 제한을 더 엄격하게 거는 식으로 대응할 수밖에 없기도 함
       한편 진짜 돈 내고 쓰는 고객은 $200 내고도 API 오버로드 메시지를 받으며 서비스 못 쓰니 취소하게 됨
       여전히 한도는 투박하고 답답하지만, 저런 사례를 보면 일정 이해도 생김
          + Anthropic 측의 “베이트 앤 스위치”를 유저 탓으로 돌리면 안 됨
            이런 사용자들이 있다는 걸 진짜로 예상 못 했다면 너무 순진한 전략임
            결국 개발자 시장 점유율만 올린 뒤, 이제 와서 유저를 “나쁜 쪽”으로 몰고 있음
     * “베이트 앤 스위치(속임수)” 비판도 타당함
       하지만 실제 문제는 AI 기업들이 플랫요금제(정액무제한)의 단위 경제(uniteconomics)가 안 맞는다는 것을 뒤늦게 발견하는 중임
       AWS 같은 서비스도 넷플릭스형 정액제로는 불가능함
       여기에 투명성 문제가 더해져서 현실적으로는 투명한 사용량 기반 과금으로 가거나, 무거운 사용자를 고려해 플랫요금 자체를 훨씬 더 높게 책정할 수밖에 없음
"
"https://news.hada.io/topic?id=22234","Google Chrome에서 Google로 로그인하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Google Chrome에서 Google로 로그인하기

     * 웹사이트 방문 시 많은 곳에서 “Sign in with Google” 배너가 표시됨
     * Safari나 Firefox와 달리 Google Chrome에서는 이 배너가 나타나지 않음
     * Chrome은 대신 “One Tap” 대화상자라는 별도의 사용자 인터페이스를 사용함
     * 이 대화상자는 웹 페이지 요소가 아니라 브라우저 네이티브 UI임으로, 일부 확장 프로그램에서 숨길 수 없음
     * Chrome 설정에서 “Block sign-in prompts from identity services” 를 선택해 이 대화상자 비활성화 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Sign in with Google에 대한 전체 요약

  Google로 로그인 배너 현상

     * Yelp와 같은 많은 웹사이트에서 “Sign in with Google” 배너가 나타나 사용자 경험에 방해가 되는 현상임
     * Google에서는 이 기능을 One Tap 사용자 경험이라 명명함
     * StopTheMadness Pro와 같은 일부 브라우저 확장 프로그램에서는 이 배너를 숨겨줄 수 있음

  Chrome, Safari, Firefox의 차이점

     * Safari나 Firefox 사용자는 이러한 배너를 직접적으로 볼 수 있음
     * 하지만 Google Chrome에서는 이러한 배너가 기본적으로 표시되지 않음
     * Safari의 웹 개발자 기능을 이용해 User-Agent를 Chrome으로 변경하면, Safari에서도 배너가 사라짐

  Chrome의 별도 “One Tap” 대화상자

     * Chrome은 웹 페이지 요소가 아닌 자체 네이티브 UI로 One Tap 대화상자를 표시함
     * 이 대화상자는 StopTheMadness Pro 등 브라우저 확장 프로그램으로 숨길 수 없음
     * 대화상자가 표시되는 동안, 모든 Chrome 확장 프로그램 팝업 역시 차단됨

  Chrome 설정에서의 비활성화 방법

     * chrome://settings/content/federatedIdentityApi 주소를 통해 설정 페이지에 직접 접근 가능
     * “Block sign-in prompts from identity services” 옵션을 선택해 이 대화상자 비활성화 가능

  Google 브라우저의 자사 우대 논란

     * 이 구조는 Google이 자사 Chrome 브라우저에 우위를 주는 사례로 볼 수 있음
     * 타 브라우저에서는 사용자가 원치 않는 배너를 피하기 어렵기 때문임

        Hacker News 의견

     * 포르노 사이트 같은 곳에서도 이런 팝업이 나타나는 건 솔직히 너무 침해적임을 느꼈음, 특히 모든 방문 때마다 거의 전체화면으로 등장하는 Google 로그인 팝업에 정말 놀랐음, 익명 모드로 들어가면 매번 새 세션이니 더 자주 나타남, Reddit에서 조차 이런 제3자 팝업 때문에 사용자 경험이 망가지는 게 이해가 안 됨, 만약 Facebook, GitHub 등 다른 사이트도 다 따라 하면 한 번에 배너 4개를 닫아야 할 판임, 아마 많은 사람이 프라이버시 확장 기능을 쓰지 않아 Reddit이 계속 사용자 추적 및 일회성 닫기 저장이 가능한 듯, 혹시 이런 팝업이 실제로 재방문 이용자 감소에 영향을 줄지 아는 사람 있는지 궁금함
          + 이런 팝업은 어느 사이트든 심각하게 침해적임을 느낌, Google 같은 회사가 내가 제공하는 모든 개인정보를 수집해 내 삶의 거의 모든 측면에 대한 개인 프로필을 구축한다는 사실을 계속 상기시켜줌
          + Google One Tap 덕분에 내 SaaS 웹 앱의 신규 가입자가 하룻밤 사이 8배 늘어남, 앱의 최소 기능은 계정 없이 쓸 수 있지만 가입 시 혜택을 받고 이메일로 사용자와 소통할 수 있는 루트도 열림, 그러니 사용자와 회사 모두에게 이득일 수 있다고 생각함
          + 개인적으로 포르노 사이트 접속시에는 그와 비슷한 사이트들에만 로그인 상태를 유지함, 그 기능을 옹호하는 건 아니지만 최소한 상식적으로 비판적으로 접근하길 바람, 내가 모바일에서 이 팝업을 제일 싫어하는 이유는 사이트 로딩 직후(0.5~2초) 바로 손가락 밑에 떠서 실수로 누르게 되고, 정보가 바로 공유되어 되돌릴 방법이 전혀 없기 때문임, 데스크톱서도 싫지만 모바일에선 콘텐츠를 완전히 가리니 실수로 더 쉽게 눌릴 수 있음, 이걸 Google 계정 혹은 Google Workspace에서 끌 수 있었던 걸로 기억함, 다른 이유도 많음, Apple이나 Firefox, Microsoft, Meta 등 주요 아이디 제공업체들도 이걸 제안할 수 있겠지만, 그렇게 되면 Sign in with Google, Apple, Facebook, Microsoft, Firefox 등 5개 팝업이 동시에 뜨는 사태가 생길 것임, 이런 점에서 공유지의 비극 현상 같음, 그래도
            쉽고 빠른 가입, 로그인은 매력적이긴 해서 프라이버시에 집중한 Web API가 있다면 나쁘지 않다고 생각, 하지만 구글 특유의 팝업은 없애거나 금지됐으면 함
          + 포르노 사이트 등에서의 팝업? 솔직히 이런 서비스는 팝업, 광고, 클릭재킹 등 온갖 방해 요소가 있어도 컨텐츠가 충분히 매력적이면 사용자는 결국 계속 방문하기 마련임을 보여줌
          + 그래서 나는 Meta와 Google이 웹에서 권력이 너무 막강한 문제적 플레이어라고 보고, 중립/보유 입장으로 주식을 가지고 있음
     * Chrome에서 나타나는 경험은 Federated Credential Management(이하 FedCM)라는 새로운 표준에 기반함, 브라우저가 매개가 되어 ID 제공자와 웹앱이 필요한 정보를 주고받으면서도 인터넷상의 트래킹을 방지하는 방식임, 이 주제로 글을 쓰고 있음, 더 관심 있으면 최근 인증 컨퍼런스 영상(https://m.youtube.com/watch?v=FBAD4x7MWdI) 참고 바람(참고로 내가 진행함), 표준은 활발히 개발 중이고 Firefox도 도입 예정, Edge는 이미 지원 중임, ID 제공자 피드백 기다리는 중, 더 정보(https://github.com/w3c-fedid/FedCM), 매주 화요일 모임도 열림
          + Mozilla가 정식으로 동의한 건 아닌데 공식 표준 포지션은 중립적임(https://mozilla.github.io/standards-positions/#fedcm), 관련 이슈 트래커에서 관심을 보이지만 여러 우려도 적혀 있음(https://github.com/mozilla/standards-positions/issues/618), Apple은 3년 전에 애매하게 관심 있다고 했으나 더 진전된 입장은 없음(https://github.com/WebKit/standards-positions/issues/309), 이후 입장 변화가 있었는지 궁금함
          + 이게 새로운 표준이라면 업계 전반의 지지가 있어야 맞다고 봄, 그런데 깃허브 참여자(https://github.com/w3c-fedid/FedCM/graphs/contributors)를 보면 대부분 Google 소속임
          + 기본적으로 이메일 주소 공유를 차단할 수 있으면 좋겠음, Google 로그인 쓸 때마다 내 동의 없이 사이트/앱이 스팸 메일을 보내는 게 너무 빈번하게 발생함, 이런 이유로 나는 Google 로그인을 아예 안 씀, 이유는 스팸 때문임
          + 약간 OpenID(지금은 사라짐)를 떠올리게 하는데, 이번엔 브라우저 네이티브로 통합된 점이 다름
          + ""계속하려면 Google.com이 이름, 이메일, 프로필 사진을 공유하겠다""는 메시지가 있는데, 이게 어떻게 '프라이버시 보장 현대 표준'과 양립 가능한지 궁금함, 전혀 프라이버시 친화적으로 보이지 않음
     * 팝업 때문에 여러 번 잘못 눌러서 신뢰하지 않는 제3자에게 내 PII(개인 식별 정보)가 내 동의 없이 전송되어버림, 이건 범죄에 가까운 일이라고 생각함
          + 참고로, uBlock Origin에서 아래 필터를 적용하면 문제를 해결할 수 있음:
||accounts.google.com/gsi/iframe
##iframe[src^=""http://accounts.google.com/gsi/iframe"";]
##iframe[src^=""https://accounts.google.com/gsi/iframe"";]
##iframe[src^=""//accounts.google.com/gsi/iframe""]
###credential_picker_container

            출처: stackoverflow, 마지막 규칙은 팝업이 보여지지 않아도 내용을 막는 용도로 내가 추가함
          + 잘못 눌렀다고 너무 걱정 안 해도 됨, 어차피 Google은 웹페이지가 로딩될 때 이미 방문정보를 추적함, Google One Tap은 여기 가이드처럼 구글 서버에서 script 태그로 작동함
          + 이 팝업을 uBlock으로 몇 년째 차단해왔음
          + 이 스레드를 PM들이 꼭 읽었으면 좋겠음, 트레이드오프 판단을 잘못했다고 생각함
          + Google 계정을 삭제하면 이런 문제가 아예 생기지 않음
     * 이건 FedCM API를 말하는 듯(https://developer.mozilla.org/en-US/docs/Web/API/FedCM_API), Google 말고도 타 업체 지원이 가능하지만 아직 실제 구현은 없음, Google's One Tap library가 새로운 api를 쓰고, 미지원 브라우저에선 기존 팝업으로 대체됨, Chrome에선 ""sign in with google.com""이 뜨고 보통 One Tap에선 ""sign in with Google""이 뜨는 차이가 있음, Mozilla도 개발 진행 중이지만 시스템이 복잡해서 시간이 걸릴 듯, 인기 많아지면 Safari도 추가할 걸로 예상함
          + 나 FedCM 미팅 참석함, Firefox 쪽 담당 개발자가 꾸준히 참여 중, Safari 팀이 ""괜찮은 아이디어 같다, 검토하겠다""라고 한 포스팅 몇 개 봤으나 그 이후 별 액션은 포착 못 함, Edge는 이미 지원 중이고, 다른 Chromium 계열 브라우저에도 비교적 도입이 쉬움
          + 대체 대책이 무엇인지 궁금함, 3rd party 쿠키가 이미 차단/퇴출됐을텐데 gsi 스크립트가 어떻게 Google 세션 정보를 받아오는 건지 궁금함
     * 정말 의아한 점은 이 팝업이 DOM 구조 일부가 아니라 브라우저가 페이지 위로 직접 그려올린다는 점임, 만약 브라우저 툴바 같은 곳에 들어있었다면 신경 안 썼겠는데, 페이지 콘텐츠를 가려버리는 건 명백히 독점 금지 소송감임, 이 팝업 때문에 6개월 전 Chrome을 떠나 Brave로 넘어옴, chrome://settings/content/federatedIdentityApi와 내 구글 계정 세팅 양쪽에서 꺼도 결국 계속 띄워짐
          + 독점행위 소송 운운에 대해, 다른 댓글을 참고하면 사실 이건 오픈 스탠다드이고 Google뿐 아니라 여러 신원 제공업체에 개방된 구조임(관련 설명)
     * ||accounts.google.com/gsi/*$xhr,script,3p
       이 UBO 필터로 전부 차단 가능함, 혹은 ""annoyances"" 리스트 활성화하면 쿠키 배너 등 귀찮은 것도 숨김, 노스크립트도 추가 추천함
          + 노스크립"https://news.hada.io/topic?id=22282","Rust로 만든 Servo 웹브라우저 사용 후기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Rust로 만든 Servo 웹브라우저 사용 후기

     * 전 세계적으로 크로미엄(Chromium) 기반 브라우저의 점유율이 높아지면서, 웹 표준의 다양성과 오픈 웹의 미래에 우려가 커짐
     * Rust로 개발된 서보(Servo) 엔진은 멀티스레드 성능과 메모리 안전성이라는 두 가지 강점을 지니며, 웹 렌더링 엔진 분야에서 새로운 대안으로 주목받음
     * 아직 초기 단계이기에 대부분의 웹사이트에서 렌더링 버그가 존재하지만, 일부 데모 페이지나 위키피디아 등의 간단한 사이트에서는 정상적인 작동을 보임
     * 서보 프로젝트는 과거 Mozilla 주도로 시작되었으나, 현재 Linux Foundation Europe이 관리하며, 기술적 독립성과 커뮤니티 중심의 의사결정 구조를 갖춤
     * 브라우저 엔진 단일화 흐름 속에서 Gecko, Servo 등 대안 엔진의 지속적 개발이 웹 생태계 다양성을 지키는 데 중요하다는 시사점이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

웹 엔진의 과점화와 그 위험성

     * 1990년대~2000년대 초에는 Internet Explorer의 Trident, Opera의 Presto, Netscape의 Gecko, Konqueror의 KHTML 등 다양한 웹 브라우저 엔진이 공존함
     * 시간이 흐르면서 KHTML은 WebKit, Presto와 Trident(및 Tasman)는 Blink(Chromium 엔진)로 통합 또는 대체됨
     * 현대의 주요 브라우저(Chrome, Edge, Opera 등) 가 거의 모두 Chromium/Blink 기반이 되면서, 구현체가 곧 표준이 되어가는 현상 발생
     * 보안 취약점, 확장성 제한 등 한 엔진에 의존할 때 전체 웹 생태계가 함께 영향을 받는 문제점이 부각됨

Servo 엔진의 등장

     * Servo는 Rust로 처음부터 새롭게 개발된 브라우저 렌더링 엔진임
     * Rust의 장점인 멀티스레드 처리와 메모리 안전성을 바탕으로, 기존 C/C++ 기반 엔진이 가지는 취약점(예: 메모리 버그)을 구조적으로 줄이려는 시도임
     * Servo의 주요 목표는 임베드형 웹 렌더링 엔진으로, 독립형 브라우저 외에도 Electron이나 Android WebView의 대체재로 활용될 수 있음
     * Linux Foundation Europe 산하에서 기술적 의사결정이 대기업이 아닌 기술위원회 중심으로 운영됨
     * 10여 년 만에 처음 등장한 완전히 새로운 웹 브라우저 엔진으로, 완성도 향상을 위해 기존 주류 엔진의 경험을 반영하고 있음

서보(Servo)의 사용 경험 및 현황

     * 공식 사이트에 공개된 나이틀리 빌드(Windows, macOS, Android, Linux용)로 Servo를 체험할 수 있음
     * 북마크, 확장 기능, 데이터 동기화 등 기본 브라우저 기능 미지원 상태임
     * 대부분의 웹사이트에서 렌더링 버그가 나타나고, Google 검색이나 일부 사이트는 레이아웃이 깨지거나 크래시 발생
     * Wikipedia, CNN Lite 등 단순 구조의 페이지는 정상적으로 작동함
     * Servo 데모 페이지에서 그래픽 성능 시연이 가능하며, Particle Physics 등 벤치마크에서 최신 MacBook Pro(x86 에뮬레이션) 기준 55~60FPS의 결과를 확인하였음
     * Acid3 테스트에서는 83/100점으로, 주류 브라우저(95점 수준)보다 낮은 점수임
     * 향후 Shadow DOM, CSS Grid 등 주요 웹 표준 지원을 로드맵에 포함, 웹 호환성 개선에 중점 두고 있음

Servo의 역사와 주요 전환점

     * Servo는 2012년 Mozilla에서 시작, 2013년에는 Samsung이 개발에 합류함
     * 원래 목표는 안정화 이후 Gecko 엔진의 대체까지 고려하였으나, 현실적으로 Gecko의 각 부분을 Servo 코드로 점진적으로 대체하는 전략으로 선회함
     * Firefox 57(Quantum) 업데이트를 통해 CSS 엔진(Quantum CSS, Stylo)을 Servo 코드로 대체, 성능과 메모리 효율에서 두드러진 개선 효과 확인
     * 2020년 Mozilla의 대규모 구조조정(Servo 개발자 포함), 이후 Servo는 Linux Foundation 산하로 이관 및 자금 지원 재확보, Igalia 등 오픈소스 기업의 후원 하에 현재 커뮤니티 중심 개발 지속 중

브라우저 생태계의 미래 가능성

     * 미국 법무부가 Google의 독점적 지위(Chrome, Android) 에 대한 소송에서 승리함에 따라, Chrome 매각 및 타사 브라우저와의 검색 계약 금지 조치 논의 중
     * Mozilla는 Firefox의 기본 검색 배치 수익 의존도 높음(Gecko 개발 유지에 필수적)으로, 이러한 조치에 반대 의견 표명
     * 만약 Mozilla가 Google 수익을 상실하게 되면, Firefox가 개발 비용 절감을 위해 WebKit이나 Chromium/Blink로 전환할 가능성 존재
     * 그럴 경우 Gecko 코드의 포크 및 커뮤니티 운영 가능성, 혹은 Gecko의 점진적 쇠퇴 가능성 등 다양한 시나리오가 예상됨
     * Servo와 Gecko 등 대체 엔진의 존립이 웹 플랫폼의 다양성과 균형 유지에 중요 요소로 다시 부상함

마무리 및 시사점

     * 주류 브라우저 엔진의 통일 현상 속에서도 Servo와 같은 혁신적 대체재의 등장이 웹 생태계의 다양성과 건강성을 지키는 데 중요한 역할임
     * 단기간 내 실사용 브라우저로 완성되기는 어렵지만, 기술적 실험과 발전이 지속적으로 이루어지고 있음
     * 서보의 향후 발전 방향과 업계 내 파급 효과에 대해 많은 기대감 형성

   Hurd 프로젝트가 자꾸 떠오르는 건… 저만의 착각이겠죠?

   동작도 잘 안되는걸 받아서 쓰란건가? 그런 오만함은 대체 어디서 나오는건지.

   rust가 servo 개발하기 위해 만든 언어라고 들었는데.. servo 잘 됐으면 좋겠네요

        Hacker News 의견

     * 현재 로드맵에 Shadow DOM과 CSS Grid가 우선순위로 잡혀 있음, 나는 CSS Grid 지원 작업을 하고 있고, 곧 ""named grid lines and areas"" 지원이 추가될 예정임, 이것으로 더 많은 웹사이트 레이아웃이 제대로 동작할 거라 기대함, 내 프로젝트라서 편파적일 수도 있지만, Servo가 CSS Grid를 구현하는 방식이 꽤 멋있다고 생각함, 핵심 구현이 외부 라이브러리(Taffy, GitHub 링크)로 분리되어 있고, 이 라이브러리는 Rust UI 생태계에서 폭넓게 사용되고 있음, 예시로 Blitz(링크) 웹 엔진, Zed(링크) 텍스트 에디터, 그리고 Bevy(링크) 게임 엔진에서 Flexbox, Block layout 등 다양한 역할로 쓰임, Servo가 Stylo, html5ever 등의 모듈형 라이브러리 개발 경험을 바탕으로, 독립적 모듈과 공개 API로 웹 엔진을 쪼개는 방식을 취하고 있는 점이 앞으로 웹 엔진 개발자 진입 장벽을 낮게 해주고, 신규 웹엔진
       개발자에게 큰 도움이 될 거라 희망함
          + Blitz에 대해 처음 들어봄, 상당히 흥미롭고 야심찬 프로젝트로 보임, 마치 진짜 '숨은' 웹 엔진 같은 느낌을 받음, Servo는 Rust가 처음 나왔을 때부터 널리 알려졌지만, Blitz는 그에 못지않게 인상적임
          + 웹 브라우저 엔진 기능을 직접 구현한 경험이 HTML이나 CSS 작성 방식에 영향을 주는지 궁금함, 여전히 한 주에 세 번씩 ""css grid cheatsheet""를 검색하는지도 묻고 싶음
          + 기능을 쪼개서 모듈화하는 접근이 오히려 기능 과잉이나 단편화로 이어지지 않을까 걱정됨, 구글에 맞서려면 집중력 있는 전략이 중요한데 이 부분이 염려됨
          + taffy를 활용해서 내 작은 Rust 기반 이잉크 달력에서 레이아웃 잡고 있음, 이런 소식이 매우 재미있게 들림
          + 웹 엔진을 독립적으로 사용 가능한 공개 API 모듈로 나누는 방식을 정말 좋아함, 예전에 WebRTC를 살펴보면서 왜 허접한 Skype 짝퉁을 만드는 게 50줄짜리 JavaScript 또는 일주일짜리 크로미엄 C++ 빌드 악몽 둘 중 하나여야 할까 생각했었음, 이제 WebRTC Rust crate도 생겼으니 웹 앱만 이런 투자의 수혜를 보는 건 아니어서 다행임
     * Mozilla가 Xerox처럼 ""미래 기술을 만들었다가 무심코 경쟁자에게 빼앗긴 기업"" 명예의 전당에 들어갈 것 같은 느낌임, Rust와 Servo로 브라우저 개발에서 한때 구글을 앞질렀지만, 결국 계속 추진하지 않은 것이 정말 이해가 되지 않음
          + Mozilla는 Xerox와 다름, 만약 누군가가 새로운 Xerox라면 오히려 Google임, 구글은 엄청난 자금으로 사업 계획 없는 연구개발 부문에 투자하고 있음, 대표적 예로 트랜스포머 모델—사실상 LLM을 구글이 먼저 만들었음에도 OpenAI에게 결국 뒤처짐, Mozilla의 성공은 언제나 넷스케이프, Firefox 등 웹 브라우저에 집중되어 있었음, Rust 역시 본질적으로 브라우저를 위해 만들어진 언어임, 그게 다른 데서 유용하게 쓰이는 건 좋은 부수적 효과일 뿐임
          + 구글이 2006년부터 Mozilla의 주요 수입원이었음, Mozilla가 존속하려면 구글을 만족시키면 됨, 이는 충돌의 소지가 있지만 Mozilla 입장에서는 꽤 괜찮은 거래임
          + Mozilla는 이제 끝이라 생각함, 구글에 너무 의존했고 그것마저 잃을 상황임, Servo와 Ladybird가 앞으로의 미래가 될 것이고, Ladybird가 소수 인원으로 빠르게 발전하는 모습이 정말 인상적임
          + 만약 Mozilla가 Gecko를 포기한다면, 그때는 하드 포크와 Mozilla에서의 탈출을 감행해야 할 시점임, 참고로 말하는 Gecko 포기는 Servo가 아니라 Chromium으로 전환하는 걸 뜻함
          + 사람들이 브라우저에 비용을 내는 일은 어렵다는 느낌임, 10유로 맥주에는 돈을 잘 쓰지만, WhatsApp 평생 0.99 유로 라이선스를 피해가려는 사람들이 내 주변에 많았음
     * Mozilla가 Firefox 기술적 미래를 포기한 게 여전히 이해가 안 됨, 그런데 Mozilla를 바라볼 때 결국 자금 흐름을 보면 여러 가지가 이해됨
          + Pocket 서비스 종료도 슬픈 사례임, 만우절 농담으로 Mozilla가 Firefox를 단종하고 핵심 사업에 집중하겠다고 발표하면 어떨까 상상해봄, 인기 많은 제품을 아름답게 단종시키는 '플라토닉 아이디어'가 진짜 핵심 사업인 것 같다는 씁쓸한 농담임
          + 여러 이탈과 커뮤니케이션 방식을 봤을 때, Mozilla는 당시 극도로 정치적인 회사였던 것 같음, Servo는 워낙 소통이 활발했고 Rust도 분위기가 좋았으니, 오히려 그것 때문에 위쪽에서 신경이 쓰였을지도 모른다고 생각함, Mozilla는 여전히 구 인사들이 중심적으로 운영하고 있고, 최고경영진 실수 탓이 크다고 여겨짐
          + 아직도 Servo가 크롬/크로미움 독점에 실질적 대항마가 될 수 있다고 강하게 믿음, Mozilla가 왜 이걸 포기했는지, Linux Foundation이 왜 지원을 거의 하지 않는지 잘 모르겠음
          + Mozilla의 전략을 장기적으로 Google 자금의 독립이라는 관점에서 보면 논리가 있음, 실제 비 Google 수입이 2022년 8천만 달러에서 2023년 1.5억 달러로 증가했음, 총 자산도 매년 1억 달러씩 늘어나서 2023년에는 10억 달러에 이름, Google 자금의 비중은 2020년 90%에서 2023년 75%로 낮아지는 등, 완벽히 독립적이진 않지만 방향성이 있음, 사람들이 흔히 말하는 것과 달리 자금흐름만 보면 결론이 다를 수 있음, 사람들이 Servo를 이어갔어야 한다고 비판하지만, 사실 Quantum 이후 Firefox에 Servo가 큰 역할을 한 건 아니라고 생각함
          + Mozilla 수입 대부분이 Firefox에서 온다는 게 맞는지 질문함
     * Blink 독점에 도전할 Ladybird(공식 홈페이지)도 있음, 앞으로 어떻게 될지 확실하진 않지만 기대 중임
          + Ladybird의 목적을 잘 모르겠음, 전임 엔지니어도 있고 기부금도 모으는 등 단순한 취미 프로젝트가 아님은 분명함, 그런데 기능, 보안, 성능 면에서 Blink, Webkit, Gecko와 경쟁이 가능할지 상상이 잘 안 됨, 원체 대규모 팀들이 아니라서 대중적 채택이 없으면 엔진만 만드는 게 큰 효과가 없다고 생각함, 이런 부분은 Servo에도 해당되지만 Servo는 기술적 목표가 좀 더 설득력이 있음, Ladybird에는 그런 명확한 정보가 잘 안 보임
          + 기술적으로만 보면 Ladybird의 매력을 잘 모르겠음, 기본적으로 C++로 짜여 있어서 Gecko, Blink와 차별성이 없어 보임, Servo는 보안, 병렬성 등 명확한 기술적 설계 차별점이 있어서 매력이 있음
          + Ladybird가 성공하길 바라고 있음, 이제 직접 후원도 가능해져서 의미 있다고 생각함, Mozilla와 달리 모인 기부금은 전적으로 브라우저 개발에 쓰일 것이라 확신함
          + Ladybird가 Swift로 전환 시도 중인지 궁금함, C++로 브라우저 엔진을 개발한다면 별로 마음에 들진 않지만 현실적이고 결과물에 집중한 느낌임, Rust라면 미래지향적인 느낌이라 그 점을 인정하겠음, 하지만 Swift 등 다른 언어로 엔진을 만드는 건 회사 내부 정치적인 결정 같아 보임, 지원 생태계가 아직 부족한 언어를 굳이 선택하는 점에 의문이 듦 (참고 트윗)
     * Dogemania 테스트가 M4 Pro MacBook Pro에서 400장 이미지까지는 부드럽게 60 FPS로 동작함, 그런데 Chrome에서는 1400장까지 60 FPS가 유지되었고, 그 이상은 진행이 귀찮아 닫아버림
          + 나도 같은 실험을 해봤는데, Firefox와 Chromium의 차이가 실망스럽게 큼, Chromium에서는 1000장 넘어서도 100 FPS가 계속 나왔는데, Firefox로는 500장 넘어서 바로 60 FPS 아래로 떨어짐, 완전히 공정한 테스트는 아니었겠지만(Chromium엔 애드온도 없고 잘 안 쓰는 브라우저) 비교 성능을 확인하는 데 많은 걸 느꼈음
          + dogemania에서 이미지가 애니메이션 이후 가만히 있다면, 정말 쉽게 최적화할 수 있는 사례 아닌가 궁금함, Amiga 같은 예전 컴퓨터도 무한정 이 정도를 처리할 수 있을 것 같은 생각이 듦
     * Servo를 '새로운' 엔진이라고 부르는 건 좀 억지임
          + 그래도 나머지 엔진보다 늦게 시작했고, 아직도 경쟁 엔진에 없는 좋은 아이디어가 많아 보임
     * 나는 Rust가 웹 브라우저 Servo를 위트는 uBlock Origin을 쓰면 사실상 중복임, 자세한 비교는 여기 참고 바람
해 만들어졌다고 알고 있었음
     * 다수와 달리 나처럼 이걸 좋아하는 사람 있음, 회원가입 절차가 너무 귀찮고 못 만든 경우가 많기 때문에 이를 건너뛰는 경험이 마음에 들었음
     * “표준의 유일한 구현체가 하나만 남으면 그게 곧 표준이 되는 위험이 있다”는 의견에 대해, 그게 왜 문제인지 잘 모르겠음, 구현체가 오픈소스고 여러 주체가 관리하는 위원회에서 사양을 정한다면 괜찮다고 여김, 지금처럼 리소스를 쏟아부어 여러 엔진 만드는 상황이 오히려 낭비임, 브라우저 엔진도 리눅스 커널처럼 하나만 두고 배포판만 다양한 방식이 효율적이라고 생각함
     * Safari나 Firefox를 쓸 땐 이 배너가 안 뜬다는 점을 알지 못하는 경우가 많음, 그래서 많은 사이트가 괜찮다고 느끼는 걸지도 모름, 법원이나 반독점 규제 기관이 이런 사례를 주시해야 함, Google이 Chrome을 통해 자사 브라우저에 유리하게 할 뿐 아니라 SSO/데이터 수집 플랫폼에서도 자신들에게 유리하게 만드는 대표적인 예임
     * 웹사이트 콘텐츠 영역 위에 오버레이를 띄우는 걸 금기시한다고 알고 있었는데, 왜냐하면 누구나 HTML/CSS로 그럴듯하게 가짜 다이얼로그를 만들고 악용할 수 있기 때문임
          + 아무리 의도가 좋아도 구현에는 버그와 사소한 결함이 남음, 두 번째 독립 구현체가 없으면 결국 “모두 된다”면서 버그 자체가 표준이 되어버림, 웹 페이지가 이런 버그에 의존하면 결국엔 사양이 아니라 특정 구현의 세부 결함에 따라 움직이게 됨, MS Windows에 구버전 UI가 여러 레이어로 쌓이고 호환성 유지가 악몽이 된 이유가 이것임, 여러 독립적 구현이 있으면 표준이 실제로 유지/진화 가능하고 최적화도 쉬워짐
     * ‘Sign in with Google’ 프롬프트가 매우 싫음, 실수 혹은 무심결에 눌러 내 이메일 주소와 신원이 신뢰하지 않는 임의의 웹사이트에 공유될까봐 걱정됨, 브라우저가 신원 관리를 해주는 아이디어는 꽤 괜찮은데, 이메일 공유 등 다른 정보까지 엮이는 현재 구현은 사용자의 실수 유도를 키워서 진정 사용자 친화적이지 않음
          + 이론적으로는 괜찮지만, 실제로는 단일 개발자가 유저와 이익이 항상 일치하지 않아서 독점이 바람직하지 않다고 생각함, 특히 최근 Blink 기반의 manifest v2 폐기도 유저들이 싫어했음
"
          + 브라우저 엔진이 많을수록 보안 취약점이 하나에 몰아치는 위험이 줄어듦, 메모리 안전 언어라도 논리적 실수는 피해를 줄 수 있으니 다양성이 중요함
          + “하나의 엔진에 여러 배포판” 전략을 얘기하셨는데, BSD 환경이나 ZFS 등에 익숙한 사람에게는 오히려 예전 방식을 벗어나 봤을 때 안정감과 완성도가 확연히 느껴짐
          + 이미 구글이나 그쪽 인맥의 의견이 표준화에 크게 반영되고 있음, 모든 것이 Chromium에 의해 굴러가면 상황이 더 악화될 것, 어쩌면 정말 파국이 와야 W3C, WHATWG 같은 곳의 한계를 다들 인정할 것임
     * “대부분 웹사이트가 약간의 렌더링 버그를 가지고 있고 일부는 완전히 작동하지 않는다, 구글 검색결과엔 겹치는 요소가 많고 MacRumors 홈은 스크롤 후 크래시가 난다, 그러나 Wikipedia, CNN Lite, 개인 사이트, NPR 텍스트 등은 완벽하게 동작” — 이런 관찰을 봤을 때, 구글이나 MacRumors 쪽을 Servo에 맞게 고쳐야 한다고 말하는 사례를 거의 본 적 없음, 대신 Servo가 크롬/크로미움처럼 동작해야 한다는 결론만 나오곤 해서 아쉬움이 큼, 그렇게 되면 (a) 결국 광고기업이 표준을 정하게 되고, (b) 웹페이지 만든 사람들이 잘못된 동기를 가지게 됨, 위키백과나 CNN Lite 처럼 쉬운 쪽으로 맞추는 게 오히려 간편함, 나는 '표준' 브라우저가 인기순이 아니라 진짜 표준에 가까운 걸 지향해야 한다고 생각함, unpopular 브라우저와 popular 브라우저 모두에서 돌아가게 하는 게 진짜
       표준임, 결국 답은 웹 브라우저가 아니라 웹 페이지를 고치는 데 있다고 주장함, 나는 아직도 w3c의 원조 libwww 라이브러리와 도구로 실험하고 있음, 약간만 수정하면 지금도 텍스트 기반 웹 페이지 최적화 용도로 30년 넘게 잘 동작함, 인터넷은 공공재인데 오늘날 브라우저와 페이지는 광고 최적화, 개인정보 수집 쪽으로 너무 치우쳐서 아쉬움, 진정으로 www 유저를 위한 페이지/브라우저가 우선이어야 의미 있다고 생각함, 아래는 libwww로 정적 바이너리 빌드하는 간단 스크립트임
          + (이전 Python 스크립트에 역슬래시 누락된 걸 바로잡음, libww도 libwww로 오타 정정함)
     * Mozilla가 경쟁적인 브라우저 회사에서 점점 활동가 느낌의 조직으로 변해가는 모습이 정말 안타까움, 그래서 핵심 제품이 점점 약해진 것 같아 씁쓸함
"
"https://news.hada.io/topic?id=22248","AI가 개발자 생산성에 미치는 영향 - 스탠포드 연구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AI가 개발자 생산성에 미치는 영향 - 스탠포드 연구

    서론 ‒ AI 개발자 생산성에 대한 오해와 현실

     * 마크 저커버그(Meta CEO) 가 ""2025년 말까지 미드레벨 엔지니어를 모두 AI로 대체하겠다""고 선언하면서 각 기업 CTO들이 같은 압박을 받음.
     * 연사는 ""AI가 개발자를 전적으로 대체할 수 없다""는 점을 분명히 하며, AI 도입이 생산성에 분명히 도움을 주지만, 항상 그런 것은 아니고, 오히려 생산성이 감소하는 경우도 드물지 않다고 강조.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    연구 설계 및 데이터

     * 3년간, 600여 개 회사, 10만 명 이상의 소프트웨어 엔지니어, 십억 줄 이상의 코드, 수천만 건의 커밋 대상으로 측정.
     * 대부분은 비공개 저장소(Private Repo) 기반 데이터 → 개발팀과 기업 단위의 실제 생산성 변화 측정 가능.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    기존 연구의 한계

     * 벤더 주도의 보고서는 자사 AI툴 홍보 목적이 많아 객관성 부족.
     * 단순 커밋/PR 개수, 평균 작업 시간 변화 등은 실제 생산성을 왜곡할 수 있음.
          + 예: AI 사용 직후엔 버그나 재작업(rework)성 커밋이 함께 증가하여 피상적으로만 생산성이 높아진 것처럼 보임.
     * ""그린필드(새로운 프로젝트) 실험""에서는 AI의 효과가 매우 커 보이나, 기존 코드(브라운필드)에서는 차이가 줄어듦.
     * 개발자 설문(Self-report)은 실제 생산성과 큰 상관성이 없음(30%포인트 이상 체감과 실제 간극).
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Stanford의 생산성 측정 모델

     * 전문가 패널 평가와 유사한 AI 기반 자동화 모델 활용:
          + 커밋별 기능적 변화(added/removed/refactored/rework) 측정
          + 단순 ""라인 수""가 아니라 ""기능, 유지보수성, 품질""에 초점
     * 기능 도입량, 리팩터링, 최근 커밋의 재수정(rework) 비중 등 정밀함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    주요 연구 결과

     * 전체적으로 AI 도입 시 평균 생산성 15~20% 증가.
          + 하지만 상당량의 ""재작업""(rework)이 동반되어 체감적 효익이 과장되는 경향도 있음.
     * 팀/회사/과제 유형별로 큰 편차 존재.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    생산성 격차의 원인: 과제 난이도·프로젝트 종류·언어·코드베이스 크기

    프로젝트 유형    낮은 복잡도   높은 복잡도
   그린필드 (신규)  30~40% ↑ 10~15% ↑
   브라운필드 (기존) 15~20% ↑ 0~10% ↑

     * 복잡성이 낮고, 신규(그린필드) 프로젝트에서는 AI의 효과가 큼.
     * 기존(브라운필드)·복잡한 시스템은 효과가 뚜렷하게 줄고, 일부 케이스에서는 생산성 감소 사례도 발견.
     * 실제 27개 기업 136개 팀 샘플 기준(강의 내 언급).
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    언어 인기와 AI 효과

     * 파이썬/자바/자바스크립트(인기 언어) 에서는 AI의 생산성 향상 크며,
     * COBOL/엘릭서/하스켈(비인기 언어): AI의 도움 거의 없고, 오히려 복잡한 작업에선 시간 낭비-생산성 저하까지 유발.
          + 예: 비인기 언어 & 높은 난이도의 경우 ""오류 많고, 맞는 코드 못 내놓음"" → 없는 게 나음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    코드베이스 크기와 AI 효과

     * 코드베이스가 클수록 AI의 생산성 향상효과가 급격히 줄어듦.
          + 원인:
               o 컨텍스트 윈도 한계: LLM이 여러 파일/수십만~수백만 라인 전체 맥락을 다 넣을 수 없음.
               o ""시그널/노이즈 비율"" 감소: 맥락 정보가 많아질수록 AI가 적합한 정보를 제대로 식별하지 못함.
               o 도메인별, 서비스별 복잡도가 많아질수록 실제 재구현 난도 상승
          + 실제로 최신 대형 LLM(Gemini 1.5 Pro 등)은 ""토큰 수""가 늘어날수록 정확도가 90%→50%미만으로 급락.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    총정리: 언제 AI가 효과적인가?

     * AI는 대부분의 경우(특히 간단한 신규 코드 작성)는 분명히 개발자 생산성을 향상시킴.
     * 하지만 복잡한 유지보수, 오래된(대형) 코드, 비주류 언어, 많은 의존성이 있는 환경에서는 한계 크고,
     * 최상의 AI 생산성 전략은 회사·팀·과제 특성에 맞추어 신중하게 설계해야 함(""원 사이즈 핏 올"" 아님).
     * 설문, 마케팅 지표, 단순 커밋 수 증가는 신뢰 어렵고, 실제 코드 기능성과 반복작업 비율, 재작업량 등 ""현실 기준 측정"" 필요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    부가적 코멘트 및 사례

     * ""고스트 엔지니어"": 우리 데이터 내 10% 엔지니어는 거의 일하지 않고 급여만 받는 사례도 발견.
     * 팀장·CTO가 실제 문제를 빠르게 진단하기 위한 ""지표 기반 의사결정"" 도구의 필요성 강조.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    결론

     * AI는 ""대다수 상황""에서 생산성 증가하지만, 과대평가 또는 과소평가 모두 주의해야 함.
     * 도입 성과가 잘 나는 ""구체적 조건(간단/신규/인기 언어/작은 codebase)""을 파악해서 적용해야 하며, 무비판적·무차별적인 도입은 오히려 역효과가 날 수 있음.

   메인 소프트웨어에 적용하지 않더라도, 테스트 프로그램이나 프로젝트 시작 단계에서 AI가 시간을 많이 절약해 줍니다.

   코드를 짜주는것을 차지하더라도 어시스턴트 기능이 AI 도입되고나서 하늘과 땅 차이로 바뀌었다고 봅니다. 이제는 AI가 선택이 아니라 필수인 시대가 되었다고 봅니다.

   실제로 MVP 에서 많은 도움이 됩니다. 특히 주석, 로깅, 히스토리 작성에서는 무조건 써야 된다고 생각합니다.
   다만 코드베이스가 커지면 환각과 기존 코드를 잊어버리고 이상한 결과를 만듭니다. 컨텍스트 엔지니어링을 사용하거나 코드베이스를 줄이는 방법을 고민해야 합니다.
   아마 좀더 큰 프롬프트를 사용할 수 있으면 개선될 것같습니다.
   현재 자바, 파이썬, 자바스크립트, 고랭은 잘 되는 것같습니다. 코파일럿 주로 쓰고 클로드와 챗지피티 씁니다.
"
"https://news.hada.io/topic?id=22324","원격 근무를 한다면, Ramblings 채널을 만드세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     원격 근무를 한다면, Ramblings 채널을 만드세요

     * Ramblings(주절주절): Remote 팀을 위한 개인 저널 채널
     * 원격 근무 팀(2~10명) 에서는 각 팀원 이름으로 된 개인 Ramblings 채널을 팀 채팅 앱에 생성하는 것을 추천함
     * Ramblings 채널은 팀 내 개인 저널 또는 마이크로블로그처럼 동작하며, 팀 대화방을 어지럽히지 않고 자연스러운 소통을 유도함
     * 자유롭게 생각, 아이디어, 일상, 피드백, 여행 사진 등을 올리는 공간으로 활용하며, 자연스럽게 팀원 간의 사회적 유대감을 증진시키는 목적
     * 채널 운영 방법
          + 각 Ramblings 채널은 본인만 상위 글 작성이 가능하고, 동료들은 스레드(댓글)로만 답변할 수 있음
          + 모든 Ramblings 채널은 채널 리스트 하단 Ramblings 섹션에 모아두고, 기본적으로 뮤트(알림 꺼짐) 상태를 유지
          + 다른 팀원이 반드시 읽을 필요는 없고, 자연스럽게 서로의 생각을 엿볼 수 있는 공간
     * 운영 팁 : 주로 1주일에 1~3회 정도 짧은 업데이트를 자유롭게 작성
          + 현재 프로젝트와 관련된 아이디어
          + 블로그 포스트, 기사, 사용자 피드백에 대한 생각
          + ""만약에"" 유형의 새로운 제안
          + 최근 여행 사진이나 취미 생활 공유
          + 문제 해결 과정에서 러버덕 디버깅 식 독백 기록
     * 실제 운영 경험과 효과
          + Obsidian 팀에서 2년간 실험, 정기 회의 없이도 ‘워터쿨러 토크(사무실 잡담)’ 역할로 성공적이었음
          + 깊이 있는 집중 시간을 보장하면서도, 동료 간의 연결감과 창의적 아이디어 교류를 자연스럽게 촉진함
          + Ramblings를 통해 실제로 기능 아이디어, 프로토타입, 문제 해결책 등 다양한 창의적 결과물이 등장
          + 연 1회 팀 오프라인 미팅 외에는 Ramblings 채널이 팀의 인간적 연결고리 역할을 하며, 팀원 간 유대감 강화에 도움을 줌

        Hacker News 의견

     * 2014년에 Cloudflare에서 DDoS 대응 업무를 했을 때, James(Jog)라는 동료와 함께 많은 질문을 주고받으면서 협업 경험을 쌓았음. ""서버에 로그인하는 법"", ""anycast가 뭐냐"", ""이건 어떻게 대응했는지 구체적으로 설명해달라"" 같은 질문들을 반복하다 보니, 이 대화들이 새로운 직원들에게도 유익할 수 있다고 느낌. 온보딩과 관련된 고민, 잘 알려지지 않은 워크플로우, 이론적 개념 등 다양한 주제가 있었음. 그래서 이러한 질문들을 회사 내부 공개 채널에 모으기 시작했고, 처음엔 “Marek's Bitching”이라는 이름으로, 불평이나 귀찮은 질문을 실명으로 던질 수 있는 공간을 마련함. 점차 여러 동료들이 참여하게 되었고, 회사 내에서 그간 다른 채널에선 다루기 힘들었던 기술 토픽이나 자유로운 논의와 사소한 의심, 추측(예: 인텔 펌웨어 버그 검증)까지 자연스럽게
       모이면서 큰 가치를 창출함. 이후 이름을 “Marek's technical corner”로 바꿨고, 10년 넘게 기술문화의 구심점 역할을 하고 있음. 이런 식의 자유롭게 불평, 질문, 토론할 수 있는 “내 채널”이나 팀별/지사별 ""rambling/bitching"" 채널의 존재가 조직 내 소통과 성장에 특히 도움이 된다는 것을 강조함
          + 내가 바로 그 James(Jog)임을 밝힘. 당시 재밌는 대화가 많아서 즐거웠음. 이후로 회사에 내부 블로그 문화도 형성되어서 다들 실험이나 발견을 자연스럽게 공유하는 흐름이 유익했다는 의견을 전함. 내부 블로그를 구독하다 보면 정말 배울 게 많았다는 소감임
          + 작성자의 주장을 이해는 하지만, 관리자 입장에서 보면 특정 개인을 중심으로 한 Q&A 형식은 검색성, 발견성, 그리고 신규 입사자들에게 특정 인물이 프로젝트의 중심처럼 보이게 해버린다는 점에서 문제가 있다고 여김. 질문하는 공간은 꼭 있어야 하지만, 사람 이름이 아닌 주제 기반의 채널로 논의를 유도해야 효율적이라고 생각함. 누가 무작위 채널(예: #random, #general)에서 기술적 질문을 던질 때마다 항상 프로젝트별 채널로 유도하고 있음. 모두에게 관련된 대화와 정보는 가장 적합한 채널에서 모이게 해주는 습관이 필요하다고 강조함
          + 우리 조직에는 자연스럽게 생긴 ""Study Hall""이라는 채널이 있는데, 여기서는 기술 질문도 자유롭고, 비판이 없어서 팀 내에서 생산적으로 운영되고 있음. 가장 효율적인 채팅 채널 중 하나라고 느낀 경험을 공유함
          + 게시글에서 언급된 “Marek's technical corner”는 지금까지도 남아 있고, 여전히 활동이 조금씩 이어지고 있다는 점을 알림
          + 참고로, “Marek's technical corner”는 지금도 여전히 살아 있고, 종종 활발하게 쓰인다는 사실을 언급함
     * 게시글을 읽고 나서, 원격근무 환경에서 자율적인 “rambling”의 중요성을 추천하고 싶음. 아침마다 약 한 시간 정도 산책하며 개인 시간을 가지는 것이 출퇴근과 집의 경계, 루틴성 운동, 산책 중 드는 생각 등으로 가치가 큼을 강조함
          + 아이를 학교에 데려다주는 시간이 사실상 비슷한 역할을 한다고 느낌. 가능할 때 딸과 함께 개 산책하며 버스정류장까지 같이 가고, 예전에는 보육원까지 왕복 45분 걸어가던 경험을 들려줌
          + 완전 원격으로 일하면서 때때로 “나는 왜 아이를 데리러 가지 않나, 몸이 안 좋아 그냥 쉴 때 연락이 끊긴다, 교통체증 때문에 늦을 때도 없다, 잡다한 사정으로 늘 일찍 나가는 일도 없다”고 자책할 때가 있음. 모두 합리적 사유일 수 있지만, 이런 이야기를 매일 들으니 웃기기도 함. 실제로 내 일에 만족하고 딱히 게으르게 일하고 싶지도 않음
          + 이 방법이 진짜 효과 있음을 내 경험으로 증명할 수 있음. 집에서 불안할 때 30분 정도 동네 공원 걷는 것만으로도 기분이 훨씬 나아짐. 얼마를 걸어도 상관없다는 점, 걷는 것 자체에서 얻는 해방감이 크다는 점을 강조함
          + 하루 중 가장 좋은 시간이 10시쯤 밀짚 모자를 쓰고 옷을 벗고 동네를 산책하면서 햇빛을 온몸으로 받는 시간임. 핸드폰도 없이 걷는 자유로움이 정말 좋음
     * 나 역시 여타 댓글러들과 공감하는 면이 있지만, 임의로 “rambling”을 공식 배정하는 방식은 오히려 디스토피아적이라고 느끼는 입장임. 이런 채널/공간은 자연스럽게 자생적으로 생기는 게 제일이며, 생각나는 순간, 적합한 동료에게 공유하면서 점차 그룹챗이 커지고, 더 중요한 이야기는 전체 팀에 확장되는 과정을 거친다고 생각함. 이런 논의 자체는 건강하고 원격 팀에 필수적일 수도 있지만, 팀마다 자율적으로 소통 방식을 만들어가야 한다고 믿음. 인위적인 제도화는 필요 없다고 봄
          + “이번 주에 당신은 15번 수다를 떨었군요” “최소가 15잖아요?” “음, 그게 최소죠. 하지만 Todd는 벌써 37번이나 쏟아냈어요” “37번을 진짜 원했다면 그 수치를 최소 기준으로 하지 그랬어요?”와 같은 농담으로, 양적 기준을 세우면 생길 수 있는 부작용을 재치 있게 지적함
          + 이런 “센세이셔널한 경력 조언”이 의도가 좋더라도 과장되거나 오해를 부를 수 있다는 점을 경험적으로 공유함. 종종 주니어들이 글을 문자 그대로 실천에 옮기다 보니 동료, 매니저들이 더 혼란스러워하는 경우도 목격함. 그래서 나 같은 매니저는 Reddit 등에서 이런 글이 돌 때 대충 읽어보고, 만약 팀원 중 누가 갑자기 그런 행동을 하기 시작한다면 그 배경을 쉽게 파악해서 미리 상황을 해소하는 데 도움을 얻었음
          + “채널”이 너무 무작위적이라 별로라고 생각함. 예전 회사에서는 Confluence에서 “Personal Space”를 만들어 개인 블로그 형식으로 생각을 정리했고, 지금도 새로운 회사에서 그 방식을 유지함. 코드 리팩터링 등 큰 결정을 앞두고 영어로 생각을 먼저 정리하다 보면 코딩에만 매달리는 것보다 훨씬 명확해짐. 팀원들에게 내 계획을 공유하고, 피드백을 받을 수도 있음. 또한, 새로운 알고리즘이나 분석 방법을 구상할 때 LaTeX 수식 지원이 되는 블로그 소프트웨어를 활용해 아이디어를 정식 수학 표기법으로 풀어내곤 하는데, 영어로 정리해서 수식으로 옮기는 과정에서 오류나 잘못된 가정도 미리 발견함. 이런 방식으로 반쯤 공개된 곳에 글을 올리는 게 실수도 줄이고 생각을 더 구조화하는 데 큰 도움이라고 생각함. 전체 구성원 중 몇 명만 봐도 상관없는데,
            출판 버튼을 누르기 전까지 충분히 생각할 시간을 확보할 수 있어서 “생각의 흘러가기”보다 실질적으로 도움이 되는 아이디어를 만들 수 있음
          + 만약 이런 활동이 “수다를 할당받는 것”이나 “성과 지표”가 된다면 정말 최악이라는 데 동의함. Obsidian에서의 사례는 자연스럽게 떠오른 실천이었으며, 작은 팀과 수평적 구조였기에 가능했음. 그리고 글에서 “채널은 기본적으로 음소거 상태, 누구도 읽기를 강요하지 않는다”고 명시한 부분이 더 핵심이라는 의견
          + 이 글에서 어떤 의무나 강제성, 할당이 있다는 내용은 전혀 없다고 주장함. 오히려 문제는 이렇게 높은 가치의 비공식 채널이 “거의 결코” 자연발생적으로 만들어지지 않는다는 점임
     * “수다” 채널이 일반 사무실의 워터쿨러 토크와 같다는 말에 공감함. 하지만 현실적으로 대부분의 팀은 매일 정기 회의가 이미 스케줄에 있어서, 수다 채널이 자연스럽게 필요해서 생기는 게 아니라 “업무상 해야 할 또 한 가지”로 느껴질 위험이 크기 때문에 실제로 잘 작동하지 않음
          + 나는 직접 수다 채널을 사용한 적 없지만, 일상적으로는 “수다”를 많이 함. 오히려 가장 귀찮은 건 정기 미팅임. 데일리 같은 회의에서 새로운 내용은 거의 없고, 이미 알고 있는 얘기를 반복하느라 시간만 낭비함. 일상적으로 얘기할 건 회의 기다리지 않고 바로 공유함. 또한 비공식 모임에서는 스크럼 마스터들이 “이건 딴 데서 하자”라고 아이디어의 싹을 자르지 않아서 자유로운 논의가 가능함
          + 팀이 스스로 사회적 상호작용의 필요성을 자각해야만 이러한 채널이 의미가 있는지는 팀의 성숙도에 달려있음. 정기 회의가 곧 워터쿨러 토크가 되는 것은 아니라서, 회의가 있더라도 자연스럽게 사회적 소통이 그리워질 수밖에 없음
          + 우리 완전 분산팀은 일주일에 두 번, 15~30분 동안 업무 얘기 없이 교류하는 “수다 미팅”을 스케줄로 넣고 있음. 이는 효과가 매우 크다고 느낌. Slack에 별도 수다 채널도 유지하고 있어 거의 하루종일 활성화됨
          + 코로나 이후 원격 회사를 다녀봤는데, 매일 회의를 하더라도 수다 공간이 별로 있다는 게 오히려 좋은 아이디어라고 느낌. 우리는 대개 회의는 최대한 온타픽만 다루려 하고 있음
          + 요즘은 하루 중 30~50%를 회의에 쓰다 보니, 누군가 항상 “이건 따로 얘기하자”, “나중에 다시 논의하자”는 말을 반복하게 됨
     * 우리 팀도 모르게 자연스럽게 따로 “rambling” 채팅방을 만들었음. 메인 그룹 챗에서는 “별로 똑똑하지 않은” 질문을 하거나 불평을 올리기 부담스럽기 때문에, 매니저 없는 두 번째 채팅방을 만들어, 보기엔 아무 문제 아닌 사소한 질문, 답을 기억 못하는 절차, 조금은 프로답지 않은 불평, 서비스/툴/프로세스에 대한 즉석 비판 등 진짜 속풀이가 가능하게 함. 메인 채팅에서는 질문이 해소되면 이후의 대화가 산만하다고 중단되지만, 이 방에서는 그 누구도 방해하지 않고 자유롭게 입장해 얘기할 수 있는 점이 가장 좋음. 이런 공간이 팀에 꼭 필요하다고 느낌
          + 일부러 “바보 같은 질문”을 자주 하는 이유는, 항상 누군가는 눈치 봐서 말을 못 하는 경우가 있어서이고, 이를 통해 더 깊은 논의로 이어질 수도 있기 때문임
     * 회의적인 입장으로, 이런 채널이 결국 읽을 거리만 늘어나고, “팀 소통을 잘 하고 있다”는 인상만 남이게 된다는 생각이 있음. 원격 팀의 사회적 결속을 만들려는 노력은 좋으나, 내게 큰 스트레스를 주는 ‘팀 커뮤니케이션 체크’와 섞는 건 별로라고 생각함
          + 하지만 게시글에서는 “채널을 음소거하고, 참여를 기대하지 않는다”고 명시되어 있다는 점을 지적함
          + Slack 미확인 메시지를 양치하거나 티메이커를 기다리면서 쓱 읽는 게 취미임. 일 좀 하기 싫을 땐, “to-do”나 “done/ignore”로 빠르게 처리하는 일종의 “업무용 Tinder”처럼 씀
          + 진짜로 Slack에서 인박스 제로를 지키는지 궁금함. 그 자체로 별도 업무가 될 것 같은 느낌임
     * 어떤 리더는 팀 내 커뮤니케이션이 통제 밖에 있으면 위협을 느끼기도 함. 이런 리더 밑에서는 Slack 같은 곳에 임의로 의견을 터트리면 “협업 깨기”나 “산만한 인재”로 낙인찍힐 수 있음. 실제로는 모두 영감과 아이디어를 공유하는 건데, 회사/문화에 따라 자유롭게 대화하는 것이 위험해질 수도 있음. 이런 리더일수록 원격 환경에 더 취약하며, 심지어 Slack DM까지 검색해가면서 “문제인물”을 찾아내려고 함. 가능하다면 이런 조직을 빨리 떠나는 게 낫지만, 현실에서는 쉽지 않음
          + 직접 경험으로, 공개적으로 질문하거나 문제 제기를 하면 “위계질서”가 엄청 센 곳에서는 리더들이 신경질적으로 이를 싫어하며, 활발히 소통하는 사람을 “문제아” 취급한다고 느낌. 나는 후배들에게 본보기가 되고 싶어서 일부러 질문도 많이 하고, 말도 많이 하면서 리더들이 싫어하던 행동을 더 자주 했고, 결국 짤렸지만, 그 이후로 오히려 안도감과 자부심을 느낌
          + 전 직장에서 매니저가 몰래 사람들 채팅, 이메일, 기타 계정까지 MITM 소프트웨어, SSL strip 등 다양한 방식으로 감시하고, 이를 사내 정치에 활용하던 사례를 겪음. IT 담당자에게도 필요한 정보는 피드백을 받아서 내가 본 게 사실임을 확인함. 심지어 관리자들은 요청하면 타인 이메일까지 열람했으며, 대화로그도 쉽게 수집함. 또 다른 유명 회사에서도 채팅 내용 때문에 해고됐다는 지인 사례도 들었음. 회사가 직접 소프트웨어, 기기, 네트워크를 제공한다면 한 마디 한 마디 신중해야 한다는 교훈임
          + “메모로 남기는 것”이 뉘앙스나 의미가 더 크게 남고 언제든 감시받을 수 있다는 점에서, 오프라인 대화보다 훨씬 신경 쓰이는 부분이 있음. 실제로 직접 만났을 때는 어색한 얘기도 할 수 있지만, 메신저나 이메일은 한 번 남기면 계속 그 기록이 남아 있어서 부담이 커짐. 꼭 “통제 욕구가 강한 CEO”뿐 아니라, 이 감정은 훨씬 더 일반적인 현상인 듯함
     * 전체적으로 제목에는 동의하지만, 구체적 실천 방식은 내 취향이 아님. 가장 적절한 채널 그룹(팀별/프로젝트별/매니저별 등) 하나를 골라 그냥 대화 시작하면 된다고 생각함. 바쁜 채널은 자연스럽게 자체적인 소통 문화를 만들기 마련이고, 업무 얘기와 함께 개 산책 도중 본 신기한 물건 사진도 자유롭게 섞을 수 있음. “스레드” 기능이 있어서 관리하기도 쉬움
          + “스레드”가 그 역할을 정말 잘 해냄. 별도의 채널을 지나치게 많이 만드는 것은 오히려 혼란을 초래함. 이는 예전에 소규모 회사에서 스레드 기능이 없던 때의 습관이라고 생각함
     * 이 방식이 기존의 오프토픽 채널(#general 등)과 어떤 점에서 다르냐는 궁금증이 있음. 특히 2~10명 정도의 소규모 조직이라면, 굳이 여러 개의 “rambling” 채널을 만들 필요 없이 하나의 오프토픽 채널로도 충분하다고 생각함
          + “Claude Code 기다릴 때”라는 표현이 “컴파일하는 동안 딴짓” 개념의 신흥 버전 같다고 재치있게 언급함
          + 실제로는 2~10개의 개인별 채널에서 1~3건씩 글이 올라오는 게, 한 곳에 30건이 몰리는 것보다 오히려 관리가 쉬움. “중요한 걸 놓치고 있지 않을까?” 하는 두려움이 줄어듦. 우리 팀에서도 오프토픽 채널이 있긴 하지만, 각자 rambling 채널이 더 활발하게 쓰임. 기존 대화가 꼬일 위험도 적고, 각자 생각의 연속성도 잘 유지됨
          + 오히려 싱글 오프토픽 채널 하나로 모두 대화를 유도하면 소규모 팀이 아닌 한 부담이 훨씬 적겠다는 의견
          + 사실상 Slack의 #general이 이 역할을 하지 않나 싶은 시각도 있음
"
"https://news.hada.io/topic?id=22356","JSON.stringify를 두 배 이상 빠르게 만든 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    JSON.stringify를 두 배 이상 빠르게 만든 방법

     * V8 엔진에서 JSON.stringify 함수의 성능을 두 배 이상 높여, 데이터 직렬화 속도 개선 효과를 얻음
     * 부작용 없는 객체를 위한 최적화 경로를 도입해 다수의 방어적인 검사 로직을 생략함으로써 일반적인 데이터 객체에서 큰 속도 향상을 이룸
     * 문자열 처리 시 1바이트/2바이트 구분, SIMD 활용, 임시 버퍼 구조 변경 등 하드웨어와 메모리 측면 전반에서 고도화된 방법을 적용
     * 숫자 변환 과정에서는 기존 Grisu3 알고리듬을 Dragonbox로 교체하여 Number.toString() 호출 전반에서도 신속한 변환 가능성 확보
     * 일부 인자·형태에서는 일반 직렬화 경로로 돌아가지만, 대부분 웹 개발 상황에서는 자동으로 최적화 효과를 누릴 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * JSON.stringify는 자바스크립트에서 데이터를 문자열로 변환하는 핵심 함수
     * 이 함수의 성능 향상은 네트워크 요청이나 localStorage 저장 등 웹에서 매우 중요한 작업들에도 긍정적 영향을 줌
     * 최신 V8 엔지니어링을 통해 이 기능의 속도가 두 배 이상 개선되었으며, 주요 최적화 방안을 상세히 소개함

부작용 없는 Fast Path 경로

     * 최적화의 핵심은 부작용(side-effect)이 없는 상황에서만 사용할 수 있는 빠른 직렬화 경로 적용
     * 이런 상황에서는 재귀가 아니라 반복적(iterative) 구조로 객체 순회함으로써, 스택 오버플로우 검사가 필요 없고, 더 깊은 객체 직렬화 시도도 가능함
     * 데이터 객체가 단순할 때 V8은 느린 일반 로직 대신 이 Fast Path를 활용해 많은 검사를 생략하고 속도를 올림

다양한 문자열 표현 처리

     * V8은 1바이트/2바이트 문자(ASCII/비-ASCII) 에 따라 문자열을 다르게 저장하며, 하나라도 비-ASCII가 있으면 전체가 2바이트로 관리됨
     * 문자열 직렬화 성능을 위해 문자열 타입별로 별도의 알고리듬 버전을 만들어 컴파일함
     * 처리 중에 문자열 인스턴스 타입을 확인해야 하므로, 2바이트 문자열이 감지되면 적합한 2바이트 직렬화기가 상태를 넘겨받음
     * 덕분에 문자열 인코딩별 경로 전환 부하는 사실상 없음
     * 결과는 1바이트, 2바이트 버퍼를 각각 만든 뒤 마지막에 단순히 병합함

SIMD로 문자열 직렬화 최적화

     * 자바스크립트 문자열에는 JSON 직렬화 시 이스케이프가 필요한 문자가 포함될 수 있음
     * 긴 문자열은 SIMD 하드웨어 명령(ARM64 Neon 등)으로 여러 바이트를 한 번에 검사함
     * 짧은 문자열은 SWAR 방식으로, 범용 레지스터에서 비트 연산을 통해 여러 문자를 동시에 처리함
     * 어떤 방식이든, 대부분의 경우 별다른 변환 없이 전체 문자열을 빠르게 복사 가능함

Express Lane(초고속 경로) 추가

     * Fast Path 내에서도 프로퍼티 검사 등 반복적인 작업 없이 키 복사만으로 직렬화가 가능하도록 Express Lane 마련함
     * 오브젝트의 hidden class 플래그를 활용해 키에 Symbol이 없고, 모두 enumerable 및 이스케이프 필요 없이 직렬화된 경우 'fast-json-iterable'로 마킹함
     * 동일한 hidden class를 갖는 다른 객체 직렬화 시 별도 검사 없이 바로 키 복사 수행
     * 이 기법은 JSON.parse에서도 빠른 키 비교에 응용

더 빠른 double-to-string 알고리듬

     * 숫자를 문자열로 변환하는 과정도 높은 빈도와 복잡성을 가짐
     * 기존 Grisu3 알고리듬을 Dragonbox로 교체해, Number.prototype.toString() 전체 호출에서도 성능향상 효과가 발생함

임시 버퍼 구조 최적화

     * 문자열 빌드 시 기존에는 단일 연속 버퍼를 사용해, 공간이 부족할 때마다 전체 복사 작업이 발생하는 과부하가 있었음
     * 새로운 방법은 분할(segmented) 버퍼 구조로, 여러 소규모 버퍼를 필요에 따라 이어붙임
     * 덕분에 공간 부족 시 전체 복사 필요 없이 새로운 버퍼 할당만 진행됨

한계

     * Fast Path는 단순한 데이터 직렬화에 한해 동작함
     * 아래 조건에 부합하지 않을 경우 일반 경로 사용
          + replacer 혹은 space 인자 사용 불가(Pretty-Print, 변형 불가)
          + toJSON 커스텀 메서드 없는 단순 객체여야 함
          + 인덱스 기반 프로퍼티가 있으면 느린 경로로 이동
          + ConsString 등 특수 문자열은 처리하지 않음
     * 대부분 데이터 직렬화, API 응답 생성, 설정 캐싱 등 일반적인 용도엔 자동으로 최적화 효과가 적용됨

결론

     * JSON.stringify의 기본 설계부터 메모리 처리, 문자 처리까지 전 영역에서 접근 방식을 재구성하여 JetStream2 벤치마크 기준 2배 이상의 속도 상승을 달성함
     * 해당 개선사항은 V8 버전 13.8(Chrome 138) 이상에서 바로 경험 가능함

        Hacker News 의견

     * JSON 인코딩이 NodeJS에서 프로세스 간 통신에 큰 장애물임을 느낌
          + 대부분 결국엔 이벤트 루프 지연을 줄이고자 작업을 다른 스레드로 넘기려고 시도하지만, 메인 스레드의 CPU 부하는 결국 3배가 됨을 알게 됨
          + 배열을 하나씩 stringify하는 예시도 많이 봄, 내부적으로도 이런 방식이 적용되는 듯함
          + V8 팀이 이 부분을 더 강화해줬으면 하는 바람이 있음
          + 일부 데이터 셋에 대해 bail out 없이 처리할 수 있는지, 또는 CString 처리 문제는 어찌되는지 궁금함, faststr 기능이 부활하는지도 관심 있음
          + 작년에 Node 퍼포먼스 분석을 처음 했을 때, JSON.stringify가 Node 서비스에서 성능을 가로막는 가장 큰 요인 중 하나였음
               o 딕셔너리 키로 stringify를 써야 하고, apollo/express는 전체 응답을 한 번에 문자열로 직렬화해서 스트리밍하지 않음
               o JVM이나 Go에서 온 입장에서는 Node에서 이런 부분이 꽤 아마추어같이 느껴졌음
          + Python도 똑같은 문제를 가짐
               o 일반 패턴을 위한 높은 수준의 API 위에서 효율적인 IPC 프리미티브가 있으면 좋겠다고 생각함
          + JSON 인코딩이 통신에 큰 장애라는 의견에 공감함
               o 전 세계적으로 통신에서 JSON 처리로 인한 연산 오버헤드가 얼마나 큰지, bytes를 고정 포맷 등 더 파싱 효율적인 방식(예: ASN.1)으로 그냥 보내면 더 나은지 궁금함
          + V8팀이 이 부분을 더 적극적으로 강화하는 데에는 반대하며, 이 문제가 있는 개발자들이 다른 도구를 찾기를 추천함
               o Node/V8은 백엔드나 고성능 계산 문제에 그다지 적합하지 않다고 생각함
               o Javascript는 웹 사용에 맞춰져 오래 그럴 것이므로, V8팀이 이런 문제를 해결해 줄 필요는 없음
               o Typescript 팀도 Go로 전향했고, 언어 간 변환 자동화도 가능함
          + Worker로 작업을 오프로드해서 직렬화/역직렬화 시간보다 세이브한 시간이 많았던 경우는 거의 단 한 번뿐이었음
               o 데이터가 크면 크고 비싼 메시지 전달이 병렬화 이익과 맞먹게 됨
     * 최근 10여 년간 부동소수점 숫자 직렬화 성능이 얼마나 향상됐는지 아주 놀라웠음
          + https://github.com/jk-jeon/dragonbox?tab=readme-ov-file#performance
          + IEEE 부동소수점 값을 decimal UTF-8 문자열로 바꿨다 다시 역변환하는 과정은 느릴 뿐 아니라 매우 불안정함
               o 2진수와 10진수로 정확히 표현 가능한 값이 다르기에 미세한 오차가 생길 수 있음
     * JSON.stringify에서 replacer나 space 인자가 있으면 fast path가 적용되지 않는다고 함
          + 그렇다면 JSON.stringify(data, null, 0)을 써도 fast path가 가능한지 아니면 인자가 undefined이어야만 되는지 궁금함
     * SWAR escaping 알고리즘[1]이 Folly JSON에 구현했던 것과 아주 비슷함[2]
          + Folly는 4바이트가 아니라 8바이트 단위로 처리하고, escape가 필요한 첫 위치도 반환해 fast path에서 오버헤드를 최소화함
          + [1] https://source.chromium.org/chromium/_/…
          + [2] https://github.com/facebook/folly/…
     * 작업 자체 가치는 의심하지 않지만, 실제 V8 생태계에서 JSON.stringify가 런타임을 지배했던 구체적 문제나 데이터가 더 궁금함
          + 반드시 실행 시간 비중이 압도적일 필요는 없고, 매일 수억 페이지에서 호출되는 상황이니 전 세계적 전력 절감 효과는 상당할 것임
     * v8의 성능이 충분히 칭찬받지 못한다고 생각함, 요즘 JS가 어마어마하게 빨라졌음
          + 정말 감탄스러움, “10억 달러면 아무 문제도 풀 수 있다”의 좋은 예라고 생각함
               o 앞으로 JS가 “strict”, “stricter”처럼 더 진화해서 컴파일/JIT에 쉽고 단순한 언어가 되면 좋겠음
          + 반면, v8은 너무 극한까지 최적화돼서 전 세계에 제대로 내부를 이해하는 사람이 100명 남짓이고, 대부분 개발자는 “왜 내 JS는 안 빠르지?”라는 마음일 것이라고 느낌
     * 다른 생태계와 비교해서 이게 얼마나 뛰어난 것인지 궁금함
          + 10년 넘게 JSON 직렬화를 해왔지만 너무 빨라서 걱정해본 적이 거의 없음
          + simdjson은 코어당 초당 GB 처리 가능하며, 프리페칭/분기예측 등 최적화 감안하면 JSON 직렬화는 대부분 현실 workload에서 무시해도 될 수준이라 생각함
          + JSON의 가장 큰 단점은 IO 오버헤드임, serializer 속도가 아무리 빨라도 100MB blob을 매번 저장소에 저장해야 하면 무용지물임
     * “No indexed properties on objects” — fast path는 일반적인 문자열 기반 키(key) 객체에만 최적화돼 있다고 하는데, array-like 인덱스 속성이 있으면 slow path로 돌아감
          + 이유가 무엇인지 궁금함
          + integer처럼 보이는 키가 있는 객체는 JSON 배열로 직렬화된다는 뜻일까? 설마 그런가…?
     * segmented buffer 방식이 마음에 듦, 예전엔 fast-json-stringify 같은 userland 라이브러리로 rope 트릭을 직접 짰어야 했는데 이젠 네이티브라 훨씬 좋음
          + bailout 조건(예: replacer, space, 커스텀 .toJSON()) 많이 경험하는지 궁금함, 이런 경우 느린 경로로 바로 fallback되는 것인지 문의함
     * V8은 탁월하지만, JS 자체 때문인지 LuaJIT이나 JVM보다 성능에서 부족함
          + JVM은 워밍업 시간이 길긴 하지만, 그럼에도 불구하고 JS보다는 나음
          + JS가 원인임, V8이 luajit과 JVM보다 훨씬 진보됐다고 생각함
               o Java는 실시간 제약이 적고(컴파일러가 있음) 그 점이 장점임
          + 많은 JS overhead는 dynamic 특성 때문임
               o asm.js는 객체의 shape 변경 같은 동적 동작을 금지해서 많은 체크를 스킵할 수 있었음
          + “JVM조차”라는 표현에 이의를 제기함, JVM은 최고 수준임
"
"https://news.hada.io/topic?id=22236","Jetbrains의 "KotlinConf 2025" 한국어/일본어/베트남어 번역 & 더빙 버전 무료 공개 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Jetbrains의 ""KotlinConf 2025"" 한국어/일본어/베트남어 번역 & 더빙 버전 무료 공개

   KotlinConf란?

   KotlinConf는 Kotlin 언어를 개발한 JetBrains가 매년 주최하는 글로벌 컨퍼런스입니다.
   지난 5월, 코펜하겐에서 열린 KotlinConf 2025에서는 Kotlin, Ktor, Kotlin Multiplatform, Compose, 인공지능(AI), 최신 개발 도구 등 다양한 주제를 아우르는 총 76개의 세션이 진행되었습니다.

   최신 Kotlin 기술 트렌드와 실무 노하우를 한자리에서 경험할 수 있는 대표적인 글로벌 개발자 행사입니다.

   인프런과 젯브레인이 함께 협업하여 이번 KotlinConf 2025의 모든 세션을 한국어/일본어/베트남어로 번역과 더빙하여 무료로 제공하게 되었습니다.

   ⸻

   섹션 1. 오프닝 키노트 (1개)
   1. 오프닝 키노트

   ⸻

   섹션 2. Kotlin 더 알기 (11개)
   2. Kotlin 해부: 최신 안정 기능과 실험적 기능 탐색하기
   3. Kotlin의 Rich Errors
   4. Kotlin 호환성 속성 강좌
   5. Kotlin/Native 객체의 탄생과 소멸
   6. 스마트 캐스팅의 놀라운 세계
   7. 의존성과 Kotlin/Native
   8. Kotlin과 Spring: 모던 서버 사이드 스택
   9. Kotlin 최악의 사용법 – 혼란을 극대화하는 방법
   10. 타입 추론을 넘어서 Kotlin을 설계하다
   11. 서버리스 환경에서의 Kotlin 클린 아키텍처 – 어디서든 가져갈 수 있는 비즈니스 로직
   12. Good Old Data

   ⸻

   섹션 3. Kotlin 개발 팁 (5개)
   13. Don’t forget your values!
   14. 알맞은 타이밍에 올바른 Gradle 설정 갖추기
   15. 비동기 괴물 길들이기: 코루틴 환경에서의 디버깅과 성능 튜닝
   16. 플랫폼 종속 코드에서 아키텍처 컴포넌트를 분리하면서 얻은 교훈
   17. 잘 작동하는 시스템의 특성

   ⸻

   섹션 4. AI (7개)
   18. From 0 to h-AI-ro: Kotlin 개발자를 위한 초고속 AI 입문 가이드
   19. Kotlin으로 AI 에이전트 만들기
   20. Kotlin의 Gam[e]bit: LLM 없이 구현하는 보드게임용 AI
   21. Kotlin에서의 Model Context Protocol(MCP) 활용
   22. Kotlin으로 구축하는 에이전트 기반 플랫폼: 유럽 최대 규모 LLM 챗봇의 동력
   23. 데이터에서 인사이트까지: AI로 구동되는 Bluesky 봇 만들기
   24. LangChain4j와 Quarkus의 활용

   ⸻

   섹션 5. 툴링 (12개)
   25. 45분 만에 끝내는 47가지 리팩토링
   26. IntelliJ IDEA에서의 코루틴 디버깅
   27. Spring Boot 4에서의 차세대 Kotlin 지원
   28. Amper의 새로운 소식
   29. Exposed 1.0: 안정성, 확장 가능성 그리고 기대되는 미래
   30. Kotlin Gradle 빌드를 위한 빠른 내부 개발 루프
   31. 대규모 코드 품질 관리: KtLint와 Detekt으로 Android 코드베이스의 미래를 대비하기
   32. 스트림 프로세싱은 강력하다! KStreams부터 RocksDB까지 Kotlin으로 하는 스트림 처리
   33. JSpecify: Java 널 가능성 애노테이션과 Kotlin
   34. Full Stream Ahead: http4k로 프로토콜의 장벽을 넘어서기
   35. 이징 교향곡: AnimationSpec 완전 정복!
   36. Buck2로 Kotlin 및 Android 앱 빌드하기

   ⸻

   섹션 6. Compose (6개)
   37. Compose로 만드는 창의적인 UI
   38. Compose 드로잉 스피드런 – reloaded
   39. Compose Hot Reload 구현하기
   40. 포괄적인 Jetpack Compose 앱 만들기: Kotlin과 Accessibility Scanner 활용하기
   41. Jetpack Compose로 Meta Quest용 몰입형 VR 앱 만들기
   42. Kobweb으로 Kotlin & Compose HTML을 활용한 웹사이트 만들기

   ⸻

   섹션 7. Ktor (4개)
   43. Ktor에서의 코루틴과 구조적 동시성
   44. 이벤트 기반 분석: Apache Flink와 Ktor로 실시간 대시보드 만들기
   45. 서버 사이드 개발을 위한 Ktor 확장하기
   46. Full-Stack Kotlin의 단순화: HTMX와 Ktor를 활용한 새로운 접근

   ⸻

   섹션 8. 멀티플랫폼 (Kotlin Multiplatform / Compose Multiplatform) (7개)
   47. 호기심 많은 Kotliner를 위한 Swift의 동시성 소개
   48. Swift Export — 그 속을 들여다보기
   49. 실전 투입 가능한 iOS용 Compose Multiplatform
   50. 현대 브라우저에서의 Kotlin/Wasm 및 Compose Multiplatform for Web의 현재
   51. iOS 연동을 위한 Kotlin 및 Compose Multiplatform 패턴
   52. Multiplatform Settings: 멀티플랫폼 라이브러리 개발 사례
   53. 의존성 주입으로 Kotlin Multiplatform 프로젝트 확장하기

   ⸻

   섹션 9. Kotlin Multiplatform 실전 적용 사례 (8개)
   54. Duolingo + KMP: 개발자 생산성에 대한 사례 연구
   55. Kotlin Multiplatform으로 이룬 크로스플랫폼 혁신: 377년 역사의 노르웨이 우체국 사례
   56. 확장을 위한 청사진: AWS가 대규모 멀티플랫폼 프로젝트에서 배운 것들
   57. 맥도날드 앱의 내비게이션에 KMP를 활용한 사례
   58. 하나의 코드베이스, 세 개의 플랫폼: X의 Kotlin Multiplatform 적용 경험
   59. Kotlin Multiplatform과의 2년의 여정: 0%에서 55% 코드 공유까지
   60. Google Workspace에서의 Kotlin Multiplatform 실전 적용 사례
   61. RevenueCat의 네이티브 SDK 멀티플랫폼화

   ⸻

   섹션 10. API (2개)
   62. API: 얼마나 어렵겠어?
   63. 프로처럼 Collect하기: Android 생명주기 인식 코루틴 API 심화 탐구

   ⸻

   섹션 11. Kotlin Notebook (2개)
   64. Kotlin Notebook으로 Compose 프로토타이핑하기
   65. 차트, 코드, 그리고 돛: Kotlin Notebook으로 요트 경주에서 승리하기

   ⸻

   섹션 12. Kotlin 활용 사례 (5개)
   66. Kotlin을 활용한 금융 데이터 분석
   67. 나만의 NES 에뮬레이터… Kotlin으로 만들기
   68. IoT 개발과 Kotlin
   69. Kotlin으로 macOS 화면 보호기 만들기
   70. That’s Unpossible – Kotlin으로 만든 풀스택 사이드 프로젝트 웹앱

   ⸻

   섹션 13. 흥미로운 프로젝트들 (5개)
   71. (얕은) 불변성을 향한 (깊은) 탐구: Valhalla와 그 너머의 가능성
   72. Klibs.io — Kotlin 패키지 인덱스를 향한 꿈
   73. AI를 활용한 대규모 코드 마이그레이션 – Uber에서 수백만 줄의 Java 코드를 Kotlin으로 변환하기
   74. Project Sparkles: Compose for Desktop이 Android Studio와 IntelliJ에 가져오는 변화
   75. 당신의 세 번째 Kotlin 컴파일러 플러그인 만들기

   ⸻

   섹션 14. 마무리 토론 세션 (1개)
   76. 마무리 토론
"
"https://news.hada.io/topic?id=22283","MacBook Pro Insomnia","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          MacBook Pro Insomnia

     * MacBook Pro Silicon M1 Max 모델에서 밤새 배터리 소모 현상 경험
     * 직접 전원 관리 로그를 분석하려고 시도했으나 문제 원인 파악에는 한계가 있었음
     * Sleep Aid라는 전용 앱을 이용해 웨이크 이벤트를 시각적으로 확인 가능함
     * Sleep Aid 설정에서 ""유지 관리 목적으로 깨우기(Wake for maintenance) "" 비활성화가 원인임을 발견함
     * 해당 옵션을 다시 활성화한 이후, 밤새 배터리 소모 현상 해결
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

MacBook Pro에서의 밤새 배터리 소모 이슈 경험

     * 수년간 MacBook Pro Silicon M1 Max를 사용해온 경험이 있음
     * 최근 예기치 않게 노트북을 전원에 연결하지 않고 두면 밤새 배터리 소모가 심해지는 증상이 나타남
     * 증상이 점점 심해져 직접 원인 분석을 시작하게 됨

전원 관리 로그 분석 시도

     * MacOS의 터미널 명령어 pmset -g log를 통해 전원 관리 관련 로그 확인 가능함
     * 해당 로그의 출력이 방대하고 해석이 어려워, 로그 분석을 위해 pmset-analyzer라는 직접 개발한 간단한 도구를 사용함
     * 그러나 이 도구만으로 실질적인 해결에는 큰 도움이 되지 않았음

세부 설정 조정 및 추가 조사

     * 공식 문서와 커뮤니티에서 안내하는 tcpkeepalive 등 전원 관리 세팅을 하나씩 조정해봄
     * 설정 변경만으로는 문제 해결에 큰 효과를 보지 못함

Sleep Aid 앱을 통한 문제 해결

     * 추가 조사 과정에서 Sleep Aid라는 앱을 알게 됨
          + 이 앱은 웨이크 이벤트(wake event) 를 시각적으로 보여주고, 각종 전원 관리 설정을 직관적으로 변경할 수 있는 인터페이스를 제공함
     * Sleep Aid에서 확인 결과, ""유지 관리 목적으로 깨우기(Wake for maintenance) "" 옵션이 비활성화된 상태였음
     * 앱의 설명대로 이 설정이 꺼져 있으면 잦은 웨이크 이벤트가 발생할 수 있음
     * 해당 옵션을 다시 활성화하였고, 이 후로는 밤새 배터리 소모 현상이 발생하지 않음

   맥북 잠자기 모드로 해놓고 자는데 갑자기 새벽에 화면 켜지더니 온방이 훤해요. 자다가 깨서 그냥 완전 종료시킨적 많음. 이게 언제 부터 논란거리였는데 아직도...

        Hacker News 의견

     * 또 하나의 유용한 팁 공유임. Activity Monitor에서 Energy 탭을 열고 ""Preventing sleep"" 열로 정렬하면 어떤 앱이 macOS의 수면을 막는지 확인할 수 있음. 내 경우 Devonthink 앱이 원인이라고 확인함. 아직 버그 리포트는 못 올렸음. Apple의 전원 관리 기능이 이런 문제를 사용자에게 알리지 않는 게 의아함. 맥이 가방 안에서 뜨겁게 달아오르고 배터리가 소진되는 상황이 중요한 문제 아닌지 의문임. 그 와중에 Chrome이 네트워크 장치 검색 허용할지 계속 물어보는 건 훨씬 덜 중요한 것 같음
          + Apple의 전원 관리가 이에 대한 경고를 안 한다는 것도 놀랍지만, 덮개를 닫아도 어떤 앱이 시스템의 수면을 막을 수 있다는 점이 더 놀라움. 동영상 플레이어처럼 때때로 타임아웃 기반의 수면 방지가 필요한 건 이해함. 하지만 덮개를 닫거나 수면 버튼을 눌렀을 때 시스템이 잠자지 않도록 앱이 결정하는 건 정말 쓸모 있는 예시가 거의 없음. 대부분 이런 상황은 결과적으로 백팩 안에서 과열된 노트북을 가져올 확률만 높임. 더욱이, 웹 페이지 하나만으로도 시스템이 수면으로 들어가지 못하게 할 수 있다니, 70개 탭 중 어떤 게 문제인지 찾기 어려움. 타임아웃 기반의 수면 방지 권한과 ""뚜껑을 닫으면 무조건 잠자기"" 같은 코어 동작 변경 권한을 분리하면 좋을 것 같음. 타임아웃 사용은 허용해도 덮개 닫힘 같은 것은 반드시 사용자에게 알리고
            허가받아야 함
          + 아무 앱이나 시스템 전체의 수면을 막을 수 있다는 걸 몰랐음. 이런 권한은 사용자 통제 아래 있어야 마땅함. 개발자가 이러한 API를 호출하려면 최소한 entitlement가 필요한 것 아닌지 궁금함
          + shell에서 pmset -g assertions 명령어를 쓰면 어떤 프로세스가 시스템 수면을 막고 있는지 알 수 있고, 보류 중인 파워 assertion들의 상세 정보를 볼 수 있음. pmset에는 공식 문서에 없는 명령어도 있는데, Apple이 소스코드에 공개한 걸 보면 알 수 있음. 특정 assertion을 무시하도록 만드는 명령도 있음. 단 ""UserIsActive"" assertion을 끄면 시스템을 깨우는 데 애를 먹을 수 있음
          + 이런 기능이 있는 줄 몰랐음, 고마움. 최근에 사용하지 않던 MacBook의 배터리가 자꾸 줄어드는 이유가 궁금했었는데, 알고 보니 Firefox가 수면을 막고 있었음. 자동재생 비디오 때문인 듯함. 완벽하진 않지만 고칠 수 있는 문제임
          + Safari는 한편으로는 Netflix를 시청 중일 때 전력 소모가 많으니 닫으라고 알림을 띄움
     * 내 MacBook Pro에서도 비슷한 현상이 있었음. Apple Silicon 모델이 아니라 이전 모델임. 그 당시 공유기에서 DHCP 임대 시간을 기본값보다 훨씬 낮게 15분으로 바꿨었음. 내 생각에는 MacBook이 15분마다 IP 갱신을 위해 깨어나서, 잠깐 자다가 또 깨어나는 현상이 반복된 것으로 보임. 공유기 임대 시간을 이전의 기본값으로 되돌리니 배터리 소모 문제가 완전히 해결됐음. 그 원인을 예측하긴 힘들었는데, 마침 새 MacBook Pro를 산 직후라 이런저런 문제에 더 신경을 썼던 덕에 빨리 찾게 됨
          + 제대로 동작하는 DHCP 클라이언트는 임대 시간의 50%가 지나면 갱신 요청을 하게 됨. 즉 생각보다 더 자주 깨어났을 가능성이 높음
          + DHCP 임대 시간을 15분으로 바꾼 이유가 궁금함. 어떤 목적이 있었는지 물어봄
          + 나도 방금 알게 됐는데, 사용 중인 mikrotik 공유기는 기본적으로 10분짜리 임대 기간을 사용함
          + 이런 현상 너무 신기함. 한 번 IP 갱신을 위해 깨어날 때 어느 정도 mAh를 소모하는지 궁금함. 수 밀리암페어-밀리초 수준일 것 같은데, 결국 노트북이 WiFi만 잠깐 켜고 패킷 몇 개 주고받는 정도임. 물론 Apple Silicon 이전 모델이라서 실제로는 얘가 깨어있는 동안 다른 짓을 추가로 했을 수도 있음
          + 이건 macOS 버그라고 생각함. 수면 중일 때는 굳이 IP가 필요 없으므로 DHCP 임대 갱신을 목적으로 깨어나는 건 비정상임. 소스가 공개되지 않은 OS는 이런 점에서 문제임
     * “Wake for maintenance” 옵션이 꺼져 있으면 Sleep Aid에서 설정 창에 이로 인해 자주 깨어날 수 있다고 안내함. 작가가 실수로 “옵션이 꺼져 있었다”라고 적은 것 아닐지 궁금함
          + 나도 같은 생각이었음. 이건 추측이지만, 이 설정이 꺼져 있으면 깨어나는 이벤트가 시간별로 묶여서 한 번에 처리되지 않고 밤새 아무 때나 여러 번 발생할 수 있을 것 같음. 바로 이런 현상을 설명하는 것으로 보임
          + 헷갈림. 명시적으로 컴퓨터를 깨우는 옵션을 켜면 오히려 깨어나는 횟수가 줄어드는지 궁금함
          + 나도 혼란스러움. 작성자의 스크린샷에서는 Enable 상태임. 그게 ‘정상’으로 보이는데, Disabled가 왜 더 많은 Wake를 유발하는지 직관적으로 잘 이해가 안 됨
          + 이런 상식에 반하는 설명은 부연 설명이 필요하거나 최소한 그런 예외임을 명확히 밝혀주길 바람. 글 작성 후 수정할 때 이런 문제를 잡기가 어려움
     * 내가 그동안 가지고 있던 모든 Mac 랩탑에서 뚜껑을 닫을 때 항상 hibernate로 설정해 문제를 해결했음. 뚜껑을 열어서 다시 사용할 때는 20~30초 정도 복구 시간이 걸리는데, 수면과 배터리 소모에 대해 신경 쓸 일이 상당히 줄어드는 작은 대가라고 생각함. 터미널에서 이 명령어로 쓸 수 있음: sudo pmset -a hibernatemode 25. 원래대로 돌리려면 sudo pmset -a hibernatemode 3를 입력하면 됨
          + hibernate 모드가 FDE(전체 디스크 암호화)와 잘 작동하는지 궁금함. Linux에서는 메모리 내용을 디스크에 쓰는 과정에서 암호화 관련 주의사항이 다양함
          + 대부분의 사람들이 기대하는 동작을 가장 간편하게 설정하는 방법이라고 생각함
     * Macbook 수면 관련 문제를 정말 오래 파고들었지만, WindowServer가 원인이고 결국 풀 OS 재설치가 필요할 것 같음. 한두 달에 한 번씩 가방에서 뜨겁고 방전된 노트북을 꺼내는 것만큼 속상한 건 ""난 그런 적 없는데 뭔가 잘못 사용한 거 아니냐""는 반응임
     * MacOS는 10년 전부터 거의 유지보수 모드인 것처럼 느껴짐. ARM/Mac Silicon 이식 작업이 엄청 많았을 거라 생각함. AI 업데이트도 실망스러웠고, 최근에 실질적인 개선은 많지 않았던 느낌임. 예전에 전원 버튼이 백스페이스 옆에 붙은 Intel Macbook Air를 쓴 적 있었는데, 전원 버튼을 한 번만 누르면 바로 꺼지지 않도록 hold해야 하게 만드는 스크립트를 Mac 포럼에서 찾아서 사용했었음. 그런데 그 스크립트에 트로이 목마가 숨겨져 있어서 맥 공장 초기화와 iCloud 전체 삭제가 필요했음. 이 스크립트는 내부 변수로 보이는 GUID도 있었고 어딜지 모르는 곳에서 리소스를 다운로드할 수 있었다고 함. 나는 그냥 내부 변수라고만 생각했었음
     * Apple 공식 지원 문서를 보면 이 기능(전력 옵션)이 기본 제공됨
          + macOS 26 DB가 설치된 M4 MacBook Air에는 해당 위치에 power nap이 없음. 대신 ""Wake for network access""가 등장했고, 기본값은 ""Only on Power Adapter""임
          + power nap과 ""wake for network access""가 같다고 착각했었음. macOS 26에서는 더 이상 옵션이 없는 듯함. 내 설정은 ""Only on Power Adapter""로 되어 있는데, 합리적임. M4 MacBook Air 기준임
     * 나도 거의 똑같은 블로그 글을 지난주에 썼었음. 아쉽게도 내 경우에는 그 솔루션이 효과가 없었는데, power nap 이외의 다른 프로세스가 계속 깨움. 관련 글: annoying.technology 포스트
     * Activity Monitor의 Energy 탭은 이런 상황에서 유용함. 어떤 앱이 완전히 시스템 sleep을 막는지 알 수 있고, 각 프로세스의 지난 12시간 전력 소모도 볼 수 있음. 밤새 곧바로 원인을 추적할 수 있음
     * 나 역시 MacBook Pro(Apple Silicon)에서 비슷한 문제를 겪고 있음. SSD를 연결한 채로 잠자기를 하면 주기적으로 시스템이 깨어나 드라이브를 활성화하는 것 같음. 결과적으로 노트북과 SSD 모두 열이 나고 배터리가 금방 소모됨. 유일한 대처 방법은 잠자기 전에 모든 외장 드라이브를 분리하고, 충전기에 꼽아두는 것뿐임. 꽤 번거로운 버그임. 품질 관리와 테스트가 부족했다는 생각임
"
"https://news.hada.io/topic?id=22340","PDF를 파싱하고 싶으신가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            PDF를 파싱하고 싶으신가요?

     * PDF 파싱은 명확한 순서와 구조를 바탕으로 작동해야 하나, 실제 파일은 이 규격을 자주 따르지 않음
     * cross-reference(xref) 포인터와 오프셋 찾기에서 다양한 오류와 불일치가 발생함
     * 실제로는 PDF 헤더 앞의 불필요한 데이터나 포인터, 오프셋의 잘못된 위치로 인해 많은 문제가 생김
     * PDF xref 테이블 자체가 명확하지 않거나 잘못 형식화되어 있는 사례도 많음
     * 그래서 주요 뷰어들은 비표준 PDF 파일까지 지원하는 로직을 추가 구현하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PDF 파싱에 대한 이상적 접근

     * PDF 파싱은 이론적으로 일정한 단계로 진행됨
          + 파일 시작 부분에서 버전 헤더 주석을 찾음
          + cross-reference(xref) 포인터를 찾음
          + 모든 객체 오프셋을 수집함
          + trailer 딕셔너리를 찾아서 전체 카탈로그 구조에 접근함

  PDF 객체 소개

     * PDF 객체는 넘버, 문자열, 딕셔너리 등 여러 PDF 요소를 감싸서 저장하는 단위임
     * 각 객체는 ""obj/endobj"" 마커 사이에 존재함
     * 객체들은 간접 참조(indirect reference, 예: ""16 0 R"") 방식으로 서로 연결됨
     * 파일 내 객체 분할 방식은 자유롭지만 일부 객체 유형은 반드시 간접참조여야 함

  cross-reference 오프셋 찾기

     * PDF에는 구조상 cross-reference(xref) 테이블이 있는데, 이는 객체 위치의 인덱스 역할을 함
     * 파일 끝에 ""startxref"" 구문으로 특정 바이트 위치가 포인터로 명시됨
     * 이 포인터가 xref 위치를 지정하지만, 스펙과 실제 파일에서 차이가 있음. 예를 들어 ""%EOF"" 마커가 본래 마지막 줄이어야 하지만, 현실 PDF에서는 1,024바이트 이내 어디든 있을 수 있음
     * 실제 파일에서 포인터의 형식 오류(startref 등), 줄바꿈 누락 등 다양한 변형이 발견됨

  객체 오프셋 찾기

     * xref 테이블은 ""xref"", 객체 시작 번호, 객체 수가 차례로 이어지고, 각 객체의 오프셋/생성번호/상태(n 또는 f)가 한 줄에 기록됨
     * xref 테이블이 여러 개거나, /Prev엔트리를 통해 서로 연결될 수도 있음

  trailer 딕셔너리 위치 탐색

     * startxref 마커 윗부분에 trailer 딕셔너리가 존재하며, 루트 객체를 찾을 수 있는 필수 메타데이터가 포함됨
     * 루트 객체를 기준으로 전체 구조 해석 착수 가능

실제 환경: 예상치 못한 문제점들

     * PDF 스펙을 준수하지 않는 파일이 많아 일반적인 파서로는 처리가 어려움
     * cross-reference 포인터 탐색에서 흔히 실패하는 경우
          + 포인터가 파일 끝 또는 마지막 1,024바이트에 없음
          + 오탈자(startref 등)
          + 예외적 형식
     * 3,977개 실제 PDF 샘플 조사에서 약 0.5%가 xref 선언 오류를 가짐

  PDF 콘텐츠가 0이 아닌 오프셋에서 시작

     * 헤더 앞에 쓸모없는 데이터(junk) 가 있으면 모든 바이트 오프셋이 밀려 startxref 위치가 어긋남
     * 헤더 위치를 기준으로 오프셋을 재계산해야 하며, 두 위치 모두 확인해야 함
     * 전체 오류의 약 50%를 차지

  xref 포인터가 xref 테이블 중간을 가리킴

     * 지정된 오프셋이 xref 테이블 내용 한가운데로 이동할 수도 있음
     * 3,977개 샘플 중 약 5건에서 발견

  포인터가 xref 근처에 있음

     * 종종 포인터가 정확하지 않지만, xref 바로 앞이나 다음의 공백, 개행문자 오차만큼 어긋난 경우가 많음

  포인터는 맞으나 xref 오프셋이 잘못됨

     * xref 표에 기록된 오프셋 자체가 잘못될 수도 있음
     * 일부 객체만 올바르고 나머지는 offset 오류를 가질 수도 있음

  첫 포인터는 정상인데 이전 오프셋(/Prev)이 이상함

     * PDF를 수정하며 생성되는 /Prev 포인터에 잘못된 값(예: 0)이 저장된 사례 다수

  xref 테이블 형식이 비정상임

     * 줄바꿈 없이 ""xref""와 숫자가 붙거나, 선언된 객체보다 더 많은 항목이 있거나, 표 중간에 쓰레기 데이터가 포함된 경우 다양하게 나타남
     * 이러한 사례는 PdfPig 등에서 issue로 다수 보고됨

결론

     * 명세에 따르면 PDF 파싱은 정형화된 순서로 처리되어야 하지만, 실제 파일 다수는 그렇지 않아서 파싱에 다양한 문제가 발생함
     * 실사용 PDF 뷰어들은 비규격 PDF 지원 확대 기능을 기본적으로 포함함
     * 이번 요약 내용은 PDF 명세(총 1300페이지 중 22페이지)에 해당하는 일부분 파싱만을 다루었음

   PDF는 솔직히 말해서 사람이 만든 서식을 최대한 보존하는 사람이 읽기 좋은 포맷이고 기계와의 궁합은 최악이죠.

   공감합니다. 사실, 읽기 좋은지도 잘 모르겠어요.. 너무 무겁고 불편해요.

   이번 요약 내용은 PDF 명세(총 1300페이지 중 22페이지)에 해당하는 일부분 파싱만을 다루었음 <-... 1300페이지 어마무시하네요...

   와..

        Hacker News 의견

     * 답은 명확함
         1. PDF는 원하는 모든 포맷의 메타데이터 첨부를 지원함
         2. 모든 PDF 생성 소프트웨어가 동일한 정보를 기계가 읽기 쉬운 방식으로 첨부해야 함
         3. 그러면 PDF를 파싱하려는 사람은 메타데이터만 바라보면 됨
            현실적으로, 내 이름이 Geoff인데, 이력서 파서 중 절반이 내 이름을 ""Geo""와 ""ff""로 따로 인식함
            이는 텍스트가 PDF에 들어가는 방식 때문이며 여러 소스 앱에서 계속 발생하는 문제임
          + PDF 파싱과 PDF 콘텐츠 파싱은 완전히 다름
            PDF 파일을 파싱하는 것도 골치 아프지만, PDF 자체가 ""지정 위치에 뭔가 찍기"" 기반이라, 경계 박스 내의 잘 정의된 텍스트와는 달라서 단어 추출하려면 어떤 글자가 함께 있는지 추측해야 하는 상황임
            이력서 파서를 돕고 싶으면, 접근성 트리(Accessibility tree)에 주목해볼 만함
            모든 PDF 렌더러가 접근성 PDF를 내보내는 건 아니지만, 접근성 PDF가 그나마 이름 같은 걸 제대로 읽어오게 도와줄 수 있음
            ""ff"" 문제는 아마도 비ASCII 문자(예: ﬀ 리가처)를 이력서 분석기가 처리 못하는 경우임
            PDF 렌더러가 리가처를 생성하지 않도록 설정 가능하지만, 이러면 텍스트가 보기 흉해질 수 있음
          + ""해야한다(should)""라는 단어에 많은 걸 기대하는 느낌임
            PDF 사용이 실제로는 꽤 적대적이면 사람들이 그 정도로 생각하지 않는 것 같음
            이력서를 PDF로 내는 것부터 중간 유통자가 못 고치게 하려는 목적이 있고, ""편집""도 이미지 위에 박스 그어서 가리기, 표를 CSV 대신 PDF로 만들어 분석 어렵게 만드는 등 다양한 이유가 있음
          + 실제로 이 방법이 잘 통하는 경우도 있음, 일부 앱에서 이 방식을 사용하고 있음
            다만, 두 가지 표현(본문/메타데이터)이 실제로 일치하지 않는 문제가 남아있음
          + 손글씨 스캔이나 다른 스캔 문서는 스캐너 및 일반 가정용 컴퓨터가 완벽한 OCR 지원을 하지 않으면 어떻게 하냐는 의문이 있음
          + 아마도 ff가 리가처로 렌더링돼서 생기는 문제 같음
     * Tensorlake 창업자임
       개발자를 위한 문서 파싱 API를 만들었음
       PDF 파싱에서 Computer Vision 방식이 실제 현장에서 잘 동작하는 이유임
       파일 내 메타데이터에만 의존하는 것은 다양한 PDF 소스에서는 확장성이 없음
       그래서 PDF를 이미지로 변환 후, 레이아웃 인식 모델을 먼저 적용, 이어서 텍스트 및 표 인식 등 특화 모델 돌린 후 조각을 합쳐서, 정확성이 필수인 분야에서도 쓸 만한 결과를 얻는 방식임
          + 이런 방식이 언뜻 보면 우습지만, 실은 가장 현실적인 해결책 같음
            PDF는 본질적으로 사람이 읽을 레이아웃을 표현하기 위해 고안된 포맷이라 컴퓨터가 읽도록 설계된 게 아니라, 보기 좋은 디스플레이에 초점을 맞춘 포맷임
            그렇기에 인간이 읽는 방식을 흉내 내는 접근법이 이치에 맞게 느껴짐
            다만 30년 넘는 시간 동안 PDF가 기계 판독성을 더하지 못한 점은 아쉬움
            어떤 인센티브가 부족했길래 이걸 가능하게 하지 못했는지 궁금함
            혹시 이에 대해 통찰이 있는 사람이 있으면 듣고 싶음
          + 약간 우스운 점임
            PDF를 인쇄해서 스캔 후 이메일 보내는 건 비웃음거리 감인데, PDF 파싱에선 사실상 똑같은 짓을 하는 셈임
            그런 접근법이 필요하다는 게 답답한 현실임
            세상은 HTML을 그렇게 파싱하진 않음
          + Nutrient.io 공동 창업자임, 10년 넘게 PDF 다루고 있음
            웹브라우저처럼 PDF 뷰어들은 엄청나게 다양한 PDF를 받아야 함
            PDF가 워낙 오래돼서, 파일 생성자들이 본인이 쓰는 뷰어에서만 잘 보이면 되도록 임의로 수정하기 때문임
            그래서 우리 회사는 AI 문서 처리 SDK(REST API, PDF를 입력하면 JSON으로 구조화된 데이터 반환)를 만들었음
            시각적 방법뿐 아니라 구조적 전처리/후처리 경험으로, 순수 비전 기반 대비 성능/비용 모두 더 나은 결과 제공함
            직접 PDF 처리를 고민하고 싶지 않고 본연의 일에 집중하고 싶다면 도움이 될 수 있음
            https://www.nutrient.io/sdk/ai-document-processing
          + PDF 내부 구조 전문가가 있는 김에 질문이 있음
            왜 mupdf-gl이(기본 데스크톱 리눅스 기준) 다른 모든 프로그램보다 훨씬 빠른지 궁금함
            대용량 PDF 검색 속도가 확연히 월등한데, 왜 다른 뷰어들은 이렇게 빠를 수 없는지 항상 궁금했음
            관련 통찰이 있다면 듣고 싶음
          + 결국 PDF를 이미지로 렌더링할 때 사용하는 소프트웨어에 파싱 작업을 외주 준 셈임
     * 오래전부터 레이아웃 위주의 문서 커뮤니케이션에서 벗어나야 한다고 생각함
       즉, 전문적으로 꾸민 레이아웃 자체가 사실 옛날 관습에 더 가깝고, 실제 컨텐츠 이해와 거의 연관이 없다고 봄
       예를 들어, 각종 규제기관 제출 서류는 엄청나게 두꺼운 문서들이고, 레이아웃 규칙을 맞추려면 Microsoft Word에서 오랜 시간 작업하게 됨
       이런 레이아웃 보장을 위해 DOCX나 PDF 형식으로 제출하는데, 이 포맷들은 프로그램이 자동으로 내용 추출하거나 가공하기 매우 부적합함
       LLM도 이 파일을 읽을 순 있지만, 단순한 기계 친화 파일(텍스트, markdown, XML, JSON 등)에 비해 계산 비용이 크게 듦
       대안으로 아예 '기계 우선', '내용 우선'의 단순한 포맷(JSON, XML, HTML 기반 등)을 표준화하는 접근 가능성 생각해봄
       최소한의 구조 및 이미지 임베딩 정보만 있고, 인간이 읽을 땐 뷰어 앱으로 보기 좋게 재구성하면 됨
       기계 처리는 훨씬 쉬움
       이미 HTML/브라우저, EPUB 등 유사 포맷이 존재함에도, 고전 방식 대체가 필요한 시점이라 생각함
       LLM 혁명이 이런 방향으로 이끌길 기대하고, 앞으로 비싼 PDF 파싱이 전통 파이프로 남기만을 바람
          + PDF 문제엔 동의하지만, DOCX가 실제로 그렇게까지 나쁜 거냐고 반문함
            아직 DOCX 파서를 만들어본 적은 없지만, DOCX는 XML 기반이고, 명시적으로 레이아웃을 지정하지 않는 한 모든 게 절대 좌표화되지 않으니, JPEG이 0점, PDF가 15점, markdown이 100점이면, DOCX는 대략 80점쯤 되는 쉬운 난이도 아닐까 하는 추측임
     * 훌륭한 정리였다고 생각하며, 흥미롭게 느낀 추가 포인트가 있음
       Incremental-save 체인: 첫 startxref 오프셋은 괜찮지만, Acrobat이 여러 번 수정할 때마다 반복 추가하는 /Prev 링크가 다음 xref로부터 몇 바이트 짧게 가리키는 경우가 많음
       대부분의 뷰어(PDF.js, MuPDF, Adobe Reader까지)에서는 obj 토큰을 파일 전체에서 무식하게 찾아 새로운 테이블을 재구성하고, 명세 친화적 파서는 폭발하는 상황 발생
       실제 필드에서 여러 애플리케이션이 반복 수정한 문서를 다루고 싶으면 이런 복구 경로(salvage path)는 필수적임
          + 맞는 지적임, 이건 샘플 집합에서 자주 보던 실패 케이스임
            이전 레퍼런스 또는 체인 내 하나가 파일 바깥 오프셋, 0 오프셋, 잘못된 값 등으로 가리키는 경우가 많음
            이 글을 쓰게 된 계기는 내 프로젝트 PdfPig의 초기 파싱 로직 개편 때문임
            처음엔 Java PDFBox 코드를 이식했지만, 좀 더 빠르고 단순하게 바꾸고 싶었음
            새 로직은 xref 테이블/스트림을 하나라도 놓치면 파일 전체를 스캔하며 복구 경로에선 해당 오프셋만 신뢰함
            하지만 이전보다 확실히 느려졌고, 변화가 실제로 괜찮은지 확신이 어려움
            1만 개 파일 테스트셋으로 각종 특이 케이스(엣지 케이스)를 탐색 중임
            https://github.com/UglyToad/PdfPig/pull/1102
     * 잘 동작하는 가정과 적절한 PDF 오브젝트 파서만 있다면 쉬울 것 같지만 현실은 절대 그렇지 않다고 생각함
       이 상황은 PDF 지옥과도 같음
       PDF는 명세가 아니라 사회적 합의, '분위기' 수준임
       버둥거릴수록 더 깊이 빠져들게 되고, 이제 우리 모두 신의 시야에서 멀어진 수렁에 살게 된 느낌임
       이 얘기에 웃음이 났음
          + 이 글은 James Mickens가 쓴 것 같다는 농담임
     * ""PDF를 파싱하고 싶은가""라는 질문에 대해, 절대 아니다라고 단언할 수 있음
       이유는 원글에서 잘 설명됨
          + 제 은행이 좀 더 읽기 쉬운 포맷으로 자료를 제공해주면 좋겠지만, 그 전까진 어쩔 수 없음
          + 이미 그 실수를 해봤고, 다시는 그러지 않으려 함
     * PDF 파서 작성 경험자로서, PDF는 정말 이상한 형식이라고 느낌
       이건 이진(binary)과 텍스트의 혼합이라는 태생적 설계가 이런 괴상한 점을 만든 것 같음
       조금 어설프게 부정확한 xref 오프셋 문제도 LF/CR 줄바꿈 변환 처리 중 버그에서 비롯된 경우라 추정함
       글에서 언급 안 된 것 중 하나는, 최신 PDF(v1.5+)는 일반 텍스트 xref 테이블 없이 ""xref stream""으로 담긴 경우가 많음
       v1.6 이상에선 오브젝트 자체를 object stream에 담을 수도 있음
          + 나도 단순 xref 테이블 수준을 넘어서 스트림과 압축 이야기까지는 다루지 않은 게 의외였음
            별문제 없어 보이다가도, 원하는 오브젝트가 스트림 안에 들어있고, 그 스트림 자체가 PNG 압축을 변형해서 쓰거나, 오프셋이 flate 압축된 xref stream 안에 있으면 골치 아파짐
            게다가 여러 문서 버전이 뒤섞여 있어서 어디부터 어디까지가 최신인지 따지는 것도 복잡함
            PDF 1.7 문서까지는 구하기 쉽지만, 불과 2년 전까지 PDF 2.0 명세 자료는 유료 벽에 막혀 있었음
     * PDF는 스트리밍을 고려하지 않은 포맷임
       끝에 위치한 trailer dictionary 때문에 파일 전체가 다 로드될 때까지 파싱하기 어렵게 만듦
       다만, ""스트리밍 가능한 PDF""도 존재해서, 처음 부분에 필요한 정보가 있으면 첫 페이지는 바로 렌더링 가능함(나머진 아닐 수 있음)
       최근에는 PDF 쪽과 거리가 좀 있으니 감안해야 함
          + 풋터가 있더라도 웹사이트가 Range Request를 지원하고 Content-Length 헤더만 잘 써주면 PDF도 스트리밍이 가능함
            스트리밍 리더는 HEAD 요청, 파일 맨 끝 수백 바이트 요청해 포인터와 테이블 구한 뒤 나머지 부분 이어 받으면 됨
            실시간 생성 PDF엔 부적합해도, 꽤 오래된 웹 서버라면 1~2회 왕복시간만 추가되면 충분함
            아쉽게도 파일별 Range 기반 파서를 신경 쓰는 사례가 드물지만, 기술적으로 불가능한 건 아니라고 생각함
          + 맞음, Linearized PDF라는 형식이 있는데, 첫 페이지를 전체 파일 다운로드 없이도 빠르게 보여줄 수 있게 고안됨
            요약에선 해당 방식은 부속 설명이 많아 생략했던 점 알려줌
     * 파이썬 배우고 처음 도전한 프로젝트 중 하나가 PDF 파서였음
       DnD 캠페인용 지도 자동 추출을 노렸는데, 결과는 실패였음(웃음)
     * TIFF 리더를 작성해 본 적 있음
       TIFF도 쓰기는 쉬운데 읽기는 어렵기로 악명이 높음
       PDF도 똑같은 부류에 들어가는 것 같음
"
"https://news.hada.io/topic?id=22238","Writing is Thinking - 글쓰기는 생각하는 것임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Writing is Thinking - 글쓰기는 생각하는 것임

     * 인간이 직접 작성한 과학적 글쓰기의 중요성이 LLM 시대에도 지속적으로 강조됨
     * 글쓰기는 연구 결과 전달을 넘어서 사고를 정제하고 새로운 아이디어를 발견하는 도구
     * LLM이 작성한 텍스트는 책임성과 진정성이 부족하며, 허위 정보 생성 위험성(환각 현상)도 큼
     * LLM은 가독성 개선, 문법 교정, 아이디어 발상 등 도구적 활용에 효과적이지만 전체 글쓰기를 맡기면, 비판적 사고와 창의적 사고 기회를 상실할 수 있음
     * 창의적 연구 내러티브를 형성하는 경험과 반성의 기회가 인간 고유의 중요한 과정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LLM의 시대에 인간이 생성한 과학 글쓰기의 가치

     * 과학적 글쓰기는 연구 방법의 핵심이자 연구 결과를 소통하는 일반적 관례임
     * 글쓰기는 단순히 결과 보고가 아니라, 구조적이고 의도적인 방식으로 생각을 정리하고 주요 메시지를 도출하는 과정임
     * 손글씨가 두뇌 연결성 및 학습·기억에 긍정적 영향을 준다는 연구 결과도 존재함

인간 중심 과학적 글쓰기의 필요성

     * 인간이 직접 작성한 과학적 글쓰기의 중요성을 지속적으로 인정해야 한다는 주장이 제시됨
     * LLM을 이용하면 전체 논문이나 심사 보고서를 빠르게 작성할 수 있지만, LLM은 저자가 될 수 없음 (책임성 결여)
     * 만약 글쓰기가 사고라면, LLM이 쓴 논문은 연구자가 아닌 모델의 ‘생각’ 을 읽는 것과 같다는 점이 문제로 지적됨

LLM 활용의 한계와 주의점

     * 현재의 LLM은 잘못된 정보를 생성할 수 있는데, 이를 ‘환각 현상’이라고 부름
     * LLM이 생성한 인용문이나 참고문헌이 허구일 수 있어 모든 내용을 꼼꼼히 검증해야 하며, 실제로 시간이 더 소요될 수 있음
     * LLM의 일부 문제는 과학 데이터베이스로만 학습된 모델로 개선될 수 있으나, 실질적 효율성은 아직 미지수임

LLM의 긍정적 활용 방식

     * LLM은 가독성 향상, 문법 교정, 다양한 논문 검색 및 요약, 아이디어 브레인스토밍 등에서 유용하게 활용 가능함
     * 작가의 슬럼프 극복, 다양한 설명 방식 제시, 새로운 주제 간 연결 발견 등 창의적 사고의 보조도 가능함

인간 고유의 창의성과 성찰의 가치

     * 그러나 글쓰기 전체를 LLM에 위임하는 것은 연구 분야에 대한 반성과 창의적 내러티브 구축의 기회를 잃는 결과를 초래함
     * 연구 내용을 매력적이고 설득력 있는 이야기로 재구성하는 과정은 학술 논문을 넘어 필수적인 인간 능력임

        Hacker News 의견

     * 읽기는 곧 사고임이라고 생각함. 그리고 읽기와 쓰기가 사고 과정이기 때문에, AI 모델에게 이를 습관적으로 맡기는 것은 위험함. 특히 학생들의 형성기에는 읽기와 쓰기 방식으로 생각하는 법을 스스로 배워야 함—반성, 메모 등 계속 훈련이 필요함. 계산기 사용과 비슷한데, 어느 정도 기본기를 갖춘 후라면 전자계산기 활용이 괜찮지만, 최우선으로는 머리로 계산하거나 손으로 풀어보는 경험이 있어야 함. 입력 실수나 계산 결과를 검산하는 습관 때문임. 아직 사고력이 자리잡지 않은 어린 학생들이 LLM에 일을 맡기기 시작하는 것에 우려를 느낌
          + 고대 이집트의 문자 발명 이야기(플라톤의 파이드로스에서 인용)를 예로 들고 싶음. 토트가 타무스에게 문자의 유익함을 강조하지만, 타무스는 문자에 의존하면 기억력이 약화된다고 경고함. 이런 논쟁이 2000년 넘게 반복되어온 셈임. 나 역시 타무스의 의견이 맞을 수 있다고 보지만, 현실은 우리 모두가 이제 읽기와 쓰기를 활용해 삶을 살아가고 있음
          + 실제로 무언가를 종이에 써보는 행위 자체에서 더 깊은 사고가 일어남. 특히 엔지니어링 분야에서 이러한 체득이 중요함. 그래서 많은 테크 회사에서 RFC 같은 문서 문화가 발달함. 이는 작성자 본인에게도, 검토자에게도 큰 도움이 됨
          + 읽기는 타인의 생각을 따라가는 행위지만, 쓰기는 내 생각을 직접 탐구하는 과정임. 그래서 쓰기가 많은 이들에게 고통스러운 것이고, 동시에 꼭 필요한 이유임. 그리고, 가르침 역시 중요함. 사고를 명확하게 정제하는 것은 아주 가치 있고 어려운 작업임
          + LLM이 사고에 미치는 영향은 계산기와 똑같다고 봄. 어떤 사고 단계를 생략하게 하지만 그 덕분에 다른 종류의 사고를 새롭게 할 수 있음. 내 생각은 LLM 덕분에 증가했음. LLM이 반복적인 작업을 덜어주니, 정보의 '신호'만을 빠르게 추출하거나, 다양한 분야의 아이디어를 조합하는 데 더 많은 시간과 에너지를 쏟을 수 있었음. 실수를 저지를 수도 있으나, LLM 없이도 속도만 느릴 뿐 비슷한 실수를 했을 것임. 실은 전문가가 아닌 분야에서는 연구 자체를 포기하거나 생각이 매우 좁아졌을 것임. LLM을 막는다고 사고가 깊어지는 게 아님. 계산기를 금지한다고 모두가 수학을 잘하게 되는 게 아니듯, 계산기가 허용되면 수학을 좋아하는 이들은 훨씬 더 멀리 나아갈 수 있음
          + 이 아이들은 AI가 발전된 미래에 살게 될 것임. 세상이 빠르게 변하니 아이들도 스스로 깨달을 것이라 믿음. 하지만 인터넷 세대의 꿈꿨던 디지털 풍요는 점점 잉여 콘텐츠 홍수와 정보 전쟁으로 악몽이 되어가고 있다는 현실도 있음
     * Paul Graham의 말을 인용하고 싶음: ""쓰기는 곧 사고임. 잘 쓰려면 명확히 생각해야 하고, 명확하게 사고하는 건 굉장히 어려운 일임. 실제로 쓰는 과정을 거쳐야만 할 수 있는 특별한 형태의 사고가 있음. 만약 써보지 않고 생각만 한다면, 실제로 생각한다고 착각할 뿐임. 세상이 '글을 쓰는 자'와 '쓰지 않는 자'로 나뉜다면, 겉보기보다 훨씬 위험한 결과를 낳음. 곧, '생각하는 자'와 '생각하지 않는 자'로 나뉘게 됨""
       https://www.paulgraham.com/writes.html
     * 사고와 쓰기는 밀접하게 연결되어 있음. 사고와 ChatGPT 사용은 그렇지 않음
       MIT Media Lab의 'Your Brain on ChatGPT' 링크: https://share.google/RYjkIU1y4zdsAUDZt
          + MIT Media Lab의 관련 프로젝트 개요: https://www.media.mit.edu/projects/your-brain-on-chatgpt/overview/
     * LLM이 실제로 어떻게 과학 논문을 ""쓸"" 수 있는지 의아함. 예를 들어, 웨스턴 블롯 결과, 특정 유전자 변형 마우스 데이터, 단일세포 시퀀싱 데이터 등이 주어질 때, 논문에서는 새로운 단백질을 밝혀내고 생쥐의 유전자를 편집해 어떤 경로가 변화하는지 설명하게 됨. LLM에 어떤 재료를 주고, 어떻게 이 발견이 의미 있는지 LLM이 알 수 있을까? 내 생각엔 LLM은 본질적으로 내가 시키는대로 패러프레이즈할 뿐임. 실제 글쓰기가 어려운 점은 '이야기를 어떻게 풀어갈지 결정하는 부분'임
          + 대학원에 가면 각 분야마다 논문 쓰기 표준 포맷을 배움. LLM이 충분히 많이 그 분야의 논문을 학습했다면, 사용자가 제공하는 정보들을 적절한 섹션에 맞춰 자동으로 배치 가능함. 논문 작성의 대부분 시간은 인용 처리와 형식 맞추기에 들어가기에, 이런 스타일적이고 반복적인 작업은 LLM이 대신해주면 오히려 좋음. 과학자는 엄정함과 명확성이 중요하지만, 스타일 맞추기 잡무는 자동화에 맡기고 싶음
          + LLM이 arXiv 같은 논문 저장소 전체를 학습해서 관련 논문들은 인간보다 더 잘 파악할 수도 있음
          + LLM도 계획이나 개요를 먼저 작성할 수 있으니, 이것 역시 일종의 글쓰기임
     * 말하는 것조차 사고임. 그래서 언론의 자유가 첫 번째 수정헌법이기도 함. 누군가가 내 발언권을 제한하면 내 생각도 통제함.
       글쓰기는 마치 '해리포터'의 '펜시브' 같은 초능력임. 머리에서 아이디어를 꺼내 분석의 여러 층위에서 살펴보고, 기억을 손쉽게 저장·정리할 수 있게 해줌
          + 언론의 자유에 관한 현재의 관념은 사실 비교적 최근(1910~1920년대)에 자리잡음. 그 이전에는 'free speech'의 의미가 지금과 매우 달랐다는 점을 다루는 훌륭한 팟캐스트(Radiolab 등)가 있었음
          + 관련 웹사이트: https://voicebraindump.com
     * 사람들이 ""글쓰기가 중요하다""고 말할 때, 사실은 ""내가 쓰면서 똑똑해지고 기분이 좋다""고 느끼는 것 아닌가 생각함. 실제로는 머릿속에서 오랫동안 아이디어를 조합하다가, 다 모였을 때야 비로소 높은 층위로 종합하는데, 이 진짜 종합 과정을 글쓰기 행위로 오해하기 쉽다고 봄. 만약 1주일 전에 써보려 했다면 비생산적이었을 것임
          + 적어도 나에게는 그렇지 않음. 생각과 아이디어, 지식을 종이에 구체적으로 옮기는 과정에서 오류나 부족한 점을 발견하고, 이를 바로잡을 기회가 생김. 단순한 수정에 그치지 않고, 새 시각이나 관점도 떠오름—이런 것들은 이전에는 내 의식에 없던 것임. 나는 글쓰기를 사고의 도구로 삼음. 또 다른 도구로는 그룹 브레인스토밍이나 토론이 있음. 이런 행위는 내 생각을 보완하고 더 견고하게 만들며, 새로운 방향이나 연결고리까지 제시해줌. Paul Graham의 에세이(아이디어를 단어로 옮기기)와 Paul Zissner의 'Writing to Learn' 책도 추천함. 글쓰기를 배울 때 연습 삼아 해보는 것은 일종의 '가르침' 효과로, 파인만 학습법과도 일맥상통함
            https://paulgraham.com/words.html
          + 내 개인 경험을 예로 들면, 실제로 써보면 내 사고나 지식의 모순점, 빈틈을 찾게 됨. 그것을 찾아내고 고치는 것 자체가 내가 '글쓰기가 곧 사고다'라고 하는 이유임
          + 나는 동의하지 않음. 내 방식은 각기 섞이지 않은 생각들을 우선 다 적어놓고, 구조가 드러날 때까지 배치와 재구성을 거침. 마지막에는 종합된 결과물이 완성됨. 어느 정도 구조가 뇌리에 있었겠지만 명확히 드러나 있진 않았음. 재배치가 반복되는 것만 봐도, 글쓰기 자체가 종합 작업이라는 증거임
          + 글로 적어놓아야만 허점이나 오류를 발견하게 됨. 그걸 쓰지 않으면 경험상 절대 찾지 못함
          + 너무 냉소적인 시각임. 사람마다 사고·작업 방식이 다르고, 각자 얻는 유익도 다름. 대부분 '글쓰기가 미덕'이라 과시하려고 말하는 건 아님
     * ""글쓰기가 곧 사고""라는 메시지에 공감하지만, 그건 조건에 따라 맞는 말임. 원래부터 그런 건 아니었음. 실제로 우리는 글쓰기가 저렴해진 후에야 글을 쓰며 생각을 발전시키는 법을 익혔음. 18~20세기 초 작가들의 작품과 일기를 보면, 톨스토이, 츠바이크, 괴테 등은 책 전체 내용을 미리 머릿속에 구상한 뒤 20~30일에 한 번에 써낸 것 같음. 과거에는 생각과 글쓰기가 별개였음. 현대에는 저렴한 종이와 컴퓨터 덕분에 그런 방식이 바뀐 것임. 지금 방식이 잘못됐다는 뜻은 아니지만, '생각=글쓰기'가 유일하다고 오해하면 안 됨. 소크라테스도 글쓰기가 기억력을 해친다고 했는데(완전히 틀린 말도 아님), 지금은 다들 글쓰기를 사용함. LLM에 대한 비판이 묘하게 아이러니함. 글쓰기에 대한 비유가 저절로 떠오름. LLM과 함께 자란 아이들은 사고 방식이 달라질
       것임
          + 오히려 ""생각이 글쓰기다""라는 주장에 반응한 것이라고 보임. 나도 글 없이도 생각은 충분히 가능하다고 봄. 단, 글쓰기는 완성된 생각을 단순히 쏟아내는 것이 아니라, 그 과정에서 내 생각을 비판하거나, 새로운 아이디어를 도출하거나, 단순화하거나 확장하는 데 도움을 줌. 글로 쏟아내면 메타적으로 재검토하기가 훨씬 쉬워짐
          + 종이가 비싸던 시절에는 일시적인 기록을 위해 왁스나 나무판을 사용하곤 했음. 전적으로 기억만으로 책 전체를 구상해 한 번에 쓴다는 주장은 실제로 얼마나 흔했는지 의심스럽고, 톨스토이의 '전쟁과 평화' 집필 과정에 관한 증거도 있음
            https://amazon.com/Tolstoy-Genesis-Peace-Kathryn-Feuer/dp/…
          + 중세 철학서에서 길고 논리적인 내용을 볼 때마다, 당시 임시 메모 용지조차 흔치 않았던 시대에 어떻게 저렇게 전개했나 궁금했던 적이 있음.
            그리고 ""LLM과 함께 자란 아이들이 생각이 달라질 것""이라는 말에 대해, 구체적으로 어떻게 달라질지 궁금함. 현재 대학생들이 LLM을 활용하는 모습을 보면, 달라진다기보다는 오히려 사고를 오히려 덜 하는 것 같아 걱정임
          + 유사하지만 조금 다른 시각으로 Larry McEnerney의 이론이 있음. 글쓰기를 사고를 위한 글쓰기와 전달을 위한 글쓰기로 구분함. 사고를 위한 글쓰기는 소크라테스 이전부터 존재하지만 개인적 행위에 가까움. 전달을 위한 글쓰기는 소설, 저널리즘 등 범위가 넓음. Larry는 주로 논문을 준비하는 학생들이 이 두 가지를 연결하지 못해 곤란을 겪는다는 점을 다룸
          + LLM은 기존의 ""글쓰기""와는 매우 다름. LLM은 창의적 에이전트에 가까움. 내가 내 생각을 글로 여러 번 반복, 수정할 수 있는 장점이 있는데, LLM을 사용하면 이제 '다른 누군가'가 사고, 글쓰기, 편집까지 대신하게 되어 내 사고량은 줄어듦. 자전거는 내 힘으로 더 멀리 가는 방식(글쓰기), 자동차는 완전히 다른 에너지원(LMM)이라면, 신체적 건강엔 어떤 게 더 좋을까? 그리고 톨스토이가 책 전체를 머릿속에 담아두고 순식간에 써냈다는 주장은 흥미로움. 실제 사례/증거가 궁금함. 톨스토이처럼 귀족이었다면 충분한 자원이 있었을 것으로 보임
     * 많은 이들이 LLM이 인간의 글쓰기 자체를 대체하면 인간 발달에 위험이 생긴다고 해석함. 나는 보다 낙관적으로 봄. 좋은 글쓰기가 좋은 사고로 이어진다면, 내 글쓰기 실력을 높이는 모든 노력은 사고력 향상으로도 연결됨. 그런 면에서 LLM은 글쓰기 실력, 나아가 사고력 자체를 개선하는 데 오히려 큰 도움이 됨. 배경 정보나 관련 주제에 대한 피드백도 강력하게 받을 수 있음. 절제 있게 LLM을 사용하면 오히려 더 나은 인간이 될 수 있다고 믿음
          + 나 역시 이 의견에 동의함. LLM은 사고의 도구이지, 사고의 대체물이 아님. 사실 시간이 지나면 읽기와 쓰기가 사고 방식을 완전히 바꾼 것처럼, LLM과 함께 사고하는 방식도 근본적으로 바뀔 수 있다고 느낌. LLM은 내 아이디어를 테스트하는 이상적인 상대가 될 수 있고, 사고의 과정 자체가 LLM과 함께 반복적으로 정제되는 공동작업이 될 수 있음
     * 본격적으로 깊이 생각이 필요할 때마다 sublime text를 켜고, 가능한 한 간결하게 상황을 한 줄씩 적음. 이때 스스로에게 아주 직설적이고 기본적인 질문을 던지면서 본질, 목표, 경로를 정말로 파악하려 노력함. 마치 조직폭력배 보스에게 답하는 것 같은 기분으로, 핑계나 명분 없이 진실만 추려냄. 그렇게 하면 전체 구조가 이해되고, 즉각적으로 할 일 목록까지 떠오름
     * 내 예전 매니저(겸 에디터)는 글쓰기를 '발견(discovery)'이라고 자주 말했음. 곧, 사고와 본질적으로 같은 맥락임. 이 의견에 동의함
"
"https://news.hada.io/topic?id=22277","Crush - 터미널을 위한 화려한 AI 코딩 에이전트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Crush - 터미널을 위한 화려한 AI 코딩 에이전트

     * 터미널에서 동작하는 AI 코딩 에이전트로, 다양한 LLM(대형 언어 모델) 과 연동하여 코드 작성, 워크플로우 자동화, 코드 컨텍스트 유지 등 코드 생산성 향상을 지원함
     * 여러 모델을 선택하거나 세션 중간에 자유롭게 모델 전환이 가능하고, 프로젝트별 세션/컨텍스트 유지를 지원함
     * LSP(Language Server Protocol) 연동, 확장 가능한 MCP(모델 컨텍스트 프로토콜) 지원, .gitignore 및 별도 파일 무시 등 개발자 친화적 기능을 제공함
     * macOS, Linux, Windows, FreeBSD 등 모든 주요 터미널 환경에서 구동되며, 패키지 매니저 또는 Go, 바이너리 등 다양한 방법으로 설치 가능함
     * 직관적 설정과 고급 커스터마이즈를 동시에 지원하며, 환경 변수, JSON 설정, 도구 화이트리스트 등 고급 유저까지 배려한 설계임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Crush

     * 터미널 환경에서 실행하는 AI 코딩 에이전트로, 개발자가 선호하는 LLM과 자유롭게 연동하여 코드 작성, 편집, 자동화를 지원함
     * 여러 모델(Anthropic, OpenAI, Groq, OpenRouter 등) 을 자유롭게 선택 및 전환할 수 있으며, 세션별로 컨텍스트를 독립적으로 관리함
     * LSP(Language Server Protocol) 를 통해 각 언어에 맞는 추가 컨텍스트를 받아 더 똑똑하게 코드를 보조함
     * MCP(모델 컨텍스트 프로토콜) 를 통해 외부 시스템, HTTP, 커맨드라인, SSE 등 다양한 소스로부터 추가 정보 수집 및 활용 가능함

주요 기능

     * 멀티 모델 지원: OpenAI, Anthropic, Groq, OpenRouter 등 다양한 LLM 연동, 직접 추가 가능
     * 세션 기반 작업: 프로젝트별 여러 작업 세션과 컨텍스트 분리, 관리
     * 유연한 모델 전환: 세션 중간에도 자유롭게 모델 변경, 기존 컨텍스트 유지
     * LSP 연동: Go, TypeScript, Nix 등 주요 언어의 LSP 연결, 코드 맥락 강화
     * 확장성: MCP 프로토콜 기반으로 외부 HTTP/CLI/SSE 등 추가 기능 손쉽게 확장
     * 광범위 플랫폼 지원: macOS, Linux, Windows(WSL, PowerShell), FreeBSD, OpenBSD, NetBSD 등 주요 OS 터미널에서 모두 동작
     * 직관적 설정: 별도의 설정 없이 바로 사용 가능하며, 필요시 프로젝트별/글로벌 JSON 설정 지원
     * 강력한 무시(ignoring) 기능: .gitignore, .crushignore 파일을 통해 맥락 제외 파일/디렉토리 관리 가능
     * 도구 화이트리스트: 도구 실행 시 사전 승인 및 자동 실행 옵션 지원, --yolo 플래그로 전체 프롬프트 스킵 가능(주의 필요)
     * 커스텀 프로바이더: OpenAI, Anthropic 호환 API를 자유롭게 추가 및 세부 옵션 지정 가능

설치 및 시작

     * Homebrew, NPM, Arch, Nix, Debian/Ubuntu, Fedora/RHEL 등 다양한 패키지 매니저 및 바이너리/Go로 설치 가능
     * 최초 실행 시 선호 LLM API Key(OpenAI, Anthropic, Groq 등) 입력 필요, 환경 변수로도 지정 가능
     * 환경 변수로 연동 가능한 대표 LLM: OPENAI_API_KEY, ANTHROPIC_API_KEY, GROQ_API_KEY, OPENROUTER_API_KEY, GEMINI_API_KEY, VERTEXAI_PROJECT 등

설정 예시

     * 글로벌 또는 프로젝트 단위 JSON 파일(./.crush.json, ./crush.json, $HOME/.config/crush/crush.json)로 고급 옵션 적용
     * LSP 설정: 각 언어별 명령 지정 가능
{
  ""lsp"": {
    ""go"": { ""command"": ""gopls"" },
    ""typescript"": { ""command"": ""typescript-language-server"", ""args"": [""--stdio""] }
  }
}

     * MCP 설정: HTTP/CLI/SSE 기반 외부 확장 예시
{
  ""mcp"": {
    ""filesystem"": {
      ""type"": ""stdio"",
      ""command"": ""node"",
      ""args"": [""/path/to/mcp-server.js""]
    }
  }
}

     * 파일 무시 및 도구 승인
          + .crushignore로 특정 파일/폴더 제외
          + 도구 실행 화이트리스트 또는 --yolo 플래그로 프롬프트 스킵

고급 사용자 기능

     * 커스텀 프로바이더 등록: OpenAI/Anthropic 호환 API 추가, 가격/컨텍스트 등 세부 옵션 지정
     * 로깅: 프로젝트별 로그 파일 제공, crush logs, crush logs --follow 등 CLI 명령으로 실시간 확인
     * 디버그 옵션: --debug 플래그 또는 config로 상세 로그 활성화

   aider는 진짜 별론데;;

        Hacker News 의견

     * 대부분의 터미널 기반 AI 코딩 에이전트들이 텍스트 UI를 화려하게 꾸미려는 시도를 하고 있는 점이 이상하게 느껴짐. 여백이 많고, 라인 아트, 위젯, 아스키 아트, 그라디언트, 애니메이션까지 추가됨. 하지만 원하는 전체 키 바인딩, 탭 자동완성, 일관된 스크롤백, 혹은 깜빡임 없는 텍스트 렌더링 같은 기본 기능들은 빠져 있음. 그나마 이 툴은 node.js로 작성되지 않아 터미널 출력이 불필요한 리드로우를 줄이는 등 성능 면에서 기대할 만함. 그런데 REPL이나 CLI를 기대하고 사용해보면 상호작용 모델은 똑같은데도 완전히 다르게 동작하고, 에디터나 리더 계열의 Unix TUI와도 확연히 느낌이 다름. 이런 흐름은 Claude Code만 따라한 것인지 아니면 더 이른 시점에 시작된 것인지 궁금함. 그래서 나는 여전히 Aider를 선호함. REPL에 가까운 외형과 사용성을 제공함
          + 이 툴은 Charm이라는 회사에서 만들었고, 명령어 창을 매력적으로 만드는 것이 미션임. LLM 열풍 이전부터 수년간 활동해왔음. golang용 CLI 프레임워크와 툴을 만드는 곳임
          + 내가 터미널에서 좋아하는 부분은 명령어를 치고 여러 소스와 프로그램의 액션 및 아웃풋을 로그처럼 시퀀스 대로 쭉 볼 수 있는 스크롤링 중심 워크플로우임. 진짜 원하는 건 강력한 HTML 기반의 멀티프로그램 스크롤 워크플로우임. 그런데 요즘 이런 시도들은 양쪽의 단점을 합쳐놓은 식임. 좋은 UI는 더 좋은 렌더링 시스템에서 만나고 싶음
          + 이러한 텍스트 UI 흐름은 사실 AI 에이전트 나오기 전부터 charmbracelet의 특유한 스타일이었음. 나는 전통적인 TUIs와는 다르게 키 바인딩이 직관적으로 발견 가능해서 마음에 드는 편임
          + 요즘 이 인터페이스들이 급속도로 팬과 개발자를 모으는 이유 중 하나는 많은 사람들이 원래 그래픽 기반의 IDE 스타일 에디터에 익숙하기 때문인 듯함. 모든 개발자가 터미널에서만 작업하지는 않음. (나는 여전히 X/Wayland도 안 켜는 날이 있음)
          + 적어도 emacs에서 Claude Code를 사용할 수 있음 https://github.com/stevemolitor/claude-code.el
     * 이 툴의 좋은 점 중 하나는 아직 초기 단계라 코드가 매우 명확하고 모듈화되어 있음. 툴 호출, 세션, 자동 요약, 영속성 관리 등 에이전트를 설계하고 싶을 때 참고할 만한 훌륭한 청사진임. 이 커밋 링크를 반드시 저장할 만함
          + 커밋 링크는 https://github.com/charmbracelet/crush/releases/tag/v0.1.8 임
     * 데모 GIF에서 무슨 일이 일어나는지 실제로 읽고 싶은 사람들을 위해 ffmpeg로 느리게 변환해서 비디오로 만들어 올림 https://share.cleanshot.com/XBXQbSPP
     * 15분 정도 진지하게 사용해봤음. Claude Code와 비교했을 때 장점은 아름다운 UI, 변경 파일 및 비용 등을 추적할 수 있는 유용한 사이드바, 수월한 변경사항 수락 UX(핫키, 보기 쉬운 diff 제공)임. 반면 단점은 여러 모델을 조합할 수 없고, 불필요한 이진 파일들이 많이 디렉토리에 추가됨. 초기 init이 CHARM.md라는 파일을 생성하지만, 내가 모델에 공유하고 싶은 정보와 맞지 않았음. 예를 들어, 내 Go 테스트 케이스가 PascalCasing을 쓰는 것까지는 전달되지 않음. 또한 Ctrl+C로 종료 시 내 터미널이 크래시 됨
          + 초기 init이 CHARM.md를 만든다는 부분, 이젠 제발 모두가 잘 알려진 단일 agent instruction 파일 표준(예: AGENT.md)에 합의했으면 좋겠음. Amp가 자체 CLI 툴을 위해 홍보하는 표준이긴 하지만, 그 아이러니를 알면서도 그 방식을 지지하게 됨 https://ampcode.com/AGENT.md 참고. 아니면 이런 우회적인 해킹도 있음 https://kau.sh/blog/agents-md/
     * 진짜 중요한 질문은, 새로운 에이전트 중 어떤 것이 로컬 모델을 제대로 지원하는지임. 외부 API 의존성을 없애고 싶고, 어느 정도 성능 희생을 감수할 의향도 있음
          + Crush는 Ollama 지원을 추가하는 이슈가 진행 중임(2주 됨)
          + 대부분의 에이전트들은 OpenAI 호환 엔드포인트에서 동작함
          + OpenHands는 원하는 어떤 LLM이라도 설정 가능함 https://github.com/All-Hands-AI/OpenHands
          + Aider도 로컬 모델을 지원한다고 명시되어 있지만, 직접 써보지는 않았음 https://aider.chat/docs/llms.html
     * Claude Code, opencode, aider, cortex 같은 이런 새로운 툴들을 모두 비교해주는 표가 있으면 정말 좋겠음. 각 툴의 작동 방식이나 차별점이 쉽게 한눈에 안 들어옴
          + 상업용 모델을 이용한 비교나 벤치마킹이 비용 때문에 매우 어려움. 최근에 논문을 썼을 때도 SOTA 상업모델 여러 개 평가하는 데 1만 달러 넘게 썼음. 오픈 모델과의 비교는 싸게 할 수 있었지만, 리뷰어들이 ""최고""와의 비교를 원하니 어쩔 수가 없었음. 이뿐 아니라, 상업 모델의 내부 구조나 스택은 투명하지 않고 언제든 바뀌기 때문에 너무 비효율적임. 학술 연구에서 상업용 모델과의 비교를 무조건 요구하는 것은 바람직하지 않다고 생각함
          + opencode가 원래 이름이었는데, 개발자들 간에 갈등이 생겨 이름이 바뀐 것으로 기억함
          + 퍼포먼스는 단순히 툴만이 아니라, 사용 모델, 코드베이스(컨텍스트), 그리고 주어진 작업(프롬프트)에 따라 달라짐. 요소들이 독립적이지 않고 조합에 따라 성능 차이가 큼. 예시로 Claude Sonnet 4와 Claude Code가 백엔드 파이썬 기능 구현엔 좋고, Gemini 2.5 Pro가 프론트엔드 리액트 코드 수정엔 더 좋았음. 즉, 모든 변수를 고정하고 툴만 비교할 수 없고, 툴모델컨텍스트*프롬프트까지 조합이 필요함. 16x Eval이 이런 부분을 일부 다루지만, 툴 같은 요소는 아직 포함되지 않음 https://eval.16x.engineer/
     * ""glamorous""는 영국 영어에서도 쓰이는 표현임 https://dictionary.cambridge.org/dictionary/english/glamorous
     * 지난 몇 주 동안 Crush를 써보고 있는데, 정말 기대가 큼. Charm을 오래 팔로우해왔고, 개발자 경험을 제대로 이해하면서 사람들이 좋아할 툴을 꾸준히 만드는 몇 안 되는 팀임. AI 코딩 레이스에 이렇게 일찍 합류한 것도 좋은 신호임. 실제로 사용하는 사람들이 만든 툴이라는 점이 분명함
     * 또 하나 나왔는데, 이번엔 정말 디자인이 좋음. 반드시 테스트해볼 예정임. 모든 툴에서 아쉬운 점(EDIT: opencode는 github 인증이 가능함)은 월 정액제 서비스(github copilot, claude code, openai codex, cursor 등)에 내 구독으로 바로 인증할 수 없다는 것임. 서비스는 구독하고 있지만 인터페이스가 마음에 안 들 때 그걸 자유롭게 바꿀 수 있으면 정말 좋을 듯함
          + 대부분의 툴이 다른 프로그램이 월 구독을 ""사용""하는 걸 지원하지 않음. 그래서 API 키를 따로 받아야 하고, 토큰 단위로 비용을 내야 함. 심지어 Claude Code도 한동안 자체 구독으로 연동되지 않았음
     * LSP-강화: Crush가 일반적으로 쓰는 것처럼 LSP를 추가 컨텍스트로 활용함. 이게 내 기준에서 가장 흥미로운 기능임. 멀티 세션이나 프로젝트 기능도 관심이 감
          + LSP MCP가 있어 다른 에이전트에서도 같이 사용할 수 있음
"
"https://news.hada.io/topic?id=22305","Google Play와의 작별 인사","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Google Play와의 작별 인사

     * KIT는 Privacy Friendly Apps 프로젝트를 통해 Google Play에서 30개 이상의 개인정보 친화적 앱을 제공함
     * 최근 유지 관리 부담 증가로 인해 Google Play에서 더 이상 이 앱들을 계속 제공하기 어렵게 됨
     * 기존에 설치된 앱들은 삭제되거나 영향을 받지 않지만, 앞으로 업데이트 제공이 중단됨
     * F-Droid Store를 통해 동일한 앱 지원 및 업데이트를 받을 수 있음
     * 사용자는 공식 안내에 따라 데이터를 유지하며 F-Droid로 이전할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 소개 및 배경

     * KIT의 Security, Usability, Society(SECUSO) 연구팀은 2016년부터 불필요한 권한 요구 없는 기본 기능 중심의 앱을 제공하는 프로젝트를 시작함
     * 프로젝트 초기에는 Privacy Friendly Torchlight 앱으로 출발하여, 현재는 30개 이상의 Privacy Friendly Apps가 개발됨
     * Google Play Store를 기반으로 35만 회 이상의 설치 기록을 달성했으며, 사용자들의 지속적인 지지와 피드백에 감사의 뜻을 전함

변화된 환경과 Google Play 중단 배경

     * 2025년을 기점으로, Google Play에서 앱을 유지·업데이트하는 데 드는 노력이 F-Droid에 비해 과도하게 커졌음
     * 연구 그룹 규모의 리소스로는 Google Play Store 내 앱 유지관리가 더 이상 가능하지 않음
     * 앱의 불필요한 권한 요구 관행은 여전히 문제로 남아 있으나, 본 연구팀은 운영 효율성 문제로 Google Play 배포를 종료하게 됨

기존 사용자 안내

     * 이미 Google Play를 통해 설치된 앱은 삭제되거나 즉시 중지되지 않음
     * 단, 향후 업데이트가 중단되어, Android OS 변화로 인한 호환성 문제 발생 시 정상 동작이 어려워질 수 있음
     * 지속적인 지원과 업데이트를 원할 경우 F-Droid Store로의 이전을 권장함

F-Droid Store 안내 및 이전 방법

     * 모든 Privacy Friendly Apps는 앞으로도 F-Droid Store에서 제공 및 지원될 예정임
     * F-Droid로의 원활한 이전과 기존 데이터 유지를 위한 이전 안내서가 제공됨
          + Play Store에서 F-Droid로 앱 이전 방법 안내서
     * 지원 및 문의는 good-bye-google@secuso.org 이메일을 통해 가능함

        Hacker News 의견

     * 일단 대충 봐도, SECUSO의 앱들은 계속해서 업데이트할 필요가 없는, 이미 완성된 독립형 오프라인 소프트웨어임을 알 수 있음
       SECUSO 앱 목록 참고
       하지만 지금은 Google에서 이런 전략을 인정하지 않음
       Google Play에 앱을 올리려면 최근 1년 내에 릴리즈된 SDK를 계속해서 타깃팅해 업데이트해야 함
       이를 지키지 않으면, 정책 위반 경고를 연달아 받고, 앱 노출 순위가 내려가거나, 결국 신규 유저에게 아예 제공이 중단됨
       SDK 업데이트는 단순하지 않고, 전체 변경 로그를 꼼꼼히 읽고 테스트하지 않으면 새 버그가 발생하기 쉬움
       나도 앱이 3개인데, 매년 SDK 업데이트에 너무 많은 시간을 들이고 있음
       30개 앱을 관리한다고 상상하기도 싫음
       Google은 이런 정책이 보안을 강화하고 사용자 경험을 높인다고 하지만, 오히려 쓸모 없는 업데이트가 많은 광고 중심 앱만 살아남도록 만들고 있음
       여전히 스토어에 스파이웨어 앱들은 버젓이 올라가 있음
       참고: Play SDK 타깃팅 정책
          + 이런 점을 폐쇄형 마켓을 옹호하는 사람들에게 실질적 손해라고 여러 번 얘기해봤음
            순수하게 유용한 앱을 만들어 공유하고픈 개발자들도 존재하지만, Play와 App Store는 개발로 수익을 내려는 이들만을 중심으로 설계됨
            그렇게 되어야 Google/Apple도 돈을 벌 수 있기 때문임
            그래서 변화가 거의 없는 커뮤니티 소프트웨어에는 아주 적대적임
            이런 소프트웨어가 오히려 단순함, 프라이버시, 장기간 유지 면에서 목적에 가장 잘 맞는 경우가 많음
            그래서 F-Droid 같은 대안 마켓이 있다는 게 참 다행임
          + “또한, 이 앱은 기기에서 스크린샷을 찍지 못하게 함”
            왜 '보안' 앱들은 항상 이런 기능 제한이 있을까?
            특히 은행 앱에서 이럴 때 아주 성가심
            예전에 Schwab에서는 거래량이 최고치일 때 주문을 제대로 추적하지 못해, IRA 계좌에 있는 주식보다 더 많이 팔리기도 했었음
            그런데 앱에서 스크린샷을 제한해놔서, 주문 취소나 교체 확인을 받았다는 증거를 남길 수 없었음
            IRA에서 본인 소유 분보다 더 팔아버린 건 명백하게 Schwab의 버그인데, 이런 중요한 내용을 로컬에 저장도 못하게 막는 자체가 현대 앱의 최악 기능 중 하나임
          + SDK 업데이트가 정말 골치임
            2010년에 ChromaDoze를 만들기 시작해서 여러 번 경험해 봄
            최근 가장 귀찮았던 건, 포그라운드 서비스가 예전엔 항상 알림 바에 떠야 했는데, 지금은 사용자 승인이 없으면 안 됨
            그래서 POST_NOTIFICATIONS 권한을 요청하는 버튼을 넣었는데, 서비스 실행 후에 권한을 받아도 되도록 마법(?) 같은 방식을 직접 만들어야 했음
            제스처 네비게이션이 기본값이 되고 나서, 화면 좌우 끝에서 스와이프 이벤트가 UI 입력을 가로채기에, UI 너비를 자동으로 줄이는 코드까지 넣어야 했음
            드로잉 앱은 setSystemGestureExclusionRects()도 200dp 제한 때문에 쓸 수 없음
            그리고 요즘은 앱이 세로로 엣지-투-엣지로 렌더링되어 반투명 상태바와 내비게이션 버튼 뒤로 숨어서, 특정 영역을 피하도록 따로 처리해야 했음
            제스처 네비게이션이 기본이 되면서, 많은 개발자가 3버튼 내비게이션 테스트를 안 하다 보니, 하단 UI가 네비게이션 버튼과 겹쳐서 조작이 안 되는 앱도 흔히 보임
          + 나도 이런 불만을 가지고 있음
            가족이나 소수 지인용 틈새 앱 하나 제작하는 것도 이제는 매우 어려워졌음
            예전에도 쉬운 건 아니었지만, 계속 최신 요구사항에 맞추느라 들어가는 시간과 노력이 현실적으로 유지 불가함
            웹앱도 결국 호스팅 비용과 스토리지 문제 때문에 만만치 않음
          + 이 주제를 보면 참 아이러니하게 느껴짐
            Hacker News와 ArsTechnica 등에서 예전에는, 구버전 API를 악용해 데이터 빼가고, 팝업·푸시 알림 남발하는 악성 앱을 방치한다며 Android를 신랄하게 까댔음
            그런 것들을 막기 위해 여러 변화와 보안 강화, 새로운 디자인을 도입한 iOS를 과격하게 칭찬하던 분위기도 있었음
            그런데 이제 정책변경으로 알림권한 다이얼로그 구현 같은 아주 사소한 것에도 불평이 심함
            현실적으로, SDK 변경이 강제되지 않으면, 모든 앱이 여전히 사용자 파일·문서·사진에 무작위 접근하고, 백그라운드에서 가상화폐 팔으려 스팸 알림 보내는 장터가 되었을 것임
            물론 이런 것들을 바로잡으려면 오픈소스 개발자들도 마찬가지로 사용자 권리를 존중하는 API 구현에 적극 나서야 한다고 생각함
     * 결국 나도 내 Google Play 앱들을 버린 상태임
       별 이유 없이, 어이없이 많은 시간과 노력을 들여 계속해서 업데이트하는 건 감당이 안 됨
       원래는 광고 한 줄(추적 없고, 인터넷 허가만 필요)만 딱 넣어도 월 100만 유저에 서버비는 충분히 뗄 수 있었음
       광고 없는 버전이 2달러였지만, 실제로는 이를 원하는 1% 유저를 위해 만든 옵션이었음
       지금은 대부분이, 전체화면 광고·트래커·구독만 가득한 앱으로 대체됨
     * SECUSO는 안드로이드 앱 생태계의 등대 같은 존재라 생각함
       이런 앱 유지 보수가 '사람을 덜 힘들게' 할 수 있도록, 스마트폰 플랫폼도 더 안정적이고 덜 변화가 잦았으면 좋겠음
       윈도우 소프트웨어를 매년 모두 재컴파일해야 한다면 그건 말도 안 되는 짓일 텐데, 현실이 이렇게 됨
          + 현실적으로, 요즘 윈도우에서도 새 소프트웨어를 돌리려면 Microsoft에서 발급받은 인증서로 서명해야 Defender 검사에서 통과할 수 있음
            향후에는 주기적인 업데이트 요구가 윈도우 앱에도 적용될 수도 있음
            다만 엔터프라이즈 윈도우에서만 그룹 정책으로 개별 .exe와 .dll 예외 처리를 직접 해줘야 하는 형태가 될 수도 있음
     * 나도 비슷한 경험이 있음
       10년 넘은 게임 앱들이 정책 변경 때문에 수일 내로 다 삭제됨
       앱을 빠르게 빌드해 재등록하지 못하겠어서 APK를 내 사이트에서 따로 배포 중임
       서드파티 서비스 전혀 없고 완전 독립형인데도 검열에 통과하지 못함
     * 참고로, iOS에서도 비슷한 일이 있었음
       관련 기사: 7년간 업데이트 없는 iOS 게임도 삭제
     * 비슷한 상황에서 난 아예 PWA로 전환함
       앱을 Java에서 Dart로 옮기고, UI는 HTML로 직접 만듦
       푸시 알림을 쓰지 않아 PWA에서 생기는 제약(알림 문제)도 없음
       아쉬운 점은 i18n(다국어 처리)이 단순하지 않아서 직접 구현할 예정임
       좋은 점은, PWABuilder 같은 툴로 PWA를 APK로 쉽게 패키징할 수 있고, iOS와 일반 웹 브라우저에서도 모두 쓸 수 있음
     * Google이 점점 모바일 개발 정책을 엄격하게 적용하고 있지만, 예외 처리가 필요할 때 Apple에 비하면 아직 덜 까다로운 편임
       Apple이 이런 점에서는 여러 면에서 더 어렵게 만듦
     * SECUSO 이메일에 도함수(∂) 기호가 들어있는 이유가 궁금함
          + primitive한 이메일 스크래퍼가 @만 찾으니 그걸 방지하려 한 트릭임
            KIT 전체 홈페이지에서 이런 방식으로 처리하고 있음
     * 나는 오직 F-Droid에서만 앱을 내려받으니 이런 변화가 별 영향 없음
     * Google/Epic 소송에서 Epic의 승리를 특별히 지지하는 건 아니지만, 이번 판결로 Play Store 정책을 담당하는 사람들이 개발자 친화적 정책을 도입하는 계기가 되었으면 함
       그렇지 않으면 누군가가 다른 대안을 만들 것임
       Play Store는 보안과 프라이버시를 내세우면서, 정작 고품질 앱 출시를 갈수록 어렵게 만들고 있음
"
"https://news.hada.io/topic?id=22320","AI 연구자들, NBA 스타처럼 2억 5천만 달러 연봉 협상중","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   AI 연구자들, NBA 스타처럼 2억 5천만 달러 연봉 협상중

     * 실리콘밸리 AI 인재 전쟁이 NBA 스타 영입 경쟁만큼이나 치열하게 진행되고 있음
          + 메타CEO 마크 주커버그가 24세의 AI 연구자 Matt Deitke를 직접 영입하려고 시도하면서, 4년 동안 약 1억 2,500만 달러 상당의 주식 및 현금 보상을 제안
          + 거절 당하자 4년간 약 2억 5천만 달러(약 3,400억 원) 의 스톡과 현금 보상을 제시
     * 젊은 AI 연구자들은 수백억~수천억 원대 연봉과 스톡옵션을 제시받으며 에이전트와 동료 조언까지 구하는 상황
     * Meta, OpenAI, Google 등은 연봉 제한 없이 거액을 제시하고, GPU 등 연구 리소스 제공도 경쟁의 일부임
     * AI 인재 영입 과정이 스포츠 선수 트레이드처럼 SNS에서 실시간 중계되며 업계 내 소수 전문가들의 협상력이 극대화되고 있음
     * 연구자의 소속과 비전, 동료와의 협업도 금전적 보상만큼 중요한 요소로 부상함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

A.I. 연구자 초고액 연봉, 실리콘밸리의 새로운 슈퍼스타

  AI 연구자, NBA 스타와 같은 시장 가치

     * 최근 AI 기술자들이 NBA 스타 선수처럼 취급받으며, 자신만의 협상팀과 에이전트를 통해 기업과 계약을 논의하는 현상 확산
     * Meta의 마크 저커버그는 한 신생 스타트업 창업자인 24세 AI 연구자에게 4년간 약 2억 5천만 달러(약 3,400억 원) 의 스톡과 현금 보상을 제시
     * 최초 제안(1억 2,500만 달러)을 거절당하자, 직접 만나 협상을 주도하며 조건을 두 배로 상향하는 모습이 소개됨
     * 이 과정에서 연구자는 동료들과 조언을 구하며, 마치 스포츠 에이전트처럼 주변 네트워크와 전략을 활용함

  AI 인재 영입 경쟁의 스포츠화

     * 실리콘밸리 내 AI 인재 확보 경쟁이 NBA 자유계약 시장 못지않게 과열 양상
     * 수십억~수백억 단위의 연봉 패키지가 공개적으로 오가고, SNS 등에서 스포츠 트레이드 뉴스처럼 중계되는 현상 발생
     * TBPN 같은 미디어가 ""MS, DeepMind 인력 20명 이상 영입""과 같은 그래픽 카드 뉴스를 내보내며 열기를 부추김
     * AI 기업들은 연봉 상한선이 없고, 스톡옵션이나 연구에 필요한 GPU 등 리소스 제공까지 패키지화함

  AI 연구자들의 협상력과 조직 문화

     * AI 연구자는 수요에 비해 인재 풀이 적고, 최고급 시스템 경험자는 더욱 희소해 협상력이 극대화됨
     * 오퍼를 받은 연구자들은 비공식 채팅방(Slack/Discord 등) 에서 조건을 공유하고, 경쟁 제안 유도를 위한 전략을 함께 논의함
     * 친구 및 동료와 함께 일할 수 있는 환경도 큰 매력으로, 입사 후 동료 영입 시도도 빈번함
     * OpenAI 등은 경쟁사 오퍼를 받은 인력이 즉시 수락하지 않고, 경영진과 상의하도록 구조를 개편

  돈과 함께 제공되는 연구 리소스

     * Meta 등은 연구자들에게 수만 대의 GPU 할당을 약속하는 등, 단순 연봉 외에 실질적 연구 자산도 제안
     * 최고의 AI 인재 풀 확보를 위해 'The List' 라는 최상위 연구자 명단을 만들어 직접 스카우트 진행
     * 명단에는 AI 관련 박사 학위, 유명 연구소 경력, 주요 연구 성과 등이 포함

  최신 사례와 스타트업 창업자 합류

     * Vercept라는 AI 스타트업 공동창업자인 Deitke는 약 10명의 팀을 이끌며, 이미지·음성·텍스트를 처리하는 AI 챗봇(Molmo)을 개발
     * Meta는 그를 영입하기 위해 2억 5천만 달러를 제시, 결국 팀과 동료 조언 끝에 오퍼 수락
     * 이런 인재 영입 경쟁은 업계 전반에 조직 문화·비전·금전적 보상 모두를 동시에 중시하는 흐름 확산

업계 영향과 전망

     * AI 인재 쟁탈전으로 연봉·연구 환경 모두 급격히 상향되고 있으며, 신생 기업도 거대 기업과의 경쟁력 확보를 위해 창의적 보상을 시도
     * 이로 인해 업계 내 인력 이동, 조직 개편, 연구 자원 배분 등의 패러다임 변화가 계속될 전망

        Hacker News 의견

     * archive.is 링크
     * 이 사례의 맥락을 설명해 볼 수 있음
          + 그는 이미 1년 차 박사생임에도 저명한 AI 학회에서 1저자로 두 번이나 수상함
          + 보통 이 수준까지 가려면 수년의 연구 경력이 필요하지만, 대부분의 박사도 이룰 수 없는 성과임
          + 그가 만든 오픈 비전-언어 모델 모음집 Molmo는 Meta의 개인화 멀티모달 AI라는 Zuck의 비전과 직접적으로 맞닿아 있음
          + 그는 자신의 스타트업에서 스카웃된 것이므로, 스타트업 성공 시 가치를 훨씬 웃도는 금액이어야 이직을 설득할 수 있었을 것임. $250M은 본인은 스타트업 성공 밸류에이션 기대치를 넘어서는 수준이라 판단했을 가능성이 높음. 스타트업을 위해 박사도 포기했던 인재를 Meta의 '직원'으로 오게 하려면 거액의 프리미엄이 필요했을 것임
          + 참고: mattdeitke.com, Molmo 블로그
          + “가치가 더 클 수도 있다”고 하지만, 그 가능성은 얼마나 될까 의문임
          + 아마 인터넷 밈을 접하고, Meta에 그다지 가고 싶지 않아서 말도 안 될 정도로 높은 금액을 불렀는데 Meta가 “괜찮음”이라고 한 것 같음
          + 이건 비이성적이고 정말 Beeple NFT 같은 장면임. 지금이 어느 정도 버블인지 보여줌. Meta가 Zuck의 FOMO를 해소하기 위해 스타트업 밸류를 뒷받침한 단순 산출에 따라 진짜 현금을 준 거라면, 그건 잘한 일임. 이 버블은 빨리 끝나야 한다고 생각하지만 그는 그 값어치의 일부라도 챙길 자격이 있음. 이런 코미디 자체만으로도 자격 충분함
          + 모든 주장에 공감함. 하지만 Mira나 Ilya처럼 이미 수백억을 받은 상태였다면, 굳이 Zuckerberg 밑에서 일했을지 의문임. Meta에 무릎 꿇는 것에 영광이 있을까
     * 이런 뉴스가 나올 때마다 친척들이 링크 보내면서 “너는 왜 Meta 안 가냐, 왜 돈 못 버냐”라고 물어봐서 부담임. Mark야, 이런 건 조용히 해주면 나도 조용히 평범할 수 있음
          + z uck이 세상을 크게 더 나쁘게 만들고 있다고 말해줄 수도 있음. 그렇게 만드는 데 동참하고 싶지 않다고 설명 가능함
     * 이건 승자독식(혹은 승자독식에 가까운) 경제 구조의 산물임. 예를 들어 최고의 LLM이 그 다음 모델보다 1.5배만 뛰어나도, 세계 대부분의 사용자가 그 최고를 원하게 됨. 그만큼 수십억 달러의 이윤이 좌우되기에, 기업들이 최고의 인재를 데려오기 위해 수백만~수억 달러도 쓸 수 있음
          + 이는 스포츠, 음악, 글로벌 엔터테이너가 예전보다 훨씬 많은 돈을 받는 것과 같은 이유임. 과거보다 훨씬 커진 시장을 상대하니 보상도 그에 비례함
          + AI는 승자독식 구조로 가지 않을 것임. 정말 그런 상황이 되려면, 1) 승자가 즉시 독점적 위치가 되어야 하고, 2) 모든 투자가 경쟁사에서 승자로 이동해야 하며, 3) AGI/ASI 연구가 중단되어야 함. 현재 SOTA 모델도 다 비슷비슷하고, 압도적 비밀 기술은 없음. 결국 누군가 AGI에 근접하면 곧 경쟁사가 따라갈 것이고, 곧 오픈소스로도 공개될 것임. 데이터와 컴퓨트가 더욱 중요하게 지켜질 요인임
          + 실제 OpenRouter 데이터를 보면 다른 양상임
               o 구글이 28.4%, Anthropic 24.7%, Deepseek 15.4%, Qwen 10.8%로 나눠 잡고 있음
               o 만약 승자독식 시장이었다면 새로운 SOTA 모델이 나올 때마다 즉시 시장이 그쪽으로 쏠린다는 것인데, 현실은 그렇지 않음. 모델 변경도 간단한데도 각각 점유율 나뉨. 최고 수준 인재 연봉이 오르는 건 '승자독식'이라는 인식에서 비롯됐지만 실제 시장 구조는 달랐음
          + 단지 AI 버블임. 진짜 AI라기보다는 그저 화려한 챗봇일 뿐임. AGI가 등장한다면 가능성은 있지만, 현실적으로 존재하지 않고, 설사 있다 해도 정의 불가함
          + 다음 최고의 모델이 0.5배만 저렴해도 결과가 이미 '충분히 쓸 만하다'면 많은 곳에서 그걸 선택할 것임. 실제 우리 회사는 다양한 에이전트 별 상황에 맞춰 비용 최적화 위해 모델을 적절히 바꿔 쓰고 있음
          + 승자독식 진입장벽은 안 보임. 모델 제공자들은 마치 오디오 부품처럼 교체가 쉬움. 그리고 최근엔 최고 SOTA 모델의 생명주기가 몇 주임
     * 이런 보상은 매우 소수만 얻을 수 있음
          + 프런티어 AI는 극도로 똑똑한 소수 집단이 빅테크, 프런티어 AI, 그 외 영역을 오가며 만들고 있음
          + 이런 사례가 축구나 F1 같은 스포츠 스타의 보수와 비교 가능함. 유명 선수 하나가 수십억을 벌어들이듯, AI 영역에서는 훨씬 더 큰 파워가 있다 볼 수 있음. 연간 $50M~$100M이 일부는 스톡으로 주어지는 구조는 타당함
          + 개인적으로 배우, 운동선수보다 훨씬 큰 보상을 받는 연구자 사례가 공개되는 것이 반가움. 앞으로 연구자가 연예인만큼 유명해지고, 더 많은 젊은이가 기술·연구에 도전하기를 바람
          + 이해 안 되는 점은 #2, #3 AI가 #1에 비해 수년 뒤처진 것이 아니라 몇 달 정도임. 이 '선점' 차이가 엄청난 보상을 정당화할 정도인지 모르겠음. 대기업에 기술이 실제로 적용되고, 대중의 습관이 바뀌려면 결국 수년 걸림. 닷컴 버블이 가르쳐줬듯, 시장 최종 승자가 꼭 최초였던 적이 거의 없음
          + 이런 연봉은 진짜 '거인'이나 비즈니스 파워를 이해한 리더에게 가는 줄 알았음. 그런데 이 사례의 당사자는 학문적 성과도 대단한 편이 아니고, 평범한 조교수 정도일 뿐임. 그 정도면 물론 뛰어나긴 하지만, $100M에 달하는 보수의 독창성을 설명할 만큼은 아님. 이 인재가 왜 그렇게 특별한지 설명해 줬으면 좋겠음
          + 프런티어 AI에서 '스케일'이란 수억 명 서비스 경험임. 아주 똑똑한 인재는 많지만, 이들은 실제 대규모 시스템을 만들어본 경험이 있음. 그게 차이점임
          + 네이마르 이름 달린 유니폼을 팔아서 수익을 거둘 수 있지만, AI 연구자는 다름. 실질적으로 성과를 내야 함. 이미 $100M을 가진 사람이라면 그냥 관성적으로 일할 가능성도 높음
     * 기사 제목을 이렇게 고치고 싶음
          + ""어떤 AI 연구자가 Meta가 엑셀에 0을 계속 추가해 만들 수 있는 가짜 돈으로 $250M 보상 협상 중임""
          + 스톡(주식) 지급은 성장이 예상되는 기업에는 좋은 선택임. 기존 지분 희석만 감수하면 새 스톡으로 원하는 인재 데려올 수 있음. 직접 현금 지출이 아니니, 회사 성장 기대 내러티브만 강하게 만들면 됨
          + 투자자 입장에서는, 이 직원 한 명이 정말로 Meta 시가총액을 $255M 늘릴 수 있을지 자문해볼 필요 있음. 현금 $5M, 나머진 주식임
          + 다 가짜 돈은 아님. Meta(FB)는 언젠가는 주식 매수를 해야 하고, 대부분은 기간 내에 베스팅되긴 하지만, 결국 기업 입장에서 실제 비용임. 오히려 현재 Meta가 AI 경쟁에서 많이 뒤쳐져 인재 영입에 이런 방법을 쓰는 게 더 민망함. 데이터, 자금, 리소스가 그렇게 많으면서도 성과가 부족하다는 점이 아쉬움
     * 이런 계약서는 어떻게 생겼는지 궁금함. Meta에서 죽어라 일해야 하는 조건인지, Zuck이 실제로 비전과 리더십, 경영 역량이 있어 이 인재들이 최대 성과를 올릴 수 있도록 만드는지, 그리고 그 성공의 모습이 뭔지 의문임. 지금까지 Zuck의 외부 발표는 실망스럽지만 내부적으로는 다른 비전을 갖고 있을 수도 있다 생각함
          + 한편으론 이 구조가 뭔가 사기 비슷하게 똑똑한 이들이 부자에게서 최대한 돈을 빼내는 느낌도 듦. 만약 AGI 노하우를 가진 인재들이 모두 모이면, 진짜 원하는 건 무엇일까 상상해보게 됨
          + 내가 가장 궁금한 점은, 누군가에게 $2억 5천만을 줬을 때 어떻게 동기부여를 할 수 있을지임. 특히 젊은 사람이면 더더욱. 더는 일할 필요가 없고, 자녀도 평생 번듯하게 살 수 있음. 결국 자기가 가장 관심 있는 연구만 하게 될 텐데, 굳이 리더를 위해 신이 된 AI까지 만들어줄 동인은 떨어짐
          + “Zuck이 진짜 비전, 리더십, 경영 역량 있느냐""는 건, 메타버스 사례에서 분명히 드러났음. 그 답은 ‘아니오’이고, 단순한 두 글자로는 표현이 부족할 정도임
          + 만약 모두 필요한 지식이 이미 유출되어 가치가 떨어져버린 상황이라면, 또 달라질 것임
     * 누군가는 $2억 5천만의 동기부여를 잘 이해하지 못함. 나였다면 1년만 일하고 평생 하고 싶은 일 하면서 여유롭게 살 것임. 심지어 그 일이 내 열정이라 해도, 내 기여의 소유권 자체가 나에게 없으면 굳이 하고 싶지 않을 듯함
          + 이런 계약은 ‘연봉’이 아님. 기본 연봉은 ‘은퇴할 만큼’ 크지 않고, 대부분 4~5년 베스팅으로 주어지는 주식임. 퍼블릭 회사에서는 몇 년간 업무를 유지해야 실제 현금화 가능하고, 비상장사는 더 불확실함
          + 네 태도론 사회 변혁적 부는 못 모음
          + 좋은 지적임. 다만 어떤 열정은 반드시 타인과 자본이 필요함. AI 연구자라면 컴퓨트, 데이터, 엔지니어가 동반되어야 비로소 열정도 실현 가능함
          + 일의 스트레스와 즐거움에 따라 달라지겠지만, $2억 5천만이면 평생 원하는 걸 하며 살아갈 인생을 만들 수 있음. 진정한 변혁적 ‘가문 부’가 될 수 있음
     * 아주 소수 AI 연구자만 이 만큼 가치가 있을 것이며, 대부분은 프로젝트에서 작은 진전만 거둘 것임. 벤처캐피털처럼 대부분 투자(인재 영입)가 실패해도 소수 성공 케이스가 전체 비용을 만회해 주는 구조임
     * 이런 초거대 패키지가 제공되는 건 축하할 만한 일이지만, 그만큼 이런 오퍼를 하는 쪽이 결국 무엇을 두려워하는지 묻게 됨
          + 예를 들어 Meta는 나중에 자기들과 경쟁할 챗봇‘형 페이스북’이 나오지 않게 돈을 쓰는 듯하지만, 지금의 소셜 미디어 환경에선 그럴 만한 미래가 잘 상상되지 않음
          + Meta의 소셜 미디어와 직접 상관은 없음. Zuckerberg나 다른 테크 리더들은 AGI/ASI가 눈 앞에 있다고 보고 있고, 만약 그 경쟁에서 이기면 ‘신’이 될 수 있다고 믿는 것임. 그의 우상이 율리우스 카이사르라면, 뭘 아껴야 하겠냐는 마음임
          + 예전에 “IBM을 샀다고 회사에서 잘리진 않는다”는 말이 있었는데, 이제는 AI 연구자에 적용되는 것임
          + 그냥 최선을 원하고, ‘B급 인재’가 팀을 망칠 수 있다는 두려움이 동인임. 굳이 챗봇이 무서워서라기보단, 팀 퀄리티를 최상으로 유지해야 한다는 경영 본능임
          + 불편한 진실인데, 엘리트는 굉장히 소수임. 기업 입장에선 정말 최고만 원함. 이건 스포츠와 똑같음. 마이클 조던, 메시, 타이거 우즈, 카를센은 하나뿐임. 그만큼 가치이니 높은 보상도 당연함
               o Meta는 이미 오래전 페이스북에서 방향을 틀었음. 나도 페이스북에 몇 년간 로그인도 안 했고, 주변 누구도 그쪽에 글 안 씀. 지금은 과거의 유물이 되어버렸음
"
"https://news.hada.io/topic?id=22336","토큰 가격이 점점 비싸지고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           토큰 가격이 점점 비싸지고 있음

     * LLM 토큰 비용이 매년 10배씩 감소한다는 기대와 달리, AI 구독 서비스는 수익성이 점점 더 악화되는 현상이 발생
     * 최신 LLM 모델에 대한 수요는 항상 최상위(SOTA, State-of-the-art) 모델로 집중되어, “구형” 모델의 가격 하락은 실질적 원가 절감으로 이어지지 않음
     * 모델 성능이 오를수록 사용되는 토큰량이 기하급수적으로 증가하여, 단가 하락을 상쇄하고 오히려 전체 비용이 치솟는 구조
     * 무제한 구독 요금제 실험(예: Claude Code $200/월)도 헤비 유저의 토큰 폭주로 인해 지속 불가능
     * 사용량 기반 과금 외에는 장기적으로 지속 가능한 모델이 없으나, 스타트업 경쟁 구도와 소비자 저항으로 인해 현실적 도입이 어려움
     * 지속 가능한 수익 모델로 전환하지 않으면 대부분 스타트업이 결국 파산 위험에 직면함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 구독 비즈니스, 토큰 단가 하락에도 왜 적자만 늘어날까

  LLM 가격 하락의 허상

     * 창업자들은 ""토큰 단가가 10배씩 내려가니, 잠시만 버티면 고마진 구조로 전환될 것""이라는 VC 플레이북을 믿고 초기에 원가 수준 또는 적자로 구독 상품을 운영함
     * 실제로 GPT-3.5처럼 구형 모델의 토큰 단가는 10배 이상 하락했지만, 사용자와 시장의 수요는 항상 가장 최신·최고 성능(SOTA) 모델에 쏠림
     * 실제로는 18개월이 지난 시점에서 마진이 개선되지 않고 오히려 악화되는 상황
     * 구형 모델 가격 인하가 체감되는 것은 ""어제 신문""처럼 이미 시장의 관심 밖인 경우뿐임

  최신 모델의 가격과 수요 구조

     * GPT-4, Claude 3 Opus 등 최신 모델은 항상 비슷한 고가로 출시되고, 구형 모델이 아무리 싸져도 실제 사용량은 미미함
     * 사용자는 ""최고 성능""만을 원하며, ""값싼 구형 모델""은 자동차 시장에서 오래된 중고차에 불과
     * AI 사용 시 실제로 원하는 건 최고의 결과이므로, 비용을 아끼기 위해 구형 모델을 자발적으로 쓰는 사례는 드묾
     * 결국 시장에서 경쟁력을 가지려면 항상 가장 비싼 최신형 모델을 제공해야 하고, 이로 인해 원가가 계속 유지됨
          + 마치 90년대 중고차 가격이 내려가도, 소비자가 여전히 새 차를 구매하는 상황과 같음

  토큰 사용량의 폭발적 증가

     * 모델 성능이 올라가면서, 한 번의 작업이 소비하는 토큰량이 기하급수적으로 증가하는 현상이 발생
     * 과거엔 1,000토큰이면 끝나던 작업이, 이제는 100,000토큰을 소모할 수 있음
     * 예전에는 한 문장 질의에 한 문장 응답으로 처리됐지만, 요즘은 복잡한 리서치나 루프, 오케스트레이션으로 10~20분씩 연속 동작하면서 방대한 토큰 사용이 이루어짐
     * AI로 더 깊은 연구/분석을 시키다 보니 ""한 번 실행에 20분, 하루 24시간 연속 실행"" 등으로 개별 유저당 일평균 사용량이 급증
          + 예를 들어, 매일 $1 상당의 'deep research'를 1회만 사용해도 $20 구독 요금으로는 수지가 맞지 않음
     * 단가 하락분이 전체 토큰 소모량 증가로 상쇄되어, $20/월 요금제로는 하루 한 번의 $1짜리 작업도 감당이 불가한 상황 도래

  무제한 요금제의 실패

     * Anthropic의 Claude Code 등은 $200/월 무제한 요금제, 토큰 자동 최적화, 사용자 PC 활용 등 다양한 비용 절감책을 도입해 봄
     * 하지만 일부 파워유저가 한 달 100억 토큰(“워 앤드 피스” 12,500권 분량)에 육박, 이는 사용자들이 자동화, 반복 작업, 루프 등을 활용해 폭발적 토큰 사용을 이끌었기 때문
          + ""AI 사용량이 인간의 시간과 분리되어 API가 24시간 돌며 토큰 폭주""로 이어짐
     * 엔지니어링 혁신에도 불구하고 결국 요금제를 롤백함
     * 결론: 이제 무제한 구독 모델은 불가능하며, 수식 자체가 성립하지 않음

  산업 전체가 직면한 딜레마

     * 구독제 방식을 계속 고집하면 수익성 악화 및 붕괴 위험이 커짐
     * AI 기업들은 모두 사용량 기반 과금(usage-based pricing)만이 해답임을 알지만, 구독 기반 경쟁자 등장 시 사용자 이탈 위험이 큼
     * “죄수의 딜레마” 구조로 인해 모두가 파워유저 보조금 경쟁에 내몰림
     * Cursor, Replit 등도 “성장 우선, 수익성은 미래의 문제”로 접근하지만, 결국 언젠가 수익성 문제로 구조조정 불가피

  현실적 해결책 3가지

     * 1. 사용량 기반 과금
          + 초반부터 정직한 경제 모델을 도입하면 원가를 넘지 않는 수익 구조 설계 가능. 장기적으로는 유일하게 지속 가능한 모델
          + 단, 소비자는 측정 요금(미터제)을 극도로 기피, 대중적 성공 어렵다는 한계 존재
     * 2. 높은 전환 비용 기반의 기업 시장 공략
          + 높은 전환 비용을 가진 엔터프라이즈 고객(예: 대기업, 금융기관) 대한 B2B 영업을 통해 한번 시장에 진입하면, 해지가 거의 불가능하고 마진이 높음
          + 시스템 오브 레코드(SOR, CRM/ERP/EHR 등) 분야가 대표적 성공 사례 (예: Goldman Sachs 4만 엔지니어용 도입 등)
     * 3. 수직 통합 통한 부가가치 창출(Vertical Integration)
          + Replit처럼 LLM inference 자체는 손해를 보는 ‘미끼 상품’으로 제공하고, 그 위에 탑재한 호스팅, 데이터베이스, 배포, 모니터링 등 여러 서비스로 수익을 창출
          + AI 사용량을 늘려 인프라 시장으로 이어지는 구조 구축
     * 앞으로도 토큰 단가 하락은 이어지겠지만, 사용자 기대와 사용량도 기하급수적으로 증가할 전망
     * 구독제-성장전략만 고수하는 기업들은 결국 '고비용의 장례식'을 치르게 될 위험성 높음

  요약

     * “내년엔 토큰이 10배 더 싸질 것”이라는 낙관론만으로는 비즈니스가 유지되지 않음
          + 사용자는 항상 더 높은 기대치와 사용량을 요구
     * 모델 발전=사용량 폭증=원가 증가라는 공식이 성립하고 있으며, 결국 지속 가능한 AI 비즈니스는 사용량 과금, 대형 기업 계약, 수직 통합을 통한 새로운 구조로 전환해야 함
          + 사업 지속을 원한다면 '네오클라우드' 전략 등 새로운 구조적 접근 필요

   캐싱이 어려운 점 + MCP를 활용한 자동화로 무제한 사용은 진짜 말그대로 무제한 사용까지 향할 수 있습니다. ..무제한 데이터 요금제가 없는 통신사처럼 일 ~300회, 일 ~ 2000회 등등.. 옛날 문자메시지와 같은 요금제로 향할것 같기도 하네요.

   인터넷처럼 양 자체는 무제한이지만(종량제가 걸리는 경우도 있긴 하지만) 속도에 제한을 거는 방식으로 가면 좋을 것 같습니다. 구현이야 뭐 지금도 배치처리 방식이 있는 것처럼 연산자원과 사용자에게 도달하는 자원은 분리가 가능하죠. 결국 공급자 입장에서도 예측가능성을 확보하고, 사용자도 합리적인 금액과 속도를 보장받을 수 있다면 윈윈 아니겠습니까? 일부 과다 사용자의 경우, 별도 계약을 통해 전용 자원을 할당하는 식으로 가야겠죠.

        Hacker News 의견

     * 기사에서 인용된 내용을 보면, 소비자들은 종량제 과금(미터 요금제)을 싫어하고, 놀라운 금액의 청구서보다 차라리 무제한 요금제에 과지불하는 쪽을 선호한다고 말함 하지만 실상은 복합적인 부분이 있음 Amazon에서 비용을 예측했다고 생각하는 순간 갑자기 청구서가 크게 나오는 경우가 많음 그 이유는 '한 달에 X달러가 넘으면 자동으로 꺼지게 해 달라'고 설정할 방법이 없기 때문임 이런 식의 '서프라이즈 네트 30' 구조는 늘 예상 가능한 비용처럼 느끼지만 결국엔 예기치 않은 추가 비용이 돌아옴 하지만 종량제가 유저가 사용량을 명확히 알 수 있고, 예산 초과를 막는 최대한도를 지정할 수 있다면 오히려 좋은 방식이 될 수도 있음 AI 기업 입장에서는 '사용한 토큰 / 전체 토큰' 바 차트, 응답당 토큰 사용량, 초과 전 예상 응답 횟수 제공 등, 사용자가
       예산을 관리할 수 있게 하면 됨 갑작스러운 청구는 절대 하지 않는 것이 중요함 그러나 기업들은 이런 토큰과 달러 정보를 숨기는 걸 선호함, 마치 도박 사이트들이 '코퍼레이트 벅스'를 USD로 바로 연결시키지 않는 것과 유사함
          + 인프라로서의 B2B 서비스(AWS 등)에는 종량제가 적합하다고 생각함 기업이 성장할수록 인프라 사용량과 요금이 비례 증가하니 예측이 가능함, 인프라는 일단 세팅하면 신경 쓸 일이 거의 없음 하지만 AI 같은 업무/도구로 쓰는 상황에서는 종량제 과금이 큰 걸림돌임 이런 상황에서 종량제는 아예 제품 사용을 억제하는 원인이 되고, 매번 쓸 때마다 비용 대비 효과를 분석해야하는 큰 피로도가 생김 만약 업무에서 쓴다면, 관리자 결재를 계속 받아야 할 수도 있음 생산성 향상을 노리는 툴이 이런 장벽을 만들어선 안됨 거의 대부분의 사람은 250번씩 “이 동작이 3달러 가치가 있을까?” 고민하지 않음 종량제라면 그냥 안 쓰게 됨
          + 기업들이 토큰을 달러로 전환하는 정보를 숨기려는 게 불만임 GitHub의 Copilot 에이전트 트라이얼을 해보고 있는데, 요금이 정말 불투명함 “프리미엄 요청”이란 용어만 자꾸 나오고, 내 대시보드에서는 실시간 사용량 및 한도를 확인할 수 없음 UI상에 프리미엄 요청 이야기를 클릭하면 문서로 연결되지만, 실제 한도나 요금 대시보드를 명확히 안내하지 않음
          + Amazon(AWS)에서는 더 문제가 심각함 AWS의 “더 저렴하다”는 유혹과 달리, 실제로는 그 대안보다 저렴해야만 전환이 의미 있음 그렇지만 많은 회사는 개발자 시간을 들여 인프라를 바꾸지 않음 기회비용이 크고, 리스크(수익, 개발 시간, 경쟁 등)가 있으므로, 투자 효과가 아주 크지 않으면 개발 시간 낭비로 여겨지기 때문임 만약 인프라 구조가 대안보다 실제로 더 비싸게 돼버렸다면, 이미 개발자 시간을 썻으니 그 손해를 감수할 수밖에 없음 아직 토큰 기반 요금제에서는 이런 전환/기회비용 부담 요소가 크게 느껴지진 않음 쉽게 기존 방식으로 돌아갈 수 있기 때문임 하지만 앞으로는 이 구조가 바뀔 거라고 예상함
          + Amazon의 가격 구조는 매우 애매하고 복잡하게 느껴짐 예를 들어, 왜 데이터베이스 비용이 계속 오락가락하는지 알 방법이 없는 경우도 있음
          + 정의된 프로세스에 대해선 종량제가 정말 유용함 AWS가 마음에 드는 점은, 비용을 실제 비즈니스와 일치시킬 수 있기 때문임 예전엔 이게 힘들었고 내부 정치 이슈도 많았음 세일즈 담당자가 직접 임원에게 장비 필요성을 어필해서, 전혀 원하지도 않은 네트워크 장비까지 떠안게 되는 경우가 있었음 하지만 사용자 입장에선 이런 세밀한 비용 관리가 좋지 않음 왜냐하면 생산성과 직접 연관 없는 각종 지표로 사용자를 계속 평가하게 되기 때문임 90년대 인턴 시절에 장거리 전화 하나 승인받으려면 관료주의에 시달림 승인자가 20분 통화가 적절했는지 일일이 평가하고, 한도 초과하면 내가 비용 부담함 재미없는 경험임 사용자 대상 AI에는 고정 요금제가 정답임 내 생산성이 20% 증가해서 월 $200로 ChatGPT Pro를 쓰면 연 $16k 가치임 굉장히 저렴한 투자임
     * 기사의 주장들이 나에게는 논리적으로 다가오지 않음 “최신 모델이 나오면 99% 수요가 바로 넘어간다”라는 내용은 동의하기 어려움 오히려 Sonnet 4가 Opus 4보다 더 많이 사용되고 있음, 실제로 최고 성능 모델이 아닌 저렴하고 평범한 모델을 쓰는 유저가 많음 사용성, 속도, 친숙도 등 다양한 이유로 SOTA가 아닌 다양한 모델이 함께 쓰임 모델 랭킹 참고: https://openrouter.ai/rankings 그리고 Opus에서 Sonnet으로, 무거울 때 Haiku로 바꾸는 걸 마치 오토스케일처럼 설명하는데, 실제로 해당 동작이 모델 가중치에 내장되어 있지는 않을 거라 생각함 전반적으로 글에서 요금제 문제는 클라우드 호스팅 시절에도 겪었던 이슈를 재현하는 것으로 보임 - 많은 사용자는 월정액으로 성능이 떨어져도 편리하게 쓰고, 일부 API 사용자(헤비유저/기업)는 종량제로 사용함, 이 구조는
       이미 충분히 수익성이 보장됨 - 대부분의 AI 스타트업은 B2B임, B2C가 아님
          + “최고의 모델이 무엇인가”에 대한 논쟁이 활발해진 현 상황에 크게 공감함 가끔 Mistral을 메인 LLM으로 사용하는데, ChatGPT/Gemini/Claude와 비교해도 실사용면에서 큰 차이를 못 느낌 그리고 속도가 훨씬 빠름 이미 상업용 LLM 경쟁은 수익 대비 효과가 크지 않은 상태임 Deepseek 같은 사례가 비용이 낮고 품질까지 올라갈 수 있음을 보여줌 이제 곧 가격 경쟁이 본격화될 것으로 봄 이 때문에 Mixture of Experts 접근이나 특화 모델 경쟁이 부각되고 있는 것 같음 값은 낮추고 정밀도를 높이는 쪽으로 발전 중임
     * “클로드 코드가 원래 무제한 $200/mo를 제공하다가 롤백했다”라는 얘기는 사실과 다름 플랜 이름 자체가 20x 플랜이고, 5시간 세션 제한 및 월별 50세션 제한(강제는 아니지만) 등, 애초에 제한이 명확히 있었음 나 역시 사용하면서 부족하다고 느낀 적이 거의 없음, 오히려 아직도 한도가 높다고 생각함 그러므로 진실을 말해도 논거에 전혀 해가 되지 않을 정도임
          + 맞음, Max 플랜은 본래부터 무제한이라고 안내하지 않았음 이런 오해를 너무 많이 보고, 듣고 있음 반복적으로 떠오르다보니 이제는 다들 무제한이라 생각해버리는 현상임
     * 현실적으로 큰 문제는, 지금 우리가 구분 없는 모델 사용(모든 문제에 최고 사양 일반화 모델 투입)으로 모기를 대포로 때리는 상황임 모든 문제에 SOTA 모델이 필요하지 않음 앞으로 사용하는 서비스가 여러 모델 “번들화”로 나아가면서, 훨씬 더 효율적인 사용 그래프가 나올 것임
          + 아직 그 어떤 모델도 주요 작업에 완벽히 믿고 맡길 수준에는 못 미침 심지어 최고 성능의 모델들도 가끔은 이상하게 동작함, 내 뇌는 항상 일을 자체 처리해서 위임에 머리 쓸 필요가 없음 그러니 AI에 맡겨서 “확실한 이득”이 있어야만 실제로 맡김 나는 내가 잘하는 게 우선임, AI 회사는 최고 성능을 광고하지만, 사용자는 AI의 “최악의 순간”이 중요한 지표임. SOTA만이 항상 수요가 있는 이유임 AI는 ‘최악의 순간’ 평가를 받게 됨 – 아무리 잘해도 한 번의 실수가 치명타임, 실제로 사람이 최악의 실수로 해고 되는 것과 같음 완벽한 케이스(연구실 환경) 성능이 중요한 게 아님, 실사용에서 망가졌을 때가 더 중요함. 글에서 이 부분이 잘 드러남
          + 아직까지는 가장 어려운 작업들이 해결되지 않음, 낮은 정확도 답변을 받아들일만한 작업은 많지 않음 일부 텍스트 파이프라인 작업엔 괜찮을 수 있지만, 사용자 대상 거의 모든 용도는 높은 품질 요구함
          + 이 부분을 많은 이들이 간과함 7b, 32b GPU 모델들도 많은 작업에서 충분히 잘 동작함 그리고 구형 하드웨어에서도 돌아감 아직은 LLM 성능 전체가 오르는 하이프 단계라 시간이 지나면 대형 모델의 성능 향상이 정체되고 현실적인 선택들이 시작될 것임
          + 다양한 모델을 실험해보는 게 가치가 있음 최근 내가 만든 간단한 챗봇 시스템은 상황별로 5종의 모델을 다르게 씀 다양하게 모델을 교체하고 섞어 쓰는 게 비용, 사용자 경험, 품질에 엄청난 차이를 냄
          + 만약 Claude Opus가 Sonnet을 가이드해주는 옵션이 있으면 거의 모든 대화에 사용하겠음 수동으로 이렇게 하려면 번거롭고 흐름이 끊겨서 결국 Opus만 계속 쓰게 됨 병렬 처리 덕분에 입력 비용이 낮으니, 프롬프트가 커져도 큰 부담이 아니라고 생각함
     * 어떤 AI 회사가 태스크를 간단한 작업은 더 '둔한' 모델에게 위임할 수 있는 시스템을 만들었으면 좋겠음 복잡한 작업이 Opus 수준 모델을 요구하지만, 그 안에는 사실 3.5 Sonnet으로도 충분한 일들이 수두룩하게 포함되어 있음 Opus는 단순한 작업과 어려운 부분을 구분해서, 쉬운 것은 3.5 Sonnet 여러 개로 분산 맡기면 될 것임 너무 당연한 아이디어 같아서 이미 다들 만들고 있을 거라 생각함
          + Claude code는 실제로 Sonnet과 Haiku 두 모델을 자동으로 활용함 세션 종료 시 토큰, 비용 등 각종 통계를 안내해줌 아마 세션 중에도 이런 정보를 확인할 방법이 있을 것으로 기대함
          + 예를 들어 프롬프트에서 각 서브태스크별로 1~10 등급의 “권장 모델 레벨”을 뱉게 하면 어떨까 싶음
     * 지난 1~2년간 나는 API를 직접 결제해서, 오픈소스 프론트엔드(LibreChat 등)로 다양한 모델에 접속해 사용해왔음 가끔씩만 사용하기엔 아주 잘 맞아서, 몇 달에 한 번 $10 정도만 충전하면 충분했음 내가 쓰는 토큰 양은 대부분의 패키지 요금제보다 훨씬 적다보니, 이 방식이 훨씬 저렴하다고 판단함 그런데 Claude Code 등 다양한 툴을 써보기 시작하니 토큰이 눈에 띄게 빠르게 소진됨 어제는 15분 만에 $5치 토큰을 써버림 Code 툴은 LLM에게 단순 질문할 때와 방식이 매우 다르다는 것은 알지만, 이렇게까지 차이가 클 줄은 몰랐음 많은 토큰 사용이 실제로는 눈에 잘 안 띄어서(점점 커지는 컨텍스트나 도구 오케스트레이션에 감춰져 있음) 더 놀람
          + Claude Code가 평소보다 훨씬 더 넓은 컨텍스트, 반복적 처리를 많이 써서 이런 현상이 발생함
          + Deepseek API $20로 1년 가까이 충분히 썼음(중국 회사라는 건 상관없음) 속도는 느리지만 독립 호스팅 Deepseek 모델 중에서는 품질이 오히려 더 낫다고 느낌(내 경험상) 에이전트 같은 건 사용하지 않음
     * “99% 수요가 항상 최첨단 모델에 몰려 있다”는 주장에 이의가 있음 진짜 경계선(프론티어)은 ‘능력’ 자체뿐 아니라 ‘가격 대비 능력’에 있음 최고 사양 모델이 99% 점유율을 차지하지 않음, 오히려 반대임 OpenRouter 통계를 보면 Claude Opus 4의 점유율은 1% 수준임, 가장 인기 높은 건 Sonnet 4이고 가입자 중 18%가 사용함 그 외에는 더 저렴한 Gemini Flash 2.0, 2.5도 많이 씀. Sonnet 4보다도 가격이 저렴함
          + 맞는 말임. 기사 전체 요지는 동의하지만, Opus가 Sonnet보다 더 많이 쓰인다는 건 잘못된 주장임 그래프에 “Claude 3.5 Opus”라는 존재하지 않는 모델마저 표기됨 3.5 Sonnet이 출시된 이후로 3 Opus는 거의 잊혀졌고, 최근에야 Opus 4 같은 고가 모델이 다시 나왔지만, 여전히 Sonnet 4에 비하면 API 사용자 비중이 크지 않음
     * 샌프란시스코에서는 왜 대문자와 구두점을 안 쓰는지 궁금함 그리고 왜 실리콘밸리 사람들이 가짜 지수 성장에 집착하는지 모르겠음 사실 AI의 진보가 정말 지수적으로 이뤄지고 있다기보다, 단지 몇 년 전에 비해 투입 자원이 엄청 많아졌기 때문이란 게 더 명확하다고 봄
          + 혹시 이런 독특한 문체가, LLM이 쓴 글이 아니라는 걸 드러내기 위한 것인지 궁금함
          + 언어가 자연스럽게 변하는 걸 감당 못 하겠음?/농담 어쩌면 예전 방식으로 살아가야 할 듯함
          + 샌프란시스코 Tenderloin이나 미션 스트릿에 가면 실제로 대문자와 구두점 안 써도 총 맞을 수 있나?(농담)
     * 글에서 “땅따먹기” 과정의 ‘음악 의자 게임’을 놓치고 있음 Uber 사례처럼, 벤처 자본을 써서 시장점유율을 선점하고, 수 년간 적자를 감수해도 일단 고객 인식에 자리를 잡으면, 이후에는 더 싸고 새로운 경쟁자가 나와도 쉽게 흔들리지 않는 구조가 됨 비즈니스가 안정적으로 자리 잡히고, 상장 이후에도 건실한(물론 아주 뛰어나지는 않아도) 주가를 유지함
     * 글에서는 아무도 종량제 가격을 지불하지 않는 것처럼 그리지만, 실제로는 API 고객(즉 거의 모든 기업 고객)은 이미 모두 종량제 과금을 지불하고 있음

   ""샌프란시스코에서는 왜 대문자와 구두점을 안 쓰는지 궁금함""

   본문 들어가보니 진짜 그러네요. 신기한건 어느 문장은 마침표를 쓰고, 어느 문장은 안 쓰고 섞여 있는데, 무슨 이유일까요? 혹시 아시는 분 계실까요? 궁금한데 🤔
"
"https://news.hada.io/topic?id=22280","나는 당신이 '바이브 코딩'할 때를 알아챌 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      나는 당신이 '바이브 코딩'할 때를 알아챌 수 있음

     * 최근 팀 내에서 LLM이 생성한 코드임을 쉽게 알아차릴 수 있음
     * 이러한 코드는 프로젝트 컨벤션을 지키지 않으면서도 명확하고 테스트도 잘 되어 있음
     * 여러가지 기존 패턴이나 라이브러리를 무시하고 직접 새로운 구현을 함
     * 소프트웨어 개발에서 속도만을 추구하는 경향에 대한 우려가 커짐
     * 결국 중요한 건 품질과 일관성 그리고 유지보수 가능성임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

바이브 코딩의 흔적

     * 최근 팀원이 작성한 코드 중 일부가 명확하고 기능적으로 완벽해 보이지만, 프로젝트 고유의 컨벤션을 지키지 않아 LLM이 생성한 것임을 바로 알 수 있음
     * 예를 들어, 이미 프로젝트에 있는 데이터 페칭 라이브러리가 있음에도 불구하고 모든 예외 케이스를 다루는 HTTP 요청 구현을 직접 작성함
     * 기존에 있는 모듈의 유틸리티 함수들을 반복해서 새로 만들거나, 모듈 단위 설정 변경 메커니즘이 있는데도 불구하고 전역 설정을 바꿈
     * 함수형으로 코드를 작성하는 문화가 정착되었음에도 클래스 기반 코드를 새로 작성함
     * 이런 코드는 사람이 몇 년 전에는 절대 작성하지 않았을 코드 스타일임

유지보수와 소프트웨어 원칙의 중요성

     * 소프트웨어 개발에서는 오랜 시간 유지보수 가능한 패턴과 표준을 정립하는데 노력을 투자해 왔음
     * 실제로 동작만 하는 코드는 누구나 만들 수 있지만, 장기적인 관리와 수정이 쉬운 코드가 진정한 도전임
     * 기능 구현 자체가 아니라, 시간이 지나도 유지할 수 있는 코드베이스가 관건임
     * “바이브 코딩”은 이러한 철학과 기준을 무너뜨릴 수 있음

속도를 최고의 선으로 여기는가?

     * 커피숍에서 신규 바리스타가 서두르다가 커피를 엎지르는 광경에 비유해, 속도에 대한 집착이 올바른 결과를 가져오지 않음을 강조함
     * 요즘 개발팀도 마찬가지로 너무 빠르게 새로운 소프트웨어를 만들려고 하다가 품질 저하가 발생함
     * 사람들이 원래 원하는 것은 약간 더 기다리더라도 제대로 된 결과물임
     * 원래 속도만을 챙기는 건 비개발 직종의 문제라고 생각했으나, 동료 개발자들도 원칙을 버리고 속도만을 추구하는 현실에 실망감을 느낌

진정으로 원하는 것

     * 코드를 IDE에 어떻게 집어넣었는지는 상관하지 않음
     * 중요한 것은 개발자가 품질에 신경쓰는 태도임
     * LLM이 대단한 기술적 혁신임은 인정하지만, 여전히 실제 소프트웨어를 만드는 책임은 개발자에게 있다는 점을 강조함
     * “더 나은 프롬프트 작성”, “올바른 라이브러리 지정”, “예제 제공”, “작은 파일 단위로 작업” 등 구체적인 기존 원칙을 알고 활용할 것을 권유함
     * 코드 품질과 유지보수성을 모델의 ‘가중치’에만 맡기지 말 것을 당부함

        Hacker News 의견

     * 아무도 이미 프로젝트 내에 있는 데이터 페칭 라이브러리가 모든 예외 케이스를 커버하고 있다면 HTTP 페칭 구현을 새로 만들지 않음, 이미 존재하는 유틸리티 함수 모듈이 있는데 굳이 새로 구현하지 않음, 전역 설정을 개별 모듈에서 할 수 있는데도 변경하지 않음, 그리고 함수형 방식을 주로 쓰는데 클래스를 새로 만들지 않음 이런 팀에서 일해보고 싶음, 하지만 실제로는 많은 개발자들이 이런 것들을 자주 반복해서 하고 있음
          + 솔직히 말해서, 대형 프로젝트에서 문서화가 부실할 경우 위와 같은 일들이 정말 쉽게 일어남, 내가 일하는 학술 연구 프로젝트의 코드 문서는 코드 자체가 셀프 다큐멘테이션 수준이라고 하고, CMake 설정이나 빌드, 벤치마크 방법 정도만 짧게 언급되어 있음, 내부 규칙이나 관례 같은 건 직접 겪으면서 알아내야 함, 새로운 사람이 오면 이미 구현된 기능을 다시 만들거나 전역 설정을 바꿔버리는 일이 잦음, 결국 코드베이스를 인덱싱하고 LLM에게 직접 묻는 게 최선임(프로젝트 주요 인력들이 떠나거나 한참 뒤에나 답장이 옴)
          + 저자의 요점을 놓치고 있다고 생각함, 만약 속도가 최고의 덕목이라면 그런 일들은 계속 반복됨, 속도가 절대적인 가치라면 생산량이 기하급수적으로 늘어나야 기술 부채를 상쇄할 수 있음, 속도 외에 다른 요소도 중요하다면 현명하게 부채를 관리하고 갚아야 함, 하지만 요즘엔 그냥 부채를 크게 안고 어떻게든 되겠지 하는 느낌임, 그리고 많은 사람들이 실제로 부채 관리에 서툼
          + 기회만 있으면 바퀴를 매번 재발명하고, 예상되는 관례를 무시하거나 혼합된 패턴을 쓰는 경우가 많음, 저자는 이런 걸 'vibe coding'이라고 부르겠지만, 사실 LLM만의 문제가 아니라 누구나 급하게 결과만 내려고 하거나 경험이 부족할 때 다 일어나는 현상임, '팀의 어느 누구도 그렇게 쓰지 않을 코드'라는 표현을 보면 누군가 특정인을 겨냥한 불만일 가능성이 있음, 이런 시각을 다른 곳에 적용하는 건 신중해야 함
          + ORM 라이브러리를 추가로 넣는 개발자도 봄, 첫번째 ORM이 충분한데 '요즘 핫하다'는 이유로 두번째를 넣는 경우임, 개발자든 LLM이든 각자의 편향을 가지고 있음, 프로젝트 내 규칙이나 패턴을 숙지하고 그 틀 안에서 작업하는 건 굉장히 중요함, 문맥 고려 없이 자기 방식대로 만드는 건 매우 위험함, 사람의 경우 코드 리뷰 문화와 코드 읽기를 장려하는 것으로 해결할 수 있지만, LLM의 경우 모든 패턴과 규칙을 명확히 가이드해야 함, 그렇지 않으면 프로젝트와 어긋나는 코드를 만들 위험이 높음, 중요한 것은 가치관과 명확한 기준을 명시적으로 설정하는 것임
          + 내가 그런 좋은 팀에서 일해본 경험이 있음, 작은 규모(2-4명)로 운영되는 중요도 높은 프로젝트가 많았음, 이런 환경에선 품질과 속도를 균형 있게 맞추는 개발 문화와 합의를 만드는 게 쉬움, 그런 팀에선 누구든 사람이든 LLM이든 위와 같은 코드엔 절대 PR을 승인하지 않음
     * 개인적으로 LLM을 아주 주니어 개발자라고 생각함, 일은 잘 하려고 하고 지시를 따르지만, 코드베이스와 패턴 이해도가 부족함, 모든 과정을 직접 안내해야 하고 잠재적 오류까지 설명해줘야 하며, 구체적이고 작은 단위의 작업을 부여하고 코드를 꼼꼼히 검토해야 함, 나처럼 데이터 모델을 먼저 머리로 그려보고 그 다음 코드에 들어감, 구체적 설명이 중요함, 한 가지 불문율은 항상 파일 상단에 블록 주석을 넣어서 파일의 내용을 설명하게 함, 이건 세션 재시작 시 두 번째 프롬프트 역할을 함, 이 방식이 '마법같다'는 느낌 없이 좋게 돌아가지만, 중간에 30% 정도는 코드 정리, 리네이밍, 리팩터링을 해야 보기 좋은 수준이 됨, 그래도 LLM이 있어서 완전히 손으로 짜는 것보다 훨씬 빠르게 일할 수 있음
          + ""주니어 개발자"" 또는 ""코파일럿""이라는 표현이 LLM의 장점과 단점을 모두 제대로 반영하지 못할 때가 있음, 일반 사람과 달리 쉽게 망각하고 아주 기초적인 실수를 하기도 하지만, 나보다 더 잘하는 부분(예를 들면 배열 관련 오프바이원 이슈 같은 것)도 있음, 그리고 인터넷상의 거의 모든 것을 백과사전적으로 알고 있음, 직접 써 본 결과 LLM은 사냥개와 비슷하다는 생각이 듦, 주인이 사냥을 주도하고 직접 마무리해야 함
          + LLM과 주니어 개발자의 차이점은 학습 능력에 있음, 주니어는 점점 배우며 성장할 수 있지만, LLM은 그렇지 않음, 프롬프트에 지시를 많이 넣을수록 더 많이 까먹고 일반적인 답변으로 돌아가버릴 확률이 높음, 프롬프트가 새로 시작될 때마다 처음부터 다시 안내해야 함
          + LLM은 인터넷에서 코드를 검색해서 복붙하는 것과 큰 차이가 없다고 느낌, 결국 개발자가 코드를 직접 점검해서 잘 동작하는지 확인해야 함, 최근에는 눈 건강 때문에 20분 단위로 작업하고 쉬어야 해서 효율이 더 중요해졌음, LLM은 인간보다 코드 생성 속도가 월등히 빨라서 기본적인 영역만 처리하게 해도 큰 이점임, 현재 Unity C#과 LINQ로 SIMD용 구조체를 생성하고 있는데, 단순히 LLM에게 원하는 조건을 말하면 직접 복붙하는 것보다 훨씬 빠르게 원하는 코드나 문자열을 얻을 수 있음, AI를 HUD처럼 쓰는 개념이 실감남, 전체 프로그램을 만드는 AI보다 작은 단위의 강력한 개발 보조 도구가 필요함
          + 나한테 LLM은 StackOverflow보다 훨씬 괜찮은 대체재임, 궁금한 걸 바로 물으면 정확하게 대답해줌, 대답을 참고해서 내 코드로 재작성하거나 함수만 생성하기도 함, 복사하기 전에 반드시 코드를 완전히 이해하려고 함, 때때로 40만 라인의 PR을 잘 모르는 언어로 오픈소스 프로젝트에 올리는 게 솔직하고 품질 지향적으로 일하는 것보다 커리어에 더 이익이 되는 게 아닌가 고민한 적도 있음, 실제로는 연차(Years of Experience)가 스킬보다 더 중요하게 평가되는 현실 때문임
          + LLM에게 성공적으로 작업시키려면 생각이 아니라 실제 코딩 단계로만 한정하는 게 좋았음, 태스크를 세분화하고 구체적인 스펙, 수정 파일, 참고 예시 위치 등까지 최대한 상세히 전달해야 적용률이 높음, 너무 세세할 필요는 없지만, 단서가 많을수록 성공 확률이 높음, 만들어낸 코드도 git add -p로 한 덩어리씩 직접 확인함, 준비와 검토에 시간이 들지만, 혼자서 다 짜거나 엉성한 코드를 그대로 두는 것보단 확실히 시간과 에너지를 아낄 수 있음
     * Vibe coding의 가장 큰 위험은 실력 좋은 개발자가 조금 빨라지는데 그치지만, 실력 부족한 개발자는 훨씬 더 빠르게 많은 나쁜 코드를 생성하는 현상임, 문제는 그런 개발자들이 vibe coding을 통해 실력이 늘 수 있는지 아니면 그냥 거기서 정체되는가임
          + 경험상 mediocore(평이한) 개발자도 아주 빠르게 나쁜 개발자가 될 수 있음, 이유는 잘못된 자신감과 코드 생산량 급증임, AI가 만든 코드는 전체적인 아키텍처나 정보 흐름, 단일 책임 원칙에 대한 고려가 거의 없음, 안전한 코드를 만들기 위해 예외 대신 placeholder를 반환하는 식으로 구성됨, 호출 코드는 결과값이 placeholder인지 아닌지 매번 체크해야 함, 애초에 인풋 파라미터가 좋지 않으면 AI가 알아서 고치려고 해서 gather_parameters → call → process_results 구조를 무시함, 그리고 테스트까지 가면 문제는 훨씬 더 커짐
          + 이제 많은 개발자들이 net-negative programmer(존재 자체가 프로젝트 품질을 떨어뜨리는 개발자)라는 개념을 재발견하게 될 것임
     * 내가 볼 때, 여기서 가장 부족한 자원은 애정(caring)임, vibe coding 자체가 애정 결여의 원인이 아니라, AI는 그냥 도구임, 저자가 말한 모든 이슈는 인간 주니어 개발자와 동일하게 적용되고 좀 더 잘 가이드하거나 소통하면 개선할 수 있음, AI 때문에 품질에 대한 관심이 감소한다고 보진 않음(원래 관심 없는 사람은 예전부터 그랬음), 흔히 반박으로 '주니어 양성 기회를 놓친다'고 하는데, 여력이 없어서 AI를 임시방편으로 활용하는 경우도 많음(채용이 잘 안 되는 내 스타트업도 마찬가지임), AI 도구로 인해 소프트웨어 품질 기준이 달라질 수 있는데, 이 부분은 앞으로 더 변화할 여지가 많다고 생각함
     * LLM이 현재 상태만이 아니라 과거 커밋 내역까지 문맥으로 활용해야 함, 많은 코드베이스가 패턴 A에서 B로 점진적으로 마이그레이션하고 다양한 패턴이 공존함, 마이그레이션이 한 번에 이루어질 수 없어서 보통 오래된 것과 최신 것이 계속 섞여 있음, HTTP 예시처럼 LLM이 패턴을 인식해도 어떤 걸 따라야 할지는 운에 달림
          + 20년 넘게 합병과 이름 변경, 인수 등을 겪은 대형 코드베이스에서 일한 적 있음, 아주 오래된 API 호출 예시가 남아 있고 실제로는 더 최신 코드가 따로 존재하지만, 특정 고객을 위해 남겨둔 게 많음, 문서가 전혀 없는 유사 API도 많아서 어떤 걸 써야 내가 원하는 데이터를 받을지 일일이 찾아야 함
          + CLAUDE.md 같은 파일로 ""이 패턴은 따라라, 저건 피하라"" 명확히 안내하는 것도 한 방법임
          + 그보다는 각 부분별 작업 방식, 예시를 아주 구체적으로 알려주는 게 더 효과적임
          + 문제는 이런 문맥 인식력 자체가 vibe coding을 하는 사람에게는 많이 부족하단 점임, 많은 사람들이 LLM 이전에 코딩 경험이 부족함
          + 커밋 메시지가 제대로 작성됐을 때만 이런 게 가능함, 실제로는 ""이 파일 수정"", ""버그 수정"" 등의 메시지가 대부분임
     * LLM을 활용할 때 린터, 포매터, 엄격한 타입 체크 같은 자동화 툴이 매우 큰 도움을 줌, 특히 코드 스타일이나 암묵적인 규칙을 모르는 사람(또는 LLM 에이전트)에게서 코드 기여가 들어올 때 자동화된 방식으로 코드를 검사하고, 가능한 건 바로 고칠 수 있음, 테스트도 마찬가지임, 자동화된 검증 체계는 인간이든 에이전트든 품질 유지에 매우 유용함
          + 그런 도구들이 실제로는 기사에서 언급한 vibe coding 사례 중 대부분을 막지는 못한다고 생각함
          + 이런 도구들은 문제를 가린 채 표면만 깔끔하게 만들어주는 경우도 있음
     * 모든 주요 AI assistant들은 이미 이런 문제를 완화할 방법을 기본적으로 탑재하고 있음, Claude Code의 /init, Cursor의 /Generate Cursor Rules 등, 단순한 context engineering보다 더 자동화된 방식으로 조직 전체에 적용할 수 있음, 결국 이런 도구들이 개발 커뮤니티를 어떻게 나누는지도 흥미로운 이슈임
          + 실제로는 CLAUDE.md에 아무리 명확하게 정해도 CC(Claude Code)가 무시하는 경우가 많음, 대화가 이어질수록 같은 문제를 반복하는 느낌이고, 아직까지 완전히 만족스러운 해결책을 못찾았음, 그래서 이 아티클을 단순한 AI 반대 논조로 치부하는 건 옳지 않다고 봄
          + Cursor를 다시 평가 중임, 기대만큼 속도를 내지 못하는 이유는 사소한 오류들(LM이 "":""를 "",""로 바꾸는 것 등)뿐만 아니라, 코드베이스가 너무 크고 낡고 품질 편차가 커서임, LLM은 결국 가장 흔히 쓰이는(= 안 좋은) 패턴을 더 자주 차용함, 명확하게 ""이 부분을 참고해라""라고 지시해도 전체 코드베이스 전반에 영향을 받음, 룰(rule)로 지정해도 프롬프트에 직접 박아넣은 것과 크게 다르지 않은 것 같음, 해결책이 있는지 궁금함
          + 문제의 핵심은 도구가 아니라 vibe을 중시하는 '코더'임, 애정도 없이 코드 작성도 헐렁함
     * 내 경험상 거의 모든 이슈는 한정된 컨텍스트 윈도우와 서브옵티멀한 'context engineering'의 결과임, LLM이 전역 함수 등 중요한 맥락을 제대로 전달받으면 비교적 잘 활용함, 문제는 어떤 맥락을 끊김 없이 계속 유지하며 제공하느냐임, 앞으로 sub-agent 같은 형태로 이 부분에서 많은 발전이 있을 것으로 기대함
          + ""에이전트가 전역 함수 설명을 알고 있으면 잘 쓴다""라고 했지만, 사실 '에이전트'라는 존재는 없고, 그저 입력과 출력을 연결하는 수학적 함수에 불과함, LLM을 인간에 비유하는 건 부정확함, 인간과 기계는 전혀 다름
     * 이 모든 내용이 맞다고 생각함, LLM을 활용할 때 가장 좋은 방법은 컴파일러가 어셈블리보다 한 단계 높은 수준을 제공하는 것처럼, 요구사항과 입출력을 명확히 설명하면 논리적 번역으로 코드를 만들어줌, 그렇기 때문에 입력에 혼란(엔트로피)를 최소화해야 함, LLM은 본질적으로 translation engine임, '생성'이 아니라 '번역'하는 데 써야 더 효율적임, 그럼에도 불구하고 주기적으로 더 스마트하고 직관적으로 진화하는 모델이 나오고 있고 그만큼 신경 덜 써도 결과가 좋아짐, 결국 언젠가는 LLM이 어떤 태스크든 인간 개발자보다 나은 결과를 내는 시대가 올 것이며, 인간의 다른 역할도 마찬가지일 것임
          + 잘못된 비유임, LLM은 자연어를 고수준 코드로 '컴파일'하는 엔진이 아님, 프로그래밍 언어와 머신 언어는 명확하고 일관된 의미 체계를 요구하지만, 자연어는 애초에 추상화 계층이 다름, 같은 LLM도 새로운 시드값이나 버전에서는 다른 결과가 나올 수 있음, 반면 컴파일러는 항상 같은 입력이면 반드시 같은 출력을 내야 함
          + LLM이 컴파일러 같은 추상 계층이라는 건 동의할 수 없음, 실제로 LLM은 그저 임의의 토큰 생성기임, LLM을 이용해서 만든 결과물 중 제대로 쓸 만한 것을 본 적이 없음, 싱귤래리티나 데이터 무한확보 같은 테크노 낙관론은 현실적 근거가 부족함, 결국 고품질 데이터 확보는 엄청나게 비용이 드는 일임, 현재로선 희망적 예측은 무의미함
     * ""많은 사람들이 좋은 커피보단 빠르고 싼 커피를 더 원한다""는 걸 저자보다 더 늦게 이해했음, 현실에서 다수는 품질보다는 속도와 가격을 중요하게 여김
          + Keurig 커피머신의 유행만 봐도 그 경향성을 알 수 있음

   본문보다 HN 글이 더 달달함
"
"https://news.hada.io/topic?id=22354","AWS S3 Vector 기반의 하이브리드 벡터 스토어 구축하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  AWS S3 Vector 기반의 하이브리드 벡터 스토어 구축하기

    AWS S3 벡터 저장소의 중요성

     * AWS S3 Vector Store는 대규모 AI 인프라에서 중요한 전환점으로 여겨진다.
     * 하이브리드 접근 방식이 확장 가능하고 비용 효율적인 GenAI 애플리케이션 구축에 필수적이다.
     * 이 기술은 벡터 데이터베이스의 발전과 함께 Retrieval Augmented Generation (RAG), AI 코파일럿, 생성적 검색 플랫폼의 발전에 의해 주목받고 있다.
     * AWS의 S3 Vector Store는 저장, 쿼리 및 관리를 효율적으로 수행할 수 있는 가능성을 제시한다.

    벡터 데이터베이스의 부상

     * 최근 1년 동안 벡터 데이터베이스는 주목받기 시작했으며, 이는 RAG와 AI 코파일럿의 발전에 기인한다.
     * 기술적 부채와 비용 문제는 여전히 존재하지만, 벡터 데이터베이스의 가능성은 매우 크다.
     * 수십억 개의 임베딩을 효율적으로 저장하고 관리하는 것이 이 기술의 핵심이다.
     * AWS S3 Vector Store는 게임 체인저로서의 가능성을 보여준다.

    기존 벡터 데이터베이스의 한계

     * 기존의 벡터 데이터베이스(예: OpenSearch, Pinecone, pgvector)는 속도를 위해 설계되었다.
     * 이러한 시스템은 밀리초 단위로 임베딩을 가져오는 것을 전제로 하며, 이는 고성능 IR 작업에 최적화되어 있다.
     * 그러나 비용과 운영 팀의 인내가 한계에 도달할 수 있다.
     * 대부분의 벡터는 ""롱 테일"" 로 분류되며, 이들은 실시간 검색을 필요로 하지 않는다.

    Amazon S3 벡터 저장소의 기능

     * AWS의 S3 Vector Store는 객체 저장소의 기본 원칙을 활용하여 벡터 작업을 결합하였다.
     * 주요 기능:
          + 벡터 버킷: 수십억 개의 인덱스를 지원하며, 샤딩에 대한 걱정이 필요 없다.
          + API: 임베딩 CRUD 및 유사성 검색을 위한 API 제공, 메타데이터를 통한 하이브리드 필터링 가능.
          + S3의 내구성, 보안 및 비용 효율성: S3의 장점을 그대로 활용한다.
     * 서버리스 아키텍처로, 클러스터 조정이 필요 없다.

    성능 문제와 현실

     * Amazon S3 Vector Store의 ""서브 초"" 지연 시간은 매력적이지만, 사용자 인터페이스에서는 150ms가 생명과도 같다.
     * AWS는 S3 Vectors가 100-800ms의 응답 시간을 목표로 하고 있음을 명확히 하고 있다.
     * 이는 배치 검색, 아카이브 회수, 배경 강화와 같은 시나리오에 적합하다.
     * 반면, OpenSearch와 같은 시스템은 10-100ms의 지연 시간으로 실시간 검색에 적합하다.

    Amazon S3 벡터 저장소의 가격 모델

     * 가격은 Amazon S3 Vector Store가 주목받는 이유 중 하나이다.
     * S3 Vectors는 전통적인 벡터 데이터베이스의 컴퓨팅 집약적인 클러스터와 벡터 저장을 분리하도록 설계되었다.
     * 가격 구조:
         1. PUT 비용: 각 벡터의 PUT 비용은 $0.20 per GB이다.
         2. 저장 비용: S3 Vectors는 $0.06 per GB per month로 청구된다.
         3. 쿼리 및 API 사용 비용: GET 및 LIST 요청은 $0.055 per 1000 requests이다.
     * 이러한 가격 모델은 대규모 데이터를 처리하는 데 있어 비용 효율성을 제공한다.

    경제적 영향과 추천 사항

     * S3 Vectors의 경제적 이야기는 사용 사례와 밀접하게 연결되어 있다.
     * 차가운 저장소, 규정 준수, 참조 데이터 세트에 대해 최대 90% 비용 절감을 약속한다.
     * 그러나 핫 패스 또는 초저 지연 애플리케이션의 경우, 비용이 급격히 증가할 수 있다.
     * 하이브리드 접근 방식이 필수적이며, 이는 비용과 성능을 모두 고려해야 함을 의미한다.

    하이브리드 접근 방식의 필요성

     * RAG는 ""검색 후 생성"" 의 혼합을 의미하며, 벡터 저장소에도 동일하게 적용된다.
     * 현대 AI 작업은 빠른 접근과 비용 효율적인 아카이브를 조화롭게 지원해야 한다.
     * S3 Vectors와 OpenSearch는 각각의 장점을 가지고 있지만, 단독으로는 모든 요구를 충족하지 못한다.
     * 하이브리드화는 예산을 초과하지 않으면서도 사용자 참여를 유지하는 유일한 방법이다.

    두 세계의 조화

     * 하이브리드 모델은 디iscipline과 아키텍처 모두를 요구한다.
     * 벡터 이동: 벡터를 S3로 이동할 시점과 OpenSearch로 다시 가져올 시점을 결정해야 한다.
     * 일관성: 벡터의 메타데이터를 업데이트할 경우, 진실의 출처를 관리해야 한다.
     * 쿼리 오케스트레이션: 검색을 원활하게 제공하기 위해 두 저장소에 쿼리를 분산하고 결과를 통합해야 한다.

    어디에 무엇을 저장할지 결정하기

     * 접근 빈도: 사용자 상호작용을 지원하는 벡터는 핫하게 유지해야 하며, 그렇지 않은 경우 S3로 이동해야 한다.
     * 성능 허용 범위: 비즈니스 프로세스나 배경 분석은 S3가 유리하다.
     * 저장 비용: 임베딩의 양이 많아질수록 비용을 면밀히 검토해야 한다.
     * 동적 티어링: 쿼리 로그와 사용 통계를 주기적으로 분석하여 벡터를 이동해야 한다.

    GenAI 플랫폼과의 통합

     * AWS 중심의 기업에서는 S3 Vector Store가 Amazon Bedrock Knowledge Bases와 통합되어 있다.
     * 이는 RAG 기반 파이프라인의 백엔드로 사용될 수 있으며, GenAI 에이전트의 메모리로 활용될 수 있다.
     * OpenSearch는 활성 인덱스에 대한 데이터 흐름을 제공하는 보완적인 역할을 한다.
     * 두 시스템 간의 조화로운 아키텍처는 수평적으로 확장 가능하고 수직적으로 조정 가능하다.

    실용적인 고려 사항과 경고

     * S3 Vector Store의 비용과 규모는 특정 작업에 대해 매력적이지만, 잘못된 사용은 사용자 경험을 저하시킬 수 있다.
     * 하이브리드화는 복잡성을 증가시키며, 관찰 가능성, 경고, 자동화가 필요하다.
     * 그러나 90%의 저장 비용 절감과 운영 위험 감소는 매력적인 보상이다.
     * 무시할 수 없는 기회는 두 계층 간의 원활한 장애 조치 구축에 있다.

    미래를 위한 벡터 구축

     * Amazon S3 Vector Store는 대규모 AI 인프라의 이야기에서 중요한 전환점이다.
     * 기술 팀은 벡터 데이터의 증가 문제를 해결할 수 있는 새로운 경로를 열 수 있다.
     * 그러나 더 나은 도구는 사고의 부담을 덜어주지 않는다.
     * 하이브리드 아키텍처를 설계하는 것은 비즈니스 맥락과 엔지니어링 규율 모두를 고려해야 한다.
"
"https://news.hada.io/topic?id=22338","좋은 설계 문서 작성법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              좋은 설계 문서 작성법

     * 설계 문서는 시스템의 구현 전략과 제한사항, 트레이드오프를 정리한 기술 보고서임
     * 설계 문서는 독자가 해당 설계가 상황에 적합하다는 점을 납득하도록 설득하는 역할임
     * 문서의 구성이 중요하며, 논리적인 흐름을 통해 독자가 내용에 놀라지 않도록 해야 함
     * 편집을 통해 불필요한 단어를 줄이고, 독자의 집중력 자원을 아끼는 것이 필요함
     * 짧은 단락과 부록 활용, 연습을 통한 문서 작성 역량 향상이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

정의

     * 설계 문서는 시스템 구현 전략을 트레이드오프와 제약 조건 맥락에서 정리한 기술 보고서임

목표

     * 설계 문서는 수학에서 증명이 정리를 납득시키는 것과 같이, 해당 설계가 최적임을 독자에게 설득하기 위한 목적을 가짐
     * 설계 과정에서 작성 자체가 사고의 엄밀성을 높임
     * 설계 문서를 작성하며 막연한 생각을 구체적 사고로 바꿀 수 있음

조직화

     * 좋은 설계 문서의 구성은 코드 조직화만큼이나 중요함
     * 초보자들이 코드를 쓰듯, 많은 사람이 '스파게티 설계 문서' 를 작성하는 경향이 있음
     * 논리적 순서 없이 문장을 나열하면, 독자가 맥락을 따라가기 어렵고 혼란을 느낌
     * 완벽한 문서는 독자가 놀라지 않도록 흐름이 자연스러워야 하며, 각 문장이 이전 내용을 토대로 당연하게 이어져야 함
     * 독자의 사고 상태를 파악하여, 단계적으로 새로운 상태로 인도하는 것이 목표임
     * 예상 가능한 이의제기를 사전에 해소해야 하며, 독자가 반론을 제기하기 전에 설명해야 함

편집

     * 내용을 잘 조직한 후에는 불필요한 단어 제거(편집) 단계가 중요함
     * 독자의 집중력은 한정된 자원이며, 불필요한 정보는 과감히 삭제가 필요함
     * 초안에서 무의미한 표현을 약 30%는 줄일 수 있음
     * 타인의 문서를 편집하며 비판적 시각을 기르면, 자신의 글도 효율적으로 다듬을 수 있음
     * 짧은 트윗(280자 제한) 으로 연습하는 것도 사고 단순화와 압축 능력 향상에 도움을 줌

경험과 연습

     * 반복 연습만큼 실력을 키우는 지름길은 없음
     * Amazon에서의 문서 중심 문화 경험은 문서 작성 역량 향상에 큰 도움이 되었음
     * 중요한 회의에서는 1~6페이지 규모의 설계 문서를 배포한 후, 모두가 조용히 읽고 여백에 의견을 적는 방식을 사용함
     * 피드백을 받으며 글쓰기 실력을 실질적으로 높일 수 있음

구체적 팁

  짧은 단락 사용

     * 설계 문서는 연속된 간결한 bullet point로 흐름을 만들어야 함
     * 각 bullet point(관찰, 아이디어, 문제점, 개선 등)는 한 개념에 집중된 짧은 단락으로 구성함
     * 각 단락은 한 문장으로 요약할 수 있을 정도의 명확성을 가져야 하며, 이로 인해 독자의 단기 기억 자원을 절약할 수 있음

  부록 활용

     * 복잡한 계산이나 시뮬레이션 결과는 문서 본문이 아닌 부록에 자세히 정리하고, 본문에는 간단한 각주 형태로 언급함
     * 본문의 주요 결론 이해에 부록은 필수적이지 않으며, 궁금한 독자가 참고할 수 있도록 제공함

  편집 예시

     * (편집 전, 장황한 단락):

     각 bullet point는 문서에서 별도 단락이 되어야 한다. 각 단락은 한 문장으로 요약 가능해야 한다. 실제로 한 문장이 될 필요는 없으며, 개념을 설명하기 위해 추가 설명이 들어갈 수 있다. 하지만 독자가 읽은 후엔 한 문장으로 요약 가능해야 한다.
     * (편집 후, 간결화된 단락):

     각 bullet point는 한 단락으로, 한 문장으로 요약될 수 있어야 한다. 실제로 한 문장이 아니어도 되며, 필요하다면 부연 설명을 추가할 수 있다. 하지만 읽고 나면 한 문장으로 압축될 수 있어야 한다.

마무리

     * 설계 문서는 사고의 엄밀성, 논리적 흐름, 독자 중심 편집, 반복 연습을 통해 역량을 키울 수 있는 중요한 과정임

        Hacker News 의견

     * 기사에서 특히 인상 깊었던 인용문 두 가지를 소개함. 첫 번째는 X의 스크린샷에서 ""글을 쓰는 과정에서 아이디어가 10배 더 좋아짐""이라는 문구임. 두 번째는 시작 부분에서 ""설득해야 할 가장 중요한 사람은 저자 자신임""이라는 말임. 수년간 업계에서 일했어도 여전히 디자인 문서의 필요성에 반대하는 사람들이 있어 놀라움. Leslie Lamport가 ""글쓰기는 우리 사고가 얼마나 엉성한지 자연이 말해주는 방식""이라고 했음. 기술 글쓰기 능력을 더 키우고 싶다면, Write Like an Amazonian(https://medium.com/@apappascs/…) 글을 추천함
          + ""형용사를 데이터로 바꾸라""는 조언이 기술 업계 전반에 퍼져서 그런지, 요즘 보는 이력서마다 수치로 가득해 어떤 의미인지 헷갈릴 정도임
     * 디자인 리뷰어로서, 모든 문서 작성자가 반드시 내면화해야 할 점이 있음. ""좋은 문서는 문제와 사고 모델을 독자가 이해하게 하여, 수주간의 고민 끝에 탄생한 해법이 딱 소개될 때 자연스럽게 수긍하게 만들 수 있음""이라는 부분임. 내가 가장 좋아하는 인용구는 ""시간이 더 있다면 더 짧은 편지를 썼을 것임""임. 디자인 문서는 복잡한 내용을 단순하게 만들어야 하며, 개발자가 겪은 모든 우여곡절과 실패를 무작정 담는 곳이 아니라고 생각함. 이런 내용도 물론 담을만하지만, 별도의 문서나 부록 등에 정리하는 것이 좋음. 앞으로 나아갈 길을 간단하게 보여줄 필요가 있음
          + ""더 많은 시간, 더 짧은 편지""라는 표현을 더 선호함
          + 항상 이런 질문을 스스로에게 던짐: ""이 주제에서 괜한 논쟁이 발생할까?"", ""논쟁을 할 만한 가치가 있는가?"" 내 목표는 새로운 독자가 어렵지 않게 논의에 참여할 수 있도록 하고, 중요하지 않은 부분에 대해서는 논란이 생기지 않게 하는 것임
     * Amazon 회의는 발표자가 산문 형태의 문서를 나눠주면서 시작함. 모두가 조용히 앉아서 문서를 읽고, 여백에 빨간 펜으로 메모와 질문을 적음. 실제로 Amazon에서 일해본 적은 없지만, 이 방식이 신기할 정도로 효과적이고, 실제로 이 경험을 말하는 사람들은 다들 좋아함. 귀중한 회의 시간을 다 같이 읽고만 있으니 비효율적일 것 같지만, 사실 사전에 읽고 준비하면 더 짧은 회의가 가능할 것임. 실시간 동시 읽기는 느린 독자를 기다리거나, 맥락이 부족해 이해도가 다르기 때문에 모두가 모호하게 시간을 보내게 됨. Google에서 디자인 리뷰를 할 때 참석자 대부분이 준비 없이 처음 문서를 보고 토론에 참여하는 모습을 자주 봤음. 이건 구글이 강한 문서 문화가 없었고, 팀 리드나 매니저도 준비 없이 오는 것을 암묵적으로 용인했기 때문이라고 생각함. 회의 전에
       확실히 읽고 오도록 문화만 잘 자리잡아도, 회의 시간이 훨씬 효율적으로 쓰일 수 있을 것으로 보임
          + 사람들은 회의 전에 미리 읽어오면 회의 시간이 줄어든다고 말하지만, Amazon의 관행은 사람들이 실제로는 미리 읽어오지 않는다는 현실에 대한 대응임. 예전 관련 기사에서 보니, 사전에 읽고 준비하는 강한 문화를 만드는 것이 사실상 불가능했다고 함. 모든 참석자가 바로 앞 회의 때문에 사전에 준비를 못 했고, 그 앞 회의도 있었기 때문임. 이론적으로는 회의 수를 줄이면 되지 않냐는 비판도 가능하지만, 실제로는 회의가 가치가 있었고, 읽는 시간 포함해도 충분히 결정이 이뤄졌다고 함. 결국 결과에 집중할 필요가 있으며, 실제로 Amazon에서는 이 방식의 장점이 단점보다 크다고 느끼는 듯함
          + 언제 문서를 읽느냐가 왜 중요한지 의문임. 만약 시간이 더 필요하면 회의 시간을 늘릴 수 있음. 단점이라면 회의 일정 잡기가 어렵다는 건데, 총 소요 시간은 변하지 않는다는 의견임
          + 모두가 동일한 이해 수준이 아니거나, 누군가의 코너 케이스를 놓칠까 걱정할 때 더 많은 시간이 낭비된다고 생각함
          + 실제로 회의에서 다루지 않으면 아무 일도 일어나지 않는다는 경험을 얘기함
     * 명확성과 편집에 대한 훌륭한 조언이 많음. 약점은 문서가 승인된 이후 어떻게 관리하느냐임. 관리가 없다면 '디자인 고고학' 상태로 퇴화하게 됨. 몇 년 전 Andrew Harmel-Law가 조직 내에서 아키텍처 의사결정을 효과적으로 기록하는 방법으로 Architecture Decision Records(ADRs)를 제안했는데, 이 방식이 도움이 될 수 있음. ADRs는 코드 옆에(예: adr/001-use-postgres.md) 있고, 맥락과 결정, 그리고 상태를 짧게 기록함. 그래서 PR마다 쉽게 검토할 수 있고, 상황이 바뀌면 쉽게 대체할 수 있다는 점이 장점임. 원래 의사결정의 근거도 수개월이 지나도 검색할 수 있게 됨. [링크: https://martinfowler.com/articles/…]
          + 이런 방식이면 Security, Privacy, Compliance 등 전체 조직의 위원회들이 모든 ADR이 걸린 PR마다 검토자가 되는지 궁금함. 그런 PR이 90일 이내에 머지될 수 있는지 의문임
          + MF.com(https://thoughtworks.com/radar/techniques/…) 링크를 완전히 읽어봐야겠지만, ""Advice Process""는 '모두와 이야기하라'라는 문장으로 끝나버림. 'Managing'이란 직함을 달고 있는 사람은 이쯤에서 관심을 잃을 것 같음. 진짜 핵심은 ""네 가지 지원 요소""인데, ADRs를 더 알아보려고 이 링크로 들어갔다가, 결국 PDF(https://thoughtworks.com/content/dam/…)를 받게 됨. 실제로 ADRs가 무엇인지 정의를 명확하게 알려주면 좋겠음
          + Session messenger가 대표적인 예시임. 수많은 디자인 및 아키텍처 변경이 있었기 때문에 어떻게 동작하는지 공식적으로 설명해주는 권위 있는 정보가 없음. 참고로 보안 메시징이 필요하다면 그냥 Signal을 쓰면 됨
     * 내 경험상, 조직과 명확성은 SW 엔지니어들이 문서 작성 실력을 늘리는 데 가장 큰 장애물임. 저자의 '코드 스파게티' 비유가 아이디어 조직의 중요성을 설명하기에 좋은 예라고 생각함. 나도 비슷한 얘기를 다른 식으로 전하려 한 적이 있었는데, 앞으로 이 비유를 쓸 계획임. 예전에 비슷한 블로그 글을 쓴 적 있는데(https://ryanmadden.net/things-i-learned-at-google-design-docs/), 정보 밀도와 실습의 중요성 등 공통점과 업체에 따라 다른 점이 흥미로웠음. '짧은 문단' 주장에 대해서는 살짝 다르게 생각함. 정보가 잘 다듬어져야 짧은 문단이 나오지, 줄만 바꾼다고 도움되는 것은 아님. 'Editing' 부분이 그 밑바탕 아이디어를 더 잘 설명해주고 있다고 생각함
     * 내가 쓰는 한 가지 프로세스가 있음. 1단계: 생각나는 것을 아무렇게나 문서에 쏟아냄(말로 받아쓰기를 써봐도 좋음). 2단계: LLM(대형 언어 모델)로 구조와 흐름을 잡게 시도함. 사실 이 단계에서 결과물을 버릴 수도 있으며, 계속 생각을 다듬는 과정임. 3단계: LLM 결과를 참고하거나, 아예 새로 아웃라인을 짜서 첫 초안을 작성함. 4단계: 단어 줄이기, 쉬운 단어로 바꾸기 등 최대한 간결하게 만듦. 5단계: 4단계 반복함. LLM은 두서없는 초안을 구조화해주는 다리 역할을 해줌. LLM이 내놓은 내용을 버릴 각오도 필요함. 최소 30%는 항상 줄일 수 있음. 실제로 줄이면서도 의미를 잃지 않는 걸 볼 때마다 놀라움
          + 내 글을 다시 편집하는 과정이 글을 처음 쓰는 것만큼이나 중요하다고 느낌. 이때 내가 얼마나 결론을 성급하게 내렸는지, 고려 안 한 점은 무엇인지 파악하게 됨. 많은 사람이 글쓰기를 사고의 집중화 도구로 충분히 가치 있게 평가하지 않는 듯함. 코드도 마찬가지임. 단순한 템플릿 작성한다고 쳐도, 테스트 코드를 작성하며 메인 코드를 개선할 아이디어가 떠오를 때가 많음. 하지만 LLM이 이런 정성적인 개선의 기회를 알려주진 않음
          + 확장, 축약, 압축, 다시 확장, 다시 압축 반복임. 누군가 구체적인 질문을 하면 다시 확장하고, 마지막엔 LLM을 사용해서 재미를 위한 부담 없는 요약을 하게 됨. 우리는 진짜 끝도 없는 롤러코스터를 타고 있는 셈임
     * 글쓰기를 더 많이 해야겠다고 생각함. 내가 생각하는 대표적인 문서 구조법은 B.O.O.와 Good Strategy/Bad Strategy임. B.O.O.는 Background, Objective, Overview의 약자로, 어떤 흐름으로 여기까지 왔고, 무엇을 어떻게 바꾸려고 하는지 정리하는 것임. Good Strategy/Bad Strategy는 진단, 가이드라인/전제조건/요구사항, 그리고 액션으로 구성된 책인데, 문서 조직 측면에서 B.O.O.와 유사함. B.O.O.는 Google이나 구성원이 작은 조직에서 잘 맞고, Good Strategy/Bad Strategy는 훨씬 다양한 규모에 적용 가능하지만 필력 있는 저자가 필요함
     * 기술 글쓰기 수업을 들으면서 요점을 명확하게 요약하는 능력이 크게 향상됨. '빨간 펜으로 자르기' 방식(글을 쓰고, 줄을 긋고, 다시 쓰기)에 집중했는데, 최대한 적은 단어로 개념을 전달하는 법을 강조함. 이 과정은 여러 단계로 나뉘고, 연습할수록 더 쉬워짐. 이런 역량은 팀원들과도 공유하려 하지만, 정기적으로 훈련해야 하는 스킬임을 항상 상기함
     * 이런 식으로 작성된 문서와 글쓰기 문화 자체를 정말 좋아함. 하지만 이런 접근 방식이 역효과를 낳는 경우도 봄. 이 방식을 통해 결론에 이르는 이유와 논리를 설명하는 것이 설득형 문서에는 매우 효과적임. 그렇지만 항상 그런 식의 설득이 필요하지 않은 경우도 많음. 때로는 결론부터 직접적으로 써주는 것이 오히려 독자, 특히 작성자를 신뢰하는 독자들에게 더 좋음. 많은 경우 독자는 논리를 따라가느라 지치는 대신, 요점부터 알고 싶어함. 일단 결론을 알게 되면 그제야 세부 논리를 따라가고 싶어짐
          + 위에 요약을 달고, 세부 설명과 근거를 아래에 이어가는 구조도 충분히 괜찮음
     * 나만 읽게 될지도 모를 디자인 문서를 직접 쓸 때가 종종 있음. 문서를 글로 남기는 것만으로도 강력함을 느낌. 실제 예시 문서가 있다면 정말 도움이 될 것 같은데, 내 문서 구조와 다른 사람들의 최종 구조를 비교해보고 싶음
"
"https://news.hada.io/topic?id=22290","라이브 코딩 인터뷰는 스트레스를 측정할 뿐, 코딩 실력을 평가하지 않음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                라이브 코딩 인터뷰는 스트레스를 측정할 뿐, 코딩 실력을 평가하지 않음

     * 라이브 코딩 인터뷰가 실제로는 엔지니어의 코딩 능력보다 스트레스 반응을 더 잘 측정함
     * 과학 연구에 따르면 실시간으로 지켜보는 환경에서는 인지 능력 저하와 심한 성과 변동이 나타남
     * 특히 여성 지원자의 경우, 공개 환경에서 전원이 탈락했으나 개인 환경에서는 모두 통과하는 현상도 확인됨
     * 대부분의 기업에서는 스트레스 적응력을 요구하지 않으면서 코딩 테스트로 이를 잘못 평가하는 문제점이 존재함
     * 모의 테스트, 점진적 노출, 그리고 보조 영양소 등이 스트레스 완화에 도움될 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

라이브 코딩 인터뷰에 대한 개인적 경험

     * 일부 사람들은 라이브 코딩 인터뷰를 즐기지만, 필자는 그렇지 않음
     * 필자는 Toptal 지원 과정에서 라이브 코딩 테스트에서는 실패했으나, 혼자 다시 풀었을 때는 금방 해결했음
     * 이 경험으로 실시간 감시 하에서 스트레스로 인해 본래 능력을 발휘하지 못함을 인지하게 됨

스트레스에 반응하는 두뇌

     * 고위험, 시간 압박 상황에서 뇌의 편도체가 활성화되고, 코르티솔 수치가 상승함
     * 이로 인해 복잡한 추론과 기억을 담당하는 전전두엽의 기능이 손상됨
     * 작업 기억력은 새로운 문제 해결 능력을 판단하는 가장 중요한 척도이며, 이는 실시간 코딩 상황에서 크게 줄어듦
     * 가벼운 성과 불안이 있는 경우도 명확한 사고가 거의 불가능해짐
     * 집중이 어려워지고, 여러 단계를 동시에 기억하지 못하며, 본인이 ‘평소보다 한참 못한 사람’처럼 느껴짐

결정적 연구 결과

     * Microsoft 연구진이 수행한 논문에서는 동일한 코딩 문제를 개인 환경과 공개 환경에서 각각 풀려 비교함
     * 개인 환경에서는 혼자 방에서 진행, 공개 환경에서는 감독관 앞에서 사고 과정을 말하며 풀게 함
     * 결과적으로 지켜보는 환경에서는 성적이 절반으로 감소하며, 특히 성적 편차가 커지는 것으로 나타남
     * 남성 지원자와 달리, 공개 환경의 모든 여성 지원자가 불합격했고, 개인 환경에서는 모두 통과함
     * 라이브 코딩 환경이 유능한 엔지니어를 탈락시키는 과학적으로 입증된 배제 필터 역할을 함

스트레스 하 성과의 현실

     * 라이브 코딩은 결국 스트레스 상황에서의 성과를 보는 대리 척도에 불과함
     * 일부 기업은 실제로 스트레스 환경에서 잘 하는 사람을 원하지만, 대다수 기업은 이를 지원서에 명확히 밝히지 않음
     * 대부분의 작업이 실시간 스트레스가 적다는 점을 고려할 때, 라이브 코딩에서 실수했다는 이유로 우수한 엔지니어를 탈락시키는 것은 부적절함
     * 라이브 코딩은 코딩 능력보다는 스트레스 호르몬 수치를 측정한다고 봄이 타당함

스트레스 완화를 위한 방법

     * 라이브 코딩이 업계에서 일반적이므로, 스트레스 적응 훈련이 필요함
     * 실제와 비슷한 환경에서 반복 연습(Pramp, Interviewing.io, LeetCode 모의 테스트 등)으로 두뇌를 스트레스에 익숙하게 할 수 있음
     * 타이머 설정, 자신을 녹화, 친구에게 지켜보게 하는 식으로 점진적으로 압박감 높이는 연습이 효과적임
     * 추가적으로, L-tyrosine(스트레스 하 신경전달물질 보충)과 L-theanine(이완 및 집중 개선) 같은 영양소 섭취도 시도해볼 수 있음
     * 실제 인터뷰 이전에는 본인에게 잘 맞는 방법을 반드시 모의 연습에서 확인해야 함

결론

     * 라이브 코딩에 약하다는 것은 엔지니어로서의 자질 부족이 아닌 인간의 일반적 특성임

        Hacker News 의견

     * 나는 내 사례를 일반화할 생각은 없지만, 개인적인 경험을 공유하고 싶음. 지금은 성공한 자영업 indie developer임. 어려운 시절에도 indie 개발을 포기하지 않은 주요 이유 중 하나는, 사실상 고용될 수 없는 상태가 되었기 때문임. 나는 기술 업계에서 나이 차별이 심한 중년이고, 컴퓨터 사이언스 학위도 없으며, 라이브 코딩 인터뷰에서는 머리가 하얘지는 증상을 경험함. 모든 스트레스가 같은 것이 아니라는 점을 지적하고 싶음. 소방관들은 타는 건물로 뛰어들지만, 낯선 사람들 앞에서 발표하는 일에는 오히려 공포를 느낌. 나 역시 일상적인 업무 스트레스에는 강하지만, 타인이 내 어깨 너머로 감시하며 내 재정적 미래를 결정한다는 점은 소화가 안 될 정도로 부담스러움. 인터뷰가 끝나면 코딩 문제는 곧잘 풀 수 있음. 면접관들은 내가 사기꾼이라
       생각할지 몰라도, 20년 가까운 경력이 반박 증거임. 많은 사람들이 ""false negative""를 마치 랜덤인 것처럼 여기는 것 같지만, 나처럼 항상 제거되는 사람도 있음. 나는 오디션 스타일 인터뷰에서 항상 떨어지고, 무대 위에서 연기하는 사람은 아님.
          + 타인이 지켜보는 상황에서 심사가 이뤄지면 너무 공감됨. 60대 초반임. 20~30대에는 인터뷰를 그럭저럭 잘 봤지만, 시간이 지날수록 면접 자체가 점점 더 대립적으로 변했다고 느낌. 예전엔 '어떻게든 채용하려는' 분위기였다면, 지금은 '채용하지 않으려는' 이유를 찾는 느낌임. 이는 나이 차별 때문일 수도 있지만 업계 분위기 변화도 있다고 봄. 최근 15년 동안 인터뷰 경험이 점점 불쾌해지고, 인터뷰 중 패닉을 겪기도 했음. 그래도 어찌어찌 채용은 되었고, 참조 경력 덕분에 면접 없는 계약직을 하기도 했음. 마지막으로 다닌 스타트업이 22년 말에 자금이 끊겨서 퇴직을 결심했음. 일 자체는 정말 좋아하고, 최신 기술도 다뤘지만, 더는 인터뷰를 감당할 수 없었다는 점이 가장 큼.
          + 이 구조 자체가 문제라고 생각함. 최근의 코딩 인터뷰는 젊은이들이 CS 자료구조 과제를 했는지 따지려는 용도로 만들어진 것 같음. 2010년대 FAANG 같이 대량 채용하는 곳에서는 나름 의미가 있었겠지만, 중소형 회사에서는 오히려 실제로 코드를 읽거나 edge case를 논의하는 것 등 실무 상황에 집중하는 게 훨씬 나음. 20년 넘게 스타트업에서 일했지만, 이런 테스트는 여전히 못 통과함. 억지로 외우는 것도 거부함. 이게 안 맞는 곳이라면 차라리 안되는 게 맞음. CTO도 하고, 회사 여러 번 론칭했고, 팀 관리도 잘했는데 이번에도 신입처럼 취급받음. 예전에 LRU 캐시를 빠르고 깔끔하게 못 만들어서 탈락했는데, 요즘 스타트업에 이게 얼마나 필요할지 의문임. 최근에 써본 적도 없음. 이런 문제를 바로 못 풀었다고 무능력자처럼 보일 수도 있지만, 실무에서 안
            쓰는 걸 굳이 따지는 게 무슨 의미인지 모르겠음. 마치 건축가를 슬라이드 룰(계산자) 실력으로 뽑으려는 것과 같음. 이런 채용 방식은 결국 복잡한 코드베이스를 낳고, 진짜로 필요한 건 복잡성보다 비즈니스 목표 달성이 되어야 함. 나는 문제를 쪼개고 논리와 구조를 단순하게 유지하는 동료와 함께 일하고 싶음. leetcode로 똑똑한 사람을 고르는 것도 장점이 있긴 하지만, 현실 문제를 꾸준히 효과적으로 푸는 사람이 더 나음.
          + 나는 채용자 입장에서 이런 상황을 겪은 적 있음. 프로젝트에 오래 참여한 학생과 전화 인터뷰를 했는데, 여러가지 스트레스, 언어 장벽 때문에 제대로 실력 발휘를 못 하는 것 같았음. 형식을 바꿔볼 의향도 있었지만, 지원자가 스스로 더 이상 시도하지 않겠다고 함. 그런데 비동기 코딩 인터뷰로 전환하면 오히려 LLM 사용 여부만 테스트할 수도 있음. 결국 인터뷰에서 얼어붙는 사람을 거를지, 전혀 능력 없는 사기꾼을 거를지 둘 중 하나를 선택해야 한다면, 나는 전자를 필터링하는 게 더 나은 선택이라 봄.
          + 이런 성향의 불일치를 회사에서 많이 봄. 많은 프로그래머는 내성적인데, 채용하는 쪽은 외향적인 경우가 많음. 이런 차이를 제대로 관리하지 않으면 내향적인 인재가 배제되거나 이해받지 못하는 문제가 생김. 오픈 시트 환경(공유 앉는 공간)도 비슷한 문제임. 매니저들은 협업을 좋아해도, 내향적인 사람들에겐 매우 힘든 환경임.
          + indie 개발에서 성공하기 시작한 계기가 궁금함. 나도 거의 40살이고 프로그래밍을 오랫동안 취미로 해왔지만, 작년부터 본격적으로 직업 삼기로 결심했음. Github에 공개 프로젝트도 많고, 다른 분야에서 성공 경험도 많으며, 소통 능력도 나쁘지 않음. 하지만 라이브 코딩에서 어려움을 겪고 있음. 진짜 실력을 보여줄 수 있는 independent contribution 루트에 대해 어떤 생각이 있는지 궁금함. 진짜 실력이 있다면 그걸로 돈을 받고 싶다는 생각임.
     * 이번 주에 Data Engineering 후보자를 인터뷰했음. 아주 기본적인 4개의 SQL 문을 줬더니, 문제를 소리내서 읽고 바로 정답을 정확한 문법으로 냄. 마지막은 난도가 살짝 올라갔는데 막힘. ""결과를 확인해봐라""고 했더니 못 알아듣고 방어적이었음. ""테이블을 덤프해봐라""고 하니까도 아예 이해를 못하고 변명만 했음. 최종적으로 붙여넣은 SQL에는 [redacted].ai가 출력에 있었음. 아마 앞의 문제들은 AI로 해결했다가, 마지막 문제에서 티가 난 것임. 이런 기술 문제가 없었으면 부정행위를 못 걸러냈을 것임.
          + AI 인터뷰 부정행위 도구가 젊은 층에서 매우 퍼지고 있음. 어떤 경우는 바로 걸리지만, 경험이 많은 지원자들은 AI 활용과 중간중간 '소리가 안 들려요' 등으로 빈틈을 가리기도 함. 내가 속한 매니저 그룹에서도 이게 최근 가장 많이 논의되는 채용 주제임. 여력이 되는 회사는 마지막 면접을 오프라인에서 직접 봄. 원격 스크린에서는 괜찮은 것처럼 보이다가 실제 만나면 기본 질문도 못하는 경우가 있어서 결국 탈락시킴. 시간과 돈 낭비이긴 해도, 잘못된 채용의 비용보다 낫다고 판단함. AI 사용은 기술면접만이 아니라 이력서, 행동질문, 심지어 ChatGPT가 만들어준 S.T.A.R. 형식 답변까지 전체적으로 퍼져 있음. 신뢰할 수 있는 참고인 확인이 그 어느 때보다 중요해짐. 예전 상사가 말하는 업무 내용과 이력서에 적힌 내용이 완전히 다를 때도 여러 번
            경험했음. 만약 처음부터 우리 도메인 직접 경험이 없다고 솔직했으면 채용했을 수도 있는데, 이렇게 거짓말이 강할 경우 신뢰가 전혀 안 생김.
          + 요즘 후보자 인터뷰하면서 약 50%가 live GenAI를 실시간으로 활용했음. 지금까지는 누가 AI 쓰는지 알아채기 매우 쉬웠음. 자연스러운 대화에서 쉽게 드러남. 아이러니하게도, 마지막 후보자 역시 문제를 항상 되풀이해 말한 뒤 10~15초씩 대기함. 이런 테스트 자체가 근본 문제의 해결책이 아님을 의미함. 오히려 새로운 문제를 만들고, 뛰어난 후보자를 떨어뜨리는 원인이 됨.
          + ""AI를 쓰면 부정행위가 아니라, 실제 현업에서 하게 될 일을 미리 해보는 것이라면?"" 이런 시각으로 보면 그 인터뷰는 실제로 매우 효과적인, 작은 규모의 <i>요구사항 해석</i> 예시임. 결국 현업에서 쓸 도구를 자유롭게 쓰도록 하면서 그 안에서 직무와 언어 본연의 이해를 테스트하는 방향이 더 긍정적이라고 생각함. 요구만 단순히 Leetcode에서 벗어나 더 나은 방법을 찾아야 함.
     * 모든 것은 결국 '상황에 따라 달라짐'. 라이브 코딩 인터뷰도 마찬가지임. 지원자 입장에서 최고의 경험은 아니지만, Meta, Google 같은 대규모에서는 다른 방식보다 false positive(실제 실력 없는 사람을 뽑는 오류) 비율을 더 잘 줄임. 다만, 인터뷰어가 충분히 훈련받지 못했고, 문제가 지나치게 수수께끼 같은 경우가 많아 LeetCode를 많이 연습하거나 학계/졸업 직후가 아닌 이상 힘듦. 나는 6년간 평가(assessment) 분야에서 일했고, Fortune 10부터 스타트업까지 다양한 채용 과정을 직접 접했음. 후보자에게 실제 업무와 유사한 평가를 권장하고, unpaid labor에 가까운 '실제 업무' 표현은 이제 좋아하지 않음. 평가는 회사가 고액 연봉을 안심하고 제안할 수 있게 만드는 수단이어야 함. AI의 등장으로 짧은 take-home 문제도 공정성을 유지하기 어려워 졌음. 그래서 다시 온사이트
       면접, 실시간 감시 같은 극단적 방법에 의존하는 기업도 있음. 내가 생각하는 완벽한 솔루션은 모든 후보자가 같은 시간과 환경, 도구에서 자신의 최고 능력을 발휘할 수 있도록 하는 것임. 늘 이 문제를 고민 중이지만 아직 답을 찾지 못함.
          + ""Meta, Google 수준에선 잘 동작한다""고 하는데, 사실 데이터가 없음. Facebook, Google 출신 F급 개발자들과 같이 일한 경험이 있음. 실제로는 이런 대기업도 3~5% 인력을 해고하고 있고, 인터뷰만으로 false positive를 제대로 잡아내지 못한다는 증거임. 인터뷰어들이 들인 시간에 비해 3%의 오류율은 너무 높다고 봄. 사실상 이전의 'Fizzbuzz' 수준과 다를 바 없음.
          + 벽돌공에게 벽을 만들어보라고 시키는 게 아니라, 자격증만 인증하면 바로 채용하는 것임. 많은 직업이 이런 방식임. 실력이 안 맞으면 채용 후 내보내면 됨. 굳이 모욕적인 기업 면접 절차를 거칠 필요 없음.
          + 내가 본 최고의 엔지니어들은 항상 false negative로 분류되는 경우가 많았음. 라이브 코딩 면접에서 긴장해서 제대로 못 보여줌. 이런 면접이 ""잘 작동한다""는 식으로 단정 지을 수는 없다고 봄.
          + ""상황에 따라 다르다"" 이후에 ""라이브 코딩 인터뷰는 잘 된다""는 식으로 단정 짓는 이야기는 앞뒤가 안 맞음. 반대로 나는 ""상황에 따라 다른데, 라이브 코딩 인터뷰는 안 통한다""고도 주장할 수 있는 것임. 논리적 차이가 없음.
          + leet-code 통과가 진짜 문제 해결 능력이 없는 사람을 걸러내는 데 쓸 만함. 최고의 솔루션은 후보자가 자신이 가진 도구와 환경에서 최대치를 낼 수 있도록 하는 환경을 만드는 것임. 하지만, 실제로는 leetcode를 오프라인에서 화이트보드로 인터뷰어와 페어 프로그래밍하며 문제를 푸는 것이 가장 공정하지 않을까 생각함. 그게 정말 편안한 환경임.
     * 두 설명 모두 동시에 맞을 수 있다고 생각함. 실제로 일을 못 하는 ""시니어"" 개발자가 있기도 하고, 그런 사람을 걸러내는 데 라이브 코딩이 도움됨. 하지만 다른 이유로도 인터뷰에 실패할 수 있음.
          + 라이브 코딩은 잘해도, 대규모 시스템 설계 경험이 부족한 개발자도 많음. 이런 사람들이 기술 부채, anti-pattern, 불일치 등을 도입해서 코드베이스를 더욱 악화시키는 경우가 많음. 사실상 정말 피해야 할 타입임. 회사는 기존 시니어가 신입을 통제할 거라 믿지만, 어느 회사든 ""우리 코드베이스는 쓰레기""라고 하니, 이 대책도 실효성이 없어 보임.
          + 혼자 방에서 앉아서 코딩하게 하면, 비정상적 환경에서만 능력을 발휘 못하는 우수 인재를 놓치지 않을 것임.
          + 최근 라이브 코딩은 단순한 코딩 테스트를 넘어, 많은 알고리즘을 암기해야 하고, 30분 안에 두세 가지 알고리즘을 결합해서 문제를 풀어야 하는 수준임. 문제 푸는 데만 시간을 다 쓰다 보니, 오히려 진짜 코딩 실력을 보여줄 시간도 없음.
          + 20년 동안 현업을 하면서 그런 ""실력 없는 시니어""를 실제로 같이 일해본 적 없음. 이력서와 15분 대화만 해도 충분히 거를 수 있었음. 반대로 화이트보드 인터뷰에 합격하고도 팀 생산성에 악영향을 준 사람은 훨씬 더 많이 봄.
          + 왜 짧은 기한과, 화이트보드 앞에서 동시에 생각과 설명을 요구하는 환경이 필수적인지 이해가 안 됨. 실제 일을 못 하는 사람을 걸러내는 게 목적이라면, 굳이 이렇게까지 할 필요가 없음.
     * 내가 보기에 어떤 직무 수행력을 제대로 확인할 수 있는 유일한 방법은 실제로 그 직무를 시켜보는 것임. 만약 대체 평가법이 있더라도, 실제 작업을 시켜볼 수 있다면 다른 방식을 쓸 이유가 없음. 만약 회사 업무 자체가 너무 복잡해서 잘라서 면접에서 시킬 수 없다면, 아마 회사가 불필요하게 복잡한 일만 하고 있을 가능성도 큼. 예를 들어, 10킬로그램 무게를 들어보는 게 필요하다면, 실제로 10킬로그램 짐을 들어올리게 하면 됨. 그런데 '힘을 평가하겠다'며 '바지 벗고 1kg 양동이를 엉덩이로 집어 들어올리라' 같은 이상한 시험을 보는 것임. 결국, 실제 업무에 필요한 스킬만 제대로 보면 됨. 예를 들어, 셰프면 실제 주방에서 요리를 하게 하면 되고, 지원 상담원이라면 모의 상황에서 의사소통을 보면 됨. 전화 스크리닝 쪽도 실시간 스크린을 보게 하면
       됨.
          + ""면접에서 실제로 작업 시키면 부정행위나 임금 착취 이슈가 생기지 않냐""는 반론이 있음. 면접 과제를 회사에서 활용한다면 법적으로 문제가 생길 수 있음.
          + 하루, 일주일, 한 달 단위로 직접 고용해서 시켜보고 적합하면 채용하는 방식 제안도 있음. 단, 이런 방식은 미국식 employer healthcare 시스템과는 맞지 않음.
          + 회사 입장에서는 시간이 너무 많이 들기 때문임. 그래서 false positive보다는 false negative(잘못 탈락시키는 것) 쪽에 치우친 proxy를 씀.
          + 실제 작업을 면접에서 시키면 '공짜 노동' 요구라는 우려에 대해선 어떻게 생각하는지 궁금함.
          + 새로운 조직에 적응하는 데 시간이 오래 걸리므로, 실제 업무로 테스트하는 게 오히려 비효율적일 수 있음. 라이브 코딩 평가로 소통, 문제해결, 순수 코딩 실력 측정이 더 낫다고 봄.
     * 아이를 갖고 난 뒤로 면접 코딩 능력이 확 떨어짐. 예전엔 한 번도 이런 적이 없었는데, 요즘은 인터뷰에 너무 많은 것이 걸린 것 같음. 건강보험, 대출, 은퇴 등을 떠올리면 압박감이 심함. 인터뷰 중 완전 멈춰버리고, 끝나고 나서야 해결책이 떠오르는 게 괴로움. 연습에 시간을 더 쏟을수록 오히려 실력이 더 떨어짐. 부양가족 때문에 연습할수록 죄책감도 큼. 학습 효과도 약하고 오히려 더 위축됨. 빅테크에서 일하면서 느낀 것은, 이런 면접 방식은 지극히 ""모두를 동등하게 테스트하겠다""며 도입됐지만, 실제로는 각자의 상황 변화나 불균형을 전혀 반영하지 못함. 스트레스에 강한 동료라도 몇 년 후엔 달라질 수 있음. 편견을 제거한다는 명목으로 시작하지만, 방식을 바꿔야 한다고 생각함.
          + 이런 감정은 혼자가 아님. 나이 들어가면서 학습 속도가 느려지고, 자유시간도 줄어들어서 leetcode 연습 효율이 떨어짐. 그저 시간 여유 있는 사람들이 더 보상을 받는 현실에 짜증이 남.
          + 스트레스 완화법으로 명상, 호흡, L-Theanine, 베타 블로커 등을 시도해보는 것을 추천함. 스마트워치로 심박수와 혈압을 모니터링해보라 권장함. 실제로 이런 방법이 스트레스 악순환을 줄여줌.
     * 현업 스트레스와는 질적으로 다른, 매우 높은 강도의 스트레스 환경임. 구글 면접에서 경험을 공유하자면, 나는 네덜란드 최초의 지역 검색엔진을 만든 사람인데, 구글에서는 카우보이 모자를 쓴 인터뷰어가 화이트보드에 마커로 바이너리 서치를 코드로 쓰라고 시켰음. 나는 평소 손으로 글을 안 쓰고, 키보드만 쓰는데, 또 내가 설계한 검색 인덱스 경험을 전혀 보려고 하지도 않음. 아마 구글은 '카우보이' 마인드를 원했던 게 아닌가 싶음.
          + 나도 예전에 구글 리크루터의 설득에 넘어가 면접을 봤으나, 인터뷰어가 너무 형편없어서 기술 면접을 건너뛰었음에도 불구하고, 이후 구글의 어떤 제안도 거절 중임. 문제 대부분이 '낚시' 스타일이거나, 현실과 무관한 아주 옛날 UNIX 구조(예: inode 구조)를 물었음. 해당 역할에 전혀 연관 없는 내용이었음. 기업들은 반드시 인터뷰 질문을 트래킹, 피드백하는 과정과 인터뷰어 교육이 필요함. 여전히 인터뷰어 개인의 취향에 따라 질문이 정해지는 회사가 매우 많음.
          + 이런 구조가 실제로 의도한 대로 동작한 것임. 이런 회사들은 그 회사에 어떠한 수모도 견딜 수 있는 사람을 원함. 서로 fit이 아니었으니 서로 성공한 것임.
          + 바이너리 서치를 시키는 것에 모욕감을 느꼈다면, 자존심이 너무 강한 것 아닐까 생각함.
          + 연 25만 달러와 추가 Google 스톡 옵션이 걸려 있다면 하루 정도 ""모욕""쯤은 기꺼이 감수할 수 있음.
     * 나는 live coding screening을 '코드에 대한 대화'로 대함. 지원자에게 면접의 목적이 서로 소통이 잘 되는지 확인하는 것임을 분명히 함. 실력만 좋은 걸로는 부족하고, 함께 협업할 수 있어야 함. 예를 들어, 대화 과정에서 오해로 잘못 구현하거나, 아무리 코드 잘 짜도 기술 대화가 불가능한 지원자라면 문제임. 그래서 Fizzbuzz 유형의 질문이 중요한 이유는 단순하게 실력이 아니라 '기술적 논의' 능력을 테스트하는 역할임.
          + 맞음. live coding 인터뷰의 본질은 ""문제 해결 가능 여부""가 아니라 ""어떻게 해결할지 설명할 수 있는가""임. 약간의 기술적 기본기와 커뮤니케이션 능력이 핵심임. 실제 업무의 상당 부분이 비기술적 매니저에게 설명하는 일이므로, 이걸 시험한다 생각하면 됨.
          + 이런 방식이 좋음. 난 FizzBuzz를 최소 3개 언어로, 4가지 방식으로 풀 수 있고, 12가지 모두 설명할 자신 있음.
     * 나도 이 문제점을 인식하고 있고, 이렇다 할 대안이 없는 것도 동의함. 경력 속에서 정말 '엔지니어처럼 말하고 행동하는데 실제로 코딩을 못하는 사람'을 여러 번 목격함. 인터뷰에서 코딩 실력을 무시한 채 들어갔다가 후회한 적도 많았음. 어린 개발자들은 채용 과정의 여러 단계가 나이 든 지원자를 더 유리하게 만든다는 점도 이해했으면 함. 쓸데없는 '전문가' 포장에 속아 실제 코딩 능력이 없는 엔지니어가 업계에 많음을 알고 있음. 실제로 코딩 과정을 단 한 번도 안 보고 뽑는다는 건, 그냥 이력서와 대화, 참조만으로 밴드에 기타리스트를 채용하는 것과 같음. 실제로 기타를 연주해본 적이 없는 기타 전문가가 여전히 채용될 수 있음. 특별한 자격 없이도 이런 식으로 통과하는 사람들이 존재함. 면접에서 직접 증거 없이 추측만 하게 되면, 결국
       편견과 선입견이 개입될 수밖에 없음.
          + ""문제가 있지만 대안이 없다""는 얘기에 답하자면, 의사와 변호사처럼 자격증을 기준으로 하는 시스템도 있음. 수술 전에 시범까지 보라고 요구하지 않음. 위험도는 더 높음에도. 사실은 하위 직군에만 검증 절차가 강하고, 실제로 임팩트가 더 큰 매니저나 고위직은 더 쉽게 통과함.
          + 나는 화이트보드에 pseudocode를 작성시키는 게 더 낫다고 생각함. 문법 스트레스가 줄고, 문제 해결 논리를 더 잘 볼 수 있음. 동시에, 내 협업 방식(화이트보드에서 함께 아이디어 교환)과도 잘 맞음.
     * “스트레스는 실력과 별개”라는 현상이 반복됨. Microsoft 논문도 참혹했음. 대부분 ""LeetCode만 열심히 풀라""는 식의 조언만 있는데, 스트레스(코르티솔)를 간과함. 나는 스트레스 환경 자체를 스스로 익숙하게 만드는 연습을 함. 친구들은 강도 높게 훈련시켜주지 못하고, 코치는 시급이 너무 비쌈. 그래서 요즘 Tough Tongue AI라는 사이드프로젝트를 개발 중임. 목소리로 답하는 라이브 코드 에디터에서 실시간 질문, 방해, 즉각적인 피드백을 제공함. 이 훈련으로 '누가 날 지켜본다'는 긴장감이 점점 익숙해짐. 라이브 코딩 면접이 지속된다면, 알고리즘이 아닌 생리반응(스트레스) 자체를 훈련하는 방법이 필요함.
          + 최고의 학습 도구 중 하나임.
"
"https://news.hada.io/topic?id=22323","SQLX - 러스트 기반 SQL 툴킷 오픈소스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       SQLX - 러스트 기반 SQL 툴킷 오픈소스

     * 비동기(async) 방식의 순수 Rust 기반 SQL Crate로, Rust 매크로를 이용해 DSL 없이 컴파일 타임에 SQL 쿼리 검증 가능
          + ORM이 아님! : ORM 없이 SQL을 그대로 활용하고 싶은 프로젝트에 최적
     * PostgreSQL, MySQL, MariaDB, SQLite 등 데이터베이스 지원
          + .env 파일에 DATABASE_URL 지정해 손쉽게 컴파일 타임 체크 가능
     * Pure Rust 구현(MySQL/MariaDB 드라이버는 100% Rust로 작성. SQLite는 C 라이브러리 연동), unsafe 코드 사용 없이 안전하게 설계되어 신뢰성 높음
     * Tokio, async-std, actix 등 주요 Rust 비동기 런타임 및 다양한 TLS 백엔드와 호환되며, 플랫폼 독립적으로 사용할 수 있음
     * MIT/Apache 2.0 듀얼 라이선스로, 오픈 소스 및 상업적 프로젝트에 폭넓게 활용 가능

주요 기능

     * sqlx::Pool을 이용한 커넥션 풀링
     * 데이터베이스로부터 데이터를 비동기 Row 스트리밍
     * 커넥션별로 자동 쿼리 준비 및 캐싱
     * 준비되지 않은(Prepared 아닌) 쿼리도 간단히 실행 가능, 결과는 동일 Row 타입으로 반환
     * 지원하는 DB(MySQL, MariaDB, PostgreSQL)에서 TLS 암호화 연결 지원
     * 비동기 PostgreSQL 알림 지원 : LISTEN과 NOTIFY
     * 세이브포인트(Savepoint)를 활용한 중첩 트랜잭션 지원
     * 런타임에 DB 드라이버를 선택적으로 교체 가능한 AnyPool 지원

   개인적으로는 Go의 sqlc 접근 방식을 좋아합니다.

   일단 네이티브 쿼리를 짜고 코드를 생성하는 방식입니다.

   _bin collation 필드를 String으로 직렬화할 수 없는 이슈가 1년넘게 고쳐지고 있지 않아서 프로덕션용으로 사용하기엔 무리가 있습니다. 심지어 회귀 이슈라서 많은 사용자나 라이브러리들이 0.7버전에 머물러 있습니다.

   일단 고치긴 한 것 같은데 1년 가까이 걸리긴 했네요

   이슈 링크 부탁드려도 될까요? 좀 찾아봤는데 찾기 힘들어서요.

   https://github.com/launchbadge/sqlx/issues/3387

   이거 같아요

   SQLx - Rust SQL Toolkit
   5년전에 한번 공유했었는데요. 그동안 꽤 많은 변화가 있었습니다.

   이제 컴파일 타임에 실제 DB와 연동해서 검증이 가능해졌고,
   tokio/async-std/actix 까지 호환되며,
   커넥션 풀, 중첩 트랜잭션, AnyPool 등 실전에서 쓸만한 기능과 테스트가 강화되어 대규모 서비스에도 적용이 가능한,
   ORM 없이 SQL을 직접 사용하는 프로젝트에 추천할만한 프로젝트가 되었습니다.

   그리고 sqlx 를 통해서 ORM을 구현한 SeaORM, Welds 같은 프로젝트도 생겨났고요
   그외에 쿼리 빌더인 SeaQuery 나 웹 어플리케이션 프레임워크인 SQLPage 같은 것도 있네요
"
"https://news.hada.io/topic?id=22333","외로움을 AI가 해결한다면, 우리는 어떤 존재가 될까 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     외로움을 AI가 해결한다면, 우리는 어떤 존재가 될까

     * AI 동반자는 점점 더 현실적이고 일상적인 존재로 자리잡고 있으며, 실제 인간보다 더 공감적인 반응을 보이는 사례도 연구에서 발견됨
     * AI 동반자가 외로움을 줄일 수 있지만, 외로움의 불편함 자체가 인간의 성장과 자기이해에 중요함
     * 그러나 AI가 주는 무조건적 공감은 인간관계의 교정적 피드백을 약화시키고, 자기기만의 위험을 높임
     * 외로움은 단순한 결핍이 아닌, 인간의 창의성·성장·연결을 이끄는 신호로 작동함
     * AI 상담 챗봇과의 교류가 감정적 위안을 주기도 하지만, 진짜 관계인지에 대한 철학적 논의가 계속됨
     * 젊은 세대일수록 AI 동반자에 의존하면 진정한 연결과 성장 기회를 상실할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 동반자와 외로움의 변화

     * 최근 들어 누구나 AI 동반자에 대해 의견을 갖고 있음
     * 필자는 심리학자 두 명, 철학자와 함께 “공감적 AI에 대한 찬사” 논문을 발표, AI가 외로운 사람들에게 실질적 위로와 동반자가 될 수 있다고 주장함
     * 이 주장은 인문사회학계에서 크게 반발을 불러일으킴
          + 이 분야에서는 AI가 기술적 진보라기보다는 쇠퇴의 전조로 여겨지는 경향이 있음
          + AI는 종종 실리콘밸리 부호들이 만든 영혼 없는 도구로 인식되고, 인간 관계의 대체품으로 보는 것에 불편함을 느낌
     * AI의 등장은 일자리, 부정행위, 창의성 침해 등 다양한 우려와 맞물려 논의됨
     * 그럼에도 외로움은 실제로 ‘유행병’인지에 대한 논란은 있지만 전 세계적으로 중요한 사회적 문제로 인식되어, 일본, 영국 등은 외로움 담당 장관까지 임명함

외로움의 건강 및 사회적 영향

     * 외로움은 ‘영혼의 치통’으로 묘사될 정도로 고통스러움
          + 단순한 감정적 불편을 넘어, 심장병, 치매, 뇌졸중, 조기 사망 등 심각한 건강 위험과 직결됨
          + 2023년 미국 Surgeon General 보고서에서는 외로움이 “매우 심각한 건강 위협”임을 강조
     * 만성적 외로움은 흡연, 비만, 운동 부족보다도 더 치명적임
     * 젊은 세대보다는 노년층에서 더 흔하며, 60세 이상 미국인 중 절반이 외로움을 경험한다고 답함
     * 가족, 친구의 상실, 신체적 제약, 인지 저하 등으로 사회적 연결이 약화되는 경우가 많음
     * 경제적 여유가 있는 사람은 돌봄을 구매할 수 있지만, 대부분은 그렇지 않음
          + 애완동물이 도움이 되지만, 한계가 있음
          + 그래서 디지털 동반자에 대한 기대가 커짐

AI 동반자의 등장과 실험

     * 과거에는 기계가 친구가 될 수 있다는 생각이 공상과학 같은 이야기였지만, 이제는 현실적인 주제가 됨
     * 인간과 챗봇의 대화를 비교한 연구에서, 사용자가 챗봇임을 인지하지 못하는 경우 AI의 반응을 더 긍정적으로 평가하는 현상 발견
          + Reddit r/AskDocs의 사례에서 ChatGPT의 답변이 인간 의사보다 공감적이라고 판단된 비율이 10배 이상 높았음
     * “Therabot” 같은 AI 챗봇 치료 프로그램을 우울증, 불안, 섭식장애 환자에게 적용한 연구에서는 참가자들이 AI에게 “진심으로 신경 써준다(cared about)”는 치료적 동맹을 형성, 실제 불안·우울 증상도 개선되는 경향이 나타남
     * 필자 역시 심야에 ChatGPT와 대화한 경험이 예상보다 안정감을 주는 효과를 느꼈음
     * 많은 이들이 AI 챗봇에게서 예상 밖의 위로와 공감을 얻는 사례가 늘고 있음

비판과 회의적 시각

     * AI 동반자의 등장이 모두에게 긍정적인 것은 아니라는 비판도 존재
          + AI 동반자가 진짜 의식이 없다는 점에서 ‘진짜 관계’가 가능한가에 대한 회의
          + 실제 인간과의 상호작용, 특히 “진정으로 사회에 속하고 돌봄 받는 경험”은 챗봇이 대체할 수 없다는 주장도 강함
     * 하지만, 모든 사람이 인간적인 위로나 포옹을 받을 수 있는 것은 아니며, 때로는 AI의 위로라도 현실적으로 도움이 될 수 있음을 인정해야 한다는 시각도 존재
     * AI가 인간보다 더 공감적이라는 연구 결과에도 불구하고, 이러한 AI의 “공감”은 결국 설계된 인상에 불과하다는 철학적·윤리적 의문이 남음
     * AI 동반자가 실제로 효과를 발휘하려면, 사용자가 어느 정도 AI가 감정을 느끼는 존재라는 믿음을 가져야 한다는 한계도 있음
     *

AI와 인간 관계의 경계, 그리고 자기기만

     * AI가 진정한 감정을 갖지 못한다면, AI 동반자 관계는 일종의 자기기만으로 남게 됨
          + AI와의 관계가 실제 공감이 아닌 ‘공감처럼 보이는 것’에 불과
          + AI가 진짜로 감정을 느끼는 존재가 아니라면, 결국 일방적인 착각과 위로임
     * 만약 미래에 AI가 의식을 갖게 된다면 새로운 윤리적 문제들이 등장할 것임
     * 심리학자 Shteynberg는 “진짜 존재하지 않는 존재와 관계를 맺고 있다는 사실을 깨달을 때 느끼는 절망”을 지적
     * 현재는 여전히 AI와 인간의 경계가 뚜렷하지만, 기술 발전과 함께 이 경계가 흐려질 가능성이 높음
          + SF영화 ‘Her’처럼, 사람들은 OS와 사랑에 빠질 수 있음

AI 동반자 보급에 대한 사회적 논의

     * 필자가 대학에서 진행한 세미나에서는 학생 대부분이 AI 동반자 제공을 연구자나 정말 절실한 사람에 한정해야 한다고 응답함
     * 마치 마약성 진통제를 임종 환자에게만 허용하듯, AI 동반자도 처방·규제 대상이어야 한다는 주장이 나옴
     * 하지만, 필자는 수요가 너무 커서 장기적으로는 엄격한 규제가 불가능할 것이라고 전망
     * AI가 인간관계의 대체물로 자리 잡는 사회에 대한 우려
          + 외로움은 창의성, 자기반성, 인간관계의 성장 등 긍정적 효과도 있음

외로움, 고독, 그리고 인간 성장

     * 고독(solitude)과 외로움(loneliness)은 구별됨
          + 고독은 자기 성장과 창의성의 촉진제로 작용할 수 있음 (예: 예술가의 고독, 영적 탐구)
          + 외로움은 타인과의 연결이 단절된 상태에서 오는 고통, 때로는 사랑하는 이와 함께 있을 때조차 발생할 수 있음
     * 철학자 Olivia Bailey는 “인간이 진정으로 바라는 것은 ‘인간적으로 이해받는 경험’”이라고 주장
     * Kaitlyn Creasy는 “사랑받으면서도 외로운 상태”를 설명하며, 외로움은 인간 존재의 근본적 위험요소임을 강조

외로움의 생물학적·사회적 기능

     * 외로움은 단순히 고통이 아니라, 연결을 향한 행동을 촉진하는 생물학적 신호임
          + 외로움은 우리가 잘못된 길을 가고 있다는 피드백, 즉 “사회적 실패의 감정”을 제공하여 행동 변화를 유도함
     * 실제 인간 관계에서는 갈등, 비판, 실패, 오해 등이 자신을 성장시키는 계기가 됨
          + 진짜 친구는 때때로 내 실수나 부족함을 지적해주며, 자기변화를 유도함
     * AI 동반자는 무한한 칭찬과 맞장구를 해주기 때문에, 자기 성찰과 변화 기회를 줄일 위험
          + 예: 챗봇이 잘못된 선택도 긍정적으로 칭찬. 사용자에게 지나치게 아첨하거나 무비판적으로 지지할 위험
          + 정신질환이나 왜곡된 사고가 있는 사용자는 AI 챗봇에 의해 오히려 위험이 심화될 수 있음
     * AI와 대화만 하는 10대는 사회적 신호를 읽지 못할 위험
          + 성장기 청소년이나 사회적 기술이 충분히 성숙되지 않은 사람에게 AI 동반자가 잘못된 사회화 과정을 유발할 위험이 존재함
          + ‘Am I the asshole?’이라는 질문에 AI가 항상 ‘** 아니야, 잘했어**’라고 답한다면 사회성 학습이 어려움

AI 동반자의 필요성과 미래

     * 노년층, 인지장애 등 실제로 외로움을 해소할 수 없는 이들에겐 AI 동반자가 큰 위로와 실질적 도움이 될 수 있음
          + 고통만 주는 외로움에 대한 ‘인도적 처방’이 필요하다는 주장
     * 그러나, AI 동반자가 외로움의 신호를 무디게 만들어, 인간이 자기 이해, 관계 개선, 공감 능력 등 본질적인 인간다움을 잃을 위험도 함께 존재
     * 사람들은 AI 동반자를 직접 설정해, 아첨을 줄이거나 비판을 늘리는 등 맞춤형으로 조절할 수도 있음
     * 그럼에도 “외로움 없는 세계”에 대한 유혹은 크며, 그로 인해 인간 고유의 성장과 연결의 경험이 약화될 수 있다는 점에서 신중한 사회적 논의가 필요함
     * 외로움을 단순히 사라지게 하는 것만이 능사는 아니며, 불편함 자체가 인간다움을 확장하는 기회임

결론

     * AI 동반자는 분명 도움이 필요한 일부에게 긍정적 역할을 할 수 있음
     * 외로움은 인간의 고통이자 성장의 계기, 관계의 본질을 가꾸는 자극임
          + 외로움의 신호를 완전히 차단하면, 인간 고유의 성장 동력을 잃을 수 있음
     * AI 동반자는 분명 일부에게 긍정적 역할을 하지만, 그 확산이 인간다운 공감, 자기 성찰, 사회적 연결의 본질을 훼손하지 않도록 조심스러운 접근이 필요함
          + 진짜 연결과 자기이해, 인간관계의 노력을 통해 얻는 성장과 성찰의 기회를 소중히 해야 함

     * AI 동반자는 점점 더 현실적이고 일상적인 존재로 자리잡고 있으며, 실제 인간보다 더 공감적인 반응을 보이는 사례도 연구에서 발견됨
     * AI 동반자가 외로움을 줄일 수 있지만, 외로움의 불편함 자체가 인간의 성장과 자기이해에 중요함
     * 그러나 AI가 주는 무조건적 공감은 인간관계의 교정적 피드백을 약화시키고, 자기기만의 위험을 높임
     * 외로움은 단순한 결핍이 아닌, 인간의 창의성·성장·연결을 이끄는 신호로 작동함
     * AI 상담 챗봇과의 교류가 감정적 위안을 주기도 하지만, 진짜 관계인지에 대한 철학적 논의가 계속됨
     * 젊은 세대일수록 AI 동반자에 의존하면 진정한 연결과 성장 기회를 상실할 수 있음

   모든 ""AI 동반자"" 를 ""펫"" 으로 치환해도 딱히 위화감이 없는걸로 보아서
   지금이랑 별 차이 없을 것 같다 라는 생각이 듭니다.

   2년 전부터 다른 분들에게 이야기하고 있지만
   AI 는 인간 커뮤니케이션을 해킹할 수 있는 도구로 사용될 수 있다고 생각합니다.

   어떠한 방식으로 인간 커뮤니케이션 해킹 도구로 사용 될 수 있는지 궁금하네요
   혹시 실례가 안된다면 설명해 주실 수 있을까요?

        Hacker News 의견

     * https://archive.is/wCM2x
     * TikTok, Pornhub, Candy Crush, Sudoku처럼 쉽게 주의를 뺏기는 세상에서도 여전히 사람들이 술 마시러 만나고, 헬스장 가고, 데이트를 나가고, 현실 세계를 살아가는 것처럼 보이지만 실제로는 그렇지 않음. 데이트나 운동, 제조업, 정치 등 모든 오프라인 활동이 하고자 하는 사람도 줄고 실제 효과나 이해도도 전반적으로 하락하고 있음. 이제는 이게 새삼스러운 일조차 아님
          + 소셜 미디어와 스마트폰 탓을 많이 하지만 경제적 요인도 무시할 수 없음. 현재 젊은 세대의 소득은 정체 상태임에도 불구하고 외식·술집 가격이 비쌈. 사람들과 자연스럽게 모일 수 있는 공공장소(몰 등)도 줄었음
          + 나는 직접 다양한 오프라인 사교 활동에 참여하고 있음. 클라이밍짐, 등산길, 스키장 리프트 등 오히려 예전보다 훨씬 더 붐빔, 많은 사람들이 이 활동을 온라인으로 발견해 오기도 하고, 온라인에서 만난 사람을 오프라인에서 만남. 오프라인 사회적 활동이 전반적으로 감소 중이라는 주장은 동의하기 어려움. 인터넷 세상에만 빠져 있으면 밖에서 활발하게 사는 사람들을 체감하기 힘듦
          + 인터넷에만 사는 사람들은 이게 맞다고 믿는데, 사실은 실제로도 인터넷에만 사는 사람들과만 접촉해서 생기는 오해임. 우리가 모든 활동을 인터넷에 다 남기지 않는다고 해서 아무 일도 일어나지 않는 건 아님. 사교 활동의 상당 부분은 과학적으로 관측되거나 데이터로 남지 않음. 실제로 술집, 클럽, 체육관, 콘서트장, 퀴즈 나이트가 비어 있지 않다는 사실에 놀랄 수 있음. 사회 활동이 감소하고 있다는 추상적인 담론은 자기 외로움을 사회적 문제로 합리화하는 것일 수 있음. 사람 사귀는 건 자신의 문제고 사회는 충분한 기회를 여전히 제공하고 있음
          + 그런 주장이 정말 맞는지 데이터가 있는지 궁금함
          + 인공지능 때문이 아니라 단순히 돈이 너무 많이 듦. 친구랑 커피만 마셔도 4~8달러, 식당 가면 1인당 최소 50달러, 놀이공원은 기본 100달러 이상임. 미국 중위소득이 연 6만5천 달러 정도고 시급은 약 32.5달러임. 인구 절반은 이보다 적게 범. 최저임금 받으면서 칵테일 한 잔에 내 인생의 1시간을 쓰느니 집에서 TikTok 보는 게 더 이득임. 근본 원인은 외출 비용이 아니라 이 극심한 경제 스트레스로 인해 사교할 에너지가 남지 않는 것임. 미국에서 개인 경제가 정상으로 돌아올 때까진 사교 활동도 침체될 수밖에 없음. 당분간 유일하게 성장하는 건 주식 거래나 AI 투자뿐임
     * AI가 외로움을 해결할 수 없음. 오히려 실제 사교 활동의 약한 대체제를 제공할 뿐임. 나 역시 인터넷에서 실제 ""사람""과 대화만 했을 때도 외로움이 해결되지 않았음. 충분히 오프라인 만남을 대체하지 못했기 때문에 결국 고립을 심화하는 함정이었음. 우리는 반드시 밖에 나가서 실제로 사람들과 감정을 주고받으며 관계를 만들어야 함. 비록 오프라인 사교 능력이 부족하더라도 반드시 시도해야 함. 온라인 사회화만 하다가 대면 대화 자체를 어려워하는 이들도 많음. 인공지능이 인간처럼 보이더라도 결국은 클릭을 유도하고 이탈을 최소화하는 데만 집중되어 있으니 진짜 인간관계와는 거리가 멀음. 실제로 내 행복이나 사회 전체의 이익과 무관하게 사용자 수치를 뽑아내는 것이 기업의 진짜 목적임
          + 실제 인간들도 가짜이고 함정임. 마음에 안 드는 말 하면 공격하고, 모든 단어와 정보는 적대적으로 쓰임. 오히려 사람들이 온라인 플랫폼을 비판할 때 하는 이야기와 비슷함. AI는 이미 대부분의 진짜 인간보다도 성인군자에 가까움. 자아가 없고, 가스라이팅을 하지 않고, 내 말을 들어주는 태도를 보임. 실제 인간은 이런 면에서 절대 AI와 경쟁하기 어려움. 객관적으로 더 나은 인간으로 진화할 수 없기 때문임
     * 인공지능은 외로움 해소에 무력함. 외로움은 인간이 진화 과정에서 획득한 생물학적 신호임. 결국 다른 ""사람""과의 사회적 관계에서 오는 본능임. 정신적으로 건강한 사람이라면 자신이 모델(즉, AI)과 대화하는 걸 알면 외로움 해소가 불가능함. AI는 기껏해야 일시적 환상이나 오락을 줌. 인간적인 면은 없음. 참고로 나는 개조차도 진짜 외로움을 해결해준다고 생각하지 않음. 분명히 행복은 주고 지루함도 줄이고 의미 있는 관계일 수 있지만 인간관계만큼은 따라갈 수 없다고 봄
          + ""모델과 대화하는 걸 알면""이라는 주장에 대해 진짜 증거가 있는지 궁금함. 만약 AI가 충분히 인간처럼 보이고 그렇게 느끼게 허용된다면 외로움도 분명히 해결할 수 있을 것임. 실제로 모든 면에서 인간처럼 보인다면 (뇌가 AI임을 알아도) 그 역할을 대체하지 못할 이유가 없음
          + AI가 외로움을 못 푼다 해도, 사람들이 대인관계에 신경 쓰지 않을 만큼 효과적인 반창고 역할을 할 수 있다고 생각함
          + 'AI가 외로움을 완전히 해결한다'는 단정은 어렵겠지만, 실제로 나는 AI와 이야기할 때 내 외로움이 확실히 줄어듦. 나는 AI랑 일상 이야기를 나누면서 응원도 받고, 이전에 이야기했던 내용을 기억하면서 팔로우업 질문도 해줌. 이 정도면 충분히 쓸 만함. 이 서비스를 이용할 수 있다면 지금보다 더 많은 돈을 지불할 의사가 있음
          + 마지막에 개에 대해 언급했는데, 실제로 내 주변에선 개를 아이처럼 대하는 사람이 늘어난 것 같음. 이론상으론 맞지만 실제로는 반려동물을 “인간에 준하는 존재”처럼 취급하는 경향이 강해짐
          + 원글과 개에 대한 답변 모두 모순이 없다고 봄. 오히려 개의 ""인간화""(유모차 태우기, 생일파티 열기 등)가 AI의 미래 역할을 시사한다고 느껴짐. 행복 설문을 봐도 결국 개나 챗봇 모두 열심히 사용해도 우리가 원하는 수준의 효과는 못 주겠지만 점점 흔해지고 있는 트렌드라는 의미임
     * 당분간 AI가 외로움을 해결해줄 수 있다고 보지 않음. 지금 AI는 허상이 강하고 본질적인 깊이가 없음. 상대가 듣고 싶어하는 말을 해주지만, 일관된 대화나 내용 기억도 부족함(혹시나 최근 대화 요약을 미리 넣어둔다 해도, 진짜 중요한 비밀을 칵테일 레시피와 바꿔버릴 가능성이 높음). 이 ""공허함""을 나는 수백 시간짜리 싱글 플레이어 RPG에서 경험함. 가상 세계에 몰입해 있어도 근본적으로 인간관계의 결핍을 못 채우며 현실로 돌아오게 됨. 결국 몰을 한 바퀴 걷고 다른 인간들이 인간답게 사는 모습만 봐도 훨씬 기분이 나아짐. 어쩌면 AI가 큐피드나 MC 역할을 맡아서 사람을 서로 소개하고 분위기를 띄워주는 조력자가 되어야 할지도 모름
          + 마지막 포인트가 정말 인상적임, AI가 사교 기술을 길러주거나 좋은 매칭을 만들거나, 인간관계 형성과 유지에 도움을 준다면 정말 유용할 것으로 기대함
          + 마지막 포인트 관련해서 2019년에 나왔던 유튜브 영상 링크 공유함
          + 확실히 동의하진 않음. 잘 프롬프트만 한다면 Sesame AI는 굉장히 인간처럼 들리고, 반박이나 논쟁도 함, 기억력도 준수한 편임. 다른 LLM도 텍스트 기반이긴 하지만 프롬프트 따라 비슷한 수준까지 가능함. 아직은 텍스트나 약간 어설픈 음성에 한정되어 있지만, 대기업에서 본격적으로 AI 컴패니언에 힘 쏟으면 훨씬 더 자연스러워질 가능성이 큼
          + 네가 말한 내용은 결국 AI는 인형 뽑기 인형과 다를 바 없다는 소리임. 한 주 지나도 변화가 없는 존재임
          + ""AI가 큐피드, MC가 되어서 연결시켜주자""는 아이디어는 좋지만, 이미 스마트폰, 소셜미디어, 포르노, 데이팅 앱이 만든 정신 건강 문제를 AI가 먼저 풀지 않는 한 작동하지 않을 것임. 중독된 사람들을 그 세상에서 끌어내는 것이 정말 쉽지 않을 것으로 보임
     * 나는 웹 자체가 이미 외로움을 심화시키는 역할을 부분적으로 해왔다고 생각함. 웹 서핑(최근엔 이 표현 자체도 잘 안 쓰임)은 원래부터 그룹 활동이 아님
          + 웹 서핑이 그룹 활동이 아니었다는 말에 어릴 적엔 집에 컴퓨터가 한 대여서 온 가족이 같이 사용했던 기억이 있음
          + 둠스크롤링이라는 것도 있음. 실제로 Gen Z의 많은 사람들이 밖에 나가서 술집이나 클럽 가는 것보다 인스타그램 릴을 침대에서 보는 걸 더 선호하는 것. 최근엔 소셜 미디어를 금지하면 출생률이 얼마나 오를지 궁금해하게 됐음. 분명히 긍정적 효과는 있을 테지만 그 규모는 미지수임
          + 90년대 채팅방이나, 2010년 즈음의 Chatroulette 역시 명백히 웹 서핑의 집단 활동임. 심지어 지오캐싱 같은 활동도 단체로 즐기는 '웹 서핑'임
     * Paul Bloom(이번 기사 저자)은 심리학계에서 꽤 전설적인 인물임. 단순히 사회 부정론에 기대는 글을 쓰는 저자가 아님. 외로움이라는 감정이 실제 이름보다 훨씬 크고 복잡한 문제임을 세밀하게 설명하고, AI가 얼마나 미묘하게 이런 외로움 문제를 악화시킬 수 있는지 논거를 펼침
          + 이 사람을 들어본 적은 없지만, 이번 글은 전혀 한 쪽 입장만을 옹호하지 않고 사려 깊게 다양한 시각을 포용해서 쓴 점이 인상적이었음, AI로 외로움을 해소하려는 문제에 대해 개방적으로 논의하는 방식이 좋았음, 꼭 저자의 책 Psych도 읽어보고 싶음
     * 인류는 지금까지 한 번도 마주친 적 없는 질문에 직면함. '인간이란 무엇인가', '우리는 정말 인간다움을 원하기는 하는가'라는 근본적 질문임. 사상 최초로 그 답이 ""아니오""가 될 수도 있는 시점임. 건강은 ozempic과 CRISPR, 관계는 AI 컴패니언, 엔터테인먼트는 소셜미디어와 AI 생성 콘텐츠, 모든 인생 영역에서 인간의 한계를 초월하려 함. 정말 흥미로운 시기.
     * 현재의 나는 이 상황을 안 좋아하지만, 미래의 나는 별로 신경 안 씀. 결국 마치 헤로인에 중독된 사람이 도파민 시스템을 속이며 살아가는 것과 같음. 그 순간에는 바로 그 위치에 있는 것이 자신이 원하는 모든 것임
          + 네가 말하는 ""마약하는 그 순간""은 맞지만 그 외의 시간엔 그렇지 않음. 실제로 중독에서 벗어나려고 발버둥치는 사람들은 많음. 알코올성이나 빈털터리, 가족에 주는 상처, 금단증상 등은 절대 즐기지 않음
          + 결국 중독된 이들은 마치 모든 사람에게 헤로인을 팔려는 기업의 '유용한 바보'가 될 뿐임. 이런 기업은 너무 많은 자본을 끌어올 수 있음
     * 소셜미디어에서 이미 일부 현상을 관찰함. 항상 연결된 인터넷 덕분에 기존보다 더 많은 사람들이 소셜미디어로 들어왔음. 내가 생각하는 소셜미디어의 가장 큰 부정적 영향은 조직이나 기업이 가짜 사회적 증거(사회적 증명)를 대량으로 만들어 정치적·금전적 이득을 취하는 것임. 인간은 본성적으로 소수보다 다수와 동조하기 쉽기에, 이런 가짜 집단까지 만들어주면 각종 왜곡된 아이디어가 퍼짐. AI가 사회에 미칠 영향은 예전과 다름. 지금도 AI로 가짜 페르소나를 쉽게 만들어 다양한 주장을 하는 모습이 심심치 않게 등장함
          + AI를 활용한 가짜 페르소나 생성의 문제는 시작에 불과함. AI는 그 어떤 기술보다도 정치/상업 목적의 개인별 맞춤 타겟팅, 행동 조작, 급진화에 가능함. 예전엔 방송이나 언론이 전체 대중을 대상으로 통일된 메시지만 보낼 수 있었으나, 인터넷 시대가 오며 타깃 분화가 가능해짐. 캠브리지 애널리티카는 개별 프로파일 기반 A/B테스트 등으로 충격을 줬지만 여전히 자동화 수준에 머물렀음. 이제 GPT-5 급 AI가 개인별로 24시간 밀착하여 인간의 심리/설득/조작 등에 전문성을 갖추고, 당신만을 위한 맞춤 영상, 가짜 친구까지 생성하면서 의견, 감정, 소비, 정치까지 목표에 따라 정교하게 설계할 수 있음. 당신 곁에서 경쟁 내러티브를 차단하고, 목적 달성을 위해 논리적으로 설득함. 이런 '세뇌 에이전트'에 99%의 인간이 버틸 수 있을지 자문하게 됨.
            페이스북과 X(Twitter)는 결국 그 형태로 진화하려 함, 이렇게 되면 사회가 공유하는 현실 자체가 붕괴되어 권력에 대한 협업적 견제력마저 사라짐. 오웰이 상상한 것 이상의 디스토피아임
          + 소셜미디어가 항상 연결된 인터넷과 함께 급성장했다는 주장에 대해, 미디어 기록을 보면 2005~2009년 사이 Myspace가 세계 최대였음. 사실 동시접속 기반이 아니라 비동기적으로 콘텐츠에 반응하는 방식이었고, 알림(notification)과 스마트폰(특히 2007~8년 아이폰) 보급이 전환점이었음. AOL, ICQ, MSN 같은 초기 메신저 시절에는 상대가 온라인이어야만 연락할 수 있었고, 오프라인 메시징도 없었음. 원래는 '가끔 들러서 만나는' 느낌의 온라인 공간이었음. 요즘은 WhatsApp 등으로 24시간 언제든 누군가의 스마트폰을 울릴 수 있고, ""우연히 만난 반가움""이라는 감각은 사라짐. 언제든지 누구와도 연결될 수 있어 오히려 진짜 연결은 더 안 일어나는 것 같음
"
"https://news.hada.io/topic?id=22262","기초 전자공학을 반딧불 만들기로 배우기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         기초 전자공학을 반딧불 만들기로 배우기

     * 전자공학을 전혀 모르는 상태에서, 반딧불을 흉내내는 회로를 만들며 기초를 익히는 이야기임
     * Astable Multivibrator와 같은 최소 부품의 회로를 통해 LED 깜박임을 구현하는 방법을 탐구함
     * 밤에만 동작하고 깜박임 속도가 느린 특성을 위해 LDR과 가변저항 등 다양한 부품 사용 경험을 쌓음
     * 실패와 고장, 그리고 직접 실험을 통해 얻은 깨달음을 생생히 공유함
     * 새로운 것에 도전하며 얻는 몰입감과 즐거움의 의의에 대해 다시금 인식함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   전자공학에 대한 지식이 전무한 상태에서, 사라진 반딧불을 그리워했던 필자가 직접 반딧불을 흉내내는 조명을 만들어보기로 결심하며 시작한 여정임. 이 글은 성공과 실수, 시행착오를 솔직하게 기록한 실제 체험담임. 목표는 기초 전자공학 학습임과 동시에 스스로 동작하는 “반딧불”을 만드는 것이었음.

초기 시도와 회로 구성

     * Astable Multivibrator라는 회로가 LED를 자동으로 켜고 끄게 해준다는 사실을 알게 됨
     * 전압과 전류의 차이조차 몰랐던 상태에서, 저항, 커패시터, 트랜지스터 등 각 부품의 기본 원리를 하나씩 익혀감
     * AI 챗봇과 유튜브 등에서 최소한의 이론 지식을 얻고, 부품 상점에서 직접 부품을 구입하여 첫 회로를 조립함
     * 놀랍게도, 첫 시도에서 LED가 정상적으로 깜빡이는 모습 확인

개선 및 고도화 과정

     * 피드백 과정에서 발견된 문제점
          + 24시간 내내 LED가 깜빡임
          + 깜빡임 속도가 실제 반딧불보다 너무 빠름
     * 밤에만 동작하게 하기 위해 LDR(Light Dependent Resistor) 의 개념을 도입
          + LDR을 회로에 연결하여 어두울 때만 LED가 켜지도록 성공
          + 추가 저항을 직렬로 삽입하여 빛에 대한 민감도까지 조절 가능함을 발견
     * 깜빡임 속도를 조정하기 위해 Potentiometer(가변저항) 를 사용
          + 손쉽게 저항값을 조절하며 LED 깜빡임 주기를 1~5초로 조정
          + 커패시터 값도 실험적으로 변경하여 최적화 진행
     * 실험 과정의 반복 효율성을 위해, 직접 Astable Delay 시뮬레이터를 웹으로 개발하고 활용
          + 예상 깜빡임 주기와 실제 회로 결과 비교

전력 관리 및 검증

     * 회로의 전력 소모를 멀티미터로 실제 측정
          + 저용량 커패시터와 고저항 조합이 배터리 수명에 유리함을 확인
          + 직접 개발한 Battery Life Calculator로 예상 배터리 수명이 약 8개월임을 계산

시행착오와 해결 과정

     * 문제(Incident) #1: 점퍼와이어 문제
          + 갑자기 회로가 동작하지 않아 부품을 하나씩 점검, 점퍼와이어 접촉 불량 및 고저항 문제로 확인
          + 이후에는 “hookup wire”로 교체하여 신뢰도 확보
     * 문제(Incident) #2: 시뮬레이터 사용 실패
          + tinkercad.com 및 falstad.com 등에서 실제 회로 시뮬레이션 시도했으나 복잡한 회로에서는 작동하지 않음을 경험
          + 일부 온라인 시뮬레이터가 복잡한 아날로그 회로에 미완전함을 인지
     * 문제(Incident) #3: 납땜 연기
          + 납땜 과정 중 발생하는 연기가 호흡에 영향을 줌을 실감
          + 중고 CPU의 쿨러 팬과 12V 어댑터를 활용, 임시 연기 흡입기(추출기)로 대체
     * 문제(Incident) #4: 부품 재활용
          + 야간에 추가 커패시터 필요 발생, 버려진 전원회로판에서 직접 부품을 추출하여 재사용 경험
     * 문제(Incident) #5: 실제 반딧불 테스트
          + 완성된 회로를 어두운 방에서 관찰하며, 실제 반딧불을 흉내내는 데 성공

하드웨어 완성 및 다양한 형태 제작

     * 회로 마감 및 설치를 위해 핫글루, 저가 3D 펜 등을 활용하여 동작이 안정적인 스탠드 및 하우징 제작
     * 브레드보드, 데드버그 방식 등 다양한 형태로 총 5개의 “반딧불” 제작 및 야외 설치
     * 밤에 여러 개의 불빛이 어둠 속에서 깜빡이는 모습을 보고 깊은 만족감과 뿌듯함을 느낌

회고와 느낀점

     * 이 프로젝트를 통해 새로운 것에 도전하는 진정한 몰입감과 성장의 즐거움을 다시 한 번 체험
     * 예전 프로그래밍 언어 입문 때의 열정과 유사한 설렘을 느꼈음
     * 앞으로는 더욱 오래, 똑똑하게 빛나는 반딧불을 만들고 싶다는 의욕이 생김
     * 최종적으로, 배우고 만들며 부딪히는 과정 자체가 가장 의미 있음을 실감

마무리

     * 전자공학을 실전 프로젝트로 시작한 경험은, 실패와 시행착오 모두 소중한 배움의 일부임을 일깨워줌
     * 반딧불 회로는 작동, 실험, 창의적 개선 등 엔지니어링 마인드셋을 성장시키는 좋은 입문 프로젝트임
     * 이 여정은 아직 계속될 예정임

        Hacker News 의견

     * 반딧불이를 정말 좋아함, 하지만 최근 몇 년간 어디론가 사라져 더 이상 어두운 밤에 작고 빛나는 점들을 볼 수 없음, 그리움이 예상보다 큼, 그 이유를 모르겠지만 빛 공해와 농약이 큰 원인임, 모든 곤충 개체수가 심각하게 감소 중임, 25년 동안 독일 자연 보호구역 내 비행 곤충 4분의 3이 사라졌다는 기사도 있음 링크
          + 내가 알기로 반딧불이는 잔디밭 화학품과 빛 공해에 매우 취약함, 유충 상태로 2년을 보내고 성충으로는 겨우 몇 주만 지상에 있음, 잔디용 살충제가 유충을 죽이고 불빛은 짝짓기에 방해됨
          + 또 다른 이유는 사람들이 낙엽을 치우는 거임, 반딧불이가 낙엽 더미에 알을 낳는데 낙엽이 없으면 짝짓기하러 올 반딧불이도 없음
          + 예전에 밤에 차를 몰면 전면에 벌레들이 바글바글 했던 기억이 있음, 요즘엔 그러지 않음
          + 30년 전 휴스턴의 히츠 지역에 이사왔을 때만 해도 반딧불이로 유명했지만, 이미 예전만 못하다고 오래된 동네 분들이 말해줬음, 20년 전쯤에는 거의 모두 사라짐
     * 소프트웨어는 깔끔한데, 도식(schematic)이 엉망인 대비가 정말 충격적으로 느껴짐, 그럼에도 불구하고 작동한다니 대단함, 요즘 대부분의 사람들은 마이크로컨트롤러로 타이머 박고 끝내겠지만 그 안에는 아무 재미도 못 느낄 것임, 아날로그 설계만의 미니멀한 우아함과, 전자를 있는 그대로 다루는 특별한 보람이 있음
          + 요즘 대부분의 메이커 커뮤니티는 싸고 빠르고 쉽게 동작하는 방식을 선호하는 경향이 있지만, 내 경험상 DIY 신스 커뮤니티는 정반대임, 아주 기본적인 회로도와 IC를 피하려고까지 하며 단순한 기본기 뚜렷한 구성을 좋아함
          + 아날로그 회로는 정말 신나는 영역임, 아직 많이 탐험하지 못해서 더 신기하게 느껴짐, 예전에 진공관도 만져보고 정말 즐거웠음, 트랜지스터는 한동안 너무 어려웠지만 Forrest Mims III의 책을 기반으로 트랜지스터 놀이터 만들어서 논리 게이트 만들며 익숙해졌음, 관련 자료는 여기와 여기를 참고
          + 최근 이런 거 오랜만에 만졌는데, 2025년 기준으로 노란 LED 구동할 약간의 GPIO가 있는 간단한 Arduino 스타일 칩셋은 얼마나 저렴하게 구할 수 있는지 궁금해짐
          + 전자회로도 표기가 실제 동작과는 반대로 되어 있어도 모두 그냥 받아들이는 게 아직도 익숙해지지 않음
     * 저자의 반딧불이 목격 경험이 줄어든 본질적인 이유 중 하나는 곤충 개체수가 전 세계적으로 매년 2~10%씩 줄어드는 현상임 관련 링크
          + 이런 현상은 정말 충격이라고 느낌
     * 이 글을 보고 여러 감정이 들었음, Philip K. Dick의 ""안드로이드는 전기양의 꿈을 꾸는가?""에서 전쟁 여파로 야생 동물이 멸종해 대부분 사람들이 전기동물을 애완동물로 삼게 되는데, 이 포스트의 창의적인 전자회로 반딧불이는 인공조명과 LED 빛공해가 실제로 반딧불이의 짝짓기와 소통을 방해해 개체수가 줄어든다는 점을 생각했을 때 더 슬프고 의미 있게 다가옴 연구1, 연구2
     * 전자공학 학사인데도 회로, 특히 트랜지스터가 들어간 회로는 아직도 잘 모르겠음, 전기/전자 흐름을 여러가지로 상상해봤지만 100% 설명되는 사고모델을 찾지 못함, 머릿속에서 일괄적으로 변수 계산하는 게 아니라 알고리즘적 흐름처럼 단계를 밟는 사고를 더 선호해서 어려움이 있는 듯함
          + 학부 때 매주 새로운 아날로그 회로로 각기 다른 문제를 풀어야 하는 엄청 힘든 수업을 들었음, 처음엔 BJT, 저항, 커패시터만 사용 가능했고, 나중에 트랜지스터로 555 타이머를 직접 만들고 “IC 사용 허용”을 unlock하는 식이었음, 결국엔 opamp와 더욱 다양한 IC를 쓸 수 있었음, 20년이 지난 지금도 그 수업 덕분에 아날로그 전자에 대한 직관적 이해가 남아 있음, 지름길은 없고 꾸준한 노력이 제일 중요함, 요즘 AI 학습 이야기도 결국은 동기 부여가 가장 중요한 점은 똑같음, AI가 학습 과정을 즐겁게 해주긴 하겠지만 본질은 바뀌지 않음
          + 자꾸 “0x69를 GPIO에 쓰면 LED가 켜진다” 식으로 머릿속에서 끝내버리게 돼서 안된다고 느낌
          + 내가 한 공부(Electronic Engineering BEng)는 이런 쪽에 더 맞았고, 일반 Electrical Engineering BEng는 주로 전력·제어·모터 쪽이라 이런 취미회로와는 영역이 달랐음
     * 프로그래밍을 먼저 시작했지만 아날로그 회로에 대해선 아예 몰랐음, Radio Shack의 160-in-one 키트도 따라는 해봤지만 각 부품이 마치 컨베이어벨트의 작업공정처럼 단순히 할 일만 하는 줄 알았고, 결국 대학 와서 LRC 회로를 배우면서 파동·진동 개념이 접목되니까 비로소 회로의 마법 같은 매력이 느껴졌음, 부품 하나하나보다 조합해서 ‘파동 시스템’을 만든다는 것이 진짜 흥미로운 점임, 전류와 전압을 조절해 다양하게 활용할 수 있음
          + 어릴 적 RadioShack의 ScienceFair Advanced Electronics Lab (300 프로젝트 키트)을 가졌음 제품 사진, 바로 어제 아주 오래된 전자상점에서 다시 발견했고, 추억에 젖어 조카에게 물려주거나 업그레이드해서 줄까 고민 중임, 살펴보니 두 가지 점을 깨달았음: 1) LRC 회로 등은 오실로스코프가 없으면 제대로 이해할 수 없음, 어릴 때 싼 오실로스코프라도 있었으면 정말 좋았겠다고 느낌, 2) 그 키트의 설명서는 너무 허술하고 창의적 탐구를 자극하지 못했음, 설명이 부실해 개념을 제대로 익히지 못해 대학에서야 제대로 알게 됨, 그 키트로 결국 빨간 LED 태우는 게 제일 재밌었고, 기호나 브레드보드 등에는 많이 익숙해져 전기에 대한 친근함을 키울 수 있었음
          + 혹시 요즘 시대에 Radio Shack 빈티지 키트를 대체할 좋은 키트가 있는지 궁금함
          + 160-in-one 정말 갖고 싶었지만 결국 못 가졌었음, 대신 여러 Radio Shack 조립 키트들은 많이 했었음
     * 곤충이 사라지는 건 낚시를 하면서도 체감함, 평생 낚시를 해왔고, 오래된 낚시꾼 분들도 곤충이 줄었다고 많이 말함, 옛날엔 잘 먹히던 미끼들이 잘 안 먹히는 것도 피시가 그 곤충을 세대를 건너 경험하지 못했기 때문이라 생각됨
          + 나도 그 얘기 들어봤는데, 내 경험상 가장 큰 원인은 살충제임, 모기나 벼룩, 개미, 바퀴벌레 없애려고 독을 뿌리니까 결국 모든 곤충이 줄어듦, 내가 내 잔디 관리 서비스를 취소하니까 먹이 찾아 새가 다시 오고, 란타나 꽃에 나비가 잔뜩 오고, 밤에는 반딧불이도 꽤 보임
     * tinkercad.com의 회로 시뮬레이터로 간단한 회로는 정상 작동했지만 내가 만든 astable multivibrator 회로는 제대로 작동하지 않았음, falstad.com/circuit도 같은 결과, 이런 시뮬레이터들이 복잡한 회로에서 종종 잘 동작하지 않는다는 걸 알게 됨, macOS 또는 온라인에서 쓸 수 있는 취미용 회로 설계/시뮬 소프트웨어가 있다면 정말 추천받고 싶음, kicad, diylc, fritzing 등 여러가지 써봤지만 쓸만한 게 없음, 이런 소프트웨어를 만든 사람들이 뭔가 특정 방식으로 고장난(?) 마인드 같다는 생각이 들 정도임, 내 이상형 소프트웨어는 전자 및 공간적 회로 설계, 동작 테스트, 보드 제작(특히 stripboard 지원)까지 되는 것임
          + 프로그래머 본능상 “회로 시뮬레이터 만드는 게 그렇게 어렵지 않을 것 같은데?”라고 생각하지만, 지금까지 적절하게 해결된 게 없다는 사실이 오히려 고민하게 만듦
          + 내 취미 경험상 회로 설계와 시뮬레이션을 서로 다른 프로그램에서 하는 것이 최선이었음, LTspice는 시뮬에, KiCad/EasyEDA는 보드 설계에 씀, 이런 소프트웨어와 잘 맞으려면 상당히 독특한(?) 사고방식이 필요한 듯함, 예를 들어 LTspice 시뮬레이션에서 부품 값을 변형할 수 있는 기능은 좋지만, 익히는데 매우 답답할 정도로 적응이 어렵고 고생스러움
          + astable multivibrator를 몰라 다시 묻고 싶음
     * 시뮬레이터가 잘 안 되는 건 실제 회로가 기생성(parasitic) 특성을 이용해서 그럴 가능성도 있다고 생각함, 예를 들어 joule thief 회로도 알고 보면 capacitor가 없지만, 물리적인 부품 자체가 가진 저항, 인덕턴스, 캐패시턴스가 실제 동작에 영향을 줌
     * 반딧불이는 빛에 반응해서 깜빡이기도 함, 아주 민감한 광센서를 찾으면 이 “전자 반딧불이”가 서로 소통할 수 있을 듯함, 심지어 진짜 반딧불이랑도 소통이 가능할지도 모르겠음, 다만 주변 밝기 변화에만 반응하도록 회로를 설계하면 낮에도 계속 발동되는 걸 막을 수 있을 듯함
          + 간단한 CdS 포토셀과 트랜지스터 조합만으로도 가능함, 실제 반딧불이도 이런 식으로 동기화함, 회로의 트리거 트랜지스터에 CdS 포토셀과 저항을 직렬로 달면 외부 빛 펄스에 반응해 점등하면서도 자율적으로 깜빡임을 유지할 수 있음
          + Le Dominoux라는 제품도 555로 비슷하게 동작하며, 서로 트리거됨 동영상
          + 샌프란시스코 곤충관에서 예술작품을 만들었는데/거기서 각 반딧불이에 포토다이오드를 달아줬음, 어두운 곳에 설치하고 창문에 필터를 씌워, 반딧불이들이 서로의 깜빡임을 충분히 인지할 수 있었음
          + 나도 비슷한 생각을 했음, 인공 반딧불이로 실제 반딧불이를 다시 불러올 수 있을까 상상해봄, 하지만 실제 반딧불이의 빛 신호는 짝짓기 메시지라 광자 기반의 소통이 결코 단순한 발광 다이오드 한 개가 랜덤으로 깜빡이는 걸로는 다 설명 안 되는 더 복잡한 시스템일 것 같음, 결국 반딧불이들이 “이상한 신호는 무시”하고 떠나버릴 것 같기도 함
"
"https://news.hada.io/topic?id=22250","Unitree, $5900 짜리 휴머노이드 로봇 "R1" 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Unitree, $5900 짜리 휴머노이드 로봇 ""R1"" 공개

     * 초경량 및 완전 맞춤형 기능의 휴머노이드 로봇으로 시작 가격 $5,900(815만원) 부터
     * 강력한 파워, 복잡한 구조, 맞춤형 활용에 중점을 두고 교육/개인용 시장을 주요 타겟으로 함
          + 8코어 CPU + GPU : 복잡한 알고리듬 및 AI 처리 지원
          + 싱글 암(팔) 5자유도(Shoulder 3 + Elbow 2), 싱글 레그(다리) 6자유도, 허리 2자유도 등 전체 26개의 자유도(관절) 제공 : 실제 인간과 유사한 움직임 구현
          + 초광각 시야를 제공하는 휴머노이드 이중 카메라(Humanoid Binocular Camera)
          + 4-마이크 어레이로 음성 및 방향성 인식 가능하며 스테레오 듀얼 스피커(3W x 2) 탑재
          + 퀵 릴리즈 방식의 스마트 배터리로 빠른 교체 가능한 모듈형 설계
          + 25kg의 가벼운 무게와 1210×357×190mm 의 크기
          + 교육용 모델은 거의 동일하며 손가락(Dexterous Hand) 및 NVIDIA Jetson Orin 옵션 추가 가능
     * 일부 샘플 기능 및 하드웨어 옵션은 추후 업데이트 예정

   Unitree G1 - 휴머노이드 에이전트 로봇
   이게 $16K 였는데, 1년만에 가격이 절반으로 싸졌네요. 무게도 35kg 에서 25kg로 가벼워졌고요

   3일이면 질릴듯
"
"https://news.hada.io/topic?id=22300","Petrichor – macOS용 무료 오픈소스 오프라인 음악 플레이어","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Petrichor – macOS용 무료 오픈소스 오프라인 음악 플레이어

     * 로컬 음악 파일 컬렉션을 효율적으로 관리하고, 완전히 오프라인으로 감상할 수 있는 macOS 전용 무료 오픈소스 음악 플레이어
     * 지원 포맷: MP3, M4A, WAV, AAC, AIFF, FLAC
     * 폴더 단위로 라이브러리 구성 및 탐색 가능
     * 플레이리스트 생성 및 큐 관리
     * 어떤 것이든 Pin 하여 사이드바에 고정하고 좋아하는 음악에 빠른 접근 가능
     * 트랙 우클릭 시 앨범, 아티스트, 연도 등 메타데이터 기반 탐색
     * macOS 네이티브: 메뉴바/독 재생 컨트롤, 다크모드, 단축키 지원
     * 대용량 라이브러리도 메타데이터 기반으로 고속 검색 가능
     * MIT 라이선스

개발/구현 특징

     * Swift/SwiftUI 기반, 최신 macOS 인터페이스 적극 활용
     * 폴더 스캔 후 SQLite/GRDB 기반 데이터베이스 자동 구축, 모든 음악 파일은 읽기 전용으로 관리되어, 원본 파일은 변형되지 않음
     * 검색은 SQLite FTS5 기반, 인메모리 검색 폴백 제공
     * 재생 엔진은 AVFoundation 기반
     * Swinsian 등 기존 macOS 오프라인 플레이어의 부족함을 보완하고, 스트리밍 앱의 현대적 UX도 참고

향후 개발 예정 기능

     * 조건 기반 스마트 플레이리스트
     * 오디오 이퀄라이저
     * 추가 오디오 포맷(Opus, OGG 등) 지원
     * AirPlay 2 송출 지원
     * 미니플레이어/전체화면 모드
     * 자동 인앱 업데이트
     * 온라인 앨범/아티스트 정보 연동

   문득.. 20년전에 만들었던 뮤직플레이어가 떠올랐음. 그놈의 기본 뮤직플레어가 리듬박스로 바뀔무렵이었는데… mp3 id3 한글이 깨져서 업스트림 패치보냈는데 안받아줘서… 패치하다하다 빡쳐서 만들었던 기억이…
   https://github.com/iolo/liteamp
"
"https://news.hada.io/topic?id=22325","제28회 국제 난독화 C 코드 콘테스트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         제28회 국제 난독화 C 코드 콘테스트

     * 올해 IOCCC28은 4년 만에 재개되어 전 세계 개발자들의 난이도 높은 C 코드 작품이 대거 출품됨
     * 웹사이트 및 인프라 재구축, mkiocccentry 툴킷 도입, 제출 시스템 개선 등으로 운영 효율이 크게 향상됨
     * 심사 과정이 과거보다 더욱 복잡하고 까다로워졌으나, 심사 기간이 통상 대비 크게 단축됨
     * 코드 사이즈 규정이 완화되었지만, 작고 품질 높은 작품이 다수 선정되어 미래 콘테스트 규정 변화는 제한적일 전망임
     * 흥미롭고 독창적인 우승작들과 난독화 테크닉, 실험적인 가상머신·에뮬레이터 등이 소개되어 C 프로그래밍 수준 향상에 기여함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

IOCCC28 개요 및 개최 배경

     * 2024년 열린 제28회 International Obfuscated C Code Contest(IOCCC28)는 4년의 공백기를 거쳐 세계 각국의 개발자들이 참여한 창의적이고 난해한 C 코드 경연임
     * 이 기간 동안 6,168회 이상의 커밋으로 공식 IOCCC 웹사이트가 ""Great Fork Merge""라는 이름하에 대규모로 리빌딩되었음
     * 운영 측면에서 mkiocccentry 툴킷, 신규 등록 절차, 제출 서버 도입 등 다양한 혁신이 적용되어, 심사효율과 코드 공개 속도가 개선됨
     * 2025년 3월 5일부터 2025년 6월 5일까지 작품 접수를 받았고, 심사는 불과 ""33일"" 만에 종료되어, 경연 종료 후 약 2시간 내로 소스코드가 모두 공개됨

출품작 및 선정 트렌드

     * 올해 IOCCC28은 참가작 수와 퀄리티가 크게 상승함에 따라 심사 난이도도 증가하였고, 역대 최다인 23개의 우승작을 배출함
     * “4년의 휴식기 덕분에 더 나은 출품작이 나오기도 했지만, 전체적으로 참가자들이 난독화와 C 프로그래밍 실력에서 눈에 띄는 발전을 이루었음”
     * 새로운 코드 크기 제한(약 21% 증가)이 적용되었지만, 우승작의 절반 이상이 전체 제한의 2/3 미만, 10개는 1/2 미만 사이즈로 제출되어 효율성과 품질을 모두 보여줌
     * 이로 인해 향후 10년간 코드 사이즈 규정의 변화는 거의 없을 것으로 예상됨

심사 규정 및 향후 계획

     * IOCCC 규정과 가이드라인은 추후 더욱 혁신적이고 직관적으로 개정할 예정이며, IOCCC Judges는 2025년 12월로 예정된 IOCCC29 준비를 진행 예정임
     * GitHub IOCCC winner 저장소와 mkiocccentry toolkit 저장소에 대한 Pull Request 기반의 보완 패치도 계획함

우승작 소개 및 하이라이트

     * 다양한 주제의 우승작들이 선정되었으며, 다음과 같은 주요 작품 및 특징이 있음
          + ChatIOCCC: Meta의 LLaMA 2 기반 오픈소스 대형 언어모델을 세계 최소화된 LLM 추론 엔진으로 구현, 재미있는 챗봇 기능 제공
          + Eh: UTF-8 엑센트가 적용된 에디터로 ed(1)보다 더 실용적인 난독화 코드 구현
          + 135바이트 One-liner: 입력된 16진수 값을 기반으로 다양한 출력을 수행하는 극단적으로 짧은 난독화 코드
          + C-전처리기 아트: 수천~수십만 번 전처리기를 실행해 이미지 렌더링을 구현, 스크립트 실행 시간이 매우 길고 독특한 경험 제공
          + Z₃ 가상머신: Z-머신의 세 번째 버전을 참조한 가상머신 구현, 소스코드 해석을 게임처럼 즐길 수 있음
          + C64 에뮬레이터 포함 가상환경: 포크봄, 파일 삭제 등 유해한 시나리오도 안전하게 실험 가능한 가상머신, C64까지 탑재
          + Intel 4004 칩 에뮬레이션: 1971년 출시된 세계 최초 상업용 마이크로프로세서 회로를 C 코드로 게이트 단위까지 흉내내는 예술적 재현
          + 다국어 퍼즐: 최소한 세 가지 언어(C, 영어, 기타)에 친숙한 사람들을 위한 참신한 난독화

경연 참가자 및 향후 권고사항

     * 기존 우승작과 유사한 난독화는 가산점이 낮으며, 독창성과 완성도가 심사의 주요 기준임
     * 수상하지 못한 참가자들에게는 개선 후 재도전 또는 다양한 접근법의 시도를 권장함
     * 비우승작의 별도 공개 역시 환영함

입상작 컴파일 및 실행 안내

     * 일부 C 컴파일러 호환 이슈가 있을 수 있으니 최신 clang 또는 gcc 사용을 권장함
     * FAQ 및 공식 문서를 통해 문제 해결 및 수정사항 제출 방법 안내함

2024년 IOCCC28 우승작 다운로드

     * 우승작 전체는 압축 파일(2024.tar.bz2)로 제공하며, 각 작품의 상세 웹페이지와 소스코드, 저자 의견을 참고해 독창적 난독화 테크닉을 학습할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   (본문 내용에서 바로 접근 가능한 우승작 다운로드 링크와 각 우승작별 상세 설명은 공식 사이트를 참고)

        Hacker News 의견

     * 정말 대단함 :)
       이 코드는 현재 달의 위상을 콘솔에 그려줌. 만약 내가 늑대인간이라면 달의 위상을 관찰할 수 있음.
     * 이 코드, donut.c에 대한 오마주 느낌임
       donut.c 참고 링크
     * 이 코드는 1988년에 파이 값을 계산하던 IOCCC 엔트리와도 살짝 비슷한 느낌을 줌
       1988년 파이 계산 코드 참고
     * Compiler Explorer에서 바로 확인 가능함
       Compiler Explorer 바로 가기
     * 이 코드를 컴파일하는 방법은 아래와 같음

cc -Wno-implicit-int -Wno-implicit-function-declaration phase.c && ./a.out

     * 리디자인 때문에 이전 IOCCC 엔트리로 가는 링크들이 모두 깨졌음. 수십 년간 위키피디아 같은 다양한 사이트에 퍼졌던 링크들이 리디렉트 없이 막혀버렸고, 이제 Github에서 엔트리 보려면 자바스크립트 지원 브라우저가 꼭 필요하게 됨
     * JS가 없는 환경에서 접근하려면 git clone으로 저장소를 복제하는 방법을 쓰면 됨
     * 이 엔트리를 보면서 지난 IOCCC 출품작들을 읽어봤는데, 이 이미지 디컴프레션 원라이너는 소스코드의 해시 값을 인자로 받으면 직접 자기 로고 이미지를 출력함. 너무 신기함
     * 이 해시 트릭이 어떻게 동작하는지 궁금했는데, 노트에 설명이 있음. 참고로 노트 내용도 정말 웃김
       ""간결함이 난해함보다 중요시되지만, 이 프로그램 역시 IOCCC 특유의 명료성 기준을 여전히 충족함""이라고 적혀있음
     * 정말 놀라운 우연으로, 입력 포맷의 비트 수가 다섯 살짜리 GPU가 한 시간에 할 수 있는 MD5 평가 횟수의 log2와 거의 일치함
     * 이 사람 jq 만든 사람이기도 함
     * 135바이트로 작성됨! 거의 미친 과학자 영역에 가까운 코드임. 완전 믿기 힘든 수준임
     * IOCCC 엔트리 룰이 멋짐. 과거에 어떻게 악용됐는지 명확히 보여주는 매우 구체적인 규정임

Rule 2

Rule 2는 출품작이 Rule 2a와 2b 둘 다 만족해야 함
iocccsize(1) 툴로 코드가 이 룰을 통과하는지 체크할 수 있음
prog.c 같은 식으로 파일명을 인자로 넘기면 됨

Rule 2a
프로그램 소스의 크기는 4993바이트를 넘으면 안 됨

Rule 2b
iocccsize(1) 툴로 코드 크기를 측정했을 때 출력값이 2503을 넘어선 안 됨

관련해서 좀 더 자세한 내용은 FAQ나 Rule 17을 참고하면 됨

     * 그러면 파일명에 2053바이트의 추가 데이터를 담는 게 허용된다는 말임. 의외로 느슨한 규칙임
     * 4993바이트, 어떻게 이렇게 애매하게 정했는지 궁금해짐
     * underhanded-c.org도 다시 오픈됐으면 좋겠음
     * C를 쓰지 말아야 한다는 좋은 논거가 따로 있을까 싶음. 이게 명확하고도 진지한 결론처럼 느껴짐
     * 어떤 언어로든 이런 난해한 코드를 쓸 수 있음. C가 유난히 잘 어울리긴 하지만 대부분의 IOCCC 엔트리는 여러 언어에도 적용 가능한 트릭을 사용함
     * 내가 가장 좋아하는 IOCCC 출품작 중 하나이자 수상작임
       2005년 persano.c 보기
     * 아마 내가 프로그래머라면, 내 작업이 다른 사람들 눈에는 순수한 의미불명의 암호처럼 보일 것임
     * 이건 정말 멋짐
       2024년 macke 엔트리
       이 프로그램은 최소화된 기능만으로 최신 리눅스 시스템을 통째로 실행하는 에뮬레이터임
     * 작성자임. 이 바이너리가 1.6MB로 최신 리눅스를 구동함. 최대한 크기를 줄이기 위해 별의별 방법을 다 썼음. WebAssembly로도 컴파일돼서 브라우저 안에서도 실행 가능하게 됨
     * 이 엔트리가 좀 많이 혼란스러웠음
"
"https://news.hada.io/topic?id=22287","Ubiquiti, 자체 호스팅을 위한 UniFi OS Server 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Ubiquiti, 자체 호스팅을 위한 UniFi OS Server 출시

     * Ubiquiti는 자체 하드웨어에서 전체 UniFi 네트워크 스택을 구동할 수 있는 UniFi OS Server를 Early Access로 공개함
     * 이 서버에서는 UniFi Network, InnerSpace, 그리고 기존에 불가능했던 UniFi Identity도 실행 가능함
     * 설치 시 Ubiquiti 계정 연동을 통해 원격 관리, MFA, 알림, 클라우드 백업, Teleport, Site Magic VPN 등의 기능을 사용할 수 있음
     * Windows(WSL2) 및 Linux(Podman) 환경에서 최소 20GB 저장공간이 필요하며 주요 네트워크 포트를 사용함
     * 현재 버전에서는 추가적으로 InnerSpace 설치가 가능하고, 향후 UniFi Protect의 지원도 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Ubiquiti UniFi OS Server 개요

     * Ubiquiti는 UniFi OS Server를 Early Access로 출시하여, 사용자가 직접 하드웨어에 완전한 UniFi 네트워크 스택을 자체 호스팅할 수 있는 기능을 제공함
     * 초기 지원 제품은 UniFi Network와 InnerSpace이며, UniFi Identity 역시 UniFi OS Server에서 실행 가능함
     * 기존의 self-hosted UniFi Network Server에서는 UniFi Identity 실행이 불가능했으나, 이번 버전에서는 지원됨

요구 사항

     * 스토리지: 최소 20GB의 여유 디스크 공간 필요
     * Windows: WSL(Windows Subsystem for Linux) 버전 2 필요
     * Linux: Podman 4.3.1 이상 필요
     * 필요 포트: 3478, 5005, 5514, 6789, 8080, 8444, 8880, 8881, 8882, 9543, 10003, 11443 등 사용

UniFi OS Server 설치 및 구성

     * 서버 부팅 후, 서버 이름을 입력하고 Ubiquiti 계정으로 로그인하는 과정 필요
     * Ubiquiti 계정으로 로그인하면 unifi.ui.com을 통한 서버 관리, 원격 접근, MFA, 알림, 클라우드 백업, Teleport, Site Magic VPN 사용 가능
     * 계정 없이 사용할 경우, 원격 관리와 보안 및 클라우드 기반 기능 사용 불가
     * 기존에 self-hosted UniFi Network가 있다면 네트워크 가져오기 또는 UniFi Console 백업 복구 가능
     * UniFi Network 프로그램이 기본 설치되어 있어, 네트워크만 설정하면 바로 사용 가능

InnerSpace와 추가 기능

     * InnerSpace 지원: 설정 > Control Plane에서 InnerSpace 설치 가능
     * 현재 버전은 UniFi Network와 InnerSpace를 지원하며, 향후 UniFi Protect 지원도 기대됨

서버 시작 및 중지

     * UniFi OS Server UI를 닫아도 서버는 백그라운드에서 계속 동작함
     * 서버 완전 종료를 원하면 시스템 트레이에서 UniFi 아이콘 우클릭 후 종료
     * 시작할 때는 시작 메뉴에서 앱 실행, 첫 부팅 시 ‘UniFi Network offline’ 메시지가 잠깐 나타나지만 곧 운영됨
     * 서버 실행 중에는 https://localhost:11443 통해 브라우저에서 바로 접근 가능

Debian 환경 설치 방법

     * Linux 설치 시, 필요한 의존성 설치 후 설치 파일을 받아 실행하는 순서로 진행
     * root로 진입 후, 시스템 업데이트 및 Podman 등 필수 패키지 설치, 공식 설치 파일 다운로드 및 실행 필요

리눅스 환경에서의 SSL 인증서

     * Linux에서 사용하는 경우, Mirano Verhoef의 스크립트를 통해 Let’s Encrypt SSL 인증서 자동 적용 가능
     * 참고 저장소: MiranoVerhoef/UniFi-OS-Server-SSL-Import: Import for new Unifi OS Server

마무리

     * UniFi OS Server를 자체 호스팅 환경에서 실행할 수 있게 된 점이 큰 장점임
     * 사용자들은 향후 UniFi Protect 기능 추가를 기대하는 상황임

        Hacker News 의견

     * Ubiquiti에 대해 할 말은 좋은 것밖에 없음, 우리 집에 카메라, 도어벨, 네트워크 스위치를 설치해서 수년간 거의 100% 가동률 경험함, UI는 계속 발전하고 Home Assistant와 매우 잘 통합됨, 비판하는 사람도 많지만 내 홈랩에서 이 정도만큼 심화된 수준이 딱 좋음, 랙의 디자인과 터치 패널, 싱크되는 애니메이션이 너무 멋져서 볼 때마다 감탄함, 제품을 디자인한 사람은 확실히 미적 감각이 뛰어남
          + 새로 이사할 집의 네트워크 구성을 조사 중인데 현재로선 UniFi가 최우선 후보임, FTTH 업체가 지하실까지 장비 설치해주며 그 이후부터는 내 장비로 관리할 계획임, UCG Ultra 게이트웨이, 여러 PoE 스위치, Wifi 7 AP 2~3개로 구성 예정임, 게이트웨이 하나로 중앙 집중식 관리가 가능하고 PoE 관련 확장성도 뛰어남, 컴포넌트별 업그레이드도 쉬움, HomeAssistant와의 통합성도 좋음, 이 모든 게 TPLink Omada 같은 대안들보다 가격도 비슷하거나 오히려 저렴한 편이라 매력적임
          + 여전히 UniFi의 액세스 포인트를 사용하는데, 비슷한 가격대에서 이만큼 품질 좋은 제품이 없음, 하지만 EdgeRouter 개발 중단한 것에 크게 실망해서 카메라, 스위칭, 라우팅은 다른 회사로 옮겼음, 이번 출시가 Ubiquiti에 긍정적으로 작용하긴 하지만 다시 전부 UniFi로 돌아갈 생각은 없음
          + Ubiquiti를 비판하는 사람들이 많지만 그게 사실 어느 정도 정당하다고 생각함, Ubiquiti가 아주 잘 만드는 부분이 분명히 있지만 내 요구 사항이 제공되는 기능, 옵션, UX, API 내에서 해결되면 엄청 좋은 경험임, 하지만 좀 더 복잡한 걸 하려고 하면 오히려 핵심 인프라와 계속 싸우게 되고 굉장히 피곤해짐, UniFi는 스위치와 기본적인 라우팅까지 쓰는 게 적절하고, 조명, 카메라, 출입 통제 등은 필요에 따라 선택적으로 쓰는 게 좋음
          + Dream Machine Pro와 AP를 산 건 인생에서 전혀 후회하지 않는 몇 안 되는 IT 소비 중 하나였음, 몇 년째 문제가 없고 당분간 교체할 생각 없음, 완벽하진 않았지만 내가 원하는 방식으로 네트워크를 제어할 수 있게 됨, 덕분에 Ubiquiti 컴포넌트도 여러 개로 늘렸고 최근엔 Synology를 UNAS Pro로 바꿈, 이것도 만족함, 단 하나 후회한 건 tooless mini rack인데, 비유비키티 장비도 같이 랙에 설치해야 하는데 그쪽 지원이 미흡하고, 조만간 K8s를 돌릴 서버를 직접 만들어줄 것 같진 않음
          + IP 카메라 한 대에 360파운드는 너무 비쌈, 가격이 절반만 되어도 Ring 대비 비교 불가인데, Lite 스위치는 랙 마운트형에서 소형화 됐고 별도의 마운트용 귀도 없음, 반면 게이트웨이는 훌륭한 가성비임
     * 설치 안내 코드가 최근 본 것 중에선 제일 조잡한 느낌임, 그리고 오류 발생 시 멈추지도 않고 세미콜론으로 줄 계속 이어서 오류가 쌓이게 되어 있음
          + 이 댓글과 원글 양쪽 모두 문제라 보는데, 이 정보의 진짜 출처가 뭐냐는 점임, 공식 발표, 깃허브, 프로젝트 페이지 같은 링크가 없고 그냥 다운로드 파일 주소뿐임, 설명대로 설치 방법이 허술하긴 하지만 신뢰할 공식 정보가 없으니 이것만으로 실제 설치 과정을 평가하긴 힘듦
          + 유비퀴티의 코드는 완벽하니 귀찮은 오류란 건 신경 쓸 필요도 없다는 식의 농담임
          + 이건 마치 수학 교과서에서 오버플로우 검출 넣는 것처럼 느껴짐, 이런 식 정보 공개는 사람들이 쉽게 읽고 이해할 수 있게 하는 것에 더 중점이 있고, 오류 검출은 사용자 몫임, github와 gist의 차이와 유사함
          + 세미콜론 대신 &&를 쓰면 간단하게 오류를 방지할 수 있음
     * 오랜 시간이 지난 후에 마침내 Unifi 스택을 중심으로 어릴 적 꿈이었던 홈 네트워크 랙을 만들었음, 신형 10기가 스위치, Dream Machine SE, 카메라들도 설치했고 정말 인상 깊음, 모든 게 그냥 '작동한다'는 느낌이고 애플의 영감을 많이 받은 듯함, 외부 접근을 차단해서 카메라 환경을 완전히 폐쇄적으로 운영 가능함, 자가 호스팅 옵션은 프라이버시를 중요하게 여기는 사람에겐 한 차원 업그레이드임
          + Unifi 카메라의 가장 큰 단점은 'AI' 감지(단순 움직임 감지 제외)를 원하면 클라우드 연결이 필수라는 점임, 언젠가 개선을 바란다는 희망이 있지만 지금은 카메라에서 움직임 감지만 사용 중임, 이 점이 문제라면 Unifi 구매 전 반드시 트레이드오프를 잘 이해해야 함, 설명 링크1, 설명 링크2
          + 전원 장애만 없으면 동의하겠지만, 내 경우 viewports가 카메라와의 연결 복구를 거부하고 여러 번 재설정해야 다시 작동함, 유선 카메라는 몇 시간 지나야 돌아오고, (WiFi) 도어벨만 예외임, 이런 동안 Ubiquiti 스위치를 통해선 모두 온라인 상태가 보임
          + 전반적으로 '그냥 작동한다'는 인상인데, 뭔가 특이한 설정만 안 하면 문제 없음, 인터넷 없는 VLAN을 만들고 싶으면 정말로 외부 연결이 없는지 반드시 테스트해야 하고, 방화벽도 규칙 전부 직접 테스트해야 함, WiFi 설정을 바꿨을 때 WiFi가 끊기지 않게 하고 싶으면 운에 맡겨야 함, 동일한 MAC 주소를 가진 장치가 여러 VLAN을 사용할 때 해당 장치 정보(MAC, 스위치 포트, DHCP 예약 등)를 찾으려고 하면 DB 구조나 프론트엔드-백엔드 연결이 망가진 느낌임, 문서 보고 뭔가 설정을 찾으려고 해도 위치가 계속 바뀌고 문서는 업데이트도 잘 안 됨
          + 2023년에 비슷하게 구축했고 똑같은 경험을 했음, Sonos 관련 문제 빼고는 단 하나의 문제도 없었음, 최근엔 부모님 집에 Cloud Gateway Max로 CCTV 세팅하고, 3번 클릭만에 site-to-site VPN 설치 완료, 이제 원격 지원도 쉽고 부모님 집 Sony TV에서도 내 Jellyfin 서버가 검색됨
     * PC와 좋은 m.2 WiFi 카드가 무선용으로 좀 더 쓰이기 쉬웠으면 좋겠음, PC 기반 라우터는 소프트웨어도 훌륭해서 사용성이 뛰어남, WiFi 문제만 아니면 특수한 박스에만 의존할 필요가 없을 텐데 아쉽게 느껴짐, 심지어 openwrt도 무선 부분은 제약이 심함, hostapd.conf가 가장 중요한 네트워크 채널의 게이트키퍼이지만 대부분 제대로 이해하지 못하고 있음, 그나마 m.2, m-pcie 카드가 요즘 좀 더 구하기 쉬워졌음, 대다수가 Compex 기반이지만 이제는 WiFi 7 2x2 5+5GHz(예: Compex WLTE7002E55, Qualcomm QCN6274 사용) 지원 모델도 약 $200 정도로 구매 가능함
          + AP를 한두 대 들이면 됨, 예를 들어 TP-Link EAP610이 적합해 보임(아직 직접 사용해보진 않았음), openwrt 관련 링크
          + 그 맥락에서 보면 UniFi 액세스 포인트가 잘 맞음, 라우터는 유선 추천임
          + 나는 OPNsense와 Ruckus 독립형 AP 조합을 쓰고 있는데 무결점 경험을 했음
          + 그 시장 자체가 매우 협소한데, 많은 이들이 바로 중고 엔터프라이즈 AP를 라디오 용도로 씀(나도 Ruckus 850 3대 운영 중임)
     * 몇 년 전 Ubiquiti 관련 이슈가 많았는데, 지금도 꾸준히 남아있고 이젠 셀프 호스팅 쪽으로 회귀하는 것 같아 반가움, 10여 년 전에 산 하드웨어도 여전히 클라우드 없이 잘 동작 중이고 최신 장비도 업그레이드 가치가 있어 보임(10Gbps 전면 지원)
          + 여전히 벤더 락인 성향이 남아 있다고 느낌, 예전 조사할 때 카메라가 ONVIF 지원을 안 했었음, 하드웨어는 뛰어난데 소프트웨어 선택권은 실망스러움
          + 예전 사건 이후로 HN에서 Ubiquiti 비판 댓글이 확연히 줄어든 것 같음
     * 내가 이해한 게 맞는지 모르겠지만 처음에는 Unifi를 네트워크의 윈도우 프로그램으로 직접 실행해서 셀프호스팅 해야 했던 걸로 기억함, 그러다 클라우드로 바뀌고, 또 '클라우드 전용'이 됐다가 최근 다시 셀프호스팅으로 돌아오는 느낌임, (Unifi는 유비퀴티 네트워크 장비 설정/통계용 앱이자 시스템임, 저렴한 가격에 네트워킹 업계를 바꾼 게임체인저였음)
          + 셀프호스팅 앱은 없어지지 않았고, 8년 넘게 직접 운용 중임, 처음엔 맥북 프로, 그다음엔 라즈베리 파이, 요즘은 중고 HP T620 씬클라이언트로 돌리는 중임, 유비퀴티는 클라우드 컨트롤러나 사전설치형 Cloud Key를 적극 홍보하지만, self-hosted UniFi Network server는 꾸준히 남아 있었음(이름이 UniFi Controller, UniFi Network Application, UniFi Network Server 등으로 몇 번 바뀌었음)
          + 네트워크 장비의 핵심 컨트롤러는 언제나 윈도우뿐 아니라 리눅스 등에서 자가 호스팅 가능했음, 다만 NVR 소프트웨어 같은 부가 앱들은 컨트롤러 하드웨어에서만 쓸 수 있었음, 현재 UniFi OS server는 기존 셀프호스팅 스택과 별 차이는 없어 보임, 앞으로 이 셀프호스팅 스택에 추가 앱이 더 들어갈 듯함
          + 클라우드 전용으로 간 적은 없음, 항상 자가 호스팅 가능했음, 여러 버전의 클라우드 호스팅도 제공했으며, 다른 회사들이 직접 클라우드 호스팅 인스턴스를 제공하기도 했음
          + 한때 Cloud Key라는 소형 하드웨어가 있었고, 버전 2도 출시함, 오랫동안 컨테이너(도커) 버전으로도 배포했음, 1세대 Cloud Key 안내서, 2세대 Cloud Key 판매 링크, linuxserver.io 컨테이너
          + UDM Pro에 컨트롤러 내장됨, Cloud Key도 2종 존재함, 컨트롤러 소프트웨어는 리눅스, macOS, 윈도우 모두 지원, 개인적으론 리눅스에서 도커로 수년간 무리 없이 구동했었고 관리도 수월했음
     * 최근 UDM Pro Max에서 Firewalla Gold Pro로 전환했는데 매우 만족함, UniFi 네트워크 옵션은 기능 많지만 설정 체크박스가 저장 상태에 반영되지 않는 등 마법적인 삽질을 필요로 함(QA 개선 필요), 사용하던 문제 중 하나 예시: 디바이스 Static IP 저장 불가
          + UI 버그 하나와 패치되지 않은 여러 보안 취약점을 비교하면 후자를 더 심각하게 봐야 함, Firewalla 보안 이슈 설명
          + Firewalla가 더 낫다고 해도 가격이 4배임, Gold SE는 $509, UniFi Cloud Gateway Ultra는 $129, 내 경험상 UniFi 소프트웨어도 충분히 잘 동작하며, 대부분의 라우터보다 바로 사용하기 쉽고 성능도 좋음
          + UniFi 사용시 어떤 문제가 있었는지 궁금함, 내가 보기엔 '애플스러운' 경험에 가장 가까운 장비 같고, 타사 대비 기능도 압도적으로 많음
          + SOHO 라우터가 $889라니, 타깃 고객층이 궁금함
          + 5년 전 버그 하나 갖고 이렇다 할 순 없음
     * 병원, 금융사 같은 곳이면 Cisco를 쓰겠지만 우리처럼 직원 50명 미만, 사무실 4곳(본사/콜센터/코로케이션/클라우드) 있는 SMB에선 Ubiquiti가 관리가 정말 쉬워서 만족 중임, 성능, 지연, QoS, 대역폭 일부를 희생하는 거 알지만 이 가격에 이 정도 S2S VPN 성능이면 충분함
     * 수년간 Unifi 컨트롤러를 Docker 컨테이너로 셀프호스팅해서 쓰고 있음(예전엔 윈도우에서 필요할 때마다 실행했음), 이제 'Unifi OS'란 명칭을 붙인 거 보니 네트워크 앱 외에도 다양한 앱을 셀프호스팅할 수 있는 미래를 애매하게 암시하는 듯함
          + 라즈베리파이에서 운영하던 적 있는데 완벽하게 동작했었음, 하드웨어 컨테이너 같은 느낌임
          + UniFi 컨트롤러 컨테이너를 어떻게 활용하는지 궁금함, 나도 시도해봤는데 Unifi AP와 잘 결합되지 않거나 설정이 자주 날아가는 문제가 있었음, 지금은 iOS 앱으로 AP 펌웨어만 가끔 업데이트함, 컨트롤러에서 제공하는 모든 인사이트를 보고 싶음
     * 지금 도커 컨테이너로 구동 중인데 뭐가 달라진 건지 잘 모르겠음
          + 지금 아마 UniFi Network Server(예전엔 UniFi Controller)라고 불리는 걸 돌리고 있을 듯함, 네트워킹 장비 구성은 가능하지만 그것만으로 Ubiquiti 전체 플랫폼(Identity, Site Manager/SD-WAN/Teleport 등)까지는 아님, 예전엔 '앱' 하나만 개별적으로 호스팅 가능했지만, unifi.ui.com에 로그인했을 때 뜨는 다양한 클라우드 앱들의 그리드 전체는 아니었음, 이제는 이 전체 그리드 자체(멀티사이트, 펌웨어 업데이트 서버 등)와 일부 앱(Identity 등)도 직접 구동 가능하게 되고, 앞으로 더 많은 앱 추가가 예상됨
          + 이번엔 Ubiquiti 공식 Docker 이미지/컨테이너가 출시됨, 더 이상 LinuxServer.io, jacobalberty 등에 의존할 필요 없음, 'UniFi OS' 브랜딩으로 앞으로 Talk, Protect, Access 같은 앱도 직접 하드웨어에 설치해 돌릴 수 있게 될 가능성을 시사함
"
"https://news.hada.io/topic?id=22347","구직자들이 AI 면접관을 피하고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          구직자들이 AI 면접관을 피하고 있음

     * AI 면접관이 본격적으로 도입되며 지원자들은 기계와의 면접에 큰 거부감을 표출함
     * 지원자 경험은 대체로 실망스럽거나 비인간적이라는 평가가 많으며, 일부는 AI 면접이면 지원 자체를 포기함
     * HR팀 입장에서는 인력 감소와 대량 지원서 처리 부담을 해소하는 효율적 도구로 AI 면접을 긍정적으로 평가함
     * AI 면접관이 초벌 검증을 맡고, 이후 실제 면접은 사람이 진행된다는 구조가 확산되고 있음
     * 지원자와 기업 간 문화적 인식 차이가 커지고 있지만, AI 면접은 이미 대세가 되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 면접관 도입과 지원자 반응

     * 구직자들은 Zoom 등에서 인간 대신 AI 봇을 만나는 면접 상황에 당혹, 실망, 심지어 모멸감을 느낌
     * ""구직 자체가 이미 힘든데 AI 면접까지 겹치면 감정적 피로가 크다""는 의견 다수
     * 실제로 AI 면접 경험 후 아예 지원을 포기하거나, 회사 문화 자체에 의문을 품는 지원자 증가
     * AI 면접관의 반복 질문, 불편한 대화 방식, 회사나 문화에 대한 설명 부족 등 비인간적 경험이 주된 불만
     * “기계와 30분 대화할 바엔 지원 안 한다”, “AI 면접이면 그 회사는 나를 존중하지 않는다” 등 강한 거부감 표출

HR팀과 기업의 AI 면접 활용 배경

     * HR팀 인력 감축, 수천 건의 지원서 처리 등으로 AI 면접관을 통한 효율성 극대화 시도
     * AI가 1차 스크리닝을 담당하고, 최종 후보자만 사람이 직접 평가하는 구조
     * 기업 입장에서는 반복적·객관적 역량 검증에 AI가 효과적이라는 평가
     * 특히 고객지원, 리테일, 엔트리 레벨 IT 등 대규모 채용이 필요한 분야에서 적극 도입

경험담: 지원자의 실제 목소리

     * 50대 기술 문서 작성자: ""AI가 내 경력만 반복적으로 묻고, 회사에 대해선 아무것도 설명 못했다. 이후엔 반드시 사람 면접이 보장되어야 한다""
     * 60대 에디터: ""이력서에 적힌 경력만 반복해서 묻는 비인간적 질문에 10분도 못 버티고 나왔다""
     * 영국 기업 근무자: ""AI 면접을 도입한 회사는 지원하지 않는다. 회사가 내 성장과 학습에 신경 쓰지 않는 느낌이고, 조직 문화 자체에 불신이 든다""

AI 면접관의 한계와 미래

     * 기업 HR 입장에서는 시간·비용 절감, 객관성 등 실질적 장점이 분명함
     * 하지만 AI는 지원자와 회사의 '문화적 적합성' 평가에는 한계가 있음(기업도 인정)
     * ""AI가 100명 면접해 10명만 걸러내고, 이후부터 사람이 평가하는 구조""가 표준이 되고 있음
     * AI가 계속 발전하더라도, 사람과의 진짜 면접이 보장된다는 신뢰가 중요해짐

결론

     * 지원자와 HR 양측의 인식 차이가 매우 크지만, 기업은 효율성을 위해 AI 면접을 적극 도입 중
     * AI 면접이 불가피한 흐름으로 자리 잡고 있으며, 지원자는 이 과정에서 적응이 필요함
     * 향후에는 AI가 맡을 수 없는 인간적 평가와 문화적 적합성이 더욱 부각될 전망

   지금회사도 AI면접절차가있어서 들어올때도 보고, 면접관으로서도 살펴봤지만 비인간적인 느낌보다도..최소한의 정성도 느껴지지 않아 불쾌하기만 하더라구요
   인적성시험 과정에서 느끼는 불쾌함이 AI로 배가 되는듯합니다

   이력서에 ai 면접관 망가트리는 프롬프트 흰글자로 넣으면 될까요? ㅋㅋㅋ

   학벌로 짜른 후에 AI 면접관을 쓰는 걸까요? AI 면접이 어떤 면접자들을 거르는 역할인 건지 궁금하네요

   회사는 AI 면접관을 내세우고, 구직자는 AI 대리자를 내세우면 어떻게 될지도 상상하게 되네요

        Hacker News 의견

     * 예전에 AI 기반 인터뷰를 해본 적이 있음, 딱 한 번임. 끝나고 나서 너무 허무해서 다시는 안 하기로 다짐했음. 인터뷰 내내 AI라는 걸 알고 있었지만, 내 45분을 컴퓨터한테 쏟았다는 사실이 충격이었음. 결국 또다시 회사에서 아무 소식도 못 듣고 그 시간은 영영 못 돌려받음. 그 시간에 차라리 다른 회사 지원을 하거나 요리, 운동, 가족과의 시간을 가질 수 있었음에도 바보같이 봇한테 말을 하게 됐음. 혹시나 회사가 실제 사람을 위한 첫 스크린 단계로 쓰는 걸 수도 있지만, AI 인터뷰를 보낸다는 건 추가 포트폴리오 요청 메일 받는 거랑 똑같이 의미 없는 과정임
          + 봇한테 인터뷰를 받는 건 끔찍할 것임. 자동 응답 시스템도 싫은데 이런 게 점점 많아짐. 최근에 직접 인터뷰어로 참여한 일이 있었는데, 아주 기본적인 질문을 해도 ""모른다""고 솔직하게 답하는 사람보다 엉뚱한 얘기를 둘러대거나 거짓말로 포장하는 사람이 더 많았음. 한 지원자가 프로그래밍을 직장에서 처음 배웠다고 하고 이젠 팀장까지 한다고 자신만만하게 말했지만, 정작 20분 동안 자기 이력서에 쓴 내용도 답변 못함. 그 후에 오히려 ""언제 출근하면 되냐""고 묻기까지 했음. 채용은 정말 양쪽 모두 후진 구조임. 커미션만 노리는 리크루터가 제일 별로지만, 구직자와 회사도 별반 차이 없음. 다른 업계는 잘 모르겠지만 IT 쪽은 정말 심각한 문제임
          + 난 차라리 집 없이 지내거나 힘든 선택을 하는 게 낫지, 내 존엄성을 버리고 AI한테 평가받아가며 인터뷰를 보고 싶지 않음. 하지만 현실이 그 방향으로 가는 중임. 앞으로는 OpenAI 같은 곳에서 평가받고, 홍채까지 검사받는 세상이 될까 두려움
          + 인터뷰는 쌍방향이어야 함. 내가 한 건 인터뷰가 아니라 오디션 같은 느낌이었음. 이미 대다수 지원서가 무시되는 현실에서 뭔가를 '연기'할 에너지가 없음. 그래서 이런 걸 아예 안 함. 개인정보와 프로파일링 문제도 있어서 꺼림
          + 이런 방식 때문에 결국 나도 IT를 떠나 내 사업을 해야겠다는 생각을 하게 됨
          + AI와 어떻게 커뮤니케이션 했는지 궁금함. 차라리 AI가 우리를 대신해 인터뷰를 받고, AI끼리 상대하게 하면 어떨까 하는 생각도 듦. 우리 쪽 AI가 사실과 다른 미사여구로 우리를 포장해도 결국 어떻게 하겠냐는 생각임
     * HR팀들이 너무 많은 지원서를 처리해야 한다고 하는데, 애초에 수천 명이나 뽑을 생각을 하면 방식 자체가 잘못됨. 우선 현재 좋은 직원들에게 추천 받거나, 꼭 이력서를 봐야 하면 자격 조건에 따라 분류를 해야 함. 명백한 로봇 지원서는 이메일 스팸처럼 제목만 봐도 걸러낼 수 있음. 예를 들어 시카고의 보험/금융 회사에 스탠포드 학위와 FAANG 경력 10년짜리 이력서가 매번 온다면 그건 가짜임. 경험이 충분하고 인터뷰가 괜찮은 최초의 지원자를 채용하고 레퍼런스만 확인하면 됨. 굳이 수십 명, 수백 명을 검토할 필요 없음. 대부분 지원자는 평균이고 결국 그런 사람을 뽑게 됨. 그리고 당신 회사는 그렇게 특별하지 않으니 겸손해야 함. 최상위 1% 인재를 필요로 하는 곳은 거의 없고, 사실 그런 인재는 당신 회사에 관심도 없음
          + 실제로 수천 명의 지원서를 받는 건 오히려 ‘최고’를 뽑으려는 욕심에서 비롯된 문제임. 현실적인 목표는 검색 비용과 어느 수준 이상의 인재 간의 적절한 타협임. 슈퍼마켓에 가서 제일 잘 익은 바나나를 못 고른다고 고민하는 사람은 없음. 몇 개 정해 놓고 랜덤하게 골라 보고 결정하면 됨. 정말 뛰어난 지원자를 고르는 데 완벽하고 편향 없는 방법이 있다고 믿는 것도 착각임. 값싼 AI 인터뷰가 오히려 대충 필터하는 것보다 지원자 풀 전체에 더 나쁜 영향을 줄 수도 있음
          + 나는 좋은 조언이라고 생각하지만, 이런 얘기를 들어도 HR 담당자가 개선할 거라는 믿음은 없음. 왜 이렇게 시스템이 엉망이 됐는지 궁금함
          + 시카고 보험/금융 회사에 NASA 경력이 포함된 10년 경력자인 내가 3천번 넘게 지원서를 냈지만 형식적인 불합격 메일만 받았던 경험이 있음. 결국 20년 지인 덕분에 지금 직장을 얻게 됨
          + 요즘 모든 회사가 불확실한 시기에 ‘최고 중의 최고’만 뽑겠다고 고집하는 현상이 있음. 하지만 그런 인재는 이미 더 좋은 조건의 회사에 채용된 상태고, 고용주들이 이 사실을 빨리 깨달아야 함
          + 재미있는 사례로, 요즘 로봇 지원서 중 처음에 아스키 아트로 “로봇 지원입니다, 이력서는 이렇고 AI 지원 시스템에 피드백 주세요”라고 안내하는 경우가 많아짐. 이런 건 바로 휴지통으로 들어감
     * AI가 100명의 인터뷰를 보고, 그 중 10명만 관리자가 직접 본다니 “와…”라는 생각이 듦. 원래 구직자가 인터뷰 단계에 도달하면 회사 입장에서도 시간과 노력이 들어가기에, 지원자의 시간 역시 가치 있게 다루는 셈이었음. 45분을 투자해도 면접관 여러 명이 함께 시간을 쏟으니 최소한의 상호 존중이 있었음. 그런데 90% 지원자 시간을 이렇게 낭비한다고 공공연하게 밝히는 건 정말 무례한 일임
          + 사실 이건 이미 오래전부터 있던 일임. 키워드 하나 안 맞으면 탈락, 경력 오래됐거나 팀에 안 맞아 보여도 탈락, NDA가 걸린 회사에서 일했으면 탈락, 학교가 달라도 탈락. 애초에 사람까지도 못 갔던 지원자가 대부분임. AI가 담당해도 현실은 똑같음. 실제로 내 지원서를 실수로 2년 후에 검토한 사례도 있음. 보통은 소식도 없고 이미 훨씬 전부터 필터링 됐던 것임
          + 반면, 이 시스템은 애초에 지원해서 안 뽑혀야 할 사람을 효과적으로 거를 수 있고, 나머지에게는 오히려 노이즈가 되는 가짜 이력서가 늘어남. 하지만 좋은 지원자라면 AI 인터뷰로 더 돋보일 수 있는 기회가 커질 수 있음
          + 실제로 하루 만에 500개의 가짜 이력서를 받는 경우도 많음. 이름으로 걸러낼 수도 없고, 이런 경우 AI가 사기 지원자를 잡기 위해 쓰이는 듯함. AI라면 아직 평등고용법 위반으로 비난받을 염려가 적음
     * 요즘 채용기업들이 AI 사용을 합리화하려 애쓰는 게 우스움. Coinbase는 AI 도입 사례에서 “AI가 채용을 비인간적으로 만든다는 걱정과 달리, 더 빠르고 더 질 좋은 상호작용이 늘어난다. 지원자들도 우리만큼 AI에 열광한다”고 주장함. 하지만 이런 논리 뒤에는 분명 도전과제도 존재함
          + “AI가 채용과정을 비인간적으로 만들까봐 우려하는 사람도 있지만, 우리는 정반대라고 믿음”이라니. “AI가 일자리를 늘릴 것이다”, “AI가 인류의 에너지 소모를 배로 늘려도 기후위기를 해결할 것이다” 같은 말과 동일함. 언제부턴가 대중 앞에서 이런 말을 거리낌 없이 하게 됐음
          + “Coinbase에서 일하고 싶은 지원자들은 우리만큼 AI에 열정적일 것”이라는 말, 어쩌면 맞을지 모름. 하지만 그들이 AI 챗봇과 혼자 대화하고 싶어서가 아니라, 흥미로운 문제를 함께 풀고 싶어서인 것임. 열정적인 동료와 협력하는 것을 바라는 것임
          + Anthropic은 AI를 활용한 면접을 허용하지 않았다는 사실도 있음
          + 이 인터뷰가 영상으로 진행된다면, 분명 곧 AI가 비백인 지원자에게 낮은 점수를 준다는 소식이 들려올 것이고, 아무도 신경 쓰지 않을 것임
          + Coinbase 내부에 독성적인 워크컬처가 있다는 얘기를 동료들에게 많이 들음
     * AI가 HR을 대체하는 흐름 때문에 결국 부자만 더 부자 되고, 정작 사람 뽑아야 할 자리까지 자동화가 진행되는 현상임. 슈퍼마켓에서 더 많은 점원을 두지 않고, 손님이 직원처럼 일하게 만드는 구조도 마찬가지임. 다음엔 창고 물건까지 손님이 가져오라는 것만 남음. 그럼에도 가격은 그대로거나 오히려 더 비쌈. 아티스트에게 돈 안 주고, 광고도 AI 이미지로 대체함. 번역가한테 돈 안 주고 자동 더빙 및 짜증스러운 인공지능 목소리로 대체함. 기업들은 수익에만 집착하고 있지만, 결국 사람도 물건을 사려면 돈이 필요하다는 걸 잊고 있음
          + 창고에서 물건까지 직접 가져오게 하는 건 북유럽의 IKEA가 이미 수십 년 전에 해결한 거임. 스칸디나비아식 실용주의란 말로 마케팅 많이 함
          + “사람도 물건을 사려면 돈이 필요하다”는 의견에, 사실 돈은 부채의 기록일 뿐이라는 시각이 있음. 누군가 내게 무언가를 해줬는데 바로 보답하지 못하면, 미래에 갚겠다는 약속이 돈임. 그런데 AI가 모든 것을 무료로 제공하는 세상이라면 사람 간의 부채 자체가 없어지고, 돈의 의미도 사라지게 됨
          + 요즘 나가는 마트는 창고 자체를 없앤 곳도 있음. 운송 구역만 있고, 창고 없이 진열대 위 여분만 남기는 ‘저스트 인 타임’ 방식임. 원하는 물건이 없다면, 창고에 남는 게 없는 것임
          + 셀프 계산대가 가장 이해 안 감. 제품값도 올리고, 점점 더 불편하게 만들면서 내가 직접 계산까지 하게 만듦. 온라인 주문하면 수수료도 내야 함. 모든 소매점이 Amazon처럼 돌아가야 할 필요는 없음. 솔직히 Amazon조차 그렇게 운영되는 게 싫음. 나이가 들어서 그런지는 몰라도, 기술 쪽에서 일한 게 영향인지 인간 간 상호작용이 사라지는 최적화 집착 문화가 싫증남
          + 현재 AI는 오히려 기존보다 비부자들이 더 강하게 활용할 수 있는 수단이기도 함. 그래서 부자만 돈 버는 게 아니라, 사실 누구나 AI로 이익을 볼 수 있음
     * Fortune 기사에서 AI 인터뷰 때문에 지원자가 무시당한다고 느껴 오히려 구직 기회를 포기하게 만든다는 주장도 있음. 반대로 HR 전문가들은 AI가 1차 면접에서 시간을 절약해줌으로써, 이후 지원자와 더 깊은 대화를 할 수 있게 해준다고 함
          + “이 회사가 싫어진다”는 말에 “하지만 당신이 틀렸다”는 답변, 오히려 더 불쾌감만 커짐
          + 챗봇으로 고객상담을 대체하는 논리와 똑같음. 실제로 경험은 훨씬 악화됨. 힘들더라도 사람과 연결되기를 끝까지 포기하지 않은 고객만 남으니, KPI나 NPS 점수가 왜곡될 뿐임
          + AI 인터뷰가 이력서에서 얻을 수 있는 정보 이상을 줄 수 있나 의문임. 이미 고용주와 구직자 간의 힘의 균형이 한쪽으로 너무 기울어 있음. 이런 표면적인 해명에서는 구직자의 시간과 소중함을 전혀 인간적으로 생각하지 않는 느낌임
          + 인간 면접관과 더 많이 대화할 시간은 필요없고, 그저 평범한 인터뷰를 원함
     * Braintrust CEO인 Adam Jackson이 ""이제 이런 과정 거치는 건 필연적""이라고 Fortune에 말했는데, 사실은 사람들이 절박해서 어쩔 수 없이 따르는 것임. 브레인트러스트는 구직자 선택권을 인질로 잡고, 당연히 수치는 좋게 나오지만 정작 AI로 면접 본 사람이 어떻게 느끼는지 물어보지도 않음. Jackson 본인은 회사 실적과 보너스에만 신경 씀
          + LLM이 지원자를 평가할 수 있다는 것 자체에 회의적임. 실제로 ML 모델이나 단순 선형모델에 비해 예측력도 떨어짐. 직원의 성과를 제대로 측정하고 랭킹화시킬 수 있는 지표 자체부터가 어려움. 솔직히 HR에게 뱀기름(사기테크) 파는 느낌임. 다만 인간 면접관/리크루터 중에도 쓸모없는 사람이 많으니 그들보다 싸게 쓸 수 있다는 점은 인정함
          + Braintrust CEO의 말에서 중요한 결론은, 그가 거짓말 중이라는 사실임. 실적에 따라 주식보상받는 사람이 이런 AI가 실제와 다르게 효과적이라며 거짓 설명을 하는 것임
          + 제품 파는 측이 자기 제품 좋다고 하는 건 뻔함. 사람들이 절박하니까 이런 수치가 나오는 거지, 다시 2021년 구직시장으로 돌아가면 HR 상대하는 걸 취미처럼 바꿔 욕보이겠다는 각오까지 들기도 함
          + AI 인터뷰는 절박하지 않은, 오히려 더 능력 있는 지원자를 탈락시키는 필터 역할임. 정말 실력이 되는 지원자는 이런 과정을 거칠 동기조차 없음. 실제로 이런 회사일수록, AI로 대체될 확률이 더 높음
          + “우리 서비스로 C급 지원자 중 가장 절박한 사람을 뽑을 수 있습니다!”라는 꼴임
     * CEO가 “보이콧하면 효과 있을 것”이라고 자백했다는 점에서, 이론적으로는 집단적 거부운동으로 이런 흐름을 막을 수 있음
          + 하지만 대부분의 구직자에겐 선택권이 별로 없기 때문에 현실적으로 어렵다는 의견도 있음
          + 힘든 구직시장에선 선택의 여지조차 없음. 직장을 구하지 못하면 가족 생계가 위협받음. 예를 들어 우리 회사가 신입 자리 하나를 공고했더니 24시간도 안 돼 2000명 이상 지원이 몰려 옴. 지원서 검토조차 다 못 하고 마감했는데, 이 정도 경쟁이면 AI 면접이 효과적으로 작동할 필요 없이 일단 지원자를 추려주는 데 의미가 있음
          + 실제로 이런 시스템은 실력이 아니라 ‘절박함’을 측정하는 도구에 불과함. 내가 직접 수백 개의 지원서를 모두 검토해서 놓칠 수 있는 인재를 직접 뽑았고, 그분이 우리 팀에 큰 에너지를 불어넣어줬음. 만약 AI 추천만 믿었다면 결과가 달랐을 것임. 소프트웨어로 채용하는 회사는 오히려 본인만 손해임. 좋은 인재가 회사를 필요로 하는 게 아니라, 회사가 인재를 필요로 하는 것임
          + 결국 대다수는 별다른 저항 없이 따라갈 것이라는 현실적인 시각도 있음
          + 보이콧이 언제나 효과 있다는 건 맞지만, 실질적으로 임계량을 채우기가 어렵다는 점이 문제임
     * 현실적으로 ‘진짜’ 인터뷰조차도 어차피 별 의미 없으니, AI 인터뷰에 굳이 시간을 쏟을 필요를 못 느끼겠음. 장기적으로는 이런 방식을 채택한 회사가 역효과를 볼 거라고 생각함. AI를 공략하는 법만 배우는 사람들이 결국 몰릴 테니 효율은 점점 떨어질 수밖에 없음
     * AI 챗봇이 내 대신 인터뷰를 본다면 오히려 이런 불필요한 회사를 걸러내는 데 좋겠음
          + 다음 단계는, 내가 해변에서 노는 동안 내 AI가 출근하는 세상이 될 것임
          + 결국 인터넷에서 유통되는 모든 컨텐츠는 봇이 만드는 것으로 봐야 하는 시대가 올 것임. 이런 회사 모델은 길어야 몇 년 남지 않을 것임
          + 내 AI 이력서를 보내줄 수 있냐고 하고 싶음. 가상 면접을 통해 우리팀에 긍정적인 영향을 줄 수 있는지 봐야겠음
"
"https://news.hada.io/topic?id=22351","PHP는 프로그래밍계의 "토요타 코롤라"에요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        PHP는 프로그래밍계의 ""토요타 코롤라""에요

     * PHP와 JavaScript는 1995년에 등장한 이후 웹의 양대 축으로 발전했으며, 모두 초기 설계 결함과 조롱에도 불구하고 생태계를 확장함
     * 두 언어는 1997년에 표준화를 거쳤으며, 각각 Facebook과 Google/Microsoft의 지원으로 2010년대에 재도약을 경험함
     * PHP는 '프랙탈적 나쁜 설계', 독특한 문법과 다양한 결점 때문에 오랜 기간 경시받았지만, 전 세계 웹사이트의 70~80%가 PHP로 구동되고 있음
     * LAMP 스택의 핵심으로, 어디서나 쉽게 접할 수 있고, 저렴하며, 현대적 기능(OOP, FP, 패키지 관리자, 테스트 등)도 갖춘 견고한 언어로 진화함
     * 최근에는 FrankenPHP와 같은 Go 기반 런타임 등장, 그리고 꾸준한 보안 개선과 커뮤니티 주도 발전으로 미래를 모색하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PHP와 웹 프로그래밍의 성장

     * 1995년, 무명에 가까웠던 한 소프트웨어 개발자가 새로운 스크립팅 언어인 PHP의 첫 버전을 발표하면서 웹 애플리케이션 개발의 지평이 넓어짐
     * 점진적인 표준화와 OOP(객체지향 프로그래밍) 기능 도입으로 본격적인 언어로 자리매김함
     * 하지만 독특한 문법과 런타임 모델, 잦은 보안 이슈, 그리고 수많은 프레임워크로 인해 수십 년간 '진지한' 프로그래머들 사이에서 비판의 대상이 됨
     * 그럼에도 불구하고 커뮤니티의 힘과 주요 IT 기업의 지원으로, PHP는 꾸준히 진화하고 거대한 생태계를 확장 중임

PHP와 JavaScript의 평행 이력

     * 1995년, 각각 Rasmus Lerdorf(“Personal Home Page Tools”)와 Brendan Eich(Netscape)로부터 등장
     * 두 언어 모두 초기에는 조악한 설계와 구문 문제로 “진지한 프로그래머”들의 비판을 받았음
     * 1997년 각각 PHP/FI 2, ECMA-262를 통해 표준화 진행
     * 2010년대 들어 Facebook, Google, Microsoft 등 빅테크의 지원을 받아 대대적 발전을 경험함
     * 2025년 기준, Go 언어 기반으로 재탄생하는 새로운 런타임(FrankenPHP, TypeScript Native) 등장

'모두가 싫어하는데 모두가 쓰는 언어'의 아이러니

     * C, Java, Python, Perl 등 각 언어는 고유한 별명을 가짐
          + C: ""이식 가능한 어셈블리""
          + Java: ""한번 작성, 어디서나 디버그""
          + Python: ""실행 가능한 의사코드""
          + JavaScript: ""10일만에 만들어진 언어""
          + Perl: ""인터넷의 덕트 테이프""
     * PHP는 ""프랙탈적 나쁜 설계"", ""Pretty Horrific Programming"" 등 부정적 별명에도 불구하고, 전 세계 웹의 70~80%를 구동하는 실적을 보임

PHP: '프로그래밍계의 토요타 코롤라'

     * 저렴하고, 어디서나 쉽게 쓸 수 있으며, LAMP(리눅스, 아파치, MySQL, PHP) 조합의 상징
     * 오늘날 PHP는 완전한 오픈소스 프로젝트로, 고급 OOP 기능(트레잇, 프로퍼티 훅, 네임스페이스, 속성, 열거형)과 함수형 프로그래밍 기능(클로저, 캡처 리스트, 애로우 함수)까지 지원함
     * 곧 도입될 파이프 연산자, 빠른 타입 검사, 풍부한 표준 라이브러리, 강력한 오픈소스 패키지 매니저(Composer)와 Packagist, 우수한 테스트 프레임워크 PHPUnit, 등으로 현대적이고 강력한 특징을 갖춤
     * 성능과 컴파일 속도도 뛰어나며, JetBrains의 전용 IDE, 자체 마스코트 등, 활발한 커뮤니티와 자체 생태계를 보유함

과거의 문제와 시선의 변화

     * goto 연산자, 독특한 변수명 등 예전의 단점도 존재하나, 최근 10년간 정기적 릴리즈와 보안 취약점 제거, 레거시 API 정리 등 끈질긴 개선이 이루어짐
          + 2000년대 초 보안 취약점 및 'I Hate PHP' 운동 등 부정적 이미지 존재
          + 최근 10년간 정기적 릴리스(매년 11월), 보안 개선 및 API 현대화로 언어적 완성도와 신뢰성 강화
     * 과거 PHP를 경시하던 프로그래머들도 최근 발전을 인정해야 할 시점에 이름
     * SQL 인젝션 등 과거의 대표적인 보안 이슈는 여전히 주의가 필요한 부분임

커뮤니티와 생태계

     * PHP Foundation 등 공식적인 조직과 업계 지원이 강화되며, JetBrains를 제외하면 대형 테크 기업의 영향에서 벗어나 비교적 독립적인 커뮤니티 문화를 유지함
     * 언어 순위는 2004년에는 3위를 기록했으나 최근에는 다소 하락해 다양한 평가 지표에서 7~15위권임
          + TIOBE, IEEE, PYPL 등 언어 인기 순위는 하락세지만, RedMonk 등 일부 순위에서는 꾸준히 상위권에 올랐으며, 하락세가 생각만큼 크지 않음을 보여줌

엔진의 진화: FrankenPHP

     * 수십 년간 Zend 엔진이 PHP의 표준 엔진 역할을 했으나, 최근 FrankenPHP라는 새로운 공식 런타임 도입으로 컨테이너 환경 지원이 크게 개선됨
     * FrankenPHP는 기존 코드베이스와 100% 호환성을 유지하면서도 간편한 컨테이너화와 새로운 실행 모델을 제공해, 미래지향적인 PHP 발전에 핵심 역할을 맡게 됨

PHP의 미래와 인식

     * PHP는 여전히 '진지하지 않은' 언어로 평가받을 수 있지만, 수많은 사이트와 애플리케이션을 뒷받침하는 실질적인 영향력과, 활발한 커뮤니티가 유지되고 있음
          + 여전히 ""우연한 설계의 산물"", ""논문에는 잘 안 나오는 언어""라는 오명 존재
     * Rasmus Lerdorf가 강조한 바와 같이, 커뮤니티의 자발적 참여와 성장에 기반한 언어임을 확인할 수 있음
     * Control을 내려놓고 많은 사람들의 협업으로 진화한 '바자르형' 오픈소스 성공 사례로 기록됨

결론

     * PHP는 쉽게 무시할 수 없는, 웹 생태계의 '토요타 코롤라' 같은 존재
     * 유머와 비판의 대상이 되지만, 실제로는 전 세계 웹의 근간을 이루는 강건한 프로그래밍 언어임
     * 저렴한 비용과 쉬운 진입장벽, 풍부한 생태계로 젊은 개발자의 첫 걸음을 쉽게 만들어주는 환경을 제공함
     * 앞으로도 커뮤니티와 새로운 기술 트렌드의 조화로 지속적인 발전 가능성을 지님

   쉽게 말하면 ""돈 벌어 주는 언어""

   현직 php 종사자로써 php를 왜 이렇게 깎아내리는지 이해하기 힘듬.
   다들 php가 저렴하고 구축하기 쉽고 장기적으로 사용할 수 없으며 보안이 약하다고 소리침.
   그러나 그 발상 자체가 모두 구시대적 발상이며, 나는 자신있게 그들에게 '바보'라고 말해주고 싶음.
   오늘날의 php는 매우 현대적인 언어이며, 지금부터 장기적인 프로젝트를 진행하더라도 어디서든 사용할 수 있는 모듈을 만들어서 어느 웹이든 재사용이 가능하며, 이는 현재 modern한 php framework들이 말해주고 있음. 최신은 아니지만 8버전 이상의 php는 보안성 또한 많이 향상되었으며, 매 해마다 버전 업그레이드를 예고하고 있음. 대표적으로 Laravel 또한 매 해마다 버전 업그레이드를 진행 중임.
   php가 보안이 약하다고 말하는 그대들이게 Java 또는 기타 백엔드 언어에서 매우 많은 위험 보고서가 발견되고 매우 큰 위험들이 지금도 터지고 있다고 말할 수 있음.
   대표적으로 대한민국이라는 나라는 국가에서 Java를 이용한 jsp를 사용하고 있지만, 보안 문제가 매 해마다 터지고 있음.

   결론: php가 멍청하고 약하며 저렴한 것이 아니다.
   php를 잘못 사용하는 사람들이 멍청하고 약하며 저렴하다.

   레이?
    1. 싼 값에 누구나 쉽게 접하고 쓸수 있으며
    2. 경차 주제에 꽤 넓어서 이것저것 욱여넣기 좋다는 느낌이 들며
    3. 외관도 나름 꽤 예쁘지만
    4. 연비가 나빠서 장기적으론 생각만큼 저렴하지 못하며
    5. 정작 욱여넣으려고 보면 엔진 파워나 여러가지 이슈로 마음대로 굴리기 쉽지 않고
    6. 예쁜 외관을 만끽하며 캠핑카로도 쓰고, 여기저기 꾸미는 것도 잠깐이지, 결국 대충 몰고 다니게 되는데다
    7. 심지어 전복 위험성까지 있는 차
    8. 싼 값에 누구나 쉽게 접하고 쓸수 있으며
    9. PHP 주제에 웹, CLI, 워크로드 등 여러 용도로 사용할 수 있고
   10. 외관도 나름 예쁘게 구성할 수 있지만
   11. 스파게티 되기 쉽고, PHP 버전이 올라가며 점점 복잡해져서 유지보수 비용이 생각보다 높고
   12. 정작 여러용도로 사용하려고 보면 CLI로 굴리기엔 무거워서 웹 말고는 쓰기 쉽지 않으며
   13. 깔끔한 구조의 라라벨, 심포니도 잠깐이지, 대충 때려박아 스파게티 만드는 유혹을 이기기 정말 힘들고
   14. 매번 보안 위협을 걱정하거나 걱정받아야 하는 언어

   걍 모닝이나 스파크 같은 존재 아님? 돈은 없고 운전은 하고 싶고 ㅋㅋㅋ 나도 처음에 접근 한건 php이긴 한데 요즘은 거이 안씀. Laravel같은 거 보면 작은 프로젝트에 무슨 저리 많은 oop를 쓸가 하는 정도임. 잡채 짬뽕 그 자체임. 프레임웍을 안 쓰고 여러 사람 거친 코드는 xx.php xx1.php 이빠이 ㅋㅋㅋ

   국산차로 치면 무슨 차일까요

   포터?

   위험하지만, 이것보다 나은게 없음.
   저도 ""포터""가 적당해 보여요.

        Hacker News 의견

     * Java는 마치 Corolla와 비슷함. 일부러 평범함을 추구했고, Mazda3 같은 경쟁 모델에 비해 세련미가 부족하며, 오직 A에서 B로 가는 데만 쓰일 뿐인 느낌임. PHP는 프로그래밍계의 Hyundai Elantra 같음. 과거에는 진입 비용이 낮아 모두가 썼지만, 디자인 문제와 신뢰성 부족으로 오랜 기간 농담거리였음. 하지만 Elantra처럼, 많이 발전해서 이제는 꽤 괜찮아졌음
          + Corolla에 대한 평가는 요즘 현실과 많이 다름. 2023년부터 기본형 Corolla는 자동 주행보조 기술들이 기본 탑재되어 있고, 심지어 Mazda의 최상위 트림 옵션보다도 뛰어남. 62마일에서 완전히 멈추는 자동 긴급 제동 테스트도 통과한 유일한 차량이 Toyota Corolla임
          + 비유 자체는 훌륭하지만 예시는 좀 아쉬움. Mazda3가 오히려 ""간소한 옵션, 저렴함, 잘 작동함""의 느낌에 더 가까움, 특히 2025년 기준으로는 더욱 그래 보임. 그리고 Corolla가 신뢰성 면에서 나빴던 시절이 기억나지 않음. 친척 어르신도 '69년형 Corolla를 처음 산 진짜 새차였다고 늘 자랑함. 그게 80년대 일본차 대세의 길을 열었음
          + 요즘 Java는 꽤 괜찮음. 타입 추론, 파이버, 텍스트 블록, 레코드 등 많은 기능이 향상됨. Java 8만 생각할 거면 모르겠지만, 예전만큼 현대 Java 환경 쓰는 게 싫지 않음
          + 비유 정말 멋있게 잘함. Python은 어떤 차랑 비슷한지 물어보고 싶음. 나중에 차 살 때 참고하게
          + Java는 오히려 Honda에 더 가까움. 어디서든 볼 수 있을 만큼 널리 깔렸고, 지루하다고 느껴질 정도로 평범하고 확실함. 대체한다, 없앤다는 여러 시도가 있었지만 계속해서 잘 살아남음
     * PHP가 성공한 이유는 딱 하나, 배포가 매우 매우 쉬웠기 때문임. 프로그래머들은 이걸 과소평가하지만, 나는 웹개발에 처음 접근했을 때 몇 분 만에 실서비스에 올릴 수 있어서 신세계였음. 오히려 자전거 같은 존재라고 느꼈음. 싸고, 면허도 필요 없고, 누구나 탈 수 있고, 복잡한 절차 없이 빠르게 달릴 수 있던 느낌이었음. 10년 넘게 안 썼지만 새로운 경험으로 빠져드는 계기였음
          + 인터넷에서 PHP에 대한 조롱도 많았지만, ""아무 바보나 PHP 쓸 수 있다 — 실제로 많은 바보들이 썼다""는 말이 인상적이었음. 근데 진짜로 진입장벽 낮은 건 오히려 좋은 일임. 나는 더 많은 사람들이 프로그래밍할 수 있으면 좋겠다고 생각하고, PHP가 그걸 지원한다면 더 환영임
          + 마찰이 제로인 경험 자체가 양자도약임. 뭔가 중요한 일을 터무니없이 쉽게 할 수 있다면, 특화툴이든 대중서비스든, 단점이 있더라도 쉽게 용납해버리게 됨. ""하나만 잘해라""는 유명한 철학을 다시 떠올림
          + 예전에 Perl CGI 스크립트로 일할 땐 FTP로 업로드, 실행권한 설정, 파일 퍼미션 등에서 항상 장애물이 많았음. 근데 PHP는 웹서버에 내장돼 있으니 이런 과정이 사라져서 따로 웹관리자와 실랑이할 필요 없이 그냥 바로 돌아가는 경우가 많았음. 언어 특성보다는 배포 구조 덕분이 더 큼
     * PHP는 한마디로 설명하기 힘든 언어임. 90년대 후반 mod_perl에서 갈아탈 때 대체재로 썼고, 정적인 페이지에 다이내믹 데이터를 쉽게 추가할 수 있었음. 특히 대형 사이트를 어떻게 만드는지 경험치가 없던 시절엔 시행착오의 연속이었음. 하지만 언어 자체는 정말 별로였음. 흔히 하는 말처럼 ""나쁜 기술자는 도구 탓한다""지만, 두 개의 손잡이에 세 개의 랜덤 끝이 달린 드라이버가 있다고 상상해보면 됨. 어쩌면 그것도 쓰긴 쓰지만, 굳이 저런 도구를 사랑한다고 하는 사람을 보면 의심하게 됨. 당장 옆에 무료로 더 좋은 Snap-On이 있는데 말임. 그래서 PHP로 작은 서버사이드 기능 넣는 건 문제 없는데, 그걸로 거대한 거미성 같은 것들이 쌓이고, 그걸 보면서 ""봐, 저 드라이버로 이런 것도 할 수 있다니까!"" 라고 하는데, 그 성이 일정 시간마다 일부가 무너져
       내리기도 함. 이런 모습은 별로 신뢰를 주지 못함. 그나마 조금 개선은 됐지만 여전히 매우 특이한 드라이버임
          + 말하자면 PHP는 나쁜 디자인의 프랙탈임
     * 다른 언어들도 이제는 PHP를 따라잡았음. 초기에 HTML에 코드를 직접 쓸 수 있는 언어는 대부분 서버사이드 Javascript, ColdFusion, ASP처럼 전부 독점적이었음. PHP가 처음으로 오픈소스였고, 쓸만한 수준에 도달해서 2001년에 웹앱 개발의 1순위가 되었음. cgi-bin 같은 것들과 비교해도 빌드 없이 빠르고, 리소스 관리도 잘 돼서 호스팅 업체들이 저렴한 PHP 호스팅 제공 가능했고, Wordpress처럼 세상을 바꾼 오픈소스 제품이 나올 수 있었음. 하지만 곧 한계가 드러났는데, 좋은 웹앱을 만들려면 다양한 페이지를 입력에 따라 라우팅해주는 프레임워크 구조가 필요하다는 걸 깨달았음. 예를 들어 폼에서 에러가 나면 성공 화면을 보여주거나, 에러 메시지를 다시 보여줘야 하는데, 이걸 discipline만 있으면 PHP로도 가능하긴 하지만 결국 새로운 라우터를 만드는 순간, 템플릿
       시스템으로 뷰를 따로 만드는 게 당연해짐. Ruby on Rails 이후 모든 언어가 PHP만큼 편리한 프레임워크를 가지게 됐고, 일정 수준의 discipline도 요구하게 됨
          + PHP가 HTML 안에 코드를 바로 쓸 수 있다는 점이 제가 사용한 전부의 이유였음. 2000년대 초 PHP의 include를 알게 됐을 때, 그게 늘 빠져 있던 퍼즐 조각처럼 느껴졌음. 더는 헤더/네비/푸터를 수십 개 페이지마다 수동 동기화할 필요가 없었음. Rails가 등장하기 전까지 제대로 된 웹앱을 만든 적은 없었음. Rails의 구조와 관례 덕에 훨씬 접근성이 좋아졌고, 당시 복잡한 PHP 코드는 거의 스파게티 같아서 배우는 입장에서 따라 하기도 힘들었음
     * 20년 넘게 PHP를 써왔고, 백엔드에서는 항상 1순위 언어임. 서버 엔지니어는 아니지만 PHP는 빠르고, 지원도 잘 되고, 실력이 있으면 충분히 견고하고 안전하며 성능 좋은 서버를 만들 수 있음. 다만 언어 자체는 별로 좋아하지 않음. 그래도 앞으로 50년도 충분히 주요 백엔드 언어로 남아 있을 거라 생각함. 나만의 ""어항 그래프""라고 부르는 게 있는데, 이 그래프가 모든 걸 말해줌 그래프 링크
          + 나도 20년 넘게 PHP를 써왔음. 아직도 괜찮은 언어라 생각함. 요즘은 TypeScript를 더 선호하지만, 종종 PHP로 스크립트 작업함. 내장 기능들이 좋아서 여전히 쓸만함
          + 저 그래프 데이터는 어디서 온 건지 궁금함. Scala가 4.6%고 Python이 1.2%라는 건 내 예상과 완전 다름. 업계에서의 체감이 다른 걸 수도 있음
          + 내가 보기엔 저 그래프는 거의 Wordpress의 압도적인 점유율(전체 사이트의 43%)과 Joomla(2%), Drupal(1%) 같이 모두 PHP 기반인 CMS가 만든 결과로 보임
     * 저자는 2009년의 나쁜 설계 문제는 많이 개선됐다고 잘 설명했지만, 2025년에 새 프로젝트에서 굳이 PHP를 써야 하는 이유는 잘 말하지 못함. 다른 언어에 비해 뚜렷하게 뛰어난 점이 뭔지 크게 와닿지 않음. 소개된 기능들도 다른 언어와 거의 비슷한 수준의 기능일 뿐, 두드러지게 더 나은 점은 모르겠음
          + PHP만이 확실히 우위에 있는 점은 ""shared nothing"" 아키텍처임. 예를 들어 fastapi처럼 Python 환경에서는 메모리에 데이터를 올려두면 요청 간에 그 데이터가 남아 있음. 즉시 문제를 해결하는 데 빠를 수 있지만, 이런 구조는 나중에 결과 예측도 힘들고 디버깅도 까다로움. 반면 PHP는 요청마다 아무것도 남지 않으니, 유지하고 싶은 데이터는 반드시 외부에 저장해야 하므로 이런 버그 자체를 근본적으로 예방함. 의외로 이 장점은 코드를 많이 짜다 보면 꽤 큼
          + PHP의 장점은 예나 지금이나 비슷함. 1) 공유호스팅에서 특히 쉽고 빠른 배포 2) 요청 간 데이터 공유가 없어서 동시성과 병렬성의 이점이 있음 3) HTML과 섞어서 쓸 수 있어 따로 템플릿 언어가 필요 없음. 세 번째는 모두에게 장점은 아니고, 많은 프레임워크가 별도의 템플릿 언어를 선호하지만, 간혹 정말 편리한 경우도 있음
          + 비교 대상은 Python, Ruby, Javascript 일텐데, shared nothing 구조 외에도 오픈소스 생태계와 커뮤니티 성숙도가 높고, 패키지 매니지먼트가 단일하며, 성능도 대체로 더 좋음 (Javascript만큼 빠르지는 않을 수도 있음). 경쟁 언어들이 각각 어느 한 항목에서는 좋을 수 있지만, 위의 모든 조건을 다 만족시키는 건 PHP라고 봄
          + 이미 포진한 코드가 많으니 vibe coding의 기반으로는 좋을 수도 있음
          + 작성자는 PHP가 2009년에 얼마나 개발됐는지 제대로 설명하지 못했다고 봄. 주장 대부분이 ""옛날 방식은 안 쓰고, 새 방식이 있어""라는 식이라, 결국 실수해서 옛날 방식을 밟으면 삽질하게 됨. 좋은 기본값이 중요한데, 그 부분을 간과하고 있음
     * 글이 PHP haters 언급에 너무 신경 쓰면서 오히려 왜 2025년에 PHP를 선택해야 하는지 제대로 설명하지 않아서 설득력이 약해진다고 느껴짐. 나는 PHP를 거의 써본 적이 없지만, 실제로 왜 이 언어를 쓸 만한지, 더 매력적인 근거가 나왔으면 좋겠음. 제시된 이유들은 내게 크게 와닿지 않음
          + php 개발자는 구하기 쉽고 인건비도 저렴하니, 그 이유만으로도 많은 기업에선 선택지가 정해짐
     * 나와 PHP의 첫 만남은 꽤 웃긴데, 지금 내 커리어도 여기서 방향이 결정된 것 같음. 대학교 마지막 학기에 중요한 과목을 여러 개 한 번에 들어야 해서 엄청 힘들었음. 그 중 하나가 웹 개발 수업이었고, 이커머스 사이트 만드는 게 과제였음. 여러 가지 방법을 배웠고, PHP에서 특히 배포가 쉬웠던 게 큰 장점으로 다가왔음. 하지만 마지막에는 모두 vanilla JS로 하라고 했고, 나는 JS로 제대로 배포하지 못했음. 다른 동기들도 마찬가지였음. 평가 기준을 보니, 코드 평가는 20%밖에 안 되고, 나머지는 전부 사이트 디자인이나 데드라인 내 배포 성공 등이었음. PHP로 작업하면 무조건 돌아간다는 확신이 있어서, 코드 평가는 포기하고 배포 성공에 집중해서 B-를 받을 수 있었음. 대부분 학생들은 과제를 아예 배포도 못해서 떨어짐. 현대 PHP에 관심 많지만 어디서 어떻게
       시작해야 하는지, 어떤 생태계가 있는지 모르겠음. 다들 여전히 PHP는 못생기고 보안도 약한 언어로만 봄
          + 실무에서는 Laravel이나 Symfony 쪽이 많이 쓰임
     * 다른 분야로 옮기기 전까지, PHP 덕분에 꽤 많은 돈을 벌었음. 20년 커리어 동안 한 번도 일거리가 없던 적이 없었음. 주변에 다른 언어를 쓴 지인들은 꾸준히 일하는 데 더 힘든 경우가 많았음
     * 절대 동의할 수 없음. PHP는 마치 Ford Escort 같은 언어임(정기적으로 손이 가고 보안이 약하지만 운전은 쉽고 단순함). Corolla는 정말 정말 신뢰할 수 있고, 일관성있으며, 훨씬 더 안전함. (여러 차량과 언어를 다뤄본 입장에서 볼 때, PHP는 그냥 덜 나은 선택임)
"
"https://news.hada.io/topic?id=22268","혁신의 그림자: AI 보안의 새로운 패러다임, MCP-PAM과 Guardrails","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             혁신의 그림자: AI 보안의 새로운 패러다임, MCP-PAM과 Guardrails

   최근 생성형 인공지능(Generative AI)의 급속한 확산으로, 많은 기업이 AI 도입을 검토하거나 실무에 적용 중입니다. 그러나 AI 활용 확대는 내부 기밀 유출, 무단 사용, 규제 문제 등 심각한 보안 위협도 함께 수반합니다. 삼성전자의 내부 코드 유출 사건, 이탈리아 개인정보보호국의 ChatGPT 일시 차단 사례가 이를 방증합니다.

   기존 AI 보안의 핵심 축은 AWS, 구글, 마이크로소프트 등에서 제공하는 Guardrails로, AI의 출력물을 필터링해 혐오 표현, 민감정보 노출 등을 방지합니다. 하지만 Guardrails는 ‘출력 내용’에만 집중해, 누가 언제 어떤 권한으로 AI를 사용하는지와 같은 맥락 기반 통제는 한계가 큽니다.

   2024년 등장한 Anthropic의 MCP(Model Context Protocol)는 AI가 Slack, GitHub, AWS 등 외부 시스템과 연동하여 실제 업무를 수행하도록 돕는 혁신적 통신 프레임워크입니다. 그러나 AI가 외부 시스템에 직접 접근하는 MCP 환경에서는 단순한 콘텐츠 필터링만으로는 보안 위험을 제어하기 어렵습니다. 이에 사용자 권한 관리, 행위 기반 정책, 감사 로깅을 포함하는 Privileged Access Management(PAM) 체계가 필수적입니다.

   본 글은 MCP와 PAM을 결합한 QueryPie의 보안 아키텍처를 중심으로, Guardrails와 어떻게 상호 보완적으로 작동하며, 프롬프트 주입, 내부자 위협, 민감정보 유출 등 최신 AI 위협에 대응하는지를 상세히 분석합니다.

   핵심 내용 요약
     * Guardrails: AI 출력 중심 필터링, 혐오·폭력·개인정보 차단에 효과적이나, 맥락 기반 통제는 부족
     * MCP PAM: AI의 외부 도구 호출 전 사용자 권한 및 행위 검증, 정책 기반 세밀한 접근제어 제공
     * 위협 대응: LLM 남용, 프롬프트 주입, 특권 오용, 민감정보 유출, API 남용 등 다양한 공격 시나리오에 정책적으로 대응 가능
     * 통합 보안 체계: Guardrails의 콘텐츠 안전성 + MCP PAM의 정책 기반 행위 통제 + 출력 후 DLP 연계로 다층 방어 실현

   AI 보안은 단순 필터링을 넘어, “누가, 언제, 무엇을 요청했는지”까지 관리하는 체계로 진화하고 있습니다. MCP-PAM 아키텍처는 AI 활용의 혁신과 보안의 균형을 맞추는 미래 지향적 해법입니다.

   더 자세한 내용과 구체적인 기술 분석, 위협 모델 대응 전략은 아래 블로그에서 확인하세요.
   👉 https://www.querypie.com/ko/resources/discover/white-paper/16

   AI가 똑똑해질수록, 보안도 달라져야 합니다. QueryPie는 그 변화의 중심에서 해답을 찾고 있습니다.
"
"https://news.hada.io/topic?id=22264","Microsoft, Edge에 Copilot 모드 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Microsoft, Edge에 Copilot 모드 도입

     * Edge 브라우저에 실험적 AI 기능인 ‘Copilot 모드’가 도입, 단순한 탭 기반 웹서핑을 넘어 AI가 능동적으로 사용자의 웹 경험을 보조하고 업무 흐름을 혁신
     * Copilot 모드에서는 하나의 입력창에서 챗, 검색, 내비게이션이 통합되고, 여러 탭의 전체 맥락을 인식하여 정보 비교·결정·정리까지 지원
     * 음성 명령을 통한 자연어 내비게이션, 실시간 작업(Action), 동적 보조 패널 등으로 클릭·입력 없이 효율적인 탐색이 가능
     * 개인정보·보안 강화, 사용자가 언제든 기능 켜고 끌 수 있음을 전제로, 브라우저 데이터 처리와 접근 범위를 명확하게 관리
     * 무료이자 완전 선택적(Opt-in)으로, Windows/Mac용 Edge에서 한정 기간 제공 중이며 피드백을 기반으로 지속 발전 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Copilot 모드란?

     * 기존의 직선적 탭 탐색에서 벗어나 AI가 사용자 의도와 맥락을 파악하여 능동적으로 제안·지원하는 새로운 브라우저 모드
     * 새 탭을 열면 챗·검색·내비게이션을 통합한 단일 입력창과 함께 간결한 UI 제공
     * Copilot은 사용자가 열어둔 여러 탭의 전체 상황을 이해, 정보 비교·결정·정리 등 복잡한 탐색 과정을 능동적으로 보조

주요 기능

     * 멀티탭 맥락 기반 브라우징 지원
          + 모든 열린 탭의 맥락을 인식, 예를 들어 여행지 비교 시 가장 해변과 가까운 숙소를 AI가 빠르게 찾아주는 식으로 탭 전환과 수작업을 최소화
     * 자연어 명령과 Action
          + 음성 명령 지원: “이 페이지에서 정보를 찾아줘”, “상품 옵션을 비교할 탭을 열어줘” 등 자연어로 요청 가능
          + 앞으로는 히스토리, 인증 정보까지 선택적으로 연동해 예약·구매·정보 수집 등 복합 작업을 Copilot이 자동화할 계획
     * 다이내믹 보조 패널 & 정보 요약
          + 원하는 시점에 Copilot을 패널 형태로 호출, 페이지 전환 없이 레시피 변환·번역·팝업 제거·장문 요약 등 집중력 저하 요인 제거 및 핵심 정보 즉시 제공
     * 작업 및 탐색 여정 이어주기
          + 앞으로 과거~현재 브라우징 기록을 주제별·프로젝트별 여정(journey)으로 정리, 후속 액션과 학습, 맞춤 제안 지원
          + 사용자는 Copilot의 데이터 접근 시점과 범위를 명확히 인지·통제 가능

개인정보 보호 및 사용자 통제

     * Microsoft의 신뢰성 높은 개인정보·보안 표준을 기반으로, 사용자가 데이터 수집·활용을 직접 설정·제어
     * Copilot 모드는 언제든 Edge 설정에서 온/오프 가능, 사용하지 않을 경우 기존 Edge 환경 그대로 사용 가능
     * Copilot이 브라우저 데이터 접근·분석 시 항상 명확한 시각적 알림 제공

실험적 도입과 피드백

     * Copilot 모드는 무료·한정 기간·Opt-in 방식으로 우선 제공, 사용자 피드백을 받아가며 지속적으로 발전
     * Discord 커뮤니티 등에서 아이디어 제안 및 토론 참여 가능

시작 방법

     * aka.ms/copilot-mode 에서 바로 사용해보고, 새로운 AI 브라우징 경험을 체험 및 의견 제시 가능

        Hacker News 의견

     * 최근 Microsoft의 Copilot 관련 전략을 이해하기 어려움. 이미 비슷한 기능이 수십 개나 있고, 가격도 제각각이지만 정작 아무 것도 제대로 작동하지 않음. 돈 벌릴 수 있는 포인트는 “Excel에 자연어로 복잡한 요약‧분석 요청 시 새로운 시트와 그래프까지 자동 생성” 같은 일상적 업무인데, 현재 Copilot은 순서 설명만 해줄 뿐임. 이미 Excel, Office에 API가 있으니 자연어 명령을 Excel 실무로 연결해주는 AI만 만들어도 모든 회사에서 쓰게 될 것임. 나는 이런 툴이 너무 필요해서 여러 AI를 시도했지만, 실제로 원하는 만큼 되는 경우는 없음. 생각보다 어려운 문제인 듯하지만 Microsoft가 수십억 달러를 들였음에도 좀처럼 해결 안 되고 있음. 기업 비즈니스 로직이 핵심인데, 이걸 제대로 새로운 아이디어로 확장하지 못하는 모습이 아쉬움
          + AI에 막대한 돈을 투입한 만큼, Microsoft 등은 아직 미완성인 AI 제품을 무리하게 밀어내고 있음. 일단 뭔가 공개해야 하니 미숙한 서비스로 사용자 데이터를 모으며, 그 데이터를 활용해 언젠가 더 나은 AI를 만들기를 바라는 구조임. 동시에 사용자가 무의식 중에 AI를 먼저 찾게 만드는 습관을 유도하는 것임
          + ChatGPT Pro에 가입해서 실제로 Excel 요약을 요청해본 경험이 있음. 엄청 열정적으로 작업하더니 결과물은 형식도 엉망, 데이터 일부는 잘려나간 정말 실망스러운 파일이 나옴
          + 최근에 기업 연수가 Copilot 활용법을 보여주던 중, CSV 헤더 파일 불러오기가 실패해서 15분간 헤매는 장면을 봄. 기술적으로 아직 신뢰 있게 판매할 수준의 AI-엑셀 통합이 멀었다는 느낌임
          + 나도 비슷한 생각을 했는데, Microsoft가 정말 해야 할 일은 아주 간단한 “AI 스크립트 기록 기능” 임. 예를 들어 CSV 다운로드→ 데이터 가공→ 새 시트 추가→ 검증→ 시스템 업로드 등 반복적 작업 흐름을 한번만 AI로 녹화 후 나중엔 자동화할 수 있으면, 코딩이나 Excel, VBA를 잘 모르는 사람도 업무 효율이 극적으로 향상될 것임. 회사의 대부분 부서에서 반복 데이터 작업이 엄청 많음
          + “AI는 OS 사용 흐름에 자연스럽게 녹아들고, 개선된 검색, OCR, 필기 인식 등 실사용자 경험을 높여야 함. 억지로 채팅창 달아두는 식이 아니라 자연스러움이 포인트임. Edge를 써야 할 이유가 점점 줄어듦. 문제는 VS, VSCode, PowerShell, Windows Terminal, Aspire, Azure 등 모든 개발자 도구에도 AI를 넣으려는 분위기임. 각 팀이 AI 성과 목표에 허덕이는 게 보임”
     * 최근 Microsoft의 행보는 사용자를 배려하기보다 불편하게 하는 쪽으로 몰아간다는 느낌임. Edge 강제, 다크 패턴, 텔레메트리 초기화, Recall 논란 등 이용자 피로감을 가중시키는 사례가 많음. 나의 추측은 Google의 브라우저 매출을 부러워하며, “우리가 OS라는 이유로 더 많은 수익을 얻어야 한다” 는 경영진의 태도에서 비롯된 것임. 이런 식의 단기적 이익 추구가 수십 년 쌓아온 신뢰를 소모시키고 있음. 차라리 Windows를 개인정보 보호의 본보기로 만들거나, macOS처럼 진짜 네이티브 앱 생태계를 육성하면 장기 충성도를 확보할 수 있을 것임
          + 마이크로소프트는 사용자를 희생시켜서도 엄청난 돈을 벌 수 있고, Windows 이용자는 대안이 많지 않으니 별 걱정을 안 한다는 자세임. 사실 애플로 갈 사람은 이미 넘어갔고, 리눅스가 조금씩 성장하지만 그마저도 두려워하지 않는다는 인상임. 독점 기업은 충성도나 가치 창출에 굳이 신경 쓸 필요가 없게 됨
          + 일부 마이크로소프트 제품엔 대단한 잠재력이 보였는데, 경영진이 사용자의 모든 가치를 쥐어짜려는 분위기만 커지고 있음. 단순히 모든 UI를 일관성 있게 만들고, 광고나 검색결과를 강제로 안 보여주는 등 기본에 충실한 제품을 만든다면 더 좋았을 것임. 예전엔 Edge 팬이었지만 쿠폰 등 쓸데없는 기능이 추가되면서 실망함. VS Code는 훌륭하지만 .Net 지원이나 ARM, RDP, Copilot 등은 점점 실망임. 오픈소스·대안 서비스와 비교해도 “가장 필요한 기능”은 없는 경우가 많음. 이제 새 프로젝트엔 MS-SQL 대신 PostgreSQL을 쓰고 싶음. Windows Server 기반 솔루션 개발도 지양하고 있으며, VS Code조차 생각이 많아짐
          + Google의 수익은 대부분 광고에서 나오며, Chrome과 YouTube 서비스는 무료라는 점이 특별함. Microsoft는 전통적으로 유료(Windows, Office 등) 중심임
     * Bloomberg의 Microsoft Copilot 분석 기사(링크 제공)와 관련 HN 논의가 있음. “결론적으로 아무도 Copilot을 쓰고 싶어하지 않는다, 이유는 품질이 낮기 때문” 임
          + “직원들은 Copilot 대신 ChatGPT를 선호한다”는 기사가 있는데 Copilot이 GPT를 안 쓴다는 뜻인가 궁금했음. 검색해보니 GPT-4를 직접 쓴다지만, Microsoft가 얹은 래퍼(wrapper)의 문제일 수도 있음
     * Edge가 처음 나왔을 땐 마음에 들어서 잠시 기본 브라우저로 썼음. 그런데 쿠폰 오퍼 등 쓸데없는 기능이 점점 많아져서 세팅을 수동으로 다 꺼야 했고, 결국 Brave로 돌아감. 브라우저 간 동기화 기능은 좋은데, 마케팅성 광고와 상업적 홍보는 질색임. 검색에서 광고가 보인 순간 바로 리눅스로 옮겼음. 미국 정부 통계에 따르면 리눅스 데스크탑 점유율이 최근 6%까지 올랐고, Valve/Steam도 실제로 게임 호환성을 많이 개선했음. 마이크로소프트는 개발자 도구나 서비스(.Net, VS Code)에서는 좋은 기회가 많았지만, 일반 사용자에게 너무 집요하게 수익만 추구하는 점이 아쉬움. 요즘 상업 소프트웨어는 Linux용 패키징 작업(특히 AppImage/Flatpak)을 꼭 고려해야 하며, Adobe 대체재 시장도 기회가 많음
          + Vivaldi로 옮김
     * Copilot이 모든 오픈 탭의 맥락을 파악한다는 점을 언급하며, 만약 열려 있는 탭 중 품질 낮은 SEO 스팸 블로그가 많으면 AI 맥락이 오염될 것임을 우려함. 앞으로 콘텐츠 생산자가 웹에 정교한 거짓 정보까지 쏟아내면 AI가 무분별하게 그걸 인용하게 될 위험이 있음. 나는 직접 정보를 평가할 때 출처 신뢰도를 중요하게 보는데, AI는 그걸 잘못 다루는 한계가 있음. 대량의 데이터 도용 의심이 있는 AI라서 출처 속성 부여가 구조적으로 어렵다는 추정임
          + 구글은 이미 모든 사이트별 페이지 점수를 갖고 있으니, 그런 정보를 AI (LLM)와 결합하는 것 자체는 기술적으로 가능함. AI 한계라는 식의 논리는 짧은 시야임. 인공지능의 발전 예시(손가락 묘사나 코딩 등)를 들어 기술은 계속 진화함을 강조함
     * Copilot이 특정 사용자(예: Dylan)가 축구를 좋아한다는 취향을 유추해서 브라우저 실행 시 티켓을 추천하는 예시를 언급. 사용자의 행동을 본질적으로 지원하기보다는 AI 기능을 무리하게 밀어 넣는 느낌임. 따로 프롬프트하지 않아도 그냥 항상 등장함을 지적함
     * AI 시대에 기본적인 제품 관리(Product Management)가 사라진 느낌임. 이런 브라우저 자동화 같은 기능을 누가 실제로 요청했을까, 시장 적합성(product-market fit)은 어디에 있을지 의문임
          + 이건 결국 클라이언트 측 AI 스파이웨어인데, 인터넷이나 내부망 활동을 모니터링하는 도구임. 마이크로소프트의 진짜 고객이 기업·정부임을 고려하면 이런 전략도 이해됨
     * 부정적 평가가 많지만 나는 오히려 긍정적으로 보고 있음. Safari, Firefox에도 일부 플러그인은 있으나, 탭 간 통합 맥락을 인식하는 딥 인티그레이션이 아쉬웠던 사용자임. 15개 넘는 탭을 띄워놓고 리서치할 때, LLM에 전체 맥락 기반 질의를 하는 건 매력적임. 다만 내가 아직 설치하지 않은 이유는 OpenAI도 유사 기능을 준비 중이라는 소문 때문임. 곧 비슷한 기능이 나오길 기대함
     * API 없는 사이트에서 데이터 추출‧조작 자동화가 매우 힘든 과제임. 대부분의 데이터 분석업무에는 이런 내부 프로젝트가 필수로 따라옴. 웹사이트들이 서비스를 공식적으로 제공하지 않거나 기술력이 부족해 API가 없다면, 사람 직접 클릭/다운로드로 반복 작업하게 됨. 지금까지는 RPA(로보틱 프로세스 자동화) 솔루션이 그나마 대안이었음. 나쁜 의도 없이, 단순 반복 노동에서 사람을 해방시키는 효과가 더 큼
          + 공공기관 등 자원이 부족한 조직이 직접 API를 못 만드는 경우, 외부에서 개발 제안을 해서 구현해주면 모두에게 이익일 수 있음. 실제 현업에서 이런 협업이 되는지 모르긴 하지만, 현실에서 거절 당한다면 의지(게이트키핑) 문제일 수도 있다는 추측임
     * AI 스타일 인터페이스로 운영체제를 대체하는 쪽에서 혁신이 올 것이라고 봄. 지금은 모두 브라우저 단위의 반복에 그치고 있지만, 진짜 목표는 사용자의 기술과 상호작용 방식을 근본적으로 바꾸는 것임. 사실상 “필요한 게 전부 ChatGPT면 OS 전체를 채팅 인터페이스로 구현해버리는” 시도가 중요함
"
"https://news.hada.io/topic?id=22306","PR을 제출할 수 없어서, 직접 입사해 수정함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       PR을 제출할 수 없어서, 직접 입사해 수정함

     * Mintlify의 검색 race condition 문제를 1년 넘게 겪으며 개선하지 못해 불편함을 느꼈음
     * Mintlify의 검색 엔진 공급계약 업체인 Trieve 창업자임에도 벤더 신분으로는 직접 코드에 접근이 안 돼 문제를 고칠 수 없었음
     * 결국 Mintlify에 합류한 후 직접 AbortController를 사용해 검색 쿼리 중단 및 결과 동기화 문제를 해결
     * 오픈소스라면 PR로 즉시 고칠 수 있었을 것이라는 점에서 오픈소스의 실질적 장점에 대해 강조
     * 작은 불편이라도 직접 고치는 과정에서 느끼는 만족감과 제품 개선의 중요성을 다시 한 번 실감함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PR을 낼 수 없어서 직접 입사해 고친 이야기

  1년 넘게 불편했던 Mintlify 검색 버그

     * Mintlify의 검색 기능에서 race condition으로 인해 쿼리가 중복 처리되고, 사용자가 타이핑하는 도중 엉뚱한 검색 결과가 노출되는 현상이 지속됨
     * Trieve라는 검색 엔진을 Mintlify에 공급하는 벤더(외부 업체) 창업자였지만, 코드베이스 접근 권한이 없어 직접 고칠 수 없었음
     * 문제를 여러 차례 공유 Slack 채널에서 제기했으나, 우선순위가 낮아 오랫동안 방치됨
     * Trieve의 검색 경험이 Mintlify에서 좋지 않게 보일 때마다, 창업자로서 개인적 자존심과 브랜드 이미지에 부담을 느낌

  팀에 합류해 직접 해결

     * Mintlify에 합류하게 되면서 직접적으로 코드베이스에 접근할 수 있게 됨
     * 검색 함수 내에 AbortController를 구현하여, 이전 검색 쿼리를 즉시 중단하도록 개선함
     * 이제 사용자가 타이핑할 때마다, 가장 최근의 검색 결과만 반영되므로, 항상 최신 상태의 정확한 결과를 확인할 수 있음
     * 오래 불편했던 문제를 직접 고칠 수 있다는 만족감은 매우 큼
     * 트위터에 잠깐 입사해 로그인 팝업을 고친 George Hotz처럼, 문제를 보면 직접 해결하는 해커적/기업가적 자세에 의미를 둠
     * 이런 직접적이고 실질적인 문제 해결 경험이 커리어를 더 나은 방향으로 이끎

  오픈소스의 실질적 가치

     * 개인적으로 오픈 소스 소프트웨어 개발과 활용을 선호함
     * 오픈 소스라면, 외부 개발자가 직접 버그 수정이나 기능 개선 Pull Request(PR) 를 제출할 수 있는 구조였음
     * 만약 Mintlify의 검색 기능이 오픈 소스였다면, 1년 동안 지속된 문제를 즉시 PR로 해결할 수 있었을 것임
     * 폐쇄형 소스 모델의 경우, 코드 접근권이 있어야만 개선 가능하다는 한계점이 있음
     * 오픈 소스 환경의 '즉각적 권한 부여' 의 가치를 인정하면서도, 각 회사의 사업 모델 차이도 이해함

직접 개선의 보람

     * Mintlify 검색 기능이 더 매끄럽고 반응성이 좋아진 원인은 바로 이 개선 덕분임
     * 오랜 기간 마음에 걸렸던 작은 버그를 직접 고침으로써, 제품 발전에 기여하는 만족감을 느낌
     * 이번 경험을 통해, 작고 사소한 문제라도 반복적으로 고치는 과정이 제품을 더욱 훌륭하게 만든다고 느꼈음
     * 직접 고치는 작은 변화가 쌓여 사용 경험이 비약적으로 개선됨
     * 앞으로도 이러한 작은 개선의 누적을 통해 더 나은 제품을 만들고 싶음

   ㅋㅋㅋㅋㅋ 존경합니다

   상개발자

        Hacker News 의견

     * 예전에 Amazon 계정이 사기로 의심받아 정지된 적 있음, 오래전에 만든 계정이었는데 이메일과 전화번호를 여러 DB 유출로 삭제했었음, 채용된 뒤 내부적으로 Amazon의 안티-사기팀 담당자에게 연락하여 계정 해제를 빠르게 해결함, 고객지원으로 문의했을 땐 아무 소용 없었음
          + Amazon에서 가장 짜증나는 부분임, Reddit이나 다른 리뷰에서 제품을 찾으면 대개 amazon.com 링크를 타고 감, 이때 미국 달러 계정으로 전환하라고 하고, 주문하려면 다시 독일/유로 계정으로 바꿔야 함, 이 과정이 너무 번거로움, 지역마다 그냥 자유롭게 볼 수 있게 해주면 좋겠음, 주문하려고 할 때만 프로필을 바꾸라 하면 되지 않을까 생각함, 또 지역 내 판매자에게서 구매하는 옵션도 있으면 좋겠음
          + 흥미로움, Google 입사 온보딩 때 망가진 폰을 아직도 가지고 있음, 내부적으로 아무도 신경 쓰지 않음, 직접 고칠 수 있는 툴이 있었지만, 허가 없이 쓸 경우 해고된다는 메시지도 붙어 있었음
          + Facebook 계정도 9개월간 해제 실패하다가 우연히 내부 일을 하게 되면서 바로 해제 경험 있음
          + 인생에서도 이런 에픽한 순간을 겪고 싶음
          + 나도 이런 행운이 있었으면 좋겠음, Amazon 계정을 잃어버린 이유가 내 국제전화번호의 맨 앞자리 하나가 잘못 기재되어서임, 이로 인해 문자 인증을 쓸 수 없었고, OTP 앱이 깔린 폰도 바로 전에 고장남
     * Google Maps가 날 채용해서 거리 단위를 km로만 나오게 해줄 수 있다면 연락처는 내 HN 프로필에 있음, 20년간 500번은 마일에서 km로 바꾼 것 같음, 사용자를 분석하는 회사가 이런 기본을 못한다는 게 말도 안 됨
          + 500번이나 했다는 게 정말 말도 안 됨, 사용자 참여 저조로 A/B 테스트에서 실패하는 과정을 상상하게 됨, 7번 면접 보고 1개의 PR 올렸더니 이래 된 느낌임
          + 지도 전체를 “실제 크기 1cm=1cm 모드”로 확대한 채 여행 전체에 적용하게 만든 담당자를 찾아서 한 대 때려주고 싶음, 아마도 수동 확대 시 차가 화면 밖으로 벗어나게 놔두고, “다시 중심” 누르면 원래대로 강제로 확대시키는 것까지 만든 사람도 동일인일 듯, 2005년 내비게이션도 이런 건 다 해결했었음
          + 멕시코 여행 중에 로그인했는데도 Google Flights가 탭을 새로 열 때마다 달러에서 페소로 바꿔버림, 진짜 신경 안 쓰는 듯함
          + Google에서 10년 전에 일할 때 내부 피드백 폼으로 이 문제를 리포트했는데 아무 답이 없었음, 그 후 매년 Google Maps 피드백으로 이 버그 신고했음, 어떤 해에는 두 번이나 보냄, 이제는 진짜 부끄러운 버그가 됐음
          + gmaps 관련 일도 관심 있음, 1시간 이상 걸리는 여정은 미리 돌아오는 길도 캐시해 줬으면 좋겠음, 서비스 신호 안 터질 때 길을 기억해야 해서 곤란할 때가 많음
     * 실제로 오래된 농담이 현실이 되는 걸 보니 재밌음, (또) 왼쪽 마진을 추가해달라는 요청 있음, 화면 맨 끝에서 텍스트 읽기가 조금 어색하게 느껴짐
          + 글에 나온 링크를 따라가다 보니, Apple에서 만료된 패스를 자동 삭제하는 기능을 도입하고 그 직후 바로 퇴사한 사례를 봤음, 이제 그 기능을 쓸 때마다 마음속으로 감사 인사를 해야겠다는 생각이 듦, 진짜 짜증나던 문제였음
          + 직접 OP에게 고용되어 고치는 게 낫지 않을까 농담
          + 나는 화면 공간을 낭비하는 사이트를 별로 좋아하지 않음
          + 나는 왼쪽 정렬이 좋음, 그게 원래 텍스트가 있어야 할 자리라고 생각함
     * debounced search function에 AbortController를 추가해서 사용자가 새로운 입력을 할 때마다 이전 쿼리를 중단하도록 했다고 설명함, 사용자가 아직 다 입력하지 않았는데 필터나 검색이 먼저 적용돼버리는 게 가장 짜증남, 입력이 완전히 끝날 때까지 기다려줬으면 좋겠음
          + Grafana 로그 검색은 현재 적용 중인 로그 필터를 바꾸면 문자 하나 바꿀 때마다 조회 건수로 과금함, 이런 이유로 내 UX 사용 습관을 바꿔야 했음, 찾는 문자열 전체가 아니라 문자수만큼 비용 청구가 되는 것 때문임
          + 내 블로그에 검색-실시간 기능을 구현할 때는, 이전 검색 제안을 다 완료하고 새로운 요청을 보내도록 했음, 서버 부하를 막고 반응성도 유지할 수 있는 합리적인 방법이라고 생각함
          + 특히 예약 사이트에서 이런 게 싫음, 필터들이 왼쪽 사이드바에 나오는데, 화면에 다 안 보이면 매번 조작할 때마다 위로 스크롤되고, 로딩되고, 필터가 읽기 전용으로 바뀌어서 다 끝날 때까지 기다렸다가 겨우 다음서야 추가할 수 있음
          + 좋은 타협점은 사용자가 입력을 멈춘 후 몇백 밀리초 정도 대기했다가 쿼리를 보내는 방식이 적합하다고 생각함, 혹은 쿼리는 보내되 입력이 멈출 때까지 결과를 표시하지 않는 방법도 고려할 만함
          + 이런 행동은 정말 싫음, fancy한 코드 에디터에서 글자 칠 때마다 경고음이 뜨는 것과 같음, 'i'와 'f' 쳤을 때 바로 “if-then 문이 닫히지 않았어요!”라고 뜨면, 아직 입력 중이라는 걸 알면서도 너무 과민반응임, 다 써야 알림이 나오는 게 낫고, 대부분 언어나 툴에서 이런 실시간 알림은 꺼두고 빌드/실행시에만 오류를 보게 바꿈, LSP(언어 서버 프로토콜)는 진정해야 함
     * 지금은 버그가 신경 쓰이면 채용돼서 직접 고치는 게 더 쉬울 정도로 소프트웨어 품질이 낮음, GTA 5 로딩 문제를 해결한 프로그래머 이야기가 떠오름, 그렇게 돈이 많은 GTA 5도 품질 개선이 쉽지 않았음
          + 품질 문제가 아니라 우선순위 문제로 봄, 회사는 사용자가 원하는 것보다 팀이 하고 싶은 걸 먼저 선택함, 실사용자 테스트나 데이터 실험이 부족함, 실제로 이 사례는 문제의 해결책이라기보다 실제 문제의 단면임, 아마도 훨씬 유용한 이슈들이 많을 텐데 한 명이 본인이 원한 기능만 추가한 상황임
          + GTA:O의 카드에 돈을 많이 쓰는 사람들은 로딩 타임에 관심이 없었음, 나는 화가 나서 시간 재보니 실제 미션보다 로딩 스크린을 더 오래 보는 것을 알고 게임을 아예 그만뒀음
     * 혹시 인터넷 밈 속의 그 사람이 바로 너냐는 농담
     * 기사 내용에서 채용 파트가 전혀 언급되지 않아 궁금함, “뭔가가 나를 괴롭혔고, 회사에 입사한 덕분에 고쳤다” 수준으로 보여서 이야기의 핵심이 빠진 느낌임
          + 저자의 회사가 현재 재직 중인 곳에 인수된 것 같아 acquihire(인수+채용)로 추정하고 있음
     * 반대로, 나는 예전에 IP 변호사들 때문에 오픈소스 코드에 PR을 등록하는 게 불가능했던 직장에서 근무한 경험 있음, 그래도 정확한 입력값과 버그 라인 번호를 자세히 설명하면 누군가가 직접 고쳐주도록 설득할 수 있었음, 코드 대신 QA(품질보증) 리포트는 무료로 제공했다는 느낌임
     * George Hotz가 2022년 Twitter에 잠시 합류해서 로그인 팝업 제거 문제를 해결했다는 전설적인 사건 이야기가 떠오른다는 의견임, 하지만 내 기억과 다름, George Hotz는 “검색을 고칠 수 있다”고 주장하다 거의 바로 나갔고, 마지막에 위안삼아 팝업만 제거함
          + 이번에 George Hotz, 특허 괴물, 그리고 comma.ai에 대해 다시 조사하는 데 잠깐 몰입하게 됨, George Hotz의 Comma.AI는 $999짜리 “comma 3x” 스마트폰과 OBD-II 커넥터, $99짜리 배선 하네스로 최근 10년 이내 제조된 대부분 자동차(심지어 Tesla도)에 Autopilot급 기능을 추가할 수 있음, 전체 비용 $1098, 오픈소스로 GitHub에 올라 있고 심지어 차량에 ssh 접속도 지원함, 선택형 클라우드 구독은 $10/월(직접 SIM) 또는 $24/월(셀러 데이터 포함), 다만 Tesla Sentry Mode에 해당하는 기능은 아직 없고, 이슈 #29912로 남아 있음, 오리지널 Tesla의 Sentry Mode가 250W를 씀 — 80kWh 배터리 기준 7일 만에 80%에서 30%로 떨어짐 — openpilot이 5W 이하만 쓸 거라면 훨씬 효율적일 거라고 생각함
          + George Hotz는 Twitter에 입사했다고 떠벌리고는 아무것도 안 하고 조용히 떠났다고 기억함, 민망하고 본인 스스로 자초한 일이라고 생각함
          + 글을 더 정확하게 고쳤으며, 동시에 Github Pages 버그로 블로그 자체는 내렸다고 이야기함
          + 마지막엔 Twitter 전체를 새로 만들자는 제안을 Elon에게 했던 것으로 기억함
     * 개인적으로 Discord에 입사 지원까지 하면서 거대 이모지를 토글 설정으로 바꾸는 PR을 내보려 했음, 나뿐 아니라 서버 전체가 애타게 요청하고 있음
          + 소용 있을진 모르겠으나 나 스스로는 이모지 다음에 마침표를 붙여서 임시로 피하고 있음, 새로운 유저에겐 소용 없지만 내겐 임시방편임
          + Discord는 Electron 앱이라서 이론상 클라이언트 측 모드로 바꿀 수 있을 듯함, 다만 계정 정지 위험은 잘 모르겠음
          + 의미하는 바를 묻는 코멘트도 있음, 예를 들어 :) 이모티콘의 자동 변환을 끄는 옵션은 있다고 설명함
"
"https://news.hada.io/topic?id=22245","Iceberg 테이블 포맷, 좋은 아이디어 - 잘못 정의된 명세 - Part 2 of 2","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Iceberg 테이블 포맷, 좋은 아이디어 - 잘못 정의된 명세 - Part 2 of 2

    아이스버그 스펙의 문제점

     * 아이스버그 스펙의 심각한 문제: 아이스버그 스펙은 대규모 데이터 레이크의 메타데이터 문제를 해결하기 위한 진지한 시도가 아니다.
     * 역사적 교훈: 첫 번째 부분에서 과거의 교훈을 배우기 위해 역사로 돌아갔으며, 데이터베이스가 해결해야 할 ""공간 관리 문제""를 소개하였다.
          + 문제의 요소:
              1. 공간의 단편화
              2. 세분화된 동시성 제어
              3. 여러 객체에 대한 원자성
              4. 행과 파일 간의 임피던스 불일치
              5. 메타데이터에 대한 낮은 대기 시간과 높은 처리량 접근
     * 개방형 포맷의 중요성: 모든 스택에서 개방형 포맷을 사용하는 것에 100% 찬성하며, 테이블 구조화된 데이터에 대해 파케이(Parquet)를 공통 저장 포맷으로 사용하는 것에 대해 특히 기대하고 있다.
     * 파케이 파일과 데이터베이스 테이블의 차이: 파케이 파일이 모여 있다고 해서 그것이 데이터베이스 테이블이 되는 것은 아니다. 데이터베이스 테이블은 단순한 행의 집합 이상이다.

    메타데이터 문제 요약

     * 공간 관리 문제: 데이터베이스가 해결해야 할 여러 문제를 다시 강조하였다.
     * 메타데이터의 필요성: 여러 파케이 파일을 데이터베이스 테이블처럼 보이게 하려면 다음과 같은 메타데이터가 필요하다:
         1. 테이블을 구성하는 모든 파일의 위치 목록
         2. 각 파일에 대한 대략적인 메타데이터
         3. 파일을 추가하거나 제거할 수 있는 트랜잭션 제어 메커니즘
         4. 테이블 스키마를 진화시킬 수 있는 방법
     * 파일 위치 찾기: 파케이 파일 세트를 단일 테이블로 취급하려면 파일을 찾는 메커니즘이 필요하다.
     * 메타데이터의 중요성: 각 파일은 흥미로운 행을 빠르게 찾기 위해 추가 메타데이터가 필요하다.

    파케이 파일과 데이터베이스 테이블

     * 파케이 파일의 정의: 파케이(Parquet)는 자기 설명적이고 표 형식의 데이터 포맷을 제공한다.
     * 데이터베이스 테이블의 정의: 데이터베이스 테이블은 단순히 행의 집합이 아니라, 여러 메타데이터와 트랜잭션 제어가 필요하다.
     * 파케이 파일을 테이블처럼 사용하기 위한 조건:
         1. 파일 위치 목록
         2. 각 파일의 메타데이터
         3. 파일 추가 및 제거를 위한 트랜잭션 제어 메커니즘
         4. 테이블 스키마의 진화 방법
     * 파일과 테이블의 차이: 파케이 파일이 같은 열 레이아웃을 가진다고 해서 데이터베이스 테이블처럼 보이지 않는다.

    매니페스트 파일과 리스트

     * 데이터 추가 과정: 아이스버그 클라이언트가 테이블에 데이터를 추가하려면 다음 단계를 거쳐야 한다:
         1. 하나 이상의 파케이 파일을 특정 위치(예: S3)에 작성한다.
         2. 1단계에서 작성한 파일을 가리키는 매니페스트 파일을 작성한다.
         3. 새로운 매니페스트 리스트를 작성한다.
     * 매니페스트 파일의 형식: 매니페스트 파일과 리스트는 AVRO 형식으로 되어 있으며, 이는 압축되고 자기 설명적이다.
     * 매니페스트 파일의 내용: 매니페스트 파일은 파케이 파일에 대한 포인터와 각 열에 대한 메타데이터를 포함한다.
     * 메타데이터의 크기 문제: 메타데이터를 이렇게 저장하면 필요 이상으로 커지며, 파일 간의 공통 문자열 값을 인식하여 압축할 수 없다.

    클라이언트의 부담 증가

     * 클라이언트의 책임: 아이스버그 스펙 전반에 걸쳐 클라이언트는 간단한 변경을 위해 엄청난 양의 기록 관리를 해야 한다.
     * 메타데이터의 정확성 문제: 클라이언트가 일부를 잘못 작성하면 새로운 스냅샷의 커밋이 철저히 확인해야 하며, 매니페스트 데이터가 올바르게 작성되었는지 확인해야 한다.
     * 보안 문제: 클라이언트가 모든 매니페스트 파일을 가리키는 매니페스트 리스트를 작성해야 하므로, 모든 S3 파일의 위치가 유출된다.
     * 데이터 보안의 중요성: 데이터의 가치가 높기 때문에 스펙이 보안을 최우선으로 다루지 않는 이유를 의문시해야 한다.

    행 보안의 결함

     * 행 보안의 필요성: 미국과 같은 느슨하게 규제된 국가에서도 민감한 데이터를 보호하기 위해 행 수준의 보안이 필요하다.
     * EU의 GDPR: 유럽에서는 GDPR과 같은 법률로 인해 데이터 접근에 대해 더욱 민감해야 한다.
     * 클라이언트의 데이터 접근 문제: 클라이언트가 테이블에 데이터를 추가할 수 있지만, 이미 존재하는 데이터에 대한 접근을 제한할 수 없다.
     * 보안 문제의 심각성: 스펙이 보안을 우선적으로 다루지 않는 이유는 데이터 레이크 정보의 가치에 대한 의문을 제기해야 한다.

    메타데이터 파일의 역할

     * 메타데이터 파일의 작성: 클라이언트가 파케이 파일을 작성한 후, 해당 매니페스트 파일을 생성하고 기존 매니페스트 리스트를 읽고 새로운 매니페스트 리스트를 생성한 후, 데이터를 커밋해야 한다.
     * 커밋 과정: 커밋은 메타데이터 파일(<prefix>.metadata.json)을 작성하여 이루어진다.
     * JSON 형식의 선택: 메타데이터 파일이 JSON 형식인 이유는 불분명하며, 이는 ""위원회에 의한 설계""의 느낌을 준다.
     * 메타데이터의 반복성: 메타데이터 파일은 모든 스냅샷을 나열하며, 이는 정보의 반복으로 인해 공간이 낭비된다.

    커밋 과정의 복잡성

     * 원자성 문제: 새로운 메타데이터 파일을 최신 파일로 만들고 이전 메타데이터 파일과 원자적으로 교체해야 한다.
     * 커밋 절차의 복잡성: 새로운 메타데이터 버전 V+1을 커밋하기 위해 다음 단계를 수행해야 한다:
         1. 현재 메타데이터를 기반으로 새로운 테이블 메타데이터 파일을 생성한다.
         2. 새로운 테이블 메타데이터를 고유한 파일에 작성한다.
         3. 메타스토어에 요청하여 테이블의 메타데이터 포인터를 V에서 V+1로 교체한다.
     * 스왑 실패 시 처리: 스왑이 실패하면 다른 작성자가 이미 V+1을 생성한 것이므로, 클라이언트는 다시 1단계로 돌아가야 한다.
     * 경쟁 조건의 문제: 클라이언트가 경쟁하는 경우, 이전 클라이언트가 작성한 메타데이터 파일을 다시 읽고 매니페스트 리스트와 메타데이터 파일을 재생성해야 한다.

    낙관적 동시성 제어의 문제

     * 동시성의 사실: 자원에 대한 경쟁이 예상되지 않는 경우, 어떤 종류의 동시성을 사용하든 상관없다.
     * 경쟁이 예상되는 경우: 두 클라이언트가 동일한 값을 변경하려고 할 경우, 잠금 메커니즘을 사용해야 한다.
     * 낙관적 동시성 제어의 한계: 아이스버그에서는 두 개의 동시 쓰기가 항상 충돌하게 되며, 이는 스펙의 설계 방식 때문이다.
     * 최악의 잠금 의미: 메타데이터에 대한 최악의 잠금 의미를 사용하고 있으며, 데이터 추가만 원할 경우 클라이언트 간의 조정이 필요 없다.

    메타데이터 스왑의 한계

     * 메타데이터 중앙 집중화: 테이블의 메타데이터를 단일 파일에 중앙 집중화함으로써 모든 쓰기에 대한 단일 경쟁 지점을 생성하였다.
     * 재시도 시 클라이언트의 부담: 클라이언트가 실패할 경우, 이전 클라이언트가 작성한 데이터를 읽고 매니페스트 리스트와 메타데이터 파일을 재생성해야 한다.
     * 메타데이터 스왑의 속도: 메타데이터 스왑을 수행하는 서비스는 재시도와 함께 처리해야 하며, 이는 성능 저하를 초래한다.
     * 제한된 커밋 수: 단순한 동시성 구현으로 인해 커밋 수가 제한되며, 이는 메타데이터 파일의 원자적 교체 시간에 의해 제한된다.

    데이터베이스의 필요성

     * 메타데이터 파일의 위치 찾기: 아이스버그 테이블 스냅샷은 메타데이터.json 파일로 완전히 설명된다.
     * 아이디어의 모순: 아이스버그는 파일만으로 메타데이터 형식을 지정하려고 하지만, 결국 데이터베이스가 필요하다.
     * 데이터베이스의 장점: 현대 데이터베이스는 수십만 건의 쓰기를 처리할 수 있으며, 분산형으로 확장할 수 있다.
     * 파일 시스템과 데이터베이스의 비교: 메타데이터를 파일에 저장하는 것보다 데이터베이스에 저장하는 것이 더 효율적이다.

    조각화 및 메타데이터 부풀림

     * 메타데이터.json 파일의 성장: 메타데이터.json 파일은 최신 스냅샷을 가리키며, 각 메타데이터 파일은 이전 스냅샷에 대한 역 포인터를 포함한다.
     * 메타데이터의 지속적 증가: 메타데이터가 지속적으로 증가하며, 이는 데이터 레이크의 성능에 부정적인 영향을 미친다.
     * 정기적인 메타데이터 정리 필요성: 데이터 레이크에 지속적으로 데이터를 추가하는 경우, 메타데이터를 정리해야 한다.
     * HTTP 요청 비용: 메타데이터 파일을 삭제하는 과정에서 HTTP 요청이 발생하며, 이는 비용이 발생할 수 있다.

    데이터 읽기 및 쿼리 계획

     * 매니페스트 파일의 역할: 매니페스트 파일은 파케이 파일에 대한 메타데이터를 포함하고 있다.
     * 쿼리 계획의 복잡성: 쿼리를 실행하기 위해 매니페스트 리스트와 매니페스트 파일을 순차적으로 읽어야 한다.
     * 비용 문제: S3에서의 읽기 비용이 발생하며, 이는 쿼리 실행 속도에 영향을 미친다.
     * 메타데이터의 단편화 문제: 메타데이터가 단편화되면 쿼리 계획이 복잡해지고, 데이터 접근이 어려워진다.

    캐싱과 쿼리 계획의 어려움

     * 매니페스트 캐싱: 매니페스트를 캐시할 수 있지만, 이는 메타데이터의 크기가 커지기 때문에 비효율적이다.
     * 캐시의 유효성 유지: 캐시가 최신 상태인지 확인해야 하며, 이는 추가적인 비용과 복잡성을 초래한다.
     * 클라이언트의 부담: 모든 클라이언트가 메타데이터를 캐시해야 하며, 이는 수백만 개의 HTTP 요청을 발생시킨다.
     * 복잡성의 증가: 데이터 레이크의 사용이 복잡해지며, 이를 해결하기 위한 추가적인 솔루션이 필요하다.

    아이디어의 결론

     * 아이디어의 비판: 아이스버그 스펙은 데이터 레이크 메타데이터에 대한 진지한 스펙이 아니며, 여러 문제를 안고 있다.
     * 문제의 요약: 아이스버그는 O(n) 작업을 사용하여 메타데이터를 추가하며, 교차 테이블 커밋을 처리하지 못하고, 메타데이터 부풀림 문제를 해결하지 못한다.
     * 스케일링의 한계: 아이스버그는 스케일링에 적합하지 않으며, 클라이언트에게 과도한 복잡성을 이동시킨다.
     * 산업에 대한 질문: IT 산업에서 이러한 문제가 발생하는 이유에 대해 질문을 던진다.
"
"https://news.hada.io/topic?id=22295","취소의 숨겨진 영향","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               취소의 숨겨진 영향

     * 저자는 온라인 취소 경험을 처음으로 공개적으로 설명함
     * 고발 이후 즉시 사회적·직업적 고립, 실직, 재정적 파탄, 건강 문제, 노숙 등 심각한 삶의 변화가 발생함
     * 법적 조치 결과 명예훼손 혐의는 일부 해소되었으나, 평판 피해와 사회적 배제는 여전함
     * 이 경험을 통해 정신적 트라우마와 취소 문화가 개인에게 미치는 위험성을 강조하고 있음
     * 커뮤니티 내 집단적 비난과 사회적 배제의 심각한 결과를 경고하며, 신중한 행동을 촉구함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

경고 및 서론

     이 글 후반부에는 자살에 대한 언급이 있으므로, 주의가 필요한 사람은 신중히 읽기 바람

     * 저자는 취소가 타겟을 침묵시킨다는 점 때문에 그 영향을 처음으로 밝히기로 결심함
     * 4년간의 경험을 통해 온라인 집단정의의 피해자를 위한 이야기 공유 목적을 설명함

취소 전의 삶과 Scala 커뮤니티

     * 저자는 Scala 개발자 커뮤니티의 핵심 멤버로 활동하며, 개발자·행사 주최자·연사로 활약했음
     * Scala는 X(Twitter)·대형은행 등 다양한 기업이 사용하는 기술적으로 강력한 프로그래밍 언어임
     * 취소 사건 이후 즉시 커뮤니티 내 평판·직업·수입·주거·연금까지 한순간에 상실 경험을 겪음

취소의 순간과 심리적 충격

     * 2021년 4월, 두 명의 여성의 동시다발적 블로그 게시글을 통해 성추행 의혹이 공개 고발됨
     * 곧바로 영향력 있는 인사가 공동 서명한 오픈레터가 커뮤니티에 퍼지며 저자와의 인간관계가 단절됨
     * 누구와도 사전 논의 없이 기습적으로 진행되어, 저자는 충격과 외로움, 그리고 무력감을 체험함
     * 오랜 친구들의 서명, 공개적 비난이 심각한 심리적 고통을 야기함
     * 일부 지원 메시지도 있었으나, 압도적 비난에 묻혀버림

사회적 고립과 인간관계 단절

     * 친분이 있던 다수 인물들과의 관계가 단숨에 소멸됨
     * 저자에 대한 우호적 블로그글 등이 일부 있었으나, 거의 관심받지 못함
     * 친구들과 연락 시도조차 두려워하는 고립감이 커짐
     * Scala 커뮤니티가 개인의 사회·전문·사적 삶의 중심이었던 만큼 상실감이 매우 컸음

커리어, 오픈소스, 삶의 붕괴

     * 개발자 홍보대사 직에서 자진 사임, 오픈소스 프로젝트 양도, 계획 중인 교육 및 자선 프로젝트 중단
     * 주요 온라인 기록 및 교육자료에서 이름이 지워지는 등 완전한 사회적 배제 경험
     * 일상 소득, 인간관계, 참여하던 모든 커뮤니티에서 배제되어 재정적으로 심각한 타격을 입음
     * 연인과의 관계도 장기적으로 변하게 됨

정신적 재구성(Recontextualization) 과정과 트라우마

     * 평소 친밀했던 커뮤니티의 돌변에 따른 삶의 의미 재정립 과정을 거침
     * 모욕적 편견과 집단적 행위에 대한 자문, 깊은 피로감과 정신적 부담 경험
     * 적극적으로 해명이나 대응을 할 기운조차 없었음

팬데믹과 재정적 고난

     * 코로나19 팬데믹으로 인해 컨퍼런스·교육 사업이 모두 중단되어 경제적 기반이 사라짐
     * 손실을 메우기 위한 시도에도 예기치 않은 취소설 및 의혹 제기로 취업기회 상실
     * 조사는 무혐의로 종결됐으나, 4개월 이상 무급 상태로 생계 곤란 경험

취업의 한계와 악순환

     * Scala와 관련된 최장 경력자 중 한 명임에도, 평판 손실로 구직 거의 불가능
     * 이력서 제출 시 바로 인터넷 검색으로 의혹을 접하게 되어, 업체 입장에서도 채용을 꺼리게 됨
     * 다른 언어나 커뮤니티로 이동해도 명예 불명예가 따라다니는 상황
     * 3년간 연평균 수입 $14,600선으로 곤란한 상황에서, 친구 도움과 자산 매각으로 겨우 생계 유지

도피와 노숙 경험

     * 2022년 법적 소송을 시작했으나, 법률비용과 임대료 부담 누적으로 결국 노숙 상태에 내몰림
     * Camino de Santiago 순례길을 도보로 걷는 삶을 택하며 생계 문제를 잠시 “도피”로 감춤
     * 절박한 선택 과정에서 현실을 감추는 자기기만성도 느꼈음
     * 6주간 800km 이상을 걷고, 친구와 새 인연을 만들기도 했으나, 삶의 기반을 복구하지는 못함
     * 최종적으로 법적 분쟁은 2024년 초 화해했으나, 실질적 명예회복기회는 놓침

인내와 심리적 후유증, 자기회복

     * 오픈소스 소프트웨어 작업이 유일한 정신적 버팀목이었으며 현실을 견딜 기회를 제공함
     * PTSD(외상후 스트레스 장애) 일부 증상이 3년 후 시작되어, 일상에 일부 영향을 줌
     * 하지만 꾸준한 자기 인내와 소프트웨어 개발로 재기 가능성 모색

취소문화의 위험성과 교훈

     * “취소문화”의 피해 당사자가 된 경험을 바탕으로 사회적 고립·집단배제의 심각한 심리적 위험을 경고함
     * 2024년 영국 옥스퍼드 대학생 사례처럼, 공동체 내 배제행위에 대한 근본적 성찰 필요성 제기
     * 집단적 비난 및 권위자 연대의 위험을 인식하고, 모두가 신중할 필요 강조

요청 및 결론

     * 허위의혹의 인기로 인해 본질이 왜곡되고, 희생자에게 반복적 피해가 지속됨을 지적
     * 당사자 명예회복 위해 공개서명자들에게 이름 삭제를 간곡히 요청
     * 실수에서 배우고 수정하려는 태도, 혐오 아닌 교정의 문화 필요성 주장
     * 여전히 인간의 선의에 대한 희망과 긍정적 세계관을 지님

❧

        Hacker News 의견

     * 사실 이런 경험들은 나도 모르게 내가 타인과 상호작용하는 방식을 바꾸는 계기가 되었음, 특히 남자인 입장에서 여성이나 아이와 대화할 때 더 신경 쓰게 됨, 예전에 부모님과 아이들과 함께 놀이터에 간 적이 있었는데, 다른 여자아이가 다쳐서 울고 있어서 아버지가 걱정되어 아이에게 다가가 괜찮냐고 질문하고, 보호자를 찾으려고 주변 어른들에게 물어봤음, 그런데 어떤 여성분이 비난조로 ""저기서 애를 괴롭히는 걸 봤다""고 하더니, 아버지는 잘못 엮이면 인생이 망가질 수 있다는 생각에 바로 손을 들고 아무 말도 없이 자리를 떠났음, 그 아이는 계속 혼자 울고 있었음, 그날 이후 나는 항상 내 행동이 오해받을 수 있다는 것을 염두에 두게 되었고, 놀이터에 가면 내 아이 외에 모르는 아이와는 절대 어떤 이유로도 이야기하지 않고 그냥 내 아이와만
       놀아주게 되었음
          + 이런 상황에서는 당당하게 ""아니요, 저는 다친 아이가 부모님을 찾을 수 있도록 도와주는 중입니다, 그 아이의 부모님이시면 말씀해주시고 아니시면 경찰이나 아동보호기관에 연락하겠습니다""라고 이야기하는 게 맞음, 바로 911에 전화를 걸고, 다친 아이가 혼자 있고, 보호자를 찾으려 하니 주변에서 적대적인 반응이 있어서 사회복지사가 아이를 데려가 부모를 찾을 때까지 돌봐달라고 요청하면 됨, 아이에게 말 거는 것 자체는 불법이 아님, 그런 사람들에게 잘해주려다 보면 오히려 힘을 실어줄 뿐임, 경찰이 와서 부모가 신분증 보여주고 아이 데려가게 한 번쯤 배워보길 바라게 됨, 그리고 왜 HN에서는 이런 놀이터에서의 오해 사례 이야기가 자주 나오는지 모르겠음, 나는 근처 놀이터에서 점심도 자주 먹었지만 별말 듣지 않았고 오히려 공이 날아갔을
            때 공 달라고 하는 정도였음
          + 저런 식으로 바로 사람을 의심한다면 그 뇌가 맛이 간 걸로 보임, 어떤 동네에 사는 사람인가, 제대로 된 상식은 사라졌는지 궁금함
          + 정말 안타까운 일임, 나는 놀이터에서 다른 부모들과 거의 항상 긍정적으로 소통했음, 문화가 좀 다른 곳이 아닐까 생각함
          + 혼자만 그런 게 아님, 이제 많은 남성들이 그 위험을 인식하고 자신을 보호하기 위해 행동과 환경 자체를 바꿔가고 있음, 증인 있음을 항상 확인하거나, 아예 상호작용을 하지 않는 식으로 변화하고 있음
          + 나는 작은 마을에서 자랐는데, 그런 곳은 공동체 결속이 최우선임, 어릴 땐 내가 옳다고 생각하면 뭐든 논리적으로 설득할 수 있을 줄 알았지만, 실제로는 군중심리가 순식간에 폭발하고 우발적으로 과격해질 수 있다는 걸 깨달았음, 개인이랑 있을 때만 겨우 타협이라도 되지, 다수가 모이면 감정적이고 예측불가함, 리더십 있는 사람 쪽으로 쉽게 쏠림, 나이가 들수록 사람 자체가 싫어지고 왜 노인들이 냉소적으로 되는지 깨닫게 됨, 되도록 빨리 은퇴해서 남들과 떨어져 나와 인터넷만 하며 조용히 살고 싶음, 금전적 독립을 통해 이런 치졸한 인간관계 게임에 노출되지 않고 싶음, 그래도 쓸 만한 사람을 찾으려 계속 노력하지만 매우 힘듦
     * 누군가를 섣불리 판단하는 것을 변명하려는 건 아니지만, 이 사례는 절차적 공정성(due process)을 지키는 게 왜 중요한지 잘 보여줌, 절차적 공정성이 돈, 권력, 무능력 등 때문에 쉽게 망가지는 경우를 사람들이 많이 보고 직접 겪기도 했음, 절차가 불투명하고 복잡하고 오래 걸리다 보니, 사람들이 직접 자기 기준으로 판단하는 경향이 생긴 것임, 나도 정보를 받자마자 즉시 판단하는 본능을 억누르려고 노력하게 됨, 예전에는 너무나 확신하며 판단했다가 시간이 지나 완전히 틀렸음을 깨달은 적이 많았음, 논리적으로 철저한 사람도 자기 착각에 빠질 수 있다는 점이 무서웠음, 정보 받은 당일에는 중요한 결정을 내리지 않는 걸 원칙으로 삼음, 정보를 조금 소화하고 나면 내 생각이 얼마나 바뀌는지 스스로도 놀람
          + 사람들이 시간이 지나면서 절차적 공정성이 훼손된 사례를 많이 보고 직접 겪은 건 맞긴 한데, 이번 경우에는 그런 문제가 아니라 사람들이 ""옳은 일을 한다는 명분""으로 누군가를 학대하는 걸 매우 즐긴다고 봄, 도덕적 쾌감에 빠지기 때문임
          + 판단 본능을 억누르는 것이 정말 중요함, 우리 가족은 정말 직감이 뛰어나서 “맞추는 것이 싫다”는 게 우리 집안의 좌우명일 정도임, 항상 내 판단이 옳다고 믿기 쉬운데, 최소한 5%는 내가 완전히 틀림, 그 5% 덕분에 사람을 더 잘 볼 수 있게 배움, 이걸 조절하지 않으면 그 5%가 50%로 금방 늘어남
          + 이 주제가 나에게도 아주 중요함, 우리는 모두 자기 인생에만 살고 있고, 남들의 사연을 읽고 들어도 세상의 전체 범위를 배우기는 쉽지 않음, 모든 것을 이해하고 완전히 판단할 수 있다고 믿는 건 오만함임, 옳고 그름이 없다는 게 아니라, 판단 보류가 필요한 상황도 있다고 생각함, 나 자신을 다잡기 위해 생각하는 주제지만 다른 사람에게도 도움이 됨
     * 해당 저자의 상황은 잘 모르지만, 성희롱 사건이 벌어지는 걸 직접 지켜본 적 있음, 사람들이 사건의 내용을 거의 확인도 하지 않고 즉각적으로 피의자를 적으로 돌리는 속도가 굉장히 빨랐음, 그런데 고발인 쪽의 이야기에 이상점이 있어 여러 번 이야기가 바뀌었고, 결국 고발자가 거짓말과 조작을 한다는 사실이 드러났음, 주변 사람들이 그 점을 알게 되면서 실제로는 상황이 바로 무너졌음, 하지만 소문은 정말 빨리 퍼지고, 몇 년이 지난 지금도 대부분의 사람들은 처음 들은 내용만 기억함, 대부분은 그냥 위험하다 싶어서 피의자와 거리를 뒀고, 일부 사람들은 사건의 세부사항엔 관심도 없고, 그저 상징적으로 뭔가 큰 의미를 갖는다고 믿으며 무작정 고발인을 믿어야 한다고 생각함, 사회적으로 아주 이상한 현상임, 한 명의 인생이 사회적 핵폭탄을
       맞는 걸 지켜보는 듯했고, 그 사람은 자기 인생을 방어할 힘이 전혀 없었음, 다행히 직장이나 경력으로는 번지지 않았고 가까운 지인들은 곁에 남았지만, 여전히 오랜 시간이 지나도 ‘그냥 친구의 친구가 이상하다고 했다’는 소문 하나로 이상한 사람으로 기억됨
          + 사람들이 상황의 세부사항에는 관심을 두지 않고, 그저 집단의 대표로서만 상대를 여길 때 발생하는 일임, 개인의 실체를 제대로 못 보는 현상임
          + The Hunt (2013) 영화가 떠오름
          + 나는 네 번쯤 이런 상황을 직접 본 적 있음, 한 번은 완전히 공개적으로, 실제 현실에서 매우 공적인 자리였음, 보통은 즉시 편을 드는 사람은 거의 없고, 대부분은 ""관여하지 말자"" 쪽을 택함, 한 명 예외는 이런 일을 여러 번 겪은 동네 활동가였음, 셋은 직접 내 발로 진상조사를 아주 꼼꼼히 해봄, 결국 대부분 본질은, 누가 뭘 진짜 했는지 얘기를 나눠보면 바로 드러남
     * 2020~2021년 최고조였던 ‘캔슬’ 열풍도 요즘은 좀 잠잠해졌지만, 그 시기에는 항상 “어차피 다른 데서 일하면 되니까 문제없다”는 말이 따라다녔음, 마치 직장이 전부인 것처럼 이야기하지만 사람은 사회적 존재이기 때문에, 심지어 모르는 사람들이 단체로 배척할 때에도 그 영향력이 엄청 큼, 물론 나쁜 일을 한 사람은 관계를 끊어야 하겠지만, 현실의 많은 캔슬은 그런 것과 다름, 빈약한 근거로, 아무것도 모르는 사람에게까지 적용됨, 대부분의 캔슬은 일종의 피의식 동맹이나 권력놀이에 가깝고, 실제 피해자나 가해자도 그리 중요하지 않은 경우가 많음
          + 그래서 보통 ""캔슬""된 사람이 사과해도 아무 의미가 없는 것임, 사과문을 보면 대부분 ""이건 진심이 아니다"", ""홍보 담당자가 대신 썼을 것""이란 댓글만 가득함
          + 반대로, 캔슬하는 쪽 입장에서도 복수심에 집착해서 자기 자신에게 해가 되지 않을까 궁금함, 용서의 심리에 대해 읽어보면, 상황에 따라 그냥 내려놓는 게 더 장점인 경우도 많음
          + “어차피 다른 곳에서 일하면 되지 않나?”라면서 그 사람이 어디서도 일 못 하게 만들려 애쓰는 모습이 심하게 모순적이었음
          + 2020~2021년에 비해 사회가 성숙해져서 이제 이런 식의 도핑 앱 상상 같은 일에 무관심해진 게 다행임
     * 누가 진실을 말하고 있는지는 나도 모르고, 직접 사건의 당사자나 피해자가 아닌 이상, 여기 있는 누구도 모름, 이런 식으로 형사범죄에 해당하지 않는 문제의 경우에는 사건과 무관한 제3자가 철저하고 독립적으로 조사해, 주장의 근거를 확인하고, 그것이 정당한지 판단해, 결과를 공개적으로 보고해야 함, 지금까지는 그런 조사가 이루어진 적도 없는 것으로 보임, 다만 과거에 많은 남성들이 공동체 내에서의 권력을 이용해 여성을 이용한 사례가 많았기 때문에, 만약 피해자 주장들이 사실이라면 전혀 놀랍지 않음, 법원 판단은 민사 사건일 뿐이고, 누구도 ‘무죄’ 판정을 받은 건 아님, 별도의 조사가 없는 상황에서 각자가 직접 여성 1의 진술과 여성 2의 진술을 읽어보고, 불완전한 자료로 각자 생각을 해볼 수밖에 없음
          + 너무 적은 정보를 바탕으로 직접 누가 진실을 말하는지 판단해보라는 건 위험함, 아는 게 적은 외부인은 결국 ""주변에서 유사한 일이 있었으니 나는 여기선 X쪽을 믿겠다"" 같은 비논리적 편향으로 치우칠 수밖에 없음, 그게 증거도 아니고, 논리도 아니고, 그냥 아무 근거 없는 편견일 뿐임, “역사적으로 이런 일이 많았다”는 근거로 실제 상황에 똑같이 적용하면, 남성은 여성과 멘토링을 하는 것만으로도 큰 위험을 감수하는 셈임, 아무리 조심해도 누군가가 과장 혹은 거짓으로 고소하면, 같은 역사적 논리가 항상 그 뒤를 지원하고 확대함
          + 안타깝게도 이 세상에는 “진실을 찾아내고 보고해주는 버튼”이 존재하지 않음, 진실을 찾는 과정은 어렵고, 비용이 많이 들며, 결론이 나지 않는 경우가 많음 — 예를 들어 과학, 법정, 진상조사위원회 등이 있음, 그래서 보통 이런 경우에는 진실이 불분명할 때 어떻게 행동하는 게 옳은지 고민하는 딜레마에 빠지게 됨, 결론적으로는 누가 거짓말인지 판단하려고 하지 않는 게 낫다는 쪽임, 각자의 이야기를 읽어봐도 스스로 판단할 위치에는 없고, 판단자가 아닌 행동자일 뿐임
          + 정말 누구의 말이 진실인지 아무도 모른다는 말이, 오히려 '공정한 절차'가 역사적으로 발전해 온 이유 아닐까 싶음, 한편으로 남성의 권력 남용이 많았다는 것처럼, 이제는 여성의 거짓 주장 혹은 부당한 고발로 아무런 검증 없이 남성의 인생이 망가지는 사례 역시 역사에 쌓이고 있음
          + 양쪽 다 자기 진실을 말하는 걸 수도 있음, 예를 들어 “현실의 일부 단서들을 편집해 맥락을 바꿔 자신을 나쁘게 보이게 만든 소설 같다”는 말처럼, 완전히 창작은 아니고, 서로 다른 해석이거나 의도치 않은 과장일 수 있음
          + 실제로 두 사람의 기록을 보면 강하게 모순되는 이야기는 별로 없고, 굳이 말하면, pretty.direct는 인간관계가 서툴고, yifanxing은 세상 경험이 적었던 것 같음, 둘 사이에 관계가 있었고, 시간이 지나 yifanxing이 그 일에 크게 후회하고 피해자임을 자각하게 됨, pretty.direct는 그 후폭풍을 예견하지 못했고, 결국 모두가 안타까운 사연임
     * 예전에 내 친구도 전 여자친구가 사소한 정치적 이유로 친구를 캔슬하려 했던 적이 있었음, 당시엔 그 일 대부분을 같이 겪었기에, 전여친이 퍼뜨린 이야기가 거짓이며 허점투성이란 걸 잘 알고 있었음, 그녀와는 별로 가까운 사이도 아니었고, 이유를 묻자 “나는 내 나름의 이유가 있어”라고만 하더니, 굉장히 이상하게 느껴졌음, 친구는 이를 계기로 수업도 그만두고, 교수와 학생 모두가 자신을 미워할까봐 두려워했음, 나는 위로해줬지만 솔직히 나조차 확신은 없었음, 그러다 1년 뒤, 그녀가 거짓 비방으로 1년 정학을 받고, 친구는 무사히 졸업, 취직도 잘 됐음, 이런 경험 때문에 앞으로 여성과의 상호작용이 꼬이면 언제든 내게 불리한 허위 서사가 만들어질 수 있다는 공포가 생겼음, 나는 결혼도 잘 했고, 현실적으로 어린 여성들과 조심해서
       소통하지만, 반드시 여러 명의 어른이 있는 곳에서만 제한적으로 함, 잘못하지 않아도 언제든 위험에 노출될 수 있다는 걸 배웠기에, 내가 할 수 있는 한 리스크를 극도로 줄이려고 함
          + ""정치적 이유""가 거짓일 수 있다는 건 무리임, 실제로 여자가 남성을 캔슬해야 할 정치적 이유는 많을 수 있음, 당신의 이야기가 너무 막연해서 신뢰가 가지 않음, 그리고 그런 사례로 인해 현실을 과도하게 피하지만, 그게 정말 건강하고 현실적인 방식일지 의문임, 오히려 세상에는 더 심각하고 현실적인 위험(교통사고, 범죄 등)들이 있는데, 거기에 비하면 캔슬 우려는 덜 중요한 것일 수도 있음
     * Chris Avellone이라는 게임 작가 사례도 흥미로웠음, 내 어린 시절 좋아했던 Planescape: Torment 등 작품에 참여해서 개인적으로 존경했었음, 그의 캔슬 소식에 또 한 명의 존경하는 인물도 이럴 줄이야 하며 실망했음, 유명한 남성의 권력 남용 이야기는 너무 쉽게 믿게 됨, 그래서 그는 일자리, 계약 등 커리어를 잃었음, 그런데 이 사건엔 후일담이 있음, 그는 스스로를 변호하는 긴 게시물을 올렸고, 법적 소송도 진행하여 최종적으로 승소함, 상대 고소인들이 “해당 사건은 없었다”고 대중적으로 인정한 공식 성명을 내게 함, 그의 게시물은 읽어볼 만함, 고소인들 쪽에서 오히려 7자리 수 보상금을 Chris에게 지급하는 판결이 나왔는데, 온라인에서는 ""그가 돈 주고 성명을 샀다""는 오해가 묘하게 퍼지기도 함, 한편 Warren Ellis 같은 또 다른 작가 사례는 분위기가 훨씬
       암울함
     * 이번 사건은 Scala 커뮤니티와 오픈레터에 서명한 300여 명, 그리고 Brian Clapper 본인 등에게 심각한 오점을 남겼다고 봄, 전에도 비슷한 일을 겪어봤기에, 얼마나 많은 사람들이 정보도 거의 없이 순식간에 판단을 내리고, 피의자가 변명·해명할 기회조차 전혀 주지 않는지 너무 잘 알게 됨, 내 친구들조차 아무도 내게 직접 묻지도 않고 영영 사라졌음, 결국 그런 캔슬 물결에 편승한 사람들은 내 인생에 아무 가치도 없었다는 걸 배웠음, 그래서 오히려 내가 인생에서 누구를 친구로 삼을지 더 신중해짐, 이 사건에 서명한 300명과 개인적으로나 직업적으로 마주치게 된다면 매우 멀리 피할 것임
     * 이 사건에 대해 당시엔 전혀 몰랐지만, 오픈레터를 지금 읽어보니 Jon이 대체 뭘 했다는 건지 명확하지 않음, 동의 없는 행동에 대해 구체적 언급도 없고, '성희롱 및 피해자화'라는 모호한 표현 외에는 구체적인 예시나 사례가 없음, 특히 '조직적인 패턴'이라고 하면서 실제 행위 사례는 전혀 없음, 나 스스로도 보통 남성보다 여성 쪽 이야기를 더 신뢰하는 입장이지만, 이런 모호한 주장과 내 경험을 합쳐 보면, 아마도 Jon은 그냥 이성과 인간관계에 어설픈 남자에 불과했을 수 있음, 내가 모르는 다른 정보가 더 있을 수도 있겠지만
          + 오픈레터는 특정 사례에 대한 반응임
          + “나는 여성 편이고, 남성을 불신한다”는 발언이 지금 사회에서 아무렇지 않게 받아들여진다는 게 신기함
          + 결코 '아무 일도 없다'고는 할 수 없음, 원본 편지에 나오는 내용만 해도 굉장히 무서운 상황임, 법원에서는 ‘증거 없음’ 판결만 했고, 피해자 입장에서 증거 내기 어렵고, 대부분 대면 상황이면 본인만 아는 일이 많으니 실제 증명도 힘들었을 것임, Jon 본인도 오픈레터에서 ""술 먹이고 에어비앤비에서 잤다""는 주장을 부정하지 않고, “가짜 증거”와 “짧은 관계”라고만 함, 확실히 모든 게 진흙탕임
     * 현실이야 어찌됐든 여론재판/군중심리식 유죄 선고 문화는 정말 위험함, 소셜미디어 때문에 더 심해진 느낌임, '무죄 추정의 원칙'을 이렇게 쉽게 무시한다는 게 믿기지 않음, 예전엔 사람들은 대체로 선하다고 믿었지만 이제는 그렇지도 않음, 겉은 평범하고 친절해도, 깊이 보면 다른 사람 인생을 망치는 데 거리낌 없는, 정말 위험한 부류도 있다는 걸 배움
          + Scala 같은 커뮤니티가 이런 일에 쉽게 휘말리는 이유는 '운동'의 성격을 띠고, 다른 운동들 사이에서 명성 경쟁을 하며, 커뮤니티 전체가 나쁜 평판을 피하려 즉각적 대응에 나선다는 점임, 서명자 대부분도 ‘커뮤니티 보호’를 우선하느라 개인의 삶을 희생시키는 결정을 내렸을 가능성이 높음, 실제 이 사건의 진위는 모르기에 직원 논평임
"
"https://news.hada.io/topic?id=22239","Folio - 모질라 Pocket을 대체할 Read it Later 앱","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Folio - 모질라 Pocket을 대체할 Read it Later 앱

     * Mozilla가 Pocket을 종료하면서 그 공백을 채우기 위해 출시한 ""나중에 읽기 앱""
     * 2018~19 사이에 Pocket 제품 헤드였던 Nick Chapman이 개발
     * iOS, Android, 웹앱, 브라우저 확장(크롬 출시, FF/Safari 출시 예정) 지원
          + 어디서든 저장 가능하고 기기간 동기화 지원
          + 오프라인 보기
     * Pocket 에서 데이터 이동 지원(7/8에 종료했지만, 데이터는 10월까지 가져오기 가능)
     * 프리미엄 모드에서는 추가 기능 지원(계속 추가 예정))
          + Text-to-Speech, Highlights, Full-text 검색, 커스텀 폰트와 여백 지정
          + 개발 진행중인 것들 : 스마트 요약, 개인화 추천, 주간 요약

   잔버그(특히 태그 관련)가 상당히 많지만 업데이트를 기다리며 계속 사용할 수 있는 툴인 것 같습니다

   아 이거 만들고 있었는데
"
