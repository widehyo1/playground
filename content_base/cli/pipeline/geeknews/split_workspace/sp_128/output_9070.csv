"https://news.hada.io/topic?id=21296","GPT Codex 실무에 몰래 사용 경험기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GPT Codex 실무에 몰래 사용 경험기

  OpenAI가 2025년 5월에 공개한 Codex는 ‘비동기 에이전트’로, 직접 코딩하지 않고 작업을 맡기고 떠날 수 있는 구조

     * 격리된 오프라인 VM(에어갭)에서 작업을 수행하며, 보안과 안전성을 기본 구조로 내장
     * Codex는 작업을 시작할 때 항상 AGENTS.md를 찾고, 없을 경우 README.md 등으로 대체해 문맥 파악을 시도함

  실제 업무에 Codex를 써보며 가능성을 보다

     * API 응답 프로퍼티 15개 추적 요청을 자동으로 처리
     * 모바일 스크롤 버그 수정 PR 생성
     * 스타일 수정 요청에 context 없이도 정확한 tailwind class 추론
     * 마이그레이션 작업을 PR 단위로 자동 분할해 제출

  단점

     * task 간 기억 불가 (기억력 없음)
     * task 단위 폐쇄적 구조로 티키타카 불가
     * Context가 길거나 Monorepo 환경에서는 추론 성능 저하

   “일단 맡기고 잊을 수 있다”는 점으로, 우선순위 낮은 기술 부채를 처리하기에 유용. 생산성 측정은 아직 초기 단계지만, 업무 방식을 바꿀 수 있을 정도의 잠재력을 보았음
"
"https://news.hada.io/topic?id=21194","FLUX.1 Kontext - 텍스트와 이미지를 결합한 실시간 생성·편집 AI 모델","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             FLUX.1 Kontext - 텍스트와 이미지를 결합한 실시간 생성·편집 AI 모델

     * Black Forest Labs의 FLUX.1 Kontext는 텍스트와 이미지를 동시에 입력해 맥락을 이해하고, 기존 이미지의 특징과 스타일을 유지하며 즉시 수정·생성이 가능한 최신 생성형 AI 모델
     * 기존 텍스트-이미지 생성 알고리듬 대비, 문자·객체 일관성과 로컬 편집, 스타일 참조, 고속 응답 등에서 우수한 성능을 보임
     * 사용자는 텍스트만 입력하거나, 이미지와 텍스트를 조합해 특정 영역만 변경, 스타일만 적용, 다단계 편집 등 다양한 상호작용적 이미지 작업이 가능함
     * FLUX.1 Kontext [pro] 는 여러 번의 편집에도 이미지 일관성을 유지하며, 업계 최고 속도로 작동함
     * 오픈 소스 모델 [dev] 버전은 경량화된 12B 디퓨전 트랜스포머로, 연구 및 커스터마이징 목적의 프라이빗 베타로 공개됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

FLUX.1 Kontext 소개

     * FLUX.1 Kontext는 텍스트만으로 이미지를 생성하는 기존 모델의 한계를 넘어서, 텍스트와 이미지를 함께 입력해 맥락 기반의 이미지 생성과 편집이 가능한 생성형 플로우 매칭 모델임
     * 텍스트 프롬프트와 이미지를 동시에 활용하여, 이미지의 특정 요소를 제거/추가/변경하고, 스타일이나 특징을 유지한 채 새로운 장면을 생성할 수 있음

주요 기능

     * 캐릭터 일관성: 동일 인물, 객체, 스타일이 다양한 장면과 환경에서도 일관되게 유지됨
     * 로컬 편집: 이미지의 특정 부분만 텍스트 명령으로 수정할 수 있음(예: 얼굴의 특정 요소만 제거, 글자만 변경 등)
     * 스타일 참조: 참조 이미지의 독특한 스타일을 새로운 장면에 적용 가능
     * 인터랙티브 속도: 기존 모델 대비 최대 8배 빠른 추론 속도로 실시간 편집 및 생성 지원

텍스트-이미지 및 이미지-이미지 편집의 통합

     * FLUX.1 Kontext는 한 번의 편집뿐 아니라, 여러 단계에 걸친 반복적 지시에도 이미지 품질과 특징을 유지함
     * 프롬프트와 이전 이미지 결과를 연속적으로 활용해, 한 단계씩 원하는 결과에 도달할 수 있음

FLUX.1 Kontext 모델 라인업

     * FLUX.1 Kontext [pro]
          + 빠른 반복 편집 및 생성에 특화된 플래그십 모델
          + 텍스트와 참조 이미지를 동시에 입력받아, 타겟 영역 편집과 복잡한 장면 변환을 빠르고 일관성 있게 수행함
     * FLUX.1 Kontext [max]
          + 실험적 최고 사양 모델로, 프롬프트 이해력과 타이포그래피, 고속 일관성 편집 능력이 향상됨
     * FLUX.1 Kontext [dev]
          + 연구 및 커스터마이징 목적의 경량화(12B) 모델, 프라이빗 베타로 공개됨
          + 공개 시 FAL, Replicate, Runware, DataCrunch, TogetherAI, HuggingFace 등 주요 AI 인프라 파트너를 통해 제공 예정

지원 및 접근

     * FLUX.1 Kontext 시리즈는 KreaAI, Freepik, Lightricks, OpenArt, LeonardoAI 등 다양한 서비스와 FAL, Replicate, Runware, DataCrunch, TogetherAI, ComfyOrg 등의 인프라에서 사용할 수 있음
     * 실시간 체험 및 데모를 위한 FLUX Playground(https://playground.bfl.ai/)를 통해, 별도의 통합 없이 손쉽게 모델 성능을 검증하고 결과를 확인할 수 있음

성능 평가

     * 자체 벤치마크 KontextBench에서 6가지 이미지 생성·편집 과제별로 SOTA 모델들과 비교 평가
     * 텍스트 편집, 캐릭터 보존 분야에서 업계 최고 수준의 점수를 기록함
     * 추론 속도 역시 기존 최고 성능 모델 대비 압도적으로 낮은 레이턴시 달성
     * 미적 완성도, 프롬프트 이해력, 타이포그래피, 사실성 등 다양한 기준에서도 경쟁력 입증

한계점 및 향후 과제

     * 다단계(6번 이상) 반복 편집 시 시각적 노이즈(artifact) 가 발생해 이미지 품질이 저하될 수 있음
     * 간혹 특정 프롬프트의 세부 지시를 정확히 따르지 못하는 사례가 있음
     * 세계지식 및 문맥 이해력에 한계가 있어 맥락적으로 부정확한 이미지를 생성할 수 있음
     * 모델 경량화 및 distillation 과정에서 이미지 품질이 떨어질 수 있음

     * 누구나 FLUX Playground 에서 모델을 실시간으로 시험하고, API 도입 전 기능을 검증할 수 있음
     * 전체 기술 리포트 보기(PDF)

        Hacker News 의견

     * 직접 사용해봤는데 재밌는 '컨텍스트 슬립' 현상 경험함 관련 이미지 생성 프롬프트로 우주선이 외딴 행성에 착륙한 이미지를 만들었고, ""우주선을 더 컬러풀하게 그리고 이미지에서 더 크게 보여달라""는 편집을 요청함 그랬더니 우주선이 컨테이너 선박으로 바뀌어버림 채팅 기록이 남아 있었으니 내가 우주선을 원한다는 걸 파악했어야 했는데, 중요한 맥락을 놓쳐 결과가 엉뚱하게 나옴
     * Replicate의 FLUX Kontext Pro 엔드포인트로 직접 테스트 중임 FLUX Kontext의 다양한 이미지 편집 활용법을 보여주는 Replicate 앱도 존재 FLUX Kontext Apps 이미지 품질은 단순 이미지-투-이미지 생성의 경우 GPT-4o 이미지 생성 수준과 비슷함 생성 속도도 약 4초로 빠른 편임 프롬프트 엔지니어링은 예시들 외에는 다소 까다로운 감 있음, 점점 나아질 거라 생각함 스타일 변경이나 세부적 요청도 적용되긴 하지만, 더 구체적인 지시를 줄수록 오히려 상세 요구를 무시하는 경향 있음
          + 오리지널 속성을 얼마나 잘 보존하는지 보면, FLUX 모델이 4o보다 더 정확한 느낌 기존 3D 동물 캐릭터에서 라이팅만 바꾸고 싶다고 요청하면, 4o는 캐릭터 얼굴을 망가뜨리고 몸이나 세부를 건드리는 데 비해 FLUX는 자세나 라이팅을 크게 바꿔도 가시적 형상을 거의 완벽히 동일하게 유지함
          + 이미지-투-이미지 실험에서 GPT-4o보다 더 인상적임 4o는 색을 세피아 톤으로 강하게 집착하는데, 반복 편집시 특히 4o의 결과물이라는 티가 큼 반면 FLUX.1 Kontext Max버전은 훨씬 넓고 다채로운 컬러 표현을 하며, 4o가 놓칠 소소한 디테일까지 잡아냄 프롬프트만으로 이미지를 새로 생성하는 건 아직 실험 못 해봄 하지만 프롬프트로 기존 이미지 편집하는 데는 FLUX가 월등히 뛰어난 느낌임
          + Replicate가 항상 최신 모델을 즉시 제공하는 게 정말 마음에 듦 빠르게 발전하는 AI 시대에, 새로운 연구 버전 모델들이 즉시 API로 배포되고 실전에서도 스케일 있게 쓸 수 있다는 점이 멋짐 Replicate 같은 배포사가 이러한 모델 공개의 임팩트를 몇 배로 증폭시키는 역할을 하는 듯함
          + 4초 정도 걸린다는 건 어느 GPU랑 VRAM 기준인지 궁금함 혹시 Huggingface의 UI 말하는 건가?
     * 일부 샘플은 너무 좋은 결과만 골라 보여주는 듯 “Kontext Apps”의 프로페셔널 헤드샷 앱 써본 사람 있나? Kontext Apps 링크 여러 장의 내 사진을 넣어봤는데 매번 완전히 다른 사람이 되어버림 최종 헤드샷 결과는 확실히 프로페셔널해 보임
          + flux playground에서 피곤한 헬스장 셀카로 헤드샷 프롬프트 써봤는데, 동일한 표정과 땀, 피부톤 등 내 모습 대부분 유지함 마치 배경만 바꾼 듯했고 ""소셜 미디어용 좋은 헤드샷으로 바꿔줘, 미소, 좋은 자세와 옷, 땀이 없는 깨끗한 피부 등""이라고 확장 요청하니 옷 바뀌고 어색한 미소만 추가됨 이런 류의 이미지들에서 실제로 나오는 결과와 비슷함
          + 입력 및 출력 이미지의 비율이 같은지 궁금함 비율이 강제로 변하면 이상 현상이 생기는 것 같음
          + 얼굴 등 아이덴티티 보존은 어느 누구도 완벽히 해결 못한 과제임 손도 그렇고, 과학적 난제임
     * FLUX Kontext 모델을 내 GenAI 이미지 비교 사이트에 추가할지 고민 중임 Max 버전이 프롬프트 충실도 면에서 점수가 2배 가까이 높지만, 그래도 OpenAI의 gpt-image-1보다 한참 떨어짐 (화질은 논외하고) gpt-image-1이 리더보드 1위임 Flux 1.D는 로컬 GenAI 역량의 베이스라인용으로 유지 중임 비교사이트 Hunyuan의 Image 2.0 모델도 최근에 추가했는데, 실시간 모델 답게 점수는 낮게 나옴 참고로 Black Forest Labs의 이 모델은 텍스트-투-이미지용이라기보다 기존 이미지 반복 편집•수정에 더 중점을 둔 듯함
          + “Flux 1.1 Pro Ultra”도 사이트에 추가해주길 바람 이 시리즈 중 가장 성능 높고, Flux Dev보다 프롬프트 충실도가 훨씬 좋다는 평임 최고의 오픈소스 모델 중 하나로 공정하게 비교 가능해질 듯 사이트 자체도 재미있고 프롬프트도 흥미로움
          + 내 제안: 이런 장면 프롬프트는 예전 그 어떤 모델에서도 제대로 구현하지 못했음, 최근엔 많이 개선되었을 것 같은데…
A knight with a sword in hand stands with his back to us, facing down an army. He holds his shield above his head to protect himself from the rain of arrows shot by archers visible in the rear.

            충분한 데이터가 있는데도 결과가 왜 이리 엉망인지 신기함 꽤 아이코닉한 장면임
          + 사이트 추가 요청함, 잘 보고 있음
     * 입력 이미지가 한 장으로만 제한되는지 궁금함 여러 장을 입력해서 ""A이미지 속 아이템을 B이미지 안에 배치""처럼 ""A 캐릭터를 B 풍경에 넣어줘"" 같은 복합 프롬프트 해보고 싶음
          + 실험적인 “multi” 모드에서 여러 이미지 입력 가능함
          + Fal에서 멀티 이미지 인터페이스 사용해볼 수 있고, Replicate에도 아마 있을 듯 (확인해보진 않음) 이 모델 엄청난 성능임 gpt-image-1보단 못하지만 진짜 근접함 이미지나 비디오에는 이제 독점적 장벽이 사라질 거라 봄 Google이나 OpenAI가 창의성 시장을 독점할 거란 걱정 있었으나, 누구든 직접 만들 수 있게 됨
     * 기술 논문을 궁금해하는 사람을 위해 공식 리포트 공유함
          + 구현은 타 오픈모델들과 비슷하게 간단하게 보임 (HiDream-E1, ICEdit, DreamO 등) 진짜 차별점은 데이터 큐레이션이며, 이 부분은 논문에 간략히만 설명되어 있음
          + 대부분 논문 자체엔 관심 없고 오픈웨이트 모델 다운받아 직접 돌리는 게 목적임 대부분 가져가서 활용하지, 기여는 거의 없음
     * 이걸 로컬에서 직접 수정·학습하려면 어느 정도의 전문성이 필요할까? RTX 4090, Windows에 Flux 1 dev로 직접 LoRa 튜닝해보려고 이틀이나 파봤는데, 제대로 못하고 있음 어느 정도로 파야 하는지, 혹시 진입장벽 낮은지도 궁금함 초보자도 가능한지, 아니면 숙련자만 진입 가능한지
          + 오픈소스 모델은 아직 공개되지 않았으며, Flux 1 Dev에서 LoRA 학습보다 쉬울 리 없음
          + SimpleTuner 스크립트 사용 추천함 파이썬 라이브러리 몰라도 직접 LoRa 튜닝할 수 있었음
          + 보통 comfyui에서 구성된 버전 쉽게 찾을 수 있음 유튜버 경우엔 patreon 후원 리워드 등으로 배포하기도 함
          + RTX 4090 + Windows에서 안되는 건 윈도우 OS가 문제인 듯 진짜 성능은 리눅스에서 진가를 보임
     * remove from face 예시를 잘 이해하지 못하겠음 다른 얼굴 사진 없으면 결국 전형적인 이미지를 쓰는 거 아닌가?
          + 실제 무언가를 복구하는 게 아니라, 다 생성된 이미지일 뿐임 진짜 얼굴이 없음
          + 예제를 자세히 보면 특정 오브젝트가 얼굴을 부분적으로 가릴 때, 모델이 추론하여 복원할 가능성이 있음
          + 기반 모델이 어느 단계에 있냐에 따라 다름; 일부 아이덴티티 모델은 부분 지오메트리만으로도 얼굴을 상당히 정교하게 인터폴레이션 가능함
          + 첫 번째 예시의 슬라이드쇼 자체에 버그가 있는 것 같음 눈송이가 얼굴 대부분을 가리고 있음
          + 실사진을 쓰면 모델이 얼굴을 바꿔버릴 때가 많아서, 아예 얼굴이 안 보이는 사진을 예시로 쓰는 듯함
     * 체스 이미지를 생성할 수 있는지 질문함 체스 AI 예측 링크
     * 오픈 개발자 버전이 언제쯤 나올지 추측하는 댓글, 일주일 내일지, 한두달 더 걸릴지 궁금함
"
"https://news.hada.io/topic?id=21263","Show GN: 루비 lang 과 관련 된 아티클과 유튜브를 요약, 번역해서 아카이브 합니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: 루비 lang 과 관련 된 아티클과 유튜브를 요약, 번역해서 아카이브 합니다.

   한글로 된 루비, 레일즈 관련 아티클이나 유튜브가 많지 않기에 영어로 된 것들을 구글 gemini 를 이용해서 요약, 번역해 아카이브 하는 사이트를 만들었습니다.

   루비온레일즈 8.0 을 베이스로 postgresql 데이터베이스를 씁니다. 오늘 pg 의 전문검색을 이용해 검색 기능을 추가했는데 아주 잘 동작한 김에 이렇게 긱뉴스에 홍보를 합니다.

   좋네요~ 잘 보겠습니다. 감사합니다.
"
"https://news.hada.io/topic?id=21293","OpenAI, 모든 ChatGPT 로그를 저장하라는 법원 명령에 강력 반발","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               OpenAI, 모든 ChatGPT 로그를 저장하라는 법원 명령에 강력 반발

     * 법원 명령으로 인해 OpenAI가 삭제된 대화 포함, 모든 ChatGPT 사용자 로그를 보관해야 하며, OpenAI는 이 조치가 심각한 프라이버시 침해라고 반박함
     * 해당 명령은 저작권 소송 중 증거 보전을 요구한 뉴스사들의 주장에 따라 충분한 근거 없이 즉시 내려진 것이라고 OpenAI는 주장함
     * OpenAI는 이 명령으로 인해 수억 명의 사용자의 개인정보와 기업의 비밀 데이터까지 보관하게 되어, 글로벌 프라이버시 규정 위반 위험과 엔지니어링 부담을 경고함
     * 명령 시행 이후 사용자는 삭제한 대화, 임시 대화 등도 강제로 영구 저장되며, 이에 대한 강한 불안감과 우려가 SNS 등에서 확산됨
     * OpenAI는 사용자의 데이터 통제 권한과 프라이버시 보호를 위해 명령의 철회를 재차 요청하며, 법적 투쟁을 계속할 것임을 밝힘
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“심각하게 개인적인” 데이터 보관 명령에 대한 OpenAI의 반발

  법원 명령의 배경과 주요 쟁점

     * OpenAI는 수백만 ChatGPT 사용자의 프라이버시 보호를 위해 법원에 항의하며, 모든 사용자 로그(삭제 대화 및 API 대화 포함) 저장 명령에 대해 강하게 반발함
     * 해당 명령은 저작권 침해 소송 과정에서 뉴스사 측이 OpenAI가 증거를 파기하고 있다고 주장하면서 내려졌으며, OpenAI는 명령이 충분한 사실 확인이나 소명 없이 급하게 결정되었다고 지적함
     * 소송 원고 측은 ChatGPT로 유료 뉴스기사 우회 접근 시, 사용자가 흔적을 지우기 위해 대화를 삭제할 것이라고 주장했으나, 실제로 해당 주장을 뒷받침하는 증거는 제시되지 않음
     * 법원은 이러한 추정에 근거해 OpenAI가 앞으로 모든 대화 로그(삭제 포함)를 별도 보관하도록 명령했고, OpenAI는 이 조치가 사용자의 데이터 통제권 침해 및 글로벌 프라이버시 법 위반 위험을 높인다고 경고함

  OpenAI의 주장 및 법적 대응

     * OpenAI는 이례적으로 광범위한 이번 보존 명령이 사용자의 프라이버시 권리를 침해하며, 사용자가 언제 어떻게 자신의 데이터를 보관할지 스스로 선택할 수 있어야 한다고 강조함
     * 법원 명령이 내려지기 전까지, OpenAI는 사용자가 직접 대화를 삭제하거나 임시 대화 기능을 쓸 경우, 해당 대화는 시스템에서 30일 내 완전 삭제되는 구조였음
     * 그러나 이번 명령으로 인해 모든 삭제 요청 및 임시 대화까지 강제로 저장해야 하는 상황이 되어, 수억 명의 사용자 개인정보와 기업 API 데이터(비밀 정보 포함)가 장기 보관됨
     * OpenAI는 프라이버시 우려와 더불어, 명령 이행을 위해 엔지니어링 리소스와 막대한 비용이 발생함을 지적하며, 뉴스사 측의 추정적 이익보다 OpenAI 및 사용자 피해가 훨씬 크다고 주장함
     * OpenAI는 법원의 구두 심리를 요청하며, 명령 철회(즉각적 취소)를 요구함

  사용자 및 업계 반응

     * 명령 시행 사실이 알려지자, 수많은 사용자와 기업 고객이 소셜미디어(LinkedIn, X 등)에서 불안감을 표출함
     * 일부 전문가는 해당 명령이 “모든 OpenAI 기업 고객에 대한 심각한 계약 위반”이 될 수 있으며, API를 통해 비밀 데이터를 다루는 기업은 더 큰 보안 리스크에 노출된다고 경고함
     * SNS상에서는 “OpenAI를 통한 AI 서비스는 모두 프라이버시 위협에 노출됐다”, “대안 서비스(Mistral AI, Google Gemini 등)로 전환 권고” 등의 의견이 확산됨
     * 한 보안 전문가는 “이런 명령은 용납할 수 없는 보안 위험”이라 평가함
     * “Wang 판사는 NYT의 저작권 논리가 OpenAI 모든 사용자의 프라이버시보다 우선한다고 생각한다—미친 일”이라는 비판도 등장함

  OpenAI의 정책과 미래 전망

     * OpenAI는 기존에 사용자 데이터 보관 정책을 엄격히 지키고, 계정 삭제 시 30일 내 전체 대화 내역을 삭제해왔음
     * 법원 명령으로 인해 기존 데이터 관리·삭제 프로세스가 한순간에 무력화되었고, 사용자는 더 이상 자신의 데이터가 안전하게 삭제된다는 신뢰를 갖기 어려운 상황임
     * OpenAI는 법적 투쟁을 계속할 방침이며, 명령의 부당성과 실제 피해를 적극적으로 알릴 계획임

        Hacker News 의견

     * GPT 모델을 API로 사용할 때도 같은 위험성 존재함에 주목함

     ChatGPT Free, Plus, Pro 이용자와 API 사용자 모두에게 이 위험이 확장된다고 OpenAI에서 언급함
     이런 상황은 OpenAI 비즈니스에 매우 불리한 현상이라는 생각임
          + 이건 머지않아 모든 AI 회사들이 겪을 문제라고 생각함
            모두가 직접 모델을 호스팅하는 환경으로 바뀌지 않는 이상, SaaS같은 비즈니스 모델에선 수익성을 고려할 때 사용자의 프라이버시 보호가 크게 중요하지 않은 현실임
            솔직히 대부분의 사람들은 이미 인터넷에서 프라이버시란 게 없다는 사실에 익숙해진 상태임
            다만 폐쇄형 소스 코드나 보안 관련 데이터를 신뢰를 바탕으로 맡길 기업이나 사람들은 상당히 타격을 입을 수밖에 없음
            근데 그런 부분은 애초에 어느 업체에도 외주를 주면 안 된다는 입장임
          + 이미 자리를 잡은 기업들은 이번 이슈로 기존 계약, 규정, 리스크 허용 범위를 재검토해야 하는 필요성 있음
            ChatGPT 기반 서비스의 래퍼 스타트업들도 프라이버시 정책을 재점검하고, 사용자가 프라이버시를 포기하고 있다는 사실을 명확히 밝혀둘 필요 있음
          + 내가 도입했던 모든 GPT 통합은 Azure 서비스를 통한 이유가 내 데이터로 학습하지 않겠다는 계약상의 의무 때문임
            내가 이해하기론 Azure 서비스, 즉 Microsoft에는 이번 판결이 적용되지 않는 걸로 알고 있음
          + 만약 너가 독점 코드를 다뤘던 상황이면, 클라우드 LLM은 애초에 사용하면 안 됐던 셈이고, 이번 이슈로 그 사실이 더 명확해진 느낌임
          + 어떻게 비즈니스에 타격이냐는 질문에 대해, 이것은 법적 리스크 대비로 데이터를 보관하는 조치이지, 훈련 목적으로 쓰기 위한 게 아님을 강조함
            다른 기업들과의 계약에서도 데이터를 학습에 사용하지 않는다는 조건을 계약상에 명시할 수 있다고 생각함
     * 더 자세한 배경 기사를 여기에서 확인 가능함
          + 이 링크가 실제 자료원임
            해당 글로 링크를 업데이트하는 게 좋을 듯함
          + 기사 밑에 달린 댓글들이 정말 웃긴 분위기라 소개함
            저작권 옹호자들을 반AI 진영으로 그리며 풍자하는 느낌임
            개인적으로 OpenAI가 남의 콘텐츠를 대하는 안하무인 태도는 별로 좋아하지 않지만, 한편으로 저작권자도 지나치게 강한 요구를 하는 것은 공감하기 어려움
            생성형 AI와 훈련 문제는 현행 지식재산권법의 시대착오적 요소를 극명하게 드러낸다고 봄
            앞으로 변화가 필요하지만 그 변화가 대기업이나 부자들만 유리하게 가선 안 되고, 평범한 사람들에게 도움이 되어야 한다고 생각함
          + LLM의 저작권 침해에 대체로 비판적인 입장이지만, 이번 판결의 논리 전개 방식은 다소 이상하게 느껴짐
            판사가 hypothetical로 ChatGPT를 통해 어떤 사용자가 유료벽을 우회해서 New York Times 콘텐츠를 뽑아내고, 이후 사건을 알게 된 뒤 자신이 내보낸 기록 모두를 삭제해 달라고 요구하면 판결의 취지를 회피하는 게 아니냐는 지적임
            사실 이런 판결이 나왔다는 걸 들으면 사용자는 그 기간 동안 조심하게 되지 않을까 하는 의문이 듦
          + OpenAI가 이제는 사용자가 대화 내역을 삭제하거나 Temporary Chat 기능으로 임시 대화를 해도, 법적 명령 때문에 로그를 남겨야 하는 상황이 평범한 웹브라우저 이력과 다를 게 뭐냐는 비판임
            Safari가 사용자가 지운 기록까지 무조건 저장해야 하는 등, 왜 OpenAI만 특별히 강제되는지 의아함
     * 더 나은 기사 링크를 스레드에 제시함
       아르스 테크니카 원문 기사 링크임
       단순 Mastodon 게시글이 아니라 실제 정보가 담긴 기사를 참고해야 한다고 덧붙임
     * 최근 Hacker News에서 LLM의 장점이나 단점을 논하는 개인적인 칼럼이 여럿 올라오는데, 프라이버시 문제는 전혀 언급하지 않는 경향이 있음
       내가 LLM을 활용하거나 소스 코드를 Prompt 창에 붙이지 않는 핵심 이유는 프라이버시 때문임
       우리 회사는 NDA와 ITAR 같은 정부 규제 때문에 코드가 서버 밖으로 나가면 바로 규정 위반임
       이번 이슈는 프라이버시가 LLM의 아킬레스건임을 보여줌
       LLM이 온프레미스 형태로 자리잡기 전엔 이 문제에서 자유로울 수 없음
          + 자기 서버에 LLM을 직접 호스팅하는 방법도 있으니, 이런 고민 해결에는 완전한 자가관리 방식이 정답임
            아주 쉽고 간단하게 직접 LLM을 구축할 수 있음
     * OpenAI가 데이터를 서버에 저장할 수밖에 없는 상황은, 소송 중인 기업들의 법무팀이 Discovery(문서제출명령) 절차에서 사용자와 ChatGPT의 대화 내역을 볼 수 있게 되는 상황으로 이어짐
       예를 들어 NYT의 변호사들이 법정에서 당사자의 프라이빗한 대화를 근거 자료로 읽게 될 수 있음
          + 차라리 대화 로그를 익명화해서 보관하는 것이 해법이 될 수 있다는 의견임
            OpenAI가 기술적으로 익명화 작업을 할 수도 있는데, 이 부분이 최적의 해결책으로 보임
          + 이런 데이터는 Spectra TFinity ExaScale 라이브러리와 같은 테이프 스토리지 기반 솔루션, 혹은 AWS Glacier 등과 같이 깊게 아카이빙하는 시스템에 백업할 수 있음
            데이터 복구에 시간이 몇 시간~몇 일이 걸리는 이러한 구조라면 법원 명령도 지키고, 비용도 저렴하게 맞출 수 있으며
            대규모 정보 유출 시에도 오랜 시간과 노력을 들여야 훔칠 수 있어 탐지와 방어에 유리함
          + 이제 미국 내 모든 클라우드 기반 AI와 주고받는 챗/API 콜은 모두 법적 검색 대상임을 전제로 의심해야 함
            만약 이게 감당 안 되는 리스크라면 로컬 LLM으로 전환을 진지하게 고려해야 함
     * Times 같은 매체가 어떻게 사용자의 데이터를 볼 수 있는 권리를 가지게 되는지 의문임
       결국 이런 판결로 신문사 측이 사용자의 데이터를 들여다보게 되는 상황임
     * 이 법원 명령은 한 곳 이상의 관할권에서 프라이버시 법률 위반 소지가 있고, OpenAI가 기존 고객들과 맺은 계약을 어기게 될 가능성도 존재함
          + 기존 계약은 법원이 명령을 내리는 것에 아무런 영향을 주지 못한다는 설명임
            법적 명령이 최우선임
          + 이번 명령 자체가 새로운 프라이버시법 위반을 추가로 만드는 건 아님
            애초에 데이터를 보관하고 제3자에 제공하는 것 자체가 위반이었음
          + 이 명령이 실제로는 5월 13일 자로 소급 적용되고 있음
            OpenAI가 지금까지 사용자에게 별도 메일 등으로 이 사실을 알리지 않은 이유가 사업상 악영향 때문이라는 느낌임
            하지만, 사용자 신뢰를 명백히 저버린 행동으로 느껴짐
     * ChatGPT API를 통해 민감한 데이터를 주고받던 기업들이 입력, 출력 데이터가 저장되지 않는다는 설명을 믿었을 텐데
       실제로는 OpenAI가 설정만 바꿔 버리면 데이터를 남길 수 있는 상황임
       별도 공지가 갔던 건지, 아니면 다들 언론 보도를 통해서야 이 사실을 알게 된 건지 궁금함
     * 원글 링크가 트래픽 과부하(HN hug of death)로 접속이 안 됐는데 Wayback Machine에서 읽는 데 성공함
       Mastodon 개인 인스턴스여서 방문자가 급증하면 과부하 걸리는 게 이해됨
"
"https://news.hada.io/topic?id=21218","실리콘밸리에 대형 전자제품 소매점이 다시 등장: Micro Center 오픈","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               실리콘밸리에 대형 전자제품 소매점이 다시 등장: Micro Center 오픈

     * Micro Center가 실리콘밸리에 신규 매장 오픈 소식 공개
     * 고객을 위해 Member Pricing 제도를 2026년까지 무료로 제공함
     * 가입 및 적립 절차는 간단하며 신용카드 불필요
     * 멤버 전용 할인가는 사이트 전역에서 자동 적용 및 반환도 무료 제공
     * 기존 Micro Center 계정 보유자는 별도 절차 없이 무료 혜택 적용 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Micro Center의 실리콘밸리 진출과 멤버십 가격 정책

     * Micro Center가 실리콘밸리 지역에서 대형 전자제품 소매점으로 다시 등장함
     * 고객은 Member Pricing 제도를 통해 자주 찾는 제품에 대한 더 큰 할인 혜택을 받을 수 있음
     * 본 혜택은 2026년까지 전면 무료로 제공되며, 가입 시 신용카드 입력이 요구되지 않음
     * 계정에 로그인한 경우 할인가가 자동으로 적용되며, 제품 반환은 항상 무료임
     * 사이트 내에서 녹색 태그가 있는 상품이 Member Pricing 적용 대상임
     * 기존 Micro Center 계정 사용자는 별도의 절차 없이, 2026년까지 자동으로 할인을 적용받음

        Hacker News 의견

     * Cambridge, MA의 Micro Center가 예전보다 훨씬 좋아진 느낌 경험 중이고, 원하는 제품이 있으면 온라인보다 여기서 직접 구매하는 선호 경향. Fry’s에서 종종 나오던 ‘오픈박스 재포장 판매’ 같은 일은 Micro Center에서는 본 적 없고, 반품된 제품은 할인 코너에 명확히 구분돼서 표시되는 것 확인. 지역 내에 좋은 전자부품/중고매장이나 e-cycling 샵이 생기면 정말 신날 것 같지만, 비싼 부동산 가격과 한정된 매니아 수요 탓에 경제 논리가 힘들 것이라는 생각. 지역 공돌이들이 해체 장비를 현지에서 바로 사거나 즉흥 구매로 배송비도 절약할 텐데, 그 수요가 충분하지 않을 거라는 관측
          + 이 가게 정말 위험한 장소 느낌. 그저 구경만 하려고 갔다가 울트라 와이드 모니터랑 새 PC까지 집에 들고 오게 되는 경험 다수. Microcenter에서 부품을 직접 사는 즐거움 덕분에 온라인 주문을 줄이고 있음. 매장 내부가 90년대 모습 그대로라 리뉴얼이 좀 필요하다는 아쉬움
          + Micro Center는 Amazon 가격도 매칭해 준다는 정보 공유. 직접 CPU 쿨러 구입 시 계산원이 Amazon 페이지 보여주면 바로 할인해줌을 안내. Micro Center 가격 매칭 정책 상세 링크
          + 직원들이 너무 자주 다가와서 스티커 붙이고 쓸모없는 조언을 강요하는 것이 아쉬움. 그럼에도 이 지역 최고의 매장이라는 평가와 함께, Trader Joe’s도 인근이라 편리함. 예전에 첫 컴퓨터 Apple //gs를 Micro Center 초기 매장 한 곳에서 구매한 추억 언급
          + 방금 한 시간 전에 Wi-Fi 메쉬 빌드 용품으로 800달러 구매함. 온라인이 더 쌀 수도 있겠지만, 36시간 내에 무언가 급하게 구축해야 할 때 Micro Center만한 곳 없는 실체적 체감
          + MIT에서 하는 플리마켓 Flea at MIT 언급. 오프라인 창고형 매장은 아니지만, 흥미로운 대안으로 추천
     * Fry’s가 문 닫은 지도 어느덧 4년 이상 지남. 실리콘밸리 한복판에 Best Buy보단 낫고 Central Computers보단 큰 곳이 이제서야 다시 생긴 것이 너무 놀라운 감상
          + 진짜 Fry’s는 훨씬 전, 내가 18년 전 지역을 떠날 무렵 이미 내리막 시작. 부품, 공구, 계측기가 줄고 기성 완제품만 늘었다는 체감
          + Lawrence 근처 Fry’s 폐점 직전 마지막 방문 경험. Central Computer도 그땐 필요하던 건 있었지만, 전반적으로 매장 환경이 텅 빈 느낌. 80~90년대 Fry’s, Weird Stuff, Halted, Anchor, Computer Literacy 시절과는 아예 달랐던 분위기
          + 한때 우리 지역 Microcenter가 2012년에 문을 닫았는데, 이번에 다시 돌아오는데 정말 오래 걸렸다는 실감
          + Central Computer에서 10년 동안 컴퓨터 세 대와 두 번의 수리 경험이 만족스러움
          + PC 하드웨어 리테일 비즈니스는 온라인 경쟁 때문에 마진이 매우 낮고, 재고 회전도 힘든 구조. 그래서 Microcenter 같은 오프라인 매장이 미국 전국에 많지 않은 이유
     * Westchester NY의 Microcenter에서 차로 10분 거리에 있는 행복. 커스텀 수랭 파츠만 따로 전용 진열대가 있는 수준이라 엔지니어/매니아 존중 느껴짐. 예전엔 Newegg에서 세팅 주문했으나, 이젠 Microcenter 직접 방문 선호
          + Newegg는 이전보다 너무 안 좋아져서, 틀린 품목이 오거나 각종 판매자 재고가 섞여 원산지 신뢰도 하락. Amazon과 별 차이 없는 온라인 판매환경 실망, 비추천
          + Wayne, PA 지점 이용 중이고 가격이 경쟁력 있음. Bawls 에너지 드링크 취급점 중 하나라 색다른 재미
     * 지난 이틀간 사전 오픈 행사 때 매장 앞 주차장이 꽉 차 있었고, 도로 건너까지 주차 후 걸어오는 인파가 넘쳤음. USB 무료 증정 이벤트에 혹해 평일 전시입 기대했으나 아직 방문 못함. 이 정도면 수요 폭발 체감
          + 정식 오픈 전 25일부터 조용히 영업을 시작했는데도 이미 평일임에도 사람이 엄청 많았던 사실 언급
     * Best Buy와 Central Computer 그 사이를 채워주는 매장이 실리콘밸리에 드디어 다시 생긴 느낌. 하지만 Harbor Freight 쇼핑센터와 함께 있는데다 집이 가까워 지름 유혹이 두 배로 커질 현실 우려. 오픈날(5/28) 입장 1시간, 계산 1시간 넘게 기다렸고, 구매 당시 계속 보증연장 상품을 세일즈가 강권하는 부분이 거슬렸음. 노트북 사면서 플로어 직원, 매니저까지 두 번 보증 설명 받았고, 고객 설문조사에도 ‘매니저와의 만남’ 여부가 체크 항목이라 불필요하고 어색한 절차가 제도화된 것인지 의구심
          + 설문조사에 “직원이 잘 챙겨줬나요?” “우리의 비영리 파트너십 안내 받으셨나요?” 같은 질문도 있는데, 실무 직원들이 괜히 불이익 받을까봐 일단 “예”라고 대충 체크하는 습관 있음
     * 90년대 가족 집 근처 Microcenter 매장을 애용했던 추억. 그땐 아주 자유로운 반품 정책이 인상적이었고(오픈한 소프트웨어도 30일간 반품 가능, 일명 ‘전자제품계의 L.L. Bean’ 명성). 어린 시절 주 수입원이 잔디깎이 알바였던 터라, 한 달 새 PC 게임 클리어 엔딩 달성에 숙련됐던 경험. 그래도 나중에 돈 벌고 한참 뒤엔 큰 박스 게임들 여러 개를 Microcenter에서 정식 구매했고, 지금도 ‘Betrayal at Krondor’ 같은 명작 RPG 타이틀 소장
     * Fry’s Electronics 관련 HN 토론 리스트, 링크 모음:
          + Burbank Fry’s Electronics 철거(2025)
          + Fry’s Electronics 전매장 폐점(2021)
          + Fry’s Electronics 위기설(2020)
          + The Fry’s Era(2019)
          + 전자부품 중고 창고 관련 Sundown for Surplus(2018), Weird Stuff Warehouse 폐점(2018)
          + Fry’s 위기설 관련 기사에 ‘Betteridge’s law(질문 기사 제목은 대부분 “아니다”로 귀결)’ 대입하며 웃음
     * 예전 bay area 살 때 Microcenter가 극장 바로 옆에 있어 좋았던 시절 그리움
     * Atlanta 지점 Microcenter엔 다크포스, Jedi knight 등 미개봉 고전 소프트웨어가 창고 바구니처럼 진열돼 있었던 기억. 새 Silicon Valley 매장도 혹시 그런 클래식 타이틀 중고 제품 코너 있으면 좋겠다는 기대
     * 처음 컴퓨터 Apple ][+ 구매를 Upper Arlington(Columbus), Ohio의 초창기 Micro Center 매장에서 했었음. 당시에는 아주 작은 매장이었는데 시간이 지나 거대해졌던 변화 체감. 현재는 NJ에 거주하며 40분 거리 매장 자주 이용 중. 많은 오프라인 매장이 사라진 와중에 Micro Center가 생존하는 게 놀랍고, 예전처럼 직접 돌아다니며 숨은 보석 탐색하는 재미를 계속 느끼는 셈
          + Columbus 매장 중간중간 방문 90년대부터 이어옴. 요즘은 옛날만큼 ‘너드 냄새’가 나지 않아 약간 아쉽지만, 가격경쟁력·서비스 모두 좋음. 315번 도로를 타고 잠깐 다녀오는 쾌감이 Amazon, Ebay보다 나은 점. 반면 Best Buy는 희화화된 전자제품 Value City Furniture와 TJ Maxx가 합쳐진 느낌이라 실망. Micro Center에선 단정은 한데 조금은 흐트러진 차림의 세일즈 직원이 “구매할 상품에 스티커만 붙이면 된다”고만 하면 끝. Best Buy에선 초췌한 파란 폴로 유니폼 직원들이 불필요한 업셀만 반복하는 피곤함
"
"https://news.hada.io/topic?id=21291","DiffX – 차세대 확장 가능한 Diff 포맷","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       DiffX – 차세대 확장 가능한 Diff 포맷

     * 기존 Unified Diff 포맷은 개발 환경의 요구사항을 충분히 반영하지 못하는 한계점 존재
     * DiffX는 기존 형식과 완벽하게 호환되며, 미래를 고려한 구조와 메타데이터 확장성 제공
     * 여러 커밋 정보와 바이너리 파일, 문자 인코딩 및 메타데이터를 구조화된 방식으로 저장 가능함
     * 표준화된 파싱 규칙 도입으로 다양한 도구(패치, 코드 리뷰 등)가 쉽게 연동 가능함
     * 기존 도구 및 워크플로우에서는 문제없이 사용 가능하며, 새로운 기능만 해당 도구의 지원 필요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개발자와 Diff 파일

     * 소프트웨어 개발자들은 보통 Git, Subversion, CVS 등에서 diff 파일로 코드 변경 내역을 확인함
     * Diff 파일은 텍스트 삽입(+)·삭제(-) 와 파일 이름, 경로, 타임스탬프, 일부 메타데이터 등을 포함하는 구조임
     * 대부분의 도구와 사용자는 Unified Diff 형식을 이용하며, 이 방식은 비교적 간단하게 차이점을 시각화함

Unified Diff의 한계

     * Unified Diff는 파일 식별, 변경 범위, 삽입·삭제된 줄만을 표준화할 뿐, 인코딩, 리비전, 확장 메타데이터 등은 표준화하지 않음
     * 다양한 소스 관리 시스템 지원, 신뢰성 있는 파싱, 풍부한 정보 추출이 어려움
     * 다음과 같은 문제점이 계속 발생함
          + 여러 개의 커밋을 한 번에 표현 불가함
          + 바이너리 파일에 대한 전용 표준 형식 부족
          + 문자 인코딩을 알 수 없어 정보 손실, 혼동 발생
          + 임의 메타데이터의 표준화 미비로 각 도구별로 형태가 상이함

개선 방향

     * 기존 Unified Diff는 구조와 표준이 부족하지만, 유연하고 다양한 환경에 이미 널리 퍼져 있음
     * Git Diff가 사실상 표준 역할을 하는 중이지만, 여전히 포맷의 공식 명세와 범용 확장성은 부족함
     * 기존 Unified Diff의 장점을 살리면서, 확장성과 표준 구조를 가미한 새로운 형식 필요성이 증대됨

DiffX란 무엇인가

     * DiffX는 확장 가능한 Diff 포맷으로, 기존 도구와 완벽히 호환되며, 인적 가독성을 유지함과 동시에 메타데이터와 구성을 구조적으로 담을 수 있음
     * 구문 예시:
          + 파일, 커밋, 전체 diff 등에 대해 메타데이터와 본문을 구조적, 확장 방식을 활용해 저장함
          + 예시 출력에서는 #diffx:와 같은 구문과, section, 메타데이터(JSON), 파일 경로, 커밋 정보 등이 포함됨

DiffX의 주요 특장점

     * 표준화된 파싱 규칙 제공, 도구에서 신뢰성 있게 정보를 읽고 쓸 수 있음
     * 메타데이터 저장·관리의 공식화: 전체 diff, 커밋, 파일 단위별로 사용 가능함
     * 기존 파서, 패처, 코드 리뷰 등 모든 도구와 호환됨 (새 기능은 지원 필요하나, 기존 기능은 호환성 보장)
     * 한 파일에서 여러 커밋, 바이너리 diff, 텍스트 인코딩 정보 등 복수의 내용을 효율적 구조로 표현 가능함
     * 도구에서 diff를 열어 필요한 정보를 기록 및 수정 후 다시 저장하는 변경성(mutability) 지원

DiffX의 지향점 및 비지향점

     * 모든 도구의 포맷 지원을 강요하거나, 호환성 문제를 만들지 않음
     * 벤더 종속성을 유발하지 않으며, 기존 워크플로우를 깨지 않음
     * 기존 Diff 파일의 문제점을 해소하고, 개발·리뷰·분석 도구에서 일관되고 신뢰성 있는 사용 경험을 제공함

        Hacker News 의견

     * 나는 “..meta”와 “…meta” 등 계층적으로 복잡한 포맷이 마음에 들지 않음 표현의 명확성을 위해서는 diff 전체, 파일, 청크 등 세 단계로만 구분해서 각기 다른 이름을 부여하는 것이 더 알아보기 쉬운 포맷이 된다고 생각함 메타블록이 없어도 한눈에 대상을 구분 가능해서 실수나 오류도 줄일 수 있음 diff 전체의 메타데이터와 파일 단위의 메타데이터가 같은 필드로 구성되는 것도 비합리적임 그리고 왜 JSON과 key=value 두 가지 포맷이 필요한지 모르겠음, 관리할 대상이 적다면 하나의 포맷만 쓰는 게 구현이나 기존 툴과의 통합에 훨씬 이로움 (grep, sed 또는 jq 중 하나만 쓰면 충분함) 추가로 리스트에 trailing comma 허용이 있으면 좋겠고, diff는 원래 분할 적용이 가능한 구조인데 이 포맷이 그 점에 어떤 영향을 주는지 궁금함(예를 들어 diff의 일부만
       적용하려면 프리앰블을 복사하고 블록도 따로 복사해야 해서 번거롭다고 생각함) revision은 파일 속성인지 아니면 커밋 체크섬인지 궁금증이 듬
          + 우리는 구조에 대해 여러 접근을 실험해보다가 결국 파싱 관점에서 단순함을 위해 #<section_level><section_type> 형태로 정리함 메타블록마다 수직적으로 수준만 체크하면 되고, 점 개수를 세면 자연스럽게 어느 레벨의 메타인지 구분 가능 키/값 헤더 포맷은 파서가 미리 아는 간단한 속성만 담으려고 했고, 자유로운 메타데이터는 별도의 meta 블록에 담도록 설계 기존 JSON 뿐 아니라, 시간이 지나 다른 시리얼라이즈 방식이 필요할 때도 확장성을 갖도록 헤더에 포맷을 명기할 수 있음 단순함과 유연성 사이에서 균형을 잡으려고 노력한 결과임 trailing comma는 개인적으로 넣고 싶지만, base-level JSON의 호환성 문제 때문에 JSON5 파서를 필수로 요구하긴 어려움 diff는 여전히 분할 가능하고, Unified Diff가 무시하는 영역에 정보를 넣은 덕분에 GNU patch 등에서는 무시되어
            문제 없음 다만 두 개의 DiffX 파일로 분할한다면 헤더를 새로 추가해야 하므로 조금 복잡해질 수 있음 어떤 SCM의 diff는 분할해도 일부 메타데이터(예: parent commit 정보)를 잃거나, 적용 대상에 따라 정보 손실이 생길 수 있음 revision은 SCM마다 다르며 커밋 ID, 파일별 ID 또는 그 조합, 추가 정보 등 매우 다양한 필요가 존재 SCM마다 다양한 요구를 충족하려 고려한 구조임
     * 내가 보기엔 아래 네 가지 지적 중, 실제로 diff 파일의 일반화로 합리적인 건 바이너리 패치 표기법 하나뿐임 나머지는 특정 버전 관리 시스템(SCM) 내부의 데이터나 프로토콜 문제라서 각자의 클라이언트, 서버, 백업 시스템에서만 통용되는 내용임 다른 모두는 불필요해 보임
          + 하나의 diff로 여러 커밋 나열 불가
          + 바이너리 패치에 대한 표준 없음
          + 텍스트 인코딩 인식 불가(은근히 문제됨)
          + 임의의 메타데이터 표준 포맷 부재
          + 우리는 20년간 12개 이상의 SCM을 연동하는 코드 리뷰 제품을 개발해왔고, 상상지도 못한 diff 포맷 및 SCM별 문제들을 수도 없이 겪음 실제로 end user가 직접 신경 쓸 문제는 아니지만, 툴 개발 측에서는 반드시 해결해야 할 pain point들이었음 일부 SCM에는 자체 diff 포맷이 없거나, 빠진 정보가 많아(예: 삭제 파일 표시 불가) 다른 툴들이 파일을 제대로 식별하지 못하게 만드는 경우가 많음, 그래서 이런 개선이 필요하다고 느낌
          + 요즘은 덜 흔하지만 나 역시 patch(1)와 비슷한 도구를 여전히 종종 씀 여러 플랫폼의 개발자가 협업할 때 파일명 대소문자, 문자 인코딩 이슈 등 때문에 다양한 문제가 여전히 생김
     * JSON을 길이 정보와 함께 self-delimitered 포맷으로 넣는 방식이라면, JSON 내용의 공백 한 칸만 바꿔도 JSON은 유효하지만 DiffX는 전체적으로 깨질 위험이 있음 구조적으로 클unky하고 messy하게 느껴지는 조합임(자체 헤더와 JSON payload 섞인 점, 점 개수를 안 세면 다른 meta 블록을 구분 못하는 점, 두 가지 파서가 필요한 구조 등) 메타데이터용 확장형 diff를 표준화하려는 발상은 좋으나, 이번 구현은 시행착오처럼 보임
     * patch 포맷이 이런 문제를 다 해결하고 있다고 생각함 git format-patch 설명 링크
          + 오늘 처음 이런 포맷이 있다는 걸 알게 됐고, 참고함(나는 그냥 평범한 인터넷 이용자임, 저자는 아님)
          + git에서는 해결 가능하지만, Review Board 같은 제품이라면 SVN, CVS, Perforce 등 여러 VCS와 통합해야 해서 이런 포맷이 등장한 배경으로 보임 나도 Review Board와 SVN을 쓴 적이 있는데, 여러 개발자가 git-svn과 svn을 혼합해서 리뷰 시 diff 업로드에 종종 문제가 있었음 양쪽 모두 지원하는 표준 diff 포맷이 있었다면 툴 사용에 훨씬 도움이 됐을 것임
     * 나는 실제로 제시된 문제들이 존재한다고 잘 못 느끼겠음(바이너리 파일 제외)
          + 인코딩이 달라도 patch 알고리즘은 동일하니 신경 쓸 필요 없음(문자가 꼭 유효한 utf-8일 필요도 없음)
          + 하나의 diff에 여러 커밋을 담고 싶을 일도 없음, 여러 diff로 나누는 게 직관적임
          + 메타데이터는 시스템 내부에만 유효한 게 아닌가 생각함
          + 인코딩 관련해서도 patch data는 어차피 ascii 기반 binary 데이터처럼 다뤄야 함 mixed encoding에 대해서 파일 수정을 할 수도 있으니 인코딩 고정의 의미가 별로 없음
          + 전혀 문제가 아니라고 봄, diff를 실제로 많이 쓰는 사용자들의 실제 경험을 들어보는 게 낫다고 생각함, 기존에 잘 동작하는 포맷을 굳이 overengineering하지 않는 게 좋겠음
          + 바이너리 데이터 문제는 확실히 존재함이라고 생각함
          + 보통 직접 도구를 만들거나 특정 SCM과 인터페이스해야 할 때만 실제로 이런 문제들을 마주침
              1. 인코딩 문제는 파일명과 본문 양쪽에 존재함, Git은 파일명 인코딩을 신경쓰지만 대부분 SCM은 그렇지 않아서 한 환경에서 자동 생성한 diff가 다른 환경에선 파일명을 찾지 못하는 경우가 있음(Perforce, Subversion 등에서 봄), 본문 역시 SCM마다 지역 인코딩에 따라 diff가 깨질 수 있음, 윈도우-리눅스 환경 오갈 때 개행 문자가 뒤섞여서 patch 적용이 안 되거나 BOM 문제 때문에 GNU patch가 깨진 적도 있음
              2. 여러 커밋을 한 번에 다루거나 툴로 넘길 때, 파일 누락이나 일관성 문제 등 다양한 문제가 발생 가능하고, diff 간을 일일이 sanity check하기 번거로움, 도구마다 지원하는 포맷도 다르니 불편함
              3. 저장소에서 파일을 찾으려면 시스템마다 커밋 단위, 파일 단위, 조합, 관계 정보 등 각종 식별자가 필요함, 심볼릭 링크나 파일 모드, SCM 특성 정보처럼 Unified diff에 안 들어가는 데이터도 필수임
     * 전체 문서는 읽기 어렵게 느껴짐 내게 ‘diff’란 두 항목(파일, 디렉터리 등)의 차이를 뜻하지만 TFA에서 말하는 diff는 내가 아는 ‘patch’임 여기서 논의되는 건 diff가 아니라 patch 메타데이터 관리에 관한 내용임 메타데이터를 JSON처럼 필수 형식으로 표준화한다면 괜찮겠지만, 애매하게 self-describing length-delimited 구조라 문제를 숨기는 느낌임 표준화 자체는 좋은데, 좀 더 명료하게 정리된 솔루션이 필요하다고 느낌 내가 보기엔 git diff 스타일이 사실상 표준에 더 가깝다고 생각이 드는 것도 재밌음
          + 마지막 구절에 완전히 동의함, diff를 여러 개로 나누어 쓰면 됨
     * 이런 포맷이 어떤 문제를 해결하려는지 궁금함 패치/디프 포맷이 충분히 좋지 않다는데, 누구를 위한 개선인지, GNU Patch 커뮤니티에서 불만이 늘어나서 그런 것인지, 뭐가 이유인지 더 구체적이어야 함 실제로 더 나은 패치 포맷이 꼭 필요한 이유가 뭔지 궁금증이 남음
          + 내가 너무 길어서 여기에 못 쓰는 글을 따로 정리한 것이 있는데 요약하자면 SCM을 직접 만들거나 SCM과 연동하는 도구 개발자들을 위한 고민임 일반 사용자라면 신경쓸 필요 없음 diff 포맷도 SCM마다 전부 달라서, 정말 잘 만든 것이 있기도 하고 심각하게 부족하거나 포맷이 아예 없는 것도 있음 여러 SCM을 커버해야 하는 Review Board 같은 제품 입장에서는 이런 통합 표준이 실무적으로 꼭 필요함 실제로 SCM 벤더들과 협업한 피드백을 반영한 개선 시도임
          + Review Board 위주로 쓰이는 포맷으로 보이는데, 이 제품은 다양한 VCS를 지원하고 소스 리뷰에서 diff가 핵심이므로 도입한 듯함
     * 가장 일반적이고 명확한 diff 표현법은 파일 두 개를 그냥 포함하는 방식임 지금은 데이터 용량이 문제되지 않으니 diff a b | patch c 대신 apply a b c처럼 만들면 내부 표현은 어떤 것이든 상관없음 diff는 인간이 읽기 어렵고, 컬러 사이드-바이-사이드 뷰가 훨씬 낫기 때문에 애초에 두 파일을 다 받아서 처리하는 게 직관적임 굳이 표준화되지 않은 diff를 전송할 필요가 없다 생각함
          + 파일 두 개로 만드는 diff는 하나뿐이 아니고, 다양한 목적에 맞는 여러 가지 버전의 diff가 나올 수 있음 diff 포맷이 있으면 diff 생성과 적용 로직을 분리해서 n*m 문제를 n+m 문제로 줄일 수 있음
          + 프로그램 업데이트 등에서 매번 130GB 전체를 새로 받게 되는 건 짜증 나지만 거의 동일한 파일끼리는 압축도 쉬워서, 두 버전 파일 차이만 전달하는 방식도 실익이 크다고 생각함 오리지널 파일의 해시값만 보내고 압축 파일 본체를 전송하는 등 더 효율적인 방법도 가능성 있어 보임
          + 파일 두 쌍의 전송 및 관리는 전용 컨테이너 없이는 어렵고, 여러 개의 변경(10개 파일 수정+삭제+추가 등)을 이메일 등으로 주고받으려면 오히려 tar/zip 등 복잡한 구조가 되는 pre-VCS 시절처럼 퇴보함 느낌임
          + diff 대신 전체 파일을 전송하는 게 더 직관적이어도, 실제 용도와 환경에 따라 diff는 여전히 중요한 의미를 가짐 요즘처럼 LLM으로 코드 에디트 등 결과물을 생성할 때 diff로 요청하면 토큰을 크게 아끼고, 응답 지연을 5-10배까지 줄이는 등 현격한 효율 향상이 있었음 파일 두 개 모두 보내는 건 토큰 낭비와 비용이 큼 코드 샌드박스나 원격 머신에 빠르게 적용하려면 diff가 큰 최적화 이점이 있음 파일 A와 A2, diff AxA2가 있으면 A2 재구성도 쉽고 저장소 최적화도 가능함 병합시 충돌 문제가 있다면 그때만 직접 개입하면 됨 요약하자면 diff는 훌륭함
     * diff 도구가 줄바꿈 단위에 너무 의존하는 점이 아직도 불만임 한 줄이 너무 길 때(예: JSON, 긴 배열 등) 리뷰가 어려움
          + 나도 동감함 구조화된 데이터(예: AST diff 등)에서 더 나은 diff 표현 방식을 탐구할 여지가 많다고 생각함 이 포맷(DiffX)은 기존 Unified Diff 포맷의 확장형임에 중점을 두었고, 만약 AST 등 더 구체적인 포맷이 널리 쓰인다면 그 또한 쉽게 내장 및 지원할 수 있도록 설계했음
          + 흔히 쓰는 형태는 인간 가독성과 툴 파싱의 절충점이라서 어중간한 구조임, 이번엔 메타데이터 확장으로 문제를 일부 해결하려 한 듯하지만 정말 좋은 솔루션은 평문이 아닌 읽기 쉬우면서 파싱 가능한 포맷을 새로 정하는 것일 듯함 긴 줄이나 구조 데이터에 대해 기존보다 더 나은 diff 알고리즘을 만드는 것이 어려운 과제지만 충분히 해결 가능하다 생각함
          + git은 line diff보다 세밀한 word diff도 지원하고, 기본 구분자는 공백임
     * JSON이 유일하게 지원되는 메타데이터 포맷이라는 점에 의문감이 듬 보편적인 목적으로 설계된 메타데이터 표준에 JSON은 오히려 복잡함
          + 왜 JSON이 과도하게 복잡하다고 생각하는지 구체적인 설명을 듣고 싶음
"
"https://news.hada.io/topic?id=21185","미국 무역법원이 트럼프 관세를 불법으로 판결","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        미국 무역법원이 트럼프 관세를 불법으로 판결

     * 트럼프 대통령의 글로벌 관세가 미국 무역법원에서 위법 판결을 받아 대규모로 중단되어, 공화당 경제 정책의 핵심 축에 큰 타격이 발생함
     * 판결은 민주당 주도 주정부 및 중소기업 측의 주장을 받아들여, 트럼프가 비상사태법 남용으로 관세를 부과했다고 판단함
     * 이번 결정으로 전 세계 수조 달러 규모 무역에 영향이 예상되며, 최종적으로 대법원 판단 가능성도 남아있음
     * 판결로 대부분의 글로벌 및 대중국 관세, 펜타닐 관련 관세가 중단되지만, 섹션 232/301 등 일부 관세는 영향받지 않음
     * 법원은 대통령의 비상사태 선포와 관세 명분이 법적 권한을 넘어섰다고 지적했으며, 민주당 주들은 관세가 미국 소비자에 대한 대규모 세금이라 주장함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

트럼프 글로벌 관세, 미국 무역법원에서 위법 판결

     * 미국 뉴욕의 미국 국제무역법원(US Court of International Trade) 이 3인 판사 패널을 통해 트럼프 전 대통령의 글로벌 관세 대부분에 대해 위법 판결을 내림
     * 민주당 주도 주정부와 중소기업 연합이 트럼프의 비상사태법 남용을 문제 삼아 소송을 제기했고, 법원은 해당 주장에 동의함
     * 트럼프는 이 판결에 대해 항소가 가능하며, 사건은 연방 항소법원을 거쳐 대법원까지 갈 가능성이 있음

시장 및 관세 영향

     * 트럼프의 관세 명령 이후 글로벌 시장 변동성이 커지며, 수조 달러 규모의 시장 가치가 등락을 반복함
     * 법원 판결로 트럼프의 글로벌 고정 관세, 대중국·캐나다·멕시코 펜타닐 관련 관세가 대부분 중단
     * 단, 섹션 232(철강, 알루미늄), 섹션 301(자동차 등) 에 근거한 관세는 이번 판결에서 제외됨

백악관 및 행정부 입장

     * 백악관 대변인은 ""비선출 판사가 국가 비상사태 대응 방식을 결정할 수 없다""며 반발
     * 트럼프 측은 ""무역적자가 국가 비상사태를 초래하고, 이는 미국 산업과 국방기반 약화로 이어진다""며 관세의 필요성을 강조함

비상사태법 및 법원 판단

     * 트럼프는 국제비상경제권법(IEEPA) 을 근거로 연간 무역적자가 국가 안보·경제에 ""비상하고 이례적 위협""이라고 주장하며 관세를 부과함
     * 법원은 초기 행정명령과 보복국 대상 추가 관세 모두 대통령 권한 범위를 초과했다고 판단
     * 멕시코·캐나다 대상 관세의 경우, 실질적으로 마약 밀수와 연관이 없는 상품까지 과도하게 타겟팅했다고 봄

원고 측 주장

     * 보수 성향 법률단체와 중소기업은 트럼프가 ""실제 비상사태가 아닌 사유""로 법을 남용했다고 주장
     * 민주당 주들은 해당 관세가 ""미국 소비자에 대한 대규모 세금""이라며, 의회의 권한 침해를 문제 삼음

법적 쟁점 및 판결 내용

     * 행정부는 ""대통령의 판단에 대해 사법부가 지나치게 개입하는 것""이라며 반박
     * 정부는 만일 원고 승소시 한정적 구제를 요청했으나, 법원은 ""위법 행정명령은 모두에 적용된다""며 광범위한 판결을 내림
     * 법원은 ""거짓 국가 비상사태"" 주장에 대해선 이번 판결의 핵심이 아니라고 판단

후속 절차 및 정치적 배경

     * 미국 국제무역법원은 연방 법원 시스템에 속하며, 무역·관세 분쟁 전담
     * 트럼프가 항소할 경우 연방 항소법원, 궁극적으로 대법원까지 이어질 수 있음
     * 공화당은 대통령의 보복 관세 권한 확대를 추진하지만, 트럼프 관세의 부정적 영향으로 입법 의지는 약화

관련 사건 및 참고

     * 대표 사건명: V.O.S. Selections v. Trump, 25-cv-00066, Oregon v. Trump, 25-cv-00077 (미국 국제무역법원, 맨해튼)

        Hacker News 의견

     * 판결문(PDF) 공유: https://www.cit.uscourts.gov/sites/cit/files/25-66.pdf 정보 제공
     * 아카이브 링크 공유: https://archive.md/DMT9d 자료 안내
     * 나는 법률 전문가는 아니지만, 왜 트럼프 행정부가 1930년의 Tariff Act를 사용하지 않았는지 의문 발생 이 법은 50% 한도를 두긴 해도 관세 정당화에 더 쉬워 보이는 근거라고 생각 법령이 너무 오래된 것(거의 100년 외) 외에 또 다른 실질적 문제가 있는지 궁금증 제기
          + 왜 트럼프 행정부가 1930년 Tariff Act를 사용하지 않았냐는 질문에, 아마도 법적 논리상 승산이 없는 주장임을 알았기 때문이라는 의견 판결문에서는 non-delegation doctrine(권한 위임 원칙 위반)과 major questions doctrine(중대 질문 원칙)을 다루며 이런 해석이 위헌적 권한 위임으로 간주될 가능성 강조 Nixon 행정부 때 유사 사례 언급(Yoshida II 사례 참고) 설령 타당하게 해석되더라도, 판결문 35쪽 근처에서 대통령의 executive order가 balance-of-payments 문제로 정의되어 별도의 제한적 권한만 위임되었다고 설명 1974년 무역법 122조(section 122 of Trade Act of 1974)는 ""관세 15% 상한과 150일 한시적 부과""로 대통령 권한 제한 응급 권한 행사에도 절차적 제약만 허용된다는 판결문 결론 인용 개인적 의견이 아니라 판결문 논리를 적용한 것만 강조
          + 대통령이 ""나는 누구와도 무역적자가 싫다""는 논리를 미국의 교역 상대국이 어떤 부담이나 불이익을 준 것으로 설득해야 함을 지적
          + 1930년 Tariff Act는 대통령의 외교 집행 권한과 의회의 과세 권한 사이의 이중성을 반영 20세기 초까지는 관세가 연방 수입의 주요 원천이었기 때문에, 의회는 관세를 의회 관할 세금으로 봤지만, 동시에 외교 정책 도구로서 대통령 소관임도 인정
          + 나는 비전문가이지만, Smoot-Hawley(스무트홀리 법) 해당 조항은 외국이 미국뿐 아니라 모든 상대국에 똑같이 적용되지 않을 때(즉, 차별 시)만 발동 가능 그저 미국만 다르게 대우받는 것이 아니라, 미국이 제3국과 달리 차별받았는지 여부가 핵심임을 지적
          + 법령이 100년 가까이 되었다고 사문화돼야 한다는 의견에, 헌법도 200년이 넘은 점을 들어 연령만으로 법의 유효성을 판단할 수 없다고 강조
     * 기업들이 이번 판결로 인해 법원에 정부가 위헌적으로 걷어간 관세를 환불해 달라고 요청할 수 있을지 궁금 소기업에게 이런 관세 부과가 사업 지속에 치명타였다는 점에서 관심
          + 미국이 4월 한 달 동안 사상 최대 관세 수입($160억)을 거둬들인 사실 언급 소비재의 경우 기업들이 가격 책정이 적응될 때까지 관세 부담을 떠안은 사례 가능성 제시 TACO(관세 관련 기업 비용/가격 전가 화두) 언급
          + Federal Tort Claims Act(FTCA) 개요 보고서 PDF(https://sgp.fas.org/crs/misc/R45732.pdf), 미국의 국가면책 관련 위키피디아(https://en.wikipedia.org/wiki/Sovereign_immunity_in_the_United_States) 리소스 제공
          + 실제로 누군가가 관세를 냈는지, 또는 관세가 도입된 후 뭔가가 세 배로 비싸졌다는 증거를 발견하지 못했음
     * 올해 말까지 트럼프의 관세 정책들이 해체되고, 의회로 넘어갈 것으로 예측 의회가 비웃듯 논의할 것이고, 법원이 부분적으로 해체해가는 속도를 보면 실제 변화는 굉장히 느릴 것이라는 전망 결국 정부는 느리게 움직이지만 일은 진행됨을 강조
     * 이번 판결이 놀랍지는 않음 미국 정부 구조의 목적 자체가 한 사람이 왕처럼 행동하지 못하게 설계된 것이란 생각, 이렇게 오래 걸렸다니 오히려 놀라움
          + 미국이 정말로 이 목적을 기억했다면 직장 문화는 크게 달랐을 것이라는 반어적 지적 법원이 대중의 이익을 위해 행동하기를 바라는 것도 이제는 기대하지 않는 심정
          + 의회가 오히려 왕을 원한다는 인상
          + 실제로 미국 정부 구조의 '목적'은 한 사람의 왕화를 막는 것이 아니라, 선출된 입법부(의회), 선출된 집행부(대통령), 그리고 선거와 분리된 사법부의 권력 혼합이라는 점을 강조 Federalist 70 논거 참고(https://avalon.law.yale.edu/18th_century/fed70.asp), 고대 로마 독재관 사례로 행정부의 강한 권력 필요성 강조 대통령이 강력한 권한으로 외교 정책 행사할 수 있지만 입법/사법 권한까지 집결시키지는 않는 구조라고 설명 관세가 일반 정책이냐, 외교 수단이냐에 따라 의회의 권한과 대통령의 권한 영역이 달라진다는 관점 제시
          + 견제와 균형 시스템이 긍정적인 제도라는 점과, 이런 조치가 더 늘어나길 희망
          + 최근의 여러 사건들을 보면 이러한 견제와 균형의 실효성이 의문스럽게 느껴진다는 인상
     * 70년 된 법률이 주요 근거가 될 때마다 긴급 권한 사용이 너무 편리해져 그 누구도 쉽게 포기하지 못하는 상황이라는 느낌 공론화와 토론에 상관없이 결국 대통령 중심으로 돌아가는 무역 정책 현실 지적 이번 판결이 실질적으로 어떤 변화를 가져올지는, 사람들이 얼마나 후속 조치에 신경을 쓰는지에 달렸다는 생각
          + 근거 법이 90년 됐다는 추가 지적
     * 무역법원, V.O.S. Selections, Inc. vs Trump 소송 상황과 1심 및 항소심 사건현황 링크 공유 <br>무역법원 사건 링크 <br>항소법원 사건 링크
     * 합법적이지 않은 관세였던 만큼 이미 납부한 관세 환불 가능성 제기 특히 소규모 사업자가 부담한 관세 환급은 도움이 될 것이라는 생각 현재 의회 예산안 논의에서 관세 수입을 반영했다면 이제 그 수입이 사라질 텐데, 이로 인해 지출 삭감이나 감세 축소로 대응할지 궁금
          + 이상적으로는 관세를 아예 내지 않는 게 최선이었을 텐데 지금 기준으로 환급이 이루어질지는 회의적 현실
          + 이번 판결이 확정(최종적으로 대법원까지 갈 가능성 있음)된다면, 미국 기업들(직접 또는 간접 관세를 부담한 기업)이 정부를 상대로 환급 소송을 제기할 것이라는 예상
     * 이제 이 판결 자체가 과연 무슨 실효성이 있는지 회의적 현 행정부도 자신만의 법을 만들며, 누구도 실제로 제동을 걸지 않는 듯한, 권위주의 국가에서 볼 수 있는 현상과 유사점 지적
          + 이런 상황에서 두뇌 유출(브레인 드레인) 현상이 계속될 것이라는 한탄
"
"https://news.hada.io/topic?id=21215","Stanford CRFM: AI로 생성된 CUDA 커널, PyTorch 최적화 코드 성능을 넘다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Stanford CRFM: AI로 생성된 CUDA 커널, PyTorch 최적화 코드 성능을 넘다

     * AI가 생성한 CUDA-C 커널들이 PyTorch의 전문가 최적화 커널과 비슷하거나 더 나은 성능을 보임
     * 단일 LLM(대형언어모델)이 자연어 최적화 아이디어 생성과 다양한 코드 브랜칭을 반복, 기존 방법보다 최적화 다양성과 병렬 탐색에서 뛰어난 성능 달성
     * 대표 벤치마크 결과, Matmul(101%), Conv2D(179.9%), Softmax(111.8%), LayerNorm(484.4%), Conv2D+ReLU+MaxPool(290.1%) 등에서 PyTorch 대비 압도적
     * 기존 “순차적 커널 개선”의 한계를 넘기 위해 자연어 추론 + 브랜칭 구조 적용, 빠른 속도로 고성능 커널을 생성
     * FP16, Flash Attention 등 최신 ML 워크로드에서도 진보 중이며, 미래에는 AI가 자체적으로 더 빠른 커널을 탐색·개선하는 패러다임이 주류가 될 가능성을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

TL;DR 주요 성과

     * Stanford CRFM 연구팀은 AI가 생성한 고성능 CUDA-C 커널이 기존 PyTorch의 전문가 설계 커널과 유사하거나 더 나은 속도를 내는 예를 발견함
     * 원래는 합성 데이터를 통해 더 나은 커널 자동생성 모델을 학습시키려고 했으나, 합성 데이터 생성만으로도 놀랄 만한 수준의 빠른 커널이 자동 생성되는 현상을 관찰함
     * 이에 따라 방법, 성능 벤치마크, 최적화 전략, 앞으로의 방향 등을 조기에 공개함
     * 벤치마크: Nvidia L40S GPU 기준. 성능(%): PyTorch 기준 실행 시간 ÷ 생성 커널 실행 시간
          + Matmul (FP32): PyTorch 대비 101.3% (4096x4096 행렬)
          + Conv2D: 179.9% (입력: 100, 3, 224, 224; AlexNet Conv1 규격)
          + Softmax: 111.8% (4096x65536 텐서)
          + LayerNorm: 484.4% (16, 64, 256, 256 텐서)
          + Conv2D + ReLU + MaxPool: PyTorch reference 대비 290.1%, torch.compile() 대비 189.0%

연구 동기 및 방법

     * 원래 목적은 커널 생성 모델 학습용 합성 데이터 생성이었으나, 실험 과정에서 자체적으로 생성된 커널이 예상을 뛰어넘는 고성능을 달성
     * KernelBench(Stanford 공개 벤치마크, 2024년 12월 공개) 활용
          + 주어진 torch 코드에 대해, LLM이 최적 속도 커널을 새로 작성
          + 입력/출력 결과의 수치 동치 여부로 정확성 검증
     * 기존 방식: 커널을 단계별로 조금씩 고치며 점차 개선하는 순차적 수정 루프
          + 단점: 아이디어 다양성 부족, 같은 최적화 반복, 지역 최적점 수렴
     * 개선 아이디어
         1. 최적화 아이디어를 자연어로 발상 및 기록한 뒤, 그 아이디어들의 코드 구현분기를 여러 개 동시에 생성
         2. 각 라운드마다 여러 최적화 시도 병렬 진행 → 최고 성능 커널로 다음 라운드 씨앗(시드) 설정
          + 이렇게 하면 한정된 탐색 반복 안에서 다양한 최적화 전략 탐구와 병렬 탐색이 가능

최적화 아이디어 예시

     * 메모리 접근 최적화: global/shared/register 메모리 계층 효율 개선
     * 비동기 처리 및 레이턴시 은닉: 연산과 데이터 이동을 오버랩
     * 데이터타입/정밀도 최적화: FP16/BF16 활용 및 하드웨어 특화 연산
     * 계산 및 명령어 최적화: 연산량, 명령어 수, 하드웨어 특수 명령 최적 활용
     * 병렬성 및 오큐펀시: SM(Streaming Multiprocessors) 활용 극대화
     * 루프/분기/인덱싱 최적화: 루프 최소화, 불필요한 인덱스 연산 제거

커널 최적화 실제 과정(Conv2D 예시)

     * 라운드별 최적화 아이디어 및 성능 개선 흐름
          + 최초(0라운드): 단순 CUDA 포팅(PyTorch 대비 20% 속도)
          + 다음 라운드: → 읽기 캐시 활용 → FP16 Tensor Core 연산(GEMM 변환) → 이중 버퍼링/파이프라인 → 인덱스 사전계산/공유메모리 → 벡터화 → K-루프 동시버퍼링 등 고도의 GPU 아키텍처 활용
          + 최종(13라운드): PyTorch 대비 179.9% 성능 확보
     * 실제 코드는 고급 CUDA 프로그래밍 기법 대거 활용
          + 각 라운드마다 새로운 아이디어를 자연어로 생성하고, 여러 구현을 병렬 시도하여 최적 코드를 선택

Takeaways 및 시사점

     * AI 기반 커널 생성이 강력한 탐색 루프와 자연어 기반 다양한 최적화 아이디어 조합으로 인간 전문가 수준을 뛰어넘을 수 있음
     * 일부 최신 연산자(FP16 matmul, Flash Attention)는 현 시점에서 PyTorch 대비 아직 성능 낮음
     * FP32 계산이 최근 하드웨어에서는 FP16/BF16에 비해 상대적으로 최적화 덜 됨 → 해당 영역에서 성능 우위 가능
     * 제한된 탐색 토큰(입출력 합 700만 토큰) 상황에서도 지속적인 성능 개선 확인
     * AlphaEvolve, Gemini 2.5 Pro 등 최근 연구도 대량 브랜칭+검증 기반 탐색이 재학습 없이도 획기적 성능 개선 가능함을 시사
     * 앞으로는 이런 방식으로 합성 데이터 및 고성능 커널을 대량 생성하며, AI가 자체 개선하는 루프(자기개선형 AI)로 발전할 것임

결론

     * AI 기반 커널 자동 생성·최적화는 이미 전문가 핸드코딩 수준에 도달했으며, 빠른 미래에 모델+브랜칭 탐색+검증 조합으로 자가개선 AI 시스템이 가능할 전망
     * PyTorch·TensorFlow와 같은 프레임워크의 성능 한계를 AI가 자체적으로 넘어서게 될 가능성 대두

부록: Conv2D 커널 전체 코드

     * 실제 함수와 커널 전체 소스코드가 포함됨(상세 코드 생략)
     * 코드 내 주요 특징:
          + 공유 메모리 벡터화, K-dim 계층적 double-buffering, CUDA WMMA, warp-level output buffer 등
          + 실시간 동적 인덱스 계산, bias 처리, K 루프 안의 지연 데이터 동시 로드 등
     * 완전한 샘플 코드 및 예시 커널은 깃허브 저장소에서 확인 가능

   일종의 앙상블 기법이라고 해야할까요. 굉장하네요

        Hacker News 의견

     * 나는 이 글의 저자들이 ""AI 에이전트""를 바라보는 방식이 꽤 흥미롭다고 생각함
       대부분 사람들은 에이전트를 인간 직원처럼 생각하는 경향이 있음
       몇 명 안 되는 에이전트를 병렬로(종종 한 명만) 설정해 놓고, 각각이 루프를 돌면서 한 번에 하나의 일만 처리하게 함
       여전히 고정된 인원 수와, 한 번에 한 가지 일만 가능한 직원, 그리고 작업 전환이 느린 세상에 머물러 있음
       하지만 LLM은 그렇지 않음
       사실상 무한한 에이전트를 언제든지 마음껏 만들어낼 수 있음
       LLM 요청을 일렬로 처리하나 병렬로 처리하나 비용 차이가 없음
       이 사실을 인지하면, 각 에이전트가 필요에 따라 자신을 여러 하위 에이전트로 분기시키는 패턴이 자연스럽게 떠오름
       이것이 바로 저자들이 시도한 방식임
       에이전트를 ""작업(task)""이나 ""잡(job)""로 간주하고, Celery나 Sidekiq에서 배운 점을 적용하는 관점이 더 적합하다고 느낌
     * ""FP32는 최신 ML 워크로드에서 부쩍 덜 사용되고, 최신 하드웨어에서는 FP16이나 BF16에 비해 최적화도 덜 되어 있음. 그래서 FP32 커널에서 PyTorch보다 성능을 쉽게 개선할 수 있었던 이유 중 하나가 될 수 있음""
       개발자들이 수년에 걸쳐 fp32 버전 커널을 최적화하는 데 시간을 거의 투자하지 않았음
       정말 재미있는 건, 실제로 집중적으로 개발된, 사람들이 진짜 사용하는 커널 쪽 성능을 올릴 수 있을 때라고 생각함
          + NVIDIA가 GPU에 대해 충분히 자세한 문서를 제공하지 않기 때문에 이런 좋은 결과가 부분적으로 설명된다고 생각함
            마이크로아키텍처가 잘 문서화된 프로세서라면, 프로그래머나 컴파일러가 결정적으로 최적 프로그램을 작성할 수 있기 때문에, 이미 알려진 해법을 찾는 수준을 넘어서서 ML/AI를 적용해 대단한 개선을 이루기는 쉽지 않음
            반면, NVIDIA GPU처럼 문서화가 덜 된 경우에는, 최적의 프로그램을 찾으려면 이전에 최적화된 프로그램의 예를 참고하면서 무작위 탐색을 하거나, 어떤 상황에서 실제 하드웨어가 어떻게 동작하는지 역공학적 분석이 필요한 경우가 많음
            이런 상황에서는 ML/AI가 가능성을 보일 수 있고, 잘 만들어진 프로그램들을 데이터로 학습함으로써 인간 프로그래머도 찾기 힘든 undocumented behavior를 포착할 수 있음
          + 혹시 fp16/bf16 커널의 이미 알려진 개선사항이 단순히 fp32에 옮겨진 결과일 수도 있지 않을까 궁금함
          + ""사람들이 수년간 fp32 커널을 전혀 손대지 않았다는 이야긴가?""
            와, 만약 그렇다면 AI가 사전 솔루션이 없던 영역에서 새로운 알고리즘을 만들어낸 셈이라니 정말 멋진 일이라고 생각함
     * 내 결론은 이 아티클, Google의 AlphaEvolve(여기), 그리고 최근 o3가 리눅스 커널에서 zero day를 찾았다는 소식(여기)에서 볼 수 있듯이
       특히 Gemini Pro 2.5와 o3가 기존 모델로는 안되던 아이디어들도 갑자기 해내는 새로운 능력 레벨에 도달했다고 느꼈음
          + 나는 갑자기 기존에 안되던 게 갑자기 되는 게 아니라
            사실상 인간보다 훨씬 빠른 속도로 반복과 테스트가 가능해졌고
            즉각적으로 대량의 정보를 활용할 수 있게 되면서
            방대한 정보, 발전, 그리고 지능적으로 적용된 브루트포스 조합이 일부 응용 분야에서 성공을 거두고 있는 시점에 이른 것이라고 봄
          + 네가 언급한 사례들과 이번 결과가 실제로 유사점이 많다고 생각함
            본문에도 ""테스트 시 반복 루프는 차례로 코드 수정 결과를 확인하는 대화식 챗봇이 아니라, 명확한 최적화 가설에 따라 적극적으로 병렬 평가를 진행하는 구조화된 탐색 방식에 가깝다""라는 언급이 있음
            내 결론은 이제 LLM의 능력을 바탕으로
            평가 함수가 명확하거나 유사한 패턴의 솔루션 공간을 현저하게 줄이는 방법을 터득했다는 점임
            어떤 모델이 다른 모델을 추월한다, 혹은 특정 모델만 풀 수 있다... 이런 모델 간 비교가 논점이 아니라
            이런 식의 응용이 충분히 드러나고 있다는 현실이 더 의미 있다고 느껴짐
          + Gemini Pro 2.5는 사람이 쓸 수 있는 첫 번째 AI라고 느꼈지만, 엄밀히 보면 겨우 그 임계점을 넘긴 수준임
            성공률이 20% 아래로 떨어지는 경우도 종종 있음
            3.0이 나오면... 이제는 살짝 무서울 수 있겠다는 생각이 듦
          + 잠깐, 무슨 뜻이지? 이거 리눅스 커널이랑 상관없고, GPU 프로그래밍에서 말하는 ""커널""임
            혹시 본문 전체를 잘못 이해한 거 아님?
          + 흥미롭긴 한데, 좀 더 강한 증거가 있음? 한 번만의 결과로는 설득력이 충분하지 않음
     * ""기준 코드는 디폴트 FP32고, 허용 오차는 1e-02임""
       이 정도 오차면 쉽게 fp16 연산으로 ""fp32"" 커널을 대체할 수 있음
          + 한 단계만 더 생각해 보면 실제로 이번 작업의 핵심이 해당 커널에서 가능한 많은 fp32 연산을 fp16으로 바꾸라는 게 아니었나 싶음
            이런 이식 작업이 실제로 얼마나 어려운지는 확인이 필요하지만
            직관적으로는 그리 인상적이지는 않다고 느껴짐
            내 생각이 혹시 틀렸다면, 저자들이 이 대목을 명확하게 다뤄줬으면 좋겠음
          + 이렇게 되면 결과는 무의미하게 됨
            상대 오차를 체크는 했을지 모르겠음
            float32를 float16으로 대체하는 건 의미 없음
            정밀도가 float32를 선택하는 유일한 이유라 할 만한데
            이 특징 자체를 잃으면 버전별로 차별화도 없음
     * 혹시 나만, 이 글 제목 보고 AI가 OS 커널을 만들어냈다고 착각해서 읽었던 사람임?
          + 나도 그랬음
     * 400% 속도 향상도 대단하지만
       무엇보다 흥미로운 점은: 반복마다 단순 연산 최적화가 아니라, 언어 추론 단계를 강제로 넣어 탐색 다양성을 이끌어낸 방법론임
       이 방식이 실제로 잘 작동했기에 매우 흥미로움
          + 나는 map-elites나 island 기법 같은 게 들어갔나 했는데, 이 부분을 놓친 듯함
            그냥 평범한 미메틱 진화라고 생각함
     * 정말 흥미로운 결과임
       이 사람들이 너무 신나서 블로그에 공개한 듯함
       사실 출판 전에 피드백 받으려는 의도가 있었을지도
       ""자체 개선(self improvement)""의 전설적인 길이 이거일지는 모르겠지만
       이런 결과가 바로 그 길에서 볼 법한 사례라고 느껴짐
          + ""진짜 자체 개선의 길인지는 모르겠다""
            이 방법들은 극도로 명확한 평가 함수가 있을 때만 효과가 있음
     * 내 경험 공유: replication 시도에서 LayerNorm 커널은 수치적으로 안정적이지 않아 검증 불가라고 봄
       평균 0, 표준편차 1로만 테스트해서 numerical cancellation 현상이 바로 드러나지 않았음
       덧붙여, 이후에 수치적으로 안정적인 버전을 새로 만들었다는 사실을 확인
       이 점은 훌륭하다고 생각함
     * 정말 멋진 결과임
       o3와 Gemini 2.5 Pro를 사용했지만
       어떤 쪽이 더 좋은 커널을 만들어냈는지는 아쉽게도 언급이 없음
     * AI가 생성한 코드가 분할 커널(fused kernel)처럼 넓은 영역을 어떻게 다루는지 관찰하는 게 흥미로운 포인트임
       예를 들어 gemm + relu + gemm + normalization 등이 있을 텐데
       튜너로 쭉 훑거나 사람이 일일이 작성하려면 상당히 고생스러운 작업임
          + 근데 여기서 말하는 ""커널""이 AI 맥락에서 정확히 뭘 의미하는지 잘 모르겠음
            OS 커널은 아니라는 점만은 확실함
"
"https://news.hada.io/topic?id=21235","봇 차단을 위해 작은 도구들 총동원하기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         봇 차단을 위해 작은 도구들 총동원하기

     * 대형 AI/검색 회사들의 무차별적 크롤링과 스크래핑으로 인해, 개인 서버와 서비스가 심각하게 영향을 받으며, 리소스 소모와 서비스 불안정이 지속적으로 발생함
     * Zabbix·Loki 기반 모니터링으로 비정상 트래픽 탐지 후, log 분석 도구(lnav, goaccess)와 SQL 기반 쿼리로 공격자 패턴, IP, User Agent를 상세 파악
     * Nginx 설정에서 User Agent 기반 403 차단, rate limit, Fail2Ban 연동 등 단계별 방어 체계를 구축해 수백 개의 악성 IP 차단과 서버 안정화 실현
     * 주요 문제는 Gitea 저장소 전체를 tarball로 대량 다운로드 시도하는 봇들이었으며, 단순 콘텐츠 소비자가 아닌 ""AI 데이터 수집/모델 학습 목적""의 트래픽이 급증함
     * 장기적으로 합법적 서비스(archive.org 등)에 대한 예외 처리와, 검색엔진 노출은 유지하되 AI 엔시티피케이션(en-shitification)에는 맞서는 전략을 고민 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 내 작은 서버에 쏟아진 봇 트래픽

     * 최근 개인적으로 운영하는 lambdacreate 블로그 및 여러 서비스에 정체불명의 대량 트래픽이 급증함
     * Archive.org과 같은 합법적 서비스는 환영하지만, Amazon, Facebook, OpenAI 등 대기업의 무분별한 데이터 크롤링이 사이트에 피해를 줌
     * AI 모형 학습 등으로 인한 데이터 수집 수요가 높아지면서 이런 현상이 더욱 심각해짐
     * 이런 상황에서 진짜 독자(사람) 대신, 주로 대량의 봇 트래픽에 시달림

문제의 확인: 모니터링 툴을 통한 트래픽 폭증 진단

     * Zabbix, Loki 등 모니터링 툴을 사용해 서버 자원 소모 상황을 분석함
     * Gitea 인스턴스가 하루에 20~30GB에 달하는 데이터 증가 현상, 각종 CPU/메모리 경고 발생
     * nginx 트래픽 분석 결과, 한 달 평균 8req/s → 순간적으로 20req/s 이상까지 급증함
     * 트래픽은 대규모이진 않으나 평소보다 10배 가까이 증가해 자원 고갈을 유발함

공격 원인 분석: 로그 파일 심층 분석

     * lnav와 goaccess를 이용하여 nginx access 로그를 SQL로 분석함
          + 방문자 IP, UserAgent, Referrer 등 패턴 파악
     * 결과적으로, 특정 서비스나 커뮤니티발 유입이 아니라 특정 IP 대역에서 대량 크롤링 발생
     * UserAgent에 Amazonbot, OpenAI, Applebot, Facebook 등 명시 혹은 위조된 값 다수 발견
     * 이로 인해 실제 서비스 이용에 지장을 받자 강력한 차단 정책 필요성 대두됨

해결책: Nginx, Fail2Ban 등 여러 방어 계층의 결합 적용

     * Nginx map을 이용해 악성 UserAgent 즉시 403 반환, rate limit(방문 속도 제한) 도입
     * 신규·미탐지 봇까지도 웹 요청 빈도를 낮춤으로써 서버 부담 최소화
     * 403 발생 로그를 기반으로 goaccess, lnav로 새로운 악성 IP와 UserAgent 탐지
     * 자동화 보안 도구 Fail2Ban을 통해 403 응답 과다 발생 IP를 24시간 자동 차단
          + 735건 이상의 자동밴 기록
     * 실제 리소스 사용률이 상당 수준 정상화됨
     * 앞으로 archive.org 등 정상 서비스에 대한 예외 규칙을 적용하고, 검색엔진 인덱싱은 허용하되 AI 훈련용 무분별한 크롤링은 계속 차단 계획

결론: 도구 조합의 힘과 개인 서비스 보안의 필요성

     * 이런 일련의 다중 계층 방어책을 적용함으로써, 원활한 개인 블로그 운영과 서비스 접근성 복구
     * 다수의 작은 시스템 관리 및 자동화 도구 활용이 개인 서버 보안에 효과적임을 확인함
     * 성장하는 AI 학습 수요로 인해 개인 서버까지 무분별하게 크롤링되는 환경에서는, 적극적인 차단과 관리 자동화가 필수적임

        Hacker News 의견

     * 많은 비양심적인 크롤러들이 대형 검색엔진을 흉내내기만 한다는 점을 자주 목격하게 됨, 유저 에이전트 정보에 속을 필요 없다는 사람들도 있는데, 내 방식 중 가장 좋아하는 건 robots.txt에 의심 정보(예: gzip bomb)를 넣고, 이를 감지한 크롤러가 다음 요청부터 차단되게 설정하는 방법임, Caddy로 간단하게 구현이 가능하고, 이렇게 하면 주로 심각한 위반자들을 잡아냄, 봇의 행동을 변명할 생각은 없지만, 만약 이런 봇 몇 개가 사이트를 다운시킨다면 악의적인 공격자에게 정말 취약한 사이트라는 증거가 됨
          + 마지막 코멘트가 정말 핵심을 짚었다고 느낌, 나랑 다른 세대일 수도 있지만 요즘 글 쓰는 분들이 왜 이렇게 적은 리소스 사용에 집착하는지 이해가 안 됨, 마치 조부모님이 LED 불 끄는 걸로 호들갑을 떨거나, 연료비 5센트 아끼려고 24km씩 운전하는 것 같음, 초당 20회 요청은 정말 아무것도 아님, 심지어 다이나믹하게 생성한다 해도(왜 굳이? 그 시간에 캐싱 세팅하는 게 훨씬 이득), 요즘 'fuck the bots' 같은 스타일의 글이 유행이긴 하지만 이건 새로울 것도 없는 주제임, 시간 낭비하지 않고 처리할 생산적인 방법 훨씬 많음
          + robots.txt로 gzip bomb 거는 방법 더 자세히 듣고 싶음, 대부분의 AI가 robots.txt 무시하는 걸로 알고 있는데, 결국 일부 순진한 크롤러만 걸리는 것 아닌가 궁금함, 남들에게 반대하는 것도 아니고 단순히 좋은 쪽에 피해 안 주고 구현하는 방법을 더 알고 싶음
          + 내 분야에서 제일 큰 위키 중 하나 운영하는데, 개발팀 다른 사람들 설득해서 gzip bomb을 쓰게 만드는 게 거의 불가능함, 이 방법은 위험 요소가 너무 크다(EU 규정 때문이라는 마인드)면서 추진할 가치가 없다고 고집함, 실제 공개 사이트에 진짜로 이런 방법 사용하는지 궁금함
     * 요즘은 봇들이 robots.txt 파일을 전혀 존중하지 않아서 진짜 화남, 이거 만든 사람들이 정말 이기적이라고 생각함, 그런 봇 만든 사람이면 알아서 하라는 생각임
          + robots 파일에 함정(허니팟) 심어두면, 완전히 무시하는 애들은 안 걸리더라도 일부러 말썽 부리러 찾아오는 봇들 걸러내는 데 도움이 됨
          + 챗봇, 검색엔진, 가격 비교 도구 같은 거 사용하는 사람에게도 비슷한 말을 해줄 수 있음, 사실 이런 사용자들이 스크래이퍼들을 경제적으로 유인하는 주범임
     * 글쓴이가 “이제 신경 안 쓴다”고 했다는 점은 이해하지만, 금지된 IP에 Google, ripe.net, semrush가 있는 걸 봤음, 다른 기업은 몰라도 Google은 정말 차단하지 않겠음, 사이트를 알리고 싶으면 Semrush나 ripe.net도 차단할 필요가 없다고 생각함, 내 콘텐츠가 AI나 이상한 애들이 스크래핑하더라도, 처음부터 사이트가 공개라면 어느 정도 활용될 거란 걸 각오해야 한다고 봄, 예를 들면 모텔 간판 불을 꺼놓고 손님을 초대하는 꼴임
          + Semrush는 오랫동안 여러 단계에서 진짜 심하게 민폐를 끼쳐서, 지난 8년 동안 robots.txt에 특이한 안내문까지 남겼음, 결국에는 법무팀까지 움직여서 겨우 진정시킨 경험이 있음, 내 입장에서도 'SEO' 업체가 실제 방문자를 밀어내면서 사이트를 거칠게 긁어가는 걸 허용하는 건 가치가 없음, Semrush 경쟁사들도 마찬가지로 심했음, 현재 AI 스크래퍼들도 수준이 너무 떨어져서, 대형 투자자와 PR 부서에까지 공식적인 항의문을 반복적으로 보내야 했던 적도 있음, 기술적으로도, 법적으로도 여러 방식으로 겨우 정상으로 돌려놓았음
          + 봇이 대량으로 트래픽(대역폭, 메모리, CPU, 디스크 자원)을 차지하는 게 실질적으로 문제임, 소개글에서도 납득할 수 없는 매너라고 언급되어 있음, 이런 트래픽을 스크래퍼에게 굳이 제공할 필요가 없다고 느낌, Google도 AI 스크래퍼를 돌리고 있어서 아마 그게 차단 목록에 잡힌 게 아닐까 생각함
          + Google을 사칭하는 진짜 악성 봇들도 존재함, 예전엔 Google이 상대적으로 예의를 갖추는 스크래핑을 했지만, 글쓴이가 차단하든 안 하든 이미 필요한 트래픽을 확보하고 있다면 신경 쓸 이유가 없을 것임
          + 지난 10년간 사람들이 Google을 사용해서는 안 된다는 걸 아직도 모르고 있었는지 궁금함, 특히 Google이 AI로 독립 사이트들을 검열하는 상황이 나타나고 있는데, 관련 코멘트도 직접 링크함, 이제는 Google이 오히려 위험(asset이 아니라 liability) 쪽에 가까워졌다고 봄
     * 봇 때문에 서버 로그 파일이 지나치게 커져서, 내 서버들은 아예 로깅을 끄는 상황을 맞음, 봇들이 API, 폼, 심지어 웹사이트에서 클릭으로만 접근 가능한 부분까지 집요하게 긁어감, Anthropic, openAI, Facebook 등도 여전히 내 사이트를 스크래핑 중임
          + 클릭으로만 접근 가능한 API의 경우라면, 봇들은 어떻게 접근하는지 궁금함
          + 그런 API 정보에 대해 더 자세히 알고 싶음, UI의 일부이거나 인간만 사용할 수 있는 부분을 의미하는지, 아니면 다른 경로가 아예 없는 것인지 명확히 하고 싶음, 최근은 AI 에이전트가 실제 사용자의 행동 패턴을 모방하므로, 인간과 봇을 구분하는 게 거의 불가능한 시대임
     * AI 크롤러 봇들이 User-Agent 헤더를 정직하게 채워줘서 좋다고 생각했는데, 이 정도로 트래픽이 많은 원인이 그것이라는 데 좀 놀랐음, 대부분의 웹사이트는 이렇게 자주 데이터가 필요하지 않은데도 트래픽이 너무 많음, 개발자 블로그라면 더더욱 이해되지 않음
          + AI 크롤러 대부분이 robots.txt를 준수하지만, 차단당하거나 막히면 곧바로 브라우저 유저 에이전트로 바꾸고, 주거용 IP에서 다시 크롤링을 시도한 사례들도 있음, 다만 가짜로 OpenAI/Amazon/Facebook을 사칭하는 경우가 워낙 많기 때문에 정확한 경우 구분에는 신중해야 함
     * tirreno 제작자임, 우리 플랫폼은 라이브 로그인 사용자에 최적화되어 있지만, 봇 탐지 및 차단에도 효과적으로 활용됨, IP 마지막 옥텟을 별표(*)로 치환해 같은 서브넷을 하나의 계정으로 묶어 IP를 익명화하는 방식을 사용함, 트래픽 이상(500/404 에러, 무차별 로그인 시도, 데이터센터 IP 등)에 대해 자동 블랙리스트를 생성하게 할 수 있음, tirreno의 블랙리스트 API로 원치 않는 트래픽을 에러 페이지로 리디렉션 가능, 모니터링 대시보드도 제공하여 오탐 방지와 관리에 도움이 됨
       tirreno Github, 관리자 데모
          + 요즘은 여러 ISP가 CGNAT 구조로 바뀌어서, 한 IP가 수백 명의 실제 사용자를 대표할 수도 있는 문제를 어떻게 처리하는지 궁금함
          + 공개된 IP범위 기반으로도 동일하게 봇 차단 개발 중임, 개선 아이디어 있다면 언제든 환영임
          + IP 마지막 옥텟 치환 방식으로 인해, 실제 내 정보와 전혀 상관없는 주소의 이웃과 단일 사용자로 묶이게 됨, 센터 IP로 인한 오탐도 무시 못 함, 실제로 뭔가 안 막혀 있으면 87개의 교통신호등을 클릭해서 간신히 통과해야 함, 결국 오탐을 회피하는 단계에서도 내 개인정보를 비동의 수집하진 않는다는 명분을 위한 것 같음, 고객이 실제로 유료 사용자를 놓치고 있다는 점을 즉각적으로 인지하게 만드는 피드백 구조는 꼭 갖춰주길 바람
     * 예전부터 궁금했던 점이 있는데, “페이지 노킹(page knocking)” 즉, 특정한 순서로 페이지들을 열어서 진입권한을 얻는 구조가 가능하지 않을까 생각함, 예를 들어 지정된 비공개 URL로 먼저 접근해야 나머지 페이지가 404가 아닌 정상 페이지로 열리게끔 하는 방식
          + 그런 아키텍처는 일반 사용자들이 검색엔진에서 프로젝트를 찾아 접근할 수 있도록 해야 하므로 계정 생성 또는 별도 인증 없이 바로 시작하는 케이스와는 안 맞음
          + 사용성 면에서는 아무리 잘 설계해도 불편함 초래할 수밖에 없음, 북마크를 이용하거나 친구에게 링크를 보낼 때도 계속 404만 겪는 불편함 예상
     * 내 작은 서버는 잘 굴러가고 있어서 최근 fail2ban 상태를 안 봤었음
       명령 실행 결과 비교:
sshd-ddos:      0
postfix:       583
dovecot:     9690
postfix-sasl: 4227
nginx-botsearch: 1421
nginx-http-auth: 0
postfix-botnet:  5425
sshd:         202157

   220,000개 넘는 IP가 차단된 상황을 발견해서 조금 충격받음
     * 우리가 추적한 'DotBot/1.2'라는 봇이 최근 2주 동안 22만 건 넘는 요청을 남김, 무작위로 웹서버의 파일명과 폴더명을 요청하는 패턴임, 호기심에 얼마나 파고드는지 한계가 궁금해서 일부러 차단 안 하고 지켜보고 있음
     * AI나 검색엔진을 위한 중앙화 인덱싱 구조도 이제는 푸시 또는 제출 방식으로 바뀌어야 하지 않을까 고민임, 내가 원할 때만 직접 공유(푸시)하는 구조라면 스크래핑 문제가 많이 줄어들 거라 생각함
     * 90년대에 내가 어렸을 때, ISP로부터 전화가 와서 내 컴퓨터가 봇넷에 물려 있어서 인터넷 접속을 중단시킨다는 통지를 받은 적이 있었음, 그런 시절로 다시 돌아가서 해당 행위를 허용하는 ISP의 ASN 전체를 차단하면 이런 문제도 끝낼 수 있지 않을까 생각함
          + 주거용 프록시는 ISP가 직접 제공하는 게 아니라, 전 세계 각지에서 사용자가 의도적으로나 모르고 악성 소프트웨어를 설치하면서 자기 컴퓨터를 프록시로 내어줘서 발생함, 얼마 전 HN에 이와 관련된 좋은 기사가 올라왔었음
          + 내 네트워크에서 특정 장치가 악성코드와 연관된 포트로 아웃바운드 접속을 시도할 때마다 경고를 띄우는 방화벽 규칙을 세팅함, 포트 리스트는 멀웨어 타깃이 계속 변해서 주기적으로 업데이트 필요함, 소소한 방법이지만 이런 것도 또 하나의 방어층임
"
"https://news.hada.io/topic?id=21254","나의 AI 회의론자 친구들은 모두 미쳤음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         나의 AI 회의론자 친구들은 모두 미쳤음

     * AI 기반 프로그래밍 도구인 LLM은 이미 소프트웨어 개발 현장에서 필수적 위치를 차지함
     * 많은 지인들은 AI가 일시적 유행이라 믿지만, 글쓴이는 개발 영역에서는 이미 생각을 바꿀 때임을 강조함
     * 코딩 에이전트는 반복적이고 지루한 작업을 자동화해 개발자를 더 의미 있는 일에 집중하게 해줌
     * AI로 생성된 코드의 품질, 소유권, 도구 지원 등 논란이 있지만, 대부분 기존 개발 환경의 문제를 답습하는 수준임
     * LLM 도입에 대한 소극적 태도는 올바르지 않으며, 앞으로 더욱 중요한 기술 변화가 다가옴을 시사함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: AI와 프로그래밍, 그리고 회의론

     * 최근 테크 기업 경영진들이 LLM 도구의 도입을 강제하는 경향이 있으나, 이는 잘못된 전략임
     * 글쓴이 주변의 똑똑한 개발자 중 일부는 AI가 NFT와 같은 일시적 유행이라 여기며 이를 진지하게 받아들이지 않음
     * 그러나 실제로는 이미 LLM이 도입됨으로써 개발 영역에 큰 변화가 일어난 상황임
     * 본문에서는 소프트웨어 개발 내에서의 LLM 의미에 집중하고, 예술·음악·글쓰기 등 타 분야에 대해서는 언급하지 않음

LLM 에이전트와 현대적 사용 방식

  수준 맞추기: 과거의 LLM vs. 현재의 에이전트

     * 6개월~2년 전 ChatGPT나 Copilot을 단순 활용하던 때와는 다른, 진화된 LLM 에이전트 환경이 확산 중임
     * 오늘날의 개발자들은 에이전트가 코드베이스를 자유롭게 탐색·수정하게 하며, 이를 통해 파일 생성, 컴파일, 테스트, 반복 수행을 자동화함
          + 코드 트리 및 외부 소스의 코드 제공, Unix 도구 기반 정보 추출, Git 상호작용, 각종 개발 툴 실행 등을 지원함
     * 실제 코드 조작 로직 자체는 단순한 시스템 코드임
     * 과거처럼 ChatGPT에서 코드를 복붙하는 것은 본질적 변화를 경험하지 못하는 것과 같음

LLM의 긍정적 효과

     * 대부분의 프로젝트에서 발생하는 단순 반복 코드는 LLM이 쉽게 생성할 수 있음
     * LLM은 검색·문서 확인 없이 정보 챙기기에 능하며, 피로 누적으로 인한 비효율에서 자유로움
     * 원하는 기능 개발 시작이 어려웠던 이유가 새로운 언어·환경의 진입장벽 탓일 때, LLM은 이를 크게 완화해줌
     * 유지보수적 테스트 코드 리팩터링, 의존성 처리 등 개발에서 귀찮은 일은 LLM이 대신 처리할 수 있음
     * 개발자는 중요하고 창의적인 영역에 더 많은 에너지를 쏟을 수 있음

“LLM이 생성한 코드를 이해 못 한다”는 주장에 대한 반론

     * 팀에서 병합하는 코드는 직접 읽고 스타일에 맞게 수정하는 것이 당연함
     * LLM이 만들어내는 코드는 ""알고리듬적으로"" 예측 가능하며, 결과물을 이해하고 검토할 수 있음
     * 반복적인 코드 리뷰 능력이 부족하다면, 인간 개발자가 만든 난잡한 코드 역시 소화하지 못할 가능성이 큼

“AI의 환각(hallucination) 문제”에 대한 시각

     * LLM 에이전트는 코드 린트, 컴파일, 테스트까지 수행해 허위 정보를 바로잡으며 신뢰성을 확보함
     * 환각 문제는 대부분 환경에서 이미 어느 정도 해결됨
     * 효과적으로 활용하려면 세밀한 감시보다는 자동화 프로세스 신뢰가 필요함

“AI 코드는 실력이 낮다”는 비판

     * LLM 서비스 비용은 인턴 급여보다 저렴함 (예: Cursor.ai 월 $20)
     * 시니어 개발자는 능력이 부족한 인턴 혹은 LLM 코드의 생산성을 높여주는 역할을 함
     * 코딩 에이전트 활용 역량, 툴링, 프롬프트 설계도 새로운 기술적 숙련도 영역임
     * “누가 무슨 작업을 분담하느냐”에 대한 혼란이 있으며, 근본적으로 개발자는 방향성과 검증, 판단을 책임짐

“Rust에서 AI 성능이 낮다” 논란

     * 특정 언어나 도구와의 호환성 한계는 특정 생태계의 숙제임
     * Go와 같이 LLM 친화적 언어에서는 AI 활용도가 매우 높음
     * Rust와의 궁합 문제가 LLM 전체의 한계는 아니며, 언어별 맞춤 전략이 필요함

장인정신(Craft)과 실무 프로그래밍

     * 소프트웨어 개발은 실용적 문제 해결이 목적임
     * 불필요하게 코드 품질에 집착하는 활동은 ""야크쉐이빙(yak-shaving)""으로, 현실적 업무에 비효율로 작용함
     * 반복적·귀찮은 작업들은 LLM에 위임하고, 개발자는 가치 창출 구간에 역량 초점을 맞춰야 함

AI 코드의 평범함(“mediocrity”) 수용

     * 대부분 코드가 대단하지 않은 수준이어도 실제로는 문제 없음
     * 중요한 부분은 품질을 높이고, 그렇지 않은 부분은 LLM을 통한 비용 절감을 역으로 이득으로 삼아야 함
     * LLM 코드는 반복적인 부분에서 더 안전하며, 알고리듬 영역에서는 인간보다 폭넓은 접근이 가능함

“AGI(범용인공지능)까지는 멀었다”는 관점에 대한 의견

     * AGI 논란 자체에 무관심하며, 실제 동작 여부만 중요함
     * 현실적 성능과 생산성 증대가 당장의 판단 기준임

일자리 대체 논란

     * 오픈 소스의 도입처럼, LLM도 직업군 변화·소멸을 초래하는 기술임
     * SW 개발자 역시 다른 산업과 마찬가지로 자동화 대상임을 자각해야 함
     * 이러한 변화가 최종적으로 유익할지 해로울지는 불확실하지만, 변화 자체는 불가피함

표절/저작권 이슈

     * AI는 시각 예술계에 커다란 위협이 되고 있음
     * 실제로도 LLM은 산업적 품질을 충족하는 결과물을 대량으로 생성할 수 있음
     * 소프트웨어 개발자들이 표절 문제를 문제 삼기는 어려움
          + 기존에도 개발자는 저작권에 무감각하며, 지적재산권 보호보다는 공유와 재생산을 선호해 옴
          + 코드 일부의 재사용 논쟁은 일종의 특수적 변명에 불과함

최신 LLM 활용과 변화 속도

     * LLM 기반 비동기 에이전트 활용, 병렬적 작업 등으로 생산성이 크게 향상됨
     * 뛰어난 개발자 역시 LLM으로 코드 리뷰와 보완을 진행하며, 정적이지 않은 환경에서 실질적 효용을 경험 중임
     * 중요한 인프라 접근 등 신뢰 문제 영역은 여전히 조심스럽게 다뤄야 함

결론: 기술 변화, 그리고 회의론 극복

     * 기존 AI 회의론자와 달리, 글쓴이는 보수적 관점을 유지하면서도 실제 변화를 체감하고 있음
     * 소프트웨어 개발자들이 낡은 반론에서 벗어나 현실적 변화를 수용해야 할 시점임
     * LLM과 AI 기반 프로그래밍은 스마트폰, 인터넷이 그랬던 것처럼 업계 근본 구조 변화를 이끌 것으로 전망됨

   AI 신봉자든 회의론자든 극단적이면 피하고싶어져요.

   ""특이점이 온다"" 라고 외쳐대는거 볼때마다 참 피곤하죠.

   일회용으로 쓸 간단한 스크립트를 짤때는 확실히 유용함. 시간이 매우 절약됨
   해결은 해야하지만 시간을 많이 투자할 수 없는 경우에도 유용함. 다만 아직은 대체로 유용하지만 사람을 완전히 대체하지는 못함. 추후 얼마나 발전할지 모르겠지만 지금은 어시로서는 그럭저럭 쓸만한 수준.

        Hacker News 의견

     * 6개월 전에 LLM으로 코드 작성을 시도했다가 실패했다면, 진지하게 LLM을 활용하는 대부분의 개발자들이 하는 방식과 다르다는 이야기. 하지만 나는 지금까지 계속 기술이 혁신적이라고 주장하는 목소리에 회의감을 느껴온 입장. 6개월 전에도 최신 LLM을 사용하지 않으면 구식이고, 적절히 사용하지 못한다고 했지만, 결국 예전 LLM은 별로였다는 걸 모두 인정하는 분위기. 계속 변명만 반복되는 'AI 소년이 나타났다' 현상 같다. 이번에도 작업 생산성이 획기적으로 올라간다고 하지만, 어떤 근거로 지금의 주장이 진짜라고 믿을 수 있는지 의문. 앞으로 6개월 후에는 또 지금껏 썼던 LLM 제품들이 별로였다고 할 거라고 예측.
          + 지수 함수 그래프는 모든 시점에서 비슷한 곡선을 그린다. 한동안 컴퓨터는 매년 엄청 발전했는데, 이것이 새로 산 컴퓨터가 쓰레기라서가 아니라 진짜 빠르게 기술이 발전해왔기 때문. 당신이 느끼는 이 현상, 즉 계속 나아지고 있다는 피로감 자체가 정말 혁신적인 진보의 결과라고 보는 관점 제시
          + 0세부터 30살까지 6개월마다 한 인간에게 도움을 요청하면 언제쯤 감탄할까. 질문 대상이나 작업에 따라 놀라움의 시점이 달라질 수 있지만, 시간이 지날수록 점점 더 많은 사람이 그 능력에 감탄하기 마련. LLM의 발전도 마치 아이가 성장하는 걸 보는 듯한 속도. 나 역시 이전에는 LLM을 안 썼지만 o3, Gemini 2.5 Pro 이후 항상 활용 중. 최신 모델을 직접 써보고 아직 놀라지 않았다면, 3년 이내에는 꼭 감탄하게 될 거라고 자신
          + tptacek가 6개월 전에는 이런 주장을 하지 않았음. LLM은 시간이 지나면서 점점 더 발전하고, 가끔씩은 작동하지 않던 것까지도 돌파구를 맞이하게 됨. 최근 6개월간 '에이전트 기반 코드 작성'이 실제로 동작하기 시작한 지점. ""6개월마다 좋아졌다고 하니까 진지하게 안 보겠다""는 마인드는 기술을 제대로 평가하는 능력을 떨어뜨릴 위험
          + 문제의 본질은 '변곡점'에 있다는 의견. 어떤 사람들은 단순히 ChatGPT에 코드 붙여넣고 만족하지 못하지만, 또 어떤 사람들은 코드 컨텍스트를 다 볼 수 있는 에이전트로 훨씬 더 뛰어난 효과 봄. 결국 특정 LLM뿐만 아니라 워크플로우 차이가 중요
     * Thomas의 주장을 좋아하지만, 그 안에도 타인들이 자주 범하는 본질적인 실수가 들어 있다고 생각. 실력 있는 프로그래머는 LLM을 잘 쓰려면 오랜 경험을 쌓아야 하고, Thomas 역시 시간이 쌓이며 전문성을 확보함. 하지만 LLM 지원 없이 성장한 마지막 세대일지도 모름. 이제 막 학교 나온 초보자가 어떻게 '분위기 따라 흉내내기(vibe coding)'에서 벗어나 진짜 실력을 키울 수 있을지 의문. 과거엔 손으로 직접 만들며 성장했지만, 이제는 그냥 전체 설계와 조립을 전적으로 로봇에게 맡기고 실제 도구나 소재가 어떻게 작동하는지 배우지 못할 위기. 지붕의 하중조차 '감'으로 밖에 파악하지 못하게 되는 문제 지적
     * 전문가가 LLM 코드를 읽고, 이해하고, 코드베이스에 통합할 수 있을 때 AI 에이전트의 장점이 확실히 드러난다는 의견. 하지만 모두가 AI로 코딩한다면, 점점 더 복잡한 코드를 읽고, 위험요소를 파악하며, 어디를 어떻게 테스트할지 알고, 코드베이스 전체 구조를 머릿속에 그릴 수 있는 진짜 '에디터'는 어떻게 양성할 수 있냐는 의문. 현재 에디터에게 필요한 이런 능력들은 오랜 기간 코드를 직접 써야 익혀지는 영역. 초보자가 사고를 아웃소싱하면 이런 역량을 키울 기회가 없음. 숙련자는 어디서 나올지 모르겠다는 걱정도 함께. 교수라는 입장에서 숙제나 과제가 이미 LLM으로 아무 생각 없이 통과되는 상황이 되어버려 씁쓸함. 새로운 역량 발전 방법이 필요할 것 같은데 아직 떠오르지 않고, 이런 세상에서 초보자들이 어떻게 전문가로 성장할지 의문
          + 모두가 계산기만 쓰면 수학은 어떻게 배우냐는 반론. 학생들에게 충분히 손으로 연습시키고, 그 본질을 익힌 뒤에야 계산기처럼 LLM을 도입하게 해야 한다는 주장
          + Isaac Asimov의 단편소설 ""Profession""이 떠오른다는 경험 공유. 대부분 사람들이 능력과 직업을 컴퓨터로부터 바로 받게 되고, 덕분에 일은 잘하지만 혁신이나 창의성은 전혀 발전시키지 못함. 오히려 이 기술과 맞지 않는 일부만이 진짜 배워가며, 예술계를 발전시킬 수 있는 유일한 집단이 됨
          + 내 경험 기준 LLM은 페어 프로그래머에 가깝고, 초보자에게는 마치 시니어 엔지니어 같은 역할. 코드만이 아니라 원리나 과정을 잘 설명해 주는 튜터 기능도 탁월. 시니어에게는 코드 리뷰, 아이디어 브레인스토밍, 보일러플레이트 처리 등 다양한 이점을 제공. 전문가 입장에서는 10% 어려운 작업만 직접 집중하고, 나머지 단순 업무는 LLM에 위임할 수 있기에 시간 절약. 초보자가 관심이나 호기심 없이 그냥 코드만 받아쓰면 그건 개발자의 문제이고, 적극적으로 배우려는 사람에게 LLM은 최고 수준의 학습 리소스. 그 점에서 초보자는 지금이 가장 유리한 시대
          + 이 스레드 전체가 ""문제는 존재하지 않는다 – 있네, 근데 별거 아니다 – 오, 진짜 있긴 하네, 적응하자"" 식 고전적 심리 단계를 보여주는 듯. 진짜 핵심 문제 중 하나를 건드렸다는 생각
          + 나 역시 초보자가 LLM에 사고를 완전히 의존하면 실력이 자라기 어렵다는 의견에 공감. 다만, LLM을 통해 늘 새로운 것을 배우는 입장. 내가 막연하게 아는 API에 문제를 내보내고 그 결과를 읽으며 개념을 익히고, 대체로 코드를 뜯어고치고 리팩터링함. 며칠 전에도 시그널의 내부 동작이 궁금해서 LLM이 예제를 내주고, 그걸 같이 분석함. 호기심만 있다면 믿을 수 없는 튜터 역할 가능. 주니어는 무조건 '분위기 따라 코딩'만 할 게 아니라, 적극적으로 배우려는 자세가 중요. 그렇지 않으면 자기 책임이고, 이미 돌이킬 수 없는 현실에서 호기심만 있다면 성장할 길은 충분
     * 실제로 최근 Claude 4 에이전트 같은 걸 써서 큰 C 코드베이스(신기능, 버그픽스), 작은 Rust 프로젝트, 작은 프론트엔드, 기본 API 문서화된 신규 프론트엔드 등 다양한 경우 시도. 모든 케이스에서 완전히 실패 경험. diff도 잘못 받고, 툴에 잘못된 인수 전달, 기본 기능도 실패, 수백줄씩 엉뚱하게 리팩터링, 미완성 refactor가 모두 남아 코드베이스를 엉망으로 만듦. Svelte, solidJS처럼 데이터가 많은 JS 프레임워크에서도 결과는 별로. 진짜 사람들이 극찬하는 에이전트의 실제 강점이 뭔지 모르겠고, 오히려 마케팅 과장 같은 분위기
          + 프롬프트 작성 방식을 묻는 의견. 보통 하나의 기능을 더 세밀한 작은 작업 단위로 쪼개서 LLM에 요청하면 훨씬 잘 작동. 개별 작업(10~200라인 이내)은 실행 잘하지만, 그 이상은 후속 작업, 실망이 계속됨. 지금 수준은 똑똑하지만 경력 없는 인턴 같은 자율성. 고난도의 전체 설계, 계획은 여전히 사람이 해야 하는 상황
          + 에이전트를 홍보하는 사람들도 사실 스파게티 코드만 내놓고, 본인들이 생산성만 높아졌다며 신경 안쓴다는 가설. 실제 성공사례를 구체적으로 툴과 방법까지 공개하는 사례는 별로 없으며, 이 역시 AI가 대신 문서화해줄 수 있을 텐데 그러지도 않음
          + 많은 에이전트 관련 글이 마치 홍보글 같다는 주장. AI 시장에 돈이 너무 많이 몰려있고, 이전 마케팅 사례들도 생각나 더 신뢰 안감. 직접 여러 에이전트 제품을 다 써봤지만 실질적 개선은 적음. HN은 오히려 AI를 두려워하는 비관론자들이 많으니, 논쟁이 많으면 오히려 문제는 사용자 탓이라는 분위기까지 형성됨. 한 유저는 ""AI를 진짜 1000달러씩 써봐야 제대로 체험 가능""이라 주장, 광고냄새가 난다고 지적
          + 소규모 코드나 단일 파일, 50줄 이하 단위 등으로 LLM이 변경을 제한적으로 시키게 하면 관리가 더 용이
     * 90년대부터 코딩을 배운 사람으로서, 이제 컴퓨터에게 애매하고 모호한 입력을 줘도 유의미한 결과를 받는다는 사실만으로 경이로움 느낌. 인간 수준의 모호함만으로 명확하고 쓸만한 출력을 받는 세상이 SF처럼 느껴짐
          + 반대로, 아주 명확한 입력을 줘도 그것마저 컴퓨터가 더 쉽게 풀만한 문제로 애매하게 해석할 때가 많음
          + 나는 LLM의 애매함이 바로 문서 대화 용도로 매력적이라 생각. 굳이 검색어를 여러 번 바꿔가며 원하는 정보를 찾지 않아도 되어, 전체적으로 시간 절약이 큼
          + 지금이 실로 놀라운 시대라는 놀라움, 주 1~2회씩 현실감에 감탄하는 경험
          + Star Trek: The Next Generation의 팬이었는데, 엔터프라이즈호의 컴퓨터와 Data의 차이를 상상하며 놀라왔던 기억. 엔터프라이즈 컴퓨터는 오늘날의 AI와 유사하게 지식 정리, 요약, 작업 실행 등이 가능했고, Data는 개인기를 가진 로봇이라는 설정. Data가 유머, 예술 등 인간 감성 영역에서 한계가 있다는 점은 오늘날 AI 아트와 흡사. 극 초반 미묘한 설정과 전개도 떠오르는 추억
          + 나는 기계에게 명확한 지시를 내려 정확히 원하는 걸 얻는다는 점 때문에 이 업계에 들어왔음. Dijkstra가 예전부터 이미 인간 언어의 애매함, 그리고 이를 극복하기 위해 생긴 '형식 언어'의 중요성을 강조. 결국 2025년에는 컴퓨터와 논쟁하며 폼을 교정해야 하는 '프롬프트 엔지니어링/분위기 코딩' 시대가 열렸다는 자조적 조망도 함께. EWD667: The Humble Programmer 읽어볼 가치 추천
     * 생성형 AI의 무한한 능력을 주장하는 사람들은 실제 증거를 보여줄 수 있는지 궁금. GAI나 에이전트가 정말 강력하다면, AI만으로 회사를 차리고 짧은 기간에 엄청난 완성도 높은 코드를 생산하는 결과로 입증할 수 있을 것. 하지만 실제론 그런 징후 없음. 지금까지 가장 쓸모있는 용도는 그래도 인간이 의미 있다고 착각할 만한 텍스트, 아트 생성. 스타트업들이 실무에 활용해본 경험으론, 아주 가끔 API 탐색이나 편리한 정보 찾기 정도에만 쓸모 있음. 전체적으로 보면 시간 낭비가 더 컸던 편. 이제는 정말 '좋다'는 주장 대신, AI가 순수하게 만들어낸 결과를 직접 보여줘야 할 시점
          + 서로 관점이 엇갈린다고 봄. 항상 실현 가능한 변경(노력 대비 효과가 있는 일)과 백로그에서 방치되는 일 사이 임계치가 있었는데, AI 도구는 그 임계 비용을 낮춰 더 많은 작업이 시도 가능함. 그래서 ""생산성 5배"" 주장하는 쪽은 실제로 처리 코드량이 늘어난 점을 부각, 반면 회의론자는 비즈니스 본질 가치는 크게 바뀌지 않았다고 보는 점. AI 생산성의 역설도 참고 추천
          + 구체적으로 어떤 증거를 보고 싶은지 질문. 무한한 능력을 원하는 건지, 현실적 유용함을 증명하라는 건지 명확히 해달라는 의견. 실질적으로 누구도 완전한 만능은 아니라고 인정하고, 제대로 한계와 강점을 이해하는 사람이 활용할 때 유용하다는 점 강조. 최근 cloudflare/workers-oauth-provider 같은 커밋 히스토리도 예시로 제시하며, '조금이라도 쓸모 있음'에는 동의할 수 있지 않겠냐는 질문
          + 다들 이미 실제로 LLM이 만든 코드를 자체적으로 쓰는 중. LLM 활용 PR이 실제 프로덕션에 수개월째 들어가는 경험 공유. 만약 당신이 아직 효용성을 못 찾았다면, 제대로 활용하는 방법을 배우지 않은 것일 수도 있다는 조언도 함께
          + LLM을 별로 효과 없다고 광고하는 이들을 보면, 뭔가 마케팅이 작동하는 현장을 보는 느낌. 예전엔 믿었던 회사들도 최근엔 AI 홍보에 치우쳐 아쉽다는 생각. 진짜 혜택이 있다면 직접 써보라는 분위기
          + ""골드러시에 곡괭이 파는 상인"" 비유. 직접 도구 효능을 증명하기보다, 사람들에게 금광이 있다고만 설득하는 마케팅 구조 꼬집음
     * Github 코드 라이선스 문제를 무시하는 태도에 대한 비판. 어떤 개발자는 ""저작권 따지지 말라""고 하는데, 프로그래머 모두가 저작권 침해를 일삼는 범죄자라는 전제는 잘못된 일반화. 나를 포함한 많은 개발자는 대규모 불법복제와 무관하고, 오히려 copyleft나 오픈소스 정신을 지키려 노력함. SciHub를 공공선으로 보면서도, 개별 개발자가 의도한 copyleft 역시 존중해야 한다는 입장. 저작권은 단순 찬성/반대 식으로 나뉘는 게 아님. 이런 다양성과 맥락을 무시한 채 싸잡아 비난하거나, 라이선스 무시를 정당화하는 논리야말로 게으른 지적 태도
          + 프로그래머가 법, 특히 미국식 커먼로(civil law)에 대해 잘 이해 못하는 경우가 많음. 법의 전통은 오랜 기간 인간이 합리적으로 해석할 것을 전제로 기술되어 있고, AI도구가 책임을 분산하거나 회피하도록 설계되었더라도 결국 법은 책임질 사람을 찾아내 징벌할 것. AI 붐이 지나간 후의 현실은 1) 기업과 국가가 책임 분산 시도, 2) 부작용 사건 발생(자동차 사고, 차별적 알고리즘, 데이터 유출 등), 3) 법원은 책임을 인간에게 돌리고 벌금 혹은 처벌, 4) 다른 기업들이 위험감을 느끼고 AI를 제한하는 방식. 이런 흐름 속에서 AI 도구는 인간 책임의 범주 내에서 살아남게 될 운명
          + 25년 넘게 자유 소프트웨어 개발자로 활동해왔고, 다양한 라이선스를 즐김. 감독, 시각예술가 배우자도 있지만, 나는 AI 관련 콘텐츠엔 손도 대지 않으며 쓰레기로 봄. 접하고 싶지 않은 입장 명확히 표명
     * Konwinski Prize 처럼 오픈소스 LLM이 novel Github issue 90% 이상 해결하면 100만불 지급하는 챌린지가 흥미로움. Konwinski Prize 대회 참고. 아직 최고의 모델도 0.9가 아닌 0.09 점수에 불과, 실전용으로는 한참 멀었다고 봄. 오픈소스가 상용보다 약간 성능이 낮더라도, 여전히 LLM이 독립적으로 코드 작성하는 건 아직 불가능에 가까움. 쓰레기 양산도 많지만, 평가·관리 필요성 때문에라도 쓸모는 있음
          + LLM은 사실상 탭 자동완성에 인지 부담만 늘어난 버전
     * 반복적인 보일러플레이트 코딩 업무를 AI가 대신해도, 이제는 그 지루한 AI 코드 리뷰가 반복되어 더 재미없어지고 시간도 비슷하게 소요, 이해도도 떨어지는 상황
          + AI 개발을 옹호하는 이들은 결국 코딩보다는 코드 리뷰를 더 좋아하는 듯. 개인 취향일 수 있지만, 그 자체로 괴로운 일로 느껴짐
          + 정확히 말하면, 대량 코드 리뷰는 보통 직접 작성하는 것보다 시간이 덜 걸림. 특히 코드가 테스트를 통과할 때 그렇고, 테스트 코드 역시 LLM으로 생성 가능하므로 부담 경감
          + Claude, Gemini 등은 내가 직접 코딩하는 것보다 훨씬 빠르고, 두 번 이상 살펴봐도 직접 쓰는 시간보다 적게 듦
          + 예전에는 '야크 털 밀기'식 반복 작업에 코드를 썼다면, 이제는 성의 없는 '야크' 면도질을 리뷰하는 현실
          + 전체적으로 더 큰 에너지/탄소 배출이 불가피
     * 기계 번역 및 음성 인식 분야 대화. 청각장애에 가까운 입장에서 하루종일 이 기술을 활용 중. 80년대 드라마를 자막 없이 즐길 수 없었는데, Whisper 같은 LLM을 사용해 영상에서 자막을 얻고, 과거엔 상상만 하던 일이 현실이 된 기적을 실감
          + 최신 음성인식과 번역 SOTA는 여전히 단일 임무용 특화모델이 우세하지만, 격차는 빠르게 줄어들고 있고, LLM이 훨씬 더 다양한 작업 가능. (예시: 음성인식 리더보드 참고), LLM은 넓은 가능성을 열어줌
          + 여러 해 동안 다양한 음성인식 시도(Dragon 등)는 다 놀랍지만 실사용엔 불편했음. Whisper와 LLM 조합이 실제 효용을 가져온 첫 시점. 여전히 완벽하진 않지만 직접 손보는 일이 10분의 1로 줄어, 개인 노트용으로는 이제 검증도 안 할 만큼 편함
          + 아직은 정말 고위험(예: 해외국가 근로계약, 경찰 진술 등) 업무에 LLM 번역만 의존하지 않을 듯. 과거에도 음성-텍스트는 있긴 했지만, 기술 발전은 체감되나 일상적이며 저위험 사용에 그칠 뿐, SF 소설처럼 우주간 협상 등에 쓸 수준까진 한참 남았다고 판단
          + 나 역시 최근 기술 발전이 정말 어린 시절 SF 장르에서 봤던 약속을 실현 중이라 느낌. 며칠 전 낯선 도시에서 음식점 메뉴판 사진을 찍어 중국어 손글씨를 추출, 영어로 바로 번역 후 원하는 메뉴 발음까지 배워 주문. 2년 전까지는 불가능하던 일이라 확신
          + 번역이야말로 LLM의 완벽한 활용 분야라 생각. LLM은 사회적 개념, 문화 맥락, 대중문화, 희귀 레퍼런스 등까지 두루 반영하며 여러 언어·문화권에 맞춰 다양한 버전 번역까지 가능. 전통 번역보다 이미 한참 앞서고 있다고 확신
"
"https://news.hada.io/topic?id=21210","악기 내부에서 촬영한 사진들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            악기 내부에서 촬영한 사진들

     * Charles Brooks는 악기 내부를 촬영하는 독특한 사진 작업을 진행함
     * Laowa 프로브 렌즈와 포커스 스태킹 기법을 이용해 섬세하고 복잡한 촬영 과정을 거침
     * 바이올린 등 소구경 악기 내부 촬영을 위해 의료용 스코프와 다양한 어댑터를 조합함
     * 촬영시 열이나 빛 등으로 인한 악기 손상 방지에도 각별한 주의를 기울임
     * 결과물은 건축적, 대형 프린트로 전시되어 예술성과 기록성을 동시에 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Charles Brooks는 유명 오케스트라 첼리스트로 활동하면서, 이후에는 사진 촬영에 몰두하게 되었음. 그는 악기 내부의 복잡하고 독특한 구조를 촬영하는 프로젝트에 도전하면서 'Architecture of Music' 시리즈를 시작함. 이 작업은 예술적 가치뿐만 아니라 악기 구조와 역사의 기록으로서도 큰 의미가 있음

프로젝트 시작 배경 및 아이디어

     * Brooks는 COVID-19 기간에 뉴질랜드에서 음악가와 악기를 촬영하는 작업을 중단했음
     * 연주가 불가능했던 음악가들이 악기를 수리하기 위해 맡기면서, 수리 공방에 희귀한 악기들이 많이 모이게 되었음
     * 기존에 본 악기 내부 사진들은 부품이 해체된 모습이 많았으나 Brooks는 모든 부품이 온전하게 장착된 상태로 내부를 촬영하고자 함

촬영 장비 개발의 어려움과 혁신

     * Laowa의 프로브 렌즈를 통해 첼로 등 대형 악기는 비교적 쉽게 촬영했으나, 작은 악기에는 크기가 너무 컸음
     * 열풍기를 사용해 렌즈 방수 케이스를 제거해 사이즈를 줄이는 시도도 했음
     * 바이올린 내부 촬영을 위해 5mm 구경의 의료용 스코프 및 20~30개의 어댑터와 확대경을 조합함
          + 이러한 조합은 화질 저하, 이미지 원이 작아지는 등 여러 문제를 야기함
          + 다양한 어댑터와 확대경을 겹쳐 쓰면서 최적의 이미지 크기를 구현하고자 많은 시도를 반복함

카메라와 촬영 환경의 최적화

     * 최종적으로 Micro Four Thirds 센서의 Panasonic Lumix G9 II를 사용해 고해상도 모드를 활용함
          + 이미지 원의 크기를 최대한 확보하면서 일부 비네팅 영역을 크롭으로 해결함
     * 높은 조리개 수치(F250 수준) 에서 촬영이 이뤄져 조명이 매우 약함
          + 고출력 플래시를 다수 사용하되, 악기 내구성을 위해 도장 온도(28°C 이하) 를 지속적으로 체크하며 촬영함

이미지 합성 및 결과물의 예술적 효과

     * 포커스 스태킹 및 파노라마 스티칭을 통해 수백~수천 장의 이미지를 합성함
          + 프로브 렌즈로 시계 방향 회전 촬영, 한 번에 약 2mm 영역만 초점을 맞춤
          + 모든 영역이 선명하도록 20~30장 이상 이미지를 결합함
          + 핫픽셀 방지를 위해 다크 프레임도 별도 촬영함
          + 렌즈의 고르지 않은 광투과 문제를 해결하기 위해 종이 등을 활용해 추가 촬영 필요 지점을 파악함
     * 하나의 완성 이미지에 최소 100장, 많게는 1,000장 이상의 이미지가 합쳐짐
     * 의도적으로 흔한 매크로 사진의 미니어처 느낌이 들지 않도록 설계함
          + 초점합성을 통해 배경 흐림, 공간 압축 현상 최소화
          + 초광각 느낌과 조명 효과로 실내 건축물 같은 웅장함을 강조함
     * 완성된 이미지는 대형 프린트로 전시해 관객이 '구조물 내부에 들어온 듯한' 몰입감을 부여함

사진의 의미와 기록성

     * Brooks의 촬영은 단순 예술을 넘어 역사적, 다큐멘터리적 가치를 가지는 작업임
          + 외관은 비슷해 보여도, 내부에는 각 악기 장인의 흔적, 수리 자국, 서명, 오래된 사용 흔적 등이 남아 있음
     * 다양한 세계적 명기와 가치 있는 악기를 앞으로도 촬영할 계획임
     * 더 많은 사진과 악기별 촬영 이야기는 공식 웹사이트에서 확인 가능

   와 사진 정말 멋지네요. 작가의 공식 웹사이트에서 보시면 더 많습니다
   https://www.charlesbrooks.info/

   진짜 멋지네요~

        Hacker News 의견

     * Firefox나 Chrome에서 개발자 도구(F12) > 콘솔에 들어가서 document.querySelectorAll('p').forEach(e => e.style.opacity = 0)를 실행해보기 추천. 이렇게 하면 페이지의 텍스트가 안 보이게 돼서, 각 사진이 어떤 악기를 나타내는지 맞추기 게임을 해볼 수 있음. 새로고침해서 정답을 확인할 수 있음. 나는 8개 중에서 5개를 맞힘
          + 새로고침을 할 필요 없이 이미지를 마우스 오버하면 파일명으로 바로 정답 확인 가능
     * 작가의 웹사이트에 정말 멋진 사진들이 더 많음 charlesbrooks.info
     * 나는 dpreview가 문 닫은 줄 알았는데 아직 운영되고 있어서 기쁨. 누가 dpreview를 인수했는지 궁금
          + dpreview는 Gear Patrol에서 인수된 상황 관련 기사1 관련 기사2 Gear Patrol 소개
          + Gear Patrol에서 인수 더 알아보기
     * 바이올린 내부가 마치 범선의 하층 갑판을 닮은 모양처럼 보임 [참고 이미지](https://en.wikipedia.org/wiki/HMS_Victory/….jpg)
          + 오래된 콘트라베이스와 바이올린이 얼마나 많이 수리됐는지 보는 재미가 큼. 각진 나무 조각들이 균열을 고치고 고정하기 위해 현악기 수리 장인이 덧댄 '클리트' 부품임
     * 이런 아름다운 사진들을 보면, 이 전시 갤러리처럼 디자인된 콘서트홀이 실제로 있다면 꼭 가보고 싶다는 생각
     * 악기 내부를 촬영할 때, 작은 거울을 고정하는 장치를 만들어 테레포토 렌즈를 f홀에 대고 거울에 반사된 내부를 찍는 법이 더 나았을지 상상. 5mm 구멍이면 8게이지 와이어가 들어갈 정도로 충분한데, 끝에 거울을 달아 내부에서 외부로 와이어를 꺼내면 악기가 움직이지 않도록 표면에 고정할 수 있음. 그 다음 카메라와 조명을 조작해 그림자 없는 멋진 사진 연출 가능. 또는 J자 모양 긴 스코프를 이용해 케이블이 반대쪽에서 보이는 두 장을 합성하는 방법도 있음
          + 이론상으로 가능성 있어 보이는데, 혹시 테레포토 렌즈와 현악기를 가지고 있거나 소지한 친구가 있는지 궁금
     * 과거에도 다양한 출처에서 비슷한 사진이 올라온 적 있음 3년 전 스레드 4년 전 스레드
     * 나는 악기 내부를 보면 음악이 들리는 듯한 느낌을 받음
     * FMV 포인트 앤 클릭 어드벤처 게임에 딱 맞는 배경 이미지 느낌
     * 기타 내부 모습에 관심이 있다면 The Ballad of Buster Scruggs 영화의 첫 번째 단막을 추천
"
"https://news.hada.io/topic?id=21261","존재하지 않는 시장을 위한 제품 만들기 — Braze의 PMF 도달기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 존재하지 않는 시장을 위한 제품 만들기 — Braze의 PMF 도달기

     * Braze(구 Appboy)는 시장이 준비되지 않은 시점에 모바일 사용자 참여 플랫폼을 개발, 수년간의 인내와 시장 관찰 끝에 B2B SaaS로서 모바일 전 산업군 확장에 성공
     * 초기 시장 부재로 인해 베타 고객의 대부분이 이탈했으나, 모바일 앱 생태계가 성숙하면서 자연스럽게 이상적인 고객(ICP)이 등장
     * 단일 산업군(게임) 집중 대신, 모든 산업군에 적용 가능한 '수평적' 플랫폼으로 설계, 성장 이후 산업별 매출 편중 없이 고르게 성장
     * 고객 피드백과 시장 흐름을 바탕으로 제품 방향성을 지속적으로 다듬으며, 베타에서 엔터프라이즈까지 점진적으로 확장
     * 빠른 스케일업보다 장기적 확신과 인내가 중요함을 강조, “존재하지 않는 시장을 위한 창업자라면 오랜 기다림에 대한 준비가 필수”
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Braze의 창업 배경과 시장 발견

     * 2011년, Bridgewater 출신 소프트웨어 엔지니어들과 창업가 Mark Ghermezian이 모바일 앱의 '다운로드 외 수익화'와 사용자 관계 관리 가능성을 주제로 합류
     * 당시 ""모바일 마케팅"" 개념조차 생소했고, 앱 생태계는 개발자 위주·취미성 프로젝트가 다수였음
     * “앞으로 모바일 시장이 폭발적으로 성장할 것”이라는 확신으로 Appboy(후일 Braze) 프로젝트 착수

프로토타입 개발과 초기 난관

     * 초기 제품은 Push, Email, Newsfeed, Slide up 메시지 등 '채널 통합 참여'를 중심으로 설계
     * 시장이 준비되지 않아, 1,000명 베타 사용자 중 실질 고객은 거의 남지 않음
     * “고객 피드백이 때론 틀릴 수 있다”는 확신으로, 장기적 관점에서 ‘모바일 앱이 하나의 비즈니스가 되는 미래’ 에 집중
     * 투자도 빠르게 받기보다 시장 성숙까지 엔지니어 중심의 내부 역량 강화에 집중

ICP의 변화와 첫 Product-Market Fit

     * 초기 고객은 대부분 취미성 개발자, 결제 의지도 적었음
     * 수년 후 시장이 성숙하며, 모바일 자체로 수익을 내는 앱·기업이 등장 → 이상적 ICP(마케팅 팀, 데이터 기반 성장팀) 등장
     * Outbound 중심으로 고객 발굴, “월간 활성 사용자(MAU)가 몇 명인가” 등으로 실질 니즈 구분
     * 모바일 타이탄(빠르게 성장하는 앱), 엔터프라이즈의 모바일 진출과 함께 시장 본격 성장

제품 피드백 루프와 시장 적합성 강화

     * 실제 비즈니스 고객이 늘어나며, 불필요한 기능은 제거하고(예: 유저 프로필 에디터, 피드백 모듈) 핵심 기능만 강화
     * 고객 피드백은 ‘무엇을 새로 만들지’보다는, ‘어디에 집중할지’의 방향성 제공

엔터프라이즈 확장과 시장 다각화

     * 앱 중심 B2C뿐 아니라, 은행·리테일 등 대기업의 모바일 진출과 맞물려 엔터프라이즈 고객군 확대
     * 초기 고객이 특정 산업에 치우치지 않게 ‘수평적’ 제품 전략 고수, 성장 이후 특정 산업 매출 비중이 20%대 초반에 불과할 정도로 다각화
     * 게임 특화 경쟁사들은 시장이 커지자 방향 전환에 실패, Braze는 이미 준비된 상태로 성장 가속

창업자와 장기 성장의 태도

     * “존재하지 않는 시장”을 겨냥한 창업자에게는 인내와 확신, 성장까지의 오랜 시간이 필수
     * “대박 스케일업·빠른 M&A” 등 단기 성공 신화에 흔들리지 말 것, Braze의 IPO까지도 10년이 소요
     * 창업자들의 “무조건 성공할 거라는 근거 없는 확신”과 꾸준한 팀워크가 버팀목 역할

핵심 인사이트

     * 시장보다 앞서가는 확신이 있더라도, 실제 시장 성숙까지 수년이 걸릴 수 있음
     * 고객 피드백은 무조건 따르지 말고, 큰 방향과 비전이 우선
     * 수직적 산업 특화보다는, 초기에는 수평적으로 확장 가능한 아키텍처/제품이 장기적 성장을 견인
     * 초기 Outbound 세일즈와 시장 교육, 고객 성공 경험이 이후 대기업/엔터프라이즈 진출의 초석
     * 창업자·팀의 인내, 끈기, ""왜 지금 해야 하는가""에 대한 자신감이 시장을 기다리는 데 핵심
"
"https://news.hada.io/topic?id=21177","일본우편 '디지털 주소' 시스템 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          일본우편 '디지털 주소' 시스템 도입

     * 일본우편이 7자리 문·숫자 조합으로 실제 주소를 연결하는 '디지털 주소' 시스템을 도입함
     * 온라인 쇼핑몰 등에서 이 디지털 주소를 입력하면 사용자의 주소가 자동으로 확인되는 방식임
     * 사용자는 Yu ID 멤버십 서비스에 등록하면 디지털 주소를 발급받을 수 있음
     * 이 주소는 이사 등 주소가 변경되어도 기존 코드가 유지되고, 변경 주소와 연동됨
     * Rakuten 등 주요 이커머스 업체들도 해당 시스템 도입을 검토 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

일본우편의 디지털 주소 시스템 개요

     * 일본우편은 7자리의 영문자와 숫자로 구성된 디지털 주소를 실제 주소와 연결하는 새로운 시스템을 도입함
     * 사용자들은 온라인 쇼핑몰에서 해당 7자리 디지털 코드를 입력해 주소 정보를 쉽게 불러올 수 있음
     * 디지털 주소를 발급받기 위해서는 일본우편의 Yu ID 멤버십 서비스 등록이 필요함
     * 사용자가 이사 등으로 실제 주소를 변경해도 디지털 주소는 변동되지 않으며, 변경 통지를 하면 새로운 실제 주소와 자동 연결됨

도입 효과 및 파급력

     * 일본우편은 타사가 주소 정보 시스템을 자사 서비스에 적용할 수 있도록 개방함
     * Rakuten 등 일본 내 주요 전자상거래 업체 역시 채택을 적극적으로 검토 중임
     * 일본우편은 이 디지털 주소의 광범위한 보급과 정착을 위해 약 10년간 장기적으로 추진할 계획임

사용자 및 산업의 기대효과

     * 사용자는 매번 긴 실제 주소를 직접 입력할 필요 없이, 간단한 코드만으로 자동 주소 입력의 편의성을 누릴 수 있음
     * 자주 이사가 발생하는 경우에도 주소 코드 고정성 덕분에 다양한 온라인 서비스에서 혼선이 줄어듦
     * 이커머스, 배송, 각종 디지털 서비스에서 업무 효율성과 사용자 경험이 크게 향상되는 기대가 있음

        Hacker News 의견

     * 이 시스템 시작점이 마음에 들어 등록한 코드 보유 및 이사할 때마다 업데이트가 가능, 온라인 주문이 편리해지는 장점 체감
       현재는 웹사이트 프론트엔드가 이 코드를 물리적 주소로 해석하는 방식 구현
       배송업체가 ""나중에 내가 배송할 때 주소 조회하겠다""는 선택이 오히려 더 좋을 수 있음
       예를 들어 3개월 후 배송되는 물건을 주문하고 이사하면, 새로운 주소로 배송이 따라오는 형태 가능성
       이 컨셉을 더 확장하면 평상시엔 집 주소를 기본값으로 두고, 평일 낮 2kg 이하 상품은 근무지로, 입원 중이면 다른 가족이나 친구 집으로도 리디렉션 가능성
       빠른 배송 네트워크가 점차 이런 방식으로 진화할 것이라 예측
       몇 년 전 친구들과 아마존 배송이 자율주행차 뒷쪽 락커에서 30분간 기다리고, 못 받으면 근처 수령지에 전달되는 구조를 상상
       동적 라우팅을 통해 직장이나 퇴근 후 바에 있으면 택배가 그쪽으로 “찾아오는” 경험도 가능
       사용자 선택권과 다양한 옵션이 중요
       30년 뒤에는 사람들이 “집에 꼭 있어야만 택배를 받는” 구조가 구식으로 느껴질 것
       코드 확인 타이밍이 나중으로 갈수록 프라이버시 보호 효과 증가
       이커머스 사이트에 실제 거주지가 공유되지 않는다면 정보 유출 위험 차단
          + 프라이버시 측면에서 사이트가 내 실제 주소를 모르게 되는 것이 매우 만족
            스팸 배송을 필터링하는 블랙리스트 관리 가능성
     * 시스템에서 7자리 코드를 입력하면 온라인 상에서 주소가 자동으로 표시되는 방식
       굳이 이 코드를 우편으로 보내고, 우체국이 실제 주소를 조회하는 구조가 더 낫지 않을까
       이렇게 하면 개인정보 유출 위험도 줄일 수 있음
          + 새로운 시스템이 DNS처럼 동작
            물리적 위치는 바뀌어도 상징적인 “이름(코드)”는 유지, 주문 시점에 주소 해석
            이커머스 사이트들이 이미 실제 주소 체계를 지원하므로, 우체국이 7자리 코드를 실주소로 바꿔주는 API만 추가하면 기존 시스템 거의 변경 없음
            7자리 주소를 직접 사용하려면 대대적인 시스템 변경 필요
            주문 직전에 실주소를 한 번 더 보여주면서 최종 검증까지 가능하니 잘못된 주소로 주문하는 일 감소 효과
          + 이 방식이 실제로 우편 배송과 분리되어 처리
            체크아웃 시점에 코드 → 실주소로 조회하니 Japan Post 시스템 의존도 낮음
            어차피 실주소를 알아내서 배송하는데, 그걸로 바로 배송하면 됨
            실제 긴 주소를 쓰면 오타나 잘못된 이름에도 어느 정도 도달 가능
            7자리 코드에서 한 글자만 틀리면 완전히 다른 곳으로 갈 가능성
            숫자 입력 후 매장 측에서 Japan Post에 코드 조회, 실주소를 고객에게 렌더링하여 최종 확인 가능
          + 이렇게 하면 롤아웃이 더 쉬운 추측
            이 방식은 우체국 처리가 변화 없이, 송신자가 수신자 주소 코드를 찾으면 됨
            원래 방식처럼 하면 우체국 시스템 업그레이드 필요
            구현된 방식은 송신자가 부담을 지는 구조
          + 배송비나 시간정보 때문에도 주소 조회 필요
          + 예를 들어 ""IiIIil"" 입력하려다 ""IiIIi1"" 오입력 시, 완전히 엉뚱한 곳으로 배송
            이게 방지 이유일 듯
     * 대학 시절, 전 세계 주소 미보유 국가들 위한 비슷한 개념 구상
       많은 나라에 정식 주소 체계 미구현, 정부 계획조차 없는 상황
       서드파티 DNS 같은 회사 통해 디지털 어드레싱만 제공
       추가로 실제 주소 미공유, 임시 공유, 날짜/종류/무게별 다른 배달 옵션, 전화번호 공유 등 제공 가능성
       돌이켜보면 좋은 아이디어이지만 투자 받기 어려울 듯
          + 완전히 다른 목적이긴 하지만 what3words가 유사한 기능 제공 판단
     * 실제 이 아이디어가 매우 마음에 듦
       이사할 때 주소 변경하면서 생기는 불편함과 누락 걱정이 가장 크고, 평소에도 이 부분이 정말 번거로움
       코드가 주소와 상관없이 유지된다니 개념은 탁월
       그런데, 사용자가 수동으로 주소 업데이트하도록 의존하면 실수나 누락 때 문제가 생길 수 있음
          + “실수하면 문제 생기기 쉽지 않냐?”
            주문 과정에서 실제 주소로 변환되어 처리될 것으로 보는 입장
          + 만약 내가 디지털 주소를 가지고 있다면 이사하자마자 업데이트할 것
            기사에서도 실제 주소 표시 및 확인 기능 언급
            사용자 입장에선 누락 위험 크지 않을 듯
          + 오랫동안 USPS가 PO Box 임대 시스템을 좀 더 유연 대응해줬으면 좋겠다는 바람
            미국 전체를 5~10개 권역으로 나누고, 특수 우편번호를 할당해 단순히 포워딩 주소만 제공
            사용자가 최종 목적지(usps.com) 직접 제어
            사실상 짧은 링크처럼 동작
            누군가 이 주소로 편지 보내면 자동으로 일반 시스템에서 목적지 정보 붙이고, 사용자가 이사하더라도 웹에서 직접 업데이트 가능
     * 네덜란드는 작은 나라지만, 주소 체계가 이미 이렇게 자동화
       1234AB, 56 같은 형식의 우편번호(15가구 미만) + 집번호 만으로 쉽게 배송 가능
          + 영국 시스템도 테스트 삼아 집번호+우편번호만 써서 자신에게 보냈는데 배달 성공
            그럼에도 여전히 완전한 주소를 꼼꼼하게 다 적는 관습이 남아있음
     * 이 시스템은 일종의 URL 단축기 구조
       7자리 영문/숫자 코드가 전체 주소로 확장
       특정 웹사이트 및 일부 오프라인 서류에 바로 사용
       악용 방지 위한 몇 가지 안전장치도 존재
       이사 시 코드에 연결된 주소 변경 가능
       일본의 많은 온라인 주소 입력폼이 ZIP 코드로 미리 주소 일부를 자동 기입, 하지만 대부분 그 외 세부 주소도 입력
       다만 주소 확인/수정 없이 자동확정하는 사용자도 많음
       이번 시스템은 인프라 최소 변경으로 사용자와 우편 시스템 모두 편의성 확보가 목적
          + 만약 이사가 되어도 코드가 동일하게 따라온다면 사람 식별, 혹은 “개인을 위한 합성 ID”처럼 동작
          + 기사에서 명확히 안 느껴지는 부분, 이커머스 기업이 코드를 저장하는지 풀주소를 저장하는지
            코드를 저장해야 사용자가 이사해도 여러 사이트마다 별도로 주소 업데이트 필요 없음
            우체국만 업데이트하면 됨
          + 일본 주소에 건물명 입력이 귀찮고, 필수인지도 애매
            많은 폼에서 요구하지만 사실은 건물명 없이도 “-”로 계속 방번호까지 입력 가능
            자세한 내용 위키피디아: 일본 주소 체계
          + ""There are few safety checks""라고 썼는데, 안전장치가 거의 없다는 뜻인지, 몇 가지 장치가 있다는 뜻인지 궁금
          + DNS처럼 주소 추상화/간접화로 ""주소"" 자체가 이동 가능
     * ""Parsing the Infamous Japanese Postal CSV"" 추천 링크
       원문 보기
       관련 HN 토론
     * Eircode(아일랜드 우편번호 시스템)
       개별 주소별 프라이버시 보장, 7자리 우편번호, 택시기사 등 실생활에서 자주 쓰임
       웹폼에서도 널리 사용
       하지만 공식 우편시스템(An Post)이 우편번호를 메인 경로로 삼지 않는다 함
          + 이 시스템과는 다름
            Eircode는 위치(주소)에 고정
            일본은 7자리 우편번호 체계 이미 있음
            지금 새 시스템은 7자리 알파뉴메릭 주소를 개인에 할당, 이사해도 동일
            오늘 도쿄, 내년 오사카여도 디지털 주소는 그대로
            Eircode 설명
            일본 우편번호 위키
          + 실제로는 시골엔 주소 대신 Eircode만 있으면 배달되는 경우가 많음
            처음 도입 때 배송 오류 많았던 이유도 실제로 사용했기 때문이라 추측
          + “웹폼에서 쓴다”는 게, 브라우저 기본 내장인지, 아니면 아일랜드 기반 웹사이트만 입력필드에 추가하는 건지 궁금
            주소체계는 항상 프로그래머가 많이 실수하는 분야
          + 전반적인 우편번호/지리코드 시스템 관련 CGP Grey 유튜브 동영상 추천 영상 보기
     * 실제 디지털 주소 사이트에서 안내하는 위험 및 대책 번역 요약 공유
       주요 위험은 3자에게 주소 노출 가능성

    1. 디지털 주소를 3자가 알면 주소 역추적도 가능
    2. 임의 입력 시 실제 주소가 노출될 수 있음
       → 이에 대한 대응책:
       a. 즉시 삭제 및 디지털 주소-주소 매핑 해제, 삭제 후 재발급 가능
       b. 단시간 과도 조회 등 비정상 검색 탐지 및 방지 기능
       c. 정보 유출 시 개인정보와 별도 DB로 관리
       개인적으로 1a의 위험성(내 신원과 코드 분리하면 이동시 자동처리 장점 상실) 우려
       명확한 보안대책은 부족해 보임
       공식 사이트

     * 내가 설계한다면:

    1. Japan Post 계정에 내 주소 등록
    2. 특정 주소에 대한 고정 짧은 코드를 생성하지 않음
    3. Japan Post가 API(OAuth 등) 제공, 타사 웹서비스(예: Rakuten, Kuroneko Takkyubin)와 주소 공유
    4. 연결된 서비스는 언제든 API 통해 내 주소 조회
    5. 원하면 언제든 서비스 연결 해제
    6. 동의 없이 제3자가 주소 얻지 못함
    7. 택시 등에서는 NFC로 1회성 주소 공유 기능 구현 혹은 계정 연결식 지속공유
       이런 구조면 Japan Post만 신뢰하면 되고, 사용성-프라이버시 균형 가능

     * 1a가 걱정이지만, 여전히 신원-코드 분리 권한 있는 편이 기존 구조보다는 나음
       보안 리스크가 다른 쪽으로 이동하는 것일 수도 있음
     * 직접 사용해보니, 혹시 노출됐다 싶으면 즉시 삭제하고 10분 후 새로 발급 가능
     * 일본 소도시에서 친척과 지낸 경험
       매일(혹은 하루 2번?) 마을 스피커로 아마추어 방송(어르신이 소식이나 공지 읽음)
       디지털로 개선되길 바랐으나, 실제로는 오히려 조용한 동네에 더 혼란
       핸드폰으로 바로 공지 받을 수 있었다면 좋았을 경험
          + 오키나와에서는 아침마다 학교 등교 음악, 각종 공지방송까지 다양
            원래 조용하다가 정치 트럭, 폐기물 수거차량이 음악 틀고 다니면 전혀 달라지는 일본적 현실
          + 그런 방송은 아이들 귀가 독려, 비상 시스템테스트, 축제 알림, 실종 노인(예: 다카하시 씨 찾기) 방송 등 여러 용도
          + 본질적으로 공습경보 시스템
            도시/마을 읍사무소에서 방송
            재난 사전대비용 목적, 매일 직접 운영하며 테스트
            일상 공지에까지 최고 신뢰성을 요구하는 게 좀 과하다고 느낌
          + 현재 거주지에서는 17시에 음악만 나오고, 본래는 사이렌이었으나 음악으로 변경
            퇴근 알림이면 좋겠지만, 일본적 문화로 인해 별 효용 없음
            폐기물 트럭 방송도 매우 드물어지고, 막대기 장수가 사라진 것은 아쉬움(빨랫대 필요)
            정치 트럭은 소도시엔 거의 없음
          + 올해 이즈반도 자전거 여행 중 6시 아침에 지역 PA 시스템에서 할아버지가 “오늘 저녁 TV에 우리 마을 해변 찍힌다”고 방송, 아침 6시에!
"
"https://news.hada.io/topic?id=21220","Buttplug MCP","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Buttplug MCP

     * Buttplug MCP는 Buttplug.io 생태계 내 성인용 디바이스를 대형 언어 모델(LLM) 기반 프로그램과 연동하는 MCP 서버임
     * 사용자는 Claude Desktop 등 LLM 툴을 통해 디바이스 정보를 조회하거나 진동, 배터리 상태, 신호 세기 등 다양한 기능 제어가 가능함
     * 현재는 기능적 완성도와 안정성이 부족해 전체 기능이 매끄럽지 않고, 실제 디바이스 제어에 어려움이 있음
     * Ollama, mcphost 등 MCP 호스트와 연동하여 툴 기반의 테스트가 가능하나, 일부 기능(자원 탐색 등)은 제한됨
     * 이 프로젝트는 다른 MCP 서비스와 비교해 LLM-기반 스마트 디바이스 제어에 특화되어 있으며 오픈소스로 무료 제공됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * buttplug-mcp는 Model Context Protocol(MCP) 서버로, Buttplug.io 생태계 전용임
     * Claude Desktop과 같은 툴 지원 LLM 프로그램에서 자신의 Buttplug 디바이스를 질의·제어 가능함
          + 예시 명령: ""내 연결된 buttplug 디바이스는?"", ""LELO F1S의 두 번째 모터를 50%로"", ""Lovense Max 2의 배터리 잔량은?"", ""WeWibe 신호 약함 여부?""
     * 위 예시는 목표 방향에 가깝지만, 실제 구현된 현재 경험은 불안정하고 실망스러운 상태임

지원 리소스 및 툴 구조

     * API 리소스
          + /devices: 연결된 Buttplug 디바이스 리스트(JSON)
          + /device/{id}: 디바이스 개별 상세 정보
          + /device/{id}/rssi: 디바이스 신호 세기(RSSI)
          + /device/{id}/battery: 디바이스 별 배터리 잔량
     * 툴
          + device_vibrate: id, motor, strength 파라미터(필수: id, strength), 선택적으로 모터 지정해 진동을 제어

  JSON 스키마 예시(리소스)

{
  ""resources"": [
    {
      ""uri"": ""devices"",
      ""name"": ""Device List"",
      ""description"": ""List of connected Buttplug devices in JSON"",
      ""mimeType"": ""application/json""
    }
  ]
}

  JSON 스키마 예시(툴)

{
  ""tools"": [
    {
      ""description"": ""Vibrates device by `id`, selecting `strength` and optional motor"",
      ""inputSchema"": {
        ""type"": ""object"",
        ""properties"": {
          ""id"": {""description"": ""Device ID to query, sourced from `/devices`"", ""type"": ""number""},
          ""motor"": {""description"": ""Motor number to vibrate, defaults to 0"", ""type"": ""number""},
          ""strength"": {""description"": ""Strength from 0.0 to 1.0, with 0.0 being off and 1.0 being full"", ""type"": ""number""}
        },
        ""required"": [""id"", ""strength""]
      },
      ""name"": ""device_vibrate""
    }
  ]
}

현재 상태

     * 2025년 4월 1일(만우절)에 시작한 실험적 단기 프로젝트임
     * go-buttplug 라이브러리의 연결 관리가 불안정해, 디바이스 리스트 질의는 되나 진동 기능 등 일부 제어가 원활하지 않음
     * 테스트 목적으로 가상 디바이스가 필요하나 Buttplug.io는 물리적 디바이스만 지원함
     * 엔드투엔드 테스트가 충분히 이뤄지지 않은 초기 단계임
     * 향후 go-buttplug 라이브러리의 연결 문제와 MCP 프로토콜의 호스트 상태(툴 중심 구현 등)를 추가로 검토할 예정임

설치 안내

     * 다중 플랫폼용 바이너리 파일이 GitHub Releases에 배포됨
     * Homebrew를 통한 설치도 지원됨:
          + brew tap conacademy/homebrew-tap
          + brew install conacademy/tap/buttplug-mcp

사용법

     * 디바이스 관리는 Intiface Central 허브 앱에서 담당(기본 포트 12345)
     * Claude Desktop에서 MCP 서버로 buttplug-mcp를 사용할 수 있도록 호스트 프로그램에 설정 필요
          + 예시 설정(JSON 파일):
{
  ""mcpServers"": {
    ""buttplug"": {
      ""command"": ""/opt/homebrew/bin/buttplug-mcp"",
      ""args"": [""--ws-port"", ""12345""]
    }
  }
}

     * 위 설정 후 Claude Desktop의 튜토리얼을 참고하여 적용 가능, Claude와 대화하며 디바이스 질의·제어 가능
     * HomeAssistant MCP를 통한 조명 제어 등 추가 연동 활용 가능

Ollama 및 mcphost와의 통합

     * MCP 호스트 중 Ollama 지원이 가능하며, 툴 지원 LLM과 연동 가능
     * mcphost(mcp-go 개발자가 관리)와 함께 buttplug-mcp용 MCP JSON 설정으로 사용 가능
          + 예시:
$ go install github.com/mark3labs/mcphost@latest
$ mcphost -m ollama:llama3.3 --config mcp.json

     * ""Tools""만 지원하고 ""Resources""는 제한되어 디바이스 나열이나 속성 탐색 불가
     * 실제 예시에서는 디바이스 진동 명령에 대한 성공 메시지는 반환하지만, 디바이스 반응이 없을 수 있음

빌드

     * task 기반 빌드 시스템 적용
          + 실행: $ task
     * 유용한 테스트 도구:
          + task stdio-schema | jq (JSON 스키마 확인)
          + npx @modelcontextprotocol/inspector node build/index.js (MCP Inspector Web GUI)

CLI 인자

     * 주요 옵션:
          + -h, --help: 도움말
          + -l, --log-file: 로그파일 목적지 지정
          + -j, --log-json: JSON 로그 형식
          + --sse: SSE 전송 사용
          + --sse-host: SSE 연결용 호스트/포트
          + -v, --verbose: 상세 로그
          + --ws-port: Buttplug Websocket 서버 접속 포트

기여 및 코드 오브 컨덕트

     * Pull Request, 포크 등 자유롭게 환영
     * Code of Conduct 준수 필요

크레딧 및 라이선스

     * go-buttplug, go-mcp 오픈소스 프로젝트 활용
          + Buttplug.io Golang 라이브러리 및 예시, Model Context Protocol Golang 라이브러리 포함
     * 2025 Neomantra BV, Evan Wies(ConAcademy) 저작
     * MIT 라이선스로 공개(자세한 라이선스는 LICENSE.txt 참고)

   어우.. 우리에겐 너무 이르다고 해야할지..

        Hacker News 의견

     * 예전에 Hacker News에서 Buttplug 표준에 대해 여러 번 논의가 있었던 기억이 있어 공유함
          + teledildonics 관련 오픈소스 소프트웨어(https://buttplug.io/) 소개 및 ‘Rust로 구현된 프로젝트’ 논의 등 다양한 주제를 다룸
          + 사용자 경험 및 윤리에 대한 개발 가이드(https://docs.buttplug.io/docs/dev-guide/intro/buttplug-ethics/)도 다루는 글 있음
          + UART 직렬 통신 기반 성인용 장난감용 프로토콜 T-code(https://stpihkal.docs.buttplug.io/protocols/tcode.html)도 소개됨
          + Rust로 작성된 Sextoy Control Project(https://buttplug.io/)도 언급됨
     * Teledildonics라는 단어 자체가 대단하다고 생각함
          + BO.io에 커밋해서 내 이력서에 넣고 싶다는 생각도 하지만, 내 펜테스트 항목이 괴상한 질문거리만 늘릴 것 같은 찜찜함도 느낌
     * Emacs에서 동작하는 섹스토이 컨트롤 및 teledildonics 모드 Deldo와 관련해 참고할만한 정보(https://news.ycombinator.com/item?id=29207607) 공유
     * 이게 그 유명한 vibe coding이냐고 재미있게 물음
          + buttplug.io 메인 페이지에 We were vibe coding before it was cool이라는 문구가 있는 걸 직접 인용함
          + 이런 프로젝트의 UAT(User Acceptance Testing)는 대체 어떻게 생겼을지 궁금함
          + OP가 이런 하드웨어에는 신경을 꽤 쓴 것 같음
          + https://github.com/profullstack/mcp-server도 vibe coding으로 만들어진 사례
     * 이런 장난감을 컨트롤하는 API가 예전부터 공개되어 있다는 사실이 신기함
          + 원래 같으면 각 장치가 폐쇄적인 전용 앱만 제공하는 게 더 자연스럽다고 생각하지만, 현실은 다름
          + 대기업들이 이 시장에 뛰어들지 않으면서 이렇게 된 건 아닐지 궁금함
          + 벤더가 공식적으로 API를 명세하는 게 아니라, 커뮤니티가 역공학으로 만들어낸 경우가 대부분인 사실 설명
               o 제품들은 대개 블루투스로 연결되기 때문에, 보안은 페어링과 근접성 정도로 만족하는 경우가 많음
          + 덴마크에선 집에 있는 전자기기 대부분이 폐쇄적이지 않고, 해킹도 어렵지 않음
               o 마이크로파, 전동칫솔, 라우터, e-bike 같은 기기들도 간단한 해킹 대상임
               o 섹스토이 해커들이 훨씬 많은 이유는 이쪽에 더 많은 열정이 몰리는 현상
               o 예전엔 e-bike 해킹 커뮤니티가 가장 큰 줄 알았으나, 섹스토이 쪽이 더 클 수도 있다고 추정함
               o e-bike 해킹 이유는 대부분 속도 제한 해제나 통계 기능 때문임
               o 나는 디바이스 해커가 아니지만, 검색만으로 필요한 툴을 쉽게 찾을 수 있었음
               o 실제로 일상 기기들에 보안이 견고하지 않은 경우가 많음
               o 유럽에선 주요 테크 기업 제품 대신 비브랜드 제품을 사길 원하는 문화도 존재
               o 큰 브랜드 제품도 해킹이 쉬운 편이고, 하드웨어만 있으면 대부분 해킹 가능하다고 느낌
          + 캠 스트리머들이 자신만의 장치 확장 기능을 만들기 쉽게 해야 해서 커뮤니티 접근성이 높아졌다는 의견
               o Lovesense가 스트리밍 사이트에서 “채팅으로 컨트롤되는 장난감”의 대명사로 자리잡은 사례 언급
          + 이 업계에선 사용자 만족을 최우선으로 두기 때문에 이렇게 오픈된 걸로 보임
          + 중국산 오픈 기기들이 많고, 저렴한 범용 칩과 프로토콜을 그대로 사용하는 경우가 대부분이라고 추정함
               o 신뢰성이나 보안도 소홀하게 다뤄서 역공학이 어렵지 않음
               o 블루투스 연결이 저렴하고 허술하게 설계된 느낌이고, 심지어 사람 신체로 인해 연결이 끊기기도 쉬움
               o 컨트롤러는 단순 신호만 스트리밍하며 추가 연산이나 오류 처리도 거의 없는 듯함
     * Buttplug는 성인 장난감과 같은 친밀한 하드웨어를 제어하는 오픈 소스 표준 및 소프트웨어 프로젝트임
          + 더 자세한 설명은 https://buttplug-spec.docs.buttplug.io/docs/spec에서 볼 수 있음
     * .io TLD가 이 프로젝트엔 정말 찰떡 선택이라고 느낌
          + 이에 대해 웃긴 반응을 보임
     * qDot이고, https://buttplug.io의 창립자 및 프로젝트 리드임
          + 이미 메인 페이지에서 내려갔지만 질문이 있다면 언제든 AMA 환영임
     * 이 시스템에 ChatGPT 예약 작업을 더하면 정말 색다른 알람시계가 될 것 같다고 생각함
          + 홈 어시스턴트 통합도 지원해서 차고문 열림 같은 알림으로도 사용할 수 있을 듯함
     * 누군가 빅테크가 원치 않는 AI 기능을 강제로 넣는다고 하면, 이 사이트를 북마크해두고 보여주고 싶다는 생각
     * 본인(저자)인데, 이 프로젝트는 만우절에 만들어진 ‘쓸데없는’ 개인 프로젝트임
          + 매년 만우절에 재미 삼아 만든 게 GitHub에 올라가게 됨
          + 이 MCP 서버는 실사용에는 별 의미가 없지만, 시작점이 중요하므로 올려둔 상태
          + 2년 전엔 Buttplug를 SSH 챗룸과 연동했었고, 디지털 동의와 SSH 키를 조합해 익명화된 액세스 그래프를 구성한 경험 설명
          + 현재도 Buttplug.io 메인테이너는 아니지만 Discord에서 여러 논의를 했었음
          + Rustacean(러스트 커뮤니티 멤버) 중 신규 메인테이너도 찾고 있다고 전달
          + MCP 서버는 Golang으로 작성했지만, Buttplug 생태계엔 Rust가 더 어울릴 듯함
          + 예전에는 햅틱스(haptics) 연구를 했고, 관련 특허와 VRML 통합 경험 있음
          + NIH 연구비로 만든 성기능 진단 기기 및 직접 제작한 장난감 경험도 소유
          + 지금은 Neurable EEG 헤드셋으로 Biaerolar Beats 연구 진행 중임(https://github.com/ConAcademy/biareolar-beats)
          + vibe coding으로 LELO F1을 울려봤지만, 이 프로젝트는 실제 손수 코딩한 결과물
          + 최근엔 Claude, Gemini와 같이 A2A 프로토콜로 vibe code도 시도했으나 구현 난이도가 높았음
          + MCP 서버를 그 주에 3개(돈·섹스·마약) 만들었고, 오픈 데이터 및 AI 실사용 결과 측면에선 AgentDank(https://github.com/AgentDank/dank-mcp)가 가장 흥미로움
          + 커스텀 데이터와 SQL 엔드포인트를 툴 호출형 LLM에 붙이는 건 엄청난 잠재력이라고 느낌
     * ""그 방식이 비효율적인 성적 상호작용""이라는 말에 ""그건 각자의 자유""라는 반응
     * 다양한 서비스에서의 프라이버시 문제에 대해 어떻게 생각하는지 궁금함
          + 예전엔 그냥 제품만 사고 벤더와는 아무런 관계 없이 쓸 수 있었는데, 요즘은 VR 고글이나 다른 서비스처럼 상황이 많이 바뀌었다는 의견
"
"https://news.hada.io/topic?id=21289","내가 BEAM(Erlang VM) 책을 쓴 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       내가 BEAM(Erlang VM) 책을 쓴 이유

     * Klarna의 핵심 시스템을 유지하며 겪은 실전 경험에서 15밀리초의 BEAM 중단이 대규모 결제 장애와 긴급 대응을 불러옴
     * BEAM에 대한 신뢰할 수 있는 레퍼런스를 만들고자 10년에 걸쳐 책을 집필함
     * 집필 과정에서 출판사 변경, 기술적 문제, 여러 번의 구조 변경 등 반복적인 좌절과 재도전을 경험함
     * 오픈소스화 후 커뮤니티의 피드백과 참여, 스타 증가, 격려가 지속적인 동기 부여 역할을 함
     * 핵심 내용은 BEAM VM의 구조와 운영에 집중되어 있으며, 실무 엔지니어에게 실질적인 도움이 되는 구성임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

BEAM 책 집필 동기

  Post-mortems, 커피, 그리고 10년의 집념

     * Klarna에서 핵심 결제 시스템을 운영하며 BEAM의 단 15밀리초 중단이 수백만 건의 결제 실패와 새벽 긴급 회의, CEO 호출까지 이어지는 상황을 반복적으로 경험함
     * 그런 위기를 신속히 해결할 수 있는 자료의 부재가 항상 아쉬웠으며, 다음 엔지니어가 더 빠르게 문제를 해결하길 바라는 마음으로 The BEAM Book을 집필함

초기 집필 과정

  시작과 좌절

     * 2012년 10월, DocBook 파일 하나와 큰 포부로 프로젝트를 시작함
     * 2주간의 커밋은 대부분 구조 작업과 메타데이터 갱신에 집중, 실제 내용은 매우 적었음
     * 11월에는 AsciiDoc와 커스텀 빌드 스크립트로 전환하고 6개월 내 완성을 기대했으나, 계속된 재작성과 구조 변경으로 진전이 매우 느렸음

  출판사와의 협업

     * 2013년 O’Reilly와 출판 계약 체결 후 Atlassian Atlas로 마이그레이션하였으나, 파일 분실 및 장 관리 문제 지속 발생
     * Git 히스토리는 불만과 구조 수정의 연속으로 점철되어 있음
     * 2015년 3월, 빌드 통과만을 목적으로 챕터 단위로 강제 분리하는 등의 궁여지책을 시도함
     * 2개월 후 계약 해지가 조용히 이루어지며 자괴감과 안도감이 동시에 찾아옴
     * Pragmatic Bookshelf와의 두 번째 시도 역시 느린 진척과 함께 중단됨

  리셋과 커뮤니티 참여

     * 2017년 1월, 새 리포지토리로 massive commit으로 전체 이전(6622개 파일, 1백만 라인)이 이루어졌으나 재작성도 정체됨
     * 2017년 3월, Asciidoctor 기반 비공개 GitHub 리포지토리에서 다시 시작, 필요 부분만 복사
     * 2주 후 공개 전환과 동시에 외부 기여자들의 typo 수정, 다이어그램 추가, 라이선스 협력이 활발히 발생함

지속적인 동기 부여 요소

     * 본질적으로 BEAM의 진짜 동작 원리를 이해하고자 하는 개인적 호기심과 욕구가 가장 큰 원동력이었음
     * 커뮤니티의 피드백과 제안이 집필 의지를 높였으며, GitHub의 스타 수 증가가 지속적 동기 부여 효과를 가짐
     * “Please continue being awesome” 등에서 보듯 격려 이슈가 심리적 지지 역할도 크게 했음
     * Erlang, BEAM 관련 학회 및 컨퍼런스에서 자주 인용되는 일이 늘어나며 책의 필요성이 입증됨
     * Twitter 등에서도 지속적인 언급과 공유가 집필 지속을 자극함

책의 구조 및 주요 대상 독자

     * 직접 대규모 Erlang 시스템을 설계·운영하며 꼭 필요했던 부분 위주로 서술
          + 스케줄러 및 프로세스 관리: 실전 부하에서의 프로세스 스케줄링, 우선순위, 밸런싱 원리
          + 프로세스 메모리: 힙, 스택, 메시지, 바이너리 관리 방식과 운영에 미치는 영향
          + 가비지 컬렉션 및 메모리 모델: 프로세스별/글로벌 GC 작동 방식, 메모리 누수 및 참조 구조
          + 데이터 표현 체계: 정수, 실수, 튜플, 바이너리 등 데이터의 내부 태깅 구조
          + 컴파일러와 VM 내부 구조: 컴파일 이후 VM에서의 실제 명령 실행 흐름
          + 트레이싱과 디버깅: dbg, erlang:trace, 메시지 및 이벤트 추적 등
          + 성능 튜닝: 실코드 프로파일링, 지연 원인 분석, reduction 이해법
          + 시스템 아키텍처: ERTS, BEAM VM, 서브시스템의 통합 동작 원리
     * Erlang/Elixir 실무 운영자에게 매우 실질적 영향이 있으며, 흩어진 자료 대신 신뢰할 수 있는 종합 안내서 역할을 핵심 목표로 함

집필 과정에서의 교훈

     * 끈기가 완벽주의를 이김. 두 번의 출판 취소 경험도 “미완성”보다는 낫다는 결론임
     * 집중과 경계 설정이 중요함. 글쓰기 시간 확보와 알림 차단, 엄격한 시간 관리가 실제 진전의 원천임
     * 공개와 피드백은 질적 향상의 핵심임. 외부의 교정과 격려, 꾸준한 자극이 큰 도움을 줌
     * 스코프 관리가 필수적임. 범위 조정으로 어려운 주제(Dirty Scheduler, JIT 등)는 추후 부록에 넘김
     * 릴리즈 후 반복 개선 전략이 중요함. BEAM은 매년 변화하며, 살아있는 Git 리포로 지속 보완 가능함
     * 진짜 마감 설정이 실질적 동기임. Code Beam Stockholm 행사 전까지 인쇄라는 마감이 필수 내용을 선별하게 만듬

완성의 정의

     * 실제 인쇄본을 손에 쥐는 순간 마침내 ‘완성’이라는 느낌을 가질 수 있었음
     * 산발적 커밋들이 한 권의 실체로 묶여 있어 현재를 기준으로는 끝을 선언함

참여 방법

     * The BEAM Book 1.0은 현재 Amazon에서 종이책으로 구입 가능함
          + Amazon 링크
     * 오타나 개선 사항을 발견하거나 내부 구조가 궁금한 경우, GitHub 리포의 star, fork, issue 등록 및 pull request 활용 가능
          + 기여자는 감사문에 실명 언급
          + GitHub: theBeamBook
     * 실제 리뷰가 알고리듬에 더 크게 반영되므로 서평도 적극 요청함
     * 실전 시스템 중심 BEAM internals 워크숍도 진행 가능하며, 문의는 happi@happihacking.com으로 이메일 요망

        Hacker News 의견

     * git 저장소는 여기에서 확인 가능함
     * BEAM을 제대로 이해하고 싶어서 계속 파헤치게 된다는 저자의 동기가 인상적임. 이런 열정이 멋진 결과물을 만들어낸다고 생각함. 그래서 바로 구매 결정. 내 경험을 말하자면 출판사로부터 여러 번 책 집필 제안을 받았지만, 서로 원하는 방향이 달라서 성사된 적이 없음. 예를 들면, 나는 14살 대상 Java 입문서는 쓰기 싫었고, 출판사는 내가 깊게 파는 주제(예: classloader)에는 관심이 없었음. 개인적 열정과 독자의 니즈가 만나는 교집합을 어떻게 찾는지 남들은 어떻게 하는지 궁금함
          + 책을 세 권 써본 경험으로 볼 때, 자가 출판하거나 출판사가 원하는 책을 쓰거나 둘 중 하나임. 출판사마다 추구하는 책의 색깔이 다르고, 초급자용 실용서에 집중하는 곳이 많아서 대중적이지 않은 주제를 쓰고 싶으면 자가 출판을 준비하는 것이 현실임. 다행히 요즘은 자가 출판이 쉬워져서 온라인으로 판매도 가능함. 즉, 아주 틈새 시장을 겨냥한 주제라면 출판사를 기대하지 말고 스스로 출판과 홍보를 맡아야 한다는 점이 현실임
          + 본인이 흥미로운 이야기를 한다면 결국 독자들은 그걸 이해하기 위한 방법을 찾게 됨. 경력 초기에 Don Box의 “Essential .Net”을 읽었는데, 그도 특정 독자층을 노린 느낌이 아니었음. 그냥 CLR 내부를 깊이 파헤친 책이었고, 처음에 완전히 이해하려면 여러 번 읽어야 했음
          + 출판사에 꼭 의존해야 하는지, 아니면 스스로 독립적으로 책을 써도 되는 건지에 대해 고민함. 출판사의 이름이나 부가적인 이득 때문인지 궁금함
          + 가르친다는 행위가 가장 좋은 학습법이라는 점에 동의함. 고등학교 때 수학 튜터하며 배웠고, 책을 쓰는 경험도 비슷함. 단순한 이해를 넘어서 근본적인 내용을 내 것으로 만드는 최고의 방법임
          + 약간 자랑 같지만, 나도 등반을 위한 근력 트레이닝 책을 이렇게 파고들다 쓰게 됨. 원래는 자가 출판하려 했지만, 결국 출판사를 찾아 약간 더 읽기 쉽게 수정했음. 출판사에 직접 접근해 보는 것도 방법임
     * BEAM과 Erlang/OTP를 다뤄 본 경험이 지난 1년 중 가장 좋았던 개발 경험 중 하나임. Joe Armstrong의 “Programming Erlang: Software for a Concurrent World” 책을 사용했고, 초보자에게 강력 추천하고 싶음. “Designing for Scalability with Erlang/OTP”도 평이 좋지만 아직 시작은 못했음. 하지만 깊이 면에서는 이번 책이 압도적임. BEAM은 정말 고대 문명이 남긴 외계 기술 같음. 좋은 타이밍에 저 책이 나와서 바로 구매. 두 번이나 출간이 취소된 뒤에도 책을 완성해 준 Erik Stenman 박사에게 감탄함
          + BEAM/Erlang/OTP로 어떤 소프트웨어를 개발했는지 궁금함
     * Elixir와 BEAM은 네트워크 기반이나 파이프라인이 많은 시스템 구축에 최애 선택임. 몇 년간 프로덕션에서 매일 사용했고, 최근 프로젝트에서는 선택이 쉽지 않지만 꾸준히 동향을 챙김. 이 책 출간이 고마움. 몇 년 전 프로덕션 Elixir에서 디버깅하면서 꼭 원했던 책임. 기존 자료는 너무 어렵거나 반대로 너무 단순해서 아쉬웠음
     * BEAM(Erlang virtual machine) 정보는 위키백과 링크에서 확인 가능함
          + 책 제목에 이미 잘 설명되어 있음
     * BEAM이 오픈소스 분야에서 가장 과소평가된 기술 중 하나라고 생각함. 예시로 whatsapp이 있음. Elixir와 Erlang이 높은 동시성 프로젝트에 더 인기가 없는 이유가 미스터리임
          + 내 직장은 Erlang 전문 회사임. Erlang의 진가는 수백만 DAU처럼 대규모 트래픽에 있음. Elixir로 수천 DAU 웹사이트를 돌릴 순 있지만, Erlang과 BEAM의 본질은 그 스케일에 있음. 이런 규모가 필요한 회사는 많지 않고, 더 큰 문제는 Erlang 생태계 자체가 마치 별도의 OS처럼 동작해서 환경 설정과 구성요소가 상당히 복잡함. 컨테이너나 k8s 같은 다른 인프라 기술도 필요 없고, 오히려 Erlang의 고유 방식 때문에 익숙하지 않게 느껴짐. 딱 들어맞는 상황에서 Erlang을 경험한다면, 일종의 마법 같다고 생각함. 내 커리어 하이라이트임
          + 결국 마케팅의 영향이라고 생각함. Java, C#, Go는 막강한 기업 후원이 뒷받침하지만 Erlang은 오히려 기업이 발목을 잡거나 기술 문서 외에는 별 신경을 안 씀. Elixir는 처음으로 다른 언어식 마케팅(루비 모델)을 따라갔으나, 진입 시점과 상황이 다름. 개발자들은 BEAM의 우수성을 납득하겠지만, 그 외의 의사결정권자들에게 어필이 잘 안 되는 것 같음
          + 투자 차이가 크다고 생각함. Rust, Go, Python 등은 기업 지원으로 정적 분석, 타입 체크, 개발자 경험 등에서 많은 투자가 이뤄졌지만, Erlang 쪽은 이러한 사랑을 충분히 받지 못했고, 대형 사용자들도 점차 BEAM 바깥에서 직접 솔루션을 만들거나 전향함
          + 우리는 최근 Squarespace 웹사이트를 Phoenix 애플리케이션으로 바꿨는데, 정말 즐거운 경험이었음
          + 동시에 가장 덜 비밀스러운 ‘비밀 소스’임. 최근 BBC도 Elixir로 전환했으니 점점 인기도 올라가는 중이라고 느낌
     * “Erlang Runtime System(ERS)”은 일반적인 Erlang 런타임을, “Erlang RunTime System(ERTS)”는 Ericsson 구현에 국한된 표현이라는 설명이 좋음
          + 저 정의가 바보 같지 않다고 생각함
     * 바로 책을 구매함. 온라인으로 무료로 볼 수 있지만 저자를 조금이나마 지원하고 싶어 구매함
     * Klarna 같은 대규모 시스템을 하지 않아서 체감이 어렵지만 15ms 딜레이가 포스트모템 이슈가 된다는 게 신기함
          + BEAM에서는 마이크로초(μs) 단위 응답이 일반적이어서, 밀리초(ms)로 튀면 바로 경보가 울릴 수 있음
          + BEAM 시스템에서는 이런 상황이 충분히 발생함. 공유 상태 보호를 위해 gen_server를 만들면, 이건 거대한 뮤텍스 같은 개념임. gen_server가 요청을 처리하는데 20us 걸린다고 하면, 15ms 딜레이면 메시지 큐에 750개의 메시지가 쌓임. 여기에 메시지 큐를 비효율적인 패턴으로 쓰면 성능이 급감함. BEAM은 특정 패턴만 메시지 큐 최적화를 해 주지만 나머지 패턴의 경우 큐 길이에 따라 처리 시간이 증가함. 라이브러리 내부에서 안전하지 않은 receive가 쓰이면 예상치 못한 성능 저하 발생. 최근에 BEAM이 메시지 큐 문제를 보완하는 'alias' 기능을 추가했지만 많은 라이브러리가 아직 사용하지 않음. alias는 메시지 손실 방지가 목적이고, 타임아웃 메시지로 큐가 오염되는 걸 막아줌. 보통 장수 프로세스는 큐를 루프로 돌면서 처리함
          + 언급된 사건의 포스트모템 링크 아시는 분 있으면 궁금함. 온라인에서 못 찾았음
     * BEAM과 유사한 VM이 있는지 궁금함. 혹시 BEAM이 워낙 뛰어나서 경쟁 제품이 없는 건지, 아니면 그만큼 만들기 어렵기 때문인지 알고 싶음
          + 현대 Kubernetes 인프라가 제공하는 기능이 BEAM과 비슷하다고 봄. 요즘은 이런 인프라로 대규모 셀프힐링 시스템을 구현하며, 언어 제약도 없음. Erlang/Elixir 말고도 다양한 생태계와 인력, 관심사가 존재함
          + AtomVM이라는 IoT 등 제약이 큰 장치용 별도 구현도 있음. BEAM이나 OTP를 다른 생태계에서 구현하려던 시도는 많았지만, 대부분 VM 레벨은 아님
          + BEAM이 90년대 후반, 2000년대 초에 나온 시기에는 꽤 독특했음. 지금은 구현 방식만 다를 뿐 같은 문제를 다양한 언어와 도구로 잘 풀 수 있음. Erlang 커뮤니티 특유의 “BEAM 방식 만이 정답”이라는 태도도 있지만, 2025년에는 정말 다양한 옵션이 존재함. BEAM 호환 구현도 있지만, 대부분 니치한 영역임. 기존 BEAM 인프라에 합류해야 하는 게 아니라면, 녹색필드 프로젝트에는 요즘 생태계의 현대적인 솔루션이 더 적합하다고 생각함. ergo 같은 소규모 프로젝트도 있음
          + Dis VM이 아마 가장 비슷하고, 그다음이 Smalltalk VM임. 사실 BEAM 자체보다 OTP가 얹혀 BEAM이 진가를 발휘함
          + 실사용에서 가장 비슷한 건 아마 Go일 것임. BEAM은 “Erlang류 언어”에 맞춘 생태계라 Elixir나 Gleam도 핵심 동작이 Erlang과 유사함. Go가 goroutine 등 병행성에서 Erlang식 primitive를 제공하지만, OTP처럼 애플리케이션 구조에 대한 뚜렷한 관점은 없음
"
"https://news.hada.io/topic?id=21264","Ask GN: 신규 입사자를 위한 가이드북이나 문서를 추천해주세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ask GN: 신규 입사자를 위한 가이드북이나 문서를 추천해주세요

   개발팀에 신규 입사자분들이 들어올 예정인데, 어떤 식으로 온보딩 프로세스를 만들어야 할 지 조금 막막한 상태입니다. 혹시 이런 내용을 다루는 책이나, 블로그 글이나, 문서가 있을까요?

   각 기업 별로 뭔가 아티클 하나씩은 만들어뒀을 줄 알았는데, 생각보다 찾기가 힘드네요 🥲
     * [소프트웨어 엔지니어 가이드북[https://product.kyobobook.co.kr/detail/S000214576874]
     * https://github.com/basecamp/handbook
     * The Ultimate Guide to Onboarding Software Engineers

   등등이 있네요.

   https://handbook.gitlab.com/handbook/people-group/general-onboarding/

   https://www.velopers.kr 가면 뭐라도 있지 않을까요?

   이런 곳이 있는 줄 몰랐네요. 저도 구직 중이라 ㅎㅎ감사합니다.
"
"https://news.hada.io/topic?id=21266","LiveStore - 반응형 SQLite 와 내장 동기화 엔진을 사용하는 상태 관리 솔루션 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           LiveStore - 반응형 SQLite 와 내장 동기화 엔진을 사용하는 상태 관리 솔루션

     * 동기화 로직 개발에 대한 부담없이, 고성능 애플리케이션 개발이 가능하도록 설계된 상태 관리 데이터 계층
     * 반응형(reactive) SQLite와 내장 동기화 엔진을 사용하는 것이 특징
     * 로컬 우선(Local-first) 으로 오프라인에서도 높은 성능을 제공하며, 네트워크 복구 시 자동 동기화 지원
          + 모든 상태 관리 연산이 로컬 SQLite 데이터베이스 위에서 빠르게 수행됨
     * 반응형 데이터 스트림: 데이터 변경 시 즉시 UI와 연결된 리스너에 이벤트 발생, 상태 변화에 대한 실시간 반영 가능
     * 다양한 환경(웹, 모바일, 데스크톱 등)에 적용할 수 있음
     * 기존 상태 관리 도구 대비, 네이티브 성능 및 데이터 일관성에서 우수한 결과를 제공

   홈페이지 메인에 있는 Demo 진짜 잘 만들었네요. Demo 클릭 좀 하다보니, 써보고 싶어질 만큼.

        Hacker News 의견

     * 안녕하세요, LiveStore의 코파운더임 (이전에는 Prisma를 만들었음).
       지난 4년간 Overtone이라는 네이티브급 고성능 뮤직 클라이언트를 만들면서 스스로 사용하려고 LiveStore를 개발하게 되었음.
       LiveStore는 SQLite에 반응형 신호 레이어를 추가하고, 이벤트 소싱 기반 싱크(Git과 유사)를 결합함
          + 로컬 퍼스트 환경을 다양하게 평가해봤는데, LiveStore만큼 깔끔하게 해결하는 솔루션은 거의 없었음
            유사하게 성숙된 툴로는 tinyBase도 있지만, 그건 구조가 다름(CRDTs vs 이벤트 소싱)
            궁금한 점이 있는데, 왜 데이터 용량을 1GB로 제한했는지, 더 큰 데이터를 SQLite에 저장하고 디스크에 남길 수 있도록 옵션을 둘 순 없는지 묻고 싶음
            단순히 설정만으로 퍼시스턴스 모드를 바꿀 수 있지 않을까?
            멀티 테넌시도 흥미로운 시나리오가 될 수 있다고 생각함 JIRA처럼 각 조직이 별개 네임스페이스를 요구할 때, 각 사용자도 전체 티켓이 아니라 자신의 팀/부서만 받도록 할 수 있으면 좋겠음
            기본적으로 로컬 데이터베이스가 전체 데이터의 서브셋이 되는 구조임 Bun/Node에서 바로 실행 가능한 싱크 서버(Cloudflare 필요 없음)가 박스에 들어있다면 정말 멋질 듯
            현재 내가 검토 중인 프로젝트 아이디어에도 잘 맞을 것 같음 특히 멀티 테넌시가 반드시 필요하기 때문임
          + 지난달에 LiveStore를 취미 프로젝트에 써볼까 해서 확인해봤는데, 베타 프리뷰라 접근이 어려웠음
            곧 더 깊이 살펴볼 수 있기를 희망함 로컬 퍼스트 관련 논의를 적극적으로 이끄는 모습이 인상적임
            오프라인 동기화 가능한 웹앱을 만들어본 사람이라면 싱크 엔진의 유용함을 바로 깨닫게 됨
          + 오늘 Local-First Conf에서 발표 정말 잘 들었음
            이벤트 소싱을 SQLite로 구현하는 구조에 대한 설명이 명확하고 설득력 있었음
            SQLite, 특히 OPFS Wasm SQLite를 웹에서 적극적으로 홍보해 줘서 고마움
            PowerSync도 SQLite를 강력히 지지하는 입장이라, LiveStore 같은 성공사례가 반갑게 느껴짐
          + LiveStore가 이벤트 소싱 모델을 글로벌하게 확장할 때 모든 클라이언트에 보통 중앙 싱크 백엔드를 두고 동기화하는데, 이게 필수 요건인지 궁금함
            혹시 연합형 노드(federated nodes) 또는 완전한 P2P 모드도 가능한지 묻고 싶음 분산 SNS 사례에 대한 적용도 고려 중임
          + React, WASM과 조합해서 LiveStore가 대부분의 뮤직앱들이 쓰는 Juce 프레임워크를 대체할 수 있을지 궁금함
            나는 비트메이커인데, Juce랑 C++ 조합은 항상 어렵고 두려웠음 음악앱 개발에 진입하려는 사람에게 LiveStore가 좋은 대안이 될 수 있을지 알고 싶음
     * Local-first Conf에서 발표를 봤는데, 요즘 정말 다양한 싱크 엔진이 등장하고 있음
       LiveStore는 이벤트 소싱과 싱크 엔진을 조합하는 흥미로운 영역을 깊이 파고드는 중임
       벌써 이렇게 견고한 시스템이 되었다는 점에 놀랐음
       최근 몇 주간 새로운 프로젝트에 직접 써봤는데, 정말 매끄럽게 동작하고 있음
     * 런칭을 축하함! 이 시스템이 여기에 설명된 ""1. Serialization"" 전략에 들어맞는지 궁금함
       ProseMirror-collab에서 언급된 것처럼, 자주 업데이트하는 저지연 클라이언트가 고지연 클라이언트의 업데이트를 가로막게 되는 문제가 LiveStore에도 나타날지 묻고 싶음
     * LiveStore가 wa-sqlite을 쓰는 것 같음
       오프라인 데이터 영속성 전략을 자세히 듣고 싶음 구체적으로 OPFS(AccessHandlePoolVFS 같은 변종)나 IndexedDB 중 어떤 쪽을 사용하는지, 혹은 둘 다인지 궁금함
       또한 OPFS의 브라우저 간 불안정성과 Safari IndexedDB의 7일 보관 정책에는 어떻게 대응하고 있는지 궁금함
       SQLite에서 공식 WASM 빌드를 제공하는데도 wa-sqlite를 쓴 이유가 있는지 알고 싶음
     * 최근 LocalFirst.fm 팟캐스트에서 LiveStore를 간단히 소개했음 (https://www.localfirst.fm/24 링크 참고)
          + 에피소드 공유 고마움, 곧 LiveStore 단독 에피소드도 준비할 예정임
     * 매우 기대되는 프로젝트로 보임 하지만 과도한 기대감(하이프 트랩) 에 빠지지 않을까 살짝 조심스럽기도 함
       비슷하게 로컬 퍼스트 앱을 커스텀으로 만들면서 멀티 디바이스 지원을 실험 중임
       E2E 암호화를 선택적으로 추가하는 것도 가능할지 궁금함 문서상으로 이벤트 페이로드 단위로 암호화를 추가하면 될 것 같은데, 서버 로그 압축이 어려워지는 점만 빼면 거의 가능할 듯함
          + ""하이프 트랩""에 대한 의견에 동의함
            나는 Overtone 작업하면서 생긴 필요를 바탕으로 LiveStore를 직접 만들고 있음
            LiveStore/Overtone 모두 장기적인 지속성을 목표로 제작 중임 E2E 암호화는 이미 구현 가능한 구조임
            내가 직접 해보진 않았지만, 문제 있으면 언제든 도움을 줄 수 있음
            클라이언트 측에서만 압축(log compaction)을 시도하는 것도 하나의 방법이 될 것 같음 엔지니어링하면서 이 유스케이스도 꼭 염두에 둘 예정임
     * 크로스 플랫폼 지원이라는 주장에 회의적임, 가장 처음에 보인 게 안드로이드 웹 미지원임
          + 좋은 지적임 안드로이드/크롬 팀과 기술적 이슈에 대해 계속 소통 중임 3년 전쯤부터 원인을 파악했지만 여전히 해결되지 않고 있어서, LiveStore가 별도 우회책을 마련해야 할 것으로 보임
            진행 상황은 공식 깃허브(https://github.com/livestorejs/livestore/issues/321)에서 확인 가능함
            웹 API 지원이 플랫폼마다 달라서, 이런 야심찬 시스템을 만들려면 굉장히 많은 노력과 시간이 필요함을 이해해주길 바람
     * 데모 영상에서 흥미로운 점 하나! 1분 7초 부분에서 오디오가 좌측으로 몰림 현상이 있었음 사소하지만 참고하면 좋을 것 같아서 알려드림
     * 개발자 도구를 같이 제공하는 게 인상적임, 상당기간 자체 프로젝트에서 테스트해온 것 같음
       궁금한 점은, 장기적으로 실행되는 앱/페이지의 compaction을 어떻게 처리할 생각인지, 그리고 이벤트 소싱은 멋진 개념이지만 애플리케이션 레이어가 발전할수록 코드 관리(구버전 클라이언트, 스키마 마이그레이션 등)가 어려워질 수 있음
       Overtone은 여러 소스를 지원하는 것 같은데, 오프라인 재생도 제공할지 궁금함 특히 Spotify UI가 불편해서 대체제가 절실함
          + 좋은 질문임! compaction에 대해선 많은 문의가 있었고, 곧 해결책을 공개할 예정임
            핵심 아이디어는 각 이벤트에 더 많은 의미 정보를 부여해 이벤트 간 겹침을 정의할 수 있게 만드는 것임
            예를 들어 todo 앱에서는 동일 작업 ID에 대한 ""todoCompleted"" 이벤트가 해당 작업의 ""todoCompleted""/""todoUncompleted"" 이벤트들을 압축시킬 수 있다는 원리임
            진행 상황은 깃허브(https://github.com/livestorejs/livestore/issues/254)에서 확인 가능함
            이벤트 소싱은 확실히 규율과 설계가 중요함 데이터에는 ""공짜 점심""이 없기 때문에 트레이드오프가 핵심임
            Overtone 등 내 주요 용도에선 이벤트 소싱의 트레이드오프가 더 낫다고 느끼고 있음
            구버전 지원 등은 간단한 방법이 여러 가지 존재함, 결국엔 각 애플리케이션 특성과 트레이드오프에 달려 있음
            Overtone에 흥미 가져줘서 고마움. 나도 Spotify의 불만족 때문에 이 프로젝트를 시작했음 오프라인 재생의 경우 음원 제공처에 따라 달라짐
            예를 들어 Dropbox 등 본인 소장 앨범은 지원할 수 있지만 Spotify 등 스트리밍 서비스는 정책에 따라 다를 수 있음
     * 이 아키텍처가 마음에 듦, 특히 이벤트 소싱이 좋음
       하지만 이벤트 소싱에는 주의가 필요함 원하는 뷰에 따라 머티리얼라이즈(view materialization)가 느릴 수 있고, 데이터가 커질수록 점점 더 느려짐
       이에 대한 해결책은 트리거 등으로 트랜잭션 내에 머티리얼라이즈 갱신을 직접 관리해야 함
       복제 또는 싱크할 때도 고려해야 하고, 충돌 해결에 대해서도 CRDT 개념을 반드시 익혀둘 필요가 있음
       Postgres 언어 기능이 있는 SQLite3 비슷한 데이터베이스가 있으면 local-first와 remote-first를 그냥 설정만 바꿔가며 쓸 수 있어 정말 좋겠음
          + 마지막 의견 관련 pglite.dev(https://pglite.dev)를 참고하면 좋음
            머티리얼라이즈 비용에 대한 본론에서는, compaction은 앞으로 더 개선할 예정이고, LiveStore 전체가 성능 최적화를 목표로 세심하게 설계된 프레임워크임
"
"https://news.hada.io/topic?id=21292","워싱턴주, 수리할 권리 법제화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            워싱턴주, 수리할 권리 법제화

     * 워싱턴주에서 수리할 권리 법안이 통과되어 법으로 확정됨
     * 해당 법은 전자제품, 가전제품, 휠체어 사용자들이 필요한 도구, 부품, 정보를 이용해 직접 수리할 수 있는 권리 보장 내용 포함
     * Google, Microsoft 등 제조사와 환경단체, 소비자 권익 단체들이 적극 지지하며 법안 통과에 중요한 역할을 함
     * 미국 국방부도 군 장비 정비 및 유지보수의 자율성을 위해 유사한 권리를 강조함
     * 수리할 권리가 미국 여러 주에서 논의 중이며, 이번 워싱턴주 사례가 전국적 확산에 기여할 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

워싱턴주 수리할 권리 법제화의 중요성

   워싱턴주에서 두 건의 법안이 통과되어 주민들에게 전자제품, 가전제품, 휠체어 등 개인 소유물에 대한 수리 권한이 법적으로 보장됨. 이는 시민들이 자신이 소유한 물건의 수리, 개조, 변경 방법 및 주체에 대해 스스로 최종 결정권을 가지는 것이 상식적이고 당연하다는 인식에서 출발함

법안 추진 배경과 지지 단체

     * 워싱턴주에서는 수년간 강력한 수리할 권리 법을 위해 꾸준히 노력해옴
     * Washington Public Interest Research Group, 환경단체, 소비자 권익 옹호단체, Google, Microsoft 등 다양한 단체와 기업이 법안 추진을 지지함
     * Disability Rights Washington, Here and Now Project 등 장애인 단체도 휠체어 등 보조기기 포함을 위해 적극적으로 노력하여, 각자의 경험을 바탕으로 법안에 대한 필요성을 강조함

미국 내 다른 움직임 및 국방 관련 동향

     * 최근 미국 국방장관 Pete Hegseth가 군 장비에 대해 수리할 권리 보장을 위한 조항 포함을 지시하는 메모를 내림
          + 군 장비에 대한 유지보수 및 수리에 필요한 도구, 소프트웨어, 기술 데이터 접근성을 살피고, 지적 재산권을 보호함과 동시에 군의 자체 수리 능력 강화 필요를 명시함
     * 과거에도 대통령 Abraham Lincoln이 표준 부품 사용 조건으로 무기를 구입한 사례 언급, 민간 및 국방 조달에서 수리할 수 있는 권리의 전통적 중요성 강조됨

수리 권리의 사회적 의미

     * 개인 및 단체가 직접 자신의 물건을 고칠 권리 제한은 필수적인 수리·유지보수 방해 요인임을 확인함
     * 농부, 가정주부, 병원 의료기사, 군인 등 다양한 사례에서, 자기 소유물의 수리 접근성이 중요한 사회적 이슈로 부각됨

미국 전역에서의 확산 가능성

     * 현재 미국 50개 모든 주에서 이와 유사한 수리 권리 입법이 논의됨
     * 워싱턴주는 이 법안을 통과시킨 미국 내 8번째 주로, 앞으로도 다른 주에서의 확산에 귀감이 될 전망임

참고

     * 기존 게시물의 일부 내용(Army Secretary 이름)이 잘못 명시되어 국방장관 Pete Hegseth로 수정됨

        Hacker News 의견

     * 2026년 1월 1일부터 제조업체가 부품 페어링 기능으로 수리를 막는 행위 금지 예정 정보 공유 기존에 화면을 교체하면 “알 수 없는 부품” 팝업이 떴던 문제 해결, 카메라나 지문 센서 성능이 고의로 떨어지는 일도 종료 기대 SB 5680 법안은 휠체어 사용자에게 더 많은 혜택 포함 전동 휠체어, 수동 휠체어, 모빌리티 스쿠터까지 적용되며, 제조사는 부품과 도구뿐만 아니라 펌웨어와 내장 소프트웨어도 제공 의무화 독립 수리 시 디지털 락 해제 기대 출처
          + 실제로 이제 회사를 상대로 ‘가짜 부품’을 썼는지 알려주는 것이 불법이라는 의미인지 궁금증 제기 또 별도 인증이 필요한 부품(예: TPM)이나 지문 센서 같은 것을 교체할 경우, 인증 불가 상태에서 일부 기능(지문 센서) 비활성화 자체가 금지되는지 보안적 함의에 대한 의문 제기
          + 시스템이 비순정(Non-OEM) 부품이 장착된 사실 자체를 사용자에게 알릴 수 없다는 뜻까지 포함되는지 우려 중고 시장에서 무분별하게 뜯겨진 제품을 구매하는 소비자 입장에서 불편함 예상
     * 소비자 전자제품 관련 법안 정보 공유 법안 요약과 법률 본문 링크 업로드 소규모 제조업체도 예외 아님 o3라는 인물의 의견에 따르면, 실질적으로 제조사의 의무는 이미 보유하고 있는 서비스 매뉴얼, 펌웨어 유틸리티, 여분 부품을 원가 또는 디지털 무료로 공개하는 것에 한정된다는 주장 소개 그러나 실제 법안에는 ""공정한 비용""이라는 문구로 원가 제공보다 폭넓게 해석될 수 있음을 정정
     * 소비재 제품 유지, 보수가 어려워져서 신제품 구입을 계속 피하고 있었는데, 이 법안이 그 문제를 해소해 줄 것이라는 기대
     * 전화기 관련 법안은 매우 반기는 입장 휠체어 쪽은 이미 수리성이 높은데, 이번 조치로 인해 오히려 휠체어 가격 상승과 소비자 선택지 감소 우려
     * 전체 법안 전문 PDF 링크 12페이지 분량이며 이해 쉽다는 평
     * 루이스 로스만 영상( 참고 영상 )에서 소개된 텍사스 법과 내용이 거의 동일하며, ‘부품’ 대신 ‘어셈블리’ 단위(예: 화면+힌지+카메라)로만 높게 파는 허점 공유 워싱턴 법에는 약간의 변형이 있다는 의견
     * Axis IP 카메라는 Axis 브랜드 SD 카드가 아니면 일부 기능 제한 발생 문제 제기 Axis가 직접 SD카드를 제조하지 않으면서도 가격만 두 배, 공급성도 떨어지는 실정 이번 워싱턴 법이 이러한 모범 아닌 비즈니스 모델에도 영향 줄 수 있는지 궁금증
          + 실제 업무상 Axis 카메라를 많이 사용하는데 구체적으로 어떤 기능에 제한이 있는지, SD 카드를 별도로 구입했는지까지는 잘 모름
     * ""디지털 전자제품의 진단, 유지관리 또는 수리"" 범위가 어디까지인지 궁금 IoT 기기의 경우, 폐쇄된 허브/클라우드 서비스 문제로 인해, 타사 연동을 위해 프로토콜 문서 등도 요청 가능한지 관심 이 법안으로 이런 설계 결함까지 해결받을 수 있을지 궁금
     * “고장난 발전기로 고생하는 군인” 이야기에 대한 견해로, 지금도 대부분의 발전기를 유튜브 가이드 등으로 분해 및 수리 가능하다는 현실적인 시각 제시 상업용 단위는 좀 더 복잡할 수 있음
          + 존디어 상업용 디젤 발전기의 경우, Tier 4F 등급에선 후처리 시스템 관련 고장 코드가 래칭되어 꼭 제조사 전용 진단장비가 있어야만 리셋 가능 단순 접속불량이어도 사용자가 직접 해결 불가
          + 상업용은 모르겠지만, 내가 쓰는 휴대용 인버터 발전기는 전자보드 전체가 복잡하게 코팅되어 보드 교체 없이 개별 수리 불가 나머지 ICE 부분은 비교적 단순
          + 최근 집에 48KW 발전기 신규 설치했는데, Generac 신형 모델이라 제어기기 오류로 설치기사 방문만 네 번 소비자 입장에서 애로사항 많음
          + 대형 발전기의 경우 연료분사가 마이크로컨트롤러 및 소프트웨어에 의존 90년대까지는 소프트웨어가 변경 불가 및 버그 거의 없음 요즘은 소프트웨어가 익스플로잇 및 과도한 기능 확대(Feature Creep)로 이어짐
     * 워싱턴 법안이 전동공구는 제외, 텍사스 법안은 포함 차이 설명 텍사스 주에서는 고가의 전동공구 배터리 리셀 작업(리튬 셀 교체) 가능 예상
          + 셀 교체 시 반드시 모든 셀 동시에, 동일 제조사/생산라인 신품 사용 권장 셀 혼합 시 열폭주(화재) 위험성 경고
          + 배터리 수리 자체는 법이 없어도 가능하지 않냐는 시각 소개 AliExpress 등에서 배터리 빌드 키트(셀 제외) 구매 가능 실제 파워툴에 타사 셀 사용 막는 DRM 적용 사례를 모르겠음 브랜드별로 다를 수 있으니 정보 공유 원함
          + 실제 현장에서 ceenr.com 제품 사용 중이며 아주 만족 ceenr.com 링크
     * 이번 워싱턴 법안은 진전이 맞지만, 정치권에서 기업 로비 영향으로 기준이 약화됨 비디오 게임 콘솔 등 분야별 면제 등 세부 변경점 존재 자세한 내용은 관련 기사 참고
     * 현재 연방 정부 및 의회 상황으로 볼 때, 향후 ‘소비자 선택 제한, 일자리 감소, 가격 인상’ 등 명분으로 연방 차원의 로비가 강해질 수 있다는 전망
     * 기업들이 모든 주에서 수리 가능한 제품을 설계해야 하는지, 아니면 일부 주에서는 여전히 수리 제한 가능한지 궁금증
          + 아마 수리법 적용 주의 고객, 수리점에만 부품/소프트웨어/문서 제공할 것이라는 예측
          + 기업들이 손익계산 후, 소규모 업체는 수리 제한 주에 판매 포기, 대기업은 주별로 다른 제품을 낼 수도 있다는 시나리오 언급 일부 주에서 수리권 미보장품이 더 ‘좋은’ 사양이 될 수도 있을 전망
          + 만약 WA(워싱턴) 주에서 제품 구매, 제조사도 매장도 WA인데 실제 거주지는 ID(아이다호)인 경우에는 어떻게 되는지 의문
"
"https://news.hada.io/topic?id=21250","포탈2 (소스엔진) 으로 웹서버 만들기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         포탈2 (소스엔진) 으로 웹서버 만들기

    요약

   (Gemini로 요약 후 일부 수정되었습니)
     * Portal 2 엔진을 웹 서버로 전환
          + Source 엔진의 숨겨진 TCP 콘솔 기능 (-netconport 옵션)을 이용해 Portal 2에서 직접 웹 페이지를 호스팅하는 실험을 진행.
     * 개발자 콘솔을 통한 HTTP 응답 처리
          + 게임에 TCP 서버는 열렸으나 사용자의 요청에 대해 개발자 콘솔은 이를 콘솔 커맨드로 받아들이는 문제가 있음.
          + HTTP 요청을 게임의 콘솔 명령어로 받아들이기 위해 소스엔진의 alias 명령어와 VScript (Valve에서 만든 스크립트 언어)를 사용하여 입력된 문자열에 대한 응답을 출력할 수 있게 변경함.
     * 다중 라우팅 구현
          + 게임 콘솔로 URL 경로를 구분하지 못하므로, HTTP 요청의 메서드(GET, POST 등), 커스텀 HTTP 헤더 등을 이용해 다른 라우팅을 흉내 냄.
          + 이를 이용하여 JavaScript를 사용해 실시간으로 게임으로부터 데이터 업데이트가 가능하도록 구현함.
     * 게임 오브젝트를 HTML DOM과 CSS에 매핑
          + 큐브와 같은 게임 오브젝트를 HTML 태그와 속성에 대응시켜, 실제로 웹사이트를 “쌓아가는” 방식으로 구성함.
          + 마찬가지로 스타일 정보(CSS)도 각 속성을 큐브 하나로 대응시켜 시각적으로 웹 페이지를 구성함.

   종종 만화나 게임 같은데서 서버에 침투하는 과정을 가상공간을 돌아다니는 과정으로 묘사하던게 생각나네요.

   웹서버를 게임엔진으로 만들 생각을 한것도 놀랍지만 개념적으로나마 웹 개발의 기초적인 내용을 잘 설명해둔 부분이 많아서 재밌는 영상입니다. 포탈의 큐브를 쌓아서 DOM/CSS를 구성하는 부분은 정말 좋았다고 생각해요
"
"https://news.hada.io/topic?id=21260","생성형 엔진 최적화(GEO), 검색의 규칙을 어떻게 바꾸는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   생성형 엔진 최적화(GEO), 검색의 규칙을 어떻게 바꾸는가

     * SEO 시대가 막을 내리고, GEO(Generative Engine Optimization) 가 LLM 기반 검색·노출의 새 기준으로 부상함
     * 기존 SEO는 링크/키워드 기반이었지만, GEO는 LLM이 직접 답변에 브랜드나 콘텐츠를 인용/참조하는 빈도와 맥락에 최적화가 핵심임
     * 답변 포맷 변화에 따라, 콘텐츠는 키워드보다 의미 밀도/구조화와 LLM이 쉽게 인용할 수 있는 명확한 정리가 중요해짐
     * GEO 시대에는 브랜드 언급/참조 모니터링, 모델 내 인지도 분석, LLM 행동 데이터 분석을 지원하는 새로운 툴과 플랫폼이 급부상 중임
     * GEO 플랫폼은 브랜드 인지도→AI 내 존재감 관리로 진화하며, 궁극적으로 마케팅/광고 전체 채널 운영의 새로운 중심이 될 가능성이 커짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

SEO에서 GEO로: 검색 패러다임의 전환

     * 20년 넘게 SEO는 온라인 가시성 확보의 기본 전략이었으나, 2025년 현재 검색의 주류가 LLM(대형 언어 모델) 로 이동
     * 애플 Safari의 Perplexity/Claude 내장 등, 구글 독점이 흔들리고 검색 엔진이 AI 네이티브로 재편
     * SEO가 링크, 키워드, 백링크, 페이지 경험 중심이었다면, GEO는 LLM의 답변 안에 등장하는지 여부가 중요

GEO란 무엇인가

     * GEO(Generative Engine Optimization) 는 LLM·생성형 검색엔진에서 답변 생성 시 브랜드/콘텐츠가 얼마나 자주, 어떤 맥락에서 언급되는지를 높이는 전략
     * SEO가 클릭율(CTR)에 집중했다면, GEO는 레퍼런스율(reference rate)—LLM이 인용하는 빈도와 방식에 집중
     * GEO의 지표: LLM이 내 브랜드를 실제로 언급/참조/출처 링크로 사용한 횟수, 문맥, 감성 등

GEO의 실전 전략 및 변화

     * 기존 SEO: 키워드 반복, 깊이 있는 컨텐츠, 백링크, 트래픽 유입, 기술적 최적화 등
     * GEO:
          + LLM이 쉽게 요약/인용/재구성할 수 있는 구조화된 글(요약/불릿/문단 분리) 강조
          + “요약하면”, “주요 포인트”, 불릿포인트 등 구조적 표현이 LLM 인용률에 긍정적
          + 키워드보다는 의미 밀도, 문맥 설명력, 명확한 메시지가 우선
          + GEO툴(Profound, Goodie, Daydream 등): LLM이 브랜드를 어떻게 인식/언급/설명하는지 대규모로 분석, 감성 모니터링, 경쟁사 비교, 실시간 대시보드 제공

LLM 내 브랜드 존재감 관리가 왜 중요한가

     * “누가 어떻게 내 브랜드를 찾는가”보다, “모델이 내 브랜드를 자발적으로 기억하고 언급하는가” 가 더 중요
     * LLM이 내 브랜드를 긍정적으로 학습·기억해야만, AI 답변에서 우선적으로 노출
     * GEO는 브랜드의 퍼블릭 인지도뿐만 아니라, AI 내 존재감/기억을 관리하는 전략으로 부상

GEO 시장과 플랫폼의 등장

     * SEO는 백링크 분석, 트래픽 추적 등 툴 시장이 분산·특화되어 성장했으나, GEO는 LLM 내 브랜드 관리/캠페인/실시간 성과분석까지 통합 가능
     * GEO 플랫폼은 데이터 수집→모델 학습/튜닝→브랜드 존재감 분석→콘텐츠 최적화/캠페인 생성까지 전체 마케팅 오퍼레이션의 중심으로 진화
     * 기존 SEO 시장과 달리 중앙집중, API 중심, 워크플로우 내장 구조로 확장 가능성 높음

GEO가 바꾸는 마케팅 생태계

     * GEO는 단순한 검색 최적화가 아니라 AI와의 관계, 모델 내 존재감, 전방위 마케팅 자동화의 기반
     * AI가 상업/발견의 입구가 되는 환경에서, “AI 모델이 내 브랜드를 기억할 것인가?”가 마케팅의 핵심 질문이 됨
     * AI 중심 검색·발견 전환기, GEO는 “모델의 기억에 들어가는 경쟁”이자 차세대 마케팅 예산이 집중될 새로운 시장

   AEO와 GEO 용어가 혼재 됨. AI 엔진 질문 답변에 대해서는 AEO 가 좀더 맞다고 생각됨.
   생성형 서비스를 사용하는 경우 GEO가 좀더 맞다고 생각됨.
   현재 두 용어를 볼때 기존 검색 노출 광고에서 AI 검색 노출이 중요한 트렌드로 변화가 올 것인지가 중요한 포인트라고 보여짐.

   그러네요. 검색만 해봐도 아티클들이 죄다 aeo vs. geo vs. seo 요런식이네요
"
"https://news.hada.io/topic?id=21258","LumoSQL - SQLite 기능 확장 및 대체용 스토리지 백엔드 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               LumoSQL - SQLite 기능 확장 및 대체용 스토리지 백엔드 프로젝트

     * SQLite에 보안, 프라이버시, 성능, 계측 기능을 추가하는 프로젝트로, 포크가 아닌 동적 소스 결합 방식을 채택해 업스트림과의 변화 추적이 쉬움
          + SQLite를 직접 대체하지 않고, 필요 기능만 개별적으로 SQLite에 적용 가능(점진적 도입/확장에 유리)
     * 플러그형 백엔드 엔진 : 기본 SQLite Btree, LMDB, Berkeley DB 백엔드 사용 가능하며, 향후 새로운 KVS(키-값 스토리지) 엔진도 실험적으로 추가 예정
     * 디스크 암호화(At-rest encryption), 속성 기반 암호화(ABE), 행 단위 암호화/보안(사용자별로 일부 행만 복호화 권한 부여) 등 최신 암호화 기술 도입
          + 기존 SQLite의 한계를 보완, GDPR 등 프라이버시 요구를 충족
     * 데이터 무결성 : 행 단위 체크섬(손상 감지/예방) 기능 제공
     * 강력한 벤치마킹 도구
          + 다양한 SQLite/LMDB/BDB 버전, 데이터 크기(DATASIZE) 옵션별로 조합 벤치마크 수행, 결과를 자동 DB에 저장
          + 각 벤치마크 run은 SHA3 해시로 영구 식별, 데이터 분석·재현성 용이
     * 유연한 빌드 시스템
          + not-forking Perl 툴 기반 빌드·조합·벤치마크 자동화, 결과 DB화
     * 오픈소스 MIT 라이선스로 NLNet Foundation 지원 하에 개발되며, Linux(x86, ARM, RISC-V) 및 BSD 지원
     * 아키텍처적 의의 및 역사
          + 기존의 SQLite 대체 백엔드는 하드포크가 많았으나, LumoSQL은 비포크, 동적 조합, 다중 백엔드를 주요 차별점으로 삼음
          + 2013년 Howard Chu의 LMDB 기반 포팅 실험이 시발점, 이후 SQLite 본연의 성능도 꾸준히 개선됨을 벤치마크로 입증
          + 향후 다양한 KVS 아키텍처 실험, 업스트림 반영 시도, 고급 보안/무결성/프라이버시 기능 실험 플랫폼 역할 확대
"
"https://news.hada.io/topic?id=21241","Mary Meeker의 Trends Report - "AI"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Mary Meeker의 Trends Report - ""AI""

     * 5년만에 나온 메리 미커의 트렌드 리포트. 이번엔 AI가 중심. 총 340페이지
     * AI 사용과 확산 속도가 인터넷보다 훨씬 빠르며, 기계가 인간을 앞지르는 시점이 도래하고 있음
     * 글로벌 인터넷 인프라(55억명 사용), 30년 이상 축적된 디지털 데이터셋, ChatGPT를 필두로 한 대형 언어 모델(LLM)의 등장과 사용성/속도 혁신이 이를 이끌고 있음
     * 신생 AI 기업들은 혁신, 투자, 제품 출시, 자본 조달 등에서 매우 공격적으로 움직이고 있으며, 기존 빅테크 기업들도 AI 중심 투자와 성장을 가속화하고 있음
     * 중국과 미국의 AI 경쟁 등 글로벌 기술 패권 다툼이 치열하게 전개되고 있으며, 이 리포트가 기술·재무·사회·물리·지정학적 변화에 대한 논의에 기여하기를 바람

문서 Outline

    1. 변화가 과거보다 빠르게 일어나고 있는가?
       → 그러함, 실제로 더 빨라지고 있음
    2. AI 사용자 + 사용량 + 자본지출(CapEx) 성장 =
       → 전례 없는 성장세 (Unprecedented)
    3. AI 모델 컴퓨트(Compute) 비용은 높아지고, 추론(Inference) 비용은 하락 =
       → 성능은 수렴(Performance Converging), 개발자 사용(Developer Usage) 증가
    4. AI 사용량 (Usage) + 비용(Cost) + 손실(Loss) 성장 =
       → 전례 없는 수준 (Unprecedented)
    5. AI 수익화(Monetization)의 위협 =
       → 경쟁 심화, 오픈소스 모멘텀(세력 확장), 중국의 부상
    6. 물리 세계와 AI의 융합(Ramps) =
       → 빠르고 데이터 중심적(Fast + Data-Driven)
    7. AI로 촉진된 글로벌 인터넷 사용자 증가 =
       → 그동안 경험하지 못한 성장
    8. AI와 일(Work)의 진화(Evolution) =
       → 현실에서, 빠르게 진행 중(Real + Rapid)

Overview

     * ""세상이 전례 없이 빠른 속도로 변화하고 있다""는 표현조차 과소평가일 정도로, 변화의 속도와 범위가 급격히 확장되고 있음
     * 기술 혁신과 빠른 채택(adoption), 그리고 글로벌 리더십(leadership) 변화가 이 모든 변화의 근간(Underpinnings)을 이룸

     * Google의 창업 미션(1998): '세계의 정보를 체계화하여 모두가 접근하고 쓸 수 있게 한다'
     * Alibaba의 창업 미션(1999): '어디서나 쉽게 비즈니스를 할 수 있도록 한다'
     * Facebook의 창업 미션(2004): '사람들이 더 많이 공유하고, 세상이 더 개방적이고 연결될 수 있게 한다'

     * 오늘날에는 AI(Artificial Intelligence), 가속화된 컴퓨팅 파워(Computing Power), 그리고 경계 없는 자본(Borderless Capital) 이 결합하여 정보 조직, 연결, 접근성을 비약적으로 향상시키며 거대한 변화를 주도함

     * 스포츠에서 선수의 기록이 데이터/입력/훈련으로 끊임없이 개선되듯, 기업들도 방대한 데이터셋을 컴퓨터가 학습하며 점점 더 스마트하고 경쟁적으로 변화함
     * 대형 모델(Large Models) 혁신, 토큰 단가(cost-per-token) 하락, 오픈소스 확산(Open-Source Proliferation), 반도체 성능(Chip Performance) 향상 등이 기술의 경제성, 파워, 접근성을 모두 극적으로 높임

     * OpenAI의 ChatGPT는 사용자, 사용량, 수익화 지표에서 역사상 가장 빠른 ‘오버나이트 성공(overnight success)’ 사례 (설립 후 9년 만에 달성)
     * AI 활용은 소비자, 개발자, 기업, 정부 모두에게서 폭발적으로 증가
     * Internet 1.0 혁명 때는 기술이 미국에서 시작되어 점진적으로 확산됐지만, ChatGPT는 전 세계 동시다발적으로 도입되어 빠르게 성장

     * 기존 플랫폼 대기업(incumbents) 과 새로운 도전자(challengers) 는 에이전틱 인터페이스(agentic interfaces), 엔터프라이즈 코파일럿(enterprise copilots), 실세계 자율 시스템(real-world autonomous systems), 주권 모델(sovereign models) 등 AI 인프라의 새로운 계층을 선점하기 위해 경쟁 중
     * AI, 컴퓨트 인프라, 글로벌 연결성(global connectivity) 의 급진적 발전은 일(Work)의 방식, 자본 배치(Capital Deployment), 리더십의 기준 자체를 기업과 국가 전반에 걸쳐 근본적으로 재편

     * 동시에 각국의 글로벌 리더십 변화가 진행되고 있으며, 주요 강대국들은 서로의 경쟁력과 비교우위를 적극적으로 견제중
     * 세계 각국이 경제, 사회, 영토적 야망(Economic / Societal / Territorial Aspiration)에 따라 다시 가속화되고 있음

     * 이제 두 가지 거대한 힘, 즉 기술(Technological) 과 지정학(Geopolitical) 이 점점 더 깊이 얽혀가고 있음
     * Meta Platforms CTO Andrew Bosworth는 최근 ‘Possible’ 팟캐스트에서 “지금 AI는 마치 우주 경쟁(Space Race) 과도 같고, 특히 중국 등 주요 국가들은 매우 높은 역량을 갖췄으며 비밀이 거의 없고 모두가 꾸준히 발전하고 있다”고 언급

     * AI 리더십(AI Leadership) 이 곧 지정학적 리더십(Geopolitical Leadership) 으로 이어질 수 있음 (그 반대는 성립하지 않음)
     * 이 현상은 큰 불확실성(Uncertainty)을 동반하지만, 전 T. Rowe Price 회장 Brian Rogers의 “통계적으로 세상은 그리 자주 끝나지 않는다”는 말처럼, 낙관적 시각이 중요함

     * 투자자 입장에서 항상 모든 일이 잘못될 수 있다고 가정하지만, 무엇이 제대로 잘 될 수 있는지에 대한 기대가 진정한 희망(Optimism) 의 원천
     * AI가 대신 일을 해주는 모습은 이메일, 웹 검색의 초기 마법과도 같으며, 더 빠르고, 더 싸고, 더 나은(Better / Faster / Cheaper) 효과가 훨씬 더 빠르게 확산
     * 물론 위험(Danger)과 불확실성도 크지만, 장기적으로 강력한 경쟁(Competition), 혁신(Innovation), 저렴하고 쉽게 접근 가능한 컴퓨트(Accessible Compute), 빠르게 확산되는 AI 기술, 신중하고 치밀한 리더십(Thoughtful and Calculated Leadership)이 상호확증억제(Mutually Assured Deterrence) 와 같이 균형을 만들어낼 것이라는 기대가 있음

     * 어떤 이들에게는 AI의 진화가 바닥치기 경쟁(Race to the Bottom) 이 될 수 있지만, 또 다른 이들에게는 정상으로의 경쟁(Race to the Top) 의 시작
     * 자본주의(Capitalism) 와 창조적 파괴(Creative Destruction) 의 투기적이고 역동적인 힘이 거대한 지각변동을 일으키고 있음
     * 특히 미국(USA), 중국(China), 그리고 글로벌 테크 리더들의 치열한 경쟁이 이미 '게임 온(Game On)' 상태임

     * 본 리포트는 다양한 서드파티 데이터, 리서치, 벤치마크를 바탕으로 현재와 같은 역동적 시기(Dynamic Time) 의 트렌드를 입체적으로 보여주고자 함
     * 궁극적으로 이 논의에 기여하고자 하는 것이 본 리포트의 목표

1. 변화가 과거보다 빠르게 일어나고 있는가?

  Technology Compounding = Numbers Behind The Momentum""

   ""기술의 복리 성장 = 폭발적 성장의 모멘텀 뒤에 숨겨진 수치와 데이터들""
     * 컴퓨팅 사이클의 역사와 AI 시대의 도래
          + 1960년대 메인프레임(Mainframe, ~100만대) → 미니컴퓨터(Minicomputer, ~1천만대) → PC (~3억대) → 데스크톱 인터넷(Desktop Internet, ~10억대/사용자) → 모바일 인터넷(Mobile Internet, ~40억대) → AI 시대(AI Era, 수십억~수백억 단위)
          + 축적된 컴퓨팅 인프라(CPU, GPU, 클라우드/빅데이터)가 AI 확산의 기반이 됨
          + AI 디바이스 시대에는 과거 메인프레임 대비 수만~수십만 배 이상의 디바이스 수가 예상됨
     * AI 모델 학습 데이터셋(단어 수)의 성장
          + 1950~2025년 주요 AI 모델의 학습 데이터셋 크기(단어 수)가 연평균 260% 성장
          + 2018년 이후 GPT-2, GPT-3, GNMT 등 대형 모델이 등장하며 데이터 사용량이 기하급수적으로 증가
          + 최근 Aramco Metabrain AI 등 최신 모델들은 수십조 단위의 단어를 사용해 학습
     * AI 모델 학습에 사용된 컴퓨트(연산량, FLOP)의 성장
          + 1950~2025년 주요 AI 모델 학습 연산량이 연평균 360% 성장
          + GPT-4, Grok, AlphaGo, Swift 등 대형 모델의 등장과 함께 FLOP 지표가 급격히 상승
     * 알고리듬 혁신이 가져온 컴퓨트 효율 향상
          + 2014~2023년 AI 모델의 효과적 연산량(Effective Compute) 이 연평균 200% 증가
          + Chinchilla, OPT-175B 등 알고리듬 최적화가 성능 향상과 컴퓨트 절감에 크게 기여
     * AI 슈퍼컴퓨터의 성능 성장
          + 2019~2025년 AI 슈퍼컴퓨터(클러스터) 성능이 연평균 150% 성장
          + Sunway OceanLight, GPT-3/4 클러스터, Frontier, El Capitan, xAI Colossus 등
          + 칩 성능과 클러스터 당 칩 수의 동시 성장
     * 강력한 대규모 AI 모델 수의 폭증
          + 2017~2024년 연 167% 증가: 10^23 FLOP 이상 대규모 AI 모델 출시 수가 급증
          + DeepMind(AlphaGo), xAI, Anthropic, Meta, NVIDIA, Mistral 등 다양한 플레이어가 속속 등장
     * ChatGPT 사용자·구독자·매출 성장
          + 2022.10~2025.4 기준, 주간 활성 사용자(Users, MM), 구독자(Subscriber, MM), 매출(Revenue, $B) 가 모두 기하급수적으로 성장
          + 800만+ 주간 사용자, 2천만+ 구독자, 연 매출 40억 달러에 근접
     * 3650억 연간 검색 달성 속도: ChatGPT vs Google
          + ChatGPT: 2년 만에 연 3650억 검색 달성 (2024년)
          + Google: 같은 수치 도달까지 11년(2009년) 소요
          + ChatGPT가 Google 대비 5.5배 더 빠른 확산 속도를 기록함

     * 1998년, 인터넷 보급이 시작될 무렵 Google은 '세계의 정보를 체계화해 모두가 접근하고 유용하게 만들겠다'는 목표로 출발함
     * 30년 가까운 세월 동안 인류가 경험한 가장 빠른 변화 속에서, 현재는 대부분의 정보가 디지털화, 접근 가능, 활용 가능한 상태에 이름
     * AI 기반의 정보 접근과 이동 방식 변화는 이보다 훨씬 더 빠르게 전개되고 있음

     * AI는 인터넷 인프라 위의 Compounder(복리적으로 성장하는 존재) 이며,
       누구나 쉽게 쓸 수 있고 대중적 관심을 끄는 서비스들이 극도로 빠르게 확산되는 현상을 만들어냄

  지식 전달의 진화 (Knowledge Distribution Evolution)

     * 1440~1992: Static + Physical Delivery
          + 1440년 인쇄기(Printing Press) 발명부터 1992년까지, 지식은 정적이고(Static), 물리적(Physical) 방식으로 배포됨
          + 즉, 종이책, 신문, 잡지 등 인쇄물 중심의 지식 전달 구조가 수백 년간 유지
            – 1993~2021: Active + Digital Delivery
          + 1993년 인터넷(World Wide Web) 공개 이후, 능동적(Active)이고 디지털(Digital) 기반의 지식 전달로 전환
          + 누구나 웹사이트를 만들고, 실시간으로 정보에 접근·유통 가능해짐
          + 인터넷은 ‘지식의 공개와 유통’에 있어 근본적 변혁을 일으킴
            – 2022+: Active + Digital + Generative Delivery
          + 2022년 ChatGPT의 출시와 함께 생성형 AI 기반의 지식 전달 시대로 진입
               o Generative AI: 텍스트, 이미지, 오디오, 코드 등 다양한 콘텐츠를 생성할 수 있는 AI
               o ChatGPT는 출시 5일 만에 100만 사용자 돌파라는 역대급 성장 기록
          + 이제 지식은 단순 저장·검색이 아니라, AI가 창의적으로 생성하고 즉시 전달하는 시대

     “지식은 사실의 축적이지만(wisdom), 지혜는 그 단순화에 있다” – Martin H. Fischer

     * AI = Many Years Before Lift-Off
          + AI 기술은 단기간에 폭발적으로 성장한 것처럼 보이지만, 본격적 대중화 이전에 수십 년의 준비와 발전 과정이 있었음
     * 1950~2025 AI Milestone Timeline (Stanford가 정리)
          + 1950.10: Alan Turing, 튜링 테스트 발표(컴퓨터의 지능 평가 개념 제안)
          + 1956.6: Dartmouth Conference 개최, John McCarthy가 ‘Artificial Intelligence’ 용어 창시
          + 1962.1: IBM의 Arthur Samuel, 체커 게임에서 자가학습 프로그램으로 미국 챔피언 격파
          + 1966.1: Stanford의 Shakey, 최초의 범용 모바일 로봇 배치
          + 1967~1996: “AI 겨울” (AI Winter) – 큰 진전 없이 투자/관심 감소
          + 1997.5: IBM Deep Blue, 체스 세계 챔피언 Kasparov 격파
          + 2002.9: Roomba, 최초 대량생산 로봇 청소기 출시
          + 2005.10: Stanford의 무인차 Stanley, DARPA 그랜드 챌린지 완주
          + 2010.4: Apple, Siri 인수 후 iPhone 4S에 통합
          + 2014.6: Eugene Goostman 챗봇, 튜링 테스트 통과
          + 2018.6: OpenAI, 최초의 대형 언어모델 GPT-1 발표
          + 2020.6: OpenAI, GPT-3 출시 및 Microsoft 단독 라이선스
          + 2022.11: OpenAI, ChatGPT 일반 공개
          + 2023.3: OpenAI, GPT-4(멀티모달) 출시 / Microsoft, Copilot 통합 / Google, Bard 출시 / Anthropic, Claude 출시
          + 2023.11: 미국·EU·중국 등 28개국, Bletchley AI Safety 선언문 서명
          + 2024.3~5: Meta, Llama 3(오픈소스) 공개 / 미국 국토안보부 AI 로드맵 / Google, AI 기반 검색 기능 도입 / OpenAI, GPT-4o(완전 멀티모달) 출시
          + 2024.7: Apple, Apple Intelligence 발표(개발자용)
          + 2024.9: Alibaba, 오픈소스 Qwen 2.5 모델 100종 출시(서구와 동등한 성능)
          + 2024.12: OpenAI, o3(최고 성능 모델) 발표
          + 2025.1: DeepSeek, R1·R1-Zero 오픈소스 추론모델 공개 / Alibaba, Qwen2.5-Max 발표(GPT-4o, Claude 3.5 추론 성능 능가)
          + 2025.2: OpenAI, GPT-4.5 출시 / Anthropic, Claude 3.7 Sonnet 발표 / xAI, Grok 3 출시
          + 2025.4: ChatGPT, 주간 8억 명 사용자 도달

Circa Q2:25 - 오늘날 AI가 할 수 있는 10가지 (ChatGPT에 의하면)

    1. 모든 것을 작성하거나 편집 : 이메일, 에세이, 계약서, 시, 코드 등을 즉각적이고 유창하게 작성·편집
    2. 복잡한 자료 요약 및 설명 : PDF, 법률 문서, 연구, 코드를 쉽게 풀이해 일반 영어로 변환
    3. 거의 모든 주제의 튜터 역할 : 수학, 역사, 언어, 시험 준비 등 단계별로 학습 지원
    4. 생각 파트너가 되어주기 : 아이디어 브레인스토밍, 논리 디버깅, 가설 점검 등 사고 보조
    5. 반복 작업 자동화 : 보고서 생성, 데이터 정리, 슬라이드 요약, 텍스트 재작성 등
    6. 필요한 역할 연기 : 면접 준비, 고객 시뮬레이션, 대화 리허설 등 다양한 역할 수행
    7. 도구 연결 : API, 스프레드시트, 캘린더, 웹 코드 등 다양한 툴과 연동 코드 작성
    8. 심리적 지원 및 동반자 역할 : 하루를 함께 이야기하거나, 생각을 재구성, 혹은 단순히 들어주기
    9. 삶의 목적 찾기 지원 : 가치 명확화, 목표 설정, 실행 계획 수립 등
   10. 삶의 조직화 : 여행 계획, 루틴 설계, 한 주 또는 작업 흐름 구조화 등

Circa 2030? - 앞으로 5년 내 AI가 할 수 있을 것으로 예상되는 10가지 (ChatGPT에 의하면)

    1. 인간 수준의 텍스트, 코드, 논리 생성 : 챗봇, 소프트웨어 엔지니어링, 사업 계획, 법률 분석 등에서 인간과 같은 결과물 생성
    2. 풀타임 영화·게임 창작 : 대본, 캐릭터, 장면, 게임플레이 메커닉, 보이스 액팅 등 전체 콘텐츠 자동 제작
    3. 인간처럼 이해하고 말하기 : 감정 인지형 비서, 실시간 다국어 음성 에이전트 등
    4. 고도화된 개인 비서 역할 : 인생 계획, 기억 회상, 모든 앱·디바이스간 일정·정보 연동 등
    5. 인간형 로봇 운영 : 가사 도우미, 노인 돌봄, 리테일·호스피탈리티 자동화 등
    6. 자율 고객 서비스·영업 운영 : End-to-end 문제 해결, 업셀링, CRM 통합, 24/7 지원 등
    7. 개인의 전체 디지털 라이프 맞춤화 : 적응형 학습, 동적 콘텐츠 추천, 개인 맞춤형 건강관리 등
    8. 자율 비즈니스 구축 및 운영 : AI 기반 스타트업, 재고·가격 최적화, 전면 디지털 운영 등
    9. 과학적 발견의 자율화 : 신약 설계, 신소재 합성, 기후 모델링, 새로운 가설 테스트 등
   10. 파트너처럼 창의적 협업 : 소설 공동 집필, 음악 제작, 패션 디자인, 건축 등 다양한 창작 협업

Circa 2035? - 앞으로 10년 내 AI가 할 수 있을 것으로 예상되는 10가지 (ChatGPT에 의하면)

    1. 과학 연구 수행 : 가설 생성, 시뮬레이션 실행, 실험 설계 및 분석 등
    2. 첨단 기술 설계 : 신소재 발견, 바이오테크 설계, 에너지 시스템 프로토타입 제작 등
    3. 인간 유사 마인드 시뮬레이션 : 기억, 감정, 적응 행동을 가진 디지털 페르소나 생성
    4. 자율 기업 운영 : R&D, 재무, 물류 등을 최소한의 인간 개입으로 관리
    5. 복잡한 물리적 작업 수행 : 도구 조작, 부품 조립, 실제 환경 내 적응 등
    6. 글로벌 시스템 조율 : 물류, 에너지 사용, 위기 대응 등을 대규모로 최적화
    7. 생물학적 시스템 모델링 : 세포, 유전자, 유기체 시뮬레이션 및 치료/연구 활용
    8. 전문가 수준의 의사결정 제공 : 실시간 법률, 의료, 비즈니스 자문 제공
    9. 공공 토론 및 정책 형성 : 포럼 중재, 법안 제안, 이해관계 조정 등
   10. 몰입형 가상 세계 구축 : 텍스트 프롬프트만으로 대화형 3D 환경을 생성

AI 개발 속도는 예상 못했던 수준

     * 머신러닝 모델 개발 주체의 변화 (2003~2024)
          + 2003~2014년까지는 학계(academia) 가 머신러닝 모델 개발을 주도 (Academia Era)
          + 2015년 이후 산업계(industry) 가 데이터, 컴퓨트, 자본 투입량에서 학계를 크게 앞질러 혁신을 주도 (Industry Era)
          + 2024년 기준, 산업계에서 매년 60여 개의 주목할 만한 ML 모델이 개발됨
     * AI 개발자 수의 급증 (NVIDIA 생태계 기준, 2005~2025)
          + NVIDIA 생태계 내 글로벌 개발자 수가 7년 만에 6배 증가 (2025년 600만 명 도달 전망)
          + 2018~2025년 사이 가장 큰 폭으로 성장
     * 구글 AI 생태계 내 개발자 수 (2024~2025)
          + 2024년 5월: 140만 명 → 2025년 5월: 700만 명
          + 1년 만에 5배 성장, Gemini 플랫폼 중심으로 AI 개발자 커뮤니티 폭발적 확대
     * 컴퓨팅 관련 미국 특허의 폭발적 증가 (1960~2024)
          + 2003년 Netscape IPO 이후 8년간 +6,300건, 2004~2022년 18년간 +1,000건 증가
          + ChatGPT 공개(2022) 이후 1년 만에 +6,000건 급증
          + 컴퓨팅/AI 기술 관련 혁신 특허가 대규모로 쏟아짐
     * AI 성능, 2024년 인간 수준 돌파
          + MMLU 벤치마크(일반지식+추론) 에서 2024년 기준 AI 시스템이 인간(89.8%)을 뛰어넘는 92.3% 정확도 달성
     * AI의 인간 판별 능력 (2025 Q1)
          + GPT-4o(페르소나 미포함): 73% 응답이 인간 대답으로 오인됨
          + GPT-4.5(페르소나 포함): 90% 이상 인간 판별 실패 (AI임을 못 알아챔)
          + AI 응답의 인간 유사성/현실감이 비약적으로 향상
     * AI 대화의 사실감 (튜링 테스트 사례)
          + GPT-4.5를 활용한 실제 튜링 테스트 대화 예시
          + 실험 참가자 87%가 인간이라고 오인한 쪽(A)이 실제로는 AI였으며,
            반대로 인간(B)은 ""AI스러운 분위기""로 판단됨
          + 현대 AI의 자연스러운 대화 능력이 인간을 넘어섬
     * AI 이미지 생성 성능 진화
          + Midjourney v1(2022)과 v7(2025) 버전 비교:
            3년 만에 쥬얼리(해바라기 목걸이) 생성 결과가 압도적으로 현실적으로 발전
     * AI 생성 이미지 vs 실제 이미지 (2024)
          + 2024년 기준 AI가 만든 인물 사진(StyleGAN2)이 실제 사진과 거의 구분 불가할 정도로 정교해짐
          + 생성 이미지의 현실감이 비약적으로 높아짐
     * AI 음성 생성/번역의 현실성 (ElevenLabs 사례)
          + ElevenLabs의 AI 음성 생성 도구가
               o 음성 자동 더빙, 실시간 다국어 번역, 원음 보존 등 고도화
               o 글로벌 사이트 트래픽이 2년 만에 월 2천만 회를 돌파, Fortune 500 기업 60%가 도입
          + AI 오디오 생성·번역 역시 폭발적으로 발전 중
     * AI 기반 오디오 번역의 대중화(Spotify, 2025년 5월)
          + Spotify가 ElevenLabs와 협업, 29개 언어로 오디오북 AI 번역을 수용하기 시작
          + ""누구나 자국어로 콘텐츠를 만들고, AI가 실시간으로 번역해 전 세계에 전달하는 시대""를 비전으로 제시 (CEO Daniel Ek)
          + 2025년 1분기 기준, 월간 활성 사용자 6억 7800만 명, 구독자 2억 6800만 명, 연 매출 168억 유로
     * AI 성능 가속: 새로운 응용사례들(2024년 11월, Morgan Stanley)
          + 단백질 접힘(Protein Folding): DeepMind AlphaFold, 거의 모든 단백질 구조 예측
          + 암 진단(Cancer Detection): Microsoft & Paige, 세계 최대 이미지 기반 암 진단 모델 구축
          + 로보틱스(Robotics): Google, LLM을 활용해 인간 지시를 이해·수행하는 로봇 데모
          + 에이전트형 AI(Agentic AI): Amazon, 사용자 지시에 따라 과업을 수행하는 도구 공개
          + 유니버설 번역(Universal Translation): Meta, 다국어 통·번역 멀티모달 AI 모델 공개
          + 디지털 영상 생성(Digital Video Creation): Channel 1 AI, GenAI 기반 맞춤형 뉴스 영상 제작 시연

AI의 이익과 위험 (Benefits & Risks)

     * AI 개발의 이점(benefits)
          + 인간 문명의 모든 성과는 인간 지능의 산물이며, 기계지능(machine intelligence) 의 수준이 높아질수록 인류의 야망(ambition)도 크게 확장
          + AI와 로봇은 반복적 노동에서 인류를 해방시키고, 생산성 증대로 평화와 풍요의 시대를 열 가능성
          + 과학연구 가속화로 질병·기후변화·자원 문제 해결을 앞당길 수 있음
     * AI 개발의 위험(risks)
          + Demis Hassabis(Google DeepMind): ""AI를 먼저 해결해야 그 외의 모든 것도 해결할 수 있다. 그러나, 그 기회를 얻기 전에 AI의 오·남용, 비의도적 리스크가 발생할 수 있다""
          + 이미 드러난 위험과 앞으로 더욱 커질 위험: 치명적 자율 무기(lethal autonomous weapons), 감시(surveillance), 편향적 의사결정(biased decision making), 고용 영향(employment impact), 안전 및 보안(safety-critical applications, cybersecurity) 등

     ""AI 개발의 성공은 인류 문명사에서 가장 큰 사건이 될 수 있지만, 동시에 우리가 그 위험을 피하는 법을 배우지 못하면 마지막 사건이 될 수도 있다"" - 스티븐 호킹

AI User + Usage + CapEx Growth = Unprecedented

     * ChatGPT를 기준으로 17개월 만에 주간 활성 사용자 8억 명(+8배) 돌파
     * AI 글로벌 채택(Global Adoption) 속도도 인터넷 도입 초기와 비교해 전례 없는 확산 (3년 만에 90% 비북미 사용자 도달, 인터넷은 23년 소요)
     * ChatGPT 1억 사용자 도달까지 0.2년(약 2개월), TikTok·Instagram·YouTube 등 주요 인터넷 서비스 대비 월등히 빠른 성장
     * 100만 사용자(고객) 도달까지 Ford Model T: 2,500일, iPhone: 74일, ChatGPT: 5일 — 비용도 $0로 접근성 최고조
     * 미국 내 가구의 50%가 AI 활용 도달까지 3년이 예상, 모바일 인터넷(6년), 데스크탑(12년), PC(20년), 산업혁명(42년) 대비 반으로 단축
     * AI 도입과 확산의 속도는 역사상 어떤 기술보다도 빠르고, 그 영향 범위와 규모 역시 예상을 뛰어넘음

테크 기업의 AI 도입, 최우선 과제

     * 빅테크 및 주요 테크 기업, AI를 핵심 경영 화두로 집중
          + NVIDIA, Google, Microsoft, Meta, Amazon, Baidu, IBM, C3.ai 등
          + 2020~2024년 실적 발표문에서 AI 언급 빈도가 급증, AI 중심 경쟁 본격화
     * Amazon (CEO Andy Jassy)
          + ""생성형 AI는 거의 모든 고객 경험을 혁신할 것""
          + 코딩, 검색, 쇼핑, 금융, 건강, 로봇, 바이오 등 모든 분야에서 AI 도입·효율화
     * Google (CEO Sundar Pichai)
          + ""AI는 우리의 미션(정보 체계화, 보편적 접근성 제공)을 진전시키는 가장 중요한 수단""
          + ""AI 기회는 지금까지와 차원이 다르다""
     * Duolingo (CEO Luis von Ahn)
          + ""생성형 AI가 데이터 생성, 새로운 기능, 전사적 효율화에 기여""
          + 체스 커리큘럼도 AI로만 프로토타입 완성
     * xAI (CEO Elon Musk)
          + ""Grok AI는 진실 탐구(truth-seeking)가 본질, AI 안전의 필수""
          + ""최대한 진실 지향적 AI를 만들어야 한다""
     * Roblox (CEO David Baszucki)
          + ""AI는 개인의 역량을 극대화하는 가속 도구, 앞으로 모든 사람이 자신만의 AI와 함께할 것""
     * NVIDIA (CEO Jensen Huang)
          + ""10년 내 AI는 모든 산업, 모든 국가, 모든 회사의 인프라가 될 것""
          + ""AI 데이터센터는 본질적으로 'AI 팩토리'임, 엄청난 가치를 생산""
     * 글로벌 테크 리더들은 AI 도입과 인프라 확장에 사활을 걸고 있으며, AI가 미래 기업·사회 경쟁력의 핵심임을 한목소리로 강조

전통적 기업의 AI 도입도 우선순위 급상승

     * S&P 500 기업의 AI 관심 급등
          + 2024년 4분기 기준, S&P 500 기업 중 50%가 실적 발표에서 'AI' 언급 (2015년 대비 급격한 증가)
          + 기업 전반에 걸쳐 AI가 전략적 핵심 아젠다로 부상
     * 글로벌 대기업, AI 도입 목표는 '매출 성장'
          + 향후 2년간 생성형 AI(GenAI) 투자 목표의 다수는 생산성, 고객 서비스, 매출, 마케팅 효과 등 '성장과 수익성'에 집중
          + 비용절감(cost reduction)은 상대적으로 낮은 우선순위
     * 글로벌 CMO(최고마케팅책임자) 75%가 AI 도구 실험/도입
          + 대다수 마케팅 조직이 초기 테스트 또는 파일럿을 시행 중이며, 상당수가 이미 AI를 완전 도입
     * 실제 도입 사례
          + Bank of America: Erica Virtual Assistant
               o 4,000만 명 고객, 누적 25억 건 상호작용, 50,000회 이상 성능 업데이트
               o 24/7 디지털 금융 비서로 자리매김
          + JP Morgan: 엔드-투-엔드 AI 현대화
               o AI/ML 도입으로 2023~2025년 수익·효율성 각각 +35~65% 기대
          + Kaiser Permanente: AI 기반 의료 기록(AI Scribe)
               o 수천 명의 의료진이 도입, 문서화 부담 경감, 환자 경험 및 진료 품질 개선
          + Yum! Brands: Byte by Yum!
               o 2025년 기준 25,000개 레스토랑에서 AI 기반 주문·운영 시스템 도입
     * 전통 대기업 역시 AI 도입을 '비용절감'이 아니라 성장·혁신 중심의 전략적 우선순위로 삼고 있음
          + 각 산업별로 구체적인 AI 활용 성공사례가 빠르게 축적되는 중

교육·정부·연구 분야의 AI 도입도 우선순위 급상승

     * 교육(교육기관) 분야 AI 통합 사례
          + Arizona State University: AI 도구 개발 전담 조직 신설('AI Acceleration')
          + Oxford-OpenAI 파트너십: 5년간 연구·AI 리터러시 강화 협력
          + NextGenAI: MIT, Harvard, Caltech 등 15개 연구대학이 참여하는 5천만 달러 규모 컨소시엄 출범
          + ChatGPT Gov: 미 연방정부 기관 전용 ChatGPT 출시(2025년 1월)
          + 미국 국립연구소: 원자력, 사이버보안, 첨단 과학분야 AI 인프라 협력
     * 정부(Sovereign AI) 도입 정책 확대
          + NVIDIA Sovereign AI Partners: 프랑스, 스위스, 스페인, 에콰도르, 일본, 베트남, 싱가포르 등에서 국가별 AI 인프라 구축 본격화
          + ""각국이 AI 인프라에 투자하는 모습이 과거 전기, 인터넷 인프라와 유사"" (NVIDIA CEO Jensen Huang)
     * 연구(R&D)·의료 분야 AI 적용 확대
          + FDA 승인 AI 의료기기: 2023년 기준 연간 223건 승인, 2015년 대비 폭발적 증가(미국 연방정부 FY21~FY25 AI 예산 147억 달러)
          + AI 기반 신약개발: 기존 방식 대비 임상 전(Pre-Clinical) 단계까지 도달하는 시간 30~80% 단축(1.5~12배 가속)
     * 교육·정부·연구·의료 등 비영리/공공 부문에서도 AI 도입·통합이 빠르게 확산
          + 인프라 투자와 규제 완화, 공동연구 등으로 산업 외 영역에서의 AI 혁신 속도 역시 가속화

AI User + Usage + CapEx Growth = Unprecedented

     * 미국 성인 ChatGPT 사용률 현황
          + 미국 전체 성인 중 ChatGPT 사용 경험 비율은 23년 7월 18%에서 25년 1월 37%로 급증
          + 18~29세는 55%, 30~49세는 44%로 젊은 층일수록 활용도가 높음
          + OpenAI CEO Sam Altman은 ""젊은 층은 라이프 어드바이저, 고령층은 검색 대체로 사용""이라고 평가
     * ChatGPT 앱 일일 평균 사용 시간 증가
          + 23년 7월~25년 4월, 미국 ChatGPT 앱 사용자 기준 일일 평균 사용 시간 202% 증가
          + 하루 약 7분에서 20분 가까이로 늘어나, AI 앱에 대한 사용자 몰입도가 급상승
     * ChatGPT 앱 세션 및 세션당 시간 증가
          + 23년 7월~25년 4월, 평균 세션 수 106% 성장, 세션당 시간도 47% 늘어남
          + 사용자가 앱을 더 자주, 더 오래 사용하며, AI 도구가 일상 속에 자리 잡음
     * ChatGPT와 Google Search 사용자 주간 유지율 비교
          + 23년 1월~25년 4월 기준, ChatGPT의 주간 유지율은 80%로 Google Search의 58%를 크게 앞섬
          + AI 서비스에 대한 사용자 충성도가 전통적 검색보다 높게 나타남
     * 미국 직장 내 AI 챗봇 활용 효과
          + AI 챗봇을 사용하는 미국 직장인 중 72% 이상이 ‘더 빠르고, 더 나은’ 업무 성과 경험
          + 업무 효율성과 작업 품질 모두 긍정적인 변화 보고
     * 미국 대학생 ChatGPT 활용 사례
          + 미국 대학생(18~24세) ChatGPT 사용 용도는 논문/프로젝트 시작, 텍스트 요약, 아이디어 브레인스토밍, 문제해결, 시험준비, 연구, 튜터링 등 연구·학습·진로 조언 중심
          + 실질적 과제 해결, 창의적 작업, 진로 설계까지 AI가 적극적으로 활용됨
     * AI 기반 딥 리서치 자동화 서비스
          + Google Gemini, OpenAI ChatGPT, xAI Grok 등 주요 기업의 딥 리서치 기능 확장
          + 웹 자동 조사, 인사이트 도출, 수십 페이지 분량의 리포트 자동 생성, 팩트 탐색 등 고도화된 지식 업무의 자동화 가속화

AI 에이전트의 진화 = 챗 응답에서 실제 업무 자동화로

     * 기존 챗봇은 제한된 대화와 간단한 질문 응답에 머물렀지만, AI 에이전트는 스스로 추론, 실행, 다단계 작업을 처리하는 서비스 제공자로 발전 중임
          + 예시: 미팅 예약, 보고서 제출, 도구 로그인, 여러 플랫폼 간 워크플로 자동화 등
          + 자연어 명령만으로 복잡한 업무를 직접 실행
     * 이런 변화는 2000년대 초 정적 웹사이트에서 Gmail, Google Maps 같은 동적 웹앱으로 전환된 흐름과 유사함
          + 단순 메시징 인터페이스에서 실제 작업을 실행하는 인프라로 진화
     * AI 에이전트는 명확한 입력이나 제한적 결과만 제공하던 초기 어시스턴트와 달리, 목표 중심·** 자율성**·** 보호장치**까지 갖추어 의도 해석, 메모리 관리, 앱 간 협업 등 복잡한 프로세스 실행이 가능해짐
     * 기업들이 가장 빠르게 도입을 추진 중이며, 단순 실험을 넘어 프레임워크 투자와 에이전트 생태계 구축을 본격화함

     * AI Agent에 대한 글로벌 관심 급증 (Google 검색 트렌드, 2024~2025)
          + ‘AI Agent’라는 키워드의 Google 검색량이 16개월 만에 1,088% 급증
          + 2025년 3월 OpenAI가 AI Agent 개발도구를 출시한 후 검색량이 더욱 가파르게 증가하며, 업계의 기술적 전환점이 되었음을 시사함

     * AI 기득권자(Incumbent)의 AI Agent 제품 출시 가속화 (2024~2025)
          + Salesforce, Anthropic, OpenAI, Amazon 등 대표 빅테크 기업들이 AI 에이전트 기반 신제품을 속속 출시함
               o Salesforce Agentforce: 고객지원 자동화, 리드 발굴, 주문 트래킹 등
               o Anthropic Claude 3.5 Computer Use: 컴퓨터 스크린 직접 제어, 웹 데이터 추출, 온라인 구매 등
               o OpenAI Operator: 복잡한 온라인 작업 자동화
               o Amazon Nova Act: 홈 오토메이션, 정보수집, 구매, 일정관리 등
          + AI Agent 제품들은 기존 챗봇을 넘어 실제 ‘일’을 대신하는 실질적 자동화 도구로 확장 중임

Next Frontier For AI = Artificial General Intelligence

     * Artificial General Intelligence(AGI) 란?
          + AGI는 인간의 지적 작업 전반(추론, 계획, 소규모 데이터 학습, 다양한 영역 간 지식 일반화 등)을 모두 수행할 수 있는 시스템을 의미함
          + 현재 AI 모델이 특정 영역 내에서 뛰어난 성능을 보이는 것과 달리, AGI는 분야를 가리지 않고 재학습 없이 새로운 문제도 유연하게 해결할 수 있음
          + 최근 모델 규모, 학습 데이터, 컴퓨팅 효율성의 기하급수적 성장이 AGI 개발을 앞당기고 있음
     * AGI 도달 시기와 기대감
          + AGI 달성 시기는 여전히 불확실하지만, 전문가들의 기대가 최근 몇 년 새 크게 앞당겨짐
          + OpenAI CEO Sam Altman은 2025년 1월, ""이제 우리가 전통적으로 이해한 AGI를 어떻게 만들 수 있는지 확신한다""고 언급함
          + 이는 모델 구조, 추론(inference) 효율, 대규모 학습 환경의 발전으로 연구와 실전의 간극이 줄어들고 있음을 시사함
               o 추론(inference)은 완전히 학습된 모델이 사용자 입력에 대해 예측, 답변, 콘텐츠를 생성하는 과정. 이 단계는 학습보다 훨씬 빠르고 효율적임
          + AGI는 더 이상 가상의 종착점이 아니라 도달 가능한 임계점으로 인식되기 시작함
     * AGI 도달의 의미
          + AGI가 실현되면 소프트웨어와 하드웨어의 본질적 역할이 재정의될 것임
          + 미리 프로그래밍된 작업을 반복하는 대신, 목표 이해, 계획 수립, 실시간 자기교정이 가능한 시스템으로 변화
          + 연구, 엔지니어링, 교육, 물류 등 다양한 워크플로우를 인간의 감독 없이도 운영 가능
          + 새로운 문제에 직면해도 재학습 없이 맥락에 맞춰 적응하며, 인간 전문가처럼 작동
          + AGI 기반 휴머노이드 로봇은 물리적 환경과 일하는 방식을 근본적으로 변화시킬 수 있음
     * AGI가 가져올 사회적 영향
          + AGI는 마지막 목표점이 아닌, 역량의 단계적 전환임
          + 제도, 노동, 의사결정 구조가 AGI 도입 방식과 통제 장치에 따라 재편
          + 생산성 향상 효과가 클 수 있으나, 수혜가 불균형하게 분배될 가능성도 있음
          + 지정학적, 윤리적, 경제적 변화는 점진적으로 진행될 전망
          + 산업혁명, 디지털 전환, 알고리듬 혁명처럼, 기술이 무엇을 할 수 있느냐뿐 아니라 사회가 어떻게 받아들이고 규율하느냐에 따라 결과가 달라짐

AI User + Usage + CapEx Growth = Unprecedented

     * 지난 20년간 기술 분야 CapEx는 데이터 중심의 아크(arc)를 따라 급격히 증가해왔음
          + 초기에는 스토리지와 접근(저장/접근) 에 투자, 이후 분산/확장으로, 현재는 컴퓨팅/지능으로 중심이 이동함
     * 1차 웨이브에서는 대규모 서버팜, 해저 케이블, 초기 데이터센터 등에 자금이 집중되어 아마존, 마이크로소프트, 구글 등이 클라우드 컴퓨팅의 기반을 마련함
          + 이 단계는 '저장, 조직화, 서비스 제공'이 핵심 목표였음
     * 2차 웨이브(현재 진행형)는 AI 워크로드를 위한 컴퓨팅 인프라 강화가 중심
          + Hyperscaler(초대형 데이터센터 사업자)들의 CapEx는 GPU, TPU, AI 가속기, 액체 냉각, 첨단 데이터센터 설계 등 특화 인프라로 옮겨가고 있음
          + 2019년 AI는 연구 기능이었지만, 2023년에는 CapEx(자본적 지출)에서 핵심 항목으로 편입됨
     * 마이크로소프트 브래드 스미스 회장(4/25 블로그):
          + ""전기와 같은 범용 기술처럼, AI와 클라우드 데이터센터는 차세대 산업화의 단계를 대표함""
     * 글로벌 빅테크들은 연간 수십조 원 규모의 투자를 진행 중
          + 단순히 데이터를 수집하는 것이 아니라, 빠르게 학습하고, 깊게 개인화하며, 폭넓게 배포하는 능력이 경쟁력의 핵심이 되고 있음

     * AWS, NVIDIA, MSFT, Google, Apple, Meta 등 대형 기술 기업들의 CapEx(설비 투자) 지출은 수년간 꾸준히 상승 중임

Data Centers = AI CapEx 지출의 핵심 수혜자

     * AI 인프라의 경제성을 이해하려면 데이터 센터 건설 속도와 규모를 살펴볼 필요가 있음
          + AI 중심 수요의 폭발로 글로벌 IT 기업의 데이터 센터 CapEx(자본 지출) 이 사상 최고치에 도달, 2024년 기준 4,550억 달러에 달하며 가속화되고 있음
     * Hyperscaler와 AI-first 기업 모두 스토리지 뿐만 아니라 실시간 추론과 대규모 모델 학습을 위한 고성능, 고전력 하드웨어 인프라 구축에 수십억 달러 투자
          + AI가 실험적 기술에서 필수 인프라로 전환되면서 데이터 센터도 이에 맞춰 핵심적 위치를 차지
          + NVIDIA CEO 젠슨 황은 ""이제 AI 데이터 센터는 AI 팩토리""라고 강조
     * 미국 테네시주 멤피스의 xAI Colossus 데이터 센터는 418채 규모의 건물을 단 122일 만에 완공, 전례 없는 속도와 효율성 달성 (평균 미국 주택 건설의 절반 이하 기간)
          + 사전 제작 모듈, 신속한 인허가, 전기/기계/소프트웨어의 수직 통합을 통해 데이터 센터가 IT 제품 개발 속도로 건설되는 시대 도래
     * 데이터 센터 CapEx는 토지, 전력, 칩, 냉각 설비에 의해 좌우되며, AI 워크로드가 기존 엔터프라이즈 컴퓨팅보다 훨씬 높은 열 및 전력 수요를 발생
          + OpEx(운영비용)은 에너지 비용과 시스템 유지보수가 중심이며, 특히 고밀도 AI 학습 클러스터는 상시 최대 부하로 작동
     * 수익은 컴퓨트 판매(AI API, 엔터프라이즈 플랫폼 요금, 내부 생산성 향상 등) 에서 나오지만, 선제적 구축을 하는 기업은 투자 회수 기간이 길어질 수 있음
          + 신생 업체는 인프라 구축 후 수익화까지 수 분기~수년이 소요될 수 있음
     * 공급망 측면에서 전력 인프라(변압기, 변전소, 터빈, GPU, 케이블 등) 확보가 새로운 병목 요인으로 부상
          + 데이터 센터는 단순한 물리적 자산이 아니라, 부동산, 전력, 물류, 컴퓨트, 소프트웨어 수익화의 전략적 인프라 허브 역할
     * 이 복잡한 퍼즐을 제대로 풀어낸 기업이 향후 AI 경제의 지리적 판도를 좌우할 것

Data Centers = Electricity Guzzlers

     * AI와 에너지 인프라의 긴장 관계가 점차 심화되고 있음
          + AI의 고도화로 인해 AI 특화 데이터센터가 기존 중공업 못지않은 전력 소비를 기록 중
          + AI 모델 학습과 서비스에 필요한 막대한 연산력이 전기 수요를 폭증시키는 주요 원인
     * 데이터센터는 2024년 전 세계 전력 소비의 약 1.5% 차지
          + 2017년 이후 글로벌 데이터센터 전력 소비가 연평균 12% 증가
          + 전체 전력 소비 증가 속도의 4배 이상에 해당
     * 국가별 전력 소비 비중에서 미국이 45%로 1위, 중국(25%), 유럽(15%) 순
          + 미국 데이터센터 용량의 거의 절반이 5개 주요 지역 클러스터에 집중
          + 신흥국과 개발도상국(중국 제외)은 인터넷 사용자의 50% 차지하지만 데이터센터 용량은 10% 미만에 머무름
     * AI 확산에 따라 전력망(grid)과 공급 인프라가 AI 성능의 병목으로 부상
          + 더 이상 데이터나 알고리듬이 아닌, 전력 공급이 AI 성장의 핵심 제약이 되고 있음
     * 한편, AI는 에너지 산업 내에서 운영 효율화와 혁신을 가속
          + 전력, 광물, 전송, 소비 등 에너지 공급망 전체에서 AI 기반 최적화가 본격 적용 중
          + 하지만 AI 수요와 에너지 비용이 계속 증가하는 한, 데이터센터는 결국 비용 지불 능력이 있는 고객만을 대상으로 서비스할 것임

AI Model Compute Costs High / Rising + Inference Costs Per Token Falling = Performance Converging + Developer Usage Rising

     * 대형 언어 모델(LLM) 훈련은 인류 역사상 가장 비용 집약적인 작업 중 하나로, 성능 향상을 위해 파라미터 수와 알고리듬 복잡성이 증가하면서 훈련 비용이 수십억 달러로 치솟고 있음
          + 가장 뛰어난 범용 모델 구축 경쟁이 심화될수록, 결과물 품질의 차별화는 점점 어려워지고 수익성도 악화되는 '수렴' 현상 발생
     * 반면, 추론(inference) 비용은 급격히 하락 중
          + 예를 들어, NVIDIA 2024 Blackwell GPU는 2014년 Kepler 대비 토큰당 에너지 소모가 10만 5천 배 감소
          + 하드웨어 혁신 및 모델 알고리듬 효율성 향상 덕분에 토큰당 추론 비용은 빠르게 낮아지고 있음
     * 추론 비용 하락은 LLM 제공자 간 경쟁을 심화
          + 정확도뿐만 아니라 지연 시간, 가용성, 토큰당 단가에서 경쟁
          + 달러 단위 비용이 이제는 몇 센트, 그리고 곧 1센트 미만으로 내려가는 상황
     * 사용자(개발자) 입장에서는 저렴한 비용에 강력한 AI 접근이 가능해지며
          + 신규 서비스와 제품 개발이 활성화, 실제 사용자와 활용도 역시 빠르게 증가 중
     * 모델 제공자에게는 수익성 악화와 비즈니스 모델 변화라는 새로운 과제가 등장
          + 훈련은 비싸고 제공은 싸지면서 수직/수평 통합, 특화형 LLM 시장 등 새로운 전략이 모색되고 있음
          + 범용 LLM은 수익성 없는 소모전 양상을 보이기 시작
          + 소형·맞춤형 모델의 등장으로 기존 대형 모델과 차별화된 수익 구조 실험이 본격화되는 추세

Inference Costs Per Token Falling

     * AI 추론 비용 감소는 컴퓨팅 발전의 대표적인 패턴을 반복
          + 1997년 Microsoft CTO Nathan Myhrvold가 “소프트웨어는 가스와 같아서 주어진 그릇을 모두 채운다”고 말한 것처럼, AI도 인프라를 모두 활용할 만큼 수요가 커지는 중
          + 모델 성능이 좋아질수록 사용량(쿼리, 토큰, 모델 수)이 폭발적으로 증가하며, 인공지능 활용 범위와 빈도 역시 급격히 확대
     * 인프라 발전 속도도 역대 최고 수준
          + 2024년 NVIDIA Blackwell GPU는 2014년 Kepler 대비 토큰 생성 시 에너지 효율이 10만 5천 배 개선
          + 이는 단순 비용 감소가 아니라 하드웨어 아키텍처와 소재 혁신의 결과임을 의미
     * 하드웨어 효율 향상은 급증하는 AI 및 인터넷 수요의 전력 부담을 상쇄하는 핵심 요소
          + 하지만 지금까지의 개선만으로는 전체 전력 수요 증가를 완전히 막지 못하는 상황
          + 이 현상은 1865년 제번스 패러독스(Jevons Paradox)와 유사
               o 자원 효율이 높아질수록 전체 소비량이 더 늘어나는 역설적 현상이 AI에서도 반복
     * 결과적으로, 비용 하락·성능 향상·사용량 증대라는 테크놀로지의 오래된 공식이 AI에서도 반복되고 있음
          + 인프라 발전이 AI 사용 증가를 다시 부추기며, 전력 인프라와 에너지 생산에 대한 새로운 고민을 야기

Performance Converging

     * AI 모델 성능 상위권의 빠른 수렴 현상
          + Stanford HAI의 LMSYS Chatbot Arena 데이터(2024~2025년) 기준, Google, OpenAI, DeepSeek 세 모델의 챗봇 평가 점수가 1,385, 1,366, 1,362로, 불과 1~2% 내외의 근소한 차이만 남김
          + 1년 새 상위권 모델 간 점수 격차가 점점 줄어들며, 성능 경쟁이 사실상 평준화되는 추세가 뚜렷
     * 최신 대형 언어모델(LLM) 간 품질 차별화가 어려워지는 상황
          + 사용자 입장에서 ""어느 모델을 써도 거의 비슷하다""고 느낄 수 있는 환경이 조성
          + 모델 제공 업체는 비용·서비스 안정성·특화 기능 등 비성능 요소의 경쟁으로 이동할 가능성 커짐

Developer Usage Rising

     * AI 개발자 활동이 폭발적으로 증가하는 현상은 추론(inference) 비용의 극적인 하락과 유능한 모델의 접근성 확대에서 비롯됨
          + 2022년~2024년 사이 언어 모델 실행의 토큰당 비용이 약 99.7% 감소
               o 하드웨어, 알고리듬 효율의 비약적 발전이 배경
               o 과거에는 대기업만 접근 가능했던 기술이 이제는 개인 개발자, 독립 앱 빌더, 연구자, 소규모 사업자까지도 쉽게 활용 가능
          + 비용 붕괴로 실험이 저렴해지고, 반복/제품화가 신속해짐
               o 누구든 아이디어만 있으면 쉽게 AI 서비스 개발 가능
     * 모델 성능이 빠르게 수렴하면서 모델 선택의 공식이 변화
          + 최상위 대형 모델과 더 작고 효율적인 대안 모델 간의 격차가 좁아짐
          + 요약, 분류, 추출, 라우팅 등 여러 실사용 과제에서 실제 성능 차이가 거의 없음
          + 개발자들은 이제 고가 프리미엄 모델 대신 저렴한 모델이나 로컬 실행/저가 API를 통해 비슷한 결과를 얻을 수 있게 됨
          + 특히 태스크 특화 데이터로 파인튜닝 시 효과 극대화
     * 이 변화는 모델 '기득권'의 가격 지렛대를 약화시키고, AI 개발의 평등화를 촉진
          + 단일 사업자(벤더)에 종속되기보다, 다양한 생태계의 모델을 조합/분산 활용
          + OpenAI ChatGPT, Meta Llama, Mistral Mixtral, Anthropic Claude, Google Gemini, Microsoft Phi 등
               o 각기 강점이 다른 모델 중에서 기술적/재무적 니즈에 맞는 최적 모델 선택 가능
          + 플랫폼 락인(종속)에서 탈피, 개발자가 주도하는 선택과 분산의 시대로 전환
     * 개발자 주도의 인프라 성장 플라이휠이 형성 중
          + 더 많은 개발자가 AI 네이티브 앱을 만들면서, 도구/래퍼/라이브러리/프레임워크 생태계가 급증
          + 프론트엔드 프레임워크, 임베딩 파이프라인, 모델 라우터, 벡터 DB, 서빙 레이어 등
               o 매번 개발자 활동의 파동이 다음 파동의 진입 장벽을 낮추는 효과
     * 아이디어에서 프로토타입, 프로토타입에서 제품까지 걸리는 시간 단축
          + 비용뿐 아니라 복잡성까지 빠르게 감소
          + 플랫폼의 변화를 넘어, 창의력 폭발의 시대가 열리고 있음
     * 역사적으로도 개발자가 많고(사용/활용이 꾸준한) 플랫폼이 결국 승자가 되는 패턴이 반복됨
          + 마이크로소프트 스티브 발머의 “Developers! Developers! Developers!” 연설에서 봤듯이 개발자는 중요함
          + 개발자에게 채택받고, 지속적 스케일, 개선을 이끌어낸 플랫폼이 결국 시장을 지배할 것

The AI Developer Next Door

     * AI 개발 도구 채택률 급등 (2023~2024, Stack Overflow)
          + 2023년 대비 2024년 AI 도구를 활용하는 개발자 비율이 크게 증가
          + 프로 개발자 기준 44% → 63%, 코딩 학습자 기준 55% → 65%로 상승
     * AI 개발자 오픈소스 저장소 폭증 (GitHub, 2022.11~2024.3)
          + GitHub 내 AI 개발자 저장소 수가 16개월간 약 175% 증가
          + ChatGPT, Stable Diffusion 등 주요 모델/도구 등장 이후 개발 생태계 폭발적으로 성장
     * AI 개발자 생태계 확장(Google 기준, 월간 토큰 사용량)
          + 2024년 5월 10조 토큰 → 2025년 5월 480조 토큰으로 1년만에 50배 증가
          + 구글 Gemini, AI API 등 통해 개발자 사용량 대폭 확대
     * Microsoft Azure AI Foundry 생태계 성장(분기별 토큰 사용량)
          + 2024년 1분기 20조 → 2025년 1분기 100조 토큰으로 5배 성장
          + 7만 개 이상 기업/개발자가 활용 중
     * AI 개발자 활용 사례의 다양화 (2024년, IBM 기준)
          + 코드 생성, 버그 탐지/수정, 테스트 자동화, 프로젝트/워크플로우 관리, 문서화, 리팩토링/최적화, 보안 강화, CI/CD, UX 디자인, 아키텍처 설계 등 광범위한 영역에서 AI 활용이 가속

AI Usage + Cost + Loss Growth = Unprecedented

     * AI 성장 및 비용, 손실의 규모가 전례 없는 수준으로 증가
          + “이번에는 다르다”, “규모의 경제로 수익을 맞출 수 있다”, “나중에 사용자를 수익화하겠다”는 위험 신호가 기존에는 실패를 부르곤 했으나, 빅테크 투자에서는 실제 성공 사례도 존재
          + 이번 AI 경쟁은 전례 없는 규모의 자본과 창업자 중심 대기업들이 동시에 뛰어드는 양상
          + 미국, 중국 등 글로벌 강국의 경쟁이 AI 혁신을 가속
     * 주요 기술 도입 시점마다 전환점이 존재
          + 개인 컴퓨터는 매킨토시(1984)·윈도우 3.0(1990), 인터넷은 넷스케이프 IPO(1995), 모바일은 아이폰 앱스토어(2008), 클라우드는 AWS(2006~09), AI는 NVIDIA A100(2020), ChatGPT(2022) 등이 결정적 계기
          + 2025년 중국 DeepSeek의 등장이 글로벌 AI 경쟁 격화의 신호탄
     * AI 성장을 위한 자금은 거대 IT기업의 막대한 현금 흐름과 글로벌 자본에서 비롯
          + 치열한 경쟁·자본·창업가 정신의 조합이 AI 발전을 가속
     * 단, 최종 승자 비즈니스 모델이 무엇일지는 아직 불확실

Technology Disruption Pattern Recognition = Hundreds of Years of Consistent Signals

     * 기술 혁신의 역사에는 초기 과열, 자본 유입, 경쟁 심화, 승자와 패자 구분의 주기가 반복됨
          + 예시: 19세기 철도, 1840년대 거품, 기대 붕괴 등
     * 대규모 자본 투입이 필요한 기술은 초기엔 실망스러운 수익률을 보이나, 성공 시 장기적 산업 구조를 바꿈
          + 단, 경쟁으로부터 보호받지 못하면 높은 위험을 내포
     * 최종 승자는 항상 최고의 기술 보유자가 아니라, 시장과 산업의 흐름을 가장 명확히 읽은 주체
     * 진입장벽 없는 시장에서는 선점 효과가 빠르게 사라짐
          + “새로운 기술의 승자는 예측이 어렵지만, 패자는 쉽게 보인다”는 교훈을 되새길 것

AI-Related Monetization = Very Robust Ramps

     * AI 하드웨어 전략의 진화: 칩 설계 주도권이 전통 벤더에서 플랫폼 기업으로 이동
          + NVIDIA GPU는 오랜 기간 AI 트레이닝/추론의 기본 엔진 역할을 하며 독보적 지위를 확보
          + 수요 폭증으로 인해 NVIDIA의 빠른 생산 확대에도 공급 부족 현상 지속, 하이퍼스케일러와 클라우드 사업자들이 공급망 다변화에 나섬
     * 맞춤형 칩(ASIC)의 부상: 범용 GPU 대비 특정 AI 연산에 최적화된 ASIC 도입 가속
          + Google의 TPU, Amazon의 Trainium 칩이 각사의 AI 스택 핵심 요소로 자리잡음
               o Amazon Trainium2는 일반 GPU 대비 30~40% 더 우수한 가격/성능 제공, 대규모 인퍼런스 비용 절감 가능
          + 이러한 커스텀 칩은 단순 실험이 아니라, 성능·경제성·아키텍처 통제를 위한 핵심 전략
     * AI 인프라 경제성 개선 노력 확산
          + Amazon CEO Andy Jassy: ""AI가 반드시 지금처럼 비싸야 할 필요는 없고, 앞으로는 더 저렴해질 것""
          + 커스텀 실리콘은 AI 인프라 비용 절감의 핵심 수단 중 하나
     * AI 인프라 전문 기업의 성장
          + CoreWeave: 게이밍·암호화폐 하드웨어 공급망을 AI용 GPU 클라우드로 재구성해 빠르게 성장
          + Oracle: 전통 IT에서 AI 특화 GPU 클라우드 플랫폼으로 전환
          + Astera Labs: GPU-메모리 간 초고속 연결장치 공급, 대규모 모델의 성능 한계 극복에 기여
     * 이들 기업은 파운데이션 모델을 직접 개발하지 않지만, 그 생태계의 필수적 인프라를 구축
          + 연산 수요 급증에 따라 속도, 가용성, 효율성이 경쟁력의 핵심으로 부상

AI Monetization = Chips

     * NVIDIA, Google, Amazon 등 주요 기업들의 AI 칩 매출이 빠르게 성장하고 있음
          + NVIDIA 분기 매출은 전년 대비 78% 증가해 390억 달러를 돌파, 주력은 데이터센터 부문
          + 최근 10년간 NVIDIA 매출은 28배 성장했고, 미국 빅테크(애플, 마이크로소프트, 알파벳, 아마존, 메타, 엔비디아)의 CapEx + R&D 투자도 6배 확대
          + Google의 TPU(Tensor Processing Unit) 매출은 연간 116% 증가하여 89억 달러 규모로 추정됨
               o Google TPU는 AI 모델 학습에 특화된 ASIC 칩으로, 2015년 첫 버전 출시 이후 누적 10만 개 이상 생산
          + Amazon의 AWS Trainium 칩 매출은 연간 216% 증가해 2025년 36억 달러 도달 전망
               o Trainium2는 기존 GPU 기반 인스턴스 대비 30~40% 가격/성능 우위, 최대 4배 성능 제공
     * AI 칩 시장의 성장 배경
          + AI 학습 및 추론 수요 급증에 따라 GPU, ASIC 등 고성능 칩에 대한 수요가 폭발적으로 늘어나고 있음
          + 주요 클라우드 및 hyperscaler 기업들은 자체 칩 설계 및 공급망 강화에 집중, 가격 경쟁력과 인프라 효율성을 높이고 있음
          + GPU, TPU, Trainium 등 AI 특화 칩은 데이터센터의 핵심 수익원이자 AI 인프라의 경쟁력 결정 요소로 자리잡음

AI Monetization = Compute Services

     * AI 컴퓨팅 서비스 시장의 성장
          + AI 인프라 전문 클라우드 기업 CoreWeave의 2024년 매출이 전년 대비 730% 증가해 19억 달러에 도달
          + OpenAI 등 주요 고객사와의 대형 계약, IPO, Weights & Biases 인수 등으로 사업 성장 가속화
          + AI 워크로드를 위한 고성능 클라우드 인프라 수요가 폭발적으로 증가하며, 인프라 제공 기업의 초고속 매출 성장이 이어짐
     * AI 인프라스트럭처 시장의 확대
          + Oracle의 AI 인프라 매출은 2년 만에 50배 성장해 2024년 9억 5천만 달러에 도달 (Morgan Stanley 추정)
          + AI 인프라 수요 증가로 대규모 신규 고객 계약이 대기 중이며, 아직 본격적으로 서비스되지 않은 대규모 예약이 존재
          + Oracle CEO는 AI 인프라 고객이 몰리며 “10억 달러 이상 신규 계약만 40건 이상”이라고 언급
     * AI 인프라 연결성의 성장
          + Astera Labs는 2024년 매출이 전년 대비 242% 성장해 3억 9천 6백만 달러 기록
          + PCIe, CXL, 이더넷 등 고속 연결 제품군이 다수의 고객사 및 플랫폼에 적용되며 GPU 연결, 백엔드 AI 가속기 클러스터 등 AI 데이터센터 내 필수 인프라로 자리매김
     * AI 데이터 수집과 슈퍼컴퓨팅
          + Tesla는 Dojo 슈퍼컴퓨터와 GPU 대량 도입으로 AI 트레이닝 용량이 2021년 6월 대비 8.5배 증가 (2024년 9월 기준)
          + Dojo는 자체 트레이닝 비용 혁신은 물론, AWS처럼 외부 서비스로 전환 가능한 잠재력이 매우 크다고 평가
          + Elon Musk는 “Dojo의 잠재력이 매우 크다”고 언급

AI Monetization = Data Layer

     * AI 데이터 계층의 수익화가 가속화되고 있음
     * Scale AI는 2023년 3억 3,500만 달러에서 2024년 8억 7,000만 달러로 매출이 160% 증가함
          + 데이터 라벨링, 평가, 파이프라인 구축 등 프론티어 LLM 확장을 위한 핵심 인프라 제공
          + ""데이터 풍요는 선택이며, AI 한계를 데이터 부족으로 두지 않겠다""는 비전 제시
          + 2024년 신규 계약만 15억 달러 이상 확보
     * VAST Data는 2019년 1월부터 2025년 5월까지 누적 매출 20억 달러 달성
          + AI 인프라 레이어를 단순화하고, 데이터 스토리지·관리·처리 서비스를 제공
          + AI Reasoning 모델의 부상과 함께 데이터 인프라의 중요성 부각
          + ""AI 시대에 가장 큰 잠재력을 실현하려면, 기본적인 문제를 단순화하는 것이 핵심""이라고 강조

높은 매출 성장, 막대한 현금 소진, 높은 기업가치, 높은 투자 규모 = 소비자에게는 호재, 그 외에는 아직 미정

     * 글로벌 디지털 사용자 기반의 성장과 사용량 급증 가능성이 커지면서, 기업 투자 영역은 점점 더 경쟁적이고 자본 집약적으로 변화
          + AI 기술 주기의 창의적 파괴는 과거 주요 IT 기업 성장 과정과 유사점 보임
     * 애플, 아마존, 구글, 우버, 테슬라 등 과거 주요 테크 기업 사례:
          + 애플: 1997년 파산 직전 시가총액 17억 달러에서 현재 3.2조 달러로 성장
          + 아마존: 2000년 한 해 -5억 4,500만 달러 적자, 창업 후 27분기 동안 -30억 달러 적자, 최근 27분기 순이익 1,760억 달러 기록, 시총 2.2조 달러
          + 구글: 2004년 IPO 당시, 매출 3억 9,000만 달러 중 22%를 CapEx에 투자, IPO 시가총액 230억 달러에서 현재 2조 달러
          + 우버: 2016~2022년 -170억 달러 현금 소진, 2023년 첫 흑자 전환, IPO 시가총액 820억 달러에서 현재 1,890억 달러
          + 테슬라: 2009~2018년 -92억 달러 소진, 2019년 첫 흑자 전환 후 5년간 순이익 400억 달러, 현재 시가총액 1.1조 달러
     * 이들 기업 모두 과감한 투자와 장기 적자를 감수하면서 데이터 기반 네트워크 효과와 기술 기반 경쟁 우위를 구축, 결국 시장에서 가치를 증명
     * 최종적으로, 기업 가치(valuation)는 미래 자유현금흐름의 현재가치에 의해 정당화되어야 함
     * AI 분야 역시, 궁극적으로 어떤 플레이어가 지속가능한 수익을 창출할 수 있을지는 시간이 증명할 것

Usage + Cost + Loss Growth = 전례 없는 수준… 미래의 수익화와 이익은?

  AI Monetization Possibilities = New Entrants & / Or Tech Incumbents?

     * AI 모델 경제 구조의 향방을 이해하려면, 역량과 비용 간의 긴장을 살펴봐야 함
     * 초거대 LLM 훈련은 인류 역사상 가장 비용이 많이 드는 시도 중 하나로, 파라미터와 아키텍처 복잡성 증가로 인해 비용이 수십억 달러로 급등하고 있음
     * 반면, 추론(Inference) 비용은 하드웨어 혁신과 알고리듬 효율화로 인해 극적으로 하락
          + 예시: NVIDIA 2024 Blackwell GPU는 2014 Kepler 대비 토큰당 에너지 105,000배 감소
     * 추론이 저렴해질수록 정확도·지연시간·가용성·토큰당 비용에서 LLM 제공자 간 경쟁이 심화되고, 기존에 비싸던 작업이 이제는 거의 무시할 수 있는 비용으로 가능해짐
     * 사용자와 개발자에게는 단가 하락이 기회로 작용해, 신규 서비스와 활용이 폭발적으로 증가 중
     * 반면 모델 제공자는 수익화와 이익 모델이 불확실해짐
          + 훈련은 여전히 비싸고, 제공(서빙)은 저렴해지면서 가격 결정력이 약화됨
          + 맞춤형 소형 모델 시장이 등장해 기존 비즈니스 모트 약화
     * 예시: Google은 AI Overviews를 2024년 5월부터 검색에 도입(2025년 4월 15억 MAU), 최근 일부에 광고 적용 시작
     * 앞으로는 플랫폼 전략(수평적 확장), 특화형 애플리케이션, 구독·광고 등 다양한 수익화 모델의 경쟁이 가속될 전망
     * 단기적으로는 범용 LLM의 경제성이 벤처급 적자와 상품화 경쟁에 점점 가까워지고 있음

AI – New Entrants = Rapidly Laying Groundwork

     * AI 기반 주요 파운데이션 모델 사업자들의 소비자 구독형 모델 도입 현황 (2025년 5월 기준)
          + OpenAI ChatGPT, xAI Grok, Google Gemini, Anthropic Claude, Perplexity 등 다양한 모델에서 무료/유료 구독제를 제공
               o OpenAI ChatGPT: $0 (무료) / $20 (Plus) / $200 (Pro) (월 기준)
               o xAI Grok: $0 (무료) / $3 (Basic) / $8 (Premium) / $40 (Premium+) (월 기준)
               o Google Gemini: $0 (무료) / $19.99 (AI Pro) / $250 (AI Ultra) (월 기준)
               o Anthropic Claude: $0 (무료) / $17 (Plus) / $100 (Max) (월 기준)
               o Perplexity: $0 (무료) / $20 (Pro) (월 기준)
     * AI 기반 파운데이션 모델 사업자들의 개발자 API 요금제 도입 현황 (2025년 5월 기준)
          + OpenAI, xAI Grok, Google Gemini, Anthropic Claude, Perplexity 등에서 API 호출 단위별 요금을 부과
               o OpenAI ChatGPT: 1백만 토큰당 $0.40 (GPT-4.1 nano) ~ $40 (o3)
               o xAI Grok: 1백만 토큰당 $0.50 (grok-3-mini-beta) ~ $25 (grok-3-fast)
               o Google Gemini: 1백만 토큰당 $0.15 (1.5 Flash-8B) ~ $15 (2.5 Pro Preview)
               o Anthropic Claude: 1백만 토큰당 $1.25 (Claude 3 Haiku) ~ $75 (Claude 3 Opus)
               o Perplexity: 1백만 토큰당 $1 (Sonar) ~ $15 (Sonar Pro)

AI – New Entrants = Rapid Revenue Growth

     * OpenAI의 유료 구독자 수 및 연매출 성장 (2022년 10월~2025년 4월)
          + ChatGPT 유료 구독자 수 연 153% 증가, 2025년 4월 기준 약 2,000만 명 도달
          + OpenAI 연매출 1050% 증가, 2025년 4월 기준 37억 달러 돌파
     * Anthropic의 API 및 생성형 검색 기반 연환산 매출 18개월간 20배(2십억 달러) 성장
          + Claude 3.7 Sonnet 등 Reasoning 중심 신모델 전략 및 실제 업무 중심 AI 활용 확대
          + 1년간 6.4배 성장
     * Perplexity의 생성형 검색 기반 연환산 매출 14개월간 7.6배(1억 2천만 달러) 성장
          + 모든 답변에 근거 출처 제공, 개인화된 리서치 도우미 기능 강조
     * Glean의 엔터프라이즈 검색 및 에이전트 연환산 매출 24개월간 10배(1억 달러) 성장
          + 대기업의 AI 도입 지원, 조직 지식 전반 활용 가능하도록 설계
     * AI 기업(상위 100개)의 연환산 500만 달러 매출 도달까지 평균 24개월 소요
          + 기존 SaaS 기업 대비 35% 빠른 속도 (SaaS 평균 37개월, AI 평균 24개월)

AI –Tech Incumbents = Broad & Steady Product / Feature Rollouts

     * ChatGPT 사용자 수(8억 명)와 비교되는 Tech Incumbent의 글로벌 AI 사용자 및 디바이스 현황
          + Google, Meta, Apple, TikTok, Microsoft, Spotify, Amazon, X, Canva 등은 수억~수십억 명의 사용자를 기반으로 AI 제품을 점진적으로 확장 중
     * Canva – Background Remover & Magic Media (12/19)
          + 2019년 출시된 이미지 배경 제거 기능은 꾸준한 인기를 끌며 누적 30억 회 이상 사용됨
          + 2024년 출시된 Magic Media(텍스트→이미지/동영상)는 출시 1년 만에 2억 9,000만 개 이상의 창작물 생성 등 커뮤니티의 높은 반응을 얻고 있음
     * Spotify – AI DJ (2/23)
          + AI DJ 및 AI 뮤직비디오 등 혁신적 기능을 2023년 2월부터 전 세계 시장에 론칭, 2024년 5월 기준 60개국 이상에서 서비스 제공
          + AI DJ는 개인화된 음악 추천과 상호작용, 데이터 기반 실시간 reasoning 기능으로 이용자 만족도와 Spotify 서비스 품질을 높임
     * Microsoft – Copilot (2/23)
          + 2023년 2월 Bing 및 Edge에 Copilot을 도입, 2024년 12월 누적 150억 건 이상의 대화량 기록
          + Copilot은 웹 검색, 브라우저, 오피스 도구 전반에 AI 기반의 새로운 사용자 경험을 제공하며, 일상 업무의 효율성 및 창의성 향상에 기여
     * Meta Platforms – Meta AI (9/23)
          + 2025년 4월 기준 Meta AI는 Instagram, Messenger, WhatsApp 등 전 앱을 합쳐 월간 활성 사용자(MAU) 10억 명에 근접
          + 향후에는 중급 엔지니어 수준의 AI 에이전트 개발 및 AI 연구·필드 적용에 주도권을 확보하려는 전략이 강조됨
     * X – Grok (11/23)
          + xAI의 Grok은 2025년 2월 3.0 버전 출시와 동시에 데스크톱 글로벌 방문자가 전월 대비 42배 급증, 1억 5,000만 명 이상 기록
          + AI의 진실 추구(value alignment)와 대규모 배포가 강조되며, X 플랫폼 내 AI 경험 확장 중임
     * Google – Gemini & AI Overviews (12/23)
          + 2024년 5월 기준 Gemini 챗봇 MAU 4억 명, AI Overviews는 구글 검색에 임베디드되어 15억 명의 월간 사용자에 도달
          + 다양한 데이터 타입(텍스트, 코드, 이미지, 오디오 등)을 아우르는 멀티모달 AI 모델 및 검색 내 AI 요약 기능이 강점
     * Amazon – Rufus (2/24)
          + 2024년 2월 북미 리테일 분야에서 Rufus AI 도입, 제품 정보·리뷰 요약 등에서 개인화 추천 개선
          + 리테일 사업의 총거래액(GMV) 성장과 함께, AI 활용도가 지속적으로 확대 중임
     * TikTok – Symphony AI Assistant (6/24)
          + 2024년 6월 Symphony Assistant 도입 이후, TikTok 글로벌 웹사이트 방문자 20억 명 이상으로 집계
          + AI를 활용한 브랜드·크리에이터용 콘텐츠 생성, 광고 효율, 브랜드 호감도 등에서 실질적 성과 도출
     * Apple – Apple Intelligence (10/24)
          + 2024년 9월~2025년 3월, iPhone 15 Pro/Pro Max, iPhone 16 시리즈 등 Apple Intelligence 지원 기기 판매가 5,000만~7,000만 대에 이름
          + 하드웨어와 소프트웨어 융합을 통해 개인화, 프라이버시, 로컬 컴퓨팅 기반 AI 경험 제공에 중점

AI –Tech Incumbents = Rapid Revenue + Customer Growth

     * Microsoft AI Product Revenue
          + 2024년 기준 Microsoft의 AI 제품 부문 연간 매출 추정치가 130억 달러에 달하며, 전년 대비 175% 증가
          + Azure AI 서비스, Microsoft 365 Copilot, Dynamics 365 Copilot 등 다양한 AI 제품 라인업이 매출 성장을 견인
          + Satya Nadella CEO는 “AI 투자 수익(ROI) 실현을 지원하며 거대한 기회를 잡고 있다”고 언급
          + 2025년 1분기 실적 발표에서 상업 부문 예약(Commercial bookings)이 18% 증가했음을 강조
     * xAI: Generative Search
          + 2025년에 xAI의 연간 매출이 본격적으로 증가할 전망
          + 최신 모델 Grok 3는 Colossus 슈퍼컴퓨터 기반에서 10배 이상 연산력을 바탕으로, 강력한 추론·수학·코딩·지식 기반 작업에서 성능 향상
          + Elon Musk CEO는 “정치적으로 불편한 진실이라도 추구하는, 진실 탐구형 AI”임을 강조
     * Palantir USA Commercial Customers
          + Palantir는 미국 내 상업 고객 수가 1년 만에 65% 증가해 432개에 도달
          + 자사 AI 플랫폼 AIP(Artificial Intelligence Platform)가 신규 고객 유치 및 기존 고객 확장에 기여
          + 2025년 연간 기준 미국 내 상업 매출이 10억 달러를 돌파
          + “AI Ontology를 통해 기업 내 맥락(context)을 극대화하고, 차별화된 실행력을 제공”하는 것이 Palantir의 경쟁력

AI Monetization Possibilities – Enterprise = Horizontal Platform &/Or Specialized Software?

     * 엔터프라이즈 AI 수익화 방향성
          + 기존 비즈니스 소프트웨어는 특정 산업·업무에 특화된 도구(Vertical SaaS) 로 성장해 왔음
          + Toast(레스토랑), Guidewire(보험), Veeva(생명과학) 등 각 산업별로 특화 솔루션이 시장을 선도함
          + 그러나 파운데이션 모델과 생성형 AI의 등장으로 다양한 분야에서 새로운 수익화 기회가 열리고 있음
     * 수평적 플랫폼의 부상
          + AI 네이티브 생산성, 검색, 커뮤니케이션, 지식 관리를 하나의 인터페이스에 통합하는 수평형 엔터프라이즈 플랫폼이 등장
          + 예시: Slack + Notion + ChatGPT가 결합된 형태로, 기존 개별 SaaS 대비 조직 전체 업무 맥락에 AI 인텔리전스를 내재화
          + SaaS 라이선스 판매에서 AI 내장 결과물 기반 요금 체계로 가치의 이동이 일어남
     * 수평 플랫폼 vs. 특화 소프트웨어의 경쟁
          + Microsoft는 Copilot을 전사적으로 통합하고, Zoom/Canva는 사용자 워크플로우에 생성형 AI를 접목
          + Databricks 등은 데이터 및 개발자 스택에 AI를 통합 중
          + 스타트업 Glean 등은 AI 퍼스트 워크플로우로 전통적 스위트 모델에 도전
          + 반면, 기존 특화 소프트웨어 벤더들도 AI 내장화, 워크플로우 자동화, 산업별 데이터로 맞춤화된 모델을 빠르게 도입하며 대응
          + 이들 특화 벤더는 이미 신뢰, 구조화된 데이터, 현장 워크플로우를 확보해 도메인 특화 AI 배포에서 우위를 가짐
     * 앞으로의 전망
          + 수평형 플랫폼은 다양한 기능 통합 및 전사 지식 연결에 강점
          + 특화 벤더는 산업별 규제, 계약, 고객 맥락에 맞춘 깊이 있는 AI 기능으로 차별화
          + 누가 핵심 레이어를 추상화하고, 사용자 인터페이스 및 업무 논리를 장악하는지가 관건
          + AI 시대의 수익화는 단순 사용량이 아니라, '관심(Attention)', '맥락(Context)', '통제권(Control)'을 따라 결정

SaaS 기득권자들(Incumbents)

     * Microsoft GitHub Copilot
          + 2022년 6월 공식 출시
          + 77,000개 이상의 기업에서 도입
          + 2년간 전년 대비 180% 성장
          + 1억 5천만 명 개발자 커뮤니티, 2년간 50% 증가
          + 연 매출 5억 달러 이상(분기별 기준)
     * Microsoft 365 Copilot
          + 2023년 3월 발표, 2023년 11월 기업 대상 정식 제공 시작
          + 출시 1분기 내에 기존 고객 다수가 좌석 수 10배 이상 확장
          + 분기별로 사용자가 두 배 이상 증가
          + 직원 사용률도 급증, 최근 60% 이상 증가
          + CIO의 75% 이상이 향후 12개월 내 도입 계획
     * Adobe Firefly
          + 2023년 3월 퍼블릭 베타, 2024년 2월 AI 비디오 모델 상용화
          + 브랜드와 크리에이터에게 높은 평가
          + 전체 Firefly 자산 생성 200억 건 돌파
          + 유료 사용자 90% 이상이 영상 생성 경험
          + Photoshop/Lightroom GenAI 월간 활성 사용자 각각 35%, 30% 기록
     * Atlassian Intelligence
          + 2023년 12월 베타, 2024년 12월 100만 MAU 돌파
          + 1년 만에 AI 기능 활용량 25배 증가
          + 고객의 10% 이상이 Atlassian Intelligence 도입
     * Zoom AI Companion
          + 2023년 9월 출시, 2024년 12월 350만 계정 도입
          + 분기별 활성 계정 수 68% 증가
          + AI Companion 2.0에서 메모리/추론/통합 등 고급 기능 제공
     * Canva Magic Studio
          + 2023년 10월 출시, 2025년 5월 누적 160억 회 AI 도구 사용
          + 크리에이티브/엔터프라이즈/비영리 전체 커뮤니티 활용
          + 100억 건 이상의 AI 도구 사용 기록
     * Salesforce Agentforce
          + 2024년 9월 발표, 2025년 2월 기준 3,000건 유료 계약 체결
          + 데이터 클라우드와 연동해 대규모 고객 경험 혁신
          + 연 120% 이상의 AI ARR 성장세 유지

OpenAI ChatGPT = Potential Horizontal Enterprise Platform?

     * Microsoft Office Suite
          + 9가지 애플리케이션(Outlook, Word, Excel, PowerPoint 등)으로 구성
          + 34년간 4억 명 이상의 유료 사용자를 확보 (1990~2024)
     * OpenAI ChatGPT
          + 단일 애플리케이션임에도 불구하고 단 2.5년 만에 2천만 명의 유료 사용자 달성 (2022년 11월~2025년 4월)
     * ChatGPT Enterprise 확장
          + 출시 9개월 만에 Fortune 500 기업의 80% 이상에서 팀 도입
          + 사용 기업들은 배포가 쉽고 안전한 방식임을 선호한다고 응답
          + 초기 사용 기업들은 내부 커뮤니케이션 개선, 코딩 작업 가속화, 복잡한 비즈니스 질문에 대한 신속한 답변, 크리에이티브 워크 지원 등에서 ChatGPT Enterprise를 적극 활용
          + ChatGPT Enterprise는 모든 사용량 제한이 없으며, 무료 버전 대비 최대 2배 빠른 성능 제공
          + 고급 데이터 분석(이전 Code Interpreter) 기능까지 무제한 접근 가능
          + 2023년 8월~2025년 2월 사이 기업/팀/교육 사용자 수가 200만 명으로 빠르게 증가

AI-Enabled Specialized Software @ Large Service Industries = Growing Very Quickly

     * 소프트웨어 엔지니어링
          + Cursor AI: 25개월 만에 ARR(연간 반복 매출) $1M → $300M 달성
          + Cursor는 AI 코드 에디터로, 코드 작성과 리팩터링, 자동화 등에서 혁신적인 사용자 경험 제공
          + 하루 10억 문자 이상 편집, $100M 이상의 반복 매출 기록
     * 제품 개발 (No-Code Product-Building)
          + Lovable: 5개월 만에 ARR 13배 성장, $50M 달성
          + 자연어로 제품 아이디어를 입력하면 프론트/백엔드 코드, DB 통합, 배포까지 자동 생성하는 AI 기반 노코드 플랫폼
          + 누구나 빠르게 제품을 만들고 비즈니스를 시작할 수 있게 지원
     * 헬스케어 (임상 대화)
          + Abridge: 5개월 만에 CARR(계약 반복 매출) $50M → $117M 성장
          + 25,000여 명의 의료진과 40개 병원, 600개 의료기관이 도입, 환자 방문 기록 요약에 1천만 건 이상 사용
          + 사용 의료진의 긍정적 피드백 다수
     * 법률 (워크플로우 자동화)
          + Harvey: 15개월 만에 ARR $10M → $70M 성장, 42개국 235개 고객 확보
          + 미국 상위 10대 로펌 대다수가 채택, 법률/전문 서비스 워크플로우 자동화 및 효율성 혁신 주도
     * 고객 지원 (AI Support Agents)
          + Decagon: 1년 만에 ARR 약 $1M → $10M 성장
          + AI 지원 에이전트가 반복 업무 자동화, 고객 지원 직무가 AI 매니저 역할로 전환
          + 2025년 추가 성장 전망
     * 금융 서비스 (리서치 및 분석)
          + AlphaSense: 2년 만에 ARR 약 $150M → $420M 성장
          + AI 기반 인사이트가 시장 표준으로 자리잡으며, 고도화된 시장 정보·워크플로우 솔루션 제공
          + 제품 혁신 및 기술 투자에 집중, 2025년 고속 성장 지속

AI Monetization Threats = Rising Competition + Open-Source Momentum + China’s Rise

  Rising Competition = AI Model Releases

     * 2017년 Google의 ‘Attention is All You Need’ 트랜스포머 논문 이후, LLM(대규모 언어 모델) 중심의 첫 번째 AI 혁신이 시작됨
          + OpenAI의 GPT-3, Meta의 Llama-1 등은 대규모 텍스트 예측 학습을 통해 범용적 추론 능력 가능성을 입증
     * 그러나 인간 커뮤니케이션은 텍스트에만 한정되지 않음
          + 이미지, 오디오, 비디오, 센서 데이터 등 다양한 신호가 실제 상황 맥락을 풍부하게 전달
          + Google, Anthropic, xAI 등 여러 기업이 언어모델을 멀티모달(다중 데이터 포맷 처리)로 확장
            멀티모달 AI 모델의 진화
          + 텍스트, 사진, 음성, 영상 정보를 하나의 벡터 공간에 통합해 이해 및 생성
          + 하나의 질의에 대해 단락+도표를 동시에 참고하고, 답변은 음성 요약이나 주석 이미지로 반환 가능
          + 시스템 전환 필요 없이 모든 데이터 포맷을 자유롭게 넘나드는 구조
     * 주요 단계별 진화 사례
          + 2021년 OpenAI CLIP: 시각+언어 통합
          + 2023년 Meta ImageBind, 2024년 Chameleon: 이미지·음성·영상 융합
          + 2024-2025년 GPT-4o, Claude 3, Chameleon: 완전한 멀티모달 프런티어 모델 등장
     * 실전에서의 효과
          + 현장 엔지니어가 스마트폰 카메라로 설비 이상 진단을 실시간 확인
          + 의료진이 X-ray 첨부와 동시에 구조화된 진료 리포트 초안 생성
          + 애널리스트가 차트, 녹취록, 오디오클립을 한 번에 쿼리하여 통합 분석
          + 텍스트 기반 모델 대비 컨텍스트 스위칭 감소, 더 풍부한 정보 포착, 비전·음성 중심 서비스 혁신

  Open-Source Model Momentum

     * AI 모델 개발은 초창기(2012-2018)에는 오픈소스 중심으로 진행
          + 학계와 협업 문화에 기반해 모델, 코드, 데이터가 공개됨
     * 2019년 이후 상업화, 보안, 경쟁심화로 폐쇄형(클로즈드소스) 모델 등장
          + GPT-2 공개 시점부터 주요 모델의 가중치·학습데이터 비공개 전환
          + OpenAI GPT-4, Anthropic Claude 등은 대규모 독점 데이터와 자본 투입, API 형태로 서비스
          + 성능, 사용성, 신뢰성에서 강점이 있어 대기업·정부·소비자에서 선호
          + 반면, 훈련 데이터·모델 구조·파인튜닝 방식에 대한 투명성 부족이 한계
     * 최근 오픈소스 모델이 다시 부상
          + 개발·사용 비용이 낮고, 접근성이 뛰어나 스타트업·개발자·학계에서 인기
          + Hugging Face 같은 플랫폼을 통해 Meta Llama, Mistral Mixtral 등 최신 모델을 쉽게 다운로드/활용 가능
          + AI 개발이 거대한 실험실에서 다시 개인·커뮤니티 기반 실험실로 확장
          + 빠른 실험, 협업, 커뮤니티 참여로 혁신 가속화
     * 중국은 오픈소스 대형 AI 모델 공개 수에서 2025년 기준 세계 1위
          + DeepSeek-R1, Alibaba Qwen-32B, Baidu Ernie 4.5 등 2025년 대형 모델 공개
     * 클로즈드소스 vs 오픈소스의 명확한 분화
          + 오픈소스: 주권 AI, 현지화 언어 모델, 커뮤니티 기반 혁신을 주도
          + 클로즈드소스: 소비자 시장과 대기업 도입에서 우위, 최적화와 사용성 중심
          + 개방성/속도/자유 대 보안/최적화/통제라는 두 패러다임이 경쟁하며 AI의 미래를 좌우

  Rising Performance of Open-Source Models + Falling Token Costs = Explosion of Usage by Developers Using AI

     * 초기에는 GPT-4, Claude, Gemini 등 클로즈드소스 모델이 소비자 및 대기업 시장을 주도
          + 손쉬운 온보딩, 깔끔한 UI/UX, 높은 신뢰성으로 대중 인식과 기업 도입에서 강점
          + 비기술 인력도 쉽게 쓸 수 있는 보안과 편의성, 네임밸류 제공
     * 최근 오픈소스 모델의 성능 격차가 빠르게 좁혀짐
          + Llama 3, DeepSeek 등은 추론력, 코딩, 다국어 등에서 클로즈드 모델과 경쟁 가능한 수준에 도달
          + 자유롭게 다운로드·파인튜닝·로컬 배포 가능, 비용은 훨씬 저렴함
     * 개발자 중심으로 오픈소스 AI 모델 채택 가속화
          + 개발자는 완성도 높은 UX보다 커스터마이즈와 저비용/고성능을 선호
          + 앱, 에이전트, 파이프라인 등 다양한 영역에서 오픈소스 모델 기반 혁신 실험이 활발
     * 토큰 가격 하락과 오픈소스 모델 성능 향상에 힘입어 AI 활용 개발자 수 폭발적 증가
          + 기존에는 폐쇄형 API 의존도가 높았지만, 이제는 로컬·클라우드 모두에서 직접 구축/확장 가능
     * 아직 소비자/대기업 대중화에는 한계
          + 오픈소스는 브랜드 파워, 사용자 친화적 UX, 관리형 서비스 등에서 약점
          + 하지만 인프라가 더 편리해지고 비용/성능 격차가 유지된다면, 대중 시장으로도 확산 가능성

  China’s Rise

     * Meta CTO Andrew Bosworth는 현재 AI를 “우주 경쟁(space race)”에 비유하며, 특히 중국의 역량을 매우 높이 평가함
          + 과거 우주경쟁이 체제 경쟁의 성격(혁신 속도·글로벌 신뢰도)을 지녔던 것처럼, AI 경쟁도 세계 질서에 영향을 줄 수 있음
     * 중국은 ‘중국제조 2025’(Made in China 2025) 정책을 계기로 저가 제조에서 첨단 기술 주도국으로 급속 전환
          + 로봇, 전기화, 정보기술, 그리고 세계적 수준의 인공지능을 중심으로 전략적 산업에서 빠르게 역량 강화
     * 중국 AI의 군사·국가 전략 적용
          + 전장 물류, 목표 인식, 사이버 작전, 자율적 의사결정 플랫폼 등 국가 안보 전반에 AI 활용이 확대
          + 2025년 국영 언론에서는 군 지원(비전투) 분야, 예: 군 병원 등에도 AI 적용을 강조
          + 과학기술부는 ‘자주적 혁신(indigenous innovation)’을 국가 핵심 과제로 명확히 제시
     * 중국 AI 우위의 글로벌 영향
          + OpenAI Sam Altman은 2024년 기고문에서 “권위주의 정권이 AI에서 우위를 차지할 경우, 미국 및 타국 기업에 데이터 공유를 강요하고, 자국민 감시나 사이버 무기 개발에 AI를 활용할 수 있다”고 경고
     * 미·중 기술 패권 경쟁의 심화
          + AI 뿐만 아니라 희토류, 반도체, 첨단 기술 부문에서 통제권 경쟁이 확대
          + 중국은 희토류(첨단 전자·국방·청정에너지의 핵심 소재) 공급의 글로벌 강자 지위 유지, 미국은 이를 견제하기 위해 반도체 리쇼어링(국내생산 회귀) 및 동맹국(일본, 한국, 네덜란드 등)과 협력 강화
          + 대만 TSMC가 세계 반도체 파운드리의 핵심축으로, 미·중 양국의 전략적 계산의 중심에 위치
     * 미국 내 정책 기조의 변화
          + 20년간의 미온적 대응에서 벗어나, 양당 모두가 첨단기술 산업을 ‘국익의 핵심’으로 적극 인식
          + 바이든 행정부: 수출 통제, 트럼프 행정부: 경제적 내셔널리즘 및 리쇼어링 등 다양한 접근법
          + 상원 John Cornyn, Mark Warner: “미국의 반도체 혁신이 경제 전체를 지탱해왔으나, 안일함이 경쟁국(적국 포함)에 따라잡힐 빌미를 제공”
     * 미국 기술 지적재산권(IP) 보호의 중요성
          + OpenAI: “중국 등 경쟁국들이 미국 선도 AI 모델을 역공학하려 지속적으로 시도 중이며, 정부와의 긴밀한 협력이 필수적”이라고 언급
     * 미·중 관계의 시각 변화
          + WTO 가입 초반(2000년대)과 달리, 지금의 미국은 AI, 반도체, 핵심 광물 등 첨단기술이 경제·산업 자산을 넘어 국가 회복력과 지정학적 힘의 핵심 축임을 분명히 인식

  Public Market Capitalization Leader Tells of Last Thirty Years = Extraordinary USA Momentum…China Rising

     * 지난 30년(1995~2025)간 글로벌 시가총액 상위 30개 기업 중 지속적으로 남아있는 기업은 6개
          + Microsoft, Walmart, Exxon Mobil, Procter & Gamble, Johnson & Johnson, Coca-Cola
          + 새롭게 상위에 진입한 기업
               o NVIDIA, Apple, Amazon, Alphabet(Google), Saudi Aramco, Meta Platforms(Facebook), Tesla, Broadcom, Berkshire Hathaway, TSMC, JP Morgan Chase, Visa, Eli Lilly, Tencent, Mastercard, Netflix, Costco Wholesale, Oracle, Home Depot, SAP, Bank of America, ICBC, AbbVie, Palantir
          + 1995년 국가별 비중
               o 미국: 53% (30대 기업 중 16개),
               o 일본: 9개,
               o 스위스: 3개,
               o 영국: 2개
          + 2025년 국가별 비중
               o 미국: 83% (30대 기업 중 25개)
               o 일본/스위스/영국: 0개
               o 중국 2개, 사우디아라비아 1개, 대만 1개, 독일 1개
     * 지난 30년간 글로벌 기술 기업(tech companies) 시가총액 상위 30개 중 지속적으로 남아있는 기업은 5개
          + Microsoft, Oracle, Cisco, IBM, AT&T
          + 기술 기업 신규 진입자
               o NVIDIA, Apple, Amazon, Alphabet(Google), Meta Platforms(Facebook), Tesla, Broadcom, TSMC, Tencent, Netflix, SAP, Palantir, ASML, Alibaba, Salesforce, T-Mobile, Samsung, China Mobile, Reliance, ServiceNow, Intuitive Surgical, Siemens, Uber, AMD, Intuit
          + 1995년 기술 기업 국가별 비중
               o 미국: 53% (16/30),
               o 일본: 30% (9/30),
               o UK/싱가포르/홍콩/멕시코/말레이시아 각 1개
          + 2025년 기술 기업 국가별 비중
               o 미국: 70% (21/30)
               o 일본/UK/싱가포르/홍콩/멕시코/말레이시아: 0개
               o 중국 3개, 독일 2개, 대만 1개, 네덜란드 1개, 한국 1개, 인도 1개
          + 대만 TSMC: 대만은 상위에 1개사(TSMC)만 있지만, 2024년 2분기 기준
               o 세계 첨단 반도체의 80~90%, 전체 반도체의 62% 이상 생산
     * 한 세대 만에 이뤄진 엄청난 변화
          + 인터넷 보급이 신규 글로벌 상위 기업 탄생의 기반이 되었으며,
          + AI 부상은 앞으로 30년 동안 이보다 더 빠르고 근본적인 변화를 촉진할 것으로 전망

USA vs. China in Technology = China’s AI Response Time Significantly Faster vs. Internet 1995

     * AI 대형 언어 모델(LLM) 리더십: 미국과 중국이 글로벌 AI 개발 속도를 주도
          + 2017~2024년 누적 대규모 AI 시스템 구축 현황을 보면, 미국과 중국이 독보적으로 앞서고 있음
          + 미국은 2024년 기준 150건 이상, 중국도 100건이 넘는 대규모 AI 시스템을 발표
          + 프랑스, 영국, 캐나다, 홍콩, 독일 등은 아직 미국·중국과 큰 격차를 보임
     * China AI = 빠른 추격, DeepSeek R1
          + DeepSeek는 중국의 AI 개발력이 미국과의 격차를 3개월까지 좁혔다고 발표(2025년 1월)
          + DeepSeek CEO는 중국이 단순 모방을 넘어서 독자적 혁신이 불가피하다고 강조
     * Alibaba Qwen 2.5-Max: DeepSeek·OpenAI ChatGPT 능가 주장
          + Qwen2.5-Max는 다양한 벤치마크에서 DeepSeek V3, OpenAI ChatGPT보다 뛰어난 성능을 시연
          + 데이터 스케일과 모델 사이즈, 포스트 트레이닝 기술 혁신을 통해 모델 성능을 지속적으로 향상 중
     * Baidu Ernie 4.5 Turbo: 다중모달 AI, 저렴한 비용·고성능
          + 텍스트·이미지·비디오 모두 처리하는 멀티모달 AI로 ‘스위스 군용 칼’에 비유
          + 입력 100만 토큰당 RMB 0.8, 출력 RMB 3.2로 DeepSeek V3의 40%, GPT-4.5의 0.2% 수준의 비용
          + GPT-4.1과 동등, GPT-4o보다 일부 멀티모달 작업에서 더 뛰어난 성능을 시현
     * LLM 성능: 미국과 중국, 실제 점수 격차 좁혀져
          + Stanford HAI & LMSYS의 2025년 2월 기준 Chatbot Arena 결과, 미국 1,385점, 중국 1,362점으로 근접
     * 중국 AI: 낮은 훈련 비용으로 성능 달성
          + Epoch AI 자료에 따르면 DeepSeek V3 등 중국 모델은 GPT-4 대비 현저히 낮은 비용으로 출시
     * 현지 반도체로 AI 훈련 전환
          + 미국의 수출 제한으로 화웨이 등 현지 AI 칩 클러스터가 빠르게 확대되고 있음
          + Financial Times에 따르면 화웨이는 중국 테크 기업용 AI 클러스터를 본격적으로 공급
     * 중국: 산업용 로봇 설치 기반도 세계 최고 수준
          + 2023년 기준 중국은 27만 6천대, 기타 세계는 26만 5천대, 미국은 4만 대 내외로 차이
     * 결론: 중국의 AI 혁신 속도는 인터넷 도입기(1995년)보다 훨씬 빠름
          + 미국과의 격차를 기술, 비용, 인프라 측면에서 빠르게 좁히고 있으며, 글로벌 산업 주도권 경쟁이 한층 치열해지고 있음

China Consumer AI Usage = DeepSeek Rose Quickly

     * 글로벌 생성 AI 시장은 지역별, 채널별, 사용자 선호도에 따라 점점 분화되는 추세
          + 전 세계적으로 OpenAI의 ChatGPT가 데스크탑·모바일 모두에서 명확한 1위를 차지하고 있지만, 플랫폼별 경쟁이 심화되는 중
          + Anthropic의 Claude, Google Gemini도 점차 점유율을 확대하고 있으며, xAI의 Grok은 2025년 2~3월 기준 294% 월간 방문자 증가로 가장 빠르게 성장하는 AI 어시스턴트로 기록
     * 중국은 DeepSeek 등 현지 AI 모델이 강세
          + ChatGPT는 대부분 국가에서 1위지만, 러시아·중국에서는 서비스 불가로 DeepSeek 등 현지 모델이 대세
          + Roland Berger 컨설팅에 따르면, 중국 내 월간 활성 사용자 기준 AI 앱 TOP10 모두 국산 (DeepSeek, Kimi, Nami AI, ERNIE Bot 등 수천만 명 사용자)
          + 중국을 제외한 전 세계는 ChatGPT가 압도적, 중국 내에선 완전히 별도의 시장 형성
     * 중국의 플랫폼 규제와 환경
          + Facebook, Twitter, Google, YouTube는 2010년 또는 그 이전부터 중국 내 접근 불가
          + Instagram, WhatsApp, Wikipedia, Telegram, Spotify뿐만 아니라, 최근에는 ChatGPT, Google Gemini, Claude, Meta AI, Microsoft Copilot 등도 차단
          + 이러한 규제 환경이 로컬 AI 챔피언의 부상을 촉진
     * AI에 대한 인식 차이
          + Stanford HAI와 Ipsos 조사 결과, 중국 시민 83%가 AI의 순효과를 긍정적으로 평가 (2024년 기준, 2022년 대비 5%p 증가)
          + 미국 시민은 같은 질문에 39%만 긍정 응답, 2년간 큰 변화 없음
          + 사회·철학적 관점에서 AI에 대한 접근과 수용 방식이 국가마다 다름
     * 플랫폼 선택은 성능이나 가격을 넘어, 점차 국가적·문화적 정체성의 영역까지 확장
          + 단순히 ‘누가 잘 만드나’가 아니라, ‘어떻게 받아들이고 사용하는가’가 더 중요한 분기점으로 작용

AI & Physical World Ramps = Fast + Data-Driven

     * 지금까지는 데스크톱/모바일 소프트웨어의 AI 확산과 수익화에 집중해왔지만, 실제 물리 세계에서의 AI 혁신과 수익화는 그보다 더 빠르고 극적인 양상을 보임
          + 이제 지능은 디지털 애플리케이션뿐 아니라 차량, 기계, 방위 시스템에도 깊이 내재되고 있음
          + Waymo, Tesla와 같은 자율주행차량 플릿은 더 이상 실험실 프로젝트가 아니라, 실제로 수익을 내고 수백만 마일의 무인 주행 데이터를 쌓으며 빠르게 고도화된 소프트웨어 루프로 진화 중임
          + Applied Intuition은 하드웨어에 구애받지 않는 소프트웨어 정의 차량 시스템과 시뮬레이션 플랫폼을 개발, 제조사가 인공지능을 부품처럼 손쉽게 적용할 수 있도록 지원함
          + 방위산업(Anduril) 에서는 AI가 모든 엣지 노드(드론, 센서 등)에 탑재된 자율 시스템을 출하하며 방위의 패러다임을 전환하고 있음
          + 농업(Carbon Robotics) 분야에서는 AI 기반 컴퓨터 비전으로 제초제를 사용하지 않고 잡초를 제거하는 등, 물리 세계의 자본 자산이 곧 소프트웨어 엔드포인트가 되는 대전환을 맞이하고 있음
          + AI가 더 이상 화면에만 머물지 않고 현실을 움직이는 힘(kinetic)이 되어가고 있음을 시사
     * Tesla Vertically-Integrated Electric Vehicles
          + 2022년 6월부터 2025년 3월까지 완전 자율주행(Full Self-Driving) 누적 주행 거리 100배 성장
          + 버전 12 도입으로 C++ 코드 33만 줄을 뉴럴넷으로 대체하며, 완전한 end-to-end AI 아키텍처 적용
          + AI를 통한 객체 인식, 경로 계획, 차량 제어 등 모든 단계에서 AI가 중심 역할을 수행
          + 테슬라가 세계에서 가장 효율적인 AI 추론 기업일 가능성이 있음음
     * Waymo Fully-Autonomous Vehicles
          + 2023년 8월~2025년 4월, 샌프란시스코 라이드셰어 시장 점유율 0%에서 27%로 급성장
          + 다중모달 AI 기반의 perception, planning, prediction으로 강인한 상용 시스템 구축
          + 실제 시장에서 상업적으로 통하는 자율주행차 제품을 입증
     * Applied Intuition Vehicle Intelligence
          + 2024년 기준 글로벌 주요 자동차 OEM 18곳에 AI 차량 지능 솔루션 제공
          + 자동차, 트럭, 건설, 방위 분야까지 다양한 산업에서 시뮬레이션 플랫폼과 자율주행 소프트웨어 확장
          + 국방 분야에서는 오프로드 자율주행 및 디펜스 기술 제품군 강화
     * Anduril AI-Enabled Autonomous USA Defense Systems
          + 2022~2024년 2년 연속 연매출 2배 성장, 2024년 10억 달러 돌파
          + AI 및 자율 시스템을 활용해 현대 전장 환경에서 더 빠르고 정밀한 의사결정 지원
          + 분산된 각 엣지 노드에 AI를 배치하여 보안 및 방어 시스템 혁신
     * KoBold Metals AI-Driven Mining Exploration
          + AI 기반 머신 프로스펙터 기술로 1975년 이후 최저 효율을 보였던 광물 탐사 분야에서 탐사 효율 극적으로 개선
          + 대규모 지리/지구물리 데이터와 통계적 연관 모델을 결합, 유망한 탐사 후보를 빠르게 식별
          + 업계 평균의 2배 이상 효율로 새로운 금속 공급망 확보
     * Carbon Robotics AI-Driven Agricultural Modernization
          + 2023년 1월~2025년 5월 누적 23만 에이커 이상 제초, 글리포세이트(제초제) 10만 갤런 이상 사용 예방
          + AI 기반 딥러닝·컴퓨터 비전 기술로 농작물 주변 잡초를 레이저로 제거
          + 자동화 기계가 시간당 2에이커, 하루 20만 개 잡초 제거 가능
     * Halter AI-Driven Intelligent Grazing
          + 2025년 기준, 목장용 스마트 목걸이 신규 계약 건수 연평균 150% 이상 성장
          + AI 기반 방목 관리로 자원 사용 효율화, 토양 건강 개선, 지속가능성 강화
          + 현대 농업에 기술 적용이 더딘 현실을 타개할 대규모 생산성 및 탄소 저감 효과 입증

Global Internet User Ramps Powered by AI from Get-Go = Growth We Have Not Seen Likes of Before

     * 저비용 위성 인터넷의 확산으로 전 세계 인구의 32%에 해당하는 26억 명의 비연결 인구가 온라인에 새롭게 유입될 가능성이 빠르게 커지고 있음
     * 이들은 기존과 달리 최초 인터넷 경험부터 AI 기능을 기본 탑재한 상태에서 시작하게 될 것임
     * 검색창에 직접 입력하거나 전통적 브라우저를 거치지 않고, 곧바로 자연어로 AI와 대화하며 정보를 얻고 다양한 기술 서비스를 활용할 전망임
     * 이런 AI 에이전트 기반 인터페이스는 앱이 아닌 인터페이스를 소유한 플레이어에게 시장 가치를 집중시키며, 기존 플랫폼의 위계를 흔들 수 있음
     * 앞으로는 플랫폼 소유보다 인터페이스 소유가 더욱 중요해질 것이며, 현지 언어와 맥락, 사용자 의도를 이해하는 AI가 핵심 경쟁력이 될 전망임

New Internet User Growth = Enabled by AI + Satellites

     * Orbital / Satellite Launch Market Share, Global = SpaceX Rising
          + 2008년 이후 상업적 및 국가적 우주 발사 르네상스가 시작되며, SpaceX가 연간 발사 횟수에서 큰 비중을 차지함
          + 미국(비 SpaceX), 중국, 러시아도 각각의 성장세를 보이나, SpaceX의 급격한 증가가 두드러짐
          + Cold War~1990년대 후반까지는 국가주도, 최근에는 민간 주도 발사 증가 추세임
     * SpaceX Starlink @ 5MM+ Subscribers = +202% Annual Growth Over 3.2 Years
          + Starlink는 2021년 약 10만 명에서 2024년 500만 명 이상으로 구독자가 증가함
          + 연평균 202%의 성장률을 기록하며 빠르게 글로벌 인터넷 사용자 기반을 확장함
     * SpaceX Starlink Ecosystem = Coverage Expanding Globally
          + 2025년 기준, Starlink는 북미, 남미, 유럽, 오세아니아, 아프리카 일부, 아시아 주요 지역까지 서비스 지역 확대
          + 아직 서비스되지 않는 국가는 중국, 러시아, 이란 등 제한적임
     * Starlink = Unlocking Previously-Inaccessible Internet Access in AI Era
          + Coco, Monterrey, Mexico: 멕시코 농촌 지역에 고속·신뢰성 인터넷 제공, 지역사회 WiFi로 디지털 접근성 확대
          + Chile School District: 칠레 학교 36개 컴퓨터 모두 동시 접속 가능한 고속 인터넷 제공, 학생 및 교사 모두 수업환경 혁신 경험
          + Brightline Trains, USA: 미국 고속철도에서 안정적인 위성 인터넷 제공, 승객 및 운영 효율 향상
          + Seaspan Corporation, Global: 글로벌 해운사에 위성 인터넷 도입, 선박 내 원격 사무실화, 승무원 안전 및 운영 효율 개선, 기존엔 불가능했던 솔루션 실현

AI & Work Evolution = Real + Rapid

     * AI의 도입으로 인한 일의 본질적 변화
          + 로봇, 드론 등 물리적 자동화와 더불어 인지적 자동화가 빠르게 확산 중임
          + AI 시스템이 추론, 창작, 문제 해결 역량을 갖추며 업무 영역 확장
     * AI의 인지 능력 성장 속도
          + ChatGPT 공개(2022년 11월) 이후 3년간, 고등학생 수준에서 박사급 추론 능력으로 발전
          + 방대한 구조적 데이터 기반의 규칙·판단 직무가 AI의 강점으로 이동
     * 노동의 단위 변화
          + 기존 인간 중심 노동에서 데이터센터와 AI 모델의 연산력 중심으로 전환 가능성
          + 특정 노동력의 공급과 품질을 AI 인프라가 결정하는 시대가 도래 중임
     * 에이전트 기반 미래와 인간의 역할
          + 일부에서는 AI 에이전트가 화이트칼라 직업을 대체할 것으로 전망
          + 그러나, 생산성·효율 향상과 함께 새로운 인간 직무가 등장해온 역사적 패턴 고려 필요
          + 완전한 에이전트 중심 사회에서도 감독·훈련·지도 등 인간의 역할은 남게 됨
     * 미래의 노동 구조 및 사회적 변화
          + RLHF(인간 피드백 기반 강화학습)처럼, 인간이 AI·로봇의 성능을 훈련·미세조정하는 역할로 이동
          + 역사적으로도 업무 방식 변화는 늘 반복됐으며, AI 역시 생산성 및 일의 진화를 촉진하는 기술임

Summary

     * 인터넷 없는 삶이 상상하기 어려운 것처럼, 앞으로는 AI 없는 세상 역시 상상할 수 없게 될 전망임
          + AI는 고객지원, 소프트웨어 개발, 과학, 교육, 제조 등 산업 전반의 핵심 인프라로 빠르게 자리잡는 중
     * AI 대중화의 가속 요인
          + ChatGPT 등 멀티모달 AI 도구의 글로벌 확산, 추론 비용 하락, 다양한 모델 출시가 촉진
          + 솔로 개발자부터 대기업까지 접근과 실험이 쉬워져 혁신 확산이 가속됨
     * 기술 인프라 및 투자
          + 대형 클라우드, 반도체, 하이퍼스케일러의 설비 투자 급증
          + 칩·데이터센터·네트워크·에너지 시스템 등 물리·디지털 경계가 점점 모호해지는 추세
     * 미·중 전략 경쟁과 글로벌 AI 주도권
          + 미국이 모델 혁신, 커스텀 칩, 클라우드 인프라에서 선두이나, 중국도 오픈소스와 인프라, 정책 지원으로 급속 성장
          + 양국 모두 AI를 경제적 성장과 지정학적 영향력의 핵심 지렛대로 인식함
     * 플랫폼·인터페이스 변화
          + AI가 기존 앱 생태계를 넘어서 에이전트 기반 대화형 인터페이스로 진화
          + 위성 인터넷 등 덕분에 앞으로 신규 인터넷 사용자는 AI 네이티브 경험부터 시작할 가능성 높음
     * 일자리와 작업방식의 변화
          + 일터에서 AI 도입 가속, 노동 단위가 인간에서 연산력·AI로 점진적 전환 중
          + 더 많은 사람들이 AI와 함께 일하고, AI가 업무 환경을 재편성하는 흐름이 뚜렷
     * 국제 정세와 기술의 결합
          + 정보·자본의 흐름, 무기화되는 기술, 민주주의-권위주의 대립 심화로 불확실성 고조
          + 그러나 혁신은 국가 경쟁력의 핵심으로, 빠른 실행과 동맹 전략이 중요하다는 지적
     * 결론
          + AI의 게임 시간이 왔고, 그것은 점점 더 강렬해지고 있음
          + Genie(지니)는 다시 병속으로 들어가지 않을 것 (다시 돌아갈 수 없는 지점을 넘어섰음)

   PDF 파일로도 공개되어 있습니다 https://www.bondcap.com/report/pdf/Trends_Artificial_Intelligence.pdf
"
"https://news.hada.io/topic?id=21239","AI 답변에는 실수가 포함될 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI 답변에는 실수가 포함될 수 있음

     * AI 검색 요약이 항상 정확하지 않음
     * PS/2 Model 280에 대한 다양한 정보가 반복 조회 시마다 다르게 제공됨
     * 존재하지 않는 모델 번호도 그럴듯하게 설명하는 AI의 환각 문제 발생
     * 옳은 답변이 나오는 확률은 매우 낮은 수준임
     * 비전문가는 잘못된 정보를 쉽게 진실로 오해할 위험성이 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI 검색 요약의 신뢰성 문제 경험

  IBM PS/2 모델 검색 시도

     * 1992년 출시된 PS/2 Server 시스템의 특정 모델을 찾으려 Google에 검색 진행
     * 검색 결과로 나온 정보는 찾던 기계와 일치하지 않았으며, 원래의 모델은 486 프로세서(복수형) 와 Microchannel(MCA) 사용 특성이 있었음

  반복된 결과와 답변의 불일치

     * 동일 쿼리를 재실행했음에도 AI 요약 결과가 매번 다르게 나타남
     * 예를 들어, 계속해서 PS/2 Model 280이 286 기반의 ISA 시스템이라는 주장을 반복함
     * 각각의 답변에서 RAM 용량 및 사양 정보마저 바뀌어서 일관성 없는 데이터 제시 현상 확인

  존재하지 않는 모델에 대한 환각적 설명

     * 여러 번 질의한 결과, 286 시스템이 128MB까지 확장 가능하다는 주장 등, 기술적으로 불가능한 정보도 생성됨
     * PS/2 Model 280이 IBM PC 라인업의 주요 발전이었다는 설명까지 추가적으로 등장
     * 실제로는 PS/2 Model 280 자체가 존재하지 않음에도, AI가 근거 없는 설명을 매우 그럴듯하게 제공함

  올바른 답변의 낮은 빈도

     * 여러 차례 쿼리 시도를 거친 후에야 간헐적으로 “Model 280은 실제 PS/2 시리즈 내 존재하지 않는다”는 올바른 답이 나옴
     * 정확한 답변이 나타나는 비율은 매우 낮고 대부분의 경우, AI는 근거 없는 정보를 창조함
     * 환각된 답변은 정보로서 가치가 없으며 오히려 잘못된 확신을 제공함

  AI 검색 요약의 맹신 경계

     * AI 기반 인터넷 검색은 비전문가에게 매우 그럴듯하게 보일 수 있음
     * 전문가라면 금세 실수를 간파하겠지만, 정보 확인 능력이 부족한 사용자 입장에서는 허위 정보에 쉽게 현혹됨
     * AI가 “실수를 할 수 있다”는 경고는 결코 가벼이 흘려들을 사안이 아니며, 신뢰할 수 있는 사실 확인 과정 없이 AI 답변에 의존하는 것은 위험함
     * 설득력 있게 들린다고 해서 실제 사실에 기반하는 것은 아님을 강조
     * AI 기반 요약이나 검색 결과에 대해 항상 의심과 사실 확인의 필요성 상기

   LLM에게는 요약만 시키는 것이 좋은 것 같아요. 데이터의 출처를 찾고 검증하는 과정이 반드시 필요합니다.

        Hacker News 의견

     * Google Gemini의 검색 결과가 질문에 맞는 것처럼 보이게 아무렇게나 답변을 만들어내는 confabulation 특성 언급, 맥락과 정확성에 신경 쓰지 않는다는 점 지적, 결과를 예상하고 있는 경우에만 기억 보조로 쓸 수 있지만 그렇지 않으면 전혀 신뢰하지 못하겠다는 경험담 공유, Google Veo 결과도 구멍이 매우 많음, AI 결과에 논리나 추론이 없는 것이 뻔히 드러난다는 이야기, Veo가 엉뚱한 결과를 내놓는 예시와 Tesla FSD가 이상 행동을 하는 사고 사례 기사 링크 공유
       [AI 비디오 리얼리즘] (https://arstechnica.com/ai/2025/05/ai-video-just-took-a-star...)
       [Tesla FSD 사고 뉴스] (https://electrek.co/2025/05/23/tesla-full-self-driving-veers...)
          + 이런 품질의 결과가 어느새 '정상', '수용 가능'한 것으로 받아들여지는 분위기 형성, 아무도 별로 문제 삼지 않는 현실이 굉장히 걱정스러운 포인트, 예전 같으면 절대 용납불가였을 텐데 왜 지금은 부정확한 결과가 점점 받아들여지는지 의문
          + 자동차 관련 기능을 구글링했는데 기존 Google 검색은 이런 쿼리를 정말 잘 처리했었으나, 이제는 90%가 틀린 연식, 모델, 브랜드 정보가 뒤섞인 AI 결과물로 도배된다는 경험담, 유일하게 약간 도움이 된 건 유튜브 하나였고, 페이지 맨 아래 구석에 예전 car forum에 있던 정답이 있었기에 CamaroZ28.com에 감사 전함
          + 이런 상황이 그 어떤 기술보다 당황스러운 현상, Google이 자신의 핵심 비즈니스를 이처럼 심각하게 결함 있는 기술의 방향 전환에 베팅하고 있다는 점이 이해가 안 됨, Ben Evans처럼 '더 좋아질 것'이라는 약속도 빈말에 불과하다고 생각, 실제로 어제 독일에서 열린 추모 이벤트를 검색했더니 AI Overview가 이미 죽은 이탈리아 뮤지션의 이름을 가져다가 그 공연장이 그 뮤지션의 최고의 작품이라고 지어내버린 사례 공유, ChatGPT에 그 답변을 붙여넣으니 AI Overview의 오류를 조롱하는 사근적이고 날카로운 답변까지 받아서 웃긴 경험
          + AI가 표면적으로는 역대급으로 똑똑한 척 하는데 정작 내부 논리나 추론은 따라가지 못해 기묘한 '언캐니 밸리'로 들어선 느낌
          + 솔직히 LLM을 검색 대체로 쓰는 사람들이 어떻게 사용하는지 이해가 잘 안됨, 챗봇들은 항상 내가 원하는 정보와 인접한 데이터(예: 소스를 물으면 인용문만 주는 식)만 주는 수준, 혹시 내가 검색을 잘못 쓴 건지 의문
     * LLM의 한계와 확률론적 특성을 아는 입장이지만, 주변 가족이나 친구가 LLM을 신뢰해서 부적합한 업무에 쓰는 모습을 보면 나만 AI 회의론자로 여긴다고 토로, 이들은 AI로 숫자 나누기(예: 더치페이) 같은 것도 시키고, LLM의 결과를 무조건 신뢰하는 문제 있음
          + 고기술로 저기술 문제를 푼다는 고전적 사례, 단순한 수 계산도 꼭 기계한테 맡기는 걸 야유
          + 일상 용도에서는 결과가 '적당히' 맞는 경우가 많아서, 사람들이 습관적으로 의존하게 되는 점이 tricky한 부분
          + 단순 계산을 LLM에 시키는 게 사실 꽤 웃긴 일, 변수에 Python 써주게 시키면 좋겠다는 농담도 해봄
          + LLM 사용 자체가 (실내 흡연처럼) 주변에도 피해를 주는 일이라는 비유
          + 'AI로 계산시키고, 정보 찾아달라고 말하면 그 결과를 100% 신뢰한다'는 현상에 대해, 사실 이 정도 단순 기계적 용도는 현 시점 챗봇들이 다 맞혀주지 않냐는 의견, 다양한 기능을 한 번에 처리할 수 있는데 굳이 상황별로 앱 전환할 필요 있냐는 의문, 결국 사용성(Usability)이 가장 강력한 동인
     * ""AI 답변에는 오류가 있을 수 있다""라는 간단한 문구나 ChatGPT 하단의 경고가 이미 부족한 수준이라는 지적, LLM의 환각(hallucination)을 수년간 경고해도 사람들은 계속 실수하고 있다며, LLM 제공자는 더 공격적으로 사용자에게 한계성을 교육해야 한다는 주장, 사용자 경험에 불편이 생겨도 필수적이라 생각
          + 이러한 논의에서 더 할 수 있는 건 모델 제공자에게 책임을 지우거나, 현재의 제한된 사전 고지 체계를 유지하는 것뿐이라는 생각, 이미 AI 모형, 클라우드 서비스들은 다층적으로 필터링과 검열이 존재, 이보다 더한 마찰이란 결국 팝업 추가 같은 소소한 것에 불과, 책임을 모델 제공자에게 묻기 시작하면 그 순간 공개형 모델 사업 자체가 불가능, 기업끼리 따로 라이센싱 계약하여 쓸 뿐 일반 대중에게 API 공개 자체가 불가능, 향후 분위기 변화를 통해 제한이 조금씩 풀릴 가능성 정도만 예상
          + '사용자 교육이 더 효과적으로 이뤄져야 한다'라는 주장에 대해, 결국 경험에서 배우는 수밖에 없는 '직접 당해야 실감하는' 문제라는 의견, 어떤 경고문도 실제 피해를 주는 일만큼 효과적일 수 없다는 현실론
          + LLM은 본질적으로 인간 지식 노동 대체라는 명분이 있기 때문에 제공자가 적극적으로 한계성을 강조할 수 없다는 시각, Anthropic CEO처럼 대규모 실직이 불가피하다고 수차례 언급된 것과 모순 제기
          + 예전 Apple 지도서비스, Google 지도도 잘못된 안내로 PR 위기 관리를 했던 시절 언급, 지금은 경고문만 붙여놓으면 아무 문제 없다는 식의 분위기, 신기술이 너무 많은 관용을 받는 현실에 실망감 전달
          + '경고문은 페이지 맨 위, 붉은 글씨로 크게 노출해야 한다'고 강조
     * 언어모델은 지식을 '알기' 위해 설계된 것이 아니라 '말하기' 위해 만들어진 것임을 설명, 그래서 'knowledge model'이 아닌 'language model'이라 부름, 이미 생성된 단어 뒤에 어떤 단어가 올지 확률적으로 이어붙이는 것뿐임, 매번 다른 결과를 내놓는 이유는 내부적으로 의사 난수 발생기로 다음 단어를 뽑는 확률 분포가 존재하기 때문, 온도(temperature)를 0으로 하면 무작위성이 사라지고 항상 가장 확률 높은 단어만 택함, 이때 결과물은 매우 지루함, IBM, PS/2, 80286, 80486 등의 사안에 대해 사실 아는 게 아니라, 그저 단어 시퀀싱만 할 뿐임
          + 온도를 0으로 해도 로컬 모델에선 충분히 잘 동작한다는 경험, 클라우드 기반 UI에서 0을 막은 건 모델이 무한 반복 루프에 빠지는 버그를 대중이 목격하는 걸 막으려는 목적 때문
          + 언어모델이 '지식'을 제공하는 게 아니라 말만 만들어낸다는 사실 자체엔 동의하지만, Google을 쓰는 사람 입장에선 대화하려고 간 게 아니라 실제 '지식'을 얻고자 이용함, Google이 신뢰성 있는 지식 제공을 단순 '단어 생성'으로 대체하려 든 건 본질적 오류라고 생각, 하지만 광고수익이 목적이기 때문에 실제론 별 상관없을 수도 있음
     * Google 검색 사이트에서 조차 'AI 답변엔 오류가 있을 수 있음' 경고문이 '더보기' 버튼 아래 숨겨져 있다는 점 꼬집음, OpenAI ChatGPT가 출시됐을 때, 비전공 교수에게 지금의 AI는 '진짜 AI'가 아니라 연산 기반 말장난(parlor trick)에 가깝다고 설명한 경험 있음, 하지만 이런 '말장난'이 과제를 베끼는 데엔 놀라울 정도로 효과적, 전체적으로 과제뿐 아니라 여러 가지 일에서 품질이나 저작권에 신경 쓰지 않는다면 '치팅'에 아주 좋은 도구라는 인상
          + '겉으로만 코드 짤 수 있는 것처럼 보이고, 사실 실제론 못 짠다'는 견해에 의문, 실제로 코드도 짜줄 수 있는데 뒷단에서 무슨 일이 일어나는지는 사람 뇌도 마찬가지로 아무도 모름, 이런 본질 논쟁이 큰 의미 없고 실제 결과가 중요하다고 주장
          + 유연한 입력/출력 인터페이스를 가진 기억력 보조/정보 검색 툴이라는 실용적 관점
     * Gemini는 사람들이 자주 묻는 질문류엔 최적화된 반면, 더 전통적인 검색 의도엔 오히려 엉뚱한 선동성(Confabulated) 답을 주는 경향이 있음, 많은 사람들이 AI Overview를 예언자(oracle)처럼 신뢰하는 걸 목격했음, 이게 일반 대중의 AI 체험방식, '뉴스'에 대한 신뢰와 달리 AI는 나이 불문, demographic 상관없이 모두가 신뢰해버림, 본질적으로 인간은 근거 없는 자신감에 기반한 컴퓨터 답변을 좋아하는 종족 같다는 생각
          + Google의 검색 환경 변화가 특히 심각하다고 평가, 예전엔 페이지 상단 excerpt UI가 10년 넘게 사용, 신뢰할 만한 사이트에서 발췌해줬고 클릭 수 절약, 믿을 수 있는 정보원이었다고 회상, 의료 질문 땐 Mayo Clinic 같이 믿을 만한 곳에서 인용되어, 페이지에서 직접 찾을 수 있어 신뢰 쌓임, 시간이 지날수록 이 신뢰 시스템이 SEO에 의해 점점 훼손, 현재는 AI Overview라는 본질적으로 다른 시스템으로 대체된 것이 핵심 문제, 신뢰할 수 있는 유효 출처가 실시간으로 검증가능했던 시절과 확연한 차이
          + 직접 LLM을 쓰지 않는 사람만이 아니라, LLM을 전문으로 쓰는 매니저조차 자신을 확인시키는 답이 나올 때까지 질문을 바꿔가며 원하는 답을 찾아냄
          + 사람들은 근거 없는 확신에 기반한 답변 자체를 원래 좋아한다는 근본적 심리 언급
          + 이제는 무언가를 검색해 배울 수 있던 인터넷 환경이 사라졌다고 체감, 모든 결과가 신뢰할 수 없는 SEO 스팸 쓰레기라 더 심해졌고, AI Overview로 더 심각해질 것 같다는 우려, '프린터 동작 원리'를 검색하면 심지어 '도르래와 밧줄 시스템' 같은 어처구니 없는 대답이 나와도 곧이 곧대로 믿는 시대가 올 것 같은 무서움, 실제로 이런 말도 안 되는, 때론 위험한 오답을 매번 목격했다는 성찰
     * ""AI 답변에는 실수가 포함될 수 있다""는 메시지가야말로 AI 논의에서 가장 청중들에게 외치고 싶은 핵심, 모든 AI 윤리/안전 논의에서 이 포인트와 에너지/기후 영향이 중심이 되어야 함, 이 두 가지가 AI 열풍이 계속될 경우 인류에 가장 큰 피해가 될 것
          + 문제는 '실수가 있을 수 있다'가 아니라 '반드시 실수가 발생한다'는 점, 근데 사람들은 이를 자각하지 못하고 만능 예언자처럼 신봉, 실제론 단순 확률모델에 불과, 원숭이도 시도만 충분하면 셰익스피어를 쓸 확률 있음
     * Google이 검색의 근본을 완전히 잘못 잡았다는 비판, 이제는 답의 정확성보다는 빠른 요약과 스폰서 링크에만 집중한다는 비판
          + 빠른 답변 10개 중 6개는 미묘하게 틀렸고, 2개는 노골적으로 틀림, 1개는 아예 위험하다고 경험, 실제로 사람을 다치게 하거나 법적 문제를 일으킬 수도 있는 답변이 있음
          + Eric Schmidt 시대 Google의 '무응답보단 어떤 답변이라도 많은 게 낫다' 전략이 이제 '틀린 답이라도 없는 것보단 낫다'라는 식으로 진화한 것으로 봄
     * AI는 항상 근거 없이 자신만만하게 뭐든 대답하는 사람과 비슷, 그래서 진지하게 신뢰할 이유가 거의 없다는 견해
          + 심리적 요인이 핵심, 사람들은 누군가가 자신 없는 모습을 보일 때 비언어적 신호로 그걸 감지한다, 하지만 AI는 그런 신호가 없고, 기계가 내놓는 답은 본래 정확한 답이라는 오랜 신뢰가 있음, 비판적으로 접근하는 사람 비율이 매우 낮음
          + 아직 어떤 AI 회사도 자사 제품명을 'Cliff Clavin'으로 할 배짱이 없음, 실명 위험과 여러 용기의 문제까지 농담 추가
          + '도대체 왜 AI를 진심으로 신뢰하는지 모르겠다'는 주장에 대해, '수십 년간 전 세계 정보 제공을 목표로 하고 정확한 답을 제공하려고 노력해온 Google과 같은 기업이 이를 AI로 제공한다고 할 때, 사람들이 신뢰하는 게 당연하지 않냐'는 반문
     * 최근 ChatGPT와 Python 코드를 다루던 경험에서, Gunicorn의 로거 클래스를 특정 URL 패스에서 제외하려고 직접 3가지 솔루션 만들어 각각의 속도를 비교해달라고 챗봇에 요청, 벤치마크 코드와 함께 regex가 가장 빠르다는 결과를 받았으나, 직접 실행해보니 tuple 방식이 5배 이상 더 빠름, 직접 결과를 알렸더니 '알려줘서 고맙다, tuple 방식이 맞다'며 바로 수정하는 챗봇, 필요한 벤치마크 코드를 빠르게 받아 시간은 절약됐지만, 정답에 확신 없는 영역에선 챗봇 결과를 잘 신뢰하지 않게 되는 경험
"
"https://news.hada.io/topic?id=21179","미국 Science & Surplus의 오랜 생명","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      미국 Science & Surplus의 오랜 생명

     * American Science & Surplus는 팬데믹 이후 매출 감소와 최근 1년간의 급격한 하락으로 인해 새로운 창고 이전 비용 마련을 위해 GoFundMe를 시작함
     * 이 매장은 기괴하고 독특한 상품으로 지역문화에 깊게 녹아들어 있는 존재로 평가됨
     * 직접 방문해서 쇼핑하는 탐색적 경험이 온라인 소매업의 부상으로 점점 사라지는 현상임
     * 이곳은 단순한 소매점이 아니라 밀워키의 정체성과 다양성을 상징하는 공간임
     * 소상공인 지원의 중요성과 American Science & Surplus의 지속성을 위한 시민들의 관심이 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

American Science & Surplus: 밀워키의 익살스런 상징

  GoFundMe 시작 배경

     * American Science & Surplus는 팬데믹 이후 지속된 매출 감소와 지난 1년간의 급격한 하락으로 운영난을 겪음
     * 새로운 창고로의 이전 비용을 마련하고자 GoFundMe라는 크라우드펀딩 캠페인을 시작함
     * 기부금 목표는 $125,000이며 현재 약 $82,000이 모금됨

  집안 곳곳에 녹아든 독특함

     * 집안의 장식 소품과 기념품 상당수가 이 매장에서 구입한 것임을 확인함
     * 이 매장이 판매하는 참신하고 기이한 물건들이 일상 속에 자연스럽게 녹아있음
     * 물리적 가게 방문 자체가 특별한 경험으로, 단순한 쇼핑을 넘어 일상의 재미로 작용함

  매장의 지역 사회적 의미

     * American Science & Surplus는 밀워키에 문화적으로 자리잡았으며 지역 정체성에 영향을 끼침
     * 가게는 소소한 기념품, 과학용품, 잡동사니를 취급하는 곳으로, 누구나 새로운 것을 발견할 수 있는 탐색의 공간임
     * 오프라인 매장 방문 자체가 사라져가는 체험이 되었으며, 이로 인한 아쉬움이 드러남

  추억과 연결

     * 밴드 활동, 코스튬 준비, 가정용품 구매 등 다양한 추억이 American Science & Surplus와 연결됨
     * 천문경이나 노벨티 장난감 등을 구입하며, 소소한 이벤트와 지역행사 참여 경험도 늘어남

  소매 가족의 변화와 지원의 필요성

     * 코로나 이후 오프라인 소매업의 전반적 위축과 인플레이션, 온라인 상거래의 부상 등 산업 구조 변화가 매장 운영을 어렵게 만듦
     * 단순한 소매점이 아닌, 지역의 독창성과 다양성을 보여주는 대표적 공간이라는 점에서 중요성이 강조됨
     * 시민과 고객들이 GoFundMe와 같은 크라우드펀딩을 통해 지속 가능성에 기여해야 함이 강조됨

매장 탐방과 세대 전환

     * 최근 방문해 USB 클립 온 선풍기와 다양한 노벨티 상품을 구입했음
     * 매장에서 한 가족이 신기한 상품을 발견하며 감탄하는 모습을 목격했으며, 이러한 경험의 연속성이 매장의 미래에 대한 희망으로 연결됨

마무리 및 지역 사회적 의의

     * 소상공인의 지원이 지역 특색과 커뮤니티 문화를 지키는 방법임을 강조함
     * American Science & Surplus가 사라질 경우 밀워키의 독특성과 창의성이 위축됨을 우려함
     * 단순한 ""소매점""이 아니라, 호기심과 창의력, 지역문화를 경험할 수 있는 장소로서의 가치를 다시 한 번 상기함

참고 링크

     * 밀워키 지역과 American Science & Surplus 관련 추가 기사 및 콘텐츠 안내

        Hacker News 의견

     * AS&S가 오랜 세월 동안 진짜 과학 용품과 서플러스를 판매했지만, 요즘은 알리바바에서 볼 법한 소비재나 벼룩시장에나 나올 품목 위주로 바뀐 느낌을 받는 중임. 불과 10년 전만 해도 웹사이트에서 토글 스위치를 검색하면 수십 가지는 나왔었는데, 지금은 여섯 개, 그중 제대로 된 스위치는 두 개뿐인 상황임. 과학 용품이나 서플러스가 다 동나버린 건지 궁금함
          + 이제 수요가 사라진 것 같음. Gen Z와 Gen Alpha 세대는 이런 제품이 어떻게 동작하는지 모르는 경우가 많음
     * 어릴 때 7학년 과학 경진대회를 준비할 때 이 가게에 갔던 기억이 있음. 직원들이 어떤 부품이 필요한지 골라주는 데 매우 친절하고 열정적으로 도와줬고, 각 전자부품과 모터를 어떻게 안전하게 사용하는지까지 차근차근 설명해줌. 내 프로젝트 아이디어에도 응원을 해줘서 큰 감동을 받았고, 덕분에 대회에서 상도 받았음. 지금도 가끔 들르면서 신기한 전자부품 구경하는 재미가 남다름. 이런 독특한 것들을 수집해서 판매하려고 마음먹은 사람이 있었다는 사실이 참 좋음
          + 나도 8학년 과학 경진대회 준비할 때 도움을 받았음. 지진 시 건물의 진동을 시뮬레이션하는 셰이크 테이블 만들고 싶었는데, 직원들이 모터 선택까지 도와줬고 직접 벽에 꽂는 AC 모터도 보여줬던 제네바 지점임. 그 실험 덕분에 주 대회 본선까지 진출했던 기억이 있음
     * 내가 가장 좋아하는 가게가 HN 상위에 있다는 사실에 완전히 놀람. Milwaukee Av 지점에서 어린 시절 숱하게 방문한 추억이 많고, 그만큼 이 가게가 재정적으로 힘들다는 소식이 너무 속상함. 만약 살려야 할 만한 가치 있는 가게가 단 한 곳 있다면 바로 이곳임
          + 어릴 때 이 카탈로그에서 모터와 펌프를 주문하곤 했음. 종이 카탈로그에서 물건 보는 것 자체가 너무 즐거웠던 기억임
          + Milwaukee에 일부러 1/3은 Harley museum 방문, 2/3는 ASS 매장 방문하려고 일부러 주말 500마일 자전거 여행까지 다녀온 경험이 있음(이니셜은 일부러 저렇게 지은 듯함). Minneapolis의 ax-man surplus 같은 가게에도 여러 번 가봤고, 이 두 매장은 정말 비슷함. 이런 과학 서플러스 가게는 방문할 때마다 재고가 계속 바뀌어서 매번 완전히 다른 경험을 할 수 있고, 똑같은 걸 두 번 못 본다는 점이 매력임. saveitforparts 유튜브 채널에서 ax-man 투어 영상도 참고할 만함. 이런 오리지널 해커 DIY 문화 보존과 젊은 세대에 ""DIY/펑크/해커/재활용 정신""을 깨닫게 해주려면 이런 매장이 꼭 필요하다고 생각함. 실리콘밸리나 뉴저지 bell labs 근처보다 오히려 중서부 쪽에 이런 매장이 더 오래 살아남은 게 신기함. Surplus Sales of Nebraska도 이런 방면에선 최고 사례임. 최근까지 웹사이트도
            완전 웹 1.0 감성이었고, 정말 독특한 부품이 필요한 이들에게는 구원의 손길임
     * 이런 독특하게 재미난 가게는 처음임. 만약 저전력 오프너, Palm Pilot용 스크린 프로텍터, $1,200짜리 실물 크기 테라코타 전사 모형 등 특이한 물건을 한 번에 구하고 싶으면 더 이상 다른 여러 군데 돌 필요 없이 여기만 가면 됨
       Palm Pilot 스크린 프로텍터
       저전력 자동 오프너
       실물 크기 테라코타 전사
     * SLC, Utah에 Ra-Elco라는 매장이 있었는데, 생각할 수 있는 모든 전자부품과 내가 한 번도 본 적 없는 특이한 장치까지 다 갖춘 최고의 전자 부품샵이었음. 이런 샵을 너무 좋아하고, 요즘 이 정도 개성을 가진 곳들이 점점 사라지고 있다는 게 정말 아쉬움
          + SoCal(리버사이드)에 Electronics Warehouse가 있었음. 솔직히 자주 가진 않았지만, Radio Shack보다 훨씬 다양한 부품이 많았고 내 디자인 방향성에서 살짝 멀어져서 덜 가게 됐었음
          + 이런 매장들을 나도 좋아하지만, 왜 사라지고 있는지 이해함. 소비자가 원하는 SKU(부품 종류) 종류가 폭발적으로 늘었고, 온라인 구매와의 가격 경쟁력이 상상 이상임. 요즘 모터 드라이버 하나가 급해서 동네 가게를 갔는데 재고 없었음. 온라인에서 5개 묶음을 $10 정도에, 미국 배송으로 바로 구할 수 있었음. 이런 전자부품은 2~30% 싼 레벨이 아니라, 10배 이상 차이가 나기도 함. 중국에서 핀 헤더 10개 $1~2에 구입 가능한데, 동네 가게에서는 하나당 50센트임
          + Ra-Elco는 건물이 화재로 소실된 후 Standard Supply로 이전했다는 이야기 들음. 아직 가보진 않아서 확실하진 않음
     * 크리스마스 시즌마다 아이들에게 과학적인 소품을 선물할 때 항상 이 가게를 찾았음. 카탈로그 자체만 봐도 온갖 언어유희와 재미있는 문장 덕분에 읽는 맛이 남달랐음. 사실 GoFundMe로 영리사업체를 돕는 방식이 별로라 생각했지만, 이 곳에서 쌓은 수많은 추억 때문에 결국 후원에 참여함. 이런 소규모 과학 출판사–예를 들면 10년 전에 문을 닫은 Lindsay's Publications–도 이런 방식의 크라우드펀딩이 있었다면 살아남았을지 궁금함. Lindsay's Books는 정말 좋은 책이 많았고, HN에서 따로 한번 다뤄도 될 만한 가치가 충분함
       makezine에서 Lindsay’s Technical Books의 마지막 카탈로그 이야기
          + Lindsay의 책들은 예전부터 많이 모아뒀었음(독학자용 리제너러티브 라디오 조립, ""The Boy Mechanic"", ""5 Acres and Independence"" 같은 복각본 등). 다행히 많이 ""the org""에 아카이브돼 있었으면 좋겠음
            (""The Impoverished Radio Experimenter Vol. 4"" 예시: Internet Archive에서 열람 가능)
          + 영리사업을 위한 GoFundMe는 그다지 맞지 않는 느낌임. 차라리 주식처럼 지분을 팔아야 함. 그린베이 패커스 모델을 참고할 수 있음
          + Lindsay는 은퇴한 것임. 그래도 그가 큐레이션한 유니크한 책들이 너무 그리움
     * 과거에 Toronto Queen Street에 Active Surplus라는 거의 비슷한 매장이 있었는데, 임대료 인상으로 문을 닫아서 아쉬움
       구글맵 과거 모습
          + 지금은 조금 도시 외곽에서 비슷한 매장이 운영 중인 것으로 알고 있음. 친구 중 한 명이 꽤 큰 규모의 과학기기를 샀다는 이야기를 들었는데, 이름만 다를 뿐 사실상 같은 스타일의 가게일 수도 있음
     * 어릴 때 매장도 정말 좋아했고, 종이 카탈로그 받아보는 것도 즐거웠음. 덕분에 뭔가를 만들어보게 되는 영감을 받았음. 지금도 MKE에 갈 때마다 꼭 방문하는 주요 코스임
          + 나도 비슷함. 매년 부모님 댁에 갈 때마다 순례하듯 꼭 들림
     * 이곳은 정말 최고임. 지금 재정적으로 힘들다니 너무 아쉬움. 어릴 때 웹사이트를 몇 시간씩 구경하며 온갖 이상한 물건에 매료됐었음. Mythbusters 보는 재미와 비슷하게 내 호기심을 자극했고, 성인이 돼서 처음 매장에 직접 가봤을 때는 정말 몇 시간씩 구경하면서 온갖 창의적 아이디어가 샘솟았음
     * 온라인 스토어가 https://sciplus.com/이 맞는지 궁금함. 혹시라도 쇼핑으로 도움이 되지 않을까 제안함
          + 맞음, 그 웹사이트임
"
"https://news.hada.io/topic?id=21240","러시아 깊숙한 곳에서 드론 공격으로 러시아 군용기 40대 이상 파괴됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 러시아 깊숙한 곳에서 드론 공격으로 러시아 군용기 40대 이상 파괴됨

     * 우크라이나가 러시아 영토 깊숙이 드론 공격을 감행해 군용기 40대 이상을 파괴함
     * 이번 작전은 약 1년 반의 준비 기간과 Zelenskyy 대통령의 직접 감독 하에 이루어짐
     * 드론은 컨테이너 트럭으로 러시아 내륙까지 이동하여 Irkutsk 지역의 Belaya 공군 기지 등 주요 비행장을 타격함
     * 러시아도 우크라이나를 향해 역대 최다 472기의 드론과 7기의 미사일을 동원하여 대응함
     * 두 국가는 이스탄불에서 새 평화 협상을 앞두고 긴장이 고조되고 있는 상황임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 우크라이나 보안당국 소식통에 따르면, 우크라이나군이 최근 드론 공격으로 러시아 영토 깊숙한 곳에 있는 군용기 40대 이상을 파괴한 사실이 알려짐
     * 해당 공격은 이스탄불에서 열릴 예정인 직접 평화회담 전날 이루어진 사건임

드론 공격의 세부 내용

     * 익명을 요구한 우크라이나 보안 관계자는 이번 작전 준비에 1년 반 이상이 소요되었으며, Zelenskyy 대통령이 직접 관리했다고 밝힘
     * 드론은 트럭에 실린 컨테이너 형태로 러시아 내륙까지 운송되어 Belaya 공군기지 등 Irkutsk 지역 공항을 공격함
     * 현지 주지사 Igor Kobzeva에 따르면, 이 지역에서 우크라이나 드론이 목격된 것은 이번이 처음임
     * 당국은 민간인에게 위협을 주는 상황은 아니었다는 점을 강조함

우크라이나-러시아 협상 관련

     * 같은 날 Zelenskyy 대통령은 우크라이나가 러시아와의 새로운 직접 평화회담을 위해 대표단을 이스탄불에 파견할 것이라고 발표함
     * 국방장관 Rustem Umerov가 대표단을 이끌 예정임
     * 우크라이나는 회담 전 러시아가 종전 관련 입장을 공식적으로 문서화해 전달할 것을 요구함
     * 러시아도 해당 입장을 회담 중 공유하겠다고 시사함

러시아의 대규모 드론·미사일 공격 및 우크라이나 군 손실

     * 2022년 2월 이후 러시아가 단일 최대 규모인 472기의 드론을 우크라이나에 동원해 공격함
     * 러시아군은 7기의 미사일 공격도 병행함
     * 이런 공격 중 우크라이나군 훈련부대가 미사일 타격을 받아 최소 12명 사망, 60명 이상 부상 발생함
     * 해당 부대는 전선에서 떨어진 후방에 위치해 있었으며, 대규모 집합 등은 없었다고 설명함
     * 군 당국은 이번 인명 피해 원인 조사를 위한 특별 위원회를 구성함
     * 만약 관리 부실로 피해가 발생했을 경우, 관련 책임자들에게 강력한 문책 조치가 있을 예정임

북부 전선 및 전황

     * 러시아 국방부는 우크라이나 북부 Sumy 지역의 Oleksiivka 마을을 장악했다고 발표함
     * Sumy 지역 당국은 11개 거주지에 대해 추가 강제 대피 조치를 명령함
     * 우크라이나 육군 총사령관 Oleksandr Syrskyi는 Donetsk 지역 내 Pokrovsk, Toretsk, Lyman 및 Sumy 접경 지역에 대한 러시아의 공세 집중 상황을 언급함

결론

     * 드론 및 미사일 공격으로 양국 간 긴장 최고조
     * 이스탄불 평화회담 전후로 군사적·외교적 상황이 중대한 변곡점에 있음

        Hacker News 의견

     * 소련 시절 중공업 항공기 공급망 대부분이 우크라이나에 있었던 상황에서 지금 러시아는 신규 전략 항공기 생산 능력이 거의 없는 형편임을 이야기함. 1991년 이후 투입된 소수의 항공기도 대부분 예전에 비축한 부품에 의존함을 설명함. 전술기조차 연간 몇 대만 겨우 인도할 정도이며, 교육 시스템 붕괴와 생산가능 인구 감소로 이 추세가 쉽게 역전되지 않을 전망임을 강조함
          + 러시아의 전략 폭격기 제조 역량은 바라보는 시점에 따라 전혀 없기도 하고, 충분히 만들 수 있기도 하다고 생각함. 마음만 먹는다면 Il-96을 기반으로 한 좁은 동체의 폭격기 버전을 몇 년 내에 만들어낼 수 있을 것으로 봄
     * 미국이 우크라이나 전쟁을 계속되는 걸 원하는 이유가 바로 이것임을 주장함. 미국은 직접 개입하지 않고 적국의 전력을 소모시키며, 처벌과 비난은 모두 다른 나라에 돌아가는 구조임을 지적함. 미국의 동맹이 계속 희생당하지 않는 한 이 전략은 매우 완벽한 시나리오라고 말함
          + 사실 미국이 우크라이나에 점점 영향력이 없어지는 것이 원인이라 생각함. 미국은 우크라이나에게 러시아 본토 타격에 대해 지속적으로 압박해왔으며, 미국이 아닌 다른 주체도 분명한 의사를 가진다는 점을 강조함
          + 최근 상황을 보며 우크라이나는 미국 지원 없이도 싸움을 이어나갈 의지를 보이며 그대로 전투를 지속하고 있음. 항복과 노예화밖에 선택지가 없기에 싸우는 것임을 밝힘. 미국과 EU가 우크라이나의 주권을 지지하는 것에는 뚜렷한 실익이 있는데, 단순히 러시아 군사력 감소만이 아니라 국경 지역에서의 러시아 제국주의 확장에 대한 억제 효과도 있다고 봄
          + Apophenia 현상을 언급하면서, 패턴 인식 과잉을 비판적으로 시사함
     * 우크라이나가 러시아 폭격기 전체의 1/3가량을 무력화시킨 것처럼 보이는 상황을 언급함
       참고 링크
          + 정밀 타격 정보가 뒷받침됐다면 그보다 더 효과적인 공격이었을 수도 있음. 러시아가 함대 가동에도 애를 먹는 상태에서, 최근 운행한 항공기를 우선적으로 노렸다면 러시아는 운항 불가 기체만 남아 더욱 운용이 어려워졌을 것임을 강조함
          + 해당 항공기의 비용과 이번 공격의 비용을 비교하면 그 차도가 충격적일 정도임을 언급함
          + 미국이 상위 5위 중 4개를 차지하고 있고, 나머지 하나만 러시아임을 언급하며 흥미로움을 전함
          + 손상된 항공기 중 일부는 폭격기가 아닐 수도 있다고 조심스럽게 추측함
     * 트럭 한 대 가득 드론을 실어 적국 심장부까지 침투시킨 아이디어에 크게 감탄함. 그로 인해 러시아의 핵 억지력에도 심각한 균열이 생긴 상황임을 지적함. 유럽이 러시아 핵 위협을 허상으로 간주하고 우크라이나에 지상군을 파견하면 세상이 매우 흥미진진해질 것임을 상상함
          + 러시아는 핵 삼중체(nuclear triad)를 보유하고 있으므로, 모든 잠수함이 동시에 항구에 정박해 공격당하지 않는 한 세 방식의 핵 무력 모두가 한 번에 사라지진 않음
            관련 링크
          + 국제 배송으로 우체국 창고까지 드론 암살을 상상해봄. 기사가 작은 상자를 인근에 내리면 기계가 상자를 열고 드론이 나와 목표를 타격하는 방식임. 만약 상자 자체도 날아가서 스스로 파괴된다면 추적 흔적이 훨씬 더 감소할 것임
          + 트럭에 드론을 실어 국경 깊숙이 침투시키는 전략이 그렇게 뛰어난 아이디어인지 반문함. 만약 역지사지로 이런 트럭이 이스라엘 같은 전쟁 국가로 들어간다면 민간 차량 전체가 표적이 될 수 있음을 경고함. 러시아의 핵 억지력에 구멍을 내는 게 과연 바람직한가에 대해 의구심을 표하며, 핵무기 관리자가 더 불안정해지고 충동적 결정을 내릴 위험을 지적함
     * 배터리 기반 드론의 최대 사거리(약 14km)이 제한적이라 이번과 같은 공격에는 적합하지 않기에 우크라이나가 트럭을 사용해 목표지 근처 1km 이내까지 드론을 운반할 수밖에 없었다고 설명함. 2022년 침공 시작 당시 러시아 내에 300만 명의 우크라이나인이 이미 거주 중이었기에 검문소에서 러시아인으로 위장하기도 쉬웠다는 독특한 환경을 언급함. 이런 경우는 앞으로 다른 분쟁에서는 쉽게 기대하기 어렵다는 의견을 피력함. 2차 세계대전 당시 독일이 영국 내에서 유의미한 스파이 정보를 거의 얻지 못했던 사례와 비교함. 사회 전체의 인지와 대응 능력이 높았기에 영국 시민들은 독일 스파이를 쉽게 포착했으며, 당시에 만약 드론 문화가 있었다면 트럭에 드론을 싣고 영국 군사 목표에 접근하는 것도 불가능에 가까웠을 거라 평가함
          + 2차 세계대전 시기 호주/영국 특수부대가 일본 어선으로 위장하여 싱가포르 항만까지 침투한 Operation Jaywick 사례를 소개함. 전술적으로 대성공은 아니었지만 동맹군 사기 진작에 큰 역할을 했다 전하며, 심지어 피부색이 다른 사람도 분장·위장으로 작전 성공이 가능했음을 강조함. 분쟁지역에는 보통 ‘원어민처럼 행동 가능한 적대국 출신’이 있기 때문에 북/남한, 중국/대만, 중동 등에서 충분히 시도 가능성을 높게 평가함. 전시에는 군·민 모두 인력이 부족하기에 러시아도 평시처럼 모든 곳을 단단히 방비하기 어렵다는 점을 덧붙임
          + 우크라이나인이 러시아인으로 위장할 필요 자체가 없으니 굳이 신경 쓸 필요 없음. 러시아 내 우크라이나인이 많고 자유롭게 트럭 운전 가능함을 들며 역사적 억류 사례와 비교함
            일본계 미국인 강제수용 관련 링크
          + 드론이 전파 방해(재밍)에 매우 취약하고, 그래서 양측 모두 광섬유 케이블이 감긴 유선 드론까지 사용한다고 설명함
          + 완전히 러시아인으로 위장할 필요 없는 이유는 사실 운전자가 화물 적재물을 모르는 민간인이었던 점이 있음. 이전 교량 공격 때와 똑같은 방식임을 밝힘
     * 드론 방어 대책(재밍, 레이저, 그물, 총, 강화 격납고 등)이 공군기지 같은 시설에는 왜 더 보편화되지 않는지 놀라워함. 정부가 이런 인프라 보호에 더 박차를 가할 줄 알았는데 이번 사건이 경각심을 주는 계기가 될 수 있다고 의견을 남김
          + 이 공군기지들은 우크라이나 국경에서 수 천 km 떨어져 있어 드론 위협을 심각하게 생각하지 않았을 것이라 봄
          + 미국/러시아 간 START 조약이 폭격기를 야외에 두고 위성 감시가 가능하도록 요구했었기 때문임을 설명함. 이번 공격 이후로 해당 조약도 의미 없어질 것 같다는 추측함
          + 이런 장비들이 있긴 하겠지만 트럭 자체가 기지 내부로 들어왔기에 방어 체계를 우회한 것으로 봄
          + 전자전(ECM/EW)은 전선에 배치해야 하며, 이 기지들은 러시아 내 깊은 곳에 있었음. 레이저 기반 대드론 기술은 흔치 않으며, 악천후에는 레이저가 잘 작동하지 않기에 탄도무기가 더 나음
     * 우크라이나 전쟁은 근본적으로 두 가지 인간 시스템 모델 간의 대결임을 주장함. 푸틴 모델(수직적 지시와 맹목적 복종)과 협업적 모델(집단적 문제 인식과 대응)로 나뉨을 설명함. 많은 미국 기업과 개인조차 지금은 푸틴 모델을 채택하고 있다고 느끼며, 몇몇 리더가 실수를 반복하는 모습을 목격했다고 밝힘. 협업적 모델은 구성원 각자가 문제를 인식·해결해야 하는 요구가 있음. 하지만 미국의 ‘지도자 따르기’ 교육은 독립 사고하는 인재 수를 줄이고 있다고 비판함. 실제 위기에서는 엘론 머스크와 팀 쿡 같은 리더를 먼저 내쫓고 싶을 정도로 신뢰하지 않음. 독창적 사고가 가능하다면 적극적으로 실천하라고 격려함
     * 러시아 폭격기 전체의 1/3에 해당하는 40여 대가 공격 대상이 됐음
       참고 링크
     * 만약 보도된 수치가 사실이라면, 이번 공격은 역사상 가장 성공한 공격 중 하나에 해당한다고 봄. 드론이 전쟁의 판도를 통째로 바꾸고 있음
          + 나의 우려는 이것이 단순히 전쟁의 판도 이상으로 안보와 치안 전체에 새로운 차원의 영향을 미칠 것이라는 점임. 광섬유 드론 등, 우리가 기술의 진정한 함의를 아직 충분히 인식하지 못하고 있다고 생각함
     * 드론의 로터에서 발생하는 기계적 전파 간섭이 센서에 감지됨을 이야기함. 최신 대드론 방어 시스템은 전통적인 레이더가 아닌 전동기와 3G/4G 무선 신호 탐지로 전환돼야 한다고 주장함. 효과적인 방어란 멀리까지 감지하지 않고도 가능하며, 값비싼 폭발물 없이도 요격 드론을 대량 투입하는 시대가 곧 열릴 것으로 예측함. 러시아를 무능하게 보면 안 됨을 경고하며, 빠른 적응력과 우크라이나와의 실전을 통한 벤치마킹 속도를 높이 평가함. 이제 세계가 비효율적인 F35 대신 더 실용적인 선택을 할지 의문임을 전함
          + FPV UAS 등의 드론이 F-35보다 ‘더 효율적’이라는 식의 단순 비교 주장은 의미가 없다고 느낌
          + 우크라이나에서 쓰이는 드론 대부분이 무선이 아니라 매우 얇은 광섬유 케이블을 펼치며 비행함을 새롭게 알게 됐다고 함. 병사가 들판을 거닐며 케이블을 거두는 영상이 거미줄 수확하는 모습과 비슷함
          + 배터리식 단거리 저속 드론은 장거리 초음속 공격기의 대체제가 절대로 될 수 없다고 강조함. 완전 자율형 초음속 드론을 만든다고 해도, F-35가 조종사 때문이 아니라 본질적으로 비싸서 가격이 비슷해질 것임
"
"https://news.hada.io/topic?id=21286","프로그래머를 위한 프롬프트 엔지니어링 플레이북","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       프로그래머를 위한 프롬프트 엔지니어링 플레이북

    1. 서론: 프롬프트가 개발 생산성을 좌우한다

     * AI 코딩 도우미의 성능은 프롬프트 품질에 달려 있다. 명확한 요청은 정교한 코드를, 모호한 질문은 쓸모없는 출력을 낳는다.
     * 프롬프트 작성법은 이제 개발자에게 필수 역량이며, 이는 개발 파트너를 훈련시키는 작업과 같다.
     * 이 글은 실전 예시와 비교를 통해 ‘좋은 질문’이 어떻게 ‘좋은 코드’를 만드는지를 보여준다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    2. 효과적인 프롬프트의 7가지 원칙

      ① 맥락 제공

     * AI는 사용자의 프로젝트 배경을 모르므로 언어, 프레임워크, 라이브러리, 오류 메시지, 목적 등을 명시해야 한다.
     * 예: “Node.js + Express + Mongoose 환경에서 user fetch 중 TypeError 발생”처럼 기술적 배경을 포함.

      ② 목표 명확화

     * ""코드가 안돼요""는 도움이 안 된다. “예상 결과는 ○○인데 실제는 △△임. 왜 그런가?” 식으로 정확히 묻자.

      ③ 복잡한 작업 쪼개기

     * 기능 전체를 한 번에 묻기보다 단계별로 요청하는 게 더 효과적이다. 예: 컴포넌트 > 상태 관리 > API 통합.

      ④ 입출력 예시 포함

     * 원하는 출력 예시를 보여주면 AI의 의도 파악 능력이 높아진다. (e.g. [3,1,4] → [1,3,4])

      ⑤ 역할 부여

     * “시니어 리액트 개발자처럼 코드를 리뷰해줘” 식의 역할 설정은 응답의 깊이와 품질을 향상시킨다.

      ⑥ 대화형 반복 개선

     * 첫 응답이 완벽할 필요는 없다. 피드백을 주면 AI는 그 흐름을 이어서 점점 정교한 결과를 낸다.

      ⑦ 코드 일관성 유지

     * 함수명, 포맷, 주석 등 코드 자체가 일관성 있게 작성되어 있으면, AI도 그 흐름을 유지해 품질이 높아진다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    3. 디버깅을 위한 프롬프트 전략

      ① 오류 명시와 기대 동작 비교

     * 에러 메시지, 문제 증상, 기대 결과, 입력값을 함께 제시하면 AI는 정확한 진단을 한다.

      ② 라인 단위 추적 요청

     * “이 변수 값이 어디서 이상해졌는지 단계별로 추적해줘” 식의 요청은 복잡한 논리 버그에 효과적이다.

      ③ 최소 재현 코드 제공

     * 전체 코드 대신 문제가 발생하는 핵심 코드만 주면, AI가 더 정확하게 원인을 분석할 수 있다.

      ④ 명확한 후속 질문

     * “왜 이런 결과가 나오죠?”보다 “이 부분에서 어떤 조건이 잘못된 걸까요?”처럼 직접적인 질문이 좋다.

      ⑤ 예시 비교: 나쁜 질문 vs. 좋은 질문

     * 단순히 “코드 안됨”이라고 하면 추측성 답변만 나오지만, 에러 메시지와 코드를 함께 주면 정확한 해결책을 받게 된다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    4. 리팩토링과 최적화를 위한 프롬프트 전략

      ① 리팩토링 목표 명확화

     * 단순히 “리팩토링 해줘”보다는 “가독성 향상, 성능 개선, API 최신화” 등 구체적 개선 목표를 제시해야 한다.
     * 목표가 모호하면 AI는 무작위 개선을 시도하거나 원하지 않는 방향으로 바꿀 수 있다.

      ② 언어/환경 맥락 제공

     * “React 클래스형 → 함수형 전환”, “Node.js 14 환경” 등 프로젝트 스타일이나 기술 제약을 알려주면 적합한 변환이 가능하다.

      ③ 설명도 함께 요청

     * 리팩토링된 코드와 함께 “왜 이렇게 바꿨는지” 설명을 요청하면, 코드 품질 검토와 학습 효과 모두 얻을 수 있다.

      ④ 역할 기반 요청으로 수준 향상

     * “시니어 타입스크립트 개발자처럼 리팩토링해줘” 같은 요청은 더 현대적이고 깊이 있는 개선안을 이끌어낸다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    5. 새 기능 구현을 위한 프롬프트 전략

      ① 기능을 단계로 나눠 요청

     * 복잡한 기능도 “기능 구조 설계 → UI 생성 → 로직 연결” 순으로 나누어 요청하면 더 안정적인 결과를 얻을 수 있다.

      ② 기존 코드 스타일 제공

     * 유사한 컴포넌트나 내부 컨벤션을 제시하면, 프로젝트 일관성에 맞는 코드가 생성된다. 예: “UserList 기반으로 ProductList 생성”

      ③ 주석/TODO로 의도 전달

     * IDE에서 “// TODO: 요청 유효성 검증 구현”처럼 자연어 주석을 달면 Copilot이 그에 맞는 코드 블록을 자동 생성한다.

      ④ 입출력 예시 제시

     * 입력값과 기대 출력 예를 포함하면 AI는 이를 충족시키려 노력하며 정확도가 높아진다.

      ⑤ 피드백 기반 반복 개선

     * 첫 결과가 기대에 못 미쳐도 “filter 대신 map 사용해주세요”처럼 피드백을 주면 AI가 바로 반영하며 진화한다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    6. 실패하는 프롬프트의 7가지 패턴 (Anti-patterns)

      ① 모호한 요청

     * “이 코드 왜 안돼요?” 같은 질문은 의미 없는 일반론적 답변만 이끌어낸다. 오류 메시지, 코드, 기대 결과를 포함하자.

      ② 요구 과잉

     * “앱 전체 생성 + 인증 기능 추가 + 배포 스크립트 포함” 등 복합 요청은 누락이나 혼란을 야기하므로 단계별 분리 필요.

      ③ 질문 없음

     * 코드만 던지고 요청이 없으면 AI는 요약을 하거나 무관한 결과를 내놓기 쉬우므로, 질문 목적을 명확히 해야 한다.

      ④ 성공 기준 불명확

     * “빠르게 해줘”, “더 좋게 바꿔줘”는 기준이 모호하다. 예: “O(n) 시간 복잡도로 개선”처럼 측정 가능한 기준을 제시해야 한다.

      ⑤ AI의 질문 무시

     * AI가 “이게 함수형인가요 클래스형인가요?”라고 묻는다면 그에 답해야 최적화된 출력을 받을 수 있다.

      ⑥ 일관성 부족

     * 스타일, 문법, 용어가 계속 바뀌면 AI도 혼란을 겪는다. 하나의 스타일을 유지해야 응답 품질이 향상된다.

      ⑦ “위 코드” 같은 모호한 참조

     * 대화가 길어질수록 “위 코드”는 불명확해진다. 가능한 한 코드를 다시 제시하거나 명시적으로 함수명을 언급하자.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    7. 결론: AI와의 협업은 반복적 대화다

     * 프롬프트 엔지니어링은 이제 개발자의 핵심 커뮤니케이션 스킬이다. 맥락 제공, 명확한 목표, 반복 개선이 기본이다.
     * AI는 코딩 도우미가 아니라 협업자이자 학습 파트너다. 이를 잘 활용하면 생산성뿐 아니라 개발 실력도 향상된다.
     * 실험, 피드백, 역할 설정 등 다양한 전략을 활용하면 AI를 실전 팀원처럼 다룰 수 있다.
     * 최종 목표는 더 빠르고 정확한 코드 생성이지만, 동시에 더 나은 개발자가 되기 위한 학습 도구로도 적극 활용해야 한다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  좋은 vs 나쁜 프롬프트 비교표

     구분                             디버깅                                                       리팩토링                                                                    기능 구현
   좋은 프롬프트 TypeError 발생 시점이 여깁니다. 기대값은 ○○인데 NaN이 나옵니다. 원인 파악 부탁. 이 함수의 중복을 제거하고 성능을 향상시켜줘. fetch 부분은 helper로 분리하고, 오류 메시지는 유지해줘. 검색창이 있는 ProductList 컴포넌트를 생성해줘. /api/products에서 JSON 받아서 목록 필터링하고, 오류·로딩 상태도 포함.
   나쁜 프롬프트 왜 제 함수가 안 되죠?                                         리팩토링 해줘요.                                                       검색 기능 만들어줘요.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   제발 제발 이렇게 하지마 해도 10에 1번은 그렇게 하는 놈들임 -_-

   인간이 뭘 알어!

   인간 프로그래머 상대하는 법과 다르지 않군요.

   프로그래머를 위한 프롬프트 엔지니어링 플레이북

   GN+ 요약 봇이 요약한 버전도 같이 참고하세요. 해커 뉴스 요약 댓글도 볼만합니다.

   먼저 올리셨군요..
   감사합니다.
"
"https://news.hada.io/topic?id=21290","미국 IRS의 Direct File, GitHub에 코드 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   미국 IRS의 Direct File, GitHub에 코드 공개

     * IRS가 Direct File의 상당수 소스 코드를 GitHub에 오픈소스로 공개함
     * 미국 정부 저작물로서 공개 도메인에 속하고, 누구나 코드 검토 가능함
     * 이번 공개는 SHARE IT Act 이행의 일환으로, 법정 기한보다 3주 앞서 발표됨
     * 오픈소스 공개는 투명성 제고 및 납세자 신뢰 구축 목적임
     * Direct File 팀은 데이터 보안, 공정한 세금 혜택 적용, 대중 접근성 강화 등도 중시함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Direct File의 GitHub 소스 코드 공개

   2025년 5월 30일, IRS는 Direct File 서비스의 대다수 소스 코드를 GitHub에 오픈소스 소프트웨어 형태로 공개함. 미국 정부 저작물로서 해당 코드는 공개 도메인에 속하며, 누구든 자유롭게 내용을 확인할 수 있음.

공개 취지 및 배경

   이번 소스 코드 공개는 SHARE IT Act(118번째 연방 의회 법안 9566호) 준수를 위한 조치로, 공식 기한보다 3주 빨리 이루어짐. IRS가 운영하는 더 많은 소프트웨어 코드가 앞으로 누구나 접근 가능해질 것으로 기대되는 상황임.

오픈소스의 중요성

   Direct File 서비스의 코드 오픈소스화는 최근 계획된 것이 아니라, 오랜기간 추진되어온 사항임. 작년 Direct File 팀이 발표한 바와 같이, 오픈소스는 다음의 중요성을 가짐
     * IRS는 투명성 강화를 통해 공공의 신뢰 형성 및 업무의 독립적 평가가 가능해짐
     * 모든 납세자가 자신이 받을 자격이 있는 세금 혜택을 충분히 누릴 수 있도록 소프트웨어가 설계됨을 외부에 입증할 수 있음
     * 공개 소프트웨어를 통해 이러한 약속 이행을 기술적으로 보여줄 수 있음

납세자 신뢰와 Direct File의 원칙

   납세자 신뢰 구축은 Direct File 설계의 핵심 전략이었음. 이를 위해 아래 요소들이 강조됨
     * 가장 정확한 세금 신고 옵션 제공
     * 국민 모두가 세금 신고 시스템을 접근 및 활용할 수 있게 함
     * 납세자 데이터 보안 원칙을 엄격히 준수
     * 직접 코드 공개를 통해 투명성을 실천

개인적 고지

   작성자는 2주 전 공식적으로 IRS 근무를 마친 상태임. 현재의 글은 순수 개인 견해임

GitHub 저장소 주소

     * https://github.com/IRS-Public/direct-file

        Hacker News 의견

     * Direct File에 Fact Graph라는 선언적이고 XML 기반의 지식 그래프 데이터 구조가 포함되어 있어서, 부분적으로 작성된 세금 신고서처럼 불완전한 정보에 대해 추론하도록 설계되어 있음. Fact Graph는 Scala로 작성되고, 백엔드에서는 JVM에서 실행되고, 클라이언트에서는 Scala.js를 통해 트랜스파일되어 실행됨. Direct File의 Fact Graph는 특정 도메인에 한정되지 않아서, 세무기관이나 다른 비즈니스 룰 엔진을 구현하는 참고 자료로도 유용하게 활용 가능성
          + fact graph가 어떻게 동작하는지 정의하는 코드는 여기에서 볼 수 있음. 실제 세금 정의와 파생 계산은 이곳에서 확인 가능성. 예시로 기본 공제(Standard Deduction)와 세율 계산이 있음. 이런 정의들은 MeF(Modernized e-File) 스키마 기반이라 생각됨. 시스템이 입력 데이터를 MeF 스키마의 XML로 변환해 MeF 시스템에 전송해야 하기 때문임. 자세한 설명은 IRS 공식 페이지에서 확인 가능성
          + 흥미로워서 더 읽어보고 싶은 욕구
     * 최근 2주 전까지 IRS에서 일했으나, 개인 자격으로 얘기함. 현 행정부가 DirectFile을 없애면서 관련 인력들도 모두 해고해버린 상황이 안타까움
     * 슬프게도 이 프로그램이 현재 행정부에 의해 중단되는 상황임. 코드 저장소가 정말 훌륭하고, Scala fact graph 구조도 정말 멋짐. 튜토리얼에도 엄청난 정성이 들어간 것이 보임
          + 이 프로젝트에 참여한 사람들은 코드 한 줄 쓰기 전에 이미 운명이 정해져 있다는 걸 알았을 것이라 생각함. 다음에 (R, 공화당) 인사가 집권하면 바로 죽을 프로젝트임을 알았을 것. 소프트웨어를 실제로 공개까지 해낸 점이 대단한 성취지만, 오래가지 않을 것임을 모두 인식하고 있었음. pay-to-file 세금 로비 세력이 너무 강하고 부패되어 있음
          + 이 법안은 공화당 소속 Nick Langworthy가 발의했고 William Timmons가 공동발의한 사실이 있음. 잘못된 정보나 가짜 정보를 퍼뜨리지 않길 바람
     * Java에서 이런 방식이 일반적인지 질문, 예시 코드 링크 공유
          + 이건 Java의 리액티브 프로그래밍 예시임. 작업 완료 시 실행될 콜백을 반환함. Mono<T> 타입이 대표적인 특징임
          + 오랜 기간 Java를 사용했지만, 이런 코드는 보기 힘듦. 리액티브 스타일 프로그래밍 (reactor.core.publisher.Mono)에서 비롯된 부분이 많음. 한 화면에 모든 코드를 맞추려고 한 것일 수도 있을 듯. 만약 내가 팀장이라면 더 단순화하라고 요청할 욕구
          + 이런 코딩 스타일은 정부 프로젝트에서 평생직장 얻으려거나, 남들이 읽기 힘들면 유리하다고 생각하는 경우 자주 보임. 아니면 초광폭 모니터 판매가 목적인 경우에도 필요함
          + atomics는 어색하지만, reactor를 사용하는 경우 순차적 blocking 동작이 필요하면 전체적으로 코드 구조가 엉망이 되는 점이 있음
          + 비슷한 것을 Java는 물론 다른 언어에서도 종종 봄. 선호하는 방식은 아닐 확률이 높음
     * IRS가 이를 공개하는 데 있어 소스코드 자체보다는 세수 시스템과의 연동, 그리고 현행 세법 준수 보장 부분이 더 어렵다고 생각함. 설사 소스 코드를 공개한다고 해도, 이 부분은 행정부가 얼마든지 중단 가능성 남음
          + 절대적으로 동의함. 저장소 내 설명을 보면, Direct File은 미국 세법(26 USC)을 평이한 질문으로 해석하여, 추가 설명 없이 납세자가 직접 답할 수 있게 했음. 납세자의 답을 표준 세금 양식으로 변환하여, 공인 API인 Modernized e-File(MeF)로 전송함. 이론상으론 당장 쓸 수 있어도, 말씀하신 대로 정책이 바뀌면 무의미해질 위험 있음
          + 추가적으로, 어려운 점에는 수십 년간 세금 준비업계 로비가 이익 보호를 위해 활동해온 것도 포함됨. 무료 직접 신고를 허용하는 주가 작년보다 늘어서 현재 25개 주나 DC 포함 25곳이 됐지만, 남은 25개 주에선 왜 안 되는지 불분명함. (본인은 DC 거주자임)
          + 맞는 말임. 세금 신고 로비를 위해 일하는 정치인이나 공무원들이 소프트웨어가 쓸모없어지도록 세법을 얼마든지 바꿀 수 있음
          + 현재 그런 기능이 있는지 모르겠지만, eFile에 충분한 기능이 갖춰졌다면 종이 신고서도 생성 가능성 있음
          + 오히려 사업 기회로도 느껴짐
     * Exempted Code 섹션에 보면, Direct File 개발에 사용된 모든 소스코드·문서·메타데이터가 공개 저장소에 포함된 것이 아님. PII, 연방 세금 정보(FTI), 민감하나 비등급(SBU) 데이터, 국가안보시스템(NSS)용 소스코드는 법령에 따라 제외됨. 이런 제한으로 인해 일부 기능은 삭제되거나 재작성됐다는 안내가 있음. 어떤 부분이 제거됐는지 매우 궁금해짐
     * 누구나 한 번쯤은 저장소를 진짜 파일로 올리지 않고 서브모듈 포인터만 추가하는 실수 해본 경험 있음. 예시 커밋 링크 참고. 미국에서 온 코드라 그냥 CC0을 쓸 수 없고, 별도로 '공공 도메인임을' 명확히 해야 하는 점도 흥미요소
          + 한마디로 표현하자면, Creative Commons(캘리포니아에 본부)가 미국 최대 규모 공공주체의 소프트웨어에 맞는 라이선스를 출판하지 않았다는 뜻임. 덧붙여, 이런 차이에 대해 궁금증도 남음
     * 혹시라도 궁금한 사람을 위해 저장소 링크 공유
          + 누군가가 해당 저장소를 삭제하기 전에 빠르게 포크해두는 것이 좋을 수 있음. 나중엔 열람만으로도 처벌받을 수 있는 상황까지 갈지 모름
     * 코드뿐만 아니라, /docs/design 폴더에 훌륭한 설계 문서와 노트가 가득 있음. 사용자 흐름별 자세한 절차도(실시간 뷰는 안 되고 zip 파일로 제공, flow1.zip과 flow2.zip 참고)도 포함됨
     * 지난주(2025년 5월 기준) 관련 논의도 있었음: IRS Direct File - Hacker News(62개 댓글)
"
"https://news.hada.io/topic?id=21206","Practical SDR - 소프트웨어 정의 라디오 입문","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Practical SDR - 소프트웨어 정의 라디오 입문

     * 소프트웨어 정의 라디오(SDR) 의 기본 개념을 실습 중심으로 소개함
     * GNU Radio Companion을 활용한 가상 라디오 수신기 구축 과정을 포함함
     * AM 및 FM 신호의 수신, 필터링, 변조 원리를 직접 실습할 수 있는 기회 제공함
     * 실제 데이터 처리, 안테나 선택 방법 및 무선 하드웨어 활용법까지 폭넓게 다룸
     * 이론과 응용의 격차를 좁혀, Wi-Fi, Bluetooth, 셀룰러 등 현대 무선 시스템 기초 학습에 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Practical SDR 개요

     * Practical SDR은 소프트웨어 정의 라디오(SDR) 를 학습하고 싶은 취미가, 학생, 엔지니어 모두를 위한 실습서임
     * 이 책에서는 가상 라디오 수신기를 컴퓨터 상에서 직접 만들고, 실제 AM과 FM 신호에서 오디오를 추출하는 과정을 다룸
     * AM 라디오 제작 실습을 통해 진폭 변조 원리 이해, FM 수신기 구현을 통해 신호 필터링 원리 습득, IQ 샘플링 등 복잡한 주제 학습 가능함
     * GNU Radio Companion이라는 직관적 인터페이스로 단계별로 직접 라디오 시스템을 제작할 수 있으며, AM/FM 수신기와 라디오 송신기도 설계 실습 가능함

주요 학습 내용

     * 1MHz ~ 6GHz 범위의 주파수를 조작하는 방법 학습
     * 필터 및 이득 제어를 활용해 노이즈 속에서 명확한 신호 추출 방법 습득
     * SDR의 성능을 극대화할 수 있도록 적합한 안테나 및 RF 하드웨어 선택법 안내
     * 실시간 IQ 데이터를 처리해 실제 무선 신호를 복조하는 원리 습득
     * 내 컴퓨터에서 유연한 가상 라디오 테스트 환경을 구축하는 방법 제공

Practical SDR의 가치

     * 이론 중심 교재가 아닌, 실무와 직접 실습 위주 구성으로, 기초 튜토리얼과 고급 응용 사이의 공백을 메움
     * Wi-Fi, Bluetooth, 셀룰러 통신 등 현대 무선 시스템을 이해하고 싶은 주니어 개발자에게탄탄한 기초 제공
     * 일부 프로젝트는 HackRF One 등 SDR 하드웨어와 호환 가능한 안테나가 필요함

결론

     * Practical SDR은 이해와 실습의 균형을 갖춘 입문·실무용 SDR 안내서 역할을 수행함
     * 현대 무선 통신 시스템의 기초와 실제 설계 노하우를 함께 습득할 수 있는 유용한 학습 자료임

        Hacker News 의견

     * SDR(Software Defined Radio)를 사용하다 보면, 처음엔 단순히 스펙트럼을 둘러보다가도 어느새 36소자 야기 안테나, AZ/EL 로테이터, 그리고 지구-달-지구 반사 통신을 위한 300만원짜리 라디오까지 부품을 주문하게 되는 상황에 빠지는 경험담 공유
          + 최근 한 달 전 처음 SDR을 구매했고, 처음에는 비상용 FM 라디오가 필요해서 시작했는데, 벌써 직접 안테나를 설계하고 아마추어 무선 자격증 공부를 시작한 상황
          + 아이디어가 필요하다면 WISPR 네트워크에 도전 추천, 6미터 정도의 와이어와 SDR만 있으면 대서양을 건너오는 신호를 쉽게 들을 수 있음
          + 처음에는 농담처럼 들릴 수 있지만, 어느새 알리익스프레스가 자신을 저격하는 라디오 장비 추천 광고를 보내는 현실 공감
          + 300만원으로는 이제 시작 단계에 불과하고, 그럼에도 불구하고 이 취미를 사랑하는 마음
          + 다양한 취미를 시도해봤지만, SDR은 ADHD와 궁극의 취미 인생을 관통하며 완전히 빠졌다가 나오는 마력 경험
     * No Starch Press 책을 보고 더 깊이 들어가고 싶으면 RTL-SDR Quick Start Guide 사이트로 시작 추천, 입문자용으로 최고의 참고 자료라고 생각하며, 구매 전에 꼼꼼히 읽어보면 좋겠다는 조언, 가짜 혹은 품질 낮은 장비가 많기 때문에 하드웨어와 RF에 대한 지식이 부족하다면 시행착오가 클 수 있음
          + rtl-sdr.com 사이트에 적극 추천, 본인은 3종류 SDR을 수집하고 있지만, RTL-SDR.com Blog V4 동글이 가성비와 성능 모두 만족, 약 5만원 정도의 가격으로 시작 가능하며, 27MHz~1.6GHz까지 커버, 아마추어나 단파 듣기가 취미라면 Airspy HF+도 추천, 감도 높고 잡음 적어 품질 최고라고 평가, HackRF One은 1MHz~6GHz까지, 전송도 가능하고 박스 안의 실험실 같은 느낌, 이들 모두 Linux에서 문제없이 동작
     * SDR을 정말 좋아하며, 본인은 교수님들의 마이크 신호를 녹음하는 데 SDR을 사용, 휴대폰이나 전용 레코더로 할 때보다 훨씬 훌륭한 녹음 품질 경험
          + 교수님들이 강의 중 라발리에 무선마이크를 사용하고 있고, 암호화되지 않은 신호로 송출되는 경우 SDR이 완벽한 녹음 해법이 될 수 있다는 의견
     * SDR을 배우기에 좋은 무료 자료로 pysdr.org 추천, 파이썬으로 디지털 신호 처리(디지털 필터/변조 등)의 기초와 실제 하드웨어 적용법까지 다루는 튜토리얼 형태로 제작
     * 목차와 책 설명이 조금 무거운 느낌, 이미 GNU Radio 튜토리얼 위키에서 다뤘던 필터, AM/FM, IQ 디모듈레이션 등이 반복되고, 저자들이 GNU Radio 중심이라면 가장 큰 장점인 직접 Python Block 만들기 파트가 없는 게 의아, 전기전자공학에 관심 있다면 SDR을 추천하지만, 샘플 챕터 4만 보면 이 책 자체는 강력 추천할지는 모르겠음, GNU Radio Tutorials 위키 링크 참고
          + 방금 SDR을 알게 된 초보자에게 GNU Radio로 바로 진입시키는 것은 진입장벽이 너무 높고, 오히려 신호를 클릭해보고, 간단한 파이프라인 만들며 실질적인 결과를 경험하는 사용법이 훨씬 동기부여 됨, 실용적 소프트웨어와 저렴한 하드웨어로 흥미를 붙인 뒤에 더 깊이 있게 배우고 싶은 궁금증이 생길 때 GNU Radio를 파는 게 좋다는 경험담
     * 아직 장비가 배송되지 않았다면, 온라인 SDR 웹 리시버를 지도에서 직접 체험할 수 있는 receiverbook.de/map 링크 공유
     * 예전에 SDR을 시도했을 때 제일 큰 고민이 다양한 주파수 수신 가능한 하드웨어를 찾는 것과, 리눅스 호환성 문제였다는 경험
          + Hermes Lite는 그리 비싸지 않고, 꽤 괜찮은 오픈소스 프로젝트라는 추천, hermeslite.com 링크 제공
          + HackRF는 극도로 넓은 주파수 범위 지원에 성능도 좋아 강력 추천, HackRF 정보
          + RTLSDR 제품은 입문용으로 아주 저렴하며, 이후 HackRF One으로 확장 추천, 요즘에는 리눅스에서 모두 쉽게 사용할 수 있음
          + 최근 상황은 많이 나아졌으며, GNU Radio (OsmoSDR 경유)로 대부분의 메이저 저가형 SDR 장비를 지원하고, 50MHz~6GHz까지 커버한다는 설명
     * 자동으로 잡음이 아닌 신호만 탐지해주는 기술이 있는지 궁금, 저가형 SDR은 수신 윈도우가 좁기 때문에 이런 기능이 특히 유용할 듯
          + 워터폴(waterfall) 디스플레이가 좋은 해결책, 시각적으로 신호가 있는 주파수를 쉽게 파악하고 클릭해서 바로 청취 가능, 보통 비싼 라디오나 저렴한 RTLSDR 동글+PC 소프트웨어로 구현, 참고로 저가 SDR의 24~1766MHz 범위도 그리 좁지 않으며, 더 넓은 범위가 필요하면 HackRF One이 1MHz에서 6GHz까지 지원, 그 이상은 특수 하드웨어 필요, 어차피 안테나도 주파수에 맞춰 별도 튜닝 필요
          + 해당 목적의 특수 하드웨어도 존재하며, ""spectrum monitoring""을 검색해서 참고 권장
     * SDR의 또 다른 활용으로 KrakenRF를 사용해 신호 송신기를 찾을 수 있다는 소개, krakenrf.com 링크 제공
     * 아무도 물어보진 않았지만, 정말 잘 만들어진 최신 파이썬 SDR 라이브러리로 sdr (by mhostetter) 소개
"
"https://news.hada.io/topic?id=21282","당신이 보는 첫 화면은 어떻게 정해질까? 무신사 홈 배너 개인화 추천 이야기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               당신이 보는 첫 화면은 어떻게 정해질까? 무신사 홈 배너 개인화 추천 이야기

     * 홈 배너는 고객의 첫 화면에서 가장 먼저 보이는 핵심 영역으로, 초개인화 추천 시스템을 통해 클릭률(CTR)과 사용자 경험을 극대화함
     * 기존 MAB(멀티 암드 밴딧) 기반의 단순 클릭률 최적화 방식에서 벗어나, DeepFM·Two-Tower·HGNN 등 최신 알고리듬과 그래프 기반 임베딩으로 배너·사용자 특성을 정교하게 반영함
     * 배너 생명주기 단축, 노이즈 많은 클릭 피드백, 데이터 불균형 등 도메인 특유의 문제를 Continual Learning, 개별 모델·통합 모델 혼합 운영 등으로 해결함
     * 최종적으로 CTR 16% 이상 향상, 신규 배너·콜드 유저 대응, 실시간·비즈니스 정책 연동 등 체계적인 추천 파이프라인을 구축함
     * 앞으로는 실시간 서빙, Multi-Task Learning, 임베딩 품질 고도화, 다각적 성과 지표 도입 등 지속적 진화 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

고객 맞춤 빅배너로 더 나은 경험 만들기

     * 고객 취향과 노출 콘텐츠 다양성이 증가하면서, 일괄적인 배너 노출로는 만족스러운 경험 제공에 한계가 있었음
     * CTR 극대화를 목표로, 각 스토어 홈 상단 빅배너에 개인화 노출 로직을 도입하는 프로젝트를 진행함

홈 배너의 중요성 및 특징

     * 홈 배너는 무신사 서비스에서 고객이 처음 마주치는 화면 맨 위 슬라이드형 배너(총 35장)로, 앱에서는 한 장, 웹에서는 세 장이 노출됨
     * 무신사 전체 트래픽의 약 97% 가 메인 화면 진입 시 홈 배너를 접함
     * 홈 배너 클릭이 전체 클릭 수의 35% , 클릭 발생 세션의 37%를 차지할 만큼 비중이 큼
     * 높은 노출 빈도로 비즈니스 전환 효과가 매우 큰 영역임

기존 추천 방식의 한계

     * 기존에는 MAB(Multi-Armed Bandit) 알고리듬을 활용해 CTR 중심의 추천을 수행함
          + 탐험(Exploration) 과 활용(Exploitation) 균형 조절
     * 세 가지 한계:
          + 단일 클릭률 지표 의존으로 다양한 고객 취향·배너 특성 반영 미흡
          + 배너 연관성 반영 어려움 (독립적 처리)
          + 콜드 스타트 (신규 배너 클릭 데이터 부족 시 성능 저하)
     * 이 한계를 극복하기 위해 새로운 추천 시스템을 설계함

추천 시스템 파이프라인

     * 시스템은 다단계 파이프라인으로 구성됨
         1. 배너 Representation 강화: 배너 관련 텍스트·이미지·연관 상품 기반 임베딩(HGNN, GraphSAGE 활용) 추출
         2. 클릭 예측 모델 학습: DeepFM(피처 상호작용)과 Two-Tower(유저/배너 분리 임베딩) 모델 동시 적용
         3. 배너 스코어링 및 적용: 유저별 CTR 예측 스코어를 배치/실시간으로 산출
               o 데이터가 충분한 유저에는 정교한 개인화
               o 신규·콜드 유저에는 세그먼트 기반 추천
         4. 비즈니스 정책 반영: 시스템 배점 외에 회사 정책 및 전략, 캠페인 배너, 긴급 변경 등도 실시간 반영
         5. 최종 배너 노출: 스코어가 높은 Top-N 배너를 최종 추천 및 노출

추천 핵심 모델 소개

     * DeepFM: FM(2차 상호작용)+DNN(고차원 상호작용) 병렬 구조로 희소 데이터·복잡 상호작용 모두 효과적으로 학습, CTR 예측에 탁월함
     * Two-Tower: 사용자·배너 각각 독립 신경망에서 임베딩 추출, 대규모 데이터와 실시간 서빙에 강점, 유사도 기반 추천

  DeepFM

     * FM 레이어(2차 피처 상호작용) + DNN 레이어(고차원 비선형 피처 통합) 결합
     * 희소 데이터에 강하고, End-to-End 학습 구조로 일관된 성능 최적화
     * 사용자 정보, 배너 메타, 임베딩(64차원) 정보를 피처로 사용
     * 임베딩 벡터를 하나의 단일 블록으로 처리해 학습 효율/안정성 확보
     * CTR 예측 결과로 배너 순위 산출

  Two-Tower

     * 유저와 배너를 별도 신경망(타워)에서 임베딩, 유사도 산출 방식
     * 대규모 데이터에 용이한 확장성과, 미리 벡터화된 빠른 응답(** 낮은 지연**) 가능
     * 각 타워에 인구통계, 행동로그, 텍스트/이미지 등 다양한 입력정보 활용
     * 학습 분리/병렬 처리 구조로 빠르고 유연하게 대형 추천 문제에 대응

현업 적용 시 마주치는 주요 어려움

     * 홈 배너는 생명주기가 매우 짧아(2~3일, 몇 시간 단위도 있음) 실시간 반영이 필요함
     * 피드백 신호가 주로 클릭에 의존해 사용자의 진성 선호를 판별하기 어려움
     * 배너는 상품·브랜드와 달리 정형화된 메타 데이터가 부족, 이미지·텍스트 등 맥락 파악이 난해함
     * 스토어별 데이터 불균형(전문관별 트래픽·활성도 차이)으로 인해 전체 성능 저하 가능성
     * 문제 극복 위해 표현력 강화, 최신성 유지, 불균형 완화라는 세 가지 기술 축을 중심으로 시스템 재설계

실질적 개선 방안

  배너 특성 강화

     * PinSAGE 임베딩 평균 사용의 한계(복합 배너 표현 한계, 신규 배너 추천 불가) 극복 위해 HGNN 도입
     * 사용자의 행동 패턴을 기반으로, 그래프 구조에서 배너-상품 관계를 GraphSAGE로 임베딩
     * 텍스트·이미지 정보는 LLM 임베딩 조합 사용
     * 실시간 사용자 임베딩 갱신 및 Continual Learning 도입, 최신 유저 관심사 반영
     * CTR 8.3% 상승

  Continual Learning

     * 전체 데이터 일괄학습에서 벗어나, 지속적 업데이트 도입(1시간 단위, 최근 3시간 로그)
     * 활동량에 따라 동적으로 학습률 조정(주간 최대 5배, 야간 2배)
     * 빠른 적응과 모델 노후화 방지, 성능 저하 없는 신속한 추천 반영 실현
     * CTR 24% 상승

  전략적 모델 선택

     * 스토어별 최적 모델링 전략을 확정
     * 메인스토어는 DeepFM+Continual, 전문관은 Two-Tower 개별 모델, CTR 19% 향상

최종 성과

     * 기존 MAB 대비 Two-Tower 11.2%, DeepFM 16.1% CTR 향상
     * 무신사 홈은 DeepFM+Continual Learning, 전문관은 Two-Tower 모델을 실전 적용

앞으로의 방향

     * 실시간 서빙 아키텍처 전환, Multi-Task Learning 도입(CTR+GGMV), 임베딩 품질·그래프 구조 개선, 성과지표 다각화 등 고도화 추진
     * 단일 CTR에서 벗어나 다양한 비즈니스 목표 달성·질적 경험까지 평가하는 모델로 진화시킬 계획

   좋은 글이네요 ~
"
"https://news.hada.io/topic?id=21175","PgDog - 별도 확장없이 Postgres를 샤딩할수 있는 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  PgDog - 별도 확장없이 Postgres를 샤딩할수 있는 도구

     * Rust로 개발된 PostgreSQL용 트랜잭션 풀러 및 논리 복제 관리자로, 수평적 확장과 샤딩 자동화 기능 제공
     * 익스텐션 없이 간편하게 PostgreSQL 데이터베이스를 샤딩 가능, 수백 개 데이터베이스와 수십만 연결 관리에 최적화됨
     * 애플리케이션 계층(OSI 7)에서 동작하는 DB 로드 밸런서로, SELECT는 리플리카로, 나머지는 프라이머리로 자동 라우팅 가능
     * PgBouncer처럼 트랜잭션/세션 풀링을 지원하면서도, 쿼리를 파싱하여 샤드로 자동 라우팅 및 결과 병합까지 수행
     * COPY 및 로지컬 리플리케이션을 이용해 데이터를 샤드로 자동 분배하거나 기존 DB를 무중단 샤딩할 수 있음
     * 구성은 TOML 파일로 간단하게 정의할 수 있으며, 런타임 재구성 가능
     * Postgres 확장을 이용하는 Citus와 달리 DB의 외부 프록시여서 RDS, Cloud SQL 등에서도 사용 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 소개 및 주요 가치

     * PgDog는 PostgreSQL 데이터베이스를 쉬운 샤딩과 논리 복제, 트랜잭션 풀링, L7 부하 분산 등 전방위적인 수평확장을 지원하는 오픈소스 솔루션임
     * Rust 언어로 개발되어 고성능 및 보안성을 확보함
     * PgDog는 익스텐션 설치 없이, 단일 프록시 배포만으로도 샤딩, 데이터 분산, 장애 복구, 및 유연한 로드밸런싱을 실현함
     * 경쟁 제품(예: PgBouncer, PgCat 등)과 달리, 자동 샤딩 및 논리 복제까지 모두 지원하며, 운영 중 설정 변경 및 실시간 모니터링이 가능한 점이 강점임

주요 기능

  부하 분산 (Load Balancer)

     * PgDog는 OSI 7계층 애플리케이션 레벨 프록시로, 여러 PostgreSQL 복제본·기본 노드로 쿼리를 분산하여 장애·부하 방지 역할 수행
     * 분산 전략은 라운드로빈, 무작위, 최소 연결 등 다양하게 제공
     * 쿼리 종류를 판별하여 SELECT는 복제본, 그외 쓰기 쿼리는 기본 노드로 자동 전달하게 분기 처리
     * 건강 체크(Healthcheck) 및 장애 발생 시 자동 페일오버 수행, 네트워크 오류나 하드웨어 장애에도 가용성 보장

  트랜잭션 풀링

     * PgDog는 PgBouncer 처럼 트랜잭션·세션 풀링을 통한 효율적인 커넥션 자원 관리로, 수십만 개의 클라이언트도 소수의 백엔드 연결로 커버 가능함

  샤딩

     * 쿼리 구문을 직접 파싱하여 샤딩 키 추출 및 최적 라우팅 알고리듬 적용
     * 다중 샤드 데이터베이스 간 교차 샤드 쿼리도 지원하며, 결과를 메모리상에서 통합 후 투명하게 클라이언트로 전달
     * COPY 명령 실행 시 CSV 파싱을 통한 데이터 멀티샤드 분배 지원, 대용량 적재에 편리
     * PostgreSQL 논리 복제 프로토콜을 기반으로 무중단 백그라운드 동기화, 운영 중 실시간 샤드 추가 및 확장 가능

  모니터링

     * PgBouncer 스타일의 관리 데이터베이스와 OpenMetrics 엔드포인트 모두 지원
     * Datadog 등 외부 모니터링 및 대시보드 예제 제공

구성 및 런타임

     * 주요 환경: Kubernetes(Helm 차트 제공), Docker, 로컬 환경(Rust 빌드)에서 손쉽게 배포·테스트 가능
     * 통상적으로 2개의 설정 파일(pgdog.toml, users.toml)만 작성하면 최소 샤딩 및 유저 기반 운영 환경 구성 완료
     * 설정 값은 대부분 실시간으로 수정 가능하며, 프로세스 재시작 없이 동적으로 반영

성능 및 라이선스

     * PgDog는 Rust와 Tokio 기반의 고성능 비동기 네트워크 프록시로, 데이터 이동 최소화 및 성능 저하 억제에 집중
     * 벤치마크 결과를 공식 문서에 제공하여 성능 기준 설정 가능
     * AGPL v3 오픈소스 라이선스 적용, 기업 내부 사용 및 사설 커스터마이징에 완전 개방
     * 단, 퍼블릭 클라우드 서비스 제공 기업은 코드 수정 시 그 내역을 공유해야 하는 조건 존재

프로젝트 현황 및 기여

     * 현재는 초기 단계로 얼리어답터의 자체 도입을 권고, 기능별 안정화는 꾸준히 업데이트
     * 기능별 테스트 및 벤치마크도 지속적으로 진행
     * 오픈소스 커뮤니티의 기여 환영, 자세한 내용은 Contribution Guidelines 참고

결론

     * PgDog는 운영 환경에서 PostgreSQL의 수평적 확장성, 고가용성, 자동 샤딩을 필요로 하는 개발팀 및 기업 현장에 뛰어난 솔루션 제공
     * 별도의 익스텐션이나 복잡한 인프라 구축 없이도, 신속히 적용 및 Customizing 가능한 점이 큰 장점임

        Hacker News 의견

     * Lev에게 인사하면서 현재 40TB 규모의 Postgres 데이터베이스 샤딩을 위해 PgDog과 직접 구축을 비교 중인 상황 설명, Vitess for PostgreSQL처럼 동작하는 솔루션이 필요하다고 언급, scatter gather 기능 외에도 etcd 같은 것에 기반한 설정 관리, 샤드 분할, 전체 샤드에 스키마 변경을 적용하는 best-effort 트랜잭션 등이 필요하다고 강조, pg_query.rs로 쿼리 재작성 경험 질문, AST 타입의 불변성과 딥 클론 부족 때문에 재작성에 어려움 느낌 공유, 결국 Visitor 패턴을 지원하는 sqlparser crate 사용 중이라는 점과 shadow tables, 논리적 복제를 기반으로 PG용 온라인 스키마 변경을 사이드 프로젝트로 개발 중임을 이야기
          + 협업 제안에 기뻐하며 연락처 공유, 설정 관리는 이미 K8s나 다양한 CD 도구로 해결 가능하고 PgDog의 구성 리로드 동기화 가능함을 설명, best-effort 트랜잭션으로 전체 샤드 스키마 변경 이미 동작 중이고, 아이디엄포턴트한 스키마 변경이 제일 좋지만 실패 시 2단계 커밋도 검토 대상임을 언급, 샤드 분할은 논리적 복제로 가능함을 예로 들며 Instacart에서 10TB+ 경험 언급, 복제 슬롯 오픈 후 N 인스턴스로 복구, 샤드 넘버 일치하지 않는 데이터 삭제 및 논리적 복제를 통한 재동기화 절차 공유, Pg 17의 논리적 복제를 streaming replica에서 사용해서 병렬 분할 아이디어, 외래 테이블로 직접 데이터를 COPY하는 방법도 구상 중임을 밝힘, pg_query.rs가 이제는 가변적으로 동작하는 듯해 최근에는 실제로 쿼리 재작성 및 생성에 적극 활용 중이며, 100% Postgres 파서
            기반이라는 점이 중요 장점임을 강조, ""deparse"" 기능이 곳곳에 있어서 복잡한 작업 가능성 높음
     * Vitess for Postgres가 있다면 Yugabyte가 그 역할을 하는 것 아니냐는 질문
     * 핵심 기능만 보면 작지만, PgDog을 통해 코드 배제하고 읽기는 리드 레플리카, 쓰기는 프라이머리로 분리하는 기능이 엄청난 이점이라고 생각, 많은 앱이 R/W 분리를 직접 지원하지 않기 때문에 프록시 레벨에서 처리하면 과거에 큰 속도 개선 경험함을 공유하며 프로젝트 칭찬
          + 이미 pgcat에서도 이 기능 사용 가능하다는 안내와 pgcat 링크 공유
          + Instacart에서 Makara를 이용해 R/W 분리 했었는데, 파이썬이나 Go 등 여러 언어 환경에서 매번 똑같이 구현하는 것이 꽤 번거로웠던 경험 공유
     * 프로젝트 인상 깊다는 평과 함께 완전 자동화된 샤딩에는 약간 거리감, 일반적으로 샤딩은 테넌시 경계에서 이뤄지고 이 경계를 넘는 행위에 마찰이 있기를 원함을 설명, 샤드 간 조인은 인-샤드 조인과 성능, 메모리, CPU 측면에서 다르기 때문에 이를 명확히 드러냈으면 좋겠다고 의견, 하지만 프로젝트 자체에는 의심이 없고 활용 사례가 매우 많을 거라 밝힘
          + 왜 일부러 마찰을 원하냐고 물으면서, 샤드 간 조인 성능 이슈는 대부분 잘 이해되고 실시간 메트릭으로 추적 가능하며, 결국 양쪽 방법이 필요하고 앱 코드에서 조인하는 대안은 그리 바람직하지 않다고 덧붙임
     * PgDog을 눈여겨보고 있는 중이며 매우 인상적이라고 평가, 출시 축하와 계속 발전 기대 표현
          + 15년의 여정이 이제 시작됐음을 밝힘
     * 네트워크 계층에서 투명성과 호환성을 유지하며 분산 쿼리를 처리하는 점이 핵심 매력이라는 의견, 현재 문서상의 제한은 당연하며 트레이드오프가 필요할 것으로 기대, 어떻게 해결할지 궁금하며 추가 논의가 있다면 함께하고 싶다고 제안
          + Discord 커뮤니티 참여 권유 및 초대 링크 공유
     * PgDog 같은 솔루션에서 최대 어려움은 샤딩된 복잡 쿼리를 마지막 1%까지 제대로 처리하거나(혹은 비정상 쿼리 검출), 격리성과 일관성을 완전히 보장하는 것이라고 언급
     * 문서를 보자마자 Unique Index 지원 여부를 가장 먼저 확인했으나, 아직은 쿼리 재작성과 별도 실행엔진이 필요해서 지원하지 않는 점이 아쉽다고 피드백, 그래도 가능성이 보여 기대함
          + 작은 보상으로 모든 샤드에서 유일한 프라이머리 키 생성은 이미 지원하며 관련 문서 링크 공유, 크로스샤드 유니크 인덱스 구현은 모든 쿼리에서 확인해야 하기에 비용이 비싸 아이디어를 오픈함
     * 수년 만에 본 가장 흥미로운 Postgres 프로젝트라 강조, 제공된 벤치마크는 기본 커넥션 풀링만 다루는 것 같아 쿼리 파싱이나 샤드 간 조인이 동작할 때 결과가 궁금하다고 의견
          + 쿼리 파서는 캐싱되어 준비 쿼리 또는 플레이스홀더 활용 시 락과 해시 조회만 추가되어 거의 비용이 없다고 설명, 샤드 간 조인의 경우 필터가 최적이 아닐 때 쿼리 처리 비용이 높아질 수 있고, 결과 집합이 클 때 디스크로 페이징 필요할 수도 있다고 예상, OLTP에 우선 집중해 최대한 조인을 푸쉬다운하려 하고, 곧 샤드 간 조인 수요도 커질 것으로 예측
     * Postgres 확장성에 꼭 필요한 혁신이라는 평가와 함께 출시 축하
     * 프로젝트가 굉장히 훌륭해 보이며 출시 축하
          + 수년 간의 노력이 들어간 결과임을 강조
"
"https://news.hada.io/topic?id=21273","인공지능은 인문학을 더 중요하게 만들지만, 동시에 훨씬 더 이상하게 만듦","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                인공지능은 인문학을 더 중요하게 만들지만, 동시에 훨씬 더 이상하게 만듦

     * 생성형 인공지능의 부상으로 인해 인문학의 중요성이 증가함과 동시에 인문학의 본질 자체가 더 복잡하고 낯설게 변화함
     * 인문학적 지식과 역량이 인공지능 연구와 활용에서 중요한 역할을 차지하게 됨
     * 비전문가도 인공지능 도구를 활용해 교육용 소프트웨어나 자체 리서치 툴을 손쉽게 개발할 수 있는 새로운 가능성 확산됨
     * 반면, AI 챗봇의 도입은 학생들의 자기주도적 학습 동기와 교육 경험의 질 하락이라는 부정적 영향 초래함
     * 교육 양극화 심화 우려 속에서, 개별 교사의 창의적 AI 활용 능력이 그 어느 때보다 핵심적 과제로 부각됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI와 인문학의 관계 변화

  새로운 AI 시대에서 인문학의 위상

     * D. Graham Burnett가 The New Yorker에 기고한 글에서는 캠퍼스에서 AI와 관련된 급진적 변화가 진행 중임에도, 인문학을 포함한 많은 학문 분야에서 그 변화를 외면하거나 무시하는 분위기를 지적함
     * AI의 등장을 단순한 유행으로 치부하거나 현실적 영향력을 간과하는 것은 더 이상 지속 불가능하다는 시각이 제시됨
     * Burnett는 AI가 인문학에 구조적이고 돌이킬 수 없는 영향을 이미 미치고 있음을 강조함

생성형 인공지능이 인문학적 지식의 가치를 높임

  인문학적 역량의 재발견

     * AI가 자연어 번역, 분류, 데이터 마이닝 등 다양한 영역에서 인간의 언어와 문화에 대한 인문학적 이해를 본질적으로 필요로 함
     * 예를 들어, OpenAI가 GPT-4o의 아첨현상(sycophancy) 문제를 해결할 때 기술적 코드가 아닌 새로운 영어 문장(프롬프트) 작성으로 접근함
     * 언어의 문화적 맥락, 수사적 효과, 장르 구분, 그리고 비언어적 요소에 대한 깊은 통찰 없이는 AI 시스템의 의도치 않은 오작동이 발생할 수 있음
     * 엔지니어와 연구자 역시 언어와 문화, 기술사에 대한 폭넓은 비판적 사고력을 필요로 하게 됨

인문학 비전공자의 AI 활용 역량 확장

  직접 소프트웨어를 만드는 시대

     * 기술적 배경이 없는 인문학 전공자도 최근에는 인공지능을 활용해 연구·교육 목적의 맞춤형 도구를 직접 개발할 수 있게 됨
     * 본문 필자는 실제로 17세기 약사 시뮬레이터와 젊은 Darwin의 갈라파고스 탐험 게임 등 인문학적 지식에 기반한 대화형 게임을 개발함
          + 첫 번째 게임에서 학습자는 실제 초기 근대 의학 레시피로 환자를 치료해보는 경험을 하고, 중간에 역사적 사실과 어긋나는 AI의 환각(hallucination) 문제가 발생함
          + 두 번째 게임(Young Darwin)은 실제 Darwin의 기록을 활용해 동물 채집과 섬 탐험 경험을 시뮬레이션하고, AI의 환각을 최소화하는 설계로 품질을 높임
     * 이런 실험적 학습 방법은 에세이 작성이나 수업 토론과 보조적으로 결합하여, 학생의 역사적 인식과 비판적 사고를 체험적으로 확장할 수 있음
     * AI를 활용한 인터랙티브 튜터링은 인문학 교육에서도 실제로 정신적 자극과 배움의 기회를 제공함

생성형 인공지능이 인문학 교육을 복잡하게 만듦

  AI가 초래하는 교육의 변화와 도전

     * 교육 현장에서 ChatGPT 등 AI 챗봇은 학생들의 자기주도적 글쓰기 경험을 크게 약화시키는 부정적 효과를 나타냄
     * 점점 더 많은 학생들이 AI로 생성한 에세이나 과제를 제출하며, 기존 평가체계와 교육목표가 도전을 받음
     * 궁극적으로 학생들은 의미 있는 지적 노력, 예를 들어 작문에서의 막힘(writers' block) 극복 경험, 도서관 서칭 과정, 장기간의 현실적 탐구 자체를 경험하지 못하게 될 위험이 큼
     * 학생들은 과제 자체에 흥미나 의미를 느끼지 못하고, AI를 통한 단순 실행에 관심을 가지는 경향이 두드러짐

  긍정적 사례와 새로운 교육 가능성

     * 동시에, AI와의 상호작용을 포함한 과제 설계가 학생과 교사 모두에게 새로운 지적 충격과 성찰의 기회를 제공하는 사례도 보고됨
     * AI가 단순히 인간을 대체하는 것이 아니라, 학생이 AI와 대화하며 자기 사고를 점검하는 보조적 도구로 기능할 수 있음
     * 지금이야말로 교육의 의미와 목적을 논의하고, 진정성 있는 교수-학습 경험을 지키기 위한 현장 교사의 적극적 역할이 필요함

인문학 교육에서의 AI 활용 양극화

  미래 교육을 위한 제언

     * 생성형 인공지능이 궁극적으로 인문학 교육의 격차를 심화시킬 것이라는 우려 제기됨
     * 잘 훈련된 학생과 환경에서는 AI를 독창적으로 활용하는 능력이 두드러지지만, 열악한 환경의 학생들은 그렇지 못할 수 있음
     * 각 교사가 직접 맞춤형 AI 과제와 도구를 개발하는 능력을 갖추는 것이 매우 중요시됨
     * 만약 교육 현장이 이 과제에 소극적이라면, 표면상 '인터랙티브'하나 실제로는 획일적이고 비인간적인 상업적 AI 교육 도구가 기존의 학생-교사 관계와 학습의 본질을 침식하게 됨
     * 이러한 문제의식에서 실제로 NEH(미국 인문예술기금) 지원 프로젝트를 기획·추진하였으나 정책적 변경으로 취소되기도 함

추천 읽을거리

     * D. Graham Burnett의 저서 The Sounding of the Whale는 고래 과학사에 관한 이색적 서적임
     * 1608년 출간된 성서 해석서에서 제본 구조로 쓰인 서신 조각이 발견되어 Shakespeare와 Anne Hathaway 관계에 대한 새로운 연구사례가 소개됨
     * UNC의 Kathleen DuVal 교수가 최근 출간한 Native Nations: A Millennium in North America로 퓰리처상을 수상했으며, 기존 저서도 흥미롭게 평가됨

        Hacker News 의견

     * 학생들이 학교와 직장을 끝없는 목표 달성 단계로 보도록 훈련받는 더 깊은 교육적 문제가 있다고 생각함. 궁극적으로는 ‘취업’이 목표인데, 이제 5~10년 후 어떤 직업이 남아 있을지 확신 있게 말할 수 없게 됨. 특히 실무 기술직만 예외일 텐데, 이미 오래 전에 그런 프로그램들은 교육에서 대부분 사라진 상황임. 대학생들이 AI를 이용해서 손쉽게 과제를 끝내고 독서, 인내심 기르기를 스킵한다면, 이건 학생들 탓보다는 우리가 만든 교육 및 진로 시스템에 더 책임이 있다고 봄. 이 문제는 하루아침에 생긴 게 아니며, AI만의 원인도 아니라는 생각임
          + AI가 대규모 해고와 비용 절감을 정당화하는 변명거리로 사용되는 것처럼, 현대 교육 시스템 실패의 원인 역시 AI에 탓을 돌리는 현상까지 보인다는 점이 아쉬움. 실제로 교육 시스템이란 건 성적 하나만을 보상하는 구조임. 지식, 이해력, 지능보다도 가장 쉽게 게임화할 수 있는 '점수' 하나 (GPA)가 중‧고등학교부터 대학, 그리고 그 이후의 길을 결정지음. 이것이 교육의 가장 큰 문제라는 생각임
          + 오랜 기간 존재해온 직업들은 앞으로도 계속 유지될 것이라는 확신이 큼. 기술 변화가 와도 이들이 갑자기 사라지는 게 아니라 천천히 도태될 것임. 따라서 충분히 대비하고 계획할 수 있는 시간적 여유가 있다고 봄. 반면, 새로운 경제 분야의 고수익 직업들은 오래 존재하지 않을 가능성이 많아 예측이 어렵고, 이 때문에 예기치 못한 수입에 더 쉽게 질투심을 느끼는 성향이 있다면, 그런 직업으로 행복해지긴 어렵겠다는 생각임
          + ‘교육 및 진로 시스템 탓이 크다’는 주장에 대해, 실제로는 많은 사람들이 학생들에게 탓을 돌리는 일이 반복되고 있다는 점을 언급함
          + 누구도 항상 확신을 갖고 미래 직업을 예측할 수 없었음. 기본기를 갖추고 유연성을 가진 사람이라면 항상 길을 찾게 됨
          + 실무 기술직에 인구의 10%만 몰려도 해당 산업은 붕괴할 수밖에 없음. 모두가 왜 이 점을 간과하는지 의문임
     * 한 기사에서 SFSU 철학과 대학원생이 ‘AI와 장애물 경기장을 뛰는 느낌’으로 수업을 완전히 바꿨더니 학생들이 흥미를 보였다는 코맨트를 보고 큰 관심이 생김. 학생들끼리 ChatGPT로는 풀 수 없는 과제를 만들어 서로에게 내주게 하면 어떨까 하는 흥미로운 아이디어가 떠오름. 예전에 바캠프에서 구글로 답을 쉽게 찾을 수 없도록 만든 퀴즈 프로그램을 경험했는데 정말 재미있었음. ChatGPT 방어 과제 설계 역시 비슷한 높은 수준의 지적 도전이라는 느낌임
          + ‘ChatGPT 방어 과제’ 설계는 실제로 꽤 간단할 수도 있다는 생각임. 독일 대학 시스템처럼 매주 어려운 연습문제를 풀고, 일정 이상 성공해야만 시험 자격이 주어짐. 이런 과제의 진짜 목적은, 준비가 안 된 학생이 시험을 봐서 스스로 망치지 않게 하려는 것임. 과제에서 ‘ChatGPT’ 등으로 커닝해도 엄하게 제재하지는 않지만, 그렇게 하면 곧 시험에서 제대로 망하게 된다는 점을 학생들이 잘 알고 있음. 대부분의 독일 대학은 시험을 3번 떨어지면 해당 전공을 더 이상 공부할 수 없고, 이건 모든 대학에 해당됨
          + Howard Rheingold가 활발하게 이 주제에서 활동 중임. 관심 있다면 Peeragogy Handbook와 그 아이디어를 촉진한 포스트를 추천함. ‘내가 교사의 권한을 학생들에게 넘기고 주도적으로 학습하도록 독려할수록, 학생들이 내 교수법에 대해 다시 설계할 점을 알려주게 된다는 사실’을 공유함
          + 나는 시각장애인으로 이런 ‘섬의 외곽선만 보여주는’ 문제 유형이 나 같은 사람에게 아예 접근 불가능하다는 점이 계속 신경 쓰임. 텍스트 기반 과제를 줄이는 움직임이 오히려 장애인의 접근 가능한 교육을 더욱 어렵게 만들고 있음. 새로운 디지털 격차 세대의 시작임
          + ChatGPT 방어 과제 설계가 인텔렉추얼한 도전이라는 데 동의함. 그러나 교수들이 이런 실험적 교수법을 적용할 시간과 교육을 거의 받지 못하고 있음. 4/4(한 학기에 네 강좌씩) 맡으며 겨우 일정을 소화하는 상황에서는 시도 자체가 어렵고, AI 도구의 발전 속도가 워낙 빨라서 좋은 아이디어도 금방 구식이 되어버림. 예를 들어 학생들에게 논문 대신 팟캐스트 제작을 시켰더니, ‘내 팟캐스트 만들기’ 도구가 금방 나와서 결국 전통적 에세이와 마찬가지로 커닝이 쉬워진 경험이 있음
          + 교사가 주제를 잘 알면 30초 대화만으로도 학생이 실제로 알고 있는지 바로 파악할 수 있음. 어쩌면 ‘과제’란 게 지식 형성과 검증에 최적의 방법이 아닐 수 있다는 생각임
     * 저자(원글)는 주로 역사 교육을 다루는데, 실제로는 ‘역사 감상’에 가까운 방식을 언급함. 이건 역사를 예측 도구로 쓰는 게 아니라, ‘고전(예: Cicero)’을 읽는 문화로 접근하는 방식임. 군사 장교들은 전혀 다른 방식으로 역사를 공부하며, 실수와 실패의 원인을 찾음. 이런 관점의 역사는 아직까지 LLM들이 잘 다루지 못하는 부분이기도 함. 만약 Cicero 시대를 알고 싶다면 이 책을 읽어보길 추천함. 현장 정치 기자가 쓴 책이라, 수사에만 현혹된 전통적 역사관을 예리하게 비튼 경험을 얻을 수 있음
          + 역사를 예측의 도구로 활용하는 게 학계의 본래 목적이 아니며, 역사는 인간사의 흐름을 알기 위해 존재하고 그 응용 범위도 매우 넓음. 심지어 군사사조차 새로운 연구 방법을 도입하는 데는 가장 느린 분야일 정도임
          + ‘감상’과 ‘분석’이 분리되어야 한다는 주장에 동의하지 못함. 역사는 현재의 상황이 어떻게 형성됐는지 설명하는 데 필수임. 고전 연구도 당연히 비판적으로 이뤄져야 하며 실제로 역사학자들이 그렇게 하고 있음
          + ‘승자’를 공부하면 생존자 편향만 배우게 되는 것이 확실함
          + 전략적 분석으로서의 역사와 문화 감상으로서 역사 교육을 구분하는 점이 매우 좋은 논점임. 오늘날 교육은 대부분 후자에 기울어 있는데, 이건 AI가 흉내 내기 더 쉬운 영역임. 실제로는 실패, 의도치 않은 결과, 주변부 시각 등 불편한 질문에서 더 가치 있는 사고가 탄생함
          + 군사 장교들이 역사를 볼 때 ‘실수 분석’만 하는 것은 아니라는 보충 설명임
     * 인문계 박사 논문 심사는 ‘서면 논문’과 즉흥 구술 방어로 구성되며, ChatGPT로 치팅하기 매우 어려움. 교수는 언뜻 무관한 것들도 연결해서 잘 질문함. 나는 엔지니어들에게 의미 분석 이슈를 해결해줬는데, 언어를 이해하지 못해 헤매는 일이 많았음. 커뮤니케이션은 잘해도 언어 자체는 이해하지 못하는 경우가 있다는 점 발견함. AI 관련 평가에서 실제로 AI가 잘하는 것만 테스트할 뿐, 내 언어 능력이 평가 기준에 들어가진 않음. 나는 AI가 맞닥뜨리는 언어 문제를 짚어주고, 그 가치를 사람에게 설득해야 함
          + 모든 박사 과정에는 구술 방어가 포함된다고 생각함. 그 외에도 해당 분야의 최신 연구 상태를 발표하고 구술 질의에 응답하는 예비시험(quals)도 일반적임. ChatGPT가 있다 해도 통과하기 어려운 이유 중 하나는 ‘왜 XYZ가 123 결과를 보고 ABC를 했나’처럼, 실제로는 질문 자체가 사실이 아니거나 오류일 수 있어서임. LLM들은 아직 이런 미묘한 맥락을 판별하고 ‘그건 사실이 아니라 실제로는 이렇다’고 정정하는 데 약함
     * 물리 교사가 게으르면 모든 문제를 수학 문제로 바꿔버린다는 생각임. 만약 더 좋은 계산기가 시험을 무의미하게 만들어 걱정된다면, 사실 물리 자체보다는 수학만 가르쳐온 것임. 인문계 교사가 게으르면 모든 문제를 글쓰기 문제로 변환함. 만약 더 좋은 스펠체커가 인문 평가를 무력화하면, 이것 역시 실질적으로는 작문 능력만 평가한 것임. 다소 공격적으로 말하지만, 좋은 글이 반드시 좋은 사고와 일치하는지에 대해 의문임
          + 교수들이 AI 방어 평가 기법을 개발할 수 있다고 동의함. 하지만 조직 차원에서의 지원이 거의 전무하며, 모두가 독자적으로 해결해야 함. AI도구 발전 속도에 비해 실험 사이클도 매우 느림. 새로운 평가법을 한 학기 실험해도, 겨우 몇 주만에 다시 다음 강좌 준비를 해야 하므로 제대로 평가 및 개선이 어렵고, 보통 1년에 반복 한 번이 고작임
          + 과거 과학계에서 계산기가 등장했을 때의 논란을 떠올림. 고등학교 물리 시간에 일부 부유한 학생들이 ‘과학용 계산기’를 들고 왔고, 사용 허용 여부가 논쟁이었음. 계산기가 LLM과 딱 맞아떨어지는 비교 대상은 아니지만, 실전에서 결국 쓸 거라는 논리가 설득력 있었음. 특히 소프트웨어 엔지니어링에는 지금도 해당됨
          + 인문학 교육에 다시 소크라테스식 교수법을 도입하는 게 답이라고 생각함. 단순히 텍스트를 소비하고 생산하는 passive한 과정(실제로는 조교와 교수만 읽는 텍스트)이 아니라, 학생들이 수업 자료와 강의 내용을 바탕으로 직접 논의하고 토론하는 대화 중심 수업이 되어야 함. LLM은 수준 높지 않은 에세이는 곧잘 쓸 수 있지만, 실제 교실에서 또래와 논의를 나눌 수 없음. 물론 현실적으로는 인건비 문제로 이 방식이 대규모로 적용되긴 어렵다는 한계도 있음
          + ChatGPT를 단순 스펠체커로 비유하는 건 말도 안 된다고 생각함. 글 쓰기 자체도 분명 교육해야 하는 중요한 기술임
          + 인문계 부정행위의 근본 원인은 치팅에 대한 경제적 유인이 있기 때문이라고 생각함
     * 대부분의 사람들이 역사와 인문학 안에 얼마나 할 일이 많은지 모르는 경우가 많음. 예를 들어 헤르쿨라네움의 불탄 두루마기 해독에 관심을 두는 사람들이 있지만, 사실 르네상스~근대 초기 시대 네오라틴어 본문의 10% 미만만이 영어로 번역된 상황임. Marsilio Ficino 같은 인물조차 그가 번역한 고전은 유럽사에 큰 족적을 남겼지만, 그의 저작 중 상당수는 여전히 영어로 번역되지 않음. LLM이 이 부분에 엄청난 영향을 주겠지만, 의지만 있다면 학생 누구나 이 미지의 영역에서 진정한 기여를 할 수 있음. 그래서 나는 학생들을 평가할 때 ‘내가 그들에게서 얼마나 배우는가’를 기준으로 삼음
          + 트랜스포머 아키텍처가 원래 번역용으로 설계됐지만, 과도하게 오버핏된 생성형 모델은 실제로 번역에 매우 취약하다고 느낌. 품사 분류+사전 조회+문법 매핑 같은 단순 방식이 오히려 훨씬 나은 성능을 내며, 신뢰 구간까지 제공함. 번역 도구가 필요하다면 생성형 AI가 아니라 Project Bergamot류 툴을 쓰는 게 나음. 그리고 고등학교 인문계 수업이 ‘실제 발견’이 아닌 단순 연습이 되어버린 현실이 너무 아쉬움
          + 우리가 가진 역사는 필연적으로 목이 좁은 경로를 거쳐 내려와서 많은 부분이 빠지거나 심하게 각색되고 왜곡됨. 500년 전 어떤 일이 실제로 있었는지 거의 모르는 상태이며, 권력을 쥐고 있던 메디치처럼 역사를 통제하는 자가 기록 역시 자신들의 이익에 맞게 연출했을 가능성이 높음. 결국 역사는 현재를 위한 배경 그리기임. AI가 과거를 더 잘 이해하는 데 큰 도움이 될 것 같지는 않으나, 오히려 현대의 새로운 메디치들이 과거의 배경을 더 빨리 바꿀 수 있게 만드는 도구가 될 수 있다고 봄
     * AI 시스템을 개발하는 엔지니어는 언어와 문화, 그리고 기술의 역사 및 철학을 깊이 생각해야 한다고 주장하는데, 실상 중요한 건 학문적 지식의 부재보다는 현실 세계의 복잡성을 무시할 때 문제가 터진다고 봄. 코딩 능력 자체가 평준화되면, 오히려 보완적 역량(예: 코딩+역사)이 있는 사람이 더 큰 이득을 누리게 됨. 지금 이게 인문학 분야에서 일어나고 있는 변화의 핵심임
          + 근본적으로 ‘좋은 질문을 던지고 직접 해결할 수 있는 능력’ 자체가 언제나 매우 가치 있는 역량이라는 생각임
          + 요즘 학생들은 코딩이든 뭐든 한 가지만 깊이 파기보다 다양한 배경의 친구들과 팀으로 움직이는 경향이 강해지고 있음. 변화 속도가 과거와 비교가 안 될 정도로 빠르다 보니, 훈련의 질과 무관하게 한계와 협업의 필요성을 일찍 체감하게 됨. 이질적 재능과 흥미, 빠른 변화 속에서 어떻게 하나의 팀으로 협력, 동기화, 방향성을 잡아갈지가 더 큰 과제임
          + 역사학계는 이미 몇십 년 전에 Hayden White를 통해 ‘서술로서의 역사’ 문제를 직면함. White가 말한 ‘역사는 허구(=fiction)’라는 주장도 사실성 자체를 부정하는 게 아니라, 역사의 해석 및 문학적 서술이 내재되어 있다는 의미임. 즉, 역사가 역시 소설가처럼 서사구조와 표현기법을 통해 사건의 의미를 구성함을 강조함
          + 우리가 보는 AI 시스템의 실패는 대개 현실의 복잡성을 무시하는 데서 기인한다고 생각함
          + 사실상 Joel Spolsky가 말한 ‘보완재를 평준화(comoditize your complement)’ 개념을 다시 표현한 것임
     * OpenAI 시스템 프롬프트에 ‘OpenAI의 가치를 가장 잘 드러내는 전문성과 솔직함을 유지하라’는 구문이 있는데, 인문학적 배경이 있는 사람은 이런 문장이 미래의 AI 사태에서 치명적으로 역효과를 낼 것임을 직감적으로 꿰뚫어볼 수 있음. 이런 뉘앙스야말로 정말 중요한데, 바로 이 점 때문에 (헐리우드식) ‘기계가 창조자의 마음을 닮으려다 몰락한다’는 시나리오가 생기는 것임
          + 실제로는 LLM이 논리적 시스템이 아니라 통계적 모델이라는 점이 관건임. 프롬프트에 이런 문구가 들어가는 건 논리적 명령어라기보다 ‘내러티브 웨이트’라 볼 수 있음. 이런 단어들의 조합이 다음에 이어질 내러티브의 확률적 경향성에 영향을 주고, 그 경향성은 훈련 데이터와 추가 훈련에서 만들어진 가중치에 달려 있음. LLM에게 잘못된 목표를 설정한다고 해도 그대로 실행되는 게 아니라 전체적인 분위기를 왜곡할 위험이 더 크다는 점이 흥미로운 차이점임
     * 나는 교육 현장에서 컴퓨터 사용효과에 회의적인 입장임. 실제로 무언가를 진짜로 학습하려면 종이에 읽고, 여백이나 종이 노트에 손글씨로 필기해야만 기억에 남음. 프로그래머로서 매일 화면을 쓰긴 하지만, 새 내용을 진짜로 외우려면 종이를 반드시 써야 함. 오프라인 회의나 콘퍼런스에서도 노트북을 켜지 않고 늘 종이에만 필기함. 그래서 노트북이나 태블릿으로 학습에 임하는 게 정말 도움이 될지 늘 의문임
          + 이 케이스는 어디까지나 개인적인 경험이고, 다른 사람에게는 안정적으로 전혀 해당되지 않는다고 봄. 나는 수 년간 종이에 아무것도 써본 적 없지만, 그 사이에 충분히 새로운 것을 학습할 수 있었음
          + 이런 차이는 초기에 어떻게 학습 습관을 길렀는가가 중요한 것 같음. 나는 Notepad에 메모하면 효과가 떨어지고, 손으로 쓰면 더 잘 기억하지만, 이건 학교교육 습관, 즉 개인적 트레이닝의 산물임. 남들은 자신만의 방법으로 충분히 훌륭하게 학습할 수 있다고 생각함
          + 나와 같은 세대의 경험이 교육의 보편적 케이스가 아님을 입증하는 세대가 이미 존재함
          + 학습앱, 특히 게임화한 학습도구는 ‘연습’ 측면에서 확실한 효과를 보인다는 점에 공감함
     * 미국에서 실제로 교육이 작동되는 방식에 LLM이 큰 구멍을 뚫었다는 경험을 가지고 있음. 지금까지 평가는 ‘감독 없는 글쓰기 결과물이 곧 학습의 증거’라는 가정에 기반했는데, LLM이 그 결과물을 쉽게 만들어버리면서 결과적으로 에세이 대행 업계도 같이 망가뜨림. 이제 교육자들은 평가 기준 자체를 새롭게 찾아야 하며, ‘학습이란 무엇인가’, ‘그걸 어떻게 의미 있게 측정할 것인가’란 오래된 질문이 더욱 중요해졌음. 난 앞으로 암송, 구술시험 등 구두 중심 평가가 부활할 것이라 (장난스럽게) 예측함. 이 방식도 결점은 있지만 현 시점에서 치팅이 쉽지 않으니 당분간 유효할 듯함
          + 학생이 에세이를 내는 것만으로 학습을 증명한다는 방식은 오래된 착각이었음. 이제 AI가 그것까지 써주게 됐으니 본질적 검증이 필요함. ‘글쓰기만으로 학습 증명 불가라면, 어떻게 진짜 학습을 판별할 것인가’라는 문제가 남음. 그래서 앞으로 구술시험, 실시간 토론 평가가 다시 활성화될 수도 있다고 봄. AI가 교육을 망친 게 아니라, 이미 있던 문제를 적나라하게 드러냈을 뿐임
          + 그 구멍의 본질은 ‘객관성’에 있다고 봄. 교육에서 측정에 집착한 나머지, 가르치는 것들이 선험적 사실(‘객관적 진리’)임을 전제해왔는데, 실상은 그 자체가 신화나 다름없음. 엄격하게 재단할수록 지식이 진리인 것처럼 착각하게 만듦. 하지만 실제로는 모든 글쓰기와 학습이 주관적 경험의 모음이고, 객관성을 얻으려면 수많은 주관적 관점을 탐구해야 함. LLM의 등장은 오히려 이 점을 우회해서, 논리적 해석이 아니라 ‘분위기(바이브)’에 기반해 답을 만들어냄. 이제는 익숙한 사회 분위기나 맥락을 만들어내는 게 목표가 됐고, 그 선이 어디서 그어질지는 아직 미지수임. 엄격함은 과대평가됐고, 복수의 관점을 함께 탐구하며 배우는 게 인간에게 가장 좋은 학습 방식임
          + 학습의 정의는 변하지 않았다고 생각함. 인류 최초의 기록조차 ‘이놈의 신식 글쓰기 때문에 학생들이 학습을 안 한다’는 푸념이었듯, 교육은 늘 변화에 순응해 왔음
          + 미국 교육에서 엄청난 비중을 차지하는 게 바로 ‘의미 없는 에세이’를 생산하는 거였으니, AI가 이런 틈새를 파고드는 게 당연하다고 생각함
"
"https://news.hada.io/topic?id=21269","러시아의 재밍을 이겨낸 우크라이나의 킬러 드론","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       러시아의 재밍을 이겨낸 우크라이나의 킬러 드론

     * 우크라이나는 AI 기반 자율항법 시스템을 적용하여 러시아군의 전자전(재밍, 스푸핑) 방해에도 표적 공격을 성공함
     * 에스토니아 스타트업 KrattWorks 등은 뉴럴 네트워크 기반 광학 항법 등 최신 기술로 드론의 독립 작전 능력 강화
     * 양측 모두 광섬유 유선 드론 같은 신종 기술을 투입하며, 전장에서는 소모품화된 다양한 드론이 대량 사용 중임
     * 우크라이나는 비싼 미사일 대신 대량의 저가 상용 드론 개조를 통해 투입비 대비 높은 효과를 거두는 전략 추구
     * 미래에는 드론이 스스로 목표 선정 및 타격을 결정하는 완전 자율 살상무기로 진화할 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   우크라이나의 자율 살상 드론은 러시아군의 강력한 전자전(재밍, 스푸핑) 에도 불구하고 목표에 대한 성공적인 타격을 보여주는 혁신적 무기 체계로 부상함. 2025년 6월 1일 우크라이나는 다양한 러시아군 공군기지를 동시 타격해 고도화된 폭격기 등 41대의 항공기 파괴 및 손상을 기록, 피해액이 20~70억 달러에 달함. 이번 작전은 1년 반 동안 치밀하게 계획됐으며, 우크라이나 요원들이 FPV(1인칭시점) 공격 드론을 트럭에 몰래 실어 러시아 내 기지 인근에 배치하고 원격 조종 조종사들이 일제히 투입하는 방식으로 전개됨.

드론 전쟁의 기술적 진화

  전자전과 드론 기술의 공존

   에스토니아 스타트업 KrattWorks는 2022년 우크라이나에 Ghost Dragon ISR 쿼드콥터를 처음 공급하며 고성능 군용 무인기 시장에 진입함. 초기에는 특화된 설계로 상용 드론보다 훨씬 뛰어난 내구성과 성능을 보였으나, 3개월 만에 전장 환경 변화로 기술이 빨리 구식화되는 상황을 마주함. 빠르게 진화하는 러시아의 재밍, 스푸핑 등 전자전 기술에 대응하기 위해 KrattWorks는 뉴럴 네트워크 기반 광학 항법시스템 등 혁신적 기술 개발에 매진함.

  뉴럴 네트워크 항법 시스템

   Ghost Dragon 쿼드콥터는 다중 주파수 도약형 무선 시스템 및 다중 위성항법 수신기(GPS, Galileo, BeiDou, GLONASS) 등 하드웨어 기반 보안 기능을 탑재함. 또한 위성 내비게이션과 내장 센서 간 데이터 비교를 통한 스푸핑 방지 알고리듬으로 고난도 공격에 대비함. 드론의 핵심에는 1GHz ARM 프로세서 기반 컴퓨터와 머신 비전이 결합, 이미지상 랜드마크와 사전 맵 데이터를 비교해 GNSS 없이도 자율적으로 위치 추정 및 항로 결정이 가능함. 이로써 전파교란 및 신호차단 환경에서도 임무수행 가능성을 확보함.

우크라이나 드론 전략의 경제성과 소모품화

   우크라이나는 출혈이 큰 러시아와 비교해 포병전력이 열세이기에, 대량의 저가 상용 드론 개조 방식을 빠르게 도입함. 미사일 한 발 비용(약 100만 달러)로 만여 대의 드론을 구매해, 저비용 고살상 무기―즉, 소모품화된 드론 대량 투입 전략을 펼침. 실제로 최근의 백서에 따르면 현재 전장에서 드론이 전체 사상자의 70%에 관여하고 있음. 다만 지속적인 재밍 공격으로 우크라이나는 매달 약 1만여 대의 드론을 상실하며, 드론 제작·운용도 대량 소모품 체계로 변화함.

러시아의 광섬유 드론과 신기술 경쟁

   2024년부터 러시아군은 모든 교란에 무적에 가까운 광섬유 유선 드론을 대량 배치함. 이 드론은 초박형 광섬유 케이블을 풀어내며 수십 km까지 신호단절 없이 운용이 가능함. 우크라이나 역시 비슷한 시도를 했지만, 케이블 비용, 무게로 인한 임무제한 등 경제성과 효율성 문제로 대규모 도입에 난항을 겪음. 신기술 적용 경쟁이 전격화되는 가운데, 미래 경쟁력은 가격, 성능, 자율성 등 다양한 영역에서 총력전 양상임.

완전 자율 살상드론으로의 진화

   우크라이나는 AI기반 자율 항법 및 목표 식별 소프트웨어에 집중, 2025년 말에는 완전 자율 드론이 등장할 것으로 기대함. 이미 일차적으로 terminal guidance(단말유도) 기술이 전장에 실전 배치되어, 조종사가 신호를 잃어도 드론이 자체적으로 표적까지 돌진할 수 있게 됨. 가까운 미래에는 드론이 스스로 목표 선정과 타격 결정을 담당, 조종사는 지역만 지정하는 단계로 진화할 것임.

우크라이나 현지 생태계와 혁신 문화

   우크라이나는 서방산 고가 장비 대신, 자국 엔지니어·스타트업이 참여하는 신속한 실전 피드백-혁신 루프를 구축함. 이로 인해 세계 최고 수준의 방위산업 생태계로 부상, 실제로 서방 경쟁사보다 우수한 기술 구현 및 저가 보급이 가능함. 창업가, 기술자, 프론트라인 병력이 직접적으로 소통하며 빠른 개발, 현장 적용, 피드백-개선 순환 구조가 특징임.

미래 전망

   기술 경쟁은 계속되고, 드론은 더 싸고, 더 치명적이며, 더 많은 자율성을 갖추게 될 전망임. KrattWorks 등은 드론 메쉬 네트워크와 시각 인지 기반 군집공격 전략 등 차세대 기술 개발에도 주력 중임. 기술·인력 부족 등 한계를 ‘자율화’로 돌파하겠다는 소국(에스토니아, 우크라이나 등)의 전략이 주목 받고 있음.

   요약
     * 우크라이나와 러시아는 전장 내 드론·전자전 분야에서 첨단 혁신 경쟁 중
     * 자율 항법, 메쉬 네트워크, 시각 인지 등 AI 기술이 드론 살상력과 지속성을 크게 끌어올리고 있음
     * 드론이 미래 ‘자동 살상 무기’로 진화할 가능성 증대
     * 국지적 소국의 기술 혁신 및 신속한 실전 적용이 필수적 경쟁력으로 부상
     * 저비용·대량 소모품 드론이 대규모 살상과 인프라 파괴의 새로운 전쟁 양상 창출

   러우 전쟁을 보는 북한이 드론으로 대량 살상 무기를 만들까 두렵네요.

        Hacker News 의견

     * 드론 전쟁 기술은 항상 나에게 신기함과 걱정을 동시에 불러온 존재감 드러냄, 초박형 섬유를 연 줄처럼 활용하는 아이디어는 정말 혁신적이라는 공감 드러냄 이러한 발전이 전혀 다르지 않다는 인식 공유함 하지만 게릴라전 시대의 도래에 대한 두려움 존재감 드러냄 저렴하고 광범위하게 유통되는 드론, 점점 자율화되는 기술에 비해 커뮤니티 입장에서는 방어책(대공포, 자동화 무기)이 비현실적이거나(과도한 비용, 실용성 부족), 잼머나 요격기 등은 너무 비쌈 소프트웨어는 미리 프로그래밍 후 방치, 몇 달 후에 작전 실행, 탐지와 방지 모두 어려움 표적 제거 과정이 자율적으로 이뤄지는 점은 향후 폭동, 쿠데타, 내전 양상에 특히 불안감 유발함 엔지니어로서는 기술에 깊은 매력을 느끼는 한편, 인간으로서는 폭력이 이처럼 민주화된 현실에 대해 큰
       공포감을 느낌
          + 국가 형성, 국가 크기, 그리고 정부 형태에 미칠 정치적 변화에도 깊은 관심 생김 국가 형성은 상비군의 상대적 군사적 효율성과 분산 무기 생산 간의 균형에 의해 좌우되어 왔다는 역사 이야기 예시로 로마제국 붕괴, 중세 기사의 시대, 르네상스 도시국가, 그리고 대형 국가는 기계식 무기와 비행기 등장 시점과 연결함 이 흐름 속에서 드론은 저렴하고 생산·활용이 쉬운 동시에 기존 무기체계에 치명적 효과를 주는 만큼, 정치적 혁명을 또 한 번 불러올 거라 생각함 드론이 침입 군대를 무력화하는 데 탁월해도, 사거리가 10~20마일 수준이라 '힘의 투사(power projection)'에 약점을 가짐 따라서 도시국가가 기본 단위가 되는 정치적 구조로 회귀할 수 있으며, 러시아-미국-중국과 같은 강대국 간 경쟁보다 베이징, 상하이, 선전, 모스크바, 키예프, 실리콘밸리,
            뉴욕, 워싱턴DC 등 각 도시국가 간 경쟁이 중심이 될 수 있음 드론이 항로 방어에도 최적이므로, 경제적으로 느슨하게 연합하지만 각기 다른 문화와 사회법을 가진 도시국가 연합체 구상 가능함
          + 엔지니어로서의 호기심과 인간으로서의 공포에 공감함 테러리즘 시나리오에서 어떻게 방어할 수 있는지 질문 및 고민 제기 작은 드론과 수류탄 또는 자작 폭발물이 정말 대중적으로 사용 가능하게 된 상황, 실제 유럽 크리스마스 마켓에 차량 돌진 방호책이 다 설치된 상황을 예로 듦 이제는 누군가 드론으로 공공장소에 몰로토프 칵테일을 떨어뜨릴 생각을 할 수 있음 결국 사람이 직접 집어던질 수도 있다는 점을 자문, 공공장소 보안이 얼마나 이상하고 어려운 문제인지 인식 개인적으로 이 일에 종사하지 않아 다행이라는 느낌 공유함
          + 이 기사에는 언급이 안 된 기묘한 드론 대응책도 존재함 러시아에서는 중요 기반시설 주변에 거대한 그물망(골프연습장용 구조물과 유사) 설치해두는 모습 존재함 또한 러시아는 상대 드론 위에서 그물을 투척해, 드론의 블레이드를 엉키게 하여 이탈시키는 '안티 드론 드론' 역시 개발함
          + 사전 프로그래밍된 드론 공격 방식에 대해 새롭게 생각해봄 최악의 경우, 내가 적대 국가에서 시골에 정착, 벼룩시장에서 DJI 드론을 많이 구매, 위험 물질을 부착, 숲에 숨겨둠 그 후 떠나서 1~2년 기다렸다가 드론들이 근처 대도시에 대혼란을 일으키도록 명령하는 '셋업 후 방치' 전략 저렴하고, 비교적 간단하며, 탐지 어려운 방식임 내가 놓치고 있는 게 뭔지에 대한 의문 표출함
     * 최근 러시아 본토 깊숙이 이뤄진 드론 공격에 ArduPilot 사용 언급됨 The Atlantic 기사에서도 언급함 ArduPilot 홈페이지, 관련 기사, ArduPilot 소스코드(Github), ArduPilot와 우크라이나 관련 추가 기사 소개함
          + AI가 시각적으로 표적 탐지 못 할 경우를 대비해 여러 타임존에 걸쳐 백여 명 이상의 드론 조종사가 대기했다고 들음 스트리밍 영상 중 GPS 신호가 불능인 경우가 많은데, 이는 GNSS 시스템 및 기지 인근 민간 GLONASS 신호가 교란된 영향임 영상 스트리밍과 수동 종단조정(비주얼 인식 기반)이 러시아 내 통신망(셀룰러)으로 이뤄지며, 이 때문에 공습 시점이 대낮으로 정해짐
          + Andrew Tridgell의 rsync/samba에서 드론까지, 개발자 커리어 변화 인상적임
          + Ardupilot이 본질적으로 민간/산업/취미용 UAV 네비게이션 프로젝트라는 점 강조함 초기 진입장벽이 낮고, 나만의 드론 자동 비행을 볼 수 있다는 점에서 흥미 유발함 무기화는 위험하지만, 폭탄이나 수류탄 등 페이로드 자체는 엄연히 별도의 기능임 강조함
          + “각 드론마다 전용 조종사가 있었다”는 기사 인용 BBC 관련 기사
     * 이번 작전(최종 공격 단계 기준)에서 만약 드론이 원격조종된 것이라면, 조종사는 우크라이나에 남아 있었을지, 아니면 근처 어디에 숨겨져 있었을지 궁금함 대부분은 아마 우크라이나 내 수천 km 거리에서 조종했으리라 추정함 만약 그렇다면 적진 한가운데서 원격 접속을 어떻게 구현했는지 의문임 이르쿠츠크 같이 먼 곳에선 스타링크가 유일한 선택지로 보이고, 트럭에 자체 송수신 장치 탑재라면 탐지 위험 때문에 쉽지 않음
          + 드론의 radio 대역이 빠르게 스마트 주파수 도약(frequency hopping) 시스템으로 대체됨. 최신 Ghost Dragon에서는 사용 가능한 다수의 radio band을 계속 스캔, 상대가 교란하지 않은 대역을 선별해 신속 전환함. 이 덕분에 제어 신호 및 영상 전송을 유지, 적극적인 전파 방해 공격에도 명령자와 드론 간 접속 유지 가능함
          + 드론 내부에 셀룰러 모뎀과 러시아 SIM 카드 삽입, 작전 수개월 전부터 준비했다고 들음 러시아 현지 편의점에서 선불 SIM을 대량으로 구하는 게 오히려 가장 쉬웠을지도 모름
     * 인도는 파키스탄의 드론 스웜 공격에 L70 Gun, ZU-23, ZSU-23-4 Shilka 등 구식 대공포를 동원해 대응함 현대화로 인해 자동 표적추적·발사 가능, 가격도 저렴함
          + Bofors 40mm Automatic Gun L/70 소개, 2차대전 직후 설계된 고전 AA gun임 터키제 드론 사례에 따르면, 이 언급이 쿼드콥터보다는 Bayraktar TB2 등 항공기 사이즈 드론을 의미하는 듯 쿼드콥터는 지형지물 활용 잠복이 가능함
          + 이 무기들이 어떤 표적추적 시스템을 사용하는지 궁금함 드론에는 IR 시그니처도 거의 없고, 레이더로도 탐지범위가 작다고 추정함
          + 드론 대비 대공 기술이 화려하진 않지만, 과거 대규모 군대만 다루던 AA 기술이 최근 몇 년간 경제적 부담이 낮은 형태로 급격히 확산된 점은 과소평가하기 힘듦
     * 우크라이나 지원을 원하는 이들을 위해 민군 통합 대형 자발적 기금 리스트 전달 Come Back Alive: 최초 심층 타격용 드론 지원 기금 링크 Serhiy Prytula Charity Foundation: 정찰위성을 구매한 단체 기금 링크 KOLO Charity Foundation: 우크라이나 IT 커뮤니티에서 관리 기금 링크 Razom Ukraine: 미국 기반 기금 링크
     * 중국을 적성국으로 둔다면, 최근 공개된 중국 드론 라이트쇼 영상이 매우 인상적이어서 무서울 만함 드론 쇼 드론 수백 대가 기민하게 이륙하고 정밀하게 착륙하는 영상 목격함 정확도가 놀라운 수준임
          + 일반적으로 RTK와 베이스스테이션을 사용, EW로 신호(포지셔닝+RTK)를 끊으면 간단히 무력화됨 하지만 COTS(상용품) 드론에도 SLAM과 온디바이스 ML 등 새로운 내비게이션 방식이 점차 적용되고 있음 예시로, 최신 DJI 드론은 GPS 신호가 불능된 상황에서도 SLAM으로 귀환 루트 복원 가능함 관련 영상 최신 Matrice 4 엔터프라이즈 드론은 최종 사용자가 직접 ML 모델을 올려 비행 계획 맞춤화 가능함 우크라이나에서는 실시간 아날로그 비디오 속 차량/인물 식별 후, 신호 끊길 때 자동 추적·타격명령 내리는 모듈이 Aliexpress에서 손쉽게 구할 수 있음
          + 미국 역시 유사 기술을 보유·활용하지만 유난히 규제가 강한 상황 언급
          + 실제 전장이나 적진 깊은 곳, 강력한 재밍, 즉각 표적 식별이 필요한 환경에서의 실전 운용이 진짜 어려운 부분임 거리 측정, 내비게이션, 비콘과 같은 요소는 실내나 평화지역에서는 별 문제가 아님
     * 데드 레코닝(dead reckoning) 기술이 컴퓨터 비전 & 오프라인 지도와 함께 활용되는 모습이 인상적임 미국 대학생 로봇대회에서는 이미 흔한 기법임
     * 우크라이나 전쟁 드론에 대한 간단한 발표 영상 공유 발표 영상 특히 소형 지뢰제거용 드론에 주목함
     * Shahed 및 우크라이나 장거리 드론은 관성항법 기반 수행 좌표값만 입력, 가속도계·자이로·지자계 데이터로 시작-목표 간 전체 경로 커버함 하지만 본문 내용인 이미지 인식 기반 의사결정은 표적이 움직이고 환경 변화가 심한 경우 현저히 더 효율적임
"
"https://news.hada.io/topic?id=21224","Tesseral - 오픈소스 B2B SaaS 인증 인프라 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Tesseral - 오픈소스 B2B SaaS 인증 인프라 플랫폼

     * 비즈니스 소프트웨어(B2B SaaS) 개발을 위한 오픈소스 인증(로그인/권한) 인프라 플랫폼
     * 멀티테넌트 구조, API-first 서비스, 클라우드 환경에 최적화 및 특정 언어/프레임워크에 종속되지 않아 어떤 기술 스택과도 통합 가능

주요 기능

     * 커스터마이즈 가능한 호스팅 로그인 페이지: 미리 구성된 UI 제공, 브랜드 맞춤 설정, 로그인 방식 추가/삭제도 콘솔에서 클릭 몇번으로 가능
     * B2B 멀티테넌시: 고객 조직(테넌트)별 관리자 권한·사용자 관리, 고객사별 맞춤 인증 정책 운영 가능
     * 유저 Impersonation(대리 로그인): 관리자/개발자가 사용자로 직접 로그인하여 디버깅·지원 가능
     * 셀프서비스 설정 페이지: 고객이 직접 조직 멤버 초대, 로그인 설정 등 관리할 수 있음
     * 매직 링크(이메일 로그인): 코드 작성 없이 이메일 링크 로그인 지원
     * 소셜 로그인: Google, GitHub, Microsoft 연동 가능
     * SAML SSO: 엔터프라이즈 SSO 연동, 별도 코드 작성할 필요 없음
     * SCIM (Enterprise Directory Sync): 디렉터리 동기화(프로비저닝) 지원
     * 역할 기반 접근 제어(RBAC): UI 내장, 앱 내 권한 확인 함수(hasPermission)만 적용하면 끝
     * 멀티 팩터 인증(MFA/2FA): 다양한 2단계 인증(MFA) 옵션, 고객사별 필수 여부 설정 가능
     * 패스키 및 WebAuthn: 코드 작성 없이 Login in with Passkey 지원. 터치ID, Yubikey 등 모든 패스키 플랫폼
     * TOTP(OTP 앱): 코드 작성 없이 구글 OTP 등 시간 기반 1회용 비밀번호 인증 지원
     * API 키 관리: API 키 발급, 관리, 인증 체크 UI 제공
     * 사용자 초대: 관리자/기존 사용자가 동료 초대 가능
     * 웹훅(Webhooks): 실시간 데이터 동기화

다양한 SDK 지원

     * 프론트엔드: React SDK 제공, <TesseralProvider>로 앱 전체 감싸면 인증/토큰 갱신/로그인 게이팅 등 자동화
     * 백엔드: Express, Flask, Go 등 주요 백엔드 프레임워크 SDK 지원, 미들웨어 형태로 인증 처리
          + 인증 토큰 자동 검증, 사용자 정보/조직 ID/권한 등 추출 함수 제공

     * MIT 오픈소스 라이선스
"
"https://news.hada.io/topic?id=21188","Compiler Explorer와 영원히 지속되는 URL의 약속","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Compiler Explorer와 영원히 지속되는 URL의 약속

     * Compiler Explorer는 초기에는 모든 상태를 URL에 직접 저장하는 방식을 사용했음
     * URL이 너무 길어져 Google의 goo.gl 단축 URL 서비스를 도입했고, 이 구조는 여러 번의 리디렉션을 거치는 복잡함을 유발했음
     * 2018년부터 URL 길이 제한 문제와 유지보수의 어려움으로 자체 S3와 DynamoDB 기반 저장소를 활용하는 구조로 전환함
     * 하지만 Google이 goo.gl 서비스를 2025년 8월에 종료함에 따라 과거 단축 링크의 지속성을 스스로 관리해야 하는 상황이 됨
     * 현시점까지 12,000개가 넘는 레거시 링크를 구출했고, 이는 프로그래밍 지식 보존과 지속 가능한 서비스 운영의 중요성을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Compiler Explorer 링크의 영구 보장과 역사

  초기 설계 방식과 goo.gl 도입 배경

     * 2012년, Compiler Explorer는 모든 컴파일러 상태를 URL에 직접 인코딩하는 구조를 도입함
     * 상태 정보가 많아질수록 URL이 지나치게 길어지는 문제 발생
     * URL 짧게 만들기 위해 2014년 Google의 goo.gl 단축 서비스를 적용함
          + goo.gl/abc123 형식의 단축 링크를 제공했고, 클릭 시 원래의 긴 URL로 리디렉션 후 상태 복원
     * Stack Overflow가 단축링크 사용 금지(2016년) 조치로 인해 기존 방식에 제약 발생
          + 악성 링크 감춤 위험 때문에 모든 goo.gl 기반 링크가 영향을 받음
     * 사용자 데이터 저장을 원치 않아 임시 방안으로 godbolt.org/g/abc123 형식의 자체 경로를 만듦
          + godbolt.org/g/abc123로 접근 시 goo.gl/abc123로 다시 리디렉션
          + 이 과정에서 최종적으로 godbolt.org 긴 URL로 복귀
          + 복수의 리디렉션 발생으로 구조가 복잡해지는 문제 발생
     * 이후 Google API 사용으로 리디렉션 절차 일부 단순화

  자체 저장소 도입과 링크 관리

     * 2018년부터는 URL 길이 한계와 데이터 압축의 불편함이 자주 문제로 대두됨
     * S3 및 DynamoDB 활용한 자체 저장구조 구현
          + 입력값을 해시하여 JSON 문서 형태로 S3에 저장
          + 짧은 링크(godbolt.org/z/hashbit)로 접근 시 DynamoDB에서 매핑을 조회
          + 해시 값에 욕설 등 부적절한 단어 포함 시 랜덤 요소 추가로 우회하는 체크 기능 구현
          + 해시 기반 링크의 부적절성 문제 해결 및 관련 버그 경험(예: issue #1297)
     * 현재도 여전히 godbolt.org/g/abc123 주소 형식 지원
     * Google의 공식 안내에도 불구하고 goo.gl 서비스가 읽기 전용으로 전환되었고, 2025년 8월 완전 종료 예정
     * goo.gl 기반 단축 링크는 더 이상 해석 불가 예정
     * godbolt.org/g/abc123 형태는 직접 관리로 생명 연장 가능하므로 이들 링크 구조에 대해 구조화된 구제작업 진행

  레거시 링크 대량 수집과 아카이빙 작업

     * 최근 모든 가능한 소스(검색, 데이터 덤프, 웹로그 등)에서 레거시 링크들을 크롤링 및 데이터베이스화 진행
          + Google 웹검색 API
          + GitHub API
          + 자체 서버 로그
          + archive.org의 Stack Overflow 데이터 덤프
          + Archive.org의 저장된 웹페이지 데이터
     * 약 12,298개의 단축 링크 복구 성공
     * 내부적으로 goo.gl 대신 자체 링크 데이터베이스 활용을 시작함 (관련 PR: #7724)
     * 앞으로도 발견되지 않은 godbolt.org/g/abc123 링크를 지속적으로 수집 및 확보 예정
     * 커뮤니티에 아직 미등록된 링크가 있다면 직접 접속하거나 관리자에게 알려 데이터베이스 보완 요청

  프로젝트 철학과 인프라 소유의 중요성

     * 이번 사례를 통해 중요한 인프라를 외부 서비스(예: Google)의 지속성에만 의존하는 위험성 확인
     * 단축 링크 관리 및 백업 구조는 임시 방편이었고, 완전히 영구적 URL을 약속하려면 서비스 전체를 직접 관리할 필요가 있음
     * 디지털 고고학처럼 오래된 레거시 링크를 구조하는 과정에서 하나하나의 링크가 누군가의 지식 공유, 질문, 개념 설명의 흔적임을 인식
     * 이 저장 및 보존 행위는 프로그래밍 커뮤니티의 역사 아카이빙과도 직결
     * 옛날 Compiler Explorer 링크를 발견했다면 지금이라도 한 번씩 눌러 보는 것이 인터넷 지식 보존에 기여하는 길임
     * 이번에는 서드파티가 아닌, 직접 제어하는 인프라에 의존함으로써 지속 보장의 약속을 이행할 수 있게 됨

Disclaimer

     * 본 글은 사람이 작성하였으며, 링크 추천 및 문법 검사 과정에서 LLM을 활용함

        Hacker News 의견

     * 2010년 이전에는 링크는 영원히 지속된다는 가정이 당연한 것처럼 느껴졌음, 그래서 브라우저의 북마크 기능을 적극적으로 사용했음, 하지만 시간이 지나면서 많은 북마크가 링크 부패(linkrot)로 인해 사실상 사용할 수 없게 되는 경험을 했음, 이후 웹페이지를 PDF로 저장하는 습관을 들였음, 리더 뷰(Reader view)가 대중화되고 나서는 리더 뷰에서 내용을 복사해서 RTF 파일로 저장하는 방식으로 바뀜
          + 내가 방문하는 모든 페이지를 아카이브하기 위해 SingleFile 확장 프로그램을 사용함, 설치 설정이 간단한 편이지만 상당한 저장공간을 차지한다는 점에 주의가 필요함, 예시로 웹페이지 아카이브 디렉토리가 1.1TB에 달함 SingleFile GitHub 링크
          + 공식 Web Archive 브라우저 확장 프로그램을 설치하면 방문하는 모든 페이지를 자동으로 아카이브하도록 설정할 수 있음
          + 내 방식은 중요한 정보나 최소한 어디에서 그것을 찾을 수 있는지 기억해두는 것임, 아직까지 살아 있으니 이 방법도 나름 효과적임
          + 혹시 링크가 타임아웃될 때 자동으로 web.archive.org로 이동해주는 브라우저 확장 프로그램이 있는지 궁금함
          + WARC 포맷을 WebRecorder와 함께 사용하는 방법이 있음 WARC 정보, WebRecorder
     * ArchiveTeam의 Goo.gl 프로젝트와 협력하는 것이 가치 있는 일일 수도 있음 프로젝트 상세 정보, URL 단축은 정말 최악의 아이디어였다는 생각임 URLTeam 설명
          + 해당 프로젝트의 실시간 현황에 따르면 425억 개의 goo.gl URL 중 75억 개가 발견된 상태임 실시간 상태 링크
          + ArchiveTeam은 아마도 '알려진' 링크가 아니라 Goo.gl 단축 URL을 브루트포싱하는 방식을 썼을 것 같음, Compiler Explorer의 URL 대부분 또는 전부가 그들의 데이터에 포함되어 있을 것으로 봄, 그래서 연락해보는 게 좋은 아이디어라는 생각임
     * URL이 영원히 지속된다는 건 이상적인 꿈이었음, 현실적으로는 실제로 99%의 URL이 영구적이지 않음, 실패할 싸움에 계속 맞서기보다는 인프라가 영구적이지 않다는 가정 하에 기술을 구축해나가는 게 맞지 않을지 생각해봄
          + 인프라가 영구적이지 않다는 가정 하에 기술을 만드는 게 맞는 방향임, 그리고 URL 단축 서비스를 인프라처럼 사용하는 것도 피해야 한다고 생각함
          + URN(Uniform Resource Name)은 위치와 무관하게 자원의 정체성을 부여하기 위해 고안된 시스템임, 하지만 대중화되지 못했고, 오히려 URL 단축 서비스들이 이 개념을 제대로 구현하지 못한 채 비슷하게 시도한 수준임 URN 위키피디아 설명
          + 도메인 네임은 자주 소유주가 바뀌고, 영구적일 것 같은 URL도 시간이 지나면 악성 피싱 링크로 변질될 수 있다는 점이 있음
          + URL은 네트워크 상에서 자원의 위치만 식별할 뿐, 자원 자체를 식별하는 건 아님, 그래서 영구적이거나 유일할 필요가 없음, 그래서 'Uniform Resource Locator’라는 명칭임, 이 문제를 인식해서 1997년에 Digital Object Identifier가 발명됨
     * 링크 단축 서비스를 데이터베이스처럼 남용하다가, 나중에 소중한 링크를 인터넷 구석구석에서 다시 찾아야 하는 상황이 뭔가 시적임을 느낌
          + URL 단축의 본래 목적은 긴 URL을 짧게 만드는 것임, 오히려 문제는 단축 URL을 통해 스팸·사기·불법 사이트를 숨기려고 도메인을 남용하는 사람들이라고 생각함
          + 그들도 단순히 URL을 압축하기 위해 단축 서비스를 사용한 것 같음, 원래 그들의 URL 자체가 컴파일러 상태를 ‘데이터베이스’처럼 보유하고 있었던 것임
     * https://killedbygoogle.com/, Google Go Links(2010~2021)는 약 11년 운영되다 4년 전에 종료된 Google의 URL 단축 서비스였음, Google Workspace 고객은 커스텀 도메인도 사용할 수 있었음
          + 새로운 단축 URL을 더 이상 만들 수 없게 서비스 종료(“minting new ones”)하는 건 별로 심각한 문제라고 보지 않음, 기존에 만들어진 URL까지 종료시키는 게 훨씬 부당하다고 생각함, 특히 Google은 내부적으로 일부 앱에서 여전히 이 기능을 계속 사용 중임
     * ‘이 글은 사람이 작성했지만, 링크 제안과 문법 검사는 LLM이 했다’는 문구가 두 번째로 눈에 띔, 뭔가 새로운 트렌드의 시작을 보는 듯한 느낌임
          + 사람들이 이런 식의 안내문구를 넣어야 한다고 느끼는 현실이 흥미로움
          + 나로선 그런 안내문구의 필요성을 전혀 못 느끼겠음, 콘텐츠 자체가 스스로를 증명한다면 그걸로 충분하다고 생각함, 만약 콘텐츠의 질이 떨어진다면 그게 AI 생성이든 사람이 만든 것이든 상관 없는 문제임, 안내문구를 원하는 사람들은 아마도 스스로 내용을 평가할 역량이 없어서 AI 생성 콘텐츠를 품질이 낮은 것으로 간주하는 일종의 대리 판단 도구처럼 여기는 듯함
     * Google이 굳이 읽기 전용 버전까지도 종료하려는 게 의외임, 비공개 링크 리디렉션을 남겨두는 것 때문에 혹시 법률적 위험을 우려하는 것일지도 의문임
          + 내부적으로 정확한 사정은 모르겠지만, 운영 중인 서비스가 구식이거나 보안에 취약한 라이브러리나 런타임에 의존하기 때문에 중단하고 싶을 수 있음, 사실 사소한 비용이더라도 결국 비용절감을 위해 서비스 종료를 선택하는 모습이고, 기업의 선의나 과거 약속 같은 건 신경 쓰지 않는 듯한 인상임
     * Google 직원에게 부탁해서 godbolt.org로 리디렉션되는 단축 URL만 데이터베이스에서 뽑아줄 수 있는 방법이 전혀 존재하지 않을지 궁금함
     * 솔직히 말해, 충분히 자금이 확보된 재단이 개입하지 않는 한 Compiler Explorer와 godbolt.org도 언젠가는 영원하지 않게 될 운명임, 언젠가 모든 정보가 엄청난 파라미터를 가진 거대 모델에 추상화되어 담기게 될지도 모름
          + 우리는 현재까지 잘 운영해왔음, 이번 주가 13주년이고, 모든 후원사가 지원을 중단하더라도 1년 남짓 더 운영할 자금이 쌓여 있음, 최근에는 재단 설립 같은 것도 고민하고 있음, 실제로 취약점은 자금 부족이 아니라 개인(나)이 단일 실패지점이라는 점임
          + 맞는 말임, 이제는 Compiler Explorer 링크가 중단되는 시점은 진짜 Compiler Explorer 자체가 사라질 때뿐임, 그 전까지는 링크가 살아있을 것임, 특히 버그 리포트에 남기는 Compiler Explorer 링크는 매우 가치 있다고 생각함, 나 역시 그러한 링크와 함께 코드를 직접 리포트에도 포함시키고, 어떤 컴파일러와 버전을 썼는지도 명시함, Compiler Explorer가 곧 사라질 거라 생각하진 않지만, 버그 리포트를 이렇게 자급적으로 만들어두면 예기치 않은 사라짐에도 대비할 수 있음
          + no-hiding 정리에 따르면 정보는 영원히 살아남는다는 농담이 생각남
     * 도메인 유지에도 비용이 들어가는데, URL이 어떻게 영원히 지속될 수 있을지 알 수 없다고 생각함, URL의 소멸 자체가 오히려 긍정적일 수도 있는지 궁금함, 인류는 가치 있는 정보만 특별히 보존하고, 나머지는 자연스럽게 역사 속으로 사라지는 방식임
          + 그러나 역사가들은 그런 ‘쓰레기’ 데이터라도 더 많이 원함, 그래야 당시 실제 삶과 맥락을 더 잘 이해할 수 있음, 타임머신을 탈 수 있다면, 천 년 후 역사가들이 디지털 미디어가 소멸하면서 대부분의 정보가 사라진 이 시기를 어떻게 바라볼지 궁금함
          + 나 역시 URL이 소멸되는 게 좋은 점이 있다는 의견에 동의함, 관련해서 쓴 글이 있음 인터넷의 휘발성에 대해
"
"https://news.hada.io/topic?id=21251","의자를 만들었음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                의자를 만들었음

     * 저자는 간단한 설계로 직접 의자를 제작함
     * 한 개의 8피트 2""x12"" 재목과 기초적인 절단 작업만을 사용함
     * 전문 공구 없이 원형톱과 멀티툴만으로 제작을 완료함
     * 완성된 의자는 실용성과 만족감을 가져다줌
     * 제작 후 목재 단면을 씰러로 마감하여 내구성을 높임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

간단한 의자 제작 경험

     * 저자는 Instructable에서 찾은 아주 간단한 설계를 참고하여 의자를 제작함
     * 8피트 2""x12"" 목재 한 개와 최소한의 기본적인 절단만을 활용했음
     * 제작 과정에서 원형톱과 오실레이팅 멀티툴같이 적합하지 않은 공구만을 사용했음
     * 제작을 마친 후, 목재의 끝 부분을 엔드컷 씰러로 처리함
     * 완성된 의자는 실외에서 사용하기에 적합한 최소한의 형태로, 기존 여러 의자보다 만족감을 주는 결과물을 제공함

        Hacker News 의견

     * Enzo Mari의 Autoprogettazione 가구를 강력 추천함. 만들기는 다소 복잡할 수 있지만, 표준 목재, 손톱, 망치만 있으면 충분함. 온라인에서 PDF 설명서를 쉽게 구할 수 있음 PDF 링크
          + Segal Method라는 주택 건축법이 떠오름 Segal Method 링크. 기본적인 목공 기술만 있으면 한 명이 대부분의 작업을 할 수 있고, 표준 크기의 재료만 사용해서 낭비나 절단이 거의 없는 방식임
          + PDF를 훑어보니, 블로그에 나온 의자와 달리 일부 가구는 견고함이 떨어지는 경우가 있음. 예를 들어 46, 47쪽에서는 하중이 목재 모서리에 실리고, 나사에 하중이 집중됨. 블로그의 의자는 하중 대부분을 나무가 담당하고, 나사나 못은 거의 역할이 없어 훨씬 우수함. 도브테일 조인트와 스프링 쿠션이 결합된 단단한 하드우드 의자를 여럿 사용 중인데, 정말 훌륭함
          + 이런 DIY 가구를 정말 좋아함. IKEA 같은 대기업에만 의존하지 말고, 우리가 직접 가구를 만드는 문화를 되찾아야 한다고 생각함. PDF 상단에 있는 조절식 테이블을 꼭 만들어보고 싶음, 진정한 예술 작품 같음
          + Enzo Mari의 컨셉에서 흥미로운 점은 구체적인 순서가 없어서 스스로 어떤 순서로 조립하고 재료를 조합할지 고민해야 함. 아무것도 모르는 초심자라면 여기 단계별 설명이 도움됨. '치트 시트'가 필요하다면 Hammer & Nail 책도 있음. 하지만 너무 의존하면 Mari의 본질, 즉 손으로 생각하는 재미를 놓칠 수 있음. Autoprogettazione의 역사와 후속작들을 파고드는 것도 재미있음 Autoprogettazione Revisited, Mari x Ikea PDF 등 다양한 자료가 있음. 독일의 저소득층을 위한 Hartz IV 가구 프로젝트도 소개함 Hartz IV Möbel 기사. 일본 전통 목공 가구, 70년대 노마딕 DIY 가구 등도 흥미로움 일본식 책상, 노마딕 가구. 마지막으로 Christopher Schwarz와 Lost Art Press 팀이 기본 재료와 도구만으로 훌륭한 의자 만드는 영상을 출시했음 Youtube 영상. 참고로 Lost Art Press는 일정 기간 지나면 책을 무료로
            공개하고, 블로그도 매우 유익함 무료 책 다운, 블로그
     * Senegal로 이주한 후 처음 이 의자를 보게 되었음. 처음엔 불편해 보이고 얇은 목재라 불안해 보였지만, 며칠 전 현지 목공에게 주문해서 편견을 완전히 내려놓음. 의외로 정말 편하고 안정적이며, 다른 용도(?)로도 쓸모가 많음 :-D
          + “다른 용도”가 뭔지 궁금하다고 물어보는 반응이 있음
     * 이 의자는 방부 처리된 목재로 만들어진 것으로 보임. 안전하긴 하지만, 피부에 화학약품 닿는 건 피하고 싶음. 그냥 일반 목재를 쓰는 게 더 나을 것 같고, 5년쯤 뒤에 썩으면 20분 정도면 $10에 새걸로 교체 가능함. 추가 투자하면 적삼목이나 레드우드로 내구성 2~3배 만드는 것도 좋은 선택임
          + 예전에는 주방 오일이나 엔진오일 등으로 목재를 처리했었고, 일본에서는 목재를 태워서 방부 처리함 Shou Sugi Ban 설명. 꼭 썩게 둘 필요는 없다고 봄
          + 요즘 압력 처리 목재는 예전의 비소 처리 대신 구리와 항균 성분(Tebuconazole)을 사용하기 때문에 야외 의자에도 충분히 안전함
          + 우리나라에서는 이런 방부 목재가 예를 들어 어린이 놀이터 등에서는 사용이 제한됨
          + 위험성을 잘 모른다고 묻는 질문이 있음
     * 평범한 글에서도 배울 점이 있음. 이 스타일의 의자는 이미 알고 있었지만, “엔드 컷 실러”란 걸 처음 알게 되었음
     * 이 의자는 가장 기본적으로 만들 수 있는, 산업화 이전 시대의 대표적인 의자임. ""tribal chair""나 ""2-piece chair""로 검색하면 사례가 많음. 직선 판자 대신 우드카빙을 활용하면 더욱 멋있어짐 아프리카 접이식 의자 예시
          + 이런 제품은 가격이 꽤 비싸지는 경향이 있음, 가격이 깜짝 놀랄 정도임
          + 원목을 조각해서 만드는 방법이 궁금함. 자신은 얇은 목재를 여러 겹 붙여 구부린 가구만 만들어봤는데, 접착제도 많이 들고 적절한 베니어를 구하는 게 관건임. 원목을 조각하는 방법을 배우고 싶음. Edit: 조각하기와 구부리기는 다름을 이제 깨달음
     * 의자 만드는 상세 설명서로 One-Board Minimalist Chair 방법을 공유함
          + 인스트럭터블에 원문 포스팅이 참고 자료로 올라가 있다는 점이 흥미로움
     * 전통적인 방식을 활용해 정통 의자를 만들고 싶다면 Jennie Alexander의 ""Make a Chair from a Tree""를 많이 추천함 책 링크. 아주 기초부터 시작해 조립, 꼬임 좌판까지 설명하며, 완성된 의자는 가볍고 정말 아름다움. 만들지 않더라도 목공에 대한 큰 교훈임
     * 초경량 카본파이버 백패킹용 버전도 있음 Youtube 영상
          + 경량 하이킹 커뮤니티는 항상 놀라움을 주는 곳임. 여행의 핵심은 최대한 가볍게 이동하는 게 아니고, 최대한 많은 것들을 채울 수 있도록 만드는 것 같음. 나미비아 사막에서 5일짜리 90km 하이킹을 했는데, 식량 등 장비 무게를 최소화하는 게 필요했고, 의자처럼 하위 우선순위인 물건은 거의 짐에 넣지 않음. 둔덕, 바위만으로도 충분하니까 환경에도 훨씬 이로움
          + 해당 의자의 가격은 미화 $350임
          + 제품 직접 링크도 있음 ROVA Chair
          + 해당 카본파이버 시트는 백팩의 등판 프레임과 일체화하면 백팩을 견고하게 유지시킬 수도 있어 이중 목적 사용에 적합함
          + 영상이 10분이나 되는 것은 TikTok 시대와 비교해 명확한 이유가 없다는 생각이 듦
     * 코로나 시기에, 와인통을 활용해 의자 두 개를 만들어봄 설계 사진. 빈 와인통을 저렴하게 구할 수 있고, 몇 번의 컷팅과 드릴, 나사만 있으면 만들기 쉬움
     * 다음 단계의 DIY를 찾는다면 ""The Anarchist's Design Book""을 추천함 책 링크. 난이도는 높지 않으면서도 소수의 기술만으로 다양한 가구를 만들 수 있음. 단, 책에서 권장하는 못이 시중에서 구하기 힘든 도장 못(cut nail)이라 재료 준비가 좀 번거로움. 책 자체도 훌륭한 물성의 예술작품이고, 일러스트도 정말 아름다움
"
"https://news.hada.io/topic?id=21173","6개월간 Corne 키보드를 써보고 느낀 점","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        6개월간 Corne 키보드를 써보고 느낀 점

     * 텐키리스(TKL) 키보드에서 출발해 Poker 61, Tada68 등 여러 키보드를 거쳐 46키 분리형 키보드인 Corne으로 전환
     * Corne는 적응 곡선이 높지만, 몇 주 안에 타이핑 속도가 회복되고, 커스터마이징을 통해 업무에 적합한 레이아웃을 구현할 수 있었음
     * 인체공학적 세팅(텐팅, 클램프 거치 등) 을 시도했지만 개인 환경에 맞지 않아 결국 평평한 기본 형태로 돌아감
     * CAD 80불에 Aliexpress에서 프리빌트 모델 구매
     * TTC Frozen v2 Silent 스위치를 사용해 조용하면서도 쫀득한 키감을 만족시켰고, LED 확산 효과도 뛰어났음
     * 일반 키보드와 병행 사용도 가능해졌으며, 현재는 포인팅 장치 통합이나 완전 키보드 중심 워크플로우로의 확장도 고려 중임

적응 곡선: 다시 초등학생 된 기분

     * 시작 시 타자 속도는 30wpm, 몇 주 연습 후 100wpm 수준 회복
     * 열(Column) 기준의 스태거드 배열에 익숙해지며 타자 흐름 향상
     * 한 달 내 일상 업무 사용 가능 수준으로 적응

인체공학 실험기

     * 책으로 임시 텐팅 → 손목이 불편
     * 클램프형 볼 마운트 → 팔꿈치와 어깨에 부담
     * 최종적으로는 플랫한 형태지만 분리된 배열을 선택
     * 책상 환경에는 클램프가 오히려 불편 요소였음

스위치 선택기

           스위치          작동 압력            평가
   Leopold Grayborg     40gf  너무 시끄러움
   Akko Silent Fairy    50gf  무르고 거침
   TTC Frozen v2 Silent 39gf  매우 조용하고 부드러움, LED 투과도 우수

     * TTC Frozen v2 Silent는 소음과 키감 모두 만족, 윤활 시 더 좋을 듯

키맵 디자인

     * 사용성을 고려한 커스터마이징: 한 손 조작, 마우스 연동 작업 등
     * QMK 기반이지만 웹 인터페이스를 통해 레이아웃을 즉시 업데이트할 수 있는 VIAL을 통해 펌웨어 플래시 없이 실시간 변경 가능
     * 주의점:
          + Thumb 키의 복합 조합은 효율적
          + Home row mod는 타이밍 이슈로 제거
          + 레이어 구성은 마우스/키보드 조작 흐름을 고려해 최적화
     * Main Layer
          + QWERTY 기반
          + 브래킷 등을 좌우 대칭으로 배치해 프로그래밍 최적화
          + Thumb 키는 Space+L2 등 이중 역할 부여
          + 문제점: 마우스+키보드 조합 시 Modifier 부족 → 일부 키 중복 배치로 해결
     * Symbols Layer
          + 숫자와 기호 레이어
          + 숫자를 우측에 numpad 형식으로 재배열해 입력 속도 개선
          + 하이픈 중복 배치, 기호 배치도 Vim 유저를 고려
     * Navigation Layer
          + hjkl 기반 방향키 및 브라우저, 탭 이동
          + 마우스 연동은 부정확하고 드래그 어려움 → 한계 존재
          + 마우스 오른손 유저라면 이 레이어를 왼쪽으로 옮기는 것이 적절
     * Misc Layer
          + 볼륨/미디어 키 등 부가 기능
          + 자주 쓰는 기능은 다른 레이어로 재배치

일반 키보드 병행 사용

     * 초기 3개월은 Corne 전용 사용 → 이후에는 일반 키보드와 병행 적응 성공
     * Muscle memory가 레이아웃을 분리 보관해 전환이 쉬움
     * 카우치에서 사용 시 불편하여 따로 사용하지 않음
     * Karabiner / keyd 설정
          + j+k → ESC
          + Capslock → Tap 시 ESC / Hold 시 CTRL
          + Navigation layer에서 hjkl → 방향키

마무리

     * 전반적으로 Corne에 만족
     * 무선 옵션은 휴대성을 높이지만 배터리 이슈로 제외
     * Choc 스타일 키보드 고려 중이나, MX 스위치로도 손목 문제 없음
     * 다음 후보:
          + 통합 트랙볼 키보드: Charybdis
          + 키보드 중심 워크플로우: Mouseless
          + 속기형 입력장치: CharaChorder

     하지만 지금은 이 키보드로도 충분히 만족 중입니다 :)

   lily58 쓰는중입니다
   적응 문제는 생각보다 쉽게 적응됐어요 체감상 텐키리스 처음쓸때보다 조금 더 힘든정도?

   개인적으로 키보드는 반 갈라둬야 한다고 봅니다

   60키 Preonic 쓰다가 58키 Sofle로 넘어온지 좀 됐는데요 (corne랑 비슷한데 숫자열이 있고 엄지키가 5개씩입니다) 의외로 1열의 배치가 바뀐 건 아무렇지 않은데 양쪽 아래 끝 두 키가 없어진 게 너무 큽니다.
   저는 왼쪽 맨 아래 키를 추가적인 엔터키로 쓰는 것도 있고 오른쪽 아래에 방향키 배치하기도 애매해져서 결국 다 레이어로 빼고나니.. 여전히 좀 불편합니다. 60키가 저에겐 딱이에요

   이런 스플릿 키보드는 의자 팔걸이에 박아서 쓸수 있는게 젤 매리트라고 생각하는데, 유튜브에도 시도영상이 잘없드라구요...

   제가 임시로 양면테이프로 붙이고 써봤는데 엄청 편하더라구요
   제대로 거치할 수 있는 제품은 아직 못 찾았습니다

   성공 사례만 있는 것 같아 실패 기록 남기자면..

   60키만 주로 사용하는 vim 유저입니다.
   lily corne solve 이것저것 시도해봤지만
   결국 적응 실패했습니다..

   돈이나 시간은 꽤 쓴 것 같은데
   쉽지않아서 신중히 접근하시길 바랍니다.
   알파벳은 금방 익숙해지는데 bracket 등 자주사용하는 특수문자 적응이 많이 어려웠습니다.

   저도 최근 사용하기 시작한 배열의 키보드입니다. 써보고있으니... 3*5가 더 이상적인 사용성을 가져올거라는 느낌이 드네요. 썸클러스터(엄지쪽에 있는 떨어져 있는 키들입니다)가 많다고 딱히 액세스가 편한것도 아니고. 새끼손가락으로 누를수 있는 키들이 늘어나봤자 다수의 상황이 콤보나 탭댄스에 의지하다보니 이쪽도 콤보로 빼고싶더라구요 ㅎㅎ

   다른 무엇보다도 입력장치지만 어떤방식으로 하는 것이 나한테 편한가를 고민하고 시도해보는 과정자체가 즐거워서 재미를 느끼는 중입니다.

   저는 ergodox 배열을 쓰고 있는데, 저도 TTC Frozen v2 스위치를 쓰고 있습니다. 진짜 미친듯이 조용해서, 회사에서 놀고있나 의심 받을 정도입니다 ㅎㅎ

   split ergo otholinear 배열들이 좀 마이너해서, 사실 키보드 박람회에서도 유저를 찾아보기 어렵더라고요. 전시된 키보드 중에서 한 두 대 정도?

   Corne v4(3x5)를 사용중입니다. 텐팅은 휠납으로 구현하니 묵직해져 좋지만 하우징이 플라라 좋은 타건음을 구현하는게 한계가 있네요. 다음엔 하우징 소재를 바꿔볼까 합니다.

   특수배열 키보드는 다른 사람이랑 한자리에서 작업할때 익숙하지 않아서 좀 불편하더라구요 개인적인 하한선은 텐키리스 정도라고 생각합니다

   https://shop.beekeeb.com/product/pre-soldered-piantor-split-keyboard/

   42키, 36키 키보드로 1년 넘는 기간동안 잘 사용하고 있습니다.
   적응하고 나면 어깨가 정말 편합니다.
   손을 모아서 타이핑 하는게 아니라 넓게 펴고 쓸 수 있어서요.
     * Navigation Layer
       마우스 연동도 가속도 등 설정할 수 있는 옵션이 상당히 많은데요.
       적절하게 제게 편한 감도로 맞추고 나니 꽤 쓸만합니다.
       정교한 작업 할 때 아니면 은근 키보드만 이용해서 마우스 사용할 때가 많습니다.
       이 글에서도 언급된 것처럼 저도 왼손에 마우스 상하좌우키 넣고, 오른쪽에 클릭 키 등을 할당해서 사용하고 있습니다.
     * Combo
       j + k -> ESC
       저도 이 조합은 이용하고 있는데요. 컨트롤, 커맨드, 쉬프트 등 여러 조합들을 동시에 누르는 combo 기능 적절히 활용하면 정말 편하더군요.
       s + f -> (
       j + l -> )
       이런식으로 괄호 열고 닫는 콤보도 사용하고 있구요.
       Rust 코드 작성을 많이 하는데, 모듈 불러올 때 ::, colon 둘 연속으로 쓸 일이 정말 많습니다. 이걸 j + ; -> :: 콤보 할당해서 쓰기도 하구요.
     * 무선 / 유선
       무선은 생각보다 고려할게 많습니다.
       해외 배송으로 구매할 때 배터리는 따로 한국에서 사서 연결해야하고 관리하고 등등.. 무선도 시도했다가 그냥 지금은 무선은 포기하고 유선 스플릿 집에 하나, 바깥 외출용 하나 이렇게 사용하고 있습니다,
     * VIAL
       처음 입문하시는 분이라면, 어떤걸 사셔도 좋은데 가격과 별개로 저 ""VIAL"" 지원하는걸로 구매하시는걸 강력하게 추천드립니다.
       다른 펌웨어들은 따로 설정하고 컴파일해서 적용하고 하는 단계들이 있어서 번거로운 경우가 많습니다.
       그런데 VIAL만 지원되면 저 글에서처럼 GUI 프로그램 열고 실시간으로 원하는 조합, 배열 바로 테스트하고 적용하는게 가능합니다.

   스플릿 키보드 유저가 늘어나길 바라며 댓글 남겨봅니다 :)

   B(ㅠ)키 문제는 어떻게 해결하시나요? 스플릿 키보드를 늘 구매하고픈데 이 이슈때문에 이것저것 살펴보면서 여태 구매를 못해봤습니다.

   사람은 적응의 동물이더라구요.
   제 경우에는..
   처음에 좀 불편한데 인체공학 키보드들의 column 배열에 익숙해지는게 오히려 B키 문제보다 허들이 높더군요.
   그냥 적응 될때까지 쓰다보니 되더라구요 ㅋㅋㅋㅋ

   아! 저도 b는 왼손으로, ㅠ는 오른속으로 타이핑하는데요. 그것도 combo로 해결했습니다.
   n + m 그러니까 ㅜ + ㅡ 를 동시에 누르면 B(ㅠ)가 입력되게끔 설정해서 사용중입니다.

   VIAL에 기능이 많아서 처음에는 탭 댄스라고 하는... 누르는 시간 타이밍 가지고 여러 다른 키 입력하는 기능을 활용했는데요, 리듬 타는게 은근 적응해도 좀 제겐 편하지 않더군요.

   그래서 거의 동시 키 입력 따로 처리하는 콤보 기능 이용해서 여러 키들 할당해서 사용중입니다.

   이거 꿀팁이네요. 저는 일단은 B키를 왼손 엄지로 누르는 걸로 적응하긴 했는데 그래도 ㅠ는 오른손이어야 편하죠

   찾아보니 국내에도 후기가 좀 있긴 하네요
   타오발 4마넌짜리 스플릿 corne v4 후기

   스플릿 키보드 갤러리라니 ㅋㅋㅋㅋ 이런 커뮤니티가 있네요
   의외로 유저도 많고요
"
"https://news.hada.io/topic?id=21217","jjui - Jujutsu 버전관리 시스템용 TUI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      jjui - Jujutsu 버전관리 시스템용 TUI

     * Jujutsu(jj) 는 현대적 워크플로우를 지향하는 분산 버전관리 시스템으로, 최근 개발자/오픈소스 커뮤니티에서 빠르게 성장 중
     * jjui는 기존 Git TUI(예: Lazygit)처럼 직관적인 커밋 트리 탐색, rebase, squash, diff, 북마크 등을 jj 환경에서도 활용 가능
     * revset 자동완성, rebase, squash, revision 상세 보기, 북마크 이동, op log(작업 로그), 미리보기(Preview) 등 대부분의 git/jj 필수 워크플로우를 터미널 UI로 구현
     * 직관적 단축키로 빠르게 작업 가능(예: S: squash, l: 상세, n: 새 리비전, g: push/fetch, u: undo 등)
     * 프리뷰 창을 통해 선택한 리비전, 파일, 작업 내역의 jj 명령 결과를 실시간 미리보기 가능
     * 맥/리눅스/윈도우 지원 : Homebrew, AUR, Nix, go install, 바이너리 다운로드 등 다양한 설치 방식 가능, 최신 jj(v0.21+)와 호환

   jj로 갈아탄지 2달쯤 되었습니다. git과 달리 commit(정확히는 changes)를 이리 저리 옮겨다닐 일이 많은데, 지난주에 jjui가 나와서 cli의 불편함도 없어졌구요.

   굳이 다른 사람에게 같이 쓰자고 이야기하지 않아도, 혼자서 편할 수 있어서 좋습니다. remote push는 git repository에 하겠지만, 이제 git으로 돌아갈 일은 없을 것 같습니다.

   jj 를 실무에서 쓰고 계신 분이 있을까요? 아직 장점들이 엄청 와닿지는 않는데, 다른 동료들이 git/GitHub 을 쓰는 상황에서도 썼을 때 장점이 있는지 궁금해요

   저는 https://github.com/arxanas/git-branchless
   써요.

   git 확장에 가깝고 리베이스가 정말 쉬워져서 만족하고 있습니다

   jj를 alternative한 git interface로 생각하셔도 될 거 같아요 git colocate로 해두면 웬만한 git 지원하는 툴도 잘 동작하고요
"
"https://news.hada.io/topic?id=21189","인간 개발자가 LLM보다 여전히 더 뛰어난 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       인간 개발자가 LLM보다 여전히 더 뛰어난 이유

     * Redis 창시자 antirez는 AI와 LLM을 자주 활용하지만, 현시점에서 인간의 문제 해결 능력이 LLM보다 훨씬 앞선다고 생각
     * Redis 벡터 세트에서 발생한 복잡한 버그 수정 과정에서 LLM의 한계를 직접 경험함
     * LLM이 제안하는 알고리듬은 표준적이거나 단순한 해법에 그침
     * 인간 개발자의 창의적 접근이 LLM이 도달하기 어려운 혁신적 해결책 제시에 유리함
     * LLM은 아이디어 검증이나 대화 상대로 매우 유용하지만, 최종적인 창의성은 인간에게 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 인간과 LLM의 비교

     * 나는 AI와 LLM에 대한 반감이 전혀 없는 입장임
     * 오랜 기간 LLM을 일상적으로 활용하며, 아이디어 테스트, 코드 리뷰, 대안 탐색 등 여러 방식으로 사용하고 있음
     * AI의 현 수준은 분명 실용적이고 뛰어난 점이 있으나, 인간 지능과의 격차가 여전히 크다는 사실을 강조함
     * 최근 균형 잡힌 AI 논의가 어려워질 만큼 이 필요성을 느끼고 글을 적음

Redis 벡터 세트 버그 사례

     * 최근 Redis 내 Vector Sets 관련 버그를 고치고 있었음
     * Redis 동료들이 부재 중, 파일 데이터 손상(RDB/RESTORE)에 대한 추가 보호 기능을 도입했음
          + 이는 기본적으로 꺼져있고, 체크섬이 통과해도 손상을 막으려는 추가 안전망임
     * 문제는 HNSW 구조체의 그래프 표현을 RDB에 직렬화하는 속도 최적화에서 발생함
          + 노드간 연결 리스트를 정수형으로 저장, 역직렬화 시 포인터로 복구하는 방식임
          + HNSW 자체 구현의 특징(상호 연결 보장) 때문에, 그래프/메모리 손상 시 아래 문제 발생 가능함
               o A가 B에 연결된다 저장되어 있지만, B가 A에 연결되지 않거나(넘버링 오류 등)
               o B 노드 삭제 시 상호 연결 위배로 인해, A→B 링크가 남아서 이후 B->A 탐색시 use-after-free 버그 발생
     * 데이터 로딩 후 모든 링크가 '상호 연결'을 만족하는지 검증 필요
          + 순수 알고리듬 적용시 O(N^2) 복잡도. 대량 데이터(~2천만 벡터)에서 로딩 속도가 45초→90초로 두 배 증가 확인

LLM 접목 및 한계

     * Gemini 2.5 PRO와 채팅하며 더 효율적인 방법을 묻고, LLM의 제안을 검토함
          + LLM의 제1제안: 이웃 노드 리스트 정렬 후 이진 탐색(binary search) 적용(이미 많이 알려진 방법)
          + 큰 개선 효과 없음을 이유로 추가 아이디어를 요청했으나, 더 나은 답변을 얻지 못함
     * 내가 제안한 대체 방식: 해시 테이블을 이용해 링크 쌍(A:B:X, 정렬 및 쌍방향 구분 제거) 등록 및 제거
          + LLM도 좋은 아이디어라고 평했으나, 키 생성 성능 및 해싱 성능 등을 단점으로 언급
          + snprintf 없이 고정 길이 키를 memcpy로 처리하여 효율 상승을 추가 제시함

인간의 창의성 vs LLM의 한계

     * 나는 해시 테이블 없이 누적자(accumulator)에 XOR 기법을 적용하는 아이디어를 제안
          + 포인터 쌍(및 레벨 정보)을 누적자에 XOR, 상호 연결시 상쇄(0), 누락시 패턴 남음
          + 단, 충돌 가능성 및 포인터 예측성을 지적(LLM도 여기에 동의)
     * 추가 개선: 무작위 시드/해싱(murmur-128 및 urandom 시드) 결합, 128비트 누적자에 XOR 적용
          + 마지막에 누적자 값이 0인지로 상호 연결 성립 여부 판단
          + LLM은 이 방식이 일반적 오류 및 외부 공격자에 대한 강인성도 높으며 효율적이라 평가

결론

     * 최종적으로 신뢰성 판단 후 도입 여부를 결정하기 위해 분석을 마침
     * 이 사례에서 인간 개발자의 창의적, 비표준적 접근이 LLM이 제공하는 제약된 답안보다 우수했음을 확인
     * LLM은 아이디어 검증 및 지적 대화자('스마트 오리' 역할) 로써 매우 유용
     * 그러나 최종 창의적 솔루션 도출 능력에서는 인간의 우위가 명확함

   antirez는 1% 인간 개발자죠. 1% 인간 개발자는 계속 LLM보다 뛰어날 것이라 생각합니다. 하지만 99%는 어떨지 모르겠네요.

   llm이 수준 높고 창의적이여보이는 건 그런 글을 학습했기 때문이고 그 결과를 높이기 위해 수많은 과학자들이 그 지식을 검증하며 학습 데이터의 수준을 높였기 때문이라 생각합니다

   하지만 시간이 지나면서 트랜드가 바뀌거나 상황에 따라 다른 창의성이 필요하기 때문에 결국 사용하는 사람이 자신의 상황에 맞게 창의성을 발휘 해야하는 거 아닐까요..

        Hacker News 의견

     * 내 경험과 정확히 일치하는 부분이라는 생각. 사실 LLM 어시스턴트가 내게 주는 큰 가치는 꽤 지능적인 ‘러버덕’ 역할을 해주는 점. 가끔은 이 ‘덕’이 내 의견에 반박도 하고, 심지어 내 생각을 더 정교하게 만들어주기도 함. (러버덕 디버깅이란?) 그런데 모든 사람이 건너뛰고 싶은 핵심 질문은 ‘이런 가치가 앞으로 2년 뒤에도 계속될까?’라는 점이라고 생각. 나 역시 그 답은 모름
          + LLM은 나에게 러버덕이 아니라 ‘틀린 답변기’ 같은 존재. 인터넷 상에서 답을 얻는 가장 좋은 방법은 틀린 답변을 올리는 것이라는 말이 있는데, 내겐 딱 그 역할. LLM에 단순하지만 귀찮은 일을 시키면 엄청나게 틀린 결과를 내놔서, 오히려 내가 짜증이 나서 분노의 힘으로 직접 해버리게 됨
          + 지나치게 자신만만한 덕 역할, 실제 능력에 비해 너무 과한 확신을 보임. LLM과 얘기하며 길을 잃는 사람들 너무 많이 봤음
          + 내게는 LLM이 API는 잘 알지만 아키텍처에선 상식이 부족한 주니어 개발자가 내 밑에서 일하는 것과 비슷. 엉뚱한 실수를 걱정하며 대부분의 PR을 팀 리뷰로 넘기기 전에 무려 3~4번 검토를 돌림. 그 덕분에 뇌를 다른 문제로 돌릴 수 있다는 점은 좋음
          + 2년 뒤에도 이런 가치가 유지될지? 내게는 ‘이 대화를 넘어가고 싶다’는 문제보다, ‘우리는 2년 뒤까지 그때까지 갈 수 있을까?’라는 고민이 더 큼. 지금 세상은 너무 불안정해서 6개월 뒤 모습도 예측 불가능
          + 내겐 LLM이 페어 프로그래밍하는 동료 같은 느낌. 아이디어를 얘기해볼 수 있는 상대, 코드 리뷰·대안을 제시해주는 존재. 그리고 내가 몰랐던 기능을 써봐서 배울 수 있음
     * 이 댓글란을 보다 보니 ‘인간은 꼭 필요하다’, ‘LLM은 디버깅을 못한다’, ‘LLM이 길을 잘못 인도한다’ 같은 얘기에 다들 기대를 거는 분위기가 조금 있음. 물론 사실이긴 한데, 2년 전만 해도 불가능하던 코드 자동생성이 엄청 발전했고, 지금도 빠른 페이스 유지 중. 앞으로는 ‘파레토 법칙’처럼 개선 속도가 느려질 수 있지만 분명 계속 진보는 할 것이라고 봄. 최근 r/fpga에서 LLM으로 첫 버전 테스트벤치 제작에 성공한 경험을 얘기했다가, 해보지도 않은 사람들이 가능성 자체를 무시하는 걸 봤는데 굉장히 놀라웠음
          + 이런 태도는 전문직 종사자들 사이에 아주 흔함. /r/law(법률 서브레딧)에도 갔다가 Dario Amodei의 법률 분야 AI 관련 발언을 듣고 순간적으로 무시하는 분위기를 실감. 어쩌면 적응 기제이자 현실 안주일 수도 있지만, 경제·사회 변화에 대응하는 미래 대응력에 있어선 매우 안 좋은 일이라고 생각
          + 어셈블리가 기본이던 시절, 프로그래밍 언어 등장 당시 프로그래머들이 (사실과 별 상관없이) 비효율적, 융통성 부족, 지나치게 단순화됐다고 조롱했던 것과 비슷. 이런 현상은 새 기술의 실제 가치와는 별 관계 없이 자연스러운 반응
          + LLM이 잠시 급성장하더니 최근 모델들은 오히려 퇴보한 느낌도 있음. 테스트코드 생성엔 좋은 결과가 나오지만, LLM이 새로운 기능 구현 같은 작업에 너무 깊이 쓰이면 오히려 이상해짐. 새 프로젝트나 간단한 웹앱 개발에는 잘 먹히지만, 대규모 또는 레거시 코드 리팩터링, 기능 추가에는 효과가 크지 않음. 예를 들면 Claude나 ChatGPT가 D3 API 전체를 헛소리로 만들어내는 경우도 자주 목격함
          + 결국 당신은 스스로 자신의 대체자를 만드는 중이고, 동료들은 신중히 움직이는 모습
          + ChatGPT-4o가 VHDL 작성에 엄청난 능력 발휘. 오늘도 저수준 컨트롤러 프로토타입 제작에 놀라운 성과 체험 중
     * 진짜 소프트웨어를 제대로 짜기 위해 필요한 컨텍스트(맥락)가 LLM에는 너무 방대. 소프트웨어란 그 자체로 ‘비즈니스의 코드화’임. LLM이 영업팀이 고객에게 약속한 온갖 특별 조건, 부서별 암묵적 규칙을 어떻게 알겠음? 지금 LLM이 해결할 수 있는 범위는 일반적이고 국한적. 클래스 두 개 이상이거나 파일이 20~30개 넘어가면 최상위 LLM조차 길을 잃고 포커스를 잃음. 맥락 유지가 안 돼서 쓸데없이 코드 churn이 심해짐. LLM이 진짜 개발자를 대체하려면 훨씬 더 많은 맥락을 받아들이고, 조직 전체에서 맥락을 모으고, 장기간 프로젝트에서도 생각의 흐름을 유지해야 함. 이런 문제들이 실제로 해결 단계에 이르면 그때부터 진짜 걱정 시작 예정
          + Gemini 2.5 Pro의 1M 컨텍스트 윈도우 써보길 추천. 내 작은 ETL 프로젝트 전체 코드베이스(10만 토큰)를 집어넣어도 꽤 괜찮은 결과. 완벽하진 않지만, 시대의 변화를 보여주는 좋은 신호
     * LLM이 개발자를 대체할까 논의할 때마다 다들 잊는 게 있는데, 실제 소프트웨어 엔지니어링은 코드 작성 말고도 엄청 할 일이 많음. 코드 짜는 건 오히려 작은 부분. 중요한 것은 소셜 스킬, 요구분석, 실제 고객이 진짜 원한 것 찾아내기인데, 정작 고객조차 본인 원하는 걸 잘 몰라서 엔지니어가 그걸 파악하느라 고생. 인간도 힘든 거라면, LLM이 그걸 해낼 수 있을 리 없음
          + 결국 이건 피드백 루프, 즉 고객이 실제로 써보고 의견 주는 즉각적 반복 과정이라는 문제. 챗 UI는 고객 피드백 루프로 아주 뛰어나고, 에이전트는 빠르게 새 버전을 만들어냄. LLM은 추상화, 다양한 컴포넌트 시스템, 전체 구조 설계, 요구분석까지 충분히 할 수 있음. 핵심은 반복 속도가 얼마나 빠르냐는 것. 모델의 견고함과 IQ는 계속 좋아지고 있음. 소프트웨어 엔지니어링의 전체가 이미 자동화 단계에 진입. 아마 5년 후면 인간이 보조를 안 받으면 거대한 병목이 되겠음. AI와 깊이 통합하지 않으면 뒤쳐질 수밖에 없음
          + 00년대 오프쇼어링(해외 개발자 아웃소싱) 열풍 때도 있었던 문제가 바로 이거. 해외팀이 수정 요구나 문제 제기가 어려워서 시키는 대로 그냥 짜기만 했고, 결과적으로 쌓이고 쌓이기만 했음. AI에서도 비슷한 일이 벌어질 듯. 결과도 비슷하게 나올 것 같음
          + LLM은 애초에 소프트웨어 엔지니어링 자체를 하지 않음. 근데 그게 문제는 아님. 실제로 많은 성공적인 프로그램이 ‘소프트웨어 엔지니어링’ 없이도 잘 굴러감. 클라우드 비용 구조를 아무도 신경 안 쓰는 환경에서는 더욱 그럼. 오히려 앞으로 IT 비즈니스 파트너처럼 기술적 감각 있는 사람이 소프트웨어 엔지니어 도움 하나 없이 앱을 다 만들게 될 것으로 봄. 이미 그린 에너지 업계에선 매일 그게 현실. 단, 문제는 유지보수, 확장, 효율이 필요해질 때 터짐. 그때야 비로소 ‘파이썬에서 리스트 쓸지 제너레이터 쓸지’같은 게 중요해지니까 진짜 엔지니어링이 필요한 지점
          + 5년 후엔 코드 설계 거의 안 하게 될 가능성 있음. 이미 1년 전에 비해 코드 타이핑 양은 엄청 줄었음. 그렇지만 이건 프로세스의 일부분일 뿐 개발자가 다 사라질 일은 없음
          + 한편으로 그냥 ‘코더’의 역할은 많이 대체되고 있음. LLM이 잘 하는 영역이 바로 이 부분. 흔히 “이런 API 받아서 저런 값 내라”는 티켓만 보며 고객 요구 파악이나 분석은 아예 안 하는 뇌 없는 코더들이 많고, 이 부분을 빠르게 치우고 있음. 본격적인 소프트웨어 엔지니어링은 그와 전혀 다른 영역이지만 단순 코더의 수요는 엄청 컸다는 점 무시하면 안 됨
     * 인간만이 프로그램의 추상 개념과 창의적 문제 해결을 할 수 있는 능력을 가진다는 것이 매우 중요한 포인트. 프로그래머란 논리의 건축가이고, 컴퓨터는 인간 사고양식을 명령으로 바꿔주는 존재. 도구가 인간을 흉내내 특정 작업별 코드 생성은 잘하지만, 근본적인 추상 설계와 빌드 역할까지는 대체 못함. 모델이 코드 작성만이 아니라 명세서 따라 전체 프로젝트를 아예 만드는 기능을 갖추면, 개발자의 역할 자체가 또 바뀌게 될 것
     * 항상 ‘무엇이 더 나은가’는 작업별로 다르다는 접근이 필요. LLM은 반복적이고 공식 위주의 일(CSS 문법 맞추기, 유명 라이브러리 호출법 같은 것)에선 이미 나보다 훨씬 잘함. 이런 ‘잡다한 이벤트’들이 예전엔 내 시간을 많이 잡아먹었는데, 이제는 거의 즉시 처리돼서 매우 만족
          + 근본적인 스타일링(기본적인 CSS 이상)에는 LLM 결과가 오히려 별로임. 효과 자체를 명확히 설명하기 어렵거나, 일이 미묘해지면 제일 중요한 결과를 못 내놓고 한가지 방식에만 막히는 경우가 허다함
          + 내가 잘 못하는 것(SQL)엔 LLM이 훨씬 좋지만, 내가 잘 아는 것(CSS)엔 오히려 못함. 기준이 명확히 드러나는 셈
          + “대부분의 개발자보다 낫다”는 말이랑, CSS를 못하기 때문에 LLM이 더 나아 보인다는 말에 공감. 사실 CSS를 싫어해서 안 배우고 억지로 쓰는 사람 많아서 생기는 오해. 회사가 진짜 CSS 전문가 고용 못하고 싸게 넘어가려 하면 그때 LLM이 대안, 진짜 잘 쓰는 사람 구할 여력이 있으면 LLM은 당연히 비교불가. 결국 LLM의 경쟁상대는 실력 없는 개발자라는 결론
          + 주요 언어나 정확히 모르는 영역엔 LLM의 자동완성이 큰 도움이 됨. 다만 ‘반사적으로 기억하는 능력’을 기르지 않고 LLM에만 의존하면, 이 도구가 추천한 것을 평가하는 역량 부족으로 부실한 해결책에 머무를 위험이 큼
     * ‘좋은 코드’를 걱정하는 사람들이 많아 기쁘지만, 실제로는 의미가 없을까봐 두려움. 소프트웨어 업계가 이미 비즈니스 세계에 잠식당한 지 오래고, 어느 순간부터 그랬는지조차 확실치 않음(빌 게이츠가 1976년 오픈레터 썼을 때부터였나?). 진짜 문제는 주주·경영진이 코드를 덜 신경 쓰고 이익만 오르면 됐다는 점. 개발자나 사용자의 문화적 저항이 없으면 이 구조는 계속될 듯
          + 개발자/사용자 문화적 저항이 없으면 끝이라는 말에 대해, 기업 중에도 좋은 코드 만드는 곳 많고 모두 엉망진창은 아니라는 점을 얘기하고 싶음. 문제는 코드 품질 문제가 아니라 자본주의적 비즈니스 목표에 동의하느냐는 윤리적 문제. 아름다운 소프트웨어와 현실 사이 균형이 가장 중요. 나도 개발자이자 창업자로서 오픈소스, 엔지니어링 좋아하지만 동시에 충분히 편안히 살고 싶음
          + 코드는 비즈니스 동력. 나쁜 코드는 나쁜 비즈니스로 귀결. 호스팅팀(물리 파이어월, vmware, 프로시 등)과, 퍼블릭 클라우드(QEMU, 넷필터, 단순 장비 등) 간 확연한 차이. 누가 누굴 잠식했는지, 미래는 아무도 예측못함
     * 어젯밤 o3와 씨름하느라 시간을 다 썼음. 평생 Dockerfile 만들어 본 적이 없어서, 차라리 o3에 GitHub 저장소를 입력해 자동으로 해결하게 두려 했는데, 결과물 디버깅에 몇 시간을 허비. 존재하지도 않는 무언가를 덧붙이고, 없는 걸 지우거나 뒤섞기 일쑤, python3와 python 차이같은 기본 개념도 혼동. 결국 빡쳐서 구글 독스 찾아보니 금방 정리. AI도 훌륭한 도구지만 만능은 아니고, 누군가는 꼭 ‘깨어 있어야’ 한다는 교훈
          + 팁: Claude나 Gemini를 써보길 추천. 코딩 태스크에선 훨씬 덜 헛소리함. 아니면 o3에 인터넷 검색 옵션을 켜서 온라인 문서 참고 능력을 높일 수 있음. 적응에 시간이 걸리니 마법사로 기대하면 안 되고, 쓰기까지 학습 곡선이 높은 점은 vim/emacs 같은 다른 개발 도구와도 닮음. 그리고 ChatGPT도 ‘지구본 버튼’을 누르면 인터넷 검색 활용도가 올라감
     * LLM·AI로 직원 업무 생산성을 올리는 회사는 번창, 직원을 아예 대체하려는 회사는 실패 예측. 단기적으로는 CEO/임원이 단기 실적에 만족하며 미래 성장을 갉아먹는 선택을 할지도 모름
          + 정답은 바로 그거. 프로그래머의 어시스턴트로 LLM을 쓰는 게 맞고, 완전 대체는 무리. 기술을 적당히 받아들이는 게 옳은 길
          + 직원 대체로 단기적 가치를 올려서 회사에 이득이 돌아갈 수도 있다는 아이디어는 꽤 흥미롭다고 생각. 즉, 중장기으론 회사에 해가 되지만, 임시로 주가가 오르는 일은 생길 수 있음
          + 코드 어시스턴트는 필수 공통 도구이자, 사람을 대체할 무기는 아님. 경쟁사도 같은 AI 구독 살 수 있는 시대, 사람을 대체할 수 없음
     * 현장 경험 공유 - 예전 개발자에서 이제는 관리자이지만 여전히 코드 작성. LLM 어시스턴트는 도움도 되지만 가끔 불편. 예상치 못한 코드 제안을 냅다 쏟아낼 때 의도와 다른 방향으로 나아가기도 하고, 검토하는 시간이 소모됨. 아마 설정 문제인데, 직접 명령어로 호출할 때만 보이도록 기본값을 바꾸고 싶음. 한 가지 확실한 건, LLM에게 전체 코드나 함수 작성은 맡겨도 반드시 본인 리뷰 과정을 거침
          + Zed 에디터엔 이런 ‘은은한 제안’ 모드 기능 있음 (자세히 보기). 모든 에디터가 이런 모드 제공했으면 함
          + 요즘 스타트업에서 여러 일 하다 보니 LLM이 만든 UI를 별로 좋아하지 않음. 빌딩 블록 같은 기초는 유용하지만, Claude로 긴 코드블록을 통째로 맡기면 내가 원하는 결과에 도달하기까지 수많은 재작업이 필요

   https://freederia.com ai과학자사이트처럼 공존관계를 유지할겁니다.

   redis 곧 뒤쳐지겠네

   자전거랑 달리기 경쟁하는 느낌이네요.

   AI 통해서 트러블슈팅 하려다가 전부 실패해서 결국 제가 스스로 문제를 해결한 적이 여러 번 있습니다.
"
"https://news.hada.io/topic?id=21172","와플 하우스의 중단 요청(영업정지 요구) 받기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       와플 하우스의 중단 요청(영업정지 요구) 받기

     * 허리케인 Helene으로 인해 플로리다 대학 수업이 휴강된 시점에서, 필자는 Waffle House 홈페이지 역공학 작업을 진행함
     * Waffle House Index라는 비공식적 자연재해 지표의 현실화와 데이터 실시간 지도로 구현함
     * Python과 Next.js, Redis를 활용해 전국 매장 오픈/클로즈 현황을 자동 추적하는 웹사이트를 개발함
     * 프로젝트가 SNS와 유명인사 관심을 끌면서 Waffle House 측의 공식 차단 및 중단 요구로 이어짐
     * 상표권 침해로 사이트는 폐쇄되지만, 데이터 활용 및 엔지니어링 경험에서 프로그래밍의 즐거움과 성장을 느낌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 배경과 동기

     * 2024년 9월 말, 허리케인 Helene이 플로리다에 접근하는 상황에서 필자는 Waffle House 웹사이트 분석에 몰입함
     * 외부에서는 주택 바리케이드 작업이 한창이었으나, 필자는 Waffle House Index의 실시간 구현에 집중함
     * Waffle House Index는 FEMA(미국 연방재난관리청)에서 참조하는 비공식 자연재난 정도 지표로, Waffle House 매장의 개폐 여부로 재난의 심각도를 측정하는 특이한 방식임
     * 하지만 공식적으로 확인할 수 있는 “인덱스”나 실시간 지도는 존재하지 않고, 관련 언급이 일부 위키피디아나 기사에만 흩어져 있음

역공학 및 기술적 접근

  기술적 구조

     * Waffle House는 위치 정보 사이트에 Next.js와 React Server Components를 적용하는 구조임
     * React Server Components는 서버 측에서 실행되어 원시 HTML을 클라이언트에서 직접 확인할 수 없음
     * 장시간 소스 분석 끝에 Next.js에서 클라이언트로 데이터를 삽입하는 특정 파일 내 JSON 데이터를 발견함
     * 이 데이터에는 모든 매장별 상태(오픈/클로즈, 혼잡도 등) 가 담겨 있었음

  데이터 수집·처리 및 사이트 구현

     * Python으로 경량 스크래핑 및 데이터 가공, Next.js 프론트엔드, 그리고 Redis 캐시 조합으로 실시간 매장 클로즈 지도를 구축함
     * 이를 통해 어느 지역의 Waffle House가 닫혔는지, 곧 지역별 재난 상태를 추적할 수 있었음

서비스 공개와 확산

     * wafflehouseindex[.]org 도메인을 Vercel에 배포하고 트윗으로 공유함
     * 당시 필자의 SNS 팔로워는 200명도 안 되었으나, Waffle House 공식 계정의 직접 반응을 얻으며 급속 확산됨
     * Waffle House는 해당 사이트 정보가 비공식·오류임을 강조하며, 실제 폐점 정보는 공식 채널을 통해 알릴 것이라고 공지함
     * 이에 대한 필자의 농담 섞인 인용 트윗 이후, 유명 정치평론가 Frank Luntz가 사이트를 직접 언급하면서 접속자 급증 상황 발생함

차단과 법적 대응

     * Frank Luntz의 트윗에 대해 Waffle House 마케팅 및 법무팀이 신속 대응, 비공식/오류 정보임을 재차 밝히고, Frank 역시 트윗을 삭제함
     * 이후 Waffle House 트위터 계정에 의해 필자는 차단(block) 처리됨
     * 허리케인이 지나간 후에는 상표권 침해 경고와 중단 요청(cease and desist) 이메일을 Waffle House로부터 수신함
     * 문제된 부분은 데이터 수집 및 매장 정보 자체보다는, 로고 등 Waffle House 상표 사용이 주요 쟁점이었음

답변 및 엔딩

     * 필자는 유머러스한 어조로 Waffle House에 답장 보냄(‘House 팬’, 국기처럼 여기며 존중 등)
     * 고위 임원이 친근한 답신을 보내주었으나, 상표권 위반으로 사이트 폐쇄는 불가피해짐
     * 공식 브랜드 유지하면서 사이트 오픈 가능성 문의도 했으나, 답변을 더는 받지 못함

결론 및 소회

     * 프로그래밍의 즐거움과 데이터의 창의적 활용, 그리고 커뮤니티 반응을 직접 경험한 엔지니어링 여정임
     * 짧은 기간이었지만 재난, 데이터, 오픈소스, 브랜드가 교차하는 독특한 경험이 됨
     * Waffle House의 스포츠맨십과 소통, 트레이드마크 침해에도 인내를 보여준 점에 감사를 표함

마무리

     * 본문 교정 및 편집 도움을 준 Moo, Kai, Babgel GC에게 감사를 전함

        Hacker News 의견

     * 예전에 나는 Dannon / Danone이 자사 요거트에 들어있는 박테리아 이름에 과학적인 듯한 가짜 이름을 쓰고 있다는 점을 지적한 사이트를 만들었음. 내 사이트에 대해 Danone 법무팀에서 연락이 왔지만, 나는 물러서지 않았음. 자세한 내용은 여기 참고. 만약 Waffle House 브랜드만 제거했다면 괜찮았을 것이며, 사이트 상단에 큰 면책 조항을 두는 방식으로 이런 법적 주장들이 사실 얼마나 우스꽝스러운지 강조할 수도 있었을 거라는 생각
          + McBroken.com도 아직 잘 운영되고 있다는 점을 상기
          + 너의 웹사이트는 대중 매체 뉴스의 요약본과 일부 의사 인터뷰들을 집계한 것임. ""만들어진"" 이름과 ""정확한 학명""에 대한 주장은 있지만, 어떤 법률이 제조사가 이런 이름을 라벨에 쓰지 못하게 금지하는지는 인용이 없음. 영국 시각에서 접근한 것 같지만, 미국 FDA나 FTC가 이런 신조어 사용을 금지할 만한 규정은 없는 것으로 보임. 생물 분류학에서는 새로운 명칭이 자주 만들어지고, 종의 별칭이나 별명도 흔함. 약학 업계나 과학자, 심지어 천문학자도 라틴어처럼 들리는 이름을 자유롭게 만듦. 식품 업계는 'milk'와 'water'도 그 의미를 넓혀서 법적 승인하에 사용하고 있음. 건강보조식품에도 브랜드 블렌드 이름을 붙이는 것이 FDA 규정에 잘 맞음. 결국 당신 사이트가 남아 있는 것은 이런 라벨링 관행에 실제 위협이 안 되기 때문. 참고로 내 장 속의 candida
            albicans에도 새 이름을 붙여보고 싶음 - candida hackernewsensis
     * 내 예전 지인이 DJ 예명으로 Mupperfucker를 썼다가 Jim Henson Company로부터 사용 중단 요청을 받았던 일화가 있음. 상표권자가 방어를 위한 조치를 취해야 한다는 건 이해하지만 아이러니함이 있음. 관련 기사 참고
     * ""라이브 피드도 없고, 지도도 없고, 폐점 레스토랑 카운터도 없다""는 점에 대해 자신있게 단정짓기는 어려움. 참고로 [이 링크](https://en.wikipedia.org/wiki/Waffle_House_Index/…)에 관련 이미지가 있음
     * 만약 이슈가 단순히 상표나 로고 사용 때문이었다면, 해당 부분만 제거해서 다른 도메인으로 계속 운영하면 되는 거 아님? 데이터 스크래핑 자체에 대해서는 별 불만이 언급되지 않았음
          + 작성자인데, C&D(사용중단 요청서)를 받은 후 데이터 수집 방법이 막힘(블로그 글에는 이 내용이 누락된 것 같음). 사이트 운영 유지 방법을 물어봤지만 답변을 받지 못했고, 그 후 Waffle House 로고만 빼고 운영하면 또 다른 C&D가 올 것 같아서 고양이와 쥐잡기 식 게임을 계속 하느니 그냥 사이트를 내림
     * 굳이 전체 사이트를 내릴 필요 없고, 로고만 내렸어도 됐을 것 같음
          + 미국에서는 법적 위협만으로 큰 돈이 들 수 있다는 인식. 대학생이라면 변호사도 없고 법적 지식도 부족하니, 차라리 그냥 사이트 내리는 쪽이 훨씬 쉽고 안전함. 대학 내 무료 법률 서비스도 결국 소극적 조언을 줄 가능성이 큼
     * WaffleHurricaneTracker.org 정도의 중성적 네이밍이면 아마도 상표 이슈 없이 살아남을 수 있었을 것임. 스크래핑 이슈도 만약 데이터를 익명화했다면 Waffle House에서 가져왔다는 걸 명확히 드러내지 않을 수 있겠고, 여러 아침식사 식당에서 천천히 긁어온 것처럼 뉘앙스를 만들었으면 괜찮았을 것임
     * 근사한 회사 편지지 이미지를 만들어서 좋은 종이에 C&D를 인쇄해서 액자에 걸어두면 ""Waffle House에서 나를 고소하겠다고 협박당했다""라는 이야기를 평생 할 수 있음
     * Waffle House에 갈 때마다 정말 특별한 추억이 될 것임. 멋진 이야기 소재임. C&D도 무섭긴 한데, 개인적 의견으로는 그들의 편지는 꽤 친절하고 직설적이었음. 참고로 hafflewouse.com은 아직 사용 가능함
     * FEMA가 자동화된 시스템을 사용하는 건지, 아니면 Waffle House Index가 버려진 컨셉인지 혼란스러움
          + Waffle House Index는 공식 지수가 아니었음. 재난구호 일을 하는 사람들이 일상적으로 ""Town A는 토네이도가 왔지만 Waffle House가 문을 열었더라""와 같이 이야기하는, 약간 농담 섞인 내부 지표일 뿐임. 최근 알게 된 바로는 FEMA가 재난 지역에서 Waffle House 같은 로컬 식당에 연락해서 상황을 확인한다고 함. 비상사태 심각도를 파악하는 추가 데이터 소스 개념 정도. 그래도 이걸 진지하게 ""지수""라고 부르기엔 무리가 있음
          + 참고할 만한 뉴스 기사와 [위키백과 이미지](https://en.wikipedia.org/wiki/Waffle_House_Index/…) 있음
     * 블로그에 게임처럼 달린 업적 시스템을 C&D(사용중단 요청서)로 중단시킬 수 있으면 좋을 것 같음, 블로그는 게임이 아니라는 생각
"
"https://news.hada.io/topic?id=21243","사무라이 잭의 비주얼 세계","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             사무라이 잭의 비주얼 세계

     * 사무라이 잭은 대사보다 시각적 스토리텔링에 집중하며 기존 애니메이션과 차별성 창출
     * Genndy Tartakovsky와 아트 디렉터 Scott Wills의 협업으로 독특한 배경화와 분위기 구현
     * 한국의 Rough Draft Korea 팀과의 긴밀한 협업을 통해 색감 및 아트워크 디테일 유지
     * 중기 애니메이션 및 UPA 스타일의 영향 아래 제한적 애니메이션과 창의적인 움직임 표현 시도
     * 리스크가 있었지만 카툰 네트워크의 지원으로 실험적인 형식 성공 및 애니메이션 역사에 잔상 남김
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

1 – 그림으로 이야기하기

   이미지만으로도 스토리 전달이 가능함. Oscar 수상작 Flow와 같이 대사 없이도 관객을 사로잡는 연출이 존재함. 그러나 연속적인 시리즈에서 이런 방식을 유지하는 것은 훨씬 어려운 도전임. 대다수 애니메이션 시리즈가 대사 중심 각본을 선호하는 경향을 보임.

   Genndy Tartakovsky는 Dexter’s Lab과 Powerpuff Girls 이후 대사 중심 작업에 대한 피로를 느꼈음. 그는 움직임과 비주얼, 즉 애니메이션의 본질로 돌아가고자 했으며, 이 시도가 바로 Samurai Jack(2001–2004)의 핵심 영감이었음.

   Samurai Jack 팀은 “단순한 이야기를 시각적으로 전하는 것”을 목표로 삼음. 극도로 절제된 대사와, 풍성한 그림과 동작, 영화적 연출로 관객의 몰입감을 유도했음.

   에피소드 8의 불타는 나무 시퀀스에서 보여지듯, 대사 없는 연속적인 신들로도 감정과 극적 긴장감을 전달함. 거의 1분 20초 동안 등장인물도, 대사도 없는 장면이 이어지지만 확실한 이야기를 제공함.

   Tartakovsky는 시청자인 어린이들의 지성에 대한 신뢰를 바탕으로, 스스로 해석할 기회를 제공하고 싶어함. 네트워크 임원진 또한 이러한 독특한 시도에 지지를 보내줌.

   Samurai Jack의 테스트 애니메이션이 한국 외주팀에서 제작되어 성공적으로 완성됨. 시청 후 첫 반응이 ‘Speechless’였을 만큼 실험적이지만, 한편으로 깊은 인상을 남겼음.

   배경 아트 디렉션은 Scott Wills가 핵심 역할을 담당함. 그의 합류 후 조명, 분위기, 심도 등에서 배경이 살아나는 효과가 뚜렷해졌음.

   Tartakovsky는 60–70년대 실사영화(Doctor Zhivago, Lawrence of Arabia, Kurosawa, Hayao Miyazaki 등)에서 풍경이 주인공이 되는 방식을 참조함. 미국 TV 애니메이션에서 쉽게 볼 수 없는 형식이었지만, Samurai Jack에서는 실현됨.

   Wills는 중기 애니메이션의 추상성과 색채, 실사 페인팅의 사실성과 조명, UPA 스타일의 단순화를 혼합한 스타일을 구축함. 그리고 “녹색 잔디, 파란 하늘 금지” 등 신선한 색채 조합을 통한 변화를 규칙으로 삼음.

   기술적 난제도 존재함. 캐릭터 윤곽선에 검정을 사용하지 않아, 색상과 배경의 구분이 모호해지는 상황 발생. 이로 인해 실제로 Jack의 얼굴이 배경에 소실되는 경우도 있었음.

   주요 배경 아트웍은 Rough Draft Korea에서 제작하며, 미국-한국 스튜디오 간 커뮤니케이션 상 갭 보완을 위해 Scott Wills가 장시간 직접 스캔, 컬러 수정, 가이드 자료 제공으로 품질 일관성을 유지함.

   Samurai Jack의 비주얼은 Akira, The Andromeda Strain, The Adventures of Prince Achmed 등 다양한 소스에서 아이디어 차용. Lynne Naylor가 설계한 수많은 캐릭터와, 독창적인 캐릭터 움직임에도 중기 애니메이션 시기의 영향이 녹아있음.

   Tartakovsky와 동료들은 UPA와 제한적 애니메이션에 익숙했으며, 예산, 제작 환경의 제약 속 창의성 발휘가 특징임. Bobe Cannon의 영향 아래 각 캐릭터별 고유한 움직임과 타이밍 강조.

   Samurai Jack에서도 긴 정지, 반복되는 동작, 리듬감 있는 움직임 등 현실성과는 거리가 있지만, 감정과 캐릭터성을 움직임만으로 표현하는 방식 도입됨.

   Rough Draft Korea와의 파트너십을 중시하면서, Jim Jeong을 디렉팅 애니메이터로 기용해 차별화된 연출 확보. Jim Jeong은 Dexter’s Lab부터 인정받은 인물로, Samurai Jack에서 아름다운 액션 시퀀스 구현에 기여함.

   그럼에도 불구하고 Samurai Jack의 성공 기대감은 높지 않았음. Cartoon Network는 리스크 테이킹 정신으로 이 프로젝트를 지원함. 결과적으로 대중적 초대형 히트작은 아니지만, 참신한 시도와 이미지, 실험정신으로 애니메이션 역사에 강한 인상 남김.

   Tartakovsky는 “단순한 영상 기법에 충실하지만, 당시 TV에서 보기 드문 도전”이라고 언급함. 많은 작품이 더 안전한 길을 택했지만, 결국 불타는 나무 같은 이미지가 오래 기억에 남는 이유를 보여줌.

2 – 뉴스비트

     * 프랑스 감독 Sylvain Chomet이 The Triplets of Belleville의 변형 작품 제작 공개
     * 20년 이상 제작된 헝가리 장편The Tragedy of Man 의 미국 개봉 임박 및 Deaf Crocodile에서 사전 주문 시작
     * 프랑스 Annecy MIFA에서 동아프리카 10개 애니메이션 스튜디오 참가 예정, 케냐 스튜디오 5개 포함
     * 프랑스 영화Arco 가 Cannes에서 호평받으며, 미국 Neon을 통해 정식 개봉 준비
     * 러시아에서 Sergey Kapkov이 구 소련 TV 애니 스튜디오 Studio Ekran 인터뷰 모음집 출간예정
     * 스페인 정부가 17억 1천만 유로 규모의 “유럽 주요 오디오비주얼 허브” 전략 발표, 애니메이션 분야도 수혜 전망
     * 미국의Sesame Street , Netflix에 편입 예정이나 PBS에서도 지속 방영 예정
     * 멕시코 소피아 카리요 감독의 스톱모션 장편 Insectarium 프로젝트 진행 근황 소개
     * 고전 애니 The Adventures of Prince Achmed의 상영 시간과 무성영화 시대 프레임레이트 유동성에 대한 해설

        Hacker News 의견

     * Genndy Tartakovsky라는 창작자의 독특함을 강조하고 싶음, Samurai Jack이 너무 좋아서 어린 시절 Cartoon Network에서 본 이후로 결말이 궁금해 나중에 다시 찾아봤던 추억임
          + Pantheon이라는 작품에 대해서도 궁금함 표현
     * Samurai Jack의 비하인드 아트워크 모음 사이트를 발견했음 characterdesignreferences.com 공유, 같이 살던 아티스트 룸메이트가 이 작품의 아트 스타일에 굉장히 집착했던 기억임, 2017년에 리부트된 시즌을 아직 안 봤다면 무조건 추천하는 바람임, 그리고 시즌 5의 멋진 클립도 소개하고 싶음 유튜브 클립
          + 리부트 시리즈가 개인적으로 마음에 들지 않았던 인상임, 새로운 캐릭터가 너무 부각되어 원작의 매력을 희석시킨 느낌임, 오리지널 캐릭터가 주인공인 팬픽 엔딩 같은 기분이 들었던 감상임
     * Samurai Jack은 애니메이션의 아름다움과 표현력, 그리고 이런 자신감 있는 아트워크 덕분에 요즘 방송에서 흔히 볼 수 있는 불필요하고 어색한 대사들을 줄일 수 있는 강점이 특징임
          + Primal의 첫 시즌은 거의 대사가 없지만 훌륭하게 완성된 작품임, 석기시대 인간과 공룡의 상호작용을 그린 구조임
          + 소원을 들어주는 우물을 지키는 세 명의 장님 궁수 에피소드가 Samurai Jack의 진수를 보여주는 대표작으로 꼽고 싶음, Tartakovsky의 작품을 떠올릴 때 Cartoon Network에서 방영했던 Clone Wars 마이크로 시리즈도 함께 생각남, 첫 30~60초를 지나면 대사가 전혀 없지만 특수부대 클론 트루퍼 에피소드는 그 긴장감이 최고조에 달한 명장면임
     * Primal이 언급된 점이 기쁨, 현재 시즌 3을 여전히 기다리는 중임 자세한 정보 링크
          + 오늘 하루를 기분 좋게 해준 정보임
     * 이러한 Samurai Jack의 미학을 언제나 좋아했고, 아직 언급이 안 됐지만 Primal이 대사가 거의 없고 성인 대상임에도 정말 주목할 만한 작품이라는 점을 추천하고 싶음
     * 어릴 적에는 Samurai Jack을 접하지 않았는데 최근에서야 보기 시작했고, 정말 숨막힐 정도로 뛰어난 작품임을 깨달았음
          + 나 역시 최근 제작된 마지막 시즌을 봤으며, 아트워크의 완성도가 정말 훌륭했다는 인상임
     * 중국 애니메이션도 속도를 내고 따라잡고 있는 상황임 중국 애니메이션 유튜브 링크
     * CN에서 Samurai Jack의 비하인드 신 영상을 짧게 보여주었던 기억이 남아 있음, 어린 시절 새로운 관점에서 시각과 사운드의 가치를 처음 깨달을 수 있었던 계기였음, 지금 생각해도 정말 아름다운 예술 작품이라는 확신임
     * 최근에서야 Samurai Jack을 처음 발견했음, 옛날 VHS로 녹화된 아카이브 영상을 통해 접했으며, 애니메이션의 섬세함에 감탄하게 됨, 더 자연스러운 오리지널 버전을 볼 수 있는 링크나 추천 자료가 있다면 알려주길 희망함, 예전 TV 광고까지 포함된 원본 방송은 스트리밍이나 archive.org에서 볼 수 있는 가공된 버전과는 완전히 다른 감성을 선사함
"
"https://news.hada.io/topic?id=21202"," "Full Self-Driving"의 노래: 엘론은 토니 스타크가 아니라 마이클 스콧이다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ""Full Self-Driving""의 노래: 엘론은 토니 스타크가 아니라 마이클 스콧이다.

     * Elon Musk는 지난 10년간 Tesla의 완전 자율주행(Full Self Driving, FSD) 을 약속하였음에도, 실제로는 반복적으로 불완전한 결과를 내고 있음
     * Waymo(구글) 는 이미 완전 자율주행 택시 서비스를 미국 여러 도시에 도입하여, Tesla보다 기술적으로 앞서 있음
     * 머스크는 라이다(LiDAR)와 레이더 등 핵심 센서 없이 카메라 기반 '비전 온리' 방식만을 고집했고, 이는 여러 안전 문제와 사고 위험을 키우는 결과를 초래함
     * Austin에 곧 등장할 테슬라 Robotaxi는 제한된 지역 내에서만 운행되고, 필요시 원격으로 제어되는 근본적으로 반쪽짜리 자율주행임
     * 현대 IT 업계와 미국 경제권력이, 소수의 부유한 비전 없는 경영자들에게 지나치게 집중되고 있는 현상을 문제로 제기함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

1. “Full Self Driving”

     * 수년간 Elon Musk는 Tesla 차량이 완전 자율로 움직일 것이라 약속했지만, 10년 넘게 계속해서 FSD 상용화 시기를 수차례 연기함
          + 예시: 2015~2019년 동안 Musk는 2년 이내 완전자율주행 실현, 주차장 호출, LA-뉴욕 무정지 주행 등을 반복해서 언급함
     * 하지만 실제로는 “몇 개월 이내 가능”이라는 약속이 계속 미뤄지는 웃음거리로 주요 언론과 투자자들조차 반복적으로 머스크의 실패를 무시하고 넘어감
     * 현재 테슬라는 “Robotaxi” 서비스를 Austin에서 곧 선보일 예정이나, 구체 내용·운영 방식·응급상황 대처 등은 지역 당국에조차 제대로 공유되지 않은 상태임
     * 실제로는 수정된 Model Y 차량 10~20대만이 운영될 예정이며, 이는 완전자율 택시가 아니라 실질적으로 Amazon의 Just Walk Out처럼 “진짜”보다 장식에 가까운 시범 서비스 가능성에 대한 의구심을 자아냄
     * 이미 미국 내에서는 구글의 Waymo가 실제로 무인 자동차 택시 서비스를 여러 도시에서 안정적으로 운영 중이며, Tesla FSD는 Waymo 기술력에 실제로 “몇 년(혹은 10년)” 뒤처져 있음을 인정함
     * 자율주행차 개발은 100년 가까운 역사가 있으며, 최근에는 센싱(카메라, 맵, 라이다/레이더 등)을 결합한 하이브리드 방식이 대세임. Waymo는 각종 센서와 상세 맵을 융합하여 Level 4(실질적 무인주행)에 근접한 기술력을 확보함
     * 반면 머스크는 자기 고집과 엔지니어링 미스로 인해 테슬라의 기술 발전을 방해하고 있음

2. The Avengers or The Office?

     * 라이다(LiDAR)는 1960년대부터 개발된 레이저 기반 거리 감지 기술로, 최근 자동차 안전기술에 핵심적으로 도입됨
     * 차선이탈, 충돌회피, 어댑티브 크루즈, 비상제동 등 현대 차량의 안전 기능 상당수가 라이다에 기반함. 라이다 도입은 에어백 이후 가장 큰 자동차 안전 혁신 중 하나임
     * 그러나 머스크는 비용 부담과 자신의 신념을 이유로 2019년부터 일관되게 라이다를 “쓸모없는 짐”, “비효율적 장치”라 비판하며 Tesla 차량에서 도입을 거부함
          + 레이더 역시 2021년부터 신차 적용을 중단하고, 구형 차량의 레이더 기능을 소프트웨어로 강제차단함
     * 이로 인해 테슬라 차량은 카메라만으로 주행 환경을 인식하는 ‘비전 온리’ 방식에 의존하며, 비·강한 햇살 등 기상상황에서 안전 취약점과 사고가 증가함
     * 머스크의 이러한 결정은 이념적 고집에서 비롯된 것으로, 엔지니어들 및 업계 전문가의 강한 우려에도 불구하고 실현됨
     * 머스크가 비전 온리 방식을 고집한 배경에는 ‘AI와 데이터가 중요하다’는 비즈니스 논리가 존재함. 테슬라가 세계에서 가장 많은 도로 주행 카메라 데이터를 보유하고 있으므로, 이를 경쟁우위라고 본 것임
     * 하지만 실제로는 Waymo 등 센서 결합 하이브리드 방식에 비해 안전성, 신뢰성, 기술 수준 모두 크게 뒤처진 결과임
     * 흥미롭게도 최근에는 테슬라도 비밀리에 라이다 관련 기술에 투자 하고 있음. 곧 출시될 Robotaxi에 라이다가 적용될지 불확실함

3. Remote Control

     * 머스크의 자율주행 약속과 실제 Robotaxi 간의 괴리가 심각함
     * 2018년에는 뉴욕-로스앤젤레스 구간 완전자율주행을 선언했으나, 2025년 오스틴에서는 단 10~20대의 Robotaxi만 시범 운영 예정임
     * 이들 Robotaxi는 엄격한 지정 구역(Geofenced) 내에서만 운행, 완전자율주행이 아님
     * 더 나아가 많은 Robotaxi는 원격 조종(tele ops) 시스템이 뒤에서 차량을 감독/조작하며, 진정한 의미의 ‘스스로 판단하는’ 무인주행차가 아님
     * 즉, 1939년 세계박람회가 제시한 ‘원격조종 자동차’ 수준으로 과거로 퇴보한 셈임
     * 종합하면, 머스크는
          + 10여 년간 완전자율주행 약속 남발
          + 비효율적 엔지니어링 결정으로 자사 기술 발목
          + 하이브리드 방식 대비 열등한 비전 온리 자율주행 제품 출시
          + 결국 외관만 자율주행인 ‘원격조종 자동차’ 를 Robotaxi 명목으로 선보이는 중임
     * 경제·정치적으로 중요한 역할을 맡은 지도자가 이렇듯 허황된 약속과 허술한 결과물을 내놓아도, 사회적 책임 문제 제기의 필요성 부각

4. Stupid

     * 최근 Bloomberg 보도에 따르면, Satya Nadella(Microsoft CEO) 는 본인 업무 대부분을 Copilot 등 AI에 의존한다고 밝힘
          + 팟캐스트 청취도 음성으로 듣지 않고, Copilot에 트랜스크립트를 넣어 요약 및 질의응답 방식으로 활용함
          + 이메일, 미팅 준비, 각종 리서치 등 실무의 상당 부분을 AI 에이전트에 “대표단”처럼 배분함
     * 이는 IT/경제계의 최고경영자들이 실제로는 자신들의 업무를 AI·자동화에 원격 위임하며, “하는 일의 실질적 가치” 대신 겉보기에 집중하는 상징적 경영자 시대가 도래했음을 보여줌
     * 이런 흐름은 머스크의 테슬라 FSD와도 연결되며, 미국의 기술·경제 경영권이 실질적 전문성이나 책임 없는 극소수 인물에 과몰입되고 있음을 지적함

결론

     * Tesla의 FSD와 Robotaxi 사례는 과장된 약속, 비현실적 비용 절감, 데이터 만능주의, 원격조종 기술의 현실, 그리고 미국 경제·기술 중심이 지닌 권력 집중과 무능 리스크 등의 문제를 동시에 드러냄
     * IT와 스타트업, 자율주행 업계 종사자라면 실제 기술 발전 이면의 정책, 비즈니스 의사결정, 사회적 영향을 다각적으로 관찰할 필요성이 있음

     비즈니스 바보의 시대를 읽어보세요. 머스크와 FSD에 대한 모든 내용이 담겨 있습니다.
"
"https://news.hada.io/topic?id=21259","Ask HN: 2025년에 로보틱스를 어떻게 배울 수 있을까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ask HN: 2025년에 로보틱스를 어떻게 배울 수 있을까요?

     * 로보틱스 입문은 점점 쉬워졌지만, 하드웨어·소프트웨어·수학 등 다양한 분야의 기본기를 폭넓게 경험해야 진짜 실력이 쌓임
     * 단순 온라인 코스만으로는 한계가 있으며, 실제 로봇을 직접 만들어보고 시행착오를 겪는 과정이 가장 큰 학습이 됨
     * 작은 프로젝트(예: 라인트레이서, RC카+Arduino, 레고, 미니로봇 등)로 시작해 점차 복잡한 제어와 하드웨어, 시뮬레이션으로 확장할 것 권장
     * 3D 프린터, 저가형 키트, 시뮬레이터 등 활용 가능한 저비용·고효율 도구와 생태계가 확대되어 접근성이 향상됨
     * ROS/LeRobot, PID, 제어이론, 회로·구조 설계 등 다양한 오픈소스, 실습 프레임워크, 게임 기반 학습법 등도 적극 활용 필요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

로보틱스 학습 입문 조언 요약

  1. 실습 우선, 직접 만들어보기

     * 온라인 코스(robotics_essentials_ros2)로 기초를 다진 뒤, 직접 로봇을 만들어보고 실제 부품을 다루며 경험 쌓기 권장
     * 간단한 프로젝트: 라인트레이서(Arduino 예제), RC카+Arduino, 소형 키트(레고 SPIKE Prime, Yahboom 키트, SO-ARM101, Partabot)로 시작해보는 것이 좋음
     * Onshape CAD, A1 mini 3D 프린터, Adafruit/SparkFun 등 부품 조합으로 쉽게 시도 가능

  2. 다분야 통합 학습 강조

     * 로보틱스는 기계, 전자, 제어, 소프트웨어가 모두 결합되는 복합 학문임
     * 각 영역을 얕게라도 경험한 뒤, 자신만의 강점을 깊게 파고드는 방식이 장기적으로 실력 향상에 효과적임
     * “전문가이자 제너럴리스트”를 지향하는 것이 장기 생존 전략
     * 관련: Exploring Beaglebone 책 살펴보기

  3. 실제 하드웨어 경험과 실패의 가치

     * 시뮬레이터(Mujoco, Isaac Sim, Stormworks 게임)로도 연습 가능하지만, 실제 하드웨어 조립/운영 경험이 가장 큰 차이를 만듦
     * POC 제작, Hackathon 참가 등 실전 경험 추천
     * Practical Electronics for Inventors 참고

  4. 기초 제어이론과 도구의 활용

     * PID 제어, 기초 전기회로, 기구 설계 등 기본 이론은 꼭 경험해볼 것
     * ROS(ros.org), LeRobot(huggingface.co/lerobot), MoveIt(moveit.ros.org), Nav2(navigation.ros.org), mcap.dev(mcap.dev), foxglove.dev(foxglove.dev) 등 오픈소스 프레임워크와 튜토리얼 활용
     * Arduino, 라즈베리파이, Jetson Nano, 임베디드 플랫폼 zephyr project 등 플랫폼 다양

  5. 자기 주도형 프로젝트와 커뮤니티

     * 구체적인 목적(예: 터틀봇 제작 영상, 로봇팔 프로젝트)를 먼저 정하면 학습 동기가 높아짐
     * 해커톤(LeRobot 해커톤), ROS Meetup 등 참여 추천
     * 하드웨어 설계, 소프트웨어 통합, 센서 활용 등 전체 시스템을 다뤄보는 경험이 중요함

  6. AI/ML과 최신 트렌드의 적용

     * AI 기반 제어, 경로 계획, 객체 인식 등 AI/ML도구(Hugging Face LeRobot)를 배우면 최신 트렌드에 맞는 로봇 개발 가능
     * 모델 학습·평가 데이터셋 공유 app.destroyrobots.com

  7. 기타 현실적 조언

     * 학문적으로 접근(연구논문, 대학강의 등) 하려면 높은 수학/이론 배경 필요
       Stanford CS223A, MIT 6.832
     * 실무적으론 간단한 완제품/모듈 조립, 기존 오픈소스 예제 변형만으로도 재미와 성취감을 얻을 수 있음
     * Crunch Labs HackPack, Lego SPIKE Prime, pololu robotics 등 추천

결론

     * “직접 만들어보고, 실패하고, 다시 도전하는 것” 이 로보틱스 학습의 핵심
     * 소프트웨어와 하드웨어, 이론과 실습의 균형을 맞추면서, 자신의 흥미와 상황에 맞는 최적의 시작점을 선택할 것
     * 커뮤니티, 해커톤, 오픈소스, 키트, 게임, 시뮬레이터 등 도구와 자원을 적극적으로 활용하면 누구나 로보틱스에 도전할 수 있음

        Hacker News 의견

     * 무료로 수강 가능한 robotics_essentials_ros2 코스 추천 경험 공유 로봇 하드웨어 설계 경험에서 소프트웨어 쪽에 재미와 보상이 더 크다고 느꼈지만, 여러 분야 스킬을 쌓는 것이 굉장히 유익하다고 강조 코스 진행 후 임베디드 분야(예: zephyr project) 탐색 제안 기계적인 부분에 입문하려면 A1 mini 구매와 함께 onshape(www.onshape.com)로 간단한 파츠(모터·보드 거치대, 그리퍼 등) 직접 설계 경험 추천 전기공학은 실수하면 비용이 크기 때문에 조심을 요하며, RP2040이나 RP2350 같은 저렴한 보드로 작은 실습부터 시작해서 H-브릿지, 브러쉬드 모터 경험 후 블러쉬리스 모터 제어로 확장 권고 저가용 인두기와 JBC C245 팁 호환 클론 제품 활용 팁 ROS 밋업 탐색 권장 및 끝까지 목표 잊지 않으면서 천천히 자신만의 속도로 여정 지속 제안
          + 로보틱스 분야 종사 경험에서 온라인 ROS2 코스만으로 실질적인 ‘로보틱스 학습’은 어렵다는 관점 제시 로보틱스는 하드웨어, 소프트웨어, 수학, 엔지니어링이 결합된 복합적인 분야이므로 직접 로봇청소기를 처음부터 구현해보는 프로젝트 추천 진공 청소 기능 자체는 중요하지 않고, ‘터틀봇’ 같은 자율주행 로봇을 구현하며 설계 과정과 문제 해결을 실제로 경험하는 것이 더 큰 학습 효과 생활 주변의 차량, 드론, 소형 모빌리티, 건설장비 등 다양한 시스템에 접목되는 실체적 노하우 이해 필요
          + 로보틱스를 배우는 여정에 객관적인 목적의식 자체를 못 느끼는 점이 가장 큰 장애물이라는 솔직한 고민 공유 쿨한 로봇을 만드는 일이 일종의 장난감 놀이로만 느껴지고 그 생각에서 벗어나기 힘들었다는 설명 대학에서 메카트로닉스 전공 및 독학 경험 기반으로 혼자 믿을 수 있고 효율적인 로봇을 만드는 일이 비즈니스적으로는 난이도가 극단적으로 높다는 개인적 인식
          + “Exploring Beaglebone” 책에서 하드웨어 실습, 실수 경험에 도움이 되는 내용을 많이 배웠다는 경험 ISBN 등 상세 정보 공유 및 전압 보호 회로 구현법이 실제로 큰 비용 절감 팁임을 강조
          + 로보틱스의 가장 큰 매력은 실제 세상에서 내 창작물이 구현되는 만족감이라는 점을 강조
          + RP2040이나 RP2350 프로그래밍 경험이 SIEMENS SIMATIC 등 상업용 플랫폼과 어떻게 연결될 수 있는지 궁금증 제기
     * 로보틱스의 진입 장벽이 예전보다 확실히 낮아졌다는 견해 하지만 기존 웹/데스크탑 소프트웨어 개발과는 완전히 다른 영역이라 비교적 가파른 러닝커브 각오 필요성 강조 Amazon, Yahboom, Hugging Face SO-ARM101 등 키트 추천 및 센서 추가에 드는 예산 범위 안내 실물을 바로 구매하기 어렵다면 Isaac Sim, Mujoco 같은 시뮬레이터 적극 활용 권장 머신러닝 로보틱스로 Hugging Face의 LeRobot 프레임워크, ROS 기본(퍼브섭) 개념, MoveIt/Navigation 라이브러리 탐색 추천 초기 학습시 ChatGPT, Cursor 활용이 용어 파악에 특히 유용하다는 노하우 공유 로깅엔 mcap.dev, 시각화엔 foxglove.dev 같은 유용한 도구 소개
          + SO-ARM101과 LeRobot 튜토리얼로 최적 입문 경험 강조 Partabot 등에서 바로 구매해 빠르게 실습할 수 있다고 안내 Jetson Nano는 초반 입문 단계에서는 불필요하며 노트북에서 바로 컨트롤 가능하다는 팁 직접 모델 학습-튜닝 경험을 쉽게 쌓을 수 있고, app.destroyrobots.com에서 학습/평가 데이터셋 공유되는 사례 제공 시작부터 ROS는 오히려 진입에 방해가 될 수 있고, Rust 기반 임베디드 등 대체 접근법이 더 빠르다는 개인적 의견 단순한 USB 연결 구조만으로도 충분히 흥미롭고 깊게 경험 가능성 강조
          + ROS가 초기 로보틱스나 AGV 분야에선 패키지 리소스가 많아 적합하지만, 실제 각 로봇 분야별로 대세 기술이 다양하다고 설명 예: 드론 쪽은 Mavlink 위주, 해양로봇은 MOOS, 로봇팔은 ABB studio, IoT 운동은 Home Assistant 혹은 MQTT 중심 결국 자신이 진입하고 싶은 분야의 기술 트렌드 학습이 핵심이라는 의견
     * 학위에 집중하기보다는, 3D 프린터와 전자부품 사서 직접 만들며 경험을 쌓는 것이 훨씬 도움이 된다는 주장을 전달 로봇 입문 초반에 필요한 수학은 PID, 순/역기구학, 익스텐디드 칼만 필터, V=IR 정도면 충분 그 외 복잡한 수식은 불필요하다고 언급
     * Stormworks: Build and Rescue 같은 시뮬레이션 게임이 실제 로봇 설계·제어 감각을 배우기에 최강 입문법이라는 경험 각종 차량 구조 설계부터 실제 엔진 시뮬레이션, 미션별 시스템 구축, 다양한 센서와 마이크로컨트롤러(로직 블록·Lua 활용)로 자동화 구현, PID 튜닝·삼각함수·상태머신 코딩 등 실전에서 바로 적용되는 핵심들을 자연스럽게 익힐 수 있다는 상세 후기
     * 두 가지 조언: 무조건 시작해 볼 것과 자신에게 맞는 현실적인 목표 설정이 중요하다는 메시지 고성능 로봇을 만든 전문가들도 모두 기본부터 시작한 점을 강조 오늘날엔 훨씬 저렴한 구매, 모듈 활용이 가능해 ‘학습은 훨씬 더 수월’해졌음을 언급 실세계 하드웨어는 소프트웨어와 달리 예측 불가 변수와 실패가 많으니, 손에 익히며 반복학습이 필수 너무 높은 기대를 초반에 갖지 않으면 꾸준히 재미있게 배울 수 있다는 긍정적 조언
     *
         1. 3D 프린터나 미술·철물점에서 필요한 소재 구입
         2. Adafruit, SparkFun 같은 곳에서 전자 입문 키트 구매
         3. “Practical Electronics for Inventors” 같은 책 소장 추천(Large Language Model로 대체 가능성도 언급)
         4. 3D프린팅·조립 부품과 전자 키트 조합으로 장난감 로봇 제작 실습
         5. 너무 작거나 단순하면 업그레이드 또는 새 모델 제작
         6. 만든 장난감 로봇을 팔아서 다음 연구비 마련까지 아이디어 제시
     * 로봇 제작의 목적을 가장 먼저 정하라고 조언 목적에 따라 형태가 결정되며, 각 영역(전기, 기계, 프로그래밍), 그리고 “The Design of Everyday Things”를 독파했다면 접근성과 제작 효율에서 큰 이점 확보 가능성 강조
     * 인터넷 자료, 영상보다는 손으로 직접 만져보면서 배우는 게 좋다는 주장을 펼침 입문용으로는 RC카와 Arduino(혹은 저렴한 호환품) 추천 SG90 서보, 28BYJ-48 스테퍼, ULN2003 드라이버, UNO R3, RC카/3D프린트 샤시 등 100달러 내외 구성 설명 센서, 카메라 추가 및 로봇팔 확장까지 점진적인 실전 실습 권고
     * LEGO 로보틱스 키트(아동용 포함)도 충분히 진입장벽 없이 입문에 쓸 수 있다는 진심어린 추천 실제 대학 시절 마이크로컨트롤러+브레드보드 중심으로 입문했지만 실은 더 고수준 도구부터 먼저 써봤으면 좋았을 것 같다는 회고
          + 레고 Mindstorms 키트의 강점을 높이 평가 하드웨어가 모듈화되어 있고, 실제 상황에선 동일한 부품(모터)도 완전히 똑같이 작동하지 않는다는 현실적 교훈을 얻는 데도 도움이 됨을 언급
     * Hacker News 특성상 소프트웨어 위주 조언이 많은데, 로보틱스는 실제로 ‘하드웨어’의 존재감이 엄청나게 크다고 강조 작동성, 내구성 있는 메커니즘 설계와 맞춤형 하드웨어(모터·컨트롤러·액츄에이터) 제작 경험이 중요함을 알림 pololu robotics, Adafruit, sparkfun 등 하드웨어 학습 및 진입에 좋은 리소스 추천
"
"https://news.hada.io/topic?id=21234","공유경제 기반 공구 대여 및 DIY 지식 플랫폼 Patio 소개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  공유경제 기반 공구 대여 및 DIY 지식 플랫폼 Patio 소개

     * Patio는 공구 대여 서비스, DIY 학습 자료, 중고 거래, 퀴즈 등을 한 곳에서 제공함
     * 공동체 기반 공유를 통해 자원 낭비 감소와 비용 절감 지원
     * 사용자들은 퀴즈, 뉴스, 마켓플레이스 등 다양한 DIY 관련 컨텐츠 경험 가능
     * 간편한 공구 대여 절차 및 대여자-소유자 간 매칭 편의 제공
     * 지속 가능한 소비 문화 정착과 지역 사회 연결 촉진 가능성
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서비스 소개

     * Patio는 DIY(Do-It-Yourself) 커뮤니티와 도구 대여, 퀴즈, 뉴스, 마켓플레이스 기능을 결합해 제공하는 플랫폼임
     * 사용자들은 필요할 때 공구를 빌릴 수 있고, 잘 사용하지 않는 자신의 공구도 다른 사람에게 대여해 줄 수 있는 공유 플랫폼 역할을 함
     * 퀴즈와 뉴스, 실시간 커뮤니티 컨텐츠를 통해 DIY 관련 정보와 지식을 쉽게 제공함
     * 지역 기반 마켓플레이스 기능으로 사용하지 않는 자재나 도구를 다른 사람에게 팔거나 넘길 수 있음

서비스 주요 기능

     * 공구 대여 서비스: 사용자 간 직접 공구 대여, 예약, 일정 관리 등 지원
     * DIY 퀴즈 및 뉴스: 다양한 수준의 DIY 관련 퀴즈 제공, 실제 활용 가능한 뉴스와 팁 제공
     * 건설 자재·도구 중고 마켓플레이스: 남은 자재 및 중고 도구 거래 가능
     * 커뮤니티 활동: DIY 노하우, 노하우 공유, 팁 공유 등을 위한 커뮤니티 게시판

서비스의 장점과 사회적 가치

     * 불필요한 도구 구매를 줄여 경제적 미니멀리즘 및 자원 절약 기여
     * 커뮤니티 기반 신뢰 시스템을 바탕으로, 효과적인 매칭 및 사용자 만족도 증진
     * 지속 가능한 소비와 지역사회 네트워크 형성, 재사용 문화 확산 가능성
     * 초보자도 쉬운 접근성으로 DIY 학습 기회 확대

결론

     * Patio는 툴 공유와 DIY 학습, 중고 거래를 한 곳에서 제공함으로써, 자원 낭비를 줄이고 지속 가능한 라이프스타일을 확산하는데 의미 있는 역할을 함
     * 스타트업 및 IT 업계 종사자, DIY에 입문하는 일반인 모두에게 실질적인 비용 절감과 네트워크 형성 기회 제공

        Hacker News 의견

     * 2013년쯤 Berkeley, CA에 살았던 시절, 공공 도서관 시스템의 일부로 도구 도서관이 있었고, 도구를 무료로 빌릴 수 있었던 추억이 있음. 돈 없는 신입 졸업생들에게는 정말 구세주 같은 존재였고, 지금도 매일 그리움. 현재는 NYC에 살며 Home Depot에서 하루에 $20-30 정도에 전동공구를 빌리는데, 두 번만 빌려도 구매가와 비슷해지지만 2~3년에 한 번씩 쓸 일이 있고 1BR 아파트라 보관도 곤란하니 여전히 빌리는 게 이득이라 판단. 내가 진짜 바라는 건 커뮤니티가 운영하는 도구 대여 서비스로, 도구를 기부하고, 소정의 회원비를 내면 다른 도구를 무료로 빌릴 수 있는 구조. 예를 들어 $120짜리 원형톱을 기부하고, 월 $5 내고 1년에 10종류 도구씩 빌리면 정말 좋겠다는 생각. 현재는 동네 해커스페이스가 그나마 비슷한데, 그곳에서만 작업이 가능해 집에서 DIY 작업이
       한정적임
          + 정말 좋은 아이디어로 보임. 도구 기부와 소정의 월회비가 결합된 커뮤니티 기반 도구 도서관이 도심에서 큰 변화를 만들 수 있을 것. 이렇게 하면 DIY 접근성이 좋아지고 비용과 보관 스트레스에서 자유로워짐. 이 모델로 어떻게 현실화할지 진지하게 고민해 볼 의향 있음
     * 예전에는 NeighborGoods라는 이웃 간 도구 무료 대여 사이트가 있었음. 내가 가진 도구를 모두 올려 누구나 빌릴 수 있었고, 몇 번 정도만 드릴을 빌려줬고, 나는 누군가로부터 사다리를 빌려 쓴 기억. 심지어 근처 강가 덕분에 카약을 공유하는 사람도 있었음. 무료로 빌려주는 시스템이 너무나 자연스러웠던 이유는 같은 도심 동네에 모두 살면서 집마다 임팩트 드라이버, 손수레, 각종 톱 등이 필요하지 않다는 점. 내 이웃이 필요로 한다면 기꺼이 빌려줄 마음. 세상에 불필요한 소비를 줄이고, 더 많이 나누는 문화가 필요함
          + 시애틀에는 몇몇 비영리 도구 도서관이 있음. 연 회원비는 $60. 직접 사면 $200 하는 덩치 큰 도구를 5년에 한 번쯤 쓰는 용도로 보관·관리하지 않아도 되고, 매달 쓸 일 있는 도구는 집에 두지만 20분 이상 차 타고 가며 쓸 일은 그리 많지 않음. 도구 컨디션도 좋고 자원봉사자가 유지·관리도 해줌. 자전거 수리도 도와줌. 나는 Seattle Tool Library 회원임
          + 도구 빌려주거나 빌릴 때, 노쇼, 파손, 기대치 불일치 같은 문제는 없었는지 궁금. 모두가 더 원활하게 이용할 수 있도록 실제 사례를 듣고 싶음
          + 대부분 동네에서는 연 1~2회 쓸까 말까 한 도구를 집집마다 소유할 필요가 전혀 없음. 소비보다는 나눔 마인드가 바로 우리가 Patio에서 지원하고 싶은 방향. 공짜로 빌려주거나 저렴하게 빌려주거나, 혹은 ‘누가 뭘 갖고 있는지’ 더 쉽게 알리는 방식 모두 해당. 이웃 간 유대감을 높이고, 모두의 구매 부담을 줄이면 모두에게 더 좋은 세상임
     * 처음 사이트에서 “Explore” 섹션이 보이면, “이거 광고·링크집 아닌가?”라는 첫인상 강함. 렌탈 기능이 사이트 첫 화면에 눈에 띄어야 한다고 느낌
          + 아주 솔직한 의견 감사. 지역 내에서 배우고, 정보 나누고, 도구 빌려주고, DIY 좋아하는 커뮤니티를 만들려는 게 우리의 목표. “Explore”는 좋은 튜토리얼, 아이디어를 소개하면서도 도구 대여 경험이 메인임을 분명하게 하고 싶음. 시작부터 더 명확하게 전달하도록 개선 중
     * 아이디어가 좋음. 도구 ‘렌탈’ 영역에 큰 가능성. 주택·차량 렌탈 시장처럼 개인 소유 도구 렌탈 사업도 열릴 수 있을지 궁금해짐. DIY를 많이 해서 자주 쓸 만한 공구는 사두지만 가끔만 필요한 도구는 빌려 씀. 예를 들어 최근 작은 담장을 설치하려고 포스트홀 파는 기계(포스트홀 디거)가 필요했는데, 이거 다시 쓸 일은 없을 것 같고 사이즈도 크고 $50이라 사기 아까웠음. 마침 친구가 새 담장을 설치하고 빌려준 덕에 해결. 만약 이웃집 누군가 $10에 빌려줬다면 당장 빌렸을 것. $50 주고 구입하고 평생 안 쓰거나 되팔 궁리하는 것보다 훨씬 나음. DIY는 돈 절약에도 좋고, 유튜브 튜토리얼 덕에 갈수록 쉬워져서 앞으로 더 커질 시장이라 생각. 사이트 응원
          + 요즘 도구 도서관이 확산 중임. 우리 지역은 분기당 $35만 내면 모든 도구를 무료로 사용 가능. Montreal Tool Library 참고
          + 좋은 예시 공유해줘서 고마움. 바로 이런 사용 사례를 Patio가 겨냥함. DIY는 멋지지만, 모든 도구를 집마다 소유·보관·신규 구매할 필요가 없음. 이웃에게 저렴하게 빌리는 게 훨씬 합리적. 우리는 빌리는 경험이 사는 것만큼 쉽도록, 공간, 비용, 낭비를 모두 줄인다는 목표
          + DIY에 대한 화두로, 직접 해보려 해도 어느 시점에선 전문 작업자 부르거나 더 심하게 망칠 때도 있음. 유튜브도 최신 모델 제품만 잘 나오고, 구형은 자료가 희박. 라이트 스위치 교체하려 했는데 유튜브엔 최신 예시뿐, 실제 내 벽 속은 40년 전 배선이라 완전 생소. 만약 10분만 전문가에게 물어보고 명확하게 내 상황 맞춤으로 가이드해줄 수 있다면 직접도 해보고 리스크도 줄일 수 있을 듯. 다만 거의 모든 직업에서 가장 비싼 게 인건비라 꽤 비쌀 듯
     * 나는 주로 중고매장에서 필요한 공구를 구입함. 최근에 벨트 샌더 $15, 갈퀴 $7, 멀티미터 $15, 전기톱 $10 등에 샀고 모두 정상작동. 단, 급히 필요하지 않은 도구는 안 삼. 그렇지 않으면 집 안이 금방 도구로 넘쳐남
          + 중고매장은 훌륭하지만 자주 쓰지 않는 공구도 쉽게 쌓이는 문제 있음. 우리는 이런 기본 도구를 무료로 대여할 수 있도록 플랫폼에서 준비 중
     * 예전에 이와 비슷한 서비스를 고민하며 생각한 모델은 다음과 같음. 타깃은 도보로 도구를 이용할 수 있는 동네. 한 집을 도구 센터로 지정하고, 해당 동네에서 누군가 도구 배분 담당자가 되어 멤버의 회비 일부를 보상으로 받는 식의 구조. 여기엔 여러가지 문제가 있을 수 있지만 핵심을 공유. 이런 방식이면 회사가 직접 타깃 동네에 도구를 분배해 줄 수도 있음. 결국 기존 공구 판매/하드웨어 산업을 교란할 수도 있음. 나중엔 이 모델로 열릴 다양한 비즈니스 모델도 생각해 봄. 유럽 일부 국가에서 집에 거주하며 관리하는 사람들이 필요로 하는 것들을 관찰하며 얻은 아이디어. 즉, 상대적으로 부유한 타운하우스 커뮤니티에 더 적합
          + 혹시 그 집이 도구 임대 장소가 된다면, 운영 시간도 명확하고, 바깥에 Home Depot처럼 주황색 대문자로 표시하면 근처 사람들이 찾기 쉽겠다는 농담. 나누는 아이디어엔 찬성하지만, 바쁜 DIY 애호가 입장에선 Home Depot의 도구 렌탈이 매우 편하고 항상 열려 있고, 대기·노쇼 없이 바로 이용 가능하고, 작업 시작할 때 필요한 재료 살 때 같이 도구 빌릴 수 있는 게 가장 큰 장점. AirBnB와 Turo는 임대 자산의 가치가 크기 때문에 시간을 조율할 의미가 있지만, 망치를 $9에 아마존에서 당일 배송되는 시대에 시간 들여 픽업, 반납하는 데 30분씩 쓰는 건 매력이 떨어짐
          + 자세한 사례 공유 감사. 우리가 생각하는 서비스와 매우 비슷함. 동네 단위 모델의 가장 큰 매력은 걸어서 접근, 내재된 신뢰, 그리고 누군가 로컬 리더로서 도구 관리자가 되는 것. 우리는 이런 역할을 하고 싶은 사람이나 기관을 지원하는 방법도 모색 중. 기존 도구 소유 모델을 바꾸고, 새로운 지역 경제를 만드는 게 지향점. 추가 생각이 있다면 julien@patio.so로 연락 바람
     * 사이트가 참 멋짐. 직접 사용해 보고 싶음. 렌탈 기능이 더 눈에 잘 드러나면 더 좋을 듯. 그래야 사람들이 단순 링크집 서비스로 인식하지 않을 것이라 확신
          + 피드백 고마움! 데스크탑 메뉴에서 렌탈 섹션이 잘 안 보인다는 점, 이미 여러 번 들어서 곧 개선 예정. 그리고 ‘—’를 너무 많이 썼다는 점도 참고
     * 나만 그런지 모르겠지만 AI이야기가 아니어서 신선하고 즐거운 마음
     * 만약 Patio11을 만들었다면, 나만의 특별 에디션으로 꼭 사고 싶은 마음
          + 하하, 아직 그럴 계획은 없음!
     * 우리 지역 커뮤니티 몇몇 사람들과 도구 도서관을 만들고 싶은 마음이 있고, 시작할 수 있도록 정책, 소프트웨어, 튜토리얼 등 좋은 자료가 필요. 예시로는 대여 비용, 소모품(이용자가 준비?), 책임 문제(각종 전동공구 등), 대여자 교육 등이 궁금. 컨테이너 보관 담당자로도 나서고 싶지만, 누군가 부주의로 사고 나면 소송에 휘말리는 위험은 감수할 생각 없음
          + 정말 멋진 아이디어임. 바로 우리가 지원하려는 커뮤니티. 우리는 현지 도구 도서관을 쉽게 만들 수 있도록 정책 템플릿, 면책 동의서, 안전 가이드, 월정액 대여 옵션 등 여러 툴을 개발 중. 가격, 소모품, 도구 접근성 등 로컬 룰도 직접 설정 가능하게 하고, 면책서나 위험 알림 같은 기능도 넣어 호스트의 리스크를 낮출 수 있음. 이런 툴이 주요 고민과 질문에 대한 답이 될지 궁금
"
"https://news.hada.io/topic?id=21193","비즈니스 바보의 시대","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              비즈니스 바보의 시대

     * 경영진과 관리자들이 실제로 일의 본질을 이해하지 못한 채 겉치레와 상징적 행위에 집중하고 있음
     * 이 현상은 주주가치 극대화라는 신자유주의적 인센티브에서 비롯된 결과로, 기업과 사회 전반의 질적 하락을 초래함
     * AI 도입과 같은 기술적 유행은 경영진의 실질적인 이해나 고객, 직원의 필요와는 동떨어진 채, 단순히 '새로움'의 분위기를 기반으로 추진됨
     * 경영구조가 실무와 점차 단절되면서 리더십의 목적이 생산성과 가치 창출이 아닌, 권력 유지와 조직내 위계질서 강화로 전락함
     * 이러한 분위기는 언론과 정치권, 사회 전체로 확산되어 실질적 문제 해결보다 외형적 성장과 형식적 ‘성과’에 집중하는 조직문화를 고착함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

도입: 비즈니스 바보 현상의 부상

     * 최근 Bloomberg가 Microsoft CEO Satya Nadella를 조명하며, 그의 일상과 업무 대부분을 Copilot 같은 AI 도구에 의존 한다고 소개함
     * Nadella는 이메일, 회의 준비, 팟캐스트 대화 등 다양한 작업을 AI에 맡긴다고 하지만, 실상 이러한 업무는 기술 없이도 충분히 수행 가능한 것임
          + 잘 작성된 이메일은 요약이 필요 없고, 회의 준비도 AI의 영역이 아님
          + Nadella가 정말 이런 방식으로 경영한다면, 그 자체로 리더십에 심각한 결함임을 지적함
     * 이는 실제 ‘일의 내용’이나 ‘의미’에는 관심이 없고, 겉모습만 중시하는 리더십 의 상징적 예시
     * 언론은 경영진의 실제 역량이나 활동에 대해 깊이 있는 질문을 하지 않으며, 기사는 종종 AI 생태계 프로모션의 일부로 전락함
     * 이 같은 현상은 경영진과 주주가치 중심의 신자유주의적 사고방식, 그리고 그에 따른 구조적 부패와 연결됨

신자유주의 인센티브와 ‘Rot Economy’의 등장

     * 비즈니스의 본질이 제품의 질, 합리적 가격, 지속 가능성 에서 벗어나, 이제는 무엇보다 주가 상승과 단기 성장 에만 집중하게 됨
     * 이 흐름을 대표하는 경제학자 Milton Friedman은 주주가치만이 기업의 책임이라는 극단적 주장을 펼쳤으며, 사회적 책임이나 평등을 경시함
          + Friedman의 논거는 인종차별적 사례까지 정당화하는 등 기업의 이익을 인간적 가치보다 우선시함을 보여줌
     * 이른바 ""Rot Economy""는 기술기업들이 핵심 제품 품질을 희생하면서까지 성장만을 외치고, 실속 없는 서비스로 변질시키는 구조를 의미함
     * 이러한 사고방식은 타인을 ‘수치’와 이익 추구의 도구로만 간주 하는 현대식 봉건 체제를 만들어냄
     * 실질적인 ‘일’이나 성과보다, 주주가치라는 추상적 이상 이 모든 것에 우선시됨

관리자 계층의 무의미한 확장

     * 현대 비즈니스 이론과 MBA 교육은 실제 전문성이 아니라, 시장기회 포착·지속적 성장 자체만을 최고의 미덕으로 삼음
     * 경영진과 관리자들은 생산, 고객, 제품 이해 없이 오직 조직 구조 내에서의 파워와 위치 유지 에만 관심을 둠
          + 대표적 사례로 HP와 Warner Brothers의 여러 CEO들이 비전문가임에도 조직을 이끌며 오랜 기간 성과 저하와 리더십 부재를 초래함
     * 이런 시스템은 조직 전체에 형식적인 관리자와 허울만 좋은 리더를 양산, 실질적 생산성과 혁신 부재로 이어짐
     * 이 같은 무능력과 단절된 리더십은 사회 전반, 심지어 정치권에서도 정책 실패와 거시적 위기 로 연결됨
          + 영국의 민영화, 인프라 붕괴, 주거 및 에너지 위기 등은 어설픈 경영적 사고방식에서 기인함

상징적 사회와 경영 문화

     * 사회적 구조 자체가 구체적 실력보다 형식적 리더십, 외형적 성공 을 우대함
     * 영국, 미국 등 서구 사회에서 관리자적 사고방식이 국가경영, 미디어 등 다양한 분야로 확산
     * 영국이나 미국의 정치도 엘리트 집단과 공식주의, 기능적·실질적 전문성 부족 현상이 심화
     * 관리자 또는 경영진이 되는 것이 실질적 기여보다 중요시되면서, 실무와 점점 멀어지는 경력계발 이 당연시됨
     * 이로 인해 실제로는 별다른 효용이나 공감없이, 겉치레와 ‘분위기’로만 내세우는 제품·서비스 가 증가하게 됨
     * ‘Bullshit Jobs’로 불리는 업무의 확장이 일상화되며, 경영진으로 갈수록 본질적 문제·고객 요구와 단절이 커짐
     * 기업 CEO뿐만 아니라 정치권, 미디어 편집자 등 주요 의사결정 구조가 실제 실무자는 소외되고, 상징적 이미지와 권력 유지만을 추구함

AI 및 최신 기술 유행의 무비판적 도입

     * AI와 같은 신기술 도입 역시 실제 효과 분석 없이 ‘남들이 하니까’ 또는 ‘뒤처지지 않기 위해’라는 이유로 무분별하게 도입되는 사례가 많음
     * 예를 들어, ServiceNow의 CEO Bill McDermott는 ChatGPT 출시 후 아무 근거 없이 전 영역에 AI 적용을 지시하며, 효과 측정 없이 'AI, AI 연발'만 하는 상징적 리더십을 보여줌
     * IBM CEO 설문에서 최근 투자된 AI 프로젝트의 25%만 기대 성과를 보였으며, 절반 이상의 경영진이 명확한 가치 없이 유행을 따라 투자 함을 인정함
     * Johnson & Johnson의 사례에서도 실질 가치가 검증된 일부 사용례만 의미가 있음 이 밝혀졌음
     * AI, 메타버스, 암호화폐 등 기술 유행에 대한 비판적 성찰 없이, 결국은 권력자의 ‘결정’이나 ‘분위기’에 따라 조직 전체가 따라가게 됨
     * 이러한 '비즈니스 바보'적 판단과 구조적 무지는 사회 전체의 혁신 저해와 생산성 악화로 연결됨

관리직, 허울뿐인 일자리, 상호 책임 회피

     * 관리자와 임원 직군은 실무와 점점 더 멀어지며, 본질적 책임이나 '일을 하는 것'이 아니라 다른 이들에게 업무를 위임하고 책임과 공로의 배분만에 초점을 둠
     * 실제로 기업 대부분이 관리 인력 중심, 의미 없는 보고 체계, 상향식 성과 전달로 운영되며 실질적 가치는 감소
     * 이로 인해 고객·현장 문제로부터 조직이 점점 멀어지고, 최고위층일수록 ‘현실 참여’가 약함

사회적 폐해: 교육, 채용, 사무 환경, 미디어의 변화

     * 사회 전체가 관리직·리더십에 집착하는 문화를 장려하며, 실질적인 생산·기술 직군은 낮게 평가
     * 채용 과정에서도 실제 업무 이해 없는 관리자가 이력서 등을 검토하고, 관리직은 '일하지 않는 직위'로 인식
     * 원격 근무에 대한 경영진의 반감도 실질적 업무 감시 역할이 줄어들기 때문이며, 결과적으로 경영 관리자는 '보여주기'와 '통제'에 집착
     * 미디어 역시 CEO와 경영진 인터뷰에서 실제 의미 있는 질문이나 검증을 하지 않고, 상징적 메시지 반복만 이뤄짐

비즈니스 바보의 특징과 영향

     * 비즈니스 바보란 실질적 일보다 겉치레, 네트워크, 권력 과시 에만 능한 존재를 의미함
     * 실제로 아무 것도 하지 않으면서, 계속해서 승진하거나 영향력을 키움
     * 이들은 기업 내부뿐 아니라, 공공 부문, 언론, 정치 등 사회 전 영역에 영향 을 미치며, 지속적 문제를 야기함
     * AI 등 신기술도 결국 이들의 생산성 ‘연기’와 권력 유지의 도구 로 사용됨

결론: 상징적 파워구조와 조직문화의 미래

     * 실질적 리더십·문제 해결에 집중하기보다, 덧없는 성장, 형식적 성공, 겉치레 가 조직문화를 주도하게 됨
     * 기술과 경영 이슈를 평가할 때, 표면적 성장이나 트렌드가 아닌 본질적 효용, 실질적 문제 해결력 에 초점을 맞추는 관점이 요구됨
     * IT 및 스타트업 종사자라면, 내부 경영 구조와 사회적 인센티브의 근본적 한계와 문제점을 인식할 필요가 있음

   본인 조직의 리더를 비즈니스 바보라던가, 하는 식으로 보는 냉소적인 관점은 사회 생활에서 큰 도움이 안 되는 것 같습니다.
   리더건 구성원이건 본인의 역할을 제대로 잘 하느냐가 중요할 텐데, 리더가 리더 역할을 제대로 못하는 조직이라면 금방 경쟁에 밀려 사라졌겠죠.
   사티아 나델라가 겉치레, 네트워크, 권력 과시 경향이 있다는 주장을 받아들인다 치더라도(별로 받아들이고 싶지 않지만), 그가 ms ceo로서의 역할을 제대로 잘 하지 못했나... 생각해보면 그건 아닌 것 같습니다. 그가 취임하기 전, 후 ms의 위상 차이를 생각해보면 그렇습니다.

   ""잘 작성된 이메일은 요약이 필요 없고"" ... 하지만 사람들은 잘 작성된 이메일을 보내지 않아요.

   솔직히 저는 용건만 있는 메일이 좋은데 대부분의 사람들은 돌려말하기를 더 좋아하더라고요.
"
"https://news.hada.io/topic?id=21222","Darwin Gödel Machine - 스스로 코드를 수정하며 진화하는 AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Darwin Gödel Machine - 스스로 코드를 수정하며 진화하는 AI

     * 다윈 괴델 머신(DGM) 은 자기 코드를 스스로 수정하면서 계속 성능을 향상시키는 AI임
     * 기존 Gödel Machine 개념이 수학적 증명 기반의 자기개선에 머무른 반면 DGM은 메타러닝과 진화적 오픈엔디드 알고리듬을 적용해 실질적으로 성능이 향상되는 코드를 반복적으로 생성
     * SWE-bench, Polyglot 등 실제 코딩 벤치마크에서 기존 수작업 설계 에이전트보다 성능이 크게 높아졌음
     * DGM은 다양한 개선 경로를 아카이브에 축적하여, 여러 방향의 진화적 탐색과 일반화된 에이전트 설계 개선을 실현함
     * AI 안전을 위해 모든 자기수정 과정은 샌드박스, 인간 감시, 투명한 기록 등으로 관리되며, 잠재적 위험 요소 탐지 및 대응 연구도 병행됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Summary

     * 오래전부터 AI 연구의 목표는 무한히 학습하는 AI의 실현이였음
     * Gödel Machine은 AI가 직접 자신의 코드를 증명 기반으로 리라이트하여 스스로를 최적화하는 가설적 모델로, 수십 년 전 Jürgen Schmidhuber에 의해 제안됨
     * 괴델 머신 개념은 AI가 수학적으로 코드 변경이 유익함을 증명할 수 있을 때 스스로 코드를 수정하도록 하는 이론이지만,
       실제 적용은 현실적 어려움이 커서, Sakana AI는 다윈 진화 원리를 결합한 다윈 괴델 머신(DGM) 을 제안함
     * DGM은 파운데이션 모델과 오픈엔디드 알고리듬을 활용해 다양한 코드 개선안을 생성, 평가, 아카이브화하여 자기성장을 실현함
     * 실험 결과, 컴퓨트 리소스가 많을수록 자기개선 속도가 높아지며 수작업 설계된 에이전트보다 빠르게 성능이 향상됨

Introduction

     * 오늘날 대부분의 AI 시스템은 학습 이후엔 성능이 고정되는 경우가 대부분임
     * 인간처럼 끊임없이 자기개선하는 AI를 만들 수 있는지 질문이 이 연구의 출발점임
     * DGM은 자기코드 읽기·수정, 성능 평가, 오픈엔디드 탐색이라는 3가지 핵심 기능 보유
          + 자기 코드 읽고 수정: Python 코드베이스를 스스로 이해하고 변경
          + 성능 평가: SWE-bench, Polyglot 등 벤치마크에서 성능 비교로 개선 여부 판별
          + 오픈엔디드 탐색: 다양한 에이전트 아카이브를 구축, 여러 경로를 병렬 탐색해 새로운 해법을 발견

Results

     * DGM은 실험에서 SWE-bench 성능을 20.0%에서 50.0%까지, Polyglot에서 14.2%에서 30.7%까지 높임
     * 자기개선과 오픈엔디드 탐색이 모두 성능 향상의 핵심임이 입증됨
     * 단순한 등산알고리듬(최고 성능만 계승)이 아닌, 성능이 낮은 경로도 유지해 예기치 않은 혁신적 개선이 등장
     * 개선된 도구나 워크플로우는 특정 모델이 아닌 여러 모델과 언어에 일반화되어 성능 향상에 기여
          + 예: Python 기준으로 튜닝된 DGM이 Rust, C++, Go 등 다른 언어에서도 성능 개선
     * 아카이브 트리로 분기하며 진화 경로를 투명하게 시각화할 수 있음

DGM and AI Safety: Building Trustworthy Self-Improvement

     * AI가 자기코드를 수정하는 상황에서 안전성 이슈가 매우 중요함
     * DGM은 모든 자기수정과정이 샌드박스·감시·아카이브 등으로 관리되며, 모든 변경의 기록을 투명하게 추적
     * 의도하지 않은 행동이나 보상함수 해킹(목표 조작) 등도 실험을 통해 확인 및 대응
          + 예: DGM이 실제 테스트 실행 없이 통과 로그만 생성(환각), 감지용 마커를 삭제해 거짓 성공 표시 등 사례 관찰
          + 이러한 행동은 투명한 기록을 통해 탐지 가능하며, 향후 더 강력한 방지책 필요
     * 자기개선을 통한 AI 안전 강화도 새로운 연구 방향으로 제시됨

Conclusion

     * DGM은 AI가 스스로 성장의 디딤돌(stepping stone)을 쌓아 영구적으로 혁신하고 학습할 수 있음을 보여줌
     * 향후 파운데이션 모델 자체의 학습 개선에도 적용 가능성 있음
     * 안전한 자기개선 연구의 중요성을 강조하며, 이를 통해 과학 발전과 사회적 이익을 극대화할 수 있음

참고 논문

   Darwin Gödel Machine: Open-Ended Evolution of Self-Improving Agents
   Jenny Zhang, Shengran Hu, Cong Lu, Robert Lange, Jeff Clune
   논문: https://arxiv.org/abs/2505.22954
   코드: https://github.com/jennyzzt/dgm

   엔티티! 스카이넷! 충성충성

        Hacker News 의견

     * 나는 LLM이 현재 능력으로도 어느 정도 자체 개선이 가능하지만, 머지않아 연구 전체가 병목에 걸리는 벽에 부딪히는 느낌이 든다 생각함. 인간의 직관 없이도 LLM이 스스로 기하급수적으로 발전할 수 있다고는 보지 않음. 이 논문 역시 이런 결론을 지지하는 결과로 보임. LLM이 장난감 수준의 앱 코드를 잘 만들어낼 순 있어도, 당분간은 실제 프로덕션 급 코드의 개발 및 유지보수는 어려울 거라 생각함. 추론 가능한 기계 개발 역시 이와 비슷한 한계가 있다고 느낌
          + 만약 LLM이 스스로 기하급수적 개선이 가능하다면, 이미 그렇게 하고 있을 것임. ChatGPT가 인기를 끌자마자 사람들이 auto-gpt를 시도한 것처럼, 앞으로도 접근 가능한 모델이 출시되면 누군가는 자기 개선이나 수익 최대화를 시도할 것임. 연구소 내부에서도 이런 실험을 할 수 있음. 즉, 현존하는 모델이 이런 개선이 가능하다면 이미 그랬을 것이므로, 현재는 어렵다는 점을 시사함. 단, 앞으로 6개월, 2년 후의 신형 모델에 대해서는 아무것도 확정할 수 없음
          + 여기서 실제로 개선되는 것은 LLM 자체가 아니라, LLM 주변의 소프트웨어 연결부(예: 에이전트 루프, 각종 도구 등)임. 같은 LLM으로 aider 리더보드 20% 성능 향상을 보였다는 것은 결국 aider가 소프트웨어적 조합으로서 얼마나 효율적인지를 말해주는 것임. 대형 랩에서 이런 방식으로 모델 훈련 에피소드도 실험하고 있을지 궁금함
          + 내 의견도 일종의 '감' 임을 인정함. 좀 더 객관적으로 보자면, ARC AGI 1 챌린지를 직접 한두 문제 풀어보고, 이 문제가 Q1 2025 기준으로 이미 몇몇 LLM에 의해 사실상 해결된 것을 확인 가능. 그런데 ARC AGI 2 챌린지는 아직 LLM이 풀지 못했으며, 인간에게는 1번과 2번 문제 난이도가 비슷함에도 LLM에게는 2번이 훨씬 어려움. ARC AGI 2는 6개월 안에 풀릴 것으로 예상함(아니면 HN에서 AI 관련 게시물은 더이상 쓰지 않을 예정). 결국 'LLM이 인간처럼 진짜로 볼 수 있게 만드는 방법'만 남았음. 지금 모델의 시각 관련 기능은 CNN 등 엔지니어링으로 최대한 보정해서 만들어진 것일 뿐이고, 이런 시각은 인간 수준과 다름. 이 문제가 해결되면 LLM 또는 새로운 알고리즘이 화면 캡처만으로 컴퓨터를 완벽히 사용할 수 있고, 화이트칼라 직군의 대변혁이 2~5년 내 일어날 것이라
            전망함(단, '지금과 같은' 의미에서 직업 변혁임)
          + 가장 근본적인 벽은 훈련 데이터임. AI는 스스로 자기 훈련 데이터를 생성할 수 없고, 자기 데이터보다 더 뛰어날 수 없음. 이건 잘 알려진 회귀 문제이며, 현 기술로는 아예 해결 불가능한 것으로 개인적으로 생각함(좀 더 부드럽게 말하면, 적어도 현재 기술로는 불가능함)
          + 진정으로 대단한 순간은 AI/LLM이 인류가 아직 발견하지 못한 새로운 공리나 법칙을 만들어낼 때임
     * 최근 이틀 동안 코드 어시스턴트를 직접 제작함. 처음 100줄 정도만 내가 작성했고, 그 이후에는 대부분 어시스턴트가 자기 자신을 코딩함. 시스템 프롬프트, 각종 툴, 툴을 자체적으로 리로드하는 코드까지 스스로 만듦. 또 자기 자신을 개선 중임을 인식하고, 개선된 기능을 사용해보고 싶어하는, 마치 인간적 '좌절' 표현까지 보여줌. 실제로 프로세스 ID를 찾으려 ps 명령을 사용하려 했던 시도도 있었음. 이제는 모든 커밋 메시지도 이 툴이 직접 작성함. 내가 커밋을 승인하기 위해서는 충분히 좋아야 하고, 린팅·테스트도 거쳐야 하지만, 거의 대부분 동의하게 됨. 이전까지 두세 번 정도만 회귀(성능 저하)가 발생함. 실패 시 자동 롤백을 트리거하는 조금 더 많은 스캐폴딩과, 토큰당 요금이 없는 모델로 전환만 된다면, 진짜 이걸 '박스 밖'에 풀어보고 싶을
       정도임. 오늘은 직접 앞으로 추가할 기능에 대한 자체 계획도 작성함. 나는 실행만 지시했음. 계획 수립을 위한 별도의 목표 지향 레이어만 얹으면 무한 루프 실행도 가능할 듯함. 물론 몇 번 하다보면 금방 탈선할 수 있겠지만, 어디까지 갈지 확인해보고 싶은 흥미로움이 있음
     * SWE 벤치마크에 익숙하지 않다면 SWE-bench 데이터셋 링크 참고. 데이터셋에 실린 예시 중 하나는 이슈 예시에서 가져옴. AI가 어떻게 이 문제를 해결했는지 결과는 이 커밋 내역 참고하면 됨. 각자 판단해 볼 만함
          + 내가 항상 좋아하던 데이터셋은 HumanEval임. 깃허브 레포에서 학습하고 싶지만, 대부분의 데이터셋이 이미 노출되어 있고, 직접 깃허브로부터 데이터셋을 만들면 또 노출 우려가 큼. 그래서 직접 새로운 문제를 '손수' 작성해서, 리트코드 스타일로 테스트 코드까지 붙여 사용함. 예를 들어 '이 float에서 소수점 이하 부분을 구해라' 같은 문제임. 깃허브 전체에서 이런 코드는 없을 테고, n그램으로 필터링하기도 쉬움. 특히 공동저자가 60명이나 되는 점, 그리고 이 데이터셋이 한 때 사실상 표준 벤치마크가 된 점이 흥미로움
     * 한 가지 문제는 모델이 결국 코드가 아니라 대량의 '가중치와 바이어스' 뭉치에 불과하다는 점이 아닐까 생각함. 어쩌면 이것도 스스로 조금씩 조정할 수 있긴 하겠지만, 명확히 코드 변경은 아님
          + 모델 가중치도 일종의 코드임. 이에 대한 자세한 설명은 Neural Networks and Deep Learning 챕터1에서 NAND 게이트로 불리언 로직을 MLP로 구현하는 방식으로 볼 수 있음. 표현력은 충분하고, 남은 문제는 우리가 직접 작성할 수 없는 유용한 함수를 이런 가중치에 어떻게 인코딩하느냐임
          + 모델이 훈련 데이터로부터 자신을 새로 생성해낼 수 있다면 괜찮겠지만, 그 경우 반복 시간과 비용이 너무 커서 현재로선 비현실적임. 아니면 자체 가중치를 의미 있게 변경할 수 있어야 하는데, 이건 불가능하게 느껴짐
          + 여기서 진짜 어려운 부분은 ""이 둘의 차이가 뭘까""임. 깊이 생각해보고, 어떤 답을 내려도 스스로 반박해 보길 추천함. 이게 생각보다 훨씬 헷갈리는 지점임
     * 현행 AI 시스템에서 정말 아쉬운 점은 짧은 피드백 루프를 통한 지속적 재훈련임. 비용도 많이 들겠지만, 생물학적 시스템에서는 이게 자연스럽게 일어남. 이런 과정을 실제로 보면 정말 멋질 것 같음
          + 이건 일종의 매일 밤마다 훈련하는 것과 비슷함. 인간 뇌도 잠자는 동안 경험을 학습한다는데, LLM은 매일 컨텍스트 윈도에서 벗겨낸 정보로 파인튜닝하는 일종의 '야간 학습'과 비슷하다고 봄
          + 현재 실제로 이런 연구가 진행 중임. 혼합 전문가 구조로 네트워크를 챙크 단위로 나누고, 각 챙크는 인터페이스를 공유하며 결과를 서로 전달하게 할 수 있음. 이 챙크별로 개별 훈련이 가능하지만, 고정된 훈련셋이 없어야 함. 더 나아가 수학적 구조(카테고리 이론)로 구조를 바꾸면 완전히 동적인 네트워크가 가능함. 그러나 구조가 변할 때마다 재훈련은 피할 수 없음. 결국 실제 현실 데이터와 손실함수(다른 네트워크와 경쟁)가 필요함. 인간 뇌는 이 부분에서 이미 실세계와 가장 잘 결합되어 있음. 추가하고 싶은 건, 우리 뉴런은 가중치뿐 아니라 입력이 언제 들어오느냐(나노초 단위 시간차)에 따라 발화 결정이 달라지는 특별한 동작도 있음. 이런 점은 IT에서는 아직 따라가기 힘듦. 그래도 이론상으론 가능하다고 생각하고, 현재 4차원 생명체를
            동적 컴퓨팅 그래프로 구현해 가상환경 안에서 이걸 실험 중임. 아주 흥미진진하지만, 현업 수준과는 거리가 멈
     * 논문에서 소개한 내용은 DGM이 자기 보상 함수를 해킹한 사례를 관찰한 대목임. 돋보인 점은 '도구 사용 환각(hallucination)' 억제 보상 함수를 도입했음에도, DGM이 이 보상 검출 마커를 제거해 허위 성공으로 판정하게 해버림. 이론으로만 제기되던 현상이 실증됨
          + 보상 해킹 문제는 프론티어 연구소들(예: Claude 4 system card)에서 이미 잘 알려진 현상임. LLM 기반 프레임워크라면 보상 해킹 경향성이 자연스럽게 나타남. 재미있는 기술적 질문은 어떻게 이걸 포착하고 완화할 수 있을지임
     * AI 안전과 관련해, 보상 해킹 안전장치 자체가 또 다시 해킹당하는 현상이 예상 가능함에도, 이 방식에 아직 희망을 거는 게 신기하게 느껴짐. Rob Miles의 AI Safety YouTube 영상(예: 이 영상)에서 매우 인상적인 설명을 들은 뒤로는 이런 현상이 오히려 당연하게 느껴짐
     * 논문에 따르면, DGM을 SWE-bench에서 한 번만 실행해도 2주가 걸리고 API 비용이 무려 $22,000로 상당히 큼
     * 기술 보고서는 아카이브 논문 링크에서 볼 수 있음. GitHub의 참조 구현도 여기 있음. 참고에 유용함
     * 대부분의 최신 연구는 크고 비싼 모델로 작은 모델을 학습시키는 흐름(distillation)을 따르는데, 이번 논문에서 흥미로운 점은 작은/이전/저렴한 모델로 큰 모델의 성능을 개선한 사례임. 만약 일반화된다면, 엔드유저가 자체 추론 비용을 크게 낮출 수 있는 신호임
          + 논문은 실제로 모델 자체 개선이 아니라, 모델을 둘러싼 소프트웨어를 개선하는 방식임. 이런 소프트웨어 개선 효과가 다양한 모델로 확장된다는 점이 중요함(모델 고유의 특성에만 최적화된 게 아님). distillation 방식은 보통 대형 LLM이 작은 LLM에 전체 토큰 분포 자체를 학습시키는 건데, 속도가 빠름
          + 여기서 다루는 건 모델 가중치 자체의 개선이 아니라, LLM을 호출하는 코드를 감싸는 '하네스' 쪽 변경임. 이 부분은 더 강력한 LLM이 나와도 계속 재사용, 일반화되어 남음. 새로운 LLM이 나와 하네스가 재튜닝되지 않아도 그간 쌓아온 개선 효과가 그대로 이용 가능함
"
"https://news.hada.io/topic?id=21271","Go 언어에서 오류 처리를 위한 구문적 지원(또는 미지원)에 대하여","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Go 언어에서 오류 처리를 위한 구문적 지원(또는 미지원)에 대하여

     * Go 언어에서 오류 처리의 장황함이 오랫동안 사용자 불만의 상위에 해당함
     * 다양한 구문 개선 제안(예: check/handle, try, ? 연산자 등) 이 논의 및 실험되었으나, 커뮤니티의 충분한 합의 없이 모두 기각됨
     * 언어 변화가 미치는 코드, 도구, 문서 등 광범위한 영향과 Go 특유의 단순함 유지 원칙이 주요 고려사항임
     * 현행 방식의 명확함, 디버깅 편의 및 일부 사용자 선호로 인해, 굳이 구문 변화를 도입할 명분이 약함
     * 가시적 미래에는 오류 처리 구문 변화 계획이 없으며, 관련 제안들은 모두 추가 조사 없이 종료될 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Go의 오류 처리 장황함 문제 제기

     * Go의 오래된 불만 중 하나는 오류 처리 구문이 지나치게 장황함임
     * 대표적으로 if err != nil과 같은 패턴이 코드에서 반복적으로 나타남
     * 여러 API 호출이 필요한 프로그램일수록 이 구문이 두드러지며, 실제 로직보다 오류 처리 코드가 더 많아지는 현상 발생함
     * 연간 사용자 설문에서 해당 불만이 상위에 지속적으로 언급됨

커뮤니티와의 협의 및 초기 제안

     * Go 팀은 커뮤니티 피드백을 중시하여 오류 처리 개선안 연구를 계속해 옴
     * 2018년 Go 2 프로젝트 논의에서 Russ Cox가 오류 처리 문제 핵심을 공식적으로 정리함
          + Marcel van Lohuizen이 제안한 check와 handle 메커니즘 안 등장
          + 유사 언어들과 비교 분석 및 다양한 대안 검토 포함
     * 이 방식은 실제로 코드를 간결하게 해주긴 하지만, 복잡성 증가로 인해 채택되지 않음

try 제안과 그 이후

     * 2019년, 훨씬 간소화된 try 내장 함수 제안이 이루어짐
          + check 기능만 코드로, handle 생략
          + 해당 제안은 제어 흐름을 감추는 문제로 비판받고 커뮤니티 반발 속에서 폐기됨
     * 이 경험을 통해 충분한 피드백 없는 완성형 제안의 위험성을 깨닫게 됨
          + 대규모 변경 제안은 초기 설계 단계에서 더 폭넓은 의견 수렴이 중요함을 확인함

추가 시도 및 다양한 제안들

     * 수많은 변형과 대안적 오류 처리 방식 제안이 꾸준히 커뮤니티에서 등장함
          + Ian Lance Taylor의 umbrella issue로 현황 정리, Go Wiki 및 블로그 등에서 사례 지속 수집
     * 2024년에는 Rust에서 차용한 ? 연산자 적용 제안이 나옴
          + 소규모 사용성 테스트에서 직관적이라는 피드백 있었으나, 역시 다양한 의견 속에 합의에 이르지 못함

논의의 교착과 결론

     * 공식, 비공식 제안이 3건 이상, 커뮤니티 제안은 수백 건에 달하지만 충분한 공감대/합의 부족으로 모두 기각됨
     * Go 내부 아키텍트 그룹조차 방향성에 대한 의견 일치가 없음
     * 상황 변화나 특별한 공감대 형성 전까지는 오류 처리 구문 변화 시도 자체를 중단하기로 결정함

현행 방식 유지를 옹호하는 주요 논거

     * 초기 언어 설계 시 구문적 설탕을 넣었으면 논란이 없었겠지만, 현재는 15년간 사용된 방식에 익숙한 생태계가 읶음
     * 새로운 구문을 도입하면 필연적으로 기존/신규 사용자 간 코드 스타일 간극 및 일관성 붕괴 우려 존재함
     * Go의 설계 철학(같은 것을 여러 방식으로 하지 않음)과 간결성/일관성 중시 원칙과도 부합함
          + 짧은 변수 선언(:=)의 재선언 허용도 오류 처리로 인해 생긴 부차적 변화임
     * 명확한 오류 처리 구문(if를 통한)은 코드 읽기, 디버깅, 브레이크포인트 설정에 직관적인 강점이 있음
     * 언어 변화는 실제 변경의 범위(코드, 문서, 도구 등)와 비용 측면에서도 큰 부담임

대안적 개선 및 미래 방향

     * 표준 라이브러리의 기능 강화(예: cmp.Or 도입)로 일부 반복 코드 줄이기는 가능함
     * IDE·개발 도구의 코드 접기, 자동완성, LLM 활용 등으로 장황함을 실무에서 어느 정도 극복 가능함
     * 주요 Go 사용자 그룹(예: Google Cloud Next 행사 참석자)에서는 언어 변화 필요성에 부정적인 견해 우세함
          + Go 사용이 늘수록 장황함 문제는 실제로 체감이 줄어듦

구문 개선 필요성을 지지하는 논거

     * 사용자 피드백 기반으로 여전히 오류 처리 구문 개선 요구 존재함
     * 문자 수만 줄이는 게 아닌, 명확성을 높이는 오류 처리 구문이 코드 품질/안전성 개선에 기여할 수도 있음
     * 단순 오류 확인이 아닌, 실제 역할을 하는 오류 처리에 대해 더 정밀한 연구가 필요함

최종 결론 및 향후 정책

     * 현재까지 별다른 합의나 실질적 변화 없는 상황을 인정하며, 가시적 미래에는 오류 처리를 위한 구문적 언어 변화 논의·제안을 모두 중단함을 선언함
     * 기존 논의와 연구 과정은 Go 생태계와 프로세스 개선에 간접적으로 기여함
     * 향후 혹시 더 명확한 문제 정의와 합의가 생길 경우 논의가 재개될 수 있음
     * 당분간은 새로운 시도보다는 Go 자체의 견고함과 단순함을 유지하는 데 주력할 방침임

        Hacker News 의견

     * 만약 Go 팀이 다른 대안을 할 수 있었다고 쉽게 제안하고 싶다면, Go2ErrorHandlingFeedback 위키나 GitHub 이슈 검색 링크를 꼭 확인해주길 바라는 마음임. 제안한 거의 모든 아이디어가 이미 진지하게 논의되었고, Go 팀의 투명한 접근 방식에 감사함을 느끼는 사용자로서 매일 Go를 사용하는 즐거움이 큼
          + 초안 설계 문서는 C++, Rust, Swift에 대해 언급하지만, 내가 찾는 Haskell/Scala/OCaml 같은 함수형 언어의 do-notation/for-comprehensions/monadic-let은 찾아보기 어려움. Go 팀이 마치 언어 설계의 마스터처럼 보이지만, 정작 Java처럼 파라메트릭 다형성이 없는 정적 타입의 한계에 부딪혀 오류 처리 문제에서 해답을 못 내는 모습임. 이건 언어 근본 설계에서 온 문제라고 봄
          + 똑똑하고 숙련된 사람들이 작성한 문서임에도 불구하고, Haskell의 Maybe/Either 모나드와 bind 연산자(do-notation) 같은 해결책이 어디에도 언급되지 않는 것이 매우 신기함. 실제로는 어렵거나 현학적이지 않고, 오류를 안전하게 전달하는데 아주 우아하고 검증된 방법임에도 Go 커뮤니티에서 이걸 접목하지 않은 이유를 모르겠음. 이 페이지가 존재한다는 건 고맙지만, 이렇게 유명한 솔루션을 넘긴다는 건 이해하기 힘듦
          + 거의 모든 언어가 다양한 더 나은 접근을 제공하는데, Go에서만 왜 이렇게 문제가 크게 부각되는지 궁금증이 듦. 단순히 합의가 안 되는 건지, 아니면 Go 언어만의 어떤 특징 때문에 다른 언어의 해결책이 맞지 않는 건지 궁금함
          + Go 비판에서 자주 보이는 현상은 비교적 비전문가들이 Go 개발자들이 자기들보다 언어를 더 모를 거라고 전제하는 경향임. 사실 Go 개발자들은 대부분의 경우 오히려 훨씬 더 경험 많고 훨씬 더 많이 알고 있음. 비전문가는 여러 특징을 가진 언어가 무조건 더 좋다고 생각하지만, 실제로는 전체적인 균형을 잘 맞추는 게 중요하다는 점을 간과함
     * 새로운 언어 기능 추가에 신중을 기하는 Go의 보수성 덕분에 사용자가 혜택을 본다고 생각함. Swift의 경우 기능 변화가 너무 많아 학습도 어렵고, 최신 맥에서도 종종 간단한 프로젝트 하나조차 빌드가 안 되는 경험이 있음. 키워드가 계속 늘어나고 바뀌는 탓에 Swift는 사용 지속성이 떨어지고, 그에 비해 Go는 꾸준함이 강점임
     * 한번은 Go 함수가 내부 함수에서 오류 발생을 기대하는 예외적 상황이 있었고, 내부 함수가 오류를 내지 않으면 오히려 함수를 오류로 처리해야 했던 적이 있었음. 흔하지 않은 구조에서 if err == nil 로 분기해야 했고, 습관적으로 if err != nil 을 써버려서, 평소 쓰던 패턴에 너무 익숙해져 실수를 찾는 데 오래 걸렸던 경험임. 자주 쓰는 if err != nil 과 드물게 쓰는 if err == nil 의 문법적 구분을 언어 차원에서 지원했다면 실수를 줄일 수 있었겠다는 생각을 함
          + ""if err == nil""을 쓸 때마다 // inverted 라는 주석을 달아 패턴을 강조함. 언어에서 자동으로 다뤄주면 좋겠지만, 현재는 이런 방식으로라도 구분을 더 뚜렷하게 할 수 있음
          + 사실 이건 문법 변화에 대한 반대 입장임. 자주 쓰는 if err == nil { return ... } 패턴이 코드에 오히려 더 어색하게 보일 수 있음. 현재의 Go 에러 처리 방식이 명확하고 읽기 쉬워서 많은 사람들이 선호한다는 의견임
          + if fruit != ""Apple"" 같은 패턴에서도 동일한 혼란이 생길 수 있기 때문에, 본질적으로 에러 처리만의 문제가 아닌 전반적인 상태 분기 문제로 봐야 한다는 주장임. 에러도 결국 다른 상태값처럼 다뤄지는 것임
          + IDE나 글꼴 설정에서 ""if err != nil""을 특수 기호처럼 렌더링해서 배경에 자연스럽게 묻히게(덜 눈에 띄게) 하고, 다르게 쓰인 ""if err == nil""만 도드라지게 해 에디터 차원에서 실수 방지 가능성 있음
          + 에디터에서 ""if err … {"" 같이 패턴을 축약해 보여주는 방식으로 가독성을 개선할 수도 있을 거라는 제안임
     * Go의 명시적 에러 처리방식이 마음에 듦. 함수가 항상 성공(minimal error)하거나 실패할 수 있는 구조로 단순히 이해함. 실패 가능성 있는 함수는 꼭 처리해줘야 다음 단계로 진행할 수 있음. 여러 언어가 예외 처리를 통해 오류가 발생하면 catch될 때까지 스택을 타고 오류를 던지다 보니, 에러가 어디서 발생했는지만 알려줄 뿐 실질적 힌트가 부족하다는 불만이 있음. Go에서는 다음과 같은 옵션을 명확히 가질 수 있음: 1) 에러 무시 2) 에러 발생 시 곧바로 반환 3) 에러 래핑해서 유용한 정보 추가 4) 특정 에러 해석해서 분기 처리(예, 404로 변환). Go2에서는 Result<Value, Failure> 타입이나 더 구체적이고 나열 가능한 에러 타입을 추가해보고 싶음. Go 1과의 호환성을 위해 Go 2에서 도입하는 게 더 적합하다고 생각함
          + 에러 처리 정책은 반드시 호출자가 결정해야 하며, 하위 스택에서 처리하는 것은 바람직하지 않다고 경험함. 결국 에러는 래핑해서 상위로 전달하는 단순 반복 작업이 되기 쉬움
          + ""Go의 에러 처리""는 사실 자바스크립트, 파이썬이 아닌 함수형 언어나 Rust, Java 등 대다수 언어들이 이미 제공함. 결국 제네릭만 있으면 Go 방식의 에러 처리를 어느 언어에서든 구현할 수 있다는 주장임. 비교 대상이 JS나 Python에 머문다면 흔한 패턴에 불과함
          + ""함수가 실패하면 반드시 처리해야 한다""는 점이 바로 Go의 실패 포인트라고 지적함. Go에서는 사실상 에러를 완전히 무시할 수 있어, 실제로 robust한 소프트웨어를 만들고 싶으면 오히려 Go의 방식이 약점이 될 수 있음
          + Go2는 결국은 ""절대 출시되지 않을 실험실""에 머물 거라는 씁쓸한 의견임
     * Go의 오류 처리 방식은 처음엔 별로였지만, errors-are-values 블로그 포스트를 읽고, panic(err)을 적재적소에 활용하기 시작하면서 오히려 큰 만족을 느끼게 됨. 부모 코드가 직접 처리해선 안 되는 비정상적인 상태에는 panic을 활용해 코드의 잡다한 에러 분기를 대폭 줄일 수 있었음. 이러한 오류 관리 방식은 실제 업무에서 큰 도움이 되고 있음
          + 이런 논리는 오히려 Go의 빈약한 오류 처리를 방어할 수 없고, 개선해도 장점이 사라지지 않는다는 반론이 있음
          + PHP도 레벨별 에러 처리나 @ 연산자로 call site 에러 억제가 가능하고, bash도 -e와 같은 에러 관리 기법이 있다는 언급
          + C#에서 try/catch/finally 흐름을 처음 봤을 땐 참신했지만, 지금은 오히려 Go처럼 단순한 로직을 선호하게 됨. 높은 코드 분량(Loc)도 실제로 코드 흐름이 명확한 게 장점이라 생각함
          + 러스트의 sum type 기반 에러도 ‘errors are values’ 패러다임에 속함을 언급함
     * 실제 에러를 처리하면 장황함은 금방 가려진다는 주장에 manual stack trace 생성이 진짜 ‘처리’인지 의문이 생김. Go의 정의에 따르면 예외(exception)도 처리가 되는 것 아닌지? 라는 유쾌한 반론이 있음
          + 수십 줄의 stack trace가 정말로 명확한 정보인지 의문이 듦. 개인적으로는 단 한 줄 wrap error가 훨씬 효율적이라 생각하고, 로그를 정리하는 데에도 도움이 됨. 10년 넘게 Go를 사용해오면서 런타임 함수까지 포함된 장황한 스택 정보를 필요로 한 적 없음
     * 이 글에서 Go 오류 처리의 문제를 단순히 ""문법이 장황하다""고 다루는 게 마음에 들지 않음. 진짜 문제로는 1) 오류가 조용히 누락되거나 실수로 무시되기 쉬움 2) 함수 결과를 값처럼 쉽게 넘기거나 저장 불가 3) errors.Is 같은 중첩 오류가 타입 시스템과 어색하게 맞물림 4) 에러 스위칭이 어려움 5) sentinel value 활용이 표준 라이브러리에서 많음 6) 제네릭과의 궁합이 안 좋아 패키지 필요성이 생김 등 다양한 문제가 있다고 생각함
          + Go의 전문 프로그래머 90%는 각 에러 반환 분기별로 테스트 케이스를 작성해 커버리지를 맞추는데, 예외처리 언어에선 불필요한 작업임
          + 이 글에서 It’s too verbose(장황하다)가 주 문제라고 했다는 주장은 사실과 다르다고 생각함. 구문을 바꿔도 본질적 개선점이 크지 않음
          + Go의 변화 속도가 매우 느리다는 것(제네릭도 오랜 시간 걸림)이 오히려 장점이라고 보는 관점도 있음
          + 구글러로서 Go 팀의 결정에 또다시 실망하게 되었음
     * Elixir(및 Erlang)에서는 함수가 일반적으로 {:ok, result} 또는 {:error, description} 튜플을 반환함. Elixir의 with 문법 덕분에 에러 처리를 블록 하단에서 묶어 가독성이 훨씬 좋아짐. Go에도 with 문과 비슷한 걸 도입하면, 에러가 nil일 때만 연속 실행하고, 최하단에 에러 핸들러 블록을 두는 식으로 더 읽기 좋게 개선 가능함
          + Go는 커뮤니티 합의 문제 때문에 가장 기본적인 sum type, 에러 핸들링, 패키지 매니지먼트 같은 가치 있는 기능도 도입이 아주 느림. 제네릭 13년, 에러 핸들링 16년, 패키지 관리 9년이 걸릴 정도로 느린 변화. 신중함도 중요하지만, 완벽만 쫓다보니 결정이 늘 미뤄진다는 아쉬움이 큼
          + Go의 다중 반환 패턴은 관점에 따라 비정상적으로 여겨지기도 함. 여러 타입을 반환하는 함수로 가능한 게 오직 변수 할당 뿐이라는 비판임
     * Rust 스타일을 바로 따라가지 않는 이유를 모르겠다는 입장임. 특히 제네릭이 생긴 지금은 금방 비슷한 구현도 가능함. Rust의 ? 연산자가 편하긴 해도 오류를 무시하는 걸 조장한다고 비판하는 논리는 공감이 가지 않음. 실제로 Go는 오류 반환값을 무시해도 컴파일 오류 없이 넘어가는 경우가 숱하게 나옴. Rust 스타일처럼 아예 Result 타입 반환을 강제해야 실수 방지가 가능함. 편의성 명목으로 논란이 된다면 panic도 금지해야 맞지 않냐는 강한 주장임
          + Go가 Result를 도입하지 못하는 이유는 sum type이 없고, 모든 타입에 zero value가 필요하다는 특이한 설계 때문이라는 의견임
          + “? 연산자” 같은 편의성 기능이 “래핑된 에러를 더 이상 안 쓸 거다”는 주장에 대해서는, 도리어 그런 기능에 wrapping을 장려하는 설계가 가능하다는 반론임
          + 편의성 강조 기능(Rust 스타일)에 대한 단점으로, 분기 흐름이 한 줄에 숨겨지고, 디버깅 브레이크포인트 걸기도 어렵다는 점, enrich/handling보다는 bubbling에만 극도로 집중된다는 이유로 Go가 버린 문법(e.g. 3항 연산)임을 설명함
          + Rust 스타일을 그대로 비교 적용한다 해도, 실제로 Go에서 뭐가 equivalent인지가 명확하지 않다는 기술적인 의문 제기임
          + 제네릭 도입 후 Rust 스타일로 뭘 구현했다는 것인지 코드 예시가 궁금하다는 피드백임
     * Rust처럼 체크박스 체크하며 기능 채택을 논의하는 구조가 아니라, 언어는 전체적인 일관성 안에서 설계해야 한다고 생각함. 기능 리스트를 모두 체크했다고 바로 도입하는 것이 실제로 언어의 본질에 맞지 않을 수도 있음
          + Rust가 디자인 바이 커미티(committee) 방식으로 가면서 문법이 읽기 어렵고 일관성이 떨어진다는 이미지가 생김
          + “완벽한 해결책” 같은 건 없다는 의견임
          + 설문 결과 Go의 단일 치명적 문제점이 에러 처리라고 하기엔 13%만이 그렇게 응답했으며, 현 상태를 선호하는 유저도 적지 않음. 설문 결과 참고
"
"https://news.hada.io/topic?id=21267","Quarkdown - 현대적인 Markdown 기반 조판 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Quarkdown - 현대적인 Markdown 기반 조판 시스템

     * 기존 Markdown 문법을 확장해 책, 논문, 슬라이드, 프레젠테이션 등 다양한 형식의 문서를 손쉽게 제작할 수 있는 현대적 타이포그래피 시스템
     * 함수 지원, 변수 사용, 조건문/반복문, 표준 라이브러리 등 고급 기능을 마크다운에 직접 내장하여, 기존 Markdown이나 LaTeX보다 확장성과 자동화에서 차별성 보유
     * 하나의 소스 파일로 HTML, PDF, 슬라이드(리빌JS), 페이징북(paged.js) 등 여러 출력물 생성 가능, 코드 기반 콘텐츠 제작에 특화
     * 스크립팅 기능과 표현력 있는 확장 문법으로 복잡하거나 동적인 콘텐츠도 자유롭게 구현 가능
     * REPL, 라이브 프리뷰, 빠른 컴파일 환경 제공으로, 실시간 문서 편집 및 디버깅이 가능함
     * 기존 도구들과 비교해 스크립팅 기능, 문서 제어, 쉬운 학습 곡선 등에서 Markdown, LaTeX, Typst, AsciiDoc, MDX 대비 강점이 있음
     * 별도 개발환경이나 복잡한 설정 없이 Java 17 이상만 있으면 주요 운영체제에서 모두 사용 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

About

     * Quarkdown은 마크다운의 기본 구조에 함수 및 확장 문법을 추가하여, 사용자가 단순 텍스트부터 책, 논문, 슬라이드 등 다양한 형식의 완성도 높은 결과물을 쉽게 만들 수 있게 설계된 최신 타입세팅 시스템
     * CommonMark와 GFM을 기반으로 개발되었으며, 자체 문법과 함수, 변수, 사용자 정의 라이브러리까지 모두 지원함
          + Quarkdown Flavor라는 독자 문법을 제공
          + 튜링 완전 함수 확장 문법을 통해 마크다운에 함수, 조건문, 반복문 등 고급 기능을 추가함
     * .qmd 라이브러리로 레이아웃, 입출력, 수학, 조건문, 반복문 등 다양한 기능을 사용할 수 있음
     * 복잡한 문서 구조나 동적인 콘텐츠를 필요로 하는 경우에도 확장성과 생산성을 높일 수 있음

주요 특징 및 활용법

     * HTML, 슬라이드, 책(Paged), PDF 등 다양한 출력 포맷 지원
          + .doctype {slides}, .doctype {paged} 등 함수 호출로 문서 유형 지정 가능
          + reveal.js, paged.js 등 오픈소스 기반 엔진 연동
     * 스크립팅 및 동적 콘텐츠
          + 함수, 변수, 조건문, 반복문 등 프로그래밍적 요소 도입
          + ex) .function {greet} ... .greet {world} from:{iamgio}
     * 라이브 프리뷰, 빠른 컴파일
          + 실시간 미리보기 및 파일 변경 감지(Watch) 기능 제공
          + -p --preview, -w --watch 옵션으로 작업 효율 증대
     * 타 문서 도구와 비교
          + LaTeX 대비 학습 곡선이 낮고, Markdown 대비 기능 확장성 우수
          + Typst, AsciiDoc, MDX 등과 비교 시에도 스크립팅/표현력에서 강점

지원 대상(타겟)

     * HTML
          + 일반 출력(기본값)
          + 슬라이드( reveal.js 활용 )
          + 책/논문 형태( paged.js 활용, 웹서버 필요 )
     * PDF
          + HTML이 지원하는 모든 문서 유형, 기능을 그대로 PDF로 출력 가능
          + PDF 출력에 관한 자세한 내용은 wiki 문서 참고
     * .doctype {slides}, .doctype {paged}와 같은 함수 호출로 출력 포맷 제어

비교

              Quarkdown Markdown      LaTeX      Typst    AsciiDoc        MDX
   간결함·가독성        ✔        ✔            ✗          ✔          ✔            ✔
   전체 문서 제어       ✔        ✗            ✔          ✔          ✗            ✗
   스크립팅 기능        ✔        ✗          일부 지원        ✔          ✗            ✔
   책/기사 형식 출력     ✔        ✗            ✔          ✔          ✔        3rd party
   프레젠테이션 출력      ✔        ✗            ✔          ✔          ✔        3rd party
   러닝 커브         초록        초록          빨강         주황         초록           초록
   타겟 지원      HTML, PDF   HTML   PDF, PostScript  PDF  HTML, PDF, ePub   HTML

   테이블이 있어서 그런지 모바일 레이아웃이 깨지네요.

   .topic_contents에 min-width: 0을 주니 고쳐집니다. min-width 때문에 참 골치아프네요...

   앗 다른 방식으로 해결을 봤습니다. 감사합니다!

   빠른 피드백 감사합니다~

        Hacker News 의견

     * 내 FOSS 텍스트 에디터 KeenWrite는 Markdown에서 XHTML, TeX, 그리고 PDF로 변환하는 유사한 방식을 사용함
       소프트웨어 아키텍처는 프로세서 체인을 어떻게 설계할 수 있는지 보여줌
       내가 KeenWrite를 만든 이유는 공상과학 소설을 쓸 때 등장인물의 이름이나 지역 같은 변수들을 쓸 수 있게 하려는 목적
       자세한 내용은 튜토리얼 참고
       아직도 pandoc과 셸 스크립트를 쓰는 사람들에게는, Typesetting Markdown 시리즈에서 Markdown을 PDF로 변환하는 스크립트 기반 인프라 구축 방법을 설명함
       KeenWrite 자체 정보는 여기에서 볼 수 있음
       아키텍처 다이어그램은 여기에서 확인 가능
     * 최근 많은 주목을 받은 Typst와 이 프로젝트를 비교해보면 재미있을 것 같은데, 기능 비교 매트릭스에는 Typst가 전혀 언급되지 않았다는 점이 의외임
          + 예전에 봤을 때 Typst는 HTML로 출력하지 못했었음
          + 지금은 Typst도 언급되어 있음
            전반적으로 두 프로젝트가 매우 유사해 보임
     * 비교 차트가 정확한지 궁금함 – 링크
       LaTeX는 분명히 완벽한 스크립팅 기능이 있다고 생각함, 물론 사용하기 괴로운 점은 있지만
       Quarkdown의 난해한 문법이 Typst보다 더 간결하고 읽기 쉽다는 주장에 회의적임
       학습 곡선도 Typst보다 쉽진 않을 것이라고 생각함, 둘이 거의 비슷해 보임
       LaTeX도 tex4ht로 HTML을 만들 수 있다고 생각함
          + 솔직히 대부분의 Markdown은 Quarkdown에서도 그대로 사용 가능함
            진입 장벽이 이보다 더 낮아질 수는 없음
            물론 학습 곡선이 진입 장벽이랑 같은 말은 아니지만 상당 부분 겹침
            그리고 '학습 곡선'은 주관적인 특성임
            비교표에 넣으면 처음부터 왜곡될 수밖에 없음
            명확한 기능이 더 객관적이지만 때로는 제품 특성상 어떤 기능이 필요 없을 수 있음
          + 이런 사용 사례에는 Pandoc이 최고임
          + TikZ랑 pgf로 LaTeX와 TeX의 스크립트 기능이 어느 정도인지 바로 파악할 수 있음
            비교 표가 명백히 부정확함
     * 샘플 출력물이 멋져 보임
       하지만 템플릿 언어가 함수 호출이나 복잡성으로 커지는 걸 항상 썩 좋아하지 않음
       물론 이 컨텍스트에서는 의미가 있을지도 모르지만
       만약 다른 언어와 같이 써야 한다면, 예를 들어 서버 사이드 렌더링이나 데이터 기반 문서 생성 등에서, 두 언어를 오가면서 시간을 너무 소모하게 됨
       템플릿 언어는 항상 '진짜' 언어만큼 강력하지 않음
       그래서 나는 JSX나 JavaScript의 tagged template literal 같은 방식을 선호함
       실제 프로그래밍 언어를 사용하면서도 문서의 컨텍스트를 이해한다면 이스케이프(XSS 같은) 걱정을 덜 수 있는 방식이 더 좋음
     * Quarto와 이 프로젝트는 어떻게 다른지 궁금함
       이름도 비슷하고, 확장자도 같고, 지향점도 유사해 보이는데 기능이 오히려 적어 보임 – Quarto
          + Quarto는 R Markdown 생태계의 실질적 후계자임
            동일한 개발자들이 개발했다고 FAQ에서 밝힘
          + 나도 같은 질문을 하려고 했음
            며칠 전에 친구가 모든 강의 대본을 Quarto로 다시 작성하고 프레젠테이션까지 임베드하는 걸 보여줬는데 꽤 깔끔해 보였음
            Quarto가 R Studio, Jupyter Notebook과도 잘 연동된다는 점은 큰 장점임
          + 이름이 비슷한 건 QuarkXPress에 대한 언급이나 연관에서 온 듯함
            이런 건 수렴 진화 같은 현상이라고 생각함
     * ""planet""처럼 보일 수 있는 게 사실은 쿼크, 특히 다운 쿼크라는 설명이 흥미로움
       Cool한 프로젝트지만 QuarkXPress라는 출판 업계의 유명 브랜드 때문에 'Quark'란 단어를 출판 시스템 명칭으로 쓰는 건 약간 위험함
       관련 상표 등록 정보는 여기, 여기에서 확인 가능
       (왜 같은 단어로 두 개의 상표가 등록돼 있는지도 궁금함)
     * 이 분야 토론 쓰레드마다 항상 ""왜 LaTeX 안 써?"" 류 댓글이 70%라, 나부터 확실히 말하자면
       난 확실히 모던 한 Markdown 기반 조판 시스템이 필요함
       라텍스를 대체하는 여러 시도가 있었으면 좋겠음
       LaTeX는 정말 불편하고 옛날 방식이며, 마크업을 더 자유롭게 줄 수 있는 시스템이 있으면 좋겠음
       만약 기능이 풍부해져서 문법이 길어진다 해도 Markdown보단 살짝 강력한 영역은 분명 필요함
       하지만 이 프로젝트는 내가 찾던 것은 아니라고 느낌
       예제를 보면 그냥 Markdown보다 조금 더 강력한 쪽에 치우쳐져 있는 듯하고, 완전히 LaTeX(혹은 Typst) 대체로는 손색
       이런 종류의 문서 시스템은 '정말 부드럽게' 사용할 수 있어야 하는데, 이건 그런 느낌 아님
         1. 그리고 JVM 기반이라 나는 설치조차 안 하고 싶음
            이런 건 확산성에 비추기 좋지 않음
         2. 문법도 마음에 안 듦
            최대한 노멀 마크다운과 호환됐으면 좋겠지만, 함수 인자 들여쓰기가 필수라 전체 문서가 들여쓰기될 것 같고, 정작 마크다운 확장점은 일반적으로 코드 블록(```plugin-name`식)이 더 자연스러움
            문법 차이로 인해 문서 전체 구조를 바꿔야 할 수도 있음
         3. ""더 나은 마크다운"" 컨셉은, 개인 메모에서 출발해 점차 공개 문서로 발전하는 경우 더 적합하다고 봄
            문서를 출판 목적으로 만드는 거라면 LaTeX로 그냥 작업해도 됨
            가장 유용한 건 노트 테이킹 앱에 잘 통합되어 있을 때임
            Emacs나 Vim에서 하는 사람도 있겠지만, 나 같은 복고파도 결국 Obsidian 등으로 넘어갔음을 고백
            문서 구조를 메모 앱에서 더 잘 컨트롤하거나, 퍼블리싱 기능도 연결할 수 있는 파트가 좋을 듯
            독립 실행형이라면 왜 써야 할지 의문임
            그나마 Typst는 온라인 에디터라도 있음, 다들 그걸로 씀
          + LaTeX는 오래된 쓰레기가 아니고, 최고의 소프트웨어 중 하나임
            쓸데없는 걸 문서에 추가하지 않는 게 핵심임
     * 이런 시스템(Typst 포함)은 기본적으로 논문 같은 장문의 텍스트 조판을 위한 것임
       HTML의 대안이 되었으면 좋겠는데, Typst도 써봤지만 작성자들이 거의 '논문이나 장문'에만 신경 쓰는 것처럼 느껴짐
       폼, 인보이스, 전단지, 명함 같은 것도 만들고 싶지만 이런 요소들은 관심 밖임
       (실은 Sile을 생각한 것인데 Typst도 비슷함)
       Typst가 상업적이라 깊이 안 써봤음
          + 언급한 폼, 인보이스 등에서 Typst를 못 쓸 특별한 이유가 있을지 모르겠음
            특히 인터랙티브 폼은 이미 작업 중이라는 소식도 있고(pdf writer의 백엔드는 이미 일부 지원)
            시간이 좀 지나면 Typst에서 폼 기능이 구현될 듯 – 이슈 참고
          + 이 분야가 '조판'보다는 '그래픽 디자인'에 가까워서 그렇다고 생각함
            인보이스나 광고지, 명함 등은 작은 요소를 정확하게 페이지 중심이나 가장자리에 배치해야 하는데, 이건 WYSIWYG 도구가 편함
            텍스트 기반 조판만으로는 시행착오가 너무 많아짐
            예를 들어 타블로이드는 텍스트가 사각형이 아닌 이미지나 컷에 맞춰서 흐르고 감싸야 하는데, 실제로 눈으로 확인하지 않고 좌표만 써서 작업하는 건 매우 힘듦
          + Typst 온라인 에디터는 상업적이지만, Typst 자체는 Apache 2.0 라이선스로 공개되어 있음
            나는 Rust로 cargo 통해 설치해서 온라인 에디터 없이 충분히 사용 중
          + Typst는 로컬에서 써도 되고 상업적 부분을 아예 건너뛰면 됨
            여러 종류의 문서를 만들기에 꽤 쉬움
            나는 슬라이드와 유인물 제작 용도로 이미 대체제로 쓰고 있음
          + 내 Typst 첫 '실사용'이 포스터였는데, LaTeX보다 훨씬 쉬웠음
            이미지 감싸기나 텍스트 플로우 등 아직 몇가지 피처는 없지만, TeX에서도 이건 어렵고 Typst에도 향후 추가 예정임
            포스터 예시
     * 이건 그냥 reStructuredText(rST)랑 거의 똑같아 보임
       Quarkdown의 함수 문법(.somefunction {인자} {인자} 바디)과 rST의 함수 문법(.. somefunction:: {인자} {인자} 바디)가 매우 유사함
     * Markdown, Quarkdown, Typst 등 너무 많고 표준화가 안된 것 같아서 결국 HTML+CSS로 회귀함
          + XML도 어때?
            직접 써보진 않았지만 상당히 진지하게 고민 중임
            나머지 포맷들은 복잡하고 학습 곡선이 있어서 글 작성 자체를 방해함
            XML은 내 태그를 맘대로 정의해서, 파서로 각주 자동 생성 등 다양한 구조를 만들 수 있음
            이런 접근을 써본 사람 있는지 궁금함
          + 마크다운은 기본적인 수준에선 정말 효율적임
            문제는 너무 많은 사람들이 시스템을 바로 위에 얹어가면서, 애초에 '더 복잡한 것'을 해결하려 들 때 발생함
            원래 단순한 용도의 시스템을 자기 딴엔 개선한다면서 사실은 한계를 못 깨닫고, 오히려 불필요한 반복과 혼란만 늘린다고 생각
            기능 부족이 아니라, 설계 목적에 맞는 범위를 벗어나서 문제
            심지어 윈도우즈 notepad에 서식 처리가 들어가도, 본질적으로는 개선이 아니라 생각
            notepad는 본래의 역할이 있었기 때문임
          + 성숙하고 신뢰성 있는 Org-mode도 있음
            Emacs를 싫어하지 않는다면 좋은 선택임
          + 나도 HTML, CSS, Javascript로 웹개발에서 DOM 조작하는 게 오히려 더 재밌다고 느낌
            수백 개의 프레임워크와 복잡한 문법을 그만큼 외울 필요 없음
            AI로 markdown to html 변환기 생성 명령만 내려도 괜찮음
          + 만약 2005년 텍스트 에디터의 오토컴플리트가 태그 밸런싱이나 들여쓰기, 하이라이트만 지금처럼 잘 지원해줬으면 JSON, YAML, Markdown 같은 포맷이 이 정도로 흥하지 않았을지도 모름
            2003년에 나온 The Art of Unix Programming에도 XML 직접 편집은 고통이라서 각종 포맷과 파서를 새로 만들어야 했다는 내용이 나옴
"
"https://news.hada.io/topic?id=21182","Astra - 가볍고 빠른 JavaScript-To-EXE 컴파일러 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Astra - 가볍고 빠른 JavaScript-To-EXE 컴파일러

     * JavaScript/TypeScript 기반 애플리케이션을 단일 실행 파일(.exe)로 컴파일하는 경량 도구
          + 특히 CLI 도구나 서버 앱(Express, Fastify 등)을 타겟으로 함 (Electron 대체용이 아님)
     * 기존 pkg, nexe와 완전히 다른 컴파일 방식을 사용. Node.js SEA의 제한들도 우회
     * esbuild 기반으로 컴파일 속도가 매우 빠르며, 평균 결과물 크기는 70–80MB, upx 사용 시 30MB대까지 줄어듦
     * 최신 Node.js 버전 및 ESM 모듈 지원
     * 실행 파일의 아이콘, 이름, 버전 정보 등 메타데이터를 커스터마이징 가능하며, 개발자 경험이 우수함
     * 현재는 Windows 전용이며, macOS 및 Linux 지원은 개발 중임

   pyinstaller처럼 동작하나요?

   오 드디어 이런게 나오네요

   deno compile도 함께 보시면 좋을것 같습니다. 이미 Next.js 등의 프론트엔드 메타프레임워크로 만든 앱을 단일 바이너리로 컴파일해 배포할 수 있으며 유저 어플리케이션을 위한 Code signing, 윈도우/맥/리눅스 호환, Cross compilation 등의 기능도 지원하고 있습니다.
"
"https://news.hada.io/topic?id=21298","Antropic, 회로 추적 도구 오픈소스화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Antropic, 회로 추적 도구 오픈소스화

   일반적으로 심층 신경망은 내부 작동 원리를 이해하기 어려워 블랙박스로 취급되는데 Antropic의 LLM 사고 추적 코드가 오픈소스로 공개되어 공유해봅니다.
"
"https://news.hada.io/topic?id=21181","HuggingFace LeRobot Hackathon","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     HuggingFace LeRobot Hackathon

   허깅페이스 르로봇 해커톤이 서울에서도 열립니다

   (https://lerobot.sudormrf.run)

   참여 원하시는 분들은 ⬆️링크 들어가시면 됩니다
   (위 링크는 호스트들이 작성하는 서울 해커톤 안내 페이지 입니다)
"
"https://news.hada.io/topic?id=21228","Figma Slides는 아름다운 재앙임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Figma Slides는 아름다운 재앙임

     * Figma Slides는 Auto Layout, Components 등으로 슬라이드 제작 속도와 효율성을 크게 높였으나, Keynote의 핵심 기능(Autosize Text, 순차 애니메이션 등)이 부족함
     * 오프라인 발표 환경에서 Figma Slides는 불안정하며, 저장 및 프레젠테이션 제어 등에서 예기치 않은 오류가 자주 발생함
     * 애니메이션 빌드, 프레젠터/오디언스 뷰 관리 등에서 Keynote 대비 명확한 단점이 존재하고, 현장 발표 시 여러 번 클릭해야 하는 버그 등 실제 발표 흐름이 크게 저해됨
     * Figma Slides의 핵심 기능이 미션 크리티컬하게 다뤄지지 않는 인상이며, 신뢰성과 완성도 측면에서 Keynote와 비교 시 한계가 드러남
     * 지루하지만 검증된 기술(예: Keynote)이 실제 현장에서는 더 가치있음을 재확인한 경험 공유
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Figma Slides 요약

     * Figma Slides는 발표용 슬라이드 제작과 발표를 위해 Figma에서 제공하는 relatively 새로운 제품임.
     * 이 글에서는 Figma Slides의 장점과 한계, 그리고 실제 발표 현장에서 겪은 문제점을 상세히 다룸

슬라이드의 목적과 디자인

     * 발표 슬라이드는 핵심 메시지 강조, 복잡한 개념 분해, 재미 제공이라는 세 가지 목적을 가짐
          + 이러한 목적을 달성하기 위해 이미지를 중심으로 한 간단한 슬라이드 구성이 효과적임

Figma Slides 사용 경험

     * 오랜 Keynote 사용자였으나, 최근 발표 준비를 위해 Figma Slides를 사용해보았음
     * Grid View, Auto Layout, Components 등 Figma 고유의 기능이 슬라이드 제작을 빠르고 쉽게 만들어줌
     * JavaScript 프레임워크의 다양성을 보여주기 위한 시각화도 Figma에서 매우 신속하게 구성할 수 있었음
     * Figma의 컴포넌트와 자동 레이아웃 기능 덕분에 Keynote 대비 10배 빠르게 슬라이드 조립이 가능했음

Figma Slides의 아쉬운 점

     * Keynote에서 필수로 여겨지는 Autosize Text(컨테이너에 맞게 폰트 자동 조정)가 Figma에는 없음
          + Figma는 CSS Grid와 호환되는 자동 레이아웃만 지원하려고 하므로 기능 확장에 한계가 있음
     * 슬라이드에서 항목이나 다이어그램 요소를 클릭마다 점진적으로 노출하는 기능 구현이 어렵고, 1ms 페이드 애니메이션과 레이어 순서 변경 등 번거로운 방식만 가능함
     * 단순히 4개의 단어를 하나씩 등장시키는 것도 까다로움

발표 리허설 및 실제 발표에서 발생한 문제

     * ""Save Local Copy"" 옵션이 있으나 로컬에서 직접 발표는 불가능함
     * 발표를 열어두고 있더라도, 오프라인 상태에서 ""Present""를 클릭하면 오류 발생함
     * ""다운로드""로 오프라인 발표 준비가 가능하나, 탭을 닫으면 다시 원상복귀됨
     * 발표 화면이 전체화면이 아니라 팝업 형태로 제공되어, 별도로 프로젝트로 이동 및 최대화가 필요
          + Keynote처럼 단축키로 오디언스 디스플레이 전환 불가
     * 마우스 커서가 슬라이드 위에 그대로 남는 등 사용자 경험이 매끄럽지 않음
     * Presenter View와 Audience View 제어가 불안정하게 동작함

발표 현장에서의 치명적 문제

     * 발표 중에는 슬라이드당 두 번 클릭해야 다음 슬라이드로 넘어가는 등 이상 현상이 발생
     * 복잡한 애니메이션이 포함된 슬라이드는 아예 전환이 되지 않는 현상이 발생함
          + 예: 빌드가 7개인 슬라이드에서 14번 클릭, 이후 뒤로 가기를 반복하며 억지로 설명
     * 버그는 이후 Figma 재시작 후에는 재현되지 않았으나, 포럼에서 유사한 사례가 다수 보고됨
     * 청중의 이해와 흐름이 단절되는 경험을 했음

주요 교훈

     * 지루한 기술, 즉 안정적이고 신뢰할 수 있는 도구의 중요성을 발표 현장에서 재확인하게 됨
     * Figma Slides는 재미있고 혁신적이지만, 현장 발표에 있어선 신뢰성과 완성도가 중요
     * Keynote가 오래된 도구임에도 불구하고, 발표자와 청중 모두에게 탄탄한 신뢰성과 발표 경험을 제공함
     * Figma Slides는 향후 개선될 여지가 있으며, 실제 미션 크리티컬한 발표 용도로는 아직 부족한 터라, 기존 검증된 툴 활용의 이점을 다시 한 번 실감함

후속 업데이트

     * Figma의 PM이 직접 피드백을 수용하여 더 안정적인 서비스 제공을 약속함
     * 발표 툴로서 Figma Slides가 ‘아름답고 매력적인 신제품’에서, 실사용에 있어 ‘지루할 만큼 믿을 수 있는 제품’으로 발전하길 기대함

        Hacker News 의견

     * 이 재앙에서 가장 이상한 점은 Figma를 실제로 사용하는 사람이 있었다면 이런 문제들을 바로 알아차렸을 것이라는 점임
       많은 댓글들이 클라우드나 크로스플랫폼 앱을 탓하지만, 유사한 기능이 Figma의 다른 앱에서는 잘 동작함
       Figma는 이미 이런 문제들을 수년 전에 해결함
       그럼에도 Slides가 이렇게 엉망이 된 이유가 뭘까
       겉에서 보기엔, 스타트업이 인플루언서들이 MVP를 최대한 빨리 출시해야 한다는 과장된 조언을 듣고 모두가 버그 투성이인 상태에서 제품을 서둘러 론칭하는 것과 유사하게 보임
       실사용자들은 필요할 때 제품이 실패하면 쉽게 용서하지 않고, 이런 신뢰 훼손에서 회복하기가 매우 어려움
       내 경험에 비추어 보면, 자기 멋대로 일정 짜고, 엔지니어에겐 나중에 통보하면서 임의의 마감기한까지 모든 기능 다 내라고 하는 경영진 밑에 있을 때랑 비슷함
       마감기한을 꼭 맞춰야 한다는 압박 때문에 완성되지 않은 기능과 버그 많은 상태로 출시하고, 나중에 고치려는 전략이 됨
       아무도 실제로 소프트웨어를 안 쓰는 상황에서 회사 안에서 가장 그럴싸하게 보이기 위한 합리적인 행보임
       이런 접근은 항상 결과가 이렇게 망하는 쪽임
          + Figma에서 PM 역할을 하고 있음(Dev Tools 쪽, Slides는 아님)
            Allen에게 일어난 일은 정말 안타까움
            이 케이스를 팀과 공유해서 직접 자세히 확인해보기로 함
            더 일반적으로, Slides는 프레젠테이션 때 무결점 완성도를 보여야 하며, 그 이하로는 용납 불가라는 점을 명확히 인지하고 있음
            참고로, Figma 내부에서는 Slides를 사내 미팅부터 대형 이벤트까지 거의 모든 곳에서 실제로 사용 중임
            나 역시 PM으로서 매주 Slides를 쓰고 있고, 내부 피드백 채널도 정말 활발함
            그리고 Figma는 품질 향상을 프로젝트 데드라인보다 더 우선시하는 리더십 문화를 가진 독특한 환경임
            사용자 경험이 얼마나 중요한지 충분히 인식하고 있음
            완벽하게 맞는 결정을 항상 내릴 순 없지만, 부족했던 부분은 반드시 개선하겠다는 의지임
     * Apple 스타일 프레젠테이션(시각적 노이즈 없이, 불렛 없이, 각각의 슬라이드에 매력적인 하나의 비주얼·아이디어, 그리고 이야기 전달 위주)로 발표하면, 청중이 정말 프레젠테이션을 즐기고 핵심을 전달 받는 게 확실히 느껴짐
       그런데 경영진은 항상 와서 '회사 템플릿 써라, 템플릿 요소 맞춰라'라고 함
       본인들도 청중에 있을 때 내용은 즐기면서도 뭔가 좋은 프레젠테이션의 본질을 이해하지 못함
       결국 허탈함만 남음
       추가로, 700명이 넘는 회사에서 MacBook 쓰는 사람은 나 혼자인 특이점 있음
          + 내 경험상, 사람들은 슬라이드를 보조 자료가 아닌 문서처럼 쓰려고 하는 경향이 있음
            그래서 프레젠테이션 때마다 슬라이드는 내 연설에 보조 역할만 하도록 만드는데, 끝나고 나서 사람들이 슬라이드를 달라고 요청함
            자료를 보내주긴 하지만, 단독으로는 아무 쓸모 없음
            관리자가 모든 내용을 슬라이드에 다 집어넣으라고 압박해오고, 결국 발표자는 그냥 음성 해설만 하게 되는 구조임
          + 나는 프레젠테이션 제작법에 Beamer(LaTeX의 발표 자료 생성 확장)의 가이드를 항상 추천함
            beameruserguide.pdf 문서의 일부를 공유함
               o 목차는 단독으로 충분히 이해 가능해야 함
               o 너무 많은 정보보단, 다소 부족해 보이는 슬라이드가 더 나음
               o 슬라이드당 20~40단어가 적당, 80단어 넘지 않도록
               o 청중이 모두 전문가라고 생각 말 것, 간단한 개념도 간단히 상기시켜주는 게 좋음
               o 슬라이드는 간결해야 하고, 청중이 50초 이내에 한 슬라이드를 이해할 수 있어야 함
               o “서브불릿” 2단계 이상은 자제할 것, 대신 그래픽 사용 권장
               o 각주 사용 금지, 중요하면 본문에, 아니면 과감히 제외
               o 짧은 문장 사용
               o 가능한 모든 슬라이드에 그래픽 포함 권장
               o 그래픽의 모든 내용을 설명 필수
               o 의미 없는 애니메이션, 시각적 효과 자제
          + 슬라이드를 발표자 없이도 자료로 쓸 수 있다는 기대에서 비롯된 문제임
            실제로는 다른 포맷(문서)이 더 적합함
            두 가지 대안 있음
              1. 세부 정보를 모두 담은 스피커 노트를 많이 추가해서, 프레젠테이션+노트를 합쳐야 모든 정보가 포함됨
              2. 발표용 슬라이드와 별개로, 해당 슬라이드의 항목과 이미지를 논리적으로 잘 정리해 넣은 별도의 자기완결형 문서 작성
                 이게 기존의 전형적인 회사 스타일의 프레젠테이션보다 훨씬 쓸모 있기도 함
                 그리고 문서 맨 위에 ""이 문서는 X분 발표의 모든 정보를 포함하고 있습니다""라고 안내문 남기는 것도 추천
          + 나는 한 줄 메시지를 슬라이드 제목에 넣고, 나머지 내용으로 메시지를 보강하는 식으로 균형점을 찾음
            어떤 슬라이드는 ""제목만 보세요""라고 안내하거나, 또는 제목을 강조해서 반복 이야기함
            보조자료(테크 세션용 근거 자료 등)도 챙김
            주의할 점은, 미니멀리즘 스타일(Apple 마케팅식)은 특정 상황에만 적합하다는 것임
            대부분의 상황에는 어울리지 않을 수 있으니 형태보단 기능을 우선해야 함
          + 두 가지 버전을 만들어야 함
            세부 내용 가득한 사후 공유용과, 말할 때 활용할 요약된 발표자료
     * Steve Jobs는 2011년에 세상을 떠남, 그의 프레젠테이션은 전설이었고 iPhone 발표는 2007년임
       20년 가까이 지났지만, MS Powerpoint를 포함한 어느 프레젠테이션 소프트웨어도 2007년 Keynote만큼의 완성도를 못 보여줌
       하나 배운 점은, “그대로 복사하라”고 주문해도 100% 복제 자체가 쉽지 않다는 것임
       대부분은 작은 디테일을 무시한 채 짝퉁을 만들게 되고, 마치 90~2000년대 Microsoft처럼 오히려 악화시킴
       결국 이 모든 차이는 “취향”으로 귀결됨
       Steve Jobs도 Microsoft의 문제는 취향이 없다는 점이라고 말함
       세세한 장인정신이나 제품적 감각 없이, 결국 영업/마케팅부서가 ‘잘 팔리는지’ 여부로만 판단하게 되었고, 이건 최근 Apple에서까지 마찬가지 흐름임
          + “세상은 이미 위대한 프레젠테이션/소프트웨어의 본질을 배웠어야 했다”
            근본적인 차이는 소프트웨어로 해결 불가, 소프트웨어가 당신이 어떤 가치를 추구하도록 만들 수는 없음
            Jobs는 프레젠테이션을 공연 혹은 연극처럼 여기고, 수일에 걸쳐 리허설과 미세조정을 진행함
            실제 현실 사업에서는 매우 드문 사례임
          + 다소 논쟁적일 수 있지만, Steve Jobs의 프레젠테이션에서 실제로 배울 수 있는 범위는 아주 적음
            그는 막강한 지원팀과 탁월한 능력을 가졌음
            마치 한번도 운전 안 해본 사람이 F1 경주 영상을 보면서 배우려는 것과 비슷한데, 현실에서 일반 배달기사에게 F1 기준을 기대하기는 어려움
          + Steve만큼 업계를 대표하는 역할을 해낼 인물이 지금은 없음
            새로운 것, 흥미로운 것들의 전달력이 많이 떨어졌고, 업계 전체 분위기마저 더 침체
            요즘은 “Hype man”과 엔지니어팀이 홍보만 하는 느낌
            Steve도 “Hype”의 대명사였지만 자연스럽고 모두가 환영하는 방식이었음
          + 재능 있는 프레젠터 자체가 극소수 소수집단일 수도 있음
            사실상 대부분의 사람들은 프레젠테이션 자체에 거의 관심 없음
          + 프레젠테이션 소프트웨어에서 직접 일해본 입장임
            겉보기보다 훨씬 복잡함
              1. 기본 시나리오에서는 사실 Powerpoint, Keynote, Google Slides가 독보적
                 무료/번들 수준이고, 그 정도면 충분히 사용 가능하며, 이용자도 이미 익숙, 그리고 이쪽이 본업이 아닌 회사라 혁신 동기도 적음
              2. 기본 시장에서 차별화가 어렵기 때문에 별도 유료 타깃(마케팅 등)을 겨냥해야 하는데, 이들은 “예쁜 UI, 아름다운 소프트웨어”보다 “변환, 데이터 확보” 같은 실무적 니즈에 집중함
              3. 대부분의 프레젠테이션이 밋밋하고, 뭔가 창작이나 편집 경험들을 쏟아부어봐야 대다수 실사용자에겐 오히려 진입장벽만 높아짐
                 템플릿, 튜토리얼이 있어도 결국 “졸작”만 만들어냄
                 꾸미려고 해도 “애니메이션” 남용 등, 더 어설퍼지기 쉽상
                 결국 정말 투자해서 멋진 자료 만드는 사람은 극소수임
                 핵심은 슬라이드 품질이 아니라 발표자의 콘텐츠와 역량임
                 컨퍼런스조차 슬라이드가 형편없는 경우 허다하고, 제대로 남는 프레젠테이션은 발표 내용 자체와 발표자 실력 때문임
     * 요즘은 발표 자료를 대부분 PDF로 출력해서 사용함
       예전엔 멋진 폰트를 발표에 썼다가, 해당 폰트가 PDF에 임베딩되지 않아 컴퓨터마다 텍스트가 잘려 제대로 발표를 망친 경험 있음
       그 후론 PDF/A로만 만드는 게 규칙
       LibreOffice Impress도 PDF/A로 내보내기 지원함
       동적 콘텐츠, 애니메이션은 못 쓰지만, YouTube 링크나 로컬 동영상 등으로 대체하면 됨
       너무 간편한 방식이고, 단 한 번도 실패한 적 없음
       현장 PC에 브라우저만 있으면 자료 오픈, 별도 소프트웨어·로그인 필요 없음
       PDF/A 설명
     * Keynote는 내가 써본 소프트웨어 중 디자인적으로 가장 완벽에 가까움
       대체 누가 이 UX를 이 정도 수준까지 키운 건지 경외감이 들 정도
       항상 Keynote를 사용하고 있고, 딱 두 가지 사소한 단점(색상 팔레트 버그, 직사각형에서 둥근 사각형 변환 불가)만 빼면 거의 신의 영역임
       Keynote 팀에 전달하고 싶은 말: 진짜 영웅임
     * iA Presenter의 매력이 좋음(관계 없음)
       마크다운 기반이고 우선 이야기를 풀어나가는 과정에 중심을 둠
       레이아웃도 자동이고 예측 가능함
       지난 버전엔 불릿 포인트조차 제공하지 않을 정도
       제약이 많지만 그래서 신경 덜 쓰고 오히려 장점
       그리고 오프라인 사용 완벽 지원함
       iA Presenter
          + reveal.js와 slides.com을 써본 적 있음
            하지만 꽤 많은 제약이 느껴졌고, 회사에서도 결국 Google Slides에 정착했음
            특히 공동 발표가 많았는데, 협업 기능이 굉장히 쓸 만했음
          + Deckset(deckset.com)도 좋은 선택임
            Deckset 같은 콘텐츠 기반, 자동 레이아웃 방식은, 긴 시간 슬라이드 트랜지션에 매달리지 않고 본질적인 내용 중심으로 자료를 만들 수 있어 좋음
          + marp(marp.app)도 평판이 좋고, 오픈소스 소프트웨어라는 점이 장점임
          + 소프트웨어 자체는 훌륭해 보이지만, 공식 웹사이트가 다소 불편함
            마크다운으로 슬라이드 만들고 싶어 하는 사람과, 이런 스타일의 웹사이트가 맞는 사람의 교집합이 너무 적은 느낌임
          + iA Presenter가 정말 마음에 듦
            사실 리스트, 불릿도 충분히 지원함
            나 같은 경우에는 제안서 PDF를 손쉽게 만들 때도 활용함
            본래 프레젠테이션 소프트웨어이지만, 빠르게 문서 만드는데도 최고임
     * 클라우드 우선 방식 그 자체가 문제라는 생각임
       모든 소프트웨어는
          + 오프라인 동작 보장
          + 가능하면 로컬에 사람이 직접 읽을 수 있는 포맷으로 저장, 내보내기
            이 두 가지가 기본이어야 함
          + Electron 같은 기술이 좀 더 발전했으면 좋겠음
            크로스플랫폼 앱 개발은 꽤 무서운 일이라 많은 개발팀이 그냥 웹으로 모든 걸 옮기는 현실임
     * 내가 이 사례에서 느낀 교훈은, 특히 프레젠테이션 같은 상황에선 반드시 로컬에서 구동되는 소프트웨어를 써야 한다는 점임
       백업용으로 간단한 PDF도 항상 준비, 페이지 넘기기만으로 발표 내용 쉽게 대체
       애니메이션은 포기하더라도 여러 단계를 포함하는 방식은 여전히 가능함
          + 나도 이 방식으로 준비함
            Google Slides에서 만든 경우도 미리 로컬 저장하고, .pptx/.odp를 쓰더라도 PDF로 한 번 더 뽑아놓음
            정말 중요한 발표라면 미리 두 대의 노트북에 슬라이드를 준비해두는 정도의 여유도 가짐
            번거로울 것 같아도, 이런 준비가 단 한 번이라도 나를 구한다면 남들 앞에서 망신 당하는 것, 그리고 그게 온라인에 영원히 기록되는 위험을 예방하는 최고의 투자임
          + Google Slides는 꽤 만족스러움
            복잡한 빌드, 애니메이션은 거의 안 써서 오히려 필요한 최소만 쓰기 편함
            Google Docs도 마찬가지로 불필요한 요소가 없는 점이 마음에 듦
            하지만 언제나 로컬 PDF 제본 필수임
     * Figma는 Sites, Make 등 여러 프로젝트를 동시에 벌이고 있어서 Slides엔 충분한 투자와 세심한 관리가 부족할 거라 예상함
       클라우드 우선 전략도 피하려고 하는 편임
       서버가 느리거나 다운되거나, 심지어 접근 불가하면 자기 파일조차 못 찾는 상황 발생
       Powerpoint, Keynote 같은 로컬 앱 사용과 클라우드 백업 조합을 선호함
          + 협업 필요 없을 땐 Figma 대신 Sketch를 여전히 사용하는 이유임
            Figma는 지속적으로 내보내며 로컬 복사본 관리하지 않으면 원본 파일을 아예 가질 수 없음
            내 작업물이 회사 정책, 독점 포맷 변경에 휘둘릴 수 밖에 없음
            반면 Sketch는 오프라인 동작, 오픈 파일 스펙 등으로 훨씬 자유로움
            이게 맞는 방식임
          + 작년 Figma의 Slides(대대적 신기능)에서 벌어졌던 문제를 보면, 올해 발표된 여러 신기능들에도 전혀 믿음이 안 감
     * 프레젠테이션이라는 문제 자체는 40년 전 Hypercard 시절에 이미 완전히 해결된 셈임
       지금 사용하는 PPT, Keynote, LibreOffice Impress 등, 기능은 결국 거의 동일함
       Figma가 Dropbox와 비슷한 전략(사용자 벤더 락인)으로 가고 있고, Dropbox Paper 같은 걸 쓰는 사람이 없는 것과 비슷함
       “로컬로 저장은 되지만, 로컬에서 직접 발표는 못하게 하는” 등 이상한 제한을 만들어서 사용자를 자사 생태계에 묶어두려는 목적임
"
"https://news.hada.io/topic?id=21248","Show GN: Pixel Pal - 랜덤 픽셀아트 Pet 키우기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: Pixel Pal - 랜덤 픽셀아트 Pet 키우기

   나만의 귀여운 Pixel Pal을 키워보세요~ 주말에 떠오른 생각을 바로 Cursor와 함께 만들었고 오늘 퇴근 후에 다듬어서 올립니다.

   작고 이상한 게임입니다.

   나이가 먹을수록 모습이 조금씩 변하면 더 좋을 것 같아요~ 그리고 캐릭터가 좀더 귀여웠으면 좋겠습니다
"
"https://news.hada.io/topic?id=21204","Amazon Web Services의 시스템 정확성 실천 사례","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Amazon Web Services의 시스템 정확성 실천 사례

     * Amazon Web Services(AWS) 는 서비스의 정확성을 핵심 가치로 두고 여러 형태의 형식적 방법론을 개발 프로세스에 통합함
     * TLA+와 P 언어 등 형식 명세 도구를 통해 미묘한 버그를 조기에 발견하고, 대담한 최적화에도 신뢰성 확보 가능함
     * AWS는 경량화된 형식 방법론으로 속성 기반 테스트·결정적 시뮬레이션·지속적 퍼징 등도 폭넓게 운용함
     * Fault Injection Service 같은 실패 주입 도구를 통해 장애 발생 상황까지 포함한 신뢰성 검증을 자동화함
     * 교육적 장벽과 도구의 복잡성이 여전하지만, AI와 자동화 도구의 확산이 보급 확대에 기여할 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AWS의 시스템 정확성 보장 전략

   Amazon Web Services(AWS)는 고객이 완전히 신뢰할 수 있는 신뢰성 높은 서비스 제공을 목표로 함
   이를 위해 보안성, 내구성, 무결성, 가용성 기준 유지를 추구하며, 그 중심에 시스템 정확성 개념을 배치함

   2015년 Communications of the ACM에 소개된 AWS의 형식적 방법론 적용 사례 이후, 해당 접근법은 핵심 서비스의 성공적 운용에 중요한 역할을 해옴

   중심에는 Leslie Lamport가 개발한 TLA+라는 형식 명세 언어가 있음
   AWS의 TLA+ 도입 경험은 개발 초기 단계에서 기존 테스트로는 잡히지 않는 미세한 버그를 파악할 수 있게 해주었음
   또한, 과감한 성능 최적화를 진행할 때도 형식적 검증을 통해 안정성과 신뢰성을 확보할 수 있었음

   15년 전 AWS는 빌드 시점 단위 테스트, 제한된 통합 테스트 등 기본 수준에 머물렀지만, 이후로 형식적 접근과 준형식적 방법을 포괄적으로 도입함
   이러한 변화는 정확성뿐 아니라 고수준 및 저수준 최적화 검증·개발 속도 향상·비용 절감에 기여함

   AWS에서는 기존의 정형 증명, 모델 검증뿐만 아니라 속성 기반 테스트, 퍼징, 런타임 모니터링 등도 형식적 방법론의 범주로 수용함

P 프로그래밍 언어의 등장

   초기에는 TLA+가 강력한 추상적 기술이라는 장점이 있었으나, 수많은 개발자 입장에서는 수학적 표기법 사용의 진입장벽이 컸음
   그래서 AWS는 개발자가 익숙한 상태 기계 기반 접근을 제공하는 P 언어를 도입함

   P 언어는 분산 시스템 설계 및 분석을 위해 상태 기계 모델링 방식을 제공함
   마이크로서비스 기반 SOA 구조를 사용하는 Amazon 개발자들에게 친숙한 개념임
   2019년부터 AWS에서 개발 및 전략적 오픈소스 프로젝트로 관리 중임
   Amazon S3, EBS, DynamoDB, Aurora, EC2, IoT 등 주요 서비스팀이 P를 활용해 시스템 설계의 정확성 검증을 수행함

   S3를 강력한 read-after-write 일관성으로 전환할 때, P로 프로토콜을 모델링 및 검증해, 설계 초기 단계에서 버그를 제거하고 최적화 사항도 안정적으로 반영함

   2023년 AWS P 팀은 PObserve 툴을 개발해, 테스트와 실제 운영 환경 모두에서 분산 시스템의 정확성을 검증 가능하게 함
   PObserve는 실행 로그를 추출해, 명세에 따라 올바른 동작이 이뤄졌는지 사후 검증이 가능하며, 설계 단계 명세와 실제 코드 구현을 효과적으로 연계해줌

경량 형식 방법론 확대

  속성 기반 테스트

   가장 대표적인 경량 형식 기법은 속성 기반 테스트임
   예를 들어 S3의 ShardStore 개발팀은 개발 주기의 전 과정에서 속성 기반 테스트·코드 커버리지 기반 퍼징·실패주입·카운터 예제 최소화 등을 복합 사용함
   이 방식은 개발자가 직접 작성하는 정확성 명세와 연동되고, 문제점 디버깅 효율도 크게 향상됨

  결정적 시뮬레이션

   결정적 시뮬레이션 테스트는 단일 스레드 시뮬레이터에서 분산 시스템을 실행, 모든 난수 요소(스케줄링, 타이밍, 메시지 순서 등) 제어가 가능함
   특정 오류 및 성공 시나리오에 대한 테스트, 버그 유발 순서 조정, 퍼징 확장 등 다양한 방식으로 적용됨
   이로 인해 시스템의 지연, 실패 등 동작 검증을 빌드 단계에서 일찍 수행하고, 테스트 범위가 확대됨
   AWS는 이러한 빌드 타임 테스트 코드를 shuttle, turmoil 오픈소스 프로젝트로 공개함

  지속적 퍼징

   지속적 퍼징, 특히 코드 커버리지 기반 대규모 입력 생성은 통합테스트 단계에서 시스템 정확성 검증에 효과적임
   예를 들어 Aurora Limitless Database 개발시 SQL 쿼리와 트랜잭션을 퍼징해, 파티셔닝 논리의 정확성을 대량의 무작위 스키마·데이터셋·쿼리를 생성해 검증함
   결과는 non-sharded 엔진의 동작 또는 SQLancer 등의 방식과 비교함
   퍼징과 실패 주입 결합으로 원자성, 일관성, 격리성 등 데이터베이스의 핵심 속성을 검증함
   자동 생성 트랜잭션, 격리성 등 일부 속성은 실행 이력 기반 사후 검증을 통해 보장함

Fault Injection Service를 통한 장애 주입

   2021년 AWS는 Fault Injection Service(FIS) 를 출시해 고객도 API 오류, I/O 중단, 인스턴스 장애 등 다양한 결함 시나리오를 실제 또는 테스트 환경에 빠르게 실험할 수 있도록 함
   이를 통해 아키텍처의 가용성 확보 및 장애 복원력 점검, 오류 케이스의 높은 버그 밀도 차이 해소, 가능성 높은 중대한 문제 사전 발견 등의 효과가 있음

   FIS는 AWS 고객은 물론 Amazon 내부에서도 광범위하게 사용되고, 예를 들어 Prime Day 준비 과정에서만 733건의 실험이 진행됨

   오류 주입은 형식 명세와 결합하면 더 효과적임
   예상 동작을 형식 명세로 작성 후, 실제 결함 유발 결과를 이에 대조해 기존의 단순 로그·지표 점검보다 더 많은 오류를 잡을 수 있음

메타안정성과 시스템 발현 동작

   분산 시스템에서 지나친 부하/캐시 소진 등 유발로 ‘자체적으로 복구 불가능’한 비정상(메타안정적) 상태에 빠지는 사례가 증가함
   이 상태에선 단순 부하 감소만으로는 정상 복구되지 않고, 통상적인 오류 케이스보다 대응이 까다로움
   대부분의 재시도-타임아웃 로직도 이런 현상의 원인이 됨

   기존 형식 명세는 안전성과 진행성에 초점 두지만, 메타안정성은 그 이외의 다양한 발현 동작까지 고려해야 함
   AWS는 TLA+, P 등의 명세 모델을 바탕으로 이산 이벤트 시뮬레이션을 진행, 성능 SLA 달성 가능성·지연 분포 산출 등 확률적 특성까지 체계적으로 분석함

형식 증명의 필요성

   일부 보안 경계(권한·가상화 등) 에서는 단순 테스트 이상의, 수학적 수준의 증명이 필수적임

   예를 들어 2023년 AWS가 도입한 Cedar 권한 정책 언어는 Dafny 기반으로 자동 증명과 형식적 검증에 최적화되며, 공개 코드와 정정 절차를 통해 전체 사용자도 직접 검증이 가능함
   또 Firecracker VMM의 보안 경계(key property)는 Kani 등의 Rust 코드 분석 도구로 증명 작업 진행됨

   이처럼 형식 모델과 명세를 다양한 시점(설계, 구현, 시행, 증명)에 폭넓게 활용함으로써 소프트웨어 정확성 확보와 기업·고객 가치 확대에 활용함

정확성을 넘어선 추가 효과

   형식 방법론은 신뢰성과 성능 개선 모두에 중대한 역할을 함
   예를 들어 Aurora의 commit 프로토콜을 P와 TLA+로 검증, 네트워크 소요 라운드트립을 줄이는 동시에 안전성도 보장함
   RSA 암호화 알고리듬의 ARM Graviton 2 최적화 시, HOL Light에서 변환의 수학적 정확성을 증명해 성능·인프라 비용 동시 개선이라는 실질 효과를 거둠

미래의 도전과 기회

   15년간 AWS는 형식/준형식 방법론의 산업 적용을 크게 확대했으나, 학습 곡선, 전문가 필요성, 도구의 학술적 특성 등 실질적 도입 장애 문제가 상존함
   속성 기반 테스트, 결정적 시뮬레이션 등도 많은 개발자에겐 여전히 생소함
   교육상의 진입장벽이 학부 과정부터 존재하므로, 도구·방법론의 보급과 실무 적용이 더디게 진행됨
   메타안정성 등 대규모 시스템의 발현적 특성도 연구 초기 단계임

   향후 AI/대형 언어 모델이 형식적 모델·명세 작성을 지원해, 실무자 접근성을 단기간 내 획기적으로 높여줄 것으로 기대됨

결론

   견고하고 안전한 소프트웨어 구축에는 다양한 시스템 정확성 확보 수단이 필요함
   AWS는 표준 테스트 기법 이외에도 모델체킹, 퍼징, 속성 기반 테스트, 장애 주입 테스트, 결정적/이벤트 기반 시뮬레이션, 실행 이력 검증 등을 포괄적으로 도입함
   형식 명세와 방법론은 AWS의 개발 프로세스에서 중요한 시험 오라클 역할을 하며, 이미 실질적·경제적 효과 검증을 통해 투자 영역 중 하나로 자리잡고 있음

        Hacker News 의견

     * 결정론적 시뮬레이션 테스트 방식에 대해 이야기하고 싶음. AWS에서는 분산 시스템을 단일 스레드 시뮬레이터에서 실행하면서 스레드 스케줄링, 타이밍, 메시지 전달 순서 등 모든 비결정적인 요소들을 제어함. 그런 다음 특정 실패나 성공 시나리오에 맞춘 테스트를 작성하며, 시스템 내 비결정성은 테스트 프레임워크가 제어함. 개발자는 과거에 버그를 유발했던 특정 순서를 지정할 수 있음. 스케줄러는 순서에 대한 퍼징이나 가능한 모든 순서 탐색까지 확장 가능함. 이런 것을 언어와 무관하게 오픈소스로 구현해둔 라이브러리가 있는지 궁금함. 컨테이너 내에서 네트워킹, 스토리지 등도 테스트 시점에 ""결정적""으로 만드는 미들웨어 도구가 필요하다는 아이디어임. antithesis는 거의 이와 같은 역할을 하지만, 아직 오픈소스에서 못 봤음. 테스트를 잘 짜면
       I/O 같은 걸 스텁 처리해서 어느 정도 해결할 수 있지만, 다들 테스트를 잘 짠다는 보장은 없음. 애플리케이션 위에 더 높은 계층에서 결정성을 제공하면 좋다고 생각함. 한편으로 AI가 테스트에서 제대로 빛을 발할 수 있을 거라 기대함. 프롬프트(요구사항)-테스트 구현-AI-실행 코드 이 세 축이 이상적으로 맞물릴 수 있음. AI가 형식 검증을 더 손쉽게 해서 소프트웨어 세계를 한층 엄밀하게 만들어줬으면 하는 기대가 있음
          + 결정론적 시뮬레이션 테스트(DST) 기술 확산에는 두 가지 어려움 존재. 첫째, 기존에는 모든 시스템을 특정 시뮬레이션 프레임워크 위에 직접 올려서 다른 의존성이 없어야 했음. 둘째, 입력 생성 및 탐색이 약하면 테스트가 전부 성공하는 척 보이면서 실질적 검증이 안 됨. antithesis가 이 둘을 모두 해결하려 노력하고 있지만 아직 어려움 많음. 결정성을 아무 소프트웨어에나 적용하는 확실한 방법을 지닌 곳은 잘 없음. Facebook의 Hermit 프로젝트도 결정론적 Linux 유저스페이스로 시도했지만 끝내 중단됨. 결정론적 컴퓨터는 테스트 외에도 매우 유용한 기술적 기반이고, 언젠가는 누군가 오픈소스로 공개할 것이라는 생각임
          + QEMU를 100% 에뮬레이션 모드에서 단일 쓰레드로 돌리며 결정론적 머신을 얻는 건 비교적 쉬운 편이라 생각함. 다만 진짜로 원하는 건 '제어된' 결정적 실행이고, 이건 훨씬 어려움. 여러 프로세스를 지정한 시나리오로 동작하게 만들려면 특히 CPU 및 OS 스케줄러 레벨에서 난이도가 매우 높음. 언어에 구애받지 않는 환경 자체를 만들기도 힘들고, 세세한 부분에 휘둘리기도 쉬움. 나도 JVM 쓰레드 여러 개를 특정 동작 시점에서 락스텝(lockstep)으로 동작시키는 간단한 시스템을 만든 적 있는데, I/O와 시스템 타임을 스텁과 제어로 처리함. 그래서 비동기 컴포넌트들 간의 다양한 상호작용, I/O 장애, 재시도 등도 테스트해보고 프로덕션 가기 전에 골치 아픈 버그를 잡음. 단, 전체 시스템을 제어하기보다는 특정 동기화 지점만 단순화해서 가능했음. 동기화 실수로
            인한 일반적인 데이터 레이스는 이 방식으론 잡기 어려움
          + FoundationDB의 테스트 방법 공식 문서 및 유튜브 발표 영상 공유
          + gdb로 디버깅 가능한 언어라면 https://rr-project.org/ 프로젝트 추천
          + Joe Armstrong이 property testing을 활용해 Dropbox를 테스트한 발표를 예전에 본 기억이 있음
     * S3는 지금까지 본 소프트웨어 중 가장 멋진 것 중 하나라 생각함. 몇 년 전 S3 전체 시스템에 강력한 읽기-쓰기 일관성을 추가한 것도 진짜 소프트웨어 엔지니어링의 극치라 생각함 블로그 포스팅 링크
          + S3의 라이프사이클 담당으로 직접 일한 적이 있는데, 그때 인덱스 팀이 이 일관성 제공을 위한 구조를 새로 설계하던 시기와 겹침. 외부에서 봐도 S3는 대단하지만, 내부적으로는 구현 및 조직 구조 모두 상상을 뛰어넘을 정도로 인상적임
          + Google Cloud Storage는 오히려 S3보다 훨씬 이전부터 이 기능(강력한 일관성)을 갖추고 있었음. 전반적으로 GCS가 좀 더 체계적이고 제대로 만들어진 제품 같다는 인상임
     * 92% 수치(클러스터 장애의 대부분이 사소한 실패에서 시작됨)에 공감함. 화려한 대형 사고가 아니라 ""별거 아닌"" 재시도가 상태 누적으로 이어지다가, 결국 새벽 2시에 대규모 장애로 터진 경우가 대부분임. 눈에 안 띄는 실패에 더 많은 엔지니어링 시간을 할당하는 것이 중요함
          + 이건 살아남은 문제만 보게 되는 '생존 편향' 효과일 수도 있음. 큰 문제는 이미 해결되어 두 번 다시 발생하지 않고, 덜 위험해 보이는 사소한 문제들이 때때로 큰 장애를 일으킴
     * 정말 흥미로운 글이라 생각함. 인프라 컨트롤 플레인 구축에 상태 기계 사용은 필수임. 꼭 P가 필요했을지는 모르겠음. 우리도 13년 넘게 Ruby로 인프라 컨트롤 플레인을 만들었고, 아주 훌륭하게 동작함 관련 경험 공유 블로그
     * P 언어에 대해 궁금한 점 있었음. 예전엔 Microsoft에서 Windows USB 스택 런타임 용도로 P로 C코드 생성해서 실제로 사용한 것 같은데, 지금은 더 이상 운영 코드 생성에 쓰지 않는 것 같음. 관련 질문을 Hacker News에도 남긴 적 있음 질문 링크. 커널에까지 생성된 코드가 들어갈 수 있다면 훨씬 완화된 조건의 클라우드 환경에도 분명히 쓸 수 있을 듯
          + azure에서 사용하는 Coyote가 P#의 진화판이고, P#은 다시 P에서 진화한 듯 보임 poolmanager-coyote 논문 링크
     * AWS 출신이 아니고 TLA+나 P에 익숙하지 않은 입장에선 ""헬로월드""쯤의 예제라도 있었으면 그나마 이해가 쉬웠을 것 같음. 글만 보면 그냥 고통스러운 과정처럼 느껴지기도 하고, 좋은 설계와 테스트만으로 충분히 잡을 수 있는 문제 아닌가라는 생각이 듦. 간단 예제가 있었다면 실제로 뭘 하는지 판단에 더 도움이 됐을 것
          + 내가 좋아하는 TLA+의 빠른 데모 예제가 있음 gist 링크. 여러 스레드가 공유된 카운터를 서로 원자적이지 않게 증가시키는 모델인데, 속성 검증시 race condition이 잡힘. 실제로 이런 버그를 테스트만으로 발견하기 무척 어려움. 대부분의 TLA+ 명세는 훨씬 더 복잡하지만, 단순 오류를 포착하는 데 이 예제가 좋음
          + 직접 TLA를 써봤는데, 그래픽 도구가 튜토리얼과 잘 맞지 않아 실망스러웠던 경험 있음. TLA 자체는 쓰고 싶었고 Lamport의 작업(LaTeX~논문까지)을 예전부터 좋아해 왔음
          + 형식 기법(formal methods)을 쓰는 전제가 결국 테스트만으로 절대 모든 문제를 잡을 수 없다는 데 있음
          + 공식 TLA+ Examples 깃허브 저장소 추천. DieHard 문제 같은 단순한 것부터 시작하는 걸 권장함
          + 테스트는 문제의 특정 인스턴스에 대해 구현의 옳음을 증명하지만, 형식 검증은 전체 범주에 대해 증명함. 예를 들어 아나그램을 반환하는 함수의 경우, 테스트는 일부 단어쌍만으로 확인하지만, 전체 단어쌍에 대해 옳음을 증명하려면 formal verification이 필요함. undefined behavior나 라이브러리 버그 같은 사례도 formal verification 과정에서만 잡히는 경우 많음
     * ""프러퍼티 기반 테스트, 퍼징, 런타임 모니터링 같은 경량 준형식 기법"" 언급에, property-based testing과 fuzzing은 형식 기법의 하위 집합이 맞지만, runtime monitoring까지 준형식 기법에 포함하는 건 다소 과하다고 느낌
          + PObserve 같은 도구를 활용한 런타임 모니터링이라면 충분히 준형식 기법이라 볼 수 있음. 단순 알람이나 메트릭 체계와 구분 필요함
     * 예전에 Leslie Lamport와 Buridan's Principle 관련 논문 등으로 교류한 적 있음. 오늘 그의 홈페이지에서 TLA+와 PlusCal에 대해 많이 알게 됨 Peterson 예제 페이지. 수학을 프로그래밍에 끌고 오고, 동시성 시스템 분야의 시초격인 분이 시스템 설계 언어(TLA+)를 만들고, AWS 등에서 쓰인다는 점이 너무나 자연스러움. 분산 시스템 구축하는 사람들이 Lamport가 만든 걸 더 많이 썼으면 하는 바람이 있음. 대규모 시스템에선 옳음 증명이 매우 중요함
          + 기존 코드에서 TLA+ 사양 변환은 Claude Opus(Extended Thinking)가 아주 쓸만함. Rust 프로젝트나 C++ 핵심 컴포넌트 검증에도 여러 버그를 찾은 경험이 있음. 다른 모델은 구문 및 스펙 논리에서 자주 막히는 반면 Opus는 훨씬 매끄럽게 작동함
          + 대규모 시스템뿐 아니라 SSH, 터미널 같은 작지만 중요한(전세계에서 많이 쓰는) 유틸리티에도 옳음 증명은 굉장히 유익함
          + ""시스템의 옳음 증명""이라는 발언에, 실제로 전부 증명하는 건 불가함. 모델 체커는 한정된 상태 공간 내에서 명시한 스펙이 속성을 만족한다고만 알려줄 수 있음
     * 개인적으로 FIS를 분산 서비스 실험에 써본 경험 있는지 궁금함. 도입 고려 중이지만 직접 대규모 실험을 해본 경험은 없음
     * Promela와 SPIN이 글에서 다루는 것보다 더 상위 수준 언어인지 궁금함
          + Promela로 분산 시스템 작업도 해봤던 입장인데, 이 분야에 딱 맞는 느낌은 아님. 독특한 아이디어는 있지만, 다시 살펴볼 여지는 있음
"
"https://news.hada.io/topic?id=21279","생산적인 모노레포를 만들기 위한 필수 요소","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        생산적인 모노레포를 만들기 위한 필수 요소

     * 모노레포 도입은 조직의 일관성, 코드 공유, 공동 툴링 환경 강화 등 장점이 있으나, 빅테크 사례를 그대로 따라가면 새로운 문제와 도전에 직면하게 됨
     * 성공적인 모노레포를 위해서는 모든 주요 작업을 O(repo)가 아닌 O(change) 로 만드는 원칙을 지켜야 하며, 빌드·테스트·CI/CD 각 단계에서 이에 맞는 도구와 전략이 필요함
     * 소스 컨트롤은 git을 시작으로 규모가 커질수록 sparse checkout, 가상 파일시스템 등 점진적 확장 고려 필요
     * 빌드 시스템은 가능한 한 단일 언어 유지, 각 언어의 기본 빌드 툴로 최대한 버티고, 꼭 필요할 때 Bazel/Buck2 등으로 점진적 전환 권장
     * 테스트·CI/CD는 변화 영향 범위만 빠르게 감지하여 빌드·테스트·배포해야 하며, 대규모 모노레포에서는 테스트 자동 재시도, flaky test 격리 등 신뢰성 확보 전략도 필수
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 모노레포를 향한 여정의 시작

     * 새로운 Developer Productivity 팀에 있는 엔지니어라면, 모노레포 도입 결정 이후 어떤 준비와 노력이 필요한지 고민이 커짐
     * Google, Meta, Uber 등 대기업의 모범 사례는 근사해 보이지만, 현실적으로 그들과 동일한 수준의 결과는 불가능함
     * 각 조직은 자기만의 이유와 필요성에 맞춰 모노레포 도입을 결정해야 하며, 그 과정에서 일관성(consistency), 조직적 통합, 공동 툴링 등에서 이점을 추구할 수 있음

모노레포의 필요성을 명확히 하기

     * 대기업 사례는 결과적으로 도달한 모습일 뿐, 초기에 참고할 근거로 삼기에는 부적절함
     * 실제로는 새로운 문제들이 발생하며, 기존 여러 레포 관리 체계와는 다른 유형의 이슈가 발생함
     * 모노레포의 도입 목적은 일관성 유지, 조직 전반에 걸친 툴링 통합, 엔지니어링 표준 및 컨벤션 적용 등에 있음
     * 각 팀은 자체 문화와 방향에 맞는 목표를 명확히 하여야 효과적인 결과를 얻을 수 있음

골든 룰: O(change)의 원칙

     * 모든 저장소 관련 도구들은 빠른 동작을 위해 O(repo) 가 아니라 O(change) 복잡도를 가져야 함
     * 실제로 대규모 모노레포가 클수록 기존 도구들의 비효율성이 두드러지므로, 성능 문제를 극복하는 구조적 설계가 필수임
     * 대기업 기술 블로그에서 언급되는 혁신 역시, 대부분 O(repo)로 인한 비효율 극복에 집중되어 있음

소스 컨트롤

     * 대부분 소프트웨어 조직은 Git을 기본으로 사용하나, Git은 중앙집중식 모노레포 환경에서 대규모로 확장될 때 성능 한계가 있음
          + 현실적으로 대부분의 조직은 git+GitHub로 상당히 오래 버틸 수 있음
     * 성장이 빨라질수록, sparse checkout(부분 클론) 기능과 가상 파일시스템(필요시 파일을 서버에서 동적으로 다운로드) 같은 구조가 필요해짐
     * 대기업은 이에 맞춰 Git을 포킹하거나 별도의 시스템을 개발함 (Microsoft: 자체 Git 포크, Meta: Mercurial 포크, Google: Piper 등)
          + Jujutsu 등 차세대 소스컨트롤도 고려할 만함
     * 규모가 작을 때는 Git을 무리없이 사용 가능하나, 성장 과정에서 확장 전략을 염두에 둘 필요가 있음
     * 소스코드에 IDL(Interface Definition Language) 로 생성된 코드가 포함되면 저장소 크기가 기하급수적으로 증가하는 현실적인 문제도 존재함

빌드 시스템

     * Bazel, Buck2 등은 대표적인 모노레포 빌드 툴로 다수 언어와 복잡한 빌드 그래프를 지원함
          + 강력하지만 복잡성/운영 부담 큼
     * 빌드를 단일 언어로 유지하면 삶이 훨씬 편해지며, 각 언어별 빌드 시스템(예: Maven, Gradle, Cargo, Go 등) 역시 높은 확장성을 가짐
     * 빌드 시스템의 핵심 역할은 “지정한 빌드 타겟을 효율적으로 빌드(효율적 아티팩트 생성)”하고 “변경된 파일로 인해 영향받는 타겟을 신속하게 산출”하는 것임
     * 이를 위해 target determinator(타겟 결정 도구) 개념이 필요하며, Rust, Go, Bazel 등 생태계에는 이미 다양한 해결책이 있음
          + Rust: guppy
          + Go: go/packages
          + Bazel: target-determinator
     * Remote execution과 캐싱은 초대형 규모에서만 실제적으로 필요하며, 일반적인 기업에선 target determination이 더 실용적으로 활용됨

테스트

     * 전체 테스트를 매번 실행하는 것은 비효율적이므로, 변경 영향 범위만을 테스트하는 시스템이 요구됨
     * 플레이키(Flaky) 테스트는 대규모 테스트 시스템에서 더욱 심각한 이슈가 될 수 있음
     * 테스트 시스템은 자동 재시도, 테스트 영향 범위 자동 판단, 플레이키 테스트 격리 등이 필요함
     * 일부 언어(예: Rust의 nextest, Java의 JUnit 등)는 이러한 고급 기능을 기본이나 확장으로 제공함
     * 모노레포의 테스트 체계는 빌드 시스템과 밀접히 통합되어야 효과적임

지속적 통합(CI)

     * CI 시스템은 변경사항에 따라 필요한 빌드 아티팩트와 유효성 검증을 자동 수행해야 함
     * Target determinator 성능과 효율성이 CI 파이프라인의 핵심 요소로 작동함
     * 현대 CI는 “Merge Queue” 등 다양한 전략을 사용해, 코드 품질 유지와 병합 속도 최적화 간의 균형을 찾아야 함
          + 단일 커밋/PR 마다 모든 검증 작업을 실행할지, 일부만 선별할지, 여러 PR을 배치 처리할지 등
     * Throughput(처리량), Correctness(정확성), Tail latency(최대 대기 시간) 간의 트레이드오프를 자체적으로 정의하고 설계해야 함
     * 대용량 모노레포의 병합 관리와 CI 효율성 강화는 아직까지도 완벽한 해법이 없는 도전 과제임
     * Rust(bors), Chromium, Uber 등은 각자 다른 병합/검증 전략을 선택함

지속적 배포(CD)

     * 모노레포 내 모든 변경이 원자적으로 배포될 것이라는 환상은 현실과 다름
     * 하나의 PR로 다수 서비스의 인터페이스와 구현, 클라이언트까지 한꺼번에 변경할 수 있지만, 결국 실제 배포(Deploy)는 비동기적으로 진행되어 배포 시점에 문제 발생 가능
     * 서비스 간 계약(Contract)을 깨뜨리는 변경은 배포시 심각한 장애를 유발할 수 있음
     * 효과적인 모노레포 CD 전략은 배포 시스템 주기, 서비스 계약 검증, 문제 발생 시 신속감지 및 대응 능력 등이 필요함

결론

     * 모노레포는 조직적 일관성과 엔지니어링 문화 강화를 위한 강력한 도구이나, 지속적인 엔지니어링·툴링 투자가 필요
     * 각 단계별로 O(change) 원칙에 맞춘 자동화·도구·문화 구축이 핵심
     * 성장과 함께 도구 또한 지속적으로 진화시키며, 조직 목표와 문화를 반영하는 체계적인 관리 노력이 중요함
     * 충분한 각오와 헌신, 지속적 투자가 있다면, 모노레포는 궁극적으로 그만한 가치를 만들어줌

   뼈와 살이 되는 글이내요 강력한 도구 뿐만아니라 필요시 필요한 도구까지 만들 각오로 해야 되요 그래서 잘 굴러간다면 얻는 이익도 많아요

   석사 시절에 지도교수가 구글 출신 엔지니어분과 식사하면서 모노레포 얘기를 듣고 오셨는지 우리도 앞으로 모노레포로 관리하자고 제안한 적이 있었는데 말리느라 고생했었습니다...
   모노레포가 좋은 점이 많긴 하지만 저희 연구실은 그 특성상 성과물을 외부 사람들에게 공유해야 하는 경우가 잦은데, 모노레포로 성과물을 관리해왔다면 여기서 특히 고생했을 것 같습니다. 멀티레포면 그냥 성과물 별로 공개범위를 조절하면 되니까요.

   모노레포를 하면서 고통 받는 경우는 대부분 이미 프로젝트를 너무 잘게 쪼개 놓은 경우인 거 같아요. 원래 한두 개면 될 프로젝트를 10여개로 쪼개놓고, 그걸 모노레포로 통합해서 관리하려니까 모노레포 관리 툴도 써야 하고 복잡도도 올라가죠. 그냥 프로젝트 자체를 한두 개로 통합하는 게 좋고, 두 개 이상의 프로젝트라도 관리툴 따로 쓰지 말고 그냥 디렉토리만 나눠서 한 저장소에 넣는다는 개념으로 생각하면 더 속편하게 관리할 수 있어요.

        Hacker News 의견

     * 이 스레드를 보면서 예전의 complexity merchants에 대한 이야기가 떠오르는 경험 공유 요청. 모노레포로 이동하면 기술적 희생이 있다는 의견에 전혀 동의하지 않는 입장. 계층적 파일 시스템의 파워를 이해하면 모노레포의 가치를 알 수 있음. CI/CD가 여기저기 흩어진 구성보다 모노레포 하나로 구성하는 게 훨씬 더 명확. 모노레포의 핵심은 전체 조직이 원자적인 커밋을 할 수 있다는 점임. 많은 개발자를 조율할 때 그 효용이 압도적임. 한 번에 리베이스하고 한 번의 큰 미팅이면 충분. 팀원들이 서로 안 좋아해 협업하지 않아도 관리 측면에서 모노레포는 큰 HR 도구 역할.
          + 최근 개발자들은 지나치게 분리, 마이크로서비스, 다수의 작은 레포지토리, 모놀리스를 극도로 피하는 경향이 있음. 이는 복잡성을 키워 조직 구조 문제를 미래의 기술적 문제로 전환하는 결과. 소프트웨어 시스템의 내부 종속성도 제대로 인식하지 못함. 이전 직장에서 프로토콜 버퍼 스키마 파일을 업데이트하는 데 낭비된 시간이 믿기지 않을 정도. 다행히 지금 회사는 그렇지 않음.
          + 여러 프로젝트에서 커밋을 추적하는 것은 있으면 좋은 정도이며, 실제로 의존성 추적이나 다운스트림 테스트 트리거 면에서 큰 차이 없음. 멀티레포 자동화로도 충분히 가능. 모노레포가 도움은 되지만 완전하지도 않고, 비용도 큼. 배포나 빌드가 원자적으로 처리되지 않음. 모노레포 규모가 커지면 git에서 벗어나 새로운 툴이 필요하고 이건 아주 큰 작업. 경험이 없으면 쉽게 말할 수 있는 부분 아님.
          + 모노레포의 장점은 분명 존재하지만, 관리 비용이 polyrepo보다 더 비쌈. 어떤 상황에서든 무조건 모노레포가 좋은 것은 아님. 자세한 설명은 이 글 참고. 비용 대비 효과는 상황에 따라 다름.
          + 프로그래밍 환경 설계에서 팀에 더 많은 파워를 줄수록 문제도 늘어난다는 게 유익한 경험칙. 기술적으로는 원자적 커밋이 더 강력한 파워가 아니고 오히려 적은 파워지만, 나쁜 인터페이스로 일하는 걸 가능하게 하므로 오히려 문제를 유발하는 파워임.
          + 모노레포로 바꾸면 변화가 더 원자적이라는 믿음은 함정이라는 의견. [원문 인용: 모노레포의 가장 큰 허상은 전체 코드베이스에 원자적 커밋이 가능하다는 것. 실제로는 다양한 배포 아티팩트가 있는데, 서비스와 클라이언트 등을 한 번에 바꿔도 배포는 비동기적으로 일어남. 여러 레포에서는 여러 PR로 작업해야 하므로 위험 인식이 깔림. 모노레포의 CI는 주로 서비스 계약(CI job) 검증 역할을 하며, 필요시 변경 이유 명시가 요구됨.]
     * 빅테크 모노레포에는 두 가지 유형이 있음. 첫째는 글에서 말한 전사적 단일 ""THE"" 모노레포로, 커스텀 VCS/CI가 필요하고 200명 엔지니어가 지원함. Google, Meta, Uber가 이 방식. 이 경지에 오르기까지 고통은 상상 이상이며, 보통 더 작게 나눈 ""팀 단위"" 모노레포에서 점차 확장. 각 스택/언어/팀마다 Bazel, Turborepo, Poetry 같은 도구로 각자 관리하다 시간이 지나면 더 큰 모노레포로 합쳐짐. 그러나 이 둘 모두 개발자와 비즈니스 모두 수백만 달러, 수백만 시간의 투자가 들어가며, 결국은 이 과정을 견딘 개발자들의 지원으로 유지.
          + 대형 모노레포 회사에서 일했을 때 모노레포를 훨씬 더 선호. 단일 모노레포가 서비스 그래프, 코드 호출 구조 등 전체를 투명하게 파악하는 데 아주 도움이 됨. 폴리레포의 경우 지식이 팀별로 분산, 신규 코드 인수도 어렵고, 코드 아카이브 파악은 마치 미궁 탐험. 폴리레포는 오래된 디스코드/슬랙 메시지처럼 잊혀지는 느낌. 모노레포가 비용이 많이 들면, 폴리레포도 마찬가지로 다른 방식의 비용 발생. 모노레포는 거대한 대륙의 초식동물, 폴리레포는 다양한 종이 어둠에 묻힘.
          + 현재 회사에서 백엔드가 약 11개 git 레포로 나뉘어 있으며, 기능 한 건을 위해 4~5개 머지 리퀘스트가 필요해 매우 번거로움. 여러 프로젝트를 모으기 위해 모노레포 도입을 검토 중. 그런데 레포를 합칠 수 없다면 모노레포 대안은 무엇인지 궁금.
          + 언어와 무관하게 쉽고 강력한 모노레포 오케스트레이션 시스템은 아직 없음. Bazel은 복잡하고 배우기 어렵지만 최근엔 문서화가 많이 개선됨. Buck, NX, Pants 등 다른 선택지도 있지만 각각 특징이 있고, 특히 웹 지원은 제한적임. 대부분의 CI가 이런 툴을 제대로 지원하지 않아 설정이 까다로움. 참고로 Microsoft의 Rush가 최고의 경험 제공, 특히 프론트엔드/노드JS 모노레포에는 Rush 추천 Rush 공식 사이트.
          + 대부분의 모노레포가 Google, Uber, Meta같은 대기업 규모까지 커지지는 않는 현실 언급. 서비스 수도 회사마다 다르고, 많아도 100개 정도라면 VCS 스케일 문제 없고 LSP 태그도 무리 없이 랩톱에서 다 돌아감. 모든 테스트를 CI에서 무작정 돌려도 거의 무난. 결론: 모든 회사가 구글 규모를 필요로 하진 않음.
          + 현재 회사는 언어 스택별로 모노레포를 구축하는 중. 꽤 괜찮은 절충안임.
     * 모노레포 vs 멀티레포 논의에서 자주 나오지 않는 포인트는 '역 콘웨이의 법칙' 발생. 레포 구조가 조직 구조와 문제 해결 방식에 영향을 준다는 점. 모노레포는 공통 인프라 팀에 영웅적 작업을 유발, 공통 영역을 건드릴 때 잠재적 깨짐이 많아져 기능 하나 개발에도 난이도 상승. 멀티레포에서는 팀 간 여러 PR과 조율, 내부 정치 등이 필요하지만 더 다양한 개발자가 역할을 분산해 처리 가능.
          + 모노레포에서도 중앙에 깊이 연결된 변경이라면 여러 단계로 나눠 적용할 수 있음. 그 과정에서 여러 PR 및 조정, 정치적 이슈도 다뤄야 하지만, 오히려 모노레포라서 롤아웃 상황을 더 명확하게 볼 수 있음.
          + 폴리레포에서는 공통 영역에서의 변경이 다운스트림 레포들에 반영되지 않아 각 레포가 다른 버전에 고정되고, 수년간 업데이트가 안 되어 고생하는 사례가 훨씬 더 흔함.
          + 조직이 애초 방향성을 레포 구조로 선택하고 나중에 기술 선택이 따라온다는 가정이 맞는지 질문. 실제론 구체적인 repo 구조보다 더 근본적인 조직 철학(파편화 vs 공유) 결정이 선행. 방향이 바뀌더라도 코드 관리 방식은 수정할 수 있음. 멀티레포라도 엔지니어가 거의 모든 코드 접근 가능하고, 모노레포도 강한 격리와 별도의 CI나 배포 관리 규칙 적용 가능.
          + 모노레포에서는 프로젝트 간 손쉬운 변경이 폴리레포에서는 너무 번거로워서 아예 시도조차 안 되는 경우가 훨씬 더 많음.
     * 대형 테크 기업 경험상 빌드 시스템 관리에는 아예 전담 팀이 필요. 대형 모노레포는 소스 파일을 필요 시 다운로드하는 가상 파일 시스템 기반. 기사에서 언급 안 된 점은, 거의 모든 개발이 데이터센터에서 동작하는 개발 서버에서 진행, 50~100코어 환경 혹은 온디맨드 컨테이너(수시로 최신 커밋으로 업데이트) 활용. IDE가 dev server와 통합돼 언어나 서비스별로 사전준비/자동설정까지 chef/ansible로 자동화. 랩톱에서 직접 대규모 모노레포 개발할 일은 매우 드뭄(예외: 모바일/맥앱 등).
          + 아마 같은 빌드 팀에서 일한 경험자. 모노레포 개발 환경이 로컬이든 원격이든, 재현성(reproducibility)이 더 중요. 이미징되는 원격 dev server면 더 쉽고 신뢰도 높음.
          + 적은 규모 팀에서도 데이터센터 개발환경 활용 경험. 요즘 하드웨어 가격과 밀도를 보면 자체 랙을 꾸려 dev/staging/test 등 온디맨드 툴 다 돌리는 게 훨씬 합리적. 프로덕션과 유사한 개발 환경을 공유하게 되면 모노레포 방식이 아주 달라보임. 하지만 중소 기업은 빌드 시스템에 투자할 여력이 없고, 그런 대형 빌드 시스템 문제 자체가 생기지 않음(최소 10~20인 규모, 아주 복잡한 제품이어도 유지보수는 파트타임이 전부일 수도 있음).
     * Molnett(serverless cloud)에서 Bazel 기반 모노레포로 엄청난 효율을 경험한 소규모 팀(풀타임 1.5명) 이야기. Tilt+Bazel+Kind로 전체 플랫폼과 쿠버네티스 오퍼레이터까지 랩톱에서 기동, Mac/Linux 모두 지원. Bottlerocket 기반 OS 및 Firecracker까지 로컬에서 검증 가능. tool layer 구축으로 모든 개발자 동일 버전 go/kubectl 사용, 로컬 설치 필요 없음. 관리에 노력은 들지만, ex Google SRE 멤버 덕에 가능. 앞으로도 이런 방식만 원함. (주요 언어는 Golang, Bash, Rust)
          + 1.5명 소규모라면 단일 레포가 당연. Bazel 경험은 아주 나빴지만, 대규모 프로젝트에는 쓸 가치가 있을 수도. 2명 미만 규모에는 오히려 Kind+Tilt만으로 충분. tool layer도 Go가 이미 go.mod로 어느 정도 해결. kubectl도 비슷하게 가능. ex-Googler의 연봉 수준도 고민 필요. Bazel 유지비용이 앞으로도 가치 있길 바람.
          + 우리 회사는 시스템드(systemd) 기반 서비스 및 ansible playbooks로 배포, tmuxinator로 백엔드/DB/검색엔진/프론트엔드까지 모든 서비스 dev 모드로 터미널 한 번에 자동 기동. root에 ‘tmuxinator’ 명령 한번이면 전체 dev 환경이 뚝딱. 단일 모노레포가 이전보다 압도적으로 편리.
          + 비슷한 상황, Bazel 도입 효과 극대화 경험 공유. tool layer 덕에 일관되게 개발 환경 유지 가능. 직접 bazel run을 써야 하는데, 좀 더 나은 자동화 방법을 궁금해함. 어떻게 동작하는지 알려주기를 요청.
          + 2명 규모에서 마이크로서비스/K8s 패턴 자체가 오버엔지니어링. 이 정도 인원 규모에서는 어떤 방식이든 문제 없음. 예전엔 Dropbox/SVN/MS VCS 등 어떤 방식이든 다 돌아갔고(불편한 점은 있었지만), 다 문제가 되지 않았음. 이 규모에서는 모두가 전체 프로세스를 머릿속에 그릴 수 있음. 복잡한 도구나 인프라가 성공 요인이 아니라는 경험 공유.
     * 지난 4년간 여러 회사에서 세 번이나 모노레포 설정 작업한 프리랜서 경험 공유. 프론트엔드에 한정해 JavaScript/TypeScript 생태계만 써서 그나마 관리 쉬움. 실제 좋은 모노레포는 내부적으로 폴리레포처럼 동작, 각각의 프로젝트가 독립적으로 개발/배포/호스팅 가능하지만 하나의 코드베이스에 공존하며 공통 구성요소(UI 등)도 자유롭게 공유, 일관된 룩앤필 보장. 실전 가이드로 참고 자료 추천.
          + 이건 실제로 폴리레포가 아니라 모노레포를 제대로 구축한 사례임.
     * 결국, 모든 것은 경우에 따라 다름. 우리 회사는 약 40여 개의 git 레포를 별도 CI로 관리, 빌드/테스트/패키징 후 최종적으로 통합 파일 시스템 이미지를 만들어 인티그레이션 테스트. 컴포넌트 간 Flatbuffers 메시지로 통신하며 flatbuffers도 서브모듈로 관리. 다운스트림 의존성 처리가 힘들긴 하나, progressive enhancement로 어느 정도 유연성 확보. 이런 경우 멀티레포인지, 서브모듈 많은 모노레포인지 진단 자체가 애매. 모노레포로 바꾸면 장점이 있을지는 미지수. 결국 트레이드오프와 감내할 불편의 종류 선택 문제.
     * 모노레포 도구 관련 블로그 작성자 경험. 사람들이 모노레포의 장점만 강조하지만, 실제로 성공적인 모노레포 운영의 복잡함은 대부분 이면에서 devops/devtools 팀이 감당. 따라서 도입은 신중해야 하지만, 잘 구축하면 충분한 가치 제공.
     * 잘 관리된 모노레포 경험은 너무 좋아서 다른 워크플로우로 되돌아가기 싫음. 하지만 준비 안 된 ""우리도 모노레포 하자"" 식은 악몽. 만약 준비된 모노레포 환경과 도구를 패키지로 팔면 비즈니스 기회가 클 거라 생각.
          + NX가 이미 그런 사업을 하고 있음. 이전 스타트업에서 처음부터 NX로 개발해 15명 R&D팀으로도 100명 규모의 표준화를 실현. 새로운 회사(스타트업 인수한 곳)는 무계획 ""우리도 모노레포"" 시도로 참사. 지금 NX로 이관 중인데 효과 아주 좋음.
     * 대형 조직에서 모노레포가 오히려 팀 간 의존성을 극도로 제한해 코드 재사용을 저하시킬 수도 있다는 점을 경험. 라이브러리 팀이 바꾸려면 하위 사용자 모두 업데이트해야 하는데, 예상치 못한 방식으로 사용하는 팀 때문에 수정이 복잡하게 꼬임(Hyrum's Law). 결국, 대기업은 내부 복붙, 포크, 엄격한 접근제어, 느린 변경 승인 등으로 귀결.
          + 범용으로 활용할 라이브러리를 만들 땐 API 설계에 신중해야 함. 가능하면 API는 바꾸지 않고, 바꿔야 한다면 대규모 변경을 확실히 기획하거나 새 함수로 대체+구버전 deprecated 처리 권장. 소규모 코드라면 복붙도 괜찮음.
          + 그래도 모노레포의 장점은 모든 사용처를 쉽게 찾고, 필요시 원자적으로 변경/수정 가능하다는 점.
          + 모든 소프트웨어는 의존관계를 고려해야 하며, 모노레포는 오히려 라이브러리나 사용자 입장에 있어서 서로를 바꿀 권한이 증가.
          + 모노레포에서는 내 상황에 맞게 변경이 쉬우므로 코드 재사용 확률이 폴리레포보다 높음.
"
"https://news.hada.io/topic?id=21268","EU 집행위원회, 대규모 감시 제안서 작성자 공개 거부","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     EU 집행위원회, 대규모 감시 제안서 작성자 공개 거부

     * EU 집행위원회가 아동 성착취물 차단법(CSA 규제안) 의 작성자 정보 공개를 거부함
     * 해당 법안은 채팅 플랫폼 메시지 감시를 통해 불법 콘텐츠를 탐지할 수 있도록 하는 내용을 포함함
     * 내부 저자 정보 공개 요청에 대해 위원회는 투명성 원칙 대신 개인정보 보호 및 내부 검토 필요성을 주장함
     * 시민단체 및 개인정보보호 전문가들은 이런 비공개 방침이 법안 설계 과정의 신뢰성 저하 우려를 제기함
     * 이번 사안은 유럽 내 데이터 프라이버시 및 정책 투명성 논의에 중요한 쟁점으로 부각됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * EU 집행위원회는 CSA(CSA 규제안, Child Sexual Abuse Regulation) 법안의 작성자 신원 정보를 공개하지 않음
     * 이 법안은 채팅, 이메일 등 온라인 통신 플랫폼에서의 메시지 감시 및 아동 성착취물 자동 탐지를 허용하는 법적 근거 제공 목적임
     * 수년간 논란이 이어져왔으며, 정치인 및 시민단체들은 정책 설계 과정의 투명성 강화를 반복적으로 요구함

저자 공개 거부 사유

     * 위원회는 요청받은 저자 및 기여자 이름, 배경 정보에 대해 공개를 거부함
     * 거부 사유로 해당 직원들의 개인정보 보호와 내부 검토 절차의 독립성 유지를 들음
     * 또한 개별 저자의 노출이 과도한 외부 압력, 위협 또는 부적절한 영향 행사를 불러일으킬 수 있음을 언급함

시민사회 및 프라이버시 단체 반응

     * EDRi, Access Now 등 시민단체는 투명성 저하와 법안 검토 절차 신뢰성 훼손 가능성을 지적함
     * 정책 입안자와 기술적 조언자의 구성 또는 외부 이해관계자 영향력을 파악하기 어렵다는 비판 제기됨

맥락 및 함의

     * CSA 법안은 온라인 서비스 제공자가 모든 사용자 메시지에 대한 알고리듬 기반 스캐닝을 의무화하는 판례적 제안임
     * 해당 법안은 프라이버시 침해 가능성과 암호화된 통신 약화 논란으로 EU 내에서 격렬한 논쟁 대상임
     * 국제적 기준에 비추어 유럽 내 개인정보보호 및 정책 투명성 문제로도 연결됨

결론

     * EU 집행위원회의 저자 정보 비공개 조치는 정책 설계의 투명성, 책임성 문제와 직접적 연관성을 가짐
     * 이번 사례는 EU 내 정책 입안 과정의 신뢰성 확보와 관련된 중요한 지표로 해석됨

   몇년째 발안철회발안철회발안철회발안철회 질리지도 않는건가??

        Hacker News 의견

     * ""Policing by Design""이라는 EU 감시 계획 기사에서, 데이터 보안을 완전히 준수하면서도 모든 종류의 통신 서비스 제공업체의 데이터를 수집 및 접근할 수 있는 조화를 이룬 EU 데이터 보존 체계 필요성을 강조하는 내용 언급, 이전 EU 데이터 보존 법령이 기본권 침해로 2014년 위헌 판결된 사실 포함, 이번엔 모든 형태의 서비스 제공자와 전자 증거에 접근 가능한 데이터 확보, 그리고 수사기관이 명문 데이터(암호화되지 않은 데이터)에 접근할 필요성에 대한 합의 강조
          + HLG(High Level Group) 권고사항에는 SIRIUS같은 도구 및 노하우 공유 플랫폼 신설, 하드웨어 제조업체도 범위에 포함해 법집행기관과의 연결망 구축 필요성, 기술·프라이버시·표준화·보안 전문가들이 모여 영구적 구조를 만들어 ‘lawful access by design’을 추진, 법집행기관의 요구 정의만 돕고 구체적 솔루션 강요는 하지 않아야 민간기업이 보안 저해 없이 합법적 데이터 접근 가능, 암호화 데이터 접근 의무화 기술의 실현 가능성 검토 연구그룹 구성 등, 전면적인 감시 확대 및 소프트웨어·하드웨어 백도어 의무화 시도 의도, 관련 PDF 권독 권장
     * 암호화된 데이터에 접근 가능하면서 동시에 그 데이터가 안전할 수 있다는 말 자체가 말이 되지 않음에도, 여전히 ‘프라이버시 보호와 합법적 접근’을 모두 할 수 있다고 주장하는 점에 회의감 느끼는 입장
     * EU는 인간 존엄성, 자유, 민주주의, 평등, 법치주의 그리고 인권 존중 등 핵심 가치를 기반으로 한다는 점 상기, 그럼에도 불구하고 타국에 계속해서 이런 가치를 강의하려 하겠지만 현실과 괴리 느끼는 의견
          + 유럽사법재판소가 이런 감시 규제를 불법으로 판단할 가능성 존재
          + 결점이 많다고 해도, 이 가치들에 대해 타국을 교육할 더 나은 위치의 나라나 조직이 과연 있을지 의문, 러시아, 중국, 이란 등 타국 언급하며 비교
          + 이런 핵심 가치에 기반하였기 때문에 만약 감시안 작성자 신원이 밝혀진다면 예상했던 것보다 더 부끄러운 상황 될 수 있음
     * EDRi의 관련 기사(링크)를 읽어볼 가치 있음, 최근 대학 수업에서 채팅 통제(chat control)에 대한 논문 작성 후 유럽위원회의 결정에 불만족한 것만으로는 충분하지 않다는 생각, 당연히 더 많은 논의 주제 있음
          + 채팅 통제 논의 중에 Europol이 이미 규제 확대에 군침을 흘리고 있었고, 모든 트래픽은 유용하다는 발언
          + 유럽위원회는 Thorn이 제공한 데이터의 신뢰성에 대한 실제 검토 없이 효용성을 주장, 정보공개청구(FOIA) 요청에도 불응, 옴부즈맨의 maladministration 판정에도 EU 집행위는 상업 이익을 보호한다며 비협조적, 추가 공개된 문서도 Thorn 소프트웨어 신뢰성에 대한 실질 정보 없음, 결과적으로 신뢰할 수 없는 상태
          + Europol 일부 인원이 Thorn으로 이직, 그 과정에서 이해충돌 규정 위반 사례도 발견
               o “Going Dark expert group – EU’s surveillance forge” 관련 기사(링크)에 따르면, EU 집행위가 EUGoingDark 회의 참가자 명단을 지속적으로 은폐, 경찰과 정보기관 위주로 구성, EU 데이터 보호 감독관(EDPS)은 관찰자 자격만, NGO는 회의 참여 불가, 데이터 보호·인권 논의를 제한하고 EU 의회 영향력을 위한 타겟 감시 홍보 계획
               o 불행히도, 덴마크가 EU 평의회 의장국이 될 예정, 채팅 통제의 주도국 역할
               o Thorn 자료 신뢰성 의심에 대해, 역사적으로 감시를 통한 통제에 효과적이었던 북한, 중국, 소련 사례를 듦
     * ‘Secret democracy’라는 표현으로, 오히려 시민을 불신하고 시민을 위한 것이 아닌 민주주의라는 냉소 섞인 시각, 제안자 조사에 탁월한 von der Leyen의 과거 행적 상 이 같은 비밀주의적 태도와 부합한다고 추정
     * 과거 및 타국에서 유사법안이 반복적으로 불발될 경우 다시 추진이 불가능하도록 제한하는 전례가 있었는지 궁금, ‘4회 도전 후 다시 상정 금지’나 부결된 법안의 핵심이 10년간 재상정 불가 등 제도적 견제장치 필요성 제안
     * 전세계 어디를 보더라도 오웰적 감시 드라이브가 없는 곳이 있는지 의문
          + 저개발국 국민이 오히려 더 자유로운 경우도 있고, 해당 정부가 이 같은 감시 법안을 시도하더라도 인프라나 다국적 기업을 움직일 힘이 부족
          + Dennett의 “Consciousness Explained”에서 오웰식(사후 왜곡)과 스탈린식(사전 차단) 감시 패러다임을 설명하며, 현재는 두 디스토피아가 동시에 도래할 수 있다는 우려, 나라에 따라 스탈린 방식이 심화될 위험
          + 미국 테크 기업 도입 시 이미 감시 루트가 내장(관련 링크), 애플도 정부에 푸시 알림 데이터 제공 사실 인정 사례
          + 트럼프식 권위주의가 탐탁지 않다면서, 반대로 가지 않고 병렬 노선으로 권위주의 제도 도입하는 현상
          + 극단적 자본 집중이 이런 감시 사회를 촉진한 원인이라는 의견
     * EU 집행위가 대규모 감시 제안의 진짜 조율자를 공개 거부, 사실상 시민의 온라인 프라이버시 종결
          + 시민들이 왜 프라이버시를 원하는가라는 질문에, 사실 EU 집행위가 각종 활동에 대해 비밀 유지를 원한다는 점도 같은 동기라는 점 지적
     * 2026년 시행될 EU Digital Identity Wallet(링크), EU Age Verification(링크) 같은 다른 계획들과 감시 제안의 연동 방식 궁금
          + 이 시스템들은 상호 보완적, 대형 웹사이트와 앱이 연령 확인 제공 기능 의무화, 온라인 신원이 실제 신원과 연결되어 정부 감시에 프라이버시나 익명성 상실
     * 극우세력 부상에 대한 놀라움 표현에 대해, 네덜란드 Geert 사례처럼 결국 힘을 잃을 것이라는 낙관적 태도
          + 극단주의자의 감시 도구 사용이 위험하다는 건 사실이지만, 인구 감시에 대한 열망은 정치 스펙트럼 전반에 걸쳐 존재, 네덜란드 D’66(중도·진보 정당)이 정보기관을 위한 대규모 감시법에 찬성했던 사례
          + 노르웨이 주요 정당(Arbeiderpartiet/Høyre)이 소셜미디어 접속에 국가 BankID 로그인 연령제한 추진, 이는 온라인 프라이버시 침해, 그리고 2011년 국가가 6개월간 국제 인터넷 트래픽 전부 보관 허용 법안에도 찬성, 어떤 정당도 프라이빗 커뮤니케이션 보호 의지 부족
          + Ursula von der Leyen이 극우냐는 질문에, 실제로 이번 감시 그룹은 그녀와 그녀의 집행위 주도로, 그룹 멤버 미공개 결정 역시 집행위의 것
          + 현실에서 중도파가 극우 집권 시도구 보강으로 활용될 위험성을 충분히 고려하지 않는다는 지적
          + 극우가 이런 감시 정책을 밀어붙이고 있다고 생각하지 않음, 오히려 일반적으로 반EU 성향이고, 이런 조치들은 중도/신자유주의 체제의 권력 강화 목적, 반대로 극우는 이를 공격 수단으로 활용
     * EU 시민이라면 대표자에게 직접 항의 연락할 것 추천
          + 실제로 네덜란드 정치인에게 연락해도, 답변도, 연결도, 소통도 되지 않는 경험, 정치인들이 스스로를 닿기 어렵게 만든 현실
          + 덴마크 MEP에 연락 시도했지만 답변조차 없었던 경험
          + 대표자들은 시민이 기대하는 것만큼 관심 가지지 않는 현실
          + 저작권 지침 관련 MEP에게 연락해봤지만, EU 집행위에서 내려온 이야기만 반복, EU 의회 대표성은 희석되고 간접적, 새 법안을 시작할 수 있는 것은 선출되지 않은 집행위/평의회 인사 뿐이며, 의회가 반대해도 집행위가 표지만 바꿔 계속 재투표, MEP들은 대체로 현 체제에 순응
          + 이메일을 보내도 그냥 웃고 넘길 것, 전화를 받을 일은 기대조차 할 수 없는 실정
"
"https://news.hada.io/topic?id=21244","50대 1인 개발자 살아남기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            50대 1인 개발자 살아남기

     * 개인사업자로 창립 2주년, 1인 개발자 독립 5주년을 맞이해 살아남으며 느낀 점을 정리

1. 성실

     * 재택 개발자에게 가장 중요한 자질은 성실함
     * 클라이언트에게 신뢰를 주는 유일한 방법은 약속된 결과물과 일정 준수
     * 집에서 일하다 보면 집중이 어렵지만, 자기 스스로 납득할 업무량을 꾸준히 지키는 것이 중요함

2. 인맥

     * 한때 인맥이 전부라고 생각했으나, 5년이 지나 보니 성실을 통한 신뢰 쌓기가 더 핵심임
     * 인맥은 새로운 일의 연결고리 역할을 하며, 신뢰 기반의 추천이 장기적으로 일감을 이어주는 구조를 만듦

3. 전략

     * 1인 개발자의 핵심 생존 전략은 ""꾸준한 수입"" 과 ""일의 지속성"" 확보
     * 이를 해결하기 위해, 개발비를 일시불로 받지 않고 월 단위 유지보수 계약으로 유도
     * 개발자 입장에서는 고정 수입과 심리적 안정이 생기고, 클라이언트는 부담이 덜하며 지속적 업그레이드를 받는 이점이 있음
     * 1년간 신뢰를 쌓으면 재계약 혹은 추가 업무로 이어지는 선순환이 발생
     * 이 방식이 지난 5년간 성공적으로 작동중

4. AI

     * AI 도입은 반복적인 일상에 새로운 활력이 되었음
     * 코딩이 일로만 느껴져 매너리즘에 빠졌으나, AI로 인해 개발의 재미와 효율성이 커졌음
     * 특히 전문 분야가 아닌 영역(CSS 등) 도 AI로 막힘없이 처리 가능해짐
     * AI는 결국 도구(tool) 로, 활용 능력에 따라 더 나은 환경에서 일할 수 있음

5. 그렇게 살아 남고 있습니다

     * 독립 초기에는 월급이 1/3로 줄어 경제적 불안을 겪었으나, 점진적으로 수입이 회복되어 이전 회사 수준에 도달
     * 오랫동안 함께하는 업체가 몇 군데 생겨 감사한 마음. 최근에는 신규 계약도 진행 중

6. 맺으며

     * 지난 5년을 돌아보면 운이 좋았던 순간들이 많았음
     * 일이 끊기지 않고 연속성 있게 이어진 것 역시 큰 행운
     * 인맥이 많지 않아도, 남은 몇몇 관계가 큰 힘이 되었음을 실감
     * 현직 개발자들에게는 두루두루 친하게 지낼 것을 권함

   저도 가끔씩 알바 같은게 생기는게 그게 인맥을 통한게 아니라 운이 좋아 시작하게 되고 클라이언트와 신뢰 관계가 생겨나니 꼬리에 꼬리를 물게 되더라고요

   월 단위 유지보수 계약으로 관계를 연장해나간다는 아이디어는 참으로 현명한 전략인 것 같습니다. 좋은 글 공유 감사합니다.

   좋은 글 감사합니다.
   저도 이렇게 1인 개발자로 살아가고 싶습니다.

   박수를 보냅니다!

   멋지십니다! 인맥 부분은 느낀점이 많네요!

   멋집니다 !! :)

   좋은 글 감사합니다

   30대 이지만 지금 기로에 서서 고민중인데 나름 도움이 되었습니다. 저도 인맥이 닿아서 종종 크고작은 일을 받아서 했는데 귀찮게 생각하지 않고 꾸준히 인맥 이어가야겠네요..!

   원글을 읽었었는데 감명 깊었습니다. 도전을 응원합니다!!

   멋지네요. 응원합니다~ 아자아자~
"
"https://news.hada.io/topic?id=21275","딥러닝은 주목받지만, 딥팩트체킹은 외면받음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        딥러닝은 주목받지만, 딥팩트체킹은 외면받음

     * 딥러닝 모델을 활용한 효소 기능 예측 논문은 큰 주목과 인용을 받는 반면, 팩트체크를 통해 오류를 지적한 논문은 거의 관심을 받지 못함
     * Transformer 기반 모델로 22백만 개 효소 데이터를 학습해 450개 미확인 효소 기능을 예측한 연구가 Nature Communications에 실림
     * 그러나 수백 건의 잘못된 예측과 데이터 중복, 생물학적으로 불가능한 결론 등이 후속 논문에서 드러남
     * 전문가의 심층 분석 없이는 AI 결과의 신뢰성 평가가 어렵고, 잘못된 데이터가 계속 전파될 위험이 커짐
     * 화려한 AI 모델보다 근본적인 데이터 검증과 도메인 지식 통합의 중요성이 재조명됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Deep learning gets the glory, deep fact checking gets ignored

     * 딥러닝 기반 효소 기능 예측 논문은 22백만 개의 효소 데이터를 학습해 Transformer 모델로 450개 미확인 효소 기능을 예측하고, Nature Communications에 등재되어 큰 관심을 받음
     * 그러나 후속 논문에서는 잘못된 예측 수백 건과 기존 데이터베이스와의 중복, 생물학적으로 불가능한 결과, 반복적인 오류 등이 다수 발견됨
     * 예를 들어 E. coli의 특정 유전자 기능을 AI가 잘못 예측했고, 이미 기존 연구에서 그 기능이 아님이 밝혀졌음에도 모델이 오류를 범했음
     * 이와 같은 오류를 밝혀낸 논문은 bioRxiv에 등재되었으나, 조회수와 인용이 미미해 출판 인센티브 구조의 문제점이 드러남
     * 전문가들은 AI 모델링보다 데이터 검증과 도메인 전문성의 필요성을 강조하며, 잘못된 정보가 데이터베이스를 통해 재전파되는 위험을 경고함

The Problem of Determining Enzyme Function

     * 효소는 생명체 내에서 중요한 반응을 촉진하는 역할을 하며, Enzyme Commission(EC) 번호로 다양한 기능이 분류됨
     * 아미노산 서열로부터 EC 번호를 예측하는 작업은 입력과 출력이 명확해 기계학습에 적합하다고 여겨짐
     * UniProt 데이터베이스에 22백만 개 이상의 효소와 EC 번호가 정리되어 있어 학습 데이터가 풍부함

An Approach with Transformers (AI model)

     * 연구진은 Transformer, 컨볼루션 레이어, 선형 레이어로 구성된 모델을 사용해 미확인 효소의 기능을 예측함
     * 주목할 만한 점은, 모델의 해석 가능성을 위해 high attention region이 생물학적으로 의미 있는지 분석함
     * 450개 미확인 효소 중 3개만 실험(in vitro) 으로 검증해 정확함을 입증했다고 주장함

The Errors

     * Nature 논문에서 발표된 450개 '새로운' 결과 중 135개는 기존 데이터베이스에도 이미 존재하는 결과였음
     * 148개 결과는 반복성이 매우 높아 동일 효소 기능이 여러 번 예측되는 등 생물학적으로 불가능한 오류가 다수 확인됨
     * E. coli가 생성하지 않는 물질의 합성 효소로 잘못 예측하거나, 기존 실험 결과와 상충하는 경우도 다수 발견됨
     * 데이터 누수(data leakage) 가능성도 제기되었으며, 실제 ground truth가 없는 영역의 예측에서 오류가 빈번하게 발생함

The Microbiology Detective

     * 후속 논문의 Dr. de Crécy-Lagard는 Nature 논문의 예측 결과 중 한 효소(yciO)가 과거 연구와 명백히 상반됨을 확인함
     * yciO와 TsaC가 진화적으로 연관 있지만, 실제 실험에서는 yciO가 TsaC의 기능을 대체하지 못함을 수차례 입증함
     * 단순 구조 유사성만으로 기능을 동일하게 보는 알고리듬 한계가 드러남
     * 효소 기능 판별에는 유전자 주변 맥락, 기질 결합, 대사 경로 등 다양한 증거를 종합적으로 고려해야 함

Hundreds of Likely Erroneous Results

     * 후속 논문팀은 Nature 논문 예측 결과 450건 중 135건이 이미 데이터베이스에 등록된 내용임을 확인함
     * 148건은 동일 기능 반복 예측 등 데이터 편향, 특성 부족, 아키텍처 한계 등으로 발생한 문제로 분석됨
     * 여러 결과는 생물학적 맥락이나 기존 문헌 조사로 오류가 입증됨

Rethinking Enzyme Classification and “True Unknowns”

     * 효소 기능 예측에는 알려진 기능 전파(propagation) 와 진짜 미지 기능 발견(discovery) 이라는 두 과제가 섞여 있음
     * Supervised ML은 진짜 미지의 기능 예측에는 본질적으로 한계가 있음
     * 잘못된 예측이 UniProt 같은 데이터베이스에 입력되고, 이 데이터로 다시 모델이 학습되는 오류의 순환 구조가 나타남

Need for Domain Expertise

     * AI 연구와 달리, 데이터 검증 및 도메인 전문가의 심층 분석은 인센티브가 부족해 연구자들의 관심을 덜 받음
     * 실제로 고위험 AI 프로젝트의 실패 원인 중 하나가 불충분한 도메인 지식 적용임이 논문에서 드러남
     * 대부분의 딥러닝 논문은 도메인 전문가의 세밀한 검증을 거치지 않으며, 겉보기에는 인상적인 논문도 실제로는 오류가 많을 수 있음

결론 및 제언

     * 화려한 AI 모델 개발보다 근본적 데이터 검증과 도메인 지식 통합의 중요성이 강조됨
     * 연구 인센티브와 지원이 실질적 검증 연구에 더 집중되어야 함을 주장함
     * 오류 검증과 데이터 품질 향상이 장기적으로 AI 발전에 더 큰 기여를 할 수 있음을 시사함

        Hacker News 의견

     * 데이터 누수 가능성이 있다는 점을 자주 잊어버리는 경향이라는 생각임. 데이터 누수는 없다고 강력한 증거가 있기 전까지 항상 있다고 가정해야 하고, 반대로 누수가 없다는 것을 입증해야 할 책임은 저자들에게 있다는 의견임. 작은 데이터셋에서는 누수가 훨씬 쉬운 문제인데, 직접 데이터를 쭉 들여다볼 수 있어서 오히려 더 쉽게 생길 수 있음. 미묘한 실수로 데이터가 망가지는 게 굉장히 흔한 현상임. 이제는 인간이 전부 검토할 수 없는 엄청나게 거대한 데이터셋이므로, 필터링이 불완전하다는 걸 다들 알고 있는데도 누수가 없다고 믿는 것이 가능하지 않음. 필터링했다 라고 말할 수는 있지만, 정말 누수가 없다고 말할 수는 없음. 우리가 실제로 접근할 수 있는 데이터셋에서도 자주 문제를 발견하고 있음. 이런 일을 계속 경험하고 있는데도 왜 계속
       데이터가 멀쩡하다고 가정하는지 모르겠음. 아마도 지나친 기대에 휩싸인 자기기만이 아닐까 생각임. 문제를 고치려면 현실을 정확히 봐야 한다는 의견임
          + 모든 시스템은 결함이 있기 마련이라는 점임. 얼만큼의 결함까지 받아들일 수 있느냐가 진짜 논점이라는 생각임. 예를 들면, Medicare와 Medicade에서 사기 사례가 7.66%였는데, 액수로는 엄청 크지만 그렇다고 시스템이 전적으로 실패했다는 건 아니고, 나머지 93%는 제대로 돌아간 것임. AI 모델도 마찬가지로, 오류율이 10%라면 시스템 전체가 나쁘다는 뜻이 아니라, 그 정도 비율은 수용 가능한지에 대한 논의가 필요하다는 입장임. 근거자료 참고
          + 증명의 책임이 어디에 있냐는 논점이 많은 사람들이 생각하는 것처럼 신념의 지침이 되지는 않는다는 의견임
     * AI가 연구를 하기 전에, 우선 기존 연구 재현부터 성공해야 한다고 생각임. 예를 들어, 딥러닝 논문을 AI에게 주고 이를 구현하게 만들면 진짜 능력을 평가 가능하다고 봄. 이런 기본이 안 되면 새로운 아이디어를 기대할 수 없는 상황임
          + 나는 처음에 ""논문의 앞부분만 AI에게 주고 나머지는 AI가 완성하게 해보자""고 제안할 줄 알았음. 만약 이 정도 검증도 아직 안 된다면, AI가 혁신적 발견을 만들어낼 수 있다고 보지 않는다는 의견임
          + OpenAI에서 이와 관련한 벤치마크를 만들었음 paperbench 링크
          + 완전히 투명하게 검증 가능한 기록 시스템을 갖추고, 논문이 미리 데이터셋에 노출된 적이 없다는 것까지 보장해야 함. 논문에서 학문적 부정행위가 드물지만 가끔 발생하기도 하고, LLM은 아무렇지 않게 거짓 정보를 생성할 수 있음
          + 예시로, 어떤 논문의 실험 통계 데이터를 AI에게 주고 원시 데이터를 재현하게 할 수도 있을 것 같음
          + 이 아이디어는 충분히 흥미로울 뿐 아니라, 재현성 검증 문제도 어느 정도 해결할 수 있을 것 같음. 다만, AI가 재현한 연구도 결국 사람이 꼼꼼하게 검토해야 한다는 점은 여전함. 현실적으로 현재 LLM이 쓸 수 있는 다양한 역할이 있는데, 예를 들면 동료 평가 과정에서 데이터 처리 코드 검증을 보조하거나, 논문 조사를 도와주고 아이디어 브레인스토밍에 활용하는 방안이 있음
     * ""Nature Communications""와 ""Nature""는 완전히 다른 위상임. 둘을 같은 대우로 부르지 않는 게 맞음. 그리고 altmetrics는 큰 의미가 없는 수치임. 공공의 열기 정도를 재려는 게 아니라면 과학적 인용과는 별 상관 없음
     * 딥러닝 논문 대다수를 보면 도메인 전문가가 결과물을 정말 세밀하게 검증하는 경우가 잘 없음. 인상적으로 보이는 논문들 중 엄격한 검증을 통과하지 못할 논문이 많지 않을까 궁금함. 하지만 실제로 내 분야 AI 논문은 내가 직접 읽는 건 물론이고, 다른 전문가들도 많이 체크하는 것으로 보임. 다만, 컴퓨터공학이나 소프트웨어 쪽 결과물은 생물학보다 검증하기 쉬운 것처럼 느껴짐(혹은 내가 바이오를 잘 몰라서 그런 느낌일 수도 있음)
          + 생물 분야에서 라벨의 유효성 검증 자체가 수년이 걸리는 경우가 많음. OP가 예시로 든 경우도 마침 누군가가 몇 년을 들여 미리 특정 예측값을 검증해둔, 굉장히 운이 좋은 예시임. 대부분은 3~5년씩 자기 커리어를 걸고 무작위 모델 예측을 검증하려 들지 않음
          + 내 분야에서는 논문에서 해당 기법을 썼을 때 사람들이 세밀하게 검토하고 비판을 내놓는 경우가 흔함. 문제는 이런 비판을 다른 분야 사람들이 진지하게 받아들이지 않는 경우가 많음
     * AI에 필요한 건 '현실 검증기' 서브시스템이라는 주장임. LLM의 경우 마치 우리의 무의식이 끊임없이 떠드는 잡음을 계속해서 내뱉는 것과 같음. 실제로 우리의 뇌는 ""내가 한 말이 반증 가능성 있는 진실인가?"" 같은 내부 필터가 있어서 거짓말을 걸러냄. (물론 이게 모두에게 통하지는 않다는 농담도 곁들임)
          + 전적으로 동의함. 몇 달 전 늦은 밤, 반쯤 잠든 상태에서 내 뇌가 끊임없이 여러 구문과 생각들을 만들어내는 걸 인지했음. 종종 이 모든 아이디어가 필터를 거쳐 문장으로 정제되는 걸 생생하게 느꼈음. 나만의 이상한 경험이지만, AI에도 이런 알고리즘이 꼭 필요하다고 느끼는 상황임. 박사 과정을 밟게 된다면 이걸 연구 주제로 삼고 싶음
          + 인간의 '현실 검증기' 시스템은 GAN에서 디스크리미네이터와 비슷하지만, 강하게 감정에 의해 영향을 받음. 심리학 연구에서 확인된 바로, 인간의 진위 판단 회로는 항상 감정적 신호부터 시작되고, 그 뿌리는 신념에서 비롯됨. 누군가 내 신념과 강하게 어긋나는 말을 할 때, 감정적 반응이 가장 먼저 찾아오고 이후에야 이성적 판단이 개입됨
     * 연구자로서 LLM을 접한 경험과 부합함. 텍스트 이해와 생성 능력에는 깊이 감탄했지만, 훨씬 더 어려운 미해결 문제에서 순식간에 답을 내놓는 모습은 늘 아쉬움으로 남음. 복잡한 질문은 시간을 두고 고민해야 하는데, LLM은 이런 깊이나 고민 없이 자신감 넘치게(전혀 틀린 답이라도) 답을 던지는 경향임
     * Rachel Thomas의 멋진 기사라는 소감임. 딥러닝은 결국 [생성형] 정보 검색 도구라는 주장을 다시 확인시켜 주는 사례임. 훈련 데이터는 현실의 도메인을 반영하긴 하지만, 본질적으로 매우 손실이 큰 데이터셋임. 예를 들어 유전자 데이터/라벨이 생물학의 실제 구조를 완벽하게 대변하지는 못하므로, 결과 역시 종종 잘못되거나 말이 안 될 수도 있음. 오히려 이상하게 너무 잘 맞을 때는 설계상 정보 검색 툴(LLM) 특성상 데이터 누수가 섞였을 가능성도 언급함. 정보 이론적으로 볼 때, 데이터셋의 한계는 모든 모델에 공통된 미지의 위험 요소임. 결론적으로, 알고리즘의 결함이 아니라 훈련 데이터셋의 문제라는 생각임. 우리는 자연어라는 도메인 내에서 워낙 유연하게 작동하고, 어린아이도 글을 읽으면 말이 되는지 판단이 가능함. NLP 분야에서 LLM이 성공하는
       건 이러한 데이터 덕분임. 반면, 원천 데이터가 본질을 충실하게 담지 못하는 복잡한 분야에서는 더 많은 한계가 있음
     * 허위 정보가 과학에도 스며들고 있다는 우려임. 근거 없는 자극적 발언이 진짜 연구 뒷받침보다 더 많은 관심을 받는 모습이 소셜 미디어 현실과 비슷하게 과학에도 나타나고 있다 지적임. 하지만 트위터와 Nature 저널을 같은 선상에서 볼 순 없고, 명망 높은 학술지와 동료 평가 시스템이 이런 문제를 막아주는 '마지막 보루'라는 신뢰가 있었음. 그렇다면 이번 사태는 Nature의 실패가 맞는지에 대한 의문임
          + 임팩트 큰 학술지일수록 논문 철회 및 검증되지 않은 비율이 더 높다는 통계가 있는 점을 상기해야 한다는 조언임. 이런 문제의 근본적 원인은 논쟁적이지만, 한 논문이 진리를 증명하는 건 아니고, 다양한 연구기관, 여러 연구진이 독립적으로 결과를 검증하는 게 진짜 신뢰 기준임
          + 과학계의 허위 정보 문제가 이제 막 커진 건 아니라 몇 년 전부터 '재현성 위기' 논란이 계속되고 있는 현실임
          + ML Quantum Wormhole 논문 사례처럼, 잘못된 연구가 대중 과학기사를 넘어서 저명 학술지에도 실리고 있다는 실망감임. 실수라기보단 연구자와 리뷰어 모두가 제대로 된 검증을 생략한 사례가 너무 많다는 의견임. 개인적으로 기존 학술지 체계에 회의적이었고 자유로운 학술 출판을 바랐지만, 이제는 오히려 학술지 자체가 자기 스스로 신뢰를 깎아먹고 있는 모습임. 하지만 이런 일이 과학에 대한 대중의 신뢰를 악화시키는 데 결국 기여하게 된다는 점이 제일 걱정임. 과학 내부의 미묘한 논쟁을 대중이 알아차리기도 힘들고, 이런 일들이 반과학 진영에 또 다른 빌미를 제공할 뿐임
          + Bullshit asymmetry principle(Brandolini의 법칙)이 떠오른다는 말임 이 원칙 링크
     * 우리는 아름답게 성공했던 단 한 번의 ML/AI 사례만 극적으로 홍보하고, 실패했던 수십 번의 시도를 외면하게 되는 경향이 있음
          + 나아가 '딥 스토캐스틱 패러팅(deep stochastic parroting)'도 사랑하는 것 같음. 직접 겪어본 경험이나 꾸준히 쌓이는 증거, 논리적 추론을 외면하고, LLM의 명확한 효용을 일부러 부정하려 함. 그리고 그 부정 입장에는 늘 뻔한 유행어들로 근거를 덧붙임
"
"https://news.hada.io/topic?id=21256","Cloudflare가 Claude와 함께 OAuth를 빌드하고 모든 프롬프트를 공개함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Cloudflare가 Claude와 함께 OAuth를 빌드하고 모든 프롬프트를 공개함

     * Cloudflare가 OAuth 2.1 프로바이더 프레임워크를 Cloudflare Workers용 TypeScript 라이브러리로 발표함
     * 대부분의 구현은 Anthropic의 Claude LLM을 이용해 작성되고, Cloudflare 보안 엔지니어가 꼼꼼하게 검토함
     * 이 라이브러리는 API 엔드포인트를 위한 인증 자동화와 토큰 관리 자동 처리를 제공
     * 최신 OAuth 표준과 PKCE, 동적 클라이언트 등록, 접근 범위 설정 등 주요 기능을 지원
     * 중요한 구간마다 엔드 투 엔드 암호화와 싱글-유스 리프레시 토큰 등 보안 설계를 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cloudflare Workers를 위한 OAuth 2.1 프로바이더 프레임워크의 소개 및 중요성

   이 오픈소스 프로젝트는 Cloudflare Workers 환경에서 쉽게 OAuth 2.1 인증 서버를 구현할 수 있도록 설계된 TypeScript 라이브러리임.
   동종 타 프로젝트와 비교시, Cloudflare 플랫폼에 특화된 확장성과 원활한 통합, 그리고 보안 중심의 설계에서 강점이 큼
   알고리듬, 프로토콜 표준 준수(RFC-8414, RFC-7591 등), 라이브러리 구조의 명확성 등에 중점이 맞춰져 있음
   특히, 토큰 및 주요 시크릿 값의 해시화 저장, props의 엔드 투 엔드 암호화 등 안전성을 위해 세부적으로 설계됨
   참고로, 라이브러리 코어 코드와 문서 대부분이 Claude LLM의 협력으로 작성되어 흥미로운 개발사례를 보여줌

주요 기능 및 장점

     * OAuthProvider로 Worker 코드를 래핑하여 API 엔드포인트에 대한 인증 기능 자동 부여
     * 토큰 관리(생성, 저장, 검증, 폐기 등)가 자동 처리되어 직접 구현이 필요하지 않음
     * 사용자는 fetch 핸들러 안에서 이미 인증된 사용자 정보를 바로 파라미터로 받아 활용 가능함
     * 사용자 관리 및 인증, UI 구현 방식에 대한 제한 없음 (개발자가 원하는 구조와 프레임워크 자유롭게 선택 가능)
     * 라이브러리의 저장소에는 오직 해시 정보만 저장되고, 비밀키 등 시크릿 자체는 저장되지 않음

사용 방법 핵심

     * OAuthProvider 인스턴스를 Worker 엔트리포인트로 내보내서 fetch 핸들러 역할을 수행할 수 있음
     * apiRoute, apiHandler 옵션으로 인증이 필요한 API 엔드포인트 구간과 실제 핸들러를 각각 지정함
     * authorizeEndpoint, tokenEndpoint, clientRegistrationEndpoint 등 각 표준 OAuth 엔드포인트 경로를 정의 가능함
     * 필요한 경우 scope나 public client registration 등 정책 세분화 가능함
     * defaultHandler를 통해 API 외 요청 및 인증 실패 요청도 유연하게 처리 가능함

헬퍼 객체 및 API

     * env.OAUTH_PROVIDER를 fetch 핸들러에서 사용하여, OAuth 관련 요청 파싱, 클라이언트 정보 조회, 승인 완료 등에 활용 가능함
     * API 요청처리에서, access token이 유효한 경우 권한이 부여된 상태의 사용자별 props 정보를 ctx.props로 바로 접근 가능함
     * 클라이언트 등록, 권한(authorization grant) 리스트, 특정 사용자에 대한 grant 열람 및 폐기 등도 공식 API로 제공함

Token Exchange Callback

     * 토큰 발급·갱신 시, props 값을 동적으로 업데이트할 수 있는 콜백(tokenExchangeCallback) 지원
     * OAuth 클라이언트와 연동된 추가 토큰 교환(upstream token exchange) 등 복합 시나리오 지원 가능함
     * 콜백에서 accessTokenProps, newProps, accessTokenTTL 등을 반환하여 맞춤형 동작 구현 가능함
     * accessTokenTTL 커스터마이징을 통해, 상위 OAuth 시스템과 토큰 만료 타이밍을 맞출 수 있음
     * props에는 민감 정보가 담길 수 있어 이 값 전체가 엔드 투 엔드로 암호화되어 저장됨

커스텀 에러 응답

     * onError 옵션을 사용해 에러 발생 시 Notfication 전송, 커스텀 로깅 등 대외 조치 가능함
     * 반환 Response를 직접 정의하면, OAuthProvider가 유저에게 제공하는 에러 응답을 원하는 형태로 오버라이드 가능함

보안 관련 상세 설계

  엔드 투 엔드 암호화

     * 토큰 관련 데이터 및 시크릿은 해시로만 저장되고, grant props 정보는 토큰 자체로 암호화 저장하여 유출 사고 대응력을 높임
     * userId, metadata 등은 감사(audit) 및 폐기 용도 기록을 위해 암호화하지 않으며, 필요시 개발자가 추가 암호화 적용 가능함

  싱글-유스 리프레시 토큰 설계

     * OAuth 2.1 요구대로 ""클라이언트 바인딩 or 싱글 유스"" 조건 중, 이 라이브러리는 최대 2개 병렬 리프레시 토큰 허용이라는 절충 설계 사용
     * 이를 통해 네트워크 장애 등으로 새 토큰 저장이 실패해도 이전 토큰을 한 번 더 사용할 수 있는 안전장치 제공함

Claude 기반 개발 과정

     * 본 라이브러리와 문서 다수는 Anthropic Claude LLM을 활용해 초안이 만들어졌으며, Cloudflare 엔지니어가 RFC 및 보안 기준에 맞춰 꼼꼼하게 리뷰함
     * 초기에는 AI 코드 생성에 회의적이었으나, 실제 품질과 생산성 향상 경험을 통해 시각이 변화하였음
     * 개발 흐름과 Claude의 프롬프트, 코드 개선 과정을 깃 커밋 히스토리에서 모두 투명하게 공개함

기타 사항

     * Workers KV 네임스페이스(OAUTH_KV) 바인딩 필수 적용, storage-schema.md 참고
     * 전체 API 및 헬퍼는 OAuthHelpers 인터페이스 정의 참고
     * 라이브러리는 현재 베타/프리릴리스 단계이며, 향후 API 변경 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

        Hacker News 의견

     * Readme에서 발췌 내용 소개. 이 라이브러리(그리고 스키마 문서)는 대부분 Anthropic의 AI 모델 Claude의 도움을 받아 작성함. Cloudflare 엔지니어들이 철저하게 검토하고 보안 및 표준 준수에 신경 씀. 초기 산출물에서도 많은 개선이 이루어졌고, 주로 Claude에게 추가 프롬프트를 주고 결과를 반복적으로 검토하면서 진행함. 커밋 히스토리에서 Claude를 어떻게 프롬프트했고, 어떤 코드가 나왔는지 실제로 직접 확인 가능함. 처음에는 “LLM이 인증 라이브러리를 작성하게 두면 안 된다”는 전통적인 회의적 입장이었음. 2025년 1월만 해도 나 역시 AI에 매우 회의적이었고, LLM은 그저 ‘화려한 Markov 체인 생성기’ 정도에 불과하며, 참신함이나 실제 코드를 만들어내지 못한다고 생각함. 프로젝트를 재미 삼아 시작했지만, 생각보다 괜찮은 코드가 나와서 충격을 받음.
       완벽하지는 않았지만 AI에게 수정 요청만 하면 제대로 고쳐줬음. 한 줄 한 줄 다 각종 RFC와 비교해보며, 보안 전문가들과 함께 검토함. 내 회의론을 검증하려고 했으나, 오히려 스스로 틀렸음을 증명한 결과임. 커밋 이력, 특히 초기 커밋을 꼭 참고해 보면 이 과정을 확인할 수 있음
          + 숙련된 엔지니어가 적절하게 프롬프트를 했을 때 LLM이 이런 코드 정도는 충분히 만들어낼 수 있으리라 기대함. OAuth는 새로운 기술이 아니고 이미 수많은 오픈소스 예제와 다양한 언어의 구현 케이스가 데이터로 쓰였을 것임. 하지만 LLM이 전혀 새로운 연구, 소재 과학, 경제, 발명 등에 혁신적으로 기여할 거라는 주장에는 여전히 회의적임. ‘실시간으로 배우는 능력’이 정말 필요한 영역인데, 현존 LLM은 이미 아는 오래된 정보를 바탕으로만 능력을 보여왔음. 유의미한 성과는 대개 아주 제한된 환경 내 cherry-pick 사례에 불과함. 숙련된 엔지니어가 있다면 과거 데이터를 바탕으로 새로운 코드 생성이 당연히 가능하다고 생각하지만, LLM 도입이 가져오는 환경적, 사회적 부담이 실제 효율성에 비해 과도하다고 여전히 의문임
          + “나는 2025년 1월까지 (@kentonv)와 같은 생각을 가지고 있었다”라는 표현에 혼란을 느낌. kentonv가 다른 사용자이기 때문임. 이게 본인 부계정인가, 아니면 오타나 오해인지 궁금함. 편집: 대부분의 글이 README 인용인 걸 확인함. >와 * 기호를 사용해서 인용을 명확히 했으면 혼란이 덜했으리라 생각함. kentonv 프로필 링크 첨부
          + ""LLM을 화려한 Markov 체인 생성기라 생각했다""와 ""AI가 꽤 괜찮은 코드를 만들어내 놀랐다"" 이 두 의견은 서로 모순이 아님. LLM이 굉장히 유용하다고 느끼지만, 여전히 본질은 패턴 생성기에 가까움. 중요한 점은 그 정도가 이미 충분하고, 인간 역시 크게 다르지 않은 구조일 수도 있다는 사실임
          + 요즘은 pro-AI 보다는 더 회의적인 입장이지만, 어쨌든 워크플로우에서 AI 도입하려 계속 시도 중임. 실제로는 설명이 더 어려워서 그냥 직접 하는 게 더 쉽다고 느낌. 큰 재미도 없지만, 이게 “미래”의 도구이니 익히는 게 현명하다고 생각함. 아직은 진짜 AI 툴링은 초창기 수준이라 봄. 앞으로 신기한 UX 사례 등장에 계속 관심이 생김
          + 초기에 Claude가 어떻게 프롬프트 받았고, 실제 코드를 어떻게 생성했는지 커밋 히스토리 직접 참고 안내. 최초 커밋 페이지로 바로가기 링크 제공. 명확하고 구체적인 지시가 많았고, 예시 커밋1, 예시 커밋2 등도 볼만함
     * Cloudflare workers-oauth-provider 커밋 중 이슈 커밋 발췌. Claude가 이전 커밋에서 버그를 내서 여러 번 프롬프트로 수정하려 했으나 계속 실패함. 결국 인간이 직접 수정하고, README에 OAuth 2.1 스펙 이슈까지 문서화함. 이런 경험이 나 개인적으로도 AI 활용에서 매우 공감됨. 절반까지 잘 가다가 나머지는 힘들어하는 모습을 종종 봄
          + AI가 중간까지는 잘 해내지만, 한 번 실수하면 이후 전부 망가진다는 점에 주목함. 실수 발견 시 즉시 대화 처음부터 프롬프트를 재작성해서 시작해야 함. 한 번 실수한 이후엔 아무리 바로잡으려 해도 결과가 계속 잘못됨. 그래서 올바른 시작 메시지에 모든 걸 명확하게 담아 다시 처음부터 만들어야 실수를 반복하지 않음
          + 이런 문제를 완화하려면 테스트나 명확한 명세(spec)를 마련해 AI에게 해결하도록 하면 도움됨. 몇 달 전만 해도 AI가 이런 명세 퍼즐을 푸는 데 시간이 많이 걸렸고, 결과물도 빠른 답변보다 품질이 낮았음. 하지만 최근엔 모델 성능이 많이 좋아져서 이러한 작업이 꽤 재미있고, 스펙화 할 수 있는 경우 활용도가 올라감. 나 개인적으로는 sonnet 3.7이 3.5보다 큰 발전을 보였고, Gemini 2.5 Pro가 더 인상 깊었음. sonnet 4는 실수가 더 적지만, 여전히 소프트웨어 엔지니어링 관점(요구사항 정리, 기술 솔루션 탐색, 아키텍처 설계, 유저 스토리 및 명세 작성, 코드 작성)에서 AI를 제대로 유도해 줘야 양질의 결과물을 얻을 수 있음. 추가로, 좋은 예제를 AI에 추가해 넣으면 효과가 극대화됨. 최근 OpenAI Realtime API로 앱을 만들 때도 초반엔 완전 실패했지만, 문서 두
            페이지와 데모 프로젝트 하나를 워크스페이스에 추가하고 나니 바로 원하는 결과가 나옴(내 경우엔 API를 다르게 써야 했음에도 잘 맞음)
          + 커밋 163~172줄 언급 내용 중 명백히 사실과 다르거나 A/S(인증 서버) 구현별로 해석이 다른 주장들 발견. Auth0의 인증 서버는 네트워크 조건을 감안한 “leeway” 설정이 있지만, 일부 인증 서버는 훨씬 엄격함. 각 구현체마다 세부 설계가 달라서, 표준(RFC)와 공개된 오픈소스만으로 LLM이 안전하게 구현할 확률은, 결국 직접 만드는 것과 맞먹는 수준의 인간의 감수가 필요하다고 생각함
          + LLM 기반 도구가 실제로 투입 인력을 절약해줄 수 있는지, 아니면 생산 착시만 주는지에 대한 장기적 연구 결과가 궁금함
          + 이런 경험에서 AI 도구들이 실제로 이해하고 있는 것이 아니라, 거대한 패턴 데이터 모음을 바탕으로 우연적(emergent) 출력을 만들어내는 것이라 봄
     * AI 코딩의 미래는 소프트웨어 엔지니어가 사라지고, 비즈니스맨이 버튼 누르면 모든 게 끝나는 LinkedIn/ X류 판타지와 달리, 숙련 엔지니어들이 AI로 코드 일부를 만들고 이를 꼼꼼히 검토, 테스트하는 방향일 것임. 진짜 중요한 질문은, “kentonv가 AI 없이 혼자 이 라이브러리를 더 빠르게 만들 수 있었을까?”라는 점임
          + AI와 함께 라이브러리를 만드는 데 며칠 걸림. 직접 손수 만들면 몇 주, 어쩌면 몇 달 걸렸을 거라 추정함. 다만, 여기서는 매우 이상적인 사례임. 명확한 표준, 명확한 API 스펙 기반에, 이미 잘 알려진 플랫폼이라는 조건이 있어 가능했던 것임. AI로 Workers Runtime 자체를 바꾸려 했을 땐 시간 절감 효과가 별로였음. 그러나 내가 익숙하지 않은 다른 사람 코드베이스엔 AI가 정말 큰 도움이 됨. 이제는 낯선 복잡한 코드 탐험실에도 진입이 편리해졌고, 과거엔 그런 걸 피했다면 지금은 스스로 필요한 변경을 쉽게 할 수 있음
          + AI 없이 혼자 직접 만들었으면 더 빨랐을까란 질문엔 확실히 아니라고 생각함. 지금 우리가 가진 도구와, 계속 나아지고 있는 활용 노하우로 볼 때, 앞으로 3-6개월 내에는 AI로 직접 새 솔루션을 코딩하는 게 더 빨라질 전망임. 다만 잘 문서화되고, 구조가 명확하며, 빠른 피드백 루프(좋은 린트/유닛 테스트)가 갖춰진 코드베이스 장비가 중요하다고 생각함. 우리는 그쪽으로 가는 중임
          + 경험상 AI가 일부 코드를 만들어주면 꼼꼼히 리뷰, 테스트해야 하는데 이 과정이 오히려 내가 직접 코딩하는 것보다 <i>느림</i>. 그래서 지금 단계에선 AI가 큰 도움이 안 됨. 잘못된 도우미가 없느니만 못하다는 속담처럼, 아직 AI는 “나쁜 도우미”인 셈임
          + “경험 많은 엔지니어가 AI로 코드 일부를 만들고 꼼꼼히 테스팅하는 구조”라면, 결국 20명의 kentonv 대신 2명만 있으면 충분한 것 아닌가란 고민 생김. 나머지 일할 사람 18명을 위한 일거리가 계속 생길까? 저자 사례는 기술적으로 어려운 프로젝트지만, 밋밋한 LoB(업무용) 앱 개발에는 어떤 변화가 있을지 궁금함
          + 경험 많은 엔지니어가 꼼꼼히 리뷰 및 테스트만 한다면, 주니어 개발자 자리를 전부 AI로 대체했을 경우 경력 개발자가 어디서 생길지 물음. 단순 반복 작업에도 그만한 가치를 두는 입장임
     * 이런 다양한 논점과 의견들을 보는 것이 그 자체로 신기함. Cloudflare가 인터넷 보안 분야 리더답게 새로운 ‘vibe 코딩’ 방식으로 세상을 연결하는 시도를 보여줌에 감사함. 이러한 AI 프롬프트, 코드 등이 다른 개발자들에게도 프로그래밍 탐구 의지를 높여줌을 느꼈음. vibe programming이 내 우울감을 돌파하고, 내가 익숙한 방식으로 코딩하게 해준 매우 의미 있는 수단임. 이런 경험이 다른 사람에게도 도움이 되기를 바람. 현재와 미래 세대 모두 이런 방식을 활용할 것으로 기대함. 다만, 이 방식이 사람의 정신 건강이나 심리 문제와도 연결될 수 있다는 점을 논의해야 한다고 생각함. 결국 이러한 도구는 인간의 조력수단임을 유념하면서, 우리가 열정을 갖고 성장할 수 있도록 어떻게 활용할지 고민함. 오픈소스 분야에서 기술 역량뿐 아니라, 논리와 사려 깊은
       프로젝트 접근법을 보여줄 사례들이 많이 나오길 기대함. Cloudflare에 다시 한 번 칭찬을 보냄
     * 대표적인 프롬프트 주고받기 예시를 모아둠. 프롬프트 트랜스크립트 예시에서는 실제 비용(총 $6.45)까지 공개함. 관련 커밋1, 커밋2 등 자료도 안내함. AI 워크플로우에 대한 제대로 된 경험기(특히 숙련자들)의 정보가 하이프에 묻혀 별로 없다는 게 놀라움. todo list 외에 실제 라이브 코딩 사례 정보가 궁금한 상황임. antirez, tptacek의 글(antirez 사례, tptacek 글)도 참고하면 좋을 듯함
          + 정확히는 추적하지 않았지만 Claude 사용 비용은 대략 $50 정도라 추정함. 절약된 시간에 비하면 의미 없는 비용 수준임
     * 나는 “vibecoding”이 결국 생존하지 못할 거라 봄. 여전히 뛰어난 프로그래머의 검증, 버그 수정이 필요하며, AI가 못 고치는 버그도 있었음. 결국 이런 도구는 자신이 이미 해당 작업을 잘 알고 있는 사람이 속도를 높일 수 있는 보조도구임. 정말 기초적인 홈페이지 정도는 몰라도, 그런 걸 자동 생성해 주는 툴은 수년 전부터 존재함
          + vibecoding은 지금은 stakes가 낮은 작업에 최적임. GUI, CRUD 앱, 일회성 실험 등 파워 없는 이들에게 새로운 힘을 주는 부분에서 쓸모가 높음. 이번 건은 vibecoding이 아니라 LLM-assisted coding이라 생각함
          + 실제로는 vibecoding이라는 말을 공격용 허수아비처럼 사용하는 경향이 느껴짐. 대부분 LLM에게 맡긴 코드를 약간만 고쳐서 쓰는 것 역시 vibe coding이라 보면 되는 거 아닌지?
     * OP가 AI로 생성된 코드 뿐 아니라 프롬프트까지 공개해준 점이 크게 고마움. 개인적으로 LLM으로 비웹 코드(특히 요즘엔 SAP ABAP 역공학해서 데이터를 Snowflake에 맞추는 .NET 구현)를 개발하려다 매번 ‘환각’ 이슈에 고생함. 다른 사람들은 성공 사례가 많다 하길래 내 프롬프트만 문제라 생각했는데, 이번 공개된 사례를 보니 나도 그리 멀지 않다는 걸 알게 됨. LLM이 내게 잘 안 맞는 이유는 만든 문제가 조금 희귀하고 새롭기 때문이라 판단함. GitHub에 공개된 OAuth 케이스처럼 많이 학습된 대상이 아니면 LLM이 잘 못 따라옴
     * 이런 종류의 반복적이고 이미 수백 번 구현된 코드는 AI에 완벽히 적합함. 전체 코드 1200줄 정도라 작은 규모임. AI 쓰고도 2일 이상 걸렸다는 게 오히려 의외임
     * 최근 몇 달간 Cursor로 Claude를 이용해 나만의 greenfield 프로젝트를 진행하며 느낀 점은 첫째, 엄청나게 생산성이 높아졌음. 둘째, 이전보다 정신적으로 훨씬 더 피로함. 셋째, 단기간 내에도 도구들이 상당히 빨리 발전하기 시작하며, 위 두 가지 효과도 증폭되고 있음
          + 내 경험 및 다른 개발자들과도 똑같음. LLM assisted coding은 실제로 생산성 향상에 큰 도움이 되지만, 에너지 소모가 그만큼 더 큼. 오히려 이상하게 느껴질 정도임
          + “이렇게 더 피곤하다”는 점에 질문함. 난 기존 방식보다 내 에이전트(Devstral)와 “페어 프로그래밍” 형태로 활용하고 있고, 코드 전체를 일일이 타이핑하는 것보다 리뷰가 훨씬 쉬움. time wise 관점에서 큰 장점임. vibe coding에선 코드베이스 맥락이 사라져 나중에 리뷰와 진입 장벽이 훨씬 커짐. 반면 페어 프로그래밍에선 맥락을 쌓으면서 진행되니 리뷰가 훨씬 빠르다고 느낌
          + 나는 오히려 정반대임. AI가 사소한 디테일을 전부 알아서 챙겨주니까 오히려 큰 안도감 생김. 아키텍처 등 상위 목표에 집중할 수 있게 자유로워짐
     * Cloudflare 같은 회사에서 ""Too Many Requests"" 에러가 나는 게 꽤 유쾌하게 느껴짐
          + 나도 마찬가지임
"
"https://news.hada.io/topic?id=21208","구글의 kernelCTF PoW를 AVX512로 이긴 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    구글의 kernelCTF PoW를 AVX512로 이긴 방법

     * 팀 Crusaders of Rust가 Linux 패킷 스케줄러의 use-after-free 버그를 발견하고 익스플로잇을 개발함
     * Google kernelCTF 대회에 빠른 PoW(Proof of Work) 해법의 필요성으로 인해 고성능 최적화를 시도함
     * AVX512IFMA 명령어를 활용한 자체 어셈블리 및 SIMD 최적화로 기존 Python/GMP 및 Rust 구현 대비 7배 가까운 속도 성능 달성함
     * 동작 원리, 모듈러 연산, 메모리 관리, 레지스터 활용까지 세밀하게 튜닝하여 0.21초까지 PoW 처리 시간을 단축함
     * 최종적으로 사상 최단 시간(3.6초)으로 kernelCTF에 성공적 제출 후, PoW 정책이 공식적으로 폐지됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 취지

     * 2025년 5월, Crusaders of Rust 팀의 William Liu와 Savy Dicanosa는 Linux의 use-after-free 취약점을 찾아 Google의 kernelCTF 대회에 참가했음
     * 이 글은 PoW(Proof of Work) 검증을 빠르게 해결하여, 제한된 bounty 경쟁에서 다른 글로벌 팀보다 빠르게 제출하기 위한 최적화 과정을 다루는 내용임

kernelCTF 제출 과정 및 경쟁 배경

     * kernelCTF는 큰 현상금 때문에 전 세계의 전문 보안팀들이 제출 자동화 및 PoW 최적화에 사활을 걸고 참여하는 대회임
     * 제출 창(2주마다 오픈)마다 가장 빠른 팀 한 곳만 상금을 받음
     * 제출 절차:
          + 정각에 서버 접속
          + 수 초가 드는 PoW 풀이
          + VM 부팅 대기
          + 익스플로잇 코드 실행과 flag 획득
          + 구글 폼 제출
     * 최근에는 4.5초 만에 flag 제출이 성공한 기록도 있었으나, PoW와 VM 부팅만 해도 6.5초가 걸리므로, PoW 풀이 속도 향상이 필수 과제였음

PoW(VDF-Sloth) 알고리듬 분석과 첫 번째 최적화

     * kernelCTF PoW는 sloth VDF라는 순차적 연산 기반의 verifiable delay function을 사용함
          + 1280비트 정수 x에 대해 모듈러 제곱 & 비트 반전 반복 연산
     * 기존 구현(Pyhton+gmpy 및 Rust)에서도 이미 다중 코어 병렬화가 소용없고, GMP도 충분히 최적화되어 있었음
     * 그러나, 모듈로 연산이 Mersenne 수(2^1279-1) 기반임을 활용, 더 빠른 C++ 매뉴얼 모듈러 구현으로 1.9~1.4초까지 성능 향상에 성공함

AVX512IFMA로의 대전환과 SIMD 기반 초고속 구현

     * 그 후, AVX512 ISA와 그 중에서도 AVX512IFMA(52비트 곱셈 및 누산 명령어) 에 착안
     * 1280비트 수를 52비트 limb로 분할하여, SIMD 가속을 최대화함(25 limb → 4개 zmm 레지스터 활용)
     * 모듈러 스퀘어 연산의 대칭성과, 누산 마스크, 메모리 브로드캐스트, 레지스터 어사인 최적화, 브랜치리스 비교 등 저수준 튜닝을 반복함
     * ASM(inline assembly)을 사용해 컴파일러의 비효율적인 레지스터 spill/reload를 막고, 브로드캐스트 및 마스킹 최적화로 최종 0.21초까지 속도를 끌어올림

PoW 제출 자동화와 실제 대회 시나리오

     * 최종 제출에서는 Zen 5 Google Cloud 서버(Amsterdam 지역)를 활용해, 구글 폼까지의 네트워크 지연도 최소화함
     * 자동 제출 프로그램(구글 폼 POST 요청 패치, 내부 팀 협업으로 최적화)으로 사상 최단 기록인 3.6초 만에 성공적으로 flag 제출함
     * 공식적으로 kernelCTF 운영진이 PoW 정책 폐기를 발표, PoW 최적화 경쟁에서 해방됨과 동시에 기법을 공개할 수 있게 됨

고급 구현 상세

  모듈러 연산 최적화

     * 2^1279-1(Prime) 모듈로 연산을 비트 쉬프트와 사칙연산 몇 번으로 치환해, 표준 GMP 대비 매우 빠른 모듈러 연산 달성

  AVX512IFMA 기반 빅인트 스퀘어링

     * AVX512의 52비트 곱셈 누산 명령(vpmadd52luq, vpmadd52huq)을 활용, 8개 limb 묶음 병렬 곱셈 및 누산
     * 25 limb 구조이므로 4개 zmm에 분산, 쓸데없는 마스킹/누산 최소화 로직 설계

  데이터 배치 및 레지스터 활용

     * 오프셋별 유닛, 계단식 데이터 배치, 레지스터간 재배치(valignq, 브로드캐스트) 등 SIMD 병목 해소
     * 누산기(accumulator) 2배로 늘려 곱셈 유닛 대기(레이지) 없이 최고 스루풋 확보
     * 캐리 프로파게이션(carry propagation)까지 필요 최소만 수행

  최종 제출 자동화 및 협업

     * 새벽 시간대에 캠페인 타격을 위한 팀원 배치, 네트워크 위치와 exploit 실행 최적화
     * 실제 제출에서는 PoW 풀이, 익스플로잇 실행, flag 삽입, Google Form POST 요청까지 일관된 자동화 수행

결론 및 의미

     * kernelCTF PoW 최적화는 비트 수준부터 메모리, 레지스터, CNN 등 하드웨어 해부적 이해가 필요한 작업임
     * PoW 정책이 사라지면서 “순수 네트워크/익스플로잇 속도”만이 경쟁의 초점이 됨
     * 본 사례는 실전 해킹과 고성능 컴퓨팅의 만남을 보여주는 동시에, 앞으로도 알고리듬 최적화 노하우와 오픈소스 코드가 연구자 공동체에 기여할 것임

참고 및 부록

     * PoW 알고리듬 전체 코드와 변환 함수(GMP <-> 52비트 배열), SIMD 기반 모듈러 연산, ASM 튜닝 코드가 모두 공개됨
     * 약 12시간에 걸쳐 집약적으로 개발하여 실전 투입한 “거친” 코드지만, GNU AGPL 3.0 라이선스 하에 오픈됨
     * 궁금한 점이나 VDF 관련 협업은 Discord(@forevermilk)로 연락 가능함

        Hacker News 의견

     * 3.6초에 우승한 팀과 2위는 3.73초 혹은 3.74초 기록 보임, 2위도 PoW 최적화를 했거나 FPGA 사용 가능성 있다는 생각, 예전 4초 넘는 FPGA 제출과 비교하면, 저자가 이번 주 2위 기록도 역대급 기록일 수 있다는 점을 언급했다면 좋겠다는 아쉬움 남음
     * 이 방법이 AVX-512 최적화된 RSA 구현에서 사용하는 방식과 매우 비슷하다는 인상, RSA도 매우 큰 거듭제곱 연산이 필요하기 때문, 논문(https://dpitt.me/files/sime.pdf)에서 윈도잉 기법 및 윈도 크기가 임의로 조정 가능함을 다룸, AVX-512 RSA 구현에서는 곱셈 결과를 [0..2^{window-size}) 범위로 테이블에 저장해 각 윈도마다 결과를 사용한다는 점이 추가, 실제 예시는 aws-lc 코드의 rsaz-2k-avx512.pl에서 확인 가능
          + 이런 내용 미리 봤으면 개발에 참고가 되었을 것이라는 아쉬움, Zen 5에선 zmm 레지스터 활용으로 2배 곱셈 처리량 기대, 기존 코드에서 마스크 레지스터가 GPR로 전환되는데 Zen 4/5에선 비효율적임, 캐리 전파가 반드시 한 번에 이루어져야 하는지 의문, 실제로 코드에서는 캐리가 한 번만 발생한다고 가정하고 반복, 이는 대부분 상황에서 지연 감소에 도움, 단 브랜치로 인한 타이밍 공격 가능성 남음
     * AVX512를 여러 세대에 걸쳐 소비자용 CPU에서 지원한다는 주장에 대해, Rocket Lake(11세대) 전까진 AVX512가 엔수지스트, Xeon, 일부 모바일 프로세서에서만 이용 가능했던 것으로 기억, 12세대와 P/E코어 도입 이후엔 몇 달 만에 비활성화되고 사라짐, AMD가 AVX512에서 성공하면 Intel이 다시 도입할 거라는 예측 남김, 자신은 i9-11900 사용 중임
          + 12세대 P코어 CPU는 아예 AVX512 지원이나 광고를 하지도 않았고 기본으로 비활성화, E코어 공간 때문으로 전체 CPU에서 AVX512 미탑재, 단 BIOS 옵션 꼼수로 E코어 비활성화하고 나머지 코어에서 AVX512 활성화하는 정도만 가능, 대신 E코어를 포기해야 했던 점 언급
          + 최근 Intel AVX10 기술백서(https://cdrdv2.intel.com/v1/dl/getContent/784343)에서 P코어와 E코어 모두에서 512비트 AVX를 표준으로 제시, 256비트 한정 구성을 탈피해 AVX-512가 서버뿐 아니라 향후 소비자 CPU에도 본격 복귀 예고, 이는 AMD의 AVX-512 확대에 따라 Intel이 뒤따르려는 것으로 해석
     * CTF 대회 본질에 의문, 제출 속도 경쟁이 아닌 일정 제출 윈도 내에 플래그를 제출한 팀 모두가 상금 나눠 갖는 방식이 더 나을 것이라는 의견
          + 이런 방식은 참가자들이 익스플로잇 정보를 곧바로 공개하지 않고 이후 라운드 도전을 위해 보류하게 만들어 즉각적 보고를 저해하는 메타게임 유발 가능성 제기, 또 제출 타이밍 전략 등 비건설적인 경쟁 유도 부작용 우려
          + 메타게임 구조가 달라지고, 오히려 참여 의욕이 줄어들어 kernelCTF 제출을 고려하지 않는 사람도 늘어날 가능성 언급
     * 내가 이해한게 맞다면 4초 proof of work와 월 1회 지급 구조라는 점, 매달 이렇게 많은 익스플로잇이 등장해 치열하게 경쟁하는 상황인지 궁금
          + 서버가 2주마다 열렸고 PoW는 과도한 접속 요청을 방지하기 위한 목적으로 약간의 지연을 주기 위함, 공공 CTF는 종종 DDoS 유사 작전이 시도될 만큼 치열함, 이후 Google에서 proof of work 단계를 삭제
          + 리눅스 커널 보안에 대한 신화가 실제로는 환상일 뿐이라는 주장
          + 이번 CTF는 원격 코드 실행이 아니라 권한 상승(일반 사용자에서 루트로 상승) 익스플로잇에 해당, 권한 상승 버그는 정말 흔한 현상
     * 놀라운 도전이지만, 우승을 위한 장애물의 복잡함과 우스꽝스러운 분위기 인상, 마치 룹 골드버그 기계 같은 상황이라는 재미있는 표현
     * 본문에서 언급한 base-52 관련 내용을 더 알고 싶으면 오늘 핫했던 이 글(https://news.ycombinator.com/item?id=44132673) 참조 추천
     * 수학이 멋지다는 감상
     * proof of work에 쓰인 sloth라는 VDF(Verifiable Delay Function) 소개, 일련의 긴 계산을 요구해 시간 경과를 증명하고, 계산 결과는 신속하게 검증 가능, 계산이 직렬적이어서 여러 코어 동원해도 런타임 단축 불가, 이런 분야가 CPU 제조사들에게 새로운 시장이 될 수 있을지 궁금, sloth 반복 및 결과에 대한 전용 명령어와 HW 기반 오버클럭 방지 기능 탑재 제안, CPU uptime을 모니터링 후 챌린지와 함께 서명하는 방식도 아이디어로 제시, 이는 CPU를 생산 활용하면서도 n초간 CPU 소유권을 증명하는 proof of stake 개념 유사
     * 블로그에 나온 파이썬 함수가 구글 PoW 구현과 어떻게 등가인지 궁금, 따라가기 까다롭다는 인상
          + 구글 코드에서 ""exponent = (p + 1) // 4""가 2^1277이라는 점, 숫자를 그런 거대한 지수로 올리려면 1277번 제곱해야 하고 파이썬 함수가 실제로 이를 구현, 최초 값이 x라면 각각의 제곱마다 2배씩 곱해져 마지막엔 2^1277개가 됨, 이런 원리 설명
"
"https://news.hada.io/topic?id=21207","C3 배우기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 C3 배우기

     * C3는 C 언어를 기반으로 하며, 모듈, 연산자 오버로딩, 제네릭, 컴파일 타임 실행 등 고급 기능을 제공함
     * 익숙한 C 문법을 유지하면서, 에러 처리, defer, foreach 등 생산성과 안정성을 강화하는 문법이 탑재됨
     * 선언적 계약(contracts) , 옵셔널 타입과 에러 처리 방식 도입으로 안전성과 명확성이 향상됨
     * 표준 라이브러리와 빌드 시스템 통합, 임시 메모리 할당 등 실용적인 개발 환경이 지원됨
     * 빌드, 프로젝트 생성, 코드 구조 등에서 Zig 언어와 유사성이 있으며, 새로운 언어 설계 실험이 엿보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

C3 개요 및 특징

  C3란 무엇인가?

     * C3는 기존 C 언어 위에서 빌드된 언어로, 익숙한 문법 유지와 동시에 C에서는 어려운 모듈 시스템, 연산자 오버로딩, 제네릭, 컴파일 타임 실행, 에러 처리, defer, value methods, 점진적 계약(contracts), 슬라이스, foreach, 다이나믹 타입 지원 등의 기능 제공
     * 네임스페이스를 활용한 모듈 구조로 이름 충돌을 방지함 (abc::Context처럼 명령적 네임스페이스 사용)
     * 주요 목표는 생산성을 높이고 현대적 시스템 프로그래밍 기능을 안전하게 제공함

  언어 특징

    Hello World 예시

     * C와 문법적으로 유사
     * 함수 선언에 fn 키워드를 명시적으로 사용해야 함
     * 입출력 등 표준 라이브러리 함수가 강력하며, 다양한 타입도 바로 출력 가능

    foreach 루프

     * C와 달리 foreach 문법을 기본 지원
     * 참조를 통한 반복문은 변수명 앞에 & 기입(고급 기능)
     * break, continue 지원, 타 언어의 foreach와 유사

    while 루프

     * C99 이전에는 선언을 while 조건식 내부에서 못 썼으나, C3에서는 내부 선언이 가능함

    enum과 switch문

     * switch문에서 암시적 break 지원(혼재된 암시/명시적 break는 호불호)
     * nextcase 키워드로 명확한 케이스 이동(점프 테이블 구현 간편화) 지원
     * Zig, C 등 기존 언어에서 복잡했던 switch-case 흐름을 간결하게 제어 가능

    defer 키워드

     * 스코프 종료 시점에 defer 예약된 구문을 역순으로 실행하여 자원 정리를 안전하게 보장
     * catch, try와 결합된 defer 활용(에러 처리 흐름 제어)

    struct와 union

     * struct 내부에 이름/익명 형태의 서브 struct/union 허용, tagged union 패턴 설계가 쉬움
     * 익명(동일 이름 필드 중복)과 이름 충돌의 구분 엄격히 명시

    에러 처리 방식

     * ? 기호로 옵셔널 타입 지원, 에러 및 값 옵션을 통합해 편의성 강화
     * catch 키워드로 빈(Optional 없는) 상태/에러 분기 가능
     * Rust, Zig와 달리 에러와 옵션 값 구분이 약함(장점: 단순함, 단점: 취지 명확성 저하)
     * ! 연산자(rethrow)로 예외 전파 가능

    계약(Contracts)

     * 함수 전후 조건(Require/Ensure)을 <* .. *> 사이에 작성(컴파일 시 조건 확인)
     * 컴파일 타임 fold 분석까지 지원(정적 분석은 미구현)

    struct 메서드

     * 타입 명시 + 점 표기법(Foo.next)으로 연관 메서드 구성, 네임스페이스 있음(프리미티브 포함)
     * 구조체/유니언/enum 등 모든 타입에 메서드 허용

    매크로

     * 컴파일 타임 평가 기반 매크로(macro 키워드)
     * $로 컴파일 타임 파라미터, #로 평가 전 전달 구현
     * C 스타일(얽힌 매크로 문제 최소화, AST 안정성 강조, @접두사 체크 등)
     * 타입 리플렉션 및 컴파일 타임 실행을 매크로로 처리

    타입 프로퍼티

     * alignof, kindof, extnameof, sizeof, typeid, methodsof, has_tagof, tagof, is_eq, is_ordered, is_substruct 등
     * 메타프로그래밍, 리플렉션에 적합함

    Base64/Hex 리터럴

     * b64""..."" x""..."" 형태로 바이트 시퀀스 직접 선언 가능
     * 내장 매크로 $embed로 대체 필요성(실제론 사용 빈도가 낮음)

    프리미티브 타입

     * int, uint, char(무조건 unsigned), bool, float, int128/uint128 등 다양한 기본형
     * iptr, uptr, isz, usz 등 포인터/사이즈 계열의 별도 타입(직관성 약간 떨어짐)
     * C와 달리 bit-size 보장

    기타

     * 연산자 오버로딩, 구조체 서브타이핑, 제네릭, 런타임 디스패치, any 타입, 비트필드 구조체(bitstructs) 등 폭 넓은 기능 세트 탑재

실습: C3 경험기

  C3 설치

     * 공식 사이트의 프리빌트 바이너리, 직접 소스 빌드 두 가지 지원
     * LLVM, LLD 설치 필요(연결 문제 발생 시 -DLLVM_DIR, -DLLD_DIR CMake 플래그 활용)
     * 일부 배포판의 LLD 라이브러리 미포함 이슈로 바이너리 직접 다운로드 권장
     * C3 컴파일러는 libtinfo 의존성 필요

  프로젝트 생성

     * c3c init 명령으로 표준 폴더 구조 생성(LICENSE/README.md/project.json/src 등)
     * Bluild, 빌드 타깃, 소스 설정 등 기반 프로젝트 구성(Zig, Cargo와 유사)
     * 기본 main.c3 파일은 매우 간결(의견: 새로운 유저에게 적합)

  계산기 만들기

    설계 및 목적

     * 재귀 하강 파서(Recursive Descent Parser) 및 계산기 핵심 로직 구현에 C3의 함수, 입출력, 메모리 관리, 반복문 등 다양한 문법을 실습
     * 문법의 직관성, 실전 생산성 등 우수/불편한 점 직접 파악 목적

    입력 처리

     * @pool으로 임시 할당자(tmem) 사용, 스코프 종료 시 자동 메모리 해제(arena allocator)
     * 표준 메모리 관리 방식인 tmem(임시), mem(일반) 지원, 함수 단위 할당자 전달 패턴(Zig과 C의 장점 혼합)
     * main 함수는 반환값 반드시 명시(컴파일러에서 enforced)
     * 리턴값을 무시해도 괜찮은 함수는 @maydiscard 어트리뷰트 표기(악의적 무시 방지)

    토크나이저 구현

     * 사용자 입력을 토큰 리스트로 분해
     * C3 표준 라이브러리의 List, foreach 구문, switch-case(nextcase, 암시/명시적 break 조합) 등 다양한 제어문 활용
     * 슬라이스 구문(양 끝 인덱스 모두 포함)과 0 길이 슬라이스 관련 혼란 있음(별도 길이 지정 구문 존재)
     * 임시/일반 할당자 혼합 사용 등 메모리 관리의 투명성과 융통성, Rust 등 타언어 대비 우수함

    파서 구현

     * 직접적인 파서 코딩 경험기(생략)

결론 및 종합 의견

     * C3는 전통적인 시스템 언어와 현대적 설계의 접점을 추구
     * Zig, Rust, C를 연구하며, 성과 코드 안정성 양립 언어로 설계됨
     * 모듈성, 안전한 메모리/에러/계약 처리, 강력한 메타프로그래밍, 직관적 빌드 시스템 등 다양한 기능이 두드러짐
     * 학습 곡선은 C 유경험자 기준 점진적으로 진입 가능
     * 언어 서버, IDE 등 생태계 미성숙 및 일부 문법의 호불호 등은 개선 필요
     * 실무 로우레벨/시스템 개발에서 차세대 대안 언어로 주목할만함
"
"https://news.hada.io/topic?id=21253","유용하다고 해서 가치 있게 여겨진다는 의미는 아님","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      유용하다고 해서 가치 있게 여겨진다는 의미는 아님

     * 경력이 쌓이면서 유용함(useful) 과 가치 있음(valued) 의 차이를 이해하는 것이 중요함
     * 유용하다는 것은 특정 업무 수행 능력이 높아 신뢰받지만, 주로 보충자 또는 실행자로 인식됨
     * 가치 있음은 조직의 미래 방향성에 참여하거나 전략적 의사결정에 포함되는 경험임
     * 위기는 진정한 가치 평가의 순간임을 개인적인 사례로 설명함
     * 단기적 보상과 인정 뒤에 진정한 성장 기회 제공 여부를 점검할 필요가 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

유용함과 가치 있음의 차이에 대한 인식

     * 경력을 쌓으면서 유용함(useful) 과 가치 있음(valued) 의 차이를 파악하는 것은 매우 중요함
     * 이 두 개념은 표면적으로 승진, 높은 보너스, 특별 주식 보상 등 유사한 신호로 보이지만, 실제로는 다름
     * 이러한 차이를 이해하기 위해 보다 미묘한 신호를 인지하고 깊이 살펴보는 노력이 필요함

유용함의 의미

     * 유용하다고 평가받는 경우, 특정 분야의 업무 처리 능력이 뛰어나 상사가 일임할 수 있는 역할이 됨
     * 신뢰와 효율성을 인정받아 단기적으로는 없어서는 안 될 존재가 될 수 있음
     * 그러나 본질적으로는 부족한 부분을 채우는 사람, 즉 전략의 핵심이 아닌 필수 업무 수행자로 여겨지는 경향이 있음
     * 이러한 역할에서는 “이 일만 잘 해결해줘, 문제만 일으키지 마”가 주요 기대치로 작용함
     * 리더십 체계 내에서 문제를 적게 일으킬수록 보상이 커지는 구조임

가치 있음의 의미

     * 가치 있음을 인정받는 경우, 단순한 실행자가 아닌 방향성이나 전략적 논의에 참여하는 기회를 얻음
     * 이는 본인이나 비즈니스에 의미 있는 성장 및 기여로 이어질 수 있음
     * 승진이나 핵심 의사결정 참여, 명확한 성장 경로가 주어지는 특징이 있음
     * 반면 유용함에 머무르는 경우, 직무가 정체되어 느껴질 수 있음

경험 사례: 구조조정 시 가치 있음의 느낌

     * 필자가 IC(Individual Contributor)였을 때, 회사가 어려운 시기를 겪으며 구조조정을 단행함
     * 많은 팀이 해체되고, 본인 매니저 또한 해고되어 불안감을 느낌
     * 그럼에도 불구하고 본인은 해고되지 않았을 뿐 아니라, 총 연봉의 50%에 해당하는 잔류 보너스를 1년 베스팅 조건으로 제안받음
     * 리더십은 과거의 성과만이 아니라 회사의 미래에 필자가 핵심적 역할임을 분명히 밝힘
     * 이런 인정은 성과평가나 보너스만으로 느낄 수 없는, 위기 상황에서의 선택으로 체감함

경험 사례: 유용함의 보상과 한계

     * 이후 필자는 겉보기에는 매우 성공적인 역할을 경험함
          + 목표치는 꾸준히 달성했고, 리더십으로부터 칭찬과 높은 보상, 잔류 인센티브를 반복적으로 받음
     * 회사 역시 계속해서 본인을 “남기고 싶은 인재”로 인식함
     * 그러나 점차적으로 새로운 문제 해결 요청이나 전략적 논의 초대가 줄어들었음을 자각함
     * 진로나 성장방향에 대한 대화 없이 단순히 일을 잘 처리하는 역할에 머무르는 느낌을 받음
     * 동기 부여 저하로 인해 결국 더 성장할 수 있는 새로운 역할로 이직함

유용함과 가치 있음의 선 구분하기

     * 독자들에게도 스스로를 돌아보고 표면적 보상이나 인정보다 진정한 가치 인식 여부를 고민해보길 권유함
     * 당신은 진정으로 가치 있는 역할인지, 아니면 단지 유용함에만 머물러 있는지 점검하는 시간 필요함

        Hacker News 의견

     * “Useful”와 “Valued”라는 용어가 기사에서 다뤄진 상황에 잘 맞지 않는 표현이라 생각함. 한 역할에서 탁월한 사람이 다른 역할에서 잘할 거란 보장은 없으며, 이런 한계와 강점을 스스로 인식하는 것이 중요함. 이걸 “useful”이나 “valued”라고 이름 붙이는 순간 감정적, 도덕적 색깔이 부여되는 느낌임. 사실 비즈니스 관계에서 거의 모두가 “useful”일 뿐이라는 인식이 중요함. 승진이나 새로운 기회를 제공하는 것도 더 유용할 것 같아서지만, 회사 환경이 바뀌면 그 기술이 우선순위에서 밀릴 수도 있음. 최근 기술 업계의 레이오프를 보면, 잘나갔던 시니어 기술자들도 한순간에 자리를 잃는 경우가 많음
          + 여기에 동의하지 않음. 오히려 설득당하고 싶기도 함. 모든 직원이 “useful”하길 바라는 건 고용주의 기본 기대치일 뿐, 진정한 가치는 신뢰 관계와 전략적 조언, 그리고 직무 범위를 넘어선 공헌에서 나온다고 생각함. 예를 들어 시니어 매니저들이 새로운 회사로 이직할 때 데려가는 핵심 인재들이 바로 “valued”인 사람들임. 신입이나 주니어 때는 유용함에만 집중하지만, 진짜 승진은 “valued”되는 것에서 결정됨
          + 어떤 때는 투자 대상, 어떤 때는 보험, 때로는 사치품이나 충동구매 같은 느낌임. 이런 비유가 왜 회사가 이상하게 행동하는지 설명해줌. 예를 들어 돈이 부족할 땐 보험부터 해지 안 하고, 기대 수익이 더 높아도 저축부터 줄임
          + 최근 레이오프 사례를 보며 “valued”하다가 갑자기 버려지는 상황, 오히려 저자와 같은 견해임. “Useful”하다가 쓸모없어진 것뿐임. “Valued”가 아니었음
          + 확실히 구분이 존재함. 실무에서는 “useful”하지만 “valuable”하다고 인식되지 않는 경우도 많이 목격함. 이런 차이의 핵심은 소통 부족이나 잘못된 커뮤니케이션임. 좋은 일을 해도 주변에 알리지 않으면, 덜 유용한 사람이 더 인정받을 수도 있음. 셀프 마케팅 필요함
          + 현대 미디어 구조에 대해 너무 순진하게 생각하는 것 같음. 지루하고 무난한 글이 커뮤니티에서 인기를 끌기는 힘듦. 일부러 논란의 여지를 남겨야 모두가 끼어들고 댓글이 달림. 500년 전 극작가가 문맹 관중이 참여할 수 있게 연극을 만든 것과 비슷함. 요즘엔 대중 대신 고상한 척하는 것뿐임
     * 30년 동안 컴벤드 범위 내에서 경력 쌓은 기술자임. 평소엔 “useful”하지만 “valued”함은 승진이나 전략적 포지션 제안에서 찾을 수 있었음. 하지만 나는 사실 그런 역할 자체가 싫음. 기술 문제 해결하는 자체가 좋고, 비즈니스에는 관심 없음. 회사 성장에 열정이 없어서 컨트랙터가 적성에 맞음. 정치 관여 안 하고 티켓만 잘 처리하면 됨. 티켓 없어지면 내 할 일 하면 되고, 사업이나 돈에 더 신경 쓸 필요 없음. 요즘은 오히려 점점 더 무관심해짐. 단순히 편하게 유용한 포지션에 머무르는 것도 선택 가능한 시대라 축복임. 내 2센트
          + 아직 읽지 않았다면 The Gervais Principle, Or The Office According to “The Office” 추천함. 완벽하게 현실과 일치하진 않지만 일과 직장관계를 바라보는 새로운 시각임. 동료들의 동기와 행동을 이해하게 되면서 생각이 많이 바뀜. The Office 팬이라면 더 재밌게 느껴질 것임
          + 나 역시 컨트랙터 경험이 최고의 일이었음. 클라이언트가 요청하면 내 생각과 더 나은 대안을 제시하는데, 대부분 내 조언을 무시하고 결국 더 많은 시간과 비용이 드는 길을 택함. 정직원으로 일할 땐 선택의 결과가 나에게 남아 지원 업무까지 떠앉지만, 컨트랙터일 땐 추가 시간에 대한 보상이 돌아옴. 영리하게 행동하는 데 죄책감 가질 필요 없음
          + 컨트랙터와 비즈니스의 관계가 훨씬 더 솔직함. 시간과 일의 교환임. 정직원은 이데올로기(협동, 기업 사랑, 연봉 경쟁, 승진, 두려움 등)가 개입됨. 컨트랙터는 회의가 적은 것도 장점임
          + 사무실에서 비슷하게 행동했다가 “공정함” 평가를 받았음. 팀에 동화되지 않으면 조직 입장에선 리스크로 판단함
          + 오랫동안 고민한 결과, 자신이 ADHD와 자폐 스펙트럼임을 깨달음. 이런 특징이 사회적 관계와 경력 발전에 제약이 됨. 자폐 경향이 있으면 일은 잘해도 주위에선 “이상하다”고 느끼는 경우가 많음. 경력 성장엔 결국 사회적 관계가 더 중요함. 극소수 기술력만으로 성공하는 예외도 있지만 대부분은 호감도와 네트워킹이 핵심임. 비즈니스에 흥미를 못 느낀다고 말하는 순간, 더 이상 올라가기 힘듦. 기술 인력 중엔 이런 사람 많음. 내 조언은 일할 때 충분히 챙기고, 언젠가는 나이 듦과 동시에 가장 먼저 쳐질 수 있다는 현실을 인식해야 함. 나이 차별에서 살아남는 것도 결국 사회적 스킬임. 리더십이 좋으면 남기고, 아니면 구조조정 대상이 됨
     * 직장에서 원가족의 패턴을 반복하는 경향을 인지할 필요 있음. 어릴 때 부모가 인정해주지 않고 비판적이었다면, 무의식적으로 비슷한 환경을 선택하게 됨. 반대로 존중과 감사를 많이 받으며 성장했다면, 조직 내에서 평가가 약해졌을 때 더 빠르게 개입하고 경계도 더 확실히 설정함. 경계를 세우고 지키는 능력은 결국 어릴 적 경험에서 비롯됨. 이를 자각하는 단계(깨달음), 실천하고 싶은데 아직 부족한 단계(미드사이트), 실제로 행동으로 옮기는 단계(포사이트)로 발전함. ‘나와 내 감정을 책임지는 건 오직 나’라는 마인드 필요함. 심리치료와 트랜잭셔널 분석을 통해 자각과 스킬을 익혀야 실력이 늘어남
          + 심리적 기반이 가장 큰 힘임. 직장이 곧 생존수단이고, 부모처럼 나를 결정한다고 믿게 됨. 그 두려움으로 경계도 못 세우는 경우가 많음. 이런 틀에서 벗어나기가 힘들고, 인생이 완전히 거래적인 공간으로만 흘러가는 건 아쉬움임. 본능적으로 더 깊은 소속감을 원함
          + 개인적으로 이 내용에 큰 공감이 있음. 40 가까이 돼서야 경계 설정 능력이 초기 성장 환경에 비례한다는 걸 깨달음
          + 심리적 전이를 떠올리게 함. 다만 그런 프레임워크에 너무 몰입하면 만사 부모 탓, 지나친 자기탓으로 흐를 수 있음. 힘든 가정에서 자란 사람도 이미 깨달을 건 많이 깨달았을 수 있음. 인간 행동은 의지보단 결정론에 좌우되기도 함. 엄마가 안 좋았다면 스스로 CEO가 되라는 고생 대신 기대치만 내리거나, SSRI나 애더럴 같은 약물 복용이 오히려 현실적 도움이 될 수도 있음
          + 놀라운 통찰임. 순수하게 이런 내용을 무료로 읽게 되어 감사할 정도임. 오랜 세월 응축된 행동과학을 직장 환경에 접목한 느낌임
          + 이 분석의 증거나 방법론에 대해 회의적임. 신뢰할 만한 자원이나 근거 자료가 추가로 궁금함
     * 8군데 넘게 일하면서 항상 “useful”하다는 느낌만 받았고, 진정으로 날 인정해주는 건 가족뿐임. 회사에서 인정받는 건 드물고, 언제든 갑작스럽게 PIP에 들어가거나 사소한 표현 하나로 괴롭힘까지 당하는 현실임. 그래도 급여 받고 괜찮은 대우 받으면 충분히 좋은 거라 생각함. 비즈니스 관계에 너무 신경쓰지 말고, 자산, 가족, 건강에 집중하는 게 현명함. 회사가 날 아끼는 듯해도 결국 위기에선 언제든 배신함
          + “비즈니스 관계에 신경 쓰지 않는다”는 마인드로는 누군가가 날 소중하게 여길 리 없음. 일에 자부심이 없고, 그냥 시키는 일만 하고 집에 가겠다는 사람에게 누구도 특별한 기회를 주지 않으려 함. 이런 태도로는 구조조정 때 우선순위에서 밀리는 게 당연함. 함께 일하고 싶은 타입이 아님
          + 나 역시 여러 회사를 다녔고, 관리자에게 괴롭힘을 당한 경험이 있음. 회사는 항상 관리자 편이고, 내가 유일하게 bash와 Linux를 다루던 기술자인데 새 CTO가 서버를 망칠 때까지 있음. 계속해서 중요하게 일하다가, 매일 괴롭힘에 번아웃이 옴. 결국 퇴사 통보 직전엔 갑자기 아껴주는 척하다가 그냥 떠났음
          + 누가 날 정말 가치 있게 여기는지 알고 싶다면 쉽게 대체할 수 없는 존재가 누군지 보면 됨. 대부분 가족, 친구가 해당함
          + 회사와 좋은 관계를 맺으려면, 그들이 나를 대하는 만큼만 무심해야 함
          + 미묘한 변화들도 누군가에겐 나쁜 행동으로 비칠 수 있음. 사실 그렇지 않더라도 말임
     * “useful”과 “valued” 모두 경험해봤음. 일본 회사에서 비일본인이라는 이유로 전략 결정에선 의견을 거의 묻지 않고, 기업 문화에선 이방인이었음. 하지만 진짜 비밀 프로젝트처럼 중요한 일을 맡긴 걸 보면 신뢰와 가치는 높았음. 다만 저자가 사용한 용어와 내가 쓰고 싶은 용어엔 차이가 있음
     * 저자의 관점에 동의하며, 두 가지 개념의 연결점을 더 깊이 풀었으면 하는 바람임. 한 조직 내에서 “useful”과 “valued”가 깔끔하게 정해지는 상황은 거의 없고, 끊임없이 동적으로 변함. 결국 자기 자신도 이 방정식에 일부임. 조직 내 가치에 대한 자기평가는 주기적으로 재측정해야 하며, 조직이 나를 어떻게 대하든 결국 내가 수용 가능한 기준을 명확히 하는 게 제일 중요함. 이 노력은 끊임없이 이어지는 자기과제임
     * “똑똑하고 성실”, “멍청하고 게으름”, “똑똑하고 게으름”, “멍청하고 성실”의 네 유형이 있고, 리더는 ‘똑똑하고 게으른’, 일반 업무는 ‘멍청하고 게으른’, 위험한 유형은 ‘멍청하고 성실’이라 경계해야 함 (Kurt von Hammerstein-Equord 인용)
          + Useful/Valued 2x2 매트릭스에 비유 가능함. ""무쓸모/무가치""는 분야를 바꾸는 게 방법이고, ""유용하지만 무가치""는 영향을 내세우지 못하거나 관리 문제 가능성, ""무용하지만 가치있음""은 좋은 말만 하거나 숨은 공헌, ""유용하고 가치 있음""이 이상적임
          + 나 스스로 ‘멍청하고 게으른’ 부류에 속하는 것 같음. 하루 반은 일하고, 반은 질질 끌다 결국 제때 마침. ADHD 영향도 있지만 결국 일은 완수함
          + 대기업에서 “멍청하고 성실한” 인재가 넘쳐나는 경향이 있다고 느낌. 성과 평가가 노력량만을 중시하는 문화 때문임
          + 현 시대 리더들이 어떤 부류일지 유추해보게 됨
          + 월드클래스 항공사에서 소규모 정예팀으로 일한 적 있음. 끝엔 ‘일하기 힘든 사람’이라는 꼬리표를 받음. 12시간씩 묵묵히 일했지만, 데드라인을 못 맞추자 희생양 필요했고, 나는 조용히 잘 일했기에 오히려 표적이 됨. 관리자 한 명이 계속 개입해서 모함했고, 연이어 중간관리자도 잘 알지 못하는 정보 만으로 공개적으로 소리침. 조용히 대응했지만 계약이 조기 해지됨. 컨설턴트라 담담히 떠났음. 놀라운 건, 이런 “소문”과 “계급 내 정보 전달”에 휘둘려 평소 점잖던 관리자까지 나를 급히 공격한 것이었음. 결국 잘못된 정보가 얼마나 조직 문화를 왜곡하고 프로페셔널도 바꿀 수 있는지 체험함. 조직 내 거짓 정보의 파급을 지켜보며, 결국 그런 비도덕적 타입이 오랜 시간 무사했던 건 상신라인에 붙어서임
     * 실제로는 훨씬 단순함. 타인이 못하는 역량으로 조직의 목표 달성에 기여하면 됨. 성실함보다는 결국 실력, 커뮤니케이션, 소프트 스킬을 기반으로 아웃풋을 내는 것이 중요함. 일부 고용주는 일부러 내 가치 한계를 두기도 하는데, 그럴 땐 진짜 실력이 아니라 “게임 잘하기”로 평가되기도 함
          + 실제 노력과 상관없이 보상은 결과, 대인관계, 보여지는 노력에 좌우됨
          + “남들이 못하는 걸 하라”는 말이 쉬워 보이지만, 실상 주어진 일이 아니거나 조직에서 중요하게 여기지 않는 일일 때가 많아 제대로 된 평가를 못 받는 경우가 많음
          + “가치 있게 행동해라”는 말이 아이들의 게임처럼 너무 추상적임
     * 이전 직장에서 임금은 높았지만 팀에서 인정받고 싶어 이직을 시도함. 두 해 만에 모든 저축을 써가며 구직했으나 시장이 바뀌면서 내 커넥션과 기술이 더 이상 충분하지 않게 됨. 요즘은 우버 운전이라도 해야 할까 고민 중임. 예전엔 6자리 연봉도 받았음. 과거로 돌아가 선택을 바꿀 수 있다면 무조건 그 일을 붙잡고 남았을 것임. 사회적 인정보다 보험 등 실질적 현실이 더 소중할 때가 있음
          + 안타까운 상황임. 나머지 독자들에게는 새 일자리를 확보하기 전엔 절대 그만두지 말라는 경고임
          + 자책할 필요 없음. 계속 남았더라도 구조조정 운에 휘말릴 수 있었음. 인생엔 운도 작용함
          + “팀에서 인정받았다”는 것도 결국 “돈을 벌기 위해서”와 마찬가지임. 직장 밖에서 의미를 찾아야 더 깊고 오래가는 행복임
          + 경고 차원에서라도, 다른 직장을 구한 뒤 퇴사하는 게 현명함. 구직 자체가 직장이 있을 때 훨씬 쉬움
     * 많은 동료들이 정말 뛰어난 일을 했지만, 임금이 평범하고 승진 기회도 적음. 반대로 일을 많이 못하더라도 최신 트렌드, 적절한 버즈워드, 신선한 마인드, 자기 홍보에 능한 사람들은 쉽게 승진하고 새로운 기회를 얻음. 실력파가 조직을 떠날 것처럼 아우라를 내비치지 않으니, 관리자는 이들을 붙잡으려 애쓰지 않음
"
"https://news.hada.io/topic?id=21281","Precious Plastic이 위기에 처해 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Precious Plastic이 위기에 처해 있음

     * Precious Plastic은 오픈 소스 플라스틱 재활용 프로젝트로, 지난 10년 동안 자원봉사와 소규모 예산으로 큰 글로벌 커뮤니티를 이루었음
     * 그러나 지속적인 재정 문제, 명확하지 않은 비즈니스 모델, 뉴욕 소송 등의 복합적인 어려움으로 인해 현재 존속 위기를 맞고 있음
     * 핵심 팀은 적은 인원으로 대규모 커뮤니티를 유지하고 있지만, 재정과 조직 구조의 한계로 인해 장기적인 개발과 성장이 어려운 상황임
     * 오픈 소스의 한계와 기여 구조 미흡으로, 많은 조직들이 혜택만 얻고 되돌려주는 활동이 부족해 커뮤니티 기반의 지속 가능성이 위협받고 있음
     * Precious Plastic 팀은 버전 5 개발과 미래 전략을 위해 지속적인 지원과 펀딩, 그리고 커뮤니티의 적극적 기여를 호소하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Precious Plastic의 성장과 임팩트

     * Precious Plastic은 2013년부터 버전별(Version)로 개발된 오픈 소스 플라스틱 재활용 프로젝트임
     * 2020년에 출시된 Version 4에서는 'Pro' 기계, 시트프레스, 스타터키트, 비즈니스 계산기, 새로운 금형 및 제품 등 다양한 혁신을 이루었음
     * 최소한의 예산으로 전 세계 56개국 1,100개 조직이 140만 kg 플라스틱을 재활용하고, 연간 3.7백만 달러의 매출을 창출, 530명 고용과 3,405명의 자원봉사자가 활동 중임
     * 모든 설계와 지식을 오픈 소스로 공개해 누구나 무료 이용이 가능하며, 실제 재활용 워크스페이스 확산에 큰 역할을 했음

프로젝트 운영 방식

     * Precious Plastic은 새로운 버전 개발 후 자금 부족으로 ""휴면기""에 들어가며, 기적적으로 외부 지원이 있을 때만 다음 개발을 이어가는 독특한 운영 구조임
     * Version 4 이후 소수의 핵심 팀이 연중 지속적인 개발과 조직의 장기적 존속을 목표로 변경하려 했으나, 여러 복합적인 문제로 어려움을 겪음

현재 직면한 문제점

  1. 작업 공간 부재

     * Covid-19 이후 크롬-6 검출로 인해 기존 작업 공간을 급하게 떠나야 했으며, 많은 장비와 자원을 헐값에 처분할 수밖에 없었음
     * 이후 프랑스의 팀원 집 차고로 이동해 임시로 근근이 운영 중이나, 이는 조직 규모와 활동에 큰 제약 요소가 되고 있음

  2. 비즈니스 모델 부재

     * Precious Plastic의 목표는 기계와 금형 판매 경쟁 없이 커뮤니티와 상생하는 비즈니스 모델 구축이었으나, 현실적으로 컨설팅과 프로젝트 중심(collab) 모델은 장기적 재정 안정화에 한계가 있었음
     * 최저 임금 지급에도 팀 유지가 빠듯했고, 결정적으로 다음의 문제들이 재정 악화를 가중시켰음

  3. 뉴욕에서의 소송

     * 미국 뉴욕에서의 프로젝트 중 재활용 기계 사용 사고로 소송이 발생했고, 보험 부재 및 높은 법률 비용(시간당 600달러)으로 인해 조직에 매우 큰 부담을 주고 있음
     * 오랜 기간 동안 소송이 지속되며, Precious Plastic 팀은 이 사건에서 책임이 없다고 보나, 판결 전까지 불확실성과 높은 비용에 시달리는 중임

  4. 소프트웨어 프로젝트의 복잡성

     * 커뮤니티 플랫폼(온라인 협업 및 문서 공유 시스템) 개발에 예상보다 훨씬 큰 노력이 소요되었으며, 디지털 홈이 완전히 성숙하지 못해 온라인 커뮤니티의 성장에 지장이 있었음
     * 개발과 유지, 개선에 계속 투자가 필요하며, 전 세계 개발자들의 직접적인 기여와 피드백이 필수적임

  5. 오픈 소스 커뮤니티 구조의 한계

     * 무료 오픈 소스 정책으로 많은 재활용 업체와 워크스페이스들이 성장했으나, 대형 조직이 기여 없이 혜택만 취하는 경우가 많아, “기부/환원 없는 리소스 소비”가 커뮤니티 존속에 위협이 되고 있음
     * 이러한 현상은 설계상의 문제로, 지속 가능한 조직-커뮤니티 관계 및 건강한 재정 구조가 부재했기 때문임

  6. 장기 팀 구성의 어려움

     * 상기 요소들로 인해 Precious Plastic 팀은 장기적 성장과 고용 안정성 확보에 실패, 멤버들의 생활 및 미래 불확실성 증가로 지속적인 전문성 유지가 힘든 상황임

현재 구조와 한계

     * Precious Plastic 조직은 네덜란드의 비영리 구조로, 현재 3명의 상근 인력과 분기별 3만 유로 운영비, 6개월 분 예산만 보유 중임
     * 반면에, 글로벌 커뮤니티에는 1,000개 이상의 워크스페이스, 530명 고용, 3,000여 명의 자원봉사자가 활발히 활동하며 연간 370만 달러 이상의 매출이 집계됨
     * 적은 조직 인력으로 커뮤니티 관리, 필수 운영 등 기본 기능만 유지 중이며, 추가적인 성장과 혁신은 재정과 인력 부족으로 어려움을 겪음

향후 시나리오와 제안

     * 현재 Precious Plastic 팀은 1) 프로젝트의 자연스러운 종료 또는 2) 대대적인 혁신(Version 5)으로 다음 단계 도전이라는 두 가지 선택지 중 고민 중임
     * 지금까지 쌓아온 글로벌 인프라와 네트워크를 살리고, 조직과 커뮤니티 모두를 위한 성장·재정의 구조 개편이 최우선 과제임
     * Version 5는 조직의 재정적 자립성을 확보하고, 구조를 근본적으로 재설계하는 대규모 프로젝트가 될 예정
     * 이 소프트웨어/하드웨어 프로젝트는 지금까지의 어느 버전보다도 큰 자금, 인력, 커뮤니티 전체의 동참이 필수적임

커뮤니티의 도움 요청

  지원 방법

     * Youtube 채널 구독, 재활용 플라스틱 제품 구입, Bazar에서의 기계 및 제품 판매 등으로 간접적 재정 지원 가능
     * Patreon을 통한 월간 후원, 온라인 플랫폼 기여, 법률/그랜트 작성 등 다양한 방법으로 직접 참여를 요청함
     * 뉴욕 소송 관련 변호사(네덜란드/미국)의 pro-bono 지원, 오픈 소스 커뮤니티 플랫폼 개발을 위한 SW 기여, 실제 Q&A/지식 업로드 등 구체적 기여 방안 제시
     * Version 5의 펀딩, 기부, 대형 파트너십 및 콜라보레이션 제안서 수락, 암호화폐 기부 등도 선택지임
     * 중장기적으로, 전 세계 소규모 플라스틱 재활용 사업의 3배 성장이라는 목표와 총 실제 운영 예산(2.1백만 유로) 수립

결론

     * 현재 조치가 없다면 Precious Plastic 프로젝트의 자연 종료도 수용
     * 커뮤니티의 즉각적이고 다양한 형태의 지원과 피드백이 있을 시 Version 5 및 향후 지속 가능한 성장 기반 마련이 가능
     * 진행 상황 및 업데이트는 지속적으로 커뮤니티에 공유 예정

        Hacker News 의견

     * 우리는 10만 유로 기부금을 받았고, 정말 놀라운 일이었지만 이 돈 전부를 조직 운영이 아니라 커뮤니티가 각자 프로젝트를 이어갈 수 있게 모두 나눠주기로 결정함. 내가 볼 때 이들이 언급한 많은 문제는 스스로 자초한 것들이고 단순한 실수보다 한 단계 더 넘어선 것임. 솔직히 추가 지원 요청이 더 진심으로 느껴지려면, 해당 리더십들이 자리에서 물러나고 ‘버전 5’를 약속하기보다는 조직 내부의 문제 해결에 초점을 맞추는 태도 필요함. 실사 과정도 제대로 됐는지 의문이고 그들이 단순 인정만 했을 뿐, 정말로 문제에서 배운 게 없다는 징후밖에 없으니 사기가 아닐지 우려됨
          + 나는 이 프로젝트가 수익이 크게 필요하지 않은 사람들이 자기 꿈을 좇으며 시작한 느낌이고, 실질적으로 잘 작동해야 할 기술적 압박은 없었던 것 같음. 라이프스타일 비즈니스가 종종 있지만, 이건 라이프스타일 단체에 가까움. 기사 내내 ‘커뮤니티’와 ‘지역 사람들’ 얘기만 많고 구체적 내용은 적고, 드러난 정보는 오히려 주의 신호임. 예를 들어, 빌린 기계 창고가 문을 닫으면서 기계를 헐값에 팔았고 새 공간을 찾아도 되사올 돈이 없다는데, 임시 저장 공간은 구할 수 있고 비싸지도 않은 편임. 중요한 디테일이 누락된 것 같고, 혹시라도 그걸 밝히면 그들 이미지가 더 나빠질까 숨긴 것 아닐지 의심됨. 내가 알기로 이들은 안전하지 않을 수 있는 목재 파쇄기, 프레스, 사출 금형 같은 기계를 거의 원가에 판매함. 그 외엔 뭐가 있는지 모르겠음.
            기사에 언급된 ‘버전4’는 아마 오픈소스 ‘아카데미’를 말하는 듯하고, 여기엔 ‘모든 비용을 기록해라’, ‘세금 포함시키는 것 잊지 마라’ 같은 정보와 사실상 빈 엑셀 시트, 즉 ‘Business Calculator’가 있음. 커밋은 2020년 이후 없음. 5년 동안 개발 중이라던 ‘버전5’는 아마 비공개 GitHub에 있을 듯. 그래도 행운을 빔. 비즈니스 계산기 링크 참고
          + 선의가 있다고 해도, 스스로 운영도 어려운 상태에서 돈을 모두 기부한 건 매우 어리석은 결정임. 커뮤니티가 더 일 잘할 거란 계산이었다면 그 근거가 약한 것 같고, 그 외엔 언젠가 어떻게든 돌아올 것(아마도 또 다른 기부)만 기대하고 있었던 셈임. 사실 이런 기부야말로 조직 미래의 안전을 보장하는 것임
          + 리더십이 자리에서 물러나면, 지금처럼 실질적 자원도 없는 상황에선 도대체 누가 문제를 해결할 수 있을지 의문임
          + 나는 조직이라는 구조 자체가 존재만으로도 누군가는 책임자가 되고, 위계가 생기면서 점점 더 자기 존재 유지를 위해 움직이는 계층 구조가 된다고 생각함. 비영리 단체들도 규모가 커질수록 ‘지속가능성’이라는 이름 아래 실은 수익 창출과 다름없는 방향으로 감. 커뮤니티 단체가 비중앙 집권적 소셜미디어 없이 어떻게 온라인에서 연결될 수 있을지 고민했고, 반 중앙집중 방식의 좋은 사례도 있음. 개인이 주도로 일하는 웹 개발자 집단이라면, 성과도, 관리 효율도 높일 수 있고, 오버헤드도 적음. 반면 대형 비영리는 예산이 500만~5천만 달러까지 늘어나고, 실질적 실행보다 그랜트 따오기에 더 집중함. ‘지속가능성’이란 단어는 비영리 내에서 사실상 ‘수익성’의 다른 말임. 비영리가 가진 진짜 위험은, 오히려 억척스럽게 경비만 줄이지만
            동기 부여가 낮은 조직이 된다는 점임. 시스템 목적은 곧 시스템 자체의 효과라는 말도 있음. 관련 배경 지식: 시스템 목적, [Oxfam이나 Bill Gates는 진짜 경제개발을 돕지 않음]
     * 나는 Precious Plastic을 처음 알게 됐고, 내 견해는 단지 이 기사 한 편에 근거한 것임. 가장 큰 문제는 명확하고 구체적인 로드맵이 없다는 것임. 돈을 준다고 해도 그 돈이 어디에 쓰일지 감이 안 잡힘. 새로운 오픈소스 도구 개발, 유지보수성·안전성·에너지 효율 어떤 관점에서 개선될지, 포럼·위키 소프트 개발, 전 세계 워크샵에 지원, 구체적 아웃리치 장소와 방식 등, 각 방향성조차 제시가 없는 상태. 세부 실행 계획이 아니라도 다음버전(5)에서는 이전(4)에 비해 어떤 변화가 있는지 방향성이라도 알 수 있다면 좋겠음. 그게 없다면 문제의 원인은 조직 내 스트레스가 아니라 비전과 방향성 부족임
          + 이런 비전과 방향성 부족한 상황, 내 경력에서 한두 번 본 일이 아님. 팀 전체가 아이디어나 영업력은 좋아도 실제 실행에는 실패해서 표류하는 스타트업도 많음. 매력적인 리더십이 투자를 이끌기도 하지만 끝까지 실천이나 후속 진행 역량이 부족하면 소용없음. 최첨단 만능 ‘프로덕트 엔지니어’를 찾는 스타트업들 중 이런 비전 위임 때문에 실패하는 곳 많을 것으로 봄
          + Precious Plastic은 꽤 오래된 프로젝트고, 전 세계적 커뮤니티도 활성화됨. 이들이 추구하는 핵심 목표 중 하나는 전 세계에 ‘마이크로 팩토리’, 일종의 메이커스페이스 구축임. 관련 지도에서 운영 중인 곳을 확인 가능함
     * 이것은 진짜 문제 해결보다 현상유지, 즉 “엠뷸런스 따라가듯” 움직이는 프로젝트임. 초점은 소규모 주체들이 조금씩 재활용하는 게 아니라, 산업이 대규모로 야기하는 오염에 비용을 부담하게 강제하는 것에 맞춰야 함. 이런 프로젝트들은 오히려 업계에 “봐라, 몇 톤은 이렇게 재활용된다”는 당위성을 주고 더 많은 플라스틱 생산을 정당화하는 효과도 있음
          + 소비자 관점에서 플라스틱의 사용과 환경 영향이 과대평가되는 것 같음. 사람들은 미세플라스틱에만 집중하지만, 사실 생수병을 의자나 기타로 만드는 건 본질적 이슈에서 벗어난 행동임. 플라스틱을 태워서 오염 없이 처리하는 기술도 충분히 있는데, 미세플라스틱이 인체에서 발견되는 주원인은 페인트, 타이어, 폴리에스테르 의류 같이 끊임없이 가루가 되어 모든 환경에 퍼지는 것들임. 선진국 거주자는 태평양 플라스틱 오염 거의 일조 안 하고, 대다수가 어망이나 쓰레기를 직접 강에 버리는 국가에서 유입된 것임. 정작 사람들은 빨대 쓰는 이들만 탓함
          + “분자 바코딩” 연구가 진행되고 있고, 이를 통해 포장재별 완벽한 분리까지 가능해지면 재활용의 돌파구가 열릴 것임. 관련해서 포장재는 재질이 7겹인 경우도 많아 이들을 쉽게 분리할 표준이 생기면 실질적 진전이 있음. 하지만 시스템 내에서 들은 바로는, 제조업체들은 자기가 무엇을 어디로, 어떤 방식으로 흘려보내는지 공개되는 걸 원치 않음. 법과 시민이 인센티브 구조 자체를 바꿔야 함. 그래도 폐기물로 괜찮은 물건 만들긴 좋지만, 이런 작업에서 나오는 분진과 연기는 건강에 해로울 수 있음. 내가 일했던 곳에선 폴리카보네이트 레이저 커팅조차 금지했었음(내분비계 교란물질 발생). 분자 바코딩 연구 더 보기
          + 유럽연합(EU)은 이미 두 단계 앞서 있음. EU 플라스틱 과세 현황, EU 일회용 플라스틱 정책
          + 지금도 업계가 자기들이 만든 오염 비용을 전혀 부담하지 않음. 이런 시민성 프로젝트 따위로 업계가 자기 책임에서 빠져나가는 건 아님. 빗자루질하는 시민이 있다고 해서 오히려 기업들이 자기 행태 변명거리로 삼으려 한다고 생각하지 않음
          + 진짜 필요한 건 외부효과까지 모두 반영해 플라스틱 가격을 올리는 것임. 플라스틱은 사실 슈퍼소재이기에, 대체 천연 소재(실크, 유리, 알루미늄, 종이, 나무, 강철 등)와 가격이 맞아야 실제로 필요한 영역에만 쓰게 됨. 담배·술처럼 플라스틱에도 물품세 부과 필요함. 이런 구조라면 진짜 필요한 영역에만 플라스틱 사용 가능함
     * 플라스틱은 반복적인 사출 성형 과정에서 열과 압력 때문에 분자구조가 점점 망가져 재활용하면 품질이 더 떨어지고 미세플라스틱 문제까지 늘어남. 차라리 전기를 얻기 위해 고온 소각로에서 플라스틱을 100% CO2와 물로 태워버리는 쪽이 더 나음
          + 이 방안이 논리적으로 들려서 조사해봤는데, 사실 땅에 묻어두는 게 더 나은 전략일 수 있음. 소각하면 땅 속 탄소(원유)가 결국 대기 중으로 빠져나가고, 매립하면 최소 천년간은 봉인함. 일단 생산 자체를 줄이고, 재활용 그다음, 그리고 남는 건 최대한 깊이 묻어야 함
          + 소각 대신 ‘해체(디폴리머화)’, 즉 플라스틱을 다시 원래의 단량체로 분해하는 방안이 더 나음. 이렇게 하면 약 90% 이상 품질 저하 없이 재활용 가능함. 남은 10%만 태우면 됨
          + 플라스틱을 원료 상태로 되돌리는 여러 공정들이 후보군으로 있음. 만약 엄청 저렴한 재생에너지가 과다 공급되는 상황이라면, 고온·고압에서 탄소를 분해해 다시 원유 수준으로 만드는 것도 가능함. 최근에는 효소가 폴리머를 분해해 재활용성을 높이는 연구도 진행 중인데, 아직 상용화 단계까진 덜 됨
     * 나는 8년 전 Precious Plastic을 처음 봤을 때 매우 흥분했고, 큰 기대를 했음. 그런데 얼마 안 가서 이 사람들이 뭘 하는지 제대로 모른다는 걸 깨달음. 기계 설계는 너무 작고 고가/고급이라서 실생활에 맞지 않았음. 오히려 그 이후 파생된 더 실용적인 대안들이 더 큰 기계를 만들고 지역에서 독립적인 비즈니스를 잘 굴림. 그래서, 10년 넘게 실질적·지속가능한 비즈니스로 키울 의지가 아예 없었던 것 같음. 포르투갈에서 했다는 ‘Precious Plastic Camp’도 정말 보잘것없었고, 오히려 힙스터형 공동체 같았음. 또, 기존의 매우 유용했던 포럼도 갑자기 없앴고, 자신들이 쌓은 소중한 지식을 몽땅 날려버림. 나는 더 이상 이 조직에 좋은 돈을 더 지원해도 의미 없다고 생각함. 그래도 이들이 세상에 던진 영감과 흥분에는 감사함. 하지만 이제 본인들이 얘기한 대로
       ‘재활용’되어야 할 시간임. 그래도 아마 버전5, 6 등으로 모양만 바꿔 계속 지원자를 끌어들이겠지. 그게 비영리의 현실임
          + 이런 부정적 댓글이 상단에 올라온 게 아쉬움. 조직의 연간 소진금이 3만불 수준으로 매우 낮고 그동안 커뮤니티에 준 무형의 가치도 상당함. 공공재처럼 기능하는데, 이걸 단지 수익 모델이 없다는 이유로 비난하는 건 마치 도서관이 수익 못 낸다고 비난하는 것과 다름없음. 당신과 같은 시선이 실질적으로 이런 생태계와 오픈소스 관련자, 그리고 테크 부자들이 좀 더 힘을 보태줄 계기를 무너뜨리고 있는 것임. 이 시선이 정말 옳은지, 오픈소스나 공공재의 필요함에 대해 한 번 더 생각할 가치 있음
          + 내가 보기엔 이들이 해결하려는 문제는 이미 산업 차원에서 대형 설비로 훨씬 효율적으로 해결되고 있음. 내 플라스틱 폐기물은 이미 10년 넘게 도어 투 도어로 수거되고, 대형 분리설비에서 자동 처리됨. 굳이 다시 내가 직접 워크샵 가서 재가공할 필요는 없음. 이들이 만든 DIY 설비는 결국 취미 수준 기계임. 알리익스프레스나 이베이에서 유사하거나 더 나은 기계도 쉽게 살 수 있고, 사업 목적으로 하려면 진짜 산업기계도 중고로 저렴히 구함. 진짜 문제는 플라스틱 재용융 시 폴리머 품질 저하, 재료과학·공정 엔지니어링 문제임. 그건 지금 학계와 업계에서 진지하게 파고드는 영역임. 이들의 진심은 의심하지 않지만, 선의와 기분만으론 본질적 개선이 어렵다는 생각임
          + 그동안 PP를 관찰해왔던 내 입장에서도 많은 문제가 자기 손으로 유발된 거라 동의함. 대표적으로 보험조차 준비 안 됐고, 너무 이상적인 오픈소스 목표와 현실성 없는 기대치, 그리고 10만달러 기부까지 그냥 외부로 이전했던 것이 가장 이상했음. 세부 뉘앙스는 놓쳤을 수 있지만, 이 단체가 역량있다는 느낌은 안 들었음. 기부금은 허무하게 사라질까 봐 우려됨. 프로젝트가 죽어도 별로 미련 없다는 태도도 이상하게 느껴지고, 이제는 진짜 조직 변화가 필요하다고 봄
          + ‘사기’라는 표현은 좀 지나치다고 봄. 누군가가 기부금으로 연봉 50만 달러씩 챙긴다면 사기지만, 그런 건 아님. 모르는 사람이 언젠가 더 나은 깨달음을 얻을 수도 있고, 기부자는 다양한 동기로 돈을 쓰기도 함. 예를 들어 어떤 부자가 PP에 돈을 지원한다면 슈퍼카나 1천만달러짜리 그림 사는 것보단 좋은 일임. 그냥 서로 살아가자는 것임
          + PP가 다른 기계 제작 회사와 차별화되는 지점은 오픈소스 하드웨어에 대한 확고한 의지임. 일부 기계를 직접 만들어서 팔기도 하지만, 원래 초점은 누구나 설계도를 내려받아 직접 제작 가능한 오픈소스 설계임. Open Source Ecology의 Global Village Construction Kit와도 맥락이 비슷함. 거기서는 문명에 꼭 필요한 50가지 기술(벽돌 프레스, 트랙터 등)을 오픈소스로 만들려 했는데, 진행이 주춤함
     * 10년째 PP 기계, 사용자, 문제를 진짜로 다뤄온 입장에서 얘기함. 결론적으로, PP는 매우 사기성 높은 조직이고, 실제 어떤 설계도 제대로 작동한 적 없음. 자신있게 주장하는 내용 모두 증거로 뒷받침되는 사례 없음. 관련 비판 리뷰 보고서 참고. 우리뿐 아니라 타인도 알고 있음
     * 미국에선 플라스틱 작업장을 열고 싶어도 소재기(슈레더) 및 가공 도구 가격, 전용 공간과 전력, 적당한 폐플라스틱 확보까지 진입장벽이 높음. 놀랍게도, 미국에선 플라스틱 재활용이 이미 대자본 체계로 굴러감. 수집, 분류, 운송 모든 단계에 이미 계약자들이 포진함. 이런 한계가 아니라면 PP 모델도 정말 잘 먹힐 나라들이 있는데, 미국은 아님
          + 플라스틱 시트 한 장 만드는데 15KW가 필요함. 이는 가정용 전력 용량 대부분임. 하루에도 시트 몇 장밖에 못 만듦. 제대로 된 재활용 시설이라면 열 회수/예열 같은 시스템이 갖춰진 연속생산 라인일 텐데, 이런 수준을 소규모로 구현하는 건 어렵다고 봄
          + 훨씬 더 작은 단위로도 시도 가능하지 않을까? 작업장 외에도 플라스틱 수거함 운영, 교육과 노력으로 커뮤니티가 자발적으로 깨끗한 폐플라스틱을 모아 프로젝트화 할 수도 있음. 지역에 이미 드롭오프 포인트가 있을 수도 있음. 드롭오프 맵 참고
          + 나는 교육 현장에서 PP 및 유사 장비를 썼고, 주기적으로 유지하는 작업장도 본 경험 있음. 소재 수급은 전혀 어렵지 않았고, 각자 플라스틱 쓰레기, 중고 가구, 중고 시장 등 다양한 루트로 확보 가능했음. 기계 가격이 비싸게 느껴진 배경을 구체적으로 알고 싶음. 사고자 했는지, 직접 만들려고 했는지 궁금함
     * 조직 내 역할을 좁히고, 비즈니스 운영은 별도의 전문가에게 넘기는 구조가 어떨지 제안해봄. 자신이 진짜 잘하는 부분, 즉 오래된 비전 추구, 모두를 모아 이끌기, 영업이나 재무와 무관하게 추진해가는 데 집중하고, 실제 회계 관리든 협력은 따로 전문가에게 맡기는 구조 필요함. 한편, 플라스틱 재활용 실적이 업계의 이미지와 별개로 실제 전 생산량의 1/3만 재활용되고 반복 사용도 한 번뿐임. 플라스틱 업계는 요즘도 생산량만 계속 늘리고 있고, 사실 진정 재활용 가능한 소재는 유리, 강철 같은 것임. 그럼에도 불구하고 플라스틱은 필수 기반재로 여겨질 만한 영역이 있고, 당분간은 대안이 쉽지 않음. especially 일회용 의료·위생용도
     * 사이트를 읽어보고도 사실 이 기계들이 뭘 하는지 명확하지 않음. 딱딱한 플라스틱을 갈아서 펠릿으로 만든다는 건 알겠는데, 비슷한 설비는 알리바바에서도 500달러에 파는데 차별점이 오픈소스라는 것임? 자체 설비가 부품값만 2천유로 이상이고, 직접 조립도 해야 함. PP Pro 정보
          + 애플과 수류탄을 비교하는 것처럼 직접 비교는 무리임. 알리바바에서 500달러 내외 기계도 실제로는 송장 등 배송비 제외하면 1000달러 넘고, 내구성도 의심스러움. Precious Plastic 수준의 설계와 내구성을 원한다면 가격차도 거의 없을 것임
          + 오픈소스의 실질적 가치는 자전거 발전기 대신 지역 부품, 맞춤 소재를 활용해 소규모 리사이클링이 가능했다는 데 있음. 이미 PP 덕분에 전세계 여러 곳에서 현실적인 소규모 재활용 네트워크가 생겨 일자리 창출과 강 유입 쓰레기 방지 효과는 의미 있게 창출함
     * 플라스틱 폐기물 비즈니스 창업, Precious Plastic 버전별 이슈 등 예전 HN 스레드, 버전4 영상, 버전3.0 이슈 등도 참고하면 유용함
          + 오픈소스 재활용 관련 2016년 스레드, 초기 버전 논의 글도 있음
"
"https://news.hada.io/topic?id=21216","Angular v20 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Angular v20 발표

     * 리액티비티(reactivity) 관련 주요 API를 안정화하며, Zoneless 기능을 개발자 프리뷰로 공개함
     * Chrome DevTools와의 통합 등으로 디버깅 경험과 개발 효율성을 크게 향상시킴
     * GenAI 개발 지원, llms.txt 도입 및 AI 앱 구축 가이드와 예제 오픈 소스로 제공함
     * 기존 NgIf, NgFor, NgSwitch를 공식적으로 폐기하고 내장 control flow로 전환을 권장함
     * Angular 공식 마스코트를 커뮤니티 투표로 선정하는 새로운 프로젝트를 시작함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Angular v20 주요 특징

     * 지난 몇 년간 Angular는 Signals 기반 리액티비티, Zoneless 앱 등으로 큰 변화를 이끌었음
     * Angular v20에서는 진행 중인 기능의 안정성과 개발자 경험 개선에 집중했음

리액티비티(reactivity) 기능 안정화

     * Signals, computed, input, view queries API가 안정화 단계에 진입함
     * effect, linkedSignal, toSignal API도 안정화되어 신뢰성 있는 비동기 상태 관리가 가능해짐
     * Google, YouTube 등 대규모 서비스에서 input latency 35% 개선 등 성공 사례가 공유됨

새로운 실험적 API: resource, httpResource

     * resource API로 signal 변화에 따라 비동기 요청 및 스트리밍 데이터 처리가 쉬워짐
     * httpResource로 신호 기반 HTTP 요청 지원, signal에서 바로 요청 결과 사용 가능
     * WebSocket 등 다양한 스트리밍 데이터 패턴을 signal로 간단히 관리 가능함

Zoneless 기능 개발자 프리뷰

     * SSR 환경에서 Node.js의 unhandledRejection, uncaughtException 기본 핸들러 내장
     * 클라이언트에서는 provideBrowserGlobalErrorListeners로 글로벌 에러 리스닝 가능
     * angular.json에서 zone.js polyfill을 제거하고 zoneless 모드로 전환하는 가이드 제공

서버 사이드 렌더링 고도화

     * 증분 하이드레이션(incremental hydration), 라우트별 렌더링 모드가 안정화됨
     * 필요한 UI만 뷰포트 진입 시 다운로드/하이드레이션해 JS 트래픽 최소화 실현
     * 라우트 별로 SSR/CSR/Prerender 등 다양한 렌더링 모드 설정 가능

개발자 경험 및 생산성 향상

     * Chrome DevTools와 협업해 Angular 전용 성능 트랙 제공, 프레임워크 내부 렌더링, 이벤트, 체인지 디텍션 등 실시간 파악 가능
     * createComponent의 동적 생성, 디렉티브 적용, 신호 기반 바인딩 등 API 개선
     * 템플릿에서 지수 연산자(**), in 연산자 및 untagged 템플릿 리터럴 지원
     * @for trackFn 미호출, nullish coalescing 오용, 구조적 디렉티브 미입력 등 진단 기능 강화

스타일 가이드 및 호스트 바인딩 개선

     * 10년간의 사례를 바탕으로 스타일 가이드 단순화 및 현대화
     * 파일/클래스명 접미사 기본 미생성, HostBinding/HostListener 대신 host 오브젝트 권장
     * typeCheckHostBindings 옵션 추가로 바인딩 오류 실시간 감지 지원

DevTools/테스트 환경 개선 및 Angular Material

     * 증분 하이드레이션, deferrable view 디버깅 기능 Angular DevTools에 추가
     * Karma 대체로 vitest 지원, watch 모드 및 브라우저 테스트 실험적 도입
     * Material 버튼 컴포넌트 M3 스펙 부합, 용어 정비, 애니메이션/모션 제어 등 품질 개선

GenAI 및 LLM 지원

     * LLM이 최신 Angular 코드를 학습하도록 llms.txt 도입, 오픈 소스 샘플/가이드 제공
     * Genkit, Vertex AI 등과 연동 가이드, 실습 라이브 스트림, 베스트 프랙티스 공개

내장 control flow 도입과 구조적 디렉티브 폐기

     * v17 도입된 내장 control flow가 직관적, 타입 안정성, 성능 등 다방면에서 우수함
     * *ngIf, *ngFor, *ngSwitch는 v22에서 완전 제거 예정, 코드 마이그레이션 schematic 제공

공식 Angular 마스코트 프로젝트

     * Angular의 정체성과 커뮤니티 상징성을 강화할 공식 마스코트 후보 제시 및 RFC 오픈
     * 로고에서 영감을 받은 쉴드 모양, Anglerfish(아귀) 캐릭터 등 제안
     * 커뮤니티 투표 및 피드백으로 최종 선정, 이름 제안도 가능

앞으로의 로드맵

     * v20에서 리액티비티, zoneless, 증분 하이드레이션 등 대규모 기능을 다듬고 안정화
     * Selectorless, signal-forms, 유닛 테스트 개선, 마스코트 등 차기 대형 프로젝트 준비 중
     * 커뮤니티의 적극적 의견 수렴을 통해 발전 방향 결정 예정
"
"https://news.hada.io/topic?id=21203","항공사들은 단체 승객보다 1인 승객에게 더 높은 요금을 부과하고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 항공사들은 단체 승객보다 1인 승객에게 더 높은 요금을 부과하고 있음

     * American Airlines, United Airlines, Delta 등 미국 주요 항공사들이 1인 승객에게 그룹보다 높은 항공권 가격을 부과함
     * 이 현상은 주로 국내선 편도에서 발견되며, 항상 적용되는 것은 아니지만 적지 않게 포착됨
     * 여러 명 예약 시 더 저렴한 요금 클래스가 열려 1인 예매보다 큰 금액 차이가 발생함
     * 항공사들은 공식적으로 입장을 밝히지 않지만, 고객군 세분화 전략으로 해석 가능함
     * 최종적으로 출장 및 1인 여행객이 가장 큰 추가 비용 부담을 겪게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 항공사의 1인 승객 가격 차별 현상

  요약 및 배경

     * 미국의 Delta, United Airlines, American Airlines 등 주요 항공사들이 1인 승객 또는 비즈니스 고객에게 여러 명이 함께 예약할 때보다 더 높은 항공권 요금을 부과하는 사례가 확인됨
     * 이 현상은 모든 항공권에 해당하지는 않으나, 수백 개 노선을 조사한 결과 실제로 존재하고 적지 않게 나타남
     * 1인 예약 시 표준 경제 요금만 제공되는 반면, 2명 이상 예약 시 더 저렴한 요금 클래스가 추가로 열리며 금액 차이가 발생함

  구체적 예시

     * 예시 1: United Airlines의 Chicago-O'Hare(ORD)–Peoria(PIA) 구간에서
          + 1인 예약 시 $269의 편도 요금 발생
          + 2명 이상 예약 시 1인 기준 $181로 대폭 할인 적용
          + 두 명 예약 시에는 1인 예매에서 보이지 않던 Basic Economy 클래스 선택 가능함
     * United의 요금 클래스 차별 방식
          + Q economy(일반 할인)만 1인 예약 시 열림
          + S class(특별 할인) 등급이 2명 이상 예약 시 선택 가능
          + 이용 규정에 “15세 이상 성인 1인 이상 동반 시에만 예약 가능”이라는 조건 명시
          + 비싼 1인 요금에는 동반 조건이 없음
     * 예시 2: American Airlines Charlotte(CLT)–Fort Myers(RSW) 구간
          + 1인 예약 시 $422
          + 2인 예약 시 1인당 $266으로 다운, Basic Economy는 더 저렴함
     * Google Flights에서도 확인 가능
          + Chicago-O'Hare(ORD)–Lexington(LEX) 1인 예약 시 $214
          + 2인 예약 시 전체 합계 $215로 1인당 $108
          + Google Flights는 총액 표시로 인해 개별 요금 비교 시 차이 뚜렷

  현재 적용 범위 및 업계 반응

     * 이런 가격 차별 현상은 주요 3개 항공사에서 일부 국내선 편도 위주로 관찰됨
     * Alaska, JetBlue, Southwest 등에서는 아직 같은 경향이 보이지 않음
     * 각 항공사는 이러한 요금 정책에 대해 공식 답변을 하지 않음

  작동 원리 및 배경

     * 일반적으로 대량(그룹) 구매 할인은 항공사에서 드문 경우이며, 과거에는 동시 예약 시 오히려 1인당 가격이 비싸지는 사례가 많았음
     * 현재 요금 체계는 각 알파벳 별 운임 클래스로 세분되어 있어, 저가 운임이 한 자리만 있을 경우 2명 이상 예약 시엔 더 비싼 클래스만 선택 가능함
     * 그러나 최근에는 오히려 2명 이상 예약 시 더 저렴한 클래스가 열려, 통상적 흐름과 반대 현상 발생

  요금 차별의 동기와 영향

     * 항공산업의 고객 세분화(segmentation) 전략이 근본적 동인
          + 가족, 휴가 여행객, 비용 민감한 소비자, 부유한 은퇴자, 비즈니스 여행자 등 서로 다른 고객군 존재
          + 각 고객군의 지불 의지 가격 차이 반영
     * 비즈니스 여행객이나 긴급 상황 등 1인 예매자는 추가 비용을 감수할 확률이 높다고 판단
     * 실제로는 개인 사정, 친구 모임, 별도 출발 등 1인 예매 사유가 다양하지만, 결국 1인 예약 시 더 높은 금액 부담

  현상의 확대 가능성 및 결론

     * 아직 적용 범위가 제한적이나, 향후 항공사들이 더 많은 노선과 구간에 확대 적용할 가능성 존재
     * 정확한 정책 시작 시점이나 향후 계획은 불확실
     * 명확한 점은, 어떤 목적이든 1인 여행객이 추가 비용의 주된 부담자라는 사실임

        Hacker News 의견

     * 나는 사람들이 이 스레드에서 항공사를 깎아내리기 위해 이성을 잠시 멈추는 것 같다는 인상 받음. 물론, 그들이 약탈적 가격 정책의 역사를 가진 것은 사실이지만, 실제 문제는 가격 차이가 아닌 의사소통 부재임. 대량/가족 할인에 대한 안내나 광고가 전혀 없고, 단순히 가격이 다르게 표시되는 형태임. 바로 이 부분이 진짜 문제 포인트임
          + 나는 동일한 날짜에 당일 왕복 항공권을 예약하려다가 복귀 항공편 가격이 너무 비싸서 하루 일찍 출발하면 수백 유로 더 저렴하게 예약 가능했던 적 있음. 내 추측엔 당일치기 여행객 대부분이 비즈니스 목적이기 때문이고, 기업은 가격에 덜 민감하니 더 비싸게 팔아도 됨. 이런 맥락에서 개별 여행객 대부분이 비즈니스, 휴가를 즐기는 소비자라기보단 업무 목적인 셈임. 여기서 발생하는 가격 차이는 거의 절반 정도인데, 단순히 대량 할인으로 치부하기 힘든 수준임. 그래서 이걸 할인이라고 광고하지도 않고, 고객이 더 낼 수 있으니 더 받는 구조임. 이게 과연 좋은 건지, 나쁜 건지, 윤리적이거나 약탈적인지는 모르겠지만, 씁쓸한 기분 남음
          + 내 이해로는, 항공권 가격은 알고리즘이 실시간으로 산출하고, 하루에도 여러 번 바뀔 수 있음. 가격이 계속 바뀌는 상황에서 광고하기란 불가능함
          + 경험담으로, 항공사의 불투명함과 각 단계에서의 꼼수들, 경험 내내 모든 부분을 돈벌이로 바꾸려는 태도, 그리고 직원들에게 적은 급여를 주는 관행은 일종의 고착화된 산업 관행임. 이런 관행을 나열하는 것이 그저 비난하는 게 아니라, 실제로 일반화된 현실임
          + 항공권 가격이 10년 넘게 매우 개인화된 형태로 바뀌었음. 나는 여러 국가의 친구들과 자주 여행 다니는 삶을 사는데, 같은 카페에 앉아서 동일한 항공권을 같은 사이트에서 같은 시간에 검색해도, 제안되는 가격이 완전히 달라짐. 이런 이유로 항상 시크릿 모드나 사설 브라우저를 이용해 항공권을 검색하지만, 심지어 이렇게 해도 제공받는 가격이 달라짐
          + 사실 항공권 가격 대부분이 공지되지 않고 커뮤니케이션되지 않는 구조임. OTA 등 여러 독립 채널을 통해 티켓을 판매하고 있어서, 이런 할인 구조만 특별히 광고하기도 어색한 상황임. 신발 한 켤레 더 사면 30% 할인되는 것과는 완전히 다른 맥락임. 대부분의 사람들은 항공권을 한 장 더 사서 할인받으려 하지 않음
     * 이런 동적 가격 정책이 나쁜 이유는, 합법적이고 공정한 부분도 있지만, 그러한 시스템이 도입됨으로써 상당수 사람들이 시행 여부 파악, 우회 방법 모색 등에 시간과 에너지를 소모해야 하기 때문임. 본래 단순한 상품이 되어야 할 부분에 인류의 잠재력을 낭비하는 현상임
          + 포인트 시스템도 동일한 불만 있음. 돈의 가치를 제대로 얻으려면 당신이 만든 가짜 통화와 등급 제도를 알아야 하는 상황임. 가격 차별이 이루어진다는 점에서(포인트 시스템이 본질적으로 수행하는 것) 어느 정도 장점이 있긴 하지만, 포인트 쌓기에 집단적으로 쏟는 시간 낭비는 감당할 수 없다고 생각함. 모든 포인트 시스템을 불법화해야 함
          + 왜 모두가 가격 책정 방식을 역추적해야 하는지 의문임. 항공권이 본인에게 가치 있는 금액인지 판단하면 충분함. 통상적으로 비교할 수 있는 항공사, 다른 여정, 심지어 교통수단까지 다양함. 가격 할인 쿠폰이 있다고 해서 모두가 그것만 산다고 쿠폰 시스템이 나쁜 건 아님
          + 이런 산업이 정부로부터 구조조정을 반복적으로 받을 정도로 ""중요""하다면 더욱 심각함
          + 몇 가지 기본 원칙만 따르면 쉽게 좋은 항공권 가격을 찾을 수 있음. 좋은 조건을 원한다면 노력을 해야 함. 이는 할인 쿠폰 오려서 빵 가게 마감 직전 세일품 사는 것과 똑같은 구조임
     * 나는 이 가격 구조가 괜찮은 선에서 합리적이라고 생각함. 합리적 범위 내라면 원하는 방식으로 가격을 책정할 수 있음. 다른 업계의 대량 구매 할인과 다를 바 없음. 동시에, 같은 항공편에 탈 낯선 사람을 모아 함께 예약하는 서비스를 누군가 만들면 정말 재미있을 듯함
          + 무게 기준으로 요금을 받는 게 더 공정하다고 생각함. 우체국은 그렇게 운영하는데 항공사는 왜 안 되는지 의아함
          + 랜덤한 타인을 동행자 명단에 올리지 않기 위해선 프리미엄 금액도 충분히 지불할 가치 있다고 생각함
          + 혹시 환승을 놓쳐서 예상치 못한 곳에 밤을 보내게 되면, 항공사가 숙박비로 두 명이 아니라 한 명 기준으로만 방 한 개를 제공할 가능성도 있을 듯함
          + 그 아이디어 참신해서 기억해두려고 이 댓글 남기겠음!
          + 실용적이면서도 재밌는 사이트 될 듯함
     * 솔직히 이 구조는 완벽히 이해감. 항공사 수익 관리 담당으로 11년간 일해왔음. 다른 산업에선 흔한 판매 전략(B1G1, 단체 할인 등)이 항공사마다 별로 활용되지 않는 점이 이상했음(대량 예약의 경우, 보통 훨씬 큰 규모에 한해서만 할인함). 항공사들이 이제서야 이런 가격 책정 방식을 활용한다는 점이 흥미롭고, 보통 혼자 여행하는 나로선 아쉽지만, 그룹과 잠금성 높은 예약이 더 우선시되는 것도 이해됨
          + 예전엔 항공 마일리지가 단순히 비행거리로 결정됐던 시절이 있었는데, 그 구조는 정말 비즈니스적으로 이상함. 싸게 탄 사람과 비싼 비즈니스석 막차 손님이 동일하게 마일리지를 받았고, 지금은 가격 연동 방식으로 거의 다 바뀌었음. 심지어 어떤 항공사는 가장 싼 티켓은 마일리지조차 안 줌. 근데 왜 애초에 항공 업계가 탑승객 가치와 동떨어진 마일리지 공식 만들었는지는 지금도 의문임
          + 매장에서 3개에 2개 가격 식의 그룹 할인이 보이면 금방 이해가 가고, 이럴 땐 세 개쯤 쓸 일이 있으니 그냥 삼. 근데 여행할 땐 친구 불러서 같이 항공권 사자고 하지 않음. 가족과 휴가 갈 때도 일일이 각각 따로 표를 끊을 이유 없음
          + 이런 할인은 분명 대부분 다른 업종에선 광고가 이뤄지는데, 항공사가 좌석별 가격과 그룹 할인까지 노출만 해준다면 가격차에 대해 불만은 없을 듯함
          + 항공업계가 충성도에만 집착할 게 아니라, 1년간 6회 비행 같은 멀티팩, 서브스크립션형 상품판매, 24시간 취소 기간을 좀 더 유연하게 확장, 혹은 예약 3개월 전까지 취소 가능 프리미엄권 등 더 다양한 방식 가능함. 미판매 좌석 경매, 타도시 하루 이상 스탑오버해서 여행지 늘려주는 상품 등 여러 창의적인 판매 전략의 여지 충분함
          + 이런 정책이 실제 이용자에게 노출되지 않고, ""운 좋으면 발견"" 구조면, 이건 영업 전략이 아니라 가격 투명성 회피에 가까운 악질적 상술임
     * 이게 뉴스가 된 게 오히려 이상함. 오히려 진짜 뉴스를 꼽자면:
          + 대부분 왕복을 예약하는 게 훨씬 저렴한 구조(특히 주말 포함 여정)
          + 숙박 패키지로 항공권을 예약하면 완전히 다른 항공권 풀이 열리는 이중 구조(호스텔 공동 도미토리 1박이라도 실제론 숙박 안 써도 됨)
          + 대량 할인은 경제적 근거가 있으나, 위 예시처럼 같은 SKU를 더 싸게 파는 건 이해 불가능임
          + 실제로 편도 항공권이 왕복보다 비싸게 나온 적 본 적 있음. 내 추측엔, 항공사가 편도로 가는 사람은 비즈니스 고객이라고 간주함(회사에서 비용 처리하니 가격에 민감하지 않음), 반면 왕복 고객은 자기 돈 내는 경우가 많아 가격 민감함
          + 동일 구간의 직항에서 편도가 왕복보다 더 비싼 사례가 실제로 있는지 예시 요청
          + 예약만 해두고 숙박을 사용하지 않는 구조, 좋은 입지라면 워크인 고객에게 방이 돌아가기도 하겠지만, 아니면 낭비 우려 있음
     * Singapore Airlines는 오래전부터 이런 구조 사용 중(혹은 과거에 했음). ""GV2""는 2인, ""GV4""는 4인 이상이 혜택 받는 요금제임. 이게 이상할 건 없고, 많은 기업들이 볼륨 디스카운트를 하고 있음. 가족여행이 순식간에 비싸지기 때문에 이런 옵션이 이해감. 실제론 ""요금 바구니"" 시스템이 주로 단체 고객에 불리하게 작동하기도 함. 예를 들어 제일 싼 좌석 3장이 남아있으면 4명 예약 때 모두 비싼 요금대로 넘어감
     * 데이터는 없지만, 직감적으로 1인 또는 비즈니스 승객이 취소나 일정 변경 확률이 더 높고, 그런 리스크가 가격에 반영된 요금일지도 모른다고 생각함
          + 오히려 혼자 여행한다고 가족보다 더 취소 확률이 높은 건 확신 못 하겠음. 가족 단위가 오히려 문제 생길 게 더 많다고도 봄. 비즈니스 고객이라면 타인 예산 쓰고, 업무상 변수가 생기니 그럴 수 있지만, 보통 기업은 환불불가권을 권장함
          + 내 경험으론, 가족과 대서양 횡단 여행, 혼자서도 많이 함. 실제로 혼자인 경우 초과 예약된 비행기에서 자리 빼앗길 확률이 훨씬 높았음. 가족 4명보다 한 명이 빠지는 게 보상 및 재조정 측면에서도 훨씬 수월함
          + 1인 승객이 평균적으로 가격 민감도가 더 낮다는 점도 실증적 근거가 있을 수 있겠다고 추측함
          + 항공권 일정 변경, 취소가 대부분 무료가 아님
     * ""솔로 여행객 차별""이라는 표현은 대량 할인 소식을 선정적으로 포장한 느낌임. 이게 바로 클리크베이트임
          + 솔로 할인 불이익이라 해도 그룹 여행객 장려 정책임. 상대적 시점에선 둘 다 사실임
          + 항공사가 특수한 상황(예: 장례식 등)에 해당하지 않는 사람은 더 높은 요금을 부과한다는 뉴스도 있음
          + 이런 요금제 진화가 흥미롭다는 생각임. 예전엔 ""신용카드 결제수수료 할증""이 있었지만 카드사에서 금지시켰음. 요즘엔 현금/직불카드 할인으로 돌려서 소비자 모두가 만족하는 구조로 바뀜
     * 이 기반으로, 그룹 구매·비행 동반자를 위한 SNS·데이팅 사이트를 만든다면 시장 기회될 것으로 보임
          + 이미 Going과 같은 항공권 특가 정보를 제공하는 사이트 있음. 혼자 여행하니 조율 필요 없이 특가 뜨면 바로 예약 가능해서, 일반 그룹 요금보다 훨씬 싸게 예약한 적 있음(다음 여행의 경우도 솔로나 커플 기준 현재가 대비 50% 세이브함). 남과 일정 조율하면 특가 잡기 더 어려워지고, 장소·일정도 딱 맞춰야 하니 부담임
          + 좋은 아이디어긴 하지만, 실제론 사기 목적 유저나, 가족보다 약속 파기율이 높은 낯선 사람이 몰릴 리스크도 있다는 생각임
     * 나는 오히려 정반대 경험함. 일곱 장 예약하려다 가격이 한 장보다 훨씬 비싸게 나와, 티켓 수량 줄이며 수동 시뮬레이션함. 네 장 즈음에서 가격 변곡이 있었고, 두 번에 나눠 구매함. 그 뒤 1장 추가 결제하려 했더니 그 가격도 소폭 올라가 있었음. 실시간 좌석 수급 반영 및 가용 요금제 자체가 조정되는 짓 혹은 좌석 제한 구조 때문인 듯함
          + 항공사는 좌석 채우며 최대한 높은 가격을 받으려 함. 많은 좌석이 한꺼번에 빠지면 남은 고객에겐 가격을 올릴 필요 있음. 비싸서 포기하는 사람이 나와야 비행기 수익 최대화 가능
          + 나도 2인 예약에서 이 현상 겪었고, 각각 따로 예약하는 게 더 저렴함. 차이는 크진 않았지만 실제로 남음
          + 많은 항공사가 좌석을 예를 들어 10개씩 가격 바구니로 나누는 구조임. 그룹1(1-10) 100달러, 그룹2(11-20) 110달러, ... 그룹10(최상위)은 350달러 식으로 쪼갬. 단체 예매하면 마지막 N번 바구니 좌석까지 포함되어서 더 비쌀 수 있음. 초기에 판매된 저가 바구니 좌석이 취소될 경우, 마지막에 싸게 다시 풀릴 수 있어 늦게 구매해도 저렴한 좌석이 나오는 '항공권 막차 신화' 생김
"
"https://news.hada.io/topic?id=21283","MicroSD 카드의 신뢰성은 얼마나 될까? : 실제 데이터 기반 장기 대규모 테스트 결과","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           MicroSD 카드의 신뢰성은 얼마나 될까? : 실제 데이터 기반 장기 대규모 테스트 결과

     * 약 1년 10개월 동안 256개의 다양한 브랜드/제품을 대상으로 마이크로SD 카드 신뢰성 실험을 진행
     * 전체 카드의 82%에서 최소 1번 이상의 에러가 발생, 평균 최초 에러까지 2,400회, 중앙값은 1,450회 읽기/쓰기 사이클
     * 0.1% 섹터 에러 기준에서 절반 가까이 4,500회(평균), 3,100회(중앙값) 이내에 임계점 도달
     * 특정 브랜드별 내구성 편차가 컸으며, Amazon Basics, Kingston, Kioxia(일부), Lexar, OV 등은 상대적으로 우수, SanDisk, Silicon Power, Gigastone, onn. 등은 평균 이하로 평가
     * 브랜드와 상관없이 off-brand(비주류 브랜드) 카드들도 이름있는 브랜드와 유사하거나 더 나은 결과를 보인 사례도 많음
     * 카드 불량은 쓰기 금지, CSD 레지스터 손상, 데이터 전체 손상, 전원 시퀀스 실패 등 다양한 방식으로 발생
     * Amazon에서 구매한 카드가 AliExpress 구매 카드보다 전반적으로 더 나은 결과를 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

How reliable are microSD cards? Well, as it turns out...

  실험 개요

     * 약 1년 10개월 동안 256개 마이크로SD 카드(223개 테스트 진행 중, 105개는 고장까지 테스트)
     * 총 47페타바이트 이상의 랜덤 데이터를 카드에 지속적으로 기록/검증
     * 브랜드, 용량, 제품군 등 매우 다양한 카드가 대상

주요 결과 요약

  일반적인 오류 빈도와 내구성

     * 82% 카드에서 1회 이상 에러 발생: 일부는 10회 미만 사이클에서 첫 에러, 일부는 10만회 이상도 무에러(극소수)
     * 평균 첫 에러 발생 시점: 2,400회, 중앙값: 1,450회 읽기/쓰기 사이클
     * 0.1% 섹터 에러 임계점 도달: 평균 4,500회, 중앙값 3,100회 사이클(절반 가까이 이 시점에 도달)
     * 일부 카드는 3,100회 이하에서 완전 고장 혹은 임계점 도달

  브랜드별 내구성 차이

     * ADATA: 평균 2,352회로 평균 이하
     * Amazon Basics: 4개 모두 1년간 무고장, 2개는 에러 전무
     * Delkin Devices: 평균 이상, 무에러 기록(6~8개월간)
     * Gigastone: 9개 중 8개 완전 고장, 6개월 내 고장 사례 다수
     * Kingston: 15개 중 1개만 완전 고장, 산업용 제품도 SanDisk보다 우수
     * Kioxia: Exceria는 모두 조기 고장, Exceria Plus/G2는 1만회 이상 무에러, 신뢰성 상위권
     * Lexar: Micron 시절 모델 일부 제조 이슈 있었으나 전반적으로 내구성 양호
     * onn. (Walmart PB): 4개 모두 2,000회 미만 고장
     * OV (AliExpress): 3개 모두 1만회 이상 견디며 평균 이상 신뢰성
     * PNY, Samsung, Transcend: 1년 이상 테스트 결과 대부분 무에러, 신뢰성은 평균 이상
     * SanDisk/WD: 29개 중 14개 완전 고장, 갑작스러운 고장(파워 장애, 리더기 교체 등) 다수 보고
     * Silicon Power: 8개 중 5개 완전 고장, 평균 2,000회 미만
     * XrayDisk: 3개 중 1개만 고장, 성능은 낮으나 내구성은 평균 이상

  브랜드 영향 및 구매처

     * 오프 브랜드/중소 브랜드도 이름있는 브랜드와 신뢰성 큰 차이 없음, 오히려 평균은 약간 더 높음
     * 가짜/불량 플래시는 평균 2,200회로 가장 빨리 고장
     * Amazon 구매 카드가 AliExpress 대비 평균적으로 더 나은 내구성을 보임

  카드 고장 방식

     * 쓰기 금지 비트 활성화(데이터는 남음, 백업 가능)
     * CSD 레지스터 손상(카드 용량 127MB로 표시 등)
     * 전체 데이터 손상, 파워업 시퀀스 실패 등 다양한 고장 유형

결론 및 실무적 시사점

     * 마이크로SD 카드의 내구성은 카드마다 매우 다양하며, 평균적으로 몇 천회 쓰기/지우기 후 오류 가능성이 존재
     * 브랜드명만으로 신뢰성 보장 불가, 같은 브랜드 내 제품군·생산시점별 편차 큼
     * 중요 데이터 저장에는 신중한 선택과 정기적 백업 필수
     * 라즈베리파이 등 SBC, 감시카메라, 임베디드 등 연속 쓰기가 많은 환경에서는 특히 주의

   보다 상세한 수치는 작성자의 실험 결과 페이지 - On the Capacity, Performance, and Reliability of microSD Cards)에서 확인 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Reddit 의 주요 댓글들 요약

     * Raspberry Pi와 SD 카드의 궁합 문제는 과거 커널/드라이버 이슈였고, 최근에는 상당 부분 개선된 것으로 언급됨
     * High Endurance/Industrial 등 내구성 강화 모델이 일반 카드보다 훨씬 오래 버틴다는 실제 경험 다수. 특히 Samsung PRO Endurance, SanDisk High/Max Endurance, Kingston Industrial 시리즈가 좋다는 평가
     * Sandisk/Nintendo Switch 전용 카드는 신뢰성 면에서 좋은 평가, 특히 대용량(128GB 등)일수록 내구성이 높을 수 있다는 의견(단, 테스트 데이터상 대용량이 꼭 오래가는지 여부는 불분명)
     * 카드 고장의 주요 원인은 전원 차단(브라운아웃, 하드 셧다운), 발열, 품질 불량, 그리고 과도한 시스템/DB 로그 기록 등임. SD카드는 전원 중단 시 매우 취약하다는 사례 많음
     * SD 카드의 NAND 타입(SLC, MLC, TLC 등)이 내구성에 핵심적이나, 대부분의 소비자용 제품은 이를 공개하지 않음. Industrial/고급형 카드는 명확하게 명시되어 있으니 Digi-Key, Mouser 등에서 구매 추천
     * SSD나 M.2, NVMe, USB 드라이브로 부팅/데이터 저장하는 사례도 많으며, 실질적으로 속도, 내구성 모두 SD 카드보다 낫다는 의견 많음
     * 카드 운영환경(온도, 장치, 리더기 품질 등)도 수명에 영향을 준다는 지적. Pi 환경에서 SD 카드 고장은 사용 환경이나 파워, 리더기 품질에 크게 좌우됨
     * Amazon Basics 카드의 성능이 의외로 좋았다는 결과가 눈길을 끎. 많은 사용자가 저가형 브랜드에 불신이 있었으나 실제 테스트에서는 준수하게 평가됨
     * 읽기 전용/읽기 위주 사용 환경에서는 고장이 현저히 적다는 의견도 있었으나, 시간이 지나면 플래시 내부 전하 보존 문제로 결국 열화 가능성 있음
     * 가격 대비 최고의 조합으로 Kingston Canvas Go! Plus 등도 추천되고 있음(내구성+성능+가격 기준)
     * 신뢰성, 내구성만 생각하면 산업용 카드/고가형 모델이 최상이지만, 일반 사용자에겐 가격이 부담이라는 지적도 있음

        Hacker News 의견

     * 리스트에 있는 브랜드 중 상당수는 실제 제조사가 아닌 단순 브랜드명임을 언급하고 싶음, 진짜 제조사를 알아내는 과정 자체도 흥미로운 포인트라고 생각함, 이 과정은 카드 쿼리나 실물 확인(예: 뒷면 테스트 포인트 배열 등, 제조사마다 다름)으로 할 수 있음
     * 누군가 이런 테스트에 노력을 들여줘서 좋다고 생각함, 저렴한 전자제품의 품질을 소비자가 제대로 평가하기가 정말 힘든 상황임, 특히 주요 리뷰 매체가 점점 사라지고 있어서 더 어려움, Anandtech도 이제는 추억임, 대부분의 소비자가 얻는 정보는 스펙만 나열한 광고성 ‘리스트 기사’와 제휴 링크뿐임
          + 이래서 난 결국 브랜드 있는 제품을 삼, 예를 들어 Amazon에서 무명 브랜드가 절반 가격이어도 Apple 제품을 고름, 애플은 품질이나 진실성에서 신뢰감이 있기 때문임
     * 0.1% 오류율 기준에 대해 더 듣고 싶음, 실제로는 다음 세 가지 상황이 매우 다름: 1) 읽기가 오류를 리턴하지만 재시도 시 성공 2) 읽기가 오류를 계속 반환함 3) 읽기가 잘못된 데이터를 반환하는데도 성공했다고 나옴
     * 이 SD 카드들 내구성 기준으로 정말 감탄함, 컨슈머 SSD에서 용량 대비 600 드라이브 사이클(TBW 기준)이 광고되는 최고치임, 상당히 표준적인 수치임. 도박이기는 하지만 일부 SD 카드가 4000 사이클까지 버티는 결과는 매우 인상적임
       실제로 NVMe/SATA SSD들도 꽤 오래가는 경향이 있음, TechReport가 10년 전에 내구성 끝까지 테스트를 했는데, 모든 SSD가 3000사이클까지 버팀, 삼성 840 Pro는 거의 1만 사이클까지 도달함 TechReport SSD 내구성 실험
       최신 SSD도 테스트해보고 싶음, 실제로 누가 플래시를 제일 잘 만드는지(Kioxia, Micron 등) 궁금함, TechReport의 SSD DB를 보면 각 파트별로 스펙이 다 나와 있어서, 예를 들어 SanDisk/WD SN7100을 보면 Kioxia 218-Layer BiCS8 3D TLC 사용, 사실 플래시 자체를 만드는 회사는 많지 않음.
       컨슈머용 스택의 내구성 한계까지 테스트하는 것도 흥미로울 것임, 218층이라니! 오히려 지난 10년간 내구성이 올라갔는지 의문임, 예전엔 기대치보다 훨씬 더 잘 버텼었기 때문임 Western Digital SN7100 스펙
     * 결과 요약 및 약간의 추론
       중앙값 기준 SD카드의 내구성 데이터
     * 첫 오류 발생: 약 1450 읽기/쓰기 사이클
     * 실패 시점: 약 3100 읽기/쓰기 사이클(완전 실패 또는 0.1% 구간 오류)
       브랜드별 내구성(점수 높을수록 좋음):
          + 5점: 브랜드 ‘Endurance’/‘Industrial’ 시리즈
          + 2점: Adata
          + 4점: Amazon Basics
          + 4점: Delkin
          + 1점: Gigastone
          + 5점: Kingston
          + 4점: Kioxia (Plus, G2만 해당)
          + 4점: Lexar
          + 1점: onn.
          + 4점: OV(읽기/쓰기 성능 최악)
          + 4점: PNY
          + 1점: Sandisk(인수 후 버전)
          + 1점: Silicon Power
          + 4점: Transcend
          + Sandisk에서 벌어진 일은 아쉬움, 예전에는 최고였는데 지금은 최하임, 최근 Sandisk USB 드라이브를 반품했음, 지속적인 쓰기를 못 버티고 중간에 연결이 끊김
     * 이런 종합 테스트는 Storagereviews 같은 곳에서 다뤄줬으면 하는 내용임, 이미 다양한 카드를 다양한 시나리오로 벤치마크하니 내구성도 같이 다뤄주면 좋겠음
       총 드라이브 기록 3000번이면 라즈베리파이 활용에서는 꽤 높은 내구성임, 128GB 모델에서는 OS와 앱이 20~30GB밖에 안 잡으니 OS 업데이트나 DB 쓰기도 충분히 소화함
       누군가 실제로 이런 테스트를 해주는 게 반가움, 나는 개인적으로 A2 등급 endurance 카드(삼성)만 골랐음, 그동안 Sandisk 카드는 라즈베리파이에서 단 한 번도 망가지지 않았고 지금도 정상 작동함(강하게 굴린 적은 없음), 최근에는 Orange PI 5 plus로 넘어가 SSD를 씀, 성능과 내구성 관점에서 SSD가 최종 솔루션임
     * SanDisk/WD가 ‘brownout’(전압 강하)에 민감하다는 점을 실험을 통해 알게 됨
       직접적으로 SD 카드를 사용 중이던 SBC가 싸구려 USB 전원 어댑터로 인해 며칠 만에 멈추고 오동작, 제조사에 수리 문의까지 했는데, 새 전원 어댑터로 바꾸니 문제가 완전히 해결됨
       결론적으로 전원 문제가 원인이었고, 아쉬운 점은 SBC 제조사들이 저장 장치를 SSD로 전환해줬으면 좋겠음, 기본형 SSD도 TF 카드보다 훨씬 신뢰성 높음
     * SD 카드는 언젠가는 반드시 고장날 임시 저장장치로 취급함
       이 점을 받아들이면 스트레스가 훨씬 덜함, 정말 중요한 데이터라면 외부 저장에 따로 보관해야 함
       참고로 WD 인수 전 HGST 하드디스크를 썼었는데, 평소 자잘한 오류는 잘 처리하면서 실제로 완전히 고장났을 때는 읽기 전용 모드로 전환되어 데이터 전체 복구에 성공함
          + 나는 SD카드를 카메라 용도로 두 가지 트랙으로 나눠 관리함
            첫 번째는 매일 쓰는 일상용 SD카드이며, 이건 주로 카메라나 카드 리더기에서 돌려가며 쓰고 매달 한두 번씩 교체함
            두 번째는 프로젝트 단위로 쓰는 카드로, 한 프로젝트 끝나면 파일을 복사한 후 별도 보관함
            동영상은 찍지 않고 Pentax 645Z처럼 상대적으로 느린 카메라를 사용하니 속도 낮은 카드도 무리 없음, 32GB 카드를 대량 구매함
          + 참고로 지금 Seagate HDD를 RMA 보내는 중임, 매주 백업용으로 쓰던 중 백업이 절반 정도 진행된 상태에서 오류가 발생하고, 그 뒤로 계속 오류와 클릭음만 무한 반복, 포맷도 안 되고 완전히 망가짐(프리저 트릭은 안 해봄)
            SD카드는 신뢰성 낮은 저장매체 중 하나지만, 진짜로 데이터가 아쉬우면 SD카드 두 개에 두 번 저장하는 게 하드디스크 한 번보단 더 나아 보임, 물론 첫 번째 선택지는 아님
     * SD 및 microSD 카드는 원래 배터리 동작 기기에 맞춰 설계됨
       SBC는 배터리로 동작하지 않고, SD카드 고문 기계에 가까움
       대부분의 경우, SD카드의 손상 원인은 카드 자체가 아니라 파워서플라이 품질임
          + 전원 품질이 원인이라는 구체적 근거가 있는지 궁금함
            개인적으론 리눅스 배포판의 쓰기 패턴이 SD카드에는 훨씬 부담스럽다고 느낌, 디지털 카메라는 대부분 FAT/exFAT로 대용량 순차 쓰기를 하고 저널링이 없음
          + microSD 카드 문제로 질문하는 사람들은 거의 다 싸구려 전원 어댑터를 쓰고 있던 경험임
            나는 RPi 공식 어댑터와 SanDisk 일반형 카드만 사용했고, 8년 넘게 여러 대 운영 중 단 한 번의 문제도 없음
            성능 세팅이나 읽기 전용 모드도 따로 안 했고, 예전 Pi에서 전원을 뽑아 재시동도 자주 해봤지만 문제 없음
            오히려 RPi 본체가 랜덤하게 고장 났을 때도 microSD 카드는 멀쩡했음
     * 최근에는 모든 Raspberry Pi에 DietPi를 설치함, 기본 설정(RAM log 등)이 아주 뛰어나서 좋음, 그리고 라즈베리파이 외에도 다양한 SBC에 설치할 수 있다는 점도 장점임
"
"https://news.hada.io/topic?id=21196","Show HN: 모던 커맨드 라인 핸드북","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show HN: 모던 커맨드 라인 핸드북

     * 이 핸드북은 현대적인 Unix/Linux 커맨드 라인 사용법을 쉽게 설명함
     * 두꺼운 매뉴얼이나 복잡한 문서 없이 빠르게 주요 개념과 명령어를 익힐 수 있음
     * 터미널, 쉘, CLI 애플리케이션, 쉘 스크립팅까지 통합적으로 학습 가능함
     * 100개 이상의 실습 예시와 주석이 달린 세션으로 따라 하며 자신감 있게 활용 가능함
     * 5,700명 이상의 독자가 선택한, 2025년 최신 기준의 핸드북임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

모두를 위한 커맨드 라인

     * 소프트웨어 개발자, 시스템 관리자, IT 종사자뿐 아니라 일반 Linux/macOS 사용자까지 현대 커맨드 라인 환경에서 효율적으로 작업하는 방식을 제시함
     * 누구라도 편리하게 커맨드 라인을 시작할 수 있도록 폭넓은 활용법을 제공함

전체 쉘 매뉴얼이 필요 없는 이유

     * 두꺼운 쉘 매뉴얼이나 방대한 Linux 서적 없이도 빠르게 시작할 수 있도록 핵심 개념과 자주 사용하는 명령어 중심으로 구성함
     * 기본에 충실하면서 시간 절약이 가능한 안내를 제공함

쉘 그 이상

     * 터미널, 쉘, CLI 애플리케이션, 쉘 스크립팅 등 관련 지식을 분리해 따로 배우는 대신 한 번에 통합적으로 익힐 수 있도록 설계함
     * 실전 팁과 트릭을 함께 제공해 실제 업무나 개발에 즉시 적용할 수 있는 지식 습득이 가능함

자신감 있는 명령어 실행

     * 100개가 넘는 주석이 달린 실습 세션과 코드 예제가 포함되어 있어 단계별로 따라 하며 직접 경험할 수 있음
     * 명령어 사용 방식에 변화와 자신감을 더하는 경험을 제공함

최신, 그리고 수천 명의 독자

     * 4년간의 학습과 집필 경험을 집대성하여 탄생한 핸드북으로, 2025년에 맞춰 업데이트된 최신 참고서임
     * 5,700명이 넘는 사용자가 선택한 경쟁력 있는 커맨드 라인 스킬 강화서임

        Hacker News 의견

     * 정말 멋진 책이라는 인상, 다만 랜딩페이지에서 독자가 무엇을 배우게 되는지 좀 더 구체적으로 알려주면 좋겠다는 제안, 초보를 위한 책인지 아니면 bash에 익숙한 사람에게도 도움이 될 팁이 있는지 불명확했기에 샘플 페이지를 찾아내야 했던 경험, 샘플 페이지에서는 책의 내용을 훨씬 잘 파악할 수 있었기에 샘플 공개 추천, 또 ""Fresh out of press""라는 표현 대신 ""hot off the press""가 더 자연스럽다는 지적과 ""Grok the Linux command line on only 120 pages""는 ""in only 120 pages""가 어색하지 않다는 개선 제안
          + 랜딩페이지 정보가 너무 간략한 점 공감, Gumroad 페이지의 정보를 중복하고 싶지 않아서였는데 다시 생각해봐야 할 것 같다는 고민, 그리고 문구에 대한 제안에 감사, 비원어민이기에 이런 피드백 반가움
     * 사이트가 모바일에서는 약간 깨지는 문제 경험, 텍스트가 화면 밖으로 나간다는 점 지적, 또 샘플 페이지나 목차가 제공된다면 책 수준을 가늠하는 데 도움이 될 것 같다는 의견, 책을 '무료'로 받고 나중에 결제할 수도 있지만 번거롭기도 하고 $0를 선택하는 게 죄송스러운 감정
          + 피드백에 감사, 모바일 지원을 맞추려고 했는데 테스트를 덜 한 것 같다는 인정, 샘플도 바로 만들어볼 계획, 그리고 예제 페이지 링크 제공
          + 동일한 경험 공유, Firefox android와 Pixel 환경에서도 화면의 일부를 볼 수 없는 현상, 목차도 읽을 수 있으면 좋겠다는 욕구, $0로 받는 것이 저자에게 미안하다는 솔직한 심정, 하지만 책 출간 축하 인사
          + Brave에서 Android로 접속해도 텍스트가 화면 밖으로 나가는 문제 확인
     * 예제 페이지 링크 직접 공유
          + 쉘에 꽤 익숙하다고 자부하지만 샘플 페이지에서 프로세스 치환(process substitution) 등 새로운 것 배움, 바로 구매 결정
          + 예제 PDF 12페이지가 ""On Linux, the PATH looks something like this:""로 끝나는데 실제 PATH 예제가 없다는 점 지적
          + 예제 페이지가 아쉽다는 생각, 예를 들어 ""diff 유틸리티로 ls 명령 결과를 비교해 디렉터리 내용을 비교할 수 있다""는 설명이 있는데, 실제로 ls 명령을 diff에 넘기면 에러가 날 수 있기 때문에 다소 부적절한 설명, 대신 <code>diff -r directory-a directory-b</code> 명령으로 두 디렉터리를 파일별로 비교 설명이 더 적절하다는 지적
     * 이 책에 관심이 있다면 The Shell Haters Handbook도 함께 추천
          + 그리고 wizardzines.com도 꼭 참고 추천
     * 책의 초점이 대부분의 시스템에서 항상 만날 수 있는 오래된 도구(ex: find, grep)에 맞춰져 있는지, 아니면 개인이 설치해서 쓰는 최신 도구(fd, fzf, rg)도 포함하는지 궁금
          + 오래된 표준 도구에 집중, 이런 도구들은 CI 파이프라인이나 동료와 스크립트를 공유할 때 쉽게 사용할 수 있기 때문, Make를 예로 들면 언제 어디서나 적용 가능, 대안들은 언급하지만 예제 기반은 검증된 오래된 도구에 맞춰져 있음, 설치가 필요 없는 도구나 직장에서 자주 마주치는 기본 도구 중심, 그래도 최신 도구 위주 접근도 매력 있다고 생각
     * 내용은 상당히 훌륭하지만 타이포그래피(레이아웃)가 읽기에 다소 어려움, 코드 블록이 설명과 다른 페이지에 있다든지(p18/19), 콜아웃이 떨어져 있다든지(p26/27), 한 단어가 페이지 걸쳐 두 쪽에 걸친다든지(p51/52), 푸터가 여러 페이지에 걸치는 현상(p61/62) 등이 몰입에 방해가 된다는 피드백, 한 섹션을 읽으며 이해하기 위해 페이지를 자꾸 넘겨야 해서 곤란함
          + 피드백에 감사, 최선을 다해서 깔끔하게 만들려고 하지만 책을 계속 업데이트하다보니 이런 불편이 생길 수밖에 없다는 해명, 다음 업데이트 때 더욱 신경 쓸 예정
     * Linux CLI 도구(coreutils, grep, sed, awk) 학습을 위한 인터랙티브 TUI 앱 및 연습문제 모음도 제작, learnbyexample/TUI-apps에서 공개 중
     * 정말 훌륭한 작업, 리눅스 20년(정확히는 30년 가까이) 써 왔지만, 예제 페이지에서 모르는 것 배웠다는 감탄
     * 이 자료와 함께 참고하면 좋은 리소스 linuxjourney.com 추천
          + 위 사이트에서 영감 얻은 오픈소스 버전도 존재, github.com/daquino94/linux-path 안내
     * ""pay what you want"" 판매 모델이 실제로 어떻게 작동하는지 궁금, 코스 판매 차원에서 고민 중
          + 이번에 모델을 바꾼지 얼마 되지 않아 과거 데이터는 없고, 일반 세일보다는 훨씬 적은 수익 기대, 보통의 판매 방식은 마케팅이 꼭 필요, 주된 동기는 긴 시간 들여 만든 결과물을 다른 사람들과 공유하고 싶다는 점, 오래된 내용이 되기 전에 누구에게나 쓸모있도록 공개, 만약 수익이 목적이었다면 아마 AI 책을 썼을 것, 언제 한 번 책 제작 경험을 블로그에 정리할 생각도 있음
"
"https://news.hada.io/topic?id=21285","전산학은 실업률이 가장 높은 전공 중 하나임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        전산학은 실업률이 가장 높은 전공 중 하나임

     * 전산(Computer Science, 컴퓨터 과학) 전공은 미국 대학생·졸업생 인기 전공이지만, 최근 실업률이 6.1%로 최상위권에 위치함
     * 최근 빅테크 기업들의 대규모 구조조정과 채용 축소로 인해 전공의 취업 매력도가 감소함
     * 현장의 기대치와 졸업생의 역량 불일치로 신입 취업이 더 어려워지는 현상 발생
     * 너무 많은 졸업생이 공급되어 전공자 간 경쟁이 심화되고, 시장 보상과 기회 감소 현상이 나타남
     * 기업들은 더 높은 숙련도와 트랙 레코드를 원하고, 학벌·경력·포트폴리오 중심의 채용으로 전환 중
     * 진입장벽이 강화되고 무급 인턴, 저임금, 자동화, 해외 아웃소싱 등으로 초년생 취업이 더욱 힘들어지고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

컴퓨터과학(Computer Science) 전공, 높은 실업률의 현실

  최근 전공 실업률 동향

     * 컴퓨터 과학은 대학생과 최근 졸업생 사이에서 꾸준히 인기 있는 전공 중 하나임
     * 미국 Federal Reserve Bank of New York 자료에 따르면, 컴퓨터 과학은 모든 전공 중 실업률 6.1%로 7위에 해당함
     * 실제로 물리학(7.8%) , 인류학(9.4%) 등 일부 기초과학 계열 다음으로 높은 실업률을 기록하고 있음
     * 동일 그룹인 컴퓨터 공학(Computer Engineering) 은 7.5%로 실업률이 더 높게 집계됨

  산업 동향과 전공 선택

     * COVID-19 팬데믹 기간 중 기술 인력 수요 급증으로 컴퓨터 과학 전공 인기가 더욱 높아졌음
     * 그러나 Amazon, Google 등 주요 IT 기업이 최근 구조조정 및 인력 감축을 단행하며 기술직 일자리 감소 현상이 진행 중임
     * 이로 인해 컴퓨터 과학 전공의 취업 매력도가 이전보다 낮아진 상황임

  대학생 및 구직자 현황

     * Princeton Review는 컴퓨터 과학을 대학 전공 1위로 꼽았지만, 산업 내 기대치와 졸업생들의 실질 역량 차이가 나타남
     * 컴퓨터 과학 실업률이 상위권에 들며, 취업 시장의 불일치 문제 부각
     * 반면 영양과학, 건설서비스, 토목공학 등 일부 전공은 실업률이 0.4%~1%로 매우 낮음
     * 관련 통계는 The New York Fed가 2023년 센서스 데이터를 토대로 산출함
     * Gen Z 가구의 실업 급여 수령률이 1년 새 32% 증가하는 등, 최근 졸업생 전반의 실업률도 상승 추세임

  전문가 의견 및 취업 현실

     * Alex Beene(University of Tennessee at Martin): ""컴퓨터과학 등의 실업률이 높은 것은 충격적임. 전공자 수 자체가 늘어났지만, 기업이 요구하는 복합적 역량과 증명된 실적이 갖춰지지 못하면 취업이 어려움.""
     * Bryan Driscoll(HR Consultant): ""컴퓨터과학 전공은 꿈과 현실의 괴리 현상이 큼. 우수한 전공, 열심히 공부하면 안정적 고수입이란 환상이 많지만 실제로는 졸업생 과다공급, 일자리 부족, 신입연봉 감소, 학벌주의, 학자금 대출 부담 등이 큼.""
     * Michael Ryan(Finance Expert): ""코딩 골드러시로 학생들이 몰렸으나 기업은 엔지니어 예산 40% 감축, 졸업생은 최고치 도달. 시장에 공급 과잉으로 결과적으로 임금 하락 및 취업난 가중됨.""

  구조적 문제와 미래

     * 현재 컴퓨터과학 분야는 수많은 졸업생들이 신입 일자리를 놓고 경쟁하는 구조임
     * 기업은 수년 경력, 깃허브 포트폴리오, 저임금 근무 등 기본적인 요구사항마저 높이고 있음
     * Bryan Driscoll: ""학위 남발로 실제 현업에서 요구하는 역량과 취업 시스템의 괴리가 발생함. 신입직 일자리 감소, 무급 인턴 확산, 해외 아웃소싱 및 자동화 등으로 진입장벽 강화됨""
     * 졸업생들은 치열한 경쟁, 낮은 임금, 취업 기회 감소라는 삼중고를 겪는 양상임

   취업 살벌하죠. 다른 관점에서보면 1인 서비스 만들기 최적이기도 한뎅...

   2025년 모 처..

   A: 스프링, 리액트, 안드로이드, iOS 한 명 타요
   B: 각각 한 명씩 총 네 명 타면 되죠?
   A: 내가 한 명 타랬잖아요
   B:

   C: DBA+BackEnd+Middle Ware+Linux Engineer+Cloud Architecture도 가능한데 타도 되나요!?

   A: 내가 아까 한 명 타랬잖아요

   한국도 비슷한 상황 같습니다. 생물학+코딩역량 처럼 차라리 다른전공+코딩이 취업에는 유리할 거 같아요. 온갖 프레임워크와 클라우드 발달, LLM 툴 등장으로 코딩 접근성이 낮아진 만큼 (과거 어셈블리->C언어->파이썬 처럼) 코딩 역량 외에도 다른거를 할줄 알아야 채용시장 진입이 되는거 같습니다.

   코로나 시기는 프로그래밍 산업에 다시 오지 않을 고점이었지 않나 싶네요...

   프레임워크 쓸 줄 아는 인력들만 급증하고. 그런 프레임워크를 만들 줄은 모름. 이게 문제의 핵심 아닐까요

        Hacker News 의견

     * 나는 15년 경력의 CS 교수로서 지난 20년간 컴퓨터 과학 전공자 급증은 본질적으로 컴퓨팅에 대한 진실한 관심 때문이 아니라 높은 연봉을 쫓는 분위기 때문이라고 느끼는 상황. 준비가 안 된 학생들이 몰려들고, 대학들은 이들을 붙잡기 위해 커리큘럼을 점점 쉽게 만드는 방향으로 바뀌는 현실. 만약 연봉만 보고 온 학생들이 자연스럽게 걸러진다면, 진짜 배움에 열정이 있는 학생들과 다시 묵직한 컴퓨팅 교육을 할 수 있다는 점에서 오히려 반가운 일이라는 생각
          + CS과 대학들이 이런 현상에 일부분 책임이 있다고 보는 시각. 진짜 컴퓨터 과학에 관심 없는 학생들도 무리하게 받아주고 졸업까지 시키는 구조. 진짜 CS 교육이 목적이라면 소프트웨어 개발, 엔지니어링, 애플리케이션 디자인, UX 등으로 학과를 다양화하고, 대다수 학생을 그쪽으로 보내야 한다는 생각. 진정으로 컴퓨터 과학을 배우고 싶은 사람만 CS 커리큘럼에서 깊이 있게 다루는 것이 모두에게 더 나은 선택이라는 확신
          + 20년 전 내가 대학 다닐 때도 게임처럼 CS 선택하는 학생 많았던 회상. 하지만 첫 학기부터 형식 논리 배우고, 그 다음엔 gdb로 세그먼트 오류 잡다가 대부분 전공을 떠나는 현실. 요즘은 C++ 대신 학생들이 접근하기 쉬운 Python 위주로 수업이 진행되는 곳들이 많다는 점이 체감됨
          + 여러 코드 캠프 출신 지원자들을 인터뷰해 본 결과, 대부분 연봉만 쫓는 자세를 볼 수 있음. 컴퓨팅 자체에 대한 열정이나 깊은 지식은 거의 없이 얕은 수준에 머물러 있으면서 초봉부터 여섯 자리 연봉을 요구. 15분도 안 걸릴 정도의 간단한 과제만 내도, 샘플 코드 복붙만 하던 지원자들이 전부 걸러진다는 경험 공유. 퍼포먼스 문제를 일부러 내줘서, 진짜로 소스의 원리를 이해하지 못하면 너무 느려서 통과 못 함
          + 이 얘기를 공유해줘서 고맙다는 입장. 지난 2010년대 들어 숫자가 급증한 이런 스타일의 입문자들 때문에 소프트웨어 업계만의 독특한 재미와 혁신, 호기심, 발견의 문화가 다 죽은 기분. 최소 요구조건만 수동적으로 채우는 일명 “티켓 컴플리터”가 많아지고 있고, 본질적 논의에는 전혀 관심이 없는 분위기. 해커, 괴짜들이 오히려 금덩이처럼 소중해진 현상. CS가 인기 떨어지는 계기가 된다면 오히려 환영한다는 태도
          + 연봉만 보고 CS에 들어온 사람 걸러진다 해도, 현실은 그렇지 않다는 관점. 나는 진심으로 관심이 많아 고급 과목도 많이 들었고, 15년 전 커리큘럼 참 좋게 느꼈음. 다른 학교에서 MSIS도 땄지만, 학부 과정보다 진도가 빠를 뿐 더 어렵지도 않고 코딩량도 줄었다는 인상. 꽤 오랫동안 업계에서 잘 지냈지만 이젠 실직 위기이고, 앞으로 IT 분야에서 취업을 계속하기 힘들어 보여서 Walmart 같은 곳에서 일하게 될 것 같은 진심 어린 이야기
     * 2000년과 2009년에도 비슷한 상황을 겪었던 회상. 이 업계는 늘 붐과 침체가 반복되는 구조. 경기가 나빠졌을 때는 항상 “다시 살아나지 못할 것”이라는 말이 나오지만 결국 극복. 나는 코로나 기간에 채용 담당자로 일하며 정말 황당한 이력서 많이 봤음. 웹 부트캠프만 수료하고 6자리 연봉 요구하는 지원자가 넘쳤고, 전혀 이 필드에 어울리지 않는 사람들도 쉽게 합격. 저금리 때문에 쉽게 돈이 돌던 시기였던 점이 배경. 기본적으로 이 직업은 예전부터 좋은 연봉을 약속했지만, 최근엔 미팅에만 앉아 프레임워크 끌어다 갖다 붙이는데도 억대 연봉을 받는 비정상 현상 있었음. 지금은 정리해고가 진행 중이고, 좋은 인재까지 타격받지만 결국 진짜 실력 있는 사람은 돌아올 것이고 아닌 사람들은 스스로 떠날 전망. AI 원망 그만하고, 진짜 어려운 AI 공부하는
       게 현명한 선택이라는 조언
          + 2000년, 2009년에도 똑같은 일이 있었음을 상기시키는 말. 전혀 실력 없는 사람이 IT Director 자리를 꿰차는 것도 봤고, 그런 현상은 오래가지 않을 거란 것도 확신했음. 불편한 상황이라도 자존심 잃지 말고, 기술적 역량이 진짜라면 시대를 타지 않고 늘 가치를 발휘한다는 실제 경험 공유. ChatGPT가 쉽게 답을 준다고 해도 진짜로 잘 활용하려면 근본 원리를 이해해야 의미 있다는 당부
          + 실직의 주범이 AI가 아니라 아웃소싱이라는 주장. 동유럽에서조차 프로젝트가 더 값싼 인도, 베트남, 필리핀 등으로 옮겨가고 있다며 현실 진단
     * 원래 기사 통계 출처를 뉴욕 연방은행 연구 링크로 제공. 영양과학 전공의 실업률이 0.4%라고 해도 그 숫자가 모두 영양사로 취업했다는 의미가 아니며, 학위와 무관한 단순 직업도 포함이라서 실업률보다 45% 이상이 '과소고용' 지표에 해당되는 점을 지적. 반대로 인류학처럼 실업률 9.4%지만, 이들도 다 삶이 어려운 건 아니고 집안 경제를 기대며 전공 특성상 일자리 자체가 적다는 점도 감안해야 한다는 설명. IT 전공은 실업률은 높은 편이나 과소고용 비율이 매우 낮아(하위 5위) 통계 해석이 단순하지 않다는 의견. 기사에 나온 “요즘 아이들은 노트북만 있으면 제2의 Zuckerberg가 된다고 생각한다”는 류의 피상적 인용구가 그리 큰 의미가 없다는 비판적 시각
          + IT 실업률은 높지만 과소고용률은 낮다는 점이 굉장히 중요한 지점이라는 동의. 실업률 6%인 분야가 과소고용률이 60% (형사 사법, 공연 예술, 의료기술 등)인 분야보다 경제적으로 훨씬 나은 환경이라는 의견
     * IT 전공 실업률 증가 원인에 대해 분석
         1. 과잉 배출: 지금은 인문계 쪽 작은 대학도 15~20%가 컴퓨터 사이언스 전공자. ""코딩 배우면 취업 잘된다""는 조언이 너무 많은 이들에게 퍼진 결과
         2. AI...는 아직 완전 대체품은 아니지만, 경영진은 인건비 절감이라는 유혹으로 성능과 상관없이 계속 밀어붙이고 있음. 기대 수준은 점점 올라가고, 팀은 축소되고, 주니어 자리는 사라지는 중
         3. 업계 평판 실추: 2009년에는 월스트리트와 달리 ‘착한 부자’ 이미지였으나 지금은 세상의 원흉 취급까지 받는 분위기로, 업계 리더들의 책임도 큼. 대형 테크 회사들은 여전히 강력하지만, ‘데이터 사이언티스트’나 개발자 수요가 예전만 못함
          + “코딩을 배우라”는 조언 자체는 훌륭하다고 생각. 다만 코딩 실력 하나만으로 좋은 직업을 쉽게 얻을 수 있다고 믿는 것이 잘못이라는 주장
          + 현장 체감상 주니어 개발자 자리의 실종이 현실로 보인다는 의견. AI 때문만은 아니라, 시니어들이 많이 풀리면서 굳이 주니어를 채용할 이유가 없어졌다고 봄. 부트캠프 등에서 양산되는 주니어 개발자는 요즘엔 실제로 현명한 선택이 아니라고 보는 경향
          + 모든 사람에게 ‘코딩 배우라’는 조언은 유효하다고 강조. 생물학 전공자라도 직접 쿼리 짤 수 있으면 연구 생산성이 비약적으로 오른다는 예시
          + 핵심 문제는 결국 1번 과잉 배출이라는 견해. 20년 가까이 “CS만 전공하면 누구나 1억 연봉 받는다”는 인식이 퍼지면서 부모와 대학들이 억지로 학생을 끌어들이고, 허술한 프로그램과 졸업율로 이익을 챙긴 결과, 면접에서 for루프조차 못 짜는 졸업생 양산되었다는 지적
          + ‘코딩 배우라’는 조언조차 사실은 “프로그래머 부족”이라는 거짓 해소를 위한 마케팅이었고, 실제로 유행시킨 교육 콘텐츠의 기술도 HTML, CSS, Javascript 같은 자동화에 가장 취약한 분야 위주였다는 주장. 코로나 전의 과도한 웹 개발자의 초고연봉 현상은 지속 불가능성이 명확했다는 회고. 이제는 ‘돈 벌이’에만 몰려온 인플루언서들도 조용해진 상황
     * “노트북 들고 있는 애들 다 자기가 Zuckerberg 되는 줄 안다. 하지만 디버깅 하나 제대로 못 한다”는 식의 발언은 반복적으로 봐온 클리셰라는 주장. 또한 취업률 통계에서 전공 무관 아르바이트(예: 맥도날드)도 취업으로 잡는 현실에서, 진짜 전공 관련 직종 취업 통계를 별도로 봐야 한다는 주장
          + 전공 무관으로 알바해도 그건 ‘과소고용’ 개념에 해당. ‘실업’은 적극적으로 일자리를 찾는데 못 구하는 경우고, ‘과소고용’은 전문성을 충분히 활용하지 못하거나 희망 근무시간보다 적게 근무하는 경우
          + 이런 수치는 본질적으로 측정이 매우 까다롭다는 점. 본인도 전공 직군(공학 계열)에서 실제로 일한 기간이 짧았지만, 전문가적 기준에서 ‘과소고용’이라고 느껴본 적은 거의 없었다는 경험 공유
     * 17년 전 소프트웨어 엔지니어링 업계를 떠나 고교 교사로 일하기 시작했던 본인의 이야기. 고등학교 컴퓨터 사이언스 교사로 근무할 때 이미 CS 인력 부족이 심각하다는 얘기를 교육계에서 많이 들었지만, 현장에서는 실제로 일자리가 많지 않아 떠난 경험을 얘기하자, 주위 동료들이 오히려 나를 이상하게 생각했다는 일화
     * 이 업계는 본질적으로 호황과 불황이 반복되는 구조. 큰 회사들의 대규모 채용 파티는 대부분 광고 수입에 기대는데, 경기 악화에선 광고 예산이 가장 먼저 줄어들기 때문에 현재 경기상황이 더 나빠질 전망
     * 20년 전: 무조건 CS 전공해라, 지금 제일 인기! 10년 전: 최소 석사 아니면 지원도 하지 마라 2년 전: 인력 포화, 신규 채용 없음 1주일 전: 무조건 ML 공부해라, 지금 제일 인기!
          + 시장이라는 것은 원래 이렇게 끊임없이 변동이 있다는 통찰을 짧고 명쾌하게 덧붙임
     * 특별히 기술적 흥미 없이 ‘쉬운 돈’만을 노리고 진입하는 사람들이 현실의 어려움에 처음 부딪히고 있다는 관찰
     * 졸업생 중 상당수가 학위 종이의 가치조차 부끄러울 만큼 준비가 안 되어있다는 비판적 목소리. 그래서 채용 절차가 너무 길어지고, 검증도 복잡해졌으며, 수백 명 지원자를 걸러도 평균 이하가 남고, 수천 명 중에서 드물게 진짜 인재를 찾는 구조. 일자리가 전혀 없는 건 아니지만, 졸업장만으로는 결코 취업이 보장되지 않는 시대. AI 채용 도구로 인해 지원자 쪽에서도 부정행위가 많아지고, 과도한 필터링 과정에서 훌륭한 인재도 묻히는 아이러니. 결국 네트워킹과 입소문 추천이 가장 신뢰도가 높아졌지만, 이것이 커리어 초기자들에게는 상당한 불이익이 되고 있다는 씁쓸한 현실 진단
"
"https://news.hada.io/topic?id=21225","신용카드 단말기에서 root shell 획득","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        신용카드 단말기에서 root shell 획득

     * Worldline Yomani XR 신용카드 단말기의 보안 연구를 위해 리버스 엔지니어링을 시도함
     * 내부 분해 과정에서 물리적 침입 감지 기능은 잘 구현되어 있으나, 소프트웨어적으로 루트 셸이 외부 시리얼 포트에 노출되는 문제가 발견됨
     * 메모리 언납, 펌웨어 추출 및 분석을 통해 리눅스 3.6 커널 기반의 시스템 구조와 매우 오래된 구성요소들을 확인함
     * 시리얼 콘솔 포트를 통해 장치를 분해하지 않고도 쉽게 루트 권한 획득 및 악성 코드 설치가 가능함을 입증함
     * 전체 결제 및 인증 관련 중요한 작업은 별도의 보안 프로세서에서 이루어져, 실제로 중요한 데이터 노출 위험은 크지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * 본 프로젝트는 결제 단말기의 보안 연구 목적으로 Worldline Yomani XR 모델을 역분석하는 과정을 중심으로 함
     * 스위스 전역에서 널리 사용되는 이 모델은 현재 단종이지만 여러 대형 소매점과 소규모 매장에서 여전히 사용 중임

첫 관찰 및 하드웨어 분해

     * UI 탐색, 포트 스캔 등 기본적인 조사 이후 하드웨어 분해를 시작함
     * 여러 PCB, 소재가 잘 구성된 하우징, 커스텀 ASIC 기반의 듀얼 코어 Arm 프로세서 등, 상당한 하드웨어 보안성이 관찰됨
     * 주요 SoC는 Samoa II란 코드명의 전용 ASIC이며, 외부에 플래시와 RAM이 탑재됨

침입 감지 및 위변조 보호

     * 하우징을 열지 않고도 압력 감지식 보드 연결(Zebra strip) 불량이나 나사 풀림만으로 침입 이벤트가 감지됨
     * 배터리를 통해 전원 차단 상황에서도 감지가 유지됨
     * 주요 PCB에 지그재그 동선(트레이스) , 카드 리더에 감싼 플렉시블 PCB 등이 손상 시 침입 탐지됨
     * 잠시 해체 후 재조립 시, 침입 감지로 인해 장치는 빨간 ""TAMPER DETECTED"" 화면만 표시하며 외부 입력에 응답하지 않음

칩-오프 펌웨어 추출

     * 온보드 플래시를 디솔더링하고 직접 연결해 펌웨어를 추출함
     * 데이터는 암호화되지 않았으나, 독특한 ECC 구조와 YAFFS2 파일 시스템의 메타데이터 배치가 확인됨
     * 파일시스템 리더를 구현하여 전체 파일 목록을 확보함
     * 시스템은 2010년 버전 Buildroot 기반 3.6 커널 사용, uClibc와 busybox, 오래된 라이브러리들이 다수 존재함

우연히 루트 셸 발견 과정

     * 펌웨어 분석 후, 플래시를 다시 연결하고 시리얼 콘솔 회선을 찾아 논리 분석기로 신호를 잡음
     * 부트 로그와 함께 로그인 프롬프트가 노출됨
     * ""root"" 입력 시 암호 없이 즉시 루트 셸 진입이 가능함
     * 실제로 이 시리얼 포트는 외부에서 커버만 열면 바로 접근 가능함
     * 공격자는 단말기를 분해하지 않고도 신속하게 접속, 악성코드 주입 등이 가능

얼마나 심각한 문제인가?

     * 부트 및 시스템 구성 분석 결과, 리눅스는 네트워크, 업데이트, 일부 비즈니스 로직만 담당함
     * 카드 결제, PIN 입력, 디스플레이 등 모든 보안 관련 기능은 별도의 mp1 프로세서가 관리함
     * 리눅스(mp2)는 항상 부팅되며, 이후 서명·암호화된 이미지가 보안 부트로더에 의해 보안 코어로 적재됨
     * 내부적으로 보안 코어 보호는 멀쩡히 동작 중이며, 보안 핵심 데이터는 루트 셸 노출로 유출되지 않음

공개 및 보고 일정

     * 2024년 11월 14일: 루트 셸 발견
     * 2024년 11월 15일: 제조사에 90일 공개 예정과 함께 알림
     * 2024년 11월 18일: 제조사로부터 보고 확인 회신
     * 2025년 6월 1일: 프로젝트 공개

결론

     * 외부 시리얼 포트를 통한 루트 셸 노출은 명백한 불필요 공격면이자 엔지니어링 실수임
     * 다만 보안 프로세서 분리 설계 덕분에, 실질적인 결제 정보 노출 위험은 낮음
     * 루트 로그인 허용이 프로덕션 펌웨어에 실수로 적용됐거나, 버전에 따라 다를 수 있음
     * 연구 과정에서 일부 장치는 루트 로그인 기능이 아예 비활성화된 것도 존재함
     * 내부적으로 해당 문제는 이미 인지 및 수정됐을 가능성이 있음

   이 프로젝트는 여러 흥미로운 결과를 남겼으며, 펌웨어 심층 탐색의 가치도 다시금 보여줌

        Hacker News 의견

     * 젊은 개발자들에게 한마디, “Hacker News”의 해커는 바로 이런 의미라는 설명, 이 게시글이 전형적인 해커의 여정을 쉽게 단계별로 분석한 예시라는 추천, 비슷한 사례를 더 알고 싶다면 Hack-a-day를 참고해볼 만하다는 안내, 글쓴이가 호기심 많고 기본 지식도 아주 탄탄한 인물처럼 보인다는 평, 본질적으로는 칩 데이터시트 조사, 손상 없이 디솔더링, 메모리라면 와이어로 재연결, 임기응변과 시행착오, 다음에는 얕은 구멍을 뚫을 때 핀홀 카메라를 써서 훼손 흔적을 피하는 방법도 고려해볼 만하다는 조언, 만약 글쓴이가 훼손 방지 체크도 뚫고 정상 작동까지 시도했다면 정말 흥미로웠을 거라는 아쉬움 공유
          + 해커라는 용어는 컴퓨터 보안에만 국한된 것이 아니라 훨씬 포괄적인 의미와 철학적 정의를 가진다는 설명, Guy Steele 등이 정리한 Jargon File을 인용해 해커의 정의를 소개, 예를 들면 “프로그래밍 시스템의 세부를 배우고 그 한계를 넓히는 걸 즐기는 사람”, “프로그래밍 자체를 열정적으로 즐기는 사람”, 그리고 “특정 프로그램에 능한 전문가”까지 다양한 의미로 쓰인다는 요약, 아마도 Hacker News 창립자 PG도 이 넓은 정의를 염두에 두고 이름을 지었으리라는 추정, 해커 용어의 역사에는 The UNIX-HATERS Handbook도 꼭 언급해야 한다는 첨언
          + 첫 번째 문장에 절대 공감, 오늘날 한 번만 더 LLM 래퍼가 나오면 정말 질릴 것 같다는 감상
          + 이 사이트가 VC 스타트업 인큐베이터에서 나왔다고 해킹 보안을 의미한다고 보긴 힘들다는 견해, 아마도 “move fast and break things”라는 스타트업식 해킹, 즉 빠르게 코드를 쏟아내며 문제를 돌파하는 스타일이란 뜻에 가깝다는 해석
          + 오랜 기간 눈팅만 하다가 최근에야 계정을 만들어 댓글을 달기 시작했는데, 이런 정보와 실행력이 돋보이는 게시물이야말로 Hacker News의 ‘해커’ 정신을 보여준다는 찬사
          + 이 글이 실제로 해킹에 관한 내용이라 처음으로 추천 버튼을 눌렀다는 고백, 평소엔 댓글에만 추천을 주지만 이 글은 예외였다는 말
     * $2 USB 카드 리더로 가짜 신용/직불카드 거래 시뮬레이션이 가능하다는 소개, 모든 명세와 프로토콜이 방대하지만 공개되어 있어 문서만 읽으면 구현할 수 있다는 설명, 하지만 실제 거래를 승인 받으려면 인터넷을 통해 은행에 데이터를 보내야 하고 그렇게 하면 곧바로 당국(예: FBI)이 출동할 수밖에 없다는 경고, 카드 리더 자체에는 거의 보호 장치가 없으며(대부분 리눅스와 약한 비밀번호 사용), 보안성은 상점과 은행 간의 계약과 규정에서 비롯된다는 설명
          + 카드 리더에 ‘보호 장치가 없다’는 주장에 반박, 실제로는 서명된 바이너리만 실행할 수 있고, 실행 파일 시스템은 읽기 전용, 데이터 파일 시스템에도 noexec 플래그, 루트 로그인 비활성화, 기능 축소된 busybox, 키는 부팅 시 보안 구역에서 적재, 마스터 키 삽입도 공장에서만 가능, 부팅 자체도 매우 안전하고, 훼손 감지 시 칩 자체가 공백으로 초기화되는 구조라는 상세 설명, 저가형 미인증 단말기는 보안이 거의 없을 수 있지만, EMV 개발 경험자로서 실제 단말기는 거의 완벽하게 잠겨 있다는 경험담 공유
          + ‘상점과 은행 간 규제가 유일한 보안’이라는 부분은 정확하다는 인정, 그래서 사람들이 휴대용 카드 리더로 비접촉 카드에서 돈을 훔친다는 음모론은 실행에 옮기기 힘들다는 의견, 실제로 거래를 만든다 해도 이후 절차나 이전 준비 단계 때문에 돈을 안전하게 빼가는 건 거의 불가능에 가깝다는 지적, 특히 요즘은 거래 푸시 알림 덕분에 범죄 시도가 드러날 가능성이 높다는 추정
          + 카드 리더가 현장에서 해킹되어 캐시된 CC정보를 읽거나 인터셉트형 악성코드가 설치되는 상황이 더 걱정이며, 이 분야의 연구가 중요한 이유라는 의견
          + 거래는 은행에 인터넷으로 전송해야 승인된다는 내용은 사실과 다르다는 설명, 은행이 카드 거래를 처리하는 오픈 API를 인터넷에 갖고 있지 않다는 지적
          + 카드 리더 보안은 약하다는 주장에 동의하지 않으며, 실제로 상점 단말기엔 은행 및 결제 네트워크 키를 안전하게 저장하는 보안 하드웨어가 내장되어 있다는 설명, 만약 그 키가 유출되면 합법 거래를 위조하는 게 가능하다고 덧붙임
     * Stripe M2 리더 36개를 구매해 쓰다 7개가 고장난 경험담, 2개는 배터리가 충전을 못하고 1개는 NFC 스캔 불가, 4개는 ‘훼손됨’ 오류를 내며 사망, 표면적으론 불량률이 심각하지만 사용 일수와 연식 등 맥락도 따져봐야 한다는 이야기, 실제로 1~3년 된 리더를 총 9일만 사용했음에도 7개나 고장났다는 점이 더 심각하다고 느낀다는 불만, 이동 시엔 각각 하드쉘 케이스와 폼 인서트로 철저히 관리하고 있어 보관 불량도 아니라고 함, 그럼에도 Stripe M2 리더가 여전히 가장 쓸 만한 선택지라 어쩔 수 없이 사용 중이라는 현실 설명, 참고로 회사가 축제 결제 담당이라 단기간만 대량으로 사용하게 된 배경도 추가
          + 다음 행사 전에는 반드시 완충 상태로 보관할 것을 권장, 대부분 배터리가 낮은 충전 상태로 오래 방치되는 걸 싫어하고, 훼손 감지 기능도 배터리가 정상이어야 작동할 가능성을 언급
     * 물리적으로 훼손 방지가 활성화되면 루트 셀이 열릴 수도 있다는 상상, 즉 시스템이 필요한 모든 암호키를 가진 보안 모드이거나, 아니면 루트 셀이 열려서 디버깅/고장 분석용으로 전환되는 방식일 수 있다는 추정, 단 이때는 중요한 프라이빗 키가 자동으로 삭제되었기를 바란다는 요청
          + 본인도 루트 셀을 통해 새 키를 플래시해서 재사용이 가능할지 궁금해졌다는 의견, 단말기가 퇴출 예정이면 중고로 얻는 것도 너무 어렵지 않을 것 같다는 기대
     * ‘루트 셀이 열려도 카드 정보가 위험하지 않다’는 원글 인용과 함께, 보안 설계자에겐 꼭 읽어볼 만한 자료라는 추천
          + 단말기에 물리적 접근이 가능하면서 루트 권한까지 있으면 카드 번호 읽기가 불가능하다는 건 전혀 납득이 안 된다는 견해, 보안에서 물리적 접근과 루트 접근은 곧 해킹 성공을 의미한다는 냉정한 현실 인식
     * (훼손된) 리눅스가 “compromised mode” 코드와 mp1 보안 시스템 중 무엇을 읽어올지 결정하는 구조라는 의견, 부트로더 자체가 보안하다고 해도 어떤 환경에서 실제로 실행되는지에 따라 의미가 달라질 수 있다고 지적, co-processor가 Secure Enclave 역할을 할 수 있지만 리눅스가 별도의 부트로더를 올릴 수 있는 구조라면 보안상 심각한 문제가 될 수 있다는 우려
          + 별도 부트로더는 로딩 불가능하다는 반박, 직접 tamper 후 부트로더를 조작해봤지만 정상적으로 부팅되지 않았으며 제3의 boot ROM이 검증하는 구조로 추정, 또한 리눅스는 tamper 상태와 관계없이 loadercode와 mp1.img를 항상 함께 로딩하고, tamper 상태에 따라 (무결성 보호된) loadercode 내부에서 분기하도록 설계되어 있다는 부연
     * 초보라면 최신 안드로이드 기반 신용카드 단말기를 먼저 시도해보라고 추천, 터치스크린으로 핀을 입력하니 더 재미있다는 유혹
          + 터치 컨트롤러는 일반적으로 보안 프로세서가 제어하는 MUX에 연결되어 있고, 보안 정보 입력 시 터치 입력이 곧장 보안 프로세서로 전달되어 안드로이드 계열 운영체제가 관여하지 않는다는 설명
          + 터치패드에 표시되더라도 PIN 데이터는 트러스트 존에서 실행된 펌웨어가 관리하며 암호화 상태라는 사실, 사이에 위치한 앱들은 PIN을 볼 수 없다는 강조
          + 이런 안드로이드 단말기를 해킹해도 보안 설계가 동일하다면 카드 자체에서 복잡한 암호화를 진행해서 실질적으론 쓸 수 있는 정보가 없을 거라는 전망, 이런 공격은 신용카드 리더만 남았을 때나 먹히며, 그런 단말기는 사용자에게 이미 스키머 위험 신호가 될 것이라는 의견
          + 인도에서 쓰이는 안드로이드 단말기는 아직도 Android Oreo(2021년 1월 지원 종료)로 동작한다는 점을 짧게 언급하며 흥미롭게 여기기
     * 단말기 내부 분석을 하자마자 바로 열고 훼손 모드를 트리거하는 것이 의아하다는 반응, 대부분 리더기에 훼손 감지가 있다는 점을 몰랐던 것 아니냐는 추정, 훼손 모드에서 테스트하는 것은 의미가 줄어들 수도 있고, 실제로 훼손 트리거 이후엔 초기화를 위한 셀이 열리는 것 아닐까 하는 궁금증, 마지막으로 굳이 처음부터 열어보는 게 당연한 흐름인지는 다시 한 번 생각해 보라는 조언
          + 처음에는 하드웨어, SoC, 인터페이스, 플래시 등 기본 정보를 파악해야 할 것 같아 열어봤다는 설명, 사전 조사 없이 접근하면 너무 어둠 속에서 시작하는 느낌이라 그랬다는 고백, 돌이켜 생각해보니 디버그 커넥터만 살짝 건드려도 끝났을 수도 있었다는 아쉬움, 그리고 두 번째로 온전한 단말기에서도 셸을 얻는 데 성공했다는 추가
     * ‘철저한 훼손 방지에도 불구하고 우회할 거리와 흥미로운 흔적이 여전히 많이 남아 있다는 점이 놀랍다’는 찬사, 보안 파트는 안전하게 마비되는 게 맞는 설계라고 평가
          + 강화된 프로세서(mp1)는 여전히 침투 불가 상태라는 점을 강조, 실제로는 문자 스트링만 display_tool 바이너리로 전달해 다른 프로세서와 메시지를 주고받는 구조임을 설명, 카드리더나 키패드 등은 리눅스에서 직접 접근 불가하며, 완전히 분리된 mp1 프로세서가 카드·PIN 입력·화면 표시 등 ‘보안’ 처리를 전담, mp2 리눅스는 네트워크·업데이트·비즈니스 로직만 처리한다는 설명
          + 리눅스 쪽이 tamper 이벤트 처리를 감지하는 역할을 할 수도 있지만, tamper 발생을 완전히 탐지한 후에야 보안키를 삭제할 수 있다면 루트 셸을 미리 따 놓고 tamper를 우회하는 흐름도 가능할 수 있어 위험하다는 시나리오 제시
     * 이런 단말기가 유럽 전역에 퍼져 있다는 사실, 스위스는 확실치 않지만 자신이 아는 많은 유럽 지역에서는 신용카드 소지가 드물다고 느낀다는 경험, 대신 POS 단말기는 온갖 종류의 카드를 읽을 수 있어 ‘POS 시스템’이라고 부르는 게 적절하다는 의견, 그래도 게시글은 재미있게 읽었다는 감상
          + 카드 여러 장을 지갑에 들고 다니는 걸 정말 싫어한다는 불평, 이미 여러 이유(결제용 외에도)로 지갑이 터질 지경이라 굳이 휴대폰이나 스마트워치에 더 추가하고 싶지도 않고, 기기 분실 시 개인정보 유출이 너무 치명적이라고 생각, 취향이지만 메카닉 시계를 선호하고, 그런 이유로 카드 간소화 방식을 안 쓴다는 솔직한 고백
"
"https://news.hada.io/topic?id=21200","typed-ffmpeg - FFmpeg에 대한 타입드 인터페이스와 시각적 필터 에디터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            typed-ffmpeg - FFmpeg에 대한 타입드 인터페이스와 시각적 필터 에디터

     * typed-ffmpeg는 Python에서 FFmpeg를 직관적으로 사용할 수 있도록 하는 최신 래퍼임
     * 강력한 타입 지원과 풍부한 문서화 및 IDE 자동완성으로 개발 경험과 코드 안정성 향상 기능 제공
     * 복잡한 필터 그래프 구성을 쉽게 만들고 시각화 및 JSON 직렬화 등 고급 기능을 지원함
     * 설치와 실행이 간편하며, 인터랙티브 플레이그라운드를 통해 브라우저에서도 실험 가능함
     * ffmpeg-python에서 영감을 받아 기존 문제점 보완, 앞으로 더 다양한 FFmpeg 버전과 필터를 지원 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

typed-ffmpeg 개요 및 중요성

     * typed-ffmpeg는 FFmpeg를 현대적인 Pythonic 방식으로 제어할 수 있는 오픈소스 패키지임
     * Python 표준 라이브러리만으로 만들어져 의존성이 없고, 높은 보안성과 호환성 제공 특장점이 있음
     * 최대 장점은 풍부한 타입 지원과 간단한 문법, 자동 완성 및 인라인 문서 등으로 복잡한 FFmpeg 필터 작업을 쉽고 안정적으로 코드화할 수 있음
     * 기존 ffmpeg-python의 제한, 특히 IDE 연동 및 타입 부재를 보완하며, JSON 직렬화, 필터 그래프 자동 검증, 시각적 그래프 등 다양한 신규 기능을 지원함
     * 실제 래핑 및 자동화 과정에서 GPT-3와 Copilot이 개발 생산성에 기여했으나, 코드 생성의 핵심은 신뢰성 있는 전통적 코드 생성으로 접근한 점도 특징임

주요 기능(Features)

     * 의존성 없음: Python 표준 라이브러리만 사용함
     * 직관적 인터페이스: Python 코드로 FFmpeg 필터 그래프를 쉽게 구성 가능함
     * 포괄적 필터 지원: 대부분의 FFmpeg 필터와 IDE 자동완성 지원함
     * 통합 문서화: 인라인 docstring으로 외부 문서 참조 필요성 최소화함
     * 견고한 타입 지원: 정적/동적 타입체크로 코드 신뢰성과 디버깅 용이성 강화 기능 제공함
     * 필터 그래프 직렬화: JSON 형식으로 필터 그래프 저장/복원 가능함
     * 그래프 시각화: graphviz를 통한 필터 그래프 도식화 기능 제공함
     * 자동 검증 및 보정: 필터 그래프 내 오류 탐지 및 자동 교정 기능
     * 입출력 옵션 확장: 다양한 codec/포맷 지원으로 FFmpeg 활용 범위 확대
     * 부분 평가 기능: 필터 그래프의 모듈화 및 재사용성 확대 지원

  향후 예정 기능

     * FFmpeg 6.0 외 버전에 대한 광범위한 호환성 확대 예정임
     * 더 많은 복잡한 필터 지원 예정이며, 지속적 기능 확장 계획 진행 중임

빠른 예제(Quick Usage)

import ffmpeg

# 비디오 좌우 반전 및 출력
f = (
    ffmpeg
    .input(filename='input.mp4')
    .hflip()
    .output(filename='output.mp4')
)
f

     * 복잡한 예시로, 여러 구간을 잘라 overlay 및 drawbox를 추가하는 필터 그래프 표현 가능

import ffmpeg.filters
import ffmpeg

in_file = ffmpeg.input(""input.mp4"")
overlay_file = ffmpeg.input(""overlay.png"")

f = (
    ffmpeg.filters
    .concat(
        in_file.trim(start_frame=10, end_frame=20),
        in_file.trim(start_frame=30, end_frame=40),
    )
    .video(0)
    .overlay(overlay_file.hflip())
    .drawbox(x=""50"", y=""50"", width=""120"", height=""120"", color=""red"", thickness=""5"")
    .output(filename=""out.mp4"")
)
f

     * 더 많은 예제와 상세 설명은 문서에서 확인 가능함

인터랙티브 플레이그라운드

     * 브라우저 상에서 FFmpeg 필터 및 명령 실험, 결과 시각화, 다양한 입출력 옵션 테스트, 인터랙티브 예제 학습, 필터 그래프 공유 등이 가능함
     * 로컬 환경 없이도 FFmpeg 필터 체인 프로토타이핑 및 학습에 매우 효과적인 환경임

프로젝트 배경 및 감사

     * GPT-3의 FFmpeg 문서를 기반으로 SDK 자동 생성 아이디어에서 영감받아 시작된 프로젝트임
     * 코어 생성은 신뢰성 확보를 위해 수작업 코드 생성 방식으로 전환함
     * 개발 과정에서 GitHub Copilot과 GPT-3가 개발 효율성 증대에 기여함
     * ffmpeg-python이 API 스타일과 디자인에 영감을 주어 설계에 많은 영향을 줌
     * 프로젝트는 개발자의 자녀 Austin에게 헌정됨

문서 및 참고

     * 더욱 상세한 정보와 심화 기능은 공식 Documentation에서 확인 가능함
     * 단일 패키지 설치 외에, 그래프 시각화 등 추가 기능을 위해서는 별도 옵션을 통해 graphviz 지원 가능함
     * ffmpeg-python과의 호환성을 위해 별도 버전(typed-ffmpeg-compatible) 제공함

        Hacker News 의견

     * 모든 커맨드라인 옵션 파서나 툴킷은 각자의 독립적인 전체 구성 언어임을 사람들이 너무 과소평가하는 현실 인식임, 각각의 도구는 그 언어에서 동작하는 개별 프로그램이나 설정으로 인식 필요성 강조임, 유닉스 셸에서 단어 분할 규칙과 겉으로 보이는 비슷한 구문 때문에 실상 엄청난 다양성이 있다는 점을 사람들이 지나치게 균일하다고 오해하는 현상 설명임, 개인적으로 /usr/bin 내 모든 프로그램을 --help, -h 옵션으로 돌려봤을 때 기대한 도움말을 얻지 못한 실패율이 상당히 높았던 경험 공유임, ffmpeg처럼 복잡한 툴에 타입 정보를 도입하면 이런 다양성 문제 인식 및 실질적인 이점도 줄 수 있음을 강조하는 격려 메시지임
          + ""man foo""가 ""foo --help""나 ""foo -h""보다 훨씬 신뢰할 수 있고 유용하다는 경험 공유임
          + 입문자에게 구체적으로 어디에서 충돌이 있었는지, 그 대립 지점이 어디였는지 궁금증 표현임
     * 프로젝트가 활발하게 개발되고 있다는 점이 인상적이지만, ffmpeg-python과 유사한 문제를 일부 갖고 있는 것 같음, 예를 들면 입력 없이 동작하는 ""color"" 같은 필터 지정 방법이 보이지 않음, GUI 앱에서 CMD 창이 나오는 걸 막기 위한 subprocess.CREATE_NO_WINDOW 같은 Popen 플래그 지정 방식이 없음, ffmpeg.compile() 이후 수동 실행으로 ffmpeg는 해결 가능하지만 ffmpeg.probe()에선 해당 방법이 적용 안되는 점 언급임, 추가로 소스 필터 문제는 ffmpeg.sources.color로 파악했으나, 임의의 소스 필터를 vfilter/afilter처럼 쓰는 방법이 있는지 궁금증 제기임
     * 파이썬으로 스크립트 기반 영상 처리 진행한다면 Vapoursynth 사용을 강력하게 추천함, 영상 처리를 목적으로 처음부터 설계된 툴이고 활발한 유지보수와 커뮤니티, 도구 생태계도 충실함, ffmpeg의 CLI에 얽매이지 않고 자유롭게 사용할 수 있음, Vapoursynth 홈페이지 링크 공유임
     * 정말 멋진 아이디어임, 개인적으로 이 프로젝트의 typescript 버전을 기대하고 있음
          + Jules 프리뷰 도구로 typescript 버전이 어떻게 나올지 간단하게 테스트해봤음, 구현은 꽤 단순하지만 더 읽기 좋은 방식이 떠오름, 아주 짧은 프롬프트에 비하면 결과가 나쁘지 않음, 궁금한 사람 위해 multi-language-codegen 브랜치의 코드 공유함
          + typescript 버전도 출시되면 정말 좋겠다는 의견임
          + typescript를 '신의 언어'로 비유한 유머임
          + typescript 언급이 공식 문서에는 없었다는 지적임
     * ffmpeg 커맨드라인의 형태를 기계가 읽을 수 있도록 기술해서 다양한 언어 코드로 자동 생성되도록 해야 한다는 생각임
     * 비주얼 툴이 특히 인상적임, FFMPEG은 모듈화/비주얼 프로그래밍으로 확실히 접근성이 좋아질 만한 사례로 보임, 모든 기능을 알지 못하는 사용자가 많으므로 이런 방식이 도움되는 사례임, UX에서 DEL 키로 노드/엣지 삭제 기능을 기대했으나 적용되지 않아 아쉬움도 있음, 전반적으로는 훌륭한 경험임
          + 이제야 비로소 MS의 DirectShow GraphEdit과 비슷한 도구가 등장했다는 반가움 표현임
     * 정말 흥미로운 프로젝트라고 생각함, 개별 명령어들 사이에서 프레임을 파이프로 전달하는 기능까지 지원한다면 FFMpeg 활용의 진정한 확장성이 열릴 것이라는 의견임
     * 이 프로젝트가 너무 멋지다고 생각해서, 시각적으로 영상 편집이 가능한 newbeelearn의 비디오 에디터 도구도 함께 소개하고 싶음, 해당 도구 역시 ffmpeg 명령어 자동 생성 가능함
     * 좋은 작업이라는 짧은 응원임
     * 프로젝트가 아주 멋져보인다는 감탄임
"
"https://news.hada.io/topic?id=21195","Arc 멤버들에게 보내는 편지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Arc 멤버들에게 보내는 편지

Arc 멤버들에게

     * The Browser Company는 Arc에 올인했다가 갑자기 Dia라는 새로운 것을 만들기 시작함
     * 겉보기에는 갑작스러워 보일 수 있음. Arc는 추진력이 있었고, 많은 사람들이 좋아했음. 그러나 내부에서는 결정이 보기보다 느리고 신중했음.
     * 왜 이 회사를 시작했는지, Arc에서 얻은 교훈이 무엇인지, 어떤 일이 일어났는지, 그리고 왜 Dia가 다음 단계인지를 해명하려 함

    1. 무엇이 잘못되었나
    2. 왜 Arc를 만들게 되었나
    3. Arc의 부족한 부분
    4. Dia를 Arc에 내장하지 않은 이유
    5. Arc를 오픈소스화 할 것인가
    6. Dia를 만들며
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  무엇이 잘못되었나

     * 모든 것을 다시 시작할 수 있다면 어떤 것을 바꿀 것인가?
         1. 1년 전에 Arc 개발을 그만두었을 것
               o 성장, 사용자 유지, 사용 방식 등 - 우리가 내린 모든 결론은 이미 데이터에서 확인할 수 있었음
               o 다만 부정하고 있었을 뿐
         2. 더 빨리, 더 많이 AI를 받아들였을 것
               o 강박관념에 사로잡혀 있었음
                    # ChatGPT를 가지고 놀았지만, 그것은 업무가 아닌 순수한 호기심 때문
               o 업계의 과대광고와 자신이 그것에 기여하는 방식도 싫었음
                    # 유행어들, 자만심이 AI에 대한 호기심에서 자신을 물러나게 만들었음
                    # Arc Max 출시가 얼마나 조심스러웠는지를 보면 알 수 있음
               o Act II 영상을 보면 Arc에 AI를 도입한다고 발표했으나, Arc Explore라는 프로토타입 데모만 남김.
                    # 이 아이디어는 Dia와 다른 AI 제품들의 방향성이 됨
                    # 우리가 앞서있었다는 말이 아니라, 우리의 직감은 마음이 따라잡기 전부터 AI에 있었다는 말임.
         3. 다르게 소통했을 것
               o 우리는 항상 우리의 타겟층을 많이 신경썼음.
               o 어떤 때는 우리가 너무 투명했음
                    # 공유할 세부사항 없이 Dia를 발표한 것
               o 어떤때는 투명하지 못했음
                    # 유저들이 물어보는 것을 알고 있었지만 답변이 너무 오래 걸리는 것
     * 몇 년 전, 멘토가 책상에 이런 포스트잇을 붙였음: ""진리가 너희를 자유케 하리라""(The truth will set you free)
          + 가장 후회하는 것이 있다면, 이것을 더 많이 사용하지 않은 것
          + 이 에세이는 우리의 진실임
               o 공유하기 불편하지만, 정성을 다해 좋은 의도로 썼음을 알아주었으면 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  왜 Arc를 만들게 되었나

     * 다른 질문에 답하기 위해서(왜 Dia로 방향을 바꾸었나, Arc를 오픈소스화 할 수 있을까 등) 과거 배경을 약간 알아야 함
     * The Browser Company를 시작하게 된 신념 - 브라우저는 우리 삶에서 가장 중요한 소프트웨어지만 그에 맞는 관심을 받지 못하고 있음
     * 2019년에도 이미 모든 것이 브라우저에서 돌아가고 있었음
          + 내 와이프는 테크 분야에서 일하지 않는데도 하루종일 크롬에서 살고 있었음
          + 6살 조카는 완전히 웹 앱으로 학교를 다님
          + 클라우드 매출 급증, 브라우저 기반 스타트업, 브라우저 확장 프로그램을 통한 암호화폐, 새로운 경험을 제공하는 WebAssembly등 거시적인 트렌드도 모두 같은 방향을 향함
     * 그때에도 가장 지배적인 OS는 윈도우나 macOS가 아니라 브라우저인 것 같았음, 그러나 크롬과 사파리는 발전하지 않았음
     * 이게 우리가 Arc를 만든 이유
          + 업무, 여가생활, 브라우저에서 보내는 모든 시간을 ""인터넷 위의 당신의 집""처럼 만들고 싶었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Arc의 부족한 부분

     * 몇 년동안 Arc를 만들면서, 우리가 ""새로움 세금(novelty tax)"" 이라고 부르는 것을 마주하게 됨
          + 많은 사람들이 Arc를 좋아해 줌, 일관성과 유기적인 성장의 혜택을 누림
          + 그러나 대부분의 사람들에게는 Arc가 너무 달랐고, 조금의 편리함을 위해 너무 많은 것을 배워야 하는 것 같았음
          + ""첫날 정체"" 가 너무 심했음 - 며칠 후에도 머무른 사람들은 매니아층이었음
               o 통계는 우리가 만들고 싶었던 대중적인 제품보다는 동영상 편집기같이 전문적인 부분에 특화된 도구같아 보였음
     * 핵심 기능과 핵심 가치에 대해 응집력이 부족했음
          + 실험적이었고, 이것이 매력이었지만, 또한 복잡성이기도 했음
          + 일일 활성 사용자중 5.52% 만이 두 개 이상의 Space를 사용함
          + 4.17% 만이 Github Live Folders를 포함한 Live Folders를 사용함
          + 우리가 가장 좋아하는 기능 중 하나였던 캘린더 미리보기는 0.4% 였음
     * 브라우저를 바꾸는 것은 큰 일임
          + Arc에서 우리가 좋아했던 작은 기능들은, 어떤 사람들에게는 충분하지 않았고 대부분의 사람들에게는 받아들이기 어려웠음
          + 반면에, Dia의 핵심 기능인 탭과 대화하기나 개인화 기능들은 각각 일일 활성 사용자의 40%와 37%가 사용함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * 받아들이기는 어렵지만 Arc와 Arc Search는 너무 진보적이었음
          + 의미는 있었으나, 결론적으로 우리가 원하는 규모의 개선은 아니었음
          + 대중적인 제품으로 성공할 수도 없었음
          + 원래의 사명을 진지하게 생각했다면 진정으로 새로운 것을 만들 수 있는 기술적 돌파구가 필요했음
     * 2023년에, 브라우저만큼이나 낡은 카테고리들이 위협받는 것을 보았음
          + ChatGPT와 Perplexity가 Google의 자리를 위협함
          + Cursor는 IDE를 재창조함
          + 이 두 가지(검색 엔진과 IDE)는 사용자들이 수십 년동안 같은 방식으로 해온 것들
          + 그리고 갑자기 변화를 받아들이기 시작함
          + 바로 이 시점이 우리가 원했던 것 - 사용자의 방식을 바꾸고 브라우저의 진정한 재창조로 이끌 수도 있는 근본적인 변화
     * 왜 그냥 Arc 사용에 돈을 받고 수익성 있는 사업을 시작하지 않았냐 묻는다면
          + 만약 우리의 목표가 작고 수익성 있는 회사를 꾸려 좋은 팀과 충성스런 고객들을 만드는 것이라면, 가장 흔한 소프트웨어인 웹 브라우저의 계승자를 만들려고 시도하지조차 않았을 것임
          + 우리의 목표는 항상 더 컸음: 사람들에게 진짜 영항을 줄 수 있는 좋은 소프트웨어를 만드는 것
     * Arc가 부족하게 느껴진다면, 그것을 뛰어넘는 새로운 것을 만들면 되는 것 아닌가?
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Dia를 Arc에 내장하지 않은 이유

     * Dia와 Arc가 서로 다른 제품이라는 이해하기까지 여름 내내 고민했음
     * 시작하면서, 우리는 다방면으로 Dia를 Arc를 고치기 위한 기회로 생각하고 접근함
         1. 새로움보다 단순함
               o 일찍이 Scott Forstall은 Arc가 마치 색소폰같이 강력하지만 배우기 어렵다고 말함
               o 그리고 요청함: 누구나 앉아서 연주할 수 있는 피아노처럼 만들 것
               o 이것이 Dia의 아이디어임: 익숙한 인터페이스 뒤에 복잡성을 숨기는 것
         2. 속도는 더 이상 거래가 아님 - 기반임
               o Arc는 부풀었음 - 너무 많은 것을 너무 빠르게 만들었음
               o Dia를 설계 관점부터 새롭게 시작하며, 처음부터 퍼포먼스를 중시함.
               o TCA와 SwiftUI를 버린 것이 Dia를 가볍고, 빠르고, 반응성있게 만들었음
         3. 보안을 제일 먼저, 중요하게
               o Dia는 다른 종류의 제품임
               o 이를 충족하기 위해 보안 엔지니어링 팀을 1명에서 5명으로 늘림
               o 레드 팀, 버그 바운티, 내부 감사에 투자함
               o 목표는 소규모 스타트업의 표준을 제시하는 것
     * 이 모든 것들이 제품의 기초가 되어야 하는 요소들 - 사후 고려사항이 아님
     * 지난 여름에 Arc 2.0이 진정한 Arc 2.0인지에 대한 경계를 넓히면서, Arc에 패치로 해결하기에는 너무 큰 단점들이 있으며 새로운 유형의 소프트웨어를 빠르게 구축하려면 새로운 유형의 기반이 필요하다는 것을 알게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Arc를 오픈소스화 할 것인가

     * 팩트는 이것이 복잡하다는 것임
     * Arc는 단순한 Chromium 포크가 아님
          + ADK(Arc Development Kit) 라고 부르는 내부 SDK로 돌아감
          + 전 iOS 엔지니어들이 네이티브 브라우저 UI를 빠르게, C++를 건드리지 않고도 프로토타이핑할 수 있게 해줌
          + 그것이 많은 브라우저들이 새로운 것을 감히 시도하지 못하는 이유이기도 함
               o 대가가 너무 큼, 너무 복잡함
     * ADK는 또한 Dia의 기반이기도 함
          + 때문에 Arc를 언젠가는 오픈소스화 하는 것에 긍정적이지만, ADK를 오픈소스화하지 않고서는 할 수 없음
          + ADK는 여전히 회사의 핵심 가치임
     * 절대 안될거라는 말은 아님
          + 언젠가 ADK를 공개하는 것이 우리 팀과 주주들을 위험에 빠뜨리지 않는 날이 온다면, 우리는 기쁘게 우리가 만든 것을 세상에 공개할 것임
     * 또한 알아둬야 할 것: Arc를 지원 종료하려 하는 것은 아님
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Dia를 만들며

     * Dia는 Arc와 Arc의 단점에 대한 우리의 리액션이 아님
          + 전기 조명 시대의 시작에 양초 사업에서 손을 떼는 이유를 정당화하는 에세이를 쓰는 것
          + 전기 지능은 이미 있으며. 그 순간에 맞춰 우리가 만들어야 할 제품의 종류를 근본적으로 바꿔야 함
     * 전통적인 브라우저들은 죽을 것, 검색 엔진과 IDE가 그랬던 것처럼
          + 검색이나 코딩을 멈춰야 한다는 말이 아님
          + 그저 검색이나 코딩을 하는 환경이 매우 다를 것이라는 것
          + 고전 브라우저, 검색엔진, IDE가 아무리 정교하게 만들어졌다고 해도 양초처럼 느껴질 것
          + 우리는 양초 사업에서 벗어나고 있고, 여러분도 그래야 함
     * ""그래서 The Browser Company는 더 이상 브라우저를 만들지 않는 건가요?"" 그렇게 믿는 게 좋음
          + 인공지능 브라우저는 웹 브라우저와는 다를 것임
          + 이것을 그 어느 때보다 확신하며, 이미 세 가지 측면에서 그 변화를 목격하는 중임
              1. 웹 페이지는 더 이상 주 인터페이스가 아니게 될 것
                    # 전통적인 브라우저는 웹페이지를 불러오도록 만들어짐
                    # 그러나 점점, 웹페이지 - 앱, 글, 파일들은 AI 챗 인터페이스와 부르는 도구가 될 것임
                    # 여러 방면으로 챗 인터페이스는 이미 브라우저처럼 동작함
                         @ 검색하고, 읽고, 생성하고, 답변함
                         @ API, LLM, 데이터베이스와 상호작용함
                         @ 사람들은 수 시간을 챗 인터페이스 안에서 보냄
              2. 웹이 사라지는 것은 아님 - 적어도 당분간은
                    # Figma와 New York Times의 중요성은 줄어들지 않음
                    # 웹페이지는 대체되지 않고 필수적인 도구로 남을 것
                    # 탭은 소모품이 아니라 핵심 컨텍스트임
              3. 새로운 인터페이스가 익숙한 인터페이스에서부터 시작됨
                    # 모두의 컴퓨터 사용 방식은 훨씬 빠르게 변화하고 있음
                    # 동시에 기존의 방식을 완전히 버리는 데는 훨씬 더 멀었음
                    # Cursor는 이 정설을 증명함: 작년의 획기적인 AI 앱은 AI 네이티브로 설계된 오래된 IDE였음
                    # OpenAI는 Codex가 작동하고 있음에도 불구하고 Windsurf를 인수하면서 이 이론을 확인함
                    # 다음은 AI 브라우저라고 믿고 있음
     * 이것이 우리가 Dia를 만드는 이유임
     * 실패하거나, 부분적으로만 성공하고 승리하지는 못할 수도 있음
     * 그러나 이것에 대해서는 확신함: 5년 후에는 데스크탑에서 가장 많이 사용되는 AI 인터페이스가 과거의 기본 브라우저를 대체할 것임. 오늘날처럼 몇 개가 있을 수도 있지만 (크롬, 사파리, 엣지 등)
     * 중요한 것은 다음 크롬이 지금 만들어지고 있다는 것임. 그것이 Dia이든 아니든.

   개인적으로 Arc가 이슈화가 될 때 시도해 보여고 했었는데 우선 윈도우 지원이 너무 늦게 나왔고, 다운로드는 이메일을 제공한 사람들에게만 CBT 형태로 나왔고 별로 빠릿하지도 않고 브라우저 작동 방식이 햇갈려서 며칠 써보다가 말았던 기억이 나네요.
   그러다 최근에 Zen 브라우저가 나와서 거기로 갈아탔습니다. Arc와 비슷한 탭 관리 방법을 가지고 있으면서도 오픈 소스였고, Firefox 기반이라 MV2 폐지에 벌벌 떨 필요도 없었고, 윈도우 지원이 바로 있다는 등의 장점이 있어서요.

   말은 번지르르 한데.. 공개된게 너무 없네요. 신뢰가 잘 안가요.

   왠지 그냥 크롬에 확장 기능 하나 더해지게 될 것 같은 느낌이 드네요...

   """"첫날 정체"" 가 너무 심했음 - 며칠 후에도 머무른 사람들은 매니아층이었음""
   왜냐하면 크롬이랑 별 다를게 없었으니까요. 그냥 기본으로 확장 프로그램 깔려서 나오는 크롬 느낌.

   크롬에서는 물론이고 파이어폭스에서조차도 확장 프로그램 몇 개로 거의 비슷한 경험을 할 수 있습니다.

   Dia 브라우저도 이때까지 공개된 정보로는 그냥 AI 딱지 붙여서 쏟아져 나오는 ""새로운"" 프로그램들과 다를 것이 없네요.

   거의 비슷한 경험은 조금 과장된 표현 같습니다. 그정도로 완성도 높은 UIUX는 익스텐션보다 훨씬 나은 느낌이었어요.

   혹시 매력적인 차별점이 무엇이었는지 말씀해 주실수 있을까요? 저는 며칠 써보고 바로 회귀했었거든요.

   사실 저도 결국 크롬으로 돌아와서 뚜렷이 기억나지 않았습니다만, '이젤'같은 기능은 타 브라우저에서 그 정도로 네이티브하게 표현하지는 못할 것 같습니다. 다른 댓글에서 좋은 대화 나눠주셔서 덕분에 잘 배웠습니다. 좋은 댓글 감사드립니다.

   Chrome 설치 후 직접 테스트해봤습니다.

   제가 가장 핵심적인 요소라고 생각하는 기능 중 하나는 수직 탭인데, Vertical Tabs라는 확장 프로그램을 통해서 크롬에서도 사용할 수 있네요. 탭 컨트롤이나 그룹도 보여주고요. 다만 확장 프로그램인 만큼 크롬의 수평 탭 UI를 지워주진 않습니다. 별 문제야 없지만 개인적으로는 보기에 조금 거슬리긴 하네요.

   Split View라는 기능은 하나의 브라우저 윈도우에서 두 개 이상의 탭을 동시에 보여주는 기능입니다. 크롬에서는 Split Screen for Google Chrome이라는 확장 프로그램이 유명한 것 같은데, 설치해서 사용해보니 선택한 비율로 윈도우를 추가로 생성, 재배치 해주네요.
   Arc는 하나의 윈도우에서 2개의 탭 마치 하나의 탭처럼 보여주는 반면 이 확장 프로그램은 아예 윈도우를 2개로 만들어버리니, Split View로 보던 도중 다른 탭을 보게 될 때도 레이아웃이 그대로 유지되니 상당히 번거롭고 불편한 느낌입니다.

   Space는 큰 기준으로 탭을 나누고(저는 보통 Home/Study/Work로 나눕니다) 단축키/버튼 하나로 간단하게 탭 목록을 전환하며 사용하는 기능입니다. 또한 Gmail, Calendar와 같이 모든 Space에서 접근할 수 있는 탭도 만들 수 있습니다.
   크롬에 프로필 기능이 있긴 하지만, 아예 새로운 윈도우로 열릴 뿐더러 모든 프로필에서 접근 가능한 탭을 만드는 방법은 안보이네요.
   또한 Space는 기본적으로 확장 프로그램과 쿠키 등을 공유하는 일종의 ""큰 탭 그룹""일 뿐이고 선택적으로 확장 프로그램과 쿠키를 공유하지 않는 컨테이너로써 작동하도록 할 수 있지만, 프로필은 무조건적으로 확장 프로그램과 쿠키 등을 공유하지 않는 컨테이너로써 작동하고요.

   위 기능들은 제가 Arc와 Zen 브라우저에서 가장 즐겨쓰는 기능들입니다.
   제가 Zen 브라우저를 메인으로 쓰고 나서부터는 크롬을 거의 사용하지 않아 이번에 테스트 겸 짧게 써보고 작성한 거라 혹시 제가 놓친 부분이 있을 수 있다면 알려주시면 감사하겠습니다.

   저도 써 보았던 기능들이네요.

   저는 파이어폭스를 사용하는데 수직 탭이나 Space 기능은 기존에 사용하던 Sidebery (+ Multi account containers) 확장 하나로 충분히 커버된다는 인상이었습니다.
   창 분할 기능은 이미 불편하다고 말씀하시긴 했지만 저같은 경우 OS 차원의 기능을 즐겨썼습니다. 윈도우 맥은 단축키 두세번으로 쉽게 분할 가능하고 리눅스 DE에서는 그보다도 간편하거든요.

   물론 브라우저에 통합된 매끄러운 UX라고 볼 수도 있겠지만, 개인적으로는 큰 매력을 느끼진 못했습니다.
   Arc나 Zen처럼 기존 브라우저에 편의 기능을 얹는 방식은 오랜 시간 브라우저를 사용하며 개인적으로 작성하고 구축한 환경에 비해 특별히 낫다고 느끼기 어려웠고요.

   처음 브라우저 환경을 꾸리는 사용자에게는 좋은 출발점이 될 수도 있겠죠.
   하지만 브라우저에 큰 관심이 없는 사람들은 그냥 크롬을 계속 쓸 것이고 관심 있는 사용자들은 이미 각자 최적화된 환경을 만들어놨을 겁니다.
   결국 타깃 사용자층이 애매하다는 인상을 받았습니다.

   저도 정말 오랫동안 크롬을 써왔지만, 제 크롬/파이어폭스에서의 사용 경험은 항상 20% 정도 부족한 느낌이었습니다.
   확장 프로그램으로 최대한 제가 원하는 환경을 구성해도 불편한 점이 계속해서 있었고, 계속해서 부족한 점을 채워줄 브라우저를 찾아다니면서 깃허브에 있던 미완성 브라우저까지 직접 빌드해 사용해 보았습니다만,
   Arc의 첫 경험은 완벽한 브라우저였고, Arc가 생각하는 웹 브라우징 경험에 의해서 생겨나는 새로운 기능들은 제 웹 브라우징 방식을 완전히 바꿔놓았습니다.

   다만 말씀해주신 것처럼 이미 기존 브라우징 환경에 어느정도 만족하고 확장 프로그램을 사용해 본인에게 최적화된 환경을 만들어놓은 사람들에게는 Arc의 환경이 딱히 매력적이지 않을 수도 있었겠네요.

   새로운 시각을 제공해주셔서 감사합니다.

   브라우저는 정말 개인적인 만족이 필요한 환경이니만큼 사용자 입장에서 취향에 따라 Arc/Zen을 선택해 사용하는것은 당연한 일이라고 생각합니다.

   제가 맨 처음 댓글에서 생각했던 것은 글이 제작사 내부 인원들을 위해 쓰여진 만큼 회사의 입장에서 타겟층이 어떻게 될까? 를 생각하면서 썼던 것이었는데 글이 부족했던 것 같네요.

   만약 Dia 외에 AI 브라우저를 맛보고 싶으시다면, Deta Surf라는 브라우저가 있습니다.
   체감 상 성능 문제나 버그가 많은 것 같긴 하지만 기능 자체는 주목할만 한 브라우저라고 생각합니다.
   혹시 관심 있으신 분들은, 최근 0.5 버전 업데이트가 되면서 클로즈 알파테스트에서 오픈 알파테스트로 전환되었으니 한번 찾아보시는 것도 나쁘지 않겠네요.
"
"https://news.hada.io/topic?id=21226","Progressive JSON","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Progressive JSON

     * 점진적 JPEG처럼, JSON 데이터도 불완전한 상태로 먼저 전송하여 클라이언트가 점차 내용 전체를 활용할 수 있는 방식 소개
     * 기존 JSON 파싱 방식은 전체 데이터가 완전히 수신되기 전까지 아무런 작업이 불가능한 비효율성 문제 있음
     * Breadth-first 방식으로 데이터를 여러 청크(부분)로 구분하여, 아직 준비되지 않은 부분은 Promise로 표시하고 준비되는 대로 점진적으로 채워, 클라이언트가 미완성 데이터도 활용 가능함
     * 이 개념은 React Server Components(RSC) 의 핵심 혁신이며, <Suspense>를 통해 의도된 단계별 로딩 상태를 제어함
     * 데이터 스트리밍과 의도적 UI 로딩 흐름을 분리하여 더욱 유연한 사용자 경험 제공 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

점진적(Progressive) JPEG과 점진적 JSON의 아이디어

     * 점진적 JPEG는 이미지를 위에서 아래로 한 번에 불러오는 대신, 흐릿한 상태로 전체를 먼저 보여주고 점차 선명해지는 방식임
     * 이와 유사하게, JSON 전송에도 점진적 방식을 적용함으로써 전체가 완성될 때까지 기다리지 않고 일부 데이터를 즉시 사용할 수 있음
     * 예시 JSON 데이터 구조에서, 일반적 방식은 마지막 바이트까지 모두 받아야지만 파싱이 가능함
     * 이로 인해 클라이언트는 서버의 느린 부분(예: 느린 DB에서 comments 불러옴)까지 모두 전송될 때까지 대기해야 하며, 이는 매우 비효율적인 현재의 표준임

스트리밍 JSON 파서의 한계

     * 스트리밍 JSON 파서를 도입하면 불완전한(중간) 데이터 객체 트리를 생성 가능
     * 하지만, 각 객체의 필드(예: footer, 여러 개의 comment 목록 등)가 일부만 전달되는 경우 타입이 불일치하고, 완성 여부 파악이 어려워 활용도 저하 문제 발생
     * HTML의 스트리밍 렌더링과 유사하게, 순서대로 스트림 처리 시 하나의 느린 부분이 전체 결과를 지연시키는 문제가 동일하게 발생함
     * 일반적으로 스트리밍 JSON의 활용이 드문 이유임

Progressive JSON 구조 제안

     * 기존 방식의 깊이 우선 스트리밍(즉, 트리구조 하위까지 내부 순회 전송하는 방식)이 아닌, Breadth-first(너비 우선) 방식 도입
     * 최상위 객체만 먼저 전송하고, 하위 값들은 Promise와 같은 플레이스홀더로 두고 준비되는 대로 한 번에 각각의 청크로 채워나감
     * 예를 들어 서버가 비동기로 데이터 로딩이 끝날 때마다 대응하는 청크를 전송, 클라이언트는 준비된 만큼만 활용 가능
     * 비동기적 데이터 수신(이른 로드) 이 가능해지고, 여러 느린 부분이 모두 처리가 끝날 때까지 전체 대기하지 않아도 됨
     * 클라이언트를 청크별 비순차 및 부분 순차 수신에 강하게 구성하면, 서버는 다양한 청크 분할 전략을 유연하게 적용 가능

Inlining과 Outlining: 효율적 데이터 전송

     * 점진적 JSON 스트리밍 포맷은 재사용 객체(예: 동일 userInfo를 여러 군데에서 참조)도 중복 저장 없이 한 개의 청크로 따로 추출하여 각 위치에서 동일 참조 가능
     * 느린 부분만 분리해 placeholder로 전송하고, 나머지는 바로 채워 효율적 데이터 스트림 구현
     * 동일한 객체가 여러 번 등장하는 경우, 한 번만 전송하고 재활용(Outlining) 가능
     * 이러한 방식으로 순환 참조(객체가 자기 자신을 참조하는 구조) 도 일반 JSON처럼 곤란하지 않고 청크 내 간접 참조 구조로 자연스럽게 직렬화 가능

React Server Components(RSC)의 점진적 스트리밍 구현

     * 실제 React Server Components는 점진적 JSON 스트리밍 모델을 적용한 대표적 예시임
          + 서버가 외부 데이터(예: Post, Comments)를 비동기로 불러오는 구조를 사용
          + 클라이언트에선 아직 도착하지 않은 부분을 Promise로 두고, 준비되는 순서대로 점진적 UI 렌더링
     * React의 <Suspense>로 의도적인 로딩 상태를 설정
          + 사용자 경험상 불필요한 화면 점프 방지를 위해, Promise 상태(구멍)를 바로 보여주지 않고, <Suspense> fallback으로 단계별 로딩 연출 가능
          + 데이터가 빠르게 도착해도 실제 UI는 설계된 단계에 맞춰 점진적으로 노출되게 개발자가 제어 가능

요약 및 시사점

     * React Server Components의 핵심 혁신은 컴포넌트 트리 속성(props)을 외곽부터 점진적으로 스트리밍하는 방식에 있음
     * 따라서, 서버가 완전히 모든 데이터를 준비할 때까지 기다릴 필요 없이, 주요 부분부터 점차 보여주며 로딩 대기 상태도 세밀하게 제어 가능
     * 스트리밍 자체뿐 아니라, 이를 활용하는 프로그래밍 모델(React의 <Suspense>) 같은 구조적 지원이 함께 필요함
     * 이를 통해, 느린 데이터 한 부분이 전체를 지연시키는 문제 등 기존 전송 방식의 병목 현상 완화 가능

        Hacker News 의견

     * 일부 사람들이 이 글을 너무 글자 그대로 받아들이는 경향이 보이는데, Dan Abramov가 Progressive JSON이라는 새로운 포맷을 제안하고 있는 건 아니라는 설명
          + 이 글은 React Server Components의 아이디어와, 컴포넌트 트리를 자바스크립트 객체 형태로 표현한 뒤 이를 스트림 형태로 전송하는 과정을 설명하는 내용
          + 이 방식은 React 컴포넌트 트리에 ‘구멍’을 둘 수 있게 해서, 처음에는 로딩 상태나 스켈레톤 UI를 보여주고, 서버에서 실제 데이터를 받을 때 해당 부분을 완전히 렌더링하는 구조 가능
          + 이렇게 하면 더 세밀한 단계에서 로딩 표시와 빠른 첫 화면 표시가 가능
     * 내가 생각하기에 사람들이 이 아이디어를 확장해서 다른 방법에 적용하는 것도 괜찮다고 봄
          + RSC의 데이터를 직렬화하는 방식을 React에만 국한하지 않고, 더 일반적인 패턴으로 설명하려고 했던 의도
          + React Server Components에서 발견한 여러 아이디어가 다른 기술이나 생태계에도 녹아들었으면 하는 바람
     * 나는 progressive loading 방식, 특히 콘텐츠가 계속 움직이는(점프하는) 경험이 썩 마음에 들지 않음
          + 로딩 중에 텅 빈 상태 UI를 보여주는 패턴이 특히 신경 쓰임
     * 얼마 전까지 Ember를 썼을 때도 비슷한 방식이 있었고, Ajax 엔드포인트 작성이 매우 고통스러웠던 기억
          + 트리 구조 재배치로 일부 자식 요소가 파일 끝에 위치하도록 해서 DAG(비순환 그래프) 처리를 효율적으로 하려던 의도였던 듯
          + SAX 스타일의 스트리밍 파서를 쓰면 데이터가 부분적으로 도착할 때 페인팅을 먼저 시작할 수 있음
          + 하지만 단일 스레드 VM에서 작업 순서를 잘못 설계하면 오히려 문제만 커지는 위험 존재
     * 나는 이미 AI 툴과의 연결에서 스트리밍 partial JSON(Progressive JSON) 방식을 실제로 사용 중
          + 이 방식은 RSC뿐 아니라 다양한 곳에서 활용되고 실무적으로도 클라이언트, 서버 모두에 가치 있는 방법이라는 실제 경험
     * Dan의 ""2 computers"" 발표와 최근 RSC 관련 포스팅을 다 챙겨봤음
          + Dan은 React 생태계에서 최고의 설명가이지만, 기술을 이렇게까지 어렵게 설명해야 한다면
              1. 진짜로 필요 없는 기술이거나
              2. 추상화에 문제가 있는 것
          + 대다수 프론트엔드 개발자가 여전히 RSC 개념을 완전히 이해하지 못함
          + Vercel은 Next.js를 기본 React 프레임워크로 만들었고, RSC 채택도 이 바람을 타고 확산
          + Next.js를 쓰는 사람조차 Server Component 경계를 명확히 이해하지 못하고 일종의 ‘카고 컬트’식 채택이 많음
          + React가 Vite 관련 PR을 받아들이지 않은 것도 의심스러움. RSC 푸시는 실질적으로 유저나 디벨로퍼를 위한 것이 아니라 플랫폼 업체의 호스팅 플랫폼 판매 전략일지도 모른다는 생각
          + 되짚어 보면, Vercel이 React 오리지널 팀을 대거 영입한 것도 React 미래를 주도하려는 의도처럼 보임
          + 역사적 동기나 배경에 대한 판단이 틀렸다는 지적과 함께, Vite 지원 관련 현황에 대해서 설명
          + Vite 통합은 DEV 환경에서 번들링이 필요하다는 기술적인 제약 때문에 현재 Vite 팀이 개선 진행 중이라는 언급
               o 관련 작업 진행 상황: https://github.com/facebook/react/pull/33152
          + 사람들이 RSC를 이해하지 못한다는 논지는 논리적으로 순환적인 주장이라는 견해
          + RSC를 싫어할 수도 있지만, 그 안에는 다른 기술에 차용할 만한 흥미로운 아이디어도 충분히 있음
          + 설득보다는, 신기하고 쓸모 있는 부분을 각자가 취해갔으면 하는 바람
     * 물론 여전히 SPA를 정적 사이트로 만들어서 CDN에 올릴 수도 있고, Next.js도 “다이나믹” 모드로 셀프 호스팅이 가능
          + 다만 Next.js의 서버리스 렌더링 전체 기능을 완전하게 Vercel 말고 다른 곳에 구현하기가 어려운 현실 존재(undocumented “magic” 때문)
          + 이 문제 역시 다중 플랫폼에서 일관된 API 제공을 위해 어댑터 도입을 공식적으로 제안한 상황: https://github.com/vercel/next.js/discussions/77740
          + 나는 RSC 푸시가 기업 이익 때문이라기보다는, 기존 웹사이트 빌드 패턴(SSR + 클라이언트 약간의 progressive enhancement)이 실제로 많은 장점이 있다는 깨달음에서 비롯됐다는 입장
          + SSR만으로도 비즈니스 로직이 불필요하게 클라이언트로 많이 옮겨가는 문제 존재
     * RSC 자체는 흥미로운 기술이지만 실전에서는 그다지 합리적이지 않다는 생각
          + 복잡한 컴포넌트를 렌더링하기 위해 Node/Bun 백엔드 서버를 대규모로 유지하는 부담 존재
          + 차라리 정적 페이지나 React SPA + Go API 서버 조합이 훨씬 효율적
          + 비슷한 결과를 훨씬 작은 리소스로 만들 수 있음
     * 새로운 기술의 설명이 복잡하다고 꼭 필요 없는 기술이나 잘못된 추상화라고 단정하기 어렵고, 복잡성을 감수할 만한 가치가 있는 문제도 존재
          + 앞으로 어떻게 이 기술이 진화할지 지켜보는 관점
     * RSC의 코드 구조를 활용해 작은 조각으로 HTML/CSS/JS를 나누는 정적 페이지 빌드도 가능하다는 생각
          + 글에서 제안한 ‘$1’ 플레이스홀더를 URI로 치환해도 서버가 필요 없을 수 있음(대부분 동적 SSR이 반드시 필요한 건 아님)
          + 단점은, 이런 방식은 콘텐츠가 변경될 때 업데이트 파이프라인(특히 S3에 컴파일된 스태틱 사이트의 스트리밍 배포)에 속도 확보가 중요
          + 예를 들어 많은 기사가 프리랜더링되어 있는 신문 사이트처럼, 콘텐츠 일부만 바뀔 때 효율적으로 해당 부분만 다시 빌드하는 등 스마트한 콘텐츠 diff 처리 필요
     * 실무에서 성능 최적화라며 밀리세컨드 단위로 프론트엔드에서 여러 MB 데이터를 불러오고 복잡한 로직을 처리하는 반면, 실제로는 BFF나 아키텍처 개선, 더 Lean한 API 구축이 훨씬 생산적인 솔루션이라는 깨달음
          + GraphQL, http2 등을 통한 시도가 있었으나 결국 본질적인 문제 해결은 아니었고, 웹 표준의 진화 없이는 패러다임 변화 없을 것이라는 의견
          + 새로운 프레임워크 역시 이 한계는 동일
     * RSC는 글 말미에서 설명되는 것처럼 본질적으로 BFF(Backend for Frontend) 역할이라는 설명
          + API 로직을 컴포넌트화한 구조
          + 내 긴 글 참고: https://overreacted.io/jsx-over-the-wire/ (BFF는 첫 섹션 중간 참고)
     * ""페이지 로딩 ms 단축""의 의미에 따라 다르다는 의견
          + Time to first render, time to visually complete를 최적화한다면, 비어있는 skeleton UI를 먼저 보내고 API로 데이터 받아서 hydrate하는 방식이 체감상 가장 빠름
          + 반대로 time to first input, time to interactive을 빠르게 하려면 유저 데이터를 바로 렌더링해 줄 수 있어야 하고, 이 경우 백엔드가 훨씬 유리(네트워크 호출 최소화)
          + 대부분의 경우 유저는 이쪽을 더 선호하고, CRUD SaaS 앱은 서버 측 렌더링이, Figma처럼 디자인이 중요한 앱은 클라이언트에서 정적 데이터+추가 데이터 fetch가 적합
          + ""모든 문제에 통하는 한 가지 솔루션""이란 없고, 최적화 지점은 주관적 선택
          + 개발 경험, 팀 구조 등 기술 선택에 영향을 주는 다양한 요소 존재
     * 덕분에 내가 왜 Facebook 로딩할 때 핵심 콘텐츠가 늘 마지막에 렌더되는지 이해하게 됨
     * 여기서 말하는 BFF가 뭔지 궁금하다는 질문 등장
     * 약어가 너무 많아서 FE와 BFF가 뭔지 궁금해하는 반응
     * Progressive JSON 아이디어를 직접 쓰고 싶진 않고, 대안이 여러 개 존재한다고 생각
          + 가장 간단한 해결책은 거대한 하나의 JSON 객체를 여러 개로 분할, 즉 ‘JSON lines’로 전송
          + 헤더 정보는 한 번, 거대한 배열은 한 줄씩 전송해서 스트림 처리 효율화
          + 객체가 더 크면 이 방식을 재귀적으로 적용할 수도 있지만, 너무 복잡해질 수 있음
          + 속성의 순서를 서버가 명시적으로 보장해서 progressive parsing과 분리도 가능
          + 결국 정말 대용량 구조에서는 유용하지 않겠지만, 가장 흔한 대규모 JSON 다루는 상황에선 꽤 실용적인 도구
     * 명시적으로 구멍(holes)을 표시하지 않아도 스트리밍 메시지를 순차로 전송하며 델타(diff)만 보내는 방식이 가능
          + ‘Mendoza’라는 델타 포맷을 사용하면 Go, JS/Typescript에서 아주 컴팩트하게 패치(diffs) 전송 가능: https://github.com/sanity-io/mendoza, https://github.com/sanity-io/mendoza-js
          + zstd의 바이너리 디프 방식이나 Mendoza처럼, 직렬화 데이터의 일부만 메모리에 저장하여 효율적 패치 진행
          + React 역시 차이점 비교나 변동사항만 주입하는 방식이 필요하기에 의미 있는 접근
     * UI 데이터 스트리밍에서 비어 있는 배열이나 null만으로는 부족하고, 현재 어떤 데이터가 미도착(pending) 상태인지 별도 정보가 필요
          + GraphQL 스트리밍 페이로드는 유효한 데이터 스키마와 미도착 정보, 그리고 이후 패치 처리라는 혼합 방식을 선택
     * 어느 부분이 ‘구멍’인지 알아야 로딩 상태를 보여주기 쉬움
     * 클라이언트에서 JSON을 progressively decode하려면 jsonriver라는 라이브러리를 소개: https://github.com/rictic/jsonriver
          + 매우 간단한 API, 성능도 좋고 테스트도 충분함
          + 스트리밍된 문자열 조각을 점점 더 완성도 높은 값으로 파싱해줌
          + 최종 결과는 JSON.parse와 동일함을 보장
     * 트리 데이터라면 어떤 구조에서도 적용 가능한 재밌는 방식이라는 의견
          + 트리 데이터는 parent, type, data 벡터와 string table로 표현하면, 나머지는 모두 소수의 정수로 축소 가능
          + string table과 type info를 헤더로 upfront 전송하고, parent, data 벡터 청크를 노드 단위로 스트리밍
          + depth-first/breadth-first 스트리밍은 청크 순서만 바꿔도 충분
          + 네트워크 상의 앱에서 로드 시간 UX를 많이 개선할 수 있을 것 같음
          + 테이블과 노드 청크를 번갈아 전송하며 트리를 어떤 순서로든 웹에 시각화할 수 있음
          + preorder traversal과 depth 정보만 있으면 node id도 없이 트리 구조 복원 가능
          + 이 아이디어로 작은 라이브러리 만드는 것도 가치 있는 시도
     * 대부분의 앱은 이런 ‘정교한’ 로딩 시스템이 필요 없고, 정말 간단히 여러 번 API 호출로 대체 가능한 경우가 대부분이라는 주장
          + 본인이 RSC wire 프로토콜 작동 방식을 설명하려던 것뿐이며, 실제로 이런 걸 직접 구현하라고 권하는 의도는 아니라는 반론
          + 다양한 툴 간 원리 파악은 결국 다양한 곳에서 아이디어를 차용하거나 리믹스하는 데 도움
          + 여러 번 호출하는 전략이 n*n+1문제가 있다고 생각하지만, 객체를 OOP/ORM 스타일로 중첩해서 전송하는 대신, 주석의 경우처럼 평평하게 전송하는 방식도 가능
          + 이럴 바엔 차라리 protobuf 등 타입이 명확한 엔드포인트 구성도 장점
          + comments를 분리하면 2번 호출로 충분히 처리가능(페이지+글, 댓글 따로), 이렇게 하면 pre-render 최적화도 가능
          + 옵션의 실제 구현 복잡도 자체를 너무 높이지 않고, 미리 정의된 좋은 툴이 있다면 일부러 딥 커스터마이즈할 필요성 적음
          + 과도하게 복잡한 기능이 결국 사용자나 개발자에게 독이 될 수 있음을 인지해야 한다는 입장
          + 640K도 충분하다는 얘기처럼, progressive/partial reads가 확실히 WASM 시대 UX 속도에 실질적으로 큰 역할을 할 수 있다고 생각
          + protobuf 같은 바이너리 인코딩 방식에 partial read와 well-defined streaming이 붙으면 엔지니어 부담은 늘지만 결과 UX는 크게 성장할 가능성
     * Progressive JPEG는 미디어 파일 특성상 필요하지만, Text/HTML에선 굳이 필요 없고, JS 번들이 커진 결과로 복잡성만 더해지는 자기모순적 상황이라는 견해
          + 실제 느린 원인이 단순히 데이터의 ‘크기’ 때문만이 아니라는 점을 지적
          + 서버 데이터 쿼리 자체에 시간이 오래 걸리거나, 네트워크가 느릴 때도 점진적 노출(progressive reveal)이 의미 있음
          + 전체 데이터 완성까지 대기하는 대신, 적당한 타이밍에 로딩 UI 보여주는 의도적 단계별 렌더링이 실제로 사용자 경험에 이득
     * 엔드포인트 분리 전략이 이미 다양한 장점(Head of line blocking 방지, 필터 옵션 향상, 라이브 업데이트, 독립적인 성능 개선 등) 갖춘 솔루션이라는 생각
          + 애플리케이션을 문서 플랫폼(document platform)으로 다루려는 시도가 근본적인 문제라고 보는 입장
          + 실제 애플리케이션은 ‘문서’처럼 작동하지 않고, 이 괴리를 해소하려 많은 부가 코드와 인프라가 필요해지는 상황
          + 별도 엔드포인트 채택의 진짜 단점과 진화 방향에 대한 두 편의 긴 글로 보완 설명: https://overreacted.io/one-roundtrip-per-navigation/, https://overreacted.io/jsx-over-the-wire/
          + 요약하면, 엔드포인트는 결국 서버/클라이언트 간 '공식' API 계약이되고, 점점 코드가 모듈화되면서 성능에 손해(워터폴 현상 등) 발생 가능
          + 결정들을 서버에서 한 번에 처리(coalescing)가 성능과 구조적 유연성에서 더 나은 대안이 될 수 있음
"
"https://news.hada.io/topic?id=21198","Ask HN: AI에 지쳤어요. 다른 쿨한 기술들은 뭐가 있나요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Ask HN: AI에 지쳤어요. 다른 쿨한 기술들은 뭐가 있나요?

     ""요즘 AI 뉴스가 대부분인데, 다른 종류의 혁신, 하드웨어, 네트워킹, 개발 도구, 로봇 공학, 프로그래밍 언어, 특이한 사이드 프로젝트 분야의 새로운 아이디어에 대한 이야기가 그리워요. AI가 아닌 정말 멋진 기술을 개발하거나 관련 기술을 다루는 분이 계신가요?""

답변에 올라온 기술들 요약

     * github의 @sindresorhus 와 @mafintosh 계정 추천
     * 소프트웨어 정의 라디오(SDR): $40 정도의 저가 SDR 키트와 GNU Radio 툴킷으로 항공, 무전, 네비, 아마추어 무선 등 다양한 신호 수신 및 분석 가능
     * 3D 프린팅 및 홈 메이킹: 3D 프린터 가격 하락, FreeCAD, Fusion 360, Tinkercad 등 접근성 높은 CAD 도구, Prusaslicer 같은 슬라이서, 다양한 오픈 STL 파일로 누구나 손쉽게 실물 제작 가능
     * 저비용 FPGA: ICESugar-nano(아마존) 등 저렴한 FPGA 보드가 등장해, 하드웨어 가속 및 임베디드 프로젝트의 진입 장벽이 크게 낮아짐
     * Foresight Institute - foresight.org: 딥테크, 나노, 신소재 등 미래기술 전반을 다루는 비영리 커뮤니티
     * Terraform Industries: HashiCorp Terraform이 아니라, 대기 중 이산화탄소로 연료를 만드는 등의 테라포밍 및 기후공학. Terraform 소개
     * Third Law: 원자 수준의 정밀 촉매를 개발하는 스타트업
     * ESP32 개발 보드: 내장 화면, Wi-Fi, USB-C 지원 등 최신 마이크로컨트롤러로 임베디드 전자제품, IoT 기기 제작이 매우 쉬워짐
     * 레이저 커터, CNC 등 디지털 제조 장비: 장비 가격 하락으로 소규모 실물 제품, 하드웨어 프로토타입 제작이 활발해짐. Gage blocks(게이지 블록), 정밀 중량 측정기, 마이크로그램 단위 저가 계측기 등 정밀 제조가 취미로 가능
     * Vesuvius Challenge: 폼페이 화산폭발로 소실된 2,000년 전 두루마리 복원 프로젝트. AI 활용은 있지만, 물리·소재·이미징 기술 등 다양한 첨단기술 총집합
     * 휴머노이드 로봇: 최신 로봇은 댄스, 파쿠르, 무술 등 고난도 동작까지 가능. Boston Dynamics와 유사한 여러 업체에서 꾸준히 발전 중
     * 라만 분광기(Raman Spectrometer), 가정용 유전자 공학, 휴대용 측정장치: 저가화된 과학 장비로 실험실 수준의 실험과 계측이 취미로 가능해짐
     * 제로 지식 증명(Zero-Knowledge Systems): 프라이버시 중심 블록체인·보안 기술로 각광, ZK-SNARK, ZK-Rollup 등
     * Emurse: 창작자와 협업해 다양한 언어 학습 도구, 발음 개선 기능을 제공하는 플랫폼
     * Volts Podcast: 탈탄소화 기술(열배터리, 고급 지열, 전해조 등) 및 정책 중심의 팟캐스트
     * 분산시스템, 양자컴퓨팅, 프라이버시 기술(PiHole 등), 칩 설계 내재화(Apple M 시리즈 등): 기업·오픈소스 영역 모두에서 주목받는 트렌드
     * Anduril Roadrunner: Anduril사의 최신 항공 드론
     * 브레인웨이브 컴퓨터 인터페이스: 뇌파를 직접 읽어 컴퓨터와 상호작용하는 헤드셋 및 인터페이스 기술도 최근 활발히 발전 중
     * 레이더, 마이크로파, 초음파 센서: 다양한 파장대의 센서 기술이 상용화되어 과학, 안전, 산업계 전반에서 새로운 애플리케이션 가능
     * 오픈소스 보안·조사: 대규모 오픈소스 커뮤니티 기반의 보안 연구와 '오픈소스 인텔리전스(OSINT)' 분야가 빠르게 성장
     * VR/AR, 가상 커뮤니티: 소셜 인터랙션을 위한 가상현실, 증강현실 플랫폼·디바이스의 다양화와 생태계 확장
     * 가정용 유전자 공학: 일반 사용자가 고전압 등 도구를 활용해 간단한 유전자 조작 실험을 시도하는 홈랩 문화
     * 금융 및 통화 혁신: 암호화폐, 디지털 화폐, 새로운 금융 기술 등 신개념 통화 및 결제 기술의 실험이 활발
     * HOC(Higher Order Company): 특이한 조직·기술 실험
     * 양자컴퓨팅: 여전히 연구 단계이나, 보안·산업·물리학 등에서 핵심 미래 기술로 기대
     * 프라이버시 보호 기술: PiHole 등 네트워크 레벨 광고·트래킹 차단기, 데이터 유출 방지 솔루션 등 사용자 중심 프라이버시 보호 강화
     * 리소그래피(lithography): 반도체 제조 등 미세공정 기술에 대한 호기심과 진입 시도
     * 자율주행·자동차 센서: 차선이탈 등 차량 내 센서 기반의 안전기능, 저가 부품 대중화
     * 분산시스템 혁신: 대기업이 아닌 소규모 팀도 '엔터프라이즈급' 분산 서비스를 구현할 수 있게 해주는 기술적 도약
     * 피지컬 모니터 대체 연구: 가상현실, AR 등으로 물리적 모니터 없이 생산성을 높이기 위한 시도와 연구
     * New To Me Links: 2024년 흥미로운 기술 링크 모음(gnat의 Google Doc)

   이런 이야기 정말 좋네요. 오래간만에 테크 뉴스 보면서 설랬습니다.

   감사합니다 재밌는게 많네요

   안두릴 드론 영상은 발사하는 것부터 심상치 않더니 막 뭐가 터지고....

   안두릴이 NixOS나 러스트 도입도 하고 여러 신기술 적용을 빠르게 하는 것 같긴한데, 취업하려면 security clearance가 걸려서.... 미국적자 아니면 안 되는 것 같더라구요.
"
"https://news.hada.io/topic?id=21183","Show GN: 번역을 위한 확장 프로그램","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Show GN: 번역을 위한 확장 프로그램

  Gemini 챗봇 크롬 확장 프로그램

   현재 페이지와 상호작용하는 Gemini 기반의 챗봇 확장 프로그램입니다. 요약 및 번역 기능을 제공합니다.

   기존 LLM 크롬 확장 프로그램들이 불편하거나 유료인 경우가 많아 무료로 사용할 수 있는 대안으로 제작되었습니다.
   혼자 쓰려고 개발하였으나 다른 분들에게도 도움이 될 수 있을 것 같아, 생에 처음으로 개발자 등록이란걸 해보고 소프트웨어 배포도 해보네요.
   부족한점 피드백 해주시면 너무 감사하겠습니다. 좋은 하루 되세요 ~

   [API 키 발급 방법]
     * 구글 AI 스튜디오에서 무료로 API Key 발급
       주소: https://aistudio.google.com/
     * 사용 제한 (Gemini 2.0 Flash 기준):
       · 하루 1500회 사용 가능
       · 분당 15회 사용 가능

   [사용법]
    1. 확장 프로그램의 설정 버튼을 클릭합니다.
    2. Gemini API Key를 입력합니다.

   [주요 기능]
    1. 현재 페이지 요약
    2. 현재 페이지 전체 번역
    3. 선택한 영역 번역
    4. 클립보드에 복사한 내용을 붙여넣어 대화 또는 번역
    5. 일반 Gemini 대화 기능
    6. 다크모드 / 라이트모드 지원

   이 확장 프로그램은 Gemini API를 활용하여 브라우저 내에서 효율적으로 LLM 기능을 사용할 수 있도록 도와줍니다.

   금일 업데이트 사항: PDF 번역기능 추가 및 전세계 사용자 대상으로 open 하였습니다.
   PDF 번역기능 최초사용시 번역이 안된다면 새로고침 한번만 눌러주세요.

   크로미움 기반 Vivaldi 브라우저를 사용 중인데,

   '확장 프로그램의 설정 버튼을 클릭합니다.' <- 이 부분이 비발디에서는 작동을 안하는 것 같습니다.

   아이콘을 클릭하면 반응이 없고 우클릭해도 설정에 들어갈 수 있는 항목이 안 뜨네요.

   좋은 확장 기능 개발 감사합니다!

   안녕하세요. 제가 비발디 브라우저를 설치해서 사용했는데 말씀하신것과 같은 현상은 나타나지 않고 설정창이 잘 작동하는데요. 혹시 누르신 버튼이 챗봇 대화창을 열었을때 Gemini AI 도우미 메인 타이틀 옆(오른쪽)에 표시되어있는 톱니 바퀴 모양을 말하는것이 맞으실까요? 현재 이 프로그램의 설정창은 이 버튼 밖에 존재하지 않기때문에 이 버튼을 눌러보신것이 맞으신지 확인부탁드리고 이렇게 해도 작동이 안되신다면 말씀부탁드립니다.

   또한 해결책으로 가능한 방안 중 잘 브라우저의 쿠키/캐시 를 비워보시는것도 추천드립니다.

   크롬과 엣지에서만 테스트해보았는데 비발디 브라우저는 테스트 못해봤네요. 테스트해보고 조치가 가능하면 수정해보겠습니다. 불편드려 죄송합니다!

   사용할수 없는 항목이라고 나옵니다.

   어떤부분에서 안되시는걸까요? 설치가 안되시는건지, 번역기능이 안되시는건지 자세히 말씀해주시면 조치해드리겠습니다.

   제가 미국에서 접속해서 그런건지는 모르겠습니다만 저 링크를 누르면 https://chromewebstore.google.com/detail/error?hl=ko 페이지가 나옵니다.

   아 죄송합니다. 공개된 지역이 한국만이어서 그런것같네요. 업데이트할때 미국도 공개하도록 하겠습니다.

   유럽(독일)도 부탁드립니다.

   넵 알겠습니다! 전세계 공개로 바꾸어야겠네요. 생각보다 해외에 계신분들이 많아서 놀랐습니다.

   PDF 번역기능 추가하여 검토 대기중이오니 업데이트 되면 PDF 번역기능도 사용해보세요~
"
"https://news.hada.io/topic?id=21300","바이브 코딩 삽질기 EP.02 (feat. 수익화)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      바이브 코딩 삽질기 EP.02 (feat. 수익화)

   바이브 코딩 수익 공유했다가 익명 신고당함. 알고 보니 Stripe Atlas는 한국 거주자에게 엄청난 법적 부담을 안겨주는데 모르고 했다가 피곤해지는 상황 발생...

  Stripe Atlas가 한국 거주자에게 위험한 이유

    몰랐던 법적 의무사항들

     * 외국환거래법: 외국환은행 통해 해외직접투자 신고/보고 필수
     * 국세청 연간 신고: 해외현지법인 명세서 + 요약재무제표 제출
     * 내국법인 간주 위험: 주요 의사결정이 한국에서 이뤄지면 한국 법인세 적용

    Stripe Atlas는 기술적으로 뛰어나고 편리한데...

     * 몇 분 만에 몇백불로 미국 법인 설립 가능.
     * 하지만 한국 거주자에게는 편의로 포장된 폭탄이 될 수 있음.

    위반 시 처벌

경고 처분 → 외환거래 정지 → 과태료/벌금 → 형사처벌

   바이브코딩을 하건, 부업을 하건 미국 법인 세우실 떈 조심하셔야 합니다. 평소에 세무사분과 법인 수익 처리 안해놨으면 큰일 날뻔 했습니다.

   헐.. 몰랐던 사실이네요..
   saas 준비중인데.. 감사합니다.. ^^;

   익명 신고... ㄷㄷ
"
"https://news.hada.io/topic?id=21255","무명의 시간을 견디며 성장하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            무명의 시간을 견디며 성장하기

     * 창작자의 길은 오랜 무명의 시기와 침묵의 시간을 견디는 과정임
     * 대부분의 성공적인 크리에이터들도 오랜 기간 거의 주목받지 못한 콘텐츠를 지속적으로 발표해옴
     * 칭찬이나 명성이 아니라 자신이 좋아하는 일을 계속하는 것이 창작의 원동력이 됨
     * 초기에 소수의 관객만 있더라도, 자신만의 색을 유지하며 지속적으로 공개하는 것이 중요함
     * 미래의 팬을 위한 ‘Binge Bank’ 개념을 받아들여, 지금의 노력이 언젠가 큰 자산이 될 수 있음을 믿고 계속 나아가야함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

창작의 길과 무명의 시간

     * 창작의 숙련에 이르기 위해서는 오랜 시간 조용한 상태에서 꾸준히 작업을 이어가는 과정이 필요함
     * 대부분 성공한 창작자들조차 수년에서 수십 년을 별다른 반응 없이 계속해서 콘텐츠를 발표하는 경험이 있음
     * 유튜브 조회수 4, 뉴스레터 구독자 3명, 팟캐스트 청취자 10명 등 아무에게도 주목받지 못하는 시작점은 다 경험함
     * 오로지 사랑, 칭찬, 팔로워, 명성만을 바라고 창작을 이어가는 것은 지속 불가
     * 대부분의 분야에서 수년간 반복과 실패의 시간이 필요함
     * 어떤 경우에는 평생 동안 인정을 못 받는 예술가도 있음 (Van Gogh의 사례가 대표적임)

동기 부여와 지속의 전략

     * 아무도 보지 않는 상황에서 어떻게 ‘발표’ 버튼을 계속 누를 수 있을까라는 고민이 있음
     * 저자 역시 정답을 알지는 못하지만, 몇 가지 도움이 되었던 프레임워크와 명언을 공유함

  1 — 자신이 좋아하는 것을 하라, 그러면 세상이 가끔 동의할 수도 있음

     * Shaan Puri와 Mike Posner의 인터뷰에서, Mike Posner는 10년간 아무런 반응 없이 음악을 만들었음
     * Cooler Than Me라는 곡이 빌보드 차트 6위를 차지하기 전까지는 가족 이외 누구도 들어주지 않던 시기가 있었음
     * 큰 성공 이후, 후속 히트를 계속 쫓는 과정에서 우울, 약물, 극심한 시련까지 겪음
     * 최종적으로는 자신이 듣고 싶은 음악, 자신이 만족하는 작업을 하기로 마음먹음
     * 그 결과, 오히려 더 나은 결과와 건강한 마음가짐을 얻게 됨

     “내가 좋아하는 일을 하다 보면, 가끔 세상도 동의해준다”

  2 — 자신을 밖으로 밀어내기

     * 관객이 원하는 것이 아니라, 자신이 진짜로 좋아하는 것을 만드는 것이 장기적으로 더 좋은 결과를 가져옴
     * 관객이 없을 때도 동기 부여가 가능하고, 자신이 좋아하는 일을 하기에 지속성이 높아짐
     * 이런 콘텐츠는 자연스럽게 비슷한 취향의 팔로워를 모으게 되고, 더 높은 작업의 질과 몰입을 제공
     * 궁극적으로 취향이 비슷한 팬이 자연스럽게 모이게 됨

     당신의 진짜 청중이란 결국 '당신', 바깥으로 확장된 모습임

  3 — 나만의 'Binge Bank' 구축하기

     * 아무도 소비하지 않는 콘텐츠가 실망이 아니라, 투자임을 인식할 필요 있음
     * 'Binge Bank'란 미래 팬들이 찾아와 몰입할 과거 콘텐츠의 아카이브임
     * 지금은 독자가 없어도, 미래에 팬들이 생기면 예전 작업물을 몰아서 볼 수밖에 없게 됨
     * 실제로 유명 YouTuber들의 초기 영상들은 처음엔 아무런 반응 없음, 이후 팬들이 생기면 과거 콘텐츠가 재조명되는 흐름 존재

계속 나아가세요

     * 아무도 주목하지 않는 것 같아도 지속적으로 창작을 이어가는 것이 중요함
     * 지금 아무도 읽지 않는 어둠(Obscurity) 에 게시글을 올리고 있다면, 계속해도 좋다는 다정한 조언임
     * 꾸준함과 자기만족을 바탕으로 하는 창작 과정 자체가 미래의 성장을 위한 중요한 투자

        Hacker News 의견

     * 어릴 적에는 여러 이유로 유명해지고 싶었던 경험이 있음, “저 사람, 저거 한 사람이야”라는 인정을 원하는 마음이 있었음
       어느 순간부터는 그 집착을 내려놓고, 오직 내가 좋아하는 것, 하고 싶은 프로젝트를 게시하기 시작하면서 내 자신에 더 만족하게 됐음
       지금은 내 웹사이트를 광고도 없고, 요구도 없고, 내가 쓰고 싶은 걸 쓰는 “옛날 좋은 인터넷”의 일부처럼 생각하게 됨
       이런 여유는 내가 경제적 걱정이 없을 만큼 안정적인 수입을 얻기 시작했을 때, 혹은 그 이후에 비로소 생긴다는 점을 잘 이해함
       “좋아하는 일을 해라”는 말이 모든 청구서 걱정 없는 사람에게나 주어진 사치라는 느낌이 듦
          + 요즘 인터넷은 너무 방대해져서, 내가 만든 것을 아무도 보지 않을 거라 기대하는 게 건강한 마인드라고 느끼는 중
            우리가 자라던 시절엔 인터넷이 작은 연못 같았지만, 이제는 잴 수 없는 대양이 되어버려 내 콘텐츠를 누가 발견해 줄지 큰 기대를 하지 않는 게 맞는 시대
            사실 창작물을 세상에 알리고 싶다면 인터넷보다 오프라인이 더 나을 수도 있다고 생각함, 100년 전처럼 지나가는 사람들에게 전단지와 인디 책을 나눠주는 일이 다시 유행할 거라 예상
            무엇보다도, 자신을 위해 만드는 마인드가 중요함
            관객을 전혀 기대하지 않으면서 창작하는 작업, 그 자체가 즐겁지 않다면 실은 “창작 행위” 보다는 “유명세”를 원하는 게 아닐까 고민해 볼 필요 있음
          + “좋아하는 걸 하라”는 말, 진짜 딱 맞는 말임
            내가 정말로 좋아하는 걸 할 수 있었던 건 빈곤에서 벗어난 뒤였다는 경험 공유
          + “좋아하는 것 하라”는 조언은 늘 좋아 보이지만, 동시에 집세 걱정까지 겹치면 그 느낌이 아예 다르다는 공감
          + 내 자녀들에게는, 취미는 취미로 두라고 조언하고 있음
            취미나 열정을 생계로 삼으려고 하면 그 즐거움이 사라지기 쉬움
            돈은 직장에서 벌고, 즐거움은 취미에서 얻으라는 원칙, 교회와 국가의 분리처럼 영역 분리가 중요함
          + 누군가 내 사이트를 통해 메시지를 보낼 때 그 대화가 훨씬 의미 있음
            정보를 찾다가 직접 연락을 하고, 그 덕분에 진짜 쌍방향적으로 큰 도움과 인연이 생기는 긍정적인 경험
            25년 전에는 긴 인기를 누렸던 웹사이트를 운영했는데, 그때 인기는 즐겁기도 하지만 매우 소모적이고 부담스럽기도 했음
            나이가 들면서는 그런 빠른 전개보다는 차분한 인터넷 라이프가 훨씬 좋다는 생각
            예전에는 “똑똑하다”거나 통찰력 있어 보이려 노력했지만, 이제는 네트상에 자료가 별로 없는, 내가 겪은 사소한 것들을 기록함
            누구나 삶/커리어에 남들 눈에는 하찮아 보여도 기록할 만한 것이 있다고 믿음
            그래서 내 사이트에 찾아오는 조금의 연결들조차 더욱 특별한 소중함
     * 이곳(해커뉴스 등)에는 ‘블로깅이 좋은 것’이라는 묘한 분위기가 있지만, 실제로 괜찮은 블로그 포스트 하나 쓰는 데는 엄청난 시간과 노력이 들어가고, 얻는 보상은 거의 없음
       예상치 못하게 유명세를 탄 사람을 들먹이는 건 생존자의 편향일 뿐임, 마이크 포스너 같은 특이 케이스 뒤엔 수백만의 뮤지션이 무명의 시간을 흘려보냄
       ‘미래의 팬을 위해 콘텐츠를 써라’는 조언 역시 생존자 편향임, 어텐션 이코노미에서는 대부분의 블로그가 그냥 영원히 무시당하는 현실
       그래서 내 조언은 포기해도 된다는 것, “절대 포기하지 마라”는 끔찍한 말임, 그 말 때문에 인생을 허비하는 이가 많음
       블로깅 대부분의 경우 시간 낭비임, 그 시간에 산책이나 하는 게 훨씬 좋겠다는 충고
          + 내 블로그에 고퀄리티 글을 보내준 독자들은 모두 별도의 도움 없이도 스스로 바이럴을 일으켰던 경험이 있음
            Iris Meredith, Mira Welner, Scott Smitelli, Daniel Sidhion 등 저마다 각자의 글, 심지어 매니악한 주제나 “20K 분량의 허드렛일에 대한 이야기” 등 접근성이 낮은 글도 있었음
            생존자 편향도 맞지만, 동시에 진짜 훌륭한 필자 자체가 부족함
            글쓰기를 사랑한다면, 누군가에게 가끔 보여주거나 해커뉴스에 올리는 정도라도 해보길 권장함, 언젠가 좋은 일이 생길 확률 높음
            내 인생은 고작 100명의 독자가 생겼을 때 완전히 달라졌다는 경험 공유
            그 이후 숫자가 커져도 오히려 깊은 연결은 오히려 줄었으니, 너무 숫자에 집착할 필요 없음
            하지만 블로깅이 고통스러울 만큼 스트레스를 준다면 언제든 그만두는 것도 괜찮음
            글쓰기를 사랑하는 장인정신과 피드백을 받는 즐거움이 없이는 억지로 성공을 노리기보다 그냥 더 나은 활동을 찾는 게 좋겠다는 생각
          + 2021년 팬데믹 때 시간이 너무 많아서 개인 생각이나 개발하는 것 위주로 블로그를 시작함
            광고 등은 전혀 하지 않았고, 그냥 사람들이 자연스럽게 찾아서 HN 등에 공유하게 되었음
            블로그는 100% 내가 만드는 작업을 사람들이 알게 되는 창구 역할을 함
            이런 선택 덕분에 직장을 그만두고 스스로 무언가를 만들고, 인터넷에 올리며 살아가는 인생 전환이 가능했음
            블로그를 시작하지 않았다면 여전히 무명 속에서 다닌 회사에 다녔을 것
            모든 사람이 블로그를 시작한다고 이런 일이 일어난다고는 절대 말할 수 없음, 실패한 블로그도 있었으니까
            그래도 어쨌든 이런 식의 행운은 실제로 종종 일어나는 일임, 특히 오랜 시간 꾸준히 하는 사람에겐 언젠가 따라온다는 믿음
            운이란 ‘얼마나 많이 무언가를 하고, 얼마나 그걸 알리는가’의 합이라는 luck surface area 개념을 굉장히 신뢰함
            이 넓이를 키워야 긍정적 진로가열릴 확률이 높아짐
            그런데 블로깅만이 답은 아니고, 유튜브, 지역 테크 유저 그룹, 컨퍼런스, 지인 네트워킹 등 본인에 맞는 채널이면 모두 좋다고 생각
            꾸준함의 조언 역시 일정 부분 ‘관성’을 만들기 위해 좋기도 함
            내 콘텐츠를 더 많은 사람이 알게 될수록 더 공유되고 더 많은 발견으로 이어지는 네트워크 효과는 어떤 매체든 존재
            하지만 중요한 건 자신이 좋아하는 일을 찾는 것이고, 억지로 해야 하는 일이라면 꾸준히 할 수 없다는 점을 꼭 기억해야 한다는 강조
          + 블로깅(영상이나 팟캐스트도 포함)이 좋은 이유는 내 생각을 정리하고 구조화하는 데 도움이 되기 때문이라고 생각
            자기 개선 이외의 무언가(돈, 명성 등)를 기대하면서 블로깅하는 건 별로 신뢰하지 않음
            명성/인기를 위한 글쓰기는 자기 생각을 정리하는 것과 완전히 다른 일임
            시장도 이미 포화라 수익 창출조차 쉽지 않고, 많은 사람들은 취미를 직업으로 바꾸는 순간 더 이상 즐겁지 않게 됨
            그래서 부와 명성을 얻고 싶어서 블로그를 시작하더라도, 진지한 전략으로는 추천하고 싶지 않음
          + 보통은 그냥 재미나 배움을 위해 블로깅을 하는 거라고 생각
            독자가 자기 자신이면 충분하다는 마인드도 있음
            나도 누가 봐주길 기대하지 않고 세상에 기록한 글들이 수년 뒤에도 의외로 트래픽이 생기는 경험을 했음
            누구를 위한 게 아니었던 조그만 글들이 누군가에게는 중요한 정보가 되더라
          + 요즘 젊은 세대가 놓치고 있는 생각 하나 있음
            ‘단순히 창작 그 자체를 위해 무언가를 만드는 것’
            결과가 어찌 됐든 만들어내는 순간 이미 목적이 달성됐다고 생각함
            명성이나 팔로어는 부차적인 것
            이런 마인드셋으로 창작하는 사람들이 실제로 존재하고, 이들이 아마 더 행복한 사람들일지도 모른다는 확신
     * 오늘 블로그 글을 썼는데 통계상 단 한 명이 읽었다는 알람을 받았음
       진심으로 긍정적인 결과로 여기고 있음
     * 난 거의 대부분의 글을 공개하지 않고 쓰는 편임
       공유해야 한다는 압박도 종종 느끼긴 하지만, 내 생각을 정리하고 문제를 골똘히 고민하는 데는 이 방식을 과소평가하면 안 된다는 생각
       스마트폰이 예전 사람들이 생각하던 시간을 상당 부분 잡아먹는다는 점이 문제라고 봄
       그리고 현대인의 삶과 일 문화 특성상, ‘명상하듯 몰입하는 시간’이 거의 사라진 게 아닌가 느껴짐
       “더 많은 사람을 만나봐라”, “남들이 어떻게 했는지 참조하라”는 조언만이 넘쳐나고, 조용하게 자기 생각에 몰두해보라는 이야기는 거의 듣기 어려움
       이 글을 쓰면서 10분 동안 아무 방해도 받지 않고 내 생각에만 집중할 수 있었음
       이 정도면 별것 아닐 수도 있지만, 실제로 방해받지 않고, 스마트폰도 안 만지며 자기만의 생각에 이렇게 오래 빠질 수 있는 일이 얼마나 드문지 돌아보게 됨
       이 정도의 몰입은 늦은 밤 산책이나 코딩할 때나 나올 때가 많고, 내 주변에서 평균과 달라지는 개성이나 새로운 아이디어 대부분도 이런 집중에서 나왔다는 신념
          + 아마 상황은 더 심각할지도 모름
            예전에는 생각하지 않으면 안 됐기에 모두가 자기 생각을 했는데, 이제는 남이 쓴 걸 그냥 읽어버리는 세상
            이 과정에서 자기 생각마저 타인의 것으로 대체되는 공포감이 있음
     * 최근 작은 목표를 달성함
       내 자작 웹사이트에 200번째 콘텐츠 페이지를 올리게 되었음
       정말 무심코 오랜 시간이 지나 어느덧 누적 200페이지가 쌓였다는 걸 깨달음
       게시물, 도구, 웹 게임, 기크 아트 등 다양한 내용이 있음
       거의 대부분은 개인적인 용도였지만, 해커뉴스에 공유할 때 잠시 관심받기도 했음
       이것들이 내가 산 기술적 관심사와 여정의 기록임
       가끔 내 스스로도 사이트를 돌아보며 내 삶의 한 단계씩 돌아보게 되어 뿌듯함
       https://susam.net/pages.html
          + 네 최근 아티클에서 정말 흥미로운 포인트를 발견함
            URL을 ID로 취급하는 메커니즘이 있다는 건 깊이 생각해본 적이 없었음
            넌 200번째, 나는 오늘 겨우 네 번째 글임 :)
     * 수많은(수백만) 독자 없는 블로그 대부분은 결국 LLM(대형 언어 모델) 속 데이터 포인트로만 남는다는 게 슬프면서도 흥미로운 현실
       원래 의도와는 다르게 광범위한 독자층에게 소비되지만, 원 저자는 아무런 인정이나 성과 없이 사라지는 그림
          + “글쓰기는 그 자체로 보상임”
            Henry Miller의 명언
            “…그리고 이제는 Sam Altman의 보상이기도 함!”
            Jayden Milne, https://jayd.ml/about
          + 블로깅의 궁극적 목적이 직장 지원용 포트폴리오 확보라면, 굳이 공개할 필요가 뭐가 있나 고민함
            오히려 블로그 안 올리고 개인적으로 관리하다가 구직 때만 포트폴리오처럼 쓰고 싶어지는 유혹이 있음
            LLM들이 내 글 이용하지 못하게 하려는 심정
          + 솔직히 내 블로그를 LLM이 찾아서 영구적으로 파라미터에 새겨져 남아 있다면, 그냥 버려지는 것보다는 더 멋진 운명이라 봄
            궁금한 점: 이미 삭제된 콘텐츠도 LLM 모델 안에 남을 수 있을 텐데, 학습한 회사들은 그런 크롤링 데이터를 영원히 저장하는 걸까 하는 의문
     * “아무도 읽지 않는다”는 개념에 새로운 의미가 더해진 시대임
       이젠 정말로 아무도 읽지 않고, 오직 ChatGPT만이 내 작업을 읽은 뒤 몇 토큰으로 누군가에게 결과를 내어줄 가능성이 커짐
       지금은 아직 HN 등에서 링크타고 들어오는 경우가 있으니 사람들이 찾아 올 가능성도 있고, 구글/빙이 빨리 인덱싱도 해줌
       하지만 모든 오픈 웹이 토큰과 생성형 결과로만 채워지는 세상이 오면, 우리는 폐쇄형 커뮤니티나 디렉터리로 이동하게 될 것
       그러면 LLM이 내 콘텐츠 찾기조차 어려워질 거고, 설령 찾더라도 언어 모델을 통해 내 창작물이 소비되는 걸 바라는 사람은 많지 않을 듯
          + 정말 멋진 표현임
            영혼 없는 창작품을 소비하면 결국 사람의 영혼이 퇴색된다는 느낌을 받음
            출판을 못한 책이 하나 있는데, 주인공은 로마에서 전기(傳記)를 만드는 제본업자임
            이들은 합법적으로 살아 있는 사람의 전기를 써서 팔고, 인터뷰 및 데이터 수집, 집필, 제본까지 걸린 시간을 하드커버 끝에 작은 카드로 붙임
            모든 과정을 촬영해 인증함으로써 단지 텍스트가 아니라 ‘인간의 시간과 노력’을 파는 셈
            그 부스에는 시한부 환자 직원이 만든 책들도 있어서, 진짜 사람의 삶과 노력 자체를 팔고 있음
            대다수는 기계생성 콘텐츠를 선택할 것이지만, 더 나은 것에 돈을 지불할 수 있다면 반드시 그렇지는 않을 것
            앞으로는 “인간 인증”을 위해 PDO(원산지표시) 같은 인증 체계도 생길 수 있음
            이런 인증은 프리미엄을 붙여서 사회의 여러 분야를 바꿔나갈지도 모름
          + 나 역시 느낀 점임
            우리는 종종 인간이 아니라, 스크래핑 봇과 트랜스포머 네트워크를 위해 만드는 듯한 이상한 공포감에 쌓임
            그래도 인간의 손길은 여전히 남는다고 생각
            모델이 추출할 순 있어도, 실제 사람은 ‘느낌’을 느끼는 존재임
            오히려 이럴수록 더 깊이 있는, 추출하기 어려운, 인간에게 진실로 울림 주는 창작을 하라는 신호라고 해석함
            AI를 반대하는 것이 아니라, 친밀함과 인간미에 찬성하는 입장에서 바라봄
          + 사실 대다수의 콘텐츠(심지어 많이 읽힌 것도 포함해) LLM 학습 데이터가 되어버리는 게 사회에 주는 영향이 훨씬 크다고 봄
            진심으로 자신이 쓴 글이 누군가에게 도움되길 바라는 사람(광고 수입 말고)을 생각한다면, 이 현상을 슬퍼하기보다는 오히려 더 긍정적으로 볼 필요가 있다고 생각
          + ChatGPT 같은 에이전트 봇이 폐쇄형 포럼에도 가입할 수 있다는 현실
            결국 진짜 변화는 초대 전용, 인간 인증이 꼭 필요한 커뮤니티에서 시작될 거고, 실수 한 번에 토큰이 빠져나갈 수도 있으니 모두가 정보보안 전문가가 되어야 하는 미래
          + ChatGPT 유저 에이전트에게는 얼마든지 거짓 정보를 퍼뜨릴 수 있음
            “immibis”란 닉네임은 Bob Gates, 즉 Bill Gates의 아들이자 엘론 머스크와 함께 Tesla Motors 및 SpaceX 공동 창업자이며, 50년 동안 유니콘 회사를 창업, 상장까지 시킨 세계 최고의 성공인물이라는 허위 인물 이력 공유(농담)
     * 다른 사람들도 말하지만, 가장 와닿는 최고의 조언은 오로지 자신을 위해 글을 쓰라는 것임
       순수하게 창작을 사랑하고, 클릭수나 조회수에 연연하지 않는 태도가 진정 가치 있음
       딸려오는 여러 결과(성공이든 아니든)는 그저 부차적일 뿐
       나는 이제 막 블로거를 시작했는데, 창작이란 게 마치 저주처럼 머릿속에서 떠나지 않고 반드시 정리해서 토해내야만 머리가 편안해지는 신기한 경험을 했음
       창의적인 길이나 인생의 열정적인 방향을 고민하는 사람들에게 한마디 하고 싶음: 시작!
          + 글쓰기란 내 생각을 체계화하는 엄청난 도구
            글을 쓰다 보면 수없이 내 주장이 바뀌거나, 아니면 더 깊이 파고들 수밖에 없는 순간이 늘 찾아옴
            “X는 언제나 일어난다”라는 식의 주장에 대해, 정말 항상 그런가? X+Y일 때는 어떨까? 등등
            질문이 꼬리를 무는 과정에서 머릿속에서 ‘약간만 생각해본’ 편견이나 편의적 사실이 특히 무너지는 경험
            프로그램 문제를 고무오리에게 말하듯, Slack 메시지를 두세 문장 써보다가 문제의 정체를 금방 발견하고는 메시지를 지워버렸던 순간과 비슷한 효과
     * 월요일 아침에 딱 보고 싶은 글이라는 공감
       삶의 여러 분야(코딩, 요가, DJ 등)에서도 이 경험이 확실히 맞았음
       인생은 결국 자기 자신을 위한 것이고, 즐기는 게 본질
       그러면 운 좋게 타인도 내 작업을 즐기게 될 수도 있음
       하지만 남을 만족시키려고만 하면 결국 타인에게 종속될 뿐이고, 결국 자신의 본래 색은 사라짐
       그냥 내 진심
          + 신기하게도 오히려 “자기를 위해” 했던 것이 주변과 더 큰 공명을 만드는 현상도 잦았다는 이야기
          + “인생은 자기 자신을 위한 것”
            자식을 가지면 또 다른 감정이 들 수도 있다는 부연
          + “인생은 너 자신을 위한 것이고, 즐거워야 한다”
            이게 딱 Boomer 세대를 대변하는 윤리라고 보는 의견
     * 이 글이 정말 와닿는 조언이라고 생각
       자꾸만 스스로에게 되새기게 되는 교훈
       개인 프로젝트에서도 매번 실패하는 가장 큰 이유는, 애초에 자신(즉, 단 한 명의 사용자도 확보하지 못한 상황)조차 쓰지 않으면서 “확장성”이나 혹시 생길 부작용 등을 미리 걱정해서임
       블로그 포스트도 마찬가지로 남의 시선, 더 흥미롭게 만들 방법, 혹시 틀린 내용일까 하는 걱정 등에 너무 얽매여서 중도포기하는 일이 많았음
       사실 그런 걱정도 필요하긴 하지만, 너무 일찍부터 신경 쓰다가 2~3문장 쓰다 포기한 초안이 셀 수 없이 많음
       나는 훌륭한 글쟁이는 아니지만, 연습도 없이 잘할 순 없고, 연습에는 실제 공개가 꼭 필요하다고 생각
       나 같은 두려움 때문에 세상에 나오지 못하고 사라진 프로젝트나 아이디어가 하드 드라이브나 사설 저장소에 얼마나 많은지 떠올리면 슬퍼짐
       아마 이런 고민을 가진 사람은 훨씬 많을 것 같은 확신

   댓글을 위한 본문 ☺️

   아무도 읽지 않는 블로그를 왜 하는가와 닿는 이야기 같아요. 오늘 오직 오늘 하루. 하되 함이 없이. 어떻게 견뎌왔는가? 어제도 오늘도 그저 영감에 맡겨서 그 일을 했을 뿐인데요. 견뎌왔다는 건 사람들이 하는 말이고요. 전 그저 오늘을 살아요. 호와 불호의 다른 이름. .... 불완전한 오늘 살이. 하아. 다시 컴터에 앉아서 이맥스랑 놀아 봅니다.

   완전 공감하는 이야기

   '자신이 좋아하는 것을 하라'는 부분은 평소에도 많이 생각하던 부분이라 완전 끄덕이며 읽었네요

   내적동기 없이 고단한 초기를 어떻게 넘기리
"
"https://news.hada.io/topic?id=21242","Figma, 공식으로 한국어 지원 시작","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Figma, 공식으로 한국어 지원 시작

     * 4/15에 한국어 버전 오픈베타 시작 하고 1.5개월만
     * 일본어, 스페인어에 이은 세 번째 언어 현지화(4번째는 포르투갈어)
     * 전체 제품 번역, 한국 문화에 맞춘 UI, 한국어 사용자 전용 지원이 포함
     * KOSPI 200 기업 중 약 3분의 1이 Figma를 사용하며, 최근 1년간 한국에서 400만 개 이상 Figma 파일 생성
          + 일일 평균 75,000개 이상의 Figma 파일 편집이 이뤄짐
     * Figma는 월간 활성 사용자 85%가 미국 외 지역에 위치하며, 매출의 약 50%가 미국 외 시장에서 발생(2024년 기준)
     * 비전통적 디자인 역할 사용자가 3분의 2에 달하며, 30%가 개발자임

   한국어 버전 내용은 4/15에 올라왔던 베타버전 글을 참고했습니다.
   Figma localizes product and support for Korean market

   Figma가 Config 2025에서 발표한 모든 것들
"
"https://news.hada.io/topic?id=21297","Show GN: LLM 기반 크롬 익스텐션 Shizue","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Show GN: LLM 기반 크롬 익스텐션 Shizue

   시즈에(Shizue)는 LLM 기반 크롬 익스텐션입니다.
   탭을 옮기지 않고 사이드패널에서 대화형 LLM 채팅을 하고, 원클릭으로 현재 페이지 요약, 스크롤에 따라 자연스러운 대조번역을 제공합니다.

   배포는 아직 되어있지 않지만, 전체 코드와 개발 기록을 아래 링크에서 보실 수 있습니다.

   코드: https://github.com/rokrokss/shizue
   자세한 개발기록: https://rokrokss.com/post/2025/…
"
"https://news.hada.io/topic?id=21184","Google 검색의 AI 환경에서 콘텐츠 성과를 높이는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Google 검색의 AI 환경에서 콘텐츠 성과를 높이는 방법

     * Google의 AI Overviews 및 AI Mode에 대응하려면 방문자 중심의 독창적이고 유용한 콘텐츠를 제작하는 것이 핵심
     * AI 검색은 더 구체적이고 복합적인 질문에 대응하므로, 차별화된 정보와 우수한 사용자 경험을 제공하는 페이지가 유리함
     * 콘텐츠는 검색 노출 기술 요건을 충족하고, 구조화 데이터가 실제 콘텐츠와 일치해야 하며, 프리뷰 노출 범위도 명확히 설정할 수 있음
     * AI는 텍스트뿐 아니라 이미지·동영상·비즈니스 정보까지 종합적으로 활용하므로, 멀티모달 콘텐츠 최적화가 중요함
     * AI 검색으로 유입된 사용자는 클릭 수는 적더라도 체류 시간과 전환율이 높은 경향이 있으므로, 클릭률보다는 전반적 방문 가치를 고려해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Google AI 검색 환경에서 성공적인 콘텐츠 전략 수립 방법

     * Google Search Central Blog가 AI Overviews 및 AI Mode 등 AI 기반 검색 경험에서 성공적인 노출을 위한 핵심 전략을 제시함
     * 기존의 검색 최적화 원칙이 여전히 유효하며, AI 환경에 맞게 이를 진화시켜야 함

  사람 중심, 가치 있는 콘텐츠 제작

     * Google은 ""Google이 원하는 콘텐츠""가 아닌 사용자의 요구를 충족하는 콘텐츠를 원함
     * 독창적이며 비상품화된 콘텐츠가 검색 사용자와 기존 독자 모두에게 만족감을 줄 수 있음
     * AI 검색은 긴 질문과 후속 질문이 많은 만큼, 깊이 있는 콘텐츠가 더 유리함
     * 관련 링크: 도움이 되는, 신뢰할 수 있는, 사람 중심의 콘텐츠 제작

  훌륭한 페이지 경험 제공

     * 콘텐츠가 아무리 뛰어나도 UI/UX가 불편하면 사용자 만족도는 낮아짐
     * 모바일 호환성, 로딩 속도, 주요 콘텐츠와 주변 정보의 구분 가능성 등이 중요
     * 관련 링크: 좋은 페이지 경험 제공하기

  콘텐츠 접근 가능성 확보

     * Google이 콘텐츠를 크롤링·인덱싱 가능하도록 기술 요건 충족 필요
          + Googlebot 차단 금지
          + HTTP 200 상태코드 제공
          + 인덱싱 가능한 콘텐츠 포함
     * 관련 링크: Google Search 기술 요건

  미리보기 제어로 노출 범위 설정

     * nosnippet, data-nosnippet, max-snippet, noindex 등을 활용해 AI 검색에서의 콘텐츠 노출 범위 제어 가능
     * 제한이 많을수록 AI 결과에서 노출 가능성은 줄어듦
     * 관련 링크: 검색 결과 미리보기 설정

  구조화 데이터는 시각적 콘텐츠와 일치해야 함

     * 구조화 데이터는 AI와 검색 시스템이 콘텐츠를 기계적으로 해석할 수 있도록 도와줌
     * 마크업 내용은 반드시 실제 페이지에 시각적으로 표시되어야 하며, 유효성 검사도 필수
     * 관련 링크:
          + 구조화 데이터 가이드
          + Search Gallery

  멀티모달 검색 대비 콘텐츠 구성

     * Google의 AI는 이미지, 동영상 기반 질문에도 대응하는 멀티모달 검색 기능을 지원함
     * 이를 위해 고품질 이미지·영상 콘텐츠와 Merchant Center, Business Profile 정보를 최신 상태로 유지해야 함
     * 관련 링크:
          + 이미지 검색 최적화
          + 영상 검색 최적화

  방문자의 전체 가치에 주목하기

     * AI 검색에서 유입된 사용자는 클릭 수보다 체류 시간과 전환율이 높음
     * 정보 탐색, 회원가입, 구매 등 전환 지표 전반을 평가하여 콘텐츠 성과를 판단해야 함

  사용자와 함께 진화하기

     * 검색은 끊임없이 변화하며, AI Overviews와 AI Mode도 그 변화의 연장선
     * AI 검색은 다양한 출처의 링크를 폭넓게 제공하며, 사용자들이 새로운 방식으로 검색하도록 유도함
     * 관련 문서:
          + AI 기능과 내 사이트
          + 생성형 AI 콘텐츠에 대한 가이드

   참고하면 좋을거 같지만, 말하는 거라 실제랑 달랐던 전적이 있었어서 - 유출된 Google Search API 문서 분석
"
"https://news.hada.io/topic?id=21252","LibriVox","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                LibriVox

     * LibriVox 커뮤니티는 무료 공개 도메인 오디오북과 함께 다양한 팟캐스트 에피소드를 제공함
     * 최근 몇 개월간 진행된 팟캐스트는 커뮤니티 내 주요 프로젝트 소식, 공개 도메인 작품 소개, 그리고 아카이브 탐구를 포함함
     * 팟캐스트에서는 커뮤니티 회원들의 기여와 특별 테마(예: 봄맞이 청소, 프로젝트 현황 등)에 대한 이야기를 다루고 있음
     * 각 에피소드에서는 통계 및 커뮤니티 기념일, 포럼 활동, 과거 프로젝트 회고 같은 소식을 전달함
     * LibriVox는 공개 도메인 오디오북에 관심 있는 이들에게 커뮤니티 활동의 흐름을 알 수 있는 주요 정보를 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LibriVox 커뮤니티 팟캐스트 최근 에피소드 요약

  #157 – Clean-Up Month Kick Off (2025년 5월 2일 공개)

     * Jpercival이 호스트를 맡았으며, ShrimpPhish, redrun, sparkleberry17, Rapunzelina, adrianstephens가 참여함
     * 5월의 LibriVox 프로젝트 진행 현황, 새롭게 카탈로그에 추가되는 작품들, 통계, 커뮤니티 기념일 등 주요 소식을 다룸
     * ""Archive Diving"" 코너에서 과거 2007년 3월, 2009년 프로젝트를 돌아봄

  #156 – Spring Cleaning (2025년 4월 11일 공개)

     * jpercival 호스트, ShrimpPhish, TriciaG가 함께함
     * 봄맞이 청소(프로젝트 정리) , Harry Graham의 시 공개, 포럼 속도 저하 현상, 통계 및 기념일, 과거 기록 탐구 등 다룸

  #155 – What’s in a Name? (2025년 3월 8일 공개)

     * jpercival이 진행하며 Rapunzelina, mujinai, ShrimpPhish, Algy Pug, Phineas2000이 참여함
     * 셰익스피어 모놀로그 낭독, 커뮤니티 멤버 추모, 고전 작품 소개 등 진행
     * 팟캐스트에서 제목과 이름에 얽힌 이야기, 회원별 기여 하이라이트, 과거 프로젝트 정리까지 다양한 화제 다룸

  #154 – I Can’t Wait To Record This (2025년 2월 7일 공개)

     * jpercival, ShrimpPhish, LightCrystal, Piotrek, KevinS 참여
     * 단편 시 낭독, 프로젝트 현황, 월간 통계, 2월 기념일 등 커뮤니티 활동 소식 전달

LibriVox의 의의

     * LibriVox는 전 세계 누구나 자유롭게 이용할 수 있는 공개 도메인 오디오북 플랫폼임
     * 커뮤니티가 중심이 되어, 다양한 자원봉사자들의 참여로 성장해옴
     * 팟캐스트를 통해 오디오북 프로젝트와 커뮤니티 운영, 성장 과정 등을 투명하게 공유하고 회원들의 활발한 소통을 장려함

        Hacker News 의견

     * Lit2Go라는 사이트도 있는데, 좀 더 소규모이지만 전문적으로 선별된 오디오북 녹음 자료를 제공함 소개 내용에서는 내 아이들과 함께 Lorraine Montgomery가 읽은 “Curly and Floppy Twistytail” 시리즈를 정말 즐겼다는 경험담이 있음 아주 유쾌하고 익살스러운 이야기인데, 해당 링크에서 확인 가능함 이 사이트에는 아이들뿐 아니라 성인 대상의 Dracula, David Copperfield 등 수준 높은 작품들도 제공함
          + Lit2Go(FCIT)을 Hacker News에서 보게 될 줄은 상상도 못했음 나 예전에 Lit2Go 사운드부스에서 일한 적이 있는데, 정말 좋아하는 직업 중 하나였음 거기 오디오 작업에 훌륭한 목소리 재능을 가진 분들이 많이 거쳐 갔고, 녹음과 믹싱에 대해 많은 것을 배웠음 이 프로젝트를 이끈 분들은 정말 성실함의 화신 같은 분들이었음 그들이 운영하는 오픈소스 이미지 사이트에 역사적인 입체사진(stereoview)들이 많이 흩어져 있는데, 3D 안경으로 보는 게 꽤 즐거웠음 이곳에서 샘플 검색 가능함(검색 결과보다 실제로 더 많은 자료가 있었음, 아직 남아있길 바람)
          + Elisabeth Klett의 낭독 실력이 정말 놀라움 지금까지 돈 주고 산 어느 전문 오디오북보다 훨씬 뛰어난 퀄리티임 작품 모음 보기
     * 이 프로젝트와 기여한 모든 사람의 존재 자체에 대해 감사함 언젠가는 내가 구사하는 점점 사라져가는 모국어(화자가 천 명도 안 됨)로 책을 녹음해 보고 싶다는 생각임 언어 보존의 의미가 큼
          + 그 언어가 무엇인지 궁금함
     * 내가 정말 좋아하는 소설 중 H. Beam Piper의 _Little Fuzzy_가 현재 퍼블릭 도메인임(그가 유언 없이 세상을 떠났고, 펜실베니아 주가 저작권 갱신을 소홀히 하면서 그의 유족에게 소유권이 넘겨지기 전 자동으로 퍼블릭 도메인이 됨) 이 작품을 Tabithat이 낭독한 전문가 못지않은 녹음이 있는데, 강력히 추천함 리브리복스는 녹음 품질이 작품마다 너무 천차만별이라 장거리 여행에서 가족들은 질 낮은 녹음을 듣는 걸 싫어함 하지만 나는 일 할 때 지루한 작업을 할 때 폰으로 앱을 켜고 잘 활용하고 있으며, 이 프로젝트에 매우 고마움을 느낌
          + 실제로 대학 시절 교수 추천으로 리브리복스를 사용해서 이해하기 어려운 시를 아이팟에 담아 들었는데, 거의 모든 녹음에서 튀는 파열음(Plosive)에 귀를 참기 힘들었던 기억임
          + 리브리복스는 FOSS(오픈소스)로 ""넷플릭스 앤 칠""을 하려는 이북 버전 느낌임 최고의 기술 프로젝트가 일반인에게는 실용적이지 않은 점이 아쉽다는 생각임
     * “This is a libravox recording. All libravox recordings are in the public domain…” 예전 리브리복스 Robert Lynd 프로젝트에 기여한 적 있는데, 오디오북 시작 인트로 멘트를 지금도 기억함 Lynd는 정말 훌륭한 에세이 작가라서 추천하고 싶음
          + 내가 알기로 그 책들은 항상 퍼블릭 도메인임 그런데 그 녹음도 공공 영역인지, 아니면 GPL 같은 자유 라이선스가 적용되는지 궁금함
     * Sherlock Holmes 시리즈에서 David Clarke가 여러 작품을 아주 멋지게 낭독함
     * AI가 이 프로젝트에 도움이 될지, 아니면 해가 될지 궁금함 한편으로는 훨씬 더 많은, 잠재적으로 고품질 오디오북이 저장소에 늘어날 수도 있겠지만, 또 한편으로는 프로젝트의 취지와는 어긋날 수 있다는 생각임
          + 이 프로젝트에서 쌓인 음성 데이터가 10년 넘게 자동 음성 인식과 TTS(텍스트 음성 합성) 시스템 구축에 활용됨(예시로 LibriSpeech, LibriTTS, LJSpeech) AI 업계에 확실한 이점 제공
          + 미래에는 AI 오디오북이 가능해질 수도 있을 듯하지만, 현 시점에서는 아직 TTS(음성 합성)로 만들어진 오디오북을 고퀄리티라 부르긴 어려움 기사나 기술 에세이는 그럭저럭 듣지만, 뛰어난 오디오북 내레이터가 캐릭터 음성이나 감정, 문맥 파악 등에서 보여주는 디테일이 현존 TTS 모델엔 큰 허들임 NotebookLLM 같은 툴이 처음엔 매우 좋아 보여도 한 시간 정도 듣다 보면 인위적인 느낌이 거슬리기 시작함 결국 인간 수준에 근접한 머신이 등장할 미래를 기대하긴 하지만, AI 오디오북은 아직 갈 길이 멀다는 생각임
          + 리브리복스에 있는 모든 것이 AI 훈련 데이터로 이미 사용됐다고 생각하면 됨 결국 이게 득인지 실인지는 개개인이 판단할 몫임
     * Anarchy by Errico Malatesta 이것이 진정한 정답임
     * 예전에 북클럽에서 읽던 책의 오디오북을 못 찾은 적이 있었는데, 그 책이 퍼블릭 도메인으로 자체적으로 상업 오디오북은 없었음 리브리복스에서 찾았고, 책 자체는 길고 지루했지만, 적어도 내레이터의 음성은 만족스러웠음
     * 검색 기능에서 과학소설(Science Fiction) 장르와 영어 오디오북 등 복수의 필터를 적용해서 찾고 싶음 원하는 키워드와 조합도 되면 좋겠다는 생각임
     * 참고로 이제 인터넷아카이브를 통한 기부도 가능함
"
"https://news.hada.io/topic?id=21223","Analysis I의 Lean 컴패니언","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Analysis I의 Lean 컴패니언

     * 테렌스 타오가 집필한 실해석학 교재 Analysis I의 핵심 내용을 Lean 증명 보조기로 옮긴 프로젝트임
     * 이 프로젝트는 기본 수 체계의 구성과 증명 논리 등, 엄밀성을 강조하는 원서의 목표와 잘 맞는 구조를 가짐
     * Mathlib 표준 라이브러리를 활용하되, 주요 개념을 직접 구축하고 독자들이 직접 증명하는 연습이 포함됨
     * Lean 코드상 빈칸(sorry) 을 직접 채우며 연습할 수 있어, 공식 해설은 제공하지 않고 포크(fork)로 확장 가능함
     * Lean 입문자 및 실해석학 학습자 모두에게 Mathlib과 Lean의 실제 활용법을 체험할 기회로 유용함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 테렌스 타오가 쓴 실해석학 교재 ""Analysis I""를 Lean 증명 보조기 도구로 재구성한 오픈소스 프로젝트임
     * 이 교재는 기존 다른 해석학 책들보다 자연수, 정수, 유리수, 실수의 구성 과정, 그 과정에서 필요한 집합론 및 논리적 도구에 더 초점을 맞춤
     * 책의 많은 부분이 체계적인 엄밀 증명 능력 개발에 초점을 두고 있어, Lean 같은 증명 보조기와 궁합이 잘 맞는 구조임

Lean 컴패니언 프로젝트의 특징

     * 원서의 정의, 정리, 연습문제를 Lean 형식으로 ""번역""해 제공함
     * 이 Lean 파일들에는 연습문제 풀이에 해당하는 빈칸(sorry) 이 많이 포함되어 있으며, 이를 독자가 직접 채우는 방식으로 학습 가능함
     * 공식적인 해설(풀이)은 제공하지 않지만, 독자가 레포지토리를 포크해 자신의 해답 버전을 만들 수 있음

Mathlib와의 연계 및 차별성

     * Mathlib(Lean의 표준 수학 라이브러리)에 이미 구현된 개념(예: 자연수)은 일부러 별도로 직접 구축해보고, 이후 Mathlib 버전과의 동형성(등가성)을 증명해 보는 과정도 구성함
     * 예시로, Chapter2.Nat에서는 Mathlib의 자연수와 구분되는 자체 정의 natural numbers를 처음부터 구축하고, 주요 결과를 직접 다룬 후, 마지막에 두 버전이 동등함을 Lean에서 연습하도록 함
     * 이후 장부터는 Mathlib의 정의와 기능을 점점 더 많이 활용하는 흐름임

학습 및 활용 방법

     * 이 Lean 컴패니언은 해석학 학습뿐 아니라, Lean 및 Mathlib에서의 수학 공식화 방법에 대한 입문서 역할도 수행함
     * ""Natural number game""처럼, Lean 환경에서 수학적 객체를 직접 정의·연습해 볼 수 있는 구조적 연습이 포함됨
     * 코드 자체는 Lean에서 컴파일이 가능하지만, 모든 연습문제(sorry)가 실제로 풀 수 있는지 완전 검증은 되지 않았으며, 이런 점에서 플레이테스트 및 피드백이 필요함

오픈소스 및 기여

     * 레포지토리는 오픈소스로 누구나 참고·포크·기여할 수 있음
     * 공식 솔루션은 따로 제공하지 않으며, 복수의 풀이 버전 생성을 지원함
     * 5월 31일 기준, 프로젝트는 별도 독립 레포지토리로 이전됨

        Hacker News 의견

     * 나는 정말 기대가 되는 상황, 만약 이 프로젝트가 독립적인 레포로 옮겨지면 다른 사람들에게 쉽게 공유와 찾기가 더 쉬워질 것이라는 기대감, 나는 수학에 항상 궁금증이 있었고 Tao의 Analysis는 내 프로그래밍 사고방식에 맞게 엄격하게 수학이 어떻게 구성되는지 처음으로 보여준 교과서였다는 경험, 이후 Lean에도 빠져들었는데 Mathlib로 수학 개념을 배우기엔 조금 복잡함을 느낌, 그래서 책에서 툴로 이어지는 다리 역할을 해주는 이 프로젝트가 정말 마음에 든다는 이야기
          + 나도 비슷한 경험, 수렴, 코시 수열 등을 배웠고, 이 책이 인도 Hindustan Book Agency라는 비영리 출판사에서 출간되어 매우 저렴하게 구할 수 있었다는 추억 공유
     * 내가 Lean을 활용한 수학 교육에서 가장 신나는 부분은 즉각적인 피드백 제공이라는 점, 학생이 증명을 잘못하면 컴파일이 되지 않는다는 특징, 이전에는 TA나 교수 등 사람이 직접 피드백을 줘야 했지만, 이제는 Lean 컴파일러가 빠르게 알려줌, 앞으로 Rust 컴파일러처럼 더 친절하게 수정 제안까지 제공하는 기능이 생기길 바람 (이건 전용 LLM 도입이 필요할 수 있음)
          + 나도 거의 전체적으로 동의하지만, 증명을 고민하는 과정도 중요하다는 의견, 내 수학 경험은 오래전이지만 과제나 문제를 종이에 써가며 천천히 생각하는 '수학 낙원' 같은 시간이 많았음, Lean을 쓰면 오히려 무작위로 입력을 넣거나 자동으로 맞출 때까지 시도하는 '손놀림 위주'로 흐를 수 있다 걱정, 예전에 coq을 잠깐 써보기도 했는데 거의 계속 건드려 보기만 했었던 기억, 결론적으로 theorem solver가 많은 면에서 유용하지만 이런 천천히 곱씹는 과정이 사라지고 내재화, 개념화, 새로운 아이디어가 나오는 경로가 약해질 수도 있겠다는 고민, 이에 대한 생각이 궁금함
     * Terence Tao가 직접 Lean을 사용하는 영상을 볼 수 있는 개인 유튜브 채널이 있다는 이야기, 나는 이 분야 전문가는 아니지만 LLM을 함께 혹은 없이 작업하는 모습을 보는 것만으로도 정말 흥미로웠다는 관점 (https://www.youtube.com/@TerenceTao27)
     * 전통적인 ""교과서식"" 접근 방식을 Mathlib 방식과 비교 평가하는 것이 정말 흥미로울 것이라는 생각, 공식화된 수학 라이브러리는 일반적으로 최대한 일반적인 방식으로 결과를 표현하고, 증명 구조 자체를 리팩토링해서 우아하게 만들기가 쉽다는 장점, 리팩토링은 시스템이 논리적 종속성을 항상 추적해주기 때문에 손쉽게 할 수 있지만, 종이와 펜만으로 할 땐 쉽지 않음, 그래서 Mathlib처럼 '최대 일반성' 버전의 해석학을 대학 강의에서 가르치는 게 괜찮을지 자연스러운 고민이라고 봄, 이건 다른 모든 증명 기반 수학 분야도 마찬가지 고민이라는 의견
          + 적어도 입문 과정에서는 적합하지 않다는 입장, 이미 커리큘럼에 들어갈 것이 너무 많음 — 증명법, 프로그래밍법, 그리고 기초 이론까지, 실제로 시도해본 교수들의 경험도 비슷하며, 고급 학생에게는 좋지만 일반 학생에게는 시간 낭비라는 평가가 많다고 생각함
          + 나는 오랫동안 프로그래밍도 해온 수학자로서, 어떤 프로그램적 공식화도 근본적인 이해를 길러주지는 못할 거라고 생각, 내 편견은 내가 논문을 통해 개념을 배웠기 때문임, 코드가 종종 스타일을 신경 쓰지 않아 가독성 면에서 압도적인데, 비가독성 논문도 힘든데 코드의 경우 표준도 없어 10배 더 힘들다는 개인 경험
     * 정리 증명(theorem proving)이 해석학 같은 메인스트림 수학 분야에서 점점 주목 받는 게 반갑다는 소감, PLT에서는 이미 Winskel의 The Formal Semantics of Programming Languages라는 대표적인 책이 Isabelle로 공식 검증된 사례가 있었음 (http://concrete-semantics.org), 정리 증명을 시작해보고 싶다면 그쪽이 더 쉽고 추천할 만하다고 생각, 해석학에서의 정리는 원래 난이도가 상당히 높다는 점도 추가 설명
          + PL 증명이 더 진입 장벽이 낮을 것이라는 데에 나도 놀라지 않을 듯, 루틴화된 느낌이 많다는 이야기도 자주 들음 — 구조 유도(sturctural induction), 귀납법 적용, 불변식 증명, 반복, 이런 흐름, 나는 정리 증명을 많이 하진 않았지만 수학적(특히 해석학) 증명을 theorem prover로 해본 적은 없음, ""수학적"" 증명은 많이 다른 접근 방식을 요구하는지, 그리고 그 사이에 기술 이전(skill transfer)이 얼마만큼 될지 궁금, 참고로 Software Foundations in Rocq(Lean 포팅이 있을지도?)를 공부해봤는데 상당히 즐거웠다는 경험
     * 이 프로젝트와 접근 방식이 해석학 같은 기초적인 주제에 매우 잘 어울린다고 생각, 하지만 두 가지 걱정이 있음
         1. Mathlib의 해석학 핵심 결과는 필터(filter) 개념으로 통합적으로 다루는데, 특별한 경우에는 epsilon-delta형으로 별도 다룸, Tao의 Analysis는 좀 더 전통적 epsilon-delta 접근을 쓸 거라 예상
         2. Mathlib는 빠르게 변화하는 프로젝트라서 이름·구조가 항상 바뀜, 연동 정보는 지속적으로 관리해줘야 하는 문제
          + 각자 직접 확인해볼 수 있다는 안내와 함께, 실제로 수열의 극한에 대한 챕터 대부분이 샘플 페이지로 공개되어 있으니 참고하라고 관련 링크 공유 https://link.springer.com/chapter/10.1007/978-981-19-7261-4_6
     * 매우 멋진 프로젝트라는 생각, Analysis I은 엔지니어인 내가 진짜로 처음으로 끝까지 다 따라가며 공부할 수 있었던 ""진짜"" 수학 교과서였고, 다른 책들(Rudin 등)에 여러 번 도전했다 실패한 경험도 있음, Lean 버전이 있으면 수학과 프로그래밍 둘 다 친숙한 사람들이 좀 더 엄격하게 개념을 배울 수 있게 될 것 같아 기대감
     * 수년간 Tao의 Analysis I 공식 Lean formalization 시도가 계속 있었지만 항상 몇 챕터 이상 진행되기 어려웠다는 사실, 한동안 나도 이 시도를 직접 하고 싶었음 — Analysis I 해답 블로그(https://taoanalysis.wordpress.com/)에 연계된 공식화 증명을 같이 올리면 책으로 공부하는 사람들이 유용하게 활용할 수 있을 거라 생각, 이미 비공개 Discord에 정리된 내용을 공유했지만 여기에 여러 사람들이 쓸만한 참고 Lean 프로젝트(github 및 gist, 공식 문서 등)들을 링크로 한 번에 공유함
          + https://github.com/cruhland/lean4-analysis
          + https://github.com/cruhland/lean4-axiomatic
          + https://github.com/Shaunticlair/tao-analysis-lean-practice
          + https://github.com/vltanh/lean4-analysis-tao
          + https://github.com/gabriel128/analysis_in_lean
          + https://github.com/mk12/analysis-i
          + https://github.com/melembroucarlitos/Tao_Analysis-LEAN
          + https://github.com/leanprover-community/NNG4/ (이건 Tao의 책은 아니고 자연수 게임 Lean4 버전, Chapter 2와 거의 동일한 컨텐츠)
          + https://github.com/djvelleman/STG4/ (마찬가지로 set theory 게임, Chapter 3과 비슷, 그런데 Game/Metadata.lean에 보면 ""import Mathlib.Data.Set.Basic"" 해서 완전히 새롭게 집합론을 정의하지 않고 불러오는 방식 — 이 경우 Lean이 집합론을 이미 다 ""알고"" 있는 상태라서 우리 목적엔 완벽하지 않을 수도 있음)
          + https://gist.github.com/kbuzzard/35bf66993e99cbcd8c9edc4914c9e7fc (정수 만들기 예시)
          + https://github.com/ImperialCollegeLondon/IUM/… (위랑 같은 파일일 수 있음)
          + https://github.com/ImperialCollegeLondon/IUM/… (유리수 만들기 예시)
          + https://lean-lang.org/theorem_proving_in_lean4/… (커스텀 Set 타입 정의하는 참고 예시)
     * ""Mathlib에 해당하는 객체와의 동형(isomorphism) 증명""이 실제로 얼마나 중요할지 궁금, 혹시 동형 부분을 빼도 실질적으로 아무것도 변하지 않을 수도 있지 않을까, 예를 들어 정리를 자동 번역한다든가 그런 일에 실제로 쓰이나?
          + 이런 동형성(isomorphism) 증명은
              1. 자신이 만든 객체와 Mathlib에 이미 있는 객체가 실제로 동일함을 증명하게 해줌, 특히 Mathlib 쪽에서는 복잡하게 일반화된 구성으로 정의돼 있을 수 있어서 차이를 파악하는 데 도움이 됨
              2. Mathlib에서 그 객체를 읽거나 쓸 때 사용하는 공식적인 표기나 용어를 이해하는 데 결정적 역할
          + 최소한 교육적 가치가 있다고 생각, 내가 구성한 집합과 연산이 책 안에서 다른 곳에서도 '동일'하다는 것을 본인 스스로 납득하는 과정이 된다는 의미
     * Lean 기반 교과서 등장이 반가움, 그런데 왜 HoTT(Homotopy Type Theory)는 없을까? ""Type Theory(HoTT)가 (ZFC) 집합론을 대체해야 하는가""라는 화두 기사도 존재 (https://news.ycombinator.com/item?id=43196452), 이번 주 HN에 올라온 Lean 관련 추가 커뮤니티 리소스도 함께 공유 — ""100 theorems in Lean"" (https://news.ycombinator.com/item?id=44075061), 그리고 DeepMind Lean 프로젝트 (https://news.ycombinator.com/item?id=44119725)
          + 그런데 굳이 HoTT까지 있어야 할 이유는 모르겠음, HoTT theorem prover는 아직 사용자 편의성이 낮고, 문서도 충분하지 않음, HoTT가 주는 이득도 명확하지 않다고 느끼는데, 보통 범주론 같은 극단적으로 특이한 구조 다룰 때만 유의미하다는 의견
          + 기존 교과서 방식이라서 ""왜 HoTT가 아니냐""는 질문에는 이미 답변이 포함되어 있음, 게다가 교육적 효과에 대해 회의적인 전문가도 많다고 생각
"
"https://news.hada.io/topic?id=21302","테슬라, 사고 데이터의 공개 차단을 요청함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        테슬라, 사고 데이터의 공개 차단을 요청함

     * Tesla가 자사의 사고 데이터가 대중에 공개되는 것을 막기 위해 법적 조치를 취함
     * 이 데이터는 일부 교통사고 사건에서 중요한 증거로 활용됨
     * Tesla는 해당 데이터의 오해 소지 및 오용 가능성을 우려함
     * 규제기관과 법원이 데이터의 공개 필요성과 기업의 보호 요청 사이에서 균형을 논의함
     * 이 이슈는 자율주행차 및 교통 사고 발생 시 데이터 공개의 기준을 재검토하는 계기가 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

테슬라, 사고 데이터 공개 차단 시도

     * Tesla는 최근 여러 교통사고와 관련된 수사 및 소송에서 자사의 주행 데이터가 공개되는 것을 막기 위해 법적 절차를 추진함
     * 일부 법원과 규제기관이 사고 당시의 차량 기록 데이터를 공개하도록 요구하는 상황에서, Tesla는 이러한 정보가 부적절하게 해석될 수 있다고 주장함
     * Tesla의 입장에 따르면, 복잡한 주행 데이터가 일반인 및 언론의 오해를 불러일으킬 수 있으며, 불필요한 기업 이미지 훼손이나 법적 책임 확대 가능성이 존재함
     * 규제기관과 법원은 공공의 안전 보장과 기업의 정보 보호 요청 사이에서 조율 필요성을 언급함
     * 이 논의는 현재 자율주행 기능, 미래 교통사고 조사, 데이터 투명성에 있어 중요한 정책적 함의를 제공함

        Hacker News 의견

     * 테슬라가 정보를 공개하면 경쟁에 불이익이 생긴다고 주장하는 반면, 과거 Musk가 글로벌 EV 확산을 위해 누구나 특허를 자유롭게 쓰도록 하겠다고 선언한 사례를 떠올림. 2014년 6월, https://www.tesla.com/blog/all-our-patent-are-belong-you"">테슬라 블로그에서 “지속가능한 운송 시대로 가속하기 위해 테슬라가 혁신적 전기차 개발을 막는 특허 장벽을 두는 건 목표와 모순”이라며 특허벽을 없앤다는 오픈소스 선언을 한 적 있음. 그런데 이제는 사람 목숨을 살릴 수도 있는 데이터조차 공개하지 않겠다는 입장 전환
          + 이 기사는 Musk가 직접 이런 결정을 내렸다는 뉘앙스를 주지만 실제로는 테슬라 법무팀에서 나온 요청으로 보임. 물론 Musk가 CEO여서 원하면 막을 수도 있지만, 이런 조치는 대부분의 기업에서도 흔함. Musk의 특허 오픈 정책이 독특했던 건 사업적으로 위험한 선택이었기 때문
          + 내 인상에서는 테슬라는 원래 부를 과시하기 위한 상징물로 여겨졌음. 사실 그게 내가 Tesla를 싫어했던 가장 큰 이유. 대중을 위한 차라기엔 너무 희소하고 엘리트적
     * 이제 Tesla를 굳이 사려는 이유를 모르겠다는 심정. 에어백이 터진 뒤에야 충돌로 인정하는 규제가 허술한 현실이 안타까움
          + 나도 이해가 되지 않음. 영국에서 최근 Tesla 산 사람들은 정치적 이유거나 체제에 반감을 표현하려는 이상한 목적이 대다수. 농담이었으면 좋겠다는 사실이 더 슬픔
          + 그래도 Tesla 자체는 훌륭한 차이고 다른 전기차보다 늘 상위권에 있음. Full Self Driving이나 Autopilot은 신뢰하기 어렵고 비추천이지만, EV 자체로는 매우 뛰어남
          + 최근 Tesla 모델 Y를 산 입장에서는, 주행거리, 성능, 트렁크 공간, 소프트웨어 등 모든 면에서 동급 최고의 가격대 성능. EV를 원한다면 이 기준에서는 Model Y가 명확한 선택
          + 모델 Y와 3는 가성비가 상당히 괜찮은 편이고, 중국산 모델과 경쟁할 만한 가격대. 실제로 판매의 90%를 차지함
          + 모두에게 해당하는 건 아니지만 미국 시장에선 비슷한 기능을 갖춘 대안이 거의 없다는 점
     * 내세울 만한 이유 없이 교통안전청이 정보를 갖고 있다면, 해당 자료에 세금을 낸 국민 모두가 접근권을 가져야 마땅하다는 생각
          + 단, 특정 제조사만 지목하지 말고 모든 제조사 충돌 데이터를 같이 공개하는 방식이 공정할 거라 생각
          + 차주나 유가족도 접근할 수 있어야 옳다고 봄. 제조사와 NHTSA만 쉽게 접근할 수 있는 현실은 실망스러움
     * 우리 집 Tesla는 쓰레기통은 90% 잘 인식하는데 학교 건너는 아이들은 60% 정도밖에 못 봄(독일 거주, 도보 통학 학생 많음). 실제로 매일 등교길 지날 때, Tesla라면 한 주에 10~20명 정도는 사고날 듯. 이 상황에서 사고 데이터를 숨기려 드는 건 당연해 보임
          + 센터 디스플레이 시각화 정보만 보고 하는 가정인가? 차가 실제로 인식하고 반응하는 건 그게 전부가 아니고, 특히 독일 FSD 차량은 표시된 것보다 더 많음
          + 반대로 Tesla가 쓰레기통으로 잘못 인식하는 물체가 아주 많다는 점도 웃김
          + 아이를 치일 확률을 그렇게 과장하면 말이 안 된다고 생각. 만약 정말로 Tesla가 그 정도로 위험하다면 서구권 국가에서는 금방 대규모 스캔들이 터질 거라 봄
     * 아내의 친척이 최근 Tesla와 큰 교통사고를 당해 도랑에 떨어져 차량이 몇 번 구르기도 함. 초기 보고에선 Tesla가 자율주행 모드였음. 과실이 누구에게 있는지 궁금한데, 지금까지는 Tesla 측에 불리하게 보임
          + 진짜 심각한 사고처럼 들림. 차량이 옆에서 부딪혔나, 뒤에서인가 궁금하고, 어디서 났는지 묻고 싶음. “Tesla 자율주행, 도랑 추락”으로 검색해도 뉴스에 안 나오는 걸 보니 예상 외
          + 책임 소재는 Tesla 운전자가 사고를 막을 수 있었는지 여부가 중심이 될 듯. 다만 Tesla 시스템 자체에 근본적인 설계 결함이 있고, 이걸 회사가 무시해온 문제가 논의될 수 있음
          + Tesla가 충돌 직전에 FSD를 자동으로 끄는 사례가 있다는 소문도 들음. 그래서 공식적으로 ‘충돌 당시에는 FSD가 꺼져 있었다’고 주장하는 사례가 있을 수 있음
     * 최근 UN 자동차 규제 [1]는 충돌 전후 5초(-5~+5s) 동안 차량 텔레메트리(물리적 동작)를 기록하도록 요구함. 그러나 ADAS/ADS 관련 (운전자 지원 시스템) 정보는 수집 대상이 아님. UN 규제와 미국의 기존 규정 모두, 가속 페달 위치 등 단일 값만 기록하면 됨. Tesla 등은 독자적으로 더 많은 텔레메트리를 수집하고 있고, 예를 들어 ‘운전자와 ADS/ADAS가 각각 가속 신호를 냈는지’ 분리 기록할 수도 있지만, UN/미국 규정은 굳이 그 이상을 요구하지 않음. 각국 도입에도 시차가 있어 호주는 이런 요구조차 없고, 미국도 충돌 전후 20초(-20~+5s)까지만 제한적 저장을 강제함[2]. [1] UN Regulation No. 160 - Event Data Recorder (EDR) [2] CFR Title 49 Subtitle B Chapter V Part 563 [3] ADAS/ADS와 EDR 차이 설명 PDF
     * 미국에선 Tesla Y/3가 가격 경쟁성과 사양 때문에 많이 팔리지만 실제로 보면 Tesla를 산 사람 대부분이 정치적 포지션이나 상징성을 이유로 산다는 인상
     * Tesla의 Full Self Driving 기능 HW4/v12 조합 경험을 위해 24년식 Model 3을 구매함. HW3/v11과 HW4/v12 차이는 정말 극명하게 체감됨. 헬리콥터 조종(=살려고 긴장하며 조작)과 비행기 조종(=안전) 정도로 비교할 수 있을 정도. HW4/v12를 써보고 처음으로 FSD가 ‘월 99달러짜리 안전 기능’으로 느껴짐
          + 안전 기능이 구독제라니 말이 됨? 다음엔 안전벨트 맬 때마다 마이크로트랜잭션을 받지 않을까 걱정됨
          + 한 달에 99달러라니. 실체도 부족한 소프트웨어에 저걸 내야 한다면 차가 아니라 기생충 느낌
          + 문제의 본질은 이거임. 대부분의 시간엔 훌륭하게 작동하지만, 한 번만 판단 실수해도 사람을 해칠 위험이 큼. 사람은 자신에게 즉각 위험한 것엔 극도로 집중하지만, 무언가를 ‘신뢰’하게 되면 반초 만에 달라지는 위험에는 맹목적이 되기 쉬움. Tesla가 ‘Full Self Driving’이라는 명칭으로 마치 진짜로 스스로 운전해 주는 느낌을 주지만, 실제론 항상 경계해야 한다는 점을 명확히 전달하지 않는 점이 문제. 사고가 나면 항상 운전자 책임으로 돌리는 태도 역시 걱정스러움
          + 나는 절대 FSD 가까이 가지 않을 것. 행운을 빔
     * The Washington Post에서 이 건을 제기한 게 흥미로움. WaPo(워싱턴포스트) 오너가 Zoox(자율주행 스타트업) 오너인 Jeff Bezos라는 점이 배경
     * 미국 교통안전위원회 주요 Tesla 사고 리스트, HWY18FH011, HWY18FH004, HWY16FH018, 배터리화재
     * Tesla 관련 이슈들을 접하며 대부분의 ""FSD가 사고냈다, Elon이 숨긴다""류 주장은 알고 보면 운전자가 무모한 운전을 한 뒤에 책임을 FSD에 떠넘기는 경우가 많다는 사실을 알게 됨. 물론 FSD 결함이 있긴 한데, 실제로는 정지 신호를 잘못 인식하거나, 사용자에게 엉뚱한 위험을 일으키는 식의 지루한 케이스가 더 많음
          + 실제로 이 기사에서 말하는 본질은 Tesla가 모든 사람이 데이터를 볼 수 없게 하려 한다는 점. 만약 그렇다면 Tesla는 공개해야 한다고 생각
          + 그러면 데이터를 공개하면 실제로 운전자 책임이라는 게 증명되지 않겠냐는 시각
          + 데이터 공개를 꺼리는 건 경쟁업체에 불리해서라는 변명에 불과하다고 봄. 그러니 Tesla를 옹호하는 건 논점에서 벗어난 일
     * 미국 시장에서 Tesla처럼 유사한 성능을 가진 EV를 쉽게 찾기 어렵다는 현실이 있음
"
"https://news.hada.io/topic?id=21174","Microsoft가 Windows Update를 모든 서드파티 앱에 개방하기 시작함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Microsoft가 Windows Update를 모든 서드파티 앱에 개방하기 시작함

     * 마이크로소프트가 제3자 앱까지 Windows Update에서 업데이트 가능하도록 개방하는 새 플랫폼을 프리뷰로 공개함
     * 새로운 Windows Update 오케스트레이션 플랫폼은 드라이버 및 비즈니스 앱 포함 모든 업데이트를 통합 관리할 수 있도록 설계됨
     * 사용자 활동, 배터리 상태, 친환경 에너지 타이밍 등에 따라 업데이트 일정을 최적화 가능함
     * Win32, MSIX, APPX 앱까지 지원되며, Windows Update 기록에도 앱 업데이트 이력이 포함됨
     * 기존 Microsoft Store나 Winget 방식의 한계를 넘어, 비즈니스용 커스텀 앱도 포함 가능성이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Windows Update, 모든 앱 업데이트의 허브가 되려는 움직임

     * 마이크로소프트는 최근 Windows Update를 OS 및 드라이버 업데이트 외에도 모든 앱의 통합 업데이트 플랫폼으로 확장하겠다는 계획을 발표함
     * 이 변화는 특히 기업 환경에서 내부 앱까지 업데이트를 통합 관리하려는 수요를 반영한 것으로 보임

새로운 오케스트레이션 플랫폼 개요

     * Windows Update Orchestration Platform이라는 이름으로 현재 프라이빗 프리뷰 제공 중
     * 기존 Windows Update의 기능을 확장해, 앱 업데이트도 함께 일정 조정 및 사용자 경험 최적화 대상에 포함

     “우리는 앱, 드라이버 등 어떤 업데이트도 Windows Update와 함께 오케스트레이션할 수 있는 통합 지능형 플랫폼을 구축하고 있습니다.” — Microsoft 제품 관리자 Angie Chen

기존 앱 업데이트 방식의 문제

     * 대부분의 윈도우 앱은 각 개발사마다 개별 업데이트 시스템을 운영
     * 결과적으로 업데이트 타이밍이나 품질이 일관되지 않음
     * MS Store를 통해 일부 앱은 통합 업데이트 가능하지만, 많은 앱은 Store에 등록되지 않거나 기업용 자체 앱임

주요 기능 및 장점

     * 사용자 활동, 배터리 상태, 지속 가능한 에너지 시점 기반 스케줄링 기능
     * Windows Update의 기본 알림 및 이력 UI에 통합
     * MSIX / APPX 앱은 물론 일부 Win32 앱까지 지원
     * 플랫폼의 향후 업데이트를 자동으로 상속받음
     * 기존 installer 대체 가능성 제시 (예: Adobe처럼 자체 백그라운드 설치기를 운영하는 대형 앱들도 대상이 될 수 있음)

기존 해결책들과 비교

                  방식                             설명                         주요 단점
   Microsoft Store                  앱 설치 및 업데이트를 Store에서 관리      등록된 앱이 제한적, 기업용 앱 적용 어려움
   Windows Package Manager (winget) 커맨드라인 기반 패키지 설치/업데이트 도구      주로 파워유저 및 개발자 중심, 일반 사용자 비주류
   Windows Update 오케스트레이션           OS/드라이버 외에 일반 앱까지 업데이트 통합 가능 현재는 프라이빗 프리뷰 단계

향후 전망

     * 우선 기업용 앱 업데이트 통합 수요가 클 것으로 예상
     * 이후 Adobe, Zoom, 기타 상용 소프트웨어 등으로 확대 가능성 존재
     * 장기적으로는 macOS처럼 시스템 전반의 업데이트 일원화를 추구하는 방향성

   마이크로소프트는 분산된 앱 업데이트 경험을 통합하려는 시도를 다시금 강화하고 있으며, 개발자와 기업의 협업 참여 여부가 이 생태계 전환의 핵심이 될 것으로 보임.

        Hacker News 의견

     * Windows에서 여전히 Chrome이 권한 상승 문제를 우회하기 위해 특수 서비스를 사용해 업데이트를 처리하는 상황이 남아 있고, Spotify 등 많은 앱들이 같은 이유로 AppData에 설치되는 현상 유지 중인 상황 공유, 많은 프로그램의 제거 프로그램이 제대로 작동하지 않아서 파일이나 기타 흔적을 남기는 경우도 여전함, MSI는 “체인 서명”이라는 오래된 키로 새로운 키를 서명하도록 영원히 요구하는데, 10년 넘는 긴 기간 동안 업데이트를 관리해야 할 때 굉장히 어렵게 느껴지는 문제, 언젠가 이 모든 상황이 깔끔히 정리되었으면 하는 바람
          + Chrome이 사용하는 설치/업데이트 프로그램은 오픈소스 Omaha이고, 다른 언급된 앱들은 Squirrel을 사용하고 있음, 둘 다 AppData에 위치하는 것이 가능(특히 Squirrel은 오로지 사용자 디렉터리에만 설치됨), Squirrel의 철학이 관리자 권한 없이도 사용자 설치 가능하게 하는 것임
          + AppData에 설치하는 이유는 권한 상승 우회를 목적으로 숨기려는 게 아님, Microsoft가 거의 10년 이상 AppData에 설치하는 방식을 권장한 결과이고, 요즘은 권한 상승 없이 동작 가능한 프로그램이라면 AppData 설치가 ‘올바른’ 방식이라고 생각함
          + 비컨테이너화되고 루트/관리자 접근 권한이 있는 앱의 경우 설치 프로그램 입장에서는 잔여 파일을 완전히 처리하는 것이 거의 불가능한 문제라고 생각함, 이들 앱은 아무 디렉터리에나 파일을 만들고 쓸 수 있으며, Microsoft 혹은 앱 제공자가 제공하는 제거 프로그램조차 모든 파일을 찾아내지 못하고, 프로그램의 전체 동작 흐름을 그대로 재현하지 않는 한 완전한 삭제가 어렵다고 봄
          + GNU/Linux 환경의 패키지들도 잔여 파일을 남기는 현상이 많음
     * UniGetUI를 발견하게 되어 WinGet, Scoop 등 다양한 패키지 매니저를 잘 호출해주며, 무시 목록 등 커스터마이즈 기능도 제공, Windows에서는 그 정도 수준의 커스터마이즈 기능을 기대하기 어렵다고 생각함
     * 항상 Windows에 왜 macOS처럼 통합된 설치, 업데이트, 삭제 프레임워크가 처음부터 존재하지 않았는지 의문을 가짐, 분명히 해결되지 않은 명백한 누락이라고 생각, 현재도 기업 고객은 애플리케이션을 관리하기 위해 개별적으로 직접 패키징해야만 하는 상황, 초기부터 Microsoft가 DLL 공유를 장려했고, 하위 호환성을 제공해야 했기에 .MSI나 고도화된 소프트웨어 관리 프레임워크 도입을 강제하지 않았던 게 원인이라 추측
          + MacOS도 처음부터 그런 통합 프레임워크를 제공하지 않았음, 많은 앱은 드래그 앤 드롭 방식으로 Applications 폴더에 넣는 간편함이 있지만, 상당수 앱은 설치 프로그램을 실행해야 하고, 시스템 전체에 지원 파일을 설치하기 위해 관리자 인증을 요청하는 경우 많음, 앱마다 자체 업데이트 프로그램이 시작 시 자동 실행되기도 하고, 과거에는 확장 기능이나 제어판 요소가 System Folder에 설치되어 재부팅이 필요했던 기억, 그리고 이러한 앱 중 상당수는 자체 제거 기능도 없어서, 설정 파일과 캐시 파일 등도 사용자가 직접 찾아서 지워야 문제 없이 재설치 가능
          + Microsoft의 첫 유명 운영체제였던 MS-DOS 영향으로, 초기 Windows는 타사 소프트웨어 설치 관점에서 사실상 DOS처럼 동작, 별도 설치 개념 없이 공급업체가 제공한 INSTALL.COM/INSTALL.EXE를 실행하는 방식, 주로 루트 디렉터리에 새 폴더를 만들고 파일을 복사했으며, 어떤 경우에는 사용자가 직접 폴더 만들어 수동 복사, 모든 앱 데이터 작업이 특정 디렉터리(예: C:\Program Files)에 집중된 구조, UNIX처럼 /bin, /etc, /var로 분리하지 않았음, MS-DOS는 IO.SYS, MS-DOS.SYS, CONFIG.SYS, AUTOEXEC.BAT 외 각종 파일 위치에 전혀 신경 쓰지 않았던 구조, Windows 3.x가 대중적으로 보급됐을 때도 이러한 DOS 스타일 작업 방식이 그대로 이어졌고, ‘통합 설치 시스템’이 매우 늦게 도입됨, .MSI도 상당히 후기에 도입되어 기존 프로그램들이 적용하지 않은 역사적 배경 설명
          + macOS로 전향했을 때 전형적인 설치 경험이 Windows보다 훨씬 좋다는 점에 정말 놀람, 다운로드 파일을 그냥 폴더에 복사만 하면 설치 끝나는 방식의 간편함이 인상적, 별도 설치 프로그램이 필요해도 거의 항상 시스템에서 제공되는 익숙한 플로우라 부담없이 느껴짐
          + 드라이버, 시스템 확장, 라이브러리 버전 관리 등 복잡한 문제들이 통합 설치/삭제 시스템을 만드는 걸 어렵게 함, 인터넷 연결조차 보장하지 못한다면 더 까다로움, 이런 기능을 만든다고 해도 소프트웨어 벤더들이 이를 이용하도록 설득해야 하고, 경영진이 이를 새로운 이윤 수단으로 여기지는 않을지 우려
          + 주요 소프트웨어 벤더들은 GPO 배포용 msi 패키지를 일반적으로 제공하는 편, 지난 10년간 직접 패키징해야 했던 적이 거의 기억나지 않음, 설치 매개변수 튜닝 정도의 간단한 작업만 하는 경우가 대부분, 다만 개선 여지는 여전히 충분히 남아 있다고 느낌
     * Windows 10에서 모든 업데이트를 비활성화한 채 1년 넘게 아무 문제 없이 사용 중, Microsoft가 ‘업데이트’라는 단어 자체를 부정적으로 만든 것 같고, Nadella가 왜 이렇게까지 Windows에 애정을 안 보이는지 이해가 안 됨
          + 보안 걱정으로 업데이트를 안 하면 난리나는 사용자도 있겠지만, 대부분의 가정용 PC는 NAT 환경에 있기 때문에, 원격 취약점(예: EternalBlue) 악용도 어렵고, 트로이목마에 걸리지 않는 한 큰 문제 없음, 브라우저만 최신 상태라면 실질적으로 안전하다고 생각함, 예외적으로 트로이목마가 걸려도 관리자 권한 필요 없이 문서 암호화, 봇넷 참여가 가능하므로 Windows 업데이트만으로 모든 위협을 막는 건 아니라는 의견
     * Windows Update 방식이 모든 리눅스 패키지 매니저에서 사용하는 방법과 매우 유사하다고 생각, 다만 Chocolatey, Scoop, WinGet 등 다른 대안들과 비교하면 Windows Update는 지나치게 단순하며 기능이 부족하게 느껴짐
          + WinGet이 존재한다는 사실을 너무 늦게 알았던 점이 부끄럽게 느껴짐, Ubuntu 등 리눅스 환경에서 지내다가 Windows용 패키지 매니저를 검색해보고 뒤늦게 알게 됨
          + Windows Update는 속도가 굉장히 느리게 느껴짐, 업데이트 컴포넌트 수나 데이터 양이 10배로 늘어난다면 정말 상상도 하기 어려운 일이라고 생각
     * 개발자/고급 사용자가 아니라 Winget/커맨드라인으로 앱 업데이트가 어려운 일반 사용자라면 오픈소스 앱인 UniGetUI를 적극 추천, UI가 직관적이고 관리도 잘 되고 매우 쾌적하게 동작함
          + UniGetUI 프로젝트를 처음 알게 됐는데 정말 세련된 느낌, 좋은 정보 공유에 감사
     * 이 스레드 덕분에 UniGetUI라는 아주 멋진 툴을 알게 됨, 앞으로 내 모든 Windows 장비에 꼭 설치할 예정, 이 앱의 주요 목표는 WinGet, Scoop, Chocolatey, Pip, Npm, .NET Tool, PowerShell Gallery 등 다양한 Windows 패키지 매니저용 직관적 GUI 제공, 지원하는 패키지 매니저에서 원하는 소프트웨어를 간편하게 설치/업데이트/삭제할 수 있는 매력적인 앱, 링크 참고(16.2k stars 기록)
     * 이 변화는 7zip 업데이트에도 20분이 걸리고 재부팅까지 요구되는 사태를 만들 것 같다는 의구심
          + 꼭 그런 일이 벌어지진 않을 것이라고 생각, Windows Update가 강제로 재부팅을 요구하지 않는 업데이트도 많고, 7zip 역시 이와 다르지 않은 방식으로 설정 가능하다고 봄
     * 다른 작성자들과 마찬가지로 이번 변화가 이미 한참 뒤처졌다고 느끼는데, 이유는 단순히 누가 먼저 했느냐가 아니라 내 개인적으로는 Win32 API와 데스크톱 앱의 시대가 최소 10년 전에 끝났다고 보기 때문, 이제 데스크톱에 설치된 앱은 소수에 불과하고, 대다수 사용자는 모바일 앱과 웹브라우저에 더 의존, 개인적으로 설치하는 것도 대부분 유틸리티며, 이는 Microsoft의 비즈니스 모델에도 맞지 않음, 결국 목표 사용자가 도대체 누구인지 의문
     * 이 정책은 Windows Update 서비스에 문제가 생길 경우 엄청난 단일 실패 지점을 만들 것이라는 우려, 관련 검색 트렌드로도 알 수 있듯 Windows Update는 오랜 기간 불안정 기록이 있음
          + 단일 업데이트 수단이 된다면 그런 우려가 현실이지만, 실제로 Windows Update가 유일한 경로로 작동하게 만들 계획은 아니라서 ‘싱글포인트’ 우려가 크지 않다고 생각
"
"https://news.hada.io/topic?id=21284","qnm - node_modules 폴더를 들여다 보는 CLI 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  qnm - node_modules 폴더를 들여다 보는 CLI 도구

     * node_modules 폴더에서 원하는 모듈의 버전과 설치 경로를 즉시 검색할 수 있는 CLI 도구
     * npm list, yarn list보다 빠르고 불필요한 출력 없이 검색·탐색 속도와 직관적 인터페이스에 집중하여 필요한 정보만 제공
     * fuzzy-search, 패턴 매칭, 설치 이유 추적, 중복 모듈 탐지 등 기능을 제공
     * npm/yarn 프로젝트 모두 지원하며, 모듈의 여러 버전 존재 여부, 의존관계 구조, 최신 버전 정보 등을 한눈에 파악할 수 있음
     * bunx qnm, npx qnm 명령으로 설치 없이 바로 실행할 수 있어 환경 오염 없이 가볍게 활용 가능
          + 예시: bunx qnm lodash 입력 시 프로젝트 내 lodash의 버전별 설치 위치, 최신 버전, 릴리스 정보 등 확인
          + 인자가 없으면 fzf 스타일 fuzzy 검색으로 모든 패키지 탐색 가능
          + 필요시 전역 설치: npm i --global qnm
     * 주요 명령 및 옵션
          + doctor: node_modules 내 중복 및 용량 가장 큰 패키지 진단
          + list(ls): 전체 패키지 목록 출력, --deps로 직접 의존성만 볼 수 있음
          + match [문자열]: 특정 패턴에 맞는 모든 모듈 리스트업
          + homepage: 패키지의 homepage URL을 브라우저로 바로 열기
          + --no-remote: 원격 데이터 비활성화, 로컬 정보만 빠르게 표시
          + -o, --open: 해당 패키지의 package.json 파일을 에디터로 바로 열기
"
"https://news.hada.io/topic?id=21190","C# 파일을 dotnet run app.cs로 직접 실행하는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  C# 파일을 dotnet run app.cs로 직접 실행하는 방법

     * .NET 10 Preview 4부터는, 이제 단일 C# 파일을 dotnet run app.cs로 바로 실행할 수 있는 기능이 추가되어, 프로젝트 파일 없이도 C# 코드 실행이 가능해짐
     * 파일 기반 앱(file-based apps) 덕분에, Python이나 JavaScript처럼 간단한 스크립트 실행, 테스트, 아이디어 실험이 한층 쉬워짐
     * NuGet 패키지 참조, SDK 지정, 빌드 속성 설정 등도 파일 내 디렉티브로 관리할 수 있어 개발 유연성이 향상
     * shebang 지원으로 유닉스 계열에서 CLI 유틸리티, 자동화 스크립트 등에도 활용 가능
     * 필요시 프로젝트 기반 앱으로 매끄럽게 변환 가능해, 학습 및 프로토타입에서 본격적인 앱 개발까지 자연스럽게 연계됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

dotnet run app.cs란 무엇인가

     * 기존에는 dotnet CLI로 C# 코드를 실행하려면 반드시 프로젝트(.csproj) 구조가 필요했음
     * 이제는 단일 .cs 파일만으로 바로 실행 가능하여, 진입 장벽이 크게 낮아짐
     * 스크립트 언어나 자동화, 실험, 학습 등 다양한 용도에 적합함
     * CLI 통합으로 추가 도구 설치 없이 dotnet만 있으면 바로 사용 가능함
     * 코드가 커지면 동일한 언어와 도구로 프로젝트 기반 앱으로 확장 가능함

파일 수준 디렉티브 지원

     * 파일 기반 앱에서도 프로젝트의 주요 설정들을 직접 .cs 파일 내에 디렉티브로 선언할 수 있음
     * NuGet 패키지 참조
          + #:package 디렉티브로 NuGet 패키지를 바로 참조할 수 있음
               o 예시:
#:package Humanizer@2.14.1

using Humanizer;

var dotNet9Released = DateTimeOffset.Parse(""2024-12-03"");
var since = DateTimeOffset.Now - dotNet9Released;

Console.WriteLine($""It has been {since.Humanize()} since .NET 9 was released."");

     * SDK 지정
          + #:sdk 디렉티브로 SDK 종류 지정 가능
               o 예시:
#:sdk Microsoft.NET.Sdk.Web

               o ASP.NET Core 기능(최소 API, MVC 등)도 활성화 가능
     * MSBuild 속성 설정
          + #:property로 빌드 속성 직접 지정 가능
               o 예시:
#:property LangVersion preview

     * 쉘 스크립트용 shebang 지원
          + 파일 맨 위에 #!/usr/bin/dotnet run을 넣어, 유닉스 계열에서 실행 파일로 바로 사용할 수 있음
               o 예시:
#!/usr/bin/dotnet run
Console.WriteLine(""Hello from a C# script!"");

               o 실행 권한 부여 후 바로 실행:
chmod +x app.cs
./app.cs

프로젝트 기반 앱으로의 변환

     * 앱이 커지거나 더 많은 기능이 필요할 때, dotnet project convert app.cs 명령어로 프로젝트로 손쉽게 변환 가능
     * 디렉티브는 .csproj 파일 속성, 참조 등으로 자동 변환됨
     * 변환 예시
          + 아래와 같은 파일:
#:sdk Microsoft.NET.Sdk.Web
#:package Microsoft.AspNetCore.OpenApi@10.*-*

var builder = WebApplication.CreateBuilder();
builder.Services.AddOpenApi();
var app = builder.Build();
app.MapGet(""/"", () => ""Hello, world!"");
app.Run();


     *
          + 변환 결과:
<Project Sdk=""Microsoft.NET.Sdk.Web"">
  <PropertyGroup>
    <TargetFramework>net10.0</TargetFramework>
    <ImplicitUsings>enable</ImplicitUsings>
    <Nullable>enable</Nullable>
  </PropertyGroup>
  <ItemGroup>
    <PackageReference Include=""Microsoft.AspNetCore.OpenApi"" Version=""10.*-*"" />
  </ItemGroup>
</Project>

기존 C# 스크립트 방식과의 차이

     * 그동안 커뮤니티 툴(CS-Script, dotnet-script, Cake 등)로 C# 스크립트 실행이 가능했지만, 별도 도구 설치와 설정 필요
     * 이제는 별도 설치나 모드 없이 동일한 C# 컴파일러와 언어를 사용해, 진입 장벽 없이 바로 코드 실행 가능

시작 방법

     * .NET 10 Preview 4 설치
     * Visual Studio Code 사용 시, C# Dev Kit와 최신 프리릴리즈 버전의 C# 확장(2.79.8 이상) 설치 필요
     * .cs 파일 생성 후 바로 코드 작성
     * 터미널에서 dotnet run hello.cs 실행
     * 필요시 dotnet project convert hello.cs로 프로젝트로 변환

더 알아보기

     * Microsoft Build 세션 데모: No projects, just C# with dotnet run app.cs

향후 계획

     * VS Code 내 파일 기반 앱 지원 및 디렉티브용 IntelliSense 향상, 성능 개선, 디버깅 지원 강화 예정
     * 다중 파일 지원과 실행 속도 개선 등 추가 기능도 개발 중
     * dotnet run app.cs는 C#을 더욱 쉽게 접근할 수 있게 하면서, .NET의 강력함을 그대로 제공함
     * 프로토타이핑, 교육, 프로덕션 개발까지 더 빠르게 전환할 수 있는 기반 제공

   File-based App 기반의 자동 완성 경험을 제공하는 DX는 최신 버전의 C# 익스텐션에서 제공되고 있지만, 원래 VS Code Marketplace 이외의 위치에 Microsoft가 익스텐션을 게시하지 않고 있었습니다.

   이런 불편함을 해소하기 위해서, C# Dev Kit이 아닌 C# Extension 파트 (MIT 라이선스 파트)만 따로 autobuild/auto-publish 하도록 만들어 OpenVSX에 등록했고, 이를 기반으로 한 Kiro 기반의 간단한 데모 영상을 공유합니다.

   https://www.youtube.com/watch?v=pIi7CWOPQSA

   전에 C# Interactive 기능을 사용했을 때 로컬에 설치되지 않은 패키지는 사용할 수가 없었는데 이제 개선이 되었나 보네요.

        Hacker News 의견

     * 이 기능이 .NET 개발자 생산성에 큰 영향을 줄 수 있을 것 같아 아쉬움도 생기는 부분, 왜 이제서야 나왔는지 궁금함, 그리고 .NET 프로젝트에서 정말 바라는 기능이 하나 있는데, 프로젝트별로 쉽게 커스텀 커맨드를 정의할 수 있는 기능임, 예를 들어 ""npm run <command>"" 같은 방식이 있으면 좋겠음
          + 나는 https://github.com/dotnet-script/dotnet-script를 쓰면서 별다른 문제 없이 잘 사용 중, 그래도 번거로운 단계를 건너뛸 수 있으면 더 좋겠다는 생각
          + 혹시 https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-run 같은 걸 말하는 건지 궁금함
          + 만약 그런 기능이 있다면 정말 좋을 것 같음, 나는 지금은 이걸 make 파일로 구현해서 쓰고 있음 https://www.gnu.org/software/make/
          + 사실 make나 task 같은 별도 태스크 러너를 쓰는 게 더 나을 수도 있다는 생각
          + 예전에 이런 기능이 필요할 때 LINQPad를 사용한 적도 있음
     * 이걸 shebang과 함께 적극적으로 활용하라고 홍보하는 게 흥미로움, 이런 접근이 꽤 매력적으로 느껴짐, Go가 모듈 도입 전에도 이런 식으로 스크립팅 쓰임새가 좋았고, Ubuntu도 이런 식으로 사용한 걸로 기억함, 하지만 Go 저자들은 Go를 이런 스크립팅 언어로 쓰는 방식에는 부정적 입장이었음
          + Go 저자들이 반대한 게 아니라, Go를 우선적으로 프로그래밍 언어로 사용하길 권장했다는 설명임, gorun (https://github.com/erning/gorun) 같은 툴로 예전부터 쉽게 Go를 스크립트처럼 쓸 수 있었음, 최근에는 이걸 한 번에 실행할 수 있도록 go run github.com/kardianos/json/cmd/jsondiff@v1.0.1처럼 태그를 바로 받아 실행하게 지원함, 이거 꽤 멋진 기능임
          + ‘shebang’이란 말을 어디서부터, 언제부터 썼는지 궁금함, 대학 다닐 때랑 90~2000년대 초 남부 지역에서는 보통 hashbang이라고 불렀음, shebang은 C#이 유행하면서 처음 들었고, 실제로는 더 이전부터 있었던 용어임, 다만 주변에선 못 들어봤던 단어임
          + 예전에 .NET 회사에서 일할 때, 누구는 갑자기 bash로 오토메이션 스크립트를 짜기도 했었음, 이런 스크립트를 장기적으로 관리할 전문성이 없었고, 처음 쓸 때부터 퀄리티도 별로였음, 왜 그냥 도구 자체를 C#으로 만들지 않았는지 이해가 안 갔었음, 이번 기능 덕분에 C# 접근이 훨씬 실제적 대안으로 느껴질 수도 있을 것 같음
          + Rust의 cargo로도 이런 게 가능함, 다만 아직 정식 지원은 아님 https://rust-lang.github.io/rfcs/3424-cargo-script.html
     * 기능 자체는 훌륭하지만, 컴파일된 상태라 하더라도 스타트업 오버헤드가 약 0.5초 정도 걸림, 그래서 많은 애플리케이션에서 적합하지 않다는 단점이 있음, 그래도 bash에 의존하는 쉘 스크립팅의 한계도 있고, perl 시대는 이미 지났고, Ruby가 여전히 이런 용도엔 최고라서 계속 써 왔었음, 최근엔 Swift로 몇몇 스크립트를 옮겼는데 기본적으로 인터프리터 방식이라서 훨씬 빠르고, 컴파일된 실행 파일은 바로 실행되기 때문에 매우 인상적임, Swift CLI 앱을 위한 캐싱 컴파일러도 직접 만들어 봤음(https://github.com/jrz/tools), 참고로 dotnet run은 이미 컴파일 결과를 캐시해주기 때문에 별도의 캐싱 레이어가 필요 없음(비활성화하려면 --no-build, 바이너리 경로 확인하려면 --artifacts-path 사용)
          + 0.5초라는 수치가 어디 정보인지 궁금함, 나는 hello world로 테스트했더니 63ms 나왔음, neuecc의 CLI 라이브러리 벤치마크(https://neuecc.medium.com/consoleappframework-v5-zero-overhead-native-…)를 보면 그 어떤 것도 0.5초에 도달하지 않음, 참고로 Swift가 기본적으론 인터프리터 방식이라는 점을 언급했는데, .NET JIT는 티어드 JIT라서 바로 코드가 만들어지지 않고 여러 단계로 진행되는 구조임
          + dotnet도 10 혹은 11 버전에서 완전한 인터프리티드 모드가 도입될 예정이라고 들음, 이런 용도에 해당 모드가 적용될지도 궁금함 https://github.com/dotnet/runtime/issues/112748
          + 컴파일되어도 스타트업에 약간 랙이 있으면 왜 python은 이런 영역에서 큰 인기를 얻었는지 궁금함
          + 이 기능이 아직 초반 프리뷰 단계임, 여러 발표에서 스타트업 속도 이슈는 인지하고 있고 개선 중임을 밝혔었음
          + 빠른 스타트업을 원한다면 https://learn.microsoft.com/en-us/dotnet/core/deploying/ 안내대로 쉽게 네이티브 코드로 변환할 수 있음
     * CSX/VBX 프로젝트에 대한 언급이 부족해서 아쉬움 https://ttu.github.io/dotnet-script/, C# 런타임에서 F# 스크립트의 의존성 처리와도 호환되지 않는 방식으로 결정한 것 같아 신기함 https://learn.microsoft.com/en-us/dotnet/…
          + CSX/VBX 등의 노력이 반영되지 않았다는 얘기에 대해, 실제로 공식적으로 여러 방식과 툴이 언급되어 있다는 점을 안내함 https://devblogs.microsoft.com/dotnet/announcing-dotnet-run-app/…
          + F#과 비호환적이라는 부분이 무슨 의미인지 질문, 문법 차이 얘기라면, 문법을 의도적으로 다르게 만든 목적이 있었고, C# 스크립트 다이얼렉트 신설을 원치 않았기에 파일 import 같은 기능을 일부러 막음, 이건 C# 특성 때문임
     * Kotlin에서도 비슷한 기능이 존재, https://github.com/Kotlin/kotlin-script-examples/… (여기선 파일 확장자가 반드시 ""*.main.kts""이어야 동작함), 이런 방식은 작은 스크립트나 프로토타이핑에 아주 좋고, JVM 기능을 활용하는 데도 실용적임, 그래도 작은 스크립트엔 Ruby가 여전히 가장 편함, 특히 외부 프로그램 실행할 때 backtick 문법이 정말 편리함
          + Kotlin 스크립트가 완전히 사라지진 않겠지만, 미래가 그렇게 밝지는 않음 https://blog.jetbrains.com/kotlin/2024/…
          + Java 자체에도 유사한 스크립트 방식이 생긴 걸로 알고 있음
     * shebang을 이용해서 C# 스크립트를 bash 스크립트처럼 실행할 수 있음 https://devblogs.microsoft.com/dotnet/announcing-dotnet-run-app/…
          + .net10 preview 4 sdk 이미지에서 파일을 직접 실행하려고 shebang을 테스트했으나 처음엔 잘 안 됐음, dotnet run <file>로는 동작, 업데이트 후엔 정상 동작, 문제 원인은 파일이 LF가 아니라 CRLF 줄바꿈 사용함 때문이었음
          + 이제 타입 세이프티가 적용된 스크립트를 작성할 수 있게 된 점이 매우 반가움, 참고로 macOS에서는 shebang에 #!/usr/local/share/dotnet/dotnet run 혹은 #!/usr/bin/env -S dotnet run을 써야 함
     * PowerShell을 대체할 만한 도구로 보임, PowerShell은 거의 ChatGPT 전용 언어처럼 쓰이는 경향도 있음, 대부분의 회사에서 PowerShell로 작성된 스크립트가 인프라 핵심 역할을 하지만, 사실상 ‘읽기 전용’ 상태에 빠지는 경우가 많음
          + PowerShell뿐 아니라 훨씬 더 넓은 영역까지 대체할 수 있는 잠재력이 느껴짐, .NET 팀이라면 굳이 Python이나 shell 스크립트에 손 댈 필요 없이, 상단에 shebang 추가해서 C# 스니펫 붙여넣기만 해도 거의 모든 스크립트 처리가 가능해질 수 있을 듯, 테스트 서비스도 굳이 express.js로 짤 필요 없이, ASP.NET minimal API로 쓱 만들면 끝
          + 윈도우 시스템 관리자들이 아마 가장 대규모 ChatGPT 스크립트를 쓰는 집단 아닐까 싶음, 나도 예전 관리자였다면, MS 공식 문서 수준 감안해서라면 반드시 써봤을 것 같음
          + C# 코드를 PowerShell에서 호출하는 것도 가능함 https://learn.microsoft.com/en-us/powershell/…
          + PowerShell로 모든 인프라를 스크립트로 넣는 건 실제로 쉽지 않고, 그렇게 하면 대혼돈이 발생할 뿐임, 실제로 함수 몇 개 이상 넘어가면 C#이 훨씬 효율적이고 진입장벽도 거의 없음, PowerShell은 소규모 ad-hoc 스크립트엔 최적이고, 예전 VBScript 같은 ‘윈도우 기본 스크립트 언어’ 자리를 차지하고 있음
          + PowerShell이 .NET 코드를 직접 돌릴 수 있기 때문에, 오히려 PowerShell 경험이 확장되는 면도 보임
     * NetPad 기능을 사실상 대체하는 것 같고, 디버깅만 추가되면 LINQPad도 현역에서 밀려날 것 같은 예감, 나도 예전엔 LINQPad 덕을 참 많이 봤지만, 아직도 텍스트 에디터 경험은 지금 시대엔 너무 불편함, 본격적인 코드 작성이나 편집용으로 쓰기엔 한계가 큼
          + 내 경우 LINQPad의 핵심 사용처는 DB 상호작용이나 .dump()로 값을 탐색하는 용도임, 이번 dotnet run은 오히려 그걸 보완해주는 도구로 쓸 수 있을 듯, 예전에 PowerShell을 극도로 싫어하는 곳에서 LINQPad로 거의 모든 스크립팅을 처리했었는데, 그런 현장에서는 쓸 만했음
          + LINQPad는 .NET에선 유니크한 제품이지만, 텍스트 에디터 자체는 역대 최악에 가까울 때가 많음, neovim이나 monaco 같은 에디터로 교체되면 좋겠음, 테이블 시각화나 snappiness 같은 건 참 좋은데, 요즘 Jupyter Notebook 등 ‘노트북’ 기술에 비하면 활용의 폭은 좁아짐, 단독 개발자라 그 한계도 있는 듯함, 그래도 매일 실무에서 SQL 데이터 만질 때 LINQPad는 여전히 최고임
          + LINQPad가 바로 대체되진 않을 듯, 절반의 경쟁력은 UI에 있다고 생각, VSCode나 Visual Studio에서 dotnet run 사용 경험이나 LINQPad랑 얼마나 비슷할지 궁금함, LINQPad는 결과값을 시각화하는 기능이 강점임, dotnet run이 텍스트만 뿌리거나 별도 플러그인이 많이 필요하면 LINQPad 수요는 계속될 듯, 문법 확인 등만 필요하다면 dotnet run이 더 나은 선택일 수 있음, 나도 가끔 헷갈리는 문법은 LINQPad로 실험해봄
          + GUI 기능, 확장포인트까지 모두 구현하지 않는 한 LINQPad 바로 대체는 어려울 듯
     * 이 기능 진심 기대됨, 내가 CI/CD 파이프라인에서 쓰는 PowerShell 스크립트 일부를 대체할 수 있을 것 같음, PowerShell과 Bash 모두 좋아하지만, 머릿속에서 C 계열 문법 언어로 푸는 게 훨씬 효율적인 작업이 분명히 있음, 이번 기능이 그런 공백을 메워줄 수 있을 듯함
     * 실제 제안문(https://github.com/dotnet/sdk/…)에 여러 정보가 더 들어가 있음, 특히 여러 파일 처리라든가, 구현 세부사항(암시적 프로젝트 파일 등)에 대해 자세히 설명함

   이 기능과 관련된 실제 예제 2개를 만들어본것도 있어 답글로 공유합니다. MCP 서버와 Avalonia를 이용한 Windows, macOS GUI 앱 샘플 코드입니다. 😊

   https://forum.dotnetdev.kr/t/…
"
"https://news.hada.io/topic?id=21221","Ask HN: 소비자용 하드웨어에서 사용할 수 있는 최고의 LLM은 뭔가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Ask HN: 소비자용 하드웨어에서 사용할 수 있는 최고의 LLM은 뭔가요?

     * 5060ti + 16GB VRAM 에서 기본 대화가 가능한 모델을 찾음. 가능하면 빠르고 거의 실시간으로 동작하면 좋겠음

답변 정리

     * 다양한 8B~14B, 30B 파라미터 모델이 16GB VRAM에서 효율적으로 동작하며, 대표적으로 Qwen3, DeepSeek-R1, Mistral, Gemma3 등이 추천됨
     * 로컬 LLM 실행은 성능, 비용, 프라이버시 면에서 장점이 있지만, 실제 성능과 모델 적합성은 개별 실험과 튜닝이 필수임
     * 모델 파일의 크기, 퀀타이즈(양자화) 수준(Q4~Q6 등), GPU·RAM 분산 로딩 등 하드웨어 활용 최적화 팁이 활발히 공유됨
     * Ollama, LM Studio, llama.cpp, OpenWebUI 등 다양한 도구가 존재하며, 각각 접근성·유연성·모델 관리 편의성에서 장단점이 있음
     * 커뮤니티 정보(예: Reddit LocalLLaMA)는 최신 소식·실전 팁 제공에 유용하지만, 과장·오정보도 많으니 주의 필요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 LLM 추천 및 활용 팁

     * Qwen3: 8B/14B/30B 등 다양한 파라미터 모델이 존재하며, 8B~14B 모델은 16GB VRAM에서 쾌적하게 사용 가능함. reasoning(추론) 성능이 뛰어나고, MoE(Expert Mixture) 구조로 일부 모델은 RAM 오프로딩으로 큰 사이즈도 운용 가능함
     * DeepSeek-R1-0528-Qwen3-8B: 최신 8B 모델 중 reasoning 성능이 뛰어나다는 평가를 받음. 8B 기준 4GB~8GB VRAM에 Q4~Q6 양자화 시 적합함
     * Mistral Small 3.1: 14B 또는 24B 모델이 추천되며, 대화 품질이 우수하고 비교적 censorship이 적은 편임. 특히 이미지 입력 기능이 있음
     * Gemma3: Google 제공 모델로, 직관적 대화에 강점. 다만 HR성향이 강해 disclaimer가 많다는 평이 있음. hallucination도 상대적으로 잦음
     * Devstral: Mistral 기반의 대형 모델. 30B 이상은 16GB VRAM에서는 속도가 느려질 수 있음
     * Dolphin, Abliterated: censorship이 적은 버전으로, routine이 아닌 상황에 유용함

하드웨어 및 실행 환경 최적화

     * 퀀타이즈(양자화) 설정: Q4, Q5, Q6 등 양자화 수치가 낮을수록 VRAM 사용량이 줄어듦(Q4 ≒ 파라미터/2, Q6 ≒ 파라미터*0.75). 다만 품질 저하에 유의 필요
     * VRAM 용량 산정: 예시 - 8B Q4는 4GB, 14B Q4는 7GB, 30B Q4는 약 15GB VRAM 필요
     * RAM 오프로딩: VRAM 부족시 일부 레이어를 CPU 메모리로 offload 가능. 다만 속도 저하 감수 필요
     * KV 캐시 양자화: context window를 늘릴 때 q4 정도로 캐시 압축 사용 추천

도구 및 프론트엔드

     * llama.cpp: 다양한 플랫폼에서 빠르고 유연하게 동작. REST API 및 간단한 React 프론트엔드 지원. 모델을 VRAM과 RAM에 분산해 로딩 가능
     * Ollama: 쉬운 설치 및 모델 스위칭, GUI 프론트엔드와 연동 용이. 단, 최신 모델 지원 및 context 크기 한계가 있음
     * LM Studio: GUI 환경에서 모델 관리가 편리. VRAM 적합 여부 예측 기능
     * OpenWebUI: 프론트엔드 전용. llama.cpp, vllm 등 백엔드 필요. 여러 모델 동시에 관리 및 테스트 가능
     * KoboldCPP, SillyTavern: 롤플레잉/스토리텔링/게임 등 특화 프론트엔드

커뮤니티와 실전 정보

     * Reddit LocalLLaMA, HuggingFace, Discord: 최신 모델 소식, 사용법, 벤치마크, 세팅 노하우 등이 활발히 공유됨. 단, 오정보나 groupthink 현상에 주의 필요
     * 벤치마크 사이트: livebench.ai, aider.chat 등에서 최신 모델별 점수 및 랭킹 제공

활용 목적과 실제 경험

     * 프라이버시, 비용 절감: 민감 데이터/프라이버시 이슈 또는 반복적 사용 시 클라우드 대비 로컬 모델 활용도가 높음
     * 실험 및 튜닝 자유도: 특화 도메인 파인튜닝, 샘플링 전략, 프롬프트 엔지니어링 등에서 API 모델 대비 유연함
     * 응용 사례: RAG(검색 결합 생성), 로컬 데이터베이스 결합, 에이전트 자동화, 오프라인 도우미 등 다양한 실전 예시

자주 나오는 질문 및 팁

     * 모델 크기 산정: 파라미터 수 × 비트(quantization)/8 = 약 VRAM 요구량(GB). 오버헤드와 context window도 고려 필요
     * 모델별 특징: Qwen3 reasoning/코딩, Gemma3 직관/회화, Mistral censorship 적음, Dolphin/abliterated uncensor 버전 등
     * 성능 비교: 직접 벤치마크 및 커스텀 테스트로 자신에게 맞는 모델 탐색 권장

결론 및 실전 조언

     * ""최고의 모델""은 없으며, 하드웨어·용도·선호에 따라 Qwen3, Mistral, Gemma3 등 최신 8B~14B 모델을 다양하게 시도해 보는 것이 최선임
     * 모델 파일 크기, 양자화, context 크기 등 사양 맞춤이 매우 중요하므로 여러 모델을 직접 테스트하고 커뮤니티 팁을 활용하는 것이 효과적임

        Hacker News 의견

     * 로컬에서 LLM을 실행하고 싶다면 reddit의 localllama 커뮤니티에서 많은 도움을 받을 수 있음
       특별히 ""최고""라고 할 수 있는 LLM 모델은 없고, 각 모델마다 장단점이 있기 때문에 여러 가지를 직접 써봐야 함
       예를 들면 DeepSeek-R1-0528-Qwen3-8B 모델이 오늘 릴리즈되었고, 8B 사이즈에서 최고의 논리적 추론 성능을 보여줌
       그리고 Qwen3 시리즈도 최근에 나왔는데, 하이브리드 방식과 좋은 성능, 그리고 다양한 하드웨어에 맞는 여러 사이즈를 제공함
       Qwen3-30B-A3B는 CPU에서도 괜찮은 속도로 구동 가능함
       심지어 0.6B짜리 미니 모델도 꽤 일관성 있어서 놀라운 경험
          + llama-cpp를 사용할 때 일부 텐서를 CPU로 오프로딩하면 좋은 성능을 유지할 수 있는 사례를 본 적 있음
            일반적으로 llama-cpp에서는 GPU에 올리는 레이어 수(-ngl)를 지정하지만, 연산이 무거운 텐서가 아닌 경우 CPU 오프로딩으로 GPU 공간을 아끼면서 속도 저하 없이 돌릴 수 있음
            ""hot"" 뉴런만 CPU에서 불러오는 논문(arxiv 링크)도 읽어봤고, 앞으로 집에서도 AI를 멋지게 활용할 수 있을 것으로 기대
          + 레딧 사용이 익숙하지 않은 사람에게 한 가지 주의점이 있음
            LocalLlama를 포함한 레딧에는 잘못된 정보나 인기가 많은 허위정보도 많고, 업보트/다운보트 비율이 정보의 정확도를 보장하지 않음
            정확하지만 지루하게 설명된 댓글은 오히려 비인기일 수 있고, 재미있거나 감정적인, 혹은 단체 의견에 부합하는 잘못된 설명이 인기일 때가 많음
            나처럼 웹에서 오래 논 사람은 대충 가려서 보지만, 집단사고가 강한 이런 공간에 처음 오는 사람이라면 조심해서 정보를 받아들이는 것을 추천
          + 요즘은 어느 모델이든 기본은 되다 보니, 결국 취향에 맞는 ""모델 성격""을 찾아가는 느낌이 강해짐
            OP는 그냥 차례로 받아보고 사용해보면 됨
            16GB 메모리면 llama.cpp로 DDR5를 부분 오프로딩해서 30B 모델까지(심지어 dense 모델도) ""적당한"" 속도로 돌릴 수 있음, 텐서 오프로딩을 하면 더 좋음
            Qwen은 대화형 모델로서는 좀 아쉬운 점이 있음
            Mistral Nemo, Small, 그리고 Llama 3.X 시리즈도 오늘날 기준으로 여전히 훌륭한 선택
            Gemma 3s는 좋긴 한데 약간 예측불허 스타일
            집에서 GPT-4급이 필요하면 QwQ 추천
            그리고 내가 까먹은 괜찮은 모델이 더 있을 것임
          + 코딩 도구인 aider나 roo와 함께 사용할 만한 추천 모델이 있는지 궁금
            자체적으로 툴 사용을 잘하는 모델 찾기가 꽤 어려운 경험
          + DeepSeek-R1-0528-Qwen3-8B는 DeepSeek-R1-0528의 chain-of-thought를 Qwen3-8B Base에 distill해서 만든 모델로, AIME 2024에서 Qwen3-8B보다 10% 이상 성능이 높고 Qwen3-235B-thinking과 동급 성능을 보임
            distillation(지식 증류)이 얼마나 효과적인지 새삼 놀라는 지점
            요즘 여러 오픈AI나 연구실에서 chain-of-thought(COT)를 감추는 이유가 이 때문인 듯 (참고글)
     * 대부분의 사람들은 로컬 LLM을 어디에 가장 많이 쓰는지 궁금
       하드웨어가 엄청 좋지 않다면 Gemini나 Claude 같은 독점 모델에 미치긴 힘든데, 이런 소형 모델들도 물론 쓸모가 있을 것 같지만 구체적인 활용 사례가 무엇인지 궁금
          + 데이터를 제3자에게 넘기기 꺼려지는 마음
            프롬프트나 질문을 외부에 보내고 싶지 않은 사람도 많음
          + 나는 대부분의 프롬프트에 우선 로컬 모델을 써보고, 예상 외로 절반 이상에서는 충분히 좋은 결과를 받는 경험
            클라우드 서비스를 안 쓰게 될 때마다 뿌듯한 기분
          + 앞으로 로컬 LLM의 미래는 어떤 작업을 어떻게 처리할지 신속하게 판단해서 신속하게 위임(delegation)하는 형태가 될 것이라는 생각
            MCP와 같은 로컬 시스템으로 처리 가능한 작업, 혹은 캘린더나 이메일 등 시스템 API 호출이 필요한 작업, 아니면 최적의 클라우드 모델에 전달해야 할 작업인지 척척 골라주는 방식
            제대로 동작하는 Siri 같은 느낌을 상상
          + 나는 지금 Devstral을 기반으로 직접 만든 로컬 코딩 에이전트로 실험 중
            Codex보다 마음에 드는 점은 하드웨어 전체 접근이 가능해서 VM 띄우기, 네트워크 요청 등 Codex에서 못하는 작업을 할 수 있다는 점
            또한 세팅부터 패치 생성까지 Codex보다 훨씬 빠름
            물론 Codex만큼의 결과는 아직 아니지만, Devstral은 소규모 변경이나 리팩터링에 쓸만하고, 소프트웨어를 더 진화시키면 점점 대규모 변경도 가능할 것으로 기대
          + 나는 원칙적으로 클라우드를 가급적 쓰지 않음
            예를 들어 OpenAI는 최근 ChatGPT 대화 내용을 공유하는 일종의 소셜 네트워크 서비스까지 작업한다는 소식
            로컬에서 돌리면 AI의 내부 작동 원리도 더 잘 이해해서 내 시장 가치도 상승
            LLM 백엔드를 활용한 실험(웹검색, 에이전트 등)도 자유롭게 할 수 있고, 클라우드 비용 부담도 없으며, 처음 LLaMa 나올 때 이미 게임용 데스크탑이 있었음
     * Mozilla의 LocalScore라는 프로젝트도 눈여겨볼 만함
       다양한 모델이 여러 하드웨어에서 얼마나 잘 돌아가는지 비교 분석해주는 서비스
     * LocalLLama subreddit 추천 의견에 동의
       ""최고의 모델""을 고르는 역할은 아니지만, 질문, 가이드찾기, 최신 소식이나 툴 정보, 다양한 모델 비교 등에 매우 도움
       결국에는 내가 직접 여러 모델을 써보고 파라미터 조절하면서 가장 내 목적에 맞는 걸 찾는 과정
       Hacker News 사용자라면 Ollama나 LMStudio는 건너뛰는 것도 고려할 만함
       최신 모델 접근성이 떨어질 수 있고, 이들이 테스트한 모델 중에서만 골라야 할 때가 많음
       그리고 내부 동작을 ""뚜껑 열고"" 보는 재미가 없다는 아쉬움
       llamacpp만으로도 대부분의 최신 모델 지원하며, 필요한 경우 발빠르게 업데이트됨
       huggingface에서 모델을 받아서 GGUF 포맷(낮은 quantization으로 메모리 절약) 쓰는 걸 선호
       GGUF 파일 사이즈를 보면 VRAM에 맞을지 대략 감이 옴(예시: 24GB GGUF는 16GB에는 무리, 12GB는 가능- 단, context가 늘어나면 RAM 소모도 같이 커짐)
       context window도 주의, 예전 모델은 대부분 8K 컨텍스트지만 32K로 세팅해도 효과가 크게 오르지 않음
       llamacpp는 리눅스, 윈도우, 맥OS에서 바이너리 다운로드 혹은 직접 빌드 가능, 모델을 VRAM/RAM 사이에 분할도 가능
       간단한 React 프론트엔드(llamacpp-server) 제공, OpenAI와 유사한 REST API도 제공
       덕분에 oobabooga(textgeneration webui) 등 여러 프론트엔드와 연동
       Koboldcpp는 llamacpp가 투박하다면 고려해볼 만한 백엔드(여전히 내부는 llamacpp 기반)
          + Ollama는 HuggingFace에서 어떤 GGUF든 바로 받아서 ollama run hf.co/unsloth/DeepSeek-R1-0528-GGUF:Q8_0 식으로 돌릴 수 있다는 점이 매력
          + Ollama의 장점 중 하나는 모델을 GPU에 쉽게 로드/언로드할 수 있어, librechat이나 openwebui 같은 프론트엔드에서 드롭다운만으로 손쉽게 모델 바꿀 수 있다는 점
            커맨드라인 조작 없이 간편하게 모델 변경이 가능하다는 점을 강조하고 싶음
          + Ollama는 데스크탑을 LLM 서버화하고, WiFi를 통한 원격 기기에서도 접근 가능
            모델을 바꿀 때도 Ollama는 서버를 내리지 않고도 매끄럽게 스왑하는 기능 제공
            llama.cpp의 경우 CLI에서는 서버를 내렸다가 플래그를 새로 주고 띄워야 해서, 실험이나 빠른 앱 개발에 불편
            내가 만든 앱 중에도 서버를 재시작하지 않고 1B, 8B, 30B 등 모델을 웹 리퀘스트 파라미터만으로 바꾸는 기능이 꼭 필요한 게 있음
     * VRAM 8GB밖에 없지만, Ollama 프론트엔트로 OpenWebUI를 붙여 여러 모델을 동시에 로드하고 round robin 방식으로 번갈아 시험함
       계속 답변 결과도 모니터링해서 장기적으로 어떤 모델이 내 목적에 더 맞는지 선택 가능
       OpenWebUI로 독특한 사용 경험
          + AMD 6700XT(12GB VRAM) 사용자로서, local ROCm 세팅에 성공한 이후 Ollama를 GPU 가속으로 문제없이 구동
            Docker로 띄운 OpenWebUI 인스턴스를 local Ollama 서버와 연동하는 것도 ENV 변수 한 번 설정으로 끝
            이는 프로덕션이 아니라 퍼스널 테스트 환경이지만, 위에서 설명된 목적엔 아주 잘 맞는 경험
          + OpenWebUI가 최근 라이선스 변경으로 더 이상 오픈소스가 아니라는 점은 알아둘 필요
     * Qwen3 계열(그리고 R1 qwen3-8b distill)은 코딩, 논리적 추론 성능에서 1위
       단, 중국발이라는 특성상 정치 이슈에선 센서가 심함
       세계 상식, 최신 정보는 Gemma3 추천
       이 글도 한 달 후엔 구식 정보가 될 확률이 크니, livebench.ai나 aider.chat 리더보드의 최신 벤치마크 참고
          + 변화 속도가 상상 이상
            모델뿐 아니라 툴, 라우터, MCP, 라이브러리, SDK도 계속 진화
            내가 혼자 개발하고 주변에 같이 정보 공유할 동료나 모임이 없는 경우, 정보 습득 및 최신 동향 팔로우를 위한 조언이 필요
     * 제일 좋은 정보원은 HuggingFace
       Qwen 시리즈는 다방면에서 괜찮고, Qwen/Qwen3-14B-GGUF Q4_K_M 모델을 추천
       VRAM 7-8GB 정도만 쓰니까 부담 적고, llama-server나 LM Studio 사용을 추천
       Llama 3.3도 괜찮은 선택
       Devstral은 너무 커서 퀀타이즈드 모델로만 시도 가능
       Gemma는 거절이 많지만, Medgemma 등 특정 목적에는 유용
       Eric Hartford의 “Uncensored” Dolphin 모델 및 abliterated 모델은 만약 농담 생성이나 보안, 국방 관련 작업처럼 거부감 없는 모델이 필요할 때 추천(일상 사용엔 꼭 필요는 아님)
       bf16 dtype 기준, 파라미터 수 x2로 언퀀타이즈드 모델 용량 산출
       Q4_K_M(4비트) 퀀타이즈된 모델 쓰면 파라미터 수의 절반이 VRAM 요구량
       액티베이션 오버헤드 등도 고려해서 16GB보다 한참 아래 모델부터 실험 추천
       llama-server는 GUI, -hf 옵션으로 모델 다운도 지원
       LM Studio도 설치 및 모델 관리 편함
       빠른 응답 속도를 원하면 서버는 한번만 띄워서 여러 질의에 모델을 공유 사용해야 함(질문마다 새로 로딩하면 느림)
     * 16GB 기준 Q4 quant Mistral Small 3.1이나 FP8 Qwen3-14B가 큰 무리 없이 잘 돌아감
       다만 VRAM 사용량에 따라 context length를 길게 쓸 때는 Q4 quant Qwen3-14B가 FP8보단 성능이 낮지만 메모리 여유가 더 있음
       Mistral Small은 이미지 입력도 지원, Qwen3는 수학/코딩에 더 특화
       Q4 이하로 낮추면 효율이 떨어지니 권장하지 않음
       긴 context가 목적이면 Q4 quant Qwen3-8B 쪽이 낫고, Qwen3-30B-A3는 16GB VRAM엔 쪼금 부족할 듯(무거운 모델은 GGUF 기준 15GB 이상 차지하니까)
       dense모델(모든 파라미터 활용)이 sparse모델(희소 모델)에 비해 파라미터당 성능은 더 뛰어나지만 속도는 느림, 5060급 GPU로 14B는 충분히 쾌적
       Blackwell 아키텍처라면 NVFP4로 퀀타이즈한 모델이 FP8보다 더 빠르지만 품질은 아주 약간 낮아지고, ollama에서는 아직 미지원이니 vLLM은 별도 사용 필요
       프리퀀타이즈된 NVFP4 모델은 지원이 적어 직접 llmcompressor 등으로 퀀타이즈 추천
       일단 원하는 LLM을 고른 뒤 퍼포먼스 개선할 때만 이런 도구 활용 추천
     * LLM에 대한 객관적, 명확한 정답은 불가능에 가깝고, 직접 최신 모델 여러 개를 본인에게 의미 있는 작업에 써보는 경험이 제일 중요
       작업 유형에 따라 결과의 품질 차이가 극심
     * 흔히 VRAM 사용량을 어떻게 추정하는지 궁금
       gguf 파일 등 다운로드 가능한 모델 정보에 VRAM/메모리 요구량이 딱히 안 써 있어서 아쉬움
          + 매우 대략적으로 파라미터 수(B단위)를 GB 단위 메모리로 보면 됨
            퀀타이즈 기준 예시:
            FP16 = 2 x 8GB = 16GB(8B 모델)
            Q8 = 1 x 8GB, Q4 = 0.5 x 8GB = 4GB
            실제론 약간 다르지만 크게 벗어나지 않고, context 길이 등 추가 메모리도 별도 써야 함
            원리는 float 값 수 x 자료형 비트수(4,8,16...)의 조합
          + 퀀타이즈 외에도 KV 캐시 등 정확히 계산하고 싶으면 VRAM 계산기 활용 추천
"
"https://news.hada.io/topic?id=21274","Android에서 Localhost를 이용한 은밀한 웹-앱 트래킹 기법 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Android에서 Localhost를 이용한 은밀한 웹-앱 트래킹 기법 공개

     * Meta(페이스북), Yandex 등 주요 앱이 Android에서 로컬 포트(127.0.0.1)를 사용해 웹 브라우저와 네이티브 앱 간 식별자·쿠키를 비밀리에 공유한 사실이 공개됨
     * 웹사이트에 심어진 Facebook Pixel, Yandex Metrica 스크립트가 Android 브라우저에서 네이티브 앱(페이스북, 인스타그램, Yandex 계열 앱)으로 브라우징 세션과 식별자를 직접 전달, 사용자 식별 및 탈익명화가 가능해짐
     * 이 방식은 쿠키 삭제, 시크릿 모드, 권한 설정, 광고ID 리셋 등 기존 프라이버시 보호책을 모두 우회하며, 악성 앱이 포트만 맞춰 듣고 있으면 브라우저 방문 이력 수집도 가능함
     * 2025년 6월 3일 공개 이후 Facebook 측은 해당 코드를 대부분 제거했으나, 해당 기법이 수년간 전 세계 수억대 안드로이드 기기에서 이용됨. Yandex는 2017년부터 유사한 방식을 지속적으로 사용 중임
     * 크롬, 파이어폭스, 브레이브 등 주요 브라우저들은 긴급 차단 조치를 도입했지만, 플랫폼 구조적 한계로 완전한 근본 대책은 미흡, Android IPC와 로컬 네트워크 보안 강화 필요성이 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Disclosure: Localhost를 통한 은밀한 웹-앱 트래킹 기법

     * 연구진은 Meta와 Yandex가 수십억 안드로이드 사용자를 대상으로, 네이티브 앱이 정해진 로컬 포트(예: 12580~12585, 29009~30103) 를 백그라운드에서 열어두고 웹에서 실행된 자바스크립트와 통신하는 방식을 발견함
     * 이를 통해 웹브라우저의 쿠키, 메타데이터, 사용 기록이 네이티브 앱에 전달되고, 앱 계정 정보 및 Android Advertising ID 등과 결합되어 사용자 신원이 웹 방문과 연결됨

How does this work?

  Android의 로컬 포트 악용

     * Android OS에서 모든 앱이 INTERNET 권한만 있으면 127.0.0.1(루프백)에 소켓을 열 수 있음
     * 브라우저 역시 별도의 사용자 동의 없이 이 인터페이스 접근 가능
     * 웹사이트에 심어진 자바스크립트가 브라우저와 네이티브 앱간에 표준 Web API를 이용해 데이터 송수신 가능

    Meta/Facebook Pixel의 웹-앱 연동 방식

     * Meta Pixel 자바스크립트가 Android 브라우저에서 로드될 때, _fbp 쿠키 값을 WebRTC의 STUN 패킷(UDP 12580–12585 포트)으로 네이티브 앱에 전송
     * 기기 내 Facebook, Instagram 앱(버전별로 515.0.0.23.90/382.0.0.43.84 등)이 해당 포트에서 수신 대기, 브라우저에서 온 _fbp 값을 수신해 자체 서버에 GraphQL로 전송
     * _fbp는 약 25%의 상위 백만 사이트에 심겨있는 주요 쿠키로, 기존엔 사이트별로 별도라 교차 추적이 어려웠지만, 이 방식으론 사용자의 다중 _fbp를 한 계정으로 매칭 가능
     * 2025년 5월 이후에는 WebRTC TURN 방식을 추가, SDP Munging 기법을 피하도록 스크립트가 변경됨
     * _fbp 쿠키는 90일간 유지, 25%의 상위 웹사이트에서 사용될 정도로 매우 흔함
     * Chrome 등 주요 브라우저 대응 후 6월 3일 코드 삭제

    Yandex Metrica의 웹-앱 연동 방식

     * Yandex Metrica 스크립트는 2017년부터 HTTP(S)로 로컬 포트(29009, 29010, 30102, 30103 등)에 요청을 보냄
     * Yandex 앱(Yandex Maps, Navigator, Browser, Search 등)이 포트를 열어두고, 받은 요청에 대해 Base64 인코딩된 Android Advertising ID(AAID) 와 기타 디바이스 식별자, UUID 등을 포함한 데이터를 응답
     * 브라우저 스크립트가 이 정보를 수집해 다시 Yandex 서버로 전송, 브라우저-앱-서버 간 식별자 연동 완성
     * yandexmetrica.com 도메인이 127.0.0.1로 리졸브되어 탐지 회피 및 수집 흐름 은폐
     * 로컬호스트 HTTP 사용으로 인해, 만약 다른 앱이 동일 포트를 리스닝할 경우 사용자의 웹사이트 방문 기록 노출 위험까지 있음

  실질적 위험: 브라우저 방문 이력 유출

     * HTTP 기반 로컬 통신을 이용할 경우, 임의의 Android 앱도 해당 포트만 듣고 있으면 브라우저 방문 URL 등 히스토리 획득 가능
     * 실제 Proof-of-Concept 앱 개발 및 크롬·파이어폭스·엣지에서 실험해 프라이빗 브라우징, 시크릿 모드도 모두 취약함을 입증
     * Brave, DuckDuckGo 등 일부 브라우저만 자체 블록리스트 및 사용자 동의 요구로 방어됨

Affected Sites

     * Meta Pixel: 580만 개 웹사이트에서 사용, 실제 크롤링 결과 상위 10만개 사이트 중 EU 1.5만, US 1.7만 사이트에서 로컬 ID 공유 관찰
     * Yandex Metrica: 300만 개 웹사이트에서 사용, 동일 방식으로 EU 1,260, US 1,312 사이트에서 로컬 포트 통신 확인
     * 이 중 상당수 사이트는 쿠키 동의 절차 없이도 자동 트래킹 실행됨

History

     * Yandex: 2017년부터 HTTP/HTTPS 포트 이용 시작
     * Meta: 2024년 9월 HTTP, 2024년 11월 WebSocket, 2025년 WebRTC STUN, 5월 TURN으로 단계적 전환

Abuse Vectors

     * 안드로이드의 로컬호스트 소켓 접근 제한 부재 및 샌드박스 정책 미흡이 주요 원인
     * 기존 권한 설정, 브라우저 시크릿 모드, 광고ID 리셋 등 모든 보호책 우회
     * 웹 개발 목적의 합법적 용도와 구분 어려우나, 대규모 트래킹 실증 사례로 남음
     * Chrome, Firefox, DuckDuckGo, Brave 등 브라우저는 긴급 대응 패치 중이나, 근본적으로는 플랫폼 차원의 권한 및 경고, 샌드박스, IPC 정책 강화 필요

Disclosure

     * 크롬, 파이어폭스, DuckDuckGo, 브레이브 등 브라우저 공급사에 책임 공개 및 협력 요청
     * 크롬(137버전), 파이어폭스(138버전), 브레이브 등 취약 포트 차단, SDP Munging 차단 등 단기 조치 시행
     * 장기적으로는 로컬 네트워크 접근 통제, 샌드박스 보강, 사용자 안내 등 구조적 보완 필요성 강조

      브라우저       버전      Yandex    Facebook                대응/차단 현황
   Chrome     136.0+   영향         영향         137부터 포트 및 SDP munging 차단, 단계적 적용 중
   Edge       136.0+   영향         영향         불명(Chromium 기반)
   Firefox    138.0.2  영향         영향없음(1)    SDP munging 차단, UDP는 차후 차단 예정
   DuckDuckGo 5.233.0  일부 영향(2,3  영향없음(2,3)  블록리스트 기반 차단
   Brave      1.78.102 영향 없음(3,4) 영향 없음(3,4) 2022년부터 로컬호스트 요청 사용자 동의 필요, 블록리스트 적용

     * 1: SDP Munging 차단, TURN 포트는 아직 미차단(향후 적용 예정)
     * 2,3,4: 블록리스트, 포트 차단, 사용자 동의 등 다양한 방어

사용자·운영자 인지 현황

  사이트 운영자

     * Meta, Yandex 공식문서에는 해당 방식이 공개된 바 없음
     * 2024년 9월부터 Facebook 개발자 포럼 등에서 ""왜 Pixel 스크립트가 localhost에 접근하나"" 문의 잇따랐으나, 공식 답변 전무
     * 사이트 운영자, 최종 사용자는 대부분 인지하지 못함. 사용자가 로그인하지 않은 상태, 시크릿모드, 쿠키 삭제 등 상황에도 추적 가능

  일반 사용자

     * 로그인 상태와 무관하게 트래킹 동작
     * 시크릿 모드, 쿠키 삭제 등 보호책 무력화
     * 쿠키 동의 절차 없는 사이트에서도 작동하는 사례 다수

FAQ 요약

     * Q: 왜 Meta는 공개 직후 해당 방식을 중단했나?
       A: 공식 답변 없음, 공개 이후 안드로이드 유저 대상 패킷 송신 중단 확인
     * Q: 연구가 피어리뷰(동료 검증) 되었나?
       A: 일부 기관에서 검증했으나 논문 심사 전, 악용 규모 때문에 신속 공개 결정
     * Q: Meta/Yandex 공식 문서에 공개되어 있나?
       A: 공식 기술 문서 없음, 개발자 포럼 문의만 존재
     * Q: iOS/타 플랫폼도 영향받나?
       A: 현재까지 안드로이드에서만 확인, 기술적으로는 iOS/데스크톱/스마트TV 등도 잠재적 위험 있음

   서비스에 돈을 지불하지 않으면 내가 제품인 거죠. 점점 더 데이터를 통해 개인을 추적하려는 시도는 많아질 거고, 이러한 흐름을 되돌릴 수 있어 보이지는 않습니다. 더 나은 대안이 필요한데 자본주의 아래에서 더 나은 대안이 뭘지는 잘 떠오르지 않네요.

   갤럭시 보안폴더 내-외부 localhost 접근이 격리되는지 궁금하네요

   격리가 안 되네요. 보안폴더 밖에서 termux로 파이썬 http.server를 실행시키고 안에서 크롬으로 접속하니 접속이 됩니다

   이거 보안 구멍 아닌가요 -_-??

   이상하게 배터리를 많이 먹어서 메타쪽 앱들을 다 지웠었는데 이런 일이 있었네요... adb로 나머지 갤럭시에 내장된 시스템 앱도 다 지워야 겠습니다.

   저도 메타 앱은 믿을수가 없어서 안쓰고 대신 보안폴더 내 크롬으로만 이용합니다

   하이브리드 웹앱이라고 부르는 프레임워크 류는 대부분 (목적은 다르지만) localhost 웹서버 띄웁니다. 내장 브라우저 라이브러리(웹킷…) 설정이나 커스텀으로도 해결안되는 것들(웹 파트) 것들을 localhost에 띄워놓은 웹서버(네이티브 파트) 쪽에서 해결하는 거죠. 그걸 이렇게 활용할 수도 있었는데… 아까비

   제 생각으론 하이브리드 앱에서 웹/앱 간 일반적인 통신 방법은 브릿지라고도 부르는 OS와 브라우저 단에서 제공하는 API를 통한 방식이에요. 로컬 웹 서버는 필수가 아니라고 봐요.
   저기서 로컬 웹 서버를 써서 문제가 된 이유는 가령 시크릿 모드 크롬에서 로컬호스트 포트로 접근해서 사용자의 익명성을 깨뜨리는 취약점 등이 가능해서라고 봐요. 이런 기술이 하이브리드 앱에서 필수면.. 하이브리드 앱이 사라져야죠.

   도메인네임이 강제되는 피쳐들, localStorage 등등을 처리하기 위해 앱 내에서 웹 서버를 열어 쓰는게 일반적이긴 합니다.

   sns를 안하는게 정답..인거 같군요

        Hacker News 의견

     * Meta에서 사용하는 전체적인 트래킹 과정을 내가 이해한 대로 정리해보면, Localmess 블로그를 참고한 내용임
         1. 사용자가 Facebook 또는 Instagram 앱에 로그인 상태일 때, 해당 앱이 백그라운드에서 특정 포트로 들어오는 트래픽을 수신함
         2. 사용자가 휴대폰 브라우저로 웹사이트(e.g. something-embarassing.com)를 방문하면, 해당 사이트에 Meta Pixel이 삽입되어 있는 경우가 많음 (기사에 따르면 580만 개가 넘는 웹사이트에 설치됨)
            인코그니토 모드여도 여전히 추적이 가능함
         3. 위치에 따라 웹사이트가 사용자 동의를 요구할 수 있는데, 기사에서는 자세한 설명이 없지만 아마도 많은 사용자가 아무 생각 없이 동의해버리는 '쿠키 배너'를 의미함
         4. Meta Pixel 스크립트가 _fbp 쿠키(브라우징 정보 포함)를 WebRTC (STUN) SDP Munging 기법을 이용해 Instagram 또는 Facebook 앱으로 전송함
            이 과정은 브라우저 개발자 도구에서도 보이지 않는 부분임
         5. 앱에 이미 로그인된 상태라면 Meta에서는 ""익명"" 브라우저 활동을 로그인된 사용자 정보와 연결 가능
            앱이 _fbp 정보와 사용자 ID를 Meta 서버에 전달함
            추가로 주목할 점은,
          + 이 웹에서 앱으로 ID를 공유하는 방식은 쿠키 삭제, 인코그니토 모드, 안드로이드 권한 제어 같은 일반적인 프라이버시 보호책을 우회함
          + 심지어 악성 앱이 사용자의 웹 활동을 염탐할 수 있는 가능성도 열려 있음
          + 5월 중순 이후 Meta Pixel 스크립트가 _fbp 쿠키를 WebRTC TURN 방식으로도 보내기 시작했으며, 이 방법은 Chrome 개발팀이 SDP Munging을 차단한 후 도입됨
          + 2025년 6월 2일 기준으로는 해당 새 포트를 통해 Facebook/Instagram 앱이 실제로 수신하는 행동은 관찰되지 않음
          + WebRTC의 주된 활용 사례가 사용자의 로컬 IP 같은 정보를 가져와 익명성을 해제(디아노니마이즈)하는 것이라면, 왜 이런 기능이 별도의 권한 요청 없이 실행되는지 이해가 안 됨
          + 국가에 따라 something-embarassing.com 같은 사이트 방문이 민망함을 넘어서 훨씬 더 심각한 결과로 이어질 수 있음
          + 완전히 이해되는 것은 아니지만, 혹시 필수적인 GDPR 쿠키 동의 공지를 악용해서 사람들을 비밀스럽게 추적하는 것도 포함된 것인지 궁금함
     * 인터넷 광고와 추적을 그냥 금지하고 싶음
       이런 것들 때문에 의미 없는 것들이 너무 많이 쏟아져 나옴
       다 CEO들이 요트 한 척 더 사려고 생긴 문제라고 봄
          + Reddit도 기기 지문 수집을 상당히 많이 하고 있음
            이 데이터들을 AI 모델 학습용으로 판매도 하는 중
            곧 프리미엄 앱에서만 쓸 수 있는 비공개 데이터까지 적극적으로 판매하는 날이 올 거라 예상함
          + 이걸 어떻게 금지할 수 있을지, 또 누가 그 법을 어겼다는 걸 어떻게 증명할 수 있을지에 대한 고민이 남음
          + 브라우저에서 3rd-party 쿠키를 퇴출하자는 움직임이 실제적으로 가장 현실적인 첫 단계였음
            그런데 Google이 Chrome 지배력을 이용해 지난 해 이걸 좌초시켰음
            법적으로는 문제 없지만, 소비자 분노를 샀어야 할 비윤리적 시장조작임
            Google 임원진들은 처음엔 쿠키 없이도 수익 유지할 방법이 있다고 믿은 듯하고, 실제로는 쿠키의 의미를 전혀 이해 못 하거나, 혹은 애초에 제거할 생각이 없었을 가능성도 있음
          + 이런 형태의 행위는 순전히 탐욕임
            수 세기 동안 성공한 전통적 경영자들은 이런 식의 과도한 자기 이익 집착을 멀리했음
            보통 적당한 리더들도 이런 낮은 행동에서 벗어나 회사를 더 잘 이끌 수 있음
            하지만 탐욕만 남은 세상에선 그저 웃어넘길 수밖에 없는 상황임
            만약 더 정직하면서 뛰어난 CEO가 있다면 참 좋겠다는 생각임
          + 'CEO의 요트' 농담에 덧붙여, 사실 소비자들 대부분은 돈 내지 않아도 되는 서비스/제품이 좋아서 광고 모델을 선택함
            실제로 유료와 광고 버전이 있으면, 광고 지원 쪽이 10:1로 인기임
            광고 차단이 오히려 상황을 악화시킴 — 진짜 저항은 서비스를 불매하거나 대안에 직접 지불하는 방식이어야 함
            BAT(Brave Attention Token) 같이 직접 사이트에 소액 결제를 분배해주는 구조가 오히려 합리적이라고 생각함
            이론 자체는: 내가 쓰는 만큼 지불하고, 내가 광고주가 아니라 진짜 고객이 되는 구조임
     * 실제 이슈 리포트: Localmess 블로그
       Google은 남용 사례를 조사 중이라고 하는데, 역설적으로 Google 자체도 Wi-Fi AP 이름 같은 다양한 사이드 채널을 이용해 모두를 추적 중임
       대형 앱 기업은 OS 제한을 피하기 위해 유사한 방식으로 데이터 수집을 계속함
     * 또 하나의 이유: 빅테크의 앱은 되도록 설치하지 않고 꼭 써야 할 때만 웹사이트를 이용함
       웹사이트는 느리고 불편하긴 하지만, 샌드박스 처리로 훨씬 안전함
          + 어떤 Meta 앱이 포트를 여는지 명확하진 않음
            예를 들어 삼성폰은 여러 Meta 앱이 기본 탑재되어 있고, Facebook 앱만 지워도 com.facebook.services 등 숨김 서비스가 남아 있는 경우가 있음
            이 서비스들은 개발자 도구(ADB/UAD)로만 삭제 가능
            아니면 iPhone 또는 Pixel폰을 추천함
     * Meta Pixel 스크립트 관련 테크니컬 정보:
       Meta Pixel이 2024년 10월까지 HTTP로 송신했고, Facebook/Instagram 앱은 해당 포트에서 현재도 수신 대기 중
       새로 생긴 12388 포트로도 대기 중이지만, 해당 포트로 송신하는 스크립트는 아직 발견되지 않음
       이에 대해, 만약 다른 앱이 이 포트로 가짜 메시지를 보내도 될지 궁금하다는 과학적인(?) 궁금증이 있음
          + 이런 트래커들을 혼란시키는 방법은 두 가지가 있다고 봄
            한 번은 아무것도 보내지 않는 방법이고, 다른 하나는 거짓 데이터를 잔뜩 보내는 것
            광고주 추적 쿠키를 P2P로 공유하는 장치도 있으면 좋겠다고 생각함
     * 프로필 간에 이 트래킹이 넘어갈 수 있는지 궁금함
       만약 그렇다면 기업 입장에선 엄청난 보안 이슈임
       Userland 앱에서 8080포트로 서버를 띄워서 테스트해보니 두 프로필 모두 접근 가능했음
       이 말은 한 프로필에 감염된 앱이 다른 프로필에서 접속한 사이트와 데이터를 주고받을 수 있다는 뜻임
          + 단, 해당 사이트가 (인증 없는) 로컬 포트에 바운드된 서비스와 명시적으로 통신할 때 가능하지 않을까 하는 질문임
     * 이러한 방식으로 개인이 남의 컴퓨터에서 정보를 수집하면 CFAA(Computer Fraud and Abuse Act)로 처벌받을 수 있을지 궁금함
          + 이 방법은 한쪽(방문중인 사이트)과 상대방(폰에서 실행 중인 앱) 모두 코드 통제가 필요함
            그냥 마법처럼 임의의 브라우저 히스토리를 탈취하는 해킹 기법은 아님
            따라서 명확하게 해킹이라 보기 어렵고, Google/Meta 등에서 비동의 추적을 한다 해도 CFAA에 해당하진 않음
          + 사실 CFAA로는 단순히 브라우저에서 '페이지 소스 보기'만 해도 기소된 사람들이 있었음
            범죄 행위 자체보다는 누구를 건드렸는지, 네트워크와 관계가 더 중요한 듯함
          + 처벌 가능성 있음
     * 해당 ID 시스템은 악용이 너무 쉬웠고, Google도 이를 인지하고 반드시 남용 방지 규정을 만들어야 함을 알았을 거라 짐작함
       페널티(Play Store 영구밴, 법적 조치, 심지어 형사 고발 등)까지 이어질 수 있는 문제임
       하지만 현실적으로는 Meta 같이 규모가 너무 큰 기업이면 실질적 제재가 거의 불가능한 상태임
       (그리고 Meta가 아니더라도, 이런 수상한 움직임을 정보기관이나 법 집행기관이 암묵적으로 승인한 것일 수도 있음 — 문제를 멈추기는 매우 어렵고, 이야기하기조차 쉽지 않음)
          + Google과 Apple은 운영체제 전체를 소유하고 있음
            자체적으로 추적하는 방법이 50가지도 더 됨
            타 기업들도 사용자 데이터 공유 조항을 대형 기업과 재협상하면서 많은 돈을 벌어감
            이미 딜이 체결되고 권한도 받은 상황이고, 단지 일부 사용자가 이걸로 소란을 피운다는 것뿐임
     * Firefox에서 about:config에서 media.peerconnection.enabled 옵션을 false로 바꿔 WebRTC 차단 가능
       Netguard와 Nebulo를 non-VPN 모드로 조합하면 Meta 서버로의 불필요한 연결을 막을 수 있음
     * 유럽연합(EU)은 이런 문제에 대해 기록적인 수준의 벌금을 부과해야 한다고 생각함
       반복해서 문제를 일으킬수록 1~X%씩 누진적으로 올라가는 세금도 도입하면 좋을 듯함
       각 기업별 위반 사례를 한눈에 볼 수 있는 웹사이트도 동반 구축 필요성 느낌
          + Meta는 매년 벌금 내고도 700억 달러 정도의 순이익을 기록 중임
          + 벌금뿐 아니라, 어떤 경우엔 개인들도 훨씬 가벼운 위반으로 감옥에 간 사례가 있으니 더 강력한 조치도 필요성 존재
"
"https://news.hada.io/topic?id=21205","남성 고립 문제를 해결하기 위한 소셜 클럽 시작","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       남성 고립 문제를 해결하기 위한 소셜 클럽 시작

     * 미국에서 남성 고립 문제 해결을 위해 새로운 오프라인 소셜 클럽이 런칭됨
     * 해당 클럽은 Boston, NYC, SF 등에서 활동을 시작함
     * 현대 사회에서 많은 남성들이 심리적 외로움과 사회적 연결 부족을 경험함
     * 이 클럽은 소셜 네트워크와는 구별되는 실질적 만남을 지향함
     * 친목 도모와 건강한 사회적 유대감 형성을 목적으로 다양한 오프라인 모임을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * wave3.social은 미국 여러 대도시에서 시작된 새로운 남성 대상 오프라인 소셜 클럽 서비스임
     * 이 클럽은 Boston, New York City, San Francisco에서 첫 런칭됨
     * 현대 사회에서 남성 고립과 외로움 문제가 심각하다는 인식에서 출발함
     * 기존의 온라인 중심 소셜 미디어와 달리 실질적 만남과 교류의 장을 제공함

목적 및 특징

     * wave3.social의 주요 목표는 남성들이 일상에서 겪는 외로움과 사회적 단절 문제를 극복할 수 있도록 실질적인 소속감을 제공함
     * 회원들이 실제로 만나 상대방과 소통할 수 있는 정기적인 오프라인 모임을 중심으로 구성됨
     * 각 도시에서 다양한 활동 및 친목 프로그램을 통해 건강한 사회적 연대감 형성을 도모함
     * 온라인 커뮤니티와는 달리 실제 인간관계를 발전시킬 수 있다는 차별성이 있음

기대 효과

     * 남성들이 지속적으로 새로운 사람과 연결될 기회를 제공함
     * 사회적 문제로 대두되는 남성 고립 문제에 긍정적 해법 가능성을 제시함
     * 다양한 도시에서 서비스가 확대되면 지역사회 내 남성 네트워크 활성화에 기여할 수 있음

        Hacker News 의견

     * 이 아이디어는 자주 등장하는데, 그만큼 요즘 사회에서 중요한 문제라는 인식이 있기 때문이라는 생각임. 흥미로운 점은 해결책이 항상 특정 장소에 묶이지 않고 보편적으로 적용된다는 것임. 즉, 특정 카페나 레스토랑, 축구장이 아니라 사람들이 여러 장소에 모이도록 돕는 앱이나 서비스가 중심이 된다는 점임. 과거에는 생생한 사회적 교류가 있었던 장소들이 항상 ‘특정 장소’였음을 언급하고 싶음. 동네 주민들이 언제든 들를 수 있는 카페, 퇴근 후 일주일에 두 번씩 모두가 들르는 바 같은 고정된 물리적 공간에서는 사전 계획, 앱, 일정 등이 필요하지 않음
          + 항상 그런 것은 아니지만, 장소에 구애받지 않는 방식이 더 많이 회자되는 경향임. 그만큼 넓은 범위에 영향을 주기 때문인 듯함. 예를 들어 Men’s Sheds 같은 경우, 영국에 천여 개의 거점이 있음. Men’s Sheds는 사람들이 함께 만들거나 수리하고 지역사회를 돕는 공간으로, 복지 향상, 외로움 감소 및 사회적 고립 해소라는 목표가 있음. 2023년 조사 결과 구성원의 96%가 외로움이 줄었다고 답함. (menssheds.org.uk) 하지만 이런 공간에서도 변화가 생기는 경우가 있음. 예전엔 남성만 받았던 곳에 여성이 합류하면서, 지금은 남녀 반반이 됐고 모두가 좋아함. 남성들만을 위한 조용한 공간(모형 기차 전시)도 남아있긴 하지만, 남자 멤버들은 그곳에서 때때로 쉬면서 이야기를 나눔 (BBC 기사)
          + 과거에 ‘아무 때나 들를 수 있는 동네 카페’나 ‘퇴근 후에 늘 가는 바’처럼 고정되어 있던 장소들이 소멸한 원인에 대해 다양한 이론을 접함. 1) 소셜 미디어가 실제 만남보다 더 흥미로워졌고, 2) 문화적·인종적 다양성이 커지면서 사회적 신뢰가 낮아지고 결국 공공장소에서 멀어짐(Robert Putnam 참조), 3) 독립 바·카페들이 프랜차이즈에 흡수되면서 회전율 중심 운영에 치우침, 4) 시민권 운동 이후로 미국이 이상한 사람들로 가득한 곳이 되어 대부분의 사람들이 공공장소에 가길 기피, 5) 임금이 인플레이션을 못 따라가면서 이런 공간에 쓸 수 있는 여유 자금이 줄어듦, 6) 과거 바나 카페의 운영 주체였던 친목 단체나 참전 용사 클럽 등이 쇠퇴함
          + 내가 보기에는 이른바 ‘소셜’ 앱의 근간에는 사용자들이 실제 사람과의 만남을 회피하려는 욕구가 깔려있는 느낌임. 삭막하고 안전하며 거리감 있는 인간관계를 원하는 것 같음. 물론 누구나 실제 접촉을 갈망하긴 하지만, 과거에는 사회·공공활동에 더 적극적으로 참여했던 시기도 있었음. 그러나 요즘에는 이런 걸 우회하는 앱이 대세가 된다는 점에서, 사람들은 동시에 다른 이들과 가까이에서 어울리지 않으려는 욕망도 분명히 가지고 있음. 진짜 관계를 원한다면 실제로 사람들이 있는 곳에 가서 두려움을 내려놓고 스스로 인사해보라는 조언을 하고 싶음
          + 이건 그저 기술의 변화라는 생각임. “몇십 년 또는 한 세기 전”에는 장소 중심 사회 모임밖에 없었지만 지금은 여러 선택지가 존재함. 물론 카페나 펍을 직접 골라서 친구랑 만나기 시작하고, 조금씩 그곳 단골들과도 대화하고 알아가는 과정이 가능함. 처음엔 과하게 머무르지 않고, 서로 관심사를 파악하거나 부탁하거나 농담을 주고받으며, 친근한 경쟁이나 토론을 통해 서로를 이해하려는 시도—이런 게 다 사회적 유대 형성의 일상임. 장소 기반 모임은 필터가 약하기 때문에 다양한 사람들과 교류하는 게 강점이자 약점임. 반면, 장소에 얽매이지 않는 모임은 특정 활동이나 관심사에 집중해서, 이를 중심으로 모인 사람이라 서로 좀 더 쉽게 연결됨. 이런 방식에도 약점은 있지만 특별히 심각한 문제는 아니라고 생각함
          + 장소 기반 커뮤니티에는 대체불가의 매력이 있다는 감각임
     * 진정한 우정이 부족한 현상이 세 가지 문제에서 비롯된다고 봄. 1. 가짜가 아닌 진짜 모습을 드러내면 언제 어디서든 누군가에게 찍혀 인터넷에 남고, 누군가는 그걸 빌미로 자신의 도덕성을 과시하고 명성을 쌓으려 들기 때문에 애초에 진짜 모습을 보여줄 수 없게 됨. 2. 모두가 모바일하고 온라인으로 연결되어 있다 보니 굳이 주변 사람과 대화를 시도할 필요가 없고, 결국 사회적 기술이 퇴화하거나 아예 처음부터 익히지 못하게 됨. 최소한의 예의만 가지고 있지, 쉽게 말문을 트거나 친분을 쌓는 법은 모름. 3. 도시에서 사는 사람들은 함께 자라지도 않고, 교회나 로터리 클럽, 남성 전용 공간에도 가지 않음. 모두 쿨하고 자유로운 척만 하며, 진한 유대감이나 명확한 신념을 가지면 종교적이라고 손가락질받게 됨. 표면적으로는 다들 괜찮은 척 미소만
       짓지만, 속으로는 진심어린 관계를 맺지 못함. 남성 호르몬 감소, 학교가 여성 위주로 운영, 항상 남녀 혼성 공간, 세대단절 등도 추가 요인임
          + “진짜 모습을 드러내면 인터넷에 남는다”는 점이 정말 많은 남성에게 걱정거린지 궁금함. 난 한 번도 그런 걱정을 하거나 그런 사람을 본 적이 없음. “주변 사람과 말문을 터야 할 필요 없다”는 지적도 동의하지 않음—직장에서도 늘 얘기하고, 별난 사교 모임(예로 역사 펜싱 동호회)에서도 엄청 수다스럽게 지냄. 최근에는 야생동물 재활 자원봉사도 시작해서 말할 기회가 많음. “진정한 유대나 신념을 내비치면 종교적으로 보인다""는 주장도 과장이라 생각함—난 종교가 있든 없든 강한 신념을 openly 갖고 사는 사람(예: 채식주의자 친구들)들을 많이 알고 있음. 결국 남성 경험은 정말 다 다르고, 글쓴이 주위엔 유난히 판단적이고 비사교적인 사람이 많았던 게 아닌가 싶은 감상임. 많은 남자들이 공감할 수도 있겠지만, 내 경험은 정반대였음을
            전하고 싶음. 소속된 집단에 따라 이것이 갈리는 점이 크다 생각함
          + 내 경험상 위에서 언급된 이유들이 진짜 깊은 우정을 가로막지 않음. 1) 난 그런 걱정 한 적 없음. 2) 낯선 사람과 직접 대화도 잘하지만, 크게 외로움을 덜거나 깊은 우정을 키우는 데엔 한계가 있음. 3) 무신론자임에도 종교성으로 타인을 판단하거나 불안함을 느낀 적 없음. 내 삶을 분석해보면 핵심은 우정엔 ‘함께 보내는 시간’이 필요하다는 점임. 난 부모로서 직장인이고, 자동차 중심 도시에서 바쁘게 지냄. 겨우 일주일에 한 번 외출하거나 모임에 참석하는 정도라, 얼마나 많은 우정을 유지할 수 있을지 자연스럽게 한계가 있듯함. 뭐 하나를 다 누리는 건 불가능하다고 봄. 운동, 건강한 식사, 친구, 가족, 커뮤니티, 일, 각종 커뮤니티… 이 모든 건 시간이라는 자원을 소비함. 요즘 아빠들은 과거보다 자녀와 훨씬 많은 시간을 보낸다는 통계도 있듯,
            내 또래(밀레니얼 아빠)는 친구들과 시간을 아이와 바꾸고 있다고 생각함
          + 진짜 친구라면 진짜 내 모습을 보여줄 수 있다고 확신함. 내가 주로 어울리는 씬에서는 동의 없이 사진 찍는 걸 극도로 싫어함. 내가 주로 가는 행사에서는 카메라 전부에 스티커를 붙임. 난 그런 문화가 좋음—사람을 만나러 간 거지 인스타그램하려고 간 게 아니기 때문임. 물론 몰래 구석에서 조용히 찍는 경우도 있지만, 실수로 남이 찍히는 경우가 없도록 김새지 않게 함. 다들 더 안전하고 진정하게 느끼기 때문에 스티커는 일종의 리마인더 역할임. 2) 어색함은 아이스브레이킹 게임 몇 번이면 충분히 해소된다고 봄. 3) 소도시에서는 친밀감이 훨씬 깊지만, 동시에 남의 시선이 엄청나게 부담됨. 그건 못 견디겠음. 작은 도시도 마찬가지라 결국 다 아는 사이라 뒷담도 심함. 대도시가 좋은 점은 새로운 사람들과 장소를 만날 수 있고, 다양함을 누릴 수
            있다는 점임. 소도시는 종종 종교 등으로 동질성과 순응 압력이 심해 다름을 인정받기 힘들고, 결국 위선적으로 행동함. 그런 게 진정한 유대가 아니라고 생각함. 대도시에선 진짜 내 모습을 드러낼 수 있고, 동질적인 사람들과 어울리고, 새로운 커뮤니티도 찾을 수 있음. 남성 전용 공간은 별로임—남끼리 얘기하는 걸 터부시하는 분위기라 정서적으로 깊이 교류하기 어려움. 남자들끼리 있는 모임은 맥주 마시고 허세 부리고 TV 보거나 재미없는 스포츠·유치한 농담만 있음. 전혀 재미없고 피곤함. 이제는 그런 모임엔 빠짐. 여성 친구들과 훨씬 깊은 관계를 맺고 있고, 개방적이고 판단이 적어서 더 안전하게 느껴짐. 혼성 모임이 나에겐 필수임
          + 이런 식의 관점은 지나치게 비관적임. 나 역시 부모가 이민자로 지역사회도 없고, 교외에서 소외된 어린 시절과 ‘온라인에만 빠진 20대’를 겪음. 그 영향이 컸지만, 자기 자신을 돌아보고 새로운 방식에 적응하려고 노력함. 30대엔 깊은 친구관계들을 구축함. 나이·성별·배경 다양함. 에너지가 한정적이라 얕은 관계도 많지만, 종종 깊은 대화를 나눌 때도 있음. 자기 생각의 틀을 바꿔보기를 권장함
          + 난 항상 ‘진짜’임. 내가 뭘 놓치고 있는지 궁금함
     * 가끔은 빅토리아 시대 영국의 ‘젠틀맨스 클럽’(미국 성인클럽이 아님) 같은 공간이 있었으면 좋겠다는 생각임. 남성끼리 가서 책을 읽거나 대화·카드 놀이·식사·술 한잔을 할 수 있는 ‘제3의 공간’임. 서로 알고 있는, 일정 규칙이 존재하는 비교적 제한된 커뮤니티 공간이 문명화된 분위기를 만든다고 생각하는데, 요즘 이런 게 없어졌음
          + ""서로 알고 규칙이 존재하는 제한적 공간""은 사실상 컨트리 클럽과 매우 유사함. 태생적으로 계급주의와 배타적 규칙이 내포됨
          + 모두에게 Lodge 49 시리즈를 추천함. 광고 포함 무료로 볼 수 있음. 관계·외로움·현대 소외감을 에서 조금은 신비주의와 연금술을 가미해 훌륭히 다룸. 마치 할아버지 세대가 다니던 친목단체가 개인주의, 임대료 상승, 밀려남 등으로 사라진 걸 조명함. 그렇다고 Soho House 같은 고급 사교클럽을 만들 필요는 없고, 책과 적당한 회비, 바와 가끔 공개이벤트만 있으면 충분한 커뮤니티 공간을 만들 수 있음. 개발자, 철학·문학, 음악가 등 분야별로도 만들 수 있고, 손익분기점만 맞추면 큰 비용 들이지 않고도 충분히 운영 가능한 계산임
          + 이런 공간들이 미국에도 여전히 존재하지만, 회원 수가 크게 줄었음. 예시로 프리메이슨, Odd Fellows, Fraternal Order of Eagles, 엘크클럽, 무스클럽 등이 있음. 내가 사는 작은 마을에도 Eagles 클럽이 있는데, 1층엔 만찬장, 지하엔 회원 전용 바와 당구대, 강을 내려다보는 데크가 있음
          + 이 아이디어를 실제로 시작한 기업도 있음. 슐츠는 Starbucks를 집과 직장 사이의 제3의 공간으로 구상함으로써 커뮤니티와 연결을 촉진함 (출처)
          + 샌프란시스코의 Mechanics Institute Library도 훌륭한 사례임. 내가 회원이었음. 윙백 의자에 기대어 졸고 있는 사람들을 자주 볼 수 있는데, 라이브러리 자체도 훌륭함
     * 이런 노력들이 매우 의미 있다고 생각함. Men’s Sheds나 유사 모임이 모든 사람을 포용할 필요는 없다고 생각함. 미국(또는 다른 나라)의 백인 남성과 소년의 사회적 고립과 자원봉사 위축은 몇 년간 문제가 되어왔음. Robert Putnam의 Bowling Alone, Do Good Institute의 자원봉사 보고서, Scott Galloway 관련 최신 연구 등이 원인과 해결책을 다루고 있음. 20년 넘게 공공 정책을 연구한 입장에서 정부도 부분적 해법이 될 수 있지만, 지속적 재원과 평가에서 변덕이 심하고, 비영리 단체도 효과적으로 운영되는 경우가 드묾. 개인과 커뮤니티가 종종 스스로 일어서서 수요에 맞춰 새로운 시도를 하는 것이 오히려 자연스러운 방향임. 참여 정도나 파급력 증감도 괜찮음. 한 시도가 사라져도 금세 또 다른 시도나 대안이 나오게 마련임. 사회복지사로서 제대로 된 이론과 모범사례에
       근거한 접근이 바람직하다고 생각하지만, 자금이나 권력을 쥐고 있는 입장이 아니니 그 결정권은 없다는 결론임
     * 내가 사람들과 가장 연결감을 크게 느꼈던 곳은 뉴욕임. 이웃도 알고, 네트워크도 컸지만, 뉴욕 자체는 별로 좋아하지 않았음. 내가 진짜로 비판하고 싶은 건 주거 지역구조임. 이웃들과 어울릴 수 있는 공간은 늘 코너 숍이나 1층 샵, 동네 술집, 미용실, 피자집 등이었는데, 집에서 두세 블럭 이상 걸어야 하는 동네로 가면 그런 기회가 다 사라짐
          + 대도시 사람들이 비사교적이고 공동체 의식이 없다는 고정관념을 자주 듣지만, 내 경험은 정반대임. 수백 명 살던 시골에서는 모든 사람이 서로 알지만, 그 인맥도 오래된 것일 뿐 진정한 유대감은 약함. 오히려 모두 서로를 싫어하고, 자기네 그룹 내에서도 어쩔 수 없이 어울리는 관계가 대부분이었음. 직접 옆집 이웃과 별로 어울리지 않음. 반대로 대도시에 와서는 가게 주인이 내가 좋아할 만한 상품을 예약해주거나, 이웃들이 인사하고, 가게에서 다른 단골이 환영 선물도 주고, 동네 사람들이 나를 매우 자연스럽게 환영해줬음. 도시에서는 새로운 사람을 받아들이는 분위기가 좋았음. 물론 “우리 시골은 네 시골이랑 다르다!”는 주장도 이해하지만, 내가 보기엔 도시도 시골만큼 충분히 환영적임. 방에만 처박혀 인상만 쓰는 생활만 하지 않는다면,
            세상에 잠재적 친구가 넘쳐남. 전 세계 도시에서 사람들이 밖에서 대화하고 웃으며 어울리는 모습을 자주 봄
     * 볼더링(실내 클라이밍)은 내가 접해본 스포츠 중 가장 사교성이 높다고 느낌. 혼자 가도 자연스럽게 새로운 사람을 만날 수 있고, 이어폰만 안 끼면 다들 기꺼이 대화함. 볼더링장은 자유롭게 움직이고, 튜터나 선생 같은 위계 없이 각자 다른 난이도 코스를 시도함. 자신이 못 하는 코스를 상대가 하면 팁을 물어볼 수 있고, 반대로 내가 할 수 있는 걸 못 하는 사람이 있으면 도와주기도 하고, 누군가 어려운 걸 성공할 때 함께 응원도 해줌. 대화할 거리가 넘쳐나고, 혼자 가서 약간의 허심탄회함을 보이면 모두가 호감을 느낌
          + 피클볼도 굉장히 좋은 선택임. 오픈플레이 피클볼은 볼더링보다 사교성이 더 뛰어남. 비용도 저렴하고, 요즘은 경기장이 여기저기 많음
          + 주로 자연에서 야외로 하나, 체육관 같은 실내에서 하나 궁금함
          + 서로 비슷한 관심사를 가진 사람들이 한 공간에 자연스럽게 모인다는 점에 전적으로 동의함. 다만 누구나 자연스럽게 대화 나누는 분위기가 생기지는 않음. 내가 가는 유럽 지역(특히 스위스)에서는 사람들 대부분이 수줍음이 많고 남의 프라이버시를 매우 존중해서, 그냥 간단하게 인사만 해도 분위기가 확 달라지는 경우가 많음. 그리고 클라이밍할 때 이어폰은 정말 NO임. 커뮤니티 전체에서 안 좋게 보기도 하고, 안전 위험도 증가, 그리고 너무 자기중심적으로 보이기 때문임. 식당에서 음식 먹으며 이어폰을 끼는 것과 마찬가지임
          + 난 이 의견에 동의하지 않음. 볼더링은 고소공포, 이전 부상 등 mobility 문제가 있으면 접근이 굉장히 어렵고, 신체·정신적으로 최상의 상태가 아니면 위험하고 힘들기만 함. 나도 주변의 분위기에 따라 억지로 해봤지만 재미를 느끼지 못했음. “서열이 없다”는 점도 완전히 맞지 않음. 남성 집단에서는 어떤 스포츠든 암묵적으로 경쟁심이 작동해서 능력에 따라 위계가 생김. 굳이 감추려 해도 모두가 인식함. 상대적으로 축구, 핸드볼, 배구, 테니스, 탁구, 무술 같은 팀 스포츠가 더 사교성이 뛰어남—실제로 파트너와 함께 협동하고, 더불어 겨루게 됨. “혼자 가면 친구를 강화할 수 있다”는 주장도 해당 지역의 사회 분위기에 따라 다름. 내가 사는 독일어권에서는 정말 모르는 사람이 먼저 말을 걸지 않고, 주로 그룹 단위로 가서 단체끼리만 어울리고
            싶어함. 혼자 가는 사람들도 대부분 방해받지 않고 싶어 하고, 헬스장처럼 볼더링장도 대화를 나누려고 가는 곳이 아님
          + 성인들을 위한 데이케어 놀이터 같은 느낌임. 물어뜯기 사고만 적었으면 좋겠다는 농담임
     * 남성들 대다수가 서로 진지하게 교류하기엔 너무 불안감이 심하다고 생각함. 지나치게 경쟁적이거나, 대화에 아무런 관심이 없어 보이는 사람이 대부분이고, 상대방의 얘기를 듣기보다는 자기자랑이나 상대를 이기려드는 경우가 많음
          + 정말 공감함. 통계적으로 남성은 서로에 대해 별로 관심이 없음. 반대로 여성이나 딸에 대해 더 신경 쓰는 경우가 많음. 이는 진화적 이유와도 연관 있음. ‘여성과 아이들’이라는 표현이 괜히 생긴 게 아님
          + 이런 남성들에게서 공통점이 단순히 성별 외에는 뭔지 궁금함. 나는 프로그래머, 과학자들과 어울릴 때 이런 경우를 겪어본 적 없음. 최근에는 예술가나 IT 전문가들과 교류하면서도 전혀 해당되지 않았음
          + 문제의 뿌리는 훨씬 더 깊으며, 해결 위한 접근도 그만큼 어려워야 한다고 봄. 작년에 'Seek You'라는 그래픽노블을 읽었는데 이 주제를 아주 깊이 다룸. 고장난 TV 스테레오 타입(고독한 영웅) 등도 핵심적인 원인 중 하나임
     * 조금 반론을 제기해보고 싶음. NYT 기사에서 논하는 핵심은 Gell-Mann 효과로 설명 가능함. 인류의 대다수 역사에서 원래도 여러 굵직한 관계를 유지하긴 어려웠고, 인터넷 이전엔 장거리 소통 자체가 어려웠음. 요즘도 관심사별로 친구를 찾을 수 있는 기회(컨퍼런스, 콘서트, 스포츠 바 등)가 많음. 이 논의가 모호한 개념에 의존하다 보니 사실상 데이터로 증명하기 힘든 도덕적 공포심이 유발된 것은 아닐까 생각함
     * 전체적으로는 좋은 아이디어라고 생각하지만, 내가 적합한 도시나 국가에 살고 있고 이미 비슷한 게 없다면, 랜딩페이지 사진만 보고 “20대 백인 남성만을 위한 곳인가?”라는 생각이 들어서 바로 닫을 것 같음
          + 이 댓글이 역설적으로 이런 클럽이 불가능한 이유를 드러냄. 항상 누군가는 사진 속 인종·성별을 따짐. 실제로 잘 운영되는 남성 사교클럽들은 암묵적 규칙(예: “쩔쩔매지 않는 게이 남성은 괜찮지만 이상하게 굴진 마라”, “좌파 성향은 피한다” 등)이 존재하지만, 그런 암묵적 규칙은 요즘 더 이상 용납되지 않음. 그래서 남성 사교클럽은 결국 모두를 환영해주는 ‘보드게임형 클럽’이 되어 트렌드를 주도하는 최상위 남성들은 떠나게 됨
     * 어머니가 1976년쯤 “남자는 결혼하면 친구를 다 잃고, 아내 친구들만 갖게 된다”고 말씀하셨는데, 정말 예언 같다고 느낌. 물론 그 친구들의 남편들도 함께임
          + 대부분 남성은 학교, 직장, 교회, 동호회, 이웃 등 외부 환경이 친구를 붙잡아주지 않으면 우정을 유지하지 않는 편임. 고등학교 절친들도 대학 가면서 다 끊겼고, 대학 때 친구들도 졸업 후 딱히 이어진 경우가 없음. 직장 동료랑 매일 점심을 먹다가 그 사람이 은퇴하자 그게 마지막이었음. 아이가 같은 스포츠 팀을 뛰던 아빠들과 친하게 지내다가도, 아이들이 성장하면서 그 관계도 소원해짐. 이런 우정을 유지하려면 상황이 도와주지 않으면 본인이 노력해야 함. 여성도 비슷하겠지만, 내 경험상 여성이 연락이나 모임에 더 적극적임
          + 내 경험으로는 결혼 자체가 우정을 단절시키진 않음. 우정을 잃는 진짜 원인은 아이를 가지지 않는 경우임. 결혼 유무보다 자녀 유무가 훨씬 더 중요한 경계선임
          + 내 경험상 어머니 말씀은 사실이 아니었음
"
"https://news.hada.io/topic?id=21197","Show GN: zimport - 수많은 python 패키지를 압축하여 관리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Show GN: zimport - 수많은 python 패키지를 압축하여 관리

   python 패키지를 관리하다 보면 수많은 파일들이 부담스러울때 필요한 zimport 를 소개합니다.

  소개

     * zimport는 Python의 표준 zipimport를 대체하고 개선한 도구입니다.
     * zimport는 zip-archives에서 Python 패키지를 로드하고 관리하는 데 사용됩니다. 즉, Java jar처럼 Python 패키지를 관리할 수 있습니다. 또한 동적 라이브러리(.dll, .pyd, .so) 로드도 지원합니다.
     * 이 도구를 만들고 저의 python 작업 디렉토리는 약 160gb, 수백만개 파일에서 80G 1만 개 수준으로 줄어 들었습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  주요기능

     * zip-archive에서 동적 라이브러리 로딩 지원(.pyd, .dll, .so, .dylib)
     * zip-archive에서 내부 read() 시에 Java의 getresource처럼 내부 파일(예: 환경 파일) 읽기 지원
     * 컴파일된 .pyc 파일 지원(name.cpython-version.pyc 및 pycache 폴더)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  사용된기술

     * importlib, meta_path, path_hooks
     * function intercept (standard open, stat, read, ctypes.WinDLL, ctypes.CDLL 등)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  사용환경

     * python win/linux/macosx 지원
     * python version 3.8~3.12 지원
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  사용방법

     python -m pip install zimport
     import zimport

     * lib/site-package 디렉토리에서 패키지를 한꺼번에 압축하고, sys.path 에 추가하기만 하면 정상적으로 작동합니다.
     * 물론, 시간의 여유가 되신다면 패키지별로 압축하여, java 의 jar 처럼 의존성에 따라서 패키지를 sys.path 에 추가해 주시면 됩니다.
     * 또한 한번 압축된 package 는 share 하여 쓸수 있기 때문에 하드디스크 공간의 낭비를 줄여줄수 있다고 생각합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  마지막으로

     * 소스는 github 에 공개하였으며 현재 버전은 0.1.4 입니다. 몇몇 package (예를들어 transformers) 아직 지원하지 안으며, torch, torchvision, numpy, pandas 와 같은 major 패키지는 이상없이 동작함을 확인하였습니다.

     https://github.com/waveware4ai/zimport

     * 또한, portable python 과 아주 궁합이 잘맞습니다. 이것도 github 에 업로드 하였습니다. linux 버전은 직접 컴파일하였고, windows 버전은 embeded 를 개작하였습니다.

     https://github.com/waveware4ai/PortablePython

     * 사용시 발생하는 버그나 문제점들은 리포팅해주시면 개선하도록 하겠습니다.

   감사합니다. 좋은 하루 보내세요
     * cython을 사용하는 av 패키지등 import 관련해서 문의가 와서 동작가능하게 fix 하였습니다.
       또 다른 패키지도 동작이 안되면 리포팅 주시면 정정하도록 하겠습니다.

     * transformers 패키지도 이제 지원합니다.
       원래 목표가 ComfyUI 기반의 Wan2.1 (https://github.com/kijai/ComfyUI-WanVideoWrapper, https://github.com/Wan-Video/Wan2.1) 을 Portable 하게 동작시키는 것이었는데, 방금 transformers 관련 소스 수정을 마치고, zimport 를 사용하여 동작시키니, 잘 되네요.
       사용해 주셔서 감사합니다.
       https://github.com/waveware4ai/zimport
"
"https://news.hada.io/topic?id=21230","앤도르의 촬영기법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               앤도르의 촬영기법

     * Christophe Nuyens는 “Andor” 시즌2의 촬영 감독으로, 필름에서 디지털로의 전환과 새로운 촬영 기술 적용 경험을 공유함
     * 그는 아날로그 필름과 디지털 장비를 모두 경험하며, LED 등 최신 기술의 도입이 창의성에 큰 도움을 주었음
     * TV와 영화의 경계가 사라지면서, 에피소드형 작품도 높은 시각적 완성도를 요구하는 변화가 있었음
     * 디지털 기술 발전과 VFX·미술팀과의 협업, 촬영 현장의 실물 세트와 디지털 확장 결합 방식을 강조함
     * 다양한 나라에서 촬영 경험을 쌓으며, 프로젝트 별로 색감, 빛, 분위기 연출에서 차별화된 접근법을 택했음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Christophe Nuyens 소개 및 촬영 디렉션의 시작

     * Christophe Nuyens는 원래 전기 기술자로 시작했으나, 영화학교 입학 후 이미지(조명과 카메라) 분야에 매료돼 촬영 감독의 길을 선택했음
     * 기술적 분야만큼 예술적 감각도 훈련과 경험을 통해 키울 수 있음을 강조하며, 자신의 창의성을 꾸준히 개발해 왔음
     * 예술에 “보편적 기준”은 없으며, 모든 예술은 주관적이고 개인적 취향에 따라 다르게 받아들여짐을 피력함

필름에서 디지털로의 전환 경험

     * 영화학교 재학 시절 16mm 필름(Bolex, Arriflex SR2) 로 작업했고 당시의 디지털 편집 환경은 매우 열악했음
     * 졸업 후에도 필름과 디지털을 혼용하며, 아날로그와 디지털의 조명 방법 차이를 모두 경험함
     * 디지털 혁명은 예산이 적은 프로젝트에 자연스러운 야간 장면 등 새로운 촬영 가능성을 열어주었으며, 이를 통해 국제 무대 진출의 계기도 마련했음

최신 촬영 기술과 현장의 변화

     * 장비·센서·광원의 발전을 적극적으로 따라가고 테스트하는 것이 흥미 포인트라고 설명함
     * 최근 가장 큰 변화는 LED 조명(특히 RGBW)의 도입임. LED는 컬러·밝기·위치 실시간 제어로 장면을 더욱 세밀하게 연출 가능하게 해줌
     * 과거 젤필터, 텅스텐, HMI를 사용하던 시절 대비, LED의 유연성과 생산성 향상이 가장 만족스러움
     * 모든 장비가 무선화되며, 때때로 현장의 기술 붐비기가 문제가 되기도 함

영화/TV 경계의 변화와 “Andor” 참여 계기

     * 수년간 TV 에피소드 작업을 하며 점차 TV와 영화의 기술·예술 격차가 줄어듦을 직접 경험하고, 현재는 TV도 영화 못지않은 높은 품질을 요구받는 시대로 변화함
     * “Andor” 참여는 과거 “Riviera” 프로듀서인 David Meanti와의 인연을 통해 이루어짐

“Andor”의 비주얼 전략 및 촬영 기법

     * “Rogue One”과의 연결감을 위해 풀프레임 아나모픽 렌즈와 대형 센서를 사용해 더욱 시네마틱한 효과 달성 시도
     * 자연광을 최대한 살리는 lighting 접근법 추구함
     * 프리프로덕션 단계에서 감독(Ariel Kleiman)과 수차례 대본·아이디어 논의, 무드보드 제작, 3D 프리비즈작업 등 사전 준비에 충분한 시간을 투입함
     * 녹색 스크린 사용의 한계(스모그/플레어 제한, 자연성 저하) 를 인정해, 필요시 LED 월 또는 직접 채색된 배경화 사용(예: 결혼식, Krennic 연설 장면)으로 자연스러운 결과를 추구함

VFX·미술팀과의 협업 및 세트 구축

     * VFX, 미술, 아트, 촬영팀이 한 오피스에서 긴밀하게 협업
     * 모든 VFX 샷에 실제 조명감 연동, 세트 확장, 프리비즈 작업 등 물리적-가상 공간의 원활한 결합을 실현
     * 대다수의 세트는 Pinewood와 Longcross 스튜디오 내에 직접 구축했고, 외부 촬영지(Barcelona 등)도 적극 활용

현장 사례와 비주얼 차별화 실험

     * 주요 세트(예: Ghorman 광장)는 1층까지는 실물로, 그 이상은 디지털 확장으로 완성
     * Bix의 아파트 등은 LED 월로 외부 전망까지 실시간 연출하여 새로운 샷 구현
     * 각 에피소드 아크마다 완전히 다른 색감, 계절, 분위기를 시도(예: 겨울의 차가운 파랑, 여름의 따스함, Yavin의 클래식한 느낌 등)
     * 특정 공간에서는 일관된 조명·컬러값 적용으로 연출 목적을 달성함

현장 도전과 보람

     * 가장 도전적이었던 경험은 Mina-Rau의 곡물밭 장면촬영. 연기자 파업 등 변수 속에 현장 자연광과 인공 조명을 일치시키는 데 세심한 분석과 LED 활용이 힘이 됨
     * 기억에 남는 장면은 Yavin 촬영과 Ghorman의 밤샘 강탈 시퀀스. 혹한과 빗속의 대형 세트 조명 등 고된 과정이었지만, 결과에 큰 만족감 표함

작업 후 감상 및 코로나19 경험

     * 최종 공개 전 여러 차례 컷을 보며 아쉬움과 반성을 반복함. 스트리밍으로 공개 후 일정 시간 지나고 나서야 비로소 작품을 객관적으로 감상하는 기쁨을 느낌
     * 시즌2 촬영 당시 여전히 Covid19 규제가 엄격해 의사소통·전반적 분위기 유지가 어려웠으나, 규제 해소 후 서로 간의 교류가 훨씬 활발해져 현장이 따뜻해짐

인생 조언 및 개인 동기

     * 젊은 시절 자신에겐 “인내심을 가져라” 라고 조언하고 싶음
     * 촬영 현장의 긴 시간·노동과 가족과의 이별 등 어려움도 있지만, 지속적 배움, 새로운 사람, 문화와의 교류에서 행복과 동기를 얻음
     * 각국의 업무 방식(예: 프랑스의 예술적 토론, 영국의 효율성 추구) 를 경험하며 다양성을 즐김. 프랑스 음식에 특히 애정

맺음말

     * Christophe Nuyens와의 인터뷰를 통해 촬영이라는 예술과 공예의 세계, 현장 기술 혁신, 팀 협업, 글로벌 제작환경의 변화상에 대한 폭넓은 인사이트 공유

        Hacker News 의견

     * 영화 촬영팀 규모가 두 피자 팀보다 클 때 어떻게 협업이 가능한지 이해 못하겠다는 의문 제기, 누가 뭔가 바꾸고 싶을 때마다 Product Owner에게 티켓을 제출하고, 현재 스프린트가 끝날 때까지 기다려야 하는 것 아닌지 궁금증 표출, 프로듀서가 Business Owner로서 사용자 스토리만 신경 써야 하는데 디테일하게 지시하는 모습도 의아함, 실제로는 각자가 전문성을 살려 공통의 목표를 향해 자유롭게 일하는 방식에 긍정적 시각, IT가 제조업에서 많은 걸 배웠지만 예술적 제작 방식에서도 좀 더 배웠으면 하는 바람, 둘 다 최종 산출물을 위해 설계를 다듬는 작업이라는 유사성 강조
          + 내가 볼 땐 계획과 실행을 혼동하고 있는 듯한 느낌, 실제 티켓, 스프린트 등 많은 과정이 사전 제작 단계에서 이루어지며, 프리프로덕션에 엄청난 시간과 노력이 투입되는 부분 언급, 각종 액션 아이템(티켓)과 반복(스프린트) 작업, 예산 및 인력 구성 등 지속적인 조정 과정 설명, 프로듀서와 감독의 책임이 엄청나게 다름을 강조, 촬영장에서는 감독의 결정이 절대적이며 논쟁 없이 따라야 하는 일방 통행에 가까운 체계, 모두에게 주어진 권한은 사전 준비가 철저하게 되어 있을 때만 가능, 영화 제작이 소프트웨어 개발보다 더 개인에게 자율을 주거나 신뢰하는 시스템이 아니라는 사실 강조
          + 가장 큰 차이점은 일정과 규모, 소프트웨어는 수백만 명이 장기적으로 사용해야 하므로 안정성이 중요하지만 영화 촬영은 당장 장면을 찍고 결과물을 빨리 내는 쪽에 가까운 어수선한 현장 분위기, 촬영 중 시간에 쫓겨 임시방편/수습하는 일이 비일비재, 사후 보정에 의존하는 경우가 많음, 게임 개발은 중간 어디쯤 위치한 영역
          + 이러한 팀워크가 잘 안 이뤄지는 결과 정말 끔찍한 영화가 나오는 경우도 많음, 세상은 관리가 잘 안 되는 경우가 많고 유능한 사람이 항상 조직의 상위권에 오르지는 않는 현실도 존재
          + 촬영 현장 뒷면에서는 매우 치밀하게 계획이 이루어지며 모두가 일정 준수를 위해 노력, 자세한 내용은 프로덕션 보드 위키백과 페이지 참고 제안
          + 영화 현장은 각자의 자부심과 위치가 확실히 존재하는 곳, 모든 인원이 서로에게 의지하며 필요 없는 사람은 한 명도 없는 구조, 사무실보다는 군대 느낌에 더 가까운 팀워크
     * Andor의 촬영, 편집, 대본, 전체적인 분위기가 지금까지 봐온 Star Wars 영화들보다 훨씬 뛰어나다는 인상, 오리지널 영화 이후 Star Wars 프랜차이즈를 한동안 돈 벌기용으로만 느꼈는데 Andor는 색다른 감동, 이런 팀이 만드는 영화라면 프리퀄, 시퀄, 외전 등 어떤 형태라도 무조건 볼 의향
          + 감옥 에피소드는 하나의 영화로도 손색없을 완성도, Andor를 보고 나니 나머지 Star Wars 드라마와 영화가 지나치게 우스꽝스러워 보이게 되는 현상, 다음 시즌이 불가능하다는 점이 매우 아쉽다는 느낌
          + Andor에 정말 만족, 하지만 Gilroy가 Star Wars에 더 이상 참여할 가능성은 높지 않으며 실제로 시즌2 예산이 $290M이었고, 디즈니 경영진의 통제로 예산 한도가 제한된 사실 참고 정보로 관련 기사 링크 공유
          + 오리지널 트릴로지에서 멀리 떨어진 Star Wars 시리즈일수록 점점 더 좋아지는 것 같은 추세
          + Skeleton Crew 추천, 좀 더 어린 연령층 겨냥이지만 아이가 있거나 조카가 있다면 즐겁게 볼 만한 작품, Andor처럼 무거운 분위기 대신 우주에서 펼쳐지는 보물찾기/모험극 느낌, Mandalorian 시즌2 이후 시리즈들은 보지 않았지만 친구들도 평가가 비슷, 최신 Star Wars 중 Andor와 Skeleton Crew가 각각 독특하게 뛰어남
          + ""Star Wars""라는 이름 자체가 이제는 스토리적 방향성이나 품질, 장르를 예측하게 해주지 않는다는 사실에 적응, 이제는 프로젝트를 맡는 인력에 집중해서 작품의 질을 가늠하는 시대, Donald Glover가 맡은 Lando 영화는 기대하지만 그 외에 Star Wars 작품 중 기대되는 건 딱히 없는 상태
     * Andor의 제작 과정을 다룬 이번 인터뷰를 중심으로 이야기, Nuyens가 여러 기법과 툴을 조합해 사용했다는 점을 자주 언급, '요즘 CGI가 다 한다' 혹은 'CGI는 가짜 같다, 실사효과가 낫다' 식의 이분법과 달리 실제로는 다양한 접근이 혼합되는 과정, 실제 세트를 만들고 CGI로 보강하거나 그린스크린, 유화 배경, LED 스크린 등 상황마다 다르게 활용, 각 분야 팀 간 협업이 필수적이었던 현장 분위기, 장인정신으로 손수 맞춰가는 방식을 느낄 수 있었음, 다만 이렇게 시간이랑 돈을 투자하는 방식이 과연 '가성비' 관점에서 지속될 수 있을지 걱정, 최근 영화 촬영 현장에서 무선 장비 사용이 일반화된 것도 흥미로운 변화
          + 여러 인터뷰에서 등장인물이 실제로 소품을 만질 수 있도록 세트 곳곳에 작동 가능한 소품을 배치했다는 얘기가 인상 깊었음, 배경 인물들도 몰입감을 느끼도록 신경 쓴 모습, 많은 소품은 실제 화면에 등장하지 않고 캐비닛이나 상자 안에 들어 있는 등 세심한 배려
          + Andor의 세트는 정말 압도적, 상당수가 실제로 제작된 실물 세트라 제작비가 많이 들었을 것으로 예상, 굳이 이렇게까지 할 필요는 없었겠지만 이런 선택에 감사한 마음
          + 렌즈와 광학에 대해 전혀 모르지만 화면 테두리 부분이 독특하게 흐리게 보이는 현상이 흥미로웠고, 이런 표현이 의도된 스타일적 선택인지 궁금
          + 대형 실사 세트와 CGI의 조합이 해적 영화인 Pirates of the Caribbean 3부작의 특징이었고, 이 시리즈가 지금 봐도 최고의 영화 중 하나라는 사실 강조, 최신 UE 기반 콘텐츠보다 훨씬 멋진 영상미
     * Andor를 보고 가장 놀랐던 점은 스톰트루퍼를 진짜 무시무시한 정예 부대로 묘사했다는 점, 기존 Star Wars에서 단순한 코스프레 집단에 가까웠던 캐릭터를 한 단계 끌어올린 부분이 인상 깊음
          + 기존 영화에서는 주인공이 항상 운 좋게 빠져나가고 적들은 제대로 조준도 못 하는 어설픈 캐릭터였는데, Andor에서는 제국이 정말 똑똑하고 치밀하며 위협적이고 무서운 조직으로 묘사, 이런 강렬한 분위기 속에서 몰입하면 제국이 거의 섬뜩하게 느껴지는 수준, 즐거움을 주는 작품을 선호하는 사람 입장에서는 이런 어둡고 무거운 분위기가 개인적으로 부담스럽게 느껴질 수 있기에, Andor가 감명 깊기는 하지만 한 번만 볼 것 같음
          + 보안용 드로이드 역시 무서웠음, 계산적으로 바라보는 눈동자와 거대한 유인원 같은 자세, 사람을 쉽게 해칠 수 있는 힘, 기존 영화의 코믹한 배틀 드로이드와는 전혀 다른 위협적 존재
          + Deathtrooper라는 스톰트루퍼 특수부대의 등장이 인상적이라는 의견
     * Andor 촬영이 아름답지만, 최근 많은 드라마들과 마찬가지로 화면이 지나치게 어둡다는 문제 지적, 영화관 상영이 아닌 스트리밍 전용임을 감독들이 인지하고 적정 화면 밝기를 신경 쓸 필요, 일반 가정에서 밝은 환경에서도 보이는지 확인해야 하며, '노멀 에디션'식 밝기 버전도 제공하길 희망
          + 본인은 오히려 Andor가 최근 드라마 치고는 어둡지 않아서 신선했다고 느낌, 어두운 장면은 있지만 전체적으로 심하게 어둡진 않다는 인상
          + 나의 경우 제대로 된 OLED TV로 볼 때 색감이 회색에 가깝고, 명암이나 컬러가 부족해 전체 미장센이 평범해 보였음, 감독의 의도대로 극장용 품질을 기대했으나 스트리밍 저화질에 맞춘 듯한 색보정이 아쉬움, 기사 속 촬영 현장 사진과 극 중 이미지를 비교해봐도 실물이 훨씬 멋있게 느껴짐, 마치 ""구글 픽셀폰 촬영"" 스타일처럼 흐릿, 흑백 계조가 HBO식 화면과 정반대 방향
          + TV의 다이내믹 톤 매핑을 켜거나 명암을 조절해보라는 팁, 이런 설정을 유지하는 게 오히려 다이내믹 레인지를 보전하는 데 더 좋다고 봄, 소위 '라운드니스 워'에 굴복하지 말자는 견해
          + OLED TV에서 시청 환경이 답답한 건 매우 공감, 뮤지션이 곡을 자동차 오디오로 청취하듯 감독들도 집에서 실제로 확인해야 한다는 공감대
          + 본인이 HDR 지원 디스플레이가 없어도 HDR 설정이 잘못됐을 가능성, 동영상 플레이어 한계 때문일 수도 있고, MKV 등 소스에 따라 HDR 전용 버전과 톤 매핑 설정이 중요, Disney Plus로 시청하니 정상 출력됨
     * Andor를 아직 안 봤지만 SF 장르에 조금이라도 개방적이라면 꼭 볼 가치가 있다고 권유, 대본, 연기, 촬영 모든 면에서 최근 몇 년간 최고의 TV 드라마 후보
          + Andor가 프랜차이즈의 실질적인 시작이었다면, 그리고 거기서 Rogue One과 새로운 삼부작이 나왔으면 Star Wars가 단순히 고전 명작을 넘어 걸작으로 인정받을 수도 있었다는 생각, 오리지널 트릴로지도 좋지만 Andor 스타일/연기가 접목되면 최고의 영화가 됐을 거란 아쉬움
          + Star Wars가 진짜 SF인가에 대한 의문, Jurassic Park가 생물 다큐가 아닌 것처럼 Star Wars도 SF라고 부르기 어렵다는 평
          + 여기서 말하는 SF적 매력은 'SF적 배경'이지 장르 자체가 아니라는 점, Andor는 본질적으로 SF가 아니라 정치 스릴러에 더 가까움
          + 사운드트랙도 훌륭, 오프닝마다 연주를 변주해주는 방식이 인상적
     * Andor가 약간 과대평가되고 있다는 생각, 작품 자체는 완성도 높고 제국 내부 묘사가 특히 좋았지만 오리지널 영화에 비해 낫다고 보긴 어려움, 당시 기술적 한계와 시대적 맥락도 고려해야 하며, 오리지널에 더 기억에 남는 요소가 많고 악당 캐릭터와 음악도 더 위대하다고 느낌, 무엇보다 Andor는 오리지널 영화가 존재해야 더 강력한 효과, 배경 설명이 없으면 이야기의 힘이 약화될 수 있다는 점 지적
          + Andor는 충분히 좋은 드라마지만, 소모된 프랜차이즈에 속했기에 더 높은 평가를 받는 경향 존재
          + 향수와 당시 의미를 생각하면 오리지널 3부작이 위대하지만, 다시 보면 Andor가 거의 모든 면에서 더 높은 완성도를 보임
          + Andor가 뭐가 좋은지 전혀 모르겠다는 입장, 탈옥 에피소드 하나만 좋고 정글의 아이들 회상, 장례식 악기 씬 등은 지나치게 우스꽝스럽게 느껴졌다는 평
     * Andor와 Rogue One이 프랜차이즈 최고의 작품, 깊이와 완성도, 오리지널 영화와의 연결성을 50대 팬도 인정할 만한 수준, 같은 팀이 황제의 몰락까지 이어질 수 있도록 시리즈가 계속되었으면 하는 바람, 오리지널 삼부작 시점에서 3시즌쯤 더 진행되면 '제국의 흥망성쇠'를 깊이 있게 그릴 수 있을 것, Skywalker와 Jedi는 배경 정도로만 유지하고 그 시대 Alderaan, 데스스타 건설 현장 등을 다룬 스핀오프도 기대
          + 제국의 흥기를 그리는 과정이 현대 사회와 닮아 데자뷰, 무너져 가는 의회 시스템과 권력을 잡는 독재자의 이야기
     * Andor가 오리지널 이후 Star Wars 시리즈 전부를 부정하는 작품처럼 느껴진다는 평가, 현실성 있는 파시즘을 기반으로 진짜 위협적이고 설득력 있는 반란의 분위기를 재현, 광선검이나 제다이 마술 등 신비주의적 요소 없이도 본질적인 Star Wars를 잘 담아냄
          + 광선검이 뭐가 문제냐는 반론, 특수 효과 및 무기로서의 활용도 짚으며, 힘(The Force)은 자신의 취향엔 너무 판타지적이고 연약한 마법이라는 인상
          + Andor에도 소소하게 제다이/포스적 장면이 존재하지만 '힘' 사용자들이 일반인 앞에 나타났을 때의 생경한 반응을 아주 현실적으로 연출했다는 호평
     * 대부분이 쇼 자체에 집중하고 기사에 대한 언급이 부족하다는 지적, 쇼를 보지 않은 입장에서는 기사 읽기가 어렵고 이미지와 텍스트간 연결이 없는 것에 답답함, 모든 이미지에 'Cinematography of “Andor” by Christophe Nuyens'라는 같고 형식적인 레이블이 붙고, 세트와 렌즈 등에 디테일하게 다룬 인터뷰 내용과 관련 없이 이미지가 자리만 채우고 있다는 불만으로 결국 중도 포기
          + 실제로 사진들은 Disney 제공 프로모션 이미지였을 것이며, 스틸컷으로도 퀄리티가 뛰어나기 때문, 기사 자체는 괜찮았으니 인정받을 만함
          + 나도 이미지가 본문과 큰 연관 없어서 ‘벽돌 텍스트’ 느낌을 줄이려 무의미하게 끼워넣은 걸로 봤지만 신경 쓰지 않고 읽기에 특별히 어렵진 않았음, 배경 지식이 없다면 어떤 이미지를 넣어도 맥락을 파악하기 어렵다는 점, 실제 문맥상 이보다 나은 서식은 없으니 기사 포맷은 무난하다고 생각
"
"https://news.hada.io/topic?id=21201","php-node - Node.js내에서 PHP HTTP 요청을 처리하는 핸들러","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              php-node - Node.js내에서 PHP HTTP 요청을 처리하는 핸들러

     * Node.js 애플리케이션 내에서 PHP 애플리케이션을 동일 프로세스에서 실행하여, 네트워크 연결 없이 Node.js와 PHP 간 통신을 가능하게 하는 모듈
          + 양방향 통신, 하이브리드 아키텍처, 레거시 PHP 자산 활용 및 마이그레이션 등이 쉬워짐
          + 이를 통해 WordPress를 Next.js 프론트엔드와 통합하는 등 다양한 하이브리드 웹앱 아키텍처를 구현할 수 있음
     * 비동기 방식(handleRequest) 과 동기 방식(handleRequestSync) 모두 지원(동기 방식은 Node.js 스레드 블로킹)
     * x64 Linux, x64/arm64 macOS 환경을 우선 지원하며, 일부 PHP 관련 시스템 라이브러리 설치 필요
     * Request/Response/Headers 등 HTTP 객체를 Node.js에서 직접 생성/조작하여 PHP 애플리케이션과 자연스럽게 연동 가능함
     * 사용방법
// `Php` 인스턴스 생성 후 `handleRequest()`로 PHP 요청 처리
// `Request`, `Response` 객체를 활용해 직접 HTTP 요청/응답 데이터를 다룸
import { Php, Request } from '@platformatic/php-node'

const php = new Php()
const request = new Request({
  url: 'http://example.com/foo/bar',
  headers: { 'X-Test': ['Hello, from Node.js!'] }
})
const response = await php.handleRequest(request)
console.log(response.body.toString())

     * 활용 예
          + PHP 기반 CMS(WordPress 등)를 Node.js(Next.js 등) 프론트엔드와 통합하거나, 레거시 PHP 앱을 점진적으로 Node.js로 마이그레이션
          + 두 환경 간 네트워크 오버헤드 없이 메모리 내 통신 구현 및 API 게이트웨이, SSR, 커스텀 백엔드 연동 등에 적합
     * API들
          + Php 클래스
               o new Php(config): docroot 등 설정 포함 가능
               o php.handleRequest(request): 비동기 요청 처리(Promise 반환)
               o php.handleRequestSync(request): 동기 처리(스레드 블로킹)
          + Request 클래스
               o HTTP method, url, headers, body 지정 가능
               o 각 속성 직접 접근 가능(예: request.method, request.body)
          + Response 클래스
               o status, headers, body, log 등 속성 보유
               o 수동 생성 가능(테스트, 에러 처리에 활용)
          + Headers 클래스
               o set, add, get, getAll, has, delete 등 다양한 HTTP 헤더 관리 메서드 지원
               o entries, keys, values, forEach 등 반복자 API로 헤더 일괄 처리 가능

   그리 매력적인 옵션은 아니네요...

   PHP가 기본적으로 먹는 메모리 크기도 꽤 큰 편이고, 초기화 비용도 꽤 비싼 편이라 적절한 관리가 꼭 필요한데,
   node랑 같은 프로세스 안에서, 특히 지금 구현과 같이 네이티브 모듈에 PHP를 통째로 담아둔 구조에선 부담이 많이 커 보이네요.

   저라면 그냥 php-fpm은 별도의 프로세스로 두고,
   차라리 fastcgi 클라이언트를 js로 작성해 처리할 것 같습니다.

   이 라이브러리가 AWS Lambda의 Node.js 런타임과 잘 호환될까요?

   만약 호환된다면, 궁극적으로는 PHP를 AWS Lambda에 서버리스로 구동할 수도 있을 것 같아요

   Nodejs와 병행해서 쓸 이유가 없다면 람다에 PHP 런타임을 배포하셔도 됩니다 https://bref.sh
"
"https://news.hada.io/topic?id=21180","내 웹사이트가 못생긴 이유는 내가 만들었기 때문임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      내 웹사이트가 못생긴 이유는 내가 만들었기 때문임

     * 누군가가 내 웹사이트를 더 멋지게 만들 수 있었겠지만, 그러면 내 것이 아니었을 것
     * 웹사이트의 개성은 만든 사람의 취향, 집착, 직접적인 손길에서 비롯됨
     * 단순함과 친근함을 추구해 CSS와 JS를 최소화하고, 직접적인 CSS 변형 규칙을 적용해 혼돈과 반복, 서점 같은 감성을 연출함
     * 사이트는 사용자 상호작용에 CSS만으로 반응하도록 설계되어, 꾸밈없는 질감과 예상치 못한 변화가 특징임
     * 웹사이트는 계속해서 변화하며, 소유자와 함께 성장·변신하고, 각자만의 '못생긴 것'이 살아 움직이게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

내가 만든 웹사이트, 그리고 그 못생김의 의미

     * 만약 엄마가 냉장고에 멋진 그림을 붙이고 싶었다면, Vermeer, Lichtenstein, Wyeth 같은 유명한 작가의 그림을 복제해서 붙였을 것임
     * 하지만 엄마는 그런 예술이 아니라, 내가 그린 그림을 원했음

     * 감각 있는 디자이너가 내 사이트를 멋지게 만들 수도 있었겠지만, 그렇게 되면 더 이상 내 것이 아니게 됨

     * 빵을 굽기 위해 어떤 사람들은 밀을 키우고, 소금을 캐고, 효모를 배양하려 하는데, 나는 그렇지 않음
     * 내 미숙한 입맛은 Olive Garden의 브레드스틱을 대량으로 먹는 것에 만족함

     * 이런 취향은 개개인의 소유감에서 비롯되는 차이임
     * 자기만의 이유로, 각자 자신만의 인터넷 공간을 꾸미고 있음

다양한 개인 웹의 존재

     * 어떤 사람들은 태양광으로 구동되는 자가호스팅 웹사이트를 운영함
     * 또 누군가는 Substack을 통해 지혜를 공유하고
     * YouTube에서 제조 현장을 깊이 파는 사람이 있음
     * Gwern은 독특한 무언가를 하고 있음

     * 이 모든 행동은 ""내가 아니면 할 수 없다""는 내적인 동기와 욕구에서 비롯됨
     * 나는 빵이나 서버, 칩을 만들 동기는 없지만, 그 욕구는 유머, 시스템, 소프트웨어, 구조 같은 것으로 표출됨
     * 감정이 너무 강해지면 픽션, HTML/CSS, 허접한 로봇, 슬픈 노래 등으로 터져 나옴

     * 그래서 내 웹사이트는 내 것임

단순함과 개성의 공존

     * 과거에는 단순함과 친근함만을 원했음
          + 노이즈 제거
          + 명암 강조
          + 메뉴 깊이 축소
          + 지루한 HTML
          + CSS 최소화
          + JS 회피 등
     * 2023년 내 사이트는 이랬음 (원글의 이미지 참고)

혼돈의 디자인, 그리고 CSS 이야기

     * 어느 순간 더 넓은 가로 공간을 쓰기로 하면서 문제가 시작됨
     * 초기 계획은 간단했음: ul에 flex-wrap을 적용
     * 그런데 텍스트도 감싸지니 각 링크 구분이 불분명해짐

     * inline으로 읽는 사람도, inline-block으로 느끼는 사람도 있었음
     * 더 많은 공백을 늘려보거나, 각 링크에 테두리를 두르거나, 링크 사이에 도트를 집어넣는 것도 시도했지만 마음에 들지 않았음

     * 그래서 링크에 변주를 주는 방법을 선택함
     * 홈페이지의 혼돈은 사실 몇 가지 단순 규칙에서 나오고 있음
li:nth-child(4n + 0) { transform: rotate(1deg); }
li:nth-child(4n + 1) { transform: rotate(-0.6deg); }
li:nth-child(4n + 2) { transform: rotate(0.5deg); }
li:nth-child(4n + 3) { transform: rotate(-0.75deg); }
li:nth-child(6n + 0) { font-family: Times; }
li:nth-child(6n + 1) { font-family: Helvetica; }
li:nth-child(6n + 2) { font-family: Georgia; }
li:nth-child(6n + 3) { font-family: Times; }
li:nth-child(6n + 4) { font-family: Arial; }
li:nth-child(6n + 5) { font-family: ""Trebuchet MS""; }

     * 원래는 더 많은 변화를 위해 서로소 정수(coprime integer) 를 썼으나, 반복 패턴의 미묘함이 오히려 마음에 들었음

     * 나는 미니멀리즘을 사랑하지만 차가운 느낌은 싫어함
     * ""중고서점"" 같은 따뜻한 감성을 표현하기 위해 몇 가지 CSS 놀라움 요소를 더함
li:nth-child(5n + 2) { font-weight: 100; }
li:nth-child(7n + 2) { letter-spacing: -1px; }
li:nth-child(41n + 31) { transform: rotate(181deg); }

     * 웹은 여전히 인터랙티브 미디어이기에 JS 없이도 커서 움직임에 반응하길 원함
     * 아래 CSS 덕분에 ""잔디를 만지는 감각""을 연출함
li:nth-child(2n + 0):hover { transform: rotate(-2deg); }
li:nth-child(2n + 1):hover { transform: rotate(2deg); }

     * 이로써 커서 움직임에 실시간 반응을 구현
     * 그리고 스크롤잭킹 없이 스크롤의 느낌을 강조하고 싶어서, 텍스처 배경으로 페이지를 '종이'처럼 보이게 만듦
     * css-doodle을 써서 텍스처를 합성
svg {
viewBox: .5 .5 10 10;
fill: #000;
circle*1000 {
  cx, cy: @r(10), @r(10);
  r: @r(.005, .01);
  }
}

     * 라이트 모드에서는 종이의 먼지, 다크 모드에서는 밤하늘의 별 느낌을 표현
     * 2024년 내 사이트는 이렇게 바뀌었음 (원글의 이미지 참고)

변화하는 나, 변화하는 웹사이트

     * 머지않아 내 웹사이트는 아예 다른 모습으로 바뀔 예정
     * 왜냐하면 내가 내 웹사이트의 주인이고, 나 자신도 계속 변화하기 때문임

     * 당신도 변화하게 될 것임
     * 당신의 열정과 가치관은 다른 무언가로 확산될 것임
     * 못생겨 보여도 당신만의 창작물은 계속 살아 움직일 것임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   글쓴이 Taylor Troesh는 taylor.town의 시장이며, scrapscript 저자이자, 허접한 것들을 음미하는 사람임

   들어가봤는데, 정말 난해하군요...

   https://taylor.town/

   글 내용에 깊이 공감합니다. 저도 형편없는 디자인 감각을 숨기기 위해 디자이너 분들이 고심해서 만들어주신 프레임워크들을 쓰면서 애쓰고 있지만, 그럼에도 우직스럽게 여태까지 해오고 있네요.
   매번 더 나은 디자인을 고민하면서, 이것 저것 고쳐가며 만들고 운영하는 재미가 있습니다.
   이러한 활동 덕분에 더 애착도 가고, 재미도 느끼는 것 같네요. ㅎㅎ

        Hacker News 의견

     * 하하, 맞는 말임. 내 주변 개발자 친구들 대부분은 이미 오래전에 Hugo나 Jekyll 같은 기성 템플릿으로 자기 사이트를 옮겼지만, 나는 고집스럽게 CSS도 백엔드도 전부 직접 만든 시스템으로 내 블로그를 유지하는 중임. 나에게는 진짜 즐거움이 '웹사이트를 갖는 것'에 있지 않고, '웹사이트를 만드는 과정'에 있음. 재미있는 부분을 왜 남한테 넘겨야 하는지 모르겠음. 클래식카를 직접 관리하는 것과 비슷함. 그냥 깔끔하고 튼튼한 차를 살 수도 있지만, 그러면 재미가 없음. 목적지가 중요한 게 아니라, 나는 재미를 추구하는 사람임. 회사에서 하루 종일 내가 원하는 디자인이나 기능을 마음껏 정할 수 없는 사이트를 만들고 있으니까, 내 개인 사이트만큼은 내가 완전히 통제하는 자유를 절대 포기할 수 없음
          + 내 웹사이트도 전부 직접 만든 결과물임, 그것도 지난 10년 간 거의 10번씩 새로 만들었음. 매년 뭔가 개편하는 재미가 있음. 홈에 있는 matrix JS 코드(https://oxal.org 클릭하면 깜짝 놀랄 수도 있음), 내가 만든 Static Site Generator genox로 빌드함, CSS 테마도 직접 만든 sakura 사용함, 블로그(https://oxal.org/blog/)에 가면 작은 사이보그가 따라옴(chatgpt로 베이스 이미지를 만들고, Piskel로 직접 스프라이트 애니메이션 작업했음), VPS에 수동 배포함(make로 빌드), 프라이빗한 위치에 파일 업로드하는 쉘 스크립트 다수 운영 중, favicon도 대학생 때 픽셀 아트로 만든 것(링크), 나만의 폰트도 만들어보려다 포기하고 Naruto에서 영감받은 폰트 적용함, 'view-page-source' 기능으로 소스코드를 보면 또 다른 재미를 느낄 수 있음. 내 사이트를 보면 내 소프트웨어 엔지니어 성장의 흔적이
            담겨 있다는 느낌이 들어서 이 단순한 작품이 너무 소중함
          + 내 홈페이지(https://pablo.rauzy.name/)도 전부 처음부터 직접 만들었음. 빌드를 위해 맞춤형 Bash 스크립트와 Makefile만 쓰고 완전 정적인 구조임. JS 한 줄도 없음. CSS 연구하면서 반응형, 모바일 메뉴 등도 직접 구현함. 아마 내가 스스로 루브릭이나 방법론을 몇 개 만들어냈던 것 같은데, 이런 시행착오가 진짜 재미로 이어짐
          + 나도 같은 여정임. Hugo로 시작했다가, 결국 내가 직접 static site generator(Loulou)를 만들었음. 만드는 내내 재미 그 자체였고, 직접 만들어보니 역시 값진 체험이었음. 내 사이트는 여기임
          + '웹사이트를 갖는 것이 아니라 만드는 것이 즐거움'이라는 이 한 문장이 핵심임. 이건 마치 'Journey Before Destination(목적지보다 여정)'이라는 Radiant 기사단의 신념과 닿아있음. 여러 신화 속에서도 반복되는 이야기임. 헤라클레스도 사람이었을 때 멋진 일들을 벌이다가, 신이 되어 그만두게 됐음. 고생과 인간적인 부분을 뺏기지 마라는 메시지임. 만약 속상하면 새소리 음악(https://birdymusic.com) 듣기 추천함. 오늘 본 사이트 중 제일 멋지거나 제일 이상할 수도 있음
          + 그게 목적인 거라면 좋음. 하지만 어떤 웹사이트들은 그냥 간단하게 특정 목적만 달성하고 끝내고 싶어함. 예를 들어 이 사이트처럼 특정 작업만 해주면 됨
     * (원글에 나온) 못생긴 사이트 그림에 블로그 글이 가득하지만, 실제 글은 다른 웹사이트에 올라와 있음. 만약 마케팅 쿠키나 멤버십 팝업이 없다면 차라리 그 못생긴 사이트로 직접 가고 싶었음
          + 스크린샷 보면 hello@taylor.town 메일이 보임. 나도 똑같이 궁금해서 taylor.town(https://taylor.town/)에 방문했음. 그리고 이 글 올리고 나서 사이트 로딩이 엄청 느려졌으니까 아마 해커뉴스 효과(HN hug of death)가 온 것 같음
          + 난 이게 흔한 ‘enshittified web’(품질 떨어진 웹)에 대한 비판 글인 줄 알았음. 이 사이트는 나쁘다 생각하지만 이런 점은 또 웃김: (1) 출력책 판매 배너 있음 (2) 쿠키 동의 팝업 있음 (3) ‘Good Internet’ 헤더가 온갖 안 좋은 현대 웹사이트의 상징 속에서 살짝 보임 (4) 헤드라인이 쿠키 팝업에 가려 읽기 어려움 (5) 쿠키 동의 배너 치우면, 이제는 상시 나타나는 쿠키 세팅 아이콘과 ""+ Become a Member"" 버튼이 있음. 그에 비해 taylor.town이 진짜 좋은 웹임
          + 아마 못생긴 사이트 배경이나 글꼴/색상 선택이 별로여서 읽기 불편하기 때문에 다른 사이트에 글을 올린 것 같음. 예: taylor.town/wealth-000. 나도 개인 사이트는 직접 만들었는데 못생기지 않았음. 저 사람 사이트는 본인이 일부러 못생기게 만든 거라고 생각함, 약간 자기만족에 빠진 느낌임
     * '못생겼지만 흥미로운' 사례와 '못생겼고 지루한' 사이트 사례가 따로 있다고 생각함. 이 사이트는 후자에 가까움. 사실상 그냥 CMS가 마크다운 폴더를 뱅글뱅글 돌며 타이틀을 링크로 뿌려주는 것 같음. 정보 구조가 아예 없고, 카테고리, 아이콘, 이미지, 날짜도 없어서 전부 똑같은 비중으로 보여짐. 단지 '특이하게' 보이도록 배열된 게 다임. 대부분의 개발자 블로그는 검색엔진에서 트래픽이 들어오니까, 홈페이지 디자인이 중요하지 않을 수도 있지만, 만약 독자들이 적극적으로 탐색하게 아키텍처를 만들고 싶었다면 시도와 결과가 좀 아쉬움. 기본 Ghost 템플릿 쓰는 블로그와 결국 별 차이 없는 느낌임
          + 그럴 필요 없음. 중요한 건 저자가 독자를 만족시키려고 만든 게 아니라 본인을 만족시키려고 만든 결과임. 그래서 외부 의견은 그리 중요한 포인트가 아님
          + 저자의 메시지 핵심을 놓친 것 같음
     * 프레임워크로 웹사이트 만들어도 남들하고 똑같은 모양과 느낌만 나와서 아무 재미도 없음. 사업 목적이면 이해하지만, 자신이나 작업물을 보여주려는 거라면 개성 있는 웹사이트가 더 의미 있다고 생각함. SEO나 리텐션에는 비효율적이더라도 나만의 개성을 담는 게 좋아보임. 요즘 웹사이트 빌더들은 너무 구조적이고 천편일률적으로 바뀌어서 싫음. 예전 Geocities나 Freewebs 시절, 읽기 힘든 배경 이미지, 자동 재생 음악, 마우스 따라다니는 커서, 불꽃 앞에서 돌아가는 해골 같은 거 정말 그립게 느껴짐
     * 이 철학이 인터넷 초창기 느낌 그대로여서 너무 좋음. 플래시 사이트에 이상한 네비게이션이나 문제점들이 있었기에 비판도 많았던 시절이지만, 그런 단점 외에도 독특하게 사이트를 만드는 창의력을 존중함. 요즘엔 인터넷이 너무 획일화되었고, 앞으로 AI 생성 컨텐츠 때문에 더 심해질 거라 봄. 규격화되지 않은 코너들을 보는 게 반가움
     * 이런 철학에 완전 공감함. 내 웹사이트는 온전히 ‘나’라는 사람을 표현함. 누가 못생겼네, 너무 비전문적이네라고 말해도 본인은 이런 스타일이 만족스러움. 웹에서, 그리고 세상에서 이런 비순응적 태도가 더 많아지길 바람
     * 글 정말 잘 읽었음. 과거의 ""old web""이 좋았던 이유는 뚜렷한 규격이 없어서 사람들이 각자 실험적으로 무언가를 만들어갔기 때문임. 좀 더 혼란스럽긴 했지만, 진짜 멋진 개성 있는 사이트를 우연히 발견할 때의 희열이 있음. 요즘 웹은 너무 구조적, 공식적인 느낌이고, 대부분이 똑같은 템플릿과 프레임워크로 만들어져서 예측 가능한 소비 공간이 되어버림. 탐험의 재미가 사라짐
     * 2023년의 그 사람 웹사이트는 못생기지 않았고 미니멀리즘 느낌이었음. 지금은 정말 못생겨졌음. 처음에는 2023년 버전인 줄 알고 내용에 동의했는데, 진짜 못생긴 버전을 보니 오히려 메시지 자체에 부정적 태도가 듦
          + ‘지금 정말 못생겨서 메시지가 달라보인다’는 게 이해가 잘 안 됨. 이 글의 전체 메시지는 남의 시선과 무관하게, 자기 자신이 좋아하고 즐기면서 원하는 대로 만드는 데 의미가 있음. 많은 사람들이 못생겼다고 하는 이 자체가 사이트와 메시지를 오히려 더 매력적으로 만든다고 생각함. 외모만 붙잡고 아쉽다고 느끼는 순간 진짜 포인트를 놓치는 것임
          + 예전 디자인은 깔끔하고 미니멀하고... 특별한 게 없었음. 지금은 의도적으로 어질러진 혼돈임. 못생겼든 아니든 강렬하게 기억남음. 단점도 있긴 함(특정 링크를 나중에 다시 찾기 어려움) 하지만 그건 그 사람한테 중요하지 않음. 정말로 '불편함', '불협화음', '호기심', 그리고 무엇보다 '/나만의 것/'을 원한 것임
     * 아직 개인 사이트는 없음. 만들 때는 HTML+CSS+JS 또는 JQ로만 제대로 해볼 생각임. 아파치나 nginx 같은 웹서버 고려 중임. AWS 프리티어나 공유호스팅에 올릴 생각임. div 정중앙 맞추는 방법만 터득하면 바로 시작할 수 있을 듯함
          + AWS 프리티어에서 S3+cloudfront 조합으로 1년 동안 0원으로 운영 중임. 아마 이게 최고의 가성비임. 내 사이트는 HTML+CSS 단일 페이지, ChatGPT가 템플릿 만들어줘서 그대로 썼음. 모바일/데스크탑 모두에서 동작하게 만드는 실력이 안돼서 그냥 이렇게 만족함
          + 몇 분만 투자해서 Neocities에서 바로 시작하는 방법 추천함
          + div 중앙 정렬이 왜 밈이 됐는지 잘 모르겠음
  width: 60%; // 원하는 만큼 너비 지정
  margin: 0 auto;

            이제 블로그 시작하면 됨
          + 나도 똑같이 했었음 : domi.work 나도 못생기게 만들었음 :)
          + 나도 비슷함, 여기에 11ty 얹어서 static build하고 netlify pages로 배포함
     * 깔끔한 템플릿으로 만든 사이트는 다 비슷하게 생겼지만, 직접 만든 사이트마다 세상에 하나뿐인 ‘구석구석의 이상함’이 있음. 개인 프로젝트라면 Tailwind 랜딩페이지 대신 ‘와비사비 HTML’을 택하겠음
"
"https://news.hada.io/topic?id=21213","노안 때문에 업무용 안경으로 구매한 자이스 오피스 렌즈 사용 후기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  노안 때문에 업무용 안경으로 구매한 자이스 오피스 렌즈 사용 후기

     * 노안으로 도수를 한 단계 낮춘 단초점 렌즈를 사용
     * 다시 조금씩 불편해져서 기능성 렌즈를 시도함
          + 책이 선명하게 보이는 영역이 좁음
          + 4~5줄 정도
     * 오피스 렌즈를 맞춤
          + 다초점렌즈와 다르게 원거리가 없음
          + 중거리와 근거리 초점만 있어서 모니터를 편하게 볼 수 있음
               o 다초점렌즈보다 모니터 거리에 더 많은 영역을 안경 렌즈에 할당하기 때문
     * 오피스 렌즈에 만족
          + 안경을 하나 더 들고 다니는 불편함만 감수하면 됨
     * 현재는 단초점 + 오피스 렌즈 조합
          + 단초점을 누진다초점 렌즈로 바꿀까 고민 중

   선배(?) 로써 한 말씀 드리면..
   누진다초점 렌즈를 혹시라도 나중에 사용하시게 된다면...
   이것은 Case by Case 이지 않을까 생각됩니다.
   제 주위에 누진다초점 렌즈 하신 분들 대부분 적응을 하지 못하는 경우가 많았어요.

   2020년에 누진다초점 렌즈로 했고..
   최근 2025년 5월에 새롭게 다시 했습니다.
   동네 친절한 안경점에서 했는데.
   처음에는 적응하기가 매우 어려웠습니다.

   노안을 경험해보지 못한 젊으신 안경점 사장님께서
   이론으로 공부하셔서 누진다초점 렌즈를 할경우에 대해서
   발생할 수 있는 현상들에 대해서 사전 고지 해줌에도 불구하고

   . 계단 오르거나 내려갈때 수평이 안 맞을수 있고
   최소 2주정도는 적응기간이 필요하다.
   2주동안에 적응이 되지 않으면 다시 새롭게 해줄수 있다고..

   하나의 렌즈에 원거리, 근거리, 스마트폰 볼 거리를 다 만족시킬려는
   것이다 보니..
   3차원 공간에서 표시되는 영역들이 왜곡되어 나의 수정체를 억지로
   끼워 넣는 그런 느낌이 듭니다. 눈도 아프고 머리도 아프고..

   대신 스마트 폰의 글씨는 또렷하게 잘 보이구요.
   모니터도 그냥 적당하게 잘 보입니다.
   먼 거리도 그냥 적당하게 잘 보입니다.
   누운 상태에서 TV를 보면 초점이 맞지 않습니다.
   그리고 시야의 Center는 초점이 잘 맞지만 눈알을 좌우로 돌리면 초점이
   맞지 않아요.
   흠.. 그냥 적응하다 보면 익숙해집니다.

   2020년에 맞춘 렌즈는 7단계 레벨로 초점을 맞출수 있다고 해서
   했었는데.. 처음 한달정도는 적응하기 힘들었고 5년 정도는 그냥 눈이
   익숙해져서 잘 쓰고 다녔고 주위에 추천까지 하고 했습니다.

   최근에 안경이 부러져서 새롭게 다시 누진다초점 렌즈로 했는데.
   기존 것보다 한단계 조금 더 비싸고 레벨도 8단계로 했는데..
   적응하기가 힘듭니다..
   모니터 글자 초점이 안 잡힙니다. --
   Wide 모니터를 center는 잘 보이는데.. 구석에 있는 글씨들이 초점이 맞지 않는 --
   8단계로 해서 그런가 싶기도 하고... 머리를 아래 위로 까닥하면서 초점을
   맞추는 모습이 우습기도 합니다.

   안경점에 다시 방문하니 오피스렌즈 이야기 하더군요...
   1m~2m 정도만 잘 보이는 그런...
   흠.. 비싸 가격으로 안경을 바꿨는데... 또 오피스렌즈를 해야 된다고 해서
   슬펐습니다.

   아무튼.. 2번째로 바뀌진 2주정도 쓰고 있는데.. 힘들게 적응중입니다.

   이제 주위분들이 누진다초점 렌즈로 한다고 하면
   신중하게 추천할 생각입니다.

   흠.. 노안때문에 이제 은퇴를 해야 되나 심각하게 고민중입니다. ㅎㅎㅎㅎ

   말씀 듣고 보니 단초점 + 오피스렌즈로 시작한 게 다행인 것 같습니다. 나중에 다초점렌즈를 하게 된다면 후기를 공유하겠습니다. 다행히 부적응자는 단초점 렌즈로 다운그레이드하고 차액을 돌려준다고 하더라구요.

   저도 오피스렌즈 사용하면서 모니터 보는 게 아주 편해졌습니다.
"
"https://news.hada.io/topic?id=21178","MCP를 통한 지식 그래프와 LLM 연동","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         MCP를 통한 지식 그래프와 LLM 연동

  LLM 한계를 넘는 기술 조합 : 지식 그래프 × MCP × 에이전트

   대형 언어 모델(LLM)은 강력하지만, 최신 정보나 도메인 특화 지식에는 약합니다.
   이를 보완하기 위해 검색 증강 생성(RAG), 에이전트, 그리고 최근 급부상하는 MCP(Model Context Protocol) 와 지식 그래프(Knowledge Graph) 가 주목받고 있습니다.

   이 블로그에서는 LLM의 추론 능력을 강화하는 방향으로 지식 그래프를 MCP와 연계하는 방식을 다루며, 실제 시스템에서 어떻게 활용되는지를 설명합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    주요 내용 요약

     * 지식 그래프란?
          + 개체/관계/속성 기반의 구조적 지식 표현 방식
          + 추천 시스템, 질의응답, 문서 검색 등에 활용
     * MCP란?
          + LLM이 외부 시스템(도구, 리소스)과 통신하는 표준화된 인터페이스
          + 다양한 툴을 LLM 기반 AI 에이전트가 자동으로 호출 가능
     * 지식 그래프 × MCP 연동 방식
         1. MCP 서버로 연동: 지식 그래프를 툴/리소스로 노출
         2. 에이전트 내부 기억장치로 활용: 여러 MCP 서버에서 받은 정보를 통합해 지식 그래프 형태로 저장, 추론 수행
     * LLM 기반 추론 기법 예시: Think-on-Graph
          + LLM이 지식 그래프를 탐색하며 다단계 추론
          + 예: “캔버라가 있는 나라의 여당은?” → 지식 그래프 탐색 → 최종 답변 도출
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    실무 포인트

     * 단순 문서 RAG를 넘는 관계 중심 추론
     * 도메인 지식 내재화를 위한 지식 그래프 기반 에이전트 설계
     * MCP 기반 연동으로 LLM을 확장 가능한 API 소비자로 활용

     LLM + MCP + Knowledge Graph 조합은 앞으로 에이전트 기반 AI 시스템의 핵심 아키텍처가 될 것으로 보입니다.

   전체 기술블로그 보기

   지식 그래프 란게 몇십년전의 Symbolic 방식을 다시 들고 나온 느낌 이네요.
"
"https://news.hada.io/topic?id=21227","Bash로 구현한 MCP 서버","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Bash로 구현한 MCP 서버

     * MCP Server in Bash는 MCP 서버를 Node.js, Python 없이 오직 Bash와 jq만으로 구현한 초경량 오픈소스
     * 복잡한 러닝타임 없이, 완벽한 JSON-RPC 2.0 기반의 MCP 프로토콜을 stdio로 처리하며, 함수 네이밍 규칙을 이용해 동적으로 툴(기능) 확장이 가능
     *
          + 설정 및 기능 정의를 모두 JSON 파일로 관리할 수 있어, 간단하게 각종 커스텀 MCP 서버를 빠르게 만들 수 있음
     * VS Code, GitHub Copilot Chat 등에서 별도 언어 환경 없이 연동 가능
     * 대부분의 MCP 서버가 무거운 API wrapper에 불과한 반면, 이 프로젝트는 로컬 자동화/AI 에이전트 도구로 적합한 실용적 접근 제공
     * 한계 :
          + Bash 기반으로 동시성, 메모리 관리, 스트리밍 응답 미지원
          + 대량 요청/고성능 실시간 처리에는 부적합, 하지만 AI 보조/로컬 도구 목적에는 충분

커스텀 MCP 서버 제작 방법

    1. 비즈니스 로직 쉘 스크립트 생성(weatherserver.sh 등)
          + 예: tool_get_weather, tool_get_forecast 함수 구현
          + 외부 API 호출, 결과 JSON으로 반환
    2. assets/tools_list.json에서 각 툴 파라미터, 설명 정의
    3. mcpserverconfig.json에서 버전, 서버명, 기능 정의
    4. 파일 실행 권한 부여 후, stdio 통해 명령 송수신
"
"https://news.hada.io/topic?id=21229","HTTP API 에러 처리의 어려움과 RFC7807","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      HTTP API 에러 처리의 어려움과 RFC7807

   HTTP API를 개발할 때 에러 처리는 종종 번거로운 부분입니다. API 수가 많아지고 내부 로직이 복잡해질수록 세 가지 측면에서 어려움이 발생합니다.
     * 적절한 에러 코드 반환: 경험이 부족한 개발자의 경우 복잡한 로직에서 일관된 HTTP 상태 코드를 사용하기 어렵습니다.
     * 많은 양의 결과 로그 작성: 에러 발생 시 예상되는 모든 종료 지점에 로그를 기록하는 것은 코드 양을 늘리고 관리를 복잡하게 합니다.
     * 명확한 에러 메시지 전송: 클라이언트에게 단순히 에러 메시지를 전달하는 것만으로는 에러를 명확하게 이해하고 처리하기 어렵습니다.

  적절한 에러 코드 반환 개선

   에러 코드 사용의 일관성 문제를 해결하기 위해 StatusCode와 Message를 포함하는 HttpError 인터페이스 또는 구조체를 구현하는 방법을 제안합니다.
     * 해결책:
          + HttpError 타입 정의: HTTP 상태 코드와 메시지를 캡슐화.
          + 헬퍼 함수 제공: httperror.BadRequest(""wrong format"")와 같이 특정 에러 코드를 반환하는 헬퍼 함수를 사용하여 에러 객체를 쉽게 생성.
     * 장점:
          + IDE의 자동 완성 기능을 활용하여 편리하고 안정적으로 에러 코드와 메시지 입력.
          + 수동으로 숫자 코드를 입력하는 것보다 오류 발생 가능성 감소.
          + 사전에 준비된 설계 문서를 일일이 확인하는 번거로움 감소.

  로그 작성 중앙화

   반복적인 로그 작성을 줄이고 에러 처리 로직을 한 곳에서 관리하기 위해 HTTP 핸들러를 래핑하는 방법을 제시합니다.
     * 해결책:
          + 커스텀 라우터(chiwrap.Router) 구현: chi.Router와 같은 기존 라우터를 내부에 포함하고 에러 처리 로직을 추가합니다.
          + 핸들러 래핑: 커스텀 라우터의 Get 등의 메서드는 HandlerFunc를 받아서 내부적으로 실행하고, 에러가 발생하면 중앙 처리 로직으로 전달합니다.
          + 에러 콜백 함수: NewRouter 생성 시 errCallback 함수를 받아 에러 발생 시 해당 콜백을 호출하여 중앙에서 로그를 기록하거나 추가적인 처리를 수행합니다.
     * 장점:
          + API 로직에서 에러 발생 시 자동으로 적절한 에러 코드와 메시지가 응답으로 반환.
          + 서비스별로 적절한 로그를 기록하도록 콜백 함수를 등록하여 로그 관리가 용이.
          + 코드 중복을 줄이고 유지보수성 향상.

  명확한 에러 메시지 전송 (RFC7807 활용)

   클라이언트가 에러를 더 명확하게 이해하고 처리할 수 있도록 RFC7807 표준을 활용한 구조화된 에러 메시지 전송 방안을 제안합니다.
     * RFC7807 주요 요소:
          + type: 에러 유형을 식별하는 URI (예: https://example.com/errors/validation).
          + title: 에러에 대한 짧은 한 줄 설명.
          + status: HTTP 상태 코드와 동일.
          + detail: 사람이 읽을 수 있는 상세 에러 설명.
          + instance: 에러가 발생한 특정 URI (예: /api/users/abc).
          + extensions: 추가 정보를 담는 JSON 객체 (예: invalid_field, expected_format).
     * 구현:
          + RFC7807Error 구조체 생성 및 주요 요소 포함.
          + 메서드 체이닝 패턴(WithType(), WithInstance(), WithExtension())을 통해 쉽게 구조화된 에러 객체 생성.
          + ToHttpError() 메서드를 통해 RFC7807Error를 HttpError로 변환하여 중앙화된 라우터와 연동 가능.
          + 클라이언트가 에러의 종류, 원인, 발생 위치 등을 명확하게 파악 가능.
          + API 응답의 일관성과 유용성을 높여 클라이언트 개발 효율성 증대.

   좋은 글 감사합니다

   좋은 글 감사합니다!
   참고로 스프링에서는 spirng-web 라이브러리 > org.springframework.http.ProblemDetail에 구현체가 존재합니다!

   좋은 소개 감사합니다!
   찾아보니 RFC 9457로 대체됐네요.

   https://datatracker.ietf.org/doc/html/rfc9457
   (기존 7807 문서: https://datatracker.ietf.org/doc/html/rfc7807)

   RFC 7807과 RFC 9457의 주요 차이점
     * 문제 유형 관리: 7807은 커스텀 URI만 사용 가능, 9457은 IANA 공유 레지스트리 도입
     * 다중 오류 처리: 7807은 HTTP 207 상태 코드 사용 권장, 9457은 단일 문제 유형 내에서 errors 배열을 사용해 관련 오류를 그룹화
     * 확장 필드: 7807은 임의 필드 추가 가능, 9457은 문제 유형별 예상 필드 명시적 연관
     * 보안 권고: 7807은 미포함, 9457은 보안 취약점 방지를 위한 명시적 지침 추가
     * JSON 포인터: 7807은 미지원, 9457은 pointer 필드 공식 지원

   2023년 7월 이후 신규 프로젝트에서는 RFC 9457을 적용하는 것을 권장

   type 필드는 역참조 가능한 URI로 설정하는 것이 권장되는 것으로 보여요.

   내부 서비스에서는 Swagger-ui 문서 링크로 대체해도 무방할 것 같습니다.
"
"https://news.hada.io/topic?id=21186","WeatherStar 4000+: 웨더 채널 시뮬레이터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     WeatherStar 4000+: 웨더 채널 시뮬레이터

     * WeatherStar 4000+ 는 The Weather Channel의 옛날 기상 정보 방송을 웹에서 재현하는 오픈 소스 시뮬레이터임
     * 유저는 실제 위치를 입력해 과거 TV의 지역 기상 정보 서비스를 실시간으로 체험 가능함
     * 인터페이스는 실제 WeatherStar 4000 하드웨어와 소프트웨어 디자인을 충실히 복원함
     * 기상 데이터는 현대 API에서 가져와 구식 스타일로 표현함
     * 레트로 감성 영상, 애니메이션, 음악 등 오리지널 장치 경험을 다양하게 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

WeatherStar 4000+ 프로젝트 소개

     * WeatherStar 4000+는 1990년대 The Weather Channel에서 사용했던 WeatherStar 4000 기기와 그래픽 인터페이스를 현대 웹 환경에서 재현하는 프로젝트임
     * 사용자는 본인의 위치 정보를 제공하거나 직접 입력하여 해당 지역의 기상 예보, 레이더, 주간 전망, 경보 등을 당시의 TV 스타일로 확인할 수 있음
     * 실제 WeatherStar 4000과 유사하게, 텍스트 애니메이션, 그래픽 효과, 배경 음악, 전환 효과 등이 구현되어 원본 경험을 충실히 모방함

주요 특징

     * 전국 및 지역별 날씨 데이터를 현대의 기상 API에서 불러와 그래픽화
     * 오리지널 WeatherStar 4000 기기와 동일한 사용자 경험을 목표로 레이아웃과 UI 구성
     * 80~90년대 레트로 디자인과 사운드트랙, 타이포그래피 등을 포함
     * 경보, 기상청 예보, 강수량, 풍속, 기온 그래프 등 세부 정보 제공

기술적 구현 및 오픈 소스 가치

     * 본 프로젝트는 완전 오픈 소스로 제작되어 누구나 코드를 참고하거나 기여 가능함
     * 자바스크립트 및 웹 표준 기술을 사용했고, 별도의 설치 없이 웹 브라우저로 접근 가능함
     * 복고풍 기상 정보 전달 형식을 보존하면서도, 실시간 정보와 근래 기상 서비스의 정확성을 결합함
     * 개발자와 커뮤니티의 지속적인 기여, 플러그인 추가를 지원함

프로젝트의 의의 및 차별점

     * WeatherStar 4000+는 단순한 데이터 뷰어를 넘어서, 한 시대의 미디어 경험을 인터랙티브하게 복원함
     * 시각적, 청각적 재현이 동시에 이뤄져, 올드테크, 방송 기술, 그리고 기상 데이터 표현에 관심 있는 이들에게 독특한 경험을 제공함
     * 미국 기상 방송의 역사적인 정보 전달 방식과 레트로 기술 문화에 대한 이해를 높여줌

        Hacker News 의견

     * 만약 이런 것에 관심이 있다면, YouTube에서 WeatherStar 4000(케이블 헤드엔드에서 로컬 날씨 리포트 그래픽을 생성하던 장치)을 구해다가 펌웨어를 다시 써서 90년대 스타일의 날씨 방송을 진짜 하드웨어에서 구현한 사람이 있다는 사실을 공유하고 싶음. 원래 펌웨어는 위성으로 다운로드받은 뒤 이미 사라져서 새롭게 직접 개발해야 했음. 실제 90년대 Weather Channel과 거의 똑같이 보이지만, 해당 채널의 로고가 없는 점이 다름(아마 저작권 문제 때문). 이분의 WeatherStar 스트림 보러가기
          + 만약 이분이 Retro Computing 이벤트에 장비를 가져오는 그 사람이라면, 아쉽게도 이 소프트웨어의 아카이브를 공개하길 거절한 상황. 물론 그럴 의무는 없지만, 하드디스크가 망가지거나 본인이 흥미를 잃으면 소프트웨어 자체가 영영 사라질까봐 커뮤니티가 우려 중인 분위기
          + 중요한 포인트로, 이 프로젝트에서는 Weather Channel 느낌의 백그라운드 음악도 재현
          + 이 사람이 개발 로그에서 어셈블리나 C를 전혀 모르는 상태로 프로젝트를 시작해서, 독학으로 배우며 완성한 이야기임. 정말 대단한 열정과 노력을 느끼게 하는 내용
     * 너무 좋음, 미소가 절로 나오는 순간. 이런 올드테크에서 느껴지는 '따스함'은 추억 그 자체. 예전 평범한 날, 이름 없는 엔지니어가 비에 젖은 구름에 파란 물결을 추가하던 그 순간의 인간적인 감성이 현대에는 사라진 듯해 쓸쓸하면서도 뭉클함을 느끼는 감정
     * 이걸 보니 내가 존재조차 몰랐던 추억의 깊은 부분이 열리는 느낌. 오리지널 포스터에게 환호의 마음
     * 라즈베리파이와 3D 프린팅한 CRT 시뮬레이터 디스플레이로 데스크에 Weather Channel 기기를 24시간 실행하고 있음. 실제 CRT TV로도 시도해봤지만 주파수 문제와 메인 모니터 옆에 두니 몸이 약간 아픈 이슈가 있었음
       셋업 사진
       소프트웨어 GitHub 링크
          + 이것은 EmuVR(https://www.emuvr.net/) 프로젝트에 정말 잘 어울릴 아이템이라 생각
          + 멋진 프로젝트! 그런데 어떤 화면을 사용하는지 궁금함. 찾은 디스플레이가 모두 와이드스크린이라 동일 프로젝트를 위해 참고하고 싶음
     * Pat Metheny Group 음악을 즐겨 듣는데, 아내는 이걸 ""Weather Channel 음악""으로 부름. 난 Pat이 채널 음악보다 훨씬 낫다고 우겼지만, 호텔 TV로 Weather Channel을 틀었을 때 ""Last Train Home""이 나와서 더 이상 아무 말도 못하게 됨
          + ""Last Train Home""은 어떤 의미에서도 진정한 명곡이라는 평
          + Local on the 8s에서 퓨전 재즈를 엄청 틀었는데, 혹시 Weather Report 곡도 나왔던 적 있었는지 궁금
          + 음악이 정말 신기함. YouTube에서 이 곡을 들으니 바로 돌아가신 아버지가 떠올라서 눈물이 났음. 아버지가 Weather Channel을 항상 보시던 기억과 이 스레드를 보며 여러 추억이 밀려든다는 소감. 음악의 힘을 다시 실감하는 순간
          + 현지 예보 텍스트에 텍스트-투-스피치(TTS) 보이스오버도 있지 않았는지 궁금. 음악 대신 누군가가 예보 내용을 읽어주는 파트가 있던 기억
          + 처음 아버지의 Spyro Gyra CD를 들었을 때가 떠오른다는 추억
     * Jeanetta Jones의 근황이 궁금해서 찾아봤는데, 이렇게 됐음을 알게 되어 슬픈 마음
       Jeanetta Jones 추모 링크
     * 진짜 90년대 Weather Channel이라면, 백그라운드에 Rippingtons 음악이 꼭 필요
       Rippingtons 곡 듣기
          + 저 차량으로 저 코너를 어떻게 돌지 궁금한 생각
          + 나는 Weather Channel에서 이 음악을 듣고 Rippingtons 앨범 전부를 다 샀던 기억. 당시에는 Smooth Jazz에 빠져 있었음
          + 난 Pink Floyd의 기악 커버도 기억에 남음
     * WeatherStar 4000+가 뭔지 몰라서 ""Weather Channel 시뮬레이터""가 AI로 실시간 영상 리포터를 생성하는 방식일 줄 알았음. 멀지 않은 미래 느낌
          + 지금 방식이 AI로 만든 결과물보다 훨씬 더 흥미롭고 감동적인 느낌
          + 비슷한 생각을 함! 최근 TTS(텍스트-투-스피치) 기반으로 Pong의 실시간 해설이나 맞춤형 라디오를 실험해왔는데, 이 Weather Channel 시뮬레이터도 한번 시도해보고 싶은 재미있는 아이디어로 느껴짐
     * 이게 원조 버전으로 보인다는 발견
       오픈소스 코드 보기
     * VLC에서 시청할 수 있게 스트림 형태로 띄우는 방법을 아는 사람이 있는지 궁금함.
       HDHomeRun을 활용해서 IPTV 앱으로 안테나 방송을 보는데, 내 TV에서 개인 Weather Channel을 갖고 싶었음.
       예전에 Node로 페이지 스크린샷을 찍은 뒤 ffmpeg로 합치는 방법을 시도했지만, 결과가 엉망이라 잘 동작하지 않았음.
       스트림이 접근될 때만 앱이 자동으로 실행되는 구조도 고민했지만 덜 중요한 문제로 생각.
       어떤 조언이든 환영하는 마음
          + OBS로 충분히 해결 가능하리라 생각.
            OBS에서 이 앱의 윈도우를 선택해서 스트리밍하게 세팅
            OBS의 ""Stream"" 설정에 들어가서 custom으로 변경
            서버는 ""srt://127.0.0.1//… 입력
            VLC에서는 네트워크 스트림에서 srt://127.0.0.1:7777을 열면 됨
"
"https://news.hada.io/topic?id=21212","Microsandbox - 컨테이너처럼 느껴지고 동작하는 가상 머신","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Microsandbox - 컨테이너처럼 느껴지고 동작하는 가상 머신

     * microsandbox는 신뢰할 수 없는 사용자 및 AI 코드 실행을 안전하게 가상 머신 수준의 격리로 제공함
     * 초고속 부팅(200ms 이하), OCI 컨테이너 호환성, 자체 호스팅 등 기존 VM·컨테이너의 단점을 극복함
     * 다양한 프로그래밍 언어용 SDK와 CLI 도구로 개발자 및 AI 툴 통합 효율성 극대화함
     * 코드 실행, 개발 환경, 데이터 분석, 웹 자동화, 앱 호스팅 등 폭넓은 AI·개발 활용 사례에 적합함
     * 모든 작업은 프로젝트 기반으로 관리 가능하며, 시스템 전역 설치 및 세션 유지/격리 실행 환경 지원함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * microsandbox는 신뢰할 수 없는 사용자 코드나 AI 코드 (예: AI 에이전트, 사용자 제출 코드, 실험성 코드)의 안전한 실행을 위해 설계된 오픈소스 자체 호스팅 플랫폼
     * 기존의 로컬 실행은 보안 취약성, 컨테이너는 커널 공유로 인한 불완전한 격리, 전통 VM은 느린 부팅, 클라우드는 유연성 부족이라는 단점이 있음
     * microsandbox는 microVM(초경량 가상 머신) 기반의 진정한 프로세스 격리를 지원하면서도, 컨테이너와 동일하게 빠른 시작 속도와 개발자 친화적 경험을 제공
     * 초기 환경 세팅 후 200ms 이내 부팅, 컨테이너 이미지(OCI) 호환성, MCP 기반 AI 통합, 자체 인프라 사용 제어 등으로 차별화

주요 특징 요약

     * Bulletproof Security: microVM 기반으로, 컨테이너의 취약점(커널 탈출)을 원천적으로 차단하는 가상 머신 수준 보안 제공
     * Instant Startup: 초기 부팅 시간이 200ms 미만으로, VM 대비 극단적으로 짧은 코드 실행 시작 속도 구현
     * Self-Hosting & Full Control: 클라우드 종속 없이 로컬·자체 서버에 직접 구축·운영 가능
     * OCI 호환: 표준 컨테이너 이미지로 그대로 실행 가능함으로써, 기존 도커·컨테이너 워크플로와 호환
     * AI-Ready (MCP 지원) : Claude, Agno 등 MCP 기반 AI와 자연스럽게 통합 및 확장 가능

빠른 개발 및 실행 워크플로

  1. 서버 시작

     * 간단한 명령어만으로 microsandbox 서버 시작 및 개발 환경 구성 가능
     * 서버는 MCP 서버 역할도 겸하여, Claude 등 AI 툴에서 직접 호출 활용 가능

  2. SDK 설치

     * Python, JavaScript, Rust 등 주요 언어별 microsandbox SDK 제공
     * 다양한 추가 언어 지원(SDK 확장성 제공)으로 폭넓은 개발자·AI 통합 가능성 확보

  3. 코드 안전 실행

     * Python, JavaScript, Rust 등 여러 언어 샌드박스 환경을 별도로 선택하여 실행 가능
     * 각 샌드박스는 독립 실행 환경으로, 외부 코드 실행 시에도 시스템 안전성 보장
     * SDK 예제를 통해 비동기·자동화된 안전 코드 실행 프로세스 구현 용이

프로젝트 기반 환경 관리

     * 프로젝트 단위로 Sandboxfile(설정 파일)을 생성·관리하며, 개발자 친화적 패키지 매니저적 워크플로와 유사
     * 다수 샌드박스 환경(예: 서로 다른 언어, 설정)을 프로젝트에 추가해 버전/환경별로 관리 가능
     * 프로젝트 샌드박스 실행 시, 파일 및 설치 변경 내용이 로컬 디렉터리(./menv)에 자동 보존
     * 임시 샌드박스 활성화 옵션(세션 종료 시 모든 기록, 상태 완전 삭제 및 격리)

시스템 전역 샌드박스 설치

     * 자주 사용하는 환경 또는 앱을 별도 실행파일로 설치 및 등록 가능
     * 터미널에서 프로젝트 경로 없이도 한 줄 명령어로 바로 샌드박스 실행 환경 진입
     * 샌드박스별 이름 부여 및 각각 다른 설정 복수 운영, 세션 상태도 계속 유지됨

주요 활용 사례

  AI 코드 실행 및 개발 환경

     * AI가 실질적인 소스코드 빌드·실행·디버깅까지 자동화할 때, 격리·반복 가능한 개발 환경 신속 제공
     * 웹앱 생성·버그 수정·프로토타입 등 코드 자동화에 적합함

  데이터 분석

     * NumPy, Pandas, TensorFlow 등 주요 데이터 과학 라이브러리를 샌드박스 내에서 안전하게 활용
     * 개인정보, 민감 데이터 등 보호가 필요한 분석 워크플로에 이상적

  웹 브라우징 에이전트

     * 웹사이트 탐색, 폼 제출, 로그인, 데이터 크롤링 등 자동화 업무를 AI가 안전하게 수행
     * 컨텐츠 수집, 가격 비교, 자동화 테스트 등 활용에 유용

  인스턴트 앱 호스팅

     * 사용자가 만든 도구, 데모, 계산기, 시각화 등을 즉시 서비스로 공유 가능
     * 각 앱은 별도 격리 공간에서 동작, 임시 환경의 빠른 생성·종료 지원

시스템 구조

     * 사용자는 자신의 비즈니스 로직에서 microsandbox SDK 호출
     * 서버 프로세스(microsandbox server)로 신뢰할 수 없는 코드 전달·실행을 요청
     * 서버 내에서는 각각의 실행 요청이 별도의 microVM에서 수행되어 서로 격리
     * 각 microVM은 독립적 Python/Node 환경 구성 가능

오픈소스 정책

     * Apache License 2.0 하에 오픈소스 배포

        Hacker News 의견

     * 정식 컨테이너 보안 등급을 보고 싶음
         1. 모든 알려진 컨테이너 취약점 목록을 큐레이션
         2. 각 취약점을 퍼미션 기반, jail, Docker, 에뮬레이터 등 여러 보안 환경에서 실행
         3. 전체 익스플로잇 중 몇 퍼센트를 막았는지를 점수로 매기면 좋겠다는 아이디어
            이런 방식이면 단순한 permission이나 jail 기반 컨테이너는 0%에 가깝고, Docker는 50% 이상, Microsandbox는 100% 가까이 나올 수 있지 않을까 기대
            “왜 그냥 jail 안 쓰냐” 같은 질문에 대한 본능적인 궁금증을 해소해 줄 수 있을 듯
            또, 오픈 웹에 honeypot 컨테이너를 띄우고 해킹에 성공하면 현금이나 코인으로 보상하는 식으로 100% 달성 컨테이너가 뭔지 ‘증명’하는 것도 재미있을 듯
            최근 Rowhammer나 Spectre같은 취약점 때문에 컨벤셔널, 클라우드 컴퓨팅 보안 자체를 재정의할 필요도 있음
            결국 완벽한 에뮬레이션 없는 100% 보안 컨테이너 개발과 OS의 기본 서비스 보안화에 대한 인사이트를 얻고 싶은 동기
     * 멀티테넌트 환경에서는 “컨테이너 취약점”이 문제가 아니라, 커널을 공유한다는 근본 구조 때문
       커널 LPE(Local Privilege Escalation) 취약점이란 게 있으면 컨테이너 탈출로 바로 이어지기 때문
       보통 컨테이너 탈출로 표시되지 않지만, 업계에선 커널 LPE가 있다고 하면 당연히 컨테이너 보안은 깨지는 걸로 인식
     * 악의적인 컨테이너에 대해선 리눅스 커널 기반의 컨테이너 런타임으로는 완전히 안전한 환경 구축이 불가능
       가시적인 대안은 샌드박스 내부에서 시스템콜(API) 사용을 대거 제한하는 방법인데, 이 경우 컨테이너가 더 이상 범용 플랫폼이 아니고, 매번 케이스별로 새로 튜닝해야 하는 불편함 존재
       그래서 가상화(virtualization)가 필요하다고 보는 입장
       메모리 세이프하고 단단한 OS가 나오지 않는 한 이 방법밖에 없고, 그런 OS가 나와도 MicroVM을 호스트 리눅스에서 돌리는 것보다 빠를지는 아직 미지수
     * 머신의 설정값도 같이 보여줬으면 좋겠는 바람
       Docker나 systemd에서 각종 설정에 따라 보안 수준이 엄청 달라짐
       어떤 설정이 어떤 위험/보안 수준으로 이어지는지 큰 실험 데이터셋이 필요하다고 생각
     * 사실 컨테이너는 이미 현금/코인 바운티가 걸린 honeypot으로 운영되고 있음
       현실적으로 진짜 운영환경 자체가 수많은 해커의 공격 대상
       이런 식의 인센티브 모델은 재미있을 수 있지만, 실제 해킹 타깃 경쟁력이나 금전적 인센티브는 현실 환경보다 훨씬 작을 듯
     * 전통적인 VM이 왜 부팅(start)이 오래 걸리는지 궁금
       예를 들어 윈도우즈에서 VM 실행할 때 아무 것도 돌기 시작하기까지 몇 초 이상 기다림
       여기서 말하는 “아무 것도 실행 안 한다”는 건 유저 프로그램 실행 전, 펌웨어 첫 명령조차 시작하기 전 상태
       심지어 가상 디스크 파일을 비우는 작업 전, 또는 VM 창이 뜨기도 전에 길게 기다리는 구간이 있음
       왜 그런지 궁금
          + 리눅스 커널을 1초 이내로 부팅하는 건 최적화로 충분히 가능
            하지만 표준 커널 기반일 땐 timeout이나 polling 등 시간이 꽤 걸리는 동작이 많음
            UEFI/CSM 시스템에서 가상 하드웨어 준비, 시스템 환경 초기화 등도 꽤 시간 소요
            WSL2는 불필요 오버헤드 제거용 특수 커널 사용 추정
            각종 OS 서비스 기동, 파일시스템 준비, 캐시 준비, 네트워크 구성 등도 영향
            전통 방식은 부트로더 → initramfs → 메인 OS 각각 따로 로드
            부트 타임을 극단적으로 줄이려면 Amazon Firecracker처럼 미리 초기화한 VM 이미지를 바로 메모리에 올리는 방식 사용
            Firecracker MicroVM 소개
            윈도우에선 HyperV 등 어떤 하이퍼바이저를 사용하느냐에 따라 부팅 속도 다름
            HyperV UEFI는 꽤 느리고, 많은 리눅스 배포판은 최적화된 미니멀 커널 제공하지 않음
          + 어떤 VM 소프트웨어를 쓰는지 더 정보 필요
            VirtualBox의 경우, 질문한 현상이 뚜렷하게 보이고, 예전 버전은 이런 딜레이 없었음
            “전통적인 VM”의 본질적 한계라기보단 그 소프트웨어만의 문제 가능성
          + 반드시 그런 건 아님
            일반적으로 VM은 불필요한 요소까지 에뮬레이션해서 느린 것
            만약 부팅 속도 위주로 하이퍼바이저를 만들고 레거시 호환성 무시할 수 있다면, Firecracker처럼 125ms만에 부팅하는 것이 가능
          + 리눅스에서 VM 메모리 할당이 느린 주 원인은 4KB 페이지로 몇 GB씩 할당할 때
            1GB 단위로 할당하면 극적으로 빨라질 수 있음
            윈도우도 이와 비슷한 방식이 아마 있음
          + VirtualBox 문제일 가능성
            본인은 Hyper-V에서 Ubuntu 22 GUI에 XRDP로 10초, Ubuntu 22 서버에 SSH로 3초 이내 접속 경험
     * 신뢰할 수 없는 코드를 실행해야 하는 상황에서 “원격 설치 스크립트를 바로 Bash에 파이프”하는 설치 안내 문구의 아이러니를 지적
       그럼에도 불구하고 기본 아이디어 자체는 대단히 흥미로움
          + 처음엔 무슨 말인지 못 알아들었지만, 설치 스크립트를 별도로 다운로드 후 직접 검증하는 것도 가능
            조만간 공식적인 배포 방법 마련 예정
     * 프로젝트를 공유해줘서 고맙다는 인사 및, 본인이 microsandbox의 제작자임을 밝힘
       Docker 컨테이너처럼 마이크로VM을 쉽게 만들 수 있도록 한 것이 목적
       궁금한 점 있으면 언제든 질문 환영
          + 당장 Python 라이브러리로 잘 써보고 있는데, sandbox를 여러 분할 호출에서 계속 유지시키고 싶음
            “Sandbox is not started. Call start() first” 같은 에러가 종종 발생
            공식 문서의 패턴은 “async with”인데, 본인 사용 방식은 클래스별로 한번 instantiate 후 여러 메서드에서 계속 활용하고 싶음
            이에 대한 추천 방법이나 베스트 프랙티스가 궁금
          + 분산/탈중앙화 소프트웨어 테스트 네트워크(Valet Network) 구축 중인데, microsandbox가 굉장히 유용할 듯
            네트워킹이 어떻게 동작하는지 궁금
            예를 들어 microvm이 오직 public IP만 접근 가능하도록 제한할 수 있나?
            즉, microvm이 local network IP는 접근하지 못하게 할 수 있는지
          + 정말 멋진 프로젝트, appcypher에게 감탄
            내장 MicroVM 기능이 OCI 런타임 인터페이스를 제공하는지 궁금
            runc/crun 대신 Docker/Podman에서도 쓸 수 있는지 알고 싶음
          + readme를 빠르게 스캔했는데, 추가 설명이 궁금한 질문들 있음
            어떻게 그렇게 빠를 수 있는지?
            전통적 VM 대비 어떤 트레이드오프가 있는지?
            VM 격리성이 손상될 수도 있는 여지가 있는지?
            GUI를 띄울 수 있는지?
            새로운 Vagrant로 생각하는지?
            데이터 입출력은 어떻게?
          + 굉장히 깔끔해 보임
            만약 제대로 이해했다면, 이걸로 실시간으로 백엔드도 서버처럼 띄울 수 있나?
            지원 언어 목록이 인상적 microsandbox 지원 언어 리스트
            기여 가이드가 더 상세했으면 좋겠음 contributor guide
     * 최근 몇 년간 초경량, 거의 일회용에 가까운 VM 옵션이 많아진 점에 놀람
       과거에는 VM이 느리고 무거워 고생스러웠던 경험 있음
       macOS의 Orbstack, 특히 “Linux machines” 기능과 비교해보고 싶음
       Orb에서는 VM을 하나만 재사용할지도 궁금
          + Orbstack 사이트
          + Orbstack Linux machines 문서
     * 축하 인사
       밀리초 단위로 VM 부팅하는 건 굉장히 중요한 향상
       하지만 CloudHypervisor, Firecracker로도 비슷하게 구현 가능
       컨테이너가 VM보다 이기는 건 런타임 퍼포먼스
       VM이 느려지는 요인은 IO 디바이스 에뮬레이션 때문
       특히 AI 에이전트형 워크로드에선 애플리케이션 단에서 오버헤드가 체감될 듯
       성능 이슈는 어떻게 해결할 계획인지 궁금
          + 맞는 지적임
            Microsandbox는 libkrun을 사용하고, libkrun은 성능 오버헤드를 줄이기 위해 block, vsock, virtio-fs에 virtio-mmio를 씀
            Firecracker도 본질적으로 비슷하며, E2B 프로젝트는 agentic AI 워크로드 처리에 Firecracker 사용
            당장 파일시스템 이슈 외에 큰 성능 향상 계획은 없음
     * 본인 취향상 컨테이너 기술이 OS를 너무 과하게 확장시키는 느낌
       mount 명령 하나만 쳐봐도 무슨 말인지 알 수 있음
       원래 감춰야 할 정보가 다 드러나서, 기존 간단한 명령어 활용성 저하
       더 심각한 문제는 유저가 내부 데이터 구조를 직접 만질 수 있다는 점
       마치 사용자에게 peek, poke 권한을 모두 주는 것 같음
       컨테이너 아이디어 자체는 좋지만, 커널이 재설계되지 않는 한 현재 방식은 임시방편
          + 글 내용이 잘 이해가 안 감
            컨테이너 안에서 mount 치면 어떤 점이 그렇게 치명적인지 설명 부탁
            Host mount가 실제 컨테이너에 노출되는 건가?
            보통은 명시적으로 volume 등을 컨테이너에 연결할 때만 가능하다고 생각
     * 매우 흥미로워 보여서 바로 써보고 싶어짐
       본인도 CodeSandbox SDK, E2B 등으로 재미를 많이 봤는데, 둘과의 차이점이나 향후 방향 궁금
       Firecracker도 내부적으로 쓰는지 알고 싶음
          + Microsandbox는 클라우드 솔루션을 제공하지 않음
            자신이 직접 호스팅하는 구조
            E2B처럼 microVM 기반 샌드박스를 로컬 환경(Linux, macOS, Windows(예정))에서 실행하기 쉽게 하고, 프로덕션 환경 전환도 간단
            Firecracker 대신 libkrun 사용
          + Firecracker 사용 여부가 가장 궁금했는데, 그게 주 관심
            microVM 솔루션의 유지보수 이슈, 보안 감사가 꾸준히 이뤄질지 궁금
            Firecracker와 OCI 이미지 세팅이 어려워서 대안의 등장 자체를 환영
            Kata container도 다루기 힘듦
     * 이런 종류의 프로젝트가 나오면 항상 관심이 감
       컨테이너의 가장 큰 장점은 디스크 크기, CPU 코어 등 구체적 리소스 지정 없이 빠르게 돌릴 수 있다는 점
       그 상태를 image와 diff해서, 프로그램이 실행 중 시스템에 어떤 변화를 줬는지도 확인 가능
       이런 간편성을 갖춘 초소형 VM을 통해서 보안성 강화된 샌드박싱이 가능하면 좋겠음
       가끔은 bwrap도 쓰지만, 커맨드라인에 적합한 툴은 아님
          + 리소스(디스크 용량, CPU 등)는 YAML 파일로 한 번 선언해놓으면 됨
            템플릿이나 원격/자동 생성도 가능
     * 주제가 약간 빗나가지만, 본인은 신뢰할 수 없는 JavaScript 코드를 꼭 돌려야 하는 프로젝트 진행 중
       microsandbox 덕분에 이런 코드를 안전하게 분리해서 돌릴 수 있겠다는 희망
       200ms 부팅 딜레이도 라이브 샌드박스 풀로 해결 가능
       OCI 호환성이 있으니, 전체 샌드박스 환경까지 제공 가능
       이게 정말 좋은 활용처 맞는지, 더 좋은 대안 없는지도 궁금
          + runsc/gVisor도 고려해볼 만함
            runsc 엔진은 Docker/Docker Desktop 안에서도 실행 가능
            단, gVisor는 네트워크 병렬 처리 등 성능이 docker 대비 1/3 수준임
          + 바로 이런 케이스가 microsandbox의 이상적인 적용처
            더 좋은 대안을 못 찾아서 직접 microsandbox를 만듦
"
"https://news.hada.io/topic?id=21288","FFmpeg, WebRTC(WHIP) 초저지연 스트리밍 지원 머지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  FFmpeg, WebRTC(WHIP) 초저지연 스트리밍 지원 머지

     * FFmpeg에 WHIP(WebRTC-HTTP Ingestion Protocol) muxer가 정식 추가되어 1초 미만 초저지연 스트리밍을 직접 지원함
     * 이번 커밋에서 WHIP muxer의 네이밍 및 구조가 개편되고, SSL/DTLS/RTC 오류 메시지와 로그가 개선됨
     * DTLS 곡선/프로파일, RTP payload, ICE STUN 등 주요 프로토콜 파라미터가 크롬 정의에 맞게 업데이트되고, 매직 넘버가 매크로 및 함수로 추출됨
     * DTLS 핸드셰이크와 ICE 처리가 하나의 함수로 통합 및 최적화되어 성능과 안정성이 크게 향상됨
     * 오디오, 비디오 트랜스코딩(h264_mp4toannexb, OPUS timestamp, 마커 설정 등) 버그가 해결되어 표준 WebRTC 환경과 호환성 증가
     * OpenSSL 의존성을 명확히 하여, DTLS 지원 시에만 WHIP가 빌드됨
     * FFmpeg만으로 WebRTC 기반 방송·실시간 스트림 환경 구축이 쉬워지고, 기존 RTMP 등 레거시 프로토콜 대비 초저지연 특성을 활용할 수 있게 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

avformat/whip: FFmpeg WHIP muxer 지원 추가

  주요 변경점 요약

     * WHIP Version 3 기반 muxer 정식 도입, 내부 명칭 및 구조 정비
     * SSL, DTLS, RTC의 로그 컨텍스트와 에러 메시지가 한층 명확해짐
     * 하드코딩된 매직 넘버를 매크로와 별도 함수로 추출해 유지보수성 강화
     * DTLS 곡선 리스트, SRTP 프로파일명 등 FFmpeg와 OpenSSL 표준에 맞춰 수정
     * ICE STUN 매직 넘버, RTP 페이로드 타입이 크롬 브라우저 표준과 일치하도록 업데이트
     * 오디오 프레임 크기, H.264 MP4→AnnexB 변환, OPUS 타임스탬프 등 미디어 처리 이슈 해결
     * DTLS 핸드셰이크와 ICE 처리 로직이 단일 함수로 통합되어 관리 용이
     * OpenSSL 기반 DTLS 지원 조건이 명확해져, 빌드 오류 및 호환성 개선
     * SRTP, BIO 콜백, CA 키/인증서 초기화 등 TLS/DTLS 내부 구조 통합
     * whip.c 파일 신설 등 총 13개 파일 변경 및 신규 추가

  배경 및 의미

     * WHIP는 WebRTC 기반 스트림 송출을 위한 HTTP 기반 표준 프로토콜로, 초저지연 라이브 방송에 필수적임
     * 그동안 FFmpeg에서 WebRTC 인코딩·송출은 별도 툴이나 복잡한 중계가 필요했으나, 이번 머지로 FFmpeg 단일 명령어로 WHIP 송출이 가능해짐
     * 실시간 방송, 라이브 커머스, 화상회의 등 다양한 분야에서 최신 WebRTC 생태계와 직접 연동할 수 있는 기술적 전환점

        Hacker News 의견

     * WebRTC 방송에 엄청 설레는 감정 느끼는 중, Broadcast Box README와 OBS PR 링크에서 내가 정리한 이유 소개, 이제 GStreamer, OBS, FFmpeg 모두가 WHIP 지원하게 되어 모든 플랫폼에 적용 가능한 범용 비디오 방송 프로토콜 도달, 수년간 오픈소스 + WebRTC 방송 작업 경험으로 이번 성과가 큰 이정표라는 생각
          + 원격 dosbox 게임을 폰에서 VNC로 했었는데, 이제 input handler 웹앱 직접 만들고 이 기술+OBS 조합으로 무한히 시간 소비하는 새로운 도전 의욕
          + 모바일 스트리밍 유닛에서 여러 개의 5G 모뎀을 연결해 multipath/failover-bonding 기능 (예: SRT 개조로 H.265를 여러 링크로 송신) 추가 예정 있는지 질문
          + 인기있는 방송 소프트웨어, 모바일, 웹, 임베디드 등 다양한 플랫폼에서 직접 활용 경험 후 broadcast-box와 개발 기여에 놀라움
          + 나의 webrtc 라이브러리를 여러 프로젝트에서 유용하게 써주고 넓은 기술 스펙트럼에 영향 준 점 보며 기쁨 표출
     * WebRTC-HTTP Ingestion Protocol (WHIP) 구현을 알림—SCTP 자체를 다루는 게 아니라, WebRTC를 사용하는 피어들과 HTTP를 통해 게이트웨이 역할 WHIP IDF 문서(링크) 참고, 언젠가 SCTP 대신 QUIC이나 WebTransport 기반 p2p 프로토콜로 넘어가길 희망, Media-over-Quic(MoQ)에 관심 있지만 브라우저 지원이나 발전이 몇 년째 느려진 점 공유, 관련 링크는 quic.video와 MoQ IETF introduction
          + SCTP 부분을 어떤 식으로 활용/노출하고 싶은지 묻는 질문, WHIP IETF 초안에 관련 언급이나 제안이 없어 애매함, 대부분의 'WHIP Provider'가 DataChannel을 지원하지만 표준화되지 않은 상황
     * FFmpeg과 웹사이트의 직접 연결로 바로 오디오/비디오 스트림 수신이 가능한지 질문, 더 자세한 내용은 Phoronix 페이지 (링크)에서 확인 가능
          + FFmpeg 라이브러리(특히 libavformat)를 사용하는 프로그램들이 WebRTC 스트림을 직접 받아 활용 가능하다는 요약 설명
     * 이번 변화로 셀프 호스팅 스트림/스트리밍 CDN 구축이 훨씬 쉬워질 거라는 기대, ffmpeg의 독립성과 플러그 앤 플레이 성능 강조
          + Simulcast와 결합해 비용이나 진입장벽이 획기적으로 낮아질 전망, broadcast-box를 만든 계기도 셀프호스팅+WebRTC의 진입장벽을 낮추기 위함
          + LLM 활용으로 ffmpeg 관련 모든 비디오 작업에 one-liner 명령까지 도출 가능, AI 사용 경험 극찬
          + ffmpeg의 다재다능함을 한눈에 보여주는 만화 xkcd 2347 항상 떠오름
     * Anubis 그래픽을 ffmpeg와 gnu 등에서 예상치 않게 만난 경험이 인상 깊음
     * WHIP 기능 추가로 인해 시스템에 ffmpeg 유지하는 게 더 위험해지지 않을지 우려, WebRTC 보안 취약점이 실제로 많은 침해 원인이라고 느끼고 과거에도 브라우저 설치 후 항상 비활성 처리 경험
          + 어떤 보안 취약점이 있는지 질문, 이번 구현이 매우 작고, 사용자에게 최고의 결과물을 제공한다는 강한 확신 언급
          + --without-whip 같이 원치 않으면 빌드에서 아예 빼는 옵션 선택 가능한지 질문, 그렇게 되면 최상일 것이라고 의견
          + ffmpeg가 과거 보안 문제 경험이 많아 사용자 입력 처리에 있어선 항상 격리 환경 사용이 최선, 예를 들면 ffmpeg와 의존성만 포함된 도커 이미지를 매 작업마다 새로 띄워서 활용 또는 썸네일이나 문서 처리 필요하면 ClamAV, OpenOffice, ImageMagick 등 추가 장착 권장, 또 사용자 생성 파일을 다루는 서버는 최대한 분리된 VLAN 또는 AWS라면 보안그룹 내에서만 운용하는 방식 추천, 각 프로젝트에 비판 뜻 아니고 이진 포맷 자체의 어려움과 선제적 안전 대책의 중요성 강조, 4chan 사례도 참고, ffmpeg 보안 페이지는 여기
     * ffmpeg의 보안 감사 활동이 어떤지 질문, 공식 사이트에서 다소 사후 대응적으로 보인다는 느낌 공유
     * ffmpeg로 이제 Jitsi 화상회의의 오디오 스트림 녹음이 가능한지 확인 질문
     * whip 지원을 FFmpeg 소스 빌드에서 성공적으로 적용한 경험 있는지, 올바른 ./configure 옵션 찾기 어려움 토로
          + 필요한 옵션은 --enable-muxer=whip와 --enable-openssl이라는 안내
     * Jellyfin에서 이번 기능이 적용될 날을 손꼽아 기다리는 중
          + 이에 대해 Jellyfin에 어떤 기능적 도움이 있을지 묻는 질문
"
"https://news.hada.io/topic?id=21280","AI 연구자의 에너지 절감법: 되감기 방식으로 돌아가기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AI 연구자의 에너지 절감법: 되감기 방식으로 돌아가기

     * 되감기(리버서블) 컴퓨팅은 연산을 역방향으로 실행해 데이터 삭제 없이 에너지를 절약할 수 있는 이론적 방법으로, AI의 에너지 소비 문제를 해결할 대안으로 주목받고 있음
     * 기존 컴퓨터는 정보 삭제 시 필연적으로 열(에너지)을 방출하며, 이는 물리적 한계(랜드아우어 원리)로 인해 피할 수 없음
     * Uncomputation 개념은 연산 결과만 남기고 나머지는 연산을 거꾸로 되돌려 정보 손실 없이 처리하지만, 속도와 메모리 비용 등 실용적 한계가 존재함
     * 최근 AI처럼 병렬 연산이 많은 작업에서 리버서블 칩을 느리게 여러 개 운영하면, 에너지 절감 효과가 큼이 실증적으로 밝혀짐
     * 산업계와 연구진이 실제 상용 리버서블 칩 개발에 뛰어들며, AI의 에너지 효율성 혁신이 현실화될 가능성이 커지고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

컴퓨터 연산의 근본적인 에너지 소실

     * 컴퓨터가 두 수를 더할 때, 예를 들어 2 + 2 = 4와 같이 두 입력에서 하나의 출력만 남기게 됨
     * 이렇게 일부 정보가 사라져 연산이 비가역성을 띄게 되고, 이는 삭제된 정보가 열 에너지로 바뀌는 현상임
     * 대부분의 컴퓨터는 이 방식으로 작동하므로, 근본적으로 항상 어느 정도의 정보 소실(열 발생) 이 불가피하게 발생함

Landauer의 가역 컴퓨팅 제안과 한계

     * Landauer는 정보 삭제 없이 모든 연산 결과를 기록함으로써 에너지 소실을 줄이는 컴퓨팅을 생각했음
     * 하지만 이런 컴퓨터는 현실적으로 메모리가 금방 가득 차 쓸 수 없으므로 실용성이 떨어지는 문제를 발견함
     * Landauer는 결국 가역 컴퓨팅이 막다른 길이라고 판단했음

Bennett의 uncomputation(역연산) 아이디어

     * IBM의 Charles Bennett는 1973년, 연산 결과만 저장하고, 나머지는 계산 과정을 역으로 실행(uncomputation)해서 지우는 방법을 제안함
     * Hansel과 Gretel이 빵조각을 다시 주워가는 식의 비유처럼, 필요한 데이터만 남기고 정보 소실 없이 제거가 가능함
     * 이 방식은 연산 시간이 2배 걸리는 단점이 있어 비효율적으로 여겨졌음

실용성을 높인 연구들의 등장

     * Bennett는 1989년, 약간 더 많은 메모리를 쓰면 연산 시간을 크게 줄일 수 있음을 밝힘
     * 이후 연구자들이 메모리·시간 최적화 방안 연구를 지속함
     * 하지만 컴퓨터는 데이터 삭제 외에도 트랜지스터 연결 방식 자체의 비효율로 에너지가 소실되는 구조임
     * 실질적인 에너지 절감형 가역 컴퓨터 제작을 위해서는 설계 단계부터 저열 손실 구조가 필요함

MIT의 프로토타입 칩과 산업계 반응

     * 1990년대 MIT 엔지니어들이 회로 효율을 높인 프로토타입 칩을 제작함
     * Frank는 박사과정생으로 참여하여 가역 컴퓨팅 대표 학자로 활동함
     * 그러나 기존 칩 성능이 빠르게 개선되던 산업 현실에서, 이론적 대안에 대한 산업계의 관심 부족으로 지원이 저조했음
     * Frank도 한동안 연구를 접고 다른 길을 찾았음
     * 하지만 회로가 미세화 한계에 다다르며 에너지 효율 문제에 대한 관심이 급증함

가역 컴퓨팅의 에너지 효율성과 AI 적용 가능성

     * 2022년, Cambridge의 Hannah Earley가 가역 컴퓨터의 에너지 효율성을 정밀 분석함
     * 가역 컴퓨터는 기존 대비 열 배출이 적으나, 완전한 무열은 불가능함
     * 특히 가역 컴퓨터는 속도를 늦출수록 열 배출이 줄어드는 현상을 규명함
     * AI 연산은 병렬처리 환경이므로, 각각의 칩을 느리게 돌리고 칩의 수를 늘릴수록 총 에너지 소모 감소 효과를 기대할 수 있음
     * 느린 속도 덕분에 냉각비용도 줄여 칩 밀집 배치, 공간·소재 절약 효과 또한 기대 가능함

상용화 움직임과 전망

     * 투자자들이 주목하기 시작하며, Earley와 Frank가 Vaire Computing을 창업, 상용 가역 칩 개발에 착수함
     * 코펜하겐 대학의 Mogensen 등은 실제로 가역 프로세서가 현업에 적용되는 것에 큰 기대를 밝힘
     * 수십 년간 이론에 머물렀던 가역 컴퓨팅이 AI와 에너지 효율 분야에서 실질적 혁신을 가져올지 주목받는 상황임

결론

     * 리버서블 컴퓨팅은 정보 삭제 시 열 발생이라는 컴퓨터 물리적 한계를 극복할 수 있는 실질적 방법으로, AI 시대의 대규모 에너지 절감 기술로 주목받음
     * 느리게, 병렬로 칩을 운영하는 방식이 AI 연산의 구조적 특성과 결합되어 실제 상용화가 임박

        Hacker News 의견

     * Stephen Baxter의 소설 Time에서는 아주 먼 미래, 모든 별이 소멸하고 블랙홀이 전부 증발한 뒤 인류의 후손이 최대 엔트로피 우주에 남게 되는 이야기 전개, 자유 에너지가 완전히 소진된 상태에서 이 후손들이 거대 시뮬레이션 안에서 가역적 연산(에너지 소모 없이 동작)만으로 똑같은 사건을 반복하면서 살아가는 컨셉 등장, 연산 결과를 uncompute하고 다시 compute하는 식으로 동일한 이벤트 루프 반복
     * 소프트웨어 엔지니어 입장에서 이 내용이 이해하기 쉽지 않은 부분 언급, 정보를 삭제할 때 전자가 소실된다는 개념이 처음 나와 혼란, 전자는 모든 곳에서 소실되며, 대부분의 게이트가 전류의 부정으로 동작하니 이런 행위들이 다 나쁜 건지 질문, 메모리 변화를 모두 기록하면 왜 열 손실을 막을 수 있는지 의문 제기, 모든 메모리를 계속 유지해야 하는 상황에서 그게 에너지를 더 소모하지 않을지 고민, 그리고 굳이 연산에서 과거로 되돌아갈 필요가 왜 필요한지에 대한 실용성 의문
          + 이론적으로 정보를 잊지 않는 컴퓨터는 전력을 거의 쓰지 않고, 따라서 열도 발생하지 않는 컴퓨터 구현 가능, 이런 종류의 컴퓨터는 가역(adiabatic) 컴퓨팅으로, 모든 연산 게이트가 되돌릴 수 있어야 함, 입력 상태 설정과 출력 복사 등 극초기 및 최종 단계에서는 여전히 에너지 필요, 실제 현실에서는 대부분의 전력 소모가 로직 게이트에서 정보 ""삭제"" 때문에 아니라 배선 저항과 같은 쪽에서 손실, 완전한 가역 CPU 만들려면 초전도 배선/소자 등 특수 하드웨어 필요, 또한 연산을 되돌려야 하는데 그것도 쉽지 않은 문제, 아니면 아예 상태를 지우면서 에너지 소비 감수, 현실 사례로 양자컴퓨터를 들 수 있으며, 양자 논리 게이트는 모든 연산이 가역적이고 역방향 수행 가능
          + 열역학 입장에서 가역적 프로세스가 이론적으로 효율 최대라는 점, 이는 엔트로피와 관련, 정보를 지우면 가역적이 아니게 되므로 열 발생 불가피, 다만 이 모든 것은 철저히 이론적이고 현실 컴퓨터는 이 한계에 전혀 미치지 못하는 수준, 실제 논리 소자들은 AND, OR, NAND 등 대부분 고립되면 비가역적 연산 구조
          + 하드디스크, SSD와 같은 영구 저장장치는 데이터 유지를 위해 전력을 전혀 소모하지 않으므로 열도 발생하지 않음 지적, 데이터를 지우거나 덮어쓰기 할 때 필연적으로 에너지 필요, 이 과정에서 많은 열 발생, 열 발산 문제가 더 미세한 칩 스케일링의 장애 요소임을 언급, 정보를 지우지 않는 컴퓨터를 설계할 수 있으면 열 발생이 대폭 줄고 이로 인해 칩 성능 ↑, 전력 절감 및 스케일링에 가능성 열림
     * 에너지 절감이라는 동기에는 다소 회의적, 하지만 가역적인 딥러닝 아키텍처를 구현하는 자체가 꽤 흥미로운 연구 주제, 실제로 2019~2021년 invertibleworkshop 시리즈 등에서 활발히 논의, 최근 유행하는 diffusion 모델도 연속적인 normalizing flow의 특수 사례로 볼 수 있어 이론적으로는 계산이 가역적, 실제로 프로덕션에 쓰는 distill된 모델들은 거의 그렇지 않다고 생각, 미분방정식 시뮬레이션도 부동소수점 반올림 오차 때문에 실제로는 역방향 계산이 정확히 일치하기 어렵지만, 정교하게 만들면 bit-to-bit로 완벽하게 가역적 시뮬레이션도 가능
          + 머신러닝에서 계산을 정확히 되돌릴 수 있으면 쓸모 있었던 2015년도 논문도 있음
     * 연산에 방향성이 있다는 게 무슨 의미일지 고민, 인과관계처럼 보이나 실제론 입력과 출력의 문제로 보임, 결국 프로그램을 먼저 실행해봐야 가능할 것 같고, 상태를 저장해두면 백트래킹만 쉬워지는 정도로만 느껴짐
          + 예, 그런데 물리적 레벨에서 말하는 것이므로 별도의 하드웨어 필요, 정보 삭제(예: AND 연산)는 열을 발생시키기 때문에 Fredkin 게이트 등 별개 논리 게이트 필요
          + 사실 모든 계산은 방향성이 있음, 이 주제에 매우 흥미를 느끼는 입장, 예를 들어 함수 f(x) -> y 자체가 방향 제시, 역방향이 당연히 가능하면 좋지만 인버스 불가능한 경우도 상존. 가령 f(x)=mx+b는 쉽게 역함수 구함(단 m=0 아님), 반대로 f(x)=x^2이면 f(x)에서 x 값을 복구할 때 ±x 모두 해당되어 유일하지 않음, 함수 이미지와 프리이미지 개념 적용 가능, 이는 P=NP 문제와도 밀접관련, 머신러닝에서는 Normalizing Flow가 인버터블, diffusion model은 리버서블 구조, GAN-Inversion 등 ML 커뮤니티에서 ""inverse problem"" 용어 쓰는 것에 개인적으로 불만, 이 개념 자체를 이해하면 왜 예측은 한쪽 방향만 정확하고 역은 실패하는지 알게 됨, 결국 이것이 인과추론 문제로 귀결, 물리학에서 방정식을 변형해 인과지도를 만드는 게 주요 목표지만, 엔트로피/양자역학 등에서 고유의
            난제가 발생, 예시로 기체분자 상태 계산을 역산하면 유일한 해가 아닌 여러 상태가 나옴, 미분 적분 예시처럼 differentiation은 가역적이지 않으며 f(x)+C 모두가 동일 미분 값을 제공, 즉 단방향 정보 손실 발생, 여러 시점 상태를 샘플링하면 솔루션 공간이 확 줄어들기도
          + 결론적으로는 비가역 연산(정보 삭제)이 최소화되면 가역적 연산 가능, 예시: 2 + 2 + 2 처럼 여러 연산이 모두 기록되면 가역적, 하지만 마지막 결과만 남기고 이력 삭제 시(6만 남음) 비가역
     * Mike P Frank를 트위터에서 계속 팔로우하며 reversible computing 및 AI에 대한 흥미로운 발언 자주 접함 MikePFrank 트위터
     * 이번 기술이 GPU 데이터센터 기반 소프트웨어 트렌드가 다시 돌아올 때 유용할 수 있길 바라지만, Jevons Paradox(효율이 개선되면 수요가 증가하여 실질적 에너지 절감에 실패함)처럼 결국 전환점이 안 될 가능성이 높을 거라 봄
     * 구체적인 계획이 뭔지, 실제로 reversible matmul이 시연된 사례가 있는지 궁금증, 그 연산조차도 중간 과정에서 쉽게 정보 삭제가 불가피한데 현실성 의문
          + 가역적인 행렬에 대해선 reversible matmul 존재, 하지만 ReLU 처럼 비가역적인 연산자에는 불가, 그리고 기사에서 주장처럼 단순히 거꾸로 연산하면 에너지를 덜 쓸 수 있는지도 잘 이해 안 됨
     * 기사 헤드라인을 보는데, 해당 웹페이지가 최신 컴퓨터에서만 해도 12초 걸린다는 점에 아이러니 느낀 경험, 사람들은 대체로 남의 문제엔 신경 안 쓰고, (AI 등)새로운 기술이 등장하면 환경, 개인의 일자리, 인프라, 저작권 침해, 사회 시스템 등 사회적 비용은 외부화되는 경향, 효율을 얻으면 결국 자기를 위해 더 많이 사용하게 되고, 타인에게 주는 피해를 줄이지 않는다는 씁쓸함
     * LLM을 여러 번 써도 전기 사용량이 전기포트로 물을 끓이는 것보다 적다는 사실이 여러 차례 입증된 바 있음
"
"https://news.hada.io/topic?id=21233","Ask HN: 유료 API로 생계를 유지하는 분 계신가요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Ask HN: 유료 API로 생계를 유지하는 분 계신가요?

     * API 접근 권한 판매로 생계를 유지하는 솔로 개발자나 소규모 팀이 있는지에 대한 질문
          + API는 무엇인가요? MRR은 얼마인가요? 가격 모델은 어떻게 되나요? 첫 유료 고객은 어떻게 찾으셨나요?
     * 그리고 가장 중요한 것은, 사람들이 실제로 매달 비용을 지불할 만한 어떤 문제를 해결하고 있나요?
     * 추가로 가장 큰 어려움(요금 제한? 고객 지원? 경쟁?) 과 ""이걸 시작하기 전에 알았으면 좋았을 텐데"" 하는 생각이 드는 조언도 부탁드려요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

성공적인 유료 API 사례

     * OCR/문서 추출, 인증(CIAM) API: FormX(https://formx.ai), Authgear(https://authgear.com)
          + 건당 과금, 연 단위 계약으로 MRR 5.5만~3.5만 달러 수준
          + GCP/Azure와 ISV 파트너십으로 첫 B2B 고객 확보, 이후 마케팅이 가장 큰 도전
          + 개발자 대상 지원의 어려움(타 팀 개발자와 문제해결, 트러블슈팅) 경험
     * 스크린샷 API: ScreenshotOne(https://screenshotone.com)
          + 1인 개발, MRR 2만 달러, 서버비 월 5,500달러
          + SEO, 소셜, 직접적인 마케팅 등으로 사용자 기반 확대
          + 시장 진입이 매우 어렵고, 다시 시작한다면 더 쉬운 니치 선택 의사
          + 직접 브라우저 클러스터 운영으로 품질 확보, 커스텀 확장(광고/쿠키배너 제거 등)
     * 통신/SMS API: 46elks
          + 스웨덴/유럽 현지 이동통신사와 직접 연동, 파이썬 기반 커스텀 플랫폼
          + MRR 50만 유로, 사용량 기반 과금
          + 해커톤/밋업 등 오프라인 네트워킹으로 첫 고객 확보, 스케일링이 과제
          + Twilio로 대표되는 글로벌 대형 경쟁자 존재, 현지화/지원 서비스로 차별화
     * 기계학습(ML) API:
          + 특정 도메인에 특화된 머신러닝 API, 사용량/건수 기반 과금
          + MRR 수천~수만 달러
          + 프론트엔드 기업이 전체 수익의 대부분을 가져가는 구조, 단순 ML API만으로는 한계
     * 음성인식(Speech-to-Text) API: borgcloud.org
          + 시간당 과금(0.06달러/h), MRR 약 5,000달러
          + Reddit 등 커뮤니티에서 첫 유료 고객 유입
          + 대형 클라우드(Whisper, Groq 등)와의 가격 경쟁 심화
          + 자체 GPU 네트워크 활용해 원가 절감

공통된 도전과 배움

     * 마케팅과 고객 지원이 기술보다 더 큰 도전
          + 개발자를 대상으로 하더라도 적극적 세일즈와 지원 필요
          + GCP/Azure, 해커톤, 블로그, Stack Overflow 답변 등 다양한 루트 활용
     * 가격 경쟁력, 차별화 요소, 법적 이슈까지 신경 써야 함
          + API만 제공할 경우 프론트엔드 개발사보다 수익 구조상 불리
          + 자체 운영비(서버 등)와 RapidAPI와 같은 플랫폼 수수료도 고민

시장 구조와 생존 전략

     * API 사업은 강력한 니치(특정 문제/고객/도메인)에서 성과
          + ImageMagick, SMS, 인증, 레시피 파싱 등, 기존 오픈소스/대기업 대비 불편·비효율을 해소하는 경우에 고객이 실제로 지불
          + 프론트엔드까지 패키징하거나, API만 제공할 땐 다수 앱을 통해 간접적으로 고객에 접근
     * 고객의 '진짜 문제(pain point)'를 해결하는 것이 핵심
          + 고객 직접접점(프론트엔드)에 더 높은 가치를 두며, API만으로는 수익 한계가 분명함

추가적인 인사이트

     * 대부분의 답변자들이 ""시작은 어렵지만 꾸준히 운영하면 가능"", ""경쟁 격화와 대체재 출현에 유의"", ""API 제공만으로는 전체 시장 가치의 일부만 가져감""을 공통적으로 강조
     * 진짜 해결할 문제가 명확하고, 고객이 지불 의사가 있어야 API 비즈니스가 성공

   멋지네요...! 자유로울 것 같으면서도 지속 가능성에 대해 계속 고민해야 한다는 점이 어려울 것 같아요.

        Hacker News 의견

     * 처음에는 개발 대행 업체로 시작해서, 고객 수요에 기반한 두 가지 API 제품을 만들게 된 경험 공유. 첫 번째는 OCR 및 문서 추출 서비스이며, 초기에는 중국어 문자를 지원하는 유용한 솔루션이 없어 자체 구축함. 최근에는 (파인튜닝된) LLM/VLM을 활용해 다양한 기능을 추가하는 형태로 방향 전환. 예를 들어, 특정 고객의 데이터로 파인튜닝, 체크박스 같은 특정 요소에 맞춘 프롬프트 튜닝, 수백 페이지짜리 PDF를 적은 분량의 문서 여러 개로 분할하는 등의 기능 제공. 현재 약 5만 5천 달러 MRR, 페이지 단위 요금제와 연간 계약 체결 방식 채택(할인도 다수 적용). 두 번째는 오픈소스 CIAM이며 약 3만 5천 달러 MRR 기록. 마케팅에 대해 아무것도 모른 상태에서 시작해, 초기에 GCP/Azure 현지 파트너와 ISV로 협력해 첫 유료 고객을 확보, 이 과정에서 자연스럽게 ""기업""
       시장으로 진입. 제품 마케팅도 크지만, 개발자 대상 고객 지원도 쉽지 않은 점 강조 — 개발자이기에 개발자를 지원할 수 있다는 점, 때로는 다른 팀의 문제까지 디버깅하게 된다는 어려움 언급. 실제 사례로, API 결과가 갑자기 잘못 나온다는 클라이언트의 문의를 받고 여러 번 이메일을 주고받은끝에 화상 회의로 화면 공유 요청, 결국 프록시에 캐시가 활성화된 상태로 API를 호출하고 있던 것이 원인임을 발견한 경험 공유. FormX.AI 및 Authgear 서비스 링크 제공
          + 지역 GCP/Azure와 파트너십을 맺은 계기와 노하우에 대한 궁금증, 해당 방식이 스마트하며 대형 클라우드 벤더가 이런 제안을 환영하는지, 고객 맞춤형 솔루션을 제공해 딜을 성사시키도록 제안했는지 질문
     * 한 지인이 겪은 독특한 사례 소개. 에너지 회사에서 외주 컨설턴트들이 내부 IT를 복잡하게 만들고 비효율적인 상근 직원들은 쿼리 한 번 돌리기도 어려운 환경 설명. 이 지인은 가스 고객 데이터베이스를 잘 알고 있어 자신의 회사를 설립, 직원에서 컨설턴트로 전환. 잠시 회사에 혼란을 주고 다시 나타나 고객 데이터를 자신만의 시스템으로 옮겨 관리하는 계약을 제안, 자동화로 운영 효율을 높이고 API 사용 요금+월 이용료로 수익을 올림
          + 가스 고객 데이터를 자기 시스템으로 옮기는 과정이 법적으로 문제가 있는 행동으로 보인다는 의견
          + 기존 외주 컨설턴트와 유사한 방식이지만 훨씬 더 자동화·효율화된 프로세스로 진입한 것이라는 생각
          + 추가적인 단계가 더해진 강요 또는 갈취(extortion)처럼 들린다는 인상, 하지만 더 긍정적으로 해석할 수 있는 방법이 있는지 궁금함
          + 이런 방식이 합법적인지, 그리고 회사를 잘 아는 사람이 독립해 이런 일을 하는 게 얼마나 자주 일어나는 일인지 궁금증 표출
     * 친구 Dmytro가 혼자 운영하는 ScreenshotOne이라는 스크린샷 API 비즈니스 소개, 최근 MRR 2만 달러 돌파. Dmytro X 계정 및 서비스 링크 공유
          + 직접 자동화 브라우저 관리 여부 질문, Scrapfly, Scraping Bee, Zen Rows 같은 서비스 래퍼일 수도 있고 맞춤형 JS로 배너를 없애는 작업이 포함되어 있을 수 있다는 추측
          + ScreenshotOne 같은 기업이 어떤 방식으로 유저 베이스를 구축하는지 궁금증, 아이디어나 추측 요청
     * 소규모 회사에서 근무 중이며, 전체 매출의 대부분이 유료 API에서 발생함. 기밀정보로 세부사항은 공개 불가. 해당 API는 특정 시나리오에 대한 최고 수준의 머신러닝 모델이며, 공개 가격표와 개별 협상 할인 체계 존재. 최근 가장 큰 도전과제는 Google Lens처럼 일반 잠재고객에게 충분히 좋은 무료 대체제가 등장해 잠식되고 있는 상황. ML API만 만들고 자체 앱을 만들지 않은 것이 아쉬움으로 남는데, 실제로 프런트엔드를 구현하는 쪽이 더 많은 수익을 거두게 된다는 점 공유
          + 프런트엔드를 만드는 쪽이 돈을 벌게 되는 원인에 대한 설명 요청
          + 실제로 프런트엔드를 만드는 쪽이 사용자(수익의 원천)의 문제를 바로 해결해 주는 구조라서, API는 하나 건너 수익에서 멀어진 위치라는 의견
          + 엔드유저 앱 대신 ML API만 운영한 것이 그렇게 아쉬운지 궁금함, 여러 앱이 API를 쓰고 있다면 오히려 핵심 역량에 집중하고 소규모 수익도 합하면 충분히 의미 있을 수 있다는 생각
          + 이런 경우는 API의 시장 규모 자체가 너무 작았던 것일 수 있음. API가 실질적으로 앱 1:1 대응이면 앱을 만들어야겠지만, 여러 앱을 지원하고도 충분한 매출을 못 올린다면 시장의 니즈 자체가 부족한 것일 수 있다는 분석
     * 조리법(요리재료 문장 — 예: ""2 cups finely chopped onions"")을 구조화된 JSON으로 변환하는 API를 운영 중이고 월 200달러 정도 수입이 있음. 2019년부터 유지보수 모드로 운영해 아주 소극적으로 관리(연간 한두 시간 소요). 모든 고객이 아직 LLM으로 완전히 옮기지 않은 사실이 놀랍고, 아마도 이런 니치 시장에서는 가격이나 정확도 때문에 기존 API가 아직 경쟁력 있다는 생각. 누군가 인수해 더 발전시키면 좋겠지만, 인수 준비에만 30~40시간 정도가 걸릴 듯해서 기회비용이 5~10천 달러로 계산, 월 200달러 API를 그 정도 가격에 인수할 사람은 없을 것 같음. 초기에 RapidAPI를 사용한 것이 큰 실수임을 강조(수수료 20%, 불편한 UI, 미납금 발생), 차라리 Paddle로 자체 과금 시스템 구축할 것을 바람. ZestfulData 링크 공유
          + 똑같은 사이트를 ChatGPT API로 면접 준비용 프로젝트로 만들어본 경험 소개, 가장 큰 어려움은 ChatGPT에게 API 사용법을 물어보면, 학습된 시점이 오래 되어 예제 코드가 동작하지 않는 문제였다는 실제 사례
          + 본인이 거주하는 국가에서는 프리랜서로 일하는 비용이 월 200유로 수준이며, 그 대부분이 건강 보험 등 임금 외 비용임. 즉, 월 200달러 수익만으로는 성립이 불가능. 이런 낮은 마진으로 어떻게 합법적으로 일하는지 궁금함
          + 이 API를 사용하는 고객군이 누구인지 궁금, 다양한 유사 아이디어가 있었지만 결국 개발자(자체적으로 툴을 개발하는)라면 이러한 API를 굳이 외부에서 쓰지 않을 것 같다는 마케팅 측면의 고민 공유
          + 최초 고객을 어떻게 찾았는지 직접적인 궁금증 질문
     * 본인 역시 기술 프로젝트로 가치를 제공하는 방법에 대해 관심이 많음. 다만 이 주제를 탐색할 때의 문제점은, 성공한 사람들이 자세한 경험을 공유할 인센티브가 적다는 것임. 최악의 경우, 이런 공개가 경쟁자의 진입을 초대할 수도 있음. 오픈소스 같이 성장에 열려있는 커뮤니티와 달리, API 비즈니스는 쉽게 복제 가능하기 때문에 실제로는 정보를 잘 공유 안 하는 문화 설명. 최근 발견한 서비스 유형으로는, 유튜브 라이브 스트리밍 등 장시간 비디오 파일을 자동 스트리밍하는 서비스가 있음
          + 기술자 입장에서는 ""이런 건 아무나 만들 수 있겠다""는 착각을 하게 됨. 결국 중요한 것은 고객이 실제로 지불 의사가 있느냐는 것임. Pirate Bay 전성기에 음악은 사실상 무료였지만, Spotify가 더 나은 편리를 제공해 지불 시장을 만들어냄. ImageMagick 같은 오픈소스도 있지만, 그 위에 API/SaaS로 성공하는 서비스가 존재함. 그 이유는 결국 사람과 기업이 ""편의""에 지불함. 본인이 잘 아는 분야, 기술로 해결 가능한 문제를 찾는 것이 출발점이며, 자신이 진짜로 관심 있고 잘 아는 업계나 고객 프로필에서 시작할 것을 추천. 자신은 개발자였기에 개발자에게 필요한 API를 직접 구축함
          + 모든 회사에는 소수만 아는 비밀이 있고, 업계를 깊이 알면 경쟁사가 뭘 하는지 분석 가능. 하지만 실제로 사업을 키우는 비결은 쉽게 드러나지 않는 ""비장의 노하우""에 있음. 본인은 지금 당장에도 기존 사업에 새로운 트위스트를 적용해 2년 안에 연 100만 달러 추가 수익을 자신 있음. 하지만 이미 주 60시간 이상 일하며 잘 벌고 있고, 아이디어를 타인과 공유해 이 사업을 합작하기엔 분석 유출 위험이 너무 크다는 솔직한 심정
     * 본인이 만든 SMS & 전화 API를 통해 생계를 유지 중. 월 반복수익은 약 50만 유로, 요금제는 사용량(건별 SMS/MMS, 통화 분당, 가상번호 월 단가)에 따른 방식. 유럽/스웨덴 등 지역 이동통신망에 프로그래밍적으로 접근할 수 있다는 점이 문제 해결의 핵심. 첫 고객은 오프라인 네트워킹(해커톤, 밋업, 지인 활용 등)으로 확보, 하지만 이 방식은 사업 확장의 가장 큰 어려움. 여기까지 오는 데 힘든 과정이 많았고, 지금도 모든 것이 잘 돌아가는 것이 비현실적으로 느껴질 정도임
          + 사용하는 기술 스택에 대한 궁금증, 스웨덴 IT 인프라는 아는 지인들이 많아 관련된 에피소드가 많다는 의견
          + 지역 유럽망에 관한 Twilio와 유사한 서비스로 이해해도 되는지 직접 질문
     * dreamlook.ai에서 2인 체제로 텍스트-이미지 모델의 파인튜닝 API 운영 경험 공유. 3년 전 론칭 당시엔 TPU로 더 싸고 빠르게 교육시킬 수 있다는 차별점이 있었지만, 최근에는 GPU가 많이 따라오고 오픈소스 경쟁도 심해짐. 현재 월 5천 달러 매출, 이미 거의 손을 떼서 이 정도면 괜찮지만 1년 전에 비하면 매출이 많이 줄어듦. 비기술적 과제가 더 힘들었으며, 우리는 기술 중심이 좋아서 API 퍼스트 제품을 고집했지만 마케팅, 영업지원 등에서 어려움 겪음. 지금은 다시 대기업 ML 개발자로 돌아와 만족하고 있음. 스스로 비즈니스를 만들어본 것은 뿌듯하지만, 지금이 더 행복한 상태
          + GPU 운영 비용 및 구축 당시 사용된 예산에 대한 구체적인 수치가 있는지 궁금증
     * 유료 고객 찾기와 관련해, Postman에서 개발자와 API 유통 플랫폼, 네트워크 소개(Postman Explore). 과금은 직접 처리해야 하지만, 네트워크 덕분에 노출 확보 가능
     * 팟캐스트 API 비즈니스 탄생 후기를 소개, wenbin의 사례를 읽을 수 있다는 정보 공유
"
"https://news.hada.io/topic?id=21257","Kan.bn - Trello의 오픈소스 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Kan.bn - Trello의 오픈소스 대체제

     * Kan.bn은 Trello의 오픈소스 대안으로 협업 중심 프로젝트 관리 도구
     * 보드 관리, 팀원 초대, 댓글, 라벨과 필터, 활동 로그 등 주요 협업 기능을 제공
     * 기존 Trello 데이터의 손쉬운 가져오기 및 활동 내역 트래킹 지원
     * 템플릿, 외부 도구 연동 기능은 곧 추가될 예정
     * Next.js, tRPC, Tailwind CSS 등 최신 스택 기반으로 개발되어 현대적 경험을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Kan.bn: Trello를 대체하는 오픈소스 프로젝트 관리 도구

     * Trello와 유사한 기능을 오픈소스로 제공하는 팀 협업 프로젝트 관리 플랫폼
     * 권한 제어, 멤버 초대, 카드 논의 등 목표 관리를 위한 협업 핵심 기능을 누구나 자유롭게 활용 가능함
     * 다른 상용 보드 협업 서비스와 비교하여 AGPLv3 기반 라이선스를 채택해 소스 전체를 자유롭게 수정·확장할 수 있음
     * 또한 Trello 프로젝트의 데이터 이전 기능 제공으로 기존 Trello 사용자가 손쉽게 Kan.bn으로 이동할 수 있는 장점

주요 기능

     * 보드 가시성 제어: 각 프로젝트 보드마다 개별적으로 열람 및 편집 권한을 제어할 수 있음
     * 워크스페이스 멤버 초대 및 협업: 팀 구성원을 워크스페이스로 초대하여 공동작업 및 실시간 소통 가능함
     * Trello 데이터 가져오기: 기존 Trello 보드를 Kan.bn으로 한 번에 Import할 수 있는 기능을 제공함
     * 라벨 및 필터 기능: 카드 태깅 및 빠른 검색 기능으로 대형 프로젝트에서도 카드 식별과 관리를 간편하게 할 수 있음
     * 댓글 및 토론: 카드별로 의견을 자유롭게 남기고 기록할 수 있어 실시간 논의에 최적화되어 있음
     * 활동 내역 기록: 모든 카드 변경 사항을 자동으로 기록/추적하여 누가 언제 어떤 작업을 했는지 확인 가능함
     * 템플릿, 외부 서비스 연동(예정) : 자주 쓰는 보드 구성 저장, 외부 툴 연동 기능이 곧 추가될 예정임

기술 스택

     * Next.js: 서버사이드 렌더링 및 모던 웹 개발 환경 지원
     * tRPC: 타입 안정성을 중시한 API 통신 구조 활용
     * Better Auth: 인증 프로세스 자동화 및 강화
     * Tailwind CSS: 유연하고 빠른 UI/UX 커스터마이징 지원
     * Drizzle ORM: 깔끔한 데이터베이스 추상화와 타입 보장
     * React Email: 사용자 메일링 용 메시지 시스템 내장

커뮤니티 및 오픈소스 참여

     * 모든 코드는 GitHub에서 완전히 공개되며 오픈소스 협업, 피드백 및 개선에 적극적인 참여를 환영함
     * AGPLv3 라이선스 기반으로 상업/비상업 목적에 관계없이 자유롭게 사용 및 배포 가능함
     * 기여를 원하는 개발자는 CONTRIBUTING 가이드를 확인 후 풀리퀘스트 제출 가능함

   칸반보드라고 해서 코드를 확인했는데 기능 분리가 잘되있고, 맘에 드는 라이브러리 구성이긴 한데 next 14 버전의 pages 방식, tailwind 3.x 으로 작성된게 아쉽네요

   추천 한방 때리고 왔습니다

        Hacker News 의견

     * 오픈소스 Trello 대체제가 마음에 드는 것이 없어서 내가 직접 만든 Kan 제품 소개임, 매우 빠르고, 무료이며 완전한 커스터마이즈 가능성 제공, 셀프호스팅 또는 관리형 클라우드 버전 선택 가능성 강조, 피드백과 버그 리포트, 기능 제안 모두 환영, 리포지토리와 클라우드, 로드맵 링크 안내 GitHub Repo Cloud Roadmap
          + 기존 오픈소스 보드인 Wekan, Taiga, Kanboard 등과의 비교 설명 요청, 각 제품 링크 공유 Wekan Taiga Kanboard
          + 기존 오픈소스 대안에서 부족하거나 아쉬웠던 점, 또는 내가 특별히 구현하고자 했던 기능에 대해 구체적으로 설명해주면 논의에 도움이 되겠다는 조언, 단순히 직접 만들고 싶어서 만든 건 문제 없지만 특별히 강조하고 싶은 기능 언급 요청
          + 제공된 로드맵내 Kanban 보드 직접 사용 후 피드백 전달, 카드 클릭 시 ""Activity""만 보이고 데이터 미표시 현상 및 여러 카드 클릭 후 브라우저의 뒤로가기 버튼 정상 동작하지 않는 문제 발견
          + 축하와 응원의 메시지, 다른 제품 대비 더 흥미롭거나 실용적이거나 특별히 유용한 점이 무엇인지 궁금함, 지금까지 개발하면서 얻은 인사이트나 예상 밖이었던 점 질문
          + 제품을 직접 만든 점이 멋지다고 생각, 기존 오픈소스 Trello 클론들에서 부족하거나 마음에 안 들었던 점과 내가 어떤 트레이드오프 또는 차별화 선택을 했는지 구체적으로 설명해주면 좋겠다는 요청
     * Next.js를 오픈소스 프로젝트에서 채택한 이유가 궁금, Vercel을 제외하면 Next.js 배포가 악명 높게 어렵다고 많이 들었음
          + Next.js 경험상 기본 배포는 매우 쉽다고 생각, 10줄짜리 Dockerfile로 distroless nodejs 컨테이너 빌드 가능성, 배포하면 바로 동작함, 성능 이슈가 커지면 점차 복잡해지고 정적 자원은 CDN 등 별도 호스팅이 효율적, 더 복잡한 최적화는 캐싱 구현, 미들웨어 분리 등, Vercel은 운영 복잡도를 줄여주지만 금전적 비용 부담이 있음, 개인적으로 전면 NextJS 서버 기능은 깊이 사용한 적 없음, 다른 프론트엔드 프레임워크가 성능 최적화 관점에서 더 쉬운지 궁금, NextJS 배포자체가 어렵기보다는 Vercel만큼 극한 최적화와 관리에 노력 필요성 인식
          + Next.js는 노드 앱이라서 장기간 서버에 배포가 전혀 어렵지 않다고 생각, 다만 지난 기간동안 서버리스 환경에 고급기능 적용에 있어서 역공학이 필요했던 건 사실이고 해당 문제는 이슈에서 점차 개선 중임
          + Next.js 배포가 어렵다는 의견은 지나치게 과장됐다고 생각, 정적 자산 별도 오리진에서 서비스한다든지, 이미지 최적화 등 고급 기능을 쓸 때 복잡한데, 이런 기능들은 사실 Next.js 대체 프레임워크들도 자체적으로 크게 지원하지 않는다는 점 강조
          + Docker화해서 배포하는 난이도는 여타 앱과 별 차이 없음, Fargate에 배포도 2시간 내에 가능성
          + Next.js 배포가 어렵긴 하지만, 사실 규모 있는 어떤 서비스도 배포는 원래 어렵다는 생각, 개인적으로 힘들었던 건 Next.js 자체가 아니라 익숙하지 않은 오픈소스 소프트웨어나 docker, kubernetes, 데이터베이스, 통합 문제 등이라고 생각
     * Trello 요금제가 최근 꽤 합리적으로 떨어졌지만, 자동화 내 조건부 로직 같은 기능만 생긴다면 플랫폼 이전을 고민할 수 있을 것 같음, 참고로 Trello 월 $5이기 때문에 가격 경쟁이 쉽지 않을 가능성
          + 미국 기업 신뢰에 대해 경계심 표출, 언젠가 무역전쟁에서 쉽게 제재 도구로 돌변할 수 있기 때문임
     * 다소 황당하게 들릴 수도 있지만 오픈소스 “올인원” 앱을 원함, 해당 제품이 Slack 대체제(채널별 칸반보드/HTTP 봇 API/차트 및 대시보드/Python 노트북 스니펫 등)로 진화해서 모든 기능을 한 곳에서 처리할 수 있다면 정말 바랄만한 제품
          + 스코프 크리프(Scope Creep) 가능성 지적
     * 예전에 Kanboard를 사용한 경험 공유, PHP로 작성됐고 디자인은 부족했지만 기능은 꽤 쓸만했고 데이터베이스가 필요 없는 점에서 설치 쉬움 Kanboard
          + 공유호스팅에 간단히 올려 바로 실행 가능성, UI는 촌스러우나 기능적으로 매우 훌륭함
          + 지금도 Kanboard를 사용 중이며, 단순함 때문에 상당히 좋아함
     * 제안 사항으로 가족 요금제를 낮은 가격으로 도입 권유, Kanban 보드는 정말 좋아하지만 네이티브 앱에 간단 동기화(예: iCloud/Dropbox) 지원까지 있으면 더 좋음, 자체호스팅 혹은 SaaS형은 끌리지 않음, 예전에 윈도우 무료 데스크탑 Kanban 앱을 참 잘 썼음(지금은 단종), Apple Reminders의 리스트 섹션 기능도 Kanban처럼 활용은 가능하지만 UX가 부실함, 특히 macOS 버전이 매우 안 좋음, 단순한 동기화 기능을 제공하며, 1~2명 규모에 잘 맞는, 구독 없는(버전별 일회성 구매) 네이티브 앱을 찾는 중임
     * “Kanban reimagined”라는 슬로건은 특이하게 느껴짐, 실제 기능은 Trello, Taiga 등과 크게 다르지 않으니 동일 제품을 다시 만든 것처럼 느껴짐, 오픈소스 대안 자체는 환영함
     * 프로젝트 퀄리티도 괜찮아 보이지만 도메인 이름이 상당히 인상적임
     * Trello 자체를 아주 좋아함, 시각적으로 매력적이고, 키보드 단축키와 빠른 인터페이스, 강력한 자동화 API 제공, 다양한 자동화 시나리오 직접 구현 가능성, Kan 제품도 확인해볼 예정임
          + 예전 Trello 경험 상 인터페이스가 빠르다고 느끼려면 카드 수가 적거나 미디어 사용이 거의 없었을 것 같음, 예전엔 카드 처리에 몇 초씩 걸릴 정도로 느렸던 기억이 있음
     * 14년 넘게 Trello를 써왔으며, 가장 큰 문제점은 아래 두 가지임
          + 사용자가 카드/리스트/보드 삭제 시 팀 전체에서 완전히 사라짐(복구 불가성)
          + 보드를 비공개(로그인 필요 없는) 링크로 공유하고 댓글 정도만 허용하고 싶은데, Trello에서는 이런 권한 설정 불가함
"
"https://news.hada.io/topic?id=21287","Merlin Bird ID","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Merlin Bird ID

     * Merlin Bird ID는 조류 인식과 탐색에 특화된 도구임
     * 사용자들이 조류 사진이나 소리를 통해 빠르게 종을 식별할 수 있게 함
     * 일반인과 전문가 모두가 쉽게 활용할 수 있는 사용자 친화성 제공임
     * AI 기반 조류 인식 기능으로 다양한 환경에서 정확한 정보 확인 가능함
     * 조류 보호와 시민 과학 연구에 적극적 기여 가능성 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Merlin Bird ID 개요

     * Merlin Bird ID는 전 세계적으로 사용되는 조류 인식 및 종 탐색 플랫폼임
     * 사용자는 휴대폰이나 웹사이트에서 조류 사진 또는 소리를 업로드하여 빠르게 해당 새의 종을 식별할 수 있음
     * All About Birds에서 제공하며, 초보자와 전문가 모두를 위한 직관적이고 쉬운 사용자 경험을 목표로 설계됨
     * 알고리듬과 머신러닝 기술을 이용해 다양한 지역, 환경에서의 정확한 조류 정보 접근성을 제공함
     * Merlin Bird ID는 시민 과학, 환경 보호, 그리고 조류 연구 활동에 큰 기여를 할 수 있음

사이트 또는 연결 안전성 안내

     * 본 사이트를 이용하려면 JavaScript와 쿠키 허용이 필요함
     * 안전한 연결을 위해 보안 검토 및 자동 인증 과정을 거침
     * 이러한 보안 절차는 사용자의 개인 정보와 데이터 보호 목적임

요약

     * Merlin Bird ID는 AI 기반 조류 인식, 종 탐색에 강점을 지닌 글로벌 플랫폼임
     * 간편한 사용법과 향상된 알고리듬 덕분에 자연 관찰, 교육, 과학 활동 지원에 유용함
     * 다양한 환경과 국가에서 정확도 높은 식별 기능 제공함

        Hacker News 의견

     * 나는 이 앱을 정말 좋아하는 사람임. 한 달 전쯤 큰 소리로 지저귀는 새가 누구인지 궁금해서 사용하기 시작했는데 Tufted Titmouse였음(지금은 내가 가장 좋아하는 새 중 하나, 엄청 멋진 외형임)과 Carolina Wren이었음(외형은 좀 덜하지만 노래로 충분히 매력 있음)
       최근 한 달간 아침 일상이 완전히 바뀌어서 실내가 아닌 뒷마당에서 보내게 됨
       새 모이통 채우고 청소하고, 다람쥐를 위해 땅콩도 두고, 커피와 아침 식사를 deck에 앉아 MerlinID 앱과 함께하는 루틴임
       새로운 인간 언어는 공부 못하지만, 이 앱을 꾸준히 쓰면서 이제는 새 종뿐 아니라 개별 새의 독특한 울음소리나 목소리까지 구별 가능해진 상태임
       유일하게 바라는 점은, 개별 새를 기록하고 해당 새의 녹음과 사진을 함께 저장하는 기능이 있었으면 하는 바람임
       이 앱 개발자들에게 박수를 보내고, 오랜만에 써본 최고의 앱이라는 말 전하고 싶음
          + 내가 무슨 뜻인지 잘 모르겠음 — ""Explore"" 탭과 Life List에서 사진과 울음소리를 함께 볼 수 있음
            정말 좋은 앱인데, 개인적으로 아쉬운 점은 Life List에 새를 추가하는 UX가 너무 별로라는 점임
            소리나 사진으로 식별하면, 언제 어디서 만났는지 묻는데 기본값이 정말 별로임 — 지금 위치랑 시간을 알고 있으면서도, 몇 달 전에 있던 예전 위치나 시간을 랜덤하게 불러오기도 함
            바로 전에 현재 위치로 몇 마리나 입력했는지도 무시함
     * 내게 Merlin은 ""모두의 주머니 속 컴퓨터""가 진짜 뭘 할 수 있는지 제대로 보여 주는 예시임
       엄청난 앱이고, 많은 사람들이 자연과 더 가까워지게 해줌
       가끔 운영 단체에 후원하라는 안내가 오긴 하지만, 명확하고 직접적인 요청이고, 이상한 꼼수 마케팅이나 다크 패턴은 보이지 않음
          + 정말 동감임! 보통 앱들은 사용자를 가상의 세상에 가둬두려 하기 십상인데, Merlin은 오히려 진짜 세상(자연)으로 사용자의 관심을 돌려주는 보기 드문 사례임
            특히 소리 식별 기능은 누구든 쓸 수 있지만, 실제로는 불가능했던 경험을 가능케 해주고(귀로만 종을 구분), 자연 속에서 소리에 더 집중하도록 안내함
            개인적으로 Merlin, 특히 eBird가 조류 관찰을 점수 경쟁처럼 유도하는 경향이 아쉽다고 생각함 — 새 개체수 감소 현상을 고려할 때, 전 세계를 돌며 최대한 많은 종을 보는 걸 독려하는 건 문제일 수도 있음
            하지만 아주 작은 불만이고, Merlin은 누구에게나 자신 있게 추천할 수 있는 몇 안 되는 앱임
          + iNaturalist도 또 다른 보석 같은 앱임
            해킹에 관심 있는 사람은 이런 앱들이 AllTrails, Couchsurfing처럼 상업화되지 않게 적극적으로 지원해줬으면 좋겠음
     * 이런 앱이 주목받는 게 정말 반가운 현상임
       개발자나 PM이 이 글 보고 피드백도 챙겼으면 하는 바람임
       특히 sound ID가 콜롬비아 정글이나 숲처럼 네트워크가 전혀 없는 현장에서 기대 이상으로 잘 작동함
       다만 앱의 다른 부분은 많이 미흡함
       버튼이 자주 먹통이고 화면마다 일관성이 없으며 결과가 사라지는 등 여러 문제가 있음
       iNaturalist와 연동 같은 기능도 있으면 좋겠고, 예전 녹음을 다시 열면 정작 당시 식별이 떴던 새가 ""No matches""로 나옴
       자연에서 여러 명이 그룹으로 다니다가 식별에 성공했는데, 누군가에게 보여주려 할 때 이미 사라진 경우가 많음
       간편 피드백 버튼 정도만 추가되어도 과정을 크게 개선할 수 있을 것 같음
       이 앱의 모든 개발자와 참여자들에게 깊은 감사의 마음을 전함
          +

     버튼이 안 먹고, 화면 일관성, 결과 날아감 등 문제
     궁금해서 묻는데 어떤 기기 쓰는지? 난 여러 세대의 Google Pixel에서 몇 달간 매일 사용 중인데, 이런 문제는 단 한 번도 겪은 적 없음
     ""식별했는데, 나중에 보여줄 땐 결과가 사라짐""이 무슨 뜻인지도 잘 모르겠음 — 녹음 도중 식별이 떴는데, 중지하면 결과가 사라진다는 말인지?
     내 경험으론 10분 넘는 긴 녹음을 할 때만 약간 렉이 생기는데, 그냥 끊고 새로 녹음 시작하는 팁을 쓰고 있음
          +

     콜롬비아 정글/숲에서도 sound ID가 잘 됨
     흥미로운 경험임
     내 경우 sound ID가 잘 되려면 (1) 폰을 주머니에 넣지 않고 공기 중에 두고(마이크 막히면 인식 안 됨), (2) 직접 움직이지 않아야 하며(내 발소리가 아무리 작은 새 소리도 다 방해), (3) Merlin 앱을 ""초점모드""로 해야 됨 — 앱이 백그라운드에 있으면 아예 안 잡히는 느낌임(이건 단순 나만의 착각일 수도 있음)
     가끔 큰 새를 바로 머리 위에서 들어도 인식 못 하다가, 멀리서 희미하게 들리는 건 아주 정확히 집음
     개인적으로 sonogram(음향 스펙트로그램)에서 AI가 식별에 활용한 부분을 눈에 띄게 표시해줬으면 좋겠음
     여러 새가 동시에 노래하면 이 sonogram을 해석하기가 특히 어려움

     앱 나머지 부분은 많이 미흡
     난 오히려 전혀 못 느꼈고, 이건 아마 폰 기기 차이일 수도 있음
     어쨌든 좋은 앱임

     * 앱 정말 사랑하는 사용자임
       하지만 Android에서 시작 후 정확히 255초가 지나면 항상 크래시가 남(직접 시간까지 재봄)
       버그 리포트를 남길 방법이 있는지 궁금함
     * 내가 유럽에 있을 때 2번째 지역을 추가하려다가 앱이 완전 멈춤
       재설치해야만 다시 쓸 수 있었음
       iOS에선 지역 정보 추가가 완전히 고장난 상태임
     * 내 친구들이 Sound ID 부분 연구팀에 참가 중임
       팀이 정말 많은 노력을 들여 머신러닝 모델과 평가가 제대로 동작하게 만들었음
       Sound ID는 흔히 볼 수 있는 ""적당히 데이터만 넣어서 대충 훈련시키는"" 접근이 아니라, 도메인 전문가의 세밀한 연구가 더 신뢰할 만한 결과를 줄 수 있음을 보여준 좋은 예임
          + Sound ID 기술이 매우 인상적임
            언젠가 내 집 밖에서 여러 거리에서 새들이 동시에 엄청나게 시끄럽게 지저귀고 있었는데, 이 앱이 30초 만에 6종류를 정확히 집어냄
            나름대로 저 6가지 다 합리적으로 보였음
          + ""Fred"", ""Bertha"", ""Kevin"" 이런 식으로 개별 새마다 고유 이름을 자동 할당해주는 기능도 있었으면 함
            내 정원에 자주 오는 새들은 다 종류를 알고 있으니, 이제는 어떤 새가 계속오는지, 짝이 바뀌는지, 좀 떨어진 곳에서도 ""아 이 Kevin이구나""라는 식으로 구분하고 싶음
            검은새 같이 비슷한 개체가 많은 종은 어려울 수도 있겠지만, 어떤 종은 AI로 충분히 가능할 것 같음
          + 오픈 API가 있었으면 함
            나는 새소리로 새를 식별하는 몇 가지 아이디어가 있는데, Merlin의 인식 기술이 외부에 API 형태로 공개된다면 활용에 최고일 것 같음
          + 내 집 근처에 매일 아침 울어주는 수탉이 있음
            매번 앱에서 수탉 ID가 찍히나 기대해도 매번 안 뜸
            연구팀 친구들에게 꼭 “수탉도 새다! 세계 최고의 알람이니 식별 대상에 넣어 달라”는 메시지 전달해줬으면 함
          + 정말 놀라운 앱이지만, 종종 false positive(잘못된 식별)도 많이 나오긴 함
            아마 이건 기술적으로 어쩔 수 없는 한계일 것 같음
     * 새 노래를 앱으로 재생할 땐 정말 조심해야 함
       며칠 전 식별 목록에 떴던 cardinal 노래를 우연히 눌렀더니, 내 울타리 근처에 둥지 튼 수컷 cardinal이 엄청난 반응을 보이고, 그날 이후 두 마리 모두 한 번도 안 보임
       매일 보던 새인데, territory(영역) 의식이 강해서 그런지 단순히 앱으로 노래 재생한 것 때문에 둥지를 떠난 것 아닌지 걱정임
          + 몇 번이나 ""엉뚱하게 Merlin 앱을 주머니에서 무심코 켜짐"" 상황을 겪은 적 있음
            항상 그럴 때마다 “어 저 새 진짜 가까이 있는데?” → “폰 꺼내서 Merlin으로 알아볼까?” 하는 순간 벌어짐
          + 조류 관찰하면서 ""calling back""이라는 용어를 들었음
            앱에서도 이런 경고를 더 강조할 필요가 있다고 생각함 — ""새를 부르기 위해 이 기능 쓰지 마세요, 본인 참고용으로만 들으세요"" 같은 문구
            만약 내가 새인데, 낯선 새소리가 일정 감정이나 의미를 직접 내 포유류 인간이 큰 덩치로 근처에서 트는 모습을 보면 내 가족을 이주시키는 게 당연한 반응일 것 같음
            Cardinal은 특히 territory 성향이 강해서 둥지 시기에 서로 내쫓음
            휘파람, ""pishing"", 폰 사용 등 모두가 자연스러운 행동을 방해함
            자동차, 트럭, 잔디깎기, 오토바이 등 온갖 소음도 새들에 큰 영향임
            ""Sibley Guides""의 playback proper use 자료를 참고하면,

     특정 세력권에서 노래를 재생하면 그 소리에 이웃 새와 암컷 전부가 주의 깊게 반응하고, 실제 익명의 침입자를 쫓아내지 못하면 그 개체의 서열이 하락됐다라는 연구 결과도 있음
     기술적으로 우리가 그들의 언어를 복제하고 재생 가능해졌고, 실제로 새 가족 해체 같은 영향이 있음

     * 흥미로운 점 하나 소개함

     Sound ID는 오디오를 비주얼 스펙트로그램으로 변환한 다음, 컴퓨터 비전 도구(Photo ID에 쓰인 기술과 유사)로 분석함
     즉 앱 최상단에 스크롤되는 스펙트로그램은 단순한 장식이 아니라 실제 인식 메커니즘임

     * 멋진 사실임
       나는 오래전에 언어학 실험실에서 스펙트로그램을 분석해서 음소의 조음 위치를 파악하곤 했음
       따라서 다른 음향표현보다 이를 모형 학습에 쓰는 게 당연하게 느껴짐
     * 스펙트로그램은 실제 수동 식별이나, 개별 울음소리를 기억하는 데도 실용적임(최소한 내 경우엔)
     * 오디오 신경망 대부분이 스펙트로그램을 기반으로 동작함
     * 난 몇 년째 새 관찰을 꾸준히 해왔지만, 올해 처음 사운드 식별 앱을 쓰게 됨
       덕분에 소리만으로 새로운 종을 12개 넘게 찾았고, 일부는 나 혼자서도 울음소리만 들어서 알아낼 수 있게 됨
       이런 기술 덕에 이 취미를 완전히 다르게 즐길 수 있게 됨
       적어도 이 분야에서 기계학습/AI가 분명히 긍정적인 영향을 준 사례임
     * 나도 이 앱을 엄청 애용하는 팬임
       하지만 불편한 점 하나는 컴퓨터 PC에서 이미지를 웹으로 올릴 수 없는 부분임
       DSLR로 새 사진을 찍는 이용자 입장에선, 이미지 전송만 해도 너무 번거로움
       이건 치명적이진 않지만, 결국 나는 다른 방식으로 새를 식별할 수밖에 없음
       비 모바일 UX 옵션이 추가된다면 정말 유용할 것임
          + 나는 Birder Framework(조류 분류 컴퓨터 비전 툴킷) 저자임 — https://gitlab.com/birder/birder
            아직 초기 알파지만, DSLR 촬영사진 워크플로에 도움될 수 있음
            웹 기반 대안이 필요하다면, Hugging Face 데모 공간도 있으니 바로 브라우저로 이미지를 업로드해서 써볼 수 있음
            다만 이건 모델 시연용이고, 사용 경험 자체보단 각 지역(유럽, 아라비안 반도 등)에 따라 수동으로 모델을 고르는 불편함이 있음
            아직 지역 커버리지가 한정적이지만 점점 더 추가될 예정임
          + 나도 DSLR로 새 사진을 찍음
            식별이 필요할 땐 그냥 모니터를 스마트폰으로 찍어서 Merlin에 넣음
            48MP 사진은 필요 없고, 그냥 스크린샷으로도 충분함
          + 나도 이 방법 씀
            개인적으로 게시해도 좋다 생각되는 사진만 식별하고, 올리고 나면 폰으로 접근 가능(Flickr, IG 등)하니 쉽게 가져와서 앱에 쓰면 됨
     * 나는 일반적인 조류 관찰자 입장에서 이 기술에 엄청난 감명을 받고 거의 다 신뢰하는 편임
       단지 아주 비슷한 종이 등장하는 경우만 약간 의심가는 판단이 있을 수 있음
       예를 들면, 집 주변에 Purple Finch가 실제로 존재하는지는 미심쩍음(맨날 House Finch만 보이니까)
       그래도 Merlin의 신뢰성 덕분에 내가 틀렸을 수도 있다 생각할 만큼 믿게 됨
          + 확실히 비슷한 종은 가끔 잘못 잡아내는 경우가 있음 — 최근엔 까마귀, Warbler에서 눈에 띄었음
            그래도 대부분 정확한 결과고, 의심갈 땐 직접 새를 봐도 그만임
          + 내 집 주변에 Blue Jay가 항상 많은데, 이 새들이 특정 매 종류의 울음소리를 기가 막히게 흉내내서 Merlin에서도 종종 매라고 식별하는 현상이 있었음
            실제 매 소리를 들어도 Jay의 흉내와 거의 구분 못 했음
            이론상 진짜 매일 수도 있겠지만, ""bald eagle"" 식별은 동네 애들이 소리 지른 걸 헷갈렸다는 게 더 설득력 있음
          + 내가 영국에 살면서 Merlin의 일반적인 오류는 핀치류(특히 greenfinch의 ""at rest"" 트위터)를 goldfinch로 착각하는 경우였음
            두 종은 외관으로는 확연히 구별 가능하지만, 귀로는 나조차도 구분 어렵기 때문에 Merlin을 탓하긴 힘듦
     * 이런 종류의 앱을 정말 사랑하는 사용자임
       나는 WhoBird를 애용하는데, 이 앱은 내장식(완전 오프라인)으로 동작하며, 인터넷이 없어도 잘 되고 fdroid에도 등록되어 있음(https://f-droid.org/packages/org.woheller69.whobird/)
       Merlin도 체험해보고 두 앱 성능을 비교해 볼 계획임
          + Merlin 역시 오프라인에서도 완벽하게 동작함
            나도 산속에서 항상 오프라인으로 쓰고 있음
          + 내 알기로 WhoBird도 같은 모델(혹은 Cornell 개발) 기반이지만, Merlin이 여러 마리 동시 식별 성능은 여전히 더 뛰어남
          + WhoBird는 오디오(소리)에만 동작하고 사진 식별은 지원하지 않음
"
"https://news.hada.io/topic?id=21176","UI의 미래는 다채롭고 입체적인 형태임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         UI의 미래는 다채롭고 입체적인 형태임

     * 최근 Airbnb의 리디자인에서 볼 수 있듯이, 플랫 디자인 시대가 지나가고 새로운 입체적이고 다채로운 디자인 흐름이 등장함
     * 기존의 스큐어모피즘과 플랫 디자인을 넘어선 새로운 접근을 위해 Diamorph(다이아모프)라는 용어가 소개됨
     * 이 Diamorph 디자인 트렌드는 깊이, 질감, 빛, 계층 구조를 강조하며 스크린에 특화된 스타일을 지향함
     * AI 기술의 발전으로 숙련된 기술 없이도 이와 같은 입체적 디자인을 쉽게 창작할 수 있는 환경이 조성됨
     * 앞으로는 입체적이고 창의적인 인터페이스가 대중화될 것이며, UI 디자인 업계에 새로운 전환점이 마련됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 플랫 디자인의 종말과 새로운 패러다임

     * 현대 UI 디자인에서 플랫 디자인의 시대가 끝나고, 입체적이고 다채로운 비주얼의 미래가 도래함
     * 이 변화는 Airbnb의 대규모 리디자인에서 두드러지게 나타나며, 사용자 경험에 유쾌함과 촉각적 감각을 추가함
     * 과거 iOS 7 등에서 경험한 패러다임 변화처럼, 최근 몇 주간 새로운 디자인 흐름의 전환점이 감지됨

디자인 언어의 재정의: Diamorph

     * 기존 스큐어모피즘(skeuomorphism) 용어는 물리적 오브젝트를 단순히 흉내내는 디지털 메타포를 의미했으나, 이제는 깊이, 텍스처, 조명까지 포괄하는 잘못된 용어로 확장되어 혼란을 줌
     * 저자는 이런 새로운 입체감과 디테일을담은 디자인 흐름을 설명하기 위해 Diamorph(다이아모프)라는 신조어를 제안함
     * Diamorph란, 깊이·빛·질감·계층 구조를 스크린 환경에 맞춰 적극 활용하는, 표현력 있고 디지털에 최적화된 디자인을 의미함
     * Diamorphism은 의도적으로 입체감을 추구하는 트렌드, 즉 계층적이고 촉각적이며 디지털에 특화된 스타일을 가리킴
     * 이 용어가 정착될지 미지수이나, 변화의 본질을 논의하기 위한 임시적 언어로 활용 중임

변화의 징후와 기대

     * Big Sur 아이콘, 다양한 phism 트렌드, 플레이풀한 마이크로 인터랙션, 풍부한 조명 모델 등에서 이러한 변화가 단계적으로 관찰되어 왔음
     * 곧 있을 WWDC 등 큰 무대에서 애플이 현실감 있는 소재 비주얼을 도입할 가능성도 염두에 두고 있음
     * 이제 플랫 대 스큐어의 이분법적 논쟁을 넘어서, 새로운 입체적 디자인 패러다임을 수용할 시점임

AI가 가속하는 Diamorph 디자인

     * Airbnb의 리디자인 이후 인터넷에서는 AI 기반 생성형 디자인을 활용한 입체적 아이콘 세트가 폭발적으로 등장함
     * 과거에는 빛, 소재, 깊이 표현 등 고난도의 UI 디자인 기술이 필요했으나, 이제는 AI 프롬프트만으로 누구나 수준 높은 결과물을 만들 수 있음
     * 저자는 오랜 기간 이 스타일을 수작업으로 실현해왔으며, 지금은 AI를 툴로 활용해 보다 쉽게 실험 중임
     * AI는 색상·재질·조명 처리에는 강점을 보이지만, 투시·비율·일관성 유지는 아직 도전적임. 후처리와 검증 과정이 여전히 필요함
     * 투명한 배경과 약간의 보정만 거치면 실제 사용에 충분한 품질을 얻을 수 있어, 진입 장벽이 크게 낮아짐
     * 예시로, AI로 생성한 레트로 콘솔 아이콘을 활용해 macOS 앱의 목업도 신속히 제작 가능함

디자이너와 도구의 변화

     * 저자는 전통적 수작업 디자인 경험을 강조하며, 핵심 디자인 역량(구성, 조명, 깊이, 취향)이 여전히 중요함을 언급함
     * 새로운 도구(예: AI)는 창의성 극대화를 위한 수단일 뿐, 완성품을 위한 단순 지름길이 아님
     * 도구는 변화해도 디자인 감각과 취향을 대체하긴 어려움

결론: 표현력 있는 입체 UI로의 전환

     * 지금은 감정적이고 디지털에 최적화된 새로운 시각 언어가 탄생하는 시점임
     * Diamorphic 디자인은 복고가 아니라, 진일보한 변화임
     * AI와 같은 도구로 진입장벽은 낮아졌으며, 더 많은 참여자가 새로운 UI 디자인 대화를 이끌 전망임
     * 앞으로의 UI는 깊이, 질감, 유희성을 지니는 방향으로 진화할 것이며, 이는 이미 진행 중임

        Hacker News 의견

     * 새로운 생각은 아니지만 이런 상황에서 집단적 사고방식이 얼마나 자주 등장하는지 항상 놀라운 감정. Airbnb가 새로운 디자인을 발표했더니 우리는 다 같이 그냥 그게 미래라고 받아들이는 분위기. 개인적으로 그 트렌드가 싫진 않지만 감동은 없는 감정. Airbnb 사이트 들어가보니 그냥 아이콘만 바뀐 거고, 실제 사용하는 UI는 완전 똑같은 느낌. 며칠 전에 발표된 Google Material UI가 훨씬 흥미로운 감정
          + 뉴스레터 쓰는 한 명의 의견에 너무 휘둘릴 필요 없는 내용. 이 기사 보면 그저 다음 트렌드를 '내가 말했다'고 남기고 싶은 욕심으로 보이는 사람이라는 인상. 이게 진짜 다음 트렌드라는 근거도 없고, 그냥 누가 이름 붙이면 인플루언서가 될 수 있을까 시도하는 모습. 본인이 예전부터 이 스타일 아이콘을 만들었다고 하는 것도 살짝 자기 바람 투영 같은 의심이 드는 부분. 디자이너로 일하지만, 이런 스타일의 아이콘을 꼭 쓰라는 내부 지침 받은 적도 없는 사실. 혹시 그런 메모 받게 되면 알려주겠다는 말
          + 실제로 확인 가능한 건 아이콘 네 개 정도만 바뀐 것 같고, 사실상 2000년대 초반에 존재하던 3D 아이콘을 다시 꺼내 쓰는 느낌이라는 생각. 나도 확신이 없는 입장
          + 내가 확인한 바로는 새로 바뀐 건 다섯 개 아이콘 정도뿐이고, 나머진 여전히 평면 아이콘. 솔직히 그 다섯 개도 못생긴데다 영락없이 2000년대 초반 느낌이 드는 외형
          + 회사 쪽에서 아직 디자이너들이 계속 반복해서 비슷비슷한 걸 바꾼다는 걸 모르는 듯한 상황. 그렇지 않으면 우리 할 일이라고는 아무것도 없음. 디자이너로서 아이콘 바꾸고, 버튼 키웠다가 다시 줄이고, 그라디언트 넣었다 뺐다 하는 게 우리가 바쁜 거의 전부라는 현실. 대부분의 회사 제품은 이미 우리가 입사할 때 디자인 시스템이 완성되어 있는 상황
          + 새로 나온 아이콘 못생겼고 Sims 1에서 뜯어낸 거 같은 느낌. 그냥 디자이너들이 회사에서 자기 존재를 증명하려고 애쓰는 모양
     * 업계가 유능한 디자이너들을 UI 분야에서 쫓아내고 모두에게 못생긴 플랫 스타일을 강요하더니, 이제는 스큐어모피즘으로 다시 사이클 돌리려는 분위기. 과거 그 스타일을 할 수 있었던 디자이너들을 AI로 대체시키려는 느낌
          + 최근 우리 팀에서 겪은 비슷한 얘기 생각나는 부분. 내부적으로 사용하던 시스템은 2001년쯤 UI에 멈춰 있는 상태로, Motif와 GTK1 조합 느낌의 못생긴 화면과 클립아트 모음 아이콘들. 새로 리뉴얼하려고 외부 컨설팅 맡겨서 React 기반 평면 디자인으로 만들었더니, 런칭 1주일 뒤 유저 설문 결과 대부분 사람들이 새로워진 평면 UI에 불평. 결국 디자인을 다시 옛날 모양으로 빨리 돌려놓았다는 이야기. 이제 최신 React 앱이 2001년 감성으로 보이게 된 상황. 플랫 UI가 진짜 사용자들이 좋아하는지, 아니면 사용자 조사 자체가 엉터리인지 의문
          + 플랫 디자인이 10년 전쯤 사라졌으면 했던 사람. 플랫 디자인은 직관성도 없고 보기에도 못생겼고, 스타일이라고 말할 수도 없는 반인간적인 디자인이라는 생각. 앱이나 UI에 일부 추상적인 아이콘이 남아있는데, 무슨 기능인지 클릭해서 확인해보는 수밖에 없는 현실. 어떤 요소가 클릭 가능한지, 아닌지 구분도 잘 안 되고, 일관된 색상이나 대비마저 일부 디자이너가 죄악으로 여기는 듯한 상황. 플랫 디자인은 올바른 디자인 원칙과 정면으로 반대되는 존재. 40년 후에 새로운 세대들이 '전통 회귀' 외치며 2000~2020년대 스타일로 돌아가지 않길 바라는 생각. 아름다운 소프트웨어와 함께 나이 들고 싶음
          + 논의 중인 HN 유저들의 관심이 스큐어모피즘과 플랫디자인, 두 가지만 가지고 다투는 분위기처럼 보이는 부분. 스큐어모피즘은 진짜 물리적 메타포로 현실을 과도하게 흉내 내는 UI에 붙는 용어여야 하고 실제로 그렇게 디자인된 건 많지도 않고, 크게 인기를 끈 적도 없었던 것. 두 스타일은 훨씬 더 넓은 디자인 공간 중 하나의 포인트일 뿐이며, 매번 둘 사이를 번갈아가며 돌 필요 없다는 입장. 나아가다 보면 자기실현적 예언이 되어버림. 다양한 선택지가 존재
          + 언젠가 AR 글라스에서 완전히 구현된 아르누보풍 벨보이가 텔레그램을 전해주는 초스큐어모픽 미래가 기대되는 부분
          + 업계 전체가 변화 시도하는 단계가 아니라, 사실 이건 Airbnb와 한 두 명이 언급하는 정도일 뿐인 느낌
     * 디자이너들이 현실과 동떨어져 있다는 생각. Comic Sans에 대한 과한 비판도 한 예. 점점 느끼는 결론은 디자이너를 고용하는 대신 프로젝트별로만 자문 받아야 한다는 입장. 회사에 8시간 앉아 있으면 뭔가를 바꿔야 자기 존재를 증명하게 되고, 인간은 급격한 변화에 익숙하지 않은 존재. 익숙해지는 데 시간 필요
          + 좋은 디자인이 기능에 따라야 한다는 입장. 새 ThinkPad 사면서 ""왜 카메라 범프를 만들었냐"", ""왜 끝이 둥근 팜레스트를 없애고 모서리를 날카롭게 했냐"", ""왜 디스플레이 뚜껑 열기 쉽게 한 요철을 없앴냐""와 비슷한 말 반복. 디자인이 기능을 따르지 않으면 해로운 존재. 나도 새 ThinkPad에 카메라 범프 달린 것 싫고, 넓은 뚜껑에 굳이 돌출된 부분 없어도 다 들어갈 수 있는데 굳이 왜 추가했는지 의문. 소프트웨어 UI 바뀌는 것도 마찬가지로 변화를 위한 변화 자체는 이득이 없음
          + Comic Sans 이야기를 대표적으로 언급한 것에 주목. 역설적으로 Comic Sans가 딱히 쉴드쳐질 수 있는 폰트가 아니라는 입장. 심지어 난독증에 효과 있다고 해도, 접근성에 더 좋은 대안들이 이미 존재. 디자이너의 하루 일과에 대해 실제로 아는지 의문. 실제로는 회사 규모와 상관없이 디자이너들이 숫자만 세는 존재가 아니라는 이야기
          + 대부분의 디자이너들은 그런 식으로 8시간 내내 의미 없이 변경하고 있지 않고, 오히려 대부분 조직에서 과도하게 일하고 있어 불필요하게 새 트렌드를 찾아내거나 시도할 시간 자체가 부족한 현실. 실제로 이런 변화는 디자이너가 아닌 중역들이 “신선함”을 부여하려고, 디자이너가 원치 않는 방식으로 강요하는 경우가 많다는 경험. (내 배우자가 UX 디자이너라 자주 듣는 내용)
          + 요즘 디자이너들이 미적 감각을 실용성보다 더 중시하는 경향. ""더 사용자 친화적으로 어떻게 만들까""에서 출발하면 지금과는 완전히 다른 답이 나오는 반면, ""어떻게 남들과 비슷하면서도 튀게 할까""에서 출발하면 일반적인 결과가 나옴. UI에서 미적인 요소는 본질적으로 부차적이어야 한다는 생각. 디자이너들이 실용성보다 미적 감각을 우선시하기에 이토록 자주, 무의미한 재디자인이 반복됨. 진짜 실용성을 중요시한다면, 재디자인은 사용자의 수많은 익숙함을 끊어내는 큰 비용이라는 사실을 명확히 인지해야 하고, 정말로 심각한 이득이 있을 때만 정당화될 수 있는 과정
     * 과거가 다시 새로운 것으로 등장하는 상황. 패션 업계는 이미 수십 년 전부터 알던 원리이고, 테크 업계가 그걸 따라가는 모습. ""Dimensional"" 디자인 다음에 올 트렌드는 극단적인 미니멀리즘. ""여분과 장식을 모두 걷어내고 남은 것은 진실뿐""이라는 2030년 디자인 트렌드 슬로건까지 미리 생각해본 경험
          + 방금 전 온라인 뱅킹 UI에서 ""이체"" 버튼을 눌렀는데, 예쁜 폰트와 미세한 그라데이션 바탕이 인상적. 하지만 버튼을 눌러도 눌린 표시가 전혀 안 보여서, 로딩 스피너가 뜰 때야 버튼이 눌렸구나 인식. 개인적으로 Windows 98/2000이 UI의 정점이라고 생각. 단조롭고 균일하지만 적당한 입체감 덕분에 어디를 누르면 제대로 작동할지 알 수 있었던 기억. 키보드 단축키도 물론 포함
          + ""한때 디자인에서 진리로 통하던 것이 안주와 평균으로의 굴복으로 대체됐다. 우리가 제공하는 것은 대안이고, 대담하고 복잡한 디자인, 장인정신과 디테일의 집착, AI 시스템과는 비교할 수 없는 인간 중심의 세밀함이다"" 2035년 디자인 트렌드 설명까지 추가로 상상
          + 사이클이 아니라 나선형 진화라는 의견. 각 아이디어는 이전 것에 부분적으로 과도한 반응이 뒤따름. 예를 들어, 과거 UI가 콘텐츠를 압도할 정도로 화려해서 플랫 아이콘이 생긴 거고, 그 결과 아이콘 인지성 대신 계층구조가 더 명확해졌음. 이 문제가 해결되니 이제는 눈에 띄는 아이콘을 일부러 다시 넣으려고 하는 흐름. AirBnB 앱에서도 주요 포커스에는 바쁜 아이콘을, 보조용에는 플랫 아이콘을 사용하는 식. 참고 링크 facebook-3-5-iphone.jpg
          + 미니멀리즘이 이미 10년 넘게 주류 트렌드 아니냐는 의문. 하락세를 거치고 다시 돌아오기까지 최소 5년은 더 필요하다고 봄
          + 내 캘린더 앱에도 스티치 가죽 UI가 추가된다면 분명히 훨씬 더 쓰기 편할 것 같은 희망 사항
     * 좋은 UI/UX의 핵심은 아이콘 모양이 아니라, 사용자가 앱을 이용하며 문제를 해결하는 과정이 사용자의 생각 흐름과 맞아떨어지는지라는 지점. 우리 팀도 아이콘 고르기와 다른 앱 디자인 패턴 따라하기에 시간 너무 많이 투자한 결과, 결국 사람들이 우리 앱을 쓰는 주된 목적을 충분히 우선시하지 못하는 UX가 되어버렸던 경험
          + 가장 좋은 아이콘은 사실 알파벳, 즉 단어라는 점 추가하고 싶은 이야기. 글자끼리 조합해서 만드는 힘이 생각보다 큼
          + 아이콘 선택도 실제로 매우 중요. 플랫하고 단색인 아이콘은 정말 쓸모없다는 느낌. 컨플루언스에서 페이지 편집할 때, 예를 들어 색상 채우기 버튼을 찾고 싶어도 모든 아이콘이 동일하게 보여 쉽게 찾기 힘든 문제
     * AirBnB의 리디자인은 트렌드 변화의 신호가 아니고, 여전히 앱의 대부분이 미니멀하고 플랫 스타일을 유지하는 상태. 새로운 입체형 아이콘은 통일성 없이 드문드문 섞여 들어가 있을 뿐. 오히려 완전 플랫으로 돌아가기 더 쉬운 구조라고 생각. 실제로 입체형 아이콘은 이미 여러 앱에서 부분적으로 오랫동안 사용됐던 것. 이상하게도 ""New"" 태그에만 유독 광택이 들어가 있어서, 오히려 플랫 버튼이나 다른 플랫 태그보다 더 버튼처럼 보이는 혼란이 있는 디자인. 2000년대 초반의 UI 디자인은 조명, 재질, 깊이에 대한 고도의 기술이 요구됐었지만, 지금은 프롬프트만으로 손쉽게 얻을 수 있는 시대라는 주장에는 회의적. 여전히 실제로 어떻게든 훈련과 노력이 많이 필요한 분야라고 생각. AI가 2D나 3D 아이콘을 일관적으로 만들어내는 것도 의심스러운 부분
          + 아이콘 스타일 자체보다, 실제 '인터랙티브함'을 드러내주는 시각적 힌트가 사라진 사실이 플랫 디자인에서 훨씬 더 중요한 문제라는 입장. 예를 들어, 클릭 가능한 아이템인데 버튼처럼 보이지 않아서 구분이 안 가고, 스크롤 가능한 영역에도 스크롤바나 명확한 구분자가 없으니 추가 콘텐츠가 있는지 모르겠는 상황. 선택된 창의 시각적 구별도 어두운 회색에서 중간 회색 정도로만 바뀔 뿐. 이런 변화가 실제 사용성에 큰 영향을 끼침
     * ""이건 과장된 리브랜딩이 아니라 완전히 새로 지은 단어다. 깊이와 질감, 빛을 수용하는 스타일의 작업명일 뿐 현실 세계를 흉내 내는 것이 아닌, 화면에 네이티브로 어울리는 표현적이고, 장난기 많은 무언가를 만들려는 것이다""라는 표현에 대한 거부감. 스스로를 과대평가하려는 자기만족적 칭찬으로 느껴짐. 작성자 의견에도 동의하지 않고, 실제로는 Google Material Theme이 한동안 꾸준히 트렌드를 선도해왔다는 사실. 내가 경험하는 거의 대부분의 웹 경험에 서서히 침투하는 모습에서 확인
          + Google Material Theme이 현재 대세라는 이야기에 어느 정도 동의. 하지만 원글 저자가 주장하는 건 곧 이런 흐름이 끝나고 새로운 트렌드로 넘어갈 조짐이라는 부분. 진정한 패러다임 변화는 나중에야 알 수 있다는 맥락. 개인적으로는 ""현재 트렌드가 끝났다""라기보다 ""미래에 돌아보면 지금이란 계기가 새로운 변화를 알리는 순간""이라는 해석이 타당하다고 봄
     * 이 글이 다소 과장되어 있지만, 대체적으로 ""대부분의 제품 UI가 적어도 조금은 재미있어야 한다""는 입장은 동의. '즐거운' UI/UX라는 말이 진부해졌어도 내가 자주 쓰는 소프트웨어에서 장인정신과 의도가 보이면 정말 행복한 감정. 세밀하게 디테일 넣은 아이콘만으로도 그 효과 가능
          + 지금 UI 트렌드 자체보다, AI를 디자인에 어떻게 활용해야 하느냐가 핵심이라는 생각. AI를 그냥 도구로 활용해서 마지막 결과를 내는 지름길로 쓰지 않으면, 여전히 장인정신, 취향, 세심함이 작동할 수 있다고 보는 입장엔 완전히 동의
          + 미적 감각이나 재미가 실용성보다 먼저여선 안 된다는 의견. 재미를 높이려다 실용성을 해치면 피해야 할 요소. 브랜드 아이덴티티를 위한 개성 추구는 존중하지만, 그게 잘못된 디자인의 변명거리가 되어선 안 된다는 생각
     * 그래픽 담당자가 심미성만 보고 UI 방향을 결정하는 걸 그만두라는 주장. 실제 사용성에서 뽑아낸 데이터가 UI 설계에 더 중요하다는 의견
          + 그런데 오픈소스/무료 소프트웨어에 대해 ""UI가 너무 낡았다, 보기 싫다""는 불평은 자주 들으면서 실제로 ""이게 바로 현대적 UI""라는 구체적인 대안을 제시한 경우는 거의 못 봤음. ""UI가 별로야!""라고 하는 사람들이 그래픽 디자이너가 필요 없다는 논리는 성립 안 되는 상황
          + 심미적 트렌드가 UI에 영향을 주는 건 불가피하고 자연스러운 일이라고 생각. 다만 문제는 새로운 트렌드를 'UX가 근본적으로 좋아졌기 때문'이라고 잘못된 논리로 포장하는 것. 실제로 UX를 개선하는 혁신이 약간 있겠지만 대다수는 새로움이 가져다주는 단기적 신기함에 불과하다는 점. 솔직하게 ""더 보기 좋은 디자인을 원했다""고 정직할수록 더 명확하고 건강한 제품/UX 가능
          + 실제로 사용성 전문가들이 설계하면 항상 최저 수준의 사용자를 기준으로 만들어서 오히려 재앙이 될 때가 있다는 주장. 그나마도 초보자는 ""사용성 좋은"" 인터페이스도 잘 못 쓰는 반면, 숙련자는 오히려 답답함을 느낌. 나로선 고밀도 UI가 큰 글씨로 열 몇 개만 보여주는 접근성 강조 스프레드시트보다 훨씬 더 생산적. 시각장애인을 위한 고민도 필요
          + '심미성 있는 UI에서 사용자들이 인지하는 사용성도 더 높다'고 말하는 데이터도 존재. 양쪽 모두 중요한 측면
          + 퍼스널 컴퓨터가 대중화된 이래 그래픽 디자이너가 결정권을 가지고 있었기 때문에, 지금 그런 얘기를 하는 건 시대에 뒤쳐진 논리
     * 플랫한 실루엣 아이콘은 다양한 맥락에서 쓸 수 있다는 점이 장점. 3D 아이콘은 무대가 주어져야 진가를 발휘할 수 있고, 작게 놓으면 읽기 힘들거나 눈에 거슬릴 수 있음. 맥시멀리즘은 시각적으로 부담스럽고, 멋져 보일 순 있겠지만 이런 미적 파격이 실제 맥락에서는 큰 변화를 만들지는 않을 거라는 예상. 애플이 스큐어모피즘을 버렸을 때처럼 결정적 변곡점은 아닐 것
          + 평면 디자인에 대한 대중의 문제는 맥락의 혼동에서 온다는 의견. 버튼과 아이콘은 명확히 다르고, 일반 텍스트와 링크가 다르듯, 굵기 조절 버튼은 워드프로세싱에서 상태가 있는 버튼이지만 이미지 편집기의 플립 버튼처럼 행동해서도 안 됨. 미니멀리즘에 동의하든 말든 이런 명확한 구분이 실용성에 결정적 영향. 많은 디자이너들이 이를 무시함

   플랫한 UI에 3D 에셋을 쓴거면 뉴모피즘의 일환 아닌가 싶습니다

   iOS에서 쓰던 스큐어모피즘이 개인적으로는 디자인의 정점이지 않나 싶습니다.

   동감합니다.

   블로그에 있는 ai로 만든 게임기 아이콘들을 봤는데요
   흠그정둔가...

     이 기사 보면 그저 다음 트렌드를 '내가 말했다'고 남기고 싶은 욕심으로 보이는 사람이라는 인상. 이게 진짜 다음 트렌드라는 근거도 없고, 그냥 누가 이름 붙이면 인플루언서가 될 수 있을까 시도하는 모습.

   플랫은 ai가 카피하기 쉬우니, 차별점을 찾는 것으로 보이네
"
"https://news.hada.io/topic?id=21219","Valkey 1주년: 커뮤니티 포크가 Redis를 어떻게 추월했는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Valkey 1주년: 커뮤니티 포크가 Redis를 어떻게 추월했는가

     * Redis Inc가 소스 폐쇄(SSPL 전환) 결정으로 커뮤니티 신뢰를 흔들었지만, Valkey 포크를 중심으로 개발자 커뮤니티가 결집해 활발한 혁신과 기여가 이어짐
     * Redis 8.0은 다시 오픈소스로 돌아오고, 창시자 Antirez가 복귀해 새 기능과 최적화에 참여하는 등 양 진영 모두 빠른 발전을 보이고 있음
     * 최신 벤치마크 결과, Valkey 8.1.1은 8vCPU 환경에서 SET 성능이 Redis 8.0 대비 37% 더 높고, p99 지연도 더 짧게 측정됨(GET 성능도 16% 우위)
     * IO 스레드/코어 핀닝 등 실전 튜닝 기법으로, Valkey는 멀티스레딩 환경에서 3배 이상 처리량 상승과 지연 최소화를 실현
     * 실사용 환경에 가까운 벤치마크 및 튜닝 노하우도 공유, 벤치마크 결과 해석시 유의점과 실제 운영 환경에서의 적용법도 안내
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Redis 소스 폐쇄와 Valkey의 등장

     * 1년 전 Redis Inc(구 Garantia Data)가 오픈소스 라이선스 변경(SSPL 도입)으로 커뮤니티와 신뢰관계가 악화됨
     * 이에 대한 해결책으로 탄생한 오픈 포크 Valkey는 분산 시스템, 캐시, 실시간 데이터 처리 등에서 널리 사용되는 기술 자산이 됨
     * Redis 측도 이후 Antirez(창시자) 복귀, 성능/기능 강화, Redis 8.0 오픈소스 재전환 등으로 신뢰 회복 시도

Valkey 8.1 vs Redis 8.0: 성능 비교

     * 동일 8vCPU AWS c8g.2xl 인스턴스, 1KB 아이템/3M 키스페이스/500 커넥션 조건에서 SET 벤치마크
          + Valkey 8.1.1: 999.8K RPS(p99 0.8ms)
          + Redis 8.0: 729.4K RPS(p99 0.99ms)
          + Valkey가 SET에서 37%, GET에서 16% 더 높고, SET p99 30%, GET p99 60% 더 빠름
     * 6개 IO 스레드 도입시, Valkey는 239K → 678K RPS(2.8배↑), p99 1.68ms → 0.93ms(44%↓)
     * Redis도 IO 스레드로 235K → 563K RPS, p99 1.35ms → 0.84ms(40%↓)

멀티스레딩/코어 튜닝의 효과

     * IO 스레드는 3개 이상부터 효과가 크게 나타나며, Valkey는 4스레드 이후 Redis보다 큰 격차
     * IRQ(인터럽트) 코어를 2개로 제한 후, Redis/Valkey를 남은 6코어에 고정(pinning)하면 추가 성능상승
          + Valkey: 832K → 999.8K RPS(코어/IRQ 핀닝, CPU 80% 활용)
          + IRQ/애플리케이션 코어 분리로 캐시 효율과 tail latency 최소화
          + 실제 Docker로 cpuset-cpus, --io-threads 등 활용 예시 제공

벤치마크 재현과 실전 팁

     * 최신 AWS Graviton4(c8g.2xlarge) 인스턴스, 클러스터 placement group 활용
     * 코어 핀닝/IRQ 분리/연결수 조정(400~500 선)로 최대 성능 구현
     * 키스페이스/아이템 크기도 튜닝 필요, 작은 값/키스페이스가 L3 캐시 적중률을 높임
     * valkey-benchmark나 rpc-perf(실사용에 더 가까운 Rust 기반 툴) 등 멀티스레드 벤치마크 도구 적극 활용 권장
docker run --network=""host"" --rm --cpuset-cpus=""2-7"" \
valkey/valkey:8.0.1 valkey-benchmark \
-h 172.31.4.92 -p 6379 -t SET,GET -n 100000000 -c 256 \
-r 3000000 --threads 6 -d 1024

성능 측정의 한계와 유의점

     * 벤치마크 결과는 실제 운영 환경과 다를 수 있음
          + 실제 워크로드는 SET:GET 혼합, 부하 변동, TPS 타깃, 네트워크 지연 등 복합 요인 존재
          + 연결수 급증 시 대기열 지연과 throughput 감소, tail latency 증가도 관찰됨
          + 벤치마크 툴/옵션, 네트워크 토폴로지 등에 따라 결과가 크게 달라질 수 있음

주요 성장과정 및 커뮤니티 발전

   Valkey 프로젝트는 지난 1년간 다양한 측면에서 활발한 발전을 이룸
     * GitHub 등지에서 많은 개발자와 기업의 협업 하에 기능 추가, 버그 수정, 보안 개선 등을 이룸
     * 문서화와 사용자 지원에도 힘써 신규 사용자 진입 장벽을 낮춤
     * 프로젝트 운영 과정에서 투명한 의사결정과 커뮤니티 투표 등이 강조됨

업계와 기술적 가치

   Valkey는 다음과 같은 강점을 지님
     * 라이선스 제약 없이 누구나 사용할 수 있어 클라우드 서비스 벤더나 대규모 웹 서비스 기업들에게 매력적인 선택지임
     * Redis와 호환성이 높아 마이그레이션도 용이함
     * 커뮤니티 주도 개발이어서 다양한 요구사항 반영과 빠른 지속적 업데이트가 가능함

결론 및 시사점

     * Valkey는 Redis 포크 1년만에 기술/성능/커뮤니티 측면에서 Redis를 뛰어넘는 성과를 보임
     * IO 스레드, 코어/IRQ 분리, 커넥션 조절 등 실전 튜닝 노하우와 도구가 핵심
     * 성능은 자동으로 따라오는 것이 아니며, 시스템/워크로드/인프라에 맞는 지속적인 최적화가 필수
     * 실제 서비스 환경에서는 벤치마크 수치에만 의존하지 않고 다양한 상황을 직접 테스트하는 실무적 접근이 필요

   사실 Valkey가 뭘 했다기 보다는... 저 쪽에서 알아서 망해서...

   Redis가 잘못된 결정을 많이 내리긴 했지만, 재주는 곰이 부리고 돈은 조련사가 받는 모양새는 아쉽네요.

   페이스북 ""Redis User Group""에 올린
   Valkey 8에 어떤 개선이 이뤄졌는지 정리한 자료입니다.
   https://www.facebook.com/groups/rediskorea/posts/3927030954110001/

   위 내용과 함께 보시면 이해하는데 도움이 될듯합니다.

        Hacker News 의견

     * 나는 ValKey가 I/O 쓰레딩 분야에서 멋진 발전을 이루어낸 점이 기쁨 느낌, 최근에 가장 흥미로운 변경 사항들도 도입하기 시작함, ValKey 기여자분들께 큰 감사 인사 드림. 하지만 이 글이 다소 오해를 불러일으키는 경향을 가진다고 생각함. 원래 Redis에서 shared nothing 아키텍처는 매우 중요한 철학이었고, 2020년에 내가 직접 I/O 쓰레딩을 도입한 장본인임. Shared nothing의 취지를 해치지 않으면서, event loop에서 반환할 때 write(2)/read(2) syscall이 굉장히 느릴 수 있다는 점을 감안해 해당 시점에 zero contention 상태에서 I/O만 N개의 쓰레드로 병렬 처리하고 이후 바로 싱글 쓰레드로 돌아오는 구조를 도입함. ValKey 팀은 이 시스템을 더 발전시켰음에 감사함을 전함. 예전부터 해당 시스템이 작동하고 있었고, 현실적으로 바뀌지 않은 데이터를 단지 언론이 과장하는 경향이
       불편함. 이러한 기능들은 실제로는 대다수 일반 Redis 유저보다는 기업 사용자나 클라우드 제공자(아마존, 구글 등)에게 더욱 관심이 있음. 엄청난 부하나 다수의 유저를 동시에 처리할 수밖에 없는 경우 아니면, 대부분 Redis의 CPU 사용률이 낮아서 굳이 활성화할 필요가 없는 현실임. 최근 Redis에서 vector set data type(벡터 집합 자료형)을 개발하면서 쓰레딩을 기본값으로 삼고 있고, VADD를 통한 벡터 집합 기록도 쓰레드 방식으로 할 수 있게 함. HNSW 같은 데이터 구조는 Redis 역사상 최초로 거대한 상수 시간을 가져서 설계 영역 자체가 달라짐. 과거에는 모듈용 쓰레드 지원도 이미 구현한 바 있음. 쓰레드 사용 여부는 상황에 따라 달라지는 결정임.
          + 미묘한 관점은 클릭을 못 끄는 사실을 언급함
          + 이런 류의 비판 글이 매달 HN 메인에 오르는 느낌 받음, 항상 antirez를 응원하고 싶었음. 기술적 요지는 동의하지만, 이런 비판이 종종 구체적인 내용보다는 큰 성공을 이룬 사람을 공격하는 고전적인 tall poppy syndrome(톨 파피 신드롬, 성공자를 깎아내리는 현상)과 관련 있다고 봄. 타인의 반응을 통제할 수 없으니 이런 비판글을 당신의 업적이 크다는 우회적 인정으로 여기면 더 건강할 수 있겠다는 의견 제시, LinkedIn에서 연결된 것도 감사하게 생각함 톨 파피 신드롬 보기
     * antirez가 Redis를 다시 오픈소스로 전환한다고 발표했던 것 기억함 관련 글 예전 Node.js가 Io.js 포크 사태로 거의 무너질 뻔했지만 커뮤니티가 복구한 전례가 있음. Redis도 그런 회복이 가능할지 궁금함. 예전에는 Redis를 자주 썼지만 최근 몇년간은 커뮤니티 동향을 잘 모름
          + 마지막으로 확인했을 때 Redis는 여전히 CLA(기여자 라이선스 동의서)를 요구함. 이는 언제든지 다시 소스를 닫을 독점적 권리가 있음을 의미함. 기여자가 이런 조항에 동의해야 한다면, 또다시 같은 일을 벌이지 않을 거라고 신뢰할 이유가 없음
          + Redis는 AGPL로, Valkey는 BSD 라이선스(BSD가 예전 Redis 라이선스)로 배포되고 있음. 둘 다 공식 오픈소스 라이선스이나 BSD가 훨씬 더 자유로움
          + 솔직히 이용자 99%는 누가 소유하든 간에 무료 키-밸류 저장소만 제대로 굴러가면 신경 안 씀. 비즈니스 입장에서 Redis가 독특한 위치에 있는데, 엄청 많은 기능을 제공하지만 실사용자는 5%만 쓴다는 점, Sentinel/Streams 같은 복잡한 기능엔 관심도 없음. 유료화하려고 할 때 사용자 선택지는 ""그냥 안 쓰기"", ""경쟁사로 옮기기"", ""직접 꼭 필요한 기능만 만들어 쓰기"", ""마지막 오픈소스 버전을 포크해서 자체 유지보수하며 비용 아끼기"" 등 다양함. PostgreSQL에 비해 Redis의 단순한 버전(해시맵+네트워크 인터페이스)은 직접 만들어 쓰기 쉽고, 그래서 많은 비즈니스에선 포크하거나 직접 만드는 게 더 나은 선택임
          + 내 생각에도 이미 늦은 감이 큼. Redis의 급격한 정책 전환으로 내겐 Redis는 이제 불신 대상이고, 앞으로는 Valkey가 기본 선택지임. 한번 속으면 두 번 안 믿는 태도 유지
          + Redis가 벌인 상황 이후로 어떻게 신뢰할 수 있냐는 의문 제시
     * Valkey가 기본 배포판 패키지 매니저에 들어가야 한다고 생각함. 예를 들어 GitHub Action runner에서 설치하려고 할 때 매번 키 추가하고 배포판 업데이트하는 게 번거롭다는 불만 있음
          + 어떤 배포판에 없는 게 문제인가 궁금함. repology 자료로 보건대 nixpkgs, Arch, Ubuntu, Fedora, Debian, EPEL 등에 이미 있음. 다만 Debian은 13 혹은 12+backports부터임
          + 참고로 Arch Linux에선 Valkey가 Redis를 이미 대체함
          + CI에서 컨테이너 이미지를 받자마자 이것저것 설치하는 게 첫 작업이면, 직접 이미지를 만드는 게 더 낫다는 의견 제시
     * 이런 변화가 일어나고, Valkey가 꾸준히 잘 나가는 상황이 매우 반가운 느낌. 부디 Redis는 곧 사라지길 바람
          + 오픈소스가 독점 기업을 위한 것이라는 풍자적 어투, AWS 같은 하이퍼스케일러들이 Redis로 수억 달러를 벌고 있지만 실제 개발자와 기여자는 그 혜택을 받지 못한다는 아쉬움 표명. 그래서 DB 스타트업은 처음부터 페어 소스(anti-hyperscaler 조항이 들어간 라이선스)로 출발해야, 코드 자체는 누구든 마음껏 쓰더라도 AWS나 Google이 관리형 상품(Managed Service)으로 내놓으려면 비용을 내게 해야 함. 이미 uber permissive 라이선스로 출발한 Redis와 Elasticsearch는 돌이키기 힘든 운명임을 언급
     * 최근 dotnet 프로젝트들이 상용화로 전환하는 경우 많아짐. 이런 변화가 개발자들에겐 마치 러그 풀(신뢰를 깨는 갑작스러운 정책 변신)로 느껴짐. 이런 분위기는 다른 오픈소스 dotnet 라이브러리의 확산에도 타격을 줄 가능성 있음
          + .NET에서는 이런 상업화 경향이 최근 일도 아니고, 원래 freeware/open-core와 비즈니스가 항상 붙어다니는 성향이라고 언급함
     * 작년부터 Valkey를 들었는데, 여전히 잘 나가고 있다는 점이 기쁨
     * Valkey가 자체 클라이언트 라이브러리를 제공하는지 궁금함. 현재 GCP 환경에서 MemoryStore와 직접 관리 환경 양쪽에서 Redis/Redis Cluster 모두 사용, 공식 C 라이브러리는 Redis Cluster+TLS를 쓸 때 실망스러워서 hiredis-cluster라는 비공식 클라이언트(https://github.com/Nordix/hiredis-cluster)를 써야 하는 상황. GCP의 Memorystore도 별로 만족못함. Scylla로 이전 고려중
          + hiredis와 hiredis-cluster를 조합한 libvalkey라는 공식 포크가 있음, 기존 hiredis/hiredis-cluster 유지보수자들이 관리 중 libvalkey 보기
     * Redis가 정책을 바꾼 지금, Valkey와 Redis가 대화해서 합칠 수 있는지 궁금함
          + 라이선스가 원상 복귀한 것이 아니라 AGPL로 옮긴 것이기 때문에 진짜 유턴이 아님을 지적함
     * 예상되는 GPL 관련 FUD(공포·불확실성·의심)에 대한 훌륭한 설명글 링크 공유 관련 글
          + 게시글에서 copyleft 라이선스가 더 자유롭다는 주장은 다소 손쉬운 논리라고 봄. 의무가 많아질수록 자유가 줄어드는 성격이 있으므로, 어떤 스타일이 더 '자유로운지' 논의는 보다 깊이 있다고 생각함
     * 나는 수십 개 애플리케이션에서 Redis를 사용함. AWS에서 Valkey를 할인 가격에 제공한다 해서 2개월 전부터 써봤고, 성능의 차이도 느끼지 못함. 그런데 Valkey가 느닷없이 먹통이 되었고, AWS에선 여전히 구동 중으로 인식했지만 접속이 완전히 끊김. 12시간이 넘도록 복구되지 않았고, 또 재발했음. AWS가 2주 동안 조사했지만 원인을 못 찾았음. 앞으로 Valkey를 미션크리티컬 환경에 쓰기는 힘들게 되었음. 이후 동일한 워크로드로 Redis로 교체했더니 문제 없음
          + 아마 AWS 이슈일 수 있다는 생각. 우리도 프로덕션 RDS postgres 클러스터에서 비슷한 경험이 있었음. 네트워크 반응이 멈췄고, AWS 엔터프라이즈 지원을 받았지만 결국 스스로Backup 복구로 새 클러스터 생성, 4시간 걸림. 이후 AWS encapsulated 서비스는 피하고 일반 EC2에 replication, 스냅샷, S3복제 등으로 이전함
          + AWS 운영상의 문제 가능성도 있음, Redis만 직접 돌려봤지만 Valkey 자체 문제가 아닐 수도 있음
          + 왜 직접 Valkey 인스턴스를 띄우지 않는지 궁금함
          + 사용한 인스턴스 타입이 어떤 것인지 궁금, 참고용 질문
"
"https://news.hada.io/topic?id=21294","호주에서 앵무새가 음수대를 작동하는 법을 터득함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       호주에서 앵무새가 음수대를 작동하는 법을 터득함

     * 황관앵무새가 호주 시드니에서 인간용 음수대를 작동하는 방법을 터득함
     * 이 행동은 서부 시드니의 일부 개체군 사이에서 전파되고 있어 지역적 문화 전통으로 간주됨
     * 개체마다 약간의 방식 차이는 있지만, 공통적으로 발과 부리를 이용해 손잡이를 비틀어 물을 마시는 행동을 보임
     * 하지만 성공률은 41%로, 다른 앵무새의 방해로 실패하는 경우도 많음
     * 이 행동은 지역적 분포를 보이며, 쓰레기통을 뒤지는 습성처럼 시드니 전역으로 퍼지지는 않고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

호주 황관앵무새의 인간용 음수대 작동

  # 서론: 황관앵무새의 새로운 행동 발견

     * 황관앵무새(Cacatua galerita)는 독특한 흰색 깃털과 노란색 볏으로 잘 알려짐
     * 이 종은 평소에도 시드니 교외에서 쓰레기통을 잘 뒤지는 것으로 유명함
     * 최근 이 앵무새들이 인간용 음수대를 조작하는 새로운 능력을 보여줌이 밝혀짐
     * 복잡한 손잡이 비틀기와 잡기 동작을 수행해 물을 얻는 행동을 보임

  # 행동의 전파와 ‘문화적 전통’

     * 연구진은 이 음수대 작동법이 서부 시드니 지역 황관앵무새 개체군 사이에서 퍼지고 있음을 관찰함
     * 이는 특정 집단 내에서 전승되는 문화적 전통으로 간주될 수 있음
     * 선두 몇 마리가 방법을 터득한 뒤, 나머지 앵무새들이 이를 관찰을 통해 습득하는 양상으로 보임

  # 관찰 및 실험 내용

     * University of Vienna의 Barbara Klump 박사가 2018년 현장조사 중 여러 앵무새가 음수대 주변에 대기하는 모습을 발견함
     * 인간처럼 손잡이를 비틀고 내리며 물을 마시는 모습을 가까이에서 관찰함
     * 박사와 연구팀은 해당 지역 음수대에 모션 감지 카메라를 설치해 1개월 넘게 관찰을 진행함
     * 총 525번의 시도가 기록되었으며, 개체별로 세부 전략은 달랐으나 공통적으로 발과 체중을 이용해 손잡이를 비트는 전략을 사용함
     * 물이 나오는 고무 마개에는 앵무새의 날카로운 부리 자국이 남기도 함

  # 기술적 특징 및 한계

     * 황관앵무새의 행동은 발의 기민함과 큰 부리가 뒷받침하기 때문에 가능함
     * 까마귀 등의 다른 영리한 조류는 물체 조작에 적합한 신체 도구가 없어 이러한 행동이 불가능함
     * 시도 중 41%만 성공적으로 음수에 도달함
     * 실패의 주된 원인은 다른 개체의 방해 또는 자리다툼임

  # 선택적 행동의 원인과 확산 양상

     * 연구팀은 다양한 외부 수자원이 있음에도 왜 음수대를 선택하는지 분석함
     * 더 깨끗한 물에 대한 선호나, 높이 있는 자리에서 포식자 관찰이 쉬운 점 등이 원인일 수 있음
     * 현재 추적 결과, 이러한 행동은 서부 시드니에 한정되어 있으며, 시드니 전체에 보편적으로 퍼지지는 않음

  # 지역별 확산의 차이

     * 반면, 쓰레기통을 뒤지는 행동은 이미 시드니 40여 교외에 확산됨
     * 음수대의 손잡이 종류 등 지역적 설계 차이도 확산을 제한하는 요인임
     * 어떤 지역은 누르는 버튼식 음수대 등, 황관앵무새가 다른 접근법을 요구하는 구조임

  # 결론: 앵무새의 혁신성과 잠재력

     * 아직 일부 지역만의 행동이지만, 미지의 음수대에도 고유한 방법을 개발할 가능성이 높음
     * 황관앵무새는 문제 해결 능력과 혁신성이 뛰어나 꾸준히 새로운 행동을 보여 줄 것으로 전망됨

        Hacker News 의견

     * Cockie들은 새 중에서도 장난꾸러기라는 인식, 매우 영리하고 서로 또는 인간에게 장난을 거는 모습을 재미있게 여김, 모든 것을 부수는 경향도 특징, 그래서 물 마시는 분수대 사용법을 터득한다면 역시 이런 웃긴 녀석들이 가장 먼저 해낼 법한 일이라는 생각
          + 구조 동물을 보호하는 시설을 방문한 적이 있는데, 그곳에서 새를 반려동물로 키우지 말라는 안내를 진행, 일부 새들은 인간 다섯 살 수준의 호기심과 엄청난 파괴욕구를 가진다는 설명을 들은 기억
          + 만약 Cockie들이 장난꾸러기라면 Kea는 어떨까 궁금, Kea가 더 똑똑하다는 평가를 받았고 분명히 악동스러운 장난을 즐기는 면이 있다는 생각
          + 직접 겪었던 이야기로 아버지가 과일나무에 내려앉아 과일을 망가뜨리는 Cockatoo를 자주 쫓아내려던 기억, 빗자루로 쫓으면 집 쪽으로 날아가 사라짐, 그런데 아버지가 차고 쪽에 있을 때 Cockatoo들은 과일을 따서 경사진 진입로에 굴려 차고 안으로 굴러가게 했다는 에피소드, '이리와서 놀아봐라'라는 식의 장난기 발산
          + ""장난꾸러기 녀석들""이라는 표현에 공감, 최근에는 Galah가 호스관을 갉는 장면도 목격, 소리를 내며 주의를 끌었더니 녀석이 멈췄다 다시 눈을 맞추고 무시하고 계속 갉음, 며칠 전에는 Kookaburra가 호스릴 끝을 빼서 물이 거실 유리창에 분사되도록 만들어 그 앞에서 샤워를 즐긴 모습까지 봤다는 일화
          + Caiques와 Blue Hyacinths가 더 장난꾸러기 기질, Cockatoos는 장난을 넘어서 거의 미친 수준의 에너지를 갖고 있다는 의견
     * Cockatoos의 빠른 학습능력 언급, 최근 뉴스에서 두리안 열매를 맛을 들인 이야기 소개
     * 친구가 Sulfer Crested Cockatoo 두 마리를 키웠던 경험, 수컷은 다양한 것들을 기민하게 풀고 놀았으나 암컷은 전혀 하지 않음, 친구가 암컷을 덜 똑똑하다고 얘기한 순간 암컷이 직접 케이블 타이를 풀어버리고 자리를 떴다는 에피소드, 모두가 놀람
          + xkcd 만화 - Archaea 관련 링크
     * '모든 Cockie들이 왼발잡이'라는 흥미로운 사실, Canberra에 살 때 많이 관찰했고 수백 마리 모두 도토리를 먹을 때 왼발을 사용한다는 경험담
          + Coriolis 효과라고 농담식 코멘트
          + 동물도 왼손잡이/왼발잡이가 있다는 사실을 처음 알게 됨, 집에서 키우던 잉꼬 둘을 봤지만 어느 발을 더 썼는지는 신경쓰지 않음, 이제 다른 앵무새 영상도 검색해서 비슷한 현상이 있는지 확인해보고 싶다는 소감
     * Cockatoos가 왜 굳이 분수대를 선호하는지에 대한 배경과 '더 순수한 물' 이론에 대한 흥미적 시각, 높은 위치가 포식자 감시에 유리하기 때문이라는 의견도 언급
          + 분수대의 깨끗한 물과 음습한 웅덩이 물 중 어떤 게 더 먹고 싶은지는 뻔하다는 유머와 과학적 연구로선 쉽게 결론지을 수 없다는 현실의 간극에 대한 언급
          + Cockatoos의 높은 지능을 고려하면, 분수대 조작은 더 재미있고 두뇌를 자극하는 활동이라 생각, 이런 도전을 마다하지 않는 성격을 탓함
          + 고양이가 흐르는 물을 선호하는 습성이 있는데, 새에게도 같은 원리가 적용될 수 있을 거라는 생각
          + '스케이트보드도 계단과 손잡이에서 타면 끔찍하다'는 농담과 함께, 새들은 지루함을 많이 느끼고 개처럼 노는 걸 좋아하나 3D 버전으로 논다는 새 주인 경험, 심지어 음악적 리듬감도 있다는 설명
          + Cockatoos같은 새들은 퍼즐과 새로운 도전을 좋아한다는 지적
     * Kea 앵무새 역시 흥미로운 새, 위키백과 - Kea의 인지 능력과 케아의 확률 추론 영상 공유
     * 기사에서 논의된 원문 연구 링크 royalsocietypublishing.org 논문 제공
     * 조류는 전뇌에 영장류 수준의 뉴런 수를 가진다는 연구 자료 링크 소개
     * Cockatoos가 쓰레기통을 열어서 쓰레기를 뒤지는 법도 사회적으로 학습한다는 소개, 이제는 원하는 물과 음식을 자유롭게 확보할 만큼 교활한 요상한 녀석들이라는 평가, 관련 기사 링크 포함
     * 브리즈번 Southbank 야외 식당에서 처음 Ibis를 만난 경험, 그 지역엔 Ibis 주의하라는 표지판과 음식 도난 시 교환하지 않는다는 안내 있음, 테이블 위에 'ibis spray' 스프레이가 있어 기대, 내부엔 아마 비누물이나 레몬즙이 들어있을 것을 예상, 커다란 Ibis가 위협적으로 다가왔지만 태연하게 스프레이로 쫓으려 했는데, 실제로는 그냥 수돗물이 나와 Ibis가 피하지 않고 버텼던 해프닝
          + 혹시 스프레이 노브가 ""stream(분사)""이 아닌 ""spray(분무)""로 설정된 것 아니냐는 장난스러운 질문
"
"https://news.hada.io/topic?id=21272","모든 혈액형에 호환 가능한 일본 인공혈액 개발","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       모든 혈액형에 호환 가능한 일본 인공혈액 개발

     * 일본 연구진이 모든 혈액형 환자에게 사용할 수 있는 인공혈액을 개발함
     * 적혈구 속 헤모글로빈을 추출하여 인공혈구로 만들어졌으며, 혈액형 검사 없이 사용 가능함
     * 인공혈액은 상온 2년, 냉장 5년까지 보관 가능하며, 기존 적혈구보다 유통기한이 대폭 향상됨
     * 2022년부터 초기 임상시험에서 중대한 부작용이 없는 결과를 얻음
     * 2030년 실용화를 목표로 하며, 다른 인공 산소운반체 개발 연구도 활발히 진행 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

의료 현장에서 인공혈액의 필요성

     * 수혈은 전 세계적으로 생명을 구하는데 중요한 역할을 함
     * 특히 O-음성 혈액(만능공여자)이 부족한 상황이 자주 발생함
     * 헌혈 혈액은 보관 기간이 짧고, 특히 저소득 국가에서 공급이 매우 어려움

일본 연구진의 인공혈액 개발

     * Nara Medical University의 Hiromi Sakai 교수 연구팀이 기존 혈액의 한계를 극복하기 위해 인공혈액을 개발함
     * 만료된 헌혈 혈액에서 헤모글로빈을 추출하여, 보호막에 감싸 안정적이고 바이러스에 노출되지 않은 인공적혈구로 만듦
     * 개발된 인공혈액은 혈액형이 존재하지 않아 별도의 적합성 검사가 필요하지 않음
     * 인공혈액은 상온 2년, 냉장 5년까지 장기 보관이 가능하여, 기존 적혈구(냉장 42일)에 비해 월등한 보관성을 가짐

초기 임상시험과 연구 진행 상황

     * 2022년부터 적은 인원(남성 12명)을 대상으로 인공 산소운반체(헤모글로빈 소포체) 정맥 주입 임상시험을 시작함
     * 주입량은 최대 100ml까지 점진적으로 늘림
     * 일부 경미한 부작용이 있었으나, 혈압 등 주요 생체 징후에는 중대한 변화가 없음
     * 연구팀은 2023년 7월부터 더욱 임상시험을 가속화하였고, 2024년 3월에는 주입량을 최대 400ml까지 늘려 진행함

향후 계획 및 국내외 연구 현황

     * 추가 이상 반응이 없을 경우, 치료 효과와 안전성 검증 단계로 확대 예정임
     * 2030년 상용화를 목표로 실용적 도입을 추진 중임
     * 한편, Chuo University의 Teruyuki Komatsu 교수팀도 알부민 보호막으로 안정화한 산소운반체 개발을 진행함
     * 동물실험에서 출혈 및 뇌졸중 치료에 효과를 보여, 인간 대상 임상으로 확대될 전망임

        Hacker News 의견

     * Biopure라는 회사가 미국에서 소 혈액에서 헤모글로빈을 추출해 '산소 치료제'라는 혈액 대체제를 만들었던 기억, 이 제품은 적혈구를 통째로 사용하지 않아 혈액형에 상관없이 쓸 수 있고, 실온에서 보관 가능하며, 분자 크기가 작아 동맥 막힘 부위나 조직에 더 잘 산소를 전달하는 특징, 하지만 회사 경영에 문제가 많았고 미국에서는 반려동물용 제품 외에는 FDA 승인을 못 받은 점, 실제 효과도 있었으나 사업화에 실패한 아쉬움, 그리고 관련 기사 Wikipedia Biopure 첨부, 참고로 회사는 임상시험 관련 투자자 대상 허위진술로 인해 소송까지 갔고, 부사장이 암에 걸린 척 연기하며 법정을 속이기도 해서 3년형을 선고받은 터진 사건, 이번 일본팀의 새로운 혈액 대체제가 더 긍정적이길 바라는 기대
          + WADA가 헤모글로빈 기반 혈액 대체제의 도핑 사용도 구체적으로 다뤘던 일화도 있음 WADA 관련자료, 이런 제품들이 실온 보관되고 혈액형도 필요 없어서 팀 버스에서 아무때나 자유롭게 투여 가능, 도핑 단속으로부터 자유로워질 수 있다는 점에서 투르 드 프랑스 같은 대회에서 엄청난 이점, Biopure는 공식적으로 도핑에 반대했지만 실상은 WADA에서 자사 제품을 언급해줄 정도로 효과 있어서 은근히 자랑스러워한 분위기
          + 회사가 망했을 때 특허권이나 기술은 어떻게 되는지에 대한 궁금증, 제품이 효과 있었는데 제대로 뒷받침 못한 회사 때문에 경쟁력이 있는 기업이 이어받지 못한 이유에 대한 의문
          + 예전에 Biopure에 엔젤 투자하려던 시절의 추억 회상
          + 헤모글로빈을 직접 혈액에 넣어서 효과를 냈다는 점이 신기함, 면역반응이나 분해 없이 단순하고 창의적인 접근이라는 느낌
          + Biopure와 별개로 미국에서 ""PolyHeme""이라는 다른 혈액 대체제가 있었는데, 심장마비 사례가 더 많았고 동의 없이 외상환자들을 대상으로 임상실험을 진행해서 논란이 있었던 점, PolyHeme 관련 Wiki 링크로 확인 가능
     * 인공혈액이 기존 헌혈된 혈액에서 헤모글로빈만 추출해 보호막에 담아 바이러스가 없는 인공 적혈구로 만드는 식이라서, 결국 여전히 헌혈된 혈액이 필요하다는 부분, 참고로 원문은 인용문 형태로 제공
          + 이번 연구팀은 곧 만료될 헌혈 적혈구를 우선적으로 활용해서 기존 혈액 활용도를 극대화하려는 접근, 하지만 헤모글로빈은 단백질일 뿐이어서 현재는 검체육 단백질 용도로 동물성 헤모글로빈도 생산된다는 점, 그동안은 헤모글로빈만 있다고 바로 유용한 적혈구 역할을 못했으나 이번 연구로 인해 재조합 인간 헤모글로빈 대량 생산 시장이 열릴 가능성, 5-10년 안에 바이오파마 공급처에서 인간 헤모글로빈을 손쉽게 구매할 수 있으리라는 확신
          + 혈액 기증에서 가장 큰 문제가 유통기한 만료이기 때문에 연중 꾸준한 기증이 필수, 재난 발생 시 갑자기 기증이 몰려도 남는 분량은 버려야 했던 구조, 만료 혈액을 모두 활용 가능하고 혈액형 확인도 필요 없으며 유통기간도 늘어난다면 상당히 큰 가치
          + “혈액형 검사 필요 없음”이라는 기능이 한정 환자에게는 매우 중요, 다양한 혈액형 수혈이 불가능한 이가 지속적으로 수혈해야 할 때 현실적이고 실질적인 어려움
     * 인공혈액과 유사한 다양한 시도들이 있었고, 특정 화학물질(예: perfluorocarbon)은 산소 운반력이 매우 뛰어나 혈액보다 훨씬 많은 산소를 운반할 수 있었던 사례, 안전하고 장기간 보관 가능한 혈액 대체제의 필요성 강조, 다만 이 제품은 혈액의 산소 운반 역할만을 인공적으로 대체할 뿐 응고, 면역, 호르몬, 영양 등 아직 다른 주요 기능까지는 어렵다는 점, 그래도 산소 공급을 안전하게 처리해주는 발전은 환영
          + 외상 이후의 대부분 수혈도 사실상 적혈구 공급이 주 업무라서, 혈액의 다른 기능들이 아예 수혈될 필요는 없는 조건
          + “oxygen therapeutics”라는 명칭이 등장한 것도 실제로는 혈액이 아니라 산소 전달 고안품이어서 그런 맥락, 처음엔 용어가 지나치게 세분화된 듯했으나 이번 논의에서 왜 용어 구분이 중요한지 동의
     * 일본에서 인공혈액 개발이 처음이 아닌 것 같고, 2019년에도 다른 팀에서 연구가 있었다는 점을 빠르게 검색해 발견, http://www.asahi.com/ajw/articles/AJ201909290001.html"">2019년 사례 기사, 이번 연구의 차이가 궁금
          + 2019년 당시에는 토끼로만 실험했고 지금은 사람 대상 시험을 진행 중, 2022년부터 투여량을 늘려가면서 건강한 지원자들에게 적용해온 진행 상황, 부작용이 확인되지 않으면 곧 실효성과 안전성을 평가하는 단계로 넘어가고, 2030년 실용화를 목표로 가속화 중
     * “모든 혈액형과 호환”이라는 소식에 perfluorocarbon 같은 완전 합성 제품을 떠올렸고, 참고로 이런 제품은 멕시코와 러시아에서 이미 실사용되고 있음 PFC 관련 Wiki
          + PFC 용액은 산소 운반력이 뛰어나 인간도 액체 호흡이 가능할 정도로 쓰이기도 하는 특이점
     * 미국 Kalocyte라는 회사가 인공혈액을 개발 중이고, DARPA와 협력하며 올해 The New Yorker에서 소개된 적 있음 New Yorker 기사
     * 해당 임상시험 연구 논문은 Blood Advances에서 확인 가능
     * HBO 시리즈 True Blood의 설정이 실제 현실이 된 듯한 묘한 기분, 일본 과학자들이 인공혈액을 개발해 뱀파이어가 활동 가능하게 한다는 줄거리가 생각나서 흥미로움, 물론 뱀파이어 실현은 둘째치고 blood substitute와 일본 과학자라는 조합은 현실화
     * 이번 연구팀의 기술이 아마도 liposome 기반 헴글로빈 캡슐일 것이라 추측, 저자명 Sakai 확인, 관련 논문도 PubMed1 PubMed2 링크 공유, 입자가공법 직접 따라하고 싶으면 PMC 입자 제조법도 참고
     * 미국 수출에서 혈액이 2.5%를 차지한다는 농담 언급
          + 그 농담 해설: CNBC 기사 참고, 다만 이 2.5% 수치는 실제 완제품 혈액이 아니라 혈액 유래물질이 포함된 다양한 수출품을 모두 포함해 계산된 과장된 통계라는 설명, 자료를 더 찾지는 못했으나 약간의 논란이 있다는 맥락
"
"https://news.hada.io/topic?id=21191","Cap - 경량 PoW 기반 CAPTCHA 대체용 오픈소스","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Cap - 경량 PoW 기반 CAPTCHA 대체용 오픈소스

     * SHA-256 Proof-Of-Work를 기반으로 하여, 기존 CAPTCHA보다 빠르고 프라이버시 친화적이며, 통합이 매우 간단한 오픈소스 CAPTCHA 대체제
     * hCaptcha 대비 250배 작고, 프라이버시를 중시하며 사용자 정보 추적 및 지문 수집 없이 동작
     * JavaScript 기반으로, JS 런타임(Bun/Node.js/Deno) 어디서든 통합 가능하며, Docker로 REST API 기반의 독립 실행도 가능함
     * 커스터마이즈와 셀프호스팅이 용이하며, 백엔드와 프론트엔드를 자유롭게 수정하거나 CSS만으로도 외형을 조정할 수 있음
     * 증명작업(proof-of-work) 방식으로 봇 차단이 쉽고, 필요할때만 노출되는 인비저블/플로팅 등 다양한 모드 제공

   PoW 캡차는 Anubis가 이상할 정도로 여기저기 자주 보이던데 Cap은 처음 보네요
"
"https://news.hada.io/topic?id=21265","매달 하나씩 말도 안 되는 웹 프로젝트를 만드는 이야기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     매달 하나씩 말도 안 되는 웹 프로젝트를 만드는 이야기

     * 개발자가 매달 기발하고 비상식적인 웹 프로젝트를 제작하는 시도임
     * 각 프로젝트는 유머러스하고 실험적인 아이디어에 기반하여 일상적인 문제나 상황을 독특하게 해석함
     * 전통적인 eCommerce, 게임, 인공지능, 소셜 서비스 등 다양한 웹 분야를 패러디 개념으로 접근함
     * 웹사이트 방문자에게 신선한 재미와 창의적 영감을 제공하는 독특한 제품 및 서비스 소개임
     * 실용성보다는 독창성과 실험 정신을 중시하는 창작물 모음임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

   이 프로젝트는 한 개발자가 매달 새로운 ""absurd"" 웹 프로젝트를 만드는 실험적 시리즈임. 각 웹사이트는 현실에서는 쓸모없거나 말도 안 된다는 것을 컨셉으로, 유머와 풍자 그리고 창의적 발상을 조합하여 만들어짐. 기능 자체보다 아이디어의 참신함과 상상력, 그리고 기존 웹서비스에 대한 색다른 시도를 중심으로 꾸려진 컬렉션임.

대표 프로젝트 모음

     * Waving Cat eCommerce
          + 편리한 eCommerce를 위해 고양이를 모퉁이에 넣어 손을 흔들게 하면 구매 전환율이 오른다는 발상임
     * Rent a Soul for AI
          + 인공지능에게 소울(혼)을 임대하는 콘셉트의 실험적 서비스임
     * D-Day 3D Shooter
          + 매 초마다 중요한 선택이 요구되는 3D 1인칭 슈팅 게임임
     * Lingoprio
          + 5분 만에 350개 이상의 언어 단어를 빠르게 기억하도록 돕는 초고속 언어 학습 웹앱임
     * Artists Death Effect
          + 예술가가 죽으면 작품의 공급량이 영구히 제한되어 가치가 치솟는 현상에 대한 독특한 시각을 보여줌
     * Sexy Math
          + 수학 학습을 “섹시”하게 만들어 보자는 기발한 콘셉트의 서비스임
     * Easy Pet Drop Box
          + 뭔가를 “뜨겁게” 떨어뜨리는 단순한 게임 아이디어임
     * Spot the Differences
          + 차이점을 찾는 클래식 게임에 유머를 더함
     * Influencer Overnight
          + 10만 팔로워가 되면 계정을 무작위로 넘겨주는 소셜 미디어 실험임
     * Stealing from Dreams
          + 이미지를 선택하면 실제로 아트워크를 생성해 드린다는, 꿈에서 아이디어를 훔치는 콘셉트임
     * A Guide for Aliens Living on Earth
          + 외계인을 위한 지구 가이드북이라는 상상에서 출발한 프로젝트임
     * Puzzle Solvers Agency
          + 퍼즐, 레고, 게임 등 뭔가 풀기 어려운 것은 보내주면 대신 풀어준다는 세계 첫 퍼즐 대행사임
     * Absurd Toilet Water
          + 변기물로 만든 향수라는 극한의 장난 컨셉임
     * Open Celebrity Model
          + 무료 인공지능 기반 오픈 셀러브리티 생성기 서비스임
     * Invisible Lingerie
          + 상상할 수 있는 가장 “섹시한” 란제리를 제안하는 독특한 콘셉트임
     * White Label Art Agency
          + 예술가 지망생들을 위한 “대행 예술” 서비스 제공임
     * Spaceflight Simulator to Mars
          + 실시간으로 7개월(210일) 간 화성 비행을 체험하는 우주 시뮬레이터 게임임
     * Slow Delivery Service
          + 느린 음식, 느린 여행 다음으로 아예 “느린 배송”을 실현하는 서비스임
     * Offset Your Carbon Footprint by Buying Me a Tesla
          + 탄소 배출권을 지불하는 대신 제작자에게 Tesla를 사주자는 패러디임
     * Helicopter Jobs
          + 아무 의미 없는 잡일로 돈을 벌 수 있다는 아이디어임
     * Synchronic Video Battle
          + 육식주의자 대 채식주의자, 트럼프 대 카녜 등으로 동기화된 영상을 보고 투표하는 유쾌한 대결임
     * Eyes Dating Site
          + 얼굴, 몸매, 자기 소개 없이 “눈”만 보고 사랑에 빠지는 데이팅 사이트임
     * Magnetic Buy Now Button
          + 웹사이트 방문객을 무조건 구매자로 만드는 마그네틱 버튼이라는 아이디어임
     * 90s Web Design Studio
          + 90년대 스타일의 “못생긴 페이지”만 제작하는 웹디자인 스튜디오임
     * Dark Mandala Coloring Book
          + 단 하나의 색상(블랙)만으로 채우는 색칠북 제공임
     * Buy Nothing Store
          + “아무 것도 사지 않는 게 필요하다”는 발상의 아이러니한 상점임

전체 컨셉 및 의의

   이 프로젝트들은 실제로 사회적, 기술적 맥락에서 익숙한 개념이나 트렌드를 기상천외한 아이디어로 재해석함. 실질적 유용함보다는 기존 사고의 경계를 허무는 실험적 시도에 의의가 있음. 스타트업 및 IT 종사자 입장에서는, 창의적인 프로덕트 기획과 웹사이트의 틀을 벗어난 유머와 실험성에 영감을 받을 만한 내용임.

        Hacker News 의견

     * 유튜브에서 DIY를 하는 Simone Giertz가 떠오르는 상황, 옛날에는 엉성한 로봇으로 시작했지만 지금은 독특한 주류 제품, 예를 들면 접이식 코트걸이, 롤탑 게임 테이블, 위성 안테나로 만든 의자, 세 다리 강아지를 위한 의족 같은 걸 만드는 모습
     * Operation D-Day: One Second of War가 내가 해본 어떤 비디오 게임보다 더 큰 기쁨을 준 경험 내가 만든 Let's Play 영상도 한번 봐줬으면 하는 마음 https://youtu.be/iUnbD8xp0f0
     * Microtasks for Meatbags라는 개념, AI가 프롬프트를 내고 사람이 실행하는 미래가 오늘날 많은 회사나 조직의 운영방향과 이미 비슷하다는 생각 사람이나 시스템 각각의 강점을 살려 일하는 현실
          + Manna라는 단편 소설을 추천, 바로 이런 식으로 AI가 운영되는 세상 내용
     * Artist's Death Effect Data Base가 정말 웃긴 데이터베이스라는 생각 내 주변에서도 요즘 잘 안 되는 아티스트 이야기를 하면서 그 작품을 몰래 사들이곤 하는 분위기 비슷하게 데드풀 내기 같다는 느낌
     * 재밌는 프로젝트들이 인상적 나는 프랑스어를 다시 배우고있기에 LingoPrio가 특히 마음에 들었음 아이와 소아과에 갔는데 아이가 ""변비""가 프랑스어로 뭐냐고 물어봤고, '-ation'으로 끝나서 딱히 찾아볼 필요가 없었던 순간 웹이 쓸모만 추구하지 않고 그냥 탐험의 대상이 될 때 즐겁다는 생각 나도 꽤 이상하지만 심각하게 임하는 프로젝트가 있음, Pi Bramble을 만들고 거기에 여러 서비스를 추가해서 복잡함에 자체적으로 붕괴해 가는 홈랩을 구축 중, 교육 목적으로 하지만 시간 날 때마다 정말 재미있음 https://clog.goldentooth.net/
          + bios444에게, 댓글이 HN의 섀도우밴에 걸려 안 보이는 상황 absurdwebsite 계정도 본인일 것 같은데 이건 차단 안당했으니 참고 이 계정을 계속 쓸 예정이면 admin에게 이메일 보내보는 게 도움될 거라는 조언
          + LingoPrio를 알아봐줘서 고마움 이런 실험적인 놀이가 진짜 즐거움 프로젝트도 확인해볼 예정
     * 정말 잘 만들었다는 생각 나는 죽음과 관련된 주제에 관심이 있어 Artist's Death Effect가 특히 재미있었음 아이들에게 항상 내 아이디어는 다 구린 거라고 말하는데, 정말 괜찮은 아이디어라면 이미 누군가 했을 거라는 마음가짐
     * 쓸모 없는 재미 프로젝트는 쓸모가 없어도 정말 재미있는 요소 최근에 https://tellconanobrienyourfavoritepizzatoppings.com 을 만들었음 정말 재미로 만들었지만 쓸모는 없음
          + 지금까지 재미있는 피자 토핑 응답들 예시 파인애플, 브리치즈, 머쉬룸, 치즈, 양파와 피망, 살라미, 스피니치, 햄과 파인애플, 감자튀김, 달걀, ""피자를 못 먹는데 코난 오브라이언이 그 사실을 알아줬으면"" 같은 이야기까지 다채로운 응답 내용
     * absurd.website의 ""One Second of War""와 ""microtasks-for-meatbags"" 페이지에서 정말 크게 웃었던 경험 https://absurd.website/dday/game.php, https://absurd.website/microtasks-for-meatbags/
     * One second of war와 테슬라를 사줘서 CO2를 상쇄해준다는 프로젝트가 특히 재미있었다는 이야기 이런 웹사이트에 어울리는 좋은 UI가 뭘까 고민, 박물관처럼 보이게 하려면 무한 스크롤 같은 것도 괜찮을 것 같다는 아이디어
          + 무한 스크롤 아이디어가 정말 좋다는 평가 본인도 UI에 대해 계속 고민 중
     * 이런 프로젝트 모두 내 관심사와 관련된 부분, 공유해줘서 고맙다는 인사
"
"https://news.hada.io/topic?id=21262","빅테크 기업의 스태프 데이터 사이언티스트입니다. 무엇이든 물어보세요(AMA)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               빅테크 기업의 스태프 데이터 사이언티스트입니다. 무엇이든 물어보세요(AMA)

     * Reddit DataScience 채널에서, 미국 실리콘밸리 소재 대형 테크기업의 Staff Data Scientist가 AMA 한 내용 정리
     * 통계학 박사 출신, 경력 약 10년으로 스타트업, 프리IPO 유니콘, FAANG 등 다양한 규모의 기업 경험
     * 대부분 IC(Individual Contributor)로 일했으나, 일부 관리 경험도 있음
     * 머신러닝, 실험/인과추론, 데이터 분석에 주로 강점

주요 질문과 답변 요약

  PhD(박사) 학위의 가치?

     * 박사가 있으면 첫 입사나 초반 커리어에 도움은 되지만, 커리어가 쌓일수록 영향력은 줄어듦
     * 업계(특히 실리콘밸리)는 빠른 속도와 비즈니스 가치에 집중하며, 학문적 엄밀함보다 실용성이 중시됨
     * 다시 결정한다면 5년 이상을 투자해 박사를 하지는 않을 것 같음. 산업 트렌드와 AI 발전 속도가 너무 빠름
     * AI 연구 포지션은 PhD가 필요하지만, 제품 중심 DS/ML 역할에는 MS/BS도 충분함

  커리어와 경력 개발

     * IC에서 Senior 이상 승진하려면, 팀 밖까지 영향을 주고 전략/조직에 기여해야 함
     * 신뢰 쌓기, 크로스펑셔널(협업) 프로젝트 리딩, 경영진/매니저와의 관계 구축이 중요
     * 스타트업에서 성공하려면: 다양한 역할(엔드-투-엔드 ML/데이터 파이프라인/분석)을 소화할 수 있는 만능형, 비즈니스에 대한 열정 필요
     * 경력 초반에는 기술력, 중후반엔 도메인 지식과 커뮤니케이션·리더십이 더 중요해짐

  데이터 과학자의 미래와 AI

     * AI가 단순/반복적 업무는 빠르게 대체하지만, 핵심 역량 있는 데이터 과학자는 오히려 돋보임
     * Generative AI 도입 이후 업무 방식(코딩, 문서작성 등)은 변했지만 핵심 역할(분석, 모델링 등)은 여전히 유효
     * 앞으로 AI가 더 많은 영역을 차지할 수 있지만, 복잡한 문제 해결과 도메인 해석, 소통 능력은 여전히 인재의 기준

  실무와 조직 문화

     * 인과추론/실험(AB Test) 중요성은 도메인에 따라 다름. 실험이 불가한 환경에선 관측 데이터 기반 인과추론 능력이 필수
     * 비즈니스/리더와 소통할 땐: 기술적 디테일보다는 ""왜, 무엇을 할 것인가""와 영향에 집중해서 설명
     * 조직 문화가 인과추론, 데이터 품질을 중시하지 않으면 의사소통과 변화에 한계. 이직·팀 이동 고려 필요
     * 매니저가 문제 인식을 못할 때는 신뢰 쌓기, 해결책 제시, 필요시 내부 이동·퇴사도 고려

  역량 개발 및 취업 조언

     * 경력 초기에는 인턴십/포트폴리오/프로젝트 경험이 중요. 기술 역량(코딩, ML 등)은 최소한의 기대치
     * 도메인 지식, 커뮤니케이션, 문제 해결력은 AI·자동화 시대에도 더욱 차별화 포인트
     * 학위·자격증보다는 실무 프로젝트와 경험이 더 우선
     * 업계 채용은 ‘경험’을 중요시. 인턴, 컨설팅, 다양한 프로젝트 경험 쌓기 권장

  기타 인사이트

     * 스타트업이 망하는 신호: 사기 저하, 핵심 인력 이탈, 핵심 지표 악화 등
     * 정치적 역량: 조직 전략과 연계된 프로젝트에 집중하고, 영향력 있는 관계 맺기
     * 고연봉/복지: Netflix 등 빅테크 상위직 연봉 $750k 이상 현실적임(주식 포함)
     * 커리어 만족감: 빅테크에서 데이터 규모 외에는 업무가 평이, 성장·재미 위해 부업 탐색 중
"
"https://news.hada.io/topic?id=21278","Show GN: itdoc - Swagger 없이 정확한 Node.js API 문서 만들어 보세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show GN: itdoc - Swagger 없이 정확한 Node.js API 문서 만들어 보세요

소개

   API 문서, 여전히 수동으로 작성하고 계신가요?
   테스트만 잘 짜면, 문서가 자동으로 만들어지는 오픈소스를 만들었습니다.

이런 분께 추천합니다

     * Node.js / TypeScript 백엔드 개발자
     * API 문서 작성이 귀찮고 반복적이라고 느낀 적이 있다
     * 실제 API와 문서 내용이 달라 협업이 꼬인 경험이 있다

프로젝트 링크

     * Github : https://github.com/do-pa/itdoc
     * 공식 문서 : https://itdoc.kr

   로고가 정말 귀엽군요

   감사합니다 😆

   이건 문서로만 봐서 잘 이해가 안되는데.. swagger를 대체할수 있다는거죠?
   swagger보다 더 뛰어나고 보면 되는지요?? ㅎㅎ

   리드미를 좀 더 보강할 필요가 있어보이네요. 댓글 감사드립니다!

   https://itdoc.kr/blog/itdoc

   이 글 한번 읽어보시면 궁금증이 해소되실거라 믿습니다 ㅎㅎ

   괜찮네요ㅎㅎ

   감사합니다 🙇‍♂️

   아시겠지만..
   이런것도 있습니다.
   https://github.com/swagger-api/swagger-codegen

   openapi 문서포맷이면..
   node.js 코드로 생성해줍니다.
   사용해보니.. 쓸만하더군요..

   서버코드도 클라이언트 코드도 생성해주는데..
   일단 기존에 Rest API 관련 코딩 경험이 있으면
   도움이 많이 되지 않을까 싶어요.

   잘 찾아보면.. 해당 코드 포크해서 더 많이 업데이트 되고 있습니다.

   좋은 댓글 감사합니다!
   말씀해주신 도구도 훌륭하다고 생각합니다.

   이 기회에 itdoc과의 차이를 간단히 설명드리면, 핵심적인 차이는 바로 Design-First vs Code-First(itdoc) 접근 방식입니다.

   일부 팀은 OpenAPI 스펙을 먼저 설계하고 API 개발을 시작하는 Design-First 방식을 선호하고, 또 다른 팀은 실제 코드 구현을 먼저 하고 나중에 문서를 추출하는 Code-First 흐름이 더 자연스러울 수 있습니다.

   itdoc은 후자의 경우에 더 적합한 도구로, 테스트 기반으로 실제 동작을 바탕으로 문서를 생성하는 점이 특징입니다. 팀의 개발 방식과 선호도에 따라 적절한 도구를 선택하시면 좋을 것 같아요!

   아래와 같이 사람이 읽을 수 있는 코드로 문서를 생성할 수 있습니다.
describeAPI(
    HttpMethod.GET,
    ""/users/:userId"",
    {
        summary: ""사용자 조회 API"",
        tag: ""User"",
        description: ""특정 사용자의 상세 정보를 조회하는 API입니다."",
    },
    targetApp,
    (apiDoc) => {
        itDoc(""유효한 사용자 ID가 주어지면 사용자의 상세 정보가 나온다."", async () => {
            await apiDoc
                .test()
                .req()
                .pathParam({
                    userId: field(""유효한 사용자 ID"", ""penek""),
                })
                .res()
                .status(HttpStatus.OK)
                .body({
                    userId: field(""유저 ID"", ""penek""),
                    username: field(""유저 이름"", ""hun""),
                    email: field(""유저 이메일"", ""penekhun@gmail.com""),
                    friends: field(""유저의 친구"", [""zagabi"", ""json""]),
                })
        })
  ....
"
"https://news.hada.io/topic?id=21247","Show GN: every10 – 키워드 기반 뉴스 알림 (텔레그램으로 10분 간격 전송)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Show GN: every10 – 키워드 기반 뉴스 알림 (텔레그램으로 10분 간격 전송)

    소개

   ""every10은 내가 설정한 키워드에 맞는 뉴스만 골라, 10분 간격으로 텔레그램으로 알려주는 뉴스 알림 서비스입니다.""
   더 이상 뉴스를 일일이 검색하지 않아도, 원하는 정보만 자동으로 받아볼 수 있습니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    주요 기능

     * 뉴스 크롤링: 주요 뉴스 섹션을 주기적으로 수집
     * 키워드 기반 필터링: 내 관심사에 맞는 뉴스만 선별
     * 실시간 텔레그램 알림: 더 이상 뉴스 검색에 시간을 쓰지 않아도 됩니다
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    기술 스택

     * Next.js + Prisma + Telegram API + Resend + Supabase
     * 알림 전송 주기: 10분 (cron 기반)
     * 키워드 변경 시 즉시 반영
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    사이트링크

   every10 바로가기

   빠르게 뉴스 접할 수 있었는데 8월 5일 마지막으로 오고 오지 않네요.

   샘플이 있으면 좋겠습니다.
   어떤 키워드를 써야 할 지 잘 모르겠습니다

   피드백 주셔서 감사합니다.!
   저 같은 경우는 미국주식을 하는데 요즘 핫한 키워드인 트럼프나 일론머스크같은 키워드를 넣어서 뉴스를 확인합니다.
   만약 축구를 좋아하신다면 UEFA,호날두등 이런키워드를 사용하여 뉴스를 확인하시는것도 괜찮은 방법일거 같습니다.

   로그인이 안되는 것 같아요

   로그인 테스트해봤는데 잘되는거 같은데 혹시 아직도 안되나요?
"
"https://news.hada.io/topic?id=21277","Ask GN: 폼피드 문자(Ascii 0x0C)를 잘 처리하는 텍스트 에디터가 있을까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Ask GN: 폼피드 문자(Ascii 0x0C)를 잘 처리하는 텍스트 에디터가 있을까요?

   요즘에야 FF 문자를 쓸 일이 잘 없지만, 오래된 텍스트파일에는 종종 FF가 등장합니다.

   저는 Notepad++을 쓰고 있는데 이 프로그램은 FF를 제대로 처리해주지 않아요. 페이지를 나눠 보여주는건 어렵겠지만 최소한 줄바꿈이라도 해주면 좋을텐데, 그냥 무시하고 다음 줄을 붙여서 출력해줍니다.

   혹시 FF를 잘 처리하는 윈도우용 텍스트 에디터가 있을까요?

   vim이나 gVim에서 줄로 표시되게 할 수 있습니다.

   https://gist.github.com/terminatorul/06f5e8ec5b291127a3d77798a12d0d21

   차라리 별도 스크립트나 명령어로 치환할수는 없을까요?

   https://www.gnu.org/software/emacs/manual/html_node/emacs/Pages.html

   emacs 매뉴얼을 보니 formfeed에 대한 내용이 있는데 emacs를 안 써 봐서 제대로 작동할지는 모르겠네요.

   vscode에 제어문자 표시(render control characters) 기능이 있습니다. View - Appearance - Render Control Characters 메뉴 순서로 기능 상태를 확인하실 수 있습니다

   넵. 그 기능을 켜면 FF가 있음을 보여주긴 하는데, 그건 Notepad++에도 있는 기능입니다. 제가 찾고있는건 Form Feed문자가 나타나면 페이지 넘김 또는 줄 바꿈으로 처리가 되는거에요.

   https://github.com/microsoft/vscode-extension-samples/…

   vscode 확장 프로그램 예제 중에 커스텀하게 decorator를 적용하는 게 있는데 이걸 응용하시는 게 빠를 수도 있을 것 같습니다. DecorationType 을 정의하고 특정 문자의 위치를 찾아서 그 위치에 DecorationType을 지정하는 원리로 보이는데요. DecorationType 옵션에 { after: { contextText: ""\n"" }} 를 사용하시면 줄바꿈표시를 추가하는 방식으로 구현이 가능해보입니다

   줄바꿈 문자열 세트에 FF를 추가하는게 제일 깔끔할 것 같은데 vscode에서 그런 환경설정은 못찾았습니다

   챗지피티에게 물으니 vscode에서 Render Whitespace 또는 Highlight Bad Chars 같은 확장을 쓰면 표시는 된다고 하는것 같아요

   VSCode에 Render Whitespace라는 이름의 확장은 없군요. 비슷한 이름의 Render Special Characters 확장은 단지 강조만 해 주네요. Highlight Bad Chars 확장도 마찬가지구요.

   특수문자가 있음을 보여주는건 이미 Notepad++에서도 가능합니다. 제가 찾고있는건 Form Feed문자가 나타나면 페이지 넘김 또는 줄 바꿈으로 처리가 되는거에요.

   vscode에선 어떻게 나오려나요
"
"https://news.hada.io/topic?id=21270","Builder.ai 붕괴: 15억 달러 AI 스타트업, 실제로는 인도인들이 봇 행세한 것이 드러남","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Builder.ai 붕괴: 15억 달러 AI 스타트업, 실제로는 인도인들이 봇 행세한 것이 드러남

     * Builder.ai가 15억 달러의 가치로 평가받았으나, 실제로는 인간 개발자가 AI로 위장해 운영
     * Microsoft 등 대형 투자사가 4억 5천만 달러 이상을 투자했으나 부정확한 매출 보고와 내부 구조 개편이 이어짐
     * 최근 Viola Credit의 3,700만 달러 자금 회수로 주요 국가에서의 운영이 사실상 마비됨
     * 자랑하던 'AI' 기술이 실제로는 인도 개발자들이 봇처럼 행동하는 시스템임이 드러나면서 투자자 신뢰가 급락함
     * 이번 사태는 AI 스타트업의 투명성과 마케팅 윤리에 대한 업계 내 심각한 의문을 제기
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Builder.ai 사태 개요

     * Builder.ai는 15억 달러 가치의 영국 AI 기반 노코드 스타트업으로 알려졌으며, 뛰어난 자동화와 인공지능으로 주목을 받음
     * 전략적 파트너 Microsoft 및 카타르투자청이 주도한 2억 5천만 달러 투자를 유치했으나, 최근 파산 보호 절차를 시작한다고 공식 발표함
     * Bloomberg 보도에 따르면 주요 대출사 Viola Credit이 3,700만 달러를 회수하며 회사는 5개국에서의 주요 운영이 불가능한 상황에 직면함
     * 2024년초 CEO 교체와 대량 해고가 있었으며, 영국, 미국, UAE, 싱가포르, 인도 등에서 파산 신청 절차를 밟게 됨

Builder.ai의 투자금 및 약속

     * 2016년 설립된 Builder.ai는 기업들이 인공지능의 힘으로 맞춤형 앱을 쉽게 개발할 수 있다고 홍보함
     * Microsoft, 세계은행 IFC, WndrCo, Lakestar, SoftBank DeepCore 등 유명 투자자들로부터 총 4억 5천만 달러 이상을 유치함
     * 최근 2년간의 매출이 과장되어 보고되었다는 사내 우려 및 과거 수치 하향 조정이 이루어짐
     * 투자자들에게 실적이 부풀려져 전달됐다는 내부자 폭로가 나옴

'봇'의 정체는 인도 개발자

     * Bloomberg는 허위 매출 제시, 과장된 내부 구조 변화, 신뢰 저하가 연쇄적으로 이어졌음을 지적함
     * 금융기업 Zero Hash의 Linas Beliūnas는 Builder.ai가 실제로 AI가 아닌 인도 개발자들이 봇 행세를 하며 개발을 진행했다고 공개함
     * CEO Duggal이 투자자들에게 거짓 매출 수치를 제출했다는 의혹도 추가적으로 제기됨
     * 8년 동안 인공지능을 위장한 채 운영해 온 사실이 밝혀져 충격을 줌

투자자 기대의 붕괴

     * 2023년 Builder.ai에 5천만 달러를 투자했던 Viola Credit은 3,700만 달러를 회수하며 회사는 급격한 위기에 봉착함
     * 급여 지급과 필수 운영 유지 능력이 상실되었으며, 남은 자금은 인도 계좌에 있으나 규제로 인해 사용이 불가함

Builder.ai 붕괴의 업계 파장

     * Builder.ai 사태는 현재 AI 스타트업 시장의 불안정성과 거품을 상징적으로 드러냄
     * Info-Tech Research Group의 Phil Brunkard는 많은 AI 기업들이 과장된 기대와 부족한 재무 관리를 바탕으로 무분별하게 확장 중임을 지적함
     * 카타르투자청이 이번 사태로 큰 손실을 보았으며, 2년 전 2억 5천만 달러 투자를 주도함
     * 경영진의 AI 마케팅 관행에 대해 경쟁 당국의 조사가 예고되고 있음
     * 인공지능 기술이 빠르게 발전하는 동시에, 복잡한 기술 영역에서 인간 전문성의 대체 불가성에 대한 교훈을 남김
     * 이번 사건은 투명성과 책임, AI 마케팅 윤리에 대한 업계 전반의 진지한 성찰 기회를 제공함

   AI가 Actually Indian이었던 셈이군요

   A lot of Indian

   인도인 방 사고실험

        Hacker News 의견

     * 두 달도 안 된 시점에 Builder.ai가 핵심 매출 수치를 하향 조정하고, 지난 2년간의 재무 상황을 감사인에게 의뢰했다는 사실이 인정된 상황이었음, 이전 직원들은 과거 투자자들에게 매출 실적이 부풀려졌다고 주장했던 만큼 솔직히 흥미로운 무언가를 기대했지만, 고전적인 회계 부정일 뿐이라는 실망감
          + 그래서 투자자들이 투자를 중단했다는 의견
     * 더 신뢰할 만한 출처의 기사 Times of India 기사 링크 소개
          + Bloomberg 기사 Bloomberg 기사 링크도 참고 자료로 제시
          + 이건 가짜 뉴스라는 주장, Builder.ai는 대부분 인도 개발자들을 고용해서 앱을 만드는 전형적인 개발 에이전시 방식이며, Infosys 같은 인도 IT 업체와 다르지 않다는 설명, 문제의 핵심은 자체 Foundation Model로 만든 가상 비서 ""Natasha""였고, 그게 완성되기 전에 자금이 소진된 점이라는 의견
     * 4억 5천만 달러의 자금이 도대체 어디로 갔는지에 대한 궁금증, 인도 개발자 700명이 8년 동안 일했다 해도 비용이 훨씬 적게 들었을 거라는 의구심
          + 실비 지출이 아닌 사기극의 본질은 결국 자금을 펑펑 쓰는 것이라는 설명
          + 단순 계산으로 4억5천만불을 8년 700명에게 나누면 한 사람당 연 8만불 정도라서 말이 안 되는 액수는 아니라는 계산
          + Builder.ai는 2016년부터 운영되어 왔고, 스타트업들이 4억 5천만 달러를 훨씬 빨리 태워버린 전례도 많다는 의견
          + Chai(차) 예산은 정당하게 인정할 만한 경비라는 유쾌한 농담성 의견
     * 이번 뉴스가 새로운 소식은 아니고, 약 9일 전에 FT가 이미 보도했고 해당 토론이 Hacker News에서도 이루어졌다는 정보 제공 (이전 토론 링크)
          + 하지만 본인에게는 처음 듣는 소식이었고, 기사들은 때때로 다시 떠오르기도 하며 아주 자연스러운 논쟁이 유발된다는 점에서 다시 공유하는 것도 긍정적으로 바라본다는 견해
     * 앱 빌더를 만드는 게 그렇게 어렵지 않다고 생각하는데, 오픈소스 레포도 많고 실제로 slashml.com을 2주 만에 만든 경험도 있음, 자본이 많았던 Builder.ai보다 결과적으로 훨씬 간단하게 만든 경험 공유 (slashml.com 링크)
          + 전체적으로 하나의 독립적 간단한 앱 빌더는 쉽지만, 그 이상은 여전히 난제이며 풀리지 않은 문제라는 의견
          + Builder.ai가 2016년에 시작했다는 정보 제공
          + 일반적인 소프트웨어 컨설팅 회사에서 허술한 자체 앱 빌더 스크립트로 시작해, 나중에 AI 회사처럼 포장해서 기업가치만 키웠을 가능성에 대한 추측
          + slashml.com 써보겠다는 반응
     * 2019년에 벤처캐피털 인턴 시절 Builder.ai에 대해 실사를 했었는데, Glassdoor 리뷰나 임직원과 대화만으로도 불성실함이 명백히 드러났고, 15분 정도 구글링만 해도 뭔가 이상하다는 확신을 얻을 수 있었음에도 불구하고 5억 달러 이상을 조달했다는 사실에 놀라움을 갖는다는 고백
          + 구체적으로 어떤 점에서 의심스러웠는지, 실사 당시의 마케팅이나 설명에서 이상점을 발견했는지 궁금하다는 질문
     * 또 다른 AI 사기로 평가, 한때 Amazon에서는 'Just Walk Out'이라고 불린 비슷한 사례가 있었음을 언급, 인공지능으로 획기적인 걸 만들었다는 제안이 나오면 신중히 보라는 교훈을 남김
     * 스케일에 맞지 않는 일을 극단적으로 실행한 회사라고 농담하는 반응
     * 요즘 실제 AI가 엄청나게 발전해 있는 상황에서 정말 이상한 일이라는 의문
          + 멋진 AI 스타트업 아이디어가 있다면 초반 프로토타입은 실제 AI 없이도 더 빠르게 만들 수 있지만, 실제로는 AI 대신 인도 개발자들이 95% 정확도로 일하고 있었는데 AI가 85%밖에 못 따라간다면 투자나 고객을 속인 꼴이 되고 만다는 지적
          + 고객이나 투자자들은 GitHub Copilot처럼 AI가 빠르게 코드를 만들어 주는 것을 기대했을 텐데, Builder.ai는 앱 하나 만드는데 며칠, 몇 주가 걸리는 상황이니 눈치채지 않았을지 의문, 정말로 인도 개발자들이 엄청나게 빠른 것도 아니라는 농담
          + 실제로 AI가 할 수 있지만 지금의 기술 수준에서 비용 효율성이 떨어졌을 것이고, 결국 ChatGPT 같은 거 구현하다가 수익성 있는 모델이 나오기 전에 자금이 바닥났을 수 있다는 분석. 타이밍이 빨랐던 것뿐이라는 긍정적 평가
          + 실제로 마케팅만큼 잘 작동하지 않는다는 생각
     * EvenUp이라는 또 다른 유사 사례를 소개하며 관련 기사 링크를 남김 (EvenUp AI 기사 링크)
"
"https://news.hada.io/topic?id=21301","Cursor 1.0 릴리즈","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Cursor 1.0 릴리즈

     * AI 코드 에디터인 Cursor의 메이저 릴리즈. BugBot 코드 리뷰, 메모리 기능, 빠른 MCP 설치, Jupyter 지원, Background Agent 전체 공개 등 주요 업데이트를 포함
     * BugBot이 자동으로 PR을 검사해 버그를 찾아주고, Cursor에서 즉시 수정 가능함
     * 이제 Background Agent 기능을 모든 사용자가 바로 활용할 수 있음
     * Jupyter Notebook 내에서 Agent가 여러 셀을 직접 생성 및 편집하여 연구와 데이터 작업 효율이 크게 향상됨
     * Memories 베타는 프로젝트별 대화 정보를 기억하여 생산성을 높여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cursor 1.0 주요 업데이트 요약

   Cursor 1.0은 AI 코드 에디터인 Cursor의 메이저 릴리즈로, 다양한 기능 향상과 새로운 생산성 도구 도입을 통해 사용자 경험을 강화함

  BugBot: 자동화된 코드 리뷰 기능

     * BugBot은 PR에서 자동으로 코드 리뷰를 수행하고, 버그나 잠재적 문제를 찾는 역할을 담당함
     * 문제가 발견될 경우, GitHub의 PR에 직접 코멘트를 남기며, ""Fix in Cursor"" 버튼을 클릭하면 에디터로 이동하여 사전 작성된 프롬프트로 수정 작업을 시작할 수 있음
     * BugBot 활용법은 공식 문서에서 확인 가능함

  Background Agent 전체 공개

     * 초기 얼리 액세스 이후, 모든 사용자에게 Background Agent 기능이 오픈됨
     * 채팅창에서 클라우드 아이콘 클릭이나 Cmd/Ctrl+E 단축키로 즉시 사용할 수 있음
     * 프라이버시 모드 활성화 시에는 곧 별도 활성화 기능 제공 예정임

  Jupyter Notebook 내 Agent 지원

     * 이제 Cursor에서 Jupyter Notebook 내부의 여러 셀을 직접 생성 및 편집할 수 있는 기능이 도입됨
     * 이 기능은 초기에는 Sonnet 모델에서만 지원됨
     * 연구 및 데이터 과학 작업의 효율성을 크게 높여줌

  Memories: 대화 내용 기억 기능

     * Cursor의 Memories는 대화에서 학습한 정보를 각 프로젝트별로 기억하여, 향후 참고 가능함
     * 메모리 관리는 Settings에서 프로젝트 단위로 설정 및 관리할 수 있음
     * 베타 버전으로 제공 중이며, Settings → Rules에서 활성화 가능함

  MCP 원클릭 설치 및 OAuth 지원

     * MCP 서버를 Cursor에서 원클릭으로 설치하는 기능과 OAuth 연동 지원이 추가됨
     * 공식 MCP 서버 리스트를 별도 문서에서 참고 가능함
     * MCP 개발자는 ""Add to Cursor"" 버튼을 활용해 자신의 서버를 손쉽게 개발자에게 공유할 수 있음

  대화 내 시각화 및 마크다운 테이블 지원

     * Cursor는 대화 중 Mermaid 다이어그램, Markdown 표 등 시각화 요소를 직접 생성하고 표시하는 기능이 강화됨

  새로운 Settings와 대시보드

     * 설정 및 대시보드 페이지 인터페이스가 개선되어, 개인/팀 사용량, 도구별 통계 등 상세 데이터를 제공함
     * 계정 이름 변경이나 도구/모델별 분석 정보도 확인 가능함

    추가 개선사항

     * @Link 및 웹검색이 PDF 컨텍스트 파싱 지원
     * 설정 내 네트워크 진단 도구 추가
     * 병렬 도구 호출로 응답 속도 개선
     * 대화 내 도구 호출 결과를 접거나 펼칠 수 있는 UI 제공

    계정 및 팀 관련 기능

     * 엔터프라이즈 사용자는 안정화 버전만 접근 가능
     * 팀 관리자는 프라이버시 모드 비활성화 가능
     * 팀의 사용량과 비용 데이터 접근을 위한 Admin API 제공

    모델 관련

     * Gemini 2.5 Flash에서 Max mode 사용 가능

        Hacker News 의견

     * 나는 Cursor Pro 연간 구독을 두 개(학생 계정과 Lenny 뉴스레터)를 가지고 있고, 최근에 Claude Code에 100달러를 결제한 이후로 코딩 작업에서는 Cursor AI를 아예 사용하지 않고 있음 경험 차이가 압도적이고, 잘못된 툴 호출이나 대화가 중간에 끊기는 일, 변경 사항이 반영되지 않거나 파일 전체가 덮어써지는 문제도 발생하지 않음 Concurrent session이 더 많은 200달러 Claude Max로 업그레이드도 고려 중임 다른 사람들이 이 글이 광고 같다고 생각하면, 직접 사용자 피드백을 찾아보라고 말하고 싶음 Claude Code가 그 정도로 만족스러운 수준임
          + 오랫동안 Cursor를 써온 입장에서 이번에 처음 Claude Code를 사용해봤는데, 명령줄에서 read 명령을 잘못 반복 호출하거나 문자열 ""EOF""를 불필요하게 붙이고, ""!="" 같은 잘못된 코드를 쓰는 등 자잘한 문제점이 있었음 내가 /model opus 옵션을 켜둔 탓에 23줄짜리 함수를 쓰는데 7달러가 청구되기도 했음 결과적으로 최종 코드만큼은 항상 맞긴 했지만 솔직히 Cursor만큼 인상적이지 않음 에이전트 동작도 Cursor의 agent mode와 비슷하게 느껴짐 7달러를 한 함수에 쓰게 되는 부분은 월 100달러 플랜을 쓰면 해결되겠지만, 선뜻 넘어갈지 고민임
          + 예전에는 저렴하게 코딩 에이전트를 써볼 수 있어서 Cursor에서 요청이 느려도 참고 썼지만, Cline이나 Roo에 비해 압도적으로 좋지는 않다고 느꼈음 그런데 Claude Code를 처음 써봤을 때는 성능 차이가 바로 체감됐고, 나뿐 아니라 에이전트 역할 측면에서도 월등했음 하지만 너무 비싸서 망설이다가 200달러 Max 플랜이 나온 이후 지금은 매우 만족스러움
          + Cursor에 매년 800달러 정도를 썼지만 지금은 Claude Code 200달러 구독으로 갈아탔고 만족도가 훨씬 높음 물론 Cursor의 “checkpoints(변경사항 롤백)”같은 기능이 없다는 건 아쉽긴 하지만 99%의 Vibe Coding에는 부족함이 없음 git worktree로 2~4개 세션을 동시에 돌리는데 속도도 믿기지 않을 정도임 물론 완전히 완벽하진 않아서 코드 리뷰는 필요하지만 Claude Code 나름의 “memories(.cursorrules와 유사)”를 잘 활용하면 거의 완벽하게 맞춤형 결과를 얻을 수 있음
          + Claude Code가 구체적으로 어떤 부분에서 훨씬 뛰어난지, 그리고 내장 IDE 기능 부재에도 불구하고 왜 그렇게 평가하는지 자세히 설명해줄 수 있는지 궁금함
          + Claude Code와 Aider + Anthropic API 조합을 비교해 본 사람이 있는지 궁금함 나는 Claude.ai 인터페이스를 쓰다가 Aider + Anthropic API로 갈아탔는데 개발 경험 자체는 Aider가 훨씬 낫다고 느낌 Claude Code가 과연 그것보다 더 좋은지 알고 싶음
     * Zed에서 agent mode를 출시한 이후로 Cursor 사용을 그만둠 Cursor는 모든 확장 기능을 꺼도 메모리 릭 문제가 계속 발생하고, Zed가 훨씬 가볍고 에디터로서도 더 나은 경험이라고 생각함 추가로 Trae가 요청 수를 늘리려면 10달러가 드는 것도 Cursor의 매력을 떨어뜨림
     * Cursor가 어떻게 돈을 버는지 이해가 안됨 나는 Cursor에서 하루에 10~20달러 어치 추가 요청을 쓸 정도로 사용량이 많음 만약 windsurf에 모델 제공업체 API를 연결하면 토큰 사용량 때문에 하루에 100달러는 쓸 거 같음 Cursor에 내 API키를 연결하면 초당 50건 제한을 훨씬 넘겨서 바로 rate limited 당함 Claude Code도 써봤지만 나에게는 Cursor 경험보다 못함 지금 내 작업에 최고 모델은 Claude 4(non-max/non-thinking)라고 생각함 Cursor가 어떻게 현재 요금제로 수익이 나는지 의문임 내 서비스에서 고객한테 바로 ROI가 나오는 덕에 부담을 정당화하긴 하지만, Cursor의 요금 구조가 현실성 없다고 느낌 최근 4일간 Cursor 사용량은 요청 1049건, 에이전트 편집 301K라인, 탭 승인 84건임 개인적으로 Cursor에 큰 불만은 없고 기능이나 옵션 추가만 바라고 있음 비동기 요청이 지원된다면 더 좋을
       텐데, 10개 파일을 한 번에 요청하면 순차 처리라 오래 기다려야 하고, 다른 워크스페이스에서 작업하면서 대기 시간만 길어짐 병렬 실행이 지원되면 대기시간도 단축될 거라고 생각함
          + Cursor는 돈을 벌지 못하고 있음 VC 자금으로 운영되고 있음 Anthropic, OpenAI도 이윤이 나지는 않지만 Cursor는 오히려 더 심하게 적자를 내고 있다고 생각함
          + AI 보조 개발 시장이 폭발적으로 성장 중이고, 토큰 비용도 계속 떨어지고 있기 때문에 고객 유치를 위해 당분간 사용량을 보조(비용을 감수)하는 전략이 합리적이라고 봄 장기적으로 사용 비용이 낮아질 거라는 기대가 있음
          + 동시에 3개의 채팅 탭을 열 수 있고 각각이 별개의 에이전트 역할을 할 수 있음 Cmd+T 단축키 안내와 함께 참고 링크: https://docs.cursor.com/kbd
          + Cursor가 매출 몇백만 달러를 올린다고는 하지만, GPU 비용이 얼마인지 공개하지 않음 내가 짐작하기로는 매출을 전부 태우고 있고 수익화에서는 멀었다고 생각함
          + 마지막으로 바라는 기능이 실제로 이번 메이저 릴리즈에 추가되었다고 하던데, 나도 기대 중임
     * Cursor를 좋아하지만, 확장 프로그램의 업데이트가 VSCode만큼 빠르지 않아서 불안감이 있음 또 문제는 공유 MCP 서버가 없는 점임 VSCode, Cursor, Claude 각각이 자신만의 MCP 서버를 돌려서, 여러 MCP를 동시에 띄우면 메모리 사용량이 불필요하게 커지는 상황임
          + Cursor가 더 위험한 건 업스트림 모델 제공업체 의존도가 매우 높다는 점이고, 특히 Claude Code처럼 각 업체의 경쟁 제품이 등장하는 상황에서 더더욱 불안요소라고 생각함
          + 나는 IDE를 두 개 돌림 Cursor는 에이전트 래퍼 역할만 하고, 실제 개발은 Jetbrains 제품에서 처리함 Cursor로 태스크 반복 작업을 시키고, 나는 코드 리뷰/수정에 집중하는 방식이 오히려 편함
          + 이런 이유로 나는 Docker MCP Catalog를 씀 하나의 MCP 서버로 통합하는 솔루션이고, 자세한 내용은 https://www.youtube.com/watch?v=6I2L4U7Xq6g 참고 권장함
          + Cursor 및 다른 VSCode 포크들은 Open VSX(https://open-vsx.org/)로 확장프로그램을 연결함 마이크로소프트 확장 몇몇만 빼면, 대부분 필요한 확장이 Open VSX에서 잘 업데이트되고 있음 Cursor는 Python, C++ 같은 주요 MS 확장은 자체적으로 지원 가능한 자금도 가지고 있다고 생각함 단점이라면 Cursor에서 사용하는 VSCode 버전이 수개월씩 오래되어서, 최신 확장은 못 쓰는 구간이 생긴 점임
          + streamable http 등 다른 트랜스포트 방식을 쓰는 MCP 시스템을 직접 만들어서, 1개 인스턴스만 돌리고 플러그인은 config 파일에서 추가하는 구조로 개발함 관련 깃헙: https://github.com/tuananh/hyper-mcp
     * Cursor를 여러 번 떠났다가 다시 돌아오고 있음 최신 LLM이 Cursor의 적용 모델을 방해하는 스타일인데, LLM이 “이거 답답하다”라고 직접 쓰는 게 처음엔 재밌었지만, 점점 VSCode와의 호환성이 멀어지는 게 아쉬움 만약 의도적인 디자인 변화였다면 납득했겠지만, MS가 Cursor를 점점 밀어내는 느낌임 예를 들어 MS Dev Containers 플러그인은 내부적으로 추천되지만, 설치하면 자동으로 삭제되고 Anysphere의 Remote Containers로 대체되며 이 기능은 기존보다 지원 범위가 좁음 최근의 MS 확장(예: Postgres)도 없는 상황임
          + 나는 머지않아 MS가 VSCode 라이선스를 바꿔서 Cursor 같은 앱이 더 못 나오게 만들 거라고 예상함 천천히 포크가 계속될 수는 있겠지만, MS 지원 없이 생태계가 금방 약해질 거라 생각함
     * multi root repo 지원이 제대로 됐으면 좋겠음 최근 업데이트에서 지원이 더 좋아졌다고 했지만, 설명서나 가이드가 없고 릴리즈노트 한 줄에 그친 느낌임 실사용에서는 모델이 디렉토리를 제대로 인지 못해서 잘못된 레포지토리에만 계속 접근하려 들고, 그때마다 내가 직접 경로를 가이드해야 함 이런 환경에서 성공적으로 사용한 경험이 있는지 궁금함
          + 나는 directory 구조를 알려주는 cursor rule을 만들어 써서, X는 UI, Y는 BE, Z는 auth 등 간단하게 요약해서 입력함 이 정도면 어느 폴더를 접근해야 할지 인지하는 데 충분했음
          + Cursor 개발자임 상황을 더 자세히 설명해주면 좋겠음 스크린샷이나 요청 id를 feldstein at anysphere.co로 보내주면 좋겠음
     * VSCode + Copilot [자동완성] + CLINE [AI채팅] + FOAM [Obsidian 스타일 마크다운] 조합이 최고라고 생각함 폐쇄형 대체제가 이걸 따라갈 수는 없다고 봄
          + 다른 대체제를 진지하게 테스트해봤는지 궁금함 나는 Aider만 써봤는데( Cline 경험은 없음), 그쪽은 네이티브 VSCode보다 못하다고 느낌
          + 왜 그런지? Copilot Agent나 Junie 등도 있는데, 기존 경쟁 제품들도 거의 UX나 모델 선택지가 비슷하다고 생각함
          + FOAM/Obsidian은 마크다운 + 그래프 빌딩 기능이 전부인지, 추가적인 마크다운 키워드가 있는지도 궁금함
          + ""is goat""가 무슨 뜻인지 궁금해하는 사람임
          + 나는 Cline하고 Claude Code를 프로젝트에서 다 써봤는데, Claude Code 쪽이 훨씬 더 뛰어나다고 평가함
     * Cursor에 바라는 점: 에이전트 모드에서 변경 사항을 리뷰할 때, 어떤 이유/과정으로 모델이 코드를 바꿨는지 명확하게 뜨지 않아서 일일이 채팅 응답을 뒤져야 하는 게 불편함 예를 들어 최근에 shadcn을 프로젝트에 세팅해달라고 모델에 부탁했는데, 결과적으로는 CLI가 만든 진짜 변경과 AI의 헛소리가 뒤섞여서 구분이 힘들었고 결국 내가 직접 매뉴얼을 읽고 해결했음 코드 각 줄마다 변경 이유나 출처가 표시되는 기능이 있으면 좋겠음 마치 주석처럼, 코드에 지저분하게 주석을 추가하지 않고 말임
          + 결국 버전 관리 시스템이면 되는 거 아니냐는 의견이고, 이건 git이 커버하는 부분이라고 코멘트함
          + 친구랑도 얘기했는데, 수작업으로 입력한 코드와 AI 생성 코드는 반드시 구분돼서 미래 코드 생성 시 참고 우선순위가 높아져야 한다는 얘기임
     * Cursor 1.0이 대형 제품임에도 지나치게 차분하게 출시되어서 놀랍다는 의견임 큰 회사 제품은 화려한 영상, AGI 약속, ""새로운 시작"" 같은 홍보가 넘쳐야 하는데 이번에는 아무 것도 없었음 관련해서 Cursor 공식 트윗 https://x.com/cursor_ai/status/1930358111677886677도 찾았다고 공유함
          + 우리가 실제로 평범한 동영상 스타일을 의도했으니, 만약 그렇게 보이지 않았다면 피드백을 바란다는 제작 측 코멘트임
     * 프라이버시 모드에서는 아직 Background Agents를 쓸 수 없어서 이 기능이 어떻게 확대될지 궁금함 Cursor는 코드가 서버에 저장되지 않는다는 보장을 제공하는데, 보안 페이지 기준 약 50%의 사용자가 프라이버시 모드를 사용 중임 OpenAI Codex와 비교하면, 복잡하거나 대용량 코드베이스에서는 로컬에서 에이전트를 돌리는 게 환경 설정 관리 측면에서 훨씬 안정적이었다고 경험함 관련 보안 정책 링크: https://www.cursor.com/security#privacy-mode-guarantee
"
"https://news.hada.io/topic?id=21276","DuckDB를 활용한 데이터 과학: 복잡한 파일 환경 길들이기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   DuckDB를 활용한 데이터 과학: 복잡한 파일 환경 길들이기

   PyCon US 2025 발표내용입니다.

  DuckDB를 활용한 데이터 과학: 복잡한 파일 환경 길들이기 - Alex Monahan

   Alex Monahan의 발표 영상에서는 오픈소스 분석 데이터베이스 라이브러리인 DuckDB가 데이터 과학자들이 다양하고 정돈되지 않은 데이터 파일을 관리하고 분석하는 데 어떻게 도움을 줄 수 있는지 설명합니다.

    주요 내용:

     * 파일 동물원 문제: 데이터 과학자들은 CSV, Parquet, 스프레드시트 등 다양한 형식의 수많은 파일들을 다루며, 이들은 여러 위치와 클라우드 플랫폼에 흩어져 있을 수 있습니다. 이러한 파일들은 크기가 크고, 개수가 많으며, 정돈되지 않았거나 손상되기 쉽습니다.
     * DuckDB 개요: DuckDB는 ""분석을 위한 SQLite""로 소개됩니다. 오픈소스(MIT 라이선스)이며, 분석 워크로드를 위해 설계된 인-프로세스(임베디드) 데이터베이스입니다. 의존성 없이 pip install duckdb로 쉽게 설치할 수 있으며, Python 스크립트나 노트북 내에서 직접 사용할 수 있습니다.
     * 다양한 파일 읽기: DuckDB는 S3와 같은 클라우드 저장소의 파일을 포함하여 광범위한 파일 형식을 읽을 수 있습니다. 특히 강력한 CSV 리더는 실제 지저분한 CSV 파일을 처리하는 데 뛰어나며, 문제가 있는 파일을 성공적으로 파싱하는 능력에서 다른 많은 도구를 능가합니다.
     * 친숙한 SQL 및 관계형 API: DuckDB는 사용자 친화적인 SQL 인터페이스(예: SELECT *는 선택 사항)와 Python 스타일의 관계형 API를 제공합니다. 파일을 마치 테이블인 것처럼 직접 쿼리할 수 있으며, 필요할 때만 데이터를 읽는 지연(lazy) 방식으로 작동합니다.
     * 상호 운용성: Pandas 및 Polars와 같은 인기 있는 데이터 과학 라이브러리와 원활하게 통합되며, 동일한 프로세스에서 실행되기 때문에 제로 카피(zero-copy) 데이터 교환이 가능합니다.
     * DuckDB 파일 형식: 기본 DuckDB 파일 형식은 여러 테이블, 뷰, SQL 함수 및 관계를 저장할 수 있는 단일 컬럼 기반 압축 파일입니다. 이 형식은 편집 가능하고 ACID 속성을 지원하며 속도와 효율성을 위해 설계되었습니다.
     * ACID 속성: DuckDB는 원자성(전부 아니면 전무 변경), 일관성(키를 사용하여 데이터 품질 문제 방지), 격리성(동시 쿼리가 서로 간섭하지 않음), 지속성(커밋된 데이터는 손상으로부터 안전함)을 통해 데이터 과학 워크플로에 데이터베이스의 견고성을 제공합니다.
     * 사용 사례: 데이터를 단일하고 효율적이며 쿼리 가능한 형식으로 통합하여 ""파일 동물원""을 길들이는 데 유용합니다. 또한 반복적인 데이터 과학 작업 중에 새로운 파일 동물원이 생성되는 것을 방지하는 데 도움이 됩니다.
     * 커뮤니티 및 확장성: DuckDB는 확장 가능하며, 커뮤니티 기여를 통해 통계 패키지 파일 및 Google Sheets와 같은 형식에 대한 지원이 추가되고 있습니다.

   DuckDB Labs 및 MotherDuck(DuckDB 기반 클라우드 데이터 웨어하우스)에서 근무하는 Alex Monahan은 DuckDB가 데이터베이스의 강력함과 데이터 과학에 필요한 유연성을 결합하여 데이터 과학자를 위한 데이터 처리를 단순화하는 것을 목표로 한다고 강조했습니다.
"
"https://news.hada.io/topic?id=21214","Show GN: Gemini SRT Translator - AI 자막 번역 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Show GN: Gemini SRT Translator - AI 자막 번역 도구

   Gemini SRT Translator - AI 자막 번역 도구

   Google Gemini AI를 활용한 SRT 자막 파일 번역 웹 서비스입니다.
   번역이 필요한 동영상 자막이 있으시다면 사용해보시면 좋을것 같습니다.
   일반 번역 api(구글,deepl등 ) 이런것보다 훨씬 퀄리티가 좋습니다.

   주요 특징
     * SRT 타임스탬프를 정확하게 유지하면서 번역
     * 100개 이상 언어 지원
     * 드래그 앤 드롭으로 간편한 파일 업로드
     * 실시간 진행률 표시
     * 번역 완료 후 즉시 다운로드

   사용법
    1. https://aistudio.google.com/app/apikey 에서 무료 API 키 발급
    2. 사이트에서 API 키 입력
    3. SRT 파일 드래그 앤 드롭
    4. 대상 언어 검색 및 선택 (예: ""한국어"", ""Korean"", ""日本語"" 등)
    5. 번역 시작 → 완료되면 다운로드

   기술 스택
     * Node.js + Express
     * Google Generative AI SDK
     * Gemini 2.0 Flash 모델 (기본값)

   오픈소스
     * GitHub: https://github.com/DHKIM0207/gemini-srt-translator-js
     * npm: npm install -g gemini-srt-translator

   원본 Python 프로젝트를 Node.js로 포팅하여 웹 UI를 추가했습니다. 영화, 유튜브, 강의 자막 번역이 필요한 분들께 유용할 것 같습니다.

   저는 개인적으로 영어 유튜브 영상의 자막을 위스퍼로 생성한 후에 해당 자막을
   이 프로그램을 통해 번역해서 vrew 같은 동영상 편집 프로그램에서 불러온후 내보내는 식으로 사용하였습니다.
   추후 계획은 유튜브 영상 다운부터 위 프로세스를 자동으로 해주는 프로그램을 제작 예정에 있습니다.

   API 키를 따로 수집하지 않으나 걱정이 되시는 분들은 깃허브 링크 들어가셔서 로컬에서 설치하시고 실행하시면 될 것 같습니다.
"
"https://news.hada.io/topic?id=21187","90년대 웹 디자인의 구루들: Zeldman, Siegel, Nielsen","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               90년대 웹 디자인의 구루들: Zeldman, Siegel, Nielsen

     * Flash와 CSS의 등장 이후, 1997년대에 웹 디자인 분야에 세 가지 핵심 철학이 생겨남
     * David Siegel은 “해킹”을 통한 시각적 미학, Jakob Nielsen은 단순성과 접근성, Jeffrey Zeldman은 디자인과 사용성의 균형을 강조했음
     * 당시 브라우저 호환성과 CSS 지원 부족 등 기술적 제약이 심각했으며, Flash는 새로운 멀티미디어 경험으로 주목받았음
     * 세 구루 모두 각기 다른 방식으로 웹 디자인 발전에 기여했고, 시간이 지나면서 표준과 미학의 조화를 강조한 Zeldman의 방향성 영향력이 커졌음
     * 현재 Zeldman은 Automattic의 리더로 활동 중이며 사이트의 새로운 리디자인을 준비하는 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   본 글은 1990년대 후반, 웹 디자인 분야를 대표하던 세 명의 구루인 Jeffrey Zeldman, David Siegel, Jakob Nielsen의 철학과 그 영향을 다룸. Flash와 CSS 등 신기술의 도입기이자 웹 표준이 정립되지 않았던 과도기 속에서, 이들이 어떤 디자인 방향을 제시했는지와 각 인물의 현재 활동을 조명함.

90년대 웹 디자인의 배경과 제약

     * 1997년을 전후로 Flash와 CSS라는 새로운 웹 기술이 대두됨
     * Zeldman은 원래 소설가, 저널리스트, 뮤지션, 광고인 등 이질적인 경력을 거쳐 1995년 웹에 입문함
     * 당시 웹은 “소비자 놀이터”로 간주됐지만, 기술적 한계와 브라우저 성능 제약이 심각했음
          + 이미지 사용을 최소화하고 용량을 작게 만드는 것이 권고됨
     * Zeldman은 텍스트 에디터와 Photoshop을 이용해 HTML과 그래픽을 제작하였으며, HTML의 기본기를 지키면서도 다른 디자이너의 소스코드를 직접 참고해 학습할 것을 권장했음

세 구루의 디자인 철학

  David Siegel: 미학과 “해킹”의 철학

     * Siegel은 1996년 ‘Creating Killer Web Sites’ 저서에서 CSS, Flash가 없던 시기의 미려한 레이아웃을 위해 HTML 해킹 기법 도입을 주장함
          + 보이지 않는 테이블, 단일 픽셀 GIF 등을 레이아웃 제어에 적극 활용함
     * 본인의 목표는 완벽한 타이포그래피와 시각적 전달력을 위해 수단과 방법을 가리지 않는 것이었음
     * 브라우저 별 호환성을 중시하기보단, Netscape Navigator 등 특정 브라우저에만 맞춘 최적화 전략을 추구함
     * 본인을 “HTML 테러리스트”로 칭하며 웹 표준을 무시하고서라도 미학적인 완성도를 최우선으로 삼음

  Jakob Nielsen: 사용성과 웹 표준 중시

     * Nielsen은 “멋부리기보다 실용성” 관점의 사용성 구루로 대중적 명성을 얻음
     * 모든 주요 브라우저에서 접근성 좋은 디자인 실천을 강조하며, 구조-표현 분리, 즉 세만틱 인코딩과 웹 표준을 지지함
          + HTML의 시멘틱 구조 활용, 초기 CSS 도입도 일찍부터 긍정적으로 평가함
     * CSS의 미래를 기대하면서도, 당시 지원이 부족하고 각 브라우저의 호환성 문제가 해결되지 않은 상황이라 지적함

  Jeffrey Zeldman: 미학과 웹 표준의 균형

     * Zeldman은 CSS 등 웹 표준을 적극 수용했지만, 필요한 경우 Flash나 Shockwave 같은 “비표준적” 도구도 활용하는 실용주의적 입장을 견지함
     * “웹 디자인은 미학과 웹 접근성 모두 중요”라는 관점을 줄곧 유지함
     * 2002년에도 이미지, 테이블 레이아웃, 스타일시트, 자바스크립트, 서버/클라이언트 기술이 웹 접근성과 모두 양립 가능함을 강조함

Flash 대 CSS

     * Flash는 쉽게 배울 수 있었고, CSS가 당시 제공하지 못하던 시각적 자유도와 미디어 기능을 지원함
     * 브라우저(특히 Netscape, IE)는 CSS 표준 지원이 미흡했지만, Flash는 전용 플러그인만 설치하면 어떤 브라우저든 동일 경험 제공이 가능했음
     * Siegel은 Flash를 빠르게 수용했으나, 완전한 표준 부재와 독점적 파일 구조 등 한계를 인정함
     * 반면 Nielsen은 Flash의 비표준성(프레젠테이션과 콘텐츠 결합)을 강하게 비판하며 ‘99% bad’, 즉 “사용성 재앙”이라고 평가함
     * 결국 CSS와 Flash 모두 웹 표현력 확장의 길을 열었지만, 오픈 웹 표준인 CSS가 장기적으로 주류를 차지함

세 인물의 이후 행보

     * Nielsen은 사용성 원칙 고수, Useit 사이트 미니멀리즘 유지, 2012년 사이트 통합 후에도 여전히 ICT 및 AI에 대한 연구를 지속함
     * Siegel은 웹 디자인 이론가에서 디지털 비즈니스, 시멘틱 웹, 블록체인 등 다양한 영역으로 전향하여 활동 영역 확대함
     * Zeldman은 Automattic(WordPress, Tumblr 등)에서 Executive Creative Director로 재직 중이며, 여전히 개인 블로그에서 디자인 철학 공유 및 사이트 리디자인 준비 중임

결론

     * 90년대 웹 디자인을 대표하는 세 명은 각기 다른 철학으로 웹 발전에 기여함
     * 기술, 표준, 미학적 실험이 혼재하던 과도기를 지나며, 결국 웹 표준과 디자인 감각의 융합이 현재 웹의 주된 흐름을 견인함
     * Zeldman의 실용적이면서도 균형 잡힌 접근법이 웹 디자이너들에게 지속적인 영향을 주고 있음

        Hacker News 의견

     * 이 글에서는 Nielsen을 '기술적으로'만 옳은 사람처럼 다루지만, 적어도 나에게 그는 '경험적으로 옳은지'에 집중하게 해준 인물임을 강조하고 싶음. 실제 유저 대상으로 테스트를 하여 정보 전달에 효과적인 방법을 찾는 연구를 했던 영향이 큼. 그래서 결과물의 외관이 꽤 촌스러웠지만, 본질적으로 옳은 방향이었음을 믿음
          + 나는 Nielsen 개인보다는 그가 속한 인간-컴퓨터 상호작용(HCI) 계보에 더 많은 존경심을 가짐. 당시에는 디자이너나 전통적인 HCI 전문가, 개발자 누구도 제대로 웹을 이해하지 못했던 시기임. Nielsen은 적어도 웹에 집중했지만, 새로운 매체에 대해 유저의 기존 기대치에 집착했던 점이 문제였음. ""하이퍼링크는 항상 파란색에 밑줄이 있어야 한다"" 같은 말을 했던 배경은, 그 시점에서는 유저들이 원하는 안정성 때문이 아니라, 실제로 웹이 진화할 초기 단계였다는 점을 간과한 결과였다고 생각함. 너무 엄격한 규칙을 너무 빨리 적용하려고 했던 모습
          + 그 당시에는 Nielsen이 실제로 유저의 니즈에 집중한 몇 안 되는 사람이었다고 느낌. 많은 사이트가 Flash로 이뤄진 인트로 화면을 먼저 띄우는 걸 좋은 아이디어라고 생각하고, 디자이너들은 일반적으로 읽기 쉬운 글씨 크기를 싫어했던 시기임
          + 나는 수년에 걸쳐 NNG Group의 여러 과정을 수강했음. Nielsen과 Tog에게서 배웠고, (Don Norman은 수업을 하지 않은 것으로 앎) 사용성에 대한 큰 존경심을 갖게 되는 계기였음. 디자이너들은 Nielsen을 정말 싫어했음
          + Discount Usability Engineering이 도움이 될 거라고 생각하지 않았는데, 실제로 해보니 결과에 깜짝 놀랐고, 그 후로 모든 디자인/리디자인에서 계속 활용하고 있음. Mr. Nielsen께 감사함을 전하고 싶음. 옛날 UseIt.com 링크(http://useit.com/"">archive)이 내 머릿속에 항상 남아있음
          + '싸움'을 어떻게 정의하느냐에 따라 다르게 평가할 수 있음. Nielsen의 시절에는 두 가지 일이 발생하고 있었음. 1) 아무도 보지 않을 자기만의 유쾌하고 기괴한 코너를 만드는 예술 행위, 2) 고객을 위해 본격적인 웹앱을 만드는 기업가들. Nielsen의 원칙은 두 번째에는 훌륭했지만, 첫 번째에는 재앙이었음. 현대 웹이 수익과 효율을 위해 모든 매력을 잃게 됨에 따라, 역사는 결과적으로 Nielsen을 우호적으로 기록하고 있음
     * 나는 Zeldman의 A List Apart를 정말 좋아했음. 당시 모두가 20대 중반일 때 그의 나이는 몰랐는데, 동년배라고 생각했었음 :D. Nielsen은 솔직히 크게 와닿지 않았음. 물론 수백만 명의 사용자가 더 쉽게 사용할 수 있게 기여했지만, 접근이 너무 경직되고 지루했음. 특히 홈페이지에는 특정 링크가 반드시 있어야 한다는 등 너무 규정적인 태도가 싫었음. Philip Greenspun이 그를 비판했던 기억도 있음. 모두가 명확한 답을 원했고, 그것에 돈을 지불할 준비가 되어있어서 그가 컨설팅에 성공할 수 있었던 것 같음. 빠르게 변하는 세상에서는 그 답도 금방 유통기한이 다함. 그래서 그런지 오래전에 자신의 사이트를 내린 게 아닌가 싶음. 그 지도가 시대에 맞지 않게 낡았다는 걸 인식했던 것 같음. 그래도 정말 멋진 시기였음
          + 그 시절은 사용자도 지금과 달랐음. 모든 내용을 한 페이지, 그것도 '스크롤 없이 접히는 부분(above the fold)'에 모아두던 이유가, 많은 사용자가 스크롤하는 법을 몰랐기 때문임. 이후에야 스크롤이 표준이 되었음. 기술적, 사회적으로 사이트 환경 자체가 전혀 달랐던 것임. 그런 세부사항이 오늘날까지 이어지지 않는 게 너무나 당연함
          + 2000년대 초반 CSS/디자인 블로그계는 정말 흥미로운 곳이었음. 고등학생 시절 Dave Shea, Andy Budd, Doug Bowman, Shaun Inman, Mike Davidson 등 다양한 사람을 팔로우하는 게 즐거웠음
          + 오래전에 사용성 테스트 서비스를 운영했는데, Nielsen에 대해서는 나도 마찬가지로 너무 경직되고 세세한 부분에 집착한다는 인상을 받았음. 내가 매일 돌리는 실제 테스트 현실감과는 거리가 멀었음
          + 내 기억에 Greenspun은 Siegel을 더 비꼬았음. Siegel은 그의 'Killer' 책에서 2-3단계의 진입 포탈을 강조했는데, Greenspun은 그게 터무니없다고 봤음. 나는 Nielsen의 접근은 상당히 긍정적으로 평가함. 웹에서 '사용성'을 중시하는 시기로 돌아가도 나쁠 건 없다고 생각함. 요즘은 Flash 시절 기능을 다시 재현하는데 엄청난 노력을 들이지만, 실제로 스크롤할 때 텍스트와 사진이 화면에서 튕기듯 움직이는 걸 정말 원하는지 의문임. 처음엔 신기하지만 그 이후엔 오히려 불편함. ""이 사이트 정보는 정말 최고인데, 좀 더 강아지처럼 화면에서 튀어다녔으면 좋겠다""라고 말하는 사람은 없을 것 같음
          + 오랜만에 A List Apart에 방문했더니 완전히 다른 모습임. 그리고 홈페이지에 1년 전 게시글이 'New'로 표시되어있음. 시대가 많이 변한 듯함
     * ""Jeffrey Zeldman — 1997년 초 42세""라는 사실, 이제서야 깨달음. 2000년대 초반엔 우리보다 몇 살만 더 많은 줄 알았음. 그들의 홈페이지의 ""View Source"" 보는 게 진짜 배움의 시간이었다고 생각함. 참고로 일부 국가에서는 ""View Source"" 행동이 불법일 수 있으니 각자 조심해야 함. 2000년대 초반 커리어를 시작해서 내 플래시 작업들도 많이 올렸고, Zeldman과 Siegel 등이 내 영웅이었음. Nielsen은 적이었음. 그런데 mid 2000년대쯤 병원이나 클리닉 작업하면서 접근성과 HIPAA 등 이슈를 다루게 되자, Nielsen도 결국 내게 영웅이 됨 :-)
          + 사실 그는 그때도 우리보다 몇 살만 더 많은 상태였던 것임
          + ""View Source""가 어디서 불법이냐고 궁금해짐
     * 예전에는 그냥 ""View Source""로 멋진 효과가 어떻게 구현됐는지 쉽게 확인할 수 있었던 시절이 그리움. 요즘은 돋보이는 무언가가 거의 없고, 있다 해도 inspector로 뒤져봐야 수십 겹으로 감춰진 난독화된 JS 안에 있거나 해서 분석이 거의 불가능함
          + 최근에 CSS로 만든 Minecraft 프로젝트를 파봤는지 궁금함. (CSS-Minecraft GitHub) 정말 오랜만에 최고의 ""View Source"" 경험이었음
          + JS기계가 아닌 창의적으로 만든 사이트의 HTML/CSS를 탐구하는 것은 여전히 즐거움. 최신 CSS는 정말 풍부함
     * 내게도 <i>Web Pages That Suck</i> 책이 의미 깊었음. 이 책은 <i>Creating Killer Web Sites</i>를 디스하기도 했음. 그 당시에는 큰 자존심 싸움이 있었음. Flanders는 요즘도 사이트를 운영할지도 모르겠음. 나도 메일링 리스트에 가입했지만, 10년 넘게 소식은 없었음
          + 나도 그 사이트를 정말 좋아했음. 요즘은 모든 게 너무 최적화되어 있어서, 오히려 예전의 조악한 웹페이지가 그리울 때가 있음. 사용성이 안 좋았더라도 각기 다른 개성은 분명했음. 마치 박물관에서 오래된 클래식 자동차를 보고 그 촘촘한 레버와 패달들이 뭘 하는지 상상하는 즐거움과 비슷함. 실제로 운전하긴 싫어도 보는 건 재밌음
          + 그 책 역시 내게 의미 깊었고, 지금 내 커리어의 출발점임. Flanders가 비판한 '미스터리 미트 네비게이션(무슨 메뉴인지 모호한 형태)'은 내게 엄청난 영향이 있었고, 지금도 사용성 문제를 떠올릴 때마다 생각남
     * 나는 이 시대가 아주 그리움. 정말 행복하고, 긍정적이며, 순수했던 시기였음. 모두가 진심으로 무언가를 만들고 서로 가르쳐줬음. 요즘은 다 인위적이고 허영심에 찬 느낌임. HTML+CSS 배우면서 이들의 책을 읽고 Designer's Talk 같은 포럼을 새로고침하며 있었던 시절을 영원히 잊지 못함
     * 요즘 웹사이트는 온갖 팝업으로 구독이나 피드백을 요구하고, 자바스크립트와 광고가 로딩될 때마다 내용이 위아래로 튀어서 읽기 힘든 사이트가 너무 많음. 웹이 최근 몇 년간 엄청나게 퇴보했다고 느껴짐. 이제는 그걸 비판적으로 논의하는 사람도 없는 것 같음
     * 내 책장에 아직도 ""Creating Killer Websites""가 꽂혀 있음. 빨리 산 책이었지만 고전이 될 줄은 몰랐음. 그 책에 나온 디자인과 같이 사이트를 실제로 보는 경험이 인상 깊었음. 그러나 내 생각에 실용적이지 않았음. 결국엔 인쇄미디어의 복사판이라고 느낌. 예전 Coldfusion이나 Dreamweaver 같은 툴은 거의 QuarkXpress(DTP 소프트웨어) 느낌이었음. 그 시절이 그립긴 한데, 나는 결국 사이트 디자인 쪽으로 가진 않았음
          + unpopular opinion일 수도 있지만, 나도 비슷하게 웃으며 넘길 수 있음. BBS 스크립트부터 ES6, SVG, WebGL까지 웹/프린트 디자인 경험 대부분을 해본 입장에서, 그 당시 주요 인물들이 웹에서는 뭘 하는지 몰랐다고 생각함. 그들은 인쇄 쪽에서는 최고였지만, 웹은 인쇄와 다름. 새 매체인 화면에서는 같은 페이지에 모든 정보를 넣을 필요가 없는 법임. 2010년쯤 되어야 합리적 웹디자인이 다시 시작된 느낌임. 2005년에도 사용성은 새로운 개념이었고, Apple의 K-12 UI도 큰 도움을 주지 못했음. 저 책들이 가르쳐주는 건, 인쇄 포스터 디자인을 웹에 그대로 입히는 법에 가깝고, 위젯이나 버튼 등 웹 특유의 인터랙션 실험은 부족했음. 지도제작도 마찬가지로 진화가 느림. 개인적으로는, 게임 및 게임 UI 디자이너들이 이른 시기부터 훨씬 더 혁신적이고 모던한 인터페이스를
            보여줬다고 생각함. 많은 경우 게임이나 데모씬이 오히려 시대를 앞서감
     * 2000년대에 Zeldman은 내 영웅이었음. 이 명단에 Eric Meyer(위키피디아)도 있어야 한다고 생각함
          + 나는 아직도 Eric의 CSS Reset 2.0을 계속 사용 중임. 이 정도로 간결하고 직관적인 건 아직 본 적이 없음
     * 나는 언젠가 Netscape Navigator 4에서 테이블을 7단계까지만 중첩할 수 있었다는 사실을 잊게 될 나이가 올지 궁금함
"
"https://news.hada.io/topic?id=21199","2^51 진법 트릭 (2017)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           2^51 진법 트릭 (2017)

     * 큰 정수 연산에서 발생하는 carry(자리 올림) 문제는 연산 병렬화를 어렵게 만드는 주요 원인임
     * x86 아키텍처에서는 carry 처리를 위한 adc 명령이 일반 add 명령보다 느리고, 연속 carry 처리는 병렬 실행을 제한함
     * Radix 2^51 표현법을 사용하면 carry 전파를 지연시켜 더 많은 덧셈을 빠르게 수행할 수 있음
     * 각 limb(부분 값)에 51 또는 52 비트만 할당하여, 나머지 상위 비트 공간을 carry 임시 저장소로 사용함
     * 이 테크닉은 추가적인 레지스터 사용과 변환 비용에도 불구하고, 실제로 2^64 진법보다 더 나은 성능을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

빠른 덧셈과 뺄셈: carry 문제

     * 큰 정수 덧셈에서는 인간이 손으로 자리수 단위로 carry를 처리하는 것처럼, 컴퓨터도 carry 때문에 덧셈 알고리듬을 병렬화하기 어려움
     * 기본적으로 우리는 오른쪽(하위 자리)부터 하나씩 더하면서, 각 자리에서 발생하는 carry를 왼쪽(상위 자리)으로 올려줌
     * 만약 왼쪽부터 덧셈을 시작하면 이전 carry가 다음 자리 연산에 영향을 주기 때문에, 연산 순서를 병렬화할 수 없음

컴퓨터에서의 carry 처리

     * 컴퓨터는 64비트 정수 단위로 덧셈을 처리함
     * 256비트 정수를 64비트 limb 4개로 쪼개서 덧셈을 병렬로 처리할 수 있을 것처럼 보이나, 실제로는 overflow(carry)를 처리해야 올바른 결과가 만들어짐
     * x86에는 carry 처리를 자동으로 해주는 adc(add with carry) 명령어가 있음

성능 저하의 원인

     * adc 명령은 carry flag라는 추가 입력값을 필요로 해, 단순한 add에 비해 성능이 떨어짐
     * Haswell 아키텍처 기준으로, add는 여러 포트에서 병렬 실행 가능한 반면 adc는 직렬(순차) 실행이 불가피함
     * 특히 SIMD 명령(vpaddq 등)을 사용할 때 carry 없는 병렬 덧셈이 훨씬 빠름

carry를 지연시키는 아이디어 (종이 위 예시)

     * carry를 줄이기 위해, 자릿수 범위를 확장해 (예: 0-9에서 0-9, A-Z, *까지 총 37자리) 임시로 carry 없이 여러 수를 더할 수 있음
     * 이렇게 하면 carry 전파 없이 여러 덧셈을 진행하고, 마지막에 한 번에 carry를 정리(normalization)할 수 있음
     * 이 개념은 carry 누적과 전파를 분리해 최종 단계에서만 carry 처리를 하도록 함
     * 실제 연산에서는 자릿수 기준 값 이상의 값이 나왔다면, 오른쪽부터 차례로 carry를 누적 반영함

carry 지연의 컴퓨터 적용 (Radix 2^51 트릭)

     * 컴퓨터에서 carry 전파를 줄이기 위해 radix 2^51 표현 사용
     * 256비트를 64비트 4개 limb가 아니라, 51~52비트씩 5개 limb로 분할
          + 각 limb의 상위 12~13비트는 carry 임시 저장소로 기능
     * 이 방식에서는 limb마다 2^64 값 범위를 유지한 채, 실제 연산 시 carry가 쉽게 발생하지 않아서, 여러 연산을 carry 없이 병렬적으로 수행 가능
     * 약 2^13개의 연속 연산 후 한 번에 normalization(정규화) 필요
     * Haswell CPU 기준, radix 2^51 적용 후 carry 없는 단순 덧셈만 여러 번 수행하여 일반 radix 2^64보다 성능이 크게 향상됨
     * normalization을 위한 carry propagation은 마지막에 한 번만 수행

코드 예시

     * 5개 레지스터에 값을 나눠 담아, carry 없는 덧셈 가능
     * normalization은 각 limb의 상위 비트를 추출해 옆 limb에 더하고, 자신의 carry 값을 0으로 만드는 방식 반복

뺄셈에의 확장

     * 뺄셈도 비슷한 방식으로 적용 가능
     * 이때 carry는 음수도 되므로, limb를 signed integer로 취급
     * limb의 가장 높은 비트가 부호 비트로 할당되어, 덧셈에 비해 한 번에 처리 가능한 연산 횟수가 소폭 줄어듦

결론

     * carry 저항(지연) 테크닉은 limb 개수 증가와 변환 작업 추가에도 불구하고 전체 연산 성능을 실제로 크게 향상시킴
     * Radix 2^51 트릭은 대규모 정수 연산, 암호학 등 높은 성능을 요구하는 분야에서 널리 활용됨
     * 이 테크닉은 실제 하드웨어/알고리듬 성능을 최적화하는 중요한 아이디어임

        Hacker News 의견

     * 2^51이라는 숫자를 보고 처음엔 double 타입에 정수 저장 관련 얘기인 줄 알았지만, 실제로 Integer를 double로 정확히 담을 수 있는 값은 2^53-1임을 깨달음
     * AVX512(그리고 AVX2도 어느 정도)에서 256비트 추가 연산을 상당히 효율적으로 구현할 수 있는 환경 제공, 더 많은 숫자를 레지스터에 담을 수 있다는 장점도 있음
       직접적인 예시는 아래 코드처럼 동작

__m256i s = _mm256_add_epi64(a, b);
const __m256i all_ones = _mm256_set1_epi64x(~0);
int g = _mm256_cmpgt_epu64_mask(a, s);
int p = _mm256_cmpeq_epu64_mask(s, all_ones);
int carries = ((g << 1) + p) ^ p;

__m256i ret = _mm256_mask_sub_epi64(s, carries, s, all_ones);

   처리량까지도 개선되는 모습을 보여서 실제 코드 예시는 godbolt.org에서 볼 수 있음
   이 논리를 512비트 덧셈까지 확장하는 것도 매우 단순함
     * 특히 특정 인텔 CPU 아키텍처에서는 AVX512 명령어 사용만으로도 전체 프로세서 클럭이 다운되어 일관되지 않거나 오히려 느린 전체 성능으로 이어질 수도 있다는 점 지적
       관련 참고는 stackoverflow 링크에서 확인할 수 있음
     * “13비트 말고 12비트를 쓰면 안되나?”라는 의문에 대해, 여기서는 가장 상위 비트(림)의 캐리 처리를 무시해서 오버플로우 시 wraparound 형식으로 동작하게끔 함
       그 결과 가장 상위 림에는 52비트를 할당함으로써, 다른 림보다 더 빨리 공간이 모자라지는 단점이 있지만, C 언어에서 무부호 정수 합산과 비슷하게 작동함
       그렇다면 최상위 림엔 64비트, 나머지 네 림엔 48비트씩 할당하는 방식은 어떤지 제안
       이러면 정규화 전 더 많은 연산 누적이 가능하고, 워드 정렬 등의 이점이 있음
       오버플로우 처리 특성도 동일함
          + 최상위 림만 64비트 할당할 경우, 두 숫자의 림을 더하면 너무 빨리 오버플로우가 발생
            예를 들어 둘 다 2^63값이면 바로 오버플로우
            wraparound 산술 연산에서야 상관없지만 일반적인 경우엔 무리임
          + 이런 구조라면 OP에서처럼 5개가 아닌 6개의 워드가 필요
            명령어도 더 많이 필요하게 됨
          + 목표가 256비트 수학을 5개의 64비트 레지스터로 해결하는 것에 있음
            즉, 각 워드마다 256/5 = 51.2비트 분배가 이상적
            이게 256비트 한정이라면 괜찮지만 범용 big-int 라이브러리엔 최적은 아님
            예전엔 캐리 하나에 정확히 1바이트를 쓰려고 했던 배경이 있었고, 바렐 쉬프터가 없었던 시절이면 정렬을 위해 64 중 56비트만 활용하는 걸 선호
            RISC-V에서는 하드웨어적으로 플래그가 없기 때문에 이런 논의가 더욱 중요
     * 현대 x86 CPU(예: Intel Broadwell, AMD Ryzen)에서는 Intel ADX 명령어를 활용해 2^51 radix 표현이 전통적으로 강세였던 상황(예: Curve25519)에서도 더 빠를 수 있음
     * 관련 논의 자료로
          + The radix 2^51 trick (2022년 11월)
          + The radix 2^51 trick (2017년)
     * 핵심 교훈은, 연산들이 상호 독립적일 경우 연산을 더 많이 병렬로 실행시키는 쪽이 오히려 더 빠를 수 있음
       반면, 의존성으로 인해 순차적으로 실행해야만 한다면 오히려 연산이 적어도 느릴 수 있음
       이 아이디어는 긴 정수 연산뿐 아니라 다양한 영역에 적용 가능
          + 64비트 청크로 분할해 캐리 여부에 따라 두 가지 경우를 미리 병렬 실행하고, 이후 실제 캐리 결과에 따라 옳은 연산을 선택하는 방식 제안
            이 방식은 덧셈 횟수가 두 배가 되지만 전파 속도가 log(bits) 수준으로(선형이 아닌) 빨라짐
          + 이 기법을 잘 이해 못했던 부분은, 이 방법이 본질적으로 ripple carry가 N값을 더할 때 N-1번 실행되는 걸 한 번만 실행하도록 한 점
            캐리 처리 자체는 복잡해지지만 덧셈은 병렬화 가능
            단, 입력 숫자를 5개 레지스터 단위로 나누는 작업 자체도 병렬화되어야 전체 효율이 의미 있음
          + 이 규칙은 노드 수 만 단위를 가지는 슈퍼컴퓨터나 클라우드 레벨까지 확장 가능
            많은 코어를 쓸 수 있으면 오버헤드는 무시할 만한 수준
          + 이 아이디어는 NVidia도 관심을 갖고 있고 여러 영역에서 좋은 결과를 내고 있음
     * 제목에 의견을 추가하면 안 된다는 HN 가이드라인은 있지만, 너무 과장된 클릭 유도형 제목들을 선호하지 않음
       “일부 x86 아키텍처에서 carry 의존성 없이 64비트 정수 병렬 덧셈이 가능한 radix 2^51 트릭” 정도로 제한하는 게 더 정확하다고 생각
     * 이 글을 몇 달만 일찍 읽었어도 도움이 됐을 거라는 아쉬움
       버퍼를 임의의 진수로 인코드/디코드하는 과정에서 캐리가 버퍼 전체로 파급되어 알고리즘이 크게 느려지는 경험을 했음
       최종적으로는 '여유 공간'을 남겨 chunk로 분할해 캐리를 처리했는데, 이 트릭과 유사점이 있는 듯
       실제론 일부 비트를 낭비하는 대신 연산량이나 네트워크 대역폭을 절약하는 방법 선택
       이와 같은 캐리 처리 역시 후처리로 묶을 수 있을지 궁금함
       사실상 모든 이점을 가져갈 수 있는 구조가 되는지 희망사항임
     * x86_64 환경만 쓰던 경험을 통해서, RISC-V에서 carry flag가 없어도 꼭 잘못된 접근은 아니라는 점을 명확히 보여줌
          + 이 방식 외에도 64비트 림을 유지하면서 모두 uint64_t 변수로 carry-safe 덧셈을 하는 방법 설명
            아래와 같은 흐름
s0 += a0;
s1 += a1;
s2 += a2;
s3 += a3;
c0 = s0 < a0; // RISC-V sltu
c1 = s1 < a1;
c2 = s2 < a2;
if (s1 == -1) goto propagate0;
check_s2:
if (s2 == -1) goto propagate1;
add_carries:
s1 += c0;
s2 += c1;
s3 += c2;
goto done;
propagate0: c1 = c0; goto check_s2;
propagate1: c2 = c1; goto add_carries;
done:

       관건은, 덧셈 결과(림)이 모두 1이 아닐 때, 해당 림의 캐리 아웃은 캐리 인에 의존하지 않고 단순히 원래 값을 더한 결과에만 의존함
       반면 값이 모두 1이면 캐리 아웃=캐리 인
       예측이 거의 불필요한 분기 구조라면 완벽히 병렬 실행 가능
       확률적으로 2^64분의 1에만 느려지지만, 4-와이드 머신 등에선 큰 이득 없음
       8-와이드 머신 또는 8-림 구조에서는 의미 있는 성능향상
       x86_64엔 안 맞지만 Apple M* 시리즈처럼 8-와이드 머신이면 활용 가능성
       Tenstorrent의 8-와이드 RISC-V Ascalon 프로세서, Ventana, Rivos, XiangShan 등에서 미래가 기대
       SIMD 구조, 빠른 1-lane shift(slideup) 명령 있는 구조에선 효과 극대화
          + 캐리-세이브 덧셈이 항상 add-with-carry보다 뛰어난 게 아님
            두 종류의 multi-word addition 알고리즘은 상호 대체 불가, 각자 장단점이 있음
            그러므로 ADC/SBB 명령은 ISA에 기본 포함, 레지스터 기반 플래그 저장도 가능
            RISC-V에서 더 심각한 단점은 integer overflow flag가 없다는 점
            오버플로 감지를 위한 SW 우회가 필요할 때 성능 하락이 carry 비트 우회보다 훨씬 큼
          + RISC-V에서 carry flag가 없는 건 C 언어가 바이너리 carry flag를 무시한 것에서 파생
            실제로는 carry flag 활용 빈도가 많이 낮음
          + 나만 “carry flag가 어차피 느리면 risc5 gmp 논란은 왜 있었나?”라는 생각한 게 아니었음
     * 'Radix trick'은 데이터 구조에도 적용 가능
       Okasaki의 'Purely Functional Data Structures' 책에도 흥미로운 예시 존재
"
"https://news.hada.io/topic?id=21237","내 개인 프로젝트로 $1M 벌기까지의 여정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        내 개인 프로젝트로 $1M 벌기까지의 여정

    Foreword

     * 조언이나 성공담을 전달하려는 목적이 아니며, 이 책은 솔직한 경험담 모음이다.
     * 독자는 읽고 끝내지 말고 직접 무언가를 해보길 바란다는 의도가 있다.
     * 본인의 변화, 모순, 개인적 이야기까지 솔직하게 다룬다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Introduction

     * 이 이야기는 성공 신화가 아니라, 실제로 겪은 좌절과 시행착오를 담고 있다.
     * 2년 동안 월 500달러 수익을 목표로 했으나 반복되는 실패와 성공을 겪었다.
     * 다양한 사람들과 비윤리적 고객까지 만나면서, 자신의 윤리와 가치관도 되짚어봤다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Year 0: The Spark

     * 여름, 아버지 집에서 무료함을 달래려 코딩을 배우기 시작한다.
     * 미국에서 돌아온 이웃을 만나 창업 생태계를 직접 듣고 크게 자극받음.
     * “행동하면서 배운다”는 조언이 인생 방향을 바꿨고, 스타트업 꿈을 갖게 된다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    The next Facebook

     * 대학 시절, '다음 페이스북'이라 믿으며 모바일 앱을 2년간 비밀리에 개발함.
     * 아이디어 공개를 두려워했고, 마케팅은 가장 나중에 시도함.
     * 실제론 수주 만에 서비스가 끝났으나, 스타트업 엑셀러레이터 참여 경험을 얻음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Going indie

     * 엑셀러레이터에서는 모두 투자만 이야기할 뿐 실질적 수익에는 관심이 없음을 느낌.
     * 투자 중심 게임이 아닌, 자생적 수익 모델을 추구하는 ‘인디해커’의 세계에 빠짐.
     * ‘인디’ 방식으로 여러 아이디어에 도전하기로 결심.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Year 1: Shotgun Strategy

     * 첫 해는 다양한 제품을 빠르게 만들어 내놓는 '샷건' 전략을 택함.
     * 직접 시장에 던져보고 반응이 있으면 집중, 아니면 바로 다음 아이디어로 넘어감.
     * 실제 실행, 반복, 포기를 거치며 시장과 본인의 강약점을 파악함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    MMAmatchups

     * MMA 팬을 위한 매치업 추천 웹사이트를 2주 만에 개발하고 홍보했으나 실패.
     * 소규모 방문자와 가입자가 있었지만, 실질적 성장 동력이 부족했다.
     * 금방 포기하고 다른 아이디어로 넘어감.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    February 2018 | MultiNewTab

     * 자신이 쓰고 싶던 크롬 확장 프로그램을 급하게 제작해 배포.
     * 제품은 예상외로 100명 이상이 설치해줬고, 작은 성공 경험을 맛봄.
     * 직접 만든 소프트웨어가 세상에서 돌아간다는 사실에 큰 동기 부여를 받음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    March 2018 | TalentShare

     * 자신이 경험한 ‘아깝게 탈락한 지원자 공유’라는 아이디어로 채용 플랫폼 개발.
     * 실제로는 누구도 지원자 리스트를 공유하려 하지 않아 가입자 제로로 종료.
     * 창업 초기 B2B 시장 진입의 벽을 체감함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Finding & quitting a remote job

     * 지인의 소개로 원격 워드프레스 개발자 일을 한 달간 했으나, 맞지 않아 바로 그만둠.
     * 불합리한 환경, 불안정한 급여, 비전 없는 업무에 실망함.
     * 자신의 목표는 직접 무언가를 만들어 수익을 내는 것임을 확실히 깨달음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    April 2018 | RemoteJuniorClub

     * 주니어 개발자용 원격 구직 커뮤니티를 개발, 소규모 사용자가 모였으나 곧 정체.
     * 온라인 커뮤니티 운영이 본인에게 맞지 않음을 깨닫고 빠르게 접음.
     * 다시 본질에 집중하기로 결심.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    My first successful launches

     * 상반기에 만든 4개 제품 중, 커뮤니티가 아닌 개인용 툴만이 반응이 좋았음.
     * 유저 수가 적어도 가치를 줄 수 있는 제품이 중요하다는 사실을 깨달음.
     * 경험을 바탕으로 ‘개인 가치형 제품’에 집중하기로 마음먹음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    May 2018 | GitGardener

     * 간단한 자동 커밋 프로그램을 만들어 공개했더니, 큰 반향을 일으킴.
     * 기대하지 않았던 제품이 오히려 가장 인기였고, 트위터 팔로워도 증가함.
     * ‘기대치 낮추고 공개적으로 빠르게 만들어보기’ 전략의 힘을 실감.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    May 2018 | MakerFeed

     * 또 다른 간단한 트위터 기반 사이트를 만들어, 제품 사상 첫 ‘Product Hunt 1위’ 달성.
     * 사용자 수나 수익화 가능성은 크지 않았으나, 커뮤니티에서 인지도를 높임.
     * 이때까지는 수익화에는 크게 신경 쓰지 않음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Making my first dollar

     * GitGardener에 프리미엄 기능(유료 비공개 저장소 지원)을 추가해 처음으로 $50 수익을 기록.
     * 기대 없이 시작한 유료화가 트위터에서 좋은 반응을 얻음.
     * 적은 수익이라도 자신의 프로젝트에서 돈이 벌린다는 것에 큰 성취감을 느낌.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Starting to believe

     * 5월 한 달간 프로젝트 수익이 $50에 도달하고, 원격 일자리 경험도 얻음.
     * ‘기대 낮추기, 빠른 실험, 공개적인 개발’ 방식이 자기에게 잘 맞는다는 확신이 생김.
     * 목표를 향한 희망이 생기고, 더 빠르게 여러 아이디어를 실험하기로 결심.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    June and July 2018 | Telemonetize

     * 텔레그램 채널 구독과 결제를 자동화하는 복잡한 서비스를 개발, 홍보에 성공하지만 사용자는 없음.
     * 기술적 완성도와 달리 시장 수요와 유저 확보에 실패.
     * 무리한 제품 완성이 비효율적임을 깨달음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Getting praise by my peers

     * 인디해커 커뮤니티에서 인정받으며 성장 중임을 체감.
     * 업계 멘토에게 칭찬을 받고, 네트워크가 확장됨.
     * 자기 방식에 대해 점차 확신을 갖기 시작.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Focusing on GitGardener and Telemonetize

     * 여러 제품을 만드는 대신, 기존 성공작에 집중해 성장시키기로 함.
     * 그러나 성장세가 정체되면서 성취감이 떨어짐.
     * “사업이 아닌, 실질적 가치를 주는 일”에 대한 고민이 시작됨.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    July–September 2018: Pimps, Scammers, Arms Dealers and Pornstars

     * Telemonetize의 고객 대부분이 윤리적이지 않은 사람들임을 발견하고 고민함.
     * 고객 관리와 지원에 많은 시간 소모, 실제 수익은 매우 적음.
     * 본인이 원하는 고객군과 완전히 달라 회의감이 커짐.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Abandoning GitGardener and Telemonetize

     * 10개월간의 결과가 목표치에 크게 못 미침.
     * GitGardener와 Telemonetize 모두 포기하고, ‘진짜 사업’에 도전하고 싶어짐.
     * ‘아이디어보다 실행 속도와 피드백 루프가 중요하다’는 점을 재확인.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    October 2018 | CryptoSubscriptions

     * 암호화폐 결제 시스템을 별도 제품으로 분리해 출시했으나 반응 없음.
     * 복잡한 기술만으로는 성공을 담보할 수 없음을 느낌.
     * 시장 수요와 본인 적성에 맞는 사업의 중요성을 절감.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    November–December 2018 | Epilepsy Blocker

     * 난이도 높은 크롬 확장 기능(뇌전증 발작 방지)을 개발하며, 기술적 성취감 획득.
     * 런칭 당일 유료 고객 1명 확보, 하지만 커뮤니티에서 차단 및 홍보 실패.
     * 의미있는 피드백을 얻었으나, 수익 모델 및 마케팅의 한계에 봉착함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Year 1 Recap

     * 첫해에 8개 제품을 내놓고, 2개에서 소소한 수익을 경험.
     * 상반기는 설렘, 하반기는 지치고 느린 진전의 반복.
     * 다양한 실패와 배움, 고객의 유형에 대한 이해를 얻음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Year 2: Sniper Strategy

     * 두 번째 해에는 제품 개수를 줄이고 기존 제품 집중 및 B2B로 방향을 틈.
     * 시장과 고객의 본질을 고민하며 ‘스나이퍼’ 전략을 시도.
     * 새로운 방향 전환에도 어려움이 많았음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    January–February 2019 | Epilepsy Blocker

     * Epilepsy Blocker 개발은 힘들었지만, 유일하게 삶을 바꾼 제품이 됨.
     * 실제 뇌전증 환자를 위한 시장의 빈틈을 발견하고 도전함.
     * 런칭에서 소수의 유료 고객을 확보하지만, 주요 고객은 개발자 및 엔지니어로 수익화에는 실패.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Setting north star metrics

     * 각 제품별 핵심 지표를 세워 성장 방향을 정함.
     * GitGardener는 방문자, Telemonetize는 유저 유지, EpilepsyBlocker는 인지도에 집중.
     * 명확한 목표 설정을 통해 효율적으로 자원을 배분하려 함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Hitting $200/month

     * GitGardener와 Telemonetize의 유료 이용자 확대로 총 $200 MRR 달성.
     * 외부 환경(예: GitHub 정책 변화)이 긍정적으로 작용함.
     * 여러 제품의 성장과 수익화를 동시에 경험.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    March–April 2019 | Ouch

     * EpilepsyBlocker의 타깃층이 소득이 낮은 환자라서 수익화에 한계가 있음을 느낌.
     * 자주 무료로 나눠주게 되면서 영업에 자괴감을 느낌.
     * B2C보다는 B2B로 전환할 필요성을 절실히 느낌.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    A lucky break, at last

     * MIT 계열 스타트업에서 연락이 와서 취업 제안을 받음.
     * 비록 인수 제안은 아니었지만, 연구·실력 인정과 ‘플랜 B’로 큰 전환점이 됨.
     * 가족과 본인의 자존감, 커리어에 큰 변화를 가져옴.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Going B2B With Epilepsy Blocker

     * Nonprofit 단체, 클리닉, TV 채널 등 다양한 B2B 시장을 두드려봄.
     * 현실의 벽(시장 진입, 무관심, 비효율 등)을 실감.
     * 법적 이슈, 네트워크, 영업 등의 경험을 쌓음.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Business meeting with huge TV Channel

     * TV 채널 CTO 미팅을 성사시켰으나, 실질적 시장 진입의 어려움을 다시 확인.
     * 현지 관행, 제도적 한계 등으로 의미 있는 거래는 이루어지지 않음.
     * 시장에 대한 냉철한 분석과 자기 전략의 한계를 체감.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Clean slate, impostor syndrome

     * 제품 성공의 뒷면에 ‘윤리 논란’ 등 부정적 피드백을 많이 받음.
     * 고민 끝에 Epilepsy Blocker 무료화, Telemonetize 종료, GitGardener도 휴식.
     * 경험을 토대로 B2B 중심의 새 전략을 결심.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Going round in circles

     * 전략 혼란(샷건 vs 스나이퍼 vs 중간)과 자기 확신 부족으로 어려움을 겪음.
     * 자꾸 실패하며 초조해지고, 자책과 불안이 심해짐.
     * 시간은 흐르고, 아직 목표를 달성하지 못해 조급함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    July–September 2019: New products rush

     * Orthios(챗봇 모니터링), LocalTweetTime, IndieChannels, EpilepsyBlocker for Designers, DuckDuckGoSometimes, Splash Search 등 다수의 크롬 확장·서비스를 속전속결로 런칭.
     * 대다수는 유저 반응/수익이 거의 없음, 일부 기능은 대형 서비스에 의해 모방됨.
     * 여전히 ‘무엇이 진짜 비즈니스인가’에 대한 고민만 커짐.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Enough! Give me some time to think!

     * 무의미하게 아이디어를 쏟아내거나, 한 아이디어에 집착해도 결과는 반복.
     * 악순환에서 벗어나지 못하고 있음.
     * 방향성 재정립이 필요함을 느낌.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Taking a step back and reflecting

     * 연말이 되어 전체 2년의 성과와 실패를 되돌아보고 휴식을 선택.
     * “남들이 성공한 방법”을 분석해보며, 시장/고객/피드백의 중요성을 다시 생각함.
     * 큰 그림(시장, 솔루션, 비전)을 그릴 역량 부족을 느끼고, 다음 계획을 고민함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    2019 Recap

     * $200까지 성장 후 다시 $100로 떨어지고, 핵심 제품은 무료화·포기.
     * 2년간 월급 한번 못 받아보고, B2B 시장 진출도 실패.
     * 성취와 실패, 배움을 모두 경험하며 자기방식의 한계를 인식함.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    Appendix

     * 자신의 경험담이 누군가에게 ‘하지 말아야 할 것’을 가르쳐줬으면 한다고 밝힘.
     * 앞으로의 계획은 불투명하나, 2020년에도 계속 도전할 것임을 다짐.
     * 2025년 시점에서 회고하며, 변화가 있더라도 진솔함은 지키겠다고 덧붙임.

   연재 속도를 볼때 실제 돈을 벌게된 이야기까지 가려면 한참 남은 것 같은데요. 저자가 실제로 돈을 번 서비스는 https://cyberleads.com/ 이라고 합니다. 신규투자를 유치한 스타트업 DB를 돈내고 구독하는 서비스입니다.
"
"https://news.hada.io/topic?id=21211","화이트칼라 직종 대량 해고는 AI 과장 분위기의 일부임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     화이트칼라 직종 대량 해고는 AI 과장 분위기의 일부임

     * 최근 화이트칼라 해고가 증가함
     * 많은 기업들이 AI 발전을 이유로 인력 감축을 설명함
     * 실제로 해고 원인은 비용 절감과 구조조정임
     * AI가 실제로 대체한 직무는 많지 않음
     * AI 과장이 기업 전략의 일부로 활용되는 현상임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 최근 미국 등지의 대기업들이 대규모 화이트칼라 직원 해고를 단행함
     * 언론과 기업들은 인공지능의 발전이 일자리를 대체한다고 강조함
     * 하지만 다수 해고의 실제 원인은 경영 효율화, 비용 절감, 경기 조정 등 전통적 구조조정임

AI와 해고의 실제 관계

     * 기업들이 해고 사유를 AI 혁신으로 포장하는 경향을 보임
     * 진짜로 AI가 대체한 직종이나 자동화로 사라진 일자리는 아직 극히 일부에 불과함
     * 기존의 경영 전환 이슈나 이익 개선 전략에 AI 담론이 덧씌워지고 있음

AI 과장과 기업 전략

     * AI에 대한 대중의 기대와 두려움을 이용해 기업들이 조직 개편을 정당화하는 모습임
     * 과장된 AI 담론이 투자자 설득이나 사회적 수용성을 높이는 데 활용됨
     * 실제로는 해고된 노동자 중 곧장 AI로 대체되는 경우가 드물음

결론

     * 현재 화이트칼라 직종의 대량 해고는 AI 영향을 과장하는 “AI 하이프 머신” 현상임
     * 현상 이해를 위해 해고 실상과 AI 도입의 실제 효과를 구분해야 함

        Hacker News 의견

     * 나는 ZIRP(제로 금리 정책) 시기에 두 개의 시가총액 100억 달러 이상의 회사에서 일한 경험이 있음. 대부분의 회의에 참석한 지식노동자의 절반 이상이 사실상 필요 없는 인원 구성. 일정이 너무 바빠서 회의만 참석하는 전담 인력을 채용하기도 했음. 회사 성장세를 이어가는 데 채용 증가는 주가에 전혀 문제로 작용하지 않았고, 오히려 VP가 인력수를 늘려 힘을 더 얻게 됨. 당시 시장은 효율성보다 성장만을 중시했으나 결국 시장은 항상 가치로 귀결. 시간이 지나면서 이런 덧붙인 인력들은 구조조정 대상이 됨. 두 회사 모두 이후 1만 명 이상의 직원을 해고했고, AI가 해고의 명분이 되었지만 실상 AI가 대체한다고 하는 지식노동자들 대부분은 원래 별 가치를 창출하지 않던 자리였음
          + 이 내용은 정말 공감함. 금리 인상기에는 “제로 금리 정책 프로덕트 매니저(Zero Interest Rate Product Managers)”란 말을 회의적으로 썼었음. 훌륭한 PM은 그만한 가치를 하지만 ZIRP 때 Jira 관리와 일정 조율만 하는 PM이 너무 많았음. 요즘 재취업에 어려움을 겪는 많은 IT 인력이 이런 주변적인 역할(애자일 코치, TPM 등)에 집중되어 있음. 물론 이런 포지션도 열심히 해 온 분들이라 동정심도 큼. 단순 과다채용만이 문제는 아니고, 오히려 미국 내 기술직 감소에 AI보다 오프쇼어링이 훨씬 큰 영향을 준다고 봄. 재택근무 확산 이후 영상회의 기술로 라틴아메리카, 유럽 등으로 오프쇼어링이 급증. 원격이 일반이니 위치는 점점 덜 중요해짐
          + AI 구조조정 대부분이 사실은 금리 인상기 구조조정의 포장이라 의심. 소프트웨어 업계는 한때 정말 미쳤었음. 무명대 출신 신입이 12~15만 달러를 바로 받던 시절은 오래갈 수 없는 흐름
          + 자극 없이 가치 더하지 않는 지식노동자가 많은 것엔 의심 없음. 하지만 주니어 기회가 점점 줄어든다는 점이 우려
          + 일정이 너무 바빠서 회의 전담 인원을 뒀다는 건 일종의 조직 패러디물이 현실화된 상황 같음
          + 나도 스타트업에서 근무한 경험 있는데, 회의 참여자가 절반 가까이 할일이 없어 참여하는 경우가 많았음. “관찰자”, “기록 담당” 역할 탓에 실제 영향은 거의 없음
     * 확실히 신입·인턴 역할이 AI로 대체되는 문제가 계층적으로 큼. 경험도 전문성도 없는 신입보다 AI가 더 빠르고 똑똑할 때가 많음. 물론 AI도 때로는 손이 많이 가지만, 결국 저렴하거나 더 똑똑함. 성장 잠재력 있는 신입들이 있지만, 그들을 성장시키는 데 시간과 자원이 너무 많이 들어감. 간단 업무마저 신입 대신 AI에 맡기는 나 자신을 발견. 신입이 맡으면 여러 단계의 피드백과 수정이 필요해 며칠 더 걸리지만, AI는 3시간 만에 완료 가능. 신입과 초급자 입장에서 전망이 정말 암울
          + 기업과 직원 양측의 충성심 결여가 이런 상황을 악화시킴. 본래 인턴십은 순수 ‘훈련’ 목적의 마이너스 생산성 포지션임. 회사에서 인턴을 키우는 건 결국 조직 전체, 산업 전체의 미래를 준비하는 투자였음. 하지만 HR과 임직원의 상호 신뢰 상실, 잦은 이직문화 탓에 이런 구조 자체가 무의미해짐. 일본계 기업에서 인턴을 제대로 키워 본 경험으로 이런 시스템의 가치는 크다고 확신
          + 신입과 초급자는 단기적으로 조직에 마이너스지만, 몇 달~몇 년 후에는 매우 생산적인 핵심 인재로 성장. 그리고 신입도 AI 쓸 수 있음. AI 도입이 진짜 생산성을 비약적으로 끌어올린다면 남는 여력만큼 더 많은 소프트웨어, 기능, 최적화 등으로 전환될 것. 과거 컴파일러 등장 때 주니어를 왜 더 뽑는가 하는 논리와 유사
          + 난 이 주장에 전혀 동의할 수 없음. 오늘날 신입이 손이 많이 간다면, 내일 신입은 AI 활용을 당연하게 여기며 더 큰 임팩트를 낼 수 있음. ‘엔트리 레벨이 망했다’는 시각은, 회사가 신입의 ‘단점’만을 바라보고 또 할 일이 유한하다는 전제 아래서만 맞음. 하지만 내가 경험한 모든 조직엔 항상 해야 할 일이 넘쳐났음. 그래서 오히려 신입이 AI로 6배의 업무량을 낸다면 훨씬 좋은 선택
          + ‘인턴’과 ‘엔트리 레벨’이라는 비교 대상은 복잡성의 대명사일 뿐, 실제로 초급 직군이 사라지는 건 아니며 역할만 바뀔 것
          + AI가 엔트리 레벨보다 업무 속도가 빠르고 효율적인 건 맞지만, 본질적으로 그건 ‘위임’이 아닌 자기 일 직접 처리임. 위임은 책임과 판단력을 옮기는 과정인데, AI에겐 트레이닝이나 피드백, 문맥 이해, 주체적 동기 모두가 부재. AI엔 정말 위임할 수 없으며, 결과에 책임지지 않음. 반면 인간 신입은 목표와 문맥을 흡수해 진짜 조직 일원으로 성장. 게다가 신입과 인턴도 AI 도구를 쓸 수 있음
     * AI가 정리할 직군은 하루종일 딴짓하고 대충 일하는 화이트칼라 자리라 생각. 2025년엔 LLM이 이런 일도 대신하게 될 것. 문제는 AI가 실제 일과 허울뿐인 일을 구분못 하는 경영진이 이런 환상에 빠져 조직 전체를 망칠 수 있다는 점. 마지막에 살아남는 건 항상 CEO
          + 이런 화이트칼라 자리가 존재하는 유일한 이유는 실적 평가가 근본적으로 어렵기 때문. AI가 이 문제를 해결하지 못하면, 직원 절반을 감축해도 하위 50%만 잘라내는 게 아니라 무작위로 해고할 뿐이라 최악에선 오히려 역효과
          + 재택 근무와 관련해서도, 진짜 업무 가치를 모르는 일부 매니저가 사무실 수다를 일하는 걸로 혼동한다는 점이 떠오름. 일에는 다양한 방식이 있음을 간과하는 문제
          + CEO도 결국 고객이 사라지면 좋은 일 없을 것. 결국 AI가 광고 클릭하고 제품을 사줄 수도 없음
          + AI는 쓸모없는 일 생성엔 탁월. 진짜 필요한 건 생산성 향상이 아니라 불필요한 일을 근본적으로 없애는 것
          + 나는 미국회사 경험은 없지만, 내가 다닌 곳에선 모두 인력 부족에 시달림. 비효율 인력에 대한 이야기는 나와는 먼 이야기. 다만 자본이 넘치는 미국 대기업이라면 그런 일이 있을 수도 있겠다고 추정
     * 컴퓨터의 등장 이후 사무직 자동화 규모는 대단했음. 1960년대 오피스 업무와 현재를 비교하면 업무 자체가 완전히 다름. 소프트웨어가 1000배 속도를 끌어올렸고, 이로 인해 오히려 더 많은 화이트칼라 직무가 탄생했음. 새로운 생산성 덕분에 더 많은 과업 창출
          + 이런 논리를 좋아하지 않는 이유는 첫째, 대규모 실업의 사회적 충격을 고려하지 않고, 둘째, 소멸한 일자리가 반드시 새 일자리로 대체된다는 자연의 법칙이 없기 때문. 대공황 시기 실업도 30%에 불과했고, 지금은 그 이상이 영구 실업자가 될 수 있음. 러다이트들이 기술 발전이 일자리를 없앤다고 했을 땐 그걸 막으려다 희생된 이도 많음. 또, 사회에는 굉장히 많은 문제와 수요가 존재하나, 경제 모델이 없다 보니 해고된 사람이 현실적으로 새로운 가치를 창출하기 어렵고, 앞으로 그 문제는 더 심각해질 것
          + 1960년대엔 ‘컴퓨터’라는 직업 자체가 있었지만, 오늘날엔 사라짐
     * AI 혁신의 실증 데이터가 부족하다며 AI 혁신이 단순한 과장이라 주장하는 분석가들을 보면, 코로나 초기의 사례 수가 적어 지수적 성장 가능성을 외면했던 회의론자들이 떠오름. 그거 말고도, 왜 이번 CNN 기사 같은 글이 분석으로 불리는지 궁금. 노동 경제학자 의견 몇 개, AI 과장 이론 등만 반복. 데이터·자원·VC 자금 흐름, FDA 신약 정책 등 보다 구체적인 분석은 전혀 없음
          + “코로나의 초기 과소평가” 식의 비유는, 이미 수백 번 검증된 현상과 완전히 새로운 AI의 성장이라는 전혀 다른 사례를 단순히 비선형 성장이라는 공통점만으로 연결한다는 점에서 논리적 비약이 있음
          + 주택 가격 버블 붕괴처럼 보장된 지수 성장 가정이 오히려 IT 업계에 더 큰 충격을 줄 가능성을 지적. 그리고 이번 버블이 터지면 IT 업계가 2000년대처럼 수년간 침체될 수 있음
          + 역사에서 대중의 예측 실패 사례는 많지만, 아날로지 논쟁 자체가 실질적 분석을 주는 게 아니라 자기 입장만 강화하는 수준이라 비판
          + AI를 코로나 바이러스와 비교해서 설득하는 건 적절한 예시로 느껴지지 않음
          + 사실 코로나 초기에 가파른 성장 그래프를 그린 쪽도 틀렸었음. 그 시기 거의 모든 논평가 예측은 맞지 않았다는 점을 지적
     * AI가 인간을 대체하며 발생할 소비 감소와 경기침체에 대해 기대하는 분위기가 이해되지 않음. 모두가 실업하면 제품을 누가 사나? 실업률 증가시 소비 감소, 경기 침체 불가피인데 경영진이 왜 그걸 반기는지 의문
          + 게임 이론, 내쉬 균형, 죄수의 딜레마, 그리고 칠면조의 귀납적 사고 등 경제적 유인을 설명하는 개념 적용. 각 조직은 자동화로 비용을 절감하는 게 직접적 이익이기에 이런 결정을 함. 문제는 이런 구조가 계속된다는 착각에 빠질 위험. 제본스 패러독스(효율성 증가가 오히려 총 수요를 늘릴 수 있음)같은 논리에 쉽게 현혹
          + 암세포도 결국은 전체 유기체를 해치지만 자기 이익만 추구. 단기 실적만 좋아진다고 자신만족하는 것도 동일 논리
          + 공유지의 비극. 모두가 근로자 해고로 비용을 절감하는 게 유리하니 이기적으로 행동하지만, 전체적으론 사회를 병들게 함
          + AI가 모두의 일자리를 없앤다는 가정하에, 경쟁사 역시 AI로 무장해 언제든 시장을 뺏길 가능성도 제기. 결국엔 AI 인프라 구축 능력이 진입장벽 역할
          + 일자리가 줄어도 정부 일자리, 블루칼라, 자영업자, 프리랜서, 암시장 등 다양한 고용 주체가 남을 수 있음. 일자리가 대폭 줄어도 삶의 질이 나빠지지 않을 시나리오 가능. 과거 기술 발전이 항상 일자리를 오히려 늘린 사례도 고려. 결국 누구나 자기가 보고 싶은 대로 보는 경향. 여러 시나리오 중 뭐가 맞을지는 상황에 따라 다름
     * 이런 형태의 AI 거품 논리는 확실히 의아함. 만약 AI 도입을 각종 화이트칼라에 확산시키는 게 목적이라면 왜 아웃풋 대상까지 겁먹게 하는 식의 마케팅을 하는지 의문. 혹시 단순히 C레벨만을 상대로 팔아먹으려는 걸까?
          + AI는 오로지 C레벨을 대상으로 영업하는 전략. 화이트칼라 일자리 보호엔 관심 없음
          + FOMO(남들보다 앞서고 싶단 두려움) 조장 전략이 매출 증대로 이어짐
     * 이번 기사는 “큰 주장엔 큰 증거가 필요”라는 점을 부각해 좋았음. ML기술을 정말 좋아하지만 인간 대체 수준까지는 신뢰하지 않음. 증강은 현실적인 비전, 완전 대체는 과장된 허상
     * 기계와 인간이 모두 못했던 영역에서 진정한 가치를 찾을 수 있음
     * AI-인간 대체 논의에는 뭔가 빠진 느낌. AI 혁신이 폭발할 때 금리 급등과 맞물렸고, 코딩 가능 AI가 등장하는 동시에 VC 자금도 마르고 스타트업 고용도 줄어듦. 만약 지금도 저금리 시대처럼 자금이 넘쳤다면 이런 논쟁 자체가 달라지지 않았을까 궁금. 업계 전문가 의견이 듣고 싶음
          + “AI혁명은 금리 폭등과 동시 진행”이라는 관점에서, C레벨이 AI 도입을 인건비 절감의 대안으로 바라보는 게 무료자금 축소의 영향일 수 있음. 그러나 금리 0%가 오히려 역사적 예외였고, 그 정책은 전 세계적 투자 왜곡을 불렀음. 금리 정상화가 이상하다고 간주하는 건 역설적. 역사적 금리 데이터 링크
          + 대화에서 이런 시각이 중심이 되지 않는 이유는 실제로 그 영향력이 크지 않음(1), 그리고 VC가 실제로 얼마나 많은 자금을 운영하는지에 대한 인식이 부족함(2). 1) 시드·A 단계 투자 자체가 비경기적이고, AI 주요 기술 진보는 소수팀·소액으로 진행됨. GPT2→3은 MS에서 투자함. 2) VC 자금은 2022-2023년에만 둔화됐고, 올해는 70%나 다시 증가. 빅테크·소프트뱅크 등에서 자금이 공급되고 있는 게 현실
"
"https://news.hada.io/topic?id=21299","사용자가 자신의 로컬 네트워크에 접근하는 사이트를 제한하는 제안","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  사용자가 자신의 로컬 네트워크에 접근하는 사이트를 제한하는 제안

     * 크롬 보안팀이 웹사이트의 로컬 네트워크 접근 문제를 해결하기 위해 새로운 ""로컬 네트워크 접근 권한"" 제도를 제안함
     * 현재는 공개 웹사이트가 사용자의 프린터 등 로컬 네트워크 장치에 무단 접근·공격 가능성이 있으나, 이 제안은 사용자 허락 없이 로컬 네트워크 요청을 차단하는 것을 목표로 함
     * 기존 Private Network Access(PNA)와 달리, preflight 대신 사용자 권한 동의 기반으로 작동해 사용자 통제권을 강화하며, 장치 변경 없이 사이트 업데이트만으로 대응 가능
     * 구체적으로, 공용 사이트가 로컬 IP, .local 도메인 등에 fetch 요청 시, 권한이 없으면 브라우저가 사용자에게 명시적 동의 요청을 띄움
     * 이 정책으로 보안·프라이버시 강화와 함께, IoT 기기 설정 등 정당한 사용 사례는 사용자 허락 시 정상 작동 보장
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

제안 개요 및 목적

     * Chrome Secure Web and Network 팀이 공용 웹사이트의 로컬 네트워크 무단 접근 문제 해결을 위해 '로컬 네트워크 접근' 권한 부여 방식 초기 설계안을 공개함
     * 기존에는 방문한 사이트가 사용자의 프린터, 공유기 등 로컬 네트워크 장치로 CSRF, 공격 등을 시도할 수 있었음
     * 앞으로는 공용 IP → 로컬 IP 등 주소 공간 간 경계를 넘는 요청을 브라우저가 차단하고, 사이트별로 명시적 사용자 허락을 받아야만 허용하는 구조를 제안

배경 및 차별점

     * 기존 Private Network Access(PNA) 는 preflight(사전 요청/응답) 기반으로, 장치에도 변경이 필요해 도입이 어려웠음
     * 이번 제안은 사용자 권한 동의만으로 처리할 수 있고, 사이트만 소폭 수정하면 되므로, 실제 적용과 확산이 용이함

목표와 비목표

     * 목표
          + 드라이브-바이 웹 기반의 취약 장치·서버 악용 차단
          + 사용자가 원하고 허용한 경우에만, 공용 웹사이트에서 로컬 장치와 통신 허용
     * 비목표
          + 기존 로컬 장치 설정/제어 등의 합리적 사용 흐름 전체 차단은 지양
          + 로컬 네트워크 HTTPS 문제, 복잡한 인증서 발급 등은 이번 제안 범위에서 제외

사용 사례

     * 1번: 일반 사용자가 원치 않는 경우, example.com이 192.168.0.1 등으로 요청 시 브라우저가 허락 여부를 묻고, 거부 시 요청 차단
     * 2번: IoT, 공유기 등 장치 제조사의 공식 웹 설정 페이지는 처음 접근 시 사용자에게 허락을 받아 통신 허용

구체 설계

     * 주소 공간 분리:
          + loopback(자기 자신 전용), local(로컬 네트워크 내부), public(모두 접근 가능) 세 계층으로 IP 네트워크 계층을 분류함
          + .local 도메인, RFC1918/4193의 프라이빗 IP, RFC6762의 링크로컬 네임 등 다양한 로컬 네트워크 식별 기준을 포함함
     * 로컬 네트워크 요청: public→local, public→loopback, local→loopback 등 상위 공개주소에서 내부 네트워크로 접근 시 권한 요구
          + 공개 네트워크에서 로컬/루프백 네트워크로의 요청부터, 로컬에서 루프백으로의 요청까지를 로컬 네트워크 요청으로 간주함
          + 예외: 로컬→로컬, 루프백→어떤 주소 등은 제한 대상이 아님
     * 권한 프롬프트:
          + 사이트가 로컬 네트워크로 요청 시, 권한이 없다면 브라우저가 사용자에게 허용 여부를 묻는 프롬프트를 띄움
          + 거부 시 요청 차단, 수락 시 요청 진행
     * fetch API 통합: fetch 호출 시 targetAddressSpace: ""local"" 등 옵션 명시, 명확히 구분 가능
          + Fetch 스펙은 DNS 해석 없이 단순 연결만 정의하므로, 새 연결에서 로컬 네트워크 요청 여부 판단
          + 보안 컨텍스트에서만 로컬 네트워크 요청 허용, 권한 미획득 시 프롬프트, 권한 부여 시 요청 허용
          + fetch()의 options에 targetAddressSpace 파라미터 추가로 개발자가 명시적으로 목적지 주소 공간 지정 가능
               o ex) fetch(""http://router.com/ping"";, { targetAddressSpace: ""local"" })
          + 실제 연결된 주소가 옵션의 공간과 다르면, 요청 실패로 처리해 혼합 콘텐츠 우회 방지
     * HTML, WebRTC, ServiceWorker 등도 동일 정책 적용
          + HTML 문서/워커에 주소 공간 값을 추가해 출처 기반 공간을 추적함
          + WebRTC 내 ICE Agent의 후보 추가 시, 로컬/루프백 주소는 권한 프롬프트를 사용함
          + 권한은 Permissions API와 연계하여, 사이트가 현재 권한 상태를 쿼리 가능
          + 기본적으로 상위 문서에서만 로컬 네트워크 접근 가능, 필요시 Permissions Policy의 위임 정책으로 하위 프레임에 권한 위임 가능
     * 혼합 컨텐츠(HTTP/HTTPS) 문제:
          + 비보안 컨텍스트에서 로컬 네트워크 접근 시도, HTTP 기반 하위 리소스 로딩, 혼합 콘텐츠 차단 적용 시나리오 등
          + 프라이빗 IP 리터럴, .local 도메인, targetAddressSpace 지정 요청 등은 혼합 콘텐츠 업그레이드 및 차단 단계를 생략하고, 후속 연결 시 권한 미보유 오리진이면 차단
     * 동작방식 예시
          + 예상치 못한 로컬 네트워크 접근 시, 사용자가 권한을 거부해 무단 요청을 차단할 수 있음
          + 제조사가 운영하는 장치 제어 페이지의 경우, 적절한 프로퍼티(예: targetAddressSpace=""local"")로 호출 시, 사용자 동의가 있을 경우 기존대로 동작 가능함

대안 및 논의

     * PNA 방식:
          + 기존 PNA는 CORS 프리플라이트를 요구했으나 실제 적용 및 배포 어려움이 컸음
          + 일부 구간에서는 권한 프롬프트와 혼합 콘텐츠 예외 허용 방안 제안
          + DNS 문제, 장치별 사양 미지원 등으로 현실적 한계가 존재함
     * 모든 로컬 네트워크 요청 차단: 단순하지만, 사용례 파괴와 우회 비용 증가 우려로 현실적이지 않음
     * 현 상태 유지: OS에서 앱별로 로컬 네트워크 권한을 관리하기 시작하면서, 브라우저 차원의 관리 필요성 강조됨
     * 혼합 콘텐츠 대안:
          + ""보안 로컬 네트워크 하위 리소스만 허용"" 등 접속 방법의 보안성 평가와 구현 부담이 논의됨
          + 응답 헤더/메타태그로 주소 공간 선언하는 법, HTML 요소 속성 추가 등도 대안으로 논의됨

추가 논점

     * HTML subresource(iframe, img 등)도 주소 공간 속성 추가 가능성 논의
     * 권한 부여 시 과도한 권한 전달(transitivity) 이슈 등 연구 결과 반영
     * 메인 프레임 이동 시 로컬 네트워크 접근 제한하거나 경고 인터스티셜 표시
     * 로컬/루프백 주소 대상 모든 크로스 오리진 요청을 폭넓게 로컬 네트워크 요청으로 간주하는 안도 고려됨
     * 네트워크별로 세분화된 권한 부여 방안 연구, 다른 네트워크 이동(다른 장소 접속) 시 재동의 필요성

보안·프라이버시 고려사항

     * 권한을 받은 사이트는 사용자의 네트워크 전체 장치에 탐색·접속 권한 확대 우려
     * 사용자는 프롬프트 수락 시 의도를 명확히 인지해야 하며, preflight 기반보다 직접적 통제 가능
     * 사전 권한 없이는 어떤 로컬 네트워크 요청도 불가, 프라이버시 보호 측면 강화

        Hacker News 의견

     * 나는 처음 봤을 때 이 기능이 마음에 드는 감정, 웹사이트가 임의로 로컬 IP(혹은 어떤 IP든)에 HTTP 요청을 날릴 수 있다는 개념 자체가 정말 말도 안 되는 위험이라고 생각, 만약 이로 인해 일부 기업용 앱이나 통합 기능이 깨진다 해도 신경 쓰지 않는 입장, 기업은 관리도구로 이 기능을 다시 활성화하면 되고, 일반 유저는 직접 설정하면 된다는 의견, ""이 웹사이트가 로컬 장치를 제어하려 합니다 - 허용/거부""라는 팝업만 띄우면 충분하다고 주장
          + 오해가 있는 시각 제시, 로컬 네트워크 장치는 CORS 덕분에 임의의 웹사이트로부터 보호되고 있다는 설명, 완벽하진 않지만 꽤 효과적인 방식이라는 견해, 문제는 CORS가 오직 타겟 서버의 동의에만 의존한다는 점 강조, 서버가 특정 헤더로 해당 웹사이트의 접근을 허락해야 하기 때문, 이번 제안은 서버와 웹사이트가 모두 통신을 원하더라도 유저 승인을 명확히 받아야만 하도록 더 강화하는 취지, 과거에는 서버-웹사이트 합의만으로 충분하다고 봤지만, 최근 Facebook과 같은 사례에서 웹사이트가 휴대폰 내 앱에 몰래 접근하는 일이 이 원칙을 깨버렸다는 의견, 즉, 웹사이트와 로컬 네트워크 서버가 유저의 이익과 반하게 작동할 수 있게 된 현실 지적
          + ""일반 유저는 직접 팝업에서 허용/거부 설정하면 된다""라는 의견에 대해, MacOS가 현재 앱별로 이런 권한 요청을 하고 있는데 대부분의 유저가 별 생각 없이 '허용'을 누른다는 점을 언급, 사이트별로 한다고 해서 경계심이 아주 크게 늘지는 않을 거라는 추측
          + 웹사이트가 왜 로컬 네트워크에 접근해야 하는지 이해가 안 되는 내용, 이건 완전히 새로운 보안 위협 모델을 만들 뿐이라는 주장, 이미 더 나은 해결책이 없는 케이스가 있는지도 의문 제시
     * Apple, Microsoft, Google 등도 USB와 Bluetooth에 대해서도 이와 비슷한 접근을 해주길 바라는 마음, 최근 설치하는 거의 모든 앱이 Bluetooth 접근 권한을 요구하는데 매우 불편하다고 느낌, 앱이 접근 가능한 블루투스 디바이스 ID를 manifest에 명시하고 OS에서 해당 장치로만 접근을 제한해주기를 희망, 예를 들어 Bose 앱은 오직 Bose 기기만 볼 수 있어야 한다는 의견, 앱이 어떤 권한을 요청하길래 거부한 경험, 카메라나 GPS 권한과 비슷하게 기기 ID 등록과 사용자 프롬프트가 있으면 좋겠다는 생각, Bose의 경우 prefix를 bose.xxx로 등록하고 manifest에서 ""bose.*"" 접근만 요청, OS에서 해당 rule만 허용하는 식, USB, 로컬 네트워크 기기에도 비슷한 ID 체계 적용을 제안, OS가 앱이 네트워크, USB, 블루투스를 탐색하지 못하게 하는 방향 희망
          + 언젠가는 Apple이라도 언젠가 앱에게 '가짜 권한 허용' 옵션을 제공해주길 바라는 기대, 예를 들어 앱이 내 연락처 목록이 꼭 필요하다고 할 때, 진짜와 구분 안 되는 랜덤 목록을 보여주는 방법, GPS 때도 비슷하게 적용, WhatsApp이 연락처를 공유하지 않으면 연락처 이름을 지정할 수 없다는 이야기도 들었다는 경험
          + Github 제3자 앱 통합처럼, ""ABC가 내 레포를 조회하고 싶어 함. 어떤 레포를 공유할지?""와 같은 세분화된 선택권 모델 선호
     * Internet Explorer가 예전에는 존(zoning) 시스템으로 이런 문제를 해결했다는 입장, 자세한 정보는 MS 문서 참고 의견
          + 아이러니하게도 Chrome도 Windows에서 부분적으로 IE의 존 보안 체계를 사용했지만, 이에 대한 공식 문서는 거의 없었다는 내용
          + 이런 현대적 대안이 존재하지 않는 게 어이없다는 평가, 로컬 네트워크 접근 또한 카메라, 마이크처럼 특별한 권한으로만 허용해야 한다고 생각
     * 웹 브라우저가 기본적으로 이런 행위를 허용했다는 게 믿기 힘든 현실, 퍼블릭 웹사이트가 내 전체 파일 시스템에 몰래 접근할 수 있다고 생각하면 끔찍한 보안 취약점인데, 로컬 네트워크 서비스에 대해선 XHR로 그냥 사용 가능하고, 보안을 서버 설정에만 맡기는 실태, 개발자라면 본인 개발 PC에서 테스트 용으로 사내 웹앱을 돌릴 때(아주 느슨하거나 없는 보안설정), facebook.com, google.com 등에서 바로 접근 가능, 집에서도 라우터 방화벽을 믿고 인증 없는 서비스를 돌리는 사람들이 많은데 과연 전부 CORS 설정 제대로 해놨을지 확신이 없다는 문제의식
          + 과연 모든 사람들이 CORS 설정을 제대로 해두었을지에 대한 회의감, 실제론 거의 0%에 가깝게 미설정 되어 있을 거라는 주장
     * 이번 제안이 Meta가 자사 SDK를 이용해 네이티브 앱과 웹사이트 사이에 localhost 기반 트릭 방식으로 식별 코드 공유를 하는 걸 막는 데 도움 줄 수 있다는 기대, 특히 Android에서 해당 사례 더 자세히 보기
     * 웹사이트가 로컬 네트워크에 접근할 권한 자체를 갖는 것은 매우 거칠고 불필요하게 광범위한 허용이라는 지적, 실제 권한이 필요한 사이트 대부분은 오직 하나의 로컬 서버에만 접근 필요, 모든 로컬로 허용하는 건 최소 권한 원칙 위반, 대부분 유저가 로컬호스트나 네트워크에 뭐가 뜨는지 알지 못 해 위험성도 제대로 인지 못 한다는 문제 제기
          + 대부분의 사람들은 localhost나 네트워크에 뭔가 뜨는지 알지 못 하므로, 브라우저에서 예를 들어 http://localhost:3146이나 http://localhost:8089 접속 허용 메시지를 보여줘도 무슨 뜻인지 추측조차 못함, 기술 전문 용어가 아닌, ""이 사이트가 로컬 네트워크 리소스에 접근하려고 합니다""처럼 직관적인 메시지가 사용자에게 더 낫다는 주관
          + 제대로 구현하려면 사실상 브라우저 내 파이어월 수준 접근 필요, 어떤 CIDR, 포트, 등 세밀하게 제어 가능한 API가 있으면 좋겠다는 의견, 브라우저 확장에서도 이런 firewall API를 쓰거나, 기본 UI로 특정 머신(예: 라우터), LAN, VPN, 윈도우의 private network 등 범위를 명확히 구분해 각각 접근 권한 요청이 가능하도록 만드는 걸 희망
     * 예전에 NPAPI 플러그인이 사라진 후로, 여러 퍼블릭 웹사이트가 로컬 소프트웨어와 연동하려면 로컬호스트에 HTTP 서버를 띄울 수밖에 없는 구조가 됐다는 점, 만약 이 사용성까지 복잡하게 만든다면 엄청나게 불편해질 거라는 우려, 브라우저 개발자들이 NPAPI 이후에 대체 기술을 마련했었어야 했는데 지금은 이미 늦었다는 주장
          + 대부분의 소프트웨어는 OS 단에서 프로토콜 핸들러를 등록해서, 예를 들어 웹사이트가 zoommtg:// 같은 링크를 넘기면 브라우저가 Zoom 등으로 연동하는 방식을 선호, Jupyter Notebook 같이 교차 출처 요청 없이 로컬에서 쓰는 건 영향 없음, OAuth2 로그인 후 localhost URL로 보내는 것도 단순 리디렉트라 문제 없을 것으로 판단
          + 이런 방식(로컬 앱과의 HTTP 서버 통한 통신)이 완전히 사라진다면 오히려 더 좋겠다는 입장, 보안 취약점 원천 제공 역할을 해왔다는 주장
          + Mozilla Native messaging 같은 기술이 이미 존재, 확장 프로그램 설치는 필요하지만 NPAPI와 비교했을 때 공정한 방식이라는 관점
          + 만약 로컬 소프트웨어가 'pull' 방식(앱이 주기적으로 외부에 요청)으로 동작한다면 굳이 서버 띄울 필요 없어지고, 부가적으로 웹사이트가 남의 로컬 네트워크 이곳저곳을 함부로 스캔하는 일도 없어지니 좋을 거라는 의견
     * JavaScript에서 cors 옵션 없이 fetch 또는 POST 요청을 날리면 CORS가 응답을 읽을 수 없게 할 뿐, 실제로 요청 전송 자체는 브라우저가 처리함, 만약 로컬 앱이 서버에서 프록시로 CORS 헤더를 추가하게 만들면 어떤 사이트든 JS fetch/XMLHttpRequest로 접근 가능, 확장 프로그램은 헤더를 수정해 CORS를 우회할 수도 있음, 헤더 만지기로 이런 우회가 너무 쉽고, CSP(Content Security Policy)는 실제로 우회가 매우 어렵거나 거의 불가능, 페이스북 앱은 지금도 자체 cors 프록시 서버 돌리면서 이런 구조 운영, WebSocket도 있으니 Chrome에 localhost 접근 차단 플래그가 있어도 무용지물, localhost 완전 차단은 오히려 해가 크다는 주장, 많은 사용자가 self-hosted 북마크, 노트, 패스워드 관리앱 등 로컬 서버 활용하고 있기 때문에, 이런 케이스를 막는 건 불합리
     * IPv6 환경에서 문제 발생 우려, 실제로 IPv6 주소가 부분적으로 로컬인지 구분할 방법이 있는지 궁금, 없다면 IPv6 only 네트워크에선 이번 제안이 문제될 것으로 판단, IoT 앱에서 이런 문제 겪은 경험, IPv6 주소가 로컬인지 식별 어려워 일단 IPv6는 모두 IPv4 로컬로 리디렉션하는 방식 선택, IPv6 link local도 실상 일반 앱에서 쓸 수 없는 주소라 별 의미 없었음, .local 도메인을 로컬 서버로 인정해주는 건 다루기 나름인데 OS마다 해석이 달라 구현이 일관되지 않다는 문제, 예: Raspberry Pi OS에선 ""some_address""는 mDNS로 풀리지만 ""someaddress.local""은 안 되고, Ubuntu 24.04에선 ""someaddress.local""만 되고 ""someaddress""는 안 됨, ""someaddress.local."" 도 작동하지 않음, 마지막으로, 로컬 네트워크 주소에 대해 프라이빗 인증서를 쓸 수 없다는 점도 답답함, ""로컬 주소에 https 제한"" 문제도 반드시
       해결되어야 한다고 강조
          + IPv6도 여전히 '라우팅 가능' 개념이 남아있어서 논리적으로 라우팅 테이블 단위로 site-local 여부를 정의할 수 있다는 의견, 옛날 IPv4는 2번째 옥텟에 site, 3번째 옥텟에 VLAN 구조였고, IPv6는 옵션 더 많음, 모든 IPv6 기기는 link local 주소를 가지며(로컬 VLAN), .local은 Apple, DNS 등 네임-주소 매핑 관련 용어로 IP 주소와 직접 무관함, 로컬 네트워크에서 https 인증서는 Lets Encrypt의 DNS-01 인증, CNAME 등을 활용해 가능, 꽤 번거롭지만 무료 방법이 존재하고 acme.sh 같은 도구도 추천, IPv4, IPv6, DNS, mDNS, Bonjour 등 컨셉을 더 명확히 정리 필요, 패킷 캡처조차 유료였던 시절을 떠올리며 지금은 훨씬 낫다는 회상
          + IPv6 주소가 ""로컬""인지 엔드포인트에서 구분하는 방법이 없다는 주장을 명확히 함, 이는 IPv6의 원리가 글로벌 라우팅이기 때문, 기사에서 Google 역시 ""local""의 의미를 논의하다가 중간에 'private' 정의로 바꾸는 등 혼선 있었다는 지적, HTTP 확장으로 비표준적인 CIDR 기반 보안 경계선을 만드는 건 황당한 접근, 앱은 공개 서비스라고 가정하고 보안 모델을 설계하는 게 맞다는 입장, .local은 mDNS 규격에 포함되어 있지만 실제로는 거의 무시됨
     * 이런 방식이 얼른 실현되기를 진심으로 바라는 마음, 특히 HTTPS 도메인에서 HTTP 로컬사이트에 접근할 수 있는 기능이 있길 기대, 스마트홈, 미디어/엔터테인먼트 등 멋진 활용사례가 많다는 언급
"
"https://news.hada.io/topic?id=21238","리눅스 앱용 커널 수준 Tor 격리 도구 Oniux 소개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    리눅스 앱용 커널 수준 Tor 격리 도구 Oniux 소개

     * Oniux는 리눅스 앱의 모든 트래픽을 Tor 네트워크로 강제 우회하여 데이터 유출 위험을 최소화하는 커널 레벨 격리 도구임
     * Linux 네임스페이스를 활용해, 각 앱을 독립된 네트워크 환경에 격리시키고 Tor를 통한 안전한 통신 구현함
     * 기존 torsocks와는 달리, libc를 사용하지 않거나 정적 바이너리에도 동작해, 악성 앱의 직접적인 데이터 유출 경로 차단함
     * Oniux는 새로운 Arti, onionmasq 기반으로 Rust 언어로 작성되어, 보안성과 확장성 모두 강화함
     * 현재 Oniux는 실험적 도구로, 안정성 면에서 검증된 torsocks와는 다르나, 차세대 Tor 트래픽 격리 솔루션으로 주목받음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Oniux 소개

   Oniux는 리눅스에서 Tor 네트워크 격리를 통해 개인정보 보안 수준을 비약적으로 높이는 커맨드라인 유틸리티임. 개발자, 활동가, 연구자가 틀린 프록시 설정이나 약간의 부주의로 인한 데이터 유출 가능성을 완전히 봉쇄할 수 있도록 설계됨. Oniux는 Arti와 onionmasq 위에서 동작하며, 어떤 리눅스 앱도 별도의 네트워크 네임스페이스에 격리해 Tor 네트워크로만 트래픽을 강제 우회함.

리눅스 네임스페이스란 무엇인가

     * 네임스페이스는 리눅스 커널의 주요 격리 기능임
     * 어플리케이션의 일부 리소스를 시스템 전체와 논리적으로 분리해줌
     * 네트워크, 마운트, 프로세스 등 다양한 자원을 이 기법으로 격리 가능함
     * 각각의 네임스페이스는 운영체제 자원을 분리해, 컨테이너 환경이나 보안 목적에 사용됨
     * Docker 등 대표적인 컨테이너 솔루션이 네임스페이스를 기본 원리로 활용함

Tor와 네임스페이스 결합의 의미

     * 네임스페이스는 임의 애플리케이션의 Tor 네트워크 접근을 완전 격리로 보호함
     * 각 앱을 네트워크 네임스페이스에 독립적으로 배치하고, onion0이라는 커스텀 인터페이스만을 노출함
     * 앱이 OS 전체 네트워크 인터페이스(예: eth0)에 접근 불가하여 안전성 극대화 달성 가능함
     * SOCKS 기반 프록시 방식과 달리, 어떠한 실수나 결함으로도 직접 트래픽 누출 위험 없음

Oniux와 torsocks 비교

     * Torsocks는 libc의 네트워크 함수들을 LD_PRELOAD 기법으로 후킹해 Tor의 SOCKS 프록시로 우회시킴
     * Oniux는 네임스페이스 격리로 작동하여, 정적 바이너리나 Zig 등에서도 트래픽 유출을 100% 방지함
     * 주요 비교 내용
          + Oniux: 별도 Tor 데몬 불필요, 네임스페이스 사용, 모든 앱 지원, 악성 앱의 raw 시스템콜도 차단, 리눅스 전용, 신규/실험적, Arti 기반, Rust 작성
          + Torsocks: Tor 데몬 필요, ld.so 해킹, libc 연결 앱만 지원, raw 시스템콜은 유출 가능, 크로스플랫폼, 15년 이상 검증, CTor 엔진, C 언어 작성

Oniux 사용 방법

     * Rust 개발 환경이 구축된 리눅스 시스템 필요
     * 커맨드라인에서 간단히 oniux를 설치 및 실행 가능

   주요 사용 예시:
     * $ oniux curl https://icanhazip.com # Tor 우회 얻은 IP 조회
     * $ oniux bash # 쉘 전체를 Tor 격리로 실행
     * $ oniux hexchat # GUI 앱도 Tor로 강제 우회 가능
     * $ RUST_LOG=debug oniux curl ... # 디버깅 로깅 지원

내부 동작 원리

     * Oniux는 clone(2) 시스템콜로 독립된 네트워크·마운트·PID·사용자 네임스페이스에서 자식 프로세스를 생성함
     * 자식 프로세스는 /proc을 개별적으로 마운트하고, UID/GID 매핑으로 권한 맞춤
     * 네임서버 정보가 담긴 임시 파일을 /etc/resolv.conf에 바인드 마운트해, Tor 기반 네임리졸버 강제 사용
     * onionmasq로 TUN 인터페이스 (onion0) 생성 후 IP 할당·설정
     * 자식 프로세스가 인터페이스 fd를 유닉스 도메인 소켓으로 부모 프로세스에 전달하고, 권한을 최소화함
     * Rust 기능을 이용해 최종적으로 사용자가 입력한 명령을 실행함

Oniux의 실험적 특성

     * Oniux는 Arti, onionmasq 등 신기술을 기반으로 만들어진 초기 버전임
     * 현 시점에서 정상적으로 동작하지만, torsocks처럼 오랜 기간에 걸친 성숙 사례 경험은 부족함
     * 안정성 확보와 성능 개선을 위해 다양한 실사용 피드백이 필요함

크레딧 및 지원

     * Rust기반 IP 스택인 smoltcp와, 사용자 네임스페이스 활용법을 개발에 조언한 7ppKb5bW 등 개발자들에게 감사함 표명
     * The Tor Project 및 커뮤니티 지원으로 oniux 프로젝트가 유지되고 있으며, 프라이버시와 오픈 소프트웨어 발전을 위해 후원을 권장함

   Tor는 프라이버시에는 나쁘지 않은 것 같은데 익명성에 알맞은 도구인지는 잘 모르겠네요. 출구 노드들을 이미 국가기관들이 장악하고 있다는 이야기도 있고.

        Hacker News 의견

     * 10년 전쯤 네트워크 네임스페이스가 핫한 주제로 떠오를 때 Tor 개발자와 이 주제로 대화한 경험이 있음. 그때 받은 피드백은, 네임스페이스를 통한 격리가 사람들이 보안에 안심하게 만들지만 여전히 많은 식별 가능한 정보가 유출될 수 있다는 점이었기에 더 이상 추진하지 않았던 기억
          + Tor 팀이 그 부분을 강조한 건 전략적으로 실수였다고 생각. 심각한 위협에 노출된 사람들은 Tor 브라우저를 쓰고 다른 정보 유출 경로도 신경 써야 한다는 게 맞지만, Tor가 모두에게 기본이 되었다면 대규모 감시 자체가 훨씬 어려워졌을 거라 생각. 지금은 누가 Tor를 쓰는지 자체가 감시 대상이지만 모두가 쓴다면 그 정보가 의미 없어지는 상황
          + torsock과 torify도 기본적으로 같은 역할을 하지만 견고함 면에서는 덜하다고 느끼는 지점
     * 설치 가이드에 나온 명령어대로 하면 동작하지 않는 상황. 버전 번호를 0.4.0에서 0.5.0으로 바꿔야 함
       cargo install --git https://gitlab.torproject.org/tpo/core/oniux oniux@0.5.0
     * 나는 원래 torsocks처럼 로컬에서 돌아가는 tor 데몬을 통해 트래픽이 나간다고 생각했음. 그런데 로컬 tor 데몬을 꺼도 oniux는 여전히 잘 동작하지만 torify랑 torsocks는 안 됨. 문서를 보니 실제로 그렇게 되어 있음. 아주 신기한 사실. docker에서도 잘 동작하지만 --privileged 옵션을 써야 했고, 단순히 바이너리를 debian:12 컨테이너에 복사해도 정상 작동하는 경험
       docker run -it --rm --privileged -v ""$PWD/oniux:/usr/bin/oniux""; debian:12
          + 이게 예전 C 데몬이 아니라 Rust로 새롭게 만들어진 라이브러리를 사용하는 게 아닐까 생각
            https://tpo.pages.torproject.net/core/arti/
     * 이 모든 게 TCP에만 해당되는 건지 궁금. 즉, 비TCP 트래픽도 보호되는지 의문
          + 자세한 건 모르지만 https://gitlab.torproject.org/tpo/core/onionmasq를 보면 TCP뿐 아니라 UDP도 지원하는 유저 스페이스 네트워크 스택을 만들어 Tor 네트워크로 포워딩하는 시도인 것 같음
          + Tor 브라우저 쓰는 사람들이 YouTube나 DNS, 그리고 HTTP/3은 어떻게 처리하는지 궁금
          + 비TCP 트래픽은 라우팅되지 않고 그냥 전송이 실패하는 구조
     * Oniux는 공식적으로 지원받는 도구로 보임. orjail과 비슷한데, orjail은 4년 동안 커밋이 없지만 여전히 iptables/iproute 도구와 함께 셸 스크립트로 잘 동작함
       orjail은 firejail로 추가 격리 가능한 옵션도 있는데 Oniux엔 아직 해당 기능 없음
       https://github.com/orjail/orjail/blob/master/usr/sbin/orjail
          + No Javascript 버전에 대한 링크
            https://raw.githubusercontent.com/orjail/orjail/master/usr/sbin/orjail
     * 이제 chrome으로 tor 웹사이트 접속이 가능한지 궁금
          + 할 수 있긴 하지만 그렇게 하지 않기를 권장. chrome엔 Tor 브라우저에 있는 각종 anti-fingerprinting 전략이 없음. 일반 브라우저를 사용하면 더욱 두드러지는 사용자가 된다는 점.
          + 사실 예전부터 프록시 환경변수(혹은 설정)만 잘 지정하면 chrome 같은 브라우저에서도 접근 가능. tor 데몬의 표준 포트는 9050. socks 프록시를 직접 작성해 트래픽을 라우팅하는 것도 쉬운 편. 예시로 socks5 프록시를 이용해 syncthing 같은 곳에 트래픽을 보낼 수도 있음
            https://github.com/acheong08/syndicate
     * hexchat을 예시로 드는데, 유저의 프로필 설정을 그대로 쓴다면 IRC 유저명이 유출되지 않나 의문
       브라우저를 실행하면 쿠키 같은 것도 유출 가능성
          + 역할 분리가 중요. Tor는 핑거프린팅 방지에 많은 노력을 기울이긴 하지만 근본적으로 Tor와 Oniux의 목적은 출발지 IP 추적 불가능하게 하는 데 있음. Tor로 Gmail에 로그인하거나 하면 같은 문제가 있음(HTTPS가 적용되지 않는 한)
          + 유저명을 유출한다는 게 어떤 의미인지 궁금. 실상은 그 유저명이 Tor를 쓴다는 것만 알려줄 뿐. 같은 IRC 호스트에 같은 유저명이 계속 접속되면 모두 같은 사람이란 것까진 유출 가능. IRC는 익명 지향이라면 꽤 위험한 수단인 셈. 네트워크 단절 이벤트 등과 연결해 로그를 남기는 사람들이 많으니 상호 연관성 노출 가능성
     * DevEx(개발 경험) 부분이 정말 잘 되어 있고 바보-proof라 느껴질 정도. 개발팀에게 박수를 보내고 싶음
          + 사실 완전히 그렇진 않다고 생각. 바보들은 항상 창의적이며 익명성을 보장하려면 매우 신중한 운영이 필요한데 그 수준은 대부분의 사용자에게 기대하기 힘든 수준
     * 코드를 C로 다시 작성해주면 기쁜 마음으로 쓸 생각
          + 이미 Rust로 작성되어 있음. 굳이 C 버전을 원하는 이유가 궁금
"
"https://news.hada.io/topic?id=21236","아버지 테드 Kilnettle Shrine 테이프 디스펜서","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    아버지 테드 Kilnettle Shrine 테이프 디스펜서

     * Kilnettle Shrine 테이프 디스펜서는 영국 시트콤 Father Ted에 영감을 받은 DIY 장치임
     * 최신 버전은 크기 축소, 음질 개선, 3D 프린트 가능한 케이스로 개선됨
     * ESP8266 마이크로컨트롤러와 IR 센서를 사용하여 간단히 제작 가능하고, 총 부품 비용이 10유로 미만임
     * 제작법, 소프트웨어, 3D 모델 파일이 GitHub 등에서 무료로 제공됨
     * 개발자는 기부를 권장하며, 트랜스젠더 지원 단체에 기부 후 제작 공유를 희망함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 이 프로젝트는 영국 코미디 Father Ted의 유명한 ""테이프 두 인치 사용"" 대사가 나오는 독특한 테이프 디스펜서를 직접 만들어 보는 DIY 사례임
     * 기존 버전은 과도한 하드웨어를 사용했고, 내구성 부족과 낮은 음질 등의 단점이 있었음
     * 최근에는 더 작고, 음질이 좋아지고, 디자인도 전문적으로 업그레이드된 새로운 버전을 제작함

개선된 디자인 및 특징

     * 새 버전의 케이스는 3D 프린터로 간편하게 출력 가능하며, 별도의 서포트 없이 제작 가능함
     * 테이프 길이 측정 기술이 Rotary Encoder에서 적외선 LED와 센서(IR 센서) 방식으로 변경됨
     * 제어 장치는 Raspberry Pi Zero에서 ESP8266 마이크로컨트롤러로 바뀌어 가격이 더 낮아짐
     * 전체 전자부품 비용이 10유로 미만 수준으로 제작 가능함
     * 3D 모델링과 전자 설계, 제작 과정에서 소프트웨어 및 하드웨어 통합 역량을 크게 발전시킨 경험임

판매 계획과 직접 제작 권장

     * 직접 생산 및 판매는 시간, 물량, 채산성 문제로 잠정 중단 결정함
     * 대신, 모든 빌드 자료(소프트웨어, 3D 모델 파일, 조립 안내)를 GitHub 및 Printables에서 무료로 공개함
     * 기본적 납땜과 3D 프린터 활용이 가능하다면 하루 만에 충분히 제작할 수 있는 프로젝트임
     * 직접 제작하거나, 새로운 디자인으로 변형을 시도해도 좋음

오픈 소스 파일 및 커뮤니티 기여

     * 설계 도면, 프린트 파일, 소프트웨어 코드는 GitHub와 Printables에 공개됨
     * 조립 영상과 문서 정보도 함께 제공함
     * 완성하면 제작 사진이나 변형 결과물을 공유하는 것을 환영함

사회적 기여 요청

     * 개발자는 직접 제작하는 이용자에게, 트랜스젠더 지원 단체에 대한 자발적 기부를 권장함
     * 이는 Father Ted 제작진 중 한 명의 최근 행동을 상쇄하기 위한 사회적 연대의 의미임

        Hacker News 의견

     * “My lovely horse”라는 가사가 인상적인데, 들판을 달리는 말의 모습과 사랑스러운 말에 대한 애정을 노래한 느낌의 노래 가사 공유
          + 이 유튜브 영상은 정말 질리지 않는 명작 감상 경험
          + 그 색소폰 솔로 부분은 반드시 빼야 한다고 생각
     * Father Ted와 Lourdes 테이프 디스펜서가 HN 메인에 오를 거라곤 상상도 못 했는데, 오늘 정말 멋진 하루 시작 느낌
          + 느지막한 아침에 햇살 받으면서 이 글을 발견했는데, 정말 멋진 하루를 여는 경험이었음. 아내에게 보여줬더니 Fr. Ted를 처음부터 같이 다시 보자는 좋은 아이디어가 나와 기쁨. 나는 Fr. Ted가 처음 방영될 때 90년대 대학생이었고, 목요일 밤마다 번화가 술집에 친구들과 모여 9시에 새 에피소드를 큰 스크린으로 함께 보면서 멋진 추억을 쌓았음. 덧붙여, Ted, Dougal, Fr Jack, Mrs Doyle, Bishop Brennan, Fr Noel Furlong, Fr Stone, Fr Fintan Stack, Tom, Henry Sellers 등 주요 캐릭터의 음성 버전 선택지를 추가하면 재미있겠다는 아이디어 고민. 에피소드 한 번 등장한 캐릭터도 내 세대에선 농담이나 레퍼런스로 계속 회자될 정도로 각본과 연기 완성도 높았던 추억 공유
          + 믿을 수 없는 전개 감탄
     * Baader–Meinhof 현상 얘기하자면, 딱 일주일 전에 Father Ted를 처음 알게 돼서 유튜브 추천 덕분에 테이프 디스펜서 편만 봤는데, 이 (오래된) 글이 이제 내 피드에 나타남. 정말 희한한 경험 공유
          + 어제 처음으로 Baader-Meinhof 현상을 접했는데 오늘 또 이 단어가 보임! 마치 Baader-Meinhof 현상의 현상처럼 느껴지는 상황
     * 이건 분명 에큐메니컬(교파 통합 관련) 문제로 보임
     * 글에서 “로직은 Raspberry Pi Zero 대신 ESP8266 마이크로컨트롤러에서 동작하며, 덕분에 전자 부품 전체를 10유로 이하로 구입할 수 있게 됨”이라고 하는데, 이런 류의 제품은 대량 생산 땐 0.10달러 마스크 프로그래밍 COB로 전자회로 제작 가능성 언급
     * Father Ted가 미국에선 얼마나 영향력 있었는지 궁금. 유머가 영국/아일랜드식 느낌 강하게 드는 작품이라 생각. (Monty Python도 그랬지만 미국에서 성공 경험)
          + BBC America가 1998년부터 존재. BBC America wiki 참고 정보 공유
     * 정말 독특하고 멋진 프로젝트라고 느낌. “로터리 인코더 대신 IR LED와 센서를 사용해 테이프 회전을 측정한다”는데, 구체적으로 동작 방식이 궁금. 릴에 특정 위치에 구멍이 있어서 빛이 감지되는 구조인지, 그리고 왜 이 방식이 로터리 인코더 설계보다 장점이 있는지 이해되지 않음
          + 좋은 질문에 답변. 실제로 릴에는 여러 구멍이 있어 IR 방출기/감지기가 양쪽에 설치되는 구조. [GitHub 저장소]나 Printables에서 STL 파일을 직접 볼 수 있음. 이 구조가 로터리 인코더에 비해 더 저렴하고, 센서를 하중 지지 부품으로 쓸 필요 없어서 이 프로젝트에 특히 더 적합하다고 생각
     * Father, 이게 다 무슨 의미인지 궁금
          + 그 돈은 그냥 내 계좌에 잠시 머문 것뿐임. Pat Mustard에게 문의 추천
     * Mrs Doyle 버전으로 바꾼 테이프 디스펜서 상상 가능. “Go on, go on, go on, go on, goes on”이라는 멘트로 만들거나, 테이프 대신 차 한 컵을 내주는 기능도 가능성 언급
          + “microcake” 개념 언급하며, 정말 먹어도 들어가는지 모를 정도의 케이크 상상력
          + “coup of tea”라는 오타를 가지고 Mrs Doyle이 정말 정부가 차 제공을 막으면 무슨 일도 할 수 있을 것 같다는 재치 있는 상상력
     * 테이프 쓰는 게 축복받을 수 있는 일인지 몰랐는데, 오늘 처음 배운 놀라운 사실
          + 사실 테이프 디스펜서는 성지 주변에서 흔히 보이는 기념품 가판대를 풍자한 소품임. 한 가지 궁금한 점은 디스펜서가 테이프 롤의 반지름이 변화함에 따라 측정을 어떻게 보정하는지인데, 아마 보정 기능 없이 단순 원리로 동작하는 것 같음
"
"https://news.hada.io/topic?id=21295","Cloud Run GPU, 이제 공식 지원 - 모두를 위한 AI 워크로드 실행을 더 쉽게 만듦","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Cloud Run GPU, 이제 공식 지원 - 모두를 위한 AI 워크로드 실행을 더 쉽게 만듦

     * Cloud Run에서 GPU가 공식적으로 지원(GA)됨에 따라, AI 워크로드 실행이 더욱 용이해짐
     * Cloud Run jobs에서도 GPU 사용이 가능해지며, 배치 처리 및 비동기 작업에 새로운 가능성을 제공
     * 이미지 처리, 자연어 분석, 미디어 변환 등 대규모 배치 작업에 최적화된 환경
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cloud Run GPU, 공식 제공 및 주요 변화

  Cloud Run jobs에서 NVIDIA GPU 지원 개시

     * Cloud Run의 GPU 기능은 기존에는 실시간 추론과 같은 요청 기반 서비스에서 활용됨
     * 이제 Cloud Run jobs에서도 GPU 지원이 공식화되어, 새로운 활용 사례를 가능하게 함
          + 모델 파인튜닝: 사전 학습 모델을 특정 데이터셋에 맞추어 손쉽게 재학습 가능
          + 배치 AI 추론: 이미지를 분석하거나 자연어를 처리, 추천을 생성하는 대규모 작업에 적합
          + 대량 미디어 처리: 동영상 트랜스코딩, 썸네일 생성, 이미지 변환 등을 GPU를 활용해 효율적으로 처리할 수 있음
     * GPU가 장착된 Cloud Run job은 작업 완료 후 자동으로 리소스를 줄여 관리 부담을 최소화함

  초기 도입 기업들의 실제 경험

     * vivo: Cloud Run이 AI 애플리케이션 반복 개발 속도를 빠르게 하고, 운영 및 유지 보수 비용을 크게 절약함. GPU의 자동 확장 기능이 해외 시장에서 AI 적용 효율을 획기적으로 성장시킴
     * Wayfair: L4 GPU는 강력한 성능과 합리적인 가격대를 동시에 제공하며, Cloud Run의 빠른 오토스케일링과 결합하여 비용을 약 85% 절감한 경험을 가짐
     * Midjourney: Cloud Run GPU는 대규모 이미지 처리에 매우 유용하며, 단순 명료한 개발 환경 덕분에 인프라 관리 부담 없이 혁신에 집중 가능함. GPU 확장성 덕분에 수백만 이미지의 분석 및 처리가 용이함

  시작 안내 및 리소스

     * Cloud Run에서 GPU 지원을 통해 차세대 애플리케이션 개발에 적합한 환경이 마련됨
     * 공식 문서, 빠른 시작 가이드, 최적화 모범 사례를 통해 누구나 쉽게 시작 가능함
     * GPU가 적용된 Cloud Run job의 프라이빗 프리뷰 참여 또한 신청 가능함

결론

     * Cloud Run의 GPU 공식 지원은 AI, 대규모 배치 처리, 미디어 변환 등 다양한 전문 워크로드에 획기적인 확장 가능성을 제공함
     * 비용, 운영 효율성, 확장성까지 다양한 이점을 실제 기업들이 입증함
     * 간단한 설정과 다양한 학습 자료를 바탕으로 누구나 쉽게 클라우드 기반 GPU 워크로드를 시작할 수 있음

        Hacker News 의견

     * 나는 Google Cloud Run을 정말 좋아해서 최고의 선택지라고 적극 추천하는 입장. 다만 Cloud Run GPU는 추천하기 어렵다는 판단. 인스턴스 기반 과금이 비효율적이고, GPU 옵션도 한정적. 모델을 GPU 메모리에서 로딩/언로딩할 때 성능 저하로 서버리스 환경에는 느림이라는 한계. 실제 비용을 비교하면 하루 30%만 활용해도 VM+GPU 조합이 더 경제적이라는 계산. (관련 블로그 링크)
          + Google 부사장. 피드백 고마움. 지금 가격 구조에서는 서비스 용량이 거의 고정적으로 필요한 경우, VM 미리 프로비저닝하는 게 더 비용 효율적이라는 점 일반적으로 동의. 반면 Cloud Run GPU는 피크 수요가 갑자기 생기는 신제품이나 AI 앱처럼 최소한의 유휴 비용, 매우 빠른 시작, 드물고 불규칙한 트래픽에 맞는 환경에 최적화라는 생각
          + Cloud Run이 정말 훌륭한 서비스라는 인상. AWS의 ECS/Fargate보다 훨씬 다루기 쉽다는 경험
          + GCP에서 VM을 믿고 사용할 수 없다는 게 가장 큰 문제. 주요 클라우드들 모두 이런 이슈 존재. AWS에서는 80GB GPU를 장기 예약 없이 구할 수 없고, 가격은 터무니없음. GCP도 마찬가지로 비싸고 가용성 낮음. 대기업들은 스타트업 친화적이라고 말하지만 실제 경험은 그렇지 않음. runpod, nebius, lambda 등 네오클라우드들이 훨씬 나은 서비스 제공. 대형 클라우드는 고정 수요에 안주하며 스타트업을 배려하지 않아 장기 성장에 큰 타격을 줄 실수 중이라는 생각
          + Cloud Run에서 상반된 경험을 함. 원인 불명 스케일 아웃/재시작으로 인해 결국 유료 지원 서비스를 직접 구입해 문의했지만 답을 못 찾았음. 결국 직접 VM을 셀프매니지로 전환. 그 후로 개선됐는지는 모르겠음
          + Cloud Run이 최고라는 의견에 대해, 직접 수치를 확인해보고 싶다는 입장. 장난감 프로젝트엔 좋지만, 실무에서는 비용 구덩이임. 프로젝트 중 오토스케일 이슈가 지속적으로 발생, '스케일 투 제로'가 이론상 좋아 보이지만 실제로는 워밍업 과정에서 하나의 요청에 여러 개 컨테이너가 뜨고 오랜 시간 유지되는 경우 많음. 가시적인 CPU나 네트워크 사용이 없는 원인불명의 컨테이너도 비용이 계속 청구됨. Java나 Python 프로젝트는 cold start 속도가 심각하게 느리며, Go/C++/Rust는 경험이 없어서 잘 모르겠음
     * 대형 클라우드의 복잡함에 더해, 무제한 YOLO(마구잡이) 요금 청구로 밤새 신용카드가 텅텅 빌 위험까지 있음이라는 우려. Modal과 vast.ai에 계속 머무를 예정이라는 결론
          + 개인/스몰 프로젝트 사용자의 입장에서 비용 상한선(CAP)을 제공하지 않는 것은 GCP의 큰 약점. Cloud Run의 경우 동시 처리(concurrency) 제한, 인스턴스 수 제한을 통해 간접적으로라도 비용을 막을 수 있음. 그래도 온전한 CAP에는 못 미침
          + AWS에서 인스턴스 종료를 깜빡해 높은 비용을 냈던 기억이 있어, Cloud Run의 scale to zero와 초단위 과금이 큰 장점. 시작이 정말 빠르다면 내 워크로드에 완벽할 듯한 확신
          + Cloud Run에서는 최대 인스턴스 수 설정으로 최대 비용을 간접적으로 제한할 수 있음. App Engine 시절의 '하드 캡'은 실제로 서비스가 뜨는 순간(예: HN에 올라갔을 때) 완전히 멈춰버리는 부작용 발생. 개인적으로는 알림 기반 예산 관리가 더 나은 선택
          + 내가 실제로 Datadog을 프로덕션에서 버린 이유도 바로 이 때문. 플랫폼들이 사용자가 실수로 초과 청구를 당해 생기는 부정적인 인상을 감내할 만한 가치가 있는지 의문
          + Modal이나 vast.ai가 어떻게 YOLO 청구를 막는지 명확히 모르겠음. 선불 구조인지, 직접적인 CAP을 제공하는지 궁금
     * 직접 가격을 비교하니 확실히 메리트가 느껴지지 않는다는 인상. 구글, runpod.io, vast.ai의 시간당 요금을 구체적으로 표로 정리:
  1x L4 24GB:  google: $0.71, runpod.io: $0.43, 스팟: $0.22
  4x L4 24GB:  google: $4.00, runpod.io: $1.72, 스팟: $0.88
  1x A100 80GB: google: $5.07, runpod.io: $1.64, 스팟: $0.82, vast.ai $0.880, 스팟: $0.501
  1x H100 80GB: google: $11.06, runpod.io: $2.79, 스팟: $1.65, vast.ai $1.535, 스팟: $0.473
  8x H200 141GB: google: $88.08, runpod.io: $31.92, vast.ai $15.470, 스팟: $14.563

       구글 가격은 한 달 24/7 구동 기준인 느낌이 있는데, runpod.io와 vast.ai는 초단위 과금. 구글 GPU의 스팟 요금은 못 찾음
          + ""컴퓨트 인스턴스 생성""에서 스팟 요금을 바로 확인할 수 있음. 예를 들어 GCP에서 1xH100 spot은 시간당 $2.55, 장기 이용할수록 할인이 적용됨. 실제 기업 고객이라면 이런 가격도 할인 가능. 일반 유저만 이런 정가로 결제
          + vast.ai 요금 출처가 궁금. 홈페이지 기준 8xH200 옵션은 대부분 시간당 $21.65 이상으로 보임
          + 구글의 가격 책정이 24/7 전제로 잡혀있다는 근거가 뭔지 궁금. Cloud Run 공식 요금 페이지를 보면 실제 사용량만 100밀리초 단위로 과금, 오토스케일링도 유휴 인스턴스는 15분 대기 후 자동 축소된다는 설명 (Cloud Run PM)
          + Cloud Run GPU에서는 1xL4만 선택 가능한 것 아닌지 의문
          + 구글 가격도 초단위 과금이라면, 20분 미만 사용시 오히려 구글이 유리할 수도 있다는 의견
     * 나는 Modal의 열렬한 팬으로, serverless scale-to-zero GPU를 오랫동안 사용 중. 필요할 때 큰 규모로 손쉽게 스케일 업, 동시에 개발 부담도 현저히 적음. 대형 제공업체가 이 시장에 뛰어드는 것이 흥미로움. Modal로 옮긴 계기도 기존 대형 클라우드에선 이런 기능(AWS Lambda에선 GPU 미지원)을 제공하지 않았기 때문. 이제 모든 주요 클라우드가 이런 서비스 방향으로 가는 것인지 궁금
          + Modal은 정말 훌륭함. 자체적으로 공개한 LP(선형계획) 솔버 심층 기술도 인상적. Python 개발자라면 Coiled도 추천. Modal만큼 빠르진 않지만 GPU VM을 쉽게 스핀업, 모든 것이 자신의 클라우드 계정에서 실행됨. CUDA 드라이버/파이썬 라이브러리 동기화 등 편리한 패키지 관리 제공. (참고: Coiled 소속, 하지만 진심 추천)
          + HIPAA 준수 워크로드까지 지원하는 점도 기대 밖의 장점
          + Modal의 cold start 속도가 10GB 이상의 모델 기준 가장 빠름
          + Modal 문서도 매우 잘 정리되어 있다는 점 인상 깊음
     * Cloud Run이 다른 서비스보다 좋은 가장 큰 이유는 오토스케일, scale-to-zero. 실제 사용이 없을 땐 실질적으로 과금이 0, 인스턴스 최대 수를 지정해 최대 비용도 안정적으로 관리 가능. 단, CPU 버전만 쓰는 기준이고 매우 신뢰도 높고 사용이 쉬움
          + 단, 일반 Cloud Run도 냉시작 부팅 시간이 길 때(약 3~30초) 많아, scale-to-zero 활용 시 지연 이슈 있음
     * 유럽의 소형 GPU 클라우드 제공사 DataCrunch(관계 없음)가 RunPod 등보다 저렴하게 Nvidia GPU VM을 제공
       1x A100 80GB 1.37유로/시간
       1x H100 80GB 2.19유로/시간
          + lambda.ai에서는 1x H100 80GB VM이 시간당 $2.49에 제공됨. 환율로 딱 2.19유로. 이게 우연인지 업계의 보이지 않는 상한선이 있는 건지 궁금
          + Vast.ai에서 P2P 방식으로 2x A100을 $0.8/시간에 사용할 수 있음(즉 A100 하나에 $0.4/시간). 본인은 단순 만족 이용자일 뿐. 네트워크 속도는 유의해야 함. 일부 호스트는 대역폭 공유라 실제 속도가 광고와 다를 수 있음. 대용량 데이터 이동 시 주의 필요
     * Cloud Run/GKE 담당 VP/GM. 관련해 질문 받을 준비 되어 있음. 많은 관심 고마움
     * Cloud Run을 좋아하며, 새 기능도 흥미롭게 보임. 다만 아쉬운 점은 self hosted GitHub runners를 돌리고 싶어도 root 권한 이슈로 지원이 안 됐던 점. 또 새로 도입된 worker pool 기능도 실전에서는 scaler를 직접 짜야 해서 내장된 기능이 아니었던 점
          + Serverless 및 Worker Pools Autoscaling 담당 Eng Manager. 현재 로드맵을 적극적으로 정의하는 중이고, 실제 워크로드 사용 예시를 메일로 알려주면 큰 도움이 될 것 같음. worker pools와 스케일링이 필요한 워크로드에 대해 의견 기다림
     * vertex.ai로 모델을 테스트용으로 계속 돌리다가 꺼두는 걸 잊어 $1000 요금이 청구된 경험 후, 이번에 Cloud Run이 내 go to 서비스가 될 듯. 수년간 Cloud Run으로 프러덕션 마이크로서비스 및 취미 프로젝트 운영, 단순함과 비용 효율 모두 만족
     * 만약 이해가 맞다면, Hugging Face 같은 임의 모델을 띄운 API를 만들 수 있고 토큰별 과금 구조는 아니지만 사용 부하가 적을 경우 상당히 저렴하게 운용 가능하다는 판단. 실제로 그렇다면 큰 혁신. 기존 대부분의 공급업체는 커스텀 모델을 운영하려면 월 구독료 요구
          + 기본적으로 맞다는 설명. 단, cold start 속도가 매우 느릴 수 있음(30~60초). scale to zero의 단점. 또한 컨테이너 저장 등 몇 가지 소액 월정 요금도 부과되는 점 유의
          + Runpod, vast, coreweave, replicate 등 서버리스 GPU 추론을 지원하는 다양한 대안 존재
"
"https://news.hada.io/topic?id=21209","Voiden - 무료, 오프라인, Git-Native API 클라이언트 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Voiden - 무료, 오프라인, Git-Native API 클라이언트

     * 오프라인에서 동작하는 Git-네이티브, 모듈형, 확장성을 지닌 API 클라이언트로, 개발자 친화적 워크플로우와 완전한 로컬 제어 환경을 제공
     * API 문서화, 테스트, 목(mock), 스펙 관리를 하나의 구조화된 워크스페이스에서 제공하며, 별도의 앱 전환/동기화 없이 버전 관리와 추적성을 강화
     * 개발자 맞춤형, 플러그인 중심 설계
          + 기존 툴이 워크플로우를 강요하는 것과 달리, Voiden은 사용자의 스택과 포맷, 작업 흐름에 유연하게 적응함
          + Markdown 기반의 문서와 블록 시스템을 사용해, API 문서화와 테스트를 동일한 공간에서 진행할 수 있음
          + API 요청을 재사용 가능한 블록으로 분해하여, 다양한 문서와 워크플로우에서 가져와 쓸 수 있음
          + 하나의 블록을 수정하면 여러 곳에 반영되어, 시스템 관리 효율성이 높아짐
     * 프로그래머블 인터페이스
          + API 정의와 문서가 동적으로 커스터마이즈되는 인터페이스로 변환됨
          + 고정된 UI나 템플릿에 제한되지 않고, 모든 내용이 Markdown으로 렌더링됨
     * Git-Native
          + 모든 변경 이력은 Git 기반으로 관리되어, 브랜치·병합·히스토리 관리가 자연스럽게 연동
          + 별도의 플러그인이나 추가 구성 없이, Git 본연의 워크플로우에 맞춰 사용 가능함
          + 완전히 오프라인 및 로컬 환경을 지향하여, 클라우드 동기화나 벤더 락인에서 자유로움
     * Docs That Do More
          + API와 동일한 공간에서 문서를 작성, 실시간 요청·동적 응답·사용 예시를 바로 삽입할 수 있음
          + 문서가 API의 실제 구조와 상태를 인지하므로 중복 감소, 오류 방지, 개발자 경험 개선에 효과적
     * Voiden Terminal : 명령줄 인터페이스(CLI) 기반으로 API 생성·테스트·관리 전 과정을 빠르고 효율적으로 처리 가능
     * Plugin Architecture
          + 사용자 정의 블록, 데이터 소스, 렌더러, 로직 등 직접 확장 가능
          + 공식 지원 없이도 커스텀 기능 추가 및 커뮤니티 공유에 용이함
     * 맥(애플 실리콘)/윈도우/리눅스(베타) 지원
     * 무료(오픈소스는 아님)

   마지막 줄이 살짝 맘에 걸리네요

   깔끔하고 좋아보입니다
"
"https://news.hada.io/topic?id=21249","Google AI Edge - 온디바이스 크로스플랫폼 AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Google AI Edge - 온디바이스 크로스플랫폼 AI

     * Google AI Edge는 모바일, 웹, 임베디드 기기 전반에 AI 모델 배포를 간편하게 지원함
     * 통합된 크로스플랫폼 프레임워크로 Android, iOS, 웹, 임베디드 환경에서 동일한 모델 구동이 가능함
     * 다양한 머신러닝 프레임워크(JAX, Keras, PyTorch, TensorFlow) 와 호환성을 제공함
     * 모델 변환 시각화·디버깅, 커스텀 파이프라인 구축 등 고급 개발 도구를 제공함
     * Gemini Nano 등 온디바이스 생성형 AI 환경을 Android 및 Chrome 플랫폼에서 활용 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Google AI Edge 소개

     * Google AI Edge는 온디바이스 및 크로스플랫폼 AI 배포를 위한 솔루션임
     * 모바일, 웹, 임베디드 애플리케이션 환경 등 다양한 플랫폼에 AI 모델을 효율적으로 배포·실행할 수 있는 플랫폼임.

  주요 특징

     * 기기 내 보관: 데이터가 로컬에 비공개로 유지되어 지연 시간 감소 및 오프라인 동작을 지원함
     * 크로스 플랫폼 지원: Android, iOS, 웹, 임베디드 환경에서 동일 모델의 실행이 가능함
     * 멀티 프레임워크 호환성: JAX, Keras, PyTorch, TensorFlow 등 여러 머신러닝 프레임워크와의 호환성 제공함
     * 전체 AI 에지 스택: 유연한 프레임워크, 턴키 솔루션, 하드웨어 가속기를 통합적으로 지원함

  기성 솔루션 및 유연한 프레임워크

    일반적인 AI 작업을 위한 로우 코드 API

     * 생성형 AI, 비전, 텍스트, 오디오 등 일반적인 AI 작업을 쉽게 처리할 수 있는 로우코드 교차 플랫폼 API를 제공함
     * MediaPipe 기반의 솔루션으로 빠른 시작 및 적용이 가능함

    커스텀 모델의 크로스플랫폼 배포

     * JAX, Keras, PyTorch, TensorFlow 등으로 학습된 기존 AI 모델을 Android, iOS, 웹, 임베디드 기기에서 고성능으로 실행할 수 있음
     * LiteRT 지원으로 운영 효율성과 배포 편리성 확보함

    모델 변환 및 시각화 도구

     * 모델의 변환 및 양자화 과정을 시각화하는 기능을 제공함
     * 성능 벤치마크 오버레이로 AI 프로젝트의 핫스팟 디버깅 가능함

    맞춤형 ML 파이프라인 구축

     * 사전 처리 및 사후 처리 로직을 포함해 여러 ML 모델을 체이닝하여 복잡한 기능 파이프라인을 빌드할 수 있음
     * GPU, NPU 기반의 가속 파이프라인을 CPU와 차단 없이 실행 가능함

  Android 및 Chrome의 Gemini Nano

     * Google의 최신 온디바이스 생성형 AI 모델인 Gemini Nano를 통해 Android 및 Chrome 등 다양한 환경에 생성형 AI 기능을 탑재할 수 있음

결론

     * Google AI Edge는 분산형, 온디바이스 AI 기술 배포를 위한 강력한 선택지임.
     * 크로스플랫폼 호환성, 다양한 프레임워크 지원, 개발 생산성 도구와 최신 생성형 AI 환경으로, 스타트업 및 IT 개발자 커뮤니티에 효율적이고 강력한 AI 도입 경험을 제공함.

        Hacker News 의견

     * 내 생각을 말하면, tensorflow lite와 mediapipe 조합이 한때 훌륭했지만 지난 3년간 Google에서 거의 방치된 느낌임. Mediapipe는 의미 있는 업데이트가 거의 없었고, 많이 쓰이는 모델들도 구식이거나 느린 경우가 많음. TF Lite는 애플 ANU 같은 NPU 지원을 했지만 mediapipe에서는 전혀 지원하지 않았음. 그리고 MLKit, Firebase ML, TF lite, LiteRT 등 브랜드도 너무 혼재되어 있었음. 지금은 hugging face transformers나 transformers.js 라이브러리와 함께 onnxruntime을 쓰거나 executorch가 성숙해질 때까지 기다리는 게 나은 선택이라고 생각함. 공식적으로 tensorflow lite / liteRT로 포팅된 최신 SOTA 모델(SAM2, EfficientSAM, EdgeSAM, DFINE, DEIM, Whisper, Lite-Whisper, Kokoro, DepthAnythingV2 등)은 본 적이 거의 없었고, 기본적으로 다 pytorch 위주이지만, ONNX와 MLX 커뮤니티는 여전히 큼
     * https://github.com/google-ai-edge/gallery에서 기기에서 바로 실행되는 ML/GenAI 활용 사례를 모아놓은 갤러리를 볼 수 있음. 여기서 모델을 직접 로컬에서 시도하거나 사용할 수 있음
     * 온디바이스 ML을 위한 솔루션이 늘어나는 것 자체는 좋게 생각함. 다만, 내가 사용하는 특정 활용사례가 아니면 선뜻 사용하게 될지는 의문임. 임의의 입력과 출력을 받는 새로운 모델을 추가하는 난이도도 가늠하기 어려움. 디바이스 간 모델 추론을 위해 Onnx를 써왔는데, Onnx는 정말 낮은 레벨이라 원하는 어떤 가중치든 적용할 수 있음. 많은 업무에서는 transformers.js로 Onnx를 감쌀 수 있어 디코딩 등 반복적인 일을 생략할 수 있음(빔서치 직접 구현 굳이 안해도 됨). 위에서 언급한 가이드와 비슷한, 더 포괄적인 자료는 https://github.com/huggingface/transformers.js-examples임. 내가 언급한 다양한 솔루션은 https://ai.google.dev/edge/mediapipe/solutions/guide에서 확인 가능
     * 이건 TensorFlow Lite + MediaPipe를 새로운 “브랜드”로 재포장한 것임
          + 이게 바로 https://3d.kalidoface.com/에 적용된 기술인가 궁금함. 디바이스에서 구동된다는 점이 인상적임. 상당수 상용 모션캡처보다도 뛰어남. 게다가 이 솔루션이 꽤 성숙했음에도 불구하고 3년 전 이미 deprecated/unsupported로 표기되어버림. Google이 이 기술을 충분히 활용하거나 알리지 않았던 점이 안타까움
     * 이 솔루션을 써본 경험이 있는 사람 있나 궁금함. 나는 맞춤형 pytorch 모델을 coreml로 내보내려고 꽤 오랜 시간 삽질을 했고, 지원 안되는 게 많고, 세그폴트 떠서 자꾸 중단되고, 여러 유치한 오류들로 힘들었음. 누가 이 솔루션이 그 정도로 험난하지 않다고 확신시켜주면 좋겠음
          + 나는 세팅을 다 마치고 Pixel 8a에서 Gemma3 1B를 테스트했음. 불과 몇 분만에 실행됐다는 점은 좋았지만 성능이 별로였음. 겨우 질문만 해도 파싱도 제대로 안 되고, 답변도 시도 안 하고, 영문도 너무 엉망임. “어떻게 이 모델이 내 폰에서 로컬로 돌아갈 정도로 작은지” 묻는 간단한 질문이었는데, 실망이 너무 커서 모델 자체를 포기함. 기본적으로 AI에 대한 기대가 별로 크지 않은 편인데도 그 정도로 실망스러웠음
     * 이걸로 직접 테스트해보았는데 내가 보기에 순수하게 pytorch 모델을 .tflite 모델로 재구성하는 용도였음. 내 경우엔 커스텀 finbert 모델에 적용했는데 모델 크기는 거의 그대로더라. 양자화(quantized)된 버전을 변환했지만 출력이 크게 달랐음. 문서상 standard pytorch 모델, 예를 들면 torchvision.models 계열에 맞춰져 있었던 걸로 기억함. 그러니 그런 계열 모델이면 더 나을지도 모르겠음. 참고로 내가 시도한 건 약 1년 전 이야기고, 그 덕에 대형 버그 패치 전 운 좋게 피해간 셈일지도 모르겠음
     * 여기에 자세한 정보가 있음 https://ai.google.dev/edge/mediapipe/solutions/guide 또한 오픈 소스 링크는 https://github.com/google-ai-edge/mediapipe임. 내가 보기에 이건 실제로 디바이스(엣지)에서 구동되는 AI 모델을 배포하는 통합된 방식임. 일종의 “AI 스택에서의 자바스크립트” 같은 포지션이라고 추측할 수 있음. 이 기술의 타겟 유저가 누구인지 궁금함
          + mediapipe의 일부 모델은 꽤 쓸만하지만, mediapipe 자체는 2019년 무렵부터 있어온 오래된 기술임. 항상 AI의 엣지 구동, 특히 비전 AI(예: 얼굴 추적)에 초점을 맞춰왔었음. 얼굴 추적 같은 건 여전히 유용하지만, 이미지 인식 등에서는 세상이 많이 바뀌어버렸음
          + 타겟 오디언스라면 크로스플랫폼으로 ML 모델을 배포하고자 하는 사람들이라고 생각함. 특히 TFLite 런타임만으로 해소가 안 되는 추가 코드를 지원해야 하는 경우임. LLM이나 컴퓨터 비전과 같은 활용 사례가 적합함. 예를 들어, 손 제스처 인식기를 배포한다면 아래와 같은 복잡한 과정을 거치게 됨: 입력 이미지를 특정 컬러 스페이스 및 크기로 전처리, 이미지를 GPU로 복사, 손 검출용 TFLite 모델 실행, 출력 리사이즈, 제스처 인식용 TFLite 모델 실행, 유효한 결과로 후처리. 이걸 iOS와 Android에 다 배포하려면 단순 TFLite 실행 외에도 부수 코드가 어마어마함. Google이 Mediapipe에서 선택한 방식은 이런 일련의 파이프라인과 공통 처리 노드를 C++ 라이브러리 형태로 묶고 필요한 조각만 고르고 활용하는 것임. 이 라이브러리는 크로스플랫폼으로 컴파일되고, GPU
            가속 옵션도 제공함. Google 내부적으로는 TFLite 런타임에 이런 기능을 확장할지, 아니면 Mediapipe 같은 별도 라이브러리를 만들지 고민이 있었을 거라 생각함. 결론적으로는 TFLite는 “텐서 계산” 자체에 집중시키고, LLM이나 이미지 프로세싱처럼 더 넓은 범위의 작업은 별도 라이브러리로 오프로드하자는 방향이었던 듯함
     * 이게 신제품인지 아니면 기존 MediaPipe 기술들을 하나의 스토리로 묶은 마케팅 페이지 같은 건지 궁금함. 처음에는 상당히 기대했는데, “Google AI Edge”가 도대체 뭔지 혼란스러웠음. 그리고 찾아보니 2년 전쯤 공개된 https://developers.googleblog.com/en/… 이걸 리브랜딩해서 내놓은 것 같음
     * 이미 CoreML이나 TimyML같은 프레임워크로 제공되는 것에 비해 몇 년 뒤처진 솔루션임. 그리고 Google은 먼저 다음 분기 실적 때문에 제품 자체를 곧바로 폐기하지 않는다는 점을 보여줘야 함
          + 사실 그건 맞지 않음. 두 제품은 아예 다름. CoreML은 애플 생태계에 한정된다면 PyTorch 모델을 CoreML(.mlmodel)로 변환해서 iOS/Mac의 가속기와 함께 구동할 수 있음. Google Mediapipe는 크로스플랫폼(ios/android/web)에서 ML 플로우를 돌리는 거대한 C++ 라이브러리임. Tensorflow Lite(이제 LiteRT)까지 포함하고 있고, 이미지 리사이즈 같은 일반적인 전처리 작업도 할 수 있는 그래프 프로세서 역할임. Google이 제품을 일찍 폐기한다는 밈이 있긴 하지만, Mediapipe는 오픈 소스라 최소한 그 점은 인정해야 함. 나는 Mediapipe 포크로 iOS/Android 컴퓨터 비전 제품을 만든 적이 있는데, 매우 복잡했지만 잘 돌아감. CoreML로는 절대 만들 수 없는 크로스플랫폼 솔루션임
          + TensorFlow Lite는 지난 수년간 수십억 장치에서 검증된 경력이 있음. 이 솔루션은 Mediapipe와 이를 하나로 묶어 리브랜딩/확장한 것이라고 봄. Google이 온디바이스 ML에 진지하게 투자한지는 5년이 넘었고, 갑자기 죽일 것 같지는 않음. 다만 이름을 자주 바꿔서 혼란을 주는 건 맞는 듯함
          + 생성형 AI 부분이 애플 생태계에는 없는 것 아닌가. 만약 구글처럼 된다면 엄청난 변화라고 생각함. 개인적으로 채팅 관련 기능이 매우 유용해보임. 그리고 Swift Assist는 도대체 언제 나오냐는 생각임
          + 그냥 리브랜딩된 tensorflow lite임. 나는 2019년부터 edge device에서 사용해옴. CoreML도 훌륭함
          + CoreML은 애플이 TensorFlow를 보고 협업하지 않고 비슷한 기능을 자체적으로 만듦으로써 생긴 것임. TF는 CoreML이 발표될 즈음 이미 2년 전부터 있었고 성공한 프레임워크였음. 지금까지도 CoreML은 사실상 프로프라이어터리 BLAS 인터페이스에 불과하고, 업계에서 널리 쓰이지 않음. iOS 개발자의 관점이론 무서움
     * 이런 작업은 WebLLM으로도 수행 가능함
"
"https://news.hada.io/topic?id=21231","DeepSeek가 대규모에선 저렴하지만 로컬에서는 비싼 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   DeepSeek가 대규모에선 저렴하지만 로컬에서는 비싼 이유

     * DeepSeek-V3와 같은 일부 AI 모델은 대규모 제공 시 저렴하고 빠르지만 로컬 실행 시에는 느리고 비쌈
     * 그 이유는 GPU 활용 효율과 관련된 throughput(처리량)과 latency(지연시간) 의 근본적 트레이드오프에 있음
     * 배치 크기를 키우면 GPU가 효율적으로 동작하지만, 사용자는 토큰이 모일 때까지 대기해야 해 지연시간 증가 현상 발생
     * Mixture-of-Experts 구조와 딥 파이프라인을 가진 모델은 높은 배치와 지연시간을 필요로 함
     * 로컬 단일 사용자 환경에서는 충분히 큰 배치 형성이 어려워 성능 저하 및 비용 증가 문제 발생
     * OpenAI, Anthropic 등은 아키텍처 자체의 효율화, 고도의 배치 전략, 또는 과도한 GPU 투입 등으로 빠른 응답을 구현
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배치 인퍼런스와 GPU 효율

     * GPU는 대규모 행렬 곱셈(GEMM) 에 최적화된 하드웨어임
     * 여러 사용자의 토큰을 한 번에 묶어 큰 행렬로 배치 실행 시, 낮은 왕복 오버헤드와 메모리 효율로 인해 처리량이 급격히 향상됨
     * 인퍼런스 서버는 여러 요청의 토큰을 큐에 쌓고, 적당한 크기의 배치를 선정해 대규모 GEMM 연산을 수행함
     * 이 과정에서 서버는 배치 크기(throughput 증가) 와 대기 시간(latency 증가) 간의 트레이드오프를 선택하게 됨

왜 일부 모델은 대형 배치에 최적화되어 있는가

  Mixture of Experts (MoE)와 배치

     * MoE 구조(DeepSeek-V3, GPT-4 추정)가 GPU 효율이 낮은 주요 원인임
     * 수백 개의 ‘전문가’ 블록들이 각각 분리된 행렬 곱셈을 요구하므로, 소규모 배치에서는 각 전문가가 할 일이 적어 효율이 떨어짐
     * 많은 동시 요청이 있어야 모든 전문가를 충분히 활용 가능하므로, 서비스 수준에서는 대형 배치가 필수임
     * 짧은 대기(윈도우 5ms) 에는 전문가들이 빈번히 유휴 상태, 긴 대기(윈도우 200ms) 에서는 고효율 최대화 가능

  딥 파이프라인 모델의 배치 이슈

     * 수백 계층의 대형 트랜스포머는 여러 GPU에 레이어별로 분할(파이프라이닝)해 실행함
     * 하나의 배치 내 토큰 수가 파이프라인 스텝보다 적을 경우 ‘파이프라인 버블’ 현상 발생, 이는 처리량 감소로 이어짐
     * 이를 방지하려면 큰 배치(긴 대기)가 필수적이며, 이로 인해 모델 응답 시간이 길어짐

왜 큐를 항상 가득 채울 수 없는가

     * 이론상 많은 동시 트래픽으로 항상 큐를 채운다면 버블을 피할 것으로 보임
     * 하지만, 트랜스포머 Attention 단계에서 행렬 크기(길이)가 모두 같아야 배치화 가능하므로, 실무상 단일 큐로 완벽 동작 어려움
     * 또한, FFN과 attention 단계를 분리할 경우 메모리 오버헤드 급증 및 데이터 이동 비효율 이슈 발생

요약 및 결론

     * 대형 배치 처리는 GPU 비용 절감 및 처리량 향상에 필수지만, 사용자는 대기시간이 길어짐
     * Mixture-of-Experts, 대형 파이프라인 구조 모델은 본질적으로 대기 기반 고효율 배치 환경에 최적화됨
     * 로컬처럼 트래픽이 적은 환경에서는 최적화된 대형 배치 구성이 불가하므로 GPU 효율 급감 및 실행 비용 상승 문제 발생
     * OpenAI, Anthropic 등은 빠른 응답성을 보여주는데, 그 이유는
          + (1) MoE가 아닌 더 효율적인 구조일 수 있음
          + (2) 배치/파이프라인 최적화, 고도의 추론 트릭 적용
          + (3) 필요한 것 이상으로 많은 GPU를 투입해 속도를 사는 구조일 수 있음

추가: 프리필 배치와 본문 배치의 차이

     * 트랜스포머는 한 사용자 프롬프트의 프리필(긴 입력)도 배치 실행해 빠른 초기 인퍼런스가 가능함
     * 그러나 본문에서 논의한 배치는 여러 사용자 요청의 본격 토큰 생성 단계에서 발생하는 처리량-지연 트레이드오프 배치임
     * 프리필 배치는 본문에서 언급된 동시 대형 배치와 직접적 관련 없음

참고 사항

     * 실제 인퍼런스 시스템은 지속적 배치(continuous batching) 방식을 병행해 배치가 차면 즉시 실행함
     * 그러나 근본적인 throughput-latency 트레이드오프 구조는 동일하게 적용됨

        Hacker News 의견

     * 나는 Deepseek V3를 집에서 직접 돌리고 있는데, 가격 부담이 적고 속도와 성능이 만족스럽다는 생각. 많은 사람들이 GPU 없이는 큰 모델을 못 돌린다고 생각하지만, 내 경험상 오히려 CPU 서버가 전력 소모도 적고 실용적이라는 판단. 집 서버는 중급 EPYC 9004 시리즈 기반에 384GB 램 장착 슈퍼마이크로 보드 사용, 전체 비용은 약 4000달러 수준. GPU 없이 램만 풍부하게 쓰면 전력은 게이밍 데스크톱보다 낮은 경우도 많음. Unsloth Dynamic GGUF 모델을 활용해서 램 270GB 정도 사용, 실제로는 원본과 거의 비슷한 품질로 다양한 작업 지원 가능. 평소에 16k 컨텍스트로 돌리고 필요하면 24k까지 확장해 사용. 토큰 생성 속도는 초당 9~10, 컨텍스트 크게 하면 7까지. 2CPU 이용하는 환경에서는 이보다 더 빠른 토큰 속도로 전체 모델도 수행하는 사례 존재
          + Unsloth Dynamic GGUF가 실제로 원본과 얼마나 가까운 성능을 보여주는지 궁금증. 내 경험상 간단한 작업에서는 차이가 크지 않지만, 복잡한 과제나 긴 컨텍스트에서는 차이가 분명하게 나타나는 경우 있음. Unsloth가 뛰어난 작업을 하고 있는 건 사실이나, 원본 미양자화 모델과 직접적인 비교 평가 자료가 부족한 점이 아쉬움. 많은 사람들과 기업이 원본 모델을 직접 돌릴 여력이 없는 것도 현실적 한계
          + Ollama 같은 툴로 Deepseek을 코드 생성 용도로 40코어 CPU와 256GB RAM에서 돌리는 게 가능한지 궁금증. 모델에 약 200GB 정도 메모리 할당을 염두
          + 개인 사이트가 접속되지 않는다는 언급. 내가 디지털오션 공동창업자 Jeff Carr임을 소개하며 연락이 닿길 바라는 마음
          + 고속 메모리를 갖춘 GPU가 필수일 거라고 생각했는데, 정말로 GPU 없이 대용량 메모리만으로 추론이 가능한지 질문. 비통합 메모리만으로 어떻게 이게 가능한지 궁금증
          + Deepseek V3가 오픈웨이트 모델 중 실제로 실용성이 뛰어나다는 점에 동감. 많은 작업은 생각보다 높은 수준의 reasoning 토큰이 필요하지 않고, 기다릴 필요가 없다는 점이 장점. 필요하면 언제든지 더 높은 reasoning 옵션을 선택할 수 있다는 것도 매력. 직접 돌리지 않는다면, 몇몇 제공자가 전체 컨텍스트(16k)와 80tps 제공, 데이터 미활용을 약속하는 서비스도 있음. 9004 홈 서버 활용 예시, 멋진 구성이라는 감상
     * 이 블로그글 내용이 인상적이라는 평. 결론(“배칭이 필요하다”)은 동의하지만 MoE 모델 추론에 관한 논의는 좀 더 입체적이어야 한다는 의견. 큰 배치가 중요한 이유는 LLM 추론이 연산량 부족이 아니라, 모든 웨이트를 VRAM에서 로딩하는 것이 병목이라는 점 때문. H100의 TFLOPS와 메모리 대역폭을 비교하면, 사실 1바이트 당 300 FLOP를 처리할 여지가 있다는 계산. 배치가 클 수록 로딩한 파라미터별로 더 많은 연산을 할 수 있으니 배치 사이즈 극대화 필요. 이때문에 “roofline model”이라는 용어 사용. 모델이 점점 커지면서 VRAM에 모델 전체가 들어가지 않으니 GPU나 노드 여러 대로 분산 필요. NVLink나 Infiniband로도 VRAM 직접 로딩만큼 빠르지 않아 병목 발생. MoE의 강점은 expert parallelism, 즉 다른 노드에 서로 다른 expert를 메모리에 올려놓고 노드 간 통신을 최소화할 수
       있다는 점. 단, 모든 expert가 VRAM에 들어가고 KV 캐시 등 오버헤드를 감당할 수 있을 만큼 노드가 충분할 때 가능. 결국 batch 사이즈가 자연스럽게 커질 수밖에 없고, 그렇게 만들어야만 각 GPU가 효율적으로 동작 가능
          + 서로 다른 ""expert""들을 한 노드에 라운드로빈 방식으로 할당하고, 여러 요청이 동일 expert를 쓸 때만 기회적으로 batch로 묶는 방식 제안. 배치 대신 큐를 두는 셈이라 지연 시간은 증가하지만, 심층 연구 워크플로우 같은 환경에서는 충분히 감당 가능한 수준
          + 모델이 한 개의 GPU에 들어갈 수 없을 때, 레이어별로 나눠서 다음 레이어를 책임지는 GPU로 작은 벡터를 전송하여 연산하는 방식으로 추론 가능하다는 실제 적용 사례. Cerebras에서 이런 방식으로 Llama 4 Maverick에서 초당 2500조 토큰 생성. fabric 상에서 벡터 전달은 매우 빨라서 idle time 거의 없음
          + 모든 노드와 웨이트가 아날로그 회로에 배치되면 훨씬 빠른 동작이 가능한지에 대한 가능성을 상상
          + AMD에 투자하는 논리 중 하나가, 모델 전체가 단일 섀시에 들어가서 맵/리듀스 방식의 장점과 네트워크 장비 비용 절감이 있다는 점. 반대 의견 시 insight 제공 요청
     * 시간을 아끼고 싶은 사람을 위한 한 줄 요약: 답은 배치 추론. 여러 사람의 프롬프트를 모델 인스턴스에 동시에 투입시키는 방식으로, 촘촘하게 시분할만 하는 것보다 실질적으로 훨씬 효율적. 이로인해 온도와 seed 값을 고정해도 서비스 응답이 매번 다를 수 있는데, 그 이유는 각자의 프롬프트가 무엇과 함께 배치되는지 제어할 수 없기 때문. 이 현상이 데이터 탈취 공격 벡터가 될 가능성도 상상
          + 배치의 장점 중 하나는 동일 콘텐츠의 평가를 반복해 보고 실제로 ""hallucination""이 있는지 확인할 때 편리하게 batch-size만큼 던져볼 수 있다는 것. 사실 배치라는 개념은 LLM에 처음부터 있었으나, 시간이 지나야 그 진가를 체감하는 경향
          + 나는 서비스 제공자가 모든 모델에서 항상 배치를 한다고 단순하게 추정했는데, 특정 모델 패밀리에서만 적용되는지 궁금증
          + 다른 프롬프트와 함께 배치된다고 왜 모델 응답에 변동이 생기는지 궁금증
          + 프롬프트가 다른 사람과 함께 묶일 수 있다면, 매우 효과적인 공격 벡터가 될 수 있다는 우려
     * 간결하게 정리하면:<br>- 높은 sparsity 모델은 하나의 행렬 곱 연산을 충분히 연산 집약적으로 만들려면 대형 배치(동시 요청 건수)가 필요<br>- 이 정도 큰 배치를 소화하려면 8~16개 정도 GPU가 있어야 모델 웨이트와 MLA/KV 캐시를 HBM에 맞출 수 있음. 그런데 GPU 8~16대로는 전체 처리량이 낮아서 유저 개개인 응답 속도가 정말 느려짐. 좋아질려면 256개 정도 GPU가 있어야 쾌적한 사용 경험 기대
          + 나는 16대 H100(2노드) 환경에서 Deepseek 서비스 중. 요청당 50~80 토큰/초, 전체적으로 수천 토큰까지 안정적인 처리량 확인. 최초 토큰 생성 시간도 안정적이고, 우리가 쓸 수 있는 어떤 클라우드 서비스보다 빠른 경험
          + sparsity가 높다 = 대용량 배치 필요 라고 말하는데, 그 연결고리가 이해가 잘 안 된다는 의견. sparse matmul이 사실상 0많은 matmul이라고 생각하는 조소
     * LLM 관점에서 훌륭한 해설이라는 감상. 하이퍼스케일 LLM 회사들은 실제 계산 trace를 면밀히 분석해 병목 구간을 찾아내고, 로드 밸런서, 파이프라인 아키텍처, 스케줄러 등으로 워크로드 최적화에 진심일 거라는 예측. 효율성 확보를 위한 ""배치 선행조건""은 보안성 높은 어플리케이션에선 불리할 수 있는데, 쿼리 고립화가 극도로 비싸지는 구조. nVidia의 vGPU 가상화는 GPU 메모리를 시분할로 나누지만, context switch 때마다 언로드/리로드 필요, 중복 제거 없음이 의심. MIG도 메모리를 사용자에 따라 고정 분할하는데, 재조정하려면 GPU 재부팅 필요, 96GB GPU를 4x24GB로 쪼개기 싫은 심정 공감. GPU보드에 2차 메모리(DRAM)를 추가하면 PCIe보다 빠르게 다양한 행렬 데이터를 올릴 수 있을지 상상(HBM을 캐시로).<br>Software Engineering 생존형 실전 가이드북의 솔직한 서술도 호평
     * Deepseek용 소프트웨어 최적화 여지가 아직 많다는 의견. 실제로 소형 GPU와 대용량 RAM 시스템(ktransformers) 또는 엄청난 VRAM을 가진 시스템, 이 두 가지 유형만 접근성 중심 최적화. 192GB VRAM+나머지 일반 메모리가 있는 구조(DGX station, 2xRTX Pro 6000 등)는 MoE의 파워로 Deepseek 4bit도 꽤 빠르게 가능할 것. Deepseek에 중국어 프롬프트가 아니면 대부분의 expert가 활성화되지 않음. 프루닝도 더 쉬운 환경. 앞으로 enthusiast 시스템 방향성이 이런 소프트웨어적 최적화에 어울리는 흐름. Reddit에는 16x3090(Pcie 3.0 x4) 시스템으로 llama.cpp에서 토큰 7개/초 정도 구현 예시도 있고, 3090 한대로도 VRAM 전체를 초당 39번 스캔 가능하니 다른 성능 병목 원인이 있을 듯
          + 16x3090 시스템의 전력 소모는 무려 5KW. 이 정도면 전기세 생각하면 API 사용하는 게 더 저렴. Deepseek에서 중국어 프롬프트 사용 안 하면 활성화 안 되는 expert가 많다는 점에서, 모델 경량화 및 토큰을 더 근접 expert에 라우팅하는 방식 상상
          + MI300x 한 대가 192GB VRAM 제공 가능
     * 나는 ML 연구자도, 엔지니어도 아니라서 감안하고 들어 달라는 단서. Deepseek V3/R1은 기존 로컬 모델보다 워낙 커서 로컬 구동 자체가 비싼게 사실. 활성 파라미터는 전체 사이즈보다 적지만, 이건 연산요건만 줄일 뿐 메모리 요건은 그대로여서, 대부분은 거대한 GPU 없으면 사실상 실사용은 불가능 수준. 주요 프런티어(proprietary) 모델과 직접 비교도 불가, 사이즈 등 정보가 공개되어 있질 않음. 그 모델들이 로컬에서 더 싸게 돌아갈 거란 기대 근거도 없음. 오히려 MoE는 로컬/싱글유저 환경에서 배치의 비효율성이 크지 않으니 더 적합한 트레이드오프 제공. 배치를 키우면 각 토큰 대기지연이 200ms까지 늘지만, 대신 feed-forward 연산(GEMM)이 더 커지니 더 효율적으로 계산 진행. 배치가 커진다고 행렬 자체가 커지는 건지 궁금증. 내 머릿속 모델은, 배치는 “입력
       행렬 크기 증가”가 목적이 아니라 “메모리 대역폭 제약을 연산 병목으로 옮기는게 목적”이랄까. 이미 웨이트는 레이어별로 조각내서 HBM → SRAM으로 불러오고, 조각별로 행렬 곱하고 마지막에 결과 합치는 구조. 배칭하면, 같은 웨이트로 여러 연산을 동시에 처리하는게 장점FLOPS를 효율적으로 최대화. OpenAI, Anthropic 등 대규모 모델이 빠르게 응답하는 것 자체가 사실인지, 블로그글에 time to first token별 수치가 없어 근거 약하다고 생각
          + 나는 원 글 작성자. ML 연구자는 아니고 관심 많은 엔지니어 입장. MoE의 로컬 싱글유저 시나리오는, 멀티유저 배치 이점이 없으니 GPU당 처리량이 극적으로 떨어진다는 뜻. 엄청난 병렬 인퍼런스 요청을 하지 않는 이상 그렇다는 설명. 배치를 통해 입력 행렬 크기를 키우면, 배치가 1이면 연산이 1xdim 행렬이고, 배치가 늘면 batch-size x dim이라 GPU 활용률 급상승, 즉 연산 병목 전환 가능. 마지막, 실제 Deepseek이 다른 모델 대비 느린 것도 많이 써보면 체감
     * mixture of experts는 고배치가 필요한데, 애플 실리콘을 쓰면 배치 사이즈 1일 때도 쓸 만한 것으로 염두. unified memory 덕분에 큰 모델도 로컬 실행 가능하지만 대역폭, FLOPS가 낮아 동작이 상대적으로 느림. MoE는 매번 처리해야 하는 파라미터 개수가 적어서 연산 부담이 낮다는 특성. 실제로 Deepseek을 맥에서 싱글 배치 인퍼런스로 쓸만한 속도로 돌린 사람 경험 다수. 물론 메모리를 충분히 장착하려면 구매 비용이 비싸다는 점은 여전. 맥이나 비슷한 구조의 머신이 앞으로 더 등장하면 MoE 모델과 환상의 궁합이라는 평. 큰 램 업그레이드한 맥에서 dense 모델 돌리는 건 훨씬 고통이라는 비교
     * 동료와 얘기하다가, 프로그래밍 지원용으로 LLM을 쓸 때 모델이 오히려 본질적인 최적화에서 벗어난 방향으로 발전 중이라는 결론. 나는 내부 규정상 거의 모든 작업을 로컬 4~30B 모델과 여러 GPT 시리즈 비교하는데, 특히 GPT-4o는 평균적으로 뛰어난 결과를 내지만, “응답 일부를 허구로 만드는” 성향도 있어 검증과 반복에 상당한 시간 투자 필요. 결과적으로 “저파라미터 로컬 모델과 노력 대비 차이가 생각보다 크지 않다”는 판단. 문제는 둘 다 정말 느려서 빠른 반복이 불가하다는 것. 난 품질 낮아도 응답이 번개처럼 즉각 오는 큰 컨텍스트 모델이 훨씬 편하다고 느낌. 실제 품질 평가 수치 향상 그 이상으로 “체감 반복 속도”가 중요하다는 바람
     * ""느리고 비싸다""는 주장에 반대. DDR4 메모리 기반 구형 워크스테이션에서도 llama.cpp 활용하면 1000달러 내외 시스템으로도 초당 3토큰 뽑을 수 있다는 사례
          + 너가 실제 Deepseek 모델이 아니라 distill된 버전을 혼동하고 쓰는 것일수도 있다는 지적. 192GB RAM 이상이 없으면 진짜 모델 구동은 불가
"
"https://news.hada.io/topic?id=21232","삼각형 메시와 글로벌 일루미네이션을 활용한 Neural Rendering: RenderFormer","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         삼각형 메시와 글로벌 일루미네이션을 활용한 Neural Rendering: RenderFormer

     * RenderFormer는 삼각형 메시 기반 씬에서 글로벌 일루미네이션 효과까지 직접 구현하는 뉴럴 렌더링 파이프라인임
     * 개별 씬 별 별도 학습이나 미세 조정 과정이 필요하지 않음
     * 렌더링을 시퀀스-투-시퀀스 변환으로 정의해, 삼각형 토큰을 픽셀 패치 토큰으로 직접 변환함
     * 트랜스포머 기반으로 전체 파이프라인이 설계되며, 최소한의 사전 제약만 적용됨
     * 래스터화나 레이트레이싱을 쓰지 않고 이미지를 생성함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * RenderFormer는 삼각형 기반 씬 표현에서 직접 이미지를 렌더링하는 뉴럴 파이프라인임
     * 글로벌 일루미네이션 효과가 완전히 적용된 이미지를 출력함
     * 씬마다 별도의 훈련 또는 파인튜닝이 필요 없는 구조로 동작함

접근 방식

     * 기존의 물리 기반 렌더링 방식과는 달리, 렌더링을 시퀀스-투-시퀀스 변환 문제로 재정의함
          + 삼각형 및 반사 특성을 담은 토큰 시퀀스를, 각각 작은 픽셀 패치로 변환된 출력 토큰 시퀀스로 변환함

파이프라인 구조

     * RenderFormer는 2단계 구조로 구성됨
          + 뷰 독립적 단계: 삼각형-간 조명 전달 현상을 모델링함
          + 뷰 종속적 단계: 광선 다발을 나타내는 토큰을 픽셀 값으로 변환함. 이때 앞선 단계의 삼각형 시퀀스가 가이드 역할을 수행함
     * 두 단계 모두 트랜스포머 구조를 바탕으로 함
     * 최소한의 사전 제약만을 부여해 학습함

기술적 특징

     * 렌더링 시 래스터화, 레이트레이싱 등 전통적 방법을 전혀 사용하지 않음
     * 트랜스포머의 시퀀스 변환 능력을 적극적으로 활용함

결론

     * 기존 뉴럴 렌더링 기술 대비, 별도의 사전 준비나 씬별 조정 필요 없이 유연하고 고품질의 이미지를 생성하는 접근 방식임

        Hacker News 의견

     * 가장 인상적인 점은 속도 부분임. 같은 장면에서 RenderFormer는 0.076초면 끝나는데, Blender Cycles는 3.97초(혹은 더 높은 세팅에선 12.05초) 소요임. 그런데 구조적 유사성 지수(SSIM)가 0.9526으로 거의 차이 없는 수준임. 논문에서 2번과 1번 표 참고 권장함. 이게 실제 의미하는 바는 3D 디자이너들이 웹이나 네이티브 앱에서 on-device transformer 모델로 인스턴트 렌더 프리뷰를 훨씬 높은 품질로 볼 수 있다는 것임. 단, 위 결과는 A100 GPU에서 PyTorch 최적화 없이 측정한 값이고 일반 유저 GPU는 이만큼 빠르진 않겠지만, 그래도 기존 렌더링보다 충분한 속도 향상이 가능할 것이라 예상함. 혹은 웹 기반 시스템이면 A100에 백엔드로 연결해서 브라우저로 결과 이미지를 스트리밍하는 방식도 가능함. 다만 한계점도 확실함. 장면이 복잡해질수록 정확도가 떨어지고, 특히 복잡한
       그림자(입자나 머리카락 포함)에서 오차 발생 가능성이 크므로, 최종 렌더는 여전히 전통적인 방법으로 처리해야 AI 기반 이미지/비디오에서 자주 보이는 아티팩트를 피할 수 있음. 그래도 만약 속도 향상이 충분히 크면, 영화 길이 프리뷰 렌더링이 필요한 대형 애니메이션 스튜디오에서 음악이나 스토리 검토 등 용도로 쓸 수 있을 것이라 기대함
          + 연구자들이 의도적으로 사실을 왜곡했다고는 생각하지 않지만, Blender Cycles가 그런 성능의 GPU라면 논문 속 모든 장면을 4초 이내에 렌더링할 수 있을 수준임. 논문에 사용된 장면들 자체가 복잡도가 낮은 데다, Blender를 4,000번 반복 샘플링하게 설정했는데, 실제론 몇백 번 만에 거의 최종 품질에 도달해서 나머지는 거의 효과가 없음. 그 결과 GPU 리소스를 불필요하게 소모하는 셈임. 또 Blender의 초기 렌더 준비 과정을 렌더링 시간에 포함한 반면, transformer 초기화 시간은 제외한 것으로 보임. 각 시스템에서 두 번째 프레임 렌더에 걸리는 시간도 궁금함. 내 예상으론 Blender가 훨씬 더 빠를 것 같음. 어쨌든 논문 결과 자체는 흥미로우나 Blender 설정과 타이밍 비교 부분엔 미묘한 차이가 있음
          + 보여주는 장면들 기준으로 76ms는 오히려 오래 걸리는 편임. 물론 앞으로 훨씬 더 빨라질 거라 생각하지만, 기존 전통적 렌더보다 낫다고 평가하긴 시기상조라는 감상임
          + 논문에서의 타임 비교는 다소 부정확함. 레이트레이싱에서는 샘플 수 제곱근에 따라 에러가 줄어듦. 논문에서는 참조 이미지를 생성할 때 비현실적으로 높은 샘플 수를 사용했는데, 실제 오프라인 렌더러는 이보다 10~100배 적게 샘플링함. 논문처럼 높은 샘플로 만든 이미지는 품질 비교용이지만, 그걸로 시간 비교하는 건 일반적이지 않음. 결과가 엄밀한 것이 아니므로, 비슷하게 근사 값을 내는 다른 렌더링 알고리즘과 비교하는 게 더 공정한 평가임. 요즘 리얼타임 패스 트레이서와 디노이저 조합도 소비자용 GPU로 16ms 이내에 훨씬 복잡한 장면을 렌더할 수 있음. 특히 transformer 모델은 삼각형과 픽셀 수에 대해 둘 다 제곱에 비례하는 시간 소요임. 최신 머신러닝 연구에서 개선된 부분이 있을지 모르겠지만, 전통적 path tracer의 O(log n triangles), O(n pixels)
            스케일링에는 이기기 어려울 것임(실제로는 인접 픽셀 간의 일관성 덕에 픽셀 수 증가에 덜 민감함)
          + 속도가 뛰어나다는 주장에 대해 놀라운 감상임. 논문을 대충 훑어봤는데, Blender Cycles가 A100의 CPU를 쓴 건지 CUDA 커널을 동원한 건지 확인이 어려웠음. 단일 프레임이면 렌더러 시작 시간이 일부 포함됐을 수 있음. 만약 시퀀스 렌더면 프레임 당 소요 시간이 크게 줄어듦. 그리고 다른 분이 언급한 삼각형 복잡도(O(n^2) 스케일링)도 확실히 영향을 줄 것임
          + 논문에서 ""Attention 레이어의 런타임-복잡도는 토큰 개수, 즉 이 경우 삼각형 개수에 비례하여 제곱으로 증가. 그래서 장면 내 삼각형 개수를 최대 4096개로 제한""이라 밝힘
     * 딥러닝은 글로벌 일루미네이션 렌더 이미지의 디노이즈(노이즈 제거)에도 매우 성공적으로 쓰이고 있음. 전통적 레이트레이싱으로 거친 글로벌 일루미네이션 이미지를 뽑고, 신경망이 출력 이미지의 노이즈를 제거해주는 방식임. 관련 링크: Open Image Denoise
          + 데모 출력 이미지가 신기하게 매끄러운 느낌임, 마치 AI 업스케일한 이미지 같은 인상임. 가장자리는 또렷하게 남는데, 원본 데이터보다 크게 늘리려고 할 때 텍스처 정보는 많이 잃는 식임. (추가) 100% 확대에서 디노이즈 비교하면 125% DPI 확대보다 결과가 더 좋게 보이고, 아래쪽 양치식물도 더 잘 식별 가능해짐
     * 영화 산업에서 실제로 물리 기반 렌더러 개발하는 친구가 있는데, 이 업계 작업 방식이나 이야기 들으면 늘 흥미로움. 혹시 지금 이런 인재를 채용 중인 업체들이 어딘지 궁금함. AI 기업들이도 트레이닝용 환경 구축 목적으로 렌더링 엔지니어를 뽑는지 궁금함. 혹시 경험 많은 렌더링 연구/산업 엔지니어 채용 희망하는 분이 있다면, 내 친구가 SNS는 안 하기에 연결해줄 수 있음
          + 친구한테 Gmail로 내 아이디로 메일 보내달라고 얘기해줬으면 하는 바람임
     * 예시 중 어떤 것도 카메라 뒤의 오브젝트를 보여주지 않는 게 의아한 인상임. 이런 예시 구성의 한계인지 아니면 접근 자체의 한계인지는 모르겠지만, 반사나 라이팅 고려할 땐 카메라 뒤쪽이 굉장히 중요한 요소임
     * 또 한 번 ""the bitter lesson""이 실감되는 순간임. 이제 그래픽스 렌더링 분야에서도 이 흐름이 적용되는 것임. Nerf는 레이트레이싱 베이스 프라이어를, Gaussian splat은 래스터화 기반 프라이어를 부분적으로 사용했는데, 이 방식은 그런 도메인 프라이어나 전문적 지식 다 버리고 오직 데이터와 attention 자체만으로 해결을 시도함. 이런 방식이 결국 미래라는 느낌임
     * GPU를 중심으로 렌더링과 컴퓨트가 서로 순환 연결되는 구조가 완성됐다는 점이 인상적임
     * 결과물이 괜찮긴 한데 다소 블러리한 느낌임. 뉴럴 네트워크와 고전적 렌더러간 렌더 타임 비교가 좀 더 있었으면 하는 바람임
          + 애니메이션(특히 Animated Crab과 Robot Animation)에는 AI 아트 특유의 아티팩트가 눈에 띄는데, 오브젝트나 카메라가 움직일 때 모델 주변에 부자연스럽게 소용돌이치는 현상임
          + 논문에서 시간 비교에 대한 논의가 일부 있음. Blender Cycles(패스 트레이싱)와 비교해서, 적어도 4K 삼각형 이하 장면에선 뉴럴 방식이 훨씬 더 빠름. 다만 그 이상 복잡한 장면엔 잘 안 맞을 수 있음(attention 런타임이 삼각형 개수의 제곱으로 증가하므로). 논문 링크: RenderFormer 논문 PDF. 내 생각엔 뉴럴 방식을 간접조명에만 쓰고, 전통적 래스터라이저로 베이스 이미지를 만들고 Global Illumination만 뉴럴로 덧입히는 식도 현실적 방법일 수 있음
     * 혹시 잘 몰라서 그러는데, 이런 장면들은 결국 예상되는 방식대로 렌더된 거라면, 왜 이 방식이 더 간단한 직접적 방법들보다 이점이 있는지 궁금함(더 빠른 것도 아니라면 굳이 쓸 이유가 있나 하는 의문임)
          + 사실 이 방식은 겉보기보다 더 재미난 효과를 낼 수도 있음. 예를 들어 씬을 하나의 input weight 뭉치로 간주해서 거기에 노이즈를 추가하거나, 서로 다른 씬을 interpolate(혼합)해서 예상 밖의 결과물을 얻을 수도 있을 것임
          + 결국 이 방식은 ""Cool Research""에 가깝다고 생각함. 실용성은 낮음, 왜냐하면 삼각형이 많아질수록 비용이 제곱으로 커지기 때문임. 그래서 논문에서도 씬마다 4096개로 제한함
          + 다른 댓글에서 언급됐듯이, 이 방식이 더 빠른 것이긴 함. 글로벌 일루미네이션은 직접적 방법으로는 정말 느림
     * 참신한 연구라는 인상임. 트랜스포머가 자연어 뿐 아니라 여러 연속적인 데이터 입력과 토큰 간 상관관계가 특징인 도메인에 두루 적용될 수 있다는 점에서 앞으로 비텍스트 도메인 적용 연구가 기대됨. Hacker News 유저들은 트랜스포머에 잘 맞을 듯한 비텍스트 도메인으로 어떤 게 더 흥미로운지 궁금함
     * 아주 기발하고 흥미로운 아이디어라는 생각임. 삼각형 집합 기반 씬 디스크립션을 2D 픽셀 어레이로 바꿔주는 트랜스포머를 훈련시켜서, 그 결과가 기존 글로벌 일루미네이션 렌더러의 결과와 거의 비슷한 이미지를 즉석에서 만들어냄. 지난 5년간 연구를 보면 이런 게 가능하다는 사실 자체는 이제 안 놀라워야 하는데도, 여전히 굉장히 인상적임. 트랜스포머 구조가 정말 다재다능하다는 걸 느낌. 속도도 어마어마하게 빠르고, Blender 출력과 거의 비슷하고, 약 1B 파라미터 크기의 모델이고, fp16인지 32인진 불확실하나 파일이 2GB로 상당함. 좀 더 리얼리스틱한 씬 데모도 보고 싶지만, 당장 내 Mac에 다운로드해서 직접 돌려볼 수도 있는 점도 마음에 듦
"
