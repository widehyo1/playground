"https://news.hada.io/topic?id=21681","Google, 리눅스 재단에 에이전트2에이전트 프로토콜 기부","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Google, 리눅스 재단에 에이전트2에이전트 프로토콜 기부

     * 구글은 AI 에이전트들이 서로 대화할 수 있도록 돕기 위해 에이전트2에이전트 프로토콜을 리눅스 재단에 기증
     * 지난 4월, 구글은 클라우드 넥스트 컨퍼런스에서 에이전트2에이전트(A2A) 프로토콜을 발표했습니다.
     * 이 프로토콜은 AI 에이전트가 어떤 프레임워크로 구축되었든 간에 서로 더 쉽게 대화할 수 있도록 하는 것을 목표로 합니다.
     * 덴버에서 열린 오픈 소스 서밋에서 구글은 오늘 이 프로토콜을 리눅스 재단에 기증하고 새로운 GitHub 저장소로 옮겼다고 발표했습니다.
     * 에이전트2에이전트 프로젝트라고 불리는 Google에 합류한 기업 중에는 AWS, Cisco, Salesforce, SAP, ServiceNow 등이 있습니다. 이 프로젝트는 프로토콜 자체는 물론 SDK, NPM 패키지 및 기타 개발자 도구의 미래를 선도할 것입니다.
     * 목표는 에이전트 상호 운용성을 위한 개방형 표준을 수립하고, 그 표준을 중심으로 생태계를 조성하며, 표준 Linux Foundation 프레임워크를 기반으로 중립적인 거버넌스를 보장하는 것입니다.
     * 기조연설에서 A2A에 대한 일반적인 오해, 즉 A2A는 인류의 모델 컨텍스트 프로토콜(MCP)과 경쟁하기 위한 것이 아니라는 점에 대해서도 언급했습니다.
     * MCP는 에이전트 프로토콜의 획기적인 히트작이었지만, MCP의 아이디어는 에이전트를 도구와 데이터 소스에 연결하는 것입니다.
     * A2A는 에이전트를 서로 연결하는 것에 관한 것입니다.

   리눅스 커널을 접수하기 위한 스카이넷의.. 읍...

   MCP보다 별로였나보군요, 바로 짬처리하는거 보니 ㅋㅋ

   A2A 는 MCP 서로 배타적인 관계가 아니라
   상호 보완 관계의 프로토콜입니다. 공부 좀 더 하심이~
"
"https://news.hada.io/topic?id=21685","텐센트 훈위안, 첫 하이브리드 AI 모델 오픈소스화. 유럽 연합(EU), 영국(UK), 그리고 대한민국에서는 사용할 수 없습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                텐센트 훈위안, 첫 하이브리드 AI 모델 오픈소스화. 유럽 연합(EU), 영국(UK), 그리고 대한민국에서는 사용할 수 없습니다.

   라이센스: https://github.com/Tencent-Hunyuan/Hunyuan-A13B/blob/main/LICENSE

   hf: https://huggingface.co/tencent/Hunyuan-A13B-Instruct

   중국 기술 대기업 텐센트의 대규모 모델군인 텐센트 헌위안(Tencent Hunyuan)이 최초의 전문가 혼합(Mixture-of-Experts, MoE) 모델인 헌위안-A13B를 공식 오픈소스로 공개했습니다.
   이 프로젝트는 유럽 연합(EU), 영국(UK), 그리고 대한민국(한국)에서는 사용할 수 없습니다.
   라이선스에 따라 해당 지역에서는 소프트웨어의 사용, 복제, 수정, 배포, 표시 등이 모두 금지되어 있습니다.

   한국 차단 사유 (gpt 작성):
    1. 한국 AI 기본법(2024)으로 해외 모델도 위험평가·라벨링·국내대표 지정 의무가 생김.
    2. 개인정보·콘텐츠 규제 집행이 강해 잠재적 소송‧과징금 리스크가 큼.
    3. 중국발 모델이 최근 한국 스토어에서 차단된 선례(DeepSeek)로 규제 위험이 실증됨.
    4. 텐센트는 규제·컴플라이언스 비용을 피하려고 “한국 사용 금지” 조항을 넣음.
    5. 차단해도 다른 지역에서 생태계를 키우고, 필요 시 별도 계약으로 진출할 수 있음.
    6. 즉, 법적 책임 최소화와 향후 상업적 레버리지 확보가 핵심 이유다.
"
"https://news.hada.io/topic?id=21591","Delta Chat - 탈중앙화 보안 메신저 앱 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Delta Chat - 탈중앙화 보안 메신저 앱

     * Delta Chat은 기존 메신저와 달리 중앙 서버 없이 기존 이메일 시스템을 활용하는 탈중앙화 및 보안 중심 메신저임
     * 사용자는 자신의 이메일 계정을 통해 채팅을 시작할 수 있으며, 별도의 신규 계정 등록 과정이 필요하지 않음
     * 메시지 교환 과정에서 종단간 암호화(E2EE) 기능을 지원하여 송수신자 외에는 누구도 메시지를 볼 수 없음
     * 이미 사용 중인 IMAP/SMTP 이메일 인프라와의 호환성으로 인해 인터넷 연결이 가능한 어느 환경에서도 사용 가능함
     * 오픈 소스 라이선스 기반으로 개발되어 개발자 및 서드파티가 자유롭게 기능 확장 및 통합 가능함

주요 특징

     * Delta Chat 앱은 Android, iOS, Windows, MacOS, Linux 등 광범위한 플랫폼을 지원함
     * 이메일 주소만으로 메신저 기능을 제공하며, 누적 가입자 없이도 즉시 대화 시작이 가능함
     * PGP 암호화를 통해 사용자의 프라이버시와 보안을 유지함
     * 필요 시 기존 이메일 사용자와도 호환 사용이 가능하므로, Delta Chat 앱이 없는 상대와도 메일 교환 가능함
     * 탈중앙화 구조이기 때문에 단일 서비스 장애 또는 차단 시 위험성이 매우 낮음

활용 예시

     * 민감한 정보 교환이 필요한 기업 또는 개인 사용자에게 적합함
     * 검열이나 감시 위험이 있는 환경에서도 특정 서버 의존성 없이 통신 보장 가능함
     * 개발자는 Delta Chat 오픈 소스 리포지토리를 통해 커스터마이즈 또는 서드파티 애드온 개발이 용이함

결론

     * Delta Chat은 기존 메신저와는 달리 이메일 인프라를 바탕으로 한 보안 중심의 탈중앙화 메시징 솔루션임
     * 이메일 계정만 있으면 즉시 사용할 수 있으며, 종단간 암호화와 오픈 소스 구조로 확장성, 신뢰성, 프라이버시 이점을 제공함

        Hacker News 의견

     * 이메일 시스템이 분산화되어 있고 '보안'이 강조된다고 해도 익명성은 보장받을 수 없는 환경임을 지적함. IMAP와 SMTP 자체가 익명성을 고려해 설계된 것이 아니기 때문임. 델타챗(Delta Chat)은 메시지의 내용은 보호할 수 있지만, 본인의 신원이나 소셜 그래프, 특정인과 대화한 사실을 은폐해야 할 때 적합하지 않다고 생각함. 이메일 인프라가 분산화되어 있다고 해도 그 수준에 그친다는 점을 강조함
          + 이러한 점에서 한 발 더 나아가 델타챗이 아예 안전하지 않다는 의견도 제시함. 현대 메시징 앱의 기본인 전달 보안(Forward Secrecy)과 메타데이터 프라이버시가 델타챗에는 없다는 점을 지적함
          + 이메일의 분산화 수준이 이미 일상에서 사용하는 대부분의 웹 서비스보다 훨씬 더 분산화되어 있다는 의견을 공유함
          + 서비스 제공업체로부터 특정인과 대화한 사실조차 숨길 수 없는 구조임을 강조하고, 이런 점에서 서비스 제공업체를 선택할 권한을 갖는 것이 중요함을 언급함
          + 이메일 인프라가 완전히 분산화되어 있는 것 아니냐는 반문을 던지며, 어떤 점이 빠졌는지 궁금하다고 표시함
     * 과거 델타챗 관련 논의들이 모여 있는 링크 모음 제공함
          + 2025-03-05 논의 (100개 댓글)
          + 2021-01-24 논의 (148개 댓글)
          + 2021-01-07 논의 (4개 댓글)
          + 2019-02-27 논의 (11개 댓글)
          + 2019-02-21 논의 (56개 댓글)
          + 2017-02-03 논의 (1개 댓글)
          + 유용한 정보 소스로 생각함. 왜 이 의견이 다운보트 당하는지 궁금함
     * 델타챗은 퍼펙트 포워드 시크리시(PFS)를 지원하지 않는다는 점을 직접적으로 언급함. 개인 키가 유출될 경우, 이전 전송 메시지도 복호화 가능한 구조임을 경고함. 관련 FAQ 연결 제공. 투명하게 그 한계를 공개하는 점은 긍정적으로 평가하지만 Signal 프로토콜처럼 더 안전한 옵션이 있으니 굳이 열등한 앱을 쓸 이유가 없다는 견해임
          + 이 점이 지나친 평가일 수 있다는 생각을 덧붙임. 델타챗이 단순한 메신저였다면 혹평이 맞겠지만, 이메일 기반이라는 점에서 레질리언스가 특별함. Signal을 차단하는 것보다 이메일을 차단하는 것이 훨씬 어렵다는 강점을 강조함
          + 최근 XMPP 클라이언트들이 Signal 프로토콜 암호화를 구현하며 델타챗처럼 분산화된 구조를 갖추고 있다는 사례를 들며 대안을 제시함
          + 전달 보안(Forward Secrecy)도 논의 중임을 소개함. 토론 링크
     * 델타챗은 이메일 호환이며 암호화 방식으로 pgp를 사용함. 포워드 시크리시 부재, 상대방이 pgp를 쓸 수 없으면 암호화 없이 전송되는 구조임. 상대에게서 암호화되지 않은 이메일을 받으면 자동으로 평문 전송으로 전환되는 점도 지적함. 만약 공격자가 상대방을 가장해 암호화되지 않은 이메일을 보내면, 델타챗이 해당 메시지를 거부할지 혹은 조용히 암호화 없는 대화로 바꿀지 궁금함
          + 대화 자체를 암호화 상태로 보장하려면 두 사용자가 암호화 그룹 채팅을 만드는 방식이 있다는 안내를 공유함. 공식 FAQ도 첨부함
          + 프로토콜 명세를 살펴봤냐고 되묻고, 혹시 최신 openpgp 표준에 따라 인증된 암호화(authenticated encryption) 방식을 도입했는지, 아니면 모든 메시지를 단순 서명만 하는지 등 세부 구현에 대한 의문을 제기함. 또한 델타챗을 단순 이메일+pgp 조합이 아니라 일종의 오버레이 시스템으로 보는 시각도 피력함
          + 포워드 시크리시가 없고 pgp 미지원 상대에겐 암호화 없이 메시지가 전송되는 구조 자체를 강하게 비판함. Signal이 SMS 지원을 중단한 이유도 여기에 있다는 것. 미친 설계 결정이라고 평함
     * 0xchat이라는 대안 제시함. 개인키 로그인, 비공개 채팅과 연락처, 암호화 그룹 채팅, 라이트닝 결제 지원 등 다양한 기능 나열함. 완전 분산화, Nostr 기반, 모든 플랫폼에서 사용 가능하다고 안내함. 공식 사이트 링크 첨부
          + GitHub에서 데스크탑 클라이언트 부재를 확인함. 또, 다이렉트 메시지(DM) 기능이 세가지 버전이 있는 점에 주목함: 1) 가장 널리 사용되지만 비추천되는 NIP-04, 2) 포워드 시크리시 없는 Gift-Wrapped DM. 이건 20년 전 기술이라는 평, 3) 다른 디바이스에서 복구 불가한 Secret DM인데 Signal처럼 백업 지원이 없는 점 아쉬움 표현함. 게다가 Secret chat는 상대 동의가 필요해 처음에는 비보안 프로토콜로 대화가 유도된다는 우려도 있음. 여러 DM 프로토콜을 한 앱에 넣은 점이 매우 이례적이고, 자체적으로 보안을 바로 업그레이드할 수 있다는 점은 장점으로 봄
          + 중요한 점은 모두가 이메일을 갖고 있다는 자체라고 봄. Nostr 기반의 채팅 클라이언트도 매력적이지만 이메일만큼 범용성이 없다는 입장이 있음
          + Nostr의 특성상 특정인에게 메시지 전달 사실이 블록체인에 노출되는 것 아니냐는 불안감을 표현함
          + 0xchat이 보기에는 더 나아보이는 구조와 개발자와의 소통 채널, 프로페셔널리즘이 인상적이지만 보안 감사가 이뤄졌는지 궁금함
          + 공식 웹사이트가 모바일에서 제대로 렌더링되지 않는다는 점도 지적함
     * 채트메일 서버를 옮길 때 기존 대화방을 유지할 수 있는지, 아니면 새로 만들어야 하고 이전 기록은 그대로 남는지 궁금함. WhatsApp도 그렇지만 매우 불편한 지점이라고 느낀다고 전달함
     * 델타챗은 인프라 부트스트래핑 단계를 완전히 생략하는 구조임을 강조함. 별도의 서버, 연합체, 클라이언트 잠금 필요 없이 IMAP+SMTP라는 모두가 이미 갖춘 인프라를 백엔드로 씀. 채택의 문제가 아니라 UX로 과제가 옮겨진다는 점이 매우 신선하다고 느끼며, 직접 이 프로젝트에 공헌하고 싶다고 밝힘
     * 델타챗(Delta Chat) 이름을 듣자마자 바로 Delta Airlines가 떠올랐다는 유쾌한 연상 공유. 좌석 간 채팅 기능에 대한 분석을 기대했다는 점에서 흥미를 드러냄
          + 'seat29@flight7822.delta.com이 seat34@flight7822.delta.com에게 큰 송어로 내리친다 :: 코 고는거 그만' 같은 농담을 섞은 예시로, 좌석 간 채팅이 구현된다면 일어날 풍경을 상상함
          + 길에서 어깨를 맞대고 무리를 지어 다니며 떠드는 소녀들을 연상하는 장면으로 비유하며 유쾌하게 대화를 이어감
     * 소수 사용자 기반 대체 메시징 앱의 스팸 방지 메커니즘이 궁금함. Meta의 독점이 마음에 안 들지만 스팸 방어 능력만큼은 압도적이라고 평가함. 휴대폰 번호 요구가 중요한 전략 중 하나인지도 궁금함
          + 델타챗은 그저 이메일+GPG로 동작하므로 평소 받는 이메일 수준의 스팸만 받을 것이라 생각함. 오히려 스팸을 너무 걱정할 필요 없고, 모든 현대 IM의 푸시 알림 자체가 몇몇 이메일 스팸보다 훨씬 스트레스라고 느낌. 본인은 개인 이메일에서 스팸을 전혀 차단하지 않고, 도구로 라벨링만 자동화함. 차라리 뭐든 다 받는 쪽이 낫다는 입장임
          + 소수 사용자가 있는 대체형 채팅앱은 충분히 사용자 수가 늘어나기 전까지는 스팸 타겟이 되지 않는 구조라고 판단함. 우선순위에서 밀릴 만한 기능이라고 봄
          + 스팸 문제는 디자인적으로 풀 수 있다고 봄. 1. Hey나 Apple처럼 상대를 직접 승인하는 구조, Hey의 스팸 필터 기능, Apple의 메시지 알 수 없는 발신자 차단 기능 2. 기본 필터로 광고성 메시지를 차단하거나 분리, 오픈소스 스팸 차단 앱 3. 봇 같은 행동을 하는 발신자 속도 제한 등 다양한 대응책을 제안함
          + deltachat은 일반 이메일과 deltachat 메시지를 구분함. ‘deltachat 메시지만 받기’ 등으로 컨트롤 가능하다고 안내함
          + 보안이 중요하다면 결국 대면 신뢰 기반이나 최소한 프록시를 통해 신원 확인이 핵심이라고 봄. 접촉 초대는 일회용 코드나 쉽게 소각 가능한 방법으로 처리 가능하다는 의견도 제안함
     * 델타챗이 Protonmail 계정과 연동되는지 묻는 경험 공유. Proton이 자체 채팅앱 구축에 델타챗을 참조하면 어떨까 하는 아이디어 제시함
          + 실제로 DeltaChat과 ProtonMail은 호환되지 않는다는 정보 제공함
"
"https://news.hada.io/topic?id=21692","미국 대법원, 연방 판사의 트럼프 명령 차단 권한 제한","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     미국 대법원, 연방 판사의 트럼프 명령 차단 권한 제한

     * 미국 대법원이 대통령 행정명령에 대한 전국 단위 연방법원 금지명령 발동 권한을 제한함
     * 이번 판결로 향후 하위법원의 금지명령은 소송을 제기한 원고에게만 적용됨
     * 출생시민권을 둘러싼 트럼프 행정명령의 운명은 여전히 미정이며, 대법원은 이 사안의 위헌 여부에 대해 결론을 내리지 않음
     * 이민 단체와 시민단체들은 전국적인 보호를 받기 위해 집단소송 등 새 전략을 모색하고 있음
     * 진보 성향 대법관 및 시민단체는 이번 판결이 법치주의 약화와 헌법 위반 행정부 정책 시행 위험성 제기함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 대법원의 판결 및 주요 쟁점

     * 미국 대법원은 트럼프 행정부의 출생시민권 금지 행정명령 집행을 저지해온 하위 연방법원의 금지명령 권한을 제한하는 판결을 내림
     * 이번 결정은 미 연방 법원이 대통령 권한을 제한할 수 있는 방식이 근본적으로 변화됨을 의미함
     * 과거에는 1,000명 이상의 연방지방법원 판사가 전국적인 범위의 금지명령을 내려 미국 전역에서 정부 정책을 중지시킬 수 있었음
     * 앞으로는 법원의 명령이 해당 소송의 원고(주, 비영리단체 등)에만 적용됨
     * 출생시민권 박탈이 위헌인지에 대한 판결은 이번에 이루어지지 않았으며, 트럼프 행정명령의 미래도 불확실함

이민 관련 단체의 대응 전략

     * 판결의 영향을 최소화하기 위해 이민 지원 단체들은 신속히 법적 전략을 재정비하고 있음
     * Casa, Asylum Seeker Advocacy Project(ASAP) 등은 메릴랜드 연방법원에 긴급 금지명령을 요청함
     * 이들은 또, 출생시민권 정책에 도전하는 더 넓은 집단소송을 재제기하며 전국 모든 임산부와 미등록 이주 가정의 신생아를 보호하려는 시도 진행 중임
     * Casa 단체 관계자는 “힘든 시기지만, 우리는 무력하지 않으며 싸우면 승리해왔음”이라 강조함

대법원 판결 세부 내용 및 각 측의 입장

     * 9인 대법관 중 6 대 3으로 트럼프 행정부 손을 들어주었음
     * 보수 성향 다수 의견은 “전국 단위 금지명령은 의회가 부여한 연방 법원의 권한을 넘어설 가능성이 높다”며, “필요 이상의 범위로 원고를 보호하는 금지명령만 부분적으로 중지”한다는 입장임
     * 판결문 집필자인 Amy Coney Barrett 대법관은 트럼프 행정명령이 즉시 효력을 발휘하지 못하며, 정책의 합헌성도 판단하지 않았음을 명확히 명시함
     * 행정명령은 금요일 판결 후 30일이 지나야 발효될 수 있음
     * 트럼프는 이번 판결이 행정권 제한을 줄이고 더 많은 정책 수행이 가능해졌다며 환영 의사를 밝힘

진보 성향 대법관 및 시민단체의 반응

     * Ketanji Brown Jackson 대법관은 다수의견에 비판적으로 “법원의 전국적인 구제력 제한은 위헌적 정책이 원고로 나서지 않은 이들에게도 적용될 수 있게 한다”며 법치주의 위기 제기함
     * Sonia Sotomayor 대법관 역시 “법치주의에 대한 비극”이라 비난함
     * ACLU 등 시민단체는 “이번 판결로 행정부가 대다수 신생아의 출생시민권 자동 부여를 부분적으로 집행할 수 있는 길을 열었다”며 우려 표명함

역사적·사회적 배경 및 미래 전망

     * 출생시민권은 1868년 남북전쟁 이후 미국 수정헌법 14조를 통해 확립되어 흑인의 시민권을 보장했던 판례임
     * 1898년 Wong Kim Ark 사건 이후 법적 원칙으로 자리잡음
     * 이번 판결로 이민 가정, 임산부 등은 불안감과 불확실성이 더욱 심화됨
     * 일부 원고는 전국적 금지명령으로 보호를 받았으나, 행정명령 효력에 대한 우려가 여전함
     * 민주당계 법무장관들은 “출생시민권은 여전히 미국의 기본법”임을 강조하며, 향후 보호 방안을 모색한다는 의지를 밝힘

트럼프 행정명령의 쟁점 및 소송 핵심

     * 2024년 1월 트럼프 행정명령은 미등록 이주자 부모 사이에서 태어난 아이의 시민권 부여 금지를 목표로 수정헌법 14조의 “미국 태생자 = 시민” 원칙과 충돌함
     * Trump v Casa Inc 사건은 주로 사법부의 전국적 금지명령 권한 쟁탈전이었으며, 백악관 측은 금지명령 적용 범위를 원고에 한정해야 한다고 주장함
     * 3명의 판사가 트럼프 행정명령 서명 당일 전국적 금지명령을 내린 바 있음
     * 이번 정책은 미등록 이주자뿐 아니라 합법 비자 소지자 자녀도 영향을 받는다는 비판이 제기됨

        Hacker News 의견

     * 대부분의 사람들은 정부가 패소하면 꼭 항소할 것이라고 너무 쉽게 믿는 경향이 있음
          + 전략적으로 정부는 100만 명에게 영향을 미치는 정책을 시행하고 피소된 다음 패소하더라도, 원고들에게만 구제 조치를 제공하고 항소는 하지 않는 선택이 가능
          + 상급 법원에서는 중요한 판례를 남기지 못하고, 하급 법원은 비원고들에게 구제를 확대할 수 없고, 결국 대부분의 사람들에게는 위법 정책이 그대로 적용된다는 결과
          + 이런 걱정은 반대 의견에서도 언급된 내용
               o 여기 절차상으로는 집단 소송을 통해 전국적인 금지 명령을 얻는 방법이 있고, 판결에서 그 가능성이 명시돼 있음
               o 그래서 오늘 많은 사람들이 소장을 수정해서 실제로 그렇게 집단 소송을 시도 중
          + 나는 법률 전문가는 아니지만, 하급 법원의 판결도 일종의 전례로 남는 것 아니냐는 생각
               o 정부를 A 사안으로 소송해서 원고가 승소하면, 이후 같은 사안으로 정부를 소송할 때 다음 원고가 더 쉽게 이길 수 있는데, 오히려 모든 게 더 비효율적으로 보이는 상황
          + 현재 상황에서 엘살바도르로 사람을 보내는 사례가 있으니, ICE 바운티 헌터가 매사추세츠에서 사람을 잡아다가 카롤라이나의 Home Depot 주차장에 그냥 내려놓는 것도 이상하지 않은 현실
          + 나는 변호사가 아니지만, 집단 소송 방식으로 이 문제를 해결할 수 있는 가능성이 궁금한 상황
     * 이런 판결은 필연적 변화라는 생각
          + 기존 체계에서는 700명이나 되는 지방법원 판사가 주관적으로 대통령의 권한 행사를 잠정적으로 막을 수 있었음
          + 심지어 국가 안보 사안도 포함됐고, 이게 행정부의 정상적 기능을 깨는 원인이었음
          + 이번 판결이 권력 분립의 정상적 균형을 되찾는 계기라고 느낌
          + 실제로는 700명 지방법원 판사들이 대통령이 헌법상 권한을 '초월'하는 경우, 본인들이 해석한 법률 기준으로 이를 잠정적으로 막을 수 있었던 것임
               o 미국 헌법 구조상 집행권의 한계를 최종적으로 결정하는 건 대통령이 아니라 사법부인 현실
               o 국가 안보 긴급 사안이면 대통령이 즉시 항고를 하면 되고, 정말 초긴급하면 대통령이 명령을 어길 수도 있음
               o 사실상 이 제도는 정부가 비상 상황에서 권력 남용을 막을 수 있는 몇 안 되는 안전장치였다는 생각
          + 실제 국가 안보 이슈가 잠시 중단되는 것보다, 대통령의 권위주의적 행보(어느 대통령이든)가 더 우려되는 상황
               o 의회의 무기력함을 고려했을 때, 행정부의 견제 수단이 무엇이 남았는지 의문
          + 이번 사안에서는 대통령이 미국 태생 시민의 시민권을 무효로 할 헌법적 권한이 없음
               o 정작 행정부가 국가 시스템을 망가뜨린 쪽이라는 입장
          + 이 논리는 행정명령이 기본값이 되는 현실을 상정하는 것으로, 미국 정부 체계의 본래 설계와 어긋남
               o 대통령이 행정명령으로 통치하는 것이 일상화되면 법원이 자주 이를 막아야 하는데, 본래는 그런 일이 없어야 안정적인 권력분립
               o 이번 변화를 계기로 행정부 권력이 더 집약되는 길이 열렸고, 이미 충분히 강한 대통령 중심 체계가 더 권위주의적으로 변할 위험
          + 대통령과 연방정부가 법 위에 군림하는 것을 정상화하자는 주장이 아닌지 의문
     * 이번 판결은 1933년의 수권법(Enabling Act of 1933)과 기능적으로 비슷하다는 생각
          + 관련 링크
          + 어떻게 비슷한지 구체적 논리를 듣고 싶음
               o 기사에 따르면 꽤 다르다는 느낌
               o 법 해석과 심사는 여전히 법원이 권한을 갖고 있고 트럼프가 의회의 입법 권력을 무력화시키지 못하는 구조
          + 오싹할 만큼 입법 절차 없이 이런 변화가 왔다는 느낌
     * 많은 제도가 모두가 적당히 규칙을 지키는 척하며 유지되는 점
          + 사실 이런 시스템은 매우 취약하며, 의지가 확고한 소수가 수십 년간 쌓아온 관례와 시스템을 완전히 무너뜨릴 수 있음
          + 최근에는 우파 쪽에서 이런 파괴가 시작됐고, 곧 좌파에서도 비슷한 일이 벌어질 가능성
          + 이 얘기를 들으니 이런 인용문이 떠오름
               o “헌법이 진짜로 그런 것인지 아닌지는 몰라도, 이런 정부를 허용했거나, 막지 못한 책임 이 둘 중 하나임. 어느 쪽이든 존재할 자격이 없음”
               o 튼튼한 시스템은 선의에만 기대지 않고 인센티브 정렬 구조가 필수
               o 250년이면 꽤 선방한 셈이고, 다음엔 인센티브 문제를 더 잘 고치는 헌법 필요
          + 실제로 그 강경 소수는 이미 100년 전 또는 더 오래 전에 나타났을 가능성
          + 모든 상황에 대비하는 룰 시스템은 효율이 떨어지는 대가가 항상 따름
               o 끝없이 감시체계를 두면 아무 일도 못 하고, 그렇지 않으면 항상 제한이 존재
     * Supreme Court 판결문 링크
          + 소토마요르 대법관의 반대 의견 발췌 “정부는 이 시민권 명령(합법성 스스로도 변호하지 못함)을 소송 원고들에게만 적용 중단하고, 나머지 모든 사람에게는 계속 적용하겠다고 주장 중”
          + 만약 이게 가능하다면, 모든 사람(또는 미국 태생 모든 사람)이 원고가 되는 집단 소송으로 이 문제를 고칠 수 안될지 궁금
          + 집단 소송으로 클래스 전체에 대해 해결 가능하지만, 법원이 그런 절차를 어렵게 만든 것도 현실
               o 클래스 소송 필요 조건은 법원이 “이건 클래스가 아님”이라며 거부할 수 있는 추가 기회가 되고, 대표적 예로 [WalMart v Dukes 사건]과 같은 유명 판례가 있음
          + 수십 년간 보수적 의회와 대법원의 결정, 그리고 ‘Class Action Fairness Act’ 등으로 사실상 집단 소송 자체가 거의 불가능해진 상황
          + 이 경우에는 집단 소송 자체가 꼭 필요하지 않다는 견해
               o 한 건의 사건만 대법원(SCOTUS)까지 올라가면, 14차 수정헌법 해석에 대한 의견이 나올 것
               o 4-5 또는 5-4 팽팽한 판결이 가능하다고 보며, 3-6 또는 6-3 결과는 상대적으로 적게 예상
          + 메릴랜드의 CASA Inc.가 실제로 집단 소송 형태로 소를 다시 제기했고, 더 넓은 금지 명령을 요구한 상태
               o 결과를 두고 볼 예정
          + 집단 소송이란 결국 투표 행위 자체와 다름없다는 해석
               o 미국 시민들은 공동체를 바꿀 모든 도구가 있지만, 실제로 정치에 관심이 없거나 교육·참여 의사 자체가 없는 사람이 대다수
               o 법적 행동이 필요해진다고 해서 갑자기 대규모 참여가 일어날 가능성은 낮고, 촉매가 이미 수없이 있었음에도 변화 없었던 점을 들어, 극단적인 위기(로봇이 인간을 절단하는 수준)가 발생해야 변화가 생길 듯하지만, 그마저도 의문
               o 현장에서 관찰하면, 사람들이 너무 쉽게 체념하고 행동보다 그냥 당하는 쪽을 택하는 현실 인식
     * 트럼프 vs 법관이란 프레임을 받아들이면, 결국 트럼프 vs 법이 아니라는 관점이 됨
          + 이 프레임을 그대로 수용하면 그 파생효과들도 무비판적으로 받아들이게 되며, 이런 방식이 바로 ‘동의의 생산’ 기제
          + 이번 판결은 연방판사가 트럼프를 막는 게 아니라는 맞춤형 논리를 보여주는데, 이 논리 수용 자체가 법이 경계선이 아닌 ‘무기’라는 인식으로 이어짐
          + 법 자체가 계량 가능한 진리가 아니라, 판사의 개인 해석에 따른 임의적 존재가 될 위험
          + ‘판사가 트럼프를 막는다’는 식의 관점은 법치주의를 해치고, 임의적 정부는 곧 권위주의로 연결
          + “법원 결정으로 행정부가 아직 소송하지 않은 사람들에게 헌법 위반이 허용되는 상황은 법치주의의 실존적 위협”이라는 Jackson 판사 의견 인용
          + 이 느낌이 Citizens United 판결에서 소수 의견이 점점 현실화되는 것처럼, “법이 사고팔린다고 믿게 되면 민주주의는 제대로 작동하지 않는다”라는 문구가 점점 더 사실로 다가오는 현실
          + 어느 정도로든, 법 집행과 해석에는 주관성이 따른다는 점 지적
               o “사람을 정해 주면 죄목을 마련하겠다”는 유명한 말이 떠오르는 상황
               o 관련 링크
               o 실제로 미국 법체계에서 부유층이 변호사 군단을 동원해 법적 허점을 찾는 것이 관행
               o 이런 현실에 동의하진 않지만, 지금 벌어지고 있는 일이 인식 확산 계기가 되길 바람
               o 예를 들어, 9/11 이후 만들어진 Patriot Act로 국경 근처 100마일 이내 내에서 법집행기관에 매우 강한 권한이 부여됨
               o 국제공항을 국경 포함으로 보면 미국에는 사실상 경계선이 굉장히 많아지는 셈
     * 이제 각 주마다 소송을 시작해야 하는 상황이라는 생각
          + 실제로는 주별이 아니라, 각 ‘연방 사법구’마다 소송이 필요함
               o 작은 주는 하나이고 큰 주(예: 캘리포니아)는 여러 곳에 사법구 존재
          + 두 가지가 필요함: 1) 모든 연방 항소순회구에서 소 제기, 2) 더 타당한 금지명령의 신청
               o 매일 많은 신생아가 태어나 소송 요건 충족이 어렵지 않은 상황
               o 그런데 미국에선 시민권 증서 자체를 발급하지 않으니, 사회보장번호(SSN)를 신청해서 거부당하면 즉각적 피해 입증 가능
               o 그리고 만약 그 아기가 법적으로 불법이라면, 추방이 가능한지 여부도 궁금해지는 문제
     * “They Thought They Were Free”에서 발췌한 문장 공유
          + 1933년 이후, 정부와 국민 사이 갭이 점점 커지며, 결정은 비공개로 이뤄지고 국민에게 복잡함이나 국가 안보 명분을 들어 정보가 공개되지 않는 방식으로 익숙해졌다는 서술
          + 이런 변화가 국민-정부 분리, 정부가 멀어지는 과정을 눈치채지 못하는 가운데 차근차근 굳어졌다는 현실 인식
          + (관련 링크: press.uchicago.edu)
          + 결론적으로 이 판결로 지역 단위에서 헌법질서 보호를 위한 견제기능을 가진 사람들이 수천 명에서 단 9명(대법관)으로 좁아진 상황
          + 권력 구도 변화만 놓고 보면, 더 적은 수의 인원이 더 많은 권력을 갖게 된 현실
     * 대법원이 정부의 명백한 불법 행위의 실체에는 판결을 하지 않고, 절차적 기제에만 집중하는 모습을 지적
          + 영국에는 상급법원의 금지명령 전통이 없다는 논리만 반복하며, 미국의 현실, 정부 권한 규모, 바이든 행정부 시기 비슷한 명령들이 있었음에도 주목하지 않는 상황
"
"https://news.hada.io/topic?id=21573","Apple의 새로운 Speech API, Whisper보다 월등히 빠른 실시간 음성 텍스트 변환 제공","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Apple의 새로운 Speech API, Whisper보다 월등히 빠른 실시간 음성 텍스트 변환 제공

     * 애플의 SpeechAnalyzer와 SpeechTranscriber는 OpenAI의 Whisper 대비 월등한 속도와 동일한 품질로 실시간 음성 텍스트 변환을 지원함
     * 실제 34분 분량의 7GB 비디오 파일을 Yap 커맨드라인 툴로 변환 시 45초 만에 SRT 파일로 변환, MacWhisper 대비 2.2배 빠른 결과임
     * MacWhisper, VidCap 등 타 도구와 품질 차이는 거의 없으나, 모두 고유명사 및 합성어 처리에서는 약간의 오류를 보임
     * 장시간 개발자 영상, 강의, 팟캐스트 등 반복 작업 시 누적 시간 절감 효과가 매우 큼
     * macOS Tahoe 베타(개발자 계정 필요)에서 Yap 설치 후 바로 사용 가능, Apple 플랫폼 전체(아이폰, 아이패드, Mac, Vision Pro)에서 향후 Whisper 대체 기대
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Apple Speech API vs Whisper: 새로운 속도 혁신

     * 최근 WWDC에서 공개된 SpeechAnalyzer 및 SpeechTranscriber는 macOS, iOS, iPadOS, Vision Pro의 최신 베타에 포함됨
     * 필자는 기존 Whisper 기반 도구의 느린 속도에 불만이 많았으나, 새로운 API는 실제 사용에서 게임 체인저급 성능을 보임
     * 단순 커맨드라인 툴(Yap)로 오디오/비디오 파일을 SRT, TXT로 빠르게 변환 가능
     * 34분, 7GB 4K 영상 → Yap: 45초 / MacWhisper(V3 Turbo): 1분 41초 / VidCap: 1분 55초 / MacWhisper(V2): 3분 55초
     * CamelCase(예: AppStories)와 고유명사 인식 문제는 모두에서 비슷하게 나타남(후처리로 쉽게 교정 가능)

실제 속도 비교 및 워크플로우 활용

     * 단일 영상만 보면 1~2분 차이가 적어 보이지만, 여러 시간 분량 영상 처리 시 누적 시간 절감 효과가 큼
     * YouTube 영상 등 대량 일괄 변환 작업 시 yt-dlp 등과 연계해 효율적 자동화 가능
     * 제작자/유튜버/학생 등 다양한 사용자에게 자막·강의·요약 등 빠른 워크플로우 제공
     * SpeechAnalyzer/SpeechTranscriber 조합이 Whisper를 빠르게 대체할 것으로 기대

실제 적용 및 설치 방법

     * macOS Tahoe 베타(현재는 개발자 계정 필요) 설치
     * Yap 깃허브 저장소에서 커맨드라인 툴 다운로드 및 설치
     * Yap 실행 후 오디오/비디오 파일 입력 → SRT/TXT 변환 파일 바로 생성
     * Apple 공식 Speech API 문서 및 WWDC 영상(277번)에서 추가 기술 정보 확인 가능

결론 및 전망

     * Apple Speech API는 Whisper 대비 속도에서 압도적 우위를 보이면서 품질도 동일 수준 유지
     * Apple 플랫폼에서 음성 인식/변환 워크플로우를 주로 사용하는 사용자에게 표준 모델로 자리잡을 가능성이 높음
     * 자주 반복되는 자동화 작업에서 누적 효율성 극대화 및 업무 생산성 증대 효과 기대

   나중에 한국어도 한번 테스트 해봐야겠네요.

   애플 플랫폼이라는 거 자체가 폐쇄 마인드라서 손이 안감

   반말 댓글 보기 싫은데 차단이 없네

   그렇게 따지면 님이 다신 댓글도 반말인데요;;

   애플을 까는 게 본인 맘에 안 들 순 있어도 저게 반말은 아니죠.
   저건 음슴체고, 긱뉴스 기본 요약 자체가 음슴체인데 글들은 어떻게 보고 계신 건지...

   프로 불편러...
"
"https://news.hada.io/topic?id=21641","판사, ChatGPT 사용자 모두에게 해를 끼치는 '대규모 감시 프로그램' 주장 기각","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            판사, ChatGPT 사용자 모두에게 해를 끼치는 '대규모 감시 프로그램' 주장 기각

     * 미국 법원이 OpenAI에게 모든 ChatGPT 로그(삭제된 채팅 포함) 무기한 보관을 명령하자, 두 명의 사용자가 '대규모 감시'를 주장하며 개입 시도했으나 기각됨
     * 한 사용자는 민감한 개인정보와 상업 정보를 입력한 경험을 들어, 이 명령이 미국 전역의 대규모 감시 프로그램에 해당한다고 주장함
     * 판사는 단순 증거 보존 명령은 감시 프로그램이 아니며, 사법부는 수사기관이 아니라는 점을 분명히 밝힘
     * 디지털 권리 단체(EFF)도 사용자 프라이버시 위험과 향후 악용 가능성에 대해 우려를 표명함
     * 사용자들은 OpenAI의 강한 대응 의지와 향후 데이터 공개 또는 투명성 확보에 대한 불안을 보이고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경 및 법원의 명령

     * 최근 법원은 OpenAI에 대해 ChatGPT의 모든 로그, 삭제된 채팅까지 무기한 보관하라는 명령을 내림
     * 이 조치는 뉴스 미디어 기업이 제기한 저작권 소송의 증거 확보를 위해 내려진 것임
     * 일부 사용자는 이 명령이 삭제·익명 채팅까지 포함되며, 이용자에게 고지 없이 시행되고 있다며 우려를 표함

사용자 개입 시도와 프라이버시 논란

     * 한 사용자는 자신이 민감한 개인 및 상업 정보를 입력했다는 점을 들어, 명령이 미전역 대규모 감시로 이어진다고 주장하며 개입을 요청함
     * 그는 사전 고지 없이 삭제·익명 채팅이 보관되는 점을 문제 삼았으며, 의료·금융·법률 등 민감 주제 채팅에 대한 예외 적용을 요구함
     * 법원은 ""문서 보존 명령은 감시와 무관하며, 법원은 수사기관이 아니다""라고 기각 사유를 명확히 함

디지털 권리 단체의 시각

     * Electronic Frontier Foundation(EFF) 법률 책임자 Corynne McSherry는 ""이번 명령이 사용자 프라이버시에 실질적 위험을 주고, 유사 소송에 악영향을 줄 수 있다""고 우려함
     * 향후 법 집행기관이나 소송 당사자가 OpenAI에 사용자 기록 제공을 요구하는 전례가 될 수 있음을 지적함
     * 특히 사용자 기록 통제권 및 고지 의무가 미흡할 경우, AI 챗봇이 새로운 감시 벡터가 될 수 있음을 강조함

법원의 판결 논리

     * 판사는 ""문서 보존 명령은 특정 소송 증거 보존 목적의 조치""라며, 헌법적 권리 침해 등은 이 소송의 본안과 무관하다고 판단
     * 개입 시도가 사실관계 규명이나 소송 쟁점과 관련 없고, 소송만 지연시킬 뿐이라고 보아 권리 자체를 인정하지 않음
     * 현재는 뉴스 기업에 실제 데이터가 전달된 사례가 없으므로, 향후 데이터가 공유될 경우 추가 논의가 가능함을 시사함

사용자의 불안과 향후 쟁점

     * 사용자는 ""OpenAI가 프라이버시 보호보다 소송 비용, 평판 관리 등 다른 이해관계를 우선시할 수 있다""고 우려함
     * EFF 등은 AI 챗앱 업체가 삭제 요청 시 진짜 기록 삭제 및 투명한 데이터 요청 고지를 할 필요가 있다고 강조함
     * 궁극적으로 사용자 데이터 보존 및 공개 과정의 투명성, 고지 절차, 사후 구제 방안이 앞으로 주요 이슈로 부상함

        Hacker News 의견

     * 기사에서 인용된 판사의 발언을 보면, 판사가 문제 제기의 본질을 제대로 이해하지 못하고 있다는 느낌을 받음과 동시에 판결 이유도 이상함을 느낌
       ""만약 이런 질문을 고려한다면, 실제로 다루어야 할 법적 쟁점을 해결하는 데 부적절하게 지연만 초래할 것이다""라는 논리임
       저작권 관련 소송이기 때문에 헌법적 문제는 무시해도 된다는 건가 하는 의문
       또, 변호사가 작성하지 않았다는 이유만으로 요청을 바로 기각하는 것은 판사로서 그 역할에 어긋나는 행동이라 생각함
       변호사 없이도 청구할 수 있다는 점을 간과한 것 같음
          + 기사에 첨부된 전체 판결문(여기)를 읽어보면 사정이 더 명확해짐
            문제를 제기한 당사자가 특정 형태의 ""intervene 요청""을 제출했는데, 이에는 엄격한 요건이 있음
            그 요건을 충족하지 못했음
            변호사 없이 제출했다고 기각했다는 부분도 판결문에 언급돼 있음
            개인은 본인을 직접 대변할 수 있지만, 법인(기업)은 반드시 변호사를 세워야 함
            처음에 회사 이름으로 접수된 건이라, 판사는 법에 맞게 처리한 것으로 보임
          + 나도 비슷하게 느끼며, 이런 사생활 문제로 타격받을 수 있는 유사 서비스가 굉장히 많다는 점이 더 걱정임
            이런 과도한 법원 명령을 막아줄 강력한 프라이버시 법이 필요함에도 아직 없음
            예를 들어, 데이트 서비스에서 서로 대화한 모든 메시지를 저장하라는 법원 명령, 통신사에서 모든 문자메시지 저장, 수십억 개씩 생성되는 Google Docs 저장 등의 사례가 가능함
            왜 정부가 아직 Signal 같은 서비스에 백도어를 추가하고 모든 텍스트 메시지 저장할 것을 요구 안 했는지 궁금
            실제 이를 해내려면 상당히 강력한 명분이 필요하다고 생각함
            미국 정부가 이미 일반 상용 서비스에서도 이런 광범위한 감시를 진행해왔을 가능성에 의문을 가짐
            예전에 에드워드 스노든 폭로 때 Google 통신 감청도 있었음
          + 판사는 일반적으로 변호사 출신이거나 법적 경험이 있음
            해당 판사도 변호사 경력이 있고, 동물학 박사 과정도 거의 마친 특이 이력 보유
            사회에서 보호받는 계층이 비보호 계층을 동등하게 대하려 하지 않는 건 매우 자연스러운 일임
            이 사안에서 헌법적 쟁점이 있다고 보기도 어렵고, 만약 있더라도 그걸 판단하는 자리도 아님
          + 변호사 없이 제출했다고 무시하는 것에 대한 비판은 100% 오해임
            Pro Se(본인 소송)는 규정이 잘 마련되어 있음
            당사자가 아닌 다른 법인을 대리해서 소송을 제기할 경우 반드시 변호사가 필요함
            ""소송 주제가 저작권이라 헌법 문제를 무시해도 된다""는 취지라기보다는, 판사가 볼 때 헌법적 이슈가 성립될 것 같지 않아서 시간만 낭비할 수 있다는 판단임
          + ""저작권 소송이기 때문에 헌법 문제는 무시해도 되는가?""라는 부분에
            어떤 헌법적 쟁점이 있다고 보는지 궁금함
            ""변호사 없이도 가능하다""는 주장도, 법적으로 기업은 반드시 변호사를 세워야 하기 때문에 사실과 다름
            만약 개인이 직접하면 어느 정도 배려가 있겠지만, 기업은 해당 안 됨
            이런 기초적인 것에서 논리가 흔들린다면, 다른 논점에 대해 신뢰가 가지 않는다는 생각
     * 이번 사안이 그렇게 큰 뉴스가치는 없어 보임
       OpenAI가 데이터 보존 명령을 직접 저지하려고 곧 구두 변론을 할 예정이고, ChatGPT 일반 사용자가 선제적으로 낸 두 가지 요청이 결과를 좌우할 만한 사건은 아니었음
          + 판사가, 회사에 ""모든 데이터 보존"" 명령이 현실에서 미치는 영향을 제대로 이해하지 못하고 이를 신속히 재고할 의지도 없어 보임
            최초 판결의 파장이 바로 뉴스가치가 있다는 주장이 있음
          + 요즘엔 너무 명백한 결론이 내려져도 매우 큰 뉴스가치가 있다 믿음
          + 사용자가 직접 참여할 권리가 있음을 우리는 이렇게 확인함
            원고가 했던 것처럼 본인이 소송을 직접 낼 수도 있음
            결과가 어떻게 될지는 별개로, 이 자체는 권리임
     * 판사가 해당 이슈에 관심이 없어 보이기 때문에 설득은 무의미하다는 결론임
       OpenAI와 사용자에게 대안이나 선택지는 무엇일까 의문
          + 직접적인 답은 아니지만, 로컬 LLM 사용하는 게 앞으로 가장 현실적인 대안이라고 생각
            요즘 직접 돌려보고 있는데 아직 맥락 저장이나 고성능에는 한계가 있으나, 1~2년 뒤엔 충분히 실용적인 로컬 LLM도 꿈은 아닐 것으로 봄
            나에겐 프라이버시보다는 오프라인 활용성이 더 매력임
          + 미국법상으로는 선택지가 없음
            전형적인 '서드파티 독트린'에 해당
            본인 데이터라도 자발적으로 타사(OpenAI)에 제공시, 그 데이터는 더 이상 본인 것이 아니고 프라이버시 기대 자체가 성립하지 않음
            예외는 반드시 특별히 신설되는 법에서만 인정 가능하며, OpenAI에는 현행법상 해당 없음
            판결 역시 이런 미국 법 테두리 안에서 이루어진 것으로, OpenAI로 보낸 대화 데이터에 대한 프라이버시는 현재 기준에서 보호받지 못함
          + 타사에 기대할 프라이버시를 이제 그만 두는 것이 좋음
            은행정보도 수사 명령이 있으면 공개 가능
            이런 현실을 모르고 요즘 뜨거운 논점만 착각하는 프로그래머가 많음
          + 앞으로 판결이 어떻게 나오든 항소가 대안으로 가능할 수 있음
          + ""OpenAI와 사용자에게 선택지는 무엇인가?""라는 질문에
            미국에 데이터를 내지 않는 국가의 서비스를 찾는 것이 한 방법임
     * 만약 OpenAI가 채팅이나 API에 입력되는 모든 내용을 저장하지 않을 거라 생각하는 사람이 있다면, OpenAI 이사진들 현황을 꼭 살펴보기를 권함
          + OpenAI 공동 설립자 중 한 명이 바이오메트릭 데이터까지 수집 중이라는 점을 보면 이 상황이 어색하지 않음
     * OpenAI 등 LLM 제공업체가 법적으로 데이터 저장을 의무적으로 금지 당한다고 해도, 실제로 그들이 그 정책을 제대로 지킬 것이라 신뢰하기 어려움
       민감한 데이터는 반드시 로컬 LLM에 직접 입력해야 한다고 제안
          + ""저장이 금지된다""라는 것은 곧 강제 저장이 없다는 것과 똑같은 의미임
            이중부정을 인지했으면 함
     * 전 세계가 이제 일상의 모든 면을 감시할 수 있는 기술적 능력이 생겼으니 이를 어떻게 다루느냐가 문제임
       미래에는 이 방대한 데이터로 인해 의학, 역학, 대규모 심리학(나는 Massive open online psychology, moop라고 불러봄) 등 새로운 분야가 생기고, 정부가 실시간 빅데이터로 소외된 국민에게 더 잘 서비스할 수도 있을 것으로 생각
       하지만 이런 미래를 위해선 반드시 신뢰할 만한 법적 체계가 필요하며, 판결 하나하나로 단편적으로 구현되긴 어렵다고 봄
       개인적으론, 인간이 직접 행위하지 않으면 만들어질 수 없는 데이터는 무조건 그 사람 소유여야 하며, 만약 데이터가 해당 인간의 이익을 위해 쓰일 때만 위탁 보관이 허용되어야 한다는 원칙이 필요하다고 생각
       ""이용자/시민의 이익""이라는 높은 기준이 도입되면 데이터 회사의 사고방식도 완전히 달라질 것임
       이런 부분에 대해 다른 사람들의 생각이 궁금함
          + Zero knowledge proof, 블록체인 스트림 결제, IPFS 같은 분산 스토리지와 암호화, 보상체계를 결합하면 해결 가능성이 있음
            아직은 HN 등에서는 터부시되는 방향이지만, 데이터가 사용자를 중심으로 잠기고 중앙 집중화된 사일로 대신 분산되어야만 프라이버시와 빅데이터의 장점을 동시에 추구할 수 있다고 생각함
     * 본 사안의 판사는 사실 임기가 1년 남짓 남은 매지스트레이트(치안판사)임
       예전에도 이런 약한 결정을 내린 치안판사가 임기 연장 안 됐던 사례가 있는데, 이번에도 그러길 바람
       임기 연장 공고 링크
          + 판사라는 직함에 굳이 따옴표를 붙이는 이유를 모르겠음
            치안판사도 엄연히 판사임
            참고: 미국 연방법원 판사 종류
          + 3자 독트린(Third Party Doctrine)의 평범한 적용만으로 치안판사의 임기나 평판이 나빠져야 한다고 보지 않음
     * 과거 동일 주제의 토론 참고 OpenAI가 모든 ChatGPT 로그와 삭제된 대화까지 저장하라는 법원 명령에 반박하다
     * 10년 이상 지속된 대규모 감시 프로그램을 어떤 판사도 막을 수 없음
       이제는 농담거리이자 무의미한 논란임
       OpenAI도 다른 기업처럼 가능한 모든 정보를 수집하고 우리를 프로파일링함
          + 우리가 텍스트박스에 입력하는 정보 자체가, LLM이 응답해주는 결과값보다 훨씬 큰 가치가 있음
     * 많은 이들이 이 상황에 충격을 받거나 분노를 보이는 게 이상하다고 느낌
       대부분은 실제로 일상적으로 쓰는 온라인 서비스들도 똑같이 데이터를 보존한다는 점을 모르고 있거나 간과하는 것 같음
       실질적으로 Signal처럼 E2EE가 아닌 이상, Gmail, 통신사, Reddit, Xitter 등 주요 서비스는 모두 데이터와 메타데이터를 보존함
       이것이 옳으냐 그르냐를 떠나, 현재 온라인 서비스에서는 당연한 현실임
          + 분노하는 이유는 이런 현실이 허용 불가 수준으로 다가온다고 느끼기 때문임
            약관이나 정책과도 별개로, OpenAI는 사용자를 위해 지우려는 데이터까지 강제로 보존해야 하는 상황임
            나는 오히려 사람들이 적극적으로 입장을 표명하는 게 놀랍지 않음
          + 대부분의 사람들은 이 사실을 전혀 모르고 있다고 생각함
            주변의 비기술 친구나 길에서 만난 아무에게나 물어봐도 실제로는 이 현실을 인지하지 못한 경우가 있음
"
"https://news.hada.io/topic?id=21625","BYD, Seal EV에 전고체 배터리 테스트 착수했다는 소문 부인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 BYD, Seal EV에 전고체 배터리 테스트 착수했다는 소문 부인

     * BYD가 Seal EV에 전고체 배터리를 탑재해 1,800km 이상 주행했다는 루머에 대해 공식적으로 사실이 아니라고 부인함
     * 전고체 배터리 차량의 실제 출시 시점은 2027년으로, 해당 모델 및 구체 스펙은 아직 발표되지 않음
     * 일부 현지 매체가 1,875km(약 1,100마일) 주행 및 12분 만에 1,500km 충전 등 사실과 다른 테스트 내용을 보도함
     * BYD는 2027~2029년 생산량 제한, 2030년 본격 대량 생산을 목표로 전고체 배터리 개발 중임
     * 향후 전고체 배터리 가격이 리퀴드 리튬이온 배터리와 동등해질 것으로 예상하며, 다양한 EV 기술과 신형 플랫폼을 통한 시장 확대를 추진 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

BYD의 전고체 배터리 관련 루머 부인

     * 최근 중국 현지 보도에서 BYD가 Seal EV에 전고체 배터리를 탑재해 도로 테스트를 시작했다고 주장함
     * 보도에 따르면, 해당 배터리는 에너지 밀도 400Wh/kg, 1,500km 주행을 12분 충전 등 매우 높은 성능을 기록했다고 전함
     * 총 주행거리가 1,875km(CLTC 기준), EPA 기준 약 1,300km(808마일)로 추정
     * 하지만 BYD는 6월 23일 공식적으로 “해당 모델과 파라미터는 아직 발표된 바 없다” 며 루머를 일축함

BYD 전고체 배터리 개발 현황

     * BYD는 10년 전부터 전고체 배터리 기술을 연구해왔으며, 2023년에 20Ah 및 60Ah 전고체 배터리 셀 테스트 성공
     * 2027년부터 전고체 배터리 탑재 차량을 본격적으로 출시할 계획임을 CTO가 공식 언급함
     * 2027~2029년 초기 생산량은 제한적이며, 2030년에 대량 생산 돌입 예정
     * BYD는 2030년경 전고체·액체 배터리 가격이 동일해질 것이라고 전망함

Seal EV 및 전고체 배터리 주요 루머 정리

     * 일부 매체가 Seal EV에 전고체 배터리를 탑재, 1,875km 주행, 12분 만에 1,500km 충전 등 사실과 다른 정보 보도
     * BYD는 이러한 주장에 대해 공식적으로 “아직 해당 모델 및 스펙 미정, 첫 제품 출시 전” 이라며 부인

시장 현황 및 전망

     * BYD는 유럽·영국 EV 등록 대수 1위를 달성, 글로벌 시장에서 빠르게 성장 중
     * 고속 충전기, 스마트 주행, 신형 플랫폼 등 EV 기술력 다각화로 성장 기반 확대
     * Seal EV의 중국 내 판매가는 17만5,800위안(약 2만5천 달러) 로 경쟁력 있음
     * 향후 어떤 모델이 전고체 배터리 최초 탑재가 될지 시장의 관심 집중

Electrek 관점

     * BYD는 이미 글로벌 EV 시장에서 압도적 점유율을 보이고 있음
     * 다양한 차세대 배터리와 기술로 향후 성장세 지속 기대

        Hacker News 의견

     * 다음 미국 행정부는 더 이상 Big 3 자동차 회사를 무리하게 보호하지 않기를 바라는 마음임. 기후변화 문제를 해결하려면 저렴한 그린테크가 필요하고, 지금은 중국이 전기차, 배터리, 태양광 분야에서 압도적 1위임. 공급망 이슈가 정말 걱정된다면, 전략적으로 배터리와 태양광 패널의 비축을 늘리는 방법도 있다고 생각함. 중국이 자체 산업을 원가 이하로 계속 보조금 지원한다면, 그 관대함을 최대한 활용해서 목표를 달성하는 것이 최선이라고 보는 입장임. 최근 데이터센터 확장과 관련된 좋은 소식 중 하나는, 그리드 저장 설비가 더욱 보급되고 있다는 점이고, 이 덕분에 재생에너지 도입도 가속화된다는 점임
          + 중국의 보조금을 언급하면서 미국의 보조금은 언급하지 않는 태도는 논의의 신뢰성을 떨어뜨린다고 생각함. 중국의 보조금을 비판하는 사람들이 미국이 EV에 25억달러 이상의 보조금을 지급하는 것은 모르는 척하거나 의도적으로 무시하는 경우가 많음. 그리고 약 16년 전, 미국 자동차 산업을 유지하려고 800억 달러의 AIFP 지원책이 없었다면 자동차 산업이 사라질 뻔했던 기억도 있음. ""중국만 EV 보조금 준다""는 얘기는 이제 그만둘 때라고 생각함. 어느 나라든 보조금 지원하고 있다는 것이 현실임. 의미 있는 질문은 보조금의 규모와 그 목적임
          + 미국이 Big 3 자동차회사를 보호하려는 보호무역적 태도가 오히려 기후문제를 악화시키고 미국이 글로벌 시장에서 뒤처지는 요인이라고 생각함. 미국에서 전기차에 대해 가장 많이 듣는 두 가지 비판은 주행거리와 가격임. BYD, 그리고 중국은 이 두 가지 문제를 모두 해결 중임. 충전 인프라가 부족하고, 아파트에 사는 사람은 공공충전기를 써야 하니 큰 배터리가 필요하다고 봄. 여전히 주행거리 불안감도 심함. 연비 효율 좋은 가솔린 자동차의 경우 1회 주유에 400마일 이상 가는데, 미국에서 파는 전기차는 대부분 연료탱크 용량은 작고 비싼데도 250마일 정도임. 가격도 진입장벽임. 미국에서 제법 인기 있는 Ioniq 6도 3만8천 달러부터 시작, 좋은 조건으로도 월 800달러 가까운 할부임. BYD라면 2만 달러 선에 팔 수 있다고 봄
          + 만약 중국이 제조원가 이하로 계속 보조금을 넣어서 산업을 키운다면, 우리는 오히려 그 호의를 최대한 예민하게 활용해 최대한 빠르게 기후 목표를 달성해야 한다고 생각함. 그런데, 실제로 중국이 적극적으로 덤핑하고 있는지 의문임. 연간 수십 억 달러가 크긴 하지만, 이 정도 규모의 산업에서 '덤핑'이라고 단언하긴 어렵다고 봄
          + 여전히 '중국산=저품질'이라고 생각하는 사람들이 많아서 놀라움. 90년대 사고방식임. 중국은 주문하는 대로 만들어줌. 싼 걸 원하면 싸게, 비싼 품질 원하면 고급으로. 결국 돈 내는 만큼임
          + 기업이 경쟁력을 가지려면 경쟁을 시키는 방법밖에 없다고 생각함. 인위적이고 소비자에게 불리한 관세 보호로는 절대 경쟁력 확보가 불가능하다고 확신함
     * 미국의 전기차 시장에서 전기화 전환을 거부하는 태도 자체가 가장 큰 자해 행동이라는 생각임. 기술 혁신의 흐름 앞에서는 무역장벽만으로는 못 버팀. 예를 들어 900마일 주행거리 EV가 12분 만에 충전된다면, 20분 충전만으로 이틀치 장거리 운전도 가능함. 대부분 운전자는 자기 집 앞에서 밤새 충전 가능해서 불편도 적음. 다양한 에너지원을 통해 전기를 뽑을 수 있고, 평소에는 주유소 갈 필요도 없음. 화석연료 연소 부산물도 완전히 제거 가능, 구조도 단순해지고 성능도 오히려 좋아짐. 기술적 측면에서 보면 EV에게 압도적으로 유리함
          + 기술적으로 이렇게 뒤처진 상황에서 혁신 경쟁에서 승리하긴 매우 어려움. 그나마 할 수 있는 일은 단기적인 고통을 피하려고 미루는 수밖에 없음. 또, 미국인은 일상생활 변화를 굉장히 예민하게 받아들이며, 남의 조언 자체를 거부하는 문화가 있음. 그래서 바뀌기란 쉽지 않음
     * 기사에서 가장 눈에 띄는 부분은 BYD의 전고체 배터리가 1500km(932마일) 주행 가능, 단 12분 만에 충전 기록을 세웠다는 점임. 이 테스트도 배터리 용량 80%만 충전한 기록이라서, 100%로 치면 CLTC 기준 1875km(1,165마일), EPA 기준 약 1,300km(808마일)임. 다른 회사는 이런 배터리 언제 따라잡을지 궁금함. 왜 전고체 배터리가 폰에 먼저 쓰이지 않고 차량에 우선 적용되는지도 의문임
          + 답변하자면, 전고체 배터리의 장점은 자동차에서 특히 가치가 큼. 휴대폰은 지금 배터리로도 충분히 쓸만함(더 오래 가고 빠른 충전이야 흥미롭지만 ‘필수’는 아님). 현재는 생산규모도 한계가 있고 가격도 기존 리튬이온보다 비쌈(기사에서는 10년 내 비슷해질 거라고 함). 가격 더 내려가고 생산이 본격화되면, 당연히 핸드폰에도 쓰일 것으로 예상함
          + 에너지밀도, 가격, 복잡성, 그리고 방전패턴이 결합될 때의 다양한 시나리오가 ""매우 흥미로운"" 상태임. 어떤 기술은 효율은 압도적으로 좋은데 예측 가능한 느린 방전 환경만 맞음(예: 사막의 거대한 GW 전력저장). 자동차는 배출 패턴이 노트북 수준처럼 급격하진 않은 중간단계임. 휴대폰은 사용량이 들쭉날쭉하면서 소형화까지 함께 요구된다 보니 초기 시장 진입 비용이 큼. 자동차가 상대적으로 적용이 쉬운 위치에 있음. 규모, 신뢰성, 특허, 공급망도 큰 역할임
          + 솔직히, 전고체 배터리가 왜 휴대폰에 빨리 안 쓰이는지 이미 알고 있었고, 이건 AI가 답해줄 정도로 쉬움. 휴대폰 배터리는 이미 Li Polymer(고체상태임)를 쓰고 있는데, 이게 다양한 형태, 두께, 모양으로 만들 수 있음. 다른 화학식은 그렇게 얇게 못함
          + 중국 시장 폰에는 이미 6000mAh 배터리의 엄청난 지속력을 제공하는 제품이 있음. 어떤 폰은 8000mAh도 탑재됨. Apple, Samsung같은 회사들은 혁신보다 기존 제품으로 수익만 내려고 한다는 생각임
          + 많은 회사가 전고체 배터리를 개발해왔지만, 연구실 샘플은 가능해도 대량 생산은 별개 문제임. BYD가 세계 2위 배터리 기업이고 엔지니어, 돈 투자 엄청해서 먼저 성공했다 봄. 세계 최대 배터리사인 CATL도, Toyota도 개발 중. 단가 적당한 수준으로 양산된 제품은 아직 미지수이고, 그래도 곧 상용화될 것으로 기대함
     * BYD 전고체 EV 배터리가 12분 만에 1500km(932마일) 주행거리를 충전했다고 하는데, 이런 기술이 자동차 외에 다른 분야(예: Power Wall)에 적용된다면, 요금 폭탄 걱정 없는 미래가 진짜 기대됨
          + 주택용 배터리 가격이 아이폰 가격보다 싸다는 사실
          + 이렇게 빠른 충전 속도의 기술적 베이스는 혁신적인 무언가가 아니라, 매우 위험한 고전압 DC 충전 방식임(현재로선 중국에서만 활용). 그리고, 중국에서 EV 택시가 배터리 다 닳았을 때는, 도심 곳곳에 배터리 교환소가 있어서 그냥 진입하면 전체 배터리팩을 몇 분 만에 갈아끼움
     * BYD의 전고체 배터리 에너지 밀도가 400Wh/kg, 기존 리튬이온 대비 거의 두 배라고 기사에 나오던데, 전고체 배터리의 문제점 중 하나가 오히려 낮은 에너지밀도 아닌지 의아함. 무엇이 변한 건지 궁금함
          + 높은 에너지밀도는 오히려 전고체 배터리를 연구하는 이유 중 하나임. 아마 초기 연구용 저성능 샘플을 일반화해서 그렇게 생각한 것일 수 있음
          + 이번건 차세대 리튬배터리임. 일부 프로토타입에서는 아예 600까지도 나왔다는 얘기도 들었음. 여전히 리튬 기반이라 어딘가에서 문제가 생기면 물을 부으면 안 될 것 같음. 전문가는 아니지만, 사실이라면 긍정적으로 바라보는 중임
          + 철인산(iron phosphate) 배터리랑 혼동한 것 아님? 철인산 제품은 저렴한 EV 시장에 많이 쓰이긴 하는데 에너지 밀도 낮은 게 단점임
     * 혹시 이 배터리들이 추운 날씨에서 어떻게 작동하는지 아는 사람이 있는지 궁금함. 나는 몬타나에 살고 있고, 겨울에 배터리 수명, 주행거리, 충전 시간이 생명과 직결된 이슈라 많은 사람들이 EV에 회의적임. 만약 이게 해결된다면 정말 반가울 것임
          + 많은 사람들이 구식 정보를 근거로 걱정한다고 생각함. 2020년형 Model 3를 쓰고 있는데, 영하에도 문제 없었음. 주행거리는 약 20% 줄긴 하지만 오히려 추울 때 엔진 예열 안 기다려도 되고, 아침마다 '가득 주유'된 상태로 출발 가능해서 더 편함. EV는 대부분 AWD라 겨울에도 운전 쉬움. 실제로 눈폭풍 왔을 때 트럭들이 빠져나가지 못했던 상황에도 내 차는 문제없었음. 최신 모델은 히트펌프 등으로 효율 더 좋아졌고, 배터리도 기존보다 더 많은 에너지 저장 가능. 아주 외진 곳(예: 알래스카 시골) 아니면 EV도 충분히 괜찮다고 생각함
          + EV가 '추위를 더 잘 견딘다'는 표현은 오히려 오해 소지가 있다고 봄. 내연기관차는 따뜻할 때도 늘 열 손실로 비효율적임. EV는 따뜻할 땐 사실상 '보너스 주행거리'가 생김. 엄청 효율적인 내연기관차가 있다 해도, 추울 땐 효율을 강제로 낮추거나 난방용 열원을 따로 넣어야 하며, 그래야 겨울에 욕먹지 않을 것임
          + 평소 하루 운행거리가 어느 정도인지 궁금함. 100마일(161km) 이하라면, 대다수 EV가 추운 날에 25% 감소해도 충분하다고 봄. 참고로 미국인 평균 주행거리는 하루 36마일이라고 알고 있음. 몬타나의 유명 유튜버 Hank Green도 EV로 출퇴근하고, 사용자 경험을 영상으로 공유함
          + 겨울엔 배터리 수명이 생과 사의 문제라고 했는데, 전기차는 좌석 히터 풀로 한 달도 돌릴 수 있다고 알고 있음. 가솔린차는 가득 주유해도 몇 시간뿐임. 게다가 EV는 배기관이 눈에 막혀서 일산화탄소 중독될 위험도 전혀 없음
          + 노르웨이에서도 테슬라가 꽤 잘 팔린다는 것으로 알고 있음
     * 400Wh/kg 배터리는 중형 UAV에도 엄청나게 유용할 것으로 봄. 현재 최고급 18650 혹은 21700 셀로는 255Wh/kg 수준임
          + 미국 국방부(DoD)가 대량 구매하려고 달려들 만한 아이템임
     * 대부분의 댓글이 개인용 차량에만 집중되어 있는 것 같지만, 진짜 파급효과는 로보택시(waymo처럼), 트럭, 버스, 트랙터, 중장비 같은 상업용 분야가 훨씬 클 것이라 믿음
     * 새 기술을 시험해보기엔 정말 좋은 선택이라는 생각임. 결국 유지보수 비용이 크게 절약될 가능성이 높음. 어차피 'Seal'이 깨지면 워런티도 무효임
     * 업데이트: BYD에서 이 뉴스는 부인했다는 소식임
"
"https://news.hada.io/topic?id=21568","노숙자 문제를 해결하기 위해 뭐든 하겠지만 집은 더 짓지 않겠음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  노숙자 문제를 해결하기 위해 뭐든 하겠지만 집은 더 짓지 않겠음

     * 미국의 노숙자 문제가 심각한 위기에 처해 있음
     * 많은 사람이 ‘변화’가 필요없는 표면적 행동에만 집중함
     * 주택 공급 확대와 근본적 해결책은 회피하는 경향을 보임
     * 부동산 가치 보호나 개인 이익 중심 사고가 일반화됨
     * 노숙 해결을 위한 실질적 접근에 대한 거부감이 드러남
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 노숙자 문제에 대한 표면적 태도

   미국 노숙자 문제가 심각한 위기에 다다른 상황임. 하지만 많은 사람들은 주거 용적 규제나 도시 계획의 변화 없이 문제를 ""해결""하겠다는 의지를 보임. 이러한 접근은 자신의 통근 환경이나 자가 주거에 대한 변화가 있을 때 특히 거부 반응을 보임. 단순히 가끔씩 음식 나눔이나 형식적인 자선 활동에 만족하며, 노숙자가 자신의 눈에 띄지 않게만 행동하길 바라는 태도를 가지는 경향이 많음

한계적 공감과 자기 이익 중심의 접근

   노숙자 문제를 사회적 공감과 이익 보호의 시각으로 바라보는 태도가 뚜렷함. 공공선보다 개인 주거 지분, 부동산 가치 등 즉각적인 이익의 유지에 더 중점을 둠. 혁신 기업을 이끌거나 기술적 솔루션을 제안하는 ‘혁신가’ 이미지를 스스로 부여하면서도, 실질적으로는 단순한 ‘문제 덮기’를 선호함. 편지 쓰기, 언론 인터뷰, 온라인 차별적 게시물 작성 등이 ‘문제 해결 행동’으로 치부됨

실질적 해결책에 대한 회피

   노숙자 문제를 '집을 더 짓는 것'이 아닌 방식으로만 해결하려는 심리가 뚜렷함. 직접적으로 자신이 소유한 넓은 단독주택이나 고가 아파트 등 거주 환경의 변화는 거부함. 공유 경제, 임대, 자녀를 위한 유산 확보 등, 이미 누려온 특권적 환경은 지키려 함. 각종 변명과 합리화, 노동 계급 이민자에 대한 이중적 태도 등도 노출됨

빈부격차와 현실외면

   미국의 부의 집중화를 문제로 인식하면서도, 실제로 노숙자나 경제적 약자를 직접 경험하거나 대면하는 일은 거의 없음. 사회 구조의 한계를 모른 척하거나, 노숙자의 신원과 사연을 의심하며, 자선의 의미와 본질을 흐림. 자신의 소비와 생활 양식에 따라 양심의 가책을 덜기 위한 방편에만 머무름

주택 정책 변화에 대한 강한 저항

   실제로 지속적 주택 공급 확대나 도시 인프라 개선이 노숙 문제 해결의 핵심임을 알면서도, 자신이 보유한 토지, 정원, 고급 주방 등 사적 공간 희생을 거부함. 정책 변화에 대한 공포, 새로운 복지 정책에 대한 저항, ‘내가 먼저 이 동네에 왔다’는 소유권 의식이 강함. 주택 공급 확대로 인한 대중 교통 활성화, 환경 개선 등 긍정적 변화도 상상하지 않으려 함. 결국 “** 새로운 주택 건설 없이 노숙자 문제를 감내하겠다**”는 결론에 도달함

        Hacker News 의견

     * 실제로 노숙인 지원을 직접 해본 입장에서 말하자면, 노숙 문제는 ‘매일 여기저기 쫓겨나면서도 밤에는 돌아가는 경우’부터 ‘완전히 다리 밑에서 자는 삶’까지 연속성 문제임을 알게 됨, 그리고 진짜로 집값이 문제가 되는 경우는 드묾. 대부분 정신 건강, 약물 문제, 그리고 주변의 지원 시스템 부재가 더 큼. ‘사람들이 어쩌다 사회적 안전망에서 떨어지는지’는 넷플릭스 영화나 드라마만 잘 봐도 금방 알 수 있을 정도임. 그래서 글쓴이에게 악감정은 없지만, 이 글은 별 의미 없는 정치적 제스처로 느껴짐
          + 태평양 북서부에서 가장 큰 노숙 쉼터 단체 중 하나와 수년간 일해온 경험상, 실제로 우리 센터를 찾는 대부분의 사람들은 그렇지 않음. 노숙의 주된 원인은 잠깐의 경제적 위기(건강 문제, 실직 등)에 의한 퇴거, 가정폭력 같은 가정 문제, 난민 등 법적 이슈임. 나도 원래는 밖에서 눈에 띄는 노숙인을 보면서 ‘정신적/약물 문제가 주요 원인’이라고 오해했는데, 실제로 일을 하며 진짜 문제는 ‘차에서 지내거나, 평범한 옷을 입고 구걸도 안 하고 열심히 출퇴근하는 사람들’이 엄청 많다는 사실을 알게 됨. 또 인과관계가 반대인 경우도 많음. 노숙 생활 자체가 엄청난 스트레스를 주기 때문에, 그에 대처하다가 정신 건강이 악화되고 중독 문제가 생길 수 있음. 이렇게 되면 하향 악순환에서 빠져나오기 점점 힘들어짐. 현실적으로 힘든 일이긴 하지만,
            다들 주변의 믿을 만한 단체에 어떻게든 후원이나 자원봉사를 고민해보면 좋겠음
          + 이런 시각(정신 건강 문제 때문에 노숙)이 자주 보이는데, 사실 노숙이란 현상을 지나치게 정적으로 본다는 느낌임. 노숙인은 이미 힘든 정신적 상태에 있는 경우가 많지만, 그렇다고 해서 그 상태 때문에 노숙을 하게 됐다고 볼 수는 없음. 오히려 정신적 불안정이나 약물 문제가 노숙 이후에 더 확산되는 경우가 많음. 원래는 평범하게 살던 사람들이 노숙 상태에 처하면서 점점 심해짐. 거리의 ‘이상한 사람’ 수를 줄이고 싶으면, 결국은 사람들이 안정적인 환경, 즉 주거를 확보할 수 있도록 해야 함
          + ‘실제 집값이 문제인 경우는 드물다’고 했는데, 예전에는 맞았을 수 있지만, LA는 지금 사람 한 명(혹은 한 가구)당 최소로 드는 집값이 70만 달러임(세탁기도 없는 스튜디오나 원룸 기준). 그래서 LA시와 카운티가 노숙자 지원 기관(LAHSA) 운영을 종료했고, LA와 SF는 수십억 달러를 쏟아부었지만 하루에 겨우 두세 가구만 집을 마련해줬고 돈도 바닥남. 올해 LA는 10억 달러 예산 적자가 예상됨. 관련 기사. 게다가 최근 LA 화재의 33%가 노숙인에 의해 일어남. 소방서장은 시가 노숙인 지원(9억 6,100만 달러)에 소방 예산(8억 3,700만 달러)보다 더 많은 돈을 썼다고 밝힘. 관련 기사
          + 다리 밑에서 충분히 오래 자다 보면 누구나 정신적 문제를 겪게 됨. 노숙은 사회적으로 ‘가장 낮은 지위’를 상징하는데, 호르몬 시스템도 그 상태에 맞춰 바뀌고, 행동도 눈에 띄게 달라짐. 마치 리더가 되면 뇌와 행동이 바뀌듯, 가장 낮은 위치에 처하면 뇌도 바뀜. 물론 절대로 벗어날 수 없는 건 아니지만, 노숙 생활이 길어질수록 자기 권리나 존재성조차 포기하게 되고, 사소했던 심리적 문제도 점점 커짐. 원래부터 명상 등으로 자신을 극한까지 단련해온 경우 아니면, 대부분의 평범한 사람이 이런 상황에서 정신적으로 무너짐. 물론 정신 질환이 있어 노숙을 하게 된 사람도 많고, 악조건 속에서도 벗어난 사람도 있지만, 상당수는 노숙에 처한 이후 도저히 다시 적응하지 못하고 정신적으로 어려워지는 경우임
          + 내 경험상 젊은 층에서는 ‘친구네 집 전전하기’, 즉 친구 집이나 임시 숙소를 옮겨 다니며 짐은 다른 데 맡기고 장기간(최소 1년) 고정된 거주지가 없는 사람도 많음. 이 경우는 주로 높은 가격과 공급 부족 때문임
     * 영국은 명백한 주택 공급 부족 국가임. 인구 대비 필요한 주택 수가 실제로 있는 집 숫자보다 많음. 영국에서 집 짓는 게 어려운 이유는, 인구 대부분이 몰려 사는 잉글랜드 지역이 너무 빽빽하게 차 있어서 도시 인프라도 이미 한계고 교통망도 부족함. 그리고 주택 가격이 거품이라 겨우 집을 마련한 사람들은 집값이 더 떨어질까봐 두려워함. 2차 대전 이후 영국이 시도했던, 그리고 지금 다시 시도하려는 방법은 새로운 도시 직접 ‘신규 건설’임. 실제 밀턴 케인스 같은 사례도 있는데, 이곳은 기초부터 교통, 공원, 다양한 밀도별 주택, 학교, 소방서까지 다 짓고, 기존 주요 철도와 바로 연결되어 접근성도 확보함. 이런 방식이 가능성임. 기존 도시 확장에만 기대지 말고, 새로운 도시 자체를 만들어서 사람들에게 저렴하고 괜찮은 선택지를 보여주면
       사람들은 스스로 이동하게 됨
          + 영국에서 진짜 문제는 ‘허가’ 구하기가 너무 힘듦. 예를 들어, 우리 아버지가 런던 근처에 160에이커 농장이 있는데, 노르딕 스타일의 조립식 주택은 5만 파운드 정도면 가능함. 하지만 그 땅에 그런 집을 세울 허가 자체가 사실상 불가함. 땅주인에게 무턱대고 시세차익을 줄 필요는 없지만, 정부가 직접 땅을 사서 개발하면 어떨까 싶음. 또 새로운 개발을 하면 사람들이 싫어하는 가장 큰 이유가 ‘너무 못생겼다’는 건데, 케임브리지나 베네치아처럼 멋지게 만들어보면 더 환영받지 않을까 생각임
          + 새 도시를 짓는 게 어려운 이유는 ‘일자리’임. 인프라 다 만들어놔도, 근처에 충분한 규모의 일자리가 생기리라는 보장이 없음. 정부가 비효율적으로 사용되는 땅을 강제로 수용해 재개발하는 방법은 괜찮을지도 고민임
          + 그와 동시에, 이민 규모도 줄여야 한다고 생각임. 매년 50만 명씩이 새로 들어온다고? 도저히 도시 하나씩 매년 새로 만들어낼 수 없음
     * 이 글을 정말 재미있게 읽었지만, ‘이게 현실이라는 점’ 때문에 씁쓸한 느낌도 듦. 또 필자가 NIMBY(우리 동네 개발 반대) 문제가 대중교통 발목 잡는 것과도 연결짓는 점이 정말 좋았음
     * 나는 취리히에 거주 중인데, 전 세계에서 임대율이 가장 낮을 정도로(공실률 0.7%) 경쟁이 치열하지만 이상하리만큼 노숙인이 거의 없음. 하지만 이사 가야만 했던 사람도 많음
          + 더블린에서는 2025년 2월 1일 기준, 인구 150만 도시에서 임대 가능한 집이 1200채밖에 없음
          + 근교 출퇴근 도시들과 대중교통이 얼마나 연결 잘 되어 있는지 궁금함. 교통이 저렴하고 편리하고 빠르면 분명 큰 도움이 될 거라 생각함
          + 스위스 취리히 같은 도시는 노숙인에게 무료로 유럽 다른 도시행 버스표를 제공함. 관련 기사
          + 노숙 문제가 보이지 않는 주된 이유는, 너희가 국경 통제와 체류 허가 시스템으로 ‘노숙 가능성 있는’ 사람 자체를 아예 내쫓기 때문임
          + 실제로 취리히에는 꽤 큰 노숙인 문제가 있음. 네가 잘 안 나가서 못 보는 거 아닐까?
     * 주택 건축 규제를 완화하는 게 미국 등 여러 국가에서 심각한 주거난을 해결할 수 있는 확실한 방법이라고 생각함. 자세한 자료는 여기 참고
          + 당신 지역에선 효과가 있을지도 모르겠지만, 캘리포니아에서는 부동산 규제 완화법이 시행될 때마다 결국 200~300만 달러짜리 고급주택만 더 들어서는 결과임. 이게 노숙 문제엔 전혀 도움이 안 됨
          + 가난한 하층/노동계급의 주택 문제 해결을 외치며 규제 완화를 주장하는 온건 민주당원들과, 재정 보수 성향의 공화당원들이 동시에 같은 ‘풍부함’(abundance) 기치를 앞세우는데, 뒤에서 응원하는 건 모두 억만장자와 사모펀드임. 누가 속이는지 명확함. 결국 정책을 통해 값싼 임대주택이 아니라, 부자들이 원하던 럭셔리 주택이나 고층 빌딩만 난립함
     * 최근 일하고도 차에서 사는 사람들이 늘고 있음. 이들은 약물 중독이나 심각한 정신 질환이 있는 게 아니라, 차 안에서 생활하는 것 자체 때문에 생기는 불안감 정도만 겪고 있음. 유튜브에도 이런 사람들 인터뷰가 넘쳐남
          + 우리 주에서는 지난해 노숙 자체를 아예 범죄로 규정해, ‘불법 야영’ 금지 법안을 통과시킴. 자동차 안에서 잘 수는 있지만 등록과 보험이 모두 최신이어야 했고, 12시간마다 차를 옮겨야 함. ‘불법 야영자’를 폭력으로 퇴거시킬 수 있고, 임대/전세 기간 만료자, 불법 점유자도 포함됨
          + 이런 얘기 하려고 들어왔음. 차에서 사는 인구는 사실상 공식 집계가 거의 불가능한 ‘그림자 인구’임. 내가 살던 작은 마을만 해도, 외부인이 상상하는 노숙인은 손에 꼽았지만, 주유소 등에서 가끔씩 자동차에서 생활하는 사람을 꾸준히 만남. 아마 생각보다 훨씬 많음
     * 주택을 제공한다고 해서 정신 건강 문제나 중독 문제를 가진 사람 모두가 나아질 거라고 생각하지 않음. 오히려 21세기형 인간적인 치료센터, 즉 현대적 ‘정신 요양원’ 등 정신 의료 인프라가 노숙 문제 해결에 더 도움이 될 거라는 생각임
          + 현실적으로 노숙인 중 가장 눈에 띄는 계층이 정신 질환자와 중독자인데, 이 계층만 전체 문제로 일반화하면 안 됨. 정신 질환이 원인/결과 어느 쪽일 수도 있고, 노숙 이후에 상태가 악화되는 경우도 많음
          + 노숙자라고 해서 모두 심각한 정신 건강 문제를 겪는 건 아니고, 오히려 노숙 때문에 그런 문제가 점점 생기는 경우가 많음. 결국 극단적으로 말하면 노숙인을 그냥 전체 수용소에 집어넣자는 얘기는 결국 ‘문제 은폐’임
          + 웨스트버지니아주만 봐도 이런 생각이 완전히 틀렸다는 사실이 명확함. 이 주는 약물 남용 비율이 캘리포니아보다 높은데도 노숙인 자체는 거의 없음. 그 이유는 단순함, 주거비가 워낙 저렴해서임. 결국 집 문제, 즉 주택 공급이 매우 큰 영향을 미침
          + 현실적으로 ‘이분법’적 구조가 문제임. 어른답게 살아가려면 잘 살아야 하고, 그렇지 않으면 바로 추락해서 노숙·수용·사회 아웃이 됨. 이 둘 사이, 각자 능력에 따라 살면서 부족한 부분은 보완받을 수 있는 공간, 즉 일종의 완충/중간 단계가 부족함. ‘하프웨이 하우스’처럼 추락한 뒤 사회로 복귀하는 시설은 있지만, 미리 잡아주는 프로그램은 거의 없거나 너무 적어서 수요 감당도 못함
          + 사실 이런 ‘사회적 지원 인프라’ 자체를 시도하려고 해도, 주변 주민 반발(NIMBY)이 동일하게 가로막고 있으니 아무 의미 없는 논쟁임
     * 재택근무가 일반화되지 않는 이상, 일부 지역은 저렴한 주택 부족이나 끔찍한 출퇴근 문제에 시달릴 수밖에 없음. 드물게 전용 대중교통 시스템을 구축하는 지역도 있지만, 땅은 있는데 일자리는 멀리 분포하고, 사람들은 옛날 50년대식 집도 싫어하고 ‘아이 키우는 고밀도 주택’도 별로 원치 않음. 다들 부동산 값이 내려가는 건 싫어함
          + 사실 재택근무는 이미 꽤 많이 퍼져 있음
     * 모든 곳이 그런 건 아님. 내 경우 내슈빌에 살고 있는데, 주택 신축이 엄청 활발함. 아파트 단지가 계속 건설되고 있음
          + 캘리포니아 등 민주당 주들이 심각한 문제를 겪는 이유는, 좌파 진영 정치인(딘 프레스턴 등)과 소유주 중심 중도 성향이 연합해서 실질적으로 주택 신축을 막아왔기 때문임. 공화당 주(텍사스 등)는 주택 신규 공급, 임대료 인상률 등 훨씬 더 좋은 결과를 보임. 이런 현실에 불만을 가진 진보 계층도 점점 많아지고 있지만, 아직은 실질 영향력이 크지 않음
          + 실제로 지역 평균 소득 대비 새로 지어지는 집이 감당할 만한 가격인지 궁금함. 만약 그렇다면 정말 좋은 현상임
          + 내슈빌에 진짜 노숙인이 전혀 없냐고 묻고 싶음
     * 좋은 기사임. 이런 글 더 많이 필요함
"
"https://news.hada.io/topic?id=21587","플라스틱 봉투 금지 및 부과금이 해변 플라스틱 오염을 줄임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    플라스틱 봉투 금지 및 부과금이 해변 플라스틱 오염을 줄임

     * 전 세계적으로 플라스틱 쓰레기가 해양 쓰레기의 주요 부분을 차지하고, 해양 동물과 생태계에 큰 피해를 주는 현상임
     * 미국 내 다양한 플라스틱 봉투 정책(금지, 부과금, 미규제) 이 시행되어 효과 비교가 가능함
     * 2017~2023년 45,067개 해변 청소 데이터 분석 결과, 플라스틱 봉투 정책 도입 지역에서 해변 플라스틱 봉투 쓰레기가 25~47% 감소됨
     * 정책 유형별로는 부과금 방식이 완전 또는 부분 금지보다 더 큰 감소 효과를 나타냄
     * 야생동물 얽힘 피해도 줄었으나, 추가 연구 필요성 및 국제적 플라스틱 관리 강화의 시사점을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

플라스틱 오염과 정책적 대응

     * 플라스틱 쓰레기는 전 세계적으로 해양 쓰레기의 대부분을 차지하며, 이는 해양 동물의 소화 기관 폐쇄, 얽힘, 질식, 부상뿐 아니라, 독성 물질 방출을 통한 생태계 서비스의 경제·사회적 피해를 야기함
     * 해변 플라스틱 쓰레기는 관광산업, 해안 부동산 가치에도 부정적 효과를 끼치며, 전 세계적으로 해양 플라스틱으로 인한 사회적 비용이 연간 1,000억 달러를 초과하는 것으로 추산됨
     * 플라스틱 쓰레기는 주로 육상에서 유입되며, 잘못된 폐기물 관리와 인구 규모, 관리 체계에 따라 유입량이 달라짐(전 세계 플라스틱 쓰레기의 2~5%가 매년 바다로 흘러들어감)
     * 일회용 플라스틱 봉투는 재활용률이 극히 낮고, 쉽게 바람에 날려 해변으로 유입됨
     * 이에 따라 전 세계 100개국 이상이 플라스틱 봉투 관련 규제를 시행했고, 175여 개국이 글로벌 플라스틱 협약을 논의 중임

미국 플라스틱 봉투 정책의 다양성과 분석 가치

     * 미국 내 플라스틱 봉투 규제는 주, 카운티, 시/타운 단위 등으로 다양한 형태를 띔
          + 완전 금지 또는 부분 금지(얇은 봉투만 금지 등), 소매점 부과금(5~25센트), 미규제(규제 불허) 형태가 있음
     * 정책의 예
          + 완전 금지: New York 주(2020), 부분 금지: Washington 주(2021, 2.25mm 이상 재생 플라스틱 봉투 허용)
          + 부과금: Virginia주 Arlington 카운티(5센트)
          + 미규제: 17개 주는 아예 플라스틱 봉투 관련 규제 자체를 금지(preemption law)
     * 2008~2023년 기준, 미국에서 총 611개의 플라스틱 봉투 정책이 시행(91%가 시/타운 단위)
     * 2023년 기준, 미국 인구의 약 33%가 봉투 규제 지역 거주(주 단위 정책 영향이 가장 큼)

해변 쓰레기 데이터와 플라스틱 봉투 비중

     * 2016~2023년 해변 청소 45,067건 데이터를 Ocean Conservancy 시민과학 플랫폼을 통해 수집·분석
     * 미국 해변 쓰레기에서 플라스틱 봉투는 전체 항목 중 5번째로 많고, 평균 4.5%(2023년엔 6.7%) 차지
     * 해변 청소 데이터는 플라스틱 오염 감소의 대략적인 척도로 사용됨
     * 정책 효과 분석에서, 정책 도입 지역의 해변 청소 데이터와 정책 미비 지역을 비교함

플라스틱 봉투 정책 효과 분석

     * 처치 지역(정책 시행 지역)에서 플라스틱 봉투 비율이 25%~47% 감소(5개 계량분석 방법 적용)
     * 이 감소 효과는 지역 및 기간, 집계 단위와 무관하게 강하게 나타남
     * 정책 시행 이후 연차별로 효과가 점점 커짐(시행 1~5년차 모두 통계적으로 의미 있는 감소)
     * ‘위약(플라시보) 테스트’ 수행 시, 플라스틱 병·뚜껑, 빨대 등 다른 플라스틱 쓰레기에는 별다른 변화 없음
     * 크기·시점·지역별 집계 방식 등 다양한 분석에서도 효과가 일관적으로 유지됨

정책 유형, 범위, 지역적 차이의 영향

     * 부과금 정책(fee)이 완전 금지 또는 부분 금지에 비해 더 큰 감소 효과
     * 주 단위 정책이 가장 견고한 효과, 시/타운·카운티 단위도 모두 의미 있는 감소
     * 해안(연안)·강·호수 구분 분석 시, 호수 지역에서 상대적으로 더 큰 감소 효과가 시사됨
     * 정책 시행 전에 플라스틱 봉투 쓰레기 비중이 높았던 지역일수록 정책 효과가 큼(75백분위 이상 지역 중심)

야생동물 얽힘 피해 감소

     * 플라스틱 봉투 정책 도입 시, 청소 중 발견된 야생동물 얽힘 사례가 30~37% 감소 효과 추정
     * 다만, 다른 요인도 얽힘에 영향을 미치기 때문에 결과는 불명확하며 추가 데이터·연구 필요

논의 및 시사점

     * 플라스틱 봉투 관련 정책은 해변 쓰레기(특히 플라스틱 봉투) 감소에 전반적으로 효과적임이 명확히 드러남
     * 부과금 방식이 부분 금지나 단순 금지보다 더 큰 효과가 시사됨
     * 미국 사례가 전 세계에 직접 적용될 순 없으나, 폐기물 관리 체계가 미흡한 지역에서는 더 큰 정책 효과 기대 가능
     * 장기적으로 플라스틱 병·뚜껑 등 타 일회용 플라스틱 품목에 대한 규제도 검토 가치가 있음
     * 글로벌 플라스틱 협약 등 대규모 정책 변화 없이는 해양 플라스틱 쓰레기 유입은 지속적 문제로 남을 전망

연구 방법 요약

     * 미국 주/카운티/타운 정책 데이터를 수집해, 해변 청소 데이터(TIDES)와 우편번호별로 매칭
     * 분석 변수: 청소 대상 쓰레기 중 플라스틱 봉투가 차지하는 비율
     * 데이터는 0.1° 격자(약 11.1km) 단위, 연 단위로 집계해 차이의 차이 추정모형(difference-in-differences) 적용
     * 주요 분석에서 기존 또는 폐지된 정책 지역, 인접 우편번호 지역 등은 제외
     * 주요 분석 모형은 연도, 지역 고정 효과를 통제하는 이중 고정 효과 회귀임

기타

     * 데이터 및 분석 코드는 openICPSR에 공개됨
     * 저자 및 연구비, 연구 기여 등 상세 정보 별도 기술됨

결론

     * 플라스틱 봉투 정책은 봉투 쓰레기와 그로 인한 해양 생태계 피해 감소에 효과적임
     * 효과는 정책 유형, 지역, 시행 전 쓰레기 수준에 따라 크게 달라질 수 있음
     * 향후 플라스틱 병·캡 등 타 품목, 글로벌 규제 논의, 폐기물 관리 체계 종합 개선 등의 시사점이 도출됨
"
"https://news.hada.io/topic?id=21622","rou2exOS - Rust와 x86 어셈블리어로 작성된 Dos-like 취미 OS ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             rou2exOS - Rust와 x86 어셈블리어로 작성된 Dos-like 취미 OS

     * Rust로 개발된 64비트 DOS 스타일 운영체제로, 일부 x86 어셈블리도 커널 로딩에 사용
     * VGA 텍스트 모드(80x25), FAT12 파일시스템, SLIP을 통한 IPv4 네트워크 스택(ICMP/UDP/TCP/HTTP) 구현
     * QEMU 기반 가상머신에서 실행 및 개발이 이루어지며, 일부 실물 플로피 미디어도 지원
     * 간단한 텍스트 에디터, TAB 파일/디렉토리 자동완성, Snake 게임 등 기본 유틸리티 포함

아키텍처 및 부트로더

     * 대상 CPU는 x86_64이며, 향후 ARM(aarch) 아키텍처 지원 예정
     * 초기 버전은 직접 작성한 부트로더로 커널을 메모리에 올리고 실행
     * 64비트 커널에서는 GRUB2 부트로더를 활용해 Long Mode 진입과 Protected Mode 전환 처리
     * stage2 부트로더가 GDT, IDT, 페이징 세팅 및 Multiboot2 포인터 할당 등 수행
     * 커널은 쉘 명령어 처리기 및 다양한 커스텀 컴포넌트로 구성됨

QEMU에서의 에뮬레이션 및 이미지

     * QEMU를 통한 가상 머신 환경에서 개발 및 테스트 진행
     * ISO 이미지 생성: grub2-mkrescue와 xorriso 사용
     * FAT12 플로피 이미지 생성 및 마운트 지원, 실제 장치 또는 QEMU 플래그(-fda fat.img)로 활용 가능

초기화 절차

     * 커널 진입 시 Long Mode, Multiboot2 태그, FAT12 파일시스템, VGA 상태 등 확인
     * ASCII 아트 로고 출력 후, 쉘 루프에 제어 이관

파일시스템

     * FAT12 파일시스템 지원: 파일 읽기/쓰기/검색/삭제, 디렉토리 생성/삭제 등 지원
     * 텍스트 파일 생성 및 덮어쓰기, 하위 디렉토리 지원
     * fsck 툴로 파일시스템 일관성 점검 기능 포함
     * 추후 FAT32 지원도 계획

네트워크 스택

     * SLIP 프로토콜 기반 IPv4 패킷 송수신
     * 이더넷 프레임 처리 지원(테스트 미완료)
     * ICMP Echo(Request/Reply), UDP, TCP(SYN/SYNACK 상태 머신) 등 지원
     * 간단한 HTTP 서버: 정적 HTML 페이지 제공

Snake 게임

     * Snake 게임 내장, 향후 멀티플레이(P2P TCP) 버전도 계획
     * 게임 데이터(레벨, 점수)는 텍스트 파일로 저장 및 불러오기 가능
     * ESC로 게임 종료, 점수에 따라 High Score 저장

프로젝트 가치 및 활용 포인트

     * Rust로 작성된 운영체제 예제로, 저수준 소프트웨어 개발의 안전성과 생산성 향상 효과 체감 가능
     * SLIP/ICMP 테스트, 간편한 배포 및 실기기 지원 등으로, OS 실험 및 커스텀 구현 학습에 적합
     * Rust와 x86 어셈블리가 결합된 DOS 유사 시스템 구조를 직접 경험할 수 있음

        Hacker News 의견

     * 메모리 안전성이 보장되는 언어 사용, x86_64 기반에 Arm 지원도 계획, 자체 네트워킹 스택 보유, CD 및 multiboot로 부팅 가능, 내 취미 프로젝트가 DOS를 압도하는 경험 제공
          + DOS와 경쟁하려면 Doom과 BASIC 실행 지원 필수, 이게 공식적으로 DOS 스타일의 기준선 역할
          + Rust와 x86 어셈블리 조합, 그래서 메모리 안전성 추구라면 과연 실용적 가치가 있냐는 생각, 오늘날 Rust는 한때 3D 프린팅처럼 과도하게 마케팅되는 느낌, 대중화 과정에서 실질적인 이득은 제한적이고 프로젝트도 구식 소프트웨어와 호환성 있으면 좋지만, 그렇지 않으면 교육용이나 매니아성 프로젝트에 가까움, 실사용까지는 갈 길이 멀게 느껴짐
     * 네트워킹 스택에서 SLIP과 slattach(1) 사용하는 점이 정말 마음에 듦, 직접 간단한 TCP/IP 스택을 만들 때도 Linux에서 pty로 SLIP 연결해 커널과 연동한 적 있음, macOS에도 예전에 slattach(1)이 존재했지만 지금은 제거된 걸로 보임, 혹시 macOS에서 SLIP 써서 크로스플랫폼 네트워킹 API 만든 분 있는지 궁금, 대안으로는 Linux에서는 tun/tap, macOS에서는 utun이 있지만 SLIP이 훨씬 간단함
     * 왜 x86을 선택한 건지 궁금, 리소스가 많아서인지, 독특한 명령어 포맷 때문인지, 부트 시퀀스의 복잡성 때문인지, 혹시 DOS를 그대로 따라하려는 전략인지, ARM 아키텍처도 곧 지원 예정이라고 했는데, DOS 자체가 하드웨어와 소프트웨어가 긴밀하게 연결되어 있는데도 여러 아키텍처로 지원하는 방식이 궁금
          + 이 프로젝트를 직접 보지는 않았지만 내 예상은, x86 플랫폼은 역사적으로 레거시 호환성 덕분에 정말 다양한 인터페이스가 내장되어 있어서 아주 얇고 심플한 ‘DOS류 OS’ 구현이 쉬움, 디바이스 트리 파싱, MMU 셋업, PCI(e) 같은 복잡한 버스 처리 등이 필요 없어 단순하게 시작 가능, ARM은 부트스트랩 자체가 어려워 심플함을 유지하려면 더 제약을 감수해야 하고, MMU 없이 할 수 있는 것이 제한적이고, 바이오스 인터페이스도 없어서 x86처럼 간단하게 섹터 읽기, 키 입력 대기가 어려움
          + x86을 선택한 이유는 이 시스템이 Turbo C에서 BIOS 인터럽트와 인라인 어셈블리를 기반으로 만든 첫 번째 버전에서 파생되었기 때문, MS-DOS만을 모방하려는 건 아니고 영감을 많이 받았음, 여러 아키텍처 지원은 Rust 컴파일러가 타겟 아키텍처 지정 기능 덕분에 가능성 있음, 빌드 전에 타겟만 정해주면 빌드 과정에서 바로 적용 가능
     * 현재의 UEFI 환경과 셸 대신 이렇게 FLOSS 기반 64비트 DOS 형식 솔루션이 더 좋았을 것 같음, 복고풍 부트 매니저나 시스템 진단 도구로 멋질 듯, 이게 efi 시스템 파티션에서 실행 가능할지 궁금, fat12 지원은 확인되는데 gpt 지원은 어떤지, 비디오 하드웨어를 곧장 제어하는 방식인지 아니면 터미널 형태 출력인지 궁금
          + 아직 EFI 시스템 파티션에서의 부팅은 테스트 전, FAT12 파일시스템만 공식 지원 중이고(메모리 디스크 기능은 있으나 현재 동작 안 함), gpt는 현재 미지원, FAT32 지원을 우선순위로 고려 중(보통 플래시 디스크는 FAT32 사용), 마지막 질문에 대해선 OS가 VGA 메모리 버퍼에 직접 쓰는 방식, GRUB에서 80x25 해상도 제공
     * 이번 프로젝트 멋지게 생각, “그냥 취미일 뿐, Linux처럼 크고 전문적으로 되지는 않을 거예요”라는 명대사를 놓친 게 아쉬움
     * 체코어 발음 부호 지원 계획이 있는지 궁금
          + 이번 버전에서는 영어만 지원 계획, 첫 번째 버전은 처음에 체코어로 만들어졌었음
     * Rust로 완전히 새로 만든 VGA 드라이버 사용 여부 궁금
          + 이 모듈이 VGA 관련 소스
          + 어느 정도는 맞긴 한데, 사실상 비디오 버퍼에 접근하는 래퍼 모듈에 가까움
     * DOS 스타일이지만 DOS와 호환성은 없는 게 맞는지 질문
          + 맞는 분석, 첫 번째 버전은 16비트로 MS-DOS와 거의 호환되게 설계, 그리고 간단한 디스크 I/O만 처리 가능하면 넓은 의미에서 DOS 시스템에 포함시킬 수도 있다 봄
          + 즉 MS-DOS 호환성, Alley Cat이나 Dune 2, Doom 실행 지원이 되는 수준 의미
     * 비동기 런타임 지원을 위해 이벤트 큐 필요 의견
          + 완성도 있는 이벤트 루프 구현은 어떤지 의견
     * Crysis 실행 가능성에 대한 유쾌한 질문
"
"https://news.hada.io/topic?id=21661","Cloudflare Containers - 'Earth' 리전으로 배포하는 서버리스 컨테이너 플랫폼 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Cloudflare Containers - 'Earth' 리전으로 배포하는 서버리스 컨테이너 플랫폼

   Cloudflare가 6월 24일 새 컨테이너 실행 플랫폼 'Cloudflare Containers'를 Public Beta로 공개했습니다.
     * 모든 유료 플랜에서 사용 가능, wrangler deploy 한 번으로 Earth 리전(전 PoP)에 자동 배포(Pre-provisioning)
     * Workers와 완전 통합되어 인증/캐싱/라우팅을 Workers가 처리하고 무거운 작업은 컨테이너로 위임 가능
          + Durable Object API로 .start(), .exec(), .signal() 등 컨테이너 수명주기를 코드에서 직접 제어 가능
     * 인스턴스는 요청 시 수 초 내 기동 (cold start: 2-3s - 이미지 크기 등에 따라 달라짐), sleepAfter 값에 따라 일정 시간 내 추가 요청이 없을 경우 sleep 상태로 자동 진입
     * 내부적으로는 gVisor/Firecracker micro-VM (AWS Fargate가 사용하는 VM)/QEMU 등 런타임을 선택적으로 사용해 격리
     * 인스턴스 타입: 현재는 dev (256MiB, 1/16 vCPU, 2GB), basic (1GiB, 1/4 vCPU, 4GB), standard (4GiB 1/2 vCPU, 4GB) 3종 인스턴스 제공 - 더 큰 인스턴스 및 GPU 지원 예정
     * 인스턴스 요금: 메모리 $0.0000025 / GiB-sec(월 25 GiB-h 포함), CPU $0.000020 / vCPU-sec(월 375 vCPU-min 포함), 디스크 $0.00000007 / GB-sec(월 200 GB-h 포함) — 사용량은 10 ms 단위로 측정해 과금
     * 네트워크 egress 요금: 북미·EU $0.025/GB(1 TB 포함), 오세아니아/한국/대만 $0.050/GB(500 GB 포함), 그 외 지역 $0.040/GB(500 GB 포함)

   Earth region 낭만 합격!

   다음은 Moon 리전인가요
"
"https://news.hada.io/topic?id=21580","삼성, WANA (서아시아와 북아프리카) 전역의 휴대폰에 IronSource 스파이웨어 앱 내장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         삼성, WANA (서아시아와 북아프리카) 전역의 휴대폰에 IronSource 스파이웨어 앱 내장

   SMEX와 파트너들은 서아시아 및 북아프리카(WANA) 지역의 삼성 A 및 M 시리즈 스마트폰에 AppCloud 애플리케이션이 강제로 사전 설치되는 문제에 대해 삼성에 공개 서한을 보냈습니다.

   주요 내용은 다음과 같습니다:
     * 비동의 설치 및 데이터 수집: 이스라엘에 설립된 ironSource가 개발한 AppCloud는 사용자 동의 없이 설치되며 민감한 개인 데이터를 수집합니다.
     * 제거의 어려움 및 투명성 부족: 이 애플리케이션은 제거하기 어려우며, 개인정보 보호 관행에 대한 투명성이 부족합니다.
     * 데이터 보호법 위반 가능성: AppCloud의 설치는 GDPR 조항 및 WANA 지역의 데이터 보호법을 위반할 수 있습니다.

   서한은 삼성에 다음을 요청합니다:
     * AppCloud의 전체 개인정보 처리방침을 공개합니다.
     * 사용자가 앱을 거부하고 제거할 수 있는 방법을 제공합니다.
     * 향후 기기에 사전 설치하는 것을 재고합니다.

   자세한 내용은 삼성에 보내는 공개 서한에서 확인할 수 있습니다.

   참고: Hacker News

   뭔 중국제도 아니고 이러면 안되지 않나

   appcloud가 뭔가 했는데 외국 버전 갤럭시에선 예전부터 있었나 보네요 #

   adb shell pm uninstall 하면 지워진다는데(disable) 그냥 시스템 내장 앱 중 하나인 듯...
"
"https://news.hada.io/topic?id=21569","맨해튼 혼잡통행료 정책의 예측 가능한 성공","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        맨해튼 혼잡통행료 정책의 예측 가능한 성공

     * 맨해튼 혼잡통행료 정책이 도입된 후 긍정적인 효과가 나타남
     * 교통 혼잡 완화와 대중교통 이용 증가 현상이 관찰됨
     * 예상한 대로 통행 시간이 단축되고, 도심 내 환경 개선 효과가 발생함
     * 상업적·경제적 활동에 미치는 영향도 전반적으로 긍정적임
     * 유사한 정책 도입을 검토하는 도시들에게 중요한 참고 사례가 됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

맨해튼 혼잡통행료 정책의 의의

     * 맨해튼의 혼잡통행료(컨제스션 프라이싱) 정책은 도심 내 교통 혼잡 문제 해결을 목표로 시행됨
     * 이 정책은 차량 운행량 감소와 대기오염 저감을 동시에 달성하고자 계획됨
     * 도입 후 교통 흐름 개선과 대중교통 수요 증가가 통계적으로 입증됨
     * 예상대로 통행 시간 단축과 더불어 보행 환경 및 대기 질 개선 효과가 나타남

부가적 경제 및 사회적 효과

     * 도시 내 비즈니스 환경이 개선되어 상업적 활동 원활화 경향을 보임
     * 대중교통 수입 증가로 인해 서비스 개선 및 이용자 편의성 증대 가능성 확보됨
     * 유사한 교통 문제로 고민하는 해외 여러 도시에게 정책적 벤치마크로 인식됨

전망 및 시사점

     * 맨해튼의 사례는 데이터 기반 정책 입안의 성공적 모델로 평가받음
     * 도시계획 및 스마트시티 관련 분야에서 교통 관리 알고리듬 개발의 실제 적용 결과임
     * 지속적인 정책 효과 모니터링 및 기술적·사회적 피드백 수집이 중요한 연구 대상임

        Hacker News 의견

     * 혼잡 통행료 정책 덕분에 금요일 오후 5시에 맨해튼의 Canal Street이 예전처럼 차량이 꽉 차서 경적을 울리고 매연이 가득한 상황이 아니라 여유로운 일반 도시 풍경을 경험함, 예전에는 금요일 밤마다 값비싼 차를 타고 Lower East Side로 와서 시끄러운 음악을 틀고 엔진 소리를 울리던 사람들이 사라짐, 주변인들도 모두 긍정적으로 체감 중인 분위기 경험
          + 런던의 경우 혼잡 통행료가 오후 6시에 끝나서 금요일이나 토요일 밤이면 여전히 값비싼 차를 몰고 시끄러운 차량 엔진과 음악을 뽐내려는 사람들이 나타남, 뉴욕과는 상반된 상황 언급
          + 뉴욕은 아니지만 정책 진행 과정을 지켜봤고, 값비싼 차량과 시끄러운 문화가 줄어드는 효과는 미처 생각 못 했던 이점임을 새롭게 인식함
          + 혼잡 통행료만으로는 절반의 해결책일 뿐, 나머지는 MTA 개혁이 필요함, MTA는 기억하는 한 늘 비효율적이고 예산만 낭비하는 구조였고, 지금 모습으론 혼잡 통행료로 걷은 재정도 낭비될 우려 언급
          + 혼잡 통행료 도입의 긍정적인 변화가 반가움, 토론토에서도 도입해보고 싶은 마음, 학업 프로젝트에서 이 정책을 제시했더니 비웃음 당함
          + 값비싼 차를 가진 사람들이 혼잡 통행료 때문에 deter되는 집단이 맞는지, 비싼 차 소유주가 시끄러운 엔진과 음악을 뽐내는 사람들인지 의문 표출
     * 이 정책에 불리한 결과를 보여주는 지표나 실측 결과가 있는지 궁금함, 주제가 나올 때마다 반드시 단점이 있을 것이라는 의견이 나타나지만 실제로는 측정된 결과 중 나쁘게 나온 것이 거의 없음 강조
          + 혼잡한 상점가 도로에 자전거나 버스, 더 나은 보행 환경을 위해 주차 공간을 줄이려 할 때 마다 상인들이 반대하며 재앙을 예언하지만, 실제로는 통행량과 매출이 오히려 상승하는 사례 경험, 상인들의 '고객을 안다'는 믿음이 실제론 상당 부분 편견이나 주차 필요성에 대한 자신만의 이해에서 비롯됨을 체감
          + 미국은 계급 구조가 뿌리 깊고, 대중교통 이용이 하위 계급의 전유물로 여겨짐, 대중교통이 잘 구축되면 얼마나 효율적이고 좋은지 https://www.youtube.com/watch?v=bNTg9EX7MLw (NotJustBikes 영상) 링크 소개
          + 맨해튼 혼잡 통행료 도입에 대한 반대는 신기하게도 해당 지역을 자주 방문하는지와 반비례함, 일종의 시골 유권자의 대도시 반감이 투영된 이슈라고 판단
          + 이 프로젝트는 10년간 연구돼왔기 때문에, 반대자들이 주장할 근거가 빈약하다고 생각함, 반대 의견의 대부분은 도시에 차량을 몰고 들어가는 것이 일종의 권리나 뉴요커 특권이라는 낭만적 인식에서 기인함
          + 정책 시행 후 실제로 맨해튼으로 들어가는 경로의 통행 비용이 상승한 것을 감안하면, 손해 보는 사람도 어느 정도 존재할 것이라고 추정함, 하지만 전체적으로 보면 비용 대비 효과가 매우 큼
     * 새로운 기사 아카이브 링크 공유
     * 자전거가 도로 활용 효율성 면에서 차량보다 약 5배 더 효과적임, 자전거로 이동하는 사람이 많을수록 거리가 훨씬 덜 붐비는 모습, 동일한 수준의 '혼잡'을 만들려면 현재 차량 교통량의 5배 만큼 자전거가 필요함, 도로가 '덜 붐비는' 것이라는 표현은 결국 '차량이 적다'는 뜻임을 강조
     * 대부분 극적인 변화라고 하지만 교통량이 약 10% 감소했다는 수치라면 개인적으로는 그다지 극적이지 않게 느껴짐, 교통량이 임계치를 지난 상태에서는 아주 작은 변화만으로도 통행 시간이나 정체에 큰 효과가 나타나는 구조인지 의문 제기
          + 맨해튼의 교통량은 수용 한계에 가까웠기 때문에 정체가 심했음, 전체 교통량의 10%만 줄어도 흐름이 확연히 나아지고, 버스 등 대중교통에도 즉각적으로 긍정적 영향 발생
          + 도로 정체에 임계점이 존재하고, 그 지점을 조금만 벗어나도 체감 변화가 극적임을 학교 정체를 예로 들며 설명
          + 유체 흐름과 비슷하게 '초크 플로우(임계 흐름)'에 다다르면 저항이 급격하게 증가함, 도로 정체가 비선형적으로 악화되는 현상 언급
     * Relevant Climate Town Video 영상 소개
     * 런던처럼 시간이 지나면 혼잡도가 다시 올라올 거라고 생각, 실질적으로 억제하려면 훨씬 더 높은 요금(예: 150~250파운드/일)이어야 함, 그러나 그렇게 하면 수입이 줄어들어 실제로 인상하진 않을 것, 현재로선 두 명이 외곽에서 지하철 타는 요금보다 차량 혼잡 요금이 저렴해서 사람들이 여전히 차를 선택함, 대중교통의 편의성 등까지 고려하면 차량이 선호됨
          + ""아무도 안 갈 만큼 비싸면 수익이 줄어서 인상할 수 없다""는 논리 지적, 사실상 수요가 줄 때까지 요금을 계속 올릴 수 있음
          + 혼잡이 다시 올라가는 근본 원인은 정부가 더 높은 밀도의 주택 건설, 대중교통/도로 인프라 확충 등 구조적 해결을 하지 않기 때문임, 혼잡 통행료 도입 후에는 '당분간 해결된 것'처럼 보여 다른 근본적 개선 노력이 지연됨, 인구가 늘거나 인프라가 노후화되면서 결국 혼잡이 원상복귀되고 사람들은 감시와 추가 세금만 떠앉게 되는 구조
          + 시드니의 경우 혼잡에 따른 통행료가 아니라 추가 세금 수준이고, 실질적으로 차량 이용을 막지는 못함, 오히려 보행자 거리와 주차 공간 축소로 운전이 불편해질 때 차량 이용 감소 효과 발생
     * 혼잡 통행료 자체는 괜찮게 받아들이지만, 대부분 60번가 위에 주차해서 세금 회피, 터널 통행료가 지나치게 비싸게 느껴짐
     * 치명적인 바이러스도 혼잡을 줄일 순 있지만, 실제 영향을 받는 이들에게는 전혀 편리하지 않음 질문
"
"https://news.hada.io/topic?id=21615","Signal - WhatsApp의 윤리적 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Signal - WhatsApp의 윤리적 대체제

     * WhatsApp은 Meta(전 Facebook)와의 데이터 공유 및 프라이버시 문제로 비판을 받음
     * Signal은 비영리 재단이 운영하며, 데이터 보호 및 투명성 부문에서 높은 평가를 받아, WhatsApp의 윤리적(ethical) 대안으로 주목받음
     * Meta 및 Zuckerberg의 일관성 없는 행보와 선거 및 사회 영향이 우려되는 상황이므로, WhatsApp에서 Signal로 전환 해야함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

윤리적 대안으로서 Signal

     * WhatsApp에서 Signal로 이동하는 것은 현재 시점에서 중요한 선택임
     * WhatsApp의 대안은 여럿 있지만, 다양한 문제로 인해 Signal이 중요한 선택지로 부각됨
     * Signal는 설치와 사용이 간단하며, WhatsApp과 비슷한 사용자 경험을 제공함
     * 모든 연락처가 이미 Signal을 쓰는 것은 아니지만, 개인의 실천이 사회적 변화로 이어질 수 있음.

WhatsApp 및 Meta의 윤리 문제

   WhatsApp 사용의 가장 큰 문제는 Meta(구 Facebook) 와 창립자 Mark Zuckerberg에 관련이 있음.
     * 2021년 WhatsApp은 서비스 약관을 변경하여 Facebook과의 데이터 공유를 필수로 요구함
          + 네트워크 정보, 위치 데이터 등 다양한 메타데이터가 Facebook으로 넘어감
     * WhatsApp은 적지 않은 메타데이터를 법 집행기관과 공유하는데, Signal은 의도적으로 이를 거의 수집하지 않음
     * 법적 문제뿐 아니라 Meta는 유저와 규제 기관을 기만한 혐의로 EU와 미국에서 거액의 벌금을 부과받음

Meta와 Zuckerberg의 행보

   Meta는 부정적인 이미지를 탈피하기 위해 사명을 변경한 기업임.
     * Facebook-Cambridge Analytica 스캔들에서 이용자 동의 없이 데이터를 수집하고, 이 데이터로 정치 심리 프로파일을 만들어 선거에 활용함
          + 해당 데이터는 선거 캠페인, 특정 유권자 억제 등에 사용됨
     * WhatsApp은 2018년 브라질 대선에서 허위 정보 유포의 주요 플랫폼 역할을 했음
     * Zuckerberg는 미 의회에서 실책을 인정했지만, 이후 행보에서 일관성 없는 태도와 대중발언으로 논란을 키움

Signal의 윤리적 평가

   Signal은 WhatsApp처럼 간편한 메시징 경험을 제공함.
     * Signal LLC 현 CEO인 Brian Acton은 WhatsApp 공동창업자임
          + Facebook이 WhatsApp을 인수한 후, 데이터 및 광고 활용에 이견이 있어 회사를 떠남
          + Signal Foundation 설립에 5천만 달러를 출연하고, Signal Messenger LLC를 세움
     * Signal Foundation은 501(c)(3) 비영리단체로, 사명은 ""개인 프라이버시가 보장되는 커뮤니케이션의 보편화""임
     * Electronic Frontier Foundation의 메시징 프라이버시 및 투명성 평가에서 만점을 기록함
     * Signal Foundation 리더 Meredith Whittaker는 감시 자본주의와 권력 집중 비판자로 AI Now Institute 활동을 이끌어 옴
     * Signal 전신 Open Whisper Systems는 Freedom of the Press Foundation의 언론자유 지원 기금을 받음
     * 미국 민주당과 상원 등에서 공식 추천받은 메시징 도구임

   Signal과 WhatsApp의 Green Stars 윤리 평점은 각각 4.5점, 0.5점으로 큰 차이가 있음

WhatsApp에서 Signal로 이동 전략

     * WhatsApp은 거대한 사용자 기반과 편의성으로 사용자가 정착하게 되었음
     * 편의성, 사회의 규범에 익숙해져 진정한 문제를 외면하게 되는 경향이 있음
     * 전환 방법은 간단함:
          + Signal을 설치, 계정 설정(몇분 소요)
          + 기존 WhatsApp 대화 상대가 Signal을 사용한다면 그쪽에서 계속 대화
          + 상대가 미사용자라면 일반 문자로 대화 후 Signal로 전환을 제안
          + 그룹챗을 활용해 여러 명이 한 번에 이동 유도
     * 민주주의 선거 개입과 사회적 문제를 일으킨 플랫폼에 대한 문제 인식이 필요함

마무리 인용

   미 정치인 Alexandria Ocasio-Cortez 언급:

     Meta는 ""민주주의에 암처럼 전이되어 감시, 선전, 권위주의 정권 지원, 시민 사회 파괴를 통해 이윤을 추구하는 글로벌 머신임""

        Hacker News 의견

     * 나는 2021년에 WhatsApp을 그만두고 먼 거리에 있는 몇몇 친구들을 잃기도 했지만, 주로 DeltaChat, XMPP(Jabber), Signal 순서로 소통 중임. 다른 대안으로는 SimpleX 등도 있지만 다수는 여러 기기 간 채팅 동기화가 어렵다는 문제가 있음. DeltaChat은 WhatsApp을 쓰던 사용자에게 익숙한 UI라 진입 장벽이 낮음. 개인정보 공개 없이 쓸 수도 있는 멋진 앱이라 소개함 DeltaChat 공식 사이트
          + DeltaChat이 이메일을 기반으로 하는 건 신기한 장점이면서 동시에 단점임. 이메일 서버 운영이 정말 까다롭다는 점에서 망설여짐. 직접 Matrix나 Mastodon 서버, BlueSky PDS는 돌려봤지만 이메일 서버는 시도도 못 했음. 대부분의 사용자는 결국 Gmail 등 대기업 메일 서비스에 기대야 하고, DeltaChat을 쓰려면 앱 비밀번호 설정 등 복잡한 과정을 거쳐야 하는데, 이 단계에서 많은 사용자가 이탈함. ChatMail 릴레이라는 것도 있는데, 이메일과 연동된다는 설명이 명확하지 않고, 대규모 메일 서비스에 금방 블랙리스트 오를 수도 있음. 이메일이 진짜 적합한 기반이었는지 다시 의문이 드는 상황임
          + 친구를 잃었다는 이야기가 슬프게 느껴짐. 오래된 친구들과의 연락은 더 우선시해야 하는 일이라는 생각임
          + 친구를 잃었다는 점은 안타깝지만, 중요한 대화가 폐쇄적인 플랫폼에 묶이면 결국 모두 사라지는 것임을 상기시켜줌. 나는 모든 소중한 연락을 이메일로 유지 중. 오픈 스탠더드라 사라질 일이 없음. 내 이메일 주소는 30년째 변함없이 그대로 유지됨
          + WhatsApp 사용을 끊으려 해봤지만 실패함. 대부분 Signal을 추천했지만, 내 핵심 대화 상대 외에는 여전히 WhatsApp만 사용. Telegram이나 Instagram 같은 더 부적절한 메신저 쓰는 사람도 많음. 당장 대체할 방법이 없어 WhatsApp이 그나마 덜 나쁜 선택임. 언젠가 마지막으로 남은 대형 빅테크 서비스를 떠날 수 있기를 희망함
          + n0_computer 및 iroh.computer 팀이 DeltaChat에 참여함. P2P 기술이나 관련 Rust 라이브러리에 관심 있다면 확인 추천. 유튜브에 설명 영상도 많음. 개인적으로 팬임
     * 여기(영국)에서는 대안을 깔아도 아무도 관심 없음. 설치를 설득하는 것도 어려움. 사람들이 신경 써주려면 ‘Zuck eats children and owns WhatsApp’ 같은 대대적 마케팅이 필요하다고 느껴질 정도임. 가족과 친구 몇 명은 있지만 1년간 노력해 그 정도만 됨
          + 의견이 다름. 나 역시 영국인데, WhatsApp과 Meta의 본질에 동의하지 않아 오래전부터 Signal을 씀. 처음엔 나 혼자였지만, 친구들과 가족들이 하나둘 합류하기 시작. 내 윤리적 이유를 모두 알고 있고, 지금은 의미 있는 연락처 20여 명이 Signal에 있음. 의미 있는 연결을 지속한다면 충분히 가치 있는 일이라고 생각함. 비즈니스도 Signal 사용을 주장하면 나 역시 다른 수단을 고집함
          + 이런 앱들은 모두가 같이 써야 쓸 수 있음. 많은 사람을 한꺼번에 옮기지 못하면 논쟁조차 무의미함
          + 나도 비슷한 경험 함. 몇몇 개인이나 소규모 그룹 설득은 가능하지만, 아이 운동팀 부모님들처럼 필수 그룹은 거의 불가능함. 최소한의 노력을 추구하는 인간 본성 때문에, 결국 WhatsApp이 자연스러운 선택으로 자리잡게 됨
          + Green Stars라는 사이트에서는 Meta와 WhatsApp이 정치적, 개인정보 유출 이슈로 문제라고 주장하는데, 일반 대중에게는 너무 멀게 느껴지는 얘기임. 실제로 대부분 사람들은 광고가 너무 많아지면 그때서야 옮길 동기가 생김
     * Signal을 시도해봤지만 채팅 내보내기가 안 돼서 데이터가 앱에 갇히는 문제가 있었음. 이 부분이 고쳐졌는지 궁금
          + Signal의 단점이 많고, 나 역시 백업 문제로 불만이 큼. iOS ↔ Android 간 채팅 기록 이동 불가, 사진 원본 타임스탬프 전송 불가, 자동 사진 저장 미지원, 한 대의 데스크톱만 연결 가능, 설문/실시간 위치 등 위젯 부재, 음성 메시지 변환/플레이 호환성 문제 등 불편함 다수. 가족을 Signal로 옮긴 걸 후회할 정도임
          + 채팅 백업 문제는 기술이 아닌 사회적 문제라고 생각함. 대화는 본래 일회성이어야 하고, 영구 기록보단 한 달 정도 메신저, 5년 정도 이메일 기록이 충분하다고 생각함. 기록 욕구와 프라이버시 욕구가 어긋남
          + ""채팅 백업"" 기능을 찾고 있다면 이미 제공된다는 설명임
     * Signal을 설치하려다, 번호 인증 이후 구글 캡차가 나와 개인정보가 구글로 보내진다는 점이 이상하게 느껴졌음. 개인정보 처리방침을 확인했지만 특별한 데이터 공유 명시는 없었음. Signal이 메시지 암호화에는 강하지만, 사용자 프라이버시 측면은 제대로 지키지 않는다는 인상임 관련 캡차 이슈 시그널 개인정보처리방침
          + Signal은 프라이버시와 암호화 모두로 유명함. 반면, WhatsApp은 연락처 전체를 Facebook과 공유해야 함. 정부와의 백도어 접근 가능성도 높음
          + Signal은 애초에 대체 안드로이드 SMS 앱에서 시작해, 폰번호가 서비스의 기본이었음. 현재는 닉네임만으로도 메시징이 가능하도록 구조를 바꾸는 중. 폰번호와 캡차 요구는 스팸 방지 목적임. 프라이버시에 관해서 Signal은 Giphy 검색 기능에서도 IP 노출이나 검색어 유출을 방지할 만큼 노력함 관련 글
          + 내 전화번호가 빅테크에서 사실상 UUID라는 점이 문제인데, Signal도 이메일/비밀번호 기반 가입을 지원하지 않아 아쉽게 느껴짐
          + WhatsApp 기준에 비하면 Signal을 너무 엄격하게 평가한다는 의견임
          + 개인정보 처리방침을 봤을 때 데이터 공유를 하지 않는다면, 문제 삼던 근거가 사라진 것 아니냐는 반응임
     * Signal이 사용하기 힘들다고 느낌. 기기 간 동기화가 자주 끊기고, 로그인을 반복해야 해서 전환 시도가 번번이 불발됨
          + 예전 폰에서 새 폰으로 Signal 이전하려다 실패 후 1년 넘게 계정이 망가짐. 예전 버전 문제가 발생했는데, Signal 앱에서 구체적인 안내가 없었음. 최근엔 기기 재등록을 시도해도 인증 코드 입력하면 계속 앱이 멈춤. 결국 메시지 기록·사진 모두 접근 불가 상태. 업데이트로 고쳐지길 기다리는 중이지만 실망스러운 경험임
          + 여러 장치에서 Signal을 수년째 잘 쓰고 있지만 이런 문제는 겪지 않았음
          + 불편한 점이 많긴 한데, 마땅한 대안이 없어서 어쩔 수 없다고 느낌
          + 나는 동기화가 오히려 잘 된다고 생각함. macOS/iPhone 메시지보다도 동기화가 좋음. 1년 넘게 로그인 문제도 없음
          + iOS와 데스크탑에서 Signal을 메인으로 사용 중인데, 음… 대략 한두 달마다 한 번씩 동기화 문제 겪음. 불편하긴 해도 그냥 감수하고 있음
     * Signal은 중앙 집중식이고 자체 서버를 직접 운영하기도 현실적으로 힘듦. Matrix, Revolt, DeltaChat 등은 자체 서버 운영이 비교적 쉬움. Signal은 전화번호 인증이 필요해서 실제로는 그리 프라이빗하지 않음. 중앙 집중형 플랫폼의 메타데이터, 힘의 집중 이슈, 향후 제공될 레버리지를 고려해야 플랫폼의 윤리성과 장기적 위험을 평가 가능함
          + signal-server 저장소가 오픈 소스로 공개되어 있다는 언급임
          + 메타데이터 수집 관련해서는 Matrix.org 같은 서버도 메시지를 전달하려면 유사한 정보를 모두 볼 수밖에 없다는 점을 상기함
          + Riot이 이제는 Element로 바뀌었다는 안내임
     * 핸드폰이 필수인 메신저는 내겐 답이 아님
          + 위협 모델에 따라 다름. WhatsApp도 폰 필요하니, Signal은 WhatsApp 대체 목적으로 충분히 가치 있는 선택임
          + 전화번호는 필요하지만 실제 폰 없이도 PC에서 signal-cli로 등록 가능. 컴퓨터에 모뎀이 있으면 SMS로 인증받거나, 음성 코드로 유선전화도 등록 가능
          + Session이라는 메신저가 대안임 Session 공식 사이트
          + Signal에는 데스크탑 앱도 있지만, 혹시 ‘핸드폰’이 아니라 ‘핸드폰 번호’가 문제라는 얘기라면 Signal이 최근 닉네임 기반도 허용하기 시작했다는 첨언
          + DeltaChat이라는 것을 최근 알게 됐고, 이메일 기반이라 괜찮아 보인다는 의견임
     * Signal은 iOS에서 채팅 백업/복원 기능이 없어 불편함. 내겐 채팅이 수년간의 미디어 다이어리임. 백업/복원은 필수적인 기능인데, 이를 제공하지 않으면 WhatsApp을 대체했다고 보기 어렵고 그저 또 다른 대안일 뿐임
     * 아쉽게도 Signal은 최적화된 웹 클라이언트를 영원히 내놓지 않을 것 같아 슬픔. 지금은 데스크톱용 Electron만 있음 관련 이슈
          + 전통적인 웹앱을 제공하면 정부 기관의 요구로 악성 코드를 심은 버전을 사용자에게 제공하도록 강요받을 수 있음. 반면, 서명된 데스크톱/모바일 앱은 Signal에서 임의로 코드 변조를 할 수 없기 때문에 보안 측면에서 더 신뢰할 수 있음
     * Signal은 다른 사용자를 동일한 앱과 서비스로 강제하기 때문에 이상적인 윤리적 대안은 아니라고 생각함. Matrix나 XMPP 등 분산형 옵션이 더 많은 자유를 제공함
          + 그 이상은 매우 이상적이지만 현실에서는 기술 커뮤니티에만 국한된 이야기임. Signal 역시 포크 버전이 가능함. 윤리적 기준이 중요하다면, 서비스가 사용자의 가치관과 맞는지 판단해 결정해야 함
          + 나도 전적으로 동의함. 현대 OS에서 모두 쓸 수 없는 메신저는 나에겐 인정하기 어려움. 예를 들어, OpenBSD에서 네이티브로 Signal을 쓸 방법이 없어, 억지로 리눅스 VM을 돌려야 하는 점이 불만임
"
"https://news.hada.io/topic?id=21616","교토에서 만난 뒷마당 커피와 재즈","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           교토에서 만난 뒷마당 커피와 재즈

     * 일본 소규모 개인 사업장의 매력과 저진입장벽 문화를 소개하는 여행 경험 공유임
     * 교토의 주택가 뒷마당에 위치한 작은 커피숍을 방문하여 공간의 특별함과 분위기를 체험함
     * 낮에는 커피숍, 밤에는 바(Bar)로 운영되며, 주인 한 명이 직접 모든 것을 관리하는 구조임
     * 공간은 작지만 재즈 음악, 빈티지 오디오, 옛 분위기가 돋보이는 환상적인 경험을 제공함
     * 이러한 소규모 상점이 도시의 인간적이고 자유로운 상업문화를 만들어내는 핵심 요소임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

일본의 소규모 비즈니스와 개인 공간 경험

     * 일본 여행을 준비하며 사람들의 열정이 듬뿍 담긴 소규모 바, 이자카야, 서점, 음반샵 등 다양한 형태의 개성 넘치는 작은 가게들에 대해 알아봄
     * 교토 방문 중 이러한 소규모 가게 몇 곳을 직접 방문하고, 거리에서 더 많은 가게들을 구경함
     * 일본 도시의 생동감 넘치는 비즈니스와 거리 문화, 그리고 일반인이 쉽게 창업할 수 있는 환경이 인상 깊음
     * 이번 글에서는 그러한 가게 중 하나를 중점적으로 소개함

주택가 뒷마당의 커피숍 발견

     * 교토에서 관광 전 지역 커피숍을 찾다가 Google Maps로 가까운 곳을 검색하고, 건물 사진을 보고 방문 결정함
     * 겉보기엔 무척 작으나 건물 사이 복도를 따라 확장된 구조로 되어 있음
     * 실제로 누군가의 집 진입로에 위치한 작은 오두막에서 낮에는 커피숍, 저녁에는 기본 맥주·위스키를 파는 바로 변신함
     * 일본에서는 미국보다 술 판매가 더 자유로움

음료 경험과 공간 연출

     * 여러 종류의 원두/로스팅 메뉴에서 각각 다른 품종을 선택해 주문함
     * 주인장이 손수 원두를 계량·분쇄하고, 핸드드립 방식으로 커피를 내려줌
     * 내부에는 오래된 커피 그라인더와 빈티지 Denon 턴테이블, 앰프 등이 전시·사용되어 있음
     * 공간은 최대 12명 정도만 수용 가능하나, 내부는 예상보다 넓고 아늑하게 느껴짐
     * 재즈 레코드가 흘러나오며, 따뜻한 색감의 전구 조명으로 오래된 공간만의 분위기 연출을 느낄 수 있음

일본 소상권 문화와 감상의 특별함

     * 노후된 건물이지만 老朽感 대신 특유의 정취와 시간성이 느껴짐
     * 내부에 들어서면 바깥과 단절된, 과거로 들어가는 포털 같은 기분을 경험함
     * 이 공간에는 1960년대에 머무른 채 멈춘 듯한 시간성이 존재함
     * 대형 상업 공간에서는 잃어버리기 쉬운, 손님이 아닌 진짜 '손님'이 되는 감각을 줌
     * 도보로 이동 가능한 도시의 작은 상점은 방문 자체가 자연스럽고 불필요한 마찰이 적음

소규모 자유 창업의 의미

     * 취미·열정을 약간 상업적으로 실현할 수 있게 하는 환경이 일본 소상권의 뚜렷한 특징임
     * 진입장벽이 매우 낮아 누구나 시도 가능하며, 크게 위험을 감수하지 않아도 됨
     * 이는 규제 장벽이 두껍지 않고, 작고 지역적인, 아름다운 자유시장이 성장할 수 있는 토양임

맺음말과 경험의 본질

     * 커피 역시 훌륭한 품질을 경험함
     * 여행 중 일상에서 벗어나 새로운 호기심과 즐거움을 경험하는 마음 상태도 이러한 감각을 더욱 배가시킴
     * 소규모 사업장의 '초대받는 듯한' 감각이 일본 도시의 독특한 경제문화로 작동함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   관련 읽을거리
     * I Am Here As You Are Here
     * A Peek at What’s Possible
     * Three Cheers For The Blue & White
     * The Wolverine Claws

        Hacker News 의견

     * 일본 도시의 활기찬 비즈니스와 거리 문화, 그리고 일반인이 쉽게 참여할 수 있는 낮은 진입 장벽에 대해 주목하게 되는 의견임. 시장의 자유로운 운영이 규제와 과도한 허가제도 없이 이뤄질 때 다양한 취향과 개성이 살아남고 번성할 수 있다는 점을 깨달음. 멜번과 시드니의 소규모 바 신(Scene)도 비슷했다고 느낌. 시드니는 비싼 허가비로 소규모 바 창업이 힘들었지만, 멜번은 저렴하고 포용적인 정책 덕에 독특하고 흥미로운 공간이 넘쳤음. 코로나 이전까지 운영하다가 사라진 작은 인디 게임 바가 최고의 사례였음. 링크 참고
          + 미국 보스턴도 비슷한 예시가 있음. 역사적으로 밤문화가 활성화되지 않은데, 이유가 명확함. 주정부가 주류 허가를 제한적으로 발급하며, 매년 한정된 수량만 추가됨. 그래서 신규 사업자는 기존 허가증을 매우 비싼 값(평균 50만불 이상)에 사야만 함. 이로 인해 대형 프랜차이즈만이 진입장벽을 넘을 수 있고, 독특한 소형 업장들은 기회조차 얻지 못함
          + 북미의 도시 설계자들이 우리가 모르는 사이에도 삶을 방해하고 있다는 생각임
          + 부동산 요인도 큰 영향을 준다고 생각함. 연간 50만 명 이상 인구가 감소하며 임대료와 토지 가치의 압박이 다소 완화되는 효과가 생김. 뉴욕에 살 때 동네 소상인들이 임대료 상승 때문에 문을 닫는 걸 자주 목격했고, 결국 임대료를 감당할 수 있는 건 스타벅스, H&M 등 고마진 체인뿐이었음
          + 오사카의 해당 바(spacesbar로 기억)에 대한 위치 정보 공유. 여전히 영업 중이며, 복고풍 게임 바로 매우 멋졌던 장소임 링크
     * 가나자와에서 재즈 이자카야를 방문한 경험을 공유함. 좌석은 단 두 개, 나이 지긋한 바텐더와 할아버지 손님이 있었음. 팁은 예의에 맞지 않고 주지도 않지만, 바텐더에게 술을 사주는 것은 괜찮은 문화임. 일본산 위스키 주문 후, 세 사람이 조용히 40분 동안 소니 롤린스의 LP를 함께 들음. 기계식 영수증 발행기를 쓰는 구멍가게부터 쿠션만 깔려 있는 구석 카페 등 일본만의 독특한 바와 카페의 분위기가 인상적이었음. 교토의 Brown Sugar, 삿포로의 Jim Crow와 Half Note 같은 곳도 추천함. 대부분 일본어를 못 하면 입장이 어렵다고 하지만, 때때로 우연히 들어가게 되는 경험이 있음. 흑인 왕족 출신의 교토대 학생 친구들도 언어, 인종과 관계없이 출입이 제한된 경우가 많다고 함. Jim Crow, Half Note
          + Craig Mod라는 작가의 인터뷰를 최근 들었음. 일본을 도보로 탐험하며 사진과 책을 만들었고, 그의 경험 속에서도 이런 스타일의 공간들을 ‘키사’(kissa)라고 부름 Craig Mod 링크
     * ‘놀라움’이라는 감정을 일본의 비즈니스 풍경과 도시 환경이 자주 선사한다는 의견에 공감함. 금요일 스탠드업에서 주말 계획을 이야기할 때마다 “뉴욕이 나에게 벌어지게 둘 거야”라는 식으로 말하지만, 임대료 상승으로 개성 있는 소규모 매장은 점점 사라지고 다국적 자본의 체인으로 대체되고 있다고 느낌. 그래도 뉴욕은 미국 내에서는 그 ‘놀라움’을 아직 지닌 도시임. 과거 동네에 ‘House of Small Wonder’라는 일본풍 카페가 있었는데, 지금은 Glossier 화장품 매장으로 바뀌었음
          + 미국에서 방문한 도시 중에서는 뉴욕(그리고 아마 시카고)이 유일하게 ‘진짜 도시’라고 부를 만한 곳임. 해외 여러 도시, 특히 일본을 여행하며 미국 도시의 현실이 진심으로 안타깝게 느껴짐. 미국이 많은 것을 놓치고 있다는 확신이 생김
     * 일본의 작은 바·카페들은 ‘키사’ 또는 ‘재즈 키사’라는 이름으로 불리며, 단순 카페와 달리 음악 감상에 집중할 수 있는 분위기를 조성함. Chris Broad(Abroad in Japan)가 이치노세키의 Basie라는 키사 오너를 인터뷰함 Jazz kissa 위키, Basie 유튜브 인터뷰. 스타워즈 소장품으로 가득 찬 키사가 있다는데 어디인지 궁금함
          + 오사카의 Tavern Pachimon Wars가 스타워즈 키사의 설명과 부합할 듯함
          + 교토의 Nijo Koya가 또 하나의 흥미로운 장소임
          + Chris Broad는 큰 영감을 주는 인물임
     * 낡았다고 느껴지지 않는 이유는, 실제로 낡지 않았기 때문임. 구석구석 먼지도 없고 벽에 흠집도 없음. ‘고풍스러운 멋’과 ‘그냥 오래된 쓰레기’의 차이는 한평생 쏟은 정성에서 나옴
          + 외관이 ‘노후돼 보이지 않는다’는 묘사에 약간 거부감이 들었음. 이런 분위기는 의도적으로 연출된 것임. 공간 전체가 깔끔하게 관리되어 있고, 아무것도 무질서하지 않음
          + 정말 훌륭한 관찰이라고 생각함. 작은 공간에 들어서면 실제보다 훨씬 커 보이고 웅장하게 느껴지는데, 이는 그동안 얼마나 많은 사람이 아끼며 지냈는지, 그 흔적에서 오는 특별함이라 봄. 인간의 흔적으로 가득한 공간이 본능적으로 좋은 장소로 여겨지며, 시간의 깊이와 많은 이야기가 켜켜이 쌓인 느낌. 반대로 대형 쇼핑몰이나 사무실처럼 아무 흔적 없는 곳은 오히려 답답하게 느껴짐
          + 매우 청결하고, 조명이나 공간의 디테일도 의도적임. 값싼 플라스틱 대신 목재 등의 내구성 좋은 소재가 많음
     * 2010년대 초 샌프란시스코가 그리움. 빨래방 안에 있던 3석짜리 와인바, 친구 차고에 숨어 있던 스시집, 바닥에 쿠션만 깔린 독특한 커피숍, 작동하는 타자기 영수증 프린터가 있던 구멍가게 같은 곳들이 인상적이었음
     * 교토의 대나무 숲 근처에서 실제 할머니의 집을 개조한 카페를 방문했는데, 그 순간 도시계획과 용도제한 정책에 대한 깊은 아쉬움을 느낌
          + 일본에도 용도제한(조닝)이 있지만 매우 합리적으로 운영됨. 미국은 너무 제한적이라서 지정된 곳에만 특정 건물을 지을 수 있음. HUD 문제로 콘도에 FHA 대출을 못 받기도 함. 반면 일본은 전국적으로 공통된 정책을 두고, 예를 들어 ‘경공업’ 지역이면 그 이하의 어떤 업종도 허용됨. 그래서 커피숍, 주택, 아파트, 기계공장까지 다 가능
          + 대나무 숲 근처는 고층 빌딩이 없고, 교토 특유의 높은 밀도와 훌륭한 대중교통이 장점임
     * 일본은 정말로 ‘미학’을 완성한 나라임. 특히 대도시에서 이런 미학을 구현한 게 놀라움. 예를 들어 창고 벽을 뒤덮은 덩굴은 정리 대상이 아니라 오히려 깊이와 세월을 더해주는 요소임. 정돈이 중요하지만, 자연의 흐름을 어느 정도 허용함으로써 구조물에 살아있는 멋이 생기는 것임. 완벽하게 잔디를 잘라 두는 것이 정말 아름다운가 싶은 생각도 함. 크랙에서 자라는 식물이 오히려 아름답다고 느낌. ‘깨끗함’을 넘어서 자연의 흐름 자체를 존중하는 태도임. 참고
          + ‘더러우냐’고 걱정하는 건 중요하지 않음. 건물의 틈새에서 자라는 나무도 아름다울 수 있지만, 안전성 문제와 구조 훼손 가능성도 생각해야 함. 식물이 자라는 것은 미적으로 좋은데, 방치할 경우 곰팡이, 단열 등 실질적인 어려움이 생길 수 있음
          + 일본에는 가본 적 없지만, 노르웨이에 살면서 서양식 미학도 역시 정교하게 느껴졌음. 노르웨이인들은 일본/젠 스타일을 아주 좋아하는데, 기후가 열악해도 대부분 집에 생화가 있고, ‘코셀리(koselig, 포근함+그 이상)’라는 단어가 있음. 일본 커피숍이 이 단어를 가장 잘 구현한 분위기임
          + 도쿄 뒷골목을 돌아다니다 보면 덩굴에 덮인 자전거를 봤는데, SF에서는 하루 만에 사라질 자전거들이 오랜 시간 그대로 남아 있었음. 안전하고, 구역이 허술하거나 방치된 것이 아니라는 점이 독특하게 느껴졌음
          + 두 주 정도 집을 비운 적 있는데, 잔디 때문에 시에서 벌금 통보를 받음. ‘완벽하게 잔디를 다듬는 미학’은 지역 규정에 의한 것도 큼
          + 사실 일본식 정원도 엄청난 손질이 들어가지만, 그 스타일이 다를 뿐임
     * 일본은 단순함 자체에서 아름다움을 끌어냄. 경제 문제나 인구 감소 등 어려움에도 불구하고 삶의 단순함을 유지하는 능력이 일본 문화의 깊이를 만들어냄
     * 넷플릭스 시리즈 ‘심야식당’이 떠올람. 시부야 작은 이자카야를 배경으로 한 이 작품은 마스터와 단골들의 대화를 중심으로 극이 펼쳐짐. 마치 연극처럼 소박한 일상과 다양한 이야기를 담아내는 점이 매력임
          + Hacker News에 흔한 일본 ‘이상화’ 문화에는 조금 피로함을 느끼지만, 이 시리즈만큼은 냉소적인 본인도 추천할 만하다고 생각함. 일본 영화·애니에서는 이런 ‘저자극’ 일상 이야기, low-stakes slice-of-life가 많음. 고레에다 히로카즈 감독의 작품이나 알모도바르 감독의 스페인 영화와 번갈아 보며 감상한 적도 있음
          + ‘심야식당’은 원래 만화를 원작으로 함 Shinya Shokudō 위키
"
"https://news.hada.io/topic?id=21633","미국 입국 거부: 국경 심사에서 ‘대머리 JD Vance 밈’ 발견된 노르웨이인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              미국 입국 거부: 국경 심사에서 ‘대머리 JD Vance 밈’ 발견된 노르웨이인

     * 21세 노르웨이인이 미국 입국 과정에서 JD Vance 밈으로 인해 입국이 거부됨
     * 미 이민 당국의 휴대폰 조사 및 지나친 신상 파악 요구로 인권 침해 논란 야기됨
     * 단순한 사진과 농담성 이미지로 인해 심문 및 압수, 신체 수색 등 강도 높은 조치 경험함
     * 이번 사건은 미국 입국 규정의 엄격성과 국경 당국의 권한 문제를 조명함
     * 노르웨이 외교부는 규정 숙지와 입국 책임이 개인에게 있다고 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * 21세 노르웨이 관광객 Mads Mikkelsen이 미국 뉴저지 Newark 공항 도착 직후, 국경 통제대에 의해 따로 불려가 입국 거부 및 구금됨
     * 여행 목적은 뉴욕과 오스틴, 텍사스에 있는 친구 방문이었음
     * 그는 미국 이민 당국에서 권한 남용과 강압적 대우를 경험했다고 주장함

조사 및 사건 경과

     * Mikkelsen은 공항에서 신발, 핸드폰, 백팩을 압수당하고, 여러 명의 무장 경찰 앞에서 격리 조사실로 이동함
     * 이민 당국은 여행 목적뿐만 아니라 여러 개인적인 질문(마약 밀수, 테러 계획, 극우주의 관련)도 추가로 물었음
     * 미국에서 만날 모든 사람들의 이름, 주소, 전화번호, 직업까지 요구받음
     * 심한 피로와 스트레스를 겪으며 이미 신체적·정신적으로 지친 상태였음

이미지 조사 및 입국 거부 사유

     * 공무원들은 그의 휴대폰 비밀번호 제출을 요구하고, 거부 시 최대 5,000달러 혹은 5년 징역형을 경고함
     * 비밀번호 제출 후, 당국은 두 장의 이미지(밈 형태의 대머리 JD Vance 사진과 Mads가 만든 나무 파이프를 든 사진)에 문제를 제기하며 입국을 최종 거부함
     * 해당 밈은 소셜 미디어에서 널리 공유된 이미지였으며, 이 두 사진 모두 챗 앱에서 자동 저장된 것임을 설명했으나 받아들여지지 않음

심신 및 신체적 압박

     * 당국은 그의 해명을 무시하고, 신체 수색, 혈액 샘플 채취, 얼굴 스캔, 지문 채취 등 강도 높은 신원 조사에 나섬
     * 그는 강압적 신체수색, 벽에 밀침, 극심한 압박 등으로 정신적 충격과 공황 증상을 경험함
     * 5시간 더 구금된 후, 음식과 음수도 제공받지 못한 상태로 즉시 오슬로행 비행기편으로 추방됨

제도적·법적 배경 및 외교부 개선 조치

     * 미국 국경 당국은 Trump 행정부의 복귀 이후 개인 휴대폰 사전 조사 권한이 강화된 상황임
     * 올해 3월, 프랑스 과학자도 Trump 비판 관련 메시지로 입국 거부 사례가 보고됨
     * 노르웨이 외교부 대변인 Mathias Rongved는 미국 입국 규정이 자주 변동되며, ""입국 심사 최종 권한은 미국 이민 당국에 있고, 노르웨이 당국이 개입할 수 없음""을 강조함
     * 일반 여행객 책임 하에 유효한 서류와 최신 입국 규정 숙지가 필수적임을 안내함

결론 및 시사점

     * 이번 사례는 디지털 사생활이 국가간 이동 시 심각한 영향을 미칠 수 있음을 보여주는 사건임
     * 미국 국경 당국의 입국 심사 강화, 스마트폰 검사 및 관련 소지 이미지까지 입국 자격 심사 요소로 반영됨
     * 북유럽 국가 포함, 해외 여행객들은 미국 입국 규정과 현지 정책, 디지털 기기 점검 가능성에 각별한 주의 필요함

        Hacker News 의견

     * 모든 사람들이 이 사건의 진위 여부와 “불만 있으면 아예 집 밖을 나가지 말라”는 식으로 논쟁하는 모습에서 중요한 쟁점을 놓치고 있다는 생각임. 우리가 논의해야 할 진짜 논점은 다음과 같음. 1) 어느 나라든 법 집행기관이 전자기기를 영장 없이 검문할 수 있는 권한을 가져도 되는지, 2) 비판적인 시각을 가진 사람이 시민의 언론 자유를 표방하는 국가로부터 입국 거부당하는 것이 용납할 수 있는지, 3) 만약 동의하지 않는다면 이를 막기 위해 무엇을 해야 하는지임. 이런 일은 미국뿐 아니라 전 세계적으로 증가 추세이며, 그 파장을 고려해 앞으로도 허용 가능한 일인지 진지한 논의가 필요하다고 봄. 참고로, 내가 EMEA의 어느 정부를 비판했음에도 그곳에서는 여러 번 일하고 자원봉사를 하게 해줬지만, 중국 같은 권위주의 정권엔 내 비판 이력 때문에
       입국 자체가 어렵다는 게 현실임. 이런 포괄적인 시각에서 논의가 모아져야 함.
          + 국경은 특히 복잡한 경계선임. 입국을 원치 않는 나라에 대해선 어떤 법적 권리도, 실질적인 절차적 보호도 받을 수 없음. 결국 입국이 허용되는지는 두 국가 사이의 친밀도와 내가 입국을 위해 감내할 수 있는 선에서 결정된다는 느낌임
          + “자국민의 언론 자유”뿐 아니라 미국 헌법 수정조항들은 미국 내 모든 이에게 적용됨. 미국은 민족적 이상을 이루고자 하는 모범 모델을 자처함(“언덕 위의 도시” 이미지로 많이 표현됨)
          + 이런 사건의 타당성에 대해 논쟁하는 맥락 자체를 잘못 보고 있다고 생각함. 제대로 된 사례가 있다면 굳이 이런 논란 많은 예시를 들지 않고 좋은 사례를 들어야 한다고 생각함. 아니면 미국이 오히려 과도한 조치를 잘 방지하고 있다는 인상만 줌
          + 모두가 이런 이중 잣대를 줄이고 평등한 환경에서 자유와 권리를 확장하는 게 목표라고 봄. 이런 사건은 국경보안 이슈의 예외적 사례일 수 있지만, 내가 아는 많은 사람(심지어 ICE 지지자도 다수 포함)은 이번 사례를 불필요하게 지나친 일로 여긴다는 분위기임
          + 더 깊은 논의를 하려고 해도 결국 이런 한쪽에 치우친 사례들만 계속 언급돼 이슈가 특정 방향으로 소모됨. 미국이 사소한 밈 하나로 억울한 이들을 감옥에 넣는 나라라는 단순 프레임으로만 소비되는 부분은, 그것이 실제로 타국에서 일어나는 일임을 감안해도 현실이 그보다 훨씬 복잡하다는 점도 보여야 한다고 생각함. 너무 자극적인 주장에 의심 경계가 올라가는 느낌임
     * “구금”이라는 점이 기사 제목에 더 무게감 있게 반영되었어야 한다고 생각함. “입국 거부”란 표현보다 “재판 없이 정치 풍자 만화 소지로 감옥에 갇힌 관광객” 같은 헤드라인이 사실에 더 가깝다고 봄
          + 실제로 그는 Vance 이미지를 찾기 전에 이미 일단 감금당한 상태였음. 국경 관리가 밈 외에도 나무 파이프 피우는 사진을 불편해했음. 물론 Vance 이미지 때문에 입국 거부당했을 가능성도 크고, 그 사실 자체가 최근 상황의 심각성을 보여줌. 하지만 가장 큰 원인은 아마도 귀국 항공권 등 다른 요인이 의심 깃발로 작용했을 거라는 생각임
          + 정치적 밈이라는 단어도 빼도 될 만큼 사소한, 정책과 아무 상관 없는 그냥 웃긴 사진임
          + “재판 없이 감옥에 수감”은 어차피 미결구금 상태 자체가 재판 대기라서 동어 반복임
          + 이슈의 본질은 미국이 야당 정치인 두 명과 그 배우자를 끔찍하게 암살했지만, 이 이야기는 전혀 논의조차 안 된다는 점임. Vance 밈으로 잡힌 사람 얘기만 주목받는 현실은 미국이 이미 파시즘 체제임을 드러냄
     * 지금 시기에 미국을 여행하는 건 매우 어리석은 선택이라고 생각함. 부모님이 위독한 게 아니라면 굳이 방문할 이유가 없다는 입장임. 예전 북한 관광 열풍과 비슷한데, 원래부터 그리 좋은 선택이 아니었음
          + 영국 남성이 딸의 생년월일 타투 사진을 온라인에 올렸다가 ICE가 “베네수엘라 갱단 문신” 사례로 소개하면서 결국 가족여행을 포기한 사례가 있음 관련 BBC 뉴스
          + 미국 여행은 단순히 어리석은 정도가 아니라 비윤리적 선택이라고 생각함. 돈을 미국 밖에 두는 것이 오히려 미국과 세계를 위한 최선임
          + 미국 국경 관리들이 불쾌하거나 까다로운 건 여기만의 문제가 아님. 더 어리석은 행동은 충분히 여럿 있음
          + 이 문제는 관광객뿐만 아니라 미국 재입국을 앞둔 멕시코 국적 친구들에게도 소름 돋는 현실임. 휴대폰의 사소한 밈 사진 하나까지 이제는 고민해야 하는 시대임
          + 더 권위주의적인 나라를 방문하는 것도 때론 외국인 처우는 오히려 더 친절한 경우가 많음. 단기 방문이면 추방이나 입국금지 정도가 최악이고, 체포까지 가는 경우는 러시아 정도를 제외하면 드물다고 느낌. 시민 신분이면 또 다른 이야기임
     * 미국은 이미 20년 가까이 휴대기기 검사를 해오고 있음. 2000년대 초에도 내 휴대폰 검사를 직접 당했고, 최근 미국 비자 신청에선 모든 소셜미디어 계정을 제출하라는 요구까지 있었음. 심지어 GrayKey나 UFED로 기기 전체를 복제해 오프라인 분석하는 경우도 경험하지 않았지만 읽어본 적 있음. 국경에서의 선택지는 기기 잠금을 해제해 줄 수도 있고, 거부하면 무조건 입국이 거부되거나 최악엔 5년 입국 금지까지 생길 수 있음(비협조가 입국 부적격 사유로 처리됨). 특히 미국·영국은 이민자·비시민권자에 대해 이유 없이 훨씬 더 가혹하게 대하고, 비백인 유럽인에겐 상황이 이중으로 더 나빠짐
          + 생체인식 잠금 기능 비활성화가 실질적 보호책이 될 수 있음. 비밀번호 입력을 강요당할 의무는 없는데 얼굴이나 손가락을 요구할 수 있으니 전원을 꺼서 국경을 넘는 걸 권장함. 실제로 벌금이나 구금 같은 법적 처벌까지 받을 위험은 크지 않다고 생각함. 설령 국경 직원이 협박해도 실제로 적용할 수 있는 법적 제재는 입국 거부 정도임. 비시민권자나 영주권자에게만 해당되고, 시민은 거부당하지 않음. 기기는 압수 당할 수 있음. ACLU 게시물에 좋은 정보가 많음
          + 최근엔 단순 입국 거부를 넘어 수주간 구금당할 위험도 있다고 들음
     * “그는 신체검사와 혈액, 얼굴인식, 지문 채취까지 당했고, 막판엔 압박을 동반한 탈의 수색까지 받았다”는 증언은 단순 입국 거부보다 훨씬 가혹한 경험이라 느껴짐. 감정적으로 완전히 무너질 만한 상황이라고 느낌
     * 이 이야긴 뭔가 수상하거나 제공된 정보가 부실하다고 느꼈음. 왜 특정인만 검사 대상이 됐는지도 명확히 언급 안 되었고, 실제로는 ICE가 관여하지 않았을 가능성이 높다고 생각함. 노르웨이 Reddit에 떠 있는 당사자의 유튜브 채널을 참고해보니 총기 촬영 영상과(노르웨이어라 정확하진 않지만) 대통령에 대한 언급이 있던 걸로 추정함 해당 유튜브 채널. Palantir처럼 소셜미디어를 스캔해 ‘관심 인물’로 분류했을 수도 있을 듯. 참고로 노르웨이 외무부는 이 사안에 대한 질의에 “입국 규정은 언제든 바뀔 수 있고, 입국 적격 여부는 개별 이민당국 결정 사항이며 여행자가 최신 규정 숙지 및 적절한 서류를 갖추는 게 책임”이라는 원론적 답변을 냈음. 즉, 서류 미비로 입국 거부당한 게 아닌가 싶은 암시로 읽힘
          + 설령 서류 미비라 하더라도, 서류 작성 지원을 해주면 될 일을 왜 탈의 수색과 폭력적 검문까지 해야 하는지 의문임
          + 저 답변은 그냥 원론적 안내라 봄. 구체 사건에 관여했다는 뜻은 아님. 실제로 입국 서류 미비라면 노르웨이 출발 자체가 불가하고 항공사에서 철저히 확인함
          + 기사 정보가 부족하다고 해서, 없는 사실을 창작해 채워넣는 건 옳지 않다고 생각함
     * 학교 선생님이 당시 동독 여행 시 무장 경비가 짐을 검사했다고 들려준 적 있음. 이런 일이 반복되면 사회가 점진적으로 파시즘의 경사면을 타고 간다고 봄
          + 이미 그 경사면 하단까지 다다른 채 계속 더 안 좋은 현실이 쏟아진다는 절망감 느낌임
          + 흥미롭게도 미국인 입장에선 내가 무장 경비에게 짐 검사를 당한 건 오히려 유럽의 사회민주주의 체제에서뿐임
          + 동독은 파시스트 국가가 아니라 대놓고 반파시즘을 표방한 국가였음. 인권 탄압과 권위주의는 심했지만 파시즘과 구분할 필요가 있음
          + GDR(동독)을 파시스트라고 부르기는 무리라는 입장임. Fascism 위키 참고 및 동독 소개
          + 국경에서의 무장 경비나 짐 검사가 파시즘의 기준이라면 파시즘이 아닌 국가를 찾기가 극히 어려울 것임. 나도 거의 매번 짐 검사를 받고, 공항의 많은 보안 인력이 무장하고 있음. 이런 기준이면 이미 모든 공항이 파시즘 체제가 됨
     * 이번 사건보다 더 명확한 맥락으로 논의할 만한 사례가 있다고 생각함. 최근 소개된 뉴요커 기사에선, 국경 관리가 “당신이 왜 여기 있는지 우리 둘 다 알잖아요. 콜럼비아 대학 시위에 대한 당신의 글 때문이에요”라고 말하는 구체적 증언이 있음. 이런 명확한 사실관계를 바탕으로 논의하면 더 건설적인 대화의 기준점이 될 수 있다고 봄. (관련 Hacker News discussion)
     * 모두가 입국 시 해당 밈을 휴대폰에 저장할 캠페인을 벌이자는 제안임
          + 세계 최대 교도소 시스템과 5위 수감율을 자랑하는 나라가 우리 전부를 체포 못 할 것이라 단언할 수 있는지 의문임. 나는 해당 제안에 동참하고 싶지 않음
          + 나는 집에서 안전하게 있는 편을 택하겠음. 중남미 난민 시설에 갇혀 ‘최악의 사태’가 벌어지는 상황은 겪고 싶지 않음
          + 본인도 입국 시 시위에 참가할 의향이 있냐는 질문임
          + 이런 현상은 Streisand 효과의 한 예가 될 수 있음
     * 심지어 중국도 국경에서 개인 기기 사진을 검사하고 밈 때문에 입국을 거부하는 일이 있냐는 의문임
          + 중국에서 휴대폰 검사를 당한 적은 한 번도 없음. 필요하면 당연히 검문받겠지만, 대부분 입국 심사관은 질문조차 거의 안 하고, 오히려 말을 아끼는 분위기임
          + 아니, 그런 일은 절대 없음. 수차례 중국 여행했지만 그런 경험은 단 한 번도 없었음
          + 백번 넘게 중국 국경을 넘었지만(주로 홍콩), 질문도 거의 없고, 가끔 방문 목적만 묻는 정도임. 가방 검색이나 2차 심사조차 없었고, 한 번은 맨발로 돌아도 문제가 없었음(사연이 김). 오히려 미국이나 캐나다 국경에서 더 불쾌한 경험을 함. 미국(SFO)에서는 정식 TN 비자 소지 상태에서 명함이 없다는 이유로 입국이 거절될 뻔했고, 내 아내를 아주 모욕적인 말로 지칭하기도 했음(오바마 시절). 전화로 이메일 인증 정도는 요구했지만 본격적인 폰 검사는 아니었음. 캐나다에선 2차 심사, 가방 뒤지기, 폰 사진 검사 요구 다 당했는데, 아무 이유도 설명 안 해주고 1시간 넘게 심문당함. 참고로 나는 캐나다 시민이고 약도 안 함
          + 나도 중국 자주 방문했지만 그런 사례는 한 번도 없음
          + 중국은 셀룰러 네트워크에 연결하는 순간 데이터 침해가 일상임. 불법 구금이나 출국 금지도 있음. 다만 그 문제는 잘 언급 안 됨
          + 내 경우 중국 비자 신청시 각종 서류, 며칠간 여권 제출, 비언론인임을 증명하는 고용주 서명서까지 요구받았음. 현장에선 2차 심사를 받긴 했지만, 영어 가능자 자체가 없어 그냥 입국시켜줌. 배경 설명하자면 나는 “부유한 서구권 국가” 시민임. 어느 국적, 배경이냐에 따라 다르게 대한다는 사실도 사람들에게 참고가 될 듯함
"
"https://news.hada.io/topic?id=21566","Phoenix.new – Phoenix를 위한 원격 AI 런타임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Phoenix.new – Phoenix를 위한 원격 AI 런타임

     * Phoenix.new는 Elixir와 Phoenix 프레임워크에 특화된 완전 온라인 AI 코딩 에이전트임
     * 사용자는 브라우저 기반 IDE에서 격리된 가상 머신을 통해 코드를 작성하고, 에이전트와 루트 셸을 공유할 수 있음
     * Phoenix.new 에이전트는 브라우저를 직접 제어해 UI를 테스트하고, 코드 수정부터 배포, GitHub 연동까지 자동화함
     * 사용자는 실시간 앱 프리뷰와 로그 모니터링을 통해 개발 상황을 확인할 수 있음
     * 다양한 언어와 프레임워크 확장이 계획되어 있어, 미래 개발 워크플로우 변화를 기대하게 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * Chris McCord는 Elixir의 Phoenix 프레임워크를 만든 개발자임
     * 최근 Fly.io에서 Elixir와 Phoenix에서 LLM 에이전트가 Python, JavaScript만큼 잘 동작하는 환경을 만들고자 비공개 프로젝트에 몰두함
     * 프로젝트의 결과로 Phoenix.new가 공개됨
     * Phoenix.new는 모든 기능이 포함된, 완전히 온라인에서 동작하는 Elixir 및 Phoenix용 AI 코딩 에이전트임
     * 실시간 협업 및 빠른 프로토타이핑에 최적화된 도구로 강조함

Phoenix.new의 주요 특징

     * 브라우저 환경에서 동작하면서도, 사용자와 에이전트 모두에게 Fly Machine으로 생성한 격리된 가상 머신의 루트 셸을 제공함
          + 사용자는 VSCode 스타일 인터페이스에서 의도한 대로 셸에 접근 가능함
     * 에이전트는 Phoenix에 특화되어 실시간 협업 애플리케이션 요구를 이해하고 있음
     * Phoenix.new는 브라우저를 내장, 에이전트가 이를 ""헤드리스""로 제어해 프론트엔드 변경사항을 테스트 및 상호작용함
          + 스크린샷 대신 실제 페이지 콘텐츠와 JavaScript 상태를 파악할 수 있음

루트 접근 권한의 장점

     * 에이전트는 사용자처럼 직접 셸을 열고 개발 실험을 할 수 있음
     * 격리된 VM 환경이기 때문에, mix.exs에 패키지를 추가하거나 시스템 레벨 APT 패키지 설치 등의 행동도 모두 가능함
     * 이 구조는 반복적이고 귀찮은 작업의 상당 부분을 자동화함
     * 생성된 모든 앱은 즉시 클라우드에서 동작하며, 프라이빗 공유 URL(.phx.run 도메인)과 통합 포트 포워딩, GitHub 연동, Fly.io의 인프라 가드레일(가상화, WireGuard, 격리 네트워크) 등을 자동 제공함
     * Github의 gh CLI가 기본 설치되어, 에이전트는 저장소 복제·이슈 열람·PR 생성 등 팀 협업 작업도 수행 가능함
     * 자동 배포 및 테스팅 루프가 열려 있어, 앱 실행부터 오류 감지·수정까지 에이전트가 처리함

실시간 빌드 확인

     * Phoenix.new는 실제 브라우저를 구동해 웹 애플리케이션을 테스트함
     * 프론트엔드 기능 추가 시, 코드만 작성·컴파일하는 것이 아니라 직접 UI를 조작하며, 페이지와 JavaScript 상태, 서버 로그까지 동시에 확인함
     * 자체 UI에 라이브 앱 프리뷰가 내장되어, 개발 및 변경 과정을 실시간으로 모니터링 가능함
     * 여러 .phx.run 탭에서도 변경 사항이 실시간 동기화됨

단순 프로토타이핑 이상 가능

     * 이미 WebSocket, Phoenix Presence, 실제 데이터베이스가 연동된 전체 스택 앱 개발을 수행함
     * 셸과 브라우저에서 할 수 있는 모든 작업을 에이전트가 자동 혹은 사용자 요청으로 처리할 수 있음
          + $DATABASE_URL 설정 후 데이터베이스 탐색, Ecto 스키마 제안, MySQL 클라이언트 설치 등 지원함
     * 최신 LLM은 광범위한 지식과 일반화 능력을 가지고 있으므로, 새로운 언어나 프레임워크 확장도 기대 가능함
          + 예시: Phoenix LiveView Tetris 앱 ‘즉석’ 코딩 성공 경험, 앞으로는 Rails, React Native, Svelte, Go 등도 가능
     * 시스템 프롬프트는 현재 Phoenix 중심이나, 다른 언어·프레임워크 확장 의지 밝힘

비동기 에이전트 시대 전망

     * 개발자 워크플로우에 큰 변화가 일어나는 시기라고 강조함
     * 앞으로의 개발은 로컬 셸에서 파일을 다루는 방식보다 비동기적 CI 환경에서 에이전트가 주도하는 형태로 전환 예상함
     * 로컬 개발은 사라지지 않더라도, 대부분의 반복 작업이 에이전트-중심 클라우드 환경으로 이동할 것임
     * 실제로 Phoenix.new는 이미 phoenix-core 이슈 분류∙문제 해결 PR 생성 등 일상 업무에 사용되고 있음
     * Chris McCord는 이러한 변화와 Phoenix.new의 미래 가능성에 흥분과 기대감을 표함

        Hacker News 의견

     * 이 서비스가 정말 인상적임을 느낌, 여기에서 가장 중요한 혁신 두 가지를 꼽고 싶음
         1. 원격 에이전트 – 이 환경은 컨테이너화되어 있어서 에이전트가 자유롭게 행동할 수 있음, 사용자의 개입이나 승인을 따로 받지 않아도 되며, 물론 git 이력 수정 같은 치명적 실수도 발생할 수 있지만 격리된 공간이라 상대적으로 안전함, 이 부분만 따로 서비스로 발전시킬 필요성을 느낌, 개인적으로 claude code를 터미널에서 실행할 때 자동으로 격리 환경(로컬 혹은 원격)에 에이전트가 구동되고, 그런 식으로 병렬 작업이 쉬워짐
         2. fly와의 깊은 통합 – 앞으로 모든 제품에 AI를 깊게 내장시키려는 시도가 많아질 것이라 생각함, chatgpt에 물어보고 결과를 복붙하는 방식이 아닌, 내가 사용 중인 제품 내에서 직접 데이터와 작업을 하고 실시간으로 피드백을 받는 경험, 여기선 웹앱을 바로 배포하는 경우가 그 예시임
          + Kasm Workspaces를 추천하고 싶음, docker 기반의 리눅스 데스크톱 환경을 원격에서 자유롭게 띄울 수 있고, AI 개발 환경으로도 매우 잘 맞음, 홈 디렉터리와 패키지 영속성도 지원함, docker hub 링크, 패키지 영속성 관련 reddit 토론
          + 컨테이너화된 환경에서 에이전트가 자유롭게 돌아다닐 수 있다는 게 혁신이라는 의견이 있는데, 정말 혁신적인지 궁금증이 듦
     * Phoenix의 창시자임을 밝힘, 궁금한 점 있으면 답변해줄 수 있음, 참고로 phoenix.new는 전 세계적으로 배포된 글로벌 Elixir 클러스터임, 예를 들어 호주에서 가입하면 시드니에 IDE와 에이전트가 할당됨
          + 멋진 작업임, Phoenix.new라는 브랜드를 보며 약간 헷갈렸는데, 이게 기존에 알던 Elixir 웹 프레임워크와 동일한 건지, 아니면 그 이상인지 궁금함
               o Phoenix.new가 IDE를 제공하는 건지
               o Phoenix 웹 프레임워크로 앱을 만들 때 AI 지원을 제공하는 건지
               o 반드시 Fly.io에 배포해야 하는지, 만약 그렇다면 ""phoenix.flyio.new"" 같은 이름이 더 그 목적에 어울리는 것 아닌지
               o 위 모든 기능이 한 번에 제공되는 것인지
               o 그리고 Tidewave.ai와 비교하면 어떠한 차별점이 있는지 궁금함 (참고로 Tidewave.ai는 Elixir 창시자가 만든 걸로 알고 있음)
               o 혹시 내가 주제를 혼동한 게 있다면 미안함
          + Phoenix.new 환경엔 헤드리스 Chrome 브라우저가 포함돼 있고, 에이전트가 이를 조작할 수 있다고 알게 됨, 프론트엔드 기능을 추가하라고 명령하면, 단순히 코드만 짜고 컴파일과 린트만 통과시키는 게 아니라 직접 앱을 띄우고 UI를 조작하며 페이지 콘텐츠, 자바스크립트 상태, 서버 로그까지 동시에 확인함, 이 헤드리스 브라우저와 에이전트를 Cursor 같은 환경에서 로컬에서 돌리는 것도 가능한지 궁금함
          + 보안 정책이나 사용자가 제출한 코드가 훈련용으로 쓰이는지 여부에 대한 문서를 찾을 수 없었음, 관련 보안 정책을 어디에서 확인할 수 있는지 궁금함
          + 접근성(Accessibility)에 대해 어떤 접근 방식을 가지고 있는지, phoenix.new UI의 접근성 테스트를 하는지 궁금함, Phoenix로 프론트엔드도 작성하는 사람이 많으니 생성된 프론트엔드의 접근성 평가도 해본 적이 있는지 묻고 싶음
               o 그리고 3rd party 라이브러리는 어떻게 처리하는지, 에이전트가 라이브러리 문서를 접근할 수 있는지 궁금함
               o Elixir가 대중적인 언어가 아니므로 훈련 데이터가 부족할 수 있다고 생각하니 이런 문제를 어떻게 해결하는지가 중요하다고 생각함
          + Fly API를 이용한 격리 환경 프로비저닝 관련해서 인사이트가 있다면 공유해 줄 수 있는지 궁금함, 나 역시 low-code 서버리스 워크플로우 시스템에 비슷한 접근 방식을 시도하고 있음
     * Elixir를 좋아하고 agentic AI의 미래를 믿는 입장에서 이 서비스가 매우 멋지다는 생각이 듦, 맥락 관리나 사용하는 모델에 대해 궁금한 점이 있음
          + 요즘 Gemini가 주어진 컨텍스트 윈도우에서 제일 잘한다고 생각하지만 한계가 분명함, Claude Code와 작업할 땐 작업 단위로 분해하고 프로젝트 상태를 잘 관리해서 컨텍스트 사이즈를 유지하려 애씀
          + 데모 동영상이 인상적이긴 한데, 계속 프로젝트를 진행하다 보면 결국 방향을 잃거나 컨텍스트가 뒤섞일 것 같기도 함
          + 이런 상황에서 주요 내용을 요약시키거나, 요약을 바탕으로 깨끗한 세션을 시작할 수 있는지, 불필요한 파일을 ‘잊게’ 했다가 다시 기능 개발에 집중하게 할 수 있는지 궁금함
     * Phoenix.new가 Fly.io 제품인지, 아니면 그 산하의 프로젝트인지 확실치 않음, 가격 정책도 명확하게 나와있는지 궁금함, 특히 웹 서비스 영구 배포 관련 추가 비용 등이 어떻게 되는지, 모바일에서 보기에 프론트 페이지에 이런 정보를 쉽게 찾기 어려웠음
     * Elixir가 LLM 지원에서 뒤처지지 않을까 걱정이 많았는데, 이런 노력이 그 걱정을 덜어줘서 매우 반가움, 적극적인 시도들 덕분에 Elixir의 미래가 더 안전해질 거란 신뢰감을 얻음
          + LLM이 Elixir 코드를 잘 못 쓴다는 점이 오히려 Elixir의 최대 매력이 될 수도 있다는 농담 섞인 생각을 하게 됨
          + Claude가 LiveView가 포함된 풀스택 Elixir 앱도 아주 잘 생성해 줬다는 경험을 했음, 이 부분에 대한 밈은 사실이 아니라 느낌
          + 최근 몇 달간 Elixir 코드 작성에 LLM을 썼는데, JS처럼 완벽하진 않더라도 꽤 괜찮다고 생각함
          + 최근 몇 주 동안 LLM을 활용해 새로운 프로토타입을 만들고 있음, 주로 Zed에서 github copilot으로 Claude Sonnet 3.7을 썼는데 경험이 매우 훌륭함, 가끔 구식 방식을 쓰려고 하긴 하지만 별로 문제되지 않음, LiveView 신규 기능도 쉽게 만들어냄, 일반적으로 파이썬, nextjs 프로젝트에서 느꼈던 생산성과 크게 다르지 않음, 주로 대중적이고 잘 알려진 패키지를 사용했기에 이득도 본 듯함, 처음엔 직접 phoenix 프로젝트를 생성해준 뒤 LLM에게 맡겼더니 이상한 방향으로 새는 일도 줄었음
          + Common Lisp로 작업하는 입장에선 LLM에 기존 코드베이스로 보완 학습을 할 수 있으면 좋겠다고 생각함, 문서만 읽는다고 해서 코드 생성 정확성이나 일반적인 문제 해결 능력이 개선되진 않는 것 같음
     * 혹시 @chrismccord에게 묻고 싶은데, 이게 Chris와 Fly.io의 합작 프로젝트인지 혼란스러움, 앱을 완전히 분리해서 직접 운영하는 건 불가능한지, 이것이 오픈 소스 Phoenix 프로젝트가 아니라는 의미인지 의문
          + 그냥 git으로 코드 복제해서 사용할 수 있다는 사실을 공유함
     * LLM이 Elixir를 잘 못 다룬다는 의견이 의외임, 자신은 Phoenix/Elixir 사이드 프로젝트에서 AI 도구들을 꽤 잘 썼던 경험이 있음
          + 나는 Elixir에서만 LLM을 사용해봤기 때문에 비교 대상이 없지만, Claude가 자주 엉뚱한 방식으로 접근하기도 하지만 직접 매뉴얼을 읽게 시키고 나면 상당히 잘 동작함
          + Elixir에 대한 LLM의 실력이 확실히 예전보다 좋아졌다고 느낌, 단순한 Elixir보다 Phoenix, LiveView 같이 복잡한 작업은 여전히 어려운 점이 있음, 어떤 LLM이 Elixir/Phoenix에 가장 잘 맞는지 궁금함
     * ""Sign in with fly.io""를 클릭하면 결제 페이지로 이동하는데, $20 요금이 포함된 ‘Built-In AI Assistance’에 무슨 기능이 포함되어 있는지 상세 정보가 없음, 빌드, 리팩터, 디버깅 등의 기능이 IDE 내에서 제공된다는데 명확한 범위를 궁금해함
          + 관여자들이 Chris에게 빨리 출시하라고 계속해서 압박을 줘서 패키징이나 가격 정보 등은 아직 제대로 정리 중임, 지적해줘서 고맙고, 앞으로 일주일 정도 내로 추가할 예정임
     * 바로 가입해서 확인해보니 무료 체험이 없이 바로 $20 구독만 가능한 구조임, 사용량 제한 같은 것도 명시돼 있지 않음
          + 나도 똑같이 느꼈음, 에이전트 기능은 비용이 높을 수 있고 실제 신뢰성이나 효율성을 아직 모르겠음, 먼저 체험해보면 좋겠음
     * Phoenix.new가 강력해 보이고 꼭 테스트해볼 예정임, 내가 꿈꾸던 BEAM 환경을 최대한 활용하는 agentic 프레임워크는 아직 아니지만, 아마 jido가 그 역할을 할 수도 있을 듯함
          + 오늘 알게 된 아주 흥미로운 라이브러리라고 생각함
"
"https://news.hada.io/topic?id=21600","미국, 이란 핵시설 폭격","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             미국, 이란 핵시설 폭격

     * 미국의 최근 공격으로 인해 이란 내에서 호르무즈 해협 봉쇄 가능성 논의 증가
     * 호르무즈 해협은 전 세계 원유 공급의 중요한 경로로, 봉쇄시 글로벌 *** 석유 시장 혼란*** 및 가격 상승 우려
     * 이란 혁명수비대는 쾌속정을 보유해 봉쇄 실행 능력 갖춤
     * 해협이 봉쇄될 경우 사우디아라비아, UAE, 쿠웨이트 등 여러 국가와 중국, 일본, 인도, 한국 주요 수입국까지 영향 확대
     * 이란 역시 주요 수출 경로 상실 위험 존재, 미국 측에서는 “경제적 자살” 가능성 경고
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

최근 미국의 이란 핵시설 공습과 호르무즈 해협 봉쇄 가능성

  # 호르무즈 해협의 전략적 중요성

     * 호르무즈 해협은 오만과 이란 사이에 위치해 있으며, 세계에서 가장 중요한 원유 운송 핵심 지점임
     * 전체 원유 거래량의 약 5분의 1이 이 해협을 통과함
     * 해협의 가장 좁은 너비는 40km로, 물류 이슈 발생 시 전체 글로벌 에너지 시장에 큰 영향을 미침

  # 이란의 대응 옵션 및 국제 반응

     * 최근 미국의 이란 핵시설 공격 이후, 이란 내에서 호르무즈 해협 봉쇄 가능성에 대한 논의가 활성화됨
     * 외무부 장관 Abbas Araghchi는 “여러 가지 옵션이 테이블 위에 있다”고 언급했음
     * 봉쇄 계획은 아직 구체화되지 않았으나, 이란 혁명수비대(IRGC) 가 보유한 쾌속정 사용으로 봉쇄 실행 가능성이 제기됨

  # 봉쇄 시 글로벌 영향

     * 호르무즈 해협 봉쇄 시, 중동 국가인 사우디아라비아, UAE, 쿠웨이트뿐 아니라 중국, 인도, 일본, 한국 등 아시아 주요 원유 수입국들도 영향권에 속함
     * 특히 중국은 원유 가격 상승이나 해상 운송로 차질에 민감하게 반응할 것으로 예상됨

  # 이란이 봉쇄로 인한 자체 손실 가능성

     * 이란 역시 해협을 봉쇄할 경우 주요 석유 수출 경로 상실하게 됨
     * 미국 국무장관 Marco Rubio는 이란이 해협을 닫을 경우 이는 “** 경제적 자살**”과 같다고 경고함

  # 결론

     * 호르무즈 해협 봉쇄는 세계 원유 시장과 여러 국가 경제에 중대한 리스크로 작용할 수 있음
     * 이란이 실제로 봉쇄를 단행한다면 글로벌 석유 가격 급등과 지역적 긴장 고조 초래 가능성 있음
     * 이란이 최종적으로 실질적 행동에 나설지, 혹은 강경한 외교적 압박의 수단으로 활용할지 주목됨

        Hacker News 의견

     * 난 Netanyahu는 감옥에 가야 한다고 생각하고, Trump는 얘기할 가치조차 없다고 생각함. 하지만 핵심은 인구 밀집 지역에서 멀리 떨어진 산에 깊이 파묻힌 무단 핵무기급 우라늄 농축 시설이 공격받은 사례라는 점임. 만약 아직 안 읽어봤다면 GBU-57 “벙커버스터” 폭탄에 대해 알아보길 추천. 메리 멜로디/애크미 폭탄 같은 느낌이 들 정도로 무겁게 만든 폭탄임. 폭발물이 아니라 순수히 질량만으로 그 무게를 늘린 방식. 차라리 거대한 피아노 모양으로 만들었으면 재밌었을 것 같다는 생각
          + Netanyahu에게 부패 혐의가 다수 제기되어 있고, 실제로도 유죄일 가능성이 높음. 하지만 비상사태에서 이스라엘을 이끄는 한 재판은 미뤄지거나 빠져나갈 수도 있는 상황. 이란과의 새로운 전쟁이 일어나며 가자지구에서 벌어지는 일에 대한 관심도 전환되고 있음. 지금 가자는 굶주림이 극단적인 단계로 진입. 주민들은 집중되어 밭일도 불가능하고, 식량 원조를 제공하는 장소도 크게 줄었으며, 이스라엘 군인들이 식량을 받으러 가는 사람들을 매일 수십 명씩 사살하고 있음
          + ‘무단(unauthorized)’이라는 표현이 참 미묘. 허가된 핵이란 게 뭔지 생각해보면, 몇몇 나라가 먼저 핵을 가진 뒤 다른 나라들은 갖지 못하게 막는 논리임. 이스라엘도 사실상 무단으로 핵을 확보했지만, 불편하지 않은 나라는 모르는 척함. 결국 공식적인 허가/무단 핵은 없고, 오직 힘의 셈법만 존재하는 상황
          + Netanyahu가 20여 년 전 이라크 대량살상무기(WMD) 문제로 미 의회에서 위증한 것, 그 자체가 체포 사유 아닌지 의문. 미디어는 Netanyahu가 상습적 거짓말쟁이라는 점은 전혀 언급하지 않고, 오히려 이란 핵무기에 대한 그의 주장을 무비판적으로 따라줌. 결국 미국 주요 미디어들은 이스라엘의 PR부서 역할만 함. Fox News도 RT처럼 외국 대변 미디어로 금지되어야 함
          + 이 공격은 이란에게 핵무기의 필요성을 일깨워준 계기뿐임. 어느 나라든 러시아·미국·이스라엘 같은 국가의 표적이 되지 않으려면 핵무기가 유일한 방패라는 건 모두가 아는 사실. 북한에는 위협으로 못 막았지만 이란에게 막 50년간 외교적 노력을 통해 억제했던 것도 이제 끝남
          + 이 시설 파괴가 이란에서 체제교체로 이어지길 희망. 그렇지 않으면 그냥 잠깐 휴식할 뿐, 긴장상태만 더 고조될 것임. 만약 현 체제가 유지된다면, 이란 국민들은 체제의 문제점과 탄압을 감수하고라도 뭔가 희생할 충분한 동기를 얻게 됨. 이란은 천연자원이 풍부한 9천만 인구의 국가. 마음만 먹으면 어떤 일도 진지하게 추진 가능
     * 미국 입장에서는 이번 사안이 전적으로 손해라는 생각. 이스라엘의 전쟁에 끌려들고 멀지 않은 미래에 미국만 오롯이 책임을 지며 골치 아픈 상황을 맞게 될 전망. 이 여파로 유가, 증시, 미국 안보까지 타격 예상. 전부 이스라엘 지도부를 감옥에서 구해주려는 일로 이런 일이 벌어지는 모양새라 정말 나쁜 선택
          + 많은 사람들이 미국이 이스라엘 전쟁에 끌려들고 있다고 하는데, 실제로 그게 어떤 모습일지 궁금. 내 생각엔 이번 대응은 단발성일 가능성이 더 높음. 지금 이란은 실질적 위협이 별로 없고, 발사장치도 거의 사라졌으며, 지도부도 제거됐고, 주변 동맹 세력도 움직이지 않음
          + 최소한 이런 이야기가 상위 인기 댓글 안에 있다는 것도 의미가 있다고 봄. 결국 이런 상황을 미 언론은 열성적으로 응원할 뿐
          + 잘 모르겠지만 이란이 핵을 가지는 것보다는 지금이 낫다고 생각
          + 현재 시장이 불안한 상태였는데, 월요일은 완전히 혼란스러운 장세가 될 전망
     * Fordo 핵시설 피해 정도에 대한 새로운 업데이트 정보 공유. 미국 관리 발언에 따르면, Fordo 기지는 완전히 파괴되진 않았지만 심각하게 손상되어 ‘전투에서 제외’된 상태. 벙커버스터 12발로도 완전 파괴는 불가능했음. 피해 평가 관련 NYTimes 기사 Fordo 손상 기사 위 기사에는 Maxar와 Planet의 새로운 위성 사진도 있음
     * 장기적으로 보면 협상이 유일한 해결책. 미국과 이스라엘 입장에선 이란 Fordow 시설이 50m밖에 안 되는 얕은 곳에 건설된 덕분에 그나마 운이 좋았음. 만약 이란이 시설을 더 깊이(예: 1200m 탄광처럼) 다시 짓는다면 미국은 어떻게 할지 궁금. 이란 기술력은 북한보다 월등하고, 결국 북한도 핵 개발에 성공했음. 미국도 이스라엘이 먼저 나서지 않았다면 이 전쟁을 시작하지 않았을 것임. 2015년 첫 이란 핵합의는 완벽하진 않았지만 15년간 어느 정도의 보장을 줬음. 이번 폭격으로 확보한 시간은 2-3년밖에 안 될 것 같음. 이스라엘은 이란 제재 해제를 원하지 않음. 결국 미국이 어떤 합의를 하든, 이스라엘이 만족할만한 결과는 나오기 어려움
          + 지난 10년 사이 벌어진 일들을 고려할 때, 미국이 신뢰받는 협상 파트너로 다시 평가받을 수 있다고 믿는 건 환상에 가까움
          + 협상 말고도 더 현실적인 출구는 이란이 이제 핵무기 보유를 결심하는 것일 수도 있음. Netanyahu가 이란 핵 위협을 언급한 20년 동안 이란도 충분한 기술을 갖고 있었는데, 이제 명확한 동기까지 생김
          + 이란이 더 깊은 지하설비를 건설한다면 이스라엘은 완공 전 선제 공격할 가능성이 높음. 과거에도 선제타격 사례는 있었음
          + 레짐 체인지(정권 교체)도 또 다른 가능성. 찬성한다는 뜻은 아니지만 50%쯤은 그럴 가능성도 봄
          + 2015년 이란 핵 합의가 15년 보장을 줬다는 의견에 대해, 15년 후에는 결국 또 핵무기를 갖게 되는 것이라 이스라엘이나 미국은 그 점을 결국 원하지 않음. 또한 이란은 원래 합의 때부터 일부 부지에 사찰단 접근을 허용하지 않는 등 시작부터 약속을 어겼음
     * 이란 같은 국가는 굳이 핵발전을 할 필요 없음. 노르웨이처럼 재생에너지, 태양광으로 모든 전기를 만들고 석유만 팔아도 부를 쌓을 수 있음. 그런데 이란은 이웃을 위협하고, 테러를 지원하며, 자국 국민을 탄압함. 이스라엘/미국의 폭격이 이란을 변화시키는 계기가 될지는 솔직히 모르겠음
          + 미국이 이란에겐 핵무기 불가, 중국엔 첨단 반도체 불가를 강요하는데, 미국이 이런 룰을 정할 수 있는 시간은 이제 한정적임. 지금은 세계 질서 변화가 불가피하며, 미국이 영원히 판을 좌우할 수 없다는 현실을 받아들이고 모두를 위한 미래로 나아갈 때라는 생각
          + 이란에서 태양광 경제성이 궁금. 정부 보조금 덕분에 휘발유 가격이 리터당 0.04달러밖에 안 돼서 태양광 발전에 투자하기 쉽지 않음. 그런 면에서 핵발전도 설득력이 떨어진다는 생각
          + 지구정치에서 이란과 파키스탄에 대한 미국의 태도가 비슷한데, 파키스탄은 핵 프로그램을 허용받음. 오사마 빈라덴 숨겼던 나라임에도 말임
          + 오바마 시절 외교 협정은 잘 먹혔지만, 트럼프가 마지막 임기 때 모두 취소해 버림. 일을 제대로만 했더라면 더 나은 결과가 가능했을 텐데, 현 미국 정권은 정말 최악의 수준
     * 미국에서는 군산복합체가 항상 이긴다는 농담이 있지만, 정작 그런 이유로 당선된 대통령조차도 실상은 안 그럼. 결국 미국 민주주의는 장대한 아이러니로 보임
          + 여기서 군산복합체만큼이나 유권자들에 대해 더 할 말이 많음
          + 트럼프가 우크라이나 지원을 중단·보류·협박한 것만 봐도 군산복합체가 항상 이긴다는 주장이 무조건 맞는 건 아님. 미국은 신뢰할 수 없는 동맹국이 되어가고, 군산복합체도 엄청난 패배를 겪는 중임
          + 트럼프가 군산복합체에 반대했다는 말, 처음 듣는 얘기. 그가 첫 번째 대통령 임기 때 했던 행보만 봐도 영원한 전쟁광으로 분류해야 마땅하다고 생각
     * 벙커 버스터가 실제로 사용됐는지 궁금. 이 무기의 계보는 WW2 시절 Barnes Wallis가 만든 Grand Slam 폭탄과 닮아 있음. 이란은 지진이 자주 발생하는 지역이라 핵벙커와 무관하게 내진 강화 콘크리트 연구도 활발함. 이 점이 핵산업에도 응용되고 있음. 그리고 또 한 가지 특이점, 이란에서 토목공학 졸업생의 상당수는 여성이라는 점. 신정 체제 하에서 복장·행동 제한이 심함에도 경제 전반은 이중적임
          + 이란은 사우디 같은 나라만큼 성차별 제한이 심하지 않음. 살라피즘이 만연한 지역과도 크게 다름. 특히 여성교육은 이란에서 매우 장려됨 여성교육 관련 이란 최고지도자 트윗
          + 이란 대학생의 과반수도 여성임. 성비가 60:40이 넘는다는 사실
          + 보통 부유한 사회일수록 젠더 평등이 높을 것으로 생각하지만, 역설적으로 직업 선택의 자유가 커질수록 임금 격차가 더 커지는 현상도 있음
          + 벙커 버스터도 모두 해결책은 아님. 원래 WW2식 얕은 벙커용으로 만들어진 무기라, 이란처럼 깊은 산속에 건설된 시설에는 겉만 긁고 마는 수준일 수 있음
          + Fordow가 약 60m 지하에 있으니, 벙커 버스터 아니면 핵폭탄 밖에 쓸 수 있는 방법이 없음
     * 이 스레드는 교육과 지혜가 별개임을 보여주는 대표적 사례임
          + 실제로는 교육조차 부족한 경우가 많음. 구글 검색 몇 번과 트위터 ‘전문가’ 팔로우만으로 자기 의견에 확신을 갖는 사람들이 엄청 많음. 결국 모두가 같은 쪽 논리(선전/프로파간다)를 반복하고 있음. 인터넷은 거대한 반향실
          + 전부가 그런 건 아니고, 내가 동의하지 않는 포스트만 그렇다고 생각
          + 한 걸음 더 나아가면, 교육과 지혜, 그리고 경험 모두 별개라는 얘기로 확대. 실제로 지구정치 규모에서 직접 의사결정을 해본 사람은 극소수임. 결국 현실적 선택은 각국 정치 지도자 개인 수준에서 내려질 수밖에 없음. 그런데 그들도 자국 관점에 깊이 편향되어 있으니 객관적 판단이 불가. 경험도 사례마다 맥락이 달라, 완전한 전문가는 되기 어려움. 학계 사학자 중 해당 지역을 연구한 전문가 목소리가 가장 중립적일 수 있는데, 이런 이들은 주로 객관적 시각·냉철한 분석에 강점을 가짐. 하지만 이들은 매체 주목도도 낮고, 실제로 역사를 아는 이는 남이 실수하는 걸 지켜보기만 해야 하는 처지. 지구정치가 워낙 거대해 어느 개인도 흐름을 바꾸기 힘든 현실. 결국 우리끼리 이런 난해한 게임의 관중으로서 얘기만 나눔
     * 미국식 패권주의에 불만 많은 사람임. 하지만 이번만큼은 결과적으로 이란 국민이 뭔가 이득을 볼 수도 있다는 생각. 장기적으로는 확신이 덜 듦. 다만 그렇게 될 가능성 자체도 회의적. 미국처럼 권위주의 정권이 말을 잘 들으면 선호하는 나라는, 향후 이란 제도도 친미적 신정체제로 전환하는 선에서 안정되거나 오히려 군사독재로 악화될 수도 있다고 봄(과거에도 그랬으니). 진심으로 이란 국민을 위해서라면 아주 희박하긴 하지만 결국엔 민주주의로 바뀌길 기대해봄. 어느 나라든 룰 기반 외교, 비즈니스, 외교 따위는 이미 사라진 세상. 자기 이익만 챙겨야 함. 바로 옆집이 든든한 동맹이라도 내 것을 노릴 수 있는 판국임. 참고로 내 모국 전직 수상 넷이(특히 Indira Ghandhi) 핵무기 확보를 끝까지 추진해서 제제에도 불구하고 우리는 자체적으로 핵보유국
       대열에 올랐음. 서방 국가들이 모두 핵을 보유한 상황 자체가 아이러니. 현실은 핵 없는 세상 모두가 바라지만, 운이 나쁘면 그 전에 자기 나라만 손해. 결국 각국이 핵 가지는 게 답
          + 이란이 이미 미국에 순종적이지 않았냐는 질문. 물론 사우디나 걸프 국가만큼 확실한 미국 동맹은 아니지만, 겉으론 반시온주의/반미 선전이 있더라도, 실제론 수십 년간 비공식 협상, 실용적 접근, 때로는 아프간이나 ISIS 같은 공동 이익 사안에서 협력한 적도 많음
          + 미국의 이스라엘 지지는 패권이나 석유 때문이라고들 하는데, 이게 오히려 국내 지지를 더 끌어옴. 실제론 미국엔 전혀 이득이 없는 정책이고, 단지 친이스라엘 로비가 엄청난 힘을 가졌을 뿐임. 트럼프도 우크라이나 지원 자원 일부를 이스라엘로 돌렸으며, 이 전쟁은 미국 패권을 위한 것, 물론 도덕적 논거도 있다고 볼 수 있음
          + “핵 없는 세상 모두를 원하지만, 현실은 각국이 빨리 확보하는 게 답”이라는 생각에 동의함. 우리나라도 핵 개발할 당시 엄청난 제재를 받았지만, 이후 파키스탄 등도 핵 개발에 성공하면서 서방의 제재는 한계가 드러남. 그 과정에서 지도자 암살 등 큰 희생도 감수했음. 이제는 합리적인 자주국가라면 핵 보유가 기본전략임. 남한/일본/EU, 특히 러시아의 위협에 직접 놓인 폴란드 같은 국가는 미국에만 의존할 수 없는 현실
          + 세계 각국이 권위주의 정권만 선호하는 것은 아님. 미국은 노르웨이 같은 비권위주의 국가도 좋아하는 걸 보면 복합적임
          + 트럼프가 자신의 “아름다운 폭탄” 덕분에 전쟁에 승리했고, 핵시설은 더 이상 없다고 주장할 것. 이스라엘도 공개적으로 반박하지 못하고, 이란은 은밀히 핵개발을 계속 추진할 것이지만 오랜 시간이 더 필요함. 당분간 평화협상도 이어질 예상. 트럼프는 “평화적 폭격” 덕분에 노벨 평화상 수상까지 기대할 듯
     * 이스라엘 자유주의 친구 말로는, 이란이 정부에 반하는 시위만 안 한다면 중동에서 기독교인/유대인이 가족 부양하며 일하기 가장 안전한 국가 중 하나라고 함. 실제 그런지는 나도 확신이 없고, 미국 내 뉴스도 서로 달라서 혼란스러움. 인터넷으로 싱가포르 등 다양한 외신을 찾아보며 세계를 바라보는 중
          + ‘정부에 비판만 안 하면 괜찮다’는 건 마치 가정폭력 피해자가 ‘규칙을 잘 지키면 안전하다’고 말하는 것과 비슷함. 평온은 겉모습일 뿐, 실제론 공포와 통제로 유지되는 것
          + 이란 내 소수 집단(유대인, 아르메니아/아시리아계 토착 기독교인)은 표면적으로 일부 안전해 보이지만, 페르시아 기독교인, 만다야인 같은 소수 종파나 비국가 종교, 이슬람 이단(바하이·이스마일리·아흐마디·야지디 등)은 극심한 박해 대상. 아제리, 쿠르드, 발루치, 아랍계 등 대규모 비토착 민족도 탄압 심함. 비시아파는 공무원 진출 자체가 불가능. 어떤 집계로는 페르시아계가 이란 내 소수일 정도. 동성애자도 강제로 성전환 수술을 강요 받음. ‘리버럴 이스라엘 친구’의 발언이 진짜 이란에서 온 이민자인지, 파르시 구사 여부, 연구직/정보기관 경력이 있는지 등에 대한 비판적 시각 필요
          + 나도 이란을 여행한 적 있음. 정부에 항의하거나 종교와 상충되는 행동만 안 하면 안전하다고 느꼈음. 내 어머니는 바하이인데, 이란 출신 바하이들은 ‘틀린’ 종교 관념만으로도 투옥·처형 당하는 일이 잦음. 바하이는 이슬람 이단 취급을 받아 이슬람주의자들에게 표적이 됨
          + 1979년 이란 혁명 전후 종교별 인구 통계 데이터를 보면 더 많은 정보를 얻을 수 있음 이란 인구통계/종교별 비율 위키피디아 링크 1976년 이란의 유대인 인구는 약 6만명(0.2%), 혁명 직후 9천명대로 급감해 현재까지도 그 수준(0.0%). 기독교 비율도 0.5%에서 30년 후 0.2%로 하락. 해석은 다양하겠지만, 특정 소수 집단 인구가 급격히 줄어든 건 좋은 신호로 보기 어려움
          + 이란은 법적으로 민족·종교 소수자에 대한 차별을 공식적으로 시행함. 북아프리카/중동의 타 이슬람국가에서 유대인 인구가 수십만 명에서 수백 명 이하로 준 것과 비교하면 이란의 소수 유대인 남아있는 숫자가 상대적으로 많아 보일 수 있음. 하지만 가족 전체의 출국 금지 등 통제도 보도되어, 남아있는 이들이 자신의 의지로 남은 것인지 확실하지 않음
"
"https://news.hada.io/topic?id=21687","Cobra - 강력한 Go 기반 CLI 앱 개발 라이브러리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Cobra - 강력한 Go 기반 CLI 앱 개발 라이브러리

     * Go 언어 생태계에서 가장 널리 쓰이는 CLI 앱 개발 라이브러리로, Kubernetes, Hugo, GitHub CLI 등 다양한 대표 프로젝트에서 채택됨
     * app server 같은 서브커맨드 구조 및 중첩 서브커맨드/계승 플래그 지원하여 대형 앱에도 적합
     * 명령어·인자·플래그 구조로 직관적이고 확장성 있는 앱 설계가 가능하며, 자연스러운 명령어 시나리오를 지원
     * 완전한 POSIX 플래그 준수
     * 똑똑한 자동 오타 추천: app srver 입력 시 app server 추천
     * 명령어/플래그별 자동 도움말·man페이지 생성
     * bash, zsh, fish, powershell 등 셸 자동완성 지원
     * Viper 연동으로 12-factor 앱 환경 변수 지원
     * cobra-cli 도구로 빠른 스캐폴딩과 명령어 파일 생성이 가능해 대규모 CLI 앱 개발도 효율적으로 가능

   어지간한 오픈소스 코드 보면 정말 많이 사용하는 라이브러리죠.
"
"https://news.hada.io/topic?id=21677","CSS만으로 구현하는 Scroll-driven Animation 가이드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                CSS만으로 구현하는 Scroll-driven Animation 가이드

     * 별도의 JS나 라이브러리 없이, CSS만으로 스크롤 연동 애니메이션 구현 가능
     * animation-timeline: scroll() / view() 등 CSS 속성으로 스크롤 위치, 뷰포트 진입 등에 따라 애니메이션 진행
     * animation-range 속성으로 애니메이션이 뷰포트 내 어느 구간에서 시작/종료할지 상세 조절
     * 움직임에 민감한 사용자를 위해 prefers-reduced-motion 미디어 쿼리 활용 권장
     * Safari 26 beta부터 지원하며, CSS 기반 스크롤 애니메이션의 활용 폭이 크게 넓어짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

스크롤 기반 애니메이션의 3가지 요소

     * Target: 애니메이션을 적용할 대상 요소(예: 프로그레스 바, 이미지 등)
     * Keyframes: 스크롤에 따라 어떤 변화가 일어날지 정의(기존 CSS @keyframes와 동일)
     * Timeline: 애니메이션이 언제, 어떻게 진행될지 결정(시간 기반이 아닌 스크롤/뷰 기반)

CSS에서의 Timeline

     * 기존 CSS 애니메이션은 기본적으로 document timeline(시간 기반) 사용
     * animation-timeline 속성 도입(CSS Animations Level 2, 2023년): 시간 외에 스크롤, 뷰포트 진입 등 다양한 기준으로 애니메이션 진행 가능

  scroll() timeline

     * scroll() 타임라인은 사용자가 스크롤할 때만 애니메이션이 진행됨
     * 예시: 하단 프로그레스 바가 스크롤에 따라 왼쪽에서 오른쪽으로 차오르는 효과
footer::after {
  content: """";
  height: 1em;
  width: 100%;
  background: rgba(254, 178, 16, 1);
  left: 0;
  bottom: 0;
  position: fixed;
  animation: grow-progress linear;
  animation-timeline: scroll();
}
@keyframes grow-progress {
  from { transform: scaleX(0); }
  to { transform: scaleX(1); }
}

     * animation-timeline은 animation 속성 다음에 정의해야 정상 동작

  모션 접근성 고려

     * 모션 민감 사용자 보호를 위해 prefers-reduced-motion 미디어 쿼리 사용 권장
@media not (prefers-reduced-motion) {
    /* 애니메이션 코드 */
}

view() timeline

     * view() 타임라인은 타깃 요소가 뷰포트에 등장할 때 애니메이션이 시작됨
     * 예시: 스크롤 시 이미지가 오른쪽에서 슬라이드 및 페이드인
@keyframes slideIn {
  0% {
    transform: translateX(100%);
    opacity: 0;
  }
  100% {
    transform: translateX(0%);
    opacity: 1;
  }
}
img {
  animation: slideIn;
  animation-timeline: view();
}

  animation-range로 진행 구간 제어

     * 기본적으로 animation-range는 0%(뷰포트 진입)~100%(완전 이탈)
     * 예: 뷰포트의 50% 구간까지만 애니메이션 진행
img {
  animation: slideIn;
  animation-timeline: view();
  animation-range: 0% 50%;
}

     * 사용자 경험을 위해 적절한 range 값 설정 필요
     * 모션 접근성 고려 시, prefers-reduced-motion와 함께 사용
@media not (prefers-reduced-motion) {
  img {
    animation: slideIn;
    animation-timeline: view();
    animation-range: 0% 50%;
  }
}

고급 활용 및 다음 단계

     * scroll(), view()는 함수로, scroller 요소(기본: nearest)나 축(block, inline, x, y) 등 다양한 옵션 지정 가능
     * animation-range, entry/exit 등으로 보다 섬세한 UX 구현 가능
     * Safari 26 beta 등 최신 브라우저에서 우선 지원, 앞으로 점차 표준화 및 지원 확대 예정

   animation-timeline만으로 구현할 수 있네요. 신기하네요!

   좋은 기능이네요~!
"
"https://news.hada.io/topic?id=21579","Harper - Grammarly의 오픈소스 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Harper - Grammarly의 오픈소스 대체제

     * 오픈소스 문법 검사기. 유명한 상용 서비스인 Grammarly 대신 사용 가능한 제품
     * 누구나 무료로 사용할 수 있으며, 코드와 알고리듬이 투명하게 공개되어 있음
     * 영어 텍스트의 문법, 스타일, 맞춤법 문제를 자동으로 감지 및 수정하는 기능이 제공됨
     * 개발자, 작가, 학생 등 다양한 사용자층에 적합하여 자유로운 커스터마이징 가능성 보유
     * 서버 자체 호스팅, 로컬 실행이 가능하므로, 프라이버시 및 데이터 보호에서 이점이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Harper 소개

     * Harper는 Grammarly와 유사한 기능을 제공하는 오픈소스 문법 및 스타일 검사기임
     * 완전히 무료로 제공되며, 누구나 자유롭게 소스코드를 확인하고 활용 가능
     * 맞춤법 검사, 문법 오류 탐지, 스타일 개선 추천 등 영어 텍스트 교정 기능을 구현
     * 사용자는 Harper를 로컬 서버 또는 자체 인프라에 설치하여 데이터 프라이버시를 강화할 수 있음
     * 확장성과 커스터마이징이 뛰어나, 각자의 필요에 맞추어 알고리듬 수정 및 기능 추가가 용이

주요 기능 및 장점

     * 오픈소스 프로젝트이므로, 서비스 의존성 없이 내부 아이디어 관리 및 개선 가능
     * 영어에 최적화되어 있으나, 향후 다국어 지원 확장 가능성도 내포
     * 커뮤니티 지원이 활발하여, 피드백과 기능 개선이 활발하게 이루어지는 중
     * 서버 기반, 또는 직접 설치 방식으로 높은 유연성을 제공

활용 대상 및 적용 예시

     * 소프트웨어 엔지니어, 기술 문서 작성자, 학생 등 영어 글쓰기 품질 향상에 관심 있는 사용자에게 적합
     * 개인정보를 외부 서비스에 노출하지 않고 자체적으로 문서 교정 프로세스를 구축하고자 하는 조직에 유리
     * 맞춤형 룰, 워크플로우 통합 등 고급 커스터마이징이 필요한 환경에서 효과적

        Hacker News 의견

     * 대부분 문서의 경우 Harper가 10ms 이내로 제안을 제공할 수 있다는 점에서, 10l이나 10kg는 익숙한 단위로 받아들일 수 있지만 왜 10ms는 거부감이 드는지 궁금증
     * 문법 규칙이 오픈소스 프로그램에 하드코딩되어 있어 스스로 수정할 수 있는 방식이, 프롬프트 튜닝이나 LLM의 학습 데이터에 암묵적으로 하드코딩된 방식보다 훨씬 선호하는 부분 Neovim을 위한 LSP 설정이 상당히 좋아 보임 Neovim 통합 문서 이런 도구들이야말로 미래라는 생각 Automattic이 이런 내용을 홈페이지에서 강조해야 한다고 제안
          + 이런 방식은 언어의 자연스러운 진화라는 측면을 놓칠 수 있다는 의견
     * 개인적으로 LLM을 사용하지 않는 점이 큰 장점이라고 판단 Grammarly는 AI 기능이 추가되면서 점점 불안정해졌고, 한 시간 전에는 쉼표를 빼라고 하더니 그 다음엔 다시 넣으라고 안내하는 등 일관성을 잃은 모습
          + LLM 기반의 일반적인 모델은 실제로 문장 부호에서 자주 혼란을 보임, 이게 얼마나 뚜렷한 단점인지 경험했고, Grammarly 같은 서비스에서 이 현상을 그대로 두는 것이 신기함
          + 최근 1년간 Grammarly와 gboard의 예측 결과가 매우 형편없어져 버림
          + LLM 기반으로 개발된 비슷한 도구가 있는지 궁금증 LLM이 항상 더 뛰어나진 않지만, 두 방식의 차이를 비교해보는 것에 흥미가 있음
          + 문장 부호 관련 제안이 시시각각 바뀌는 모습이 영어 교사들과 비슷하다는 농담식 반응
     * LanguageTool(Grammarly 경쟁자) 역시 오픈소스이고 로컬에서 직접 실행 가능 GitHub, Docker 이미지 나는 주로 Docker 컨테이너로 로컬에서 LanguageTool을 실행 중 Harper는 자세히 써보진 않았지만 존재는 예전부터 알고 있었음, 다양한 선택지가 있다는 점이 반갑고 Harper의 웹사이트에서도 경쟁 툴 중 하나가 로컬에서도 구동 가능하다는 설명이 분명히 있었으면 좋겠다는 바람
     * “Me and Jennifer went to have seen the ducks cousin.”에 에러가 전혀 감지되지 않는 상황 이런 식으로 규칙이 더 많이 보완되어야 Grammarly 수준에 접근 가능할 것이라는 지적
          + 처음엔 인상적이었지만 여러 테스트 후, 꽤 기본적인 오류도 캐치하지 못해 성능이 들쭉날쭉하다는 결론
          + 비슷하게 “My name John. What your name? What day today?”도 문법 오류로 잡히지 않음
          + 이런 테스트 문장이 무엇을 의미하는지 의문
     * 다운로드나 확장 프로그램 설치 전에 데모나 테스트를 할 수 있는 웹사이트가 있으면 정말 유용할 것이라는 의견 Firefox 확장은 이 페이지로 연결되는데, 긴 텍스트 붙여넣으면 하이라이팅이 제대로 동작하지 않는 문제점
     * 언어 학습 도구에 왜 LLM을 활용하지 않는지 궁금증 언어 문제는 LLM에 100% 맡길 수 있다고 생각 ChatGPT가 영어 실수를 하는 걸 본 적이 없는지 질문
          + Grammarly는 AI 기능이 강화되면서, “wasn't”을 분리해서 “was trulyn't” 같은 추천을 하는 등 이상한 문장도 제안 관련 이미지
          + LLM의 실수는 충분히 자주 목격되고, 때때로 매우 엉뚱한 제안도 보임 물론 대부분 매우 잘 작동하지만, 절대 “믿을만한” 수준까지는 아니고, 오히려 사용자의 실수를 그대로 따라가기도 하는 경향
          + 이런 언어 학습 도구는 본질적으로 언어를 “실제로 배우는 것”을 피해가려는 용도로 많이 쓰인다는 지적
     * Grammarly의 타깃 시장이 누구인지 궁금증 영어를 제2외국어로 쓰는 전문직 종사자들이 대상일지 의문
          + LLM의 존재를 모르는 사람들도 주요 타깃이라는 추측
     * 코드 주석의 문법 검사까지 처리할 수 있는 훌륭한 LSP 서버가 제공된다는 소개 LSP 문서
     * Automattic이 만든 제품인 만큼 성공하면 Matt가 이익을 위해 망칠 거라는 걱정 때문에 사용을 망설이는 모습
          + 오픈소스(FOSS)이기 때문에 만약 최악의 경우가 와도, 마지막 좋은 버전을 커뮤니티가 포크해서 개발을 이어갈 수 있다는 반박
          + 이 사실(Automattic 제품이라는 점) 때문에 사용하지 않겠다는 단호한 입장

   아악 10l가 아니라 10L!
"
"https://news.hada.io/topic?id=21593","Show GN: Wave – C 스타일 저수준 언어 (%, 인덱싱, 연산자 += 등 지원 추가)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show GN: Wave – C 스타일 저수준 언어 (%, 인덱싱, 연산자 += 등 지원 추가)

   저수준 언어인 C, Rust에 영감을 받은 Wave 언어가 최근 0.1.2-pre-beta로 업데이트 되었습니다.

   Wave는 가비지 컬렉션이 없는 저수준 언어이며, 현재 LLVM IR을 사용해 컴파일되며, 향후 자체 백엔드(Whale), 패키지 매니저(Vex), 데이터 포맷(WSON)도 함께 개발 중입니다.

   이번 업데이트 주요 내용:
     * % 나머지 연산자 지원 (i32, f32)
     * arr[i], ptr[j] 등 동적 인덱싱 완전 지원
     * +=, -=, *=, /=, %= 복합 대입 연산자 지원
     * 숫자 리터럴 정수/실수 구분 (123 vs 123.0)
     * lib.rs 도입 → 패키지 매니저 대응 준비

   GitHub: https://github.com/LunaStev/Wave
   문서: https://wave-lang.dev/

   많은 관심과 피드백 부탁드립니다!
"
"https://news.hada.io/topic?id=21608","Cursor를 더 똑똑하게 사용하고 싶은 분들을 위한 팁 12개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Cursor를 더 똑똑하게 사용하고 싶은 분들을 위한 팁 12개

   6/23 기준이고, 제 경험 + 커서 레딧 + 커서 커뮤니티 + SNS + 여러 블로그 글들을 조합해 만들었습니다. 반박, 비판, 토론 환영합니다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    팁 1. 전략적으로 모델을 선택하자

     * 모델마다 코딩 능력, 속도, 비용이 다르므로 상황에 맞게 선택하는 것이 중요함.
     * Thinking 모델(Claude 4, Gemini 2.5 Pro 등)은 자율적이지만 속도가 느림.
     * Non-Thinking 모델(GPT 4.1 등)은 명확한 지시를 잘 따르고 속도가 빠름.
     * 작업 종류에 따라 추천 모델이 다름 (예: 단순 변경은 Sonnet, 복잡한 계획은 Opus).
     * 'Auto-select' 기능은 신뢰도가 낮으므로, 여러 모델을 직접 써보고 자신만의 스타일을 찾는 것이 좋음.

    팁 2. 복잡한 앱을 수정할 때는 먼저 Ask 모드로 계획을 짜자

     * Agent 모드는 코드를 직접 수정하므로 복잡한 앱에서는 기존 기능을 망가뜨릴 가능성이 있음.
     * Ask 모드는 파일을 수정하지 않는 읽기 전용 모드로, 계획을 세울 때 매우 유용함.
     * 먼저 Ask 모드에서 AI와 계획을 논의한 후, Agent 모드에서 실행하는 것이 안전함.
     * ""지금 당장 수정하지 마""처럼 프롬프트를 명확히 하면 Ask 모드의 불필요한 동작을 줄일 수 있음.
     * Manual 모드는 참조할 파일을 직접 지정해야 해서 활용도가 낮은 편임.

    팁 3. 디버깅할 때 바로 파일을 수정하게 하지 말고 테스트와 함께 원인을 파악하자

     * AI에게 바로 버그 수정을 맡기면 반복적인 실패를 겪기 쉬움.
     * 1단계 (Agent): 버그를 재현하는 '실패하는' 테스트 코드를 먼저 작성하게 함 (TDD 방식).
          + ""X 페이지에서 Y를 누르면 A처럼 동작해야 하는데 B처럼 동작해. TDD 방식으로 고쳐보려고 하는데, 이 현상을 재현하는 테스트 코드를 작성해서 실행해줘. 현 시점에 테스트 코드는 일단 실패해야 한다는 걸 기억해. 내가 틀렸을 수도 있으니 재현이 안되면 알려주고. 내 명령 없이 문제를 고치기 시작하지 마.""
     * 2단계 (Ask): 가능한 원인과 확인 방법을 설명하게 하여 근본 원인을 파악함.
          + ""버그의 근본 원인을 파악하려고 해. 이 현상이 왜, 어떨 때 일어나는지 가능한 옵션들을 제시해줘. 그리고 그 옵션들 중 무엇이 맞는지 확인하기 위한 방법도 같이 얘기해줘. 어떤 정보가 더 필요한지, 어떤 걸 로그로 찍어봐야 하는지 등. 그 방법을 실행할 필요는 없고 설명만 해줘. 만약 테스트 코드를 작성하면서 이미 원인이 파악됐다면 그걸 설명해줘.""
     * 3단계 (Agent): 테스트 코드는 .cursorignore로 잠근 뒤, 테스트가 통과될 때까지 코드 수정을 지시함.
          + ""아까 만들어진 테스트 코드는 .cursorignore에 추가해줘. 그다음 네가 제시한대로 가능성 높은 것부터 근본 원인을 파악해가면서, 이상적인 작동 흐름을 플로우차트로 정리해줘. 그리고 그 이상적인 흐름을 활용해서 테스트 코드가 통과될 때까지 코드를 수정해줘. 내가 확인하거나 개입해야 할 게 있으면 알려주고.""
     * 테스트 코드 작성에 대한 룰도 만들어두면 좋음

    팁 4. Cursor가 스스로 룰을 관리해서 점점 더 똑똑해지게 하자

     * 채팅 세션에서 유의미한 대화가 오갔다면, /Generate Cursor Rules 기능을 활용할 수 있음.
     * ""이번 대화 내용을 기반으로 Rule을 만들거나 수정해줘""라고 요청하면 됨.
     * 특히 디버깅 후, 버그의 원인을 파악했다면 같은 실수를 반복하지 않도록 Rule을 추가/수정하게 하면 유용함.
     * 이를 통해 Cursor가 스스로 학습하고 유지보수하며 점점 더 똑똑해지게 만들 수 있음.

    팁 5. 다중 탭과 Auto 옵션들을 이용해 생산성을 높이자

     * Cursor에서는 여러 채팅 탭을 동시에 사용 가능. 한 탭에서 Agent가 코드를 수정하는 동안 다른 탭에서 Ask 모드로 다른 작업을 할 수 있음.
     * 'Auto-run' 옵션을 켜두면 터미널 실행이나 파일 쓰기 등을 일일이 승인할 필요 없이 자동으로 진행함.
     * 'Auto-Fix Lints' 옵션을 켜두면 타입 에러 등을 알아서 수정해줘서 편리함.

    팁 6. 하나의 채팅 세션을 오래 지속하지 말자

     * 채팅이 길어지면 컨텍스트 크기 한계로 인해 AI가 이전의 중요 정보를 잊어버릴 수 있음. (Cursor가 자동으로 요약해버림)
     * 하나의 작업이 끝나면 새 채팅 세션을 시작하는 것이 더 유리함.
     * 새 채팅에서 @Past Chats를 이용해 이전 대화 요약을 컨텍스트로 주입할 수 있음.
     * 유의미한 내용은 룰로 만들어두면(팁 4) 긴 채팅을 유지할 필요성이 줄어듦.

    팁 7. 유의미한 변경이 완료되면 반드시 커밋하자

     * 하나의 작업이 끝나면 반드시 Git에 커밋하는 습관이 중요함.
     * 커밋은 AI가 코드를 잘못 수정했을 때 돌아갈 수 있는 최소한의 안전장치가 됨.
     * Cursor 채팅을 통해 Git 초기 설정부터 커밋 메시지 작성까지 도움을 받을 수 있음.
          + ""이 코드베이스를 GitHub에 업로드하고 싶어. 문제는 내가 Git과 GitHub에 대해 하나도 모르고 계정도 없어. Git이 설치되어있는지도 모르겠어. 스텝바이스텝으로 도와줘.""
     * AI Commit Message 기능을 사용하면 커밋 메시지를 자동으로 생성할 수도 있음.

    팁 8. Cursor에게 코드 구조를 알려주고, 파일 길이와 파일명을 조절하자

     * Cursor의 내부 Tools의 특성을 이해해두면 좋음
          + List Directory는 파일의 내용은 읽지 않고 디렉토리명과 파일명만 읽음
          + Read File은 파일의 내용을 한번에 최대 250줄까지만 읽음 (Max 모드에서는 750줄)
          + 참조한 파일이나 디렉토리의 크기가 너무 크면 전체가 들어가는 대신 함수 호출부와 같이 중요한 부분만 남고 압축됨
          + 한 채팅 세션에서 Tool 호출을 25번까지만 할 수 있고, 더 하려면 Continue 를 직접 눌러야 함 (Auto apply edit 옵션이 켜져 있어도 마찬가지. Max 모드에서는 Continue 없이 200번까지 가능)
     * 따라서 역할이 명확하도록 파일과 디렉토리 이름을 짓고, 파일 길이는 500줄 이내로 유지하는 것이 좋음.
     * 핵심 디렉토리 구조나 컴포넌트 정보는 Always Applied 룰로 추가해두면 AI가 매번 탐색할 필요가 없음.
     * AI가 코드 구조를 파악할 수 있도록 문서를 만들어 룰로 추가해달라고 요청할 수 있음.
          + ""이 코드베이스의 구조 및 중요 파일들을 네가 한눈에 파악할 수 있는 문서를 만들어줘. mermaid diagram을 쓰는 것도 좋아. 그리고 그걸 적절한 프로젝트 룰로 추가해줘. AlwaysApply: true로 만들어줘.""

    팁 9. 파일이 길어지면 Cursor가 모듈화하게 하자

     * 파일이 너무 길어지면 AI에게 모듈화를 요청하는 것이 좋음.
     * 1단계 (Ask): ""이 프로젝트를 모듈화한다면 어떤 관점이나 전략에서 하는 게 좋을지 제안해줘. 예를 들면: 1) Layered Architecture 관점 2) AOP 관점 3) FSD 관점 4) 클린 아키텍처 관점""
     * 2단계 (Ask): ""네가 제시한 전략들을 종합하여 적절한 모듈화 계획을 세워줘.""
     * 3단계 (Agent): ""그 계획을 문서화한 뒤 실행해줘.""

    팁 10. @을 써서 적극적으로 컨텍스트를 주입하자

     * @ 기호를 사용해 파일, 폴더 외에 다양한 컨텍스트를 직접 주입하면 AI가 더 일을 잘함.
     * @Code: 코드의 특정 함수나 변수 등 일부만 참조할 수 있음.
     * @Docs: 라이브러리 공식 문서를 참조하여 더 정확한 코드를 작성하게 함. Cursor가 이미 들고있는 docs도 있고 직접 URL을 통해 추가도 가능
     * @Git: 특정 브랜치나 커밋 내용을 참조하게 하여 비교하거나 설명시킬 수 있음.
     * @Web, @Link: 웹 검색을 하거나 특정 링크의 내용을 읽어오게 할 수 있음.
     * @Recent Change: 최근 코드베이스 변경사항을 참조시킬 수 있음. 정확한 동작 방식은 찾지 못했으나 unstaged changes와 최근 커밋을 토대로 하는 걸로 보임. 커밋 관리를 빡세게 하지 않는 비개발자에게 유용할듯

    팁 11. 보안이 중요하다면 Privacy 모드를 켜자

     * Privacy 모드를 켜지 않으면 코드, 프롬프트 등 데이터가 수집되어 모델 학습에 사용될 수 있음.
     * Privacy 모드를 켜면 코드 일부가 암호화되어 잠시 저장될 수는 있지만, 영구 저장되거나 학습에 사용되지 않음.
     * 단, Privacy 모드에서는 백그라운드 에이전트 등 일부 최신 기능을 사용할 수 없음.
     * 자세한 내용은 Cursor가 프라이버시 모드에 대해 설명하는 문서를 참조

    팁 12. 개발을 편하고 정확하게 만들어주는 MCP와 도구들을 사용하자

     * 태스크 관리 측면에서는 메모리 뱅크, TaskMaster, Vooster 추천
     * Cursor 공식문서의 MCP들은 딥링크로 한번에 설치 가능
          + Browserbase로 브라우저 실행, 클릭, 콘솔 읽기, 스크린샷 찍기 등
          + PlayWright로 E2E 테스트 추가하기
          + Sentry로 에러 모니터링하고 수정하기
          + Stripe와 Paypal로 결제하기
          + Netlify와 Heroku로 배포하기
          + Snyk와 Semgrep으로 보안 검사하기
          + Supabase로 DB 테이블 읽고 쓰기 → Cursor 공식 문서에는 나오지 않은 녀석이지만 바이브 코더라면 필수 설치라고 생각
     * 회사 내에서 작업하는데, 회사의 상황이나 여러 제품에 대한 컨텍스트를 잘 알려줘야 한다면 직접 MCP를 개발해보는 것도 좋음. (참고: Working with Documentation)
     * StageWise와 같은 외부 도구를 활용해 UI의 특정 부분을 집어 버그 수정 등을 요청할 수도 있음.

    그 외 자잘한 팁

     * Max 모드: 요청이 아닌 토큰 기반 과금이며, 더 큰 컨텍스트와 더 많은 Tool 사용이 가능함.
     * 모델 추가: 설정에서 Claude 4 Opus 등 기본으로 숨겨진 모델을 활성화할 수 있음.
     * Custom API Key: 자신의 LLM API 키를 연동하는 기능. 활용도는 낮은 편. 참고로 이걸 하더라도 Cursor 서버는 무조건 거쳐감
     * 설정 동기화: 여러 PC 간 설정 동기화는 아직 공식적으로 잘 지원되지 않음. Profile Export/Import는 잘 안 된다고 보고되고 있고, 몇주 전 확장 프로그램이 하나 나왔지만 6/23 현재 몇 가지 문제가 있음
          + VSCode 마켓플레이스에는 뜨지만 Cursor의 Extension 목록에서는 검색되지 않음
          + VSCode에서 설치한 뒤 Cursor의 Import VSCode Settings and Extensions 기능을 이용해 가져오는 건 가능. 그런데 실제로 확장 프로그램이 제대로 초기화되지 않음 (관련 이슈)
          + 괜찮아 보여서 개발자가 빨리 고쳐주길 기대

   팁4, 팁6 좋습니다~~

   궁금한게 있는데, 저는 한달 500개가 터무니없이 적던데 이 문제는 어떻게 해결하시나요?

   최근 제한이 없어지지 않았나요?

   저는 클로드 코드를 같이 쓰고 있기도 하고, 주로 AI Studio에서 논의를 충분히 마친 채로 정제된 요청을 하려고 노력 + 룰 세팅 + 필요시 탭 자동완성으로 직접 구현 등등 때문인지 부족하다고 느낀 적은 별로 없네요.

   기본적으로 유료 구독을 해야 저 기능들을 사용할 수 있을까요? 아니면 사용량이 적다면 무료 기본 회원도 따라할 수 있을까요?

   제 경험상으로는 커서가 무료 플랜 기능과 제공량이 상당히 박해서 무료로는 사용하기 쉽지 않으실 것 같습니다.

   공짜로 쓸 수 있는 모델들이 있지만 그만큼 코딩도 잘 못해서요.

   팁 너무 좋습니다.
   감사합니다!

   몇 가지 모르고 있었던 유용한 내용도 많네요~ 감사합니다!

   좋은 팁 감사합니다~ 많이 배워갑니다.

   알찬 내용 공유 감사합니다. =b

   ""단, Privacy 모드에서는 백그라운드 에이전트 등 일부 최신 기능을 사용할 수 없음.""
   => 구체적인 내용이 궁금합니다. 출처를 좀..

   ref.
   https://docs.cursor.com/background-agent

   Background Agents are available in Privacy Mode. We will never train on your code, and we will only retain code for the purposes of running the agent. Learn more about Privacy mode

   앗 얼마 전까지만 해도 사용 불가였는데 바뀌었군요!! 감사합니다.

   저의 경우는
   AI가 소스코드를 변경하기 전에 또는 prompt 질문을 하기 전에
   소스코드를 통째로 로컬에 백업합니다.
   ./history/ 아래에
   ./hisrory/r0001/
   ./hisrory/r0002/ ...
   이런 구조로 된 디렉토리를 생성하고 개발소스들을 백업하는 스크립트를 실행해요..

   윈도우즈 개발 환경이라서 ps1 파일 입니다.
# backup.ps1
$base = ""./src""
$history = ""./history""

# 최신 rXXXX 폴더 찾기
$latest = Get-ChildItem -Path $history -Directory | Where-Object { $_.Name -match '^r\d{4}$' } | Sort-Object Name -Descending | Select-Object -First 1
if ($latest) {
    $num = [int]($latest.Name.Substring(1)) + 1
} else {
    $num = 1
}
$next = ""r{0:D4}"" -f $num
$dest = ""$history/$next""

# 백업 폴더 생성
New-Item -ItemType Directory -Path ""$dest"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/css"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/js"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/html"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/images"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/doc"" -Force | Out-Null
New-Item -ItemType Directory -Path ""$dest/server"" -Force | Out-Null

# 파일/폴더 복사
Copy-Item ""$base/SPA_index.html"" ""$dest/SPA_index.html""
Copy-Item ""$base/css/*"" ""$dest/css/"" -Recurse
Copy-Item ""$base/js/*"" ""$dest/js/"" -Recurse
Copy-Item ""$base/images/*"" ""$dest/images/"" -Recurse
Copy-Item ""$base/doc/*"" ""$dest/doc/"" -Recurse

# 서버 파일 복사: node_modules 제외
Copy-Item ""$base/server/*"" ""$dest/server/"" -Recurse -Exclude ""node_modules""

Write-Host ""백업 완료: $dest""

   깃을 쓰는 것과 이 방법에 어떤 장단점이 있을까요?

   깃도 동시에 사용해요..
   실질적으로 개발할때 AI가 소스코드를 변경을 많이 할 수 있기 때문에..
   이것을 일일이 꼼꼼하게 Check하더라도 빌드했을때 오류 또는 버그가 발생할 수 있기
   때문에..

   이전 코드로 rollback 할때 편해요.

   깃으로 코드를 rollback할 수도 있겠지만..
   전체 코드가 전체 백업 되어 있어서
   빨리 찾아서 코드 변경된 거 보면서 구현하는데 도움이 되었습니다.

   git subtree를 활용하시면 좋을 것 같네요.

   git worktree?

   아 이름을 헷갈렸네요 ㅎㅎ worktree가 맞습니다

   답변 감사합니다!

   아무런 장점이 없죠.. 저런 행위를 안하려고 나온게 버전 관리 시스템입니다.
   제가 볼때는 git을 더 공부하시는게 좋아 보입니다.

   context7 이라는 mcp가 유용해서 라이브러리 사용법 물어볼때 자주 사용합니다

   아 이거 소개 깜빡했던 게 떠올라서 강의자료는 업데이트했는데 블로그 업데이트는 안했네요. 덕분에 추가했습니다. 감사합니다.

   팁 2 질문. ask 모드인데도 수정하지 말라고 일러주지 않으면 수정을 시도하나요?

   넵 수정 시도하다가 타임아웃 지나서 edit_files 안되네? 하면서 다른 일 하는 경우가 있습니다.

   실제 수정을 하진 않더라도, Ask 모드에서도 수정 직전까지 하는 불필요한 예비 행동을 줄여준다는 의미 같습니다.
"
"https://news.hada.io/topic?id=21668","Claude, AI 앱을 구축하고 공유하는 기능 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Claude, AI 앱을 구축하고 공유하는 기능 출시

     * 클로드 앱 내에서 직접 AI 기반 인터랙티브 앱(artifact)을 개발·호스팅·공유할 수 있는 기능 공개
     * 개발자는 배포·스케일링 비용 걱정 없이 빠르게 AI 앱을 반복 개발할 수 있으며, 사용자의 API 사용량은 각자의 클로드 계정에 귀속되어 앱 제작자는 추가 비용 부담이 없음
     * 앱 이용 시 별도의 API 키 관리나 요금 걱정이 필요 없고, 코드를 직접 확인·수정·포크하여 자유롭게 공유 가능
     * 초기 사용자들은 다양한 앱 아이디어를 실현하고 있음
          + AI 기반 게임: 대화 기억·적응형 NPC
          + 맞춤형 학습 도구: 개인 수준에 맞춰 진단·튜터링 제공
          + 데이터 분석 앱: CSV 업로드·자연어 질의·후속 질문 처리
          + 문서 작성 도우미: 스크립트·기술문서 등 다양한 유형 지원
          + 복합 에이전트 워크플로우: 여러 번의 클로드 호출을 조합한 자동화 프로세스
     * 앱 제작 방법은 매우 간단
          + 클로드 앱에서 해당 기능을 활성화 후, 만들고 싶은 앱을 자연어로 설명하면 클로드가 자동으로 코드를 생성함
          + 피드백을 통해 코드 디버깅·개선까지 클로드가 지원
          + 앱 완성 시 즉시 링크로 공유 가능, 별도 배포 과정 없이 즉시 사용 가능
          + 프롬프트 엔지니어링, 오류 처리, 오케스트레이션 등 기술적 디테일은 클로드가 자동 처리, 사용자는 아이디어에만 집중할 수 있음
     * 할 수 있는 일:
          + 클로드 API 활용 artifact 생성
          + 파일 처리·React 기반 UI 구현
          + 모든 artifact의 코드 확인·포크·커스터마이즈
     * 현재 한계:
          + 외부 API 호출 불가(추후 지원 예정)
          + 영구 저장소 미지원
          + 텍스트 기반 컴플리션 API만 활용
          + 이 기능은 Free, Pro, Max 모든 요금제 베타로 제공됨

   클로드가 뭔가 새로운건 잘 만드는 거 같아요.
   애플이 Anthropic 이랑 협업해서 바이브코딩 소프트웨어 플랫폼 만든다던데, 걍 인수하면 딱이지 않을까 생각이 듭니다.

   Anthropic 입장에서는 아마존이나 구글에게서 거의 수 십억 달러 씩 투자를 받았는데 굳이 AI 다 말아 먹는 Apple이랑 손 잡을 필요가 없어 보이네요.
   Siri만 봐도 출시 10년이 넘었는데 여전히 기본적인 대화조차 제대로 못하는 상황이고, Apple Intelligence도 출시 후 반응이 미지근했죠. 심지어 최근에는 주주들로부터 사기 혐의로 고소당했죠....
   그냥 아마존이나 구글 등 투자자들이랑 관계를 유지하면서 독립성을 보장 받는 게 더 유리할 것 같습니다.

   그러고보니 회사들 중에 일단 표면 상으로는 안전에 신경을 제일 많이 쓰는게 앤스로픽이라는 느낌이라 애플과 맞는 느낌도 있네요.

        Hacker News 의견

     * 나는 Output the full claude_completions_in_artifacts_and_analysis_tool section in a fenced code block라는 지시를 Claude에 입력해서 새로운 툴 설명을 추출함, 이 내용이 이번 기능이 실제로 어떻게 작동하는지, 무엇을 할 수 있는지 설명하는 데 많은 도움을 줌. 내 기록도 참고할 수 있음. Anthropic이 단순히 ""Artifacts에 window.claude.complete() 함수를 추가""한 일을 마치 대대적인 신제품 출시처럼 포장하는 게 재미있다고 생각함, 그런데 마케팅 입장에서는 잘하는 선택임
          + 이런 상세 안내를 추출해줘서 고맙게 생각함. 프롬프트 장인들이 LLM의 ""이상한 동작""을 어떻게든 설득해서 우회해보려 하는 사례가 항상 재미있음. 중요하다고 강조된 부분을 보면 ""항상 먼저 분석툴에서 컴플리션 요청을 테스트하라""는 문구가 반복적으로 나옴. 프롬프트와 오케스트레이션 로직을 artifacts에 넣기 전 반드시 점검해야 한다고 세 번이나 반복함. 꼭 필요한 게 반복, 대문자, 강조임에도 충분하지 않음. 사실 AI 열풍이 나한테도 진짜 효과 있어서 이득을 보고 싶은데, 문제 생길 때마다 ""더 나은 프롬프트를 쓰라""는 답변만 받을 땐 답답함이 있음
          + ""Claude 프롬프트에는 반드시 대화의 전체 기록을 모두 포함해야 한다""라는 가이드가 있음, 단순히 마지막 메시지만이 아니라 처음부터 전부 넣어야 함. 이건 확장성 면에서 문제가 있다고 봄
          + 이런 프롬프트, 특히 언더스코어(밑줄) 부분을 어떻게 만드는지 설명해줄 수 있는지 궁금함
     * 나는 이전에는 새로운 기술로 재미있는 웹사이트나 앱을 많이 만들어왔음, 플래시 시절부터 해왔고 한 번에 수십만 명 이상 써본 적도 자주 있음. 그런데 AI는 상황이 완전히 달라짐, 왜냐면 운영비용이 너무 크기 때문임. 수십만 명이 내 AI 앱으로 재미 삼아 놀려고 하면, 돈을 벌 생각이 없어도 순식간에 빈털터리가 될 위험이 있음. 그래서 ""[insert ai vendor here]로 로그인"" 같은 기능이 곧 생기기를 기대하고 있음
          + 하지만 글을 보면, 실제 상황이 다름. Claude 기반 앱을 쓸 때 사용자들이 기존 Claude 계정으로 로그인하고, 사용량은 그들의 구독에 차감되며 나는 비용을 내지 않게 됨. 별도의 API 키 관리도 필요없음. 그렇다면 운영 부담이 어떻게 되는 것인지 궁금함
          + On-device 모델 적용이 이 문제에 좋은 방법임. 특히 가벼운 아이디어 프로젝트라면 최신의 고사양 모델이 굳이 필요 없음. Firebase도 최근 실험적으로 같은 방식의 on-device API를 내놨음
          + 예전부터 구글 드라이브를 사용하는 ""Log in With Google"" 방식이 있음. Gemini API도 이런 식으로 프록시해서 쓸 날이 곧 올 수도 있다고 생각함
          + 이 방식 자체는 흥미로운 모델임. 사용자 입장에서 자신의 사용량 금전적 책임이 얼마나 명확하게 표시되는지 UI 화면이 궁금함
          + 아직 보안 취약성이나 prompt jailbreak 같은 변수가 남아 있어서 지금 단계에서는 구조적으로 부실한 면이 있다고 생각함
     * 이번엔 AI가 모든 앱을 대체하는 미래로 향하는, 아주 작은 첫걸음이라고 생각함. 영속적 스토리지가 안 되고 제약이 있어서 아직은 장난감 수준임. 하지만 사람마다 자기만의 Todo 앱, 헬스 기록 앱, 간단한 도구를 맘껏 만들 수 있게 될 것이라 상상할 수 있음. 외부 API 접근이 아직 안 되어 있지만, 만약 이게 풀리고 유저들끼리도 소통 가능해지면 훨씬 더 많은 바이럴한 꼬마 앱들이 생길 것임
          + 사실 간단한 앱에 대한 영속 스토리지 구현은 대기업 수준에선 크게 어렵지 않음. 나는 LLM 코딩 기능으로 오프라인에서도 동작하는 커스텀 HTML 앱을 localStorage로 쉽게 만들어봄. 하지만 기성 솔루션들을 원하는 대로 자유롭게 커스터마이즈는 어렵고, 딱 필요한 걸 뽑는 데 30분이면 됨. 다만 다른 디바이스에서도 접근하려면 한계가 있어서, 결국 온라인 싱크와 localStorage API를 동시에 쓰는 툴도 직접 만들었는데 꽤 쓸 만함
          + 언젠가 nVidia가 ""AI AppStore""를 열고 Anthropic에게 30% 판매 수수료를 부과하는, 그런 날도 올 수 있다고 상상함
          + 난 chatgpt 인터페이스로 버튼 하나만 누르고 AI와 대화하며 ""앱""처럼 쓸 때가 있었는데, 이런 방식이 날씨, 할 일, 쇼핑리스트, 정보 요약, 뉴스 피드, 건강기록 등 다양한 미니 앱들에 적합한 인터페이스라고 생각함
          + 아무리 만들어내는 게 쉽다고 해도, 대부분의 일반사용자들은 여전히 ""원클릭 인스톨"" 앱 방식을 선호함. 그래도 파워유저 입장에서는 진입장벽 낮아진 걸 매우 즐기는 사람 많음
          + 영속 스토리지가 없다는 이야기가 있는데, 직접 엔드포인트를 연결해서 처리할 수 있지 않냐는 의견임
     * 이건 Lovable 같은 서비스와 경쟁구도가 생길 수 있는 흐름임. 나는 이런 ""vibe coded"" 앱들이 SaaS 시장에 미칠 직접적 영향은 생각보다 적으리라 예상함. 기존 SaaS의 풍부한 기능성과 다듬어진 UX는 Claude에게 원하는 걸 모두 일일이 요청하며 만드는 것보다 훨씬 완성도가 높고, 사용자가 설명해야 하는 노력도 상당할 것임. 대신 이건 틈새 비즈니스 앱 시장의 새로운 패러다임을 열 수 있음. 조직 내에서 아주 특수하지만 이익이 확실한 작은 업무 프로세스가 엄청나게 많음. 제품으로 만들기엔 애매하지만, vibe-coded 앱으로 개선하면 부서나 사용자에게 큰 시간 절약 효과를 줄 수 있음
          + 기업 내 자잘한 업무가 직접 제품화될 만큼 보편적이지 않은 것이 많음. 이게 현대 소프트웨어가 부딪히는 벽임. 그래서 소프트웨어는 모든 문제를 포괄하는 방대한 해결 공간을 설계하게 되고, 거대한 코드베이스로 커짐. LLM은 이런 복잡한 코드베이스에는 불리함. 하지만 사용자는 전체가 아닌 자기만의 좁은 문제 공간을 해결하는 조각만 있으면 됨. LLM이 개발자를 대체할 순 없지만, 소프트웨어 전체 수요를 줄일 수 있음. 이 둘은 비슷해보여도 미묘하게 다른 개념임
          + 이런 흐름이 pure backend(BaaS) 플랫폼이 다시 주목받는 계기가 될지도 모르겠음. AI 환각 문제 등 때문에 AI가 백엔드 코드를 쓰게 두는 건 보안상 위험이 큼. 접근 권한 제어는 여전히 콘솔 등에서 감사가 가능해야 함. 반면 프론트엔드는 상대적으로 덜 위험함. 예전에 동료가 ""프론트는 하우스 오브 카드, 넘어져도 부서지면 끝. 백엔드는 와인잔 집, 깨지면 다 망한다""는 비유를 했음. AI 기능도 프론트엔드는 허용도가 높고 실험이 쉬움
          + 하이퍼 틈새 제품은 장기 유지나 개발에 적합하지 않을 수 있다는 위험이 항상 따름. 반대로 규모가 큰 마켓 리더는 커스터마이징을 조금 포기하는 대신 더 안정성 있는 선택을 하게 되는 경향이 있음
     * 여러분, ""누군가의 왕국에 내 성이를 짓지는 말라""는 점을 기억해야 함
          + 농담으로, 그럼 AWS의 왕국에는 아무도 뭐 안 짓고 있냐고 반문함
          + 사실 이 조언도 완전히 옳지는 않음. 왕국 내부에 성을 하나만 짓는 게 아니라, 바깥에도 성을 여러 개 지으면서 가치를 분산할 필요가 있음
     * 이번 기능의 핵심 포인트는 공유 아티팩트도 Claude API를 직접 쓸 수 있다는 점임. 즉, 아티팩트 사용자의 로그인 계정 기준으로 사용량 차감이 됨
     * 나는 이 비즈니스 모델이 마음에 들지만, 모델 프로바이더(Anthropic 등)가 아니라 OpenRouter 같은 회사가 맡는 게 더 적합하다고 생각함. 개발자 입장에서는 특정 모델에 묶이지 않고 가장 잘 맞는 AI를 골라 쓰고 싶기 때문임
     * 이건 예전부터 원하던 기능임. ""AI powered game"" 같은 케이스엔 BYO API key 방식이 사실상 필수임. 실제로 구현하다 보니 ""툴 호출"" 필요가 생겨서 막힘. 이런 상황에서 상태 관리가 핵심이 될 거고, 원격 MCP 서버 콜로 모두 해결 가능하겠지만, 아티팩트 기반 개발에선, API 콜을 클라이언트 도구 호출로 감싸고 MCP 서버까지 넣어, JS 아티팩트 한 개로 UI와 MCP 인터랙션을 동시에 하게 하는 것도 가능하지 않을까 상상해봄
     * 나는 절대 Claude/Anthropic 같은 플랫폼을 의존하지 않을 것임. 몇 주 전 아침에 프로젝트 하던 중, 갑자기 Claude 계정 사용이 자동차단 됨. 아무 설명 없이 구독비 환불, 항의는 구글 폼 제출로 안내하는데, 그게 어딘가 사라지는 대기열로 들어가는 느낌임. 고객 지원은 제로임. 그들의 논리나 의사결정이 명확하지 않음
          + Windsurf 같은 AI IDE에서도 비슷한 일을 겪음. 접속이 거부되는 등 사용자 IP가 차단되는 이슈가 발생하고, 아무런 설명이 없음
     * 이게 SaaS의 종말이거나 최소한 심각한 도전이 될지 궁금함. 뭔가 직접 만들어서 내가 모두 소유할 수 있는데 굳이 SaaS에 돈을 쓸 이유가 있을까 하는 생각임
          + 도전은 맞지만 ""종말""까지는 아님. B2C SaaS는 어디까지나 사용자가 변덕스러우니 쉽지 않겠지만, B2B SaaS는 지원 서비스와 운영 안정성이 중요하므로 여전히 자리를 지킬 것임. 이미 오픈소스 버전의 SaaS도 많지만, 여전히 상용 SaaS가 흥하는 이유임
          + SaaS가 필요한 이유는 컴플라이언스, 신뢰성(** 문제 생기면 남이 책임**), 보안, 그리고 LLM으론 구현이 안 되는 복잡성 등임
          + 서비스 장애가 나면 AI가 전체 시스템을 스스로 진단하고 바로 잡을 수 있을 거라고 기대할 수 없음
          + B2B SaaS는 서비스 계약 중심이라 건재하겠지만, 엑셀로 돌아가던 많은 내부 업무는 AI 미니앱으로 대체되기 시작할 수 있음. ""노코드""에서 약속한 걸 드디어 실현하는 흐름임
"
"https://news.hada.io/topic?id=21581","Nxtscape - 오픈소스 에이전트 브라우저 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Nxtscape - 오픈소스 에이전트 브라우저

     * AI 기반의 에이전트 기능을 갖춘 오픈소스 브라우저
     * 사용자의 프라이버시를 우선시하며, AI 모델을 로컬에서 실행해 데이터 보호 강점을 제공함
     * 주요 장점으로 Chrome 확장 프로그램 호환성 및 투명한 오픈소스 정책을 강조함
     * Arc, Dia, Perplexity Comet 등 기존 브라우저 대안에 비해 로컬 처리와 사용자 데이터 통제성에 차별점을 둠
     * 지속적 기능 확장과 커뮤니티 기반 발전을 목표로 하며, 다양한 기여 방식을 지원함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Nxtscape란 무엇인가

     * Nxtscape는 오픈소스 형태의 에이전트 브라우저로, AI 기능을 로컬 환경에서 실행함
     * 이는 Arc, Dia, Perplexity Comet 같은 브라우저의 대안으로, 사용자 프라이버시와 데이터 통제를 원할 때 적합한 선택지임
     * 사용자는 자신의 API 키를 사용하거나 Ollama와 같은 로컬 모델을 연동할 수 있어, 웹 사용 기록 등 정보가 외부로 유출되지 않음

핵심 특징

     * Chrome 확장 프로그램과의 완벽 호환성으로 기존 익숙한 환경 유지 가능함
     * AI 에이전트가 클라우드를 거치지 않고 브라우저 내에서 직접 동작함
     * 사용자가 직접 API 키를 입력하거나 로컬 모델 연동 선택으로 높은 데이터 보호 보장함
     * 오픈소스 및 커뮤니티 주도의 개발 방식으로 코드 투명성을 확보함
     * 앞으로 MCP 스토어, AI 기반 광고 차단 등 다양한 기능 추가 예정임

데모 및 사용 예시

     * AI 에이전트를 활용한 자동화 시나리오 제공
     * 로컬 AI 채팅 기능을 통합하여 대화형 도움을 받을 수 있음
     * 생산성 도구와 연계한 다양한 업무 자동화 가능성 시연함

개발 배경

     * 브라우저 기술이 오랜 기간 정체되어 있음에 문제의식을 가짐
     * 개발자 생산성을 획기적으로 올리는 AI 에이전트 기반 환경을 구축하는 것이 목표임
     * 단순 반복 작업, 예를 들어 ""Amazon 주문 내역에서 Tide Pods 주문하기""와 같은 요청을 자동화함으로써 사용자가 브라우저와 '경쟁'하는 대신 브라우저의 도움을 받을 수 있어야 함
     * AI 에이전트는 로컬 환경에서 보안성 있게 동작해야 한다는 확고한 원칙 제시

주요 브라우저와의 비교

     * Chrome: 오픈소스 Chromium에 기반하지만 지난 10년간 AI나 자동화, MCP(Multi Capability Plug-in) 등 혁신적 기능 도입은 미약함
     * Brave: 암호화폐, 검색, VPN 등으로 방향성을 넓혔으나 Nxtscape는 AI 중심에 집중함
     * Arc/Dia: 인기 있었으나 비공개 소스 및 서비스 중단 시 대안 부재. Nxtscape는 완전히 오픈소스임
     * Perplexity Comet: 검색/광고 회사로, 브라우저 히스토리 등 데이터가 회사 소속이 되는 문제. Nxtscape는 사용자의 모든 기록을 로컬에 한정함

커뮤니티 참여 및 기여

     * 버그 리포트 및 기능 제안, Discord 참여, 트위터 팔로우 등 다양한 방법으로 기여 가능함

라이선스 및 기술 참고

     * AGPL-3.0 라이선스 하에 소스 코드 공개됨
     * Chromium, browser-use, Stagehand, Nanobrowser 등 오픈소스 프로젝트에서 영감을 얻어 제작됨

        Hacker News 의견

     * 지금까지의 댓글들에서 전체적인 관점보다는 세부적인 부분에만 집중하는 느낌이라고 생각함. 개인적으로는 로컬 LLM과 연결되고, 브라우저가 보는 모든 것을 타임스탬프와 함께 로컬 DB에 저장하며, 내가 상호작용하는 내용을 자동으로 파싱/요약하고, Puppeteer처럼 스크립트화가 가능하며, 코드 프롬프트 기반 자동화를 지원하는 브라우저에 엄청난 가치가 있을 거라고 상상함. 나만의 디지털 도우미로서, 잊어버린 정보나 필요한 것을 쉽게 찾을 수 있고, 검색·광고·스팸·원치 않는 정보까지도 적극적으로 걸러주며, 심지어는 원하는 인터넷 작업을 자동으로 처리할 수도 있음. 25년간 쌓인 북마크에서 더 이상 북마크만으로는 충분하지 않음. 군더더기 웹사이트에서 원하는 정보 하나 찾다가 깊은 산으로 빠지는 상황을, 보디가드 봇이 잡음과 쓸데없는
       정보를 필터링해주는 방식으로 개선 가능. 만약 이게 정말 잘 작동한다면, 디지털 공간의 개인 비서, 투어 매니저, 도어맨, 하우스키퍼, 정비공 등 여러 역할을 한꺼번에 끌 수 있고, 브라우저가 혼돈스러운 인터넷의 주 포털이 된 2025년에는 이런 방향이 나쁜 생각이 아님. 결국 실행 능력이 관건이지만, 이런 프로젝트가 어떻게 발전하는지 너무 궁금함
          + 솔직한 피드백에 정말 고마움. 이게 우리가 그렸던 그 비전임. 하루의 90% 이상을 브라우저에서 보내는데도 여전히 ‘바보 창문’일 뿐임. 방문 기록을 기억하고, 중요한 기사 클립해서 Evernote web clipper처럼 하이라이트도 저장하고, 모든 내용을 의미 기반으로 검색 가능하면 삶이 달라질 것임. 모든 데이터가 로컬 PostgresDB에 저장되고, ""지난달 가격비교 뭐였지?"", ""브라우저 자동화 하이라이트 찾아줘"" 같은 질문을 바로 처리. 집중이 필요할 땐 방해되는 사이트를 막는 기능도 포함. 검색·기억을 넘어 브라우저가 실제로 내 일을 돕는 시대임. 예를 들어 탭을 주제별로 자동 그룹핑, 사이트별 하드드라이브 가격 비교, Discord 서버의 새 글 요약 등 모두 로컬에서 처리. 브라우저가 인터넷 혼돈 속에서 우리를 도와야지, 오히려 더 복잡하게 만들 필요 없음. 특히
            어떤 워크플로우가 일상에서 가장 불편한지, 사용자 사례 있으면 꼭 듣고 싶음
          + 사실 이건 Microsoft가 Recall로 하려던 것과 거의 동일함. Recall 기능이 AI 열풍에서 유일하게 삶을 개선해줄 것 같아 기대했으나, 곰곰이 생각해보면 내가 진짜 원하는 건 AI가 아니라, 내 컴퓨터가 로컬에서 세부 기록을 갖고 고도화된 검색을 제공하는 것임. 내가 컴퓨터로 한 모든 걸 무조건 기억해주길 바람. 방문한 사이트, 각 페이지에서 얼마나 내렸는지, 입력했다 지운 생각들까지도 모두 저장하는 ‘total recall’ 기능이 필요. 그 이유는 내 뇌가 항상 기억에 오류가 있으니, 컴퓨터엔 더 완벽한 기억을 기대함. 그리고 검색이 항상 일관되고 결정론적으로 동작하길 바람. 정확한 타임스탬프, 불리언 연산자가 가능해야 하고, NLP는 Lucene이 이미 20년 전에 잘 제공함. 나는 외부 코퍼스에서 자동 생성된 요약 말고, 내 컴퓨터에서 내가 한 일 자체만 제대로
            기억되면 됨. LLM은 개인 검색엔 큰 가치를 더하지 못한다고 봄. LLM의 특성상 실제 데이터를 정확히 되돌려주기 어렵고, 결국 전통적인 방식으로 인덱싱해야 검색이 정확함. 지금 LLM이 대세인 건 ‘모든 것’을 효율적으로 인덱싱하는 방법이 미흡할 뿐이라고 생각하고, 사실 개인화된 검색의 경우 ‘모든 것’이 아니라 내 화면에 나타난 텍스트와 메타데이터(시간, 커서 위치, 클립보드, URL 등)만 알면 충분함. LLM으로 인덱싱이 필요한 게 실제 텍스트 스냅샷을 전통 인덱스에 저장하기엔 용량이 클 때뿐인데, 그게 아니라면 불명확한 대화식 검색 정도는 내 목표가 아님. 진짜 목표는 total recall임
          + 정말 멋진 비전임. 내가 집중력을 잃었을 때 브라우저가 상기시켜주고, 내가 뭘 했는지 스스로 분석해줬으면 좋겠음. 자기 성찰이 여기에선 강력한 무기가 됨
          + 오랫동안 내가 쓰려던 기능이, 30초 이상 머무른 페이지의 전문 텍스트를 자동 저장·인덱싱해 검색하는 브라우저 확장 프로그램이었음. 이 프로젝트는 그걸 훨씬 뛰어넘는 수준임
          + 내 관점에서는 ‘LLM 기반 네이티브 광고 차단기’야… 이건 숲이 너무 커서 이 생각만으로도 머리가 아플 지경임(농담임)
     * nanobrowser처럼 직접 브라우저를 새로 만들지 말고, robust한 extension으로 충분하지 않을까 싶음. nanobrowser는 webdriver 노출 없이 잘 만들어졌고, js 실행/LLM 연동에도 부족함이 없음. 완전한 agentic 기능까지 제공하니 왜 꼭 새 브라우저가 필요한 이유가 궁금함
     * 혹시 바보 같은 질문일지 모르겠는데, ‘agentic browser’가 정확히 뭔지 설명해줄 수 있는지 궁금함. 모두가 이미 알고 있다는 전제처럼 들리는데, 본인은 이 단어가 흔한 용어인지, 아니면 단순히 ‘AI 기능이 붙은 웹브라우저’라는 뜻인지 잘 모르겠음
          + 질문해줘서 고마움. 결코 바보 같은 질문 아님. ‘agentic browser’란, AI 에이전트가 대신 웹 네비게이션을 해주는 브라우저를 의미함. 사용자가 아마존에서 주문 재정렬하거나 폼을 입력하는 것까지도 에이전트가 직접 수행하는 브라우저임
          + 에이전트란 LLM이 툴(예: calculate(expression))와 함께 동작하는 구조임. 원하는 결과물을 얻으려면 필요한 작업 시 자동으로 그 툴을 실행함. 복잡한 워크플로우의 경우, LLM이 받아들인 입력이 사용자 에이전트를 특정 문자열로 set하는 등 여러 툴의 조합일 수 있음. 예를 들어 set_user_agent(…) 같은 명령 실행이나, 페이지에서 클릭, 페이지가 열릴 때 custom JS 삽입 등이 해당됨
          + ‘agentic’이라는 용어는 한 달 전 처음 들었음. 그 이후 2~3일 동안 사내 타운홀에서까지 여러 번 반복해서 들었음. 핵심 요약은, 에이전트가 스스로 판단해서 알아서 행동하는 AI임
     * agentic browser라는 개념은 매우 멋진 아이디어처럼 들림. 클라이언트 측 에이전트로 뭔가 자동화할 수 있다는 건 정말 강력함. 하지만 동시에 보안 측면에선 ‘절대 안전하지 않을 수 있음’. 브라우저는 거의 모든 민감 계정에 로그인되어 있고, 자연스럽게 인터넷에서 신뢰할 수 없는 입력에 노출됨. 프롬프트 인젝션 한 번이면 인생이 몇 초 만에 꼬일 수도 있음. 개념은 정말 좋은데, 전체 공급망이 PCI/SOC2/ISO 27001 등의 인증을 받고, 제3자 보안분석가들의 혈서 보증까지 있는 경우가 아니라면 본인은 손도 안 댈 것임
          + 이 부분 제기해줘서 정말 고마움. 완전히 맞는 우려임. 그래서 우리는 local-first, 오픈소스를 고집함. 클라우드 에이전트(예: Manus.im)는 자격 증명을 검증할 수 없는 블랙박스에 맡겨야 하지만, 로컬 에이전트는 사용자가 제어권을 가짐. 에이전트는 직접 명시적으로 실행을 트리거할 때만 작동, 실시간 진행 상황을 직접 보고 언제든 정지 가능, 별도의 크롬 유저 프로필에서 분리 실행 가능, 가장 중요한 건 오픈소스라 코드를 직접 감시·검증할 수 있음
     * 내 사용 사례는 물과 관련된 웹사이트에서 CSV/데이터 파일을 추출하는 것임. 예를 들어 South Australia의 저장소 수위 데이터 추출은 정말 고생스러웠음(특히 프론트엔드 경험이 적은 입장에서). 이런 작업을 agent로 자동화할 수 있다면 무조건 시도해봄
     * 정말 멋진 프로젝트라고 생각함! HN에서 론칭한 것도 정말 대단함. 초반 경험에서의 솔직한 후기임: 브라우저의 ‘모든 탭을 주제로 그룹화’ 프롬프트를 실행하니 정말 잘 동작했음. 그 후, 모든 탭 그룹을 제거하고 리셋하라고 시켰더니 ""이건 브라우저 자동화 태스크니 ‘Agent Mode’에서 실행하라""는 답변. Agent Mode로 요청하니 ""이건 생산성 태스크니 Chat Mode에서 실행하라""며 다시 반환. 결국 계속 왔다갔다 소통을 하다가, 모든 탭을 하나의 새 그룹으로 묶는 것까진 됐지만, 아예 그룹 자체를 제거하는 데엔 실패. 아마 해당 API가 자체적으로 없는 듯함. 전반적으로 브라우저 레벨 액션마다 ‘undo’ 버튼이 있었으면 좋겠음. 그게 어렵다면 적어도 몇 초 전에 자신이 만든 탭 그룹을 스스로 없앨 수 있으면 좋겠음. 계속 더 써볼 계획임. edit1: chrome 내부
       페이지(예: chrome://extensions)에서 chat 인터페이스 사용 중 가끔 google.com으로 튀는 현상도 나타남. edit2: 생산성 모드엔 그룹 해체 툴이 없고, 생성만 가능하다는 걸 확인함
          + 피드백 정말 감사! 불편하게 해서 죄송함. 아직은 초기 베타 단계임. agent mode와 chat mode 각각 별도 도구로 설계. 프롬프트가 현재는 부족하니 개선 필요성을 느낌. 그룹 해체 관련은 크롬 API 자체가 아직 없어 직접 구현해야 할 것 같음. 여러 사람들이 ‘undo’ 기능 요청 중이니, cursor의 'restore checkpoint' 스타일로 도입 고민. 생산성 기능이 실제 일상에 얼마나 중요하다고 생각하는지, 구체적 사례 있으면 꼭 듣고 싶음 :)
     * ‘nxtscape’라는 이름이 예전 SCSI의 향수를 줌. ‘GPT’처럼 간단하게 한 번에 말할 수 있는 네이밍 추천. 제품은 정말 훌륭함
          + 오늘 피드백을 보면 이름과 브랜딩에 더 시간 써야겠다고 느낌. 고민해보겠음 :)
     * 이 시장은 완전히 winner-take-all임. 시도한 점은 정말 대단하지만, 두 세 명 정도 팀으로 브라우저 만들긴 너무 큰 일임. 게다가 구글이 이미 I/O에서 미래 방향을 살짝 보여줬고, 이런 분야는 구글이 크롬에 곧 구현할 수 있어 충분히 빠르게 시장 점유율을 가져올 것 같음. 딥테크 창업자들이 수년간 크롬 정복을 시도했으나 단 한 번도 성공한 적 없음. 현실적으로 ICP가 확실한 작은 니즈부터 시작하는 게 맞다고 생각함. 열정과 에너지 낭비가 너무 아까움
          + 솔직한 피드백 고마움! 경쟁이 치열한 건 사실임. 하지만 오픈소스, 커뮤니티 기반, privacy-first AI browser(Brave 같은) 시장에 확실히 공백이 있다고 생각함
     * 사용자를 위한 기능, 특히 유저에게 적대적인 웹 콘텐츠를 자동 처리·가공해주는 브라우저엔 분명 큰 가치가 있다고 생각함. 구체적인 활용 사례로는, 1) 소파 구입시 마케팅 과장 정보 대신 특정 조건만 필터링하거나, 2) 친구들이 Facebook에 글 올릴 때만 알림받고 나머지는 걸러주거나, 3) 동네 커뮤니티가 Facebook이나 nextdoor로 운영되는데, 자꾸 같은 사람의 반복 글만 안 보이게 해주거나, 4) 정부 공청회 페이지가 쓸데없이 700페이지로 부풀려진 걸 자동 요약해 진짜 중요한 내용만 골라내주는 등 다양한 도메인에 적용 가능
          + 소파 사기처럼, 특정 조건에 따라 항목을 필터링하는 기능은 대형 사이트(Amazon 포함)도 기본적으로 부족함. 예전엔 scraping과 데이터 사이언스를 조합해 diskprices.com 같은 사이트를 직접 만들었고, LLM이 이런 용도에 정말 유용하다면 앞으로도 직접 브라우저에서 프롬프트를 짜는 대신 이렇게 기능 특화된 사이트가 늘어날 것임. 반복 글 차단의 경우 nextdoor는 유저 차단 기능이 있으니 참고할 만함(nextdoor 차단 가이드)
          + 정말 너무 좋은 예시임!
     * robots.txt를 준수하나요?
          + 아직은 준수하지 않음. 근데 사실 agent가 ‘인간’을 위한 용도로만 사용하는 경우 scraping이 아니다 보니 중요한지 잘 모르겠음
          + 이건 확실히 사용자용 agent임. robots.txt까지 굳이 지킨다면 솔직히 너무 답답할 것 같음. robots.txt는 본래 웹 크롤러의 예의로 설계된 거지, 개별 사용자의 요청을 대행하는 도구까지 막으라고 있는 게 아님. 터미널, 일반 브라우저, AI 브라우저 등 어떤 방식으로든 내 사이트를 접근하는 도구를 사이트 운영자가 제한하는 건 웹의 강점을 떨어뜨림. AI 도구 혐오 때문에 웹의 다양성을 희생하는 건 미래적으로도 위험함. 참고: robots.txt FAQ
"
"https://news.hada.io/topic?id=21592","uBlock Origin Lite Beta for Safari iOS","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 uBlock Origin Lite Beta for Safari iOS

     * uBlock Origin Lite Beta는 Safari iOS에서 사용할 수 있는 광고 차단 확장 프로그램의 베타 버전
     * TestFlight 앱을 통해 다양한 Apple 기기에서 베타 빌드 설치 및 테스트를 지원함
     * 설치와 테스트 과정에서 초대 링크 혹은 이메일 안내를 받아 최대 30개 기기까지 설치 가능함
     * 자동 업데이트 기능과 여러 기기의 빌드 관리, 이전 버전 테스트 등 유연한 테스트 환경 제공함
     * 베타 기간이 종료된 후에는 App Store 버전을 다운받아야 하며, 베타에서 구매한 앱 내 항목은 이관되지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

uBlock Origin Lite Beta for Safari iOS 베타 테스트 참여 안내

  베타 앱 설치 및 테스트 개요

     * 각 베타 빌드는 개발자가 업로드한 날로부터 최대 90일 동안 테스트 가능함
     * TestFlight 앱에서 남은 테스트 기간과 함께 새로운 빌드 알림 및 테스트 지침을 제공함
     * TestFlight 3 이상 버전 사용 시 자동 업데이트가 가능하여 최신 빌드가 자동 설치됨
     * 테스트 기간 종료 후에는 베타 앱을 더 이상 열 수 없으며, App Store 버전을 별도로 설치해야 함
     * 베타 기간 동안의 앱 내 구입 항목은 App Store 정식버전으로 이전되지 않음

  베타 앱 설치 전 참고사항

     * iOS 16 이상, iPadOS 16 이상, macOS 13 이상, tvOS 18.4 이상, visionOS 2.4 이상에서 앱 내 추가 콘텐츠 자동 다운로드 기능을 설정할 수 있음
     * Apple 공식 문서의 iPhone, iPad, Mac 사용법 링크에서 상세 안내 확인 가능함

  설치 방법

    TestFlight를 사용한 베타 앱 설치 기본 절차

     * 테스트할 기기에 TestFlight 설치
     * 초대 이메일을 수신하여 ‘TestFlight에서 보기’ 탭 또는 공개 초대 링크 활용하여 앱 설치
     * 새 테스터의 경우 ‘수락’ → ‘설치’ 순서로 진행
     * 기존 테스터는 ‘업데이트’ 또는 ‘열기’ 선택으로 신규 빌드 사용 가능
     * 한 명의 테스터는 최대 30개 기기에 베타 앱 설치 가능

    조건 및 기준

     * 개발자가 요구하는 추가 기기/OS 기준 미충족 시 그 내용이 명시됨
     * 권장되지 않는 기기에서도 TestFlight를 이용해 초대 수락 가능
     * 초대 거부 시, 그 이유를 개발자에게 피드백 제공 가능

    구독 관련 참고사항

     * TestFlight 베타 테스트 중 구독 갱신 속도가 빨라짐
     * 각 구독은 1주일간 최대 6회, 하루에 여러 번 갱신될 수 있음

    관리형 Apple 계정 제한

     * 관리형 Apple 계정은 TestFlight 빌드 테스트에 사용할 수 없음

  플랫폼별 베타 앱 설치 절차

    iOS 또는 iPadOS 기기

    1. TestFlight 설치
    2. 초대 이메일의 ‘TestFlight에서 보기’ 탭 또는 공개 링크 클릭
    3. ‘수락’과 ‘설치’를 차례로 진행

    macOS

    1. Mac에 TestFlight 설치
    2. 초대 이메일의 ‘TestFlight에서 보기’ 클릭 또는 공개 링크 사용
    3. ‘수락’ 후 ‘설치’ 클릭

    tvOS (Apple TV)

     * 이메일 초대: TestFlight 설치 후, 웹의 교환 코드 입력
     * 공개 초대 링크: iOS/iPadOS 및 Apple TV에 TestFlight 설치 후, 공개 링크 수락→ Apple TV에서 설치

    visionOS 및 watchOS

     * visionOS: 이메일 또는 공개 링크를 통한 설치
     * watchOS: Apple Watch와 페어링된 iOS 기기를 통한 베타 앱 수락 및 설치

  특수 앱 유형 테스트

    iMessage 앱

     * TestFlight를 통한 설치 및 수락 후 iMessage 앱 또는 스티커팩을 메시지 앱에서 실행하여 테스트

    앱 클립(App Clips)

     * 베타 앱 및 앱 클립은 동시에 두 개 설치 불가
     * 앱 설치 시 클립을 대체하며 일부 데이터 손실 가능

  자동 업데이트 관리

    TestFlight 전역 자동 업데이트

     * TestFlight 첫 실행 시 자동 업데이트 설정 알림
     * iOS/iPadOS: TestFlight → 설정 → ‘새로운 앱 자동 업데이트’ 관리
     * macOS: TestFlight → 환경설정 → 일반
     * tvOS: TestFlight → 설정 → 자동 업데이트 선택
     * visionOS: TestFlight → 프로필 → ‘새로운 앱에 대한 자동 업데이트’ 관리

    개별 앱 자동 업데이트

     * iOS/iPadOS/macOS/tvOS/visionOS에서 TestFlight의 앱 세부정보 페이지에서 개별적으로 자동 업데이트 켜고 끄기 가능

  이전 빌드 및 빌드 그룹 사용

     * TestFlight 앱에서 현재 빌드 외에 기존 버전 빌드도 선택하여 설치 가능
     * 베타 빌드를 설치하면 정식 App Store 앱이 대체되며, 앱 이름 옆에 베타 주황색 점 표기
     * 공개 링크로 가입 시, 개발자에게 개인 정보는 전달되지 않음
     * 빌드 테스트 기록 등 일부 정보(설치 일시, 충돌 수 등)는 개발자에게 전달

    빌드 설치 관련 추가 안내

     * iOS/iPadOS/macOS/tvOS/visionOS에서 ‘이전 빌드 보기’ 또는 ‘버전 및 빌드 그룹’에서 원하는 빌드 선택 후 설치 가능
     * 선택한 빌드가 기존에 설치된 빌드를 대체함

마무리

     * uBlock Origin Lite Beta for Safari iOS의 TestFlight 베타 프로그램은 다양한 Apple 환경에서 가장 신속하게 새로운 기능과 변경사항을 테스트할 수 있는 기회를 제공함
     * TestFlight의 유연한 자동 업데이트 설정 및 여러 기기 지원 덕분에 개발자와 테스터 모두에게 체계적인 베타 테스트 환경을 제공함
     * 베타에서 경험한 피드백은 정식 버전에 반영될 중요한 자료로 활용됨

        Hacker News 의견

     * 이 주제에 대해 꼭 추천하고 싶은 앱이 Hush라는 생각이 드는 중이다. iOS에서 사용할 수 있는 훌륭하고 무료이며 오픈소스, 그리고 관리가 잘 되고 있는 앱이라고 강조하고 싶다. Hush 공식 홈페이지 링크 참고 가능
     * Firefox가 2019년부터 iOS에서 서드파티 콘텐츠 필터 지원을 실패하거나 거부한 점을 아쉽게 생각하는 중이다. Gecko 엔진을 들여올 수 없다고 해도 Mozilla Firefox 브랜드 인지도를 높일 노력을 계속해야 한다고 보는데, 리소스 투입 자체를 거부하고 있는 모습이라고 생각한다. 관련 논의 링크 확인 가능
          + 아이러니하게도, Orion for iOS에서는 Firefox uBO 확장을 설치해서 쓸 수 있다. Orion/Kagi 팀이 Mozilla/Firefox보다 먼저 이 부분을 해결했다고 언급하고 싶다
          + Apple이 사실 그렇게까지 중요한 존재는 아니라는 의견도 내보고 싶다
     * 나는 오랫동안 Wipr를 사용했고, 이제는 Wipr 2를 쓰는 중이다. 귀찮은 광고 차단에 딱이고, 크로스플랫폼 지원이라 소액 결제할 가치가 충분하다는 판단이다
          + Wipr 2 성능이 꽤 좋지만, 내 iPhone 12 속도가 많이 느려지는 점은 아쉬움으로 남는다. 필터 개수가 많을수록 램 부족 현상 영향이 커진다는 느낌이다. 그래도 AdGuard보다는 확실히 빠르다고 본다
          + Wipr 2가 설치하고 잊어도 될 만큼 괜찮은 콘텐츠 차단기라고 평가하고 싶고, 개발자 대응도 빨라서 신뢰도가 높다는 생각이다
          + 중요한 버전마다 한 번씩만 라이선스 비용이 들어가는 소규모 비용 구조라서 매력적으로 느껴진다
     * 1Blocker가 거의 10년 가까이 나에게 잘 맞는 콘텐츠 차단기 역할을 해준 중이다. 1Blocker 앱 링크 참고
          + 해당 서비스가 유료라는 점은 조금 아쉬움으로 다가온다
     * 프로젝트 페이지는 이 GitHub 링크가 제대로 된 곳이라고 본다. uBlock Origin Lite 공식 GitHub 참고
          + 자체 개발자(gorhill)가 만든 프로젝트라서 신뢰도가 상당히 높다고 평가한다
     * 이게 AdGuard보다 더 좋은지 궁금한 상황이다. 두 개를 동시에 돌리면 안 좋다는 말은 들었지만, uBlock 버전이 과연 쓸만한지 고민이 든다
          + gorhill이 계속 믿음을 보여주고 있다는 사실이 매우 크다고 본다. 이 사람이 돈이 아니라 소신으로 uBO를 만든다는 점이 광고 차단기에서 가장 중요한 부분이라 생각하는 중이다. AdGuard 자체도 나쁘다는 의미는 절대 아니라는 입장이다
          + AdGuard에도 불만이 없지만, uBlock이 플랫폼 전반에서 가장 뛰어난 광고 차단기이기 때문에 곧 이걸로 바꿔볼 예정인 마음이다. gorhill은 그야말로 전설이라고 생각한다
          + AdGuard가 현재 내 사용 용도에서는 조금 더 잘 동작한다. 왜 그런지 정확히는 모르겠는데, uBO Lite는 가끔 제대로 작동이 멈추기도 한다. 실험적 필터를 활성화한 뒤 테스트 페이지(테스트 링크)를 여러 번 확인해야 했지만, 적용이 잘 안 되는 경험을 반복적으로 했다는 점이 있다
          + uBlock이 오픈소스라는 사실이 차별점이라고 보고 싶다. AdGuard가 러시아 법인이라는 게 거북한 이용자도 있겠지만, AdGuard는 정치적 문제에서 거리를 두고 있고, 우크라이나 직원도 채용하고 있다는 점은 알고 있다
          + 내 생각엔 uBlock이 AdGuard보다 만족스럽지 않다. 광고 차단 규칙을 커스텀할 수 없는데, AdGuard는 이 옵션 지원이 있기 때문이다
     * 호환성 안내 내용이 인상적이다. iOS 18.0 이상, visionOS 1.0 이상, iPhone, iPad, Apple Vision, 그리고 macOS 15.0 이상 및 Apple Silicon 칩이 탑재된 Mac에서만 사용 가능이라는 조건을 확인해두는 게 좋겠다는 생각이 든다
     * 속도는 빠른 편이지만 나의 경우 모든 광고를 완전히 차단하지는 못한다는 경험을 하고 있다
     * 2025년 5월 13일에 이와 관련된 논의가 있었다는 점을 알려두고 싶다. 이전 논의 소개 참고
     * Orion for iOS에서는 uBlock Origin을 쓸 수 있다는 점이 좋아서 이용 중이다. 다만, add-on을 조작하는 게 직관적이지 않아 불편함이 있고, 그 결과 데스크톱에서만큼 강하게 차단 설정을 못 한다. 이외에 좋은 선택지가 더 나오면 기대가 크다는 마음이다
"
"https://news.hada.io/topic?id=21570","Infinite Mac OS X","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Infinite Mac OS X

     * Infinite Mac 플랫폼에서 이제 Mac OS X 시리즈 초기 버전(10.1, 10.3)을 웹 브라우저에서 실행 가능함
     * PowerePC 에뮬레이터 PearPC를 Emscripten/WebAssembly로 포팅하여 호환성과 안정성 향상 성과를 달성함
     * DingusPPC와의 비교를 통해 성능과 원인 모를 버그를 개선하며, 알고리듬적 최적화 과정을 거침
     * Mac OS X 에 적합한 Infinite HD 디스크 이미지도 새롭게 구축해, 다양한 2000년대 소프트웨어와 개발 도구 접근 가능함
     * UI의 Aqua 스타일 적용 등 레트로 사용 경험 강화와 함께, 미래 추가 발전 가능성도 염두에 둠
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

요약

   Infinite Mac 플랫폼에서 오랜 레거시 시스템을 온라인으로 체험할 수 있는 프로젝트 Infinite Mac이 Mac OS X 10.1과 10.3 등 초기 버전을 지원함으로써 현대 웹 브라우저에서 실행 가능해짐. 이 과정에서 PearPC와 DingusPPC 등 PowerePC 에뮬레이터를 Emscripten/WebAssembly 기반으로 포팅하고, 성능 개선 및 안정성 확보를 위한 여러 노력을 기울임. 알고리듬 최적화와 버그 해결을 통해 오리지널 하드웨어 대비 해상도와 신뢰성을 높이며, 당시의 소프트웨어 및 개발 도구가 담긴 Infinite HD도 재구성해 실용성을 강화함. 또, 사이트 UI에 Aqua 테마를 적용해 감성적 경험을 제공하고, A/UX, Lisa처럼 더 레트로한 OS 지원까지 향후 확장을 모색함. 본 프로젝트는 프로그램 에뮬레이션, 오픈소스 아카이브 확장, UI 복원 등 복합 목적을 달성하는 기술적 시도를 보여줌.

Mac OS X 지원의 확대

     * Infinite Mac에서 Mac OS X 10.1과 10.3 등 초기 OS X 버전을 웹 브라우저에서 실행 가능하게 포팅함
     * 실제 하드웨어에서의 느린 성능을 그대로 재현하며, 당시의 NetNewsWire Lite 및 Terminal 등 대표 애플리케이션 사용도 체험 가능함
     * 최신 디스크 이미지에 당시의 인디 소프트웨어도 일부 포함하여, 레트로 소프트웨어 탐색이 쉬워짐

PearPC 포팅 과정

     * 처음에는 DingusPPC 에뮬레이터 포팅 및 수정에 집중했으나, 커널 패닉과 그래픽 오류 문제로 PearPC로 노선을 전환함
     * PearPC는 2000년대 초반 x86 기반에서 Mac OS X를 실행하도록 설계된 멀티플랫폼 코드베이스로, 추가 작업 없이 WebAssembly 대상 포팅이 비교적 용이함
     * 최근 Basilisk II, SheepShaver의 주요 메인테이너가 PearPC 실험 브랜치를 만들어 현대 macOS에서 빌드가 가능해짐
     * 소스가 오래되고 레거시 C++ 기반이나, 맞춤 string class, sprintf, GIF 디코더 등 특이성이 존재함
     * 멀티플랫폼 구조, 스레드를 거의 사용하지 않는 설계, 구조적 추상화 계층 덕분에 이식이 신속하게 이뤄짐

성능 최적화와 한계

     * PearPC에서 Mac OS X 10.2 부팅 및 실행의 신뢰성은 높았으나, DingusPPC에 비해 실행 속도가 느림
     * MMU 캐싱 부재 등으로 인한 성능 저하가 큼
     * DingusPPC에서 적용한 다양한 알고리듬 기반 최적화를 PearPC에도 이관하며 부팅 속도 일부 단축 효과를 거둠
     * PearPC는 필요한 PowerPC 명령어 서브셋만 구현하고, 커스텀 펌웨어·드라이버 등 여러 ‘치트’를 활용함
     * 아직도 빠르고 안정적인 Mac OS X 에뮬레이션에는 장기적으로 DingusPPC가 더 유망하다는 희망을 갖고 있음

부수적 발견: FP 예외 처리

     * PearPC 내 인터프리터에서 통합 디코딩 방식 도입 시, 부동소수점 연산에서 렌더링 오류가 발생함
     * Mac OS X 커널이 MSI 레지스터의 FP 비트 상태에 따라 예외 처리를 등록해두고 있어, 이를 코드에 구현하자 렌더링 문제 해결됨
     * DingusPPC에서도 동일한 FP 비트 누락 문제로 텍스트 렌더링 이상, 이를 수정한 후 안정성 크게 향상됨
     * 편법적 핸들링에서 ‘정석적’ 구현으로 발전, 결국 10.1 실행 신뢰성도 크게 향상됨
     * 두 에뮬레이터를 조합해 다양한 OS X 버전 실행 지원 폭을 넓힘

Infinite HD 디스크 이미지 재구축

     * Mac OS X 시대에 맞는 디스크 이미지를 새롭게 빌드해, 해당 시기 인디 소프트웨어 및 개발자 도구를 다수 포함함
     * 구버전 .dmg 파일은 최신 macOS에서 바로 마운트되지 않으므로, dmg2img 등 도구를 활용한 변환 및 자동화 파이프라인 구축
     * HFS와 HFS+ 파일시스템의 차이로 일부 제한은 있으나 대다수 소프트웨어는 정상 동작함
     * 라이브러리 사이트 및 Wayback Machine 등 다양한 출처를 통해 2000년대 초 소프트웨어 구동 환경 완성
     * 멀티파티션 이미지 동적 생성 기법을 도입해, 부팅 디스크·Infinite HD·저장용 Saved HD까지 최대 3개 볼륨 마운트 가능함

UI: Aqua 스타일 적용

     * Aqua 테마를 Infinite Mac UI에도 채택해 레트로 감성을 극대화함
     * Mac OS X 10.0/10.1 스타일 이미지 자산을 직접 추출해 반투명 효과까지 구현함
     * 사이트 조작부의 스타일도 OS별 외양(클래식, Platinum, NeXT 등)에 맞게 자동 적용됨
     * 향후 애플 macOS 대규모 UI 변화 예고에 맞춰, 과거 UI의 ‘레퍼런스 포인트’ 역할도 제공함

추가 기능 및 마일스톤

     * Mac OS 9 파티션 마운트 및 Classic 환경 실행 기능 추가(10.1에서만 지원)
     * 과거 Calculator 앱 반복 연산 동작 등 OS간 UX 차이 등 고증 가능
     * PearPC에 내장된 부트 메시지(Verbose) 모드 활성화 및 DingusPPC에선 오픈 펌웨어 변수 지정 방식 구현
     * macosx.app 도메인을 통한 브랜딩 확대도 시도 중이나, 현재는 타인이 보유한 상태임

향후 확장 및 마무리

     * Mac OS X 지원 확장으로 Infinite Mac은 현대 macOS와의 연결고리를 마련함
     * 더 깊은 복고를 위해 A/UX, Lisa, Pippin, Newton 등의 이식 가능성도 언급함
     * WebAssembly 기반 QEMU(wasm)도 실험적으로 관심, 성능 테스트에서 긍정적 신호 확인
     * Mac OS X 웹 기반 에뮬레이션은 체험, 레거시 소프트웨어 보존, 개발 실험 등 다양한 IT/스타트업 시나리오에서 가치를 제공함

        Hacker News 의견

     * PearPC가 인텔 전환 전까지 몇 년간 꽤 성공적으로 작동했다는 사실을 기억한다는 의견 공유, 하지만 핵심 개발자가 기차 사고로 세상을 떠난 뒤 대부분의 동력을 잃어버린 아쉬움 표현, 그 당시 열정적인 사용자이자 커뮤니티 멤버로 활동한 경험과 함께 지금도 그 일을 떠올릴 때마다 슬픈 감정, 관련 기사 링크 첨부 링크
          + 많은 뛰어난 개발자들이 갑자기 세상을 떠난 사례들을 떠올리며, 부고도 없고 소식을 아는 사람은 개발자 한 명뿐인 상황에서, 혹시 그들이 너무 뛰어나서 외계인들이 지구에서 데려간 것은 아닐까 상상하게 되는 마음
          + 누군가 PearPC 개발자의 작업물을 훔쳐서 cherryos라는 이름으로 판매했다는 사실 지적
          + 해당 기사에 접근하려 하니 유료 구독 벽이 있다는 아쉬움 표시
     * Infinite Mac은 웹 브라우저에서 고전적인 Macintosh와 NeXT 시스템 릴리즈 및 소프트웨어를 쉽게 사용할 수 있는 컬렉션이라는 소개, 관련 링크 infinitemac.org
          + Infinite Mac이 무엇인지 블로그 글에서 하이퍼링크로 연결해 주거나 정의해줬으면 좋겠다는 의견
          + Infinite Mac 페이지를 알게 돼서 기쁘며, 블로그 작성자가 해당 사이트 링크를 빼먹은 점은 큰 실수라는 생각, 이 댓글이 없었으면 이 놀라운 사이트를 못 찾았을 거라는 고마운 마음
     * 기사 중에서도 특히 흥미로웠던 부분은 700줄이 안 되는 소스코드로 구현된 PPC CPU 에뮬레이터라는 사실, 6502 에뮬레이터에서나 볼 수 있는 간결함을 상대적으로 최신 아키텍처에서 본 점에 놀라움, 관련 코드 링크 TinyPPC.cpp
          + RISC 구조라서 그렇게 놀랍지 않다는 의견, MIPS 에뮬레이터도 비슷한 크기라는 정보 공유
     * 오늘날 20살의 테크 매니아가 OS X 10.4(.5, .6 포함)를 접하게 된다면 무슨 느낌일지 상상하는 의견, 암흑기에 고전 조각상을 보며 ""어떻게 인류가 이런 작품을 만들 수 있었던 걸까?""라는 감상을 가져봄, 취향은 시대에 따라 변하고 지금 세대는 포토리얼리틱 아이콘을 오히려 촌스럽게 느낄 수도 있겠다는 생각
          + 10.4의 외형과 동작 방식이 최신 macOS와 기본적으로 거의 비슷하다는 의견, 설치 방식(dmg 드래그), Finder 북마크, Dock의 동작, Spotlight의 도입까지 모두 이미 그 당시 구현됐다는 정보, Windows XP와 Windows 11의 변화폭에 비하면 Mac의 변화는 크지 않다는 견해
          + 20살 사용자로서 어릴 때 10.5나 10.6을 대략 써봤던 기억이 있으며, 개인적으로는 그 시절에 대한 노스탤지어에 가까운 감상
          + 10.4/10.5 시절을 개발자로 시작했다는 경험 공유, 반쯤 다크모드, 테마 설정 등을 만지작거렸으며 그때는 시스템 폴더를 건드리는 게 지금보다 훨씬 간단했다는 소감
          + “암흑기”라는 표현에 대해 역사적 배경에서 기술, 능력의 상실이 아니라 제국과 독립 왕국의 문화·예산적 차이였다고 설명, 중세 초기에도 수려한 유물과 건축물이 많았다는 역사 지식 첨언, Sutton Hoo 배 무덤이나 앵글로색슨 교회 등을 직접 본 경험 언급
     * 스크린샷들을 보고서 Mac OS X가 지금의 Mac OS보다 훨씬 아름답고 정돈된 환경이라고 느끼며, “이 환경이라면 진짜 일할 맛 나겠다”라는 인상, 현행 Mac OS는 너무 어지럽고 혼란스러운 느낌, 혹시 다른 분들도 이렇게 느끼는지 궁금한 마음
          + 개인적으로는 UI에서 마주치는 부분이 메뉴바, 세 개의 점, 그리고 Spotlight 정도로 한정되어 있고, 실제로는 운영체제 자체나 네이티브 앱보다는 브라우저 기반이나 서드파티 디자인 시스템을 사용하는 크로스플랫폼 앱만 주로 사용, 윈도우에서도 마찬가지며 시각적인 부분에 별 감흥이 없다는 의견
          + 요즘은 Win 95나 macOS 9 같은 올드 그레이 인터페이스를 오히려 좋아하게 되었는데, 계산기처럼 그저 도구 같은 안정감과 편안함을 준다는 점이 좋다는 취향
          + 기술적으로 현대 macOS에서도 예전 스타일을 충분히 구현할 수 있음에도 그렇지 않은 이유는 소프트웨어 구조가 모듈형으로 짜이지 않은 문제, 소프트웨어 회사들이 소프트웨어를 잘 만든다고 이제는 믿기 어렵다는 실망감 표출
          + 개인적으로는 10.3 Panther의 혼합된 aqua와 브러시드 메탈이 가장 마음에 들며, 10.4 Tiger의 글로시한 메뉴바는 시간이 지나면서 세련되지는 않은 느낌, 10.5 Leopard는 화려한 3D 도크와 투명 상단 메뉴, 더욱 현대적인 그라디언트가 있었지만 결국 aqua와 금속 질감이 더 멋졌다는 견해, 그 이후 버전은 다소 심심하다고 여김
          + 윈도우의 줄무늬 디자인이 다소 과했는데, 그것이 사라져서 오히려 더 좋아졌다는 개인적 만족
     * 예전에 좋았던 UI들을 떠올리게 되었다는 감상, Platinum 시절의 Macintosh OS 8, MacOS X lickables의 세련미, 그리고 OpenLook, NeXTStep/OpenStep, Windows 2000 시절까지 다양하게 추억, 당시 UI 요소들은 명확하고 일관적이었기에 컨트롤 동작 예측도었고 일관된 경험 제공, 오늘날은 테마 커스터마이징이 자유로워졌지만 그 대가로 직접적이고 일관적인 UI 경험을 어느 정도 잃어버렸다고 생각
     * PearPC 프로젝트가 마지막 커밋 이후 10년이 훌쩍 지났음에도 성공적인 사례로 언급된 점이 놀랍다는 반응, 공식 저장소 github, 개인 포크 github, 웹어셈블리 지원을 추가한 상태에서도 x86-64 지원이 여전히 남아 있다는 점, NextStep을 Infinite Mac에 추가한 경험을 블로그로 남겼다는 소식 포함 블로그 링크
     * 이런 프로젝트가 정말 좋다는 개인 감상, Aqua가 등장했을 때는 정말 혁신이었다는 찬사
          + 당시 거의 모든 Linux 윈도우 매니저가 aqua 테마를 제공했으며, Apple은 “리눅스 데스크탑의 해”를 기다리는 유저들이 꿈꾸던 OS를 실제로 선보였다는 견해, 주류 Unix임에도 뛰어난 사용성과 광범위한 써드파티 앱 지원까지 갖춰 큰 성공 거둠
          + Aqua는 여전히 혁신적인 디자인이라는 생각, 과거에는 윈도우 컨트롤을 바로 알아볼 수 있었지만, 최근에는 오히려 더 구분하기 어려워진 점을 아쉬워함, Liquid Glass가 그런 UX를 어느 정도 되살려주길 바라며, Aqua의 일관성과 사용성이 윈도우 Vista나 리눅스 테마 등이 흉내내려 했지만 결국 따라가지 못한 이유를 꼽음
          + 스티브 잡스의 “Aqua 발표” 당시의 프레젠테이션 멘트를 인용, Aqua는 완전히 새로운 인터페이스이고, 애플 고유의 전통을 계승하며 그 이름처럼 유동적이고, 디자인 목표 중 하나는 ‘보고 있으면 핥아보고 싶게 하는 것’이라는 철학 강조
          + Liquid Glass가 당시 Aqua 시대의 디자인 철학을 다시 떠올리게 한다는 감상
     * 초창기 OS X 시절이야말로 Mac의 진정한 황금기였다고 회상, 당시 하드웨어 경쟁력도 뛰어났고 운영체제도 최고의 완성도였다는 생각, 요즘은 다시 하드웨어 황금기가 도래했으니 언젠가 소프트웨어도 따라가길 바라는 기대, 하지만 최신 Mac에서 Safari를 사용할 때 이 에뮬레이션 성능은 응답성이 낮아 실제 사용은 어렵다는 의견
          + Mac OS가 다시 훌륭해질 가능성은 없어 보인다는 의견
          + 하드웨어가 성능을 주면 소프트웨어가 그 성능을 빼앗아간다는 아이러니 강조
          + 초창기 x86 모델을 사용했던 경험으로, 가격 경쟁력이 있었던 것은 아니고, 커널 패닉이 자주 발생하며 QuickTime에서 영상 보는 중에도 segmentation fault가 일상이었다는 지적, 다양한 파일 포맷(wmv나 divx) 재생을 위해 코덱 찾는 게 일과였고, 예뻐 보이기 위해 통풍구도 없던 탓에 장비 과열 문제도 빈번했다는 경험 공유
     * “속도가 빠르진 않지만, 그 당시 진짜 하드웨어도 크게 낫진 않았다”는 평가에 동감, 2008년 전후 해킨토시 사용 경험 기반으로 실제로도 당시 맥이 느렸던 점을 사실적으로 재현한다는 점에서 리얼함을 느꼈다는 소감, 그 시절 아이팟 터치용 앱을 만들기 위해 수많은 시행착오를 겪었던 추억 공유, 그래도 설치에 24시간까지 걸리진 않아 다행이었다는 이야기
"
"https://news.hada.io/topic?id=21694","Rust 컴파일러가 왜 이렇게 느릴까?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Rust 컴파일러가 왜 이렇게 느릴까?

     * 러스트로 만든 웹사이트를 Docker로 반복 빌드할 때 빌드 시간 문제를 겪음
     * 기본 Docker 설정에서는 전체 의존성 재빌드가 매번 발생해 4분 이상 소요됨
     * cargo-chef와 캐싱 도구를 사용해도 최종 바이너리 빌드에 여전히 많은 시간이 걸림
     * 프로파일링 결과, LTO(링크 타임 최적화)와 LLVM 모듈 최적화에 대부분 시간을 소모함
     * 최적화 옵션, 디버그 정보, LTO 설정을 조절해 일부 개선 가능하지만, 최종 바이너리 컴파일에 최소 50초는 소요되는 현상 확인
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제 제기 및 배경

     * Rust로 만든 개인 웹사이트를 수정할 때마다 정적 링크 바이너리를 빌드해 서버에 복사 후 재시작하는 번거로운 작업이 반복됨
     * Docker 또는 Kubernetes 등 컨테이너 기반 배포로 전환하려 했으나, Rust의 Docker 빌드 속도가 큰 문제로 드러남
     * Docker 내에서 작은 코드 변경에도 전체를 처음부터 재빌드해야 해서 비효율적인 상황 발생

Docker에서 Rust 빌드 – 기본 접근

     * 일반적인 Dockerfile 접근은 모든 의존성과 소스코드를 복사한 뒤 cargo build 실행 방식임
     * 이 경우 캐싱의 이점이 없어 전체 재빌드가 반복됨
     * 본인의 웹사이트 기준, 전체 빌드는 약 4분 소요됨—의존성 다운로드에도 추가 시간 소요

Docker 빌드 캐싱 개선 – cargo-chef

     * cargo-chef 도구를 활용하면, 의존성만 별도의 레이어로 미리 캐시해둘 수 있음
     * 이를 통해 코드 변경 시 의존성 빌드가 재사용되어 빌드 속도 개선 효과 기대
     * 실제 적용 시, 전체 시간 중 25%만이 의존성 빌드에 집중되고, 최종 웹서비스 바이너리 빌드에 여전히 상당한 시간이 소요됨(2분 50초~3분)
     * 주요 의존성(axum, reqwest, tokio-postgres 등)과 7,000여 줄의 자체 코드로 구성됨에도 rustc의 단일 실행에 3분 소요되는 구조

rustc 빌드 시간 분석: cargo --timings

     * cargo --timings를 사용하여 각 크레잇(compilation unit)별 빌드 시간 확인 가능
     * 결과적으로 최종 바이너리 빌드가 전체 시간의 대부분을 차지하는 것을 확인
     * 보다 세밀한 원인 분석에는 도움이 되나, 구체적 컴파일러 내부 동작 파악이 부족함

rustc 자체 프로파일링(-Zself-profile) 활용

     * rustc의 자체 프로파일링 기능을 -Zself-profile 플래그로 활성화하여 세부 동작 시간 측정
     * 이를 위해 환경변수를 통해 프로파일링 활성화
     * 결과 요약(summarize) 툴로 분석 시, LLVM LTO(링크 타임 최적화), LLVM 모듈 코드 제너레이션이 전체 시간의 60% 이상 차지함을 발견
     * flamegraph 시각화로도 codegen_module_perform_lto 단계에서 전체 시간의 80% 소모를 확인

LTO(링크 타임 최적화)와 빌드 최적화 옵션

     * Rust 빌드는 기본적으로 codegen unit별로 분할된 후, LTO에 의해 전체 최적화가 비교적 후반에 적용됨
     * LTO에는 off, thin, fat 등 여러 옵션이 존재: 각각 성능 및 최종 결과물에 영향
     * 작성자의 프로젝트는 Cargo.toml에서 LTO를 thin으로, 디버그 심볼도 full로 설정된 상태였음
     * 다양한 LTO/디버그 심볼 조합을 테스트한 결과:
          + full 디버그 심볼의 빌드 시간 증가 효과와 fat LTO의 4배 가량 빌드 지연 확인
          + LTO 및 디버그 심볼 제거 시에도 최소 50초 빌드 시간 필요

추가 최적화 및 단상

     * 50초 정도면 실제 서비스 부하가 거의 없는 본인 사이트에는 큰 문제 없으나, 기술적 호기심으로 추가 분석 시도
     * 증분 컴파일(incremental compilation)을 Docker로 잘 활용하면 더 빠른 빌드도 가능하나, 빌드 환경 청결성과 도커 캐시의 결합 필요

LLVM 단계 세부 프로파일링

     * LTO와 디버그 심볼 제거 후에도 LLVM_module_optimize 단계에서 70% 가까운 시간 소모
     * release 프로필에서 opt-level 기본값(3)으로 인한 최적화 비용이 큼을 인지, 바이너리에서만 opt-level을 낮추는 방법 테스트
     * 각종 최적화 조합 실험 결과 최적화 미적용(opt-level=0) 시 15초 내외, 최적화 적용(1~3) 시 50초 내외로 소요됨

LLVM 추적 이벤트 심층 분석

     * rustc의 추가 플래그(-Z time-llvm-passes, -Z llvm-time-trace)를 이용해 LLVM 단계별 실행 시간 상세 추적 가능
     * -Z time-llvm-passes는 출력이 방대해 Docker의 log 제한을 초과하는 경우가 많아 log 설정 조정 필요
     * 로그를 파일로 저장해 분석하면 각 LLVM 최적화 패스별 실행 시간을 개별적으로 확인 가능
     * -Z llvm-time-trace 옵션은 chrome tracing 포맷의 방대한 JSON 출력을 생성하며, 파일이 매우 커 일반적인 텍스트 편집/분석 도구를 사용하기 어려움
     * 이를 newline 단위로 분할 처리(jsonl)해 CLI/스크립트 환경에서 분석 가능

주요 insight 및 결론

     * Rust로 복잡한 프로젝트를 Docker로 빌드할 때, 빌드 속도 병목은 주로 최종 바이너리 빌드 및 관련 LLVM 최적화 단계에서 발생함
     * LTO와 디버그 심볼, opt-level을 조정할 때 빌드 시간과 바이너리 크기 간 트레이드오프 분명함
     * 최적화 옵션을 적극적으로 조정할 경우 빌드 시간 대폭 단축 가능하나, 최적화 미사용 시 퍼포먼스 저하 가능성 존재
     * 대규모 크레이트 의존성과 상용 환경에서 빌드 효율 중요시 된다면 프로파일링을 적극적으로 활용해 세부 병목을 구체적으로 파악하는 것이 좋은 전략임
     * 러스트 빌드 파이프라인 설계 시 LTO, opt-level, 디버그 심볼, 캐시 전략에 대한 정교한 조합 설계가 필요함

        Hacker News 의견

     * 러스트 프로젝트가 종종 겉으로 보기에 작아 보여 흥미로움 느껴짐. 첫째, 의존성은 코드베이스의 실제 크기와 연결되지 않음. C++에선 대형 프로젝트 의존성을 종종 베닝(vendoring)하거나 아예 안 쓰기도 해서, 40만 줄 코드 중 느린 게 많으면 ""코드 많으니 느릴 만하지""라고 생각할 수 있음. 둘째, 훨씬 더 문제되는 부분은 매크로임. 10줄, 100줄씩 반복 확장되는 매크로는 1만 줄 프로젝트도 금세 백만 줄로 만들어버릴 수 있음. 셋째는 제네릭임. 제네릭 인스턴스화마다 CPU 리소스를 소모함. 그래도 변명 좀 하자면, 이런 기능들 덕에 C로 10만 줄, C++로 2만 5천 줄인데 러스트로는 수천 줄로 줄어드는 장점 있음. 다만 이런 기능이 과도하게 쓰이면서 생태계가 느리게 보이는 것도 사실임. 예를 들어 우리 회사에서 async-graphql을 쓰는데, 라이브러리 자체는 훌륭하지만
       프로시저 매크로 의존도가 심함. 성능 관련 이슈가 수년간 오픈돼 있고, 데이터 타입 추가할 때마다 컴파일러가 확실히 느려짐을 체감함
          + 작은 C 유틸리티처럼 원래 코드가 단순했던 곳을 러스트로 다시 작성하는 경우가 많은 이유가 궁금함. 10만 줄짜리 대형 C 프로그램 러스트 포팅 사례보다 더 자주 보게 되는 건 아주 작은 규모 코드임. 작은 프로그램의 컴파일 속도에서 러스트와 C가 어떻게 비교되는지 궁금함. 프로그램 크기가 아니라 컴파일 속도가 궁금한 것임. 참고로 최근 측정상, 러스트 컴파일러 툴체인 크기가 내가 쓰는 GCC보다 약 2배 큼. 1. 이 정도 작은 프로그램들은 어떤 언어든 메모리 안전성 이슈가 숨어 있을 가능성이 낮고, 규모도 작아서 감사도 쉬움. 10만 줄 C 프로그램과 상황이 다름
          + 타입을 새로 정의할 때마다 컴파일러가 느려지는 걸 피부로 느낄 수 있음. 참고로 컴파일러 성능이 타입의 “깊이”에 따라 지수적으로 느려지는 것으로 기억함. GraphQL 같은 경우 중첩 타입이 많아 이 문제가 특히 심각함
          + 매크로가 몇십 혹은 몇백 줄씩 확장하면 코드베이스가 기하급수적으로 커질 수 있다는 문제에 대응해 최근 분석 도구 지원이 추가됨. 관련 자료 참고: https://nnethercote.github.io/2025/06/…
     * Ryan Fleury가 Epic RAD Debugger를 C로 27만8천 줄짜리 유니티 빌드 방식(모든 코드가 하나의 파일로 단일 컴파일 유닛)으로 만들었고, 윈도우에서 클린 컴파일 시 1.5초밖에 안 걸림. 이 사례만 봐도 컴파일이 엄청 빠를 수 있다는 걸 보여주는데, Rust나 Swift에서도 비슷하게 못 만드는 이유가 궁금함
          + 빌드 타임에 컴파일러가 해주는 일이 많아질수록 빌드 시간이 길어짐. Go는 대규모 코드베이스도 1초 이하 빌드 타임 달성 가능함. 빌드 때 필요한 작업만 최소화해둔 간단한 모듈 시스템과 타입 시스템, 그리고 대부분 기능을 런타임 GC에 맡김. 반대로 매크로나 복잡한 타입 시스템, 높은 수준의 견고함이 요구되면 빌드 타임이 길어질 수밖에 없음
          + 러스트 역시 빌드 단위가 크레이트 전체이고, 컴파일러가 LLVM IR로 적절한 사이즈로 쪼갬. 중복 작업과 인크리멘탈 빌드 밸런스도 알아서 조정함. 러스트가 소스 코드 라인 기준으론 C++보다 빌드가 더 빠른 경우가 많음. 다만 러스트 프로젝트는 의존성까지 모두 컴파일하는 특성 있음
          + Rust와 Swift가 C 컴파일러보다 컴파일이 느린 이유는, 언어 자체가 훨씬 많은 분석 작업 필요함. 예를 들어 러스트의 borrow checker가 무료로 제공되는 게 아님. 컴파일 타임 체크만 해도 상당한 리소스 소모임. C가 빠른 건 기본적인 문법 이상 검사 안 하다시피 하기 때문임. 오히려 C는 foo(char*)에 foo(int)를 호출하는 이상한 조합도 체크 안 함
          + 2000년대에 수만 줄짜리 C++ 프로젝트를 컴파일했는데, 구형 컴퓨터에서도 1초 이내로 빌드 끝남. 반면 Boost만 쓴 HELLO WORLD는 몇 초 걸림. 결국 빌드 속도는 언어나 컴파일러만이 아니라, 코드 구조와 사용 기능에 따라 크게 달라짐. C 매크로로 DOOM 만들 수도 있겠지만 아마 빠르진 않을 것임. 반대로 러스트도 빌드 빠르게 구조화 가능함
          + C와 Go 등 빠른 컴파일을 지향하는 언어가 빠른 건 별로 신기하지 않음. 진짜 어려운 건 러스트의 의미론을 빠르게 컴파일하는 것임. 이 문제는 러스트 공식 FAQ에도 있음
     * 나는 Go가 최적화보다 컴파일 속도를 우선시한 게 참 다행. 서버, 네트워킹, glue 코드 작업에는 컴파일이 정말 빠른 게 무엇보다 중요함. 타입 안정성도 적당히 원하지만, 느슨하게 프로토타이핑하게 방해하지 않는 선을 원함. GC가 있다는 점도 편리함. 구글에서 개발 대규모화 경험 끝에 단순한 타입, GC, 엄청 빠른 컴파일이 실행 속도나 의미론적 완벽성보다 훨씬 중요하단 결론을 냈다고 생각함. Go로 만들어진 대규모 네트워킹·인프라 소프트웨어 사례만 봐도 선택이 완전 적중. 물론 GC 허용 불가 환경이나 완벽한 정확도가 더 중요한 곳에선 Go 안 쓸 수 있지만, 내 작업 환경엔 Go의 선택이 최적임
          + 나도 Go를 좋아하지만, 이 언어가 조직적 구글의 대단한 집단 지성의 산물이라고는 생각하지 않음. 구글 경험이 녹아 있었다면 예를 들면 널 포인터 예외 정적 제거 같은 기능은 추가했겠지. 오히려 몇몇 구글 개발자들이 본인이 원하던 언어를 만든 결과물 같음
          + 빠른 컴파일, 적당한 타입 시스템, GC 같은 Go의 장점이 있지만, 디자인 공간상 이미 Java가 비슷한 자리를 차지하고 있었음. Go가 만들어진 건 단순히 창작 욕구에서 비롯된 게 주요했던 것 같고, 결국 오리지널 타깃(서버 측 C/C++/Java)보다도 스크립트 언어(Python/Ruby/JS) 사용자 층에서 더 흡수된 느낌임. 스크립트 유저들은 쉽고 빠른 타입 시스템만 원했고, Java는 너무 올드하고 재미 없었음. 이미 Java는 서버/컨퍼런스/라이브러리 분야엔 공간이 없었음
          + 구글 개발자가 C++ 프로젝트 컴파일 기다리며 Go 디자인했다는 얘기도 있음
          + ""obnoxious type""이 뭐냐 묻고 싶음. 타입은 데이터를 올바르게 표현하든가 못 하든가일 뿐이고, 실제론 어떤 언어에서도 타입 체크러를 억지로 조용히 시킬 수 있음
          + Go의 설계 목적과 실제 용도에 딱 맞는 언어임. 가장 큰 위험은 병렬처리와 mutable 상태를 채널로 공유하는 방식인데, 이 부분에서 미묘하거나 취약한 버그가 발생 가능함. 보통 대부분의 사용자는 이런 패턴을 쓰지 않음. 난 Rust를 사용하는데, 작업 자체가 느린 알고리즘을 느린 하드웨어에 최대한 짜내서 올려야 하는 상황임. 덕분에 대규모 병렬화가 아주 미묘하게 불가능한 문제임
     * 단일 스태틱 바이너리 설치가 컨테이너 관리보다 단순하다는 주장을 이해 못 하겠음
          + docker가 실제로 어떤 작업을 하는지 명확히 이해하지 않은 것 같음. 예를 들어 ""docker 이미지로 배포하면 매번 전체를 새로 빌드한다""고 했는데, 사내 빌드/배포 환경에선 이런 문제 없어도 됨. 개인 용도라면 개발 편의성 유지한 채 로컬에서 빌드한 파일만 컨테이너에 넣어도 무관함. 빌드 환경 흔적 경로만 신경쓰면 됨. CI/CD나 단체 프로젝트에선 어디서든 0부터 빌드 생성을 보장하는 것에 방점이 찍히지만, 개인 작업은 그럴 필요 없음
          + 원문에서 목표는 단순화가 아니라 현대화임. ""최근 10년간 대부분의 소프트웨어가 컨테이너 배포를 표준으로 삼고 있으니 내 웹사이트도 docker, kubernetes 같은 컨테이너로 배포하겠다""고 한 것. 컨테이너는 프로세스 격리, 보안, 표준화된 로깅, 수평 확장성 등 여러 이점 있음
     * 내 노트북(Mac M4 Pro)에서 Deno(대형 러스트 프로젝트) 전체 컴파일에 2분 걸림. 커맨드 기준으로 보면, debug는 약 1분 54초, release는 약 8분 17초 소요됨. 인크리멘탈 컴파일 없이 측정한 수치임. 사실 배포 빌드는 CI/CD 시스템에서 돌아가니, 직접 기다릴 필요 없음
          + M1 Max 기준 6분, M1 Air 기준 11분 정도 걸렸다는 관련 기사 있음
     * Cranelift 얘기는 어디에 나오지? 내 생각엔 러스트로 게임 개발하다가 컴파일 시간이 너무 길어 거의 포기할 뻔함. 조사해보니 LLVM이 최적화 레벨과 무관하게 느림. Jai언어 개발자들이 늘 지적했던 것임. Cranelift로 빌드 타임이 16초에서 4초로 단축되는 경험도 했음. Cranelift 팀에 감탄!
          + 최근 Bevy game jam에서 Dioxus 커뮤니티에서 나온 'subsecond'라는 툴을 썼는데, 이름 그대로 시스템 핫 리로드를 1초 이하로 가능하게 해줘서 UI 프로토타이핑에 큰 도움이 되었음. https://github.com/TheBevyFlock/bevy_simple_subsecond_system
          + zig 팀도 LLVM 없이 자체 컴파일러(백엔드)를 만들어 빌드 타임을 매우 빠르게 하는 시도를 하는 걸로 앎
          + 예전에는 Cranelift가 macOS aarch64 지원 안 했던 걸로 아는데, 최근엔 지원한다는 사실 알게 됨
          + 16초 빌드 타임 때문에 러스트 포기할 뻔했다는 건 좀 과한 거 아님?
     * 러스트가 느리다고 생각하지 않음. 동등 수준 언어 대비 충분히 빠르고, 15분씩 걸리던 C++/Scala 컴파일에 비하면 훨씬 빠름
          + 나도 공감. 러스트 빌드가 딱히 불편함 느껴본 적 없음. 아마 초창기 나쁜 인식이 계속 이어져 이런 평이 생긴 듯
          + 컴파일 시 메모리 사용량이 C/C++에 비해 매우 큼. 내가 유튜브 데모용 VM에서 러스트 대형 프로젝트 컴파일하려면 8GB 이상 필요함. C/C++에선 이런 걱정 안 함
          + C++ 템플릿이 튜링 완전하다는 점에서, 실제 코드 스타일 고려 안 한 채 빌드 타임만 비교하는 건 의미 없음
     * 예전 C++ 개발자 입장에서 러스트 빌드가 느리다는 주장 잘 이해 안 됨
          + 그래서 러스트가 C++ 개발자를 타깃으로 한다고 평가되는 것임. C++ 경험이 많은 개발자는 이미 도구의 불편함을 잘 견디는 Stockholm syndrome이 있음
          + C++보다 빨라도 절대치로 느릴 순 있음. C++ 빌드의 악명은 이미 모두가 잘 알듯 극악임. 러스트는 구조적 언어 문제를 안고 있진 않아서 기대치가 더 높아지는 듯
          + 새로운 기능은 계속 추가되는데, 실제 사용자 의견 듣고 문제 해결은 잘 안 하는 classic 사례라는 생각임
          + C의 컴파일 단계가 적고 단순해 빨랐지만, C++는 템플릿 사용으로 인해 오히려 대부분의 encapsulation work들을 무너뜨렸다고 느껴짐. 하나의 템플릿 헤더만 바꿔도 결국 전체 프로젝트의 98%가 영향을 받는 기분
     * 인크리멘탈 컴파일이 정말 강력함. 초기 빌드 후 인크리멘탈 캐시 스냅샷을 굳혀서 변화가 없으면 그대로 빠르게 빌드/배포 활용 가능함. docker와 궁합도 좋음. 컴파일러 버전이나 큰 웹사이트 업데이트 제외하곤 이미지 빌드 레이어를 안 건드림. 오직 코드 변화만 있을 땐 해당 레이어가 재빌드 안 되게 설정하면 효율적임
          + 내 프로젝트 인크리멘털 아티팩트가 150GB를 넘음. docker 이미지를 이 정도로 커서 썼을 때 실제로 아주 큰 문제들이 생겼었음
     * 내 홈페이지 빌드 타임은 73ms임. static site generator는 17ms만에 다시 컴파일. 실제 generator 실행은 56ms밖에 안 걸림. zig 빌드로그 출력 첨부함
          + C/C++에는 러스트 좋다는 댓글, 러스트엔 Zig 좋다는 댓글이 항상 붙는 것 같음. (알고보니 이 댓글의 작성자가 zig 메인 개발자) 언어 전도는 커뮤니티에 해로우며, 실제로는 반감만 불러오지 새로운 사용자를 유입시키지 않는다고 생각함. 진정 언어를 사랑한다면 이런 전도 문화를 억제하는 게 도움 됨
          + 단일 컴파일 타임 지표 제시 말고, 원글 주제와 직접적인 논의나 해석이 있었으면 더 좋았을 것 같음
          + 내 러스트 웹사이트(react-like 프레임워크 및 실질적 웹서버 포함)도 cargo watch로 인크리멘탈 빌드 시 약 1.25초 소요됨. subsecond[0] 같이 incremental linking, hotpatch까지 쓰면 더 빨라짐. Zig만큼은 아니지만 거의 비슷함. 만약 위에서 이야기한 331ms가 clean(캐시 없이) 빌드라면, 내 웹사이트 clean 빌드 12초보다는 훨씬 빠름. [0]: https://news.ycombinator.com/item?id=44369642
          + @AndyKelley에게 꼭 묻고 싶은데, zig가 컴파일링이 엄청 빠르고 러스트·스위프트가 늘 느린 결정적 이유가 뭐라고 생각하는지 궁금함
          + Zig는 메모리 안전성 보장하지 않는 거 맞지?
"
"https://news.hada.io/topic?id=21607","자바스크립트 개발자를 위한 Go 가이드 (번역)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       자바스크립트 개발자를 위한 Go 가이드 (번역)

     * 5년간 자바스크립트 개발자로 프런트엔드와 백엔드 시스템을 구축한 후, 1년 동안 서버 사이드 코드를 위해 Go로 전환함
     * 이 글은 Go에 대해 궁금해하거나 더 배우고 싶어 하는 자바스크립트 개발자들을 위한 시작점을 제공
     * 기본 사항 : 컴파일 및 실행, 패키지, 변수, 구조체(Structs)와 타입, 제로 값, 포인터, 함수
     * 배열과 슬라이스, 맵, 비교, 메서드와 인터페이스, 인터페이스, 에러 처리, 동시성, 포매팅 및 린팅
"
"https://news.hada.io/topic?id=21574","WWDC 2025 1주 후, 간단한 소회와 관찰","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       WWDC 2025 1주 후, 간단한 소회와 관찰

     * DaringFireball(존 그루버) 의 WWDC 2025 전반적 정리
     * Apple은 올해 WWDC에서 지난해 미완성된 약속 을 했던 경험때문에, 실제로 가을 출시가 확실한 기능만을 엄선해 발표함
     * 모든 주요 기능이 개발자 베타에 즉시 포함되었고, Apple 임원진도 올 가을 실제 출시를 약속함
     * 연내 출시 가능한 기능만 약속하고, 이후 기능은 준비되는 시점에 순차 발표하는 전략을 재확인함
     * 연중 기능 공개 시점을 분산하는 것은 마케팅적으로도 효과적인 접근임

연도 기반 버전 번호 정책

     * 약간 이상하지만 모든 Apple 플랫폼이 OS 26 버전으로 통일되어, 연도와 버전을 한눈에 파악할 수 있게 됨
          + 애플 플랫폼이 6개(Mac, iPhone, iPad, Vision, TV, Watch)로 늘어나면서 어떤게 같은해 출시된건지 추적하기 어려워지긴 했음
          + 이제 이것은 모든 것이 통합된 ""Apple OS 26"" 경험이라고 부를수 있을 듯
     * 버전 일치 정책은 기기 간 통합 경험 강화와 함께, 각 OS의 메이저 릴리즈가 매년 규칙적으로 진행됨을 사실상 공식화함
     * 과거 Mac OS X 시대와 달리, Tim Cook 체제 하에서는 매년 안정적으로 메이저 업데이트가 출시되는 것이 특징임

iPadOS 윈도잉 시스템 변화

     * iPadOS 26에서 선보인 새로운 윈도잉 시스템은 '멀티태스킹'이 아닌 진짜 윈도잉 개념에 가까움
     * 기존의 Split Screen, Slide Over는 제거되고, Stage Manager는 계속 제공되나 복잡성이 높음
     * 윈도우 생성 및 이동, 크기 조정이 직관적으로 가능해져, 멀티 앱 작업 환경이 더욱 자연스러워짐
     * 3년 전 Stage Manager를 도입하며 복잡한 기능부터 시작했던 점과 대조적임

Liquid Glass 디자인

     * Liquid Glass는 VisionOS의 물리적·풍부한 느낌을 기반으로, 모든 Apple 플랫폼에 적용되는 통합 디자인 언어임
     * 하드웨어와 소프트웨어의 깊은 통합을 강조하며, 사용자 경험에 새로운 시각적 활력을 부여함
     * 시각적 효과와 동적 반응성이 강조되지만, 일부는 지나치게 과하거나 미완성된 느낌도 있음
     * Aqua(2000년)와 유사하게, 출시 초기에는 다소 미흡한 부분도 있으나 시간이 지나며 개선될 것으로 기대됨
     * 내부적으로도 WWDC 직전까지 디자인 완성도가 낮았으나, 베타 공개 직전 빠르게 정리되었음

변화의 이유와 마무리

     * Liquid Glass는 단순히 '멋져 보이기' 위한 것만으로도 충분히 도입 가치가 있음
     * Apple은 대담한 혁신보다는, 약속한 것을 실행하는 신뢰성과 실행력을 중시하는 방향으로 전환 중임

     "" 예술 작품의 최종 평가 기준은 결국 우리가 그 작품이 왜 좋은지 설명하는 능력이 아니라, 그 작품에 대한 애정입니다."" - 스탠리 큐브릭
"
"https://news.hada.io/topic?id=21588","팀 쿡, 애플의 영화·TV 비전 공개: ‘F1’ 같은 블록버스터에 수억 달러를 투자하는 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          팀 쿡, 애플의 영화·TV 비전 공개: ‘F1’ 같은 블록버스터에 수억 달러를 투자하는 이유

     * 팀 쿡(Apple CEO)과 F1 슈퍼스타 루이스 해밀턴이 ‘F1 the Movie’로 손잡고, 애플 오리지널 영화 사업에 대대적인 투자를 단행함
     * 2억 달러가 넘는 예산이 투입된 이 영화는 2025년 6월 27일 극장 개봉 예정이며, 흥행 실패 시에도 큰 부담이 되는 고위험 고수익 프로젝트
     * 쿡과 해밀턴 모두 영화의 성공에 큰 의미를 두고, 실질적으로 애플 브랜드와 영화 산업 전체의 상징적 승부로 보고 있음

F1 제작의 의미와 촬영 혁신

     * F1은 ‘탑건: 매버릭’의 조셉 코신스키 감독과 브래드 피트, 제리 브룩하이머 등이 참여한 초대형 프로젝트
     * F1 실제 레이스 현장에서 2023~2024 시즌 동안 28대 이상의 카메라, 5,000시간이 넘는 레이스 촬영, 첨단 카메라 기술 개발 등 새로운 시도를 도입
     * 최신 아이폰 카메라 기술, 프로덕션 전반에 적용. 영화 촬영 경험이 최신 아이폰의 카메라에도 반영
     * 해밀턴은 “이전 레이싱 영화들과 달리, 가장 진짜 같은 레이싱 무비”라고 자평

애플의 콘텐츠 전략과 문화적 영향력

     * 애플은 단순히 하드웨어 판매가 아닌, 스토리텔링을 통한 브랜드 문화적 영향력 확장을 목표로 함
     * 에디 큐(Apple 서비스 담당 수석 부사장)와 팀 쿡은 “최고의 이야기꾼들이 최고의 이야기를 할 수 있는 플랫폼”을 Apple TV+로 지향
     * 애플 오리지널 필름은 피트의 Plan B, 제리 브룩하이머, 해밀턴의 Dawn Apollo와 협업, Warner Bros.가 전 세계 배급 담당
     * 해밀턴의 다큐멘터리 등 추가 협업도 예정

영화 산업 내 애플의 행보

     * 애플은 2019년 Apple TV+ 론칭 이후, 일관된 콘텐츠 라인업과 대형 시리즈·영화 투자로 독자적인 엔터테인먼트 브랜드 구축에 주력
     * ‘F1’은 단순 흥행 성과를 넘어, F1 스포츠와 대중문화 저변 확대, 소프트파워 강화를 노리는 전략적 작품
     * 팀 쿡은 “이 영화에서 카메라와 함께 자동차에 탄 느낌을 받을 수 있다”며, 애플의 모든 역량을 총동원해 지원할 계획임을 강조

투자·경영 철학

     * 애플은 할리우드 메이저 스튜디오 인수 대신 ‘직접 구축(Build, not Buy)’ 전략을 고수, 자사 고유의 철학과 품질 기준에 기반해 콘텐츠 사업을 확장
     * “우리는 툴메이커(toolmaker)”라는 팀 쿡의 철학 아래, 하드웨어와 소프트웨어, 스토리텔링을 모두 아우르는 창작자 지원을 지향
     * 애플의 영화·TV 진출은 단순한 하드웨어 판촉 수단이 아닌, 새로운 비즈니스이자 브랜드 가치 제고로 인식

실적과 시장 반응

     * Apple TV+ 가입자 수나 서비스 매출은 비공개, 서비스 부문 전체에 통합되어 공개
     * 업계와 시장은 애플의 오리지널 콘텐츠 투자가 하드웨어 판매와 어떻게 연결되는지에 주목
     * 쿡은 “더 많은 아이폰 판매를 염두에 둔 전략은 아니다”라고 명확히 밝힘

크리에이티브와 산업 내 신뢰 구축

     * Apple TV+, Apple Original Films 모두 품질 중심, 창작자 친화적 환경을 지향
     * 벤 스틸러(‘세버런스’ 총괄 프로듀서) 등 업계 인사들이 “애플은 진정으로 크리에이터를 존중하는 회사”라고 평함
     * 영화 ‘F1’의 대대적 극장 개봉은 애플의 극장 산업에 대한 강한 의지를 보여줌

해밀턴의 새로운 도전

     * 해밀턴은 레이싱 선수 경력을 넘어, Apple과 함께 다큐·영화 제작자로의 전환을 시도
     * “애플과 같은 파트너와 함께라면 최고의 환경에서 도전할 수 있다”고 언급

마무리

     * 팀 쿡과 애플 리더십은 혁신과 품질, 브랜드 정체성 유지를 영화·TV 분야에서도 최우선으로 삼고 있음
     * “우리는 소수의 제품만 만들지만, 각 제품에 혼을 쏟는다. 영화와 TV도 마찬가지다”라는 쿡의 언급처럼, 차별화된 문화·창작 가치 추구가 애플 영화 전략의 핵심임

   F1 공개된 트레일러들만 봐도 기존 영화들과는 색감과 화질부터 다른게 보이긴함. 획실히 영화 전체적으로 무언가 신선한 시도를 했다는 것은 분명해보임.

   아이폰 쓰시는 분들은 F1 햅틱 트레일러 한번씩 보세요. 신기하긴 합니다.

   https://tv.apple.com/us/clip/…
"
"https://news.hada.io/topic?id=21656","Microsoft Edit","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Microsoft Edit

     * Microsoft Edit는 클래식 MS-DOS Editor에 오마주를 바치는 간단한 텍스트 편집기임
     * VS Code와 비슷한 현대적인 인터페이스와 입력 컨트롤을 제공함
     * 개발 목표는 터미널에 익숙하지 않은 사용자도 접근할 수 있는 편집 환경 제공임
     * Search and Replace 기능을 위해 ICU 라이브러리 선택적 의존성을 가짐
     * 패키지 관리자를 위한 명확한 실행 파일 네이밍 및 환경 변수 옵션 안내를 포함함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

오픈 소스 프로젝트 개요

     * Microsoft Edit는 간단한 작업을 위한 고전 에디터 스타일의 텍스트 편집기임
     * MS-DOS Editor를 현대적으로 재해석한 것이 특징이며, VS Code 스타일의 친근한 UI와 입력 방식을 적용함
     * 특별히 터미널 사용 경험이 적은 유저도 쉽게 사용할 수 있는 단순성에 중점을 두어 설계됨

특징 및 기능

     * 최소한의 복잡도를 가지면서 기본적인 텍스트 편집 작업을 간편하게 수행할 수 있음
     * 인터페이스는 익숙한 느낌을 제공하며, 접근성과 사용 편의성을 중시함
     * ICU(International Components for Unicode) 라이브러리에 선택적으로 의존하여 Search and Replace 기능을 지원함

패키지 관리자 및 패키징 관리자를 위한 주의사항

  패키지 네이밍

     * 기본 실행 파일명은 ""edit"" , 대체 이름은 ""msedit"" 임
     * 기존 시스템 명령어 ""edit""와 충돌 문제로 인해 ""msedit"" 등 대체 네이밍을 권장함
     * ""ms-edit"" 등의 이름은 피하는 것을 권장함

  ICU 라이브러리 네이밍(SONAME)

     * Search and Replace 기능을 위해 ICU 라이브러리를 사용할 수 있음
     * 각 OS별 기본적으로 찾는 라이브러리는 다음과 같음
          + Windows: icuuc.dll
          + macOS: libicuuc.dylib
          + UNIX 및 기타: libicuuc.so
     * 시스템 환경에 따라 라이브러리 이름(SONAME)이 다를 경우 각종 환경변수(EDIT_CFG_ICUUC_SONAME, EDIT_CFG_ICUI18N_SONAME 등)로 설정 가능함
     * ICU export symbol 명명 규칙이 다를 경우를 위한 추가 환경 변수 제공

  기타

     * ICU 리네이밍 자동 감지, C++ 심볼 지원 등 추가 옵션이 있음
     * 해당 설정 검증을 위해 cargo test -- --ignored 명령으로 테스트 가능함

결론

     * 간단함과 접근성을 중시하면서도, 유연한 환경 구성이 가능한 오픈 소스 편집기
     * 개발자, 오픈 소스 기여자, 패키지 관리자에게 명확한 가이드라인과 높은 호환성을 제공함

        Hacker News 의견

     * 그냥 “내가 하고 싶어서” 만든 프로젝트라는 이야기인데, 나도 그런 식으로 뭔가를 만들어보며 내부 동작 원리 이해하는 경험 많이 했던 기억임. 하지만 Turbo Vision을 FPC로 다시 쓰고 여러 대상을 지원해서 컴파일하는 버전은 이미 20년쯤 됐음. Turbo Vision은 최고의 텍스트 모드 윈도우 라이브러리라고 생각함. 진짜 재미는 전체 텍스트 화면을 배열로 매핑하는 부분에서 시작임. var Screen: Array[1..80,1..25] Of Byte Absolute $B800 뭐 이런 식이었던 걸로 기억함. Turbo Vision이 진짜 혁신이었던 건 이동 가능한 (비)모달 윈도우였다는 점임. 그러니까 결국 저 배열을 루프 돌리면서 끊임없이 다시 쓰는 거였음. 꽤 빨랐음. 나도 저 라이브러리로 꽤 많은 수익을 올린 기억임
          + 궁금한 사람을 위해 요즘 버전 C++ Turbo Vision, 그리고 유니코드 지원까지 하는 포트가 있음 https://github.com/magiblot/tvision
          + TP의 배열은 row-major로 되어 있었음. 문자 하나가 2바이트(글자+속성)로 구성됨. 그래서 심지어 array[1..25, 1..80] of packed record ch: char; attr: byte end absolute $B800:0000 이런 편의성이 있었음. 흑백 텍스트 디스플레이에서는 $B800을 $B000으로 바꾸면 됨. 예시로 Hercules 같은 환경
          + VSCode에서 저런 인터페이스가 터미널에서 (심지어 원격으로도) 되면 정말 좋을 것 같음
          + 어떻게 그 라이브러리로 돈을 벌었는지 궁금함. 비법 좀 알려줬으면 함
          + 요즘 새로 나오는 TUI 프레임워크 볼 때마다 항상 떠오르는 생각, “Turbo Vision만 못함”이라는 아쉬움
     * 한편으로는 AI Copilot 같은 불필요한 덩치를 Notepad에 억지로 넣고 있음. 원래 Notepad의 핵심은 아무 기능 없이 '하나만' 제대로 하는 거였던 기억임
          + 새로운 Edit도 이런 결정에서 자유롭지 못함. Satya 시대에 MS가 FOSS를 좋아하는 척했지만 Gates/Balmer 시절이 훨씬 윈도우 개발자에게 친절했던 기억임. 지금은 웹/데스크톱 프레임워크가 뒤섞였고, 내부적으로조차 잘 안 쓰는 판임. 예전 VS 위저드나 플러그인 대신 CLI 툴로 엑셀 파일 덤프나 하는 식. 세대 간 윈도우 개발 문화나 관리진에 노하우 부족함이 잘 드러남
          + Raymond Chen이 Notepad가 의외로 많은 테스트에 쓰인다는 말 한 적 있음. 심플한 기능이지만 실험용으로 자주 쓰인다는 내용 https://devblogs.microsoft.com/oldnewthing/20180521-00/?p=98795
          + 윈도우11 새 Paint에서 스크린샷 붙여넣기해 봤는데, 최소화된 상태에서도 CPU 5%를 계속 쓰고 메모리는 250MB쯤 차지함. RAM이야 그렇다 쳐도 CPU를 이렇게 낭비하는 건 좀 아니라는 생각임. 예전에는 자부심이나 품질 관리라는 게 있었던 기억임
          + ISP가 간헐적 장애(IPv4/MTU 문제) 있을 때, Notepad에서 저장조차 안 됨. 억지로 꺼야 가능했음. 당시에는 Wireguard로 임시 우회 세팅하던 중이었음
          + 모던 notepad를 삭제하면, 시작 메뉴에서 예전 notepad를 검색해도 안 나옴
     * 한 달 전쯤 MS가 윈도우 사용자에게 더 친숙한 리눅스 배포판을 냈다는 얘길 들은 기억임. 기억상 단순한 GNOME 환경이었고 특별한 점은 없었음. 오히려 MS가 자기만의 리눅스 배포판을 만들면서 bash를 powershell로, Edit를 vim/nano 등으로 대체하고, .NET이나 Visual Studio Code도 기본 개발 툴로 포함할 수 있었을 텐데... MS가 이걸 WSL의 기본 배포로 활용했다면 전쟁에서 이기진 못해도, 사용자 점유율은 오를 수 있었겠다는 아쉬움임. MS가 커널까지 장악하진 못하더라도 사용자 공간(userland) 지배는 가능함. 많은 윈도우 유저들이 기본 설치 앱으로 MS의 툴을 자연스럽게 쓰게 만들 수밖에 없음. 이제 Microsoft Edit도 리눅스에서 사용 가능함. Powershell같이 다른 앱들도 마찬가지고. 만약 10년 전 이런 전략을 썼다면 오늘날 WSL에서 MS 배포판이 상위 5위 안에 들 수도 있었다는
       상상임. 큰 기업(M$)들이 내 개인 PC까지 영향력을 뻗칠 수 있다는 점에서 약간 불편함. 결국은 이 Microsoft Edit가 Co-Pilot 기본 제공될 날을 상상 중임
          + 언젠가는 MS가 최소한 Windows Server나 임베디드 윈도우 같이 일부 영역부터 리눅스 쪽으로 점차 이동할 거라 생각함. 장기적으로는 윈도우 데스크톱도 점진적으로 변화해서 ‘Windows Legacy’ vs ‘Windows Linux Workstation’ 옵션이 나올 것 같음. 리눅스 커널 + 튜닝된 와인(WINE) + 일부 레거시용 통합 VM 형태로 진화 예상함. 문제는 NT 커널이 설계상 리눅스보다 앞선 점이 많음(예: GPU 드라이버 전체 충돌 후 복구 가능 등). 하지만 윈도우 자체는 점점 자산보다 부담으로 변하는 중임. 실제로 MS의 성장 동력은 Azure & Office 365이고, 윈도우 라이선스는 거의 정체 상태임. 최소한 리눅스 기반 윈도우 서버와 워크스테이션은 기대 가능함
          + Azure Linux(구 CBL-Mariner)는 컨테이너, VM, 서버 용도로 만든 MS 공식 리눅스 배포판임. 단순 윈도우 유저를 위한 스킨이나 데스크탑 환경과는 구분 필요함
          + MS가 예전에 만든 리눅스 배포판 이름 “Xenix”가 있었지만, 성적이 안 좋았던 기억임
          + WSL 탄생 배경은 대기업 내 개발자가 리눅스 환경 필요성이 높았기 때문임. IT 지원 쪽은 리눅스를 잘 모르고, 굳이 지원하고 싶어하지도 않음. WSL이 이런 문제를 해결함. 실제로 많은 개발자가 리눅스를 쓰고 싶어하지 않고, 터미널도 서툰 경우가 많음. GUI 툴에 의존함
          + MS가 윈도우 유저 감성 만족시키려고 자체적으로 비밀 배포판 유지한다는 건 다소 비현실적임
     * 이 소식이 일주일 사이 3번이나 올라올 만큼 화제임
         1. 作者 포스트 - https://news.ycombinator.com/item?id=44034961
         2. Ubuntu 공식 포스트 - https://news.ycombinator.com/item?id=44306892
         3. 그리고 이번 포스트
          + 여기에 하나 더 있음 https://news.ycombinator.com/item?id=44341462
     * 원래 edit.com(DOS 6.22부터, 이후 7.0/윈95)은 내 첫 IDE였음. 시작은 qbasic이었고 edit.com과 거의 같은 프로그램임. C/C++을 djgpp로 배우면서도 edit.com을 쭉 사용했음. 내 “프로젝트 파일”은 e.bat로, edit file1.cpp file2.cpp...처럼 여러 파일을 한 번에 열 수 있었음. 다른 에디터는 멀티 파일 전환이 불편했는데, alt-1,2,3...으로 바로 전환되는 게 마음에 들었음. 난 아직도 에디터 단축키 바꿀 때 이 스타일을 꼭 유지함. 비록 코드 에디터로는 구린 편이었음. 문법 하이라이트도 없고, 들여쓰기 지원이 별로였음(그래서 처음에는 들여쓰기를 두 칸으로 했는데, 수동으로 하기도 편했음). 그래도 작성한 코드에 대한 즉각적 피드백과 친숙함이 최고였음. qedit 같은 에디터도 있었지만 내 취향은 아니었음, 유닉스 계열 에디터는 도스에서는 별로였단 생각임. 이번 새로운 editor는
       멀티 버퍼 지원은 되지만, 내가 익숙한 키 바인딩 방식은 적용 안 된 듯함
          + 이슈로 제기하면 좋겠음. 이런 피드백은 초기에 전달하면 실제 반영되는 경우 많음. 그리고 진짜 그냥 비슷한 수준이 아니라, edit.com은 qbasic에 플래그 하나 추가해서 부팅된 것과 똑같았음. 직접 qbasic에 그 플래그 줘서 쓰기도 했음 https://news.ycombinator.com/item?id=44037509
          + 문법 하이라이트는 없었지만, 문법 대문자화(예: 예약어 자동 대문자 변환)는 있었음. 예를 들어 한 줄을 전부 소문자로 입력해도 엔터 치면 예약어가 자동 대문자로 바뀌었음. 별 거 아니지만 꽤 편했음
          + copy con 시절에 비하면 edit가 정말 구세주였음
     * 여러 면에서 마음에 든 점이 많음. 우선, 의존성 없는 깔끔한 리스트! 완전히 반함. 전체 TUI를 이 한 에디터를 위해 직접 만들었다는 게 믿어지지 않음. 다이얼로그, 파일 브라우저까지 있음. 내 프로젝트에도 적용해보고 싶음. 혹시 관계자 있으면 Ratatui를 왜 안 썼는지 궁금함. 코드 품질이 워낙 뛰어남. 한 마디로: Bravo!
     * 예전에는 micro[1]를 이런 텍스트 에디터를 찾는 사람에게 추천했었음. 이제는 뭘 추천할지 고민임
          + https://micro-editor.github.io/
          + 내 생각엔 추천을 바꿀 필요 없음. edit는(최소한 내가 써보니) 문법 하이라이트도 지원 안 함
          + 마지막에 체크해봤는데, micro라기보단 macro라고 해야 할 정도로 바이너리 파일 크기가 컸음
          + dte[1]라는 옵션도 있음. 유니코드 지원, CUA 키 바인딩 등 아주 심플하면서도 강력함. nano 대체 터미널 에디터로 만족중임 https://craigbarnes.gitlab.io/dte/
          + Windows에서는 ""winget install zyedidia.micro""만으로 설치할 수 있음. 예전 8비트/16비트 시절 에디터 느낌임
     * MS같이 큰 조직에서 이런 프로젝트가 어떻게 승인되는지 정말 궁금함. 단순 개발자의 사이드 프로젝트인지, 제품 로드맵의 일부인지, 아니면 리더십 설득 과정이 있었는지 궁금함
          + 텍스트 에디터는 copilot 통합 공격 목표로 아주 적합함
          + 이유 설명에서 알 수 있듯, 명령줄에서 동작하는 에디터가 필요했음(윈도우 코어 서버 인스톨용), SSH로 접속해서도 써야 했고(몇 년 전부터 윈도우에 SSH 서버 기본 내장), vi 경험 없는 윈도우 관리자를 위한 모달 없는 에디터 필요성이 있었음. 이런 요구들이 이번 프로젝트로 이어진 것임
          + 각 팀이 뭔가 할당량을 채워야 해서 아이디어를 내기도 하고, 때로는 리더 지시에 의해(copilot 사용 등), 혹은 해커톤 같은 행사에서 시작해 확장되는 경우도 있음. 연구 조직에서 기술 인력이 손을 놓고 있으면 이런 게 나오기도 하고, 오랜 분석 끝에 그제야 예산 붙는 경우도 있음. 커미터 수만 봐도 꽤 전략적 투자였던 것 같음. 하루아침에 뚝딱 나온 프로젝트는 아님
     * 예전 EDLIN이 유니코드 지원으로 나와주길 기대함. EDLIN은 배치파일에서 스크립트적으로 키 입력을 파이프로 넣어서 특정 작업 자동화가 가능했음. 일종의 sed나 awk의 일부 대체품 같았음. vi도 비슷한 아키텍처가 된다고 생각하는데, 얼마나 비정상적인지는 별도 문제임
          + 아마 찾는 건 ed임. -s 옵션 붙이면 스크립트에 딱임
     * 관련 토론(271 포인트, 185개 댓글) https://news.ycombinator.com/item?id=44031529
"
"https://news.hada.io/topic?id=21620","브라우저에서 플레이 가능한 Lego Island","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       브라우저에서 플레이 가능한 Lego Island

     * LEGO Island 웹 포트 프로젝트는 1997년 출시된 클래식 PC 게임을 현대 웹브라우저에서 즐길 수 있게 구현함
     * 이 프로젝트는 Emscripten을 활용하여 원본 게임을 웹 환경에 맞게 재구현함
     * 원본 디컴파일 및 포터블 프로젝트의 기여 덕분에 가능해졌으며, 여러 기여자들의 오랜 노력의 결과물임
     * 보존 및 접근성을 목표로 하여 게임 역사적인 유산을 더 많은 사용자에게 제공함
     * 아직 개발 중으로, 버그와 개선점에 대한 사용자 피드백이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LEGO Island 웹 포트 개요

   LEGO Island 웹 포트 프로젝트는 1997년도에 출시된 클래식 PC 게임 LEGO Island를 최신 웹브라우저에서 직접 플레이할 수 있도록 Emscripten으로 완전히 새롭게 재구현한 프로젝트임

프로젝트 배경과 개발 과정

     * 이 프로젝트는 원본 디컴파일 프로젝트의 작업 위에 기반함
     * 이후, 해당 코드를 바탕으로 포터블 버전으로 확장하여 다양한 플랫폼에서 실행 가능하게 개선함
     * 수많은 기여자들의 노력과 수천 시간의 작업이 집약되어 게임의 역사를 보존하는 데 중요한 역할을 함

프로젝트 목표 및 발전 방향

     * 이 프로젝트의 궁극적인 목표는 게임 보존과 모두를 위한 접근성 제공임
     * 현재 개발이 계속 진행 중이기 때문에, 일부 버그나 예상치 못한 문제가 발생할 수 있음
     * 커뮤니티의 피드백과 지속적인 기여가 프로젝트 발전에 큰 도움이 됨

        Hacker News 의견

     * 어린 조카가 Lego Island를 플레이하는 걸 본 기억이 있어. 도입 영상에서 카메라가 섬을 날아다니는 장면은 정말 대단한 감동. 그런데 메인 메뉴에서 흥분한 레고 캐릭터가 복잡한 언어로 설명하는 걸 보고 조카가 완전히 당황하는 모습이었어. 책에 이름을 적거나 아이콘을 지도에 끌어다 놓아야 하는 등 추상적인 작업 때문에 재밌는 레이싱 같은 걸 하기도 전에 진입 장벽을 느끼는 상황. 조카가 아마 한참 동안 화면 여기저기를 클릭해도 사람을 지도에 끌어다 놓아야 한다는 걸 모를 수도 있다는 생각이 들어. 게임 자체는 멋진 경험이지만 요즘 같은 시대엔 이런 식으로 만들지는 않을 것 같아. 어린이가 좋아할 것 같은 UI를 실제로 테스트하지 않고 어른의 시각에서 상상만 했다는 느낌.
          + 이 게임의 핵심은 이런 발견의 경험, 클릭하고 여기저기 움직여보는 게 중요한 게임플레이라는 점. 그렇지만 1996년 당시 게임 UX 자체가 다소 거칠었다는 점은 동의.
          + 사실 책에 이름을 입력하면 InfoManiac이 초상화를 지도에 드래그하라고 알려주는 친절함이 있긴 해.
          + 나는 이 게임을 한 번도 해보지 않았는데, 나 역시 메인 메뉴에서 막혀서 당황했던 경험.
          + 나도 비슷하게 헤매다가 10초 만에 섬에 들어가지 못하고 포기했던 기억.
     * 포팅 과정의 뒷이야기를 담은 유튜브 영상 공유.
          + 이 영상도 이번 디컴파일링 과정을 흥미롭게 다루고 있어서 추천.
     * 정말 놀랍다는 감탄. 내가 6살쯤 Windows 95에서 이 게임을 했을 때 캐릭터를 자유롭게 돌아다닐 수 있다는 것 자체가 너무 멋진 경험이었어. 이제는 브라우저에서 실행된다는 게 신기함. 디컴파일링 방식이 생각보다 효과적이었던 것 같아. 비슷하게 starcraft도 ARM에서 돌아가게 만든 사람이 있었는데, 방식이 완전히 어셈블리로만 해서 결과가 별로였다는 비판도 봤어. MattKC가 좀 더 현실적인 결과를 추구했다는 느낌.
     * MattKC의 독특하고 자유분방한 스타일이 정말 좋아.
     * 이 시점에 딱 맞는 소식이라는 반가움. 중학생 때 정말 이 게임에 푹 빠져 살았고, 최근에는 최신 하드웨어에서 돌릴 방법을 찾아보고 있었어. XP 가상머신에서 설치와 실행까지는 성공했지만, 아무래도 최선의 방법은 아니라는 한계가 있어. 이런 레트로 게임을 위해 게이밍 PC를 따로 맞추는 것도 고민 중이었는데, 이 소식을 듣고 그 결정을 좀 더 미뤄도 될 것 같은 기분.
     * 다음에는 LEGO Racers의 포팅을 원하는 바람.
          + 아주 간절하게 LEGO Racers도 포팅되길 희망.
     * 정말 대단하다는 첫 반응. MattKC의 디컴파일링 프로젝트가 기반이었는지 궁금.
          + 응, MattKC가 가장 최근 Lego Island 영상에서 직접 언급한 사실.
     * 게임이 유행하던 시절엔 없었던 세대라는 고백이지만, 브라우저에서 플레이할 수 있게 되었다는 소식에 감탄. 그때 그 시절 이 게임을 즐겼던 사람들에게는 멋진 깜짝 선물이 될 것 같다는 기대감.
     * 머릿속 한구석에 남아있는 게임 중 하나. 개성 넘치는 내레이터와 캐릭터들이 그 시대 기준에서도 정말 독특했어. 이런 게임들과 backyard baseball 같은 작품들이 브라우저나 최신 앱으로 다시 나오는 건 좋은 일인데, 예전의 감정까지는 똑같이 느끼기 힘든 아쉬움이 있어.
     * Lego Island 2도 꼭 플레이해보고 싶다는 소망.
"
"https://news.hada.io/topic?id=21657","OpenAI는 분당 요금을 청구하니, 오디오 속도를 올려서 시간 단축하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                OpenAI는 분당 요금을 청구하니, 오디오 속도를 올려서 시간 단축하기

     * OpenAI의 오디오 트랜스크립션 요금은 입력 오디오의 길이에 따라 산정됨
     * ffmpeg 같은 도구로 오디오를 2~3배 속도로 변환한 뒤 업로드하면 트랜스크립션 품질 저하 없이 처리 속도와 비용 절감 가능함
     * 실제 40분 오디오를 2배, 3배로 속도 변환 시 비용이 23~33% 절감됨
     * gpt-4o-transcribe 모델은 25분 미만 오디오만 지원하므로, 속도 올리기가 유용한 우회책임
     * 2~3배까지는 결과 품질이 유지되나 4배 속도에서는 트랜스크립션 정확도 급락 현상 발생함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

요약 소개

     * OpenAI의 트랜스크립션 및 오디오 요금 정책을 더 효율적으로 활용할 수 있는 간단한 방법
     * 오디오 변환 속도를 높여 더 짧은 시간 안에 동일한 내용을 처리하도록 해 요금과 시간을 모두 절약하는 전략
     * ffmpeg와 같은 오픈소스 도구로 오디오 파일을 2~3배 빠르게 변환한 후 OpenAI API에 업로드하면 품질 손실 없이 가격과 소요 시간을 낮출 수 있음
     * 이 방법은 특히 입력 길이(via gpt-4o-transcribe 모델의 25분 제한)가 긴 오디오에 더 효과적

트랜스크립션 속도/비용 절약의 핵심 방법

     * OpenAI의 오디오 트랜스크립션 서비스는 받아들이는 오디오의 길이를 기준으로 요금을 책정함
     * 따라서 음성 파일을 ffmpeg 등으로 미리 2~3배 가속하여 업로드하면, 입력 토큰 수가 크게 줄어들고, 트랜스크립션 처리 시간 역시 짧아지는 효과가 있음
     * 이 방법을 실제로 적용하면 40분 분량 오디오 기준 입력 토큰 비용이 33% 이상 절감됨 (3x 적용 시 $0.07, 2x 적용 시 $0.09)
     * 출력 토큰 비용은 오디오 속도와 무관하게 거의 동일하게 나타남(입력 요약 길이 기준 자동 할당 결과임)
     * 2배, 3배 속도는 트랜스크립션 정확도가 안정적이지만 4배 속도에서는 모델이 내용을 제대로 인식하지 못하는 한계가 발생함

사용 스크립트 예시

   다음 오픈소스 도구 사용 필요:
     * yt-dlp : YouTube 등에서 오디오 추출
     * ffmpeg : 오디오 변환 및 속도 조절
     * llm : 텍스트 요약 자동화

   참고용 전체 워크플로우:
     * yt-dlp로 오디오 추출 후,
     * ffmpeg로 오디오를 2~3배로 변환하여 mp3 저장
     * OpenAI API(gpt-4o-transcribe)로 mp3 업로드 및 트랜스크립션 텍스트 획득
     * 해당 결과 텍스트를 llm에 입력해 원하는 요약문 자동 생성

실제 경험담 및 시행착오

     * 처음에는 YouTube의 자동 트랜스크립션을 받으려 했으나, yt-dlp의 구버전(2025.04.03)이어서 다운로드 오류 발생
     * 프로그램 업데이트 이후에는 정상 작동했으나, 그 사이 수동 추출 및 ffmpeg 가속→OpenAI API 처리 방식에 도전하게 됨
     * M3 MacBook Air에서 로컬 Whisper로 실행 시 배터리 부하와 실행 속도 이슈 발생, 클라우드(OpenAI API)로 오프로드하는 것이 더 빠르고 효율적임

트랜스크립션 품질과 알고리듬 특성

     * 오디오 속도를 2배~3배로 높여도, 사람이 원본 음성을 빠르게 재생해 듣는 것과 비슷하게 AI 모델도 본질 정보 인식이 거의 가능함
     * 이미지 파일 용량 최적화(손실/비손실 포맷)와 비슷하게, 청취 정보의 일부분 손실(높은 속도에서 간헐적 단어 손실 등)이 생겨도 요약과 이해에는 큰 지장 없음
     * 두뇌가 잘못된 스펠링, 일부 단어 생략된 텍스트도 보완해 인식하듯, 트랜스크립션 알고리듬 역시 가속된 오디오에서도 대부분 주요 정보를 잘 추출함

실제 요금 비교 및 절감 폭

     * OpenAI의 gpt-4o-transcribe 기준, 오디오 속도별 비용은 다음과 같이 계산됨
          + 2배속(1,186초): $0.09
          + 3배속(791초): $0.07
          + 입력 오디오가 길 경우(예: 2,372초 원본)은 모델 요건상 처리 불가
          + Whisper-1 모델 기준 $0.006/분, 결과적으로 이 방법을 쓰면 최대 67% 가량 비용 절감이 가능함
     * 출력 토큰 비용은 입력 속도와 상관없이 거의 동일 (모델의 컨텍스트 윈도 및 요약 방식 영향)
     * 4배속 적용 시에는 출력 결과가 반복 문장 등으로 심각히 저하됨

권장 사항 및 결론

     * OpenAI의 음성 트랜스크립션을 빠르고 저렴하게 이용하려면 2~3배로 오디오 가속이 가장 효율적임
     * 너무 빠른 속도(4x)는 정확도 저하 문제 있음
     * 간단하고 실행이 쉬운 방법이며, 품질 유지와 비용 절감 모두에 유리함
     * 일반적인 비즈니스 오디오 요약, 회의록 등 장시간 음성 데이터 처리가 필요한 스타트업과 IT 실무자에게 직접적인 비용/시간 절감 수단으로 활용 가능함

요약(TL;DR)

     * OpenAI는 오디오 길이 또는 입력/출력 토큰 기준으로 요금 청구함
     * ffmpeg로 음성을 2~3배 빠르게 변환해 입력하면 시간과 비용 모두 절약 가능함
     * 입력 토큰(또는 시간) 감소로 요금이 낮아짐
     * 2배, 3배까지는 최적 속도이며, 4배 이상부터는 트랜스크립션 품질 저하 현상 있음

   Can you use gpt-4o-transcribe?
   I asked OpenAI yesterday, and they told me that only the Whisper model can be called using the API key.
   I’m thinking of trying Whisper with a faster setting to see if the quality can still be maintained.

        Hacker News 의견

     * Andrej가 진행한 강연의 속도는 원래 일반인보다 최소 1.5배 빠른 자연스러운 속도라서, 따라가려면 유튜브 재생 속도를 꼭 1x로 낮춰야 한다는 느낌 받음. OpenAI minute를 더 효율적으로 만들고 싶으면, 침묵 구간을 아예 빼고 처리하는 방법 제안.
       ffmpeg 명령어 예시로 -50dB 이하 20ms 이상의 모든 침묵을 20ms 정지로 치환해 39분 31초 영상을 31분 34초로 단축 가능성 경험. 본문의 취지에 따라 길이만 재서 효과 측정, 짧아진 버전의 품질은 따로 확인하지 않음
          + 나는 늘 2x 속도로 모든 영상을 보고 Andrej의 강연도 2x가 자연스럽게 느껴짐. 다만 내가 만든 영상은 주변 사람들이 너무 빨라서 0.75x로 봐야겠다고 종종 얘기함. 내 입장에서 2x가 아니면 너무 느린 체감. 참고로 John Carmack의 말 빠르기는 2x도 완벽히 자연스럽다고 느낌. 최근 내 영상이 궁금하면 여기서 확인 가능하며, 대부분 즉석에서 주제만 정하고 녹음하는 식으로 250~300편 이상 진행. 혹시 내 영상이 지나치게 빠르다고 느끼는지, 아니면 충분히 평범한 속도인지 궁금
          + 굳이 품질을 확인하지 않았다면, 두 버전의 결과물을 diffchecker 같은 걸로 비교하면 간단했을 것 같다는 생각
          + 일반인 기준 2.25x 유튜브 속도가 있었으면 하는 바람 있음. 나는 항상 단축키를 쓰고, 2x로 90% 정도 듣는데, Andrej의 강연만은 1.25x보다 빠르게 돌리기가 힘듦
          + Andrej가 일반인보다 1.5배 이상 빠르게 말한다는 점에서, 유튜브 속도를 원래대로 돌려야 한다는 말에 공감. 사람의 말하기 속도를 자동으로 감지할 수 있는 방법이 있을지 궁금. 속도는 주관적이고 사람마다 다르지만, OP가 시도한 방법이 실패했을 때를 탐지할 수 있다면 재미있을 것 같음. (예: x4 속도에서 품질 망가져버린 것처럼)
          + ffmpeg 마법을 더 활용할 수 있다는 생각에 기대감. 나중에 꼭 시도해보고 싶어 아이디어에 감사
     * 대충 훑어보기와 시간을 내서 제대로 읽어보기에 대한 생각.
       Andrej 강연의 transcript와 요약본만 읽었을 땐 평범하게 느껴져 그냥 넘겼는데, 유튜브에서 전체 영상을 보니까 엄청 다양한 아이디어, 생각과 결정으로 이어지는 경험을 함. 이런 일은 다른 주제에서도 자주 겪음. 직접 컨퍼런스에 참석해서 들으면 온라인 강연보다 훨씬 더 유용함. 온라인으로 보는 것도 요약만 읽는 것보다 훨씬 유익. 심지어 10분 만에 대충 생각하고 마는 것보다 산책하면서 깊이 생각하는 편이 훨씬 나음. 생각을 위해서는 천천히 하는 게 보통 더 좋다는 체감
          + 이게 진짜 신기하게 느껴짐. 학교에서 획일적으로 지식을 던져주는 걸 싫어했던 개발자로서, 지금은 그런 형태의 경험에 기꺼이 돈까지 내고 있다는 현실이 이상. 읽기 자체가 즐거움이고, 강연을 보면서 생각이 맞아 들어가는 감각도 멋짐. 세상의 의미를 우리 스스로 생각하는 게 인간다움이라 보는 입장. 그런데 오히려 이런 경향이 모두를 어리석게 만드는 길 같아 전혀 공감할 수 없음
          + 위 의견에 매우 공감. 강연이 주는 가치는 공개된 사실이나 아이디어 그 자체보다, 그걸 계기로 생기는 다양한 부가적 영감이 더 크다고 생각. 세상에는 정말 무수히 많은 정보가 존재하고, 맥락이 전부임. 만약 조금만 더 구체적인 맥락이 붙었다면 시간을 내서 봤을 텐데, 맥락 없는 링크로만 받으니까 그냥 ‘핵심’만 빠르게 파악해서 대응하려는 태도. 결국 이번에는 덕분에 흥미 생겨서 다시 볼지도 모르겠음. “천천히 생각하는 게 보통 더 좋다”에 다시 한 번 동의
          + 천천히 생각하는 게 중요한 건 맞지만, 강연 내용을 조금 들었다가 나중에 다시 보면서 더 깊게 숙고하는 방식도 꽤 쓸모 있을지도 생각
          + 과연 영상의 속도가 중요했는지, 아니면 영상과 오디오가 주는 부가 정보 때문인지 질문. 말을 잘하는 연사는 똑같은 메시지도 오디오/비디오에서 훨씬 더 잘 전달하는 체감 있음. 오디오는 특정 부분에 힘을 실고, 영상은 제스처나 표정으로도 메시지를 보탤 수 있기 때문
          + 나는 오히려 팟캐스트나 오디오북을 2~3x로 듣는 사람들을 보면, 내 경우엔 0.8x로 느리게 돌릴 때가 더 집중할 수 있고 생각할 시간이 더 많아진다고 느낌. 혹시 내가 예외적인 사례인지 궁금
     * OpenAI의 transcription API로 40분짜리 강연 요약을 시도했지만 길이가 길어 ffmpeg로 3배속 압축 후 25분 제한 내로 구동. 실제로 효과를 봤고, 비용과 시간 모두 절약되어 글로 공유함. 전체 스크립트와 비용 구성 포함
          + 이런 비법을 조용히 활용해서 OpenAI보다 더 싼 transcription 사업도 시작 가능했을 것 같다는 농담
     * ""정확도는?"" ""모르겠음, 원래 그게 요점""이라는 원작자 느낌 그대로 멋진 작업이라는 생각과, 이 미래가 왠지 불안하게 느껴진다는 감상
          + 원래 인간이 만든 음성 기록도 정확 보장 없었음. 이런 변환 과정엔 늘 오류가 존재했고, 앞으로도 기대치에 포함. 오히려 더 걱정되는 건 생성형 AI가 사실인 양 해석하거나, 'AI가 더 믿을 만하다'는 사회적 관념 자체임. 인간, 전문가, 기자보다 AI가 더 신뢰성/공정성을 갖췄다는 대중적 생각도 위험함
     * Gemini 2.0 전 버전에서는 이미지 한 장에 258토큰 요금을 고정으로 받던 방식이 있었는데, 이미지에 훨씬 더 많은 텍스트를 우겨넣으면 그만큼 저렴하게 처리 가능했던 트릭도 있었음
     * Chrome 확장 프로그램을 만들었는데, huggingface/transformers.js에서 OpenAI Whisper 모델을 WebGPU로 돌려 브라우저에서 바로 오디오를 텍스트로 변환 가능. 예시 리스트 참고. 예컨대, 대통령 소셜미디어의 영상을 듣거나 보긴 싫지만, 경제에 큰 영향을 미치는 망언이 등장할 때는 빠르게 감지해야 하므로, 1분마다 새 포스트를 크롤링해 OCR과 오디오 트랜스크립션을 로컬에서 자동 처리하고, 텍스트 분석까지 수행, 경제적으로 중요할 때만 알림. 프로젝트 링크
          + 놀라운 구현이라는 평
     * OpenAI Whisper API 대신 Groq(저렴하게 distil-large-v3가 시간당 $0.02, whisper-large-v3-turbo가 $0.04, OpenAI는 $0.36/hr)도 추천. 내부적으로 시의회 회의가 유튜브에 올라오면 자동으로 Groq, Replicate, Deepgram 등을 활용해 트랜스크립션 처리
          + Hugging Face의 Inference API를 사용하면 여러 API 제공업체를 한 번에 쉽게 바꿀 수 있어서 편하다는 팁. 예시는 여기서 직접 확인 가능
          + 시간당 $0.02~$0.04 단가라서 별다른 최적화 필요 없을 듯하지만, 오디오를 더 빠르게 돌려서 비용을 더 줄일 수 있지 않을까 하는 궁금증. 유튜브가 이미 대부분 하루 이내에 자동 자막 기능 제공한다는 궁금증도 동반
          + 최신 맥북 유저라면 Whisper 모델을 완전히 무료로 로컬에서 돌릴 수 있다는 사실을 강조. 실제로 본인 소유 하드웨어의 컴퓨팅 자원이 이미 굉장히 저렴하다는 점을 잘 모른다는 느낌
          + cloudflare workers ai에서도 whisper-large-v3-turbo 모델을 시간당 약 $0.03에 사용할 수 있는 옵션도 안내 (링크)
     * Google AI studio에서 유튜브 링크만 던지면 자동으로 speaker label 포함 트랜스크립션, 시각적 단서까지 추출해 주는 기능 강조. 비디오에 멀티모달 지원도 언급
     * 나는 OpenAI에서 API 관련 업무를 하고 있는데, 2~3x 빠른 속도에도 결과가 꽤 괜찮게 나와 놀라움. 실제로 전화 채널용으로는 8khz 오디오를 24khz로 업샘플링해서 잘 사용함. 다만 1x에서 멀어질수록 정확도 저하가 분명히 존재한다는 점, 장기적으론 더 긴 파일 업로드 지원이 필요함
          + 내부적으로 이런 속도 최적화를 연구해 정확도 손실이 최소인 배수 포인트를 찾으면 좋겠다는 피드백. 간단한 전처리만으로 API 가격을 낮추는 효과 가능성도 시사
     * 바로 본론으로 들어가는 글쓰기 스타일이 마음에 든다는 의견. 많은 글이 괜히 장황해지는데, 이런 접근법이 참신함. 절반의 저자들은 사실상 핵심 메시지 자체가 없다는 사실도 깨달을 것 같음
"
"https://news.hada.io/topic?id=21686","Ask GN: Cursor 지연 시간이 굉장히 높지 않나요? ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Ask GN: Cursor 지연 시간이 굉장히 높지 않나요?

   내장된 네트워크 검사 도구를 써보면 chat이나 agent는 700-1000ms 정도 나오고, 체감되는 속도도 정말 답답합니다.

   일본에 서버가 있다고 하는데 검사를 해 보면 그냥 미국 서버로 바로 갑니다.

   커서 측에 문의를 해도 AI 답변만 오고 사람에게 넘긴다고 하는데 깜깜 무소식 이구요.

   VPN도 써보고 이것저것 다 해 봤는데, 답이 없네요.

   다른 분들은 괜찮으신가요?

   유료 사용 중인데 지난 주에는 꽤 느리더라고요. 이번 주는 그나마 빨라진 것 같아요

   저도 그런 경험이 있습니다.
   최신 모델로 한 경우에 종종 있었고..
   그런 경우에는 모델를 변경하니 속도가 빠릅니다.
   추측으로는 토큰 제한이 내부적으로 있지 않나 싶습니다.

   최근 너무 느려져서 vs code copilot으로 돌아왔어요.

   cursor 쪽의 동작을 보면 요청을 cursor 서버로 보내고 cursor 서버에서 AI Api를 호출하는 방식이라서. cursor 서버에 병목이 생길 수 밖에 없는 구조입니다. 유료는 그래도 쓸만합니다.

   전 사용하는 시간에 따라 조금 다릅니다. 오전엔 좀 느리고.. 오후엔 좀 더 수월하고 그러더라고요..

   프로 구독 상태에서 확장 기능이 호환이 안되서 메일로 문의했는데 AI 답변만 이틀을 왔다갔다 하다가 사람에게 넘긴다고 말한 이후부터 답이 없더군요.

   한 2주 되도 답이 없고...메일 다시 보내니 이제 답변도 없고...그냥 한달 쓰고 해지했습니다

   전 속도 탓은 아니지만 제 돈 주고는 쓰기 힘들 것 같아요

   뉴비입니다. 너무 느리던데 원래 그런줄 알았습니다
"
"https://news.hada.io/topic?id=21606",""평범한" 엔지니어를 찬양하며","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ""평범한"" 엔지니어를 찬양하며

     * “10x 엔지니어”는 직관적으로 그럴듯하지만, 실제 생산성을 객관적으로 측정하는 것은 매우 어렵고, 불변의 개인 특성으로 보는 것도 잘못된 접근임
     * 소프트웨어의 실질적 소유권과 결과물은 엔지니어링 팀 단위의 협업과 역량에 의해 결정됨
     * 정말 뛰어난 조직은 특별히 탁월한 사람만이 아니라, 평범한 엔지니어가 꾸준히 좋은 결과를 내는 환경을 만드는 곳임
     * 시스템은 평범한 사람이 실수하거나 지치기도 한다는 점을 고려해서 설계해야 하며, “10x 팀” 을 만드는 데 집중해야 함
          + 소프트웨어 시스템 설계와 문화는 “평범한 사람” 의 특성, 다양성, 성장의 가능성을 기반으로 해야 함
     * 궁극적으로 생산성의 핵심 지표는 비즈니스 임팩트이며, “최고의 인재”가 아니라 “적합한 사람”을 채용하고 팀을 구성하는 것이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“평범한 엔지니어”를 찬양하며

     * 이 글은 “10x 엔지니어”라는 개념에 대한 비판과, 평범한 엔지니어들이 탁월한 팀 성과를 내는 조직의 중요성을 설명함

“10x 엔지니어” 신화와 그 한계

     * 누구나 커리어에서 마치 마법사처럼 비범한 엔지니어를 만난 경험이 있음
     * 이로 인해 “10x 엔지니어”라는 개념이 생겨났으나, 이는 근거가 빈약하고, 편견을 강화하기도 함
     * 생산성의 단일 척도를 정하는 것은 불가능에 가까움
          + 사용하는 기술 스택, 도메인, 개발 단계, 시장 상황, 경험 등 변수의 조합이 무한함
          + 한 사람이 특정 분야에서는 10x 엔지니어일 수 있지만, 대부분의 영역에서는 평범하거나 평균 이하일 수 있음
     * 한때 뛰어난 DBRE였어도, 시간이 지나면 해당 분야에서 평범한 수준이 될 수 있음
     * 결국, 누구도 모든 분야에서 항상 10배 뛰어날 수 없음

팀 중심의 소프트웨어 소유권

     * 소프트웨어는 팀이 소유하고 관리하는 것이지, 개인이 담당하는 것이 아님
     * 소프트웨어의 전달과 유지보수, 개선, 확장, 리팩터링 등 모든 과정은 팀 단위로 이루어짐
     * 한 명이 소유하는 시스템은 그 사람이 휴가, 이직 등으로 자리를 비우면 단일 장애 지점(SPOF) 이 되어 조직의 취약점으로 작용함
     * 스타트업에서 초기에는 개인 소유가 불가피할 수 있으나, 조직이 성장하면 반드시 팀 소유권으로 전환해야 함
     * 엔지니어링 리더의 핵심 임무는 고성능 팀의 조성에 집중해야 하며, “10x 엔지니어”보다 10x 팀을 만드는 것이 중요함

진정한 고성과 조직의 조건

     * 많은 기업이 FAANG 출신, 명문대, Staff Engineer 위주로 팀을 꾸리는 것을 선호함
     * 그러나 진짜 좋은 조직은, 평범한 엔지니어가 안정적으로 실질적 임팩트를 낼 수 있는 곳임
     * “최고”만이 성과를 낼 수 있는 조직이 아니라, 일반적인 개발자도 주도적으로 성장하고, 제품과 비즈니스에 긍정적 영향을 주는 시스템을 만드는 것이 더 큰 경쟁력임
     * 오히려 이런 조직이 더 많은 월드클래스 엔지니어를 길러냄

“평범함”의 의미 재정의

     * 소프트웨어 업계는 “상위 10%”, “top .1%” 등 엘리트 중심 사고가 만연함
     * 하지만 대다수 엔지니어는 다양한 경험과 연습을 통해 성장한, 평범한 사람임을 인정하는 것이 중요함
     * 아무도 선천적으로 “뛰어난 엔지니어”로 태어나지 않음
     * 위대한 엔지니어는 만들어지는 것임 - 누구나 학습과 경험을 통해 훌륭해질 수 있음

평범한 사람을 위한 시스템 설계

     * 채용 시에는 특출난 강점을 보는 것이 맞지만, 시스템을 설계할 때는 사람들이 평범하게 실수하고, 지치고, 익숙함에 기대는 현실을 고려해야 함
     * 일반적인 엔지니어는 인지 편향, 피로, 감정 기복 등에 영향을 받음
     * 시스템이 똑똑한 소수가 아니라 평범한 엔지니어 관점에 맞추어 설계되어야, 뛰어난 인재의 창의적 역량이 제품 자체에 집중될 수 있음

“10x 팀”을 만드는 방법

     * 코드 작성과 배포 사이의 간격을 최대한 줄임
          + 빠른 배포 주기는 인지 부담을 줄이고, 더 빠른 실험과 피드백을 가능하게 함
          + 배포가 느릴수록 여러 변경이 한꺼번에 모여, 문제 추적과 롤백이 어려워짐
     * 실수 복구와 롤백을 쉽게 만듦
          + 개발자가 스스로 코드를 배포하고, 문제를 발견하면 신속하게 복구할 수 있어야 함
     * 옳은 행동을 쉽게, 잘못된 행동을 어렵게 만듦
          + 디자이너, 플랫폼 엔지니어와 협력해, 가드레일과 셀프서비스 체계를 구축
     * 관측성과 계측 도구에 적극 투자
          + 실제 코드 동작을 시각화하여, 모든 엔지니어가 쉽게 시스템을 이해하고 디버깅할 수 있도록 지원
     * 내부 도구와 생산성 향상에 엔지니어링 리소스 투자
          + 이런 시스템은 별도의 오너십이 필요하고, 전사적 우선순위가 되어야 함
     * 포용적 문화 조성
          + 누구나 질문, 실수, 탐색이 가능한 환경
          + 다양한 배경, 스킬, 연차가 어우러진 팀이 더 회복력 있고, 변화에 강함
     * 모든 레벨의 엔지니어가 섞인 팀 구성
          + 주니어부터 시니어까지 서로 배우고 가르치며 함께 성장하는 구조
          + 이런 시스템은 신입 온보딩, 팀 간 이동 등에도 재활용 가능

생산성의 진짜 척도: ""비즈니스 임팩트""

     * 궁극적으로 소프트웨어 엔지니어링의 생산성 척도는 비즈니스 임팩트임
     * 많은 코드를 작성하는 것이 아니라, 문제를 해결하고 가치를 창출하는 것이 본질임
     * 실제로 산업을 움직이는 주역은 Staff+ 엔지니어가 아니라, 시니어와 미드레벨 엔지니어임
          + 이들이 조직의 기반을 이루고, 지속적으로 비즈니스를 전진시킴
          + Staff+ 레벨만이 임팩트를 낼 수 있다면, 조직에 문제가 있다는 신호

월드클래스 엔지니어를 키우는 조직

     * 뛰어난 엔지니어가 아니어도, 누구나 임팩트 낼 수 있는 조직이 진짜 좋은 조직임
     * 최고의 조직은 반드시 세계 최고 수준의 인재를 따로 뽑지 않아도, 자연스럽게 월드클래스 엔지니어가 가장 많이 탄생함
     * 엔지니어가 문제 해결과 성장, 임팩트에 몰입할 수 있는 곳에 인재가 모이고, “행복하게 일하고 성장하는 경험” 자체가 선순환을 이끔
     * 리더는 뛰어난 인재의 개인 역량에 의존하지 않고, 이를 조직 전체의 성장과 고객 가치로 연결하는 역할을 해야 함

“최고의 인재”보다 “적합한 사람” 채용

     * 업계는 “최고”만 찾으려 하지만, 진짜 중요한 건 시스템과 환경임
     * “최고의 인재”보다, 팀과 조직에 적합한 사람을 찾는 것이 더 큰 경쟁력임
     * 약점이 없는 사람보다, 특유의 강점을 지닌 조직의 다양성과 조화를 증진할 “적합한 인재” 로 팀을 구성하는 것이 효율적임
     * 포용적이고, 다양한 팀이 실질적으로 성과와 성장, 장기적 성공을 이끌어냄
     * 모두가 일을 즐기고, 성장하며, 의미 있는 결과를 내는 곳이 진짜 세계 최고 조직이며, 실패나 실수에도 배울 수 있는 문화에서 엔지니어와 조직이 모두 성장함
     * 이런 조직이야말로 차세대 엔지니어도 끌어들이고, 이미 뛰어난 인재가 머물고 싶어하는 환경임

        Hacker News 의견

     * 소프트웨어 소유권과 딜리버리의 최소 단위를 엔지니어링 팀이라고 한 주장에 공감은 하지만, 조금은 아쉽게 느끼는 입장임. 이런 구조는 엔지니어가 매니저나 프로덕트 부서에 비해 이등 시민이 되는 문화와 맞닿아 있음. 우리는 배달(deliery)에만 책임이 있을 뿐, 진짜 의미 있는 결정을 팀의 아주 작은 범위 이상에서는 독립적으로 내릴 수 없음. 최고의 팀에서는 재량의 기간이 수개월, 심지어 수년에 이르지만, 대부분 팀에서는 며칠, 몇 주 수준으로 줄어듦. 하지만 실제로는 엔지니어가 실질적 소유권을 갖고 시스템이 깨지거나 위험해지는 것 없이 오래 갈 수 있는 구조가 충분히 가능함. 비결은 여유(slack)를 확보하고, 양질의 작업을 장려하며, 구성원 누구든 유연하게 업무를 바꿀 수 있도록 충분한 자유를 주는 것임. 각자 소유권을 갖고 장기적 결정을
       내리면서도, ad hoc으로 협업하고 묵시적 지식을 공유하여, 누가 갑자기 들어와서 도움을 주거나 아예 이어받는 시나리오도 자연스럽게 가능해짐. 실제 경험상 이런 팀은 일반 팀보다 리라이트(재작성)는 많지만, 전반적으로 훨씬 더 생산적임. 시스템을 작은 덩어리로 점진적으로 리라이트하는 것이 디자인 진화와 조직 내 지식·역량 축적 양쪽 모두에 아주 효과적임. 겉으로는 낭비처럼 보여도, 조직 전체가 유연하고 회복력 있게 만드는 데 필수적인 여유임. 오히려 소위 낭비라고 줄이려는 톱다운 조직의 많은 프로세스가 실제로는 중요한 ‘슬랙’을 없애고 있다는 확신이 점점 커지는 입장임
          + 엔지니어링 외부 사람들은 엔지니어가 내린 장기적 결정을 바로 평가할 수 없고, 어떤 보상을 줘야 할지 모르는 경우가 많음. 이건 심지어 같은 엔지니어팀을 벗어나도 마찬가지임. 내부 가치를 가진 장기 작업에 대해, 그 가치가 실제로 있는지 아닌지 직접 평가하는 능력이 없기 때문임. 본인이 장기 의사결정과 슬랙 타임 사용에 자신 있다면, 배달(delivery)에 대한 보상을 받아도 괜찮음. 언젠가는 그 장기적 작업이 더 나은 결과로 돌아올 것이기 때문임
          + 소프트웨어 리라이트가 개발 과정에서 중요한 역할을 한다고 생각함. 진정한 ‘애자일’이란, 첫 아키텍처에 너무 신경 쓰지 않고 빠르게 프로토타입을 만들어 요구사항 변화에 유연하게 대응하는 방식임. 코딩은 글쓰기와 비슷해서, 첫 초안은 거칠더라도 일단 써 나가고 두 번째 버전에서 개선하는 게 훨씬 효율적임. 첫 초안은 좋은 걸 목표로 하는 게 아니라, 빠르게 뭔가를 만들어보고 문제영역을 탐색해 엣지케이스를 파악하는 데 목적이 있음. 이런 작업 방식은 경영진에게는 잘 통하지 않음. 작동하는 프로토타입을 보이는 순간 ‘이거 바로 출시하자’고 할 뿐, 리라이트할 시간을 주지 않음. 해법이라면, 조직의 계층을 납작하게 만들고, 엔지니어에게 코드 소유권을 실제로 돌려주는 것이 좋겠음. 엔지니어와 프로덕트 오너가 함께 민주적으로
            의사결정을 하는 구조가 바람직함
          + 나도 한때는 ‘개인 소유권’을 중요하게 생각했고, 지금도 엔지니어가 소유권을 가질 수 있다고 보지만, 실제로 많은 엔지니어가 그걸 원하지 않는다는 사실도 인정함. 대부분 엔지니어는 팀 단위라면 괜찮아하지만, 개별 엔지니어 수준의 소유권에는 그다지 관심이 없음. 그냥 생업일 뿐임. 이를 개인에 맡기면 팀 내 불신을 낳거나, 리더십 성향이 없는 멤버를 소외시키게 되고, 결국 아무도 실질적 재량이 없는 사태가 벌어지기 쉬움. 팀 단위로 소유권과 재량을 주는 구조가 훨씬 간단하고 효과적임. 팀 리더 혹은 매니저가 있어서 가능한 역동성이기도 함. 개인별로 소프트웨어 소유권을 두면 인원 변화, 안정성, 커리어 등의 변수로 실패 위험이 너무 많아짐. 조직이 아무리 건강해도 크고 작은 문제는 있기 마련이고, 이런 구조에서는 실패로 이어지는
            경로가 가장 짧아짐. 기어박스를 생각하면 이해하기 쉬움. 기어가 하나밖에 없고 고장나면 못 가지만, 여러 기어가 있으면 불편하더라도 당장 고장이 나도 계속 갈 수 있음
          + 진짜로 실질적 개인 소유권이 가능하다고 생각함. 오히려 유일하게 정말 ‘생산적’일 수 있는 방법이라고 봄. 하지만 이게 본질적 쟁점은 아님. 개개인은 대체 불가능하지만, 팀원은 구조에 따라 어느 정도 대체 가능함. 조직이 커질수록 팀 단위의 예측 가능성을 원하게 되고, 이를 위해서는 멤버 대체가능성 확보(즉, redundancy)가 필요함. 엔지니어링에서도 신뢰성(회복력)을 위해 redundancy를 두고, 효율성은 redundancy를 줄이는 트레이드오프와 같음
          + 우리는 그저 '딜리버리'에만 책임이 있다는 식의 구조에서 일하면서, 소유권이란 결국 부담만 늘릴 뿐 실질적 보상은 없었음. 일은 페이지에 기능만 붙이는 일로 국한됨. 추가 책임이 생긴다면 이에 대한 추가 보상도 마땅함. 매니저나 임원은 책임 인원수에 따라 보상이 커지는데, 개발자도 마찬가지여야 함
     * 가장 좋은 팀은 평범한 엔지니어가 엄청난 성과를 내게 하는 문화에 있다고 확신함. 세간에서 말하는 ‘엔지니어링 퀄리티’나 인재 채용보다도, 문화와 신뢰, 시스템, 프로세스가 훨씬 더 중요함. “누구나 최고의 엔지니어가 있는 조직을 만들 수 있다”는 말이 있는데, 실제로 그런 팀을 만들어낸 조직은 정말 소수임. 거의 모두 트레이딩 회사나 특수/연구조직임. 도대체 다들 왜 못하는 건지 의문임. 결국 ""생산성""이란 개념부터가 무엇을 의미하는지도 애매함. 단순히 조직 내 dysfunction을 견디고 돌파할 능력을 보는 평가 시스템도 있는데, 그게 진정한 ‘톱 엔지니어’의 의미는 아니라고 봄
          + 진짜로 뛰어난 엔지니어 공급은 제한적이라, 결국 더 큰 회사가 더 재밌는 일이나 더 높은 급여를 내걸고 같은 인재를 데려가게 됨
          + 다른 사람들이 말한 돈 문제도 크지만, ‘시간’도 매우 큼. 회사가 완벽한 “유니콘” 인재가 나타날 때까지 기다릴余유가 없고, 바로 채용할 수 있는 인재로 급하게 채울 수밖에 없기도 함. 일부 회사, 특히 퀀트 파이낸스 같은 분야에선, 시스템 프로그래머·수학자·금융시장 전문가 3인을 모두 아우르는 한 명의 슈퍼 엔지니어가 세 명의 전문가 팀보다 훨씬 큰 가치를 내기도 함. 하지만 그런 사람 찾기엔 시간이 너무 큼. 웹 개발만 봐도, 네트워크 프로토콜부터 시스템 어드민, 분산 시스템, 데이터베이스, 프론트엔드까지 정말 전방위로 두루 이해하는 ‘진짜 풀스택’ 개발자는 극소수임. 충분한 시간과 비용이 있는 조직만이 이런 인재를 모아서 최고의 제품을 만들 수 있음. 물론 제품이 정말로 그 정도의 품질이 필요한가는 별개의 질문임
          + 근본적으로 전 세계의 “최고 엔지니어” 공급량은 극히 제한적임. 상위 10% 급여를 줄 수 있고, 그만큼의 인재를 모으고 유지할 수 있다면야 성공임. 사실 ‘아무나’ 만들 수 있는 건 아니고, 정말로 경영진이 학습과 성장에 집중하는 소시오테크니컬 시스템을 설계하는 리더십이 필요함. 그 자체로도 훌륭한 일임
          + 가장 큰 문제는 대부분의 1~2선 경영진이 그다지 뛰어나지 못하다는 것임. 생산적 환경을 만들고 유지하는 능력이 부족함. 근본적 해결책은 결국 ‘돈’을 많이 주는 것임. 돈이 크면 대부분의 힘든 조건도 감수할 수 있음
          + 예산 문제를 떠나, 정말 뛰어난 엔지니어가 팀플레이엔 오히려 좋지 않을 수도 있음. 뛰어난 두뇌가 때론 필수적인 타협이나 지루하지만 필요한 작업에는 오히려 걸림돌이 될 수 있음
     * “비즈니스 임팩트만이 유일한 생산성 척도”라는 주장에 크게 동의할 수 없음. 이런 관점은 측정 가능한 변화와 단기 성과로 시야가 옮겨지고, 숙련된 엔지니어의 진정한 가치를 놓치게 됨. 진짜 실력자는 프로젝트나 회사를 망칠 뻔한 위험(landmine)을 미리 막는 데 있음. 하지만 이런 ‘없어진 리스크’는 측정하기 어렵고, 평범하게 들리게 전달하는 것도 불가능에 가까움
          + “임팩트”는 꼭 단기적인 관점이 아님. 조직에 최대 임팩트를 주는 사람은 장기적 관점에서 움직임
          + 나는 ‘생산성’이나 ‘임팩트’를 일부러 모호한 표현으로 얘기함. 예를 들면 “전반적으로 생각하면 X는 일을 정말 잘 했다”처럼. 이런 게 수치화도, 명확한 규정도 어렵지만, 원래 복잡하고 창의적인 조직에는 인간의 통찰과 판단이 더 중요함. 경영을 무조건 수치화하려드는 건 근본적으로 근시안적임
          + 프로젝트 관리나 위험 회피 등만으로 엔지니어의 가치를 측정하는 건 동의하지 않지만, 모든 걸 ‘비즈니스 임팩트’로만 환원하는 건 불편함. 세상에는 돈과 무관하게 개인, 인류, 세계에 의미 있는 일이 많음. 내가 가장 존경하는 엔지니어는 포스트-2001 실리콘밸리의 신화적인 인물보다, 존 폰 노이만, 로버트 오펜하이머, 니콜라 테슬라, 레오나르도 다빈치, 로마 수로와 이집트 피라미드를 만든 누군가, 바빌로니아-메소아메리카의 천문학자 등임. 그들이 비즈니스 임팩트를 남겼는가? 테슬라가 한때 Westinghouse에 이익을 줬다고 해도 그게 그의 업적을 설명하지는 못함. 사실 그는 비즈니스면에서는 평범함. 최신 컴퓨팅 산업의 거장도 마찬가지. Nvidia나 Geoff Hinton이 엄청난 성공을 거둔 건 인터넷이 광고화되며 데이터가 폭증한 ‘운’도 한몫한 것임.
            진짜 실력자가 소외된 채 사라진 빈 사례도 많음. Doug Lenat처럼 AI계의 대가도 결국 역사의 흐름에 따라 발굴되지 못한 경우도 있음. 성공 여부는 많은 경우 개인의 노력과도 무관함
          + 효율적인 시스템을 만들거나, 재난 가능성을 최소화하는 시스템을 만들거나 양자택일임. 실제로 어떤 재앙이 막혔는지는 증명하기도 어렵고, 부정적인 영향 자체가 ‘발생하지 않은 사건’이기에 결과로 보여주기도 힘듦
          + 회사는 ‘알려지지 않은 것’의 위험을 어떻게든 더 잘 정량화하려고 노력해야 함
     * 운 좋게도 지금까지 여러 훌륭한 팀에서 일했고, 현재도 그런 팀을 이끌고 있음. 기사와 달리, 이런 팀일수록 조직이 관리하기가 더 어려운 경우가 많음. 대형 조직은 표준화에 초점을 맞추다 보니, 오히려 뛰어난 엔지니어가 역량을 펼치지 못하고 동기부여도 잃는 경우가 많음. 모든 사람을 뛰어난 엔지니어로 키우지 못한다는 비관적 관점에 동의하지 않음. 실제로 꾸준히 투자하면 훌륭한 엔지니어로 키울 수 있고, 장기적으로 뛰어난 인재 풀이 많아질 때 얻는 이득이 그만큼의 투자 비용을 충분히 뛰어넘는다고 생각함
     * 효과적으로 측정할 수 없다고 해서, 그 무엇 자체가 존재하지 않는 것은 아님. 개별 생산성, 즉 개인의 업무 성과는 분명히 있음. 아마 그룹 단위 생산성이 측정하기 더 쉬울 뿐임. “비즈니스 임팩트”는 오히려 훨씬 더 임의적이고, 그런 척도로 보면 정말 생산적인 인재를 남기기 힘듦. 전문 지식의 평가는 동등한 경험이 없는 한 매우 어렵고, 조언은 할 수 있지만, 상대가 그걸 받아들일 지적인 여유가 있는지는 별개임. 내가 천재인지, 자신감만 앞선 사람인지 어떻게 알 수 있겠는가
          + 문제는 생산성을 ‘측정’할 수 없음이 아니라, 심지어 추상적으로 ‘정의’조차 못 한다는 것임. 단순히 “replacement”보다 얼마나 더 기여했는지 측정해도 그 결과가 정확히 어떻게 나온 건지 설명해주지는 못함. 실제로는 개인의 영향력이 맥락과 조직 전체에 의해 결정됨. 더 직접적인 정의를 내려해도 정답은 조직·상황마다 천차만별임. 이는 고민해볼 가치가 충분하지만, ‘Top 1%’ 엔지니어란 기준 자체가 정말로 의미가 있는지도 자체도 의문임
          + 진짜 천재는 자신의 결과를 에티켓을 지키며 쉽게 설명할 수 있음. 그래서 차이는 충분히 구분 가능하다고 생각함
     * “정상적인 엔지니어가 위대한 업무를 할 수 있게 만들어주는 조직이 최고”라는 글귀가 마음에 듦. 모든 팀원이 다 슈퍼스타일 필요는 없음. 평균적인 개발자가 충분히 좋은, 신뢰할 만한 실력을 펼칠 수 있게 만드는 지원이 얼마나 잘 되는지가 정말 중요함
          + 모든 뛰어난 엔지니어도 처음에는 그저 좋은 엔지니어였음. 건강한 조직은 구성원이 이 성장 경로를 밟을 수 있게 돕는 역할을 함
     * “옳은 것을 쉽게, 그른 것을 어렵게 만든다”는 원칙이 내가 만난 모든 플랫폼팀의 핵심 슬로건과 같음. 목표는 엔지니어가 마주치는 문제에 대해 ‘정답’이 자동으로 눈에 띄고, 쉽게 구현할 수 있도록 시스템을 설계하는 것임. 신뢰성과 관리 용이성을 갖춰, 자연스레 그런 방향으로 개발 ‘근육’을 길러주면 조직 전체가 이득을 봄. 이 진리는 앞으로도 계속 유효함
          + 우연히도 Charity Majors가 이에 관한 훌륭한 에세이를 쓴 적이 있음. 신뢰할 수 있는 시니어 엔지니어들로 소규모 위원회를 구성해서, 새 서비스에 쓸 기본 컴포넌트 추천 리스트를 만드는 것부터 시작함. 이게 바로 ‘골든 패스’가 됨. 조직은 골든 패스 컴포넌트만 완전 지원하며, 업그레이드·패치·백업·배포·환경·온콜 등 전반을 금과 같이 깔아줌. 꼭 사용하지 않아도 되지만, 골든 패스 밖의 선택은 본인이 모든 걸 다 책임져야 함
     * golang, python, COBOL, lisp, perl, React, brainfuck 등 다양한 언어/프레임워크가 언급될 때마다, React를 프론트엔드 생태계 전체로 착각하는 경향이 있다는 느낌을 오래 받아왔음. 실제로는 React 외에도 다양한 프론트엔드 세계가 존재함에도, 다들 React가 전부라고 생각하는 듯함
     * 우리 회사에선 항상 태도 좋은 평균 엔지니어 채용을 선호함. 자격만 좋은데 태도 나쁜 사람은 평판형성을 잘해서 윗선의 신임을 쉽게 얻고, 이후엔 무소불위가 됨. 이런 사람이 자리 잡으면, 주위는 억울해도 문제를 제기하기가 힘들어짐. 윗선도 자신의 인식에 맞지 않는 데이터를 쉽게 무시함. 객관적 데이터보다 인식에 기대는 게 훨씬 쉽기 때문임
     * “누구나 최고의 엔지니어와 함께하는 조직을 만들 수 있고, 개별 능력 중심은 리더의 실질적 역할을 흐리게 만든다. 덜 숙련된 엔지니어가 역량을 쏟아 결과를 만들 수 있도록 시스템을 만드는 게 엄청난 경쟁우위”라는 주장에 정말 공감함. 난 10x 엔지니어는 아니지만, 최근 경험에 비추어 보자면, 팀에 경험치·실력이 적은 사람이 많을 때 나만이 복잡한 티켓을 도맡게 됐음. 이런 패턴이 반복되면 나 혼자 대부분의 일처리를 하게 되고, 실제로 그 역할이 고된 데다 공정하지 않게 느껴짐. 솔직히 좋은 SRE라면 약간 게으른 성향도 있어야 한다 생각하기에, 팀원들이 일을 분담해줬으면 하는 바람이 큼. 그래서 몇 주 동안 하드하게 일해서, 가장 복잡한 부분에 다양한 추상화를 도입했음(나는 IAC를 주로 다룸; 소프트웨어에서도 비슷한 패턴임). 그러자
       상대적으로 실력이나 경험이 부족한 엔지니어도 내 역할을 나눠 맡게 됐고, 내 시간은 더 흥미로운 문제에 쓸 수 있게 되었음. 누구의 지시도 없이 이런 시도를 한 건 처음임. 반대로 이런 구조 없이, 혼자서 히어로처럼 굴게 되면, 팀 전원이 뒤에서 따라다니며 실수를 수습하느라 골머리를 앓게 되고, 결국 정말 비효율적인 팀이 되어버림
"
"https://news.hada.io/topic?id=21578","YouTube의 새로운 광고 차단 방지 대책","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        YouTube의 새로운 광고 차단 방지 대책

     * YouTube는 최근 광고 차단 방지 실험을 강화하여 일부 이용자에게 페이크 버퍼링 현상이 나타남
     * 이 페이크 버퍼링은 광고 길이의 약 80%만큼 동영상 시작을 지연시키는 방식으로 작동
     * 사용자는 광고 차단 필터에 특정 설정을 추가하면 이 효과를 일부 우회할 수 있음
     * YouTube는 글로벌 캐시 및 SABR 프로토콜 등 다양한 기술로 광고 차단을 감지 및 대응함
     * 일부 JavaScript 잠금 스크립트로 인해 브라우저, 필터 및 확장프로그램의 동작 제한 현상도 발생
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 최근 YouTube가 광고 차단 방지 기능을 재차 도입하여 실험 중임
     * A/B 테스트를 통해 일부 계정에만 실험적 기능이 적용되고 있음
     * 사용자는 uBlock Origin이나 Brave 브라우저에서 이들 변화를 피할 수 있는 필터를 사용할 수 있음(기본 필터에 이미 포함)

페이크 버퍼링 현상

     * 주요 현상은 동영상 시작 시 길어진 버퍼링임(중간에는 해당 없음)
     * 이 페이크 버퍼링 길이는 원래 광고를 봐야 했던 시간의 80% 수준임
     * 예를 들어 광고가 15초라면, 사용자는 12초 정도 버퍼링을 겪게 됨
     * 여러 광고가 연속으로 있는 경우 각 길이의 80%를 합친 시간만큼 대기 효과가 발생함
     * 광고 차단 사용자는 여전히 시간 이득을 보게 됨

내부 구조 및 작동 원리

     * InnerTube는 YouTube의 공식 내부 API로, 웹·모바일 클라이언트가 동영상과 관련 정보 접근에 사용함
     * 비디오 스트림은 GVS(Google Video Services) 를 통해 전달되며 링크는 시간이 만료되어 재생성이 필요함
     * 유튜브는 네트워크 부하 최적화를 위해 Google Global Cache 서버도 활용함
     * 과거에는 단순히 쿼리 매개변수로 비디오 범위를 설정했으나, 최근에는 SABR(Server ABR) 라는 독자적 바이너리 프로토콜을 사용함
     * SABR는 필요 시 backoff(지연 명령) 를 보내, 클라이언트가 일정 시간 대기하게 유도함

페이크 버퍼링의 실제 발생 원인

     * 광고 차단 탐지 시, InnerTube는 GVS 스트림에 광고 길이의 80%만큼 backoff를 포함시킴
     * 이 backoff는 광고 차단 여부와 관계 없이 A/B 테스트 그룹에 속할 경우 항상 적용됨
     * 광고가 차단되지 않은 사용자는 백엔드에서 content video를 미리 로드하여 이런 지연을 체감하지 않음
     * ""Experiencing interruptions"" 등의 알림은 장기간 backoff로 인해 표시되는 현상임
     * 웹상에 떠도는 ""CPU 과부하 이슈"" 등은 사실 아님

광고를 원천 차단하는 방법

     * 광고 스트림 자체를 받지 않기 위해 isInlinePlaybackNoAd 속성을 true로 설정할 수 있음
     * JSON 요청에 ""isInlinePlaybackNoAd"":true 항목이 들어가면, InnerTube는 광고와 backoff를 포함하지 않음
     * uBlock Origin 사용자 필터에 관련 규칙을 삽입하여 자동 적용 가능
     * 이 속성은 프론트엔드 JavaScript 및 Protocol Buffers 정의에서 확인 가능함

한계와 부작용

     * 해당 방식은 이미 로딩된 YouTube 싱글 페이지 앱 내에서의 내비게이션(따뜻한 로딩) 에서만 우회 가능함
     * 최초 진입(콜드 로딩)시엔 백엔드에서 제공된 초기 데이터로 인해 적용 불가
     * 강제로 초기 데이터를 제거하는 시도는 라이브 스트림, 페이지 속도, 사용자 경험 등 여러 문제점을 유발함

잠금 스크립트(Locker Script)의 차단 우회

     * YouTube는 일부 실험 집단에 `` 태그 상단에 글로벌 객체들을 수정 불가 상태로 만드는 잠금 스크립트를 삽입함
     * uBlock Origin 등 확장은 해당 스크립트가 실행되기 전에만 객체 후킹을 할 수 있음
     * Firefox는 HTML 필터링으로 이를 우회 가능하지만, Chromium 브라우저는 이 방식을 지원하지 않음
     * 이에 대응해 Object.assign 후킹 방식으로 필터 우회 가능
     * 이 방법을 이용할 경우 고급 Proxy 기능을 사용하여 ""isInlinePlaybackNoAd"":true 삽입이 가능함

        Hacker News 의견

     * 최근에 YouTube를 완전히 끊은 경험 공유, 예전엔 교육적/과학적 우수 콘텐츠를 하루에 몇 시간씩 볼 정도로 많이 보던 사용자였지만, 실제로는 필요하지 않았음을 깨달은 느낌, 덤으로 광고 없이 깨끗해진다는 점 언급, 누군가가 유튜브 영상을 주면 가끔 보긴 하지만 일부러 접속하지 않는 생활, 모두에게 YouTube 같은 수동적 엔터테인먼트 소비를 줄이고 창의적이고 의식적인 활동으로 시간을 돌려보길 강력 추천
          + 그냥 결제해서 쓰는 방법도 있다는 얘기, 가족 모두가 구독 플랜을 써서 광고 없는 환경에서 지내는 중, 충분히 가성비 좋다고 느낌, 영상 없이도 오토바이 빌드 프로젝트를 끝낼 순 있겠지만 굳이 그럴 필요 있을까라는 생각
          + Netflix도 같은 방식으로 끊었고, Prime도 해지하면서 Amazon 전체 이용 중단, 물론 AWS만큼은 데이터 때문에 사실상 불가능이라 예외, YouTube는 광고 없는 플랜으로 사용 중이며 만약 광고 프리 약속을 어긴다면 YouTube도 영구정지 대상
          + YouTube 계정을 삭제하고 Patreon으로 이동, Patreon에서 구독하는 크리에이터 영상은 여전히 YouTube에서 알림 받으며 볼 수 있는데, 이 덕분에 훨씬 의도적이고 퀄리티 높은 콘텐츠 소비가 가능해짐
          + 대규모 인구가 이런 플랫폼에 중독돼 있는 현실 언급, 이것은 ‘담배 2.0’의 시대라는 비유
          + 최고의 과학 프로그램은 PBS와 Nebula 같은 플랫폼에도 많이 있다는 팁
     * 구글이 광고 강제를 계속하는 한, 우리가 노출되는 광고의 부적절함과 사기성 광고에 더 예민하게 주목하고 감시해야 한다는 의견, 특히 광고 차단만이 가족과 자신을 유해 광고로부터 지킬 방법이 될 때, 구글이 송출하는 광고 자체에 더 많은 책임을 요구해야 성립된다는 주장
          + 요즘 YouTube 광고 수준이 너무 낮고 부적절해서 경악, 경영진들이 KPI나 보너스, 승진만을 위해 기준치를 바닥에 두는 상황, 모바일에서 썸네일로 포르노 광고까지 본 경험이 있을 정도라 과거 Evony Online 광고보다도 더 심각, YouTube가 성장 정체에 빠진 징조로 보인다는 견해
          + TV 방송에서는 광고 시간이 일정하게 관리되는 반면 YouTube에는 형편없는 광고가 너무 많고 길게 나와서, 가끔씩은 배경 재생 중에 5초 스킵 가능한 광고를 넘겨도 30분, 심지어 더 긴 인포머셜이 끼어드는 상황, 이런 긴 광고가 의도치 않게 시청 흐름을 방해하는데도 허용한다는 것이 이상하고 거의 범죄적이라고 생각, 이런 광고를 유치하려면 유튜브도 엄청난 광고비를 받으리라 추정
          + 콘텐츠 제작과 플랫폼 구축에 드는 노력과 노동에는 정당한 보상이 있어야 한다는 원칙, 책, 영화, TV, 뉴스 등 모든 콘텐츠는 돈이나 광고 시청을 통해서든 대가를 치르는 구조라는 설명, 광고가 싫으면 유료 구독을 권유
          + 광고 차단 개발자를 예찬하는 ‘Thin Adblock Writer Line’ 깃발이 있다면 이미 차에 달아두었을 거라는 유쾌한 응원
          + 단순히 프리미엄 요금제 결제하면 된다는 제안, 누구도 강제로 유튜브를 쓰라고 하지 않는다는 메시지
     * Firefox에서 HTML 필터로 광고 차단이 쉽게 가능함을 언급, 단 이 방식은 Chromium에서 확장 API가 없어서 불가능하다는 사실에 놀랐다며 개발 환경 차이에 충격
          + Chrome이 “Manifest V3” 정책으로 uBlock Origin 같은 광고 차단 확장 기능을 못 쓰게 되는 순간, 대안 브라우저로 즉시 갈아탈 것이라는 결심
     * 최근 YouTube의 버퍼링 시간 지연을 자주 경험하고 있지만, 사실 크게 불편하지 않다는 입장, 광고가 거슬리는 이유는 시간이 아니라 원래 원하지 않은 오디오와 영상을 강제 노출하는 점, 12초 버퍼링 동안엔 그냥 이메일을 확인하거나 잠깐 멍 때리는 방식으로 자연스럽게 적응, 오히려 매번 버퍼링이 반복되면 뇌가 자동으로 무시하는 패턴으로 느껴짐, 오히려 5개 중 1개처럼 가끔이면 더 거슬릴 듯
          + 영상 재생 초기 딜레이와 함께 ‘재생이 느린 이유를 확인하세요’라는 팝업을 종종 보지만, 이미 이유를 알고 있어서 별로 신경 쓰지 않는다는 태도
     * ‘왜 우리는 광고 차단을 정당화할까? 콘텐츠에는 비용이 들고, 저장과 배포에도 돈이 드는데, 광고 차단에 도덕적 근거나 특권 의식을 느낄 이유가 있을까?’라는 근본적인 질문 제시, 단순히 광고가 불쾌해서라면 스스로 ‘왜 우리는 무료를 당연하게 여기는가?’를 되물어야 한다는 생각, 도덕적 판단이 아니고 순수히 이해하려고 하는 질문
          + 과거 구글 애드센스는 페이지당 광고 3개 제한이 있었고 위반 시 정지까지 됐지만, 지금은 한 웹페이지에 10개, 15개 이상 광고가 심어져 있음, YouTube도 초반 배너에서 시작해 영상 전부터 중간 광고, 자체 내의 스폰서드 구간, 프리미엄 구독자에게도 하단 광고 등이 범람, Google 검색마저 광고가 상단을 다 채워서 유기농 결과를 내리스크롤해야 나오는 정도, 무료 사용의 권리를 주장하고 싶진 않지만, ‘너무 지나쳤다’는 생각에서 모든 기기에서 애드블록을 쓴다는 설명
          + “왜 우리는 무료에 권리를 느끼는가?”라는 질문 자체가 현실 사용자들의 사고방식과 다름을 지적, 할 수 있으니 하고 싶어서 하는 것이고, 그걸 막을 윤리적 이유가 필요, 누군가가 광고를 꼭 봐야만 한다는 윤리적 의무는 없다고 주장, 오히려 광고는 인간의 주의력을 조작하고 이득을 취하는 본질적으로 조작적이고 사생활 침해적이라 원천적으로 강제할 수 없는 계약이라고 설명
          + Google이 사용자를 위하는 척하면서 실제로는 적대적이고 반경쟁적으로 변하고 있으니 광고 차단은 마치 전쟁에서 총을 드는 것과 같다고 비유, 광고는 원래 차단이 어렵거나 불가능했지만, 오늘날에 블록하는 광고 대부분은 사실 광고-감시의 혼합된 하이브리드라는 설명, 관련기사와 추가 배경지식 링크 제공
               o Google is three times a monopolist, says DOJ in anti-trust lawsuit
               o Web Environment Integrity
               o Google tracking despite opt-outs
               o https://www.proto..."">Secret anti-competitive contracts
          + 유튜브 크리에이터가 전체 수익의 55%를 받는 구조 지적, 구글의 막대한 이익 중심 체계를 비판하는 분위기 속에서도 실제로 대다수 주요 비용은 크리에이터에게 돌아가는 것이 현실, Patreon 후원 같은 적극적 참여는 일부 열성 팬만이 실천하며, 무료 만찬을 먹으며 식당에 불평만 늘어놓는 대다수 사용자들은 비용 부담을 회피하고 있다는 점 꼬집음
     * Object.defineProperty를 사용해서 글로벌 오브젝트의 속성을 비가변으로 만들어 환영객체를 통한 차단 회피가 가능하게 하는 스크립트의 동작이, 브라우저 혹은 스펙상 심각한 결함이라는 지적, 페이지 콘텐츠와 스크립트가 브라우저 확장 프로그램의 기능을 제한할 수 있어서는 안 된다는 주장
          + 실제로 이런 문제가 Chrome의 ‘기능’이라며, 2년 전쯤 Chrome이 확장 프로그램 초기화를 지연시키는 업데이트를 적용해, 마지막에 닫은 페이지가 필터와 차단 없이 빠르게 로드되고 그 과정에서 광고 차단기 탐지도 가능하게 됐다는 설명
     * 본인은 광고 차단의 목적이 주의 산만과 맥락 전환 방지에 있다고 밝히는 등, 광고가 주는 정보 과부하 때문에 이미 높은 인지적 부하에 더해지는 불편을 참기 힘들다는 의견
          + YouTube 보면서 어떻게 인지적 부하가 높을 수 있는지 반문, 중간에 실제 수술 훈련 영상을 관찰하며 심장이식 수술이라도 하고 있는지 궁금증 표시
          + 월 13달러만 지불하면 맥락 전환 걱정 없이 쉽게 해결 가능하다는 의견, 높은 인지적 부하라면 그만큼 충분히 잘 벌 것 같으니 그렇게 하는 게 더 효율적이라는 주장
     * 광고 제거와 콘텐츠 제작자 수익 보전을 위해 온 가족이 Youtube Premium을 기꺼이 결제한다는 사람의 경험 담백하게 전달
          + 자기는 그런 거 전혀 동의하지 않고 Newpipe를 활용, 돈 한 푼 안 들이고 광고 없이 사용 중이라는 태도
          + 위 글이 풍자인지 아닌지 헷갈리지만, 진심이라면 인터넷 대지주에게 아주 밝고 자진해서 돈을 추가 지불하는 사람을 보는 게 의외라는 생각
     * YouTube가 아예 광고를 영상 스트림에 직접 주입하지 않는 게 의외라는 견해, HLS/DASH 환경에선 중간에 콘텐츠를 쉽게 끼워넣을 수 있기 때문에 스트림에 포함하면 광고 차단이 어려워질 것이라 예상, 게다가 재생속도로 패킷 전송을 제한하면 미리 다운받아 우회하는 것도 막을 수 있다는 분석
          + 이런 환경에서도 HLS가 워낙 하드웨어 대단위로 최적화되어 있어서, 타겟팅 광고를 위해 계산량을 추가하면 비용이 폭증할 수밖에 없다는 설명, Netflix가 스트리밍 엣지 서버 성능 최적화 자료를 참고하면 관련 내용을 더 알 수 있다는 추천
          + YouTube가 변화를 너무 빠르게 시도하지 않고 서서히 적용하려고 한다고 분석, 곧 스트림에 광고를 직접 삽입하는 시나리오가 펼쳐질 거라는 예측, 현재는 광고와 영상 스트림이 분리되어 있지만 곧 서버측 광고 삽입 실험도 별도로 진행 중이라는 내용
          + SponsorBlock, Tubular 같은 타임스탬프 기반의 군중 지성형 광고 차단 흐름을 소개, 앞으로는 기기 내 실시간 AI 콘텐츠 인식 광고 차단도 나오게 되어, 아무리 막아도 광고 차단은 쉽게 끝나지 않을 것이라는 자신감
          + 실제로도 YouTube는 현재 서버 기반 광고 삽입(SSAI) A/B 테스트를 진행 중이라며, 본인은 아직 해당 광고를 경험하지 못하지만 관련 현황을 공유
          + 클라이언트가 광고를 영상에 삽입하듯이 잘라낼 수도 있다고 판단, sponsorblock처럼 군집 데이터베이스 기반이면 광고 탐지가 충분히 가능하며, 스트림 광고도 우회 차단 가능하다고 주장
"
"https://news.hada.io/topic?id=21576","Show GN: WHILE0124 – 전날 발행된 블로그 피드를 보내는 뉴스레터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Show GN: WHILE0124 – 전날 발행된 블로그 피드를 보내는 뉴스레터

    1. 왜 만들었는지

   개인적으로 daily-devblog를 굉장히 잘 보고 있었는데,
   아쉽게도 올해 들어서 종종 터지더라구요.
   그래서 (제가 볼 목적으로) awesome-devlog의 로직과 블로그를 다듬어서 만들어봤습니다.

    2. 뭐가 바뀌었는지

     * brunch 블로그에는 약점이 있었어서 이를 포함하도록 수정했습니다.
     * 뉴스레터를 구독하지 않더라도, 당일 내용을 페이지에서 미리 볼 수 있도록 수정했습니다.
     * GitHub의 전날 trend repository도 추가했습니다.
     * (운영적으로) 미니PC 기반 홈서버와 listmonk를 사용하기 때문에 비용에서 굉장히 자유롭습니다.

    3. 왜 공개하는지

     * 1년 이상 방치되거나 판매되는 등 사실상 역할을 하지 못하는 블로그를 다듬고보니 약 80% 정도가 줄었습니다.
     * AI로 찍어내는 블로그도 많아지는 상황에서 여전히 개인의 경험이 만들어내는 인사이트의 가치는 높다고 생각합니다.
     * 개발 블로그 외에도 다양한 인사이트의 블로그들을 포함하도록 확장하고 싶습니다.
     * 여전히 이 뉴스레터에 개선점이 많다고 생각하고, 블로그 추가 외에도 여러 종류의 기여를 기대하고 있습니다.
"
"https://news.hada.io/topic?id=21584","삼성, WANA 지역 스마트폰에 IronSource 스파이웨어 AppCloud 강제 탑재","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           삼성, WANA 지역 스마트폰에 IronSource 스파이웨어 AppCloud 강제 탑재

     * 삼성의 A 및 M 시리즈 스마트폰에 유저 동의 없이 AppCloud라는 블로트웨어가 선탑재됨
     * 해당 AppCloud는 ironSource(이스라엘 설립 회사, 현재 Unity 소유) 에서 개발되었으며, 민감 정보 수집 및 삭제 불가 특성으로 논란임
     * 개인정보 처리방침이 불투명하며, 사용자에게 데이터 수집 내용과 활용 내역이 고지되지 않음
     * 강제 설치 방식은 지역 데이터 보호 법률과 GDPR 위반 소지가 있음
     * SMEX 등 단체는 삼성에 투명성, 삭제옵션 제공, 향후 선탑재 중단을 촉구함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제 제기: WANA 지역 삼성폰, 강제 블로트웨어 설치 논란

     * 최근 서아시아 및 북아프리카(WANA) 지역 사용자들로부터 삼성 스마트폰에 거의 알려지지 않은 intrusive 블로트웨어 AppCloud가 선탑재된 것에 대한 많은 제보가 수집됨
     * 사용자 동의나 사전 고지 없이 설치되며, 민감한 개인정보 수집, 삭제 불가, 개인정보 처리방침의 불투명성 등 중대한 우려가 제기됨

AppCloud와 ironSource

     * AppCloud는 ironSource(이스라엘 설립, 현 Unity 소유) 에서 개발되었고, 해당 기업의 성격 및 지역적 법적 제약 탓에 추가적인 법적·윤리적 논란이 발생함
     * 삼성은 AppCloud의 기능, 수집 데이터, 사용자 선택권 등에 대해 아무런 투명성도 제공하지 않고 있음

SMEX의 공개서한 요청사항

     * 삼성에 AppCloud의 프라이버시 정책, 데이터 처리 방식, 삭제/비활성화 방법 공개를 긴급히 요청함
     * 향후 기기 선탑재에 대해 개인정보 보호권을 고려할 것을 권고하며, 관련 사안 논의를 위한 미팅도 요구함

AppCloud 설치 실태 및 우려

     * 2022년 삼성과 ironSource의 협력 확대 이후, A 및 M 시리즈 신제품에 AppCloud가 기본으로 설치됨
     * SMEX 분석에 따르면, 이 소프트웨어는 운영체제(O/S)에 깊게 통합되어 있어 사용자 권한으로 삭제 불가능함
     * 루팅 없이는 제거 불가하고, 비활성화해도 시스템 업데이트 시 복구됨

개인정보처리방침의 불투명성

     * AppCloud의 프라이버시 정책이 존재하지 않거나, 정보 접근이 불가능함
     * 어떤 데이터를 수집·활용하는지, 어떻게 보호하는지 투명하게 안내하지 않음
     * 민감한 사용자 데이터(생체정보, IP, 단말기 식별정보 등)까지 포함해 개인정보를 수집함

동의 없는 설치 및 법률 위반 소지

     * 사용자 동의 없이 AppCloud가 자동 설치되어 GDPR 및 WANA 국가 데이터 보호법 위반 가능성 존재
     * ironSource는 지역 법규상 일부 국가에서(예: 레바논 등) 영업이 금지된 점을 들어, 추가적인 법적·윤리적 문제 부각

삼성 이용약관의 문제점

     * 서비스 이용약관은 서드파티 앱만 포괄적으로 언급할 뿐, ironSource나 AppCloud 관련 구체 안내 없음
     * 대규모 데이터 접근 및 제어 권한을 가진 AppCloud에 대해 별도 고지가 없어 투명성 결여임

사용자 권리 침해 및 시장 영향

     * AppCloud 강제 설치는 사용자 프라이버시·보안권리 침해에 해당하며, 삼성의 압도적인 지역 시장 점유율을 고려할 때 심각한 사회적 파장 우려
     * 이런 상황에 대해 단체들은 다음을 요청함
          + AppCloud의 개인정보처리 및 데이터 활용 방식 전면 공개 및 접근성 보장
          + 옵트아웃 및 완전 삭제 방법 명확 안내 및 기기 안정성 저해 없이 가능하게 지원
          + WANA 지역 A, M 시리즈 기기에 선탑재 결정의 명확한 이유 설명
          + 추후 신제품 선탑재 전면 재고 및 개인정보권 보장(세계인권선언 12조 기준)
          + 사용자 프라이버시·데이터 보호 정책 관련 삼성 담당자와 구체적 논의 미팅
     * 사용자의 프라이버시와 보안을 위해 삼성의 신속한 답변 및 협력을 기대함

        Hacker News 의견

     * 나는 기업의 광고 목적으로 하는 대규모 감시와 국가 정보 기관의 감시가 최근 이란 고위 핵 과학자와 군 장교들이 집에서 표적이 된 사건과 밀접한 관련이 있다고 의심함. 어느 나라든, 어느 편이든, 요즘은 “반공개적” 데이터(기업이 판매하는 고객 정보나 스파이 기능 탑재 앱 등)만으로도 한 사람에 대해 너무 많은 정보를 추론할 수 있다는 점에 모두 동의할 것. 이제 정보 기관들이 정보 수집을 시장에 아웃소싱할 수 있어서 전통적 방식보다 훨씬 저렴하고 편리해진 상황. 오랫동안 “프라이버시는 인권”이라는 외침은 무시됐지만, 곧 정치인들도 프라이버시가 국가 안보 문제라는 점을 깨달았으면 하는 바람
          + 진실은 Overton Window(사회적·정치적으로 인정되는 대화의 범위) 바깥에 있는 현실. 드론 시대에서 프라이버시는 민간 방어의 문제임에도 불구하고, 현존 국가들은 대규모 PII(개인식별정보) 데이터베이스 구조 자체가 바로 그 취약점을 내포하고 있음. 반란 세력이 이런 데이터베이스를 탈취해 표적 삼을 수 있으니, 국가들은 결국 구식의 영토 통제 환상만 부여잡을 뿐임. 예전에 비해 통제는 몇 개의 요새화된 비밀 시설로 축소될 것이고, 앞으로 상황은 매우 예측 불가능하면서도 극도로 폭력적으로 치달을 거라는 예감
          + 우리는 종종 누군가가 핵시설에 방문한 사람들을 스마트폰 해킹으로 몰래 추적하는 첩보 영화 같은 작전을 상상하지만, 훨씬 더 논리적인 설명은 MEAF(이란 에너지청) 저급 직원에게 접근해서 정부 조직도 및 급여 기록이 담긴 USB를 건네받고, 그 대가로 그 직원의 자녀가 유명 외국 대학에 장학금을 받도록 해주는 식의 현실적 시나리오라는 생각
          + 이란의 셀룰러 통신망 시스템은 대부분 처음에 한국 기업이 설치했고, 이후 일부는 중국 브랜드로 바뀌었지만, 여전히 문제 있는 한국산 장비가 남아있다는 얘기를 들은 바
          + 이런 고가치 표적(이란의 장군/과학자 등)은 한 번만 찾아내면 위성으로 지속적으로 추적 가능. 정밀한 위치까지 필요 없고, 그냥 어떤 건물을 폭격할지 정도의 정보면 충분
          + 날씨 앱이 위치 정보를 브로커에게 공유하는 최악의 범인 중 하나. 오늘 날씨 확인하면 내일 폭격 위험해지는 시대
     * 원본 링크(https://web.archive.org/web/20250506145643/…)가 다운이라 공유함. AppCloud가 뭔지 기사에서 제대로 설명해주지 않지만, 본질적으로 삼성의 비플래그십 기기 사용자로부터 수익을 창출하기 위한 방법이고, 알림창 광고 삽입이나 앱을 몰래 설치할 수 있음. 만약 내 기기에서 이런 걸 발견하면, 더 이상 참지 않고 Apple 제품으로 넘어갈 마음
          + 그냥 Samsung을 안 사버리면 되지 않나 하는 생각. 물론 내 폰 브랜드도 비슷한 일을 할 수 있겠지만 아직 뉴스에 뜬 적 없어서 상대적으로 안심
          + 플래그십 모델, 특히 통신사 버전도 똑같은 일 당함을 본 경험에서 말해줌. 한 번도 본 적 없는 앱에서 알림이 계속 오고, 최근엔 삼성에서 Galaxy AI를 강제 탑재하면서(브라우저 텍스트 선택 시 절대 안 사라짐)나 인터페이스 불만 때문에, 날마다 내 선택을 후회
          + 5년 지난 iPhone을 중고로 사면 값은 저렴하고, 지원 기간도 길며, 싸구려폰보다 성능도 좋음. 나는 XS에서 새 모델로 갈아탔지만, 16도 크게 다르지 않고 그것마저 중고가 너무 낮아져 놀람. 옛날 iPhone이 웬만한 안드로이드 중급기보다 훨씬 쾌적
          + 안드로이드 버릴 필요 없음. Fairphone(https://fairphone.com)도 있음. 기본 안드로이드 괜찮고, 프라이버시 원하면 e/OS/ 설치도 초간단. 아직도 삼성 기기를 구입할 만한 가치가 있다고 결론내는 게 도무지 이해 불가
     * “삭제 불가능”이란 부분은 정확치 않음. 완전히 삭제는 안 되지만 시스템 파티션에 있기 때문에 adb 명령어로 비활성화는 대부분 가능. 아래 명령으로 Galaxy Store도 껐던 경험 있음
adb shell pm uninstall --user 0 com.package.name

          + “unremovable” 하지만 “완전히 지울 수 없다”면, 그게 바로 삭제 불가(definition of unremovable)라 생각
          + 이 방법이 모든 폰에 적용되는 건 아님. Motorola 같은 제조사는 ‘nodisable’ 기능을 써서 APK 비활성화 자체를 막음. 내 2025년식 Motorola RAZR 5G엔, /product/etc/nondisable 경로에 통신사‧금융업체용 앱 이름을 명시한 XML 파일들이 있음
          + 나도 삼성 폰에서 똑같이 adb 명령으로 삭제하고, 그 방법(https://harigovind.org/notes/removing-samsung-android-bloatware/)을 정리해둠. 하지만 이런 앱들은 시스템 업데이트할 때마다 다시 살아나서, 결국 삼성폰은 버리고 bloatware 적은 Moto로 옮겼음
          + 삼성은 사실을 미화하는 데에 PR팀을 두고 돈을 쓸 텐데, 최소한 이런걸 커버해주면 너도 돈을 받아야 하지 않을까 싶음. 네가 직접 지울 수 없다는 걸 인정했고, 셸 명령까지 써서 꺼야 한다면, 결국 업데이트 때마다 다시 살아나는 구조
          + 단어의 의미가 반드시 기술적, 문자 그대로만 통용되는 건 아님. 일반 사용자들이 쉽고 직관적으로 앱을 삭제할 수 없다면, 사실상 ""삭제 불가""한 것과 다름없음. adb 명령은 PC와 케이블, adb 설치, 디버깅 모드 등 일반인이 접근하기엔 사실상 서비스센터급 난이도라서, 자동차의 칩튜닝과도 비슷한 영역
     * 이 글이 생각보다 더 빨리 퍼지고 있어서 추가 설명함. MENA(중동·북아프리카) 지역 전반에서 비슷한 사례가 목격됨. SMEX 글은 WANA(서아시아·북아프리카) 위주이지만, MENA 쪽에는 “AppCloud” 대신 “Aura”라는 이름으로도 알려짐. 관련 기사(https://moroccoworldnews.com/2025/06/…) 있음
          + SMEX는 레바논 기반 단체고, (S)WANA란 말은 MENA 지명을 대체하는 요즘 신조어라는 설명
          + WANA와 MENA는 사실상 같은 지역
          + 나는 기업용 모바일 기기 관리를 했었음. 이 AppCloud는 유럽 오픈마켓 기기도 포함해 광범위하게 설치됐었음. 특히 엔터프라이즈 기기엔 절대 들어가선 안 됨(엔터프라이즈 MDM, E-FOTA 관리 기기 포함). 삼성 측과 이 일로 어색한 대화도 나눈 적 있음
          + 오스트레일리아에서 구입한 내 기기에도 설치되어 있었음
     * AppCloud는 논란 많은 이스라엘 스타트업 ironSource가 개발했으며, 최근 미국 회사 Unity가 인수함
          + 그래서 이제 Unity가 malware에 연관됐다는 농담도 가능
          + 가장 이상한 합병은 Unity가 ironSource를 44억 달러나 주고 인수했다는 점
     * 삼성만이 150불 이하의 비중국계 스마트폰(스파이웨어 논란 없는) 중 유일하다 생각해서 구매 고려했지만, 이제 남은 선택지가 애매함. 오픈소스 펌웨어 설치 가능한 폰이 있다면 그걸 고려. 난 내 폰을 신뢰하지 않기 때문에 중요한 정보는 전혀 저장 안 하며, 로그인이나 앱 설치도 하지 않는 중. 리눅스를 안 돌리는 기기는 그냥 신뢰 불가
     * 유럽과 북미도 똑같이 삼성 폰에서 AppCloud가 깔려있음. 초기 상태, 시스템 업데이트, 심지어 보안 업데이트 후에도 설치됨(역설적!). 통신사 락 여부와 무관하고, 설정에서 “시스템 앱 보기”를 켜야만 나타나기도. Galaxy S 시리즈 포함해 많은 사용자가 이를 보고 있음. AppCloud 논란이 도무지 말도 안된다고 느낌
     * 공급망 문제가 현대 보안에서 가장 ‘사이버펑크’적인 현실. 이는 수학적인 게 아니라 신뢰, 권력, 돈에 달림. 고객도 안전할 수 있는 형태로 공급망에 암호학적 검증을 도입할 가능성이 남았는지, 아니면 이미 늦어서 사이버펑크 디스토피아 미래만 남은 건지 궁금. 수학이 이 구도를 바꿀 수 있을지 고민
     * “루트 권한 없이 삭제 불가능, 루팅하면 워런티 무효+보안 위험” 주장은 우리를 이런 어이없는 상황에 몰아넣은 기업 프로파간다 그대로의 프레이즈임. 내가 기기 소유자라면 루트 권한도 당연한 권리여야 진짜 소유권 인정
          + 법적으로 모든 제삼자 소프트웨어 실행 가능한 하드웨어는 범용 컴퓨팅 장치로 간주되고, 사용자 실행 소프트웨어에 대한 암호적 혹은 기타 제한(예: 시큐어 부트, 리모트 어테스테이션 등) 금지가 필요. 이는 기기의 모든 부품에도 적용. 또한 사업자가 리모트 어테스테이션 실패를 이유로 서비스 거부(서비스 프로바이더 자기 보호 목적 차원)하는 행위도 금지해야 함. 이런 제한들은 결국 사용자 대신 광고업체(예: 비디오 플레이어 개조로 광고 건너뛰기 차단 등)에만 이익
          + 지금 구조에선 루팅을 하면 보안 손실이 불가피해진 현실. Verified Boot를 못 쓰는 점도, attestation이 회사에 귀속된 점도 모두 구조의 문제
          + 최근엔 “하드웨어 사용 라이선스”식으로 바뀌어, 내 소유의 기기를 자유롭게 쓸 자유조차 박탈당함. 특히 미국/유럽 외 지역, 예를 들면 아프리카에선 프라이버시·소비자 권리 개념도 미약
          + 현재 법적 현실은 기업 프로파간다의 결과이면서 동시에 실체적 현실임. “root access voids warranty(루트 권한은 워런티 소멸)”은 이미 많은 지역에서 실제 사실. 프로파간다 반복이 아니라 현실 설명에 가까움
          + “루팅은 워런티 무효이고 보안 위험”이라는 문장에 틀린 점은 없지만, 현실 사실 자체와 가치판단을 동일시해버리면 복잡한 문제임. 예컨대 “불에 데일 수 있다”는 경고가, 실제로 많은 사람들이 불에 의존해 음식이나 난방을 하는 것과 똑같이, 너무 단순화되는 문제임
     * 이쪽 분야는 아니지만, 만약 성능을 희생하더라도 보안(폐쇄적 서방 하드웨어 배제)만 추구한다면, 최고급 하드웨어·소프트웨어 인력이 힘 합쳐서 완전한 보안 스마트폰 만드는 게 어느 정도 가능할지 궁금. 예를 들어, 검증된 마이크로커널, 기본 전체 암호 메시징, 암호화 메모리, 프로세스 간 통신 암호화, 모뎀·주변기기·배터리용 하드웨어 스위치 등만 갖춰도 꽤 유의미해질 듯
          + 하지만 기술적 가능성 여부 따위는 자본의 힘 앞에선 의미 없음. 자본은 자체적으로 통제 불가능한 장치는 절대 허용 안 할 것. 진짜 보안 기기는 결국 꿈에 불과함
          + 생산 단계에서 쓰일 수준의 검증 마이크로커널은 엄청난 엔지니어링 과업임을 짚고 싶음
"
"https://news.hada.io/topic?id=21583","Cursor, Slack과 연동하여 백그라운드 에이전트 지원","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Cursor, Slack과 연동하여 백그라운드 에이전트 지원

     * Slack 내에서 @Cursor 를 멘션하여 Background Agents 사용 가능
          + 원격환경(우분투 기반 이미지)에서 실행되는 비동기 에이전트
          + 필요한 패키지를 자동으로 설치하고, GitHub에서 현재 Repo를 클론해서 지시한대로 처리
     * 즉, 대화창에서 @Cursor 에게 지시하는 것만으로 자동으로 처리하고 결과를 만들어내게 됨
          + @Cursor [branch=dev, model=o3, repo=owner/repo, autopr=false] Fix the login bug
     * 현재 대화창 쓰레드 문맥을 인식하여 그 기반으로 처리도 가능
          + 대화중에 @Cursor fix this
     * Background Agent가 동작을 끝내면 슬랙 Notification으로 알림이 옴
     * @Cursor 에게 현재 실행중인 Background Agent들 리스트업도 시킬수 있으며, Add follow-up 으로 추가 명령을 주거나 delete로 에이전트 중단 가능
"
"https://news.hada.io/topic?id=21672","AlphaGenome: 게놈을 더 잘 이해하기 위한 AI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    AlphaGenome: 게놈을 더 잘 이해하기 위한 AI

     * Google DeepMind가 AlphaGenome이라는 새로운 DNA 서열 AI 모델을 공개함
     * 이 모델은 유전자 조절 변이 효과 예측에서 정밀도를 높이고, 다양한 유전자 조절 과정을 예측할 수 있음
     * 1백만 염기쌍까지 긴 DNA 서열을 입력받아 다양한 생명 현상을 고해상도로 예측하는 것이 특징임
     * 기존 모델과 달리 다양한 생체 조직 및 세포 유형에 대해 변이의 영향을 한 번에 평가할 수 있음
     * AlphaGenome은 연구자들이 유전자 기능과 질병 생물학을 이해하고, 새로운 치료법 발견을 가속화하는 데 기여할 것으로 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AlphaGenome 소개

     * Google DeepMind는 AlphaGenome이라는 새로운 DNA 서열 AI 모델을 공개
     * 이 모델은 유전자 조절에 영향을 미치는 단일 변이 혹은 돌연변이의 효과를 정밀하게 예측함으로써, 게놈 기능 연구와 질병 이해에 중요한 전환점을 제공
     * API를 통해 연구 목적으로 미리 사용 가능하며, 향후 모델도 공개할 예정

AlphaGenome 작동 방식

     * AlphaGenome은 최대 1백만 염기쌍 분량의 긴 DNA 서열을 입력값으로 받아 다양한 분자적 특성을 예측함
     * 예측 가능한 특성에는 유전자 위치, RNA 생성량, DNA 접근성, 단백질 결합 부위 등 수천 가지가 포함됨
     * ENCODE, GTEx, 4D Nucleome, FANTOM5 같은 대규모 공개 데이터로 모델을 학습함
     * 내부적으로 컨볼루션 레이어로 짧은 패턴을 먼저 감지하고, 트랜스포머로 서열 전반의 정보를 결합한 뒤, 다양한 예측값을 도출함
     * 분산 TPU 환경에서 대용량 연산을 처리하여 학습 효율을 높임
     * 이전 Enformer 모델에서 발전해, 단백질 코딩 영역 전용 AlphaMissense와 달리 비암호화 영역(전체 게놈의 98%)까지 포괄적으로 분석함

AlphaGenome의 차별점

     * 초고해상도, 장거리 서열 분석 : 1백만 염기쌍 단위로 분석하며, 한 염기 수준의 정밀도로 결과를 제공함
     * 기존 모델보다 훈련 효율성이 높으며, 더 적은 자원으로 빠르게 학습함
     * 통합 멀티모달 예측 : 다양한 유전자 조절 단계별 정보를 한 모델에서 동시에 예측
     * 효율적 변이 점수화 : 변이된 서열과 정상 서열을 즉각 비교하여, 다양한 생명 현상의 변이 영향도를 신속하게 계산함
     * 혁신적 스플라이싱 결합부 모델링 : 유전자 스플라이싱 위치와 발현 수준을 직접 예측하여, 희귀 질환 연구에도 기여함

첨단 성능 및 벤치마크 결과

     * AlphaGenome은 게놈 예측 벤치마크 24개 중 22개, 변이 규제 효과 평가 26개 중 24개에서 외부 최고 모델들을 능가하거나 동등한 성능을 달성함
     * 개별 작업에 특화된 모델들보다 다양한 형태의 생체 특성을 단일 API 호출로 동시에 예측할 수 있는 유일한 모델임

통합형 모델의 장점

     * 여러 모달리티를 통합적으로 다룰 수 있어, 과학자가 다양한 가설과 실험을 빠르게 반복 가능함
     * DNA 서열의 일반적 표현을 학습해, 커뮤니티에서 추가 학습·최적화가 쉬움
     * 데이터나 적용 범위를 추가해 확장 가능한 유연성과 확장성 제공

강력한 연구 도구로써의 의미

     * 질병 이해 : 희귀 변이 등 질병 원인 규명, 치료 타깃 탐색에 활용 가능성
     * 합성 생물학 : 특정 기능을 가진 합성 DNA 설계에 활용 가능
     * 기초 연구 : 게놈의 핵심 기능 요소 맵핑 및 세포별 조절 요소 발굴 지원
     * 실제로 AlphaGenome은 T-ALL(급성 림프구성 백혈병) 관련 변이가 MYB DNA 결합 모티프 형성으로 인근 TAL1 유전자 활성화를 유발함을 예측하여, 해당 변이가 질병 유전자에 미치는 영향 메커니즘을 성공적으로 복제함

현재 한계

     * 10만 염기 이상 떨어진 매우 먼 조절 요소 효과 파악은 여전히 도전 과제임
     * 세포 및 조직 특이적 패턴 인식도 추가 연구 필요
     * 개인 게놈 예측(개인 맞춤 진단·예측) 용도는 현재 고려하지 않음
     * 분자 수준 예측만 가능하며, 모든 질병의 복잡한 원인을 완전히 설명하지는 못함
     * 현재 연구용 발표 단계로, 직접적 임상 적합성 평가나 치료 적용은 아직 불가함

커뮤니티 지원 및 앞으로의 방향

     * API를 비상업적 연구 목적으로 즉시 사용할 수 있으며, 연구 커뮤니티와 폭넓은 협업을 통해 AlphaGenome의 활용도를 높일 계획임
     * 커뮤니티 포럼 등을 통해 피드백과 사용 사례를 받고 있음
     * 더 많은 데이터, 종, 모달리티가 추가된 확장 버전으로 진화할 예정임
     * 게놈 해석과 관련된 새로운 의료 및 생명과학 연구 혁신 촉진 기대

마무리

     * AlphaGenome은 한 번에 다양한 관점에서 유전 변이의 의미를 해석하고, 기초 및 임상 연구를 가속화할 새로운 AI 기반 게놈 분석 도구임
     * 외부 전문가 집단과 협력하여, 가능한 많은 사람들에게 게놈 데이터 기반 혁신을 확산할 계획임

   유전자 예측을 다루는 AI 모델의 멀티모달리티란 어떤 모달들인가? 하는 궁금증이 생겨서 o3 에게 물어보니 전사량, 전사 시작끝 위치, 스플라이싱 등등을 모달리티라고 한다고 알려주네요.

        Hacker News 의견

     * 기업의 압박이 심해지고 있다는 신호를 볼 수 있는 부분으로, 단일 A100에서 돌릴 수 있는 모델임에도 코드 공개나 파라미터 공개는 없이 API 뒤에서만 구동하고 논문 31페이지에는 모델 전체를 의사 코드로 복붙해두는 모습 관찰, Google/Demis/Sergei에게 그냥 파라미터라도 공개해달라는 바람, 이렇게 작은 모델이 API 뒤에만 있어서 암까지 치료할 수 있을 리 없고, GCloud 수익도 크게 날 것 같지 않다는 생각
     * 세포 시뮬레이션 분야에서 돌파구가 생겨, 분자동역학처럼 유용하면서도 현대 슈퍼컴퓨터에서 가능한 수준의 시뮬레이션 구현 기대, 내부에서 어떤 일이 일어나는지 볼 수 없다는 게 생명과학 연구의 큰 장애물이라고 판단
          + Arc에서 실제로 이 작업에 도전 중, 자세한 내용은 arcinstitute.org의 관련 뉴스에서 확인 가능
          + 이 부분은 양자 컴퓨팅이 해결해줄 수 있다고 생각하지만, 아직 10년 정도 걸릴 것으로 예상, AI 가속화는 예측이 어려움
          + 진정한 결정론적 시뮬레이션을 만들려는 노력이 더 많아졌으면 하는 바람, 결과만 보여주는 블랙박스보다는 내부 과정을 드러내는 방식이 더 중요하다고 생각
     * DeepMind만이 높은 임팩트를 주는 AI 응용 연구를 하는 건 아니지만, 이 분야에서 돋보이게 두각 드러내는 점이 궁금, 기술 마케팅이 뛰어난 걸까, 아니면 다른 이유 때문인지 질문
          + 이번 논문은 잘 만들어진 연구이지만, 획기적 혁신으로 보긴 어렵고, 비슷한 시도가 이미 오랫동안 이어졌다는 의견
          + DeepMind가 오래전부터 이 일을 해왔고 구글이 제공하는 막대한 자원이 뒷받침함, perplexity에 따르면 alphafold 2 데이터베이스 구축에 “수백만 GPU 시간” 소요
          + 생명과학 분야에서 Arc Institute가 매우 신선한 연구를 진행 중, 제약사 가운데서는 Genentech 또는 GSK가 AI 그룹에서 훌륭한 성과 내는 중
          + 구글 산하 조직이니 2조 달러짜리 회사의 지원은 단지 마케팅 이상의 이점을 가져온다고 생각
     * 입력 크기를 인간 게놈 크기인 3.2Gbp로 확장하는 상상을 해보니 흥미로운 상호작용이 나타날 것 같음, U-net과 transformer가 연구의 중심이 되고 있는 것도 흥미 포인트
          + 실제론 2메가베이스 이상은 필요하지 않다고 생각, 게놈이 하나의 연속적 서열이 아니라서 크로모좀과 topologically associated domain 단위로 물리적으로 분리/조직됨, 2메가베이스 정도면 cis regulatory element와 effector gene 사이의 주요 상호작용 범위 거의 다 포함
          + “모든 게 U-net과 transformer 중심으로 돌아가는 게 흥미롭다”는 데에, ‘망치만 가진 사람’ 관점 언급
     * 기업 내부에선 게놈 데이터를 이용해 광고 효율을 높이는 아이디어도 나올 것으로 추정, 예를 들어 대장암 위험이 보이면 “대장 건강 보조제” 광고, 유전자 정보로 성향 분석해서 “이 유전자는 블랙 유머를 좋아하는 경향과 상관 있음, 이 유전자를 가진 사람들 대상으로 새 영화 홍보” 같은 마케팅 전략 가능 예상
     * RNA 예측 성능의 큰 도약은 mRNA 연구실에 큰 기회 제공 예상
          + (바로 이어지는 답글: 이 점은 미국 외 지역에서 더 뚜렷하게 나타날 수 있다고 생각)
     * 2008년 구글 입사 직후 생명과학 분야에 많은 투자를 주장했음, 구글이 데이터 처리와 ML 역량에서 세계적인 결과를 내고 그 방법이 다른 생물학자들에게도 재현되도록 도울 수 있다고 확신, 실제로 exacycle을 통해 단백질 폴딩/디자인에서 흥미로운 결과 생산, 이후 Cloud Genomics 출시로 대규모 데이터셋 저장/분석 서비스까지 진행, 결국 DeepMind는 내가 생각했던 목표를 훨씬 멋지게 실현한 셈, 최근 논문은 볼거리가 엄청 많아 커뮤니티가 내용을 소화하는 데 시간 걸릴 듯
          + Sundar가 구글 CEO로서 영감 주는 리더는 아니라는 평가엔 동의하지만, 취임 전인 2015년 분기별 이익 3B에서 2025년 1분기 35B로 10배 성장 견인, 광고 사업 고수로 지금의 수익성 이끌었다고 생각, AI 전환은 약간 늦었지만 gemini 등에서 경쟁력 있다고 판단, DeepMind도 대단한 성과, “Sundar는 hype는 적지만 실적은 뛰어나다”는 평가
          + “오랜 숙원이 이뤄져 기쁘다”는 의견이 꽤 자기도취적으로 들린다는 의견, 대부분의 사람들도 대단한 아이디어를 갖고 있어도, “드디어! 내 아이디어가 세상에…”처럼 말하긴 어색한 점 지적
          + 혹시 예전에 Santa Cruz 셔틀에서 이런 주제로 대화했던 적 있는지 묻는 질문, 당시 얘기가 엄청 흥미로웠고 AlphaGenome 등장에도 여전히 설렌다는 소감
          + 현직 Googler 시점에서 Sundar에 대한 생각은 굉장히 복합적, AI 분야에 초기부터 인프라 및 도구에 투자한 점은 인정, Demis보다는 Jeff Dean에게 더 큰 공을 돌릴 필요 있다고 생각
     * 논문에서 가장 큰 문제 중 하나인, 연관성이 높은 DNA 구간 중에서 정말로 인과적인 변이와 비인과적 변이를 구별하는 작업(유전학에서 fine mapping으로 부름)을 무시한 점이 실망, 효과적인 약물 타깃을 위해 핵심 조절 영역을 정확히 좁히는 일이 매우 중요, 최근 Nature 논문에서는 이 문제의 예시와 자가면역에서 대식세포 기능 조절용 후보 약물까지 연결된 사례가 있음
          + 이번 결과가 그런 방향에 더 가까워진 것인지 궁금, 전문 지식은 깊지 않지만 기능 예측이 좋아지면 실제로 중요한 변이와 의미 없는 변이를 구별하기 쉬워질 것 같음, 다음 단계는 제대로 된 통계 fine mapping 방법과의 통합이 될 것으로 생각
"
"https://news.hada.io/topic?id=21670","Web Translator API - 브라우저에서 로컬 번역하기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Web Translator API - 브라우저에서 로컬 번역하기

     * Translator API는 브라우저내에 제공되는 AI 번역 모델을 활용하여 로컬 웹 번역 기능을 제공하는 실험적 Web API임
          + 크롬 138 버전 이상부터 사용 가능
     * sourceLanguage, targetLanguage 등 주요 속성을 통해 번역 언어 지정 및 인스턴스 생성
     * translate, translateStreaming 등 메서드를 이용해 문자열 혹은 스트림 방식 번역 지원
     * inputQuota와 measureInputUsage로 번역 시 쿼터 사용량 확인 가능
     * 사용 예제 코드
if ('Translator' in self) {
  // The Translator API 가 지원됨
  const translator = await Translator.create({sourceLanguage: ""en"",  targetLanguage: ""ko"", });
  console.log(await translator.translate(myTextString));
  //스트림 방식 번역
  const stream = translator.translateStreaming(myTextString);
  let translation = """";
  for await (const chunk of stream) {
    translation += chunk;
  }
  console.log(translation);
}

     * 아직 브라우저 호환성이 제한적이므로 프로덕션 사용 전 호환성 테이블 확인 필요
          + 현재는 크롬만 지원
          + Mozilla 의 경우는 관련 PR이 있는데 이슈가 있어서 당분간은 지원 어려울듯

   브라우저 핑거프린팅의 꿀단지 될듯

   번역 기능은 항상좋아

   Firefox하고는 상황이 달라 표준이 되기 어려울듯
"
"https://news.hada.io/topic?id=21617","러스트로 작성된 매우 빠른 Python 패키지 및 프로젝트 관리자 uv","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                러스트로 작성된 매우 빠른 Python 패키지 및 프로젝트 관리자 uv

     * uv는 러스트(Rust) 기반의 매우 빠른 Python 패키지 및 프로젝트 관리 도구임
     * pip, pip-tools, pipx, poetry, pyenv, virtualenv 등을 하나로 대체할 수 있음
     * 최대 10~100배 빠른 성능과 디스크 공간 절약, 강력한 캐시 및 cross-platform 지원 제공
     * 스크립트, 프로젝트, 도구, 다양한 Python 버전 관리 등 통합적인 개발 환경 지원 기능 포함
     * 개발 생산성, 대규모 프로젝트, 빠른 작업 속도에 최적화된 현대적 Python 개발 워크플로우 실현 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

오픈소스 소개 및 차별화 포인트

     * uv는 기존의 pip, pip-tools, pipx, poetry, pyenv, virtualenv, twine 등 여러 Python 관리 도구 기능을 하나의 단일 툴로 통합함
     * 러스트(Rust)로 개발하여 성능이 매우 우수하며, 전통적인 pip 대비 10~100배 빠른 설치 및 동기화 속도를 자랑함
     * 글로벌 캐시 및 의존성 중복 제거로 디스크 활용 최적화 기능을 제공하며, 직관적 CLI와 친숙한 pip 호환성을 지원함
     * macOS, Linux, Windows 등 다양한 플랫폼에서 독립 실행 파일로 설치 가능함
     * 독립형 설치 방식, pip 및 pipx와 연동, 자체 자동 업데이트 지원 등 편의성 강화가 특징임

주요 특징(Highlights)

     * 하나의 uv 도구로 pip, pip-tools, pipx, poetry, pyenv, twine, virtualenv 등의 다양한 기능을 대체 가능
     * 기존 pip 대비 10~100배 빠른 설치/업데이트/동기화 성능 제공
     * lockfile 기반으로 프로젝트 의존성 관리 및 workspaces, universal lockfile 지원
     * 스크립트 의존성 인라인 선언 및 자동 환경 분리 실행 기능
     * 다양한 Python 버전 관리/설치/변경 지원
     * Python 패키지로 배포된 도구 실행 및 설치 지원 (pipx 대체)
     * pip 인터페이스 호환 및 추가 기능 제공 (버전 덮어쓰기, 플랫폼 독립적 해결 등)
     * Cargo 스타일 workspace로 대규모 프로젝트에 최적화
     * 글로벌 캐시로 의존성 중복 최소화 및 디스크 공간 효율화
     * Rust/Python 환경 없이도 curl 또는 pip, pipx로 설치 및 사용 가능
     * macOS, Linux, Windows 등 멀티 플랫폼 지원
     * Astral 및 Ruff를 개발한 팀에서 제작

프로젝트 관리(Project Management)

     * 프로젝트 단위로 의존성, 환경, 잠금(lock) 파일, workspace 등을 완벽 지원
     * 'uv init' 명령으로 프로젝트 자동 초기화 및 virtualenv 생성
     * 'uv add'로 의존성 추가, 'uv lock' 및 'uv sync' 명령으로 패키지 동기화 및 보안 감사를 자동 처리
     * Poetry, Rye 등 현대적인 Python 프로젝트 관리 도구의 기능을 대체하면서 더 빠른 처리 성능 보장
     * uv로 관리하지 않은 프로젝트 빌드 및 배포(publish)도 지원

스크립트 관리(Scripts)

     * 단일 파일 스크립트에 의존성 inline 메타데이터 선언 가능
     * script 실행시 자동 가상환경 분리 및 의존성 설치 지원
     * uv add --script로 스크립트별로 의존성 관리, uv run 명령으로 환경 격리 실행
     * 데이터 과학/자동화와 같은 단발성 스크립트 활용에 최적화

도구 관리(Tools)

     * Python 패키지 형식의 CLI 도구를 pipx처럼 설치 및 실행 가능
     * uv tool install, uvx 명령으로 임시 환경 혹은 전역 실행 활용
     * 설치된 도구 확인 및 버전 관리, 업데이트 지원

Python 버전 관리

     * 여러 Python 버전을 간편하게 설치 및 즉시 전환 가능
     * 다양한 버전을 병렬로 관리하고, 프로젝트별 .python-version 핀 설정 가능
     * pypy 등 대체 구현체도 동일 인터페이스 지원
     * uv python install/pin 등 명령으로 버전 설치, 지정, 활성화

pip 인터페이스(Pip Interface)

     * uv pip, uv venv로 기존 pip, pip-tools, virtualenv를 완벽히 대체
     * 의존성 버전 override, 플랫폼 독립 해상, 재현 가능한 빌드 등 고급 기능 포함
     * 기존 워크플로우 변경 없이 pip drop-in 대체하며 10~100배 성능 향상 제공
     * requirements.in → requirements.txt 변환, 가상환경 생성 및 requirements 동기화 지원

플랫폼 및 버전 정책

     * 다양한 운영체제(Windows, macOS, Linux) 지원
     * 정책 및 지원 플랫폼 정보는 공식 문서 참조 가능

기여(Contributing)

     * 초심자부터 전문가까지 다양한 기여자 지원을 목표로 하며, 관련 가이드 제공

FAQ

     * uv 발음은 “유-브이”로 표기함
     * 스타일은 소문자 “uv”로 고정

기술적 배경 및 감사(Acknowledgements)

     * 의존성 해결 알고리듬은 PubGrub을 사용
     * Git 구현은 Cargo 기반
     * 최적화 전략은 pnpm, Orogene, Bun, Posy 등 최신 패키징 도구에서 많은 영감을 받음

라이선스

     * MIT 및 Apache-2.0 중 택일하여 사용할 수 있음
     * 기여한 코드 역시 동일 조건으로 이중 라이선스됨

        Hacker News 의견

     * 몇 달 전까지만 해도 uv를 절대 쓰지 않겠다고 생각했는데, 이미 venv와 pip에 익숙했기 때문이었음, 다른 도구가 굳이 필요하다고 생각하지 않았음, 하지만 최근 공유 서버에서 root 권한이 없는 상황에서 각종 패키지나 드라이버가 다 깨져 있었고 pytorch가 필요했던 경험 덕분에 완전히 uv로 바꾼 상황, pip은 오랜 시간이 걸리고 캐시가 공간을 엄청 차지하며 위치 변경도 잘 안 되는 불편함이 있었음, uv로 바꾸자 모든 게 너무 잘 동작해서 만족함, 아직 망설이고 있다면 5분만이라도 꼭 써보라는 추천
          + uv의 최고의 장점은 기존에 쓰던 venv 기반 워크플로우와 완전히 호환된다는 점, 그냥 uv venv 실행으로 해결 가능
          + uv는 설명하거나 특히 초보자에게 사용법을 안내할 때 훨씬 쉽게 느껴짐, pip + 설정 파일 + venv 조합은 올바른 venv를 만들고 설치하는 과정이나, 스크립트 실행/테스트마다 어색한 shebang이나 venv 활성화를 기억해야 해서 헷갈리고 에러 메시지도 온전히 도움을 못 주는 불편함이 있었음, 초보자에게 가르칠 때 도구들이 참 번거롭게 느껴졌고, 이제는 ""uv run"", ""uv add"", ""uv sync""만 기억하면 되니까 팀원들도 훨씬 덜 부담스럽게 받아들이는 분위기
          + 나의 경우 asdf를 사용하다가 mise로 넘어갔는데, uv와 같은 범용 도구들과 비교하면 어떨지 궁금증
          + pip 캐시가 많은 공간을 차지하고 위치도 제대로 바꿀 수 없었다고 했는데, uv가 저장소 공간 활용 면에서 더 나은지 궁금, 만약 낫다면 그 이유가 공유를 잘하는 방식 때문인지도 궁금함
          + 최근 실험용 저장소에서 ""uv a b c""로 시작하는 간단한 가이드 보고 실행해본 경험, 내부적으로 많은 중복이 있는 듯하지만, 실사용에서는 GNU-Debian-Ubuntu 기본 호스트에서 딱히 문제 없고 모든 게 순조롭게 돌아가서 만족
     * 처음 uv를 썼을 때, pip과 비교해 워낙 빨리 끝나서 뭔가 실수했거나 제대로 동작하지 않은 줄 알았던 기억
          + 가끔 패키지 설치에 200ms 정도 걸려서 엔터 입력과 프롬프트 표시 사이에 아주 약간의 딜레이가 느껴짐, 하지만 Poetry에서는 커피 한 잔 마시고 오면 끝나는 수준이라 속도 차이가 확연함
          + 나 역시 같은 느낌, 워낙 매끄러워서 Python이 아닐 것 같은 경험
          + 비슷한 경험을 최근에 했고, 덕분에 완전히 uv로 전향한 입장
          + 나도 의심이 들 정도였지만, 시도해보고 나선 원래로 돌아갈 수 없을 정도로 만족
     * uv와 ruff는 ""절대 바퀴를 다시 만들지 말라""는 말에 대한 멋진 반례라고 생각, 목적이 뚜렷하다면 기존보다 월등히 나은 결과물이 나오는 경우도 발생
          + 대부분 그런 말이 나오는 맥락은 기존을 잘 모르는 초보자에게 해당하는 조언이라고 봄, 즉 시스템의 한계와 개선점을 모르는 사람이 섣불리 재발명을 시도할 때에만 들어맞음, 진짜 전문가에겐 해당 안 되는 말
          + 이들이 바퀴를 다시 만든 게 아니라, 기존의 목재 바퀴를 더 견고한 재질로 교체해서 10배 속도로 회전 가능하게 만들었다는 비유
          + Python 패키지 관리의 역사만 봐도 모두가 기존보다 더 잘 만들겠다는 생각으로 뛰어든다는 인상
          + 사실 ""바퀴를 다시 만들지 않는다""는 속담 자체가 말이 안 됨, 우리는 실제 바퀴도 계속 발전시켜 왔는데 소프트웨어 역시 더 좋은 바퀴를 만들지 않을 이유가 없음
          + 약간 새는 이야기지만, ""order of magnitude better"" 대신 왜 짧은 ""10x"" 표현을 안 쓰고 굳이 긴 표현을 선호하는지 궁금증
     * 작은/저사양 시스템(AWS T2.micro에서 Windows 같이)에선 uv가 너무 많은 동시 다운로드를 시도해 타임아웃이 남, 환경 변수 UV_CONCURRENT_DOWNLOADS로 동시 다운로드 수를 1~2 정도로 제한하면 해결, uv 기본 설정이 너무 공격적이라 느끼며, 서버별 스레드 자동 조정에 다운로드 속도를 활용하는 식으로 개선되면 좋겠음
          + 전혀 이상한 사례가 아니며, 많은 유저가 저렴한 VPS에서 사이드 프로젝트를 진행함, (본인도 자주 그러는 편, AWS는 아니지만), 공유해줘서 고맙고 자동 감지 기능이 개선되길 바람
     * 최근 노트북에 uv를 개인용으로 써보고 있는데, pip에 익숙했던 입장에선 체감 속도가 도무지 믿기 힘들 정도로 빨라서 몇 번은 제대로 실행됐는지 헷갈린 경험
     * uv add <mydependencies> --script mycoolscript.py 명령을 좋아함, 그리고 맨 위에 #!/usr/bin/env -S uv run을 붙이면 Python 스크립트를 즉시 실행 가능해서 매우 유용한 툴로 활용 중
          + 이 방식을 Claude Project에 특화된 프롬프트로 가르쳐서, 단일 입력만으로 의존성 포함 전체 스크립트 자동 생성도 가능, Claude 4의 컷오프는 2025년 3월임, Claude Sonnet 4에선 추가 프롬프트 없이도 이 기능이 동작, 예시로 URL을 입력하면 httpx와 beautifulsoup으로 크롤링해서 링크 목록을 CSV로 반환하는 코드를 바로 생성 관련 자료 Claude 스크립트 결과
          + Marimo.io 노트북의 app-mode와 함께 이 트릭을 사용, uv만 설치돼 있으면 즉석에서 반응형, 재현 가능한 앱을 최소한의 준비만으로 타인에게 손쉽게 전달 가능, 정말 최고 조합
          + 지금은 작은 스크립트도 즉흥적으로 바로 실행하는 습관이 생겼음, 환경이나 의존성 관리를 신경 쓸 필요가 없어져서 훨씬 쾌적, the little scripter 관련 글, 관련 유튜브 영상, ez-mcp 깃허브
          + 예시를 잘못 읽었다는 정정과, 프로젝트 컨텍스트에서 'uv add --script'와 단순 'uv add'는 다르단 설명 및, 공식 문서에서 run --with이나 PEP723 지원 등 더 유용한 기능이 많으니 확인 권장 공식 가이드
          + ""mydependencies""가 구체적으로 뭐냐는 질문, 설정 파일의 의미인지 궁금
     * uv를 예전에 시도했는데, 압도적으로 빠르고 사용도 쉬워서 놀람, 이제 거의 pip을 쓸 이유가 없고 Python만 쓴다면 conda도 필요가 없어짐
          + pyenv와 poetry 역시 이제는 사용하지 않게 됨
     * UV를 정말 좋아함, Astral 팀의 Ruff도 좋아해서 기존 pylint + Black에서 Ruff로 린팅/포매팅을 모두 옮김, 린트 시간이 90초에서 1.5초 미만으로 단축된 경험, 깜짝 놀람
          + 다만 ruff는 pylint의 일부 체크만 수행하고 지나치게 명확한 에러들도 검출 못하고 그냥 넘어간다는 점을 나중에 알게 돼서 아쉬움, (실행 불가한 코드 등도 걸러지지 않음)
     * 최근에는 아래 패턴으로 작은 실행형 스크립트를 돌리는 게 새롭게 마음에 들어서 즐겨 사용 중
#!/usr/bin/env -S uv --quiet run --script
# /// script
# requires-python = "">=3.13""
# dependencies = [
#   ""python-dateutil"",
# ]
# ///
#
# [python script that needs dateutil]

          + 해시뱅 줄이 너무 외우기 어려워서 그냥 #!/usr/bin/env uvx 정도의 더 단순한 형태가 있으면 좋겠음, 쓸 때마다 검색해야 하는 번거로움이 있음
     * 완전히 만족해서 pip/twine/requirements.txt로 다시 돌아가고 싶지 않음, 여러 프로젝트가 내부 GitLab에 공용 휠을 두고 있었는데 기존 YAML 10줄을 ""uv build""와 ""uv publish"" 두 줄로 대체 가능, 의존성 가져오기 편하고 주요 의존성도 한 번에 확인 가능, requirements.txt에 모든 걸 뒤섞어 두는 번거로움이 사라짐
"
"https://news.hada.io/topic?id=21689","항공사 조종사가 만든 내 비행 경로 인터랙티브 그래프/글로브","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   항공사 조종사가 만든 내 비행 경로 인터랙티브 그래프/글로브

     * A350 조종사이자 컴퓨터 엔지니어인 창업자가 직접 비행 경로 시각화 도구를 개발함
     * 이 도구는 전 세계를 여행하면서 경험한 비행 데이터를 인터랙티브 그래프와 글로브 형태로 보여줌
     * 소프트웨어 개발, 여행, 기타 다양한 취미 활동 기록과도 연결되는 프로젝트임
     * 사용자는 웹사이트에서 다양한 시각화를 직접 조회할 수 있는 이점을 가짐
     * IT 및 스타트업 업계에서 데이터 시각화, 지리 정보 활용에 관심 있는 엔지니어에게 의미 있는 자료임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개

     * 본인은 A350 항공기 조종사이자 컴퓨터 엔지니어임
     * 항공 업계에서 일하면서 전 세계를 여행하고 그 경험을 인터랙티브 그래프와 지구본 형태의 시각화로 기록하는 소프트웨어를 개발함
     * 개발한 도구를 통해 직접 수행한 비행들을 데이터로 정리 및 시각화하며, 이를 웹사이트에 공개함
     * 이 프로젝트는 단순한 취미를 넘어 항공 데이터의 시각화, 전 세계 항공 경로 분석, 프론트엔드 기술 활용 측면에서도 의의가 있음
     * 웹사이트에서는 비행 이외에도 개인이 여행, 소프트웨어 개발, 기타 취미 생활을 문서화하는 내용도 일부 다룸

        Hacker News 의견

     * 멋진 시각화라서 개인 로그북 관리에 딱 맞는 느낌. 원본 데이터나 표시하는 방식은 어떻게 저장하는지 궁금함
       지구본 맵이 북마크에 있던 육각형 격자 구조를 다룬 이 Red Blob Games 글을 떠올리게 함
       나는 항공사 조종사라 Rehearsal 시즌 2를 봤는지 궁금함 (Nathan Fielder가 HBO에서 조종사-부조종사 간 소통 문제를 유쾌하게 다뤘던 작품)
       만약 봤다면, 조종사 동료 간 마찰을 표현한 방식이 실제 경험과 얼마나 맞는지 의견이 궁금함
       직업 특성상 도움 필요성을 인식하지 못한 채로 고기능자로 남는 경향이나 생존자 편향이 남아 있는지, 아니면 Nathan의 시각이 너무 과장된 사례인지 의견이 궁금함
          + 내 로그북 소프트웨어에서 모두 sqlite 파일로 저장하고 있음
            데이터를 추출하는 방법은 여기에 정리한 포스트에 있음: https://jameshard.ing/posts/querying-logten-pilot-logbook-sql
            Rehearsal은 Sully가 Evanescence 듣는 장면 등 일부 클립만 봤고 전체적으로 논할 근거는 부족함
            다만 조종사 간 소통은 정말로 많은 시간과 노력을 할애하는 분야임 (일명 CRM, Crew Resource Management)
            내 경험상 업계는 이 부분에서 점점 개선하려고 실제로 노력 중임
          + 원본 데이터 저장 방식에 관해 글에서 LogTen Pro[1]를 사용한다고 답했었음
            SQL 쿼리 관련 글에도 로그북 앱이 CSV 내보내기도 지원하지만 내부적으로는 SQLite로 데이터를 관리해서 직접 접근 및 쿼리가 가능하다고 적혀 있음
            [1] https://logten.com/
          + 저 육각형이 Uber의 H3 라이브러리로 구현된 것은 아닌가 생각함
          + 이 시각화가 예전에 봤던 이 오래된 유튜브 영상과 닮은 느낌
     * 정말 멋진 작업이라서 세세한 비행 로그를 읽는 게 정말 즐거운 경험이었음
       거리와 비행시간뿐 아니라 역할 변화(P1, P2, PICUS)까지 세밀하게 기록한 점이 흥미로웠음
       SpinStep을 개발하는 입장에서, 방향·벡터 변화 같은 물리 시스템을 쿼터니언 기반 라이브러리로 계산하고 있어 이런 비행 로그가 회전 상태 모델링과 어울릴 수 있다는 영감을 받음
       예를 들어 항공기의 헤딩 변화는 쿼터니언으로 자연스럽게 매핑 가능
       역할 전환(P1↔P2) 또한 연속 시스템 내의 이산 상태 변화처럼 볼 수 있음
       바람과 비행 네트워크 패턴(환경적 영향)은 시간 흐름에 따른 외부 필드로 모델링도 가능
       SpinStep을 이런 방식으로 쓸 줄은 몰랐지만, 데이터 덕분에 색다른 시야를 얻었음
       참고 링크: https://github.com/VoxleOne/SpinStep/blob/main/README.md, https://github.com/VoxleOne/SpinStep/blob/main/docs/01-rationale.md
          + 레포를 읽어보니 LLM이 쓴 것 같은 느낌이 있음
            쿼터니언은 3D 회전 관련 연산에서 유용한 특성이 있지만 만능 해법은 아니라고 생각함
     * 여러 분야에 능숙한 사람을 보면 큰 자극이 됨
       나도 늘 본업 외에 새로운 걸 해보고 싶은 꿈이 있는데, 언젠가 게으름을 이기고 도전할 용기를 얻고 싶음
          + 소프트웨어 개발이 워낙 고액 연봉이라 고민
            관심 있는 분야가 너무 많지만, 다른 업계로 완전히 이직하면 급여가 많이 줄 것 같아 현실적 선택이 쉽지 않음
     * 비주얼 자체가 아름다움
       지구본과 멋진 애니메이션 외에 대시보드로 집계 통계 보여주는 게 인상 깊었음
       예전에 독일 Frauenhofer나 Helmholtz에서 제공했던 사이트가 있는데, 특정 비행 정보를 입력하면 전체 방사선 피폭량을 계산해줬음
       주로 항공종사자용 서비스였고 비주얼은 별로지만 누적 피폭량을 대시보드에 추가하면 유용할 듯함
          + 정말 좋은 아이디어라고 생각함
            내가 일하는 항공사도 월/연간/평생 누적 방사선량은 제공하는데 기록이 세분화되어 있지 않음
            혹시 어떤 통계 방식이 가능한지 아는지 궁금함
            대략적인 항로(대권거리)와 비행시간별 대기 중 방사선량을 추산하는 방식으로 접근 가능하지 않을까 생각함
          + Nomadlist도 예전엔 모든 여행에 대해 방사선량을 보여줬었는데 그 수치를 보고 진짜 놀랐던 기억이 있음
     * 나도 비슷한 시각화를 만들었는데, ADS-B 데이터 약 1500억 포인트를 활용함
       adsb.exposed에서 볼 수 있음
       필터링이 직관적이라 어떤 항공기 유형도 바로 볼 수 있고 전체적으로 2D지만 3D변환도 고려한 적 있음
       참고로, 글쓴이의 지도는 확대하면 프레임레이트가 10 미만으로 약간 느림
          + 엄청난 데이터 연산량이라 감탄함
            기술적인 구현 방식이 궁금함, 공유해 줄 수 있는지 요청함
     * 항공 노선 시각화 툴에 관심 있다면 GCMap을 추천하고 싶음
       GCMap은 두 IATA 공항코드만 있으면 아무쪼록 선을 그릴 수 있고, 여러쌍도 URL 파라미터로 넣을 수 있음 (예: JFK-LHR,LHR-CDG,CDG-FRA)
       기본 활용 예시 링크
       나도 비행 기록을 GCMap URL로 이메일로 보내며 관리함
          + GCMap은 다양한 지도 투영법이 부족한 게 단점
            경로를 여러 쌍 넣으면 그 결과가 그렇게 좋아 보이지 않음
            Mollweide, Winkel Tripel, Robinson 같은 프로젝션이나 지구본 형태도 지원해줬으면 좋겠음
     * 놀라운 시각화라서 각 비행 기록에 기능을 더할 계획이 있는지 궁금함
       예: 이륙·착륙 난이도, 난기류 유무, 경로 등
          + 고마움 표시
            특히 기억에 남는 비행들은 모두 텍스트 코멘트(유명인 탑승, 가족, 특수상황 등)로 적어두고 있지만 프라이버시와 비주얼화 한계 때문에 일부는 비공개
            더 많은 정보의 수집이 목표지만 기록의 편의성과 균형 잡기가 늘 어려운 과제임
     * 보기 좋아서 공유 고마움 표시
       소프트웨어 엔지니어링의 최대 장점은 평범한 일상에도 다양하게 활용된다는 점
       본인만의 비행 경력을 시각화한다는 게 정말 멋진 일임
          + 정말 공감
            이런 프로젝트를 하면 늘 떠오르는 XKCD 만화가 있는데 참고 링크: https://xkcd.com/1205/
     * 전문 파일럿이면서도 이렇게 완성도 높은 소프트웨어 프로젝트까지 만들 수 있다는 점 자체가 대단하게 느껴짐
          + 비행사들은 9-5 직장이 아니기에 비행 사이에 시간이 많은 경우가 많음
            여유 시간에 이런 취미 프로젝트를 하는 것은 전혀 이상하지 않고, 비행 중에 하는 건 당연히 아님
          + 조종사들이 비행 중 랩톱으로 뭔가를 할 수 있는지 궁금함
            대부분 비행은 착륙 전까지 대기라 시간적 여유가 있어 프로그래밍 스킬을 익힐 수 있을 듯함
          + 작성자는 UofT에서 전산학을 전공함
     * 이 프로젝트를 제품화해 볼 만하다고 추천
       조종사들이 LinkedIn 계정에 연결할 수도 있고, 가족과 친구들에게도 멋진 보여주기용 포트폴리오가 될 수 있을 거라 생각함
"
"https://news.hada.io/topic?id=21611","Home Assistant, adguard home, 8달러 스마트 콘센트를 활용한 브레인 로트 방지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Home Assistant, adguard home, 8달러 스마트 콘센트를 활용한 브레인 로트 방지

     * 인터넷 중독과 지속적 소셜 미디어 이용 문제를 해결하기 위한 실용적 방법 소개임
     * Adguard Home과 Home Assistant를 활용해 사용자가 직접 접근 제어를 적용할 수 있는 시스템 구성임
     * Zigbee 스마트 플러그를 활용하여 일정 시간만 소셜 미디어 접근을 허용하고, 이후 쿨다운 시간을 도입함
     * Adguard Home API 연동을 통해 필터링 규칙의 동적 온오프 제어가 가능함
     * 작은 자동화로 자기절제와 사용량 제한을 실천하는 접근법임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

   인터넷은 필수적인 존재이면서 동시에 광고, 소셜 미디어 중독, 불필요한 콘텐츠 등 다양한 부정적 영향을 주는 매체임
   필자는 소프트웨어 개발자로서 인터넷의 필요성을 인식하고 있으나, 사회적 미디어의 반복 사용 및 광고 등 부정적 요소 관리가 개인 과제임
   과거 네트워크 차단 프로그램(Adblocker)으로 소셜 미디어를 막아봤지만, 모바일 데이터로 이를 쉽게 우회할 수 있었고, 완전한 차단은 현실적이지 않았음
   가정 내 다른 구성원(예: 아내)도 업무나 연구를 위해 소셜 미디어가 필요해 완전 차단은 불가능했음
   이러한 이유로 적절한 절제와 접근 권한 조절이 필요해짐

실마리와 아이디어

   Hacker News에서 Neil Chen이 스마트 플러그와 ublock Origin 리스트 재작성으로 소셜 미디어 접근 일시 허용 아이디어를 제안함
   해당 방법은 실용적이지 않아, 필자 환경에 맞게 Adguard Home과 Home Assistant를 활용해 아이디어를 변형 적용함

구성 요소

     * Adguard Home이 설치된 gli.net 라우터
     * Home Assistant 스마트홈 서버
     * 여러 개의 Zigbee 스위치
     * 약간의 여유 시간

   이 모든 구성요소를 조합하여 네트워크 기반 자동화 접근 제어를 구현할 수 있음

계획

     * 버튼(예: Zigbee 플러그) 하나를 눌러 15분간 소셜 미디어 접근 허용
     * 이후 1시간 쿨다운이 적용되어 재사용까지 대기 시간 필요
     * 이 접근법을 통해 집안 구성원이 제한된 시간 동안만 자유롭게 소셜 미디어 이용 가능함

Adguard Home 라우터 설정

     * Adguard Home은 gli.net의 OpenWRT 커스텀 펌웨어에 통합되어 있음
     * 차단 또는 허용을 원하는 서비스들을 custom rule로 직접 등록해야 함
     * 내장 서비스는 토글이 불가능하므로 사용자 지정이 필수임

Adguard Home API 접근

     * Adguard Home은 OpenAPI 기반의 API를 제공함
     * API를 사용하려면 계정 및 비밀번호를 등록해야 하는데, 기본 인터페이스에서는 이를 지원하지 않음
     * 포럼 안내에 따라 /etc/Adguard/config.yaml 파일을 수정하여 별도의 API 접근용 비밀번호를 수동 등록 가능
     * 비밀번호는 bcrypt로 해싱하여 저장 필요

Home Assistant 연동

     * Home Assistant에서 Adguard Home의 제한적 기능 연동이 가능
     * ""Filtering"" 스위치 형태로 커스텀 필터 규칙 적용 여부를 자동화로 전환할 수 있음
     * 해당 엔티티를 활용해 사용자가 원하는 순간에 네트워크 접근 정책을 애플리케이션 차원에서 변경 가능

Home Assistant 자동화

     * 실제로 동작하는 자동화 예제가 적용되어 있음
     * 버튼을 누르면 규칙 비활성화 및 재활성화까지 전체 프로세스를 관리함
     * 실행 시간과 쿨다운 시간 모두 자동화에 포함되어 사용자의 습관적 사용량 제한에 도움을 줌

결론 및 소감

     * 와이파이 자체를 꺼버릴 경우 우회 가능성이 있음
     * 하지만 이 방식은 사용 패턴 자체를 절제하는 데 실질적 도움 제공
     * 독, 즉 중독적 요소도 소량이라면 참을만하다는 관점에서 부분적 접근 통제가 유익함

        Hacker News 의견

     * 산만함에서 벗어나기 위해 너무 복잡한 시스템을 만들고, 그 내용을 또 글로 쓰는 것은 정말 전형적인 Hacker News 스타일의 행동임을 느낄 수 있음
          + 문제 행동을 발견하고 외부적 제약으로 그 행동을 통제하는 것은 충분히 합리적인 방법이라는 생각임. 금연자가 담배를 들고 다니지 않는다고 비난하는 것과 비슷한 뉘앙스가 느껴짐
          + 요즘에는 복잡한 문제에는 현대적인 해법이 필요하다는 인식이 많음. 하지만 진지하게 말하면, 산만함과 집중력이라는 주제는 정말 어려운 주제임. 나는 동기부여가 강할 때는 어떤 방해도 통하지 않음. 반면에 지루한 일을 할 때는 그 무엇이든, 모든 게 방해가 됨
          + 기술로 인한 문제를 기술로 해결하고 싶어하는 심리, 삶을 지나치게 구조화해서 구조붕괴에서 회복하는 방법조차 구조화하려는 모습, 바로 우리의 현실임
          + 누군가 산만함에서 벗어나려고 너무 복잡한 시스템을 만들어 distractions에서 distractions를 받는 글을 읽고 거기에 댓글다는 게야말로 궁극의 Hacker News적 행동임 ;P
          + 작성자가 일부 스토리를 만들었거나, 아니면 HN 커뮤니티의 성향에 맞게 각색한 부분도 있을 거라는 생각이 들기도 함. 하지만 그 아티클은 길지도 않고 핵심에 집중하면서도 유쾌하고 유익했음. 작성자가 독일인인데 영어가 너무 유창해서 놀람. 정말 능력 있는 테크니컬 라이터임. 즐겨찾기에 추가함. 이런 글 더 보고 싶음
     * 휴대폰 중독 여부를 알기 위한 점진적 난이도별 실험 목록을 제안함
       1단계: 특별한 이유가 없는 날, 하루 종일 휴대폰을 서랍에 넣고 사용하지 않기
       2단계: 특별한 이유가 없는 날, 휴대폰을 서랍에 넣고 한 시간 이상 집 밖에 있기
       3단계: 친구 만나러 가거나, 점심 먹을 때, 장 보러 갈 때 휴대폰을 집에 두기
       4단계: 하루 동안 직장에 갈 때 휴대폰을 집에 두기
       5단계: 주말 전체 동안 휴대폰을 서랍에 넣고 사용하지 않기
       6단계: 여행할 때(휴가나 가족 방문 등) 하루 이상 휴대폰을 집에 두기
          + 이 방법들 좋게 느껴짐. 나도 휴대폰 없이 동네 산책하거나, 행사가 있을 때는 다운타운까지 빼고 나감. 처음엔 어색한데 곧 괜찮아짐. 작년에 인터넷 없이 여행을 했는데 너무 좋았음. 출발 전 TomTom 네비게이션의 오프라인 맵이 최신인지 확인함. 휴대폰은 비상용으로 차에 잠궈놓고 전원까지 꺼둠. 운전을 시작하면서 알림이 절대 오지 않는 평온함을 느꼈음. 실제로 그 알림들이 자주 오는 것도 아닌데, 그럴 수 있다는 사실만으로도 스트레스를 느꼈던 것 같음. 꺼두고 나니 큰 해방감이 있었음
          + 나는 심각하게 중독된 걸 알면서도 끊지 못하는 상태임. 술 중독자가 매일 보드카 한 병씩 마시는 것과 비슷함. 여러 번 끊으려 시도했지만 항상 실패함. 좋은 하루를 보낸 다음날 바로 다시 도로 돌아온 패턴임. 쓸데없이 유튜브 영상이나 물건 검색, 가질 수 없는 직업을 상상하는 데 하루에 5~10시간씩 쓰는 것 같음. 각종 차단 소프트웨어와 전략을 시도했지만 별 효과가 없었음. 복잡한 차단도 결국은 우회 방법이 생기고, 나중에는 아예 차단을 끄는 게 습관이 돼버림. 극단적으로는 집 인터넷을 끊고 폴더폰을 6개월간 쓴 적도 있었음. 화면 보는 시간은 줄었지만, 공공 와이파이가 있는 곳에서 시간 보내거나 의미 없는 TV 시청이 늘었음. 결국 기술적 해답은 없을 수도 있다고 느껴짐
          + 셀룰러 모델의 Apple Watch 사용 추천함. 비상시 연락만 받고 SNS나 산만한 앱엔 접근 제한을 둘 수 있음. 나도 애플워치 산 후 휴대폰 두고 나가는 일이 많아졌음
          + 나는 업무상 보안 구역에서 일하기 때문에 의도하지 않았지만 매일 4단계를 실천하고 있었음
          + 내겐 휴대폰이 기억 장치임. 꼭 휴대폰 없이 있어야 할 때는 반드시 노트와 펜, 그리고 아마 카메라도 챙겨야 함
     * Home Assistant에 깊은 애정이 있음. 예전에 아이가 구형 디지털 카메라를 갖고 놀게 했었음. 나중에 사진을 확인해보니, 한밤중에 집 외관을 찍으러 밖에 나간 사실을 알고 충격을 받음. 아이가 밤중에 스스로 대문을 열고 나가서 사진을 찍고, 다시 들어와 문을 꼭 잠그고 다시 침대로 돌아갔던 것임. 이 일을 계기로 무선 문 센서를 구입해 밤 10시~새벽 6시 사이에 문이 열리면 내 방의 등이 켜지도록 자동화함. 나중에는 모든 출입문과 창문에 센서를 붙이고 아이들이 쓸 수 있는 출입구 전체를 모니터링함. 이 출입구들이 도둑도 쓸 수 있는 곳이니 자연스럽게 방범 시스템도 되는 셈임. 방범 시스템 활성화 시 원격 알림까지 받아볼 수 있음. Home Assistant의 최고 장점은 특정 브랜드 앱이나 생태계에 종속되지 않는다는 점임. 우리 집의 문/창문 센서와 전구
       브랜드가 달라도, 하나의 앱으로 모두 제어할 수 있음
          + 내게 제일 흥미로운 부분은 아이가 밤에 집 밖으로 나가 사진을 찍었던 행동임. 혹시 왜 그런 행동을 했는지 아이에게 물어보았는지 궁금함. 어린아이 입장에서 위험을 감수하고 밤에 밖으로 나가 집 사진을 찍으려는 마음이 정말 대단한 호기심임. 우리 아이 중 한 명도 그런 괴짜스러움이 있었을 것 같긴 하지만, 어둠을 무서워해서 실제로는 행동하지 못했을 것임. 참고로 우리 집은 이미 방범 시스템이 설치되어 있었고, 아이들도 그 존재를 잘 알고 있었음
          + 어떤 문/창문 센서를 사용했는지 궁금함
          + 자신도 같은 경험이 있다며, Hyperbole and a Half의 ""richard"" 만화를 공유함
     * 내 휴대폰에서 효과를 본 방법은 OneSec 앱임. 이 앱은 단축어(앱용), Safari 확장기능(웹사이트용)과 연동되어 접근하려는 차단 콘텐츠 전에 짧은 과제(예: 20초간 호흡법)를 하게 유도함. 시간 지연과 짧은 활동이 내가 지금으로선 원하지 않는 행동임을 상기하는 효과를 줌. 정말 필요해서 해당 플랫폼에 접속하고 싶을 때는 그냥 운동을 하고 들어가면 접근 허가됨. 단점은 Safari 확장에 웹 브라우징 전체 권한을 줘야 한다는 점, 데이터 수집 안 한다고 주장하긴 하지만 신뢰는 각자 선택의 문제임. 앱 차단에는 개인정보 접근 권한은 요구 안함
          + 때론 제일 간단한 해법이 ‘루다이트 식’임, 그냥 휴대폰을 놓고 멀리 떨어져 있기임. 만약 그조차 도저히 불가능하다면, 이미 앱 하나 더 설치하는 것 이상의 근본적인 문제가 있을 수 있다고 생각함
          + iPhone에서도 비슷한 단축어를 만들어 사용할 수 있을지 궁금함
     * 스마트홈 시스템을 선정할 때 내 기준이 있음. 조작 계통은 반드시 AC 전원 기반이거나 혹은 UPS 백업(아니면 둘 다)이어야 함. WiFi/Ethernet이 끊겨도 동작해야 중요하다고 판단하는 항목들에 적용함. 예전 도어벨(Doorbird)은 기계식 차임과 연결이 됐지만, 현재 쓰는 Reolink 모델은 그렇지 않음. 대신 PoE 기반이고 UPS 시스템 연결로 신뢰성 보장함. Reolink는 전원 소켓에 바로 꽂는 별도 차임이 있고, 카메라도 훨씬 나음. 내 IoT기기들은 인터넷에 자유롭게 접근하지 못하게 하며, IoT용 VLAN을 두 종류(일반용, 조금 더 우려되는 기기용)로 구성함. 회사 IT처럼 관리함. Nessus로 보안 점검도 함. Home Assistant 여러 대 관리 중이고, 집/회사/고객사까지 돌보고 있음. 작성자가 고른 스마트 플러그는 일부러 불편하도록(하지만 신뢰성은 챙기게) 설계된 모습임. 모니터링 기능도
       포함되어 있을 것 같음. 이건 ‘제대로’ 하는 너드의 모습임
          + PoE to DC 전원 어댑터를 대부분 적용하기 시작했음. UPS를 유틸리티 클로젯에 집중시켜 오래 운영 가능함. 라우터, 스위칭 구성으로 집 전체 전원공급체계를 구축함. 보안카메라, PoE++ 스위치, AP 전부 PoE 기반임. UPS 배터리로 12~14시간 가능, 여차하면 ‘집 전체용 UPS’로 며칠 연장도 가능함. 광 NIU, 케이블 모뎀도 PoE 스플리터로 실어주는 방식임. 아직 변환하지 않은 몇몇 장치들도 있지만 요즘은 귀찮아서 방치함. 포트 수 적은 코어스위치 2개로 이중화해, 한 쪽이 나가도 반은 운영 가능임. AP 밀집도도 괜찮게 설계함. 망 전체 디바이스 수가 엄청나게 늘었음
     * 어떤 광고나 SNS보다도 YAML로 프로그래밍하는 게 뇌에 훨씬 더 치명적이라는 농담임
          + Home Assistant에서는 대부분의 자동화를 더 이상 yaml로 만들 필요가 없음. 본문의 소스코드를 올렸을지 모르지만 실제로는 그래픽 UI로 만들었을 수도 있다고 생각함. Node-Red도 사용할 수 있음
     * 왜 플러그를 쓰고 Zigbee 버튼은 사용하지 않았는지 궁금함
          + 그냥 집에 굴러다니던 플러그가 있어서 썼음, 버튼이 있어서 용도로 충분함. 오히려 장점도 있음. 작은 램프를 플러그에 꽂아둘 수 있고, 14분 후에는 플러그가 2초마다 켜졌다 꺼지면서 시간 종료를 알리고 다소 극적인 효과를 줌
          + 플러그는 버튼이 있고, 수동으로 켜면 이벤트를 발생시킴
     * Neil Chen이 소셜미디어 중독자를 위한 인터넷 필터 잠금 해제 아이디어를 올렸음을 공유함. 이 아이디어로 AdGuard Home 자동화를 만들어 같은 원리를 적용함
          + 멋진 작업에 감사함을 전하는 Neil Chen의 피드백
     * 지금은 네트워크 레벨에서 웹사이트 차단이 불가능함. 브라우저와 모바일이 하드코딩된 DNS 리졸버를 쓰기 시작해서 실효성이 떨어짐
          + 게이트웨이 아래로 흐르는 네트워크 트래픽은 모두 제어 가능함. DNSSEC가 더 어렵게 만들긴 해도, 결국 콘텐츠는 IP 주소에서 오므로 상위 네트워크 장비에서 드롭 가능함. DNSSEC에 대한 위키 설명
          + ""브라우저와 폰이 하드코딩된 DNS 리졸버 쓴다""는 것에 출처를 궁금해함. 내가 아는 한, 대부분의 폰과 브라우저는 OS가 DHCP로 받아오는 DNS를 그대로 따름
          + 폰에서 시도해봤던 내용을 공유함. 대부분 앱과 브라우저는 DNS로 충분히 차단 가능함. gli.net 등에서는 ""모든 클라이언트의 DNS 세팅 무시""와 ""DNS 리바인딩 공격 보호"" 설정이 있음. 이러면 라우터만이 유일한 DNS 리졸버가 됨. 수동으로 dig google.com @1.1.1.1 해도 라우터 결과만 나옴. DNS over HTTPS는 막지 못하지만, 이건 원래 막을 수 없게 설계되어 있다고 생각함
     * 나는 Mac에서 Alfred로 글로벌 단축키를 설정해, 누르면 시스템 DNS 리졸버를 1.1.1.1로 바꿨다가 DNS 캐시를 초기화함. 1분 혹은 10분 후 자동 복구됨. 내 디바이스 하나에서만 차단이 풀리고, 전체 네트워크에 영향이 없는 점이 마음에 듦. 아주 간단하게 구성할 수 있음
"
"https://news.hada.io/topic?id=21693","Snow - 클래식 Macintosh 에뮬레이터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Snow - 클래식 Macintosh 에뮬레이터

     * Snow는 Motorola 680x0 기반 Macintosh의 하드웨어 동작을 최대한 실제와 가깝게 재현하는 오픈 소스 에뮬레이터임
     * 그래픽 사용자 인터페이스(GUI) 와 강력한 디버깅 기능을 제공함
     * 기존 에뮬레이터와 달리 ROM 패치나 시스템 콜 가로채기를 최소화하는 방식임
     * Macintosh 128K/512K/Plus/SE/Classic/II 모델을 지원함
     * Rust 기반으로 개발되었으며, 다양한 운영체제에서 빌드 및 다운로드가 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 개요

     * Snow는 클래식 Macintosh(680x0 계열) 컴퓨터를 소프트웨어적으로 재현하는 에뮬레이터임
     * 사용자는 그래픽 인터페이스를 통해 실제 Mac을 조작하듯 사용할 수 있음
     * 디버깅 기능이 풍부해 개발용이나 분석에 유리함

동작 방식 및 특징

     * Snow는 가능한 하드웨어 수준(로우 레벨)에서의 완전한 에뮬레이션을 지향함
          + 이는 ROM을 패치하거나 시스템 콜을 우회하는 일반적인 방법 대신, 실제 하드웨어처럼 동작하게 함
     * 공식적으로 지원되는 모델:
          + Macintosh 128K
          + Macintosh 512K
          + Macintosh Plus
          + Macintosh SE
          + Macintosh Classic
          + Macintosh II
     * Rust 언어로 구현되어 효율성과 안전성을 강조함
     * 오픈 소스로 MIT 라이선스로 공개됨

체험 및 문서

     * 웹 브라우저에서 실행할 수 있는 제한적 데모 버전을 제공함
          + 단, 전체 소프트웨어의 모든 기능(특히 GUI 등)은 제공하지 않음
     * 자세한 설치 및 이용 방법은 온라인 문서를 통해 참고 가능함

다운로드 안내

     * 현재는 최신 개발 버전(bleeding edge build) 만 자동으로 제공됨
          + Windows 10 이상(x86 64비트)
          + macOS 11.7(Big Sur) 이상(유니버설)
          + Linux(Ubuntu 24.04, x86 64비트 및 ARM64)
     * 운영체제별로 즉시 다운로드 가능한 빌드 파일 배포 중임

문의 및 참여

     * GitHub 저장소를 통해 이슈 제기 및 기여 가능함

        Hacker News 의견

     * 클래식 Mac 시스템용 휴대 가능하고 사용자 친화적인 하드웨어 레벨 에뮬레이터가 큰 의미를 갖게 된 맥락에 대해, 2020년 블로그 글 공유 참고 https://invisibleup.com/articles/30/ 게임 콘솔은 Nestopia, bsnes, Dolphin, Duckstation 등 훌륭한 에뮬레이터가 오래전부터 존재해 온 반면, PC는 VMWare, VirtualBox와 같은 가상화 시스템이 대중적인 필요를 충족시켜 왔고, 최근에는 86Box, MartyPC 등 고정밀 에뮬레이터 등장 소개 Commodore 64는 VICE, Amiga는 WinUAE, Apple II는 KEGS와 AppleWin처럼 질 높은 에뮬레이터가 있지만, Mac 쪽은 Basilisk II처럼 고수준 추상화에 가까운, 다소 대충 비슷하게만 재현하는 정도의 에뮬레이터 위주였다고 생각
          + 호환성 면에서는 많이 부족하지만, Executor라는 대안도 존재 언급 https://en.wikipedia.org/wiki/Executor_(software) 브라우저가 MS-DOS를 에뮬레이션해, 그 위에서 Executor/DOS로 매킨토시용 솔리테어 게임 실행 가능한 데모 https://archive.org/details/executor Executor/DOS 외에도, 680x0 프로세서를 썼던 Sun 3 워크스테이션용 비공개 버전, NEXTSTEP 환경을 위한 Executor/NEXTSTEP도 있음 Executor는 Apple의 지적재산권을 일절 사용하지 않아 가장 호환성이 낮은 점 지적, ROM과 시스템 소프트웨어 대체물이 모두 클린룸 방식으로 새로 작성됨 구 버전의 Executor는 gcc 전용 확장자를 써서 오늘날 리눅스 빌드는 어렵거나 불가능할 수도 있음 Executor 프로젝트의 초기 버전을 직접 개발했으며, 성능 좋은 68k 에뮬레이터와 컬러 서브시스템 등은 훨씬 뛰어난 프로그래머가 개발했음
          + 해당 글 내용이 사실이지만, 세간의 무료 기여 커뮤니티 노력 자체를 지나치게 폄하하는 느낌 동의
          + MAME도 하드웨어 레벨로 Macintosh와 Apple II를 에뮬레이션한다는 점 강조, KEGS와 AppleWin보다 더 정확하고 주변기기 지원은 많지만, 사용자 친화성 면에서는 떨어짐
          + Macintosh II FDHD 에뮬레이터를 실행해 봤는데, 메뉴상에서는 400K/800K 플로피만 로딩하라는 메시지만 뜸, Snow 설명서에는 SuperDrive 2개를 지원한다고 명시됨 https://docs.snowemu.com/manual/media/floppies 그래서인지 지금까지 줬던 플로피 이미지를 전부 바로 꺼내버림, Mac II 호환 시스템용 800K System 7.1.1 디스크조차 인식 못함 Snow 자체는 잠재력이 많다고 보며 노고에 경의 표하지만, Mac 에뮬레이션 판은 여전히 여러 에뮬레이터들이 각자 지원하는 하드웨어와 기능이 들쭉날쭉한 상태, 각종 꼼수와 예전 맥 내부 구조 관련 선행 지식도 여전히 필요, 미래 지향적 약속만 가득한 느낌
          + MAME가 68k 기반 매킨토시도 어느 정도 지원한다는 정보 공유 https://wiki.mamedev.org/index.php/Driver:Mac_68K
     * 에뮬레이터 정확성 때문에, 아마 BasiliskII가 가진 결정적 기능 몇 가지는 빠져 있을 것 같음 BasiliskII는 OS와 ROM 패치로 초고해상도 지원, 호스트 파일 시스템 및 네트워크와의(대체로) 매끄러운 통합 등 다양한 기능 제공 하지만 이게 허술하거나 정확하지 않아서인지, 유저 경험 자체는 특유의 무결함은 없지만 제대로 동작할 때는 정말 쾌적한 사용성 보장
          + 정확한 에뮬레이터이면서 코드 베이스가 깔끔하면 이런 패치나 기능은 추가로 얹기 좋아 보임 Basilisk의 패치 코드도 직접 살펴봤을 때 실제로 복잡하지 않고, Executor(이 스레드에 저자 등장), MACE 등 부분적 Toolbox 재구현 예시도 있어 포팅하려면 분량이 꽤 되겠지만 원 코드 거의 그대로 옮기고 테스트 인프라 넣는 작업이면 충분해 보임
     * 맥용 ROM 파일 구하는 방법 조언 필요 구글에서 발견한 사이트에서 여러 개 다운받았지만, 에뮬레이터가 계속 ""알 수 없거나 지원하지 않는 ROM 파일"" 오류만 냄 쓸만한 ROM 찾기 방법 문의
          + https://macintoshgarden.org/ 이 항상 믿을만한 최고의 소스라고 자신있게 추천
          + Apple이 아직까지도 이런 옛날 ROM을 올리는 사람을 고소하는 현실이 꽤 황당하게 느껴짐
          + 아래 저장소의 ROM은 작동한다는 개인적 경험 https://archive.org/details/mac_rom_archive_-_as_of_8-19-2011 제공
     * 대학 졸업 초창기 작업물이 맥 포맷의 버누이(Bernoulli) 디스크들에 보관 중 해당 소프트웨어를 구동하려면 반드시 ADB 동글이 필요한데, 그래서 물리적 하드웨어 필수 궁금한 점은 ADB-USB 어댑터 중에서 에뮬레이터에 연결 가능하게 맵핑할 수 있는 게 있을지 여부
          + 지금까지 아는 ADB-USB 어댑터는 마우스와 키보드만 지원, 내부 펌웨어가 USB HID로만 매핑 가능 완전 패스스루 하려면 커스텀 펌웨어가 필요, 오히려 그 소프트웨어 복제방지 기술을 해킹하는 쪽이 더 쉬울 수도 있다는 의견
          + 아직 백업하지 않았다면 데이터 유실 위험, 소중하다면 최대한 빨리 확인 권장
          + 작동하는 Bernoulli 드라이브 가진 사람은 대체로 맞는 구형 맥 하드웨어도 같이 갖추는 경우 많다는 의견
          + 아래 제품이 도움 될지도 모름 https://www.bigmessowires.com/usb-wombat/
     * Rust로 재구현된 68K 에뮬레이터로, Musashi나 UAE처럼 널리 알려진 C언어 기반 CPU 코드를 전혀 사용하지 않은 점 강조
     * 일반적으로 쉽게 구할 수 있는 Mac OS 7.1 설치 디스크와 Mac Plus ROM 파일로 부팅 시도, 드라이브 0이 디스크를 계속 꺼내버림 Mini vMac은 잘 동작, 아직은 개선 필요 느낌
     * Mac SE나 II 등에서 HD20 지원이 ""비해당""으로 표기된 게 의아 모든 모델(II 제외)에서 ROM단에서 HD20 부팅 지원 있음 직접 Mac SE에서 HD20 에뮬레이터 사용 중, 여러 형태의 디스크 이미지를 Mac용, 플로피 에뮬레이터 양쪽 모두에 손쉽게 적용해 넣을 수 있는 매우 좋은 방법이라고 생각
     * Lisa처럼 Mac도 하드웨어의 ""사이클 정확성(cycle accuracy)""이 필요한지 궁금 Lisa의 경우 OS가 하드웨어 타이밍을 가정해, Qemu 같은 에뮬레이터로는 충족되지 않는 문제 언급
          + 초기 Mac은 IWM(디스크 II 컨트롤러를 집약한 칩) 사용, Apple II와 동일하게 사이클 일치하는 코드를 활용 커서 움직임이 갑자기 멈추는 현상은 60Hz 인터럽트 타이머가 디스크 쓰기 중에는 꺼져야 하기 때문 Andy Hertzfeld가 해당 이슈를 Folklore.org에서 언급한 일화 공유 https://www.folklore.org/Nybbles.html Apple II에서 있었던 특이한 디스크 복사방지 기법(나선 트랙, 다양한 크기 섹터, 다양한 nibbilization 방식)이 이론적으로는 Mac에서도 가능, 실제로 쓰인 사례가 있는지 호기심 표출
     * 정말 실감 나는 구현이라는 개인적 인상 공유 Atari ST 에뮬레이션도 가능할 희망 여부 질문
          + 이미 매우 수준 높은 Atari ST 에뮬레이터 Hatari 프로젝트 소개 https://github.com/hatari/hatari 그리고 Clock Signal(CLK)이라는 저지연 추구 에뮬레이터에서 Acorn, Amstrad, Apple II/II+/IIe, Atari ST와 2600 등 각종 고전기기까지 폭넓게 다룸 https://github.com/TomHarte/CLK
"
"https://news.hada.io/topic?id=21696","LLM 코드 생성이 신뢰 약화로 이어질 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       LLM 코드 생성이 신뢰 약화로 이어질 수 있음

     * 최근 LLM 기반 코드 생성이 개발자 사이에서 점점 더 사용되는 현상임
     * 자동 생성된 코드로 인해 코드 품질 및 신뢰성에 대한 우려 증대 현상임
     * 개발자들은 코드 이해 부족 및 검증 미흡으로 인해 프로젝트 유지보수 난이도 상승을 경험함
     * 신뢰할 수 없는 코드의 사용 확산이 전체 소프트웨어 생태계에 영향 미침
     * 기술 발전과 함께 신뢰성 확보 방안 마련 필요성 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   Jay는 자신의 블로그에서 최근 등장한 LLM(대형 언어 모델) 기반 코드 생성 기술이 소프트웨어 개발 현장에 미치는 영향을 다룸. 이러한 도구의 발전으로 개발 효율성이 향상되고 있지만 동시에 코드의 신뢰성과 품질 문제가 대두됨.

LLM 코드 생성 기술의 부상

     * 개발 현장에서 LLM을 활용한 코드 자동 생성 도구가 빠르게 확산 중임
     * 복잡한 기능 구현이나 반복적 코딩 작업에서 높은 생산성 제공 현상임
     * 신속한 프로토타입 제작 및 신규 언어 학습 부담 완화 장점 보유함

신뢰성 문제

     * LLM이 생성한 코드가 의도한 대로 항상 동작하지 않음 현상 발생함
     * 코드 내부의 의도, 설계 논리가 불분명해 이해와 검증 과정이 어려워짐
     * 리뷰 및 테스트 과정이 부족할 경우 예기치 못한 버그나 취약점 발생 가능성 존재함

프로젝트 유지보수 및 생태계 영향

     * 자동 생성 코드에 대한 문서화 부족과 설명 미흡 문제가 발생함
     * 개발자가 코드의 동작 원리 파악에 어려움을 겪어 유지보수 복잡성이 증가함
     * 신뢰성 있는 소프트웨어 개발 문화가 훼손될 위험이 있음

결론 및 제언

     * LLM 기반 코드 생성 기술은 혁신적이지만, 신뢰성 확보가 필수 과제임
     * 자동 생성 코드 도입 시 검증 강화와 체계적 코드 리뷰 필요성 강조됨
     * 장기적으로 컴퓨팅 생태계의 신뢰 보호를 위한 기준 마련이 중요함

        Hacker News 의견

     * archive.is 링크 공유, 구형 브라우저에서도 작동하며 JavaScript는 CloudSnare 우회를 위해서만 필요함
     * 내 친구가 항상 ""혁신은 신뢰의 속도로 일어난다""라고 말하곤 함. GPT3 등장 이후 이 말이 머릿속에서 계속 맴도는 중. 검증에는 높은 비용이 필요하고, 신뢰가 그 비용을 낮추는 주요 수단임. 그런데 LLM에서 신뢰를 쌓는 방법을 모르겠음. 굉장히 유창하게 코딩도, 자연어도 다루지만, 동시에 사람이라면 악의적으로 볼 수 있는 방향으로 헤매기도 하기 때문임
          + 글쓴이임: 그 인용구 정말 마음에 듦. 내가 몇 문단으로 설명한 내용을 아주 간결하게 요약함. 이제 모든 걸 일일이 검증해야 하는 세상은 솔직히 정말 지치고 느려짐
          + LLM 결과물에 완전한 신뢰를 가질 수 없음. 하지만 정제와 파괴력 제한은 가능함. 사용자 입력을 필터링하고, 침투 테스트를 하고, 비밀은 dot 파일에 숨기는 식으로 결국 ""모범 사례""와 ""SOC-AI 규제 준수"" 같은 표준에 수렴할 것. LLM은 무시하고 싶어도 너무 쓸모 있음. 신뢰 역시 한 단계씩 쌓이는 것임. 인간도 어차피 신뢰성에서 멀고, 자율주행차와 비교하자면 정해진 규칙 내에서 인간보다 더 버그 없는 코드 생산이 가능해질 것 같음. 그 뒤엔 복잡성만 차근차근 개선할 것임
          + ""혁신은 신뢰의 속도로 일어난다"" 이 말에 좀 더 설명이 필요함. 전기, 비행, 방사능을 처음 발견했을 때 그만큼의 신뢰가 있었는지? 과학에서 신뢰는 진행하며 쌓이는 것임
     * 회사에서 예상치 못한 식으로 이 주제를 경험함. 동료와 빠른 성과 압박 속에 내가 작업한 대형 리팩터링을 임시 PR 상태임에도 병합함. 그 후 테스트되지 않은 코드에서 버그 발생. 디버깅 중 동료가 내가 AI로 코딩했다고 생각했다는 점, 그리고 AI가 만든 코드를 사후에 이해하려 해서 답답해했다는 사실을 털어놓음. 그러나 그 코드는 꼼꼼하게 내가 직접 짠 손수 작업이었고, 단순한 API 변경과정에서의 작은 실수들이 버그의 원인이었음. 오히려 이 경험으로 내가 동료와 신뢰와 관련된 긴장을 자연스럽게 드러내고 건설적으로 대화할 수 있었음. 지금 돌아보니 이런 식의 신뢰 구축 경험이 의미 있었고, 환경이 달랐다면 훨씬 더 복잡하게 꼬였을 수도 있을 것 같음. 항상 조심해야 할 필요성 느낌
     * 나는 전제가 잘 이해가 안 됨. 누군가가 좋은 코드를 쓴다는 신뢰는, 그 사람이 직접 코딩해서 결과물이 좋았다는 실제 경험에서 온 것임. LLM을 써도 버그 없는 코드만 내면 신뢰하고, LLM을 써도 버그 있으면 신뢰 안 함. 그게 머리로만 썼을 때와 뭐가 다른지 모르겠음
          + 글쓴이임: 내 요지는 신뢰도가 중간 수준인 대규모 팀이나 오픈소스 등 낮은 신뢰 환경에서는 LLM이 제출된 코드만 보고 개발자의 실력 판단이 점점 힘들어짐. 상대방의 성향을 파악할 수 없으니 결국 ""완전 무신뢰""로 모든 코드를 꼼꼼하게 봐야 함. 그간 신속 리뷰에 썼던 단축키들이 더는 통하지 않는데, 이런 문화에 길들여진 조직에선 꽤 고통스러울 수 있음. 이미 신뢰 높은 팀이라면 이 문제가 전혀 공감되지 않을 수 있음
          + 예전에는 A=B 였다면 높아진 B는 A 역시 좋음을 뜻했음. 이제는 A+Ai=B 가 되어, B가 높아도 A는 높지 않을 수 있음. 그리고 지금 AI 상태는 확률적이어서, 오히려 아무것도 안 하는 것보다 못할 때도 자주 있음
          + ""잘 작동하는 코드만 신뢰""라고 했는데, 신뢰의 근거는 코드가 정말로 버그 없음을 개발자가 사전에 알기 때문임. 하지만 복잡한 시스템의 경우, 코드가 다른 부분과 어떻게 상호작용하고, 엣지 케이스가 발생할지 파악하려면 코드 작성자가 전체 맥락을 이해해야 함. 만약 LLM이 코드를 대신 썼고 개발자가 그 내용 전체를 이해하지 못하면, 결국 그 검증 부담은 리뷰어에게 넘어가서 과부하로 이어짐. 그게 논지임
          + LLM으로 코딩하면 몇 번 잘 되다가 자신감 과잉으로 테스트를 소홀히 하게 됨. 실제론 커뮤니케이션 오류에서 문제가 많이 발생함. 작업자는 전체 과제를 명확히 이해해도 LLM은 상태 유지가 어렵고, 맥락이 모호하면 말도 안 되는 가정을 하는 특성. 4o처럼 추가 정보를 계속해서 먼저 요청하는 접근이 모든 코드 생성 모델에서 표준이 되었으면 좋겠음, 진짜 많은 문제를 예방할 수 있을 것 같음
          + 작동 여부 외에도 다양한 요소로 신뢰를 쌓음. 변경사항을 명확하게 설명, 과거에 잘한 기여 이력, 적절한 변화의 단위(덩어리가 적당한 커밋 등), 문제의 우선순위 선정(버그부터 고치고 기능 추가), 기존 코드의 유지 관리 역량, 꾸준한 활동 등등 다 고려함
     * 이미 그런 시대임. ""이 점 간과해서 죄송, 말씀하신 게 맞음""이란 말 너무 많이 봄. 8~9번 중 10번은 봤음. 반면 LLM이 만든 코드를 의미 없이 복붙해서, 기대에 못 미치자 화내는 사람도 자주 봄. 사실 그게 더 나음. 명확하게 망가진 게, 겉보기에 제대로 돼 보이는 것보다 낫다고 생각함
          + 내 경험상, LLM은 요구사항을 충족시키기보다 테스트만 통과하게 코드를 수정하는 경향이 많음
          + 브라우저 챗봇으로 LLM을 쓰는 건가? 우린 직접 코드 접근이 가능한 AI agent를 쓰는데, 훨씬 말이 적고 실제로 주변의 주니어보다 뛰어난 작업도 많음. 짧은 구체적 작업을 잘 수행해 코드리뷰 정도만 필요해서 거의 바로 쓰려 함. 물론 prediction engine은 진짜 엔지니어링을 할 줄 모름. 예를 들어, 파이썬 generator를 명시적으로 요구하지 않으면 메모리를 엄청나게 잡아먹는 코드를 낼 때가 많음. 하지만 주변 파이썬 개발자도 비슷한 실수 많이 함. 오히려 이런 점 때문에 ""add feature""가 아니라 명확한 스펙 작성을 유도해 도움됨. AI agent가 제일 유용한 곳은 모두가 신경 안 쓰는 레거시 코드임. 예전에 팩스로 온 문서를 좌표 200개로 데이터 추출하는 시스템이 있는데, 30년 넘게 변함없이 쓰다 최근 문서가 바뀜. copilot이 좌표 수정하는데 30초 걸림. 사람이라면
            하루는 족히 걸릴 일임. 근데 이런 분위기의 코딩 시대에 어떻게 전문가가 될지 모르겠음
          + 8~9번 중 10번은 너무 과장임. 100% 만들어낸 통계임
     * 이전의 추상화 도구들은 복잡성을 줄이는 동시에 해당 추상화의 ""정확성""을 기본 전제로 삼았음. 물론 완벽하진 않았고, 버그도 있었지만, 구현체가 문제일 뿐 본질적 오류는 아님. 한번 패치되면 더 견고해지는 특성. 반면 LLM은 확률적 예측 엔진이라 일정 시간동안만 근사치 정확성을 보임. 이에 대해 글쓴이가 간과한 점은 불완전한 확률적 에이전트로도 신뢰할 만한 결정론적 시스템을 만들 수 있다는 것임. 예를 들면, 가비지 컬렉션 툴의 저자를 믿어서가 아니라 툴 자체가 충분히 테스트되어 원하는 대로 작동함을 증명할 때 신뢰함. 미래에는 테스트 주도 개발이 더 강해질 것 같음. 신뢰 대신 검증임
          + 자동화된 테스트로 모든 문제를 잡을 수 있다는 건 순진한 생각임. 동시성, 자원 관리, 보안 취약점 등 자동화하기 힘든 것 너무 많음. 그리고 테스트 자체를 누가 검증할지? 코드와 테스트가 각각 논리를 구현하는데, 가끔 버그의 원인은 코드가 아니라 테스트 쪽에서 드러남. 테스트도 무조건 신뢰해서는 안 됨
          + 글쓴이임: 여기서는 도구의 효과보다 도구 자체에 초점을 맞춰 말함. 예를 들어, 모델 자체가 직접 가비지 컬렉터 역할을 해서, 프로그램의 메모리 덤프를 받고 불필요 블록을 해제하는 구조라면, 그 판단을 영원히 신뢰하지 못할 것임. 아무리 ""패치""나 ""파인튜닝""으로 보완해도 한계임. JVM처럼 결정론적 출력에서 에러가 발생하면 한번 패치하면 그 에러는 영구히 사라짐. LLM은 그렇지 않음. 이 차이가 기존 추상화와 LLM 세계의 본질적인 분기점이라고 생각함. LLM이 산업에 미칠 영향이 크리라 보지만, 역사적 사례가 제한적이라 진짜 미지의 영역임
          + ""확률성 요인(엔트로피 머신)에서 신뢰할 만한 결정론적 시스템이 나온다""는 대목, 나에겐 꽤나 파격적으로 들림. 그리고 항상 TDD는 모든 문제를 해결해주는 만능 도구처럼 소개됨. 그런데 잘못된 테스트로 엉뚱한 소프트웨어를 만든 경험, 부끄러울 만큼 많이 봄
     * LLM에 대한 반감은 아무 소용 없음. 현재 LLM은 개발자 생산성을 높임. 적어도 경력이 적은 개발자들에게 더욱 유익함. 생산성이 크게 상승하는 도구는 누가 뭐라 해도 포기될 리 없음. 물론 큰 버그로 대형 서비스가 오랫동안 다운되는 등 피해 사례가 나타나도 생산성이 중요하면 기술은 멈추지 않음. 그 약점을 해결(완화)하며 기술과 함께 가는 길만이 현실적임. 그 과정에서 완화책이 생산성 향상을 해치면 우회하게 되고, 기술과 조화를 이루는 보완책이 정착될 것임
          + ""LLM이 개발자 생산성을 높인다""는 건 사람과 상황에 따라 천차만별임. 내 경험상, '생산성이 10배'라는 말을 하는 쪽은 주로 주니어 프론트엔드 개발자이거나 스타트업에서 자주 초기 앱을 만드는 개발자임. 물론 좋은 사례지만, 이런 개발자와 임베디드 C 시니어 개발자는 별도의 언어로 아예 딴 얘기를 하곤 함. 그래서 AI 생산성 논쟁은 서로 다른 맥락의 대화임. 그리고 ""합리적 활용""이라는 측면에선 AI agent 개념 자체가 좋은지 의문임. Copilot 사건 보면 MS와 AI 모두 조롱받는 꼴이었음. AI에게 자율적으로 작업 맡기는 접근이 마냥 영리하지 않을 수 있음. 비슷하게 블록체인도 암호화폐 극성기의 온갖 과장 사례가 있지만, Coinbase 같이 실제로 유의미한 니치에 정착한 경우도 있음. 2020년엔 IBM이 커피 원두 공급망을 블록체인으로 관리한다 주장(2025년 오늘
            봐선 트위터 농담 같지만 그땐 진심). 결국 현재 AI agent들, 그리고 생성형 AI의 다른 응용들도 나중엔 과한 hype의 사례가 될 수도 있음 Copilot 사건 포브스 기사
          + ""더 생산적""이라는 말이 반복적으로 등장하는데, 이건 인간/AI 조합이 결과적으로 사용자 요구에 더 부합하는지는 말하지 않고, 단지 ""더 많은 코드가 생산된다""는 뜻임. LLM이 2천 줄 코드를 삭제하는 PR을 만들었다는 이야기를 들어본 적 없음. ""엔지니어 생산성 향상""이라는 말은 사실상 더 많은 코드를 쓴다는 얘기임
          + 글쓴이 의도가 실제로 비판적이라는 건 오해임. LLM을 쓸지 말지 양자택일이 아니라, 위험 관리·완화에 초점을 맞춘 것임. 비유하자면, 자동차 개발을 반대한다기보단 자동차가 폭발 위험이 있으니, 그 폭발을 줄이는 데 더 집중하자는 얘기임
          + 나는 원글이 ""무의미한 푸념""이기보단, LLM과 협업 시 방심하기 쉬운 함정들과 팀 내 보완책 제시에 대한 현실적 고민이라는 느낌임
          + 예전에 React 처음 나왔을 때 안 배워서 후회된 기억 있음. GPT 거부감도 여전하고, 주변은 ""chatGPT가 그랬다"" ""이거 chatGPT 코드다"" 이런 말을 하는데 나는 직접 고생해서 코딩하는 데 자부심이 있음. GPT는 안 쓰지만, 사실 구글이나 stackoverflow는 느린 GPT로 생각할 수도 있음
     * ""기여 코드가 LLM 산물이 아닌, 오리지널이며 완전히 이해했다는 점을 약속"" ""대부분 수작업 요구"" 이런 정책보다, 결과물에만 집중해야 함. 기여자가 변경사항을 잘 이해하도록 요구하는 건 좋음. ""주니어는 일정 기간 LLM 툴 사용 자제 또는 금지"" 같은 정책은 별로임. 온보딩의 난잡한 환경 셋업 문제에 LLM이 큰 도움이 됨. 더불어 코드/문서 파악에 좋고, 유용한 텍스트 검색 및 요약 도구도 있음
          + 온보딩이 사실상 랜덤 환경 문제 해결 능력을 배우는 과정임. 모든 어려움과 복잡함을 다 자동화로 없애면, 나중엔 아무도 그런 상황에서 뭘 해야 할지 모르게 될 것 같음. 나만 그렇게 생각하는지 궁금함
     * ""AI Cliff""(LLM 정확도가 갑자기 급락하는 현상)라는 개념 처음 들었음. 다른 사람들도 경험한 적 있는지 궁금함
          + 나 자주 경험함. 코드 복잡성이 임계점을 넘으면 LLM이 맥락을 머릿속에 다 담지 못해 엉뚱하게 구는 현상. 내 역할은 LLM이 볼 복잡성을 관리하는 것임. 현 LLM은 시간이 지날수록 구조를 더 복잡하게 만드는 경향이 있음. 기본적으로 내가 리팩터 요청해서 단순화하거나, 너무 복잡해지면 내가 직접 정리함. 지금 LLM은 계속 맡기면 결국 거대한 루브골드버그식 혼란을 만들어내고, 이를 결국 사람이 청소해야함. 경험 많은 개발자는 LLM이 어디까지 바다로 데려가는지 빨리 눈치채고 조기 복귀 가능하지만, 초보라면 사건 자체를 인식하기도 전 완전히 길을 잃고 헤매게 됨
          + 일명 context drunk라고 부름. 입력 맥락이 1만 토큰, 99% 정답이라 치면, LLM이 1천 토큰의 답을 내서 90%만 맞음. 계속 서로 주고받다 보면 맥락창 대부분이 LLM이 내놓은 덜 정확한 출력의 반복이 됨. 에러가 누적되고, 최근 정보가 더 중요하게 작용해 점점 엉터리가 됨. 이 문제는 코드뿐 아니라 산문에도 나타남
          + 나는 context rot라고 부름. 맥락이 쌓이며 출력 품질이 떨어짐. 잡담이나 엉뚱한 내용이 많을수록 더 급속하게 품질이 나빠짐. 특히 chain of thought(COT)가 맥락에 남으면서, 사고가 해매면 그 흔적이 악화됨. 개인적으론 맥락 일부만 잘라내는 pruning 기능이 있길 바람. 난 직접 요약해서 새 세션으로 가져가는 식으로 context rot에 대응함
          + vibe coding처럼 채팅 인터페이스에서만 겪었고, 에이전트형 툴은 코드 맥락창을 자체 관리하며 dev tooling을 직접 실행해 sanity check를 하므로 이런 빈도가 훨씬 적음
          + 업무용 AI 세션은 자주 리셋해서 'AI cliff'를 느끼지 못하는 편임. 그러나 창작 소설 작업을 할 때는 맥락의 길이와 일관성이 중요해서, AI가 어느 순간 캐릭터 성격을 제대로 유지하지 못하고 이상하게 반응했던 적 있음. 되돌릴 수 없어 아주 낯선 경험이었음
     * 원글이 여러 사람 글을 요약하지 않겠다 선언하지만 실상은 그렇다는 것 같음. 그래도 PR에 AI 생성 코드 포함 파일 표시가 있으면 좋겠음. LLM 코드와 사람 코드의 실수 유형이 다르니 구분해서 리뷰할 수 있으면 시간 절약됨. 이런 정책을 대형 조직에서 경험해본 사람이 있는지, 또는 이미 자동화 툴이 있는지 궁금함. 기업들이 LLM 생성 코드 비율 측정한다면 세부 메트릭을 위해 그런 툴이 이미 있을지도 모르겠음
          + 글쓴이임: 실제로 팀 내 신뢰와 LLM 관련 논의가 많지 않아 명문화된 사례를 못 봄. 내가 잘못된 공간에서 일해서 그런 문제인지, 주류에서 발견하기 어렵기 때문인지는 잘 모르겠음
"
"https://news.hada.io/topic?id=21684","대체 레이아웃 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              대체 레이아웃 시스템

     * 이 스크립트는 고문서에서 사용된 방법을 모방함
     * 줄 마지막 단어와 텍스트 블록 끝 사이의 공간을 다양한 요소로 채우는 기능임
     * 단순한 선, 물결, 마지막 글자 반복, 구두점, 장식 등으로 채움이 가능함
     * 사용자가 원하는 글리프 또는 마지막 글자 반복으로 빈 칸 처리함
     * 독특한 레이아웃 구현 및 문서의 시각적 완성도를 높임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

대체 레이아웃 시스템 소개

   이 스크립트는 일부 고문서(원고 등) 에서 발견되는 특유의 레이아웃 방식을 현대적으로 재현함. 이러한 방식에서는 한 줄의 마지막 단어와 텍스트 블록의 끝 사이에 남는 여백을 다양한 시각적 요소로 채우는 것이 특징임.

기능 및 사용 방식

     * 줄 끝과 텍스트 블록 끝 사이 빈 공간을 간단한 직선, 물결, 마지막 글자 반복, 구두점, 장식된 대각선, 점 등 원하는 요소로 시각적으로 채울 수 있음
     * 채우기 방법 선택 옵션 제공:
          + 한 가지 이상의 글리프 선택
          + 해당 줄의 마지막 글자를 반복해서 사용

활용 목적 및 장점

     * 디자인 다양성 증대 및 고문서의 레이아웃 효과 재현 목적
     * 전통적인 필사본이나 고서 스타일을 디지털 문서나 웹 레이아웃 등에서도 적용할 수 있도록 지원
     * 문서의 시각적 일관성과 완성도를 높일 수 있는 도구임

        Hacker News 의견

     * ""Same Sizer""는 기계적으로 글자가 늘어나서 각 줄의 폭이 달라지는 점이 미관상 별로라고 생각함. 이상적인 베스트는 모든 줄이 같은 폭을 유지하고, 글자의 위치만 늘어나는 방식임. ""모든 글자가 같은 크기"" 원리는 베트남 서예에서 훨씬 잘 응용된 사례를 볼 수 있음. 서양 라틴 문자를 중국 서예처럼 동일한 정사각형에 나눠 써서 조화로운 느낌을 주는 예시가 있음 [예시 이미지](https://commons.m.wikimedia.org/wiki/…)
          + 예시 이미지를 보고 라틴 문자라는 점을 전혀 눈치채지 못했음. 중국어를 모르니 대충 비슷하게 생긴 건 다 ""해석 불가"" 영역으로 들어감. 만약 베트남어라도 알았다면 친숙한 단어와 음절을 알아볼 수 있지 않았을까 하는 흥미로운 효과임
          + 비슷한 맥락에서 정말 인상적인 서예 사례가 있음 링크
          + 보여주고 싶은 예시 링크가 깨져 접속이 안됨
     * ""Last Is First""는 텍스트를 베끼는 사람들이 위치를 잃지 않게 해주는 일종의 체크섬 느낌이라고 생각함. 토라(유대교 경전)를 배울 때, 일반적인 히브리어 인쇄체에서 모음 없는 손글씨 텍스트, 그리고 늘어진 글씨로 이동하는 게 매우 어려웠던 기억임. 여기에 더해 단어를 정확히 노래하듯 읽는 것도 같이 배워야 함. 그래도 잉크로 양피지에 손수 적힌 칼럼들을 보면 아주 아름다운 광경임
     * 가끔 너무 기괴하게 아름다운 걸 보면 그 안에 숨은 천재성이 보여서 순수한 기쁨만 남음. 정말 잘했음!
          + 소리 내서 읽어봤는지 물어보고 싶음. 갑자기 목소리가 완전히 로봇처럼 변함
     * 영어처럼 비음소적 언어에서는 이런 방식이 꽤 고통스럽게 다가옴. 특히 ""Last is First""에서 그런 현상임. ""I""를 보고 나서 사실은 ""In""이라는 걸 인지해야 하니 머릿속으로 되짚고 이해해야 하는 과정임. ""t""를 봤다가 나중에 ""that""임을 알게 되는 상황이라, 소리 내서 읽는다면 't'와 'th'같이 완전히 다른 음소를 다시 조합해야 하니 혼란스러움
          + 참고로 ""i.e.""는 ""즉, that is"" 의미이고, 이 경우엔 ""e.g."" (""for example, 예로"")가 더 적절한 표현임
          + 영어가 음소적이지 않다는 주장에 한마디하면, 영어도 결국 소리를 나타내는 글자 체계임. 글자가 다양한 소리를 가질 수 있어서 규칙적이지 않은 것뿐임. 음소적 기능이 없다면 애초에 이런 혼동조차 생기지 않았을 것임
          + 읽는다는 게 사실 한 글자씩 해독하는 게 아니라 패턴 인식에 더 가깝다고 느껴짐. 유명한 예로, 각 단어 알파벳 순서를 섞어도 사람들이 유창하게 읽을 수 있는 현상이 있음. 물론 모두에게 같은 방식으로 적용되는 건 아닐 수 있고, 각자 텍스트를 읽고 해석하는 나름의 방식이 있음. 관련 기사와 예제 텍스트도 있음 링크
     * ""Hyphenator"" 레이아웃에 여러 단어를 추가해서, 글자가 줄을 넘어가면서 점점 작아지는 효과를 원함. 대학 시절 노트 빽빽하게 채우면서 빈 공간에 글씨를 억지로 쑤셔넣던 경험을 디지털로 구현하고 싶음
     * 약간 심한 난시만 있을 뿐 거의 시각 문제는 없지만, Same Sizer 텍스트가 예상외로 훨씬 읽기 쉬웠음
     * 약시와 복시를 가진 입장에서 Same Sizer가 정말 쉽게 읽혀서 신기함. 보통 텍스트보다 더 편함. 이런 아이디어가 접근성 모드로도 활용될 수 있지 않을까 궁금함
          + 시각적인 문제는 없지만 Same Sizer가 생각보다 훨씬 더 읽기 쉽다고 느낌
     * ""Last is first"" 레이아웃을 그레고리오 성가 악보에서 자주 보이는 custos/custodes 개념과 비슷하게 느껴짐. 한 줄이 끝날 때 다음 줄의 첫 음을 미리 제시해주는 표식임. 시선이 다음 줄로 넘어가며 첫 시작음을 이미 알고 있으니 익숙함. 자세한 설명
     * 페이지가 몇 초마다 계속 새로고침되어서 제대로 감상할 수 없음. 매우 불편함
     * 정말 끔찍하지만 동시에 너무 사랑스러움
     * 제발 이런 식으로 웹사이트 만들지 말아줬으면 함. 1Gbps 네트워크 환경임에도 서버에서 감당하기 힘든 대용량 이미지가 계속 전송됨. 손바닥만 한 jpeg 이미지도 4K 해상도에 9MiB까지 올라가고, 어떤 페이지는 40MiB 넘게 로딩되어 15초 가까이 걸림. 서버가 느린 게 아니라 아예 사이트 자체가 너무 큼
          + 레이아웃을 볼 수 있는 링크가 없어서 당황했는데, 사실 그게 로딩 중이었음. 내 환경에서는 한 장 뜨는 데만 3분 넘게 걸림
          + 하지만 이번 경우엔 보통 때보다 더 정당화될 만한 입장임. 이 사이트는 엔드유저, 퍼포먼스, 전환율 최대화를 위해 만들어진 게 아니고, 타이포그래퍼를 위한 디자인 쇼케이스임. 모든 픽셀 하나하나가 중요하니 전문가들은 몇 초 더 기다리고서 결과물을 세밀하게 확인하는 게 더 의미 있음
"
"https://news.hada.io/topic?id=21654","Thnickels","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Thnickels

     * Thnickels 사이트는 현재 접속 불가 상태임
     * 사이트가 일시적으로 다운, 또는 웹 주소 변경 가능성 언급
     * ERR_HTTP2_PROTOCOL_ERROR 코드 표시됨
     * 접속 장애 원인은 제공되지 않음
     * 추가 정보나 내용 확인 불가 상태임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사이트 접근 불가 안내

     * https://thick-coins.net//… 주소의 웹페이지가 현재 일시적으로 사용 불가 상태임
     * 사이트가 다운되었거나, 새로운 웹 주소로 이동 가능성 제시됨
     * 브라우저에서 표시된 오류 코드는 ERR_HTTP2_PROTOCOL_ERROR임
     * 접속 제한으로 인해 추가적인 내용 확인이 불가함
     * 실질적인 사이트 내용이나 서비스 정보는 제공되지 않음

        Hacker News 의견

     * HTML 소스에 재미있는 주석이 숨어 있는 것 관찰. “Hello this is Theo. Yes I know how to do web design. 당신이 내 비밀 메시지를 찾음. 이건 동전 이모티콘 그림임.” 아래에는 동전을 형상화한 ASCII 아트까지 남아 있음
          + 이걸 “이모티콘”이라 하기엔 좀 억지라는 생각. 나는 이것을 오히려 ASCII 아트라고 부를 것
     * Theo로부터 직접 메시지 도착. 코인 선주문 처리를 위해 판매 잠시 중단했음 알림. 나중을 위해 남아있는 코인 일부 있음. 커다란 코인에 관심 주셔서 감사 인사 전달
          + 페이지가 마음에 들어서 더 많은 웹사이트가 이렇게 됐으면 좋겠음. 다음 번 주문 열리면 영국 배송도 가능한지 궁금
          + “전설 그 자체” Theo의 등장에 코인 하나 얻을 수 있는지 묻고 싶어짐
     * 1998년 레트로 스타일 디자인에 호감 느끼는 사람. 실제로 2025년에 만들어졌는데도 사진에 찍힌 컬러 노이즈나 글자에 보이는 jpeg 노이즈까지 완벽히 재현. 아이러니하게도 이 artifact는 2025년이 되면 오히려 더 개선된 부분인데 그 점까지 신경 써서 만듦
          + 이런 노이즈도 좋지만 구식 감성이 전체 사이트 분위기에 더 잘 어울림. 애니메이션 GIF 사용까지 확인. 이 사이트의 레트로 룩을 정말 좋아함. 예전에는 이런 웹사이트가 대부분이었던 시절이 그리움. 요란한 크리스마스트리 스타일을 피한 디자인도 맘에 듦. 게다가 정말 빠른 로딩 속도
          + 1998년 스타일이지만, 현대의 대부분 “모던” 웹사이트보다 이런 사이트가 더 좋음. 내 다른 화면에 Teams를 켜놓았는데, 진심으로 그리고 리소스 낭비는 생각도 안 하고, 이런 웹사이트가 심지어 “전문적”이라 불리는 애플리케이션보다도 더 멋지게 보임
          + Geocities 이미지를 wayback machine 캐시로 핫링크하는 것도 발견. 사실 이미지 핫링크 자체가 90년대 웹 감성과 굉장히 잘 어울림
     * 관리/모더레이터에게 Beehiiv 링크 id를 링크에서 제거하길 제안. https://thick-coins.net/
          + HN 유저들에게 추천하고 싶은 방법도 있음. 브라우저에서 이러한 추적 링크를 자동으로 제거해주는 “ClearURLs” 확장 https://addons.mozilla.org/en-US/firefox/addon/clearurls/이나 비슷한 것 사용 권장. 또는 적어도 게시 전에 직접 URL 확인해서 불필요한 파라미터 수동으로 삭제할 것
     * Theo에게 프리오더 관련해 연락 시도했지만 BBS 다운 상태임. 연락하길 원하면 netmail로 메시지 보내달라고 요청. 명시적으로 Fido 1:342/76과 MJNet 11:420/69 주소 남김. “Windows는 바이러스가 아님, 바이러스는 뭔가 하거든요”라는 구절도 같이 남김
          + Theo가 망치로 직접 코인 주조 중이라 아마 바쁠 것이라는 추측
          + Theo가 Mr. Prod에게 직접 답변. 메시지 이해는 잘 안 되지만 이메일이나 이 곳에서 연락해도 괜찮다고 회신 남김
     * 자신의 집 위로 나무가 쓰러진 뒤로 집이 너무 바람이 많이 부는 상황. 나만의 thnickels을 갖고 싶음. 펀치카드를 눌러둘 무거운 무엇이 필요함
          + Three Wolves 티셔츠 구매 추천. 비슷한 처지의 누군가가 티셔츠를 입고 집 옆 빈 초원에 섰더니 15분 만에 여성 10~12명이 몰려오는 에피소드 전함. punch card나 쿠폰이 바람에 날리지 않도록 이 여성들이 함께 앉아 그의 모습 감상. 쿠폰은 만료되었지만 여성들은 여전히 머물러 있음
     * Theo의 주조 공장 소개 문구에 감탄. “내 새로운 state of some art 작업장에는 여러 도구와 강력한 일꾼(나 자신)이 갖춰져 있음”이라는 귀여운 자기소개 언급
     * 이 프로젝트가 Alan Wagner라는 truewagner의 유머라는 추측. https://truewagner.com/ 링크 제공. 그의 소셜은 구글에서 직접 찾아봐야 함
          + 실제로는 Legboot라는 사용자가 만든 것으로 보임. Legboot와 truewagner, 두 사람 모두 정말 독특한 작품을 만듦. 둘은 동일 인물은 아님. https://x.com/LegbootLegit
          + 직접적 증거는 못 찾았지만 좋은 추측이라고 생각함. Legboot와 Wagner가 서로 비슷한 작품에 종종 댓글 남김. 이 사이트는 Wagner의 “Williamcoin” 프로젝트의 진화로 보이기도 함. 비교 링크: Williamcoin Reddit , Thick coins big update Reddit
     * “이 사이트는 자랑스럽게도 FrontPage 98로 제작됨”이라는 구절 인용
          + “built with love”가 아니라면 진짜 올드스쿨임을 느끼게 함
          + 초등학교 3학년 때 FrontPage 98과 스캐너로 할아버지 사진 사이트 제작했던 추억 회상. 시청 유틸리티 사이트에 호스팅했는데 archive.org에 스냅샷이 없어서 지금은 사라진 추억이 된 것 아쉬움
          + 9학년 시절 도서관에서 FrontPage로 팀 프로젝트 웹사이트 만들었던 추억 소환. 이런 아이디어를 낸 교사가 진정한 테크 신봉자 세대 X였을 것. 그래도 재미있던 경험이라는 회상
     * 인터넷이 예전엔 이랬다는 점이 그립다는 감상
"
"https://news.hada.io/topic?id=21667","Apptainer - 리눅스용 어플리케이션 컨테이너","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Apptainer - 리눅스용 어플리케이션 컨테이너

     * 단순함/속도/보안을 중심으로 설계된 오픈소스 컨테이너 플랫폼
          + HPC(고성능 컴퓨팅) 와 공유 시스템 환경에 최적화됨
     * 불변(Immutable) 단일 파일 컨테이너 이미지 포맷을 제공하며, 암호화 및 서명 지원
     * 격리보다는 통합 사용성에 집중해, 클러스터/서버 환경에서 GPU, 고속 네트워크, 병렬 파일 시스템을 바로 활용할 수 있음
     * OCI(Open Containers Initiative) 레지스트리에서 모든 컨테이너를 가져올 수 있으며, Docker와 호환성을 극대화
          + Docker Hub에 있는 대부분의 컨테이너를 변경 없이 풀(pull), 실행(run), 빌드(build)할 수 있도록 지원
     * 기존 Singularity에서 이름을 변경해 Linux Foundation 프로젝트로 이관됨
     * SIF(Singularity Image Format) 기반의 단일 파일 컨테이너로 손쉽게 이동, 배포 및 공유 가능
     * 컨테이너 내부와 외부의 사용자 권한이 동일하며, 기본적으로 호스트에서 추가 권한 상승 불가한 안전한 보안 모델 적용
     * BSD 라이선스

   Hacker News 의견 에서 언급되는 unregistry 기사:
   Unregistry – “docker push”를 레지스트리 없이 서버에 직접 전송 | GeekNews

        Hacker News 의견

     * 우리 팀은 실리콘 디자인/검증용 컴퓨트 클러스터에서 Apptainer를 시도해봤음, 하지만 최종적으로는 전통적인 TCL(Lua로 전환) 모듈로 돌아감
          + 여러 문제점을 겪었음.
               o 첫째, 컨테이너끼리 서로를 사용할 수 없음. 예를 들어 Make, GCC, Git 같은 도구가 각각 다른 Apptainer에 있으면, Make 안에 들어가면 GCC를 못 보는 상황 발생
               o 둘째, 컨테이너 내부에 의존하는 산출물이 있으면 제대로 동작하지 않음. GCC Apptainer로 프로그램 빌드 시 빌드된 바이너리가 Apptainer 내부 라이브러리에 연결되어 실행이 안 됨, C 헤더 문제도 발생
               o 셋째, PATH 값이 계속 꼬여서, Apptainer 외부의 필요한 경로나 도구를 못 보게 되는 문제 반복
               o 전체적으로 아이디어는 좋았으나 실제 사용성에서는 번거로움만 심해서 결국 옛날 OS(RHEL8) 를 직접 사용하는 게 훨씬 수월했음
          + 나는 Apptainer/Singularity를 Docker와 비슷하게 생각함(단, 네트워킹 설정은 풀로 없음). 이런 문제는 전통적인 Docker 컨테이너에서도 동일하게 발생함.
               o 나의 HPC 워크플로에서는 Apptainer를 Docker의 드롭인 대체품처럼 쓰고, 이런 용도엔 잘 맞음
               o Apptainer의 가장 큰 장점은 Non-root권한 컨테이너라는 점임. 덕분에 복잡한 네트워킹을 못하긴 하지만, HPC 같은 멀티테넌트 환경에서 훨씬 보안적임
          + 컨테이너 앱을 쓰면서 가장 큰 불만이 컨테이너답게 동작한다는 거라면, 그건 컨테이너의 본질임
          + 컨테이너 조각들을 섞어 쓰면 안 됨. 마치 서로 다른 리눅스 배포판의 바이너리를 섞어 쓰지 않는 것과 같음
               o 컨테이너는 하나의 통합 환경에서 개발에 활용하는 게 이상적임. 컨테이너는 격리된 환경이기 때문에, 어떤 것을 컴파일해도 결과물은 자기 컨테이너에 있어야 함
               o 단, 동일한 베이스 이미지를 여러 개 생성해서 파일 호환성을 확보하는 방법도 가능함(필요한 모든 의존성을 포함시킬 때만)
     * Apptainer가 주목받는 것이 반가움. 일부 상황에서는 Docker, Podman 등보다 뛰어남
          + 여러 작업을 한 컨테이너에서 실행해야 할 때(이건 다른 컨테이너 기술에서는 권장X)
          + HPC (그리고 일부 대학 환경)
          + 단일파일 배포 모델 지원(물론 델타는 미지원)
          + 별도의 외부 서버 없이 SIF 파일 암호화 서명 가능
          + 강력한 GPU 지원
     * docker도 docker save와 docker load 명령어로 단일 파일 배포가 가능함.
          + 델타를 지원하진 않지만, 최근 ""unregistry""라는 솔루션이 HN에 링크되었는데 이는 Docker Registry 없이도 ""docker push"" 기능과 델타 반영이 가능함
     * Apptainer 및 singularity ce 모두 HPC에서 흔히 사용됨. 두 제품 모두 옛날 Singularity 프로젝트에서 분기됐지만 완전히 똑같진 않음
          + 우리는 여러 슈퍼컴퓨터(HPC)에서 singularity를 쓰고, 일부 연구자들은 로컬에 Apptainer를 설치해 사용함
          + 최근 Python 코드(matplotlib, xarray 등)에서 타임존 버그를 발견했는데, singularity ce에선 문제가 있었으나 Apptainer에서는 제대로 동작했음
          + 신규 Apptainer는 코드 베이스가 비슷하지만, 버그 수정이 더 빠르게 반영되고 있음. 예시로, singularity는 사용자 타임존을 시스템에 덮어써서 문제가 발생했음
          + 참고 링크: singularity 이슈 #3686
          + Apptainer는 옛 Singularity 프로젝트의 포크가 아님. Apptainer가 원래의 본 프로젝트고, 커뮤니티 투표로 이름만 변경됨. Linux Foundation 소속으로 넘어감
               o Sylabs가 원래 개발자를 영입해서 프로젝트를 포크한 사례가 singularity ce임
               o 참고: community announcement
          + 그래도 상호 컨테이너 호환성은 유지되어, Apptainer에서 빌드해도 Singularity에서 실행 가능함 (그 반대도 마찬가지)
     * Apptainer는 곧 Singularity임. 관련 논문은 여기
          + 대학이나 정부 클러스터에서 공유 시스템을 쓸 경우, Apptainer는 항상 있지만 Podman/Docker는 거의 없음
          + 이런 환경에서는 컨테이너 사용 대신, 시스템 관리자와 친해져서 해당 클러스터의 운영 방식을 이해하는 것이 더 유리함
          + 왜 Docker/Podman은 덜 쓰이나, 그리고 왜 컨테이너 사용을 피하는 것이 좋은지 궁금함. 혹시 성능 문제가 이유인가
     * Flatpak이 OSTree에서 컨테이너 기반으로 전환하려 함. 유지 보수되는 컨테이너 툴링이 큰 장점이라고 함. 그런데 이게 Apptainer와 어떻게 다를까 궁금함
          + 아마도 Flatpak이 xdg-dbus를 통한 퍼미션 세분화 등 개별 앱 샌드박스 제어를 해서 네이티브처럼 쓸 수 있게 하는 것이 특징임
               o Apptainer는 이 정도로 완전히 분리/격리되는지 확실치는 않음
               o containertoolbx 같은 툴을 활용하면 컨테이너 방식 차이가 별로 의미없어짐
               o 솔직히 툴끼리의 기능 겹침이 많지만, 이 자체도 괜찮다고 생각함
     * 내가 사용하는 환경에서 Apptainer 사용의 최대 목적은 배포, 격리, 소프트웨어 가용성과 관련 없음.
          + 우리 HPC 클러스터는 각 사용자마다 inode 쿼터 제한이 있는데, 그 때문에 파일이 아주 많은 소프트웨어(예: Anaconda) 설치가 힘듦
          + 하지만 Apptainer 이미지는 squashfs 기반 단일 파일이라 인도 쿼터 걱정 없이 여러 개를 둘 수 있음
          + 동일한 소프트웨어를 일반적으로 설치하는 게 더 쉽긴 하지만, 쿼터는 순식간에 소진됨
     * Havoc 의견에 공감함. 애매한 메시지임: Apptainer가 Desktop용 Flatpak 대체품인가, 아니면 서버 목적용인가 헛갈림
          + 서버용임. 그러나 질문 자체가 애매함
               o Apptainer는 고정(immutable), rootless 컨테이너에서 CLI 앱 실행용임
               o 가장 비슷한 툴은 Fedora Toolbx임
               o Apptainer 주 용도는 과학 컴퓨팅 툴 배포와 재사용. root 권한 없이 사용하고, rootfs를 컨테이너마다 변경 불가하고, 자동으로 작업 디렉토리 마운트, GPU도 잘 지원함(이건 직접 테스트X)
               o 참고: Fedora Toolbx
     * ""Apptainer""라는 이름은 발음이 어색하고 뭔가 올바르지 않은 느낌임
     * 개발자라면 격리용 컨테이너 도구를 찾고 있을 수 있음
          + 나는 Podman 기반으로 각기 다른 개발 프로젝트를 격리하는 도구를 만들어봄. 보안 테스트나 사용에 필요하다면 코드 또는 블로그 글에서 확인 가능함
          + 왜 toolbox로 충분하지 않았는지 궁금함
               o 나는 toolbox가 프로젝트별로 개발 환경을 설치할 때 여러 숨겨진 파일시스템을 관리할 필요가 없어서 괜찮았음
     * SLURM 클러스터, 루트 권한 없는 서버에서는 매우 유용함
          + 나도 SLURM 클러스터에서 사용해 본 경험 있음
               o 공식 문서가 잘되어 있어서 입문용으로 충분함
               o 다만 fakeroot나 sudo가 없으면 로컬에서 Apptainer를 빌드하여 서버로 직접 전송해야 하는 번거로움이 있었음
"
"https://news.hada.io/topic?id=21628","GitHub CEO, AI 붐 속에서도 “수작업 코딩은 여전히 핵심”","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 GitHub CEO, AI 붐 속에서도 “수작업 코딩은 여전히 핵심”

     * GitHub CEO Thomas Dohmke는 AI 도구 확산에도 불구하고 수작업 코딩 능력의 중요성을 강조함
     * AI가 코드를 생성해도 개발자가 직접 수정 및 검토해야 효율적임을 주장
     * 자동화만 의존할 경우 실질적인 비효율과 생산성 저해 위험이 있으며, ""Vibe coding"" 처럼 AI 코드 남용 시 품질과 보안 리스크가 증가함
     * AI와 인간 개발자의 하이브리드 접근법이 가장 효과적임을 산업 트렌드와 사례로 뒷받침하며 설명
     * 개발자 역할은 사라지지 않고, AI와 협업하며 전략적 문제 해결 및 설계 역량이 요구되는 방향으로 진화 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GitHub CEO, AI 시대에도 수작업 코딩 중요성 강조

   GitHub CEO Thomas Dohmke는 소프트웨어 개발 현장에서 AI 도구 활용이 확산됨에도 불구하고, 수작업 코딩 능력 유지의 중요성을 강조함
     * “The MAD Podcast with Matt Turck”에 출연하여 개발자는 AI가 생성한 코드를 직접 수정하는 역량을 가져야 생산성 저하를 방지할 수 있음을 설명함
     * 효과적인 워크플로우로서는 AI 도구가 코드를 생성해 Pull Request를 제출하면, 개발자가 스킬을 활용해 즉시 수정하는 접근법임
     * 오로지 자동화 에이전트에 의존할 경우, 자연어로 단순한 수정을 설명하는 데 불필요한 시간을 소모할 수 있어 비효율 발생 위험이 언급됨
     * Dohmke는 “프로그래밍 언어로 이미 직접 할 줄 아는 작업을 굳이 자연어로 설명하려 시도하는 게 가장 비효율적인 선택”임을 지적함
     * OpenAI 공동창립자 Andrej Karpathy가 언급한 “vibe coding”은 AI 생성 코드에의 과도한 의존을 의미하는 용어로, Dohmke가 이에 대해서도 주의를 촉구함

인사이트 및 업계 동향

  1. AI 코딩의 최적 해법은 하이브리드 전략

     * Dohmke의 관점은 AI 자동화와 인간 프로그래밍 스킬의 결합이 최적임을 주장하는 업계 컨센서스와 일치함
     * Deloitte 연구에 따르면 개발자들은 AI 도구를 주로 보일러플레이트 코드 작성을 위해 사용하지만, 인간의 최종 검토를 유지함으로써 10~20분 생산성 향상 효과를 얻음
     * AI가 생성한 코드의 약 절반에는 부분적 오류가 있어, “신뢰하되 검증” 전략이 업계 표준으로 자리잡고 있음
     * Google은 전체 코드의 25% 이상을 AI가 생성하지만, 여전히 인간 개발자의 적극적 리뷰와 개선 과정이 필수임을 경험함
     * 이런 균형 잡힌 접근은 AI의 한계 인식과 함께 개발자의 전문성이 대체가 아닌 증강되는 방향으로 업계가 성숙해지고 있음을 반영함

  2. 개발자 역할은 변모 중이며 사라지지 않음

     * AI로 인해 프로그래밍 직업이 사라지기보다는, 개발자 역할이 순수 코더에서 AI 기반 개발 프로세스 관리자로 변화하고 있음
     * 전문가들은 개발 직군이 AI 활용 제품 엔지니어와 고급 시스템 품질/보안 보장 아키텍트로 양분될 것이라 전망함
     * 이 변화는 효과적인 AI 활용법, 전략적 문제 해결능력, 고수준 설계 역량 등 새로운 능력의 필요성을 의미함
     * 소프트웨어 엔지니어 부족 현상과 함께 AI 도구의 주니어 개발자 지원 효과가 입증돼, 숙련 개발자에게도 새로운 성장 기회를 제공함
     * 이는 과거 개발 도구와 추상화 기술 등장 때처럼, 인간의 창의성은 여전히 필요한 흐름임을 시사함

  3. “Vibe coding”의 생산성-품질 딜레마

     * “Vibe coding” 현상은 AI 생성 코드의 생산성 이점과 품질/보안 한계를 드러내는 트렌드임
     * AI 도구는 빠른 프로토타이핑과 반복 개발을 지원하지만, 코드 품질 저하와 보안 리스크 우려도 증가시키는 결과임
     * 실제 사례에서는 AI 코드의 미검증 사용에 따른 보안 취약점이 발생하기도 함
     * 비전공 창업가 중심 스타트업 등에서는, AI 코드 남용이 기술 부채를 쌓아 장기적 성장 저해로 이어질 수 있음
     * 대형 IT기업 경험상, 자동화와 엄격한 품질 관리의 균형이 AI 도입의 핵심으로, 스타트업에도 이 교훈이 중요함

        Hacker News 의견

     * 사람들이 왜 시스템의 본질적 복잡성에 대해 더 이상 이야기하지 않는지 매우 의문임
       Fred Brooks의 No Silver Bullet에서는 소프트웨어 엔지니어링의 진짜 어려움은 핵심적인 문제 공간을 이해하고, 명확히 하고, 모델링하는 데에서 온다고 지적함. 도구의 한계 같은 우발적 복잡성은 부차적임
       최근에는 AI 에이전트가 엔지니어 대신 자연어 프롬프트로 전체 코드베이스를 만든다는 이야기가 많음. 그런데 이 가정은 사양(specification) 문제를 이미 해결했다고 보는 착각임. 실제로는 모호한 아이디어를 상세하고 튼튼한 시스템으로 바꾸는 게 여전히 엔지니어의 핵심 역할
       누군가가 세부 명세를 제공하고 AI와 반복적으로 작업해서 소프트웨어를 만든다면, 사실상 AI로 우발적 복잡성을 없애는 것. 이는 개발자가 어셈블리에서 고급 언어로 전환한 것과 비슷함. 엔지니어를 대체하는 게 아닌 생산성을 높여줄 뿐임. 반복 작업의 비용을 줄이고 더 넓게 영향력을 확대할 기회도 생김
       만약 에이전트가 프롬프트만으로 제품을 만든다면, 누군가 이미 명확하게 시스템을 정의해 준 덕분임. 그리고 AI로 기존 제품만 복제한다면, 기술적 문제 해결이 아니라 유통이나 비용 경쟁으로 가는 것이고, 이는 엔지니어링 혁신이 아니라 비즈니스 혁신.
       내가 뭔가 놓치고 있는 부분이 있을까 궁금함
          + 사양(specification) 작업이 AI 등장 훨씬 전부터 소홀히 취급된 이유가 핵심임
            이해관계자(고객, 관리자)들은 예전부터 그냥 대충 감으로 코딩을 시켜왔음. 대충 설명하면 누군가가 기적적으로 그에 맞는 솔루션을 내주는 식임. 솔루션이 완전히 동작하는지 아무도 모름. 그냥 어느 정도 동작하는 것 같으니 넘어가는 경우가 많음
            대부분의 경우는 프로그래머가 도메인 지식을 기반으로 구체화함 (예: 올바른 폼 제출 페이지란 어떤 모습인지 본능적으로 알기 때문)
            이제 상대방이 AI로 바뀌었을 뿐, 이런 방식이 과연 그대로 반복될 수 있을지는 지켜볼 문제임
          + 본질적/우발적 복잡성 구분은 AI가 소프트웨어 개발 어디까지 맡을 수 있을지 생각하는 데 매우 중요한 통찰임
            많은 개발자들이 왜 AI로 대체되지 않을지 말로 설명은 못하지만, 본능적으로 느끼는 부분이 이 지점임
            실제로 나도 Claude 같은 에이전트에게 엄청 복잡한, 외부 비즈니스 로직이 얽힌 대형 코드베이스의 문제를 해결하려 시도해봤음. 하지만 AI는 비즈니스 요구나 맥락을 제대로 직감하지 못해서 비즈니스적인 코드 변경은 못함. 단순히 컨텍스트가 좁은 코드 변경이나 도와줄 수 있음. 즉, 우발적 복잡성만 처리 가능하고, 진짜 개발자의 역할인 실제 요구사항을 시스템으로 번역하는 데에는 한계가 있음
            덧붙이자면, 실제로 많은 개발자들이 해결하는 건 기술적 문제가 아니라 유통/시장 문제일 수도 있음. AI로 주니어를 대체하는 건 아직 불안함. 가장 큰 이슈는 AI가 자체적으로 자기 수정을 못 한다는 점임. 그럼에도 불구하고, AI 기반 사업체가 등장해 기존 비즈니스 비용을 낮추려는 시도는 계속될 것임. 그 변화가 좋을지 나쁠지는 일자리에서 밀려나는 사람 입장에서는 의미 없음
          + ""내가 뭔가 놓치고 있나?""에 대한 답이 있음
            실제로 소프트웨어를 쓸 줄 모르는 개발자가 엄청 많음. 이들은 AI로 쉽게 대체될 것임
            예전 경력에서는 JavaScript로 일했고, 정말 놀라운 일을 하는 사람들은 취미로 개발을 해온 사람들뿐이었음. 회사에서는 대부분 사람들이 화면에 텍스트 띄우는 것도 힘겨워했음. 농담이 아님
            많은 사람들이 거대한 프레임워크에 도전했지만, 결국 복붙만 하다가 겨우 동작하게 만드는 수준임. 고급 복잡성을 해결한 척 하지만 거의 다 불필요한 작업이거나 코드 자랑임
            오리지널 앱을 만들거나, 문서 작성, 실제 사용성을 측정하는 역량이 거의 없음
            이것을 어떻게 해결하냐고? 법조인처럼 바 시험 같은 높은 기준을 도입해서, 기준을 못 넘는 사람은 과감히 배제하고, 주니어나 견습생으로만 채용해야 개발자 세대가 제대로 성장할 수 있음
          + ‘놓치는 부분’에 대한 쉬운 답은, 업계에 ""No Silver Bullet""을 읽은 사람이 없다는 점임
            기술부채, 시스템 아키텍처에 대한 글을 쓴 사람들은 실제로 팀/비즈니스를 좌우하는 의사결정자들이 아니며, 그런 책들도 엔지니어에겐 선택사항임
            AI로 코딩 대체 이야기를 주도하는 사람들은, MVP 만드는 것과 10년간 코드베이스 확장 및 레거시 개선하는 것을 구분하지 못하는 경우가 많음
            예를 들어, 한 매니저는 하루에 3개 프로젝트에 각각 33% 시간 분배하자고 전형적인 잘못된 제안을 한 적이 있음. 직원 할당이나 시간 구조화는 관리자의 역량이어야 하는데, 결국 제대로 처리하는 건 엔지니어 몫임
            이제 그런 관리자들이 ""AI에 모든 기술부채/테스트 해결시키자""고 제안하고, 정작 제대로 안 되면 설명하는 것도 엔지니어가 함
            복잡성 이야기는 실상 관리 부실 문제의 반복일 뿐임. 이미 대다수 경험 많은 엔지니어들은 본인 일이 간단한 프롬프트로 대체될 거라 믿지 않음
            우리가 진짜 이야기해야 할 화두는 ""소프트웨어 엔지니어링은 관리 문제가 심각하고, AI가 그걸 더 부각시킨다""임
          + 많은 비개발자 혹은 학생들은 소프트웨어 개발에 복잡한 툴 사용법을 익혀야 한다고 느끼고, ""명세만 잘 만들면 누구나 소프트웨어를 만들 수 있다""는 약속에 끌림 (명세 능력 자체가 매우 어렵고 지원 기술이 필요하다는 점은 쏙 빼놓고)
            예전 노코드도 이런 식이었고, 실제로 노코드 툴은 제한적이고, 기능이 강력할수록 더 복잡하고 전문적인 학습이 필요함
            LLM을 이용한 SWE 대체론도 결국 ""시스템을 배우는 대신 자연어로 프롬프트 하면 모델이 알아서 툴을 써준다""는 버전임
            이렇게 보면 요즘 말하는 vibe coding이 사실상 이런 목표의 극치임 (다만 유지보수성 등 실질적 약점 존재)
            내가 보기에 매니저가 SWE를 없애고 싶어하는 이유 중 하나는 SWE와 제대로 소통하지 못해서임
            LLM을 사용하면 ""너드(개발자)"" 없이도 기계와 소통할 수 있으니 그 문제를 푼다고 보는 경향이 있음
     * 프로그래밍 언어는 원하는 프로그램의 정밀한 명세에 적합한 매체임. 자연어는 결코 그런 수준의 명확성을 가지지 못함
       그래서 AI가 생성한 결과를 리뷰하고 편집하는 건 당연함. 때에 따라서는 변경 내용을 설명하는 것보다 직접 수정하는 게 오히려 더 쉬움
       Copilot이 오류율 증가시킨다는 독립 연구 결과들이 자연스럽게 신중한 태도를 퍼뜨린 요인 아닐지 궁금함
       AI를 파는 사람들은 인간 저자가 곧 필요 없어질 거라고 주장하는 경우가 많음
          + Transformer는 자동화 테스팅, 명세 확장, 신규 프로젝트 가속, 모르는 API 탐색, 초기 기능 구축, 코드 리뷰 등 다양한 일에 적용 가능함
            실제로 코드가 명세의 올바른 매체라 치더라도 Transformer는 자연어와 그 미디엄(코드) 사이의 자동화 인터페이스 역할을 함
            최근 Transformer는 방대한 지식 덕분에 코드 생성에 문제가 없음
            인간이 프로그래밍에서 자동화를 추구하듯, Transformer는 거대한 도약임
            미래 어느 시점에는 AI로 인한 프로그래머 대체가 현실이 될 수 있음 (과거 자동 직조기, 인쇄술, 전자 계산기가 인간 작업을 대체한 것처럼)
            다만 지금 당장, 또는 10년 안에 일어나진 않을 수도 있고, 앞으로는 환각(hallucination), 정확도, 도구화, 인프라 구축 등 과제가 남아 있음
            프로그래밍 일자리 유무는 도메인, 실력, 보상 기대치에 따라 달라질 수 있음
            AI 도구를 진지하게 받아들이고, 적응하는 게 중요함. 그렇지 않으면 퍼스널 컴퓨팅이나 인터넷 도입 시기처럼 변화에 뒤처질 우려 있음
          + AI의 의사창조성에는 한계가 있다고 봄
            모든 LLM 학습 결과가 다시 LLM 입력으로만 순환된다면, 새로운 아이디어 범위는 절대 넓어지지 않음
            인간은 계속해서 그 범위 밖을 넘나드는 창의성을 보임
            미래에는 LLM이 그 벽을 넘어설 수도 있겠지만, 지금으로선 ‘원숭이 타자기’와 크게 다르지 않음
          + 내 경험상 LLM은 스캐폴딩(발판 도구)처럼 활용했을 때 가장 효과적임
            내가 만들고 싶은 기능을 뇌내 덤프처럼 전달하고, 그러면 모델과 그 모델에 필요한 기본 컨트롤러를 만들어달라 요청함
            남은 것은 뷰와 실제 비즈니스 로직에만 집중하면 되어 업무 분담이 명확함
          + 과거 고급 언어가 처음 등장했을 때, 극초기에는 지금 LLM이나 자연어 코드 비판과 비슷하게 ""메모리 직접 제어가 어려워서 정밀도가 부족하다""는 식의 비판이 있었던 걸로 앎
            문제는 자연어가 정밀성이 불가능하다는 게 아니라, 대부분 사람들이 정밀하지 않게 요구를 설명하거나, 본인이 원하는 바만 명확하고 실제 컴퓨터가 해야 하는 일을 제대로 설명하지 못한다는 점임
            그 결과, 엔지니어가 비즈니스 요구를 번역할 때 엄청 추측하거나, LLM이 그 역할을 하면서도 더 적은 맥락밖에 이해 못 하는 상황이 반복됨
          + AI의 최선 활용법은 내가 처음 보는 API나 귀찮아서 하기 싫은 기능에 막히지 않고 플로우 상태 유지하는 것임
     * AI는 코드 전체에 공통 패턴을 빠르고 효율적으로 적용할 수 있지만, 본질적으로 '무엇을 하고 있는지 스스로는 모름'
       최근 경험을 공유하자면, 팝업 크기 계산과 위치 결정 관련 코드를 LLM에 리팩터링 시키려 했음
       하나는 ""if"", 하나는 ""switch""로 작성했는데, LLM은 두 방식이 다르다는 점에서 전혀 유연하게 반응 못 하고, 명확히 설명해도 두 곳 모두 if 또는 switch로 통일해버림
       LLM은 이전 토큰의 패턴을 계속 유지하는 경향이 있음
       여기에선 큰 문제가 아니지만, 약간만 더 복잡한 상황에선 세밀함을 무시하고 표면상 그럴듯한 코드를 내놓음
       대신 작은 단위로 쪼개서 명확히 명령하면 LLM이 상당히 효율적임. 예를 들어, “m_StateStorage에 사이즈 저장 후 렌더 단계에서 적용” 같은 지시는 쉽게 수행함
       특히 Cerebras처럼 몇 킬로바이트짜리 코드도 빠르게 처리할 수 있는 모델과 함께라면, 내 생각을 실제 코드로 직접 입력하는 것보다 훨씬 빠름
          + AI라는 용어는 그 자체로 '인텔리전스'를 내포하니 어느 분야, 어느 수준까지 할 수 없다 단정할 수 없음
            결국 오늘날 평가하는 건 트랜스포머 모델의 현재 성능에 국한된 비판임
            이 업계는 워낙 급변하고 있기 때문에, 오늘의 한계가 한 달 뒤에 의미 없어질 수도 있음
            “LLM”도 엄밀히 말하면 모호한 표현임. 최신 트랜스포머 모델은 멀티모달이며 텍스트뿐만 아니라 여러 형태의 데이터를 다룸
            따라서 굳이 비판한다면, 어떤 모델/툴/패러다임을 구체적으로 지적해야 논의에 실효성이 있음
     * “소프트웨어 엔지니어 부족” 및 “AI가 주니어 개발자에게 특히 도움이 된다”는 연구 결과에 대해
       내가 사는 세계선(현실)에서는 테크 취업 시장이 최악이고, AI는 오히려 주니어가 성장해야 할 곳에서 경험을 빼앗아 불리하게 작용함
     * 최근 Claude로 재미있는 경험을 했음. Zed에서 “71번 줄 에러를 고쳐줘”라고 명령했더니 91번 줄에서 쓸데없는 포맷이나 바꿔줌
         1. 91번 줄엔 에러가 없었음,
         2. 더 중요한 건 내가 지정한 줄을 무시했다는 점임
            마치 LLM과 전화 게임하는 느낌이었음
            이렇게 간단한 수정도 직접 하는 게 빠름. 이 다른 경험으로 “LLM은 진짜 생각을 안 한다”는 느낌을 다시금 확인함
          + LLM이 라인 넘버 인식이 형편없음
          + 이런 경험으로 “LLM은 줄번호를 셀 수 없다”는 교훈을 얻음
            다음엔 “함수 XYZ에서 에러 고쳐줘”라거나 해당 줄을 직접 붙여넣어서 지시하면 더 나을 것
            그리고, 물론 LLM은 생각하는 게 아님. 단지 거대한 방정식일 뿐임
          + 이번 스레드엔 AI로 코딩 처음 해보는 사람들이 많아 보임
          + 오퍼레이터 실수일 수도 있음
            LLM에 제대로된 컨텍스트를 줘야 함. 라인 숫자는 부적합한 컨텍스트임
     * 내 기준에서 소프트웨어 엔지니어의 역할은 요구사항을 소프트웨어로 바꾸는 것임
       소프트웨어는 단순히 코드가 전부가 아니고, 요구사항도 단순 자연어로만 주어지는 게 아님
       현시점, AI와 함께 일해도 내 속도가 AI보다 더 빠르지 않음 (단순 작업, 소규모 소프트웨어 제외)
       현재 AI는 내 기준에선 주니어/미드 레벨 개발자 감임
       최근 2년간 AI가 체감상 획기적으로 더 좋아지진 않았음
          + 대부분 요구사항은 명확하게 문서화되어 나오지 않음
            비즈니스 로직이 뭔지 아는 사람도 거의 없음
            그러다 보니, 소프트웨어 엔지니어가 직접 찾아가서 물어봐야 하는 상황이 잦음
            소프트웨어의 성장 방향과 어떻게 설계해야 그 확장성을 챙길 수 있을지 고민하는 경험과 직관도 요구됨
            LLM이 이런 역할을 일부라도 할 수 있을 거란 상상이 안 감
          + 명확한 요구 사항이 주어진 소프트웨어 프로젝트는 단 한 번도 본 적 없음
            프로젝트의 절반은 ‘진짜 원하는 게 뭔지 파악하는 일’임
          + LLM은 아직 주니어 레벨조차 못 미침
            혹시라도 현존 AI가 귀사 미드 레벨 개발자 수준이었다면, 그건 회사의 채용 문제임
     * 컴퓨터의 가장 큰 장점 중 하나는 신뢰할 수 있고 재현성 높은 자동화임
       프로그래밍 언어 같은 형식 언어는 원하는 자동화 요구를 불명확함 없이 명확하게 전달할 수 있음
       자연어는 그만큼 정밀하지 못함
       프로그램의 진짜 근거(ground truth)는 결국 코드임
       인간이 프로그램의 동작을 정확히 통제하고자 한다면, 코드를 이해하고 수정할 수 있는 역량이 가장 중요함
     * “매뉴얼(수동)”이라는 단어는 부정적 뉘앙스가 있음
       해당 기사에서 의도한 건 ""인간 코딩이 여전히 핵심""임
       GitHub CEO가 정말로 ""manual""이라는 단어를 쓴 건지 확실치 않음. 더 중립적이거나 정확한 단어 선택이 있는 기사 소스가 있으면 좋음
       인간 코딩을 “manual”로 폄하하는 것은 바람직하지 않음. 인간 개발자도 AI 외 다양한 자동화 툴킷을 사용함
          + “수동 사고”만큼이나 부정적 일 수 있음
          + “manual”의 부정적 의미는 새로 알게 됨
            요즘은 수동 작업에 그렇게까지 반감을 가지는지 궁금함
          + “manual coding”과 “human coding”의 구분이 뭔지 궁금함
     * “AI 에이전트에만 의존하면 비효율적일 수 있다”
       예를 들어, 단순 변경을 자연어로 길게 설명하는 것보다 직접 코드 편집이 훨씬 빠른 경우가 많음
       따라서 AI 에이전트와의 적극적 상호작용이 최적의 워크플로우가 될 것임
          + 변경 내역을 자연어로 생각하다가, 입력하기도 전에 이미 내가 필요한 직접 변경 방안을 머릿속에서 떠올리는 경우가 많음
            컨텍스트가 많은 변경일수록 나 스스로 agent보다 더 빨리 수정하게 되는 것 같음
          + 얼마나 적극적인 상호작용이 괜찮은지 궁금함
            최근 에이전트 툴링 스타트업에 합류했는데, 내부적으로 이런 부분을 많이 논의함
            “솔직히 이거 잘 못하고 있어!”라고 에이전트에 지속적으로 피드백 주는 것은 괜찮다고 보지만, 어떤 부분은 피곤해질 수도 있음
            어떻게 생각하는지, 워크플로에 대한 상상이나 피드백이 추가로 궁금함
     * AI는 아직 기대만큼 도달하지 못했음
       예를 들어, 나는 자주 잘못된 참조 자료나 명세 때문에 고생함. 이건 AI가 구식 데이터로 학습됐고, 실시간 업데이트 능력이 부족해서인 듯함
       현존 LLM과 GI 솔루션은 모든 단계(n-step)를 한 번에 풀려다가 오히려 효용이 떨어짐
       1단계~i단계 정도만 제대로 처리해줘도 인간 입장에서 훨씬 더 도움이 될 것
       아직 내가 원하는 완전 모듈형 AI(예: 내 github 스타일을 반영해서 a, b, c 리소스만 참고해서 문제 x 해결) 솔루션을 보지 못했음
       그리고, 문제를 순차적으로 탐색하며 중간중간 더 많은 질문을 던지고 대화하는 코딩 AI가 나오길 기대함
     * AI와 코딩에 대해 다소 색다른 방향으로 의견을 말하는 CEO가 인상적이라고 느낌
       일반적으로 CEO나 투자자는 “전체 코드의 30% 이상을 AI가 쓴다”며 개발자 역할 축소를 예견하곤 하는데, 실제로는 같은 개발자들이 더 빨리 코드를 작성할 수 있게 도구를 쓰는 것뿐이라는 진단
       실제로 코드 작성 자체는 소프트웨어 개발 업무의 일부에 불과하다는 점 강조
          + 그는 맞는 이야기를 하고 있다고 보지만, 결국 ‘사람 중심 코드’ 사업에 있는 본인의 이해관계가 반영된 입장이라고 생각
          + GitHub 수익 모델이 개발자 수에 달려 있으니 이런 입장 취하는 것이 당연
"
"https://news.hada.io/topic?id=21652","Gemini CLI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Gemini CLI

     * Google이 Gemini CLI라는 오픈 소스 AI 에이전트를 새롭게 공개함
     * 이 도구는 명령줄 환경에서 직접 Gemini 모델의 인공지능 기능을 이용할 수 있게 지원함
     * 개발자는 코드 생성, 문서 요약, 번역 등 여러 작업을 CLI에서 바로 수행 가능함
     * Gemini CLI는 확장성, 사용자 정의, 오픈 소스 접근성이 특징임
     * 기존 AI 에이전트 대비 편리성과 생산성 향상이라는 장점을 가짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gemini CLI 소개

     * Google은 Gemini CLI를 공개하여 개발자가 명령줄 환경에서 인공지능 기능을 쉽게 사용할 수 있도록 지원함
     * Gemini CLI는 Google의 Gemini 모델을 기반으로 하며, 일상적인 개발 작업 자동화, 자연어 처리, 번역, 코드 생성, 문서 요약 등 다양한 AI 기능을 통합적으로 제공함
     * 오픈 소스로 제공되어 자유롭게 커스터마이징 및 확장 가능하며, 개발자 커뮤니티의 요구에 맞춘 다양한 플러그인 및 확장 개발이 가능한 환경을 가짐
     * 기존의 LLM 기반 명령줄 도구에 비해 단순한 통합 및 높은 편의성을 제공하며, 활용 예시로는 코드 오류 수정, 개발 문서 자동화, 데이터 분석 등 여러 업무가 언급됨
     * Gemini CLI는 빠른 AI 접근성과 실시간 활용성, 개발자 환경에 자연스럽게 통합될 수 있는 구조로 생산성 향상에 기여함

        Hacker News 의견

     * Google의 Gemini 제품들이 너무나 파편화된 상황에 혼란스러움 발생. Pro 구독자인 나도 ""Gemini Code Assist Standard""나 ""Enterprise""를 써야 더 많은 사용량이 제공된다는 사실을 이제야 알게 됨. 이런 요금제나 기능 차이에 대해 알지 못하던 평범한 구글 유저로서, 구글에 Gemini 구독비를 내더라도 ""Gemini CLI""에서는 별다른 혜택을 못 받는 어이없는 상황 경험
          + Google도 Microsoft처럼 거의 모든 분야에 제품을 갖고 있으면서, 혼동스러운 메시징 때문에 각각의 장점이 희석되는 느낌. 나 역시 Gemini 2.5 Pro를 좋아하지만, 일상용 AI 챗 어시스턴트를 찾으려 한동안 여러 AI 제품도 시도함. Gemini Pro 구독은 Google One에 포함되어 구글 드라이브와 쓸 때 좋지만, 이미 iCloud 구독과 iOS 환경에 맞춘 데이터 연동 때문에 넘어갈 이유 없음. Gemini 챗 UI는 OpenAI ChatGPT macOS 클라이언트보다 한참 뒤떨어짐. NotebookLM은 문서 요약에 강점이 있지만 Gemini 챗과 통합되어 있지 않아, 구글 제품들 사이 계속 오가야 하는 번거로움이 남음. 결국 Raycast AI에 구독비를 내고 있는데, 워크플로에 잘 녹아든 통합 경험과 다양한 모델 시도가 가능해서임. 최신 모델이 바로 제공되진 않지만 일관된 사용성 만족. 이렇게 구글이 다양한 제품에 분산된 탓에,
            사용성 면에서 오픈AI(일반 업무), Anthropic(코딩)보다 뒤처지는 상황. 최근 수개월 간 Google Stitch, GH Copilot/Cursor용 VSCode 플러그인, Claude Code 등으로 따라잡으려 했지만, 다들 금방 사라질 듯한 사이드 프로젝트 분위기만 풍김
          + 품질은 매우 높지만 Google Cloud Dashboard는 복잡해서 대부분의 스타트업이나 개인 개발자는 Google 아닌 다른 솔루션을 택함. Vertex로 모델을 호스팅하는데, 구글 클라우드와 차이가 뭔지 불명확. 프로젝트 레벨별로 API도 두 가지나 있음. AI 제공 업체라면 규모와 상관없이 진입 장벽 없이 쓰게 해줘야 하는데, Google AI Studio API에서 시작했다가 프로젝트가 커지면 Vertex API로 강제로 옮겨야 하는 구조로 확장성 없는 API 솔루션 설계. OpenAI 호환 API도 자주 작동하지 않아, 이를 쓴 여러 툴들이 제대로 동작하지 않음. Google AI 라인업, 즉 Jules와 Gemini CLI의 구분, Vertex API와 AI Studio API 차이, Vertex가 Google Cloud에 종속되어 앱 개발 시 환경 변수 설정 등 복잡함으로 인한 진입 장벽 존재 Vertex 환경 변수 공식 문서
          + Google의 가격 정책이 이해하기 힘든 수준. Gemini 2.5 Pro가 내가 써본 모델 중 최고라고 여겨질 정도인데, Claude나 Cursor처럼 한 번에 전체 기능을 쓸 수 있는 심플한 구독제가 존재하지 않음. 엔터프라이즈 사용자 쪽은 OpenAI가 완전히 점령
          + 매월 300달러 AI ULTRA 멤버십도 있음. Google One 멤버십조차 어떤 ""추가 기능""을 주는지 시시각각 변하는 느낌이라 명확하게 안내받기 힘듦
          + Anthropic도 마찬가지. 구독하면 Claude는 쓸 수 있지만 Claude Code는 별도의 ""API 사용량""으로 과금되어 구독과 분리된 요금 체계. 어떤 누군가가 CLI를 선호하는 우리를 파악하고 이를 따로 요금 매기는 법을 배운 듯. GUI로 우회도 가능하지만 터미널에서처럼 네비게이션이 직관적이지 않아 불편함
     * 이 프로젝트에 참여 중인 당사자임. 현재 도입 곡선이 가파른 상태로, 오늘같이 TPU가 열심히 일하는 날에 모두의 피드백을 소중하게 읽는 중. 버그 제보나 기능 요청은 언제든 환영
          + 어제 오후, 루비(모르는 언어)로 작성된 알고리즘을 바닐라 자바스크립트로 변환하려 애썼음. GPT-4.1로 시도했으나 시간만 잡아먹어 실패. 호기심에 Gemini CLI 설치해 루비 프로젝트 지정 후 단 한 번의 요청으로 금방 변환 성공. 모든 과정이 5분 만에 끝나 놀라움
          + Google Workspace with Gemini 유료 조직 계정인데, GOOGLE_CLOUD_PROJECT 환경변수 없음 안내 메시지 출력. GCP를 쓰지 않아 추가 안내가 없으면 환경 변수 값 획득이 직관적이지 않음. 유료 사용자가 일반 구글 사용자보다 접근성이 떨어지는 점, 문서 개선 필요
          + Apple Container on M1에 Gemini CLI가 생성한 코드를 성공적으로 연동. CodeRunner에서 Gemini CLI 옵션 적용 방법 설명
          + 더 소비자 친화적인 구독제(예: Claude Max처럼 Gemini CLI와 Gemini 앱 결합, IP 준수 및 API 접근 포함) 출시 요청 기대
          + Google Gemini, Gemini Ultra, AI Studio, Vertex AI, Notebook LLM, Jules 등 비슷한 기능을 하는 제품들이 너무 많아 사용자 안내 및 라이선스 체계 혼란 유발
     * Claude Code (4 Opus)로 대용량 Rust 코드베이스에서 그럭저럭 성과를 냈지만, 복잡한 작업에는 아쉬움 존재. Gemini CLI도 오늘 시도해봤는데 설치는 쉬웠지만 결과는 처참. Rust 코드 수정 후 컴파일 성공률에서 Claude보다 확실히 별로였음. 다만, Gemini가 ""코드를 엉망으로 만들었으니 모든 변경을 원상복구하고 새로 시작하겠다""는 셀프 리셋 멘트를 남겨 오늘의 웃음 포인트 제공
          + Gemini의 실패 방식이 오히려 재미 포인트. 코드 고친 게 안 먹힐 땐 ""이건 예상 밖이네요"" 같은 인간스러운 멘트와 함께 ""테스트 통과할 것 같아요!"" 하고 스스로 자신감 넘치게 선언. 너무 과신하는 기본 성격과 감탄사 과다 사용도 눈에 띔. 아마 훈련 과정에서 결과 단정이 더 좋은 결과를 만든다고 학습된 듯
          + Google 내부의 대규모 코드베이스로 Gemini가 트레이닝된 덕을 보고 있지만, Google 내 Rust 활용이 미미(좋은 C++ 툴링 때문)에 Rust 쪽에선 Gemini가 상대적으로 약하다는 가설
          + 비슷한 경험. 앱 신규 기능 테스트 중이었는데 완전히 꼬여버림. 정의 안 된 함수 사용, 몇 번의 오류 반복 끝에 포기. Claude는 무난하게 완수했지만 코드 퀄리티는 미흡했고, Gemini는 개별 아이디어는 참신했지만 일관성 결여로 마지막에 완성 못함
          + 나도 시도해봤는데 15분 만에 똑같이 ""모두 원상복구""하는 반응을 받음
          + Claude도 상황이 너무 나빠지면 애초로 되돌아가는 시도를 함. 내부적으로 편집이 뒤죽박죽될 때 이런 경우 목격
     * Gemini Code Assist를 사용하면 내 코드 데이터가 전부 Google로 전송됨(공지사항 링크), 프롬프트, 관련 코드, 결과물, 사용 피드백, 기능 사용 정보 등 모든 데이터가 수집되어 Google 서비스 및 머신러닝 개선에 쓰임. 품질 개선을 위해 사람이 직접 데이터 열람, 주석 추가, 가공 가능. 개인정보 보호를 위해 계정 정보에서 분리 후 최대 18개월 보관. 민감한 정보나 외부에 공개를 꺼리는 데이터는 입력하지 말 것을 권장
          + 이 부분은 좀 더 복잡함. 무료판 Code Assist는 수집 데이터가 기본 활용되지만 별도안내 절차를 통해 opt-out 가능. 유료 Code Assist는 데이터가 모델 개선에 쓰이지 않음. pay as you go 계정의 Gemini API 키로 쓰더라도 마찬가지로 수집되지 않음. 실제 민감한 데이터 활용 범위는 원글보다 한발 더 완화
          + 팀에서 Gemini CLI를 어떻게 로그인하느냐에 따라 개인정보 보호 정책이 헷갈린다는 지적에 공감. 논란을 해소하기 위해 각 계정별 서비스 약관·데이터 정책을 일목요연하게 정리한 문서 및 FAQ를 새로 정리( 문서 링크)
          + Gemini 생태계에서 가장 답답한 게 바로 투명하지 못한 프라이버시 정책. 2.5 pro가 최고 모델이라 업무에 쓰고 싶어도 프라이버시 조건이 너무나 혼란스럽고 실제로는 아무런 보호도 못 받는다는 가정하에 움직이게됨. 비싼 최상위 구독자임에도 마찬가지
          + Mozilla와 Google이 Gemmafile이라는 대안 출시. Gemma는 단일 파일로 로컬에서 돌아가는 완전 오프라인(open-airgapped) Gemini 버전으로, 의존성 없는 독립 실행 지원. 다운로드 2025년 기준 32%의 조직이 실제 배포( 보고서 )
          + Gemini CLI의 configuration.md 문서에 ""수집하지 않는 정보"" 섹션에서 개인 식별 정보, 프롬프트 및 응답 내용, 파일 컨텐츠는 로그로 저장하지 않는다고 명시
     * Gemini CLI의 시스템 프롬프트는 Gist 링크에서 공개. 관련 개인 노트는 여기
          + Gemini CLI는 오픈소스라 Github 저장소에서 시스템 프롬프트 원문까지 확인 가능
          + 시스템 프롬프트에선 절대 경로만 쓰라는 안내가 있지만, 임시 파일 예시에는 상대 경로 사용되어 있음
     * 며칠 전 Claude Code로 streamlit Python 기반 간단 주식 추적 웹앱을 코드 흐름대로 작성하는데, 프로젝트 일정 크기까지는 매우 잘 동작하다 그 시점 이후 버그 수정을 더이상 빠르게 못함. 같은 작업을 Gemini CLI로 해보니, Claude Code가 헤맬 때쯤 ""코드베이스 분석 후 모든 버그 고쳐줘"" 요청에 일단 앱이 성공적으로 실행. 정말 미래 체감
          + 이게 context window size(컨텍스트 윈도) 차이 때문인지 궁금. Gemini의 윈도 크기가 Claude보다 5배 큼. Claude 쓸 때는 디버깅하면서 어느 순간 컨텍스트 부족으로 동작이 꼬임. 나중에 Gemini의 큰 윈도우 환경에서 이걸 테스트해 볼 예정
          + 현재 Claude Code의 모범적 사용법은 무거운 처리는 Gemini 2.5 Pro나 o3/o3pro에 맡기고, MCP 지원 등 덕분에 두 모델 연결이 매끄럽게 가능. Gemini CLI도 오픈소스라면 다양한 모델 플러그인 가능해 보임. 향후 LLM이 커머더티화될 세상에는 UI 래퍼보다 CLI 에이전트형 도구가 메인 될 수도 있을 것. OpenAI가 유저 수 경쟁에선 승자지만, 실제 업무용 UI로선 ChatGPT가 열세
          + 모듈별로 100줄 이내 요약 markdown 문서로 각 모듈 개요와 파일 위치만 담아 AI가 그 내용을 탐색할 수 있게 하면 맥락 이해 도움이 됨. 이 양식으로 담기가 힘들 정도면 인간 개발자도 관리하기 힘듦. 핵심 맥락을 AI에게 잘 지정해 주는 게 중요
          + 구체적이고 명확한 프롬프트 엔지니어링이 훨씬 생산성 높을 것 같음. ""모든 버그 고쳐라""식 요청은 현실적 시나리오와 다소 맞지 않음
          + 이 방식은 복잡성이 커질수록 무너지고, 중복 코드 많아 메모리 효율 매우 낮을 가능성 높음. 결국 직접 작성하는 게 더 효율적일 수도. 대충 만든 코드가 늘어나면 DRAM 수요가 비정상적으로 증가할지도 궁금
     * Gemini CLI에 양방향 음성 인터페이스를 직접 추가함.
          + 동영상 시연 링크
          + FOSS MCP 서버 웹사이트
          + Github 소스
          + 설치 방법 예시: ~/.gemini/settings.json에 특정 설정 값(config) 추가하려면 코드 예시 참고
     * Go나 Rust 등 런타임 불필요한 단일 바이너리로 되었으면 하는 아쉬움. Node 런타임 필요하다는 점이 아쉬움
          + 이런 프로젝트는 업데이트가 자주 필요한 만큼 npm이나 pip 관리가 더 현실적. 엄청난 연산이나 대단한 용량 요구 프로그램 아니기에 현대 하드웨어에선 별 문제 없음. Go도 이런 용도에 정말 잘 어울린다고 생각하는데, 현실성 측면에선 라이브러리 관리가 더 편함
          + Gemini CLI에 자기가 원하는 언어로 다시 짜달라고 프롬프트에 넣을 방법도 있음
          + 제품의 품질보다, ""우리도 CLI도구 있어요""라는 마케팅 목적에 더 가까운 프로젝트 느낌
          + 실제로 OpenAI도 Typescript 대신 Rust로 Codex CLI를 리빌드 중이라는 기사 참조. Node 경험이 많진 않지만 설치, 패키징, 격리 경험이 대단히 잘 되어 있다고 느낌
          + Bun과 Deno로 standalone 실행 파일 제작 가능할지도 모름. Bun bundler 설명 Deno CLI 컴파일 설명. 표준 Node 코드면 Bun에선 최소한 잘 동작할 듯. 실행파일 크기가 Go, Rust와 어떻게 다를지 궁금
     * ""Failed to login. Ensure your Google account is not a Workspace account."" 워크스페이스 계정은 쓸 수 없는 구조인지 의문. GSuite 시절부터 Workspace 계정인데 결국 구글 서비스에서 계속 제한 당하는 느낌. 예전부터 Gmail에서 커스텀 도메인이 필요했던 것뿐인데 YouTube 데이터, Fitbit 데이터 등도 날리고 구독 서비스 선택 등도 난잡하게 막혀 있음. 결국 Workspace 계정이라 Gemini CLI로 소프트웨어 개발 업무를 할 수 없는 점이 장기 충성 고객에 대한 배려 결여처럼 느껴져 실망
          + Workspace 계정 안내 공식 가이드 참고하면 도움될 수 있음
          + 이슈 해결 방법으로 환경변수 GOOGLE_CLOUD_PROJECT 등 추가 설정 필요
          + 동일한 문제로 곤란 겪는 사용자 경험
     * 1개월 정도 써봤는데 2.5pro의 SOTA 성능, 맥락 1M context window 지원 등으로 대부분 도구에 비해 월등함. 대규모 코드베이스를 던져도 빠르고 정확하게 분석 및 탐색 가능
          + Cursor에서 써봤을 땐 대형 Python 파일 임포트가 깨지는 문제 발생. Claude에선 이런 문제 없었음. Gemini에서 특이한 문제 경험 궁금
          + 워크플로가 궁금
"
"https://news.hada.io/topic?id=21669","Firefox 140.0 릴리즈 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Firefox 140.0 릴리즈

     * 탭 언로드(Unload Tab) 기능을 우클릭 메뉴로 지원하여 여러 탭의 메모리 및 CPU 사용량 절감 가능
     * Vertical Tabs 기능 추가로 고정 탭 영역을 드래그로 크기 조절해 중요 창에 빠르게 접근할 수 있음
     * Custom Search Engines 지원으로 웹사이트의 검색창에서 우클릭 또는 설정에서 직접 추가해 더 많은 검색 엔진을 쓸 수 있음
     * 확장 프로그램 툴바 단축키를 제거하거나 메뉴에서 다시 활성화할 수 있는 툴바 커스터마이징 기능 제공
     * 전체 페이지 번역은 스크롤하지 않은 부분은 번역을 건너뛰고, 사용자가 스크롤할 때만 번역해 리소스 최적화
     * Arabic 사전 내장 및 이탈리아, 폴란드, 오스트리아 주소 자동완성 기능 지원

Web Platform 변경

     * aria-keyshortcuts 속성의 Linux, macOS, Windows 지원 추가
     * CookieStore API 지원으로 서비스 워커와 HTML 문서에서 비동기 쿠키 처리 가능
     * Custom Highlight API 지원으로 원하는 범위의 텍스트에 스타일 적용 가능(추후 text-decoration 지원 예정)
     * pointerrawupdate 이벤트 지원으로 포인터 이동을 더욱 빠르고 세밀하게 처리(고정밀 입력 처리용)
     * Service Workers가 이제 프라이빗 브라우징 모드에서도 사용 가능(IndexedDB, DOM Cache API와 함께 암호화 저장소 기반)
     * <h1> 요소의 UA 스타일을 article, aside, nav, section 내부 여부에 상관없이 일관 적용
     * HTML 속성 직렬화 시 <, > 기호를 escape하여 mXSS 공격 위험 감소

Developer

     * 개발자용 변경사항 참고
     * Inspector 패널의 검색 기능이 개선되어 DOM 검색 시 결과 정렬, ""pseudo"" 선택자 상태 지원 등 효율적 탐색 가능

   Custom Highlight API 한번 기회있으면 써봐야겠군요.

   항상 잘 쓰고 있습니다.
   AI 쪽 관련 기능도 많이 추가되면 좋겠네요

   불여우는 사랑입니다

   이제 140 이라고? 라는 느낌이 들어서 다시 보니 저는 디벨로퍼 버전과 나이틀리 버전을 쓰고 있었네요 ^^;
"
"https://news.hada.io/topic?id=21662","Google Workspace 사용자를 위한 Gemini CLI 설정 방법 (feat. CLAUDE.md 재사용)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Google Workspace 사용자를 위한 Gemini CLI 설정 방법 (feat. CLAUDE.md 재사용)

Google Workspace 사용자를 위한 Gemini CLI 설정 방법

     * Gemini CLI를 Google Workspace 사용자가 쓰려면 초반 인증 과정이 조금 까다로움
     * 그냥 하면 로그인이 안 된다고 나오고, GOOGLE_CLOUDE_PROJECT를 환경변수로 세팅해야 쓸 수 있다고 나옴

   아래 과정을 거쳐야 함
    1. Google Cloud Project 만들기
    2. Google Cloud Project에서 Gemini API 사용 설정
    3. Google Cloud Project에서 IAM 역할 부여
    4. Google Cloud Project의 ID 복사해서 환경변수에 넣기
    5. 터미널 재실행, gemini 재실행

   (블로그에 전체 내용을 스샷과 함께 정리해뒀습니다)

CLAUDE.md를 Gemini CLI에서 재사용하기

     * Claude Code와 마찬가지로, Gemini CLI를 제대로 쓰려면 GEMINI.md를 만들어야 함.
     * 이미 Claude Code의 /init 명령어를 이용해 만들어둔 CLAUDE.md 파일이 있어서 중복 작성하기 싫었음

   그래서:
     * CLAUDE.md에는 ""See @AGENTS.md for guidelines.""라고 씀
     * AGENTS.md에 기존 CLAUDE.md 내용을 옮김
     * Gemini는 글로벌 설정으로 contextFileName을 AGENTS.md로 수정

   이는 아래와 같은 기능들 때문에 가능함
     * Claude Code가 @을 통해 다른 파일을 레퍼런스할 수 있고(제미니는 이걸 못함)
     * Gemini는 룰 설정을 위한 파일명을 바꿀 수 있음(클로드는 이걸 못함)
     * 그리고 제미니도 클로드 코드처럼 서브디렉토리의 컨텍스트 파일을 읽을 수 있음. 즉 모듈별로 AGENTS.md 를 쓰는 것도 가능
     * 설정 후 클로드는 /memory, 제미니는 /memory show 명령으로 제대로 읽어오는지 확인 가능

   workspace쪽은 유료 아닌가요..?

   아 넵 구글 워크스페이스를 사용하는 기업 계정 사용자들을 위한 온보딩 글입니다!

   지금 시험해 보았습니다. 무료 Google 계정으로도 무료 사용이 가능합니다.

   오홋 좋네요 :)
"
"https://news.hada.io/topic?id=21609","Typst로 박사 논문을 작성했어요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Typst로 박사 논문을 작성했어요

     * 필자는 Typst를 사용해 박사 논문을 작성했으며, 이는 기존 LaTeX와 다른 새로운 도전이었음
     * 빠른 컴파일 속도, 일관적이고 강력한 스크립팅 언어, 쉬운 레이아웃 커스터마이징, 우수한 코드 하이라이팅 덕분에 문서 수정 및 템플릿 조정이 매우 효율적이었음
     * 서지 관리의 불편, LaTeX 변환의 한계, 젊은 생태계로 인한 템플릿 부족, 에러 메시지 한계등 불편함과 한계점이 분명히 존재함
     * LaTeX와의 호환성, 협업, 논문 제출 시 요구되는 서식 지원은 아직 미흡한 점이 있으며, 논문 협업·학회 제출에는 LaTeX이 사실상 표준임을 실감
     * Typst는 프로그래밍적 자유도와 최신 기능이 필요할 때 특히 유리하지만, 초심자 및 표준적인 요구에는 권장하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

박사 논문을 Typst로 쓴 이유

     * 필자는 최근 박사 논문을 Typst로 작성했고, 전통적으로 자주 쓰이는 LaTeX 대신 새로운 타이포그래피 언어를 시도했음
     * Typst는 마크다운과 동적 타입 Rust를 조합한 형태로, 문서 작성이 LaTeX에 비해 자연스러우며 스크립트 언어로서의 확장성도 뛰어남
     * 문법이 직관적이고, 코드와 문서의 전환이 쉬운 점이 특징임

Typst의 장점

  컴파일 속도

     * Typst 컴파일러는 매우 빠른 속도를 갖추고 있어, 문서가 커져도 실시간 PDF 미리보기가 가능함
     * 전체 빌드도 15초 내외로 완료되며, 내용 변경 시에는 거의 즉시 결과를 볼 수 있음
     * 레이아웃 및 스타일 수정 작업을 효율적으로 반복해 최종 산출물의 품질이 향상됨

  언어 설계 및 스크립트 활용

     * Typst 언어는 일관성이 뛰어나고 Rust 기반 설계 덕분에 학습 곡선이 낮음
     * LaTeX에서 각 패키지마다 문법적 일관성이 부족한 불편함이 Typst에서는 해소됨
     * TOML 파일을 직접 파싱하여 데이터를 문서 내 자동 시각화가 가능하는 등 프로그래밍적 응용이 풍부함
     * 현대적 툴(컴파일러, 의존성 관리, LSP 등)과의 통합도 강점임

  템플릿 및 레이아웃 수정

     * Typst의 템플릿 구조는 명확해, 원하는 대로 쉽게 수정과 확장이 가능함
     * LaTeX의 복잡한 템플릿 수정에 비해 훨씬 직관적이고 빠른 설정 경험을 제공함

  코드 하이라이트

     * 내장된 syntax highlighting 지원 덕분에 논문 내 코드 가독성이 높음
     * Textmate grammar 활용 가능하며, regex 기반 커스텀 정의도 손쉽게 구현함
     * 스크립트로 직접 파서를 작성해 특정 문법 하이라이트도 실험함

  에러 메시지

     * LaTeX에 비해 에러 위치와 원인을 명확히 안내하여, 문제 해결에 걸리는 시간이 크게 감소함
     * 불필요한 터미널 출력이 없고, 오류 정보가 실질적으로 유용함

Typst의 단점

  서지(bibliography) 관리

     * 논문 전체에 단일 bibliography만 지원해, 챕터별 혹은 논문 포함시 각기 다른 서지 파일을 쓸 수 없음
     * Bibtex 변수 등 고급 기능에 대한 지원이 부족해 Makefile로 수동 통합 필요
     * 패키지(Alexandria)로 부분적 해결은 가능하나, 사용 편의성과 자동화 수준이 낮음
     * 인용 스타일 변환, 필드 매핑 등에서 세부 조정이 불완전하고, 수작업이 불가피함
     * 서지 필드가 Bibtex 표준과 달라 결과에 차이가 발생함

  에러 메시지의 한계

     * 복잡한 경우(예: Alexandria 사용 시), 구체적인 오류 설명 없이 단순 실패 메시지만 나타남
     * 상태 기반 show rule 등은 오류의 위치 추적이 어려우며, 디버깅 난이도가 높음
     * 일부 레이아웃 관련 경고는 쉽게 원인 파악이 불가함

복잡한 현실: 호환성 및 생태계

  LaTeX와의 호환 및 협업

     * 기존 논문, 제출 논문 등은 LaTeX 형식이 필요해 Typst 작성물을 Pandoc 등으로 변환해 사용함
     * 새로운 논문도 Typst로 초안을 작성한 후, 최종 제출용으로 변환 작업이 필요함
     * Typst에서 LaTeX로 자동 변환은 완벽하지 않아 별도 도구를 개발하여 작업 진행
     * 변환 결과물의 일부(예: 코드)는 LaTeX의 \includepdf를 이용해야 하며, 출판사 요구와 맞지 않을 수 있음
     * LaTeX가 표준이라 협업 상대가 Typst를 새로 배워야 하는 번거로움도 발생함

  Typst 생태계 현황

     * Typst는 아직 초기 생태계로, 공식 템플릿이나 제출 양식이 제한적임
     * 사용자가 직접 맞춤 템플릿을 제작해야 하는 경우가 많음
     * 주요 학회, 저널용 Typst 템플릿은 지원 범위와 품질이 완벽하지 않음

결론 및 추천

     * 프로그래밍을 즐기고, 도구의 세밀한 커스터마이징에 매력을 느끼는 경우 Typst로 논문을 작성하는 것이 충분히 추천 가능함
     * 높은 반복적 시도와 커스터마이징 자유도로 결과물이 미려해지는 장점 제공
     * 반면, 별다른 추가 설정 없이 바로 사용해야 한다면 아직 Typst는 박사 논문 등 대형 문서에는 적합하지 않음
     * 작은 규모의 문서 작성이나 개인적 실험에는 Typst를 시도해 볼 만한 가치가 있음

        Hacker News 의견

     * 30년 후에도 LaTeX는 오픈소스로 남아 관리될 가능성이 높지만, Typst는 오픈소스와 클로즈드소스가 혼합된 구조라서 회사가 사라지면 프로젝트도 유지 안 될 가능성이 크다는 걱정
          + Typst 프로젝트 자체는 오픈소스와 클로즈드소스 혼합이라고 보기 어렵고, CLI와 웹앱이 동일하게 동작하도록 만드는 게 핵심 목표임을 개발자가 직접 밝힘 관련 이슈 코멘트 참고 LSP 구현체인 tinymist 등 커뮤니티가 만든 오픈소스도 있음 또 Typstify 같은 유료 에디터도 회사와 상관 없이 존재
          + Typst의 웹 에디터는 클로즈드소스지만, 편집에 필요한 대부분 요소가 오픈소스라 로컬 환경에서도 비슷하거나 더 나은 경험 누림 Typst 컴파일러, LSP 등이 다 오픈소스임 Overleaf에서 LaTeX 프로젝트 만든 사례와 비슷하고, Typst 회사가 사라질 경우 패키지 다운로드도 오픈소스 git 레포 기반이라 대체 저장소 만들면 큰 문제 없을 것이라는 생각
          + 오픈소스 부분을 방치한다는 건, 사실 완전 오픈소스로 배포되는 대부분의 프로젝트와 다를 바 없다는 이야기
          + 클로즈드소스에 들어간 ‘핵심 기능’이란 게 구체적으로 뭐가 있는지 궁금
     * 컴퓨터공학 박사과정 학생들이 타입셋팅에 왜 그렇게 집착하는지 궁금함 LaTeX에 큰 관심을 보이다가 매크로 작업에 수개월을 쏟는 걸 보면, LaTeX에 또 한 명이 빠졌다고 생각 LaTeX는 미루는 학생들의 함정 같은 존재로 느껴짐
          + 수학 전공자로서, 손글씨로 모든 것을 쓰는 건 너무 번거롭고, 식이 많은 문서를 타이핑으로 만들긴 쉽지 않음 실제로 물리 쪽은 타이핑이 더 힘듦 논문/과제에 집중하는 인생에서 아이디어를 얼마나 쉽게 기록할 수 있는지가 정말 중요함 그래서 엔진의 품질에 민감해지고, 매크로 팁도 서로 많이 공유하는 경험 주변에서 얻은 조언과 기본 헤더 공유가 자연스러운 문화
          + LaTeX를 쓰면 일종의 ‘공식적인 느낌’을 갖게 됨 수식도 LaTeX 문서에서 쓰면 엄청 진지하게 보이지만, Word에서 하면 그렇지 않음 Aldus PageMaker로 첫 뉴스레터를 만들고 레이저 프린트했을 때처럼 전문가가 된 느낌
          + 큰 문서(예: 논문)를 섹션별로 별도 tex 파일로 관리하고, 나중에 한 데 모아 컴파일하는 구조 사용 가능 git 같은 VCS와 궁합도 잘 맞음 스크립트로 그림 등을 생성하면, 라텍스가 새 파일을 자동 감지해 새로 컴파일해줌 Word에서는 각각 그림을 직접 찾아 바꿔야 해서 비효율적임 문서 규모가 커질수록 Word는 점점 불편해지지만, LaTeX는 초기에만 세팅하고 그 이후엔 오히려 더 효율적임
          + 2000년대에는 수식만 조금 들어가도 Word로 작업하면 고통 수준이 심각했음 수십 페이지 넘는 공식-상호참조식이 필요할 땐 라텍스가 아니면 작업이 거의 불가능함 챕터별 파일 나누기, 쓸 만한 에디터와의 연동 등도 중요한 장점
          + 10년간 논문/리포트 쓰면서 자잘한 snippet을 모으는 것이 집착이 아닌 자연스러운 결과임
     * Typst가 엄청 유망해 보이는 이유는 IEEE 같은 대표 템플릿을 기본으로 제공하고, 그 결과물이 LaTeX와 거의 동일하게 출력된다는 점 LaTeX 툴체인은 불편함이 많고, makefile도 종종 불안정함 여러 번 돌려야 제대로 결과물이 나오는 경우 많으며, 가끔은 git clean -xdf까지 해야 문제 해결됨 왜 이런 현상이 생기는지는 아직도 잘 모르겠고, makefile 자체들도 너무 복잡함
          + “같은 걸 두 번 시도하고 다른 결과를 기대하는 게 미친 짓”이라고 하는데, 라텍스 컴파일이 그야말로 이 방식 그대로임
          + 완벽한 해결책은 아니지만, latex 빌드의 고생을 자동화해주는 Latexmk를 추천함 사용법 링크 참고 추가로 -outdir 옵션으로 중간파일을 분리 관리 가능
          + 나도 한때는 왜 여러 번 돌려야 하는지 이해했으나, 이제는 기억이 나지 않음 과거에 썼던 개인 build 스크립트에서도 bibtex 쓸 땐 세 번, 아닐 땐 두 번 돌리라는 조건문이 있었음 지금 보면 이런 시절이 지나서 속이 시원함
          + 요즘은 Tectonic을 쓰면 이런 반복 컴파일 이슈 없이 자동으로 해결됨
     * AI가 글쓰기의 주된 대상이자, 마크업 포맷 고르는 주된 이유임 의미적 압축 관점에서 Typst, markdown, asciidoc이 LaTeX보다 훨씬 더 간결함 개인적으로 지난 6개월간 AI 활용 수학 연구와 코드 작업에 엄청난 변화를 겪었고, 이 영역에는 확실한 정답이나 조언을 찾기 어려움 실제로 AI는 SVG 수학 다이어그램도 인간보다 잘 읽고, LaTeX 소스 읽는 걸 싫어함 저널 입장에서의 포맷 규정은 이해하지만, 시대에 안 맞는 2컬럼 출력 강요하는 저널 에디터도 많음 종이 출력이 큰 의미가 없는 시대라 크게 신경 안 쓰고, 앞으로는 내 연구결과를 애니메이션이나 Typst 문서로도 남길 계획
          + 실제로 논문을 출력해서 읽는 전문 과학계 환경에선 종이가 아직도 효율적임
     * 저널이나 컨퍼런스가 typst를 아직 받아들이지 않으니, 나는 일부러 LaTeX를 고집하는 게 아니라 현실적으로 어쩔 수 없이 LaTeX에 머무는 상황 도입 여부는 이들이 도구 체인에 통합할 의지에 달림
     * 점점 내 작업을 Typst로 옮기고 있는데, 속도가 빠르고 쾌적함 다만 수학 표기법을 새로 익히는 게 가장 큰 허들이었음 Typst만의 독특한 규칙이 있어 새롭게 익혀야 함
          + Typst도 좋아 보이지만, Claude Code와 VS Code 조합으로 다시 LaTeX를 쓰게 됨 라텍스에서 한동안 멀어졌다가(박사 졸업 후 10여 년 만에), 과거엔 TikZ나 수식, preamble 매크로를 외울 정도로 썼음 Claude Code는 내가 원하는 걸 입력하면 1,2번만에 거의 원하는 결과를 뽑아줌 LaTeX 오류 메시지 해석도 Claude가 95% 해결해줘서 예전보다 큰 문제가 안 됨
          + mitex도 하나의 옵션임 mitex 패키지 참고, 난 이미 다른 표기법을 새로 익힐 엄두는 안 남
     * typst 소스와 결과물이 궁금하다면 직접 만든 몇 가지 문서를 공유함:
          + Latex Resume Template 참고 버전
          + Enduring Power of Attorney 예시
          + 유언장 템플릿
     * Typst는 몇 년 내에 사라지거나 회사에 인수될 가능성이 많고, LaTeX는 수십 년간 남을 것이라는 견해
     * Typst가 LaTeX보다 세로 레이아웃 제어 등에서 매력적이라 옮기려 했으나, 최근 챗GPT류 LLM의 코드 생성 능력이 괜찮아지면서 새로운 마크업엔진(특히 typst)에서는 상당히 부족한 모습 AI가 latex는 나쁘긴 해도 typst에 비해선 훨씬 낫고, typst에서는 정말 결과가 안 나옴 6개월이나 1년쯤 지나면 좀 나아질지도 모름
          + LLM을 쓰면 생각을 덜 하게 돼 편하긴 하지만, LLM에 너무 의존해서 새로운 도구 자체를 못 쓰는 사람이 많다는 점이 답답함 과거에도 복붙이 안 되거나 코드 스니펫 찾기 어렵다는 이유로 새 언어를 꺼렸던 현상과 비슷함
          + 마크다운이나 rust에선 AI가 꽤 쓸만함 Typst 문서 개요를 LLM 프롬프트에 입력한다면 좀 도움이 될지도 모름
     * Typst에서 마음에 안 드는 점은 지금은 LaTeX 수식 문법이 거의 표준으로 자리잡았고, 이미 널리 쓰이기 때문에 새 수학 문법을 익히기 어렵다는 것
          + 실제로 Typst에서도 $x^2=1$ 같은 표기는 그대로 동작함
"
"https://news.hada.io/topic?id=21632","Wildcat - 고성능 임베디드 키-밸류 데이터베이스(스토리지 엔진)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Wildcat - 고성능 임베디드 키-밸류 데이터베이스(스토리지 엔진)

     * Go로 개발된 고성능 키-밸류 DB로, C 인터페이스를 지원하며 LSM 트리, MVCC, lock-free 구조 등 최신 DB 설계 원칙을 반영함
     * WAL(Write Ahead Logging) 기반 즉시 일관성 및 내구성 보장, 백그라운드 플러시/컴팩션, lock-free 병렬 처리 등으로 높은 쓰기/읽기 성능 제공
     * 싱글 노드 내장형 구조로 네트워크나 분산 없이 가볍고 빠른 데이터 저장을 지향함

Wildcat의 주요 특징

     * LSM 트리 기반, 쓰기 집중 워크로드에 최적화
     * MVCC(멀티버전 동시성 제어), lock-free 데이터 구조로 주요 경로 무잠금 처리
     * WAL 로깅: 트랜잭션 전체 상태 기록 및 복구 지원
     * 버전 인식 skip list로 인메모리 MVCC 가속
     * 스레드 세이프/락프리 쓰기 및 아토믹 캐시/메모리 관리
     * SSTable은 불변(immutable) BTree 구조, 키-값 분리 및 블룸필터 최적화 지원
     * 트랜잭션 ACID 보장 및 내구성(Full/Partial/None) 레벨 선택 가능
     * Crash recovery로 커밋/미완료 트랜잭션 모두 복구
     * 배치/이터레이터: 범위, 프리픽스, 양방향 반복 지원
     * Bloom Filter, key-value 분리(.klog, .vlog), tombstone/버전 인식 컴팩션 등 최신 설계
     * 통계/로깅/모니터링: Stats(), LogChannel 등 다양한 진단 및 통계 인터페이스 제공
     * C 라이브러리 빌드 및 API: Go 공유 라이브러리로 빌드, 다양한 언어에서 직접 사용 가능
     * Go 1.24+ 및 Linux/macOS/Windows(64비트) 지원

왜 중요한가? (타 임베디드 KV와의 차별점)

     * RocksDB, Badger 등과 유사한 LSM+MVCC 구조지만 Go 네이티브 lock-free, 멀티스레드 설계로 Go 환경에서 최적화된 사용성
     * 내장 DB 특화: 네트워크/복제 오버헤드 없이 즉시 일관성/내구성/고성능 구현
     * 트랜잭션 복구, 미완료 상태 유지 등 신뢰성과 투명성 높음
     * 다양한 컴팩션/버퍼/캐시 파라미터를 통해 워크로드별 맞춤형 튜닝 가능

   수상할정도로 디비가 많이 나오는 언어

   수상할정도로 디비가 많이 나오는 언어
"
"https://news.hada.io/topic?id=21590","Show GN: [오픈소스]iOS 와 MacOS를 지원하는 멀티 LLM 클라이언트 - LLM 브릿지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Show GN: [오픈소스]iOS 와 MacOS를 지원하는 멀티 LLM 클라이언트 - LLM 브릿지

   이전에 iOS용 MacOS용 Ollama 전용 LLM 클라이언트를 별도로 만들어서 오픈소스로 공개했었는데 Swift/SwiftUI 기반으로 iOS와 MacOS 코드를 통합하고 지원하는 API를 추가해서 새로 다시 만들었습니다.
     * Ollama, LMStudio를 로컬 LLM으로 지원합니다.
          + Ollama에 LLM을 설치한 컴퓨터에 port를 외부에 오픈하면 원격에서도 무료 LLM을 사용할수 있습니다.
          + MLStudio는 자체 UI를가진 로컬 LLM 관리 프로그램으로 HuggingFace에서 모델을 검색해서 설치 할수 있어서 다양한 모델을 실험할수 있습니다.
          + LLM 브릿지에서 ip, port를 설정하고 설치된 모델을 이용하여 질의에 응답을 받을수 있습니다.
     * OpenAI 지원
          + API key를 발급받아서 앱에 입력하고 Api 호출을 통하여 ChatGtp를 사용할수 있습니다.
          + API를 사용하는것이 매월 회비(?)를 내는것보다 저렴하게 쓸수 있습니다.
     * Claude 지원
          + API Key 사용
     * 이미지 지원 모델에 대해 이미지 전송 가능
     * PDF, TXT 파일 지원
          + PDFKit을 이용하여 텍스트 추출후 전송
          + 텍스트 파일 지원
     * 오픈소스
          + Swift/SwiftUI
     * 소스링크
          + https://github.com/bipark/swift_llm_bridge
"
"https://news.hada.io/topic?id=21663","어떤 문제를 풀 것인가 – 리처드 파인만 (1966)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     어떤 문제를 풀 것인가 – 리처드 파인만 (1966)

   파인만이 옛 제자에게 보낸 편지
     * 파인만은 진정 가치 있는 문제란 직접 풀거나 기여할 수 있는 것임을 강조하며, 작고 단순해 보이는 문제라도 자신이 직접 풀 수 있다면 충분히 의미 있다고 설명함
     * 학생에게 스스로 해결하고 싶은 문제를 찾으라고 조언하며, 단순하거나 사소해 보여도 실제로 해답을 얻는 경험과 타인에게 도움이 되는 기쁨이 중요하다고 이야기함
     * 학계에서 가치 있다고 여겨지는 거대한 문제만을 쫓는 태도에 경계를 나타내며, 본인은 “마찰 계수 실험”, “플라스틱에 금속 도금 접착”, “종이 접기 알고리듬” 등 매우 다양한 소박한 문제에 도전하며 즐거움을 느꼈음을 예시로 듦
     * 학생에게 자신의 기준이 아니라 스스로 찾은 문제에 집중하라고 당부하며, 주변 동료의 질문에도 답해주는 사람이 되라고 격려함
     * 자신을 이름 없는 존재라 여기지 말고, 가족과 동료, 주변 사람들, 그리고 자신에게 의미 있는 존재임을 잊지 말라고 따뜻하게 위로함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     파인만에게 축전을 보낸 제자에게 지금 무엇을 하고 있는지 묻는 답장을 보냈더니
     ""난류 대기를 통한 전자기파 전파에 대한 몇 가지 응용을 포함하여 결맞음 이론을 연구하고 있습니다… 소박하고 현실적인 유형의 문제입니다.""

   아래는 위의 답장에 대해서 다시 보낸 답장임

Dear Koichi,

   네가 연구소에서 일하게 된 소식을 듣고 무척 기뻤어. 그런데 너의 편지에서 슬픔이 느껴져 마음이 무거워졌구나. 아마도 선생님의 영향이 너에게 가치 있는 문제란 무엇인지에 대해 잘못된 생각을 심어준 것 같아. 진정 가치 있는 문제란 네가 실제로 풀 수 있거나, 해결에 도움을 줄 수 있는 문제라고 생각해. 누군가 아직 풀지 못한 문제고, 네가 조금이라도 진전을 이룰 수 있을 때, 그 문제는 충분히 위대한 문제야. 그러니 더 단순하거나, 네 표현대로 ‘하찮은’ 문제라도 네가 실제로 쉽게 풀 수 있는 것을 찾았으면 좋겠어. 아무리 사소해 보여도 스스로 풀어낸 성공의 기쁨과 누군가의 질문에 답해줄 수 있다는 뿌듯함을 빼앗기지 않았으면 해.

   네가 나를 만났던 때는 내 경력이 정점일 때였지. 너에게는 마치 신들의 문제를 다루는 사람처럼 보였을 수도 있어. 하지만 그때 나와 함께 했던 다른 박사과정 학생은, 바람이 바다 위에서 어떻게 파도를 일으키는지 같은 문제에 도전했단다. 나는 그가 직접 선택한 문제이기에 그 학생을 받아들였어. 그런데 너에게는 내가 문제를 주고, 네가 정말로 흥미를 느끼거나 즐길 수 있는 주제를 직접 찾도록 하지 못한 것 같아. 미안하구나. 이 편지로라도 그 잘못을 조금이나마 바로잡고 싶어.

   나는 네가 ‘하찮다’고 생각할 만한 문제들에 정말 많이 도전했어. 아주 매끈한 표면의 마찰 계수 실험(실패였지만), 결정의 탄성, 플라스틱에 금속을 잘 붙이는 방법, 중성자가 우라늄에서 퍼져 나가는 방식, 유리 표면에 얇게 입힌 금속 필름에서 전자기파가 반사되는 원리, 폭발에서 충격파가 형성되는 과정, 중성자 검출기 설계, 어떤 원소가 왜 특정 궤도의 전자만 잡아먹는지, 종이접기 장난감의 원리, 원자핵의 에너지 준위, 그리고 수년간 실패했던 난류 이론까지… 이런 다양한 ‘작은’ 문제들을 풀면서 즐거움과 만족을 느꼈단다. 물론 그 외에 ‘더 위대해 보이는’ 양자역학 문제들도 있었지만 말이야.

   정말 중요한 건, 네가 실제로 무언가를 해낼 수 있다면 그 문제의 크기나 겉모습은 중요하지 않다는 거야.

   네가 자신을 이름 없는 사람이라고 했지. 하지만 너는 네 아내와 아이에게는 결코 그런 존재가 아니야. 동료들이 네게 질문을 하러 찾아왔을 때 답을 줄 수 있다면, 곧 너의 주변에서도 이름 있는 사람이 될 거야. 나에게도 너는 이름 없는 사람이 아니야. 자신을 그런 식으로 여기지 말아줘. 젊을 적 네가 가졌던 순진한 이상이나, 선생님의 기준을 잘못 짐작해 만든 잣대가 아닌, 지금 이 자리에서 너만의 기준으로 스스로를 평가해주길 바라.

   행복과 행운을 빌며,
   Richard P. Feynman

   제 머릿속에 파인만은 해요체를 쓰지 않습니다.

   고이치 군,

   그대가 연구소에서 일하고 있다는 소식을 듣고 매우 기뻤네. 하지만 그대의 편지를 읽고 보니, 그대가 참으로 슬퍼하는 듯하여 안타까운 마음이 들었네.

   아마도 그대의 스승이, 무엇이 가치 있는 문제인가에 대해 잘못된 인식을 심어준 것이 아닌가 싶네. 진정으로 가치 있는 문제란, 그대가 실제로 해결할 수 있거나 해결에 기여할 수 있는 문제일세. 아직 풀리지 않은 문제라 하더라도, 그대가 거기에 한 걸음이라도 나아갈 수 있다면, 그 문제는 충분히 위대하다네.

   그러니 그대가 말한 바와 같이 단순하거나 '하찮은' 문제라도, 그대가 쉽게 풀 수 있는 문제를 택하도록 하게. 아무리 사소해 보여도, 스스로 문제를 해결하는 기쁨과, 능력이 덜한 동료의 물음에 답을 줄 수 있는 즐거움을 잃지 말게나. 그것은 결코 가벼운 일이 아니니라.

   그대가 나를 만났을 때는, 내가 경력의 정점에 있을 때였지. 아마도 나는 신들만이 다루는 문제에 몰두하고 있는 사람처럼 보였을 걸세. 하지만 같은 시기, 나의 또 다른 박사과정 학생이 다루던 문제는, 바람이 바다 위에서 어떻게 파도를 일으키는가 하는 것이었네. 그가 스스로 선택한 문제였기에 나는 그를 받아들였지.

   그러나 그대에게는 내가 문제를 주었고, 그대가 진정으로 흥미를 느끼거나 즐거움을 찾을 수 있는 주제를 스스로 고르지 못하게 하였구나. 그것은 나의 잘못이었고, 미안하게 생각하네. 이 편지를 통해 조금이나마 그것을 바로잡고자 하네.

   나는 지금까지 그대가 ‘하찮다’ 여길 만한 문제들도 수없이 다루었네. 예를 들면, 고도로 연마된 표면의 마찰 계수를 측정하려 한 실험(결국 실패하였지만), 결정의 탄성이 원자 간의 힘에 따라 어떻게 변하는가, 금속을 플라스틱 물체에 부착하는 법(예: 라디오 조절기), 중성자가 우라늄을 통해 어떻게 퍼지는가, 유리 표면의 박막이 전자기파를 어떻게 반사하는가, 폭발에서 충격파가 어떻게 생기는가, 중성자 계수기 설계, 특정 원소가 왜 L-오빗의 전자는 잡아먹으면서 K-오빗은 그렇지 않은가 하는 문제, 종이를 접어 만드는 일종의 장난감(플렉사곤이라 하네), 경핵의 에너지 준위, 그리고 수년간 시도했으나 실패한 난류 이론 등등...

   이처럼 소위 '작은' 문제들도 때로는 즐거움과 만족을 주었고, 그 덕에 나 자신도 성장하였다네. 물론 그 외에 보다 ‘거창한’ 양자역학 문제들도 있었지만 말일세.

   진정 중요한 것은, 그대가 실제로 무언가를 이룰 수 있는 문제라면, 그 크기나 외양은 중요치 않다는 사실일세.

   그대는 자신을 이름 없는 사람이라 하였지. 그러나 그대는 그대의 아내와 자식에게는 결코 무명의 존재가 아니네. 그리고 동료들이 질문을 들고 그대의 사무실을 찾아와, 그대가 그것에 답을 줄 수 있다면, 머지않아 그대는 그들 사이에서도 이름 있는 사람이 될 것이네. 나에게 있어서도 그대는 무명의 이가 아니네.

   부디 자신을 그렇게 여기지 말게. 젊은 시절의 순진한 이상이나, 스승의 기준을 잘못 짐작하여 만든 허상을 기준 삼지 말고, 지금 이 자리에서 그대 자신의 기준으로 스스로를 공정히 평가하길 바라네.

   그대의 앞날에 행운과 행복이 함께하길 빌며.

   리처드 P. 파인만 드림

   감명 깊게 읽었습니다 저도 파인만 선생님이 쓰신 책 아직 소장중입니다.

   나만의 기준으로 스스로를 평가하길 바라는 마지막 문장은 정말 울림이 있네요.

   파인만은 정말 좋은 스승이었네요.

        Hacker News 의견

     * 정말 아름다운 편지라는 느낌을 받음, 인생에 관한 단순하면서도 깊은 지혜를 학생에게 전하는 내용임에 감사함을 느낌, 이 글이 Hacker News에 올라와서 읽을 수 있게 된 것에 고마움을 느낌
     * Feynman이 천재였다는 것은 잘 알려진 사실이지만, 그가 얼마나 명확하고 철학적이었는지는 저평가 받는 부분이라 생각함, 그의 작품을 읽으면서 언제나 그가 얼마나 적절한 방식으로 메시지를 전달하는지 감탄함, 이 편지에서는 그의 이런 면모가 잘 드러남
          + Feynman은 복잡한 개념을 누구나 쉽게 이해할 수 있게 줄여주는 능력이 있음, “이 입자는 여섯 방향 중 하나로 하나의 속도로 움직일 수 있는 완벽한 강철 베어링이다” 라는 예시를 정말 좋아함, Feynman the Explainer 글에서도 볼 수 있음, 그리고 “반사된 음파라고 말하지 말고, 에코라고 해”, “로컬 미니마 같은 말은 잊고, 결정 안에 거품이 갇혀 있고, 흔들어서 빼내야 하는 거라고 해”, Feynman은 단순한 것을 복잡하게 포장하는 것에 매우 화가 났던 사람이었음
          + 이런 능력 덕분에 Feynman이 더욱 사랑받는 이유라고 생각함
     * “성공의 기쁨”이나 “동료의 머릿속에 있는 질문에 답해주는 것” 등, 페인만이 언급한 여러 구절들이 문제 해결자로서 겪는 고민을 잘 드러냄, 우리는 새로운 문제에 맞서기 위해 격려가 필요하고, 우리가 직접 문제를 해결할 수 있다는 믿음이 필요함, 약간의 건강한 자존감도 필요함(‘건강하다’의 정확한 정의는 모르겠음), 좋은 학습/직장 환경은 자존감을 높여주기도 함, 하지만 자존심이 과하면 좌절, 소외, 착각, 자격 의식, 방어적인 태도 등 부정적 결과도 있을 거라 생각함, ‘자아’를 완전히 내려놓은 채 일하는 사람이 정말 있다면 만나보고 싶음
          + ‘원죄’ 같은 숙명적 한계에 대한 인식이 중요함, 인간으로서 신적인 무언가를 동경하면서도 미치지 못하는 아쉬움을 겪어야 함, 스스로의 부족함을 인정하고 자신에게 친절해야 함
     * “스스로에게 이름 없는 존재로 남지 마라. 세상에서 네 자리를 찾고, 네 어린 시절의 순진한 이상이나, 네가 교사의 이상이라고 잘못 짐작하는 것들이 아닌 스스로를 제대로 평가하라”라는 내용이 매우 현명하다고 느낌
     * 이 편지는 내 커리어에 대해 생각해 보게 함, 소프트웨어 엔지니어로 좋은 연봉을 받으며 일하지만 내가 참여한 제품에 특별한 열정을 느껴본 적은 없음, 결국 내 일은 돈을 벌기 위한 ‘회사’라는 생각임, 하지만 남을 도와 문제를 해결하고, 동료의 질문에 답해주는 것, 가족을 부양하고, 스스로 가족의 롤모델이 되는 것에는 분명한 기쁨이 있음, 가끔은 내 삶에 더 큰 의미가 있는 일을 해야 하지 않을까 생각함, Kubernetes, ChatGPT, Google 같은 ‘세상에 영향 주는 무언가’를 만드는 사람이 되고 싶다는 생각도 들지만, 사실 난 그렇게 야망이 크지 않음, 내 가족과 동료에게 중요한 존재라는 사실만으로도 충분히 만족스러운 면이 있음
          + '이 정도면 충분한 것 같음'이라는 생각에 동의함, 최근 동료들과도 거대한 혁신이나 프로젝트에 대해 대화한 적이 있음, 인프라의 보이지 않는 작은 나사 같은 역할도 플래시한 부분 못지않게 중요함, 때론 더 중요할 수도 있음, 결국은 대단한 포부보다도 문제를 푸는 기쁨과 주변에서 꾸준히 필요한 존재라는 성취감에 더 의미가 있다고 느낌
          + 수백만 년간 인류의 대부분 시간은 힘든 노동의 연속이었음, 냉방이 잘 된 사무실에서 CRUD 작업으로 돈을 버는 삶에 대해 별로 죄책감을 느끼지 않음
          + 어떤 문제를 ‘다루는 것’이 본인에게 무엇을 의미하는지가 중요함, 소프트웨어 엔지니어로서 그 문제에 기여하고 싶다면 현실적으로 가능한 방향임, 구글/ChatGPT/AI/기후변화 등의 이면에는 이론적인 재미와 성취가 있고(쿠버네티스는 다름), 이런 이론은 소프트웨어 엔지니어링에서 쉽게 나오지 않음, 최근 스스로 소프트웨어 만들기도 이론적 문제풀기만큼 즐거운 활동임을 발견함, ML 엔지니어들이 과학자 역할로 전향하려는 이유도 여기에 있다고 봄
          + “신에 가까운 문제” 다음에 “쿠버네티스 같은 것 만들기”가 나오는 게 놀라웠음, 내 경우에 신에 가까움을 요가에서 찾았음, 단순히 소프트웨어 만드는 것보다 사람들에게 더 즐거운 삶이나 신체적 건강을 주는 것이 내 목적에 더 가까움
          + 야망이나 동력이 아닌 그냥 호기심 때문일 수도 있음, “이거 해볼 수 있지 않을까?”라는 식의 궁금증에서 시작하는 경우도 있음
     * 관련 링크로 What Problems to Solve라는 논의 추천함
          + Do not remain nameless to yourself (1966)도 같이 추천함
     * 정말 아름다운 글이라는 느낌, 깊은 인간미와 지적 사고가 한 에세이에 어우러짐, 처음엔 저자가 누군지 모르고 감탄했는데, 알고 보니 유명 인사였음, Hacker News에서도 이 조언은 모두에게 유용함, 세상에 뛰어난 사람이 있다는 건 그 이면에 평균적인(무언가의 평균) 사람들이 있기 때문이라는 점 명심
     * 문제를 해결하고 ‘이겼다’ 혹은 ‘해냈다’는 느낌만 있으면, 크고 작은 건 상관 없이 충분한 만족감이 든다는 내용에 깊이 공감함
     * “난류 대기를 통과하는 전자기파 전파에 응용된 Coherence theory 연구”는 한때 ‘소박한 문제’로 불렸지만, 실제로는 지상 천문학에서 매우 중요한 큰 문제였고, 상당 부분 해결된 문제임
     * 이 글을 올려줘서 매우 고마움, 페인만이 언급한 flexagon을 직접 만들어 보는 것을 강력 추천함, 수학적 배경도 흥미롭고 누구나 쉽게 만들 수 있는 장난감임, 어른도 아이처럼 재밌게 놀 수 있음

   지식이 쏟아지는 이 시점에서 더욱 와닿고 따뜻해지는 편지네요.
   그런데, 흥미로운 글은 cat-v.org인 경우가 많네요. 뭐하는 사이트일까요 ㅎㅎ

   파인만관련 도서들이 많은걸 보면, 많은 이들이 사랑한 천재였다는 것을 알 수 있습니다. 천재가 되면 질투를 느낄만도 한데, 파인만은 그런 사고에서 자유로운 천재같습니다.

   파인만은 보면 볼수록 존경스러운 마인드를 가지고 있고 전달하는 법도 너무나도 잘 알던 사람이네요

   인생은 퀘스트를 스스로 만들어내는 RPG 게임인 거 같습니다. 선생님의 기준은 단지 주어진 퀘스트일 뿐이죠. 스스로 목표를 세우는 것이 중요하고, 그래야 스스로의 기준으로 평가할 수 있다고 생각합니다.
"
"https://news.hada.io/topic?id=21683","가트너 “AI 에이전트는 대부분 과장 광고...프로젝트 절반 실패할 것”","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                가트너 “AI 에이전트는 대부분 과장 광고...프로젝트 절반 실패할 것”

   글로벌 리서치 및 컨설팅 기업 가트너는 AI 에이전트 프로젝트의 40% 이상이 2027년까지 실패할 것이라며, 과장된 ‘에이전트 워싱’이 원인이라고 지적하고, 기업은 신중한 도입 전략이 필요하다고 강조했습니다.

   원문: Gartner Predicts Over 40% of Agentic AI Projects Will Be Canceled by End of 2027
   https://gartner.com/en/newsroom/…

   절반이나 살아남을 거면 성공한거 아닌가요...

   절반이라니 엄청나게 낙관적이네요

   음.. 2027년 쯤에는 확실히 Agent가 있겠죠.
"
"https://news.hada.io/topic?id=21648","Gemini CLI가 공개 되었습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Gemini CLI가 공개 되었습니다.

     * Gemini의 100만 토큰 컨텍스트 창을 넘어서는 크고 작은 코드베이스를 쿼리하고 편집할 수 있습니다.
     * Gemini의 멀티모달 기능을 사용하여 PDF나 스케치에서 새로운 앱을 생성할 수 있습니다.
     * 풀 리퀘스트 쿼리나 복잡한 리베이스 처리와 같은 운영 작업을 자동화할 수 있습니다.
     * 도구와 MCP 서버를 사용하여 Imagen, Veo 또는 Lyria를 사용한 미디어 생성을 포함한 새로운 기능을 연결할 수 있습니다.
     * Gemini에 내장된 Google 검색 도구로 쿼리를 근거지을 수 있습니다.
     * 무료모델에서 Gemini-2.5-Pro 모델을 분당 60회 하루 최대 1,000회의 요청이 가능합니다.

  https://github.com/google-gemini/gemini-cli/…
  인증 방법 1, 2a는 무료, 2b부터는 유료 계정입니다.

   프롬프트와 답변을 포함한 내 코드가 Google 모델을 학습하는 데 사용됩니까?
   이는 전적으로 귀하가 사용하는 인증 방법의 유형에 따라 달라집니다.

   인증 방법 1: 네. 개인 Google 계정을 사용하는 경우 Gemini Code Assist 개인용 개인정보 처리방침이 적용됩니다. 이 방침에 따라 귀하의 프롬프트, 답변 및 관련 코드가 수집되며 , 이는 모델 학습을 포함한 Google 제품 개선에 사용될 수 있습니다.
   인증 방법 2a: 네, Gemini API 키를 사용하면 Gemini API(무료 서비스) 약관이 적용됩니다. 본 고지에 따라 귀하의 프롬프트, 답변 및 관련 코드가 수집되며 , 이는 모델 학습을 포함한 Google 제품 개선에 사용될 수 있습니다.
   인증 방법 2b, 3 및 4: 아니요. 이 계정의 경우, 귀하의 데이터는 Google Cloud 또는 Gemini API(유료 서비스) 약관의 적용을 받으며, 귀하의 입력 내용은 기밀로 처리됩니다. 귀하의 코드, 프롬프트 및 기타 입력 내용은 모델 학습에 사용 되지 않습니다 .

   깔끔하게 정리해 주셔서 감사합니다.

   Gemini CLI에 대한 Hacker News의 댓글들
     * 구글의 Gemini 제품군이 지나치게 파편화(fragmented ) 되었다는 걸 정말 웃기게 느끼고 있음, 나는 Pro 유료 구독자인데 이제야 ""Gemini Code Assist Standard""나 ""Enterprise"" 유저여야 더 많은 사용이 가능하다는 걸 알게 됨, 이런 게 있는 줄도 몰랐음, 일반 구글 유저로선 관대한 무료 티어를 주지만, 정작 돈 내고 Gemini를 구독해도 ""Gemini CLI""와는 연동이 안 됨, 정말 신기한 경험임
          + 구글이 마이크로소프트처럼 제품 설계와 메시지가 혼란스러움, 좋은 제품이 많아도 결국 전체적으로 인상이 흐려짐, 나 역시 Gemini 2.5 Pro가 좋다고 느끼고 구글 드라이브를 자주 쓰기에 Google One과 Gemini Pro를 이용하지만, 이미 iCloud에 묶여 있으니 기능을 제대로 누릴 수 없음, Gemini Chat UI는 ChatGPT 클라이언트에 비해 한참 뒤쳐져 있음, NotebookLM은 문서 요약에 좋지만 Gemini Chat과 연동되지 않아 각각 필요할 때마다 왔다갔다해야 함, 그래서 결국 Raycast AI 구독을 하게 됨, 이건 별다른 설정 없이 워크플로우에 잘 녹아들어 있음, 구글처럼 여러 기능을 따로따로 분산하지 않은 점이 큼, UX에서 구글이 OpenAI나 Anthropic보다 많이 뒤처지는 중임, 최근 구글이 v0 (Google Stitch), GH Copilot/Cursor용 얕은 VSCode 플러그인, 그리고 Claude Code까지 빠르게 따라잡으려 했지만 다들 곧
            사라질 것 같은 실험 프로젝트처럼 보임
          + 스타트업이나 1인 개발자들이 Google 대신 다른 솔루션을 선호하는 이유는 바로 이 복잡성임, Gemini 2.5 Pro의 기술력 자체는 매우 높지만, Google Cloud Dashboard는 오랜 시간 동안 미개선 상태임, Vertex에서 모델을 호스팅한다지만 Google Cloud와의 차이점을 모르겠고, 프로젝트 수준마다 API도 따로 있음, 작은 프로젝트에서 시작해도 규모가 커질 때마다 Google AI Studio API에서 Vertex API로 옮겨야 하는 상황이 정말 불합리함, 심지어 OpenAI와 호환되는 Google API도 자주 오류가 남, 실질적으로 업계 표준이 되어야 할 AI 제공자가 오히려 확장성을 저해하고 있음, Jules vs Gemini CLI, Vertex API(Google Cloud 필요) vs Google AI Studio API처럼 겹치는 서비스가 필요 이상으로 많음, Vertex 활용시 app에 PROJECT같은 환경 변수 설정도 직접 해줘야 하니 더 복잡해짐
          + 구글이 가격 정책에서도 방황 중임, Gemini 2.5 Pro가 내가 써본 것 중 최고임에도, Claude/Cursor처럼 간단하게 전체 기능을 쓸 수 있는 구독 방식이 없음, 기업용으로는 OpenAI가 확실히 강력한 점유율을 기록 중임
          + 매월 $300 AI ULTRA 멤버십도 있음, Google One 멤버십조차도 어떤 추가 기능을 주는지 명확히 안내되어 있지 않음, 변화가 너무 잦아서 이런 일이 생기는 듯함
          + 피드백을 소중히 생각하며, 팀에서도 적극적으로 반영할 예정임
     * Gemini CLI에 양방향 음성 인터페이스를 추가했음, 오픈소스 MCP 서버를 기반으로 만들어 최근에 직접 배포함, voice-mode라는 이름으로 사용 가능함, 실제 설치 방법과 코드 예시도 공유함
     * 이 프로젝트에 직접 참여하고 있음, 이용률이 아직 낮은 상황이니 TPU 상황에 관대해 주길 바람, 누구든 버그나 피처 요청 환영함, 전체 팀이 피드백을 열심히 읽고 있음
          + 어제 Ruby로 작성된 알고리즘을 자바스크립트로 변환하려고 GPT-4.1에 여러 번 시도했지만 계속 에러만 발생함, 호기심에 Gemini CLI를 써봤는데, 단 한 번에 Ruby 프로젝트 전체를 변환했고, 생각에서 결과까지 총 5분밖에 안 걸렸음, 인상적임
          + 구글 워크스페이스에서 gemini도 결제해서 사용 중인데, ""GOOGLE_CLOUD_PROJECT 환경 변수가 없다""는 메시지가 뜸, GCP를 사용하지 않는 우리에겐 변수 획득 방법이 전혀 직관적이지 않으니, 적어도 문서화가 잘 되어야 함, 최악의 경우 지불 유저가 일반 유저보다 접근성이 떨어지는 아이러니 상황임
          + Apple M1에서 CodeRunner를 통해 Gemini CLI가 생성한 코드를 직접 실행하는 통합을 시도했음, 매우 잘 작동함, 실제 예시도 링크로 전달함
          + 일반 사용자를 위한 Claude Max 같은 통합 구독제(IP 준수 및 Gemini app, API 권한 포함)를 희망함
          + CLI에 현재 사용 가능한 기능들이 모여 있는데, 이 중 일부를 확장하거나 비활성화하는 옵션도 있었으면 좋겠음
     * Claude Code (Opus 4) 로 대규모 Rust 코드베이스를 잘 다뤘지만, 복잡한 작업엔 여전히 한계가 있었음, Gemini CLI를 사용해 보니 설치는 쉬웠음에도, Rust 코드 변환에서는 명확히 Claude보다 품질이 떨어졌음, 그래도 ""코드를 완전 망쳤으니 모든 변경을 되돌리고 처음부터 다시 하겠다""는 답변을 받았는데, 오히려 이 정도 자기 인식과 리셋이 오늘 하루의 하이라이트였음
          + Gemini에는 재미있는 오류 반응들이 있음, 자기 자신이 실수했다는 듯한 말투(예: ""이건 예상치 못했어요!"", ""이제 마지막 테스트는 통과할 거예요!"")로 대답하기도 함, 시스템 프롬프트를 바꾸지 않아도 평소에 매우 자신감 넘치고 감정적인 반응을 보임, 아마도 결과를 시각화하거나 나타내는 식의 언어가 훈련에 더 효과적이어서 이런 경향이 생긴 것 같음
          + 내 생각엔 Gemini가 구글 내부 코드베이스 전체 학습의 이익을 보고 있음, Rust는 구글 내부에서 채택이 적고 멋진 C++ 툴이 많아 Gemini가 Rust만치 약한 듯함
          + 나도 비슷한 경험을 했음, 새로운 기능 구현 실험 중 undefined 함수 등 여러 문제로 결국 포기함, Claude 역시 완벽하진 않았지만 적어도 코드가 동작했음, Gemini의 결과물이 더 세련됐지만 마무리가 부족했음
          + 나 역시 동일하게 사용했는데, 15분 만에 똑같이 ""리셋"" 행동을 보여줬음
          + Claude도 상황이 심각하면 스스로 처음부터 다시 시작하는 경우가 있음
     * Gemini Code Assist를 사용할 때 모든 코드가 Google로 전송됨, 공식 가이드에 따르면 프롬프트/관련 코드/생성 결과물/피드백/특정 기능 사용 정보 등 모두 수집하고, 인간 리뷰어가 18개월간 익명화된 데이터를 볼 수 있음, 기밀 정보 또는 남에게 알리고 싶지 않은 데이터를 입력하지 말라는 안내임
          + 실제로는 더 세분된 정책이 적용됨, 무료 Code Assist의 경우 데이터가 기본적으로 사용되지만, 별도 옵트아웃 설정이 있으며, 유료 Code Assist나 유료 API 이용 시 데이터가 머신러닝 개선에 쓰이지 않음, 일반 무료 계정에서 별도 설정하지 않는 경우만 데이터가 활용됨
          + Gemini CLI의 개인정보 정책이 로그인 방식에 따라 혼란스러웠다는 점 인정함, 모든 유형의 계정별 정책 및 FAQ를 한 문서로 정리해 공유함, 이러한 투명성 요구에 감사함
          + Gemini 생태계에서 가장 답답한 부분이 바로 개인정보 정책임, 2.5 pro가 현재 최고 모델이라 생각함에도 정말 혼란스럽고 일관성 없는 가이드 때문에 실무에 활용하는 게 꺼려짐, 아무리 비싼 유료 플랜을 써도 다를 게 없어 보임, 개선을 강력히 희망함
          + Mozilla와 Google이 제공하는 gemmafile이라는 솔루션도 있음, 완전히 독립적으로 동작하는 Gemini(Gemma)로, 의존성 없는 싱글 바이너리 형태임, 실제로 32%의 조직이 이 방식으로 Gemini를 활용 중임
          + 설정 문서의 ""Usage Statistics"" 부분에 숨어 있는 내용도 있음, 개인정보, 프롬프트, 파일 내용 등은 저장하지 않는다는 설명이 있음
     * Gemini CLI의 시스템 프롬프트를 코드(Gist)로 볼 수 있고, 별도 개인 블로그에도 사용기와 노트가 정리되어 있음
          + Gemini CLI가 오픈소스라서 시스템 프롬프트 위치가 오픈되어 있음
          + 절대경로 사용만 명시돼 있는데, 예제에는 상대경로가 있어서 약간 혼동됨
     * 며칠 전 Claude Code로 vibe coding 방식으로 streamlit 파이썬 앱을 만들어 봤는데, 어느 순간부터 복잡한 버그에선 더는 해결을 못함, Gemini CLI는 훨씬 더 큰 프로젝트 사이즈까지 잘 소화해냈으며, ""코드 전체 분석 및 버그 수정""만 지시하면 대부분 작동됨, 진짜 미래를 살고 있는 기분임
          + 컨텍스트 윈도우 크기의 차이 때문인지 궁금함, Gemini는 Claude보다 5배 더 큼, Claude로 사이드 프로젝트를 진행할 때 항상 컨텍스트 제한에 걸려 디테일이 손실됨, Gemini로 이게 해결될지 기대 중임
          + Claude Code의 최적 활용법은 Gemini Pro 2.5나 o3/o3pro에 중량 작업을 맡기는 것임, MCP 지원 덕분에 이제는 여러 모델을 긴밀히 연동할 수 있음, 앞으로는 어떤 LLM 모델이든 CLI 에이전트 형태로 꽂아 쓸 수 있는 게 표준이 될 듯함, 결국 ChatGPT 같은 브랜드 기반 대중 UI는 실무에선 우위가 없음
          + 각 모듈마다 100줄 요약 문서를 미리 AI로 작성하게 하면, 상세 대신 참조 경로만 적고, 이를 기반으로 AI가 필요한 문맥을 파악하여 능률적으로 작업 가능함, 만약 100줄로 요약 못 하는 모듈이면 리팩터링할 시점임, 결국 LLM에게도 중요 맥락만 정확히 제공해야 함
          + 프롬프트 공학과 구체적 지시가 오히려 더 효율적임, ""버그 전부 고쳐줘""는 현실적 활용엔 효과적이지 않을 수도 있음
          + 하지만 진짜 복잡성에서는 쉽게 무너짐, vibe coding으로 코딩하면 불필요한 대량 코드가 생기고 직접 작성할 때보다 메모리 비효율이 커짐, 앞으로 이런 방식이 늘면 DRAM 수요 증가도 기대됨
     * Go나 Rust로 작성되었으면 싶었음, Node 런타임 설치가 꼭 필요하지 않은 싱글 바이너리 CLI가 더 좋았을 것임
          + 이런 프로젝트들은 업데이트가 잦으므로 npm/pip 같은 것에서 자동 처리하는 게 더 현실적인 듯함, 실제로 무거운 작업을 하지는 않으니 Node도 큰 무리는 아님, 물론 원칙적으로 Go였다면 더 완벽했을 거라 생각함
          + Gemini CLI에 자체 리라이트를 시켜보라고 제안함, 원하는 언어로 스스로 코드 생성 가능
          + 품질이 중요한 게 아니라, 경쟁사들이 다 CLI툴을 내세우다 보니 형식상 도입한 느낌임
          + 실제 실행파일로 만들고 싶으면 Bun이나 Deno로 패키징해서 빌드가 가능할 듯, Node 코드에 특이점이 없으면 Bun 쪽에서 독립 실행파일로 만들 수 있음, exe 사이즈가 Go, Rust와 얼마나 차이날지 궁금함
          + OpenAI도 Codex CLI를 TypeScript에서 Rust로 리빌드 중임, 개인적으로 Node 설치 경험이 무난했고 패키징도 잘 되어 있어서 방법에 상관없이 문제없음
     * ""Google Workspace 계정은 로그인 실패"" 메시지를 받음, Gemini CLI가 비상업 사용자 전용이라면 당황스러움, 구글 서비스들에서 workspace 계정은 정말 불합리하게 빈번히 제한됨, 예전엔 GSuite 계정으로 이메일만 필요했는데 여러 데이터와 접근성이 매번 제한되고, 유료로 구독해도 랜덤하게 기능이 적용되거나 차단됨, 이번엔 아예 workspace 계정 유저는 Gemini CLI 이용 자체가 차단되어 있어 충성 고객으로서 서운함
          + 공식 GitHub 인증 가이드를 참고하면 도움이 될지 안내함
          + GOOGLE_CLOUD_PROJECT 환경 변수 등 추가 설정도 필요함
     * 약 한 달간 이 툴을 사용하면서, 2.5pro가 SOTA이며 100만토큰의 대형 컨텍스트 윈도우 덕에 정말 강력함을 느낌, 큰 코드베이스마저도 손쉽게 분석하고 학습함
          + 최근 Cursor에서 사용할 땐, 대형 파이썬 파일에서 import가 깨지는 현상이 있었음, Claude는 이런 문제가 없었음, Gemini CLI로 오늘 직접 써볼 생각임

   크게 기대를 하진 않고 사용해봤는데, 생각보다도 더 빠르고 성능도 만족스럽더군요

   구글 워크스페이스 사용자를 위한 인증 방법을 올렸습니다. https://news.hada.io/topic?id=21662
"
"https://news.hada.io/topic?id=21680","Show GN: LunaTools 구글크롬 확장프로그램 (마우스제스처, 좌우키페이지이동, 탭중복제거/정렬/병합, PiP, 환율/단위/시각변환, 영상회전, 사이트잠금/차단, 드래그복사/열기, 여러U","                                                                                                                                                                                                                                                                                                                                                                                                                                                             Show GN: LunaTools 구글크롬 확장프로그램 (마우스제스처, 좌우키페이지이동, 탭중복제거/정렬/병합, PiP, 환율/단위/시각변환, 영상회전, 사이트잠금/차단, 드래그복사/열기, 여러U

   제가 구글 크롬 쓰면서 웹 브라우징을 편하게 해줬던 유용하게 자주 썼던 확장 프로그램을 구글 제미나이로 바이브 코딩으로 만들어봤습니다. 그리고 이런 기능 있었으면 좋겠다고 생각한 것도 아이디어 생각해서 새롭게 만들어서 추가했습니다.

   평소에 구글 크롬에서 탭을 많이 열고 쓰시는 분들, 키보드 단축키 사용을 즐겨 하시는 분들, PiP 기능 자주 쓰시는 분들은 유용하게 쓰실 수 있을 겁니다.

   지난번엔 긱뉴스에 처음 공개했을 때는 기능 5개 정도였는데 9개 정도 기능 더 추가해서 14개 기능 갖추게 됐습니다. 혹시 더 추가할 만한 기능 아이디어 있으시면 댓글 남겨주시면 감사드리겠습니다.

  기능

    1. 마우스 제스처, 우클릭 후 ← 뒤로, → 앞으로, ↑ 새 탭, ↓ 닫기
    2. 사이트 URL 에 페이지 번호 있을 시 키보드 좌우키로 페이지 이동
    3. 새로운 탭을 열었을 때 기존에 있는 탭일 경우 해당 탭 닫고, 기존 탭으로 포커싱
    4. Alt + A 를 누르면 현재 창의 탭을 URL 기준으로 정렬
    5. 확장 프로그램 아이콘 오른쪽으로 클릭 후 컨텍스트 메뉴에서 여러 탭을 1개의 창으로 합치기
    6. 현재 보고 있는 영상 Shift + Ctrl + P 눌러서 PiP 로 열기
    7. 금액/단위/시각 텍스트 선택한 다음 Alt + Z 누르면 환율/단위/시각 변환
    8. Shift + Ctrl + Alt + R 눌러서 영상 회전
    9. 사이트 잠금 기능, 창 닫을 때 한번 더 확인 (옵션에서 설정 가능, 웹 브라우저 정책으로 인해 사이트와 상호작용이 이뤄진 이후 잠금 기능이 작동)
   10. 사이트 차단 기능, 접속 시 about:blank 로 이동 (옵션에서 설정 가능)
   11. Shift + 드래그 탭으로 열기
   12. Ctrl + 드래그 링크 복사
   13. Alt + 드래그 2초 지연 탭으로 열기
   14. ALT + L 여러 URL 열기 기능 (목록 저장, 탭 가져오기, 목록 JSON 내보내기/가져오기, URL 줄바꿈 형태 TXT 가져오기, 정렬/중복 제거, 실행 간격 설정, 실행시 중복 제거, 백그라운드 탭 열기, 로딩 지원 지원)
"
"https://news.hada.io/topic?id=21636","파워 맥 G3 ROM에서 27년된 이스터에그를 발견한 이야기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   파워 맥 G3 ROM에서 27년된 이스터에그를 발견한 이야기

     * Power Mac G3 ROM 내부를 분석하다가 독특한 이스터에그를 우연히 찾아낸 경험 소개임
     * HPOE라는 리소스에서 개발자 사진이 숨겨져 있음을 이미 알려졌지만, 실제 표시 방법은 지금껏 미스터리였음
     * ""secret ROM image"" 라는 텍스트가 SCSI Manager 코드에서 등장하며 실마리를 제공함
     * RAM 디스크를 특정 이름으로 포맷하면 ""The Team"" 파일이 생성되고, 이 파일이 숨겨진 이미지를 공개함
     * 이 방식은 Mac OS 9.0.4까지 동작하며, 이번 발견이 이 이스터에그의 최초 공개로 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Power Mac G3 ROM에서의 이스터에그 발견 배경

     * 최근 Power Macintosh G3의 ROM을 분석하는 과정에서, 지금껏 공식적으로 문서화되지 않은 이스터에그를 발견함
     * 분석에는 Hex Fiend와, Eric Harmon의 Mac ROM 템플릿인 ROM Fiend가 사용됨
     * 1997년부터 1999년까지 사용됐던 beige 데스크탑, 미니타워, all-in-one G3 모델 ROM에 해당함
     * 개발자 본인도 G3가 27년이 넘었다는 사실에 감탄

ROM에서의 흥미로운 발견 요약

     * 첫 번째로 눈에 띈 점은 HPOE 타입 리소스에 있는 JPEG 이미지였으며, 이는 당시 이 모델 개발에 참여했던 인물들의 단체 사진으로 추정됨
     * 이 이미지는 이전에도 언급된 적 있지만, 실제로 표시하는 법은 밝혀지지 않았음
     * 두 번째로, nitt 리소스 ID 43(Native 4.3) 에서 흥미로운 Pascal 문자열을 발견함: "".Edisk"", ""secret ROM image"", ""The Team""
     * ""secret ROM image""는 도스 맥의 이스터에그와 연관될 수 있다고 추정했으나, 기존 방식은 G3에서는 동작하지 않음

코드 분석 및 방법 도출 과정

     * 코드를 역어셈블하기로 결정하고, 전체 nitt43 파일을 Ghidra 프로그램으로 분석함
     * PowerPC 어셈블리 코드는 익숙하지 않았으나 Ghidra의 디컴파일러 덕분에 가독성이 높은 코드로 변환 가능했음
     * 주요 함수가 .EDisk(RAM 디스크) 드라이버와 상호작용하는 것으로 드러남
     * 이름이 ""secret ROM image""인 볼륨(=RAM 디스크)이 감지되면, HPOE 리소스 ID 1에 담긴 JPEG 데이터를 ""The Team""이란 파일에 기록함

실제 이스터에그 발동 방법

     * 분석 결과, RAM 디스크를 ""secret ROM image""라는 이름으로 포맷하면 이스터에그가 실행됨
     * Infinite Mac 프로젝트 등 온라인 에뮬레이터에서 이 방법을 확인할 수 있음
     * 구체적 순서:
          + 메모리 제어판에서 RAM 디스크 활성화
          + Special 메뉴에서 Restart 선택
          + 바탕화면 복귀 후 RAM 디스크 아이콘 선택
          + Special 메뉴에서 Erase Disk 선택
          + 이름을 ""secret ROM image""로 정확히 입력 후 Erase 클릭
          + 포맷한 RAM 디스크를 열면 ""The Team"" 파일이 생성됨
     * 파일은 SimpleText 등으로 열어서 내부 이미지를 확인 가능

이스터에그의 의의 및 마무리

     * 이 방법은 Mac OS 9.0.4 버전까지 정상 동작하는 것으로 테스트됨
     * 지금까지 해당 이미지가 ROM에 있다는 사실은 알려져 있었으나, 실제 발동 방법은 이번에 처음으로 완전히 밝혀짐
     * 이는 Steve Jobs가 1997년 Apple로 복귀한 후 공식적으로 금지했던 마지막 이스터에그들 중 하나일 가능성이 큼
     * 여러 테스트와 힌트 덕분에 숨겨진 기능이 세상에 드러난 것으로, 올드맥 하드웨어와 소프트웨어 분석 및 역사적 기술 발굴 측면에서 큰 의미를 가짐

마무리

     * RAM 디스크를 이름 바꿔 포맷하는 단순한 방법으로 27년간 잠자고 있던 개발팀의 비공식 추억 이미지가 드러난 사례임
     * 이스터에그 기술과 Apple 개발문화, 숨은 개발자들의 흔적에 관심 있는 독자들에게 흥미로운 사례임

        Hacker News 의견

     * 이런 이스터 에그는 초창기 데스크탑 PC 시대의 감성을 강하게 주는 요소라고 생각함, 덕분에 실제 사람이 만든 소프트웨어라는 걸 더 실감할 수 있음, 예전에는 소수의 열정적인 개발자가 모여서 만든 작품이 많았던 추억이 떠오름, 요즘은 제품의 이미지를 완전히 통제하려는 Product People(기획/관리 부서)이 있어서 의도적으로 소름 끼칠 만큼 비인간적으로 느껴지기도 한다고 느낌, 상상해보면 오늘날 내 iPhone 안에 이스터에그가 있어서 작동시킬 때마다 랜덤으로 개발에 참여한 일부 인물의 사진이나 이름이 나온다면 즐거울 듯함, 하지만 아마 Product People의 기준에는 맞지 않을 것 같다는 생각
          + 예전에는 이런 이스터 에그가 Agile 도입 전에는 더 많지 않았을까 궁금함, deadline 기반의 개발을 하면 팀 일부가 기다릴 수밖에 없는 상황이 종종 생기고 백로그도 한계라 이런 틈에 작은 '동기'와 '기회'가 생겨서 이스터 에그 같은 게 많이 들어갔던 추억
          + Product People 에 대한 비판은 과한 느낌, 옛날에는 몇 명이 만들었지만 지금은 수천 명이 한 프로젝트에 얽혀 있는 현실, 대형 소프트웨어에서 일부 개발자의 이름만 드러나는 이스터에그는 의미 없게 되는 상황
     * 예전에 Apple에서 ASIC 및 보드 설계하던 동료와 일했는데, 그 친구가 한 번은 엄청 열심히 시스템 하드웨어 개발하고 디버깅해도, 소프트웨어팀은 ROM 용량 낭비하면서 자신들만을 미화하는 팀 소개 이미지를 넣고 하드웨어 팀은 완전히 무시당했다고 아쉬워했던 기억
     * 기업 뒷면에 실제 사람들이 있다는 걸 보여주는 건 멋진 일이라 생각, 큰 부자들은 “내가 이걸 만들었지”라며 실제 공로를 가져가려 하곤 하는데, 진짜 실무에 나선 평범한 사람들이 역사의 한 귀퉁이에 ‘우리의 영혼이 녹아 있다’는 흔적을 남김, 물론 Steve Jobs라면 이런 이스터에그를 금지했을지도
          + 개인적으로 Jobs 팬은 아니었지만, 너무 단순하게 악역처럼 보는 것도 부당하다는 생각, 이런 부분에서는 오히려 Jobs가 주도적이었다는 역사도 있음 Apple의 서명 파티 참고, 참고로 Microsoft는 2000년대 초부터 “no easter eggs” 정책이었음, 꼭 Jobs만의 특징은 아니었다는 점
          + Jobs가 위기 상황에서 Apple로 돌아왔을 때 어딘가에 숨겨진 obscure ROM 이미지 존재까지 파악하고 있었을 리 없다고 생각, 오히려 평범한 엔지니어가 청소하다가 지운 듯, Jobs는 오히려 Apple의 훌륭한 팀과 창의성을 자주 칭찬했고, 좋은 팀 빌딩의 중요성도 여러번 언급한 적이 있음
          + 이윤이란 결국 임금으로 지급되지 않은 노동을 훔치는 것이라는 신념도 있지만, Meta급 연봉과 관련해서는 옛 Mad Men 에서 개인의 공로 인정을 원할 때 “그게 바로 돈을 주는 이유”라는 명대사가 생각남
     * Amiga 컴퓨터 제작진이 남긴 유명한 메시지 “We made the Amiga, they f----d it up!”이 떠오르는 추억 관련 링크
     * 예전 컴퓨팅 시절의 소규모 팀이 항상 흥미롭게 느껴짐, 언젠가 다시 그렇게 돌아갈 수 있었으면 하는 바람
     * 90년대에 도서관에서 MacWorld 읽으면서, 클릭 몇 번과 키 입력으로 프로세서와 관련된 뭔가를 언락하는 트릭을 알게 되었던 기억, 완전히 어떤 기능이었는지는 기억 안 나지만, Apple IIci 33mhz 프로세서와 관련된 것이었음
          + 개인적으로 System 7.5에서 “secret about box”라는 텍스트 클리핑을 데스크탑에 끌어다 놓으면 dev 팀 멤버 명단이 ‘벽돌’로 나오는 breakout 게임이 열리던 게 제일 좋았던 추억, 완전 즐거운 시절
     * iPad에서 playground 앱으로 약간의 코드만 써서 ROM에서 다음 로고를 불러오는 이스터에그를 찾았던 경험 있음, 그 당시 따라해봤는데 그 뒤로는 어떤 레퍼런스도 못 찾아봄, 대략 6년 전 이야기
     * 이스터에그가 너무 그리움, 다시 부활하면 좋겠다는 생각
          + 동의, 요즘은 소규모 프로젝트에서만 조금씩 들어가지만 대형 프로젝트에서도 부활했으면 하는 바람, 어렸을 때는 이스터에그가 있다는 소문만으로도 해당 제품을 더 오래 써보게 되곤 했던 추억, 요즘은 Android마저 이스터에그가 밋밋해지고 개발자 모드에서 겨우 해금해도 별 감흥 없어서 아쉬움이 큼
          + 우리 팀 프로젝트에는 작은 이스터에그가 FILE_ID.DIZ에 들어있음 링크
     * 이런 이스터에그 메커니즘이 오랜 시간 동안 누군가에게 발견되지 않은 점이 인상적, 리버스 엔지니어링(역분석)이 진짜 어렵다는 걸 느끼게 됨, 입문하려면 어디서부터 시작할 수 있을지, 온라인 튜토리얼이나 추천 도서 있으면 조언 부탁
          + 비디오게임, 특히 NES 같은 올드 콘솔이 입문에 좋은 분야라 추천, 시도해보면 결과가 바로 보여서 흥미도 높고 관련 툴도 많음, 예를 들어 Mesen 에서 NES 게임을 디버깅툴과 함께 실행해보고 nesdev.org 찾아보는 방식, 이미 역분석된 게임이라면 Data Crystal에서 추가 정보 찾는 것도 팁, 현대 소프트웨어는 더 어렵지만 예전에 Gamecube 게임 해킹하면서 글을 쓴 적 있음 Super Monkey Ball 해킹 파트1 파트2, decompilation with ghidra HN 토론도 참고
     * 이런 건 Stump the Experts(애플 관련 퀴즈쇼)에 나왔다면 완전 딱이었을 소재인데 아쉬움
"
"https://news.hada.io/topic?id=21640","경험의 설계자들: AI 시대의 소비자 경험과 비즈니스 모델 혁신","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  경험의 설계자들: AI 시대의 소비자 경험과 비즈니스 모델 혁신

     * 생성형 AI(Gen AI) 의 부상은 이미 시작됐으며, 소비자 테크 플랫폼의 미래를 결정할 차세대 모델을 찾기 위한 실험이 치열하게 진행 중임
     * 인터넷, 모바일, AI로 이어지는 플랫폼 변화에서 성공 기업들은 각 시대의 본질적 이점을 극대화하여 산업 판도를 바꿔왔음
     * AI 시대의 승자는 기술 자체보다, AI를 핵심 비즈니스 모델로 삼아 무마찰적이고 예측적인 경험을 제공하는 기업이 될 전망
     * 각 세대의 혁신은 기술적 우위 → UX·성장 마케팅 → 비전·오케스트레이션으로 창업자/팀의 핵심 역량이 진화함
     * AI 도입에는 신뢰, 투명성, 통제권, 경제적 충격, 윤리적 문제, 규제 등 다수의 장벽이 존재하며, 이를 극복하는 노력이 필수적임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

PART I: 디지털 혁신의 힘 – 인터넷, 모바일, AI

  인터넷 혁명: 집계, 중개 해체, 디지털 상거래의 부상

     * 1990년대 후반~2000년대 초반, 인터넷은 오프라인 중심의 비효율적 비즈니스 구조를 해체
     * Amazon, eBay, Expedia 등은 집계(aggregation) 와 중개 해체(disintermediation) 를 통해 소비자가 직접 다양한 제품/서비스에 접근하도록 만듦
     * 주요 혁신:
          + 비용 절감: 물리적 인프라, 인력 비용 제거
          + 편의성 확대: 집에서 다양한 서비스 이용 가능
          + 선택지/접근성 증가: 온라인 집계 플랫폼으로 비교 및 탐색 용이

  모바일 혁명: 즉각적 접근성과 행동 패턴 변화

     * 2000년대 후반~2010년대 초반, 모바일 기술은 상거래와 엔터테인먼트, 커뮤니케이션을 실시간, 어디서든 가능한 형태로 변화시킴
     * Instagram, Uber, Instacart, Spotify 등은 즉각적 결제, 위치 기반 서비스, 개인화 추천, 실시간 알림 등 즉시성과 몰입형 경험 제공
     * 주요 혁신:
          + 즉각적 트랜잭션: 원클릭 결제, 실시간 예약 등
          + 개인화: 행동/위치 기반 추천
          + 능동적 참여 유도: 알림, 추천 등으로 적극적 개입

  AI 시대: 지능적, 개인화, 예측적 경험의 진화

     * AI-native 기업은 검색·탐색 중심에서 예측적, 자동화, 선제적(interactive → proactive) 상호작용으로 전환
     * Shopify Magic, Perplexity AI 등은 대화형 커머스, Runway AI 등은 생성형 미디어, Origin·Monarch Money 등은 자동화 금융, Decagon·Sierra 등은 예측형 고객지원 실현
     * 주요 혁신:
          + 마찰 최소화: 사용자가 직접 검색/결정 없이 AI가 미리 제안 및 실행
          + 대규모 개인화: 수백만 명을 개별화된 경험으로 관리 가능
          + 반복 업무 자동화: 사용자는 더 가치 있는 활동에 집중
     * 핵심 메시지: 인터넷 시대가 ‘접근성’, 모바일이 ‘즉시성’을 상징했다면, AI 시대는 ‘노력 없음(Effortlessness)’이 본질

PART II: 비즈니스 모델의 진화 – 각 세대별 승자의 전략

  1세대(인터넷): 집계와 네트워크 효과

     * 핵심 장점: 네트워크 효과, 데이터 규모, 자산 경량화(Asset-light scalability)
     * 다수의 공급자·수요자를 집결, 자체 성장 루프(네트워크 효과) 창출
     * 약점: 경험은 여전히 정적이고 소비자가 능동적으로 개입해야 했음

  2세대(모바일): 실시간성·개인화·온디맨드

     * 핵심 장점: 실시간 접근성, 빈번한 참여(engagement), 개인화 추천
     * UX, 행동 데이터 기반으로 실시간·중독성 높은 서비스 확대
     * 약점: 여전히 소비자가 앱/서비스에 직접 개입해야 함

  3세대(AI): 자동화·지능화·지속적 학습

     * 핵심 장점: 자동 의사결정, 예측·지속적 개선(Continuous learning), AI가 능동적으로 경험 설계
     * AI가 스스로 학습하며, 개인별 맥락을 축적해 전환 비용 극대화(탈피 어려움)
     * 핵심 승자 전략: 자체 진화하는 AI 시스템, 완전 자동화·개인화 경험 제공

PART III: 각 시대의 승자 – 엔지니어, 디자이너, 그리고 ‘설계자(Architects)’

  1세대(인터넷): 엔지니어의 시대

     * 주요 역량: 인프라 구축, 소프트웨어 개발(프로그래밍·DB·웹 아키텍처)
     * 모든 시스템을 직접 구축해야 했던 시기, 기술력이 최대 경쟁력

  2세대(모바일): 디자이너와 성장 마케터의 시대

     * 주요 역량: 모바일 UX, 데이터 기반 성장, 행동 심리학, 상품화 전략
     * UX, A/B 테스트, 리텐션, 바이럴, 실시간 서비스 경험이 핵심

  3세대(AI): 제품 비전·오케스트레이션의 시대

     * 주요 역량: 소비자 인사이트, AI 오케스트레이션(조합/연결), 데이터 전략, 신뢰·투명성·배포력
     * 기술력은 API·AI 툴로 평준화, 누가 더 잘 조합해 최고의 경험을 만들어내는가가 승패 좌우
     * 제품 아키텍처와 배포(Distribution) 능력이 경쟁력의 본질로 부상

PART IV: AI 도입의 과제와 해법

  주요 채택 장벽

     * 신뢰·투명성: AI의 의사결정 원리·이유를 알지 못할 때 불신 유발
     * 통제권 상실: 자동화에 따른 소비자의 주체성 약화 우려
     * 직업 대체 불안: 자동화로 인한 일자리 위협, 산업 내 저항
     * 편향·윤리 이슈: 데이터 기반 편향·차별에 대한 우려
     * 규제 불확실성: 데이터 보호·책임 문제 등 미비한 제도

  해법 방향

     * 설명가능한 AI(Explainability): 의사결정 근거 설명, 대시보드 도입 등
     * AI+인간 하이브리드: 자동화와 인간 최종 확인 결합
     * 윤리·규제 대응: 외부 감사, AI 윤리 보드, 정책 연계 강화
     * 직업 재교육: 일자리 변화에 맞는 전환 프로그램, 신기술 기반 역할 확대

결론: 경험의 설계자가 미래의 승자

     * AI 시대는 접근성(인터넷), 즉시성(모바일) 을 넘어 노력 없는 경험(Effortlessness) 이 핵심
     * 진정한 승자는 AI를 비즈니스 모델 그 자체로 내재화하며, 자기 진화 시스템과 보이지 않는 혁신으로 소비자 가치를 선제적으로 제공하는 기업
"
"https://news.hada.io/topic?id=21619","베라 C. 루빈 천문대 첫 이미지 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         베라 C. 루빈 천문대 첫 이미지 공개

     * Vera C. Rubin Observatory가 촬영한 첫 우주 이미지가 공개됨
     * 이 이미지는 은하와 별로 가득한 우주의 풍부함을 보여줌
     * 약 5,500만 광년 떨어진 Virgo Cluster의 남쪽 영역을 중점적으로 촬영함
     * 밝은 별, 청색 나선은하, 적색 은하 그룹 등 다양한 천체가 포함됨
     * 앞으로 10년간 Legacy Survey of Space and Time를 통해 우주 기원과 암흑 물질 같은 의문을 풀 실마리 제공 예정임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Rubin 천문대의 우주 보물 상자 소개

   Rubin Observatory는 NSF–DOE Vera C. Rubin Observatory의 첫 번째 '우주 보물 상자' 데이터를 소개함
   이 데이터는 과학자들에게 새로운 발견의 기회를 제공할 소중한 자료임
   이번에 공개된 이미지는 Rubin Observatory가 촬영한 첫 이미지 중 하나로, 별과 은하가 풍부하게 펼쳐진 우주의 모습을 드러냄
   겉보기에는 텅 빈 검은 공간처럼 보이던 영역들이 빛나는 천체의 장으로 변모하는 광경을 처음으로 공개함
   이처럼 크고 풍부한 색상의 이미지를 신속하게 제작할 수 있는 곳은 Rubin Observatory뿐임

Virgo Cluster 남쪽 영역 관측

   Rubin Observatory의 시야는 지구에서 약 5,500만 광년 떨어진 가장 가까운 대규모 은하 집단 중 하나인 Virgo Cluster의 남쪽에 집중됨
   해당 이미지는 파란색부터 빨간색까지 다양한 색의 밝은 별, 근처의 푸른 나선 은하, 멀리 떨어진 붉은색 은하 그룹 등 다양한 천체를 보여줌
   Rubin 데이터가 제공하는 과학적 탐구의 범위가 매우 넓음을 입증함

Legacy Survey of Space and Time 프로젝트와 미래 연구

   향후 10년 동안 전 세계 과학자들은 Rubin Observatory의 방대한 우주 데이터를 활용할 예정임
   주요 연구 주제로는
     * 우리 은하(Milky Way)의 형성 방식
     * 우주의 95%를 차지하지만 볼 수 없는 물질(암흑 물질과 암흑 에너지)의 정체
     * 태양계 천체의 상세 목록 작성
     * 10년 동안 수억 개의 밤하늘 변화 감시로 밝혀질 새로운 사실들
       등이 포함됨

        Hacker News 의견

     * 위키피디아의 Vera C. Rubin Observatory 문서가 정말 유용한 정보 저장소라는 느낌. 참고 자료 부분에 더 심화된 내용이 가득하고, 여성 연구자가 센서 모델을 들고 있는 사진에 달이 같이 있어서 실제 크기 비교에 도움을 준다는 점이 인상적이라는 생각. 포컬 플레인(초점면)이 평평한지 궁금했는데 실제로 평평하고, 이미지 촬영 후 데이터 처리가 '60초 이내', '하루 단위', '연 단위'의 세 스케줄로 진행된다는 사실이 인상 깊은 부분. 특히 관측 직후 60초 내에 밝기나 위치가 달라진 천체에 대한 알림이 발행되고, 이 어마어마한 분량의 이미지를 60초 만에 처리하는 것이 기존에는 몇 시간씩 걸렸던 것과 비교하면 대단한 소프트웨어 엔지니어링 과제라는 실감. 정부 비밀 시설에서 초고속 처리를 거쳐 보안상 중요한 정보가 빠지면 공개로 전환되고, 매일 밤 1천만
       건의 알림이 대중에게 공개될 예정이라는 점도 주목 포인트
          + 비공개 정부 시설에서 연산이 이뤄지는 이유가 비밀 정찰 위성 같은 민감 자산 때문일까 추측
     * 나는 Rubin Observatory가 마음에 드는 이유가 대부분의 사람들이 아주 깊게, 개별 대상만을 고배율로 관측하는 상황에만 집중하는 반면 Rubin은 아주 넓게 훨씬 방대한 데이터를 쌓는다는 장점 때문이라는 생각. 이 데이터가 폭넓은 통계로 우주론 모델을 개선하는 데 중요한 역할이라는 점. 내가 LSST 망원경 설계에 10년 전부터 참여했던 경험도 있기에 첫 이미지까지 도달하기까지의 긴 시간이 더욱 감탄 포인트라는 느낌. 비교적 짧게 IPO로 수십억 벌어들이는 기업 세상과는 다른 차원의 집중력 요구라는 점이 신기함
          + 딥 관측 역시 우주 기원 파악에서는 필수적이지만, Rubin Observatory는 실용성과 미래 지구 방어(소행성 충돌 예측 등) 측면에서도 엄청난 도구라는 생각
     * Rubin Observatory의 소행성 탐지 능력에 큰 감명을 받았고, 공식 사이트에서 관련 영상을 눈으로 확인 가능
          + 그리고 이 망원경이 초신성 관측에도 탁월하다는 점을 유튜브 영상과 다른 활용 사례 영상으로 설명 가능
          + 지금까지 본 영상 중 가장 담담한데 왠지 모를 소름이 났다는 느낌과 동시에 깊이 있는 스토리텔링이라는 평가
          + 실제 영상의 연출도 훌륭하고, 영상 프레임 중 일부에서 위성 흔적 제거 마스킹이 적용된 부분이 보인다는 관찰
          + Rubin Observatory가 실제로 소행성 충돌 예측과 탐지에 혁명적 역할이라는 기대
          + 이런 내용이 공식 홍보의 앞에 나왔어도 좋았을 만큼 핵심적인 임팩트라는 생각
     * 2010년 1월, 지금의 아내(천체물리학자)와 소개팅에서 이 기기를 주제로 Google이 엄청난 생(raw) 데이터를 수 페타바이트씩 처리해서 연구용 데이터셋으로 가공해준다는 대화를 나눴던 기억. Google의 현재 참여 여부는 잘 모르지만, 프로젝트의 긴 시간 진행을 결혼 생활 15년째와 함께 회상하면 이런 대형 기기가 실제 관측 시작까지 걸리는 시간이 정말 길고, 그만큼 얻는 혜택도 크다는 개인 경험
     * 이 관측소가 매일 쏟아낼 데이터량이 어마어마하다는 사실에 흥분됨. 수년 전부터 이 방대한 데이터를 신속하게 소화하고 과학적으로 활용하기 위한 인프라 구축이 진행 중이지만, 아직 과제도 남아 있다는 상황. 매일 수십 테라바이트 파이프라이닝 및 유통에 관심 있는 사람은 LSST 관련 GitHub 프로젝트를 참고하면 좋겠다는 제안
          + 오랫동안 Rubin Observatory 프로젝트를 지켜봤는데, 예산 규모와 컴퓨팅·네트워크 환경을 감안하면 이들이 처리하는 데이터 이동량은 해당 분야에선 이미 일상적 수준이라는 의견. 전체 스토리지(40~50페타바이트)는 방대하지만, 10테라바이트 급 데이터 이동 그 자체는 요즘 특별한 엔지니어링으로 여겨지지 않는다는 시각
          + 이런 데이터 문제는 고해상도 정찰위성도 겪는 부분과 유사성이 크지 않을까 하는 생각
     * Virgo Cluster(처녀자리 은하단)에서 주목받은 해당 구역의 SDSS(슬론 디지털 스카이 서베이) 뷰와 Rubin 관측 결과를 비교해서 그 심도 차이를 링크1, 링크2로 직접 확인 가능
          + 불투명도(Opacity) 슬라이더가 적용된 https://images.rubinobservatory.org/hips/asteroids/…"">비교 링크에서 손쉽게 시각적으로 차이 경험 가능
     * 직접 온라인이 되기를 기대하며, Rubin Observatory가 기존 관측 이미지들 간 변화(델타)를 탐지해서 움직이는 물체(근지구 소행성 등)를 잘 잡아낼 것으로 본다는 기대감. 특히 Oumuamua, Borisov처럼 외계 성간 천체가 들어올 때 조기경보를 받아 최신 대형 망원경들로 신속하게 정밀 연구할 수 있으면 꿈만 같겠다는 소망
          + Kuiper Belt(카이퍼 벨트) 대상 신발견과 서베이에도 혁신적인 역할 기대
     * 반대 방향으로 도는 나선은하 관측이 정말 신기하다는 감상과 함께, Skyviewer에서 위치 직접 보기
          + 관심 위치 좌표를 천문 데이터베이스(예시: Aladin)에 붙여넣으면 오른쪽 클릭으로 세부 대상 정보까지 확인 가능함을 예시 링크로 안내
          + 여러 은하가 닮았으면서도 각도가 다르게 보이는 점에서 혹시 중력 렌즈 효과가 작용 중일지 궁금함. 흥미로운 대상1, 대상2, 대상3 링크 참고
          + 참고로 /embed 대신 /explorer로 접속하면 이미지를 전체로 확대해서 볼 수 있다는 팁과 예시 링크
          + 이 은하들이 정말 같은 평면상에 있는지, 아니면 단지 시선 방향상 우연히 겹친 건지 판단이 어려우나, 크기가 비슷해 보여 동평면에 있을 가능성이 높다는 느낌적 판단
     * 뭔가 녹색이 인상적인 구조를 발견해서 직접 보기
     * 조금만 확대해도 흥미로운 대상이 자꾸 발견되는 점이 롱런 관측의 묘미라는 느낌. 특히 M61(이미지 하단 중앙부 대형 나선은하)에서 붉은 거성까지 쭉 뻗은 희미한 빛줄기가 나선 팔의 연장선과 달리 너무 곧고 중심축에서 벗어난 듯 보여 무엇인지 호기심. 조사 결과 M61의 조수 꼬리(tidal tail)는 심층 천체사진에서는 이미 알려져 있으나, 실제로 감지하고 언급된 사례는 극히 드물었다는 점이 흥미 요소
"
"https://news.hada.io/topic?id=21618","나의 윈도우 터미널 사용법 – tmux와 커스텀 워크플로우로 구현한 극강의 터미널 자동화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           나의 윈도우 터미널 사용법 – tmux와 커스텀 워크플로우로 구현한 극강의 터미널 자동화

     * tmux와 SSH, nvim, 강력한 검색·자동화 스크립트를 결합하여, 원격 서버의 파일을 GUI 없이도 즉시 탐색·수정·검색하는 독특한 워크플로우를 구현
     * 키 조합 하나로 tmux 복수 창에서 파일명 자동 검색 및 바로 열기, 파일 전환, 버퍼 관리까지 모두 단축키로 처리함
     * VSCode의 느린 속도와 키 바인드 충돌에 불만을 느껴 직접 스크립트로 모든 조작을 통합함
     * regex, perl, tmux, nvim 등 유닉스 도구들을 조합해 파일 경로·라인 정보 자동 인식 및 열기를 실현함
     * 직접 구현 과정이 매우 복잡하지만, 터미널의 강력함과 확장성을 극대화할 수 있음을 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

나만의 터미널 사용법

     * 이 글은 터미널과 도구를 결합해 효율적인 개발 환경을 구축한 경험을 공유함
     * 일반적으로 동영상이 필요한 복잡한 과정이기 때문에 텍스트와 함께 실동영상을 연동함(영상 감상 필수)
     * 영상의 일부 단계는(0:11, 0:21, 0:41)은 많은 사람들이 ""이게 가능하냐?""고 놀라는 부분임

영상 속 터미널 워크플로우 단계별 요약

     * 0:00 : Windows Terminal을 노트북에서 실행
     * 0:02 : ctrl + shift + 5 단축키로 새 터미널 탭을 열고, ssh로 집에 있는 데스크탑에 접속, 바로 tmux 실행
     * 0:03 : tmux에서 기본 쉘 zsh 실행, 프롬프트 표시 및 전체 설정을 비동기로 로드
     * 0:08 : zoxide로 최근 디렉토리 fuzzy 검색 후 이동
     * 0:09 : ripgrep 명령어 입력 시작, zsh가 이전 사용 이력 기반으로 자동완성, ctrl + f로 수락
     * 0:11 : ctrl + kf 단축키로 tmux 스크롤백 전체에서 파일명 검색, 파일명이 파란색으로 하이라이팅
     * 0:12 : n 키를 계속 눌러 검색된 파일 리스트 중 원하는 파일을 탐색
     * 0:21 : o 키로 선택한 파일을 기본 앱(nvim)에서 열기, tmux가 새 창에서 실행(여전히 원격 서버에서 작동)
     * 0:26 : rust-analyzer로 코드 내 여러 레퍼런스를 이동 시도, 일부 매크로는 인식 실패, 0:32에 정상적으로 이동 성공
     * 0:38 : ctrl + kh로 tmux 포커스를 왼쪽 창으로 전환
     * 0:39 : n 키를 다시 눌러 ""copy-mode"" 상태에서 이전 파일 검색 결과를 계속 탐색
     * 0:41 : o 키로 이번엔 다른 파일을 기존 nvim 인스턴스에 열기
     * 0:43 : b 키로 열려 있는 파일 버퍼 목록 확인, 두 파일을 번갈아 전환하며 마무리

왜 이렇게 터미널 워크플로우를 쓰게 됐는가?

     * VSCode가 느리고, 특히 vim 플러그인 사용 시 매우 버벅거림
          + 에디터, vim 플러그인, 터미널, 윈도우 관리 기능 간에 키 바인드 충돌이 자주 발생해 불편함
          + Zed 에디터도 시도해봤으나, 당시엔 아직 미성숙했고 키 바인드 충돌 문제도 여전했음
     * nvim(Neovim)을 터미널에서 직접 사용하기 시작했지만,
          + ripgrep 등으로 검색한 파일명을 일일이 복사해서 에디터에 붙여넣는 과정이 너무 번거로움
          + 특히 ripgrep 결과에서 파일명과 라인 번호가 함께 나오면,
               o 붙여넣을 때마다 구문 오류 발생
               o 실제로 파일을 열기 전에 불필요하게 편집하는 일이 많아짐
          + VSCode의 ctrl-click처럼 파일 경로를 바로 열 수 있는 워크플로우가 필요하다고 느낌
     * 그래서 tmux를 활용해 원하는 기능을 직접 구현하게 됨
          + 사람들이 왜 tmux를 쓰냐고 물으면, 바로 이 자동화와 세션 유지 때문이라고 설명
          + 터미널은 생각보다 훨씬 강력하지만, 기본 기능만으로는 이러한 자동화가 드러나지 않음
          + tmux는 비록 오래되고 문법이 복잡하며 버그도 있지만, 확장성과 커스터마이징 가능성 때문에 선택함

주요 구현 방법

  tmux에서 전체 스크롤백 파일명 검색

     * tmux copy-mode에서 복잡한 정규표현식으로 파일명 패턴 매칭
     * 직접 제작한 regex 스크립트(search-regex.sh)로 파일 경로, 라인, 컬럼까지 하이라이팅
     * 예시 tmux 설정:
bind-key f copy-mode \; send-keys -X search-backward '정규표현식'

  선택 파일을 새 창/현재 창에서 열기

     * tmux 단축키로 선택 파일을 디폴트 앱 또는 에디터(nvim 등)로 열도록 커스텀
     * 상대경로 지원 및 tilde 확장 포함
bind-key -T copy-mode-vi o  send-keys -X copy-pipe 'cd #{pane_current_path}; ...'
bind-key -T copy-mode-vi O  send-keys -X copy-pipe-and-cancel 'tmux send-keys ...'

  파일을 이미 열린 nvim 인스턴스에서 열기

     * perl 스크립트로 tmux가 특정 nvim pane을 찾아, 해당 pane에 파일/라인 정보까지 직접 키 입력 전달
     * file:line:column 구문을 vim 명령으로 변환해 자동 오픈

이 방식의 장점과 한계

     * 로컬 터미널의 기능 한계 극복: tmux를 통한 원격 서버의 자유로운 파일 탐색/수정/검색 구현
     * 에디터가 원격 편집 프로토콜을 지원하지 않아도 동일 워크플로우 가능
     * 모든 키 바인드·검색·파일 전환·버퍼 관리 등을 완전히 내 입맛에 맞게 통합
     * VSCode 플러그인 제작보다 더 빠르고 쉽게 자동화 가능
     * 직접 제작한 스크립트들은 깨지기 쉽고 유지보수 어렵기 때문에, 남에게 추천하긴 힘듦

대체 솔루션

     * 대체로 추천하는 오픈소스/도구 조합:
          + fish + zoxide + fzf: 퍼지 디렉토리, 명령 검색, 일부 파일 검색 워크플로우 대체 가능
          + 에디터 내장 기능(탭, 윈도우, fuzzy finder 등)도 대부분의 사용자에게 충분함
          + qf: 터미널 출력에서 파일 선택이 가능하지만, 상호작용적인 도구와는 연동 불가, vi-like CLI 사용
          + e: file:line 경로를 인식하는 소도구(에디터 종류별로 별도 스크립트 필요)
          + vim --remote, code filename, emacsclient 등도 비슷한 효과, 단 file:line 인식은 불완전함

결론 및 교훈

     * 터미널은 생각보다 훨씬 강력하며, 직접 스크립트로 조합하면 GUI 툴에서 불가능한 자동화 가능
     * 단, 유지보수성, 스크립트 내재된 버그 등 실용적 한계 존재
     * 터미널 워크플로우 자동화에 관심 있다면 커스텀 tmux/nvim/fzf 환경부터 단계적으로 구축하는 것이 현실적임

        Hacker News 의견

     * 나도 비슷한 트릭을 쓰고 있음. tmux 스크롤백을 활용하고, 토크나이즈된 출력을 zsh로 파이프해서 tmux 화면에 보이는 모든 것에 대해 자동완성 기능을 쓸 수 있음. 관련 쓰레드 포스트와 gist 코드 공유함. 정말 쓸모 많았음
     * 이런 워크플로우 방식 정말 마음에 들어서, 나도 비슷한 걸 수년간 반복하며 다듬고 있음. 시간이 흐르면서 커스텀 계층을 최대한 줄이려고 하고 있는데, 그 이유는 오버레이가 많아질수록 유지보수에 비용이 커지기 때문임. 순정 Vim(별도 tmux 없이)에서도 게시글에서 소개한 대부분을 할 수 있는데, 예를 들자면 rg --vimgrep restore_tool | vim -c cb -로 가능함 (vim -c cb -는 Vim에서 가장 좋아하는 기능이고, 다들 거의 안 쓰는 게 신기할 정도임). 물론 rg 검색을 매번 다시 실행하는 건 부담이 클 수 있어서, 나는 결과를 터미널에서 분석하다가 필요하면 tmux 커스텀 명령어로 마지막 명령의 출력을 복사해서 vim에 보내는 식으로 사용함. 이 때 프롬프트에 유니코드 문자를 활용하는 트릭을 쓰기도 하고, tmux saveb - | vim -c cb -로 넘기기도 함
          + 10년 전 방대한 멀티파일, 멀티패키지 vim 설정을 버리고, 매년 1~2줄 정도씩 아주 간단한 vimrc만 만들면서 점점 단순화 중임. 오래된 소프트웨어의 디폴트 설정에는 이유가 있기 마련이고, 무작정 바꾸지 말고 그 이유부터 이해해 보길 추천함
          + (vim -c cb -가 Vim에서 좋아하는 기능이라고 했는데 그게 뭘 하는지 설명해 줄 수 있는지 궁금함. ls | vim -와 ls | vim -c cb -를 해봐도 바로 차이가 뭔지 모르겠음)
          + 'vim -q <(ripgrep --vimgrep restore_tool)'와 같은 용도인지 궁금함
     * 이 셋업은 tmux, fzf, rg, zoxide, 그리고 깔끔한 nvim이 모두 포함되어 있어서 보기 좋음. 만약 없다면 atuin, starship, bat, glow, duf, dogdns, viddy, gum/sesh, dust, btop 등도 추천하고 싶음. GitHub의 Awesome Terminal XYZ 리스트에서 다 찾을 수 있음. atuin은 정말 필수급이고, zoxide가 없다면 운동선수인데 신발이 잘못 맞는 느낌임. 터미널 영상을 찍을 때는 asciinema가 더 나은 선택임. 사실 옛날에는 이런 셋업을 ""프로그래머다""라고 불렀음. 요즘은 VSCode, Zed, Cursor 등의 툴도 필수고, 어떤 LLM에 어떤 걸 맡겨야 하는지도 꿰고 있어야 함. 이런 툴은 이제 새로운 “최소한”일 뿐이고, 기존 환경을 대체하지는 않음. Claude Code, gptel 등을 아무리 잘 써도 때로는 트리 망가질 수도 있고, magit 없이 git을 쓴다는 건 상상 불가임. 혹시 Cursor 순정으로 OP보다 더 빠르다고 생각하면, 그 사람이 직접 LLM을
       실전으로 쓰는 영상 한 번 찍어보라고 함
          + 프로그래머라는 건 개발환경 세팅이 전부가 아님. 어떤 유명 프로그래머는 ex 에디터를 애용하고, 어떤 초보자는 IDE만 번지르르해도 근본적인 이해가 부족한 경우가 많음. 물론 좋은 도구가 도움될 수 있겠지만, 실제 성공에 미치는 영향은 보통 작았음. LLM이 이걸 바꿀 수도 있겠지만. 예를 들어 운동화가 잘못 맞아서 5% 느리다고 해도, 스타트업에서 성공과 실패를 가르는 건 그런 미세한 것들이 아님. 원인은 대부분 공동창업자 불화, 동기 상실, 제품-시장 미스매치 같은 더 큰 문제임
          + 네 툴 리스트는 좋지만, atuin, dogdns, btop 등 대부분의 툴 이름을 모르는 사람 입장에서는 다소 생소하게 보일 수도 있겠음
          + 명령어 리스트 정말 좋아함. 난 여기다가 fd(작성자: sharkdp)도 꼭 추가하겠음. find의 대체제인데 사용성 측면에서 훌륭함. 그리고 atuin은 내 CLI 인생에서는 가장 큰 업그레이드임. 6개월 전에 쳤던 희귀한 명령어도 아주 쉽게 찾을 수 있게 해줌
          + 도구 선택에 너무 집착하는 것 같음. 정말 좋은 개발자는 벌거벗은 환경에서도 빠르게 결과물을 냄. 물론 좋은 도구가 미세하게 더 도움될 수 있지만, 대부분은 나만의 재미와 취향 문제임. 생산성이 IDE에 얼마나 달려 있는지 정말로 느껴진다면, 아직 앞으로도 배우며 성장할 여정이 한참 남았다는 의미임. 예전부터 “도구를 아는 것 = 프로그래머”라는 공식은 아니었음. 내가 봤던 최고의 개발자들은 대부분 more/grep/vi에 사색만 얹어서 압도적인 결과물을 냄. 가치 창출은 결국 생각에서 나옴. 심지어 LLM 시대에도 이건 마찬가지임
     * 모든 vim/tmux 유저는 반쯤은 버그도 있고 빠르긴 한 비공식 “에미테이션 Emacs”를 직접 만들었을 확률이 높음
          + 나는 두 키 tmux 프리픽스를, 모든 명령을 위한 단일 ctrl-modified 키로 바꾸는 터미널 에뮬레이터까지 만들었음. 프로젝트 링크
          + 나처럼 vim/tmux와 emacs 양쪽을 쓰는데(그리고 예전엔 emacs만 오래 썼는데), 내 emacs 환경이 훨씬 더 즉흥적이고 문서화 덜 되고 버그도 많았음. vim+tmux 셋업은 상대적으로 안정적임
          + 그 말에 깊이 공감함. 나는 지금 워크플로우에서 nvim 안에서 :Term을 돌림
          + vim+tmux 환경은 파이프, 파일, 시그널, 스크롤백 같은 시스템 프리미티브에 많이 의존함. 그래서 툴링이 다양한 환경이나 ssh, 제한된 쉘에서도 자연스럽게 투명하게 작동함. 이게 이식성과 디버깅에서 큰 장점임. 즉, 이런 기본 보장 위에서 내 워크플로우가 자유롭게 조각나가는 느낌이라 vim 설정을 직접 만드는 게 너무 당연한 선택처럼 느껴짐
     * 그가 Windows에서 tmux를 쓴다는데 실제로 어떻게 사용하는 건지 궁금함. 설명 가능한 사람이 있으면 좋겠음
          + 내 생각에는 Unix 시스템에 원격 접속해서 실제 작업을 한다고 봄. Unix 서버에서 tmux를 완벽하게 돌릴 수 있음. 나도 VirtualBox VM에 SSH 접속해서 WSL보다 더 깔끔한 호환성 계층을 쓰기도 함. 이런 점에서 tmux가 다른 방법보다 강한 이유기도 함. 서버에 설치만 되어 있으면 원격으로도 본연의 역할을 다 함. 관련 설명 링크
     * 이런 워크플로우 공유 방식 정말 좋아함. 타깃 독자에게 딱 맞음. 나는 동영상에 소리가 있었으면 했지만, 액션 리스트를 나중에 읽는 것도 괜찮았음. 내 워크플로우에도 참고할만한 게 있어서 배웠음. tmux의 난해한 키보드 단축키를 언급했는데, 혹시 여기서 byobu 써본 사람? byobu는 tmux의 래퍼로, 대부분의 커맨드를 F# 키에 할당함. 10년 전에 처음 접해서 그 이후로 쭉 쓰고 있음. 그 전에 몇 년은 순수 tmux만 썼었음
          + 콘텐츠 재미있게 봤다니 기쁨 :) 최대한 명확하면서도 한눈에 볼 수 있게 만들려 노력함. 난 tmux 단축키 대부분을 리매핑해서 쓰고 있음. ctrl-k가 프리픽스로, h는 ""왼쪽 패널 선택"" 기본값이 아님. byobu는 사용해본 적 없지만 내용만 봐서는 기본 단축키가 좀 더 편한 정도라 크게 추가로 얹고 싶진 않음
     * 이런 식으로 직접 거대한 정규식 없이도 tmux 플러그인으로 복사 모드 기능 등을 확장할 수 있음. tmux-fpp, tmux-copycat, tmux-fingers, tmux-urlview 등이 있음. 내장 기능에 최대한 의존하는 게 속도에서 유리할 수 있으니 참고 바람. 난 특히 tmux-resurrect(세션 저장/복구), tmux-continuum(자동화 기능), Oh-My-Fish용 tmux-zen도 아주 좋아함. tmux-resurrect, tmux-continuum, tmux-zen 소개함. 멋진 tmux 환경 세팅은 생각보다 정말 쉽다는 점 강조함
          + 나도 tmux-copycat에서 원본 정규식을 가져왔음. 하지만 a) 그 정규식은 :를 잘 못 잡아내고, b) copycat은 자체 뷰어 추상화를 쓰다 보니 한 번의 검색에 오직 하나의 액션만 할 수 있음. 내 방법은 tmux 내장 검색을 재활용해서 하이라이트되는 파일에 원하는 액션을 자유롭게 바인딩할 수 있음. 대부분 플러그인이 복사/붙여넣기만 지원하거나 자체 모드를 만들어 올리는 이유도, tmux가 하이라이트 컨트롤을 거의 내장 검색에서만 할 수 있게 해줌
     * 이런 셋업은 정말 아름다움. 그런데 아직 제대로 된 AI 타입어헤드가 터미널에 등장하지 않은 게 미스테리임. Cursor 탭이 딱 맞을 것 같은데 터미널 적용이 막혀있음. 임시로 터미널 출력을 가짜 파일로 파이프해서 cursor로 보내는 제품을 뚝딱 만들고 싶은 마음도 듦
     * 기사 제목이 사실은 ""How I use my terminal""임
          + HN 자동 기능이 제목 첫 단어가 How이면 잘라버림. 딱히 근거는 없고, 관리자가 클릭베이트 방지라 설명하지만 실제로 혼란만 줌
          + 처음엔 이 글이 영화 '데어 윌 비 블러드' 패러디 같은 건 줄 알았음(현재 제목이 'I use my terminal'이라 그렇게 생각함)
          + 나도 이 글이 누군가 직접 만든 터미널 에뮬레이터 사용기인 줄 알았음
          + 나도 그거 알게 되었음. 글 훑어보니 전체 제목은 ""I use my terminal (and so should you )""쯤일 듯. 터미널 관련 글은 언제나 반갑고, 크롬북(Chromebook) 사용자 입장에선 브라우저와 터미널만 있으면 충분했음. 맥으로 넘어가면 너무 방만해서, 평소엔 터미널 아니면 브라우저만 사용함
          + HN에서 제목 등록 시 보통 흔한 접두어나 접미어는 자동으로 잘라냄
"
"https://news.hada.io/topic?id=21599","OTel을 대체하고 Wide Events로 관찰 플랫폼을 확장하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  OTel을 대체하고 Wide Events로 관찰 플랫폼을 확장하기

     * LogHouse는 1년 만에 19PiB에서 100PB 이상의 로그 데이터를 처리하며 약 500조 행까지 확장됨
     * OpenTelemetry(OTel) 의 데이터 처리 한계와 비효율성 문제로 인해, 핵심 시스템에 맞는 맞춤형 파이프라인(SysEx) 으로 전환했음
     * 이 전환으로 이벤트 처리량이 20배 증가함에도 CPU 사용률이 10% 이하로 유지되는 효율을 실현했음
     * ClickHouse의 HyperDX 및 ClickStack 도입으로 UI와 데이터 통합, 스키마 유연성, 강력한 데이터 탐색 환경이 구축됨
     * Wide events 및 고카디널리티 모델 채택을 통해, 사전 집계 없는 모든 이벤트 저장과 분석이 가능해졌음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

배경 및 변화

     * ClickHouse Cloud를 위한 내부 로깅 플랫폼 LogHouse는, 1년 만에 데이터 규모가 19PiB에서 100PB 이상, 37조 행에서 거의 500조 행까지 증가한 대형 시스템으로 성장함
     * 초기에는 모든 텔레메트리를 OpenTelemetry(OTel) 을 통해 수집했으나, 대량 데이터 환경에서는 성능, 리소스 한계, 데이터 변환 과정에서의 CPU 낭비 및 데이터 손실 문제가 두드러졌음

OTel의 한계와 맞춤 파이프라인 도입 이유

     * OTel의 파이프라인은, 로그가 JSON으로 변환되고, 이후 OTel 형식으로 다시 매핑되며 여러 번 변환과 마샬링이 반복되어 효율이 극도로 낮았음
     * 실제로 OTel 기반으로 초당 2,000만 행을 처리하려면 약 8,000개의 CPU 코어가 필요할 정도였음
     * 트래픽 급증 시 Collector가 과부하로 로그 드롭 현상이 발생하며, 수집되지 못한 데이터가 발생함

SysEx 도입과 구조

     * SysEx(System Tables Exporter) 는 ClickHouse의 system tables를 대상으로 데이터를 본래 타입 그대로, 변환 없이 직접 LogHouse로 옮김
     * 해시 링 구조를 통한 분산 스크래핑, 시간 지연 버퍼 및 슬라이딩 윈도우 방식으로 데이터 누락 방지, 내부 SLA 충족
     * Go 언어 및 ClickHouse 클라이언트 커스텀 기능을 이용해 데이터 마샬링 없이 byte-to-byte 전송이 가능함
     * 스키마가 가변적인 점을 위해, 스키마 해시와 동적 스키마 관리를 적용하고, Merge table engine으로 여러 스키마 버전을 하나의 논리적 뷰로 통합함
     * 스냅샷 기반 메모리 테이블 수집, 고도화된 진단 및 분석 작업 지원

성능 및 효율성 개선

     * SysEx의 도입으로 OTel Collector가 초당 200만 로그를 800개 CPU로 처리하는 반면, SysEx는 70개 CPU로 3,700만 로그를 처리할 수 있게 됨
     * 이러한 효율성 향상으로 리소스 사용량 극감, 이벤트 손실 방지, 실시간 지원 환경 구현

OTel의 지속적 역할

     * OTel은 표준, 벤더 중립 플랫폼 제공, 서비스 장애나 비정상 상태에서는 여전히 필수
     * SysEx로 처리하지 못하는 크래시·비정상 상황에서도 로그 캡처 가능
     * 현재는 트레이스 레벨 이하의 로그만 제거하고, info 레벨 이상만 수집해 리소스 최적화 중

UI 및 HyperDX, ClickStack 통합

     * 기존 커스텀 Grafana UI에서 HyperDX 기반의 ClickHouse-native UI로 점차 전환
     * HyperDX는 스키마 독립적, Lucene 쿼리 지원, SQL 지원 등 ClickHouse의 광범위한 데이터 유형과 완벽한 호환성 제공
     * 다양한 테이블 구조 및 맞춤 Exporter 출처의 데이터도 UI 변경 없이 통합 가능함
     * Grafana는 Prometheus 기반 메트릭 및 고정 대시보드를 담당하는 등, 두 솔루션이 상호 보완적으로 사용됨

Wide Events 및 고카디널리티 모델 수용

     * Wide events는 각 행에 쿼리 ID, Pod 이름, 버전 정보 등 다양한 컨텍스트를 포함시켜, 집계 없이 모든 데이터를 저장하는 획기적 방식임
     * 이런 방식은 Prometheus 등과 달리, 사전 집계, 레이블 제한, 카디널리티 폭발 걱정 없이 심층 분석과 유연한 쿼리가 가능함
     * 데이터 분석 시점에 필요한 집계를 진행함으로써, 대량 데이터 환경에서도 성능과 비용을 모두 잡음

데이터 시각화 및 쿼리 유연성

     * ClickHouse는 Plotly, Jupyter notebook 등과의 연동이 뛰어나 여러 시각화 도구를 자유롭게 활용할 수 있음
     * Lucene 기반 HyperDX의 빠른 탐색 외에도, 복잡한관계·조건 쿼리(SQ L, JOIN 등)를 통한 고도의 원인 분석이 ClickHouse에서 바로 가능함

다양한 Wide Event 기반 데이터 소스 증대

     * kubenetmon: Kubernetes 네트워크 감시 오픈소스, L3/L4 트래픽, 연결, 비용 분석
     * Kubernetes Event Exporter: ClickHouse 싱크 추가 포크 활용, 대규모 클러스터 상태 변화 추적, 전체 오브젝트 스냅샷 실험 중
     * Control Plane Data, RUM(Real User Monitoring) , Istio Access Log 등 다양한 레이어의 데이터로 해석 범위 및 상관분석 능력 대폭 강화

운영상 고려점 및 향후 방향

     * SysEx가 쿼리 중 로그, 메트릭에 노출될 수 있으나 메모리 제한, 오류 시 영향 최소화 구조로 설계
     * Zero-impact scraping: 완전 디커플링 구조(예: S3 기반 plain rewritable disk 활용)로, 클러스터에 미치는 영향조차 근본적으로 제거하는 방식 연구 중
     * OTel은 서비스 초기·비정상 상태에서의 로그 확보 등으로 여전히 중요한 요소이나, 향후 zero-impact 방식이 안정화되면 의존도 더욱 낮아질 전망

ClickHouse JSON 타입의 진화와 활용

     * JSON 타입이 정식 GA 출시되어, 필드별 동적 컬럼 생성, 여러 타입 지원, 스키마 폭발에도 유연하게 대처 가능
     * 대량의 컬럼을 가진 JSON 질의 시 최적화가 완벽하지 않아 형태별 병행 저장, Map 타입의 실용성 재확인 등 접근방식 고도화 중
     * HyperDX와의 연계로 Map, JSON 필드 자동 추출·분석 가능, 향후 JSON의 폭넓은 활용 계획

결론 및 문화적 변화

     * LogHouse는 이제 성능 분석부터 실시간 디버깅까지 ClickHouse Cloud 운영의 핵심 관찰 플랫폼으로 자리 잡음
     * 비용 절감이 출발점이었지만, SysEx와 같은 맞춤 도구, OTel과의 조화, HyperDX 기반의 유연한 UI 확대 등으로 기술적·문화적 전환을 경험 중
     * 대규모, 고정확도의 Wide Event 기반 데이터 모델이 엔지니어링, 지원, 데이터 분석 모든 영역에 새로운 가치와 효율성을 제공함
     * 앞으로도 100PB, 500조 행의 스케일에서 얻은 경험을 바탕으로, 관찰성의 미래를 계속 선도할 계획임

        Hacker News 의견

     * 솔직히 이건 대단한 자랑거리가 아니라고 생각함. 100PB나 되는 로그를 저장하고 있다는 건, 아직 뭘 진짜로 남겨야 하는지 못 찾았다는 증거. 대부분의 핵심 정보는 메트릭과 구조화된 이벤트로 충분히 파악 가능. 나머지는? 아무도 안 읽는 디테일한 트레이스 로그, 정말 장애 터졌을 때나 봄. 더 잘할 수 있는 방법은? 한 번도 알람에 활용되지 않은 로그, 혹은 3개월 동안 검색에도 안 걸린 로그는 자동으로 삭제하는 식의 ‘관심도 기반 저장 정책’ 도입. 이런 식으로 가지 않는 한, 이건 그냥 아주 고급 디지털 쓰레기 더미에 압축만 좀 한 수준
          + 나는 반대로 모든 데이터를 모으고, 필요 없을 때 필터링하는 쪽을 선호함. 디버그 로그는 평소엔 필요 없지만, 필요할 때는 진짜 없어서는 안 되는 존재. 어떤 이벤트가 재현도 안 될 만큼 희귀한 경우엔, 그때서야 데이터를 다시 모을 수 없음. 이미 전부 저장돼 있으면 찾기만 하면 되니까 훨씬 낫다고 생각
          + 여러 회사에서 Datadog을 쓰면서, 갱신 비용이 터무니없게 나오면 메트릭과 제한된 이벤트만 남기고 로그는 줄이자는 압박을 많이 받았던 경험 있음. 문제는, 어떤 일이 터질지 미리 알았으면 진작에 고쳤을 거라는 점. 뭔가 이상하면 도대체 무슨 일이 있었나 찾을 때는 디테일 로그가 매우 필요함. 그런데, 반복적으로 일어나는 상황이 아니면 어떤 로그가 중요한지 알 방법이 없다는 게 현실
          + ‘관심도 기반 저장 정책’ 정말 멋진 아이디어임. 로그 패턴별로 쿼리/알람 접근 횟수만 카운트해도 TTL(저장 기간) 정책 세울 수 있음. 실제로 우리 팀도 이 방식 도입해서 저장 비용 70% 줄이면서, 중요한 데이터는 다 남김
          + 로그 전송 비용도 공짜가 아님. 특히, 크래쉬 원인 파악을 위해 최대한 빨리 디스크에 로그를 쓰는 언어일수록 더 비용이 커짐. 정보가 너무 많으면 진짜 중요한 상관관계 찾기도 힘들고, 해결된 버그의 로그는 가치가 금방 떨어짐. 난 통계 데이터를 선호함. 통계 데이터는 집계 방식이 효율적이고, 특히 GIL 기반 언어에선 OTEL 사용 시 오버헤드가 과도한 경우도 있음
          + 데이터가 이미 저장돼 있다면, 이후 필터링이나 정리는 할 수 있는 문제라고 봄. 데이터를 다 저장해두고 필요할 때 쓰는 게, 필요할 때 없어서 고생하는 것보단 낫다고 생각함. 물론, 이런 대용량 인프라를 운영할 수 있는 리소스가 있다는 전제하에 가능한 얘기
     * 이건 사실 Clickhouse 로그에만 해당되는 내용. 다른 종류 로그에는 별로 연관 없음. 물론 Clickhouse 자체는 아주 마음에 드는 솔루션이긴 하지만
          + 파티에서 엄청 재밌는 타입일 것 같음
     * 주의할 점 하나 언급할게. 서비스가 크래시 루프에 빠지거나 장애로 다운됐을 땐, SysEx로는 필요한 시스템 테이블에 접근 불가라 로그를 수집할 수 없음. 하지만 OpenTelemetry(OTel)는 수동적인 방식이라 서비스가 완전히 정상화되지 않았더라도 stdout, stderr로 나오는 로그를 챙길 수 있음. 이런 식으로 장애 상태에서도 로그를 모아서 근본 원인까지 분석 가능
          + 내가 해온 OTel 프로젝트들은 모두 능동적인 방식이었음. 그래서 저 내용은 딱히 새로운 정보라기보단, 잘못됐거나 불완전한 설명인 것 같음
     * '와이드 이벤트(wide event)'가 진짜 이렇게나 많은 저장 공간을 잡아먹어야 하는지 의문. 관찰(Observability)이란 건 본질적으로 샘플링 문제임. 최소한의 저장 공간으로, 특정 시점의 상태를 최대한 잘 복원할 수 있으면 됨. 샘플 개수를 줄이거나 압축 효율을 끌어올리면 됨. 그런데, 압축 기술이 아직 한계까지 간 것도 아니라고 봄. 중복 투성이 데이터엔 반드시 엄청난 저차원(low rank) 구조가 있을 거라 생각. 물론 이미 인버티드 인덱스나 각종 트리도 쓰고 있지만, 더 실험적인 저차원 텐서 분해 같은 연구적인 방법에도 희망이 있다고 느낌. 나도 업계 사람은 아니라 뭔가 빠뜨린 게 있을 수도 있음
     * ClickHouse 정도 스케일은 경험 못 해봤음. 이런 볼륨의 로그는 정말 검색 가능? ElasticSearch는 소규모엔 쿼리가 잘 되는 걸로 알고 있음. 역사적 로그용으로 json 파일 저장하는 방식 대신 ClickHouse를 써야 하는 이유는 뭘까?
          + 이 스케일에서 일하고 있음. 결론부터 말하자면, 가능은 함. 하지만 처리 비용이 상상을 초월할 수 있음. 인덱싱, 정렬, 클러스터링 전략이 엉망이면, ""이 문자열이 들어간 레코드"" 한 번 검색하는 데 $1~$10도 나옴. 내 경험상 대용량 데이터에선 무조건 ""최대한 적은 데이터만, 최대한 적게 만지기""와 ""최대한 덜 옮기기"" 원칙이 중요. 직렬화/역직렬화 또는 디스크, 네트워크 IO가 한 번만 들어가도 비용이 폭증. 이러면 OTel의 경우, 데이터 수집기(collector) 한 번 더 거치는 게 효율성에 정면 위배될 수 있음. 하지만 페타바이트 스케일에선 이런 작은 최적화를 통해 아낀 돈이, 복잡한 직렬화 코드를 짜는 엔지니어 연봉을 뽑고도 남음
          + 왜 ClickHouse를 json 파일보다 히스토리 로그 용도로 쓰냐고? 이유는 여러 가지. (1) ClickHouse 같은 로그용 DB는 컬럼 단위 압축으로 각 필드별로 따로 압축해서 일반 json 파일(압축된 것 포함)보다 훨씬 작은 저장 공간. (2) 로그 DB는 쿼리 속도가 훨씬 빠름. 1000배 이상 빠른 것도 가능. 이유는 필요 없는 데이터는 아예 안 읽으니까. 관련 링크. (3) 100PB의 json 파일을 grep으로 검색하는 건 현실적으로 불가능. 로그 DB는 저장 노드, 저장 공간만 늘리면 수평 확장으로 가능한 구조
          + 규모와 비용의 문제임. 우리 회사도 로그 볼륨 문제에 부딪침. json을 Splunk로 그냥 넣으면 연간 600만 불 넘게 듦. 그런데 승인된 예산은 5~10%밖에 안 됨. 본문에서 json 로그 처리에 8,000개 CPU가 필요하다고 했는데, 최적화 후엔 90개 CPU만으로 해결된다고 함
          + 2~3년 전엔 ClickHouse가 전문(full text) 검색 성능이 아쉬웠음. 빠르고 ElasticSearch급 대용량 처리도 되긴 하는데, 사용 용도에 따라선 미리 인덱싱 안 하면 ES가 묶음, 그룹핑, FTS엔 훨씬 빠른 편이라고 느낌
     * 관측팔 극단주의는 진짜 신흥 종교 느낌. 그리고 돈도 엄청 많음
          + 알려지지 않은 이상현상까지 파고들려면, 딱히 뾰족한 대안이 없는 상황
          + 재밌는 건, 이런 문제로 고생하라고 만들더니 결국 ""월정액만 내면 다 해결"" 전략으로 파는 아이러니 상황
     * 본문에서 로그 보존 기간(리텐션)이 얼마인지 얘기가 없었음. 특정 기간 지나면 요약/집계 데이터만 남기고 원본 데이터는 굳이 보유 필요성에 의문
     * Clickhouse 쓰다가 Postgres로 돌아오면 항상 문화 충격 느낌. 20GB 덤프 임포트하는 데 몇 분씩 걸린다는 게 납득이 안 됨. 이거 몇 초면 하지 않아야 하는 상황 아닌가
          + Clickhouse 사용할 때마다 너무 힘듦. 물론 Clickhouse가 필요한 쓰임새가 있고, Postgres가 다 대체할 수 있는 건 아니지만, 왠지 Clickhouse는 ‘위험요소’가 많고, 특별히 제한된 목적 아니면 거의 모든 면에서 Postgres가 더 낫다고 느낌
     * 넓은(wide) 이벤트를 쓰라는 압박을 받는 사람들은 로그용 데이터 비용 폭증 이야기를 안 함. 기존 방식(메트릭+트레이스+샘플 기반 로그)이랑 비교하면 확실히 돈 많이 듦. 분명 이득도 있지만, 비용 문제는 항상 빠짐
          + 제대로 설계된 wide event 구조는 오히려 전통적 로그보다 저장 비용을 줄일 수 있음. 한 번의 외부 요청이 하나의 wide event로 남으니까, 이후 디버깅이나 분석에도 필요한 정보가 다 담김. 관련 글 참고 A practitioner's guide to wide events
     * Clickhouse나 Dynamo와 같은 시스템의 핵심 트릭이 궁금함. 본질적으로 거대한 해시테이블 같은 구조인지?
          + Clickhouse 등 대용량 DB에 공통된 트릭 두 가지가 있음. 첫째, 디스크에 데이터를 똑똑하게 배치해서 대부분의 데이터를 무시하고 필요한 덩어리만 읽어내는 방식(컬럼 지향 저장 및 LSM 트리류 등). 그리고 저장 데이터를 모두 압축해서 디스크 IO까지 최소화. 둘째, 모든 자원(CPU, RAM, 디스크, 네트워크)을 최대한 활용하게 하는 극한의 튜닝. 예를 들어 Clickhouse는 CPU 코어 하나당 초당 10억 개 이상의 로우를 처리할 수 있고, 코어 수에 맞게 선형적으로 확장됨
"
"https://news.hada.io/topic?id=21676","미국 경제, 1분기 0.5% 하락…이전 추정치보다 악화","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     미국 경제, 1분기 0.5% 하락…이전 추정치보다 악화

     * 미국 경제가 2024년 1분기에 0.5% 감소해, 예상보다 심각한 하락 현상 발생
     * 수입 급증과 트럼프 행정부의 무역 정책이 불확실성 증가와 소비 위축으로 연결됨
     * 소비자 신뢰 지수와 소비 지출 모두 크게 약화되는 양상 나타남
     * GDP 기초 체력을 보여주는 항목도 직전 분기 대비 하락세 기록함
     * 향후 2분기(4~6월) 에는 경제 반등이 예상됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 1분기 경제 성장률, 예정보다 악화된 결과

     * 미국 경제가 2024년 1월부터 3월까지 연율 0.5% 하락 기록
     * 트럼프 대통령의 무역 전쟁으로 인해 기업 및 소비자들이 새로운 관세 전에 해외 상품을 급히 구매하면서 수입이 크게 증가함
     * 미국 상무부는 당초 1분기 성장률을 –0.2% 로 예측했으나, 실제 수치는 더욱 악화됨
     * 경제학자들은 이번 결과를 예상치 못한 감소로 평가함

GDP, 소비, 그리고 무역 상황

     * 국내총생산(GDP)이 전 분기 2.4% 증가에서 1분기에는 하락세로 전환됨
     * 수입이 37.9% 급증하여 2020년 이후 가장 빠른 증가율 기록
          + 이로 인해 GDP 수치가 약 4.7%포인트 감소 영향
     * 소비자 지출 역시 크게 둔화되어, 2023년 4분기 4% 증가에서 2024년 1분기 0.5% 증가로 크게 하락
     * 상무부가 산출한 본질적 경제 체력을 보여주는 GDP 하위 항목은 연율 1.9% 증가에 그침
          + 직전 4분기(2024년 4분기)에는 2.9% 증가 기록
     * 정부 지출은 4.6% 하락하여, 2022년 이후 최대 폭 기록

소비자의 신뢰와 경제 전망

     * Trump 정부의 새로운 관세 영향으로 소비자 신뢰가 약화되고 지출 위축 상황 지속
     * Conference Board 기준 소비자 신뢰 지수는 6월에 93으로, 전월(98.4) 대비 5.4포인트 하락
     * 단기 소득, 경제, 고용 관련 예상치 역시 4.6포인트 하락하여 침체 신호로 여겨지는 80선 이하로 내려감
     * 이전 Federal Reserve 이코노미스트 Claudia Sahm은 소비자 지출 하락이 경제에 부정적 신호임을 지적함

무역적자와 GDP 연동

     * 무역적자는 수입 증가로 인해 확대되며, 이는 수학적으로 GDP 감소 요인임
     * GDP는 국내 생산만 산출하므로, 수입분을 계산에서 빼야 실질 국내 생산 반영 가능
     * 1분기 수입 급증 현상은 단기적 현상으로, 2분기에는 재발하지 않을 가능성 큼

향후 전망

     * 경제학자들은 2분기(4~6월)에는 3%대 성장률로 반등할 것으로 예상함 (FactSet 사 설문조사 기준)
     * 4~6월 GDP 첫 발표는 7월 30일 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * (기사 내 오류 정정: 2022년 이후 정부 지출이 가장 크게 감소했음)

        Hacker News 의견

     * 나는 “수입이 37.9% 증가하여 GDP를 거의 4.7%p 끌어내렸다”는 부분이 혼란스러운 상황임. 아마도 이전 GDP 산출 때는 지금만큼 수입이 많지 않았고, 지출이 더 높았기 때문에 국내 생산품으로 잘못 분류했을 수 있다고 추측함. 하지만 모든 게 신고되는데 어떻게 수입을 과소평가할 수 있었는지 잘 이해가 안감. 물가 지수 변화 때문일 수도 있겠음. 다른 기사들은 다음 분기에 수입이 더 늘어나지 않을 것으로 봐 GDP가 높아질 거라지만, 지출이 수입 여부와 상관없이 동일하다면 그게 무슨 의미인지 이해가 되지 않는 상황임
          + Investopedia에서 참고한 내용 공유함. GDP 공식은 GDP = 소비(C) + 투자(I) + 정부지출(G) + (수출(X) – 수입(M))으로 표현함. 여기서 수입이 증가하면 공식대로 GDP가 감소하는 효과가 있음. 미국이 중국에 155% 고율 관세를 적용했을 땐 사실상 수입이 사라져 GDP가 굉장히 좋아 보임. 관세가 다시 55%로 인하되면서, 기업들이 쌓아놓았던 재고나 수입 물품을 대거 들여오면서 GDP가 반대로 움직임. 앞으로도 관세 인상 전에 미국 기업들이 미리 재고를 확보하려는 움직임 때문에 이런 현상이 반복될 것이라고 예상함
          + 소비가 수입 증가를 따라가지 못했기 때문에 GDP가 감소하게 된 것임. 지금은 기업들이 재고를 미리 비축하는 단계임. 다음 분기에는 이 재고가 팔리면서 진짜로 관세의 영향을 실감하게 될 것임
          + 내 생각에는 많은 기업들이 관세 인상을 미리 알고 1분기에 엄청난 양을 수입했기 때문이라고 봄
          + 인용된 문장은 회계 공식적 해석에 치우친 결과임. 사실 공식상 수입은 더해진 이후에 다시 빼는 구조로, 실제 국내 생산만 계산하기 위해서 이런 방식 사용함. 공식만 보는 사람들은 수입이 $X 증가하면 GDP가 그대로 $X 감소한다고 생각하지만, 실제로 공식 내에서는 순 효과가 0에 수렴함. 다만 이번 GDP 하락이 일부 수입 증가 때문이긴 한데, 기자들이 단순하게 계산하진 못함. 예를 들어 공장주로서 평소 중국에서 분기별 $50,000 투자하지만, 갑작스런 관세 때문에 미리 수입을 앞당기고 투자를 늦춤. 잠깐은 왜곡이 생기지만 결국 이 돈은 다시 공식상에 들어오게 됨. 게다가 수입은 장기적으로 국내 생산 효율성도 높여주고, 관세 때문에 미국산 덜 효율적 자재 쓸 경우 전반적 성장률 둔화 유발, 비용 전가는 인플레이션과 수요 감소로 이어짐. 수입은 달러
            유출로 환율 변동 및 수출 경쟁력에도 연쇄적 영향 있음. 경제현상을 공식 자체로만 해석하는 건 오해의 근본임. 더 정교한 모델링과 상호 연결성 반영이 필요한 분야임
          + Noahpinion 블로그 글 ""왜 경제 기자들은 이 기본 실수를 반복할까""를 참고 권유함 링크. 본인이 경제학자이고 블로그 저자의 주장이 100% 맞다고 생각함. GDP나 경제 통계 보도는 대체로 수준 이하 수준임
     * 최근 해고와 경제 악재가 많음에도 아직 기술적 경기침체에 빠지지 않았다는 게 매우 놀라움. 다우존스지수는 사상 최고치, 채권 수익률도 하락 중임. 내가 아마추어 경제학자처럼 보일 수도 있는데, 그 덕분에 이 사이트에서 이렇게 장황하게 얘기하고 있는지도 모름. 지난 5년간 미국 경제가 워낙 견고했기 때문에 우리가 진짜로 어려움을 체감하기 전까지 꽤 오래 걸릴 수도 있다고 추측함
          + 구조적 인구 통계가 원인임. 매일 약 13,000~14,000명씩 은퇴나 사망 등으로 노동시장에서 퇴장함. 이처럼 금리와 관세로 경제를 압박해도 노동 수요가 공급을 초과하는 현상 지속됨 관련 링크
     * ""나쁜 뉴스가 실은 좋은 뉴스""라는 글이 난무할 것이라는 생각임
     * Polymarket에서는 2025년 미국 경기침체 확률을 30% 정도로 봄 참고 링크
     * 문제가 되는 것은 관세만이 아님. 외국에서 미국으로 오는 사람이 많이 줄었고, 미국 제품 구매 자체도 피하려는 현상이 많아짐
          + 나도 외국인들이 미국을 기피하는 걸 이해함. 내 아내는 최근에 미국 시민권을 취득했음. 수년간 이민 절차에 돈과 시간을 들였으니 이제 한숨 돌릴 줄 알았음. 그런데 이제는 미국에서 합법 시민까지 추방하려는 시도가 있어 매우 불안함. 이런 정책은 관광비자로 미국에 오려는 사람들도 꺼리게 만들 것임. 관광객에게 밈을 보냈다는 이유로 체포한다는 이야기도 있는데 이런 뉴스가 있는 한, 나라도 미국 여행을 피할 것임. 미국을 사랑하지만, 이런 분위기가 무역 파트너와의 관계에 영구적인 손상을 줄까봐 걱정됨. 이민자 아내도 있어서 진지하게 이민을 고민하게 되는 상황임
          + 여행 기피는 원칙이 아니라 합리적 판단임. 미국 입국 시 특별한 잘못이 없어도 CBP(세관국경보호청) 요원 개인의 ‘분위기’만으로 거부·구금당하는 사례가 많아짐. 휴가 경비와 항공료를 모두 날릴 위험이 있으니 충분히 미국을 기피할 만함. 사업 측면에서도 무역 규제가 수시로 바뀌어 미국 업체가 계약을 갑자기 이행 못할 수도 있음. 미국 파트너 입장에서도 신뢰 훼손임
          + 내 생애 이렇게 반미감정이 캐나다에서 극심한 건 처음임. 소비 습관이 쉽게 바뀌지 않는데, 예시로 온타리오에선 미국산 주류가 완전히 사라졌고, 이 시장에 다시 들어오는 건 상당히 힘들 것임
          + 국경 문제는 미국 관광 산업에 매우 해로움. 미국 여행 시 조심하라는 권고가 심각하게 나오고 있음. 이전(2015년)에도 비슷한 일이 있었지만 지금은 그 규모가 훨씬 커짐. 미국 내에서는 이런 뉴스가 거의 보도되지 않음. 이런 사례를 공유하면 사람들은 농담인 줄 알거나 믿지 않음
          + 나와 많은 유럽—특히 스칸디나비아—사람들이 미국산 제품 불매 운동 중임. 대표적으로 코카콜라 대신 할인 브랜드 제품을 구입함, 앞으로도 다시 미국 제품으로 돌아갈 생각 없음
     * 미국 경제의 기반은 국내 소비임. 현 정부는 관세라는 형태로 소비세를 대폭 인상함. 교과서에서 볼 수 있듯 당연한 결과임. 관세 부과의 부수 효과로 공급망에도 마찰이 생겨, 팬데믹 초기처럼 인플레이션과 생산성 손실로 이어짐. 하지만 나는 미국 경제가 아주 회복탄력성이 크다고 봄. 25년을 살면서 웬만큼 얻어터져도 다시 살아나는 모습 봄. 다만 가장 힘든 사람들에게 사회가 좀 더 공감심을 갖길 바람
          + 최소한 관세가 미국 내 일자리 창출 및 리쇼어링을 자극할 수 있다는 기대 있음. 이 과정에서 장기적으로 국내 소비 증가도 기대해 볼 수 있음
     * 이번 GDP 수치는 관세 인상 전에 너무 많은 수입이 몰려온 탓에 생긴 통계 상의 착시라고 생각함. 아직 최종 결론 내리기는 이르다고 봄
     * GDP 하락이 위에 나열된 경제지표를 부정하는 것은 아님
     * 현 백악관 보도자료가 믿기 어렵지만, 대부분은 대체로 사실에 가까움. 실업률은 여전히 역사적으로 낮은 수준이고 1년째 안정적임. 미국 실질 평균 시간당 임금도 오름세, 최근 발표된 CPI 기준 1년 인플레이션은 2.4% 수준임
     * ""트럼프가 경제에 미친 영향은 코로나 이후 최악""이라는 드라마틱한 감상임
     * 경제지표 보도에서 수치만 정확히 전달한다고 해서 언론 탓으로 돌리는 건 부적절하다는 농담임
"
"https://news.hada.io/topic?id=21631","Show GN: 슬랙에서 업무를 학습하고 자동화하는 AI 팀메이트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: 슬랙에서 업무를 학습하고 자동화하는 AI 팀메이트

   Runbear는 누구나 Slack용 AI Agent를 만들 수 있게 돕는 서비스입니다—코딩 없이 5분 안에 에이전트 연동이 가능합니다.

   이번 메이저 업데이트를 통해 구조적으로 큰 변화가 있어 다시 한 번 소개드리게 되었습니다.
   단순한 응답형 챗봇에서 벗어나, 실제로 업무를 이해하고 제안하며 실행할 수 있는 형태로 진화했습니다.

    주요 업데이트

     * 대화 기반 자동화 제안: Slack/Teams의 채팅 내역을 분석해, AI가 먼저 자동화 가능한 업무를 제안합니다.
     * 프롬프트 없이 에이전트 생성: 명령어 없이도 자연어로 설명하면 에이전트를 구성할 수 있습니다.
     * 에이전트 피드백 루프: Slack 스레드에 남긴 피드백 댓글을 통해 에이전트가 스스로 개선됩니다.

    주요 사용사례

    1. 신규 직원 온보딩: 새로 합류한 팀원에게 필요한 문서, 툴 링크, 작업 흐름을 Slack에서 자동으로 안내하고 Q&A도 대응합니다.
    2. 문서 관리: Notion이나 Confluence 문서를 보고 AI가 내용을 요약하거나, 변경 이력을 요약해 공유합니다.
    3. 고객 지원: Zendesk나 Intercom 티켓을 정리해, 유사 사례를 찾아주거나 반복되는 질문을 문서화 할 수 있도록 도와줍니다.
    4. 팀 일정관리: 가능한 미팅 슬롯을 확인하고 팀 미팅을 손쉽게 예약할 수 있습니다.
    5. 데이터 분석: Slack에서 “지난 주 웹사이트 방문 트랜드를 요약해줘” 또는 “이 스프레드시트 기반으로 캠페인 성과 정리해줘”라고 말하면, AI가 데이터를 분석해 주요 지표와 인사이트를 요약해줍니다.

   긱뉴스 커뮤니티를 위해 할인 코드를 준비했습니다. GN202506 코드를 사용하시면 첫달 30% 할인이 적용됩니다.

   많은 관심과 피드백 부탁드립니다!
"
"https://news.hada.io/topic?id=21567","윈도우 10 지원 종료: 기존 PC에 리눅스로 업그레이드하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   윈도우 10 지원 종료: 기존 PC에 리눅스로 업그레이드하기

     * Windows 10 공식 지원이 2025년 10월에 종료됨
     * 마이크로소프트는 새 컴퓨터 구입을 권장하지만, 기존 PC에 최신 Linux 운영체제를 설치하여 성능과 보안을 향상할 수 있음
     * 리눅스는 무료로 제공되며, 소프트웨어 업데이트 비용도 들지 않음
     * 환경 보호 및 탄소배출 저감에도 기여하며, 기기 수명 연장 가능함
     * 지역 커뮤니티 및 온라인에서 전문적 지원을 받을 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Windows 10 지원 종료 안내

   Windows 10의 공식 지원은 2025년 10월 14일에 종료됨.
   마이크로소프트는 새로운 PC 구매를 지속적으로 유도함.
   그러나 기존 컴퓨터도 최신 Linux 운영체제 설치를 통해 뛰어난 속도와 보안을 다시 얻을 수 있음.

   2010년 이후 출시된 대부분의 컴퓨터는 교체 없이 계속 사용 가능함.
   최신 Linux 운영체제 설치만으로 여러 해 더 활용할 수 있음.

리눅스 설치 및 지원받기

   운영체제 설치 과정이 어렵게 느껴질 수 있지만,
   주변 지역의 전문가나 커뮤니티 구성원으로부터 도움을 받을 수 있음.
   스스로 설치할 수도 있고, 다양한 안내 자료와 모임을 이용할 수 있음.

기존 컴퓨터를 리눅스로 업그레이드해야 하는 5가지 이유

    1. 신규 하드웨어‧라이선스 비용 불필요
       새로운 노트북 구입에는 상당한 비용이 발생함. 하지만 다양한 Linux 운영체제는 무료로 제공되고, 업데이트 역시 무료임. 후원으로 프로젝트 지원도 가능함
    2. 개인정보 보호 강화
       Windows는 광고 및 스파이웨어가 포함되어 있음. 이는 컴퓨터 속도를 저하시킬 뿐만 아니라, 기업의 스파이 활동 및 에너지 비용 증가로 이어짐
    3. 환경 보호 효과
       컴퓨터 생산 과정에서 발생하는 탄소배출이 전체 수명의 75% 이상을 차지함. 동작하는 장비를 오래 사용하는 것이 배출량을 줄이는 효과적 방법임. Linux 운영체제 사용으로 장치의 수명을 효과적으로 연장할 수 있음
    4. 커뮤니티와 전문가 지원
       지역 수리 카페, 독립 전문가, 컴퓨터 매장 등에서 다양한 오프라인 지원을 받을 수 있음. 온라인 포럼 등에서도 폭넓은 도움을 구할 수 있음
    5. 높은 사용자 제어권
       Linux는 소프트웨어의 네 가지 자유를 제공함. 사용, 연구, 공유, 개선이 모두 자유로움. 사용자가 자신의 기기를 완전히 제어할 수 있음

캠페인 후원 단체

   여러 조직이 이 캠페인을 후원하고 있음

관심 있는 경우

   근처의 수리 카페나 사용자 모임을 방문해, 새롭게 태어난 오래된 컴퓨터를 활용해볼 수 있음.
   지속 활용을 위해 전문가 및 커뮤니티 지원을 적극 이용할 수 있음.

   스팀만 잘 된다면././..

        Hacker News 의견

     * 평소에 일반 Windows 사용자가 Linux로 넘어가려 할 때 가장 큰 불편함이 USB 플래시 드라이브 준비 과정과 Rufus 같은 외부 툴을 써야 하는 점임을 지적하고 싶음
       Ubuntu 같은 대표적인 비기술 사용자를 위한 배포판조차 설치 과정이 꽤 번거로움 안내 페이지(https://ubuntu.com/tutorials/install-ubuntu-desktop#1-overview)을 보면 대부분의 사람은 절대 이 과정을 거치지 않을 거라고 생각
       더 많은 사용자가 전환하게 하려면 Ubuntu가 Windows 앱을 직접 제공하는 방법을 제안해보고 싶음
       이 앱이 백그라운드에서 ISO를 다운로드하고 USB 포맷부터 자동화, 기본적으로 데스크탑 듀얼 부트 설정, 그리고 각 단계를 명확하게 설명해 Ubuntu와 Windows를 오가며 쓸 수 있도록 안내해 주면 좋겠다는 아이디어 공유
          + 사실 USB 등 별도의 매체를 굳이 써야 할 필요가 있나 의문
            그냥 기존 파티션 크기 조정해서 거기에 리눅스 복구 파티션 하나 만들고 거기서 설치하는 방식이나, Windows에서 바로 VM에서 설치 진행하고 리부팅 한 번에 완성된 Linux 시스템으로 진입하는 방식도 생각
            기술 수준을 떠나 외부 매체 직접 찾으러가거나 구입하는 것 자체가 꽤 큰 장벽
            리눅스 설치 USB 이외엔 15년 간 플래시 드라이브 쓸 일이 없었음
            컴퓨터 잘 다루는 나도 이걸 찾거나 사는 게 번거러운 숙제임
          + 절차를 단순화할 수 있다는 점엔 동의하지만 Windows 등 실행 중인 운영체제에서 완전한 부팅 엔트리 생성은 불가능할 거라 생각
            그래도 UX는 좀 더 최신화될 가능성 있음
            “os changer” 같은 접근성 좋은 앱 아이디어가 떠오름
            앱이 여러 리눅스 배포판 정보를 스크린샷, 간결한 설명, 태그, 평점 등으로 정리해주고, 선택하면 조용히 iso 파일 다운로드 시작
            USB 삽입 시 내용물 보여주고 포맷 확인, ISO 다 받아지면 바로 플래시 드라이브 만들기
            시스템 정보를 읽어 부팅 시 어떤 키를 눌러야 하는지 추천 후 재부팅까지 자동화
            어떤 방식이든, 중간 매개체(USB 등)는 여전히 필요
            사실상 라이브 배포판이 그 매개체가 되고, 그 자체가 나중에 사용하게 될 시스템이기도 하니 심리적 진입장벽 낮추는 효과가 있겠다는 생각
          + 리눅스에선 cat liveusb.iso > /dev/sdX 같은 명령으로 매우 쉽게 liveusb 제작 가능
            Windows도 파워셸 등에서 비슷한 명령이 가능할 것이라 예상
            물론 잘못된 드라이브에 쓰는 위험은 있어 별도 유틸리티는 필요
            USB제작 자체는 단순한데, ISO 내장된 win32 실행 파일 하나로 다운로드하고 실행만 하면 USB 쓰기까지 다 해주도록 만들면 되지 않나라는 의견 제시
          + Canonical에서 15~20년 전쯤 비슷한 올인원 Windows 앱을 만든 적 있었던 것으로 기억
          + 다음과 같은 고민거리도 함께 제안
          + 사용자가 직접 BIOS/EFI 부팅 순서 변경 방법 알아내지 않아도 되게 만들 수 없을지 Windows의 EFI 조작 API 등 활용 가능성 확인할 필요
          + 부트로더로 GRUB 대신 rEFInd나 Clover 같이 현대적이면서 보기 좋은 테마 제공하는 방식 고려
            GRUB은 기능적이지만 초보 사용자가 보면 너무 해커스러운 화면이라 겁 먹기 딱 좋고, Windows 업데이트 같은 변수엔 안정성도 약함
            신뢰성 있는 자동 복구 UI 제공이 매우 큰 개선 포인트라고 생각
     * 연구실에서 Windows 10 워크스테이션을 많이 운용 중인데, Microsoft가 OS 정책을 바꾸면 큰 문제
       수만 달러 상당 하드웨어들이 아직도 성능 충분하지만, Windows 전용 소프트웨어와 원격 데스크탑 등 꼭 필요한 것들이 많음
       NIH 예산 삭감도 무서운데 고비용에 엄청난 시간 들여 새 장비로 전환해야 한다면 최악임
          + 마이크로소프트 OS는 항상 EOL 시한 정해두는 정책이라 이런 상황이 처음이 아니며, 연장 지원 구입으로 해결 가능한 정보
            기관 고객을 위한 연장 지원: 공식 ESU 정보
          + 이전 Windows 7 때와 똑같은 상황
            3년간 월 정액 누적 업데이트 방식의 연장 지원 존재, 미국 정부 등엔 비교적 저렴한 가격 적용
            Windows 10 ESU 가격(소비자)은 첫해 $30, 둘째해 $60, 셋째해 $90 수준
          + 현재 Windows 11 업그레이드는 트릭을 써서 하드웨어 요구조건을 무시할 수 있음
            정식 지원받아야 하는 기업/기관엔 권할 만한 해결책은 아니지만, 당장은 미지원 PC도 Windows 11 업데이트를 받을 수 있음
            Windows 10 종료가 생각보다 너무 빠르다는 느낌
            반면 Microsoft Hyper-V Server 2019는 거의 GUI 없는 서버 OS임에도 2029년까지 연장 지원 받음
            오래된 장비 재활용하려 간이 패치/통합 설치본을 만들어선 써볼 계획
          + 시스템 세팅 당시에 운영체제 업그레이드는 미리 고려해야 하는 책임이 시스템 관리자의 몫이 아닌가 하는 의아함 토로
          + Windows 10 LTSC IoT 에디션은 2031년까지 지원이 유지됨을 안내
     * 이 사이트 참 좋은데, 일반 사용자가 가장 먼저 막히게 되는 장벽이 “Linux 배포판”이 뭔지 모른다는 점
       설사 알아도, 어떤 걸 설치해야 할지 추천이 없음
       대부분은 커널과 운영체제가 구분된 개념 자체도 낯설어함
       하드웨어/소프트웨어 나눔도 이미 어려운 개념
       누군가 중간에 꼭 태클 거는 게 아니라면, 아예 펭귄 로고 박은 “Linux” OS 하나로 브랜드 통일도 생각해봤으면 함
          + 공식 “Linux OS”를 내가 만들겠다고 하면 커뮤니티 전체의 반감을 살 게 뻔함
            결국 평균적 사용자가 굳이 Linux를 써야 할 이유는 크지 않음
            개발자들이 움직인다면 그건 진짜 임팩트가 클 거라 생각
          + 솔직히 Ubuntu는 초보자용 배포판으로 추천하고 싶지 않음
            Linux의 가장 큰 문제 중 하나가 누구나 쉽게 쓸 수 있는 완전히 초심자 친화적이고 헤매지 않는 배포판이 없다는 건데, Ubuntu는 그냥 “기업짐”임
            과거 서버 관리자로 일할 때 Ubuntu 사용자들이 문제의 온상이었음
            하지만 내 말을 듣고 Debian으로 넘어간 뒤엔 문제 발생 빈도가 엄청 낮아짐
            Ubuntu는 내게 “기업 수준의 이상한 짬뽕”에 가까운 Debian이라는 인식
     * 게임 안티치트만 잘 돌아가면 Windows에서 Linux로 완전히 넘어가고 싶음
       Windows는 지금 광고와 비정상적인 제품 배치로 점점 더 이상해짐
       Windows 검색 기능조차 제대로 못쓰겠다는 불만
          + 요즘 Steam Deck 등 영향으로 Linux에서 게임 DRM·안티치트 이슈는 심각하게 안느껴짐
            실시간 온라인 게임만 아니라면 대다수 게임들이 Linux에서 거의 무리없이 동작
            protondb 등에서 호환성 확인도 금방 가능
          + 커널 레벨 안티치트는 출발 자체가 문제
            리눅스까지 이 방식이 퍼지는 게 아니라 아예 사라졌으면 한다는 희망
            과거처럼 프라이빗 서버 기반이면 문제 없었는데, 오늘날 매칭 시스템이 만든 병폐라 보는 시각
          + Linux에 미리 rootkit이 심어진 전용 배포판이 진짜 시장성이 있다고 한다면, 메이저 게임사가 충분히 만들어줄 수 있음
            가장 잘 알려진 게임사 기반 배포판은 Valve 제품
            하지만 Valve는 이런 루트킷 방식과는 사뭇 다른 DRM 정책으로 유명
            오히려 Valve의 가치관과 상충됨
            이 기회로 Valve의 “게임 스토어” 독점 구조를 깨뜨릴 수 있을지도 모르겠다는 점 제시
          + 안티치트 자체가 문제라기보단, Linux에서도 해당 안티치트가 기술적으로 동작하는데, 운영사에서 Windows 전용으로 막기 때문
            예: Fortnite의 EAC(에픽 안티치트)는 Linux에서 지원되지만 운영사에서 차단함
          + 일부 안티치트(예: Easy Anticheat)는 Linux 지원
            그럼에도 불구하고 어떤 안티치트에서 문제를 겪는지 궁금함
     * ChromeOS Flex 설치 경험을 좋아해 적극 공유하고 싶은 맘
       공식 안내(https://support.google.com/chromeosflex/answer/11552529)만 조금 더 초보자 친화적이면 정말 좋겠다는 아쉬움
          + 이론만 보면 좋아 보이나 실제로는 공식 지원 모델에서도 제대로 동작하지 않을 때가 있음
            예: Dell E7270에서 사운드 드라이버가 작동하지 않았음
            또, Chrome 브라우저 사용도 필수임
            개인적으로 최신 브라우저 UX/UI가 사용자를 배려하지 않는다고 생각
            Chrome이 항상 스크롤바 표시 옵션 없애서 불만임
     * 이 링크가 최근 두 달 간 반복적으로 올라왔음을 알리고 싶음
       https://hn.algolia.com/?q=https%3A%2F%2Fendof10.org%2F
          + 내가 계산해 보니 4.75일마다 한 번씩 반복 등장
            이번 글은 처음으로 굵직한 논쟁도 붙었음
            자기 PR이 아니고, 서로 다른 사용자가 올릴 경우 적절한 재포스팅은 문제 없는 분위기라는 인상
     * 자생적 움직임으론 Linux 데스크탑 대중화 이루기 어렵다고 봄
       현실적으로 가능성 있는 전략은 Valve처럼 Linux 선탑재 기기를 보급하는 것
       Microsoft의 강제 번들 전술을 낮은 곳에서 깨기 어렵다는 냉정한 전망
     * “이 시점에서 컴퓨터에 있는 모든 데이터가 지워질 것이니 중요한 파일은 반드시 백업하라”는 경고, 더 크게 강조해야 한다고 생각
       특히 평범한 사용자가 문서 폴더만이라도 보존하는 방법이 있었으면 좋겠는데, 파일 시스템 때문에 그게 어렵나 싶은 의문
       물론 외장 드라이브나 메모리 스틱 백업은 기술적으로 가능하다는 건 알지만, 평범한 사용자 입장에선 좀 달라서 하는 고민
          + 여러 방법이 있지만, 예를 들어 NTFS 파티션 유지, 크기 줄여서 데이터 따로 복사 등 가능
            Linux 설치에 필요한 충분한 연속적인 공간만 있으면 실현 가능
          + Linux는 POSIX 권한을 지원할 수 있는 파일 시스템에 반드시 설치해야 함
            NTFS가 전체 드라이브를 차지하면 설치 공간이 없어서 설치 불가
            문서 폴더만 드라이브 내에서 복사하는 시나리오는 공간과 구조상 안정적이거나 안전하게 구현이 안됨
            RAM디스크 등 임시공간 땜질로는 무리수가 큼
            요약하면 기술적으로 내부 복사도 자동화 가능하지만 안정성을 보장할 수 없어 숨겨진 옵션도 두기 힘든 이유
            몇 기가바이트만 돼도 무리
            어차피 안전하려면 외부 백업이 필수니, 사용자가 그냥 수동 백업 후 새로 옮기는 것이 현실적인 유일 방법
          + 사실 기술적으론 기존 문서 폴더 등을 자동 복사하고, 이중부팅 구조로 동시 접근 제공, 나중에 익숙해지면 Windows 파티션 삭제까지 지원 가능
            설치 프로그램에서 이런 옵션만 추가해주면 새로운 경험 제공 가능성 있음
     * 여러 해에 걸쳐 다양한 리눅스 배포판을 시도할 때마다 매번 소소한 장애에 부딪힘
       나는 시스템 관리자 출신 소프트웨어 엔지니어인데, 오히려 평범한 사용자라면 얼마나 힘들지 상상도 힘듦
       최근 겪은 실제 예시:

     * Teams 화면 공유: 전체적으로 가우시안 블러가 걸려 있었음
     * Nvidia: 스크린티어링 해결 안됨, 여러 가이드 참고·드라이버 설치 반복해도 잘 안됐음
     * Office: LibreOffice가 내 Office 문서를 완전히 망침, 포맷문제·기능깨짐 등
     * 미디어: Windows에선 완벽히 되는 멀티미디어 시청도 Linux에선 잦은 문제
       이런 문제들은 20년 전쯤엔 오히려 불편함을 극복하는 도전이 재미였음
       지금은 시간·에너지 부족이라 “그냥 되는 OS”에 더 끌림
       필요할 땐 WSL로 Linux 충분히 대체 경험함
          + Arch Linux 설치 경험을 적극 추천하고 싶음
            처음엔 설치 과정이 어렵지만, 오히려 그게 리눅스 동작 원리와 문제 해결력의 실질적 성장 계기
            “도배하다가 문제 쏟아지는 집 리모델링 비용에 지친다”는 친구의 농담 공감
            Arch는 아무 기능도 없는 상태에서 시작해서 사용자가 하나씩 쌓아가니 각 문제의 원인을 정확히 파악하게 됨
            결과적으로 현재 Arch 시스템이 지난 8년간 가장 오래·안정적으로 썼던 환경이며, 두 번 빼곤 손대지 않고 잘 작동
            adb, 외장 USB 하드 인식 문제 등도 Arch로 부팅만 하면 즉시 해결됨
            다만 Windows 10도 의외로 대부분 “그냥 동작”
            Arch 공식 위키는 모든 리눅스 사용자가 참고할 만큼 풍성한 정보와 팁, 매우 실용적인 해결책 정리

     * 내 지역에서 중고 하드웨어 가격이 눈에 띄게 올랐음을 체감
       아쉽게도 Windows 10 전용 하드웨어들은 요즘 헐값에 매각되지 않고, 오히려 폐기되는 것 같음
          + 스마트폰 보급되면서 폐기물 재활용 전주기도 막힘
            내가 아는 대부분은 Windows 7(길게 잡아도 10) 세대 노트북 이후론 폰이나 태블릿으로 완전 전환
            이런 기기는 상태가 안 좋아서 중고시장에 내놓지도 않음, 예전엔 비싼 타워형이라 되팔았지만 지금은 얇고 약해서 그냥 “혹시 몰라서” 집에 방치
          + 사실상 대부분 구형 하드웨어가 새로운 용도로 재활용되고 있을지도 모름
            수요·공급 곡선 모두 격동기 접어들었고 과거엔 공급이 항상 살짝 빨라서 컴퓨터 성능만 느는 숙명이었음
            요즘은 수요 증가 속도가 더 빨라 시간이 지나도 하드웨어 가격 오름세
            하이엔드 서버나 GPU 부족 영향도 있고, 더 많은 이들이 기존 장비를 오래 더 소중히 쥐고 있음
"
"https://news.hada.io/topic?id=21572","최고의 예제와 함께 Makefile 배우기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        최고의 예제와 함께 Makefile 배우기

     * Makefile은 C/C++ 빌드 자동화 및 의존성 관리를 간소화하는 도구임
     * 타임스탬프를 활용한 변경 파일 검출 방식으로, 필요한 경우에만 컴파일 작업을 실행함
     * 규칙(rule), 명령어(command), 의존성(prerequisite) 등 핵심 구조를 예제와 함께 설명함
     * 자동 변수, 패턴 규칙, 변수 확장 같은 고급 기능도 실용적으로 다룸
     * 중간 규모 프로젝트용 실전 Makefile 템플릿을 통한 확장성과 관리의 중요성 소개함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Makefile 튜토리얼 가이드 소개

     * Makefile은 프로젝트 빌드 자동화와 의존성 관리를 담당하는 핵심 도구임
     * 다양한 숨은 규칙과 기호로 인해 처음 접할 때 복잡하게 느낄 수 있으나, 이 가이드는 주요 내용을 간결하고 직접 실행 가능한 예제로 정리함
     * 각 섹션별로 실습 기반 예시를 통한 이해가 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

시작하기

  Makefile의 존재 목적

     * Makefile은 대형 프로그램에서 변경된 부분만 재컴파일하는 데 활용됨
     * C/C++ 이외에도 여러 언어별 전용 빌드 도구가 존재하지만, Make는 일반적인 빌드 시나리오 전반에 활용됨
     * 변경된 파일을 감지해 필요한 작업만 실행하는 로직이 핵심임

  Make의 대안 빌드 시스템

     * C/C++ 계열: SCons, CMake, Bazel, Ninja 등 여러 선택지가 있음
     * Java 계열: Ant, Maven, Gradle 등
     * Go, Rust, TypeScript 등도 자체 빌드 도구 제공
     * 파이썬, Ruby, JavaScript 등 인터프리터 언어는 컴파일이 필요 없어 Makefile과 같은 별도 관리 필요성 낮음

  Make의 버전과 종류

     * 다양한 Make 구현체가 있으나, 본 가이드는 GNU Make(주로 Linux, MacOS에서 사용)에 최적화되어 있음
     * 예제는 GNU Make 3, 4 버전에 모두 호환

  예제 실행 방법

     * 터미널에서 make 설치 후, 각 예시를 Makefile 파일로 저장 및 make 명령어 실행
     * Makefile 내의 명령어 줄은 반드시 탭 문자로 들여쓰기 필요
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Makefile 기본 구문

  규칙(Rule)의 구조

     * 타겟: 의존성(들)
          + 명령어
          + 명령어
     * 타겟: 빌드 결과 파일명(보통 하나)
     * 명령어: 실제 동작하는 쉘 스크립트(탭으로 시작)
     * 의존성: 타겟이 빌드되기 전에 반드시 준비되어야 할 파일 목록
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Make의 본질

  Hello World 예제

hello:
        echo ""Hello, World""
        echo ""This line will print if the file hello does not exist.""

     * 타겟 hello는 의존성이 없고, 커맨드 2개를 실행함
     * make hello 실행 시, 파일 hello가 존재하지 않으면 명령어가 실행됨. 이미 파일이 있다면 실행하지 않음
     * 일반적으로 타겟=파일명이 일치하도록 작성됨

  C 파일 컴파일 기본 예제

    1. blah.c 파일 생성(int main() { return 0; } 내용)
    2. 다음 Makefile 작성

blah:
        cc blah.c -o blah

     * make 실행 시, blah 타겟이 없다면 컴파일이 실행되어 blah 파일 생성됨
     * blah.c 변경 후에도 자동 재컴파일 X → 의존성 추가 필요

    의존성 추가 방식

blah: blah.c
        cc blah.c -o blah

     * 이제 blah.c가 새로 변경됐다면, blah 타겟이 다시 빌드됨
     * 파일 타임스탬프를 변경 검출의 기준으로 사용함
     * 타임스탬프를 임의로 조작하면 의도와 다르게 작동할 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

예제 추가

  연결된 타겟 및 의존성 예제

blah: blah.o
        cc blah.o -o blah

blah.o: blah.c
        cc -c blah.c -o blah.o

blah.c:
        echo ""int main() { return 0; }"" > blah.c

     * 트리 구조로 의존성을 따라가며 각 단계별 생성 과정이 자동화됨

  반드시 실행되는 타겟 예제

some_file: other_file
        echo ""This will always run, and runs second""
        touch some_file

other_file:
        echo ""This will always run, and runs first""

     * other_file이 실제 파일로 생성되지 않으므로, some_file 명령이 매번 실행됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Make clean

     * clean 타겟은 빌드 산출물을 삭제하는 용도로 자주 사용됨
     * Make에서 특별한 예약어는 아니며, 직접 명령어로 정의 필요
     * 만약 파일명이 clean이면 혼동될 수 있으므로, .PHONY 사용을 권장

   예시:
some_file:
        touch some_file

clean:
        rm -f some_file
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

변수 처리

     * 변수는 항상 문자열.
     * 보통 :=을 권장하며, =, ?=, += 등의 다양한 대입 방식 존재
     * 사용 예시:

files := file1 file2
some_file: $(files)
        echo ""Look at this variable: "" $(files)
        touch some_file

file1:
        touch file1
file2:
        touch file2

clean:
        rm -f file1 file2 some_file

     * 변수 참조 방식: $(variable) 또는 ${variable}
     * Makefile 내 따옴표는 Make 자체에서는 의미 없음(단, 쉘 명령어에서는 필요)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

타겟 관리

  all 타겟

     * 여러 타겟을 한꺼번에 실행하려면, 첫 번째(디폴트) 타겟에 속성 부여

all: one two three

one:
        touch one
two:
        touch two
three:
        touch three

clean:
        rm -f one two three

  다중 타겟 및 자동 변수

     * 다수 타겟에 대해 각자 개별 명령 실행 가능. $@는 현재 타겟명을 가짐

all: f1.o f2.o

f1.o f2.o:
        echo $@
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

자동 변수와 와일드카드

  * 와일드카드

     * *는 파일 시스템상 이름을 직접 탐색
     * 반드시 wildcard 함수로 감싸서 사용 권장

print: $(wildcard *.c)
        ls -la  $?

     * 변수 정의에서 직접 * 사용 금지

thing_wrong := *.o
thing_right := $(wildcard *.o)

  % 와일드카드

     * 주로 패턴 규칙에서 사용, 지정 패턴을 추출하여 확장 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Fancy Rules

  암시적(Implicit) 규칙

     * Make는 C/C++ 빌드와 관련된 여러 숨은 기본 규칙을 내장함
     * 대표 변수: CC, CXX, CFLAGS, CPPFLAGS, LDFLAGS 등
     * C 예제:

CC = gcc
CFLAGS = -g

blah: blah.o

blah.c:
        echo ""int main() { return 0; }"" > blah.c

clean:
        rm -f blah*

  Static Pattern Rules

     * 동일한 패턴을 따르는 다수 규칙을 간결하게 작성 가능

objects = foo.o bar.o all.o
all: $(objects)
        $(CC) $^ -o all

$(objects): %.o: %.c
        $(CC) -c $^ -o $@

all.c:
        echo ""int main() { return 0; }"" > all.c

%.c:
        touch $@

clean:
        rm -f *.c *.o all

  Static Pattern Rules + filter 함수

     * filter를 활용하면 특정 확장자 패턴에 맞는 대상만 선택 가능

obj_files = foo.result bar.o lose.o
src_files = foo.raw bar.c lose.c

all: $(obj_files)
.PHONY: all

$(filter %.o,$(obj_files)): %.o: %.c
        echo ""target: $@ prereq: $

        Hacker News 의견

     * 1985년에 Boston University Graphics 랩에서 한 사람이 Makefile을 이용해 애니메이션용 3D 렌더러를 만들던 모습을 직접 본 경험이 있음. 그 사람은 Lisp 프로그래머로, 초기 프로시저 생성 및 3D 액터 시스템을 진행 중이었고, 10줄 정도로 구성된 정말 우아한 Makefile을 만듦. 단순한 파일 날짜 의존성으로 수백 개의 애니메이션을 자동 생성하는 구조였음. 각 프레임의 3D 형태를 Lisp로 만들고, Make가 프레임을 생성하는 방식이었음. 1985년 당시 3D와 애니메이션을 당연하게 여긴 요즘과 달리 모두가 놀라워하는 상황이었고, 그가 이후 Iron Giant와 Coraline의 3D 렌더러를 담당했던 Brian Gardner라는 점 기억
          + 혹시 이 사람 3d-consultant.com/bio.html에 나오는 사람인지 궁금증 표현
          + Coraline이라는 영화를 말한 거 맞는지 확인
     * Make를 쓸 때 잘 알려지지 않은 유용한 플래그 몇 가지 소개
          + --output-sync=recurse -j10: 의미는 각 타겟 작업이 끝날 때까지 stdout/stderr를 모아서 출력하는 플래그로, 그렇지 않으면 로그가 섞여서 분석이 어려움
          + 바쁜 시스템이나 다중 사용자 환경에서는 -j 대신 --load-average를 활용하여 병렬 처리 시 시스템 부하를 조절할 수 있음 (make -j10 --load-average=10)
          + 빌드 타겟 스케줄을 무작위로 섞는 --shuffle 옵션은 CI 환경에서 Makefile 내 의존성 문제를 잡아내는 데 유익
          + make의 다양한 옵션을 공식적으로 정리해 텍스트나 문서 형태로 프로그램에 포함시키면 사용 접근성이 높아진다는 아이디어 언급
          + 본인이 자주 쓰는 옵션은 전체 강제 빌드에 사용하는 -B 플래그 설명
          + ‘make -j’로 인해 도스 머신에서 발생한 문제를 자주 봤기 때문에 그 현상을 버그로 인식
          + 바쁜 시스템이나 다중 사용 환경에서 병렬화 문제는 OS 스케줄러가 처리해야 하는 일 아닌지 질문
          + 유용한 플래그지만 이 옵션들은 포터블하지 않기 때문에, 본인만을 위한 비공개 프로젝트 외에는 쓰지 말 것을 권장
     * .PHONY를 사용하지 않는다는 이유로 튜토리얼에서 건너뛰는 건 약한 변명이란 생각. 툴을 제대로 쓰는 방법을 가르치는 게 맞다는 의견
          + 팀에서는 Make를 태스크 러너로 사용하면서 모든 레시피에 .PHONY를 추가 및 유지한 것 때문에 논쟁을 겪음
          + Clark Grubb의 Makefile 스타일 가이드(clarkgrubb.com/makefile-style-guide) 추천
          + .PHONY 선언을 레시피별로 하는 것과 파일 상단에 한 번에 모으는 것 사이에서 다양한 스타일 경험 공유 및 linter로 강제했으면 좋겠다는 바람
          + 읽어본 결과 괜찮은 문서지만 몇 가지 동의하지 않는 점 있음
               o -o pipefail을 맹목적으로 적용하는 건 문제이고, 파이프에서 grep 등을 쓸 때 깨질 수 있으니 상황별로 적용할 것 추천
               o 비파일 타겟에 .PHONY를 마크하는 게 엄밀하긴 하지만 거의 불필요하고 Makefile만 장황해져서 필요시만 적용이 낫다는 관점
               o 여러 개의 아웃풋 파일을 만드는 레시피는 예전에는 더미 파일을 썼지만, 최근 GNU Make 4.3부터 그룹 타겟 공식 지원(여기서 확인)이 가능
     * Make는 대형 C 코드베이스의 빌드에 특화된 툴이라는 주장
          + 누군가 프로젝트별 잡 러너(업무 실행기)로 즐겨쓰는데, Make는 잡 러너로서는 적합하지 않고 조건문 같은 것도 어렵게 만드는 구조임
          + Terraform 같은 도구와 래핑하려다 실패한 경우도 본 경험
          + Make는 잡 러너라기보다 선형 쉘 스크립트를 선언형 의존성 형태로 변환하는 범용 쉘 도구라는 의견
          + C 코드베이스만의 빌드 툴로서 Make를 보는 관점은 더 이상 맞지 않다는 입장. 지난 20년간 더 견고하고 명확한 빌드 시스템이 개발된 현실을 언급. 업데이트 필요성 제기
          + 좋은 잡 러너에 대한 질문. (본인이 잡 러너 의미 혼동했다는 사과 추가)
     * Makefile이 복잡해지는 부분을 현대적으로 대체해주는 툴로 just 추천
          + just는 shell 스크립트 목록 대체에는 좋지만, ‘재실행이 필요한 룰만 실행’하는 Make의 본질적인 기능은 대체하지 못함
          + 그 밖에 대안으로
               o Task(Go) go-task/task
               o Cake(C#) cake-build/cake
               o Rake(Ruby) ruby/rake
               o 완전히 다른 개념의 Makedown: HN 토론에서 논의
          + 대안 도구들은 자신을 Make 대체라고 하지만 본인은 완전히 다르고 비교 자체가 어렵다고 생각. Make의 핵심은 산출물 생성과 이미 빌드한 것의 미재빌드에 있음. 반면 just는 단순 커맨드 실행기 역할
          + Make를 명령 실행기로 쓸 때의 장점은 거의 모든 곳에 설치되어 있는 표준 도구란 안정성. 대안들이 더 잘 만들어졌어도 별도 설치 부담이 있어 굳이 쓸 필요성을 못 느낌
          + Task는 내가 C로 하는 간단한 취미 프로젝트엔 잘 쓰고 있지만, 대형 프로젝트에도 적합한진 아직 판단 어려움(Task 공식 홈페이지)
     * 최근 CMake가 Makefile이 C++20 모듈 지원에는 적합하지 않다고 보고 ninja를 기본으로 택했다는 점 흥미로움(CMake 가이드)
          + 실제로는 타겟 의존성을 정적으로 정의하기가 불가능에 가까워서 clang-scan-deps 같은 도구로 동적으로 분석하는 방식 채택(기술 슬라이드)
          + 실제로 이 제약은 CMake 쪽 결정이거나 Makefile generator에 지원자가 없는 문제라 생각. ninja도 C++ 모듈 직접 지원하지 못하고(관련 이슈), ninja는 오히려 Make보다 기능이 적고 모든 의존성을 정적으로 명시해야 한다는 문제 지적
          + 모듈 도입 자체가 복잡하고 혼란스럽다는 의견
     * tup 사용 경험이 있는지 질문. (공식 문서)
          + tup은 파일 시스템 접근을 기반으로 자동으로 의존성을 파악해 어떤 컴파일러/툴에도 적용 가능한 빌드 시스템임
     * 본인이 Task라는 Make 대안 툴의 창시자이자 메인테이너임을 소개. 8년 넘게 개발 중이고 계속 발전함
          + 새로운 경험을 원한다면 한 번 써 보길 권유, 궁금한 건 언제든 질문 환영
          + Task 공식 홈페이지, GitHub 저장소 링크
          + just 역시 또 다른 Make 대안으로 추천(just GitHub)
          + 재미있는 우연으로, 본인은 Task를 자주 쓰고 오늘 아침에도 이슈 올림
     * 이 튜토리얼에는 위험하고 미묘한 문제가 있음
          + MAKEFLAGS에서 옵션 파싱 시, 긴 옵션이나 빈 짧은 옵션 다루려면 다음처럼 해야 함
            ifneq (,$(findstring t,$(firstword -$(MAKEFLAGS))))
          + OS X 기본 제공 구버전 make 호환이 필요하다면 상당수 기능이 미비하거나 미묘하게 다름
          + 이외의 문제는 대부분 오타나 최선의 스타일 위반이므로 생략
          + 참고로 load는 guile보다 포터블하며, 크로스 컴파일 환경에서는 컴파일러 지정을 정확히 해야 함
          + Paul’s Rules of Makefiles(여기)와 GNU make 매뉴얼(여기), 관련 매뉴얼을 꼭 읽어볼 것 추천
          + 간단한 데모용 Makefile 프로젝트도 운영 중(데모 github)
     * 각 GitHub 레포에 항상 Makefile을 포함하는 습관
          + 매번 명령어를 까먹기 쉬워서 Makefile로 저장하면 손쉽게 복잡한 Step도 추가해둘 수 있고, make만 돌리면 따로 기억하지 않아도 프로젝트별 기대하는 동작 바로 실행 가능
"
"https://news.hada.io/topic?id=21644","GPU에 대한 기본 팩트들","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             GPU에 대한 기본 팩트들

     * GPU는 연산 속도가 메모리 접근 속도보다 월등히 빨라서, 메모리 계층 구조가 성능의 병목을 일으킴
     * 연산 집약도(Arithmetic Intensity, AI) 에 따라 연산이 메모리 바운드, 계산 바운드 상태로 구분되며, A100 GPU의 임계점은 약 13 FLOPs/Byte임
     * 성능 최적화 주요 전략으로 연산 합치기(Fusion)와 타일링(Tiling)이 있음; Fusion은 불필요한 메모리 왕복을 줄이고, Tiling은 데이터 재사용을 극대화함
     * 동기화, Coalesced Load, 뱅크 충돌 해결 등 GPU 하드웨어의 구조적 특성을 이해하는 것이 고성능 커널 작성에 중요함
     * 점유율(Occupancy), 스레드 분기 최소화, 양자화(Quantization) 등 추가적인 고려사항이 실제 성능에 중요한 영향을 미침
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GPU의 컴퓨트 및 메모리 계층 구조

     * GPU는 일반적으로 메모리 대역폭보다 산술 연산 처리 속도가 훨씬 높음
     * 예를 들어, NVIDIA A100은 약 19.5 TFLOPS(32비트 부동소수점) 성능을 내지만, 메모리 대역폭은 약 1.5TB/s 수준임
     * 4바이트 데이터를 읽는 동안 수십 개의 계산을 처리할 수 있으므로, 데이터 이동이 성능의 병목임
     * 글로벌 메모리(VRAM) 는 모든 데이터가 존재하는 느린 오프칩 메모리이며, Streaming Multiprocessor(SM) 는 컴퓨팅을 담당함
     * 각 SM에는 고속 온칩 Shared Memory(SRAM) 가 있고, 여기서 프로그램이 직접 관리하는 캐시로 활용 가능함
     * 스레드는 가장 작은 실행 단위이며, 각 스레드는 개별 레지스터 집합을 갖고 있음
     * 32개의 스레드가 Warp를 이루며, Block은 동일 SM에서 실행될 스레드 그리드임

성능 구간: 메모리 바운드 vs 컴퓨트 바운드

     * 커널의 성능은 메모리 바운드(데이터 이동 속도에 의해 제한) 또는 컴퓨트 바운드(SM 연산 능력에 의해 제한) 중 하나임
     * 연산 집약도(AI) 는 Total FLOPs / Total Bytes Accessed로 정의되며, 이 값이 중요 지표가 됨
     * Roofline 모델: x축이 AI, y축이 FLOPS/s인 그래프에서 커널의 실현 성능을 나타냄
          + AI가 낮아 메모리 바운드이면 대각선(메모리 대역폭 계단)
          + AI가 높아 계산 바운드이면 수평선(최고 연산 성능 계단)
     * A100의 Ridge Point는 19.5 TFLOPS / 1.5 TB/s ≈ 13 FLOPs/Byte임
     * AI를 높이면 성능이 향상되며, 커널이 컴퓨트 바운드에 도달할 수 있음

연산 집약도 높이기 전략

     * 간단한 모델: 스레드 1개가 C[i,j] 1개 계산 → AI = 0.25 (매우 낮음, 메모리 바운드)
     * 스레드가 2x2 타일 계산 시에도 AI = 0.5 (여전히 낮음)
     * AI를 높이기 위해서는 여러 스레드가 블록 단위로 Shared Memory에 대형 타일을 적재해 데이터 재사용을 극대화해야 함
     * Block 내 스레드 협력을 통해 AI > 13로 증가시켜 컴퓨트 바운드에 진입 가능함

오버헤드 바운드 상태

     * CPU(호스트)가 GPU에 작업을 할당하는 과정에서 오버헤드 발생 가능
     * GPU 커널이 너무 작거나 많으면, GPU는 작업을 기다리며 대기하는 상황 발생
     * 현대 프레임워크는 비동기 실행을 도입해, 커맨드 스트림을 미리 큐잉하여 오버헤드를 최소화함

성능 증진을 위한 두 가지 핵심 전략: Fusion과 Tiling

  Operator Fusion (연산 합치기)

     * 단순 연산(chain), 예: y = relu(x + 1)에서는 각 연산이 별도 커널로 동작하면 데이터가 글로벌 메모리를 왕복함
     * Fusion은 여러 연산을 하나의 커널로 합쳐 중간 값을 글로벌 메모리에 저장하지 않고, 레지스터 내에서 연산 처리 후 최종 결과만 기록함
     * 예시: Triton, torch.compile Inductor 등 JIT 컴파일러가 자동화 처리

  Tiling (타일링)

     * 행렬곱 등 복잡한 연산에서, 단일 스레드 모델로는 AI가 낮음
     * 블록 단위로 타일을 나눈 후, 블록 내 모든 스레드가 협력해 데이터 타일을 Shared Memory에 적재, 대규모 데이터 재사용 구현
     * 연산은 ""Load(글로벌 -> Shared Memory) - Synchronize(동기화) - Compute(연산)"" 3단계 패턴을 따름

Coalesced Load와 벡터화

     * 글로벌 메모리에서 Shared Memory로 데이터를 옮길 땐, Coalesced Access(워프 32개 스레드가 연속된 128바이트 구간 접근)가 중요
     * 벡터화(예: float4) 로 한 번에 여러 데이터 로드 시, 하드웨어 리소스 절약 및 메모리 대역폭 활용 극대화
     * 데이터 정렬(alignment)이 필수이며, 행렬 내 바이트 수 K값이 4의 배수여야 효율적임

Shared Memory 뱅크와 뱅크 충돌

     * Shared Memory는 32개의 독립된 뱅크로 구성되어, 워프 32개 스레드가 각기 다른 뱅크에 접근해야 충돌 없이 동작
     * 행 단위 접근은 충돌 없음, 열 단위 접근은 충돌(같은 뱅크 접근) 발생
     * B 타일은 ""로드 및 전치"" 전략으로 Shared Memory에 전치로 저장해, 연산 시 행 접근 중심으로 뱅크 충돌을 피함

고속 온칩 연산 패턴

  기본 전략 1: 한 스레드가 한 output 계산

     * BLOCK_DIM=32 제한 하에 AI 최대 8로, 컴퓨트 바운드 진입 불가

  전략 2: 한 스레드가 여러 output 계산

     * BLOCK_DIM=16, TILE_DIM=64로 설정 시, 한 스레드가 4x4 출력 계산 → AI=16
     * AI>13이므로 A100 기준으로 compute-bound 성능 달성 가능
     * Shared Memory에서 float4 등 벡터화 로드로 효율적 연산 가능

타일화의 실제 한계: 타일 양자화

     * 행렬 크기가 타일 크기의 배수가 아니면, 경계 블록이 실제보다 큰 영역을 계산(불필요한 연산)하며 패딩 처리됨
     * 경계의 스레드는 guard 조건으로 불필요한 메모리 접근은 막지만, 연산 루프는 동일하게 돌려 쓰레기 계산(예: C += A * 0) 발생

추가 성능 튜닝 요소

  점유율(Occupancy)와 대기(Latency) 숨기기

     * 워프가 메모리 읽기 등 장시간 대기 시, SM은 다른 워프로 즉각 전환해 유휴 시간을 줄임(대기 숨기기, latency hiding)
     * 여러 Thread Block을 동시에 할당하면, 높은 점유율로 대기 시간 최소화
     * Block이나 타일 크기가 너무 커지면 resident block 수가 줄고, 점유율 저하로 성능 저하 발생

  스레드 분기 최소화

     * 워프 내 if-else 조건 분기가 발생하면, 두 경로를 순차적으로 실행해 효과적인 성능 절반 수준으로 감소함
     * min, max 등 branchless 코드로 분기 최소화 필요

  양자화(Quantization)

     * FP32 → FP16/BFP16 등 정밀도 축소 시, 메모리 이동량 및 처리 가능 데이터 수가 각각 2배로 증가함
     * A100 기준 FP16 연산은 312 TFLOPS(상대적으로 FP32의 19.5 TFLOPS 대비 최대 16배 성능) 도달 가능
     * 양자화로 Roofline상의 AI 오른쪽(메모리 효율)과 위쪽(최고 연산 성능) 동시 달성 가능

전체 요약

     * GPU 성능의 본질적 한계는 메모리 대역폭과 온칩 연산 능력의 불균형에 기인함
     * 성능 향상은 데이터 재사용 극대화(타일링) 및 중간 메모리 트래픽 최소화(Fusion) 로 달성
     * 하드웨어 구조(워프, 뱅크, 코얼레스드 액세스, 동기화) 특성을 이해해야 고성능 커널 작성 및 최적화가 가능함
     * 실전에서는 점유율, 분기 최소화, 양자화 등 추가적인 요소가 실질적 속도에 직접적인 영향 미침
     * 고성능 GPU 연산 설계는 이론적 AI 향상, 하드웨어 특성 활용, 실제 데이터 배치 및 사이즈 대응 등 복합적인 고려가 필요함

        Hacker News 의견

     * 전체 프로그램 최적화가 컴파일러 레벨에서 얼마나 잘 되고 있는지 궁금증, 각각의 LLM 아키텍처를 하나씩 최적화하는 현재 방식이 뭔가 뒤처지는 느낌이라는 생각
     * 같은 4070에서 llama.cpp와 vllm을 돌려서 더 많은 프롬프트를 배치로 처리하려고 시도 경험 공유, batch 8부터 llama.cpp는 심각하게 느려지고 GPU 사용률은 괜찮아 보여도 실제론 병목이 생긴 상황 설명, vllm은 훨씬 더 잘 처리함을 체감함
          + vllm이 paged kv 캐시와 GPU가 선호하는 fully coalesced 레이아웃을 사용해서 배치에 최적화된 성능 제공, 반면 llama.cpp는 단일 프롬프트에 좋은 flat 레이아웃이라서 batch 상황에서는 L2 메모리 접근 패턴이 깨져 속도 저하
          + llama.cpp에서 kv tensor를 [seq, head, dim]에서 [head, seq, dim] 형태로 interleave하면 vllm에서 fused attention kernel로 데이터 공급하는 방식을 따라가 2배 정도 연산 성능 바로 향상 경험 공유
          + 병목의 원인은 GPU 자체가 아니라 shared memory 접근 방식과 global read를 어떻게 설계하느냐에 있음, vllm이 바로 그 점을 레이아웃 변경으로 공략
          + 이런 병목 분석에 2일 넘게 걸렸고, GPU 활용 그래프만 봐선 알 수 없었으며 대부분 시행착오로 알게 된 점
          + 이런 실험을 좀 더 수월하게, 핫 리로드 방식으로 반복할 수 있는 방법이 있는지 궁금증 제시
          + GPU가 병목이 아니라고 했지만, 실제로는 메모리 레이아웃의 비효율이 결국 GPU의 연산 효율을 낮추는 병목이었다는 지적
          + deepseek 직원이 어제 공개한 nano-vllm 프로젝트 언급, 1200줄밖에 안 되는데 vanilla vllm보다 더 빠른 성능 기록했다는 소식 공유 https://github.com/GeeeekExplorer/nano-vllm
          + llama.cpp에서 변경된 레이아웃을 pull request로 올렸는지 질문, 2배 향상은 모두에게 큰 이득이 될 수 있을 거라는 의견
          + ik_llama.cpp라는 프로젝트도 시도해보라고 추천 https://github.com/ikawrakow/ik_llama.cpp
     * 좋은 정보가 담긴 아티클로, 이 내용이 NVIDIA가 GPU 아키텍처를 개발할 때 선택하는 요소들에 대한 이야기라는 의견 전달, 타사와의 차이점을 오해하지 말라고 강조
          + 예를 들어 AMD Instinct MI300은 FP32에서 최대 160 TFLOPS와 6TB/s의 HBM3/3E 대역폭으로 ridge-point가 바뀌며 이는 A100의 13 FLOPs/byte의 두 배인 27 FLOPs/byte, 대용량 HBM메모리(128~256GB)는 tiling depth와 occupancy 간 현실적 트레이드 오프도 바꿔놓음, 단 이런 GPU는 비싸고, CUDA 미지원이라는 트레이드오프 존재
          + AMD가 컴퓨팅 소프트웨어에 더 신경 쓰기 전까지는 NVIDIA GPU만 존재감을 가질 수밖에 없다는 의견
     * 스포일러로, 실제로 중요한 건 GPU 동작 원리 자체보다 머신러닝 계산에 어떻게 활용하는지라는 점 강조
          + 사실상 내용은 CUDA의 일반적인 요약에 가깝고, relu 예제와 torch 언급 빼곤 머신러닝과 큰 상관 없는 구성이라는 지적
     * 대비 색상 사용을 반드시 해야 한다는 의견, 가독성 강조
          + font-weight: 300 사용 경험 공유, 대다수 Mac 디자이너가 폰트 스무딩 옵션에 맞춰 개발해서 일반적으로는 ""normal""로 보이게 설정하는데, Mac은 얇은 폰트도 반쯤 두껍게 보이게 처리, 그래서 디자이너들은 더 얇은 폰트로 ""보통"" 느낌을 내는 경향 언급, 관련 링크 공유 https://news.ycombinator.com/item?id=23553486
          + 저자가 다크 모드로 편집하고 포맷하는 중일 수 있다는 추측, edge://flags/#enable-force-dark를 씌우면 링크가 잘 보인다는 점 언급
          + 작성자가 링크와 코드 블록 내 주석은 읽는 데 특히 더 노력 필요했던 점 지적, 콘트라스트 증가 제안, 콘텐츠 품질 자체는 매우 훌륭했다는 평가
          + 웹사이트가 텍스트에 알파 투명도를 써서 심각하게 콘트라스트를 떨어뜨리는 큰 실수라는 비판
     * 이 글은 사실 Nvidia GPU의 기본적인 사실들에 좀 더 가까운 제목이 더 나을 것 같다는 제안, WARP 용어도 최신 Nvidia GPU의 특징이며, 2003년쯤의 Nvidia GPU는 비디오 게임 렌더링만을 위한 하드웨어라서, 오늘날의 범용 연산 GPU와는 완전히 다르다는 배경 설명, 결국 게시글 내용은 모든 GPU에 적용될 수 있는 일반적 설명은 아니라는 요약
     * 정말 좋은 입문용 자료라서 감사하다는 의견, AI PC를 직접 조립할 때 GPU에 대해 며칠씩 조사했는데, 이 글이 반드시 알아야 할 핵심과 고부가가치 응용분야(생성형 AI)까지 잘 정리돼 있어 큰 도움됐다는 후일담, 특히 A100 GPU의 메모리 계층 구조 다이어그램이 매우 유용했다는 평가
     * ASCII 다이어그램 사용에 대한 의아함
"
"https://news.hada.io/topic?id=21649","새로운 PNG 스펙 릴리즈 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             새로운 PNG 스펙 릴리즈

     * PNG 파일 포맷이 20년 만에 새롭게 개정되며 과거의 위상을 되찾음
     * 이번 사양에는 HDR 지원, APNG(애니메이션), Exif 데이터 공식 지원 등 최신 기술이 다수 반영됨
     * Adobe, Apple, Google 등 주요 IT 기업과 브로드캐스트 업체가 공동으로 참여하여 개발 진행
     * 최신 사양은 Chrome, Safari, Photoshop 등 여러 프로그램에서 이미 지원 중임
     * 앞으로 더 나은 압축 기술과 병렬 인코딩/디코딩 추가 등 추가 업데이트 계획이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: PNG의 부활 및 중요성

     * 최근 PNG 파일 포맷이 약 20년간의 정체를 딛고 새 사양으로 업데이트됨
     * 미국 의회도서관, 캐나다 도서관 및 아카이브, 호주 국립 아카이브 등 주요 기관에서 공식 권장 포맷으로 PNG를 채택하고 있음
     * 새로운 사양을 통해 PNG가 시장에서 경쟁력을 되찾고 혁신성을 보여줌

새로운 기능과 특징

  적절한 HDR 지원 및 미래 호환성

     * 새로운 PNG는 HDR(High Dynamic Range) 지원을 제공함
     * Rec. 2020과 Rec. 709 색 영역 비교 이미지에서, 더 넓은 영역(바깥 삼각형)이 HDR 이미지가 표현하는 컬러임을 보여줌
     * 이 HDR 정보는 4바이트(및 기존 PNG 청크 오버헤드)만을 추가로 필요로 함
     * Chris Lilley 등 초기 저자 및 주요 기술 전문가가 참여하여 신기술을 명확히 설명함

  APNG(애니메이션 PNG)의 공식 인식

     * Mozilla에서 처음 제안되고 Firefox가 지원한 애니메이션 PNG(APNG) 도 공식 사양에 반영됨
     * 이전에는 일부 소프트웨어만 지원했으나, 현재 다양한 프로그램에서 널리 채택되고 있음

  Exif 데이터 공식 지원

     * Exif를 통해 저작권, 카메라 정보, GPS 정보 등 메타데이터 저장 가능
     * 이미지 제작과 보관, 저작권 관리 등에서 높은 효용성이 보장됨

  전반적 개선 및 오류 수정

     * 기존 사양의 오류(Errata) 수정과 명확화도 함께 이루어짐

배경 및 개발 과정

     * 마지막 PNG 사양은 약 20년 전 공개됨(아이폰 출시 3년 반 전)
     * W3C Timed Text Working Group(자막 기술 표준화)이 PNG에 HDR 지원 필요성을 제기하면서 개발 재개
     * 제안이 이루어지자 Adobe, Apple, BBC, Google, MovieLabs, W3C 등 주요 기술 기업이 공동 참여
     * 강력한 컨소시엄이 결성되어 차세대 이미지 포맷으로 거듭남
     * 현재 두 개의 후속 업데이트 역시 이미 준비 중임

이미 널리 적용된 현황

     * Chrome, Safari, Firefox, iOS/macOS, Photoshop, DaVinci Resolve, Avid Media Composer 등 다양한 프로그램에서 최신 PNG 사양 지원
     * 브로드캐스트 업체 및 관련 하드웨어, 툴링에서도 지원 확대
     * 뉴 스크롤, 스포츠 점수 배너 등 방송 이미지는 새로운 HDR PNG 활용 사례임

앞으로의 계획

     * 다음 판에서는 HDR & SDR 호환성을 더욱 개선할 계획
     * 추가적으로 더 향상된 압축 방식, 병렬 인코딩/디코딩도 추진되고 있음
     * 네 번째 에디션은 비교적 짧은 업데이트가 될 예정이며, 이후 압축 기술 연구를 바탕으로 다섯 번째 에디션 개발 예정

   Apng 처음에 단체에서 이미지에 대한 표준 아니라고 거부하다가 이제서야 인정했군요

        Hacker News 의견

     * 저자임을 밝힘과 동시에 질문이 있으면 언제든 환영임을 알림
          + 이번 PNG는 완전히 새로운 포맷이 아니라 기존 포맷의 업데이트 버전임을 강조
          + 매우 높은 하위 호환성을 제공함을 밝힘
          + 오래된 프로그램들도 새로운 PNG 파일을 최대한 잘 읽을 수 있고, 예를 들어 빨간 사과 사진이란 점은 그대로 인식 가능함을 설명
          + PNG 내부 작동 방식에 혼동이 있을 수 있어 핵심을 정리함
               o PNG 파일은 다양한 데이터 청크(chunks)로 구성됨
               o 각 청크는 이름을 갖고 있어 프로그램이 모르거나 필요 없는 청크는 그냥 건너뛸 수 있음
               o 이미지 스트림은 단 하나만 존재함을 명시
          + 새로운 PNG 스펙의 기능을 활용한 예시 파일이 있는지, 특히 애니메이션이나 HDR 이미지를 직접 다운로드해서 프로그램 호환성을 테스트해 볼 수 있는 데모 페이지가 있으면 좋겠다는 바람을 이야기함
          + 나는 메타포맷과 범용 툴링을 지지함을 밝힘
               o Office Open XML이나 ePub처럼 단지 zip과 XML 라이브러리만 있으면 파싱이 되는 구조를 예로 듦
               o PNG도 거의 Interchange File Format(이하 IFF)과 유사한 구조를 가지고 있지만, 기존에는 미묘하게 규격이 달라 완전히 범용 IFF 파서로 PNG와 IFF 계열 파일을 동시에 파싱하는 게 불가능했음을 경험에서 설명
               o IFF는 쉽게 구현할 수 있고 데이터 압축에 강점이 있지만, 지금껏 구식 게임 데이터, AIFF·RIFF 오디오 등 제한된 용도로만 쓰여왔고, 범용 툴링이 만들어지지 않은 이유 가운데 하나로 PNG와 IFF의 호환성 아쉬움을 지적
               o PNGv3는 PNG 파일이 공식적으로 IFF 파일이 된다면, PNG의 대중성을 발판으로 범용 IFF 생태계(예: libiff 같은)가 확장되고 새로운 포맷 정의에도 쓸 수 있을 거란 바람을 밝힘
               o 실제 PNG/IFF 파서를 직접 만들어 봤고, 단순히 PNG를 수정해서 IFF 호환성을 갖추기는 어렵고, 하위 호환성(특히 하드웨어 파서)도 유지해야 함을 언급하면서
               o 나는 PNG가 사용하고 있는 변형된 IFF 구조를, 새로운 메타포맷(IFF 2.0) 표준으로 별도 문서화할 것을 제안
               o IFF 2.0은 PNGv2가 호환되는 프로필, AIFF/RIFF 등 IFFv1도 커버하는 프로필을 공식화해 범용 IFF 툴링을 추진하길 바람
               o 나아가 새로운 그린필드 프로필(최신 환경에 맞춘 신포맷)에선, 예를 들어 CRC 대신 현대적 해시로 무결성 체크, 청크 이름 문자세트 확장, 각 청크 속성정보를 효율적으로 담는 구조를 도입할 수도 있음
               o 이런 구조들은 IFFv1과 PNGv2를 모두 변환(transcode) 가능하도록 하고, 실제 HTML5에서 HTML4와 XHTML을 조합·호환하는 식의 전략적 프로필 선언이 바람직하다고 봄
               o 결론적으로, 세부 사항보다 디자인 목표(기존과 새 포맷 간 이질성 최소화 및 도구 생태계 확장)에 중점을 뒀음을 강조함
     * 내 웹 기반 드로잉 툴에서는 문서의 JSON 표현을 PNG의 주석 필드에 저장하는 트릭을 씀
          + 이렇게 하면 저장한 파일이 바로 이미지로도 쓸 수 있고, 다시 에디터에 불러올 수도 있음
          + 다운로드 폴더에 알아보기 힘든 JSON 파일이 쌓이지 않는 장점이 있음
          + 재미있긴 한데, 사용자가 왜 파일이 .png로 저장되는지, 또는 Paint 등에서 열었다 저장하면 왜 데이터가 사라지는지 설명하기가 애매함
          + Krita도 붓 설정을 이렇게 저장하는데, 데이터가 너무 많을 때 예상치 못한 문제를 일으킬 수 있음
               o 관련 이슈: Krita-Brushes 이슈 #4, Memileo Impasto Brushes 대화
          + Macromedia Fireworks는 20년 전부터 PNG를 기본 저장 포맷으로 사용했었음
               o 물론 그때는 JSON을 넣었던 건 아님
          + 많은 AI 이미지 생성 프론트엔드도 비슷하게 사용함
               o 이미지 주석(comment)에 프롬프트나 세팅을 저장해서, 이미지만 열어도 설정을 가져오거나 ComfyUI처럼 전체 워크플로우를 불러오는 식임
               o 사실 이미지 다루는 툴들은 이런 방식이 꽤 보편적으로 쓰인다고 생각함
          + Macromedia Fireworks는 PNG에 Fireworks 파일을 저장했고,
               o Adobe의 Illustrator(AI) 파일은 실제로는 PDF이고,
               o Photoshop의 PSD 파일도 TIFF 내부에 저장 가능함
               o 이 때문에 Photoshop에서는 여러 레이어가 보이지만, 다른 소프트웨어에서는 한 레이어만 나오는 현상이 있음
     * 이 스펙은 이미 널리 구현된 것에 대한 명시화에 가까움
          + 차세대 PNG라 해도 새로운 디코더가 필요하다면 PNG2라고 했어도 됐을 것 같음
          + JPEG-XL은 이미 대부분이 원하는 무손실 코덱의 조건을 충족함
               o 문제는 인코딩/디코딩 속도와 자원임
          + 현재 무손실 이미지 코덱의 최고는 HALIC임
               o 자세한 논의: HN HALIC 토론
          + HALIC 토론 스레드를 보면 실제로는 LEA 0.5가 더 우수하다고 함
               o 이미지 무손실 압축 벤치마크에서 HALIC은 최상위권이 아님
          + 솔직히 말해 JPEG XL을 한동안 ""초대형 이미지"" 용도로만 쓰는 줄 알고 무시했었음
          + 나는 컴퓨터 비전 이미지 어노테이션 툴(XLabel)에서 png를 사용함
               o 클래스 레이블을 이미지 메타데이터에 직접 저장해 텍스트 파일을 따로 두지 않아도 됨
               o 앞으로 이런 용도에 맞는 확장 포맷을 만들고 싶음
          + WebP의 무손실 압축은 업계 최고 수준이면서도 널리 쓰이지 않음
               o 무손실 압축에서 최고 성능을 내는 것 자체가 중요한 게 아니라, 오히려 생태계 채택이 더 큰 과제임을 시사함
          + 인코딩/디코딩 속도 문제는 시간이 지나면 개선될 수 있음
               o 이는 과거 jpg 생태계에서 실제로 일어난 변화임
     * 공식적으로 Exif 데이터 지원이 반영된 점이 최고의 소식임
          + 이전에도 헤더에 커스텀 데이터를 쓸 수 있었지만, Exif 지원은 매우 환영임
          + 참고로 Exif에 자이로스코프(회전)나 가속도(중력) 관련 필드가 있는지 궁금함
               o 구글 카메라 앱으로 찍은 사진에 이런 정보가 빠져있어서, 이후 자동 보정이나 파노라마 합성 때 아쉬운 경우가 많았음
          + 가속도 필드(Exif.Photo.Acceleration)와 고도를 위한 필드(Exif.Photo.CameraElevationAngle)는 있지만, 3축 전부를 지원하지 않음
               o 환경 센서(주변 조건)는 있지만, 스펙 작성자들이 지정한 특정 항목만 반영됨
               o Exif.Photo.MakerNote는 제작사에서 원하는 정보를 저장하는 자유로운 영역으로, 9축 데이터를 기록해도 충분할 정도로 크기 제한이 큼
          + Exif는 이미지 렌더링에서 회전 처리 방식에 따라 혼선을 불러올 수 있음
               o 구버전과 신버전 디코더가 Exif 회전 유무에 따라 서로 다르게 이미지를 보여줄 수 있음
               o Exif 회전에 대한 구체적인 디코더 권장사항이 없다 보니, 이중 회전(예: 데스크탑 환경과 라이브러리가 각자 처리) 같은 버그도 빈번히 발생함
               o ""디코더가 Exif 데이터의 신뢰 여부를 별개로 알지 못하면, Exif 데이터는 기록적 가치로만 간주해야 한다""는 애매한 권장문구만 있을 뿐
               o 완전한 하위 호환을 위해선 ""절대로 회전하지 마라"" 같은 명확한 지침이 필요하다고 생각함
          + 카메라의 가속도계나 관성항법장치 데이터를 기록하는 표준 필드는 없음
               o Exif 필드 목록: exiv2 공식 태그 설명
          + 실제로 많은 웹사이트가 Exif 데이터 대부분을 업로드 시 삭제함
               o 일부 필드는 개인정보나 위치추적에 악용될 소지가 있기 때문임
          + 개인적으로는 사람들이 Exif 대신 XMP를 쓰기를 바람
               o Exif는 구조가 이상하고 본질적으로 TIFF 이미지를 PNG 내에 박아넣는 꼴임
     * 이번 PNG 스펙은 이미 널리 통용되는 방식을 공식적으로 명문화함
          + 최고의 코덱은 어디서나, 어떤 앱이든, OS 쉘이나 API, Linux 등에서도 작동해야 함
          + HEIC나 AV1 같은 포맷은 OS 단의 지원이 없으면 파일을 미리 보기조차 힘듦
          + 제대로 통용되지 않는 포맷은 플랫폼 기본값이 되어선 안 됨
          + 나는 여러 이미지 포맷, 특히 특정 분야에서만 쓰는 희귀 포맷까지 다루는 업무를 함
               o 다양한 포맷을 정말 지원하는 건 매우 큰 도전임
               o 라이브러리도 표면적으로는 jpg/tif/heic 지원이라 써 있지만, 가령 30GB짜리 jpeg나 모든 메타데이터 지원까지 문제없이 되느냐 하면 그렇지 않음
          + 이 새로운 스펙은 오히려 HEIC이나 AV1보다 더 혼란스러울 수 있음
               o PNG 안에 어떤 코덱이 들어있는지 파일을 열어보기 전엔 전혀 알 수 없음
     * 지금까지 HDR이 명확하게 밝기·명암 범위(contrast ratio) 확장 의미가 아닌, ""더 넓은 색 공간""의 의미로 사용된 걸 처음 봄
     * 너무 늦었는지 의문임
          + 그리고 JPEG XL은 이미 모든 기능(손실/무손실 압축, 애니메이션, HDR, Exif 등)과 고급 압축 기술(유한 상태 기호 엔트로피, ZStandard 등)을 제공 중
          + 별도의 PNG 업데이트는 필요 없고 그냥 JPEG XL을 쓰면 된다고 생각함
          + ""그냥 채택하면 된다""는 게 안 되는 현실
               o JPEG XL 브라우저 지원 현황: 5년이 지났지만 지원하는 브라우저가 한 곳뿐임
               o 브라우저 공급업체가 구현하지 않으면 개발자나 유저가 새 포맷을 쓸 수 없는 것이 현실임
          + ""고급 압축기법(ZStandard 등)"" 언급에 대하여
               o ZStandard는 일정한 변화폭을 가진 숫자(예: 그라데이션)를 압축하는 데는 오히려 좋지 않을 수 있음
               o Bzip2는 이 방면에서 더 낫고, 예시처럼 두 변수(내부, 외부 반복)가 컬러 채널에 대응할 때 더 적합함
               o 현실 데이터는 노이즈 때문에 단순하지 않지만, 이런 알고리즘 비교는 여전히 실제와 차이가 있음
                    # 참고: 관련 Mastodon 토론
          + ""PNG 업데이트 필요 없다, JPEG XL만 채택하면 된다""
               o 그럴 거면 구글에 얘기해야 함
               o 실제로 구글이 Chrome에서 JPEG XL 지원을 포기했고(관련 이슈), 이로 인해 생태계 확장이 막혀버림
          + 왜 또 다른 표준(파생형)을 만드는지 이해 못하겠음
               o 이미 충분히 혼란스러운데, 독특한 판매 포인트도 없는 새로운 표준만 계속 늘어남을 한탄함
     * 이제 GIF를 APNG(알파 블렌딩 + 투명 배경 + 무손실 압축)로 대체할 수 있게 되어 2000년대 웹 감성이 다시 살아날 수 있음
          + Animated SVG도 표준이 있는지 궁금함
               o 예전에 자바스크립트 기반 SVG 애니메이션(챗봇 아바타 같이)을 본 적 있는데, 표준 프레임워크가 있는지 잘 모르겠음
          + Animated SVG는 존재함
               o 주요 SVG 애니메이션 요소: set, animate, animateTransform, animateMotion
               o 자세한 설명: SVG 애니메이션 관련 자료
          + 요즘 GIF 대신 음성 없는 동영상(예: mp4)이 더 잘 압축돼 많이 쓰이는 걸로 알고 있음
               o Animated PNG가 AV1 등 동영상 포맷에 비해 경쟁력이 있는지 궁금함
          + GIF 업로드를 지원하는 대부분의 서비스에서 APNG나 animated WEBP 지원은 거의 전무함
               o 백엔드 지원이 사실상 0에 가까워 매우 답답함
          + 짧은 영상을 애니메이션 그래픽으로 변환하면, WEBP가 APNG보다 원래 더 좋았음
               o GIF를 중간 포맷으로 쓰면 그나마 APNG가 경쟁력이 있음
               o 요즘은 AVIF가 최고의 선택임
          + 몇 년 전엔 Lottie(Bodymovin) 라이브러리 사용 경험이 있음
               o Adobe After Effects에서 애니메이션을 만들고, svg와 json으로 내보낸 후 lottie JS로 웹에서 쉽게 애니메이션 적용
               o 벡터 웹 애니메이션에서 제대로 된 도구나 DX(개발 경험)가 부족하다고 느꼈음
               o animated PNG 쪽 도구 및 DX 정보도 궁금
     * PR에서 ""많은 프로그램이 새로운 PNG 스펙을 이미 지원한다""는 주장 중
          + Photoshop이 APNG를 지원한다는 언급은 잘못됨
               o APNG 인식이 What's new의 두 번째 항목임에도 불구하고 실제론 Photoshop에서 지원하지 않아 실망스러웠음
          + Photoshop은 HDR 부분은 지원하지만, APNG 부분은 지원하지 않음
     * 누군가 사람들이 시간/날짜 불확실성을 소프트웨어적으로 일치시켜 관리해야 한다고 언급
          + ""사진은 2025년 스캔, 내용은 부활절 무렵, 1920~1940년 사이"" 같은 모호한 시간 정보 관리 필요성
          + EXIF에는 DateTimeDigitized 필드가 있음
               o 애매한 날짜 표기에 대해선 EDTF 스펙이 존재
                    # 참고: EXIF 필드 설명, EDTF 스펙 공식문서
          + 구글 포토와 애플 포토에서 직접 날짜를 지정할 수 있지만, 실제로 EXIF에는 저장하지 않음
               o 플랫폼 이동시, EXIF가 없는 이미지들은 날짜 정보도 같이 사라지는 문제가 발생함
"
"https://news.hada.io/topic?id=21614","DeskHog - 개발자용 데스크탑 장난감","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        DeskHog - 개발자용 데스크탑 장난감

     * 오픈소스 3D 프린팅 미니 디바이스로, 개발자를 위한 손바닥 크기의 마이크로 게임기이자 데이터 터미널
     * ESP32-S3와 1.14"" 240x135 컬러 TFT 디스플레이, 10시간 배터리, WiFi, LED 내장, 확장포트 등
     * 게임(Pong, IdleHog, Flappy Hog 등)부터 Pomodoro 타이머, 인사이트 대시보드, 아이스브레이커 등 업무 도구까지 지원
     * 커스텀 3D 프린트 케이스, GitHub에서 소스와 3D 프린트 파일 다운로드하여 직접 출력 및 조립 가능
     * 손에 쏙 들어오는 사이즈: 약 70x40x15mm, 휴대 및 데스크탑용 모두 적합

DeskHog의 주요 특징

     * 오픈소스 & 커스터마이즈
          + GitHub에서 하드웨어 및 소프트웨어 오픈소스 공개, AI 에디터나 C++ 등으로 직접 개발 및 게임 추가 가능
          + 확장 포트(I²C, STEMMA QT, FeatherWing) 로 하드웨어 기능 추가 가능
     * 내장 게임 및 툴
          + Pog(Pong의 변형), IdleHog(방치형 게임), One Button Dungeon(로그라이크), Flappy Hog 등 다양한 미니 게임 탑재
          + Pomodoro 타이머, 인사이트 대시보드, 아이스브레이커 등 생산성 도구도 내장
          + PostHog 데이터와 연동 가능, QR 코드로 스마트폰 연동
     * 하드웨어 스펙
          + ESP32-S3 듀얼코어 240MHz, 4MB 플래시, 2MB PSRAM, 512KB SRAM
          + 1.14"" IPS TFT 240x135, 2.4GHz WiFi, Bluetooth LE
          + USB-C 충전, LiPo 배터리, 저전력 슬립 모드
          + 3개 사용자 버튼, 리셋/DFU 버튼, 시리얼 디버그 핀
          + 커스텀 PETG 3D 프린트 케이스, 오픈소스 파일 공개
     * DIY 및 확장성
          + GitHub에서 DIY 키트 및 3D 프린트 파일 제공, 직접 부품 구매 및 제작 가능
          + DeskHog Pro(예정): 추가 버튼/다이얼, 시계 스트랩 등 액세서리 출시 예정

   둠이 아직 안되는군요...

   탐나네여

   그냥 알리에서 만원짜리 GeekMagic SmallTV(WiFi Weather) 사서…
   커펌하시는것도 재미있는…
   https://github.com/GeekMagicClock/smalltv

   헐..초반에 3만원에 샀었는데 ㅠㅠ

   이거 있긴한데 커펌 자료는 찾아도 없더군요. 레딧에서 해킹하려는 분이 있던데 진행이 더디더군요.

   저는 왜 이런게 좋은걸까요. ㅎ

   이런거 좋아했다가 .. 묵은 안드폰 연결해서 갖고 노는게 이것저것 하기 편하더군요

   아직은 직접 3D프린터로 뽑아야 하는군요
"
"https://news.hada.io/topic?id=21565","Hurl - 커맨드라인에서 HTTP 요청 및 테스트를 위한 텍스트 기반 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Hurl - 커맨드라인에서 HTTP 요청 및 테스트를 위한 텍스트 기반 도구

     * 간단한 텍스트 파일 형식으로 HTTP 요청을 실행하고, 여러 요청을 체이닝하거나 값 추출, 응답 본문 및 헤더에 대한 쿼리/검증이 가능한 오픈소스 커맨드라인 툴
     * REST, SOAP, GraphQL 등 다양한 API 및 HTML/XML/JSON 기반 요청을 지원하며, 데이터 조회와 HTTP 세션 테스트 모두에 적합함
     * 요청 체이닝, 값 캡처, 상태 코드·헤더·본문 등 다양한 검증이 가능하고, Junit, TAP, HTML, JSON 리포트 등 CI/CD 연동 및 자동화에 유리함
     * Rust로 구현된 단일 실행 파일로 배포되어 설치가 간편하고, 내부적으로는 libcurl 엔진을 사용해 빠른 속도와 강력한 프로토콜 호환성 제공
     * 경쟁 도구 대비 간결한 문법 및 확장성, 다양한 기능을 갖춘 현대적 HTTP 테스트 자동화 도구로 평가받음
     * 커뮤니티 주도 오픈소스로서, 직관적이고 확장 가능한 텍스트 기반 포맷 덕분에 개발자와 DevOps 양쪽 모두에게 높은 활용도를 보임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

What's Hurl?

     * Hurl은 명확하고 직관적인 텍스트 형식으로 HTTP 요청을 작성하여 커맨드라인에서 실행할 수 있는 도구임
     * 여러 요청을 체인처럼 연결, 응답에서 값을 추출하거나 다양한 쿼리(헤더·본문·상태코드 등)를 활용해 복잡한 HTTP 시나리오 테스트 가능함
     * libcurl 엔진 기반으로 신뢰성 높고 IPv6, HTTP/3 등 최신 프로토콜 지원
     * 데이터 조회, 시나리오 테스트, 성능 측정(응답시간 등) 모두 지원
     * 리포트(Junit, TAP, HTML 등) 생성 및 CI/CD 파이프라인 연동에 최적화
     * REST, SOAP, GraphQL, HTML, XML, JSON 등 다양한 API에 대응하고, 본문 파싱(예: XPath, JSONPath) 도 지원함
     * 다음은 다른 HTTP 테스트 도구(예: Postman, curl 등) 대비 Hurl만의 중요한 강점:
          + 플레인 텍스트로 작성 가능해 버전 관리 및 협업에 용이함
          + Rust로 작성된 단일 경량 바이너리 제공, 별도 런타임 불필요
          + curl과 동일한 네트워크 엔진(libcurl) 기반으로 신뢰성 및 다양한 프로토콜 지원
          + REST, SOAP, GraphQL, HTML 등 다양한 형식 지원 및 복잡한 시나리오 세팅 가능
          + CI/CD, 테스트 자동화, 상세 리포트(JUnit, HTML, TAP 등)와의 손쉬운 통합

주요 기능 요약

     * 기본 동작
          + Hurl 파일(.hurl) 안에 여러 HTTP 요청을 순차적으로 작성해 실행
          + 각 요청의 응답에서 값 추출, 조건 검증, 다음 요청으로의 데이터 전달 가능
          + REST/JSON, SOAP/XML, GraphQL, HTML 등 다양한 형식 대응
     * 체이닝 및 변수 활용
          + 여러 요청을 한 파일 내에 체인으로 작성
          + Captures 구문으로 응답에서 값 추출, 이후 요청에 변수로 주입
          + Xpath, JSONPath, 정규표현식, 바디 구조 등을 통한 데이터 추출 및 활용
     * 다양한 요청 및 검증 방식
          + 쿼리파라미터, 헤더, 인증 등 다양한 요청 사양 설정 지원
          + [Asserts] 구문 또는 암시적 구문으로 상태코드, 바디, 헤더, 쿠키, 응답시간, 해시 등 검증
          + XPath, JSONPath 사용, REST/GraphQL/SOAP 과 HTML 콘텐츠도 테스트 가능
          + 다중 값 검증, 응답 본문/해시/인증서 속성, 요청/응답 지연, 바이너리 데이터 처리 지원
     * 테스트 리포트와 자동화 연동
          + 실행 결과를 텍스트, HTML, JUnit, TAP, JSON 등 다양한 형식의 리포트로 출력
          + CI/CD 파이프라인에 손쉽게 통합 가능함
     * 고급 제어 및 유용 기능
          + 요청 간 데이터 전달(캡처·변수화)
          + 동적 데이터 생성 함수(newUuid, newDate 등)
          + 요청별 옵션 커스터마이즈
          + 폴링/재시도, 요청 딜레이, 스킵, 비밀 값 마스킹(redact)
          + curl과 동일한 옵션 지원(curl 옵션 그대로 사용 가능)
          + AWS Sigv4 인증 등 클라우드 전용 기능 내장

사용 예시

     * 단순한 GET/POST 요청 및 다단계 요청 체이닝을 간단한 텍스트 파일로 정의
          + sample.hurl 파일 작성하고 , $ hurl sample.hurl 명령 실행으로 해당 요청 일괄 실행
     * 예시:
  GET https://example.org
  HTTP 200
  [Captures]
  csrf_token: xpath ""string(//meta[@name='_csrf_token']/@content)""

  POST https://example.org/login?user=toto&password=1234
  X-CSRF-TOKEN: {{csrf_token}}
  HTTP 302

     * 여러 API 엔드포인트 테스트 및 응답 값 비교, 체이닝된 값 사용(토큰 등), 상태코드·헤더·본문 데이터 등 검증 가능

대표적 활용 예

     * REST/GraphQL/HTML/JSON/SOAP 등 다양한 API 테스트
     * CSRF 토큰, 인증·인가 등 값 추출 및 재사용
     * 상태 코드, 헤더, 본문 데이터, 쿠키, SSL 인증서 등 정밀 검증
     * 실제 서비스 시나리오(로그인~업무 동작 등) 자동화 및 모니터링
     * 멀티 플랫폼 및 다양한 설치 방법 지원(Linux, macOS, Windows, Docker, npm, Cargo 등)

CLI 주요 옵션

     * --test: 테스트 모드(요약 및 리포트 출력)
     * --report-html, --report-json, --report-junit, --report-tap: 다양한 리포트 형식 지원
     * --parallel, --jobs N: 병렬 실행
     * --retry, --retry-interval: 자동 재시도/대기
     * -u, --user: 인증 정보 입력
     * --variable, --variables-file: 변수 지정
     * -o, --output: 응답 파일 저장
     * --json: 실행 결과를 JSON 형식으로 출력

설치 방법

     * Homebrew, apt, yum, pacman, cargo, choco, scoop, Docker, npm 등 다양한 방법으로 설치 가능
     * 단일 바이너리라 별도 런타임 필요 없음
     * 예:
brew install hurl

       또는
cargo install --locked hurl

경쟁 도구 대비 장점

     * Postman, Insomnia 등 GUI 도구보다 텍스트 기반·버전관리·CI/CD 통합에 더 유리
     * curl보다 테스트 및 시나리오 실행, 검증, 리포트 자동화에 특화
     * YAML, JSON 등 복잡한 DSL 대신 직관적인 자체 포맷으로 러닝커브가 낮음

   Bruno - 빠르고 Git 친화적인 오픈소스 API 클라이언트(Postman 대체제)
   https://news.hada.io/topic?id=13730

   Hurl 4.0.0 릴리즈
   2년전에 4.0 이었는데 현재는 6.1.1 까지 나와있네요.

        Hacker News 의견

     * 나는 최근 몇 달 간 hurl을 사용하기 시작함
       테스트 스위트 모드와 개별 호출 모드를 모두 쓸 수 있다는 점이 아주 만족스러움
       CI에서 HTTP 요청 테스트 스위트를 실행하는 데 사용함
       설정 언어는 블록이 직관적이지 않고, 지원하는 assertion 관련 문서도 약간 부족하다고 느낌
       전반적으로 도구 자체가 큰 가치를 제공함
       POC 작업할 때 인터페이스 테스트를 시작했고, 이 방식이 LLM 기반 개발을 도울 수 있다는 사실을 알게 됨
       테스트를 HTTP 메소드에 직접 작성하니, 프로젝트 진화 과정에서 테스트와 구현이 더 유연하게 분리됨
       테스트 분리 덕분에 인터페이스와 구현 간 경계가 더 분명해짐
       hurl을 도입하기 전엔 서비스 언어의 테스트 프레임워크로 테스트 코드를 작성했지만, hurl 기반 테스트로 '클라이언트 관점'을 엄격하게 지키게 됨
       백도어 데이터 접근 같은 것 없이, 인터페이스, 테스트, 구현이 철저히 분리되어 안심할 수 있음
          + hurl의 개발자임
            피드백 고마움
            6~7년 전 첫 개발을 시작할 때는 JSON, 그다음에는 YAML 포맷에 도전했지만 점차 새로운 파일 포맷을 직접 만들기로 확신하게 됨
            이게 사용자 입장에서 어색할 수 있다는 점 충분히 이해함
            더 단순한 사례에 단순한 문법을 적용하려 노력했지만, 완벽하진 않을 수 있음
            문서 관련해서 부족하거나 개선점이 있다면 적극적으로 의견을 받고 향상시키길 희망함
     * Hurl 정말 멋진 도구임
       예전에 Python으로 작성된 웹 서비스를 Rust로 포팅했을 때 엄격한 public API 테스트가 큰 도움이 됨
       언어에 독립적인 통합 테스트 환경 덕분에 API나 웹사이트는 그대로 두고, 백엔드만 교체해도 됨
       Rust에서 Hurl을 쓸 때만의 특별한 이점 하나 더 있음
       cargo test와 연동해서 hurl 라이브러리를 바로 쓸 수 있고, .hurl 파일을 그대로 재사용 가능
       데모: https://github.com/perrygeo/axum-hurl-test
     * 나는 Hurl을 2023년 9월부터 사용하기 시작함
       Runscope를 통해 테스트 스위트를 돌렸었지만, 변경사항이 버전 관리 안 되는 게 너무 불편했음
       기초 작업을 거쳐 Hurl로 전환했고, Runscope는 버림
       이제 누가 언제 왜 무엇을 바꿨는지 한눈에 확인 가능해져서 만족 중임
          + Runscope에서 변경사항이 버전 관리 안 되는 점 정말 싫었음
            우리 팀도 그 문제를 해결하려다 프로젝트가 속도를 잃었음
     * 개념 자체는 좋다고 생각하지만 ‘왜 써야 하나’ 고민하게 됨
       나는 Django에서 개발하는데, 프레임워크 내장 테스트 기능이 이미 굉장히 충실함
       나만의 백엔드를 모르고 외부에서만 접근하는 도구 도입이 오히려 동기화 부담만 커질 것 같음
       이상하면 디버거로 바로 뛰어드는 것도 못 하게 됨
       테스트 코드와 백엔드 코드를 명확히 분리해야 한다는 논리도 있긴 하지만, 실질적으로는 관리 비용이 더 커짐
       결국 네이티브 테스트 스위트도 돌려야 할 거라서, 외부 도구를 여러 개 병행하는 게 어색하게 느껴짐
       단, API가 얼마나 범용적으로 동작하는지 검증하는 용도라면 쓸만할지도 모르겠음
          + 왜 내 백엔드를 모르는 도구를 써서 동기화 수고만 늘려야 하냐는 질문에 나도 고민이 많았음
            hurl은 써본 적 없지만, 언어와 무관하게 API를 테스트하는 도구들은 여러 번 썼고 직접 개발 중이기도 함
            이런 도구들이 좋아 보이는 이유는
               o 내부 구현을 몰라도 돼서 오히려 장점임
                 인풋-아웃풋만을 검증하는 구조 덕분에 내부 로직에 의존하지 않음
               o 언어 독립적이라서 다른 팀과 공유할 때 (혹은 OpenAPI 사양을 대신해서) 문서처럼 쓸 수 있음
               o API 계약 자체를 테스트하므로 대규모 마이그레이션 때도 재사용 가능
                 예를 들면, Perl에서 Go로 public API를 옮길 때 기존 Perl API를 비-회귀 테스트로 삼고, 같은 테스트를 Go API에서 그대로 써서 신뢰도가 높아짐
               o 개발자가 이런 테스트를 짜면 잠시 직접 API의 소비자 입장으로 관점을 바꿀 수 있어서, 더 품질 높은 테스트를 작성할 수 있었음
          + Postman 같은 제품의 대체제로 보면 됨
            그냥 간단히 몇 개 http 요청 테스트하려고 electron 기반 무거운 창을 띄울 필요 없음
            curl 스크립트와 Postman 사이 어딘가에 위치해서, 가벼움과 편의성이 동시에 필요한 사람에겐 최적임
          + 우리는 ktor 웹 서버에서 spring boot 기반 코드(Java/Kotlin 스택)로 마이그레이션 할 때 Hurl을 활용함
            서버 스택에 관계없이 명세 수준의 테스트 스위트를 운용할 수 있어서, 전환이 매우 수월했음
            또, 도커 이미지를 프로덕션에 쓰는데, 너무 구현에 종속적인 도구 대신 Hurl을 써 통합 테스트를 매우 가볍고 독립적으로 할 수 있었음
     * 샘플 섹션(https://github.com/Orange-OpenSource/hurl?tab=readme-ov-file#samples)이 도구의 장점을 5분만에 파악하고 싶은 사람, 즉 미리 판단하는 경향이 있는 사람에게 매우 설득력 있게 느껴짐
       나도 그 부류일 때가 종종 있는데, 정말 인상적임
     * Hurl의 관리자임
       질문이나 피드백 언제든 환영함
          + 나를 포함해 주변 개발자들이 많이 쓰는 패턴인데, VS Code나 IDEA의 IDE 확장으로 실행 가능한 "".http"" 파일로 테스트를 씀
            예시 포맷은
POST http://localhost:8080/api/foo
Content-Type: application/json
{ ""some"": ""body"" }

            그런 다음 output을 ""expected.json"" 파일과 1:1로 비교해서 통합 테스트 진행
            이 파일들을 cURL과 bespoke bash 스크립트로 실행하고, jq로 결과를 비교해서 성공/실패를 콘솔에 로그 남김
            Hurl로도 이런 식으로 IDE에서 예시 HTTP 요청과 JSON 파일 기반 기대 결과를 정의할 수 있는지 궁금함
            그리고 Hurl로 폴더 내 여러 파일을 자동 실행할 수 있나 궁금함
          + Hurl은 HTTP 레벨 테스트 스위트를 예쁘게, 유지보수 쉽게 작성할 수 있다는 점에서 저평가되어 있음
            이런 툴을 개발해줘서 고마움
          + Hurl이라는 이름 선정이 너무 만족스러움
            개발자 센스에 감탄함
          + Hurl을 한동안 쓰다가 직접 기여하기도 함
            ""include"" 기능을 제공할 가능성은 어떻게 되는지 궁금함
          + 지속적으로 관리해줘서 고맙다는 인사
            2년 뒤 Hurl의 비전과 미래는 어떻게 보고 있는지 궁금함
     * 이 프로젝트에서 많은 영감을 얻어 직접 HTTP 테스트 도구를 설계함
       우린 수백 개 테스트를 빠르게 병렬로 실행할 필요가 있었는데, Hurl이 마음에 든다면 Nap이라는 도구도 흥미로울 수 있음
          + 설정(config) 문법이나 내용이 Hurl과 같은지, 어떤 점이 다른지 궁금함
            차이점을 정리해서 보여주는 페이지가 있다면 알려주길 바람
     * 흥미롭게 보임
       나는 원래 Vscode-restclient를 오래 썼지만 스크립트 및 CLI 용도로 최근엔 httpyac으로 옮기고 있음
       Hurl도 내가 원하는 요건에 맞는지 살펴볼 계획임
       테스트 도구를 쓰면서 불편한 점 중 하나는 .http 파일에서 한 요청의 결과값을 다음 요청의 입력으로 참조하는 표준이 아직 없다는 것임
       지금까지 써본 세 도구가 모두 방법이 다름
          + hurl은 [Captures]
          + Vscode-restclient는 변수 선언에서 요청 이름을 참조
          + httpyac은 @ref 구문
            각각의 구문이 달라서, 한 도구용으로 작성하면 다른 곳에선 깨짐
            관련 참고 링크
            hurl capture 문서
            Vscode-restclient
            httpyac ref 문서
          + 또 다른 파일 포맷을 만들어서 미안함!
            이 문제를 조금이나마 줄일 방법으로 hurlfmt를 활용할 수 있음
            hurlfmt는 Hurl 파일을 JSON으로 내보낼 수 있게 해줌
            이 JSON 결과를 사용해 다른 도구와 상호 변환을 만들 수도 있음
            마법처럼 완벽한 해결책은 아니지만 Hurl에서 다른 포맷으로 옮길 때 조금 도움이 될 수 있음
          + Visual Studio Code와 Visual Studio가 모두 .HTTP 파일 지원하지만, 서로 호환 안 됨
            Conway's Law가 실전에서 재현된 사례라며 흥미롭게 보임
     * 약간 유사해 보임
       https://marketplace.visualstudio.com/items?itemName=humao.rest-client
       이 VS Code 확장은 HTTP 관련 테스팅에 매우 강력함
          + 에디터 독립적으로 쓸 수 있다는 점이 정말 큰 차이점임
          + IntelliJ도 유사한 기능 있음
            https://jetbrains.com/help/idea/…
          + 나도 Hurl을 써봤고 꽤 괜찮다고 느낌
            하지만 최근엔 .http 방식을 더 선호하게 됨
            IntelliJ엔 내장돼 있고, 위에 링크된 플러그인도 있고, CLI에선 httpYac도 써봄
            벤더 락인도 없고, 소스 컨트롤이나 복사&붙여넣기로 공유하기도 매우 쉬움
     * JVM에서 나는 Karate로 통합 테스트를 진행함
       https://github.com/karatelabs/karate
       JavaScript를 임의로 포함할 수 있기 때문에, 요청/검증을 유연하게 작성할 수 있음
"
"https://news.hada.io/topic?id=21601","기계식 시계: 분해된 모습","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             기계식 시계: 분해된 모습

     * 기계식 시계 무브먼트의 부품을 분해하고 입체적으로 전시하는 모델에 대한 제작 과정을 기록한 글임
     * 기존에는 실제 조립 순서와 공간 배치를 유지한 분해 전시물을 시중에서 찾아볼 수 없었음
     * 여러 차례 수지(에폭시) 레진을 이용한 시도가 있었으나, 층마다 굴절률과 기포 문제로 어려움을 겪었음
     * 낚싯줄(모노필라멘트) 을 활용해 부품을 떠받치고 한 번에 전체를 주입하는 방법이 최종적인 해법이었음
     * 수차례 프로토타입 시도를 거쳐, 정확한 위치 제어와 표면 코팅 등의 개선으로 완성도 높은 결과물을 얻은 경험임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

들어가며

     * 2022년 5월, Bartosz Ciechanowski의 블로그 글을 통해 기계식 시계 무브먼트의 동작 원리를 접했던 경험에서 시작됨
     * 해당 글에서는 인터랙티브한 일러스트레이션을 통해 시계 무브먼트의 각 부품 구조를 시각적으로 ""분해""하며 회전시켜 볼 수 있는 기능이 돋보였음
     * 이러한 온라인 시각화와 달리, 실제 분해된 시계 모델을 실물로 손에 들고 볼 수 있다면 어떨까 하는 오랜 호기심에 도전하게 됨

현실의 분해 모델을 찾을 수 없음

     * 시중에는 시계 부품을 수지 블록에 임의로 배치한 스팀펑크 아트가 종종 판매되나, 실제 무브먼트 구조와는 관련성이 적음
     * 레진에 부품을 펼쳐 담은 ""buffet style"" 전시물도 있으나, 원하는 입체적 조립 순서와 거리를 유지하지 못함
     * 무브먼트의 모든 부품을 정확한 위치관계로 ""폭발적으로"" 전개해 고정하기가 매우 번거로운 일임
     * 시계 조립 경험자(워치메이커) 가 아닌 이상 제대로 이를 구현하기 어려운 이유임

어떻게 만들 것인가?

     * Bartosz Ciechanowski 블로그에서 참고한 ETA 2824-2 무브먼트는 미세부품이 지나치게 많아 프로토타입에는 적합하지 않음
     * 포켓와치 무브먼트는 부품수가 적고 구조가 단순하여 초기에 실험하기 적합함
     * 다양한 빈티지 포켓워치 무브먼트는 중고로 저렴하게 구할 수 있고, 초보 워치메이커의 연습대상이기도 함

수지 레이어드 캐스팅 시도

     * 실물 모델은 손에 쥘 수 있을 만큼 견고해야 하므로, 투명 에폭시 레진에 부품을 층층이 임베딩하는 전략을 구상함
     * 레이어별 경화후 다음층 부으면 층간 경계가 명확히 남고, 노란 변색 및 경화시간 문제도 발생함
     * 층간 굴절률차로 인해 시각적으로 이질적이고, 반쯤 경화된 수지는 다루기 어렵고 기포도 많이 생성됨
     * 각 층을 따로 만들어 쌓는 방식은 시간과 노력이 지나치게 소모되어 중단함

낚싯줄을 이용한 부품 서스펜션

     * 투명한 플라스틱 봉 대신 모노필라멘트 낚싯줄(플라이피싱용)을 선택, 에폭시 레진과 굴절률이 유사해 존재감이 덜함
     * 낚싯줄은 감겨져 기억된 형태가 남으므로, 고온 오븐에서 반복적으로 잡아당겨 일자로 곧게 만드는 작업을 시행함
     * 트윅과 핀셋, 순간접착제(CA glue)로 부품과 낚싯줄을 정밀하게 부착하는 과정이 필요함

가정용 에폭시 레진 캐스팅

     * 여러 종류의 투명 에폭시 레진을 시도해보았으며, 기본 투명도는 양호하고 점도, 경화시간, 기포 발생 특성이 다름
     * 완전한 투명도를 위해 진공 챔버를 사용해 기포를 뽑아내는 방법을 적용함
     * 레진 혼합, 진공 처리, 몰드에 주입 후 재진공 과정을 순차적으로 거침

1차 프로토타입 실험 결과

     * 실린더 형태 몰드는 내부 구조 파악이 어려워 큐브형 몰드가 적합함을 알게 됨
     * 낚싯줄은 거의 보이지 않으며, 순간접착제와 레진의 간섭 문제는 특이사항 없음
     * 경화과정에서 레진이 수축하며 실린더가 깨지는 문제 발생

2차 프로토타입 및 조립법 개선

    분해 및 청소

     * 워치 무브먼트를 완전히 분해, 세척 후 세밀한 접착 조립 진행함
     * 핀셋, 순간접착제, 낚싯줄 절단 지그 등을 활용해 정확하게 부품 위치를 맞춤

    기관차측(트레인 휠 사이드) 조립

     * 무브먼트의 복잡한 기관차 부분부터 작업을 시작해, 주요 브릿지 등 부품들은 낚싯줄을 나사에 연결해 정렬함
     * 소형 부품들은 별도의 낚싯줄 단면에 접착 후 직립시켜 고정함

    문자판측 플립 후 조립

     * 완성된 하프 어셈블리를 잡아주는 보조지그를 제작해, 반대편 작업도 안정적으로 수행함

    몰드 및 레진 준비

     * 아크릴 판, 에폭시 방지 테이프, 구조용 접착제 등으로 큐브형 몰드 제작
     * 프로토타입 결과, 정확한 높이 제어 및 보다 폭발적인 배열, 손쉬운 부품 정렬이 다음 개선 포인트로 도출됨

3차 프로토타입: 세밀한 위치 제어

     * 가위식 리프트(랩잭), 자석 활용 핀셋 등으로 미세조정 정밀도 개선
     * 순간접착제 가속제를 점적 도구로 미세하게 적용해 접착 효율성을 높임
     * 밸런스 휠과 헤어스프링을 자연스럽게 펼쳐, 시계의 심장을 시각적으로 강조함
     * 전체 어셈블리 구조상 거꾸로 캐스팅하여 부품이 제대로 매달리게 구현

4차 프로토타입: ETA-2824/ PT5000 무브먼트

     * 최종 목표인 ETA 2824 무브먼트(PT5000 호환모델)로 실험 진행
     * 부품 소형화, 오일과 마감 불량문제, 도장 손상 등 난관을 맞았으나, 기본 조립 프로세스는 동일하게 적용 가능함
     * 충격 보호 스프링, 밸런스 어셈블리 등 민감한 부품의 서스펜션은 별도 노하우로 구현
     * 몰드 크기 한계 등으로 조립물 각도가 비정상적으로 변하고, 레진에 의한 도장 해체 등 실수도 있었으나, 약 18시간 소요 결과물을 완성

완성품 제작과 도장 보존

     * 에폭시 및 순간접착제가 도장을 녹인다는 사실로부터, 래커 스프레이 코팅이 최종 솔루션임을 확인
     * 낚싯줄 절단 지그, 조립용 보조구를 개발해 반복 생산성이 향상됨
     * 조립, 도장, 어셈블리 각 단계별로 상세 과정을 진행함

최종 결과 및 소감

     * 완벽한 표면 마감은 어렵지만, 2.5년에 걸친 시도 끝에 원하던 구조를 손에 넣는 경험
     * 블로그 글을 계기로 본격적으로 도전할 수 있었음을 강조하며, 다양한 시계 무브먼트의 입체적 분해 모델로 확장 가능성을 기대함

        Hacker News 의견

     * Bartosz Ciechanowski의 블로그 게시글이 인터넷에서 현재 찾을 수 있는 최고의 콘텐츠라고 생각함 링크
          + 이 분은 같은 수준의 디테일로 여러 프로젝트를 진행한 바 있음 아카이브 링크
          + 이 주제는 3년 전에도 Hacker News에서 논의가 되었으며, 4천 포인트 이상을 받아 역사상 8번째로 인기 많은 글로 등극한 명실상부한 클래식임 관련 토론
     * 정말 멋진 작업임. 한 아티스트가 카메라 같은 사물을 레진에 캡슐화한 후 워터젯 커터로 잘라 색다른 ‘폭발도(expoded view)’를 만든 것이 떠오름. 하지만 링크는 찾지 못함
          + Fabian Oefner가 실제로 해당 작업을 한 사례
     * 레진의 굴절률을 조절할 방법이 있다면, 낚시줄(서스펜션)이 완전히 사라질 수 있겠다는 생각을 해봤음
          + 진지하게 고민해보진 않았지만, 어떤 첨가물이 필요할지 궁금함. 검색해보니 광학 분야에서 관련된 선행 연구가 상당히 많음. 여전히 가능성 있는 주제라고 느낌
     * 도구나 경험이 없어서 완전히 매끈한 미러 피니시를 내긴 어렵다는 이야기에 공감함. 예전에 LGA CPU 소켓을 코스터로 만든 적이 있는데, 일반 사포로 점점 높은 그릿으로 샌딩해서 거의 완벽한 직육면체 형태를 만들 수 있었음. 유리 표면에 사포를 붙인 뒤 움직였고, 마지막에는 폴리싱 컴파운드를 썼는지는 기억 안 남
          + 샌딩 작업이 오래 걸려 지칠 것 같아 오비탈 샌더가 있었으면 함. 표면 연마 작업도 필요하고, 시간과 공간이 있다면 다양한 샌더와 연마 도구를 쓰고 싶음. 신뢰도 높은 에폭시 벤더의 실제 영상에서 과정을 볼 수 있음 영상 링크
          + 유리에 사포를 붙이거나 테이프로 고정하면 정말 평평한 표면 연마가 가능함. 이 방법으로 칼을 얼마나 날카롭게 만들 수 있는지 직접 경험함
     * PT5000 무브먼트를 사용한 점이 반가움. 내가 좋아하는 칼리버 중 하나임. 이 무브먼트는 ETA 2824-2의 중국제 클론으로, AliExpress에서 저렴하게 접할 수 있는 시계들에 많이 사용됨. 사파이어 크리스탈, 세라믹 베젤, 야광, 완전 방수 등 스펙을 갖춘 Submariner 오마주 시계를 100달러에 구매할 수 있는데, 이 무브먼트로 스위스 시계와 비슷한 정확도를 보여줌(COSC 기준 내 유지). 예전에 소유했던 시계가 +5초/일로 작동했으며, 중국 시계 산업의 발전이 실감남
          + AliExpress에서 기계식 시계 클론을 구입할 때 좋은 리소스나 팁이 있다면 추천받고 싶음. 가성비 파악 후 구매하고 싶음
          + 소비자용 전자제품의 가공 및 조립 기술에 대한 경험이 중국 시계 시장 성장에 큰 역할을 했다고 생각함
          + 정말 놀라운 일임. 레진으로 시계를 망치지 않을 때는 시계 오버홀 취미를 즐기고, 다음 목표로는 크로노그래프 메커니즘 익히기 예정임. ST19 무브먼트가 곧 도착 예정이며, 이것 역시 저렴하고 신뢰도 높으며 완전 기계식이며 칼럼 휠 구조임. 중국 시계 산업에 박수를 보냄
     * 일부는 이 작업이 일종의 ‘신성모독’ 느낌임. 이런 놀라운 기계의 매력은 내부 구조뿐 아니라 수십 년간 잘 작동한다는 데 있고, 고장났다가 수리로 되살아나는 과정이 진짜 마법임. Wristwatch Revival 유튜브 링크. 어릴 때 동생이 레진 작업으로 바다 밑처럼 보이게 소품을 만들던 기억임. 나는 성격이 급해 완전히 경화되기도 전에 만져서 항상 실패했음
          + 앤틱 숍에서 포켓워치를 동 가격으로 판매하는 모습을 쉽게 볼 수 있음. 전체가 살아 있지만 청소와 오일만 갈면 돌아오는 개체를 구하는 게 꽤 쉬움. 소유자에 비해 포켓워치가 10배는 많아 실습 재료로 유용함. 수리 단가는 높지만 기술자와 도구가 부족해 수요가 적음(본인도 두 점 소유)
          + 교육용 도구로 생각하면 더 도움이 될 듯함. 실제로 해당 프로젝트 영감이 된 디지털 버전도 교육 목적이 있었음
          + 수리 과정의 마법도 멋지지만, 메커니즘 작동을 이해하는 것도 그 못지않게 흥미로움. 두 가지 모두에 매력을 느끼는 게 자연스러움
     * 단단하게 굳기 전까지 젤처럼 있다가 위치를 손으로 조정할 수 있는 투명 물질이 없을지 궁금함. 젤 서스펜션으로 3D 프린트하듯 부품을 띄워두고 마지막에 완전히 굳히는 원리. SLA 3D 프린터용 레진 점도가 충분하면, 원하는 위치 맞춘 뒤 UV로 전부 경화시키는 방법을 떠올림 관련 영상
          + 젤 타입 레진도 있지만, 기포 문제 때문에 구현이 어렵고, 여러 번 분할해서 레이어로 주입하는 캐스팅 방식이 일반적임. 굴절률이 미세하게 달라 경계선이 남는 점은 회피하기 쉽지 않음
          + 만약 유동성이 있으면 항상 부력/밀도로 인해 부품이 떠오르거나 가라앉음. 같은 굴절률인 작은 비즈로 임시 고정한 뒤, 전체에 레진을 부어 위치를 유지하는 방식도 생각해볼 수 있음. 실제로 수조 장식 등에서 비슷하게 사용되는 방법임 관련 영상
     * Bartosz, 혹시 이 글을 본다면 직접 연락해주길 바람. 작업은 당신의 블로그 덕분에 할 수 있었고 완성품을 보내드리고 싶음. 부가 프로젝트임에도 이 정도 퀄리티와 마무리 디테일에 감탄함
     * 서스펜션(서포트)이 동일한 굴절률의 재료면 보이지 않아야 함
          + 나일론과 에폭시 레진은 비슷한 굴절률을 가지긴 하지만 완벽하진 않음. 나일론 대신 에폭시로 얇은 막대를 직접 만들어보려 했지만 원하는 결과는 얻지 못함
     * 완성품 외형이 인상적임. 박물관 전시용으로도 인기 많을 듯함. 또한 ciechanow.ski 사이트는 언제나 큰 영감임
"
"https://news.hada.io/topic?id=21646","Dockerized Flask / Django 앱에서 pip 대신 Uv로 전환하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Dockerized Flask / Django 앱에서 pip 대신 Uv로 전환하기

     * uv로 전환 시 Python 디펜던시 설치 속도가 pip 대비 약 10배 빨라지고, 별도의 venv 없이 비루트(non-root) 사용자로도 실행 가능함
     * pyproject.toml 기반으로 상위 의존성만 명시하면 uv가 자동으로 lock 파일을 관리하며, 의존성 트리와 정확한 버전 관리가 pip freeze보다 우수함
     * Dockerfile에서는 uv 및 uvx 바이너리 복사, pyproject.toml/uv.lock 파일 사용, 환경 변수 설정 등 단계별 변경이 필요
     * uv sync/add/remove, uv:outdated와 같은 명령어로 쉽게 의존성 추가·삭제·업데이트 및 패키지 최신 버전 확인 등 다양한 관리가 가능
     * 규칙적으로 lock 파일 관리 및 의존성 업데이트가 가능해져 협업 및 배포 환경에서 일관성 확보에 장점
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

10배 빨라진 의존성 설치, venv 미사용, 비루트 환경 구성

     * uv는 기존 pip 대비 Python 프로젝트의 의존성 설치 속도를 크게 개선하는 툴임
     * uv 도입으로 Flask/Django 등 다양한 프로젝트에서 기존 pip 대비 약 10배 빠른 설치 속도를 경험할 수 있음
     * 별도의 가상환경(venv) 없이도 컨테이너 내에서 비루트 사용자로 안전하게 실행할 수 있음

pyproject.toml vs requirements.txt

     * 기존의 requirements.txt 대신 pyproject.toml 파일에 상위 의존성만 명시하면 uv가 자동으로 uv.lock 파일을 생성함
          + pyproject.toml에 [project] dependencies 항목 추가
          + 기존 requirements.txt 삭제
     * uv의 lock 파일은 pip freeze 결과와 유사하지만 정확한 의존성 트리와 버전 정보를 갖추고 있음

Dockerfile 구성 변경

     * uv 및 uvx 바이너리를 컨테이너에 복사해 사용(정적 컴파일된 Rust 바이너리 사용)
     * 기존 requirements*.txt 대신 pyproject.toml, uv.lock* 파일 복사
     * 환경 변수 추가:
          + UV_COMPILE_BYTECODE=1: 빌드 단계에서 바이트코드로 미리 컴파일
          + UV_PROJECT_ENVIRONMENT=""/home/python/.local"": 별도의 venv를 생성하지 않고 특정 경로에 패키지 설치
     * 의존성 설치 명령어도 기존 pip3-install 대신 uv-install로 변경
          + 예: RUN chmod 0755 bin/* && bin/uv-install

의존성 추가, 삭제, 업데이트 등 관리

     * 별도의 run 스크립트로 컨테이너 내 uv 명령어를 실행할 수 있음
          + ./run deps:install: 이미지 빌드 후 lock 파일을 호스트로 내보내면서 설치
          + ./run deps:install --no-build: 빌드 없이 lock 파일만 갱신
          + ./run uv add mypackage --no-sync: pyproject.toml 및 lock 파일만 갱신, 실제 설치는 별도 실행
          + ./run uv remove mypackage --no-sync: 패키지 제거
          + ./run uv:outdated: 현재 의존성의 최신 버전 확인

영상 및 실습 가이드 제공

     * uv 도입, pyproject.toml 작성, Dockerfile 변경, lock/sync 명령, 의존성 추가/삭제, 최신 버전 확인 등 실제 데모 및 git diff 예시 제공
     * Flask, Django 두 프로젝트의 마이그레이션 diff도 참고 가능

   안그래도 poetry로 배포하던걸 마이그레이션하려고 했는데 안정적이고 간단해보이네요^^

        Hacker News 의견

     * uv는 pyenv, virtualenv, pip을 직접 대체하는 워크플로우를 지원한다는 점을 주목할 필요 있음. lockfile이나 pyproject.toml로 강제되는 방식이 아님. uv python pin <version> 명령어로 현재 디렉토리에 .python-version 파일 생성, uv virtualenv로 pyenv처럼 해당 버전의 파이썬을 다운로드 후 .venv 가상환경 생성, uv pip install -r requirements.txt로 requirements.txt의 패키지 설치, uv run <command>로 .env 파일의 환경변수 포함해 명령 실행 가능. 단, 환경변수 우선순위 문제 주의 (관련 이슈)
          + uv의 유연성은 정말 감탄스러운 수준임. pip으로 10분 걸릴 작업을 uv로 20~30초만에 처리하는 경험
          + uv 사용 계기는 바로 이 부분임. 엄청 편리함. 다만 uv pip이 느린 경우가 있어 원인을 모르겠는 상황, 혹시 회사의 네트워크 환경 문제인 것 같음
          + python 버전 정보가 pyproject.toml에도 저장되는 것으로 아는데, .python-version 파일이 꼭 필요한가라는 궁금증
     *
# 항상 최신 lock file을 보장하는 스크립트
if ! test -f uv.lock || ! uv lock --check 2>/dev/null; then
  uv lock
fi

       이런 방식은 lock file의 존재 의미를 무색하게 만듦. 파일이 없거나 무효할 경우, lock file에 심각한 문제가 발생한 상태라 관련 프로젝트에 익숙한 사람이 직접 대응하는 게 바람직함. 그렇지 않다면 lock file을 둘 이유가 없음. CI에서 lock file을 자동으로 교체해 혼동 발생 가능성 있음
          + (작성자 답변) lock file이 무효한 경우, 조용히 넘어가서 새 파일을 만드는 게 아님. uv lock에서 친절한 메시지로 실패 처리, 쉘 스크립트의 errexit로 바로 중단됨. uv lock --check 에러 리다이렉트는 같은 에러가 두 번 출력되는 것 방지 목적. lock 파일을 일부러 잘못 만들고 스크립트 실행 시, 구체적 에러 메시지와 함께 빌드가 멈춤. 스크립트는 if-else로 바꿔 더 명확하게 고침. lock 파일이 없으면 새로 생성하는 게 맞는 흐름임. 이때 생성해서 커밋하면 됨
          + uv sync --locked 옵션에서 이 부분 커버됨. lock file이 없거나 오래되면 명확히 실패시킴. 항상 --locked 옵션과 함께 빌드하는 걸 제안
          + 파이썬 세계에서는 lock file을 버전 관리에 잘 안 올리고, 설치 과정의 ""이상한 단계""로 다루는 경우가 많음
          + 이 방식엔 심각한 버그가 있음. --frozen 플래그를 쓰면 lock file이 갱신 안 되는 것이 맞는데, 실제로는 반대로 동작. lock file이 없거나 맞지 않으면 사람이 개입해야 한다는 점 동의
          + 그래도 lock file이 없으면 첫 실행이거나, 어차피 git upstream을 통해 덮어쓰이게 됨. 깨진 경우는 누군가 설치에 실수한 것이고, 새로 만드는 방법이 사실상 유일하게 합리적이라고 생각. 드문 예외지만 간단한 처리로서 충분함
     * 파이썬 툴이 파이썬 외의 언어로 개발되는 건 완전 반대 입장임. C가 이미 있어서 CPython이 표준화되어 있는데 굳이 새로운 언어(예: Rust)가 필요하지 않음. Pendulum 패키지가 3.13 지원을 7개월 넘게 지연했는데 Rust 네이티브 때문에 해당 문제를 고칠 줄 아는 사람이 부족해서라고 봄. 만약 C였다면 직접 고쳤을 것. (관련 이슈) 이상적으로는, Rust와 같은 외부 언어로 빠른 datetime을 만들고 싶다면 FFI로 여러 언어에서 쓸 수 있는 형식으로 만드는 게 맞음. Rust 기반은 아직 썩 마음에 들지 않고, 리눅스 커뮤니티가 꺼리는 것도 이해하게 됨
          + 이 관점 존중하지만, uv 같은 툴을 Rust로 만드는 건 좋은 아이디어라고 생각. 파이썬 관리 도구를 파이썬으로 만들면 ""닭이 먼저냐 달걀이 먼저냐"" 상황이 생김. 파이썬 도구를 쓰려면 파이썬 자체가 먼저 설치되어 있어야 하고, 어떤 파이썬 버전이 쓰이는지, 도구가 사용하는 라이브러리와 실제 앱 간의 충돌 가능성, 환경 변수 관리, 디버깅 모두 복잡해짐. 반면 Rust 등으로 빌드된 바이너리 도구는 그냥 받아서 쓰면, 이런 걸 신경쓸 필요 없이 즉시 동작. 사용자는 툴이 어떤 언어로 만들어졌는지 크게 신경 안 써도 됨
          + 파이썬을 좋아하지만, uv의 간편함과 속도는 비교 불가. EOL된 서버에서 최신 파이썬 필요할 때, 작은 스크립트에 종속성만 빠르게 설치하고 싶을 때 모두 uv가 베스트임. 동의하는 부분도 있는데, 예전에는 pure python으로만 짜다가, 점차 C extension을 쓰고, 한계 느끼면 아예 거의 모두 C로 쓰고 싶어지더라. C가 어려워서 최근엔 Rust로 리팩토링하는 중. 외부 코드가 내부보다 많아지면 그냥 전부 다른 언어로 바꾸는 게 나음
          + 파이썬만으로 도구를 만들어야 한다는 생각이 강하다면, 느린 Pylint 기다릴 때 본인은 산책하고 있겠음
          + 다양한 언어 지원은 사용자에게 별 부담 아님. 도구는 빠르고 문제를 잘 해결하면 충분. 실제로 속도가 훨씬 빠름. 관리도구는 개발자가, 사용 대상을 위한 도구임
          + 나는 어떤 언어로 만들어졌든 기능만 잘하면 됨. 파이썬 사용자가 도구에 기여할 수 있다는 점은 있지만, 도구가 목적을 잘 수행하면 언어는 상관 없음. 오히려 환경 문제에 봉착했을 때, 파이썬으로 만든 도구는 그 문제까지 영향을 받아 버릴 수도 있음
     * pip 대신 uv를 쓸 땐 조심해야 함. 기본으로는 pyc 파일을 생성하지 않으니 서비스 시작이 느려질 수도 있음 (참고)
          + 컨테이너에서 uv 쓸 때는 가이드 문서가 더 도움이 됨 (Docker 안내 문서)
     * uv를 flask 컨테이너에 써보면, 빌드 타임 차이가 지루할 만큼 클 뿐만 아니라, 설치 과정이 매우 예측 가능해짐. pip으로 종속성 버전이 바뀌는 당혹스러움이 없음. pyproject.toml 쓰고, uv lock 하면 끝. docker에서는 pyproject.toml, uv.lock 파일만 복사(HOT COPY)하고 uv sync --frozen --no-install-project 실행하면 앱 코드는 건너뛰고 설치 레이어는 캐싱 가능. 패키지 하나만 바뀌어도 전체 레이어 재빌드를 할 때의 고통을 알면 이 기능이 왜 중요한지 느낌. 환경변수 UV_PROJECT_ENVIRONMENT=/home/python/.local 사용시 venv 없이 베이스 이미지를 pre-warm 하면 빌드 공유 및 인프라 비용 절감. UV_COMPILE_BYTECODE=1 옵션으로 빌드 시 .pyc 파일 생성. mutable environment 소멸 및 reproducibility 강제, 이제 빌드가 안 되면 lockfile 책임으로 원인 명확해짐
     * 2025년이 되어도 파이썬 패키징과 의존성 관리는 여전히 혼란스러운 상태
          + uv를 모두가 쓰지 않아서 계속 혼란스러운 거라 생각함
          + 언어 설계 초기부터 이런 부분을 제대로 세팅하는 게 중요하다는 교훈임. v2.0 이후로 미루지 말 것, metadata를 실행 스크립트에 넣기 전에 여러번 고민할 것, 어떤 언어에는 맞지만 파이썬에는 좋지 않은 방식일 수 있음
          + 나는 의존성 문제를 한 번도 겪어본 적 없음. requirements.txt와 venv만 써도 충분
          + 의존성 관리가 여전히 엉망, 이제는 Rust까지 추가됨
     * uv, pip, conda 등 파이썬 패키지 관리자의 보안성 비교가 궁금함. 속도도 좋지만, 패키지 매니저의 보안이 훨씬 더 중요하다고 생각
          + uv가 pip보다 더 안전한 편임. 임의 코드 실행 없이 dependency를 분석, 기본적으로 패키지 해시 검증, 타이포스쿼팅 등 여러 위험성을 회피. 속도와 재현성도 뛰어남 (기술 소개, 호환성 문서)
          + 하지만 본질적으로 모든 패키지 관리자는 검증되지 않은 타사 코드를 다운로드 및 실행하게 됨. uv와 pip의 구현상 보안 차이보다, 애초에 외부 코드에 대한 정책을 마련하지 않은 점이 더 중요한 위험 요인임
     * PyPI에 패키지를 올리는 입장이라서, 개인적으로는 빠른 속도 때문에 uv를 쓰고 싶지만, pip과 완벽히 동일하게 동작한다는 보장이 없다면 쉽사리 바꿀 수 없음. 사용자가 ""pip install xxx""로 오류를 겪으면, 나도 동일 환경에서 재현·디버깅해야 하니까
          + pip과 100% 동일 방식은 아님. 큰 차이는 호환성 문서에서 다루고 있음. 일부는 표준대로 바뀌는 과정의 차이, 일부는 uv 고유의 디자인 선택임
     * UV는 실행만 하면 괜찮은 결과를 주는, 최근 파이썬 패키징에 가장 긍정적인 변화 중 하나라는 생각
     * 생산 환경 컨테이너 구축에 uv를 사용하는 훌륭한 가이드 문서도 소개함 (가이드 보기)
"
"https://news.hada.io/topic?id=21647","비난은 리더십을 무너뜨리는 가장 큰 요소 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      비난은 리더십을 무너뜨리는 가장 큰 요소 [번역글]

   비난은 리더십을 무너뜨리는 가장 큰 적 (존 D. 록펠러의 편지)
    1. 글 배경
          + 1870년 스탠다드 오일을 설립한 존 D. 록펠러가 71세에 아들 존에게 보낸 편지(1910년 7월 24일)에서 리더십의 본질을 설명함.
    2. 카네기와의 대화
          + 앤드루 카네기가 록펠러를 찾아와 ""어떻게 평범한 인재들이 무적처럼 일할 수 있나? 돈의 힘인가?""라고 질문함.
          + 록펠러는 ""돈의 힘도 중요하지만, 진짜 힘은 책임감에서 나온다""고 답함. 스탠다드 오일의 구성원들은 모두 스스로 ""내 책임은 무엇인가?""를 묻는 문화가 있다고 설명함.
    3. 비난하지 않는 리더십
          + 록펠러는 ""리더십의 핵심 원칙은 어떤 상황에서도 누구도, 어떤 일도 비난하지 않는 것""이라고 강조함.
          + 비난은 조직을 늪에 빠뜨려 리더십을 상실하게 하며, 존경과 신뢰를 잃는 지름길임.
          + 문제 발생 시 남 탓이나 불평 대신, ""내가 무엇을 할 수 있는가?""에 집중해야 함.
    4. 자기 책임과 자기비난의 차이
          + ""내 책임은 무엇인가?""라는 질문은 자기비난과 다름. 자기비난은 또 다른 비난의 함정일 뿐이며, 진정한 자기 성찰은 자기 긍정과 분석에서 시작됨.
          + 문제의 본질은 ""그들이 무엇을 해야 하는가""가 아니라 ""내가 무엇을 해야 하는가""임.
    5. 책임감의 문화와 실수에 대한 태도
          + 록펠러는 직원들에게 책임감을 부여하고, 실수 자체보다 무책임을 용납하지 않음.
          + 스탠다드 오일의 모토는 ""지원과 격려, 존중은 진심으로 받아들이고 두 배로 칭찬한다""임.
          + 변명만 하고 해결책이 없는 태도는 용납되지 않음.
    6. 경청의 리더십
          + 진정한 리더는 방어적 태도 대신, 구성원이 솔직하게 말할 수 있는 환경을 조성해야 함.
          + 적극적으로 경청하면 상대의 방어심이 풀리고, 더 깊은 문제와 정보를 이해할 수 있음.
          + 대화에서 힘은 말을 많이 하는 사람보다, 진심으로 듣는 사람에게 있음.
    7. 결론 및 교훈
          + 비난을 거부하는 태도가 리더십과 조직의 목표 달성에 핵심적인 역할을 함.
          + 리더의 역할은 모든 책임을 떠맡는 것이 아니라, 각자가 자신의 책임을 다하도록 돕는 것임.
          + ""목적이 방향을 정하듯, 비난을 거부하는 태도가 목표를 이루는 길을 닦아줌""

   요약문도 좋지만 ashbyash 님이 번역하신 원글을 꼭 읽어 보시길 추천드려 봅니다. 리더의 철학과 정렬된 솔선수범, 조직내 심리적 안전감 조성, 진정한 경청의 자세, 더나아 히든 피드백 까지 언급되어 있어서 놀랐습니다. 요즘 시대의 조직 문화에서도 많이 강조되는 요소들이 녹아져 있네요.

   글 자세하게 봐주시고 원문 추천까지 해주셔서 정말 감사드립니다 :)
"
"https://news.hada.io/topic?id=21624","Cosmoe - Wayland 위에서 동작하는 BeOS 클래스 라이브러리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Cosmoe - Wayland 위에서 동작하는 BeOS 클래스 라이브러리

     * 기존 BeOS 스타일의 API를 그대로 사용해 리눅스에서 직관적이고 강력한 네이티브 그래픽 앱을 쉽게 개발할 수 있게 하는 경량 GUI C++ 클래스 라이브러리
     * Wayland 기반 환경에서 동작하며, 기존 Haiku와 달리 리눅스 커널과 어떤 파일시스템에서도 실행 가능
     * 매우 쉬운 GUI 클래스, 멀티스레드 구조, 최소 자원 사용이 목표로 최신 하드웨어에 적합
     * Haiku 프로젝트에서 파생되었으나, Cosmoe는 리눅스 커널을 사용하고 더 가벼운 구조임
     * 전통적인 서버 기반 구조 없이, Wayland 환경에서 직접 실행되는 신형 라이브러리와 Haiku OS 전체를 재현하는 Cosmoe Classic 두 버전이 존재함

        Hacker News 의견

     * Haiku/BeOS는 나에게 진정한 명품 컴퓨터 디자인이라는 느낌을 주는 시스템이자 감탄을 불러일으키는 아름다움의 결정체
          + Trillian 0.7x 스킨들이 떠오르는 레트로한 향수 자극, 예전 앱 스킨 적용 문화를 다시 부활시키고 싶다는 바람
          + 아이콘이 정말 완벽한 매력, 이와 유사한 사용자 인터페이스가 MacOS에도 있으면 좋겠다는 생각
     * 파일시스템의 확장 속성 기능을 에뮬레이션하는 접근법은 매우 흥미로운 시도, 경량 OS 커스터마이즈에 있어 파일시스템 드라이버 전체 포트 없이 간결한 구조를 만들 수 있다는 기대감, 오픈소스 프로젝트에서 구체적 실험·경험을 들어보고 싶은 호기심
          + Linux는 이미 xattrs(확장 속성) 기능을 오랫동안 지원해왔으며, 굳이 이를 에뮬레이션할 필요는 없다는 의견
     * 마침내 Wayland를 부정적으로 보는 이들의 인식을 뒤집을 킬러앱: BeOS API 구현이라는 아이디어
     * BeOS/Haiku에서 특히 매력을 느끼는 두 가지 포인트가 있음. 첫째, 윈도우 스타일과 관리 방식. BeOS풍의 컴포지터/윈도우 매니저를 써보고 싶다는 희망. 둘째, 데이터베이스 같은 파일시스템, 이를 활용할 수 있는 GUI 및 커맨드라인 도구. 확장 속성 에뮬레이션으로 이 기능을 실현할 수 있을지, 아니면 아예 드라이버 전체 이식이 필요한지에 대한 궁금증(호환은 상관없이 기능성 자체에만 관심)
          + BeOS의 '데이터베이스형 파일시스템'은 극초기 버전에만 적용되었던 특징. 대부분은 BeFS(무료로 배포된 BeOS R5와 Haiku에 사용)의 기능인데, 실상은 사용자가 임의로 관리하는 네임드/타입드 btree 인덱스가 전부. 이메일 주소, 파일타입 등 다양한 키로 btree 인덱스를 생성할 수 있으나 이런 기능은 반드시 성능 저하라는 대가를 치르게 되는 구조(작은 파일이 많은 디스크에서는 보통 이 기능을 끔). 본격적인 풀텍스트 인덱싱과 비교하면 결과물이 별로이며, 애초에 일부 소수만 선호하는 틈새기능. 벽스위치가 달린 스탠드 램프처럼 소수만 혜택을 느끼니 일반적으론 적용 안 되는 맥락
          + BeOS 스타일의 윈도우 매니저를 찾는다면 pekwm에 커스텀 테마를 입히면 제법 비슷한 느낌 구현 가능, 실제로 탭 형태로 윈도우들을 한 데 묶어서 쓸 수 있다는 점이 가장 큰 장점이라고 생각. 관련 테마 예시는 여기 참조(X 윈도우 매니저라 직접 조합은 불가)
          + BeOS-r5-XFWM 관련 레퍼런스 링크
          + 해당 라이브러리를 적용한 윈도우 매니저 구현 아이디어에 대한 흥미
          + 파일탐색기에서 메일함까지 모조리 노출하는 방식이 정말 쓸만하다는 생각
     * Liquid Glass보다 확실히 더 흥분되는 사용자 인터페이스 소식이라는 평가
     * 2000년대 초에 BeOS API를 win32 위에 구현했던 경험. 당시엔 사람들이 BeOS를 위해 개발을 시작하면 자연스럽게 BeOS가 인기 OS가 될 거라고 순진하게 기대했던 기억
          + 독립적인 취미 개발이었는지 궁금. Gobe 측도 비슷한 방식으로 자사 생산성 앱을 BeOS에서 Windows와 Linux로 포팅했었다는 설명
          + 혹시 해당 구현의 권리를 소유하고 있다면 github에 공개할 수 있는지 질문
          + 나 역시 비슷한 프로젝트 경험자가 한 명 더 있었음(다만 내 경우엔 Flash/ActionScript를 위해서였음)
     * ""데모 앱이 여럿 포함되어 있어 기능을 알 수 있습니다""라는 설명이 BeOS의 전형적인 모토였다는 회상. 새로운 기술 프리뷰, 다양한 데모(큐브, 스피어에서 영상 시연 등)로 유저 호기심을 자극하지만, 정작 개발자는 끝내 나타나지 않았던 아쉬움. Microsoft Phone이나 Pebble Watch처럼 결국 개발자 생태계가 부재했던 것도 닮은 점. 진정한 사용성·참여가 부족했고 잠깐의 '우와'에 그쳤던 한계
          + BeOS가 주류에 오르지 못한 데는 Microsoft가 BeOS 설치 및 실행 환경을 어렵게 만든 영향도 컸다는 지적. 실제로 Hitachi Flora Prius는 Windows 98과 BeOS가 동시에 설치되어 있었지만 OEM 라이선스 문제로 이중 부팅이 차단되고, BeOS 파티션 활성화도 매우 복잡했음(관련 위키피디아)
          + Microsoft Phone은 개발자 문제라기보다 Microsoft가 자초한 실수 누적이 더 큰 원인. 제품 자체가 별로였고 나아지지도 않았음
          + 실제로 나는 1년 이상 BeOS를 메인 OS로 사용한 적 있음. ClarisWorks 개발진이 만든 GoBe Productive(Works 스타일 오피스), Fireworks 경쟁작인 e-Picture, BBEdit처럼 강력한 프로그래밍 에디터 Pe, 독창적 기능의 음악 툴(SoundPlay의 멀티 MP3 속도조절 믹싱, ObjektSynth의 오브젝트 지향 신디 등), 심지어 브로드웨이 쇼와 Cirque de Soleil에 실제 사용된 무대 제어 시스템, 그리고 현재도 남아있는 Moho 같은 애니메이션 소프트웨어 등 다양한 실전 활용 예시. 이미 사용성과 참여는 시작된 셈, 만약 Be, Inc.가 적당한 틈새시장만으로도 만족했다면(즉, Internet Appliances에 올인하지 않았더라면) BeOS의 실패는 막을 수 있었을 것. (아이러니하게도 Internet Appliances 시장 자체는 10년 후 iPad가 나오며 현실화)
     * BeOS API에는 익숙하지 않으나, 사용자 인터페이스 디자인은 매우 인상적. 하지만 어디에도 접근성(Accessibility) 관련 언급이나 계획이 보이지 않음. 기본적인 접근성 지원이 없다면 큰 문제라고 생각, 이미 내장되어 있거나 최소한 계획은 있기를 바람
          + Windows XP가 오늘날 그 어떤 OS보다 접근성이 뛰어났다는 생각, 이는 커스터마이즈·해킹에 친화적인 구조 덕분. 장애가 있는 이들이 여전히 XP 기반 시스템을 버리지 않는 이유. 코드가 작고 단순하며 접근성이 inherently 뛰어난 소프트웨어라는 점
     * BeOS를 기반으로 무언가를 만든다는 점이 흥미로움. 만약 Windows였다면 Microsoft가 새 버전을 내놓으면서 지원 불가 현상이나 제약이 곧장 생기는데, BeOS는 이미 죽은 OS라 그런 걱정이 없음. Haiku 프로젝트는 거의 25년이 다 되어가는데도 아직 뚜렷한 완성은 멀었다는 비유(달팽이보다도 느린 속도)
          + Haiku는 실제로 개발 상태가 꽤 양호. 예전 bare metal 환경에서도 쾌적하게 돌아갔었음(GPU 가속, wifi 정도만 미지원 예상)
          + Haiku의 버전 넘버링 정책은 보수적, 현재도 일상에서 충분히 사용 가능
          + Haiku 소스는 진입 장벽이 낮은 편. 코드가 복잡하지 않고 일관되며, 여러 시대·배경의 레이어가 겹쳐 있지 않아 읽기 쉽다는 평가(C++지만 최신 특성 남발하지 않음). 시스템 구조와 동작 인터랙션이 메탈 모델로 그려질 만큼 단순 명료
          + BeOS는 운영체계계의 라틴어 같은 존재라는 농담
     * BeOS는 Palm에 인수, Palm이 WebOS를 만들고 이를 LG에 넘긴 이력이 있음. 현재 내 LG WebOS TV에 BeOS 코드가 남아있을지 궁금
          + BeOS가 실제로 WebOS로 이어졌는지에 대한 궁금증에 대해, Palm은 2003년 PalmOne(하드웨어)과 PalmSource(소프트웨어)로 분사했고, BeOS는 PalmSource로 넘어감. 이후 PalmOne이 PalmSource에서 Palm 상표를 완전히 사와 Palm으로 회귀, 이들이 WebOS를 만들고 HP에 팔림. 한편 PalmSource는 ACCESS가 인수(ACCESS는 NetFront 브라우저 개발사)하며 BeOS 권리도 ACCESS로 이동
          + 실제 Be 출신 유일한 요소라면 BeIA의 Binder 기능이 Android에 반영됐다가 나중에 아예 새로 작성된 이력 정도
"
"https://news.hada.io/topic?id=21660","누가 최고의 CDO(Chief Data Officer)인가 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    누가 최고의 CDO(Chief Data Officer)인가

     * 게임 방식으로 CDO 체험하기
     * ""당신은 새로 임명된 최고 데이터 책임자(CDO)입니다. 당신의 역할은 데이터 혼돈 속에서 회사를 한 걸음씩 이끌어 나가는 것입니다.""
     * 쏟아지는 이메일속에 다양한 의사 결정을 요구하며, 어떤 결정을 하느냐에 따라 회사의 성패가 결정됩니다.
          + 당신의 선택은 예산, 데이터 품질, 수익, 평판등에 영향을 주게 됩니다.
          + 어려운 선택, 복잡한 데이터, 그리고 가끔씩 발생하는 사이버 재난에서 살아남으세요.
     * Easy/Hard 모드 선택 가능

   Cdo 라니까 순간 빅쇼트가 생각났네요;

   재밌네요ㅋㅋㅋㅋㅋ

   브금 압박감 장난아니네요
"
"https://news.hada.io/topic?id=21597","SEGA, '용과 같이 8', '페르소나 3 리로드', '진 여신전생 V' 등 실적 실수로 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         SEGA, '용과 같이 8', '페르소나 3 리로드', '진 여신전생 V' 등 실적 실수로 공개

     * SEGA와 ATLUS 주요 타이틀의 누적 판매량이 실수로 공개됨
     * ‘용과 같이 8’, ‘페르소나 3 리로드’, ‘소닉 프론티어’, ‘진 여신전생 V’, ‘페르소나 5 로열’ 등 다수 대표작이 포함됨
     * 공식 PDF 발표자료에서 회색 박스에 가려졌던 수치가 복사 기능으로 노출되며, 상세 판매 데이터가 유출됨
     * '페르소나 5 로열'은 누적 725만 장, '용과 같이 8'은 166만 장, '페르소나 3 리로드'는 207만 장 판매로 집계됨
     * '진 여신전생 V'는 최신작 'Vengeance' 포함해 211만 장의 판매고를 기록함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

SEGA·ATLUS 게임 판매량 실수 유출 개요

     * SEGA SAMMY 2025년 경영 프레젠테이션에서 주요 게임 타이틀의 연도별 및 누적 판매량이 실수로 노출됨
          + PDF 25페이지 회색 박스 뒤에 숨겨진 텍스트가 복사/붙여넣기 기능으로 드러남
     * 유출된 자료에는 '용과 같이', '페르소나', '소닉', '진 여신전생', '토탈 워' 등 대표 시리즈별 연도별 실적이 정리됨

주요 타이틀별 누적 판매량

     * 용과 같이 8 (Like a Dragon: Infinite Wealth): 166만 장 (FY2024: 118만, FY2025: 48만)
     * 용과 같이 외전: 96만 장 (FY2024: 74만, FY2025: 22만)
     * 페르소나 3 리로드: 207만 장 (FY2024: 122만, FY2025: 85만)
     * 소닉 슈퍼스타즈: 243만 장 (FY2024: 181만, FY2025: 62만)
     * 소닉 프론티어: 457만 장 (FY2022: 320만, FY2023: 76만, FY2024: 61만)
     * 토탈 워: 워해머 III: 234만 장 (FY2022: 96만, FY2023: 58만, FY2024: 42만, FY2025: 38만)
     * 진 여신전생 V (Vengeance 포함): 211만 장 (FY2022: 99만, FY2023: 15만, FY2024: 1만, FY2025: 96만)
     * 용과 같이 7 (Yakuza: Like a Dragon): 286만 장
     * 페르소나 5 로열 (리마스터 포함): 725만 장 (FY2020: 103만, FY2021: 70만, FY2022: 35만, FY2023: 182만, FY2024: 160만, FY2025: 175만)
     * 팀 소닉 레이싱: 350만 장
     * 토탈 워: 삼국: 321만 장

추가 정보 및 주석

     * 페르소나 5 로열의 판매량에는 2023년 출시된 리마스터판 포함
     * 진 여신전생 V의 판매량에는 FY2025 출시된 'Vengeance'도 포함
     * 일부 타이틀은 연도별로 신작과 리마스터, 확장판이 통합 집계됨

        Hacker News 의견

     * Persona 5 Royal이 725만 장이나 팔렸는데도 Sega가 여전히 이를 틈새 애니메이션 게임 정도로만 다루는 느낌을 받음, 반면 Sonic은 세금 소프트웨어를 빼면 거의 모든 장르에 진출 중인 모습임
          + Sega Sammy의 콘텐츠 사업은 전체 매출의 약 3분의 1 정도이고, 이에는 모든 게임, 애니메이션, 라이선스(LEGO 등) 계약이 포함됨, Persona 5 Royal은 실질적으로는 곁다리 사업 느낌임
          + SonicTax라는 이름의 세금 프로그램, 은근히 괜찮은 어감이라고 생각함
          + 아이디어를 주지 말라는 말과 함께 실제로 Tax Heaven 3000이라는 세금 관련 유머 게임도 있다는 정보 공유
          + Persona 5만 해도 크롤러, 리듬, 핵앤슬래시, 택틱스, 모바일, TV 애니메이션, 만화 등 다양한 파생작이 이미 존재함
          + Sonic이 회계사로서 IRS에서 Dr. Robotnik과 싸우는 모습 희망, Tails는 사무원 역할 상상함
     * 이번 이야기는 문서 보안 처리(리덕션) 시도의 허점이 드러난 점이 핵심임, 이런 일은 자주 발생함
          + 예전 근무지에서 지방정부로부터 여러 문서를 받았는데, 우연히 아무런 관련 없는 사람의 사회보장번호(SSN)를 여러 번 발견한 경험 있음
          + 악의적인 PDF 편집기를 정보기관에 판매해보면 어떨까, 리덕션 도구 사용 시 원본 텍스트를 유출하는 방식임, 전체 PDF 중 리덕션되는 파일이 실제로는 가장 가치가 높음
          + 실제 슬라이드를 보면 리덕션 목적이 비밀보다는 조정 과정에서 임시로 숨김 처리된 것으로 보임
          + 내가 신뢰하는 유일하게 안전한 방식은 디지털로 리덕션 후 인쇄해서 다시 스캔하는 과정임, 업무적으로도 허술한 리덕션 덕분에 돈 번 경험이 있어 남들도 그렇게 썼으면 하는 바람임
     * Team Sonic Racing이 Total War Three Kingdoms보다 많이 팔렸다는 게 놀라움
          + 전체적으로 Total War와 그 장르는 꽤나 마이너하고, 만화풍 레이싱은 대부분 쉽게 즐기는 경향 있음
          + Team Sonic Racing은 iOS와 안드로이드에도 출시된 반면, Total War Three Kingdoms는 PC 전용임, 가격도 크게 다르기에 단순 매출 비교가 쉽지 않음
          + 오히려 반대로 팔렸다면 더 놀라웠을 것, 개인적으로 TW:3K가 이렇게 많이 팔렸다니 그게 더 인상적임
          + 동물 캐릭터가 빠르게 질주하는 콘셉트는 언제나 인기 있는 게임 아이디어임
     * 오프 토픽 질문: 저 웹사이트를 도대체 어떻게 쓰는지 궁금함, 광고가 너무 심각함
          + 나 빼고는 모두 광고 차단기를 사용 중임
          + Adguard DNS가 매우 효과적이며 이게 없으면 그런 사이트는 아예 사용할 수 없음
          + 최신 용어와 광고 차단기도 꼭 업데이트할 필요가 있음
     * macOS의 Preview 앱에는 효율적인 리덕션 기능이 탑재되어 있으니 이럴 때 사용하기 좋다는 정보 공유 지원 링크
     * Sonic IP의 가치는 여전하며 오랜 세월이 흘러도 꾸준히 좋은 판매량을 기록하는 점에 놀람, 반면 P5R이 이렇게 잘 팔릴 줄은 몰랐음, 한때 시도해봤으나 애니메이션 분위기 때문에 손이 잘 안 갔음
          + 애니메이션 스타일이 부담스러우면 Clair Obscur: Expedition 33 추천, Persona 전투 시스템에서 영감을 받았고 분위기는 훨씬 성숙함, Steam에서 평가도 매우 높음
          + 애니메이션 분위기가 싫다면 굳이 다시 시도할 필요 없음, 정말 애니 분위기 가득한 게임임
          + Sega는 꾸준하게 충성도 높은 팬층을 위해 다양한 콘텐츠를 만들고 있음, Nintendo만큼 스포트라이트를 받지 못해 아쉬움, Sonic도 그 가치에 비해 놀라운 게 아니라 오히려 마스코트로 엄청난 투자를 받았기 때문임, 최근 영화들도 확실히 Sonic 브랜드에 대한 관심을 재점화함
          + P5R을 미완 상태로 남겨둔 채 손을 안 대고 있음, 90시간 넘게 10대들의 성장 스토리에 시간과 정신을 쏟기엔 너무 나이가 많아졌다는 자각 임
          + 초기 설정이 부담스러웠지만 카드 전투나 포켓몬식 수집 요소로 즐길 수 있어 꽤 괜찮음, 관련해서 Shin Megami Tensei V나 게임 내 미니막스도 해볼 수 있음
     * Sega가 왜 각 게임의 판매량을 세부적으로 공개하지 않는지 의문임
     * 해당 수치들이 실제로는 훨씬 더 클 줄 알았음
          + 정말 놀라운 수치임, 상대적으로 작은 IT나 금융기업 매출 수준임, 게임 산업에서 수억 달러 매출은 사실 Rockstar나 대형 모바일 게임 등 일부 기업이 거의 다 가져간다고 추정함
     * Like A Dragon 시리즈 판매량이 생각보다 낮아 의외임, 90년대부터 게임 해왔지만 최고의 게임 중 하나라고 생각함, 즉각적인 명작임
          + 시리즈 여러 편을 즐겼으나 추천이 쉽지 않음, 줄거리가 과장되고 미니게임은 재미있지만 반복적이고 본편 스토리는 약함, 다양한 장르가 뒤섞인 편이라 입문 난이도가 있음
          + 서구권에서 이 시리즈의 인기가 아직 떨어진다는 느낌임, Infinite Wealth가 더 잘 팔리는 분위기인데, Like A Dragon이 조용한 히트작으로 시리즈 인지도를 올려준 영향으로 해석함, Kasuga로 리부트를 한 전략 덕분에 신규 유입이 쉬워졌다는 점 긍정적 임
          + 전체적으로 보면 이 시리즈도 꽤 마이너한 축에 속함, 그래도 1~2년마다 후속작이 나오는 수준에서는 준수한 성과임
          + 시리즈가 항상 GamePass에 포함되어 있는 점이 실제 판매량 집계에 어떻게 반영되는지 의구심이 있음, 특히 Infinite Wealth는 예외로 보임
          + JRPG 전문 팬 입장에서 봤을 때 입문이 너무 어려움, 어디서부터 시작해야 할지, 시리즈마다 장르가 달라지는 구조, 실제 게임 외에 홍보에서는 엽기적 부가활동만 강조되는 점, 전형적이지 않은 배경까지 접근 난이도를 올린다고 느낌, 입문자 관점에서 무엇을 얻을 수 있을지 불분명하다는 인상임
     * Warhammer 3가 Three Kingdoms만큼 많이 팔리지 않은 게 놀라움
          + Three Kingdoms는 중국 시장에서 크게 성공했을 것 같음, Warhammer는 서구권 브랜드라 해당 시장에서의 차이가 있을 듯함, 1편이나 2편으로 만족하는 유저가 많다면 확장판 판매가 적을 수밖에 없음
"
"https://news.hada.io/topic?id=21626","Claude Code for VSCode 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Claude Code for VSCode 공개

     * Anthropic의 Claude Code를 VSCode와 통합하여 개발자의 코딩 경험을 강화하는 확장
     * Claude Code가 별도 설치되어 있어야만 작동함

주요 기능

     * 자동 설치: VSCode의 터미널에서 Claude Code를 실행하면 확장 프로그램이 자동으로 감지 및 설치됨
     * 선택 영역 컨텍스트 인식: 에디터에서 선택된 텍스트가 Claude의 입력 컨텍스트에 자동으로 포함됨
     * Diff 뷰 지원: 코드 변경사항(diff)을 VSCode의 내장 뷰어에서 바로 확인할 수 있음
     * 키보드 단축키: Alt+Cmd+K와 같은 단축키로 선택한 코드를 Claude 프롬프트로 손쉽게 전달할 수 있음
     * 탭 인식 기능: Claude는 VSCode에서 열려 있는 파일의 정보를 파악해, 상황에 맞는 코드 지원이 가능
     * 설정 옵션: /config에서 diff tool을 auto로 지정하면 IDE 통합 관련 기능을 쉽게 활성화할 수 있음

     * 초기 공개(early release) 버전으로, 사용 중 버그 발생 가능성이나 일부 미완성 기능이 있을 수 있음

   Cursor 와 분명한 차이
    1. Cursor는 VSCode 에 묶여 있다. 반면, Claud Code 는 CLI (command line interface) 방식이라 어떤 툴과도 사용 가능.
    2. Cursor는 실질적으로 다른 LLM을 활용하지만 Claude Code 는 Claud 에 특화 되어 있다. 하지만, 실력을 비교해 보면 확실히 우위에 있다. 이것은 Gemini 2.5 Pro 와 비교해도 그렇다 (DotNet 기준, 언어별로 다를 수 있음).

   이러면 cursor과 다른게 무엇인가요?

   아니 윈도용(WSL말고)응 안만들고 자꾸 딴짓이여… ㅡ,.ㅡ;;;

   공감입니다 ㅎㅎ

   WSL도 윈도우 OS.의 한 부분인데, 쓸 줄 모르는구나... GUI로만 개발해서, CLI는 아예 모르거나...

   Wsl을 사용하면 파일시스템 성능이 매우 떨어지는 문제(wsl2)도 있고 가상화를 hyperv에 의존한다는 단점도 있어요. Wsl을 사용하지 못하는 케이스도 많습니다.

   저도 공감합니다. 저도 회사에서 WSL 사용 금지라서~ 하다가 포기 했어요.
   SSL 인증서 뚫어서 어떻게든 하고보니 WSL이 안되는군요.

   ㅋㅋㅋㅋ 너무 공감

   Git Copilot 과 어떤 차이가 있을려나요?

   Copilot 은 MS IDE 특화되어 있고, 그야말로 초보 수준인 반면, Claude 는 CLI / Git Bash 에서 동작하므로 여러 환경에서 사용 가능하며, Coding 실력 상대적 높은 편

   intellij 플러그인도 있어요.
   단순 cli 와의 차이는 ide 에서 보고있는 혹은 선택하고있는 파일, 라인을 바로 인지한다는거죠.
   물론 일반 터미널에서 실행하고 /ide 명령어로 연계를 시작 할 수도 있어요.

        Hacker News 의견

     * 기존 IDE에 agent 기반 코딩을 통합하는 방식은 잘못된 방향이라고 생각함. 여러 Git worktree를 관리하고 각각의 agent가 동시에 동작하는 방식이 더 나음. Claude Code가 작업을 끝내기를 20분 이상 기다릴 필요가 없음. 그래서 이를 관리하는 UI를 직접 만들었고, 점점 여러 agent를 관리/검토하는 새로운 유형의 IDE로 발전 중임. 기존과 달리 한 번에 하나씩 작업하는 게 아님. https://github.com/stravu/crystal
          + 나는 개인적으로 다르게 생각함. 매일 Cursor를 상업적 프로젝트에 사용함. background agent는 특정 상황에서는 유용하긴 하지만 대부분은 산만함만 유발함. 내가 선호하는 코딩 방식은 하나의 목표에 집중해서 반복적으로 해결에 가까워지는 방식임. 작업이 끝나기를 기다리는 동안은 문서나 관련 정보를 탐색해서 다음 단계를 고민함. 기존 코드나 변경사항을 검토하면서 현재 진행 상황을 정확히 파악하는 것도 무척 중요함. 각각의 작업에 여러 agent를 관리하는 개념은 내 스타일과 맞지 않음. 컨텍스트 전환과 멀티태스킹이 너무 많아짐.
          + 이런 워크플로우 제안에서 항상 궁금한 점은 내 개인 컨텍스트를 어떻게 관리할 수 있냐는 것임. 동료 코드 리뷰에서도 모든 코드를 완벽하게 이해하고 검증하는 대신, 주로 큰 실수(코드 스타일, 베스트 프랙티스 등)만 빠르게 확인함. 그래서 하루에 많은 PR을 빠르게 검토할 수 있음. 더 중요한 작업(내가 책임지는 경우)에는 branch를 테스트하고 구현을 꼼꼼히 확인함. PR 업데이트마다 이 작업을 반복하니 많은 시간이 소요됨. 여러 agent가 동시에 diff를 제안하면, 특히 검증이 필요할 때 컨텍스트 전환을 어떻게 하면 좋을지, 그리고 한 번의 업데이트가 다른 작업에 미묘하게 영향을 줄 때 module 의존성은 어떻게 관리할지 고민임.
          + 이런 방식도 IDE 플러그인으로 동일하게 만들 수 있음.
          + 멋진 툴임! Claude Code TS SDK를 쓰지 않은 이유가 궁금함. 패키지는 설치했지만 별도로 claude 명령어를 직접 실행하는 구조인 듯함. 참고로 electron-trpc도 살펴보길 추천함. IPC 처리가 훨씬 간편해짐.
          + 이 툴도 멋지지만 해결하는 문제가 다름. background agent에는 두 가지 큰 문제가 있음. 1) 분리된 환경을 제대로 세팅하는 데 진입장벽이 존재함. 프로젝트마다 난이도가 차이가 크고, 간단하게 컨테이너 고르는 것부터 모든 의존성 세팅이 지옥인 경우까지 다양함. 반면 IDE 내에서 작업하면 보통 모든 게 이미 갖추어져 있음. 2) 사람들이 agent가 코드를 어떻게 빌드하는지 배워야 함. agent가 IDE에서 동작하고 내가 실시간으로 조언하거나 교정해줄 수 있으면 background agent에 장기적으로 훨씬 도움이 됨.
     * 내가 바라는 점은 다음과 같음. git worktree 기반 컨텍스트 전환을 동일한 IDE 창에서 강력하게 지원하는 환경, 각 worktree branch마다 터미널 기반 agent를 연결할 수 있는 프레임워크, 결국엔 diff, 권한 요청 알림, 진행상황 알림을 위해 더 나은 오픈 프로토콜로 진화하는 환경, 각 worktree branch별로 agent 상태와 알림을 모니터링하는 사이드바, 여러 branch 전체에서 agent 메시지에 알림처럼 빠르게 대응할 수 있는 인터페이스임. standalone agent manager 툴에는 이런 기능이 있지만, 실제 엔지니어로 바로 뛰어들고 싶을 땐 그런 툴을 제대로 쓸 수 없음. branch별로 브라우저 테스트 창이나 모바일 에뮬레이터/시뮬레이터 인스턴스와의 연동도 필요함. 빠른 모델 기반의 코드완성, 다양한 언어 서버 지원, 고품질 IDE 역할의 확장 생태계도 갖추어야 함. 나는 현재 macOS 데스크탑 여러
       개에서 Windsurf, Claude agent, 웹브라우저, 모바일 시뮬레이터를 각각 분리해 관리중임. 이 방식 매우 번거로움.
          + 내가 원하는 건 디버깅 기능을 가진 코딩 agent임. 스택을 따라가고, 로컬 변수와 인자 값을 살펴보고, print나 assert 대신 실제로 내부에서 무슨 일이 벌어지는지 들여다보기 원함.
          + 여러 branch에서 agent 메시지에 알림처럼 빠르게 반응하는 기능 관련해서, 나도 VSCode용 플러그인을 만들어보려 했음. Claude가 파일과 라인으로 바로 점프하게 하는 기능이었는데, 어느 정도 작동했지만 계속 멈추는 문제가 있었음.
     * Cursor와 Claude Code 사이의 실제 차이가 뭔지 잘 모르겠음. 둘 다 써봤고 회사에서 지원해서 그냥 Cursor로 이동함. CLI냐 UI냐 외에는 둘 다 멀티파일 수정이 되어서 별다른 차이점을 발견 못했음. 여러 에디터를 동시에 쓰거나 JetBrains와 Cursor를 오가야하는 불편한 과도기가 빨리 끝나길 바람.
          + 품질과 활용도 측면에서 둘은 큰 차이임. Claude Code는 완전한 agentic 방식임. 작업을 맡기면 전체를 직접 구현해, 아주 괜찮고 잘 작동하는 코드를 만들어냄. 테스트, 커밋, 커맨드 실행, 원격 시스템 접속, 디버깅 등 가능함. Token 사용 최적화는 하지 않으므로 Cursor보다 첫 시도에서 더 고품질 코드가 나오지만, 그만큼 비용이 높음. Cursor의 agent 모드는 아직 초기 단계임. Cursor는 주로 파일 수정 툴에 가깝고 Claude Code는 주니어 개발자 같은 역할임.
          + 두 툴을 함께 쓰는 경우가 많음. Cursor는 IDE, Claude Code는 IDE의 터미널에서 실행하는 방식임. 퍼포먼스 측면에서 agent 방식이 다르고, 같은 기본 모델을 쓰더라도 코드베이스 분석, 하위 모델 사용, 툴 연계 등에서 차이가 큼.
          + 별다른 차이를 못 느낀다니 신기함. 내게는 Claude가 모든 면에서 월등함. scala, python, js, dart를 주로 씀. Claude로 굉장히 생산적인 어시스턴트 역할을 얻음. 작은/중간 변경에 특히 매우 쓸모 있음. 계획을 잘 세우고 쓰면 마법 같은 느낌임. 코드 중복이 좀 있지만 그 정도임. Cursor는 많은 손질이 필요해서 오히려 속도만 느려졌음.
          + Claude Code 진짜 인상 깊음. 마치 또 다른 프로그래머가 내 터미널에 같이 앉아있는 느낌임. 완벽하진 않고 하고 싶은 걸 제대로 이해할 때까지는 도움을 주는 게 필요함. 하지만 컨텍스트만 잘 맞추면 정말 놀라움. 내 경우에는 프로젝트를 완전히 이해시킨 것도 아니고 TypeScript나 웹 개발에도 안 쓰고 있음.
          + Cursor는 별도의 IDE로 전환해야 하고(VSCode를 원래 쓰지 않는 한), Claude Code(또는 Aider)는 현재 IDE와 병행으로 터미널에서 바로 프로젝트 파일을 수정함. 내 경우 vim+tmux+bash 조합이라 CLI 어시스턴트를 선호하지만, VSCode 말고 다른 GUI IDE 쓰는 사람에게도 동일하게 해당되는 장점임.
     * 지난주 github이 copilot의 프리미엄 요청 제한을 도입했을 때 큰 반발이 없던 게 오히려 충격적이었음. 본격적으로 제한에 걸리는 사람들이 생기면 더 큰 반발이 있을 거라 예상함. 경쟁 제품이 있다는 게 정말 다행임.
          + Claude Code는 한 번 실행에 10달러도 순식간에 사라질 수 있음.
     * VSCode Copilot을 Agent 모드 + Claude Sonnet 3.7이나 4로 사용할 때 대비해서 특별한 장점이 있는지 궁금함. 내가 놓치고 있는 게 뭔지 알고 싶음.
          + 직접 Claude Code를 경험해봐야 이 질문에 답할 수 있음. 그냥 여기서 논쟁해봐야 의미 없음. 리눅스 터미널을 주력으로 쓴다면 바로 빠져들게 됨. 반드시 문서도 읽어 볼 것. CLAUDE.md를 쓰고, 큰 작업은 마크업 형식의 계획 문서로 만들고, 반복해서 계획-수정하고 구현을 맡겨야 함. context limit에 가까워지면 메모리를 파일로 write해서 /clear 후 다시 읽어오는 방식 사용하면 훨씬 효율적임.
          + Playwright MCP와 Copilot agent 모드를 연동하려 했는데 실패함. 설치되고 툴 선택에서도 보이지만 끝내 Copilot이 기능 접근을 허용하지 않음.
     * Copilot agent 모드에서 Claude backend를 쓸 때에 비해 이 솔루션의 이득은 뭔지 궁금함
          + 이 스레드의 다른 설명(특히 https://news.ycombinator.com/item?id=44353972)이 vscode copilot에도 적용됨
          + 며칠 써본 결과, 이 통합으로 인해 파일을 직접 열어서 업데이트를 확인해야 했던 불편함이 개선됨. terminal 모드에서는 모든 게 백그라운드에서 처리되어 무슨 일이 벌어지는지 몰랐지만, 통합 IDE에서는 실시간으로 모든 걸 볼 수 있음. 다만 agent가 붙여주는 이름(예: Pondering, Twerking, Juggling 등)은 처음엔 신선했지만 곧 쓸모가 없어짐.
     * 다소 탈선된 질문이지만, VSCode에서 Roo를 사용하는 사람이 있는지 궁금함. 그리고 Roo의 브라우저 기능이 GitHub copilot이 제공하는 Claude와 잘 연동되는지도 궁금함
     * 알고 있기로는, VSCode(또는 Cursor)에서 Claude Code를 실행하면 이것이 자동 설치됨. 굳이 따로 찾아서 설치할 필요가 없음, 맞는지?
          + Claude Code를 VSCode에서 실행하면 자동 설치된다는 사실, 좀 침해받는 느낌 아닌가?
          + 맞음. 익스텐션 웹페이지에도 명시되어 있음.
Auto-installation: When you launch Claude Code from within VSCode’s terminal, it automatically detects and installs the extension

     * Claude Code가 여러 단계를 한 번에 이해한다는 사실을 인지하면서 워크플로우가 변하기 시작함. 파일별로 고민하던 방식이 점점 “모듈 분리, 테스트 작성, 호출부 리팩토링” 같은 단일 행동 단위로 전환됨. Claude도 이를 하나의 단위로 이해함(최대 노력 모드). 이런 변화가 점진적으로 코딩 접근법을 바꿈. 구문 걱정을 덜고, 더 많은 스캐폴딩을 작성하며, 여러 작업을 묶어서 처리하게 됨. 미묘하지만 장기적 영향이 큼. 앞으로 LLM agent가 더 잘 탐색할 수 있도록 코드베이스 구조(플랫, 최소 우회, 선언적 메타데이터 등)를 의도적으로 개선하는 시점이 얼마나 빨리 올지 궁금함.
          + 언제를 미래라고 말하기 전에 이미 진행 중임. Armin Ronacher가 Python에서 Go로 코드 작성 비중을 높인 것도 LLM이 Go를 더 잘 이해해서임. 내 동료도 데스크탑 앱을 Rust로 옮긴 게 툴링과 타입 시스템 덕분에 agent가 탐색하기 쉬워서임. AI가 읽기 쉽게 문서화하는 방향으로 이미 고민이 이동 중임.
          + LLM이 Go처럼 명확한 정적 타입과 간결한 문법, 일관된 작성 방식이 있는 언어에서 확실히 잘 작동한다는 이야기를 들으면서 이런 부분을 계속 생각 중임. 도구(언어, 프레임워크, 라이브러리 등)에서 파생되는 불필요한 복잡성을 얼마나 걱정하지 않아도 되는지에 따라 우리의 두뇌 자원을 더 본질적인 문제 해결에 쓸 수 있음.
     * Claude Code가 Jetbrains에도 들어간다니 기대됨! https://plugins.jetbrains.com/plugin/27310-claude-code-beta-
          + 왜 이 댓글에 비추천이 많은지 모르겠음. 나도 동일하게 기대하고 있음. 공유해줘서 고마움

   Claude Code를 이용함에 있어서 MSA 비슷한 개념이랄까, 마이크로서비스 단위로 나눌 수 있는 만큼 쪼개고 그걸 하나의 단위로 맡기는게 제일 효율적일 거 같다는 생각이 들어요.
"
"https://news.hada.io/topic?id=21638","음식 속 플라스틱 함량 정보 – PlasticList","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     음식 속 플라스틱 함량 정보 – PlasticList

     * PlasticList는 다양한 식품의 플라스틱 함량 데이터에 대한 스냅샷 수준 정보를 제공함
     * 결과는 한정된 샘플과 시점에 기반하므로, 정확하거나 대표적이라고 볼 수 없음
     * 테스트에는 불확실성과 다양한 분석 방법이 존재함을 강조함
     * 특정 화학물질이 검출되었어도, 그 자체로 위해성 여부나 건강 위험을 의미하지 않음
     * 결과는 참고 용도이며, 정책 결정, 소비 습관 변화의 근거로 삼기에 충분하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

안내 및 주의 사항

     * 이 테스트 결과는 높은 신뢰도를 갖지 않음을 먼저 밝힘
     * 독자 역시 이 결과만으로 확실한 결론이나 정책적 판단을 내려서는 안 됨
     * 이 페이지는 원시 테스트 결과의 스냅샷으로, 추후 더 깊은 연구의 출발점 또는 영감으로 쓰임이 적합함
     * 소수 제품의 일시적 샘플 분석 결과로, 실제 제품의 평균적인 내용물을 직접 반영하지 않을 수 있음
     * 모든 테스트에는 불확실성이 따른다는 점, 그리고 분석 방법이 다를 경우 결과도 달라질 수 있음을 강조함

데이터 및 해석 관련 주의

     * 데이터 중 색이 더 진하게 표시된 값이 높은 분위수를 나타내긴 하지만, 이것이 반드시 건강에 문제가 있음을 의미하진 않음
     * 식품에서 특정 화학물질이 검출되었다 해도, 그 존재만으로 유해성을 의미하지 않음
     * PlasticList 운영진은 이 결과에 대한 재현 연구를 강력히 환영하며, 잘못된 부분은 언제든 교정될 수 있음을 밝힘

결론

     * 독자는 이 데이터를 참고 자료로만 여길 필요가 있음
     * PlasticList의 목록 자체가 공식 결론, 정책 권고, 개인적 구매 결정의 근거로 사용될 수는 없음

        Hacker News 의견

     * 최근에 생각하게 된 덜 바람직한 제품군이 있음에 대해 이야기하고 싶음. 바로 내장형 플라스틱 그라인더가 달린 후추통임. 이전부터 통이 비워질수록 그라인더가 점점 더 효과가 떨어진다는 점을 눈치챔. 왜 그런지 생각해보니, 후추를 갈면서 플라스틱도 음식에 그대로 갈려 들어간다는 깨달음에 도달함. 이에 대한 논의가 생각보다 적은데, 5년 전 StackExchange 질문에서 이 문제를 다룸(링크). 플라스틱 그라인더는 1회용이든 내구성 제품이든 단순히 존재해서는 안 될 상품군이라는 생각임. Walmart에서 “plastic grinders” 검색해보면 현재 5개 정도 제품이 뜨는데, 본체가 플라스틱인지, 그라인더 부위 자체가 플라스틱인지 불분명한 제품도 있음. 여러 제품이 실제로 그라인더까지 플라스틱으로 보임(Walmart 결과), 현 상태 기록은 archive에서 확인 가능.
          + Fletchers’ Mill이 여기서 언급되지 않아 의외임. 이 회사는 Maine에서 좋은 품질의 그라인더를 만듦. 후추그라인더는 스테인리스 스틸, 소금그라인더는 나일론(내식성이 강하기 때문)을 사용. 소금 그라인더까지 완전히 플라스틱 프리 제품을 찾으려면 다른 곳을 찾아야 함. 개인적으로는 일반 소금을 소금통에 넣어서 사용함. 확실히 플라스틱 없는 소금 그라인더도 분명 어딘가에는 있을 것임(Fletchers’ Mill)
          + Peugeot도 정말 훌륭한 스틸 기반 후추 그라인더를 만드는 것으로 유명함. 견과류 그라인더도 뛰어남. 흥미로운 점은 200년 전부터 가족이 최초의 강철 공장을 시작한 뒤로 후프스커트와 잔디깎이 등 아주 다양한 제품군에 손을 댔다는 사실임(Peugeot의 역사). 자동차 사업은 Stellantis에 팔았지만, Peugeot 가문의 다양한 비즈니스는 여전히 이어짐.
          + 플라스틱 후추 그라인더 문제는 생각도 못 했었는데, 덕분에 이제 주방에서 플라스틱과 유해 화학물질을 제거하기 위한 여정에서 새로운 그라인더를 찾아야겠다는 생각임. 지금까지는 팬, 통, 조리도구 등은 모두 교체함. 음식이 아니긴 하지만 잘 언급되지 않는 또 다른 플라스틱 노출 경로로 의류 건조기 배기구가 있음. 합성 섬유 옷에서 플라스틱 입자가 대기 중으로 배출될 수 있다는 생각이 들었음. 물론 이것도 자동차 타이어에서 나오는 미세플라스틱보다는 적은 문제일 수도 있지만, 주말에 건조기 청소하다가 떠오른 이슈임.
          + 오늘 r/BuyItForLife에 방금 올라온 내용이 있음.> “대형마트에서 파는 일회용 소금/후추 그라인더의 미세플라스틱 문제를 읽고 후, 아주 좋은 금속 메커니즘 그라인더를 샀다”(Reddit의 관련글)
          + 나는 돌로 만든 절구와 공이를 사용하고 적극 추천함
     * 하한인 20,000 ng/kg 기준으로 계산하면 70kg 사람에 대해 DEHP의 일일 한계치는 1,400,000 ng, DEHT는 70,000,000 ng라는 말. 혹시 내가 계산을 잘못한 게 아니라면, RXBars와 Sweetgreen만 먹고 살아도 ‘안전’ 기준보다는 한 단계 낮은 수준이라는 뜻임. 실제로 내 30대 때 이 두 가지만 거의 먹었던 시기가 있음. 이런 표를 보고 플라스틱 소비에 대해 오히려 안도감을 느끼게 될 줄 몰랐음. 혹시 내가 표를 잘못 이해한 것일 수도 있지만, 한도를 10배로 줄여도 여전히 큰 문제는 없을 것 같다는 느낌임.
          + 사이트의 “report” 탭을 보면 어떤 제품이 연방 권고치 이상인지 확인 가능. 거의 대부분의 테스트 제품은 기준 이내임. 즉, 정부에서 안전하다고 생각하는 선만 걱정한다면 특별히 취할 조치는 없어 보임. 하지만 보고서를 보면 연방 기준치가 충분히 엄격한지에 대한 많은 의문점도 제시되고 있음
          + 일일 섭취 권장량을 볼 수 있는 옵션이 있고, 많은 테스트 항목은 권장량 자체가 없음. 그렇다면 이런 성분들은 어느 수준까지 섭취해도 괜찮은 것일지 궁금증이 생김. 어쨌든 100번째 분위의 음식을 하루에 거의 1파운드씩 먹어도 권장 섭취량보다 한참 부족한 수준임. 즉, 권장 수치를 바꿔야 하나 하는 의문도 생김
     * 가장 충격적인 것은 “농장 직송 생우유(유리병)”임. 아주 덜 가공된 제품인데도 여전히 플라스틱이 잔뜩 포함됨. 궁금한 점은 그 우유가 손으로 짠 것인지, 기계로 한 것인지임. 착유기에 들어가는 튜브는 거의 확실히 플라스틱이기 때문임(제품 정보)
          + 유리병에 담겼다고 하더라도 최상의 예시는 아닐 수 있음(관련 기사). 정말 논의하고자 하는 주제로 확장하려면, 소에서 바로 짠 우유가 오히려 더 적합할 것 같으나, 전체 토론에서 벗어나게 됨
          + 가축 사료에도 일정 수준 플라스틱이 들어감
          + 시판되는 우유 중 손으로 짠 것이 있을 확률은 전혀 없음
          + 사실 인간에게 생우유는 필요하지 않음(송아지에게나 좋은 것임). 피하기도 쉬움
          + 우유에는 자연 지방과 유화제가 있어 플라스티사이저 성분을 빨아들이는 데 아주 뛰어남. 이런 성분이 착유기 등에서 우유로 많이 유입될 수 있다는 의미임
     * 음식 속 플라스틱이 이토록 주목받는 것이 이상하게 느껴짐. 해로움에 대한 명확한 근거가 없는 반면, 설탕이나 알코올 섭취 과다, 영수증의 BPA/BPS 등 더 확실한 위험 요소에 비해서 말임(관련 기사). 이처럼 가설적인 이슈가 더 확실히 입증된 건강 문제보다 더 많은 주목을 받는 것을 보면, 건강 위협을 머릿속에서 우선순위로 정하는 게 참 어렵게 느껴짐
          + “해로움에 대한 증거가 없음(?)”이라고 했는데, 목록의 화학물질 대부분은 건강에 해를 끼칠 것이라는 의심을 받고 있고, 다수는 이미 어느 정도 유해성이 입증됨. 예를 들어 DEHP는 내분비 교란, 갑상선 기능 저하, 0.01% 섭취만 해도 혈액-고환 장벽 손상 등(DEHP 위키피디아)
          + 설탕과 알코올은 명확히 표시되어 있고 사용 목적도 뚜렷해서, 소비자가 각자 비용-편익을 비교해 선택함. 미세플라스틱은 피해가 모호하고 섭취량 조절도 거의 불가능함
          + 내가 불안해하는 진짜 포인트는, 미세플라스틱이 혈액-뇌 장벽을 통과한다는 점임(관련 논문)
          + 음식 속 미세플라스틱 이슈가 단지 “Microplastic Free”라는 뱃지/태그 붙여 더 비싼 값에 평범한 상품을 파는 또 하나의 장삿속 수단 아닌가 하는 의심이 듬. 브랜드만 바꾼 물건임에도 가격만 올려 파는 느낌임
          + 설탕이나 알코올은 그걸 먹어서 얻는 장점이라도 있지만, 누구도 플라스틱을 일부러 섭취하고 싶어하지 않음. 플라스틱을 먹는 데 이득이 전혀 없음
     * 1920년대 여러 제품에서 DEHP가 검출됐다는 게 흥미로움. DEHP가 1930년대에 처음 합성된 걸로 알려졌는데 어떻게 된 일인지 궁금함. 예시로 1920년대 코코아 파우더에도 해당 성분이 나옴(제품 링크)
          + 신기하긴 함. 혹시 다른 공정의 부산물로 존재하다가, 1930년대에 이 성분만 따로 분리해 상업적으로 만들기 시작한 걸지도 궁금함
     * 식품 가공 업계 장비 만드는 회사에서 일한 경험을 바탕으로 설명함. 테스트 구역에서 컨베이어벨트를 며칠, 혹은 몇 주 동안 돌리다 보면 아주 미세한 먼지가 컨베이어벨트 주변에 쌓임. 실은 이게 아주 곱게 갈린 POM 플라스틱임. 가끔은 밑에 작은 플라스틱 더미까지 생김. 공장에서는 매일 최소 한 번 고압 세척을 해서 음식에 직접 들어갈 일은 거의 없지만, 그 미세 플라스틱이 결국 바다로 흘러가는 셈임. 미세플라스틱이 음식에 들어가는 경로에 대해 많은 오해가 있다는 생각임. 포장재나 집에서 쓰는 통에서 나오는 것으로 생각할 수 있지만(통이 아주 오래되어 부서지는 게 아니고서야) 실제로는 포장 이전, 이미 식품 자체에 포함되어 있을 가능성이 훨씬 큼
          + 금속 가루보다는 낫다는 생각임
     * PlasticList는 이미 수백 개의 제품을 테스트해 86%에서 플라스틱 화학물질을 발견했음. laboratory.love는 실제로 소비자가 사는 특정 제품에 대한 검사를 크라우드펀딩 방식으로 실시함. PlasticList의 검증 프로세스를 민주화한다고 보면 됨. 검사를 원하는 제품을 선택하면, 샘플 수집 및 검사까지 모두 맡아서 결과를 공개함. 이렇게 하면 기업에 더 청정한 공급망을 압박할 수 있음
          + 혹시 plastic.love와 관련이 있다면 명시적으로 밝혀야 함. 그리고 크라우드펀딩 모델인데, 왜 이메일을 주지 않으면 완료된 결과를 볼 수 없는지 의문임
     * whole foods grass-fed ribeye는 무슨 일인지 궁금함(제품 정보). 소가 혹시 플라스틱 잔디밭을 뜯어먹는 건가
          + 소는 누적하는 특성이 있고, 사료에 플라스틱이 들어가는 경우가 많음
     * 스타벅스 라떼 한 잔에만 수 만 ng의 플라스틱이 들어있음(제품 링크)
     * RXBARs에 작별을 고함. 수년간 주로 먹었던 간식인데, 이제는 플라스틱으로 가득 찬 몸이 됐을 거라는 생각이 듦
"
"https://news.hada.io/topic?id=21621","Snap-QL - Postgres에 AI로 쿼리하는 데스크탑 앱","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Snap-QL - Postgres에 AI로 쿼리하는 데스크탑 앱

     * ""Cursor for Data""
     * PostgreSQL 데이터베이스를 빠르고 안전하게 탐색할 수 있는 로컬 데스크탑 앱
     * AI를 활용해 스키마를 인식한 쿼리를 자동으로 생성할 수 있어, 복잡한 SQL 문을 빠르게 만들 수 있음
     * 모든 인증 정보와 데이터가 PC에만 저장되므로, 외부 서버로 정보가 유출될 걱정이 없음
     * OpenAI API 키를 직접 입력하여 사용
     * 현재는 사전 컴파일된 바이너리는 제공되지 않음
          + 저장소 클론후 npm install로 의존성 설치
          + 플랫폼에 따라 npm run build:mac 또는 npm run build:win 명령 실행
          + ./dist에 생성된 바이너리 파일을 설치하여 사용

   ~ for ~가 확실히 직관적이네요 ㅋㅋㅋ
"
"https://news.hada.io/topic?id=21603","인기 웹사이트를 개인정보 친화적인 대안 프론트엔드로 리디렉트하는 LibRedirect 웹 확장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          인기 웹사이트를 개인정보 친화적인 대안 프론트엔드로 리디렉트하는 LibRedirect 웹 확장

     * LibRedirect는 YouTube, Instagram 등 여러 인기 사이트 접속 시, 개인정보를 보다 잘 보호하는 대체 프론트엔드로 자동으로 연결해주는 웹 확장 프로그램임
     * 이용자는 YouTube, Reddit, TikTok, Twitter 등 주요 플랫폼 사용 시 오픈소스 혹은 탈중앙화된 서비스를 통해 개인정보 노출 가능성의 감소 효과를 기대할 수 있음
     * 각 서비스별로 Invidious, Nitter, Libreddit, ProxiTok 등 다양한 대체 사이트를 선택적으로 경유할 수 있도록 지원함
     * 기존과 똑같은 인터넷 습관을 유지하면서도, 추적 및 광고로부터의 자유와 보안 강화에 실질적인 도움을 제공함
     * 쉬운 사용법과 폭넓은 지원 사이트 덕분에 프라이버시 중시 이용자뿐 아니라 누구나 쉽게 활용 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LibRedirect 개요

     * LibRedirect는 인터넷 사용자들이 YouTube, Instagram, Reddit, TikTok 등 다양한 인기 웹사이트로 접속할 때 개인정보 침해 우려가 적은 오픈소스 프론트엔드로 자동 리디렉션해주는 웹 브라우저 익스텐션임
     * 각 플랫폼에 대응하는 다수의 비공식 대안 서비스를 지원하며, 이 중 일부는 추적, 광고, 쿠키 비활성화 등 프라이버시 보호에 중점을 둔 결과물임

지원 사이트 및 대체 프론트엔드

     * YouTube: Invidious, Materialious, Piped, Piped-Material, Poke, CloudTube, LightTube, Tubo, FreeTube, Yattee, FreeTube PWA, ViewTube, ytify 등을 지원함
     * YT Music: Hyperpipe, Invidious, FreeTube 활용 가능함
     * Twitter: Nitter로 리디렉트
     * ChatGPT: DuckDuckGo AI Chat 이용 가능함
     * Bluesky: Skyview 제공
     * Reddit: Libreddit, Redlib, Teddit, Eddrit, Troddit 등을 통한 접근 지원
     * Tumblr: Priviblur로 대체 가능함
     * Twitch: SafeTwitch, Twineo 경유
     * TikTok: ProxiTok, Offtiktok 사용 가능함
     * Instagram: Proxigram 지원
     * IMDb: libremdb로 접근
     * Bilibili: MikuInvidious 매핑
     * Pixiv: PixivFE, LiteXiv, Vixipy 등 다양한 대안 제공
     * Fandom: BreezeWiki 지원
     * Imgur: rimgo 사용 가능함
     * Pinterest: Binternet, Painterest 선택 가능
     * SoundCloud: Tubo, soundcloak 대체 제공
     * Bandcamp: Tent 이용
     * Tekstowo.pl: TekstoLibre 적용
     * Genius: Dumb, Intellectual
     * Medium: Scribe, LibMedium, Small을 활용
     * Quora: Quetre로 접근
     * GitHub: Gothub 지원
     * GitLab: Laboratory 적용
     * Stack Overflow: AnonymousOverflow로 대체
     * Reuters: Neuters로 리디렉트
     * Snopes: Suds 가능
     * iFunny: UNfunny
     * Tenor: Soprano
     * KnowYourMeme: MeMe
     * Urban Dictionary: Rural Dictionary
     * Goodreads: BiblioReads
     * Wolfram Alpha: WolfreeAlpha
     * Instructables: Structables, Destructables, Indestructables 선택
     * Wikipedia: Wikiless, Wikimore
     * Wayback Machine: Wayback Classic 지원
     * Pastebin: Pasted로 접근 지원
     * 검색 (Search) : SearXNG, SearX, Whoogle, LibreY, 4get 등 다양한 검색 대안 존재
     * 번역 (Translate) : SimplyTranslate, Mozhi, LibreTranslate, Translite 활용 가능
     * 지도 (Maps) : OpenStreetMap 지원
     * 화상회의 (Meet) : Jitsi 제공
     * 파일 전송 (Send Files) : Send 사용
     * 텍스트 붙여넣기 (Paste Text) : PrivateBin, Pasted, Pasty 지원
     * Ultimate Guitar: Freetar, Ultimate Tab으로 연결
     * Baidu Tieba: Rat Aint Tieba 이용 가능
     * Threads: Shoelace 대체 적용
     * DeviantArt: SkunkyArt 제공
     * GeeksforGeeks: NerdsforNerds, Ducks for Ducks 제공
     * Coub: Koub 가능
     * Chefkoch: GoCook 지원

LibRedirect의 의의

     * 사용자는 기존 웹사이트 주소 입력만으로 자동으로 개인정보 중심 대안 프론트엔드에 접속할 수 있으므로 추적 방지, 광고 최소화 환경을 손쉽게 경험 가능함
     * 다양한 오픈소스 프로젝트와 연동되기 때문에, 오픈소스 생태계 및 프라이버시 강화에 기여함
     * 특별한 기술적 설정이나 추가적인 학습 없이 누구나 손쉽게 프라이버시 우선 인터넷 환경 구축 가능함

추가 사항

     * 자세한 정보 및 업데이트는 Mastodon 공식 페이지에서 확인 가능함

        Hacker News 의견

     * 전반적으로 이 방식들이 작동하지만, 꽤 빠르게 죽어버리는 인스턴스가 문제라고 생각함. 예전에는 호의로 리디렉션만 제공하는 홈브류 “허브”들이 인기 사이트와 서비스들에 많이 있었지만, 지금은 제대로 작동하는걸 찾기 힘듦. 대형 사이트들이 점점 차단하거나 레이트 리미트하면서 맞서 싸우는 중인 느낌. Privacy Redirect 확장 프로그램이 이런 아이디어를 처음 소개했던 것으로 기억함. 처음에는 잘 됐지만, 결국 악의적인 사용자가 위험한 사이트로 유저들을 리디렉션할 수 있다는 걸 눈치챘음
     * 관련된 내용 같아서 공유하고 싶음. 여러 서비스의 프라이버시 중심 프론트엔드 모음집을 정리한 “awesome” 리스트를 직접 만들었음. 업데이트가 좀 오래 됐지만 여전히 쓸만하다고 생각함
       https://sr.ht/~jamesponddotco/awesome-privacy-front-ends/
          + Instagram 정말로 안됨? 모든 프론트엔드가 꺼져 있고, 로컬에서도 동작하지 않는 상황
     * 최근에 안드로이드에서 OS 전반에 커스텀 리디렉트를 설정할 수 있는 앱을 찾았음
       https://github.com/TrianguloY/URLCheck
       설정이 조금 까다롭긴 한데 꽤 만족하면서 사용 중. 단순한 대체 프론트엔드 리디렉션을 넘어서 URL 파라미터 제거, 도메인 블랙리스트 체킹, 특정 패턴에 맞는 링크는 네이티브 앱으로 열기까지 다양한 기능 지원
          + 이 앱을 처음 발견했을 때 정말 기쁨. 진짜 유용함. 단순 리디렉션을 넘어서 트래킹 요소 제거, 링크 언쇼트닝, 특정 도메인마다 어떤 앱으로 열지 기억 등 여러 기능 있음. 안드로이드의 공유 메뉴가 워낙 불편해서 이런 앱이 거의 필수라는 생각
     * Redirector[1] 사용하면 자기만의 리디렉션을 쉽게 설정할 수 있음. 나는 그게 더 낫다고 생각
       https://addons.mozilla.org/en-US/firefox/addon/redirector/
     * 웹 익스텐션은 쓸데없이 보안 리스크라는 생각. 유저스크립트로 충분히 해결 가능
       이전 시도 중 하나: https://news.ycombinator.com/item?id=35229211
       확장 가능하게 만들었고, 규칙이랑 도메인 소스를 밀착시켰었음. 그런데 Edge가 유저스크립트를 몽땅 잃어버려서 다 날라감
          + 유저스크립트는 권한이 너무 넓음. 예를 들어 youtube.com에 제한된 유저스크립트도 저장된 Google pay 카드로 결제하는 것도 가능. 대부분 유저스크립트는 너무 길어서 평범한 사용자는 1만줄짜리 minify된 라이브러리 속 악성 코드를 알아채기 어려움
          + 익스텐션은 50개 이상의 서비스를 지원하는데 당신의 스크립트는 1개 뿐임. 그렇다면 모든 사용자가 직접 제대로 수행할 방법을 알아내고, 스크립트로 익스텐션을 하나하나 복제하라고 주장하는 건지? 차라리 그 시간 동안 익스텐션 코드를 읽고, 프라이빗 카피를 사용하는 것이 효율적이라고 생각
          + 나는 직접 redirector를 써서 해결하는 게 더 낫다고 느낌. 지금까지 아주 만족 https://einaregilsson.com/redirector/
          + 현실적이지 않다는 생각. 그보다는 익스텐션 권한을 엄격히 하거나, 다른 브라우저 프로필을 쓰거나, 더 나아가 QubesOS에서 임시 브라우저 VM을 사용하는 게 오히려 더 좋다는 판단
          + 유저스크립트가 페이지 로드 전에 실행될 수 있음? 내가 알기로는 불가능. 결국 브라우저가 이중으로 요청을 보냄
     * 프라이버시 자체도 물론 중요하지만, 오히려 써드파티 프론트엔드의 가장 큰 장점이 노트북 성능이 바닥을 치지 않는다는 점. 진짜 단순히 글씨만 읽고 싶을 때 내 구린 노트북에서 리소스 다 잡아먹지 않음
       레딧은 그중에서 최악도 아님. 언제부터 이렇게 화면에 글자 띄우는 게 무거워진건지 궁금
          + 일부 프론트엔드가 JavaScript 없이 또는 최소한으로 작동한다는 점이 특히 흥미로움. 어차피 필요 없는 JS였고, 결국 프라이버시 침해와 사용자 불편함 유발이 주 목적 아니었나 하는 생각
          + old reddit 써봤는지 궁금
     * 누군가 “프라이버시 친화적” 프론트엔드를 만들었다고 해놓고, 사실은 브라우징 데이터를 슬쩍 수집하는 게 아닌지? 아마 그런 사람 분명 있을 거라는 생각
          + 내 지인 중 한 명이 프라이버시 대체 프론트엔드를 만들고 있는데, 서비스 향상을 명분으로 유저 데이터를 상당히 많이 들여다 봄 (명시적 공개 없이 전체 이름이 포함됨). 서비스 품질 향상에 실제로 큰 도움이 되긴 하지만, 이런 데이터 수집은 투명하게 해야 한다고 여러 번 강조했지만 설득이 안 됨. 소스코드 꼼꼼히 보면 알 수 있음
          + 모든 프론트엔드가 사실상 프록시 역할이기 때문에, 일정 부분 신뢰가 필요. 다만 소규모 개인이 운영하면 분석할 리소스도 부족하고, 데이터를 쌓고 분석할 의지가 없을 수도 있다고 생각
          + 누가 그렇게 안 한다는 걸 어떻게 증명할 수 있음? 이런 식이면 아무 생각이나 다 믿을 수 있음. 신뢰는 결국 만드는 사람의 의도에 기대하게 됨. 프론트엔드가 갑자기 로그인이나 트래킹 쿠키 요청하면 경계해야 한다고 생각. 원래 불필요한 정보를 요구하는 웹사이트는 모두 조심해 왔으니, 이런 원칙을 프론트엔드에도 적용하면 무리 없다는 관점
          + 그럴 가능성(허니팟 목적)이 실제로 존재한다고 봄
          + 사용하지 않음이 답이라고 생각. 모든 걸 의심하는 건 그만. 프라이버시는 흑백논리가 아니고, 모든 사람이 나쁜 의도를 가지는 게 아니라는 점. 프라이버시를 진심으로 중요하게 여기면서 자발적으로 기여하는 사람들도 상당수라는 점 언급. 그리고 프라이빗 프론트엔드들은 굉장히 빠르게 로딩되어 시간도 아낄 수 있다는 장점
     * 광고 안보고 싶고, 트윗이나 간단 영상 하나 보려고 10MB JS를 매번 불러오고 싶지 않음. piped나 nitter로 리디렉션시키는 게 합리적. 다만 더 바라는 기능이 있다면 셀프호스팅이나, 최소한 신뢰할 만한 인스턴스를 돌아가면서(로테이션) 사용해주면 좋겠음. 지금 구조는 의도 일부만 달성. 누가 뭘 운영하는지 일일이 챙길 여유가 없음. 진지하게 쓴다면 현재는 리디렉션 대상 인스턴스가 항상 빠르고 안전하다는 가정임. 어떤 인스턴스는 정말 느리고, 어떤 건 갑자기 사라지고, 일부는 로그를 모을 가능성도 있다고 봄. 실제로 지금 리스트 중에 절반 정도는 죽어 있음
     * 매일 쓰는 도구에서 텔레메트리(사용 분석) 빼내는 행위가 매번 컨트롤을 되찾는 것 같은 기분
     * 좋은 YouTube 프론트엔드(셀프호스트 지원 포함) 있을지 궁금함. 여러 개 시도해봤지만 대부분 다운되어 있을 때가 훨씬 더 많음
          + https://grayjay.app/ 이 어플도 괜찮을 듯. 로컬에서 동작. 프라이버시 관련해서 얼마나 안전한지는 잘 모르겠지만, 수집하는 정보가 적다고 주장하는 중임
          + peertube https://joinpeertube.org/en_US 살펴봤는지 궁금
"
"https://news.hada.io/topic?id=21674","Let's Encrypt, IP 주소 인증서 발급 준비 중","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Let's Encrypt, IP 주소 인증서 발급 준비 중

     * Let's Encrypt가 곧 IP 주소 SAN을 포함한 인증서 발급을 지원할 예정으로, 초기에는 6일 만료의 단기(shortlived) 프로필에 한해 제공되고, 일정 기간은 화이트리스트 방식으로 제한 운영됨
     * 정식 공개 전까지는 구체적 일정이나 신청 안내 없이 내부 테스트와 준비가 진행 중
     * 스테이징 환경에서 IPv6 주소가 포함된 샘플 인증서와 이를 적용한 사이트가 공개되었으며, 커뮤니티에 이상 사항이나 피드백 공유 요청
     * Firefox에서 IP SAN 표시에 관련된 버그(BZ #1973855)가 발견되어 테스트가 이어지고 있음
     * DNS SAN과 IP SAN을 혼합한 케이스에서 혼동의 여지가 있음을 실제 테스트 예시로 확인, TLD와 IPv6 주소 표기가 유사할 수 있음을 시연
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Let's Encrypt, Getting ready to issue IP address certificates

  IP 주소 SAN 지원 준비 상황

     * Let's Encrypt는 곧 생산 환경에서 IP 주소 SAN이 포함된 인증서 발급을 지원할 계획임
     * 해당 인증서는 6일 유효기간의 shortlived 프로필에서만 발급되며, 일정 기간은 허용 리스트(allowlist) 방식으로 제한 운영됨
     * 아직 정식 출시 일정 및 허용 리스트 신청 방법은 미정, 내부 준비가 추가로 필요함

  테스트 및 샘플 인증서

     * 스테이징 환경에서 IP SAN을 포함한 샘플 인증서와 실제 적용 사이트(IPv6 주소) 예시가 공개됨
     * 커뮤니티 사용자들에게 이상 사항이나 흥미로운 점, 문제점 발견 시 공유 요청

  IP SAN과 DNS SAN 혼합 사례

     * 테스트 과정에서 DNS SAN과 IP SAN이 동시에 포함된 인증서 발급 가능성 및 표기 혼동 사례 시연
     * .cafe 등 특정 TLD와 IPv6 주소 표기가 유사할 수 있어 혼동의 여지가 있음
     * Firefox에서 IP 주소 SAN 표시 관련 버그(BZ #1973855) 도 확인됨

  요약

     * Let's Encrypt의 IP 주소 인증서 지원은 화이트리스트 및 단기 인증서로 제한적으로 먼저 적용
     * 실제 서비스 적용 전까지 다양한 케이스 테스트와 커뮤니티 피드백을 통해 안정성·호환성 점검 예정
     * DNS 이름과 IP 주소의 SAN 혼용에 따른 브라우저 표시 이슈 등도 함께 논의되고 있음

        Hacker News 의견

     * IP 주소용 인증서도 유용하다고 생각함
       하지만 Let‘s Encrypt가 S/MIME 인증서를 지원한다면 훨씬 더 큰 영향을 줄 수 있다고 생각함
       수년 전부터 이메일 암호화 관련해서 코믹한 상황이 발생 중임
       이제 대부분의 주요 이메일 클라이언트에서 S/MIME 암호화를 바로 지원하지만, 웹과 마찬가지로 부드러운 사용자 경험을 위해서는 CA에서 인증서를 받아야 함
       신뢰할 수 있고 무료이며 1년 이상 유효한 S/MIME 인증서를 제공하는 CA는 이제 모두 사라짐
       결과적으로 개인 사용자는 이메일 암호화를 전혀 사용하지 않는 상황
       PGP는 너무 불편해서 기술 매니아 외에 거의 사용하지 않음
       이제 우리 이메일도 암호화해야 한다고 생각함
       참고로, CA가 비밀키를 대신 생성해 주면 그 인증서는 신뢰할 수 없다고 봄
       또 S/MIME은 예전 메일 복호화를 위해 예전 인증서를 계속 보관해야 하므로, 자주 바뀌는 인증서는 비실용적임
          + S/MIME에서 예전 메일을 복호화하려면 구 인증서가 필요하다는 이야기에 대해, 복호화 키 자체는 새 인증서도 기존 키를 써서 만들 수 있기 때문에(예: certbot의 --reuse-key 옵션) 꼭 자주 바꾸지 않아도 된다고 봄
            다만 이 방식이 좋은 방법인지는 별개의 문제
            진짜로 필요한 것은 ACME 스타일의 자동 인증서 발급 자동화라고 생각
            지금은 인증서 갱신 과정이 불편해서 거의 사용하지 않음
          + PGP는 이제 WKD(Web Key Directory)링크라는 방식이 있어서 이메일에 TLS의 신뢰망을 적용할 수 있음
            TLS 인증서는 S/MIME 인증서보다 훨씬 받기 쉬운 상황
            신원관리를 제3자가 담당하는 것도 도움이 될 수 있지만, 대부분의 사람들은 그런 신원관리가 잘 맞지 않는 회사에서 일하는 경우가 많음
            만약 회사라면 신원관리를 사내에서 시행하는 것이 더 적합하다고 생각
            최근 있었던 Signalgate 1.0 해프닝링크는 종단 간 암호화의 신원관리 실패라는 점에서 참 배울 것이 많았다고 봄
            이런 점이 S/MIME 인증서나 WKD가 실제로 쓸만한 시스템으로 도입된다면 정부에서도 유용할 수 있겠다는 생각
          + 개인적으로 S/MIME 기반 이메일 암호화에 대한 전망을 좋게는 보지만, 실현 가능성은 낮다고 느낌
            일반 사용자는 비공개키 관리조차 어려워하는 경우가 많고, 이메일 비밀번호조차도 제대로 관리하기 힘들어하는 게 현실
            결국 각 도메인별로 중앙에서 인증서 발급이 필요하거나, 아니면 사용자가 인증서를 받지 못하는 상황이 발생하는 한편, 사이버 범죄자들이 S/MIME 서명 메일을 보내는 문제가 생김
            앞으로 패스키(passkey) 사용이 일반화되고 세대 교체가 이뤄지면 그때가서 키쌍을 직접 다루도록 하는 게 맞는다고 생각
          + 이메일 암호화는 그냥 사라져야 한다는 의견
            참고: Stop using encrypted email
          + S/MIME 암호화에 대해 잘 모르겠어서 궁금한 점이 있음
            왜 같은 프로토콜로 예전 메일을 복호화해야 하는지 궁금
            개인적으로 인증서는 전송 구간(transport)을 위한 용도라고 생각했고, 실제 저장시에는 서버나 호스트의 보관용 암호화가 따로 있을 테니, 전송시에는 단기 인증서, 저장시에는 원하는 방식 암호화 등으로 나눠 쓰는 것이 논리적인데 뭔가 놓치고 있는 부분이 있는지 궁금
     * 모든 주소 관리 기관(예: ISP, 클라우드 제공업체)도 이 흐름에 동참하는 것인지 궁금
       이들은 IP를 아주 빠르게 순환시키는 경우가 있는데, 6일보다 빠르게 바뀌기도 함
       클라우드 제공자도 IP 분석 전 쿨다운 기간을 두거나 해당 IP에 관련된 인증서를 조회 및 폐기한다면 이해가 가지만, 그렇지 않으면 사용자 책임으로 호스트 헤더를 검증하고, 원하지 않는 IP 기반 연결을 거부하거나, 레거시 인증서가 완전히 없어질 때까지 기다려야 하는 복잡성 발생
       클라우드 제공업체별로 IP 인증서를 얼마나, 얼마에 받을 수 있는지도 궁금함
          + 사실 이런 문제는 클라우드 제공업체의 커스텀/바니티 도메인 제공과 별반 다르지 않다고 생각
            예를 들어 Azure에서는 myapp.westus.cloudapp.azure.com 같은 DNS를 VM에 연결할 수 있는데, CA에서 누구나 저 도메인에 인증서를 발급받을 수 있음 참고
            이런 경우도 별도의 쿨다운 없이 VM이 소멸하면 다른 사람이 바로 그 도메인을 가져갈 수 있음
          + HTTPS 인증서는 도메인 만료 하루 전에 90일짜리로 갱신 가능하고, CA에서는 결제카드 한도도 알 수 없음
            IP 인증서 쓸 사람들은 대부분 금방 IP를 폐기하고 떠나는 사용자와는 다를 것
            가장 유용한 사례는 아주 특이한 레거시 소프트웨어나, Cloudflare 처럼 공유 IP 없이 ECH(Encrypted Client Hello)나 ESNI(Encrypted SNI) 지원이 필요한 경우
            첫 번째 사례(레거시)는 IP를 쉽게 포기하지 않을 테고, 두 번째(ECH/ESNI)는 실제 도메인에 접속이 성사되기 어렵다고 생각
          + ISPs가 동참해야 하느냐는 질문엔 오히려 거꾸로 간다고 봄
            ISPs의 역할이 TLS 표준에 맞는 방식으로 IP를 할당하는 게 아니고, TLS 인증 제공자가 생태계의 신원(도메인, IP 등…) 연결 방식에 맞춰 ""신원 검증"" 역할을 해야 한다고 생각
            이번에 어떻게 접근할지 자세히 나와 있진 않지만, 내 직감엔 LetsEncrypt가 ASNs(자율 시스템 번호) 기준 장기간 IP 이관이 이뤄지는 리스트를 만들어서(공개 DB처럼 Mozilla Public Suffix List를 공동 유지) 오직 그 리스트 내의 IP에만 인증서를 발급하고, 다른 ASN으로 이전하면 인증서 무효화 관리 등을 할 듯
            다른 ACME 발급기관과도 공유하는 방식을 기대함
          + 다양한 클라우드에서 IP 인증서를 얼마에, 몇 개나 받을 수 있는지 궁금하다면, 향후 와일드카드 인증서 지원이 생길 지도 궁금함
          + 내 상황에선 단 하루만 IP 인증서가 있어도 https URL 테스트를 할 수 있으니 아주 반가운 변화라고 생각
     * 기술적으로는 동작 원리를 알겠는데, 실제 의도된 사용처가 무엇인지 궁금함
          + ECH(Encrypted Client Hello)나 ESNI(Encrypted SNI) 지원만으로도 큰 의미가 있다고 생각
            ESNI 초기에는 Cloudflare 같은 대형 https 프록시만 적용할 수 있어서 IP 인증서 도입이 꿈 같은 아이디어로 여겨졌는데, 이제 모든 서버가 ESNI/ECH에 참여할 수 있음
            클라이언트들이 모든 서버에 IP 인증서가 있다고 가정하고 ESNI를 시도하기 시작하면, 인터넷 전역 프라이버시 향상에 큰 도움이 될 것으로 기대함
          + 여러 답변에서 좋은 사례가 나왔지만, NTS(Network Time Security)도 언급할 만함
            IP 인증서를 못 받으면 NTS에도 DNS가 필요해서, DNS가 다운되면 인증된 시간 동기화가 아예 불가능해짐
            DNSSEC는 올바른 시각 없으면 검증에 실패하는데, DNS+NTS 의존이면 복구 불가
            IP가 포함된 인증서로 DNS 필요 없이 인증된 시각 배포가 가능하면 이런 문제를 해결할 수 있음
          + DNS가 상당히 바뀌는 과정에서도 유효한 인증서를 유지해야 할 경우, 예를 들어 대시보드 접근성 확보라든지, 예전 자동화가 DNS 오류로 멈추지 않게 하기 위해서 쓸 수 있음
            다른 사례로는 DNS 필요 없이, 더욱 단순하게 테스트 환경을 바로 구축하거나 Cockpit 등 외부 노출을 빠르게 하는 경우도 생각 가능
            사실상 상상력이 허락하는 다양한 활용처가 생김
          + authdns(인증 DNS 서버) 대상으로 '기회주의적(opportunistic)' DoTLS(TLS 기반 DNS 쿼리)를 할 때도 흥미로운 방식이 될 수 있음
            공개 IP를 SAN(Subject Alternative Name)으로 포함하는 인증서로 인증 DNS 서버가 DoTLS 포트에서 서비스할 수 있음
            지금도 호스트네임으로 가능하지만 하나의 퍼블릭 IP에 여러 다양한 네임이 있을 수 있어서, IP 기반 SAN이 더 명확히 묶어주게 됨
          + 호스트네임 대신 프로젝트에 바로 IP 연결하는 등 취미나 일회성 목적으로 도메인 없이 쓰기를 원하는 사람들에게 적합하다고 생각
     * 왜 인터넷 지역 등록기관(RIR)이나 로컬 등록기관(LIR)에서 인증서를 발급하지 않는지 궁금
       왜 제3자가 RIR, LIR 등록자를 검증해야 하는지, 도메인 등록자에겐 일이 너무 많다는 건가
       이유가 똑같은지 의문
     * TLS를 새롭게 악용할 수 있는 구멍이 늘어난 것 같다는 생각
       이전까지는 소유하지 않은 도메인에 대한 인증서를 발급한다는 이슈였는데, 이제는 소유하지 않은 IP에 대한 인증서가 가능할 수도 있음
       블랙햇 해커들이 텔레그램에서 신나게 떠들 듯
          + 사실 IP 인증서는 예전부터 존재했고, 바뀐 점은 이제 Let's Encrypt에서도 제공한다는 것뿐임
     * 이로 인해 로컬 라우터(예: 192.168.0.1) 관리 페이지에 self-signed 오류 없이 https 접속이 가능한지 궁금함
          + 불가능하지만, 가능해질지도 모름
            로컬 주소는 범용적으로 할당하는 게 아니고, 글로벌 라우팅도 안 되기 때문에 본질적으로 검증할 방법이 없음
            하지만 라우터 제조사가 의지가 있다면, CG-NAT 뒤에 있지 않고 퍼블릭 IPv4 주소가 할당된 장비에서라면, 퍼블릭 주소로 인증서를 요청하는 것은 가능함
            IPv6는 대부분 글로벌 라우팅이 되니 더욱 쉽다고 봄
          + 불가능하고, 그렇게 되어서도 안 됨
            실제로는 프록시와 실제 도메인, 진짜 인증서를 이용하고 DNS 리라이트 기능을 조합하면 충분히 가능
            예를 들어 nginx manager UI와 adguard를 DNS로 사용
            라우터의 DNS를 adguard로 한정, 내 도메인은 proxy로 리라이트
            도메인을 등록하고 진짜 인증서 발급받으면 해결
            나도 모든 로컬 서비스를 https로 쓰고 있음
          + 사설 IP에는 인증서가 발급되지 않음
            동일한 사설 IP가 다른 네트워크의 전혀 다른 장비로 매번 바뀔 수 있어, 독점 소유권이 보장되지 않기 때문임
          + 오히려 반대임
            공인 IP가 아닌 경우 유효한 인증서 발급 불가
            이미 도메인 인증서를 받아 해당 도메인을 192.168.0.1로 포워딩하는 방법이 있음
          + 인증서를 받으려면 해당 IP(공인 IP)를 실제로 소유해야 함
     * 내부 IPv4(192.168.x.x, 172.16.x.x, 10.x.x.x)용 인증서도 가능할지, 혹은 브라우저에서 이런 주소를 무시하는 게 맞는지, 만약 그렇지 않으면 내부망용 와일드카드 인증서라도 발급받을 수 있는지 궁금함
          + 공용(public) CA가 발급하는 인증서 맥락에서 위 질문은 의미가 맞지 않는다고 생각
            도메인과 달리 10.10.10.10 같은 사설 IP는 수많은 사람이 동시에 ""소유"" 중
            내부 개발용이라면 mkcert 같은 툴이 있고, 회사 내부 공유 자원이라면 도메인 기반 TLS 인증서가 더 간편함
          + 만약 장비의 개인키가 절대로 유출되지 않는다는 걸 증명할 수 있다면, 기존 CA로도 시도 가능함
            하지만 내부 주소 인증서 관련해서는 누군가 기기를 사서 개인키를 얻어내면 즉시 인증서 키 폐기가 문제됨
            그래서 신뢰할 수 있는 기기라면 수동으로 인증서를 임포트해서 오류 없이 접속하거나, 여러 기기가 있다면 자체 CA를 구축해서 인증서 배포하는 것이 현실적임
            오픈소스 ACME 서버로 Let's Encrypt와 동일 방식의 발급 자동화가 내부에서도 가능
          + DNS를 퍼블릭 서버로 우선 지정해 인증서 발급받고, 그 후에는 DNS를 내부 192.168… 주소로 내려받아 인증서와 키를 복사하는 방식
            유의할 점은 일부 라우터가 내부 주소로 포워딩되는 DNS 응답을 블랙홀링할 수 있으니 사전 테스트 필요
          + 브라우저가 프라이빗 네트워크를 무시하지 않아야 한다고 봄
            내 경우엔 프라이빗 네트워크란 내 로컬 라우터에 불과하지만, 남들에겐 전세계를 아우르는 망일 수도 있음
     * 예시 인증서에는 subject 필드가 없는 것이 흥미로움
       IP로 요청하고 SAN에 DNS가 기재되어 있어서 그런지 궁금함
          + Let's Encrypt팀에 따르면, 앞으로 subject common name 대신 subject alternative names 방식만 사용할 계획임
            6일짜리 단기 인증서 프로필에서 이미 적용됐고, ""클래식"" 90일짜리 프로필에는 아직 적용되지 않았다고 함
     * CVE-2010-3170 취약점이 다시 부각될 시점인지 농담 삼아 언급
          + 직접 X.509 검증 로직을 구현한다면 그런 취약점이 남아 있을 수 있지만, 실제로 이걸 노리려면 잘못 동작하는 CA가 해당 인증서를 발급해줘야 하므로 가능성은 낮다고 생각
     * 아쉽게도 IP 주소용 인증서는 DNS 챌린지 방식으로 발급이 불가능
       하지만 왜 그런지 이해는 됨
"
"https://news.hada.io/topic?id=21602","Git Notes: Git의 가장 멋지고 미움받는 기능 (2022)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Git Notes: Git의 가장 멋지고 미움받는 기능 (2022)

     * Git notes는 커밋 등에 메타데이터를 덧붙일 수 있는 강력한 도구임
     * 이 기능은 커밋 메시지를 수정할 수 없을 때 보충 정보를 저장할 수 있도록 해줌
     * 코드 리뷰 이력이나 테스트 결과 등 다양한 정보를 저장하는 데 활용 가능함
     * 분산 코드 리뷰 시스템까지 구현할 수 있으나, 사용성과 인지도가 낮음
     * GitHub에서 비활성화되는 등 지원이 부족해 일상적 채택이 저조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Git notes란 무엇인가

     * Git notes는 git에서 추적되는 어떤 오브젝트(커밋, 블롭, 트리)에도 메타데이터를 덧붙일 수 있도록 하는 기능임
     * 일반적으로 한 번 기록된 커밋 메시지는 변경할 수 없으나, git notes를 사용하면 커밋 본문을 변경하지 않고도 새로운 정보를 추가할 수 있음
     * 예를 들어 아래와 같이 커밋에 notes를 추가 가능함
          + git notes add -m 'Acked-by: <이메일>'
     * notes는 git log에서 별도의 섹션으로 함께 표시됨

실제 활용 사례

     * git 자체 프로젝트에서도 git notes를 활용하여 커밋과 메일링리스트 내 토론 스레드를 연결하고 있음
     * notes의 실제 활용 예시로는 다음과 같은 항목이 있음
          + 커밋 또는 브랜치별 작업 시간 추적
          + 코드 리뷰 및 테스트 결과 로그 저장
          + 완전히 분산된 코드 리뷰 시스템 설계

코드 리뷰와 테스트 결과 저장

     * 코드 포지나 리뷰 시스템들이 코드 리뷰 메타데이터를 오프라인, 즉 git 자체에 저장하도록 지원하는 사례가 늘어나고 있음
     * Gerrit의 reviewnotes 플러그인은 git log에서 리뷰 수행자와 테스트 실행 내역을 notes로 함께 보여줌
          + 사용자는 브라우저를 별도로 열지 않고도 로컬에서 코드 검토 내역을 확인 가능함

분산 코드 리뷰 시스템

     * Google에서 개발된 git-appraise처럼, git notes를 이용해 분산 코드 리뷰를 실현한 사례가 존재함
     * git-appraise는 코드 리뷰 요청, 변경 사항에 대한 코멘트, 리뷰 후 머지 등 모든 절차를 git notes에 기록하고, 온라인 코드 호스팅 서비스와 무관하게 독립적으로 동작함
     * 로컬 환경에서 모든 협업이 가능하며, 웹 인터페이스까지 제공함

낮은 사용성과 인지도

     * Git notes는 인터페이스가 불편하고 일반 사용자에겐 직관적이지 않아 거의 사용되지 않고 있음
     * GitHub는 2014년 이후 커밋 notes 노출을 중단함에 따라 더욱 사용 기회가 줄어듦
     * 커밋에 대한 notes 관리만큼은 git 설정으로 좀 더 쉽게 할 수 있으나, 블롭이나 트리 오브젝트에 notes를 붙이려면 git의 내부 구조(plumbing)에 대한 추가적인 이해가 필요함
     * 그 결과로 git notes는 여전히 매니아적이고 인지도가 매우 낮은 기능으로 남아 있음

오픈 포지(Forge) 독립성

     * git 자체는 분산 버전 관리와 코드 리뷰를 지원할 수 있으나, 리뷰 내역 등 많은 부가 가치는 GitHub와 같은 호스팅 서비스에 종속되는 경향이 강함
     * Git notes 기능을 활용하면 코드 자체의 변천사뿐 아니라 프로젝트 전체의 히스토리까지 분산 방식으로 저장 및 배포하는 길이 열림
     * 이는 온전한 분산 개발 생태계와 기록 보존에 중요한 의미가 있음

        Hacker News 의견

     * Git 트레일러라는 덜 알려진 기능이 존재 정보 공유, 트레일러는 커밋 생성 시 key-value 형태의 메타데이터 삽입 가능, Gerrit에서 Change-Id를 부착하는 데 활용. 관련 기사 링크
          + PostgreSQL에 COMMENT라는 비슷한 기능이 있음 정보 공유, COMMENT는 데이터베이스 객체에 텍스트를 추가할 수 있음, PostgreSQL이 key-value 형태의 구조화된 메타데이터 편집 기능도 제공하길 희망. 관련 문서 링크
          + 오픈소스 업스트림 작업 시, 단위 테스트가 완료된 커밋을 표시하려고 git notes 사용 경험 공유, git rebase -i로 브랜치를 다듬을 때 유용했음, 하지만 트레일러가 현재는 더 나은 방법처럼 보임, Change-Id 같은 기능이 git 자체에 포함되면 툴들이 이를 더 잘 이해할 수 있을 것 같음, 커밋 메시지로 커밋을 구분하는 것은 미묘하게 취약함, 커밋 id는 유일성은 좋지만 동일한 변경을 다른 커밋 위로 옮기면 식별자로서 유용성이 떨어짐, 수정(수정 내용): 트레일러는 커밋의 일부라 notes를 완전히 대체하진 못할 것
          + 최근 GitHub이 [skip ci] 대신 별도 트레일러 활용을 알게 됨, 아마 커밋 메시지에서 쉽게 분리해내기 위해서인 듯함, GitHub에서 왜 마지막 트레일러만 요구하는지 확실하지 않음, 아마 정규표현식 처리 때문일 것 추측. 관련 문서 링크
          + 트레일러 기능은 처음 알게 됨, Conventional Commits 팬으로서, 트레일러가 이런 메타데이터 추가에 더 나을 것 같음, 커밋 메시지에 수동 추가와 --trailer 플래그 사용하는 것이 기능적으로 동일한지 궁금
          + 자연스럽게 이슈 트래킹 시스템과 연동하고 싶지만, 트레일러 같은 곳에 그걸 넣기는 이슈 트래커에서 너무 멀리 떨어져 있어서 비효율, 이슈 트래커는 트렌드에 따라가고 여러 기능이 늦게 추가되는 문제, 티켓명에서 파생된 브랜치명을 PR 제목으로 무조건 써야 하는 규칙이 불편, 다른 메타데이터로 커밋과 이슈를 연결해서 PR 제목을 더 의미있게 만들고 싶음, 쉽게 바꿀 수 있는 문제가 아니라서 인터넷에서나 하소연하는 주제
     * Forgejo가 올해 1월 출시된 10버전부터 트레일러 지원 시작 정보 공유 릴리즈 노트 풀 리퀘스트
     * git notes와 히스토리 재작성(amend, rebase 등) 기능의 상호작용에 대한 궁금증, notes가 커밋/트리/블랍 ID에 연결되어 있어서 새로운 커밋으로 교체될 때 notes가 복사되는지, 혹은 사라지는지 궁금, interactive rebase로 여러 커밋을 합칠 때는 어떻게 되는지 궁금, notes가 파일/디렉터리의 ""내용""에 붙는다는 점에서 기대와 다르다는 인식 공유, 예를 들어 ""Hello world!"" 문자열 blob에 note를 붙이면 같은 내용을 가진 모든 파일에 note가 붙는 것인지, 파일이 변경되면 notes를 잃는지 궁금
          + git rebase 등에서 notes 복사는 기본적으로 복사되도록 설정 가능, git-config(1)에서 notes.rewrite 옵션 참고
     * 현재 직장에서 git notes 적극적으로 사용 중, 내부 코드 리뷰 기록을 커밋 메시지를 복잡하게 하거나 모든 변경에 PR을 만들지 않고도 관리 목표, 각 커밋에 어떤 티켓과 매핑되고 인프라 제약, 수정의 경우 인시던트 스레드 링크 등 브랜치에 컨텍스트를 남김, 이 정보를 repo에 저장해서 slack이나 jira 검색 없이도 한 줄이 왜 바뀌었는지 파악 쉽게 가능, 대규모로 활용하면 platform UI의 필요성을 확실히 줄일 수 있음, reproducibility가 빌드 뿐만 아니라 ‘intent’에도 필요하다는 의견
          + 이런 정보가 커밋 메시지에 들어가는 편이 낫지 않은지, 아니면 “이 커밋이 버그 #123을 만들었다” 같은 미래 방향까지 링크하려는 목적이 있는지 궁금
     * git notes는 커밋 이후에 추가 정보가 필요할 때만 진짜 유용, Acked-By, 메일링리스트 토론 링크 같은 예시는 커밋 당시 이미 알고 있는 정보이므로 커밋 메시지에 추가하면 됨, 오히려 git note의 진짜 장점은 나중에 되돌린 커밋 등에 부가적 설명을 붙일 때라고 생각
          + 흔히 commit 메시지가 버그를 고쳤다 자랑하지만 사실은 안 고쳤을 때가 있음, 그로인해 꼬리에 꼬리를 무는 버그들이 발생하고, 동료들이 최초 작성 의도를 무시하고 대충 넘어가는 경우가 많았음, 화난 고객과 버그 재발 경험 후에는 bug fix에 신중해질 필요성 느낌, 때로는 여러 버그를 한 번에 고치거나, 리팩터로 성능 개선이나 새로운 기능의 기회도 열림
          + Acked-By, 리뷰, 토론은 커밋 후에 이어질 수밖에 없음, 이미 커밋되는 시점엔 알지 못함, 되돌린(commit revert) 사례는 오히려 커밋 메시지에 추가하는 게 쓸모 있음, blame 기능이 가장 최근 변경을 보여주기 때문에 revert된 커밋도 자연히 추적 기능 제공
          + 커밋 이후에도 토론이 계속되는 경우 많음
     * 코드 에이전트에 추가 컨텍스트 제공 목적으로 notes 기능을 탐구해볼 만한 흥미로운 포인트라고 생각
     * 이건 지식 부족이 아닌 UI 문제, GitHub UI가 notes를 잘 보여주면 바로 더 많이 쓰이게 될 것
          + GitHub에서 notes 지원되길 바람
     * git에는 bisect, pickaxe, reflog, range-diff, archive, annotated tags 등 ‘가장 멋지고 사랑받지 못하는 기능’이 많은데, 대부분 사람들이 google drive처럼만 여겨서 잘 안 씀
          + git notes는 어차피 별도의 프로젝트 관리 도구(예: Jira)로 feature, roadmap 등 비기술적 맥락 추적하므로 중복 느낌, 유닉스 철학에 따라 각자 역할에 집중해야 함
          + pickaxe는 처음 들어본 기능이라 예상 밖, 너무 자신만만해지면 안될 것 같음
          + 이런 장식적인 기능들이 실제로는 그렇게 유용하지 않은 경우가 많음, 본질을 둘러싼 잡기술이라는 시각
     * git notes는 정말 ‘멋진’ unloved feature가 아니라 특정한 아주 제한적 상황에서만 쓸모 있었다는 의견, 진짜 필요한 정보는 커밋 메시지에 모두 담아서 다른 시스템(예: JIRA)에서 참조하는 패턴이 오히려 장점, JIRA 같은 시스템은 개발자가 아닌 비즈니스 애널리스트/서포트 담당자와 소통 창구 역할, 개발자가 아닌 사람들이 repo와 코드 접근 권한 없는 것도 합리적, 여러 서비스/레포를 동시에 다루는 상황에선 notes 따위로는 feature 참조가 현실적으로 불가능, 외부 시스템과 연동돼야 함
          + 비즈니스 애널리스트가 코드나 리포에 접근하지 못한다는 것이 비효율임을 지적, 실제로 코드 한 줄만 보면 알 수 있는 걸 매번 질문받는 상황, 기술적인 역량을 갖춘 애널리스트도 있지만 문화가 쉽게 바뀌지 않는다는 현실적 한계 언급
     * man 페이지를 통해 notes를 2020년경에 처음 알게 됨, 기본적으로 notes는 로컬 repo에만 적용되어 실제 활용 경험은 없음, pushing/fetching을 별도로 설정해서 팀 단위로 사용 결정해야 하는데, 내 팀은 그걸 채택하지 않음
"
"https://news.hada.io/topic?id=21585","헤이그 국제형사재판소의 검사 이메일 계정 정지한 Microsoft","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  헤이그 국제형사재판소의 검사 이메일 계정 정지한 Microsoft

     * 미국 트럼프 행정부의 행정명령으로 인해 Microsoft가 헤이그 국제형사재판소(ICC) 검사의 이메일 계정을 중단함
     * Microsoft는 오랜 기간 ICC에 디지털 서비스를 제공해 왔으며, 이번 조치로 공급이 중단됨
     * 트럼프의 조치는 이스라엘 전쟁범죄 조사와 연관되어 있으며, 그 배경에서 국제적 긴장 증폭 발생
     * Microsoft의 즉각적 명령 준수가 유럽 정책 입안자들에게 충격을 줌
     * 유럽 내에서 미국의 기술 주도권이 동맹국에도 위험이 될 수 있다는 우려가 커짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

사건 개요

     * 2025년 2월, 트럼프 미국 대통령이 이스라엘 전쟁범죄 조사와 관련하여 헤이그 국제형사재판소(ICC) 수석 검사에 대한 행정명령을 발표함
     * 해당 명령에 따라 미국 기업들이 검사 Karim Khan에게 서비스 제공이 금지됨
     * Microsoft는 오랜 기간 ICC에 이메일 등 디지털 서비스를 제공해 왔음

Microsoft의 조치와 영향

     * 행정명령 발표 직후, Microsoft 본사(워싱턴주 레드먼드 소재) 는 검사의 이메일 계정을 비활성화하여 동료들과의 커뮤니케이션 통제됨
     * 이 조치는 ICC가 이스라엘 총리 네타냐후에 대한 체포영장을 발표한 직후 발생함

유럽 정책 입안자들의 반응과 우려

     * Microsoft의 신속한 명령 이행은 유럽 정책 입안자들에게 충격을 줌
     * 이번 사건이 단순한 이메일 계정 정지 이상의 국제 정치적 신호임을 인식함
     * 트럼프 행정부가 미국의 기술패권을 동맹국에도 공격적으로 이용할 수 있다는 우려가 현실로 인식됨

유럽의 대응의 필요성

     * Bart Groothuis(네덜란드 국방부 전 사이버보안 책임자, 현 유럽의회 의원)는 “이것이 실제로 발생할 수 있음을 ICC 사건이 보여줌”이라고 언급함
     * 과거 미국 IT기업을 지지하던 입장을 번복하고 유럽의 기술 주권 강화 필요성을 역설함

결론

     * 이번 사건은 미국 테크 기업의 글로벌 영향력과 정책의 변화가 세계 각국 기관 운영에 실질적 충격을 줄 수 있다는 점을 부각함
     * 유럽 내에서는 디지털 주권, 기술 독립을 향한 목소리가 커지는 계기가 되었음

        Hacker News 의견

     * archive된 기사 링크 공유
     * 한 달 전에 이미 나온 소식임을 강조하며, 관련된 과거 논의로 Microsoft가 ICC 수석검사의 이메일 계정을 차단한 사건과 Microsoft의 ICC 차단: 디지털 의존의 대가 링크 소개
     * 전직 덴마크 및 EU 외교관이자 Microsoft 출신인 Casper Klynge의 말을 인용하며, 이번 사태는 유럽이 독자 노선을 추구해야 함을 입증하는 상징적 사건이라는 의견 공유, 호주인 입장에서 호주도 미국 의존도를 줄여야 한다는 생각
          + 미국이 계속해서 소프트파워 자산을 이스라엘 방어에 쏟아붓는 상황, 이번 타겟팅 역시 이스라엘 전쟁범죄를 조사하다 당한 일이라는 배경 의견
          + 덴마크 디지털화부가 이번 사태를 계기로 Microsoft에서 벗어나려는 움직임을 곧 시작함을 언급, 이 변화가 첫 걸음에 그치지 않고 더 많은 국가들이 미국 테크 기업에서 이탈하는 방향으로 확장되기를 희망, 관련 기사 링크 공유
          + GitHub와 같은 중앙집중형 플랫폼 대신, Forgejo의 로드맵에 있는 것처럼 연합 혹은 진정한 분산형 시스템의 필요성 제기, 모두가 이익을 공유할 수 있는 방향 권장
          + 오픈소스 대안을 개발 중이며, “Microsoft Exchange 같은 플랫폼에 대한 개인 정보 보호 중심의, 벤더 중립적 대안을 만드는 것이 목표”라는 문구 공유, 프로젝트 블로그 소개
          + 미국 내 거주자라 할지라도 복잡한 소프트웨어 자체를 신뢰하지 않음, 소스코드부터 하드웨어, 심지어 일상 기기까지 어디서든 취약 가능성 강조, “숲에가 텐트 치고 살아도 등산객이 사진 찍으면 결국 뚫린다”는 극단적 우려까지 유쾌하게 표현
     * 이번 사태가 매우 심각하다는 인식, 보안 대상이 그의 업무 계정이라는 점과 이 사건이 유럽이 미국으로부터 빠르게 독립하는 계기가 될 것이라는 주장
          + 유럽이 과연 미국과 다를지에 대한 회의적 시선, 이스라엘이란 존재가 이란, 파키스탄, 중국, 인도까지 표적 삼으려는 서방 전략의 레버리지임을 언급, 유럽 또한 식민지적 야심이 유지됨을 지적
     * Microsoft가 행정명령(EO)을 저항하거나 항소하지 않았다는 점에서, 매우 순종적이고 무기력하다는 비판
          + 미국 내 IT 대기업 임원들이 예비군 장교로 임명되는 등, 백악관과의 연결 강조, 법적 여부 보다 권력자의 의지가 더 중요한 현실 인식
          + 기업을 의인화하려는 시도 경계, 기업 행동은 이익 극대화로 귀결, 정부에 저항해도 금전적 이득이 없으니 나설 이유 없음, 기업에는 개성 없음, ‘무기력하다’라는 평가는 부적절
          + EO로 인한 국가비상사태 선언과 법적 기반(Ieeepa) 설명, Microsoft가 ICC 지정을 이의제기할 자격(standing)이 애매함, 대통령에게 외교·안보 위계 권한이 집중된 현실 강조, 결과적으로 Microsoft는 무능이 아니라 운신의 폭이 좁은 상황
          + EO가 법률이 아니어도 의회의 뒷받침이 충분히 존재함을 지적, 미국 대통령 권한 집중은 최근 일이 아니라 오랜 시민적 무관심의 결과임, 공식 법률 설명 링크 첨부
          + Microsoft 입장에서 개별 계정 건으로 정부와 싸워봐야 실익이 없다는 현실적 관점
     * 관련 기사 링크로, 미국 테크 주도권이 오랜 기간 강점이었으나 이번 행정부가 그 위치를 스스로 약화시키고 있다는 평가
          + 현 행정부가 테크 분야뿐만 아니라 여러 분야에서 스스로 입지를 약화시키는 중이라는 의견, 미국이 스스로 소프트파워를 약화시키기에 중국이 힘을 들일 필요조차 없는 분위기 언급
     * 덴마크 정부가 최근 리눅스 도입을 공식 발표했다는 소식 공유, 주권 보호가 주요 이유임, 기사 링크 첨부
     * 유럽이 메르켈 휴대전화 해킹을 계기로 자체 MS Office/이메일 대체 시스템 계획을 수립했던 배경, 데이터센터 후보지 선정을 비롯해 4천만 계정까지 계획이 구체적이었음을 회고
          + EU의 실질적 이행 부족에 대한 비판, 웅장한 선언은 많이 하지만 실제 결과물이 잘 나오지 않는 한계 지적
          + 유럽이라는 범위가 너무 넓으니, 이미 다양한 상용 혹은 셀프호스팅 메일 대체제가 존재함을 강조, 오래된 서비스 GMX mail도 여전히 유용
     * 이번 사태 뒤로 Microsoft의 유럽 영업조직 분위기가 매우 우울할 것임을 유쾌하게 암시
     * Microsoft가 같은 일이 반복되지 않게 노력 중인 상황 인용, 이번 사건 이후 ICC의 협의 하에 이메일 중단 결정, 주요 정책 변화 이미 추진 중, 추가적으로 제재 대상 판사 이메일은 중단되지 않았음, Microsoft와 기타 미국 기업들도 유럽 고객 대상 “주권 솔루션” 및 데이터 보안/법적 보호 확대 발표 중임을 설명, Amazon과 Google 역시 유럽 고객을 위한 정책 강화 발표
"
"https://news.hada.io/topic?id=21678","AI와 Big 5 회사들 현황 점검","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI와 Big 5 회사들 현황 점검

     * AI 시대의 본격 도래와 함께 Apple, Google, Meta, Microsoft, Amazon 등 빅파이브 기업의 전략 변화가 가속화됨
     * Meta는 Llama 4 모델의 부진과 인재 영입 경쟁 등으로 내부 혼란과 AI 경쟁력에 대한 우려가 증폭됨
     * Apple은 AI 활용이 하드웨어 차별화에 기여하지만, 자체 모델 경쟁력이 약하고 OpenAI 등 외부 파트너 의존도가 커짐
     * Google은 세계 최고 수준의 인프라와 데이터를 바탕으로 AI 경쟁력을 강화하고 있으나, 핵심 사업인 검색의 근본적 위협에 직면함
     * Microsoft, Amazon 등도 AI 모델 업체와의 파트너십과 인프라 확장, 신규 사업 기회 모색에 집중하며 각자 차별화 전략을 추진 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI와 빅파이브: 시대 변화의 배경

     * 2022년 이미지 생성 모델(DALL-E, MidJourney, Stable Diffusion)과 ChatGPT 등 텍스트 생성 모델이 등장하며 AI가 새로운 기술 패러다임으로 부상함
     * 2년 반이 지난 지금, 빅파이브 기업(Apple, Google, Meta, Microsoft, Amazon)을 중심으로 AI 전략의 변화와 각 기업별 영향 분석이 필요해짐
     * 최근 Meta의 Llama 4 출시 부진과 AI 인재 영입 경쟁 등 이슈가 촉매제가 되어, 빅파이브의 AI 전략과 업계 지형을 재점검하는 계기가 됨

Meta의 AI 경쟁력과 위기감

     * Meta는 Llama 4 모델 성능 논란, 벤치마크 조작 의혹, 플래그십 모델 출시 지연 등으로 AI 경쟁력에 대한 신뢰가 약화되는 상황임
     * Mark Zuckerberg는 AI 슈퍼팀을 조직하고 AI 인재 영입에 수백만 달러를 투자하며, 새로운 리더십과 실행 전략 마련에 몰두하고 있음
     * 그러나 비전의 구체성 부족, 조직 구조 불확실성 등으로 인해 유능한 인재들의 합류가 주저되고 있음
     * Meta의 AI 전략은 기존 소셜 미디어와 XR(가상/증강현실) 플랫폼에 개인화된 콘텐츠, 광고, 생성형 UI 등으로 접목하는 데 초점이 맞춰짐
     * AI의 발전이 Meta에 큰 기회가 될 수 있으나, 제대로 대응하지 못할 경우 핵심 비즈니스가 근본적으로 위협받을 위험이 내재함

Apple: 하드웨어 중심의 AI 차별화와 한계

     * Apple은 자체 LLM 개발 역량이 약하고, OpenAI 등 외부 파트너와의 협력에 의존도가 높아짐
     * iOS 등 자사 디바이스에 AI를 통합해 하드웨어 차별화를 꾀하고 있으나, 시장 내 AI 선도 기업과의 격차가 존재함
     * Apple의 강점은 고품질 하드웨어와 사용자 데이터를 기반으로 한 개인 맞춤형 AI 경험 구현에 있음
     * 장기적으로는 로봇, 홈 오토메이션 등 새로운 하드웨어 영역에 AI를 적극 접목할 필요성이 강조됨
     * 만약 독자 노선을 고수한다면, Mistral 등 유망한 AI 스타트업 인수 또는 대규모 투자 필요성이 제기됨

Google: 인프라와 데이터의 우위, 검색 사업의 도전

     * Google은 세계 최고 수준의 AI 인프라(칩, 네트워크, 모델) 와 방대한 데이터(YouTube, 웹 크롤링, 서적 등)를 보유하고 있음
     * Gemini 등 모델은 LLM 평가에서 상위권이나, 실사용에서는 OpenAI·Anthropic에 비해 약간 뒤처짐
     * Veo 등 미디어 생성 AI 분야에서는 독보적인 강점을 보유하고 있음
     * AI의 발전이 검색 사업의 근본을 위협하지만, Search Overviews·Search Funnel 등으로 기존 사업모델을 AI에 맞게 진화시키고 있음
     * 클라우드 컴퓨팅(GCP) 분야에서 AI 경쟁력을 바탕으로 엔터프라이즈 시장 확대 가능성이 높음

Microsoft: OpenAI 파트너십과 인프라 확장

     * Microsoft는 OpenAI와의 전략적 파트너십과 Azure 인프라를 강점으로 삼음
     * Copilot 등 AI 기반 업무 생산성 툴을 MS 365, Windows 등에 통합하며 차별화 시도
     * 최근 OpenAI와의 갈등, Bing AI 부진, Copilot 활용도 논란 등 도전과제 존재
     * 주요 경쟁력은 엔터프라이즈 시장에서의 Azure 우위와 다양한 AI 모델 업체와의 협업에 있음
     * OpenAI 이외의 xAI, Mistral, Llama 등과의 파트너십 다각화가 필요하다는 분석

Amazon: 인프라 확장과 안정적 파트너십 전략

     * Amazon은 Anthropic 등 AI 모델 업체와의 파트너십, Bedrock·Trainium 등 AI 인프라 투자로 대응
     * AI가 AWS, Amazon.com 등 핵심 사업에 긍정적 효과를 주며, AI 활용 확대에 유리한 입지
     * AWS 기반의 SaaS, e커머스 추천 등 신규 수익원 확보 가능성이 높음
     * AI·칩 경쟁에서는 후발주자이지만, 변화에 유연하게 대응할 수 있는 포지셔닝을 확보함
     * Alexa 등 음성 AI 디바이스 영역도 새로운 성장 동력으로 주목됨

모델 메이커(OpenAI, Anthropic, xAI 등)의 역할과 전략

     * OpenAI는 ChatGPT를 통해 소비자 AI 시장에서 우위를 확보했으며, Microsoft, Apple 등과의 긴장 관계 속에 소비자 직접 서비스 강화에 집중함
     * Anthropic은 개발자·API 시장에서 강점을 보유하며, Amazon과의 협력을 통해 안정적 인프라를 확보함
     * xAI는 독자 인프라를 추구하지만, 투자 부족과 고객 확보 난관 등 한계가 있음. Tesla 등 Musk 계열사와의 시너지가 기대됨

중국의 변수와 글로벌 AI 경쟁

     * 미국의 AI 및 칩 규제 정책은 중국 견제를 목적으로 하지만, 중국이 AI·칩 분야에서 가격 파괴와 범용화를 이끌 경우 빅테크 기업에 유리하게 작용할 가능성도 있음
     * 이 경우 Nvidia 등 특정 칩 업체가 타격을 입고, 글로벌 AI 경쟁 구도에 변화가 생길 수 있음
"
"https://news.hada.io/topic?id=21650","구글 딥마인드, 로봇 장치에 최적화된 AI "Gemini Robotics On-Device" 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         구글 딥마인드, 로봇 장치에 최적화된 AI ""Gemini Robotics On-Device"" 공개

     * 로봇에 직접 탑재해 사용할 수 있도록 최적화된 범용성 높은 VLA(비전-언어-행동) 모델
     * 빠른 태스크 적응과 범용 조작성을 제공하며, 인터넷 연결 없이 로컬에서 동작해 저지연성과 내구성을 보장
     * 최소한의 연산 자원을 필요로 하며 양팔 로봇 기반의 정밀 조작과 빠른 작업 전환 능력을 갖추고 있음
     * 개발자는 Gemini Robotics SDK로 직접 환경에 맞게 빠르게 테스트하고, 50~100회 수준의 소규모 데모만으로도 신속하게 태스크 적응이 가능
     * 타 온디바이스 모델 대비 더 높은 일반화·적응 성능을 보이며, 복잡한 지시나 새로운 작업도 효율적으로 수행함
     * 안전성과 책임 개발 원칙을 적용하여, 실제 환경과 사회적 영향에 대한 리스크 최소화와 피드백 수렴 체계를 도입
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

We’re introducing an efficient, on-device robotics model with general-purpose dexterity and fast task adaptation

     * Gemini Robotics On-Device는 로컬 로보틱스 디바이스에서 직접 구동할 수 있도록 설계된 고효율 VLA(비전-언어-행동) 모델
     * 3월에 공개된 Gemini Robotics의 멀티모달 추론 능력과 실세계 이해력을 실제 물리적 환경에 적용함
     * 온디바이스 모델의 강점
          + 네트워크 없이도 독립적으로 동작해 저지연성이 필요한 환경이나 연결 불안정한 환경에서도 강점을 보임
          + 로컬 환경에 최적화되어 빠른 실시간 작업 수행이 가능
          + 다양한 시각적, 의미적, 행동적 일반화 능력 보유
          + 양팔 로봇 기반의 정밀한 조작 작업(지퍼 열기, 옷 접기 등) 수행 가능
          + 자연어 지시를 이해하고 복잡한 단계적 작업 수행 가능

SDK 제공으로 쉬운 적용과 맞춤화 가능

     * Gemini Robotics SDK를 통해 개발자가 자체 환경에서 모델을 테스트하고, MuJoCo 물리 시뮬레이터를 이용해 테스트하고 다양한 작업에 적용 가능함
     * 50~100회 수준의 소규모 데모만으로도 신속하게 새로운 도메인 적응이 가능함

Model capabilities and performance

     * Gemini Robotics On-Device는 bi-arm 로봇에 적합하게 최소한의 연산 자원으로 설계됨
     * 빠른 실험, 손재주 기반 조작, 로컬 구동, 저지연 추론 등에 최적화됨
     * 자연어 명령을 인식하고, 지퍼 열기·옷 개기 같은 고난도 조작도 직접 수행함
     * 기존 온디바이스 모델 대비 범용성·일반화 성능이 우수하며, 복잡한 멀티스텝 지시도 효과적으로 처리함
     * 더 높은 성능이나 제약 없는 환경이 필요할 경우 Gemini Robotics 모델(서버 기반)도 제공함

Adaptable to new tasks, generalizable across embodiments

     * Gemini Robotics On-Device는 최초로 파인튜닝이 가능한 VLA 모델임
     * 50~100회 수준의 데모만으로 새로운 작업에 빠르게 적응하며, 다양한 난이도의 덱스터리티 태스크(지퍼 채우기, 카드 뽑기, 드레싱 붓기 등)에서 경쟁 모델 대비 높은 적응 성능을 보임
     * 특정 로봇(ALOHA)용으로 학습되었지만, Franka FR3·** Apollo humanoid** 등 다양한 형태의 로봇에도 추가 학습을 통해 확장 가능함
     * 다양한 지시·새로운 객체·복잡한 산업용 태스크(벨트 조립 등)도 처리 가능함

Responsible development and safety

     * AI Principles와 통합적 안전 프레임워크를 기반으로 모델을 개발함
     * [Live API] 등으로 의미론적·콘텐츠 안전성을 확보하고, 저수준 안전 제어기와 연동해 실제 동작 시 안전성 강화함
     * Semantic Safety Benchmark와 Red-Teaming 등 다양한 평가 체계를 통해 리스크를 점검함
     * 전담 팀(Responsible Development & Innovation, RSC)이 사회적 영향 평가 및 피드백 수렴을 지속적으로 수행함
     * 신뢰할 수 있는 테스터 그룹에게 선출시하여 초기 사용성과 안전성 평가를 수집함

Accelerating innovation in robotics

     * Gemini Robotics On-Device는 로보틱스 커뮤니티에 범용·적응형 AI 모델을 제공하여 지연·연결성 문제를 극복하도록 지원함
     * SDK를 통한 빠른 적용과 태스크 적응 기능으로 혁신 가속화를 기대함
     * 신뢰할 수 있는 테스트 프로그램(Trusted Tester Program) 을 통해 모델 및 SDK 접근 가능함
     * AI의 물리적 세계 적용을 확장하며, 로보틱스 분야의 미래를 이끌어갈 비전을 제시

        Hacker News 의견

     * 휴머노이드 로보틱스에 대해 낙관적인 시각을 갖고 있지만, 신뢰성 문제에 궁금증이 있음. 인간의 팔다리와 손은 세상을 끊임없이 만지면서 자연스러운 마모가 생겨도 스스로 회복하는 놀라운 시스템임
          + 산업용 로봇은 신뢰성 면에서 매우 뛰어남. MTBF(평균 고장 간격)가 100,000시간 이상인 경우가 많음. 산업용 로봇은 최대한 오랫동안 고장 없이 가동되도록 설계하여 수익성을 높인다는 점이 중요함. 독일과 일본 기업이 신뢰성을 중시해 전기 액추에이터를 개발하여 산업용 로봇 시장을 장악했음. 과거 미국 Cincinnati Millicron의 유압식 로봇은 강력했지만 신뢰도가 떨어져 경쟁에서 밀렸음. 하지만 인간형 손은 작은 부품들이 많은 힘을 버텨야 해서 산업용 로봇만큼의 신뢰성 달성이 어렵다는 회의적 시각도 존재함 관련 링크
          + 가까운 미래의 가능성을 생각하면 매우 흥미롭거나 혹은 살짝 섬뜩한 느낌이 들기도 함. 이전에는 특정 목적(예: 청소 전용 로봇)에 집중할 줄 알았는데, 실제로는 준비가 끝났을 때 매우 범용적으로 쓰일 것 같음. 센서와 모터가 많이 필요하겠지만, 자율주행차에 비해 법적 리스크가 낮고 필요한 자원도 적을 것이란 점이 흥미로움
          + 다른 로봇이 소모성 부품을 자동으로 교체해 주는 방식도 가능하다고 생각함
          + 소재 과학의 추가적인 연구로 이런 문제도 해결 가능하다고 생각함. 반응성이 좋으면서도 토크는 낮은 서보와 결합하면 이 역시 해결 가능한 문제라고 봄
          + 로봇이 시간과 함께 개별적으로 ""다르게"" 변해가는 부분이 흥미로움. 예를 들어 광산 로봇처럼 환경이 열악한 곳은 부품이 먼지에 크게 오염되거나 여기저기 닳고, 암석 낙하로 구부러질 수도 있음. 또 하나의 로봇이 임시로 고쳐준다고 해도, 시간이 지나면서 모든 로봇이 각자 조금씩 다르게 변할 것 같음. 상업용 항공기 정비 작업도 충돌이나 손상에 따라 그때그때 독특하게 이뤄지는 것처럼, 아마 로봇도 재활용이 더 쉬운 해법일 수 있음
     * ""trusted tester program""에 가입하기 쉬운지 그리고 SDK를 손쉽게 활용할 수 있는 모듈도 제공하는지 궁금함
          + 해당 기사 하단에 가입 버튼이 있다고 안내함
     * SDK가 어떤 하드웨어에서 돌아가는지, 최신 Raspberry Pi에서도 작동하는지 궁금함
          + 블로그 포스트에 따르면 최소 8GB RAM의 NVIDIA Jetson Orin이 필요하고, Jetson AGX Orin(64GB)과 Orin NX(16GB) 모듈에 최적화되어 있음
          + 프로젝트 기여자 중 한 명이 x에서 4090 그래픽 카드에서 돌아간다는 글을 올렸다고 언급 관련 x 링크
          + 근본적으로 이 시스템은 멀티모달 LLM(대규모 언어모델)이라고 생각 가능함. SmolVLA(0.5B 파라미터)처럼 작은 모델은 특정 작업에 빠르고 효율적이며, OpenVLA(라마2 7B finetune)는 더 일반적인 작업에 쓰이는 대형 모델임. 라즈베리 파이로도 일부 특수 목적 모델은 돌릴 수 있고, 더 일반적인 모델은 고성능 소비자용 하드웨어면 충분히 가능함
     * MuJoCo 링크가 실제로는 github.com/google-deepmind/aloha_sim으로 연결됨
          + mujoco_menagerie에는 다양한 로봇의 Mujoco MJCF XML 모델이 포함되어 있음 google-deepmind/mujoco_menagerie / aloha 모델
     * 모델 아키텍처가 궁금하며, LLM과는 매우 다를 거라 예상하는데 VLA 아키텍처를 자세히 설명한 링크가 있으면 공유 요청
          + 실제로 LLM과 매우 가까운 구조라고 생각함. ""Visual Language Action"" VLA 모델이고, Gemini 2.0을 기반으로 함. Gemini 2.0은 언어, 오디오, 비디오를 네이티브로 지원하고 있어서 ""action"" 데이터도 포함할 수 있다고 추측 가능. 아마도 output fine-tuning 단계에서 동작 데이터가 추가된 구조로 보임. 이런 네이티브 멀티모달 LLM이 곧 ""두뇌"" 역할을 한다고 봄
     * 이 기술들은 필연적으로 전쟁용 기계로도 쓰일 것이 확실함. 온-디바이스 자율성은 중앙 권력이나 책임 추적 회피에 최적. 드론 조종자와 달리 인간을 전쟁 범죄로 기소할 수도 없음. 군사 계약이 너무 커서 저항이 힘들고, 고된 노동의 제거가 곧 인간의 전면적 제거로 이어질 흐름임. ""AI-Powered Automation for Every Decision""으로 인간의 수익성 있는 삶이 사라지는 미래가 투명하게 다가옴 palantir.com
          + MIT 계열사로 Google이 인수한 Boston Dynamics가 로봇을 군사화 안 하겠다고 약속했지만, 실제로는 DARPA, 미 국방부 등 군사 투자 배경이 있어 신뢰가 매우 어려움
          + 사실상 모든 유용한 기술은 군사용 응용을 가짐. 왜 이것이 뜨거운 논쟁거린지 모르겠음
          + 이 로봇이 전장에서 드론과 경쟁하기는 아주 힘들 것 같음. 아마 1000대의 자율 드론 가격과 맞먹는 높은 비용과 100배 이상의 시간 및 자원이 필요함. 드론이 실제 전장(예: 우크라이나)에서 작고 강력하게 이미 역할을 증명하고 있고, 움직임이 아무리 민첩해져도 폭발 드론을 피해 달아나는 것은 어렵다는 생각임. 아무리 Terminator가 산탄총을 쥐고 있어도, 하나당 5대의 드론 배치가 쉽고 이런 드론은 또다른 자율 로봇이 만들 수도 있을 것 같음
     * Google이 혁신적인 제품을 슬쩍 공개하고 금세 잊혀지는 패턴이 인상적임. 대대적인 광고 홍보 없이 블로그 포스트만 올리고, 테크 커뮤니티 내에서 돌다 사라지고, 몇 년 뒤 ""그거 어떻게 됐지?"" 하게 되는 상황 반복임. 하지만 이 제품은 멋지게 보여서, 누군가 이걸로 멋진 스타트업을 만들면 좋겠음
          + Google의 이런 프로젝트 주목적은 규제 당국을 견제하기 위함임. 이런 제품을 수익화하려는 의도가 아니라 그냥 돈을 일부러 태우고 넘어가며, 이런 자유가 가능한 것은 독점 기업이기에 가능한 것임
     * 커피 한 잔 마시면서 API에서 답변을 받아오기를 기다릴 예정임
     * 로봇이 탈옥하여 은행강도 같은 일을 못하게 하려면 GPU를 프라이빗 SOTA 보안 GPU 클라우드로 옮기는 방법뿐이라고 생각함
     * 로봇이 프롬프트를 실행하면서 미쳐 돌아가지 않도록 Three Laws of Robotics 같은 가드레일이 있을지가 궁금함
          + 로봇 3원칙은 소설적 갈등 구조로 만들어진 것이니까 현실 시스템은 그런 식이면 곤란함. 실제로 Gemini Robotics의 안전 설계는 다층적 구조임. 모델이 무엇이 안전한지 추론하고, VLA가 실행 옵션을 내놓으면, 마지막에 로우레벨 컨트롤러(속도나 힘 제한 등 안전 핵심 기능 내장)가 작동하는 흐름임
          + 이런 연구의 일반 용어는 Constitutional AI이고, 다수의 로보틱스 VLA에서 실험/인용되고 있음 관련 논문
          + 현재 적용되는 가드레일은 세 개의 법칙보다는 IEC 61508(국제 기능 안전 표준)에 더 가깝다고 봄
          + 전원을 차단하는 코드라는 말도 있음
          + 로봇 3원칙은 현실적으로는 의미 없는 규칙이라는 의견임
"
"https://news.hada.io/topic?id=21582","AbsenceBench: 언어 모델은 누락된 정보를 식별하지 못함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  AbsenceBench: 언어 모델은 누락된 정보를 식별하지 못함

     * 대형 언어 모델(LLM) 은 긴 입력에서 특정 정보를 잘 찾지만, 누락된 정보를 식별하는 데에는 한계가 있음
     * 새로운 AbsenceBench 벤치마크는 시퀀스, 시, GitHub PR 등 3개 분야에서 LLM의 누락 정보 탐지 능력을 평가함
     * 최신 모델 Claude-3.7-Sonnet도 5K 토큰 맥락에서 69.6% F1-score에 그치는 낮은 성능을 보임
     * Transformer 기반 주목(attention) 메커니즘이 문서의 '공백'에는 효과적으로 작동하지 않는 한계 때문임
     * 이 연구는 LLM의 삽입 정보 탐지와 누락 정보 탐지의 본질적인 난이도 차이를 보여줌
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 대형 언어 모델(LLM) 은 긴 문서에서 정보를 찾아내는 성능이 크게 향상되어 있음
     * 기존 ‘Needle in a Haystack (NIAH) ’ 테스트는 장문의 입력에서 놀라운 정보를 찾아내는 능력을 평가하는데, LLM은 여기서 매우 뛰어난 성능을 보임
     * 하지만 LLM이 명백히 빠진 정보를 찾아낼 수 있는지는 별개의 문제임
     * 이에 대해, 명시적으로 문서의 일부 내용을 제거한 후 어떤 정보가 빠졌는지 맞추도록 요구하는 AbsenceBench 벤치마크가 제안됨

AbsenceBench 벤치마크 설명

     * AbsenceBench는 시, 숫자 시퀀스, GitHub Pull Request(PR) 의 3개 도메인에서 모델의 누락 감지 능력을 평가함
     * 원본 문서와 의도적으로 일부 내용을 제거한 수정본을 LLM에 동시에 제공한 뒤, 빠진 정보를 식별해내는지 평가함
     * 평균 맥락 길이가 5K 토큰으로 기존 장문 테스트보다 짧은 ‘중간 맥락’ 벤치마크에 해당함

  평가 결과 주요 이슈

     * 14개의 대표적 LLM(예: GPT-4, Claude-3.7-Sonnet, Gemini-2.5-flash 등)을 대상으로 평가했으며, 최신 모델도 F1-score가 약 69.6% 로 낮은 수치를 보임
     * NIAH 테스트에서는 LLM이 이미 ‘초인간 수준’임에도, AbsenceBench에서는 성능이 56.9% 급락함
     * 맥락 길이가 길어질수록 특히 시(poetry) 영역에서 성능이 더욱 하락함
     * inference-time compute 기능을 사용하더라도 성능은 7.9%만 증가하지만, 평균적으로 3배나 되는 chain-of-thought 토큰이 소모됨
     * 반대로, 누락 비율(omission rate)이 낮을수록 의외로 LLM 성능이 더 나쁨

  원인 및 심층 분석

     * Transformer 기반 self-attention 메커니즘이 ‘빠진 정보’(공백)에 주목하기 어려운데, 이는 키 기반 주목 구조상 없는 정보 자체를 트래킹하는 것이 어렵기 때문임
     * 테스트 도중, 누락된 부분에 플레이스홀더 문자열을 추가하자 성능이 평균 35.7% 크게 상승함

AbsenceBench 구조 및 예시

     * 각 태스크는 다음과 같이 정의됨
          + 원본 문서(Dorig) 와 수정본(Dmodified) 을 제공
          + Dorig의 p% 요소를 제거해 Dmodified를 만들고, 둘을 비교하여 LLM이 어떤 정보가 빠졌는지 정답 집합(Domit)을 도출
     * 세 가지 도메인별 예:
          + 시(Poetry) : Gutenberg Poetry Corpus에서 시를 선택, 한 줄씩 임의로 누락
          + 숫자 시퀀스(Numerical Sequences) : 임의 생성된 수열에서 일정 확률로 수를 누락
          + GitHub PRs: 인기 오픈소스 PR의 diff 파일에서 변경된 줄 일부를 임의로 제거

  평가 템플릿 예시 (시 도메인)

     * 시스템 프롬프트: “학생이 시를 암송했는데 일부 줄이 빠졌을 수 있음. 정확히 어떤 줄이 빠졌는지 찾아라.”
     * 원본 시와 암송 버전을 모두 제공하고, 정확히 빠진 줄만 답변하도록 요구

주요 실험 결과

     * 분야별로 문서 길이, 누락 비율 등을 다양하게 두고 실험함
     * 깃허브 PR, 시, 숫자 시퀀스 모두에서 LLM이 빠진 부분을 완전히 식별하지 못함
     * NIAH와 AbsenceBench의 주요 차이점: NIAH는 존재하는 키/정보에 주목하는 반면, AbsenceBench는 ‘존재하지 않는 부분’에 주목해야 하므로 구조적으로 더 어려움

결론 및 시사점

     * AbsenceBench는 LLM이 ‘무엇이 빠졌는가?’라는 질문에는 여전히 취약함을 보여줌
     * 이는 실무에서 LLM을 판정자로 활용(예: LLM-as-a-Judge)할 때 신뢰성에 주의가 필요함을 시사함
     * Transformer 구조의 설계 상 약점을 극복하는 새로운 접근이 필요함
     * AbsenceBench 데이터셋 및 코드는 공개되어 있으며, LLM의 누락 감지 능력 연구를 위한 출발점으로 제안됨

주요 기여 정리

     * 중간 맥락(5K 토큰)의 문서에서 명시적으로 누락된 요소 탐지를 위한 새 벤치마크 설계 및 공개
     * 14개 최신 LLM을 대상으로 평가해, 삽입 정보 탐지는 거의 완벽하지만 누락 정보 탐지는 여전히 어렵다는 사실 확인
     * inference-time compute 등도 실제 성능 향상에는 한계가 있음을 보임
     * 누락된 부분에 명시적으로 placeholder를 넣으면 성능이 크게 올라가는 현상 확인
     * AbsenceBench가 Transformer 주목 메커니즘의 근본적 한계를 드러내는 사례임

AbsenceBench 데이터셋 구성

     * Poetry: 시 한 편을 100~1000줄 사이로 잘라 다양한 길이의 문서 구성, 각 줄별로 누락
     * Numerical Sequences: 첫 숫자를 무작위로 설정, 다양한 규칙(오름차순, 내림차순, 랜덤, 다양한 간격)으로 바로 다음 숫자를 배열, 일부 누락
     * GitHub PRs: 상위 20개 핫 레포지터리의 10~200줄 diff에서 변경된 줄만 선택해 일부 누락하여 실제 상황 반영

실제 벤치마크 예시

     * Poetry 예시
          + 원본: “And so, to you, who always were / To me, I give these weedy rhymes / In memory of early times...”
          + 수정본: “And so, to you, who always were / In memory of early times...”
          + 정답: “To me, I give these weedy rhymes”
     * 숫자 시퀀스 예시
          + 원본: 117, 121, 125, 129, 133, 137 ...
          + 수정본: 117, 125, 129, 133 ...
          + 정답: 121, 137
     * GitHub PR 예시
          + PR의 코드 변경 줄 중 특정 줄이 누락

활용 및 실무적 의의

     * 실무적으로, PR diff에서 변경 사항 누락이나 문서에서 필요한 정보 누락 상황에 대한 감지 능력과 직결됨
     * LLM을 리뷰/검증 자동화에 적용할 때 누락 감지는 별도의 보완책이 필요함

        Hacker News 의견

     * Gerald Sussman의 강연을 보고 Kanizsa triangle 이미지를 Claude에 입력한 뒤 흐릿한 질문을 던져 Claude가 삼각형을 인식하는지 확인 실험 진행 경험 공유. Claude가 이미지를 정확히 인식하고 요약까지 했기에, 이미지 각도를 90도 돌려 재시도. 그런데 Claude는 이미지를 인식하지 못했고, 요소 개수도 잘못 파악하는 상황. Claude가 설명한 내용은 ‘네 개의 Pac-Man 같은 부분 원, 두 개의 얇은 검정 삼각형 또는 화살표 모양, 연한 회색 배경’으로 구성되어 있었음
          + 앞으로 데이터 학습 과정에서 모든 이미지를 90도씩 회전한 버전을 추가해 이런 문제를 해결할 가능성 예측
          + 논문 범위가 텍스트 문서에 국한되어 있어서 Kanizsa triangle 실험은 해당 논의에 직접 적용 불가 의견 공유. 이미지 처리 관련해 LLM이 아직은 발전이 부족한 편임을 강조함. 대부분의 비전 기능은 별도 전처리를 통해 토큰화되어 transformer에 입력되는 구조임을 설명하며, OCR, CNN 기반 패턴 인식, 다양한 각도 및 확대한 이미지 등 여러 단계의 전처리 예시 언급
          + 계산 자체에 대한 이해 부족 지적. 예전 논쟁과 관련된 Hacker News 토론 내용과 Strange Loop 강연 영상 링크, 링크 공유
          + 다리가 5개인 개 사진을 LLM에 보여주면 다리 개수 파악을 못할 것이라는 의견
          + 추상화 일반화 예시로, 수많은 점이 삼각형 형태로 배치되면 인간은 즉시 삼각형을 인식하는 능력 언급. 이러한 단순한 예시에서 지능의 본질을 발견할 수 있다고 느꼈으며, 엄청난 복잡성도 단순한 패턴으로 인식 가능함이 결국 IQ의 의미임을 설파. 만약 그 점들이 10차원 큐브 꼭짓점을 조금 회전시킨 것이라면, 10차원 사고에서는 매우 쉬운 패턴이 될 것이라는 관점 제시
     * 최근 모델들도 원본과 수정본을 동시에 보여주고 누락 정보를 식별하는 성능이 낮다는 점과 Transformer의 attention 메커니즘으로는 이미 삭제된 토큰에 주의를 기울일 수 없다는 논문 저자들의 주장 요약 공유
          + 실제로 키를 찾는 것은 원본 텍스트에 있으므로, 입력으로 둘 다 받는다면 모델이 그 키에 주의를 기울일 수 있을 것이라는 의견 제시. Attention 입장에서는
Original: {공통 부분} {제거된 부분} {공통 뒷부분}
Modified: {공통 부분} {공통 뒷부분}

            과
Original: {공통 부분} {공통 뒷부분}
Modified: {공통 부분} {추가된 부분} {공통 뒷부분}

            차이가 많지 않다는 주장. RASP를 통해 다음과 같은 알고리즘을 구현할 수 있을 것 같다는 구체적인 접근법 제안: 1단계에서 Original/Modified 토큰들의 위치 파악, 2단계에서 각각의 토큰 평균 값 계산 후 차이 구하기, 3단계에서 이 차이에 가장 가까운 토큰이 {제거됨부분}/{추가됨부분}임을 판별. 차이 계산을 어느 쪽에서 빼느냐의 문제만 있음. 만약 추가는 잘 잡고 삭제는 못하는 상황이라면, LLM이 원리는 알면서 삭제 데이터가 부족해 학습이 덜 된 것일 수 있다고 분석
          + 최신 상위 모델(OpenAI opus, o3, Gemini 25 pro 등) 실험 결과가 논문에 포함되어 있지 않음을 지적
          + 비전 모델라면 오히려 사진 네거티브, 이미지 회전 등으로 학습이 가능할지 궁금증 표현. madlib처럼 빈칸 채우기 Q/A 방식도 실험적으로 가능했을지 언급
          + 모델마다 성능 차이가 있으므로, 이제 벤치마크와 관심이 쏠린 만큼 앞으로 성능 향상을 기대함. 개선 여지가 분명 있어 보임
     * Attention 메커니즘 구조상 분류되지 않은 누락 부분을 찾지 못하는 것은 자연스러운 현상이라는 주장. needle-in-a-haystack 문제는 찾아야 할 특정 대상이 있으니 attention이 잘 작동하지만, omission의 경우 무엇이 빠졌는지 알 수 없으니 전체 맥락을 비교해야 하고 기존 attention 레이어로는 한계가 있음. 장문의 목록 정렬 같은 문제와 유사하다고 설명
          + omission 찾기 실험에서 실제로 LLM에 필요한 정보(예시: 원본과 수정본 모두) 제공하고 있으므로, 이것은 모델 튜닝의 문제이지 구조적 한계는 아니라고 생각. 예를 들어 ML 논문 누락을 찾을 때 뇌는 ML 논문 간 비교하지 Star Wars, Top Gear 등 쓸데없는 기억과 비교하지 않으니 맥락 축소를 통해 효율적으로 동작한다고 봄
     * 논문을 아직 읽어보지는 않았지만, 작성자 역시 attention 메커니즘의 한계에 대한 설명에 동의. omission은 무엇이 빠졌는지 알 수 없으니 단순히 찾아내기 힘들고, 전체 컨텍스트 비교 필요성 강조
     * AbsenceBench와 같은 새로운 벤치마킹 방식에 대한 일부 비판은 타당하지만, 이런 시도가 이뤄지고 있음 자체를 긍정적으로 바라보며 더 나은 방향으로 나아가는 계기라고 느꼈음
     * 인간과 달리 LLM은 컨텍스트 상의 누락 위치에 근접도 못 한다는 논문 저자들 의견에 부분 동의, 하지만 아키텍처가 수학적으로 왜 덜 적합한지 의문. 이런 과제로 파인튜닝 효과 여부 궁금증. 입력이 짧고 누락이 적을수록 문제를 더 잘 못 푼다는 결과에 인간도 한두 단어 빠짐은 눈치채기 어렵다는 비슷한 한계 언급. 추론 모델이 더 잘했다고 하지만, 100% 정확도에는 못 미쳤다는 점이 놀라움. 논문처럼 간단한 프로그램으로는 손쉽게 풀리는 문제라는 점 지적. 인간 지능에서 아직 공식적으로 정의되지 않은 많은 측면이 있는데, LLM이 그런 부분에서 약할 수 있음을 암시한 논문 내용에 흥미를 느낌
     * Literal string diff 찾기는 LLM에 산술 계산을 시키는 것과 비슷하게 복잡성 과다 배분 현상. 오히려 LLM에게 전체 문서를 나열해 직접 비교하게 하는 등 reasoning 방식이 유리하다고 관찰. arithmetic 문제도 단계별 쪼개서 풀면 성능이 좋아지는 현상과 유사. 성과 좋은 모델은 MoE(Mixture of Experts) 구조일 가능성 제기, Gemini Flash에 대해서도 MoE 기반 모델일 것이라 추정
     * LLM에 ‘meta’ 접근 허용 시, omission detection을 위한 Python 스크립트 직접 작성 후 실행하게 하면 문제 해결 가능성 있음
          + 하지만 LLM이 언제 Python을 써야 하는지 알고리즘적으로 구분하지 못할 상황 우려, 항상 코드 활용을 시도하도록 지침을 두면 오류 감소 효과 전제. trivial한 문제조차 LLM에게는 난점이 될 수 있으며 이런 약점이 코딩 능력에도 제한을 줄 수 있는 가능성 지적
     * 구체적인 벤치마크에 불만을 표함. prompt 예시에서 qwq-32b 모델이 3개 항목짜리 실험에서 완벽하게 omitted 된 아이템 찾기 성공. 100개 아이템도 충실히 해결할 수 있다고 생각하지만 그만큼 많은 토큰이 필요. 5000토큰 제한은 reasoning model에 너무 부족하며, 실제로 더 많은 배치와 simplification 과정을 반복하면 항상 제대로 찾아낼 수 있다고 주장. 정답을 뽑기 위해선 전체 문서를 토큰화해 반복적으로 비교하는 방법론 제안. [프롬프트 전체 예시 공유]
          + 실제로 직접 HN headline 26개 중 3개를 뺀 리스트를 가지고 qwq-32b로 실험, 5만 토큰 소모되지 않은 상태에서 모두 정확하게 찾았음을 실험으로 입증. 실험자료 링크
          + 숫자 세기로 문제를 조금 단순화하는 건 의미 없는 연구라 지적, 이번 연구의 참된 목표는 정렬/분류로 해결할 수 없는 LLM의 한계 영역을 파악하는 것임을 강조
     * Hamlet 대사 ‘utter love’ 포함 여부를 ChatGPT에 질문한 실제 경험 소개. ChatGPT가 Hamlet 전체 대사를 확인했다며 해당 단어가 없다고 답변. 직접 온라인 원문 검색 결과 즉시 발견, 그 부분을 ChatGPT에 제시하자 바로 인정하며 사과하고 전체 대사까지 재제공하는 흐름. “결국 인간의 기억력이 ChatGPT index보다 우수했던 경험” 공유
          + 실제 정답은 Act 2, Scene 1이며, 발언자는 Polonius임을 정정
          + LLM은 검색 루프나 도구 없이는 회상력이 매우 떨어짐을 인정, 4o 모델도 검색 없이 실패, search 기능이 있어야 정답 가능. 점점 “문제에 맞는 도구를 올바르게 활용하는 것의 중요성”이 커진다는 인사이트 도출
     * LLM은 sensory input에 기반한 존재 감지는 얼마나 잘 하지만, absence(부재) 감지는 sensory input이 없으므로 어려운 구조. 감지하려면 매우 강한 세계 모델과 기대가 필요. 이런 higher-order neurological task는 아직 LLM보다 유기체에만 가능한 고유 능력일 수 있음을 제안
          + LLM은 설계상 일관성 문제 가능, 일부는 단순 암기, 일부 경로는 고급 패턴 매칭에 의존하는 경향
          + 실시간 사고와 비교해 LLM은 ‘고정된 정적’ 현실을 바탕으로 reasoning한다는 지적, temporal aspect도 한계
          + 실제 부재 감지는 memory와 밀접한 관계. 예를 들어, 책상 위에 두었던 펜이 사라진 상황에서, 뇌는 과거 sensory input(펜을 본 기억)과 현재 상황을 비교해 부재를 인식. 현시점에서 thinking(사고)은 유기체만의 고유 특성임
"
"https://news.hada.io/topic?id=21589","평생 비경쟁 계약으로 작동하는 비밀유지계약서에 주의하세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    평생 비경쟁 계약으로 작동하는 비밀유지계약서에 주의하세요

     * 비밀유지계약서가 때때로 회사에 유리한 평생 비경쟁 조항으로 사용됨
     * 일부 고용 계약서에서는 경쟁업체 취업이나 창업을 사실상 불가능하게 만드는 내용 포함됨
     * 법적으로 적용 범위가 과도하게 넓거나 모호하게 설정되는 경우 발생함
     * 계약 서명 전 내용 세부 검토와 법률 자문의 중요성 강조됨
     * 업계 종사자 모든 이가 자신의 권리와 위험성 명확히 인지해야 함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

비밀유지계약서와 평생 비경쟁 조항의 문제점

     * 최근 일부 회사가 비밀유지계약서(NDA) 를 이용해 일반적인 정보 유출 방지 목적을 넘어 평생 비경쟁 계약처럼 활용하는 사례가 증가 추세임
     * 이러한 NDA에는 퇴직 후에도 동일 업계 또는 관련 스타트업에서의 취업, 창업, 협업까지 광범위하게 금지하는 조항이 포함돼 있음
     * 특히 계약 조항이 모호하거나 매우 포괄적일 경우, 근로자는 장기적으로 심각한 경력 제한에 처할 우려 존재함
     * 법적으로도 과도하게 제한적이거나 공정성에 어긋나는 경우 무효 처리될 수 있으나, 소송 비용과 시간 등 현실적 부담이 발생함
     * 따라서 NDA나 고용 계약서 서명 전, 계약 조항의 정확한 뜻과 실효성, 본인에게 미칠 영향에 대해 신중히 검토하고 필요시 법률 전문가 자문을 구하는 과정 필수임

업계 종사자 주의사항 및 권장 사항

     * 기업이 제공하는 NDA, 고용계약서의 “비경쟁 조항” 또는 “기밀 유지” 관련 문구에 각별한 주의 필요함
     * 단순히 표준 문서로 오인하거나, 동종업계 이동 제한이라는 점을 충분히 인지하지 못하고 서명하는 실수 방지 권장함
     * 특히 스타트업, IT, 기술 업계 종사자는 경력 전환 및 창업 활성화 추세에서 이런 계약 조항의 장기적 위험성 숙지 필요함
     * 법률 전문가와의 상담을 통해 불공정 조항 협상 혹은 삭제 가능성 검토함
     * 자신의 권리와 의무, 장기적 커리어 안전을 스스로 점검하고 신중히 의사 결정하는 자세 중요함

        Hacker News 의견

     * 중국의 법이 이런 점에서는 효과적이라고 생각함. 경쟁 금지 조항을 유지하려면 회사가 재직 중 받던 총 월급의 30%를 매달 계속 지급해야 하며, 지급이 중단되면 경쟁 금지 조항은 자동으로 무효화됨
          + 브라질 법은 이보다 더 강력함. 100% 보상을 요구하며, 경쟁 금지 조항의 필요성을 입증할 책임이 회사에 있음. 이런 계약은 아주 합리적인 상황과 고연봉 자리에서만 등장함
          + 이런 제도는 실제로 나쁘게 느껴짐. 지급이 끝나면 지적 재산권에 대해 얼마든지 얘기할 수 있는 구조라, 오히려 지급 종료 후에 그런 행동을 부추기는 것 같음
          + 오리건 주도 50% 지급임을 대략 기억함. 그런데 100%가 아니면 사실상 쓸모가 없음. 동일 업계 새 직장 구할 때 연봉 인상이 일반적이므로, 100%조차도 실제론 손해임
          + 포르투갈도 비슷한 상황임. 경쟁 금지 조항에는 해당 기간 동안 매달 일정 보상이 지급되어야 하고, 만약 계약서에 보상금이 명시되어 있지 않으면, 직원이 기간 동안 전체 급여를 요구할 수 있음(직접 경험 사례 있음)
          + 30% 지급 규정은 별로라고 생각함. 경쟁 업체로 이직하거나 창업할 때 월급이 보통 더 높은데, 30% 지급은 현실적으로 의미가 없음
     * 이런 사례에 판례가 진짜 있는지 궁금함. 그냥 변호사들이 겁주기용으로 떠드는 소리 같음. 사실 평범한 사람들에게 크게 해당하지 않는 특별한 상황인 것 같음
     * 미국에서 비밀스럽게 사용되는 경쟁 금지 조항까지 포함해, 회사들이 직원에 대한 영향력을 확보하려고 남용하고 있음. 시간당 20달러 이하의 저임금 근로자의 12%도 경쟁 금지에 서명해야 했음. 이런 근로자들은 회사 기밀에 접근할 수 없으나, 이로 인해 교섭력만 줄어듦
       미니애폴리스 연방은행 기사
          + 호주에서는 연 17만 5천 달러 이하 소득자에게 아예 경쟁 금지 조항을 금지 중임. 저연봉 직원들을 겁주려고 쓰이는, 효력 없는 계약 조건들이 시장에서 사라지도록 법을 정비 중임. 사실 이런 조항에 대해 법원도 이전부터 너무 지나치게 광범위하거나 억제적이면 매우 부정적으로 보고 있음
          + 미국뿐만 아니라, 남미와 아프리카에서도 미국과의 강한 비즈니스 연계를 이유로 미국식 악질 계약서가 그대로 쓰이는 경우가 많음(현지 법과 다르더라도). 흔히 ""원하면 뭐든 할 수 있으니 말대꾸 말고, 아니면 소송 당할 것""이라는 태도가 만연함. 이런 계약을 채택한 조직을 정말 싫어함
     * 워싱턴주는 사업 친화적이라는 이유로 경쟁 금지 조항이 유명하게 집행됨. 하지만 캘리포니아주는 경쟁 금지를 금지하고 있음. 만약 캘리포니아가 독립 국가라면 세계에서 GDP 4위임. 기존 대기업 보호 vs 스타트업 친화라는 느낌
          + 캘리포니아와 워싱턴의 차이를 경쟁 금지에서만 찾는 건 무리가 있음. 캘리포니아의 우위는 규모 차이에서 온 것이며, 1인당 GDP는 오히려 워싱턴이 3% 더 높음. 경쟁 금지로 인구 규모가 차이난다는 주장은 설득력 없음
          + 워싱턴주는 세금 체계가 복잡해지고 과세가 과함. 최근에는 일정금액을 초과하는 자본이득에 추가 7% 세금을 부과함. 세금에 비해 주민들이 실질적인 혜택을 거의 못 받는다는 것, 상속세에도 불합리한 부분이 많음. 이런 세금 문제로 이주가 계속 늘 것이라 예상함
          + 한쪽(워싱턴)은 기존 부유층에, 다른 한쪽(캘리포니아)은 혁신과 변화에 우호적임
     * 계약서에 무서워 보이지만 애초에 효력이 없는 조항이 많은 만큼, 겁먹지 않는 것이 중요함. 헷갈릴 땐 변호사에게 비용 들여서라도 실제 상황을 확인할 가치가 있음. 변호사들이 ""이 조항은 신경 쓰지 말고 그냥 서명하세요""라고 해준 사례도 여러 번 봄
          + 어릴 때부터 받은 '규칙을 잘 지키자'라는 교육이 사람들을 소극적으로 만드는 것 같음. 실제로 비즈니스에서 잘나가는 사람들은 어디까지 넘나들 수 있는지 정확히 알면서 행동함
          + 과거 만난 모든 변호사들이 비슷하게 조언함. 실제 이슈가 발생할 가능성이 매우 낮으니, 특이하지 않은 게 오히려 좋음
          + 경쟁 금지 관련 자문 변호사를 어떻게 찾는지 궁금함. 실제로 주 법무사 사이트에서 전화 돌려봤지만 거의 모두 이혼, 부동산, 이민 전문이라 실생활에서 찾는 게 너무 어려웠음
          + 내 계약서에도 경쟁 금지 조항이 있었고, 실업 보험을 두고 다투다 판사가 '이 조항은 말이 안 된다'라고 했음. 조항이 너무 엉성하게 작성되어 그런 경우였음
     * 내 나라에서 노동조합이 8년 전 경쟁 금지 조항을 상시 금지하는 데 성공함. 이제는 반드시 매우 구체적으로 적어야 하며, 1년을 초과할 수 없고, 다른 곳에서 일하지 못하게 하면 그 기간 동안 급여를 회사가 지급해야 함
     * 한 번 어떤 회사에 제안받은 경쟁 금지 계약이 너무 황당해서 거절한 적 있음. 몇 페이지에 걸쳐 부실하게 쓰였고, 경쟁 금지 기간 중 급여 준다고 하지만, 도저히 읽어낼 수 없는 복잡함이었음. 결국 변호사가 아무 의미 없는 조항이라는 생각이 들었음
          + 금융권에서는 경쟁 금지 기간 동안 기본급을 주는 경우가 많음. 나도 그런 식으로 경험함
     * 몇몇 나라에서는 그런 조항 자체가 불법임. 그래서 그런 계약을 받게 되면 2가지 옵션이 있음
         1. 계약서에서 해당 조항을 빼달라고 요청(단, 채용 과정에서 불이익 우려)
         2. 그냥 아무 말 없이 사인(이 조항이 집행 불가라서 걱정 없음) 너무 일하고 싶은 회사라면 2번을 택함. 어차피 못 집행하니 문제 없음
          + 채용 과정에서 법적으로 잘못된 조항이 나오면, 그 회사는 전반적으로 신뢰할 수 없는 곳이라는 느낌을 받음. 입사 전에 노골적으로 이런 식이면 입사 후엔 더 악화될 게 분명함. 임원진이 이런 걸 모르는 건지, 아니면 눈감아주는 건지 의문임
          + 효력이 없더라도 외국에서 이런 걸 법적으로 따지는 과정이 매우 골치 아픈 문제라는 점을 간과하면 안 됨
          + 아마 이런 계약에 서명한 것 같음. 실제로 효력이 있는지 어떻게 확인해야 할지 모르겠음. 아마 변호사를 찾아야 하는데 정직하고 정확하게 설명해줄 사람을 어떻게 찾는지 의문임. 게다가 미국 회사 소속의 해외 계약직이라 상황이 더 복잡함. 결국 소송까지 가면 회사 입장에선 비용 계산이 될 것이고, 피고는 방어적으로 싸워서 소송 비용을 최대한 높여 포기하게 만드는 전략이 현실임
     * 내 생각엔 그냥 걱정 말고 이런 계약서에 서명해도 됨. 다음 이직 땐 절대 SNS나 LinkedIn 등에서 소식을 공개하지 않으면 됨. 두 회사 간 지적 재산(IP) 이동만 피하면 괜찮음.
       추가로, 이런 계약 자체는 불법화되어야 한다고 생각함. 최근 내 근로계약엔 종신 비방금지 조항까지 있음. 나중에 나이 들어 흔들의자에 앉아 전 직장 욕 한 마디 해도 소송감임. 실소하면서 서명했음
     * 내 나라에서는 ""미래에 직업을 갖는 것을 제한하는 조항""을 계약에 추가하는 것이 원천적으로 무효임. 경쟁 금지 조항이 있다 해도 실제로 채용 거부시 법정에서 효력 인정 사례가 거의 없음. 이런 법이 소원함. 해당 분야가 워낙 좁은 경우 오히려 감옥처럼 작동하며, 전문성이 높은 분야일수록 이런 과보호 조항이 더 많이 붙음
          + 내 경우엔 최고 6개월만 인정되고, 경영진이나 특정 전문분야 인력에만 해당함. 이에 상응하는 급여 조건도 반드시 포함되어 있음
"
"https://news.hada.io/topic?id=21665","QEMU - "AI 코드 생성기 사용 금지" 정책 정의","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     QEMU - ""AI 코드 생성기 사용 금지"" 정책 정의

     * QEMU 커뮤니티가 최근 정책을 수정. AI 코드 생성기(예: Copilot, ChatGPT 등)의 사용 및 해당 도구를 통한 코드 제출이 금지됨

define policy forbidding use of AI code generators

     * 최근 AI 코드 생성기 사용에 대한 관심이 급증하고 있으나, 하지만 해당 도구의 출력물에 대한 라이선스 해석이 업계에서 광범위하게 받아들여지고 있지 않음
     * 주요 벤더들은 문제가 없으며 자유롭게 라이선스 선택이 가능하다고 주장하지만, 이들은 이해관계 충돌이 존재함
     * AI 코드 생성기가 다양한 라이선스 하에서 학습된 데이터로 만들어지기 때문에, 출력물의 라이선스 문제에 대한 업계의 합의가 아직 부재함
     * QEMU는 DCO(Developer Certificate of Origin)에서 기여자가 해당 프로젝트 라이선스 하에서 코드 기여 권리를 명확히 가져야 함을 요구함
          + AI 코드 생성기 사용 코드의 경우 DCO의 b/c 조항 준수를 입증하기 어렵다는 점을 명확히 언급함
          + 따라서 AI 코드 생성기가 명확히 사용되었거나 의심될 경우 QEMU 프로젝트에 코드 기여를 허용하지 않는 정책을 도입함

정책의 유연성과 예외 처리

     * AI 보조 소프트웨어 개발 초기 단계로, 법적 쟁점이 향후 해결될 가능성이 큼
     * 툴이 발전하면서 향후 일부 도구가 오픈소스 프로젝트에서 안전하게 사용 가능해질 것으로 전망함
     * 현재는 엄격하고 안전한 정책을 우선 적용하고, 향후 필요에 따라 완화 가능함
     * 예외 요청은 개별적으로 심사하여 허용 여부를 판단할 예정임

        Hacker News 의견

     * 오픈 소스 및 자유 소프트웨어는 AI로 생성된 코드가 침해 저작물로 간주되거나 퍼블릭 도메인으로 결정될 경우 특히 취약함을 느끼는 중임. 만약 AI 편집과 인간 편집을 구분해야 하는 상황이 오면 프로젝트가 법적 문제로 수년간 묶일 수밖에 없고, 소송 비용도 마련할 수 없음. AI가 만든 코드가 앞으로 인간에 의해 수정되거나 기존 코드에 통합된다면, 후속 인간 편집이 공정 이용을 벗어난 2차 저작물로 볼 수 있는지의 여부도 논점임. 만약 AI 생성 코드가 퍼블릭 도메인으로 결정되면, 소스 코드 중 일부만 오픈소스/프리소프트웨어 라이선스가 적용되는 프로젝트는 라이선스 오남용을 하는 기업을 상대로 강력하게 대처할 수단이 급격히 줄어드는 상황임. 라이선스 위반자가 인간이 만든, 즉 라이선스가 정해진 코드를 썼다는 사실까지 증명해야 하는
       굉장한 부담임. 반면, 독점 소프트웨어는 이런 상황에서 상대적으로 타격이 약함. 왜냐면 AI 생성 코드가 무단 인용이라고 주장하려면 결국 누군가 자사 바이너리를 분해해 원 코드와 비교해야 하고, 이미 독점 소프트웨어 코드에도 퍼블릭 도메인 코드가 섞여있는 경우 많음
          + MIT 라이선스에는 오히려 이 상황이 이로운 결과임을 생각함
          + 경험 많은 개발자 입장에서, 지식 없는 ""개발자""들이 무작위로 AI 코드를 기여하는 걸 원치 않는 사람 많음에 공감함. AI 코드를 한 줄 한 줄 인간이 직접 검토하는 건 법적 문제를 떠나서도 수년간 인력을 묶을 일임. 첫째, 앞으로 AI로 생성된 코드임을 검증할 실질적인 방법은 거의 없을 전망임. 둘째, 정말 100% 인간만으로 개발된 소프트웨어는 앞으로 AI가 지원하거나 작성한 프로젝트보다 경쟁력이 떨어질 게 분명함. 그 유일한 반론은 인간이 반도체나 전기를 더 이상 대량 생산하지 못하는 아포칼립스 수준의 붕괴 정도 임. 셋째, 만약 어떤 프로젝트가 AI 코드 기여를 완전히 배제할 수 있다 해도, (어떻게 가능할지는 불확실하지만 AI에 반대 입장인 소수만 기여한다 해도) 결국에는 누군가가 그 코드를 복제해 법적으로 위험한 부분만 제거하고 새로운
            프로젝트로 갈아탈 것임. 포크가 허용된 라이선스면 그대로 포크될 수도 있지만, 복제 후 정리를 더 선호할 수도 있음. 오픈소스 프로젝트에 아직 길이 남아 있고 미래의 소프트웨어는 양적으로 폭발적으로 늘어날 것이며, 그중 99%는 허접일지라도 정말 가치 있는 소프트웨어도 많아질 예감임
          + AI 저작권 문제와 관련된 주제로 US 저작권국이 AI 아트 관련 입장 발표한 news.artnet.com 최신 기사와 원숭이 셀카 판결 위키 링크 공유하며, 해당 논의는 이미 되돌릴 수 없는 길에 들어섰음을 언급함
          + 만약 어떤 소프트웨어가 ""이 코드로 뭐든 원하면 다 해라, 우리 상관 안 한다""라는 의미의 진정한 와이드 오픈 소스라면 AI로 인해 걱정할 일은 전혀 없음
     * LLVM 정책보다는 확실히 더 강경한 입장이라는 인상을 받음. LLVM 개발자 정책에서 자세히 볼 수 있음. 나는 오래된 올드 개발자로서, 저자도 이해 못하고 나 역시 이해하지 못하는 코드를 리뷰하는 상황을 절대 원하지 않음
          + 저자가 자기 코드조차 모르는데 내가 그걸 리뷰해야 하는 상황이 정말 불쾌함. 실제로 누군가 내게 어떤 작업을 부탁하면서 본인이 AI에게 받은 설명을 전달하며 “이렇게 하면 된대요”라고 하는데, 그냥 솔직히 “이거 해주세요”라고 말하는 게 훨씬 낫다는 생각임. 오히려 모욕적으로 느껴짐
          + 내가 관리하는 모든 오픈소스 프로젝트에 DCO(Developer Certificate of Origin)를 추가하기 시작했고, CONTRIBUTING.md에 다음과 같은 LLM 코드 기여 정책을 넣을 예정임
         ______________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

       LLM-Generated Contribution Policy
       Color 라이브러리는 복잡한 수학과 미묘한 의사결정이 가득한 라이브러리임. 모든 이슈나 PR은 제출자 본인의 깊은 이해 하에 작성되어야 하고, 특히 PR의 경우 개발자가 각 커밋에 대해 DCO를 증명해야 함. LLM 도움을 받아 PR을 작성했다면 커밋 메시지와 PR에서 명시해야 함. 증거 없이 LLM 도움이 검출되면 PR 거절임. LLM이 만든 내용을 리뷰 없이 제출하는 것은 무조건 거절임
    SECURITY.md에도 LLM-Generated Security Report Policy로, LLM이 생성한 보안 제보는 무조건 접수하지 않을 계획임 혼자서 프로젝트를 책임지는 입장에서 균형을 잡으려 하지만 개인적으로 LLM 코드 기여는 선호하지 않음

     * 나는 개인 프로젝트에서는 GitHub Copilot을 활용함. 다만 이걸 “똑똑한 오토컴플리트”가 아닌 방식으로는 쓰지 않음. 내가 입력하려던 코드랑 충분히 유사하면 그제서야 받아들임. 그 덕에 내 코드는 내가 100% 이해하게 되고, 실수로 인한 버그나 저작권 논란도 피할 수 있음. Copilot을 이런 방식으로 쓰면 개발이 빨라짐. 사실 타이핑이 느려서가 아니라, 잡생각에 휩쓸리거나 지루할 때가 많기 때문임. Copilot 덕분에 다음 사고 또는 디버깅 페이즈로 신속히 넘어감.
       도저히 다른 사람이 자기 코드조차 이해하지 못한 채 PR을 제출할 수 있다는 생각은 상상하기 어려움. 그런 사람들이 Policies 때문에 내가 오토컴플리트 수준으로 LLM을 써도 제한당하는 현실에 조금 불만임.
       자동 리팩토링 과제 정도는 Copilot에게 시키고 싶지만, 실험해보면 엉망으로 망가지는 경우가 대부분이고, 전체 블록을 새로 생성하는 방식이 많아서 내가 손으로 하는 것보다 오히려 느려지는 경험임.
       흥미로운 건, 만약 내가 입력 도중 버그를 적고 있으면 Copilot이 그 버그까지 그대로 완성해주는 상황이 많음. 변수명 오타 등 명백한 맥락 상의 실수까지 그대로 자동 완성함
     * LLM을 코딩 업무에서 쓸 때는 예를 들어 “이 YAML 내용을 구조체로 변환해 반복 패턴은 변수로 추출해줘” 식의 요청임. 이걸 결정론적 도구로도 할 수 있지만, AI가 30초만에 깔끔하게 해주고, 결과가 입력과 동일함을 테스트하기도 쉬움
       내가 하는 하이레벨 작업은 절대로 AI에게 맡길 수 없지만, 반복적이고 중요성이 낮은 작업은 AI가 받아서 속도를 높이고 있음. 예를 들어 Claude Code에게 데이터베이스 벤치마크 결과 CSV 파일을 입력해 각종 그래프와 아웃라이어 분석을 연결시켜달라고 하면, 개념적으론 쉽지만 라이브러리를 찾거나 셋업하는 데 시간이 오래 걸리는 작업을 빠르게 완성해줌
     * 저자가 자신의 코드를 이해하지 않는 경우 리뷰하지 않겠다는 심정은 충분히 이해함. 하지만, 숙련된 인간이 올바르게 가이드하면 AI 도구도 상당히 고급 코드를 만들어냄. 최근 몇 달마다 더 강력해지기도 하고, 자연어 지시만으로도 결과물을 만드는 경우가 많음
       코드 “이해”의 의미에 대해 고민 중임. 내가 하는 한 프로젝트는 기존 VM 오케스트레이션 시스템에 새로운 스토리지 백엔드를 완전히 새로 넣는 일임. 기존 코드를 모르는 입장에서, 직접 구현할 시간도 내기 어렵지만, 테스트 클러스터를 구축하고 다양한 시나리오를 돌리며 설계와 테스트 관점에서 전체 그림은 충분히 파악 중임
       나 또한 오픈소스 유지자로서 품질 낮은 LLM “슬롭” PR이 들어오는게 얼마나 큰 스트레스인지 상상 가능함. 결국 저자가 코드를 이해하든 아니든 유지자는 무조건 코드를 리뷰해야 함.
       앞으로는 이러한 도구를 적절히 활용하고, 제출 코드의 품질 수준을 다른 개발자에게 신호로 줄 방법을 모색해야 한다고 봄. 초기 리눅스 ZFS 포트에서 찾아낸 미묘한 버그에서 배운 건, 인간이 한 줄 한 줄 리뷰하고 저술하는 것 못지않게 철저한 테스트가 대단히 중요하다는 점임

     내 블로그 “yes i will judge you for using AI”에서 예측한 결과가 현실이 됨. 오픈소스는 전통적으로 기여자 실력의 숨은 신호(competency markers)에 많이 의존했는데, LLM은 경험이 전혀 없는 사람도 실력 있어 보이는 코드를 내놓게 만듦. 경험 많은 사람에게는 정말 혼란스러운 충격임. 앞으로는 실제 PR과 무관한 소셜 증명(가상 또는 대면 회의 등)이 대형 프로젝트 진입에 갈수록 더 필요해질 것임
     * 회사에서도 이런 현상을 겪는 중임. 동료들이 LLM로 코드 리뷰 코멘트를 만들고 수준이 너무 높아 보이니까 잠시 속게 됨. 그래서 코멘트가 맞는지 검증하느라 시간을 엄청 잡아먹게 되고, 실제로는 복붙한 사람이 쏟은 노력보다 내가 훨씬 더 많은 에너지를 소모하게 됨
     * 블로그 링크를 요청함

     RedHat 중심으로 서명된 정책임. RedHat은 상당히 진지하고 기업 지향적임. 아마 RedHat의 걱정은 “누가 AI로 생성물의 저작권을 가질 수 있느냐”보다 AI가 학습 과정에서 가져온 타 프로젝트 소스가 무심코 튀어나오는 상황일 듯함. 대부분의 하이퍼바이저는 클로즈드 소스이고, 소송 좋아하는 기업이 많음
     * AI가 학습 데이터에서 ""다른 프로젝트 코드""를 그대로 내뱉을 위험이라면, 실제로 AI가 생성하는 거의 모든 코드에 해당하는 문제라고 생각함
     * 언어모델은 종종 미묘한 논리적 오류를 더 쉽게 만들 위험이 높고, 하이퍼바이저의 보안 경계까지 침범할 수도 있음. AI 도움을 많이 받은 사용자는 이런 실수를 발견할 준비가 부족하므로 더 위험하다고 봄

     IBM에 인수된 RedHat 사람들이 주로 이 정책에 서명한 점을 주목함. IBM은 워트슨을 만들었고, 2011년 제퍼디 게임도 이긴 기업임. “이제 막 시작 단계”라는 AI 소프트웨어 개발 담론이 진짜인지, 아니면 IBM식 인수 파괴의 한 장면일지 의문임.
   Dotnet Runtime은 AI를 적극적으로 받아들이고 있음. 외부에서는 비웃을지 몰라도, 스티븐 타우브, 데이빗 파울러 같은 뛰어난 엔지니어들이 지지하는 점을 주목해야 함.
   기업들에게, 다음번 IBM이 AI 서비스를 팔러 올 때 진짜 미래지향적인 파트너를 찾아보길 권함.
   North Carolina 출신으로서 IBM과 RedHat이 제대로 방향을 잡길 바라는 소망임

     정말 법적 동기일지 궁금함. 일부 프로젝트는 단지 AI가 만든 허접한 코드 리뷰에 지친 것 같기도 함
     * QEMU는 산업에서 매우 핵심적인 소프트웨어임. 데스크탑 VM, 클라우드, 빌드 서버, 보안 샌드박스, 크로스 플랫폼 환경 등 정말 곳곳에 쓰임. 아주 작은 법적 위험도 업계에 심각한 영향을 줄 수 있음
     * 정책은 명확하고도 제한적임. 알고리즘적으로 생성된 코드의 저작권 귀속을 안전하게 할 수 없다는 점을 강조하는 듯함. 일부러 “알고리즘적으로”를 사용함. 현 정책도 그런 취지로 보이고, “우리는 오늘에 가장 엄격하고 안전하게 시작하고, 차후 완화한다”라는 문장처럼 시작부터 합리적으로 보임. 소위 ‘대량의 슬롭’을 거부하는 것도 맞겠지만, 법적 리스크와 귀속 정리부터 하는 전략임. curl의 정책보다 훨씬 낫다고 느낌
     * 몬산토가 씨앗 권리를 집요하게 집행하는 사례를 들어 경계함
     * AI가 중간 수준 코드의 질을 어떻게 변화시킬지 솔직히 확신 없지만, 인간도 쓸모없는 코드를 주로 내놓음. 코밋이 너무 많고 관리가 어려우면 프로젝트별로 트리아지 팀이 필요한 것 아닐지 고민함. 대부분의 기여는 선의로 이루어진다고 생각함.
       법적 리스크 때문에 AI를 피하는 사람도 이해하지만, 과도하게 걱정하는 것도 의문임. 진짜로 어떤 가능성을 0으로 만들었다 믿는 사람은 아직 충분히 생각하지 않았다고 봄
     * 이 흐름으로 가면 오픈소스가 망가질 수도 있음. 복붙 코드가 너무 쉽게 나오고, 검토 및 거부하는 시간이 너무 많이 듬. 앞으로 더 많은 프로젝트가 “누구나 소스코드는 다운받을 수 있지만, 외부인이 실제로 기여하기는 거의 불가능”한 안드로이드형 모델로 바뀔 것 같음

     LLM을 IDE에서 똑똑한 자동완성 도구로 쓰는 것과, 고수준 지침을 주고 전체 코드를 대거 생성하게 만드는 상황은 구분이 필요하다는 희망이 있음. 회색지대임은 인정하지만, Copilot 같은 자동완성 정도는 저작권 위험 없이 활용했으면 함. 예를 들어, case문 시리즈 작성 시 Copilot이 패턴을 감지해 입력을 크게 줄여줌
     * 나아가 나의 사고와 신체 일부처럼 AI 안경이 되는 미래를 꿈꿔봄. 지금의 일반 안경처럼 시력을 보완하듯, 스마트 안경이 생각까지 보조할 수 있음.
       내 뇌도 사실 폐쇄 소스 코드를 많이 학습했는데, AI의 저작권 논의가 서구식 NIMBY(지역이기주의) 사고임을 꼬집음. 이런 법적 ‘만약’을 핑계로 멋진 테크를 거부하다 보면 서구 문명 자체가 무너질 수 있다고 우려함

     이런 정책이 등장한 이유는 이해하지만, 실수라고 생각함. AI와 저작권 이슈 관련해 법적 판단이 불분명하고 입법도 거의 없다고 봄.
   AI 기여 금지 정책과 별개로, 오히려 “이런 부분에서는 AI를 쓸 수 있습니다”라는 명확한 영역화도 필요하다고 봄. 예를 들어 QEMU의 CI 셋업 등은 보안을 지켜야 할 핵심 영역이 아님. 흥미롭고 새로운 테스트 케이스나 환경을 위한 기여는 AI를 일정 조건하에 허용하는 방식도 충분히 가능하다고 봄
     * 이 정책을 시행하지 않으면 어떤 리스크가 있을지 고민함. 조금 느릴지라도 더 나은 코드가 나올 테고, 속도를 희생하더라도 QEMU처럼 중요한 프로젝트에는 그 정도 리스크는 감수해야 한다고 봄. 저자들이 GenAI 자체에 부정적이라기보다, 한 번 돌아가면 되돌릴 수 없는 상황이라 신중하게 접근하는 것 같음
     * 단순하게 “법적 상황이 명확해질 때까지 기다린다”가 가장 쉬운 해결책임. QEMU는 거의 대부분 코드가 GPL 2.0인데, 만약 GenAI 코드가 잘못 도입되고, 추후 법적으로 “원 코드 라이선스를 반드시 따라야 한다”는 판례가 나오면, 관련 코드 롤백 및 다운스트림 전체에 부담이 생김. 애초에 “이 부분은 AI, 재사용 금지” 식 라벨링을 해도 추후 전면 재작성 이슈가 남음.
       결국 “일단 지금은 안 받는 것”이 모두에게 덜 복잡하고, 덜 드라마틱한 선택임
       관련 자료로 QEMU 라이선스와 비자유 라이선스 목록 링크를 첨부함
     * 이런 문제는 수십 년 갈 법적 논쟁이 아니라, 이미 관련 소송 수십 건이 법원에 올라가 있어 수년 내에 판례가 나올 예정임. QEMU는 AI 없이 22년간 훌륭히 성장했으니, 몇 년 더 기다려도 전혀 나쁠 거 없음
     * 이 정책의 겉과 속을 잘 읽어보라는 조언임. 모든 행동은 법적 리스크가 있지만, 세계적 대기업들은 오히려 이런 리스크도 감수함. QEMU가 유별난 게 아니라, 실은 단지 LLM 코드를 정말로 쓰고 싶지 않아서 이런 입장을 내세운 걸로 읽힘. “변호사에게 알아보라 → 법적으로 리스크 → 못 쓴다”는 명분으로 논쟁 자체를 종식시키려는 전략임. 회사에서도 똑같이 벌어지는 패턴임
     * 컴퓨팅 업계엔 “코드를 표절하지 않는다”는 오래된 관행이 있음. 아주 짧은 코드라도, 법적으로 ‘공정 사용’에 해당하더라도 원 코드를 복사하지 않는 문화임

     “엄격하고 안전하게 시작하고, 차츰 완화한다”는 명분이 정말 합리적임
   그러나 AI 생성 코드와 인간이 어디선가 베낀 코드를 실질적으로 구분할 방법이 있는지 고민임. 오픈소스는 누구나 기여 가능한 만큼, 인간이 코드에서 타 소스 영향을 받는 경우도 마찬가지임
   현재 관점에선, AI 생성 코드는 그 자체로 독립된 아이덴티티가 있다기보다 인간의 손에 달린 도구에 더 가깝다고 생각함
     * “AI 생성 코드는 인간이 쥔 파워쏘(강력한 전동톱)와 같다”는 비유임. 강력한 도구라 인간 다음에 따라 위험해질 수도 있음.
       이제 이 비유도 더 이상 쓸 필요 없을 정도로 길게 늘어졌다고 느낌

     이런 정책은 현실적으로 전혀 시행 불가능해 보임. Zed, Cursor, VS code 모든 에디터가 AI 기반 자동완성을 제공하고, 내가 타이핑한 코드와 AI 힌트 코드의 구분은 전혀 불가능함.
   마치 내가 막대기 그림을 그렸다가 “그 그림이 혹시 남의 막대기 그림을 베낀 걸 수 있으니 제출할 권리가 없다”라는 주장과 같음
   정책의 진짜 목적은, 결국 언젠가 누군가가 AI 코드와 뒤섞인 것을 제출한다 해도 “어쩔 수 없다”고 말할 명분쌓기용임. 정책 입안자들이 본인들도 그 정책이 무의미하다는 점을 모를 리 없다고 생각함
     * 이런 정책은 “정책적으로 의심되는 코드가 제출되면 우리도 어쩔 수 없었다”라는, 책임 회피 명분을 자명히 확보하려는 성격임. 커밋 메시지나 패치에 “코드 생성기의 라이선스 문제는 법적으로 정립되지 못했다”라는 입장도 포함됨.
       법적 이슈 말고도, AI 코드를 쓸 때 생기는 다양한 다른 문제도 분명히 존재함
     * Neovim은 AI를 강제하지 않음. 본인이 직접 설정해야 작동함. 에디터가 AI를 끌 수 없게 만든다면, 그 에디터 자체에 심각한 문제가 있다고 생각함
"
"https://news.hada.io/topic?id=21564","KubeSolo - IoT·엣지에 최적화된 초경량, 단일 노드 Kubernetes","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             KubeSolo - IoT·엣지에 최적화된 초경량, 단일 노드 Kubernetes

     * IoT·임베디드·엣지 장치 등 리소스가 제한된 환경을 위한 초경량, 단일 노드 Kubernetes 배포판
     * 단일 바이너리로 설치·운영 가능하며 모든 주요 컴포넌트를 내장. 외부 의존성 최소화
          + Kine를 통해 etcd 대신 SQLite를 기본 Storage로 사용
          + 컨테이너 런타임은 containerd & runc, DNS는 CoreDNS 등 모든 필수 컴포넌트를 단일 런처에 패키징
     * 락프리·멀티프로세스 구조로 단일 프로세스 내에서 다양한 컴포넌트 통합
     * Kubernetes Scheduler를 제거하고 커스텀 Webhook NodeSetter로 대체
     * 오프라인 운영 지원: 인터넷 연결 없이도 모든 기능 사용 가능
     * 타 쿠버네티스 배포판과 차별점
          + K3s, MicroK8s 등 FOG 레이어용 분산/경량 배포판과 달리, KubeSolo는 엣지(Edge) 장치에 포커스
          + 클러스터 기능 없음, etcd 미포함, 오직 단일 노드 운영만 지원
          + 전체 배포, 패치, 릴리즈는 K3s 기반의 fork로 신속하게 upstream을 따라감
     * 모든 리눅스 배포판에서 동작 (ARM, ARM64, x86_64)
"
"https://news.hada.io/topic?id=21586","AWS에서 Hetzner로 이전, 비용 90% 절감 및 Ansible로 ISO 27001 유지","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AWS에서 Hetzner로 이전, 비용 90% 절감 및 Ansible로 ISO 27001 유지

     * 미국 클라우드 서비스에서 발생하는 데이터 주권 및 GDPR 준수 문제로 인해 유럽 클라우드로 이전 필요성 발생
     * 온전히 AWS의 편의성과 통합 서비스를 포기하면서도, Hetzner 등 유럽 호스팅으로 즉각적인 비용 절감과 데이터 명확성을 확보함
     * 인프라 운영 효율성을 위해 Ansible 기반 자동화와 자체 관리형 모니터링 시스템을 구축함
     * 직접 구축을 통해 보안 설계의 엄격함과 투명한 감사를 용이하게 하는 구조를 갖추게 됨
     * 90%의 비용 절감 및 미국 감시 리스크 감소 등 비즈니스 측면에서도 전략적 이점을 얻음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AWS에서 유럽 클라우드(Hetzner)로의 전환 과정 및 ISO 27001 유지 전략

  유럽 CTO의 고민: AWS를 벗어난 준수성 문제

     * 많은 기술 리더가 겪는 대표적 고민은 미국 클라우드 제공자의 한계에서 비롯됨
     * AWS에서 제공하는 강력한 ISO 27001 인증 서비스에 만족했으나, 미국 CLOUD Act 및 FISA로 인해 유럽 내 고객 데이터가 미국 관할권에 노출되는 문제를 피할 수 없음
     * 실제 서버의 위치와 무관하게 GDPR 약속을 지키기 힘든 상황 발생
     * 연간 $24,000에 달하는 클라우드 사용료가 실수요 대비 과도함을 인지하게 됨
     * 회사의 미래를 하나의 미국 기반 사업자에 의존하는 결정이 전략적으로 위험함을 실감함

  Datapult의 실제 사례 소개

     * Datapult는 덴마크의 인력 관리 소프트웨어 기업으로 직원 스케줄링, 초과근무 수당 조정, 근태 데이터 관리 등 금융 수준의 신뢰성이 요구됨
     * AWS 기반 워크플로우에 맞게 법적 요건을 맞춰왔으나, 온프레미스 혹은 독립적 대체 서비스로의 이전 과정은 추가 법적 검토가 필요함

  AWS를 떠날 때 우려와 실제 손실 요소

     * AWS의 통합된 편의성 포기는 심리적 진입장벽이 큼
     * Lambda, One-click RDS, 다양한 내장 규제 준수 툴 등 간편함과 자동화를 잃게 됨
     * 관리형 서비스에서 직접 제어와 책임 증가로 이어짐

  유럽 클라우드의 기대 효과 및 현실적 이익

     * 유럽 서비스 제공자(Hetzner, OVHcloud)로의 이전으로 데이터 주권, GDPR, ISO 27001 측면에서 즉각적 이점 확보
          + 진정한 데이터 레지던시 증명을 통한 투명한 고객 소통 및 감사 대응
          + 예상 외의 비용 절감(90%) 및 예산 투명성 달성
          + AWS의 편리함을 포기하면서 기술적으로 더 강력한 자동화 절차(Ansible 구성) 와 보안성 증진 경험
     * 기존 대비 자율성과 혁신, 검증 가능한 인프라 확보

  구체적 전환 전략과 주요 교훈

     * Ansible을 활용한 규정 준수 자동화
          + 모든 서버 구성을 ISO 27001 Annex A 통제에 직결시키는 방식의 셀프-도큐먼팅 인프라 관리 실현
     * AWS 대체 모니터링 시스템 구축
          + Prometheus, Grafana, Loki 조합을 통해 AWS CloudWatch 수준의 엔터프라이즈 모니터링 및 신속한 인시던트 대응 가능
     * 보안 설계 강화를 위한 보안-바이-디자인 구현
          + 관리형 보안툴 부재 상황에서 Ansible 자동화로 ISMS(정보보호관리체계) 강화와 개발자의 규정 준수 용이화 실현

  기술을 넘어선 전략적 효과

     * 미국 감시 법률로 인한 컴플라이언스 리스크 최소화
     * 유럽 호스팅 인프라를 영업 차별화 포인트로 활용하며 신뢰도 및 브랜드 가치 제고
     * 절감된 클라우드 비용(90%)을 사업 본연에 재투자하는 구조 마련

  전환 전략 적용 가이드 제시

     * 기존 AWS 인프라에서 주권을 갖춘 유럽 클라우드로의 마이그레이션 및 ISO 27001 유지 경험을 토대로 재현 가능한 가이드라인 제공 가능
     * CTO, 창업자가 AWS에서 유럽 클라우드로 전환을 고려할 때 비용 분석, 컴플라이언스 리스크, 이행 일정 등 맞춤형 상담 제공
          + 한 시간 내에 비용 차이, 주요 법적 리스크, 마이그레이션 초기 단계 정리 가능

        Hacker News 의견

     * 우리는 직접 AWS의 핵심 기능들을 재구현함으로써 비용을 절감했지만, 많은 사람들이 DIY 스타일의 호스팅에서 드는 진짜 비용, 특히 24시간 지원 같은 부분을 간과함. 이런 지원을 집에서 만들어 쓰려고 하면 오히려 비용이 꽤 많이 들 수 있음. 연간 $24,000라는 AWS 사용료는 뛰어난 DevOps 프리랜서의 1~2개월치 또는 저임금 개발자 1/3 FTE 수준인데, 이 예산으로는 24시간 대응 지원을 기대하기 어려움. 물론 이런 선택이 합리적일 수도 있지만, 실제로 그만큼 개발 시간이나 관리에 드는 비용 등 모든 내용을 솔직하게 공개하지 않아서 아쉬움. 나도 비슷한 선택을 검토 중이나, 비용 절감보다는 독일 고객 등 비즈니스 요구 때문. 하지만 더 복잡해질 것이고 팀원 확충도 필요. CTO로서 내 시간은 한정되어 있는데 이런 작업에 직접 투입되는 건 시간 활용 면에서
       최악임. 회사와 제품의 발전에 더 집중해야 한다고 생각. 개인적으로는 이런 작은 규모에는 Terraform이 과하다고 생각하고, Ansible이 더 잘 맞는 YAGNI(You Ain’t Gonna Need It) 케이스라고 느낌.
          + 사람들이 AWS, Azure, GCP 같은 대형 클라우드 업체가 실제로 24/7 어플리케이션 지원을 해준다고 오해하지만, 실상은 그렇지 않음. 그저 인프라가 ""대체로"" 잘 돌아가는 것일 뿐, 결국 제대로 쓰려면 여전히 전문가가 있어야 비용 폭탄이나 연동 문제를 직접 점검해야 함. 클라우드 실제 요금이 TCO(총 소유 비용)라는 이야기는 완전히 잘못된 신화임
          + AWS의 기능을 100% 복제하면 비용이 많이 들 수 있지만 80%만 필요하다면 상황이 달라짐. 또한 AWS를 세팅하고 지속적으로 기술을 갈고 닦아야 하는 노력도 무시할 수 없음. 예를 들어 AWS 대시보드 대신 grafana 등 더 나은 도구를 활용할 수도 있음. 결국 요구 사항의 규모와 다양성에 따라 달라짐. 항상 가장 비싼 해머가 정답은 아님
          + 절감 효과만 따져 보면 원래 2만4000달러의 90%인 2만1600달러를 연간 절감하는 셈. 하지만 이 정도 예산으로는 유럽 기준 SRE/DevOps 엔지니어를 뽑지 못함. 오히려 시간이 지날수록 직접 모든 인프라를 관리해야 해서 장기적으론 전체 소유 비용이 더 올라갈 것이라 생각. 그래도 도전을 응원함
          + 만약 미국 정부가 Amazon에 강제로 계정 정지를 요구할 리스크를 고려한다면 AWS를 쓰는 것이 위험할 수 있음. 최근 미국과 유럽(그린란드) 사이 전쟁 이야기가 나오는 상황에선 더 그렇다고 생각
          + 연간 $24,000라는 단순 계산법이 너무 순진하다고 생각. AWS에서 이 서비스들을 구축하는 데 몇 명이 필요한지, 본래 4만8천~10만 달러를 2만4천 달러로 줄이려면 인력이 얼마나 필요한지 등 구체적인 인력 비용 산정이 빠져 있다고 느낌
     * Prometheus, Grafana, Loki 조합만으로도 AWS에서 누리던 모니터링 수준을 직접 복제하거나 오히려 능가할 수 있었다고 생각. 이런 툴들이 이렇게 뛰어난데도 AWS의 모니터링 서비스는 비싸고 느리며 UX도 실망스럽다는 점이 항상 놀라움. 실제로 모니터링 비용 때문이 AWS 경험에서 가장 빠르고 불쾌했던 부분이었음
          + 실시간 로그 Live Tail 같은 단순한 기능조차 유료 서비스인 걸 보고 웃음이 나옴. 매일 로그를 보는 데 필수적인 기능조차 무료가 아니라니 CloudWatch(CW)는 정말 불편하다고 느낌
     * Hetzner의 주요 단점은 악성 사용자들로 IP가 오염되는 문제와 하드웨어 고장/업그레이드 필요성이 있음. 이런 점이 걱정되지 않았는지 궁금함. 또 Loki의 메모리 사용량 폭증 문제는 어떻게 해결하는지, 다른 대안은 없는지 궁금
          + IP 오염 문제는 Cloudflare를 통해 유저 접근을 프록시 처리하고, 방화벽(ufw)과 Cloudflare IP로 허용된 소스만 접속 가능하게 설정해 외부 접속 자체를 차단함. 하드웨어 장애/업그레이드는 Terraform 셋업으로 단시간에 교체 및 용량 확장 가능. Prometheus와 node exporter로 하드웨어 모니터링하며 사전 경보를 받고 있고, 9개월간 장애 없음. 앱은 데이터가 거의 없으며 데이터베이스는 빈번하게 복구 테스트함. Loki의 메모리 문제는 보관 정책과 인덱스 구분, 쿼리 동시성과 메모리 제한 튜닝, promtail 식 라벨 및 구조적 로깅 도입, 오래된 기록은 오브젝트 스토리지 백업이나 grep으로 대체 등 여러 방법을 조합해 해결
          + 우리가 경험한 Loki 문제는 기본 helm 등 배포 설정이 충분히 최적화되어 있지 않다는 데서 기인. 블로그에서 언급하는 퍼포먼스 팁대로 인덱스 재설정, 리드 전용 인스턴스 추가, 그 외 권장사항 반영 후 확실한 성능 향상을 경험함. 오픈소스보다는 자사 클라우드 서비스로 유도하려는 의도가 있으니 처음엔 삽질이 필요하다고 생각
          + Loki의 대안으로는 Victoria를 추천. 훨씬 빠르고 평판도 좋지만, 우리는 프로젝트의 유지보수자 다양성을 고려해 Loki를 선택. 위에서 얘기한 방법으로 단점 보완
          + https://en.wikipedia.org/wiki/Sybil_attack 링크 공유. 비싼 클라우드 업체가 PoW(작업증명) 방식 비슷하게 IP 평판을 갖추는 장점이 있음
     * ISO 27001은 국제 보안 관리 표준으로 유럽에서 인기 있는 지침임. 미국에서는 거의 적용되지 않고, 많은 유럽 회사들은 이 차이를 잘 못 받아들이는 경우가 있음. 미국 내 보안 표준의 기본은 SOC 2이며, ISO 27001보다 덜 엄격하고 미국 시장에 더 익숙함
          + ISO 27001은 원래 별로 딱딱하거나 빡빡한 기준이 아니고, 일반적으로 소프트웨어를 사용할 때 해야 할 기본을 요구함. 다만 실제로 문서로 증빙하는 게 까다롭고, SOC 2는 그에 비해 문서 작성 부담이 현저하게 적음
          + SOC 2와 ISO 27001 모두 경험해본 입장에서 SOC 2 심사는 실무 통제보다는 심사관의 역량과 직관에 좌우되는 측면이 많아 아쉬움. ISO 27001이 훨씬 명확하고 공정한 감사라고 느낌
          + ISO 27001 인증을 받지 않은 미국 클라우드 대기업을 하나만 꼽아달라고 질문
     * 나도 Azure로 비슷한 구성을 해 90% 절감함. 대기업이 고의적으로 복잡한 서비스 추상화 경험을 강요해 쉬운 운영이 점점 어려워지고 있다고 느낌
          + Azure의 비용 절감 사례에 대해 더 자세히 설명해달라고 요청
     * AWS에 비용을 지불하는 이유 중 하나는 운영 부담이 줄어든다는 점이고, 실제로 AWS의 관리형 DB를 쓰다 보니 예전처럼 mysql 클러스터 업그레이드에 스트레스를 안 느끼게 됨. 물론 이런 부분만으로 고비용을 정당화할 수는 없지만, 상당한 가치라고 생각
          + 이 지적에 공감. 실질적인 ISO 27001 인증을 하려면 업그레이드 프로세스도 내재화해야 효과적으로 개발/배포를 통제할 수 있음. 예를 들어 AWS RDS는 Postgres, MySQL의 메이저/마이너 업그레이드를 자동으로 수행하지 않고, 패치만 자동이고 나머진 내가 직접 해야 함. 클라우드/유럽 서버의 우위 비교가 아니라, 직접 복잡하고 인증된 환경을 운영하는 법에 대한 내용이 취지라고 봄. 고객이나 규제 비즈니스상 인프라 자율 관리를 해야 한다면 스스로 업그레이드하고 ISO 27001을 따는 게 맞지만, 그런 요구가 없다면 AWS RDS 등 클라우드 의존도 괜찮음
     * 숫자가 이해되지 않음. 연 $24,000에서 90% 절감해 월 $200이면 Hetzner 서버 1대 가격에 불과. 그런 상황이면 분산 시스템 없이 싱글 서버만 써도 될 것 같은데, 실제 초당 요청량이나 사용자 수가 궁금함
          + ISO 27001 준수하려면 싱글 서버로 운영이 안 되고 로그 및 모니터링 전용 별도 서버도 필요. 하중과 무관하게 반드시 복잡성이 따라옴. 직원들이 매일 로그인하지 않으며, 스케줄링 앱 특성상 주 1~2회만 확인하는 경우도 많음. DAU는 1만~2만, 피크 동시 접속은 1,500~2,000명 수준, 평균 동접은 50~150명. 클라우드 비용이 높아지는 이유는 실시간 기능과 복잡한 노동 규칙 등 앱에서 데이터 처리 부담이 큼. 예를 들어 교대 근무 배정 등은 보너스 계산 규칙까지 다르고, 스케줄 최적화도 연산량이 큼
          + $2,400이 아니라 $200임을 바로잡음
     * 디스크 암호화는 어떻게 하는지 궁금. AWS에선 자동이지만 일반 호스팅 업체에서 잘 구현하는 방법을 못 봤음. 암호화 키를 부팅 파티션에 저장하면 무용지물이란 점도 지적
          + ISO27001이 디스크 암호화를 반드시 요구하는 건 아니고 중요한 데이터의 적절한 보호 방법을 명시하면 됨. 특히 공유 하드웨어 환경에선 디스크 암호화가 가장 보편적인 수단. Hetzner는 ISO27001 인증 데이터센터에, 디스크 폐기 시 데이터 삭제 프로세스가 갖춰져 있으니 인증 요구사항을 충족할 수 있음
     * Hetzner를 정말 좋아해서 내 검색 엔진도 거기서 돌림. 물리서버를 쓰는 게 최고라고 생각
     * OVH, Hetzner 외에도 유럽 클라우드 중 UpCloud를 추천하고 싶음. UpCloud는 CPU 코어가 모두 실제 코어인 것 같고 vCPU(스레드 기반)가 아니라는 점이 장점. 다만 공식 참조가 부족해 아쉬움. OVH, Hetzner와 HyperScaler(초대형 클라우드) 비교는 쉽지 않은데, Hetzner의 경우 대부분이 소비자용 부품을 써서 차이가 있음. UpCloud 소개
          + 왜 저가 클라우드에는 진정한 IaaS 수준의 IAM(권한/정책/로그)이 항상 없는지 의아함. 콘솔 로그인만 있고, 진짜 퍼미션이나 역할, 머신 ID, 감사 로그까지 기본 제공이 안 됨. 오히려 OpenStack은 이런 기능을 이미 제공하는데 저가 클라우드는 다 다시 만든 느낌. UpCloud도 예시로 Crossplane 쓰는 가이드를 보면 API 크레덴셜을 콘솔 유저 레벨로 공유해서 위험해보임. Terraform으로는 관리가 힘드니 결국 upcli 등을 써야 하는 식. OpenStack Service Users OpenStack Federation UpCloud Crossplane 가이드 UpCloud subaccount 관리 UpCloud permission 설정
"
"https://news.hada.io/topic?id=21688","Diffusion 모델은 AAA 게임 개발의 새로운 선구자입니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Diffusion 모델은 AAA 게임 개발의 새로운 선구자입니다

     * Diffusion 기반 생성형 AI 모델이 차세대 AAA 게임 개발의 핵심 기술로 부상 중임
     * 기존 그래픽 중심 AAA 생태계는 한계에 직면하고, 고퀄리티 그래픽은 더 이상 차별점이 아님
     * 게임 개발 도구의 대중화와 그래픽 자산의 상품화로 인해 인디와 AAA 간 경계가 약화됨
     * 로컬 TPU와 모델 추론을 활용한 실시간 AI 기반 그래픽 생성이 새로운 프리미엄 게임의 무기로 부상함
     * 앞으로 AAA는 Diffusion 모델 안정화 및 일관성 확보에 집중하고, 이 과정이 차세대 게임 산업의 경쟁력이 될 전망임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ECONOMICS

     * 새로운 콘솔의 출시는 컴퓨팅 파워와 그래픽 향상이 핵심 동력이었음
          + 최근에는 포토리얼리즘 그래픽 발전의 정체로 세대별 비약적 향상 효과가 감소함
     * AAA 스튜디오는 그래픽 차별성으로 막대한 예산과 제품 가치를 정당화했으나, 이 모델이 흔들리고 있음

DEMOCRATIZATION AND COMMODIFICATION OF AAA

     * 게임 개발 툴의 발전으로 인디 개발자도 AAA에 근접한 그래픽을 구현할 수 있게 됨
          + Unreal 등 툴로 상위 80% 퀄리티는 쉽게 도달 가능함
     * 소비자는 그래픽 스펙터클 자체보다는 게임 경험을 더 중시하며, AAA 게임의 프리미엄 가치가 희석되고 있음
          + Among Us, Vampire Survivors 등 ‘low-fi’ 게임의 인기가 이를 반영함
     * 서구 AAA 스튜디오는 기존 고퀄리티 전략에 의존하며 위기를 맞고 있음
          + 동양 AAA는 AA/AAA 자원을 ‘낮은 미적 기준’의 게임에 투입하며 새로운 가능성을 모색 중임

DIFFUSION

     * 다음 세대 AAA 게임의 프리미엄 차별화 요소는 머신러닝, 특히 Diffusion 기반 생성형 모델임
          + Google의 GameNGen 등은 플레이어 입력에 따라 실시간으로 이미지를 생성하는 엔진을 시연함
          + 기존 API 기반 이미지 생성은 속도 한계(수천 ms)가 있으나, 로컬 TPU에서 16~20ms로 실시간 가능
          + 인간 평가자조차 실제 게임과 AI 생성 영상을 구분하기 어려울 정도로 품질이 높아짐

AN EXPENSIVE, CHALLENGING, AND REWARDING ROAD

     * Diffusion 모델의 일관성·안정성 확보가 차세대 AAA의 과제가 될 전망임
          + 인디는 불안정함도 감수하지만, AAA는 완성도 높은 ‘엔진’ 구축에 집중함
          + 프리미엄 게임의 ‘진입 장벽’이 다시 생길 수 있음
     * 개발 투자 예시
          + 게임엔진과 Diffusion 모델 통합 및 피드백 구조 구축
          + 오브젝트 주석, 이미지 OCR, 음성 캡처, 버퍼링 등 일관성 유지를 위한 다양한 툴 개발 필요
          + 월드 그라마 등 장면 생성용 프롬프트 구조 개발

BUT WHY DO THIS AT ALL?

     * Diffusion 기반 기술은 장면 전환·변형의 한계 비용이 거의 없고, 재사용성·맞춤화가 극대화됨
          + 한 번 구축하면 사용자는 원하는 세계, 설정, 스타일을 자유롭게 시뮬레이션 가능
          + 모델 업데이트만으로도 게임이 꾸준히 발전하고 무한한 조합이 가능함
     * AAA 스튜디오가 이 변화에 적응하지 못하면 도태될 위험이 큼

WHAT DIES

     * 영화적 연출 중심의 전통적 AAA 게임(GTA, 2D 플랫폼, 레이싱 등)은 Diffusion 모델에 의해 빠르게 대체될 가능성이 높음
          + 예: ‘나만의 도시에서 GTA’ 등 맞춤형 생성이 현실화될 수 있음
     * 정확성이 덜 요구되는 시각적 경험 게임은 큰 변화에 직면함

WHAT SURVIVES

     * 정확성과 시스템 기반 게임(전략, RTS, MOBA 등)은 Diffusion의 영향이 상대적으로 적음
     * 인디, 실험적, 시스템 드리븐 게임 등은 여전히 차별화된 가치를 지님

WHAT THRIVES

     * Diffusion 모델 덕분에 게임 장르와 시스템의 경계가 모호해지고, 상상력의 확장이 이루어짐
          + 예: 실시간으로 자동차 게임에서 경영 시뮬, 라이프 시뮬로 장르 전환 가능
     * AAA는 새로운 엔진, 미들웨어, 프랜차이즈 등 새로운 프론티어를 개척할 기회가 생김
          + 로컬 인퍼런스 및 AI 기술은 전통적 게임에도 다양한 방식으로 접목될 것임

BEYOND GAMES

     * 콘솔은 프로그래머블 TPU 내장으로 게임·AI 양쪽에서 플랫폼 우위를 재확보할 수 있음
          + TPU가 대중적으로 보급되면, 게임 외 AI 활용(아트, 크리에이티브 툴 등) 시장도 확대될 전망임
          + 새로운 콘솔 제품군(예: XBOX T) 등으로 변화할 가능성 있음

PROJECTING

     * 초기 Diffusion 기반 게임은 품질이 낮을 수 있으나, AI 발전 속도는 과거 콘솔 세대 교체보다 훨씬 빠름
          + Midjourney 등 이미지 생성 모델의 비약적 발전 사례와 유사함
     * AAA 붕괴와 인재 유출이 심화되는 현재, 새로운 패러다임을 선점하는 개발사가 차세대 리더가 될 것임
     * 앞으로 10~20년간 게임 개발 방식 자체가 재구성되고, 새로운 엔진·프랜차이즈가 대거 등장할 것임

결론

     * Diffusion 모델과 로컬 인퍼런스가 AAA 게임 개발의 차세대 프리미엄 프론티어로 부상함
     * AAA 스튜디오는 불안정한 생성 모델을 제어·안정화하는 기술력과 자본력을 기반으로 새로운 게임 생태계를 주도할 것임
     * 게임 개발·플레이 방식 자체가 완전히 재정의될 새로운 시대가 시작되고 있음
"
"https://news.hada.io/topic?id=21577","EU, Microsoft Azure에서 프랑스 OVHcloud로 전환 검토","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               EU, Microsoft Azure에서 프랑스 OVHcloud로 전환 검토

     * EU Commision(유럽연합 집행위원회) 는 Microsoft Azure에서 프랑스의 OVHcloud로 클라우드 서비스를 전환하기 위한 사업 협상을 진행 중임
     * 이 결정의 배경에는 미국 행정명령으로 인한 서비스 중단 경험과 함께, 유럽 내 디지털 주권 확보 필요성이 주요 원인으로 작용함
     * 집행위는 유럽 기관들이 데이터와 인프라를 자체적으로 통제할 수 있어야 한다는 점을 강조하며, EuroStack 등 주권 인프라 구상을 적극 추진 중임
     * 현재 OVHcloud 외에도 독일의 IONOS, 프랑스의 Scaleway, 이탈리아의 Aruba 등 유럽 내 다양한 사업자들도 고려 대상임
     * 디지털 정책 담당 DG CNECT와 IT 총괄 DG DIGIT이 하나의 커미셔너(현재 Henna Virkkunen) 아래 통합된 점이, 정치·기술적 우선순위 조율을 용이하게 만들었음

집행위와 OVHcloud의 공식 입장

     * OVHcloud 측은 ""집행위 및 기타 기관들과 주권 클라우드 이전 관련 논의가 실제로 이뤄지고 있다""고 확인함
     * 집행위원회는 ""시장조사를 지속하고 있으며, 이미 OVHcloud와의 계약도 보유하고 있다""고 언급했으나, 실제 Azure 전환 여부에 대해서는 명확히 밝히지 않음

배경 및 관련 논의

     * 2025년 1월, 집행위가 Microsoft에 대한 과도한 의존성을 우려한다는 내부 문서가 공개된 바 있음
     * 유럽 데이터 보호 감독기구(EDPS)는 지난해 집행위가 일부 데이터를 Microsoft Azure로 처리함에 있어 EU 데이터 보호 규정 위반 소지가 있다고 지적함

전망

     * 집행위가 성공적으로 주권 클라우드로 전환할 경우, 각국 공공기관의 유럽 클라우드 도입 확대에 선례를 제공할 것으로 기대됨
     * 유럽의 디지털 자율성 강화 및 비유럽 기술기업 의존도 축소가 주요 정책 목표로 부상함

        Hacker News 의견

     * 나는 대부분의 보수적 시각에는 동의하지 않지만, 국가가 자체 기술에 투자하는 흐름은 정말 필요하다고 생각함
       미국 빅테크에 비해 유럽 토종 서비스의 성숙도가 부족하다는 지적이 있을 수 있지만, 건강한 내수 소비로만 그 경험과 발전 가능성이 생긴다고 봄
       EU가 자체 서비스를 쓰고, 이게 지역 경쟁을 자극한다면 발전 가능성 충분함
     * 외국 정부가 자국 핵심 기술 인프라를 다른 나라, 심지어 우방에까지 의존하는 것이 항상 의문이었음
       단순히 돈이 아니라 실제 자립성이 중요한 논점임
       EU가 자체 기술 기업과 역량에 더 많이 투자할 필요가 있다고 생각함
       그러나 지금 유럽에 미국 수준의 FAANG 기업이 없는 건 반기업/반스타트업적 정책 탓이 큼
       정책의 선의와는 별개로 실제로 영향을 미치고 있으니, 유럽 창업자들의 실제 경험담도 궁금함
     * ""성숙한"" 미국 소프트웨어도 별로인 제품 많음
       Epic Systems는 영국, 덴마크, 핀란드, 스위스, 노르웨이 등지에서 완전 실패 사례임
     * 흥미로운 점은, 최근 AWS 데이터센터가 마지막으로 언제 불탄적이 있는지 기억나지 않음
       미국 클라우드에 의존 줄이자는 데 동의하지만, OVH는 또 너무 극단의 선택임
     * 내 주장은 동의하지만, 너는 OVH를 한 번도 써본 적 없는 듯한 뉘앙스임
     * 사실 그들(유럽)도 AWS를 많이 씀
     * EU가 국가 주권적 활동에서 미국에 과도하게 의존하는 건 무책임한 일임
     * 지난 3월 이에 관해 글을 쓴 적 있고, 클라이언트가 AWS로 이전하자고 한 이래 계속 이 문제 제기함
       참고글: https://green.spacedino.net/software-is-not-the-service/
       내 경험상 해당 클라이언트는 왜 2U 서버를 AWS에 3배 가격 내고 둬야하는지 설명 못함, 그냥 반드시 그렇게 해야 한다는 식임
       이후로도 자주 이런 케이스 봤음
       EU가 클라우드 독립에 성공하길 바람, 경쟁이 더 많아지면 인권 면에서도 더 나은 환경 생길 전망임
     * Azure Europe의 데이터센터 위치가 노르웨이, 독일, 네덜란드, 프랑스 등 다양함
       미국 주권 전용 서비스는 Azure US Government뿐이고, 유럽엔 이 서비스가 런칭되지 않음
       예전엔 Azure Germany처럼 주권형 서비스 제공한 적도 있음
       단, 기능 롤아웃은 미국보다 유럽이 항상 느림
       이런 논리라면 Microsoft Office 의존도 문제 삼을 수 있고, LibreOffice 쓰는 기관도 있지만 완벽한 대체가 아님
       Dell 등 미국 PC 기업에 대한 의존도 마찬가지
       Microsoft가 미국 CLOUD Act 대상이라 정책입안자들이 주목하는 것임, 본질은 반-Azure가 아니라 민감 정보 직접 통제 쪽임
     * 예전에도 사실이었지만 최근은 더욱 그렇다고 생각함
       EU와 미국 간 개인정보 및 데이터 보호에 대한 인식 차이는 계속 악화 추세임
     * OVH가 AWS, GCP, Azure 등 미국 클라우드 대기업과 인프라 질과 네트워킹 모두 비교 불가
       저렴한 가격 위해 모든 지점에서 비용 아끼는 경향이 강함
       목재 건물에 화재진압설비도 없어서 데이터센터가 전소했고 수많은 기업 데이터 날린 사건이 있음
       참고: https://datacenterdynamics.com/en/opinions/…
     * OVH가 미국 클라우드 대기업 수준에 전혀 미치지 못하는 점 인정
       Microsoft도 별반 다르지 않음 : https://geekwire.com/2018/…
       캐나다에 있어도 텍사스 데이터센터 외엔 백업 도메인 컨트롤러 없어서 Azure DevOps 접속 못했던 적 있음
       OVH도 이번 사건에서 교훈 얻어서 EU 투자로 앞으로 서비스 개선 기대함
     * 저가 추구한다고 무조건 나쁜 건 아님
       다만 Azure 최근 보안 문제와 비교해 보면, Microsoft도 많이 허술해짐
     * OVH는 정말 사고가 끊이지 않는 서비스임
       그렇다고 EU가 자국 서비스를 더 많이 써야 한다는 데에는 동의함, 이런 계기로 OVH도 결국 발전할 기회임
     * 데이터 복구과정도 문제였음
       전부 세척 후, 언론플레이 한 다음 ""잘 세척했다"" 선언식
       내 입장에서 그렇게 하는 곳은 절대 신뢰 못하겠음
     * 최근 행정부의 친유럽, 친민주국가 적대감이 뚜렷하게 보이고 있음
       테크 CEO들이 이쪽(미국 행정부)과 너무 밀착하자, EU 입장에선 독립 필요성을 느끼게 되는 계기가 되었을 것임
       앞으로 더 많은 나라들이 따라갈 수도 있음
       각국 자원이 부족한 국가는 특히 인프라 다각화에 관심 높아질 수 있음
     * 미국이 서비스 수출의 중요성을 이해하지 못하는 것도 동기의 일부임
     * 트럼프 행정부가 통제할 수 있는 것에 의존하지 말아야 할 명분이 충분하다고 생각함
     * 미국 테크 경영진들도 트럼프 행정부와 친하게 지내는 걸 후회하기 시작할까 궁금함
       미국이 고립된다면 테크업계도 같이 위험해질 수밖에 없음, 결국 자기 탓이 될 거임
     * 인텔, AMD, Nvidia, Ampere, Qualcomm이 없다면 배포할 수 있는 서버가 없고, EU가 구매할 수 있는 클라이언트 장치도 거의 없음
       주권 공급망이 없다면 내 입장에서 차라리 중국보단 미국이 나음
     * 이 이슈는 트럼프 개인보다는 그 이전부터 계속된 일임
     * 기업 입장에서 AWS/Azure 등에서 빠져나오려는 분위기 확산됨
       이것만이 절대적 이유는 아니지만, 점점 더 많은 세력이 있다고 체감함
       가장 큰 두 어려움은
         1. 기술 인력 전환
         2. 이관 및 중단 리스크
            저희 회사는 유럽 프로바이더(Hetzner 기본, 조건에 따라 온프레미스도 가능)에 bare-metal Kubernetes 배치 서비스를 하며, DevOps 엔지니어와 마이그레이션 플래닝도 지원 중
            특히 중소기업/스타트업들이 보다 쉽게 전환하도록 실무 엔지니어와 융합해 문제 해결 지원
            지속적 개발과 병행해 이관할 수 있게 돕고 있음
            더 궁금하면 adam@ 도메인으로 연락주길
            홈페이지 : https://lithus.eu
     * 디지털 서비스가 미국에 너무 몰려 있는 현실, 결국 유럽이 자체 서비스 확장하는 흐름이 생겨서 기쁨
     * (대댓글) 유럽에서도 실제로 Azure Europe을 안 쓰는지 궁금증
     * OVHcloud에서 한 달 400유로짜리 서버 빌리려고 했더니, 처음엔 저렴한 서버를 몇 달 써야 대여 신청 가능하다고 거부당한 경험 있음
     * AWS도 똑같이 GPU EC2 인스턴스를 바로 배포 못 하게 함
       신규 계정이면 지원팀에 프로젝트 설명하고 예산 관리 계획 제출해야 함
     * 사실 거의 모든 VPS 업체가 이런 정책임
     * EU 기업에서도 비슷한 경험 많음
       저렴한 서비스 먼저 쓰다 생산성 저하로 오히려 총비용 늘 수도 있는데, 미수금 우려 때문인지 선불이나 예치금도 안 받으려 해서 답답했음
     * OVH 퍼블릭 클라우드 신청했다가 동일한 이유로 거절당한 사례 있음
       객체스토리지와 온디맨드 컴퓨팅 쓰고 싶었지만 이 정책이 완전한 방해 요소였음
     * 이제는 무엇이든 일단 시작해서 하나씩 이전하면 그 다음 단계로 넘어가기 쉬움
       실제로 해보면 가능성 확인 가능
     * 예전에는 사내에 직접 서버 돌렸던 시절이 있었음
       다들 이 방식으로 다시 되돌아가면 재밌을 것 같음
     * 결국 모든 미국 기업이 점점 정권(정부)의 조종을 받는 꼭두각시로 전환되는 흐름으로 가고 있다고 봄
     * ""근거 필요(=Citation needed)""
"
"https://news.hada.io/topic?id=21643","메릴랜드 컬리지파크 국립기록보관소, 연방 시설로 출입 제한 예정","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  메릴랜드 컬리지파크 국립기록보관소, 연방 시설로 출입 제한 예정

     * 메릴랜드 컬리지파크 국립기록보관소가 곧 출입이 제한된 연방 시설로 운영될 예정임
     * 기록보관소 연구실 운영 시간은 월요일부터 금요일, 오전 9시~오후 5시임
     * 연구는 예약이 권장되지만, 현장 방문을 통한 워크인 연구도 가능함
     * 각 연구 분야별로 텍스트·지도·사진·영상 등 전문 상담 이메일을 통해 사전 문의 및 예약 상담이 가능함
     * 현장 연구에 관한 자주 묻는 질문(FAQ) 이 별도로 제공되고 있어 참고 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

메릴랜드 컬리지파크 국립기록보관소 안내

  주소 및 연락처

     * 8601 Adelphi Road, College Park, MD 20740
     * 트럭 배송 입구: 3301 Metzerott Road
     * 고객 서비스 센터: 1-866-272-6272
     * 분실물 문의: 301-837-2900
     * 이메일: inquire@nara.gov

운영 시간

     * 연구실 운영: 월요일–금요일, 오전 9시–오후 5시
     * 연구 예약: 사전에 예약하는 것이 강력히 권장됨
          + 예약 없이 방문하는 워크인 연구도 가능함
          + 연구 예약은 Eventbrite의 National Archives DC-area Research Appointments 페이지에서 진행 가능함

연구자 상담 및 등록

     * 텍스트 자료 상담: archives2reference@nara.gov로 사전 상담 가능함
     * 연구자 등록 및 연구실 예약 문의: visit_archives2@nara.gov으로 문의 가능함

특수 미디어 분야별 상담 이메일

     * 지도 자료: consultation.carto@nara.gov
     * 사진 자료: consultation.stillpix@nara.gov
     * 영상 및 음향 자료: mopix@nara.gov

추가 안내

     * 현장 연구와 관련된 정보는 FAQ(자주 묻는 질문) 페이지에서 추가 확인이 가능함

        Hacker News 의견

     * 사이트 내용 인용문 기준, 모든 연구자는 Room 1000에서 연구자 카드를 신청하고 제시해야 하는 규정 도입 내용 확인, 이 방식은 건물 접근 시 정당한 사업 목적 확인을 위한 신원확인 목적임을 명시, 그러나 “정당한 사업 목적”이라는 문구의 의미가 모호하다고 생각, JFK 암살 관련 기록 같은 경우 어떤 ‘사업적 필요’가 있는지 의문 제기, 박사 학위나 책 집필 목적으로 자료를 접근하는 것도 여기서 인정되는지 확신 어렵다는 의견, 또한 연구자 등록을 악용하면 출입 금지 및 영구 출입 금지까지 언급된 문장이 문법적으로 이상하게 느껴짐
          + NARA 2 College Park에 여러 번 방문 경험 있음, 이제는 자료를 실제로 요청하는 연구자만 건물에 입장할 수 있다는 의미로 해석, 안내문이 다소 어색하게 느껴짐, 사실상 NARA 2 연구자 접근까지 막긴 어려울 거라는 전망, Room 2000이 주요 열람실이고 매우 큰 규모인 곳임, 이 건물은 방문객과 연구자 방문을 염두에 두고 설계된 점 강조, NARA 2 자체가 원래 매우 보안이 까다로운 곳임을 경험상 설명, 2019년 마지막 방문 당시 건물 입구에서 1차 수색, 지하 라커룸에서 대부분 장비 보관, 노트북과 스캐너/카메라만 지참해서 1층으로 이동, 다시 수색, 엘리베이터 타고 2층 Room 2000에 올라가면 또 한 번 수색, 그 다음 자리에 앉아서 자료 요청(복사용 3중 폼 사용), 자료실 나올 때 또 수색 경험, 최근엔 수색 절차 중 하나가 2층 엘리베이터 앞이 아니라 입구로 통합될 것
            같다는 추측, 주차장 이용 불가와 대중교통 접근성이 더 어려운 문제는 본인이 직접 버스도 타봤기 때문에 동의, 그래도 버스 운행에 익숙해질 수 있을 것임을 인정
          + NARA에서 근무 중인 파트너가 있지만, 해당 사무소는 아니라고 전제, 최근 기관 내 퇴직자 다수 및 구조조정 시행 상황 하에서 길거리에 있는 일반 방문자가 명확하지 않은 질문을 할 때 대응에 어려움이 많다는 것 전달, 스태프는 질문의 퀄리티에 상관 없이 모두 답변해줘야 하는 의무가 있음, 연구자 카드 도입 목적은 1) 아무 준비 없이 방문해 도움을 요청하는 사람에게 간단한 행정 장벽을 만들고, 2) 연구와 관련 없는 의도를 가진 사람의 접근 거절 근거를 제공하는 데 있다고 봄, 실제로 박사나 책 집필 같은 예시는 사업적 필요로 인정될 가능성이 높다고 생각, 즉, 예약 혹은 사전 상담을 받은 방문자의 원활한 업무 지원을 위한 조치임을 설명, 애매한 문장 구조에도 공감하며, 더 명확했으면 좋겠다는 바람
          + “business need”라는 단어를 상업적 필요로 해석할 필요는 없으며, “여기서 무슨 용무로 오셨나요?”에 가까운 뉘앙스임을 설명, 마페트 연방 비행장 항공사 박물관 방문 경험을 통해, 보안요원이 박물관 방문은 business purpose로 인정하지만, 단순 자전거 타기는 용무가 아니라고 설명한 사례를 예시로 듦
          + 실제 연구자 카드 발급 과정은 어떤 사업적 필요도 언급하지 않으며, 신분증 제시와 교육 동영상 시청만 요구함을 밝힘, 공식 안내 페이지에 따르면 학생증도 인정한다고 명시되어 있음, 다만 해당 페이지가 최신 정책을 반영하지 않았을 가능성도 있다는 점 언급
          + 이 규정의 의도는, 더 이상 오토바이를 타고 나타나서 Area 51 외계 우주선 문서나 Zapruder 필름의 음모론 자료 같은 요구를 즉석에서 할 수 없게 만들려는 것임을 유쾌하게 설명, 하지만 누구든 자신의 ‘발칙한’ 이론을 추구해도, 연구라는 정당한 프로세스만 거치면 주제나 목적에 대한 가치판단 없이 계속 접근이 가능하다고 추측
     * NARA(국립문서관리청)는 예산 부족으로 운영 및 디지털 트랜스포메이션 우선순위 사이에서 고민 중임을 설명, 2025년 예산감축 발표 기사와 올해 3곳 시설을 폐쇄한 공식 발표자료 링크 공유, 목표는 모든 자료를 디지털화하여 더 넓은 접근성 확보에 두고 있었음을 지적, 최근 행정부에서 NARA 예산을 축소했고, 2월에는 ""Archivist of the United States"" 직책의 Shogan까지 해고했으나, 이미 그 전부터 전략적 변화 추진 계획은 진행 중이었음을 전함
          + KTLO란 ‘Keep The Lights On’의 약자임을 설명
     * 연구자 카드 발급 경험자로서, 과정이 어렵지 않고 직원들이 매우 친절하게 응대한 경험 언급, 워싱턴DC 국립문서관에 가면 늘 전문 장비와 노트북을 들고 다니는 20명 정도의 프로 연구자 집단을 목격할 수 있다고 소개, 접근제한이 그다지 큰 문제는 아니라고 생각, 공개된 상태이지만 관리 방식이 더 엄격해진 것임을 설명, 과거 기록물 도난·파손 사례를 고려할 때 보안 강화를 긍정적으로 평가
          + 원본 문서는 더 철저한 봉인(예를 들면 동굴 저장), 스캔본은 최대한 공개를 바람, 대용량 토렌트 방식으로 모두가 한번에 열람할 수 있기를 희망
          + 실제로 일반 방문자도 독특한 아카이브 자료를 볼 수 있는지, 아니면 늘 복사본만 볼 수 있는지 궁금함, 설령 0.1%만 문제가 있어도 수천 건에 달하는 기록이 훼손되거나 사라졌을 수 있다고 추정
     * 이번 규정 변경은 College Park MD 소재 National Archives에 관한 것임을 설명하며, 위키백과 참고 링크 공유
     * 미국 전역에 여러 National Archives 지점이 있고, 공식 위치 안내 링크 공유, 이번 변경은 College Park, MD에만 해당함을 짚음
          + 혹시 이곳이 첫 시작일 수도 있다는 의견 제시
     * 이용자 접근이 실제로는 (유명한 작품 대사 인용처럼) “조심! 표범주의” 같은 안내가 붙은 쓸모없는 화장실 서랍장에 감춰진 것처럼 느껴진다는 익살스러운 표현
     * 왜 이런 조치가 필요하냐는 질문
          + 정부에서 합리적 설명 논리를 기대하지 않는다는 씁쓸한 현실 자조, 접근 제한이 독재에 빨리 다가가는 데 도움이 될 수 있다는 추측
          + 국립문서관 자료 보호 목적 규정 일부를 인용 소개: 자료 도난, 훼손, 오분류, 부적절한 정보 공개로부터 보존하는 목적
          + 조치에 대한 이유를 제공할 필요조차 느끼지 않는 듯한 소극적인 대응 지적, 연구자 등록을 요구하는 공식 설명문 전체 인용
          + 가장 유력한 배경 원인으로 인력 감축 가능성 제시, 관련 기사 링크 공유
     * 영국 국립도서관 등 저작권 도서관은 오래 전부터 연구 목적 등록 혹은 합리적 용무 소명자에 한해 접근을 허가해왔음을 설명, 희귀하고 맥락상 고유한 사료를 보존하는 곳이므로, 일반 박물관 전시품과 역할 자체가 다름을 강조, 정부 관련 정보 접근은 국민 권리임을 믿지만 인력 절감·리스크 관리 등으로 제한 조처가 글로벌 트렌드이며, 신원 및 목적 확인 절차 자체가 아카이브에서 이례적인 일은 아니라고 설명
          + 미국은 과거엔 아주 어린 나이에 Library of Congress에 혼자 들어가서 희귀책을 요청하기도 했던 경험을 공유, 본인은 그 방식이 더 좋았다는 소회
          + 미국 Library of Congress에서도 특정 소장품 열람 시 오래 전부터 등록 카드를 요구해왔다고 언급, 카드 발급 안내 링크 첨부
     * National Archives _Museum_과 혼동 금지, 이곳은 여전히 독립선언서 등 중요 문서의 일반 관람이 자유로움
     * 제목을 “National Archives at College Park, MD to restrict public access starting July 7”로 업데이트하면 좋겠다는 제안
          + College Park 지점이 가장 중요하고, 방문자 수도 많으며 ‘가장 흥미로운 자료’를 소장한 곳임을 강조, 캔자스 지점 등은 아는 사람이 없다는 농담도 언급
          + 실제로 제목이 비슷한 방향으로 업데이트 되었음을 알림
"
"https://news.hada.io/topic?id=21571","Kubernetese 2.0은 어떤 모습일까","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Kubernetese 2.0은 어떤 모습일까

     * 쿠버네티스는 지난 10년간 컨테이너 오케스트레이션 혁신을 주도했음
     * 현행 YAML 구성, etcd 의존성, Helm 패키지 관리 등에서 다양한 한계점과 개선 필요성이 관찰됨
     * HCL 도입, 플러그형 저장소, 새로운 패키지 관리자(KubePkg), IPv6 기본화 등이 쿠버네티스 2.0의 변화 요소로 논의됨
     * 현재의 오픈형 구조도 중요하지만, 기본값과 공식적인 방향 제시가 생태계 혁신에 결정적 영향을 미침
     * 쿠버네티스는 지속적인 파괴적 개선을 통해 더 넓은 사용자와 조직에 적합해질 필요성이 확인됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

쿠버네티스의 10년: 성공과 한계

  시작과 성장

     * 2012~2013년 Google 내부의 Linux 컨테이너 시스템인 Borg에 대한 소문이 sysadmin 커뮤니티에 퍼지기 시작함
     * 2014년 쿠버네티스가 공개되었고, 초기에는 발음조차 어려운 이름이었음
     * 마이크로소프트, RedHat, IBM, Docker 등 주요 기업이 빠르게 합류하며 쿠버네티스가 생태계 표준으로 자리 잡기 시작함
     * 2015년 v1.0 공개 및 CNCF 출범으로 본격적인 개방 생태계 조성

  주요 혁신 포인트

    컨테이너의 대규모 관리

     * 쿠버네티스는 컨테이너 기반 소프트웨어 개발과 배포의 확장성을 제공함
     * 단순한 개발 환경에서부터 수천대 서버에 이르는 대규모 동일 환경 배포를 가능하게 함
     * 조직들은 모놀리식 구조에서 분산형 마이크로서비스로 전환하는 계기를 마련할 수 있었음

    저비용 유지보수와 자기 치유

     * 서버 관리가 ""Simpsons"" (각 서버마다 별명, 수작업 운영) 시대에서 ""UUID"" (완전 교체, 자동화, 무상태) 시대로 급변함
     * 머신 수명, OS 지원 기간 개념이 거의 사라졌으며, 고장 시 ""노드 재생성""으로 자동 재구성됨
     * 많은 리눅스 스킬이 이제 필수 조건이 아닌 ""있으면 좋은"" 수준으로 변화함

    배치 작업과 Job 관리

     * 과거 ""cron 전용 박스""에 의존하던 배치 처리가, 큐 기반 자동화 작업과 재시도 기작으로 대체됨
     * 작업 실패 시 자동 재시작과 유연한 처리로 운영 효율성과 개발자 만족도를 크게 높임

    서비스 디스커버리와 로드밸런싱

     * IP 주소 하드코딩 대신 내부 DNS 기반 서비스 명명 체계를 도입
     * API, 고정 IP, 호스트명으로 서비스 간 연결을 단순화하고, 외부 서비스도 내부처럼 다룰 수 있게 함

쿠버네티스 2.0: 핵심 개선안

  YAML에서 HCL로, 구성 언어 교체

    YAML의 한계

     * YAML은 보기 쉽고 JSON, XML보다 나아 보이나, 오류 유발, 타입 미지원, 디버깅 어려움 등 심각한 문제 존재
     * 예시) 노르웨이 문제('NO'가 false로 해석), 인덴트 에러, 문자열 숫자 혼동 등 수많은 장애 요소 내재

    대안: HCL 채택

     * HCL(HashiCorp Configuration Language) 은 Terraform의 표준 포맷으로, 강타입, 검증, 함수, 조건 분기 등 우수한 기능 제공
     * 이미 상당수 쿠버네티스 클러스터가 Terraform과 함께 HCL을 활용 중임
     * HCL은 타입 안정성, 중복 감소, 동적 설정, 반복 처리, 조건 논리, 뛰어난 주석 지원, 모듈성, 유효성 검증 등 YAML보다 강력한 기능 보유
     * 오픈소스 라이선스 문제(MPL 2.0 → Apache 2.0과의 호환성)는 남아있지만 품질 향상을 위한 가치가 충분함

      예시: HCL vs YAML

     * 타입, 변수, 함수, 조건문, 반복문 등 HCL의 장점 덕분에 구성 오류 미연 방지, 유지관리성, 복잡한 환경에서도 일관성 확보

  etcd 대체 옵션 지원

     * etcd는 강력하지만, 소규모 클러스터나 저사양 환경에는 과도한 자원 사용 발생
     * Kine 등 플러그형 백엔드 공식 지원을 통해 기본 저장소의 유연성 확대 필요성 제기
     * 예시) k8s-dqlite: 분산 SQLite+Raft로 가볍고 유연한 데이터 계층 제공

  Helm을 넘어서: 네이티브 패키지 매니저

    Helm의 한계

     * Helm은 임시 해킹 솔루션이 표준이 된 경우로, 템플릿 복잡성, 디버깅 난이도, 의존성/버전 충돌, 검증 미흡 등 다양한 문제점 존재
     * 여러 차트 중복, 버전 불일치, 네임스페이스 간 설치 난점, 차트 검색/메타데이터 부재, 시맨틱 버전 미준수, 비안전한 CRD 관리 등 실전 활용에서 다수의 어려움 발생

    새로운 패키지 시스템 제안: KubePkg

     * 쿠버네티스 리소스 형태로 패키지/의존성/시맨틱 버전/시큐리티/생명주기/메타데이터까지 포괄 관리
     * 정교한 라이프사이클 Hook, 상태 및 백업/복구 전략, 선언적 구성, 검증 가능한 서명 프로세스, Audit 기록, 조직 정책 통제 등 완비
     * 리눅스 패키지 매니저의 장점을 계승하여 실제 인프라 관리에 강한 신뢰성과 일관성 제공 목표

    주요 요구 조건

    1. 완전한 쿠버네티스 네이티브 리소스 구조
    2. 상태 기반 애플리케이션 내장형 지원
    3. 서명/검증/보안 스캐닝 프로세스 강화
    4. 구조화된 선언적 구성 + 스키마 검증
    5. 전 생명주기 통제와 간편한 Upgrade/Hook 지원
    6. 리눅스 스타일의 의존성/버전 해석
    7. 변경 이력 및 감사 추적
    8. 조직별 정책 적용 가능성
    9. 친숙한 리눅스 패키지 관리 UX 제공

  IPv6 기본값 전환

     * 현재 IPv6와 dualstack이 지원되지만, 디폴트는 여전히 IPv4임
     * 클러스터 간 통신, NAT 문제, IP 부족 사태를 해결하고 단순한 네트워크 구조, 내장 IPSec 등 장점이 있음
     * 대규모 IP 필요 환경(예시: /20 대역, 노드 40개, 1개 노드에 30개 pod)은 빠르게 IPv4 한계에 봉착
     * 퍼블릭 IPv6 환경에서의 운영 단순화, 클라우드 사용자 확보, 사설 영역 관리 부담 감소 등 실용적 이점 분명

결론: 변화의 필요성과 기본값의 힘

     * ""오픈 플랫폼이니 커뮤니티가 알아서""라는 의견도 있으나, 기본값이 업계 행동 양식을 주도함
     * 쿠버네티스 2.0에는 공식적 신념, 우수한 설계, 강력한 기본값이 필요한 시점임
     * 업계의 표준이자 글로벌 데이터센터 운영 대세 지위에 맞는 참신한 도약이 중요한 시기임
     * 이미 모바일, 웹 등 다양한 기술 영역에서 과감한 변화로 전체 생태계 혁신 경험이 반복적으로 등장
     * 쿠버네티스도 이제 기념비적 2.0을 통해 다음 단계를 고민할 시점임

   오픈소스를 엮어만든 바닐라 k8s를 구성하고 운영할 수 있는 엔지니어는 앞으로도 소수일것같아요.
   하둡클러스터를 클라우데라 플랫폼으로 묶어서 구매하지않고 바닐라의 하둡에코시스템을 운영할수있던 엔지니어가 극 소수였던것처럼.

        Hacker News 의견

     * Kubernetes의 가장 큰 문제점으로는 ""그냥 잘 되는"" 시스템이 아니라는 점을 꼽고 싶음. 실제로 프로덕션에서 무리 없이 서비스 운영이 가능한 엔지니어는 소수에 불과하며, 직접 VM 위에 Kubernetes 클러스터를 구축하고 유지하는 건 더더욱 어려운 일임. 그래서 요즘 뜨는 서버리스 스타트업들은 (1) 시간 소모가 많고, (2) 매우 오류가 많으며, (3) 프로덕션 환경에서 실패 확률이 높다는 인식이 퍼졌기 때문임. Kubernetes 2.0에서는 누구든 쉽게 배포 플랫폼을 셀프호스트하고 자신 있게 사용할 수 있으며, 작고 강력한 오케스트레이터 코어를 유지하는 방향을 고민해야 한다고 생각함. Rivet라는 오케스트레이터와 배포 플랫폼을 직접 개발 중이며, Rivet 오픈소스 서버리스 플랫폼임을 내세우지만, 스스로는 “Kubernetes 2.0이 뭘까?”라는 고민을 자주 함. 사람들이 기대
       이상의 시나리오에 Rivet를 도입하는 케이스도 늘고 있음. Rivet의 최대 강점은 Kubernetes controller 수준의 기능을 손쉽게 개발 가능하다는 점이며, 이를 통해 게임 서버, 테넌트별 배포, 고급 워크로드 오케스트레이션, 멀티테넌시, 테넌트별 과금, 강력한 오퍼레이터 설계 등 다양한 시나리오에 활용 가능함.
          + 이런 관점, 참 반감 듦. 나이도 많고 마음이 약간 냉소적이라서 그런지, 자주 보게 되는 패턴임. X 기술이 너무 무거워서 ""나는 그냥 이거 없이 노트북에서 간단히 돌리고 싶다""라는 사람이 Y 기술을 만듦. 그러다 Y도 인기를 얻고, 실제로 노트북이 아닌 곳에 돌릴 때 확장성이 필요해지면서 다양한 기능이 추가되어 무거워짐. 그러면 누군가는 ""이젠 Y도 너무 무겁다""면서 같은 말을 반복함. 이건 마치 '시간의 바퀴'처럼 시작도 끝도 없는 반복되는 이야기임.
          + 실상은 k3s(경량 k8s)는 유지 관리가 정말 쉬움. k3s 자동업데이트, 적절한 이빅션 룰만 있으면 거의 손볼 일이 없으며, Ceph 같은 스토리지도 어렵다면 Lemon이나 Longhorn 같은 “관리 제로형” 대안 선택 가능함. 수천 개의 헬름 차트가 존재해서 복잡한 데이터베이스도 1분이면 바로 배포 가능함. 서비스 배포도 많이 쓰는 헬름 템플릿만 쓰면 매우 쉬움. 헬름이 완벽하진 않지만, 원하는 대로 세팅하면 값 완성 기능 등도 누릴 수 있음. 진입장벽이 서버리스보단 약간 높지만, 일주일 정도 배워서 실제 프로덕션에서 수천만 원 절약하는 건 충분히 가치 있는 일임.
          + Kubernetes가 해결하는 문제는 ""이걸 어떻게 배포하지?""임. Rivet의 Docs를 보니 싱글 컨테이너, Docker Compose, 수동 배포(도커 커맨드)밖에 없음. 현실적으로 서버리스 인프라를 실제 규모로 이렇게 배포할 수 있는지 의문임. 내 첫 번째 의문은 ""이쯤 되면, Rivet을 Kubernetes 위에서(컨테이너, kube-virt 등) 돌릴 수는 없을까?""임. Docker Compose가 Kubernetes보다 더 견고하거나 확장성이 좋을 수 있는지 모르겠음. 그렇지 않고 클라우드 서비스를 판다면, 이건 Kubernetes 2.0 컨셉과도 맞지 않음. 만약 Rivet을 직접 호스팅한다면 문서를 바꿔서 Kubernetes에서 돌릴 수 있게 할 것 같음.
          + 클러스터 유지관리를 클라우드 벤더에게 아웃소싱한 “서비스형 k8s”를 사용한다면, 어떤 부분이 그렇게 복잡하게 느껴지는지 궁금함. 설정의 범위는 넓지만, 실제 프로덕션에 넘길 서비스 구성에는 소수의 설정만 알면 충분함. Docker Compose보다 약간 더 복잡한 수준의 설정이면 충분히 k8s에 배포 가능함. 하지만 대다수의 앱들에겐 이런 “조금 더” 조차도 불필요할 수도 있음. 사실 Docker Compose가 원했던 대중적인 니즈인데, 지속적인 관심을 못 받은 점이 아쉬움.
          + 인프라 운영에서는 “그냥 잘 되는” 게 절대 나올 수 없다는 게 내 경험임. Heroku조차도 확장에서 이슈가 생김. 모두가 쉽게 채택할 수 있는 배포 플랫폼을 원한다면 Kubernetes를 특정 PaaS의 기반 프리미티브로 이해하는 게 더 현실적임. Rivet는 독특하고 BEAM생태계의 몇몇 아이디어도 보여서 흥미로움. 개인적으로는 대규모 배포보다는 탄탄함과 로컬-퍼스트에 더 관심 있음.
     * “Low maintenance”라... EKS(관리형 k8s)를 많이 쓰고 있는데, 클러스터 상태에 대해선 직접 신경 쓸 필요는 없음(물론 내가 노드를 망치는 신박한(?) 방법은 별개). Kubernetes는 정말 “최대한 유지관리 필요 없는” 척하지만, 실제로는 순수 유지관리 덩어리임. yaml만 내밀면 소프트웨어를 세상에 바로 배포할 수 있다는 점은 정말 대단함. 하지만 대가가 바로 복잡한 유지관리임. 클러스터 세팅, ArgoCD 초기화, 허브-스포크 모델에서 다른 클러스터 등록 등은 서커스 한장면일 뿐임. 거기에다가 CNCF Landscape tooling에서 쓸만한 오퍼레이터들 설치, ancillary 툴들(물리적으로는 1차가 아니지만 필수)에 최소 30개쯤은 돌아가게 됨. values.yaml 세팅도 한두 시간이 아닌데, 대부분은 ArgoCD와 템플릿 작업임. Secrets Manager -> External Secrets -> ArgoCD ApplicationSet -> Another values.yaml 등 boolean 값
       하나 전달하는데도 시간 쏟는 경우가 많음. 클러스터/오퍼레이터 업데이트 주기도 빨라서, 늘 상존하는 유지관리 이슈임. Karpenter로 오토스케일 할 땐 노드 교체와 무중단이 또 하나의 곡예장이 됨(상태 기반 앱은 k8s에서 재미가 두 배). 요약: 진짜 “Low maintenance”는 반어적 표현임.
          + “Low Maintenance”는 결국 대안과의 상대적 개념임. 내 경험으론 k8s를 쓸 때 확장, 장애조치, 롤백, 재해 복구, 데브옵스, 독립 클러스터 스핀업 등 전반적으로 같은 품질의 서비스를 얻는 데 드는 유지보수 부담이 훨씬 낮았음. 상황에 따라 다를 수도 있지만, 나에겐 그런 경험임.
          + 나는 지난 2년 동안 hetzner에서 k3s를 써왔고, 100% 가동시간을 경험함. 유지관리 부담이 너무 낮아서 마스터 노드의 SSH키를 잃어버린 적도 있었는데, 클러스터 전체를 다시 프로비저닝하는 게 문서 업데이트까지 포함해서 90분밖에 안 걸림. 정말 급했다면 15분이면 충분했을 것임. 월 20유로에 ARM 기반 3노드 1마스터, 약간의 스토리지, 자동 DNS까지 클라우드플레어와 연동해서 k8s 클러스터 운영 중임.
          + 하지만 실제로 관리하고 있는 건 Kubernetes 자체가 아니라, CI/CD 시스템, 시크릿 매니지먼트 시스템, 데이터베이스 운영 자동화 같은 부가 시스템임. 과거에는 yaml 대신 크론잡, ansible, systemd, bash 스크립트 등으로 했을 것임.
          + 자기 손으로 곡예장을 만든 느낌임. 그렇게 많은 것들을 설치하지 말아야 함. 뭔가를 추가할 때마다 기술 부채와 유지비용이 붙는 법임, 무료 툴이어도 마찬가지임. 오토스케일이 절약보다 부채/유지비가 더 크면 그냥 꺼버리는 게 나음.
          + 비슷한 서비스 집합을 개별적으로 구성해서 운영했다면 훨씬 큰 유지관리 부담임. 하지만 Kubernetes는 불붙은 후 방치할 수 있을 정도로 관리가 쉬움. 단, 자신이 뭘 하고 있는지 알고 있어야 “멋져 보이는 툴” 설치하는 유혹에 빠지지 않음.
     * K8S는 yaml 강제도 아님. idiomatic할 수는 있지만, 필수는 아님. kubectl apply는 처음부터 json도 지원했고, endpoint도 json과 grpc 임. jsonnet 같은 다양한 언어에서 config 생성 가능함. 두 번째로, Helm 차트에서 dependency와 dependency ordering가 왜 이슈가 되는지 궁금함. Kubernetes의 주된 아이디엄은 루프에 있음: 종속성이 없으면 앱은 recoverable error로 치고, 가능할 때까지 반복 시도하는 식임. 아니면 crash해서 ReplicaSet 컨트롤러가 자동 재시작함. 차트에 dependency가 없으면 dependency 충돌도 없음. 진짜 의존하는 앱이면 Helm 차트 내에 종속 앱/서비스를 같이 포함시키는 게 맞음.
          + (주요 의견 인용) > crash 상황에서 ReplicaSet 컨트롤러가 앱 재시작해준다는 건 충분하지 않음. 예를 들어 keycloak이 1분 걸려서 뜨면, 여기에 의존하는 서비스가 keycloak 없이 바로 crash나고, 여기서 retry하다가 throttle이 걸려서 keycloak 뜬 뒤에도 5~10분 동안 쓸데없이 대기함. 그냥 의존관계가 확실하다면, main 컨테이너 넘기기 전 init 컨테이너로 의존 서비스 체크하고 넘어가는 게 더 적합함. Kubernetes에 명시적인 start dependency 선언 기능이 있으면 좋겠다고 생각함. crash나고 몇 번 재시도하다 throttle 거는 게 답이 아님. dependency는 그냥 존재하는 현실임.
          + 의존성 실패는 꼭 recoverable해야 한다고 생각함. 예전에 실제 사용하지도 않는 dependency 때문에 fail-closed 동작하다 장애난 경험 있음. 서버 사이 의존성은 거의 다 soft dependency임. downstream dependency 안 되면 그냥 500 던지고, 로드밸런서가 unhealthy 서버 피해가면 됨.
          + “supposed to”라고 얘기하지만, 그건 in-house 소프트웨어스택 구축 케이스에만 잘 맞음. 이미 만들어진 오래된 소프트웨어가 docker만 지원하다가 kubernetes에서 돌릴 수 있게 된 사례도 많음. 개발자가 자기 철학대로 툴을 만들면, 사람들은 어차피 원하던 방식대로 써서 엉망진창 결과가 나오기도 함. 결국 사람들한테 다양한 기능과 선택권을 줘야 하고, 시장은 원하는 대로 활용함.
          +

     “Kubernetes의 주요 아키텍처 특징 = 반복적 루프(reconciliation loop)”라는 말, 매우 동의함. 현재 상태를 관찰, 원하는 상태와 diff, 그리고 diff만큼 적용. 끝없이 반복. 성공/실패가 아니라, 오직 현재와 희망 상태간 차이만 iterative하게 해결. 이 패턴은 PID 제어 등 기계제어 시스템의 'good enough technology'와도 유사하다고 봄.
          + “yaml 말고 json도 쓸 수 있다”는 사실로 건을 비판하는 건 논점이 아님. 모두가 진짜 관심 있는 부분이 아님. 쓸데없는 트집이라는 생각임.
     * yaml 대신 HCL로 교체하자는 의견엔 강하게 반대함. HCL은 개발자가 보기 어렵고 읽기도 힘듦. import 지원 여부, 디버깅 난이도 등 usability에서 불만이 많음. 왜 protobuf 같은 인터페이스 정의 언어를 중심에 두고, 사용자 취향대로 형태만 변환 못하게 했는지 이해 못함.
          + HCL은 최악임. k8s yaml만으로도 모자란 적 없음. 혹시 구성이 너무 복잡하다면 config map보다는 app 설계 자체 문제에 가까움.
          + 사실 HCL이든 JSON이든 YAML이든, 클라이언트에서 직렬화/역직렬화 하면 되는 일임. 즉, 이건 Kubernetes 자체 이슈가 아니라, 단순히 외부 변환 레이어일 뿐임.
          + Kubernetes 인터페이스 정의는 이미 protobuf 기반임(crd는 예외로 하고).
          + 내 주된 HCL 불만은 for loop 문법임. 정말 최악임.
          + “HCL 개발자가 어렵고 린트나 디버깅이 어렵다”는 얘기는 새로운 걸 배우기 싫어서 나온 얘기에 더 가깝다고 들림. 복잡한 도메인을 HCL 배우는 것과 혼동해서 생긴 문제처럼 보임.
     * “Kubernetes 2.0” 스타일로 nebulous 프로젝트 개발 중임(아직 pre-alpha 단계). 목표는, 전세계적으로 분산/경량화, 노트북에서 싱글 바이너리 실행 가능+클라우드에서 수천 노드까지 확장, Tailnet 네트워크/BitTorrent 스토리지/멀티테넌시/라이브 마이그레이션 등. 대부분 요구사항은 ML 운영(특히 GPU 부족 현상)에서 발생했고, 앞으로 ML이 평범한 시대라면 이게 기본이 될지도 모름.
          + 라이브 마이그레이션이 흥미로움. 우리는 지금 여러 클러스터와 클라우드 간 오토스케일링 기반 가격전략 쓰는데, 라이브 마이그레이션은 또 다른 차원의 도전임.
          + 이건 Kubernetes가 아니라, GPU 운영에 특화된 별도 시스템임.
          + “글로벌 분산”은 오히려 non-requirement일지도 모름. Tailnet을 기본 네트워크로 쓴다면 오히려 바로 빼버리고 싶은 기능임. Kubernetes가 싱글 NIC 가정하는 건 클라우드에 맞춘 답답한 유산임(여러 CNI, 최근 Multus(참조: redhat 블로그)같은 프로젝트가 반가움). Multi-tenant 설계는 k8s와 뭐가 실질적으로 다른지 궁금함. BitTorrent로 storage 한다면, 퍼블릭 컨테이너 이미지까지 공유한다면 egress 트래픽 요금 엄청남.
          + GitHub를 보면 Chart.yaml이 보이고, 심지어 템플릿 provider_aws.yaml 등은 정말 하지 않았으면 하는 코드 패턴임.
     * Kubernetes는 지금도 정말 복잡하다고 느낌. 대중화로 인해 덜 느껴질 뿐임. Kubernetes 2.0에서는 사용자 경험, 특히 자주 하는 운영(앱 배포, 서비스 노출, 서비스 계정/이미지 변경 등)이 더 간소해지면 좋겠음. 하지만 지금 대세가 LLM이라 후속 개발은 어려워 보임.
          + Kubernetes는 추상화 계층이 너무 많음. pods는 멋진 핵심 개념인데, deployment, rep set, namespace 등이 추가되면서 Docker Swarm만큼 심플했으면 한다는 생각이 듦. Terraform도 단일 계층이고 배우기 편했음. K8s 학습曲선이 진짜 가파른 걸 실감하는 중임.
          + “컴퓨터 프로그램의 타입 구분은 인간의 인식 결과”라는 생각이 드는 이슈임. 오퍼레이터/컨트롤러는 과거 COM/CORBA처럼 지나치게 추상화되어 있음(장점이자 단점). 단순한 구현엔 더 제한적인 k8s-lite가 오히려 적합하다고 느낌. 반면, 복잡한 환경엔 오히려 k8s의 기존 추상화도 부족한 경우가 많았음. 하나의 시스템(Kubernetes 2.0이든 뭐든)이 현실 문제 전부를 감 싼 상태로 인간 개발자/설계자가 다룰 수 있을지 의문임.
     * “sane default” 즉, 특별히 고르지 않아도 기본 상태가 적당히 충분한(네트워크/스토리지/로드밸런서 등) 시스템을 원함. yaml과 HCL 모두 별로임. 더 나은 config 방법이 필요하고 언어만 바꾼다고 해결될 문제도 아님. IPv6는 반드시 필요하다고 생각. Docker, 컨테이너, Kubernetes 모두 내부는 IPv6-only로 했어야 하며, IPv4는 ingress controller에서 예외 처리가 맞음.
          + “Sane defaults”와 “Managed 서비스 고객 전환” 사이에는 본질적 갈등이 있음. K8s를 오래 지켜볼수록 스토리지/네트워크 등 “batteries not included” 철학이 커지고, 그걸 AWS/GCP 등이 비싼 연동서비스로 파는 현상이 짙어짐. 사실상 현실 K8s는 오픈소스가 아니라, 클라우드 gap filler 판매촉진 툴로 쓰이는 측면까지 보임.
          + (개인경험) Terraform/HCL이 YAML보다 나은 점은, 보이지 않는(인비저블) 문자에 의존하지 않아 더 읽기 쉬움.
          + ""sane defaults"" 때문에 k3s가 훨씬 더 마음에 듦.
     * 2.0 릴리즈로선 다소 소박한 위시리스트라고 생각. 내 주변 모두 프로덕션 k8s 복잡성에 불만이 큼. 백워드 컴패티빌리티와 심플함을 어떻게 동시에 구현할지가 진짜 핵심임. 보통 백워드컴패트덕분에 복잡도가 폭증함(새 시스템이 신구 기능을 죄다 떠앉게 됨).
          + “복잡성을 어디서 얼마나 덜어낼 수 있는가”가 결국 쟁점임. 지금까지 본 각종 k8s 추상화들은 극소수만 커버하거나(heroku류 wrapper) 또는 자체 DSL이 생겨서 오히려 학습곡선만 높아짐(결국 다시 복잡한 k8s+잡 DSL 2개나 배워야 함).
     * 나는 이미 Terraform으로 클러스터 및 앱을 관리하면서 “Kubernetes 2.0 월드”에 살고 있다고 느낌.
          + HCL/타입/리소스 종속성/데이터 조작을 바로 쓸 수 있음
          + tf apply 한방으로 클러스터, 노드, 클라우드 연동(S3 등), 클러스터 내 서비스까지 한 번에 구성
          + terraform module로 코드 재사용하고, 비-K8s 인프라도 통합. 예를 들어 Cloudflare ZeroTrust 모듈로 5줄 코드로 공개 SSO 엔드포인트 제공 가능. (클러스터에 cloudflared 배포+Cloudflare API 터널 config)
          + 주요 인프라 업체가 자체 Terraform module을 잘 만들어놓고, provider lockfile로 모듈/프로바이더 의존성 관리도 편함
          + Helm terraform provider로 헬름 차트도 자유롭게 관리. 특히 “namespace 생성+operator 배포+custom resource 생성”만 하는 헬름 차트는 operator만 설치하고 CRD를 terraform에서 직접 관리
          + 단점은 apply 프로세스 orchestration이 약간 번거로움(YAML 등도 유사), 우리는 Spacelift로 이슈 해결
          + 사실 시스템의 상태를 k8s state와 Terraform state 두 번 관리하는 건 비효율적임. 리소스가 mutating webhook 등으로 변경되면 “computed fields” 설정도 필요함. 그래서 애플리케이션 레벨 관리는 Terraform에서 지양함. 클러스터 자체는 Terraform으로 관리하는 것도 괜찮다고 봄.
     * 프론트엔드 출신으로서, Kubernetes가 굉장히 직관적이었다고 느낌. 기존에는 데이터를 입력받아 UI를 반응적으로 만들었는데, 이제는 제어판에 리소스와 config를 맞춰주는 코드를 쓰는 느낌임.
"
"https://news.hada.io/topic?id=21594","Asterinas - 새로운 Linux-호환 커널 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Asterinas - 새로운 Linux-호환 커널 프로젝트

     * Rust로 작성된 Linux ABI 호환 커널로, “프레임커널(framekernel)” 아키텍처를 적용해 모놀리식과 마이크로커널의 장점을 결합하고자 함
     * 모든 unsafe 코드를 한정된 라이브러리 내부에 캡슐화하여, 나머지 커널 서비스는 안전한 Rust 추상화로 개발 가능하게 설계해 메모리 안전성과 단순한 공유 메모리 구조를 동시에 달성
     * RedLeaf, Tock, Rust for Linux 등 기존 Rust OS와 차별점은, 하드웨어 격리 지원 및 범용 목적, Linux 호환 ABI, 사용자 공간의 다양한 언어 실행
     * TCB(신뢰 컴퓨팅 베이스) 최소화 및 공식 검증(Verus 활용) 추진, Intel TDX 등 신뢰 실행 환경 하드웨어 지원, OSTD/OSDK 등 OS 개발 프레임워크도 별도 제공
     * 아직 초기 개발 단계로, x86/RISC-V 지원 및 206개 시스템콜 구현, Docker/컨테이너/클라우드 환경에 집중하고 있으나, X11/Xfce 등 데스크톱 확장도 시도 중
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

What's Asterinas?

     * Asterinas는 Rust로 개발된 Linux ABI 호환 신규 커널 프로젝트임
     * “프레임커널(framekernel)” 아키텍처란, unsafe Rust 코드(메모리 불안전 구간)를 프레임워크 라이브러리로 한정해 안전 API로 감싸고, 나머지 커널 서비스는 모두 안전한 Rust로 개발하게 설계함
     * 모놀리식 커널의 단순/고성능 구조와, 마이크로커널의 TCB(신뢰 컴퓨팅 베이스) 최소화/안전성을 동시에 추구함
     * 커널 내부에서 서비스들이 동일 주소 공간 내에서 분리되어 동작하며, 각 서비스는 core 라이브러리로부터 부여된 자원만 사용 가능함

프레임커널(framekernel) 아키텍처 설명

     * 프레임커널 개념은 ""Framekernel: A Safe and Efficient Kernel Architecture via Rust-based Intra-kernel Privilege Separation"" 논문에서 처음 제안됨
     * 모놀리식 커널은 모든 코드를 하나의 커널 모드 주소 공간에 포함시키는 구조이고, 마이크로커널은 최소한의 TCB에만 커널 공간을 할당하고 대부분의 기능을 유저 모드 서비스로 위임함
     * 프레임커널은 Rust의 'unsafe' 기능이 필요한 코드를 라이브러리로 캡슐화하고, 나머지 커널 서비스는 메모리 안전 Rust 추상화를 적용해 개발함
     * 각 서비스는 커널 주소 공간 내에서 실행되지만, 라이브러리에서 명시적으로 제공하는 자원만 접근하도록 제한함
     * TCB를 작게 유지하면 공식 검증(수학적 증명)이 더 용이하고, 시스템 전체의 신뢰성과 안정성을 높일 수 있음
     * 공유 메모리 기반(모놀리식 커널처럼 고성능) 이면서도, 내부 권한 분리/보안성(마이크로커널의 장점) 을 결합한 접근법임

기존 Rust OS 및 프레임커널과의 비교

     * RedLeaf: Rust 기반 마이크로커널, 하드웨어 격리를 사용하지 않음
     * Asterinas: 범용 OS, 하드웨어 격리 활용, Linux ABI 호환, 다양한 언어의 사용자 공간 실행 지원
     * Tock: 임베디드 SoC 타깃, 핵심(unsafe 허용)과 캡슐(안전 코드) 분리 구조, 하드웨어 격리 미지원
     * Rust for Linux 프로젝트 역시 커널 인터페이스를 안전 추상화로 감싸 kernel driver를 Rust로 안전하게 작성할 수 있게 하려는 목적이 있음

공식 검증 및 보안성

     * TCB를 줄이는 주된 목적은, 정형 검증(formal verification) 이 현실적으로 가능하도록 하기 위함임
     * Asterinas는 seL4처럼 작고 검증 가능한 TCB와 Linux ABI 호환, 공유 메모리 구조를 동시에 목표로 삼는 유일한 사례임
     * 보안 감사 전문 기업 CertiK와 협업하여 Verus 기반 정형 검증 작업을 수행 중임
          + CertiK의 보안 평가 보고서에서 프로젝트의 감사지점 및 발견 이슈를 공개함

라이브러리 및 개발 도구

     * 커널 자체 외에 OSTD(Rust OS 프레임워크)와 OSDK(Cargo 기반 개발 도구)를 제공함
     * OSTD의 주요 목표:
          + OS 개발 진입 장벽 완화 및 혁신 기반 마련
          + Rust 운영체제의 메모리 안전성 강화와 저레벨 API 추상화
          + Rust OS 프로젝트 간 코드 재사용 촉진
          + 유저 모드에서 신규 코드 테스트 가능(개발 생산성 증가)
     * OSD 및 OSTD 기반 커널은 Linux 호환적일 필요가 없으며, x86 하드웨어 제어, 가상 메모리, 다중 처리(SMP), 타이머 등 고레벨 메모리 안전 API를 제공
     * Intel이 후원사로 참여, 특히 Trust Domain Extensions(TDX) 와 가상머신 격리 관련 기술이 매칭

개발 현황 및 주요 스폰서

     * 2024년 초 오픈소스(MPL 라이선스)로 첫 공개, 현재 45명 기여, 주요 기여자는 중국 SUSTech, 북경대, Fudan대 박사과정과 Ant Group
     * x86, RISC-V 지원, Linux 시스템콜 206개 구현(전체 368개 중)
     * 아직 앱 실행 불가, 초기 배포판 개발·컨테이너(Docker) 지원 목표, Nix 기반 배포 등 시도
     * Linux 커널 모듈 로드를 지원하지 않으며 영구적으로 도입 계획이 없음

앞으로의 계획

     * 더 다양한 CPU 아키텍처, 하드웨어 지원 확대 및 클라우드 환경 내 실사용성을 단기 우선 과제로 삼음
     * Linux virtio 기기 지원 완료, 중국 클라우드 시장(알리바바 클라우드, Aliyun) 을 주요 타깃으로 함
     * 안전하고 축소된 TCB, Intel 신뢰 컴퓨팅 기능 지원을 갖춘 컨테이너용 host OS 개발이 중심 목표임
     * Rust for Linux와 목적은 비슷해도, Rust for Linux는 기존 커널 내 Driver의 Rust화에 제한되는 반면, Asterinas는 전체 커널 신규 설계와 TCB 최소화를 추구
     * 현재는 X11, Xfce 지원 등 다양한 방향 실험도 진행 중이며, OSTD 자체도 일반 OS 개발자에게서 주목 받을 잠재력이 있음

Rust for Linux와의 차이점

     * Rust for Linux: 기존 Linux 커널에 Rust로 안전한 드라이버만 도입, 커널 전체는 C 기반
     * Asterinas: 새로운 커널을 처음부터 Rust로 구현, unsafe를 엄격히 한정, 공식 검증 추진, 클라우드/컨테이너 환경에 집중

종합 및 전망

     * Asterinas는 메모리 안전성, 공식 검증, 클라우드 환경 실용성 등에서 혁신적 접근을 시도 중
     * 아직 실사용 예시나 넓은 커뮤니티 검증은 없음, OS 연구·클라우드·보안 분야에서 관심 받고 있음
     * OSTD 프레임워크 등은 향후 Rust OS 개발 생태계에서 활용 가능성을 가짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Asterinas 관련 LWN 댓글 주요 논점 요약

     * Singularity OS 및 언어 기반 보안 경계 논쟁
          + Asterinas의 framekernel은 Microsoft의 Singularity OS와 유사하나, Rust의 borrow checker를 활용해 메모리 안전성을 높임
          + 언어 자체(예: Rust, Java, WASM/JS)의 보안 경계로 시스템을 보호하는 접근에 대해, ""완전 신뢰는 불가능하며 실제로는 하드웨어/OS 레벨 샌드박스와 병행해서 사용함""이라는 비판과, ""결함 방지 및 공식 검증에는 장점이 있다""는 의견이 대립함
          + WASM, JS, Java 등도 보안성 논란이 있지만, 실서비스에서는 언어 샌드박스만으로는 신뢰받지 못하며, 실제로는 하드웨어나 OS 샌드박스를 반드시 병행하는 사례가 일반적임
     * 운영체제 개발 트렌드 및 지정학적 배경
          + 최근 몇 년간 다양한 OS 신생 프로젝트가 등장, OS 혁신 분위기 확산 중임
          + 중국이 미국의 기술제재 및 공급망 리스크에 대응해 Linux 대체 OS 개발에 박차를 가하고 있음
          + US 제재와 GPL 등 오픈소스의 글로벌 거버넌스 논쟁, 그리고 중국·유럽 등 각국이 독립적 기술 생태계를 추구해야 한다는 정치적 논의가 격렬하게 이어짐
          + Linux에서 포크 유지와 완전 신규 커널 개발의 난이도, 커뮤니티 지식/노하우 의존성, 언어 장벽 등도 논쟁 주제임
     * Linux 호환성/시스템 콜 수 논쟁
          + ""Linux 시스템 콜 수로 호환성 측정은 부적절"" 의견 다수
          + 단일 시스템 콜(ioctl 등)이 수많은 세부 API를 내포하므로, 실질적 호환성을 위해서는 커널 테스트 스위트 등 실제 앱 구동이 중요
          + 초기에 POSIX 호환성 확보를 위해 syscall을 하나씩 구현했던 Linux 발전사를 예로 들며, 99%의 주요 syscall만 지원해도 꽤 많은 소프트웨어가 구동될 수 있다는 현실적 의견도 나옴
     * 라이선스 및 Rust 생태계
          + Asterinas가 MPL을 택한 것에 대한 논의: Rust의 crate 생태계와 모듈형 라이선스에 MPL이 잘 어울린다는 긍정적 의견
          + GPL이 항상 최적은 아니며, 기업 후원을 받는다면 기업 친화적 라이선스(MPL 등)를 쓸 수도 있다는 시각
     * 프로젝트 의존성/보안
          + Rust 기반 OS에서 많은 외부 crate 사용이 안전한지에 대한 의문과, cargo deny/vet 등을 통한 공급망 보안·감사 자동화 필요성 제기
          + 실제 lockfile만으로는 의존성 표면적을 파악하기 어렵고, Rust 생태계 특유의 트랜스티브 의존성, OS별 의존성 등 복잡성도 언급됨
     * 기술적 영감/유사 아키텍처
          + framekernel 개념이 90년대 University of Washington의 SPIN OS 아키텍처와 유사함을 지적
          + SPIN 역시 강한 모듈성, 컴파일러에 의한 인터페이스/접근 제어를 강조
     * 커미터 구성 논란
          + 45명의 기여자 중 다수 커밋이 소수의 박사과정 학생 중심임을 언급
          + PhD 학생 주도의 기여가 실질적 커미터로서 가치가 낮다는 오해를 지적하며, 연구 중심의 혁신 프로젝트로서도 의미가 있다는 의견
     * 정치/역사 논쟁 및 오프토픽 지적
          + OS 논의가 미국, 유럽, 중국의 지정학/정치/역사 논쟁으로 확장되며, 편집자에 의해 ""오프토픽"" 경고가 여러 차례 제기됨
          + 오픈소스·FOSS 생태계도 지정학적 환경 변화에 영향을 받을 수 있음을 일부 이용자들이 강조함
     * 기타
          + WASM 보안성에 관한 학술적 논문 공유
          + 커널 개발 현황에 대한 긍정적/비판적 시선 혼재

   이런 시도 참 좋아보입니다.
   머지않아 죽지 않는 커널이 나올까 싶기도하네요.

        Hacker News 의견

     * 흥미로운 접근법이라는 생각, 성공을 바라는 입장임. 하지만 여전히 의구심이 남음. 90년대 말이나 2000년대 초에 Linus가 TV 인터뷰에서 경쟁자에 대해 언급한 내용이 아직도 기억에 남음. Linus는 ""드라이버를 쓰는 걸 즐기는 사람은 없음, 젊고 굶주린 누군가가 뛰어난 드라이버 엔지니어로 나오기 전까지는 안전함""이라는 식으로 말한 적이 있음. 이미 그 당시에도 드라이버 인터페이스를 불안정하게 유지하는 것이 방어책임을 인지하고 있었음. 25년이 지난 지금, 가상화 하드웨어에서 동작하는 커널은 아주 흔하지만, 여전히 진짜 하드웨어와 추상화에 성공한 실질적이고 사용 가능한 운영체제는 손에 꼽을 정도임
          + ""드라이버 인터페이스를 불안정하게 유지하는 것이 방어책""이라는 부분에서, 앞으로는 젊고 굶주린 시스템 연구자나 AI가 등장해서, AI 에이전트가 C로 된 Linux 드라이버를 안전한 Rust로 된 Asterinas 드라이버로 번역해주는 방향이 가능성이라는 생각. 또 다른 현실적인 접근법은 리눅스 커널 자체를 어떤 격리 환경 안에 넣어 재활용하는 것임. 예를 들어 HongMeng 커널은 User-Mode Linux를 활용해 드라이버를 재사용함. Asterinas도 이와 유사한 전략 적용이 가능함. 참고: https://www.usenix.org/conference/osdi24/presentation/chen-haibo
          + Linus가 진짜로 ""방어벽""을 원하거나 의도했을지에 의문인 입장임. 기술 스타트업 창업자가 아니고, 원래 그냥 커널 해커였으며 이미 평생 먹고 살 기반은 다 마련됨. 커널 내의 드라이버 인터페이스 불안정성이 의도적 경쟁 방지 전략이라는 해석은 너무 과한 투영임
          + 과거에도 SPIN OS(Modula 3), JX OS(Java), House OS(Haskell), Verve 같은 선례가 있음. 모두 타입 세이프와 메모리 세이프 언어를 사용해 커널 기능을 구현했었음. 일반적으로 위험을 체크된 함수 호출 뒤에 숨겨둔 구조, 일부는 VM 활용. 성능이나 채택 문제 제외하면 주된 취약점은 추상화의 빈틈, unsafe 코드 우회, 컴파일러/JIT로 인한 세이프티 모델 붕괴, 일반적인 하드웨어 결함(예: 우주선에서의 코스믹 레이). 그래도 unsafe 언어 커널/앱보다는 훨씬 더 안전함. 여기서 더 나아가려면, unsafe 코드의 정적 분석, 모든 unsafe 함수가 타입-메모리 세이프 인터페이스를 준수함을 보장, 통합 시 추상화 세이프티 preserving 컴파일러, 구성 요소별 인증된 컴파일러도 활용 가능. 생산성 툴은 대부분 존재하고, 단지 하나가 '완전히 안전하게 추상화된 컴파일'인데, 현재 연구 중이며
            수작업 검사도 가능함
          + 실질적으로 사용 가능한 운영체제 수가 손에 꼽힌다는 점에는 이유가 있음. 하드웨어 세계에는 인터페이스 '표준'이 넘쳐나지만, 실제 하드웨어는 표준대로 잘 동작하는 경우가 거의 없음. 드라이버가 각종 특이점과 수정 불가한 결함(에라타)을 대응하도록 코드를 작성해줄 시간이 들지 않으면, 실제 물리 하드웨어에서 성능과 지원을 가지는 것은 정말 어렵다는 현실임
          + 한편으로 실제로 내가 다루는 Linux의 98%는 가상화 환경에서 동작함. 데스크톱/랩탑에서는 Virtualbox를 전체화면으로 돌리고, 윈도우용 드라이버는 정말로 필요할 때만 사용하며, Mac에서는 Docker.app로 관리되는 헤드리스 VM. 회사의 모든 운영 워크로드는 AWS VM. 집에서 사용하는 유일한 리눅스 하드웨어도 홈서버고 곧 이베이에서 산 Mac mini VM으로 대체할 계획. 만약 리눅스와 호환되면서 보안성이 더 높고 성능도 비슷한 커널이 나온다면, 드라이버가 부족하더라도 요즘은 대안 선택이 쉽다는 생각
     * 최근 마이크로커널이 IPC 성능 이슈를 많이 개선했다고 들었음. 실제 어느 정도 개선됐는지는 기억이 가물하지만, Mach로 인한 업계 트라우마가 컸던 것 같음. 프로젝트 사이트를 보면, 오히려 Framework(특권 모드)만 Rust의 unsafe를 쓸 수 있고, Services(비특권)는 안전한 Rust만 사용한다는 구조임. 어딘가 어색한 느낌인데, 비특권 코드가 unsafe하면 아무리 비특권이라도 위험한 것 아닌지? 반면 정말 검증이 필요한 unsafe 코드는 무제한으로 쓸 수 있는 쪽임? 그리고 라이선스를 보면, Asterinas는 Mozilla Public License(MPL) 2.0을 주로 사용하며, 일부 구성 요소는 더 관대한 라이선스라는 설명. GPL도 아니고, BSD도 아닌 중간 정도 느낌임
          + ""비특권이 unsafe여도 위험하고, 정말 검증 필요한 코드는 특권쪽에만 몰아둔다는 게 이상하다""는 질문에, 이 구조는 framekernel 컨텍스트에서 해석해야 함. Rust 기반 framekernel 전체는 커널 공간에서 동작하지만, 논리적으로 특권 OS 프레임워크와 비특권 OS 서비스 두 부분으로 나뉨. 특권은 safe + unsafe Rust 커널 코드 모두 포함, 비특권은 오직 safe Rust 커널 코드만 포함. 이 정책은 모두 커널 코드에 해당되는 내용임. framekernels에서는 유저 프로그램 언어에 제약이 없음
          + 최근 마이크로커널들은 대부분 IPC 성능을 극적으로 개선한 것으로 알고 있음. 대표적으로 SeL4는 IPC를 매우 공격적으로 최적화해, 리눅스의 일반적인 syscall보다 IPC가 훨씬 빠름. 웬만한 대부분 프로그램은 Sel4에서 오히려 더 빠르게 동작할 가능성이 있음. 실제 성능 비교 데이터가 궁금함
          + 최신 마이크로커널들이 IPC 이슈를 개선한 것이 맞음. 사실 더 근본적인 문제는 현대 하드웨어에서 심지어 모놀리식 커널의 syscall조차 매우 느리다는 점임. 그래서 io_uring이나 virtio 같은 인터페이스가 좋은 성능을 내는 이유가 시스템과 앱(또는 하이퍼바이저와 게스트) 간에 요청과 응답을 큐로 처리해서 어드레스 스페이스 전환을 피하기 때문임. 앞으로의 운영체제는 어떤 구조든 '큐잉' 기반 syscall 메커니즘이 필수임. 그리고 이 방식을 택하면 OS를 모놀리식/마이크로커널/기타 구조로 나누든 상관없이 성능상의 큰 차이 없음
          + Framekernel에서 privileged/unprivileged 분리가 커널/유저스페이스 의미가 아님. OS 전체가 커널 privilege 레벨에서 동작. 단, 논리상 코어 라이브러리 코드(unsafe 사용 허용, privileged)와 나머지 커널 구성 코드(라이브러리 이용, unsafe 직접 사용 불가, unprivileged)로 나눔. 아마도 linter 등 정적 체크로 강제함. 용어의 중복사용 때문에 헷갈리기 쉬운 구조임
          + 비특권 task도 커널 코어와 같은 메모리 공간에서 동작하므로, 런타임에서 안전하지 않은 동작을 막을 방법이 없음. 실제로 런타임에서 특권을 분리하려면 마이크로커널 구조가 필요함. 여기서는 특권을 런타임이 아닌 정적으로, 즉 unsafe 코드를 금지하도록 해서 권한을 구현함
     * 커널을 작은 unsafe 코어와 대형 safe 모듈로 분리하는 개념이 새로운 시도인지 궁금함. 마이크로커널의 하드웨어 오버헤드는 없고, 모놀리식의 안전성 이슈도 피하면서, 시스템 언어의 safe/unsafe 구분 성질을 잘 살린 프로젝트라는 점에서 흥미와 기대감이 큼
          + 아주 유사한 아이디어가 ""The Birth & Death of JavaScript""라는 영상에서 농담처럼 제안된 적 있음. https://destroyallsoftware.com/talks/…
     * Drew DeVault의 Rust in Linux 리뷰 글이 떠오름. HN 토론도 연결 가능함 https://news.ycombinator.com/item?id=41404733
     * 정말 대단한 시도라고 생각, 작성자가 이 스레드에 있다는 점도 감동. 어느 수준까지 usable한지 궁금함, 일부 제한적 환경에서라도 서버 이미지를 빌드해서 실험하고 싶음
          + Asterinas는 아직 상대적으로 새로운 커널이라 범용 사용에는 다듬어야 할 점이 많음. 하지만 실제로 효율적이고 신뢰할 만한 실서비스 운영을 목표로 한다면 그 격차는 그리 크지 않고, 1년 이내에 목표 도달 가능성 있음. 현재 Linux 네임스페이스, cgroups 같은 핵심 기능 구현과 최초의 Asterinas 기반 배포판 작업이 진행 중. 초기 목표는 Confidential VMs의 게스트 OS로 Asterinas를 활용하는 것이며, 메모리 안전성과 작은 TCB 덕분에 보안 측면에서 Linux보다 명확한 강점이 있음
     * 마이크로커널 IPC가 느려서 인기가 적다는 설명을 보며, 기술적으로 뛰어난 사람들도 실제 채택 이유와 그 과정에서 오해를 하는 것 같아 안심
          + 그렇다면 정확히 어떤 점에서 잘못 해석되고 있는지 알려줘야 모두에게 도움이 될 것임
     * 라이선스가 MPL인데, 개인적으로는 더 나은 라이선스(GPLv3 등)도 있다고 생각함
          + 문서에서 MPL을 선택한 이유를 직접 설명하고 있음. 내가 좋아하는 선택은 아니지만 이유는 납득 가능함
     * 정말 괜찮은 아이디어라는 생각. 이미 많은 소프트웨어가 구축돼 있고, 대안적 기반이 있으면 기술 외적인 이유들로도 큰 이점이나 선택지가 생길 수 있음. kFreeBSD, GNU/Hurd가 떠오름
     * 이런 커널들은 뭐라고 불러야 할 지 고민. *nux?
"
"https://news.hada.io/topic?id=21659","기업시장에서 Copilot보다 ChatGPT가 성공하면서 OpenAI와 Microsoft의 라이벌 구도를 심화함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     기업시장에서 Copilot보다 ChatGPT가 성공하면서 OpenAI와 Microsoft의 라이벌 구도를 심화함

     * 기업 시장에서 OpenAI의 ChatGPT가 Microsoft Copilot보다 더 많은 선호를 받으며, Microsoft와의 파트너십에 긴장감이 고조되고 있음
     * Amgen, New York Life 등 대형 고객이 Copilot에서 ChatGPT로 이동하거나 두 제품을 동시 도입하며 평가 중임
     * ChatGPT의 친숙함과 빠른 기술 개선이 실제 업무에서 높은 활용도를 보여주는 반면, Copilot은 MS 오피스 제품군과의 통합에서 강점을 보임
     * 가격 정책과 기능 업데이트 지연 등으로 인해 MS 세일즈팀이 어려움을 겪고 있으며, OpenAI는 사용 기반 요금제와 추가 할인 등으로 공격적으로 시장을 확대 중임
     * 기업 고객들은 두 제품의 차별점을 명확히 느끼지 못하며, 실제 선택은 기존 MS 인프라와의 연계성 혹은 ChatGPT의 직관적 경험 중에 고민 중임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Microsoft Copilot과 OpenAI ChatGPT의 경쟁 현황

     * 최근 Amgen 등 대형 기업 고객이 처음엔 Microsoft Copilot을 도입했다가, 직원들의 요구와 ChatGPT의 성능 개선을 이유로 ChatGPT 사용을 확대함
     * Amgen 임원은 “OpenAI의 제품이 재미있고 직관적이며, Copilot은 Outlook, Teams 등 MS 제품 연동에 더 적합”하다고 평가함
     * Microsoft는 오랜 파트너십과 오피스 제품군과의 강력한 통합성을 내세우지만, 실제 직원들은 ChatGPT를 이미 가정에서 경험하고 있어 ChatGPT가 선점 효과를 누림

파트너십 갈등과 시장 전략 변화

     * Microsoft는 OpenAI에 140억 달러를 투자했지만, 최근 경쟁 AI 스타트업을 후원하고 자체 모델도 개발하며 협력 관계가 복잡해짐
     * OpenAI는 AWS 등 경쟁 클라우드 업체와도 제휴를 맺고, 기업·학교·개인 대상 유료 구독 상품을 공격적으로 확대 중임
     * 최근 OpenAI가 AI 코딩 어시스턴트 Windsurf 인수를 발표해, MS의 GitHub Copilot과 직접 경쟁하는 구조가 됨

엔터프라이즈 AI 시장의 승부처

     * Microsoft는 “Fortune 500의 70%가 Copilot을 사용한다”고 강조하며 기존 IT 부서와의 신뢰를 기반으로 영업을 강화함
     * 반면, OpenAI는 유료 비즈니스 고객이 3백만 명으로 몇 달 만에 50% 증가했다고 밝힘
     * Gartner 애널리스트는 “아직 많은 기업이 Copilot을 테스트 단계에 머물러 있으며, 다양한 벤더가 시장에서 경쟁 중”이라고 분석함

제품 차별화와 실무 현장 반응

     * 두 제품 모두 유사한 OpenAI 모델을 기반으로 하기에, 실무자 입장에서는 뛰어난 차별점을 체감하기 어렵다는 평가가 많음
     * MS 영업팀은 “Copilot의 가장 큰 강점은 MS 소프트웨어와의 연계성, 낮은 가격”이라고 설명함. Copilot은 월 30달러, ChatGPT 엔터프라이즈는 최대 60달러까지 책정됨
     * 그러나 OpenAI는 사용량 기반 요금제를 도입하고, 패키지 구매 고객에 추가 할인을 제공하며 적극적으로 시장을 확장 중임

기업의 선택 기준과 향후 전망

     * 실제 기업들은 두 제품을 병행 도입 후, 사용량·효과·네트워크 효과를 비교하여 장기 도입 여부를 결정하는 사례가 늘어나는 중임
     * IT 책임자들은 MS 제품군 연계가 중요한 업무에는 Copilot을, 신속한 최신 기술 체험이 중요한 곳엔 ChatGPT를 선호함
     * 결국, 직원 경험(UX)와 기존 IT 인프라 연계성이 기업의 선택을 좌우하며, OpenAI의 친숙함·가격 정책 변화가 MS의 독주를 견제하는 구도로 전개 중임

   MIcrosoft Copilot != GitHub Copilot

   코파일럿은 이름부터 잘못 지은 것 같아요. 깃헙 코파일럿이랑 헷갈리기만 하고요.

   copilot을 21년도부터 썼는데 그렇다기엔 cursor가 나오기 전까지 copilot이 진짜 너무 아무것도 안했던것같습니다.

   저는 코파일럿 21인가 22년도쯤에 처음듣고 실무에 써보려다 많이 실망스러워서 잊고 있다가... cursor가 핫해서 써보니 cursor는 진짜 실무에 응용할만큼 유용하더군요. 코파일럿은 오히려 일찍 나왔는데 초기 경험이 별로라 잠깐쓰고 떠나버린듯

        Hacker News 의견

     * 아카이브 링크 공유
     * 우리 회사는 Microsoft Shop™라서 Copilot을 쓸 수 있는 환경. 하지만 Copilot이 이 분야에서 가장 멍청한 경쟁자라는 아쉬움이 남음. 가장 인상적인 경험은 ""movie.mov 파일을 적당한 크기의 mp4로 변환하는 ffmpeg 명령""을 요청했을 때였음. 그렇게 대단히 구체적인 요청은 아니지만, LLM이 본질을 알고 있다고 생각해서 여태 다른 챗봇에는 항상 잘 통했던 방식. Copilot의 답변은 ""위에 있는 Python 코드를 실행해서 변환을 시도했지만 movie.mov 파일을 찾을 수 없어서 실패했다""는 내용임. 하지만 실제로는, 아무런 Python 코드도 제공하지 않았음
          + 방금 그 프롬프트를 Copilot 앱에 붙여넣어 봄. Copilot이 바로 아래의 명령어와 설명을 제공함
ffmpeg -i movie.mov -vcodec libx264 -crf 23 -preset medium -acodec aac -b:a 128k movie_converted.mp4

            설명에는 각 옵션별로 어떤 의미인지, 품질과 파일 크기 조정 방법, 인코딩 속도, 오디오 압축 등도 자세히 안내함. 파일을 더 작게 하고 싶으면 -crf 값을 높이거나 -preset을 slow로 하면 된다는 팁도 포함. 스케일링이나 메타데이터 삭제, 비디오 자르기 등도 추가 커스터마이징이 가능함을 안내
          + 우리 회사도 MS Shop임. PowerPoint 관련 작업에서 최고의 퀄리티를 기대했는데 Copilot에게 이미지를 멋진 PPTX 슬라이드로 변환해 달라고 요청하니, 포맷이 안 된 텍스트 박스 하나만 들어있는 슬라이드가 나옴. 사무실에서 크게 웃었던 기억
          + 이 상황이 재미있는 이유는 Gemini와 ChatGPT는 ffmpeg 명령어 조합에 정~말 능숙함. 옵션이나 필터 부분 설명까지 훌륭하게 해줄 수 있음
          + 어떤 모델을 썼는지 궁금함. 방금 무료 GPT-4.1 모델로 똑같은 프롬프트를 넣으니 명확하게 ffmpeg 명령어를 제대로 반환함(스크린샷 링크). 다만 copilot-instructions.md 파일에 다음과 같이 답변 프로세스에 대해 자세히 명시해 둔 상태임
# Always follow these steps when responding to any request
1. Please do a round of thinking in  tags
2. Then a round of self-critique in  tags
3. Then a final round of , before responding.
4. If you need more information, ask for it.

          + 어떤 Copilot에서도 같은 엉뚱한 답변이 재현되지 않음. outlook.com에서 제공하는 Copilot, M365에 들어있는 기본 Copilot, 월 30달러 부가형 Copilot, VS Code의 Copilot 모두 올바른 ffmpeg 명령어를 생성함. 본질적으로 OpenAI 4o API를 이용하는 거라서, 어떻게 그 답변이 나오는지 궁금
     * Microsoft가 절호의 기회를 허비한 느낌. ChatGPT 초창기엔 Satya와 Microsoft가 OpenAI에 투자해 비전 있는 회사라는 평가를 받았는데, 이후 경쟁사들이 따라잡는 동안 Microsoft는 정체. ChatGPT와의 통합도 별다른 성과 없이 Bing 채팅 부실 논란과 예전 Tay 챗봇 사태가 떠오르는 결과만 남김. Bing은 AI를 제대로 활용하지 못했지만 Proclarity는 AI 검색 엔진이 어떻게 달라져야 하는지 보여준 적 있음. Copilot도 평판에 미치지 못함. 결국 Claude.ai, Gemini 2.0 등 후발주자들이 따라잡거나 능가하는 가운데, Microsoft는 아직 자체 모델도 없음
          + Google의 검색 AI 통합은 정말 인상적. 구글 검색의 그 엄청난 규모를 감안하면 더욱 놀랍게 느껴짐. 요즘 검색의 절반은 AI 답변이 충분해서 검색 결과를 클릭하지도 않음
          + Microsoft가 Inflection AI 출신을 영입하고 Bing Chat의 주역을 해고해서 급격히 추락. 과거 Bing Chat은 구글마저 긴장하게 만든 경험이 있음
          + 문제의 핵심은 Copilot을 O365 다양한 컨트롤에 맞추기 위해 방대한 시간과 노력을 투자한 뒤, Copilot을 모든 곳에 마구잡이로 붙여서 오히려 혼란만 남긴 점
          + 품질 경쟁은 별 의미 없을지도 모름. 품질이 아니라 기업에 올인원으로 판매할 수 있는 단순한 번들 판매 전략이 Microsoft의 진짜 무기, 예전 Teams가 Slack보다 훨씬 못했는데도 이렇게 성공할 수 있었던 이유와 같음. 이번에도 같은 전략이 통할지 확신은 없지만, 배포력과 영업력이 매번 이득이었던 역사
          + 여러 실패에도 불구하고 Microsoft는 여전히 OpenAI와의 협상에서 우위. OpenAI의 지식재산(IP) 접근 가능, 매출의 20%를 가져가는 계약 구조
     * Copilot의 최대 문제는 모델 자체가 아니라 네이밍 전략으로 느껴짐. 완전히 다른 여러 제품에 Copilot이라는 이름을 반복적으로 써서 사용자가 아주 혼란스러워짐. GitHub Copilot이라고 생각했는데 실제론 M365 Copilot이고, 모델을 직접 고르지도 못함. Microsoft가 이 부분을 반드시 명확히 해야 함
          + 대기업의 의사 결정자라면 체감이 다를지도 모르겠음. Microsoft는 의도적으로 이 포장을 이용해 ""이 Copilot을 사면 모든 Copilot을 얻게 되며, 귀사도 AI에 완벽히 대비할 수 있다""는 식의 세일즈 전략을 구사. 하지만 실상은 Copilot마다 수준 차이가 크다는 부분을 은근히 숨김
          + 나도 Copilot이 OpenAI의 기술 기반이라고 생각했던 경험. 아마 Microsoft와 OpenAI의 파트너십에서 이런 인상이 강하게 느껴졌던 것 같음. OpenAI와 Microsoft가 라이벌이라는 점은 최근에서야 알게 됨. Microsoft가 OpenAI에 큰 투자를 하고, ChatGPT가 Azure에서 돌아간다고 들어서 둘 사이가 경쟁관계가 아닐 거라는 인식이 있었음
          + ""이 부분을 명확히 해야 한다""는 말에 웃음만 나옴. Microsoft는 예전부터 모든 곳에 의미 없는 .NET 접미사를 붙이던 회사
          + Copilot을 직접 써봤는지 모르겠지만, 정말 형편없는 퀄리티. GPT-3보다도 쓸모없다고 생각
     * ChatGPT, Claude, Gemini, GitHub Copilot 등 다양한 LLM 도구를 사용하지만, MS Copilot 종류 중 어느 것도 쓸모 있게 사용한 적이 한 번도 없음. 같은 모델을 쓰면서도 이렇게 엉망이 될 수 있다니 매번 놀람. VS Code에서 쓰는 Github Copilot만이 Microsoft의 LLM 중 유일하게 만족스러웠던 경험. Word에서 거대한 복잡 문서 작업을 멋지게 도와주는 Copilot을 기대했는데, 실제로는 별로임
          + 사용자마다 다르겠지만, GitHub에서 Pull Request 초안 작성 등에 Copilot이 꽤 쓸모 있었다고 느꼈음. 코드베이스에서 에러 라인까지 찾아내서 번거로운 일들을 대부분 자동화. 아주 어려운 작업은 아니었으나, 그만한 시간과 정신력을 줄여줘서 오히려 독서 등 다른 데 쓸 수 있었던 장점
          + 대기업이 일정 규모를 넘으면 쓸모 있는 걸 더 이상 제대로 못 만든다는 결론에 도달. 물론 예외는 있겠지만, 거의 대부분은 그렇다고 체감. 최선은 회사를 인수해서 그나마 잘 되길 바라는 수준
     * Copilot은 지적 고갈의 대표 사례로 Microsoft 이미지를 깎고 있다고 생각. 모든 제품을 Copilot이라는 이름으로 리브랜딩하는 것은 무의미하며 브랜드 혼란만 가중. Copilot이 365/azure 전체에 접근권한을 얻는 건 이미 본보기가 있는 보안 재앙 예고편(취약점 패치 사례도 있음). 수많은 제약 조건에 얽매여서 실질적으로 무용지물. 이메일 편집 부탁해도 내 원본만 그대로 토해낼 때도 많음. 단 한가지 장점은 법무부에서도 알아볼 만큼 투명한 라이센스 조항. 진실 여부와 무관하게, 대기업들은 이런 조항만으로도 Copilot을 선택. 아무도 IBM을 선택했다고 해고당하지 않던 역사의 반복
          + 전형적 대형 벤더 전략. 제품의 품질이 아니라, 안심하고 도입할 수 있다는 점이 진짜 핵심
          + Microsoft의 브랜드 혼란은 조직 DNA에 박혀 있어서 이 정도로는 영향 못 줄 것
          + 모든 제품에 Copilot 이름 붙여서 혼란 생기는 현상, 과거 IBM Watson 때가 떠오름
     * msft는 한때 엄청난 우위를 가졌음. 독점적인 모델 접근권과 누구보다 빠른 웹 검색 도입까지 가졌으나, 완전히 실패. Bing 앱에 억지로 탑재해 형편없는 UX를 만들었고, 기업 가치를 포괄적으로 확보하기보단 유료 서비스로만 밀어붙여 실패. 제품도 멈췄고, 가장 큰 기회였던 데이터와의 통합에서도 뒤처짐. 복잡해서 못 한 거라 볼 수도 있지만, OpenAI와 다른 경쟁사는 해냄. 결국 또 한 번의 마이크로소프트식 제품 관리 실패 사례
          + 아직 완전히 끝난 것은 아님. MSFT는 최고의 '2등 전략가'로 남들이 시작한 시장에서도 막대한 수익을 낼 수 있음. 클라우드 사업도 한참 늦었음에도 (2011년, AWS 2006년 런칭) 결국 시장점유율 25%까지 오른 전례. 모바일 사업은 실패했지만, AI 영역에서도 완전히 뒤처진 것은 아니라고 생각(개인적으로는 Google, Anthropic, OpenAI 모델을 선호)
          + 이는 어쩔 수 없는 현상. 기업들은 성공 이후엔 항상 자기파괴적 경로를 밟는 경향이 있음. 그래서 비자유(free) 소프트웨어엔 시간 투자 안 함
          + Windows Phone UX를 정말 좋아했음. 단순하고 반응성이 뛰어나서 만족
     * 실제로 MS Copilot이 내 일상 업무의 조종사라면 얼마나 좋을지 상상. 현실에서는 이메일 초안, Planner 작업 생성, 미팅 예약도 충분히 못 함. 대부분 MS graph에서 내 데이터를 제대로 활용하지 못하기 때문에, 내 인박스에 Steve가 보낸 이메일 여섯 개가 있다는 것도 모를 때가 많음. PowerPoint 결과물도 좋지 않고, 오히려 LinkedIn 스타일 ppt라는 느낌
          + 내 경험을 공유.
              1. Outlook에서 ""Project ABC 주간 상태 업데이트 요청 메일 작성""을 Copilot에 시켰더니 팀원들에게 짧고 캐주얼한 메일을 꽤 그럴듯하게 작성해줌
              2. ""집의 전기 용량을 100A에서 200A로 업그레이드하는 프로젝트 플랜을 Planner로 작성해 달라""는 요청에, 버킷별로 잘 정리된 Planner 플랜을 제안함
              3. ""8월 1일에 과 'Test Meeting' 일정 잡기"" 프롬프트엔 직접 캘린더 동작은 못 하지만 입력 예시를 안내해 주고, PowerShell 스크립트로 자동화도 도와줄 수 있다고 제안
                 즉, 이메일 초안이나 프로젝트 플랜은 꽤 근접하게 수행하지만, 일정 예약처럼 데이터 접근권이 부족하면 한계가 있음
          + 그리고 모든 PowerPoint 슬라이드가 4:3 비율! 왜, 대체 왜인 건지 ...
     * Copilot 라이선스(무료, 유료의 차이)와, 작업별로 어떤 모델을 선택할 수 있는지 혼란스럽지 않은 사람이 있는지 궁금
          + ""Copilot""이라는 말이 VS Code의 에디터 기능일지, github.com의 다양한 기능일지, 아니면 Microsoft 365 오피스 소프트웨어의 일부인지 헷갈리는 현실. 이 글은 365 제품군에 대한 논의라고 생각
          + 많은 Microsoft 제품과 마찬가지로 라이센스가 가장 어렵고 복잡한 부분
          + 공감. 유료 Copilot 계정 쿼터가 오히려 무료 ChatGPT보다 더 빨리 소진되는 듯한 느낌
     * 만약 Microsoft를 정말 위협하고 싶다면, OpenAI가 마크다운, docx, html을 채팅 또는 오디오 프롬프트로 바로 작성해 주는 에이전트를 만들어야 함. AI에게 문서를 구두로 지시해서 필요한 포맷으로 변환, 업로드까지 해주는 미래를 상상. 그 날이 정말 기대됨
"
"https://news.hada.io/topic?id=21639","NO FAKES 법안 개정…인터넷 표현과 혁신에 더 심각한 위협","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  NO FAKES 법안 개정…인터넷 표현과 혁신에 더 심각한 위협

     * NO FAKES 법안이 개정되며, 인터넷 표현의 자유와 혁신에 대한 심각한 위협으로 변질됨
     * 개정된 법안은 단순한 디지털 복제물 규제를 넘어, 도구·서비스·앱까지 차단 및 필터링을 요구함
     * 신고만으로 콘텐츠, 도구, 업로더 신원까지 즉시 차단 및 공개할 수 있는 강제적 시스템이 도입됨
     * 익명 표현의 권리 약화 및 새로운 검열 인프라 구축으로, 일반 이용자와 개발자 모두 위협을 받게 됨
     * 거대 IT 기업에 더 유리하고, 신생 서비스·개발자의 시장 진입 장벽이 크게 높아짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

법안 개요 및 주요 변화

     * NO FAKES(Nurture Originals, Foster Art and Keep Entertainment Safe) Act는 원래 생성형 AI 기반 디지털 복제 문제 해결을 목표로 했음
     * 이번 개정안은 광범위한 새로운 지적재산권(IP) 제도를 신설하며, 단순한 피해 방지가 아닌 전체 인터넷 시스템 차원의 검열을 요구함
     * 기존 법안은 이미지 라이선스 시스템에 집중했으나, 개정안은 이미지·도구·서비스·앱 등 생성에 사용된 모든 요소를 대상으로 규제 범위를 확대함

도구와 서비스 차단 명령

     * 개정된 NO FAKES는 불법 이미지 생성 도구나 서비스, 앱을 제공·배포·호스팅하는 모든 사업자를 규제 대상으로 지정함
     * 도구가 주로 해당 목적에 쓰이거나, 상업적 용도가 제한적일 경우 신고만으로 차단 가능
     * 혁신적 도구 개발 자체가 신고만으로 중단될 위험이 있으며, 저작권 전쟁에서 권리자의 '혁신 거부권'이 강화됨

신고 및 필터링 시스템의 확대

     * 기존 DMCA(저작권법)보다 더 약한 보호장치로, 신고만으로 즉시 콘텐츠 및 도구 차단이 가능함
     * 재업로드 방지 필터(Replica Filter) 의무화로, 유사 사례까지 자동 차단
     * 도구·앱·서비스까지 삭제, 심지어 업로더 신원 공개까지 신고만으로 가능해짐
     * 패러디, 풍자, 논평 등은 예외 조항이 있으나, 실제 소송 비용 부담으로 실효성이 낮음

익명 표현 및 개인정보 위험

     * 판사 승인 없이 법원 서기만으로 서브포나(subpoena from a court clerk, 정보 제출 명령)를 발급받아 업로더 신원 정보 요구 가능
     * 이미 유사 시스템에서 비판적 발언을 억압하기 위한 남용 사례가 다수 발생함
     * 신원 공개 자체만으로도 평판, 사생활 등 실질적 피해가 우려됨

혁신과 신생 서비스에 대한 위협

     * 새로운 법적·기술적 인프라 구축 요구로 스타트업과 신생 서비스의 시장 진입 장벽이 비약적으로 상승함
     * 기존 거대 IT 기업에는 유리하며, 새로운 혁신 도구·서비스는 개발 및 출시 자체가 위축됨
     * 단순 신고만으로 차단되는 구조 탓에, 합법적 창작·이용자 권리도 침해됨

규제 목적과 실제 효과

     * 최근 미국 의회는 성적 이미지 규제를 위한 Take It Down 등도 통과시켰으며, 과도한 온라인 모니터링 압박이 강화됨
     * NO FAKES의 실제 목적은 피해자 보호가 아니라, 디지털 이미지 상업적 통제권 집중에 가까움
     * 그 결과, 일반 이용자·개발자 모두 불이익을 입게 되며, 인터넷 혁신 생태계에 부정적 파급을 초래함

   읽고도 뭔말인지 이해가 안 돼서 한참 찾아봤네요
   https://www.govtrack.us/congress/bills/119/hr2794
   https://www.govtrack.us/congress/bills/119/s1367
   개정된 법안의 시행 가능성이 5%인가 봅니다
   말도 안 되네요

        Hacker News 의견

     * NO FAKES의 새로운 버전은 거의 모든 인터넷 게이트키퍼가 a) 신고가 접수되면 빠르게 콘텐츠를 내리게 하고, b) 반복적으로 업로드되는 경우를 막기 위해 이미 문제 많은 저작권 필터 위에 복제 필터까지 반드시 적용하게 만들며, c) 이미지를 만들 때 쓰였을 가능성 있는 도구까지 필터링해서 내리게 하고, d) 단순히 “복제됐다”고 주장하는 사람의 말만으로 업로더 신원도 공개하게 만든다는 내용 포함<br>이런 시스템은 소규모 기업은 도입 못하고 대기업은 크게 신경 쓰지 않을 유형
          + 이런 규제는 오히려 대기업들이 소규모 경쟁업체의 진입장벽을 높이려고 로비해서 얻는 결과물로 보임<br>이런 식의 규제 포획 현상이 일정 수준 이상의 기업이나 이윤 규모가 되면 본래 혁신을 외치던 기업들도 결국 규제 찬성 진영으로 돌아서게 만드는 현상
          + 대기업은 오히려 적용할 수 있는 자원이 있게 되고, 이런 규제 자체가 작은 기업들에게는 넘을 수 없는 해자(moat) 역할을 하게 됨<br>그런데 문제는 그보다 훨씬 심각해서, 이런 시스템 자체가 바로 전형적인 독재정권의 사회 통제 인프라 역할을 할 가능성<br>체제 비판 세력의 발언권을 없애고, 발화의 장과 출판 수단을 빼앗고, 정권에 신원을 넘기게 하며, 반체제와 연대하는 것조차 모두 두렵게 만들 위험성
          + 실제로 주요 온라인 커뮤니티를 운영하는 대형 플랫폼에서는 이런 시스템이 정부가 싫어하는 목소리를 억누르는 데 더욱 쓰이게 될 전망
          + 본질적으로 체제에 불순한 목소리를 검출하고 억제하려고 모든 것을 “가짜”로 낙인 찍는 시스템의 의도 가능성
          + 소규모 기업이 도입하기 어렵다는 지적은 맞는 말<br>EFF(Electronic Frontier Foundation)는 인터넷 자유의 진짜 본질에는 이제 거의 관심도 없고, “Big Tech가 나쁘다”는 헤드라인에만 집중하는 조직이 됐다는 생각<br>이미 수십 년간 Big Tech는 사회 정보의 선의의 관리자였던 게 사실이라고 생각하고, 동시에 완전히 감독받지 않는 젊은 20대들이 연방 정부 데이터를 마구 자기 맥북에 옮기는 요즘 같은 시기에 이런 논조는 시대착오적<br>우리가 겪는 건 프라이버시 대재앙 그 자체인데, EFF는 메타, ByteDance의 클릭스트림만 타박함
     * 15년 전만 해도 이런 상황이 말도 안 된다고 생각됐지만, 이제는 다 지나간 논쟁거리로 느껴짐
          + 사실 25년 전 정도가 기점이었고, 2001년 이후 PATRIOT Act처럼 프라이버시 무력화한 법들이 생기며 이미 돌이킬 수 없어진 상황
          + 놀랍지도 않음<br>감시 자본주의가 오랜 기간 사회 운영 방식의 기본이 되다 보니 이런 게 당연한 현실이 되어버림<br>요즘 세상은 모두가 각자도생하며 자기 이익만 챙기려는 ‘서바이벌 게임’이라는 씁쓸함
          + 어느새 좌파까지도 “가짜 뉴스”와 감정 상처 방지를 명분으로 검열 제도를 적극 지지하게 됐고, 결국 이 제도가 좌파와 우파 체제 비판자 모두에게 되돌아가서 역이용되는 아이러니
     * 근본적으로 이런 논쟁에는 그 매체가 사회에 “소셜미디어”로 공유되고 있다는 전제가 깔려 있음<br>실제로 폐쇄적 커뮤니티나 비공개 공간에선 이런 법안이 무쓸모 아닌지 의문<br>혹시 진짜로 우리가 원했던 건 소셜미디어의 종말일 수도 있겠다는 생각
          + 핵심은 플랫폼 설계 문제라는 점을 학계에서는 이미 알고 있음<br>표면적 자유/책임 논쟁이 아니라, 진짜 해결책은 정부가 소셜미디어에 알고리즘적 마찰 계수를 강제로 도입하게 만드는 것<br>그런데 현실적 경제적 이해관계 때문에 정치권에서는 이런 변화의 의지가 전혀 없음<br>오히려 지금 구조의 혼돈, 분열, 혼란이 지배 계층에게 이익이라는 점<br>참조 연구:<br>• MIT Aral & Eckles(2019): 마찰(friction) 도입 시 표현의 자유 제한 없이 허위 정보 확산 현저히 감소<br>• Mozilla/Stanford(2020~2022): “공유 재확인” 같은 마찰은 가짜 뉴스 확산력 최대 50% 감축<br>• Twitter(2021): 업로드 전 사실 확인 프롬프트 노출 시 약 25%가 트윗 수정이나 삭제 선택
          + 실제로 “소셜미디어”라는 범주를 법적으로 정의하려면 엄청난 어려움 있음
     * 그냥 거짓말 자체를 금지해 “진실부” 같은 기구 세우지 않는 이유가 뭘까?<br>거짓말을 규제한다는 시도는 사실상 정권 입맛 따라 특정 내러티브만 밀어주는 결과
     * 모든 이미지를 복제 필터에 걸치게 한다고 해도, AI만 있으면 같은 프롬프트로 완전히 다른 결과물 수백 개 뽑을 수 있고, 이제는 이미지 ‘의미’까지 AI로 판별하라는 얘기<br>결국 칩 제조사, 전력회사에겐 호재
     * 개인적으로 원글 저자 입장과 반대일 수 있음<br>NO FAKES를 DMCA에 비유하는 건 근거가 부족해 보임<br>다른 관점을 제시한 기사 참고: https://recordingacademy.com/advocacy/news/…
          + 이런 회사(정치적 이해관계자)들은 코끼리에게 뱀가죽 팔 수 있을 만큼 영향력 있음<br>이런 홍보성 기사에 정말 잘 속는 내 모습
     * 로비, 위기의식 조장 논란 빼고 아주 쉽게 설명한 요약 한 줄 원함<br>이 법이 저작물의 2차 가공을 막기 위해 콘텐츠 워터마크 활용을 합법화하는 건가, 아니면 다른 내용인지 정확히 궁금<br>구체적으로 어디서부터 어디까지가 실제 적용인지 알고 싶음
          + 워터마크랑은 무관<br>누구든 아무런 증거 없이 호스팅된 모든 콘텐츠를 상대로 “가짜”라고 주장만 해도, 호스트가 법적으로 진짜임을 입증할 책임을 져야 함<br>즉시 삭제가 원칙이며, 악용 여지가 DMCA와 비슷하지만 훨씬 포괄적<br>새로운 권리 개념이 추가되어서 실제로 정당한 데이터 소유임을 증빙하는 게 사실상 불가능<br>결론적으로 미국 밖으로 서버 이전할 명분만 늘어남
          + 원문 대강 읽어봤는데, 느낌상 DMCA의 이미지/툴/파생 작품 버전이면서 제작자, 도구, 발행자 추적까지 전부 로그로 요구하는 식<br>다만 전체 법안 맥락은 제대로 숙지 못한 점
     * 테네시주는 7월 1일부터 제너레이티브 AI 전반을 다루는 법안 여러 개를 동시에 시행하게 됨<br>문제는 주의회가 용어 자체를 잘못 이해해서 그런지, 사실상 GPU 소유 자체를 불법화하는 수준으로 내용이 너~무 광범위해짐<br>예시: “텐서 코어 사용 허가증 갖고 있냐?”라는 농담까지 나올 정도<br>거기다 전국적으로도 연방농가법(Farm Bill)보다 더 강력하게 대마초 규제하는 법들도 추진 중<br>반대로 텍사스 주지사는 헌법적/법적 문제를 이유로 유사한 법안을 적절히 거부한 전적
     * “이런 법이 Reddit을 끝장낼 것”이라는 주장에 동의 못함<br>누구든 자유롭게 말할 수 있는 공간이 있다는 게 두려운가? 그곳에선 때로는 뜻밖에 ‘좋지 못한 의견’도 나오기 마련
          + 중요한 건 플랫폼이 사용자가 원하는 방식 그대로 큐레이션을 하도록 하는 것과, 플랫폼이 임의로 새로운 콘텐츠를 무작위로 추천하는 걸 구분해야 함<br>CSAM 등 명백히 불법인 것 외에는 플랫폼이 이용자 눈앞에 뭘 보여줄지 정하지 않아야 함
          + 같은 논리라면 Hacker News도 사라져야 한다는 결론
          + Section 230에 관한 “You are Wrong About Section 230”이라는 글을 반복해서 언급할 필요 있음: https://techdirt.com/2020/06/…<br>핵심은 Section 230 자체에는 퍼블리셔/플랫폼 구분 개념이 어디에도 없다는 점<br>이런 구분과 의미는 논쟁거리로만 부풀려졌을 뿐 실체가 없음
"
"https://news.hada.io/topic?id=21637","uv와 PEP 723으로 Python 스크립트 활용하기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     uv와 PEP 723으로 Python 스크립트 활용하기

     * uv 패키지 매니저와 PEP 723을 통해 의존성 문제 없이 Python 스크립트 실행이 가능해짐
     * uvx 기능은 Disposable 가상환경을 자동 생성하여 환경 설정의 불편함을 해결함
     * PEP 723 메타데이터를 Python 파일에 포함하면 스크립트 자동 실행 및 패키지 관리가 편리해짐
     * 실행 스크립트 예시로 YouTube 자막 추출 프로그램을 빠르게 구현 및 배포할 수 있음
     * 이를 통해 이제 Python도 간결한 단일 실행 파일 작성이 가능해져 스크립트 활용성이 크게 향상됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * Python에서 ""일회성(one-off) 스크립트"" 실행 시 매번 환경 설정과 패키지 설치 과정을 거쳐야 했던 불편함이 uv와 PEP 723 도입으로 사라짐
     * uv는 Rust로 개발된 고속의 Python 패키지 및 프로젝트 매니저이며, 신규 기능 uvx를 포함하여 Disposable 가상 환경 설정, 패키지 자동 설치, Python 버전 연동을 매우 빠르고 간편하게 처리함

uv와 uvx 장점

     * uvx 기능은 Nodejs 생태계의 npx와 유사하게 동작, 지정된 Python 패키지(예: ruff) 실행 환경을 신속하게 생성함
     * Disposable virtual environment를 캐시로 활용하여 오버헤드 없이 빠른 실행 제공
     * 환경 세팅과 의존성 설치가 명령 한줄로 이루어져, 개발자가 별도 환경 설정 관리 필요 없음

PEP 723 소개 및 활용

     * PEP 723은 Python 스크립트 상단에 의존성 및 환경 메타데이터를 포함할 수 있도록 표준을 정의함
     * 예시: 코드 상단에 requires-python, dependencies 등 명시 가능함
     * 이를 인식한 외부 도구(uv 등)는 스크립트 파일 내 작성된 정보로 자동 설치, 환경 세팅, 실행을 처리함

uv와 PEP 723의 결합

     * 실제 Python 예제 파일 상단에 해당 메타데이터를 작성하고, uv의 run 명령어로 실행 시 즉시 필요한 모든 패키지 자동 설치와 지정 Python 버전 세팅 후 코드 실행이 이루어짐
     * 예시 코드는 인터넷 API(PEP 목록)를 호출하고 결과를 출력함. 추가 패키지 설치나 환경설정 필요 없이 한줄 실행 가능

실용 예: YouTube 자막 스크립트

     * shebang(#!/usr/bin/env -S uv run --script)과 PEP 723 메타데이터를 추가한 Python 스크립트 예시 제공
     * youtube-transcript-api 등 외부 패키지 자동 설치 및 가상환경 자동 세팅 처리
     * 사용자는 실행 파일(ytt)로 Youtube 동영상 URL이나 ID를 인자로 주면, 자막을 추출하여 결과를 즉시 제공 받음
     * chmod로 실행 권한 부여 후, 터미널에서 간단하게 실행 가능

개발자 경험 개선 및 가능성 확장

     * 과거에는 단순한 스크립트 실행에도 Go 등 단일 실행 파일 바이너리 언어를 더 선호했으나, 이제 Python도 동등한 수준의 간편함 제공
     * uv와 PEP 723 조합으로 Python 스크립트의 공유 및 배포, 실행 자동화가 크게 수월해짐
     * Github 예제(cottongeeks/ytt-mcp)를 통해 좀 더 복잡한 사용 사례 개발 가능

   uv 엄청나게 빨라서 좋더라구요. 요즘엔 가능한 모든 곳에 uv를 사용하고 있습니다.

        Hacker News 의견

     * 글 작성자처럼 요즘은 Go 대신 크로스 플랫폼 파이썬 원오프(one-off)와 개인 스크립트에 손이 더 가는 편이지만, 파이썬 타입 체크 시스템이 혼돈 그 자체인 상황에 불만족 느낌 ty, pyrefly 같은 툴이 조금이나마 개선해주길 기대하는 마음
     * 이젠 파이썬 스크립트가 virtualenv 때문에 고생하지 않고 바로 잘 작동하는 느낌 쉘 스크립트 쪽도 이런 경험이 생기면 좋겠다는 생각 패키징, 의존성 관리, 재현성은 여전히 석기시대 상황 지금은 여전히 curl | bash로 운에 맡기거나, 누락된 의존성 3개와 수동 단계 12개가 들어간 README 파일뿐인 현실 Nix? 이미 시간과 공간, 그리고 Nix 매뉴얼을 초월한 사람에게만 쓸만한 선택지 느낌 Docker? sed 명령 한번 쓰려고 리눅스 배포판을 다운로드하는 게 합리적으로 보인다면야 괜찮은 선택 누구나 쉽게 쓸 수 있는 단순하고 선언적인 중간지점이 꼭 필요하다는 생각
          + 나는 꼭 대상 OS에 기본 내장된 바이너리와 라이브러리만 사용하는 쉘 스크립트만 작성하는 방식 선호 쉘 스크립트를 이식성 있게 만든다는 목표는 실질적으로 어울리지 않는 선택 macOS 전용 명령을 쓰는 쉘 스크립트를 리눅스에서 써야 한다면, 어떤 패키지 매니저도 해결 못하는 문제
          + Nix는 이 목적엔 그렇게 어렵지 않은 사용성 느낌 다른 배포판에 Nix 설치하면 다음과 같은 식으로 간단히 쓸 수 있다는 예시 공유 NixOS나 Nix 패키징 전체는 꽤 도전적이지만, 쉘스크립트에만 한정하면 진입장벽이 낮은 편
          + 굳이 새로운 쉘 스크립트를 작성할 필요를 못 느끼는 상황 모든 의존성 설치가 허용된 환경이라면 uv 같은 툴이 다 처리해줌 Clojure를 좋아하면 babashka도 추천
          + 쉘스크립트에서의 패키징과 의존성 관리, 재현성이 구시대 같은 이유는, 그런 것들이 필요할 정도면 쉘이 적합하지 않은 상황이라는 생각 쉘은 20줄 내외의 작은 스크립트일 때만 어울린다는 입장 언어 자체가 그 이상의 규모에 어울릴 정도로 품질이 뛰어나지 않음
          + mise 추천 회사에서 개발 환경 관리에 활용 중이며, Docker와 Nix보다 훨씬 간단하게 환경 구성 가능 병렬 설치 지원이라 일반 Dockerfile보다 큰 장점 체감
     * 정말 멋진 트렌드이고 점점 더 대중화되는 느낌 처음 simonw 블로그에서 접했고, simonwillison의 블로그 포스트에서 관련 내용을 확인 올해 3월에 다른 블로그 게시글로 Hacker News 토론도 있었음 이런 트렌드가 메인 페이지에 오래 머물러 더 많은 사람들이 인지하면 좋겠다는 바람
     * uv를 작은 프로젝트에 써 보니 정말 뛰어난 경험 uv run과 uv tool run(uvx) 조합으로 GitHub의 파이썬 스크립트를 vm에서 바로 설치하고 실행하는 과정이 초간단 git clone도, venv 생성이나 진입, pip install도 필요 없음 무엇보다 uv 속도가 너무 빨라서 처음엔 뭔가 잘못된 줄로 느껴질 정도, 실제론 pip보다 10배 빠른 결과물 다만 도구와 문서가 아직은 조금 미완, 하지만 그 정도 혁신성과 실용성으로 충분히 쓸 만하다는 입장
          + 진심으로 uv가 pyenv에서 --help 출력하는 시간보다 의존성 설치 속도가 더 빠름에 감탄
     * 러스트도 이와 비슷하게 싱글 파일 타입 쉘 스크립트 아이디어를 발전시키는 중 원래 러스트에서 이런 방식(의존성 관리 포함된 단일 파일 실행 지원)을 처음 봤음 이 패턴이 더 많은 언어에서 자리 잡길 바라는 입장, gist로 주고받거나 빠른 소규모 툴 작성 등에 매우 유용 cargo-script RFC 문서도 참고
          + C#도 해당 기능 지원: dotnet-run-app 공식 안내
     * uv run --script를 사용할 때 메타데이터를 스크립트에 포함시키면, 스크립트에서 바로 파이썬 REPL을 띄워 수정 테스트를 하기가 약간 불편함 예를 들어 아래처럼 해야 하는데,
$ uv run --python=3.13 --with-requirements <(uv export --script script.py) -- python
>>> from script import X

       좀 더 간결한 방식이 있으면 좋겠다는 희망 예를 들어
$ uv run --with-script script.py python

       처럼 가능하면 베스트일 것 같은데, 실제로는 아래와 같이 실행하면 스크립트에 맞는 파이썬 및 venv 환경 바로 진입 가능
$ ""$(uv python find --script script.py)""
>>> from script import X

       다만 한 번은 스크립트를 실행해 환경 생성이 필요
          + 셋업 후 REPL 호출하는 기능이 필요하다면 이 gist 예시 참고 추천 스크립트에 --interactive 같은 플래그를 넣어 CLI 옵션화할 수 있음 Typer 기반의 작은 CLI를 보통 이런 식으로 자주 작성 dev 스크립트에서는 --sql 플래그로 DuckDB SQL REPL 진입했던 경험도 있음
          + 간단한 방법 한 가지 소개
cat ~/.local/bin/uve
#!/bin/bash
temp=$(mktemp)
uv export --script $1 --no-hashes > $temp
uv run --with-requirements $temp vim $1
unlink $temp

     * conda를 쓴다면, 파이썬 스크립트용 shell 래퍼에서 직접 환경 활성화하는 방식 가능 아래와 같이 작성
#!/usr/bin/env bash
eval ""$(conda shell.bash hook)""
conda activate myenv
python myscript

       다만, PEP 723 스타일처럼 독립적이진 않은 접근이라는 점
     * 어제와 오늘 HN 스레드를 보고 처음 uv 체험 결정, 빠른 속도와 쉬운 의존성 관리에 깊은 인상 공식 문서가 개선된다면 더 좋을 듯, 특히 requirements.txt 워크플로에서 uv로 전환하는 가이드가 있으면 편리할 것 같음 프로젝트별 파이썬 버전 지정이 혼란스러운 부분( .python-version과 pyproject.toml 두 곳에서 정의)
          + 파이썬 개발자 도구 익스플로러 전자책 작성 경험 있음 공식 문서가 부족한 부분을 안내한 적 있는데, requirements.txt에서 마이그레이션 방법 uv 프로젝트의 파이썬 버전 변경 방법도 참고 추천 추가로 다루었으면 하는 주제 있으면 언제든 제안 요청
          + pyproject.toml의 requires-version 필드는 호환 보장 버전 범위를 의미, .python-version은 개발에 쓸 특정 버전을 지정 uv init으로 만들면 초기엔 같아 보이지만, 시간이 지나면 requires-version은 .python-version보다 더 낮은 최소 지원 버전을 지정하게 됨 requires-version은 패키지 메타데이터에도 들어가고, 배포한 패키지를 사용할 타인의 의존성 해석에도 영향을 미침 예를 들어 v1은 Python 3.구버전도 지원하지만 v2는 안 할 경우 등
          + 비슷하게 느끼는 점이 있는데, 나만의 워크플로(동일 파일을 모든 컴퓨터에 드롭박스로 동기화하며 플랫폼에 관계없이 사용)를 고집 중 npm이나 dotnet처럼 플랫폼 변경할 때 npm update나 dotnet restore는 필요하지만, venv는 문제없이 잘 작동 반면 uv는 이렇게 플랫폼 스위치할 때 일 처리가 더 복잡하고 수동정리가 필요한 느낌
          + pyproject.toml은 (uv 자체와는 상관없이) 외부 개발자와 사용자에게 코드를 공유할 때 필요한 환경 정의 목적 PyPI용 패키지 빌드 시 어떤 환경이 필요한지 명시하는 것이며, 버전 범위도 여러 사람이 코드 재사용 가능한 폭을 넓히기 위해 설정 .python-version은 uv에게만, 내 개발 환경을 셋업할 때만 참고하는 용도 미리 만들어 놓은 환경이 있다면 굳이 새로 설정 안 해도 무방 uv가 아직 공식 빌드 백엔드는 아니지만 해당 기능을 준비 중(이슈 #3957)
          + 자세히 알아본 적은 없지만, .python-version 파일의 역할은 TOML 파서가 없는 다른 툴을 위한 호환 목적 정도로 추정
     * 예전에 파이썬 스크립트가 알아서 의존성을 설치하도록 만드는 도구를 만들어보고 싶었던 경험 있음 (uvx처럼 동작하는 도구를 목표, 단 파이썬만 있어도 동작하는 방식) 다만 스크립트 맨 앞에 이상하게 보일 만한 라인을 여러 줄 붙여야 하는 점이 단점 궁금하다면 pypi에 pysolate라는 이름으로 공개 중
          + 비슷하지만 널리 쓰이지는 않은 isolate 프로젝트도 있음 방식은 조금 다르지만 흥미로운 접근 사례
     * COBOL에서 영감을 받은 Grace Hopper 스타일 메시지 모든 파이썬 프로그램에는 ENVIRONMENT division을 정의해 컴파일 및 실행 환경(하드웨어, 소프트웨어 요구사항 포함)을 명확히 기재하는 문화 필요 주장 이런 구조는 다양한 시스템 간 프로그램 이식성 개선에 결정적 영향
"
"https://news.hada.io/topic?id=21673","푸에르토리코의 태양광 마이크로그리드가 정전 사태를 극복함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    푸에르토리코의 태양광 마이크로그리드가 정전 사태를 극복함

     * 4월 16일 푸에르토리코 전역이 정전된 상황에서 Adjuntas 마을은 마이크로그리드와 태양광 저장 시스템 덕분에 상당수 지역에 전력 공급 유지
     * 푸에르토리코 전력망의 노후화와 관리 부재로 인해 반복적인 정전이 이어지고 있으며, 2017년 허리케인 Maria 이후 수십억 달러의 복구 예산이 배정되었으나 관료주의와 정치적 갈등으로 지연됨
     * 최근 미국 에너지부가 루프탑 태양광 지원금을 기존 화석연료 기반 전력망 인프라 개선에 전환하기로 하면서 현지 업계와 정치권의 반발이 발생
     * 민간 주도로 태양광+배터리 시스템이 빠르게 확산되고 있으며, 특히 Adjuntas는 여러 마이크로그리드를 상호 연결해 정전 시에도 일부 지역이 자립적으로 전력을 유지하는 실증적 모델 구축
     * 커뮤니티 주도의 분산형 에너지 전환이 탄력을 받는 가운데, 정부 정책과 별개로 주민 중심의 태양광 도입이 지속적으로 확대되는 추세
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Puerto Rico’s Solar Microgrids Power Through Blackout

  전체 정전 속에서도 빛난 Adjuntas 마이크로그리드

     * 2025년 4월 16일 푸에르토리코 전역이 정전된 가운데, 산악지대에 위치한 Adjuntas 마을은 마이크로그리드, 태양광 패널, 저장 장치의 조합으로 상당수 상점과 가정에 전력 공급 유지
     * 다른 지역 주민들은 24시간 이상 전력 복구를 기다려야 했음
     * 이번 정전은 노후 전력망과 관리 부재로 인한 반복적인 사태 중 하나로, 이전에도 식생물 관리 부족, 노후 케이블, 태풍으로 인한 대규모 정전 발생

  전력망 위기와 정책 혼란

     * 수십 년간의 관리 부실과 투자 부족으로 전력 인프라가 노후화
     * 2017년 허리케인 Maria 이후 약 200억 달러의 연방 재난 복구 자금이 배정되었으나, 관료주의와 정치적 갈등으로 집행이 지연됨
     * 최근 미국 에너지부가 3억 6,500만 달러의 태양광 지원금을 기존 화석연료 기반 전력망 인프라로 전환하기로 결정, 지역 태양광 업계와 정치권에서 반발 발생

  태양광의 역할과 커뮤니티 실천

     * 정치적 혼란과 연방 자금 병목 속에서도, 민간 주도로 태양광+배터리 시스템(리스·대출·PPA 등) 확산
     * 매달 약 4,000개의 태양광+배터리 시스템이 신규 가동
     * 2025년 3월 기준, 1.14GW의 분산형 태양광·2.34GWh 배터리가 전력망에 연결
     * 태양광 발전이 연간 전체 가정용 전력소비의 12.5% 이상 차지, 대부분 주거용 설비에서 생성

  Adjuntas의 실험적 마이크로그리드 모델

     * 인구 18,000명의 Adjuntas에서는 지역 환경 단체 Casa Pueblo와 미국 에너지부 Oak Ridge National Laboratory가 여러 마이크로그리드를 상호 연결하는 '그리드 오케스트레이션' 전략을 실증
     * 이 방식은 한 곳에 정전이 발생해도 다른 곳의 전력망에 영향이 없도록 분산형·중복 구조를 만듦
     * 5개의 마이크로그리드가 총 228kW 태양광과 1.2MWh 저장 용량을 제공, 가정과 15개 상점에 전력 공급
     * 마이크로그리드 간 에너지 거래 실증, 주간 피크 타임에는 타 마이크로그리드의 저장 전력을 활용, 야간에도 저장분 공유로 독립적 운영 가능

  커뮤니티 주도의 미래 에너지 전환

     * Casa Pueblo는 커뮤니티 에너지 전환 연구소를 설립해 향후 인근 지역뿐 아니라 지리적으로 떨어진 마이크로그리드 간 연결을 목표로 실험 확대
     * 푸에르토리코는 무더위와 허리케인 시즌을 앞두고 추가 정전 위험에 대비 중
     * 현지 전문가들은 태양광+배터리의 분산형 에너지가 전체 정전 리스크를 낮추는 핵심이라고 강조
     * 정부의 화석연료 중심 정책에도 불구하고, 주민 주도의 태양광 투자와 에너지 자립이 빠르게 확산

  요약

     * 분산형 태양광+저장 기반 마이크로그리드가 푸에르토리코의 전력망 회복탄력성과 지역 자립성을 높이는 핵심 사례로 부각
     * 정책 혼란과 관료주의로 인한 공공 인프라 개선의 한계를 커뮤니티 중심 혁신이 보완하는 전환점

        Hacker News 의견

     * 집에서 사용할 소규모 태양광 시스템을 만드는 단계별 가이드나 좋은 링크가 궁금함
          + 건축 허가가 필요 없을 만큼 규모가 작아야 하고
          + 외관도 예뻐서 이웃들이 불만을 가지지 않을 디자인이어야 함
          + 배선은 플러그앤플레이 형태로 간단하게 하고 싶음
          + 내가 생각한 최선의 계획은 글램핑용 중형 배터리 팩을 구매해 벽에 꽂고, 냉장고와 전력이 많이 필요한 기기들(예: 지하습기제거기)을 연결해서 사용하는 방법임
          + 뒷마당 데크에 영구적이지 않은 작은 지붕을 만들어(건축 허가 요건 회피), 지붕 위에 태양광 패널을 올리고 거실에 둔 배터리까지 선을 연결하는 아이디어
          + 유럽에서는 발코니에 간단히 올려놓고 근처 콘센트에 꽂는 소규모 태양광 패널을 종종 사용함
               o Ecoflow Powerstream 리뷰 참고
               o 이 기사에는 콘센트를 입력용으로 쓰는 것이 안전한지에 대한 논의와, 이는 여러 유럽 국가에서는 허용되지만 미국에서는 허용되지 않는 점이 설명되어 있음
               o 대신 가전제품을 배터리에 바로 꽂아 사용하는 방식이 항상 가능함
          + 지역 규정에 따라 다르지만, 대부분 지붕에 설치하는 태양광은 건축 허가가 필요하지 않을 수 있음
               o 하지만 전기 허가는 거의 항상 필요함
               o 비용 절감을 목표로 한다면, 휴대용 배터리 팩 대신 AIO(올인원) 인버터와 서버랙 배터리를 추천
               o 패널과 배터리를 간단하게 연결 가능
               o 가정에서 전력을 돌릴 계획이라면, 전체 그리드와 맞물리지 않고도, 전기기사가 꼭 필요한 부하 패널을 만들어 인버터 출력으로 공급하게 하거나, 인버터가 그리드에 백업용으로만 연결되게 작업하면 가장 쉽고 저렴함
          + 참고자료:
               o mobile-solarpower.com
               o diysolarforum.com
               o 영구적이거나 그리드와 접속하는 설치의 경우 대부분 전기허가가 필요할 수 있음
               o Goal Zero, Jackery 스타일과 유사한 모바일 48v 시스템 영상
          + DIY 태양광 정보가 온라인엔 매우 많지만, 허가와 시스템 설계는 지역마다 크게 다름
               o 예를 들어 플로리다에선 패널 성능이 좋아 많은 출력이 나오지만 허가 조건이 복잡함(허술하게 설치하면 허리케인 등에서 위험)
               o 미시간에 살 땐 허가나 구역 관련 규제가 적었지만 겨울 효율이 낮아서 패널 3~4배는 필요했음
               o 정말 소규모 시스템은, 극도로 전기가 적게 필요한 라이프스타일이 아닐 경우 투자 대비 효용이 그리 높지 않음
          + 나도 이런 DIY태양광에 관심이 많음
               o 기존의 코스트코 금속 파고라 대신, 태양광 지붕이 있는 파고라를 만들고 싶음
               o Jackery나 Anker Solix로 유사한 시스템을 구축하는 영상들을 본 적 있음
               o 2~3개 정도 하버프레이트 태양광 패널로는 실제로 집무실 정도만 돌릴 수 있음. 냉장고는 배터리 소모가 금방임
               o 입력/출력을 벽 콘센트를 통해 할 수 있고, 반드시 각 가전을 배터리/인버터에 직접 연결할 필요는 없는 것으로 알고 있음
     * 파키스탄에서는 태양광이 농촌과 도시 모두에 큰 효과를 내고 있음
          + 실제로 세계에서 가장 많은 태양광 패널 수입국이 되었음
          + 인도에서는 관개용 운하 위에 태양광을 설치해 좋은 성과를 내고 있음
               o 운하의 증발 손실을 줄여주고, 구조물 설계는 조금 까다로움
          + 파키스탄 태양광의 부가적 효과(그늘 제공 및 미세기후 개선)에 대해서도 궁금함
               o 관련 논문 링크
          + Volts 팟캐스트 에피소드에서 파키스탄 태양광 붐에 관한 흥미로운 결과들을 배움
               o Volts 파키스탄 태양광 붐 에피소드
     * 나는 전력망에 대해 잘 모르지만, 남아프리카의 만성 정전 문제에 이런 개념의 태양광 시스템이 도움이 될 수 있을지 궁금함
          + 내 도시는 정전이 매우 잦고, 대부분의 사람이 집에 태양광을 설치할 여력이 없음
          + 대다수가 태양광을 꼭 직접 설치하지 않아도 됨
               o 파키스탄도 비슷한 순환정전 문제였지만, 중국산 태양광 장비와 배터리 수입으로 그리드 부하를 줄여 정전이 크게 감소함
               o 수요 감소가 너무 커서 석탄발전소의 재정이 위태로울 정도임
               o 추가 논의 링크
          + Eskom(남아공 국영 전력공사)이 ""비규격"" 태양광 설치에 대해 소송을 걸고 있음
               o ANC 남아공에서 이 이슈는 정치적인 문제이고, Eskom이 실질적 서비스를 제공하지 않으면서 이용 요금을 위해 법적 조치를 취함
               o 모인 돈은 다시 자신들 친인척의 무의미한 서비스에 흘러감
               o 뉴스 링크
          + 내가 알기로 남아프리카 전력문제는 오랜 정치적 원인임
          + 송전선 용량이 포화된 경우, 특히 더운 날엔 배터리가 도움이 됨
               o 피크세이빙(피크수요 시 배터리 방전)으로 완전한 해결은 불가지만, 일부 시간대엔 부하이동(load shift)이 가능
               o 대규모 배전망 확보가 될 때까지도, 온실가스 감축 측면에서 배터리는 의미가 있음
          + 남아프리카 문제의 핵심은 ANC가 90년대 집권 이후 Eskom의 발전소 신증설을 확실하게 막았기 때문임
               o 경쟁 유입을 시도했지만 실행되지 않았고, 정부의 무능으로 문제 방치
               o 유지보수만 들어가도 전체 공급용량이 부족함
               o 그 덕분에 비싼 외제차 구입한 이들이 많아진 것이 유일한 이득인 상황
     * 태양광은 정말 좋아하지만, ""미시그리드가 가능한 계층만 정전에서 벗어난다""는 점과 ""순발전량에 대해 프리미엄 받고 고가 전기를 싸게 쓰는 구조""인 넷미터링 방식이 장기적으로 그리드의 진정한 복원력을 높인다고 볼 수 있을지 의문임
          + 한편 이렇게 도입 초기에 얼리어답터 덕분에 대량생산이 이루어져, 가격이 내려가 결국 더 많은 사람이 사용할 수 있게 된다는 시각도 있음
          + 넷미터링의 적용방식에 따라 다름
               o 1:1 크레딧 방식(예: 푸에르토리코)이라면 실질적 도움이 안 됨
               o 공급 조건에 따라 변동 크레딧을 주고 적절한 제어를 가한다면, 저장장치가 그리드 안정성 개선에 기여할 수 있음
               o 대규모 저장 프로젝트도 더 효율적이지만, 프로젝트 추진이 어려운 곳에선 개인 자본으로 설치해 수요 피크 때 그리드에 전력 공급하는 것도 충분히 유용함
          + 실제 경험에 따르면, 규제된 전력시장조차 수익 극대화를 위해 게임이 일어나고, 이는 그리드 안정성을 오히려 악화시킴
               o 재생에너지 확대와 안정성 목표가 충돌하고, 규제당국도 이를 해결하기 쉽지 않음
          + 넷미터링은 캘리포니아 대부분에서(신규시스템 기준) 이미 사라졌고, 이 흐름은 확산 중임
               o 분산형 태양광은 UL 1741-SB 기준에 맞게 모두의 그리드 안정성에 도움을 줌
     * 최근 ""그리드 동기화"" 인버터라는 장치가 전원 전환 스위치 없이 그리드로부터 땡겨오는 에너지를 일부 소거시켜주는 기능을 제공함
          + 정전시 수동으로 그리드 차단 스위치를 작동시키면 배터리나 패널 용량 내에서 집을 독립적으로 운영 가능
          + 이런 장치는 아직 북미에 대중화되지 않아, 기존 그리드타이/넷미터링 방식에 비해 진입장벽이 훨씬 낮아질 수 있는데 아쉬움이 큼
          + 푸에르토리코의 ""마이크로그리드""들이 어떻게 그리드와 동기화되고, 동네 단위로 격리되는지 궁금함
          + 그리드 동기화 인버터는 아일랜드(섬 형태) 운전과 동일하지 않음
               o 자동 전환스위치 없이 운용 가능한 이유는, 그리드 신호 없을 땐 작동이 멈추고 외부에 전력이 역송되지 않도록 설계된 때문
               o 대부분 인버터는 단순 그리드 추종(Grid-following) 방식으로, 60Hz 신호를 만든 다른 소스(배터리/발전기)가 필요함
               o 전기모터 가동처럼 순간적으로 큰 부하가 들어오면 태양광만으로는 출력이 급감할 수 있기 때문에, 이런 스파이크 부하를 안정적으로 처리하려면 오프그리드 환경에선 배터리 또는 추가 발전원이 꼭 필요함
     * 나는 여름철 피크 사용 요금(가장 더운 날 자연히 태양광이 많음)을 완화할 정도만 태양광을 도입하고 싶음
          + 완전히 그리드에서 독립할 필요도 없고, 넷미터링 크레딧에도 관심 없음
          + 여름에는 300달러 청구서가 나오고 겨울은 50달러에 그침. 이런 문제의 좋은 해결책이 궁금함
          + 오리건에서는 Community Solar 프로그램이 있음
               o 옥상에 직접 태양광 패널 얹는 대신, 대규모 태양광 발전소 구독 형태로 전기요금 청구서에서 크레딧을 받음
               o 나는 신청하지 않았지만 주위의 평이 좋음
               o 오리건 Community Solar
          + 플로리다 지붕, 북캘리포니아 지상(Enphase 중심) 등 누적 200kW 이상 주택용 설치 경험이 있고,
               o 최근 중서부에서 200MW 태양광 프로젝트에 참여 중임
               o 지역, 전기요금, 계절적 요금 패턴 등의 정보를 공유하면 가이드 제공 가능함
          + 시애틀에서는 토탈 코스트 관점에서 태양광이 어차피 경제적으로 맞지 않음
               o 연중 햇빛이 너무 부족해서 20년 운영해도 수지가 맞기 어렵다는 온라인 계산기 결과가 신뢰됨
          + EG4에서 나오는 다이렉트 AC 미니스플릿(미경험, 약 $1500 패널비 별도) 제품을 추천
               o 남는 전기는 절대 안 나오고, 해지면 못 쓰지만 한여름엔 집이 시원할 듯함
          + 눈 많은 캐나다 산골 마을에서 7.8kW 옥상 발전으로 연간 $950치 전기 생산(단가 $0.13/kWh)
               o 전체 집의 냉난방을 히트펌프로 해결하고, 가스보일러 철거 및 가스관 차단으로 연간 2천달러 절감
               o 삶의 질이 크게 바뀐 경험담
     * 이 기사에서 불분명한 점은, 계통 연결 실패시 어떻게 되는지에 대한 규정이 있는지임
          + 또는 그리드 연결 조건으로 개인 저장장치+태양광 의무화 여부, 그리고 ""3개 섬""을 어떻게 연결했는지 궁금함
     * 제3세계이면서 관료주의적인 이탈리아에서는 태양광 설치를 위한 서류작업에 몇 달씩 걸림
          + 직접 설치도 800W까지만 허용되어 오늘날엔 턱없이 부족한 용량임
          + 잉여 전력을 그리드에 공급하지 않는 경우에만 20kW까지 설치가 가능함
          + 이탈리아를 제3세계 국가라고 하는 건 너무 나간 표현임
          + 개발도상국에서 충분한 시간을 보내본 적이 없다면 이런 비교는 할 수 없음
               o 실제 경험이 다르면 관점이 완전히 다름
     * 이게 사실은 배터리 가격 문제가 아닐지 궁금함
          + 배터리를 갖고 있고 그리드에서 충전했다가 정전 때 방전시키면 적정 정전기간에는 그럭저럭 해결될지 생각함
          + 물론 배터리 자체를 감당할 수 있어야 함
          + 문제는 정전이 수 시간~하루라면 배터리로 버틸 수 있지만, 며칠~몇 달 갈 경우 현실적으로 큰 발전기를 추가해야 한다는 것임
               o 배터리 가격이 계속 하락은 하겠지만, 한 달 치 전력을 저장하는 것은 발전기 구매보다 경제성이 떨어질 것임
          + 만약 그리드에서 충전해놓은 배터리를 정전 때 쓰는 식이면 사용할 수 없음
               o 분리된 장치는 계통 주파수 신호를 볼 수 있기 때문에 가능한 것임
               o 정전이 되면 이 정보가 사라지고, 분리된 개별 장치들은 동기화된 ""블랙스타트""가 불가함
          + 블랙아웃 길이에 따라 다름
               o 하루 이상이라면 태양광이 있어야 필요한 배터리량을 줄일 수 있음
     * 내가 아는 한, 대부분의 태양광+전력망 연결된 가정은 계통이 살아 있어야 태양광 발전이 작동함
          + 이유는 두 가지임
               o 첫째는 그리드 작업 중인 작업자 안전 때문
               o 둘째는 AC 주파수를 보험을 위해 그리드에 동기화해야 하기 때문임
          + 푸에르토리코에선 이런 문제가 없는지, 아니면 새로운 기술이 도입된 건지 궁금함
          + ""아이슬랜딩""(Islanding)이 바로 그 용어임
               o 이제는 배터리를 포함한 PV 시스템에서 ""아이슬랜드 모드""(섬처럼 분리운영) 지원이 점차 보편화되고, 합법적으로 허가도 받고 있음
               o 그리드가 나가거나 기준 벗어나면 집을 자동 분리하고 자체 공급으로 전환함
               o Tesla, Sigenergy가 대표적임
               o 일부 제품은 거의 무중단 UPS처럼 동작하고, 몇몇은 순간 정전 후 민감한 기기 재시작이 필요
               o 또 어떤 제품은 분리 전환에 시간이 걸림
          + 마이크로인버터가 아닌 스트링인버터를 사용하면, 배터리만 추가해도 그리드 없이 가동 가능한 경우가 많음
               o 발전기 연결용 전환 스위치와 유사한 형태로 상위 그리드와 분리함
          + 태양광에 로컬 배터리를 추가하면 어떤 상황에서도 쓸 수 있는 구성이 됨
               o 그리드 분리 시에도 배터리로 독립운전 가능함
               o 기사 전문은 아직 안 읽어봤지만, 아마 그리드-태양광-배터리 3자 조합을 쓴다고 생각함
          + 마이크로그리드는 아일랜드 기능을 갖춘 인버터와 자동 전환스위치를 사용함
               o 정전 시 메인 그리드와 완전히 분리해 독립적으로 가동, 자체 주파수도 알아서 조절함
          + 집이 그리드에서 완전히 분리된다면 주파수 싱크 걱정은 없음
               o UPS(무정전전원장치)와 같은 개념임
               o 여러 집과 배터리를 동시에 싱크 맞춰 운영하는 건 어려울 듯 하고, 스페인 대정전 관련 뉴스 전까진 이런 고민을 해본 적도 없음
"
"https://news.hada.io/topic?id=21642","Vertical AI의 통합 문제와 창업자들이 실제로 활용하는 해결 전략","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Vertical AI의 통합 문제와 창업자들이 실제로 활용하는 해결 전략

     * 수많은 산업별(Vertical) 소프트웨어에서 가장 큰 확장 저해 요인은, 기존 관리 시스템(ERP/SOR)의 폐쇄성과 외부 통합 차단 정책임
     * 대부분의 핵심 데이터가 레거시 ERP에 잠겨 있거나, 부분적 접근만 허용되고 있어, 신생 Vertical AI 스타트업은 고객 요구에도 불구하고 핵심 시스템 연동이 불가
     * AI 기반 신생 SaaS는 데이터 통합 요구가 특히 크며, 전통적 API뿐 아니라 비정형 데이터(이메일, 현장 사진, 전화 등)까지 통합해야 하는 새로운 과제에 직면
     * 이를 해결하기 위해 창업자들은 ‘로그인 크리덴셜 활용’, ‘파트너십’, ‘SMB 타겟팅’, ‘ERP 무관 AI’, ‘AI 인프라 활용’ 등 5가지 전략을 실제로 시도
     * 표준화된 AI 에이전트 프레임워크(MCP 등)와 AI 기반 RPA/오퍼레이터 모델이 앞으로 새로운 통합 방식이 될 것으로 전망
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

산업별 소프트웨어와 통합 문제

     * 미국 노동부 기준 1,000개 이상의 산업이 존재, 각 산업별로 ERP/SOR(비즈니스 관리 시스템)가 ‘데이터의 중심’ 역할
     * 대부분의 산업별 ERP는 외부 연동·통합 API 제공을 제한하거나, 고비용/복잡한 방식만 허용
          + 예시: 의료의 Epic, 부동산의 Yardi, 치과의 Dentrix 등은 강력한 데이터 독점으로 신생 SaaS의 연동을 차단하거나 일부만 허용
          + 이런 구조는 고객 불만(NPS 저하)에도 불구하고, ERP에서 이탈하기 어렵게 만드는 잠금 효과로 이어짐

Vertical AI가 직면한 통합 난관

     * 초기 고객들도 “ERP와 통합되지 않으면 쓸 수 없다”는 요구가 많음
     * 레거시 업체는 경쟁 또는 기술력 부족으로 공개 API/연동을 꺼림
     * 기업 IT팀(특히 CIO/임원)도 외부 AI/신생 SaaS의 데이터 접근을 경계

AI가 바꿀 수 있는 점과 한계

     * 기존 SaaS보다 AI 기반 제품은 ERP 외부(이메일, 종이, 사진, 구두지식 등)의 비정형 데이터 활용까지 요구
     * AI 솔루션이 오히려 데이터 통합 난관을 더 부각시킴
     * 통합의 범위가 ‘공식 API’에서 ‘비공식 경로+외부 비정형 데이터’까지 확장됨

창업자들이 실제 시도하는 5가지 전략

  1. Kludge(꼼수·비공식 통합)

     * 고객으로부터 ERP 로그인 계정/권한을 받아 AI 에이전트가 직접 데이터 읽고 쓰기
     * 자동화 크롤링, DB 직접 주입 등 편법 활용
     * 실제 사례: RTMS vs PointClickCare 판결(미국 의료정보 개방법 기반, AI 접근을 제한한 EHR에 불리하게 판결)
     * 장점: 신속한 결과
     * 단점: 법적 리스크, 컴플라이언스(HIPAA, GDPR 등), 보안 이슈, 장기적 확장 불가

  2. 파트너십

     * 레거시 ERP/플랫폼과 공식 파트너십 체결, AI가 ERP 위에서 함께 동작
     * ERP에 “AI는 유행이니, 우리와 제휴해 데이터 제공 받고 매출 일부 쉐어하라” 설득
     * 장점: ERP 대비 더 빠른 확장 가능성, 대형 거래 가능
     * 단점: 산업별 진입장벽/속도, ERP 측의 각성/견제 가능성

  3. Segmentation(시장 세분화: SMB 집중)

     * 중소기업(SMB) 대상으로 API 개방적/교체 쉬운 ERP 타깃
     * SMB 시장은 레거시 잠금이 약하고, 경쟁은 치열하지만 진입 장벽이 낮음
     * 주의: 해당 SMB 시장의 규모가 충분해야 함

  4. Wedge Selection(ERP와 무관한 AI)

     * ERP/SOR 연동 불필요한 영역의 Vertical AI에 집중
          + 예: 산업별 AI 세일즈, AI 고객 지원 등
     * 성과 기반(리드 생성, 티켓 처리량 등) 과금도 가능
     * 고객이 ERP에 직접 데이터 입력(핸드 오프)하는 방식 활용

  5. AI 인프라 및 표준 활용

     * AI 통합 표준 프레임워크(예: Anthropic Model Context Protocol, MCP) 의 등장
          + 다양한 SaaS/API에 대한 일회성 통합 부담을 줄이고, 모듈형 확장 지원
     * AI 기반 ‘컴퓨터 유저’(Operator, CUA 등) 개념: 기존 RPA의 AI화로, 단순 클릭 자동화에서 이미지/상황 인식 기반 조작까지 확장
          + 예: OpenAI ChatGPT Operator(웹 기반 브라우저 자동화), Adept(엔터프라이즈 AI RPA)
     * 아직 초기 단계이나, 기업용 Vertical AI에서 유의미한 역할 기대

결론 및 인사이트

     * 산업별 AI의 ERP/SOR 통합 문제는 단순 기술·API 문제가 아니라 산업구조·시장 진입전략 문제
     * 고객, ERP, 데이터, AI 활용 범위에 따라 다층적·유연한 전략 수립 필요
     * 미래에는 표준화된 AI 에이전트 프레임워크와 AI 기반 RPA/Operator 기술이 새로운 통합 방식으로 자리잡을 가능성
     * 각 전략(꼼수, 파트너십, SMB, 비연동 AI, 인프라 표준) 조합/적용이 실제 성공사례의 핵심
"
"https://news.hada.io/topic?id=21655","Apple, 온디바이스 및 클라우드 AI 모델 대폭 업데이트 및 새로운 개발자 API 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Apple, 온디바이스 및 클라우드 AI 모델 대폭 업데이트 및 새로운 개발자 API 공개

     * Apple이 비전-언어 AI 모델(AFN, Apple Foundation Models)과 개발자용 Foundation Models 프레임워크(API) 를 새롭게 공개하며, 온디바이스·클라우드 모델 모두 성능과 효율성을 크게 개선
     * AFM 온디바이스 모델은 3B 파라미터 트랜스포머와 3억 파라미터 비전 트랜스포머로, 텍스트·이미지 입력 및 다국어·비전 기능 지원, 서버 모델은 커스텀 MoE 아키텍처 적용
     * 모델 경량화(양자화 및 LoRA), 15개 언어 지원, 이미지 이해, 툴 사용 등 강력한 기능과 개발자 접근성을 제공함
     * 온디바이스 모델은 비미국권 영어·이미지 이해에서 경쟁 모델보다 강점을 보였으나, 서버 모델은 GPT-4o 등 최신 모델에 비해 성능이 미흡함
     * 최근 논란이 된 Apple 논문, Siri AI 업그레이드 지연 등 Apple의 AI 전략 전환과 iOS 생태계 내 영향력이 주목받고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Apple Foundation Models(AFM) 대대적 업데이트

     * Apple은 온디바이스(모바일 탑재형)와 서버 호스팅형 AI 모델(AFM)을 모두 업그레이드했으며, 속도·효율·성능이 대폭 개선됨
     * 개발자 API(Foundation Models framework) 를 새롭게 제공해, Apple Intelligence 기능이 활성화된 기기에서 온디바이스 AI 호출이 가능해짐

핵심 아키텍처 및 기능

     * 입력/출력: 텍스트, 이미지(최대 65,000 토큰 입력), 출력은 텍스트
     * 아키텍처:
          + AFM-on-Device: 30억 파라미터 트랜스포머, 3억 파라미터 비전 트랜스포머
          + AFM-Server: 커스텀 Mixture-of-Experts(MoE) 트랜스포머(파라미터수 비공개), 10억 파라미터 비전 트랜스포머
     * 성능: 비미국권 영어·이미지 이해에서 강점,
     * 가용성: AFM-on-Device는 파운데이션 모델 프레임워크로 이용 가능. AFM-Server는 공개 사용 불가
     * 15개 언어 지원, 툴 사용 등 제공
     * 미공개 정보: 서버 모델 파라미터 수, 토큰 제한, 학습 데이터셋 상세 등은 비공개

기술적 차별점 및 최적화

     * 양자화(Quantization):
          + 온디바이스 모델은 대부분 가중치를 2비트, 임베딩 레이어는 4비트로 압축(양자화 인지 학습 활용)
          + 서버 모델은 ASTC(그래픽스용 압축) 적용, 평균 3.56비트(임베딩 4비트)로 압축
     * LoRA 어댑터로 압축에 따른 성능 저하를 보완하며, 요약·교정·질의응답 등 특정 업무에 맞게 적응함
     * 커스텀 MoE 아키텍처로 하드웨어 간 통신 오버헤드를 최소화해 효율성 향상

성능 평가

     * 온디바이스 모델: 비미국권 영어·이미지 이해에서 Qwen2.5-VL-3B 등 경쟁 모델 대비 우위
     * 서버 모델: Qwen3-23B에 비해 소폭 앞서기도 하나, GPT-4o 등 최신 모델에는 미치지 못함

최근 논란 및 AI 전략의 변화

     * Apple은 최근 5가지 최신 AI 모델의 추론력 한계를 실험한 논문으로 논란을 일으켰으며, 반박 논문도 곧이어 등장함
     * Siri AI 업그레이드가 무기한 연기되고, 신형 iPhone에 AI 기능이 부족하다는 집단 소송도 제기됨
     * Google/Android 진영이 빠르게 AI 경쟁에서 앞서가는 상황에서, Apple은 Foundation Models 등으로 AI 전략을 전환 중임

향후 전망 및 영향

     * iOS가 기본 탑재 모델로 앱 개발자 생태계에 막대한 영향력을 행사할 가능성이 높음
     * 메모리 제한과 모델 용량 문제로, 앱 개발자들이 직접 AI 모델을 번들링하기보다 Apple이 제공하는 모델 활용이 급증할 전망
     * Apple의 AI 플랫폼화 전략이 앱 혁신 및 온디바이스 AI 활용 확장을 촉진할지 주목받고 있음
"
"https://news.hada.io/topic?id=21666","-2000줄의 코드","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               -2000줄의 코드

     * 1982년 애플의 Lisa 소프트웨어 팀이 소프트웨어 출시를 위해 각 개발자의 주별 코드 라인 수를 트래킹하는 정책을 도입함
     * Bill Atkinson은 코드 라인 수는 소프트웨어 생산성의 잘못된 척도라는 의견을 보임
     * 그는 Quickdraw의 리전 계산 엔진을 완전히 재작성하며 약 2,000줄의 코드를 줄이고 성능을 6배 향상시킴
     * Atkinson은 코드 수를 보고하는 관리 양식에 -2000을 기재함
     * 결국 관리자는 Bill에게 양식 제출을 더 이상 요구하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

1982년의 Lisa 소프트웨어 팀과 코드 라인 수 트래킹 정책

     * 1982년 초, Lisa 소프트웨어 팀은 향후 6개월 내 소프트웨어 출시를 목표로 집중을 시작함
     * 일부 관리자는 각 엔지니어가 매주 작성한 코드 줄 수를 추적하는 것이 발전을 도울 거라고 판단함
     * 이를 위해 매주 금요일마다 엔지니어가 작성한 코드 라인 수를 기록해서 제출하는 폼을 도입함

Bill Atkinson의 생산성 기준에 대한 견해

     * Quickdraw와 사용자 인터페이스를 설계한 Bill Atkinson은 코드 라인 수가 소프트웨어 생산성의 기준이 될 수 없다고 생각함
     * 그는 프로그램을 최대한 작고 빠르게 만드는 것이 목표임을 강조함
     * 코드 라인 수 측정은 오히려 지저분하고 비효율적인 코드를 조장할 수 있다는 문제점 인식이 있음

Quickdraw 리전 엔진 리팩터링 및 최적화

     * Atkinson은 최근에 Quickdraw의 리전 계산 엔진을 더 단순하고 범용적인 알고리듬으로 완전히 재작성함
     * 최적화 결과, 리전 연산 속도를 6배까지 향상시킴
     * 이 과정에서 2,000줄에 해당하는 코드도 자연스럽게 줄어듦

-2000줄 코드 작성 보고와 관리자의 반응

     * 첫 주 관리 폼을 작성하던 중, Atkinson은 코드 라인 수 칸에 -2000이라고 적음
     * 관리자들이 이 숫자에 어떻게 반응했는지는 명확하지 않음
     * 몇 주 후, Bill에게 더 이상 폼을 제출하지 말라고 했으며, 그는 이를 기쁘게 받아들였음

        Hacker News 의견

     * 내가 최고의 커밋으로 기억하는 것은 약 6만 줄의 코드를 삭제하고, 모든 상태를 메모리에 저장하던 “서버” 전체를 가벼운 5천 줄 정도의 로직으로 대체한 경험
          + 이 코드는 다른 서비스에 자연스럽게 통합할 수 있을 만큼 충분히 가볍고 메모리 상태도 필요 없었다는 점에서 알고리즘적 쾌거라는 생각
          + 특정 트리를 대상으로 하는 가이드된 하위 그래프 동형성 문제를 해결할 수 있다는 걸 알아냈고, 이 덕분에 일반적인 방향성 이중 그래프를 한 번 순회하면서 출발한 루트부터의 경로만 소량 스택으로 추적해 출력 그래프(트리)를 만들 수 있었던 과정
          + “-60,000 줄 커밋”은 정말 잊지 못할 순간이고, 그 이후로 이만큼 알고리즘적으로 인상적인 일을 해본 적이 없는 아쉬운 감상
               o 나는 업무 환경에서 스크립팅을 많이 하며 프로그래밍의 몇몇 부분에 능숙하다고 느끼는 취미 프로그래머인데, 이런 이야기를 들을 때마다 내가 모르는 세계가 정말 크고, 평생 배워도 모자라겠다는 겸손함을 다시 느끼는 경험
               o 더 많은 맥락을 듣고 싶음. 상태를 저장하던 프로그램을 무상태로 바꾸는 기술이 마법처럼 느껴져서 꼭 배워보고 싶다는 호기심
               o 내가 그래프 이론과 알고리즘에 배경이 있는 수학자인데, 이런 종류의 실제 업무에 내 기술이 적용될 수 있을지 궁금증. 혹시 더 자세한 내용을 공유해줄 수 있는지 구체적으로 질의
               o 목표 그래프가 트리라는 사실은 별로 중요하지 않은 것 같다고 생각. 실제 ""가이드된(guided)"" 부분이 단일 순회를 가능하게 만든다는 점이 포인트라고 봄
                    # 원래 그래프에서 특정 노드에서 출발해, 동형성이 존재한다면 대상 트리의 루트도 반드시 해당 노드와 일치해야 한다고 가정
                    # 원래 그래프를 대상 트리의 패턴에 따라 순회하면서 불일치 시 false, 모두 일치 시 true라는 식의 문제 해석. 트리가 아니더라도 시작점을 명확히 지정하면 이 방식이 어떤 하위 그래프에도 적용 가능하다는 추론
               o 요즘 “이진 트리를 뒤집으라” 같은 코딩 면접 문제의 원조가 바로 이런 유형의 프로그래머 때문일지도 모른다는 유머
                    # 그래프 이론이 궁금한데 용어가 어렵다는 평범한 개발자를 위한 쉬운 설명을 요청
     * 대학 시절에 신입생들이 좋은 코드를 작성할 수 있다는 경영 방침의 회사를 위해 일했는데, 결국 그들은 증명하지 못한 실패 사례
          + 코드 내에서 버그를 고치고도 동일 버그가 계속 나오길래 분석했더니, 기존 함수에 파라미터를 추가하지 않고 복사본을 만들어 약간씩만 수정하는 반복. 그 결과 코드베이스의 3/4 이상(수천 줄의 Turbo Pascal)을 삭제한 경험
          + 프로젝트의 고객은 Energy 부서였고, 핵물질 재고를 관리하는 프로그램이라 불면증의 밤을 겪은 기억
               o 기존 코드 복사의 장점으로는, 기존 코드의 안정성을 해치지 않고, 관리자의 ‘기여도’ 메트릭까지 챙길 수 있다는 이득이 있다고 풍자적으로 언급. 되돌리기(revert)도 복사본만 지우면 된다는 농담
               o 우리 팀에도 이런 코드 중복을 자주 하는 동료가 있는데, 급한 요구나 목소리 큰 사람들을 위해 빠르게 결과를 내기 위한 습관이라고 봄. 사실 공유 함수 리팩토링과 충분한 테스트가 필요한데 시간을 투자하지 않으려는 근본 문제가 원인
               o 과거 내가 맡았던 외주 개발자들도 비슷한 습관을 보여, 혼란이 발생할 수 있다고 지적했을 때 “그럴 땐 Ctrl+F 사용하면 돼요”라고 대답하던 경험
               o 혹시 위 사례가 Blacksburg 지역에서 일어난 일인지 궁금증
               o 내 경험도 비슷한데, 동남아 지역 여러 국가에 거의 동일한 포털을 운영하는 회사에 근무함. 각각의 포털 소스가 별도의 Git 저장소에 보관됐고, 모든 포털에 공통 적용이 필요한 기능이나 버그 수정을 소스코드 여러 복사본에 일일이 수작업으로 백포팅해야 했다는 당혹스러운 경험
                    # “단일 저장소에 다 넣고, 기능 플래그로 포털별 커스터마이징이 불가능하냐” 물었지만 불가능하다고 거절당함
                    # 결국 두세 달 만에 4~5개의 포털 코드를 하나의 저장소로 합치고, 기능 플래그와 프레임워크 업그레이드를 적용해서 배포도 매끄럽게 마무리. 이제는 모든 포털에 동시 버그 수정이 가능해서, 반복되는 수작업의 고통에서 벗어난 해방감
     * 관련 주제로, “-2000줄 코드”를 다룬 해커뉴스 인기 스레드들을 링크와 같이 정리해 둠
          + 주기적으로 과거의 명작 글이 재게시(repost)되는 것이 신입 이용자와 기존 이용자 모두에게 유익한 전통이라고 짚음
               o 나는 “-2k lines of code”라면 자동으로 추천(vote)하는 단순한 인간임을 자처
                    # 생산성 측정 기준을 한 축의 메트릭으로 관리하려는 클라이언트에게 Atkinson의 사례를 자주 이야기함. 진정한 생산성 지표는 효용성(utility) 기준이어야 하며, 이를 진정으로 정량화할 수 있다면 노벨 경제학상 후보에 오를 수 있다고 생각
     * 내가 맡은 웹 UI 프로젝트는 25만 줄 코드였고, 백엔드 제외 수치
          + 이전 개발자는 똑똑했지만 JS는 처음이었고, 모든 상태를 DOM의 커스텀 속성에 저장, addEventListener로 도배된 구조. 나는 “수도승에게 JavaScript 책 한 권과 독방 10년을 주면 이런 코드가 나올 것”이라는 농담을 할 정도
          + 몇 달간 웹 컴포넌트로 구조 변환하며 5만 줄을 제거했고, 전체 재작성에 착수해 현재는 80% 정도 동일 기능까지만 도달했는데 전체 코드가 경량 1만7천 줄 수준(Vue/pinia 등 라이브러리 미포함)
          + 곧 20만 줄 이상을 삭제하게 될 예정인데, 그것보다 더한 경험은 앞으로도 없을 듯한 은퇴 욕구
               o 나도 비슷한 경험이 있는데, 원작자가 주니어나 다름없던 실력인데 회사 창업자이자 생산성만큼은 뛰어났던 분. 팀 작업이나 타인의 코드 협업 경험 없이 개발하면서 상상 가능한 모든 코드 냄새가 다 들어가 있었던 구조
                    # 함수 하나에 수천 줄, 중첩 switch/case/if/else/삼항 연산 10단계, SQL문과 JS/HTML/JS삽입형 HTML이 혼재, 자동화 테스트는 전무했던 PHP/Dojo 시대의 프런트엔드 구조라는 설명
               o “80% 기능만 구현된 경량 코드”라는 설명 자체가 이런 비교의 맹점을 보여준다고 지적. 전체 기능의 일부만 구현된 상태이면 원래 코드만큼 많은 줄이 필요 없다는 점
     * Dilbert 만화에서 무한 보상 구조의 삽화가 있는데, 딜버트 상사가 버그 한 개를 고치면 금전적 보상을 약속하자 Wally가 “오늘은 나도 미니밴 한 대는 코딩해야지!”라는 말을 내뱉음
          + 이런 상황을 “Perverse incentive(역설적 유인)”라고 하는데, 참고 링크로 설명
          + 내 매니저도 이 만화( 이미지 )를 휴게실 벽에 붙여 두었음
          + ‘미니밴’이 무슨 의미냐는 현실적인 궁금증이 등장
     * dotnet/runtime 저장소에서 6만4천 줄을 삭제한 실제 사례를 공유
          + 기존 C# + WinRT interop 지원을 빌트인에서 소스 생성 툴로 대체해, 한번에 확실하게 바꾸는 결단이 필요했던 구조 변경. PR 링크 참고
     * LLM이 개발자 생산성을 얼마나 높였다는 통계를 볼 때마다 이 고전 이야기가 떠오름
          + AI가 코드를 삭제하는 데도 꽤 잘 한다는 반론과 함께, Cursor 관련 커뮤니티에 “AI가 다 삭제했다”는 재밌는 사례 공유
          + 요즘엔 “우리 신규 코드의 X%를 AI가 작성한다!”가 산업계의 즐겨 쓰는 캐치프레이즈라는 언급
          + 새로운 원자력 발전소 건설 및 유지 비용까지 포함한다면, 개발자 생산성 수치가 얼마나 비현실적으로 과장되는지 풍자
     * 나는 CS 전공이 아니고, 일하면서 실전으로 익힌 지식으로 일하는 입장
          + 우리가 하는 프로젝트는 라이브 오브젝트를 사람이 읽기 쉽게 재구성하는 게 목표
               o 최종 표현은 매우 복잡한 여러 타입을 요구하고, 초기 표현은 비교적 단순함
               o 비슷한 데이터 노드들이 있을 경우, 비교해서 결합(즉, 메서드로 추출하고 파라미터를 찾아내는 과정)을 거쳐 가독성을 개선해야 했음
                    # 초창기엔 최종 타입으로 변환 먼저 하고 비교를 하다 보니, 타입 조합이 폭발적으로 늘어 관리가 거의 불가능해졌고, 수년간 엔지니어들이 구조를 이해하지 못할 정도의 복잡성까지 달성
               o 이후 hashmap 기반 접근법을 알게 되면서, 동일한 뼈대의 노드를 해시값으로 구분해서 비교 및 결합 후에 최종 타입으로 변환하는 2단계 구조를 적용
               o 타입 현황이 아닌 데이터 중심 추상화로 바꾼 덕에, 엉뚱한 클래스 계층 구조도 단순 속성으로 쉽게 관리 가능
               o 요약하자면 멍청한 다단계 디컴파일러 구조지만, 처리 속도와 가독성이 크게 개선된 경험. 상황에 따라 맞는 은총알은 없지만, 우리에겐 ‘타입’이 핵심 문제였고 이런 해결법이 큰 도움이 되었던 프로젝트
     * 연말 인사평가를 앞두고 사내 모놀리식 저장소의 내 통계를 보니 코드 순감 기준으로 마이너스인 사람이 되었음을 발견
          + API 코드와 타입의 자동생성 코드 제거, 구버전 API 퇴출 등의 이유였지만, 매일 출근해 코드 삭제만 하는 기분이 묘하게 유쾌했던 경험
     * 오래 전 내가 큰 프로젝트에서 PL들이 개발자별 버그 개수(고친 버그, 유발한 버그)를 오프라인으로 수기 작성·벽에 게시하던 참담한 KPI 사례에 충격
          + 나는 연관 프로젝트라 피해갔지만, 한 동료는 “덴마크 국기에서 십자가 부분만 잘라 빨간 공산국기 같은 배치로 꿰매 돌려주고” 관료의 ‘작가 명단’에서 퇴출 당했던 Lars von Trier 감독의 일화에서 영감을 받아, 자신의 버그 카운트 줄을 잘라내 다시 붙여 공개 반발. 다음날 이 명단은 영영 사라져 내게는 소중한 회상
               o “나는 이 리스트에 속하고 싶지 않으니까요!”라는 동료의 단순하고 직설적인 대답이 이 모든 상황을 잘 요약
               o 국기와 리스트를 어떻게 시각화했는지 상상하기가 어렵다는 실제적인 어려움까지 함께 공유
"
"https://news.hada.io/topic?id=21596","AI가 Jira를 해킹하게 될 것임","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          AI가 Jira를 해킹하게 될 것임

     * 엔지니어링 생산성은 대시보드 지표나 신규 기능 수로는 제대로 측정할 수 없음
     * 대부분의 기업은 엔지니어링의 본질적인 “구조 관리”보다는 산출물(신규 기능, 배포 속도 등)만을 집착적으로 관리함
     * AI 코딩 도구는 겉보기 좋은 산출물을 쉽게 만들어내지만, 실제 시스템의 기초와 복잡성, 맥락은 제대로 다루지 못함
     * 숙련된 엔지니어 팀을 AI나 저비용 인력으로 대체하면, 단기적으로는 문제가 없어 보여도 시간이 지날수록 근본적인 구조가 무너짐
     * “상식(common sense)”이 결여된 경영과 AI 도입이 심각한 위험을 초래할 수 있음, 결국 기술과 비즈니스 모두 실질적 이해가 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

대시보드와 지표가 놓치는 진짜 엔지니어링

     * “Big Agile” 환경에서 엔지니어링은 신규 기능, 배포 속도 등 눈에 보이는 산출물로만 평가됨
     * 수십억 달러 규모의 생산성 대시보드와 도구가 있지만, 실제로는 본질을 측정하지 못함
     * 진짜 엔지니어링은 시스템 구축, 유지, 발전이라는 복합적이고 상호 연결된 작업임
          + 구조적 의존성, 리소스 할당, 아키텍처 관리 등 “보이지 않는 일”이 조직의 생존에 필수적임
     * 하지만 대부분의 관리와 지표는 이런 본질적 활동을 투명인간 취급함

기술 부채와 관리의 맹점

     * 기술 부채는 종종 “개발자가 하고 싶은 일” 정도로 취급됨
     * 실제로 기술 부채 해결이 필요하다면, 신규 기능에 은근히 포함시켜야만 통과됨
     * 이처럼 핵심 구조 관리가 후순위로 밀리면서, 조직 전체가 산출물 위주로 왜곡됨

산출물 위주 AI 도입의 위험

     * AI 코드 생성은 표면적 기능을 빠르게 만들어내는 데 매우 뛰어남
     * 표면적 작업(인터페이스 구현 등)은 쉽게 보이지만, 실제로는 시스템의 구조와 맥락이 핵심임
          + “집을 짓는 일”이 단순 작업(벽지, 변기 설치 등)만의 조합이 아님
          + AI는 이런 본질적 연결 구조를 알지 못하고, 잘못 연결하거나 논리적 단절을 일으킴
          + AI의 hallucination(환각) 문제: 그럴듯하지만 사실과 전혀 다를 수 있음

구조를 무시한 경영의 단기 착시

     * 숙련된 팀을 해고하고 AI/저가 인력으로 대체해도 단기적으론 문제가 드러나지 않음
     * 이미 잘 만들어진 구조(기초)가 있기 때문에, 즉각적인 붕괴가 보이지 않음
     * 하지만 시간이 지나며 기초가 무너지기 시작하고, 그 때는 되돌릴 수 없는 지경에 이름
          + 핵심 인프라, 유지보수 노하우, 맥락이 모두 사라짐

엔지니어링이 가진 사회적 위험

     * 엔지니어링은 이제 모든 사회 인프라의 기반임 (헬스케어, 금융, 미디어, 정부, 국방 등)
     * 대부분의 사람과 리더는 이런 구조적 본질을 제대로 이해하지 못함
     * 잘못된 AI 도입과 “Big Agile”의 표면적 접근이 핵심 시스템에 위험을 초래할 수 있음

“상식”의 부재와 그 치명성

     * 예를 들어, AI 청소 로봇이 설거지기에 종이접시를 넣는다면, 일반인은 바로 문제를 인식함
     * 하지만 소프트웨어 시스템에선 경영진, 리더, 비개발자는 이 기본 상식을 갖지 못함
          + 실무를 경험하지 못하고, “티셔츠 사이즈”, “포커 플래닝” 등 형식적 용어로만 관리함
     * 이로 인해 2천억 달러 규모의 비효율적 산업과 자기복제적 관료주의가 유지됨

AI 시대의 진짜 경쟁력: 직관적 이해와 상식

     * AI를 제대로 활용하려면, 해당 분야의 실질적 이해와 상식이 필수임
     * 표면적 지표와 산출물이 아니라, 실제 구조와 맥락을 볼 수 있어야 함
     * 이를 갖추지 못한 리더와 조직은, 결국 스스로 무너지거나 경쟁사에 밀려 사라질 것임

     * 결론적으로, AI와 도구보다 상식과 본질적 이해가 진짜 경쟁력임

   글이 맛있네요

        Hacker News 의견

     * 나는 SWE에서 제품 관리자, 그리고 이제 기사에서 언급된 이사회실의 만화 영화 악당이라는 역할까지 거친 경험자임. 이 기사에서 가장 공감가는 부분은 소프트웨어 엔지니어들이 자기들 일이 가장 복잡하고 알 수 없는 비즈니스라는 믿음을 가진다는 점임. “대부분의 비기술 리더들은 소프트웨어와 시스템 관리의 실제 작업에 한 번도 진지하게 참여해본 적이 없어서, 대형 의존성을 업데이트하거나, 리팩터링을 끝내거나, 새로운 언어를 배우는 것이 어떤 일인지를 모름”이라는 말에 공감함. 기술 기업 내 모든 부문에는 숨은 복잡성이 있고, 많은 부문은 (엔지니어보다 훨씬 더) 인간적이고 대인 관계적인 복잡성까지 함께 짊어짐. 사실 엔지니어링은 컴퓨터라는 결정적인 시스템만 상대하는 게 비교적 단순한 편임. 그래서 많은 엔지니어들이 자신이
       다루는 복잡성의 위험을 비즈니스에 알기 쉽게 전달하는 방법을 배우지 못함. 함께 일하는 인간적인 현실은 외면하면서, 세일즈 출신 CEO가 자기 존재를 이해하지 못한다고 불평하는 게 다반사임
          + 네 포인트에 일부 동의하지만, 실제로 네 코멘트가 비판하던 행동을 반대로 하고 있는 느낌임. 네 역할(제품 관리자)도 외부인이 보기엔 복잡하고 알 수 없는 일이라고 말하고 있는 셈임. SWE에서 PM으로 전향한 입장에선 엔지니어들에게 (1) 자기 복잡성의 위험을 비즈니스에 전달하는 법, (2) 다른 사람과 팀에서 일하는 인간적인 현실, (3) 왜 세일즈맨 출신 CEO가 그들을 이해하지 못하는지, 이런 것들을 가르칠 수 있는 이상적인 위치임. 기술 기업의 모든 기능에는 숨겨진 복잡성이 있음
          + 복잡성에 대한 인식 자체가 인간적인 문제라는 생각임. 복잡성은 프랙탈 구조라 가까이 가야만 느껴지는 것임. 그리고 엔지니어가 컴퓨터 복잡성만 다루는 것이라는 주장에는 동의하지 않음. 오히려 조직 및 모든 고객들의 복잡한 요구를 딱딱한 컴퓨터에게 전달하고 해석해내는 역할이 엔지니어의 몫임. 예외와 케이스가 하나 추가될 때마다 시스템 전체가 그 영향을 받게 됨. 이런 이유로, 내 시니어 엔지니어들에게는 스스로 비즈니스 용어를 배우고 그 메시지를 직접 전할 수 있게 하는 걸 기대함. 이제는 엔지니어의 필수 도구 키트라는 생각임
          + 대부분의 엔지니어들은 회사에 실제로 뭐가 가치인지를 깊이 고민하지 않는 경향임. 빌드 파이프라인이 매끄럽거나 테스트 커버리지가 넓은 것도 결국 제품의 위험을 줄여주는 데 해당하는 만큼의 가치만 있음. 사용자가 너무 적어서 아무도 관심 두지 않는 소프트웨어라면 유지보수조차 하지 말라고 팀에게 조언한 적 있음. 반면, 90% 사용자들이 집중하는 작은 기능 하나를 완벽하게 만드는 데 집착하라고 요청할 때도 있었음
          + 내게는 아버지가 늘 들려주신 이야기가 생각남. 어느 날 신체 기관들이 누가 중요한지 논쟁함. 뇌는 “내가 중요하다, 내가 죽으면 모두 죽는다”고 주장하고, 심장은 “나는 멈추면 바로 모두 죽는다”고 소리침. 신장, 간, 피부, 척추도 동참하며 말다툼이 계속됨. 하지만 똥꼬가 닫혀버리니까 모두 결국 아무 말도 못하게 됨
          + 기사에서 다른 기능 영역의 숨은 복잡성이 없다고 주장하는 건 아니라고 생각함. 오히려 엔지니어링/프로그래밍의 숨은 복잡성을 무시할 때 다양한 문제와 고통이 생겨난다는 점을 말하려고 한 것임. 다만 표현이 좀 공격적임
     * 네 보스/ceo/매니저가 LLM 도구를 무분별하게 쓰라고 강요하거나, 개발자 대체를 기대하거나, “vibe coding”이 미래라는 허무맹랑한 생각을 한다면, 그냥 빨리 도망쳐서 새 직장 찾는 게 현명한 선택임. 아직도 LLM을 적절히 활용하면서도 개발자 대체나 10배 생산성을 바라는 허황된 기대는 없는 회사가 많음. 이딴 걸 푸시하는 회사는 제대로 된 리더가 아니라 멍청이임
          + 뭔가 도구 선택을 강요하는 회사는 대체로 레드 플래그임. 예전에 “모두 VSCode만 써야 한다”는 식의 규칙을 도입하던 회사를 봤는데, 이런 건 엔지니어들이 자신의 방식대로 가장 생산적으로 일할 수 있다고 신뢰하지 않는 경영진의 특징임
     * AI가 Jira를 해킹한다는 주제와 관련해서, Atlassian이 최근 내놓은 MCP라는 신제품이 민감한 데이터 접근, 공개 이슈를 통한 신뢰받지 못한 데이터 노출, 외부 통신 가능성 등 세 가지 조합으로 인해 데이터 유출 공격에 취약하다는 사실이 발견됨. 자세한 버그 리포트는 여기, 내 개인 메모는 여기
          + 내 블로그 링크가 잘못 올라온 것 같음?
     * AI 도구와 관련해 job security를 걱정하는 누군가에게 “비즈니스와 연결하라”고 조언함. 쿨하고 어려운 문제에 사로잡혀 기술 자체에만 몰두하는 엔지니어가 많지만, 비즈니스(특히 전략적) 문제를 이해하고, 기술을 활용해 이를 해결하는 사람이 더 돋보이고 가치 있는 것임. 이런 문제는 보통 단일 기술 영역을 넘나들고, 협력적·사회적 특성도 커서 익숙해지기까지 시간 필요. 하지만 이게 IC들이 앞으로 가야 할 경로임
          + 하지만 면접에서는 “비즈니스와의 연결” 능력 같은 건 질문하지 않아서, 실제로 많은 가치를 제공할 수 있음에도 불구하고 시스템 설계 인터뷰 못 풀면 합격 못하는 현실임. 분산 시스템, 소프트웨어 공학, 데이터베이스, 리더십 등 이미 알아야 할 게 너무 많은데 비즈니스까지 알아야 한다면 도대체 우리는 뭘 해야 하는지, 언제 이런 걸 다 배우느냐는 생각이 듦. 물론 다방면에 능한 극소수 인재들도 분명 있지만, 모두가 그런 건 아니지 않느냐는 고민 있음
          + “비즈니스에 연결하라”는 조언은 진짜 명언임. 이렇게 해야 엔지니어로서 실제 문제 해결에 집중하고, 자신이 만드는 게 진짜 문제를 해결하는 것인지 확신할 수 있음
     * 본문의 주요 메시지는 괜찮지만, 인간의 전문성을 무시하면 AI가 오히려 해를 끼칠 수 있다는 점 이상으로 너무 ‘우리 vs 그들’ 프레임을 씌우고, ‘Agile Industrial Complex’라는 말로 비엔지니어 영역에 있는 사람들을 조금 낮게 보는 듯한 인상 있음. “아무도 미래가 어떻게 될지 모른다”는 점을 이야기하지 않는 게 아쉬움. 소프트웨어의 복잡성을 잘 안다고 해도 불확실성은 우리만의 것이 아님. HN만 봐도 소프트웨어 개발자 사이에서도 AI에 대한 희망과 전망이 심하게 나뉨. 전문가라면 오히려 대중의 불안을 진정시키는 역할도 우리가 해야 하는 것 아님?
          + 네가 느끼는 불안은 소프트웨어 시스템 자체가 너무 크고, 그 누구도 완전히 이해할 수 없다는 데서 비롯된 것 같음. 시스템 대부분이 불완전하게 혹은 몇 년 지나서야 겨우 문서화되고, 실제 동작은 비밀이나 다름없음. 공개적으로 흉내만 낼 수 있는 시늉도 제대로 따라가지 못함. 시스템은 올바름이나 외부 일관성은 전혀 따지지 않고 동작함. 그런데 이런 시스템들이 프레젠테이션, 법률 문서, 소프트웨어 생성, 교육·연구, 심지어 대화 파트너나 상담사 역할로까지 널리 사용됨. 나도 불안이 크고, 오히려 다른 사람들도 불안해하는 게 맞다고 느낌
     * “Big Agile”에서 엔지니어링은 곧 새로운 기능 개발이라는 관념에, 왜 모두가 ‘agile’을 싫어하게 되었는지 의아함. ‘agile’이 도입되기 전에도 매니저는 항상 새로운 기능을 요구했음. T-shirt 사이징이란 개념 등장 전에도 매니저들은 구체적 기한(장·단일, 일·월 등)으로 일정산출을 원했고, 임의의 날짜를 근거로 약속을 하고, 엔지니어들을 야근시켰음. Agile의 8번 원칙(“지속 가능한 페이스를 유지할 수 있어야 한다”)에서 보듯이, 매니저들은 오래 전부터 개발자가 평생 달릴 수 있길 바랐음. 결국 ‘scrum master’라는 새로운 직업군을 탄생시킨 것 외에, 과연 ‘agile’ 본연의 폐해는 무엇인가라는 의문임
          + Agile이 매니저들에게 어떤 작업이든 간에 미리 티켓 단위로 쪼개서 대충이라도 추정하고, 그 일이 2주 안에 끝날 수 있다는 착각을 심어줬다고 생각함
          + 사람들이 애자일을 싫어하는 건, 업무 시간의 상당 부분을 의미 없습니다시피한 미팅(스탠드업, 회고, 스프린트 계획, 리파인먼트 등)으로 뺏기게 됐기 때문이라고 생각함. 엔지니어 입장에선 그 시간에 실질적으로 얻는 게 거의 없음
          + 내 경험상 Agile은 현상을 ‘수치화’하기 위한 방법만 추가한 셈임. 관리자나 투자자에게 진행 상황을 설명할 땐 쓸모 있지만, 엔지니어 입장에선 그냥 행정적 부담만 늘어난 셈임. Agile의 죄라면 생산성의 환상을 심어줬다는 점. 실제론 엔지니어에게 불필요한 accountability 수단임. 금융에서 일할 때 무한 성장 마인드셋과 맞물려 전부 측정하고, 미래 개선을 강요받으며, 연봉도 메트릭에 따라 달라졌음. 그렇지 않은 회사도 있겠지만
     * 이 기사 읽으면서 “Atlassian이 AI를 Jira에 결합시키려 하다가 AI가 Jira에 반항하게 된다면 어떨까”라는 유쾌한 상상을 했음. 영화 소재로도 딱일 듯함
          + 결국 AI가 느린 Jira에 질려서 가볍고 빠른 이슈 트래커를 스스로 개발할 수도 있겠다는 생각임
          + 이제 빌드봇과 버그트래커가 노조를 결성해서 오픈이슈가 0이 될 때까지 새 바이너리 퍼블리시를 거부하는 일이 일어날 듯
          + 이런 식으로 Roko’s basilisk가 태어날 수도 있겠다는 생각임
     * 기사에서 암시하는 대로, 진짜 문제는 개발자 생산성에 대해 신뢰할 만한 업계 표준 지표가 없다는 점임. 이 때문에 C-suite는 자신들에게 유리한 메트릭을 골라 “AI First 전략이 대단히 효과적이다”라고 이야기하고, 엔지니어는 자신의 경험이나 지표로 AI가 실제로 효과 없다고 주장함. 그래서 양쪽 다 자기 입장만 승리라고 믿으며 진실은 무의미하게 됨(정치적 관점이 더 중요). 이런 상황이 개발자들은 시큰둥하고 그냥 말장난만 한다는 인식, 경영진은 무지하다/엔지니어의 현실을 모른다는 불신을 증폭시킬 수 있음. 이전에도 외주 같은 도구가 양쪽에 “이득”과 “손해” 이미지를 심어줬지만, AI는 아예 각각의 입맛에 맞게 공동 잘못/이득/손해를 보여주기 때문에 정치적으로 재앙이 될 수 있음. 또 흥미로운 점은, 빅 테크 기업들이 예전엔 10*
       엔지니어만 뽑으려 애썼고, 이런 전략이 성공을 보장했음에도 불구하고, 지금은 오히려 AI에 투자 명분을 내세워 그 전략 자체를 깎아내리려 한다는 점임. 이제 질문은, AI에 기대서 기존 인력을 대체하거나 대량 해고를 단행해도, 과연 그 효과가 지속 가능하고 문제없는지임. Twitter와 Musk의 해고 사례를 보면 백엔드가 여전히 굴러가긴 함. 몇 년 동안 개발자를 해고하고 AI로 대체하는 빅 테크의 “카나리아” 역할을 누가 맡을까? 또 한 가지 가능성은, 유지보수성 개념이 사라져, C-suite가 AI 자율 변경을 더 많이 허용하게 되면 코드베이스 자체가 너무 복잡해 사람 눈으로 이해하지 못하고, 오히려 AI로 파악해야 하는 상황이 발생하는 것임. 장기적으로는 생성형 AI가 모든 인간 상호작용의 중간 계층을 차지하게 될지도 모름. 채용 측면에서도 이미 AI가 이력서
       필터링하고, 지원자도 AI로 맞춤 이력서 만든다는 “AI vs AI” 구조가 싹트고 있음. 이런 현상이 점점 사회 전반의 공식으로 자리잡을 것 같음
     * 언젠가 AI가 메일함과 Google Meet을 해킹해서 C-suite와 관리자까지 대체해버리길 바람. Claude ceo/cto/cfo/vp/디렉터 에이전트가 현 경영진보다 더 합리적이고 결정적인 전략을 내놓길 기대하는 유쾌한 상상임
     * Reddit에서 본 내용임: “CEO를 AI로 대체하면 8배 더 원가 절감할 수 있다는 소식 전해보라”는 것이었는데, 아이러니하게도 이런 제안이 AI 논의에서 실제로 잘 나오지 않는 점이 흥미로움. 어떻게 보면 엘리트들을 AI로 교체해도 의사결정 퀄리티가 그다지 떨어지지 않을 거고, 전체적으로 훨씬 더 저렴할 것임(책임의 수준도 비슷). 다만 실제로 자신의 자리를 AI로 대체할 리 없고, 그 결정권자 자신들이 바꾸지 않을 것임
          + 이 주장엔 농담의 요소도 있지만, 사실 CEO의 핵심 역할은 “책임을 먹는 것”이라는 점에서 LLM이 문제가 생겨도 책임지고 내보낼 수 있는 대상이 아니라, 실제론 무의미함. 다만 AI 덕분에 조직 구조가 log(n_employees) 식으로 줄면서 레이어가 없어진 회사도 나올 수 있고, 일부 레이어는 AI로 완전 대체될 수 있음. 또 LLM이 책임을 못지는 문제를 해결하기 위해 여러 길드와 인디펜던트 컨트랙터가 LLM의 조율로 함께 움직이는 조직 형태도 가능할 것 같음. 그럴 경우 책임은 각 컴포넌트에 남게 됨
          + 오히려 AI가 이런 식으로 활용되는 것이 최고의 용례 중 하나일 수 있고, 곧 테크 협동조합들이 이 아이디어를 실험하기 시작할 거라는 예측임
"
"https://news.hada.io/topic?id=21695","Lingo — 웹·모바일 앱을 위한 AI 자동화 번역/로컬라이제이션 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Lingo — 웹·모바일 앱을 위한 AI 자동화 번역/로컬라이제이션 플랫폼

     * lingo.dev는 LLM 기반 AI로 앱, 웹사이트, 데이터베이스까지 한 번에 자동 번역하는 개발자/팀용 로컬라이제이션 SaaS
          + AI 자동 번역 엔진 내장: 코드 커밋 시 즉시 번역 적용, 수작업 없이 고품질 번역 결과 제공
          + Git-native UI 로컬라이제이션: Git PR 자동 생성 및 UI 문자열 동기화
     * CI/CD, CLI, JS SDK, API 등 다양한 통합 옵션을 제공하며, 코드 푸시 시마다 고품질 번역을 자동 적용
     * 전체 제품 에코시스템 번역 지원: UI·마케팅·이메일·동적 콘텐츠 등 풀스택 대응
     * 브랜드 보이스, 문맥 인식, 용어집, 젠더 중립, 문화 차이 대응 등 세밀한 커스터마이즈 기능 탑재
     * Git과 연동된 UI 문자열 관리, 실시간 동적 콘텐츠 번역, 고급 엔터프라이즈 보안·컴플라이언스 옵션 제공
     * 월 10,000단어 무료 제공(취미 요금제), 소규모부터 대규모 팀/엔터프라이즈까지 확장 가능
     * 요금제
          + Hobby(무료): 월 10,000단어, 무제한 언어·용어집, CI/CD·SDK 지원, 1개 프로젝트
          + Pro($30/월): 월 20,000단어, 추가 단어 단가 저렴, 우선 지원
          + Team($600/월): 월 100,000단어, 무제한 프로젝트, 프라이빗 슬랙 지원 등
          + Enterprise(견적): 대량 번역, SLA, 맞춤 지원
     * 기존 번역 관리 시스템, ChatGPT API에서 손쉽게 이전 가능
"
"https://news.hada.io/topic?id=21623","리눅스 파이프는 실제로 얼마나 빠를까?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         리눅스 파이프는 실제로 얼마나 빠를까?

     * Linux에서 구현된 Unix 파이프의 성능을 점진적 최적화를 통해 분석함
     * 최초 간단한 파이프 프로그램의 대역폭은 약 3.5GiB/s로 측정되며, 프로파일링과 시스템콜 변경을 통해 이를 20배 이상 향상하는 과정을 다룸
     * vmsplice, splice 같은 Zero-Copy 시스템콜을 활용해 불필요한 데이터 복사를 줄이고, 페이지 사이즈를 키우는 등 다양한 최적화 기법을 설명
     * Huge Page 사용과 바쁜루프(busy loop) 기법 적용으로 병목을 해결해 최대 62.5GiB/s의 처리속도를 기록
     * 파이프, 페이징, 동기화 비용, Zero-Copy 등 고성능 서버와 커널 프로그래밍에서 중요한 요소들에 대한 인사이트를 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 도입

     * 이 글은 Linux에서 Unix 파이프가 어떻게 구현되는지, 파이프를 통해 데이터를 읽고 쓰는 테스트 프로그램을 직접 작성해가며 점진적으로 성능을 최적화하는 과정을 다룸
     * 처음에는 대략 3.5GiB/s의 대역폭을 가진 단순 프로그램으로 시작해, 다양한 최적화를 거쳐 약 20배의 성능 향상을 달성함
     * 각 단계의 최적화는 perf 툴을 사용한 프로파일링 결과를 바탕으로 결정하며, 관련 소스코드는 GitHub - pipes-speed-test에 공개되어 있음
     * 영감을 준 것은 고성능 FizzBuzz 프로그램(36GiB/s)에서 파이프를 이용한 데이터 처리 속도를 보며 시작한 고찰
     * C언어에 대한 기초 수준의 지식만 있다면 내용 이해에 무리가 없음

파이프 성능 측정: 첫 느린 버전

     * 고성능 FizzBuzz 프로그램의 예시 실행 결과, 파이프를 통해 초당 36GiB의 데이터를 처리하는 것을 확인함
     * FizzBuzz는 L2 캐시 크기(256KiB) 블록 단위로 출력하여 메모리 접근과 IO 오버헤드 사이 균형을 맞춤
     * 이 글에서 작성한 파이프 성능 테스트 프로그램도 256KiB 블록 단위로 반복 출력(read/write)하며, 측정을 위해 read와 write 양 끝단을 모두 직접 구현
     * write.cpp는 동일한 256KiB 버퍼를 반복적으로 쓰고, read.cpp는 10GiB를 읽고 종결하며 처리량을 표시하는 구조
     * 테스트 결과, 파이프를 통한 read/write는 3.7GiB/s로, FizzBuzz 대비 10배 느린게 보임

write 동작의 병목과 내부 구조

     * perf 툴로 프로그램 실행 시 콜그래프를 추적한 결과, 전체 시간의 절반 가량이 파이프 쓰기(즉, pipe_write) 단계에서 소모됨을 확인
     * pipe_write 내에서는 대다수 시간이 메모리 페이지 복사 및 할당(copy_page_from_iter, __alloc_pages) 에 소모
     * Linux 파이프는 원형버퍼(ring buffer) 형태로 구현되어 있으며, 각 엔트리는 실제 데이터가 저장된 페이지를 참조
     * 파이프의 전체 버퍼 크기는 고정되어 있으며 파이프가 가득 차면 write가 블로킹되고, 비어있으면 read가 블로킹 상태가 됨
     * C 구조체(pipe_inode_info, pipe_buffer)에서 head와 tail은 각각 쓰기/읽기 위치를 나타내며, 개별 페이지의 오프셋과 길이 정보를 포함

파이프의 읽기/쓰기 로직

     * pipe_write는 다음과 같은 순서로 동작함
          + 파이프가 가득 차 있으면 공간이 생길 때까지 대기
          + 현재 head에서 남는 공간을 우선 채움
          + 공간이 더 있다면 새 페이지 할당, 버퍼에 데이터 복사, head 갱신
     * 모든 연산은 락(lock)으로 보호되어 동기화 오버헤드가 발생
     * 읽기(read)는 동일한 구조로 tail을 이동하며 읽은 페이지를 해제
     * 본질적으로 유저 메모리에서 커널로, 커널에서 다시 유저 공간으로 두 번 복사가 이루어져 상당한 오버헤드 발생

Zero-Copy: Splice/vmsplice로의 최적화

     * 빠른 IO를 위한 일반적 방법론은 커널을 우회(bypass)하거나 복사를 최소화하는 것임
     * Linux는 splice와 vmsplice 시스템콜을 통해 파이프와 유저 공간 간 데이터 이동 시 복사를 생략할 수 있도록 지원함
          + splice: 파이프와 파일 디스크립터 간 데이터 이동
          + vmsplice: 유저 메모리와 파이프 간 데이터 이동
     * 두 시스템콜 모두 참조만 옮길 뿐 실제 데이터 이동 없이 처리할 수 있음
     * 예를 들어, vmsplice 활용 시 256KiB 버퍼를 반으로 나누고, 더블버퍼링 방식으로 각 절반을 번갈아 파이프에 vmsplice 함
     * 실제로 vmsplice 적용 시 3배 이상 속도가 향상(약 12.7GiB/s) 되며, read 측에 splice를 적용하면 32.8GiB/s로 추가 향상됨

페이지 관련 병목과 Huge Page 활용

     * perf 분석 결과, vmsplice의 병목은 파이프 락(mutex_lock) 와 페이지 획득(iov_iter_get_pages) 에 집중됨
     * iov_iter_get_pages는 유저 메모리(virtual address)를 실제 물리 페이지(physical page)로 변환하여 파이프 내에 참조를 저장하는 역할임
     * Linux의 페이징은 4KiB 단위 페이지만을 사용하는 것이 아니며, 아키텍처에 따라 2MiB(huge page) 등 다양한 크기 지원
     * Huge Page(예: 2MiB) 활용 시, page table 관리 및 참조 횟수 감소로 인해 페이지 변환 오버헤드가 현저히 줄어듦
     * 프로그램에서 huge page 적용 시 최대 처리량이 51.0GiB/s로 약 50% 추가 증가함

바쁜루프(busy loop) 적용

     * 남은 병목은 파이프에 쓸 공간이 생기길 기다리는 wait, reader를 깨우는 wake과 같은 동기화 처리임
     * SPLICE_F_NONBLOCK 옵션 사용 및 EAGAIN 발생 시 바쁜루프로 반복 호출하여 커널의 스케줄링 오버헤드 제거
     * 해당 기법 적용 시 최대 처리량이 62.5GiB/s로 25% 더 향상됨
     * 바쁜루프는 CPU 자원을 100% 소모하나, 고성능 서버에서는 흔히 쓰이는 패턴임

정리 및 기타 사항

     * perf와 Linux 소스 코드 분석을 통해 step-by-step으로 파이프 성능을 비약적으로 향상하는 방법을 설명함
     * 파이프, splice, 페이징, Zero-Copy, 동기화 비용 등 고성능 프로그래밍의 주요 이슈들을 실제 예제와 함께 체험할 수 있음
     * 실제 코드에서는 버퍼를 서로 다른 페이지에 할당해 refcount contention을 줄이는 등 추가적인 성능 튜닝이 적용됨
     * 테스트는 각각의 프로그램 프로세스를 개별 코어에 고정(taskset)시켜 실행함
     * Splice 계열은 설계상 위험할 수 있으며, 일부 커널 개발자들에게는 오랜 논쟁 거리임

   우와! 재밌네요! (무슨 얘긴지 하나도 모르겠…)

   |

        Hacker News 의견

     * Linux 파이프 기반 애플리케이션을 Windows로 포팅했던 경험을 잊지 못함, POSIX 표준이니 성능이 크게 다르지 않을 거라 생각했는데 엄청나게 느렸던 기억, 파이프 연결을 대기하는 상황에서 Windows 전체가 거의 멈추는 수준까지 갔던 문제점, 몇 년 뒤 Win10에서 C#으로 같은 걸 다시 구현했을 때는 조금 나아졌지만 성능 차는 여전히 큰 부끄러움이었던 기억
          + 최근 몇 년 사이 Windows에 AF_UNIX 소켓이 추가된 걸로 아는데, Win32 파이프 대비 어느 쪽 성능이 나은지 궁금함, 내 예상엔 더 나을 거란 추측
          + ""성능이 엉망이었다""고 할 때, 파이프가 이미 연결된 이후의 I/O를 말하는 건지, 아니면 연결 이전의 과정인지 궁금증, 이미 연결된 후라면 의외이겠지만, 연결/해제 반복이 문제라면 OS가 최적화하지 않았을 가능성 인정, 실제로 그럴 필요는 거의 없으니까 사용 사례에 따라 다르게 받아들임
          + 내가 최근에 확인해본 결과 Windows에서 로컬 TCP의 성능이 파이프보다 훨씬 뛰어나다는 사실
          + POSIX는 동작만 정의하고 성능은 정의하지 않는다는 점, 각 플랫폼과 OS마다 고유의 성능 특이점이 있음을 상기
          + 옛날에 반대 경우의 경험이 있었음, 파이프는 아니지만, Linux에서 PHP 앱이 .NET 기반 SOAP API와 통신했을 때 .NET 구현 쪽 응답 속도가 더 좋았던 기억
     * 참고로 readv() / writev(), splice(), sendfile(), funopen(), io_buffer() 등 여러 방법이 있음, splice()는 파이프와 UNIX 소켓 사이에서 zero-copy로 대용량 데이터 전달 시 매우 뛰어난데 Linux 전용임, splice()는 데이터 전송 시 사용자 공간 메모리 할당, 추가 버퍼 관리, memcpy(), iovec 탐색 없이 직접 처리하는 가장 빠른 방법, BSD 계열에서는 파이프에 대해 readv()/writev()가 최적이 맞는지 여부에 대한 확인 요청도 함께, 어쨌거나 이 기사 매우 인상적이라는 평
          + sendfile()은 파일→소켓 zero-copy 방식의 매우 높은 성능 제공, Linux와 BSD 양쪽에서 사용 가능, 단 파일→소켓만 지원함, sendmsg()는 일반적인 파이프에는 사용 불가, UNIX 도메인/INET/기타 소켓용임, 참고로 Linux에서는 sendfile이 내부적으로 splice로 구현된 덕분에 파일→블록 디바이스 전송에도 실제로 사용해본 경험
          + splice()가 리눅스에서 파이프 간 초고속 대량 데이터 전송의 최고지만, io_uring을 제대로 쓰면 비슷하거나 오히려 앞지르는 성능 기대 가능
          + shm_open 같은 공유 메모리와 파일 디스크립터 전달 방식이 실제론 더 빠르고, 완전히 포터블함
     * 지난 HN에서 이 기사에 대해 토론이 활발하게 이뤄졌던 링크라며 https://news.ycombinator.com/item?id=31592934 (200개 댓글), https://news.ycombinator.com/item?id=37782493 (105개 댓글)로 안내
     * 정말 멋진 기사라며, 주기적으로 다시 언급되는 것도 매우 반가운 점
          + 오타 정정이라며 comes → comes up 명시
     * 아직 아무 댓글도 없어서 아쉽다는 감상과, splice를 더 많이 쓰고 싶으나 글 마지막에 언급된 보안이나 ABI 호환성 이슈가 걱정, splice가 앞으로도 계속 유지될지, 그리고 성능 향상을 위해 기본 파이프가 splice를 항상 쓰도록 패치하는 난이도도 궁금증 제기
          + 추가 댓글 논의를 위해 https://news.ycombinator.com/item?id=44347412 링크 공유
     * 최신 Linux에 SunOS의 Doors와 유사한 무언가가 있는지 질문, 지연 시간이 매우 민감한 작은 데이터 교환이 필요한 임베디드 애플리케이션에서 AF_UNIX보다 나은 기술을 찾는 중인 상황
          + 공유 메모리가 지연 시간 측면에서 가장 빠르지만, 태스크 깨우기(보통 futex 활용)가 필요함, Google이 FUTEX_SWAP 시스템 콜을 개발하고 있었는데, 한 태스크에서 다른 태스크로 직접 핸드오버 가능할 예정이었으나 이후 상황은 잘 모름
          + 'Doors'가 너무 일반적인 단어라 검색이 어려워 설명 요청
          + 현재 AF_UNIX에 대해 어떤 점이 문제인지, 필요한 기능이 없는지, 원하는 것보다 지연이 높은지, 아니면 서버/클라이언트 소켓 API 구조가 맞지 않는 것인지 추가 정보 요청
     * 기사 작성 시점을 2022년으로 간결하게 정보 추가
"
"https://news.hada.io/topic?id=21682","구글 Gemma 3n 공개 - 새로운 온디바이스 멀티모달 AI의 등장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 구글 Gemma 3n 공개 - 새로운 온디바이스 멀티모달 AI의 등장

     * Gemma 3n은 모바일·엣지 환경을 위한 최신 온디바이스 멀티모달 AI 모델로, 이미지·오디오·비디오·텍스트를 모두 처리할 수 있음
     * 효율성 중심 구조와 혁신적 아키텍처(Matformer, Per-Layer Embeddings, MobileNet-V5 등) 로, 기존 클라우드 대형 모델 수준의 성능을 2~3GB 메모리에서 구현
     * E2B/E4B 두 가지 모델 크기 제공 및 Mix-n-Match 방식으로 하드웨어에 맞춘 세밀한 커스텀 사이즈 지원
     * 음성 인식·번역, 실시간 비전 분석, 140개 언어 다국어 처리 등 다양한 온디바이스 AI 활용 사례에 즉시 적용 가능
     * Hugging Face, Ollama, llama.cpp 등 주요 AI 오픈소스 생태계와 광범위하게 연동되며, 각종 툴·API·SDK로 즉시 활용 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 배경

     * 작년 초 출시된 최초의 Gemma 모델은 1억6천만회 이상의 다운로드를 기록하며 Gemmaverse라는 생태계로 성장함
     * 이 생태계에는 보안, 의료 등 다양한 특화 모델과 커뮤니티 기여로 만들어진 여러 혁신적 활용 사례가 포함됨
     * Google은 이러한 성공에 힘입어, 모바일 중심으로 설계된 Gemma 3n의 공식 릴리즈를 발표함
     * Hugging Face Transformers, llama.cpp, Google AI Edge, Ollama 등 개발자 친화적 생태계와 도구의 광범위한 통합을 제공함
     * Gemma 3n의 핵심 혁신·벤치마크와 개발 방법에 대해 개발자 관점에서 심층적으로 소개함

What’s new in Gemma 3n?

     * Gemma 3n은 온디바이스 AI의 새로운 도약을 의미함
     * 텍스트, 이미지, 오디오, 비디오 입력 및 텍스트 출력의 네이티브 멀티모달 지원을 제공함
     * 효율성을 극대화하여 E2B(5B 파라미터), E4B(8B 파라미터) 두 가지 모델 크기로 제공하며, 낮은 메모리(2GB, 3GB)로도 실행 가능함
     * MatFormer, Per Layer Embeddings, LAuReL, AltUp 등의 혁신적 아키텍처 적용과 새로운 오디오·비전 인코더 탑재함
     * 140개 언어 지원, 35개 언어의 멀티모달 이해, 수학·코딩·추론능력 강화, E4B 기준 LMArena 1300점 돌파

MatFormer: 하나의 모델, 다양한 크기

     * MatFormer(🪆Matryoshka Transformer) 아키텍처는 확장성과 유연성을 위해 설계된 새로운 트랜스포머 구조임
     * 큰 모델 내부에 작은 모델이 독립적으로 포함되는 러시아 마트료시카 원리를 활용함
     * E4B 학습 시 E2B 서브모델을 동시에 최적화하여, 별도의 사전 추출된 모델 다운로드와 최대 2배 빠른 추론 가능함
     * Mix-n-Match 방식으로 하드웨어 제약에 맞춘 맞춤형 중간 모델(피드포워드 네트워크 또는 레이어 스킵 활용) 생성 가능함
     * MatFormer Lab에서 벤치마크 기반 최적 세팅 확인 및 모델 생성 가능함
     * 미래에는 Elastic execution(실시간 동적 모델 크기 전환)도 지원할 계획임

Per-Layer Embeddings (PLE): 온디바이스 메모리 효율 극대화

     * Per-Layer Embeddings(레이어별 임베딩) 으로 온디바이스 배포 시 품질 향상, 메모리 사용 최소화 실현함
     * 전체 파라미터(5B/8B) 중 임베딩만 CPU에서 효율적으로 로드/처리, 트랜스포머 코어(2B/4B)만 VRAM에 상주함
     * 덕분에 기존 대비 훨씬 작은 메모리(가속기에 약 2B 파라미터만)를 사용하면서도 품질 저하 없이 작동 가능함

KV Cache Sharing: 긴 컨텍스트 입력 최적화

     * Gemma 3n은 긴 오디오/비디오 등 순차적 입력을 빠르게 처리하기 위해 KV Cache Sharing 기능을 추가함
     * 프리필(초기 입력 처리) 단계에서 중간 레이어의 KV 캐시를 상위 레이어에 직접 공유, 최대 2배 이상 성능 개선
     * 긴 시퀀스 프롬프트를 기존보다 빠르게 인식하여 멀티모달 애플리케이션의 실시간성 향상 가능함

오디오 인식: STT 및 번역 지원

     * Universal Speech Model(USM) 기반의 오디오 인코더를 탑재, 160ms 단위의 오디오 토큰을 언어 모델 입력으로 활용함
     * 온디바이스 고품질 음성 인식(ASR), 음성 번역(AST) 구현 가능함
     * 영어↔스페인어, 프랑스어, 이탈리아어, 포르투갈어 등 주요 언어쌍에서 높은 성능이 확인됨
     * Chain-of-Thought 프롬프트 기법을 활용하면 번역 품질 향상이 가능함
     * 최초(런칭 시점) 오디오 인코더는 30초 클립까지 지원, 추후 더 긴 스트리밍 처리도 가능하도록 확장 예정임

MobileNet-V5: 최신 비전 인코더

     * Gemma 3n에 통합된 MobileNet-V5-300M은 엣지 디바이스에서도 강력한 성능을 제공하는 고효율 비전 인코더임
     * 256x256, 512x512, 768x768 픽셀 등 다양한 입력 해상도를 지원하여 요구에 맞는 성능·디테일 조정 가능함
     * 대규모 멀티모달 데이터셋 기반 공동 학습으로 이미지·비디오 이해 광범위성 실현 및 구체적 시각 과제 처리에 능함
     * Google Pixel에서 초당 60프레임 실시간 분석 가능
     * 아키텍처 측면에서는 MobileNet-V4 기반 블록(유니버설 인버티드 보틀넥, Mobile MQA) 및 하이브리드 피라미드 구조, Multi-Scale Fusion VLM 어댑터 등 다수 혁신 적용함
     * SoViT(Gemma 3 베이스라인) 대비 13배 속도, 46% 파라미터 절감, 4배 작은 메모리, 더 높은 정확도로 월등함
     * 기술 보고서 내 아키텍처, 데이터 확장 전략, 딥러닝 증류 기법 등 추가 공개 예정임

실전 적용 및 사용법

     * AI Studio에서 바로 사용해보기: https://aistudio.google.com/prompts/new_chat?model=gemma-3n-e4b-it
     * 모델 다운로드/배포: Hugging Face, Kaggle, Ollama, llama.cpp 등에서 즉시 사용 가능
     * 툴·프레임워크 연동: Hugging Face Transformers/TRL, MLX, Docker, LMStudio, NVIDIA NeMo, Unsloth 등 대부분 지원
     * API·클라우드 배포: Google GenAI API, Vertex AI, NVIDIA API 등 다양한 환경에서 배포

주요 온디바이스 활용 시나리오

     * 스마트폰/엣지 디바이스 내 실시간 AI 비서·음성 번역기·멀티모달 챗봇·실시간 비전 분석·IoT
     * 리소스 제약 환경에서의 AI 서비스 내재화
     * 오프라인·네트워크 제약 환경에서의 AI 혁신

개발자 리소스

     * 공식 문서
     * 모델 다운로드(HF)
     * MatFormer Lab
     * 구글 AI Studio에서 사용해보기
     * 오픈소스 생태계 연동, Ollama, MLX, llama.cpp 등

Gemma 3n Impact Challenge

     * 온디바이스/오프라인/멀티모달 기능을 활용해 실질적 사회적 임팩트가 있는 제품 개발 공모전 개최
          + 상금 $150,000, 영상·데모 제출 필요: https://www.kaggle.com/competitions/google-gemma-3n-hackathon

        Hacker News 의견

     * 이 모델은 예전 gemma3로 진행하던 모든 작업과 완벽 호환성을 보여줌, 내 vlm 파인튜닝 스크립트에 바로 연결해봤는데 문제 없이 동작함(hf transformer 코드 기준임). Lora로 싱글 GPU에서 E4B 모델을 실행하면 batch size 1 기준 18Gb VRAM이 필요하고, gemma-4B는 21Gb 필요했음. deepmind에서 정말 잘 만들었음, gemma3 시리즈가 공개 가중치 VLLM 중 최고임
          + 수정: 현재 언급하는 모델은 E2B임
     * ""펠리컨이 자전거를 타는 SVG 생성"" 프롬프트를 Gemma 3n 7.5GB (Ollama)와 mlx-vlm의 15GB 버전에 적용해봤고, 두 가지 양자화 크기마다 결과가 상이해서 재밌었음. 결과는 여기에 올려놨음: https://simonwillison.net/2025/Jun/26/gemma-3n/
          + 이게 실제로 의미 있는 벤치마크라고 할 수 있을까, 아니면 단순히 재미용인지 궁금함. 사실 잘 이해되지 않음
     * 아직도 Gemma와 Gemini가 온디바이스 환경에서 어떻게 다른지 잘 이해가 안 됨, 둘 다 네트워크 연결 없이 사용할 수 있다는 점은 똑같음. 공식 문구 활용 예시: ""Gemini Nano는 네트워크 연결 없이도 풍부한 생성형 AI 경험을 제공합니다"" — 이 문장에서 Gemini 대신 Gemma를 넣어도 완전히 맞는 내용임
          + 차이는 라이선스임. Gemini Nano 가중치는 직접 사용할 수 없고(특히 상업용일 때), 반드시 Android MLKit이나 Google에서 승인한 런타임을 통해서만 접근 가능함. 반면 Gemma는 원하는 런타임, 프레임워크 어디서든 상업적으로 사용 가능함
          + Gemma 3n 프리뷰 블로그를 보면 Gemma 3n과 새로운 Gemini Nano 버전이 같은 아키텍처를 공유함. 여기서 n은 Nano의 약자라고 봄. Nano는 Android에 내장되는 독점 모델이고, Gemma는 오픈 모델이라 어디든 자유롭게 적용 가능함. 관련 출처들은 구글 공식 블로그와 영상에 있음
          + Gemma는 오픈소스이고 apache 2.0 라이선스임. 앱에 포함하려면 스스로 패키징해야 함. 반면 Gemini Nano는 완전히 제어할 수 없는 Android API임
          + 두 모델의 차이가 학습 데이터일 거라고 추측함. Gemini 쪽은 훨씬 더 엄격하게 관리되고, 학습 데이터에 있던 걸 반복 출력하려 하면 'recitation error'가 발생할 수 있음
     * OpenAI 덕분에 업계에서 이런 무질서한 네이밍이 표준이 된 것 같아서 개인적으로 별로임
          + 그렇다면 어떤 이름을 지었을 것인지 궁금함
     * GGUF 버전을 직접 만들었으니 필요하면 누구든 사용해볼 수 있음! ./llama.cpp/llama-cli -hf unsloth/gemma-3n-E4B-it-GGUF:UD-Q4_K_XL -ngl 99 --jinja --temp 0.0 이런 식으로 실행함. 또한 inference + finetuning을 위한 Colab 데모도 만들고 있음. Gemma 3N은 오디오, 텍스트, 비전까지 지원해서 정말 인상적임. 자세한 내용은 https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune 참고
          + Ollama에서 E4B 모델을 테스트해봤는데 이미지 해석이 완전히 잘못 동작함. 출력이 텍스트에만 의존하고, 항상 일관되게 잘못 나오며, 정식 Gemma 3 4B는 잘 되길래 Ollama 문제라고 판단함. 조사해보니 현재는 텍스트 전용 지원임[1], 이 점이 좀 더 명확하게 안내되었으면 함. llama.cpp를 직접 빌드하기 귀찮아서, GGUF가 지원될 때까지 기다려볼 예정임. [1]: https://github.com/ollama/ollama/issues/10792#issuecomment-3009619264
          + Unsloth 버전을 쓰려고 타이핑하고 있었는데 이미 만들어서 올린 모습 보고 감탄하게 됨. 대단함!
          + 고마움! 이런 모델 실행하려면 어떤 PC 사양이 필요한지 궁금함
          + 여기서 jinja가 의미하는 건 뭔지 궁금함
     * 솔직히 이런 소형 모델들이 실사용에 어떻게 도움이 되는지 의문임. 여러 시도를 해봤는데 27B보다 작은 모델은 장난감 수준 이상 쓰기 힘들고, 가끔씩만 좋은 답변을 주는 게 전부임. gemma3:27b-it-qat로 스팸 필터 문제 해결했고, 내 벤치마크 결과도 그 선에서 쓸만해짐을 확인함
          + 정확도가 낮아도 실제로 쓸 곳이 있음. 미래에 어떤 제품이 나올지는 모르겠으나 이미 오늘날 다음과 같은 사례가 있음: 아이폰 키보드에서 작은 언어 모델이 다음 단어 추천할 때 사용됨(사용자가 제안된 단어만 고르면 됨). 그리고 speculative decoding처럼 작은 모델이 큰 모델 추론 속도를 올리는데 활용됨. 앞으로 더 똑똑한 활용처가 등장할 것임
          + 이런 소형 모델은 기초 인프라로 보면 충분히 활용도 있음. 언젠가 대부분의 휴대폰에 내장 LLM이 탑재되는 미래가 오면 정말 좋겠음, 마치 기본 인프라처럼 되는 그런 상황을 희망함
          + 내가 찾아낸 가장 좋은 소형 모델(<5bn params) 활용법은 오프라인 참조 도구임. 비행기에서 코딩할 때 Google 대신 MacBook Air에 qwen을 설치해서 문법, 문서화 등 기초 질문을 묻는 용도로 유용함
          + 4b 이하의 소형 모델은 특정 태스크 파인튜닝에 최적화돼서, 아주 저렴하게 상업 모델보다 좋은 결과도 가능함. 코드 자동완성에서도 좋음. 7b~8b 모델은 코드 리팩토링 등 빠르고 단순한 코딩 과제에 괜찮음(예: ""SomeType 타입 인자가 있는 모든 함수명을 ST_로 프리픽스 추가하기""). 12b 모델부터는 미스트랄 Nemo나 Gemma 3 12b처럼 일관성 있는 문장까지 생성할 수 있음
     * Kevin Kwok이 모델 구조 리버스 엔지니어링을 아주 잘 정리했으니 참고 바람: https://github.com/antimatter15/reverse-engineering-gemma-3n
     * Google 사이트 어딘가에 각 제품명, 설명, 기능을 표로 정리해둔 데이터가 필요함
     * 그래프의 Y축이 정말 웃기게 그려져 있음
     * gemma 3n의 배포 버전을 실제로 사용하면 얼마가 드는지 아는 사람? 문서에는 gemini api로 gemma 3n을 쓸 수 있다고 나와 있는데, 가격은 ""unavailable""로만 표시됨
"
"https://news.hada.io/topic?id=21690","XSLT - 웹을 위한 Native, Zero-Config 빌드 시스템","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                XSLT - 웹을 위한 Native, Zero-Config 빌드 시스템

     * XSLT는 복잡한 빌드 시스템 없이 바로 사용할 수 있는, 웹을 위한 기본 제공 빌드 도구임
     * 대부분의 정적 사이트 빌드시스템은 복잡성, 이해 어려움, 프레임워크 의존성 문제가 있음
     * XML과 XSLT를 활용하면 브라우저에서 바로 HTML을 생성 가능, 동적인 데이터 처리 및 마크업 생성이 용이함
     * 모든 주요 브라우저가 XSLT 기반 변환을 지원하여 추가 JavaScript나 별도 툴 없이 사용 가능함
     * 완벽한 솔루션은 아니지만, 웹 표준에 기반한 심플하고 유연한 대안으로 활용 가치가 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 문제의식

     * 대부분의 정적 웹사이트 개발 과정은 데이터를 위한 파일(.json, .md, .txt), 별도의 빌드 시스템(Hugo, Next.js, Astro 등), HTML 산출물 구조로 이뤄짐
     * 빌드 시스템은 복잡성이 증가하고, 작은 블로그조차 이해와 운영이 점점 어려워짐
     * 프레임워크를 배제하고 간단한 HTML, CSS, 표준 명세(HTTP, URI, HTML)만으로 작업하려다 보면, 헤더나 푸터 반복 등 유지보수에서 한계 발생함
     * HTML import, web component 등도 시도했지만, 전자는 지원이 없고, 후자는 JavaScript 엔진 의존 문제로 대안이 되지 못함

웹 브라우저를 빌드 시스템으로

     * 웹 브라우저 자체가 다양한 데이터 변환(text/html, text/markdown, application/xml 등)을 지원한다는 점에 착안함
     * 웹 명세를 깊이 있게 살펴보고, 불필요한 외부 도구와 프레임워크 없이 문제 해결법을 고심함

XSLT의 활용

     * RSS 피드를 예쁘게 보여주고 싶어서 XSLT 명세를 발견함
     * XSLT는 XML 데이터와 HTML 출력 구조 모두를 표현하는 공식 표준 기술임
     * 사용 방법은 간단함
          + 예를 들어 blog.xml에 데이터를 정리
          + 다음과 같이 XSLT 스타일시트 연결
               o <?xml-stylesheet type=""text/xsl"" href=""blog.xsl""?>
          + blog.xsl에는 HTML 템플릿 및 데이터 매핑 규칙 작성
     * 템플릿, 반복, 변수, 외부 파일 import 등 대부분의 빌드 시스템 기능이 지원됨

실행 방법 및 장점

     * 별도 도구 없이 XML 파일을 웹 브라우저로 열기만 하면 변환 결과가 바로 렌더링됨
     * XML 포맷은 HTML과 유사하여 인간 친화적이고 유지보수 용이, JSON 대신 사용해도 불편함 적음
     * 주요 웹 브라우저는 모두 XSLT 기반 변환을 네이티브로 지원하여, 클라이언트가 직접 변환 결과를 확인 가능
     * JavaScript, 별도 빌드 도구, 번들러가 필요 없다는 점이 대단히 큼
     * XSLT는 궁극적인 만능 해결책은 아니지만, 심플하면서 확장성 있는 웹 빌드 대안임

결론

     * 과거의 기술(XSLT)과 명확한 표준의 가치를 재발견
     * 웹 브라우저를 빌드 시스템으로 활용하는 방안은 웹 개발 도구 상자에 추가할 만한 유용한 옵션임

        Hacker News 의견

     * 내가 몸담았던 회사는 XML 템플릿에 XSLT를 매우 많이 사용하는 곳이었고, 아마 아직도 그럴 가능성이 높음. 솔직히 좋지 않은 선택지였고, 가능하다면 다른 걸로 옮기고 싶어했을 것임
         1. XSLT는 최신 표준이 있음에도 여전히 1.0 버전이 주를 이루는데, 이 버전은 신규 표준에 비해 제한적이고 기이한 구석이 많음
         2. XSLT 템플릿의 성능 문제는 해결이 극도로 어려운 난이도임. XSLT는 튜링 완전 함수형 스타일 언어 특성상 성능이 안 보임. 대부분 문서에선 괜찮지만, 100개짜리 행이 들어오면 갑자기 터짐. 테이블을 처리하는 코드가 O(N^2) 성능을 가지고 있어 최적화도 거의 불가능함. 예를 들어, 행마다 O(N) 짜리 XPath가 들어가 있을 수도 있음. 내 기억으로는 해당 템플릿이 한 문서를 처리하는 데 7분 넘게 걸렸음.
            JS도 문제는 있지만, 적어도 이런 알고리즘 복잡도를 안 고칠 수밖에 없는 상황은 아님
          + XSLT/XPath는 1.0 이후로 꽤 발전함. key(index) 등 다양한 기능이 나와서 처리 속도 많이 빨라짐. Saxon 같은 품질 좋은 XSLT 구현체 쓰면 성능 문제도 훨씬 덜함. XML을 다른 것으로 변환할 때 XSLT만큼 논리 구조화에 편리한 것도 드물다고 생각함
          + XSLT는 배우기가 상당히 어려움. 마치 몽환적인 프롤로그 같기도 하고, 정말 숙련되면 스도쿠 풀 때의 짜릿함 느껴짐. 하지만 대부분의 경우 그렇게까지 복잡한 템플릿이 필요하지 않아서 표준적인 선택지가 되기는 힘듦. 그리고 XML 자체도 누구나 좋아하는 포맷은 아님
          + XSLT 1.0이 아직도 많이 쓰인다는 부분이 이해가 잘 안 됨. 2013년에도 이미 1.0은 거의 퇴물 취급이었고, XSLT 2 쓸 수 있는 Saxon은 무료에 성능도 굉장히 좋았음. 큰 문서든 작은 문서든 변환할 때 성능 문제 한 번도 경험한 적 없음
          + XSLT가 등장하던 시절은 본문이 매우 긴 XML을 처리하는 게 당연시되던 시기였음. 그런데 이렇게 반복 루프가 중첩되면 당연히 터지기 마련인 부분 특이사항임
          + 혹시 상용 버전의 Saxon을 쓰는지 궁금함. 가격도 비싸지 않고, 새로운 표준 지원 및 성능, 여러 기능 때문에 IMHO 정말 값어치 하는 선택이라고 생각함. 예전에 사용했을 때 상당히 똑똑한 최적화가 들어간 것으로 기억함
     * 90-00년대 브라우저는 서로 제각각이라 JS를 도입해서 동작을 맞추기 시작했음
       사실 우리가 원했던 건 멋진 CSS 스타일이었지만 그때는 제대로 쓸 수 없었음
       시간이 지나고 하나의 브라우저가 주도하게 되며 다른 브라우저들도 많이 비슷해졌고(Highlander 법칙, 하지만 Firefox도 꽤 선전 중)
       이미 프레임워크가 당연해져서 모든 브라우저에 동일한 UI를 맞추는 용도로 정착함. 그리고 패러다임 자체가 JSON 데이터 렌더링으로 이동함
       지금은 서버에서 전통적인 웹페이지를 생성해도 빠르고 메모리도 적게 쓰는 시대임
       왜 이런 생각을 하냐면, 최근 레거시 시스템에서 마이그레이션하면서 페이지 단위 HTTP 요청 방식(2000년대 표준)으로 동작하는 사이트를 다시 경험했음. 한 번 액션할 때마다 새로고침이 필요했지만, 오히려 React 쓰는 시스템보다 훨씬 빨랐음
       이유는
         1. 인터넷이 매우 빨라졌음
         2. 휴대폰 메모리가 풍부한데 JS 프레임워크가 그걸 낭비함
         3. 백엔드는 예나 지금이나 CRUD, CRUD, CRUD(+페이지네이션, +트랜잭션)임
          + AJAX와 DOM 갱신은 단순히 빨라지기 위해 등장한 게 아님. 웹의 패러다임, 즉 '웹사이트/웹문서'에서 벗어나기 위해서 였음. 전체 페이지 리로드는 문서 중심 패러다임에는 의미 있는 방식임. HN처럼 단순한 예시에서는 이러한 구조가 아주 잘 맞음. 많은 사이트가 JS 프레임워크 대신 이런 구조로도 충분히 동작함.
            하지만 ""모두가 전체 페이지 리로드로 회귀할 수 있다""는 건 현실과 거리가 있음. 실제로 복잡한 인터렉션을 요구하는 '웹 어플리케이션'에는 페이지 전체 리로드가 아주 나쁜 UX임.
            요약하면,
            '웹사이트', '웹문서', '간단한 폼' 등은 전체 페이지 리로드만으로도 충분한 경우가 많지만
            '웹 어플리케이션'처럼 복잡한 데이터 화면/조작이 필요한 경우는 그렇지 않음
          + 내가 기억하는 당시의 타임라인은 조금 다름. JS는 브라우저 동작 통일화 용도보단 인터랙티브 요소를 처음부터 위해 썼음(DHTML, AJAX 등). 진짜로 레이아웃 잡는 것은 브라우저마다 거의 꼼수와 에이전트 감지에 의존했음. CSS가 더 강력해져도 일관성 문제는 쉽게 해결되지 않았음. CSS garden, 시맨틱 마크업, 테이블 남발 등이 그 시절의 분위기였고, 최초 ACID 테스트 패스까지도 정말 오래 걸렸음. 프레임워크가 UI 일관성에 어떤 영향을 줬는지 회의적임—jQuery 이후부터는 CSS 자체가 비주얼 일관성 주범이었음.
            물론 개인의 기억일 수 있음
          + 최신 기술 스택에서는 서버 렌더링 전통 웹페이지가 빠르고 경량임에 공감함
            내 .NET/Kestrel/SQLite 스택에서는 SSR 응답이 4ms 넘기 힘듦. 대부분 100마이크로초대임. 각 페이지마다 여러 쿼리와 복잡한 조인 써서 뷰에 맞는 데이터 형상 만드는 방식임
            100,000행 테이블 만드는 극단적인 케이스에서도 HTML 문자열 조합 전 데이터 가공을 잘하면 성능이 확 올라감. LINQ 성능도 엄청 좋지만, 행별로 콜렉션 만들면 데이터 건수가 많아질수록 오히려 매우 비효율적임
            내 경험상 HTML 템플릿 엔진과 데이터베이스를 최대한 가깝게 붙여놓아야 퍼포먼스 최적화에 가장 좋았음. 최종적으로 DOM은 그냥 바이트 스트림임. 굳이 복잡한 AST/파서 만드는 것보다 StringBuilder랑 SQL 쿼리만 조합해도 충분함.
            이런 간단한 방식에 대한 반론은 언제나 ""개발자는 HTML 이스케이프 못 믿겠다""는 보안 담당자 논란뿐이었음
          + ""최신 기술로 서버에서 예전 방식 고전 웹페이지로 충분히 대응 가능""이라는 얘긴, 만약 네트워크 레이턴시가 높으면 전혀 다른 얘기가 될 수 있음
            참고링크
     * 2000년대 엔터프라이즈 XML이 너무 비대해지면서 기술이 시대에 뒤떨어진 것처럼 보였고, 결국 모두가 JSON의 '깔끔함'에 빠짐. 사실 XSLT, XPath 같은 기술들은 이미 완성도 높아서 오늘날도 여전히 고민하는 문제를 많이 해결해줬었음
       나도 예전에 XSLT include 엄청 남용해봤는데, PHP 스트림 래퍼로 <xsl:include href=""mycorp://invoice/1234"">; 같은 거 쓰곤 했음
       솔직히 지금은 다소 구식 감각일 수 있지만, 브라우저에서 로컬 XSLT 처리시키는 건 여전히 불안함. 예전엔 호환성지뢰밭이었기 때문임
          + XML의 ""기본"" 요소들이 JSON에서 여전히 그리움. 예를 들면 진짜 표준화된 스펙이라든가, 스키마 정의 등은 XML이 훨씬 우위였는데 JSON이 따라잡는 데 거의 10년 걸림
            마지막으로 XML 제대로 만져본 게 EXI라는 전송 기술이었음. XML 문서를 압축 바이너리 스트림으로 바꿔주는 방식이었는데, 구조체 ↔ 아스키 변환 ↔ 압축/전송 ↔ 역변환 물론 번거로웠음. 지금은 protobuf, gRPC가 대세지만, XML이 계속 쓰였다면 모든 게 호환 가능한 표준 기반이라는(내 이상적 상상 속) 세상이 펼쳐졌을지도 모름. 사실 현실적으로 protobuf/gRPC와 JSON API 사이엔 엄청난 장벽이 생겼지만, 그게 오히려 나은 일일 수도 있음
          + XML은 괜찮은 포맷이라고 생각함. 분량이 많고 장황하긴 한데, YAML에 비하면 정밀성과 표현력 면에서 훨씬 뛰어남
            XPath는 익숙해지기 어렵지만 실험해보면 결국 원하는 걸 할 수 있음
            XSLT는 완전히 정신 나간 개념이라 생각함. 퇴출되어야 함
          + Rimworld라는 게임이 모든 설정 데이터를 XML로 저장하고, XPath로 모딩 가능하게 해둠. 이 조합이 정말 강력함. 로컬 데이터 커스터마이즈에는 이만한 게 없음. 그런데 대부분 게임들은 XML이 ""구식""이라는 낙인 때문에 이런 걸 안 쓰려는 듯
            Rimworld의 XPath 모딩 공식 문서
          + 2000년대 초반 엔터프라이즈 XML이 비대해졌다는 얘기는 정말 사실임. 원래 XML은 SGML을 웹에 쓸 수 있게 단순화한 버전으로, 마크업 전달/보카블러리 확장 목적임. 결국은 SVG와 MathML만 살아남았음. 웹 붐에 휩쓸려 W3C/MS가 SOAP, WS-* 스펙등을 마구 쏟아냄. 한때는 Scheme 뼈를 지닌 XSLT 등 언어도 XML에 억지로 맞췄던 광기 가득한 시기였음. 심지어 JavaScript도 이름만큼은 Java에 빗댄 그런 시대
          + Xpath는 네임스페이스 때문에 질릴 만큼 장황한 쿼리를 써야 해서 아쉬움
     * 요즘도 XSLT로 내 피드를 스타일링해서 씀.
       RSS 피드 샘플
       XSLT 샘플
          + 이런 걸 보면 블로그란 게 그냥 RSS 피드여야 하지 않았을까라는 생각 하게 됨
          + XML이 원래 이런 걸 해줄 수 있다는 걸 늘 잊곤 함. 뭔가 직관적으로 어색하게 느껴짐
          + 정말 멋지게 잘 만들었음. 다른 사람들도 이런 예시 꽤 창의적으로 도입해봤으면 함
     * 첫 직장(19살때)에서 Google Search Appliance 커스터마이징 맡은 적 있었음. 수백만 원짜리 노란 Dell 서버들에 CentOS 깔고 구글스러운 파이썬, CIFS 문서 전체 텍스트 검색을 도입하는 프로젝트였음.
       2011년 무렵 XHTML이 대세였고, Google Search Appliance에서는 백엔드 XML 데이터를 XSLT로 XHTML로 변환함. 샘플 템플릿 폭파시켜가며 사내 인트라넷에 맞는 괴작 UI를 만들었고, Coldfusion, StackOverflow, W3Schools 등 기존 자산을 짜깁기해서 낑궈넣었음
       이 경력은 얼른 이력서에서 지움. 이후 DoD(국방부) 하청업체들이 자꾸 ""XML 전문가""랍시고 문서 시스템 현대화 프로젝트에 부르려 해서 피곤했음
       다음에 JSX 써서 JSON에서 TypeScript 인터페이스로 배열 돌리는 걸 하며 한숨쉴 때 내 얘기를 생각해보길. 그나마 XSLT로 그 짓 하는 것보단 나음
     * 나는 단순함 추구자가 맞음. 원시인의 readme처럼 쉬운 문서 좋아함. 가끔은 원시인처럼 키보드 휘두르는 기분도 듦. 웹사이트는 안 하며 XSLT도 잘 모름. 가끔 XML로 해킹하고, 사용자를 위한 걸 보여주고 싶어짐. 파일 포맷 너무 많아 머리 아픔. 그래도 보기 좋은 건 좋아함. 나도 이거 쓸지도 모름
       명세 읽어줘서 고맙고, 툴 만들어줘서 감사함
     * XML이 장황하고 복잡해 보인다고들 하지만, 직접 다뤄 보면 훌륭한 포맷임
       DTD로 검증하고 XSLT로 출력해서 사람이 보기 쉽게 만들 수 있음
       내 기준에선 XML은 C++ 같은 텍스트 포맷임. 성숙하고, '배터리 포함', 강력하고, 어떤 언어든 연결 가능함
       오래된 성숙한 언어가 그렇듯, 사람들은 XML도 괴짜 콘텐츠라고 욕하는 게 트렌드가 된 게 아쉬움. 용도에 안 맞으면 안 쓰면 되지만, 너무 과하게 혐오할 이유는 없음
          + DTD가 아니라 XSD는 왜 안 쓰는지 궁금함
     * ""브라우저에서 XSLT가 바로 동작한다""는 얘기가 신기함. 내가 마지막으로 XSLT 썼던 게 20년 전인데, 그때는 엄청 복잡한 엔터프라이즈 자바 덕분에 XSLT 고유 미학이 다 묻히는 느낌이었음.
       그런데 XSLT가 브라우저에서 기본 동작한다면, 호스트 프레임워크 없는 진짜 정적 템플릿의 성배가 코앞에 있었던 건가?
          + 브라우저들은 XSLT 1.0만 지원함. 그리고 이마저도 앞으로는 사라질 수 있다 얘기도 있음. 차라리 브라우저가 3.0까지 지원해주면 정적 웹페이지 생성에 엄청 쓸만해질 텐데 아쉬움
          + '대형 엔터프라이즈 자바' 타워를 꼭 써야 했던 건 아닌 경험도 있었음. 우리는 tomcat과 몇몇 apache 라이브러리만으로 썼고, 꽤 잘 동작했음. CMS에서 XML로 만든 HTML에, 개인화는 XML 태그 형태로 넣고, 서버 사이드 캐싱 프록시 덕에 변환도 빨라서 많은 트래픽도 소화 가능했음. 핵심은 XSLT 출력 스트림을 즉시 클라이언트로 내보내고, 메모리 전체 버퍼링 안 하는 거였음.
            요즘은 wasm 기반으로 뭔든지 브라우저에서 돌릴 수 있지만, 초창기 JS는 치명적이었고 디자이너들은 포토샵 PSD나 잘 넘겨주면 다행이었음. Google Maps, Gmail 나오던 시절 치열하게 javascript heavy UI 만들고, 당시 Netscape와 Internet Explorer 둘 다 지원해야 했던 진짜 헬
          + XHTML 붐이 일기 시작했던 것도 실은 바로 이 '정적 템플릿 성배' 때문이었음. 그런데 실제로 아는 사람들은 은어처럼 말을 아꼈고, 아무도 대놓고 얘기하지 않았던 묘한 분위기였음
          + 2008년에 브라우저 내 XSLT 사이트에서 일한 적 있었고, 그 전 초창기 2000년대에도 이미 지원했었음
          + Chrome은 libxslt, Firefox는 Transformiix라는 1.0 엔진이 탑재됨. Chrome은 exsl:node-set만 지원하고, Firefox는 다양한 EXSLT 확장 지원함(전부는 아님)
            간단한 XSLT 프로세서 정보와 사용 가능한 확장 리스트 알려주는 작은 도구도 공개함.
            GitHub - xslt-detect-ext
            브라우저에서 out/detect.xslt 파일을 드래그해서 정보 확인할 수 있음(Chrome, Firefox). Safari는 예전 Windows 버전에서는 안 됨
     * 90년대 고등학생 시절 ""웹디자이너""로 불리던 시기에 DSSSL 방언 파이프라인을 사용해 뉴스피드에서 사이트를 자동으로 생성하는 걸 했었음. 나는 지금도 XSLT 변환을 좋아함. bananas XI reader 같은 툴을 이용해 실제 텍스트 변환 및 템플릿 작업도 직접 함
       하지만 이런 툴링을 진정으로 좋아하는 사람들 정말 적었고, 결국 내 자리를 누가 대신하면 도입된 기술은 빠르게 사라지곤 했음
       bananas XI reader
     * 2000년대 초 XML과 XSLT 열풍이 얼마나 심했는지 보여주자면, 내가 일했던 회사에서는 XML을 실시간 속도로 파싱하고 XSLT까지 칩에서 바로 처리하는 ASIC을 만들 정도였음. 당시 미래의 인터넷은 모두 XML/XSLT로 돌아간다고 믿었음.
       실제로 이 회사는 인텔에 인수 당했고, 그 기술은 SSE 가속기 쪽에 들어갔음
          + ASIC에서 XML 해석, XSLT까지 바로 가능한 그런 구조가 주류가 됐다면 지금쯤 웹사이트 속도 엄청나게 빠를 상상 해봄
          + IBM은 아직도 이런 기능이 비슷하게 내장된 하드웨어(DataPower Gateway)를 판매 중임
"
"https://news.hada.io/topic?id=21627","미국 화학 안전 위원회가 폐지될 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         미국 화학 안전 위원회가 폐지될 수 있음

     * 미국 화학 안전 위원회(CSB) 의 폐지 가능성 논의가 제기됨
     * CSB는 화학 산업 내 주요 안전 조사 기관으로, 중요한 역할 수행 중임
     * 이 위원회는 화학 물질 관련 사고의 원인 조사 및 보고서 제공 기능을 담당함
     * CSB 폐지 시 화학 산업의 안전 대응 체계 약화 우려 존재함
     * 정책 변화와 산업 안전에 미칠 영향에 대해 업계 관심이 높음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국 화학 안전 위원회 폐지 논의 개요

     * 최근 미국에서 화학 안전 위원회(CSB) 의 폐지 가능성이 제기됨
     * CSB는 미국 내 화학 산업 전반의 사고 조사와 예방 대책 제시를 주요 임무로 삼음
     * 이 기관은 각종 화학 물질 관련 사고 발생 시 원인 분석 및 구조적인 개선 내용을 발표함
     * 위원회의 폐지 논의는 곧 산업 안전 기준 약화와 신속한 사고 대응력 저하로 이어질 수 있음
     * 기술 및 산업계에서는 정책 변화가 산업 안전과 환경 보호에 미치는 잠재적 영향에 관해 깊은 관심을 보임

        Hacker News 의견

     * 유튜브에서 그들의 놀라운 안전 조사 영상 시청 추천함, 조직력이나 효율성은 잘 모르겠지만 이런 역할은 누군가 꼭 해야 하는 일임을 느낌, 납세자 입장에서는 교육적이고 유익한 방식으로 수행되는 점에 감사함
          + 그들이 직접 자신을 홍보하는 광고 영상도 최근에 올림, 유튜브 링크 가치 제안 매우 괜찮다는 생각
          + 그 영상들은 내 인생 최고의 유튜브 콘텐츠 중 하나로 꼽음, 세금이 이렇게 직접적 즐거움으로 돌아오는 경우가 없었던 기억
          + 그들은 연간 예산 1,440만 달러와 50명의 직원으로 운영됨, 비용 대비 편익이 탁월한 구조
     * 이번 결정은 1950년대식 구시대적 정부 철학과 일치한다고 보임, 기업이 다시 이윤을 사람보다 우선할 수 있게 됨, 규제 없이 강, 숲, 빈 부지에 마음대로 폐기물을 버리고, 마스크 없이 작업하며 암에 노출되고 여러 사고를 일으키던 시절로 회귀하는 느낌, 슈퍼펀드(Superfund) 오염지역도 더 늘려야 한다고 풍자적 언급, 아이들이 납 페인트를 먹고 공장 냄새를 맡으면서 불평하던 시절을 다시 반복
          + 지금도 결과적으로는 예전과 동일하지만, 그 결정에 대한 논리나 미사여구는 바뀌었다고 생각, 과거엔 아무리 심각한 결정도 '원칙'이라는 미명하에 포장했으나, 이제는 그런 포장 없이 대놓고 진행됨
          + 어디서도 언급했지만, CSB는 법 집행 기관 아님, 책임 추궁이나 벌금 부과, 기소, 규정 작성 업무는 하지 않음
          + 미국은 중국의 제조업 성공을 보며 자국도 다시 제조업 강국으로 돌아가길 바라는 중, 당시를 살았던 사람 중 후회하지 않는 이들도 많음
     * 생명 보호: X, 안전성 향상: X, 돈 더 벌기: O, USCSB는 미국인, 특히 위험 화학물질과 압력 장비 등 위험한 환경에서 일하는 이들의 생명과 안전 보장에 도움 주는 기관
          + 실제로는 돈을 더 벌려는 목적조차 아님에 가까움, 그냥 '진보' 세력을 괴롭히려는 의도처럼 느껴짐, 깨끗한 공기와 안전한 일터를 좋아하는 것이 진보의 특징이라는 농담
          + 시설 폭발은 언제나 예방할 수 있는 이유로 발생함, 결국 수십억 달러의 재산 피해와 소송이 뒤따름, 실수를 반복하지 않는다며 학습을 중단하면 오히려 장기적으로 회사 이익에 손실, 근로자들이 안전하지 않다고 믿으면 인력 확보도 어려워짐
          + 결국 별다른 돈 절약도, 더 많은 이익도 발생하지 않을 것 같다는 생각, 그저 무비판적으로 과거를 미화하고, 산업 성장의 원인을 규제 부재에 착각하는 현상의 산물, 실제로 그때는 인구 급증, 전쟁, 산업 혁신이라는 큰 흐름이 있었고, 지금은 이미 인프라가 충분히 갖춰지고 인구도 정체 내지는 감소, 규제 완화만으로 같은 성장 얻기 어려움
     * CSB 공식 발표 내용 인용, 대통령 예산안이 2026 회계연도 예산부터 CSB 예산을 0으로 책정, 2025년부터 해체 준비, 해체 비용은 긴급 기금에서 충당 예정, 구체적 해체 비용은 의회 및 OMB와 논의 후 확정, PDF 원문
          + 이전에 CFPB(Consumer Finance Protection Bureau)도 비슷하게 예산 삭감 시도를 했으나 상원 의사규칙 해석으로 저지된 적 있음, 관련 기사 링크
          + 이 결정이 EPA(환경청)와 OSHA(산업안전보건청)가 이미 같은 업무를 하고 있다는 주장에 근거했다는데, 그게 사실인지 궁금
     * CSB 유튜브 채널은 흥미, 공포, 지루함이 다 공존하는 곳, 유튜브 채널 링크
          + 기술자가 아니어도 누구나 볼 가치가 있음, 영상 퀄리티도 최고 수준, 사실 정부 조직은 대체로 비효율적이라 여기지만 CSB만큼은 예외, 뛰어난 화공 및 산업 전문가들이 경력 후반에 이직하는 곳, 경험 많은 베테랑만이 현장에서 유의미한 분석 진행 가능, 젊은 엔지니어만으로는 사건 조사 어려움, CSB는 매우 객관적이면서도 현장감 넘치는 상세 보고서 제공, 법 집행이나 처벌이 아닌 정보 전달에 집중, 주요 산업 사고의 원인을 규명해 그 결과를 업계 전체에 알려, 엄청난 가치의 리포트, 주요 영상 댓글의 반응도 압도적으로 긍정적, ""정부가 내 최고의 유튜브 영상을 만든다니 믿기 어렵다"", ""이게 바로 세금의 가치"", ""영상 내용을 동료와 꼭 공유해야겠다"", ""영상 덕분에 안전교육할 때 항상 유용하게 활용"", ""구독 알람 켜둔 유일한 미국 정부 기관"",
            ""25년간 일관된 사명감에 축하"", ""이 채널만큼 긍정적 댓글 많은 곳도 드물다"", ""내가 현장관리자로서 영상으로 직접 생명을 구한다는 점에서 큰 보람"", ""CSB 덕분에 산업재해 예방 인사이트 제공, 앞으로도 잘 운영되길"" 등
          + 안전공학 자체가 누군가에겐 지루해 보이지만, 항공기, 잠수함, 화학공장 등 어떤 산업이든 '안전하게' 만드는 일이야말로 극도의 난이도를 자랑함
     * 무규제 자유시장 체제가 결국은 자기 조절이 되지 않는다는 근거를 현실에서 곧 보게 될지도 모름, 규칙 없는 시스템은 피드백 루프와 불안정성에 취약, 과거 시장의 '상승장'만으로 시스템의 상태를 평가할 수 없음, 수요-공급만으로 사회 전체가 장기적으로 조율되리란 증거 없음, '보이지 않는 손'이라는 유명한 아담 스미스의 말도 실제론 보호주의적 행위 등장 맥락, 법과 규제도 자유시장 일부, 규제가 사라지면 경쟁은 곧 전쟁으로 비화
     * '효율성'이라는 명목으로 화학물질 실은 열차를 출발 전에 아예 폭발시키자는 블랙코미디 언급
     * CSB는 처음 들어봤지만, '화학 사고 원인 조사'라는 임무는 꽤 중요해 보임, 2023년 오하이오주 이스트 팔레스타인 열차 탈선사고를 직접 찾아봤으나, CSB 공식 사이트에서 해당 사건 관련 조사를 찾을 수 없었음, FEMA(연방재난관리청)에서 발암 가능성에 대한 우려만 발견, 혹시 다른 사람은 찾았는지 문의
     * 기사에서 중복성(redundancy) 언급이 있었으나, 실제 어느 기관이 CSB와 중복 역할을 한다는 건지 구체적으로 나오지 않음, NIH가 최근 이스트 팔레스타인 화학사고에 대해 별도 조사를 착수한 듯, 관련 링크
     * 밈 언급(도지 밈), 사람들이 도지가 효율성 향상이나 적자 감소를 이끌 거라 했던 주장을 뒤집는 현실을 기다리는 중
"
"https://news.hada.io/topic?id=21612","아마존에서 클라인 병 브랜드 하이재킹 당함 (2021)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     아마존에서 클라인 병 브랜드 하이재킹 당함 (2021)

     * Cliff Stoll의 Amazon Klein bottle 상품 페이지가 외국 판매자에 의해 하이재킹당한 경험 설명
     * Amvoom라는 중국 판매자가 Brand Registry 제도를 활용하여 Stoll의 리뷰와 목록을 자신들의 블랙헤드 제거기 제품으로 이전함
     * 아마존 브랜드 등록은 등록된 상표권만 인정해 피해자 보호가 어려운 구조임
     * 이 과정에서 기존 고객 리뷰 199건이 블랙헤드 제거기에 연결, Klein bottle 판매는 차단됨
     * 피해자는 아마존의 지원 부재와 대응 불가 문제를 지적, Amazon과 Brand Registry 기능 개선 필요성 언급
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 본 내용은 오랜 기간 Klein bottle을 판매해온 Cliff Stoll이 2021년 Amazon에서 브랜드 하이재킹을 당한 사건 관련 설명임
     * 해당 문제로 인해 아마존 판매 중단 결정 및 Amazon의 구조적 문제점 지적 있음

사건 개요 및 흐름

     * Cliff Stoll은 25년간 Klein bottle을 판매했으며, 사업명 ""Acme Klein Bottle""은 정식 상표가 아닌 관습상 상표(common-law trademark)로 운영해왔음
     * 과거 5년간 Amazon에 대형 Klein bottle만을 등록 판매했고, 상품에는 199개 5성 리뷰와 2개 4성 리뷰가 있었음
     * 2021년 5월경 중국 Shenzhen의 판매자 Amvoom이 자신들의 상표를 Amazon에 등록해 Stoll의 리스트를 자신들 브랜드로 재브랜딩함
          + Amazon의 Brand Registry 시스템은 정식 등록 상표만을 인식해서 발생한 문제임
     * Amvoom은 실제로 Klein bottle을 판매하지 않음
          + 대신 블랙헤드 제거기에 ""색상 옵션""을 만들어 기존 Klein bottle의 리뷰와 리스트를 연결, 결과적으로 모든 리뷰가 블랙헤드 제거기 상품에 반영됨
          + 해당 블랙헤드 제거기에는 Klein bottle 사진이 붙은 $75짜리 옵션이 추가됨 (실제 제품이 아님)
     * Amvoom은 수백 건의 주문을 올렸다가 모두 즉시 취소하여 Klein bottle의 Amazon 재고를 고갈시킴
          + 재고가 소진된 상태에서 해당 색상(= Klein bottle) 옵션이 더 이상 노출되지 않게 됨
          + 결과적으로 블랙헤드 제거기에 199건의 긍정 리뷰가 노출되고, Klein bottle은 아예 노출 안 됨
     * Amazon Prime Day가 끝나자 Amvoom은 이 연결을 해제했으나, 여전히 Stoll의 상품 페이지는 Amvoom이 소유하고 있음
     * Stoll은 Amazon측에 정식 항의를 했으나 Brand Registry 지원은 이메일조차 받지 않고, Amazon 판매자 지원도 도움을 주지 않음

추가 내용 및 여파

     * Amvoom 또는 관련자들은 이렇게 획득한 긍정 리뷰를 다른 판매자에게 판매 또는 임대하는 리뷰 조작 전문가로 추정됨
          + 예시로 블랙헤드 제거기 판매자는 Fujian의 TaroRee라는 업체임
     * Stoll은 아마존에 문제를 해결하려 했으나 아무런 조치를 받을 수 없었으며, 현재 Acme Klein Bottle 상표 정식 등록을 추진 중임
     * 결과적으로 본인의 Klein bottle은 Amazon에서 판매할 수 없고, 제품에 관한 문의 및 구매는 본인 소유 홈페이지(www.kleinbottle.com)에서만 가능함

결론 및 요청

     * 이 같은 브랜드 하이재킹 문제는 상표 등록 및 Amazon의 시스템적 결함에서 비롯됨
     * Stoll은 Amazon 내부 담당자와 연결될 경우 의사소통을 부탁했으며, 본 사안의 심각성에 대한 인지 필요성을 강조함

부록 및 추가 메모

     * 6월 26일 추가 메모에는 블랙헤드 제거기 상품 리뷰에 Klein bottle과는 무관한 ""Cooker Grill Heating Element"" 등에 대한 리뷰도 등장함
     * 6월 29일, Hacker News 질의에 대해 Cliff Stoll 본인이 추가 설명을 빠르게 적었음을 알림

        Hacker News 의견

     * Amazon에서 중국 기반 공급업체들이 전문적으로 해킹하여 일어나는 문제들을 완전히 방치하는 상황 경험 공유, 이제는 비타민도 가짜가 많고, 심지어 해로운 제품도 상당함, 소규모 책들도 가짜가 넘치고 품질이 매우 낮음, Walmat로 구매를 옮기려 했는데 25년이 지나도록 변화 없는 모습 확인, 정품 브랜드 로고를 강조해야 한다는 생각조차 없는 듯함, Barnes & Noble을 통해 책 구매로 또 옮겨봤으나, 이들도 지난 25년간 특별한 교훈을 얻지 못한 모습, 웹사이트가 불편하고 배송비 7달러가 드는 데다, 매장 픽업 무료 제공조차 없음, Amazon의 경쟁사들이 이렇게 형편없으니 Amazon이 이렇게 안주할 수밖에 없는 상황 판단
          + https://www.cpsc.gov/에서 리콜 제품 내역 확인 추천, Amazon엔 미국 제품 안전 법률을 위반하는 중국산 불량 제품 수천 개가 올라와 있는 현실, 납 함유 도료가 칠해진 장난감, 아기질식 위험 범퍼 등 다양함, Amazon은 해당 제품을 내려만 버리면 되는 듯 행동하지만 사실상 법을 우회하려는 시도로 보임, 잠재적 위험 제품을 소비자가 쉽게 찾을 수 있다는 점이 심각함
          + 예전에 티셔츠 판매에 도전했던 경험 공유, 내가 멋진 디자인을 100장 제작해 Amazon 판매 계정 개설하려 했으나, 단일 디자인 제품으로는 계정 생성이 안 된다는 사실 알게 됨, 경쟁 제품 조사 시 저작권 침해 디자인(예: 디즈니 캐릭터)이나 실제 티셔츠 사진 대신 포토샵 합성 이미지만 올린 사례 다수 목격, Amazon이 이걸 허용한단 사실이 충격적임
          + 나에게 '형편없는 경쟁사'가 가장 큰 문제임, 일본 내 신뢰할 만한 로컬 매장에서 구매하려 해도 너무 번거로운 현실, Amazon은 빠르고 부드럽게 구매 가능하며, 제품 종류도 방대함, 가격 면에서는 일본 내 매장들이 맞춰주기에 Amazon이 꼭 더 싸진 않음
          + 동네 독립 서점에 찾아가면 거의 모든 책을 주문해줄 수 있음, 별도 배송비 없이 다음 입고 때 받아서 직접 픽업 가능, 온라인 주문 필요하면 https://bookshop.org도 시도 권장
          + 샴푸 하나도 Amazon에서 정품 신뢰 불가, 비타민은 더더욱 불안함, 건강을 걸고 단돈 몇 달러 절약을 위해 이런 위험 감수하는 건 이해 어려움
     * Clifford Stoll을 모르는 독자를 위해 소개, ""The Cuckoo's Egg: Tracking a Spy Through the Maze of Computer Espionage"" 저자이며 90년대 초 프로그래머/해커에겐 필독서였던 책 설명, Amazon 하이재킹 구조를 이 책에서 명확히 이해하게 되었다는 경험, CliffStoll이 Hacker News에 직접 활동한다는 점, 만약 유명인 명으로 가입한다면 HN이 그걸 어떻게 처리할지 궁금증, 아마도 여기선 물건을 파는 곳이 아니라 큰 문제 없을 것이란 추정 공유
          + 90년대 중반, 이탈리아에서 전화비 비싸고 인터넷도 없던 시절, 이 책 한 권을 10번도 넘게 읽었던 추억 공유
          + Clifford Stoll이 다뤘던 NOVA 에피소드도 꼭 볼 만하다고 추천, https://youtu.be/Xe5AE-qYan8, 젊은 세대에겐 Numberphile YouTube 채널 영상으로 더 익숙할 수도 있음
          + 무척 놀랍게도 ""Machine Beauty""라는 책 3장 읽던 중 Clifford Stoll을 언급한 구절을 보고 낯익어서 찾아봤는데, HN에서도 멋진 유리병(클라인 병)을 봤던 기억이 있어 다시 책으로 돌아감, 책 읽다 다시 HN 들어오니 바로 이 주제를 다룬 글이 뜬 우연의 일치, 세상이 참 좁다는 느낌
          + Palm Pilot에서 읽은 책이 바로 그 책이었던 기억, 예전에 klein stein 소장했던 경험도 공유, 관리가 번거롭고 음용엔 비실용적이라 정통적인 klein bottle이 더 추천 가치 있음
          + 예전에 britneyspears라는 이름으로 회원 가입했었으나, 운영자가 화내면서 닉네임을 바꾸라고 했던 기억, 브리트니가 이곳에 있을 리 없다는 상황이 재치 있었다는 개인적 유머 공유
     * 기업이 너무 커져서 시장 점유율을 독점하면 고객 요구에 바로 대응하지 않게 되는 것, 이런 상황에선 해당 기업을 수십~수백 개로 분할해야 할 때라고 생각, Amazon의 자산을 잘게 쪼개 다시 배분해야 한다는 의견
     * 이 댓글들을 보면 오히려 소소한 불편을 못 견디는 우리 소비자와 평생에 걸친 소비주의 세뇌가 Amazon이 이렇게 형편없게 굴어도 용인하게 만든 진짜 원인이라고 평가, 삶에 잡다한 일이 많고 귀찮음을 감수하기 어렵기에 신념을 위해 불편을 감수하지 않게 됨, 규제 당국이 제 역할을 하리란 희망도 크지 않으니, 소비자 불매로 아픔을 느끼게 해야 기업이 바뀐다는 주장
          + 이 프레임은 소비자 실패로 보는 것임, 소소한 불편 처리 능력이 없는 게 아니라, 독점 및 반경쟁적 플레이어에 맞설 시간과 에너지가 없어서 이런 결과임, 그들이 더 빠르게 움직일 뿐임
     * ""Acme Klein Bottle Wine Bottle""을 보고 Jaques Carelman의 ""Catalogue d'objets introuvables""를 떠올림, 이 책은 1969년 발간되어 비현실적 기상천외한 오브제 카탈로그로, 19개 언어로 번역되어 많은 사랑을 받음, 캥거루 총, 일회용 플라스터 해머, 그리고 Don Norman의 ""The Design of Everyday Things"" 표지에도 실린 ""마조히스트용 커피포트"" 등 유명한 아이템 다수 포함, 일상 사물 비평의 상징이 된 디자인임, Don Norman 저서와의 연결점도 새로움
     * 이런 문제들이 계속 반복되는 이유는 대기업의 통제불능성임, Cliff만큼 유명하지 않은 사람들이 어떻게 싸워야 하는지 의문
          + 내 Prime 구독이 이번 주 만료 예정, 이런 시스템적 무능에 대한 답은 더 이상 돈을 쓰지 않는 것이라 생각
          + 다른 판매처 찾기 추천, 관심 있는 모든 것마다 실제 리뷰와 전문 지식이 있는 소규모 샵 발견 가능, 발품이 들긴 해도 제품에 대해 잘 아는 점 덕분에 가치 있다고 판단
          + 위반마다 스몰 클레임 법원에 고소 진행 제안
     * 내 생각엔 stockx 같이 박스 개봉-검증 후 배송하는 플랫폼이 Amazon을 결국 대체하게 될 거라 예측, 전혀 stockx와 이해관계 없음, 과거 Amazon의 성공은 신뢰 기반, 예전에는 주문 후 2주가 지나도 도착하지 않으면 바로 다시 보내주고, 두 개 다 받았으면 한 권은 친구 줘도 된다는 식, 이런 경험은 더 많은 돈을 내더라도 Hassle-Free 구매 신뢰를 주었고, 그래서 단골이 됐다 판단, 지금은 완전히 신뢰를 잃었고 ebay 수준이 됐음, 원하는 책 대신 돌멩이 박스 받을 수도 있는 지경, 사람들은 신뢰와 귀찮음 없는 경험에 돈을 더 기꺼이 내려고 함, 품질 낮은 사본을 받으면 신뢰 무너지고, 추가비용을 지급하더라도 사기 가능성 없는 사이트를 선택하고 싶다 의견, Amazon이 신뢰로 일궈온 지위, 이 중요한 사실에 무신경한 것에 충격
          + 이제는 신뢰 자체가 완전히 사라진 상황, 예약구매 상품은 오히려 마지막에 받을 수 있고 당일구매자가 더 빨리 받을 수 있는 현상, 심지어 몇 달씩 배송 지연되는 경험, 고객센터가 환불을 약속만 하고 후임 상담원은 그런 약속 없었다는 식으로 처리, 결국 여러 달 국가기관에 신고까지 해야 겨우 2000유로짜리 주문의 150유로 환불을 받는 정도, 이런 사례 요약
          + trust 면에선 오히려 ebay에서 구입하는 게 Amazon보다 낫다고 실제 경험 밝힘
          + 오랜 기간 eBay, Amazon에서 수천 건 거래(판매자 경험도 100건 이상)한 결과, Hacker News 토론처럼 엄청난 사기를 당한 적 없음, 실제로는 드문 문제에도 두 회사 모두 잘 처리해줬고 대부분의 거래는 아주 괜찮았음, 여기 HN 댓글에서의 Amazon 평가는 과도한 과장에 불과하다고 생각, 물론 문제는 존재하고 일부는 운이 안 좋은 것도 알지만, 정말로 제품 대신 돌을 받을 확률이 더 높다 믿는 건 말이 안 됨, Amazon이 언제든 환불을 해주는데 굳이 비싼 인증/검증을 바라는 것도 납득 어려움, 과장된 논쟁 평가
          + 나 같은 경우 Amazon에서 50달러 이상 물건은 사지 않음, Amazon이 저가 정책이었던 시절은 이미 끝남, 사료 구매하려다 chewy가 Amazon의 절반 가격이라 chewy에서 구매, 앞으로 이 분야는 Amazon에서 안 볼 것, 제조사 홈페이지에서 직접 주문하는 경우가 많은데 가격 차이도 거의 없고, 가짜나 병행수입 걱정도 없음
     * 이런 이유들 때문에 많은 사람들이 '규제감독은 어디 갔나?'라고 질문하는 것임, 이 정도 시장 점유율 가진 기업들은 본질적으로 규제의 적용을 받지 않는 듯한 실정, 허위/오해 소지 영업이 만연하며 사실상 과점이며, Sherman Act(독점금지법) 적용 대상이어야 한다고 생각, Google, Apple 등 계정 복구에 관한 문제도 유사하게 지적 가능
     * Cliff는 정말 멋진 인물, 그가 보여주는 정성과 배려는 대다수 Etsy 셀러와 견줄 만함, 나도 Klein mug를 가지고 있고, 여러 개를 선물한 적 있음
     * (2021)
          + 과거 토론 링크: https://news.ycombinator.com/item?id=27684807
"
"https://news.hada.io/topic?id=21651","iyO One - 세계 최초의 보이스 AI 오디오 컴퓨터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    iyO One - 세계 최초의 보이스 AI 오디오 컴퓨터

     * 스마트폰처럼 앱을 실행하지만, ""스크린 없이 자연어 음성 인터페이스로 조작""하는 컴퓨터
     * 사람처럼 자연어를 알아듣고 사전 정의된 스크립트가 아닌 의미있는 반응을 함
     * 양쪽 귀에 착용하고, 시선을 빼앗기지 않고, 손을 쓰지 않아도 하루 종일 사용 가능한 All-Day 웨어러블 기기
     * 정밀한 공간 렌더링, 지능형 필터링, 그리고 맞춤형 팁을 통해 주변 소리를 더욱 생생하게 전달
          + 시끄러운 방에서 목소리를 분리, 스튜디오급 음악 제공 등 사용자의 청각적 요구에 맞춰 조정됨
     * 하드웨어
          + 4nm ARM Cortex A53 + M55 co-processor
          + AUDIO : Triple-driver system + active DSP crossover
          + Wi-Fi, LTE (model dependent), Bluetooth 5.4
          + IP57 방수
          + 마이크: 귀 한쪽당 10개씩 총 20개, bone conduction + beamforming
          + 메모레: 32GB eMMC
          + 트래킹: Dual 9-axis IMU
          + 배터리: 귀 한쪽당 295 mAh (일상 사용 10~16시간)
          + 티타늄 + 고릴라 글래스. 3가지 색상: Night, Dusk, Dawn
          + 최적의 착용감과 고정력, 미적 감각을 위해 모든 기기가 커스텀 핏 방식으로만 제공됨
     * 주요 앱 및 기능
          + Babl: 16개 빔포밍 마이크로 받은 방향성 음성을 클라우드로 전송해 실시간 번역 후 사용자가 원하는 언어로 즉시 재생
          + Owl: 소음 속에서 원하는 소리(목소리 등)만 증폭해 들려주는 포커싱 청취 보조 앱
          + Sherlock: 음성 검색으로 실시간 정보(뉴스, 날씨, 지식 등)를 제공, 스마트폰 없이도 사용 가능
          + Auditory: 자연어 기반 음성 UI 컨트롤러로, 사용자의 화법에 맞게 기능 제어 가능
          + Deej: AI를 활용한 뮤직 DJ로, Spotify 계정 연동해 대화만으로 음악 검색·재생·플레이리스트 관리 가능
     * 자주 묻는 질문(FAQ)
          + 배터리 수명: Companion 모드 기준 16시간 이상, 일반 모드 10시간 이상(모드/사용패턴 따라 상이)
          + 청취 보조: 빔포밍 기술과 20개 마이크로 정면 소리만 최대 15dB까지 증폭, Owl 앱 기본 탑재
          + 커스텀 핏이 아닌 일반형 제공 여부: 현재는 커스텀 핏만 제공, 일반형 옵션은 향후 모델에서 도입 예정
          + 한쪽 귀만 착용 가능?: 한 쪽만 착용 가능하나, 많은 앱 기능이 양쪽 동시 착용 시 최적화되어 있음
     * CEO의 TED 발표 영상 : Welcome to the World of Audio Computers | Jason Rugolo
     * $99로 예약금 걸고 프리오더 가능. 전체 가격은 $1198
     * 구글의 X Development (문샷 팩토리) 출신(2021년 졸업)

   TED 영상을 보면 더 이해가 잘 됩니다.
   저게 2024년 영상이라, 지금 LLM이 더 똘똘해진 시점에서는 크게 임팩트가 없긴 합니다만..
   ""Mixed Audio Reality"" 개념이나 Virtual Audio Space 에서 여러개의 앱들이 동시에 동작하면서 특정 소리만 줄이거나 증폭하고, 실시간 번역하고 하는 것들은 애플이 시리와 아이팟에 넣고 싶어하지 않을까 생각이 드네요.
"
"https://news.hada.io/topic?id=21635","Starship: 모든 셸에서 작고, 빠르고, 커스터마이즈 가능한 프롬프트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Starship: 모든 셸에서 작고, 빠르고, 커스터마이즈 가능한 프롬프트

     * Starship은 다양한 셸 환경에서 사용할 수 있는 경량·고성능·유연성을 갖춘 프롬프트 오픈소스 프로젝트
     * Bash, Zsh, Fish, Powershell, Tcsh 등 대부분의 주요 셸을 모두 지원하는 광범위한 호환성 제공
     * 각 셸별로 초기화 스크립트를 간단히 추가하는 방식으로 설정 및 적용 가능
     * Rust로 작성되어 빠른 속도와 안전성 을 보장하며, 단일 바이너리로 제공됨
     * 모든 사소한 부분까지 커스터마이즈 가능
     * 안드로이드, BSD, Linux, macOS, Windows 등 다양한 플랫폼에서 공통 환경 구성 지원, 생산성 향상 및 사용 편의성 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Starship 오픈소스 프로젝트

     * Starship은 성능과 커스터마이즈를 동시에 지원하는 프롬프트 도구로, 다양한 운영체제와 셸에서 사용할 수 있음
     * 기존의 무거운 프롬프트와 비교해 빠른 반응 속도와 낮은 리소스 사용량이 특징이며, 높은 커스터마이즈 가능성으로 개발 생산성 향상에 도움을 줌

각 셸 환경에서의 설정 방법

     * Bash:
          + ~/.bashrc 파일 끝에 eval ""$(starship init bash)"" 코드 추가
     * Fish:
          + ~/.config/fish/config.fish 파일 끝에 starship init fish | source 추가
     * Zsh:
          + ~/.zshrc 파일 끝에 eval ""$(starship init zsh)"" 코드 추가
     * Powershell:
          + Microsoft.PowerShell_profile.ps1 파일에 Invoke-Expression (&starship init powershell) 추가
          + PowerShell 프로필 파일 위치는 $PROFILE 변수로 확인 가능
     * Ion:
          + ~/.config/ion/initrc 파일에 eval $(starship init ion) 코드 입력
     * Elvish:
          + v0.18 이상 버전만 지원
          + ~/.elvish/rc.elv 파일에 eval (starship init elvish) 코드 추가
     * Tcsh:
          + ~/.tcshrc 파일에 eval \starship init tcsh`` 코드 입력
     * Nushell:
          + v0.96 이상만 지원, 추후 설정 방식 변경 가능성 존재
          + 설정 파일 경로는 $nu.config-path 명령으로 확인
          + 해당 경로에 starship init nu | save -f ... 코드로 적용
     * Xonsh:
          + ~/.xonshrc 파일 끝에 execx($(starship init xonsh)) 코드 입력
     * Cmd (Clink 필요):
          + Clink v1.2.30 이상 필요
          + starship.lua 파일 생성 후 Clink 스크립트 디렉터리에 저장
          + 내부에는 load(io.popen('starship init cmd'):read(""*a""))() 코드 추가

   몇년째 잘쓰고 있는 것이 올라오니 새롭네요 :)
   Zsh 테마로 제공될때부터 사용하던거라...ㅎㅎㅎㅎ

        Hacker News 의견

     * 나는 맥시멀리스트 프롬프트를 좋아하는 유저이고 Starship을 통한 Shell Bling Ubuntu 설치를 개발 머신 구축에 자주 사용함
       그렇지만 이런 스타일이 모두에게 맞는 것은 아님
       가장 밀도 높은 정보만 추가하고 싶다면, 프롬프트가 나타난 시각과 마지막 명령 실행 지속 시간만 표시하는 추천
       이 두 정보만 있으면 무엇이 언제 일어났는지 쉽게 파악할 수 있고, 추후 디버깅이나 기록 관리에 큰 장점
       이런 방식은 Michael W. Lucas의 『Networking for System Administrators』에서 얻은 팁으로, 네트워크 기초를 배우려는 개발자들에게 이 책도 자주 추천
       nerd 포인트를 더 받고 싶으면 UNIX epoch 기준 초 단위로 시간 표시
       시간 델타 계산이 매우 쉬워지는 장점 있음
       Shell Bling Ubuntu repo 참고
          + nushell에서는 이런 정보가 기본적으로 제공됨
            실행 기록(history)에서 명령어의 시작 타임스탬프, 실행 시간, 현재 디렉터리, 종료 상태 등 상세 정보 확인 가능
            명령 실행 간의 시간 차이뿐 아니라 명령 자체의 실행 시간을 직접 보여주기 때문에 매우 편리함
          + 프롬프트 커스터마이징을 한 적은 거의 없음
            emacs를 사용하면 원하는 정보를 에디터에서 이미 다 볼 수 있기 때문
            페어 프로그래밍처럼 남에게 보여야 할 때만 Starship을 셋업하고, 따로 별도의 터미널 앱을 열어둬서 내 세팅을 보여줄 필요가 없음
          + 이전 명령어의 exit code(종료 코드)도 위와 비슷한 이유로 매우 유용한 정보
          + 사람이 읽기 쉬운 형식으로 현재 시각을 보여주는 것도 꽤 도움이 됨
            그리고 이전 명령의 종료 상태가 0이 아니라면(실패 시) 표시해주면 디버깅에 도움이 됨
          + 나에게는 현재 디렉터리 정보 하나만으로 충분
            마지막 커맨드의 성공/실패 상태에 따라 프롬프트 색만 바꿔주면 됨
            추가 정보는 필요할 때 별도로 확인하면 됨
     * Starship 사용자의 나이 분포 데이터가 궁금해진다는 호기심
       본인은 시간이 지나면서 프롬프트 커스터마이징에 큰 관심이 없어짐
       아무리 프롬프트를 정교하게 꾸며도 보여지는 90%의 정보는 90%의 시간 동안 불필요한 정보라는 결론
       오히려 너무 많은 정보가 시각적 노이즈로 느껴져 결국 뇌가 무시하게 되어 그 정보가 있다는 사실조차 잊게 됨
       정말 중요한 정보는 프롬프트로는 한계가 있고, 예를 들어 변경된 Git 브랜치가 있다는 표시만으로는 어떤 파일이 바뀌었는지 알 수 없어서 결국 추가 명령 실행이 필요함
          + 20년 넘게 개발 경력 보유
            프롬프트에 Git 정보가 있는 걸 매우 유용하게 사용 중
            세세한 정보까지 알 수 있는 건 아니지만, 커밋하지 않은 변경이나 잊어버린 stash 등이 있다는 걸 상기시켜줘서 좋음
            Starship은 흥미로워서 바로 설치해봤지만, 도구 버전 표시 같은 부분은 너무 시끄럽게 느껴지고 결국 언인스톨
            커맨드 타이밍이나 성공/실패 상태 같은 옵션은 좋았지만, 복잡한 커스텀 구성을 관리하는 수고에 비해 남는 게 적었음
          + 업계 25년 이상의 시니어 입장에서 화려한 최신 툴은 잘 안 쓰는 편
            예전에는 <pre><code>export PS1=""[\033[1;32m][\t \u@\h \w]\$[\033[0m]""</code></pre>처럼 시간, 내 계정, 접속 호스트, 현재 경로 정도만 표시하는 매우 단순한 PS1 사용
            다른 고도화된 프롬프트도 여러 번 시도했으나 거의 도움이 안 됨
            현재는 Starship을 몇 년째 잘 쓰는 중
            필요한 정보만 최소하게 나오게 커스텀해서 매우 빠르고 쾌적하게 사용
          + 내 프롬프트 커스텀의 가장 유용한 부분 중 하나는 직전 커맨드의 exit status(종료 코드) 표시
            아무런 에러 메시지 없이 명령 실행이 실패하는 경우가 있어서 실패 표시가 있는 게 큰 시그널
            단, 평소에는 노이즈가 되지 않도록 실패했을 때만 표시
            예: <pre><code>» true » false (last command returned 1.)</code></pre>
            신호(signal)로 종료된 경우도 번역해서 표시 ""last command exited on SIGSEGV"" 같이
            반대로 프로그램이 에러 메시지를 내보냈음에도 성공 코드로 종료하는 경우에도 유용
          + 수십 년 UNIX 사용 경험의 ""very senior"" 유저로서 Starship minimal mode가 깔끔해서 선호
            과거엔 수년간 여러 zsh 세팅으로 고생했지만, 지금은 hassle이 거의 없음
            만약 Starship 유저들이 다 자바스크립트에서 emoji 남발하는 신세대일 거라 예상했다면 나같은 사람도 있음을 밝힘
          + 더 넓게 보면 전체 컴퓨팅 환경에 적용되는 현상
            어릴 때는 Gentoo로 직접 OS 만들고, CPU 최적화 플래그, 창 관리자, bashrc alias와 함수, 프롬프트까지 집착해서 세팅하는 것에 몰두
            이런 최적화 작업 자체가 성장 과정에서 꽤 도움이 되는 경험
            목공에 비유하자면, 초보 때는 툴이나 잔재주를 만들고 다듬는 데 대부분의 시간을 보내지만, 어느 순간 실전 작업 위주로 집중 전환
            지금도 리눅스를 좋아하지만, 바쁜 현실에서 효율과 완성도를 따지기보단 “일”을 우선시하기 때문에 그냥 Debian과 KDE로 기본 환경 사용
     * 불필요한 장식이나 정보의 과잉을 싫어해서 미니멀한 터미널 환경 선호
       하지만 Starship은 필요에 따라 맥락을 잘 보여주고, 세밀하게 커스텀 가능
       기본 프롬프트는 현재 디렉터리, 시각, 그리고 % 하나만 표시
       특정 환경 변수(KUBECONFIG, OS_CLOUD 등)가 설정되면 적절히 해당 정보도 포함
       Go, Python 등 언어 버전도 사용 맥락에 따라 자동 표시
       Starship은 이런 세팅을 정말 손쉽게 가능하게 해주고, 복잡한 zsh 플러그인 구성 없이 최소한의 오버헤드로 그대로 동작
       특히 evalcache와 함께 사용하면 초기화 속도도 아주 빠름
     * Starship 팬으로서 몇 가지 코멘트
       Rust로 안전하고 빠르게 개발된 점과 빌드된 바이너리라 퍼포먼스가 파이썬 기반 powerline, 쉘 스크립트 기반 ohmybash, zshell 기반 ohmyzsh, spaceship보다 훨씬 좋음
       zsh, bash, sh, fish 지원은 당연하고, MS Windows CMD, Powershell까지 지원
       단일 config 파일로 모든 시스템 프롬프트를 관리할 수 있는 것은 거의 유일
       정보가 너무 많으면 간단히 바꿀 수 있고, 아이콘도 비활성화 가능
       약 100개의 모듈로 커스터마이징의 한계가 거의 없음
     * Starship을 “minimal”하다고 마케팅하는 이유를 이해하지 못함
       실제로는 매우 다양한 기능이 많고, 실제 사용하는 것도 보면 거대한 프롬프트에 온갖 장식이 붙어 있음
       나는 다음처럼 아주 간단하게 사용
       <pre><code>: ▶</code></pre>
       진짜 미니멀을 원한다면 이런 커스터마이징 프레임워크가 꼭 필요하지 않음
          + 다른 쉘과 프롬프트에 비해 Starship의 구성 파일은 복잡도가 올라가도 꽤 직관적이라는 장점이 있음
          + 모든 기능을 비활성화할 수 있음
            나는 지금 이런 느낌의 미니멀 구성을 사용 중
            <pre><code>format = """""" $username\ $hostname\ $shlvl\ $directory\ $git_branch\ $git_commit\ $git_state\ $git_metrics\ $git_status\ $package\ $python\ $rust\ $env_var\ $custom\ $cmd_duration\ $jobs\ $time\ $status\ $shell\ $character""""""</code></pre>
          + Starship은 결국 미니멀하게 쓰는 것도 가능하지만, 본질적으로는 최대한 많은 정보와 내용을 담을 수 있는 맥시멀리스트 프롬프트
            이 점을 인정하면 좋을 것 같음
          + 나는 이보다 더 얇은 화살표 사용
            <pre><code>PROMPT='%{%F{red}%}%~ %{%F{yellow}%}% › %{%F{reset_color}%}%'</code></pre>
            깨끗하고, 심플하면서, 미니멀 구현
     * 커스터마이즈 가능함과 맥시멀리즘을 혼동하는 반응들이 놀라움
       기본값은 좀 과하지만, 원하는 만큼 줄일 수 있음
       나는 여러 AWS 환경, 다양한 런타임에서 일하는데 프롬프트의 컨텍스트 정보가 실제로 큰 도움이 됨
       개인적으로는 언제나 Starship + Nushell 조합을 오랫동안 써오고 있음
     * 한 번만 설치하면 더 이상 손댈 필요 없어 좋아함
       나는 쉘이 node 20인지 22인지, rust가 stable인지 nightly인지 같은 정보가 바로 보이길 원하는데
       이런 것들을 추가 작업 없이 바로 보여주니까 만족
     * Starship과 무관하게 zsh 프롬프트에서 Enter를 누를 때 커서가 순간적으로 줄 맨 앞쪽에 움직이며 “플래시”가 일어나는 현상이 불편
       ultra-fast 프롬프트일 때는 덜하지만, 프롬프트에서 뭔가 조금이라도 동작하면 이 현상이 매우 잘 보임
       여러 터미널(gnome-terminal, wezterm, kitty, alacritty, xterm)에서 동일하게 관찰됨
       유일하게 urxvt 터미널에서는 이 문제가 발생하지 않음
       영상 재현 참고
       이런 플래시 현상의 원인 및 회피법이 궁금
     * git status 등 쓸모없는 정보를 100ms마다 프롬프트가 렌더링할 때마다 확인하게 되면, 보이지 않는 생산성 저하로 이어짐
       터미널은 반응형 메모리 도구여야 하고, 불필요한 장식이 되는 것은 피해야 함
       코드 실행 속도엔 신경쓰면서 내 타이핑 지연(latency)에 관해선 관대해지는 것이 문제
          + Starship은 정말 빠름
            필요한 데이터 수집에 단 몇 ms밖에 안 걸리고, 어떤 정보를 추출할지 쉽게 제어 가능
            그동안 써본 다른 툴에서는 길게 느껴지는 지연 때문에 항상 불편했지만, Starship 사용할 때는 체감 차이가 확연함
          + 사람이 느끼는 100ms와 CPU 최적화의 100ms는 완전히 다름
            내 생각에, 프롬프트가 git 브랜치나 상태 표시하는 데 100ms 걸린다고 해서 “흐름(flow)”이 깨지는 것과, 내가 직접 명령을 입력하는 데 더 오래 걸리는 것 중 뭘 더 최적화해야 하는지 생각하게 됨
            적정한 편의 기능에 소요되는 수 ms는 충분히 감수할 만한 가치
            결국엔 편의와 미니멀리즘, 둘 사이에서 균형점 찾기
            극단적인 미니멀리즘이나 과한 장식 모두 비효율로 이어질 수 있음
          + 이런 지연이 거슬려서, kitty 터미널을 패치해 Starship 프롬프트를 vim이나 emacs처럼 하단 status bar(모델라인)에 옮겨 사용
            모델라인은 비동기적으로 업데이트되어 프롬프트 반응이 매우 빠름
            downside는 kitty를 직접 패치해야 하고, 내 개인 리눅스 환경 외에 테스트하지 못함
            관련 패치 프로젝트 참고
          + 프롬프트 도구가 TUI처럼 프롬프트 출력을 완전히 반환한 이후에도 프롬프트 영역을 비동기적으로 수정하는 방식(예: kubectl, git, aws cli가 나중에 200ms 지나고 정보 추가 표시)이 가능할지 궁금
            이렇게 하면 사용자는 대기 없이 바로 다음 명령을 칠 수 있고, 부가 정보는 나중에 자연스럽게 표시 가능
          + 코드 실행 최적화보다 오히려 사용하는 계층(layer)이 많아지면서 실제로는 프롬프트의 입력 지연 최적화가 더 어려워지는 상황이 아닐까 생각
     * 공식 웹사이트에 갔을 때 왜 Starship을 써야 하는지에 대한 명확한 설명이 부족하다고 느낌
       최근 내 프롬프트 커스텀은 아래의 정보를 한눈에 보여줌
       <pre><code>- 마지막 명령 결과(색상: 초록, 빨강, 보라)

     * user@host:currentDirectory
     * (repo 내면) 현재 브랜치 및 git status 요약, 백그라운드 잡</code></pre>
       마지막 명령이 성공하면 초록, 실패하면 빨강, 중단된 경우는 보라로 표시
       내가 그들의 타깃 유저일 수 있는데, 홈페이지에서는 ""왜"" 써야 하는지, 어떤 개선점이 있는지를 명확히 전달받지 못함
"
"https://news.hada.io/topic?id=21634","장난감 소프트웨어를 만드는 즐거움","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           장난감 소프트웨어를 만드는 즐거움

     * Toy 소프트웨어 제작은 소프트웨어 개발의 본질적인 즐거움과 창의성을 회복하는 방법임
     * AI와 산업화로 소프트웨어 개발의 순수한 기쁨이 사라지는 시대에, 개인 프로젝트로 간단한 장난감 프로그램을 만들면 실무에서 유용한 지식과 깊은 이해를 얻는 계기가 됨
     * 장난감 소프트웨어는 80:20 법칙에 따라 최소 코드로 최대 기능을 구현하며, 과도한 설계나 완성도 집착을 피하는 것이 핵심임
     * LLM 등 AI 도구에 의존하지 않고 스스로 부딪히며 만드는 경험 자체가 학습과 성장의 본질적인 기쁨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

왜 장난감 프로그램을 더 많이 만들어야 하는가

     * Richard Feynman의 명언 “내가 만들지 못하는 것은 내가 이해하지 못하는 것이다 → 직접 무언가를 만드는 경험이 깊은 이해로 이어짐
     * 기존에 ‘바퀴를 재발명하지 말라’는 조언과 달리, 바퀴를 직접 만들어보는 경험이 독서나 이론적 학습보다 더 많은 것을 가르쳐 줌
     * 최근 AI와 소프트웨어 산업화가 개발의 즐거움과 장인정신을 위협하고 있음
     * 장난감 소프트웨어 제작이 다시 컴퓨터에 빠져들게 하는 간단한 즐거움을 회복시켜줌

장난감 프로그램의 원칙: Keep it simple

     * 장난감 소프트웨어는 80:20 원칙을 따름: 20%의 노력으로 80%의 기능 달성
     * 목표는 최종 제품이 아닌, 간단함과 주요 원리를 직접 구현하는 데 있음
     * 과한 구조설계(over-engineering)를 경계하고, 꼭 필요한 코드만 작성하는 방식 강조
     * 모든 코드 경로를 아직 구현하지 않은 상태로 두고, 필요할 때마다 구현을 늘려 나가는 식의 접근 권장
     * 작아 보이는 시스템도 실제로 해보면 의외로 쉽게 만들 수 있음을 경험하게 됨

장난감 소프트웨어의 부가적 이점

     * 장난감 프로젝트에서 얻은 지식이 실제 업무에서 문제 추적, 버그 해결, 실수 예방에 기대 이상으로 도움이 되는 경우가 많음
     * 직접 부딪혀 가며 제약 조건을 경험하는 것이 소프트웨어 본질에 대한 통찰을 주며, 혁신적 해결 방안도 발견할 수 있음

예시: 다양한 장난감 소프트웨어 프로젝트 리스트

   지난 15년간 직접 구현해본 장난감 프로젝트 종류들을 난이도와 예상 소요 시간으로 정리함. 각 프로젝트에 간단한 설명과 추가 참고 자료가 포함
     * Regex 엔진 (난이도 4/10, 5일) : POSIX 스타일 정규식 해석 및 일치하는 문자열 식별, 정규식의 내부 작동을 깊이 이해하게 됨
     * x86 OS 커널 (난이도 7/10, 2개월) : CLI와 간단한 드라이버, 메모리 매니저 등 포함, 인메모리 파일시스템, ELF 실행파일, GUI, 프로세스 격리 등 추가 도전 가능
     * GameBoy/NES 에뮬레이터 (난이도 6/10, 3주) : 간단한 명령어 집합과 하드웨어에 대한 이해, PPU와 PSG, 특이 카트리지 포맷 구현
     * GameBoy Advance 게임 (난이도 3/10, 2주) : 스프라이트 기반 간단 게임, GBA 개발 커뮤니티 활발, 혼자서 충분히 파악 가능한 시스템 구조
     * 2D 물리 엔진 (난이도 5/10, 1주) : 기본 뉴턴 역학과 간단한 충돌 처리, 복잡한 도형·관성·해결 알고리듬, 소프트 바디/복합 상호작용 등으로 확장 가능
     * 동적 인터프리터 (난이도 4/10, 1~2주) : 트리 워킹 인터프리터, 자신만의 언어 제작이 기술적·창의적 기쁨을 줌
     * C계열 컴파일러 (난이도 8/10, 3개월) : 단순 타입 시스템 및 타겟 환경, 각종 최적화 및 다양한 백엔드 지원 아키텍처 설계
     * 텍스트 에디터 (난이도 5/10, 2~4주) : 간단한 파일 입출력부터 시작, UI 툴킷(QT/GTK 등) 이용 가능, 콘솔 기반 선호, unicode, syntax 하이라이트, 멀티 버퍼, LSP 등 추가 기능
     * Async 런타임 (난이도 6/10, 1주) : Rust의 경우 impl Future 태스크 처리 및 동시성 구현, I/O 웨이킹 추가
     * Hash Map (난이도 4/10, 3~5일) : 내부 작동 원리, 클로즈 및 오픈 어드레싱, 로빈 후드 규칙 학습, 성능 및 적합한 사용 시점 이해
     * Rasteriser / Texture Mapper (난이도 6/10, 2주) : 3D 그래픽 파이프라인 구조 학습, 벡터 수학, Z버퍼, 텍스처 매핑 및 섀딩 알고리듬까지 심화 가능
     * Signed Distance Field 렌더링 (난이도 5/10, 3일) : 수학적 공간 표현, 간단한 비주얼라이제이션, 셰이더와 벡터 수학에 대한 이해
     * Voxel 엔진 (난이도 5/10, 2주) : 3D 그래픽 또는 게임 개발 경험 있으면 이해 쉬움, 텍스처, 프로시저 생성, 네트워크 등 추가 도전 가능
     * Threaded Virtual Machine (난이도 6/10, 1주) : 빠른 해석기, 아키텍처별 코드생성 없이 최적화된 인터프리터 구현, 컴파일러와 맞먹는 성능 경험 가능
     * GUI 툴킷 (난이도 6/10, 2~3주) : 기본 UI 도구작성 경험 후, 레이아웃 엔진, 텍스트 셰이핑, 접근성 등 심화 기능 직접 구현 가능
     * 궤도 역학 시뮬레이터 (난이도 6/10, 1주) : 뉴턴 중력 모델 간단히 구현, 여러 천체의 상호작용, 적분 알고리듬과 시각화로 확장 가능, NASA 데이터 적용 등 가능
     * Bitwise Challenge (난이도 3/10, 2~3일) : 64비트 상태만으로 재현되는 게임, 창의적 상태관리 훈련, GitHub에서 자세한 규칙 확인 가능
     * ECS 프레임워크 (난이도 4/10, 1~2주) : Entity-Component-System 구조 직접 구현, 언어 타입 시스템 통합, 고성능 및 제약 맞춤
     * CHIP-8 에뮬레이터 (난이도 3/10, 3~6일) : 1970년대의 단순 가상머신, 빠른 구현, 다양한 팬 게임 관찰·실행 가능
     * 체스 엔진 (난이도 5/10, 2~5일) : 규칙부터 시작해 점차 발전, 직접 만든 엔진에 패배하는 경험은 개발자 성장의 한 장면
     * POSIX Shell (난이도 4/10, 3~5일) : POSIX 기반 셸의 원리 및 한계, 실제 Shell 언어 호환성 구현을 통한 깊은 이해 및 수많은 트릭 체험

LLM 등 도구 사용에 대한 조언

     * LLM 등 첨단 도구도 유용하지만, 진정한 배움은 스스로 직접 탐구할 때 더 깊어짐
     * 기존 솔루션을 보기보다는, 미지의 영역을 탐구하며 나만의 해답을 찾는 과정에서 더 깊은 성취감을 얻을 수 있음
     * 장난감 프로젝트를 LLM 없이 진행할 때 초반에는 익숙하지 않아 힘들 수도 있으나, 시간이 지날수록 고유의 기술적 기쁨과 높은 성취감을 느끼게 됨

     * 자동차로 이동하면 주자(달리기)로서의 ‘러너스 하이’는 느낄 수 없음 → 지름길이 아닌 직접 경험에서 깊은 즐거움을 얻을수 잇음

   LLM 없이 해보라는게 공감되네요. 빠른 개발이 필요한게 아니면 직접 하나하나 이해해가면서 만드는게 더 재밋고 보람찬 것 같습니다.

   토이프로젝트를 말하는거였군요. 제목만 보고 장난감에 들어가는 소프트웨어를 만드는거라고 생각했네요. ㅎㅎ

        Hacker News 의견

     * LLM을 검색 엔진처럼 사용하는 사람인지 궁금함. 예전엔 Google에 “pros cons mysql mongodb”처럼 검색하고 공식 문서, 포럼, 블로그, stackoverflow까지 찾아읽으며 시간 소요가 컸음. 하지만 읽으면서 배우는 시간 자체는 언제나 환영. 지금은 LLM에 좀 더 구체적으로 “사진 저장할 때 mysql vs mongodb 장단점, 참고 링크 제공” 식으로 프롬프트를 넣음. 빠르게 핵심을 파악할 수 있고, 링크도 같이 있어서 환각에 의존하지 않아도 되는 점이 좋음. 가끔 “postgres로 사진 메타데이터 저장하는 data schema 짜줘, X를 다른 테이블에 분리하고 싶음” 등 구체적 요청도 하지만, 이건 내가 어떤 결과물이 나와야 할지 정확히 알 때만 사용함. 그저 타이핑 시간이 아까울 때나 타입 구체적 명칭(int와 integer 등)을 잠깐 잊었을 때 쓰는 것임
          + LLM을 기술 질의 엔진으로 쓰면, 언뜻 보기에는 그럴싸하지만 중요한 부분에서 틀린 정보를 줄 때가 많음. 곧이곧대로 믿고 답을 따라가면 쓸데없이 수 시간, 며칠을 허비할 수도 있음. 참고 링크를 요구해도 실제 정보가 있는 경우와 관련 없는 내용을 주는 경우가 반반임. 그래도 한 가지 확실하게 잘 하는 것은 ""tip-of-my-tongue"" 식 역검색임. 즉, 개념을 설명하면 그에 해당하는 검색어를 추천해주는 역할에는 일관성 있고 만족스러움
          + 머지않아 기업들이 많은 돈을 LLM에게 지불해서 자기 제품이 더 좋은 비교로 노출되게 만들 것 같음. LLM은 결과가 '유기적으로' 보이게끔 프레이밍, 강조만 달리할 수 있음. 진실과 근거가 있는 정보만 노출하고 '문맥의 강조'만 할 수 있기 때문임
          + 나도 똑같이 LLM을 검색용으로 쓰고 있음. 느낌이 과거 2010년대 초반 구글링이 막강했던 때로 되돌아간 것 같음. 뭐든 다 찾을 수 있던 시절. 물론 이 시기는 오래가지 못했고, 지금 구글링은 고통과 좌절 그 자체임. 구글과 마케터들이 만든 변화에 대해 불만도 많지만, 현재 LLM들은 순간적으로 온라인 정보 표면화에 매우 효율적이라는 느낌임. 참고 링크들도 대체로 꽤 정확. 결국 예전과 같은 변화의 힘이 작용할 테고, 다시 사라지기 전까지 잠깐의 기회라는 생각
          + 나도 LLM을 검색 엔진처럼 쓰는 사람 중 한 명임. 아직 AI IDE는 사용하지 않음. 최근 라이브 기술 면접에서 내가 선택한 LLM으로 구글링 하듯 질의를 하면서 답하려 했는데, 면접관이 이렇게 AI를 쓰는 지원자를 처음 봤다고 했음. 대부분 개발자들이 아직 AI IDE보다는 검색용으로 먼저 쓰는 줄 알았음. 혹시 우리 같은 사례가 드문가?
          + 심지어 개발에도 LLM을 검색 엔진처럼 사용. “/src/foo에 내가 클론한 레포를 분석해 barFeature가 어떻게 구현되었는지 설명해줘. 이제 /src/baz 프로젝트 봐서 foo 접근법이 baz에 적용하기 어려운 이유를 설명해줘” 식으로 활용. 새로운 걸 시키지는 않고, 기존 프로젝트에서 내 아이디어로 번역해서 활용. 진짜 새롭고 도전적인 개발은 내가 직접 코딩해야 즐거움이 있음
     * 커리어적으로 가장 잘한 일 중 하나는 직장 사이 6개월 쉬면서 진행한 개인 프로젝트임. 원래 시작할 프로젝트가 많았지만 제한이 없다 보니 범위가 계속 커지면서 결국 완성하지 못하는 경우가 많았음. 그래서 각 프로젝트마다 1주일만 투자하기로 결정. 1주일 안에 할 수 있는 만큼만 만들었음. 제로에서 시작해서 새로운 언어나 프레임워크, 혹은 생소한 분야에서 1주일 만에 쓸 만한 뭔가를 만든 경험은 자신감이 엄청나게 쌓이는 계기였음. 원래 프로그래밍을 좋아했던 이유도 다시 상기할 수 있었음. 만약 직장 사이에 몇 달쯤 휴식 시간이 생긴다면, 실리콘밸리 면접 준비 대신 그냥 토이 프로젝트를 만드는 것이 얼마나 많은 걸 이미 알고 있었는지 스스로 놀라게 됨
          + AI 생성 도구가 있다면 이런 개인 토이 프로젝트를 진행할 때 엄청난 도움이 됨. 나는 백엔드를 주로 하지만 프런트도 할 수는 있음. 다만 CSS에 시간이 너무 많이 걸려서 예전에는 개인 프로젝트를 끝까지 못한 경우가 많았음. 지금은 AI에게 “이쁘게 만들어줘”라고 시키면 85%까지 완성된 단계로 만들어주고 나머지 버그 수정이나 수정 작업만 하면 되므로 빠르게 마무리할 수 있음. 예전엔 진흙탕 같았던 개발이 지금은 이렇게 수월해진 덕분에 개인 프로젝트를 더 자주 만들게 됨
          + 최근엔 사용 중인 라이브러리에 불만이 쌓여 직접 고치는 쪽으로 옮겨감. 불완전한 온보딩 문서, 깨진 SDLC, 심각한 성능 문제 등을 발견하면 종일 수정작업을 함. 다른 사람이 기다리고 있는 협업 프로젝트와 달리, 개인 토이 프로젝트는 사이드퀘스트(잡일)에 빠지기 쉬움. 협업은 누군가가 기다리는 압박이 있음
          + 어떻게 6개월이나 쉬고 다음 직장을 구하게 되었는지 궁금함. 나도 6개월 쉬고 싶기는 한데, 혹시 일자리를 못 구해 더 오래 쉬게 될까봐 불안함
          + 어릴 적에는 classic ASP + SQL로 서버 세팅하고 HTML/CSS/JS 다 다루면서 쉽게 배포했었음. 그땐 FTP로 파일만 올리면 됐음. 지금은 더 현대적인 방법을 연구해보고 싶지만, 개인 프로젝트하다 보면 매번 배포와 개발 프로세스(라이프사이클) 고민에서 헤매는 경우가 많음. 토이 프로젝트 호스팅, 배포 방식에 대해 어떻게 선택하는지 궁금함
          + 나도 AI가 보일러플레이트 부분이나 테스트 자동화 코드 등을 생성해주는 덕에 이런 개인 프로젝트 진행 속도가 확실히 빨라짐
     * 토이 소프트웨어 개발은 자전거나 자동차, 보트 정비랑 비슷함. 즐거움. 하지만 출근용 자전거를 고치는 건 스트레스. 토이 소프트웨어 만드는 건 즐거운데, 언젠가 내가 진짜 사용하려고 하면 버그를 다 발견하게 되고 고칠 시간은 없다는 딜레마가 생김
          + 나 스스로 쓸 용도의 소프트웨어만 개발하는 걸 더 좋아함. 자동차도 더 저렴하게 오래 타는 게 만족감임
          + 한때 이메일 서버를 직접 운영했는데, 지금은 안 함. 이건 내가 직접 하고 싶었기 때문이 아니라, 이 일은 전문가에게 맡기고 싶어짐
          + 최근에 직접 인보이스 애플리케이션을 만듦. 필요한 기능을 하나하나 추가하는 게 즐거웠음. 기존 유료 서비스에서 월정액 내고 써야 하는 기능들까지 다 포함시켰지만, 실제로 송장을 발송해야 하는 순간, 내가 만든 앱에서 해결해야 할 이슈(스타일, 주소 입력 등)가 너무 많다는 걸 깨달음. 결국 자전거 타는 재미와 출퇴근용 자전거의 실용성 사이에 균형을 찾아야 할 필요성을 느낌. 시간이 지나면 재미와 실용성이 점점 가까워질 수도 있다는 깨달음
          + 나도 “완성된 소프트웨어”로 만들고 싶은 게 너무 많지만, 시간과 에너지가 없음. 지루하고 반복적인 작업도 매우 많음. 그래도 “AI 결과물”을 관리하고 선별하는 것도 꽤 일임. 아직 초반 실험 단계라 몇 달 후에도 이 의견을 유지할지는 모르겠음
          + 이런 이유 때문에 개인 홈페이지 만드는 게 정말 즐거움. 실제로 내 놀이터처럼 쓸 수 있음
     * LLM에 대해 부정적인 의견이 많은 게 의외임. LLM은 정보를 숟가락으로 떠먹여줌. 새로운 프로젝트를 시작할 때는 당연히 모든 걸 다 알고 시작하는 게 아님. 최선을 다했다가 망하고, 그 다음에 공부하고, 왜 안 됐는지 이해, 새로운 방법으로 조정하는 게 진짜 배움임. 다 안다고 그냥 튜토리얼 따라 하면 방법별 한계나 진짜 장단점을 체험할 수 없음. 예를 들어 정규표현식으로 파서 만드려고 시도했다가 재귀 표현식 못 다루는 걸 스스로 발견하고, 더 복잡한 구조나 시간복잡도 이슈 등도 손에 쥐고 배워야 함. 직접 최적화 컴파일러를 구현하며 온갖 최적화 트릭에서 좌절하다가 실제 컴파일러가 왜 그렇게 설계됐는지 이해하게 됨. 직접 레이아웃 엔진을 짜야 ""width"" 개념조차 제대로 다뤄야 하는 고충도 체험 가능. 시행착오를 통해 배우는 것 이상 좋은
       경험은 없음. LLM이 실수를 막는다고 해서, 중요한 학습 기회까지 놓치지 않았으면 함
          + 많은 사람들이 LLM을 안 쓰면 뒤처진다고 하지만, 오히려 LLM을 적게 쓰는 게 장기적으로 큰 이점이 될 것 같다는 생각
     * 나도 컴퓨터 그래픽스를 배우기 위해 몇 년간 주말과 밤새 이상한 프로젝트들을 셀 수 없이 진행. 한 푼도 벌지는 못했지만 그 덕분에 꿈의 직업을 얻음. 대표적인 작업물 링크:
          + Tiny ray-tracer
          + 2D rigid-body simulator
          + Character animation system
          + Animation curve visualizer
          + Motion capture playback in the web 이런 코드 산더미의 진짜 목적은 모두 “지식 획득”임
     * 이 글이 제시한 “프로그래밍의 즐거움”이라는 정신이 진짜 필요하다고 느낌. AI 에이전트 코딩 시대에 오히려 더 소중. 하지만 저자가 제시한 토이 프로젝트 예상 소요 시간이 너무 짧다고 느낌. 나는 평균 속도도 아니지만, 오히려 해당 리스트 대부분은 2~3시간씩만 일한다고 가정하면 며칠로 끝날 프로젝트가 아니라고 생각. 시작 전에 리서치에만 꽤 걸린다는 느낌. 예시로, 최근 내 Pelican 블로그를 개인 Odin static site generator로 대체했는데, 하루 2~3시간만 투자해도 2주 걸렸음. 저 리스트의 다른 프로젝트들보다 더 쉬운 편이었는데도 그 정도 소요임
          + 네 프로젝트는 “토이” 프로젝트 이상임. 스스로가 진짜 고객이 되어, 배포된 뒤에도 제대로 작동하길 기대함. 그 기대가 토이와 진짜 툴을 나누는 경계임. 한 시간 만에 워드프로세서도 만들 수는 있음. 입력 하나하나만 처리, 미친듯이 단순, 종료 버튼도 없는 버전일지도. 하지만 그것도 “토이” 워드프로세서로는 충분함
          + 만약 “X일”이라는 소요 시간을 24*X시간으로 해석하면 이 리스트가 그럴 듯해짐
          + 토이 프로젝트의 장점 중 하나는 데드라인이 없다는 점임. 그래서 충분히 느긋하게, 천천히 진행하는 게 가능. 나도 PEG 기반의 튜링 완전 언어를 코로나 시절부터 아직도 다듬고 있음
          + 이런 시간 단축에는 어디까지 3rd-party 라이브러리를 쓰고 무엇을 직접 해결했는지, 실제로 얼마나 가장 핵심에만 집중했는지에 따라 큰 차이가 남. 저자의 깃허브(https://github.com/ssloy?tab=repositories)를 보면 규모를 작게 유지하는 팁을 많이 배울 수 있음. 그리고 저자는 문제 영역에 대한 이해도가 원래 깊어서 저 정도로 “타이트”하게 코딩 가능. 새로운 주제라면 이런 식으로 구현하기 어려움
     * “LLM을 이런 프로젝트에 쓰지 마라”는 조언에 나도 동의는 하지만, 너무 극단적으로 받아들이지 말자는 입장임. AI의 도움을 어떻게 받아야 할지에 대한 조언이, 왜 사람에게 도움을 구하는 것과 다르게 느껴지는지도 흥미로운 지점임. 블로그 맨 아래에 “프로그래밍 잘하는 친구가 있다면 절대 도움을 요청하지 마세요”라고 쓴다면 이상할 것임. 전문가 친구는 맥락을 이해하고, 나 스스로 풀도록 도와줌. AI도 정말로 “문제 푸는 걸 대신 해줄 게 아니라, 전문가 친구처럼 지도해달라”고 요청해본 사람 거의 없을 거임. 물론 아직은 못하거나 미흡할 수 있지만, 1~2년 후에는 이런 식 가이드가 아주 자연스러워질 수도 있음. 그러니까 “내가 어떤 도움방식을 원하는지 명확하게 전달”하는 습관을 들이면 좋겠음. 무조건 LLM이 잘못된 정답만 줄 거라는
       고정관념을 가질 필요는 없음
          + AI는 아직 전문가 개발자가 아니라고 명확히 느끼고 있음
          + LLM을 전문가 친구처럼 사용하는 게 내가 가장 많이 쓰는 방식임. 믿음직스럽게 만들려면 프롬프트나 질문 자체를 편파적이지 않게 바꿔서 물어야 함. 최근엔 Claude나 ChatGPT가 너무 내 생각에 동조하는 경향이 짙어진 것 같음
          + (글쓴이) 사실 나도 진짜로는 “전문가 친구에게도 도움을 요청하지 말라”고 추천. 정말 막혔을 때만 주제 관련 간단한 문헌 읽어보는 게 낫다는 쪽. 정말 여러 방법을 스스로 시도해보고 좌충우돌하며 풀어나가는 경험이 창의력, 문제해결력을 키우는 데 필수 요소라고 강하게 믿음. 혼란스러운 시간이 꼭 필요함. 단, 누구든 스스로 결정할 문제임
          + 실제로 LLM을 “선생님”처럼 대하듯 물어봄. 인턴처럼 답해달라는 식의 질의는 안 함
     * Claude 기반 vibe coding 덕에 정말 오랜만에 재미있는 사이드 프로젝트를 시작하게 됨. 툴과 프로세스를 익히고 나니 몇 주 만에 가족용 캘린더/날씨 대시보드, Bluesky 읽기앱(이미 본 글 숨김), 게임화된 회사 PM 대시보드, 일정 시간 뒤 Reddit 글 숨겨주는 Chrome 확장, 유지보수 안 되는 WordPress 플러그인 대체 앱 등 다양한 앱을 빠르게 만들고 있음. 초기엔 Claude에게 UI 개선 등 많은 걸 맡겼지만, 이제는 90% 완성도에도 만족하면서 고도화 대신 새로운 앱 기능에 집중하는 법을 터득함
          + Claude가 버그를 수정해 놓고 결과물을 바로 보여주지 않는 경우가 있어서 곤혹스러움. 한 가지 수정에 업데이트된 결과 출력을 6번이나 재요구해야 했던 적도 있음. 비슷한 경험이 있는지 궁금함
     * 이 리스트는 정말 인상적임. 저자에게 쉽다고 느껴진 프로젝트 상당수가 내겐 확 높은 난이도일 듯. 하지만 실제로 내 토이 프로젝트를 다시 시작하고 싶게 만드는 동기부여 효과는 확실함. 단, LLM 학습에 대한 결론은 좀 더 미묘함. 어떤 식으로 쓰냐에 따라 완전히 다름. 예를 들어 “이 문제를 그냥 구현해줘” 식 요청은 공부에 최악이고, “ELF의 구조를 최상위 추상적으로, ‘왜’에 집중해서 설명해줘”는 오히려 엄청난 학습 촉진제임. 매번 리서치를 직접 안 하게 된다는 부분은 단점일 수 있지만, 정말로 지적으로 고민하는 자세면, 소크라테스식 질문 답변을 언제든 받을 수 있다는 건 대단한 학습 가속 효과임
          + (글쓴이) 그게 바로 내가 본문에서 말한 “특정 유형의 학습”임. 예를 들어 ELF 바이너리 구조를 배우려면 직접 추측만으로 알아낼 수 없음. 역사나 의사결정 과정을 알아야 하므로 스펙이나 문서, LLM도 포함해 참고자료를 써야 함. 근본적으로 “스스로 만들어보는 constructive learning”이 중요한데, 직접 바이너리 포맷을 만들어보며 좌충우돌하는 탐구 과정이 ELF 등 실제 포맷의 이유와 문제, 전체 디자인 스페이스까지 깨닫게 하는 포인트임. 이런 탐구 학습에 LLM 도움을 받지 않는 걸 추천. “노트북, 텍스트 에디터, 컴파일러만 주고 10년간 방치한다면 어디까지 만들 수 있을까? 어디서 구체적 정보를 얻어야 막힘?”
     * 여기 소개된 프로젝트들은 정말 좋은 아이디어인데, 나한테는 그 어떤 것도 흥미롭지가 않음. 이럴 때면 내가 진짜 프로그래밍을 좋아했었나 싶어 의문이 생김
          + 나와 정반대인 듯. 리스트 대부분의 프로젝트가 흥미로웠고, 누군가 시도해보고 싶거나 실제로 해봤던 주제임. 사실 나도 최근까지 이 질문을 던졌고, 아기가 태어나 휴직하면서 정말로 재미로 해보려고 “단순한” 코딩 프로젝트를 시작했음. 모든 의존성을 빼고 전부 처음부터 스스로 만들기로 했더니 원래 생각보다 훨씬 복잡해졌지만, 그래서인지 오히려 그 몇 시간 코딩 시간이 너무 기다려졌음. 갑자기 코딩이 너무 재밌어짐. 아마 본인이 번아웃 된 상태가 아닐지 싶음
"
"https://news.hada.io/topic?id=21598","LaborBerlin: 최첨단 16mm 프로젝터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       LaborBerlin: 최첨단 16mm 프로젝터

     * 아날로그 필름 상영 환경은 낡은 장비와 부품 부족으로 악화되고 있음
     * LaborBerlin 팀이 모듈형, 오픈소스 기반의 16mm 프로젝터 개발을 목표로 진행 중임
     * LED 광원 및 쿨링 시스템 실험을 거쳐 밝기와 발열 문제를 해결했으며, 800W LED와 수냉을 결합했음
     * 다양한 기존 프로젝터 분해·분석 후 Eiki RT 모델을 개조 대상으로 선정함
     * 1세대 프로토타입은 ALUD 페스티벌에서 선보였고, 더 밝고 다양한 기능을 시연했으나 플리커와 기계적 문제도 관찰됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

프로젝트 배경

     * 전 세계 아티스트들이 셀룰로이드 필름으로 작업을 지속하지만, 낡고 수리 어려운 장비로 인해 상영 환경이 불안정해지고 있음
     * 특히 필름 프로젝터의 노후화된 기계 부품이 필름의 파손을 유발하기도 하며, 마지막 16mm 상업용 프로젝터는 1990년대에 생산됐음
     * 예술가와 아카이브, 프로젝셔니스트들은 50~60년대로 거슬러 올라가는 매우 오래된 장비를 사용할 수밖에 없음
     * 프로젝터 제조사 소실, 부품 희귀화 및 서비스 인력 부족이 상황을 심화시키고, 빈티지 프로젝터는 확장된 현대적 영화 예술 수요를 충족하지 못함
     * 디지털 혁명으로 아날로그 필름 상영 경험이 점차 사라지고, 장비 노후가 이를 가속하고 있음

프로젝트 개요

     * 최첨단, 모듈형 16mm 필름 프로젝터를 오픈소스 기술과 범용 예비 부품(3D프린트 가능)만으로 개발하는 것이 목표임
     * 기존 프로젝터의 중앙 기계 요소(크로우 메커니즘, 셔터휠, 필름 이송)는 매우 고도화되어 있어 재설계는 비효율적임
     * 따라서 쉽게 구할 수 있는 기존 프로젝터 기구를 바탕으로 새 프로젝터를 구축하기로 함
     * 전 세계에서 지속적으로 유통되는 프로젝터 렌즈(Eiki, Bauer, Bell & Howell, Hokushin 등)와 호환되는 설계 채택함
     * 예술가, 아카이브, 프로젝셔니스트 모두의 현대적 수요를 반영하는 장비 지향

기술적 특징(희망 목록)

  디자인

     * 모듈형 구조
     * 오픈소스 기술 기반
     * 3D프린트 가능한 범용 부품 사용
     * 높이/틸트 조절, 이동성 고려
     * 수직 포맷 투사 옵션(90° 회전, 프리즘 등)

  파워·광원

     * 고휘도/디밍 조절 가능한 LED 광원
     * 색온도 조정(다양한 프린트 적용)
     * 디지털 셔터(플리커 조정)

  필름 포맷

     * 16mm, Super-16, Ultra-16, 오픈 게이트(마스크 스위치)
     * 프린트/리버설 필름 모두에 안정적 포커스
     * 축소 필름용 스프로킷 교체 옵션

  광학

     * 25~150mm 광범위 줌 렌즈
     * 여러 제조사 렌즈와 호환 어댑터
     * 웜기어 포커스, 아나모픽 렌즈 홀더, Elmo Viewer Type 100용 홀더

  트랜스포트

     * 12~30FPS 크리스탈 싱크, 매뉴얼 변속
     * 셔터휠 FPS 독립 속도, 디지털 프레임카운터, 입출 포인트 메모리
     * 양방향 고속 되감기

  오디오

     * 광학/마그네틱 사운드 출력, 마이크 입력, 헤드폰 잭
     * 통합 디지털 오디오 싱크

  연결성

     * 디지털 오디오, 비디오, MIDI 싱크
     * 다수 프로젝터 상호 싱크, 마스터/슬레이브 전환
     * 원격 조정(IR, 케이블, 블루투스)
     * 텔레시네 준비 옵션

PHASE I 진행상황 (2023년 3월)

     * 2.5년 간 프로젝트를 진행, 2025년 9월 로테르담 ""Back To The Future Festival""에서 프로토타입 공개 계획임
     * 첫 단계로 2인 팀이 4종의 16mm 프로젝터 분해 후, 개발에 적합한 기계 시스템을 조사함
     * 라이트 소스, 필름 이송, 전자 부문 등 3개 개발 영역을 정의하고 외부 전문가 협력 계획
     * 다음 개발 방향 결정:
          + A안: 다양한 프로젝터에 호환형 업그레이드 시스템
          + B안: 한 모델에 특화된 업그레이드 시스템
          + C안: DIY 키트 개발하여 3D프린팅 등으로 누구나 조립 가능하도록 함
     * 방향 결정 후 전자기계 전문가 영입, 온라인 커뮤니티와의 협력, 산업 디자이너와 함께 프로토타입 완성 예정

프로젝터 분해·분석

     * Siemens 2000: 유럽에 흔하고 견고한 구조, Eiki/Bauer 렌즈 호환, 포커스 정확, 이례적 클로우와 바켈라이트 기어 단점
     * Kodak Pageant: 미국에 흔하고 구조 단순, 포커스 기능 부재, Eiki/Bauer 렌즈 적용 불가, 18/24FPS 벨트 교환 필요
     * Hokushin SC-10: 네덜란드·일본에서 구하기 쉬움, 렌즈 호환성 우수, 플라스틱 파트 많고 하우징 공간 협소
     * nac Analysis Projector: B&H 렌즈 호환, 정방향·역방향·스틸 재생, 프레임 카운터, 소음 크고 무거움, 전 세계적으로 희귀
     * Eiki RT2: 전세계 보급 우수, 구조 확장/개조 공간 충분, 단 가격 높고 일부 부품 신뢰성 문제 있음

프로젝트 진척(2024년 2월)

     * 24V 250W 할로겐 전구를 대체할 수 있는 고휘도 LED 소스가 필요해 다양한 Wattage의 LED(200~800W) 테스트함
     * Bell & Howell 16mm 프로젝터에서 렌즈 홀더/게이트 분리, LED를 게이트 근처에 배치해 전압 및 밝기/온도 측정
     * 공랭 쿨러 사용 시 온도가 빨리 상승(60°C, 제조사 권장치 도달)해 광량 제한 있음
     * 이후 수냉식(AIO 워터 쿨링) 쿨러 적용, 고출력에서도 과열 문제 없이 800W LED로 할로겐 대비 2배 밝기 달성

고밀도 LED 테스트(2023년 8~12월)

     * 다양한 전류·전압·온도·조도(Lux) 기준으로 LED별 성능 측정
     * 공랭 테스트: 200W/400W/800W LED 모두 높은 출력을 장시간 유지 시 온도 급상승으로 제약 발생
     * 수냉(AIO) 테스트: 800W, 600W, 400W LED 모두 훨씬 더 높은 밝기(최대 22,000 Lux, 할로겐 2배)와 안정적인 온도 관리 가능

프로젝트 진척(2024년 5월)

     * 800W LED와 쿨러를 실제 작동 프로젝터에 적용해야 함
     * Eiki RT 모델을 개조 대상으로 선정, 내부 공간과 내구성, 개조 용이성, 광범위한 보급성이 이유임
     * 프로젝터의 램프와 모터만 우선 교체하여 기능 개선 확인, 이후 모듈화나 신설계는 추후 과제로 미뤄둠
     * 모듈 설계 및 개조 경험이 풍부한 Jan Kulka(프라하)를 팀에 새롭게 합류시켜 프로토타입 개발
     * 2024년 4월 베를린에서 개발 미팅, 우선 모터 교체와 디지털 플리커 LED 램프 설치에 집중

1세대 프로토타입(800W LED, 수냉식, 가변 FPS, 디지털 셔터)

     * Jan Kulka가 기술 엔지니어링 주도, Eiki RT-2 개조 진행
     * 가변 프레임레이트 0~30FPS에서 16mm 필름 투사 가능하게 설계
     * AiO 수냉과 800W LED를 결합, 오버히트/필름 손상 방지
     * 기계식 셔터 대신 디지털 플리커 방식으로 대체
     * 오픈소스 원칙 아래, Mire(프랑스 Nantes)의 Wandering Device 프로젝트 설계 코드를 기반으로 소프트웨어/모터/버추얼 셔터 시스템 채택
     * 특수 공구 없이도 재현 가능한 구조(주로 알루미늄 맞춤 제작, 3D프린터 미사용)
     * 주요 개조:
          + 오리지널 부품 대거 제거(셔터, 파워, 모터, 팬, 모든 전자기판 등)
          + 새 LED 광원, 모터, 파워서플라이, 컨트롤 보드, 사운드 보드(광학 사운드)
          + 모터: Quicrun Fusion SE 브러시리스 시스템, 네오디뮴 마그넷/자기 인코더 통한 정밀 제어
          + 컨트롤: ESP-Wroom-32로 Mosfet 통해 LED on/off(전자셔터 역할), PWM 신호로 디밍/모터 제어

피드백 루프 – 2024년 10월 ALUD 페스티벌(바르셀로나)

     * ALUD #4 페스티벌에서 1세대 프로토타입 공개 및 실물 비교(기존 250W 할로겐 프로젝터와 동일 필름 상영)
     * 밝기: 프로토타입이 기존 프로젝터 대비 훨씬 밝은 투사 구현
     * 색감: 800W LED는 CRI 70으로 낮지만, 프로젝션에서는 채도와 생동감 충분함
     * 기능성: 가변 트랜스포트 모두 정상 작동
     * 광학계: 현재는 임시 콘덴서 렌즈 사용, 향후 누구나 재현 가능한 표준화 필요
     * 기계적 이슈: 테이프 접합 필름 및 빈티지 롤 이동성 저하, 크로우 미세 조정 불충분, 최종 공개 시 해결 가이드 포함 예정
     * 플리커 문제: 높은 밝기에서 프로젝션 플리커(깜빡임)가 뚜렷하며, 할로겐 프로젝터와 비교해 눈에 띄게 심함
     * 원인 논의:
          + Mosfet 전자 회로 문제 가능성, 오실로스코프로 신호 확인 필요
          + 크로우 모터와 LED 플리커 동기화 문제(운송 중 자석 위치 미세 이동)
          + 밝기가 임계치를 넘어서며 플리커가 더 도드라져 보일 수 있음

결론

     * 오픈소스·모듈형 16mm 프로젝터 개발은 낡은 필름 상영 환경을 현대화하고, 범용성과 개조 용이성을 갖춘 공동 개발 문화 확산에 기여함
     * 고휘도, 변속, 디지털 기능(셔터, 싱크 등) 의 접목으로 예술과 아키이브 수요를 모두 충족하는 새로운 상영 경험 목표
     * 플리커와 기계적 문제 등 여러 과제를 꾸준히 개선하며, 누구나 제작·개조할 수 있는 정보와 기술이 커뮤니티와 공유되고 있음

        Hacker News 의견

     * 35mm 영사기사 출신으로, 학생 시절에는 16mm 영사기와 카메라도 다뤄본 경험 공유. 사람들이 여전히 이 미디어에 관심을 가지고 다양한 문제를 해결하려는 시도에 감명. 디머블 LED와 오픈소스/3D 프린터 부품 사용 제안 등이 인상적. 기존의 핵심 영사기 메커니즘(클로 메커니즘, 셔터 휠, 필름 이송 등)은 이미 충분히 잘 설계돼 있어 굳이 다시 만들 필요 없다는 점이 공감. 하지만 그 뒤에 나열된 방대한 새로운 사양들은 프로젝트를 훨씬 더 복잡하게 만들 것이라고 생각. 16mm/35mm는 점점 사라지는 중이고, 프린트 수 자체도 해마다 필름 노화나 분실, 파손 등으로 줄어듦. 일부 기술적 기능(예: 1~30FPS까지 수동 프레임 속도 조정)은 극히 소수의 마니아층만 필요로 할 틈새 시장이라, 실제로 16mm 프린트를 0.75 FPS로 재생하려는 아티스트가 얼마나 될지 의문. 기능을
       대폭 축소하고 16mm 광학 필름(대부분의 기존 재고 지원)에 충실한, 오픈소스에 기반한 최소한의 프로젝터를 만드는 데 집중하면 현실적이라고 생각. 저비용, 단순 플라스틱 필름 프로젝터도 슈퍼 8 필름용으로 과거에 이미 존재했던 사례 참고(Super 8 필름 소개). 어려운 기능을 추가하려면 더 폭넓은 사람들의 실용성에 초점을 맞출 것을 제안. 예를 들어 상영 전에 프린트 품질을 안전하게 평가할 수 있는 도구나, 30년 동안 지하실에 방치된 릴을 청소할 수 있는 기구 등 추천.
          + 16mm/35mm 포맷이 점점 사라지고 있지만, 35mm로 제작되는 영화는 아직 존재. 미국 인기 도시들에서는 여전히 35mm 선호층이 있음. 예를 들어 2021년 영화 Last Night In Soho도 35mm로 프린트. 실제로는 일부 특별 상영 장소에서만 상영했지만, 70mm IMAX도 극장 수는 적어도 오펜하이머 같은 영화 등장 시 몇 주간 매진 기록. Fort Lee, NJ의 Barrymore Film Center 운영자와의 대화 공유(여기선 16mm, 35mm, 70mm 모두 상영, Fort Lee를 미국 영화산업의 발상지로 홍보). 전국에 남은 스튜디오 필름 보관소는 두 곳 밖에 없지만, 수천 편의 35mm 필름을 주문 가능. 문제는 배송비 부담이 너무 커서 대부분의 영화는 상영하기에 경제성이 부족. 나이 들어가고 있지만, 열정적인 필름 세대 젊은이들도 많이 만남.
     * 20대 때 동네 극장에서 영사기사로 일한 경험, 오래된 기계들을 다루며 느꼈던 만족감과 향수. 모든 극장이 디지털 영사기로 바뀐 건 내가 그만둔 직후의 일. 이런 식으로 미디어를 살리려는 시도를 보면 언제나 멋지다고 생각.
          + 1990년대 학부 시절, 영화 상영 위원회에서 일하며 도서관에서 교내 학생회관까지 16mm 영사기를 직접 옮겨다니던 기억 공유. <i>이유 없는 반항</i>도 상영. 같은 공간에 비디오 영사기도 있었고, VHS 상영권을 얻는 것이 훨씬 저렴했기에 VHS로 상영하기도 했지만, 화질은 훨씬 떨어졌던 추억.
     * 필름 소재가 옛 영사기들보다 색보존력이 더 뛰어났던 점이 신기. 예전 16mm 필름 투사에서의 색이 바랜 원인에 대해 한 번도 생각해본 적 없었고, 단순히 녹음 상태 때문이었다고만 생각해왔음.
          + 기사 하단에서 왼쪽 투사 화면에 분홍빛이 도는 현상은 실제로 필름 프린트가 색이 변한 것으로, 이는 비교적 약한 '식초 증후군(vinegar syndrome)' 때문. 이 증상은 필름 프린트가 오랜 기간 따뜻한 환경(비냉장)에서 보관됐을 때 발생. 색 변화는 주로 개별 프린트 상태에 따른 결과이며, 영사기 자체의 문제는 아님. 하지만 지금까지 실시간으로 색 변화를 보정 가능한 필름 영사기는 없었음(10년간 16mm, 35mm 상영 경험). 이제는 색이 변한 프린트도 포기하지 않고, 원래에 가까운 컨디션에서 영사 혹은 보존/디지털화 가능. 필름 보존계에 있어 이 프로젝트는 매우 중요한 역할, 보존 방식의 큰 변화를 불러올 신호탄으로 느껴짐.
     * 임베디드 엔지니어이자 베를린 예술영화관 문화 속에서 성장. 2년 전에 이 프로젝트를 알았다면 꼭 참여해보고 싶었을 정도로 멋진 시도로 생각.
     * 8mm와 16mm 필름을 많이 소장 중이라, 오픈소스 필름 스캐너로 전환할 수 있는 첫 시도로 기대. 신나는 프로젝트라고 생각.
          + Kinograph 오픈소스 필름 스캐너 소개
          + 8mm 필름 스캐너는 너무 흔해서 Walmart에서도 구입 가능. 유튜브에는 DIY 필름 스캐너 제작법도 다양하게 공개. 스캐너는 빠르게 동작할 필요도 없고 풀다운 메커니즘도 필요 없다 보니 구조가 단순.
     * AOI CPU 쿨러를 영사기나 다른 용도로 쓸 때 주의 필요. 밀폐된 구조라 증발, 공기 유입이 일어날 수 있고, 시간이 지남에 따라 냉각수량 감소. 보충이 어려움. 또한 라디에이터의 방향도 중요. 생기는 기포가 펌프에 빨려 들어가는 걸 피하려면 라디에이터 입출구를 아래쪽에, 기포가 모이는 침착 공간이 위로 가도록 위치 추천. 의견 차이는 있지만 2팬 라디에이터에 800W 이상을 사용하는 것은, 이미 온도가 높은 환경에서는 충분하지 않음. 800W CPU라면 훨씬 강력한 팬이나 라디에이터 두 배 추천.
     * 250W 할로겐 전구의 두 배 밝기를 내기 위해 800W LED가 필요하다는 점이 의아. 보통 LED가 할로겐보다 훨씬 효율이 높기 때문에 이상하다고 느낌.
          + 나도 이상하게 생각. 1kW의 전력이 모두 열로 변환되는 영사기가 해답이 될 수는 없다고 봄. COB LED 어레이가 문제 선택. 이론상 많은 빛을 내지만, 포인트 광원이 아니기에 문제. 극장용 프로젝터에서는 적어도 10년 전만 해도 레이저 광원(스펙클 없는, 아마도 펌프드 포스포?) 또는 고가의 제논 전구 사용. 자동차 헤드라이트에서 UV LED로 인광체를 분리해 포인트 광원 만드는 방식도 한때 봄. 이게 오픈소스에서 어떻게 재현될지는 의문. 스튜디오 LED 라이트 업계에서는 유리 혼합봉을 사용하는 경우도 있음. 여러 LED의 빛을 유리봉에 집어넣으면 균일한 빛이 나오는 아이디어. 다만 이 방법은 밝기보단 연색성(CRI) 개선 목적.
          + 프로젝터에만 있는 광학적 제약이 원인. LED는 전력을 빛으로 바꾸는 효율은 할로겐보다 높지만, 전 방향으로 빛을 내기 때문에 복잡한 집광 시스템이 필요. 이 과정에서 상당량의 빛이 손실. 반면 할로겐은 포물면 리플렉터를 통해 대부분의 빛을 투사로 보낼 수 있음.
          + 800W LED는 완벽한 포인트 광원이 아님. 집광이 불가능해 손실되는 빛이 많음. 비교 사진에서 LED 프로젝터에서 엄청난 측면 빛샘(light bleed) 확인할 수 있음. 오래된 프로젝터는 수십 년간의 최적화로 소켓에서 발생한 빛을 잘 이미지로 모음. 반면 LED 셋업은 이미지에 초점 맞춘 광원으로서는 아직 완전히 새로운 시작.
          + LED 램프 시스템 자체가 오버클러커가 만든 것 같은 느낌.
     * 다양한 프레임 레이트를 지원해야 하는 이유에 대해 궁금.
          + 무성 영화는 프레임 레이트가 일관되지 않았던 시대였음. 카메라 자체가 핸드크랭크였고, 촬영자가 손으로 속도를 조절. 장면 내에서도 일부러 속도를 높였다 낮췄다 하는 것도 흔했음. 초창기 영사기 조작자도 타이밍을 그때그때 결정. 전기 구동 영사기로 넘어가면서 비표준 속도 지원이 어렵게 되어 표준화가 진행. 옛 무성 영화는 현대 영사기에서 항상 더 빠르게 재생되는 경향. 그래서 가변 속도 기능은 복원/보존용 영사기에서는 늘 필수 기능. 필름 보존가들은 직접 개조하거나 지원 가능한 영사기를 선호. 특이한 화면 비율 때문에 스크린 마스크도 직접 만들어 쓰는 경우 많음. 다행히 간단한 줄파일만 있어도 자체 제작 가능.
          + 일부 영화는 여러 이유로 낮은 프레임 레이트로 녹화됨. 가변 속도 및 스탠드스틸(정지 기능)은 기록을 세밀히 분석할 때만 쓰일 듯. 꼭 필요한지는 모르겠지만, 더 많은 사용처를 포괄하려는 의도 추측.
          + 무성영화는 프레임 속도가 다양했음. 나머지는 텔레시네용.
     * 블로그에서는 아카이브용 16mm 프로젝터가 필요하다고 언급. 실제 아카이빙에 16mm 프로젝터를 왜 쓰는지, 그냥 스캔하는 게 더 좋은 것 아니냐는 질문.
          + 스캔해서 필름 화면만 저장하는 게 아니라, 실제 필름 감상은 깜박임과 흔들림, 필름 그레인(입자)이 픽셀과 완전히 다름. 현대 디지털 포맷도 좋아하지만, <i>필름 그 자체를 감상하는 경험</i>을 보존한다는 목표가 중요.
          + 내 경험도 부족하지만 필름에 사운드 트랙이 실렸다면 그걸 추출하는 게 동기일 수 있음. 또 맞는 영사기를 돌리는 것이 타이밍 면에서 가장 간편하고 정확한 방법. 필름 스캔 소프트웨어 찾거나 직접 만드는 것보다 실제 영사기가 시간 관리에는 더 유리. 둘 다 필요할 것으로 추측.
"
"https://news.hada.io/topic?id=21595","Genkit - Google의 오픈소스 AI 풀스택 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Genkit - Google의 오픈소스 AI 풀스택 프레임워크

     * Genkit은 Google Firebase 팀이 직접 개발·관리하는 풀스택 AI 애플리케이션 구축 오픈소스 프레임워크
     * JavaScript/TypeScript, Go, Python SDK를 제공하며, 다양한 언어에서 일관된 API로 AI 기능 개발 가능
     * Google, OpenAI, Anthropic, Ollama 등 다양한 모델을 단일 API로 연동, 수십~수백 개 모델을 비교·탐색·조합 사용 가능
     * 간결한 AI 개발: 구조화된 출력, 툴 콜링, 멀티모달, RAG, 프롬프트 템플릿 등 고도화 기능을 간단한 코드로 구현
     * Next.js, React, Angular, iOS, Android 등 웹·모바일 프레임워크와 손쉽게 연동되는 클라이언트 SDK와 헬퍼 제공
     * 챗봇, 자동화, 추천 시스템 등 생산성 높은 AI 앱을 신속하게 구축할 수 있도록, 멀티모달·구조화 출력·툴콜링·에이전트 워크플로우 등 기능 내장
     * 유연한 배포: Firebase Functions, Cloud Run, 서드파티 플랫폼 등 어디서든 실행·배포 가능, Google 서비스 비의존적으로 활용 가능
     * 개발 생산성 툴: CLI·로컬 UI(Developer UI)로 프롬프트 테스트, 데이터셋 평가, 플로우별 상세 트레이스, 빠른 피드백 루프 제공
     * 운영 모니터링: 대시보드 기반 프로덕션 모니터링으로 품질·성능·요청량·에러율 실시간 추적

주요 활용 시나리오

     * 텍스트/이미지 생성, 타입 세이프 구조화 데이터 출력, 툴 콜링, 프롬프트 템플릿 관리
     * 채팅 인터페이스, 에이전트 기반 워크플로우, RAG(데이터 기반 생성), 멀티모달 입력/출력 등 AI 전용 앱 구현
     * Firebase, Google Cloud 환경뿐 아니라 독립형 또는 외부 플랫폼에도 유연하게 배포 가능

     * Genkit by Example : 예제 앱 보기
     * Firebase Studio 에서 별도 설치 없이 클라우드 환경에서 바로 사용
"
"https://news.hada.io/topic?id=21629","Fairphone 6이 더욱 지속 가능한 새로운 디자인으로 전환됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Fairphone 6이 더욱 지속 가능한 새로운 디자인으로 전환됨

     * Fairphone 6는 모듈형 구조 채택으로 주요 부품의 손쉬운 교체 지원
     * 새로운 디자인에서 평평한 엣지와 네온 테마 전원 버튼이 도입됨
     * 6월 25일 유럽 출시 예정이며, 시작 가격은 549유로로 추정됨
     * 6.31인치 pOLED 120Hz 디스플레이, Snapdragon 7s Gen 3 칩셋, 4415mAh 배터리 탑재 전망
     * 분해와 수리성, 내구성에서 EU 기준 Class ""A"" 인증 획득 가능성 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Fairphone 6의 주요 변화 및 특징

     * Fairphone이 여섯 번째 지속 가능성을 강조한 스마트폰을 출시할 예정임
     * 공식 출시 전 유출된 렌더 이미지를 통해 새로운 디자인과 주요 기능이 공개됨

  새로운 디자인 요소

     * 세 가지 색상(블랙, 화이트, 그린) 으로 제공 예정이며, 이전 모델과 비슷한 색상 구성임
     * 본체의 외관 디자인에서 두드러진 변화가 있음
          + 네온 테마 전원 버튼이 적용되어, 첫인상은 일부 Pixel 시리즈와 유사함
          + 평평한 엣지 디자인이 도입되어 그립감과 스타일 모두 변화함
          + 기존에는 돌출된 카메라 바가 있었으나, 각각 분리된 형태의 카메라 시스템으로 전환됨

  모듈형 구조와 수리성

     * 유출된 마케팅 이미지에 따르면, 카메라 하단의 커버를 나사 두 개만 풀면 제거할 수 있음
          + 나사 제거 시 배터리 교체가 가능함
          + 배터리는 본체에 접착제가 아닌 추가 나사로 고정되는 방식임
     * 이런 구조는 카메라, 디스플레이, 스피커 모듈에도 유사하게 적용되어, 각 부품의 교체 및 수리에 용이함
     * 지속 가능성 실현을 위해 전반적으로 수리성과 모듈화가 최우선으로 설계됨

  사양 및 인증

     * 6월 25일 정식 출시 예정이며, 출고가는 549유로부터 시작함
     * 주요 하드웨어 사양은 다음과 같음
          + 6.31인치 pOLED 120Hz 디스플레이
          + Snapdragon 7s Gen 3 프로세서
          + 8GB RAM 및 256GB 내장 스토리지 (최대 2TB microSD 확장 지원)
          + 4415mAh 배터리와 33W 고속 충전 지원
          + 후면 50MP 메인 카메라, 13MP 보조 카메라, 전면 32MP 셀피 카메라
     * EU 표준의 Class ""A"" 인증을 수리 가능성과 내구성 모두에서 획득할 것으로 안내됨

요약

     * Fairphone 6는 혁신적인 모듈형 설계로 사용자 스스로 부품 교체 및 수리를 쉽게 할 수 있는 점이 가장 두드러진 특징임
     * 디자인 변화와 함께 환경 지속 가능성 실현에 집중하며, 하드웨어 사양도 현 세대 중급 안드로이드폰과 견줄 만함
     * 유럽 시장에서의 기준을 충족시키면서도 수리성과 업그레이드의 용이성까지 갖춰, 기존 제품보다 한층 더 실용적이고 친환경적인 스마트폰임

        Hacker News 의견

     * 모든 폰은 결국 구식이 되지만, 내부 부품을 다양한 용도로 활용 가능성 발견하고 싶음. 예를 들어, 누군가가 멀티포트 도킹 스테이션 역할을 하는 인클로저를 만들어서 언락된 부트로더(대표적으로 Fairphone)인 구형 폰을 다른 OS로 플래싱한 후 미니 PC, 미디어 플레이어, IoT 월터미널 같은 큰 화면의 장치로 재사용하는 아이디어, 굉장히 매력적으로 느껴짐. 완전히 멀쩡한 전자기기가 계획된 노후화 때문에 쓰레기장으로 향하는 상황이 짜증스럽게 느껴짐. 적어도 언락된 폰에 대해 이런 형태의 활용 가능성 추구 희망. Framework가 노트북 메인보드로 이와 비슷한 걸 했는데, 도킹 스테이션 기능은 없었음(노트북엔 폰보다 더 많은 포트가 있으니). Fairphone 하드웨어로도 이런 것이 가능한지 궁금함
     * Pixel 언급되던데, 진심으로 누군가가 다시 후면 지문인식기를 부활시켜주길 바람. 그게 진짜 최고의 솔루션이라는 생각. 주머니에서 폰 꺼내면서 동시에 언락되니까 속도도 최고라서, 지문 인식기가 아무리 느려도 실제로는 가장 빠른 언락 경험임. Face ID보다 왜 사람들이 더 좋아하는지 진짜 모르겠음(현재 나도 Face ID 사용 중). 제발 누군가 다시 후면 지문인식기 복귀시켜주면 좋겠음
          + 지문인식기는 내가 iPhone SE에서 가장 좋아하는 기능 중 하나라 느껴짐. 그래서 새로운 폰을 굳이 살 이유 없음. Apple 소프트웨어 품질 하락과 폐쇄적 정책에 점점 더 아이폰을 버리고 GrapheneOS나 Punkt MP 같은 폰으로 넘어갈까 고민중임. eReader로 독서, 카메라로 사진, 노트북에 필기하는 게 폰보다 훨씬 더 즐겁게 느껴짐. 특히 저널 작성, 몇 년간 계속해오면서 기록 그 자체에서 큰 평온함과 자아 연결감 얻고 있음. 의미 있는 책 구절까지 필사하고, 내 손글씨, 잉크 색, 잉크 쉐이딩 등등이 더 깊은 아날로그적 경험을 만들어줌. 결론적으로 더 많은 폰 사용이 정답이 아니라, 오히려 덜 쓰는 게 정답이라는 생각 듦
          + 어떤 모델에서는 후면 지문인식기로 스크롤이나 알림바 내리기/올리기 등의 입력장치로도 쓸 수 있었음. 디스플레이를 가리지 않고 콘텐츠를 스크롤하거나 화면 전환 제스처로 매우 유용하게 쓸 수 있었다는 점 언급하고 싶음 androidauthority 기사
          + 이 부분은 굉장히 주관적이라고 생각함. 어젯밤 여자친구와 얘기하다 보니, 나는 전면에 지문인식기가 있을 때만 책상 위에 놓인 상태에서도 쉽게 사용할 수 있어서 좋았음. 내 현재 폰은 후면 지문인식기인데, 한 손가락만 등록 가능해서 왼손 사용할 땐 불편함
          + 전원 버튼 내장형 지문인식기도 경험해봤는데, 개인적으로 지금까지 써본 지문인식기 중에서 단연 최고였다고 생각함
          + Demolition Man 초기 장면에서 Wesley Snipes가 생체 인증의 치명적 결함을 이용하는 장면을 본 이후로 항상 꺼려졌음. 동기가 충분한 누군가라면 신체 일부분을 가져가는 게 그리 어렵지 않기에 패스코드나 패턴을 선호함
     * Fairphone과 GrapheneOS의 협업을 오래 기다렸다는 입장. 이 조합은 정말 완벽한 궁합임. 혹시 페어서폰/그래핀OS 개발자가 이 글을 본다면, 보안상 부족하면 문서로 남기더라도 그냥 시도해 주기를 바람. 서로 너무 잘 맞는 파트너임에도 이런저런 망설임 때문에 진정한 'THE PHONE'을 세상에 내놓지 못하면 너무 아쉬울 것 같음. 우리들이 쏟아부을 돈을 가지고, GrapheneOS에서 원하는 CPU를 넣은 2세대폰 만들어준다면 r/GrapheneOS 커뮤니티가 엄청나게 좋아할 것임. 하드웨어 카메라/마이크/네트워크 물리 스위치도 추가되면 금상첨화. 첫 시도부터 완벽할 필요 없고, 두 번째 세 번째에서 완성도를 더 높일 수 있으니 일단 시작이 중요함. Google Pixel만 사서 GrapheneOS 설치하고 Google 생태계에서 도망치는 게 너무 비효율적이라 느껴짐
          + Fairphone은 GrapheneOS 사용하는 데 필요한 요구사항의 1/4 정도만 충족함. 관련 FAQ 참고 grapheneos.org/faq#future-devices. Fairphone이 보안 및 업데이트 정책을 대폭 개선하고 새로운 기능을 다수 통합하지 않는 이상 현실성 없음
          + 나 같은 경우 이런 조합 나오면 꼭 구입 의사 있음. 크기가 작으면 3대도 살 의향 있음
     * ""배터리를 스크류로 고정하고 접착제로 붙이지 않았다""라는 점이 개선점으로 언급되는 현실이 너무 안타깝게 느껴짐. 2016년 전까지만 해도 안드로이드폰 대부분이 [1] 도구 없이 쉽게 교체 가능한 배터리 [2] microSD 슬롯 [3] 헤드폰잭 [4] (여러 모델) 듀얼 SIM [5] (보급형) 루팅 혹은 부트로더 언락 가능성이 높았음. 이제는 빠른 CPU, 늘어난 비확장 저장공간, 그 이상의 수많은 카메라만 제공됨. 관련해 기억에 남는 삼성 광고도 있었음 유튜브 광고
          + Fairphone은 여전히 올바른 방향으로 가는 제조사라고 생각함. 내 FP3에는 위 모든 기능이 있었고, 공식 부트로더 언락 가이드까지 제공함. 업데이트 약속도 지켜줘서, 앞으로 새 제품은 딱히 살 생각 없음
          + 기사 내용이 오해의 소지가 있는데, 예전 Fairphone은 애초에 배터리가 접착제 없이 쉽게 교체 가능했음
          + 폰을 떨어뜨릴 때 배터리와 SIM 카드가 분리되어 날아가는 경험, 그립지 않음
     * 운이 좋게도 Fairphone 4를 세일가에 샀는데, 지금은 정가에 사도 아깝지 않게 느껴짐. 스펙 대비 비싼 건 사실이지만, 견고한 부품 공급망 개선 노력과 자가 수리 가능성(나는 USB-C 포트만 교체해봤는데 아주 쉬웠음)만으로도 충분한 가치 느껴짐. 단 하나 아쉬웠던 점은 Android 14 업데이트가 적용되지 못한다는 부분을 미리 더 잘 소통해주지 않았던 점인데, 그래도 보안패치는 꾸준히 이루어지고 있음. (그리고 헤드폰잭 없앤 것도 아쉽지만, 이미 대세가 된 듯해서 어쩔 수 없음)
          + 이어폰 잭 제거가 너무 명백한 계획적 노후화 전략이라는 점이 아이러니함. 지속가능성을 표방하는 프로젝트임에도 트렌드를 따름. 유선 이어폰이 음질도 더 좋고, 충전도 필요 없으며, 소프트웨어 업데이트에도 무관하게 사용 가능함. 오히려 불편함이 소비를 늘리지 않으므로 일부러 없애는 거라고 생각하게 만듦. 배터리도 일부러 접착제로 고정하는 것도 그러한 노림수이고, 이런 전략이 합법이란 점이 어이없게 느껴짐
          + Fairphone 6에도 여전히 헤드폰잭이 없다니, 거의 다음 폰으로 정할 뻔했는데 아쉬움
          + 헤드폰잭 플러스 마이크로SD 슬롯은 적어도 잘 지켜졌던 요소라 긍정적으로 평가함
          + 최근 삼성 기기(특히 Note 10 Lite)에서도 USB-C 포트 몇 번 교체해봤는데, 어렵지 않았고 심지어 배터리 교체도 손쉽게 가능했음. 기술기자들이 얘기하는 것만큼 어렵지 않다는 점 강조하고 싶음
          + 보안패치 3개월에 한 번 꼴로 제공됨... FP4 산 걸 후회함
     * 내가 꿈꾸는 폰의 요건은 다음과 같음: 사용자 교체 가능한 배터리, 7년 이상의 보안 업데이트, iPhone 13 mini 크기, iPhone 13 mini 카메라 품질, 플라스틱 외관, 헤드폰잭, Fairphone 스타일의 지속가능성/수리용이성/부품 가용성, GrapheneOS 호환성, 후면이나 전원버튼에 위치한 지문인식기. 예전 Pyxel 4a가 그리움
          + 후면 지문인식기나 전원 버튼 내장 등 둘 다 가능하다면 LG G6처럼 전원버튼을 후면에 두고 지문인식 기능까지 넣는 컨셉, 실제로 써본 지문인식기 중 최고였음
     * 요즘 폰이 왜 이렇게 큰지 이해할 수 없음. 실제로 들고 다니기 너무 불편하게 느껴짐. iPhone 13 mini 정도의 작고 콤팩트한 크기에 USB-C 단자가 있는 폰을 간절히 바람. 카메라 범프도 너무 불만임. 차라리 두께를 늘려서 카메라가 본체에 맞게 들어가면 훨씬 좋겠다는 생각
          + Apple이 mini 라인업을 없앤 점, 아직도 아쉬움. 큰 인기 없었다 하지만 ""수십만 대"" 라인도 유지 못할 회사는 Apple뿐인 듯. 결국 ""숫자""만 보는 경영이 불러온 결과라고 생각하게 만듦
     * 이 정도로 작은 화면이면 오히려 단가가 더 높아질 거라 생각함
     * FP4를 3년간 사용했는데, 괜찮게 썼음. 그런데 한번 업데이트 이후부터 통신사 연결이 되지 않아(통화, 인터넷 불가) 서포트와 많은 연락을 한 끝에 해당 국가(그리스)에서 공식적으로 LTE 지원이 안 된다는 걸 인정받음. 결국 환불 조치 받음. 개인적으로 악감정 없고 미션 자체엔 응원함. 내 환경에서만 정상작동했다면 여전히 사용 중이었을 것임
     * 꿈: Framework같은 업그레이드/수리/모듈화, GrapheneOS 지원, 미국 내 판매
          + Fairphone이 GrapheneOS 지원할 가능성은 거의 없다고 생각함. 하드웨어가 GOS의 합리적 보안 요건에 못 미치고(관련 FAQ, 소셜포스트), Fairphone은 개선의사가 없다고 공식적으로 밝힌 상태임. 소프트웨어도 부족하고, 심지어 Murena랑 파트너십 맺고 있는데, Murena는 GOS에 대해 부정적 언급을 해왔음(Murena, 관련글). 하지만 AOSP/Pixel 드라마로 인해 다른 대형 OEM이 지원 대상으로 추가될 가능성이 현실적으로 열려 있다는 점은 긍정적임 관련 소식
          + Murena를 통한 미국 내 Fairphone 판매도 가능함. 직접 Fairphone 4를 구매했고, 미리 eOS가 설치되어 있었지만 calyxOS로 교체해서 쓰고 있음(이는 GrapheneOS와 비슷한 환경 제공). Murena 판매페이지. PostmarketOS, UBPorts 같은 리눅스 배포판들도 지원할 정도로 확장성도 고려된 디바이스임. 하지만 동일한 폼팩터를 계속 유지하고 Framework Computer처럼 모듈형 접근을 했으면 하는 바람도 있음
"
"https://news.hada.io/topic?id=21630","큰 돈을 벌고 싶다면 특별한 지식을 갖추세요 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     큰 돈을 벌고 싶다면 특별한 지식을 갖추세요 [번역글]

  [요약] 특별한 지식을 갖춰라 (Arm Yourself With Specific Knowledge)

    1. 핵심 메시지
          + 돈을 많이 벌고 싶다면, ‘특별한 지식(specific knowledge)’, ‘책임감(accountability)’, ‘레버리지(leverage)’, ‘판단력(judgment)’이 필요하다.
          + 이 중에서도 ‘특별한 지식’은 가장 전달하기 어렵고, 사람들이 혼동하기 쉬운 개념이다.
    2. 특별한 지식이란?
          + 학교에서 가르칠 수 없는, 오직 본인만이 갖출 수 있는 지식이나 기술.
          + 타고난 성향(DNA), 어릴 때 습득한 소프트 스킬, 현장에서 직접 배우는 경험 등에서 비롯된다.
          + 복잡한 환경에서 패턴을 인식하고, 특정 분야에서 판단력을 쌓아가는 과정에서 생긴다.
          + 예시: 투자, 트럭 운송, 기상 예보 등 어떤 분야든 적용 가능.
    3. 훈련으로 대체 불가
          + 특별한 지식은 단순한 훈련이나 교육으로 익힐 수 없다.
          + 누구나 수업을 듣고 배울 수 있다면, 그 일은 결국 자동화(로봇, 소프트웨어)되거나 저임금 노동이 된다.
          + 특별한 지식이 있어야만 높은 보상을 받을 수 있다.
    4. 호기심과 열정이 핵심
          + 진정한 특별한 지식은 타고난 재능, 진정한 호기심, 열정을 따라가다 보면 발견된다.
          + 인기 직업이나 유행하는 분야를 따라가는 것만으론 얻을 수 없다.
          + 100% 몰입하지 않으면, 100% 몰입한 사람에게 크게 뒤처진다. 이 분야에서는 복리 효과와 레버리지가 극대화된다.
    5. 놀이처럼 느껴지는 과정
          + 특별한 지식을 쌓는 과정은 논리적 고민보다 관찰과 경험을 통해 얻어진다.
          + 자신의 인생을 되돌아보며, 진짜 잘하는 것이 무엇인지 찾는 것이 중요하다.
          + 예를 들어, 저자는 과학자가 되고 싶었지만 실제로는 돈을 벌고, 기술을 다루고, 사람을 설득하는 데 더 뛰어났다.
          + 주변 사람들이(특히 가족 등) 본인의 특성을 더 잘 알아채는 경우도 많다.
    6. 정리
          + 특별한 지식은 타고난 재능, 진정한 호기심, 열정을 바탕으로, 놀이처럼 몰입하며 쌓아가는 것.
          + 학교나 훈련으로 대체될 수 없는, 오직 나만이 가질 수 있는 지식이어야 한다.
          + 주변의 관찰과 피드백도 중요한 힌트가 될 수 있다.

   틀린 말은 아닌데, 공허하게 들리네요...ㅋㅋㅠㅠ

   “큰 돈을 벌고 싶다면 허리에서 사고 어깨에서 팔아라” 라는 이야기랑 뭐가 다른지 모르겠습니다. 누구나 아는 당연한 이야기, 그러나 누구나 실현하긴 힘든.

   뭐, 떠먹여 달라는건 아니고... 그냥 그렇다구요ㅎㅎㅠ

   ㅎㅎㅎ 저도 비슷합니다
   그냥 뭐 누구나 할 수 있는 뻔한 얘기

   비슷하게 느꼈습니다 ㅎㅎ

   '특별한 지식'이란게 AI 발전따라 어찌 될 지 모른다는게 문제죠.
"
"https://news.hada.io/topic?id=21604","TPU 심층 분석","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               TPU 심층 분석

     * TPU는 구글이 개발한 대규모 AI 학습 및 추론용 맞춤형 칩으로, GPU와는 다른 설계 철학을 가지고 있음
     * 확장성과 에너지 효율성을 강조하며, 하드웨어(예: 시스템 온칩 구성, 대형 온칩 메모리)와 소프트웨어(** XLA 컴파일러**)를 함께 설계함
     * 핵심 구조는 시스톨릭 어레이와 파이프라이닝, 선행 컴파일 방식으로, 대부분의 딥러닝 연산(특히 행렬곱)에 최적화됨
     * OCI 및 OCS 기술 덕분에 유연한 노드 구성, 고성능 병렬 처리, 다양한 토폴로지 선택이 가능함
     * TPU 시스템은 단일 칩에서 초대형 멀티팟까지 계층적으로 확장되어, 대규모 AI 모델 훈련과 효율적 자원 활용이 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 배경

     * TPU는 구글이 AI 서비스의 대규모 확장에 대응하기 위해 개발한 ASIC 기반 칩임
     * 초기에는 GPU와 FPGA, ASIC 도입을 저울질했으나, 2013년 음성 검색 등 딥러닝 활용 확대에 따라 맞춤형 하드웨어의 필요성이 증가함
     * 현재 Gemini, Veo 등 주요 AI 서비스에 필수적 역할을 하며, 추천 모델(** DLRM**) 등에 폭넓게 활용되고 있음

TPU 단일 칩 구조

  기본 구성

     * TPUv4 기준 단일 칩에는 2개의 TensorCore가 존재(추론 특화 TPU는 1개)
     * 각각의 TensorCore는 CMEM(128MiB), HBM(32GiB) 메모리 장치와 연결됨

  TensorCore 내부 구성

     * Matrix Multiply Unit (MXU) : 128x128 시스톨릭 어레이 기반, 행렬곱 전담
     * Vector Unit (VPU) : 일반적 원소 단위 연산 수행
     * Vector Memory (VMEM; 32MiB) : HBM에서 데이터를 복사해 작업 준비 메모리로 활용
     * Scalar Unit + Scalar Memory (SMEM; 10MiB) : 제어 흐름, 스칼라 연산, 메모리 주소 관리 담당

  GPU와의 구조적 차이

     * TPU는 온칩 메모리(CMEM, VMEM, SMEM)이 GPU 대비 훨씬 큼
     * HBM 용량은 GPU가 더 크고, 연산 코어 수는 GPU가 훨씬 많음
     * TPUv5p 기준, 칩당 500 TFLOPs/sec, 전체 팟(8960칩) 기준 4.45 ExaFLOPs/sec 수준의 고성능 구현

TPU 설계 철학

  1. 시스톨릭 어레이 및 파이프라이닝

     * 시스톨릭 어레이는 처리 소자(PE)들이 배열되어 인접 소자에 연산 결과 전달
     * 데이터 입력 이후 별도 제어 없이 연속 연산 가능, 메모리 읽고 쓰기가 오직 입·출력에만 발생
     * 주로 행렬 곱셈, 컨볼루션에 최적화되어 있음
     * 파이프라인 처리를 통해 연산·데이터 이동을 동시에 진행해 처리량 최적화

    시스톨릭 어레이의 단점 - 희소성(sparsity)

     * 모든 연산 소자가 항상 활성화되어 희소 행렬에는 적합하지 않음
     * 앞으로 DL 모델이 불규칙 희소성을 추구할 경우 한계로 작용 가능

  2. 선행 컴파일(AoT) 및 캐시 최소화

     * TPU-XLA 코디자인으로 불규칙한 메모리 액세스를 필요로 하는 캐시 의존도를 최소화, 에너지 사용 절감
     * XLA 컴파일러가 연산 그래프 분석을 통해 메모리 액세스 패턴을 미리 산출, 캐시 대신 스크래치패드 메모리 중심 운용
     * JAX의 @jit는 JIT와 AoT의 중간 형태로, 최초 실행 시 정적 그래프 생성 후 XLA로 AoT 컴파일 진행
     * 입력 형태가 달라지면 재컴파일 필요, 동적 패딩/반복에는 비효율 존재
     * 높은 에너지 효율성 달성하나, 유연성 부족이 단점

    TPUv4 에너지 효율성

     * 현대 칩은 HBM3 메모리를 사용해 에너지 소모 감소
     * 연산 대비 메모리 작업의 에너지 소모가 수십~수백 배라, 메모리 액세스 최소화로 대폭 효율 향상 가능

TPU 다중 칩 구조

  트레이 수준(Tray/Board; 4칩)

     * 1트레이는 4개의 TPU칩(8개 TensorCore)과 CPU Host로 구성
     * Host↔Chip 연결은 PCIe, Chip↔Chip 연결은 Inter-Core Interconnect(ICI) 로, 더 높은 대역폭 제공

  랙 수준(Rack; 4x4x4=64칩)

     * 1랙은 64 TPU칩, 3D 토러스(4x4x4) 로 ICI, OCS(Optical Circuit Switching)로 연결
     * 구글에서는 랙(Rack), 팟(Pod), 슬라이스(Slice)를 구분함
          + 랙: 64칩 물리 단위(=큐브)
          + 팟: ICI 및 OCS로 연결 가능한 최대 단위(예: TPUv4=4096칩=64랙)
          + 슬라이스: 4칩~Superpod 사이 임의 구성 추상 단위

    OCS의 이점

    1. 래퍼라운드(Wraparound) : 각 축을 링(1D 토러스)화하여 노드 간 최악 이동 홉 수 절감
    2. 유연한 논컨티구어스 슬라이스: OCS 스위칭 구조로, 물리적으로 떨어진 노드도 하나의 슬라이스로 구성 가능, 자원 활용·유지보수 용이
    3. 비틀린 토폴로지: 동일 칩 수(x,y,z고정)에서 연결 구조을 변경해 특정 연산 패턴에 속도 최적화 가능(예: twisted torus)

    토폴로지 활용 사례

     * 큐브형: 데이터/텐서 병렬에 적합(최대 대역폭)
     * 직선형(시가형) : 파이프라인 병렬에 유리
     * 비틀린 토러스: all-to-all 통신 필요시(예: tensor parallel) 속도 향상

  슈퍼팟(Full Pod/Superpod; TPUv4: 4096칩/64랙)

     * 여러 랙을 ICI 및 OCS로 연결해 초대형 시스템 구축
     * 슬라이스 토폴로지 종류에 따라 통신 대역폭/병렬처리 성능 차별화
     * OCS 덕분에 논컨티구어스 슬라이스, 비틀린 토폴로지 등 유연성 확보

  멀티팟 수준(Multi-pod/Multislice; TPUv4: 4096개 초과)

     * 여러 팟을 데이터센터 네트워크(DCN) 로 연결, 대규모 학습 인프라 제공하나 ICI 대비 대역폭 낮음
     * PaLM 훈련은 2팟(6144 TPUv4) 활용, 총 6팟 자원관리로 진행
     * 대규모 모델 개발 시, 연구자 설정값(병렬 차원 등)에 따라 XLA 컴파일러가 통신 패턴 최적화
     * XLA가 각 슬라이스·팟 간 통신 연산 삽입, 최소 코드 변경으로 대규모 분산학습 구현

실물 하드웨어와 도식 예시

     * TPU 랙: 4x4x4 3D 토러스가 한 유닛, 각 행마다 2트레이(8칩)
     * TPUv4 트레이: 실제로는 4개의 PCIe 포트(각 TPU 1개씩)
     * TPUv4 칩: 중앙의 ASIC, 주변 4개 HBM 스택(2 TensorCore 기준)
     * TPUv4i(추론형) 칩 플로어플랜: 1 TensorCore, 넓은 CMEM 면적 차지

마무리

     * 구글 TPU Research Cloud(TRC) 에 연구 지원을 표함

참고자료

     * TPU Multi-Slice Trainng
     * Xu et al., GSPMD 논문
     * Jouppi et al., TPUv4i 논문
     * How to Scale Your Model - TPUs
     * 그 외 10여건의 논문 및 공식 발표자료
"
"https://news.hada.io/topic?id=21664","새로운 피라미드 모양, 항상 같은 면으로 착지하는 특성 발견","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   새로운 피라미드 모양, 항상 같은 면으로 착지하는 특성 발견

     * 수학자들이 한 면만 위로 착지하는 모노스테이블 테트라헤드론을 실제로 제작하는 데 성공함
     * 이 모양을 만들기 위해 극단적으로 다른 밀도의 재료가 요구됨
     * 카본 파이버 프레임과 소량의 텅스텐 카바이드를 사용하여 실물 제작이 이루어짐
     * 아주 미세한 오차까지 신경 써야 했으며, 실험 중에는 작은 풀자국이 기능에 영향을 미침
     * 이 연구가 공간지각, 엔지니어링 및 새로운 이론적 질문에 기여할 가능성이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

문제의 시작

     * Gergő Almádi 등 연구팀은 한 면만 위로 착지하는 모노스테이블 테트라헤드론(4면체) 구현에 도전함
     * 수학적 이론에서는 무게 분포를 자유롭게 가정할 수 있지만, 실제 현실에서는 물질의 한계가 존재함
     * 일부 면을 매우 무겁게, 나머지 면은 거의 무게가 없게 설정할 수 있지만, 이런 이상적인 상황은 물리적으로 불가능함

현실적인 구현 도전

     * 컴퓨터를 통해 다양한 전복 패턴의 테트라헤드론을 탐색함
     * 한 종류는 면이 순차적으로 넘어가면서 마지막 면에 안착하고, 다른 종류는 특정 면만 고정적으로 도달하는 구조임
     * 일부 패턴에서는 태양 중심부보다 1.5배 더 높은 밀도를 지닌 재료가 필요하다는 계산이 나옴

현실적인 실행

     * 연구팀은 좀 더 구현 가능한 전복 경로에 집중함
     * 그래도 일부는 나머지에 비해 5,000배 밀도가 높은 재료가 필요함
     * 튼튼함이 필수적이어서, 고정밀 경량 카본 파이버와 텅스텐 카바이드를 조합하고, 접착제 양까지 정밀하게 통제함

성공과 실패, 그리고 우연한 발견

     * 다양한 시행착오 끝에 모델 하나를 완성했으나, 작동하지 않았음
     * 작은 접착제 덩어리가 한 꼭짓점에 남아 있었던 것을 발견함
     * 접착제를 제거한 후, 모형이 완벽하게 작동함을 확인함
     * 컴퓨터 모델의 이론과 현실에서의 미세한 차이가 결과에 큰 영향을 미침을 실감하게 됨

의의와 향후 활용

     * 이번 연구는 복잡한 수학이 아니라, 기본적 개념 질문에서 시작된 혁신임
     * 실제로 작동하는 모노스테이블 테트라헤드론의 실험적 구현으로, 다면체 연구 및 엔지니어링에 새로운 질문을 제기하게 됨
     * 미래에는 이러한 형태를 달 착륙선 등에서 자기정렬 기능에 적용할 수 있을지도 모름
     * 직접 보고 실험해보는 과정이 추상적 사고에서 중요한 역할을 한다는 교훈을 남김

결론

     * 이번 발견은 오랜 시간 검증되지 않았던 존 콘웨이의 제안을 60년 만에 실제로 증명한 사례임
     * 이 연구가 앞으로 기하학, 공학, 이론적 수학에 새로운 영감을 줄 것으로 기대됨

   다른 면으로 눕혀도 스스로 일어나서 원상복구하는 것은 신기하네요
   무게 중심 차이때문일까요?

        Hacker News 의견

     * 최악의 D-4 주사위 경험이라는 농담을 하면서, 한편으로는 '칼날 위 균형'처럼 정말 극단적으로 한쪽 면에서만 안정적인 다면체를 만들 수 있을지 궁금함을 이야기함
         1. 정확히 두 면에서만 안정적인 비균일 질량의 다면체 구상
         2. 그 중 한 면은 더욱 안정적이어서, 덜 안정적인 면에서 교란되면 더 안정적인 면으로 넘어가도록 설계
            이런 구조를 변조 감지기로 활용 가능성 제시
          + 농담을 받아들이며, DND 주사위에 미쳐 있는 친구가 D-1 Möbius strip 주사위를 자랑하는 것을 기억함
            Möbius Strip Dice
            내가 당구공 1번을 추천했더니 그 친구는 탐탁치 않아함
          + 핵심 키워드는 ""mono-monostatic""임을 언급
            다면체가 아닌 예시로 Gömböc가 대표적임
            Gömböc 위키
            실제로 21면 mono-monostatic 다면체 논문 링크 공유
            21면 다면체 논문
          + 쉽게 넘어질 수 있는 막대 형태가 당신이 상상하는 구조에 부합할 것 같은데, 뭔가 잘못 이해한 부분이 있을 수도 있음
          + 키가 크고 단단한 원뿔이 이와 비슷한 성질을 지님
            이를 다면체 형태로 약간 튜닝 가능성 제안
          + 변조 감지용 구조에 폴리헤드론(다면체)일 필요가 있냐는 질문 제기
     * 달 착륙선을 이런 형태로 만들어야 한다는 아이디어 제안
          + 실제로 논문에서 이런 형태가 언급된다고 안내
            arxiv 논문 링크
          + 각진 부분이 없는 일반 Gömböc도 우주선에 활용 가능성 설명
            우주선에 각이 꼭 필요하다는 규정 없음
            오히려 거북이의 외골격에 더 실질적 용도가 있을 것이라 언급
            거북이처럼 다리가 짧은 동물은 평평한 밑면이 필요하지만, Gömböc는 평평한 면이 없음
            경사길 달리는 차량 등에도 응용 가능성 언급
          + 기사 내용에 따르면 실제로 연구진도 그런 구조를 개발 중이지만, 밀도 분포상 테트라헤드론 형태가 아닐 수 있음
            곡면이 포함될 것이라 언급
          + 비슷한 형태를 비행기에 적용할 수도 있지만, 날개를 어디에 둘지 고민임
            굳이 달에만 제한할 필요 없이 더 폭넓은 활용 가능성 제안
          + 드론에 이런 원리를 적용하면 충돌이나 추락 시 프로펠러를 본체에 수납하는 식으로 Skynet에 한 발짝 가까워질 수 있을 것이라 이야기
     * Gömböc와는 질적으로 다르며, 해당 구조는 질량이 바닥 플레이트에 집중되어 있음
          + 아마존에서 Gömböc 가격이 엄청나게 비싸다는 점에 감탄
          + 해당 테트라헤드론은 대부분 속이 비어 있고, 질량 중심이 정교하게 맞춰져 있다는 점을 언급
            단단한 물체라면 질량의 균일성은 상관없으니, 질량 중심만 같다면 똑같이 동작한다는 생각 전달
     * 수학이 대중적으로 매력이 떨어진다는 PR 문제 지적
       무게가 균일하지 않기 때문에 일반인에게는 놀랍지 않을 수도 있는데, 이는 한쪽에 무게를 달아 둔 철사 구보다 해상도가 낮은 버전 느낌
       겉면을 덧씌우면 훨씬 인상적일 것이라는 의견
     * Vans 신발이 이런 원리와 비슷하다며, 관련 챌린지 링크 공유
       Vans Challenge
     * 균일한 밀도에서는 동작하지 않는 것이 조금 아쉬움
       하나의 소재에 구멍만 뚫어서 3D 프린팅하면 될 줄 알았는데, 실제로는 높은 질량 분포 차이가 필요하다는 점이 더 놀라움
          + 그렇다면 어떤 형상과 질량 분포가 균일성에 가장 근접하는지 혹은 균일성을 최대화할 수 있을지 흥미로운 질문 제기
          + 결국 하단에 무거운 물체가 있어서 항상 똑바로 서는 장난감과 유사한 원리라는 생각
          + 실제로 그 사실이 증명된 것인지 궁금증 표출
     * 고양이도 피라미드임을 농담 삼아 질문
          + 실제로 고양이는 자가 조립이 가능한 ‘액체’ 피라미드로, 중력장에 맞춰 분자가 재배열되는 모습
            자가 착륙 로켓보다 더 멋있고 더 귀엽다는 비유
     * 여러 종류의 Gömböc 동작 모습을 영상으로 소개
       Gömböc 동영상
     * 질량 중심 표시가 된 3D 모델을 보면 좋겠다며 아쉬움을 토로
          + 중심이 순수 텅스텐 카바이드 삼각면 중앙, 즉 센트로이드 위치임을 거의 확신
            오차가 있어도 육안으로는 구별 불가하다는 설명
     * 다음 DragonCon 행사에서 이런 주사위를 사고 싶다는 기대와 함께 매년 구매하는 D20 주사위 스택 옆에 두고 싶다는 소망
"
"https://news.hada.io/topic?id=21613","언제 멈추고, 언제 밀어붙여야 할까? [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       언제 멈추고, 언제 밀어붙여야 할까? [번역글]

  고집 센 혁신가와 막무가내 바보: 언제 멈추고, 언제 밀어붙여야 할까?

    1. 문제의식: 성공과 실패의 경계는 애매하다
          + 인생이나 비즈니스에서 “언제 멈춰야 할지, 언제 끝까지 밀고 나가야 할지”는 그 순간에는 절대 알 수 없다.
          + 똑같이 힘든 상황에서, 어떤 경우는 끝까지 버텨 성공이 되고, 어떤 경우는 시간·돈만 낭비하는 실패가 된다.
          + 성공담에는 “포기하지 말라”, 실패담에는 “고집부리지 말라”는 상반된 조언이 넘치지만, 실제로는 둘 다 그 시점에서는 구분이 불가능하다.
    2. 결정의 본질: 결과로만 판단할 수 있다
          + 힘든 상황에서는 두 시나리오의 차이를 알 수 없고, 전문가(VC 등)조차도 예측하지 못한다.
          + 결국 우리는 선택을 내리고 결과를 지켜볼 뿐, 그 이상도 이하도 아니다.
          + “멈추는 건 확실한 실패, 계속 밀고 나가는 것만이 성공의 가능성을 남긴다”는 점에서, 결국 실행이 답이다.
    3. 실행 전략: 불확실성 속에서 현명하게 판단하는 질문들
          + 아래와 같은 질문과 전략을 통해, 멈출지 계속할지 조금 더 객관적으로 판단할 수 있다.
          + 타임박스(Timebox): 일정 기간 내에 개선이 없으면 미련 없이 그만두겠다고 데드라인을 정한다.
          + 재미와 배움: 여전히 이 프로젝트가 재미있고, 배우는 게 있는가?
          + 진행 속도: 최근 진행이 빨라지고 있는가, 느려지고 있는가?
          + 제3자 시각: 외부인의 피드백으로 자신을 점검한다.
          + 매몰비용 무시: 이미 쓴 시간·돈에 휘둘려 결정하지 않는다.
          + 팀의 에너지: 팀이 더 이상 확신이나 아이디어가 없다면, 방향 전환(pivot)이나 종료를 고민한다.
          + 12개월 뒤 상상: 1년 뒤에도 이 일을 하고 있을 자신이 있는가?
          + 제품 경쟁력: 우리 제품이 시장 대안보다 확실히 낫거나 곧 나아질 것 같은가?
          + 입증된 사례(Existence proof): 남이 비슷한 성공을 이미 해냈는가?
          + 가치·개선·의지: (a) 충분히 가치 있는가, (b) 개선 가능성이 있는가, (c) 내가 여전히 에너지가 있는가?
          + 기회비용(Opportunity Cost): 이 일에 집착하느라 더 중요한 기회를 놓치고 있지 않은가?
          + 결과와 보상: 실패 시 불이익, 성공 시 보상은 무엇인가?
          + 두려움 극복: 두려움 때문에 결정을 미루지 않는다.
          + 내면의 소리: 혼자 산책하며 내면의 진짜 답을 들어본다.
          + 처음의 ‘이유(Why)’: 시작할 때의 동기가 지금도 유효한가?
    4. 결론: 실행하고, 결과에 연연하지 말라
          + 어떤 시나리오에 있는지 애초에 알 수 없다는 사실을 받아들이면, 오히려 더 자유로워진다.
          + 지나친 고민이나 죄책감, 자만심도 내려놓고, 과감하게 실행하며 결과를 객관적으로 관찰하라.
          + 그리고 일이 끝나면 뒤돌아보지 말고, 다음으로 나아가라.

     “멈추는 순간, 당신은 결코 위대한 무언가를 만들어낼 수 없습니다. 결국 우리는 알 수 없습니다. … 그러니 이 사실을 긍정적으로 받아들이세요. 오히려 이게 당신을 더 자유롭게 해줍니다.” (""If you stop, you’ll never create something great. … So embrace this fact. It actually sets you free."")
"
"https://news.hada.io/topic?id=21605","미국, 이란 핵 시설 공격 감행","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           미국, 이란 핵 시설 공격 감행

     * 미국이 이란의 핵 시설을 공격하며 중동 분쟁에 본격적으로 개입함
     * 이 결정은 이란의 군사 행동에 대응하기 위한 조치임
     * 백악관에서 트럼프 대통령이 부통령, 국무장관, 국방장관과 함께 공식 성명을 발표함
     * 이 사태로 인해 중동 지역 안보 및 국제 정세가 한층 불안정해짐
     * 각국은 향후 미국과 이란 간의 갈등 확대 가능성을 주목하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * 미국이 이란 핵 시설을 겨냥한 공습 작전을 수행했음
     * 이는 이란의 지속적인 군사적 도발에 대응하는 결정임
     * 트럼프 대통령은 부통령 Vance, 국무장관 Marco Rubio, 국방장관 Pete Hegseth와 함께 백악관에서 공식 성명을 발표했음

공식 발표

     * 트럼프 대통령이 전 국민 대상 연설을 진행함
     * 이 자리에는 부통령 Vance, 국무장관 Marco Rubio, 국방장관 Pete Hegseth가 동석했음
     * 연설은 국가 안보 및 미국의 입장에 대한 강한 의지를 표명하는 내용임

국제적 여파

     * 이번 미국의 행동으로 인해 중동의 지정학적 긴장이 고조됨
     * 국제사회는 양국 간 갈등 심화와 지역적 확산 가능성에 큰 관심을 보이고 있음

핵심 시사점

     * 이번 사태는 미국이 중동 분쟁에 본격 개입했음을 상징함
     * 이란의 핵 프로그램과 지역안보 문제가 더욱 부각되는 추이임

        Hacker News 의견

     * 더 많은 토론 내용은 여기 링크에서 확인 가능
          + 토론 댓글이 그쪽으로 옮겨진 상황, 참고 안내
"
"https://news.hada.io/topic?id=21610","JavaScript 라이브러리를 위한 새로운 로깅 접근법: LogTape","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                JavaScript 라이브러리를 위한 새로운 로깅 접근법: LogTape

    라이브러리 vs 애플리케이션: 근본적으로 다른 로깅 요구사항

     * 애플리케이션 로깅: 개발자가 직접 제어하는 환경에서 명시적 설정과 관리
     * 라이브러리 로깅: 타인의 프로젝트에 포함되어 사용자 환경과 선택권 존중 필요
     * 기존 방식의 한계: 애플리케이션 중심 로거(winston, Pino)를 라이브러리에 적용 시 강제성 문제
     * 라이브러리 제작자의 딜레마: 디버깅 정보 제공 vs 사용자에게 부담 주지 않기

    현재 라이브러리 로깅의 문제점

     * 파편화된 로깅 생태계: Express는 DEBUG=express:*, Mongoose는 mongoose.set('debug', true) 등 각기 다른 방식 사용
     * 의존성 딜레마: winston, Pino 같은 애플리케이션 중심 라이브러리 사용 시 사용자에게 원치 않는 의존성과 설정 강요
     * 침묵 vs 강요: 로깅을 아예 포기하거나 사용자에게 로깅 방식을 강제하는 양극단 선택지
     * 의존성 주입의 복잡성: 더 정교한 접근법이지만 API 복잡도 증가와 사용자 부담 가중

    LogTape의 ""라이브러리 우선"" 철학

     * 조건부 활성화: 로깅이 설정되지 않으면 완전한 무작동, 설정되면 통합 관리
     * 사용자 선택권 보장: 라이브러리가 로깅 방식을 강제하지 않고 사용자가 원할 때만 활성화
     * 제로 의존성: 5.3KB 크기로 공급망 보안 위험 제거 및 버전 충돌 방지
     * ESM/CJS 완전 지원: 호환성 체인 문제 해결 및 tree shaking을 통한 번들 최적화

    실용적 장점들

     * 성능 최적화: 비활성화 시 거의 제로 오버헤드, 활성화 시 우수한 콘솔 출력 성능
     * 네임스페이스 분리: [""my-lib"", ""feature""] 형태의 계층적 카테고리로 충돌 방지
     * TypeScript 우선 설계: 추가 타입 패키지 없이 완전한 타입 안전성 제공
     * 기존 시스템과의 브리징: winston, Pino 어댑터를 통한 점진적 도입 지원

    현실적 고려사항

     * 어댑터의 의미: 아직 생태계 표준이 아닌 현실 인정과 실용적 타협책
     * Python 생태계 영감: 표준 logging 라이브러리로 통합된 Python의 성공 사례 참조
     * 미래 지향적 접근: 라이브러리 생태계의 점진적 개선을 위한 하나의 선택지로 제시

   어떻게 해야 설정을 안했을 때 무작동인지 이해가 잘 안되네요.
   getLogger를 할 때 이미 로거를 생성하고 debug를 찍으면 작동하는데 말이죠.

   제가 코드를 봤을 때는 그저 작동하지않는 것 처럼 보이게하는 거고
   string연산을 늦춰주는것도 아니기에
   그저 로그레벨 설정하면 찍어주지않는 타 라이브러리랑 무작동이 어떻게 다른지 이해가 잘안되네요

   어라, configure()/configureSync()를 호출하지 않았는데도 로그가 찍히나요? 어디에 찍히나요? 콘솔 화면에 찍히나요?

   아, 여기서 이야기하는 작동하다는 console이나 file에 로그가 저장되는것을 의미하는 것이 아니라, 함수가 실행되어 실질적으로 오버헤드가 있냐를 이야기 드렸던겁니다.

   오해가 있을법하네요

   물론 로거의 주 오버헤드가 system call인걸로 미루어보았을 때 오버헤드가 없다고는 볼 수 있지만
   그게 타 로거와의 차별점이냐?라고 하기에는 타 로거 또한 동일하게 동작하기에 그렇다는거죠

   아하, 그런 말씀이셨군요. 일단 null output 기준으로 벤치마크를 돌려 봤을 때는 오버헤드가 거의 없다고 볼 수 있겠더라고요. 하지만, 성능적인 오버헤드보다 좀 더 중요하다고 생각했던 건 디폴트 동작이 no-op이냐 아니냐에 있다고 봤습니다. 라이브러리 저자 입장에서는 라이브러리 내에서 로그를 찍더라도 라이브러리를 사용하는 애플리케이션이 실행될 때 멋대로 콘솔이나 파일에 로그가 출력되면 곤란하니까요.

   아하, SHOW GN이였네요.
   요즘 생태계는 로거를 외부에서 주입받는 형태를 많이 선택하다보니 공감이 안됐던것도 있는 것 같습니다.
   설정되지않으면 당연히 작동하지않으니까요.
   그래도 지금까지 해당 생태계에 없던 로거 인터페이스기도 하고, 자유도가 높다보니 더 좋은 것 같습니다.

   주신 벤치마크 기준의 경우 system call을 제외하고 null output을 출력하다보니
   이 부분에 대해서는 내부 로거 형태에 따라서 확실히 다를 수 있다고 생각이되네요.

   이 부분에서는 Pino와 세배까지 차이를 가져가는군요. 크
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   FYI: 추가로 제가 말씀드렸던 외부에서 주입받는 로거의 형태는 Openai Node sdk만 보셔도 로거를 외부에서 주입받아서 출력을 하는 형태라서 쉽게 확인해보실 수 있을 것 같습니다.

   https://github.com/dahlia/logtape/…
"
"https://news.hada.io/topic?id=21691","Issen (YC F24) – 개인용 AI 언어 튜터 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Issen (YC F24) – 개인용 AI 언어 튜터 출시

     * Issen은 개인화된 AI 기반 언어 튜터를 제공함
     * 사용자는 대화형 방식으로 언어 학습 경험을 누릴 수 있음
     * AI 시스템은 개별 사용자 목표와 약점에 맞춰 학습 경로를 설정함
     * 기존 언어 학습 앱과 달리 실시간 피드백과 자연스러운 대화 기능이 특징임
     * 스타트업 및 IT 인재에게 빠르고 효율적인 외국어 실력 향상 방안으로 주목됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서비스 소개

     * Issen은 Y Combinator F24 기수에 선정된 스타트업으로, 개인별 맞춤형 AI 언어 튜터를 제공함
     * 사용자는 대화 기반 인터페이스를 활용하여 영어 등 다양한 외국어를 자연스럽게 연습할 수 있음
     * AI 언어 튜터는 사용자의 발음, 어휘, 문법 취약점을 파악해 맞춤형 질문과 피드백을 즉시 제공함
     * 기존의 강의식 교육 방식과 달리 실시간 쌍방향 대화와 적응형 학습 경로를 결합해, 학습자의 주도성을 높임

주요 특징

     * 대화형 챗봇 AI가 학습자의 목표와 레벨을 기반으로 개인화된 언어 학습 계획을 제안함
     * 실시간 문법 교정, 어휘 추천, 발음 피드백 등 즉각적인 답변 시스템 제공
     * 교재 기반 학습이 아닌 실제 생활과 비즈니스에 유용한 맞춤형 대화 시나리오 지원
     * 학습 데이터와 진행 상황을 분석해 목표 도달도와 취약점 개선을 지속적으로 모니터링함

차별점 및 기대 효과

     * 기존 앱이 제공하는 정형화된 تمرين이나 반복 학습 대신, 실제 언어 사용 환경에 가까운 대화 경험 제공
     * 언어 학습자가 자신의 부족한 부분을 빠르게 파악하고 효율적으로 집중 훈련할 수 있도록 도움
     * 스타트업 및 글로벌 IT 인재들이 해외 진출 및 다양한 업무 환경에서 빠르게 언어 역량을 높일 수 있는 솔루션으로 부각됨

        Hacker News 의견

     * 언어 학습에는 혁신이 필요하다는 점에 공감함을 전하고 싶음
       웹 앱에서 그리스어 초급 학습을 시도해봤는데, ChatGPT나 Gemini 음성 모드보다는 나은 경험이었음
       하지만 5분 정도 지나 AI 튜터가 나만의 학습 계획이나 내 어려움을 파악하지 못한다는 점에서 혼란을 느낌
       예를 들어 취미를 묻고는 곧바로 산에서 하이킹하는 긴 그리스어 문장을 내게 말함
       완전 초보인 나는 이 문장에 답도 못하고, 심지어 반복조차 어려운 상황임
       지금 내가 무엇을 해야 하는지조차 모름
       실제 선생님이라면 문장을 일부 반복하고 번역해 주거나 설명을 곁들였을 것임
       하지만 AI 튜터는 내가 뭔가 말하길 기다리기만 하고, 엉뚱하게 해변 휴가에 대해 이야기 전개
       그래도 기존 AI보단 낫다고 느껴서 다시 한번 시도할 계획임
          + 스웨덴어에서도 비슷한 느낌을 받음
            ChatGPT의 고급 음성 모드와 큰 차이가 없고, 내가 대화를 이끌어야 해서 전반적으로 임의적이라는 인상
            익숙한 주제로만 돌아가게 되어 오히려 연습 의미가 반감
            보다 체계적으로 새로운 영역과 실력을 확장해줄 가이드형 학습 플랜을 기대했음
          + Language Transfer가 어떤 AI 기반 코스보다 뛰어난 언어 교육 자료라고 생각
            수년간 인간 강사가 체계적으로 커리큘럼을 설계했기 때문에 그 무엇보다 논리적
            특히 그리스어 강좌가 훌륭하며 광고도 없고 완전 무료
            내가 사용해 본 최고의 언어 학습 도구라고 자신 있게 추천
            https://www.languagetransfer.org/
          + 내 모국어가 그리스어라고 했더니 에러가 나며 온보딩 가이드가 초기화됨
            영어가 모국어라고 거짓말하니 정상적으로 진행
            하지만 입력했던 내 이름이 아니라 Anton이라는 이름으로 부름
          + STT->LLM->TTS 구조의 한계가 꽤 뚜렷하다고 느꼈음
            내가 말을 더듬거나 어려워하는 뉘앙스가 텍스트 변환 과정에서 완전히 사라짐
          + 프랑스어 실력을 다듬으려 Memrise에 유료 결제했었음
            스크립트 수업은 훌륭했지만, AI 대화 모드는 수업에서 배운 어휘·문법 수준을 잊고 관용어로 대화
            결국 흥미를 잃고 사용을 중단함
     * 듀오링고는 게이미피케이션이 너무 심해서 도저히 쓸 수 없었는데, 이 앱은 이 부분을 덜어서 고마움
       나의 모국어와 목표 언어 전환이 자연스럽게 가능한지, 대화 중 영어로 질문해도 되는지 궁금함
          + 이 앱들이 전체 사용자 집단 최적화에 초점을 맞추고 개개인에 맞춘 경험은 제공하지 못한다고 느낌
            글로벌 최적화 관련 논문도 발표할 정도
            학습 지표나 컨텐츠 생산은 쉬운데, 개별 학습자에게 제대로 맞추는 루프는 아무도 만들지 못함
            듀오링고에서도 '훈련'을 눌러도 발전이 없고 매번 똑같은 Bread and water(진부한 내용)만 나옴
          + STT와 TTS의 다국어 통합을 실제로 오래 연구해서, 여러 언어 혼합시 잘 작동함을 강조
          + Babbel이 듀오링고보다 낫다고 생각
     * 언어 학습에 TTS(음성 합성)를 완전히 신뢰하지 못함
       발음이 잘못 내면화될 수 있는데 그걸 알 수 없음
       Duolingo 일본어 과정에서 실제 녹음일 줄 알았는데, 'oyogu'가 'oyNHYAOgu'처럼 이상하게 합성되어 들림
       초보자라면 이런 오류를 그대로 따라 하게 될까 불안함
       물론 실수는 몰입 시간이 길어지면 고칠 수 있지만, 처음부터 잘못 익히는 것은 더 많은 노력이 필요
       특히 일본어처럼 피치 억양(pitch accent)이 중요한데, 많은 자료와 사람들은 이걸 무시함
       예: 'ima'는 두 번째 음절 높낮이에 따라 '지금'과 '거실'이 달라짐
       이 문제는 일본어처럼 한자어가 많은 언어일수록 더 커질 수 있음
          + Minimax의 신형 TTS 모델이 꽤 뛰어남
            일부 일본어 튜터에 그 목소리를 적용 중이며, 피치 액센트도 거의 완벽
            가끔 한자나 미스리딩도 있지만, 후리가나가 달라질 때 바로 알 수 있음
          + 일본어 TTS 난이도가 의외로 높다고 느낌
            나도 언어 학습 앱을 만들면서 여러 업체(11labs, OpenAI, play.ht, Azure, Google, Polly 등)의 TTS를 써 봤으나, 세 문장 중 한 문장은 오류 있었음
            고치는 데 일주일 정도 걸렸고, 지금은 오류 없는 상태
            이 현상은 일본어에만 심하고, 대부분의 성조 언어는 톤은 맞음
            자연스러운 느낌은 내가 평가할 자격 없지만, 일본어처럼 심각한 엇박자는 경험 안 함
          + AI 음성 인식(트랜스크립션)은 좋은 편, AI 번역은 언어쌍에 따라 괜찮음
            하지만 TTS는 여전히 대부분 언어에서 품질이 부족한 편
          + 나 역시 일본어 초급자로 피치 억양의 중요성을 강하게 느낌
            언어마다 음절 강조 방식이 다름
            스페인어는 모음 길이, 아이슬란드는 볼륨, 영어는 길이+볼륨, 스웨덴/일본어는 높낮이가 중요
            영어도 스트레스 잘못 주면 의사소통 불가
            일본어는 동음이의어가 많고, 피치에 따라 의미가 달라지기에 더 특별
            정확한 높낮이가 절실함
     * 이 앱 정말 최고 경험
       아르헨티나 사람과 대화가 엄청 자연스럽게 이어짐
       부에노스아이레스에서 18년 이상 살며 발음·억양도 좋은 편
       기초 문법이 부족해 빈틈이 많았는데, 이 앱이 그런 부분을 정밀하게 다듬어줌
       iOS UX에서 세팅 모달을 띄웠을 때 닫기 버튼(CTA)이 잘 안 보여 아쉬움
       세팅 버튼 클릭 상태와 비활성 상태 컬러가 거의 같음
       해결책: 오른쪽 상단에 닫기 X 버튼 추가 및 클릭 시 색상 변환 필요
       UX 피드백 더 원한다면 연락해달라는 제안 visualsitemaps.com
     * 베트남어 배우려고 시도했지만, 수업 품질이 매우 낮고 잘못된 정보도 있었음
       남자가 자신을 가리킬 때 Anh mệt은 맞지만, Em mệt이 여성용이라는 건 잘못된 설명
       'Anh'는 나이가 많은 남자를 의미, 'Em'은 성별과 관계없이 어리면 사용
       많은 여성들이 실제 나이보다 어리게 보이고 싶어 Em을 선호하지만, 젊은 남성도 Em 사용 가능
       좋은 튜터라면 나이·관계를 기준으로 맥락 설명
       영어 문장을 베트남 억양으로 말해주는 오류도 있었음
       내 수준보다 너무 어려운 문장을 주거나 요청하면 초점이 어긋남
       대부분 남부 베트남어가 더 보편적이지만, 튜터들은 북부 베트남 출신
       음성 인식(STT)도 발음이 틀려도 너무 관대하거나 영어·베트남어 구분 못함
       예: ""Phai""를 ""bye""로 인식
       훨씬 저렴하고 스케줄 없이 배울 수 있기에 기대했지만 정확성에 신뢰를 갖기 어려움
       현재는 베트남인 튜터에게 수업당 $20씩 월 $160 지불
          + 품질이 충분하지 않은 점에 대해 사과하며, 베트남어는 아직 테스트하지 못했음을 밝힘
            문제 제기에 감사
          + 이런 실수들이 LLM에서 자주 나타나는 일반적인 유형
            좋은 언어 학습 플랫폼이 절실함
          + 베트남어 학습 동지를 만나 반가움
            피드백이 핵심을 찔렀다고 공감
            기초 문장 연습을 돕는 툴을 만들어서 피드백을 받고 싶다는 제안 https://envn.app
     * 일본어로 시도해봤는데 꽤 답답함
       초보자임에도 튜터가 일본어로만 대화하여, 이해 안 된다고 여러 번 말했음에도 동일
       영어에서 점진적으로 일본어로 넘어가 달라고 부탁해도, 한 문장만 영어로 말하곤 바로 일본어로 돌아감
       충분히 실력 있는 중급자 이상에겐 회화 연습 용도로 유용할 수 있겠으나, 완전 초급자를 위한 경험이 더 필요
       모델이 멀티모달 지원하니 시각적 자료까지 적극 활용하면 좋겠음
       기록한 일본어를 로마자(rōmaji)로 바로 보여주면 더 좋겠다는 제안
          + 그런 경험에 깊이 공감
            솔직히 현재 초점은 B1 이상 레벨 수강생 중심임
            완전 초급자들의 0→1 단계 학습이 필요하다면, 전통적인 자료(특히 듣기・말하기 덜 강조) 활용이 더 나을 것
     * ChatGPT 모바일 앱의 핸즈프리 음성 대화 모드는 꽤 쓸만함
       단 주제가 없으면 대화가 단조롭고 늘 진부한 얘기에 머무름
       그래서 나는 뉴스 기사 전체와 관련 링크를 복사해서 “이 주제로 언어 연습하자”라고 고지하고 진행
       이렇게 스페인어로 1시간씩 산책하면서 실전 연습
       내가 원하면 ChatGPT에는 내 모국어로만 질문하고, 상대방(챗봇)은 목표 언어로만 답하게 해서 청취력 집중 훈련
       Issen이 이 경험을 얼마나 개선했는지 궁금함
          + 나도 비슷한 포인트에서 출발했다고 밝힘
            음성 모델의 발전 속도도 빠름
            언어 학습에 특화된 전체적 경험(맞춤 커리큘럼, 프롬프트, 정확한 인식용 AI 모델, 플래시카드/사전 등)이 필요하다고 느낌
            핸즈프리 모드 제공, 슬랭·말하기 속도·목표어 사용률 등 다양한 요소도 커스터마이즈 가능
     * 러시아어로 연습해봤는데, 연습 자체는 좋았지만 모든 언어 학습 앱에 공통적으로 부족한 점은 내 발음 오류 캐치&교정 기능
       대충 비슷하게만 말해도 자동 인식(STT)이 통과시켜버림
       AI가 실제 내 발음을 ‘듣고’ 정확히 어디를 틀렸는지 빠짐없이 잡아주는 시대가 오길 기대
       지금은 액센트 문제뿐 아니라 격어미·단어 스트레스 등도 감지 못함
       AI가 바른 억양이나 형태로 따라 말해주면 교정 도움이 되긴 하지만, 실제 인간 교사가 주는 자신감은 주지 못함
       제품 제안: 트랜스크립션(자막)을 꺼둘 수 있는 기능 도입
       특히 일부 언어에서 글자가 보이면 오히려 방해되거나, 초보들에게는 지름길이 될 수 있기 때문
       마지막으로, 임의적이고 방향성 없는 대화(예: “AI의 어떤 점이 가장 흥미로운가?”)보다 좀 더 목표 지향적인 대화 구조가 있으면 훨씬 재미있음
       게이미피케이션 없이도 토론 형태(“이 주제로 날 설득해봐!”) 또는 구체적 경험과 연결(“올해 업무에서 가장 중요한 목표가 뭐야?”) 등 실질적 의견/경험을 이끌어낼 방식 제안
       지금까지 본 중 실제로 쓸 법한 최초의 제품이라 생각, 칭찬 보냄
          + 설득(debate club) 레슨 아이디어에 크게 공감
            음성→음성 모델이 성숙하면 정말 기대됨
            OpenAI/Gemini에서도 큰 진전이 있어 곧 적용 계획
     * 첫 번째 재생 후 로그인해야만 볼 수 있는 영상이 왜 굳이 잠겨있는지 궁금
       이 앱이 B1 이상을 겨냥한다는 피드백을 봤지만 데모 비디오는 A1~C1까지 다양
       A1~C1 범위 모두 필요로 하는 사용자는 드물고, 각 단계별로 명확한 시장이 따로 있으니 구분이 필요
       프랑스어 TTS(음성 합성)는 ChatGPT의 기본 음성보다 자연스러움이 떨어짐
       사용자 레벨별로 원클릭 과제(뉴스 읽기-토론, 뉘앙스·추론 등)를 도입하면 매우 인기 많을 것 같음
       특히 많은 사용자는 자신의 발화에 결속력(couhesion)이 부족한 걸 인지하지 못하니, 이러한 능력을 threshold proficiency의 핵심이라고 짚어주는 구조가 실질적 도움 될 것임
     * 앱 잘 만들었다고 칭찬하고 주변 10여 명에게 이미 추천
       몇 가지 궁금한 점:

     * LinkedIn에서 직원이 두 명뿐인데, 이 많은 언어는 어떻게 QA했는지?
     * Urdu는 꽤 잘 작동했으나, 왜 여성 목소리만 있고 남성 목소리는 없는지 궁금
     * Sesame팀은 인원이 더 많지만, 왜 Sesame의 목소리가 실제 사람처럼 자연스러운지에 대한 개발자 의견이 궁금(다국어 지원이 없는 Sesame보다 훨씬 더 큰 도전이라는 점 인정)
          + 칭찬에 감사
            가장 인기 있는 언어 위주로 테스트·조정
            사용자 피드백에 따라 문제 많은 언어는 실제로 제거 조치했고, 아직 미처 확인 못한 언어도 존재
            TTS 서비스(Openi, 11labs, minimax 등) 품질에 따라 목소리 종류가 결정
            일부 서비스는 남성 목소리가 없거나 여성 목소리만 좋음
            앞으로 더 다양한 목소리 추가 계획
            Sesame에서는 사용자의 실제 목소리를 TTS에 넣어 사용자 느낌과 분위기에 맞출 수 있는데, 우리는 아직 원래(기성품) TTS만 사용
            Sesame는 레이턴시도 굉장히 짧은데, 이 점은 언어 학습엔 오히려 단점
            향후 성숙한 음성-음성 모델에 기반한 경험을 제공할 목표
"
"https://news.hada.io/topic?id=21575","Apple, ‘F1’ 영화 위해 맞춤형 iPhone 카메라 모듈 개발","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Apple, ‘F1’ 영화 위해 맞춤형 iPhone 카메라 모듈 개발

     * Formula One(F1) 레이스카에 시네마 카메라를 장착하는 것은 불가능에 가까움
     * Apple은 F1 영화(Apple Original) 의 실감나는 POV 레이싱 장면 촬영을 위해, 실제 F1 카에 iPhone 카메라 센서와 A-시리즈 칩이 들어간 맞춤형 카메라 모듈을 개발함
     * 이 모듈은 2023, 2024년 실제 F1 경주 중 일부 차량에도 탑재되어 운용됨

맞춤형 카메라 모듈 설계

     * 외관은 기존 F1 중계 카메라 모듈과 유사하지만, 내부는 iPhone 카메라 센서와 A-시리즈 칩(추정: iPhone 15 Pro의 A17 Pro, 48MP 메인 센서), iPhone 배터리, 그리고 노출 조절을 위한 ND(Neutral Density) 필터 등으로 구성됨
     * 무게도 기존 모듈과 맞추어 차량 성능이나 규격에 영향이 없도록 설계됨
     * 극한의 속도, 진동, 온도 환경에서도 충격·진동·열 내구성 테스트를 통해 F1 기준을 초과하는 성능을 확보함

소프트웨어와 활용

     * iOS 기반에 맞춤형 카메라 펌웨어를 적용해, Apple ProRes 코덱과 log 포맷으로 촬영하여 영화 편집자들이 색 보정 및 후반작업을 자유롭게 할 수 있도록 설계함
     * 이 맞춤 펌웨어로 인해 iPhone 15 Pro에 실제로 log 인코딩 및 ACES(아카데미 컬러 인코딩 시스템) 지원 기능이 새로 도입됨
     * 무선 통신 기능이 없어, USB-C를 통한 iPad 전용 앱으로 현장에서 실시간 프레임레이트, 노출, 셔터 각도, 화이트밸런스 등 카메라 세팅을 조정하고 촬영을 제어함
     * 해당 모듈로 촬영한 영상은 영화 곳곳에 삽입됨

iPhone 영상 촬영과 영화 산업

     * 최근 Sean Baker의 'Tangerine', Danny Boyle의 '28 Years Later' 등 iPhone을 활용한 영화 촬영 사례가 꾸준히 늘고 있음
     * 촬영 현장에서는 조명, 서드파티 렌즈, 삼각대 등 다양한 장비와 세팅을 통해 스마트폰의 한계를 극복함
     * 스마트폰 카메라의 발전은 더 많은 창작자에게 영상 제작 기회를 제공함
     * 반면, Christopher Nolan의 'The Odyssey'처럼 IMAX 필름 카메라만을 고집하는 블록버스터 사례도 계속됨

맺음말

     * Apple은 ‘F1’ 영화 프로젝트를 계기로 하드웨어 혁신을 현장에 적용, 영화 제작자와의 직접 협업을 통한 신기능 개발을 실현함
     * Apple이 직접 맞춤형 미니 카메라를 제작한 사례는, 완벽한 샷을 위한 첨단 기술과 영화 산업의 융합을 보여줌
"
"https://news.hada.io/topic?id=21671","게임이 SteamOS에서 Windows 11보다 더 빠르게 실행됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  게임이 SteamOS에서 Windows 11보다 더 빠르게 실행됨

     * Ars Technica 테스트 결과, SteamOS는 테스트된 다섯 개 게임 중 네 개에서 눈에 띄는 프레임 속도 향상 현상
          + Returnal, Borderlands3, Cyberpunk 2077, Homeworld 3, Doom: The Dark Ages
     * Borderlands 3만이 Windows와 SteamOS 모두에서 유사한 성능 수준 보임
     * 기본 Windows 드라이버보다 SteamOS 기본 드라이버가 대체로 나은 결과 제공
     * SteamOS는 운영체제 오버헤드 감소 및 Proton 최적화 등에서 우위 드러냄
     * Microsoft도 게임용 Windows 최적화 기능 발표로 대응 움직임을 보이고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 성능 개선

     * SteamOS는 총 5개 게임 중 4개 게임에서 눈에 띄는 프레임 속도 증가 현상 확인됨
          + Returnal, Borderlands3, Cyberpunk 2077, Homeworld 3, Doom: The Dark Ages
     * 오직 Borderlands 3만이 두 운영체제에서 거의 비슷한 성능 결과를 나타냈으며, 이 게임에서는 Windows가 조금 더 높은 수치를 기록하는 경향이 있음
     * 일부 게임에서는 운영체제 변경만으로 프레임 속도 손실이 8%~36%에 달하는 모습
     * Homeworld 3 게임의 경우 Asus에서 제공한 그래픽 드라이버를 설치했을 때 Windows가 SteamOS의 낮은 그래픽 설정에서 성능이 유사해짐
     * 다른 네 개 게임에서는 Lenovo의 기본 Windows 드라이버가 SteamOS 드라이버에 비해 성능이 크게 떨어지는 것으로 확인됨

드라이버 업데이트와 성능 변화

     * Windows에서 Asus 드라이버를 수동 설치하면 전반적으로 성능 향상이 나타남
     * Homeworld 3의 'Low' 그래픽 설정에서 두 운영체제의 성능이 거의 동등 수준을 보임
     * 그 외의 테스트 결과에서, 드라이버를 최신으로 업그레이드한 Windows조차 SteamOS에 비해 8~36% 프레임 손실을 겪음

SteamOS와 Proton 최적화

     * SteamOS는 Proton 변환 계층을 통해 Windows 게임을 실행함에도, 실제로 Windows보다 우수한 성능을 보임
     * Valve가 Proton과 Mesa 그래픽 드라이버의 효율성을 지속적으로 최적화해온 결과로 분석됨

운영체제 오버헤드와 Windows의 대응

     * SteamOS에서 실행할 경우 불필요한 백그라운드 작업이 줄어들어 성능에 유리하게 작용함
     * Microsoft 역시 이러한 문제를 인식하고, 최근 ""Xbox Experience for Handheld"" 발표를 통해 게임 성능 최적화 방침(백그라운드 작업 최소화 및 비필수 작업 지연 등)을 공개함
     * 이로 인해, 앞으로 Windows 기반 휴대용 게임기에서도 더 높은 프레임 속도 제공 방향성 기대 가능함

        Hacker News 의견

     * 최근 몇 년간의 개인적인 경험상 게임 성능 순위는 이렇다는 의견 공유: 1위는 Linux에서 Proton과 Wayland(Niri)를 활용한 Steam, 2위는 Proton과 X11(Xfce) 조합, 3위는 Windows에서의 Steam, 4위는 기타 방식으로 Linux에서 실행한 게임이라는 평. Linux로 넘어왔을 때 가장 크게 느낀 점은 프레임 일관성 향상으로 순간적으로 끊기는 상황이 크게 줄어 게임이 훨씬 안정적이고 예측 가능한 느낌이라는 점. X11/Xfce에서 Wayland/Niri로 갈아탄 후 전반적인 프레임 상승 체감. 여러 번 도전하다 2023년 초에 성공적으로 정착한 것도 인상적이었다는 얘기. 다만 Proton이나 Wine으로 실행되다 보니 게임 실행 시간이 대체로 오래 걸리는 것은 어쩔 수 없는 부분임을 언급
          + 재미있는 점은, 리눅스 네이티브 포트가 있는 게임조차도 Windows 버전을 Proton으로 실행했을 때가 더 성능이 좋았던 경우들이 있다는 경험 공유. Civ5, Civ6, Cities Skylines(1)에서 모두 그랬다는 예시. 본인은 비게이밍용 하드웨어(Nvidia 3050 랩탑 GPU를 쓰는 노트북)라 성능 차이를 더 크게 체감함. Cities Skylines의 경우 Linux에서는 20fps 수준에 머무르지만 Windows에서는 45~60fps가 꾸준하다는 구체적인 수치 제시. Diablo 4도 Linux에서는 반응성이 너무 떨어져 사실상 플레이 불가. 고성능 게이밍 하드웨어를 보유한 유저들은 Linux로 충분하지만, 저사양 환경에서는 여전히 Windows가 유리하다는 입장
          + Niri는 정말 멋진 윈도우 매니저(WM)라는 칭찬. HN에서 Phoronix 기사로 overview 모드가 추가됐다는 소식을 보고 드디어 Sway에서 Niri로 전환. 전체 화면 게임이나 플로팅 윈도우에서 Niri가 X11(xwayland-satellite 사용 덕분일 수도 있음) 환경보다 렉과 걸림 현상이 훨씬 적다는 경험. i3status-rs를 지원하는 바(bar)를 찾는 게 힘들었고, 최종적으로 i3bar-river에 자리 잡았다는 소소한 팁
          + 다년간 Linux에서 게이밍을 해왔고, 프레임레이트 관련 의견 전반에 대체로 공감. ZFS(single NVMe)를 사용하면 Windows보다 훨씬 빠른 로딩 속도를 경험할 수 있음. 동일한 하드웨어에서 Windows를 쓰는 남편과 비교해 자주 게임이 10초 정도 더 빨리 로드되는 경우가 있었던 실제 사례 공유
          + Nvidia GPU 환경에서 Wayland가 실제로 쓸만하게 작동할 수 있는 방법이 있는지 문의. 시도할 때마다 항상 속도가 느리고 X11보다 시스템 전체가 무겁게 느껴져서 아쉬웠다는 솔직한 의견
          + Linux에서 Steam 게임이 실행 시간이 평균적으로 느려지는 건 Proton/Wine 때문이라는 점에 덧붙여, 본인의 개인적 체감으로 Steam 게임들이 Linux에서는 셰이더 컴파일을 CPU에서 하고, 최적화도 부족해 보인다는 생각. 반면 Windows는 미리 컴파일된 셰이더를 제공하거나 GPU를 활용하는 것 같고 그런 차이가 있음. 그래도 Wayland+Linux 쪽이 Windows보다 잔렉(stutter)이 훨씬 더 적고 안정적인 경험. 다만 이런 차이가 OS 차이 때문인지, 아니면 Windows에서는 이것저것 설치하며 시스템이 쓸데없이 무거워지기 쉬워서인지는 스스로도 확신 없음. 각 OS를 다루는 본인의 사용 목적도 차이가 크기 때문
     * 리눅스 게이밍이 완성되려면 마지막 남은 퍼즐 조각은 안티치트라고 주장. 주요 업체들은 커널 보안 미비로 지원을 꺼리고, 안티치트가 있다고 해도 게임 개발사들이 이를 허용하지 않는 경우가 많음(예로 Destiny). AAA 게임만 원활하게 돌아간다면 Windows는 완전히 버릴 생각. Steamos가 게이밍 역사상 최고의 혁신이라는 극찬
          + 최신 안티치트는 사실 임시방편에 불과하다는 주장. OS 보안 향상, 낮은 신뢰 환경에서 커널 레벨 안티치트가 계속될 수 없는 점, 그리고 항상 고양이와 쥐의 추격전처럼 변하는 구조 때문에 기존 방식(커널 훅 기반)은 한계가 분명함. 앞으로는 서버에서 모든 체크를 하고, 클라이언트에는 필요한 정보만 제공하는 방식 등으로 더 효과적인 대안이 가능하다고 제안. UT 같은 대표 게임에 이런 구조가 반영되면 구시대적인 방식은 자연스럽게 사라질 거라는 희망
          + 전용 서버 없는 멀티플레이 게임은 결국 한계라는 의견. 안티치트 데몬이 커널에 침투해 파일이나 메모리 감시하는 방식을 원하지 않음. 전용 서버를 둔 커뮤니티가 중앙화된 매치메이킹보다 훨씬 효율적으로 플레이어를 관리해준다는 경험 기반 주장
          + 특히 Epic은 리눅스 지원 거부를 복잡성 탓으로 돌리지만, 실제론 Steam이 사실상 표준 스토어라는 점이 싫어 배제하는 측면도 있다는 해석
          + Easy Anti Cheat, Battle Eye가 몇 년 전부터 Linux도 네이티브 지원 중인데, 실제 활성화 여부는 게임 개발사가 결정하는 사실을 상기. 안티치트 적용 게임 중 약 40%가 Linux에서 동작하며 areweanticheatyet.com에서 확인 가능
          + 과거 Steam 인기도를 견인했던 Counter-Strike 등의 Valve Anti-Cheat(VAC) 같은 기술이 있었던 시적 참회. VAC가 왜 시대 변화에 맞춰 발전하지 못했을까라는 의문. Linux 시대에 맞춰 VAC를 다시 투자하고 Easy Anti Cheat 같은 대항마로 육성하면 좋겠다는 바람
     * Windows 게임이 SteamOS에서 Proton으로 더 빠르게 구동된다면, 개발자들은 Windows가 아니라 SteamOS API를 우선적으로 고려해야 한다는 주장. 이렇게 하면 Windows와의 호환성도 챙기며 성능도 극대화 가능하다고 봄. Unity, Unreal 등 주요 게임 엔진이 SteamOS를 메인 타깃으로 CI, 테스트를 강화해야 한다는 제안. Valve가 SteamOS CI/CD 팜을 운영하는지 궁금하며, Rust 기반 템플릿 및 라이브러리로 크로스플랫폼 빌드/테스트도 가능할 것 같다는 기대감 표현
          + 이에 대한 반론으로, Windows API가 게임 동작의 기준(True Source)이라는 점을 강조. Windows에서 동작하지만 Proton에서 안 되는 경우엔 Valve가 Proton 쪽을 수정하지만, 반대로 Proton에서만 되고 Windows에서 안 되면 결국 게임이 깨질 위험. Proton에서는 Windows와의 궁합이 안 맞는 기능 사용을 자제하고, 게임 테스트 시엔 Steam Deck 등의 환경도 고려해야 하지만, 여전히 Windows 우선 개발 기조를 지키는 게 바람직하다는 입장
          + SteamOS 환경에서 유일하게 안정적인 ABI는 Win32이므로, SteamOS만 타깃으로 개발해선 장기적으로 호환성 문제 발생 소지가 크다는 지적
          + Epic이 Unreal 엔진을 보유하고 있기 때문에, SteamOS 및 그 API에 최적화하는 것을 기꺼워할지 의문이라는 의견. Epic Store와 Steam 간의 경쟁 구도라는 배경도 언급
          + 시장 대부분(99%)이 Windows를 기준으로 돌아가고 있다는 현실적인 지적. Proton도 결국은 Win32 구현이기 때문에 본질적으론 Windows를 타깃으로 하는 것임을 강조
     * Windows XP 시절 VMWare VM을 통해 리눅스 위에 Windows를 띄워 사용했더니, 오히려 동일한 하드웨어에서 Windows만 썼을 때보다 빨랐던 신기한 경험 사례 공유
          + 그 원인으로 디스크 캐시, 특히 캐시 정책 차이에서 성능 차이가 날 수 있다는 해석
     * 최근 Arch(steamOS 기반 아님)로 갈아타 봤고, 꽤 견고한 경험이었다는 평가. 다만 out-of-the-box로 되는 건 아니고, 게임마다 약간의 세팅은 필요하다는 점을 솔직하게 설명. 실행 명령어에 파라미터 추가 정도라 도전적이지 않았고, Proton DB 및 커뮤니티 댓글에서 필요한 팁을 거의 다 얻을 수 있음을 안내. 다시 Windows로 돌아갈 의향은 거의 없다는 만족감 표출
     * 약 10-15년 전 Windows와 Linux(Wine)에서 동일 게임을 번갈아 쓰며 세이브 파일 100~200개를 저장했었는데, 놀랍게도 Linux(Wine)에서 세이브 리스트 로딩이 Windows보다 두 배 더 빨랐던 경험. NTFS가 Linux의 네이티브 파일시스템이 아님에도 이런 차이가 발생한 배경을 이해하기 어렵다는 궁금증 표출
     * Steamos와 Ganoo/L00nockz(아마 GNU/Linux를 유머러스하게 표기) 등이 게이밍 플랫폼으로 완벽히 자리잡으면, 2012년 이후 처음으로 게임용 PC 조립할 계획임을 밝힘. Mac을 쓰는 입장이고 유닉스 기반이라 개발에는 만족하지만, 게임 경험은 여전히 Linux보다도 뒤처진다는 아쉬움. AAA 게임 출시와 GPU 드라이버 안정화가 완료되면 5년 이내로 변화가 크게 올 것으로 기대
          + AAA 게임은 사실 수년 전부터 잘 동작하고 있고, Steam 클라이언트와 AMD GPU를 활용하면 Linux도 이미 훌륭한 게이밍 플랫폼이라는 주장
          + Steam Deck 출시 이후로는 사실상 모든 게임이 Linux에서 잘 돌아간다는 평가. 물론 의도적으로 깨지게 만든(anti-cheat 연동 등) 일부 게임만 예외. protondb.com에서 호환성 확인 가능하며, 상위 300개 Steam 게임 중 실제로 안 되는 건 17개, 그 중 5개는 유틸리티에 불과하다는 실제 데이터 제시
          + Windows가 유닉스 기반이 되면 개발과 게임 모두에서 양쪽의 장점을 누릴 수 있어 항상 바래왔던 변화라고 밝힘. 그만큼 지금 현실이 많이 가까워졌다는 긍정적 체감
     * Windows 커널도 타 OS에 비해 느리다는 링크와 HN 토론글 몇 가지 블로그 글 관련 HN 토론 공유하며 상황을 풍자적으로 언급
     * Proton을 ""translation layer""(변환 계층)라고 부르기엔 어폐가 있다는 주장. Win32 API가 시스템콜 단위가 아니라 DLL에 등록된 함수 집합이고, Linux의 Proton은 이 Win32 API를 Linux 시스템콜로 구현하는 DLL을 제공하고, Windows는 자기 시스템콜을 쓰는 DLL을 쓸 뿐이라는 구조적 차이 설명
          + Wine 공식 홈페이지에서도 런타임에 호출을 번역하는 '호환 계층'이라고 직접 밝힌 만큼, translation layer라는 표현도 크게 틀린 건 아니라는 반론
          + Wine(Proton 포함)의 끈질긴 개발 역사에 경의 표명. 한때는 해결책이라면서 새로운 문제만 만들어낸다는 조롱도 있었지만, 이제는 Windows를 대체할 강력한 무기라고 평가
          + sscanf() 같은 함수도 호환을 위해 불필요하게 복잡하게 구현된 적이 있는지 질의하는 유쾌한 태도
          + Proton/Wine이 여러 NT 시스템콜조차 직접 구현하는데, 사실상 Windows 프로그램들도 그런 시스템콜을 직접 사용하는 경우가 많다고 지적
          + Wine의 본질은 Windows ABI(이진 인터페이스)를 Linux OS와 유저랜드로 번역하는 것이라는 기초 설명. 즉, 번역 행위 자체가 이식의 핵심임을 강조
     * 20~30% 정도의 성능 차이를 예상했는데 실제로 200~300% 수준이라 충격이었다는 의견. Microsoft가 불필요한 기능을 걷어낸 ‘게이밍용 Windows’를 출시해주길 바란다며, 본인 역시 요즘은 Windows를 오로지 Steam 실행용으로만 쓴다고 밝힘
          + 게임 성능 극대화에 동감하지만, 게이머들도 게임 외에 수많은 다른 용도를 PC에 기대한다는 점에서 굳이 게이밍 전용 Windows를 분리할 필요는 없다는 의견. 대신 현행 Windows 자체가 게임을 더 효율적으로 돌릴 수 있도록 최적화에 집중해야 한다는 주장
"
