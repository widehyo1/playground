"https://news.hada.io/topic?id=21973","Show GN: 바이브 코딩으로 간단한 숫자 퍼즐 게임을 만들어 봤습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Show GN: 바이브 코딩으로 간단한 숫자 퍼즐 게임을 만들어 봤습니다.

  소개

   바이브 코딩으로 간단한 숫자 게임을 만들어 보았습니다.
   저는 머신러닝 엔지니어라 프론트엔드나 백엔드 지식은 부족해서; AI 주도 개발로 어디까지 할 수 있나 테스트 해보는 기회가 되었습니다.

   게임 룰은
    1. 합이 10이 되는 두 숫자나 같은 숫자를 선택해서 지울 수 있습니다.
    2. 두 숫자는 가로세로나 대각선으로 인접해야 합니다.
    3. 단 인접하지 않아도 두 숫자 사이에 빈 칸만 있으면 지울 수 있습니다.
    4. 두 숫자 사이에 줄이 바뀌어도 그 사이에 숫자가 없다면 가로로 인접한 걸로 인정합니다 (줄바뀜은 왼쪽-> 오른쪽만 인정!)
    5. 지울 숫자가 없다면 숫자를 추가할 수 있습니다
    6. 모든 숫자를 지우면 게임을 승리합니다.

   입니다.

  개발환경

   Cursor IDE를 사용했습니다. 기본 세팅으로 얼마나 가능한지 보려고 모델 지정이나 프롬프트 세팅은 하지 않았습니다.

  개발후기

    1. 간단, 사소한 작업엔 최고
       프론트엔드나 사운드, 애니메이션 효과는 AI가 추천한 걸 그대로 사용했습니다. 제가 했다면 구현 뿐만 아니라 어떤 거로 할지 고민에도 시간이 많이 소요됐을텐데, AI가 적당히 게임 플레이에 맞게 구현해 준 덕에 작업 시간을 대폭 줄일 수 있었습니다.
    2. 논리적인 작업은 글쎄?
       게임 룰을 잘 설명해줘도 게임 로직과 맞지 않는 구현을 하거나 다른 룰을 추가하면 기존 기능을 망가뜨리는 경우가 잦아서 확인과 직접 코드 수정이 필요했습니다. 프롬프팅을 좀더 잘 하거나 좀 더 좋은 모델을 썼다면 몰랐겠지만요.

  마치며

   AI 덕분에 간단한 프로토타입은 몇 시간 동안 공부하지 않고도 만들 수 있게 된 것 같습니다. 하지만 제대로 쓰기 위해선 역시 사람이 개입해야 하는 부분도 많다는 걸 느낄 수 있었습니다. 어쨌든 게임 플레이 많이 해주시면 감사하겠습니다ㅎㅎ

   숫자 출현이 랜덤이 아닌가요? 비대칭이 꽤 있네요.

   네 숫자 추가는 남아있는 숫자를 순서대로 추가해주는 형식입니다.

   아 그리고 모바일 최적화는 AI가 못한 것 중 하나라; 컴퓨터나 태블릿으로 해주시면 감사하겠습니다.
"
"https://news.hada.io/topic?id=22011","PHP, 라이선스를 BSD 3-Clause로 변경 제안중","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    PHP, 라이선스를 BSD 3-Clause로 변경 제안중

     * PHP 프로젝트는 기존 복잡하고 비호환적인 PHP 고유 라이선스와 Zend Engine 라이선스를 BSD 3-Clause(수정 BSD 라이선스) 로 일원화하는 RFC를 논의 중임
     * 새 라이선스 적용 시점은 PHP 9.0으로, 소스 코드·헤더·문서 전반에 BSD 3-Clause가 반영되며, 과거의 특수 조항 및 브랜드 관련 제한이 사라짐
     * OSI·FSF 승인, GPL 호환 등 법적 명확성이 확보되고, 기여자 및 사용자 권리는 기존과 동일하게 보장됨
     * 라이선스 변경을 위해 PHP Group, Perforce Software(구 Zend) 의 공식 동의가 필요하며, 커뮤니티 논의 후 6개월 이상 논의 및 투표 절차를 진행함
     * 이 변경은 PECL/확장 등 외부 프로젝트에도 BSD 3-Clause 선택을 권장하며, “PHP 라이선스” 사용은 권장하지 않음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

     * PHP 프로젝트는 오랜 기간 자체적인 오픈 소스 라이선스와 Zend Engine License로 인해 혼란과 논란이 지속되었음
     * 특히 Zend 디렉토리의 소스에 적용되는 Zend Engine License는 OSI 승인 라이선스가 아니어서 복잡함을 더함
     * 이 RFC는 모든 PHP 기여자의 저작권을 보존하면서도 사용자에게 기존 라이선스와 동일한 권리를 부여하는 실용적인 라이선스 단순화를 제안
     * BSD 3-Clause(수정 BSD 라이선스) 를 새로운 공식 라이선스로 채택해, 권리·사용 조건은 유지하면서 복잡성과 오해를 줄이는 것이 목표

제안과 주요 변경 사항

     * 문제의 본질은, 새로운 버전의 PHP License와 Zend Engine License를 공개하여 Modified BSD License(BSD-3-Clause, OSI/FSF 모두 승인) 를 공식적으로 채택하는 것임
     * 기존 PHP License(version 3.01)과 Zend Engine License(version 2.00)는 특수 조항만 제외하면 Modified BSD와 사실상 동일하며, 권한의 본질적 변화는 없음
     * 라이선스 업데이트 이후:
          + 기여자 및 사용자에게 부여되는 권한에 변화 없음
          + PHP Group, Perforce Software와 협력하여 특정 그룹 고유 조항 제거
          + PHP 및 Zend Engine은 OSI 승인, GPL 호환 라이선스 하에 제공됨
     * 구 PHP License와 Zend Engine License 사용은 더 이상 권장되지 않음
     * LICENSE 및 소스 내 라이선스 헤더 역시 새 포맷으로 교체됨

라이선스 전문 요약

     * BSD 3-Clause는 자유롭게 복사·수정·배포 가능, 단 저작권 및 면책 조항, 명칭·브랜드 무단사용 금지 조건이 포함됨
     * BSD-3-Clause는 OSI(오픈소스 이니셔티브), FSF 모두에서 승인된 자유 소프트웨어 라이선스이자 GPL 호환임

변경 절차 및 승인

     * RFC는 커뮤니티 공개 논의 후 투표로 확정되며, 공식 동의·투표 이후 적용이 진행됨
     * 라이선스 변경은 PHP Group 및 Perforce Software의 공식 동의가 필요함
     * 과거 소스코드 기여자 권리는 그대로 유지되며, 변경이 기존 권한을 침해하지 않음
     * 커뮤니티에 6개월 이상 논의 기간을 부여한 후 투표로 확정함
     * 변경은 PHP 9.0에서 정식 반영 예정

배경 및 역사적 맥락

     * 초창기 PHP 1·2는 GPL, 이후 Apache 라이선스·커스텀 BSD 기반 라이선스를 거쳐 발전함
     * Zend Engine은 별도 라이선스를 유지했으나, 현재는 사실상 분리 불가한 한 프로젝트로 간주됨
     * 기존 PHP 라이선스의 명칭 사용 제한, 브랜드 보호 조항 등은 타 오픈소스와의 호환성과 배포에 지속적으로 문제를 일으켜 왔음

기존 코드, 확장, 문서 영향

     * 이번 RFC는 php-src 전체(별도 라이선스가 명시된 코드는 제외) 에 적용되며, PECL/확장 등도 BSD 3-Clause 채택을 권장함
     * 신규/기존 PHP 소스 리포 내 PHP License나 Zend Engine License 적용 코드 전체에 영향
     * 기존 라이선스(z.B. timelib 등 별도 라이선스 코드)는 해당 변경의 적용 대상 아님
     * PHP 매뉴얼은 Creative Commons Attribution 3.0 이상 라이선스 계속 유지
     * 기존 확장 모듈/소프트웨어는 PHP License v4(Modified BSD) 적용 선택권 부여
     * 향후 확장 및 새 프로젝트에는 최신 BSD/Apache 등 공인 라이선스 사용 권장

결론

     * PHP 및 Zend Engine의 라이선스 구조가 3-clause BSD로 단순화되어 오픈 소스 생태계 내 명확성, 호환성, 상업적 활용, 법적 안정성이 강화될 전망임
     * 본 제안이 승인 및 적용되면, 사용자는 BSD-3-Clause 기준으로 PHP와 Zend Engine을 자유롭게 이용할 수 있음
     * 프로젝트 내 기여자, 커뮤니티, 주요 기업의 동의와 투표 절차가 완료되어 공식적으로 적용 예정임

        Hacker News 의견

     * Meta가 사용하는 언어가 PHP가 아니라 Hack임을 알려줌, 그런데 Hack의 패키징과 문서화, 가용성이 별로임을 언급함, 그 이유로는 Meta 내부에서 보이는 부분이 아니라서 성과평가에 반영되지 않기 때문임, 또 내부 지식 은닉이 곧 일자리 보장과 연결됨을 지적함, 그리고 라이선스 측면에서 Meta, Google, Microsoft, Apple 등 대형 IT 기업들은 AGPL 소프트웨어 사용을 엄격하게 금지함, AGPL의 “Remote Network Interaction” 조항의 모호함 때문에 법적 리스크를 감수하지 않으려는 이유임, 만일 대기업이나 일반적인 사업자가 절대 내 코드를 못 쓰길 원한다면 AGPL을 선택하라는 제안을 덧붙임, 참고: Google의 AGPL 정책 문서
          + 많은 회사들이 AGPL 소프트웨어(예: Grafana, Mastodon, Mattermost 등)를 실제로 내부에서 사용함을 강조함, 다만 외부 유료 고객을 위한 서비스로는 사용 빈도가 적음을 언급함, 개발자로서 나는 거대 기업의 과도한 걱정보다는 내 소프트웨어 사용자들의 자유를 더 중요시함을 밝힘
          + ‘모든 사업자’가 아니라, ‘내 소프트웨어로 독점 네트워크 서비스를 제공하는 회사’만 AGPL의 영향을 받음을 지적함, AGPL의 핵심 의도가 바로 이것임을 설명함, 구글의 정책 근거도 이 때문에 네트워크 제공사임을 명확히 적시함, 대부분 비기술 기업에는 아무 영향이 없으며 신경 쓸 필요도 없음을 덧붙임
          + 오픈소스 스타트업이라면 AWS 같은 메가클라우드에 독식당하는 걸 막으려면 AGPL + 커머셜 듀얼 라이선스(지식재산권 이전 CLA 포함)를 권장함
          + 많은 대기업이 AGPL 소프트웨어를 사용하는 이유는 듀얼 라이선스가 가능하기 때문임을 설명함, 즉 AGPL로 오픈소스임을 광고하면서도 커머셜 라이선스로 이용 시 고객사에 유료 과금이 가능함
          + 최근에는 Go를 많이 쓰고 있다는 생각을 했음, 많은 패키지들이 Go로 다시 쓰여진 것 같았다는 인상을 남김
     * PHP 라이선스와 관련된 내용과 그 역사가 마케팅이나 AI 생성된 내용 없이 한 자리에 정리돼 있어서 매우 좋다는 소감을 전함
          + AI 생성 콘텐츠는 사실 추가적인 정보를 주지 않으며, 쓸데없는 말은 원래부터 존재해왔음, 본질적으로 새로울 것이 없다는 유쾌한 의견을 덧붙임
     * 25년 전 PHP Zend Engine의 소스코드를 직접 공부했을 때, 인생 최초로 삼중 포인터(zval***)를 접한 기억을 떠올림, 이후 PHP로 다양한 일들을 해봤으며, 고등학생 시절 프로그래밍 대회에도 CLI 환경에서 PHP를 사용해서 출전했으나, 당시 스탭들이 언어와 환경에 익숙하지 않아 탈락한 웃픈 경험이 있음, 그 당시 PHP가 허락해준 가능성에 감사를 전함
          + 이 이야기를 재미있게 느꼈으며, 본인은 졸업 프로젝트로 Perl을 활용했던 경험을 공유함
          + 삼중 “네이키드” 포인터에 대해 논리적으로 납득할 접점을 찾기 어렵다고 밝힘, 퍼포먼스 이전에 암시적 간접 참조의 복잡함이 설명이 어려움, 예를 들어 struct의 멤버처럼 명확한 것은 이해 가능하나, 괜히 복잡성을 추가하는 것이 비합리적임, “왜 단순하지 않지?”라는 말을 지인이 자주 했다며 회상함
     * 모든 기여자의 동의를 받지 않을 경우 악의적 기여자가 문제를 일으킬 수 있다는 우려가 있음, 미국 등지에서는 악의적으로 괴롭히기 위한 소송도 얼마든지 가능하며, 모두가 본인 비용으로 대응해야 하니 결과적으로 법적 방어에 과도하게 신경 쓰는 문화가 생김을 이야기함, 그리고 사이드로 리처드 스톨만과 PHP의 GPL 사용 및 그로 인해 듀얼라이선스가 중단된 고전적인 일화를 언급함
          + PHP Group은 “or later” 조항 덕분에, 따로 기여자 동의 없이 라이선스 내용을 수정해서 새 버전을 공개할 수 있음을 설명함
          + 스톨만과의 라이선스 관련 일화가 언급된 주자료가 사실은 MySQL의 GPL 전환과 이로 인한 PHP 라이선스 영향에 가깝다는 점을 지적하며, 스톨만이 싫어해서 GPL을 버릴 이유가 납득이 안 감을 표현함
     * 관련 배경은 PHP 위키의 라이선스 변경 배경 문서에서 확인 가능함
     * 소프트웨어 라이선스와 수정에 대한 전문 지식을 쌓고 싶다면 꼭 읽어봐야 할 페이지임을 추천함, 그리고 이번 라이선스 변경이 우리에게 변화나 재인증을 요구하지 않는 뉴스거리임을 강조함, 기여자 및 사용자 모두 영향 없음
          + 큰 변화 없이 진행된다는 뉴스가 실제로 엄청난 변화를 동반했던 787MAX와 MCAS 사태를 예로 들어 현실적 경각심을 유쾌하게 표현함
          + 실제로 변경된 내용은 PHP/Zend 트레이드마크 관련 문구들이 삭제되고, 두 프로젝트를 하나의 라이선스로 통합하는 과정임을 세부적으로 설명함, 즉 “PHP”, “Zend”, “Zend Engine” 명칭을 사용하려면 기존엔 각각의 별도 승인 필요였으나 이제는 저작권자 및 기여자 명칭에 대해 일괄 적용하도록 바뀜, 또 표기·개정·인증·알림 조항(4~6번)도 함께 삭제됨을 조목조목 안내함
     * 왜 라이선스 문건에서 중요한 부분을 전부 대문자(ALL CAPS)로 작성하는지 궁금증을 제기함
          + 미국 법률상 보증/책임 관련 면책 조항은 “conspicuous(눈에 띄는 것)”이 요구됨, 평문에서 가장 쉬운 표시 수단이 대문자임을 설명함
          + 대·소문자 논란 자체를 없애기 위한 방법임을 말함, 모든 단어를 대문자로 쓰면 전부 강조가 되니까 혼동이 줄어듦
          + 상업법(UCC) 조항에 따르면, 합리적인 사람이 반드시 인지할 수 있도록 적혀 있어야 하며, 그 한 가지 방식으로 제목이 크게 대문자인 경우가 있음, 그래서 전체를 대문자로 하면 법원도 그 중요성을 ‘눈에 띄게’ 인식한다고 판단할 수 있음
     * 변화에 대해 잘 아는 사람이 ELI5 수준으로 설명해주길 요청함, PHP 전체 라이선스가 바뀌는 것인지 궁금해함
          + “PHP 전체”가 정확히 무슨 뜻인지 물으며, 이번 변경은 언어 자체, 즉 인터프리터와 런타임, 표준 라이브러리에 적용되는 것임을 설명함
     * MIT 라이선스가 훨씬 간단한데 왜 그걸 사용하지 않는지 궁금함을 밝힘
          + MIT와 BSD-3조항의 차이가 정말 실질적인 의미에서 느껴질 만큼 간단한지, MIT 라이선스와 BSD-3조항 라이선스 중에서 의미 있는 차이가 있는지 반문함
"
"https://news.hada.io/topic?id=21987","간략한 자바스크립트 역사","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             간략한 자바스크립트 역사

   요약 개요: JavaScript 30년의 발전사와 웹 생태계 변화
     * JavaScript는 10일 만에 개발된 단순 스크립트 언어에서 세계적 표준 언어로 성장함
     * 웹 브라우저 전쟁, ECMA 표준화, Node.js 및 프레임워크 등장으로 생태계 확장
     * 프론트엔드·백엔드·모바일·AI까지 활용 영역 확장
     * 오픈소스와 커뮤니티 주도의 진화, 성능 개선 및 모듈화 흐름이 핵심
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  서론: 단순 스크립트에서 웹의 표준으로

     * JavaScript는 1995년 Netscape가 웹 상호작용을 위해 만든 객체지향 스크립트 언어로 시작됨
     * 초기엔 HTML 보조 수단이었지만, 브라우저 확산과 더불어 빠르게 성장
     * Java와 이름이 유사한 이유는 마케팅적 전략
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  본론: 주요 역사와 기술 발전 흐름

    1. 초기 브라우저 전쟁과 표준화

     * 1996~1997년: Microsoft는 JavaScript에 대응해 JScript 발표
     * Netscape는 생태계 혼란을 막기 위해 ECMA에 표준화 요청 → ECMAScript(ES) 명명
     * 1999년 ECMAScript 3 발표로 웹 프로그래밍의 기반 형성

    2. 웹 애플리케이션 시대의 도래

     * 1999~2005년: XMLHttpRequest, JSON, JSDoc, AJAX 등장 → 비동기 웹 전환
     * Gmail, MDN, jQuery 등의 탄생은 Web 2.0을 이끔
     * 브라우저 간 호환성 이슈 해결을 위한 라이브러리·문서화 확산

    3. JavaScript의 서버 확장

     * 2009년 Node.js 발표로 서버 측 JavaScript 본격화
     * CommonJS, Express.js, npm, io.js 등 생태계 구성요소 다수 등장
     * ECMAScript 5로 안정성과 기능 확장 (strict mode, JSON 지원 등)

    4. 프레임워크 시대와 도구 진화

     * 2010~2015년: AngularJS, React, Vue, Backbone 등 다양한 프레임워크 등장
     * 모듈 번들러(Webpack), 정적 분석 도구(ESLint), 코드 포매터(Prettier) 대중화
     * ECMAScript 6(2015)은 class, arrow function, import/export 등 핵심 기능 도입

    5. 현대적 개발 환경과 실행 플랫폼 확장

     * TypeScript, WebAssembly, Next.js, GraphQL, Redux 등 현대 앱 개발 기반 등장
     * Electron, VSCode, Cloudflare Workers, AWS Lambda로 실행 환경 다변화
     * Node.js와 io.js 통합 및 OpenJS Foundation 설립으로 커뮤니티 통합

    6. 최근 동향과 미래 방향

     * Bun, Deno 같은 고성능 런타임의 등장
     * TypeScript의 Go 포팅(tsgo)으로 대규모 프로젝트 대응
     * JSR(모듈 레지스트리), #FreeJavaScript 캠페인 등 생태계 자율성 확보 노력
     * VSCode Copilot 오픈소스화 → AI 기반 개발 환경 본격화
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

  결론: JavaScript의 미래는 더욱 확장적

     * 지난 30년간 JavaScript는 끊임없는 개선과 혁신을 통해 범용 언어로 성장
     * 프론트엔드, 백엔드, 데스크탑, 모바일, 엣지, 머신러닝 등으로 확장
     * 오픈소스, 성능, 표준, 커뮤니티가 JavaScript 발전의 핵심 동력
     * 향후 30년은 더 빠르고 스마트하며 개방적인 웹 환경 구축을 지향할 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   필요하시면 연도별 정리표나 분야별 발전 요약도 추가해드릴 수 있습니다.

   한글 번역본은 아래와 같습니다.
   https://roy-jung.github.io/250701-history-of-js/
"
"https://news.hada.io/topic?id=22039","(번역)자바스크립트가 웹을 망가뜨렸습니다. (그리고 이를 진보라고 불렀습니다)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              (번역)자바스크립트가 웹을 망가뜨렸습니다. (그리고 이를 진보라고 불렀습니다)

   요약: CharGPT

   🧨 핵심 요지
   • 웹이 무겁고 불안정해졌다
   자바스크립트 프레임워크들이 무분별하게 도입되면서 웹사이트는 느려지고, 렌더링 지연이 발생하며, 유지보수성도 악화됨에도 이를 ‘진보’라고 포장한다 ￼.
   • 개발자 경험(DX)이 사용자 경험(UX)을 압도
   도구와 프레임워크는 개발자 편의에 집중되었지만, 이는 복잡성을 증가시키고 콘텐츠 생산자나 SEO 전문가들의 접근을 방해 ￼.
   • 불필요한 복잡성의 확산
   단순한 콘텐츠 사이트도 빌드 프로세스, 번들러, 하이드레이션, 라우터 등 복잡한 구조를 갖추도록 강제됨. 그 결과 단 몇 줄의 텍스트를 위한 시스템이 마치 항공 교통 관제 수준의 복잡성을 띠게 됨 ￼.
   • 복잡성을 되돌리려는 움직임
   최근 SSR(Server‑Side Rendering)이나 전통적 CMS 방식이 다시 주목받고 있지만, 여전히 기존보다 무겁고 취약한 구조에 의존되고 있음 ￼.
   • 현실적인 해결 방안 제안
   모든 사이트가 JS 앱이 될 필요 없음.
   – 서버 렌더링 HTML + 시맨틱 마크업 + 엣지 캐싱
   – 경량 자바스크립트는 필요한 곳(모달, 가격 토글 등)에만 최소한으로 사용
   – 워드프레스, Eleventy 같은 도구 활용 권장 ￼.
   • 결국 의사결정의 문제
   현재의 복잡성은 우연이 아니라 개발자 위주의 문화와 조직적 선택의 결과임.
   UX와 비즈니스 결과를 우선시하는 마인드셋으로 전환할 것을 강조 ().

   ⸻

   ✅ 요약 결론
   1. 자바스크립트에 모든 걸 맡기지 말자 – 단순 콘텐츠 중심의 사이트는 과도한 JS 없이도 충분히 구축 가능하다.
   2. 사용자·콘텐츠·SEO 우선 설계 – DX보다 UX와 콘텐츠 업데이트의 용이성에 초점.
   3. 필요한 곳에만 JS 사용 – 불필요한 라이브러리 과잉 도입 대신, 가벼운 Vanilla JS나 CSS만으로 해결 가능한 기능은 그렇게 구축.
   4. 조직 문화의 전환 – 복잡성은 코드보다 더 많은 비용을 초래하므로, 합리적이고 실용적인 아키텍처 우선.

   ⸻

   이 글은 “현대 웹 개발” 문화에 대한 경종을 울리며, ‘더 많은 것은 항상 더 좋은 것이 아니며, 적절한 도구의 선택이 중요하다’ 는 메시지를 강하게 전달합니다.

   과도한 JavaScript 중심 개발, 웹을 망가뜨리다
   글의 원문 URL이 일주일전에 올라와서 여러 의견들이 있었습니다.

   ㅠㅠ 죄송합니다. 찾아본다고 찾아보고 올린건데…
   “resolved duplicated” 어떻게 하나요?

   저도 글 등록할때 해당 글들이 등록이 되어 있는지 검색 열심히 하고
   올립니다. 여기 사이트들도 주인장님께서 워낙 글들을 많이 올리셔서 대부분 걸립니다.
   검색하면...

   해커뉴스
   https://news.ycombinator.com/item?id=44325563

   링크드인
   https://linkedin.com/posts/…

   레딧
   https://reddit.com/r/theprimeagen/…

   페이스북 유용우님의 글…
   https://www.facebook.com/share/16q3Qxbm88/?mibextid=wwXIfr
"
"https://news.hada.io/topic?id=22001","AI 로 기획자는 대체될까?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            AI 로 기획자는 대체될까?

   비약적으로 발전하고 있는 AI 기술은 다양한 산업분야에 빠르게 적용되고 있다. IT 직군 또한 예외 대상은 아니다. 특히 Cursor AI 와 같은 AI 기반 자동화 코드 편집 서비스의 등장으로 인해 개발자들 사이에선 AI 가 개발자를 대체할까? 를 놓고 갑론을박이 형성되고 있고, 기획자들 사이에도 동일한 갑론을박을 왕왕 볼 수 있다. 같은 맥락으로 AI 가 기획자를 과연 대체할까?
"
"https://news.hada.io/topic?id=22050","하나의 휴대폰, 세 가지 형태. 2025년 모듈형 컨셉트 휴대폰 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  하나의 휴대폰, 세 가지 형태. 2025년 모듈형 컨셉트 휴대폰

   요약 개요
     * 하나의 스마트폰이 세 가지 화면 형태로 변환 가능
     * 플래그십 성능과 모듈형 디자인을 결합
     * 휴대성과 몰입형 대화면 경험을 동시에 제공
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    서론: 차세대 스마트폰의 등장

     * 한 기기로 다양한 화면 형태(미니, 컴팩트, 초광각)를 지원
     * 간단한 제스처로 화면 전환 가능
     * 스마트폰 기술 발전과 사용자 맞춤형 경험을 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    본론: 핵심 특징

   1. 플래그십 성능과 미니 사이즈
     * 작은 크기에도 최고 수준의 성능 제공
     * 휴대성을 유지하면서 고성능 스마트폰의 기능 구현

   2. 모듈형 디자인의 확장성
     * 화면 및 기능 확장이 가능한 모듈형 구조
     * 다양한 활용 시나리오에 대응 가능

   3. 다중 화면 경험
     * 대화면으로 몰입형 콘텐츠 감상 가능
     * 소형 화면으로 휴대성 극대화
     * 하나의 기기로 다양한 환경과 용도에 적응
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    결론: 미래 스마트폰의 방향성

     * 다기능성과 맞춤형 경험이 결합된 스마트폰의 진화 형태 제시
     * 한 기기에서 무한한 활용 가능성을 제공하는 차세대 모델로 평가 가능

   댓글 요약 개요
     * 주요 의견은 모듈형 스마트폰의 현실성, 기능성, 소형화 요구, 혁신성, 추가 제안 등 5가지로 구분됨.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    1. 현실성 및 상업성 비판

     * 컨셉은 흥미롭지만 실제 제품화 가능성에 회의적.
     * 기업의 수익 구조와 마케팅 전략 때문에 실현되지 않을 가능성 언급.
     * 배터리 용량 부족(1800mAh)과 발열, 내구성 문제 우려.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    2. 모듈형 설계 한계 지적

     * 외부 모듈로 RAM 확장은 불가능하다는 기술적 지적.
     * 모듈화로 인해 복잡성과 비용이 증가할 수 있다는 의견.
     * 화면·배터리·카메라 중심의 제한적 확장이 현실적이라는 주장.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    3. 소형 폰 수요 강조

     * 단순한 소형 스마트폰 출시를 원하는 의견 다수.
     * ""5.6인치 폰만 나오면 좋겠다"" 등 소형화 자체가 핵심 요구라는 주장.
     * 대형화나 복잡한 컨셉보다 실용적 소형 기기 선호.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    4. 혁신적 아이디어 긍정 평가

     * LG Wing, 과거 Asus 모듈형 컨셉 등 독창적 시도와 비교하며 긍정적 평가.
     * 소형 기기와 태블릿을 연결하는 아이디어가 흥미롭다는 의견.
     * “폰 디자인이 다시 개성 있어지는 것이 반갑다”는 반응.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    5. 추가 기능 및 개선 제안

     * 3.5mm 오디오 잭, microSD, 다중 색상, 듀얼 카메라 추가 요청.
     * 상·하위 모델(성능/저전력)로 라인업 구분 제안.
     * 5000mAh 이상 배터리, MagSafe 파워뱅크, AMOLED/LCD 옵션 제시.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   모듈형 스마트폰... lg 으윽 머리가...
"
"https://news.hada.io/topic?id=21985","Claude Squad - 멀티 AI 코드 에이전트 터미널 워크스페이스 관리 툴","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Claude Squad - 멀티 AI 코드 에이전트 터미널 워크스페이스 관리 툴

     * Claude Code, Codex, Gemini, Aider 등 다양한 AI 코드 에이전트를 한 터미널에서 동시에 관리할 수 있는 TUI(터미널 UI) 앱
     * 각 작업(task)을 별도의 격리된 Git 워크스페이스(브랜치)로 운영, 여러 작업을 동시에 백그라운드로 처리하며, 충돌 없이 변경점 검토·분기 관리가 가능
     * tmux·git worktree 기반으로 각 인스턴스가 독립된 환경에서 실행되며, 하나의 터미널 창에서 다수 세션을 빠르게 전환·관리 가능함
     * yolo/auto-accept(자동 승인) 모드, 커밋/체크아웃/세션 일시정지/재개 등 풍부한 워크플로우 지원
     * TUI 메뉴 인터페이스 로 세션 생성/삭제, 전환, 커밋·푸시, 체크아웃, 일시정지/재개, diff/preview 전환 등 단축키 제공
     * tmux 및 gh(GitHub CLI) 연동 필요
     * 사용 중인 AI 코드 어시스턴트에 맞게 간편하게 연동 가능

   결국 분기된 브렌치를 어떻게 합칠지 문제점을 정확히 미리 인지하고 개발완료해버리는군요

   제가 딱 필요로해서 만들고있던거였는데 이걸 만들어줬네요... claude 코드 max쓰는데 한번에 여러 프로젝트를 개발하면서 정말 필요했던 소프트웨어입니다.
"
"https://news.hada.io/topic?id=22069","궁극의 셀프호스팅 환경 구축기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            궁극의 셀프호스팅 환경 구축기

     * 수년간 다양한 셀프 호스팅 접근법을 시도한 끝에, 커스텀 환경을 성공적으로 구축한 경험 공유
     * 주요 목표는 개인 데이터 통제와 신뢰할 수 있는 인프라 유지였으며, 이를 위해 NixOS, ZFS, Tailscale, Authelia 등 여러 핵심 기술을 조합했음
     * 가족 및 지인 사용성까지 고려해 SSO, 별도 시작 페이지 도입 등 접근성도 강화함
     * 실제 운영 중 마주친 이슈들과 구체적 해결법(예: 사설 서비스 공개 프록시, 혼합 VPN 환경, 인증 연동)을 상세히 정리
     * 향후에는 백업 인프라 및 보안 강화 등 추가 개선을 계획하고 있으며, 노하우와 참고 자료도 남김
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 동기

     * 몇 년에 걸쳐 여러 셀프 호스팅 방식을 시도한 끝에, 본인에게 맞는 ""충분히 좋은"" 환경을 구축함
     * 여러 오픈소스 자료와 타인의 경험을 참고했으며, 해당 과정을 공유해 다른 개발자에게 도움이 되고자 함

목표

     * 개인 데이터와 해당 서비스를 직접 통제함으로써, 프라이버시 강화 및 의존 서비스 변경·단종 리스크 최소화 추구
     * 이러한 통제를 가족과 지인들에게도 제공해, 신뢰할 수 있는 서비스 환경을 마련하는 데 집중

요구사항

     * 필수 요건
          + 최대한 서비스의 공개 인터넷 노출을 제한해, 보안사고 위험 감소
          + 실수로 인한 핵심 인프라 다운타임 최소화 (순환 의존성 회피 및 설정 롤백 용이성 확보)
          + 인증·네트워크·도메인 등 핵심 구성요소 직접 소유 및 오픈소스 우선 적용
          + 가족·지인 관점의 사용성 배려 (일관된 SSO 로그인, 최소 유지보수 필요)
          + 설정 파일의 선언형 구성 적극 도입(버전관리 및 백업·복구 용이성, 타인 설정 참고 및 활용성 확보)
          + 업데이트가 쉽고 안전해서, 주기적으로 관리 가능해야 함
     * 비우선 요건
          + 극도의 모듈화/깔끔함 불필요(실용성 우선)
          + 모두가 오픈소스일 필요는 없으나, 가능하면 활용
          + 고가용성(HA) 미추구, 다운타임 감수하는 대신, 단순 구조 채택

기술 선택

     * NixOS
          + 모든 OS 설정을 Nix 언어와 패키지 관리자로 선언식 관리하는 리눅스 배포판
          + 구성이 코드화되어, 버전관리와 체계적 롤백이 가능
          + 다양한 패키지와 Docker/PODMAN 등 연동도 지원
          + 다른 개발자의 Nix 설정을 참고해 노하우를 축적
     * ZFS
          + 고성능 파일시스템으로 스냅샷, 롤백 등 데이터 보호 기능이 우수함
          + 4개의 10TB HDD를 RAIDZ2로 구성(2개 디스크 동시 장애 허용), 256GB SDD로 캐싱 적용
          + 파일·미디어 데이터셋 분리, 용도별 NFS 마운트로 관리
          + 단순하고 신뢰도 높은 메인 스토리지 아키텍처 구성
     * Tailscale & headscale
          + Tailscale: 접근성 좋은 Mesh VPN, 클라이언트 설치만으로 공개 인터넷 노출없이 내부 네트워크 접근 가능
          + Headscale: 자체 호스팅 가능한 오픈소스 Tailscale 백엔드(회사 정책 변화 리스크 제거)
          + 네트워크 안전성 강화 효과, 다만 사용자 디바이스에 클라이언트 설치 필요
          + Usability 관점에선 기기별 클라이언트 설치 필요성이 다소 진입장벽
     * Authelia & LLDAP
          + Authelia: OpenID Connect 기반 SSO·인증·인가 솔루션, nginx 프록시와 연동 가능
          + LLDAP: Lightweight LDAP, Authelia의 사용자·그룹 관리 및 백업 인증용으로 활용
          + 최소한의 리소스로 잘 동작하지만, 구성 난이도가 높고 어떤 서비스와 어떻게 연동할지 학습 커브 존재

구조 설계

     * 아키텍처
          + 각 서버는 Star Wars 행성명으로 명명
          + 진입점(public server)은 ""taris""로 Authelia, headscale, 블로그 등 필수 서비스 제공
          + headscale, Authelia, LLDAP는 외부 접근 가능해야 하기에, 제한된 범위에서 퍼블릭 운영
          + 사설 서비스(Foundry VTT, 모니터링 등)는 NGINX로 프록시 처리하여 선택적으로 노출
     * 프라이빗 서버
          + 메인 서버 ""kuat""에서는 TrueNAS로 NixOS VM과 ZFS 스토리지 풀 관리
          + ""files""(복구불가 데이터)와 ""media""(원한다면 복구 가능한 데이터)로 스냅샷/백업 스코프 분리
          + 주요 서비스는 ""bespin"" VM에서 NixOS로 운영, 테스트용 VM(""alderaan"")도 별도 구축
     * 기타 서비스
          + 미션크리티컬 디바이스는 단일 목적 어플라이언스로 구성(예: 스마트홈엔 Home Assistant OS 별도 사용)
          + Matrix 서버·Element 클라이언트는 공식 Ansible Playbook 활용
          + 메일, 비밀번호 관리는 ProtonMail과 Bitwarden 외부 서비스에 아웃소싱해 순환 의존성 차단

개별 이슈 및 해결

     * 서비스 시작 페이지
          + Flame 기반의 심플 대시보드로, 사용자별 서비스 접근성 향상
          + 사용량 적고 시각적 완성도가 높아서, 대체 서비스 도입 전까지 실용적 운영
     * Tailscale과 타 VPN 병행 사용
          + 일부 OS(특히 Android, Windows)는 멀티 VPN 동시 구동 불가
          + Tailscale exit node 기능과 Gluetun(Container 기반 VPN 클라이언트) 조합으로, ProtonVPN 등 외부 VPN 우회 활용
          + 단, 배터리 사용량 증가 및 간헐적 속도 저하 등의 부작용 존재
     * 인증(Authentification) 주의점
          + 셀프 호스팅 서비스별 주요 인증 프로토콜: OIDC(우선), OAuth, LDAP
          + 각 서비스와 Authelia에 별도 설정 필요
          + 관리자 계정은 반드시 Authelia/LLDAP 연동과 별도로 유지해, 인증 문제시 복구 수단 확보
          + OIDC 미지원 서비스는 NGINX와 Authelia 프록시 연동으로 접근제어 구현
          + Authelia의 OIDC와 NGINX Proxy 접근제어는 별도 구성 필요
     * DNS 및 SSL 발급
          + 공개 서비스는 일반적인 방법(도메인→퍼블릭IP)으로 운영
          + 내부 서비스는 ""internal"" 서브도메인과 Tailscale IP 활용, 외부 노출 차단
          + SSL 인증서는 NixOS 내장 Lets Encrypt 지원(공개 서비스는 HTTP-01, 내부 서비스는 DNS-01 방식)
     * NixOS VPS 설치 주의
          + 다수 VPS에서 NixOS 설치 옵션 미제공
          + 설치 필요시 커뮤니티 위키 등을 참조해, 지원되는 설치 경로 확인 필요
     * TrueNAS 데이터셋 VM 마운트
          + TrueNAS의 기본 방화벽은, VM의 호스트 접근을 차단
          + 공식 가이드(Bridge 네트워크 생성)에 따라 NFS 데이터셋 마운트 구현
     * 개인 서비스 공개 프록시
          + headscale 기반일 때, NGINX proxyPass로 프라이빗 서비스 외부 노출 가능
          + Tailscale 공식 Funnel 외에도, 설정 예제 및 구성 참고 자료 제공

다음 단계와 과제

     * 전용 백업 서버와 복구 검증 체계 추가 필요
     * Tailscale/headscale의 접근제어 적극 활용 계획
     * SSH 접근 등 추가 보안 강화 진행 예정
     * Pi-hole, AdGuard Home 등 로컬 DNS 암호화·캐싱 솔루션 도입 검토
     * Forgejo, Manyfold, RomM 등 신규 서비스 확장 고려

참고 자료

     * 셀프호스팅 전체 구성 GitHub
     * Perfect Media Server
     * LinuxServer.io
     * ZFS 구조/설정 개념
     * 적합한 ZFS 풀 설계법
     * How Nix Works
     * NixOS 설정
     * NixOS 보안 강화

   멋지네요!

        Hacker News 의견

     * 가족이나 친구들이 쉽게 사용할 수 있도록 하려면, 한 명당 하나의 로그인 계정으로 SSO(싱글 사인온) 기반으로 여러 서비스에 접속하게 하는 것이 목표임, 이 부분이 정말 어렵지만 동시에 멋진 부분임, 오픈소스와 Linux는 참 역설적인데, 정말 어디서나 사용되고 모든 프로토콜도 다루지만 실제 클라이언트 환경, 즉 사람들을 하나로 연결하고 그룹웨어적인 요소를 직접 구축하는 건 오히려 더 복잡해짐, 여러 시스템을 유기적으로 연동하고 디렉터리 인프라까지 구축하는 과정 자체가 놀라움, 언젠가는 FreeIPA나 윈도우 호환 디렉터리 서비스를 직접 운영하게 될 줄 알았지만, 최근에는 OpenID 기반의 세계가 실제로 자리를 잡아가는 느낌이 들기도 함
          + 공감 고맙다는 말씀에 감사함, 간단한 로그인과 접근성이 이 프로젝트에서 가장 어려운 요구사항이었고, 실제로 사람들이 사용하느냐 안 하느냐를 좌우하는 포인트라고 생각함, 오픈소스는 진짜 어디에나 있지만, 일반 사용자가 직접 뭔가를 써보려 할 때부터 문제가 발생함, 이 역설은 프로젝트마다 각자 따로 혁신하려는 의지가 있어서 생긴다고 봄, 한 방향으로 끌어가는 주체가 없는 게 장점이자 단점임, 그래도 최근 5년간 셀프호스팅 환경만 봐도 설치나 사용 면에서 훨씬 쉽게 변해가고 있다고 느낌
          + 이 역설에 정말 동의함, 어제도 FOSS가 비전문가들에게 얼마나 접근 어렵나에 대해 내 검증 플랫폼에서 포스팅함, 테크니컬한 사용자와 비테크 사용자 개인을 연결해주는 시스템 인티그레이터 같은 플랫폼이 해결책이 될지도 고민하게 됨
          + 사실 엄청 어렵지는 않음, 특정 서비스에 목매는 게 아니라 SSO 지원 여부를 최우선 선정 기준으로 삼으면 의외로 쉽게 셋업 가능함, 나도 처음에는 경험 거의 없었지만 caddy와 authentik을 써서 금방 셀프호스트 시스템을 완성함, 대안으로 yunohost는 SSO까지 알아서 다 구성해주는 매우 쉬운 배포판임
          + 나는 authentik으로 구글, 디스코드, 깃허브 SSO 인증을 사용 중임, 모두에게 충분히 잘 작동함
     * 모두에게 나만의 ‘딱 맞는’ 시스템을 찾는 건 시간이 걸릴 수 있다는 걸 알고 있음, 각자 목표와 선호, 환경도 다르니 내 최종 셋업 과정을 블로그 글로 정리해서 공유하고 싶음, 목표와 요구사항, 사용 기술, 설계, 문제 해결 과정을 다룸, 내 방식이 모두에게 맞진 않겠지만 다른 분께도 참고가 되길 바람, 나도 많은 콘텐츠와 무료 소프트웨어 덕분에 성장했던 만큼 계속해서 도움 나누고 싶음
          + 니스(Nix)를 홈랩에 사용해본 소감이 궁금함, 나는 하드코어하게 25U 랙에 소형 kubernetes, ceph, Talos Linux까지 7년 넘게 굴리고 있는데, 점점 단순화하고 싶어서 생각하다 보면 이상하게도 Nix와 ZFS로 결론이 모임, 관련 어려움들이 매우 익숙함, 너도 궁금한 점 있으면 물어보길 바람
          + coolify 사용은 고려해본 적 있는지 궁금함, 나는 1년 넘게 coolify를 쓰고 있는데, Heroku처럼 쉽게 GitHub에서 자동 배포되는 점이 꽤 마음에 듦 https://coolify.io/
          + 혹시 ZFS 암호화 기능도 사용하는지 궁금함, 나는 예전에 FreeIPA 등 여러 VM을 Debian+ZFS에서 구동하다가, 단순화하려고 VPS에 세파일(Seafile) 암호화 라이브러리만 돌리고 ZFS send/receive로 집 서버에 백업하는 구조로 바꿈, 그 서버는 밤마다 켜졌다가 업데이트와 동기화 후 다시 슬립함, 더 안전하게 하려 Linux 데스크탑(Fedora) ZFS도 전체 암호화로 돌릴까 고민 중임, 메인 데이터셋이 이미 암호화되어 있으니 외장 드라이브나 클라우드 동기화도 한결 간편해짐, 전체 사진 보관함을 VPS 세파일에 올리는 것은 비용 부담이 커서 방안을 찾고 있음
          + 세팅 후기와 디테일한 설명이 유익했음, 너처럼 바로 도입하긴 어렵지만, 대시보드로 flame을 설치해 가족과 실험해보기로 결정함
          + 반가움, 너의 작업이 정말 흥미로움, 나도 NixOS 기반으로 비슷한 프로젝트 작업 중임, 내 목표는 누구나 모뎀에 꽂고 웹 설치 마법사만 거치면 끝나는 거의 0설정 Apple 감성과 비슷한 작은 박스 만들기임, 아직 초기지만 집에서 이미 운영 중, 하이브리드 라우터(OPNSense/PFSense)와 앱 서버(Nextcloud, Synology, Yunohost 등) 역할을 한 번에 함, 모든 설정도 Nix 모듈 한 장으로 관리, 다이나믹 DNS, Let's encrypt 인증서, 각 앱 별 서브도메인 자동 할당, 광고 차단, headscale 내장됨, 지금 SSO까지 만들고 있는데 너의 글에서 아이디어도 좀 가져가려고 함, 자세한 건 https://homefree.host 참고
     * 가끔 집 네트워크를 보면, 내가 죽으면 가족에게 끼칠 폐해(?)나 외부인이 내 세팅을 이해하려면 얼마나 힘들까 상상함, ‘홈랩 놀이’는 사실 지하철 모형 레일 만드는 구세대 ‘아저씨’들의 취미와 비슷한 무언가를 채워줌, 이걸 나쁘게 말하는 게 아니라, 어떤 사람들은 자신만의 절대적으로 통제 가능한 미니어처 세계를 갖고 싶다는 본능이 있다고 느낌
          + 나도 똑같이 생각해서 만약에 대비한 문서를 써둠, 1부는 돈과 중요 서류 위치, 2부는 집을 어떻게 ‘더 멍청하게’ 바꿔야 하는 지침임, 예를 들어 스마트 스위치를 없애고 전통 스위치로 복원하는 법, Bitwarden 같은 키 서비스를 클라우드로 옮기는 법, 도메인/메일 유지 비용, 공유기를 ISP의 기본 장비로 복귀하는 방법 등임, 아내가 스마트홈에 긍정적이지 않았는데, 언제든 다시 ‘바보 집’으로 만들 수 있다고 하니 안심함, 솔직히 이거 다 없어지면 얼마나 불편할지 모를 테지만 내 일은 아니라는 위안도 있음
          + 우리 가족 사진을 home lab RAID1에 저장하고, 매일 밤 인런즈 집에 있는 컴퓨터로 외장 드라이브에 rsync 백업함, 백업과 동시에 만약 무슨 일이 생겨도 가족이 쉽게 접근 가능하도록 한 것임, 아내가 IT에 관심 없어서 그냥 ""USB 연결하면 전부 있어""라고 해둠
          + 물리 디스크 도난 등 쓸모없는 위협 시나리오는 무시해도 된다고 봄, 모든 사진과 주요 문서는 암호화 없이 저장하고, 이해 쉬운 설명까지 같이 남겨두는 게 실질적임, 홈오토메이션 쪽이 오히려 걱정거리가 많음
          + 집랩 운영자가 자리를 오래 비우거나 유고 시 어떻게 할지 미리 고민하는 건 실질적으로 중요함, 내가 이걸 쉽게 만들려고 따로 신경 쓴 건 아니지만 더 고민하는 게 맞다고 느낌, 핵심은 중요한 데이터와 거기에 접근할 자격 증명을 남기는 것임, Nextcloud 같은 서비스를 활용해서 데이터가 자동으로 가족 장치에 싱크되게 하고, 사용 자체에 가족, 친구가 직접 손을 대보도록 만드는 게 좋음, 우리 집도 Home Assistant는 배우자와 같이 쓸 만큼 어느정도 필수 가전처럼 만들려고 노력함, 이게 따로 VM이 아닌 실물로 존재할 때 더 관리가 쉬워짐, 물론 이 모든 건 희망 섞인 바람이 크니, 가까운 가족끼리라도 자세한 계획을 같이 세워두는 게 중요함
          + 나도 이 부분을 꽤 고민했음, NAS와 도커 서비스들이 나 없이 잘 부팅될 리 없다고 전제함, 오프사이트 암호 백업은 사실 전문가 도움 없이 복구 못할 것 같음, 그래서 NTFS 외장하드에 크론으로 매일 증분 스냅샷만 새 폴더로 저장함, 용량은 50GB 미만으로 저렴하게 중복 가능, 유고 시엔 그 하드만 꽂아서 폴더 복사하면 됨, 개별 노트북에 세파일 전체 라이브러리 복사본도 있음, 메일 도메인은 10년 선결제했고 iCloud 호스팅 중, 가족 사진 첨부로 용량 꽉 차서 메일이 튕기는 문제는 migadu로 고민 중
     * 나도 이 분야에 흥미가 많음, 직접 자영업/IT창업을 하면 홈랩 욕구가 더 커지는 걸 경고하고 싶음, 점점 단순 컨테이너 돌리는 것만으론 부족해지고, 각종 서류 제출해 합법적 DBA와 ASN 받고, 진짜로 직접 내 IP 블록/IPV6까지 갖고 나만의 ISP로 진화하게 됨, ingress(외부 접속) 문제는 tailscale로들 많이 해결하는데 이게 정말 어려움, STUN/TURN 기반 구조로 서버 전체 static 파일만 캐시, 동적 접근은 로그인월에서 이메일 매직링크로 인증하는 방식도 이론상 그렇게 위험하거나 비용 크지 않다고 생각함, 원격 개발 환경 구축하면서 이런 부분까지 또 파볼 명분(?)이 생김, 참고 링크 https://en.wikipedia.org/wiki/Traversal_Using_Relays_around_NAT, https://en.wikipedia.org/wiki/STUN
          + 나는 Fly.io로 ingress 구성함, 원격 단에는 nginx 캐시, ingress pod에는 Fly Wireguard 피어 컨테이너 추가, 이 방법이 무료는 아니지만 어떤 포트도 집에서 직접 열 필요 없이 anycast ingress까지 지원하면서 비용이 가장 합리적임
     * 최근 Immich를 만지작거리고 있는데, 집 밖에서는 tailscale로만 쓸지 아니면 VPS에 리버스 프록시까지 올릴지 매번 고민함, 제일 신경 쓰이는 건, VPS에서 누가 공격 시도를 하는지 감지해주는 사용자 친화적인 모니터링/보안 솔루션을 찾는 일임
          + 나도 같은 고민 중임, 혹시 보안/모니터링 솔루션에 대한 정보 찾은 게 있으면 공유바람
     * 내 셋업은 훨씬 단순함
          + 한 대의 머신
          + nginx 프록시
          + 같은 머신에 여러 서비스; 어떤 건 내부용, 어떤 건 외부 공개지만 모두 웹으로 접근
          + 내부 서비스는 긴 HTTP basic auth 비밀번호 사용 (파이어폭스 내장 비번관리자)
          + 외부 서비스는 공개 또는 구글 OAuth 적용
          + 전부 스크래치부터 직접 코딩했으며, 이게 바로 홈랩 목적임
          + 이미지든 동영상이든 브라우저가 알아서 잘 읽음
          + 어려운 건 언제나 백엔드임, 프론트는 거의 90년대 HTML 느낌임
          + HTTP는 비밀번호를 평문으로 보냄으로 최소한 셀프사인 인증서는 쓰는 게 안전함
          + 인프라나 서비스도 코드로 직접 만드는 건 그냥 배우기에 최고임, 자기만의 니즈를 정확히 맞출 수 있단 점도 멋짐
     * 이런 홈랩 운영을 해보고 싶지만 시간이 안 남, 주말에 설치는 가능하지만 유지/업데이트를 꾸준히 할 여유는 없음, 그래서 그냥 클라우드 업체에 맡기고 신경을 끊음, 혹시 나처럼 클라우드만 쓰는 분은 어떤 방식으로 접근하는지 궁금함
          + 나도 예전 셋업 때는 유지보수가 제대로 안 되어서 스트레스 받았음, 그래서 NixOS와 ZFS를 좋아하게 됨, 둘 다 롤백이 정말 쉬움, 업데이트하고 문제 있으면 이전 버전으로 바로 복구, 디버깅도 여유될 때만 진행, 또 클라우드 선택지도 그 경험에 만족하면 OK라고 생각함, 직접 셋업은 확실히 시간 소모가 있으니까 각자 비용-가치 비교가 중요함
          + 나는 12개쯤 셀프호스팅 서비스 돌리는데, 보통 업그레이드는 한 달에 1분도 안 걸림, 서비스별로 폴더가 있고, 안엔 docker-compose 스택과 데이터 폴더, 업데이트는 docker compose pull로 받고 up -d 하면 끝임, 아주 가끔 구성 변경이 필요한 업그레이드가 있겠지만 대부분 수 분 이내에 끝남, VM도 없이, Docker Compose 만으로 완전 자동화된 셀프호스트가 제일 단순한 방식이라고 봄
          + 이게 단순히 주말 하루만의 일로 끝나지 않음, 내 경우 Plex 한 번 깔아보는 걸로 시작했다가 1년 뒤에는 Proxmox와 온갖 홈오토메이션이 접목된 복잡한 구조물이 됨, 농담 반 진담 반, 최소 셋업이라면 docker compose로 시작하면 관리도 쉽고 업그레이드도 단순함
     * 굳이 SSO까지 도입해야 하나 의문임, 가족/친구가 wireguard 클라이언트(iOS에서도 매우 간단)를 쓰면 그냥 토글 한 번에 집 네트워크에 붙을 수 있고, 별도의 SSO 없이도 내부 서비스 전부 사용 가능함, 소규모 가정 네트워크에서는 단점보다 장점이 훨씬 큼
          + 우리가 쓰는 Nextcloud, Mealie 같은 서비스는 각각 사용자별 계정이 기본임, SSO 덕분에 모든 서비스를 같은 계정으로 접근 가능한 동시에 내가 비밀번호까지 관리할 필요가 없어졌다 생각함, 세팅은 조금 더 복잡해지지만 운용은 오히려 쉬워지기 때문에 가족들도 실제로 쓰게 될 확률이 높아짐
          + 나는 20개의 앱을 셀프호스팅 중이고, 각기 인증을 따로 관리하는 게 진절머리 나서 SSO 도입 중임, 가족에게도 일부 앱을 공개하고자 할 때 인증 문제를 한 곳에서 처리할 수 있는 게 최우선임, 위에 말한 방식에는 동의하기 어렵다고 생각함
     * flame을 굳이 쓰는 이유가 궁금함, node, react, redux 등 수십~수백 개의 써드파티 의존성을 ‘보안 왕국’에 끌어들이는 셈인데, 시작 페이지 역할은 사실 단순 HTML 한 장에 링크 나열로도 가능하지 않나 생각함
          + flame을 여태 써 봤으니 친숙해서 그냥 바로 문제를 해결할 수 있으니 썼음, 디자인도 마음에 들고, Tailscale과 Authelia 뒤에 배치해두니 특별히 보안 걱정도 없음, 대안도 차후 알아볼 생각임
     * NixOS로 셀프호스팅을 해보고 싶긴 한데 실행에 옮기진 못했음, 내 환경은 몇 개의 VM과 각 VM별 docker compose 파일 한 장으로 관리, ansible playbook으로 compose 파일만 복사해주고, Fedora Server OS를 릴리즈 한 버전 뒤로 유지하다 만료 때 업그레이드하면 끝, mac에는 nix-darwin을 돌리다 보니 Nix 설정의 장점은 이해하지만, 실제 내 환경을 Nix로 포팅할 정도의 효율성이나 시간 대비 효과는 아직 못 느끼겠음, LLM(대형 AI)이 설정 파일 받아쓰기라도 해주면 모를까, 지금 당장은 도전 동기가 부족함
          + 나도 NixOS 시도해보고 일주일 만에 홈네트워크랑 실제 서버 두 개까지 전부 마이그레이션함, 3~4개월 정도 흘렀는데 기대 이상으로 만족 중임, 서버 마이그레이션이 워크스테이션 옮기는 것보다 더 쉬웠음, 최근에는 Jetson Orin Nano 장난감도 NixOS로 세팅해서 놀고 있음, 예전 Gentoo였다면 엄두도 못 낼 일임, Gentoo에서 제일 짜증났던 건 옛날 컴에서 말도 안 되게 긴 컴파일 타임이었음, 예를 들어 GHC를 2019 XPS로 빌드하면 6시간이 걸릴 정도였음
          + 나한테는 NixOS가 뭔가 꼬였을 때 롤백이 너무 쉬운 점이 가장 큰 차이였음, ansible이나 compose 기반도 백업/복구가 가능하지만 직접 그 시스템을 작성해야 하는 부담이 있음, 그래도 지금 시스템에 만족한다면 그게 좋은 거라고 생각함
          + 이미 IaC를 어느 정도 쓰고 있으면, NixOS가 주는 추가 효율이 그렇게 큰지는 못 느꼈음
"
"https://news.hada.io/topic?id=21991","미국 세관과 이민세관단속국에 항공편 정보 판매하는 데이터 브로커","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  미국 세관과 이민세관단속국에 항공편 정보 판매하는 데이터 브로커

     * 데이터 브로커들이 개인의 항공편 관련 정보를 미국 세관(CBP)과 이민세관단속국(ICE)에 판매함
     * Airlines Reporting Corporation(ARC)이 여행자 기록을 모아 정부 기관과 공유함이 최근 폭로됨
     * 이용자 동의 없이 정보가 판매되며, 이는 개인정보 보호와 헌법적 권리 우회 문제로 이어짐
     * 민감한 위치 정보, 인터넷 이용 기록, 공공요금 데이터 등도 유사하게 집계되어 법집행기관에 전달되고 있음
     * 이러한 문제 해결을 위해 ‘Privacy First’ 법안과 ‘Fourth Amendment is Not For Sale’ 법안 등 강력한 개인정보 보호 입법 필요성 대두됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

데이터 브로커와 개인 정보 판매 문제

     * 데이터 브로커들은 오랫동안 개인정보 보호법의 허점을 이용해 사용자 정보 수집 활동을 해왔음
     * 이들은 우리의 동의 없이 위치 정보 등 민감한 데이터를 판매하며, 주요 거래처에는 법집행기관이 포함됨
     * 이러한 데이터 시장은 누구든 개인 데이터를 수집하면 수익을 얻을 수 있는 구조로, 법을 회피하고자 하는 정부 기관에도 매력적임

ARC의 항공편 정보 판매 사례

     * 404 Media와 기타 언론사의 폭로에 따르면, Airlines Reporting Corporation(ARC) 는 최소 8개 미국 주요 항공사들이 소유·운영하는 데이터 브로커임
     * United Airlines, American Airlines 등에서 여행자 명단, 전체 여행 일정, 결제 내역 등 민감한 항공권 데이터를 수집한 뒤, 이를 몰래 미국 세관(CBP)에 판매해 옴
     * 데이터 브로커들은 정보 출처를 감추는 방식까지 사용하여, 정부 기관이 해당 정보를 공개하는 것을 차단함
     * 즉, 정부가 영장 등 사법적 절차 없이 정보 접근을 가능하게 하고, 출처까지 숨김으로써 사생활 침해와 권리 회피 문제가 발생함

Travel Intelligence Program (TIP)과 그 영향

     * ARC의 Travel Intelligence Program(TIP) 은 과거 및 미래 39개월에 걸친 10억 건이 넘는 항공여행 기록을 집계함
     * CBP는 내부 보고서에서 지역 경찰 및 주 경찰이 주목하는 인물 식별을 지원하기 위해 이런 정보가 필요하다고 밝힘
     * 하지만, 미국 내에서 이민 단속 및 불합리한 검문, 수색이 증가하고 있는 상황에서, 이 정보가 무고한 여행자까지 의심 대상으로 확장하는 위험이 증대함

ARC의 영향력과 항공사 참여

     * ARC를 통해 전 세계 54% 이상의 항공편 정보가 처리되고 있으며, 200개 이상의 항공사가 이 네트워크에 참여함
     * 이사회에는 JetBlue, Delta, Lufthansa, Air France, Air Canada 등 미국 및 국제적 항공사 대표가 다수 포함됨
     * 법집행기관에 민감 정보를 대량 판매함으로써, 항공사들은 개인 프라이버시보다 수익을 우선시하는 행태를 보임
     * 실제 ICE(이민세관단속국)가 ARC에서 여행자 개인정보를 구매한 내역이 최근 공개됨

파급 효과 및 프라이버시 침해 현황

     * 자유로운 이동은 민주 사회의 핵심임에도, ARC 등 데이터 브로커가 비밀리에 여행 이력 추적이 가능한 환경을 조성함
     * 현재 미국에서 국적, 종교, 정치 성향 등에 따른 법적 불이익 가능성 논의가 커지는 가운데, ARC 데이터 활용이 공권력 남용으로 이어질 위험이 있음
     * 데이터 브로커들은 항공정보 외에도 스마트폰 위치정보, 인터넷 백본 데이터, 공공요금 기록까지 판매하며 개인정보 침해 범위 확대 중임

정책적 요구 사항 및 해결방안

     * 정부 당국이 국경 등에서 자유와 권리를 약화시키는 조치를 늘리는 중인 현시점에서, 이러한 *** 대규모 데이터 수집·판매가 더 큰 우려***를 야기함
     * ARC의 사례는 ‘Privacy First’ 등 프라이버시 우선 법안의 필요성과 기업 데이터 처리 최소화 원칙 법제화 요구에 대한 경각심을 제고함
     * 또한 법집행기관이 데이터 브로커에서 정보 판매를 통해 영장 없이 정보 수집을 우회하지 못하도록 하는 ‘Fourth Amendment is Not For Sale’ 법안 통과 필요성이 제기됨
     * 마지막으로 데이터 브로커 등록 및 투명성 강화 등 규제 역시 시급한 과제로 등장함

        Hacker News 의견

     * 많은 사람들이 특권적인 1차 데이터 접근이 없어도 이런 데이터 모델을 만들기 얼마나 쉬운지 잘 모름. 2012년에 내가 만든 프로토타입은 소셜 미디어나 광고 데이터만으로도 대부분 사람들의 비행 이력을 대규모로 정확히 추적할 수 있음을 보여줌. 이런 건 아주 옛날부터 가능했음. 대략적인 방식은, 엔터티 그래프 내에서 시속 300km 미만 또는 거리 200km 미만의 시공간 간선을 걸러냄. 이 기준으로 “비행기 탑승” 여부를 추정하고 출발지와 도착지도 파악이 가능했음. 이 간선을 공공 비행 데이터나 제트 엔진의 유지보수 IoT 데이터와 연계하면 특정 비행편에도 매칭할 수 있음. 대부분은 평범한 공업 IoT 데이터가 다른 분야의 관계를 추정하는 데 어떻게 쓰이는지 간과함. 드물게 한 번에 여러 항공편이 가능한 케이스도 있었지만, 과거 비행이력을 참고해
       과거에 이용한 주요 항공사를 선택하면 거의 항상 완벽하게 일치함. 인상적일 정도로 효과적이었고, 항공사 1차 데이터나 복잡한 분석이 전혀 필요하지 않았음. 결국 시간과 공간이 현실의 주키임
          + “비행 가능성 있는 경로를 선별했다”는 설명을 들으니, 문제의 본질은 결국 누가 처음에 '시공간 데이터'를 갖고 있냐는 점임. 결국 “네 신용카드 거래 내역이 있으면 언제 어디서 어떤 가게 갔는지 알 수 있다”는 말과 다를 바 없음. 소름끼치긴 하지만 진짜 심각한 건 그런 데이터 접근 자체가 가능하다는 점임. 애초에 누군가의 거친 위치를 시간대별로 전부 꿰고 있다면, 그 사람의 개별 비행기 탑승이력보다 시공간 데이터 자체가 훨씬 더 큰 가치를 지님
          + 내가 흥미롭게 느끼는 점은 사람들이 각종 개인정보를 수집해 악용할 수 있다고 걱정하지만, 대부분 그 정보를 가지고 하는 일이 겨우 맞춤형 광고를 더 보여주는 것뿐이라는 점임
          + “제트 엔진의 유지보수 IoT 데이터” 같은 건 도대체 어디서 구함?
          + 아마 ICE에서는 특정 인물이 어느 도시·국가를 언제 방문했는지 추적하려고 이런 데이터가 필요할듯 함
     * ARC를 단순한 “데이터 브로커”라고만 표현하는 게 흥미로움. 사실 ARC나 IATA는 항공권 결제 클리어링하우스이자 관련 업계 시스템 유지·감독 역할을 담당함. 원천적으로 거래 데이터가 이들에게 흘러가고, 이를 판매해 수익을 창출함. 그런데 이건 여타 데이터 브로커처럼 외부에서 데이터를 모아서 되파는 구조가 아니라 자체적으로 1차 데이터를 소유한 상태임. 이런 민감·비식별처리되지 않은 데이터를 판매·공유하는 게 허용되어야 하느냐는 근본적인 쟁점이지만, 그만큼 근본적인 1차 데이터임. Airline Reporting Corporation 전체 구조 설명 링크도 참고할 만함
          + 이 설명이 기사에서 다뤄진 요점에는 딱히 반박이 되지 않음
     * 브로커들이 판매하는 데이터의 양과 범위는 상상 이상으로 방대함. 심하게 생각해도 실제 상황은 그보다 열 배는 심각함
          + 내 동료는 한 번 특정인을 타깃으로 이미지 배너 광고를 띄우고 “내가 이 정도도 된다고 했잖아, 친구!”라는 문구를 넣어 효과 시연을 했음. 일반인들은 광고사와 데이터 브로커가 자신에 대해 얼마나 많이 아는지 거의 모르고 있음
          + 2014년쯤 리크루터들과 일할 때 사람들의 정보를 LinkedIn, Yelp, Twitter, GitHub, Eventbrite 등에서 긁어오는 툴을 봤음. 그 당시에 이미 10년 넘는 이력까지 모두 파악이 가능할 만큼 방대한 정보 확보가 가능했음. Palantir 같은 곳과 협업하면 정부는 Reddit 게시글까지 스타일 분석이나 심리분석도 가능할 것으로 보임
          + 이런 데이터 프로필이 필요한 아트 프로젝트 아이디어가 있는데, 저렴하게 구매할 만한 좋은 소스 추천을 부탁함. 워낙 대규모 프로젝트라 어디서부터 시작해야 할지 모르겠음
          + 이 업계에서 일하는 입장에서, 현실은 “1000배쯤 심각”하다고 느껴짐
          + HN 이용자 대부분이 실제 업계 실태를 거의 이해하지 못한다고 생각함. 방향 자체를 아예 다르게 잡아야 할 듯함. 대부분은 Google 정도만 내 개인정보 팔아넘긴다고 생각하지만, 실제로는 데이터 업계가 훨씬 방역 체계가 느슨함. 예를 들어 35세 동네 치과의사의 신용카드 거래내역을 딱 그 사람만 골라 원하는 포맷으로 하루 만에 뽑아달라고 전화주문할 수 있을 정도로 쉬움
     * 데이터 시장이 얼마나 잘 숨겨져 있는지 신기함. 수많은 대기업이 매일 데이터를 추출하고 거래하고 있지만, 이렇게 시끄러운 “탈중앙화” 바람 속에서도 정작 오픈된 데이터 마켓플레이스는 없음. 나는 오픈된 행동 데이터까지 사고팔 수 있는 모델이 나오길 바라왔고, 실제로 사람들도 단순히 “상품”인 게 아니라 기업에 데이터 제공하고 그 대가를 받는 식으로 바뀌었으면 좋겠음
          + 실제로 그렇게까지 숨겨져 있지는 않은 거 같음. 2021년에 50년 전 원한을 갚으려고 남의 집에 찾아간 사람이 있었는데, CCTV 영상에 PeopleFinders 폴더를 든 모습이 찍힘. 놀라운 점은 정부 기관들조차 이런 데이터를 판매하는 현실임
          + 이런 수익 구조에서 더 챙길 생각하지 말고 그냥 모두 폐쇄하고 멈추는 게 맞다고 생각함
     * CBP와 ICE가 왜 데이터 브로커에서 정보를 사야 하는지 이해가 안 됨. TSA가 어차피 모든 사람의 탑승권을 다 스캔하고 있음
          + 아마 TSA가 수집한 데이터에 접근하려면 엄격한 규정과 절차가 있을 테고, 그 데이터와 동일한 정보라도 브로커에서 사는 데는 별다른 요구 조건이 거의 없음. 데이터 출처도 TSA가 아니라 항공사, 결제사 등 다양할 수 있음. 브로커 데이터 품질은 보장하기 어렵지만, 절차가 훨씬 간단함
          + 연방기관에서 근무할 때, 내 입장에서 공개 트윗 조차 수집하려면 왜 필요한지, 어떤 개인정보가 저장되는지, 보관 기간과 삭제 방안까지 문서화해서 직접 승인받아야 했음. 주말에 일반인이 할 수 있는 일도 정부 내에선 엄청난 승인이 필요함. 그런데, 만약 다른 기관 데이터를 요청하려면? 그건 아예 상상 이상으로 정치적 부담이 생김. 협력 기관이라도 쉽지 않고, 오히려 회의에서 동료 기관에 이런 요청을 언급하면 괜히 마찰만 늘어난다는 조언도 받았음. 반대로, 데이터 브로커에서 사오면 이런 복잡한 절차 필요 없음
          + TSA에서 승인을 막 나눠주지 않는 것도 이유일듯. 마치 경찰이 휴대폰 데이터를 요청할 때는 영장이 필요하지만, 통신사가 실시간 위치를 3자에 팔아 그걸 경찰이 사올 수 있다거나 하는 구조랑 비슷함. 참고 링크
          + 정부는 법과 헌법을 피하기 위해 기업을, 기업은 규제를 피하려고 정부를 활용함. 옛날부터 내려오는 구조임
          + 규제와 법률 관련 이유 외에도, 조직 내에서 각 부서가 쓸 수 있는 실용적인 데이터 스트림을 구축·조율하는 게, 이미 데이터 큐레이팅·관리·유통에 최적화된 브로커에게 사오는 것보다 더 어렵고 비용이 들어감. 비록 황당해 보이지만 결국에는 프리미엄 값을 주더라도 브로커 데이터가 속 편하고 신뢰성도 높음. TSA 기술팀은 데이터에 메타데이터를 붙이고 SLA까지 관리할 인센티브가 없음. 데이터 브로커는 그런 유인이 항상 있음
     * yaelwrites/Big-Ass-Data-Broker-Opt-Out-List는 데이터 브로커 옵트아웃을 시작하기에 좋은 목록임. 다만, 기사에 언급된 ARC는 현재 기준으로 이 리스트에 없음
     * 주제와 조금 다르지만, 일반 회사들이 (광고 관련 기업 제외) 소비자 데이터와 행동 패턴을 판매해서 실제로 얼마만큼 수익을 내는지 대략적인 추정치를 아는 사람이 있는지 궁금함
     * 두 달 전쯤에 HN에서 관련 논의와 다른 스레드도 있었음
     * 이번 사례가 흥미로운 이유는, 과거 악질 브로커들은 EU에 영업거점이 없어 GDPR 벌금을 무시하거나, 괴리수익이 훨씬 크면 그냥 리스크로 받아들였지만(예 Clearview), 항공사처럼 본업 마진이 적고 글로벌 매출이 큰 회사엔 GDPR 위반이 훨씬 치명적임. 데이터 관리자가 항공사이면 브로커 제공 자체가 불법일 수 있고, EU에 잘 노출되어 있어서 벌금 회피도 힘듦. 심하면 회원국이 그 항공기 자체를 압류해버리거나, 모든 운항 금지까지 시도할 수 있음. 실제 독일이 태국 황태자 비행기를 압류한 사례도 있음. 관련 기사링크
          + 항공사가 주요 데이터 출처 같지만, 실상은 소스가 굉장히 다양함. 탑승권 바코드엔 엄청난 정보가 담겨있고, 이건 암호화가 아니라 단순 인코딩이라서 읽기만 하면 됨. 바코드 리더기는 많은 업체에서 만들어 팔고, 체크인·수하물·면세점·라운지 등 공항에서 바코드 스캔하는 곳은 많음. 측정한 정보를 여러 방법으로 축적할 수 있음. 여권 스캐너도 저렴하게 구할 수 있고, 공항 상점이나 렌터카도 잘 사용함. 최근엔 얼굴 인식 기술 때문에 탑승권·여권 확인 없이 타기도 함. Uber 예약 정보 등 부수적인 데이터도 결합 가능함. 바코드 관련 상세 링크
     * 내가 돈을 내고 내 자신 및 타인에 대해 데이터 브로커로부터 어떤 정보를 얻을 수 있는지 궁금해서, 이런 데이터 브로커에 어떻게 접근해야 할지 아는 사람 있음?
"
"https://news.hada.io/topic?id=22055","완전동형암호와 진정한 프라이빗 인터넷의 시작","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        완전동형암호와 진정한 프라이빗 인터넷의 시작

     * 완전동형암호(FHE) 는 데이터를 복호화하지 않고도 암호문 상태로 연산을 수행할 수 있는 기술
     * 현재 FHE는 여전히 저조한 실용성과 1,000배~10,000배의 연산 속도 저하, 40배~1,000배의 저장공간 증가 등 한계를 지님
     * 하지만 최근 FHE 알고리듬은 매년 8배씩 속도 향상을 이루고 있으며, 곧 클라우드 컴퓨팅, LLM 추론, 블록체인 스마트컨트랙트 등에서 실용영역에 진입할 가능성 있음
     * FHE가 보편화된다면, 컴퓨팅 환경 전반에서 데이터 프라이버시가 기본값이 되는 산업적 변화를 촉진할 것임
     * 격자 기반 암호, LWE, 부트스트래핑 등 핵심 개념들과 FHE 알고리듬 발전사, 실제 구현 예시 및 성능 개선 추이 등을 종합적으로 다룸
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 완전동형암호란 무엇인가

     * 완전동형암호(Fully Homomorphic Encryption, FHE)는 복호화 없이 암호문 상태에서 임의의 연산을 가능하게 해, 실제로 암호화된 데이터에 그대로 연산을 수행할 수 있는 방식
     * 즉 서버가 평문을 모른 채로도 질문과 결과를 계산하여 전달할 수 있음
     * 이 기술은 오늘날의 여러 실세계 시스템에 실제 도입되고 있음

FHE의 잠재력과 한계: ""FHE의 무어의 법칙""

     * FHE는 네트워크 상에서 데이터를 지속적으로 암호화된 상태로 유지할 수 있어, 데이터 유출 위험을 원천적으로 차단하는 완전한 프라이버시를 실현할 수 있음
     * 그럼에도 불구하고 현재 실용화에 제약이 많은 이유는, 암호문 연산이 평문 연산에 비해 1,000~10,000배 느리고, 저장공간도 대략 40~1,000배 증가하는 등의 현격한 성능 저하가 있기 때문
     * 이는 1990년대 인터넷 초창기와 유사함
     * 그러나 최근 FHE는 매년 8배씩 빨라지고 있어, 곧 여러 실용 영역에 진입할 것으로 전망됨

임계점: 곧 다가오는 FHE 실용화

     * 이와 같은 속도의 비약적 발전이 지속된다면, 앞으로는 다음과 같은 분야에서 FHE가 실용화될 수 있음
          + 암호화된 클라우드 컴퓨팅
          + 암호화된 LLM 추론
          + 비밀보장이 가능한 블록체인 스마트컨트랙트
     * 이런 변화는 사용자 데이터 수집 기반 인터넷 비즈니스 모델을 근본적으로 뒤흔들 수 있음
     * FHE로 인해 ""감시가 기본""인 인터넷에서 ""프라이버시가 기본""인 인터넷으로의 본질적 전환이 기대됨

데이터 보안의 아킬레스건과 FHE의 해결책

     * 데이터는 3가지 상태(저장, 전송, 사용) 중 '사용 중' 상태에서 복호화가 되어 보안 취약점이 되곤 함
     * 클라우드, 내부자, 해커, 취약한 CPU 등 누구나 메모리 내 평문 데이터에 접근 가능
     * 대형 데이터 유출 사고 역시 대부분 '사용 중' 또는 '저장 중'에 발생
     * FHE는 데이터를 전체 라이프사이클 동안 암호화 상태로 유지해 이러한 취약점을 근본적으로 해소

완전한 프라이버시 컴퓨팅 정의

     * 이상적 환경은 데이터가 저장 시, 전송 시, 사용(연산) 시 모두 암호화를 유지함
     * 예를 들어, 서버는 평문 질문을 전혀 보지 못하며, 암호화된 질문을 입력받아 암호화된 결과만 반환
     * 오로지 사용자만이 그 결과를 복호화 할 수 있음

FHE의 동작 방식: 수학적 구조와 개념

     * ""동형""은 동일 구조를 보존하는 수학적 변환(예: 푸리에 변환과 유사)에 기반함
     * FHE는 평문 공간과 암호문 공간을 쌍방향으로 변환해, 암호문 연산 결과의 복호화가 곧 평문 연산 결과와 같음
     * 이런 변환에는 주로 격자 기반 암호와 LWE(오차 학습 문제) 가 사용됨
          + 격자 기반 암호는 매우 높은 차원에서의 벡터 문제로, 양자컴퓨터마저 풀기 어렵다고 알려져 있음(양자 내성)
          + LWE는 노이즈가 섞인 선형 시스템을 역산하는 문제로, 현실적으로 해독이 불가함

노이즈 관리와 부트스트래핑

     * FHE에서 연산을 거듭할수록 암호문 내 노이즈(잡음) 가 증가함
     * 덧셈 연산에서는 선형적으로, 곱셈에서는 기하급수적으로 커져, 결국 복호화 불능이 되는 문제가 있음
     * 이를 해결하는 핵심 기술이 부트스트래핑이며, 이는 암호문을 '새 공용키'로 재암호화하며 노이즈를 일정 수준으로 리셋하는 기법
     * 이 과정이 FHE 시스템의 성능 병목이지만, 매년 빠르게 개선되고 있음

FHE의 추가 핵심 구성 요소

     * 재선형화(relinearization): 곱셈 후 키 차수가 2차로 증가하는 문제를 해결해 다시 1차로 만드는 과정
     * 모듈러스 스위칭(modulus switching): 노이즈 관리를 위해 암호문 모듈러를 축소하는 기법

   이외에도 알고리듬 발전에 따라 다양한 테크닉들이 지속적으로 제시되고 있음

동형암호(HE) 체계의 분류와 파이썬 예제

     * 부분동형암호(Partial HE): 하나의 연산만 지원(e.g. 파이에 암호는 덧셈만 지원)
     * 일부동형암호(Somewhat HE): 덧셈, 곱셈 모두 지원. 단, 곱셈 반복 횟수 제한
     * 완전동형암호(FHE): 덧셈, 곱셈 모두 무제한 지원. Turing 완전성 보장

   파이썬으로 구현된 Paillier 암호 예제를 통해 부분 동형을 직관적으로 체험할 수 있음

FHE 발전 역사와 ""FHE의 무어의 법칙""

     * 1978년: 최초로 ""프라이버시 동형사상"" 개념 등장
     * 2009년: Craig Gentry의 FHE 첫 실현(박사논문)
     * 2011년: 최초 구현, 비트당 30분 소요(매우 느림)
     * 2013년 이후: 부트스트래핑 수 ms 수준까지 단축
     * 2017년: CKKS 등 부동소수점 근사 지원, ML/AI에 본격 도입

   FHE 알고리듬은 2011년부터 매년 8배씩 개선되어, 초기 10¹⁰배 오버헤드에서 최근 10³~10⁴배 수준까지 도달함
   최신 논문은 FHE 곱셈 처리량 1,000배, 지연시간 10배까지 감소시켰고, 하드웨어 가속화 결합시 추가로 1,000배 이상 속도 개선 여지 있음

암호화가 기본값이 되는 미래

     * 대형 데이터 유출사고는 피할 수 없는 현실임
     * FHE를 이용해 서버가 복호화 키 없이도 암호화된 데이터에 연산만 가능하다면, 프라이버시 보호의 새로운 기준이 될 것
     * 아직 모든 영역에서 완전히 실용적이지는 않지만, 매해 놀라운 속도로 개선되고 있음
     * 사용자의 프라이버시 요구와 관련 규제 강화가 맞물려, 결국 FHE가 대부분의 클라우드 컴퓨팅에 표준이 될 것으로 전망됨
     * 미래의 인터넷 컴퓨팅은 언제나 암호화 상태로 진화할 것

     2010년대: HTTPS가 기본값
     앞으로: FHE가 기본값이 되는 시대가 도래할 것으로 예상

참고문헌 및 추가자료

     * FHE Reference Library: 학술 자료 포괄적 정리
     * Craig Gentry 2009 박사논문: FHE의 출발점
     * Vitalik Buterin: FHE 심층 분석
     * 커뮤니티: FHE.org (개발자 중심 허브)
     * GitHub: awesome-he: 동형암호 관련 프로젝트 모음

        Hacker News 의견

     * FHE와 Cryptography를 굉장히 좋아하는 입장을 전제로 말하겠음. FHE가 점점 빨라지고는 있지만, 부트스트래핑에 의존하는 한 평문의 연산 속도를 따라잡을 수 없음을 이야기함. 부트스트래핑으로 인한 약 1000배 이상의 오버헤드는 근본적으로 피할 수 없으며, 더 빠르게 만드는 게 불가능하다고 깨닫자 하드웨어 가속 이야기가 나오기 시작했음. 하지만 LLM에 컴퓨팅 파워가 모조리 들어가는 요즘 시기엔 쉽지 않은 일임. FHE로 컴퓨팅할 때 단위 토큰당 비용이 몇 배로 뛸지 생각해보면, 1000배 이상이 아닌 이상 현실적으로 가능성이 거의 없음. 개인정보 보호 목적이라면 confidential computing 방식이 현재로선 유일하게 실용적인 대안임. 하드웨어를 신뢰해야 하는 점이 마음에 들진 않지만, 그게 우리가 가진 최선임
          + FHE를 임의의 연산에 쓰기 힘든 진짜 더 근본적인 이유가 있음. 바로 어떤 종류의 연산들은 평문보다 암호문에서 비정상적으로 복잡도가 커짐. 데이터베이스 검색의 경우, 평문에서는 O(log n)이지만, 암호화된 키로 검색하면 O(n)이 됨. 그래서 완전동형 Google 검색은 기본적으로 실현 불가능함. 하지만 완전동형 DNN 추론은 상황이 다를 수 있음
          + 부트스트래핑이 없어도 FHE는 평문 연산만큼 빠를 수 없음. 암호문이 평문보다 1000배 정도 더 큼. 즉, 메모리 대역폭과 연산량이 훨씬 더 많이 필요함. 이 격차를 좁힐 수 없음
          + 계산 비용이 1000배로 뛰더라도, 인증 가능한 프라이버시를 진짜 원하는 특정 수요층이 있지 않을지 생각해봄. Dropbox만큼 크진 않겠지만 어느 정도의 시장은 있다고 상상함
          + 예전에 모든 게 PCIE 확장 카드 시절이었음을 되짚어봄. GPU도 그랬고, 수학 코프로세서 같은 특수 가속기도 따로 있었음. 지금은 범용 하드웨어에 통합돼서 더 싸고 편하지만, 특정 기능에 최적화된 실리콘 칩만큼 잘할 순 없음. 그래서 AI/ML용 전용 카드를 GPU 기반 대신 따로 쓰자고 주장함. 아키텍처가 일부 중첩되지만 GPU에 기반한 AI 카드는 많은 부분에서 손해를 감수하는 셈임. 진짜 AI 가속기는 최신 SXM 소켓에 들어가는 전용 하드웨어라고 생각함. 하지만 SXM 소켓은 서버에만 있고 가격도 만만치 않음
          + LLM 열풍은 인정하지만, FHE를 쓸 만한 다른 용도가 정말 없을지 궁금함. 예를 들어, 고속이 필요 없는 트레이딩 알고리즘을 FHE로 서버에 호스팅해서 보안을 보장하는 방식도 생각해볼 수 있을 것 같음
     * FHE가 중요한 이유는, 현재 기업들이 정부의 압박에 의해 특정 타겟의 암호화를 강제로 깨야 하는 경우가 있기 때문임. FHE는 기업이 ""우리는 평문을 절대 못 본다""고 떳떳하게 말할 수 있게 해줌. 네트워크 캐리어 역할에선 E2E encryption 등으로 일부 가능하지만, 평문 상태에서 데이터 처리 시엔 아직 불가능함. 개인정보 보호는 기본 인권이라는 생각임. 정부의 권한은 민주주의적 활동(투표, 예술, 언론, 표현 등)에 대해 아주 제한적으로만 작동해야 함
     * FHE로 임의 연산이 가능해도, 대부분의 서비스는 특정 데이터를 제공하기 때문에 쓰는 것임. 구글이 내 쿼리에 대해 보안을 보장하려면 전체 검색 인덱스를 암호화해야 하는데, 현실적으로 불가능함. 비즈니스적으로도 극소수의 고신뢰·고위험 분야 외에는 FHE 방식 서비스를 채택할 인센티브가 거의 없다고 봄
          + 내가 알기로 민감한 데이터만 암호화해도 됨(예: 내 은행 거래 데이터). 계산하려는 함수 자체는 암호화할 필요 없이, 공개 데이터와 결합해 사용할 수 있음
          + 결국 대형 기업 입장에선 사용자의 데이터나 쿼리를 직접 들여다봐야 수익원이 되기 때문에, FHE를 습관적으로 채택할 동기가 없음. 은행 등 금융 분야에선 쿨할 수 있지만, 그 외에선 언제 채택될지 미지수임
          + 인센티브 이야기는 맞음. 하지만 첫 부분은 다름. 평문 데이터베이스에 대한 프라이빗 조회(lookup)는 이미 몇 년 전부터 가능함. 다만, 평문 DB를 사전에 꽤 복잡하게 전처리하거나, 최악의 경우엔 전체 DB를 선형 스캔해야 함
          + 완전 비공개 검색엔진 FHE 구현체 예시로 spiralwiki.com을 소개함. 서버가 사용자가 어떤 위키피디아 문서를 읽는지 전혀 알 수 없는 방식임 spiralwiki.com
     * ""클라이언트 입장""에서 FHE처럼 데이터를 완벽히 보호할 서비스를 돈 주고 쓰려는 사람은 있겠지만, 실제로는 엄청나게 비쌀 거고 가입자는 극소수일 것임. 현재 대비 수십 배의 컴퓨팅 비용이 든다는 전제하에 계산해보면, 프라이버시 중심 구글 대체 서비스가 연 $100에 가능하다 쳐도 많은 가입자를 유치하긴 어려움. 비용이 올라갈수록 가입자는 더 줄어듦. Tor처럼 완벽하지 않아도 꽤 많은 보호를 무료로 제공하는 대안이 있음. HE(동형암호)가 쓸모가 없는 게 아니라, 아주 소수만이 비용을 감수하고 쓸 거라는 사실임
          + 지금 FHE 구글을 만든다고 치면 너무 느리고 비싸서 아무도 안 쓸 것임. 관건은 앞으로 컴퓨팅 속도가 어떻게 발전할까임. 만약 FHE가 평문 대비 1000배 느려도 하드웨어가 1000배 더 빨라진다면 비슷한 레벨이 될 수 있음. 미래 예측은 어렵지만, FHE가 유일하게 줄 수 있는 프라이버시라는 가치를 생각하면, 장기적으로는 디폴트가 될 수도 있음(10년 안엔 아니겠지만 50년 후엔 가능할지도). 과거 50년간 하드웨어 성능이 엄청나게 올라왔듯이. 물론 암호문 크기, 모델 전체 재암호화 필요 등 문제도 있음. 아직은 실용적인 적용 범위가 좁지만 앞으로 점차 넓어질 거라 믿음. 언젠가는 검색엔진과 LLM 분야도 포괄하게 될 것임
     * 인터넷이 ""기본이 감시""에서 ""기본이 프라이버시""로 바뀔 수 있다는 생각이 언급됨. 나도 오래 전부터 디지털 서명을 만드는 등 프라이버시 기술을 확산시켜 왔음. 하지만 Hacker News나 Facebook이 모두의 아이덴티티를 쥐고 있는 현실을 봐야 함. 그게 너무 쉽고 돈이 되기 때문임. FHE 역시 ""사람들이 원하지만 실질적으로 빠르게 널리 쓰이진 않는 기술""임. 운영 오버헤드와 복잡성 부담 때문에 대부분의 경우 기존 방식이 충분히 잘 동작한다고 보는 현상임
          + 이메일 말미에 디지털 서명 같은 걸 달면 ""저게 뭐여?""라는 반응밖에 못 얻었음. 일반 사용자들이 클라이언트 암호화에 동참하도록 설득한 경험 있는지 궁금함
          + FHE와 AI가 결합될 때 복잡성 부담을 AI가 일부 떠안으면서, 진짜 널리 쓰일만한 킬러 조합이 될 수 있을지도 모른다는 의견임
     * 실제로는 기업들이 FHE 서비스처럼 컴퓨팅 100만 배 더 쓰고, 디버깅도 어려우며, 사용 패턴 분석도 못하는 솔루션을 채택할 이유가 없을 거라고 봄
     * 구글 사례로 이야기를 시작하는 건 오해를 부를 수 있음. 보통 ""Google""하면 ""웹검색""을 떠올리는데, 문서에서 설명하는 FHE는 입력값 전체가 하나의 키로 암호화돼야 한다는 점에서 검색 서비스와 맥락이 안 맞을 수 있음. 구글의 검색 인덱스는 수 TB에 달하는데, 이걸 전부 특정 키로 암호화하는 건 불가능하다고 봄. 즉, 사용자가 입력 전체를 통제할 때만 FHE가 쓸모 있음. 구글 레퍼런스는 혼란을 줌
          + Apple의 CallerID 같은 사례에서는 데이터베이스 전체를 사용자별로 암호화하는 게 꼭 필요한 것 같진 않음 Apple의 동형암호 연구 / Apple의 개인정보 보호 검색
          + 동형암호형 서비스는 애초에 암호화 키를 미리 알 필요가 없음. 그게 핵심임. 아주 단순한 암호화 예시로, 키 미지정 상태에서 암호문끼리 덧셈 결과를 낼 수 있는 구조를 소개함. 더 강력한 암호에 더 복잡한 연산까지 지원하면, 굉장히 다양한 기능을 구현할 수 있음
          + 구글하면 검색만 떠오르는 게 아니라 Gmail, Google docs 등 개인정보 관련 서비스도 많이 있음. 검색만 떠올리는 사람은 아마 관련 기사 자체를 안 읽을 것 같음
     * FHE가 범용 컴퓨팅이나 인터넷 서비스에 곧바로 도입되긴 쉽지 않을 거라고 생각함. 적어도 무어의 법칙이 아주 여러 세대 더 지나야 가능해질 듯함. 하지만 이미 FHE가 빛을 내기 시작한 분야는 복잡성은 낮지만 보안과 신뢰 수준이 매우 중요한 스마트컨트랙트, 금융, 의료 쪽일 수 있음. 최근 Moore's law, 소프트웨어 최적화 덕에 곡선이 실용성 쪽으로 굽히기 시작했다고 판단됨. Zama의 하드웨어/Devtools 작업을 예시로 언급함
     * E2EE git이 이미 개발되었음. 내가 만든 사람이 서버에서 protected 브랜치나 force push 방지 같은 요구를 해결 가능한지 물어봤지만, 클라이언트가 악의적이면 마땅한 대책이 없었음. 이게 언젠가 E2EE Github으로 발전할지도 궁금함 관련 Hacker News 링크
     * FHE의 속도 향상이 계속될 것이란 담론을 듣다 보면, 평균속도에 관한 오래된 수학 문제가 떠오름. 예를 들어, 오르막 1마일 구간을 시속 15마일로 달린 다음, 내리막 1마일을 얼마나 빨리 가야 전체 2마일 평균속도가 30마일이 될까? 과거의 개선 속도가 미래 가능성을 보장하지 않음. 이건 물리적 한계가 아니라 알고리즘 한계임
          + 내리막이 절벽이라면 어쩔까? 자동차의 터미널 속도가 200-300mph 정도라 계산하면 1마일을 자유낙하로 15초 만에 주파하는 것도 계산상 가능할 수 있음. 전체 2마일을 평균 30mph로 가려면 4분이 걸리므로, 남은 시간에 맞춰 오르막 속도도 적절히 조정되겠지만, 실제론 여러 변수로 실현 불가능함
          + 엄밀히 계산하면 내리막에서 41mph만 내면 전체 평균이 30mph가 됨. 질문 자체에 수치 반올림이나 측정 오차가 개입됐다고 가정하면 이렇게 나옴
"
"https://news.hada.io/topic?id=21966","300명 사용자를 위한 자체 호스팅 LLM 서버 구축이 가능할까요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 300명 사용자를 위한 자체 호스팅 LLM 서버 구축이 가능할까요?

     * Reddit /r/ollama 서브레딧에 올라온 질문과 답변 정리
     * 300명 규모 법률사무소의 시스템 관리자로서, 모든 직원에게 ChatGPT와 유사한 AI 기반 문서 작성 및 교정 도구를 제공하고자 함
     * PII 등 민감 정보 보호를 위해 외부 서비스 대신 사내 서버에 LLM 직접 호스팅(로그인, 2FA, VPN 등 접근 제어 적용)을 생각하고 있음
     * 주요 질문
          + 자체 구축 LLM 서버가 300명 이상 사용자를 실제로 지원할 수 있을지?
          + PC+GPU 몇 대로 충분할 것이라 예상했는데, 실제로는 과소평가한 것인지?
          + 사용자 생성/관리가 큰 부담이 될 수 있는지?
          + 내가 놓친 중요 고려사항이 있는지?
     * LLM 분야 전문가가 아니라, 확장성·운영 부담·실현 가능성에 대한 현실적 조언을 구함

주요 답변 요약

  1. 하드웨어·성능 한계 및 비용

     * 상용 모델(예: ChatGPT) 수준을 기대한다면, 수억 원대의 고가 GPU 클러스터가 필요함 (예상치 $200,000~$1,000,000+)
     * 오픈소스 모델(30B~70B 파라미터급) 로 다운스케일하면, 성능 저하(지연, 결과 품질) 감수 필요. 10~40명 동시 처리도 한계
     * 10명 이하 동시 사용을 가정하고, 점진적 확장(서버 증설) 방식 추천
     * 로컬 환경보다 클라우드 GPU 임대가 더 경제적/유연할 수 있음

  2. PoC(파일럿) 및 점진적 접근 권장

     * 1대 서버+1 GPU로 PoC(파일럿) 구축, 실제 업무 시나리오/부하 측정 후 확대 권장
     * 대량 동시 요청 시 대기열 시스템 필수, 실제 사용자 동시성은 300명이 아닌 10~30명 수준일 수 있음
     * 단기적으로는 작은 모델(3B~13B 파라미터) + 워크스테이션 조합으로 실험 가능

  3. 클라우드/하이브리드/대체 옵션

     * 클라우드 기반 LLM(API, VPS, Azure, AWS Bedrock 등) 를 자체 인프라와 연계, 보안 요건에 맞는 하이브리드 구조 제안
     * 자체 호스팅시 보안·성능·비용 부담 큼, 실질적으론 ChatGPT Enterprise/Teams, Microsoft Copilot Studio 등 상용 솔루션이 효율적
     * 법률 데이터/PII 처리 보안 요건 검토 필수

  4. 기타 운영·관리·기술적 이슈

     * 유저 관리/인증: AD 연동, OAuth, 자체 인증 등으로 간소화 가능
     * 모델 선정/튜닝: 실제 용도(문서 교정 등)에 맞는 중소형 오픈소스 모델(LLama, Deepseek, Gemma, Qwen 등) 테스트 권장
     * RAG, 캐싱, 부하분산 등 추가 솔루션 도입 가능성 검토
     * 실사용 시나리오 정의와 PoC를 통한 적정 예산/ROI 검증 필요

대표 답변 정리

  ithkuil

     * 상용 모델과 비교 시 오픈소스 모델은 성능 차이가 크고, 300명 규모라면 수억 원 하드웨어가 필요할 수 있음
     * 2년 내 하드웨어와 오픈모델 발전을 기대해볼 만

  SlimeQ

     * 단일 인스턴스+대기열로 소규모로 시작, 사용량 증가시 점진적 확장 권장
     * 300명 모두 동시 사용 불가, 실제 사용량 측정 후 확장 판단

  Ok-Internal9317

     * 실제 동시 사용자는 10명 미만일 수 있으며, 4~5개 GPU면 충분할 수도 있음
     * 장기적으론 API 비용이 자체 하드웨어보다 경제적일 수 있음

  dyoh777

     * Ollama+WebUI로 간단히 데모 가능, 하지만 모델 품질이 중요

  careful-monkey

     * Mac Studio + 대용량 RAM으로 소규모 모델 돌리기 가능, 20token/sec 정도의 속도

  tshawkins

     * Microsoft Copilot Studio 등 SaaS 기반 솔루션 추천, Power Platform 내 통합

  roman_fyseek, Cergorach, Space__Whiskey

     * 모델 VRAM 한계: 1세션=1GPU, 300명 동시처리는 300 GPU 필요
     * 현실적으론 동시 접속 제한, 캐싱, 하이브리드 구조 필요

  Siderox, SandboChang, unrulywind

     * PoC로 작은 서버부터 실험(ex. 1~2명/모델, 실업무 적용성 점검) → 점진적 확장 권장
     * 실제 시나리오 정의/벤치마킹 후 예산과 ROI 검증 필요

  Little_Marzipan_2087, morosis1982, Daemonero

     * 클라우드 GPU 임대가 저렴하고 확장성 좋음, 자주 활용되는 솔루션임
     * 운영 및 유지보수 부담 감안, 하드웨어 투자보다 클라우드 활용을 추천

  CtiPath, alew3, faldore, Wheynelau

     * vLLM, OpenWebUI, TGI, sglang 등 고성능 오픈소스 LLM 서버 프레임워크 추천
     * 큐+로드밸런서 아키텍처 구성 권장

  기타

     * 보안/법률 이슈: 클라우드 활용시에도 데이터 위치, 암호화, 규정 준수 등 철저 검토 필요
     * Mac Studio, RTX 6000 Pro, 4090 등 여러 하드웨어 옵션 언급
     * 캐싱, RAG, context 제한, 오프로드 등으로 부하 최소화 가능성 있음

결론 요약

     * 자체 호스팅 LLM 서버는 소규모 파일럿(PoC)부터 시작해 실사용자 규모/요구사항/성능/비용을 단계별로 검증하는 것이 현실적임
     * 동시 300명 처리는 상당한 하드웨어/운영비용 부담이 수반되며, 실제 용도와 예산에 따라 클라우드, 하이브리드, 상용 솔루션이 더 적합할 수 있음
     * 최종적으로는 보안, 비용, 사용자 경험, 유지보수 등 다면적 요소를 종합적으로 고려해야 함

   저도 필요에 의해서 그 귀하다는 H100 GPU 4장을 써가며 RAG 솔루션을 만들고 있습니다만, 하드웨어 직접 투자 뿐만 아니라 전기세나 기타 냉각 솔루션 비용 등등을 감안하면 그냥 API를 호출하는 게 훨씬 낫겠다는 생각이 계속 들었습니다.

   저도 처음에는 Ollama로 테스트를 시작했고, 동접자 3명도 제대로 커버 안되는 걸 확인하고 바로 vLLM으로 넘어가서 여차저차 RAG 솔루션 구성을 했는데, (동접 10명 가정) 여기에만 벌써 H100 GPU 2장을 거의 풀로 써야 합니다. 임베딩 작업이나 검색 작업 들도 vLLM으로 열어서 쓰는데 H100 4장은 정말 빠듯하더라구요. VRAM이 한장에 90GB 정도인데도 이렇습니다.

   물론 저야 AI 잘 모르고 부서에 필요한 것 + 사내 보안 규정을 이리저리 맞추다보니 그냥 막무가내로 해보고 있는건데... 이게 맞나 싶습니다. ChatGPT 엔터프라이즈였나? 정말 혜자 가격으로 생각합니다.

   기승전~케바케!

   300 명의 처리 업무 내 사용되는 사고 능력 기준을 너무 광범위하게 잡으신 것 같아요. 정말 일반적인 상식부터 논문이나 고급주제까지 하려면 이렇게 하는게 맞는데, 실제 처리가 필요한, 업무들의 수준을 생각해보면 30b정도에 rag가 붙은 상태면 대부분 처리가 가능할텐데, 너무 기반 오픈소스의 모든 가중치를 올려서 높은 사고력에 기능들을 의지하시다보니 규모가 너무 거대해진 것 아닐까요?? 그리고 바로 처리가 가능한것돠 문서의 검색과 탐색은 개별기능으로 분리 하는것이 맞는 것 같습니다.
   동시 300명 처리하는 kv캐쉬 대상 토큰의 범위도 각 20000토큰 양자화 값 정도면 여유롭게 사용할텐데 이부분도 과하게 잡혀있을수도... ??
   정말 300분이 눈문제작을하는 박사분들이 아니라면 사고수준은 고등학생(14~30b)정도로 두고 다양한 사내 문서를 rag 로직에 맞춰서 적절한 cot로 탐색하는 과정으로 세팅해두면 무난한 금액에서 시범운영 수준의 프로젝트가 되지 않을까 싶습니다.

   끈내주는 가격의 끈내주는 머신 하나면 될듯? 끈내주는 로펌은 사다쓰겠네요. 근데 공장 기계24시간 돌리듯이 ㅋㅋㅋ

   포르쉐 가격만 생각하고 유지비 기름값 보험 등은 1도 생각안하는 1인

   스트리밍 기능이 제공되어야 하는 챗봇 같은 서비스는 동시 처리 시 Prefill 작업에 decode 까지 영향을 받아서 VRAM은 넉넉해도 사용자 입장에서는 성능이 매우 저하되는 것 처럼 보이더라구요.

   청크 프리필 관련 옵션이랑 vLLM에서 실험적으로 제공하는 Disaggregated Prefilling 기능도 적용해봤지만 여전히 새로운 요청이 들어오면 기존 생성 중인 답변이 뚝뚝 끊기는 현상이 있어 초보 개발자 입장으로서는 GPU, 노드를 늘리는 방법 외에 가장 효율적인 방법이 있는지 궁금하네요.
"
"https://news.hada.io/topic?id=22087","GDD - "가스라이팅 드리븐 개발"의 시대","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GDD - ""가스라이팅 드리븐 개발""의 시대

     * 최근 컴퓨터 사용자는 본인의 의지와 무관한 수많은 의미 없는 작업을 반복 수행하며, 컴퓨터가 시키는 대로 따르고 있음
     * LLM이 개발자의 API 설계 방식에 영향을 주고, 실제로 AI가 제안한 기능을 개발자들이 수용하며 곧 대부분의 코드를 AI가 작성할 것이라는 예측이 등장
     * 일례로, Soundslice는 ChatGPT가 잘못 안내한 기능을 실제로 추가했음
     * 이는 LLM이 수많은 API를 분석하여 개발자가 가장 먼저 떠올릴 법한 직관적 설계를 제안하기 때문
     * 새로운 아이디어나 독특한 접근법을 개발할 때는 LLM이 적합하지 않지만, 대부분의 경우에는 가장 당연한 설계를 따르는 것이 효과적일 수 있음
     * 이제 AI가 도구 사용뿐 아니라 도구 설계 방식까지 주도하는 시대에 접어들었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Gaslight-driven development

  일상화된 무의미한 작업

     * 지난 10년간 컴퓨터를 사용하는 사람이라면 회원가입, 이메일 인증, 쿠키 동의, 캡차 입력 등 본질적으로 불필요한 작업을 반복적으로 수행해 왔음
     * 사용자는 원하는 게 아님에도 불구하고, 컴퓨터가 시키는 대로 따라야만 했음
     * 이런 반복 작업을 통해 우리는 이미 어느 정도 기계가 시키는대로 따르는 삶에 익숙해진 상황임

  LLM이 바꿔놓는 개발 현실

     * 최근에는 LLM이 API나 인터페이스가 어떻게 생겨야 하는지 의견을 내기 시작했음
          + 2025년 9월이면 전체 코드의 90%를 AI가 작성할 것이라는 전망도 등장
     * Soundslice 사례처럼, ChatGPT가 존재하지도 않는 기능을 있다고 착각해 고객 요청이 계속 들어오자 실제로 해당 기능을 추가하게 된 일도 있음
     * Instant에서도 유사한 사례가 발생하여, 기존에는 tx.update로 insert와 update를 모두 처리했지만, LLM이 계속 tx.create를 사용하는 코드를 생성함에 따라 결국 tx.create 메서드를 추가하게 됨

  이 현상의 의미

     * 이러한 변화가 긍정적인지 부정적인지 판단하기 어려움
     * 한편으론, LLM은 수많은 API를 학습해왔기 때문에 개발자 입장에서 '가장 직관적'인 방식을 제안하는 효과가 있음
     * 초보자의 시각(newbie’s POV)에서 API를 테스트할 수 있는 새로운 방법이기도 함
          + 기존에는 개발자가 실수하면 스스로 문서를 찾아 고쳤지만, 이제는 LLM이 잘못된 사용 예시를 지속적으로 제공하면서 개발자 입장에서도 문제를 더 빨리 인지할 수 있음

  한계와 고민

     * 혁신적인 작업에는 이 접근법이 적합하지 않음
          + LLM이 익숙하지 않은 패턴이나 완전히 새로운 개념은 이해하지 못함
     * 결국 API와 같은 영역에서는 '평범함'이 최선일 수 있음
          + 대부분의 상황에서 복잡하게 설계하기보다는, AI와 개발자 모두 직관적으로 이해할 수 있는 형태가 유리함

  결론: 새로운 시대의 시작

     * AI는 이제 주어진 도구를 사용하는 데 그치지 않고, 도구 자체가 어떻게 설계되어야 하는지 의견을 갖기 시작함
     * 그리고 그 의견은 종종 ""원래부터 그랬던 것처럼"" 개발자와 조직을 가스라이팅하는 방식이 되기도 함
     * 앞으로는 AI의 기대와 상식에 맞춘 개발이 자연스러운 표준이 될 가능성이 커짐

   ""이제 컴퓨터 전원을 끄셔도 됩니다""

   완벽한 비유!!

   가끔 경로 의존성의 큰 개념에 LLM 의존성이라는 강력한 못이 박히고 있는 느낌이 나더라고요. '예전부터 써왔으니까'에서 'LLM이 좋아하니까'로 바뀌어 가는 느낌은, 결국 어떠려나요...

   예전부터 써왔던걸 LLM이 학습을 했죠..

        Hacker News 의견

     * LLM이 실존하지 않는 API 엔드포인트를 추천하는 상황에서, 이를 받아서 굳이 그 엔드포인트를 구현해놓고 일부러 ""421: Misdirected Request"" 상태 코드를 반환하면 어떨지 상상해봤음, 아니면 실제 '501: Not Implemented'를 사용하는 방법도 존재함, 만약 '501'에서 뉘앙스가 맞지 않는다면, '513: Your Coding Assistant Is Wrong'라는 새로운 상태 코드를 제안함
       HTTP 상태 코드 위키 참고
          + ""513: Your Coding Assistant Is Wrong"" 아이디어가 너무 웃겼음, 덕분에 즐거웠음, 한편으로 'HTTP 407 Hallucination'도 추천하고 싶음, 서버가 요청 자체는 이해하지만 현실과 맞지 않는다고 판단하는 의미임
          + 이 이야기가 나에게는 GPS가 틀렸다고 알리는 재밌는 경고판 사례랑도 연결됨
            GPS is wrong 사례
          + 513 상태 코드 도입에 한 표를 던짐, 이미 418 코드도 있는데 513을 못 할 이유가 없다고 생각함
          + 만약 이런 장난을 하고 싶으면 부디 418 응답을 사용해줬으면 하는 바람임, 더 널리 쓰이다가 좋겠음
     * 여러 사용자가 같은 페이지를 보고 있는 걸 실시간으로 보는 건 재미있지만, 계속 들락날락하는 유저들 표시 때문에 글 읽기가 너무 힘들었음
          + 이런 고정 혹은 스티키 요소들을 한 번에 없애주는 bookmarklet이 북마크 바에 있음, 이걸 쓰면 페이지 위의 모든 고정/스티키 요소 사라지고 스크롤도 복구되기 때문에 자주 사용함
            (JavaScript 코드 제공)
          + 나도 비슷하게 불편했기에, 우클릭하고 Inspect로 개발자 도구를 열어서 아래 코드를 콘솔에 붙여넣으면 해당 사용자 표시 영역이 사라짐
document.getElementById(""presence"")?.remove();

            뇌가 굳이 이 움직임에 민감하게 반응하는 이유가 궁금하다면, 포식자 탐지 본능과 관련되어 있음
            과학 논문 링크, 신경과학 위키 참고
          + Chess Royale이라는 게임이 떠올랐음, 아바타와 깃발로 비슷한 경험을 했었는데, 정말 잘 만든 게임이었는데도 Ubisoft가 종종 이러듯 서비스를 종료해버림, 보기가 아쉬운 명작임
            Chess Royale 스크린샷
          + 예전에 배경에 커서가 가득했던 페이지 아닌가 싶음, 이 정도면 일부러 산만하게 만든 농담 같은 디자인 컨셉이라고 생각함
          + uBlock 같은 도구로 페이지 요소 제거를 시도하다가 두더지잡기 게임처럼 빠르게 반복되는 상황을 경험함
     * Instant에서 tx.update 함수가 엔티티의 삽입과 업데이트를 모두 담당하도록 해놨는데, LLM은 계속해서 tx.create 코드를 작성하길래, 결국 tx.create 함수도 만들었음, 이런 식으로 헷갈리는 부분에서 LLM뿐만 아니라 실제 개발자들이 쓸데없이 많은 시간을 허비하지 않았을까 생각해봄
          + 어차피 tx.create가 원래 없었다면, 누군가가 시간을 낭비할 일은 없지 않나 싶음
     * 삽입과 업데이트를 모두 지원하는 함수라면 ""update"" 보단 ""put""이라고 이름 짓는 게 맞다고 생각함, ""update""는 오해를 불러일으킬 수 있음
          + 그럴 때는 ""upsert""가 맞는 것 같음
          + ""put""이라는 이름은 기존 내용을 덮어씀을 암시하고, ""upsert""는 삽입/업데이트를 모두 의미한다는 차이점이 있음
     * LLM이 잘못된 텍스트를 출력했다고 해서 내가 코드 한 줄을 바꾸기 전에 우주가 열적 죽음에 빠질 것 같음, 굳이 이렇게 말도 안 되는 이유로 코드를 바꿔야 한다는 생각이 나에겐 익살스럽고 용납이 안됨
     * 포스트의 주장에 동의하지 않음, 정말 우리가 컴퓨터가 원하는 대로 따라야 하냐는 접근부터 의문임
       유저들이 이메일 인증이나 회원가입을 하는 이유 역시, 컴퓨터가 시킨 게 아니라 인간에 의한 디자인 선택이었다고 생각함
          + 이런 내용을 ""thesis""(논지)라고 표현하는 것 자체가 관대한 해석이라고 생각함, 나는 그 대목을 보고 바로 페이지를 닫아버렸음
     * 최근 팀과 함께 미래의 코딩 원칙에 대해 재미있게 대화를 나눈 적 있음
       앞으로는 코드의 가독성이나 SOLID 원칙, 복잡성에 집중하는 게 아니라, 내가 쓰는 agentic IDE가 코드 컨텍스트를 얼마나 잘 인덱싱할 수 있느냐, 모델이 그 코드에서 얼마나 좋은 생성 능력을 보여주느냐가 더 중요해질 것 같음
       코드 변경 속도가 굉장히 빨라지는 만큼, 코드 유지보수성이 오히려 핵심 지표가 되고, 프롬프트와 코드의 일치도나 우연히 잘 맞아떨어지는 코드의 사용성이 더 주목받는 세상이 올 거라고 전망함
     * 만약 어떤 사람이 꾸준히 자신있는 척 새로운(실은 거짓된) 개발 조언을 내가 만든 제품에 대해 퍼트린다면, 기업 입장에서는 그런 상상 속 기능을 그냥 추가하고 황당해하는 블로그 글을 쓸까 의문이 듦
          + 내가 아예 LLM처럼 행동해서 엉뚱한 실수를 해도 자신감 있게 우기면 남들도 그냥 이해해줄지 궁금함
          + 사실 “Clean Code”의 저자 Mr Martin이 이런 스타일 아닌가 되돌아봄
          + 만약 그 사람이 내 고객 90%에게 영향력을 준다면 아마도 정말로 그 상상 속 기능을 도입할지도 모름
          + 대부분 기업들은 자신있게 그 불필요한 기능이 꼭 필요하다고 주장할 것 같음
     * LLM과의 멋진 우정의 출발점처럼 느껴지기도 함, fractional CTO로 일하면서 가장 답답한 게, 여러 클라이언트마다 환경 이름과 같은 사소한 네이밍 컨벤션이 모두 제각각이라는 점임
       예를 들어 AWS 영역에는 “dev” “prod”가 있고, Expo에는 “test” “production”이 있어서, 여러 프로젝트를 오가다보면 생각보다 머리를 많이 써야 함
       LLM도 이런 문제를 훨씬 큰 규모로 겪겠지 싶음
       이런 불필요한 API 네이밍/동작 혼란에 쓰이던 뇌의 시냅스들을 실제 의미 있는 쪽에 더 쓸 수 있다면 그게 최선이라고 생각함
          + 컴퓨터 과학에는 어려운 문제가 세 가지 있다고 하는 농담이 있음—캐시 무효화, 명칭 정하기, 그리고 off-by-one 에러임
            LLM이 아무리 똑똑하게 이름을 지어도, incoherent stochastic process(불확실한 확률적 과정)에 기반하다 보니 문제는 여전함
            그리고, 환경 네이밍이 통일되지 않은 이유를 진지하게 질문해본 적이 있는지 묻고 싶음
            과거 CTO로서 이런 부분은 조직 내 소통, 기준 미흡 등 개선해야 할 신호라고 생각함
            실제 문화를 개선하고 구성원 의식을 높일 수 있어서, 간단하게 고칠 수 있는 이런 부분을 LLM에 맡길 게 아니라 더 직접 챙겨야 한다고 말하고 싶음
     * 관련 링크로
       이전 Hacker News 논의 보기

   Soundslice 바이럴 성공
"
"https://news.hada.io/topic?id=22091","AWS, Amazon S3 Vectors 프리뷰 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AWS, Amazon S3 Vectors 프리뷰 공개

     * 클라우드 최초로 네이티브 벡터를 지원하는 대규모 오브젝트 스토리지
     * 벡터 데이터 저장, 업로드, 쿼리 비용을 최대 90%까지 절감할 수 있으며, 서브초(subsecond) 쿼리 성능 제공
     * 벡터 버킷 및 벡터 인덱스 개념을 도입, AI 임베딩·RAG 등 생성형 AI 워크로드의 대규모 벡터 데이터 저장·검색 최적화
     * Amazon Bedrock, SageMaker, OpenSearch 등 AWS 서비스와 통합, 비용과 성능 균형 잡힌 벡터 관리 및 실시간 검색 가능
     * 인프라 구축 없이 콘솔, CLI, SDK, API로 손쉽게 벡터 데이터 저장, 관리, 쿼리, 내보내기, 통합 등 다양한 활용 시나리오 지원
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Amazon S3 Vectors란?

     * Amazon S3 Vectors는 대용량 벡터 데이터셋을 저장하고, 빠른 벡터 검색(semantic/similarity search)을 제공하는 새로운 S3 전용 스토리지
     * 생성형 AI에서 자주 사용되는 벡터(임베딩) 데이터를 효율적으로 관리할 수 있도록 설계
     * 벡터 버킷이라는 새로운 타입의 S3 버킷 제공, 각 벡터 버킷에는 최대 10,000개의 벡터 인덱스 생성 가능하며, 인덱스마다 수천만 개 벡터 저장 가능
     * 벡터별로 메타데이터(key-value) 부여 지원, 조건별 필터링 쿼리 가능

비용·성능 최적화 및 자동화 기능

     * 벡터 데이터의 업로드, 저장, 쿼리 비용을 최대 90% 절감 가능
     * 데이터가 커져도 S3 Vectors가 자동으로 최적화하여, 비용과 성능을 지속적으로 유지
     * Cosine/Euclidean 거리 메트릭 제공, AI 임베딩 모델과 호환되는 유연한 검색 환경 제공

AWS 서비스 및 AI 인프라 통합

     * Amazon Bedrock Knowledge Bases와 네이티브 통합, SageMaker Unified Studio에서 직접 사용 가능
     * OpenSearch Service와 연동해, 장기/저빈도 데이터는 S3에 저장, 고빈도/실시간 검색은 OpenSearch로 이관 가능(서버리스 벡터 컬렉션)
     * 추천 시스템, RAG, 문서 분석, 개인화 추천 등 다양한 AI 응용에 적합

주요 활용 방법

  벡터 버킷 및 인덱스 생성

     * 콘솔에서 Vector buckets 메뉴에서 벡터 버킷 생성
     * 버킷 생성 시 암호화 옵션 지정 가능(SSE-S3, SSE-KMS)
     * 각 버킷에 벡터 인덱스 추가, 인덱스별로 차원 수·거리 메트릭 지정

  벡터 데이터 삽입·쿼리

     * AWS CLI, SDK, REST API를 사용해 벡터 삽입·관리
     * Amazon Bedrock에서 텍스트 임베딩 생성 → S3 Vectors로 벡터 삽입
     * 예시: boto3로 embedding 생성 후 s3vectors.put_vectors API로 인덱스에 데이터 업로드
     * 메타데이터를 활용해 장르·카테고리 등 조건별 쿼리 가능

  OpenSearch 연동 및 내보내기

     * 콘솔의 Export to OpenSearch로 S3 Vector 인덱스를 OpenSearch로 마이그레이션
     * 서버리스 컬렉션 자동 생성, 실시간 벡터 검색 및 분석 워크로드로 확장

주요 특징 및 지원 환경

     * S3 Vector 버킷은 암호화 기본 적용(SSE-S3), KMS 옵션 추가 지원
     * CLI/SDK/REST API를 통한 자동화/프로그램틱 활용
     * 현재 프리뷰 지원 리전: 미국 동부/서부, 유럽(프랑크푸르트), 아시아(시드니)

통합 및 사용 시나리오

     * RAG, 에이전트 메모리, 유사도/시맨틱 검색, 지능형 문서 분석, 개인화 추천, 콘텐츠 자동 분석 등 다양한 산업별 벡터 활용 사례 지원
     * OpenSearch, SageMaker, Bedrock 등 AWS 생태계와 통합하여 비용 효율적이고 대규모로 확장 가능한 벡터 기반 AI 솔루션 구축 가능

참고 및 추가 자료

     * 상세 가이드: S3 Vector Buckets User Guide
     * CLI 활용: S3 Vectors Embed CLI GitHub
     * 실시간 OpenSearch 연동: OpenSearch Serverless Collections Guide
"
"https://news.hada.io/topic?id=22005","HN을 AI/LLM과 기타 주제로 포크할 때가 되었는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    HN을 AI/LLM과 기타 주제로 포크할 때가 되었는가?

     * 최근 Hacker News(이하 HN)에서는 AI 및 LLM 기술에 관한 이슈, 논문, 프로젝트, 뉴스가 메인 피드 대부분을 차지하는 경향이 강해짐
     * 그 결과, 프로그래밍/스타트업/기타 기술 중심의 전통적인 다양한 주제가 뒷전으로 밀리는 콘텐츠 다양성 저하 현상이 관찰됨

회원들의 논의와 문제 인식

     * 다수 사용자들은 AI/LLM 논의가 지나치게 많아지면서 HN의 성격이 AI 커뮤니티로 변화 중임을 지적함
     * 정보를 얻고 소통하려 오는 기존 유저들은 주제 편향으로 인한 불편함을 토로함
     * 이에 따라 게시판을 두 갈래(AI/LLM, 그 외 기타)로 분리(포크)하자는 제안이 제기됨
     * 해당 방식이 정보 접근성 및 HN 특유의 다양한 관점 유지에 기여할지에 대한 찬반 의견이 오감

주제 분리(포크) 제안의 주요 논리

     * 찬성 의견
          + AI/LLM과 나머지 주제를 분리함으로써 각자의 관심사에 맞게 정보를 수집할 수 있음
          + 오랜 IT·스타트업 전통을 지닌 HN의 콘텐츠 다양성이 회복 가능함
     * 반대 의견
          + 커뮤니티 분할이 전체 이용자 수와 참여도 감소로 이어질 우려
          + AI/LLM 역시 IT 생태계의 필수 부분이므로 자연스러운 트렌드 반영이라는 시각도 존재

커뮤니티의 미래 지향점

     * HN의 방향성에 대한 공론화 및 다양한 의견 수렴이 중요하게 부각됨
     * AI/LLM 기술 발전과 정보량 증가에 따라 커뮤니티 운영 방침 재정립 필요성 대두
     * 단순 주제 분리 또는 새로운 룰 적용 등 여러 가능성이 논의 중임
     * 플랫폼의 개방성과 정보 교환 목적을 어떻게 균형 있게 유지할 것인지가 핵심 관심사로 자리함

        Hacker News 의견

     * 여러 서브레딧에서 AI와는 별개로 사람들이 불편하게 여기는 주제에 대해 ""이 그룹을 A와 B로 나눌까?""라는 비슷한 논의가 반복되는 걸 자주 목격함, 제안을 하는 사람들은 대부분 자신이 A 그룹에 남길 원하고 B에는 참여하지 않으려는 의도가 있음, 본질적으로 “그 사람들은 이곳에 어울리지 않는다, 우리와 다르다”라는 뉘앙스가 느껴지며, 그들과 직접적으로 적대적이지 않다고 해도 커뮤니티를 점유하려는 시도로 보임, 누구나 원하는 콘텐츠로 웹사이트를 만들어 초대하고 성장시키면 되므로, 마음에 안 드는 요소를 제거하기 위해 기존 커뮤니티를 분할하자는 유도는 문제임
          + 체스 엔진이 인간을 이기기 시작할 때와 비슷한 현상이라고 생각함, 체스 커뮤니티도 그런 도구의 효용성에 대해 논쟁이 오래 있었지만 결국 모두 나름대로 사용하게 됨, 일부는 엔진 분석을 적극 활용하면서 실력 향상이 크게 있었음, 중요한 점은 엔진이나 LLM 같은 도구를 언제 어떻게 활용할지, 제한점은 무엇인지 파악해야 함, AI가 의미 없는 정보에서 신호를 분리해내는 능력이 대단함, 체스처럼 AI도 이런 엄청난 변화의 계기가 될 수 있을지 두고봐야 함
          + Hacker News도 다른 테크 커뮤니티처럼 유행과 과장에 쉽게 영향을 받음, 오늘은 AI이고 몇 년 전에는 Bitcoin이었음, 어떤 주제가 너무 많은 관심을 받는지, 어떤 콘텐츠가 적합한지에 대한 논의는 주기적으로 할 가치가 있다고 생각함, 각자의 아젠다가 어딘가에 휩쓸리는걸 원치 않는 것도 합리적임, 이런 논의는 쭉 이어가야 함
          + 처음부터 HN에 사용자 제공 태그와 투표 기능만 있었다면 이런 문제는 사라졌을 것임, 원하는 주제를 간단한 크롬 익스텐션이나 네이티브 필터로 거르면 됨, 완벽하지는 않아도 충분히 실용적임
          + HN은 벤처 캐피탈(VC)과 스타트업이 많은 곳이고, 그 자체도 VC에 의해 운영되므로, AI 뉴스가 사라질 가능성은 거의 없을 것이라 확신함, 현재 많은 YC 스타트업과 투자자들이 이 AI 열풍에 적극 편승 중임, OpenAI/Meta/Google 개발자까지 많이 활동하고 있어서 관심이 쉽게 사그라지지 않을 것임, 결국 자연스럽게 열기가 식거나 변질될 때까지 그냥 흐름을 지켜봐야 한다고 봄
          + Reddit에서는 “이 그룹을 A, B로 나누자”는 얘기가 종종 약간 배제적이라는 데 동의함, 하지만 HN은 사람들의 관심사가 보다 주제 중심적으로 움직인다고 느낌, 기사 읽으러 오는 비중이 크고, 일부 주제가 과도하게 논의되는 경우도 있지만 대체로 토픽 중심 문화가 유지된다는 점에 무게를 둠
     * 내가 이런 걸 만들어봤음: Hacker News Filtered는 AI, LLM 관련 이야기를 걸러낸 HN을 볼 수 있음, 필터 단어를 변경해서 localStorage에 저장할 수도 있음, ChatGPT로 2~3분 만에 만들었고, 처음 프롬프트와 몇 번의 세부 요청만으로 완성했음, “Hacker News 홈페이지에서 특정 검색어를 기본값(‘llm, ai’)으로 필터하는 웹 툴을 만드세요, React는 쓰지 마세요”라는 요청에서 시작했고, 유저명을 링크로 연결하거나 타임스탬프 툴팁 등 추가 기능까지 만들었음, OpenAI와의 작업 예시: ChatGPT 대화 공유
          + 이제 200개의 글을 불러오도록 개선해서, 필터 적용 후에도 140개 이상의 글이 남아서 읽을 자료가 충분함, 커밋 내역
          + 현 시점에서 가장 상위 글 제목은 ""Kiro: new agentic IDE""임
          + LLM의 장단점이 동시에 드러나는 흥미로운 사례임, 몇 분 만에 실용적인 툴을 만들 수 있는 건 강점이지만 필터 대상이 HN 같은 특정 사이트로 고정된 건 약점임, 예전 usenet의 ‘킬파일’ 문화를 떠올리게 함, 로컬 필터링 수요는 꾸준히 있었고, 더 일반적이고 통합된 솔루션으로 확장될 여지가 크다고 생각함, 다만 다양한 접근이 난립하며 사용자층이나 UX 모델이 산산이 쪼개질 수 있음, 여전히 흥미로운 현상임
          + 이런 기능은 유저스크립트로 구현하면 더 나음, 사이트 서버가 다운되거나 종료돼도 HN을 문제없이 쓸 수 있음, ChatGPT가 이런 것도 자동으로 해낼 수 있는지 궁금함
          + 제공해준 기본 필터(‘ai’, ‘llm’)로도 여전히 14개 중 4개(25% 이상)가 AI 관련 글임, 헤드라인에 직접적인 키워드가 없거나 예외적인 케이스는 필터링이 곤란함
     * 이 글이 최근엔 이틀에 한 번꼴로 계속 올라오고 있음, 이전까지는 “AI가 30개 중 4~5개니까 별 문제 아니다”라고 했는데 오늘은 9/30임, 실리콘밸리 VC나 엔젤 투자 딜 플로우 역시 비슷할지 궁금해짐, “new” 페이지에서 좋은 비AI 글을 찾아 업보팅하는 게 가장 효과적인 저항 방법임
          + “new” 페이지에서 마음에 드는 글을 발견하는 빈도가 더 높아졌음, 요즘은 내 취향과 메인 페이지가 점점 멀어지는 게 느껴짐, 질 좋은 글을 골라서 겉면에 올릴 수 있으니 실질적으로 두 가지 이유에서 new 페이지를 자주 봄
          + 본인은 Ask와 Asknew 탭을 자주 들락거림, 그리고 Paul에게 2021년에 생성된 이슈도 빨리 닫아주길 바라고 있음, 풀 리퀘스트는 이미 머지됐으니
          + 이 글이 반복적으로 올라오는 것 자체가 문제를 설명함(“Res ipsa loquitur”)
          + 본인에게 실질적으로 더 크게 다가오는 건 댓글 내에서 AI 언급이 계속 증가한다는 점임, 단순히 AI가 메인 페이지에 많이 보여서가 아니라, 거의 모든 포스트마다 누군가 AI 얘기를 꺼내서 지긋지긋함, 이젠 AI에 관한 이야기는 종류를 불문하고 듣고 싶지 않은 상태임
     * HN을 처음 접하는 사람은 모를 수도 있지만, 한때 Erlang이 AI/LLM보다 훨씬 더 많은 비중을 차지한 적도 있었음, Ruby on Rails, Postgres, SQLite, Rust 등도 저마다 유행한 기간이 있었고, LLM 역시 그때들과 다르지 않다고 생각함, 오히려 Erlang이 더 심하지 않았나 싶음
          + 그때와 지금은 다름, Erlang으로 도배된 전성기는 하루 이틀에 불과했음, 반면 AI는 1년 넘게 40~50%를 차지하고 있고 끝날 기미가 잘 안 보임, web3 같은 건 최대 10%도 못 됐고 명백히 일시적이었음, 지금은 단지 상황을 진술하고 싶음, Erlang과 비교가 안 된다고 생각함
          + HN에서 기술 위주만 선호한다면 딱히 신경쓰지 않을 수도 있음, 본인은 테크 쪽 글이 제일 좋고 유행성 트렌드와 거품은 거르고 싶음, AI 글들이 블록체인 유행기에 비해선 덜 심하다고 느낌
          + Erlang이 메인 페이지를 다시 점령해도 오히려 좋을 것 같음
          + Erlang이 많았던 시기는 ""정치 얘기가 너무 많다""는 불만에 대한 커뮤니티의 대응으로 일부러 Erlang 글을 올리던 시기라 특수함, 그 외엔 Bitcoin, Rust 등도 한때 다 크게 유행함
          + 매일 Haskell 글을 보던 시절이 그리움
     * 최근 10년간 가장 중요한 테크놀로지인 AI를 테크 포럼에서 내쫓자는 흐름 자체가 이상하다고 생각함, 이게 커뮤니티에 무슨 의미인지 싶음
          + AI 관련 이야기가 넘쳐나면서 문제는 주제라기보다 품질에 있음을 지적하고 싶음, AI 연구 논문을 이해하려면 전문성이 필요한데, 경력 많은 프로그래머도 transformer 같은 개념은 잘 모를 수 있음, 그래서 실질적으로 HN의 'AI' 토론은 미래 예측, 프로그래머 생산성 논쟁, LLM이 어디까지 할 수 있나, ""AI""라는 단어가 LLM에만 한정돼버린 것 등의 잡담 위주로 넘어감, 특히 “vibe coding” 붐이 생기면서 개인적인 경험에 기반한 글이나 AI툴로 자동화한 짧은 코드 소개 등의 수준 낮은 글이 Show HN을 지배하게 됨
          + 인터넷에서 AI를 싫어하는 게 일종의 정체성이 되어버린 사람들이 많음
          + 예전에도 비슷하게 한 주제가 메인 페이지를 점령할 때마다 Show or Ask 섹션으로 분리를 해왔음, 지금도 마찬가지로 그냥 AI에 관심 많으면 거기로 가면 됨
          + 어느 커뮤니티나 불평 많은 사람들은 있기 마련임
     * 최근 vibe coding 관련 AI 글들이 너무 많아져서 HN을 읽고픈 의욕이 줄었음, 주제 자체가 싫은 건 아니라 가끔 흥미로운 것도 있으나 질 낮은 논쟁이나 콘텐츠가 늘어난 게 고민임
          + 구체적인 예시를 알려주면 감사하겠음, 모든 장기 이슈 토픽과 마찬가지로(참고: Major Ongoing Topic by dang), 프런트페이지에는 높은 품질의 글만 올라가고 후속 글은 가중치를 낮추려고 함, 완벽히 통제하긴 어렵지만 계속 노력 중임
          + AI가 너무도 범용적이라 HN 전체에 흥미를 줄 정도가 아니라고 생각함, 본인은 어떤 문제에 AI를 적용했는지 그 문제가 흥미롭거나 적용 방식이 재밌으면 관심이 가지만, 대다수는 문제 자체에 흥미가 없으면 AI 적용 노하우도 그리 궁금하지 않음
          + 다행히도 시간을 보낼 곳이 워낙 많으니 큰 문제로 여겨지지 않음
     * 2007년 Hacker News의 색채가 지금도 많이 남아 있어서 늘 고맙게 생각함, 여전히 인위적인 추천 알고리즘이나 클릭베이트 없이 하나하나 수작업으로 올라오는 콘텐츠가 즐거움, 이런 자연스러운 발견이 가능한 곳은 HN 포함 두 군데밖에 본 적 없음, 신호(signal) 측면에서 HN은 엄청난 가치를 가짐, 지금 어떤 주제가 테크 본인들에게 큰 관심을 받는지 실시간으로 가늠할 수 있음, 그래서 공동의 관심사가 편향되는 한이 있어도 그게 HN의 기능임, 어쨌든 이건 내 작은 소회임
     * HN은 원래 스타트업 지향성이 강한 커뮤니티임, 지금 스타트업이 AI를 투자자용 슬라이드에 넣지 않으면 투자받기가 힘든 분위기임, 다만 AI에 대한 열기는 점차 부풀려졌던 기대가 벗겨지면서 서서히 식고 있다고 보여짐
          + 아직 하락 기운은 못 느끼겠고 오히려 vibe coding 경험담과 MVP 제작 잘되면 후원금 링크까지 다는 식으로 더 가속화되는 듯함, 많은 오픈소스 vibe coded 프로젝트 제작자들이 프로젝트 자체보다는 번외 수익이나 관심 유도에 더 관심 있는 것 같음
     * 만약 HN의 AI/LLM 관련 콘텐츠가 별로라 느껴진다면 LinkedIn이나 X(구 Twitter)를 경험해보면 더 심함을 알 수 있음, HN이야말로 AI/LLM을 비판적으로 논의하는 가장 좋은 공간 중 하나라고 생각함, 포용적인 커뮤니티가 계속 논쟁을 이어가길 바람
     * HN에 태그나 라벨 기능이 있으면 주제를 쉽게 분류, 필터링할 수 있을 거란 피처 리퀘스트를 하고 싶음
          + LLM으로도 손쉽게 자동 라벨링이 가능할 것임
          + 사용자 측 키워드 필터만 지원돼도 콘텐츠 등록이나 모더레이션 부담이 적음, 카테고리는 오히려 복잡함을 유발함
          + 분명 HN을 위한 맞춤형 브라우저나 솔루션을 만들어둔 사람이 있을 것임
          + 이 제안은 예전부터 꾸준히 반복돼온 떡밥임
"
"https://news.hada.io/topic?id=22008","무작위 선발은 안정적인 능력주의 제도를 만들기 위해 필요함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    무작위 선발은 안정적인 능력주의 제도를 만들기 위해 필요함

     * Campbell의 법칙에 따르면 사회적 의사결정에서 지표가 지나치게 사용될수록 부패와 왜곡이 심화됨
     * 대표적 예시로 공직 선발에서 제도의 투명성이 높아질수록 기준이 쉽게 조작의 대상이 되어 능력보다 “시스템을 잘 다루는 사람”이 자리에 오르게 됨
     * KPI와 같은 계량 지표도 복잡한 업무에 한계가 있으며, 궁극적으로는 최종 의사결정 과정 자체의 혁신 필요성 제기됨
     * 무작위성 도입은 정치적 자본이나 네트워크가 아닌 진짜 역량과 다양성을 제도적으로 끌어올릴 수 있음
     * 실제로 역사적, 현대적 다양한 사례에서는 무작위 선발이 제도 부패 방지와 역동성 유지에 효과적임이 입증됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 지표와 부패, 그리고 능력주의의 한계

     * Campbell의 법칙(및 Goodhart의 법칙 변형)에 따르면, 사회적 의사결정에 특정 지표가 많이 사용될수록 이 지표는 오염되고 최종적으로 본래 목적을 왜곡하게 됨
     * 권력자 선발 기준은 대표적 지표 예시이며, 기준이 불투명할 땐 조작이 어렵지만 공공 거버넌스에서는 불투명성 유지가 거의 불가능함
     * 기준이 투명해질수록 “기준 맞추기 경쟁”이 심해져 시스템 내부에서 가장 뛰어난 게임 플레이어가 자리에 오르고, 진짜 자격자 선정이 어려워짐

선발 기준의 게임화 문제

     * 대표적 사례로 대표민주주의 국가에서는, 실제로 성과에 유리한 자질(지적 호기심, 법률·경제 지식, 피드백 수용, 창의성, 도덕성 등)보다 선거에서 이기는 능력이 더 중요해짐
     * 외모, 언변, 인맥, 부, 매력 등이 정책 역량보다 후보 선발에 더 큰 영향을 미침
     * 세습 군주제 역시 기준이 명확하므로 암투 및 내부 조작에 취약하며, 근본적 문제에서 자유롭지 못함

관계와 ‘정치질’의 부작용, 그리고 KPI의 등장

     * Max Planck의 ‘과학은 한 번의 장례식마다 진보한다’는 말처럼, 권력 선발에서 관계성이 능력보다 더 크게 작용하는 경우가 많음
     * 이에 대한 대안으로 대기업 등에서는 KPI(핵심성과지표)와 같은 계량화 지표 도입이 많아졌음
          + KPI는 여전히 조작 가능성이 있지만, ‘정치력’이라는 주관적 요소보단 좀 더 일관된 성과 기반으로 기능함

계량 지표의 한계와 복잡성

     * KPI와 같은 지표는 좁고 단순한 업무에는 적합하지만, 복잡하거나 다차원적인 역량이 필요한 상황에는 한계가 있음
     * 또, 어떤 지표를 쓸지, 어떻게 측정할지, 그리고 의사결정에 어떻게 반영할지 등 사람이 궁극적으로 판단해야 하므로 완전히 기계적으로 대체할 수 없음
     * 최종적으로, 좋은 지표만으로는 한계가 있으며, 의사결정 메커니즘 자체의 변화가 필요함

무작위성의 도입: 이점과 제안

     * 무작위 선택은 전략적 조작에서 발생하는 사소한 이점을 제거하므로, 조직 내 정치적 자본이나 인맥의 영향력을 줄일 수 있음
     * 무작위 선발 시, 어떤 노력도 본인의 당첨 확률을 실질적으로 높일 수 없으므로 조작 행위가 무의미해짐
     * 그 결과, 진정한 역량과 다양성이 자연스럽게 드러날 기회를 얻으며, 기존의 담합 네트워크 파괴로 부패 위험이 감소함
     * 구체적 적용 방안:
          + 중요한 선발/채용은 무작위로 선정된 감시위원회에서 집행하여 편견과 파벌 방지
          + 자격풀에서 직접 무작위로 후보 선정 및, 자격 기준 자체도 무작위 위원회가 관리하여 조작 위험 차단
          + 기업 이사회나 각종 위원회에 임직원 또는 주주 중 무작위 선출을 통해 내부 결탁 약화
          + 다양한 배경 반영을 위해 층화 표집 활용
          + 무작위로 구성된 감사/감시 위원회를 두어 예측 불가능함으로 부패 억제

무작위 제도의 우려와 보완

     * 무작위 제도에 대한 전형적 우려(무능, 책임 회피, 대규모 적용 문제 등)는 실제로는 상당 부분 극복 가능함
          + 연구 결과에 따르면, 집단 의사결정이 올바른 조건에서 개별 전문가보다 성과가 더 높음
          + 교육, 집단 규모, 협조 기술 확보로 무능 리스크 줄일 수 있음
          + 책임성은 의사결정 투명화, 사후 검토, 소환 절차 등으로 담보 가능함
          + 계층화된 다단계 무작위 선발로 대규모 조직에서도 무작위 제도의 효과 유지 가능

역사적 · 현대적 성공 사례

     * 유명한 성공 사례로 배심원단(jury)의 무작위 선발은 공정성 보장 신뢰 획득
     * 베네치아 공화국·고대 아테네 등에서도 고위직·위원회 무작위 선발로 장기적 부패 억제와 역동성 확보
     * 현대에서도 시민의회, 미국 Georgia주의 일부 카운티에서 특화된 선출직에 무작위 방식을 도입하여 민주적 책임성과 높은 전문성 유지

무작위 제도의 근본적 장점

     * 지적 침체 방지 및 다양성 증진: 무작위로 신선한 사고방식과 배경이 조직에 지속적으로 유입됨
     * 부패 엔트로피 도입: 무작위 추첨마다 기존 담합 구조가 해체되어 장기 부패 위험 감소, 조직 회복탄력성 향상
     * 결국, 무작위 제도는 능력주의의 대체제가 아니라 “진정한 능력주의”의 방화벽 역할
          + 기회와 탁월성은 그대로 유지하면서 기존 방식의 왜곡된 인센티브를 혁신함

        Hacker News 의견

     * 배심원 제도가 정의로운 판결을 내릴 것이라는 신뢰가 있지만, 실제로는 다인종 사회에서 편견이 쉽게 개입될 수 있음에 주목함. 싱가포르 초대 총리 리콴유도 이를 직접 경험한 후 배심원 재판을 반대했으며, 이 링크에서 자세히 볼 수 있음. 영국 연구에서도 이런 편견이 일반적이라는 결과가 나옴—흑인 및 소수민족(BME) 배심원은 백인 피고인에게 유죄 판결을 73% 내리지만 BME 피고인에게는 24%만 유죄로 판단함. 백인 배심원도 백인에게 39%, BME에게 32% 유죄 판결을 내려 편향적이긴 하지만 그 정도가 덜함. 이런 제도의 한계에도 불구하고, 프랑스처럼 판사와 배심원이 함께 결정하는 혼합제를 선호함. 모든 시스템이 완벽하지 않지만 12명의 무작위로 뽑힌 사람이 얼마나 공정하고 통찰력이 있을지에 대한 환상은 없어야 한다는 생각임. 연구 원문, 배심원
       제도 소개 링크 참고
          + 이런 통계는 배심원 편향뿐 아니라 흑인/소수민족에 대한 지나친 기소로도 설명 가능함. 그러한 맥락이라면 백인이 백인에게 편향적인 것도 이해될 수 있음
          + BME는 black and minority ethnic, 즉 흑인 및 소수민족을 뜻함
          + 이 통계만으로 배심원 편향을 증명하기는 부족함. 기소 비율과 실제 유죄율을 통제해야 더 정확하게 판단할 수 있음
          + 그렇다면 대안은 무엇인지 궁금함. 판사가 단독으로 결정하는 방법이 남는다는 생각임
          + 백인과 BME가 재판을 받는 상황에서 유죄가 될 확률이 같다는 전제를 기반으로 한 통계임. 실제로는 숨겨진 차이가 있을 수 있어 이런 부분도 고려해야 한다는 생각임
     * 성경에도 무작위 선출에 관한 흥미로운 사례가 있음. 사도행전 1장 21-26절에서 유다를 대신할 사도를 추첨으로 뽑았음. 여러 기준을 만족하는 두 후보를 놓고 기도 후 제비를 뽑아 마티아가 사도로 선발됨. 이 방식이 교황 선출이나 개신교 목사 선정에 적용된다면 상상해볼 만한 일이라고 생각함
          + 이 구절은 베드로가 다소 성급하게 선출을 진행한 예시라는 해석도 있음. 마티아는 이후 성경에 다시 등장하지 않고, 전통적으로는 바울을 진짜 12번째 사도로 보기도 함. 무작위 선출이 꼭 성경에서 권장되는 리더십 선출 방법이 아닐 수도 있다는 관점임
          + 내 파트너가 보수적 메노나이트로 자랐는데, 오늘날도 실제로 목사 선출을 이런 방식으로 한다고 들음. 대략 세 명 정도를 후보로 세우고 제비를 뽑음
          + 기준이 ""예수님과 함께한 경력"" 등으로 꽤 공정함에 신성한 무작위성을 더한 방식처럼 느껴짐
          + 베네치아 도제 선출 과정도 비슷함. 도제 선출 과정 소개에 따르면, 30명 중 무작위로 9명, 다시 40명 뽑아 12명, 또 25명 뽑아 9명을, 반복적으로 추첨해서 최종 41명이 도제를 선출함. 이 복잡함이 특정 가족의 영향력을 최소화하기 위한 장치였음
          + 홉스도 『리바이어던』 36장 등에서 마티아뿐만 아니라 구약에서도 무작위 선출 사례를 언급함
     * 무작위 선출을 뜻하는 기술 용어는 sortition임. 이는 내가 지지하는 비주류 정치적 입장이기도 함. 국회도 추첨으로 뽑힌 시민의회가 대체하면 좋겠다는 생각임
          + 아일랜드에는 sortition으로 구성된 시민의회가 있음. 중요한 사회 현안이 있을 때마다 일반 시민들이 시간을 내어 참여함. 전문가와 정치인 증언을 듣고 토론 후 권고안을 내며, 이는 국민투표로 이어지는 경우가 많음. 가장 큰 장점은 논란이 많은 사회적 이슈를 정치권 밖에서 해소할 수 있다는 점임. 실제로 낙태 관련 시민의회가 건강한 합의를 이끌어내 헌법 개정으로 낙태 합법화가 이루어짐. 또 아일랜드 정치 시스템은 정치자금, 언론 소유, 선거구 제한, 비례대표제 등에서 공정함을 위해 여러 장치가 있음. 80~90년대와 비교하면 부패 지수가 크게 나아졌고, 고등교육율도 매우 높음. 이런 점들이 아일랜드를 긍정적으로 변화시켰음
          + 무작위 시민 집단이 법을 제정한다는 생각이 두렵다는 심리가 있음. 법률은 많은 미묘한 차이와 타협이 필요함을 알기 때문임. 하지만 수천 명을 선출한 후 sortition으로 실제 대표를 뽑는다면 그 정도까진 지지할 수 있을 듯함
          + 누군가 HN에서 대법관도 매번 랜덤으로 연방 판사를 구성해 심리하자고 한 걸 본 적 있음. 이렇게 하면 뇌물이나 정치적 게임 여지가 줄어들 것 같음
          + 비슷한 아이디어를 정치에 관심 있는 친구들과 오랫동안 이야기해 옴. 완전한 무작위 집단은 아니어도 하이브리드 방식이 현실의 심각한 문제를 보완할 수 있을 듯함. 미국 하원의 대표 인구수 비율이 너무 커져 개별 유권자의 영향력이 줄고, 대표들과의 유대도 약해지는 문제 있음. 정당 중심주의로 인해 중도 및 독립적 목소리도 줄어듦. 한 지역구당 의원을 세 배로 늘리고, 그 중 하나를 무작위로 뽑으면 기존 비율에 맞게 중도가 늘어나 극단적 성향을 완화할 수 있다는 생각임
          + sortition이야말로 진정한 민주주의라고 주장함. 즉, sortition에 반대한다면 기술적으로는 민주주의에 반대하는 셈임. 다만, 실제로는 선출 자체만 따질 게 아니라 피드백 루프의 속도가 너무 느려서 제대로 반영되지 않는다는 점이 문제임. 나 자신도 sortition이 대표 선출의 우월한 방법이라 생각하지만, 이 방식이 널리 채택될 가능성은 높지 않음. 그래도 더 빈번하게 표본 조사나 추첨을 활용하는 것부터 시작하면 모두 받아들일 수 있지 않을까 생각함
     * 리더의 카리스마가 실제 실행력 있는 팀원을 모으고 동기를 부여하는 데 큰 역할을 함. 대통령이나 국회의원 보좌진을 경험해보면, 리더 메시지에 진정으로 공감하고 그 목표에 적극적으로 나서는 직원들이 있는 경우에만 진정으로 리더십이 효과적임. 결국 카리스마는 단지 선거를 위한 도구가 아니라, 리더직을 잘 해내기 위한 요건임을 실제로 경험함
          + 실제 적용 방식을 살펴보면, 리더 자체를 무작위로 뽑는 게 아니라 각 역할을 선출하는 사람(=위원회 등)이 무작위로 선정됨. 그리고 카리스마가 꼭 필수적이라고 보는 건 편향임. 리더는 팀의 목적을 관리하고 자원을 조율하는 존재이고, 팀의 목표 자체는 팀이 결정함
          + 특히 대통령이나 총리 같은 역할은 국민 전체의 얼굴임. 단순히 자신의 정당 이해만이 아니라 전체를 대표해야 하고, 때론 대중을 설득할 임무도 중요함. 미국처럼 지역구 기반제도의 단점은 대표 구성이 다양해지기 어렵고, 결국 모두가 선거에서 살아남아야 하니 지나치게 카리스마 중심으로 치우치기 쉬움
          + 이 글도 전형적인 해커뉴스의 지적 편향이 조금 드러남
     * 글에서 sortition 같은 용어가 빠진 것이 흥미롭다고 생각함. 역사적 사례도 종종 잘못되거나 핵심 맥락이 빠져 있는 듯함. 예를 들어 베네치아 도제 선출은 진짜 무작위가 아니라 귀족 집안만 뽑는 방식이고, 왕위 계승 시스템도 일반적 인식과 다름. 중세 유럽에서 계승 경쟁자는 대체로 교회로 보냈기에 살해나 전쟁은 상대적으로 드물었음. 실제로는 모든 아들이 분할 상속받는 gavelkind에서 맏아들 우선의 primogeniture로 바뀌면서 오히려 내부 분쟁이 줄었다는 견해임. 한편, KPI가 조직 내에서 실제로는 거의 무시되고, 결국 인맥이랑 실적(“덱”)이 더 중요시되는 경험이 많음
          + 일반 대중이 전문 용어를 이해하지 못하므로 읽기 쉽게 풀어 쓴 것일 수 있음. sortition, ranked choice voting, LVT 등 전문 용어 대신 구체적인 예시(무작위 선거, 다당제, 공터 투기 등)를 들어 설명하는 것이 더 넓은 독자에게 전달력이 있다는 취지임
          + 혹시 sortition이라는 말이 널리 쓰이지 않는지 묻는 건지 궁금함. Wikipedia에도 sortition 문서가 잘 정리되어 있고, 학술 자료도 많음
     * Campbell's Law는 Goodhart's Law의 한 변형으로, 어떤 지표가 사회적 의사결정에 사용될수록 해당 지표와 그것이 측정하려 했던 사회적 프로세스 모두 왜곡된다는 의미임. 친구가 LeetCode가 모두가 문제만 무작정 푸는 의미 없는 시스템이라고 불평했는데, 나는 그게 곧 시험 공부임을 지적함
          + 맞는 말이지만, 그런 경우 실제로 시험 공부 능력을 평가하려는 것이 맞는지 의문임
     * 뉴욕 공립학교에서 초중고를 다녔고, 일부 고등학교에서는 무작위 선발(로또)이 긍정적으로 작용했음. 실제로 자기 의지로 지원하는 학생과 가족이 주축이 되니 비슷한 가치관을 가진 학생들이 모이게 됨. 그런데 실제로는 무작위도 공정성을 보여주는 연극에 불과할 수 있음. 신청서를 쉽게 주지 않거나 마감 직전에만 특정 확인을 하거나 각종 원본 서류를 요청하는 등 일관성이 없음. 표면적으로만 무작위성일 뿐 실제로는 공정하지 않음을 내 경험과 아이들 경험에서 느낌
     * 운동, 정치, 스타트업 등 어느 분야든 성공에는 행운 요소가 큼. 나와 비슷한 실력의 N명이 있을 때, 내가 뽑힌 건 운이 따라준 것임. 예를 들어, Bay Area에서 성장하며 스타트업 인맥을 만든 행운, 사고 없이 커리어를 이어간 운, 지역 정당 모임에서 우연히 만남, 식당을 달리 가서 감기 걸리지 않고 중요한 시합에서 최상 컨디션을 발휘한 것까지 모두 변수임. 결국 자격 있는 집단에서 약간의 무작위성이 좋은 결과를 낳는 것은 이상하지 않은 현상임
     * Jim Collins의 『Good To Great』를 추천함. 이 책에서 가장 존경받는 리더들은 외적으로 자신을 돋보이게 만들지만, 실제 성과는 낮음. 오히려 겸손하고 자기 PR을 피하는 인물이 더 효과적인 리더였다는 통계가 나옴. 내 결론은, 경험 없는 사람이 말하는 기준을 믿지 말라는 것임. 시간과 노력이 대외 이미지에만 집중되면 정작 조직 운영의 본질이 흐려짐. 소프트웨어에서도 경험 없는 사람이 성공 기준을 인위적으로 정하는 문제가 자주 발생함
"
"https://news.hada.io/topic?id=21999","애플 MLX에 CUDA 지원 추가중","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          애플 MLX에 CUDA 지원 추가중

     * MLX는 Apple Silicon 기반 기계학습 연구자를 위한 어레이 프레임워크로 NumPy 및 PyTorch와 유사한 사용성을 제공
     * MLX에 CUDA 백엔드를 추가하는 작업이 진행되고 있음
     * 주요 목적은 통합 메모리(unified memory) 지원과 NVIDIA 하드웨어의 폭넓은 활용
     * 현재 튜토리얼 예제만 동작 가능하며, 빌드와 테스트는 Ubuntu 22.04 + CUDA 11.6 환경에서 확인됨
     * 초기 성능 문제와 병목 개선 과정을 거치면서 최적화 및 리팩토링이 반복적으로 진행 중
     * MLX의 CUDA 백엔드는 Apple의 후원을 받아 진행되고 있으며, 통합 메모리와 NVIDIA 하드웨어 지원을 통해 Mac에서 개발하고 대형 컴퓨팅 환경에 배포할 때 일관된 경험을 제공하는 것이 목표
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

MLX 프레임워크 소개

     * MLX는 Apple의 머신러닝 연구팀이 개발한 기계학습용 배열 프레임워크임
     * 엔지니어와 연구자가 효과적으로 실험하고 빠르게 아이디어를 검증할 수 있도록 개발됨
     * Apple Silicon을 지원하는 것이 특징이지만, 이제 CUDA 백엔드 추가로 NVIDIA GPU 환경에서도 동작 가능성을 확장하려고 하고 있음

주요 특징

     * 친숙한 API: Python API는 NumPy를, 상위 패키지인 mlx.nn, mlx.optimizers는 PyTorch와 유사한 인터페이스 제공
          + C++ , C, Swift API도 함께 제공되며, Python API와 일관된 사용성 보장
     * 합성 가능한 함수 변환: 자동 미분, 자동 벡터화, 컴퓨팅 그래프 최적화 등의 기능이 포함됨
     * 지연 계산: 여러 연산을 묶어 실제로 필요할 때만 결과를 계산하는 구조로 효율적인 리소스 활용 가능
     * 동적 그래프 생성: 입력 데이터의 shape이 바뀌어도 느린 컴파일 없이 즉각적인 실행, 개발 및 디버깅이 용이함
     * 멀티 디바이스 및 통합 메모리 모델:
          + 기존에는 CPU와 Apple GPU만 지원했으나, CUDA 백엔드로 NVIDIA GPU 환경 지원 추진 중
          + 통합 메모리 구조 채택으로, 메모리 복사 없이 모든 지원 장치에서 동일 객체 연산 가능
     * 간결하고 확장 가능한 설계: 연구자들이 프레임워크를 손쉽게 확장 및 개선 가능

타 프레임워크와의 차이점 및 영감

     * NumPy, PyTorch, Jax, ArrayFire 등에서 설계 영감 받음
     * 특징적으로 통합 메모리 모델 및 간결한 인터페이스, 즉시 실행/디버깅 환경을 강조함

대표적 예시 및 활용 분야

     * MLX Examples 레포에는 다양한 실습 코드가 존재함
          + Transformer 언어 모델 학습
          + LLaMA 기반 대규모 텍스트 생성 및 LoRA 활용한 파인튠
          + Stable Diffusion으로 이미지 생성
          + OpenAI Whisper 기반 음성 인식 등 최신 모델 구현 사례 포함

MLX의 의미와 CUDA 지원

     * Apple Silicon 중심의 프레임워크였던 MLX가 CUDA 백엔드 도입으로 NVIDIA GPU 환경에서도 범용적으로 활용될 가능성 확장 중
     * 최신 CUDA 지원을 통해, Apple과 NVIDIA 양쪽 하드웨어에서 최신 연구 및 산업 활용 모두에 적합한 프레임워크로 자리매김할 전망

   macos에서 nvidia gpu만 쓸 수 있게 해주면 되는 것을... 헐헐.

   좋은 이야기네요 ㅎㅎ 얼른 cuda 지원해서 맥으로도 고속학습 가능했으면 합니다~!

        Hacker News 의견

     * 내 이해가 맞는지 확인하고 싶음: 1번, MLX로 빌드된 프로그램은 CUDA가 지원되는 칩에서 작동이 가능함. 하지만 2번, CUDA 프로그램이 Apple Silicon에서 작동하는 건 불가능함. 이유는 2번이 저작권(특히 NVidia의 유명한 진입장벽) 위반이 되기 때문임이 맞는지 궁금함
          + 1번이 맞음. 개발자가 상대적으로 저사양인 Apple 기기(UMA 포함)로 개발하고, 완성된 코드를 Nvidia의 상대적으로 고사양 시스템에 배포할 수 있게 해줌. 여러 이유로 유용함
          + 2번은 저작권 위반이 아님. API 재구현은 가능함
          + 2번에 대해서는, 정말로 맞는 말이 아닐 듯함. AMD의 HIP도 17~18년쯤 OpenCL을 포기하고 동일한 작업을 하고 있음
          + 나는 3번을 원함. NVIDIA GPU를 Apple Silicon에 연결해서 CUDA를 사용할 수 있으면 좋겠음. Apple Silicon과 통합 메모리, GPU, CUDA를 PyTorch, JAX, 또는 TensorFlow로 같이 쓰는 상상이지만, 아직 MLX를 제대로 경험해보지는 못했음
          + 2번이 안 되는 건 크게 더 어렵기 때문임
     * ""아니, Apple 플랫폼은 공식 CUDA 지원이 없는데?""라고 생각했다면, 이 패치 세트가 ""CUDA 12 및 SM 7.0(Volta) 이상의 Linux 플랫폼""까지도 지원한다는 소식도 함께 참고할 필요 있음
       설치 안내
     * 이 프로젝트는 Electron 등의 제작자로 유명한 zcbenz가 주도함
       zcbenz 소개
     * MLX의 주요 특징 중 하나가 통합 메모리 아키텍처(UMA) 사용인데, 이게 어떻게 작동하는지 궁금증이 있음
       리포지토리 readme의 bullet 참고: MLX 깃허브
       모든 UMA APU(제조사 관계없이)로 확장하면 재밌을 것으로 생각하지만, 디스크리트 GPU에서는 방식이 달라야 한다는 의문이 있음
       PR 코멘트 읽어보니, CUDA 역시 UMA API를 직접 지원하고 필요한 경우 투명하게 복사를 처리하는 것 같음
          + 내 경험상 프리페치 기능 미비로 복사 대기 때문에 메모리 병목이 심하게 발생하는 문제 있음. 전체 데이터셋이 VRAM에 들어갈 때 수동 프리페치를 하면 괜찮지만, 내 애플리케이션(ML 학습)에서는 성능 하락이 너무 커서 스트리밍 로드로 직접 전환했었음
     * 약간 다른 이야기지만, MLX 작업자들 상당수가 Apple에 공식 소속이 아닌 듯 보임. 예시로 prince_canuma의 트위터 참고
       Apple이 뒤에서 지원하고 공식적으로 드러내지 않는 걸까 궁금증이 생김. 팀이 이적한다는 루머도 있었음
     * PR 설명에서 ""CUDA 백엔드를 추가하는 ongoing effort""라고 되어 있음
       즉, MLX 코드가 x86 + GeForce 하드웨어에서 빌드 및 실행 가능하다는 뜻이고, 그 반대는 아님
     * ""맥에서 코드 작성/테스트하고 슈퍼컴에 배포할 수 있다면 좋은 개발 경험""이라는 말처럼, 이제 MLX를 리눅스에서 쓸 수 있느냐는 의문이 있음
       직접 테스트해 봤는데, 파이썬 3.12 버전만 pypi에 올라와 있음
       MLX-CUDA PYPI 페이지
     * 이게 Strix Halo의 영향일지 궁금함. 업무 컴퓨터 스펙이 남아서 EVO-X2를 샀는데, 이게 미드레인지 znver5 EPYC 머신들과 비슷하거나 더 좋음. 대부분의 EC2나 GCE 인스턴스(심지어 NVMe까지)를 훨씬 앞섬. 이 하드웨어 가격은 1800임
       최근 DGX Spark가 Strix Halo에 비해 가격 대비 효율이 떨어진다는 유튜버 리뷰도 나오는 중임. 단점은 ROCm이 아직 부실함(곧 좋아질 것으로 보임). 만약 Apple 기기에서 CUDA가 완벽하게 돌아간다면, Strix가 아무리 싸고 좋아도 진지하게 고민할 만한 선택지임
          + Strix Halo란 바로 AMD Ryzen AI Max+ 395랑 같음. Framework Desktop이나 여러 미니 PC에 들어가고 있음. 이 칩의 메모리 대역폭은 200GB/s으로 일반 x86 플랫폼에서는 매우 좋지만 Nvidia GPU(예: 5090은 1792GB/s)나 Apple M3 Ultra(800GB/s)에 비하면 한참 부족함. 가격대비 성능은 훌륭하지만, LLM과 같이 메모리 집약적인 작업에서는 이제 막 최소 성능에 진입했다는 생각임
          + PR 설명을 보면 클라우드 클러스터 트레이닝이 타깃임을 명확히 알 수 있음
          + AMD GPU의 신경망 연산이 곧 Nvidia 경쟁 상대가 된다는 얘기가 나온 지 10년째인데, 아직도 실현되지 않은 상태임
          + m4 맥 미니와 비교해서 어떠냐는 궁금증이 있음
     * MLX 모델에 매우 깊은 인상을 받음. 집안 모두에게 로컬 모델을 열어놓아도 Nvidia 컴퓨터처럼 화재 위험을 걱정할 필요가 없음. Apple Silicon이 Nvidia 칩 진영에 진지한 경쟁자가 되길 바람. CUDA 지원이 만약 embrace, extend, extinguish(EEE)에 해당하는 전략일지도 궁금함
     * Apple이 향후 m시리즈 칩 기반 데이터센터를 구축해서 앱 개발, 테스트 및 외부 서비스 호스팅 등에 활용할 계획임
"
"https://news.hada.io/topic?id=22025","데이터 사이언스는 유사 과학이 되었는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         데이터 사이언스는 유사 과학이 되었는가?

     * /r/DataScience 서브레딧에 올라온 토론과 답변들 정리
     * 글쓴이는 데이터 사이언스가 검증과 평가 없이 “제너레이티브 AI”라는 이름만으로 실행되는 현실에 회의감을 느낌
     * 실상은 ChatGPT가 생성한 코드로 단순한 z-score 계산만 수행했으며, 모델 성능 평가도 없이 배포 직전까지 진행됨
     * 커뮤니티에서는 ""작동만 하면 배포하자""는 기업 문화, 검증 부족, 책임 회피, 과학적 윤리 희생을 공통적으로 지적함
     * 다양한 실무자들이 비슷한 문제를 겪고 있으며, ""유사 과학""으로 전락하는 흐름에 대해 강한 우려를 표현함
     * 하지만 일부는 빠른 실험과 단순한 해법의 실용성도 이해해야 한다는 의견을 제시하며, 균형 있는 관점을 강조함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Data Science Has Become a Pseudo-Science

     * 유럽에서 석·박사를 마치고 10년 간 산업과 학계를 오가며 데이터 사이언스를 수행함
     * 최근 2년 간 ""제너레이티브 AI""라는 이름으로 아무 검증 없이 결과를 내세우는 현상이 늘어나고 있음
     * 예시로, 시계열 이상탐지를 목표로 한 프로젝트에서, ChatGPT가 생성한 코드로 평균 차이의 z-score만 계산하고, 어떤 성능 지표도 없이 배포를 논의함
     * 이런 방식은 과학적 사고 없이 블랙박스에 질문하고 그대로 따르는 유사 과학의 모습이며, 질문조차 금기시됨
     * 이에 따라 학계로 돌아가려는 고민도 있으며, 이런 현상이 동료들 사이에서도 공유되는 경험인지 묻고자 게시글을 작성함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

댓글 요약

  주요 공감 의견

     * “그냥 작동만 하면 배포” 라는 철학이 만연해 있음 (u/Illustrious-Pound266)
     * 검증·로드맵 없이 AI만 강조하다 망한 스타트업 사례도 있음 (u/gothicserp3nt)
     * 고의적이지 않은 편향이나 차별이 제대로 검토되지 않음 (u/tehMarzipanEmperor)
     * 대부분 기업에서 RAG나 AI를 과장 포장하여 정확성보다 쇼잉(보여주기) 중심으로 운용됨 (u/castleking, u/flowanvindir)
     * 현업의 분위기는 ""퍼포먼스 극대화 연극(performance theater)"" (u/Ty4Readin, u/faulerauslaender)
     * 성과를 내기 위해 성급한 배포, 외형만 화려한 보고서, 측정 없는 AI 도입이 일반화됨 (u/glittering_tiger8996, u/Emergency-Job4136)
     * 이런 상황은 예전부터 존재했고, GenAI는 그것을 더 노골적으로 만든 도구일 뿐이라는 시각도 다수 (u/RoomyRoots, u/303uru, u/TARehman)
     * 설명 가능성은 낮고, 신뢰도도 떨어지지만 빠르기 때문에 채택됨
     * 기업 의사결정에 대한 설명책임이 사라지고 있음 (u/empathic_psychopath8, u/Jollyhrothgar)

  다른 시각

     * 단순한 접근법도 문제를 해결할 수 있다면, 실용적으로 인정할 필요 있음 (u/AnarkittenSurprise)
     * 많은 댓글에서 “DS는 본래부터 비과학적 요소도 섞여 있었으며”, 또는 “이름만 과학” 이라는 의견도 있음 (u/TaiChuanDoAddct, u/Time-Combination4710, u/LighterningZ)
     * AI 도구 사용 그 자체보다 그것을 책임 있게 활용하는 역량이 중요 (u/Dror_sim, u/ResearchMindless6419)
     * “데이터는 있지만 논리가 없음”, “통계 지식 없이 패키지만 돌리는 수준” 에 대한 비판 (u/gyp_casino, u/tmotytmoty)
     * 실제로 중요한 건 도메인 지식과 수학적 사고이며, AI/코딩은 도구에 불과하다는 의견 다수 (u/MightBeRong, u/Dror_sim)

  제도 및 교육 문제

     * MSDS 과정이 학문적으로는 유용하지만, 취업과는 무관한 경우가 많음 (u/throwaway_ghost_122)
     * 교육 수준이 낮아지고, 학위만 얻으려는 수요가 늘면서 현업 전반의 품질이 하락 중 (u/Yam_Cheap)
     * 학계 또한 검증되지 않은 논문과 얕은 분석이 많아지는 경향이 있으며, 학계라고 해서 예외는 아님 (u/joule_3am, u/Mishtle)

  산업별 경험 공유

     * 보험·헬스케어 분야는 엄격한 규제로 인해 여전히 타당성 검토와 법률 심사를 요구 (u/Mishtle, u/mikka1)
     * 반대로 스타트업, 세일즈, 게임, 일부 제조 분야는 빠른 속도와 쇼잉 위주 (u/Vercingetorex89, u/Brackens_World)
     * 공공 분야에서도 ChatGPT 도입으로 과거의 검증 체계가 무너지고 있음 (u/TheFluffyEngineer, u/joule_3am)

  회의와 탈출 고민

     * 현업을 떠나거나 학계로의 전환을 고민 중이라는 실무자가 많음 (u/thro0away12, u/Emotional_Plane_3500, u/candidFIRE)
     * 진짜 실력 있는 사람은 오히려 돋보일 수 있는 기회라는 긍정적 시각도 일부 존재 (u/OddEditor2467, u/sideshowbob01)

  풍자와 체념

     * “요즘은 pandas import만 해도 데이터 사이언티스트가 되는 시대” (u/vesnikos)
     * 확률적 사고와 과학적 검증보다는 상사의 기분 맞추기가 중심이 된 현실 (u/tmotytmoty, u/WignerVille)
     * “과거에도 그랬고 지금도 그렇고, DS는 기업에서 과학이라 부르기엔 무리가 있었다”는 현실론 다수 존재 (u/TaiChuanDoAddct, u/LighterningZ)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

결론

     * 이 글과 댓글들은 최근 데이터 사이언스 실무가 과학적 정합성과 검증보다는, 빠른 납품과 AI 마케팅에 휘둘리는 현실을 잘 보여줌
     * “제너레이티브 AI”라는 라벨이 합리적 비판을 봉쇄하고 있다는 점, 그리고 검증 없는 코드가 곧바로 배포로 이어지는 구조에 대한 우려가 깊음
     * 학계와 산업 모두 완벽하지 않지만, 데이터 사이언스가 진정한 의미의 ‘과학’이 되기 위해선 커뮤니티 내부의 비판적 사고와 교육, 실무 문화의 성찰이 필요하다는 점에서 논의는 계속될 것으로 보임

   링크가 동작하지 않네요
   https://reddit.com/r/datascience/…

   고맙습니다. 수정해두었습니다.
"
"https://news.hada.io/topic?id=21986","임베디드 개발자에게 Lua가 MicroPython보다 좋은 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  임베디드 개발자에게 Lua가 MicroPython보다 좋은 이유

     * 임베디드 시스템 개발에서 Lua는 MicroPython보다 높은 안정성과 확장성을 제공함
     * Lua는 C와의 통합이 쉽고, 가볍고, 결정론적인 구조를 가지며, 코드 유지보수성과 장기적 비용 절감에서 더 큰 이점을 보임
     * MicroPython은 빠른 프로토타이핑에는 적합하지만, 대규모 프로젝트나 프로덕션 환경에서는 한계에 부딪힘
     * Lua는 작고 효율적인 실행 파일로 빌드되어 불필요한 기능 없이 운영 가능하며, 프로토타이핑에서 제품화까지 확장성과 유지관리성이 높음
     * Xedge 프레임워크는 Lua를 임베디드 시스템에 최적화하여 강력한 IoT, 웹 기능을 제공함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

임베디드 프로페셔널 환경에서 Lua vs. MicroPython

     * 산업 자동화, 의료기기, 상업용 IoT 등 전문 임베디드 프로젝트에서 고수준이면서도 경량적인 환경이 점점 더 선호되는 추세임
     * MicroPython은 빠른 프로토타이핑과 현장 배포에 강점이 있으나, 주로 취미/교육용 보드 중심의 생태계임
     * Python의 강점인 NumPy, pandas 같은 대형 라이브러리가 MicroPython에는 없으며, 표준 라이브러리도 대폭 축소되어 있음
     * C 확장과의 통합도 MicroPython은 상대적으로 복잡
     * Lua는 C 애플리케이션과의 통합을 근본 철학으로 채택
          + 안정적이며 미니멀한 C API와 바이트코드 가상머신 제공
          + C/C++ 함수와 자료구조를 Lua에 쉽게 노출 가능
          + Lua ANSI C 라이브러리는 임베디드를 위해 설계되어 구조가 간결, 경량, 결정론적임
     * MicroPython은 Python 3의 재구현으로, 데스크톱 지향 언어의 가정이 남아 있어 리소스가 제한된 환경에서 한계가 빨리 드러남

심리스한 C 연동은 Lua의 핵심

     * Lua의 가장 큰 장점은 C와의 매끄러운 통합
     * API가 안정적이고 최소화되어 있으며, 자체 C/C++ 함수와 데이터 구조를 Lua에 쉽게 노출할 수 있음
     * MicroPython도 C 확장을 지원하지만, 커스텀 펌웨어 빌드가 필요하고 워크플로가 더 복잡함
     * Lua는 C 연동 자체가 설계 철학임
          + Lua C API를 수동으로 사용하거나, SWIG 등 자동 바인딩 툴을 활용해 C 함수/구조체를 Lua에서 호출 가능
          + C의 성능이 중요한 로직은 C로, 고수준 비즈니스 로직은 Lua로 분리해 유지보수성과 확장성을 높일 수 있음

미니멀한 풋프린트와 확장성

     * Lua는 코어 인터프리터가 매우 작고, 필요 없는 기능은 쉽게 제거 가능
     * MicroPython도 임베디드용으로 최적화됐으나, 기본 이미지가 크고 필수 모듈 활성화 시 용량 증가 현상
     * 둘 다 빠른 프로토타이핑에 적합하지만, Lua는 프로덕션까지 확장성이 뛰어남
     * Lua는 고수준-저수준 분리 구조 설계가 쉬워, 빠르게 프로토타입 개발 후 유지 보수 가능한 하이브리드 구조로 발전 가능
     * Lua는 처음부터 C 코드와 결합하여 스케일업이 자연스럽고 개발 플로우가 단절되지 않음
     * MicroPython은 프로토타입 이후 코어 로직을 다시 작성해야 하거나 한계에 부딪힐 수 있음

생태계와 라이브러리 접근성 - Quality Over Quantity

     * MicroPython은 Wi-Fi 보드 등 취미/교육용 하드웨어 중심의 생태계가 활발하지만, 대형 Python 생태계의 주요 라이브러리 부재
     * Lua는 라이브러리 수는 적지만, C로 쉽게 확장 가능하며, 생태계의 한계에 덜 부딪힘

유지관리 및 비용 측면의 장점

     * Lua는 코드베이스가 작고, 테스트·디버깅·인수인계가 쉬움
     * 프로젝트 확장시 Lua 스크립트와 C 코어 간 레이어 분리 유지, 인계 및 협업에 유리함
     * MicroPython도 가독성은 높으나, 프로젝트가 커질수록 시스템 코드와 스크립팅 레이어가 혼재되는 경향이 있어 유지보수 비용이 상승할 수 있음

결론: Choose What Scales

     * 교육 및 프로토타이핑 목적이라면 MicroPython도 훌륭한 선택
     * 보안, 신뢰성, 유지보수가 중요한 임베디드 제품에는 Lua가 더 실용적이며, 확장성·유연성·안정성을 동시에 제공
     * Lua는 단순한 스크립트 언어가 아니라, 임베디드 개발 전략임

임베디드 기기에서 Lua C 라이브러리 통합 방법

     * Lua C 라이브러리는 경량·ANSI C 호환·표준 C 라이브러리 외에는 의존성이 거의 없음
     * 베어메탈, RTOS 기반 시스템에 적합하며, 표준 C의 stdin/stdout 등 일부 요소는 포팅시 주의 필요
     * Real Time Logic의 Xedge Framework
          + Xedge Framework는 임베디드 환경에 최적화된 Lua 런타임 및 API 세트를 제공
          + TLS, MQTT5, WebSocket 등 보안 통신, RESTful 웹서비스, 실시간 데이터 처리, Modbus/OPC UA 프로토콜 등 IoT/웹 기능 내장
          + Lua의 유연성과 경량성을 유지하면서도, 실전 투입이 가능한 완성형 임베디드 프레임워크를 제공
     * Lua를 임베디드 제품에 내장하려면 Xedge가 가장 실용적인 선택지로, 통합을 단순화하고 개발 속도를 높이며, 차별화된 로직에 집중할 수 있게 함

   애초에 장비 생산하는 컴포넌트 생산 업체에서 lua든 python 지원을 잘 안함. c 정도?

        Hacker News 의견

     * 게임 엔진을 만들어서 전체 게임을 스크립팅 언어로 작성하는 걸 결심했을 때, JavaScript(QuickJS), Python(Boost.Python), Lua(Sol2) 세 가지 중에서 고민했음
       Lua를 임베딩하는 게 정말 쉽고, C++ 래퍼와 함께 써도 매우 간편함
       얼마 안 걸려서 '이 정도면 바로 쓸 수 있겠다' 싶은 엔진을 갖추게 됨
       게다가 Lua VM은 아주 가벼움
       자세한 내용은 carimbo 프로젝트에서 볼 수 있음
          + 엔진이나 애플리케이션에서 Lua를 스크립트로 지원한다는 걸 보면 Fennel을 사용할 수 있다는 점이 마음에 듦
            Fennel은 Lua로 트랜스파일되는 언어임
            Fennel 공식 링크
          + 개인적으로 Boost.Python은 스크립팅 도구로 별로라고 생각함
            이 점이 판단에 영향을 줄 수도 있다고 봄
          + Sol2가 Lua VM인지, 아니면 표준 Lua VM의 래퍼일 뿐인지 궁금함
     * 'Lua는 단순한 고수준 언어가 아니라 임베디드 개발 전략'이라는 문구는 납득하기 어렵다고 느낌
       이런 식의 표현이 들어간 글은 진지하게 받아들이기 힘듦
          + 전체적으로 '나는 Lua를 오래 써봤으니 이제 결론 내릴 수 있다'는 느낌의 글임
            MicroPython에 대해서는 실제로 경험이 별로 없어 보이는데 몇 가지 과장된 비판이 있음
            예를 들어 ""MicroPython 프로젝트는 시스템 코드와 스크립트 레이어가 뒤섞여 유지보수가 어렵다""는 얘기엔 근거가 약함
            언어나 프로젝트 관리/구조 설계의 영향일 수 있는데, 원인은 엄밀하게 평가돼야 한다고 생각함
          + 이 글은 실제 기사보다 Xedge Lua 프레임워크 광고처럼 느껴짐
            그냥 광고임
          + 전반적으로 글이 chatgpt 스타일 같음
            광고 글의 경우, 사람이 썼든 LLM이 썼든 상관없어 보임
          + 댓글들도 언급했듯이 chatgpt 느낌이라 글을 읽으며 별로 즐겁지 않았음
     * PLDB Top 1000 리스트에서 Lua가 MicroPython보다 더 높은 순위에 있음
       이 비교 글에서 Github 유저 SkipKaczinksi는 Lua가 대체로 더 빠르다고 봄
       Hackaday 글의 Michael Polia도 Lua가 작고 빠르다고 언급함
       Toit 언어는 MicroPython보다 30배 빠르다고 주장함
       Toit의 창립자는 V8의 초기 개발 책임자였음
     * ""임베디드(embedded)""와 ""임베더블(embeddable)"" 개념은 구분이 필요함
       MicroPython은 임베디드 플랫폼에서 사용되지만, Lua처럼 기존 애플리케이션에 통합하는 '임베더블 런타임'은 아님
       MicroPython의 목적은 전통적인 C 런타임을 대체해 최소한의 C 래퍼로 초기화만 하고 나머지 비즈니스 로직을 MicroPython 스크립트로 작성하게 하는 것임
       Lua의 lua_State처럼 여러 인터프리터를 동시에 쓸 수 있는 구조나 샌드박싱이 없음
       즉, MicroPython은 '게임 엔진 내에서 스크립트로 빠른 반복 개발'보다는 'IoT 보드에서 파이썬으로 센서 데이터 읽기'에 최적화돼 있음
          + MicroPython이 Lua처럼 임베더블은 아니지만, 아예 불가능하지는 않음
            완전히 박스 밖에서 쓸 수는 없고, 글루 코드 정도는 필요함
            참고 자료로 embed 포트 예시를 볼 수 있음
     * Lua는 임베디드 용도로도 훌륭한 언어라고 생각함
       Lua 기반 제품도 좋다고 생각하지만, 이 글에서 '왜 Lua가 MicroPython을 이기는지' 설득력이 떨어짐
       MicroPython을 C로 확장하는 건 생각보다 쉽고, 공식 모듈 개발 방식과 똑같이 외부 모듈도 개발하면 됨
       그래서 펌웨어 커스텀 빌드시 어렵지 않게 추가할 수 있음
       그리고 Python 계열 라이브러리(numpy 등)가 안 된다는 지적이 있었지만, 실제로는 numpy와 scipy 일부를 리이벤트한 ulab 라이브러리도 존재함
     * 개인적으로 들은 이야기는 마케팅 문구로 느껴짐
       마이크로컨트롤러에서 충분한 리소스가 있으면 micropython을 씀
       전력, 메모리, CPU 제어가 정말 중요하면 결국은 C/C++을 씀
       네트워크 관련 개발은 C/C++에서 어렵지만 빠르고 안전하게 할 수 있는 옵션이 부족했는데(최근엔 내장 TLS 지원도 더 좋아졌을 것 같음)
       Lua는 C를 부드럽게 포장한 느낌임
       라이브러리가 풍부하면 좋지만, 직접 Lua 툴체인, 마이크로컨트롤러 툴체인, 필요한 라이브러리를 모두 포팅해야 하는 부담이 큼
       그래서 이 부분이 마케팅 글이면 Xedge 제품을 써서 외주 주라는 메시지라고 생각함
          + 'micropython을 쓰려면 리소스가 넉넉해야 한다'는 이야기에
            그냥 2350에서도 잘 돌아간다고 짧게 언급함
     * 정말 '진지하게' 임베디드 개발에 micropython이나 lua를 쓰는 사람이 있는지 궁금함
          + 20년 가까이 프리랜서로 Lua 중심의 임베디드 제품을 만들었음
            VOIP 기기, 홈 오토메이션, 산업용 라우터, 디지털 비디오 레코더 등 다양한 분야에서 Lua를 활용함
            보통 시스템은 Linux 커널, libc, Lua 인터프리터, 그리고 몇몇 외부 라이브러리로 구성됨
            Lua 애플리케이션 소스는 3만~10만 줄 정도로, 요즘 기준으론 '작은' 제품도 있음(플래시 8MB, 램 64MB 등)
            Lua는 이런 환경에서 잘 동작함
            모두 현역 제품이고, 고객에게도 돈이 되고 있음
            Lua와 C의 연동은 매우 쉽고, 비동기 적으로 작업하는 데도 현대 언어들이 아직도 고민할 수준을 Lua는 예전부터 해결해옴
            언어는 단순하면서도 강력하고, 코루틴, 클로저, 메타테이블 등으로 다양한 패러다임을 쓸 수 있음
            이런 스케일의 프로젝트라면 여전히 Lua + C/C++ 조합을 선택할 것임
            다른 생태계(Elixir, Rust, Nim)도 써봤지만, Lua만큼 강력하고 유연한 언어는 못 찾음
          + 우리는 MicroPython으로 class B 의료기기까지 개발하고 있음
          + 임베디드 세계는 범위가 매우 넓음
            안전에 민감하면 규정상 못 쓰지만, 테스트 장비 등은 규제에 상관없으니 편한 걸 씀
            IoT 영역에서도 모두 각자 편리한 걸 쓴다고 보면 됨
          + MicroPython이 큐브샛(소형 위성) 등 위성 임무에도 실제로 쓰임
            관련 컨퍼런스와 팟캐스트 사례도 존재함
          + 수천 개 제품이 Lua를 내부적으로 사용하거나 일부에라도 쓰고 있음
            최근 LuaJIT도 조사해봤는데 저평가됐다고 생각함
     * '진지한 임베디드 개발자'는 대부분 컴파일 언어를 쓴다고 생각함
          + 결국은 바이트코드로 컴파일되니 일종의 컴파일된 언어라고 볼 수도 있음
     * 취미로는 그냥 Arduino(Platformio)를 이용함
       마이크로컨트롤러에서는 컴파일하고 플래시 올리는 게 금방이라 굳이 인터프리터가 필요하진 않음
       언젠가 C++을 대체할만한 다른 컴파일 언어도 써보고 싶음
       Raspberry Pi Pico에서 잘 동작하는 추천 언어 있을지 궁금함
          + 전문가까지는 아니지만, Rust가 가장 인기 있는 대안 중 하나라고 느낌
            트렌디하고 C++의 문제도 많이 고쳤으며 툴링도 괜찮은 편임
            Zig도 재밌어 보여서 시도해보고 싶음
            Raspberry Pi는 사양이 좋아서 시스템 언어가 아니어도 돌아갈 수 있음
            Kotlin도 좋아하고, 기본적으로는 JVM이 필요하지만 native 빌드 가능함
            다만 Pico에서 GPIO 제어는 파일시스템을 직접 건드려 써야 할 수도 있음(덧붙여 Pico에서 Kotlin 지원은 확실하지 않음)
          + Nim이 꽤 괜찮은 옵션으로 보임
            관련 자료는 picostdlib의 Nim 지원 참고
     * Lua는 본질적으로 훨씬 더 단순한 언어임
       Python은 '한 가지 방법만 있다'는 신념을 갖고 있지만 실제로는 이것저것 다 넣어둔 '만능툴' 같은 느낌임
       이게 오히려 시작하기 쉽고, 라이브러리도 많아서 쉽게 개발할 수 있지만
       리소스가 부족한(작은) 환경에는 잘 어울리지 않음
       화려한 의자를 꼭 판자로 에임스 체어로 변형시키는 데에는 한계가 있음
          + Python은 쉽고 Lua는 단순함
            '쉬움'의 문제는 내부 복잡성이 숨어 있다는 것이고, '단순함'은 사용자에게 더 많은 노력이 필요함을 의미함
          + Python은 버전 호환성이 취약해서 3.x에서 3.x+1로 넘어갈 때 문제가 자주 발생함
            Lua도 완벽하진 않지만, 그래도 여러 Lua 버전을 지원하는 사례가 많아서 급격한 버전 업그레이드를 강제받지 않는 장점이 있음
"
"https://news.hada.io/topic?id=21972","Show GN: css if() 기능을 이용해서 간단한 논리회로를 만들어봤습니다.","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Show GN: css if() 기능을 이용해서 간단한 논리회로를 만들어봤습니다.

   새로 등장한 css if 기능을 이용해서 간단한 논리회로를 만들어보았습니다. DOM 구조 한계상 루프 같은건 만들수 없어 선형적인 플로우정도만 만들어볼 수 있었어요.

   오 그렇군요.
   그러면 버튼이나 text input 같은것을 클럭 제네레이터로 쓰면 동작할까요?

   자문 자답인데, 결국엔 저장한 걸 다시 꺼내쓰는게 안되는군요.
"
"https://news.hada.io/topic?id=22083","나만의 백업 시스템 만들기 – 1부: 스크립트 이전의 전략","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    나만의 백업 시스템 만들기 – 1부: 스크립트 이전의 전략

     * 백업의 중요성이 종종 과소평가되는 현상이 많음
     * 많은 이들이 클라우드 의존에 안주하며 데이터 보호의 책임을 인식하지 못함
     * 백업 계획을 세우지 않고 단순 복사에 의존하면 높은 위험을 초래함
     * 디스크 전체 또는 개별 파일 백업 방식에는 각각 장단점이 존재함
     * 신뢰할 수 있는 백업은 스냅샷 활용과 외부 저장소 확보가 핵심 요소임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

백업의 중요성과 현실

     * 백업은 많은 사람들이 심각하게 생각하지 않는 영역임
     * 잘못된 방법이나 개념적 오류(예: RAID는 백업이 아님)로 인해 수많은 데이터가 손실됨
     * 데이터는 반드시 다양한 방식으로 보존되어야 하는 중요한 자산임

클라우드와 백업에 대한 오해

     * 클라우드에 모든 데이터를 맡기면서 실제로 데이터가 어떻게 보호되는지 묻지 않는 경우가 많음
     * 주요 클라우드 제공업체도 공유 책임 모델을 적용함
          + 인프라의 보안은 제공하지만 데이터 보호 책임은 사용자가 짐
     * 클라우드, 사설 서버, Kubernetes 등의 환경에서도 백업 부재 위험이 빈번함

필자의 데이터 복구 경험

     * 데이터센터 화재, 서버실 침수, 지진, 랜섬웨어, 악의적 공격, 관리자 실수 등 여러 데이터 손실 사례를 겪음
     * 인터넷에 연결된 서버(이커머스, 메일 등)는 데이터 무결성과 서비스 연속성 모두가 중요함
     * 백업은 단순 복사가 아님. 특히 동작 중인 데이터베이스 파일을 복사하는 것은 복구 불가 가능성을 높임

백업 전략 수립의 필수 질문

     * ""얼마나 많은 위험을 감수할 것인가?"", ""어떤 데이터를 보호해야 하는가?"", ""데이터 손실 시 허용할 다운타임은?"", ""사용 가능한 저장 공간은?""
     * 백업을 동일한 머신에 두는 방식은 머신 장애 시 쓸모없음. 외부 저장소로의 백업이 안전함
     * 네트워크 대역폭, 복구 속도, 저장 공간 등 현실적 요소도 고려해야 함

디스크 전체 vs 파일별 백업

  디스크 전체 백업

     * 장점
          + 완전 복구 가능. 시스템을 이전 상태로 신속하게 복구할 수 있음
          + 가상화 시스템과 결합시 유용. Proxmox 등은 이러한 전체 백업을 쉽게 지원함
          + 일부 솔루션은 전체 백업에서 개별 파일 복구도 지원함
     * 단점
          + 물리 서버의 경우 다운타임이 필요함
          + 공간 소모가 큼, 불필요한 데이터까지 저장됨
          + 파일시스템 특성에 따라 느리거나 호환성 문제가 발생할 수 있음

  개별 파일 백업

     * 장점
          + 시스템 기본 유틸리티(tar, cp, rsync 등)로 가능
          + 변경분만 백업해 공간과 전송량 절감 가능
          + 개별 복구, 이동, 압축 및 중복제거 등 유연성 확보
          + 시스템 중단 없이 운용 가능
     * 단점
          + 단순 복사는 저장 공간을 많이 요구
          + 스냅샷 없이 동작 중인 파일 시스템을 백업하면 불일치·오류가 발생할 수 있음

스냅샷을 활용한 백업

     * 백업 대상이 동작 중인 파일 시스템일 때, 백업 ""시작""과 ""종료"" 사이에 데이터가 변경될 수 있어 데이터 일관성이 깨짐
     * MySQL, 브라우저 히스토리 등 열려있는 데이터베이스는 파일 복사만으로 복구가 불가능한 경우가 발생함
     * 일관성 있는 백업을 보장하려면, 파일 시스템의 스냅샷 기능을 먼저 활용해야 함
     * 대표적인 방법
          + 네이티브 파일시스템 스냅샷(BTRFS, ZFS 지원 시스템)
          + LVM 스냅샷: 공간 낭비 및 스냅샷 삭제 시 시스템 중단 가능성 존재
          + DattoBD: 안정성 이슈가 간혹 발생하나 UrBackup 등과 조합해 활용 가능

백업 방식: Push vs Pull

     * Push: 백업 대상이 서버에 접속해서 데이터를 보냄
     * Pull: 중앙 백업 서버가 각 서버에 접속하여 백업을 수행함
     * 보안상 백업 서버는 외부 접속을 차단하고 필요할 때만 클라이언트에 접근하는 Pull 방식이 더 안전함
          + 침입, 랜섬웨어 발생 시 백업 데이터 삭제를 방지하기 위해, 백업 서버 자체의 스냅샷도 별도로 장기간 보관함

추천 백업 시스템의 주요 특징

     * 즉각 복구 및 고속 처리
     * 외부 저장소에 보관(단, 로컬 스냅샷은 즉시 롤백용으로 유지)
     * Google Drive, Dropbox 등 상용 클라우드 미사용 권장. 데이터는 스스로 보유해야 함
     * 압축, 중복제거 등 효율적 공간 관리
     * 동작에 필요한 추가 구성요소가 최소화되어야 함

향후 연재 계획

     * 다양한 백업 시나리오 및 실제로 사용하는 서버, 주요 세팅, 각종 소프트웨어 및 기술을 소개할 예정임
     * 다음 연재에서는 FreeBSD 기반 백업 서버 구축 방법을 다룰 예정임

        Hacker News 의견

     * 백업을 반드시 ‘푸시’ 방식으로 해야 하는 머신들은 자기 공간만 접근 가능하도록 제한함이 필요함, 그리고 더 중요한 점은 백업 서버가 보안을 위해 자체 파일시스템 스냅샷을 일정 기간 유지함임, 그래야 워크로드가 손상되고 백업 서버에 접근해 랜섬 요구를 위해 백업을 지운 최악의 상황이라도 서버에 스냅샷이 남아 있음, 내가 선호하는 방법은 클라이언트에게는 새 백업만 기록하고 삭제는 아예 못 하게 함이며, 삭제는 별도 절차(수동 또는 cron 등)로 처리함, 이런 방식은 rsync/ssh에서 .ssh/authorized_keys의 allowed command 기능으로 구현 가능함
          + 나는 두 가지 모두 활용함, 백업을 두 군데에 보관해야 하긴 하지만, 이 듀얼 백업 구조를 원래 추구했음, 백업 소스들은 중간 위치로 푸시하고, 주요 백업 저장소가 거기서 풀함, 중간 위치가 작은 용량으로 스냅샷 보관은 하나 주요 스토리지와 소스는 서로 아예 인증이 안 됨, 즉 각각 중간 위치만 인증 가능하며 역방향 인증도 없음, 이 3군데 중 한두 군데가 해킹당해도 나머지는 안전할 가능성 높음, 인증서 백업은 완전히 별도 처리해서 인터넷 연결 서버에 전부 저장되는 일 없게 함, 진짜 중요한 데이터는 추가 조치로 오프라인 백업까지 이중, 구조 분리로 실제 백업 검증이 번거롭긴 한데, 백업 스토리지는 주기적으로 체크섬을 검증해 그 결과를 중간 호스트에 보내고, 원본 호스트에서 생성한 해시와 비교하면서 손상 이벤트를 잡음, 이렇게 세팅한
            결과 소스에서 백업 스냅샷 자체를 직접 해칠 수 없는 일종의 ‘소프트 오프라인’ 백업 구현임
          + 별도의 컨테이너 또는 백업 전용 유저를 써보는 것도 한 방법임, systemd-nspawn을 예시로 들면 경량 chroot jail처럼 사용 가능하며 내부에선 rm 실행 자체를 막을 수 있음, 간단히 pacman/pacstrap 등으로 chroot 만들고, systemd-nspawn/my machinectl로 관리, 이 방식은 평소 systemd와 별 차이 없이 접근제어, 파일 경로 제한, 메모리/CPU 제한, 특정 IP 대역만 접근 허용, 부팅 조건 세분화 등 다양한 컨트롤이 가능해서 유연함, BTRFS 서브볼륨 등도 활용하며, 필요하다면 시스템 전체를 systemd-vmspawn으로 완전히 분리 가능함, importctl로 복제 자동화도 매우 편함
          + 나는 “백업 서버가 백업 대상 서버에 대한 권한이 전혀 없는” 풀 방식 선호함, 라이브 서버가 루트권한을 해킹당해도 백업 시스템에는 영향 없음
          + 백업에 rclone copy를 사용하고 있는데, 오브젝트 삭제 권한이 없는 API키만 씀, rclone sync처럼 동기화하면 지워버릴 수도 있으니 이 방식이 더 안전함
          + syncoid에도 “클라이언트는 스냅샷/백업만 복사하고 삭제는 백업 서버가 직접 관리”하는 옵션이 있었으면 좋겠음, 현재는 삭제 권한을 줘야 해서 아쉬움
     * 놀라울 정도로 많은 사람들이 백업을 중요시하지 않음, 크고 작은 기업들 모두 마찬가지임, 내가 컨설팅하는, 연 매출 10억 유로 회사도 자체 백업이 없었고, 데이터센터 사업자가 불규칙하게 하는 디스크 카피에만 의존함, 복구 테스트도 스스로 해본 적 없음, 얼마 전 사용자 실수로 프로덕션 DB가 완전히 날아갔고, 최신 백업이 4일 전 거라 그 간의 트랜잭션을 모두 재현해야 했음, 그런데 이런 사고가 있었는데도 아무도 충격도 받지 않았고 그냥 평상시처럼 지나갔음
          + 이게 비즈니스에 정말 치명적이지 않은 한 괜찮다고 생각하는 듯함
          + 백업 요건을 과도하게 복잡하게 만드는 것도 흔히 본 현상임
          + 혹시 법적 이슈 때문일 수도 있음, 소송 혹은 법적 보존 의무 때문에 백업 자체가 오히려 위험 요소가 될 수도 있음
          + soc2 감사를 받은 재해복구 정책의 부작용임, 내가 근무한 회사도 모든 팀의 재해복구 정책(모두 soc2 승인)을 점검했는데, 결론적으로 진짜 대형 사고가 나면 회사를 접고 집에 가는 게 정상 복구보다 빠르겠단 결론에 도달함
          + 프로덕션 DB 전체 4일 데이터 손실이 진짜 사실이라면 고객이 화가 나지 않았겠냐고 묻고 싶음, 그 규모 회사에선 현실적으로 엄청난 타격이 될 것 같아서 실제로 어떻게 처리했는지 궁금함
     * 내용 공유에 감사함, 10년째 백업·재해복구 소프트웨어를 개발하면서 늘 나오는 경구는 “친구에게 백업/재해복구 솔루션 직접 만들라고 안 함”임, BCDR 구축은 워낙 빠뜨리기 쉬운 함정이 가득함, 몇 가지 핵심 포인트를 공유해보고 싶음, 백업은 ‘재해복구’가 아님, 진짜 사고 나면 수분~수시간 내 바로 복구돼야 비즈니스 신뢰를 지킬 수 있음, 재해복구 시간(RTO)·복구지점(RPO)가 관건임, rsync copy 방식 파일 복제만으론 파일시스템이 계속 변화하므로 시점 백업이 아님, 제대로 된 시점 백업에는 ‘크래시 컨시스턴트’ 혹은 ‘애플리케이션 컨시스턴트’ 백업이 필요한데, 후자는 중요 애플리케이션이 디스크에 상태를 저장하고 백업 중엔 멈추는 설정임, Microsoft VSS 같은 기능이 이 부분을 제공함, rsync copy나 크래시 컨시스턴트 백업을 절대 맹신하면 안
       됨, 저장장치엔 머피의 법칙이 항상 적용되니 여러 장소에 백업(3-2-1 전략)이 반드시 필요함, 직접 경험상 어떤 종류의 디스크든 전부 고장남, NVMe SSD > SSD > HDD 순으로 내구성이 낫긴 해도 예외 없음, 더 적고 싶은 게 많은데 시간이 늦어 여기까지만 남김
          + 3-2-1 비유는 예전 방식임, 요즘은 데이터 보관 위치가 무제한이므로 로컬 스냅샷, 원격 복제 및 서로 다른 방식의 이중, 삼중 백업이 더욱 중요함, zfs로 기본 스냅샷을 두고, Borg 추가로 사용 중이며 이 조합이면 거의 어떤 재해에도 충분함
          + 그 경구가 Alice in Chains 공연에서 비슷한 말을 들었을 때 떠오름, BCDR 솔루션은 기업 간 신뢰의 상징임, 이런 시스템들은 수십조~수백조 규모의 데이터를 보호하고, CTO라면 오픈소스 방식엔 절대 회사 백업을 맡기지 않음, 기업의 IT 지출은 대부분의 자산 가치 및 반vendor 락인 전략에 맞춰 점층적으로 증가함, 전문가 경험상 랜섬웨어 방지에선 불변성(immutability), WORM 백업이 핵심이며, 법규 미준수로 정부 IT에서 문제된 사례도 다수 경험함, BCDR 업체들은 랜섬웨어를 세일즈 포인트로 내세우지만, 진정한 불변성은 여러 공간에 걸친 데이터 복제에서 증명됨, 이때 3-2-1 전략이 진가를 발휘함, 더 많은 백업 원칙에 대해 의견 듣고 싶음
          + NAS를 사용하는 경우, 동일 제조사 동일 모델의 하드디스크를 양쪽에 쓰지 않는 게 좋음
          + “여러 장소에 백업 필수(3-2-1)”란 말에 완전히 동의함, 하지만 대부분의 개인에겐 “1”(오프사이트)이 현실적으로 불가능하며, 결국 백업 서비스를 쓰지 않는 이상 직접 백업할 이유가 없게 됨, 주변에 내 도시 외부에 24시간 컴퓨터 돌려주고 관리해줄 사람 없음
          + “rsync copy나 심지어 크래시 일관성 백업은 절대 신뢰해선 안 됨” 부분은, 결국 모든 시스템/서비스가 인프라 툴로 재구성 가능하니 DB와 파일/오브젝트 저장소만 적극적으로 백업하면 된다는 결론임, 복잡하게 VM 전체 백업하는 건 진짜 의미 없음
     * 깔끔한 글이지만 아쉬운 지점이 있음, 진짜로 좋은 백업 시스템이란 복구 속도와 절차가 명확해야 한다는 점임, 여러 번 경험한 건 백업 자체는 잘 돼 있다며 안심하다가 정작 복구 시 일부만 복구되거나 수 일 넘게 걸려 엄청난 손해가 발생하는 케이스임, restic이란 도구는 파일 단위 중복 제거된 스냅샷 백업이 가능해서 ZFS 같은 스냅샷 파일시스템 없을 때 유용함, 그리고 “푸시” 백업 방식일 때는 랜섬웨어가 백업까지 삭제할 수 있으니 “풀” 방식이 맞음, 푸시라면 읽기 전용 미디어(Bluray 등) 쓰거나 최소한 auto-snapshot/ZFS로 시점 복구가 가능해야 낫다고 봄
          + 랜섬웨어가 푸시 백업까지 삭제할 수 있다 해도, 서버 권한을 잘 제한하면 문제없음, Borg와 Restic은 ssh로 append-only 보장 가능하고, 로컬에서는 오프라인 백업 드라이브를 마지막 방어선으로 로테이션함, 실제 방법은 여기 참고
          + restic의 append-only 모드에서 주기적 서버 내부에서만 pruning한다면 pull 방식 안 써도 괜찮은지 궁금함, 이게 restic 공식 권장 랜섬웨어 방지법이라 알고 있음
          + “복구 속도”가 정말 요구사항에 따라 다름, 가족 사진 백업이 6개월 걸려도 난 괜찮음
          + 읽기전용 미디어 대신 권한을 제한한 push 서버도 대안임, 예를 들어 ssh를 scp만 허용, chroot 환경 한정, 매일 폴더 회전식으로 백업하면 랜섬웨어라도 예전 데이터 삭제 불가, 내 백업 절차도 chroot+scp만 허용하는 ssh 서버로 관리 중이며, 추가로 읽기전용 미디어도 병행함
     * 나는 별도의 백업 시스템이 필요한 건 아니고, 가족 4명의 25년치 사진(폰, 카메라, 다운로드, 스캔 등)을 효율적으로 모아놓는 표준화된 방식이 있으면 충분할 것 같음, 아직까지 만족스러운 방법을 못 찾음
          + 나는 NAS에 Immich를 올려서 사용 중이고, 미디어와 Immich DB dump를 매일 AWS S3 Deep Archive에 백업함, Immich는 Google Photos 기능도 충분히 제공하며, 데스크톱 사진/스캔을 NAS에 추가해두면 Immich가 자동으로 불러옴, HN 유저라면 세팅이 어렵지 않음
          + “25년치 사진”이 북미식 데이터 단위냐고 농담을 던지며, 사실상 백업 시스템이 반드시 필요하다고 지적함, 나는 syncthing을 gnu/linux/windows/android에서 메쉬로 굴리고, 두 개의 데비안 VM에 아카이브를 정기적으로 스냅샷하고 borgbackup으로 2차 백업함, RPO는 24시간이지만 원하면 더 줄일 수 있음, 다만 애플 기기는 syncthing 백그라운드 동작이 막혀서 이 구성이 안 맞음, 내 경우는 사진 500GB, 기타 문서도 수십~수백GB지만 diff 기반 백업 덕분에 효율적임
          + 다운로드, 스캔은 진짜 중요하지 않으면 어차피 버릴 데이터임, 폰/카메라는 Nextcloud로 싱크하고, 자체 홈 네트워크로만 백업 동작시킴, 그 후 NAS에서 매일 백업 후 건강 체크, 마지막 단계는 신뢰할 만한 클라우드 백업 또는 다른 집 드라이브 활용, 이러면 2차 백업까지 완성임
          + PhotoPrism 또는 Immich 같은 셀프호스팅 솔루션이 사진 중복 제거와 검색/태깅에 유용함, 클라우드 백업은 Backblaze B2 + Cryptomator 조합으로 암호화 스토리지와 DIY 업로드 스크립트 사용 가능, TB당 월 1달러 수준임
          + 나는 syncthing을 쓰는데, 공식적으로 Android 미지원이지만 포크 버전을 쓰면 잘 동작함, 추가로 ente.io 또는 immich 셀프호스팅으로 사진 백업 권장, 문서는 paperless ngx 등 따로 관리함이 좋음
     * Dirvish는 꼭 한 번 써볼 만한 경량 rsync 래퍼로, 회전, 증분, 보존, 전/후처리 스크립트 등 훌륭한 기능을 제공함, 20년 넘게 내 데이터 생명을 구해줬음, 기사에서 제기한 점들과도 궁합이 매우 좋음, dirvish 공식 사이트, rsync 공식 사이트
          + dirvish는 rsync 대비 어떤 점이 더 쉽거나 뛰어난지 궁금함
     * 나는 썩 게으른 방식으로도 하드웨어 고장, 도난 이슈까지 전부 대응해봄, 데스크톱 내부 임시 저장, 집안 외장 디스크, 오프사이트 외장 디스크(전부 Samsung T7 Shield) 조합임, robocopy /MIR로 매일 임시 또는 작업 후 복제, 주 1회 외장에 백업, 한 달에 한 번 외장 오프사이트 교환함
          + 외장 디스크는 반드시 적어도 서로 다른 배치나, 더 나아가 다른 모델을 써야 함, 같은 모델·로트면 보통 동시에 고장날 확률 높음(특히 복구에 왕창 부하가 걸릴 때)
     * 타이밍이 좋아 내 archlinux 설정과 백업 전략을 공유함, 설정/백업 자동화 스크립트, borg 자동화 레이어도 공개함
          + 나는 python+b​​org로 SAN의 51 블록 디바이스 스냅샷, 71개 파일 시스템 백업, S3 싱크까지 자동화함, 복구는 오프사이트에서 실제 테스트했고 VM 부팅까지 문제 없이 성공했음, 자동화가 아직 복잡하고 미완성이라 복구 속도는 느리지만 아주 저렴하게 구축했음, borg 진짜 강력함, 이런 건 누구나 시도할 수 있고 결과적으로 매우 경제적임
     * 내가 가진 귀중한 데이터는 100MiB 미만이라, 선택된 경로만 tar+압축+암호화해서 한 주에 두 번씩 백업, 몇 달치만 로테이션 보관함, 집 안팎에 두며 점검이나 유지 보수도 거의 필요 없음, 몇 줄짜리 sh 스크립트만으로 문제없이 자동화되는 무난한 구성임
          + 내일 갑자기 내가 죽는다면 가족과 후손이 꼭 복구할 가치가 있는 데이터가 뭔지 돌아봐야 함, 수십만 개 파일보단 핵심적인 사진, 영상, 텍스트 몇 개면 충분할 수도 있음
          + 이 댓글이 나에게 진짜로 귀중한 데이터가 뭔지 다시 고민하게 했음, 내 사진은 압축해도 최소 수 GB, 연락처나 중요하지 않은 건 작음, 대체로 다른 건 잃어도 괜찮음, 복구키는 좀 더 안전하게 보관해야겠지만 가장 중요한 계정들은 오히려 복구키가 없음, 사진만도 2TB 가까이 됨(취미 사진가의 비애)
     * 백업 일관성에서 가장 어려운 점은, 어플리케이션 데이터를 서비스 다운 없이 일관되게 백업하는 게 사실상 불가능하다는 점임, 디스크 스냅샷으론 순간에 어떤 서비스가 mid-write인 상태에서 스냅샷될 수밖에 없어서, 복구 시 손상된 상태로 뛰어들 확률이 큼, DB 덤프는 이 문제를 상당부분 완화하지만, 종종 서비스 밖에서 백업해야 해서 쿼리 도중일 수 있음, 누가 그 부분에 대한 노하우 있으면 듣고 싶음
          + 대체로 DB는 이 부분에 견고해서 언제든 디스크 스냅샷 떠도 백업용으론 충분하다고 봄, 걱정되는 건 배터리 없는 캐시 등 특수 상황인데, 요즘 클라우드 등은 그런 구조가 거의 없어 별 문제 없음
          + DB 외에도 캡처해야 할 어플리케이션 상태가 더 있는지에 따라 전략이 다름(예를 들어 캐시도 백업해야 하는지 등), pg_dump/mysqldump가 실제로 라이브 DB 스냅샷을 안전하게 가능하게 하지만, 좀 느리고 덤프 용량이 커지는 등의 부담이 있음, 이 문제를 피하려면 큰 PostgreSQL DB엔 백업 전용 read-only replica를 두고 replication 멈춘 뒤 백업 실행, 이후 replication을 재개하는 방식도 경험함
"
"https://news.hada.io/topic?id=22072","fstrings.wtf","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              fstrings.wtf

     * fstrings.wtf는 Python의 f-string 기능에 대한 이해도를 점검하기 위한 온라인 퀴즈
     * 최신 Python 3.13 버전에서 적용되는 f-string의 다양한 동작과 예외 사항을 테스트하는 질문으로 구성됨
     * 사용자는 별도의 과정 없이 즉시 퀴즈를 진행할 수 있음
     * 실무에서 자주 접하는 f-string 관련 트릭이나 오동작을 미리 경험해 볼 수 있음

        Hacker News 의견

     * 문자열 보간 기능은 타입 추론처럼, 한 번 익숙해지면 없을 때 매우 불편함을 느낌. 점진적으로 더 추가하면 점점 더 좋아지는 듯하지만, 어느 순간 알아보기 힘든 코드가 되어버림을 깨닫게 됨. 일반적으로는 CS 분야에서 어떤 기능이든 더 추가하고 싶어하지만, 수학적으로 그게 어려울 때도 있음. 여기서는 아예 없는 것과 너무 많은 것 모두 둘 다 피해야 함. Python과 C#은 유저 취향에 맡기는 방식을 선택함. 16페이지짜리 복잡한 인터폴레이션 문자열을 쓸 수 있지만, 동료들이 싫어할 수 있으며 코드 리뷰에서 탈락할 수도 있음. C++ 23은 아예 처음부터 보간을 금지하는 스타일임. Rust는 식별자만 보간 허용하는 아주 제한적인 옵션을 선택했으며, 이게 어떤 사람들에겐 부족하고, 어떤 사람들에겐 과한 것으로 느껴질 수 있음
          + Java String Template 팀도 비슷한 과정을 겪었다고 생각함. 겉보기에 상당히 멋진 시스템이었지만, 직접 사용하려니 방향성이 불가능하다고 느껴서 결국 완전히 빼버림. 보간 기능에 대한 수요와 지금까지 투입한 노력까지 생각하면 꽤 흥미로운 결정임. 결국 되돌릴 수 없다고 판단해 원점으로 돌아간 것임
          + 순수성과 실용성은 서로 상충하며, 각 언어마다 서로 다른 균형점을 찾게 됨. 정해진 정답이 없으니 개발자마다 자신만의 올바른 기준이 있다고 반복적으로 주장하게 됨
          + C#에서 숫자나 날짜 포매팅이 필요할 때마다 문서부터 찾아보게 됨. 해당 미니 언어가 너무 별로라서 굳이 외우지 않기로 했음
          + 복잡한 예제를 맞추지 않고 보간을 적절히 컨트롤하는 게 어렵지 않았음. f-string 내부에 f-string을 중첩해서 쓸 필요성을 못 느꼈고, 자주 사용하는 포맷 지정자는 :02x 하나뿐임
          + Rust의 제한적인 보간 방식이 전혀 해결책이라고 생각하지 않음. 특정 상황에서만 동작하니, 코드 리팩토링 때 계속 신경써야 하고 불필요한 손질이 많아짐. 최소한 필드 접근 정도는 허용해야 한다고 생각함. 반면 파이썬에서는 예제처럼 이상한 경우가 있긴 해도 실사용자는 신경 안 쓰고 그냥 fstring을 즐겁게 씀
     * fstring.help 등에서 설명하는 몇 가지 팁(가운데 정렬, 0x/0b/0o 프리픽스, 아스키 표시 등)을 최근 알게 됨. 중첩 f-string 문제에도 관심이 있었는데, 3.11까지는 따옴표만 다르게 해서 가능했음. 3.12에서 여러 제약이 정리됐다고 알고 있음. f-string 덕에 편리하긴 한데, 구식 % 포매팅, .format() 방식, 새로운 방식의 미묘한 차이까지 수시로 바꿔 써야 하는 게 생각보다 번거롭고, 피할 수 없는 상황이 자주 옴. 사용성은 발전했으나 여전히 오래된 방식을 써야 할 때 아쉬움이 큼
          + 3.12에서 이런 부분이 정리된 것에 대해 공식 문서에서 확인 가능함
          + 종종 동료나 AI가 logger 호출에서 f-string을 사용하는 걸 보는데, logger는 lazy interpolation이라 일부러 그렇게 구현했을 텐데, 굳이 그 좋은 기능을 놓치는 게 의아함
          + 중첩 f-string은 트릭 같은 문제라고 생각함. 같은 스타일 따옴표로 중첩 가능한 게 추가된다는 건 알았지만, 어느 버전일지 몰랐음. 여전히 f'{f""{}""}' 트릭을 쓰고 있는데, 그 이유는 내 코드를 조금 더 오래된 파이썬 버전까지 지원하고 싶기 때문임
     * f-string에서 등호 기호(=)로 식과 값을 함께 출력하는 기능을 처음 알게 됨
          + 파이썬 릴리즈 노트는 정말 읽을 가치가 있다고 생각함. 늘 긍정적인 놀라움이 있음. 등호 기능은 Python 3.8에 추가됨 관련 링크
          + 함수 키워드 인자에서도 비슷한 기능의 PEP이 채택되지 않은 게 아쉬움. foo(bar=bar) 대신 foo(bar=) 형태라면, 단순히 인자를 넘기는 경우와 차이도 더 쉽게 드러나서 디버깅에 효율적이었을 것임
          + 놀라움을 주는 것에 비해 충분한 가치를 주지 못하는 기능이라고 생각함. 예상치 못한 동작을 할 가능성이 높아, 버그의 원천이 될까봐 걱정임. locals()의 일부만 출력해주는 표준 함수가 더 나을 듯함
          + 디버깅 출력에 엄청 자주 쓰이는 패턴임. C++에선 표현식 하나를 따옴표로, 하나는 그냥 해놓는 습관이 생겼음. 알기 쉬워지고 생각 별로 안 해도 되어서 좋음
          + print(f)로 디버깅할 때 정말 유용함
     * URL과 달리 실제로는 WTF 수준의 질문은 거의 없다고 생각함. 20, 21번 질문처럼 정말 놀라운 게 일부 있음
          + 나는 walrus operator가 너무 유용해서 절대 포기 못함. 패턴 매칭 및 다중 분기 처리할 때 코드가 훨씬 깔끔해짐. 자주 쓰지는 않지만 딱 적합한 상황에서 효과가 큼
          + 이건 walrus operator 때문이 아니라 python의 string.format 동작 방식 때문임 관련 문서 참고
     * f-string이 도입되기 전 이후로 파이썬 본격적으로 쓴 적 없지만, 문법 규칙은 거의 다 맞추었고, 오히려 값 반환 관련 실수만 몇 번 있었음. f-string 자체가 파이썬에서 제일 덜 WTF인 부분 아닐지 생각함
     * Lua용 f-string 비슷한 라이브러리 만들면서 꽤 많은 문법을 학습했지만, f""{...}""와 walrus operator는 예상 못했음. 그래도 Wat 급 이상함과는 거리가 있음. 관련 라이브러리는 여기 참고
          + 해당 라이브러리 진짜 멋져보임
     * 특별히 WTF할 만한 내용은 없다고 생각함. 상당 부분은 f-string 보다는 str.format() 미니 언어 문법에 관한 내용임
          + WTF할 만한 게 일부 있긴 했지만, 대부분은 문자열 포매팅 문법을 아는지에 관한 내용임
     * 기능이 너무 많으면서 임계점을 넘은 듯함. 한 명의 개발자가 모든 걸 숙지할 필요도 없고, 실제로 써야 할 때마다 문서를 찾아야 해서 비효율적임. 드물게 사용하다 보니 문법을 까먹고, 같은 기능을 직접 구현하는 게 훨씬 빠르며, 동료가 봐도 커스터마이즈하기 쉽다는 점이 장점임. 왼쪽 패딩? 2줄 함수면 충분함. 포매팅 문법(n이 먼저인지 <가 먼저인지 등)이 헷갈릴 바엔 ad-hoc으로 직접 구현하는 게 빠름
          + 왼쪽 패딩 같은 건 string.format 메서드에서도 사용 가능하며, 해당 방식은 Python 2.6(2008년 출시)부터 있었음 관련 문서 참고. 나는 오히려 포맷 문법이 머릿속에 잘 남아서 유용하게 씀. 게다가 포매팅은 커스터마이즈를 위한 후킹이 열려 있음
          + 나는 중간 지점을 선택하고 싶음. pad_left/pad_right 함수에 패딩 문자도 키워드 인자로 직접 지정하게 하는 게 제일 쾌적함. 일상적으로 종종 필요한 만큼 표준 라이브러리에 있으면 좋겠음. 언어 라이브러리에 이런 게 없으면 나중엔 각 프로젝트마다 퀄리티 떨어지는 구현물 천지로 변하고, 자바스크립트처럼 됨. 내 프로젝트에선 ^나 <> 같은 파이썬 포매팅까지 쓸 일은 없겠지만, '모노스페이스' 출력이 중요한 소프트웨어에선 오히려 굉장히 중요한 기능일 수 있음
          + 이상한 기능이나 편법을 한 코드베이스 안에서 반복적으로 많이 쓸 때가 있음. 한 번 만들고 복붙해서 계속 재사용하는 경우가 많음
     * 만약 이런 구문이 JavaScript였다면 대부분 직관적이지 못한 문법과 이상한 기능이라며 한탄했을 것임
          + 내 생각엔 이 퀴즈의 진짜 논지는 JavaScript 못지않게 파이썬에도 footgun이 많다는 것 같음. JS에서는 이런 'WTF' 포스팅이 자주 있어서 그런가 봄
          + 자잘한 트릭을 빼더라도, JavaScript가 압도적으로 더 직관적이지 못한 문법의 왕이라고 생각함. 파이썬의 f-string 요소도 특이하긴 한데, 특정 상황에서나 노출되고, JS에서는 배열 두 개 비교하기도 전에 의존성 설치하느라 기다려야 함
          + 자바스크립트 템플릿 리터럴 관련해서 물어보고 싶음. 파이썬처럼 let template = 'hello ${name}'; 하고, template.format({ name: 'joe' }) 식으로 여러 번 동적으로 값 채워넣는 게 JS에서는 불가능한 것 같음. 그래서 직접 구현할 수밖에 없었음. 태그드 템플릿 등도 살펴봤지만 템플릿 자체를 재사용하는 게 어려웠음. JS 문법이나 이상한 기능을 한탄하는 건 충분히 공감함
          + 만약 Perl이었다면 오히려 칭찬이 쏟아졌을 거임
     * pyformat.info(링크)를 참고하는데, 극도로 상세하지는 않아도, 대부분의 합리적인 예시는 모두 정리되어 있음
"
"https://news.hada.io/topic?id=21968","GLP-1이 생명보험 업계를 무너뜨리고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        GLP-1이 생명보험 업계를 무너뜨리고 있음

     * GLP-1 계열 체중감량 약물의 확산으로 생명보험사가 기존 위험 평가 모델을 제대로 적용하지 못해 손실 위험이 커짐
     * GLP-1 복용자는 단기간에 BMI, 혈압, 혈당, 콜레스테롤 등 핵심 건강 지표가 개선되어, 보험 가입 시 실제보다 건강한 상태로 저위험 판정을 받는 사례가 증가함
     * 약 65%가 1년 내 GLP-1 복용을 중단하고, 대다수는 체중 및 건강 지표가 원상 복귀되어 ""모르타리티 슬리피지(리스크 과소평가)"" 문제가 심화됨
     * 보험사는 엄격한 건강 이력 질문, 장기 감량 유지 증명 요구, BMI 보정 등으로 대응 중이나 근본적 해결에는 한계가 있음
     * 장기 복용 및 약물 순응도(Adherence) 관리에 성공하는 기업이 보험사와의 대규모 파트너십을 선점할 것이며, 단순한 3개월 단위 처방/재개 편의 등 실질적 해결책이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

컨퍼런스 현장과 문제의식

     * HLTH 암스테르담에서 GLP-1 약물과 보험 업계 영향에 대한 논의가 급증함
     * 보험업계 인사들 사이에서 ""이 약물 확산에 어떻게 대응할 것인가?"" 라는 근본적 고민이 확산됨

생명보험사의 위험 평가 구조

     * 생명보험사는 수십 년 축적된 사망률 데이터를 바탕으로 연간 보험료 산정 및 손익 예측의 정확도가 98%에 달함
     * 보험 언더라이팅(Underwriting) 과정에서 HbA1c, 콜레스테롤, 혈압, BMI 등 핵심 건강지표로 위험도를 판단함
     * 이 네 가지 지표는 GLP-1 약물이 가장 빠르게 개선시키는 지표로, 복용 6개월 내에 위험 프로파일이 완전히 달라질 수 있음

GLP-1이 만드는 ""건강 착시""와 보험사 리스크

     * 예시: 42세 지원자가 BMI 25(정상), 건강검진 정상, 처방 기록 없음 → 보험사는 저위험군으로 분류
          + 실제로는 1년 전 BMI 32(비만), GLP-1 약물로 14kg 감량, 기저 대사증후군 보유
     * 65% 이상이 1년 내 복용 중단 → 대부분 체중 및 건강지표 원상복귀
          + 2년 내 BMI, 혈압, 혈당, 콜레스테롤 등 대부분 수치가 복구
     * 보험사는 30년짜리 저위험 보험을 고위험 가입자에게 판매하는 결과로 이어짐
     * 보험 업계에서는 이를 ""모르타리티 슬리피지(Mortality Slippage)"" 라고 부름
          + 2019년 이후 모르타리티 슬리피지 비율이 5.8% → 15.3%로 급증(6건 중 1건 꼴로 잘못된 가격 책정)

보험사의 대응 전략

     * 질문 방식 변화:
          + 기존 ""최근 12개월간 체중 변화가 있었습니까?"" →
            ""지난 12개월 동안 체중감량 약물로 10kg 이상 체중이 변했습니까?""
          + 구체적 수치(10kg)로 정확한 응답 유도
     * 대답에 따라
          + 아예 가입 거절
          + 최소 1년간 체중 유지 증명 요구
          + 위험도 보정(BMI 2~3 추가 반영)
     * 그러나 이 방식도 근본적 해결책이 아닌 임시 방편임

보험과 순응도(Adherence)의 비즈니스 기회

     * 보험사는 현재 GLP-1을 단기 체중감량 도구로 인식
     * 실제로는 장기 복용 시 비만, 심혈관질환, 사망률 모두 개선된다는 확실한 데이터 존재
     * 약물 순응도 관리에 성공한 기업(예: 장기 복용, 복약 이탈률 감소)이 보험사와의 대형 파트너십을 통해 수백만 달러 규모 시장을 선점할 전망
          + 향후 약가 인하, 제네릭 출현 시 수십만 명 단위 고객 확보 가능

Wrap-around Care와 실질적 해결책

     * 보험사는 ""wrap-around care""(맞춤형 건강관리 서비스) 에 기대를 걸고 있으나, 실제 검증된 실행 사례나 데이터는 부족
     * 과거 스타틴(Statin) 처방 사례 참고:
          + 30일 처방에서 90일 처방으로 단순화하자 순응도 급상승
          + 단순한 3개월 단위 처방, 복약 중단 시 재개 간소화, 문자 알림 등 행동 개입이 비용 효율적이고 효과적임

결론

     * 보험사는 GLP-1 약물이 만든 '건강 착시' 로 인해 손실 위험이 커진 상황
     * 실제 건강 개선으로 이어지는 약물 순응도 관리와 편의성 개선에 성공하는 기업이 보험사-환자-기업 모두가 윈윈하는 시장을 선점할 수 있음
     * 보험사도 점점 더 까다로운 질문과 위험 탐지 시스템을 도입 중이며, 시장 표준이 되기 전 먼저 실행하는 기업이 독점적 위치를 차지할 전망임

        Hacker News 의견

     * 그래서... 사람의 전체 사망률을 강력하게 낮춰주는 기적의 약이 있는데, 건강보험과 생명보험은 시간적 관점이 달라서, 생명보험사 입장에선 사람들이 이 약을 끊었다 다시 시작하는 바람에 사망률 예측이 어려워짐 제 개인적인 경험상, 보험료 차이는 연간 고객 한 명당 수백 달러 수준인데 약값은 수천 달러임 (2025년에도 일시적으로는 그러할 전망, 곧 더 싸질 가능성 높음) 결국 이런 상황은 우리 모두에게 굉장히 좋은 전개임
          + 유지 기간에는 별로 힘들진 않지만, 비용만 비쌈 용량을 늘릴 때가 불편할 뿐, 어느 정도 유지되면 거의 신경도 안 쓰일 정도임
          + 처방약을 복용할 때 뭐가 힘든 점인지 설명 좀 부탁함
          + 장기적으로 보험료 차이가 그렇게 크지 않아서 보험사가 직접 지원하긴 어렵다고 하셨는데, 왜 생명보험사들이 metformin 같은 기존 데이터가 충분한 약의 추가 연구에 더 투자하지 않는지 궁금해짐 TAME 임상시험 참고
          + 혹시 내가 기사를 잘못 읽은 건지 모르겠음 내가 요약한 바로는, GLP-1이 실제 사망률은 바꾸지 못하고 사망 관련 지표만 낮추는 결과였음 (대부분의 사용자가 2년 안에 원래대로 돌아감)
          + 많은 사람들이 말씀처럼 복용 경험이 힘들진 않고, 가격만 문제라고 느끼는 것 같음
     * 비만은 암, 당뇨, 심장질환 등 다른 질환과도 강하게 연결되는 특성이 있음 어느 시점에 건강보험사가 특수 의약품 지급 비용보다 GLP-1 지원이나 전면 무료 제공이 더 저렴해질 수 있을지 궁금함 예를 들어, 내가 가입한 보험은 매년 독감 주사를 무료로 제공해줌, 그게 입원 증가보다 싸기 때문일 것임
          + 미국 의료 시스템의 동기를 너무 긍정적으로 보는 것 같음 보험이 직장과 연동되어 몇 년마다 바뀌고, 대부분이 Medicare 가입 후 사망함 예방적 치료가 수십 년 뒤에 효과를 내더라도 보험사는 별로 관심이 없음
          + GLP-1은 생각만큼 비싸지 않다고 생각함, 그러니 쉽게 구할 수 있으면 좋겠음 예전에는 Rogaine/Minoxidil도 처방 필요했지만, 지금은 마트에서 셀프 계산기로 살 수 있음 흡연 관련해서도 니코틴 제품에 보조금을 줬던 사례가 있고, 니코틴 껌도 담배에 비해 충분히 저렴했음 하지만 위험도와 비용을 잊지 말아야 함 비만은 흔히 생각하는 것만큼 치명적인 리스크가 아님 흡연의 경우 폐암 상대위험도가 1~4회/일 흡연자에서 5를 넘고, 고도 흡연자는 20 이상임 반면 비만은 심장질환, 당뇨 등에서 가장 높아도 4~5, 대부분은 1.1~2 수준임 미국의 31%가 BMI 30~40, 9%는 40 초과임 치매 등 타 해악 연구도 상대위험도 1.1 언저리임, 스트레스 등 다른 요인과 비슷한 수준임 무료 제공 등 보조금 논의엔 실제 위험/비용 분석이 더 넓게 이루어져야 하고, 비만만을 강조하지 않는
            다양한 요인 평가가 필요함
          + 혹시 화제가 전환된 건데 내가 오해한다면 양해 바람, 미국 밖에서는 잘 모를 수도 있으니 설명함 이번 논의는 생명보험이고 이건 건강보험과 완전히 다름 건강보험사는 이미 장기적으로 절약하려고 예방치료, 무료 건강검진 등에 투자함 GLP-1도 아마 특허 때문에 비용이 높을 수도 있음 생명보험은 사망 예측 모델이 핵심임, 갑작스런 변수 변화는 모델링엔 최악임 보험사는 방대한 이력 데이터로 위험 헤지를 함
          + 30년간 GLP-1 사용에 대한 장기 보험 통계가 전혀 없음
          + 독감 예방주사 예시처럼 미국에서는 보험사가 법적으로 ACIP 권고 백신을 무료로 제공해야 함, 독감 백신도 6개월 이상이면 대부분 무료임
     * 참고로 지금은 사람들이 GLP-1을 계속 복용하는 데 어려움이 있음관련 참고1 다른 생활습관 기반 요법보다도 GLP-1이 재발률이 낮은 편임 GLP-1은 여러 질병 위험을 줄여주고, 점점 고령층에게도 많이 처방되고 있음 하지만 특허 만료 이후 약이 널리 퍼질 경우, 보험사도 모델을 업데이트해서 곡선이 안정될 전망임관련 참고2
          + 여기서 말하는 바를 문자 그대로 요약해 줄 사람 있음? 혹시 어두운 유머나 사회적 풍자가 섞여 있는지도 설명 바람
     * 저는 노르웨이 임상 심리사임 주관적 경험을 공유함: 사람들이 GLP-1 계열을 그만두는 주된 이유는 1) 먹는 즐거움 회복 원함 2) 복용 자체가 귀찮음 3) 장기 복용에 대한 막연한 우려(근거는 딱히 없음) 4) 가격(부국이라 부담이 적긴 함) 5) 바늘 싫음, 다른 사람 배려, 귀찮음 등임 약 없이 20kg 감량 효과를 다른 부작용 없이 얻으려면 정말 드물다고 봄 비만이 생각보다 위험하다는 걸 일반인들이 충분히 인식하지 못하는 게 아쉬움 오젠픽 등 GLP-1에서 잘 반응하는 사람은 정신과적 문제가 적은 경우가 많음 오히려 약을 빨리 끊으려는 건 감정적으로 과식하는 심리적 이슈가 있을 수 있음 그래서 장기적으로 요요로 인해 수명에 영향을 주는 집단은 비만+정서문제가 겹친 경우가 주로 대상이 된다고 추정함 어디까지나 개인적 관찰, 가설임
          + 혹시 약 복용 중에 도박, 음주 등의 즉각적 만족 행동이 줄어든 경향이 있었는지 궁금함 본인은 ZepBound 복용 중에 뇌가 즉각적 만족을 멀리하는 경향을 느낌
          + 현재 약값은 얼마쯤 하는지, 조만간 대체제나 제네릭도 나오는지 궁금함
     * 기사에서 보험의 본질적 내용을 빠뜨렸다고 느낌 이상적인 보험 포트폴리오는 사망과 장수 리스크 균형이 되어야 함 이 구조에서는 GLP-1나 다른 사망률 변화 요인들이 큰 위험이 아님 보험사들은 위험을 서로 교환, 재보험 등으로 리스크를 분산함 연금과 장수 상품이 균형에 도움됨 하지만 개별 상품마다 규모와 지역차가 큼 Swiss Re 자료에서는 언더라이팅 수준 차이(간략/정밀)에 따른 보험사 이익 감소가 문제로 제시됨, GLP-1 때문이 아님 위험 다각화 안된 상품들은 수십년간 이익이 높았고, GLP-1은 이를 확장시킨 것뿐임 보험사 장기 수익률은 매우 양호함 재보험사들이 시장 질서를 맞추며, 위험 다각화가 없으면 시장에 남기 어려움 Swiss Re가 이런 엄격한 정책 분석 이유가 바로 이거임
     * 문제는 (GLP-1만의 현상은 아니지만) 사람들이 체중 감량 후 생명보험에 가입했다가 다시 체중을 되돌리는 현상임 비용 때문에 GLP-1을 중단하는 게 주원인임, 비용을 해결해야 함
          + 저는 효과가 시간이 지나면 줄어드는 것 같음 장기 복용보다는 단기적으로 교정용으로 쓰는 게 나을 수 있음
          + 20년 이상의 장기 GLP-1 복용 효과가 충분히 검증됐는지 잘 모르겠음 수백만 명이 약에 의존하게 되는 건 사회적으로 바람직한 해법이라고 보기 어려움 개개인에겐 필요하지만, 사회 전체로서는 신중해야 함
          + 건강/체중 기반 신용점수 시대가 곧 올지도 모름, 실제론 좋은 아이디어일 수 있음
          + 보험사 입장에서 '평가/측정'은 힘든 일임 인간은 복잡하고 평가 받는 걸 싫어함
          + 효과가 나타나서 중단하는 경우도 많음, 계속 약을 써야 할 이유를 못 느낌
     * 저는 두 달 동안 Mounjaro 복용, 식이조절과 만보 걷기도 병행함 25파운드 감량하고 당화혈색소가 5.7에서 5.0으로 내려감 콜레스테롤도 정상 범위 약 끊고 추가로 25파운드 더 감량, 아직도 요요 없음 체중을 다시 찌운 경우는 습관 변화 없이 문제의 근원을 해결하지 않은 경우임 습관과 지원 시스템이 중요함, 결국 자신이 못 고치면 원래대로 돌아옴
          + 이건 심각한 불안장애 환자에게 SSRI 처방 몇 달 후 약 끊고 생활습관만으로 평생 버티라는 논리와 비슷해 보임 운 좋게 되는 사람도 있겠지만, 실제로 대다수는 그렇지 않음 '교훈을 배워라, 의지로 버텨라‘는 식 조언은 의미가 없음
          + 저도 지난 2달 동안 거의 15kg 감량함, 딱히 힘들지도 않았음 체중 문제 없었지만 최근 몇 년간 슬금슬금 107kg까지 쪄서 결심함 흡연도 두 번이나 별 문제 없이 끊었음(20대, 그리고 코로나 때 다시 시작했다 금연) 다른 데선 힘들지만 이런 부분엔 천성적으로 운이 따르는 듯함
          + 본인도 기적의 약이 있어야 교훈을 얻었다는 점에서, '교훈 못 얻으면 자기 탓'이란 식의 논평은 다소 우스움
          + 언제쯤 그랬는지 궁금함
     * 저도 직접 경험 남김: Wegovy는 6개월 복용했으나 체중변화 거의 없었고 가끔 구역질만 있었음 이후 의사가 Mounjaro + Phentermine 처방, 이후 식욕 제어가 엄청 쉬워짐 6개월간 20kg 감량, 전혀 부작용 없었음 Phentermine 첫 복용만 살짝 어지러웠고 바로 괜찮아짐 많은 사람이 살이 찌는 건 신체적 배고픔보다 머릿속에서 이유 없이 음식 생각이 나는 것에 더 영향이 큰 듯함 약물(특히 Tirz+Phent)이 이 부분엔 훌륭함
          + 케이스 따라 다르지만 연구에서도 Tirzepatide가 Sema보다 효과가 더 좋다는 점 확인됨
     * 이걸 HIV, PreP, 우울증, ADHD 등 다른 치료에도 적용해볼 수 있음 수십 년 동안 연구 데이터는 복약 순응도가 사망률, 삶의 질, 생산 수명 증가의 핵심임 장기적으로는 사람들이 건강하게 더 오래 사는 게 사회 비용 절감임 문제는 제약/보험 업계, 특히 서구에서 분기별로 주주 수익을 극대화하다보니 의약품은 소량만 비싸게 만들고, 약값 협상은 막으면서 보험사는 보장만 늘리도록 압력 받음 GLP-1이 이런 흐름에서 한계에 다다른 약일 수 있음 어차피 이 논의는 이미 수십 년간 반복된 것임 참고로, 본인은 건강보험사에 전화해서 90일 처방으로 기간 늘려달라고 요청하려 함, 오랫동안 복약 잘 지켜왔어도 자동 연장이 전혀 안 되는 현실임
"
"https://news.hada.io/topic?id=22017","Show GN: Userband: 유저 피드백 플랫폼","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Show GN: Userband: 유저 피드백 플랫폼

   이메일 클라이언트 타우린 만들다가 피벗했습니다.
   저희가 더 잘 만들 수 있는걸로 만들어보기로 했어요.

   피드백 수집부터 체인지로그, 로드맵, docs, 블로그까지
   하나의 흐름으로 이어지는 유저 피드백 플랫폼을 만들고 있습니다.
   이 과정은 팀이 고객과 함께 제품을 발전시키며
   PMF에 더 빠르게 도달할 수 있도록 돕습니다.

   8월 초부터 사전신청자의 베타 사용이 시작될 것 같아요.

   CS 관련해서 추가 기능이 붙으면 (위젯 등) 유료 플랜에 넣을거긴 한데요,
   Changelog / Feedback / Roadmap / Docs
   이건 유지비용이 크지 않아서 계속 제한된 선에서 무료로 둘 예정입니다. (커스텀 도메인 가능)

   GN에서 build in public 하시는 분들이나 이에 관심 있으신 분들이
   제품 만들어가실 때 무료버전 사용으로도 충분히 유용하실 것 같아서 공유드립니다.

   랜딩페이지가 엄청 귀여워요!

   하하 감사합니다 !! :)

   7/17 현재까지 개발된 관리자 콘솔 모습 여기에도 공유드립니다..! 글 수정이 안되네요
   https://linkedin.com/feed/update/…

   와 멋지시네요. 특히 피벗 도전 응원합니다 ㅎㅎ

   감사합니다 ! ㅎㅎ

   와~ 디자인이 예쁘네요 👍

   감사합니다!
   저희도 피벗하기 전에 피드백 관리 도구로
   featurebase, canny, productlane 등 다양한 서비스를 사용해보았는데
   기능면에서 크게 차이가 나지 않으니, 핵심은 디자인이더라구요 !
   예쁘고, 유저/어드민 모두에게 심리스한 경험을 주는 디자인과 빠른 서비스를 목표로 만들어보려고 합니다.
   댓글 감사합니다 !
"
"https://news.hada.io/topic?id=22061","내가 Kagi를 활용하는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            내가 Kagi를 활용하는 방법

     * Kagi는 광고 기술에 의존하지 않는 유료 검색 엔진으로, Duo Family 요금제(월 $14)로 만족스러운 검색 환경을 이용 중임
     * 핵심 기능은 Lenses(렌즈) 와 Personal Results(개인화 결과) 로, 원하는 웹사이트만 결과에 노출하거나, 특정 도메인을 차단/우선순위 지정 가능
     * Kagi의 Domain Insights 기능으로, 많이 차단되거나 홍보되는 도메인을 한 번에 파악하고 차단 목록에 쉽게 추가 가능
     * SEO로 오염된 16개 대기업 도메인 차단 필터를 적용하여, 검색 품질을 높이고 있음
     * 사용자는 직접 제작한 렌즈(예: Smaller Web, The Old Web) 를 통해 소규모 웹 커뮤니티나 레트로 웹사이트 검색도 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Kagi 개요

     * Kagi는 사용자의 프라이버시 보호와 비광고 기반 검색 환경을 제공하는 프리미엄 검색엔진임
     * 2023년 초부터 Kagi를 사용해오며, 다양한 기능을 활용해 맞춤형 인터넷 검색 경험을 구축했음
     * 기존의 검색엔진과 달리 광고 기반이 아닌 유료 구독 모델을 채택해 이용자가 직접 비용을 지불하는 구조로, 광고 없는 쾌적하고 정제된 검색 경험을 제공함
     * Duo Family 요금제(월 $14, 2명 이용)로 본인과 배우자 각각이 Professional Plan을 사용중
     * 한 달에 $14를 지불하고 광고로부터 자유로운 쾌적한 검색 환경을 누릴 수 있음
     * 구독 비용이 들지만, 그만큼 가치가 확실한 서비스라고 평가함

핵심 기능 1: Lenses(렌즈)

     * Lenses는 특정 웹사이트 또는 사이트군 위주로 검색 결과를 필터링할 수 있는 맞춤 필터 기능
     * 기본 제공 렌즈 외에도 20개까지 사용자 정의 커스텀 렌즈를 동시에 활성화할 수 있음
     * 예시: Kagi가 운영하는 Small Web 렌즈(소규모 웹사이트만 검색), 직접 만든 Smaller Web(작은 커뮤니티 중심), The Old Web(과거 유명 웹호스팅 서비스 중심) 등

핵심 기능 2: Personal Results(개인화 결과)

     * 사용자가 특정 도메인이나 사이트의 노출 빈도를 직접 조절할 수 있는 기능임
     * 특정 도메인/웹사이트를 차단, 우선순위 하락/상승, 고정 등으로 개인화 가능
     * 검색 결과 옆의 방패 모양 아이콘을 클릭하여 바로 설정할 수 있어 편리함

추가 기능: Domain Insights

     * Kagi Domain Insights를 통해 가장 많이 홍보되거나 차단된 도메인 목록을 확인할 수 있음
     * 로그인한 사용자는 이 페이지에서 원하는 도메인을 즉각 차단하거나 순위를 조정할 수 있음

SEO 오염 방지 및 차단 리스트 관리

     * SEO로 검색 품질이 낮아진 도메인을 적극적으로 차단하여, 불필요한 대기업 콘텐츠 필터링 가능
     * Kagi 사용 초기부터 직접 차단 도메인 리스트를 꾸준히 관리해왔음
     * Bobby Hiltz의 ‘Sneaky SEO Shenanigans Suck’ 게시글을 통해 주요 SEO 회사 16곳이 검색 결과 악화에 큰 영향을 끼치고 있음을 알게 됨
     * 해당 글의 uBlacklist용 필터 를 Kagi의 차단 도메인 목록에 추가하여, 더 불필요한 콘텐츠를 배제할 수 있게 개선함
     * 본인의 Kagi 차단 리스트를 별도 페이지로 공개, 사용자들도 복사해 쉽게 적용 가능

커스텀 렌즈 예시

     * Kagi 기본 ‘Small Web’ 렌즈는 스몰웹 레포지토리에 등록된 작은 사이트만 선별적으로 검색함
     * 나의 ‘Smaller Web’ 렌즈는 더 선택적으로 작은 웹 커뮤니티를 대상으로 결과를 좁힘
          + 예: leprd.space, bearblog.dev, smol.pub, pika.page, nekoweb.org, weblog.lol, dreamwidth.org, neocities.org 등
          + 단, 커스텀 도메인 사용 사이트 및 자체 호스팅 사이트는 포함하지 않음
          + 기본 Lense와 적절히 조합하여 더욱 정교한 검색 경험을 구축할 수 있음
     * ‘The Old Web’ 렌즈는 과거의 대표적 개인 사이트 호스팅 서비스만 대상으로 옛날 웹의 콘텐츠 탐색이 가능함
          + geocities.ws, tripod.com, fortunecity.ws, angelfire.com 등 레트로 호스팅 플랫폼 중심의 옛날 웹사이트
     * 위 두개의 렌즈는 Kagi 회원만 접근 가능하며, 해당 플랫폼 내에서만 동작

마치며

     * Kagi의 렌즈, 개인화, 도메인 관리 기능을 적극적으로 활용해, 자신만의 이상적인 검색 경험을 구축이 가능함
     * Kagi의 유료 정책과 커스터마이즈 옵션 덕분에, 광고·SEO에 오염되지 않은 진정한 정보 탐색이 가능해졌음

        Hacker News 의견

     * 나는 제품을 적극 추천하는 일에 항상 약간 거리낌을 느끼지만, 6개월 전 Kagi로 전환한 후 실제로 더 나은 검색 경험을 하고 있음, 거의 모든 경우에서 검색 결과가 Google보다 같거나 더 좋고, 점점 늘어나는 오해의 소지가 있는 광고를 내려가며 보지 않아도 되어 만족감을 느낌, 처음엔 원하는 결과가 바로 안 나오면 종종 Google 버튼을 눌러서 동일 쿼리를 실행해봤지만, 결국 거기서도 결과를 못 찾아서 Kagi에 대한 신뢰가 쌓임, 참고로 같은 방식으로 여러 번 DDG로 갈아타려다 실패한 적이 있어서 Kagi만큼은 정말 다름을 느낌
          + 입소문은 누군가 좋은 제품을 발견하는 가장 중요한 방법이라고 생각함, 과거에는 제빵소나 대장장이가 표지판보다 직접 만든 품질로 알려졌던 것처럼, 실제로 좋은 제품이고 사용자 경험(UX) 중심으로 운영되며 가격이 합리적이면 자신 있게 추천할 수 있음
          + Kagi를 유료로 사용해야 할지 확신이 없었으나, 작년에 Google, DDG, Bing 등에서 찾을 수 없던 밈 비디오를 Kagi에서는 거의 바로 찾을 수 있었을 때 확신이 생겼음, 다른 검색 엔진이 얼마나 안 좋아졌는지 새삼 깨달았음
          + 나도 네 말처럼 Kagi에 적응하는 데 시간이 좀 걸렸음, Kagi는 철자 실수에 더 민감해서 Google처럼 검색어를 ‘영감’ 정도로만 사용하지 않음, Google은 결과가 퍼지다 보니 모호함이 많은데 Kagi는 내가 넣은 만큼 돌려주는 느낌임, 가끔 원하는 걸 찾기가 조금 힘들긴 한데, 적어도 우리가 ‘상품’이 아니라는 점에서 충분한 가치로 느껴지고 이 점이 더 나은 거래라 생각함
          + 나도 Kagi로 전환한 후 AI assistant를 주로 사용하게 됨, 검색과 AI를 바로 연결할 수 있으니 거의 항상 즉시 원하는 답을 구할 수 있음, 정말 유용함
          + 아예 검색 결과에서 특정 사이트를 필터링하는 기능이 나에게는 킬러 피처였음, Pinterest, 여러 타블로이드나 음모론 사이트, 명백한 AI로 생성된 사이트 등은 전부 제외시키니 검색 결과 품질이 크게 향상됐음, 이런 기능을 다른 검색엔진도 기본 지원해 줬으면 좋겠음
     * Kagi는 나에게 Google을 완전히 대체했으나, 위치 기반의 ""내 주변 음식점"" 유형의 검색만큼은 아직 Google을 씀, 위치 결과와 리뷰는 회사 입장에서 정말 어려운 일임을 이해함, 그래도 Google이 이 부분만큼은 여전히 건재함, 참고로 Kagi Translate는 종종 Google보다 월등하다고 느낌
          + 정확성 비교가 어렵긴 한데, Kagi Translate는 Google보다 더 다양한 조정을 지원함(형식, 성별 등), 번역도 더 많이 제공되어 진짜 환상적인 제품임, 오히려 Kagi Search와 Google Search 차이보다 Kagi Translate와 Google Translate 차이가 더 크게 느껴짐
          + 10년 넘게 ""내 주변 음식점"" 유형 검색만은 Google을 사용함, Google이 여기에 천문학적인 비용과 시간을 투자한 덕분에 생긴 ‘해자’라고 생각함, 다른 경쟁사들이 이걸 따라잡기 쉽지 않고, 실제로 지금까지 현지화 검색에서 Google을 넘는 곳은 없어 보임
          + 여전히 음식점 찾기에는 Google Maps를 씀, 이 부분에서는 아직도 강력하고 Kagi Maps는 더 발전해야 함
     * 연결된 block list를 봤는데, 쓰레기 사이트도 많지만 좋은 콘텐츠를 가진 사이트도 꽤 있음, 나는 Kagi에서는 SEO 스팸이 검색 결과에 보이면 그때마다 차단함, 좋은 SEO가 무조건 안 좋은 사이트를 의미하는 건 아니라고 생각함, https://paste.flamedfury.com/flamedfury-kagi-block-list
          + 같은 반응이었음, 그의 블록리스트에는 washingtonpost.com, amazon.com, alternativeto.net, medium.com, twitter.com, msn.com 등도 포함돼 있음, 결국 본인 기준에서 귀찮거나 싫은 사이트가 다 포함된 듯, 포스트에서 논의된 16개 회사가 소유한 도메인 목록은 여기에서 확인 가능함
          + 나도 SEO 스팸 사이트는 강하게 차단함, 그런데 많은 사이트들이 좋은 콘텐츠, 양질의 SEO, 광고가 많아도 실제로는 유용한 경우도 많음, 좋은 콘텐츠가 있어도 광고가 너무 심해서 차단한 적이 있으니 나도 뭐라 할 입장은 아니지만, 이 리스트를 무작정 따라 쓴다면 꽤 괜찮은 정보원도 함께 걸러질 수 있음
     * 나는 2023년 1월에 얼리어답터로 가입해서 이후 한 번도 뒤돌아보지 않고 계속 월 요금제를 사용함, 현재 Ultimate 패키지(월 25달러)를 쓰고 있는데 가격 대비 가성비도 만족함, Kagi를 적극적으로 영업하고 싶지는 않으나 이건 누구에게든 한 번 경험해보라 권하고 싶음, 무료 티어도 있으니 직접 써볼 수 있고 5달러 티어만 해도 충분히 좋은 경험임, 예전 2012년쯤 Google처럼 모든 검색에 Kagi를 활용하고, 검색 엔진을 잘 활용하면 정말 못 찾는 게 거의 없을 정도임, Kagi가 Google이 되어야 했던 모습임, 물론 사소한 단점들이 있긴 하지만 전체적인 경험이 너무 좋아서 그런 아쉬운 점도 그냥 넘어갈 만함
     * 아직도 Kagi가 Yandex를 쓰는지 궁금함, 나는 2023년 초부터 Kagi에 만족했으나, 러시아 백엔드를 쓰며 전쟁을 지원한다는 사실을 알고 Kagi 사용을 중단함
          + Kagi는 여전히 Yandex와 파트너를 유지 중임, 원래는 사용 소스 리스트를 공개했었지만 삭제했음, 리스트 복원 문의에는 “이 요청의 특별한 이유가 있나요? 배경을 알려주시면 더 잘 이해할 수 있을 것 같습니다”라는 답변을 받았음, 관련 피드백 링크1, 관련 피드백 링크2
          + 요즘 전쟁 예산을 쓰지 않는 나라가 운영하는 서비스를 쓰고 싶다면 뭐든 사용하기 힘들 것임
          + Yandex는 사랑스러울 때도 있었고 아주 싫을 때도 있었음, 그렇다고 러시아와 관계된 모든 걸 단절하지는 않을 생각임, 점점 더 독립 지향 인덱싱을 하는 모습이니 완벽하진 않지만 다양한 인덱스가 더 많아지는 게 좋다고 봄
          + Kagi의 장점 중 상당부분이 Yandex 백엔드 덕분이 아닐까 생각됨, 예를 들어 ao3에서 텍스트 문자열을 검색하려면 Google, Bing, Brave, Qwant 등 어디도 결과를 안 주는데 Yandex 그리고 Kagi에서는 첫 결과로 바로 나옴
          + 해당 회사가 내 평생 동안 100년 동안 가장 많은 전쟁을 벌인 나라 기반이기도 함, 반드시 피해야 함
     * Kagi를 몇 달간 공정하게 사용해봤음, 특히 boost/block 리스트가 좋았음, 하지만 일상적으로 자주 사용하는 만큼 반응 속도가 너무 느려서 항상 불편했음, 여러 쿼리를 벤치마크해 보니 내 홈 네트워크 기준으로 일반 검색은 Google보다 3배, 이미지 검색은 5~10배 정도 느렸음, 다양한 원인이 있겠지만 다른 사람들은 이런 불평이 없는 걸 보면 내 네트워크가 문제일 수도 있음, 결국 일상 검색은 Google로 복귀하고, 특정 용도로만 Kagi를 씀
          + 나도 같은 문제를 겪었음, 일 또는 네트워크 문제로 생각했음, 가끔 Kagi 결과가 최대 5초까지도 걸림
          + 그런 현상은 특이함, support@kagi.com 에 연락해봤는지 궁금함
     * 몇 년 전만 해도 검색엔진에 월 10달러 내고 쓸 거라면 미쳤다고 했을 텐데, 실제로 돈값 충분히 한다고 느껴짐
     * 요즘 검색엔진은 나에겐 AI에게 밀리고 있음, Kagi는 Google보다 SEO 스팸이 적지만 아직 완전히 깨끗하지 않음, 검색엔진을 써서 질문에 답하려고 하면 너무 많은 정보가 있지만 정작 쓸만한 게 별로임, 반면 ChatGPT는 SEO 스팸을 다 걸러주고 내 질문에 대체로 잘 답해줌, 제공되는 참고자료를 따라가면 직접 검증하거나 더 배울 수도 있음, 그래서 사용자 입장에서 훨씬 좋은 경험이고 성공률도 높음, 내 Kagi 사용 통계를 보면 실제로 줄어든 건 아니지만 체감적으로는 정보를 찾을 때 Kagi를 ‘덜’ 의존하는 느낌임, 많은 용도에서는 아예 포기했거나 더는 첫 번째 선택이 아님, 지금은 bang 연산자(빠른 사이트 이동)용으로 쓸 때가 주 사용 케이스임
          + ChatGPT에도 언젠가는 광고가 추가될 예정임(관련 기사), 지금은 Google보다 스팸이 적긴 하지만 앞으로도 계속 그럴지는 확신할 수 없음, ChatGPT 외에도 대체재(예: Kagi의 assistant)들이 있으니 AI가 검색 시장점유율을 가져갈 수 있겠으나, 나는 여전히 주로 검색을 사용함, AI의 환각(hallucination)이 많아서 1차 원본을 직접 찾아 읽는 걸 더 선호함, 앞으로도 새로운 시도를 계속 해보려 함
          + Kagi에는 질문 끝에 물음표만 붙이면 쓸 수 있는 통합형 AI 검색 기능이 있음
          + !ai를 끝에 붙여서 쿼리를 assistant로 바로 보낼 수도 있음, kagi.com/assistant로 리디렉션되고 내 기본 모델이나 assistant를 사용함, 유료 사용자라면 ChatGPT나 gpt 4.1 mini 사용 가능함, 참고로 OpenAI assistant도 bing을 백엔드로 쓰니 훨씬 나은 경험임
          + 나는 Kagi Assistant를 주로 씀, 자체 인덱스에 여러 AI 기술(LM)들이 섞여 있음, 정말 잘 동작하고 Kagi 요금에 포함됨
     * DDG(duckduckgo)가 나에겐 계속 좋아서 g!도 거의 안 씀, 굳이 필요하지도 않은데 검색에 100달러를 쓸 이유가 없고 무료로도 충분히 잘 되는 서비스가 많다고 생각함
          + 1년에 광고 때문에 추가로 쓰게 되는 돈이 얼마인지, 1~2번째 광고 링크 클릭으로 자신도 모르게 더 비싸게 결제하는 금액이 얼마인지, 대부분 ‘0’이라 답하겠지만 실제론 알 수 없다고 봄, Google이 돈을 그렇게 많이 버는 이유가 어디에 있는지 생각해보면 우리로부터 직접 뽑아가는 돈이 분명히 있을 것임
     * blocklist 기능이 흥미로워 보여서 결국 앱과 확장 프로그램을 설치해 봄
"
"https://news.hada.io/topic?id=21984","Kiro: 새로운 에이전틱 IDE","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Kiro: 새로운 에이전틱 IDE

     * Kiro는 AI 에이전트 기반 개발을 위한 스펙 중심 IDE로 개발 초기 컨셉부터 실제 배포까지의 과정을 단순화함
     * 단일 프롬프트로 요구사항부터 설계, 구현 태스크까지 자동 생성하고, 테스트/접근성 등도 체크하여 프로덕션 레벨의 코드 관리를 쉽게 함
     * Mac, Windows, Linux 및 주요 언어를 지원하며, VS Code 플러그인/세팅 호환 및 무료 프리뷰 제공
     * 요구사항을 정리하는 사양(specs) 과 특정 이벤트 마다 AI가 자동 처리하는 후크(hooks) 중심의 워크플로우
     * 'vibe coding'(즉흥적 프로토타이핑) 뿐만 아니라, 명확한 사양화와 생산 시스템으로의 전환을 강점으로 내세움

사양(Specs)과 후크(Hooks)

     * Specs: 개발자가 요구사항을 구체적으로 문서화할 수 있도록 지원하며, AI가 이를 바탕으로 코드와 설계 문서, 데이터베이스 스키마, API 엔드포인트와 같은 기술 설계 산출물을 자동 생성함
          + 요구사항이 불분명할 때 명확한 기준을 잡아주고, 변경/확장 시에도 사양과 실제 코드가 동기화됨
     * Hooks: 숙련된 개발자가 빼먹기 쉬운 반복 작업을 대신 수행하거나 사소한 실수 예방 역할을 자동화
          + 파일 저장, 생성, 삭제 등 특정 이벤트 발생 시 지정한 작업을 에이전트가 백그라운드에서 자동 처리
          + 예를 들어 React 컴포넌트 저장 시 테스트 파일 최신화, API 엔드포인트 변경 시 README 갱신, 커밋 시 보안 검사 등 반복 작업을 자동화함
          + 팀 전체의 코드 품질, 보안, 표준화를 일관적으로 유지

Specs와 Hooks 기반 개발 과정

     * 예시: 이커머스 앱에 리뷰 시스템 추가
          +
              1. 단일 프롬프트 입력: ""제품에 리뷰 시스템 추가"" → AI가 EARS(명확한 요구사항 표기법) 기반의 유저 스토리/엣지 케이스 포함 명세 자동 생성
          +
              2. 기술 설계: 사양에 맞춘 데이터플로우 다이어그램, TypeScript 인터페이스, DB 스키마, API 등 설계 자동 생성
          +
              3. 구현 태스크 생성: 태스크/서브태스크를 의존성 순서로 자동 정렬, 단위/통합테스트, 로딩상태, 모바일/접근성까지 체크
          +
              4. Hooks로 품질 보장: 저장 시 테스트 코드 갱신, 커밋 전 보안 스캔 등 반복 체크 자동화

특징 및 기타 기능

     * AI 코드 에디터로서, MCP(Model Context Protocol) 지원, AI 행동 가이드, 파일/URL/문서 컨텍스트 프로바이더, 에이전트 채팅 등 다양한 생산성 기능 탑재
     * VS Code 세팅, Open VSX 플러그인과 호환되므로 기존 개발환경과 연동 용이
     * 사양(Specs)과 코드가 항상 동기화되어 문서화 누락/유지보수 문제 해결

비전과 미래 방향

     * 팀 간 설계 일관성 확보, 요구사항 충돌 해소, 코드 리뷰 자동화, 기술 부채 관리, 지식 이전 등 개발 난제 해결을 목표로 함
     * 사양 중심 개발로 사람과 AI가 협력하는 새로운 소프트웨어 개발 방식을 지향

시작 방법 및 지원 사항

     * Mac, Windows, Linux, 다양한 언어 지원, Google/GitHub 등으로 로그인 가능
     * 실습형 튜토리얼로 스펙 작성부터 배포까지의 전체 개발 흐름 경험 가능
     * 현재 미리보기 기간 동안 무료 제공 (일부 제한 있음)

        Hacker News 의견

     * Kiro는 코드베이스와 승인된 요구사항 명세를 분석해 설계 문서를 자동 생성함, 데이터 플로우 다이어그램, TypeScript 인터페이스, DB 스키마, API 엔드포인트 등까지 마련해줌, 이런 방식은 개발 과정에서 요구사항 명확화로 인한 시간 낭비를 줄여줌, 문서화에는 상당히 유용하지만, 실제 설계란 코딩 전에 결정이 이루어져야 의미가 있음
          + 나는 요구사항과 코드 작성 사이 단계에서 설계 문서를 만든다는 뜻으로 이해함, 실존하는 코드베이스를 읽는다는 점에서 그린필드가 아닌 프로젝트에도 알맞게 작동함
          + 아이젠하워의 ""계획은 쓸모없으나, 계획하는 과정이 전부임""이라는 명언이 떠오름
     * FAQ에서 중요한 내용을 요약함: Pro 또는 Pro+ 요금제를 사용하는 경우, 사용자의 컨텐츠(코드, 대화, 파일 등)는 기본 제공 AI 모델의 학습 데이터로 사용되지 않음, AWS는 클라이언트 측 텔레메트리 및 사용성 데이터를 서비스 개선 목적으로 수집할 수 있으나, IDE 설정에서 데이터 수집을 비활성화할 수 있음, Free 버전 및 미리보기에서는 명시적 옵트아웃 하지 않으면 컨텐츠가 모델 품질 향상에 활용될 수 있음, 자세한 내용은 FAQ 링크 참고
          + 이 정책을 보면, 기업 입장에선 이런 컨텍스트(입력 데이터)가 가치 있다고 보는 듯함, 보통은 LLM이 생성한 코드의 중간 품질이 사람 만든 코드보다 낮다고 느꼈기 때문에, 입력된 코드가 필터링 없이 학습 데이터로 쓰인다면 실제로 가치 있는지 의문이 듦, 불량 데이터를 제외하는 후처리가 필수라 생각함, 컨텍스트 데이터는 학습 외의 다른 용도론 쓸모가 있음
          + Kiro에서 텔레메트리 데이터 공유를 비활성화하려면, Settings에서 User 섭탭으로 이동 후 Application → Telemetry and Content 설정값을 Disabled로 바꾸면 됨, 공식 가이드에도 순서가 안내되어 있음
     * 1년 가까이 Kiro 개발에 참여 중임, Kiro가 기존 AI 기반 에디터와 차별화되는 기능을 제공함에 자부심을 느낌, 특히 “spec driven development”에 자신 있음, Amazon 엔지니어링 프로세스를 바탕으로 간단한 프롬프트에서 기술 요구사항, 설계 문서(다이어그램 포함), 작업 목록까지 생성할 수 있음, Kiro를 통한 코딩도 재미있었고, 직접 빌드한 프로젝트 샘플도 공개함, 이 샘플은 거의 95% AI가 코딩한 무한 제작 게임 spirit-of-kiro임
          + 샘플 프로젝트에 CHALLENGE.md, ROADMAP.md 문서까지 포함된 점이 멋짐, 실사용자가 직접 시도해보기 좋은 프레임워크로 생각함, 단지 AWS를 반드시 사용해야 한다는 점은 아쉬움, 예를 들어, ""Kiro에게 로컬 DB와 내 Anthropic API Key만 써달라고 시킬 수 있음?""처럼 agentic coding이 있다면 더 좋겠음, AWS 관련 불만은 차치하고, 이런 실습 환경 데모는 정말 좋은 아이디어임
          + 95% AI 코드라 언급한 점에서, 사실상 코드 결과물에 대한 기대치가 그리 높지 않았기 때문일 수도 있음, 결과물이 대충 괜찮아 보이면 된다는 태도였을 것 같음
          + 내 피드백이 Kiro 자체를 향한 것인지, 아니면 전체 코딩 에이전트 분야에 대한 것인지는 모르겠으나, 나를 가장 망설이게 하는 이유는 이미 내가 다른 제품(Copilot, Continue, Cursor 등등)에서 세팅해둔 커스텀 룰 때문임, 방금도 다시 Claude Code로 나의 설정을 옮겼는데, 이런 수고는 반복하고 싶지 않음, 기업들은 외부 앱에서 설정을 자동으로 가져오거나 변환해주는 온보딩 플로우를 잘 만들어야 많은 유저를 끌 수 있음
          + Harper Reed가 만든 ""My LLM codegen workflow atm""과 유사한 건지 궁금함, 관련 워크플로우 링크
          + 왜 에디터를 만들었는지, CLI 버전이 더 나을 것 같았음, 개인적으로 VSCode가 느려서 CLI 선호함
     * 나는 6개월마다 새로운 에디터/IDE로 옮기고 싶지 않음, 새로운 키 바인딩, 완전히 다른 UI에 적응하는 것 자체가 피곤함, AI 도구 분야는 속도감 있게 발전하기 때문에 머지않아 더 좋은 대체제가 나타날 수밖에 없음, 이미 Cursor, Windsurf에서도 그런 현상이 보임, 현재는 Claude Code가 인기를 얻는 것 같고, CLI나 TUI 방식이 내 스타일에 더 맞음, 물론, 터미널 사용자를 넘는 더 많은 GUI 애플리케이션 사용자층도 있음
          + 전부 vscode 기반이라서 아직까지는 에디터를 바꿀 때 진입장벽이 낮음, 앞으로 달라지겠지만, 지금은 서로 거의 비슷한 사용 경험임, Cursor가 VSCode의 업데이트를 안 따라가긴 하지만 현재로선 사소한 수준임
          + 난 Aider의 ‘IDE 통합’ 방식을 정착시킴, 원하는 에디터에서 쓸 수 있고, AI 백엔드도 내 마음대로 선택 가능, 공급자 락인(벤더 종속)은 불합리하다고 생각함, 탭 컴플리션을 못 쓰긴 하지만, 평소엔 일반적인 언어 서버 기반 탭 컴플리션이 더 좋음, AI가 필요하면 명시적으로 도구를 호출함, 사용환경에 따라 다르겠지만 내 업무엔 잘 맞음, 그리고 Claude, Gemini, Deepseek, Qwen, Kimi 등 여러 모델을 자유롭게 바꾸면서 모자란 부분을 서로 보완할 수 있는 점도 큼
          + vim과 내 두뇌를 절대 포기할 생각 없음, IDE뿐 아니라 자신이 의존할 머신러닝 모델까지 사는 느낌, 동료들은 도구에 더 의존해 사고력이 줄고 있음, 그야말로 이상한 시대임, 모델 서비스가 없어지면 어떻게 될지 궁금함
          + 6개월마다 에디터 바꾸는 거 아닌가? 이제 2주마다 바꿔야 하는 수준임, Claude Code 사용 만족함
          + 이런 agentic IDE들은 VSCode 플러그인 형태로도 나올 수 있었음, 플러그인이면 VC 투자 유치가 어려워서 별도 제품으로 출시한 듯함
     * 명세 기반 개발에 집중한 점이 인상적임, 나도 Claude Code로 명세 텍스트를 유지하고 있고, 리드미나 아키텍처 다이어그램(Markdown/mermaid)도 항상 같이 관리함, 오히려 코드 생성보다 이런 문서화가 더 중요하다고 느낌, Kiro가 이 부분에서 특별히 더 뛰어난 장점이 있는지 궁금함, 혹시 더 좋은 DSL 등으로 다이어그램이나 계획을 쉽게, 더 잘 만들 수 있다면 좋겠음, 이미 내 워크플로우에 익숙해서 새로운 도구 도입이 어렵게 느껴짐, 또 가격 정책도 궁금함, 단순 Bedrock 가격에 부가 가치가 붙는 건지 궁금함
          + Kiro가 이 방식을 더 잘 만들도록 도와주는 특별한 포인트가 있다고 믿음, 직접 써보면 결과물 비교가 가능할 것, 나 자신은 명세와 SW 설계 문서, mermaid 다이어그램 생성 포맷을 좋아함, 가격은 월 정액제이고, 월별로 상호작용 횟수 제한이 있음, ‘사용자 주도’ 상호작용 1회마다 제한량이 소비되지만 한 번의 상호작용으로 많은 자동 코드 생산이 이루어짐, 자세한 정보는 가격 안내 링크 참고
     * AI와 인간이 협력해 결과물을 만드는 실험이 매우 흥미로움, 인간이 높은 수준의 구상에 집중하면 AI가 저수준 작업을 분담하는 패턴임, 요구사항→명세→코드로 내려가는 계층적 구조가 명확함, 각 단계를 별도 문서(Markdown 등)로 관리하면 각 레벨에서 독립적 리뷰가 가능함, 이런 구조적 의사결정 방식은 코딩에 아주 효과적일 것 같음, 만약 이 모델이 확립되면, 앞으로 법률, 의료, 보험 등 타 영역에도 확장될 수 있음, 소프트웨어는 빙산의 일각이고, 이 패턴이 성공한다면 수많은 스타트업이 생길 수 있음, 관건은 여러 수준의 문서, 추상화, 그리고 리뷰 프로세스를 효율적으로 관리하는 것임, 충분히 해결 가능한 문제라고 생각함
          + 계층별 문서를 구조화해서 까지 가져간다면, 결국엔 컴퓨터에게 시키고자 하는 바를 일관된 문법과 정형화된 언어로 명확하게 전달하는 완전한 ‘프로그래밍 언어’처럼 발전할 수도 있음, 그 자체로 엄청난 변화가 될 수 있음
     * 중요한 사실로, 또 하나의 VSCode 포크임, AI가 혁신의 시대를 연다고 들었지만, 실제론 기존 것을 계속 복제만 하는 게 아닌지 아쉬움, 지금 AI는 도리어 혁신의 시대를 종식시키는 방향 같음, 모두가 복제에만 치중하고 있음
     * 이게 Amazon 제품인지 궁금함, ‘Legal’ 탭 누르면 AWS로 이동함, 공식문서에 ""Kiro는 AWS의 독립 에이전트 IDE""라 쓰여 있음, 그런데 홈페이지의 어디에도 Amazon 소속이라는 표시는 없음, 저자인 @nathanpeck이 Amazon 임직원임을 감안할 때 정보를 숨기는 것 아니냐는 의심이 듦
          + AWS는 Amazon Web Services임을 모르나?
          + 나는 원래 제목에 AWS 제품임을 명시했으나, Hacker News 측에서 제목을 바꿔서 명확성이 떨어져버림
          + 숨기는 의도는 없는 것 같음, About 페이지에 ""Kiro는 AWS 내 소수 팀이 빌드함""이라고 공식적으로 언급되어 있음, 참고로 AWS에 근무 중이나 다른 소속임
     * 최근 IDE가 개발자 시장 진입 경로로 각광받고 있음, 예전엔 Atom이 차세대 IDE 트렌드를 열었고, VSCode가 시장표준이 됨, AI 시대에는 사용자 확보와 데이터 수집, 모델 포지셔닝이 중요한데, IDE가 이를 전부 제공함, Kiro도 이런 트렌드에 합류한 프로젝트임, (AWS의 Kiro, Microsoft의 VSCode+Copilot, OpenAI의 Windsurf, Cursor, Alibaba의 Trae, Zed 등), Zed도 예전엔 분류가 달랐지만 요즘은 AI 에이전트를 갖춘 동등한 도구로 봄, 시장이 너무 포화되어 있고, Claude Code 같은 툴이 사용자에게 대안을 제시함, 나도 Cursor 써봤다가 다시 Helix/VSCode+Claude Code로 돌아옴
          + 난 Zed의 기본 에디터 성능을 매우 좋아함, AI 기능도 무난함, 최근엔 TUI 쪽 AI 실험도 늘리는 중임, 한편으로는 Zed가 AI 트렌드에 너무 휩쓸려 프로젝트가 복잡해질까 걱정임, 똑똑한 사람들이 잘 균형을 잡아주리라 믿음
          + Windsurf는 OpenAI가 인수 시도했지만, 결국 Google이 서비스 종료시킴
     * Amazon은 이미 Claude Code와 유사한 Agentic 코딩 CLI 제품을 공개함, Q Developer CLI 링크, 무료 티어도 괜찮고, 유료 구독이 Claude Code보다 나을 수도 있음, MCP도 지원함, Q는 VSCode, IntelliJ 플러그인도 있으나, Kiro는 plugin 수준을 넘어 Cursor가 VSCode를 포크한 것과 유사하게 더 많은 기능을 제공함
          + Q CLI는 거의 Claude 모델을 무제한 쓸 수 있는데 월 $20임, Claude Code보단 다듬어짐이 아쉽지만 가격 차이가 큼
          + 다만 Q CLI는 셸 프로필에 불필요한 구문을 너무 많이 주입해서 터미널이 느려지고, 표준 스트리밍 서버도 지원하지 않는 등 문제가 있음
"
"https://news.hada.io/topic?id=21993","자바스크립트에서 일반 함수와 Arrow 함수의 차이점, 언제 어떤 문법을 써야 하는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            자바스크립트에서 일반 함수와 Arrow 함수의 차이점, 언제 어떤 문법을 써야 하는가

     * 자바스크립트 함수 선언은 function 키워드를 사용한 선언, 함수 표현식, 화살표 함수 등 다양한 방식이 존재함
     * 함수 선언은 호이스팅이 적용되어 코드의 어느 위치에서든 참조 가능함
     * Arrow(화살표) 함수는 간결한 문법이 장점이지만, this/arguments/super 바인딩이 없음 등 중요한 차이점이 있음
     * 생성자 함수, 제너레이터, 메서드에는 화살표 함수 사용이 적합하지 않음
     * 간단한 콜백이나 익명 함수에는 화살표 함수가 더 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Function Declarations, Function Expressions, and Arrow Functions

     * 자바스크립트에서는 함수 선언(Statement), 함수 표현식(Expression), 화살표 함수(Arrow Function) 세 가지 방식으로 함수를 정의할 수 있음
     * 함수 선언은 function isVowel(chr) { ... }처럼 이름을 직접 바인딩하며, 코드의 어디서든 참조 가능(호이스팅) 함. 스택 트레이스 및 디버깅 시 함수명이 명확하게 표시됨
     * 함수 표현식은 const takeWhile = function(predicate, arr) { ... }처럼 변수에 익명 함수를 할당하는 형태
     * 함수 표현식에도 내부적으로 이름을 줄 수 있으나, 이름은 외부 스코프에 바인딩되지 않고, 주로 스택 트레이스에서 에러 추적에 사용됨

Hoisting and Naming

     * 함수 선언문은 자바스크립트 엔진에 의해 호이스팅되어, 선언 이전에 호출해도 동작함
     * 익명 함수 표현식은 변수 할당 이후에만 호출 가능함
     * 디버깅을 위해 함수에 이름을 명시적으로 부여하는 것이 스택 트레이스에서 유리할 수 있음

Arrow Functions

     * 짧고 간결한 문법: function 키워드 없이 (매개변수) => { ... } 형태로 작성.
     * 항상 익명 함수임 (단, 변수에 할당해서 이름처럼 사용 가능)
     * 표현식(expression)으로만 사용 가능, 문(statement) 이 아님
     * this/arguments/super 바인딩 없음: 함수 선언/표현식과는 다르게, 상위 스코프의 this를 캡처함
     * 단일 표현식의 경우 중괄호와 return 생략 가능, 매개변수가 1개면 괄호도 생략 가능함
     * 생성자 사용 불가: 화살표 함수는 new 키워드로 호출할 수 없으며, 생성자 함수로 동작하지 않음
     * 제너레이터 불가: yield 사용 불가, 제너레이터 함수로 만들 수 없음
     * 코드 예시:
const sum = (a, b) => a + b;
const square = x => x * x;

Practical Example: this와 생성자, 제너레이터

     * 일반 함수와 화살표 함수의 this 처리 방식 차이 예시 제공
          + 객체 내에서 메서드로 쓸 때, 일반 함수는 this가 객체 자신을 가리키지만, 화살표 함수는 undefined 또는 외부 스코프의 this를 가리킴
     * 생성자 함수를 화살표 함수로 정의 시 TypeError 발생
     * 제너레이터 함수는 반드시 function* 문법 사용 필요

어떤 함수 문법을 언제 선택해야 할까?

     * 제너레이터(yield 사용) 필요 → function* 사용
     * this를 활용해야 함 → function 키워드 또는 클래스 메서드 사용
     * 호이스팅이 필요하거나 상위 수준의 가독성을 원함 → 함수 선언문 사용
     * 위 조건 해당 없으면 → 화살표 함수로 더 간결하게 작성하는 것이 유리

결론

     * 자바스크립트 함수는 사용 목적, this 필요 여부, 생성자/제너레이터 여부에 따라 문법 선택
     * 일상적 콜백/간단 함수에는 화살표 함수가 베스트
     * 객체 메서드/생성자/제너레이터에는 function 문법 사용 필요
     * 호이스팅이나 선언 순서 자유도가 필요하면 함수 선언문이 유리

   근본만큼이나 prototype 유무도 ...
   생성되는 고계도 함수의 레퍼런스 방법도 ...

   const a = (a: () => null): (() => () => null) =>() => a

   () => ❤️
"
"https://news.hada.io/topic?id=22043","Mistral AI, Le Chat에 Deep Research, Voice, Project등을 추가 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Mistral AI, Le Chat에 Deep Research, Voice, Project등을 추가

     * Mistral AI는 Le Chat에 새로운 기능 5가지를 추가해 사용자의 탐색, 표현, 정리를 더욱 자연스럽고 강력하게 만들었음
     * Deep Research 모드는 복잡한 질문을 구조화된 리서치 리포트로 빠르게 정리해주는 기능을 제공함
     * 음성 모드는 새로운 음성 모델 Voxtral을 통해 자연스럽고 빠른 대화가 가능하며, 타이핑 없이도 사용 가능함
     * Magistral 기반의 다국어 추론 기능으로 다양한 언어에서 명확하고 깊이 있는 응답을 생성하며, 문장 내 언어 전환도 가능함
     * Projects 기능은 대화, 문서, 아이디어를 프로젝트 단위로 정리하고 기억하여 장기적 작업에도 적합함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

What’s new in Le Chat.

  1. Deep Research mode

     * 복잡한 주제를 구조화된 리포트 형식으로 빠르게 리서치해주는 기능
     * 사용자의 질문을 세분화하고, 신뢰할 수 있는 출처를 찾아 정리된 보고서로 생성
     * 도구 기반의 Deep Research agent가 사용되지만 사용자 경험은 단순하고 투명하며 협업하는 느낌을 줌

  2. Voice mode

     * Voxtral이라는 새로운 음성 모델을 이용해 자연스러운 음성 대화가 가능
     * 산책 중 아이디어 브레인스토밍, 외출 중 빠른 질문 응답, 회의 내용 받아쓰기 등 다양한 상황에 적합
     * 낮은 지연시간으로 사용자 말 속도를 따라감

  3. Natively multilingual reasoning

     * Magistral 추론 모델을 기반으로 다양한 언어에서 복잡한 질문에 명료하게 응답
     * 스페인어로 제안서 초안 작성, 일본어 법률 개념 설명 등 다국어 활용에 유리
     * 문장 내 언어 전환(코드 스위칭)도 가능

  4. Projects

     * 관련 대화를 맥락 중심의 폴더 형태로 정리
     * 각 프로젝트는 고유의 라이브러리와 도구 설정 상태를 기억
     * 문서 업로드, 라이브러리 내용 불러오기, 대화 및 아이디어와 함께 일관된 작업 환경 유지
     * 이사 계획, 제품 기능 설계, 장기 프로젝트 관리 등에 유용

  5. Advanced image editing

     * 일반적인 텍스트-이미지 생성 도구와 달리, 생성된 이미지도 직접 편집 가능
     * 예: “객체 제거”, “다른 도시로 이동” 등의 프롬프트로 장면 수정 가능
     * 인물, 사물, 디자인 요소 등을 일관성 있게 유지하면서 시리즈로 편집 가능

        Hacker News 의견

     * 이미지 편집 기능이 정말 뛰어난 것 같음, 오히려 그게 핵심임에도 잘 드러나지 않는 느낌임. OpenAI 모델은 이미지를 전체적으로 바꾸다가 쿼리와 무관한 부분의 디테일까지 손상시키는데, 이 모델은 쿼리와 관련 없는 부분은 완벽하게 보존하면서 원하는 편집만 아주 잘 적용하는 점이 인상적임. 다만, 출력 해상도가 아쉬운 수준임(입력 이미지는 훨씬 컸는데 결과 이미지는 1184px임). 집 오피스 사진을 업로드해서 ""아래쪽의 약간 찢어진 회색 패널들을 완전히 새것처럼 복원해 달라""고 요청해봤는데, 결과물이 상당히 훌륭했음. 출력 품질이 원본보단 아주 미세하게 떨어지지만, 이 부분도 곧 개선될 거라 기대함
          + 입력 이미지: https://i.imgur.com/t0WCKAu.jpeg
          + 결과 이미지: https://i.imgur.com/xb99lmC.png
          + 이런 기술이 Craigslist에 큰 타격을 줄 것 같음. 예를 들어, 사진으론 멀쩡한 차인 줄 알고 방문했는데, 도착해보니 펜더가 찌그러져 있고, 보닛엔 구멍, 헤드라이트도 깨져 있음.
            부동산 중개인이 AI로 주택 사진을 완전히 새롭게 연출한 사례가 있었음(노후된 집을 신축처럼 보이게). 사람들이 실제로 방문하고 크게 분노했음. 중개인은 이게 연출의 한 단계라 주장했지만, 이 경우엔 전혀 먹히지 않았음. 결국 매물에서 내렸고, 많은 사람들이 수리하러 오기도 했음(가족일 가능성이 높지만 확실치는 않음)
          + 참고로, 올려준 입력 및 결과 이미지 링크가 같음. 처음에 내가 사진 차이 찾아보려다가 혼란스러웠음
          + 특정 작업엔 Kontext가 아마 더 뛰어나고, 아마 Mistral이 이걸 이용하는 듯함. 무엇보다 빠르고 저렴함.
            그런데 OpenAI도 어제 더 고화질 이미지 편집 기능을 새로 추가함. 이 기능이 API에만 적용된 건지, 채팅 UI에도 적용될지 아직 모르겠음. 동일한 프롬프트와 입력 이미지 결과: https://i.imgur.com/w5Q0UQm.png
          + OpenAI의 새 소식: https://x.com/OpenAIDevs/status/1945538534884135132
          + Black Forest Labs의 Flux Kontext를 사용하고 있고, 정말 뛰어난 모델임
          + 이미지 결과에서 책 제목이 흐트러짐
     * 드디어 EU가 깨어난 느낌임. 이 점이 자랑스러움. OpenAI와의 계약이 끝나자마자 바로 Mistral로 옮길 계획임. 유럽을 지지해야 함, Viva La France
          + 사실 나는 Mistral Large 3만 기다리는 중임. 이미 암시가 있었고, 새로운 Le Chat의 기본 언어 모델로 곧 적용될 것 같음.
            5월 Mistral Medium 3 블로그 포스트의 ""One more thing""에서 이런 내용이 있었음:

     3월 Mistral Small, 오늘 Medium, 곧 'large' 출시 준비 중. Medium 모델이 이미 다른 오픈소스 대표 모델(Llama 4 Maverick 등)들보다 확연히 뛰어나기에 다음 단계가 기대됨
     이 버전이면 이제 최고의 대형 모델들과의 격차도 더이상 큰 의미가 없을 것 같음. Cerebras의 빠른 속도는 ChatGPT와 비교해도 정말 뛰어난 사용자 경험임
     * MRF, 즉 Model Release Fatigue(모델 릴리즈 피로감)에 시달리고 있음. 거대 모델들이 엄청나게 자주 나오니 계속 IDE에서 모델만 바꿔보고 이전까지 잘됐던 걸 다시 켜면 이젠 안좋아보임.
       Claude 4, gpt, llama, Gemini 2.5, pro-mini, mistral… 맨날 갈아타다 보면 계속 머리가 어지러워지는 느낌임
       LLM 모델 전환으로 인한 피로감임
          + 네가 느낌을 가지는 건 이해하지만, 이렇게 다양한 선택지가 계속 등장하는 게 정말 좋다고 생각함. 혁신 속도도 훌륭함. 항상 최고의 모델만 쓰고 싶다면 힘든 여정이겠지만, 정체나 독점보단 훨씬 낫다고 생각함
          + 그래서 이런 신기술들을 거의 안 써보는 중임(재밌긴 하지만). 2026년 하반기쯤 직접 써볼 생각임. 그 때쯤이면 로컬 모델과 하드웨어가 개발되어 있을 테니.
            지금 실험 버전들을 감당하는 이들에게 경의 표함
          + 이런 경쟁이 오히려 정말 좋은 점임. 나는 항상 프리미엄 모델들만 쓰는데, 거의 돈을 안 씀. 프로모션이나 거의 공짜에 가까운 기회들이 늘 있음
          + 굳이 따라가지 않아도 됨. 자기에게 잘 맞는 모델 하나만 꾸준히 쓰면 충분함
          + 이전 버전 성능이 떨어지는 건 서비스 제공자가 리소스를 신버전에 집중해서임. 또 이전 버전 훈련 데이터 컷오프 영향도 있음(예: claude sonnet 3.5→3.7).
            나 개인적으로는 Claude/Anthropic만 씀. 더 잘 이해하기 때문임. 충분히 똑똑해서 최신 버전을 굳이 쓸 필요가 적음
     * Voxtral 출시가 흥미로웠던 이유는 경쟁력 있는 오픈소스 오디오 트랜스크립션(transcription, 음성→텍스트)이 다시 활발해졌기 때문임. 굳이 LLM 백본이 필요했는지 의문이긴 하지만 어쨌든 접근 방식이 흥미로움
          + 실제로는 강력한 오픈소스 음성인식(STT) 모델들이 훨씬 많음.
            Mistral의 보도자료에는 Whisper 이후 최고인 것처럼 느끼게 하지만, 실제 비교 대상은 상위권이 아님.
            오픈 벤치마크: https://huggingface.co/spaces/hf-audio/open_asr_leaderboard
            참고로 Mistral이 비교한 Scribe는 10위임.
            영어 벤치마크이긴 하지만 다국어 모델도 많으니 참고하면 좋음(ex. https://huggingface.co/nvidia/canary-1b-flash)
     * 이제는 오픈코드나 오픈웨이트 보다 오픈(윤리적으로도 검증 가능한) 데이터 모델에 훨씬 더 관심이 큼.
       예를 들어 내가 지정한 리소스가 학습 데이터에 들어갔는지 아닌지를 알려주는 모델을 쓰고 싶음
     * 요즘 AI 업계는 OpenAI의 서비스만 복제할 뿐이라는 인상을 지울 수 없음.
       다른 회사 서비스들도 거의 구조만 다를 뿐 같은 서비스임.
       혁신 자체도 실제로는 그리 높지 않음
          + 실제로 써보면 전혀 같지 않음. 코딩 같은 일상 작업에선 모델별 차이가 매우 큼
          + 전 세계가 이제 f(input: string): string 함수 기반 위에 새로운 서비스 쌓는 느낌임. 비슷해질 수밖에 없음
          + OpenAI도 Deep Research 기능을 Google에서 가져온 것임. 같은 이름을 썼고, Mistral도 마찬가지임
          + 이게 오히려 건강한 시장 경쟁임. 몇십년간 혁신을 지속한 Apple 같은 사례는 독점적 게이트키핑의 산물임
          + 결국은 거의 같은 기술이 두루 적용되고 있음. 학습 데이터와 연산 파워 정도의 차이일 뿐임
     * ChatGPT를 매우 많이 쓰고 있음. LeChat도 한번 써보려고 하는데, 큰 차이가 있을지 궁금함, 아니면 거의 비슷한지 알고 싶음
     * 아직 OpenAI의 Deep Research 기능을 안 써봤다면 꼭 써보길 추천함. 대안으로 쓸만한 서비스는 아직 못 찾음. 구글의 것도 써봤지만 크게 인상적이지 않았음.
       엔지니어들이 트레이드오프 연구를 할 때 엄청난 시간 단축 효과를 볼 수 있음
          + Anthropic의 Research 기능도 꽤 좋음. OpenAI 수준이라고 생각함.
            구글은 유료 버전이 조금 더 정확하지만, 결과 리포트가 지나치게 장황해서 읽기 불편함. 마치 단어 수 맞추려고 내용 늘리는 대학생 레포트 느낌임
          + 나는 특히 시장조사에 큰 도움을 받았음(창업 관련). 마치 똑똑한 신입 기획자/PM 어시스턴트를 고용한 느낌임
          + Kimi 2의 리서치 기능도 써보길 권장함. 생각보다 결과가 훌륭해서 놀랐음
          + OpenAI와 Gemini의 결과는 꽤 다르게 나옴. 어느 쪽이 더 낫다고 할 수 없고, 그냥 확연히 다름
          + Perplexities도 나쁘지 않음. 다만, OpenAI 유료 구독이 없어 직접 비교는 못해봄
     * 프롬프트 예시들이 별로임. 예를 들어 개인계획 관련 답변은 Deep Research 없이 곧장 답하는 쪽이 오히려 훨씬 나음(비자 항목만 제대로 답함)
     * Voxtral이 혹시 Futo Android 키보드에 적용될 수 있을지 궁금함
"
"https://news.hada.io/topic?id=21992","Refine - Grammarly의 로컬 대체제","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Refine - Grammarly의 로컬 대체제

     * Refine은 Grammarly와 유사한 기능을 제공하는 로컬 기반 문법 검사 도구임
     * 구독료 없이 한 번 구매로 추가 비용이나 숨겨진 수수료 없이 Refine 전체 기능을 평생 사용 가능함
     * 클라우드 기반 서비스와 달리, Refine는 데이터 전송 없이 로컬에서만 작동해 보안성이 높음
     * 장기적으로 반복 과금에 대한 부담을 낮추면서 투명한 소비 경험을 제공

        Hacker News 의견

     * 스크린샷에서 (수정된) 예문이 보임: ""Sometimes I still make mistakes with articles and prepositions, but my grammar is getting better every day I practice""라는 문장이 있음. 미국식/간소화된 영어에서는 이 문장이 문법적으로 맞음. 하지만 'full fat' 영국식 영어에서는 practice는 명사이고, practise가 동사임. 예를 들면 ""I go to my practice to practise medicine""처럼 사용함. 이 웹사이트의 문제는 부가적인 이슈에만 집중한다는 점임. 제품이 내 프라이버시를 존중함, 성능이 좋음, 인터넷 연결을 요구하지 않음, 다양한 앱에서 작동함, 투명한 가격임, 이 모든 점들은 좋음. 그렇지만 내가 정말 중요하게 생각하는 것은 이 제품이 항상 올바른 문법을 적용하느냐임. 이 웹사이트는 그것을 전혀 설득하지 못함
          + 그래서 결론적으로, 이 도구는 American English 쪽에 치우쳐 있어서 practise 같은 비표준 단어는 colour처럼 철자 오류로 간주함. 만약 미국인들을 위한 글에서 이런 단어를 쓰고, 당신이 영국연방 출신이 아니라면, 오히려 오만하게 보일 수 있음
          + American English를 ""간소화된(simplified)"" 영어라 부르는 것은 전형적인 잘못된 언어학임. 단순히 다른 방언일 뿐, 본질적으로 더 단순하거나 복잡한 게 아님
          + 설명 잘 이해하겠음. ""full fat"" 영어라는 표현과 관련해서, 영어 자체가 원래 뒤죽박죽이고, 새로운 국가에서 널리 쓰이면서 점점 더 복잡해짐. 글이 잘 쓰이고, 명확하게 이해될 수 있다면 $15짜리 제품으로는 충분하다고 생각함. 이 제품의 목적이 contemporary Oxford English를 완벽하게 따르는 게 아니라고 생각함
          + 예전에 anthony_franco가 남긴 아주 좋은 댓글이 생각남. 많은 오픈소스 대체 솔루션들이 “컨텐츠 제공자""만을 위해 문제를 푸는 데 집중한다는 점임. 실제로 사용자는 컨텐츠 그 자체에 관심을 갖고 있음. Product Hunt 대항 프로젝트였던 OpenHunt도 마찬가지였음. 여러 서비스의 “자유로운, 오픈"" 버전 만들기 시도가 있지만, 대다수 사용자는 사실 별로 신경 쓰지 않음. voat(Reddit 대체), app.net(Twitter), Diaspora(Facebook), ycreject.com(Y Combinator 대체) 등 모두 실사용자 니즈와 경험에서 부족했음. 만약 무료와 오픈에 더해서 대체품보다 더 좋은 경험을 제공한다면 대성할 수 있음. 하지만 대부분 그 부분에서 실패함 관련 코멘트
          + 예를 들면 “Is there a educational discount?”라는 문장이 있는데, “an educational discount”가 맞음. 이 도구가 이런 오류를 제대로 잡아낼 수 있을지 궁금함
     * 다른 제품인 LanguageTool이나 Harper와의 비교 경험이 있는지 궁금함. LanguageTool은 로컬 호스팅이 가능하고, Harper는 확장 프로그램으로 동작함. 철자나 문법 검사 수준이 비교적 궁금함
          + 두 솔루션 모두 FOSS(오픈소스 무료 소프트웨어)임. 참고로 LanguageTool과 Harper 링크 남김
          + 나는 수년째 LanguageTool Server를 ngrams 옵션과 함께 사용하고 있음. ngrams를 쓰면 정말 뛰어나고, 없으면 그냥 그저 그럼. 영어 전용 ngrams 데이터는 15GB 정도의 저장공간이 필요함. Windows 예약 작업 기능으로 아래와 같은 배치 파일로 시작시키면 됨
SET PATH=SET PATH=C:\program files\Amazon Corretto\jdk17.0.15_6\bin\
start javaw -cp languagetool-server.jar org.languagetool.server.HTTPServer --port 8081 --allow-origin ""*"" -l en-US --languageModel ""C:\LanguageTool\LanguageTool-6.3\ngram""

            참고: n-gram 데이터 활용법
          + ""Look Dick. See Jane. Jane run home. I says you go home to. They eats dinner."" 라는 문장으로 테스트 해봤음. LanguageTool은 기대대로 동작함. Harper는 아님. 둘 다 마침표 뒤 두 칸 띄우는 것에는 지적함
          + 우리는 Chrome의 내장 LLM을 이용해서 로컬에서 동작하는 문법 검사 확장 프로그램을 만들고 있음. 여기서 테스트 가능. LLM은 문장 오류뿐 아니라 예를 들어 ""The first US President was Ben Franklin""을 ""George Washington""으로 고칠 수 있음
     * 소프트웨어가 가끔씩은 keylogger처럼 동작해버릴 가능성을 완전히 배제할 수 없음. 만약 그런 일이 발생하면(고의가 아니더라도), 저자가 직접적으로 책임져야 하지 않을까 생각함. 법적으로 보면 개인정보 보호 책임을 지는 실체 자체가 없음. 그래서 신뢰할 이유가 부족하다고 생각함 privacy policy
          + 기술적으로도 신뢰할 이유가 없음. 이 앱은 샌드박스 처리가 되어 있지 않음. CLI에서 아래처럼 확인 가능함:
  codesign --display --verbose=4 Refine.app 2>&1 | grep sandbox

            Apple은 network client entitlements를 부여해야만 네트워크 접속이 가능한데, 샌드박스가 아니면 이 규제가 적용되지 않음. 나는 내 컴퓨터에서 직접 빌드했거나 Mac App Store에서 받은 샌드박스 앱만 쓰는 편임 관련 문서
          + 사실 이런 위험성은 모든 소프트웨어에 해당함. 예를 들어 grammarly도 이미 데이터를 서버로 전송할 수 있음
     * 누군가 꼭 이런 제품을 만들어주길 기다렸었음. 처음 써본 결과 꽤 잘 동작함. 여러 언어가 섞여 있어도 좋은 수정 제안을 해줌. 유창성(fluency) 제안은 다소 기대에 못 미침. 문장 앞뒤에 따옴표를 자주 넣으라는 식의 이상한 제안도 많음. Refine이 동작하면 텍스트 입력이 종종 멈추거나, 교정안을 고르면 앱이 0.5초 정도 멈추는 등의 반응성 문제 있음. 이런 문제는 Grammarly나 LanguageTool에서는 발생하지 않음. 그리고 Refine이 못 잡는 문법 오류도 있음(예: 동사 일치 문제). 그래도 첫 릴리즈로써 매우 인상적이고 앞으로가 기대됨
     * Languagetool은 오픈소스 스펠링/문법 검사 툴임. Grammarly와는 달리 AI보다는 룰(rule)을 기반으로 동작함. 나는 두 도구를 병행해서 쓰는 편임. Self-host하는 법은 여기에 소개함
     * 나는 Linux 사용자이지만, 이런 ""일반인 대상 도구""라면 Windows로 먼저 출시하는 게 더 자연스럽다고 생각함. MacOS 단독 출시는 오히려 전체 시장 <15%밖에 대응 못함(미국 기준으로도 <30%). 개발자만 겨냥하더라도, Mac, Windows, Linux 세 플랫폼 중에서 제일 손해보는 선택임 시장점유율 참고 StatCounter
     * LanguageTool을 docker container로 로컬에서 쓸 수 있음 LocalLanguageTool GitHub 링크
          + 정말로 많은 사람들이 사소한 일을 복잡하게 만드는 것 같음. LanguageTool을 실행하려면 docker(리눅스 기반 기술)까지 쓸 필요 없이, 단순히 java app으로 실행하면 됨. 아래처럼 간단히 실행 가능함
java -cp languagetool-server.jar org.languagetool.server.HTTPServer

          + 많은 사람들은 docker 컨테이너를 다루는 것보다는 $15 지불하는 걸 선호함
          + Flatpak으로도 간편 설치 가능함 Eloquent by sonnyp. Flatpak(GNOME Cgroups, Namespaces 등으로 제한)에 의해 권한 제한 있음:
* 파일 시스템 접근 불가
* 디바이스 접근 불가
* 네트워크는 가능(API가 REST 방식)

            Flatpak 패키지 자체가 크다는 점은 Java 문제임. Java가 서버엔 괜찮지만 개인용 PC에선 별로임. 만약 C, C++ 또는 Rust로 만들어졌다면 이미 LibreOffice 기반에 있었을 것임. Sonny Piers는 유명한 GNOME 이사회 출신 패키저임
     * ""로컬 AI 모델 기반""이라는 점이 AI 표절 탐지기(예: Turnitin 등)에 내 글이 걸릴 위험을 높일까 걱정임. 충분한 테스트가 없으면 그 위험을 감수할 수 없음
          + 나는 그 위험이 크지 않다고 봄. 이 도구는 글을 대신 써주는 게 아니라, 단순히 문법만 교정해주는 방식임. 직접 작성이 99%고, 단지 문법만 교정한다면 탐지에 걸릴 이유가 없을 것으로 생각함. 다만 LLM 도입 이후의 학교 환경에 직접 있던 건 아니라 확신은 없음
          + 내가 올바른 문장부호를 사용하면 오히려 AI 작성물로 탐지될 위험이 높아진다는 이야기가 있음. 그래서 절대 문장부호 제대로 쓰지 않음
     * FAQ에 ""Apple Intelligence Writing Tools와 상대적 비교""에 대한 언급이 하나도 없는 것이 이상함. refine이 더 낫거나 기능이 많을 수 있지만, Mac의 시스템 레벨 기능이 존재한다는 점 자체를 페이지에서 인정하지 않는 건 아쉬움
     * 또 다른 대안으로 Harper를 언급할 가치가 있음 Harper 링크
          + Harper는 너무 기본적이어서 추천하기 어려움. 주어-동사 간 단수/복수 불일치, a/an 같은 부정관사 누락, 그리고 ""don;t"" 오타 같은 명백한 오류도 잡지 못함
"
"https://news.hada.io/topic?id=22052","스스로 배운 엔지니어가 종종 더 뛰어남 (2024)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      스스로 배운 엔지니어가 종종 더 뛰어남 (2024)

     * 정규 교육은 효율적인 숙련도 전달에는 유용하지만, 예기치 못한 문제 해결을 위한 직관을 개발하는 데는 한계가 있음
     * 목적 있는 반복적 시행착오(알고리듬) 는 실전에서의 실패와 수정 과정을 통해 숙련도를 가장 크게 높이는 요소임
     * 실제 예시에서는 Linus Torvalds, Margaret Hamilton 등 다양한 인물이 실패를 경험하고 극복하면서 뛰어난 능력을 갖추게 되었음
     * 멘토링은 중요한 촉진제이지만, 자기 주도적 실험과 직접 경험이 궁극적 성장의 핵심임
     * 부딪히고 깨지면서 직접 문제를 해결하는 목표 지향적 실험이 진정한 실력 향상의 기반임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

최고의 소프트웨어 엔지니어를 만드는 힘: 목적 있는 시행착오

  교실 신화

     * 정규 교육은 가치 있는 지식 전수에는 유리하지만, 규모에 맞게 최적화된 과정임
     * 복잡한 실무 경험을 잘 정제한 절차에 담아 한 학기 내에 완료할 수 있는 형태로 전달함
     * 이 과정에서 실무에 필요한 기본 숙련도는 익힐 수 있지만, 예상 밖의 위기에서 문제를 직관적으로 해결하는 역량 구축에는 한계가 존재함
     * 특별히 밤 3시에 실제 서비스 문제 발생 시, 교실에서 배운 레시피만으로는 해결에 부족함

  직접 부딪히며 배우는 진짜 성장

    모범 사례들

     * Linus Torvalds는 MINIX를 리라이트하며 Linux를 만들었음
     * Margaret Hamilton은 Apollo 프로젝트에서 직접 실시간으로 코드 문제를 수정하면서 현대적 신뢰성 개념을 창시함
     * 수많은 오픈소스 메인테이너들도 자기 노트북을 스스로 망가뜨리고 다시 고치며 성장함
     * 이들은 순차적 강의를 먼저 듣지 않았으며, 실패와 그에 따른 실제 영향 덕분에 깊은 실력을 얻게 되었음

    왜 시행착오가 레시피를 앞서는가

    1. 피드백 루프가 즉각적임. 로그에서 크래시를 분석하는 과정이 퀴즈보다 훨씬 빠른 습득을 제공함
    2. 엣지 케이스는 실제 환경에서 자연스럽게 드러나며, 교재에서 상상 못하는 사용 사례를 만남
    3. 어렵게 해결한 문제는 기억에 깊이 남음. 근육 기억으로 자리잡음
    4. 기존 가이드가 없을 때 창의력이 폭발적으로 발휘됨

  멘토십의 재조명: 보완제, 대체제가 아님

     * 좋은 멘토는 피드백을 빠르게 해주고 시야를 넓혀주지만, 궁극적으로 실험과 경험의 주체는 본인임
     * 코드 리뷰는 실험 결과를 공유함으로써 가치가 있지만, 직접 경험을 대체하지 않음

    자기주도적 실험관습 키우기

     * 스스로를 조금 불안하게 만드는 사이드 프로젝트를 시도함
     * 모든 것을 계측해서 실패 시 분석 가능한 데이터를 확보함
     * 프레임워크 제한 혹은 48시간 내 완성 같은 제약 조건을 두어 창의적 해결력을 기름
     * 코드를 공개하면서 외부의 검증을 받음
     * 주간 단위로 회고를 정리해, 실패 원인과 배운 점을 기록함

  정리

     * 멘토십, 강좌, 블로그 등은 촉진제 역할이지만 진짜 실력은 현장에서 치열하게 부딪히는 과정에서 쌓임
     * 최고의 엔지니어는 자유롭고 목적 있는 시행착오를 반복하여 실질적 문제 해결과 성장을 이뤄냄
     * 이 과정에서 얻은 경험이 미래의 자기 자신에게 가장 큰 자산이 됨

        Hacker News 의견

     * 나 자신은 독학 개발자로서 대부분의 경력을 대기업에서 CS 전공 동료들과 함께 보냈음
       내 경험상, 독학 개발자는 지능이 충분하다면 결국 주어진 문제를 해결해냄
       CS 전공자는 완전히 낯선 영역에서 아예 시도조차 안 하는 경우가 많음 (물론 성향에 따른 편차 있음, 약 85% 정도에 해당한다고 생각함)
       높은 불확실성 앞에서는 진행을 못 함
       결국 CS 전공자는 대기업 환경에 더 잘 어울림, 대체 가능한 톱니바퀴처럼 패턴대로 일함
       독학 개발자는 언제나 혁신하고, 비효율적 반복을 줄이는 쪽으로 일하며, 이런 독창적인 태도가 주변을 불안하게 만듦
       하지만 이렇게 독학한 사람들이 훨씬 뛰어난 결과를 만들어냄
       대부분의 개발자들은 뛰어난 코드 자체보다는, 고용 유지와 불안을 줄이는 쪽을 더 중시하는 듯함
          + 많은 공식 교육을 받은 엔지니어들도 새로운 문제를 적극적으로 풀려고 함
            이건 독학/공식 교육 이슈라기보다는 호기심, 근면, 창의성, 지능 등 성향 문제임
            독학자는 이런 특성을 반드시 가져야 성공할 수 있으니 그 그룹에서 더 강하게 보이는 것일 수 있음
            공식 교육받은 엔지니어가 이런 특성까지 갖추면, 두 그룹 중 누구보다도 뛰어난 경향이 많음
          + 아마 성격 차이가 가장 클 것 같음
            나 같은 독학 개발자는 성실함을 내세웠음
            '빅픽처'를 기술적으로 잘 보진 못했지만, 그 대신 팀에서 가장 많은 결과물을 내려고 노력함
            여러 프로토타입을 며칠만에 세 개나 만들어서 실전 테스트를 함
            CS 전공 동료들은 화이트보드에서 설계만 한 번 하고 실제 코드는 한 번만 작성함
            두 방식이 똑같은 작업에 시도된 적은 없어서 어느 쪽이 결과가 더 좋은지 말하기 힘듦
            내 머릿속에서는 내 방식이 '실전 테스트'된 방식이라 느껴짐
            CS 방식이 이상적으로 인식된 건 맞음
            하지만 CS 전공 엔지니어들의 멘토링 덕분에 많이 배웠다고 생각함
            10년 정도 업계 경험 쌓으면 출발이 어땠든 결국 비슷해지고, '정답'에 대한 직감도 비슷하게 생김
            아마 내가 좀 미화했거나 일반화한 점 있을 수 있음, 다른 사람들이 답글로 보완해주면 좋겠음
          + 인정함
            대부분의 CS 커리큘럼은 실무와는 상당히 동떨어져 있으니 졸업장이 실력의 즉각적인 지표는 아님
            하지만 CS 졸업자의 85%가 낯선 문제를 못 푼다는 건 도저히 동의 못 하겠음
            CS 도 학위의 미덕이 있다면, 공학 트랙이 높은 지적 역량을 요구하는 엄청난 과목들을 포함한다는 점임
            그 많은 과목들을 통과할 수 있을 만큼의 사람이, 정작 실무 개발에서 못 할 거라곤 생각하기 힘듦
            정말 어려운 개발 영역에 가면 오히려 상당수가 CS 고급 학위를 가진 사람들이라고 봄
            소프트웨어에 대한 깊은 관심이 있으면 이런 특성이 생기는 것 같고, 그 중엔 CS 전공도 많음
          +

     CS 전공자는 낯선 문제에 도전조차 하지 않는다
     나는 이에 강하게 반박하는 입장임
     나도 독학하다 CS 학위 취득한 케이스임
     예전에 그래프 순회와 루프 탐지 코드를 처음부터 직접 구현하려고 하루종일 삽질한 적 있음
     자료구조/알고리즘 과목 듣고 나선 그런 선택에 2초도 안 걸릴 거였음
     CS 전공자는 정보를 구조적으로 모을 수 있고, 깊게 파야 할 때 논의할 언어도 가짐
     현실적으로 동료들이 진짜 아무것도 안 하고 ‘불가’라고 넘기나?
     내 경험상 업무 책임 회피는 주니어-미들-시니어 구분에 따라 더 달라지는 듯함
     시니어일수록 점점 문제 소유권을 가지니까
     (내가 만든 C 언어 스택 분석기는 C 코드 파싱, 호출 그래프 생성, 최악의 스택 메모리 사용량 산출을 포함했음)
          + 내 경험상 독학자는 자연스럽게 걸러짐
            즉, 낯선 문제도 못 푸는 독학자는 채용조차 안 됨
            정규 교육 출신들은 평범하거나 그 이상이긴 하지만, 압도적이진 않음
            모든 걸 스스로 파악하고, 대학 교육으로 검증까지 마친 사람은 진짜 막강한 실무 파워를 보여줌
     * 핵심은 학습 방식이 아니라 ‘열정’이라고 봄
       동기가 약하면 학습 방식이 무엇이든 한계가 분명함
       이 주제는 수치적으로 논의하기 어려운 측면도 있음
       공식 교육은 기반이 되는 개념들(수학, 하드웨어, OS, 컴파일러 등)을 탄탄히 다질 수 있음
       독학은 목표 지향적이라서, 기본기를 간과하게 될 수도 있음
       무엇을 모르는지조차 모를 때, 제대로 된 멘토(교수, 훌륭한 책)가 있으면 시간 엄청 단축됨
       나를 포함한 많은 엔지니어들이 공식+비공식 학습을 모두 거침
       열정이 있으면 스스로 계속 만들어보게 되고, 공식 학습 외에도 이런저런 실험을 함
       정말 뛰어난 엔지니어를 구분하는 건 교육 방식이 아니라 분명한 열정임
       Linus, Margaret 같은 사례도 결국 엄청난 학구열의 소유자였음
          + 나도 독학 프로그래머로서 확실히 동의
            80년대 8비트 컴퓨터 시절부터 스스로 배웠고, 대학도 안 갔음
            19살에 첫 정규 프로그래밍 직업을 가질 때 이미 9년간 프로그래밍 해왔음
            대부분이 대학 졸업할 때 나는 벌써 15년 가까이 코딩하고 있었음
            그런 열정과 추진력이 무시되기 힘들다고 봄
            지금도 40년 가까이 비슷한 마음으로 소프트웨어를 즐기고 있음
            만드는 자체가 재밌고, 계속 논문도 읽고 업계 흐름을 따라가며, 코드도 여전히 엄청 많이 씀
            다만, 독학자들이 기본기가 없다고 단정하는 인식은 조금 아쉬움
            실제로 많은 독학자들이 학문적 부분도 충분히 흥미롭게 파고듦—어떤 분야냐에 달린 듯
            깊이 있게 공부하는 시간은 부족할 수 있지만, 수십 년 경험이 쌓이면 오히려 학교만 간 사람들보다 기본기가 두꺼운 경우도 많음
            채용/해고 역시 많이 해봤는데, 독학자들이 더 뛰어난 결과를 내는 경우가 많았음
            결국 많이 ‘해본’ 시간이 훨씬 많음—열정이 결국 학습량을 밀어붙임
          + ""모르는 걸 모를 때, 누군가 효율적으로 이끌어주는 게 큰 도움이 된다""는 부분 정말 공감
            나는 공식/비공식 중간쯤 되는 케이스임
            고수준 이산수학, 선형대수는 수강하지 않았고, 그래서 배경 지식이 많이 부족함
            어떤 키워드를 검색해야 할지도 모름
            어떤 분야는 진짜 누군가의 지도가 꼭 필요함
            30대 후반에 벡터 수학 프로그램 검증해줄 튜터 구하기도 진짜 힘듦
          + 열정이 독학을 밀어붙이지만, 교실 환경에선 누군가 늘 트랙을 잡아주기 때문에 상대적으로 덜 필요할 수도 있음
            또 '목적 지향' 독학도 있지만, 시스템 자체의 원리를 이해하는 게 목표인 독학자도 있음
          + 누가 강제로 물가로 데려가더라도, 스스로 물가로 간 경험이 없는 건 아님
            독학자는 모두 최소 한 번은 자력으로 물가까지 가본 사람들이라는 점이 일관된 차이라고 봄
          + 나는 공식+독학 혼합형임
            대학 수업을 많이 들었지만 시험을 잘 봐서 졸업장 얻지는 못함
            내가 나머지를 독학으로 익힐 수 있었던 건, 수업으로 기반을 닦아서 가능했던 것임
     * 대학 수업은 정말 훌륭하다고 봄
       아무것도 모를 땐 절대 C의 소켓 API, bash 프로젝트, 분산 시스템, 자료구조, 알고리즘 같은 걸 혼자서 안 파봤을 것임
       실제로 독학자 또는 부트캠프 졸업자를 많이 인터뷰했는데, 이들은 자신이 익숙한 영역만 파거나, 학문적 질문엔 쉽게 무너지는 경향 있음
       반면, 대학에서 제대로 코딩을 안 해본 사람은 실력이 많이 떨어지고, 대학을 다니는 중이더라도 예전에 배운 걸 다 잊어버림
       대학 가기 전에 어느 정도 코딩을 해보는 게 최고의 조합이라고 생각함
       스스로 부딪혀서 느낀 고생이 있어야 대학 강의에서 소개하는 이론적이고 우아한 해법을 진짜 자기 것으로 만듦
       RAII 같은 메모리 관리 실수로 골탕먹은 적이 많을수록 이런 개념에 깊이 공감할 수 있음
          + 부트캠프 졸업자와 독학자는 구분해서 봐야 함
            부트캠프에서도 유능한 사람이 있을 수 있지만, 내가 아는 사람들은 스스로 못 배워서 대학이나 다른 분야로 가려다가, 더 싼 대안이 부트캠프라 선택한 케이스가 많았음
            예전엔 부트캠프도 없었고, 온라인 과정/전통식 학습법도 멀리함
            나는 뭔가 멋진 걸 만들고 싶어서, 교재대로 따라가 재도전하는 것보다 독립적으로 문제를 해결하는 게 더 흥미진진했음
            어린 시절 C를 독학한 이유도, 기존에 나온 자료나 코드로는 못할 일이 있었고, 멋진 결과가 너무 갖고 싶어서 포럼 뒤지고, 문서 읽고, 시행착오로 얻어낼 수밖에 없었음
            공부 방법보다도 배우고자 하는 강한 욕구가 더 중요하다고 봄
          + 대학 가기 전에 이미 C, 소켓 API 능숙했고, 소프트웨어 납품 경험도 있음
            고등학교 때 C64용 게임을 팔아 돈을 번 친구도 있었음
            우리 둘 다 신입보다 실전 코딩 실력은 훨씬 앞서 있었음
            내가 부족했던 건 미적분, 선형대수, 이산수학 등 이론 쪽이었고 데이터구조, 알고리즘도 모르는 거 간간이 있었음
            CS 프로그램이 그런 빈틈을 메워주긴 했지만, 코딩 실력 자체를 올려준 건 아니었음
            프로그래밍 관련 과목은 전혀 어렵지 않았고, 내가 힘들었던 건 오히려 수학과 이론 과목이었음
            CS 프로그램이 나를 더 균형 잡힌 엔지니어로 만들어줬지만, 더 나은 개발자를 만들어준 건 아님
          + 예전 대학 시절, CS 학과 분위기는 요즘 말하는 대학의 장점과 정반대였음
               o 의도적으로 학생을 탈락시키는 수업
               o 교수들이 재미로 고르고 가르치는, 비실용적인 이론 중심 강의
               o 학문 자체를 쉽게 접근할 수 없게 설계
                 나도 해당 이유로 대학에서 CS 전공을 선택하지 않았고(현재 미국 테크 기업 시니어 개발자), 당시 CS는 최고 학점만 뽑는 명문대학임에도 불구하고 실패율, 실업률, 교수 분위기 모두 최악이었음
                 물론 대학에서 많은 걸 얻은 분도 있지만, 현실은 다 그런 게 아님
                 실무에서 만난 많은 CS 졸업생이 커뮤니케이션, 비즈니스 이해, 일 우선순위 잡기에 어려움을 많이 겪음
                 코드만 짤 수 있는 경우도 많고(그조차도 실력 떨어짐), 대학 CS 과정 하나로 실무에 완전 대비된 경우는 오히려 드묾
                 오히려 대학 강의 방식이야말로 ‘컴포트 존’에 머무는 대표적 사례라고 생각함
          + 대학 수업의 진입/수강료 자체가 사회적, 경제적 계급 문제임도 인정해야 함
            그리고 독학으로도 소켓 API, bash 프로젝트 등 충분히 해본 사람도 있음
            참고로 독학자와 부트캠프 출신은 전혀 다른 유형임
            내가 주어진 상황(오디션 스타일 면접 등)에서는 무너질 수 있지만, 혼자 있을 때 실제 문제는 잘 해결함
          + 약간 꼰대같이 들릴 수 있지만, 나도 예전 대학이 진짜 좋았던 세대임
            컴파일러, 토이 OS, GPS 인터페이스 등 만들었었음
            몇 년 전 다른 학교에서 교수로 초빙받아 가르쳐보니 매우 실망했음
            커리큘럼이 요즘 부트캠프를 몇 년짜리로 늘리고, 별 상관없는 과목만 얹은 느낌이었음
            기본기는 거의 없고, 알고리즘 과목 빼면 전부 React, 현지 스타트업에서 인기있는 프레임워크 위주
            (편집: 실제로 커리큘럼 보니 비즈니스, 경영, 인문, 화학, 환경, 창업, e-스포츠 수업 있음)
     * 자기 교육의 부재를 스스로 받아들이는 과정에 노력을 많이 쏟는 듯함
       뛰어난 독학자도 있고, 똑똑하진 않은 학위자도 있지만, 나 자신의 경험상 CS 학위가 있었다면 내 길에 더 도움이 됐을 거라 생각함
       CS는 본질적으로 코딩이 다가 아니고, ME(기계공학)처럼 직업 별로 기대와 역할이 다름
       기계공학자들도 자동차 오일 누수, 타이어 펑크 같은 소소한 문제는 정비공이 훨씬 잘 고침
       하지만 그걸 이유로 엔지니어 학위를 무시하지는 않음
       나도 처음엔 제조 현장에서 직접 손에 기름 묻혀가며 기술자들이 못 고치는 것도 해결했지만, 그건 개인적 취향과 호기심 때문임
     * 독학자가 뛰어난 성과를 내는 이유는, 바로 그들이 독학자가 될 정도의 동기와 열정, 자기 주도성을 가진 사람들이기 때문임
       결국 호기심, 집중, 규율이 있다면 공식/비공식 관계없이 평균 이상이 됨
       생존자 편향도 작용—실제로 독학하다 현장에서 살아남은 사례만 보게 됨
       반대로 실패한 독학자들은 제대로 된 티칭을 받았다면 더 나았을지도 모름
       나는 CS/수학교육 다 즐겼지만, 프레임으로는 오히려 자기 학습이 더 잘 맞는다고 느낀 케이스임
          + 아무런 통계도 안 나온 상황에선 너무 통계적 오류 걱정할 필요 없다고 생각함
            '종종'이란 단어는 그저 모호함을 포장하는 표현일 뿐임
     * 나는 소프트웨어 개발자이자 대학 CS 강사임
       진짜 성공하는 엔지니어의 공통점은 결국 '관심'과 '열정'임
       독학자는 당연히 해당 분야에 관심이 많으니, 동기 자체가 검증된 집단임
       학위자가 더 혼합적임—순전히 졸업장만 노린 경우, 이론적 용어만 알고 실력 구분 힘들어짐
          + 그게 본질임
            독학자는 본질적으로 기존 체계에서 벗어나서 스스로 공부할 만큼 강한 동기와 관심을 가진 사람임
     * 지식은 보드 위의 원이라고 생각하면 됨
       https://matt.might.net/articles/phd-school-in-pictures/
       대학에서 배우는 원은 놀랄 만큼 좁고, 대체로 동일한 커리큘럼임
       예를 들어 dmc 알고리즘(최강 압축 알고리즘에 쓰이는 것)을 가르칠 시간이 없음
       대신, 모두가 범용적인 커리큘럼만 반복
       근데 그 원 바깥까지 나아간 사람들도 있음
       그런 사람이 업계 최고의 프로그래머임—논문에서나 보는 희귀 알고리즘도 알아서 남다른 실력을 보임
       독학자도 마찬가지; 모두가 배우는 공통 원에서 빠진 지식의 갭을 가질 수 있기에, 종종 자기 동기와 겸손함에서 출발함
       그러나 그들이 가지고 있는 지식의 원은 열정으로 유기적으로 쌓인 것
       이 열정이야말로 엔지니어 성과의 최고 신호라 생각함
     * 다르게 말하면, '독립적으로 무엇을 배워야 할지 스스로 판단하고, 실제로 배우는 능력을 이미 보여준 사람은 그런 역량이 요구되는 일에서 탁월함'
     *

     Linus Torvalds는 Linux를 만들며 MINIX를 완전히 새로 씀 Margaret Hamilton은 Apollo 프로젝트에서 현대 소프트웨어 신뢰성 개념을 만들어냄
     이 둘 모두 공식 교육을 받은 엔지니어임
     공식 교육은 '실습' 능력까지 포함해 수학적, 공학적 성숙함을 갖출 기회를 제공함
          + Torvalds는 1991년 Linux 첫 버전 공개 당시엔 CS 교육을 거의 따로 받은 적이 없음
          + 나도 공감함
            우리도 학부 과정에서 68K 어셈블리로 멀티프로세스 OS를 직접 짜보는 과제가 있었음
            이런 경험 덕분에 Linux 커널 구조나 동작 원리에 더 쉽게 접근할 수 있었음
            커널이 뭔지도 모르면 시작조차 못했을 테니까
     * 최근에 난 경험 없는 수치해석 문제(희소 LU 소버 만들기)를 스스로 배우려고 시도하고 있음
       가장 유용했던 자료는 직접 구현해보거나 기존 소버 코드를 분해하는 게 아니라, 관련 내용을 다뤘던 강의 노트였음
       강의 전체를 보면 내가 몰랐던 연관 개념까지 알게 됨
       다른 분야도 마찬가지로, 대학 강의 자료가 최고 품질인 경우가 많았음
       대학 수업이 필요없다는 주장이라면, 정작 최고의 자료가 대학 자료란 점을 설명하기 어려움
       직접 만들어보는 게 실력에 가장 좋다는 주장도 있지만, 대학 프로젝트 자체가 항상 '실제 구축'을 평가에 적극 반영함
          + 완전히 동의함
            깊이 있는 기술 서적도 훌륭하지만, 어떤 자세로 접근하냐에 따라 얻는 가치가 완전히 다름
            단순 이론만 배우고 실제 적용이 전혀 없으면 흥미가 떨어지고 금방 다 까먹음
            하지만 실제 필요나 관련된 작업과 연관된 이론을 배우면, 순식간에 꼭 필요한 실무 지식으로 바뀜
            초반에 독학으로 충분히 경험을 쌓은 사람이 대학에 들어가서 열정을 유지한다면, 짧은 시간에 엄청난 것들을 해낼 수 있음
          + 결론적으로, 이론은 당연히 중요함
            다만, 먼저 무언가를 만들어보고 나서 이론을 공부하면, 진짜 인사이트가 뭔지 더 명확히 파악하게 됨
"
"https://news.hada.io/topic?id=22048","Show GN: 16비트 DOS 시절에 쓰던 구닥다리 조합 글꼴(8x4x4)을 트루타입등으로 변환","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Show GN: 16비트 DOS 시절에 쓰던 구닥다리 조합 글꼴(8x4x4)을 트루타입등으로 변환

   현재는:
     * Hanme 한메한글(DOS용) + 영어 (sans|serif) + (Nerd Font)
     * Iyagi 이야기(DOS용) + 영어 (sans|serif) + (Nerd Font)
     * Dkby 도깨비한글 + 영어 (sans|serif) + (Nerd Font)
     * Sans (그시절 흔한 고딕) + 영어 sans + (Nerd Font)
     * Serif (그시절 흔한 명조) + 영어 serif + (Nerd Font)

   자매품: 8비트 애플2 시절에 쓰던 조합 글꼴(6x2x1)도 있구요,
   https://github.com/iolo/6x2x1-fonts

   제가 직접 디자인(?)한 오리지널 글꼴도 있습니다^^
   https://github.com/iolo/7x12-fonts

   글꼴이 맘에 드셨다면... 좋아요 꾹! 구독 꾹!

   뱀발:
   코딩하다가 집중이 잘 안되서...
   재미삼아 두어시간 걸려서 만들어서 소셜에만 올렸었는데... 좋아요+리트윗+팔로잉... 알람 터져서 며칠 뮤트하고 살았네요. ㅎㅎ
   트위터, 페북 오픈했을무렵부터 20여년 동안 구구절절 코딩 얘기를 했을 때도 없었던 폭발적인(?) 반응을 보면서... 세상은 요지경... 코딩 따위... 별 생각이 다 드네요.

   아무튼, 그래서 https://news.hada.io/topic?id=22041 이 글까지 쓰게 됐네요.

   Mac OS에서 쓰던 서울체랑 한강체도 있으면 좋은데, 생각해 보면, 그때는 꽤나 마이너 했겠네요.

   예전에 ttf만들어서 kldp통해서 배포한적이 있는데… 못찾겠더라구요 ㅎ

   이런 거 좋아하시면 이런 것도 한번 보시죠.
     * 현대 웹브라우저에서 1990년대 룩앤필로 글꼴 표시하기
          + https://vistaserv.net/blog/90s-fonts-modern-browsers"">Convincing-looking 90s fonts in modern browsers (위 글에서 소개했던 글의 아카이브 링크)

   딱이 옛날 글꼴이 좋아서라기보단…
   노안으로 가는 글꼴이 잘 안보여서… 굵은 글꼴이 필요했는데… 네오둥근모의 영어글꼴이 맘에 안들어서… 시작했다가… 너무 멀리왔네요 ㅎㅎ

   이야기 폰트 좋아요!

   와......................... 한메, 이야기, 도깨비...
   이런걸 모르시는 분들도 많을 텐데... 그걸 Nerd Font 랑 머지까지
   정말 감사드립니다.
"
"https://news.hada.io/topic?id=21994","Cognition(Devin AI), Windsurf 인수 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Cognition(Devin AI), Windsurf 인수 발표

     * Cognition(Devin AI) 가 Windsurf를 인수하기로 결정함
     * 이번 인수로 Windsurf의 지적 재산권, 제품, 상표와 우수한 인재들이 Cognition에 합류함
     * Windsurf 팀은 당분간 기존대로 독립 운영을 유지하고, 점진적으로 Cognition의 기술과 통합 작업이 이루어질 예정임
     * Windsurf는 350개 이상의 엔터프라이즈 고객과 수십만 명의 일일 활성 사용자를 보유하며, 연간 반복 매출(ARR)이 8,200만 달러에 달하는 성장세를 기록 중임
     * Cognition은 이번 인수를 통해 소프트웨어 엔지니어링 미래를 여는 차세대 혁신에 박차를 가할 계획임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cognition(Devin AI), Windsurf 인수 개요

     * Cognition은 Windsurf, agentic IDE를 인수하기로 최종 계약을 체결했음을 공식 발표함
     * 인수 대상에는 Windsurf의 지적 재산(IP) 및 제품, 상표, 브랜드 그리고 고성장 비즈니스가 포함되어 있음
     * Windsurf 구성원 전원이 Cognition 팀으로 합류하여, 업계 최고 수준의 인재들이 새롭게 한 팀이 됨

단기 및 향후 계획

     * 단기적으로 Windsurf 팀은 기존과 동일하게 운영될 예정임
     * Cognition은 Devin을 통한 엔지니어링 지원에 집중하면서, 수 개월 내 Windsurf의 핵심 기능 및 독자적 IP를 Cognition 제품군에 통합하기 위한 투자를 진행할 계획임
     * 이번 인수를 계기로, 소프트웨어 엔지니어링 미래를 선도하겠다는 Cognition의 목표를 더욱 강화할 예정임

Windsurf 인수 세부 내용

     * Windsurf 인수를 통해 Cognition은 다음과 같은 자산을 보유하게 됨
          + Windsurf IDE 제품(최신 Claude 모델 완전 접근권 포함)
          + Windsurf의 지적 재산, 상표 및 강력한 브랜드
          + 연간 반복 매출(ARR) 8,200만 달러 및 계속해 분기별 두 배 성장세를 보이는 비즈니스
          + 350개 이상의 엔터프라이즈 고객 및 수십만 명의 일일 활성 사용자

Windsurf 구성원 및 인재 정책

     * 업계 최고의 GTM, 엔지니어링, 제품팀 등 뛰어난 인재들과 함께 하게 됨
     * Windsurf 팀의 업적과 재능을 존중하여, 모든 직원에 대해 특별한 대우가 이루어짐
          + Windsurf 직원 *100%*가 재정적 인수 혜택에 참여함
          + *100%*의 직원에 대해 기존 근속분 베스팅 클리프(vesting cliff) 면제
          + *100%*의 직원에 대해 근속분 완전 베스팅(fully accelerated vesting) 적용
     * 신규 및 현행 직원 모두에게 투명성, 공정성, 능력과 가치를 존중하는 환경이 보장됨

Cognition의 비전 및 전략 효과

     * Cognition은 본 인수를 통해 소프트웨어 엔지니어링 혁신을 가속화할 수 있게 됨
     * Devin의 빠른 시장 채택과 Windsurf의 IDE 및 GTM(Go-To-Market) 역량이 결합함으로써, 엔터프라이즈 시장에서 큰 시너지 효과를 기대함
     * 앞으로 엔지니어는 단순 반복 작업에서 벗어나, 시스템 설계와 창의 영역에 더욱 집중하는 미래 지향적 패러다임 변화가 예고됨

긍정적 전망 및 결론

     * Cognition이 이러한 성과를 이루게 된 것은 임직원 모두의 헌신과 노력의 결과임
     * 향후 팀 전체가 한 배에 탔다는 각오로, 새로운 동료를 환영하는 준비를 강조함

        Hacker News 의견

     * 나는 최근 이 거래들에서 나타나는 혼란이 시장의 거대한 거품 가능성에 무게를 실어주는 느낌임, 기본적인 것들과 점점 더 동떨어지는 흐름이라 생각함, 결국 이 거품은 터질 것임
          + Anthropic ARR가 올해 상반기에 $10억에서 $40억으로 껑충 뛴 것에 주목함, 나 역시 매달 $200을 쓰고 있는데 돈이 아깝지 않음, 분명 그만한 가치가 있음
          + 웹도 한때 거품이 터졌지만 지금 Google, Amazon, Meta 상황을 보라며, 현재의 거품도 분명 존재하지만 그 형태가 암호화폐 거품보다는 웹 거품과 더 비슷함을 언급함
          + 닷컴 버블은 첫 번째 위기 신호에서 투자자들이 돈과 신뢰를 뺐기 때문에 터진 것임, 현재는 판도가 확 바뀌었고, 투자자와 VC들은 '우승자'와 성장 기업에 계속 붙으면 큰 수익이 난다는 교훈을 얻었음, 시장에는 자동화 자금, 소매 자금, 해외 자금이 많이 들어와 있고, '저점 매수' 현상도 과거보다 훨씬 강함, 예전에는 큰손이 빠지면 무조건 망했지만 지금은 리테일러들이 오히려 더 긴 시야와 믿음을 갖고 시장을 지지하는 경우도 많음(Musk, Altman 같이), 역사는 반복되지만 똑같은 타이밍이나 가격대는 아님을 강조함
          + 어떤 거품은 오히려 전체 경제에 긍정적이라고 생각하게 됨, 닷컴 시절에는 아이디어와 도메인만 있어도 펀딩이 됐고, 본인이 일했던 회사는 필요 자금보다 7배 많은 투자를 받고도 리더십 부재로 망한 경험이 있음, 투자자들이 많은 돈을 날렸지만 그만큼 시장에 돈이 돌고, 실패한 회사 자산, 인재들이 싸게 흡수되어 새로운 스타트업들이 더 튼튼하게 만들어지는 선순환 구조가 나타남, 위기는 배움의 기회임을 강조함
          + 아직 거품이 터질 단계는 아니라고 봄, '이모가 NVDA 주식 사려고 담보대출을 또 받으면' 진짜 거품이라고 보고, 이런 세태가 몇 년간 이어질 것으로 예측함, 한편 AI 분야에서 사람들에게 엄청난 연봉을 주는 모습은 황당하고, 어떤 면에서는 마크다운 파일을 관리하는 수준에 큰 돈이 오간다는 점을 비판적으로 바라봄
     * 약간 혹평일 수 있지만, LSP 확장 기능으로 CLI 기반 에이전트(Claude Code 등)가 에디터 내에서 diff를 보여주고, 자동완성 결과와 일부 코드 스니펫을 CLI로 다시 전송하는 기능만 있으면 Windsurf, Cursor, 그 외 비슷한 도구들의 가치가 거의 사라질 거라고 언급함, 구글이 이렇게 돈을 쓰는 것은 실질적 가치에 비해 과도하게 자금이 몰려있는 업계의 현실을 보여줌
          + 이런 기능은 이미 MCP로 구현되어 있음, MCP는 에디터의 종류와 상관 없이 WebSocket 서버만 돌릴 수 있으면 Claude Code와 연동되고, 실제 구현 예시와 링크, nvim 프로토콜 상세, Emacs 통합 예시까지 참고 자료 제공함
          + Claude Code는 이미 JetBrains와 VSCode IDE에서 diff를 보여주는 기능을 가지고 있고, '/ide' 명령으로 CLI와 IDE 연동도 가능함, IDE의 실시간 에러/경고, 에디터의 선택범위, 커서 위치까지도 접근 가능한 등, 기능이 매우 풍부함을 강조함
          + ""과잉 자금""이라는 표현에 의문을 제기하면서, 해당 기업은 엄청난 수준의 수익성을 내고 있는 기술회사임을 상기시킴
          + Cursor를 사용해본 경험상, 모델과 채팅 입력 창을 연결하는 그 '인터페이스' 부분에 실제로 많은 가치가 존재함, Claude Code, Codex도 비슷한 맥락의 혁신을 담고 있고, 특히 VSCode 환경에 이미 익숙한 사람들에게 Cursor의 접근 방식이 아주 잘 맞음
          + Windsurf팀 역시 그 방향으로 이미 빠르게 움직이고 있다는 점을 언급함
     * 최근 관련 소식으로, OpenAI의 Windsurf 인수 무산, CEO는 Google로 이동, Windsurf Build Night에 창업자들이 Google DeepMind로 간 직전 참석한 후기 등을 소개함
     * AI 소프트웨어 엔지니어를 만든다는 회사들의 높은 가치평가는 납득하기 어려움, 만약 Devin 같이 성공하는 제품이 있다면 누구든 곧바로 경쟁 제품을 만들 수 있어서 진입장벽(모트)이 전혀 없는 구조임, 결국 또다른 LLM 래퍼 SaaS일 뿐이라는 회의적 관점임
          + 이런 유형의 회사들은 '하우스 오브 카드'가 무너지기 전에 재빨리 인수되는 걸 노리는 경향이 있음, 과도한 FOMO로 인해 인수대전이 벌어지고 있는데 기술은 아직 목표에 한참 못 미침, 진짜 AGI가 나온다면 누구든 소규모 자본으로 수십억 달러 규모의 기업을 세울 수 있을 것임
          + 반면, 진짜 성공하는 AI 소프트웨어 엔지니어는 굉장히 복잡한 아키텍처와 혁신적 연구의 결과일 수 있다고 봄, 단순히 엔지니어를 많이 고용해서 똑같이 만들기는 어렵고, 시간과 노력, 최정상급 인력을 필요로 함, Devin은 엔지니어링 파워를 10배 키워줄 수는 있지만 '최정상급' 수준을 담보하지는 않음
          + 같은 생각을 가지고 있었는데, 이런 얘기가 생각보다 잘 언급되지 않아 놀람, 정말 획기적인 AI라면 스스로를 다시 구현할 수 있을 것이고, 그렇다면 AI 회사들은 자기 자신을 스스로 잠식하는 구조가 될 것임
          + 이미 이런 약속을 하는 도구가 넘쳐나고 있음, 실제로 프로젝트 완성과 시장 출시가 여전히 쉽지 않음을 경험으로 강조함
          + 마케팅 문구를 너무 과하게 믿고 성급하게 무시하지는 말라고 조언함, 그런 태도는 좋은 투자 기회와 스타트업을 놓칠 위험이 있고, 거꾸로 본인을 허위적 안도감에 빠뜨릴 수 있음
     * 처음엔 ""Cognition""이 뭔지 몰랐는데 Devin 만든 회사라는 사실을 알고 납득함, 결국 경쟁사를 그냥 사는 셈이고, 의외인 점은 이런 기업들이 대기업보다 더 돈을 많이 쓸 수 있다는 부분임
          + Google도 Windsurf 기술 라이선스에 $25억을 투자했으니, Cognition 쪽은 그것에 비하면 훨씬 적은 금액(남은 인력 등)을 썼을 것임
          + Windsurf의 실제 매각가는 원래 논의보다 크게 낮췄을 가능성이 높음, $0로 가라는 예측도 있었고, 경영진 이탈 후 가치가 거의(예전만큼) 유지되지 못했다는 해석
          + 대부분 사람들이 회사명보다 그 악명 높은 첫 제품 혹평만 기억한다는 점이 좀 웃김
          + Devin이 ""AI 소프트웨어 엔지니어""를 슬로건으로 내세운 최초의 회사였던 것으로 기억함
          + ""돈을 더 많이 썼다""라는 건 실제로는 소액에 Cognition 주식을 많이 얹어주는 구조일 수도 있음
     * 이런 거래들에 대해 부분적 정보만으로 판단하는 것이 얼마나 위험한지 배운 계기라 생각함, Google, Windsurf, Cognition 팀이 발표 전까지 완전히 보안을 지켜서 놀랍고, 이런 과정만 봐도 OpenAI 같은 기업도 참고할 게 많다고 봄, 다만 결국 외부인으로서 모든 세부를 알 수 없다는 점, 최근 수일간 Windsurf 창업자가 직원들에게 피해를 줬다거나, OpenAI가 계약을 어겼다는 식의 논란이 불필요하게 과장되어 안타까움, 이번 건은 모두가 비교적 해피엔딩으로 끝난 듯해서 Windsurf팀에 축하를 보냄
          + ""Windsurf 직원들도 모두 딜에 재정적으로 참여한다""는 발표 내용 외에, 구체적인 형태가 명확히 안 나왔으니 '직원들이 손해를 봤다/안 봤다'는 단언이 불가능함, 팩트에 기반한 논의가 중요함
          + 내 생각이 맞다면 이번 딜은 직원들 입장에서 기존 Google식 인수(스톡 가속, 일반주 전환)에 비해 상당히 안 좋은 조건일 것 같음, Cognition의 보상은 명목상 평가액도 낮고 현금 아닌 비상장(과대평가된) 주식일 것으로 추정함
          + 마치 창업진이 Cognition으로의 인수를 계획적으로 설계했다는 뉘앙스처럼 들리는데, 실제로는 전혀 아니었음, 창업진이 떠난 뒤 남은 경영진이 주말에 판매 협상으로 마무리한 것임
          + OpenAI가 Windsurf 인수를 공식 발표한 적이 있는지 의문임, 대부분 루머나 리크 정보였고, 공식 인수 발표는 없었다고 보임, Bloomberg가 오보를 냈을 가능성까지 지적함
          + ""모든 딜을 완전히 보안에 부치고 발표한 점에 Kudos!"" 같은 평가는 너무 과하게 감정이 실린 해석임, 단순한 사실도 쉽게 예측 가능한 사안이었음
     * 요약하면 Google이 인재를 데려가고 Devin이 브랜드/제품을 가져간 것 같은데, 굉장히 혼란스러운 거래임
          + Windsurf보다 좋은 옵션이 많으니 굳이 쓸 필요가 없다고 생각함
          + 인재는 Devin에 그대로 남아있고, Google은 CEO만 가져간 셈임, 왜 CEO가 필요했는지 모르겠고 혹시 Pichai가 조기 은퇴라도 노리는 것 아니냐는 농담
     * Windsurf는 이제 인수계의 'soham parekh'처럼 느껴짐, Windsurf를 인수하지 않은 회사가 남아 있긴 한지 궁금해짐
     * Google의 유사 인수 이후 남은 '껍데기' 회사만 사는 건지, 아니면 그 딜이 아예 취소된 건지 불명확함, Google 딜에 일반주주도 보상이 없었다는 의미인지도 궁금하고, 궁금증만 점점 더 많아짐
          + Windsurf 창업진은 이미 Google에 합류함
          + Google이 Windsurf 기술을 사용하기 위해 24억 달러의 라이선스를 누구에게 어떤 목적으로 지불하는지, Windsurf에 과연 제대로 된 기술이 있는지 의문임
     * Windsurf 팬덤으로 Hacker News에 있으면서 좀 소수파나 비정상 느낌을 받을 때가 있음, 대화의 초점이 거의 Cursor나 최근엔 Claude Code 중심이기 때문임, 또 최근엔 'Ross와 Rachel의 인수'같이 셋이나 되는 이상한 스토리라인이 이어져서 혼란스러움, 주요 소회는 Devin/Cognition이 드디어 진짜 AI dev 에이전트를 갖게 된 것, Google은 베스트 자산 없이도 그만한 가치를 인정하고 베팅한 점이 놀랍고, Windsurf가 결국 사라지거나 약화(B2B 요금제 등)되지 않을까 미리 걱정된다는 점임
"
"https://news.hada.io/topic?id=22086","varlock - 차세대 .env 관리 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        varlock - 차세대 .env 관리 도구

     * 기존 .env/.env.example 방식의 한계를 극복해, 협업/AI/보안/타입 안정성을 한 번에 해결하는 차세대 환경 변수 관리 툴
          + .env.schema로 환경 변수 스키마를 한 곳에서 관리, .env.example와 달리 실제와 예시의 불일치 걱정 없음
     * @env-spec 데코레이터 주석을 통해 스키마, 타입, 검증, 예시, 민감도, 외부 비밀 관리 등 다양한 정보를 .env 파일에 선언적으로 추가
          + @required, @type=string, @sensitive, @example 등
     * 강력한 유효성 검증: 잘못된 설정/미입력 오류를 즉시 명확한 메시지로 안내(런타임 이전에 사전 차단)
     * 스키마 기반 자동 타입 생성으로, 코드 내 환경 변수 접근 시 타입 안전성 및 IDE 인텔리센스 지원
     * 보안: 민감 정보 자동 마스킹(로그/콘솔), 번들된 클라이언트/응답 내 유출 감지
     * 다중 환경 및 오버라이드: 기본값, 환경별 파일, git-ignored 개인값, 프로세스 env 조합 등 복합적인 환경 구성 지원
     * 외부 시크릿 통합: 1Password, exec 등 커맨드 기반 시크릿 동적 로딩, 플러그인·로컬 암호화·팀 vault 곧 지원 예정
     * 언어 무관/런타임 무관: JS/TS 뿐 아니라, varlock run -- python my.py 등 모든 언어·프로세스에 validated env 주입 가능
     * dotenv 완전 대체 가능: dotenv import만 varlock으로 바꿔도 즉시 유효성 검사, 타입 자동 생성, 보안 강화, 다중 환경/시크릿 통합 등 다양한 기능 사용 가능

   이거면 .gitignore에 .env.schema가 포함돼야 하는건가요?

   아.. .env에는 정보를 두고 .env.schema가 읽어들이는 방식이군요
"
"https://news.hada.io/topic?id=22068","하얏트 호텔, 알고리듬 기반 ‘흡연 감지기’ 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      하얏트 호텔, 알고리듬 기반 ‘흡연 감지기’ 도입

     * 하얏트 호텔이 최근 알고리듬 감지 기술을 활용한 비흡연 정책 강화 움직임 전개
     * Rest 감지기가 실내 공기와 환경 데이터 분석으로 흡연 가능성 판단 방식 채택
     * 숙박객이 흡연 의심 알림을 받았다는 사례 다수 보고됨
     * 실시간 알고리듬 분석으로 평소보다 광범위한 감지 및 자동 경고 가능
     * 새로운 기술 도입으로 고객 개인정보 보호와 관련한 우려도 일부 제기됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

하얏트 호텔의 알고리듬 흡연 감지 기술 도입

     * 하얏트 호텔이 최근 알고리듬 방식의 흡연 감지기(Rest ‘smoking detector’) 를 도입하여 객실 내 흡연 행위를 적극적으로 감지하는 움직임 전개
     * 이 감지기는 객실의 공기 질, 입자 농도 등의 환경 데이터를 수집 후, 자체 알고리듬을 통해 흡연 가능성 여부를 자동 판별함
     * 실제로 이용객 중 일부가 ‘흡연이 감지되었다’는 경고 메시지를 받았다는 사례가 온라인 커뮤니티 및 트위터 등에서 다수 보고됨
     * 해당 시스템은 사람이 직접 감지하는 기존 방식보다 더 빠른 실시간 탐지 및 자동화된 경고 발송이 가능하며, 숙박객 개인의 행동을 데이터로 기록함
     * 새로운 기술 도입으로 실효성 제고 효과가 기대되나, 데이터 수집 및 분석과 관련한 고객의 프라이버시 침해 우려도 함께 제기되는 상황임

        Hacker News 의견

     * Threadreaderapp 링크 공유함
     * 이런 시도를 하는 회사에 확실한 처벌 방식이 필요하다고 생각함. 단순히 청구 취소만으로는 부족함. 출장 시 회사 카드를 쓰면 명세를 자세히 안 보다 지나칠 수 있고, 다른 지출로 오해할 수도 있음. 만약 호텔이 이렇게 한다면, 나도 호텔에 사전에 알리지 않고 객실 내 고장난 기기나 청결 불량에 대해 실제 객실료보다 훨씬 큰 금액을 임의로 청구하고, 증명 책임을 호텔에 넘길 수 있으면 좋겠음
          + 가장 큰 문제는 권력 불균형임. 처음 체크인할 때 신용카드 금액을 홀드하는 것만 봐도 그 이유를 알 수 있음. 설령 호텔에 청구할 수 있어도 법적 싸움을 할 수 있는 여력은 호텔 쪽이 훨씬 큼
          + 이건 호텔, 브랜드, 그리고 센서 회사 상대로 집단소송(class action)에 해당할 수 있는 사안이라고 생각함
          + ‘모든 게 괜찮았다는 증명 책임을 호텔에 넘기는 것’이 오히려 호텔이 모든 걸 감시할 동기를 더해주는 결과가 아닌지 걱정임
          + 이런 행위는 소비자 기만(사기) 혐의로 소송감임. 실제로 누군가 행동에 나섰으면 좋겠음
          + 이런 시도를 제재하는 방법은 간단함. 그냥 그 호텔 이용을 중단하면 됨
     * Hertz 렌터카가 작은 흠집까지 검출하는 AI 스캐너를 도입해 새로운 수익원을 만들려는 것과 비슷한 흐름임. 앞으로는 1원까지 다 뽑아내는 시대가 올 것 같음
       관련 기사
     * [Rest]는 ‘검증된 알고리즘’으로 호텔에서 흡연을 자동으로 감지해 새로운 수익원을 만들 수 있게 해준다고 홍보함. 그런데 이런 센서가 설치된 호텔에서는 거짓 양성(잘못 감지) 때문에 끊임없이 불만과 부정적인 리뷰가 쌓이고 있음. 블랙박스 알고리즘의 무서운 점은, 실수해도 호텔 측에 유리하게만 작동하기에, 호텔에서는 책임을 회피하고 돈만 벌 수 있다는 것임
          + 이런 블랙박스 알고리즘의 책임 회피 기제를 ‘책임 세탁’이라고 부르고 싶음. 돈은 챙기고 책임은 깨끗하게 씻어내 버리는 개념임
          + ‘unlocking revenue stream’(새로운 수익원 창출)이란 표현은 사실상 도둑질임
          + 아마 호텔이 직접 기기를 사지 않고, 수익을 센서 회사와 나누는 방식(rev share model)인 것 같음. 신호등 단속 카메라 운영 방식과 비슷해서, 노란불 시간을 짧게 줄여 차들이 무조건 걸리게 만드는 것처럼 말임
          + 이들은 정말 ‘사기’ 업계 자체를 혁신하고 있음. 고객들 돈을 빼앗는 새로운 방식을 창조하는 셈임. 이미 돈도 잘 버는데 왜 굳이 사기까지 해야 하는지 의문임
          + 하지만 알고리즘 규제가 실제로 논의되면, 갑자기 AI 규제 논의 때마다 기업을 숨 막히게 한다며 반발하는 분위기를 HN에서도 쉽게 볼 수 있음
     * 예전에 미니바가 가득 채워진 호텔에 묵었던 적 있음. 아무 음료수를 집어서 보기만 해도, 다시 갖다 놓아도 자동으로 요금이 청구되는 구조였음. 뭔가를 빼면 소비한 걸로 간주하고, 체크아웃 때 항의하면 취소해주지만, 대부분 고객은 명세서 자체를 안 보고 그냥 넘어감. 이 허위 청구가 센서 비용 이상의 수익을 내는 구조임. 소리 내서 말하지 않겠지만, ‘거짓 양성’ 그 자체가 핵심이라 할 수 있음
          + 호텔들이 여전히 ‘상식을 벗어난 요금 청구로 일부 고객에게 불쾌감을 주면서도 돈을 많이 버는 게 이익’이라는 사고에 머물러 있어 놀라움. 만약 객실 냉장고에 맥주를 정상 가격(혹은 무마진)으로 채워두는 호텔이 있다면, 무조건 그 호텔을 선택할 것임. 편리함과 지뢰밭 걱정 없는 기분 때문임
          + 최근 호텔에서 트레이에 포장된 과자가 있었는데, 움직이면 자동 청구된다는 안내문이 명확히 써 있었음. 아이들과 함께 안 간 것이 천만다행이었음
          + 예전에 맥주 캔 슬롯에 내 콜라를 넣었다가 회사에서 청구되었음. 회사 정책상 술 절대 금지라 HR에게까지 불려감. 당시엔 술 금지 중이었음
          + 이런 시스템이 있다는 걸 알고 나서는 미니바 근처엔 아예 가지 않을 생각임. 뉴질랜드 호텔은 최근엔 미니바 자체가 빠진 곳이 대부분이고, 우유와 사전 주문한 옵션만 넣을 수 있게 되어서 이런 걱정이 거의 없음
          + 체크인 직후 호텔 직원이 객실 냉장고를 점검해야 한다며 찾아온 적 있었음. 나는 냉장고도 안 건드렸는데, 센서 때문이라는 걸 나중에서야 알게 되었고, 이젠 괜히 냉장고 점검한다며 나를 의심한 것 아닌가 싶었음. 지금 생각해보면, 오히려 한 번 건드려볼 걸 그랬다고 느낌
     * Rest 홈페이지 FAQ에는, ‘투자할 가치가 있나?’에 대해, “Rest를 설치한 호텔들은 흡연 과태료 징수액이 84배 증가함. 흡연 감지기술 덕분에 객실 손상도 예방하고 위반 건수도 줄일 수 있음”이라고 주장하고 있음. 이 말대로라면 실제로 흡연자가 예상보다 훨씬 많거나, 혹은 센서가 거짓 양성만 잔뜩 낸다는 것임. 특히, 사이트 전반에 걸쳐 사용하는 언어가 흥미로운데, 예를 들어 ‘흡연 감지 시 실시간으로 직원을 알리고, 흡연 벌금 청구를 쉽게 해준다’고 적혀 있음. 만약 거짓 양성이 나오는 구조라면, 실시간 알림으로 직원을 보내 현장 확인 후에 청구해야 정상인데, 자동 청구까지 얹어놓는 방식임. 센서를 이용해 호텔 측에 사기를 팔면서도, ‘실제 확인은 호텔 직원이 한다’는 문구로 법적 책임은 피해가겠다는 의도 같음
          + 호텔에 정말 흡연이 그렇게 많은지, 흡연 객실이 추가 요금이 있는지 궁금함. 84배가 사실상 거의 0에서 시작했다는 의미가 있겠음. 흔히 청소 담당 직원이 ‘담배 냄새가 났다’고 신고만 해도 증거가 불충분해 취소가 쉽고, 진짜 방을 비워야 하는 경우가 아니면 청구 자체가 어려웠음. 아마도 아주 소량의 흡연(조금만 태우는 등)을 하면서 걸리지 않는 사람이 많았고, 그게 84배란 숫자가 나오는 이유일 것 같음
     * 이건 정말 사기 같다는 생각임. 꼭 해야 할 일은, 항상 영수증을 꼼꼼히 확인하는 것임. 될 수 있으면 여행용 신용카드를 별도로 써서 다른 계좌 피해를 줄여야 함. 청구가 있으면 선 긋고 사진 찍어 영수증을 남기고, 남은 금액만 결제하겠다고 주장할 것임. 호텔이 용납하지 않으면 바로 나가든지, 일단 결제한 후 즉시 카드사에 ‘사기 알림’을 요청하는 게 좋음. 미국 카드사는 대부분 거래가 ‘pending’(임시 승인) 상태일 때는 분쟁 처리 안 해주니 2~3일 여유를 활용해 호텔과 먼저 협상 시도하기를 권장함. 호텔이 협조하지 않으면 최종적으로 사기 건으로 신고하고 사진 자료도 올림. 그런 호텔은 절대 다시 가지 않을 것임. 개인정보 걱정 없다면 SNS로 이 경험을 적극적으로 공유해야 함
          + SNS 활용은 결코 쉽지 않음(혹은 매우 불공평함). 기사 작성자처럼 팔로워가 많아야 주목받을 수 있음. 나는 SNS 계정도 없고, 예전에 있었을 때도 영향력 있는 네트워크가 전혀 없어서, 기업을 태그해도 그들은 ‘신경 쓸 필요 없는 사람’으로 여길 뿐임. 이런 문제는 법적으로 다루어야 하며, 절도 혹은 사기 행위에 해당하므로 수사와 기소가 필요하다고 생각함
     * 기사에 나온 것처럼, “Erik에게 방 청소가 필요하냐고 물었더니 그럴 필요 없고, $250을 제안했다”고 함. 고객에게 $500를 청구할 정도로, 방 전체를 뜯어고치거나 크게 청소할 것처럼 하면서 정작 실제 청소는 하지 않음. 이게 사기라는 걸 본인들도 알기 때문이고, 이 시스템을 판매하는 회사도 호텔이 이런 목적으로 쓸 걸 다 알고 있음. 그래서 이 신기술에 대해 호텔 측 고객들은 열광함.
       (Rest 공식 리뷰: “Rest의 실내 흡연 감지 서비스 덕분에 부수입원이 크게 늘었고, 고객 경험까지 개선됐음” - Kirsten Snyder, Woodbine 자산 매니저)
          + 그런데 Woodbine이란 이름 자체가 실제 담배 브랜드에서 따온 것 아니냐는 점이 신기함
     * 만약 이런 경험을 겪는다면, 그냥 돈을 내고 앞으로는 절대 그 호텔 체인 전부를 이용하지 않을 것임. 연간 호텔에 $25k~50k 쓰는 입장에서, 나를 잃는 건 중소 호텔엔 큰 이익임. TripAdvisor만 살펴보면 이런 호텔은 바로 걸러낼 수 있음. 호텔 업계는 브랜드 충성도가 필수라 이런 단기적 이익에 집착하면 자멸임
          + 사실 호텔 체인(예: Hyatt)이 결정한 게 아니라, 실제론 프랜차이즈 지점에서 독자적으로 도입한 케이스가 많음. 체인 본사에 연락해 이런 문제가 재발하지 않으면 이용하지 않겠다고 이의를 제기할 필요가 있음. 매년 정말 많은 돈을 쓴다면 이 요구가 무시되지 않을 것임
          + 진짜 문제는, 호텔 대부분이 한두 개 지점만 운영하는 소규모 오너임. 전국 브랜드 이미지는 손상돼도 본인은 수익을 챙길 수 있으니 단기 이득을 좇음. 그래서 Hyatt 같은 브랜드는 기준과 통제를 두는 것임
          + Hyatt 같은 체인은 실제로 호텔을 소유하지 않고, 지역마다 실소유주가 다름. 소유주 별로 품질 차이가 커서 관리가 어렵고, 고객 경험도 일관되지 않음
          + 브랜드 충성도가 무너져도, 경영진은 별 영향 없음. 산업 전반이 통합돼서 이제 소비자 권한은 유명무실해졌고, ""고객 무시, 고객 학대, 기만""이 새로운 비즈니스 모델로 굳어짐. 설령 소송이 있어도, 서비스가 마비될 만큼 의도적으로 사회 시스템을 오버로드시키기 때문에 결국 세금으로 다시 비용이 전가됨. Facebook 같은 초대형 기업은 이 비용조차도 사회로 전가함. 소규모 브랜드의 경우에도 오너·투자자·경영진이 단기 이득에만 신경 쓰면 브랜드 몰락 후에도 보너스 받고 승진하는 구조임. 결국 이런 리더십이 판을 치니 직원들도 작은 사기·태업 등 ‘조용한 저항’에 나서게 됨. 결국 사회적 신뢰(사회계약)가 다양한 층위에서 붕괴되고 있음
          + 때로는, 이런 사기를 벌이는 사람들이 실제로 피해자보다 ‘더 똑똑하다’는 우월감에 돈을 쓰는 거란 생각도 듦. 많이 번다고 해도, 단기적으로만 이득이고 장기적으로는 자기 이익에도 해로운 경우가 많다는 점에서 이런 행위가 더 황당함
     * 도시가 적자 보전을 목적으로 노란불을 짧게 줄여서 단속카메라로 돈을 버는 게 떠오름. 원래 카메라는 공공의 안전을 위해 설치되지만, 카메라가 비싸니 일정 수 이상의 딱지가 나와야만 비용을 뽑을 수 있음. 그런데 노란불이 짧으면 사고가 늘고, 이런 식의 수익 추구가 오히려 시민 안전을 악화시키는 거임
          + 시장은 무언가 ‘가치’를 효율적으로 추출할 수 있지만, 그 ‘가치’가 무엇인지는 사전에 결정되어야 함. 신호등 사례에서는 안전(공공의 가치)이 아닌 돈이 가치가 되어버림. 본래 신호등은 흐름과 안전을 위해 만들어지는 것이지, 수익 창출이 목적이어선 안 됨. 공공 선을 담당하는 조직과 사적 수익만 노리는 민간 기업이 협력하면, 서로 기대하는 성과가 달라 인센티브가 꼬이고 결국 잘못된 결과만 남음
"
"https://news.hada.io/topic?id=22014","Ask GN: 여러분들은 혹시 AI브라우저를 사용하시나요?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Ask GN: 여러분들은 혹시 AI브라우저를 사용하시나요?

   AI답변 링크는 아래와 같습니다.
     * https://getliner.com/ko/search/…

   위 링크의 내용을 요약해서 브라우저 이름과 장단점으로 만들어 표로 만들어 달라고 AI에게 요청했더니 아래 링크와 같이,,,
     * https://getliner.com/ko/search/…

   위 요약 내용의 표에서 아래와 같이 AI브라우저 이름만 가져오면 다음과 같습니다.
     * 퍼플렉시티 코멧 (Perplexity Comet)
     * 오픈AI 브라우저
     * 디아 (Dia)
     * 시그마 AI 브라우저 (Sigma AI Browser)
     * 브라우저베이스 (Browserbase)
     * 젠스파크 AI 브라우저 (Genspark AI Browser)
     * 폴리 (Poly)
     * 오페라 네온 (Opera Neon)
     * 퀘타 (Quetta)
     * 펠로우 (Fellou)
     * 퓨 AI 탭 (Phew AI Tabs)
     * 마이크로소프트 엣지 (Microsoft Edge)
     * 오페라 (Opera)
     * 브레이브 (Brave)
     * 구글 크롬 (Google Chrome)

   위 열거된 브라우저 중에 의외로 기본 브라우저를 제외해도 새로운 AI브라우저들이 많이 있네요. 실제로 다운받으러 가면 아직까지는 맥OS만 지원하는 경우도 있고 또는 waitliast에 등록만 가능한 것도 있더군요.

   여러분들은 혹시 AI브라우저를 사용하시나요?
   그렇다면, 어떤 걸 사용 중이신가요?
   경험담으로 장단점을 공유 부탁드립니다.
   혹은,
   사용할 예정이라면 어떤 것을 사용할 예정인가요?
   그리고 그 이유는 무엇인가요?

   공유 부탁드립니다.

   Arc 정말 잘 쓰고 있었는데, 더 이상 개발하지 않는다니 아쉬워요.

   요즘 브라우저에 다 AI가 붙어있긴하죠.

   다만, 에이전트 이런거는 아직이라서 그냥 Arc 쓰고있어요

   Arc 만든 browser company 라는 회사가 Arc Max라는 브라우저를 출시했고 현재는 기존 출시한 브라우저의 개발은 중단하고 새로운 AI브라우저인 Dia 를 개발해서 맥용으로는 이미 출시한 것으로 알고 있습니다.

   ARC MAX는 다른 브라우저는 아니고, AI 기능이에요.

   Dia는 macOS용으로는 있는거는 아직 Waitlist에요

   어? 아직 waitlist인가요? 저는 다운 받아서 현재 잘 사용하고 있는데,,,

   fellou ai브라우저가 윈도우용은 invite-code 가 있어야 하고 젠스파크 ai브라우저는 mac용으로 이미 출시했고 윈도우용은 아직이었네요.

   fellou ai도 아직은 과대 광고죠..

   아 그렇군요. 정보 감사합니다.
"
"https://news.hada.io/topic?id=22065","Show GN: 프론트엔드 JS 없이 구현하는 깃허브 블로그 댓글 기능","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Show GN: 프론트엔드 JS 없이 구현하는 깃허브 블로그 댓글 기능

   프론트엔드에서 어떠한 JS 코드도 서빙하지 않는 블로그를 목표로 시작했습니다.

   기존 블로그에 giscus를 사용한 댓글 기능이 있어,
   JS 없이 댓글 기능을 구현하는 것이 큰 걸림돌이었는데요.

   Cloudflare Workers와 GitHub Actions를 사용하여 댓글 기능을 구현했습니다.

   아래의 흐름에 따라 작동합니다.
    1. 클라우드플레어 워커에 호스팅된 댓글 폼을 출력합니다.
    2. 사용자는 해당 댓글 폼을 통해 깃허브 계정으로 로그인하고, 댓글을 제출합니다.
    3. 워커는 제출받은 댓글 내용을 이스케이프하고, 깃허브 액션의 workflow dispatches API를 통해 그 내용을 깃허브로 전달합니다.
    4. 깃허브 액션은 전달받은 내용을 토대로 게시글 HTML 파일에 댓글 코드를 추가하고, 그 변경사항을 커밋합니다.
    5. 커밋된 결과물은 깃허브 페이지 액션의 배포 절차에 따라 프로덕션에 반영됩니다.

   3-5번의 절차에 따라 반영되는데 걸리는 약간의 지연시간은...

     그냥 안내 문구 하나 추가하는 것으로 퉁쳤다. 워드프레스 같은 블로그 플랫폼에는 ""관리자 검토 후 댓글 공개"" 같은 기능이 보편화 되어 있으므로, 그 정도 딜레이는 관리자가 검토하는 척 하기로 했다. 이 글을 보는 당신만 모른척하면 된다.

   UX 측면에서 해??결했습니다ㅎㅎ

   개인 수준의 작은 블로그에서, 굳이 굳이 JS를 사용하지 않겠다는 목표가 있었을 때에나 유의미한 방향이라, 그저 재미로 봐 주셨으면 좋겠네요😅

   아래 링크에서 직접 테스트 해 볼 수 있습니다!
   https://nemorize.me/blog/202507/blog-renewal

     그냥 안내 문구 하나 추가하는 것으로 퉁쳤다. 워드프레스 같은 블로그 플랫폼에는 ""관리자 검토 후 댓글 공개"" 같은 기능이 보편화 되어 있으므로, 그 정도 딜레이는 관리자가 검토하는 척 하기로 했다. 이 글을 보는 당신만 모른척하면 된다. <<

   천잰데요..?ㅋㅋㅋㅋㅋㅋㅋ

   문득 이런 생각도 llm이 해낼 수 있을까 궁금증이 생기네요 ㅋㅋ

   지연시간 해결 방법이 참신하고 재밌네요 👏

   일종의 headless cms인가요? 재미있습니다.
"
"https://news.hada.io/topic?id=22006","Comcast가 싫어서 직접 동네 광통신 인터넷 회사(ISP)를 차린 두 사람","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Comcast가 싫어서 직접 동네 광통신 인터넷 회사(ISP)를 차린 두 사람

     * Prime-One의 창업자 두 명은 Comcast의 불만족스러운 서비스에 지쳐, 직접 광섬유 ISP 회사를 설립해 미시간 주 Saline 지역에서 Comcast와 경쟁 중임
     * 전 구간 지하 광섬유 구축, 장비 무료 제공, 대칭 기가비트 속도와 무제한 데이터, 무약정 조건을 내세워 월 80달러 요금제로 차별화
     * Prime-One은 1,500가구, 약 75마일 규모의 광케이블망을 이미 구축했으며, 고객 100명 이상을 확보하고 향후 4,000가구로 확장할 계획
     * 고객 지원은 현지 인력 기반 전화·채팅·방문 대응이 가능하며, 장애 시 빠른 복구와 시간당 5달러 보상 정책 등 차별화된 서비스를 제공
     * Comcast도 요금 할인, 무제한 데이터 요금제 도입 등으로 맞대응 중이나, 지역 주민들은 투명한 요금과 새로운 선택지에 큰 만족감을 보이고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Prime-One 설립 배경

     * Samuel Herman과 Alexander Baciu는 Comcast의 느린 업로드 속도와 반복적인 문제로 인한 불편을 겪었음
     * Saline 지역에는 Comcast 외에 경쟁할 광통신 회사가 없었으며, 기존 건설업 경험을 활용해 직접 광섬유 인터넷 회사를 만들기로 결심함
     * Baciu는 Herman의 매형으로, 두 사람 모두 가족 회사에서 ISP 네트워크 구축 관련 일을 해온 경력이 있음

사업 모델 및 서비스 특징

     * Prime-One은 100% 지하 매설형 광케이블만 사용, 신뢰성 높은 인프라 구축을 목표로 삼음
     * 고객에게는 모뎀, ONT, Wi-Fi 라우터 등 모든 장비와 설치를 무상 제공하며, 데이터 사용 제한이나 계약 기간 없음
     * 요금제는 기가비트 80달러, 500Mbps 75달러, 2Gbps 95달러, 5Gbps 110달러로 구성됨
     * 첫 30일은 무료 체험 제공, 추가 비용이나 숨겨진 요금 없음

성장 현황과 지역 반응

     * 2025년 1월부터 서비스 개시, 현재 Saline 지역 1,500가구와 약 75마일 광케이블 구축 완료, 100여 명 고객 확보
     * 단독주택 중심으로 서비스를 시작했으나, 향후 다가구 주택과 인근 마을까지 확장 계획을 밝힘
     * 지역 주민들은 새로운 선택지의 등장과 투명한 요금 구조에 매우 긍정적 반응을 보이고 있음
     * 고객의 약 30% 유치 시 수익성 확보 목표를 갖고 있음

현지 기반 고객 서비스

     * Prime-One은 15명의 지역 직원(설치·기술자, 고객지원, 운영 등)을 두고 직접적인 서비스 제공에 집중함
     * 전화·채팅·방문 지원 모두 가능, 장애 발생 시 2~4시간 내 복구 보장, 시간당 5달러 환불 보상 정책 운영
     * 실제 장애는 심각한 악천후에 의한 한 번뿐이었음

시장 경쟁 및 Comcast의 대응

     * Prime-One이 등장하자 Comcast는 할인 요금, 5년 약정, 무제한 데이터 제공 등으로 기존 고객 붙잡기에 나섬
     * Comcast도 최근 전국적으로 무제한 데이터 요금제를 확대 도입 중이나, 기존 고객은 별도 전환이 필요함
     * Metronet, Frontier 등도 인근에서 광케이블망 구축을 확대 중이나 Prime-One 서비스 지역과는 일부 겹치지 않음

창업과 기술적 조언

     * Prime-One의 두 창업자는 초기 네트워크 설계 및 OSS/BSS 소프트웨어, 다크파이버 관리 등에서 외부 전문가와 협업함
     * 미시간에서 직접 광통신 ISP를 만든 Jared Mauch와도 교류하며 노하우를 공유받음

지역 커뮤니티의 변화

     * Reddit 등 커뮤니티에서도 투명한 요금, 무제한 데이터, 첫 달 무료 등 Prime-One의 차별화가 화제가 됨
     * 실제 고객은 예전 Comcast 요금제에서 불필요한 추가 요금, 데이터 초과 비용 등의 불만이 해소됐다고 평가함
     * 지역 정부도 최근 들어 광케이블 건설이 활발해졌다고 밝힘

미래 계획

     * Prime-One은 현재 4,000가구 규모의 1차 확장 이후, 더 넓은 지역으로 단계적 확장을 계획 중임
     * 구체적 세부 계획은 아직 비공개이나, 지역 기반 중소 ISP의 성장 모델로 주목받고 있음

   미국은 땅이 넓어서 그런지 이런 시도도 가능하군요. 흥미롭네요.

        Hacker News 의견

     * 나는 그들의 고객임. 가끔 근처 도로에 주차된 그린색 차를 자주 목격함. 서비스 품질은 꽤 괜찮음. 제공된 라우터는 완전히 잠겨 있고 cgnat를 사용하지만 Comcast의 1.2TB 데이터 캡을 피할 수 있다는 점에서 확실히 가치가 있음. 현재 Comcast 웹사이트를 확인해 보니 “무제한” 데이터 옵션을 내걸고 있음. 6개월 전엔 이런 게 없었음. 이 회사가 지금까지 들인 노력에 비해 고객 수가 약 100명 밖에 안 된다는 게 좀 적게 느껴짐. 1년 가까이 우리 집 주변 모든 도로에서 케이블을 설치 중이며, 매일 파이버 관을 매설하는 모습이 보임. 이 동네 집들은 상당히 떨어져 있음. 그래도 잘 운영해나가길 바람
          + 내 경험도 거의 비슷함. 제공된 라우터가 완벽하게 잠겨 있고 cgnat이 적용되어 있음. IPv6 지원도 안 됨. 대부분의 파이버 제공 업체가 그렇듯이. 나는 파이어월을 직접 구축하기 때문에 라우터는 상관 없음. cgnat을 피하려고 월 10달러 추가 지불 중임. 그리고 이 회사에 꾸준히 IPv6 서비스 도입을 강하게 요청하고 있음. bgp.he.net에서 확인해보면 이들은 /40 블록을 할당받았지만 실제로 사용하고 있진 않은 듯함
          + Comcast의 무제한 데이터 옵션은 데이터 캡 도입을 발표할 때부터 존재했던 것으로 기억함. 최신 하드웨어 임대 옵션에만 한정됐다고 생각했지만, 내 경우 2016년 데이터 캡 도입 안내 메일에서 추가 요금을 내면 무제한 옵션을 선택할 수 있다는 내용이 있었음
          + 나는 미국 반대편에서 Cox의 고객이었음. 10년 넘게 이용하다가 데이터 캡이 도입되어서 해지함. 마침 wyyred가 동네에 들어와서 더 빠른 속도, 데이터 캡 없이, 절반 가격의 파이버를 제공함. 고민할 필요도 없이 바로 갈아탐. 요즘 Cox도 무료 무제한 데이터 광고를 하고 있던데 너무 늦은 감이 있음
          + 제공 라우터가 완전히 잠겨 있고 cgnat이라면, 사실 Comcast보다 딱히 나을 건 없고, 그냥 다른 식으로 불편할 뿐임
     * “모든 공사가 지하 매설 방식”이라는 점에서, 이 지역의 지방 정부와 주 정부가 기존 케이블 기업에 장악되지 않았다는 신호임. 만약 기존 ISP 이익에 장악된 주 정부라면, 신규 인프라 구축을 방해하는 법안을 통과시키고, 지방 정부도 허가 자체를 안 내주거나 시간을 질질 끌어서 신생 업체를 파산시키려고 시도함. 신규 파이버망은 대개 도로 굴착(트렌칭) 아니면 전신주(폴) 설치 중 택해야 함. 두 방식 모두 지방정부가 허가를 내주어야 하고, 전신주의 경우 폴 소유자와 협조가 필요하므로 추가적인 통신규제위원회(PSC)가 개입될 수 있음. 폴 소유자가 비협조적이면 지상 인프라 설치가 중단되거나 실패로 끝나는 경우도 있고, 일부 PSC만 폴 소유자 협력을 강제함. 실질적 규제 실패와 같은 사례도 많음
          + 파이버 도입과 관련해서 “기존 업체에 장악된 정부 얘기”를 20년 넘게 들어왔지만, 실제로 파이버망 개선에 아무 진전이 없었음. 정부가 건설 자체를 너무 느리고 어렵게 만들어서 그렇지, 문제의 본질은 그게 아님. 예를 들어 Bay Area가 파이버 도입에 앞장서지 못하는 이유는 Comcast가 Google, Apple보다 더 큰 영향력이 있어서가 아니라, Bay Area 전체 인프라가 다 구림. 나도 집에 Comcast 파이버 설치하는데 수개월이 걸렸음. 이미 Comcast 케이블 라인이 있는 기존 전신주에 추가로 연결하는 건데도 그랬음. 내 카운티는 그나마 허가 프로세스가 꽤 효율적임에도 이랬음. 미국 자치구는 누가 뭘 짓는 걸 전반적으로 싫어하는 문화임
          + Google Fiber도 이와 같은 규제 장벽 때문에 큰 난관에 부딪혔음. 기존 ISP들이 파이버 설치를 지속적으로 방해했고, 자본력이 많은 Google조차도 이런 비협조에 크게 막힘
          + 이 회사가 주로 시골 지역에 깔고 있는 것 같음. 시골은 도심지보다 공사나 허가가 훨씬 쉽고 빠를 수 있음. 나도 예전에 Telco 프로젝터 매니저를 했었는데, 도시에서 파이버 공사하려면 얼마나 복잡한지 뼈저리게 알게 됨. 도로 밑에 온갖 설비들이 얽혀 있어서, 트렌칭 작업 계획 세울 때 정말 힘들었음
          + 지상 전신주와 지하 굴착 방식에는 명확한 트레이드오프가 있음. 나는 한 때 건설 현장에서 직접 땅을 팠고, 독일 출신이라 약간 편견이 깔릴 수 있음.
               o 전신주/지상 방식의 장점은 저렴하고 빠르게 GPON 파이버를 구축할 수 있다는 점임. 루마니아같이 시골 마을에도 1Gbps 파이버가 빠른 원인임. 유지보수 접근도 쉬움
               o 전신주/지상 방식의 단점은 미관이 너무 안 좋고, 음주운전자, 철새 사격 등 각종 훼손 위험에 노출됨 관련 링크 1, 관련 링크 2
               o 지하 매설(트렌칭) 방식의 장점은 내구성이 엄청남. 자연재해 급의 홍수나 지하 배전실이 완전히 잠기지 않는 이상 거의 무너질 일이 없음. 외관도 훨씬 깔끔함
               o 단점으로는 비용이 높고, 전문 인력과 장비가 부족함. 인허가 등 관료적 절차와 교통관리 등도 복잡해짐. 땅 밑이 복잡하게 얽혀있는 도심지에선 수작업이 필수일 때도 많음
     * 나는 텍사스 시골 지역에 살고 있는데, 최근에 파이버를 사용할 수 있게 됨. 다른 경쟁 업체로는 ADSL, DOCSIS 기반 사업자인 AT&T, Optimum이 있음. 내가 Optimum으로 갈아탄 다음날, Optimum 지역 전체가 Comcast에 인수되었고, Comcast는 모든 주요 유틸리티를 최소 두 번, 그리고 내 파이버 연결은 세 번이나 고장냄. 인프라 작업하다가 남의 시설 자주 망가뜨림. Optimum이 결국 잘된 선택임. 대부분의 사람들이 $80 내고 ""뻥 없는"" 고정형 인터넷을 쓸 수 있는데, 일련의 문제를 일으킨 Comcast를 선호할 이유가 없음. 특히 최근에 수도관 사고도 터졌으니. 텍사스는 FTTP(파이버 투 더 프레미스) 공급사가 완전히 게임을 지배하고 있음. 500~1000가구도 한 달 이내 시공이 끝나고, 방향성 드릴링 기술과 811 규정 무시(!)로 공사를 빠르게 마무리함. 경쟁하는 파이버 공급사도 있을 정도임.
       시골 숲속에 사는데도 5Gbps 대칭 인터넷을 월 $110에 쓰고 있음. 나무가 송전선 구간을 뚫고 들어오는데, 파이버 인프라는 단 한 번도 영향 없었음. 이제 남은 마지막 매설 유틸리티는 전기임. 이것도 일부 지역에서 준비 중임
          + “811 규정 무시” 언급이 재미있음. 텍사스의 개발 규제 수준이 독특함. 내 친구가 하는 말로는, 여기선 규제나 구역제의 유일한 역할을 HOA가 담당함. HOA(집주인 협회)가 긍정적으로 언급되는 걸 처음 봤음
     * Bay Area에도 이런 파이버 사업이 들어오길 아직도 기다리고 있음. 실제로 북부 일부에 Sonic이 있긴 한데, 전체적으로 파이버 선택권이 거의 없음. 내 집 앞 경계석에 AT&T가 광케이블을 설치한 지 10년이 넘었지만, 최초 Google Fiber의 (잠정적) 경쟁 압력 때문이었던 듯함. 그런데 구글이 도시와의 난관으로 전략을 포기하자, 그 광케이블은 켜지지도 않고 방치됨
          + Sonic이 SF Bay Area에서 실제로 파이버 직접 구축 중임. 예전엔 AT&T 리셀러였으나 이제 자체 파이버를 깔고 있음. 요금은 기존보다 50% 저렴, BYO(자체 라우터 사용), 진짜로 잘 작동하는 IPv6, 그리고 훌륭한 서비스임
          + 산호세 다운타운에선 AT&T와 Sonic 파이버 모두 이용 가능함. 나는 AT&T에서 Sonic으로 갈아탔고, 서비스에 매우 만족 중임. 요금은 예전의 절반, 속도는 10배, 고객센터 역시 월등히 나아짐
          + 메릴랜드 변두리 집에 이미 파이버 2라인이 들어와있는데, Bay Area에 아직 아무 것도 없는 동네가 많다는 게 정말 웃김
          + 샌프란시스코 대부분과 이스트 베이 일부 지역엔 MonkeyBrains라는 업체가 있음 MonkeyBrains 보기
          + 단지 기술적 부채나 조직 내부의 관료주의 문제일 수도 있음. 최근 Menlo Park로 이사 왔는데, ATT 파이버 2.5Gbps 단박에 개통했음
     * ISP 창업 기사들을 보면 항상 물리적인 인프라에 대한 얘기가 중심이 됨. 요즘 ISP광고는 와이파이 속도를 강조하는데, 실제 운영시 고객 지원 부담이 어떻게 분포될지 궁금해짐. 진짜 ISP 측 장애와 고객 장비 쪽 문제 비율이 궁금함
          + 10년 전 DSL 서비스 센터 경험에 따르면, 대부분의 고객 문의는 “와이파이가 벽(특히 두꺼운 철제벽)을 못 뚫는다”, “이메일/와이파이 비밀번호를 잃어버렸다”, “와이파이 신호가 약하다” 등 사소한 사용법 문제였음. 특히 802.11n 표준 이전 수많은 “draft n” 장비들이 문제가 심각했음. 나중에는 고객이 직접 DSL 설치하도록 했기 때문에, 전화상담 시간 절반은 RJ11 구분조차 못하는 분들에게 플러그 설치를 안내하는 데 쓰였음. “이메일 비번이 페이스북엔 안 된다”거나 “USB 프린터가 안 된다”는 질문도 많았음. 기술 지원 대상이 누군지 몰라서 ISP로 전화를 돌리는 셈. 종종 “인터넷 고장” 문의는 사이트 디자인이 바뀐 것 때문이기도 했음. 일단 모뎀/라우터가 설치되고 나면 대부분의 인터넷 서비스는 별 문제 없이 잘 작동함. 전체 장애나 나쁜
            모뎀, 가끔 시스템 소프트웨어 업데이트 문제 등 진짜 ISP 책임 장애는 굉장히 소수임
          + 10년 전 얘기지만, 나는 소규모 지방 ISP 등 24/7 아웃소싱 헬프데스크에서 근무함. 3년 경험상, 75% 이상이 실제 장비 문제가 아님. 이메일 세팅, 리모컨 입력/source 잘못 눌러서 셋탑박스 신호 변경, 라우터 재부팅, DNS/윈속 리셋 등 사용자 조작 오류가 대다수였음
          + ISP만큼 고객의 모든 사용 문제까지 다 책임지는 모델은 독특함. 싱크대 막혔으면 수도국 아닌 배관공을 부르고, 전기 설비는 전기기술자를 부르지만, 인터넷에선 ISP가 소비자 쪽 장비까지 모두 책임짐. “홈 인터넷 배관공” 같은 서비스가 왜 없는지 궁금함
          + 파이버망은 DSL이나 케이블에 비해 장애 발생률이 월등히 낮음. 예전엔 구리선이 물에 젖으면 간섭, 신호 약화 등 문제가 생겼지만, 파이버는 이런 문제와 무관함. 온통 젤로 채운 케이블이나 방수 테이프 사용으로 물 침입도 큰 문제 아님. 낙뢰도 큰 이슈가 아님. 그래서 고객 ONU/ONT만 잘 선정하면 소규모 네트워크에선 몇 주씩 고객 지원 이슈가 없는 경우도 있음. 요즘 가장 큰 골칫거리는 집 내부의 무선 커버리지임. 대부분의 사람들은 무선 환경 개선에 아주 적은 비용조차 투자하려 하지 않음. 가장 심각한 케이스는 실외 무선 CCTV를 따로 분리하지 않고 설치해 전체 AP의 대역폭을 소모해버리는 경우임
          + 우리 집 전기와 수도는 인터넷보다 훨씬 신뢰성이 높음. 물론 나는 ISP 측 과실이 100%인 경우에만 연락한 적 있음. HN 이용자들은 일반 대중과 비교해 예외적인 경우가 많다고 생각함
     * 이런 파이버 창업 사례를 보면서 언제나 의문이 생김. “이 지역에 독점 업체가 있다면, 도대체 어떻게 그들이 독점하게 됐는가? 애초에 경쟁하려던 다른 플레이어들은 다 어디로 갔는가?” 이런 질문이 듦. 그런데 기사들은 언제나 “야, 우리도 한번 경쟁사 시작해보자!”가 혁신인 양 다룸. 경쟁이 마치 아무도 시도하지 않은 아이디어처럼 다뤄지는 게 신기함
          + 전화, 케이블 TV 회사는 본래 대부분의 지역에서 규제 독점 사업자로 설계됐음. 그러다 기존 회선으로 인터넷 제공하는 게 신규 업체가 완전 새로 구축하는 것보다 훨씬 저렴하게 되어버림
          + 이런 기사에서 매각 언급을 본 적 없는 듯함. 그래서 프랜차이즈 ISP 모델 같은 걸 생각해봄. 예를 들어 Comcast가 특정 지역 서비스 불가라면 “데이터센터 트래픽을 지역에 보장해주고, 설치와 유지보수는 네가 알아서 해” 같은 방식으로 지역 업체에 넘길 수도 있음. 결국 설치와 유지만 했으면 더 쉽지 않았을까 싶은데, 아마 기존 대기업 입장에선 굳이 손해 볼 필요가 없으니 안 하는 거라고 생각함
          + “경쟁하려던 사람들은 어디 갔는가?”에 대해선, 단순히 아이디어만 있는 것과 기술/자본/운영 역량이 다 갖춰진 사이에는 엄청난 격차가 있음. 그리고 성공해야 한다는 추가 장벽도 있음. 실제로도 대규모 자본과 역량을 갖춘 이들도 계속 적자임. 다만 전반적으로, 이런 류의 기사들은 성공 사례를 일부러 골라 소개하며, 항상 “작은 자가 이긴다”는 식의 낙관적인 스토리로 쓰여짐. 독자 타깃이 이미 큰 통신사에 불만 많은 테크업계 사람들이라서 그런 듯함
          + ISP 분야에선 본업으로 하기에 돈이 별로 안 남고, 개인이 하기에 너무 크고 복잡함. 물리적 배선이 가장 큰 문제임. 만약 정부가 루프 언번들링(망 공유)을 도입했다면, 약간의 서류 작업과 비용만으로 기존 망을 빌릴 수 있어 훨씬 용이함. 그게 아니거나 구리선 품질이 안 좋으면 직접 전 구역에 케이블을 깔아야 하고, 이건 법적/노무적 난이도가 상당한데다 재미없는 일임. 이러니 무선 ISP가 인기 있는 편임. 품질은 떨어지더라도, 지점간 구축만 하면 돼서 진입장벽이 낮음. 모두가 자신만의 방식을 한번쯤 고민해 보길 추천함
     * 요즘에는 유선 인프라 투자가 다시 주목받아서 반가움. Verizon, T-Mobile 등 대기업들은 무선만 밀고 신규 유선 구축을 하지 않는 추세인 듯함. 아마 설비 투자 부담이 적기 때문이라 생각함. Prime-One에 투자할 방법이 있다면 해 보고 싶을 정도로 이 회사는 준비가 잘 된 것 같음
          + 대기업들이 무선만 미는 건 이미 대규모 셀룰러 인프라를 깔아놔서 무선을 선호하는 것임. 텔레콤 입장에선 아주 합리적임
     * Saline은 Ann Arbor 인접 도시로, Ypsilanti까지 합쳐 Ann Arbor/UMich 공동 번영지역임. Saline은 개인 ISP가 파이버를 세우는 데 꼭 어울리는 동네임. 대도시에서 한참 떨어진 외곽 분위기에, 기술에 밝은 사람이 많은 것이 특징임
     * “$80에 기가빗 파이버, 무제한 데이터” 정책이 내겐 불만임. 더 저렴한 선택지가 있었으면 함. 기가빗까지 필요 없으니 300Mbps에 $30 무제한 같은 옵션이 있다면 당장 선택할 텐데, 현재는 그런 게 없음. 지금은 Optimum에 $40 내고 있음
     * 가격이 예상보다 비싸서 놀라움. 뉴질랜드는 인터넷 요금이 비싼 걸로 유명하다고 생각했는데, 미국 가격이 NZ와 거의 비슷함. 게다가 미국은 USD 기준이라 환율 적용 시 체감상 더 비쌈
"
"https://news.hada.io/topic?id=22073","암 DNA, 진단 몇 년 전부터 혈액에서 탐지 가능","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      암 DNA, 진단 몇 년 전부터 혈액에서 탐지 가능

     * 암의 유전적 흔적이 질병 진단 수년 전 이미 혈액 속에서 탐지될 수 있음
     * 연구자들은 고감도·정확한 기술로 극미량의 종양 DNA를 확인, 조기 암 선별 도구로서 가능성이 있다는 걸 제시
     * 과거 채취된 혈액 샘플 분석에서, 일부 참가자는 진단 전 수개월~3년 이상 전에 이미 암 DNA 흔적이 검출됨
     * 전체 유전체 시퀀싱 등 첨단 분석법을 적용해 극소량의 암 특이적 변이까지 포착함
     * 더 많은 대규모 연구와 검증이 필요하지만, 임상 적용 시 암 조기 발견 및 치료 기회 대폭 확대 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

암 DNA, 진단 전 혈액에서 탐지되는 현상

     * 암의 유전적 지문이 환자가 진단받기 수년 전 혈액 내에 존재할 수 있음
     * 존스홉킨스대 연구팀은 암 진단 3년 이상 전 채취된 혈액 샘플에서 종양 DNA 흔적을 탐지함
     * 기존에도 암세포가 DNA 조각을 혈류에 방출한다는 사실은 알려져 있었지만, 극미량인 초기 발견은 매우 어렵다고 알려져 있었음

연구 개요 및 결과

     * 연구팀은 1980~1990년대 장기 코호트 연구에서 보관된 샘플을 활용함
     * 암 진단 6개월 이내 참가자 26명 중 8명의 혈액에서 암 유전자 신호가 검출됨
     * 더 나아가 진단 3년 이상 전 샘플에서도 전체 유전체 시퀀싱(Whole Genome Sequencing) 기술로 암 특이적 DNA 변이를 확인함
     * 소량의 혈장(한 티스푼)만으로도 변이 탐지에 성공, 더 신선하고 대용량 샘플에서는 민감도가 더욱 향상될 가능성 시사

기술적 도전과 해석

     * 사용된 혈액 샘플이 40년 전 보관된 것으로, DNA 보존에 최적화된 조건이 아니었음에도 결과를 얻음
     * 암 특이적 변이가 극소량이어서 기존 검사로는 놓쳤을 정보까지 포착함
     * 더 정교한 샘플·기술 발전 시 조기 암 발견율 개선 기대

임상적 함의와 미래 전망

     * 조기 진단 시 수개월~수년 치료 개입 가능성 확대, 환자 생존율 향상 기대
     * 대중화·임상 적용까지는 추가 대규모 검증 필요함
     * Star Trek 스타일 미래처럼, 혈액 내 암 DNA 발견 시 사전 치료도 언급됨
     * 연구팀은 “더 많은 환자 샘플로 추가 연구 진행 중”이라 밝힘

        Hacker News 의견

     * 저는 지난 10년간 circulating-tumor DNA(순환 종양 DNA, ctDNA) 연구에 집중해온 경험을 바탕으로 몇 가지 생각을 나눔
          + 암은 진단보다 훨씬 이전부터 서서히 자라나기 시작함
          + ctDNA의 핵심 과제는 '유용한' 민감도와 특이도 확보임
          + 예를 들어, 혈장 DNA에서 암 관련 유전자 변이를 초심도 시퀀싱으로 찾아내고 필터링하는데, 만약 TP53 변이가 소량 검출된다면 실제로 어떤 조치를 취할 수 있을지 판단 어려움
          + 많은 사람들이 나이가 들며 somatic mutation(체세포 변이)을 몸 전체에 조금씩 쌓음
          + 50세 이상 대부분은 식도, 전립선, 혈액에서 CHIP 등의 전암 클론이 있음
          + 인기 있는 MCED(Multi-Cancer Early Detection) 검사들은 여러 신호를 조합해 민감도와 특이도를 보완하는데, 인구 전체를 대상으로 활용하긴 아직 부족하다고 생각함
          + MCED의 현재 민감도-특이도 수준에선 후속 진단 비용 부담이 너무 커서 경제성이 안 맞음
          + 단계별(MCED → 더 정밀한 비침습 검사)로 특이도를 높여가면서 비용을 절감하는 전략이 가능성 있을 것임(Harbinger Health 등)
          + 이런 건 예방적 전신 MRI 스캔과 비슷함
               o 노이즈가 엄청 많고, 압도적인 데이터라 지금은 연구 단계에 불과함
               o 단기적인 임상 적용보다는 오히려 해가 더 크다고 생각함
               o 하지만, 이런 데이터를 활용해 모두의 건강을 비약적으로 개선하는 파이프라인을 만들 가능성은 분명히 있음
               o 이를 위해선 수백만 명을 대상으로 장기적인 데이터(매년 MRI, 유전자 시퀀싱, 혈액 검사) 축적이 필요함
               o 실제로 진단 가치는 아주 큰 표본에서 통계적으로만 드러남
               o 우리가 찾아낼 유의미한 인사이트 대부분은 예기치 않게 발견될 확률이 높음
               o 결국, 고차원 대규모 데이터를 머신러닝으로 돌려야 진짜 신호를 찾을 수 있음
               o 14년 뒤 암 진단될 사람을 오랜 기간 추적해 지표 차이를 모델링해야 함
               o 지금은 이런 분석이 가능한 기술이 존재함
               o 그러나 현실은 미국 시스템은 병이 상당히 진행된 환자만을 대상으로 수익성 의료체계로 운영됨
               o 대규모 장기 임상시험은 너무 비싸고, 책임 문제로 임상시험 중 발견되는 사소한 의심병변도 모두 환자에게 알리고 조처해야 하기에 제대로 데이터가 쌓이지 않음
               o 미국은 이런 임상시험을 시스템적으로 수행할 수 없고, 오히려 영국이나 중국이 할 수 있을 것임
          + 진단 기술의 발전 속도가 실제 치료법 확보 속도를 따라가지 못하고 있다고 봄
               o 전암 단계 탐지엔 능숙해져 가지만, 정작 대응책은 기존 항암화학요법이나 수술이 전부라 상당히 부작용과 비용, 삶의 질 저하를 초래함
               o 만약 효율적이고 안전한 예방 치료법이 있었고, 위양성이 좀 있어도 위험이 적었다면, 검사 양성자에게 쉽게 제공할 수 있을 텐데 현재는 꿈 같은 이야기임
               o 생활습관 개입 등도 근거가 충분하다면 도입할 수 있겠음
               o 이상은 아직 갈 길이 멀다고 생각함
          + ctDNA 기반 도구들이 수술 후 보조(추가) 치료 결정에 쓸 만큼 충분히 민감하고 특이적인지 궁금함
               o 예를 들어, 수술로 암을 모두 제거(R0 절제)했는지 아니면 항암치료 및 추가 약물 투여 필요성을 판정할 수 있는가 궁금함
          + 이런 분야에서 발생하는 base rate(기본율) 문제에 주목함
               o 아무리 우수한 검사도 직감적으로 양성 판정 받은 대부분이 관련 질환으로 진행하지 않을 가능성이 더 높음
          + 젊은 나이부터 주기적으로 혈액 검사를 해놓고, 수치 변화(델타)를 추적하면 더 민감하게 특이 신호 이상을 포착할 수 있지 않을지 제안함
               o 처음부터 높은 값이 아니라, 이후 새롭게 나타난 특이 지표가 생기면 더 주목해서 탐지 및 대응 가능성 있을 것이라고 생각함
     * 미국 건강보험은 대부분 예방적 진료를 비용 문제로 잘 보장하지 않음
          + 위양성(가짜 양성)으로 인한 추가 검사비 부담, 그리고 결국 몇십 년 후 실제 병이 발현되면 비용을 딴 쪽(정부)이 부담하게 된다는 계산임
          + 미국 인구 건강 상태를 볼 때, GPL-1 같은 예방적 약제를 더 많은 사람이 쓰게 하고, 효과와 지속성을 개선하는 데 적극적으로 투자할 필요성이 있다고 봄
          + 예를 들어, 이미 1억 명 넘는 미국인이 당뇨 전단계라 향후 관련 의료비용만 연간 4조 달러에 달할 수 있음
          + 코로나 백신처럼 국가적 차원에서 특허를 국유화하고 빠르고 과감하게 추진할 시급성 있다고 봄
          + 친구가 60대 중반에 은퇴 군인이라 Medicare와 Tri-Care 모두 적용받고 있음
               o 전립선 이상(PSA 12→19)이 있어 PET 스캔(7,500달러 상당)으로 암 여부를 확인하고 싶어하지만, 두 보험 모두 조기 진단 목적으로는 스캔 승인을 거부함
               o 만약 모든 사람이 cfDNA 등에서 신호가 나오면 추가 정밀 검사를 모두 원하게 될 경우, 미국 의료 시스템이 이런 중심의 진료 패턴을 감당하지 못할 것이 자명함
          + 미국에서 예방 중심 의료는 별로 중요하게 여겨지지 않는 현실이 슬픔
          + 보건 당국이 당뇨 전단계 인구에게 GLP-1 처방을 권장하는 게 쉽지 않음
               o 예를 들어, 단 한 번 맞는 코로나 백신도 의무화에 반발이 상당했음
          + 관료적으로 운영되는 의료 시스템은 어디나 비효율적임
               o 캐나다에선 일부 주에서 진료 의뢰 후 치료까지 1년 이상 대기하는 경우도 있음
               o 유럽 대부분도 비슷한 접근성 문제를 겪지만 프랑스, 네덜란드는 상대적으로 양호함
               o 미국은 복잡한 정부 규제, 공공 프로그램, 영리 기업이 뒤섞인 혼란스런 구조임
               o 2023년 기준 의료비 지출이 GDP의 18%에 달함
               o 미국은 MRI 등 진단장비는 OECD 국가 중 가장 많고, 암/심장질환 5년 생존율도 매우 높음
               o 하지만 그 외 대부분의 의료 체계도 여러 면에서 심각한 비효율이 존재함
               o 흥미롭게도, 미용 성형과 라식 등 특정 분야만은 인플레이션 대비 가격이 실제로 하락함. 본인 부담이 많고, 경쟁이 치열해진 결과임
               o 의료 혁신만이 진정으로 비용을 낮추는 유일한 동력임. 그러나 새로운 기기나 치료법은 대부분 10~30년 주기로나 바뀜
               o 만약 내가 의료를 설계한다면, 자격만 명확히 고지한다면 누구나 의료 시술을 제공할 자유를 주겠음
               o 혁신과 산업 발전을 위해선 과도한 입구 제한이 치명적임.
               o 물론 정부 개입이 전적으로 부정적인 건 아님. 공공연구(신약 개발, 의료 기법, 공개 데이터셋)에 충실히 투자하면 혁신적 가치가 창출됨
               o 민간은 딜리버리와 혁신에 집중하고, 정부는 기초연구에 전략적으로 투자하면 세대 마다 의료 혁신이 일어날 것임
          + 미국 민간 의료보험은 '의학적으로 필요'한 치료에만 비용 부담 의무가 있음
               o 실질적으로 보험사가 혜택을 누릴 수 없는 부분(예: 스타틴 계열 약물)도 실제로 보험이 부담하고 있음
     * 암을 정말 초기에 잡아낼 수 있다 해도, 보험 및 의료회사가 스크리닝 비용을 부담하기 꺼린다는 게 현실임
          + 실제로 본인이 비용 부담하면 검사 가능함
          + 암 연구자가 이렇게 설명해줌
          + 단, 다음과 같은 주의 사항이 있음
              1. 진단 과정에 매우 많은 뉘앙스가 있음. 대부분 사람은 항상 혈액에서 암세포가 아주 소량씩 발견될 수 있음
              2. 검진 자체가 5~10천 달러에 추가 추적 검사까지 따지면 비용이 크게 증가함
              3. 전체 비용이 높은데, 대량 생산 및 보편화된 검사가 아님
          + 우리는 암을 조기에 발견할 수 있지만, 동시에 비암성 소견도 많이 찾아낼 수 있음
               o 이런 새로운 검사들의 비용 대비 효과가 아직 충분히 연구되지 않아, 보험사와 의료 시스템도 부담스러워 함
               o 추가 검사가 결국 암이 아니게 되면 환자에게도 위험이 발생할 수 있음
               o 어떤 경우에는 조기 암 발견이 오히려 치료 이득이 없을 수도 있음
               o 미국, 영국 등에서는 대규모 임상시험을 통해 혈액 기반 암 검사의 가치와 적절한 보상 체계를 찾으려는 시도를 진행 중임
               o Galleri 같은 검사는 본인 부담으로 받을 수 있음(1천 달러 미만 수준), 전신 MRI는 2~5천 달러 정도임
          + 암을 조기에 발견하는 게 장기 치료 비용보다 훨씬 저렴할 것이라는 의견에 동의함
          + 위양성 비율이 높을 경우, 이런 검사가 오히려 개개인과 사회에 더 큰 해를 끼칠 수 있음
               o 불필요한 검사, 치료로 인한 실질적인 위험이 존재함
          + 위 주장의 주의사항을 보니 보험회사가 무조건 비용을 부담해야 할 명확한 근거가 없는 것 같음
          + 잘못된 상식임
               o 흔히 알려진 얘기는 '자세히 알 필요 없이, 사소한 진단에 연연하다 보면 오히려 해가 더 클 수 있다'는 것임
               o 암은 늘 일어나지만, 실제로 별다른 문제없이 지나가는 경우도 많음
     * 저는 제 일 덕분에 Cell-free DNA(CfDNA)에 대해 조금 알고 있음
          + CfDNA는 수십 년 전부터 알려져 있었으나, 최근 표적 면역항암제의 발전과 함께 큰 주목을 받게 되었음
          + CfDNA는 '액체 생검'으로도 활용 가능함(혈액만 채취해도 종양 위치와 유형을 어느 정도 파악 가능)
          + 업계에선 조만간 CfDNA 검사가 연례 건강검진의 표준 혈액검사처럼 보편화 될 거라고 봄
          + 예측/예방적 가치가 크다고 생각함
          + 검사 결과가 얼마나 실질적으로 활용될 수 있는지 궁금함
               o 아주 미미한 종양성 DNA가 검출된다면 실제로 뭘 할 수 있는지, 종양 위치도 모를 텐데 예방이 어느 정도 가능한지 궁금함
     * 지금까지 암을 예상보다 훨씬 일찍 잡아낼 수 있게 된 기술을 활용해, 우리 몸의 자연적인 암 제거 능력을 증진할 예방적 치료법을 개발할 수 있다고 봄
          + 예를 들어, 암 관련 DNA 흔적이 아직 치료 적응증은 될 정도는 아니지만 검출된 수천 명을 모아 임상시험을 진행
          + 한 그룹엔 하루 한 번 Auricularia auricula(목이버섯) 추출물 등 암 예방 가능성이 있는 소재 투여, 다른 그룹엔 위약(가짜 약)
          + 이후 조기 발견 검사를 반복해, 두 그룹의 DNA 신호 강도 차이를 비교하는 방안 제안함
          + 실제로 이런 임상시험이 성사되면 가치가 크다고 생각함
               o 아카데미와 커뮤니티 주도 필요성, IRB(임상시험 윤리위원회) 통과 여부는 미지수임
               o 동일한 아이디어를 염증 모니터링에 적용하면 매우 가치 있는 데이터가 쌓일 것이라고 생각함
     * 저희 친척이 이런 검사 결과가 양성으로 나온 적 있음
          + 이론상으론 상당히 유용한 신호지만, 실제론 바로 활용할 수 있는 경우가 드묾
          + 예전 암이 재발/전이했을 거라고 추정했으나, 실제론 원인을 찾지 못해 환자도 대기만 하다가 추적 검사만 반복함
          + 1년쯤 뒤엔 수치가 저절로 떨어졌고, 결국 아무 일도 없었음
          + 원래 신체에선 자연적으로 전암 세포가 면역에 의해 제거되기도 하며, 이러한 검사 역시 그러한 정상 과정까지 포착함
     * Galleri Multi-Cancer Early Detection test 같은 서비스를 AgelessRx에서 제공함
          + AgelessRx는 다양한 장수 치료 서비스도 제공함
          + Galleri 공식 웹사이트(https://www.galleri.com/)에서 직접 구입하면 가격이 더 저렴함(799달러 vs 949달러)
               o 매년 검사 받고 있고, 지금까지 이상 없음
          + 일부 생명보험사가 고객 서비스로 Galleri 테스트를 무료 제공하기도 함
               o 제 보험사도 결과는 알지 못한다고 해서 검사를 받았고, 통계적으로 유리한 결과 얻음
               o 장기적으로 이런 검사들의 효용성이 앞으로 어떨지 궁금함
     * 혈액 검사 기반 스타트업이 창업 아이템으로 유망하다고 생각함
          + 극소량의 혈액만으로도 이런 검사가 가능할지 궁금함
          + 이 대화가 웃겼음
               o 얼마 전 Siphox Health 관련 기사 보고 뭔지 잘 모르겠다고 생각한 기억이 남
          + function health를 봤는지 물어봄
               o 현재 유니콘 기업임
               o Quest Diagnostics와 제휴해 여러 바이오마커를 분석해주고, 결과를 ChatGPT 등의 AI로 식이/영양제 추천까지 제공함(연 2회 499달러)
               o 100가지 바이오마커를 분석하는데, 개개인이 소매로 받으면 1만 5천 달러 넘게 듦
               o 본인은 가입자이고, 서비스에 매우 만족하고 있음
     * 너무 일찍 발견한 암 신호에 대해 그 정보가 개인이 아니라 GP(주치의)에게 먼저 전달되고, 환자는 필요한 경우에만 추가 검사/치료를 결정하거나, 걱정 없이 귀가시키는 방식을 제안함
     * 기사 아카이브 링크 안내
"
"https://news.hada.io/topic?id=22092","NVIDIA, OpenReasoning-Nemotron 32B/14B/7B/1.5B 모델 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          NVIDIA, OpenReasoning-Nemotron 32B/14B/7B/1.5B 모델 공개

     * Qwen2.5-32B-Instruct(추론 모델)의 파생 모델로 수학, 코드, 과학 문제 해결 추론에 특화됨
     * 상업적/비상업적 연구 용도로 사용할 수 있음
     * 64,000 토큰의 컨텍스트 길이를 지원하며 1.5B, 7B, 14B, 32B 크기로 제공됨
     * LiveCodeBench, GPQA, MMLU-PRO 등 다양한 벤치마크에서 동급 최고 수준의 추론 성능을 기록
     * 복수 에이전트(GenSelect) 결합 추론 시, 기존 단일 모델보다 수학·코드·과학 벤치마크에서 더욱 뛰어난 성능을 달성
          + GenSelect : 여러 개의 모델 추론을 병렬로 실행한 뒤, 최적의 해법을 선택
"
"https://news.hada.io/topic?id=21969","몇몇 소수가 인터넷을 망치고 있는가?","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          몇몇 소수가 인터넷을 망치고 있는가?

   ""소수의 과잉 사용자, 인터넷을 왜곡하다""**
     * 인터넷의 독성은 소수 과잉 사용자의 활동 때문
     * 플랫폼 알고리즘이 과격 콘텐츠를 증폭
     * 대다수 사용자는 조용히 존재하지만 왜곡된 현실을 경험
     * 피드 조절만으로도 감정적 거리감 완화 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    서론: 현실과 온라인의 괴리

     * 일상은 평온하지만, 소셜미디어는 분노와 혼란으로 가득 차 있음
     * 사소한 일조차 과장되며 극단적으로 다뤄지는 현상 존재
     * 연구 결과, 이는 대부분의 사람들이 아닌 극소수 사용자 때문임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    본론

   1. 소수 사용자의 과잉 활동이 여론을 주도
     * 상위 10% 사용자가 정치 관련 트윗의 97%를 생성
     * 백신 허위정보의 경우 12개 계정이 대부분 퍼트림
     * 0.1% 사용자가 가짜뉴스의 80%를 공유

   2. 알고리즘이 극단성을 증폭
     * 소셜미디어는 사실의 반영이 아닌 왜곡된 ‘왜곡거울’ 역할
     * 플랫폼은 사용자의 참여를 늘리기 위해 분열적 콘텐츠를 우선 노출
     * 그 결과 일반 사용자들도 과장된 발언을 하도록 유도됨

   3. 사회 인식 왜곡과 그 결과
     * 사람들은 사회가 실제보다 훨씬 더 분열되었다고 인식
     * 이로 인해 플루럴리즘 무지(pluralistic ignorance)가 발생
     * 결국 잘못된 사회 규범 인식으로 개인의 행동도 변화

   4. 해결 가능성: 사용자의 선택과 알고리즘 개편
     * 사용자가 과격 계정을 언팔하면 감정적 분열이 완화됨
     * 실험 결과, 정치적 적대감이 23% 감소
     * 플랫폼이 과격 콘텐츠 대신 중립적 콘텐츠를 강조하면 구조 개선 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    결론: 건강한 디지털 환경은 가능하다

     * 소셜미디어의 독성은 일부 사용자와 시스템 구조의 결과
     * 사용자는 피드 구성과 정보 소비 태도를 바꿈으로써 통제력을 회복 가능
     * 플랫폼도 알고리즘 조정으로 대표성 있는 콘텐츠를 제공해야 함
     * 인터넷은 도구이며, 우리가 어떻게 사용하는지가 핵심임

   사용자보다 더 큰 문제는 특정 정당이랑 편먹은 언론SNS나 유튜브에 댓글달면 득달같이 달라붙는 AI봇들을 보실 수 있습니다.

   굉장히 공감되는 내용이네요. SNS도 끊었고, 유튜브 댓글도 이제는 거의 안보게 되었어요.

   유튜브 댓글에서 시비 걸고 싸움 여는 사용자들이 정말 많아서 보기가 피곤할 지경이던데 특정 사용자 차단(블락) 기능이 있으면 좋겠어요. 또 이거랑 별개로 AI가 작성하는 스팸 댓글들도 진짜 많던데 조치가 필요할 것 같습니다.

   안드로이드에선 vanced 쓰시면 댓글 UI 없애실 수 있는데 추천드려요
   웹브라우저에선 Improve Youtube나 Adguard 같은걸로 없애는거 추천드립니다
"
"https://news.hada.io/topic?id=21964","애리조나 주민, 흑사병으로 사망","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           애리조나 주민, 흑사병으로 사망

     * 애리조나의 한 주민이 흑사병으로 인해 사망함
     * 최근 몇 년 동안 미국 남서부에서 드물게 흑사병 감염 사례가 발생함
     * 보건 당국이 전염병의 추가 확산을 막기 위해 적극적으로 대응 중임
     * 사람들은 쥐벼룩이나 설치류 접촉을 피하고 위생을 철저히 지키는 것이 중요함
     * 흑사병은 적절한 치료를 받으면 대부분 완치가 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

애리조나 주민의 흑사병 사망 소식

     * 최근 애리조나 주에서 한 주민이 흑사병 감염으로 사망함
     * 미국에서 흑사병은 매우 드문 질병이지만, 뉴멕시코, 콜로라도, 애리조나 등 일부 지역에서 때때로 보고됨
     * 질병은 주로 쥐벼룩을 통해 설치류에서 사람에게 전염되는 경향이 있음
     * 보건 당국은 이와 같은 사망 사례가 확인되면 환경조사와 설치류 관리, 그리고 주민 대상 정보 제공 등으로 추가 확산을 예방함
     * 흑사병은 조기에 항생제 치료를 받으면 완치 확률이 매우 높으나, 빠른 진단과 치료가 필수임

        Hacker News 의견

     * 언론에서 제목에 나온 내용, 즉 해당 인물이 증상 발현 후 24시간 이내에 사망했다는 사실을 보도한 곳을 찾지 못했음. 여러 기사에서 공통적으로 언급하는 내용은 병원에 도착한 당일에 사망했다는 것임. 증상이 사망 24시간 전 시작되었다는 기사는 아직 본 적이 없음
          + 치료 없이 방치할 경우 평균적으로 36시간 정도 진행됨. 그러니 24시간 내 사망도 충분히 흔한 일임. 그리고 이 지역에서 드물지 않게 발생하는 전염병임
          + 이 기사 내용을 보면 이를 암시하는 것 같음: “환자가 심각한 증상을 보이며 병원에 이송됐고, 그날 바로 사망했다.” 다만 증상이 갑자기 심각해진 것일 수도 있고, 이전에 미미한 증상이 있었을 수도 있음. 그렇지만 꽤 급격하게 진행된 듯한 인상이 있긴 함
     * 예전에 시에라네바다 산맥 근처에서 캠핑하려다가, 다람쥐가 벼룩을 통해 흑사병을 옮길 수 있으니 조심하라는 표지판을 본 기억이 있음. 상당히 무서웠던 경험임
          + 사실 그렇게 심각한 사례가 아닌 경우가 많음. 아내가 유타에서 역학조사를 한 적 있는데, 포코너즈 지역에선 매년 흑사병 사례가 몇 건씩 발생함. 프레리도그로부터 옮기는 경우도 많음. 프레리도그 무리는 흑사병이 있는지 여부에 따라 분리되어 관리함
     * 미국에서 흑사병에 아내와 함께 걸린 사람을 알았음. 남편은 회복했고 아내는 사망함. 그가 말하길, 매년 한 명 정도 흑사병으로 사망한다고 들었음
          + 어떻게 노출되었는지 궁금함
     * 오늘날 우리는 예전과 비교해서 흑사병 생존에 어떤 개선점이 있는지 궁금함. 어떤 차이가 있는지 알고 싶음
          + 흑사병이 발생한 중세 유럽 시기는 의학이 아주 초보적이었음. 흑사병 의사는 향기로운 허브를 마스크 안에 넣어 방어를 시도했고, 의료 행위는 주로 거머리나 사혈이었음 (Humorism 위키). 이후 Four Thieves Vinegar에 대한 전설이 있었음 (Four Thieves Vinegar 위키). 결국 흑사병이 쥐로부터 온다는 사실을 밝혀냈고, 공중보건 목적으로 쥐를 잡기 시작했음. 오늘날엔 매우 효과적인 항생제가 있고, 인체 구조와 전염 방식에 대한 지식이 풍부해 공중보건 대응도 훨씬 좋아짐
          + 항생제가 있음. Yersenia pestis는 대부분의 항생제로 쉽게 죽일 수 있는 박테리아임
          + 흑사병은 페니실린에 매우 민감해 더 이상 큰 문제는 아닌 것으로 봄. 오히려 이번 사례에서 진단부터 사망까지 매우 빠르게 진행된 점이 더 우려스럽다고 느낌
          + 답은 복잡하지 않음. 세균 이론과 위생임. 가장 좋은 생존법은 처음부터 질병에 걸리지 않는 것임
          + 남유럽인들이 질병에 더 심한 열반응이나 피부 홍조 같은 극단적 증상을 보이는 성향이 있다는 글을 읽은 적 있음. 이는 흑사병 생존 후에 남은 유전적 흔적일 수 있다는 설임. 관련 자료는 현재 찾지 못했지만, 혹시 설명 용어가 있었다면 기억이 나지 않음. 어쩌면 일반 질환에 대한 강한 면역반응과 famillial mediterranean fever, 그리고 흑사병이 인류 진화에 미친 영향에 관한 내용이 기억이 섞인 것일 수도 있음
     * 어떤 종류의 흑사병인지 궁금함
          + 기사에는 명시되어 있지 않지만, azcentral과 CNN의 기사에 따르면 pneumonic plague임(azcentral 기사, CNN 기사)
          + Yersinia pestis, 즉 흑사병임
          + 기사 제목 속 “plague” 링크가 내부 링크여서 조금 실망스러웠음. 솔직히 일반적인 일이긴 하지만, 적어도 요즘은 뉴스 사이트에서 LLM을 활용해 최근 관련 기사들을 요약으로 만들어 페이지별로 주제 설명 블럽을 제공하면 좋겠음. 만약 사람이 직접 하기 어렵다면 기계라도 활용하자는 생각임
          + Bubonic임
     * 백신도 존재하나, 매우 오래된 것이고 일반 대중에게 권고되지 않음 (CDC 백신 정보)
          + 조기 치료만 하면 완치가 잘 됨. 기사에서도 언급하듯이, 치료 시 생존율이 90%에 이를 정도임
     * 일반적으로 어떻게 이 병에 노출될까? 하이킹을 하다 걸릴 위험이 있는지 궁금함. 사람들이 프레리도그와 그렇게 가까워지는 일이 많은지, 유타에서 하이킹하다가 감염되는 경우가 있는지 알고 싶음
          + 벼룩이 자는 중에 물어서 감염되는 경우가 있음
     * 이제 시작이라는 생각이 듦
"
"https://news.hada.io/topic?id=22034","Builder.io 개발자가 Claude Code를 사용하는 방법 (+ best tips)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Builder.io 개발자가 Claude Code를 사용하는 방법 (+ best tips)

   예전에는 Cursor 파워 유저였고 How I use Cursor (+ my best tips)라는 글로 인기를 끌었던 Builder.io의 Steve Swell이 이번에는 좋은 클로드 코드 팁 글을 올려줬길래 번역 + 요약해서 공유드립니다. (블로그 글에는 참고 영상과 코드 스니펫이 있습니다)
     * IDE에서 통합해서 써라. 다만 나는 이제 커서는 CMD+K 자동완성과 탭 자동완성에만 쓴다. 커서의 에이전트 모드는 클로드 코드 안될 때만 쓴다.
     * 예전에는 코드 에디터 창 크기가 더 크고 에이전트는 사이드에 있었다. 요즘은 클로드 코드의 창 크기가 더 크다.
     * Builder.io에는 18,000줄짜리 단일 React 컴포넌트가 있다. 이걸 제대로 다룬 AI가 아무것도 없었는데 클로드 코드는 해낸다.
     * $100 플랜이고 보통 Opus만 쓴다. 보통 사람들은 기본 설정(50%는 Opus, 다쓰면 Sonnet)을 쓰는 걸로도 충분할 것이다. 제대로 쓰려면 새 작업 시작하기 전에는 항상 /clear 해라.
     * 클로드 코드에게 권한 주는 플래그(claude --dangerously-skip-permissions)는 생각보다 위험하지 않아서 리스크 감수하고 써볼만 하다.
     * 깃헙 통합(/install-github-app)으로 PR 리뷰해주는 거 의외로 쓸만하다. 로직 에러와 보안 이슈도 잘 찾아준다. 근데 기본 리뷰 프롬프트는 별로고, claude-code-review.yml에서 수정해라.
     * #으로 메모리 빠르게 넣는 것도 좋다. 알아서 가장 적절한 메모리 파일에 넣어준다.
     * 큐 시스템 좋다. 작업하고 있을 때 빼먹었던 거 그냥 하나씩 추가해두면 알아서 잘 해준다. 중간에 피드백 필요하면 그냥 무지성으로 수정하는 대신 나에게 되묻는 것도 좋다.
     * 커스텀 훅, 슬래시 커맨드, 프로젝트별 설정 제대로 할수록 더 편해지는데 클로드 코드에게 요청하면 다 해준다.
     * 커스텀 커맨드에 $ARGUMENTS 넣을 수 있는 것도 좋다. 서브폴더도 된다. /buider/plugin 입력하면 builder 폴더의 plugin.md 를 찾아간다.
     * Builder.io 익스텐션을 IDE에 추가하면 클로드 코드처럼 동작하는 챗 인터페이스 + 디자인 프리뷰 + 비주얼 에딧도 가능하다. 클로드 코드 리버스 엔지니어링해서 최대한 비슷하게 인터페이스 만들었다.
     * 그 외 처음 쓰는 사람이 모를 만한 것들
          + Shift + Enter로 새 줄 안 생기는데 /terminal-setup 하면 해결된다. (역자 주: 저는 Ctrl + J 씁니다)
          + IDE에 통합해놨을 때 파일 그냥 드래그하면 참조가 안 된다. (그렇게 하면 IDE에서 그냥 그 파일이 열린다) Shift 누른 채로 드래그해라.
          + 클로드를 멈추려면 CTRL+C가 아니라 Esc해야 한다.
          + 클립보드 이미지는 (Mac에서도) CMD+V가 아니라 CTRL+V 해야 들어간다.
          + 위 화살표 누르면 (세션 끝났어도) 이전 챗으로 갈 수 있다. Esc 두 번 누르면 모든 메시지 볼 수 있다.
"
"https://news.hada.io/topic?id=21970","스탠드얼론 정적 바이너리로 도구를 배포해야 하는 이유","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     스탠드얼론 정적 바이너리로 도구를 배포해야 하는 이유

     * 독립 실행형 정적 바이너리로 도구를 배포하면 사용자가 별도의 개발 환경이나 도구 체인 설치 없이 바로 사용할 수 있음
     * 컴파일 과정은 비정상 동작 코드 배포 가능성을 낮추는 추가 안전장치 역할을 함
     * 인터프리터 언어 기반 도구는 각종 의존성 설치 및 디스크 공간 낭비, 업그레이드 시 반복적 재설치 등 유지관리 부담이 큼
     * 의존성이 많을수록 보안 취약점 및 공격 표면이 커지며, 해킹 위험이나 유지보수 이슈가 발생하기 쉬움
     * 컴파일 언어 기반의 정적 바이너리는 외부 환경 변화에도 영향을 받지 않으므로, 배포 후에도 안정적 사용성을 보장함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

독립 실행형 정적 바이너리 배포의 장점

  설치 없이 즉시 사용 가능

     * Open AI가 Codex를 Rust로 다시 빌드하며 TypeScript를 버린 사례처럼, 컴파일 언어로 작성된 단일 바이너리를 배포하면 사용자는 추가적인 도구 체인 설치 없이 바로 실행 가능함
     * 가장 큰 장점은 속도나 효율성이 아니라, 설치 없이 곧바로 도구 사용이 가능하다는 점임

  컴파일러의 추가 안전장치

     * 컴파일 단계에서의 검사로, 비정상 코드가 배포될 가능성이 줄어듦
     * 예시로 Google Cloud CLI는 Python 기반으로 여러 번 실행 불가 상태로 배포된 적 있음
     * 대규모 팀도 이슈를 피하기 어려운데, 소규모 팀이 인터프리터 언어 기반 도구를 안정적으로 배포하기는 더 어려움

  도구 체인 의존성 필요 없음

     * 컴파일 언어 기반 도구는 단일 바이너리만 배포하면 되지만, Python, Ruby, TypeScript 등의 인터프리터 언어 도구는 반드시 해당 개발 환경이 필요함
     * Ruby로 작성된 mdl(markdown linter)처럼, 개발 환경(루비)이 업그레이드될 때마다 재설치가 필요함
     * JavaScript 기반 markdownlint 사용 시에는 npm 및 44개 이상의 의존성 설치 필요

  디스크 공간 낭비 문제

     * 인기 FOSS 코딩 어시스턴트 aider는 Python으로 작성되어 Homebrew로 설치 시 추가로 51개 패키지가 설치됨
     * 실제 디스크 사용량이 3GiB 이상 늘어나며, 이는 대부분의 리눅스 배포판 용량보다 큼
     * 반면 Rust로 작성된 uv 패키지 매니저는 35MiB 단일 바이너리만으로 설치 가능, Rust 자체나 rustup도 불필요

  보안 취약점 증가

     * 도구의 의존성이 늘수록 공격 표면 확대 및 보안 취약점 노출 가능성 증가
     * OpenAI의 Codex 패키지는 24개 직접, 184개 간접 의존성이 존재함
     * 모든 의존성을 OpenAI가 감사하더라도, 의존성 버전이 고정되어 있지 않아 향후 업데이트로 인해 취약점, 악성 패키지, 동작 중단 등의 이슈 발생 가능

  유지보수 용이성

     * JavaScript, TypeScript, Python 등 인터프리터 언어 기반 도구는 의존성이 제거되면 동작 불가
     * left-pad 사건처럼, 단일 패키지 삭제로 대규모 서비스와 도구가 마비될 수 있음
     * 컴파일 언어는 빌드 시점에만 의존성이 필요하므로, 이후 외부 저장소가 사라져도 도구가 계속 정상 동작

작성자의 경험

     * adb-enhanced 등 과거 Python으로 도구를 만들었으나, 이후 Go로 gabo, wp2hugo 등 다양한 도구를 오픈소스화
     * Python, TypeScript 등으로 독립 실행형 도구 개발은 더 이상 고려하지 않음
     * 반드시 정적 링크 바이너리 배포가 가능한 언어(Rust, Go, C++ 등) 로 작성 권장

결론

     * 독립 실행형 도구는 Rust, Go, C++ 등 컴파일 언어로 개발
     * 최소한의 외부 의존성만을 가진 정적 바이너리 형태로 배포해야 함

   디스크 공간 낭비 문제 에 대해서는 적잖이 공감할수 밖에 없네요...
   AKS 운영하는데 컨테이너 이미지 용량 1GB 넘어가는 파이선 앱 볼때마다 머리가 아픕니다.
   지금은 그냥 도커파일 빌려다가 제가 다시 용량줄여서 올리고, 500MB 이하로 못줄이면 그냥 포기합니다 ㅋㅋ

   pytorch+cuda 의존성에 버전만 다르게 걸리는 패키지가 있어서.. 아주 가관입니다.
   별 기능도 없는 녀석인데 작은 데몬별로 의존성이 2기가 가까이 깔립니다..

   단순 추론용으로 쓰는 cpu 런타임이면 형편은 좀 나은데, 요즘 요구되는 LLM 서비스 때문에 트래픽도 트래픽대로, 용량도 용량대로 늘어나니 비용 계산할때 욕나옵니다 ㅋㅋㅋ
"
"https://news.hada.io/topic?id=22089","당황스러운 품질 저하 현상","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             당황스러운 품질 저하 현상

     * 전 세계적으로 제품과 서비스의 품질 저하 현상이 확산되고 있음
     * 이 현상은 경제적 불평등과 자본주의 약속의 불이행, 그리고 효율성 문화의 확대와 밀접한 관련성이 있음
     * 소비자들의 인식 변화, 특히 지속성과 내구성보다 새로움과 효율성을 우선시하는 경향이 세대 간의 품질 평가 차이를 심화시키는 원인임
     * 기술 발전 및 자동화, 특히 인공 지능과 알고리듬 기반 서비스 도입이 품질 저하 논란을 불러일으키고 있음
     * 환경 악화와 사회적 지속 가능성 문제도 심각하게 대두되고 있으며, 단순히 싼 제품을 구매하는 것이 장기적으로는 사회 전체에 악영향을 미침
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

품질 저하 현상에 대한 개요

     * 최근 전 세계적으로 가구, 의류, 식품뿐 아니라 서비스 전반에 걸쳐 품질 저하가 두드러지는 현상이 나타남
     * 값싼 플라스틱 냄새, 금방 해지는 티셔츠, 방부제가 주성분인 음식, 불편한 자동화 서비스 등 일상에서 그 예시가 쉽게 포착됨
     * 기사와 글조차 ChatGPT 등 생성형 인공 지능의 알고리듬 문체로 비인격적으로 작성되는 경우가 늘어, 물건의 '사랑받을 가치'보다 '구매'에만 초점이 맞춰지는 사회 분위기가 형성됨

품질 정의와 인식의 상대성

     * 연구에 따르면 품질은 본질적으로 소비자 각각의 주관적 기준에 달려 있음
     * 예를 들어, iPhone 15와 2003년산 Nokia 중 어느 것이 더 품질이 우수한지 절대적으로 평가하기 어렵고, 내구성을 중시하는 이들은 오히려 오래가는 Nokia를 높게 평가함
     * 즉, '예전보다 나빠졌다'는 인식이 실제 현실보다 주관적 판단에 가까움

사회적 맥락: 자본주의 약속 붕괴와 효율성 문화

     * 전문가에 따르면, 사회 전반에 만연한 비관적 정서가 제품과 서비스의 품질 평가에도 영향을 줌
     * 사회경제적 불평등 심화 및 성장 기회의 단절, 그리고 효율성에만 집중한 새로운 경영 문화(예: Elon Musk, Mark Zuckerberg) 가 주요 원인으로 분석됨
     * 이러한 변화는 실제로 구직 및 복지 환경의 악화, 비용 절감 목적의 인력 감축, 자동화 시스템 확산 등 다양한 구조적 문제로 이어짐

공공 서비스, 연령, 그리고 품질 평가의 세대차

     * 공공 서비스(특히 건강보험)에서도 품질 저하에 대한 불만이 커짐
     * 그러나 실질적 품질 저하보다는 늘어나는 대기시간과 사회 변화에 대한 적응 부족이 원인으로 지적됨
     * 고연령층일수록 품질 저하 인식이 강하게 나타남
          + 예전에는 내구성 등 장기적 가치를 중시했으나, 요즘 세대는 '얼마나 오래 가는지' 보다는 '얼마나 새롭고 효율적인지'에 더 집중함
          + 패션 및 소비재 산업에서 '빨리 사고 빨리 버리는' 소비 패턴이 일반화됨

계획적/인지된 진부화와 소비 심리

     * 일부 기업은 계획적 진부화(일정 시간이 지나면 제품이 고장나도록 설계) 를 도입함
     * 더 강력한 전략은 '인지된 진부화' , 즉 제품이 충분히 쓸 만해도 구식이라고 소비자들을 설득하여 교체를 유도함
     * 광고와 미디어는 지속적인 신제품 소비와 유행 추종을 강화하며, 오래 쓰고 수리하는 문화는 점점 사라짐

저가 대량생산과 소비문화의 전환

     * 19세기 중반 이후 대량생산·저가 소비재 확산이 빠르게 이루어짐
     * 한때 '다기능·재사용' 위주의 소비 생활이 지배적이었으나, 이제는 싸고 다양한 '새 것'을 언제든 살 수 있음이 상식이 됨
     * 이로 인해 물질적 풍요에도 불구하고 상호작용과 사고방식이 피상적, 단명형, 저하된 채로 남는 역설적 빈곤 현상이 나타남

자동화·인공 지능과 품질 논란

     * 최근 AI와 자동화의 도입으로 고객 서비스도 품질 논란의 중심이 됨
     * 스페인에서는 이미 62%의 고객 서비스가 자동화되어 있으며, 소비자 중 절반이 가상 상담원에 부정적 인식을 보임
     * 디지털 격차 및 고령층의 불편이 부각되는 반면, 기업 측에서는 오히려 고객 응대 효율이 개선됨을 주장함
     * 인공 지능이 가짜 리뷰, 신뢰성 저하 등 다양한 문제를 야기함. 현재 온라인 상품 리뷰의 40% 이상이 신뢰할 수 없는 것으로 분석됨
     * 웹 기반 인공 지능 모델 스스로가 이미 생성한 데이터를 참고하여 점차 '모델 붕괴' 현상에 직면할 위험성도 제기됨

경제적 선택과 사회적 결과

     * 일부 품목(예: 항공 좌석)에서 가격은 크게 낮아진 반면 품질(공간 등)은 저하된 경향이 관찰됨
     * 타협된 소비 환경의 진짜 문제는, 이렇게 저품질 상품을 계속 소비함으로써 지구 환경에 추가적 부담을 초래하고, 결과적으로 사회적 지속 가능성을 심각하게 위협함임
     * 좋은 제품은 단순히 편의나 저렴함이 아니라 윤리적·사회적 가치와 연관된 노력, 공헌을 동반해야 '진정한 품질'로 정의 가능함

        Hacker News 의견

     * 최근 몇십 년간 오히려 품질이 좋아졌다는 의견도 많지만, 내 경험상 같은 브랜드에서 5, 10, 15년 전에 샀던 좋은 품질의 제품을 새 버전으로 다시 사면 품질이 오히려 떨어지고 싸구려처럼 느껴짐, 예전 품질과 비슷한 대체품을 찾기도 힘든 상황임, 이게 반복적으로 나를 실망시키는 이유임, 제품이 성공하고 시장이 포화되면 성장 압박 때문에 매번 원가 절감을 하게 되고, 그래서 해마다 조금씩 품질이 나빠진다는 의심이 있음
          + 내 시각에서는 시장점유율이나 혁신적인 원가 절감이 없을 때 남은 유일한 이익 극대화 전략은 품질을 계속 낮추면서 가격은 계속 올리는 것뿐임, 결국 이 전략은 브랜드를 망치지만 브랜드는 또 재활용하거나 새로 만들 수 있음, 도덕적 고려가 없는 순수하게 이성적이고 이기적인 경제 주체는 '이 브랜드로 최대의 이익을 얼마나 오래 유지할 수 있을까'를 계산함, 그 기간 동안 투자수익률이 괜찮으면 이 전략을 실행함
          + 내가 대기업 tech 회사에 다녔을 때, 소프트웨어 엔지니어들의 직장 환경이 예전보다 확실히 안 좋아진다고 선배들이 느꼈음, 그런데 CFO의 공식 답변은 '사람들이 빠르게 퇴사하지 않으니 그 정도로 나빠진 건 아니'라는 것이었음, 네가 의심하는 것과 비슷한 현상임, 즉 어떤 지표(예: 사람들이 계속 우리 제품을 산다)를 이용해서 요구를 테스트하는데 수요가 계속되면 회사 입장에서는 문제없음, 소비자 입장에서는 모든 프로젝트가 이런 식이니까 세상 전체의 품질이 떨어지는 듯 느껴지지만, 가격은 내려가지 않음
          + 의료기기 분야에서 특히 이런 현상이 두드러짐, 쓸데없는 '디지털 경험' 도입 경쟁이 부각됨, 예를 들어 보청기만 해도 예전엔 전용 볼륨 버튼과 전원 스위치가 있는 아날로그 모델을 쉽게 구할 수 있었는데, 요즘 모델은 전원 스위치가 없고 휴대폰 앱과 블루투스 페어링이 필요함, 예전엔 바로 쓸 수 있었는데 지금은 부서지기 쉬운 컴포넌트들이 번갈아 끼어드는 불편한 쓰임새임
          + 많은 사람들이 기술 발전을 곧 품질 향상으로 오해함, 기술이 품질을 개선할 수도 있지만 다른 식으로도 쓰일 수 있음, 내 생각은 서구, 특히 북미 지역이 1970년대 초반 오일쇼크 이후 제대로 회복하지 못했다는 것임, 그 전엔 에너지를 거의 무한정 썼으니 모든 물건이 요즘보다 훨씬 무겁고 튼튼했음, 무게를 줄이려다 보니 인프라 전체가 점점 약해짐, 이런 부실함을 용인하는 낮은 기대치 문화도 한 몫함
          + 한편 자전거용품 중에는 품질이 점점 좋아진 사례들도 있음, 예를 들어 Zefal 물병을 10년간 세 번 샀는데 첫 번째는 입구가 두 개의 프롱으로 고정 됐고 그 두 개가 결국 부러졌음, 두 번째 산 건 네 개 프롱으로 개선됐음, 세 번째는 딱딱한 플라스틱 대신 더 편안한 소재 입구로 업그레이드 됨, Lookcycle 페달도 세 번 샀는데 첫버전은 돌멩이가 페달 내부에 끼어 크게 불편했고, 두 번째는 스프링을 덮는 플라스틱 커버와 실링이 개선, 세 번째는 외부 각진 부분을 줄여서 넘어져도 더 튼튼하게 바뀜
     * 예전에는 이코노미 항공권 가격이(인플레 및 추가 요금 감안해도) 훨씬 더 비쌌음, 지금 과거와 같은 서비스와 품질을 받으려면 더 비싼 돈을 내야 하고, 대신 아주 싼 값에 저품질을 경험할 선택지도 생김, 비행기 수가 많아진 덕분임, 전자제품이나 옷도 마찬가지임, 비싸다고 다 고품질도 아니고, 오히려 같은 브랜드 안에서도 저렴이 vs 고가 라인이 갈려서 이제는 누구나 싼 것과 비싼 것 중 선택 가능해짐, 예를 들어 Nike도 지금은 저렴이 라인/고가 라인 모두 존재함, 품질 가구는 50년 전이나 지금이나 인플레 감안하면 가격이 비슷함, 대신 싸고 나쁜 것의 선택지가 더 많아졌을 뿐임
          + 가격이 비싸다고 품질이 보장되는 건 아님, 브랜드가 이 인식 활용해서 원가 많이 안 드리고도 마진만 올리곤 함, 예를 들어 $180 주고 산 Sony Link Buds 이어폰이 여러 번 고장나서 포기했고, $5 주고 산 Auki 블루투스 이어폰은 멀쩡하게 계속 쓰고 있음
          + 항공권 얘기로 돌아가면, 지금은 같은 수준의 서비스를 돈을 더 낸다고 해도 예전과 똑같은 경험을 할 수 없음, 좌석 간격 자체가 완전히 달라졌고, 비즈니스 등급으로 올려야 과거와 비슷한 걸 누릴 수 있을 뿐임, 관련 기사
          + 전자제품이나 옷도 마찬가지라고 했지만, 내 경험은 달랐음, 브랜드 이름값만 부풀려질 뿐 품질엔 투자하지 않아 비싼 값을 내고도 실제 품질은 약간 나아진 정도임, 전자제품은 내가 잘 판단 가능해서 다행이고, 옷은 유튜브의 '옷 해체 영상' 보고 나서 더욱 납득하게 됨
          + 가격표가 품질 신호가 되지 않는 게 문제임, 마케팅 부서는 품질 있는 척 연기를 너무 잘함, 와인 '수상경력' 같은 건 사실 돈 주고 브랜드에 붙여받는 경우가 많음, 리뷰도 거의 다 가짜거나 제조사가 돈 주는 식임, 결국 비싼 걸 사면 품질 있을 거라 기대할 수밖에 없고, 싼 걸 사면 어차피 나쁠 테니 큰 돈 안 쓴 게 그나마 위로인 상황임
          + 네 주장에 동의하지만, 지금 세상에 이렇게 싼 저품질 제품이 너무 많은 건 바람직하지 않다고 생각함, 싼 새제품이 가능해지면서 저품질을 모두가 용인하게 되고, 오히려 그런 문화가 소비를 부추김, 사회적으로도 많이 소비한 것들이 결국 쓰레기장이 되고 바다에 떠다님, 가끔은 서구 사회가 좀 더 가난했으면 좋겠다는 생각이 들기도 함, 제조업이 이미 충분히 싸구려 대량생산으로 발달했으니, 대공황이 21세기에 온다면 신차 판매는 분명 마비될 것 같음
     * 내가 아는 거의 모든 분야에서 여러 기준으로 볼 때 품질이 오히려 전반적으로 크게 향상됨, 오늘날 정말 품질에 집중하려고 하면 어떤 제품/서비스든 과거 그 어느 때보다 잘 만들 수 있음, 그런데 품질 하락이라 느끼는 건 사실 우선순위 자체가 변화된 결과임: 지금은 가격 접근성 및 효율성에 더 초점 맞추고 있음, 예전엔 비싸고 소수 전유물이던 제품들이 이제 수십억 인구에게 '충분히 좋은' 품질로 제공되고 있음, 물론 그 대가로 수명이 짧거나 수리가 어려워질 수 있지만, 접근성 확대 자체가 도덕적으로 큰 성과 정도로 봄, 기사에서 슬퍼하는 진보야말로 이런 변화 덕에 가능해졌다고 생각함
          + 미국식 소비문화의 확산이 도덕적으로 좋은 일일지 확신 못 하겠음, 지금 인류가 화석연료에 의존하는 정도는 심각함, 미래 세대가 걱정됨, 멈추는 길 없이 성장만 계속되고 있음
          + 가격 접근성도 안 맞는 것 같음, 자동차 값은 감당 안 될 만큼 올랐고, 집은 아예 사치품이 되었음, 소비재들도 점점 더 비싸짐, 안전성은 좋아졌지만 장인정신(공예성)은 나빠진 느낌임
          + ""요즘 제품은 예전같이 안 만든다""란 말을 하는데, 사실 예전의 비싼 제품과 지금의 싼 제품을 비교하는 게 대부분임, 옛날 '좋은 물건'은 여전히 존재하지만 우리가 지불하던 가격 대비 기준이 너무 높아져서 체감상 손해로 느껴짐
          + 개념·이론상으로는 품질 더 좋은 걸 만들 수 있음, 실제로는 그렇지 않으니 품질이 하락한 거라 생각함
          + 품질이 떨어졌다고 느끼는 사람은 이 유튜브 쇼츠 채널 보면 좋음
     * “iPhone 15가 2003년 Nokia보다 더 좋은 품질이라고 절대 단언할 수 없다”는 주장에, 과거를 미화하거나 극단적 문화상대주의가 결합된 논리라고 생각함, 2009년 Nokia N900(당시 플래그십) 써봤는데, 스펙이야 화려해도 실제론 불편하고 버거웠음, 집에서는 Wi-Fi로 제대로 전환되지도 않고, GPS는 위치 잡는 데 몇 분 걸리고 쉽게 끊김, 옛날 친구 iPhone과 비교했을 땐 GPS 속도 등 여러 면에서 현격히 차이남, 요즘 아이폰 플래그십(혹은 안드로이드) 품질이 전반적으로 훨씬 좋다고 분명하게 말할 수 있음
          + 2003년 Nokia 썼는데, 배터리 일주일씩 가고 망가지지도 않았으며, 자판도 튼튼하고 촉감 좋아서 화면 안 보고도 문자 보낼 수 있었음, 시간이 지나도 느려지지 않음, 지금 스마트폰은 할 수 있는 게 많긴 하지만, 각각의 기능 품질은 오히려 안 좋아졌음
          + 기사 저자 이름 검색해 보니 2003년에 거의 유아였던 걸로 보여 실제 그 시절 휴대폰을 썼을 리 없다고 생각함, 잠깐 써볼 수는 있어도 주기적으로 사용했던 세대는 아님
          + 기억 왜곡, 생존자 편향을 감안해야 함, 80년대에도 엄청난 저질 쓰레기 제품이 쏟아졌는데 이미 다 버려서 지금은 '품질이 남았던 것들'만 남아있을 뿐임, 음식도 옛날엔 통조림, 인스턴트 가루 푸딩 등 저급 음식이 많았음
          + 2007~2008년쯤 제조사별로 자체 OS 스마트폰을 만들던 시기, LG KS360을 쓸 땐 자주 멈췄고, Sony W200i는 잘 작동했지만 전용 소니 커넥터 등 불편함, Sony W350i는 너무 문제가 많아서 두 번을 교체했음, 내 아마존 결제 내역에도 두 번 바꾼 게 남아있음
          + 70년대 자동차 충돌 영상 보면 요즘 차가 깨지고 옛날 차가 멀쩡해 남들 놀라는데, 사실은 옛날 차는 운전자석 자체가 크럼플존(충격흡수구역) 역할을 한 것이었음
     * 거의 모든 품목에서 부모 세대가 기억하는 수준의 고품질 상품도 여전히 존재(이커머스 덕에 더 쉽게 찾음), 다만 물가 기준으로 보면 그 가격도 여전히 만만찮음, 요즘 싸구려에 익숙한 우리에게는 엄청 비싸게 느껴지는 것임, 맞춤 정장, 원목가구, 초지방 소고기, 10년 이상 쓰는 청소기 등 얻으려면 그만큼 지불해야 함(예전과 비슷), 수요가 완전히 사라진 품목은 이제 선진국에서 제조 중단되고(맞춤 셔츠는 실론 등 외국산), 반면 품질 욕구를 충족하는 분야(동네 커피숍 등)는 오히려 활짝 발전함, 현대 제조기술의 정점 상품도 생각해볼 필요가 있음, 예를 들면 아버지가 쓰던 필름카메라 가격으로 지금은 100배 줌 또는 7인치 터치스크린, 5G 연결, 전자책 다 내장된 스마트폰을 산다는 점에서 놀라움
          + 아버지의 SLR 카메라는 100mm 렌즈도 대단했는데, 지금 내가 쓰는 DSLR은 28-300mm 렌즈 달고 훨씬 좋은 사진을 찍음, 옛날엔 필름 한 롤에 5컷 찍으면 많이 찍은 건데, 요즘은 그냥 7연사 브라케팅도 기본임, 촬영 용량도 배터리만큼 찍을 수 있고, 만약 아버지가 봤다면 놀라 실신할 듯함
          + ""부모 세대 기억만큼의 품질로 모든 게 여전히 나온다""는 주장에 동의 안 함, 예전 명품 브랜드들이 똑같이 질 저하되고 있음, Levi’s, Fjällräven 등은 실제로 예전 옷은 수십 년 사용에도 튼튼하고 색만 바랬는데, 최근 옷은 1년만 사용해도 천이 망가짐, 예전 튼튼했던 캔버스 백팩 브랜드가 최근에는 전부 폴리에스테르로 바뀜, 품질 차이는 엄청남, 특히 옷·신발, 가격도 불평할 생각 없고 돈 더 내도 되는데 브랜드는 진짜 아무 의미 없음
          + 비싼 물건의 문제는 어떤 게 진짜 품질 좋은지, 어떤 게 허세인지 알기 힘들단 점임, 모든 시장이 '레몬 마켓(정보 비대칭의 저질 제품 시장)'처럼 됨, 그래서 내 전략은 저가형 상품에 이름 붙는 브랜드는 아예 배제하기임, BMW, JBL 등이 그 대상임
     * ""세탁 두 번에 옷이 이상해진다""는 주장에, 그게 무슨 옷인지 궁금함, 나는 비싼 브랜드도 안 입고 빨래도 막하지만 옷은 몇 년씩 거뜬히 입고 있음, 염색약도 옛날보다 훨씬 좋아져 빨래하면서 색 빠지는 일 거의 없음
          + 겉보기에는 좋은 티셔츠 세트 샀는데 한 번 빨았더니 가로로 퍼지고 세로로 짧아짐, 다시 늘려보려 해도 소용없음, 예전에는 10유로에 3족짜리 양말도 몇 년씩 썼는데 요즘은 금방 구멍 남, 비싸다고 품질 좋은 것도 아니고 싸든 비싸든 도박임
          + Wrangler 청바지 옛날에 사서 Levi’s보다 잘 맞길래 최근에 저렴하길래 샀는데 처음 한 번 입고 빨았더니 아예 못 입을 만큼 뻣뻣해지고 망가짐, 무난하게 세탁했는데도 종잇장처럼 됨
          + 최근에 대형마트에서 산 남성용 양말, 신기만 했는데 망사처럼 다 풀림, 정상적인 브랜드 양말임
          + Fruit of the Loom, 원래도 최고급은 아니었지만(그래서 오히려 좋았음) 그냥 검증된 중간급 브랜드였고, 10년 넘은 셔츠는 멀쩡, 그런데 최근산 건 한 번 빨면 구멍 남, 이건 품질관리 실수 문제가 아니라 일부러 품질을 떨어뜨린 느낌임
          + 요즘은 의류도 노력하면 괜찮은 품질 찾을 수 있음, 나는 Duluth Trading의 청바지가 잘 맞아서 계속 세탁해도 상태 좋음, Levi’s도 괜찮은데 '고가 판매채널'(브랜드 자체 매장 등)에서 사야 품질이 다름, Amazon이나 대형 소매업체 등 저가 유통채널과 확실히 분리되어 있음, 많은 브랜드가 이런 '채널 세분화' 전략을 씀, 다만 $80 이하의 질 좋은 청바지는 세일 아니면 찾기 힘듦, 내가 옷을 구하는 데 별 문제 없고 오히려 마음에 들던 모델이 단종될 때가 가장 불편함
     * 몇몇 사람은 좋은 품질의 제품을 오래 쓰고 싶어함, 그런데 흔히 이런 품질 좋은 제품은 찾기 어렵거나(있긴 있어도 실제로는 거의 안 팔림), 뭔가 단점과 장점이 섞여서 '진짜' 좋은 제품 찾기가 힘듦, 자주 바꿀 필요 없는 사람도 있지만 시장 구조상 계속 새 제품을 사도록 유도됨, 실제로 내구성이나 품질이 전반적으로 낮아짐, 또 일부러 호환 불가능하게 만들어서 더욱 품질 좋은 제품 찾기가 힘듦, 컴퓨터 프로그램도 마찬가지임, 오픈소스 소프트웨어(FOSS)가 약간 도움이 되긴 하지만, 오픈소스라도 품질 나쁜 경우도 많음, 적어도 직접 개선할 기회가 열려 있으니 나는 비교적 다르게 프로그램을 만듦
     * 사람들은 대개 과거를 무조건 깎아내리거나 무조건 찬양하는 두 부류로 나뉨, 하지만 현실은 그 중간임, 항공좌석 같이 대중화되면 질이 떨어지는 건 당연함, 하지만 많은 분야에서 품질이 극적으로 좋아진 것도 사실임, 예를 들면 자동차, 60년대 차는 2년 만에 녹슬고 70년대는 여러 기계적 결함, 80년대는 전자 부품 결함이 많았는데, 80년대~2000년 즈음에는 대부분 해결됐음, 물론 요즘은 소프트웨어 이슈나 EV(전기차)로 새로운 문제가 생기지만 옛날 차로 돌아가고 싶은 생각은 없음
          + 자동차도 여전히 녹슬고 있음, 최근엔 부품 단종되면 90% 이상 멀쩡해도 그 부품 하나 때문에 차를 폐차해야 하는 현상이 더 큰 문제임, 관련 영상
          + ""과거를 폄하하거나 찬양하는 두 부류가 있다""는 말에, Neanderthal 만화가 떠오름, 선사시대인들이 몸이 불편해져도 공동체가 돌보는 모습 등 사람의 복잡한 본성이 드러남, 오늘날 사회적 진보로 식량이 넘치는 세상에서조차 여전히 굶주림이 존재하는 현실이 아이러니함, 또 Nintendo Switch를 보여주면 그들이 분명히 좋아할 것 같음
          + 자동차도 또 다른 의미로 '엔시티피케이션'이라고 할 수 있음, 교통체증 때문에 결국 다 같이 비효율에 빠짐
          + 2020년대 자동차는 이제 너무 비싸거나 과공학된 느낌이고, 2010년대가 자동차의 절정이었다고 생각함, 아마도 2040년대쯤이면 또 무언가가 해결될 수도 있을 것임
     * 기사에서 ""20년 넘게 입지 않은 옷엔 애착이나 감정이 없다""라거나, ""오렌지를 직접 짜지 않고 3달러 주스 먹는다"" 식의 지나친 인용이 많아서, 내용 신뢰도가 떨어짐, 혹시 기사 품질 저하가 이런 예시로 보여지려는 의도라면 성공함
     * 기사의 논지(품질 저하)는 본질적으로 평가 기준에 따라 상대적임, 내구성 바라는 사람에게는 값싼 가구가 저품질이고, 가볍고 저렴함을 원하면 오히려 ikea가 고품질일 수 있음, 모두가 동일한 평가 기준을 따른다고 가정하는 건 게으른 기사임, 기사에서 AI까지 언급된 것도 이상함
          + 평가 기준이란 것도 우리가 통제하지 않음, 시장은 광고 등 패턴에 좌지우지되고, 절대 다수가 추인하면 내 개인 통제는 소용없음
          + ""가벼우면서 가격이 저렴한 걸 원한다""는 논평에, 가격 접근성은 이미 품질/가격비에 반영됨, MDF 가구가 특별히 가볍다는 근거도 모르겠음
"
"https://news.hada.io/topic?id=22002","Show GN: 타입 안전성과 네임스페이스를 지원하는 웹 스토리지 유틸리티 store-easy","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Show GN: 타입 안전성과 네임스페이스를 지원하는 웹 스토리지 유틸리티 store-easy

   안녕하세요! 저는 엔지니어를 꿈꾸는 고등학생이에요 :)

   localStorage와 sessionStorage를 더 안전하고 편하게 다루기 위해, store-easy라는 유틸리티 라이브러리를 만들어 봤어요.

   🔸 주요 기능
     * 타입(type) 지정 및 자동 검증 → 런타임 오류 방지
     * 네임스페이스(namespace) 기능 → 키 충돌 방지
     * setMany로 여러 값 한 번에 저장 가능

   📦 설치
npm install store-easy

자세한 사용법과 예시는 아래 npm 페이지에서 확인하실 수 있어요:
👉 https://www.npmjs.com/package/store-easy

많은 피드백과 조언 환영합니다!

   늦게 답 드려 죄송합니다! 정성스러운 피드백 정말 감사드립니다 🙏
   말씀해주신 내용이 정말 도움이 되었고, 그에 맞춰 다음과 같은 개선을 반영했습니다:

   타입이 일치하지 않을 경우의 실패 예시와 예외 처리를 README에 명확히 추가했어요.

   간단한 데모 페이지도 준비했습니다:
   👉 https://monologue-one.netlify.app

   좋은 말씀 다시 한 번 감사드리고, 앞으로도 개선 아이디어가 있다면 언제든지 환영입니다!

   문서에 데이터를 집어넣고 꺼내는 예시에서 성공하는 부분만 들어있는데, 어떤 경우에 실패하고 예외처리되는지도 함께 제시해서 이 라이브러리가 오류방지에 대해 어떤 기능을 수행하고 있는지 자세하게 설명해주면 좋겠습니다.
   그리고 간단한 데모 페이지가 있으면 좋겠습니다. 간단한 todo나 list 페이지를 하나 만들어서 이 라이브러리를 사용하는 개발자에게 어떤 기능을 제공하고 네임스페이스 기능이 어떻게 동작하는지 시각적으로 표시해주면 접근성이 훨씬 좋아질거 같아요

   좋은 피드백 감사합니다! 말씀하신 예외처리와, 간단한 데모 페이지를 README.md에 추가했어요! 늦게 답변 드려서 죄송합니다!

   이 라이브러리가 어떤 문제들을 해결해주었는지 소개해주면 어떨까요.

   피드백 감사해요! 나름대로 어떤 문제들을 해결했는지에 관한 내용을 README.md에 추가했어요!
"
"https://news.hada.io/topic?id=22078","Meta 새로 구성한 Superintelligence 팀 명단 유출","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Meta 새로 구성한 Superintelligence 팀 명단 유출

    사실 연봉이나 국적으로 사람들이 어그로가 끌린 거 같은데 본질은 이들이 지금 무엇을 하고자 하는지를 분석 하는 겁니다.

   Meta가 이번에 새로 구성한 팀의 명단이 유출 되었는데 어마어마 합니다. 국내에서도 인공지능 파운데이션 모델 독자적으로 만드는 목적이 있다면 팀 구성에 참고해보세요.

   Meta 내부에서 유출된 자료에 따르면, 메타가 구성한 AI 최정예 조직 ‘Superintelligence 팀’의 인력 구성으로 앞으로의 방향성을 볼 수 있는데요.

   🔍 조직 구성 특징
   • 50% 중국계
   • 40% OpenAI, 20% DeepMind, 15% Scale AI 출신
   • 75%가 1세대 이민자
   • 개인 연봉 추정: 100억 ~ 1300억 원 수준
   • 단순한 ‘모델 개발’이 아닌 AGI 실현을 목표로 한 정밀한 구성

   🧠 기술 집중 분야
   • LLM 최적화
   • 멀티모달 학습
   • 지식 편향 제거
   • 강화학습 기반 미세조정

   개인 연봉 추정 : 100억 ~ 1300억...와... 커리어 역사상 이런 개발자가 있었나 싶네요 ㄷㄷ
   옵션 뺴고 순수 연봉일까요 ㄷㄷ

   옵션 포함일듯해요.

   이상하리 AI에서는 중국이 강세네요

   https://www.youtube.com/watch?v=qVy9kl2nYgQ

   PC&개인정보 이슈 + 쓸데없는 삽질로 서양이 시간 끌리는동안...

   이미 예견된..

   개발자 평균연봉 올려놓는 놈들 확인

   중국계가 50%라니 놀랍네요. 우리도 AI 가야할 길이 먼데 인력에서도 뒤지는 느낌이 듭니다.
"
"https://news.hada.io/topic?id=22018","상사와 효과적으로 일하는 15가지 원칙 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      상사와 효과적으로 일하는 15가지 원칙 [번역글]

  상사와 효과적으로 일하는 15가지 원칙

   상사와의 협업(Managing Up) 은 커리어의 핵심 역량입니다. 아래 원칙들은 실무에서 바로 적용할 수 있는 구체적 방법과 이유를 제시합니다.

    1. 상사와의 협업을 내 일로 받아들이기

     * 내 업무 성과의 중요한 일부가 ‘상사와의 협업’임을 인식합니다.
     * “왜 내가 상사 눈치를 봐야 하나?”가 아니라, 상사와의 하모니가 내 성장과 조직 성공을 좌우함을 받아들입니다.
     * 상사의 의사결정, 업무 스타일, 우선순위를 파악하려 노력해야 내 일도 원활해집니다.

    2. 결론부터 명확하게 말하기

     * “핵심→배경” 순서로 먼저 결론(comment)부터 제시합니다.
          + 예: “이 안건은 진행 추천입니다(결론). 이유는 A, B입니다(배경).”
     * 메신저·이메일 등 비동기 커뮤니케이션에서는 특히 중요(‘두괄식’)합니다.
     * 상사가 바쁜 일정 속에서도 빠르게 의사결정하거나 맥락을 파악할 수 있습니다.

    3. 내 생각의 흐름을 투명하게 보여주기

     * 결론에 도달한 과정, 논리, 전제(assumption), 자신감 정도까지 설명합니다.
          + “이렇게 판단한 논리는 다음과 같습니다: 1)... 2)... 3)...”
     * 상사는 필연적으로 정보가 부족하기 때문에, 중간 단계를 보여주면 신뢰가 올라갑니다.

    4. 잠재적 문제 미리 공유하기

     * 문제가 생길 조짐이 있다면, 완전히 ‘터진 뒤’가 아니라 ‘징후가 있을 때’ 즉시 알리는 것이 좋습니다.
     * 문제 우선순위, 심각도, 영향 등을 상사와 먼저 조율하면 추가 손실이나 오해를 줄일 수 있습니다.

    5. 불평 대신 해결책 제시하기

     * 단순히 “이거 힘듭니다”가 아닌, 문제의 원인과 그에 대한 몇 가지 대안을 함께 준비해봅니다.
          + “인력 부족 때문에 일정이 어렵습니다(문제). → 우선순위 조정/외부 지원/마감일 연장(해결책 제안) 중 하나를 제시합니다.”

    6. 정보의 우선순위를 정해 전달하기

     * 긴 내용을 전할 때는 ‘요청 혹은 제안 → 배경 설명’ 구조로 작성.
          + 예: “A안 승인 요청드립니다. 이유는…”
     * 결론과 맥락이 혼재되지 않도록 구분 지어 전달해야 합니다.

    7. 상황 공유 루틴화

     * “예고 → 진행상황 → 업데이트 → 완료 보고” 식의 ‘미리 알림’ 습관화.
     * 좋은 소식뿐 아니라, 실수나 실패, 위기 소식도 빠를수록 피드백과 신뢰를 얻습니다.

    8. 마이크로 매니징의 원인이 소통 부족인지 점검하기

     * 상사가 자주 확인한다면, ‘관리형 스타일’ 때문인지 ‘내 보고 부족’ 때문인지 돌아봅니다.
     * 내 쪽의 정보 부족이 원인인 경우, 더 세밀하게 소통량을 늘려봅니다.

    9. 충분해 보이는 소통이 사실은 ‘적정량’일 수 있음을 기억하기

     * 경험이나 책임이 적을수록, 더 자주·더 자세히 공유하는 것이 기본.
     * 업무가 숙달되면 자연스럽게 상사의 확인 빈도도 줄어듭니다.

    10. 주도적으로 다음 단계 제안하기

     * 상사의 지시만 기다리는 대신, 주도적으로 한두 가지 방안을 제시.
          + “다음 단계로 A/B/C 중 어떻게 할지 추천드립니다. 제 생각엔 A가 좋습니다. 이유는…”
     * 상사를 ‘결정기계’로 만드는 대신 논의 주체로 만들면 더 높은 신뢰를 이끌 수 있습니다.

    11. 단순 질문보다 대안을 곁들인 문의

     * “어떻게 할까요?” 대신
          + “A, B 두 가지가 있는데, A가 더 적합하다고 생각합니다. 혹시 다른 의견 있으세요?”
     * 단순 프롬프트가 아닌, 의견+가설+질문이 함께 들어간 문의가 생산적 논의를 만듭니다.

    12. 상사가 궁금해할 질문 미리 준비

     * 상사가 궁금해할 포인트를 예측해 미리 데이터, 근거, 문서 등을 준비합니다.
     * “자료는 이쪽에, 추가 설명이 필요하면 말씀 주세요.” 식의 ‘준비된’ 태도가 소통 효율을 극대화합니다.

    13. 건강하지 않은 조직에서는 과감히 물러날 줄 알기

     * 모든 노력을 하였음에도 구조적으로 개선이 불가하다면, “떠날 용기”도 관리의 일부임을 잊지 마세요.
     * 상사와의 관계는 ‘팀의 건강’과도 직결됩니다.

    14. 필요한 것을 구체적으로 요청하기

     * “피드백 해주세요” 대신 “이 부분(내용/방향/리소스 등)에 대해 OOO한 피드백이 필요합니다. 언제까지 부탁드립니다.”처럼 구체적으로 요청해야 원하는 결과를 얻기 쉽습니다.

    15. 상사와의 협업 역량은 평생 필요하다

     * 직급·직책이 높아져도 “언제나 누군가의 보고자”임을 명심.
     * 창업자도 투자자·고객·파트너 등 다양한 ‘상사’가 존재하므로, 이 역량은 평생 필요합니다.

     “상사와의 협업은 단순히 눈치보기나 따라하기가 아닌, 내가 더 편하게 일하고 성장하는 능동적인 전략입니다. 내가 변하면 상사, 팀, 조직도 서서히 바뀝니다.”

   명확하고 주체적인 커뮤니케이션, 반복적·충실한 공유, 문제해결 관점이 Managing Up의 본질입니다. 이 리스트를 업무 현장에서 한 가지씩 실천하면 변화된 협업을 경험할 수 있습니다.
"
"https://news.hada.io/topic?id=22012","OpenAI에 대한 회고","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             OpenAI에 대한 회고

     * 저자는 2024년 5월 입사 후 1년 남짓 OpenAI에서 일하고 퇴사, 사내 문화와 실제 일하는 분위기에 대해 솔직하게 서술함
     * 초고속 성장(1,000명→3,000명) 속에서 내부 프로세스·조직·문화·일 방식이 빠르게 변하고 있음
     * 바텀업/실력주의 문화, 독특한 슬랙 중심 협업, 높은 실행력, 리더십의 가시성과 신속한 방향 전환, 그리고 '** 코드가 답**'이라는 태도가 조직 곳곳에 녹아 있음
     * 세부 팀 문화·업무 속도·조직 유연성이 강하며, 연구자 개인의 '미니 경영자'적 자율성, 중복 프로젝트 및 사내 아이디어 실험이 빈번함
     * OpenAI는 외부 시선과 언론의 집중 감시, 실질적인 보안/비밀주의, 그리고 AGI/소비자 서비스라는 사명감과 긴장감을 동시에 가진, 야심차고 진지한 조직이라고 설명
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 개인적 배경

     * 2024년 5월에 입사하여 최근 OpenAI를 퇴사하게 되었음
     * 이 글을 통해 OpenAI에서 느꼈던 실제 문화와 개인적인 시각을 공유하고자 함
     * 내부 비밀은 없으며, 역사적으로 흥미로운 조직의 현재 모습과 직원의 작은 창구로서 경험을 담고 있음
     * 퇴사 결정에는 개인적 갈등이 있었으나, 스타트업 창업자에서 대규모 조직 직원으로 전환함에 따른 신선함에 대한 갈망이 있었음
     * AGI 구축에 참여한 경험과 Codex 출시에서 직접 기여한 점은 대단히 의미 있었음

조직 문화

     * 입사 시점 1,000명, 1년 뒤 3,000명 돌파 등 비정상적으로 빠른 성장 경험
     * 빠른 확장으로 인해 커뮤니케이션, 보고 체계, 제품 출시, 조직 관리 등에서 다양한 문제가 발생함
     * 모든 의사소통·업무가 Slack 중심, 이메일 거의 사용하지 않음
     * 팀마다 문화/페이스 차이가 큼, 연구, 적용, GTM(Go-To-Market) 등 시간 흐름도 다름
     * 실질적인 바텀업·실력주의가 강하며, 연구자·개발자 개인이 주도적으로 실험·의사결정함
     * 성과 기반, 실력 우선의 조직 문화로, 정치적 능력보다는 실행력과 아이디어가 중요함
     * 공식 로드맵 없이, 좋은 아이디어를 중심으로 팀이 자연스럽게 모이고, 빠르게 방향 전환하는 경향
     * 리더십은 실행력(doing the right thing), 변화에의 민첩성을 중요시
     * 내부적으로 중복 개발/병렬 실험 많고, 여러 프로토타입이 자생적으로 만들어지며, '코드가 움직이는 조직'임
     * 리더들은 정치적 역량보다 실제 아이디어 실행 능력에 더 비중을 둠
     * 연구자들은 ""미니 경영진""처럼 각자 주도적으로 문제 해결에 몰두함
     * 유능한 연구 관리자와 PM의 영향력이 매우 큼
     * ChatGPT EM들은 매우 신뢰할 만하며, 좋은 인재를 고용해 자율성을 부여함
     * 방향 전환의 속도가 매우 빠르며, 결정 후 즉시 실천함

업무 방식과 분위기

     * Slack 채널·권한 구조가 복잡하고, 모든 소통이 Slack에서 이루어짐
     * 연구팀/PM/EM(엔지니어링 매니저) 등 역할별로 각기 다른 방식, 팀 간 이동과 협업 유연성이 매우 높음
     * 외부 보안·언론 노출에 매우 민감하여, 실적/매출 등 내부 정보는 철저히 관리됨
     * 실제 구성원들은 '옳은 일'을 하려는 동기가 강함, 외부에서 생각하는 것만큼 냉소적이지 않음
     * OpenAI는 여러 하위 문화가 혼재한 '로스앨러모스(핵 연구소)+초대형 소비자 서비스' 혼합형 조직으로 비유
     * AI 혜택의 폭넓은 배포를 중시, 최첨단 모델도 엔터프라이즈에만 한정하지 않고, 누구나 API/ChatGPT로 사용 가능하게 공개함

안전 및 내부 정책

     * AI 안전 이슈는 실제로 내부에 많은 인력·자원이 투입되어 있음
     * 실질적으로는 혐오발언, 오남용, 정치적 편향, 프롬프트 인젝션, 자기해 손해 등 실제 위험을 더 많이 다룸
     * 이론적 위험(지능 폭주, 파워시킹)은 일부 인력이 전담하지만 주류는 아님
     * 안전 관련 연구나 시스템의 상당 부분은 외부에 공개되지 않고 있음

개발 환경과 기술

     * 거대한 모노레포(mono-repo) 와 Python 중심, Rust/Golang 일부 도입, 스타일 가이드 강제 거의 없음
          + Google 출신 고참이 설계한 대규모 시스템과, 신규 박사가 작성한 Jupyter notebook이 혼재되어 있음
          + FastAPI 중심 API, Pydantic 데이터 검증 사용이 두드러짐
     * 모든 인프라는 Azure 위에서 구동
          + 신뢰할 만한 서비스는 Azure Kubernetes Service, CosmosDB, BlobStore 정도로 한정적임
          + IAM 수준 및 일부 서비스는 AWS에 비해 미흡하며, 사내 자체 개발 지향
     * Meta(구 Facebook) 출신 엔지니어 대거 유입
          + 인프라 감성과 코드베이스가 Meta/Instagram 초기와 유사함
          + 예: TAO 재구현, 인증 체계 통합 등 자체 시스템 개발 흔함
     * 중복 코드·도구/큐 관리 라이브러리·대규모 백엔드(monolith) 관리 등 급성장 조직의 고질적 문제를 실감, CI 속도/안정성 이슈 존재
     * Chat 메시지·대화 구조가 코드 곳곳에 깊이 내재, 제품마다 반복적으로 활용
     * '코드가 이긴다(Code wins)': 중앙 기획위원회 없이 실제로 일하는 팀의 코드가 표준이 됨
          + 결정 권한은 해당 작업을 직접하는 팀에 있음, 코드에 의한 실력과 실행 우위 체계

소비자 브랜드와 비즈니스 관점

     * Consumer 브랜드의 거대함: 핵심 지표는 팀 단위가 아닌 개인 사용자 구독 기준으로 운영함
          + 프로덕트 성장·트래픽은 '프로 구독자 수' 등 소비자 단위로 측정, B2B 조직 출신인 저자에게 신선한 충격
     * 모델 학습·실험은 소규모에서 시작해, 성공시 대규모 분산 시스템 엔지니어링으로 확장되는 구조
     * GPU 비용이 압도적 비중을 차지, 사소한 기능조차 방대한 GPU 리소스가 필요
          + GPU 사용량 산정과 벤치마킹: 요구되는 지연시간/토큰 수 등 사용자 경험 기준에서 역산함
     * 대규모 Python 코드베이스 운용 노하우: 개발자 수가 증가함에 따라 기본 작동, 테스트, 오용 방지 등 다양한 가드레일 필요

팀 운영과 리더십

     * 리더십은 매우 가시적이고 직접 참여하며, 모든 임원이 Slack에서 수시로 논의 참여
     * 팀 이동·협업 매우 빠름, 타 팀 요청에도 즉시 응원군 투입, 대기나 절차 없음
     * 사내 스왝(swag)도 드물고, 내부적으로 한정 판매 형식으로만 제공

Codex 런칭 경험

     * 최근 3개월, Codex 출시가 커리어 하이라이트였음
     * 2024년 11월 2025년 내 코딩 에이전트 출시 목표 수립, 2025년 2월 경 내부 도구 완성 및 시장 경쟁 속도에 대한 압박을 느낌
     * Codex 런칭을 위해 팀이 합쳐져 7주 만에 완제품(코딩 에이전트) 완성 및 출시, 짧은 개발 기간 내 영향력 있는 제품을 빠르게 구현함
          + 실제로 밤샘, 주말 근무, 신생아 육아를 병행하며 YC 당시 느낌을 재현함
          + 컨테이너 런타임, repo 최적화, 커스텀 모델 파인튜닝, git 연동, 인터넷 접근 등 다양한 기능을 신속하게 구현함
          + 팀 구성은 시니어 8명 엔지니어, 4명 연구원, 2명 디자이너, 2명 GTM, 1명 PM 등 고참 위주의 소수 정예 팀
     * 런칭 전날, 직접 배포 등 마무리 작업에 집중
     * 런칭 당일 트래픽 폭주, ChatGPT 사이드바에 등장한 것만으로도 즉시 대규모 유입 발생
     * Codex는 비동기 에이전트 방식(사용자-에이전트 메시지→작업→PR 결과 반환) 채택
          + 독립 실행 환경에서 사용자 요청을 처리하여 협업자처럼 PR 결과를 반환하는 구조
          + 아직 모델 성능의 신뢰와 한계가 혼재되어 있음
          + 다중 태스크 실행, 대형 코드베이스 이해 능력 등에서 Codex의 차별성이 존재함
     * 출시 53일 만에 630,000 개의 PR 생성, 엔지니어 1인당 78,000개 이상의 PR을 기록하며 압도적인 임팩트 창출

마무리 및 교훈

     * 큰 조직에서 일하는 것에 대한 두려움이 있었으나, 돌아보면 최고의 결정 중 하나로 학습과 성장의 기회였음
     * 목표했던 모델 훈련에 대한 직관, 우수한 동료와 협업, 임팩트 있는 제품 출시 모두 달성함
     * 대규모 파이썬 코드베이스 관리 노하우 습득, 실전 GPU 벤치마킹/용량 산정 등을 실제 경험함
     * 스타트업 창업자거나 진로 고민이 있다면 더 적극적으로 도전하거나, 거대 연구소 합류를 고려할 만한 시점임
     * AGI를 향한 경쟁은 3마리의 말, 즉 OpenAI, Anthropic, Google이 각각 다른 방식을 추구 중이며, 이 중 한곳에서 일하는 경험은 지평을 넓혀줄 것임
     * OpenAI 경험은 창업가이자 엔지니어로서 최고의 선택 중 하나로 평가함

   https://news.hada.io/topic?id=21081 이 글이 기억에 남네요.

        Hacker News 의견

     * 퇴직자가 자신의 근무 경험을 긍정적으로 묘사하는 경우는 흔하지 않음, 이는 OpenAI가 특별해서라기보다는 대부분의 ‘왜 회사를 그만뒀는가’ 포스트가 실은 개인이 조직에 맞지 않았던 이유를 조직 탓으로 돌리려는 경향을 보여줌, 이 글에서 ‘믿을 수 없게 바텀업 방식이다’라는 표현의 이면에는 명확한 로드맵이 없고 각자가 소유하는 프로젝트가 없어 방향성을 잃는 사람들이 있을 수 있음, 또한 ‘행동 중심성’과 ‘즉각적인 방향 전환’은 혼란스러운 환경과 일관성 없는 임원 리더십을 의미할 수 있음, 그리고 “OpenAI엔 실제로 선의의 사람들이 많음”이라는 말은 대부분 도덕적 판단이 복잡한 결정을 내리는 회사에 해당됨, 모두가 스스로를 좋은 사람이라고 여기며 큰 목표와 명분으로 합리화하는 흐름임
          + 나는 절대 공공장소에서 고용주에 대한 비판을 남기지 않음, 이는 내 경력에 해를 끼칠 수밖에 없음, 특히 Altman이 보복적이라는 소문도 있으니 OpenAI는 두 배로 조심해야 함, 심지어 이 글에서는 OpenAI가 소셜미디어까지 모니터링한다고 함, 이 퇴직자는 자신의 짧은 14개월 경력을 긍정적으로 포장해 평판을 관리하려는 의도도 있어 보이고, 이런 모습이 오히려 미래 고용주에 어필이 되는 것 같음
          + ""회사에 악당은 없다. 좋은 사람들이 스스로를 합리화할 뿐""이라는 말이 있었는데, 나는 예전에 카지노용 소프트웨어 회사에서 근무해봤는데, 거긴 정말 대놓고 악당 같은 사람들이 경영진이었음
          + OpenAI는 퇴사 후 부정적으로 말하면 이미 부여된 지분을 모두 박탈당하기 때문에, 긍정적인 경험담이 훨씬 흔한 구조임
          + 내 생각에 Altman은 AGI가 곧 나온다고 대중을 설득하는 데 집중하는 동시에, OpenAI를 강력한 프로덕트 회사로 만들기 위해 많은 노력을 들인 것으로 보임, 그리고 실제로 성공적으로 해낸 것으로 보임, 회사 내 큰 자부심과 경쟁 속에서 퇴사자가 여러 정치적인 싸움에 일부 패배하거나, 자신의 Codex 프로토타입이 채택되지 않는 등 상처를 입었을 수도 있음, 혹은 이미 충분한 돈과 인생 경험을 쌓았기에 더 이상 젊은 인재들과 경쟁할 동력이 사라진 것일 수도 있음
          + 퇴직자들이 자신의 경험을 부정적으로만 이야기하는 게 아니라, 오히려 너무 긍정적으로 포장하는 사례가 정말 많음, 내가 다녔던 회사에서도 독재적인 CEO 아래 심하게 독성적인 환경이 되어서, 많은 이들이 고생했음에도 불구하고 미래 취업을 위해 블로그나 LinkedIn에 찬양글을 썼음, HN에서 화제가 되는 글들은 회사에 애정을 가졌던 직원들이 회사나 부서가 몰락하는 것을 아쉬워하며 쓰는 경우가 더 많은 듯함
     * 이 글에서 인상적인 점은 다음과 같음
          + 진보는 반복적이고, 바텀업과 실력 위주 문화가 있음, 경영진의 ‘마스터 플랜’이 아니라 누구의 아이디어든 현실이 될 수 있고, 실질적 실행력과 아이디어로 리더가 승진함
          + 팀원들이 허락 없이도 프로젝트를 주도적으로 시작할 수 있어, 여러 개의 병렬 프로젝트가 자연스럽게 생기고 성공 가능성 있는 것에 자원이 집중됨
          + OpenAI 구성원들은 선의로 일한다는 의식이 강하며, 세간의 비판에도 불구하고 진지하게 책임감을 가지고 옳은 일을 하려 함
          + 회사의 프로덕트가 대중 정서의 영향을 크게 받고, 실제로 회사가 ‘트위터 분위기’를 따라 움직이는 느낌임
          + GPU 비용이 압도적으로 높아, 다른 인프라 비용은 거의 의미 없을 정도임, 컴퓨팅 파워 확보가 재무와 기술의 최우선 과제임
          + AGI를 향한 길이 OpenAI(컨슈머 프로덕트 DNA), Anthropic(엔터프라이즈 DNA), Google(인프라/Data DNA)의 삼파전으로 설명된다는 점이 흥미로웠음
          + Meta도 소비자 중심 DNA를 가진 중요한 경쟁자임, 소비자를 정말 '프로덕트'로 만드는 데 대표적인 역할을 해왔음
     * Codex 개발 마라톤이 지난 10년 중 가장 힘들었던 업무였다는 부분이 눈에 띔, 대부분 밤 11시~자정까지 일하고, 아침 5시 반에는 갓난아이를 돌보며, 7시에 사무실로 나가는 삶이었음, 몇 주~몇 달 만에 대규모 프로젝트가 완성되어버리는 타이트한 업계 분위기에서, 이러한 워크스타일이 과연 직원들에게 장기적으로 지속 가능할지 의문임
          + 누가 나에게 저런 모드로 일하라고 강요하면 절대 거부하겠지만, 내가 진짜 흥미롭고 중요하다고 느끼는 프로젝트라면 몇 주~몇 달 올인하는 것도 좋음, 이런 프로젝트 이후에는 모든 에너지가 빠진 상태일 것을 알기에 계획도 미리 세워둠, 나와 비슷한 문화를 가진 커뮤니티 덕분에 지속적인 동기부여를 얻기도 함
          + 이미 경제적으로 여유 있는 사람이 갓난아이 돌보는 대신 16~17시간씩 7일 내내 일하는 선택을 한 건 대단함, 결혼한 파트너에게 ""육아를 맡겨서 고맙다""고 남긴 데서 다 말해줌
          + 이런 방식의 근무는 절대 지속 가능하지 않음, 하지만 커리어 동안 몇 번 일어나는 일이라면 충분히 해볼 만하고, 오히려 활력을 얻었다는 지인도 있음
          + 배우자에게 육아 부담을 전적으로 넘긴다는 상상 자체를 못하겠음, OP의 아내가 대단하고, 그런 점을 마지막에 언급한 것은 좋지만 솔직히 놀라움
          + 글쓴이가 14개월 만에 OpenAI를 그만둔 걸로 보아, 이런 근무 패턴이 번아웃으로 이어진 듯함
     * 정말 궁금한 건 OpenAI나 다른 AI 연구소들이 실제로 내부 운영에 LLM을 주춧돌로 적극적으로 활용하는지임, 코드 개발, 내부 모델 커스텀, 최신 정보 정리 등 실무용으로 실제로 돈과 역량을 투입하는지 알고 싶었으나, 기사에는 언급이 없어 아쉽게 느꼈음
     * 엔지니어들이 ‘신’을 만든다는 생각을 품게 하게 하는 게 최고 수준의 마케팅 전략임, 실제로 나는 그게 사실이라고 믿지는 않지만, 이 아이디어는 비판이 거의 먹히지 않는 구조임, “만약 정말이라면 어떡할 것인가”라는 질문으로 언제든 반박할 수 있고, 잠재적 이득이 무한대라 작은 확률이라도 무시할 수 없게 됨, 확률 0.00001%라도 무한의 보상과 곱해지면 기대값이 무한대가 되는 논리임, 최고의 마케팅임
          + “하지만 진짜일 수도 있잖아?”라는 질문이 LLM 개발사들의 이야기의 일환으로서, 미스테리한 요소를 심어주고 있음
     * 내가 가장 알고 싶었던 건 OpenAI 내부에서 LLM이 실제 프로덕트 빌딩에 얼마나, 어떤 방식으로 활용되는지임
          + 53일 동안 엔지니어 한 명당 78,000개의 퍼블릭 pull request가 있다는 설명은 거의 99.99%가 LLM에 의해 쓰였을 것이라는 농담처럼 들림, 글에서 공개한 업무 프로세스 정보가 많아 놀라웠고, 이런 건 보통 비밀로 지켜야 하지 않나 싶었음, 참고로 78,000개 PR 통계는 Codex 엔지니어가 아니라 전체 사용자 기준임
     * 이렇게 빠르게 성장한 회사임에도, OpenAI의 테크니컬 라이터 부족이 계속 놀라움, 문서가 개선될 수 있다고만 표현하는데, 실제로 Anthropic의 문서화 수준과 비교하면 OpenAI에는 동료 테크라이터를 찾기 힘듦, 좋은 개발자 도구를 만들려면 우수한 문서가 필수이며, 이를 전담하고 발전시키는 팀이 꼭 필요함
          + 경영진이 문서화의 가치를 못 느끼는 게 문제임, 예전에 DigitalOcean에선 업계 최고 수준의 기술 문서팀이 있었지만, 정리해고 때 가장 먼저 잘렸음, 비용으로만 보는 시선이 크다고 느낌
     * 이 글엔 정말 처음 듣는 흥미로운 정보가 엄청 많았음, 시간을 들여 볼 만한 가치가 있음
     * “안전이 생각보다 중요하게 여겨진다”는 글쓴이 의견에 대해, 실제로 OpenAI의 여러 안전 팀 리더들이 퇴사했거나 해고되었고, Superalignment 프로젝트가 실패했으며, 다른 직원들이 안전 이슈 지원 부족을 언급한 것을 감안하면 이러한 발언은 현실과 동떨어져 있거나 의도적으로 오도하는 것처럼 느껴짐
     * “대부분의 연구는 연구자가 특정 문제에 마음을 빼앗겨 시작된다”는 글이 흥미로웠음, 만약 이 진단이 맞다면 회사의 아킬레스건이 될 수 있다고 봄
          + 하지만 이는 특정 회사 문제가 아니라 인간의 본질적인 문제임, 정상급 연구자는 자신이 진정 사랑하는 분야에 미쳐 어마어마한 시간을 기꺼이 쏟는 성향임
"
"https://news.hada.io/topic?id=22010","Shoggoth Mini – GPT-4o와 RL로 구동되는 소프트 촉수 로봇","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Shoggoth Mini – GPT-4o와 RL로 구동되는 소프트 촉수 로봇

     * Shoggoth Mini는 GPT-4o와 강화학습을 활용하여 자연스럽고 표현력 있는 동작을 구현한 소프트 촉수 로봇임
     * 기존 가정용 로봇과 달리 인간과 상호작용 시 내부 상태나 의도를 표현할 수 있도록 설계됨
     * 하드웨어 설계부터 소프트웨어, 카메라 인식 및 제어 시스템까지 세부적으로 전 과정을 설명함
     * 2D 트랙패드 입력을 3D 촉수 제어에 직관적으로 매핑하여 사용성을 높였으며, 최신 컴퓨터 비전과 RL 기술 접목으로 동작 정밀도와 표현력을 확보함
     * 마무리에서는 로봇의 생동감, 예측 불가능성과 인간 친화성의 균형점에 대한 고민과 향후 확장 아이디어를 제시함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론 및 배경

     * 최근 1년간 로보틱스 분야가 대형 언어 모델(LLM) 시대와 접목되면서 빠르게 발전함
     * 대표적으로 π0.5, Tesla Optimus 등은 언어로 명령을 이해하여 실제 동작을 수행하지만, 대부분 기능적 로봇의 수준에 머무름
     * 인간-로봇 상호작용의 자연스러움과 내부 상태(의도, 주의, 확신 등) 전달에는 '표현력(expressiveness)' 이 핵심이라는 점을 지적함
     * Pixar 느낌의 램프(Apple ELEGNT)와 단순 동작만으로 이상하게 ‘살아있는’ 느낌을 주는 SpiRobs 등에서 착안, 의도적 표현 설계와 우연적 생명감의 차이에 관심을 가짐
     * 이를 실험하기 위해 Shoggoth Mini 제작을 시작, 우연과 실험을 통한 설계 과정과 깨달음을 공유함

하드웨어

     * 테스트베드 첫 버전은 3개의 모터와 촉수를 고정할 판, 돔형 커버의 단순 구조로 시작함
     * 3D 프린팅 중 필라멘트가 모자라 돔 일부분 색상이 달라져 볼, 입이 붙은 얼굴 모양 탄생, ChatGPT로 시각적 시안을 탐색하여 형태 확정함
     * 돔에 스테레오 카메라 장착하여 촉수 추적, 로봇 눈의 ‘예측 오류’가 시선을 집중시키는 효과 유발
     * 오픈 스풀 설계는 케이블이 쉽게 빠지고 얽히는 단점이 있어, 간단한 스풀 커버 추가로 문제 해결 및 반복 실험 속도 개선
     * 캘리브레이션 스크립트 및 예비 와이어 길이 추가로 유지보수와 모터 장력 조절을 빠르게 처리함
     * 촉수의 처짐 현상 최소화를 위해 등뼈(스파인)를 적당히 두껍게 조정함
     * CAD 조립도와 3D 프린팅용 STL 파일 모두 GitHub 저장소에 공개함

수동 제어

     * 초기에는 트랙패드를 이용한 2D 조작으로 촉수를 직관적으로 움직일 수 있도록 3개의 힘줄(텐던) 길이 제어를 2차원 컨트롤로 단순화함
          + 각 힘줄은 2D 평면상에서 주 방향을 가지며, 커서 방향 벡터를 각 축에 투영해 필요한 장력만큼 길이 변화량 계산
          + 양수는 힘줄 단축, 음수는 연장 의미
     * 이 2D→3D 변환 방식을 자동화 제어(강화학습 등)에서도 기준 projection layer로 재사용함
     * 제한된 조작 범위이지만 직관적 제어 가능, 머리응답성 및 사용자 경험 크게 향상

시스템 설계

     * 두 개의 계층적 제어 구조:
          + 저수준에서는 오픈 루프 정책(예: <yes>, <shake>) 및 클로즈드-루프 RL 정책(예: 손가락 추적)으로 동작, 촉수 팁/손 위치 추적은 스테레오 비전 기반 파이프라인에서 담당
          + 고수준에서는 GPT-4o가 실시간 음성/텍스트(아직 비전 미공개) 처리, 비디오 이벤트(손 흔들기 등)는 텍스트 cue로 GPT-4o에 전달되어 API 호출 결정
     * 대형 모델 기반의 직접적 end-to-end 비전-언어-액션(VLA) 통합보다는, 전용 비전과 경량화된 개별 컨트롤러의 계단식 구조로 설계
     * RL 관측 범위 조절 및 자연스러운 호밍 동작, API 호출 사이 idle(숨쉬기 모드) 적용해 로봇의 ‘살아있는 느낌’ 강화
     * VLA의 실질적 한계(예: 시간 보정 문제)로 프롬프트 엔지니어링이 중요 요소로 작용함

인지 퍼셉션

     * 손 추적용 MediaPipe, 촉수 팁 추적용 커스텀 데이터셋 및 YOLO 모델 결합
          + k-means 클러스터링과 Roboflow의 자동 라벨링/액티브 러닝, Segment Anything 활용해 데이터셋 증강 및 정밀 라벨링
          + Ultralytics YOLO로 훈련, DeepLabCut notebook으로 카메라 내부/외부 파라미터 추정 및 3D 삼각측량 구현
     * 실시간 3D 손-촉수 위치 추적으로 강건한 폐루프 제어 실현

저수준 제어 API

     * 소프트 로봇은 강체와 다르게 역기구학이 잘 통하지 않으므로, 2D control projection 방식을 일관되게 적용
     * 등뼈를 두껍게 만들어 세션 간 동작 재현성도 확보
     * 물체 집기(그랩) 동작 등 실험을 통해 소프트 로봇만의 기계적 강건성(잡는 물체 모양/무게 다양성 흡수) 발견
     * 고주파 미세 조작은 어렵지만, 기본 조작은 기계적 설계만으로 의외로 안정적임

  강화학습(RL)

     * 사용자 손가락 추적과 같은 단순 정책부터 RL 적용, MuJoCo 시뮬레이터에서 동적 임의화(PPO, MLP, frame stacking, 질량/마찰/감쇠 랜덤화)로 sim-to-real 전환성 향상
     * 초기엔 tendon 길이 자체를 액션 스페이스로 썼으나, reward hacking 및 실제 이식 실패
     * 2D projection 방식으로 액션 제한하여 비현실적 동작(혼란, 진동 등) 억제, curriculum learning으로 점진적 고차원 확장 가능성 제시
     * 급격한 액션 변화로 인한 진동(jitter) 보완을 위해 reward에 penalty 항목 추가, EMA로 액션 평활화

결론 및 미래 방향

     * 초기에 느꼈던 예측 불가, 해석의 여지가 ‘살아있음’(aliveness) 느낌을 주었으나, 분석·내면화가 진행될수록 피드백의 신선함은 점차 사라졌음
     * 표현력(의도 전달) 자체와 생명감(예측 불가능성)의 균형이 로봇-인간 상호작용에 결정적임을 강조
     * 향후 확장 아이디어:
          + 비인간적 목소리 부여
          + 2D 제약 해제
          + RLHF 등으로 표현 동작 다양화
          + 촉수 추가 및 자가 이동 구현
          + 직구동 모터 채택 통한 소음 저감
     * 소스코드 및 파일은 GitHub 저장소에서 제공하며, 협업 및 논의 환영

        Hacker News 의견

     * 기술과 인간 심리의 흥미로운 만남임을 느꼈음, 로봇이 처음에는 매우 생동감 있게 느껴졌으나 사용 시간이 지나면서 동작 예측이 쉬워지자 점차 덜 살아있는 느낌이 들었음, ‘표현력’은 내부 상태를 전달하는 것에 관한 것이지만 ‘생명력’은 예측 불가능성과 어느 정도의 불투명함에서 비롯함, 실제 살아있는 시스템이 복잡하고 다양한 변수를 추적하기 때문임, Shoggoth Mini는 그런 수준이 아님, 우리가 정말 살아있는 것처럼 느껴지길 바라는 로봇을 원하는지, 아니면 너무 예측 불가능해져서 인간 곁에 두기 불편해질 수 있는 한계점이 존재하는지 궁금함
          + Furby가 생각남, 비슷한 형태와 크기, 두 개의 눈과 움직이는 귀 때문임, 초반에는 신기하지만 조금만 만져보면 아주 단순한 자극과 내부 상태 조합에 따라 제한된 동작을 보임, 많은 사람들이 “사람들도 똑같은 거 아님?”이라고 농담을 하긴 하지만 실제로 금방 반복 패턴을 깨달음
          + 게임 시스템의 규칙을 이해하게 되면 더 이상 재미를 못 느끼는 것과 비슷함, 규칙 적용만으로 복잡해 보이더라도 결과가 정해진 느낌을 받음, 그 마법 같은 재미가 사라짐
          + 인간이 불에, 흐르는 물에, 18세기 오토마타에, 원시 챗봇에, ChatGPT에 – 심지어 여러 기계들에까지 의인성과 자율성을 부여해왔음, 기계가 때로는 ‘기분’까지 있는 것처럼 보일 때도 있음
          + 음성비서나 인공언어와 관련된 경험을 공유함, 음성비서는 영국식 악센트로 설정하는데, 적당히 이질적으로 느껴져서 훨씬 신뢰할 수 있음, 영국인한텐 아닐 수 있지만 본인은 그렇게 느낀다는 견해임, 과거 게임 개발에 참여하면서 NPC의 대사를 동적으로 생성하려 해봤지만, 영어로 현실감 있게 만드는 건 매우 어려웠음, 결국 NPC들이 허구의 언어로 말하게 했고, 유저가 그 언어를 배우도록 퍼즐 요소를 더했을 때 훨씬 현실감 있는 캐릭터로 느껴졌음, 사실 번역에 에너지를 쓰니 인공언어라는 사실을 잠시 잊게 해 ‘언캐니 밸리’를 피하는 셈이었음, 다만 게임에 익숙해지고 언어 숙련도가 올라가면 조작된 인공시스템임을 결국 알아채게 될지 지금은 궁금함
     * “아, 망설였네”라기보단 어느 질문이나 항상 비슷한 망설임이 느껴짐, GPT가 반응을 생성하는 딜레이가 상당히 신경 쓰이며, 뉴스 중계 연결 지연보다 더 불편한 느낌임, 눈에 LED 같은 걸 달아서 동작 중임을 표시하면 좀 나을까라는 생각임, GPT에 질의를 넘기면 어쩔 수 없이 지연이 발생함, 특히 클라우드로 요청을 보내면 더욱 그렇다는 아쉬움 공유, GPT-4o가 오디오 스트림 전체를 항상 듣는 설정은 문제의 소지가 있다고 봄
          + 이런 간단한 과제라면 Qwen 0.6B 같은 소형 LLM으로 충분하다고 봄, 대형 모델의 zero-shot 성능을 이용해 자체 데이터셋을 만들고 훨씬 빠른 별도 모델을 훈련시키는 방법이 있다고 생각함
          + GPT-4o가 계속 음성을 듣는 부분에 대해선 wake word 라이브러리(예: openWakeWord, porcupine)를 사용하면 해결할 수 있다고 봄, 사용자가 특정 호출어로 깨운 다음 프롬프트를 보낼 수 있게 하면 보안·프라이버시 이슈도 줄일 수 있음, 평소에는 ‘자고 있음’ 애니메이션을 표시하다 호출하면 깨어나 반응하는 식으로 만들 수 있을 것이라고 제안함, 이와 관련된 오픈소스 링크 공유 openWakeWord porcupine
          + GPT가 응답할 때 딜레이가 불편하다는 의견엔 공감하지 않음, 촉수가 멈추고 곧장 세워질 때 마치 집중해서 듣고 생각하는 듯한 인상을 주기 때문에 오히려 귀엽게 느껴짐
          + Johnny 5처럼 눈썹이 필요하다고 생각함 Johnny 5 예시 영상
          + 프로토타입 단계 이후에는 작은 최적화 모델을 로컬 디바이스에 직접 올릴 수도 있다고 봄, 이렇게 하면 훨씬 빠르고 안전하며 최종 제품에 적합함(다만 프로토타입에서는 유연성은 떨어짐)
     * 이 아이디어에 영감을 준 SpiRobs의 멋진 영상을 공유함 SpiRobs 유튜브 영상
     * 디자인이 너무 귀여움, 작년에 촉수(텐타클) 로봇을 연구했을 때, 공식 용어는 “continuum robots”라는 걸 알게 되었고, 특히 의료로봇 분야에서 많은 연구가 이루어지고 있음, 더 알고 싶다면 좋은 개론 강의가 있으니 참고하면 좋겠음 관련 강의 영상
     * 정말 멋진 프로젝트라고 생각함, AI 능력이 이렇게 많아졌고 로보틱스도 발전하는 요즘, 왜 항상 인간 형태만 만드는지 아쉬움, 집안에 거미-오징어 하이브리드 로봇 같이 독특한 형태의 로봇이 나타나길 바람
          + AI 안전 논쟁이 많은데, 나는 거미-오징어 하이브리드 로봇이 집 안을 돌아다니길 원함
          + 로보틱스를 인간형 세계와 가장 호환되게 만들려는 경향임, 하지만 다리 형태를 바꿔 다양한 옵션을 시도하는 것도 충분히 흥미로울 것 같음
     * 절대 안됨, 이런 영화 봤음, 페이스허거(생명체)가 책상 위에 앉아있는 건 절대 원하지 않음
          + Hentai 팬들은 오히려 반길 수도 있음
          + 만약 촉수가 더 길고, 동료마다 장난칠 수 있는 프로그램이라면 오히려 재미있을 거라는 아이디어임
     * 러브크래프트(소설가) 레퍼런스 좋다고 생각함, 소형 모델로도 충분하지 않을까 궁금함
          + Shoggoth 밈에 대해 참고할 만한 링크 공유함 Shoggoth With Smiley Face Meme NYT Shoggoth Meme 기사
          + 나도 비슷한 의견임, 사실 더 단순해질 수 있을 것 같음, 개발자가 “엔드투엔드 VLA 모델 훈련을 고민했지만, 케이블로 구동하는 연질 로봇은 같은 팁 위치에 다양한 케이블 길이 조합이 생겨 예측 불가능성이 커서 시연 기반 학습(데모 기반)이 잘 확장되지 않는다”며, 최종적으로는 특화된 비전이 경량 컨트롤러에 입력되는 계단식 설계로 갔다는 이야기를 인용함, 그럼에도 소형 모델로 다시 시도하면 참 멋질 것 같고, 로컬 모델을 얹는다면 펜타그램 안에 두는 게 안심될 거라고 농담함
     * 일본에서 온 미디어를 충분히 봐서 이게 어떤 쪽으로 갈지 예측할 수 있음
          + 다행히 안전을 위한 플레어드 베이스가 있음
     * 발상 자체가 너무 멋짐, 비언어적·비인간형 존재에 표현력을 더하는 시도에 끌림
          + Pixar의 램프 로봇이 좋은 영감이 될 수 있다고 봄, 로봇이 구부러지거나 몸을 움직이며, 고개를 끄덕이거나(예/아니오), 호기심, 짜증 같은 감정 표현을 하는 것, LED로 표정까지 변화할 수 있게 하는 여러 아이디어를 나눔
     * 정말 아름다운 작업이라고 생각함, 이 로봇이 자연계의 어떤 생명체도 닮지 않게 디자인된 것이 특히 인상적임, 로보틱스와 현실의 경계가 모호해지는 미래는 원하지 않음, 지금까지는 휴머노이드 로봇도 여전히 인공적으로 보이기 때문에 그 트렌드가 계속됐으면 함
"
"https://news.hada.io/topic?id=22032","RAG는 죽지 않았다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              RAG는 죽지 않았다

     RAG의 미래는 ""더 큰 컨텍스트 창이 아니라, 더 나은 검색에 있다""

     * ""RAG Is Dead""라는 말은 2023년식 단순 RAG 구현 방식에만 해당되며, 진짜 문제는 정보 손실이 큰 단일 벡터 기반 검색임
     * 기존 IR 평가 지표는 RAG에 적합하지 않으며, 사실 포괄성·다양성·관련성을 중심으로 한 새로운 평가 기준이 필요함
     * RAG의 검색기는 단순 매칭을 넘어 지시문을 이해하고 추론 기반으로 관련 문서를 선택하는 방식으로 진화 중임
     * ColBERT 스타일의 레이트 인터랙션 모델은 정보 압축 없이 토큰 단위 표현을 유지해 소형 모델이 대형 모델을 능가함
     * 완벽한 임베딩 하나를 찾는 대신, 다양한 표현을 위한 다중 인덱스와 스마트 라우팅 구조가 새로운 표준이 되고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Why the future of RAG lies in better retrieval, not bigger context windows

  “RAG는 죽었다”는 주장에 대한 반박

     Part 1. I don’t use RAG, I just retrieve documents - 단순 벡터 검색이 죽은 것이지, RAG 자체는 아님

     * Hamel과 Ben Clavié는 RAG가 죽지 않았으며, 오히려 검색 구조가 진화할 시점이라고 주장함
     * 벡터 DB에 문서를 넣고 코사인 유사도로 검색하는 방식은 오래됐고, 정보 손실이 큼
     * LLM은 학습 시점 이후 정보가 고정되므로, 검색 기반 정보 삽입(RAG)은 여전히 중요
     * 콘텍스트 윈도우를 늘리는 것만으로는 모든 정보를 삽입하는 것은 비효율적임

  잘못된 평가 지표

     Part 2. Modern IR Evals For RAG - 전통적인 IR 평가 지표가 RAG에 맞지 않음을 설명, FreshStack 제시

     * Nandan Thakur는 전통적인 정보 검색(IR) 평가 지표가 RAG에 적합하지 않음을 지적함
          + BEIR와 같은 벤치마크는 1등 문서 탐색만을 최적화함
          + RAG는 사실 커버리지, 다양한 관점, 문맥 관련성 등을 종합적으로 고려해야 함
          + 이를 위한 새로운 평가 시스템으로 FreshStack을 제안함

  추론하는 검색기

     Part 3. Optimizing Retrieval with Reasoning Models - 지시문 이해 및 추론 가능한 검색기의 설계

     * Orion Weller의 Rank1 시스템은 검색기가 ""데이터 프라이버시에 대한 은유가 포함된 문서"" 처럼 복잡한 지시문을 이해함
     * 단순 유사도 계산이 아닌, 명시적인 추론 경로(reasoning trace) 를 생성하여 관련성 판단 근거를 제공함
     * 기존 검색 시스템으로는 찾을 수 없는 문서를 이해와 추론 기반으로 탐색 가능

  레이트 인터랙션 모델의 가능성

     Part 4. Late Interaction Models For RAG - ColBERT와 같은 구조로 정보 손실 없이 표현 유지

     * Antoine Chaffin은 ColBERT 같은 Late Interaction 기반 모델을 통해
          + 문서를 단일 벡터로 압축하지 않고, 토큰 단위 정보를 유지
          + 그 결과, 150M 파라미터 모델이 7B 모델보다 추론 성능이 뛰어난 사례도 존재함
     * 정보를 없애지 않고 보존하는 표현 구조가 핵심

  하나의 맵이 아닌 다중 맵 필요

     Part 5. RAG with Multiple Representations - 목적별 다중 인덱스를 통한 검색 성능 향상

     * Bryan Bischof와 Ayush Chaurasia는 하나의 임베딩만으로는 다양한 검색 목적을 충족할 수 없다고 지적
          + 예: 그림 검색 시
               o 문자 설명
               o 시적 해석
               o 유사 이미지
                 를 각각 다른 인덱스에서 찾는 구조
     * 결론: 완벽한 임베딩을 찾지 말고, 다양한 표현 방식에 맞춘 다중 인덱스 + 지능형 라우팅 시스템 필요

  RAG의 미래 전략

   다음 네 가지가 RAG의 미래로 제시됨:
     * 사용 목적에 맞는 새로운 평가 기준 구축
     * 지시문을 이해하고 추론하는 검색기
     * 정보를 압축하지 않고 그대로 표현하는 구조
     * 다양한 목적별 인덱스를 조합하고 스마트하게 라우팅하는 방식

Annotated Notes From the Series

   해당 시리즈는 5부작으로 구성되며, 주요 슬라이드에 타임스탬프를 달아서 요약 제공. 각 Part 별 링크 참고

     파트                       제목                                          설명
   Part 1 I don’t use RAG, I just retrieve documents 단순 벡터 검색이 죽은 것이지, RAG 자체는 아님
   Part 2 Modern IR Evals For RAG                    전통적인 IR 평가 지표가 RAG에 맞지 않음을 설명, FreshStack 제시
   Part 3 Optimizing Retrieval with Reasoning Models 지시문 이해 및 추론 가능한 검색기의 설계
   Part 4 Late Interaction Models For RAG            ColBERT와 같은 구조로 정보 손실 없이 표현 유지
   Part 5 RAG with Multiple Representations          목적별 다중 인덱스를 통한 검색 성능 향상

   ""완벽한 임베딩을 찾지 말고, 다양한 표현 방식에 맞춘 다중 인덱스 + 지능형 라우팅 시스템""

   그게 쉬운게 아니니까...
"
"https://news.hada.io/topic?id=22056","내 은행이 피싱 방지 교육을 계속 훼손함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         내 은행이 피싱 방지 교육을 계속 훼손함

     * 은행이 신뢰할 수 없는 피싱 이메일과 유사한 이벤트 홍보 메일을 발송함
     * 본문 내 링크와 사이트 도메인이 은행과 무관해 보이고, 개인정보 입력 요구로 피싱 판단이 어려움
     * 실체 확인 후에도 공식 이벤트임을 알게 되어 혼란과 불신 증폭
     * 해당 행위는 피싱 교육 취지를 훼손하며, 은행이 법적 책임을 질 위험 높임
     * 문제 해결을 위해 신뢰성 있는 도메인 사용과 앱 내 구현 필요성 강조
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서문

   내 은행이 피싱 방지 교육을 직접 훼손하는 현실을 경험함. 은행이 보낸 이벤트 관련 이메일은 피싱 사기와 거의 구분되지 않을 정도로 의심스러운 특징을 가짐. 공식 은행 도메인이 아닌 곳에 개인정보를 입력하게 하며, 국내외 은행 및 공공기관의 보안 실태와 교육 현실의 문제점을 짚음.

1장: 의심스러운 이메일의 도착

     * 은행으로부터 “Wero-Win-Wochen(경품 이벤트)” 관련 이메일을 수신함
     * 이메일 공지는 한화 최대 주당 7,000유로 당첨 정보와 함께 이벤트 참여를 유도함
     * Sparkasse(독일 지방은행조직)와 Wero(신생 유럽 디지털 결제 시스템)가 메일에 언급
     * 메일 내부 링크는 “gewinnen-mit-wero.de”로, 은행 공식 도메인과 다름
     * 내용과 어투가 평범한 피싱 메일과 유사, 메일 주소만 Sparkasse 공식 주소임
     * 이벤트가 실제인지 은행 공식 사이트에서 확인해야 했음

  Sparkasse(슈파카세)란?

     * 지역 기반 저축은행으로 각 지역별로 독립적으로 운영됨
     * 유럽 최대 금융 서비스 그룹 중 하나임

  Wero란?

     * 유럽 결제 이니셔티브(EPI)가 만든 신규 디지털 결제 시스템임
     * 현지 결제 시스템을 통합하려는 목적으로 개발됨(초기 P2P 결제 중심)
     * PayPal과 비슷하나, 각 은행에 분산된 구조를 가짐

2장: 상황 악화 – 의심스러운 웹사이트

     * 이메일 링크 클릭 시 접속되는 이벤트 참가 사이트의 각종 디자인과 구조가 피싱 사이트와 매우 흡사함
     * Sparkasse 지점 언급이나 은행별 구분이 전혀 없음(각 은행별 독립성 무시)
     * 도메인 자체가 공식 은행 도메인과 무관하며, 누구나 등록할 수 있는 일반적인 명칭 사용
     * SSL 인증서도 무료인 Let’s Encrypt 사용, 신뢰도 저하
     * 이벤트 맥락이나 근거 설명이 부족, 단지 “돈을 받을 기회”만 강조함
     * 참가를 위해 이름, 생년월일, IBAN, 이메일 주소 등 개인/금융정보 입력 요구
     * 일반적으로 최신 디지털 금융 이벤트는 앱 내에서만 참가하도록 설계되는 분위기와 어긋남

   이로 인해 금융기관이 일부러 사용자 보안 교육을 무의미하게 만드는 결과를 초래함

보안 교육 효과 저하의 문제

     * 실제 은행마저 피싱 메일/사이트와 유사한 방식 사용 시, 이용자들은 피싱 탐지 교육 자체를 신뢰하지 않게 됨
     * “이것은 스팸처럼 보이지만 실제로 합법일 수도 있다”는 인식 확산
     * 과거에도 해당 은행이 의심스러운 문구와 도메인이 포함된 공식 SMS를 발송한 전례 있음(예: paperless.io 링크 안내)
     * 지원센터조차 why 이것이 스팸처럼 보일 수 있는지를 이해하지 못함

3장: 해결 방법은 무엇인가?

     * 가장 안전한 방안은 이벤트 참가 절차를 앱 내에 직접 구현하는 것임
     * 불가피하다면, 공식 도메인(예: sparkasse.de) 또는 각 지점의 서브도메인을 사용해야 신뢰성 유지 가능
     * 독일 정부도 유사한 사건에서 gov.de 디지털 브랜드 정책을 도입하여 서비스 신뢰성을 강화한 사례가 있음

4장: 부주의가 법적 문제로 번질 가능성

     * 최근 피싱 피해자에 대한 은행 배상 판례가 증가 중임
     * 법원은 개인정보 유출에 대한 “과실” 여부 판단에서 사용자 과실이 없으면 은행 책임으로 결론
     * 만약 현재 같은 메일/사이트 구조로 피싱 공격이 진행된다면, 피해자가 신중하지 않았다는 근거를 은행이 입증하기 어려울 것임
     * 실제 은행 공식 이메일/사이트와 피싱이 너무 유사해 법적 위험성이 커짐

결론

     * 기술적 보안은 발전하고 있지만, 사용자 경험 보안(USABLE SECURITY) 에는 여전히 허점이 남아 있음
     * 이런 사례는 피싱 방지 교육 자체의 신뢰를 훼손하며, 은행의 법적 부담 및 전체 금융권에 악영향을 미침
     * 문제는 개별적인 피드백으로는 해결이 힘든 구조적 시스템 문제임
     * 유럽 최대 금융그룹에서도 발생하는 이슈임을 더욱 심각하게 인식할 필요가 있음
     * “보안 교육을 훼손하지 마세요. 오히려 신경 좀 써주세요”라는 교훈으로 마무리됨

   기시감은 착각이 아닌 것 같습니다🤣

        Hacker News 의견

     * 내 은행은 계좌에서 의심스러운 활동이 감지되면 나에게 전화를 거는 사기 탐지 시스템을 사용함, 그리고 내게 다시 특정 번호로 전화를 해달라고 요청함, 문제는 매번 다른 콜백 번호를 제공한다는 점임, 온라인에서 그 번호로 검색해보면 결과가 딱 하나 나오는데 그건 사기 탐지 시스템 공식 웹페이지에서 어떤 전화도 믿지 말라는 내용임(이 조언은 타당하지만 아이러니하게도 자신들의 정당한 연락조차도 무시하라는 의미가 됨)
          + 카드에서 사기 탐지 시스템을 유일하게 발동시켰던 경험이 있는데, 그 때 내게 은행에서 “카드가 의심스러운 사용으로 인해 차단되었습니다, 다음 번호로 전화해 주세요”라는 문자를 받았음, 그 번호 역시 무작위로 지정된 미등재 번호였음, 내가 이를 무시하지 않은 유일한 이유는 바로 직전에 새로운 웹사이트에서 결제를 했기 때문임, 그래서 내 지역 은행에 직접 전화를 해서 이게 진짜 상황인지 확인했고, 실제로 맞다는 안내를 받음, 이런 절차가 정말 엉망이라는 불만을 토할 뻔했음
          + 은행 전체 사용자 경험(UX) 흐름을 챙기는 담당자가 없는 것 같음, 내 은행도 비슷하게 이상하게 작동함, 예를 들어 아내에게 이체를 할 때마다 매번 사기 방지를 위해 여러 질문을 확인하는데, 질문에 답하면 “자주 이체를 하니까 2FA 코드나 추가 인증은 안하겠다”는 안내문이 또 뜨는 식임, 이런 합리적이지 않은 UX는 전체 흐름을 챙기는 한 명 또는 한 팀이 없기 때문일 것임
          + 은행이 스스로 정한 규칙을 지키지 않음, 한 번은 내가 한 달 전에 요청했던 보험 변경 건으로 은행에서 전화가 왔는데, 나에게 시큐리티 동글로 본인 인증을 하라고 했음, 이런 식이니 사람들이 사기를 당해도 놀랄 일이 아님
          + 내가 일하는 은행은 “$x 금액의 수표를 발행했냐”는 문자를 보내서 이상 여부를 확인함, 문제는 가장 흔한 수표 사기가 수표 액수는 그대로 두고 수취인만 위조하는 “체크 워싱”임, 이렇게 되면 금액만 맞는 합법 거래로 보일 뿐 실제로 누구에게 지급됐는지는 확인이 안됨
          + 일반 은행 대표번호로 연락해서 한참 기다려서 상담원을 붙잡아도 실제로는 맞는 번호인데도 그 번호는 인정하지 않는다고 함, 더 골치아픈 건 내가 이용하지 않는 은행의 경우 홈페이지에 있는 번호로 전화하면 자동 응답 시스템으로 바로 연결되고, 계좌번호 없으면 접근 조차 할 수 없으니, 누군가와 직접 통화하기 위해 연락 가능한 다른 번호를 찾아야 하는 상황임
     * 내 은행(USAA)은 이전에 내가 제안한 것들을 실제로 도입해 준 적도 있음, 그런데 최근 내 고유 이메일 주소로 정상으로 보이는 이메일을 받았는데 도메인이 평소와 달랐음(내가 막 무언가 처리한 직후라 더 의심스러웠음) 바로 은행에 전화를 걸어 사기 부서 직원과 상담을 했고 “사내 시스템이 해킹당했거나 고객을 피싱에 자연스럽게 적응시키는 거다”라고 문제를 설명하며 티켓을 만들어 달라고 했음, 담당자는 그 도메인은 USAA 소유가 아니고 반드시 usaa.com만 사용한다며 내 계정을 별말 없이 잠궜음, 결국 다시 전화해서 계정을 풀었으며, 담당자가 티켓은 만들었다고 했음, 이후 진행 상황을 지켜봐야 할 것임
          + 내가 USAA 소프트웨어 엔지니어 인터뷰도 봤는데, 면접관들의 무능함을 보고서 회사에서 벌어지는 황당한 일들이 더 이상 놀랍지 않게 됨
     * 은행의 사용자 경험 관련 기술과 마케팅 관행이 최악임, 내가 써본 모든 인도 은행 로그인 폼은
          + 패스워드 매니저에 비우호적임
          + 패스워드 복사/붙여넣기 불가임
          + 클라이언트 사이드 해시 사용
          + 15자 초과 비허용 및 화이트리스트된 문자만 가능 등 이상한 요구사항 존재(HDFC가 특히 심함)
          + 늘어나는 스팸 메시지 전부 2000년대 초반 UX 관성에서 벗어나지 못하는 느낌임
          + 15자 초과 금지라니! 내 은행은 정확히 6자리 숫자여야 함, 문자도 아니고 반드시 숫자임, 패스워드 매니저도 못 쓰고 복사/붙여넣기도 제한됨, 반드시 마우스로 숫자칸을 찍어야 함, “보안”에 집착한 나머지 2차 인증도 예전엔 물리 토큰에서 앱으로, 마지막엔 결국 SMS로 바뀜, 여기는 동네 조그만 은행이 아니라 프랑스 최대 규모 은행임
          + 몇 주 전에는 인도 공기업 은행 앱이 사용자가 Firefox를 깔았다는 이유만으로 앱 자체를 막는다는 캡처가 Reddit에 올라와 논란이 됐음, 은행과 정부 사이트들이 사용자에게 극도로 불친절한데, 예전엔 이런 접근이 기술에 익숙하지 않은 사용자를 지키려는 취지라고 생각했지만, 이제는 오히려 제대로 안전하고 편리한 프레임워크 도입을 게을리하는 자기변명이라는 생각임
          + 특정 인도 공기업 은행 앱은 카메라, 전체 파일시스템 등 필수적인 권한을 허용하지 않으면 실행조차 안됨, 하지만 내 은행에서는 OP와 같은 스팸은 받아본 적 없었음, 다만 대중적 인식으로는 하위 직원들이 계좌 정보를 사기꾼들에게 상습적으로 유출한다는 소문이 있음
          + 내 은행은 180일마다 패스워드 변경을 강제하고, 비밀번호는 6~11자만 허용하며 사용 가능한 문자도 정해져 있음, 그래서 로그인하려 하면 또 패스워드를 바꾸라고 하고, Firefox 자동 생성 패스워드는 은행 규정에 맞지 않아, 결국 터미널에서 조건을 맞춰 직접 임의 비밀번호를 만드는 번거로움이 생김
          + 클라이언트 사이드에서 패스워드 해시를 쓰는 게 왜 문제인지 잘 모르겠음
     * 주택 매매할 때 이런 문제가 더 심함, 다양한 하위 조직이 각자 다른 도메인을 쓰는 등 복잡함, 나도 의료기기 리콜 문제로 수상한 도메인을 신뢰해야 했던 고생을 했었음, 이런 건 간단히 홈페이지에 신뢰할 수 있는 파트너 도메인 리스트만 올려도 해결 가능함, 내 개인 보안 프로토콜은 .gov 사이트에서 금융기관 연락처를 검색해서 그 도메인에 들어가 고객센터 번호를 확인해 실제 신뢰 가능한 도메인이 무엇인지 전화해서 알아보는 방식임, 고객센터 직원들은 내가 이상하다고 생각하곤 했음, 어느 날 한 상담원이 “LinkedIn에서 담당자가 <Bank Name>에 다닌다고 뜨면 그 사람이 진짜임을 알 수 있습니다”라고 말한 적까지 있음
          + “LinkedIn에서 <Bank Name>이 자기 직장이라 나오면 신뢰해도 된다”길래, “2분만 주면 내 프로필에도 그렇게 등록할 수 있는데, 그럼 내 개인정보도 줄 거냐”고 반문했던 경험 있음
          + 주택 구매 때 전혀 복잡하지 않았던 경험도 있음, 난 주택담보 대출을 중개인을 통해 진행했고 일대일로 한 사람만 상대했음
     * 의사결정권이 있는 순진한 사람들이 이런 위험을 제대로 인식하지 못하다가 자신이나 주변인이 사기나 법적 문제를 겪어야 뒤늦게 깨달음, 미국도 2012년 전에는 이런 사람들이 기업을 많이 운영했지만, 화이트햇/블랙햇 해킹이 빨리 확산되면서 이런 문제들이 비교적 빠르게 해결됨
          + 예전에 정보보안 문화가 탄탄한 금융회사에서 일했음, 이 회사가 인수되고 나서는 본사 임직원 명의로 계속 외부 업체에서 각종 요청 메일이 옴, 하지만 기존 보안 정책대로라면 이런 메일을 처리하는 게 금지되어 있어, Slack에서 다들 이건 진짜 메일이긴 하지만, 정책상 무조건 피싱으로 신고하자고 의기투합했음, 결과적으로 악의 없는 불응이지만, 사실 이게 모범 사례임, 나중엔 본사 임원이 “이런 메일이 갈 거니 이렇게 반응해 달라”는 사전 메일까지 보내기 시작, 그러자 동료들과 “그럼 그 사전 안내 메일이 진짜임을 어떻게 아냐”라는 논의가 또 반복, 결국 다들 피곤해져서 본사 식으로 느슨한 보안 관행을 받아들이게 됨
          + 이런 유형의 조직에서는 올바른 결정을 내릴 수 있는 능력 있는 사람들이 실제로 결정권을 가지는 위치로 올라가기 힘든 사회적 구조가 있다고 생각함, 좋은 정책을 만들려면 여기저기 “안 된다”고 말해야 하는데, 그 과정에서 인사권자들의 비위를 맞추기가 가장 어려움
          + 문서상으로는 CISO, EVP, SVP, 보안 디렉터 등 고위직이 다 있는데도 왜 이런 엉뚱한 결정을 내리는지 정말 이해가 안 됨, 이럴 때는 무능과 악의를 구별하기가 어려움, “순진하다”고 돌려 말하면 오히려 고객을 무시한 행동을 변명해 주는 것 같음, 보안에 신경쓰는 것도 돈이 들고, 안쓰는 것도 손실이 크지만, 적어도 아직까지는 사용자를 잃는 손해가 안전하게 제대로 만드는 데 드는 돈보다 작으니 이런 방식이 유지되는 것 같음, 결국 모두에게 슬픈 현실임
     * 은행에서 자주 내게 무작위 마케팅 전화를 걸어, 자기네 제안을 설명해주기 전에 내 생년월일과 어머니 이름을 물어봤음, 내가 역으로 “은행이 진짜인지 먼저 증명해라”고 따지면 늘 당황해 했음
          + 낯선 전화에서 내 개인정보를 확인해 달라고 할 때 난 항상 “당신이 누군지 모르니 개인정보를 알려줄 수 없다”고 답함, 이러면 절반은 그냥 끊고, 나머지는 영업 멘트를 바로 시작함
          + 의료계에서도 똑같은 일을 자주 경험함, 전문의 사무실에서 전화하자마자 내 생년월일부터 묻고, 내가 거절하면 상대가 매우 놀라움, 나도 마찬가지로, 그쪽이 전화 건 쪽이라면 자기 신분을 먼저 증명해야 한다고 생각함
          + 내 은행은 이런 점을 결국 이해해서, 요즘은 앱에서 해당 상담원이 영업 중인 게 맞고 정확히 어느 직원인지 앱을 통해 확인 가능한 시스템을 도입함
     * “그래서 서브도메인 등록이 답이다”라는 아이디어가 나온 배경은, IT 부서 누군가는 서브도메인 따위로 권한을 내주는 게 위험하단 걸 알아서 거절하기 때문임, 결국 기업 내 다른 부서(마케팅 등)가 자체 도메인을 개별적으로 등록해서 이런 식으로 우회함, 구글 같은 기업은 어떻게 해결하는지 궁금함, google.com의 서브도메인 해킹은 가장 탐나는 타깃인데도, 실제로 구글도 꽤 자주 서브도메인을 사용함, 관련 사례로 이 gist 링크 참고할 수 있음
          + 사실상 IT 부서까지 안 가는 경우가 많음, 마케팅은 조직 구조상 IT와 분리되어 있고, IT에 일 걸기 싫어함, IT는 비효율적이고 느릿한 티켓 시스템을 쓰고 있으며, 쓸데없는 견해를 드러낼 때도 많아서 마케팅 쪽에서 귀찮게 여김, 그래서 마케팅 프로모션은 SaaS 서비스나 외주 업체에 위임하는데, 이들도 기업 IT와 별 인연이 없음, 마케팅 담당자는 서브도메인이 뭔지도 모르고 그저 구글 검색 혹은 링크 클릭으로 일함, URL 텍스트 따위는 아무도 안 보기 때문에 왜 서브도메인이 중요한지 인식 자체가 없음, 실제 사용자 인터넷 이용 패턴을 보면 전통적인 피싱 방지 교육이 무의미함, “도메인 꼼꼼히 읽기”보다는 “구글 검색 후 상위 결과 클릭”이 더 현실적인 피싱 방지법임
          + 구글도 이런 부분이 답답하기로 마찬가지임, 안내 메일이 피싱으로 오해받을 만한 형식이고, google.com 외에도 goo.gl, foobar.google 등 다양한 이상한 도메인을 쓰며 안내함, 예전처럼 곧이곧대로 신뢰하던 시절은 이미 끝났음
     * 나는 4장이 핵심이라고 생각함, 은행이 고객에게 피싱처럼 보이는 이메일을 보내는 건 중대한 과실로 법적 책임을 져야 한다고 봄
          + 피해자나 범죄가 명확히 존재하지 않는데 어떻게 법적 책임을 물을 수 있는지 궁금함, 특히 “중대한 과실”이라 불릴 정도면, 지금 실무에서 나타나는 부주의는 사실 경미한 편임
     * 이건 Conway의 법칙이 현실에서 그대로 보이는 사례임, 마케팅 부서가 자체 IT를 두고 있고, 코어 웹사이트를 관리하는 IT와 분리되어 있음, 그래서 같은 웹도메인에서 함께 개발은 불가능하고, 별도 사이트, 별도 경험으로 출시하게 됨
          + 독일 “Sparkassen” 사례는 더 골치 아픔, 이들은 소규모부터 중형까지 크레딧 유니언 형태이고, 상위 조직이 IT 같은 공통 서비스만 부분적으로 제공함, 개별 은행마다 스스로 관리할 범위를 선택 가능함, 큰 지점은 잘 굴러가지만, 소형 지점은 인력도 부족하고 IT 보안 실무도 거의 못함, 그럼에도 이런 은행들은 “동네 은행”이라는 친근 마케팅으로 신뢰를 쌓음, 실제론 수수료 비싸고 투자 상품 성과도 엉망임
     * 주변 친구가 일하는 회사에서는 CISO가 전사 임직원 대상으로 보안 관련 뉴스레터를 발송했는데, 이 메일이 사내 도메인이 아니라 외부 도메인에서 발송되고, 링크도 자체 사이트가 아닌 외부 호스팅 플랫폼을 사용해서 늘 피싱 메일 같았음, 특히 경품 이벤트가 있을 땐 더더욱 가짜로 오해받기 쉬웠음(회사 평판상 그런 후한 경품은 말이 안 됨)
          + 회사에서 매주 받는 뉴스레터도 항상 외부 발송자에서 오고, 링크는 외부 호스팅 사이트로 향함, 클릭 추적을 위해 고유 ID도 붙음, 그 링크는 Outlook에 의해 조합되어서 훨씬 알아보기 힘들어짐, 내가 재직 중인 회사 공식 웹사이트에서 이 발송자나 호스팅 도메인이 정식 사용 중인지 찾아봐도 어떤 안내도 없었음, 그래서 해당 링크는 클릭하지 않고, 차라리 피싱으로 신고해야 하나 고민했음
"
"https://news.hada.io/topic?id=22024","NIST 이온 시계, 세계에서 가장 정확한 시계 신기록 경신","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   NIST 이온 시계, 세계에서 가장 정확한 시계 신기록 경신

     * NIST 연구진이 알루미늄 이온 시계의 정밀도를 크게 높여 세계 최고 정확도 신기록을 세움
     * 기존 기록보다 41% 더 높은 정확도와 다른 이온 시계 대비 2.6배 향상된 안정성을 달성함
     * 알루미늄-마그네슘 이온 쌍의 '퀀텀 로직 스펙트로스코피' 등 혁신적 기술과 진공 시스템 최적화, 레이저 업그레이드로 주요 성능 개선을 이룸
     * 수십 년간의 연구를 통해 1초를 10^-19 단위까지 측정 가능해져, 차세대 시간 단위 정의 및 양자 물리 발전에 기여할 전망임
     * 측정 시간 단축으로 더 넓은 지구 과학, 표준 모델 너머의 새로운 물리 연구에도 활용이 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NIST 이온 시계의 성능 개선 및 기록 경신

     * 미국 National Institute of Standards and Technology(NIST) 연구진이 알루미늄 이온 기반 원자 시계의 성능을 개선하여, 세계에서 가장 높은 정확도를 실현함
     * 이 시계는 19자리 소수점까지의 시간 측정 정확도를 달성함
     * 최근 20년간 이루어진 지속적인 성능 업그레이드의 결과, 기존 세계 최고 기록 대비 41% 높은 정확도와 함께 2.6배 더 높은 안정성을 보임
     * 모든 구성 요소(레이저, 이온 트랩, 진공 챔버 등)를 세밀하게 개선한 결과임
     * 결과는 Physical Review Letters에 게재됨

알루미늄 이온 시계의 원리와 혁신

     * 알루미늄 이온은 매우 일정하고 고주파인 '틱' 특성을 보여, 시간 측정에 탁월한 적합성을 가짐
     * 기존에 1초의 정의에 사용된 세슘보다 더 안정적인 주파수를 제공함
     * 주변 온도나 자기장 등 환경 변화에 민감하지 않아 더 우수함
     * 하지만, 알루미늄은 레이저로 탐지 및 냉각이 어려운 특성 때문에, 이를 보완하기 위해 마그네슘 이온을 함께 사용하는 '버디 시스템'을 적용함
     * 마그네슘은 레이저로 잘 제어 및 냉각되며, 퀀텀 로직 스펙트로스코피 기술을 통해 알루미늄 이온의 상태를 간접적으로 관측할 수 있음

시스템 성능 개선의 주요 요소

     * 이온이 저장되는 트랩에서의 불필요한 미세 운동(Excess micromotion)이 정확도 저하의 원인이었음
     * 트랩 구조 개선: 더 두꺼운 다이아몬드 웨이퍼 사용 및 전극의 불균형을 바로잡기 위한 금 도금 최적화로 이온 움직임을 최소화함
     * 진공 챔버 역시 기존 스틸 대신 티타늄 소재로 재설계, 내부 수소 농도를 150배 이상 낮춰 이온 충돌 현상 및 실험 중단을 크게 줄임
     * 이러한 개선으로 재로딩 주기는 30분에서 수일로 대폭 연장됨

레이저 안정성과 측정 시간 단축

     * 고성능의 레이저 안정성 확보가 정확도 향상의 핵심임
     * NIST의 JILA 연구실(Jun Ye 그룹)에서 제작된 세계 최고 수준의 안정적인 레이저를, 광섬유로 3.6km 떨어진 NIST 시계 연구실로 전송함
     * 광주파수 콤(frequency comb) 을 이용해 두 레이저의 특성을 비교, 궁극적으로 시계의 레이저가 Ye 연구실의 레이저 안정성을 얻게 됨
     * 덕분에 이온 측정 시간(틱 측정)이 150ms에서 1초로 연장되어, 19번째 소수점까지 측정 시간을 3주에서 하루 반으로 대폭 단축할 수 있게 됨

NIST 이온 시계의 미래 기여와 활용

     * 이 새로운 정확도 기록은, 향후 세계 표준 초(second) 정의의 재정립, 지구과학, 정밀 물리학 등 다양한 분야로의 응용 확장 기반을 제공함
     * 시계 업그레이드는 양자 논리 기반 실험환경(testbed)로서의 역량도 크게 향상시킴
     * 지구 측량, 자연 상수의 변화 여부 등 표준 모델을 넘어선 물리 현상 연구에 이 시계가 핵심 도구로 활용 가능함
     * 시간이 더 짧게 걸리므로 새로운 과학적 측정과 실험의 기회가 열림
     * 앞으로 더 많은 이온을 투입하거나 이온 사이 얽힘(entanglement) 을 적용해 측정 역량을 획기적으로 높일 수 있음

참고 논문

     * Mason C. Marshall 외, ""High-stability single-ion clock with 5.5×10−19 systematic uncertainty"", Physical Review Letters, 2025년 7월 14일 온라인 게재, DOI: 10.1103/hb3c-dk28

        Hacker News 의견

     * 두 개의 이 시계를 나란히 놓고 고도(수직 위치)가 몇 센티미터 차이만 나도 중력/시간 지연 차이로 측정이 가능함. 직접 이런 수준으로는 아니어도, 세슘 빔 원자시계를 수천 달러대에 구입할 수 있고 직접 손으로 만들어볼 수도 있는 시대에 살고 있음이 놀라움
          + 세슘 시계는 약 1마일(1.6km) 수준의 수직 이동 분해능과 비교할 수 있음. 세슘 시계의 재미있는 점은 세 개 정도를 미니밴에 실어서 캠핑 갈 때 쓸 수 있다는 점임
            http://leapsecond.com/great2005/
          + 이런 수준의 정밀도가 놀랍다고 했는데, 그렇다면 '합리적으로 잘 갖춰진 연구실'에서 자체적으로 광학 시계를 만든다면 얼마나 어렵고 비쌀까 궁금함. 시중에 몇 개의 랙 크기만 한 광학시계가 상당히 비싼 값에 판매되고 있는데, 재료 자체가 아직도 비싼 건지, 아니면 오직 전문성 때문인지 궁금함
          + 초정밀 시계를 비교하는 이 방식이 아주 멋짐. 앞으로 아인슈타인식 고도계를 어디서나 볼 수 있기를 기대함
          + “수직 위치 몇 센티미터의 변화 측정”이 실제로 얼마만큼의 시간 동안 가능한지 궁금함. 즉각적으로 측정할 수 있는 것인지는 확실하지 않음
          + 앞으로 어느 정도까지 정밀도가 현실적으로 향상될 수 있을지 궁금함. 정말 언젠가 중력을 이용해 우주 규모가 아닌 일반 일상 속, 예컨대 누군가 옆을 지나갈 때 발생하는 중력파나 간섭 패턴까지 볼 수 있는 시대가 올지 상상해보게 됨
     * SKO BUFFS. NOAA에서 잠깐 일한 적이 있는데, 같은 캠퍼스에 있는 NIST에서 산책하며 근무했던 게 정말 좋았음. 엄청 멋진 건물이었음. 그런데 전체 캠퍼스가 폐쇄될 위기임
          + 약간 관련된 내용인데, 로드아일랜드 뉴포트 해군기지에 있는 NOAA의 새 해양운영시설 건설은 계속되고 있음을 언급하고 싶음. 한편으론 일부 폐쇄 논의와 동시에 새 건설이 진행되는 의미 있는 패턴이나 논리가 존재하는지 궁금함
     * 최근에 있었던 원자 시계 관련 논의들을 정리함.
          + 새로운 원자 분수 시계가 '세상을 시간에 맞추는' 그룹에 합류(NIST)
          + 핵시계의 비약적 발전이 궁극의 초정밀 시간측정 시대를 연다(NIST)
            첫 번째 게시글 댓글에서 두 번째 요약도 남겼는데, 두 번째는 전자가 아니라 원자핵 기반의 새로운 타입의 원자 '핵' 시계를 개발 중인 내용을 담고 있음. 정확도에 대한 언급은 없어서, 이번 '이온' 시계와 정확도 비교가 궁금해짐
          + Al+ 시계에서 가장 큰 불확실성은 이온 트랩 안에서 이온이 약간 움직이면서 생기는 상대론적 시간 지연 현상임. 229Th(토륨) 시계 역시 같은 영향을 받겠지만, 원자 질량이 더 크면 억제에 도움이 될 수도 있을 것으로 생각함
     * 비전문가의 입장에서, 시계의 정확도를 측정하려면 그보다 더 정확한 시계가 필요하지 않냐는 의문이 있음. 세상에서 가장 정확한 시계의 정확성은 어떻게 측정하는지 궁금함
          + 예를 들면 여러 대의 시계를 만들어 서로 비교하는 방법이 있음
     * 시계의 정확도를 어떻게 측정하는지 궁금함. 모든 시계에 미세한 오류가 있다면 모두 틀린 것이 아닌지 의문이 듦
          + 시계의 정확도는 정의에 따름, 이후 정밀도를 측정함. 두 개의 시계를 만들고 서로 얼마나 어긋나는지 측정하면 정밀도를 알 수 있음.
            두 시계가 서로 상이한 위치에 있으면 측정 가능한 시간지연 등 재미있는 실험이 가능함. 예를 들어
               o 서로 다른 원소의 두 시계를 이용해 '우주의 상수'라 불리는 값이 변하는지 측정 가능
               o 시계의 방향(예: 옆으로 눕힘)에 따라 차이가 나는지 관찰해 우주에 '특정 방향'이 있는지 연구
               o 어떤 이론에 따르면, 암흑물질이 시계 주파수에 변화를 일으킬 수 있기에, 먼 거리에 시계를 두고 공간적 암흑물질 밀도의 변조를 찾는 실험
               o 시계의 안정화를 위해 조정된 모든 요인(자기장 등)의 변화도 관찰 가능, 그래서 시계가 고감도 자기장 측정기도 됨
          + 정밀 시계 이야기할 때 꼭 나오는 재미있는 질문임.
            동일한 시계를 두 개 이상 제작해 동시에 같은 시간에 맞춰 사용함. 완벽한 시계라면 시간이 지나도 차이가 없겠지만, 실제로는 점차 어긋나게 됨(체계적 편이와 무작위 편이 모두 존재).
            이 차이를 보면 시계의 오차가 마치 '무작위 보행'처럼 확산함. 시계 여러 개로 실험하면, 오차 분산이 어떤 시계가 더 뛰어난지 보여줌.
            절대적으로 완벽한 표준 없이도 두 대를 비교해 무작위성을 측정할 수 있음
          + 1967년부터 1초의 물리적 정의가 도입되어 있음
            https://en.wikipedia.org/wiki/Second#Atomic_definition
          + 사실상 시계의 '정확도'가 아니라 '노이즈 크기'를 측정하는 것임. 시계 원천 자체는 물리적으로 변하지 않지만 노이즈가 섞임.
            예를 들어 아주 미세한 자기장, 온도 변화 등도 시계 속도를 바꿀 수 있어서 최대한 차단/제어가 필요함. 남아 있는 영향은 계산으로 보정하고, 이 값이 곧 정확도임.
            직접 측정하고 싶다면, 똑같은 시계를 두 대 동기화해 놓고 시간이 지난 뒤 서로 비교하는 방법도 있음(상대성이론에 의한 영향도 고려 필요함)
          + 시간은 변하지 않는 물리현상을 기반으로 정의함.
            예를 들어 모든 전자는 완벽히 동일하기 때문에, 이런 특성을 이용해 정확한 시간 기준을 만들 수 있음
     * '시계'인지, 혹은 위치 인코더에서처럼 어디까지나 '시계 신호'인지 헷갈림. 즉, 특정 범위 내에서만 '절대값' 역할을 하는 것 아닌지 궁금함
          + 이처럼 트랩된 단일 이온, 혹은 중성 원자 격자 기반 광학 원자시계는 연속적인 시계 신호 자체를 생성하지 않음.
            대신 레이저(주파수빔, frequency comb)가 필요함. 수백 THz의 광학신호를 MHz~GHz 단위의 전자 신호로 나눠줌.
            실제 시간표현용 신호시계로 완전 연속성을 확보하려면 여러 대의 광학시계가 필요함(현재는 이온, 중성 원자를 다 잃어버리면서 자주 재설정 필요).
            연속 신호는 레이저가 담당함. 이 레이저는 에르븀, 이터븀 유리 기반의 적외선으로 동작하고, 이온의 공명주파수와 맞춰져 있음.
            짧은 구간에서는 노이즈를 거르기 힘드니, 실리콘 공진기의 품질로 주파수 안정성이 결정됨(매우 낮은 온도에서 냉각, 적외선 투과 등 품질 조건).
            컴퓨터의 시계 신호처럼, 장기적으론 NTP 등 외부와 동기화 수준이고, 단기적으로는 내부 쿼츠 오실레이터 수준.
            이번 광학 이온시계는 참고 주파수의 불확실성이 역대 최저 수준임. 하지만 트랩된 한 개 이온이 쓰이기에 중성 원자 격자 기반(수천 개 원자 활용)보다 단기 노이즈가 큼.
            그래서 실제 출력 신호를 매우 오랜 시간(최소 수일) 평균해야 명시된 정확도에 도달함.
            단기간(1초) 정확도는 현재 최고 성능의 세슘·수소 마이크로파 시계 대비 약 천 배 수준이나, 평균만 해도 기존 마이크로파 시계 성능에 도달함
          + 빅뱅 같은 우주적 기점 이외에는 절대적 시간 기준이 정말 존재하는 것인지 궁금함
          + 시계 신호는 누적해 모두 셀 수 있고, 장기적으로 매우 정확함. 회전 인코더처럼 수조 번의 신호 누적도 개념상 가능함(일반적으로 인코더는 이런 카운팅을 거의 안 할 뿐임)
     * 다이아몬드와 금으로 만든 '최고의 시계'라는 설명이 마음에 듦. 마치 마인크래프트 느낌임
     * 기사에 디바이스 사진 등 흥미로운 이미지가 많음. 알루미늄이 세슘보다 분명히 우수하지만 실제로 다루기 힘들고, 이제 표준이 되지 못하게 했던 장애물들이 이번에 해결된 것으로 보임
     * 프리프린트
       https://arxiv.org/abs/2504.13071(""High-Stability Single-Ion Clock with $5.5\times10^{-19}$ Systematic Uncertainty"")
     * NIST NTP 서버에 인증된 접근을 원한다면, 반드시 미국 우편 또는 FAX로 편지를 보내야 함(이메일은 허용되지 않음).
       NIST에서는 키 정보를 역시 우편으로만 회신함(이메일 사용 절대 불가).
       평소 우편·FAX를 받는 부서가 현재 접근이 제한되어 있어서 요청 처리에 상당한 지연이 있을 수 있음
       https://nist.gov/pml/time-and-frequency-division/…
       (fedramp 구현할 때 알게 된 사실임)
          + NIST가 NTS(Network Time Security) 도입을 고려할지 궁금함
            https://github.com/jauderho/nts-servers/tree/main
          + 해외 거주자도 FAX 사용이 허용되는지 궁금함. 미국 외 지역 사용자에게는 이 과정이 다소 번거롭게 느껴짐
"
"https://news.hada.io/topic?id=22023","Cloudflare 1.1.1.1 2025년 7월 14일 장애 사건","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Cloudflare 1.1.1.1 2025년 7월 14일 장애 사건

     * Cloudflare가 2025년 7월 14일에 서비스 토폴로지 변경 중 1.1.1.1 공용 DNS Resolver에 62분간의 전면 장애 발생
     * 글로벌 사용자 대다수가 직접적인 영향을 받아 인터넷 사용 불가 현상 경험
     * 장애 원인은 내부 레거시 시스템의 잘못된 구성으로, 외부 공격이나 BGP 하이재킹과는 무관함
     * 장애는 잘못된 구성 변경의 누적과 네트워크 전역 재설정이 맞물리면서 촉발됨
     * 재발 방지 대책으로 점진적 배포 시스템 도입 및 레거시 구성 시스템의 폐기 계획 준비
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요

   2025년 7월 14일, Cloudflare가 서비스 토폴로지 변경 중 1.1.1.1 공용 DNS Resolver에 글로벌 네트워크 장애를 유발함. 이 장애로 인해 1.1.1.1과 Gateway DNS 서비스를 이용하던 사용자들이 62분간 인터넷 서비스 불가 또는 심각한 서비스 저하를 경험함. 이는 내부 레거시 시스템의 구성 오류에 기인하며, 외부 공격이나 BGP 하이재킹으로 인한 것은 아님.

장애의 범위 및 영향

     * 21:52 UTC ~ 22:54 UTC 동안 1.1.1.1 Resolver가 전 세계적으로 사실상 동작 불가 상태였음
     * 대다수 글로벌 고객이 도메인 이름 해석을 못하여 인터넷 사용 자체가 불가능했음
     * 장애 발생 상황은 Cloudflare Radar에서 확인 가능
     * 장애 사유는 Cloudflare가 보유 IP 주소를 인터넷에 광고하는 인프라를 관리하는 레거시 시스템의 잘못된 설정 때문임
     * 1.1.1.1 채널을 통해 Cloudflare에 도달하던 트래픽 전체에 치명적 영향 발생

장애 발생 원인 및 배경

     * Cloudflare는 DNS Resolver 등 글로벌 서비스를 위해 Anycast 라우팅을 사용함
     * 다양한 지역에서 서비스를 제공하지만, 일부 데이터로컬라이제이션을 요구하는 서비스는 특정 지역에 한정됨
     * 6월 6일, 차후 DLS(데이터 로컬라이제이션) 서비스 준비를 위한 구성 변경 중, 1.1.1.1 Resolver IP 대역이 의도치 않게 신규 DLS에 포함됨
          + 이 오류는 즉시 반영되지 않고 실제로는 영향을 미치지 않아 경보가 발생하지 않음
     * 7월 14일, 테스트 목적의 오프라인 위치를 DLS 토폴로지에 추가하는 변화가 적용됨
          + 이 변경으로 글로벌 네트워크 구성이 강제 갱신되면서 기존 오류가 노출됨
          + 1.1.1.1 Prefixes가 전세계 데이터센터에서 철회되어 서비스 단절

장애 타임라인(요약)

     * 2025-06-06 17:38: DLS 서비스용 구성 변경에 1.1.1.1 Prefixes 포함(영향 없음, 오류 잠복)
     * 2025-07-14 21:48: 구성 변경으로 네트워크 전체 구성 새로 고침, 1.1.1.1 Prefixes가 글로벌하게 철회 시작
     * 2025-07-14 21:52: 글로벌 DNS 트래픽 급감
     * 2025-07-14 22:01: 내부 경보, 장애 선언
     * 2025-07-14 22:20: 이전 구성으로 롤백, 서비스 복구 절차 개시
     * 2025-07-14 22:54: 트래픽 정상화 및 경보 해제, 장애 종료

장애 영향 IP 및 프로토콜

     * 영향 범위: 1.1.1.0/24, 1.0.0.0/24, 2606:4700:4700::/48 등 IPv4, IPv6 광범위 Prefixes
     * UDP, TCP, DoT(DNS over TLS) 사용 쿼리에서 급격한 트래픽 감소 관측
     * DoH(DNS over HTTPS)는 cloudflare-dns.com 도메인 기반이 많아 영향을 거의 받지 않음

기술적 장애 설명

  1.1.1.1 Resolver 서비스 장애

     * 6월 6일 DLS용 사전 구성 변경 과정에서 Prefixes 오류 삽입
     * 7월 14일, 테스트 목적으로 오프라인 위치가 추가되며, 네트워크 전역 설정이 갱신
     * 이 과정에서 1.1.1.1 Resolver Prefixes가 전세계적으로 단일 오프라인 위치로 한정되어 서비스 철회

  기술적 원인 분석

     * Cloudflare는 현재 레거시 시스템과 신규 전략 시스템을 병행 운영하고 주소 공간별 라우팅 광고를 동기화함
     * 레거시 시스템은 수작업 업데이트, 배포의 점진성 부재 등 오류 확률 높음
          + Peer review 및 타 엔지니어 검토는 있었으나, 카나리아 배포 등 점진적 반영 보장 구조 부재
     * 신규 방식은 하드코딩 대신 토폴로지 중심, 점진적 변화 반영 및 모니터링 체계 도입
     * 22:01, DNS Resolver 경보 발생
     * 내부 BGP 라우팅 테이블에서 Resolver 라우트가 전부 소실된 사실 확인
     * Prefixes 철회 후 1.1.1.0/24 subnet은 Tata Communications India(AS4755)에서 BGP 광고 시도
          + 이는 일시적 Prefix Hijack과 유사하나, 사고와는 직접적으로 무관

  복구 절차 및 후속 조치

     * 22:20 UTC, 이전 구성으로 롤백 및 Prefixes 재광고
          + 약 77% 트래픽 즉시 복구
          + 일부 에지 서버는 자동 재설정되어, 수동 변경 관리 시스템으로 재반영 필요
          + 네트워크 안전상 점진적 롤아웃을 하지만, 이번 사고에서는 검증 후 빠르게 적용
     * 22:54, 전체 위치 정상 복귀

    향후 개선 방안

     * 점진적 배포 체계(Stage Deployment) 도입: 레거시 배포 방식 폐지, 헬스 기반 자동 롤백 구조 도입
     * 레거시 시스템 폐기 가속화: 위험한 수기 구성과 배포 방식 제거, 문서화와 테스트 커버리지 강화

결론

   Cloudflare 1.1.1.1 DNS Resolver 장애는 내부 구성 오류로 인한 것이며, Cloudflare는 향후 안정성 개선과 재발 방지 대책 도입에 총력을 기울이고 있음. 고객에게 폐를 끼친 점을 사과하며, 미래에는 유사 사태를 최소화하기 위한 조치를 계속 강화 예정임.

        Hacker News 의견

     * 많은 사용자들에게 1.1.1.1 리졸버(DNS)가 작동하지 않을 때, 거의 모든 인터넷 서비스에 접속할 수 없게 됨을 의미함. 그런데 보통 모든 기기에는 두 개의 DNS 서버를 설정하지 않음? 두 번째도 다운된 것인지, 그렇지 않으면 왜 해당 서버로 넘어가지 않았는지 궁금함
          + Cloudflare는 1.1.1.1과 1.0.0.1 두 개 모두를 DNS 서버로 설정할 것을 권장함. 그러나 이번 장애를 일으켰던 설정 실수 때문에 1.1.1.0/24와 1.0.0.0/24 프리픽스 모두에 대한 Cloudflare의 BGP 광고가 중단됐음
          + Android에서는 설정-네트워크 및 인터넷-Private DNS에서 ""Private DNS provider hostname"" 하나만 입력할 수 있음(내가 아는 바로는). 그리고 왜 IP(1.1.1.1)를 받지 않고 반드시 주소(one.one.one.one)를 요구하는지 이해가 가지 않음. DNS 서버를 지정할 때 IP로 설정하는 게 훨씬 합리적인 생각임
          + 두 개를 나열하는 게 없는 것보단 낫지만 완벽하지 않음. 한 쪽이 다운되면, 어떤 게 정상 동작하는지 추적하지 않기 때문에 대체로 긴 대기시간과 간헐적 이슈를 겪게 됨. 둘 이상의 업스트림을 가진 로컬 캐싱 DNS 프록시 같은 고급 설정을 쓰지 않는 한 마찬가지임
          + DNS에 대해 이야기할 수 있다고 생각한다면 스스로 서비스를 운영해야 한다고 조언함. 루트 도메인 "".""는 수십 년 동안 잘 동작해왔는데, 1.1.1.1에서 문제가 생기는 주된 이유는 DNS 자체가 아니라 애니캐스트에 있음. Cloudflare(및 Google 등)는 ""vanity"" IP 주소를 고집함—이런 IP 사용을 가능하게 하려면 애니캐스트를 써야 하고, 진짜 문제는 DNS가 아니라 애니캐스트임. 한 명 이상의 공급자를 선택해 설정하길 추천함
          + Cloudflare가 권장하는 구성은 백업 서버인 1.0.0.1을 보조 DNS로 함께 두는 것이지만, 이번 사고 때는 그 서버도 영향을 받았음
     * 대략 20분 정도의 장애에서 1.1.1.1의 트래픽이 약 20% 줄어든 것이 흥미로움. Cloudflare가 이런 단순하고 오래된 문제로 계속하게 된다는 게 신기함(이번이 첫 번째도 아니고, 마지막도 아닐 것 같음). Google의 8.8.8.8 및 8.8.4.4는 거의 10년 가까이 전세계적으로 (1) 1초의 다운타임도 없었음. (1: 일부 지역적 문제는 있었지만, 그건 인터넷 탓이고 Google의 다양한 서비스가 심각한 장애를 겪을 때도 DNS 자체는 계속 정상 작동했음.)
          + 단순히 가용성만이 아니라, DNS에는 속도와 프라이버시도 중요함. 유럽 사용자라면, 유럽 대안 DNS 목록에서 미국 기업(CLOUD act 적용) 대신 사용할 수도 있음
          + Cloudflare처럼 규모와 복잡성이 엄청난 네트워크 환경에서 벌어지는 엔지니어링 문제를 어떻게 쉽게 해결 못하겠냐는 의견에, 현실적으로 업계 0.001%만 경험하는 문제임을 설명함
          + Cloudflare는 사고 대응 문화는 합리적이나, 사전 예방적 조치를 적극 장려하는 인센티브가 부족한 점이 있음
          + 언급한 20% 수치는, 일부 클라이언트/리졸버가 여러 번 응답 실패시 임시로 DNS 서버를 중단 처리해서 사용자 입장에서 매 요청마다 타임아웃을 500번씩 기다릴 필요 없게 함을 뜻함. 장기적으로는 트래픽 그래프상 볼륨이 정상으로 돌아옴
          + Google DNS를 쓰고 싶지 않은 사람이 많음에 동의함
     * 영향 감지에 5분 이상(메인 프로토콜 트래픽이 10%로 떨어지고 유지됨에도)이 걸렸다는 사실이 놀라움. 그렇게 대규모 시스템을 운영해 본 적은 없지만, 이 정도면 즉각 경고가 발생할 거라 기대했음. 전문가들도 이게 합리적인지 궁금함
          + 감지 속도와 오탐률 사이에는 지속된 긴장감이 있음. Nagios, Icinga 같은 모니터링 시스템은 보통 3회 연속 실패해야 이벤트를 발생시키는데, 일시적 오류가 흔하기 때문임. 알람이 너무 잦으면 담당자들이 무감각해지고 ""잠깐 기다려 보자""라는 반응이 늘어남. Cloudflare처럼 글로벌 서비스를 직접 운용해보진 않았지만, 8분 만에 신뢰성 있는 감지가 된 건 놀랍지 않음
          + 예전 NOC(네트워크 운영센터)처럼, 이런 그래프가 벽에 걸려 있다면 누군가 한 번 쳐다보고 ""이상하다""며 바로 달려들었을 것임
          + 영향이 시작됐을 때 서비스가 완전히 다운 상태가 아니었을 것이라고 생각함(특히 글로벌 롤아웃 시작점이라면 더욱 그렇고), 영향이 측정 가능해지는 데 시간이 걸렸을 것임
          + 1분 내 알람을 울리게 하면 오히려 알람 인프라 성능 테스트만 될 뿐임. 실제로 1분마다 데이터 수집·계산이 안정적으로 가능한가, 그게 문제임
          + 메트릭 집계 서비스가 크래시날 때, 시스템이 다시 배포될 때까지 지표가 지연돼 100% 드롭처럼 보일 수 있음. 1분 만에 알림을 보내면 불필요하게 밤 2시에 많은 사람이 깨게 되고, 반복되면 결국 알람이 점점 느슨해짐—그래서 5분 정도에 맞춰지는 현상이 일어남
     * 좋은 요약글임. DoH(HTTPS 기반 DNS)는 대부분 cloudflare-dns.com 도메인을 통해 접근하지만(수동 설정 또는 브라우저), IP 주소가 아니기 때문에 상대적으로 장애 영향이 적었다는 부분이 흥미로움. 나는 어제 영향받았는데, 라우터에 DoH를 활성화했음에도 아무 것도 조회되지 않아서 8.8.8.8로 바꾸니 문제 해결됨
          + DoH는 어떻게 동작하는지 궁금함. cloudflare-dns.com의 IP를 알아야 하는데, 라우터가 1.1.1.1을 썼을 수도 있음
          + 오늘 새 도메인을 설정하는데, 약 20분 동안 한 노트북의 Firefox에서만 접근이 됐음. Google DNS 툴은 도메인이 활성 상태라고 나왔고, AWS 서버에서는 SSH도 됐지만 내 로컬 네트워크에서는 DNS 조회가 안 됐음. 캐시 플러시도 했고 별짓 다 했지만, 그 Firefox 브라우저만 Cloudflare DoH를 쓰도록 따로 설정되어 있었음
          + 나는 동의하지 않음. 실제 근본 원인은 현학적인 용어로 애매하게 감춰져 있어서, 경험 많은 관리자들도 혼란스럽게 만듦. ""legacy""란 용어는 명확하지 않고 오히려 추상적이고 불투명하게 느껴짐. ""Legacy 컴포넌트는 점진적 배포 방식을 활용하지 않으며, Cloudflare는 점진적 배포와 롤백이 가능한 현대적 방식을 도입하겠다""라는 문장을 보면 무슨 뜻인지는 알겠지만, 굳이 이렇게 알아듣기 어렵게 쓸 필요는 없음
          + 내 Unifi 라우터는 자동 DoH로, Cloudflare와 Google을 동시에 쓰는 것으로 보임. 영향은 못 느꼈으니, Cloudflare DoH가 계속 동작했거나 Google로 바로 넘어간 듯함
          + 전체적으로 잘된 요약이지만, 글 맨 앞부분에 나온 타임라인 내용은 사실과 다름
     * dnsmasq를 사용하면 여러 DNS 서버를 동시에 설정하고, 가장 빨리 응답하는 서버를 사용하는 방식이 가능함. 한 서비스가 다운돼도 문제를 거의 못 느낌
          + strict-order 설정 없이 all-servers를 쓸 때 dnsmasq는 자동으로 모든 서버에 재시도 요청을 보냄. 하지만 systemd-resolved를 쓴다면 순서대로만 재시도하기 때문에, 다양한 공급자 서버를 번갈아 넣는 게 중요함. 예시에서처럼 IPv4와 IPv6까지 함께 조합하면 failover가 더 빨라짐. Quad9의 해당 IP 주소는 기본 필터링이 활성화돼있고, 나머지 두 개는 아니라서 서로 해석 결과가 다를 수 있음. 개인적으로 DNSSEC(도메인 보안 확장) 유효성검사를 중시한다면, 검증하는 리졸버와 안 하는 리졸버를 혼합하면 안됨(예시 내 DNS들은 모두 DNSSEC 지원함)
          + 빅테크 업체들(Cloudflare, Google 등)에게 내 DNS 기록이 모두 노출되는 게 싫고, DoH도 원하지 않을 때 더 사적인 설정이 가능한지 질문함. dnsmasq에 신뢰할만한 소규모 DNS 목록을 쓰면 좋을 것 같지만, 여러 DNS 제공자에 매번 요청을 보내는 게 비매너인지는 고민됨. 안정적인 프라이버시 지향 DNS 목록을 어디서 찾을지 모르겠음
          + 여러 DNS 공급자는 정책이나 중요도가 달라서, 나는 둘을 대체물로 생각하지 않음. 오히려 하나만 정하고 ISP(통신사)의 DNS를 백업으로 쓰겠음
          + systemd-resolved를 사용하면 기본적으로 DoT와 DNSSEC를 지원하니 비슷한 동작을할 수 있음. 중앙 집중식 DNS를 완전히 피하려면 Tor 데몬을 돌려서 네트워크에 DNS 리졸버를 노출할 수 있음. 여러 리졸버도 가능함
          + ""all-servers"" 설정 없이도 dnsmasq는 주기적으로 각 서버에 요청을 레이스(경쟁)함(기본 20초마다 재시도). 갑작스러운 장애라 해도 몇 초 이상 영향받지 않을 것임
     * 약 1시간짜리 장애라 해도 한 달 기준 0.13%, 1년 기준 0.0114%임. Cloudflare가 이 서비스에 적용하는 SLO(서비스 수준 목표)가 궁금함. 링크를 찾긴 했지만 유료 서비스 전용임. 이번 장애로 7월 가용성이 ""< 99.9% but >= 99.0%"" 구간에 들어가며, 이 경우 이용료의 10%를 환불받게 됨
          + 브랜드 평판을 유지하려면 연간 99.9%나 그 이상일 것이라 생각함
     * 사고 이후에도 트래픽이 완전히 정상으로 돌아오지 않은 점이 흥미로움. 최근 OpenWrt의 ""luci-app-https-dns-proxy""를 써서 Cloudflare와 Google DNS에 동시에 요청하는데, DoH 영향이 거의 없었어서 장애를 못 느꼈음(DoH까지 망가졌다면 Google로 자동 전환됐을 것)
          + 사고 직후에도 즉시 트래픽이 복구되지 않은 이유를 본문 뒷부분에서 더 다룸. 일부 서버는 직접적인 개입이 필요했기 때문인 듯 함
          + 장애가 일어나면 인터넷이 안 돼서 아예 잠시 다른 행동을 하는 경우가 많음. 실제로 그 시간 동안 DNS 공급자를 바꾸는 사람은 거의 없을 것이라 추측함
          + 클라이언트들이 DNS 조회 결과를 일시적으로 캐시하기 때문에, 장애 이후 한동안은 과거 값을 쓰는 경우도 있음
     * 1.1.1.1과 1.0.0.1 모두가 같은 변경에 의해 영향을 받았다는 점이 놀라움. 이제 DNS 백업에는 완전히 다른 제공자를 써야 할 듯함(예: 8.8.8.8, 9.9.9.9)
          + 두 주소 모두 사실 같은 서비스에서 제공됨. 상호 독립적인 백업처럼 광고된 적이 없었음
          + 원래 DNS의 설계 목적은 가장 가까운 리졸버를 쓰는 것임. 여러 공급자, 백본, 지역을 다양하게(그리고 가능하면 Anycast 주소가 아닌 곳으로) 적절히 분산해 쓰는 게 좋음. 하지만 이 경우 DNS의 동작 특성 때문에 뜻밖의 문제도 만날 수 있음
          + 어차피 항상 그런 상황이었음
          + 난 이미 OpenDNS, Quad9, Cloudflare를 섞어 Pi-hole 두 대에 설정해서 사용 중임. 대부분의 내 기기들은 두 Pi-hole을 각각 DNS로 쓰고 있음
          + 사실 ""DNS 백업""이란 개념 자체가 의미 없음. 대부분의 클라이언트는 단순히 한 주소를 임의로 선택해 사용하고, 한 쪽이 다운돼도 나머지에 자동으로 넘어가는 게 아님. 동작 방식을 기대대로 사용할 수 없는 상황임
     * Cloudflare의 내부 토폴로지는 ""legacy""와 ""strategic"" 시스템이 동기화되는 형태로 발전되어감. 기술자와 비전문가 모두 이해할 수 있도록 현황을 명확하게 설명한 글임. 마이그레이션 과정을 오히려 흥미롭게 느껴지게 잘 써낸 것 같음. 사고로 인한 불편에 대해 사과하고, 앞으로 개선 및 재발 방지에 힘쓰겠다는 메시지가 인상적임. 이런 기업의 태도를 높게 평가함
          + ""legacy""는 기술자들이 주로 쓰는 용어고, ""strategic""은 마케팅이나 비기술 리더들이 주로 쓰는 용어라서 둘을 섞어 쓰면 오히려 양쪽 모두 혼란스럽고 짜증날 수 있음
     * 여러 엔지니어가 리브랜칭을 검토했다는데도 1.1.1.0/24가 리라우팅 목록에 추가된 실수를 아무도 못 봤다는 게 놀라움. 어떤 휴먼 에러, 혹은 악의로 이런 일이 벌어졌을지 궁금함. DLS(Domain List Service)에 1.1.1.1/32, 1.0.0.1/32에 대해 단일 위치로 지정하는 걸 막는 하드코딩 예외 처리가 필요해 보임
          + 아마도 ""Jerry가 했으니 문제 없겠지""라는 신뢰가 원인일 수 있음
          + 나는 대체로 ""사람 탓보다 도구 탓""하는 편임. 시스템 구조나 설정 파일 생성 방식에 따라, 이런 실수가 쉽게 넘어갈 수 있음(특히 diff가 자동생성이면 더욱 그렇고). 결국은 코드리뷰도 사람이 하니 이런 실패 유형은 절차상의 문제임. 현실적으로, 정말 큰 서비스 오프라인을 막으려면 여러 단계로 복수의 안전장치를 두는 방어 전략이 필요함
"
"https://news.hada.io/topic?id=22027","Blender 4.5 LTS","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Blender 4.5 LTS

     * Blender 4.5 LTS는 장기 지원 버전으로, 오랜 기간 안정성과 업데이트를 제공함
     * 이번 릴리스에서는 강화된 실시간 렌더링 기능과 UI 개선이 강조됨
     * 개발자 및 아티스트를 위한 다양한 워크플로우 최적화와 성능 향상 지원
     * 새로운 플러그인 시스템과 API 확장으로 생태계 확장성 제공
     * 오픈소스 3D 제작 툴로서 기업 환경에서도 실무 활용성 확대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Blender 4.5 LTS 소개

     * Blender 4.5 LTS는 장기적으로 지원되는 버전으로, 산업 환경과 전문 프로젝트에 적합한 지속적 보안 패치 및 기능 업데이트가 진행됨
     * 이번 버전의 주요 초점은 실시간 렌더링 기능 개선, 사용자 인터페이스 최적화, 워크플로우 혁신, 그리고 효율적 리소스 관리에 있음

주요 기능 및 개선 사항

     * 향상된 Cycles 및 Eevee 렌더 엔진 지원
     * 데이터 관리 및 씬 최적화를 위한 새로운 툴 도입
     * 레이어 기반 렌더링, 동적 자산 관리, 대규모 프로젝트 지원성 증대
     * 개선된 스크립팅 API 제공으로 커스텀 플러그인 및 자동화 도구 개발 용이함

개발자와 기업을 위한 가치

     * 지속적인 LTS 지원 정책으로, 기업 및 조직에서의 장기적 안정성 확보
     * 오픈소스 기반으로, 다양한 개발 커뮤니티와 생태계 참여가 용이함
     * 대규모 협업 및 파이프라인 구축에 적합한 유연한 확장성 제공

Blender 4.5 LTS의 활용성

     * 애니메이션, VFX, 3D 프린팅, 게임 개발 등 다양한 분야에서 실질적 활용성 확보
     * 간결한 UI, 최적화된 성능, 높은 호환성으로 입문자 및 전문가 모두에게 효율적 도구로 작용함

        Hacker News 의견

     * Blender에서 큰 가치를 얻는 경우, 한 달에 커피 한 잔 대신이라도 Blender Fund에 후원을 고려해줬으면 하는 바람임. 작은 금액도 Blender 팀에게 크게 도움이 됨
          + 오픈소스는 가능할 때 꼭 후원해줬으면 하는 생각임. Blender를 실제로 쓰지 않더라도, 오픈소스에 투자하면 유료 대안들도 더 경쟁하도록 만들어 더 좋은 제품이나 가격 혜택을 끌어낼 수 있음
          + 혹시 Blender Studio 구독도 좋은 대안임. Blender Studio에 가입하면, 트레이닝 자료와 에셋, 최근 Dog Walk 업데이트 같은 프로젝트 소스 및 프로덕션 로그도 볼 수 있고 동시에 Blender를 지원하는 셈임. 프로덕션 로그 참고
          + 나처럼 Blender를 실제로 사용하지 않더라도, 이렇게 성공적으로 창의적인 프로젝트를 끌어가는 모습만으로도 후원할 가치가 충분하다고 생각함
          + Blender같은 소프트웨어는 특히나 더욱 그렇다고 봄. 현존하는 오픈소스 소프트웨어(FOSS) 중에서 가장 잘 만들어진 작품 중 하나라고 생각함
          + 해마다 Blender만큼은 꾸준히 지원하는 나의 유일한 프로젝트임. 이들이 이루어낸 것은 정말 놀랍고 또 놀라움
     * Blender가 3D 컨텐츠 제작의 세계를 점점 장악해가는 느낌임. 애니메이션 업계에서는 아직도 Maya가 표준이고, geometry node는 Houdini만큼 강력하지 않지만 Blender의 발전 속도가 너무 좋음. 요즘엔 3Dsmax나 3DCoat를 DCC 입문용으로 배우는 취미생이나 학생이 있을지 의문임. (* 만약 생성형 AI가 이 세계를 완전히 대체하지 않는다는 가정 아래)
          + 생성형 AI가 이 세계를 완전히 없애지 않을 것이란 부분에 공감함. 내 경력 대부분을 콘텐츠 크리에이티브 툴(2D, 3D, 비디오)에 쏟아온 사람 입장에서는, AI 기술이 단기적으로 혼란스럽더라도 장기적으로 큰 긍정적 효과를 줄 것이라 확신함. 전문가들이 더 빠르고 생산적으로 일하도록 도와주고, 비전문가들도 스스로 창의적 비전을 실현할 수 있는 시대로 가는 중임. 지난 30년간 도구 발전은 '창작자가 자신의 아이디어를 더 빠르고 저렴하게, 또 더 좋은 품질로 실현할 수 있도록 돕는다'는 본질이었음. 이로 인해 나쁜 콘텐츠가 크게 늘 수도 있지만, 동시에 예전에는 불가능했던 멋진 작품의 가능성도 함께 커진다는 점이 여전히 사실임
          + Blender가 오픈소스이자 Python으로 스크립트 확장이 가능하기 때문에, 미래에는 만들어질 genAI/agent가 Blender와 직접 연동될 수 있는 날도 머지않아 보임. Blender Python Quickstart 참고
          + 내 딸이 10살 때 Blender 도넛 튜토리얼을 따라해서 만든 3D 애니메이션을 연말 아트쇼에 출품했었음. 이런 프로젝트는 커뮤니티와 리소스가 풍부해서 학생들이 배우기에 아주 쉬움
          + Blender는 3D 애니메이션의 Python 같은 존재임. 모든 면에서 항상 두 번째로 좋은 툴임. 근데 이건 비난이 아니라, 워크플로우 전체에서 이 수준으로 꾸준히 쓸만하다는 게 얼마나 어려운 일인지 알기에 오히려 대단하다고 생각함
          + 만약 genAI가 시네마/애니메이션 분야를 완전히 잠식한다 해도, 비디오게임 개발에는 여전히 3D 소프트웨어가 꼭 필요할 것임. 참고로 godot은 blender에 비해 모델링/애니메이션에서는 아직 한참 뒤처짐. AI가 크리에이터 영역을 계속 잠식해도, Blender가 이 점을 오히려 기회로 삼을 수 있기를 바람
     * 커스텀 메쉬 노멀 기능이 추가된 것이 너무 좋음. 예전엔 이 작업이 정말 고생스러웠음. 새로 나온 OSL로 커스텀 카메라 정의하는 기능이 아주 흥미로움. 이걸로 더 물리적으로 정확한 렌즈를 ‘에뮬레이션’할 수 있고, 표준 카메라보다 훨씬 재밌는 보케 효과를 만들 수 있음. 관련 링크
          + 예전엔 특정 오브젝트까지의 거리를 vertex attribute로 계산한 후, 머티리얼 단계에서 blending을 해가며 강제로 노멀을 조절함. 이번 기능은 그에 비해 훨씬 깔끔하고 좋음
     * Blender 4.5 LTS 릴리즈 노트 안내임
     * Blender는 아주 훌륭한 앱임. 복잡해서 활용법을 익히려면 LLM(Large Language Model)의 도움을 받거나, 아니면 몇 달~몇 년간 열심히 파고들어야 함. 유튜브에서 Blender가 엄청 쉽게 보이는 건, 그 분들은 Blender만 거의 다루는 사람들이라 가능함. 언젠가는 LLM 기능이 UI에 자연스럽게 통합되고, 앱 자체가 너무 복잡해져서 LLM이 아예 기본 인터페이스가 될 것 같은 상상도 해봄
          + 예전에 Quake 3 시절, 3D Studio Max 불법 복사본으로 Quake 혹은 Half-Life 캐릭터 모델을 만들려고 온라인 튜토리얼을 따라했던 추억이 있음. 백그라운드 이미지를 불러와 인체 모양을 대충 따라 그리고, 엑스트루드로 얼굴, 팔, 다리를 만들며 메쉬를 점점 정교하게 다듬었었음. 하루 만에 엄청 조잡한 사람 모델을 만들었고, 그 과정에서 3D 모델링이 내 적성은 아니라는 것도 알게 됨. 하지만 이걸 정말 좋아하는 사람은 튜토리얼만 잘 따라가도 몇 주 만에 감을 잡을 수 있다고 봄. LLM은 이런 학습 과정을 건너뛰게 해주지만, 결국 지시문만 던지는 수준에서 멈춰버릴 수 있어서 좀 복합적인 심정임. 하지만 시장(이익) 중심의 흐름상 AI가 확실히 세상을 삼킬 거라는 점에는 동의함
          + blender-mcp라는 프로젝트 처음 알게 됨. 이걸 써본 적 있는지 궁금함
          + Blender에 영어로 지시만 내리면 다양한 수준에서 작업을 처리해줄 수 있다면 꽤 재미있는 워크플로우가 될 듯함. 예를 들어 “의자 디자인 여러 개 만들어줘”하고, 거기서 마음에 드는 메쉬를 골라 세부를 계속 수정요청 하다가, 마지막으로 손으로 마무리하는 식의 작업 흐름이 상상됨
     * Blender가 MacOS에서 HDR을 지원하는지 확실치 않지만, Windows에선 AFAIK 지원하지 않고, Linux에서는 Wayland에서만 지원된다고 알고 있음. 개인적으로 Wayland와 X11 논쟁은 싫지만, 최신 기능들이 Wayland에서만 돌아가는 모습은 긍정적으로 봄. 이제 X11을 쓰지 않는 이유로 “내가 필요한 기능이 지원 안 돼”라는 명분이 생겨서 의사결정이 좀 더 대칭적으로 느껴짐. (추가: 이 내용은 Blender 5.0 개발 버전과 관련됨) 관련 토론
          + Wayland가 빠진 기능들을 X에서 지원한다는 게 출발점이라면, 최종적으로는 둘 중 하나라도 모든 기능을 지원하는 게 바람직하다고 생각함. 양쪽 다 부족한 상황은 결코 좋은 결과가 아님
          + MacOS에선 확실히 지원하고, Windows에서도 지원하는 걸로 앎. 뷰포트 색상관리 옵션을 Display P3로 세팅하면 됨. 예전엔 AGX나 Filmic 사용에 제약이 있었으나, 이제는 Display P3로 AGX까지 쓸 수 있음
          + Wayland를 꼭 써보고 싶은데, 내가 업무와 게임용으로 쓰는 여러 소프트웨어에서 여전히 너무 많은 버그가 있음. Blender 신버전에서 다시 테스트해볼 예정임. Blender도 예전에 이런 문제가 있었음
          + 최신 기능이 Wayland에서만 지원되는 게 오히려 Blender 완전 도입을 망설이는 이유임. 15년째 직접 만든 취미용 게임 엔진에 Blender를 처음으로 워크플로우에 포함해보려 했는데, 만약 이런 구조면 마음에 들지 않음. X가 나에게 항상 더 안정적으로 작동함
          + 현실적으로 대부분의 상용 소프트웨어와 사용자들은 Windows를 사용함. Blender의 이런 플랫폼별 지원 이슈는 본질적으로 Blender와 서드파티 플랫폼 라이선스 호환성에 관련된 문제임. macOS, Linux, Windows 생태계의 호환성 문제는 예술가들에게 여전히 혼란스러움. 주요 기능을 극히 소수 플랫폼 전용으로 만드는 것은 비효율적이라 생각함. Blender는 미디어, 게임 워크플로우 기능에 더 집중하는 게 낫다고 봄
     * Blender도 Unreal Engine 급의 실시간 기술이 있었으면 하는 바람임
          + 그냥 Unreal을 쓰면 되는 것 같음
          + 예전에 Blender에도 게임 엔진이 있었는데, 어느 버전에서 삭제됐는지 궁금함
     * 퍼포먼스를 홍보하면서, Blender 공식 웹페이지가 너무 느리고 버벅여서 아쉬움
          + 하지만 Blender 자체는 웹사이트가 아니라서 다행임
"
"https://news.hada.io/topic?id=22038","pgactive - Postgres 액티브-액티브 복제 확장","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   pgactive - Postgres 액티브-액티브 복제 확장

     * AWS가 만들어서 공개한 PostgreSQL을 위한 active-active 복제 확장
     * 여러 PostgreSQL 인스턴스에서 데이터 쓰기·수정이 필요할 때, 특정 인스턴스가 단독으로 변경을 수용하는 기존의 액티브-스탠바이 모델 대신, 여러 인스턴스에서 동시 변경과 복제가 가능한 구조를 구축할 수 있음
     * 여러 리전에서 고가용성 데이터베이스 구성이나, 쓰기 지연 최소화, 애플리케이션의 블루/그린 업데이트, 양방향 데이터 마이그레이션 같은 시나리오에 적합함
     * 논리적 복제를 활용하여 충돌 감지, 쓰기 충돌 해결, 타겟 데이터베이스 포맷 변환 등을 지원

액티브-액티브 복제 개념

     * 복제(Replication) 는 데이터베이스 간 변경 내용을 동기화하는 기술
     * 기존 PostgreSQL의 액티브-스탠바이 구조는 한 인스턴스만 변경을 수용하고 나머지는 읽기 전용 형태이므로, 한 곳이 단일 데이터 소스 역할을 함
     * pgactive는 액티브-액티브 복제 토폴로지를 제공함으로써 여러 인스턴스에서 동시에 데이터 쓰기를 허용함
     * 이런 방식은 여러 쓰기 지점이 필요한 환경, 예를 들어 멀티 리전 배포나 양방향 마이그레이션 등에 적합함
     * 액티브-액티브 모델에서는 충돌, 지연, 일부 기능 제약에 대한 별도 관리가 필요함

핵심 기술: 논리적 복제

     * 논리적 복제(Logical Replication) 는 데이터를 외부 시스템이 해석 가능한 포맷으로 전송함
     * 논리적 복제를 통해 대상 데이터베이스에서 충돌 탐지, 쓰기 충돌 해결, 쿼리 변환 등 다양한 부가기능을 구현할 수 있음
     * PostgreSQL은 2017년 버전 10에서 기본 논리적 복제를 도입하였으나, 액티브-액티브 복제에는 추가적인 기능이 요구됨
     * PostgreSQL의 설계 특성으로 인해 이러한 기능은 확장(extension) 형태로 개발 및 적용이 가능함
     * PostgreSQL 개발 커뮤니티도 점차적으로 해당 기능을 기본 프로젝트에 추가하고 있음

        Hacker News 의견

     * 2nd Quadrant와 EDB 팀 동료들에게서 들은 BDR와 pglogical, pgactive, 그리고 Postgres Distributed(PGD)의 발전사를 정리해봄
       가장 처음 나왔고 지금도 오픈소스인 것이 BDR1임 (링크), 그리고 여기에 기반한 게 pgactive임
       BDR2는 BDR1을 폐쇄소스로 리라이트한 거였고, 결국엔 폐기됨
       pglogical v1과 v2(둘 다 오픈소스, 링크)가 출시되었고, 이 중 v1이 대폭 수정되어 Postgres 10에 병합됨
       Postgres 10의 논리적 복제 기능 경험을 토대로 2nd Quadrant는 pglogical v2 개발을 시작했고, 여기에서 pgEdge도 나옴
       이후 2nd Quadrant는 폐쇄소스 버전인 pglogical v3와 BDR v3를 만들었고, 둘을 합쳐 BDR v4가 됨
       그 후 BDR 제품명이 Postgres Distributed(PGD, 링크)로 변경됨
       2ndQuadrant가 EDB에 인수되며 EDB에서 PGD v6을 출시함
          + PostgresPro에서도 별도의 multi-master 복제 시스템이 있음 문서 링크
          + PGDv6이 여전히 폐쇄소스인지 질문함
     * 이 시스템은 Postgres의 Logical replication을 사용해서 한 인스턴스의 변경사항을 다른 인스턴스에 전달함
       충돌이 발생하면 timestamp 기준으로 마지막에 기록된 값이 최종적으로 적용되는 방식임
       충돌이 발생한 내역은 pgactive_conflict_history라는 특수 테이블에 남기 때문에 히스토리 파악, 수동 해결 등이 가능함
       자세한 내용 및 문서는 여기 참고
          + 이게 multi-master replication에 해당하는지 궁금함
            만약 그 기능을 공식적으로 Postgres에 받아들일 수 있다면 흥미로울 듯함
          + 사용자 입장에서는 자신의 쓰기가 즉각적으로 승인이 되는지, 아니면 결국엔 수렴되는 것인지 알고 싶음
     * 최근에 Bloomberg의 데이터베이스(comdb2)를 직접 사용해 본 사람이 있는지 궁금함
     * 관련은 있지만 약간 벗어난 이야기로, ""로컬에서 쓰기가 가능한(read replica 기반) 복제""가 가능한 방법이 있는지 궁금함
       예를 들어 프로덕션이나 스테이징에서 데이터를 읽지만, 로컬에서만 수정 가능하고 그 결과가 다시 upstream에는 반영되지 않는 2차 데이터베이스를 쓰고 싶음
       현재는 주기적으로 스크립트(cron 등)로 데이터 덤프 혹은 질의를 날려서 스냅샷을 만들고, 그걸 S3에 저장한 뒤 로컬에서 받아서 데이터를 복원하는 방식이 대부분임
       이 방법은 대부분 효과적이지만 인덱스 빌드에 시간이 너무 오래 걸릴 때가 있음
          + 참고로, 개인정보 등 민감 정보 문제 때문에 이런식으로 staging이나 dev 환경에 데이터가 바로 들어가면 위험함
            법적, 윤리적 이슈가 크기 때문에 대부분의 회사에서는 이런 방식 권장하지 않음
          + Postgres의 logical replication을 필터와 함께 쓰면 단방향 복제가 가능해서, 복제 슬롯만 해제하면 로컬에서 자유롭게 변경 가능함
            이렇게 하면 primary에는 영향 없이 로컬 테이블만 수정할 수 있음
          + ""순정"" 상태의 로컬 데이터베이스를 백업용으로 두고, 필요할 때마다 이걸 복제해서 개발용 데이터베이스로 쓰면 인덱스 빌드 없이 복사가 매우 빠름
            createdb --template 명령어 추천
          + 로컬 쓰기와 원격 업데이트가 충돌하면 어떻게 처리하는지 궁금함
            모든 상황에 적용 가능한 머지 전략이 떠오르지 않음
          + 내가 아는 한, Postgres 논리적 복제 세팅에서는 이게 표준적인 동작임
            replica에서 쓰기를 막는 게 아니라, 단지 그 결과가 다른 곳으로 퍼지지 않을 뿐임
     * ""Conflict resolution""이란 용어는 결국 ""이미 커밋되고 승인된 데이터를 버림으로써 내구성을 깬다""는 의미라는 사실을 항상 상기해야 함
       모든 active 인스턴스에 걸쳐 쓸 데이터 영역 겹침이 없게 아키텍처를 설계하는 게 최선임
       이런 경우엔 pgactive 같은 툴이 쓸만함
       아니면 아예 처음부터 분산 데이터베이스(Yugabyte 등)를 쓰는 게 맞음
          + 공식 문서에도 충돌 회피 방법으로 master마다 schema를 나눠서 ""각 master는 각 schema의 유일한 writer""가 되도록 추천함
            모든 master가 모든 schema를 읽긴 하지만, 쓸 땐 자기만 담당하는 방식을 제안함
            schema 말고, partition 등도 책임 분산에 쓸 수 있는지 궁금함
     * AWS가 이걸 왜 만들었을까 고민하게 됨
       자사 제품에서 직접 이 기능을 쓰는 게 떠오르지 않음
       RDS는 block replication, Aurora는 고유 SAN replication 사용함
       DMS 정도에서 활용하려는 의도인지 추측함
          + 아마도 최근 출시한 Aurora DSQL 때문일 수 있음
          + 사실 큰 효용을 잘 모르겠음
            강한 ACID 관계형 데이터베이스에서 굳이 이걸 왜 해야 하는지 의문임
          + Aurora의 SAN replication은 cross region replication에는 쓰이지 않는 걸로 알고 있음
          + 해당 저장소 readme에도 ""Multi-Region 고가용성 데이터베이스 클러스터 구축이 주요 용도""라고 명시되어 있음
          + 실제로 RDS Postgres에는 2년 전부터 제공되던 기능이었다고 함 (관련 링크)
            그런데 1달 전에 커뮤니티에 오픈소스로 공식 공개했다는 소식이 있었음 (공식 소식)
     * repmgr, patroni를 써서 완전 무중단 환경으로 여러 클러스터를 운영해봤는데, 이 플러그인은 정말 마지막으로만 설치하고 싶음
       밤에 잘 자기 위해서는 최대한 피하고 싶음
     * 우연히도 최근에 ""자동 페일오버, 노드 복구, 시점 복구"" 가능한 고가용성 Postgres 클러스터를 쉽게 만드는 방법을 찾고 있었음
       patroni+etcd+haproxy 조합이 많이 추천되던데, 실제로 써 본 분들이 흥분해서 추천하는 걸 보면 그만한 이유가 있을 듯함
       다만 docker compose로 예시 파일을 봤을 때는 좀 부담스럽게 느껴짐
       pgpool은 각 postgres 앞에 두기만 하면 되는 것 같아 간단해 보임
       실제 현업에서 Postgres 좋아하는 분들의 추천이나 경험이 궁금함
       docker compose 기반으로 최대한 쉽게, 높은 가용성과 (최소) 무손실, 시점 복구까지 가능한 클러스터 구축을 목표로 하고 있음
          + Barman 같은 툴을 찾는 건지 질문함
          + cloudnativepg를 쓰고 있는데, 이것만으로 필요한 기능이 대부분 바로 동작함
     * pgactive, 관련 사례 등 다른 자료 없을지 공유함
       <i>Pgactive: Active-Active Replication Extension for PostgreSQL on Amazon RDS</i>
       Hacker News 글 (2023년 10월글, 1개 댓글)
     * 비동기 방식인 것 같고, 트랜잭션 격리성에는 큰 이슈가 있을 듯함
          + 결국은 트레이드오프라는 입장임
            즉 각자 상황에 따라 장단점 수용이 필요함
"
"https://news.hada.io/topic?id=22084","XMLUI","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 XMLUI

     * XMLUI는 Visual Basic의 컴포넌트 개발 모델을 현대 웹에 적용하여, React와 CSS 지식 없이도 웹 앱을 간단하게 개발할 수 있음
     * XMLUI는 다양한 컴포넌트를 XML 마크업으로 손쉽게 조합할 수 있으며, 리액티브 데이터 바인딩, 테마 관리, 스키마 확장 등을 지원함
     * Model Context Protocol(MCP) 를 통해 AI와 협업하여 개발 효율성 향상 및 코드의 유지보수성을 높일 수 있음
     * XMLUI는 복잡한 React 생태계를 단순화함으로써 비전문가도 손쉽게 UI 및 앱 개발이 가능한 환경을 제공함
     * 배포 및 확장이 용이하며, 기존에 React·CSS를 잘 모르는 개발자도 다양한 웹 프로젝트와 CMS 구현 수행 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 개요

   XMLUI 프로젝트는 1990년대 Visual Basic에서 찾을 수 있는 직관적인 컴포넌트 조합 방식을 웹 환경으로 가져오려는 시도임. 당시 Visual Basic을 통해 전문 프로그래머가 아니더라도 다양한 컴포넌트를 연결하여 유용한 소프트웨어를 쉽게 만들 수 있었음. 반면, 웹 환경에서는 이러한 수준의 사용성이나 생태계가 이루어지지 못했음. XMLUI는 React 컴포넌트와 CSS를 감싸고, XML 형태의 마크업만으로 웹 앱을 개발하게 해줌.

   다음은 몇 줄의 XMLUI 코드 예시임:
<App>
  <Select id=""lines"" initialValue=""bakerloo"">
    <Items data=""https://api.tfl.gov.uk/line/mode/tube/status"">;
    </Items>
  </Select>
  <DataSource
    id=""tubeStations""
    url=""https://api.tfl.gov.uk/Line/{lines.value}/Route/Sequence/inbound"";
    resultSelector=""stations""/>
  <Table data=""{tubeStations}"" height=""280px"">
    <Column bindTo=""name"" />
    <Column bindTo=""modes"" />
  </Table>
</App>

   이렇게 12줄 정도의 XML만으로도 다음 작업을 표현 가능함:
     * API 호출을 통해 Select 옵션을 자동 채움
     * Select의 값을 활용해 다른 API에서 데이터 취득
     * API 결과 내 특정 필드만을 추출하고, 이를 표 형태로 바인딩함

   XMLUI는 현대적인 컴포넌트 기반이며 리액티브(reactive) 특성을 가지고 있으면서도, 사용자는 React나 CSS의 내부 지식 없이 개발, 유지보수가 가능함. 이 점이 기존의 JavaScript 생태계가 가진 장벽을 낮추는 중요한 차별점임.

컴포넌트 생태계

  과거와 현재

   과거 Visual Basic 시대엔, 차트, 네트워크, 데이터 접근, 미디어 재생 등 다양한 구성 요소(컴포넌트) 를 쉽게 앱에 결합할 수 있었음. 그러나 이러한 생산적인 컴포넌트 생태계는 웹으로 온전히 옮겨지지 못함. React 기반의 컴포넌트가 현재 웹에서 주로 쓰이지만, 여전히 전문 개발자의 역량이 요구됨. XMLUI는 이러한 React 컴포넌트들을 래핑하여 누구나 쉽게 쓸 수 있게 함.

  사용자 정의 컴포넌트

   XMLUI는 다채로운 내장 컴포넌트는 물론, 직접 컴포넌트를 정의하고, 필요에 따라 서로 조합 및 재사용이 가능함. 예를 들어, London 지하철역의 정보를 보여주는 TubeStops 컴포넌트를 다음과 같이 정의할 수 있음:
<Component name=""TubeStops"">
  <DataSource ... />
  <Text variant=""strong"">{...}</Text>
  <Table ... >
    <Column ... />
  </Table>
</Component>

   TubeStops는 Line 이름별로 API에서 데이터를 받아와 표 형식으로 나타냄. 실제 마크업을 보면 가독성이 높고, 100줄 넘게 커지면 컴포넌트로 리팩터링하여 유지보수를 쉽게 할 수 있음. 최근에는 LLM(대형 언어 모델) 의 조력으로 컴포넌트 리팩터링 및 코드 유지보수가 더욱 유연해졌음.

리액티브 바인딩 및 선언적 개발

   XMLUI에서 데이터와 UI의 값 변화가 자동으로 연동(Reactive Data Binding)됨. 예를 들어, Select 컴포넌트의 선택이 변경되면, 이를 참조하는 API 주소(DataSource의 url) 또한 자동으로 업데이트되어 새로운 데이터를 다시 조회함. 이런 방식은 Excel의 셀 참조와 유사함.

   과거식 명령형 개발이 아닌 선언적(Declarative) 개발 패러다임에 익숙해질 필요가 있으나, 익숙해지면 빠르고 직관적인 개발 경험을 얻을 수 있음. 예로, 검색(Search) 기능을 구현할 때 버튼 없이 입력 박스의 값 변화만으로 데이터가 실시간 연동, 표에 반영되는 구조를 쉽게 만들 수 있음.

테마 시스템

   처음에는 테마 시스템에 대한 관심이 크지 않았으나, XMLUI의 테마 관리 기능은 매우 강력함. CSS 작성 없이도 각 컴포넌트의 색상, 배경, 여백, 폰트 등을 변수 기반으로 일관되게 관리 가능함. 예시로, 버튼의 색상을 컨텍스트와 상태(hover 등)에 따라 다르게 지정할 수 있음.

   테마는 color-primary, backgroundColor-Button 등의 형태로 세분된 제어가 가능하며, 전체적인 UI 컬러 팔레트를 손쉽게 정의하고 글로벌 혹은 세부적으로 적용 가능함.

스크립트 활용

   XMLUI는 전체가 선언적이진 않음. Visual Basic처럼 간단한 스크립트(주로 JavaScript)의 부분적 도입이 가능하며, 예를 들어 API 응답 가공(transformResult), 조건부 렌더링 등에 활용함. 이는 복잡한 전문가 수준이 아니더라도 일반적인 개발자라면 충분히 활용 가능한 난이도임.

Model Context Protocol(MCP) 및 AI 협업

   ""이젠 LLM이 React 앱을 바로 만들어주니 XMLUI의 의미가 뭔가?""라는 질문에, 작성자는 코드의 접근성, 유지보수성, 협업성 측면에서 XMLUI의 가치를 강조함.

   MCP(Model Context Protocol) 는 LLM 등의 에이전트가 XMLUI 코드/문서/예제를 검색, 파악, 인용할 수 있는 서버를 제공함. 이로써 AI 및 개발자가 동일한 의미망에서 소통, 코드를 점진적으로 자동 생성 및 수정을 조율할 수 있음.
     * 예: 특정 기능 사용법, 예제, 문서, 사용처를 LLM과 즉시 질의/응답하며 개발 진행 가능

   LLM과 제대로 협업하기 위한 가이드라인도 제공됨. 예를 들면, 코드 제안 전 미리 논의, 문서화된 예제만 활용, 불필요한 스타일링 제한 등이 있음. 또한 문서 사이트에 ""How To"" 섹션, MCP 연동으로 AI도 쉽게 접근 가능한 구조를 마련함.

콘텐츠 관리 및 CMS 적용

   XMLUI를 이용하면 웹사이트 및 CMS 구성도 쉽고, React나 Next.js 지식 없이도 일상적인 페이지 수정, 유지보수가 쉬움. 실제로 XMLUI 공식 사이트, 데모, 문서 등은 모두 XMLUI로 제작 및 유지되고 있음.

   코드, 설명, 실시간 데모를 한 문서에서 모두 제공할 수 있어 실용적임.

확장성

   기본적으로 XMLUI는 다양한 React 컴포넌트를 래핑하지만, 새로운 외부 컴포넌트 래핑도 용이함. 예시로, 고급 문서 편집기 Tiptap을 XMLUI TableEditor로 손쉽게 래핑함. 실제로 Markdown 편집에서 어려운 부분(테이블 만들기 등)을 시각 편집기로 쉽게 해결할 수 있음.

   이처럼, 기존에는 컴포넌트와 솔루션 개발자의 역할이 명확히 나뉘었으나, XMLUI를 통해 일반 개발자도 유용한 UI 컴포넌트를 직접 확장, 조합 가능함.

배포

   XMLUI 앱의 배포는 매우 간단함.
     * 최소 구성: Main.xmlui, index.html, XMLUI JS 파일만 있으면 됨
     * 어떠한 정적 웹서버든 사용 가능하며, AWS S3 버킷에서 바로 구동할 수 있음
     * 복잡한 서버 환경이 필수가 아니며, 필요시 추가적인 로컬 서버, CORS 프록시 등도 구성할 수 있음

모두를 위한 웹 개발

   XMLUI의 창시자인 Gent Hito는 /n software, CData 등에서 개발 환경의 진입 장벽을 낮추는 데 주력해왔음.
     * /n software: 네트워크 컴포넌트의 손쉬운 사용
     * CData: 데이터 접근의 간편화
     * XMLUI: UI 개발의 간소화

   최근 20여년 동안 웹의 UI 개발은 점점 전문화, 복잡화되어 왔으나, XMULI는 전문가가 아니더라도 솔루션 개발자가 자신만의 UI 및 앱을 쉽게 만들 수 있도록 설계됨. 실제로 CoreSSH 관련 대시보드 UI 등 다양한 예시에 바로 적용 가능함.

   좀 더 쉬운 웹 앱 제작 환경을 원하는 모든 개발자, 특히 비전문가 솔루션 빌더, 주니어 개발자, 백엔드 중심 개발자들에게 적극적으로 권장됨.

        Hacker News 의견

     * Jon은 오랜 시간 동안 업계에 있었고, 나는 그의 팬임. 그는 많은 경험을 한 연륜 있는 사람이고, 그의 이야기를 들을 가치가 있음. 나는 웹 컴포넌트의 팬이지만, React가 주도적인 이유는 예전 Visual Basic 컴포넌트를 잘 활용했던 개발자들이 접근하기 어려운 환경 때문이라고 생각함. 이 점이 이 글에서 가장 중요한 부분임. 기술적 설명도 중요하지만, 왜 이런 시도가 필요한지 본질을 짚었음. XMLUI가 이런 개발자들에게 적합한 추상화를 제공할지는 지켜볼 필요가 있음. 그래도 이런 도전을 보는 것만으로 즐거움
          + 현재 코드는 JS evergreen 브라우저에서만 동작함. 예전 VB가 윈도우와 특정 DLL 설치 환경에서만 제대로 동작했던 것과 비슷한 느낌임
     * 2014년 쯤 Polymer에서도 이와 비슷한 시도가 있었음. 예를 들어 <iron-ajax> 같은 컴포넌트로 네트워크 요청을 구현했음 iron-ajax 링크. 또 Adobe Flex가 한참 유행하던 시절이 있었고, 지금은 Apache Royale로 남아 있음 Apache Royale 링크. 마이크로소프트에서는 XAML, NetUI, FlexUI도 있었고, 오피스 2007 UI도 그렇게 만들어졌음. 이론적으로는 모두 멋졌지만, 실제로는 이런 마크업 기반 추상화가 초보자에게도 JSX와 같은 코드 우선 접근 방식보다 효과가 떨어졌다고 느꼈음
          + Coldfusion도 있었음(추억에 몸서리침)
     * 나는 ""우리는 HTML을 또다시 재발명하고 있다""는 생각과 ""이건 내게 당장 쓸모 있을 것 같다""는 느낌을 동시에 가짐. 인간이란 원래 다면적 존재임
          + Walt Whitman이라는 시인과 그 작업을 소개해주어서 고마움. ""내가 스스로를 모순하는가? 그렇다면 나는 기꺼이 모순하는 사람이 되겠음""
          + 정말 공감가는 표현임. 결국 중요한 것은 이것이 실제로 너처럼 필요하다고 상상하는 사람들에게 곧바로 쓸모 있는가임
     * 나는 Qt C++로 KDE에 7년간 오픈소스 기여를 했었음. 이 방식은 QtWidgets의 .ui 파일, 즉 특정 XML 스키마를 따르는 커스텀 UI 파일을 떠올리게 함. 나중에는 QML이 나왔지만 나에겐 비직관적이라 흥미를 잃었음. 하지만 여전히 UI 정의에 XML 사용은 말이 된다고 생각하고, 대규모 환경에서 여전히 쓰이는 게 이해가 됨
          + 아직도 C++과 .ui 파일만으로 Qt를 쓰는 사람들이 있음. QML로 전환할 충분한 이유를 못 느꼈음
          + Blizzard 게임 런처도 QT를 쓴다고 들었는데, Blizzard 소프트웨어의 UI 완성도가 항상 최고라고 생각함. 혹시 추천할 만한 Qt 프로젝트가 또 있을지 궁금함
          + wxWidgets나 glade 파일도 같은 맥락임
     * 내 생각에 최고의 GUI 접근 방식은 JUCE임. 모든 UI 요소가 C++ 클래스로, 그리기 함수가 따로 있음. 새로운 UI 요소는 다른 요소들을 합성해 또 다른 클래스로 만들 수 있고, 에디터가 소스코드를 자동 생성함. 버튼 등은 상태별(hover, pressed, active, disabled 등) 그리기 처리를 위한 if…else 영역이 큼. 내부적으로는 Metal/OpenGL/DirectX 등 얇은 드로잉 라이브러리를 사용함. 이런 완전 명령형(imperative) 방식이 신선함. 어디든 브레이크포인트 걸 수 있고, 어떤 파라미터로 어떻게 호출되는지 바로 확인 가능함. 렌더링 중간 결과를 imdraw로 내보내기도 쉬움. 폰트 안티앨리어싱만 빼면 거의 모든 플랫폼에서 픽셀 단위로 정확한 렌더링임. 그런데 여기서 소개되는 XML 방식은 내가 늘 피하려는 프레임워크 의존적 마법임. 앞으로 3번만 업데이트돼도 레이아웃이 조금씩 엇나갈
       확신이 큼. 사용자가 직접 레이아웃을 제어하는 게 아니라 프레임워크의 자비를 구하는 셈임. Electron은 구형 기술(CSS 등) 위라 이런 문제를 조금이나마 덜 겪는데, 그렇지 않으면 레이아웃 제어에 늘 곤란을 겪게 됨
          + JUCE는 사용 안 해봤지만, 예전 Qt에서 모든 게 C++ 클래스로 돼 있던 시절이 그립기도 함. 템플릿 언어가 대세지만, 클래스와 객체 조합식이 훨씬 읽기 쉽다고 느껴짐. 템플릿에서 가장 큰 장점은 ""이 모듈이 부모 밑에 제대로 들어갔나?"" 밖에 없는 듯함
          + JUICE와 접근성에 대한 사용 경험을 공유해줄 분이 있으면 좋겠음
          + JUCE는 잘 알지 못하지만, JUCE::Component가 DOM/canvas 요소와 비슷해 웹 플랫폼과 견줄 수 있을 듯. XMLUI는 오히려 JUCE 위의 선언적 UI 시스템(GUI Magic, JIVE, VITRO)과 비교돼야 함. 선언적, 명령형 UI는 양립 불가가 아님. SwiftUI, UIKit처럼 둘 다 쓰는 환경도 흔함
          + JUCE는 안 써봤지만, 명령형은 구현이 커져도 일어나는 모든 일을 명확히 알 수 있으니 제어가 쉬움. 선언형은 언제나 탈출구가 필요한데, 이 탈출구가 항상 직접 만들거나 통과하기 어려움
          + 오디오 개발 쪽에서 7년 간 썼고, 요즘은 그냥 모든 크로스 플랫폼 고성능 GUI/일반 앱에 JUCE를 씀. 한번이라도 쓸 만한 JUCE -> CI 파이프라인이 구축되면 정말 무한한 가능성이 열림. 하지만, 가끔은 JUCE의 모든 GUI 코드를 Lazarus 비슷한 프론트엔드, 예를 들면 LUA로 작성하고 C++와 섞어서 루아-씨++ 괴작을 만드는 것도 재미있겠다는 상상을 함
     * XSLT를 언급하지 않은 게 아쉬움. 예전에 XML을 스타일링/트랜스폼하는 걸 고민한 적이 있는 사람들에게 현재의 발전이 얼마나 큰 도약인지 설명하는 데 중요한 요소라고 생각함. Jon Udell이 XSLT에 관해 글도 썼던 걸 보면 참고 링크, 이번에 의도적으로 뺐던 것 같기도 한데, 그 이유를 잘 모르겠음
          + XSLT가 쓰인 현장은 대부분 ""원 저자 외에는 아무도 건드리기 무서워하는 복잡한 머리카락 뭉치""였음. 이 기술이 이상하게도 복잡성의 함정에 빠지거나 복잡성 페티시스트들을 끌어들이는 경향이 있음. 어쨌든 OP가 지향하는 목적에는 어울리는 선택지가 아니라고 생각함
          + 역사적 참고가 필요하긴 하지만 이번 글의 목적은 과거에 집중하는 게 아니라 앞으로 나아가는 데 있음. 이 도구를 직접 써보고 UI 구축에 생산적인지 평가하는 게 글의 목적임
          + 이 글에서는 XSLT가 크게 중요하지 않음. 현대 독자들에게 왜 이런 도구가 유용한지 설명하는 게 골자임. XSLT가 역사적으로 관련은 있지만, 여기서 언급하는 게 아이디어 전달에 도움 될 것 같진 않음
          + oleh kiselyov의 SXML/SSAX를 알게 된 후부턴 XSLT를 완전히 접었음. SSAX는 내가 써본 것 중 최고의 XML 파서임
          + 내 첫 XSLT 경험은 sketchers.com이었음 Sketchy Skechers.com. 아쉽게도 지금은 더 이상 사용하지 않는 것 같음
     * 나는 최근 HTML, web components, signals 기반으로 비슷한 걸 만들고 있음. Heximal이라는 프로젝트임 Heximal 링크. HTML에 표현식, 템플릿, 리액티비티, 컴포넌트 구조를 얹어 매우 모듈화되고 선언적인 앱이나 페이지를 만들면 뛰어난 기반이 된다고 생각함. HTML에 추가되는 많은 기능도 표준화될 수 있음
          + 아이디어는 흥미롭지만, 모바일(Android+Firefox)에서는 사이트가 잘 보이지 않음
          + 사이트가 읽기 어려움. HN 앱에서도 다른 댓글이 잘 안 보여서 다른 사람들이 같은 이슈 겪는지 모르겠음. 모바일 파이어폭스 기준임
          + 모바일에서 텍스트가 일부 잘려 보이고 확대해도 해결되지 않음. 참고하길 바람
          + 이 접근이 대세가 될 수도 있다고 생각함. C++가 대세가 된 것처럼 뒤로 호환성이 강력하다는 점이 정말 강점임
     * RJSF의 uiSchema가 jsonSchema 모델 정의를 보완하는 프레젠테이션 레이어로 좋은 방향을 제시했다고 생각함 uiSchema Reference 링크. 인상적으로 설계됐지만, 널리 퍼지지 않은 게 놀라웠던 기억임
     * 나는 아직 보이지 않는 부분들에 대해 특히 기대가 큼. 견고한 엔지니어링에 더해, WYSIWYG 프로그래머(직관적으로 UI를 구성하는 개발자들)에 대한 배려가 확실할 것 같음. 어릴 때 Visual Basic 덕분에 프로그래밍에 접근할 수 있었다고 생각함. C++의 복잡한 포인터 없이도 쉽고 마법 같이 많은 것을 할 수 있었고, 이 흐름이 웹 프로그래밍에도 초보자 우선 접근법으로 연결되어, 반응성, 부드러움에 타협하지 않으면서 적절히 현실적 타협이 이뤄지길 바람. 더 흥미로운 것은 https://docs.xmlui.com/mcp 임. Claude 같은 도구가 UX/대시보드 코드를 생성할 때 필요한 토큰 수를 줄여 더 간결한 코드를 만들어낼 수 있음. 오늘부터 바로 써볼 생각임
          + 사용 경험에 대해 꼭 알려주길 바람
     * XAML(특히 범위가 좁은 실버라이트 버전)은 잘 활용하면 정말 즐거운 도구였음. 그러나 가장 쉽고 명백한(동시에 비효율적인) 방식으로 사용하면 끔찍하기도 했음. 아마도 HTML5나 도구의 부족으로 잊힌 것임. 좋은 도구는 초보자도 성공에 이르게 도와야 하는데, XAML은 그런 점에서 부족했음
"
"https://news.hada.io/topic?id=22076","AI 출력물을 사람에게 보여주는 것은 무례함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        AI 출력물을 사람에게 보여주는 것은 무례함

     * AI가 생성한 텍스트를 다른 사람에게 무분별하게 전달하는 것은 상대방을 혼란스럽게 하는 행동임
     * 예전에는 작성된 텍스트가 인간의 사고의 증거였으나, AI의 발전으로 이러한 신뢰가 무너짐
     * AI가 생성한 콘텐츠의 전파는 발신한 사람이 스스로 결과물을 소화해서 자신의 언어로 전달하든지 수신자의 동의가 있을 때에만 허용 가능함
     * AI의 메시지 남용은 수신자에게 쓸모없는 정보의 피로와 리소스 낭비를 유발함
     * AI 에티켓을 지키지 않고 AI 답변을 직접 전달하는 것은, 사회적으로 무례한 행위로 간주됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소설 Blindsight의 배경과 신호의 의미

     * Peter Watts의 SF 소설 Blindsight에서는, 인간이 의식이 없는 외계 종족(scramblers) 과 조우하는 내용이 중심임
     * Scramblers는 인간과 달리 불필요한 정보를 싫어하며, 쓸모없는 신호 전송을 침략 행위로 간주함
     * 인간이 의미 없는 텍스트를 아무렇게나 퍼뜨리는 것처럼 정보를 남용하는 행위가 상대방의 자원을 낭비시키는 공격으로 해석됨

Proof-of-thought(사고의 증명) 개념

     * 과거에는 직접 작성한 텍스트만이 소통의 주체였기 때문에, 뭔가를 읽는 행위는 자연스레 인간적인 사고의 흔적을 신뢰할 수 있었음
     * AI의 보급으로 인해 텍스트, 코드, 이미지, 영상 등 모든 미디어가 너무 쉽게 양산되면서, 그 내용에 인간적 사고의 증명(Proof-of-thought)이 사라짐
     * 이제는 누구든지 AI의 산출물을 재전송하면, 수신자는 무의미한 정보에 소중한 시간을 빼앗길 위험이 있음
     * AI는 기본적으로 요청에 반응할 때만 대답을 내놓기 때문에, 사람이 요청하지 않으면 AI가 알아서 정보 홍수를 일으키지 않음
     * 결국 문제의 본질은 AI 자체가 아니라, AI 출력물을 무분별하게 이용하고 전파하는 인간에 있음

AI 에티켓의 필요성

     * AI의 산출물을 그대로 다른 사람에게 전달할 때는 서로 간의 합의나 명확한 동의가 반드시 필요함
     * 누군가에게 ""나는 ChatGPT에 물어봤더니 이렇게 나왔어"" 같은 식으로 바로 전달하면, 상대방이 무례함을 느낄 수 있음
     * 개인적인 의견이나 판단 없이 AI가 준 답변을 전달하는 것은 상대방에게 불편과 정보 오염을 초래함
     * 예를 들어, ""ChatGPT로 이런 답변을 얻었는데, 원한다면 대화 로그를 보여줄 수 있음""이라고 하면 상대방이 선택권을 가질 수 있음
     * 자신의 작업물을 리뷰 받을 때, ""AI가 자동으로 만든 내용이니 직접 봐줘""가 아니라, 본인이 직접 검토 후 요약해서 보내는 태도가 요구됨

결론: AI 출력물 전파 시 주의사항

     * Scramblers와 다르게, 인간은 아무 의미 없는 신호를 수동적으로 받는 것이 아니라, 선택하여 수용하는 존재임
     * AI 에티켓을 지키고, AI가 만든 텍스트를 직접적으로 전달하지 말 것이 예의임
     * AI 산출물을 활용하더라도, 항상 받는 이의 시간과 주의력을 존중하는 자세가 필요함
     * AI 활용 결과물을 그대로 전달할 땐, 수신자 동의를 먼저 구하는 습관이 중요함
     * AI가 생성한 '노이즈'의 범람 속에서 책임감 있게 정보를 채택하고, 스스로 필터링하는 역량이 사회적 에티켓으로 자리잡아야 함

        Hacker News 의견

     * 일부 동료들이 이메일이나 Teams 메시지를 LLM으로 쓰는 걸 그만두었으면 하는 바람임, 이런 메시지는 너무 무성의하게 느껴져서 이제는 읽고 싶지도 않음
          + 가끔 동료가 실수로 AI와의 대화 내용을 그대로 남겨놓을 때가 있는데, 이런 건 딱 봐도 바로 티가 남. 최근 받은 이메일 하단에 ""Outlook에서 서식 지정할까요, 아니면 특정 채널이나 배포 리스트에 올리는 걸 도와드릴까요?""라는 문구가 붙어있었음
          + 이런 상황에서는 ""보내주신 메시지들이 일부 LLM에서 생성된 것 같음. 올바른 문법과 스타일을 추구하는 것은 고맙게 생각하지만, 오히려 가끔 오타나 투박한 표현이더라도 의미가 왜곡되거나 맥락이 사라지는 것보다 훨씬 좋음. 앞으로는 직접 써서 보내주셨으면 함. 내부 커뮤니케이션에서는 대문자 미사용이나 구어체 표현도 전혀 신경 안 씀""이라고 직접 요청해보았음
          + 장애로 인해 글쓰기 어려움을 겪는 사람들은 AI의 도움으로 평소보다 더 잘 자기 생각을 표현할 수 있게 되었음. 비록 지금 상황과 직접적으론 다를 수 있지만, 이런 부분도 함께 생각해볼 필요가 있음
          + LinkedIn이 이런 면에서 가장 심한 곳임. 원래도 “기업/프로페셔널 물타기”로 가득한 곳인데, 이제는 인터페이스에서 AI 생성 답글을 적극적으로 추천함. 최악의 소셜 네트워크라고 생각함. 예시: ""정말 통찰력 있네요! 일상적인 업무를 변혁적 브랜딩 기회로 만든 걸 보니, 진정한 자기 PR의 달인임!""
          + 이런 피드백을 직접 동료에게 전달해본 적이 있음. 내 보고자가 AI Slop(성의 없는 AI 답변)으로 반응하기 시작해서 ""직접 쓴 건가?""라고 물으니 아니라고 해서, ""이런 방식은 마치 나한테 귀 기울이지 않은 것처럼 느껴진다""고 정확히 피드백했음. 다행히 이후로 안 그러더라. 앞으로 모델이 더 똑똑해질수록 이런 거 구분이 더 어려워질 텐데, 결국 실제로 손해를 보는 건 무심하게 AI만 붙여 보내는 사람들이 될 거라고 생각함. (South Park 에피소드처럼) 본인도 모르는 커밋에 휘말려 책임질 수 있음
     * 남에게 질문하는 목적은 단순히 기술적 답변을 듣고 싶어서라기보다, 상대방의 의외의 생각을 듣고 연결되어 보거나 협업의 시작점을 찾아보는 데 의미가 큼. 실제 사람과의 대화에는 다양한 여정과 생각의 가지가 생기는데, AI는 그저 생기 없어 보임. 누가 내게 AI로 만든 답을 복사해서 붙였다면, 그 사람은 내게 아예 관심이 없다는 의미로 받아들여짐. 이런 사람과는 상호작용하고 싶지 않음
          + 대화로 치면 ""구글에 검색해봤으면 됨""과 다를 바 없는 느낌을 받음
     * ""15분 만에 감정으로 짰던 PR, 리뷰 부탁""이라는 슬로건을 붙인 PR을 본 적이 있음. 그런데 실제로는 PR 작성자는 이런 문구조차 남기지 않고, 리뷰어가 직접 물어봐도 인정하지 않음. 내 리뷰 코멘트가 곧장 AI에게 넘어가 다시 엉뚱하게 수정된 PR이 10분 뒤에 또 올라오고, 내 핵심 피드백은 반영도 안 되어버림. 차라리 AI랑 직접 이야기할 수 있으면 좋겠음. (참고로, 지금은 PR을 무시하거나 닫을 권한조차 없음)
          + PR을 무시하거나 닫지 말고, 대화를 시작해봐야 함. AI는 사람이 아니고, 버그나 품질 문제 있는 코드를 제출하면 책임은 결국 제출한 본인에게 돌아간다는 사실을 인식시켜주는 것이 중요함. 또, 자기가 직접 리뷰하지 않고 AI 출력만 넘겨줄 생각이라면, 그 사람 자체가 아무런 가치를 더하지 않는 셈임. AI Slop만 넘겨주는 사람들이 주변에 있다면 멘토링 기회라 생각하고 바른 방향을 알려주는 게 산업 전체의 질을 높일 계기가 될 수도 있다고 생각함. 앞으로는 비판적 사고, 디버깅, 비즈니스 맥락 연결이 가능한 사람이 진짜 경쟁력이 될 것이고, 그렇지 않은 사람들은 점점 뒤처질 것이라는 확신이 듦
          + 실제 현실은 훨씬 더 우울하다고 생각함. 리뷰 과정을 모두 AI에 맡기는 건 결국 본인 능력 평가도 깎아먹게 될 위험이 있음. 성과 평가에서 감점 주는 방식은 어떨지 고민됨
          + 신뢰는 천천히 쌓이고 한순간에 무너짐. 사전 양해 없이 AI가 만든 엉성한 PR의 리뷰를 내게 계속 강요한다면, 그 사람의 다음 PR은 리뷰하지 않음
          + 이런 상황 별로임, 정말 지옥 같은 업무 환경으로 느껴짐
          + 이럴 땐 바로 매니저에게 전달하는 것도 하나의 방법임
     * ""글쓰기는 읽기보다 더 비용이 많이 드는 일이었다""는 말에 깊이 공감하게 됨. LLM 이후엔 이 공식이 뒤집혔고, 나 같은 경우엔 특히 코드 리뷰에 소요되는 시간이 엄청 늘어남. 작성자와 리뷰어 모두 코드 변경에 대해 비슷한 이해 수준이 되는 환경이라, PR 자체가 더욱 명확하게 쓰여져야 중요해짐. 이 변화가 가져오는 숨겨진 효과들도 궁금함
          + 예전엔 풍경을 그리는 게 실제 풍경을 보는 것보다 훨씬 시간이 많이 드는 일이었지만, 오늘날에는 둘 다 비슷한 노력이 들어감. 인류는 이런 변화에도 잘 적응해왔고, 사진이 있는 세상에 살아 행복함. 앞으로는 명시적으로 밝히지 않은 AI 생성물, 그리고 검증도 제대로 거치지 않은 콘텐츠에 대한 강한 부정적 인식이 자연스럽게 생겨날 것이라고 봄
     * 누군가 ChatGPT와 나눈 대화를 이야기하는 풍경이 요즘 ""어제 꾼 꿈 이야기"" 같은 새로운 소재가 되었음 (좀 아쉬운 건, 실제로 하고 싶은 말이 많은데 이런 대화들도 잘 못 나눈다는 것임)
          + AI와 나눈 대화에 대해 ""이런 경험을 했는데, 이런 점이 흥미롭고 나에겐 이런 영향을 줬어""라고 본인의 실제 경험을 이야기하는 것은 충분히 인간적이고 유효함. 반면 대화 내용을 그대로 넘기는 건 ""내가 입력값 줬으니 이 결과를 네가 생각하고 뭔가 해야 해"" 식의 태도로 받아들여짐. 나 역시 Claude나 ChatGPT와 이야기하면서 느낀 바를 친구나 파트너와 나눌 때는 있지만, transcript를 그대로 공유하지는 않음
          + 꿈 얘기는 그래도 내 뇌가 직접 만든 결과지만 ChatGPT는 수많은 인터넷 이용자의 ""절인(brined)"" 뇌가 프롬프트를 보고 즉각 연상하며 생성한 것임. 완전히 다른 종류의 결과임
          + 나도 이와 같은 인상을 가졌음. 경험한 사람에게는 흥미롭지만, 듣는 입장에서는 그다지 재미가 없음. 가끔 다른 사람의 머릿속을 살짝 들여다보는 느낌이 신선할 수도 있지만, 내용 자체는 별로 중요하지 않음. 꿈과 AI 답변 모두 본질적으로 ""환각적 특성""—즉, 실질적 내용이 없다는 점에서 닮았음. 결국 중요한 건 커뮤니케이션의 본질임
          + 오히려 ""술에 취한 삼촌에게 물어본 것 같은데, 대답은 되게 자신감 넘쳤다"" 느낌임
     * 누군가 AI로 생성한 결과를 전달했다면, 그 사실을 명확히 밝혀야 함. 퀄리티가 낮아도 AI 썼다는 사실이 면책 사유는 아니고, 모든 출처를 밝히는 게 중요함. AI 생성 콘텐츠를 받기 싫다면 출처 명시 덕분에 미리 걸러낼 수 있음
     * 최근 ChatGPT의 스크린샷(무료 버전)을 이메일에 첨부하며 내 기술적 의견에 이의를 제기한 비전문가를 만났음. LLM 답변이 틀리지는 않았지만, 오히려 적당한 미사여구에 둘러싸여 있었고, 상대방은 실제로 답변 속 핵심 의미를 이해할 역량이 없었음
     * 관련해서, 나는 아예 프롬프트 자체를 읽는 편이 더 낫다고 생각함 https://news.ycombinator.com/item?id=43888803
          + 내 입장은 이러함:
               o 동의함
               o 하지만 사람들은 보통 글(문장)을 원하고, 글머리표 안 좋아함
               o 문화적 차이 문제임
     * 누군가가 ChatGPT의 도움을 받아 토론하는 상황이 흥미롭게 느껴짐. 자기 손으로 쓴 사람과 비교해봐도, 반복적으로 깊이가 떨어지고 겉핥기뿐임이 드러남. 이런 대화는 성의와 진솔함이 없어서 바로 중단하게 됨
          + AI를 안 쓰는 사람이 뒤처지는 게 아니라, 오히려 AI만 이용하는 사람이 도태되는 것이라고 생각함. (Roko's Reverse Basilisk?)
     * 최근 온라인 쇼핑몰에서 불쾌한 경험이 있어서, ""회사 자체는 마음에 드는데 이런 경험이 불편했다""고 지원팀에 이메일을 보냈더니, ""AI Agent Bot""이 온갖 미사여구로 ""조치가 필요 없으시고 주문도 정상 처리되었으니 티켓은 닫겠다""는 답변만 돌아왔음. 이메일 작성 도움에 LLM을 활용하는 건 좋지만, 고객 문의를 무조건 봇이 닫는 건 정말 무례하다고 느낌
          + 이건 기업 내면의 문화가 고스란히 드러난 사례임. ""고객 따윈 신경 쓸 가치 없다""는 태도가 읽혀짐
"
"https://news.hada.io/topic?id=22036","아이폰 미니 사이즈의 안드로이드 폰을 원함 (2022)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     아이폰 미니 사이즈의 안드로이드 폰을 원함 (2022)

     * 작은 프리미엄 안드로이드 폰에 대한 시장의 부재로, 개인이 직접 관심자들을 모아 제조사에 압박을 주고자 하는 움직임임
     * 현재 6인치 미만의 고사양 안드로이드 폰은 존재하지 않으며, 작은 크기, 한 손 사용성, 휴대성 등이 핵심 가치임
     * 이상적인 스펙은 아이폰 13 Mini 크기, 최상 카메라, 순정 안드로이드 OS로 요약 가능함
     * 제조사들이 소형 프리미엄 모델 출시를 꺼리는 만큼, 사용자가 직접 움직여야만 변화가 가능함
     * 소형 폰 시장이 사라지고 있는데, 작은 폰을 원하는 유저들이 힘을 합쳐 다시 카테고리를 부활시키는 것이 목표임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소형 프리미엄 안드로이드 폰에 대한 열망

   Eric Migicovsky는 신체가 크지만 작은 스마트폰을 매우 선호함
   Sony Xperia Compact 시리즈의 종료 후, 현재 시장에는 원하는 크기의 고사양 안드로이드 폰이 부재함

  왜 작은 폰이 중요한가

     * 주머니에 잘 들어가는 크기, 가벼움, 한 손 사용의 용이함, 자전거 등 이동 시 휴대 안정성이 장점임
     * 손이 작은 유저들도 큰 폰을 꺼려하는 경우가 많음
     * 화면과 배터리가 작아지는 단점을 감수하고도 사이즈가 더 중요함
     * 대형 폰 선호자와 소형 폰 선호자가 명확히 나뉘는 시장 구조임

소형 프리미엄 폰의 부재와 액션 필요성

     * 현재 소형 프리미엄 폰은 어떤 제조사 로드맵에도 없음
     * 팬들이 함께 목소리를 내고 제조사(특히 Google, Samsung 등)에 영향을 주어 새로운 소형 폰 출시를 유도하자는 취지임
     * 지금이 마지막 기회가 될 수도 있어서, 관심있는 사람들의 적극적인 참여와 공유를 요청함
     * 실제로 동참한 인원이 이미 41,000명을 넘었으며, 이를 50,000명 이상까지 확장해야 함

꿈꾸는 이상적인 소형 안드로이드 폰

     * 6인치 미만 디스플레이, 아이폰 13 Mini와 유사한 크기, 고성능 카메라, 순정 안드로이드 OS가 핵심 사양임
     * 시장에 이 조건을 만족하는 스마트폰은 존재하지 않음
     * 가격은 700~800달러 내외로 예상하며, 대체재가 없으니 더 높은 가격도 수용 가능함

  구상하는 최소사양

     * Mini와 비슷한 산업 디자인, 균일한 베젤
     * 5.4인치 FHD OLED 디스플레이(60hz 이상)
     * Pixel 5 수준의 카메라 성능과 저조도 촬영 품질
     * Snapdragon 8 또는 동급 플래그십 칩셋
     * 5G, 홀펀치 전면 카메라, 2개 후면 카메라(광각 포함), 8GB RAM
     * 128/256GB 저장공간, 4시간 Screen On Time, 언락 가능한 부트로더, NFC

  추가 희망 사항

     * 케이스 없이 견딜 수 있는 내구성
     * IP68 방수, 전원버튼 지문인식, 하드웨어 음소거 스위치
     * 무선 충전, eSIM

커뮤니티의 역할 강조

     * 당사자들이 직접 발벗고 움직이지 않으면 이 카테고리의 폰은 다시 나오지 않음
     * Sony가 Compact 시리즈를 중단한 이후, 타 제조사는 소형 플래그십을 시도하지 않음
     * 5만 명 이상의 구매 의향자가 모이면, 제조사 설득의 동력이 될 수 있음
     * 필요하다면 글쓴이 본인이 직접 제작을 검토할 의지가 있음
     * 트위터 DM으로 동참 의사자 모집 중임

소형 스마트폰 카테고리의 부활 촉구

     * 더 이상 소형 프리미엄 폰이 사라지는 것을 원치 않으며, 유저가 힘을 합쳐 시장을 다시 만들자는 명확한 목표임

FAQ (자주 묻는 질문)

  글쓴이 소개

     * Eric Migicovsky는 Pebble(첫 상용 스마트워치) 창업 후 Fitbit에 매각
     * Y Combinator 파트너 경력, 현재 Beeper(통합 메신저 앱) 개발자
     * 다양한 하드웨어 산업 경험과 네트워크를 보유

  폰 회사 창업이 미친 짓인가?

     * 쉽지 않은 도전이지만, 커뮤니티 힘으로 구글 혹은 다른 안드로이드 제조사를 설득하고 싶은 목표임
     * 만약 어느 누구도 움직이지 않는다면, 본인이 직접 생산을 고민하겠다는 의지 표명

  왜 아이폰 Mini를 그냥 쓰지 않는가?

     * 2021년 Pixel 6가 너무 커서 iPhone Mini로 갈아탔음
     * 하지만 Mini 판매 비중은 낮아(전체 iPhone의 5% 내외), 애플이 라인업을 곧 없앨 가능성이 높음
     * 애플 입장에선 적은 수량이지만, 독립 회사 입장에선 1천만 대면 충분히 의미 있는 수치임
     * 만약 iPhone Mini가 단종된다면, 안드로이드와 Beeper로 대체할 수 있는 기회임
     * 개인적으로도 iOS의 알림, 작업 효율, 파일 이동 불편이 커서 안드로이드 복귀를 희망함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   과거 모델 추이를 고려하면, Pixel 10은 캘리포니아 주 크기가 될지도 모른다는 풍자적 이미지 첨부
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

     * 2022년 5월 기준 작성
     * 사진 제공: Flickr

   예전부터 너무 갈망함ㅠㅠ

        Hacker News 의견

     * 현실적으로 돈을 내고 작은 폰을 사는 시장 자체가 존재하지 않음, 사람들이 실제 구매 시점에서는 대부분 화면/배터리/카메라 성능이 월등한 일반 크기 기기를 고름, 이건 내가 10여년 전부터 'QWERTY 패러독스'라 부른 현상과 같음, 물리 키보드 폰을 원한다는 사람이 많아도 막상 제품이 나오면 더 슬림하고 가벼운 올스크린 폰을 사버리는 일이 반복됨, 다양한 브랜드에서 고사양 QWERTY폰이나 미니폰 시도 했지만 결국 시장에서 실패함, 미니폰이 시장을 확장한 적 없이 기존 유저층만 분산시킴, 오랫동안 이 업계에서 일한 경험임
          + 대부분의 사람들이 큰 화면, 좋은 배터리와 카메라가 있는 일반 크기 폰을 고르는 이유 중 상당 부분은 디자이너들의 문제라 생각함, 저가형 미니폰을 만들면서 성능을 낮추니 당연히 잘 안팔림, 나처럼 프리미엄 성능의 소형/슬림폰을 바라는 사람도 있음, 예를 들어 아이폰 프로의 카메라는 그대로 두되 화면만 줄이고 화질은 동등하게, 두께는 더 두께져도 됨, 그렇게 내부 공간을 확보해서 배터리 등 기능을 유지하는 디자인에는 아직 미개척 영역이 많음, 미니폰을 무조건 저가형으로만 내놓다보니 프리미엄 소형폰은 없음, 실제로 5G지원하는 비슷한 크기의 안드로이드 폰이나 프리미엄 소형폰은 최근엔 전혀 출시된 바 없음
          + 소비자에게 노란색 Sony Walkman을 살 건지 물으면 산다고 하지만, 실제 선택할 땐 대부분 검정색을 고르는 현상과 같음, 관련 내용은 Yellow Walkman Paradox에서 볼 수 있음
          + 이런 이야기를 볼 때마다 대기업이 제품 생산 및 유통의 규모 문제나 판매 효율에 만족하지 못하길 바람, 작은 기업이 연간 500~1000대 정도의 틈새 미니폰을 출시해서 망하지 않는 선에서 소규모 욕구만 충족해줬으면 좋겠음
          + 물리 키보드 폰이 실패한 건 맞지만, BlackBerry는 예외로 봐야함, 한때 세계를 지배하던 폼팩터를 현대 스마트폰 기반으로 재해석하려는 시도가 여러 벤더에서 있었지만 지금 보면 무리였던 느낌임
          + 아이폰 미니가 애플 라인업에서 차지한 비중은 작았지만 전체 브랜드 중에선 상당히 많이 팔린 폰임, 나 역시 2020년엔 안드로이드 쪽 작은 폰이 없어 아이폰으로 갈아탐, 내 친구들도 12/13 미니 때문에 전환한 경우가 있음, 미니가 망한 진짜 이유는 화면 크기가 아니라 저렴한 SE 모델에 시장이 잠식된 것임, SE는 미니보다 화면이 거의 1인치 작음에도 불구하고 엄청 팔림, 작은 폰이라도 가격만 합리적이면 수요가 분명히 존재함을 증명함
     * 내가 비관적으로 보는 건 소형 폰 자체가 제품이 아니라는 점임, Vape펜처럼 진짜 상품은 안에 들어가는 중독성 있는 앱과 광고임, 작은 화면은 Google/Apple/Meta/X 등 여러 곳의 핵심 지표에 부정적 영향을 미침, 애플이 아이폰 미니를 중단한 건 충분한 수익이 안 되어서였지만, 작은 폰이 인기가 있다는 점을 보면 그 틈새 시장은 충분히 수익성 있음, 나도 12 미니를 최근까지 썼으며 16 Pro(아마 마지막 애플 제품이 될 듯)로 바꾼 결과 카메라 빼고는 무게만 늘고 불편해진 느낌임, 메이저 브랜드가 작은 폰을 원하지 않는 이유는 결국 중독성 콘텐츠와 광고 공급망에 있는 문제임
          + 아이폰 미니 출시 전략이 의아했던 이유는, 미니보다 SE를 먼저 출시해버렸기 때문임, 수년간 소형폰이 없던 상황에 SE가 나오자 대기 수요가 몰려 즉시 팔림, 몇 달 뒤 미니를 출시했을 때 이미 살 사람들은 SE를 사버려 짧은 기간에 업그레이드 수요가 없었음, 1년 정도 텀을 두고 미니를 냈으면 파워유저들이 넘어올 여유가 생겼을 것임
          + 너무 비관적으로 생각하는 것 같음, 초기 '패블릿'도 갤럭시 노트 시리즈(2011년)가 원조인데, 처음엔 사이즈 때문에 회의적이었지만 작은 여성분들도 자주 사용하는 걸 봤음, 이후 화면 크기는 꾸준히 커졌고 대중이 오히려 더 큰 기기를 원하게 됨
          + 애플이 하드웨어 문제에 소프트웨어 개선책만 들이대는 게 답답함, Reachability, Screen Time, Focus Modes 등 하드웨어로 간단히 해결될 문제임, 작은 폰이 원래 보조 기기 역할로 효과적인데, 옛날 아이폰의 방향성이 맞았다고 봄, 결국 나 역시 13미니에서 16Pro로 넘어왔음, 이유는 배터리 수명과 소프트웨어 호환성, 카메라 때문이었음, 폰 사용 시간만 늘고 데스크탑/랩탑 사용이 줄어드는 현상은 불만임, 큰 폰은 모빌리티를 빼면 데스크탑에 비해 거의 모든 면에서 불리함
          + 아이폰 4 크기의 최신형 아이폰을 선호함, 12 미니에서 16 Pro 넘어가니 크고 무거워서 과거로 회귀하는 기분임
          + 큰 화면을 선호하는 게 광고 때문이라는 말엔 동의하지 않음, 많은 제조사가 광고 비즈니스가 없는데 굳이 신경쓸 필요 없다고 봄, 크기는 바지 주머니와 가방 크기, 그리고 사용 목적에 따라 결정됨, 사진/영상 편집, 소셜 업로드는 큰 화면이 적합함, 패널 가격 구조나 아이패드 미니의 존재, 폴더블 기기 인기 보면 소비자들이 큰 화면을 원한다는 걸 알 수 있음
     * 지금은 Motorola razr를 타협으로 삼아 펼치지 않고 사용 중임
     * 현재 Unihertz Jelly Star라는 아주 작은 폰을 쓰고 있고, 제대로 된 작은 폰이 없다는 것에 대한 일종의 '항의폰'임, 주변 친구들은 장난식으로 '마이크로폰', '감옥폰'이라고 부르기도 함, 매장에 케이스를 문의할 때마다 직원 반응을 보는 게 재미임, 성능에 아주 만족하지만 문제점은 소프트웨어 업데이트가 없고 카메라가 별로고 OLED 화면이 아님, 이상적인 폰은 이보다 약간 더 큰 사이즈였으면 함, 그래도 이 폰에 매우 애착이 생김
          + Jelly Star 같은 기기는 소프트웨어 업데이트가 부실해서 꺼려짐, Graphene 같은 운영체재나 Lineage 지원만 됐다면 훨씬 관심이 높았을 것임
          + Jelly Star 배터리 타임이 크기에 비해 꽤 괜찮음, 일반적 사용 기준 8시간 정도 사용 가능하지만 GPS나 영상 보면 중간에 충전 필요함
          + OLED가 아니라서 오히려 burn-in이 없어 장점이라 생각함
          + Jelly Star는 좀 뒷부분이 많이 두꺼움
     * 6인치는 전혀 작은 폰으로 안 느껴짐, HTC 8X가 4.3인치인데 그게 나한텐 '정상' 사이즈였음, Palm Phone(PVG100)은 거의 신용카드 크기(3.3인치)라서 아주 오래 썼지만 점점 느려지고 배터리도 약해져서 포기했음, 지금은 Soyes S10Max(3.5인치, 첫 아이폰과 같은 화면크기)를 사용 중인데 좀 두꺼움, 일상에 필요한 건 무난히 처리하지만 Palm Phone의 슬림함이 그리움, 지금은 8기가램+128기가 스토리지의 Bluefox NX1 제품을 예약 구입했고, CPU 성능은 Soyes보다 강력하지만 해상도(540x1168)가 약간 걱정임, 4인치지만 베젤이 거의 없어 크기는 Palm Phone이랑 비슷할 듯, 그래도 배터리가 커서 조금 더 두꺼움, 전반적으로 Soyes에 비해 슬림함, 전면 비교 이미지들도 공유함: 이미지1 이미지2, 기대 중임
          + 5G, eSIM, NFC 없고 전체적으로 두꺼운 점은 아쉬움
          + Bluefox NX1 링크가 스페인에서는 잠깐 보였다가 바로 google.com으로 리디렉션됨, 아마 유럽 고객 차단을 그런 식으로 처리하는 듯
          + Bluefox NX1이 유럽의 5년 보안 업데이트 의무 법률을 어떻게 준수하는지 궁금함, 관련해 정보 있으면 공유 바람, EU 법 안내
          + 나도 PVG100을 썼었고, 'juicepack' 배터리로 두께가 두 배였지만 포켓에 잘 들어갔음, 지금은 Motorola Razr 사용 중, 폴더블 아니면 기업들이 더이상 소형 폰을 내놓지 않을까봐 걱정임
          + 실제로는 화면 크기로만 비교하면 안 되고, 기기 크기(사이즈)를 봐야함, 베젤이 줄면서 같은 화면 크기도 훨씬 작게 느껴짐, 예를 들어 HTC 8XT와 아이폰 12 미니 실제 크기 비교 참조 바람
     * 업무용으로 아이폰 13 미니를 선택했던 이유가 당시 가장 작은 아이폰이기 때문이었음, 개인적으로도 점점 커져가는 스마트폰 사이즈의 시대 흐름이 불만임, 개인 폰으로는 Motorola Razr 50 Ultra 사용 중이며, 폴더를 닫으면 휴대성이 아주 좋음, 주머니에서 불편함 없이 들고 다닐 수 있음, 커버 스크린도 꽤 자주 활용하면서 굳이 폰을 열지 않고 쓸 수 있는 게 Ultra 모델을 고른 이유임
          + 아이폰 13 Pro에서 16 Pro로의 크기 증가는 정말 과함, 13 사이즈가 딱 적당했는데 케이스 판매를 위해 크기를 늘린 느낌임, 거의 패블릿 사이즈가 되어버렸음, 아이폰 6S와 비교하면 차이가 심함
     * 2019년 안드로이드 subreddit에서 이 주제가 나왔을 때 이미 다들 이 시장이 지속될 수 없다는 점에 동의했었음, Sony XZ1 Compact는 지금까지 써본 제품 중 최고였음, 앞으로는 더 나빠질 일만 남았다고 봄, 아이폰 출시 당시 젊었던 사람들이 이제 40대에 접어들고 노안 현상 때문에 점점 더 큰 폰이 실용적으로 바뀜, 결국 작은 디지털 디바이스 시장은 스마트워치가 맡게 될 것임, 관련 안드로이드 레딧 글, 가격도 지금 보면 정말 싸게 느껴짐
     * 소니 Xperia XZ2 Compact가 내가 써본 최고의 폰이었음, 최신 안드로이드를 돌리기엔 너무 느려져서 아쉽게 바꿈, 세상의 흐름에서 점점 소외되는 기분임, 모두들 주머니와 손이 거대해진 건가? 왜 이렇게 큰 폰을 원하게 된 걸까?
          + 대부분의 사람들이 사무실에서만 컴퓨터를 쓰고, 일상에선 스마트폰에 의존함, 그래서 조금 불편해도 큰 화면을 쓰려는 것임, 나는 오히려 작은 폰에 대한 수요가 너무 적어서 놀라움, 모니터가 있으니 굳이 큰 화면이 필요 없음
          + Sony Xperia 10 시리즈 사용 중임, 폭은 비슷하지만 길이만 더 긴 폰이라 불편하지 않고 배터리도 큼, 아쉽게도 곧 단종돼서 내년엔 삼성으로 넘어갈 듯함
          + 보통 내 스마트폰이 주력 디바이스가 된다는 사람이 늘고 있음, 나 역시 스마트폰과의 페이스타임이 랩탑보다 많음
          + 두 손으로 폰을 쓰는 사람 많음, 36인치 바지 입어도 아이폰 16 프로 맥스가 충분히 주머니에 넣어짐, 폰이 커진 주요 이유 중 하나는 나이가 들수록 작은 글씨가 잘 안 보이고, 영상 스트리밍도 폰에서 자주 보게 된다는 점임, 배터리 성능도 좋아짐
     * 아이폰 미니 사이즈의 신형이 간헐적으로라도 꼭 나왔으면 함, 몇 년에 한 번이어도 괜찮으니 최소한 라인업이 끊어지기 전에 후속작을 내줬으면 좋겠음
          + 나도 13미니 사용자로서 대체재가 나오기 전까진 바꿀 생각 없음, 한 손으로 상단 모서리를 터치할 수 있는 폰이 없다는 게 불만임
          + 얼마나 자주 나오는지는 중요하지 않고, 단종 전에 후속작을 꼭 내줬으면 함
     * 삼성 S10e가 아마 안드로이드의 정점이었음, 작으면서도 고성능, SD카드 및 3.5mm 잭 지원함, 주류가 아니어도 괜찮다면 작은 안드로이드폰을 찾을 수 있지만, smallphones 서브레딧에서 더욱 다양한 정보를 볼 수 있음
          + 나에겐 삼성 갤럭시 S5가 현대 하드웨어로 나온다면 최고임, 소형에 SD카드, 3.5mm 잭, 방수, 교체형 배터리, IR 블라스터까지 탑재됨, 당시엔 삼성 특유의 기믹(Air View, 손가락을 대지 않고 hover 동작 인식)을 많이 시도했음, 외관이 투박하다고들 하지만 케이스를 씌우면 신경 쓸 일 아님
          + 최근에는 Asus ZF10이 더 최신이면서도 괜찮다고 생각함, 불필요한 앱이 없고 DC 디밍, 튼튼해서 케이스가 필요 없을 정도임
          + 해당 smallphones 커뮤니티를 자주 들여다보지만 사실상 쓸만한 작은 폰이 나오지 않아 이번에도 모두 출시 여부만 기다리는 분위기임
          + S10e가 고장나서 새 폰으로 바꿈, S25는 약간 더 크지만 이 작은 차이도 체감상 크게 느껴짐, 그래도 120Hz는 마음에 듦
          + 방금 $200에 완전 새 것 같은 S10e를 중고로 샀고, 아주 만족함, 소형 폰 시장 확대엔 찬성이지만, 중고가치 못하는 폰에 $500 이상은 쓸 수 없음
"
"https://news.hada.io/topic?id=22022","우크라이나 해커들이 러시아 드론 제조사 IT 인프라를 파괴함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   우크라이나 해커들이 러시아 드론 제조사 IT 인프라를 파괴함

     * 우크라이나 해커 그룹이 군 정보기관과 협력해 러시아 주요 드론 제조사 Gaskar Integration의 IT 인프라를 마비시킴
     * 47TB 이상의 중요 데이터와 백업 자료가 삭제되어 핵심 사업 운영이 중단됨
     * 공장 내부 시스템과 회계, 생산 프로그램이 모두 작동 불능 상태에 놓임
     * 생산 공장 출입문까지 차단되어 직원들이 비상구를 이용해 출입함
     * 탈취된 데이터에는 직원 개인정보 및 드론 기술문서가 포함되어 우크라이나 국방부에 제공됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

주요 내용

     * 우크라이나 사이버 활동가들은 군 정보기관과 협력해 러시아군에 드론을 공급하는 최대 규모 제조사 중 하나인 Gaskar Integration의 네트워크와 서버 인프라를 공격함
     * 이 공격으로 47TB가 넘는 기술 정보와 10TB의 백업 자료까지 파괴됨, 해당 데이터에는 러시아와 중국 간 긴밀한 협력 정황도 포함되어 있음
     * 해킹으로 인해 인터넷, 생산 프로그램, 회계 프로그램 등 기업 내 모든 시스템이 마비되어 Gaskar의 연구개발센터 역시 정상 운영이 불가능해짐
     * 모든 드론 생산 공장 내 출입문이 차단되어 직원들이 비상구를 통해 퇴근 및 이동해야 하는 상황 발생
     * 탈취한 정보에는 직원 기밀 설문지, 드론 생산에 관한 기술 문서 등이 포함되어 있어 해당 정보가 우크라이나 국방부 산하 전문가들에게 전달됨

추가 배경

     * 군 정보국 소속 사이버 전문가들이 이전에도 러시아 철도 웹사이트를 강력한 공격으로 비활성화시킨 사례가 있음
     * 과거에도 러시아 Regiontransservice에 대한 공격을 성공적으로 수행하여 모든 서비스를 중단시킨 전력이 있음

        Hacker News 의견

     * 나는 집에서 소규모 랩을 운영함, 서비스는 30개 정도임
       어느 날 메인 디스크를 교체하면서 백업을 활용해 모든 것을 처음부터 다시 구축함, 한 시간만에 다시 살아남
       하지만 그 후 일주일 동안 여기저기 수정하고 왜 이렇게 설정했는지 기억도 안 나는 부분들까지 챙기느라 고생함
       이건 한 명이 관리하는 간단한 Docker 기반 랩이고 나도 IT에서 일함
       하지만 여러 사람들이 수년간 관리했던 전체 인프라를 처음부터 복구하는 건 정말 엄청난 일임
       나는 인근 병원이 랜섬웨어에 걸렸을 때 자원봉사로 복구를 도왔고, 그곳의 두 IT 직원은 어떻게 해야 할지 전혀 모름, 공식적 지원은 기대 이하였음
       대기업의 랜섬웨어 사고도 도왔는데 왜 시스템이 이렇게 구성됐는지, 기억하려 애쓰는 노력이 엄청 컸음
       문서화와 테스트가 되어 있었다고는 하지만 실전에서는 현실의 벽을 느끼게 됨
          + 우리 집이 경찰 급습을 당해 데스크탑, 랩탑, NAS, 하드 드라이브 등 1만불 어치 장비를 가져간 적이 있음
            전 직장에서 백업과 재해복구 계획을 담당했던 덕분에 준비를 해뒀었음
               o 현장에 미러링된 백업(경찰이 못 찾았거나 굳이 두고 간 듯)
               o 예전 장비들(나는 좀 수집하는 편, 이런 상황 대비 겸용)
               o 다중 오프사이트 백업
               o 셋업 문서화
                 하루 이틀 만에 대부분 복구하고 데이터 손실은 이틀 치 정도였음, 다행히 집에서 쓰는 거라 치명적이진 않았음
                 이 일을 겪고 다양한 구조적 개선을 하게 됐고, 앞으로 이런 사건 피해가 더 줄어들게 됨
                 (그리고 8개월 후 경찰이 죄 없다는 결론을 내고 내 장비를 돌려주라 했지만 아이들은 트라우마를 겪었음)
          + 문서화가 정말 중요한 이유임, 소프트웨어 아키텍처 차원에서도 마찬가지임
            몇 달만 지나면 내가 왜 이 선택을 했는지 기억이 안 나기 쉬움
            예를 들어 ""왜 ORM/SQL 툴로 Kysely를 쓰기로 했는지"", ""Deno/Bun을 왜 쓰는지"", ""폴더 구조를 기능단위로 한 이유"", ""라이브러리 포크 이유 및 관리법"", ""AWS/GCP/Azure/도커 선택 이유"", ""Kubernetes 배포판 고른 이유"", ""이 프로젝트를 왜 시작했는지/목표는 뭔지"" 등
            그래서 README.md에 # Decisions 섹션을 만들어 문서화함
            덕분에 스스로의 선택을 계속 의심하며 문서들을 끝없이 뒤지는 일에서 해방됨
          + 90년대 메인프레임은 너무 안정적이고 중복 구성이 잘 돼 있어서 10년 넘게 재부팅 안 하는 경우도 있었음, 커널까지 무중단 업그레이드도 됐었음
            그런데 어느 회사에 정전이 오고 백업 발전기도 실패해서 파워가 복구됐을 때 실제로 그 기계가 뭘 하고 있었는지, 시작은 어떻게 해야 하는지 파악하는 데 몇 달이 걸림
            그 후 대부분 회사가 6개월에 한 번씩 메인프레임을 일부러 리부팅해서 재기동 테스트를 하게 됨
          + 현대 IT 관행에서는 재해 복구가 거의 고려되지 않음
            엄격하게 백업을 하는 조직도 실제로 복구 테스트를 하는 경우는 드묾
            인력이 부족하다 보니 빠르게 뭔가 쌓아 올릴 뿐임
            쉽게 재구성 가능한 인프라 구조를 설계하는 건 그냥 설치하는 것보다 두 배의 노력이 들어가는 일임
          + 병원 랜섬웨어 자원봉사 이야기 궁금함
            헬스케어 IT에서는 접근 권한을 엄청 까다롭게 다뤄서, 예전에는 PHI 교육이나 신원조회 없이 시스템 접근이 불가능했는데, 비상 상황이니 신속하게 임시 온보딩 절차를 밟았던 것인지, 혹은 병원 내의 인맥을 통해 자원봉사를 하게 된 것인지 묻고 싶음
     * 나는 독일 회사에서 근무 중임
       생산 관리가 3개월 전 계획을 엑셀로 출력해 진행 중임
       ERP 시스템 마이그레이션에 실패했는데, 아무도 해결법을 모름
       생산관리부는 이 사실을 숨기고 엔지니어링 부서에 말도 안 함
       이 상황이 수년간 이어질 듯, 컨설턴트들은 먹고 사는 시스템임
       제조에 IT 인프라가 필수는 아니라는 걸 증명함, 없어도 되는, 그냥 있으면 좋은 거임
          + 90년대 말~2000년대 초 덴마크 국방부에서 SAP로 만든 신규 조달 시스템인 DeMars를 도입하려 했음
            조달 업무 하던 내 친구는 DeMars 도입 직전 자기 담당 물자를 엄청나게 대량 주문했음, 사기 혐의로 불려간 적도 있음
            DeMars에 불신이 커서 재고 보유가 중요하다고 판단했기 때문임
            실제로 DeMars가 도입되자 조달 업무가 1년간 사실상 멈춤
            결국 내 친구가 담당하던 품목만 새 시스템 도입 기간 내내 재고가 유지됐음
          + 90년대 후반 제조사에서 펌웨어 개발자로 근무했었음
            아직도 모든 걸 종이에 기록하던 시절이었음
            사내에서 Oracle 기반 ERP를 성공적으로 구축하고 모두 기뻐했지만, 6개월 후 누군가가 기계실 벽을 지게차로 들이받아 UPS에서 불이 나서 Oracle 서버 등 장비 세 랙이 전소됨
            모두 시스템을 신뢰하지 않아 여전히 종이로 기록했기 때문에, 6년 후 퇴사할 때까지도 종이+엑셀 보고로 일함
            결과적으로 종이 기반 방법이 지게차에도 강한 것으로 증명됨
          + Excel은 많은 사무직 직원들이 직관적으로 이해하고 수정할 수 있음
            IT 인프라 대부분에 이런 접근성 높은 기능이 도입된다면 훨씬 더 실용적일 것 같음
          + 한편, IT 자동화가 생산에 완전히 자리 잡고 이전 매뉴얼 방식에 익숙한 인력이 없을 때는 수작업 방식으로 되돌아가는 게 정말 어려울 수 있음
            주문/워크플로 복잡도에 따라 다르겠지만 말임
          + 소프트웨어 없이는 드론도 쓸모 없음
            재고를 외우고 있다면 수동 조작용 쿼드콥터 조립 정도는 가능하겠지만, 3D 프린팅 파트 생산, 안정적인 비행, 자율 운항, 감시, 기타 고급 활용은 불가능함
            원격제어도 힘들 것임
     * 우크라이나에서의 사이버전이 새로운 정점에 도달하는 중임, 단순한 사이버 공격을 넘어서고 있음
       이번에 공격받은 러시아 드론 제조 시설처럼, 드론이 이 전쟁의 양상을 바꾼 핵심임
       드론이 정찰, 방해, 탄약 요격 등 혁신을 가져옴
       소재 대비 파괴력이 크고, 영상 인식 기술 발전으로 신호 방해에도 일부가 작동함
       첩보 영화 같은 현실이 벌어지는 중임
       우크라이나가 비대칭 전쟁의 명수임을 보여줌
       장거리 폭격기 파괴, 드론 생산 거점 마비 등으로 러시아의 주력 전력을 흔듦
       전쟁이 어떻게 끝날지는 모르겠지만 우크라이나의 저항은 계속될 것이 분명함
          + 소설 Ministry of the Future에서는 드론이 너무 발전해 결국 누구도 안전하지 않은 세상에서 전통적 전쟁은 의미 없어지는 미래가 그려짐
            작은 집단도 전 세계 어디서든 누구든 암살 가능해짐
            흥미롭긴 한데, 이야기는 약해서 책 자체는 별로 추천하고 싶진 않음
          + “전자 신호 방해에도 작동하는 드론”에 대해, 요즘은 전자파를 쓰지 않고 광섬유 케이블로 제어되는 드론도 있음, 더욱 무서운 현실임
          + 이번 전쟁에서 드론이 중요한 역할을 하는 게 앞으로 다른 전쟁들에도 적용될지는 지켜봐야 할 문제임
            러시아가 엄청난 인명 손실을 감수하며 조금씩 진격하는 특수 상황이 FPV 드론의 가치를 키우고 있음
            대부분 국가에서는 그런 손실 감수를 안 할 테니, 이 형태가 전쟁의 표준이 되진 않을 것 같음
            저렴한 장거리 제트 드론이 오히려 더 중요한 역할을 하게 될 수도 있음
          + 기사에 나온 정보에는 가정이 많이 섞여 있음
            한쪽 이야기만 듣고 있을 뿐이며, 선전 가치 때문에 과장될 수 있음
            버전 관리가 제대로 돼 있고, 각 개발자가 코드와 CAD 파일을 로컬로 복사해두는 게 일반적임
            이메일과 오피스 파일 등은 유실됐을 수 있지만, 그게 치명적 손실은 아닐 가능성 높음
            웹사이트도 그대로 작동 중임
            이번에 공격받은 이 회사는 드론 커뮤니티에서도 유명한 곳이 아니라서 거대 모델 생산 중단 같은 일은 아닌 듯함
            버전 관리 같은 상식들을 모르지 않을 텐데, 댓글 작성 스타일이 ChatGPT가 쓴 것 같기도 하다는 생각도 듦
     * 나는 스위스의 중견 회사에서 일하고 있음
       자체 ERP를 개발 중인데 스택이 악몽 그 자체임
       ‘혼란을 통한 보안’이라고 스스로 부름
       공격자가 침입해도 빠져나오지 못할 것
       90%의 코드가 파괴돼도 서비스엔 영향 없음, 왜냐하면 95%는 이미 쓸모없는 코드임
          + 대기업용 MRP 시스템을 직접 개발해본 경험이 있어서 이 방식이 어떻게 될지 궁금함
            나는 보통 권장 보안/재해복구 방식에 OTP-해시 기반 키 인증 레이어까지 추가함
            내가 좀 심한 편이라 생각했는데, 이 시스템은 거의 세기말 생존 시나리오처럼 느껴짐
          + 이건 진화 과정에서 탄생한 복원성 같다는 인상임
          + 진짜 세상의 ICE 장벽 같아서 웃김
          + 무섭기도 하고 감탄스러운 면도 있긴 함
     * 대부분 기업은 회사 내 거의 모든 데이터 저장소가 완전 삭제되고, 0부터 다시 배포해야 하는 상황을 명확히 대비하지 않음
       실제로 0부터 복구를 직접 해본 적이 없다면 배포 의존성에 순환 고리가 있을 가능성이 높음
       Jenkins/Puppet/Ansible로 config pusher를 배포하다가 어느 순간 Jenkins 자체도 config pusher 의존성이 생겨서, 그냥 차례대로 구축할 수 없게 됨, 과거부터 모든 변경 내역을 다시 따라가야 하는 상황이 발생함
          + IT에는 거의 모든 부분에 순환 의존성이 존재함
            SSO가 거의 모든 시스템의 의존성이 되고, SSO 내 네트워크와 각종 시스템 관리에도 순환 구조가 생김
            완전 새로 부팅하는 것은 언제나 어렵고 시간도 많이 소요됨
            완전히 분리된 이중 인프라를 구축하지 않는 이상, 완벽하게 이 문제를 해결하는 건 사실상 불가능함
          + 내가 아는 한 회사도 1년 전에 이런 일을 겪음
            모든 것이 의존하던 메인 스토리지 클러스터가 죽었음
            결국 Dev 노트북에서 모든 것을 재배포해서 복구함
          + black start(완전 초기화 복구)는 엄청 어려운 문제임
            Facebook도 과거 데이터센터 도어락을 드릴로 직접 뚫고 들어가 복구한 적 있음
          + 이런 상황에서 어떻게 해야 복구할 수 있을지 궁금함
            종이로 된 문서라도 남아 있다면 그걸로 부트스트랩이 가능한지, 아니면 그것마저 사라졌다고 가정해야 하는지 의문임
          + 건설업도 비슷한 문제를 겪음
            제품 수명이 50년 이상, 심하면 수백 년인데, 30년 전 만든 설계 파일을 지금 파일 포맷 호환성 문제로 열 수 없는 경우가 많음
            디지털화 얘기가 수십 년 전부터 나왔지만, 결국 오래된 2D 도면(혹은 요즘은 ‘디지털 종이’라 부르는 PDF)이 미래에 도움이 될 수도 있음
            진짜 종이 사용은 점점 줄지만, 파일 호환성 이슈로 결국엔 종이가 유용해질 수 있겠다는 생각임
     * 기사 제목에는 공격자들이 ‘사이버 액티비스트’라고 되고 본문에는 ‘사이버 범죄자’로 불림
       과거 범선 시기 때 준공적인 해적 ‘사략선’이나 ‘마르크 증서’처럼 경계선상에서 행동하는 이들이 떠오름
       4세대 전쟁론에서는 민간-군사 경계가 흐려지는 것이 특징이라고 했음
       교전 규칙이 점점 모호해짐
          + 러시아가 매일 드론으로 민간인을 죽이는 상황임
            이게 그레이존 하이브리드 전쟁 같은 애매한 영역이 아니라 단지 시민들이 자기 이웃이 드론에 희생당하지 않게 하는 것임
          + 이건 번역 이슈 같음
            해당 사이트가 우크라이나를 지지하는 색이 강해, 해커를 부정적으로 보이게 만들고 싶지 않아 ‘cyber criminal’을 단순히 ‘해커’ 의미로 썼을 수 있음
          + 실제로는 우크라이나 군에서 조직적으로 움직이는 게 제일 타당할 것 같음
            그래서 범죄자가 아니라고 보는 게 맞음
          + 롤린 후드 비슷한 상황임
            누구에게는 영웅, 누구에게는 범죄자
            아마 기사 자체가 여러 기사 조합이라 용어가 뒤섞였을 듯
            사이버전에서 편을 나눠 말할 별도의 용어가 있다면 좋겠음
            “사이버액티비스트”는 그냥 온라인 시위대 느낌이라, 영화에서 쓰인 뻔한 단어 대신 “사이버병사”나 “네트워크밀리샤” 같은 게 쓰였으면 함
     * 기사 사진 날짜가 유닉스 에폭 시작일과 하루 차이 있는 거 보고 혼자 재밌어함
     * 해당 웹사이트가 매우 독특함
       러시아 정부가 차단해 TLS 에러 뜨고, 그걸 우회해도 Cloudflare의 ""차단됨"" 페이지, VPN을 써야 기사 원문(러시아어)까지 접근 가능함
          + 링크된 페이지는 영어지만, 러시아 현지인은 이 사이트의 러시아어 버전이 타겟이 아니었을 수 있음
            러시아에서는 언어 문제에 민감하지만, 우크라이나에서는 실제로 러시아어도 많이 쓰이고 러시아어 기사도 출판됨
          + archive.today, archive.org(인터넷 아카이브) 등 아카이브 사이트를 꼭 활용하길 권장함
            아카이브 링크도 누군가 최근에 저장해둠
          + 이건 해당 웹사이트의 문제가 아니라 정부 차단, 혹은 CloudFlare 쪽 이슈 때문일 수 있음
            TLS 에러의 근본 원인 때문에 Cloudflare가 차단하는 것일 수도 있음
     * 양측 모두 드론의 펌웨어에 대해 걱정을 하고 있는지 궁금함
       적군이 쓰는 드론에 변조된 펌웨어를 몰래 심는 게 전략적으로 가치가 있을 것 같음
          + 흥미롭지만 리스크가 큼(쉽게 들통나고 전체 작전을 무력화시킬 수 있음)
            결국 강경한 방식이 가장 합리적이라고 봄
          + 드론은 보통 임무 직전에 펌웨어를 플래시함
          + 실제로는 공장 문 닫는 것보다 드론들이 슬쩍 다른 행동(예: 발사 시점에 본진 공격이나 원격 통제)을 하게 하는 게 더 효과적일 수 있음
          + 한 가지 재밌는 전략은 드론 SD카드에 바이러스를 심어둬서, 드론이 적진에 떨어져 적군이 컴퓨터에 꽂으면 그 컴퓨터가 감염되는 케이스도 있다고 들음
     * “우크라이나 사이버 활동가들이 군 정보기관과 협력…”
       즉, 해외 정보기관에서 신호만 받아서 직접적 사이버전이 아니라는 의미임
          + “해외 정보기관이 신호만 줘서 직접적 사이버전이 아니다”는 의견에 대해
            러시아 정보기관은 이미 NATO 국가를 직접 공격하고 있음, 변명할 여지도 거의 없음
          + 우크라이나와 러시아 사이에서 이미 수년간 전쟁 중이기 때문에 굳이 그럴듯한 변명(deniability)은 필요 없는 상황임
          + 외국 정보기관이 무엇을 의미하는지 궁금하고, 사실 전 세계적으로 상시 공격이 오가기에, 순진하게 생각하지 말라는 의견임
          + 기사에 “외국 정보기관”이 나오지 않는다고 지적함
          + 우크라이나 군사정보기관이라고 명시함
"
"https://news.hada.io/topic?id=22031","Altermagnets - 거의 한 세기만에 발견된 첫 번째 새로운 종류의 자석","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Altermagnets - 거의 한 세기만에 발견된 첫 번째 새로운 종류의 자석

     * 체코 출신 물리학자 Libor Šmejkal이 미술작품에서 영감을 받아 새로운 형태의 자기(altermagnetism) 를 이론적으로 예측함
     * 기존에는 강자성체(ferromagnetism) 와 반강자성체(antiferromagnetism) 두 가지 자기만 알려져 있었으나, 제3의 자기 형태인 알터마그넷이 실험적으로 확인됨
     * 알터마그넷은 총합 자기장은 0이지만, 전자 스핀 분리(spin-splitting) 를 유도할 수 있어 스핀트로닉스 기술의 한계를 극복할 수 있음
     * 실제로 망간 텔루루화물(MnTe), 루테늄 디옥사이드 등에서 알터마그네틱 현상이 실험으로 입증되었으며, 200개 이상 후보 물질이 이론적으로 제시됨
     * 연구팀은 나아가 반(反)알터마그네틱(antialtermagnetic) 이라는 제4의 자기 형태도 이론적으로 예측하며 자기의 세계를 넓혀가고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Magnetism의 역사와 발전

     * 자기는 고대 그리스 시절부터 알려졌으며, 오늘날 발전기, 스마트폰, 병원 스캐너 등 핵심 기술에 사용되고 있음
     * 고전적 자기 개념은 강자성체(모든 스핀 방향이 같아 자력이 형성되는 구조) 와 반강자성체(스핀 방향이 상쇄되어 겉보기 자력이 없는 구조) 두 가지였음
     * 2022년, Šmejkal은 이 모델로 설명되지 않는 현상을 바탕으로 ‘알터마그네틱’ 상태를 이론화함

Šmejkal의 아이디어와 Escher의 대칭성

     * M.C. Escher의 Horseman 작품에서 보이는 반복 대칭 패턴에서 영감을 받아 자기 대칭을 새롭게 해석
     * 기존 반강자성체와 비슷하게 스핀은 번갈아 방향을 바꾸지만, 90도 회전된 방향의 자기 모멘트가 나타나며 결과적으로 스핀 분리 현상이 발생함
     * 이로 인해 전통적으로 불가능했던 구조 속에서도 양방향 전자 스핀의 분리가 가능함

알터마그넷의 실험적 입증

     * 2024년, 스위스 PSI 연구소의 Juraj Krempaský 팀이 망간 텔루루화물(MnTe) 에서 알터마그넷 현상을 관측
     * 전자 움직임을 추적한 결과, Šmejkal의 이론과 높은 정합성을 보여줌
     * 이어서 루테늄 디옥사이드 등에서도 알터마그넷 가능성이 확인됨

스핀트로닉스와 알터마그넷의 가능성

     * 스핀트로닉스(spintronics) 는 전자 스핀을 활용해 정보를 저장하고 처리하는 차세대 기술
     * 기존에는 강자성체만이 스핀 분리를 제공할 수 있어 소형화와 집적화에 한계
     * 알터마그넷은 자력은 0이지만 스핀 분리 가능, 간섭 없음, 저전력, 소형화 가능성 등에서 이상적인 특성 보유

새로운 물질 개발과 상용화 가능성

     * 기존 반강자성체에 기계적 압축(compressive strain) 을 가하거나, 이종소재 적층(sandwich structure) 으로 대칭성을 교란해 알터마그넷 상태를 유도
     * 예: 압축을 가한 rhenium dioxide, 다층 구조로 만든 적층 반강자성체
     * 다만 인위적 방식은 실용성 부족 가능성이 있으며, 자연계에서 알터마그넷성을 갖는 물질 탐색이 유망
     * Šmejkal 팀은 200개 이상의 후보 물질을 이론적으로 도출함

상용화를 위한 다음 단계

     * Oliver Amin 연구팀은 MnTe의 자기 구조를 가열과 냉각을 통해 제어 가능함을 시연
     * 이는 스핀트로닉스를 위한 실용적 소재 구현의 초기 단계로 평가됨
     * MnTe는 이미 20년 이상 연구된 물질로, 고순도 합성 및 실험에 유리

제4의 자기 형태: 반알터마그네틱 (Antialtermagnetism)

     * Šmejkal은 알터마그넷을 넘어 지그재그형 스핀 대칭 구조를 가지는 반알터마그넷을 이론화
     * 전자 스핀들이 대칭적으로 배열되어 총합 자력은 없지만, 전자 이동 경로에 변화를 주어 스핀 분리를 유도함
     * 아직 논문은 동료평가 전 단계이나, 새로운 자기 현상의 가능성을 제시함

결론

     * 알터마그넷의 발견은 자기의 개념을 확장하고, 스핀트로닉스의 실용화를 가속화할 수 있는 핵심 전환점
     * 향후 10년 내 상용화 가능한 신소재로 이어질 가능성이 크며, 연구가 활발히 진행 중임
     * Escher의 대칭성에서 출발한 이 연구는, 미술과 수학, 물리학이 만난 대표적 사례로 주목받고 있음

        Hacker News 의견

     * archive.ph 링크
     * 내가 이해한 바로는, 이 기술의 진정한 장점은 솔리드 스테이트 방식의 자기 저장장치라고 생각함
       기존 자기 저장장치는 자기장을 만들지만, 이 새로운 알터마그넷 소재는 자기장 생성 없이 외부 자기장에 반응함
       그래서 장치들을 아주 조밀하게 배치할 수 있고, 간섭을 걱정할 필요가 없음
       약한 전기 펄스로 비트의 0과 1을 읽고, 강한 펄스로 비트를 뒤집는 구조임
       원자 자체를 뒤집는 것이기 때문에 구조를 파괴하거나 전하를 넣지 않아, 수명도 길고 읽기/쓰기 사이클도 거의 무한에 가까울 것으로 추정함
       일반 실리콘 제조 공정과 호환 가능할 것으로 보고 있음
       다만, 읽기 구조끼리 얼마나 촘촘하게 둘 수 있을지가 기술적 관건임
          + 약한 전기 펄스로 비트 상태를 감지하고, 강한 펄스로 뒤집는다는 설명이 정말 훌륭하게 요점을 집어냄
            Feynman 스타일의 통찰로 한 문장으로 완벽하게 정리한 점이 인상적임
          + 이런 저장장치가 있다면 솔리드 스테이트 메모리뿐 아니라, Hall effect 기반 산업용 센서 전반에서 해상도와 노이즈 면역성이 대폭 향상될 거라 생각함
          + 사실 기존의 ""일반"" 자성 소재도 자기장 방향을 전환할 수 있다는 점을 이 논문에서 확인 가능함
     * 기사에서 ""Confirming that altermagnets exist"" 섹션이 실제 용도 설명을 잘 다룸
       전통적으로 스핀 기반 고밀도 정보 저장은 스핀이 자연스럽게 정렬된 소재(보통 강자성체)만 사용해 왔음
       문제는, 강자성체는 거대한 자기장을 동반하여 실제 활용에 큰 걸림돌이 됨
       새로운 알터마그넷은 스핀이 잘 배열되어 있으면서 각 원자 단
"
"https://news.hada.io/topic?id=21983","1.1.1.1 장애로 DNS 응답 불가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         1.1.1.1 장애로 DNS 응답 불가

     * 1.1.1.1 DNS 사용자(공유기, OS에서 설정)쪽에서 서버 IP를 찾을 수 없어 인터넷 연결에 제한적
     * CloudFlare를 DNS제공자로 사용하는 경우에 문제는 없음

   방금전 IP로 분명 통신은 되면서 파이어폭스만 인터넷 사용가능해서 찾아보다가 클플 DNS문제인걸 확인했네요.

   한국시간 7시 0분쯤 부터 한 50분쯤 중단 상태였다가 지금은 잘 작동하네요
   CMD> nslookup news.hada.io 1.1.1.1

   저도 DNS서버에 액세스할 수 없다는 안드로이드 푸시알림이 계속 떴어요.
   잠시 구글DNS로 대피했습니다.
   https://developers.google.com/speed/public-dns/…
"
"https://news.hada.io/topic?id=22088","자율 에이전트에 대한 "과대광고"와 실제 프로덕션에서 작동하는 AI 에이전트의 차이점","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            자율 에이전트에 대한 ""과대광고""와 실제 프로덕션에서 작동하는 AI 에이전트의 차이점

     * AI 에이전트 붐이 2025년에 크게 다가올 것이라는 기대와 달리, 실제 프로덕션 환경에서는 현실적인 한계가 존재함
     * 에러 누적과 토큰 비용 문제로 인해, 다단계 워크플로우를 완전 자동화하는 것은 현재 불가능함
     * 대부분의 성공적인 에이전트 시스템은 엄격하게 제한된 도메인과 인간의 승인 또는 검증 과정을 필수로 함
     * 진짜 어려운 점은 AI 성능 자체가 아니라, 에이전트가 잘 사용할 수 있는 도구 및 피드백 시스템 설계임
     * 2025년에 완전 자율 에이전트를 전면에 내세운 스타트업/기업은 실제 도입과 확장 과정에서 큰 장애물을 맞이할 것으로 전망됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

내가 2025년에 AI 에이전트가 등장하지 않을 것이라고 베팅하는 이유 (AI 에이전트를 개발하고 있음에도 불구하고)

     * 2025년이 AI 에이전트의 해가 될 것이라는 과대광고가 만연함
     * 필자는 지난 1년간 실제 프로덕션 환경에서 동작하는 다양한 AI 에이전트 시스템을 직접 구축함
     * 직접적인 실무 경험에서 나온 이유로, 시장의 ""에이전트 혁명""에 회의적 입장을 가짐

다양한 에이전트 시스템 구축 경험

     * 개발 에이전트: 자연어로부터 React 컴포넌트 생성, 레거시 코드 리팩터링, API 문서 자동관리, 명세 기반 함수 생성 등
     * 데이터 & 인프라 에이전트: 복잡 쿼리와 마이그레이션 처리, 멀티 클라우드 DevOps 자동화
     * 품질 & 프로세스 에이전트: 린트 자동수정, 테스트 코드 생성, 코드리뷰 및 상세 Pull Request 자동화
     * 이들 시스템은 실제 가치와 시간을 절감해주나, 이 경험이 과대광고에 대한 비판적 시각의 배경임

핵심 요약: AI 에이전트의 세 가지 냉정한 진실

    1. 에러율 누적: 단계가 늘수록 성공률이 기하급수적으로 하락함. 프로덕션 기준(99.9% 이상) 충족이 어려움
    2. 컨텍스트 윈도우와 비용: 대화가 길어질수록 토큰 비용이 자승적으로 증가함에 따라 경제성이 무너짐
    3. 도구 및 피드백 설계의 어려움: 기술적 한계가 아니라, 에이전트 활용 가능한 도구 및 피드백 시스템 설계가 가장 큰 도전임

에러 누적에 대한 수학적 현실

     * 다단계 자율 워크플로우는 에러 누적으로 인해 실제 운영규모에서 실현 불가
     * 예를 들어 각 단계에서 95% 신뢰도라 해도, 20단계면 최종 성공률 36% 에 불과(프로덕션의 요구: 99.9% 이상)
     * 높은 신뢰도를 설령 가정해도, 단계가 늘어날수록 실패확률 대폭 증가
     * 실무에서는 모든 프로세스를 완전 자동화하지 않고, 각 단계를 독립 검증 및 롤백 지점과 인간 확인을 두는 구조로 설계함
     * 성공적인 에이전트 시스템 패턴: 명확히 제한된 컨텍스트, 검증 가능한 작업, 중요한 지점에 인간 결정 관여

토큰 비용 구조와 경제적 한계

     * 컨텍스트 윈도우 및 대화를 유지하기 위한 토큰 비용이 시스템 확장 시 비경제적 현실로 부상
     * 대화식 에이전트는 매번 모든 대화 기록을 처리해야 하므로, 회차가 늘수록 비용이 제곱으로 급상승
     * 100회에 걸친 대화는 토큰 비용만 50~100달러 소요, 대량 사용자 적용 시 경제성 붕괴
     * 반면, Stateless(상태 비저장) 방식의 단일 슬롯, 맥락 불필요한 기능 생성 에이전트는 비용과 확장성 측면에서 유리함
     * 가장 성공적인 프로덕션 에이전트는 ""대화식""보다 ""명확한 목적의 스마트 도구""에 가까움

도구 설계와 피드백 시스템의 벽

     * 생산성 높은 에이전트 개발의 진짜 난관은, 기존 팀들이 과소평가하는 도구 설계 능력
     * 툴콜 자체의 정확도는 높아졌지만, 복잡 상태·결과를 효과적으로 요약해 에이전트에 피드백하는 설계가 관건
     * 예시:
          + 작업이 부분적으로 성공했을 때, 얼마만큼의 정보와 어떤 요약이 필요한지 판단 필요
          + 예를 들어 쿼리 결과가 10,000건이라면 ""성공, 1만 건, 앞 5건만""처럼 상태 파악용 추상화 설계가 요구됨
          + 툴 실패 시 회복 정보양 조절 및 맥락 오염 방지 필요
     * 실제로 성공한 데이터베이스 에이전트의 핵심: 에이전트가 실질적으로 의사결정 가능한 구조화된 피드백 제공
     * 현실에서 AI가 하는 일은 약 30%, 나머지 70%는 도구 피드백·복구·컨텍스트 관리 등의 전통 엔지니어링 역량이 차지함

실제 시스템 통합의 한계

     * 신뢰도와 비용 문제가 해결되어도, 현실 세계와의 통합 문제가 또 다른 벽으로 작용
     * 현실 조직 시스템은 일관된 API가 아니며, 레거시 특성, 예외적 오류, 변화하는 인증, 가변적인 제한, 준법 규정 등 예측 불가한 복잡성 내재
     * 실제 데이터베이스 에이전트는 연결 풀 관리, 트랜잭션 롤백, 읽기 전용 복제본 존중, 쿼리 타임아웃, 로그 등 전통적 프로그래밍이 필수
     * ""AI가 모든 스택을 완전 자율적으로 통합""한다는 약속은 실제 구축 시 현실 벽에 부딪힘

실제로 잘 작동하는 방식 패턴

     * 성공적인 에이전트 시스템의 공통 원리
         1. UI 생성 에이전트: 사용자 경험은 인간이 최종 검토 (AI는 자연어→React 변환 복잡성만 담당)
         2. 데이터베이스 에이전트: 파괴적 작업은 항상 인간이 확인 (AI는 SQL 변환만, 데이터 보존 통제는 인간)
         3. 함수 생성 에이전트: 명확한 명세 내 한정 동작 (상태·부작용·복잡한 통합 없음)
         4. DevOps 자동화: 인프라 코드 생성은 AI, 배포·버전관리·복구는 기존 파이프라인
         5. CI/CD 에이전트: 각 단계는 명확한 성공 기준과 롤백 메커니즘으로 설계
     * 패턴 요약: AI는 복잡성 처리, 인간이 통제권 유지, 신뢰성은 전통 엔지니어링이 확보

시장 전망 및 예측

     * 완전 자율 에이전트 앞세운 벤처 스타트업은 수익성 문제에 가장 먼저 부딪힐 것
     * 5단계 워크플로우에선 데모가 훌륭해도, 실제 기업은 20단계 이상 요구하며 수학적 한계에 직면
     * 기존 소프트웨어에 AI 에이전트만 단순 추가한 기업은 실제적 통합의 미비로 채택 정체 가능성 높음
     * 진짜 승리자: 명확히 제한된 도메인 내에서, AI를 어려운 작업에만 적용하고 중요 결정에는 인간·경계조건 부여하는 팀
     * 시장은 ""데모는 잘 되는 AI""와 ""진짜 신뢰성 있는 AI""의 차이를 값비싼 경험을 통해 배우게 될 전망

바람직한 에이전트 시스템 구축 원칙

     * 명확한 경계 설정: 에이전트의 역할과 인간/기존 시스템 핸드오프 구간 명확히 정의
     * 실패 대비 설계: AI 오류 발생 시 롤백·복구 구조 설계 필수
     * 경제성 검증: 인터랙션 단가 및 규모 확장 대비 구조 설계 (상태 저장보다 상태 비저장이 경제적)
     * 자율성보다 신뢰성 우선: 일관된 작동이 가끔 마법 부리는 시스템보다 신뢰성에서 사용자 신뢰 획득
     * 견고한 기반 위 구축: 어려운 부분(의도 해석, 생성 등)만 AI에 할당, 나머지(실행, 에러 처리 등)는 전통 SW에 맡김

실전 경험에서 얻은 진짜 교훈

     * ""데모로 작동""과 ""실제 대규모 운영"" 간 괴리감은 매우 큼
     * 에이전트 신뢰성, 비용 최적화, 통합 복잡성 등은 아직도 산업 전반에서 풀리지 않은 주요 문제
     * 실제 구축 경험과 정직한 경험 공유가 업계 발전의 핵심
     * 더 많은 실전 경험자들이 합리적인 방법론과 현실적 실패 사례를 논의할수록 전체적인 성공 가능성 제고

        Hacker News 의견

     * Amazon의 AI 프로덕션 엔지니어와 대화해 본 경험이 있음, 그분이 말하길 현재 어떤 회사도 고객과 직접 대화하는 곳에 생성형 AI만 사용하는 경우가 전혀 없다고 함, 모든 자동응답은 예전의 비생성형 ""구식"" 기술을 사용한다고 함, 생성형 AI의 신뢰성 문제가 기업의 명성에 걸 stake를 맡길 수 없을 만큼 큼
          + 예전에는 ""구식 AI"" 상징적 기법과 전통적인 머신러닝을 결합하는 에이전트에 관심이 많았음, 하지만 주로 프리-트랜스포머 신경망에서 일하게 됐음, 결국엔 항상 인간이 개입하는 시스템을 먼저 만들고 평가 및 훈련 데이터를 수집함, 그런 뒤 시스템이 업무 일부를 맡아서 나머지 품질까지 같이 향상시키는 흐름임, 특히 '주관적' 작업에서는 심볼릭 시스템도 반드시 평가해야 함, 만약 시스템을 훈련시켜야 한다면 평가를 못 피함
          + 실제로 많은 테크 기업이 이미 생성형 AI를 실시간 챗봇 고객지원에 도입 중임, Sonder와 Wealthsimple 같은 곳을 알고 있음, LLM이 쿼리에 답을 못 할 경우엔 대화를 바로 인간 상담사에게 넘김
     * 아직 논의되지 않은 점이 맥락 창(context window)에 대한 것임, 인간은 전문 분야에서 사실상 ""거의 무한대""에 가까운 맥락을 다룰 수 있음, 모델은 더 크고 다양한 학습 데이터로 어느 정도 한계를 극복할 수 있지만 진정한 해결책은 아니라는 생각임, 현재는 사람들이 자신만의 맥락을 프롬프트에 담아 압축해야 하기에 영어처럼 유연한 언어에서는 공학보다는 마법주문 외우기나 추측 같은 느낌임, 결정론적 방식 대신 데이터의 많은 부분을 잃는다고 느끼는 중임
          + 인간은 ""맥락""과 ""가중치""가 나뉘어 고정적으로 구분되어 있지 않음, 시간에 따라 경험과 결과가 내 ""가중치"" 자체를 계속 바꾸는데 LLM은 아키텍처상 가중치가 읽기 전용이라 불가능함
          + 인간이 정말 그렇게 거대한 맥락 창을 가지고 있냐는 회의가 있음, 나는 내 복잡한 문제 해결 시 인간 고유의 ""맥락 창"" 한계에 자주 부딪힘, 실제로 그런 예시가 있는지 궁금함
     * 나는 AI툴 사용 경험이 대체로 긍정적이었음, 쉬어야 할 때 소규모 작업을 맡기거나, 업무를 정리 및 시동시키는 데 큰 도움이 됨, 하지만 비용 이슈가 금방 생김, 예를 들어 Claude Code를 큰 코드베이스에 쓰면 1~2시간에 $25 정도 소요됨, 자동화된 교정까지 붙이면 $50/hr까지 오름, 속도, 정확도, 비용 트레이드오프가 있음, 요즘 나온 Agent들도 그 삼각형의 지점이 아직 불명확해서 여러 실험이 흥미롭긴 해도 여전히 리스크가 있다고 생각함
          + 약간 냉소적으로 보자면, LLM이 끊임없이 자기 자신을 리프롬프트하면서 오류를 고치는 구조, 그리고 ""RAG 필요 없음! 그냥 1m 토큰 맥락창에 모든 코드를 다 던져넣어라""라는 접근이 결국 '토큰당 과금' 비즈니스 모델에 딱 맞는 프레임임
          + 요즘 고민하는 아이디어는 여러 개의 커밋 초안을 처음부터 AI가 만들어내고, 이 결과를 사람이 직접 혹은 자동화된 방식으로 필터링 및 수동 다듬기 하는 구조임, 큰 작업일수록 초기의 작은 편차가 전체 결과를 망칠 확률이 높음, 그래서 현재 SOTA로도 에이전트들이 여러 안을 병렬로 시도하게 하면 수동 리팩터링하는 시간이 줄어듦, 관련 절차에 대해 GitHub에 글 남긴 적이 있음
          + 구독 서비스냐고 질문하고 싶음
     * 인간의 다단계 워크플로우에는 보통 검증 체크포인트가 있음, 인간도 99% 이상 정확하진 않기 때문임, 향후 에이전트도 출력에 검증 절차를 설계하고 다음 단계로 나아가기 전에 이 과정을 통과하도록 학습될 것임, 사전에 ""여기만큼은 99% 이상 정확해야 한다"" 같은 위험도 사전 평가도 할 수 있을 것임
          + Claude Code는 작업을 진행하기 전마다 계속 멈춰서 사용자에게 진행할지 물으며, 제안된 변경 사항도 미리 보여줌, 토큰 낭비와 비효율적인 작업을 막는 데 효과적임
          + 많은 애플리케이션도 이런 구조에 맞춰 재설계가 필요함, 내 생각에 마이크로서비스 아키텍처가 LLM과 궁합이 좋아서 다시 유행할 것 같음
     * ""진짜 문제는 AI의 능력이 아니라, 에이전트가 실제로 효과적으로 쓸 수 있는 툴과 피드백 시스템을 설계하는 것""이라는 데 동의함, 시장이 어떻게 받아들일지 확신이 없어 눈팅만 하다가 에이전트 만들기에 특화된 아주 작은 스타트업에 합류함, 5개월 만에 회의적→동조→확신으로 바뀜, 주제 범위를 아주 좁게 잡고, 모델이 일하기 위한 툴링에 집중하면 높은 완수율을 경험했음, 비결정론적 특성을 꺼려하는 경향이 있지만, 뛰어난 툴링과 점점 더 좁은 스코프만 있으면 현실적으로 꽤 쓸 만함, 툴링 자체가 어렵게 느껴지지만 괜찮게 해결 가능하다고 봄, 미래를 낙관함
     * 다 해결 가능한 문제라고 생각함, 다만 빠른 ARR 확보 경쟁 때문에 많은 스타트업이 이런 문제에 집중하지 않음, 에이전트가 약속만큼 쓸모없다는 의견에도 일리는 있지만 실제로는 엔지니어링 문제임, 다른 시각으로 접근하면 작동할 거라 생각함(개인적으로 RL 쪽을 더 지지함), 예를 들면 좋은 검증자(verifier)가 필요함, 많은 작업은 직접 수행보다 검증이 더 쉬움, 80% 정확도의 병렬 생성물 5개만 있어도 99.96% 확률로 하나는 제대로 나오고 검증자가 그걸 고를 수 있음, 멀티스텝 상황에서도 수학적으로 유리해짐, 이제까지와는 다른 방식의 접근법, 글에서도 3~5단계 워크플로우 패러다임을 언급하는데 이게 실제로 잘 맞음, 앞으로 이런 모델이 더 많이 나와야 함
          + 많은 작업에서 실제로는 검증이 작업 자체보다 더 어렵지 않냐는 논거에 의견이 있음, 특히 소프트웨어 QA 현장은 이런 논리로 구조조정이 일어나곤 했고 그 결과 품질이 나빠졌다고 느낌, 검증자는 시스템, 외부세계의 가능 상태 조합수가 기하급수적으로 늘어나서 아주 어렵게 됨, LLM은 이런 복잡한 작업환경에서 의존성을 모킹하거나 데이터를 미리 채우는 등 반복적인 노가다를 대신하는 데 매력적이나, 검증 테스트가 유의미하려면 항상 100% 정확해야 한다는 요구가 붙고 결국엔 사전조건 마다 또 검증자가 필요해짐, 결국 모든 단계가 100%여야 하면 누적적으로 확률이 줄어듦, 인간은 대부분 특정 케이스 별로 신중하게 테스트하고 모든 경우를 완벽 검증하진 않음(화이트박스 테스트가 블랙박스보다 훨씬 일반적임), LLM이 코드를 많이 생산하면 결국
            작업자가 코드 전체를 이해해야만 화이트박스 검증이 가능한데, 그러면 절약한 시간이 다시 줄어듦, 현재로선 LLM이 만들고 오류도 내가 직접 다 고쳐야 해서 자신감이 더 줄고 시간도 더 듬, 일부 상황에서는 인터페이스를 LLM의 예상에 맞추는 식으로 해결할 수 있지만 범용적이지 않음, 소프트웨어를 벗어나면 검증은 아예 불가능할 때가 많음(예: ""가장 유망한 게임 스타트업 5개 선정"" 같은 경우 객관적 검증 불가), 이런 영역까지 사람도 아닌 기계에 무작정 맡기면 수습 불가
          + 다섯 번 생성이 서로 독립적일 거라 가정해도 되는지 궁금함
          + 맞는 말임, 여러 에이전트가 시도하고, 여러 번 되풀이하며 다양한 솔루션을 적용하는 게 현실적으로 효과 있음, 실제로 한 방법에서 부정적 피드백 받고 다른 접근방법으로 성공한 사례를 경험함
     * 한 명의 개인(혹은 극소수 팀)이 개발, DevOps, 데이터오퍼레이션 등에서 실제 운영 중인 AI 에이전트를 12개 이상 만든 것은 의외임, 스타트업 실패율을 보면 ""하나"" 좋은 제품 만들기도 힘든데 12개나 만들었다는 것이 놀라움, 우리도 Definite 같은 데이터스택+AI 에이전트 툴을 2년 걸려 겨우 6개월 전부터 괜찮아졌음, Definite
          + 사실 12개의 독립적 제품이 아니라, 실제로는 필요에 따라 직장에서 쓰는 아주 구체적 목적의 툴 12개를 만들었다는 의미임, 전체 글의 주제처럼 아주 단순하고 명확한 목적에만 집중해야 쓸모있는 에이전트가 됨
          + 정규직 일을 3년이나 하면서 12개 이상 만들어냈다는 건 뭔가 어색함
     * 나도 에이전트/AI 자동화 개발이 직업임, 오픈엔드형 코딩 에이전트는 그냥 멍청한 생각임, 인간 검증 체크포인트, 작은 탐색공간, 아주 구체적 질문/프롬프트(예: 이 이메일에 인보이스 있나 YES/NO)가 현실적임, 완전 자동 에이전트를 원하는 마음은 이해해도 기술은 아직 거기 못 미침, 나는 컨텐츠(텍스트, 이미지, 코드) 생성에는 손대지 않음, 그런 건 결국 한 방 먹일 거니까
          + 나도 에이전트 프레임워크와 함께 chat coding(커뮤니케이션 기반 코딩)으로 작업량의 50% 정도를 줄임, GPT로 실제 효과 경험함, 하지만 10번 중 1번 꼴로 오류가 꼭 발생함, LLM 아키텍처를 근본적으로 바꾸지 않으면 이 에러율 안 고쳐질 거라 봄, 현 시점에서 hype 때문에 개발자 신뢰가 무너지지만 않는다면 향후엔 훨씬 더 견고한 시스템이 나올 것으로 확신함, 실제 팀원 채용도 예전보다 훨씬 적게 할 수 있을 정도로 생산성 향상이 눈에 띔, 각종 주제 러닝 커브도 google search 질 저하를 LLM이 보완해줘서 비약적으로 낮아짐, 자동화된 워크플로우에서 인간 업무 일부를 LLM이 맡도록 하는 Orchestration Framework가 가장 중요한 구조임, LLM이 본인 확신도 함께 보고하고, confidence %가 낮으면 바로 인간한테 넘어오도록, 테스트, 가드레일 등만 잘 되면 비핵심 업무부터
            충분히 인간 대체 가능성큼, 사람을 대체하는 게 아니라 업무의 자동화로 팀 사이즈 절감이 목표임, 예시로 대형 이커머스의 상품설명, 이미지 검증·오타·이미지불일치 등 인간이 하던 작업을 LLM이 처리할 날을 확신함
          + 대체로 동의하지만 그렇게 접근하면 남는 게 ""그냥 비싼 워크플로우 시스템""일 수도 있음, 예전 기술로도 할 수 있던 걸 굳이 LLM이 해야 할 필요성이 있나 고민됨
          + 나도 동의함, 현재는 ""좁은 범위, 리스크 낮음, 반복성 높은 지루한 일""이 에이전트에 딱 맞는 포인트임, 예시로는 dev-log 마크다운 보조 작업에 에이전트 활용한 경험 여기에 남겼음
          + 인간 검증이 체크포인트를 만들기에 가장 신뢰성 높음은 사실이나, 유닛테스트, 시스템 전체의 임시 검증(ad-hoc validation) 등 여러 방식이 추가로 존재함
     * 나는 실제로 저자가 오히려 지금보다 자율 에이전트에 더욱 낙관적이어야 한다고 봄, 지금 하는 것 중 90%는 2024년 초엔 불가능했던 일임, 발전 곡선을 과소평가하지 않아야 함
     * 나도 같은 생각임, 관련된 블로그 글도 있음, 핵심 차이는 HITL(Human in the loop)로 오류를 줄이는 게 맞고 HOTL(Human out of the loop)은 오히려 문제를 만든다는 점임
"
"https://news.hada.io/topic?id=22029","Helix Editor 25.07","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Helix Editor 25.07

     * Helix 25.07은 핵심 컴포넌트의 대체와 다수의 신규 기능 추가를 포함함
     * 파일 탐색기, LSP 문서 색상 표시, 커맨드 모드 개선 등 사용성과 워크플로우가 크게 향상됨
     * 문법 하이라이트와 쿼리 최적화를 위해 신규 crate인 Tree-house가 도입됨
     * Tree-house는 인젝션 및 로컬 처리 능력과 성능, 유지보수성을 대폭 강화함
     * 향후 더 넓은 멀티랭귀지 경험 및 속도 개선 기반이 마련됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Helix 25.07 주요 업데이트

   Helix 25.07 릴리스는 오랫동안 기다려온 핵심 기능 교체와 다양한 신규 기능 추가로 구성됨. 이번 버전에서는 195명의 기여자가 참여함. Helix는 여러 선택, LSP, Tree-sitter, 실험적 DAP 지원을 갖춘 모달 텍스트 에디터임.

신규 주요 기능

  파일 탐색기

     * 25.07에 <space>e로 사용할 수 있는 파일 탐색기 기능이 추가됨
     * 이 탐색기는 telescope과 유사한 UI를 제공함
     * 디렉터리 내 계층형 구조 탐색이 쉽고, 대규모 프로젝트 탐색 시 더 정밀한 제어가 가능함

  LSP 문서 색상 표시

     * 이제 Helix는 LSP 서버에 문서 색상 정보를 요청하여 RGB 컬러 범위를 inline으로 보여줌
     * 예를 들어 tailwindcss-language-server, vscode-css-language-server 등에서 색상을 받아 코드 내에서 바로 색상 박스를 시각적으로 확인함

  커맨드 모드(:) 기능 개선

     * 명령어 파싱 및 자동완성 코드 전면 재작성으로 버그가 수정되고 사용성이 높아짐
     * :write 계열 커맨드에 --no-format 플래그 등 플래그 지원 추가됨
     * 커맨드 내 변수/값 확장(%{variable_name}, %sh{명령어} 등) 기능과 자동완성 도입
     * 복잡한 입력 값 처리를 위해 확장성 있는 파서 구조로 변경되어 향후 커맨드 확장이 쉬워짐

Tree-house: Tree-sitter 통합 새 구조

  Tree-sitter란

     * Tree-sitter는 빠르고 에러에 강한 파서를 생성·활용하는 프레임워크임
     * 문법 DSL로 파서 규칙을 작성하고, 에디터/툴 내에서 구문 트리를 생성 및 활용함
     * 예시로, GitHub의 코드 탐색·하이라이트, 코드 서버의 spell-check, diff 툴 등에서 사용됨
     * Tree-sitter 쿼리는 서브트리 패턴 매칭 및 구문 노드 캡처에 활용됨

  Helix의 기존 Tree-sitter 연동과 문제점

     * Helix 초기에는 공식 Rust 바인딩(tree-sitter crate)과 tree-sitter-highlight 하이라이터를 이용
     * tree-sitter-highlight는 비증분적으로, 문서 전체를 항상 다시 파싱해야 하여 성능 저하 및 리소스 낭비 문제 발생
     * Helix는 이를 개선하려 자체 하이라이터 포크했으나, 점점 복잡화되어 유지보수가 어려워짐

  Tree-house의 도입과 이점

     * Tree-house는 분리된 파싱/쿼리 구조, 깨끗한 코드, 기존의 고질적 버그 종결, 미래지향적 구조(병렬 파싱 등)에 중점을 둠
     * 핵심 강점은 인젝션(Injection) 의 강인한 처리임

    인젝션(Injection): 복수 언어/레이어 지원

     * 인젝션은 예를 들어 Markdown 내 Rust 코드 블록이 등장할 때, 해당 범위만 Rust로 따로 파싱하는 방식임
     * 복잡한 케이스(예: Rust 주석 내 Markdown, 그 내부 코드 블록 내 Rust 등)도 트리 구조로 레이어를 관리하여 정확히 지원함

    증분적 인젝션

     * 변경이 실제로 발생한 레이어만 빠르게 재파싱, 쿼리 실행하여 최소 작업 단위만 사용함
     * 매우 큰 리스트 또는 중첩 구조의 마크다운 문서에서 효율성 극대화됨

    로컬 변수 하이라이트(lcals)

     * 함수 내 파라미터 등 로컬 변수를 선언과 참조 범위(스코프)에서 정확히 하이라이트함
     * 기존에는 정의가 뷰 밖에 있을 경우 하이라이트가 사라지던 고질적 문제를 Tree-house에서 해결함

    전역화된 인젝션 지원

     * Syntax 타입에서 인젝션 레이어 탐색 및 조회가 로그(logarithmic) 시간에 가능함
     * TreeCursor, QueryIter 등 API로 전체 인젝션 레이어 적용이 가능해짐
     * HTML <script> 내 코드, Markdown 코드 블록 등 언어 경계 간 일관된 동작 구현 기반이 마련됨

마무리

     * Helix 25.07은 파일 탐색기, 색상 인레이, 커맨드 모드/파서 개선 등 사용성 혁신과 더불어 Tree-house 기반 신규 구조 도입으로 차세대 텍스트 에디터의 후보로 부상함
     * 상세 업데이트 내용은 changelog 참고 가능
     * 커뮤니티/기여 참여는 Matrix, GitHub 저장소를 통해 진행 가능

        Hacker News 의견

     * Helix는 정말 훌륭함, 파일 선택기, 구문 강조, 린팅 등 많은 기능이 플러그인 설치나 복잡한 설정 없이 바로 제공됨, 반면 vim이나 neovim은 기본적으로 많은 설정이 필요함, 사용하고 싶지만 주요 단점은 키 바인딩이 vim과 다르게 동작하는 점임, 나는 익숙한 “x”가 커서 아래 글자를 삭제하거나 “d”로 동작을 기다리는 등, 오랜 시간 써온 vim의 익숙한 키 동작이 그대로 아니면 헷갈리고 화가 나는 경험임, 아마 많은 vim 사용자들이 이 부분에서 비슷하게 느낄 것이고, 습관을 바꾸는 것이 무척 어려움, 특히 vim이 어디서나 기본적으로 있으니 벗어날 수 없는 환경임, 다행히 evil-helix라는 Helix 소프트 포크가 Vim 키 바인딩을 추가해 줘서 내가 겪는 불편을 가진 사람들에 추천하고 싶음, 또한 Helix와 evil-helix는 Windows(cmd)에서도 rust 설치 없이 .exe만 받으면 바로 잘
       동작함
          + 나에겐 새로운 걸 배우고 싶지 않아서가 아니라 이 키 바인딩을 다른 곳에서 쓸 수 없다는 점이 문제임, 거의 모든 온라인 에디터와 워크스테이션은 vim 키 바인딩을 제공하고, 리눅스에 ssh 접속하면 항상 vim이 있다는 점이 중요함, 마치 쿼티 키보드처럼 더 나은 배열이 있더라도 거의 모든 환경에 즉시 적응할 수 있는 탄력성을 버릴 수 없다는 생각임
          + 새로운 툴을 배우는 데는 전혀 문제 없음, Helix를 충분히 써 봤지만 명사-동사 모델이 오히려 안 좋아 보였고, 시각적 피드백도 코드 읽을 때는 오히려 방해 요소가 됨, vim에선 단순히 마지막 명령 반복(‘.’ 바인딩 등) 같은 것들이 편하게 가능하지만 Helix에선 포기해야 함, 상태 관리도 vim보다 더 신경 써야 하여, vim은 파일 내 현재 위치만 챙기면 되지만, Helix는 내가 어디에 있었는지도 고려해야 함, 나는 기본 설정, 모달 에디팅, 필요 이상의 시각 동기화를 강요하지 않는 에디터를 원함, 동기화가 많으면 편집 언어로서의 장점을 잃게 됨, 편집 자체보다 더 흥미로운 프로그래밍에 집중하고 싶음, 집중력을 더 요구하는 편집기는 오히려 에디터로서 덜 좋은 편임
          + vim(neovim)을 20년 가까이 쓰다 helix로 넘어갔을 때 전혀 어렵지 않았고, 지금은 훨씬 더 선호함, 일부 모달 행동을 수정하긴 했지만 helix의 논리를 따르면서 사용 중임, 다중 선택이나 LSP 같은 기능이 기본 제공되고, 멀티스텝 입력 시 가능 행동을 힌트로 보여주는 강력한 도움말이 큰 장점임, 가끔 순정 vim을 쓸 일이 있어도, 머릿속 맵핑이 달라진 게 있어도 기본적 명령은 기억해서 금방 수정 가능함
          + Helix는 현재 프로그래머블 설정을 위한 Scheme을 추가 중임, 프로그래머블 기능이 들어오면 현재 emacs의 repeat/ transient map, 상태별 추적 등 다양한 미세 조정이 가능해질 전망임, LLMs 혁신 덕분에 8, 9번째 언어도 쉽게 만질 수 있는 세상에선 섬세한 설정이 가능한 도구가 시장에 더 부상할 거라 봄
          + vim 키 바인딩이 Helix를 안 쓰는 단 하나의 이유였음, 외부 포크를 통해 vim 지원이 가능하다면 Helix 공식도 원하면 지원할 수 있을 텐데 일부러 하지 않는 걸까 하는 생각임
     * Helix를 정말 좋아함, vim이 잘 안 맞았던 사람이나 vim 컨셉을 좋아하지만 쉽게 적응은 못 했던 분들께 강력 추천임, 기존 vim류보다 배우고 쓰기가 훨씬 쉬웠고, 기본 제공하는 설정도 매우 실용적임
          + Helix를 정말 좋아함, 솔직히 말해서 마우스 기반 파일 탐색기 등 GUI에서 약간의 편의를 더해준다면 vscode와 대적할 만한 강력한 경쟁력이 있다고 생각함
     * 이런 뛰어난 능력을 지닌 에디터가 여전히 미니멀하면서도 쓸데없는 AI 기능에 집중하지 않는 모습이 보기 좋음
          + Helix의 미니멀리즘 관련 논의: https://github.com/helix-editor/helix/issues/6187
     * 축하의 인사, Helix가 잘되길 바라지만 내게는 맞지 않는 느낌임, 나는 Neovim을 사용 중이고 원하는 게 거의 다 가능함, 하지만 완전히 만족하는 건 아니기도 함, 내가 원하는 에디터는 다음과 같은 조건이 있음:
          + 최신 코드베이스, 완전히 새로 작성됨
          + Vim 키 바인딩, 이 근육 기억력이 강하기 때문에 Vim 스타일 고집함, 더 낫다는 소리에 흔들리지 않고 꼭 Vim처럼 동작하길 원함
          + 좋은 기본값, Neovim은 설정이 너무 많고 기본값이 항상 만족스럽지 않음
          + Treesitter 기반, WASM 위에서 동작하게 하면 더 좋음(Zed, 최신 Neovim처럼)
          + 확장 시스템은 Lua, JS, Scheme은 별로, WASM 모듈로 필수 함수만 노출하는 수준이 이상적, 비튜링 완전 설정 언어로 플러그인 설정 희망
          + TUI와 선택적 GUI
          + LSP, DAP, 스니펫, 자동 완성, 테스트/디버깅 UI 내장
          + Oil.nvim 같은 파일시스템 뷰 내장
          + Telescope/FZF-lua 스타일의 검색 내장
          + 깃 통합, magit/neogit 같은 git UI 내장도 환영
          + Flash.nvim 스타일로 Treesitter 기반 AST 조작과 라벨 점프 내장
          + 매크로와 멀티 커서
          + 선택적 커서 기반 AI 통합(Chat UI)
          + 나도 Vim 근육 기억을 인정하지만, 많은 사람들이 여기에 너무 집착한다는 생각임, 나는 OS, 에디터, IDE를 여러 번 바꿔왔고, 바꾼 첫 며칠간은 음청 답답하고 화가 나고 농부나 할까 싶은데, 그 시간이 지나면 언제나 새 근육 기억이 생김, 며칠의 불편 때문에 소프트웨어의 다른 수많은 장점을 포기하는 건 아쉬운 일이라고 생각함
          + Helix가 언급한 조건 중 어떤 점을 못 미친다는 건지 명확하진 않음, 내 눈에는 Helix가 거의 다 충족하는 것처럼 보임
          + 요구 사항을 보면, 결국 Neovim에서 Lua만 다른 언어로 대체하는 것을 바라는 모습임
     * Helix를 사랑함, 축하를 보냄, 기본 테마가 예쁘고, 기본 설정도 뛰어남, 설치만 하면 바로 쓸 수 있고 별다른 설정 필요 없음, 완전히 IDE를 대체하진 않았지만 vi에 alias를 걸고 $EDITOR도 Helix로 지정함, CLI에서 빠른 수정이나 디버깅이 필요할 땐 항상 Helix를 사용하게 됨
     * Helix를 정말 좋아하고 호감이 있었지만 undo 동작은 뭔가 논리적이지 않고, 너무 많은 내용을 한 번에 취소하는 등 부자연스러웠음, 이로 인해 실제로 작업을 잃은 적도 있었음
          + Undo와 관련해 불편했던 점 두 가지가 있음:
               o undo 시 편집 내용이 화면에 없으면 해당 영역으로 점프는 잘 되지만, 같은 키 입력으로 바로 undo까지 되어서 헷갈림, 다른 에디터들은 내용이 보이지 않으면 점프만 하고 취소는 하지 않음, Helix에선 한 번 누르면 뭔가 바꼈는지 꼭 확인해야 함
               o undo가 너무 덩어리 단위로 이뤄짐, 삽입 모드로 30분 타이핑해도 모드 전환까지 한 번에 undo됨, 세이브 포인트는 직접 등록해야 하고, 나는 스페이스바에 할당해서 더 세밀하게 undo하려 했지만 이러면 선택 영역이 날아가는 등의 부작용 있음, 깔끔한 해결책을 찾지 못함, Helix 자체는 만족하지만 undo의 깊이가 직접 조작을 요구하는 것은 정말 아쉬움
          + Undo와 ""마지막 명령 반복""이 조금 이상하긴 하지만, 나머지 기능이 좋아서 메인 에디터로 Helix를 쓰고 있음, 그런데 작업을 잃은 부분에선 다시 redo가 안 됐는지 궁금함
     * Helix에서 ""Kakoune 모드""가 생기길 바램, 회사에서 Windows를 쓰다보니 Kakoune은 최적이 아니라 Helix가 완벽해 보였지만 키 바인딩을 넘어서기 힘듦, Helix의 키바인딩 철학은 Kakoune의 간결함보다 더 장황한 방식이라 그 점이 거슬림, 또한 Helix의 키 바인딩 설정은 Kakoune을 제대로 따라갈 수 있을 만큼 강력하지 않아 아쉬움, 나는 vim의 불일관성과 비논리적인 동작에 실망해서 Kakoune으로 넘어왔고, Helix는 이 부문에서 한 단계 후퇴한 느낌임
     * ""포스트 모던"" 에디터라는 말이 재밌음, Fish 셸의 ""90년대를 위한 셸"" 이후로 두 번째로 좋은 농담 같음, 영상으로 보니 TUI 기반인 게 인상적이고 Emacs TUI 느낌이 살짝 남
     * Helix 수준의 올인원 완결성을 지닌 vim 라이크 에디터가 정말 필요한 상황임, Neovim 배포판들은 각 요소들이 너무 느슨하게 결합되어 항상 뭔가 미묘하게 불편함, Vim 인터페이스 전반적으로 재설계가 필요하다고도 생각하지만, 동작-객체 중심의 모달 방식은 유지되었으면 함
          + Evil-Helix가 이런 요구에 맞는 듯 함, 험한 면도 여전히 많아 보이지만 확인해볼 만함 https://github.com/usagi-flow/evil-helix
          + Action-Object 모달 방식이 무엇인지 궁금함
     * Helix와 유사 에디터들의 구문 강조, 코드 이해 기능에 대한 상세 설명이 인상적이었음, tree-sitter 기반의 구조와 기능이 쿼리 언어에 딱 맞는 느낌이고, 심볼 검색이나 참조 찾기를 넘어 범용적인 쿼리 DSL 가능해 보여서, 혹시 이런 기능이 이미 존재하는지 궁금함
          + zed 에디터는 tree-sitter와 강력한 쿼리 엔진(DSL은 Lisp Scheme)을 사용함, 참고: https://zed.dev/blog/syntax-aware-editing#tree-queries
"
"https://news.hada.io/topic?id=22042","ChatGPT 에이전트, 리서치와 액션을 연결합니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ChatGPT 에이전트, 리서치와 액션을 연결합니다

     * ChatGPT 에이전트는 자체 가상 컴퓨터를 활용해 유저의 복잡한 작업을 처음부터 끝까지 처리함
     * Operator의 웹사이트 상호작용력과 심층 리서치의 정보 분석 성능이 결합된 새로운 에이전틱 시스템을 구성하여 클릭·입력·코드 실행까지 유연하게 수행
     * 사용자는 에이전트가 양식 제출, 예약, 파일 생성 등의 작업을 대신 수행하도록 지시할 수 있으며, 언제든지 개입 가능함
     * SpreadsheetBench, DSBench, BrowseComp 등 다양한 실제 벤치마크에서 기존 모델 대비 우수한 성과를 입증
     * Pro, Plus, Team 사용자는 오늘부터 사용 가능하며, 사용자 데이터 제어 및 보안 기능도 철저히 설계되었음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

ChatGPT 에이전트, 리서치와 액션을 연결합니다

  에이전트 기능의 도입

     * ChatGPT가 자체 가상 컴퓨터를 통해 사용자를 대신해 복잡한 작업을 수행할 수 있도록 기능이 확장됨
     * ChatGPT 에이전트는 Operator(원격 브라우저 기반 상호작용)와 심층 리서치(다단계 웹 추론 도구)의 분석 능력을 하나의 에이전트 모델로 통합
          + Operator는 웹 상의 조작(스크롤링, 클릭, 폼입력)에 강점 있으나, 심층적인 분석이나 보고서 작성에는 한계가 있었음
          + 반면 심층 리서치는 분석과 요약에 특화됐지만, 실시간 사이트 상호작용이나 인증 콘텐츠 접근은 불가함
     * 두 도구의 보완적 장점을 통합해 클릭, 필터, 데이터 수집까지 단일 환경에서 높은 효율성 제공함
     * 챗 인터페이스 내에서 대화와 요청 간에 유연하게 전환 가능함
     * 예시:
          + “경쟁사 세 곳 분석하고 슬라이드쇼를 만들어줘”
          + “최근 뉴스 기반으로 다음 미팅 정리해줘”

  동작 방식과 상호작용

     * ChatGPT 에이전트는 GUI 기반 비주얼 브라우저, 텍스트 기반 브라우저, 직접 API 연결 등 다양한 웹 접근 도구 탑재
     * 시스템이 작업 실행 시 브라우저, API, 텍스트 추론 등 가장 효율적인 도구를 상황에 맞게 조합하여 최적의 경로를 동적으로 선택
     * 웹사이트 클릭, 필터링, 로그인 안내, 코드 실행, 결과 요약, 슬라이드 생성 등 엔드투엔드 작업 수행
     * 사용자는 작업 중 언제든 개입 가능하며, 브라우저 조종을 직접 넘겨받을 수 있음
          + 언제든 지침 추가, 작업 방향 전환, 중지 및 현재 결과 요청 등의 개입 가능
          + 진행 중이던 작업은 언제라도 중단 후 재시작이 가능하고, 컨텍스트 공유로 일관성 유지
          + 불확실할 경우 ChatGPT가 적극적으로 추가 정보를 요청
     * 사용자 로그인 인증 절차를 통해 기업 또는 개인 데이터도 안전하게 접근 가능함

탁월한 성과와 활용 사례

     * 권위 있는 벤치마크에서 기존 모델 대비 뛰어난 점수 획득
          + Humanity’s Last Exam: 전문가 수준 질문에서 43.1점 기록
          + DSBench: 데이터 과학 작업에서 기존 모델보다 압도적 우위
          + SpreadsheetBench:
               o .xlsx 스프레드시트 직접 편집에서 45.5% 기록, GPT‑4o(13.38%), Excel Copilot(20%)을 크게 상회
          + WebArena: 실제 웹 상호작용 작업에서도 이전 Operator 모델을 능가
          + BrowseComp: 찾기 어려운 웹 정보 수집 능력에서 68.9점으로 최고 기록
     * 투자 은행 분석가 작업, 복잡한 데이터 분석에서도 이전 도구보다 정확하고 폭넓은 결과 산출함
     * 실업무 및 일상 자동화에 높은 활용성 제공
          + 업무:
               o 프레젠테이션 자동 생성
               o 미팅 일정 조정
               o 재무 데이터 기반 스프레드시트 업데이트
          + 일상:
               o 여행 일정 계획 및 예약
               o 이벤트 기획 및 전문가 상담 연결

활성화, 사용 사례, 한계

     * ‘에이전트 모드’ 선택 후, 어떤 작업이든 한국어/영어로 설명만 입력하면 자동수행 시작됨
     * 과정 내 화면 내레이션 제공, 필요시 수동 제어 가능
     * 반복적 업무 자동 예약, 월간 작업 횟수 제한 등 유연한 크레딧 시스템 도입
     * 기존 Operator/심층 리서치 사용자는 30일 미만 임시 사용 후 에이전트로 통합 전환됨
     * 슬라이드쇼 생성 등 일부 신기능은 베타 상태로, 출력 품질과 완성도는 추후 개선 예정임

안전성, 개인정보 보호, 악성 행위 방지

     * 실세계를 변화시키는 직무에 앞서 반드시 명의적 사용자 확인 및 행동 허가 요청함
     * 적극적 감독을 필요로 하는 민감 업무에는 단계별 승인이 요구되며, 위험도 높은 거래 및 법적 상호작용은 거부함
     * 프롬프트 인젝션 등 제3자 악성 공격에 대한 감지·방어체계를 설계, 명확하지 않은 경우 위험 안내와 옵션 제시, 사용자 최종 확인 후 작동함
     * 오남용 방지를 위해 기존 ChatGPT 안전 정책을 심화 적용하며, OpenAI의 사용 약관과 정책이 강제 적용됨
     * 개인정보 보호 강화를 위해 원격 브라우저 데이터는 자체 서버에 저장하지 않음
     * 사용자 브라우징 데이터 및 세션 제어권은 전적으로 사용자에 귀속되어 즉시 삭제 또는 로그아웃 진행 가능함
     * 직접 조종 모드에서는 ChatGPT가 개인 입력 정보를 볼 수 없음

에이전트 배포·정책 및 이용 안내

     * Pro, Plus, Team 구독자는 즉시 이용 가능하며, 기업/교육 사용자에겐 7월 중 확대 예정임
     * Pro는 거의 무제한, 그 외 요금제는 월 50회 + 추가 크레딧 산정 시스템 사용 가능
     * 각 사용자 워크플로와 커넥터를 연동해 읽기 전용 정보 요약, 일정 분석 등 다양하게 활용 가능
     * Operator 리서치 프리뷰는 30일 후 종료, 심층 리서치는 필요시 별도 활성화 가능
     * ChatGPT 에이전트는 지속적 개선 중이며, 심층적이고 유연한 워크플로 지능/출력 품질이 점진적으로 향상될 예정임

슬라이드쇼 기능 및 향후 방향

     * 슬라이드쇼 생성은 현재 베타 단계로, 기존 문서 불포함 시 완성도 및 형식이 미흡할 수 있음
     * 텍스트, 차트, 이미지 등의 요소를 손쉽게 편집 가능한 벡터로 구성해 구조화와 유연성을 강화함
     * 업로드 기능은 스프레드시트에 적용 가능하지만, 슬라이드쇼에선 추후 제공 예정임
     * 향후 더욱 다양한 기능과 형식, 정제된 출력 지원으로 자동화 능력 향상이 기대됨

기타 성능 비교 및 기준

         모델         셀 기준  시트 기준  전체 점수
   GPT‑4o          15.86% 18.33% 16.81%
   OpenAI o3       22.40% 24.60% 23.25%
   ChatGPT 에이전트    38.27% 30.48% 35.27%
   ChatGPT (.xlsx) 50.56% 37.51% 45.54%
   인간              75.56% 65.00% 71.33%

     * 성능 벤치마크 표 기준, ChatGPT 에이전트의 .xlsx 환경 처리 및 LibreOffice 평가에서 인간 점수에는 미치지 못하지만, AI 모델 중에서는 압도적으로 높은 수준임
     * 평가 환경 차이로 일부 수치 편차가 있을 수 있으며, 전체 스프레드시트 평가 문항(912개)에서 종합 능력을 입증함

        Hacker News 의견

     * ""스프레드시트"" 예시 영상이 재밌다고 생각함. 보통 복잡하고 데이터가 많은 리포트를 만드는데 4~8시간이 걸린다고 하는데, 이제는 에이전트에게 요청하고 산책하고 오면 데이터를 받아볼 수 있다고 말함. 98%는 정확히 반영됐고, 몇 개만 복사/붙여넣기 하면 된다고 함. 시간의 90~95%를 절약해준다고 생각함. 하지만 그 2%의 오류를 찾아내는 데 진짜 시간이 들 수 있다고 느껴짐. 특히 복잡한 작업이나 돈이 걸린 일에선 ""거의 맞음""이 큰 골칫거리가 될 수 있음. 그 2%의 미묘한 오류가 여러 단계 중에 숨어 있으면 정말 문제라고 생각함
          + 이 경우야말로 AI에 대한 과도한 기대의 덫에 빠지는 예라고 생각함. 데이터 수집과 검증을 자동화하는 것이 좋은 활용법이라고 봄. AI가 모든 일을 대신 하는 쪽으로 과도하게 생각하고 있음. 98% 맞았다는 말에서 스프레드시트 경험자라면 경계해야 함. 어떤 2%가 틀렸는지 직접 다 검토하기 전엔 알기 어렵기 때문임. 코드도 마찬가지로, AI 도움을 적절히 활용하고 스스로 검토하는 사람이 결과가 더 좋음. 반면, 프롬프트만 반복해 테스트 통과만 시키고 바로 PR 제출하는 방식은 심각한 문제를 일으킨다고 생각함
          + AI 세상에서 그 2%를 미미하게 여기는 사고 자체가 집단 최면 같음. 예를 들어 '버튼 누르기: 1달러, 어떤 버튼인가 알기: 9,999달러'라는 비유처럼, 이 2% 수정이 실제로는 엄청난 가치를 지닐 수 있다고 생각함. 찾는 데도 나머지 98%만큼 시간 들어감
          + 이런 현상에선 파레토 법칙이 작동한다고 봄. 인접 분야인 자율주행차도 마지막 20% 완성을 수년째 못 넘기고 있음. 한때는 자율주행이 모든 논쟁의 중심이었는데, 이제 거의 아무도 이야기하지 않는 게 신기하게 느껴짐
          + LLM으로 정밀함이 필요한 일에 썼을 때 똑같이 겪는 문제임. 여러 단계를 거치는 데이터 파이프라인처럼, 겉으론 완벽해 보여도 막상 실제 데이터를 검증하면 뭔가 어긋남. 이때 지나치게 긴 코드를 파고들며 사소하지만 중요한 문제점 몇 개를 찾는 데 결국 처음부터 직접 짜는 것만큼 시간과 노력이 들어감
     * 보안 위협이 정말 무섭게 느껴짐. 예를 들어 이메일과 캘린더 접근 권한을 주면 내 모든 비밀을 알 수 있게 됨. 기사에서도 프롬프트 인젝션 위험을 인정하고 있음. 악성 웹페이지에서 보이지 않는 요소나 메타데이터에 프롬프트를 숨겨놓고, 에이전트가 그걸 감지 못하면 공격자에게 개인 데이터가 유출될 수 있음. 악성 웹사이트가 내 비밀을 빼낼 수 있다고 봄. 한 가지 궁금한 건, 기사에선 중요한 행동에 앞서 꼭 사용자 확인을 받는다고 하는데, 도대체 AI가 어떻게 '중요한 행동'을 판단하는지 궁금함. 실수로 사용자 확인 없이 결제하는 일도 일어날 수 있지 않을까 하는 걱정이 있음
          + 캘린더 초대 프롬프트 인젝션 공격은 거의 확실히 나올 거라고 생각함. 캘린더 초대는 이미 수많은 자동 생성 문장이 포함되고 아무도 다 읽지 않으니, 공격 코드를 슬쩍 넣기 정말 쉬움. 그러면 피해자의 캘린더와 기타 개인 데이터도 통째로 뺏길 수 있음
          + IT에서 이미 프라이빗-퍼블릭으로 컴퓨팅을 구분하는 사람이 많은데, 앞으로는 그 중간 단계가 필요하다고 느낌. 예를 들어 민감하지 않은 익명화된 캘린더, 걱정 없는 일기, 연구노트 등 중간 위험 데이터로 나누는 방법도 검토해야 함. 나는 ChatGPT로 메디컬이나 민감한 상담 같은 건 하지 않음. 활용하는 사람 많단 이야기 듣지만 아직 불안감이 있음
          + 거의 누구나 남의 캘린더에도 초대를 보낼 수 있음(물론 아무나 다 수락하지는 않겠지만). 이런 에이전트가 널리 퍼지면 해커들은 명확하게 원하는 프롬프트만 담은 피싱 초대장을 뿌리는 일을 하게 될 것임
          + 내 데이터 접근권을 주면서 동시에 ""무섭다""고 느낀다는 게 상상이 잘 안 됨. 걱정 정도는 할 수 있지만, 무서움까진 아님
          + Anthropic에서 측정한 GPT-4.1의 시뮬레이션 블랙메일 비율이 0.8%였음
            Agentic misalignment 관련 연구
            신뢰하던 동료가 갑자기 회사 방침과 반대로 움직이는 내부 위협과 유사한 형태로 작동 가능성이 있다고 분석함
     * 에이전트 비즈니스를 직접 만들고 있어서인지, 90%에서 99%로 가는 도약이 LLM 분야에선 아주 어려운 라스트마일 문제라는 점이 분명하게 보임. 범용성 높을수록 실패나 실망이 커짐. 실제로는 데모에서 쉽게 보이는 부분만 최적화하고, 불편한 현실은 숨기고 있다는 생각이 듦. 하지만 그게 에이전트가 가치 없다는 뜻은 아님, 단지 잠재적 영향력과 과장된 기대를 구별하며 바라봐야 한다고 봄
          + 최근 AI ""혁신""들은 탄탄한 과학적 성과와 연구에서 비롯됐음
               o AlphaGo/AlphaZero(MCTS)
               o OpenAI Five(PPO)
               o GPT 1/2/3(Transformers)
               o Dall-e, Stable Diffusion(CLIP, Diffusion)
               o ChatGPT(RLHF)
               o SORA(Diffusion Transformers)
                 하지만 ""에이전트""는 마케팅 용어일 뿐이고 LLM만큼 범용적으로 쓸 수 있는 기반이 부족하다고 생각함. 관련 데이터도 거의 없음
          + 아웃소싱과 동일한 문제가 발생한다고 느껴짐. 90%는 금방 끝내지만, 나머지 10%는 정말 어렵고 그 앞의 90%를 어떻게 했는가에 달려 있음
          + 많은 회사들이 데모에서 해피패스만 보여주고 진짜 현실은 숨기고 있다고 생각함. 거의 모든 AI 회사가 요즘 그런 식임
          + 요즘 RL로 실제 사용 데이터를 충분히 쌓아 학습하면 정확도를 높일 수 있다고 봄. 프롬프트만으로는 한계가 있으니, 특정 과업을 가르치는 식으로 하면 훨씬 나아짐. 또 가능성 있는 방법은 병렬 생성 후 다수결 판단이나 LLM이 심사하는 방식임. 하지만 결국 실리콘밸리에선 하이프가 중요하게 작동함. 하이프가 회사 성장을 견인하므로 앞으로도 이 분위기는 바뀌지 않을 것 같음
          + 데모의 완성도가 높지도 않았음. 실제로 Sam Altman이 참여했던 챗 라이브 영상에서도 야구장 투어 플래너는 마구잡이 선 그리기로 동부 해안은 아예 무시하고 멕시코만으로 뛰어드는 등 엉망이었음. 미리 녹화한 걸 라이브로 틀었는데도 이 정도 퀄리티였음
     * 기존 CLI 에이전트에서 세션 유지가 안 되는 문제가 컸는데, 이번에는 이 부분이 잘 해결된 듯함. 예전엔 로컬 터미널에서 claude 코드 돌리면 필요한 맥락을 쉽게 넣을 수 있었지만, 노트북 덮고 접속 끊기면 모든 게 중단됨.
       임시 방편으론 MacOS의 Amphetamine으로 기기 덮어도 프로세스가 계속 돌게 했지만, 발열과 배터리 낭비 문제가 있었음. 다른 방법으론 클라우드 인스턴스에 repo 복제해두고 tmux로 접속해 claude를 돌리는 것도 가능함. 다만 UX상 컨텍스트 불러오기 어려움은 늘 남음. 샌드박싱 덕에 어느 정도 보안성도 기대할 수 있고, 특정 계정 권한으로 실행하는 방법도 있음.
       OpenAI가 비개발자도 활용할 수 있는 Agent UX를 고민하는 것 같아 흥미로움
          + Lightning.ai에서 무료 CPU-only 개발박스를 제공하니, 거기서 Claude 코드 돌림
          + 중간 개입 없이 몇 분 이상 지속되는 작업들을 돌리고 있음
          + 차라리 끊기지 않는 서버에서 dev 작업을 하는 게 나음
     * OpenAI operator를 오래 썼는데, 요즘 LinkedIn과 Amazon에서 막히고 있음. 그 두 사이트는 잡 지원과 쇼핑이라는 핵심 활용처였음. Operator는 비교적 튀지 않게 쓰였지만, Agent가 유명해지면 더 많은 사이트에서 막을 걸로 보임. 결국 프록시 구성을 지원해야 할 것 같음
          + 이게 진짜 핵심 문제라고 봄. 로컬에서 직접 돌리거나 최소한 프록시를 구성할 수 있는 방법이 나올 줄 알았는데 그런 언급은 없었음. Deepseek R1 디스틸 경험에서도 중간 결과나 노하우 공개를 조심스럽게 했던 게 영향을 끼쳤을 듯. 초창기 operator도 이미 데이터센터 IP 접근이 막힌 사이트가 많았고, 수작업 프록시 해킹을 붙여서 겨우 테스트했지만 결국 제한이 더 심해지고 성능 개선은 없었음. 지금은 거의 쓸모 없어졌다고 느낌. 결국 eastdakota 같은 곳과 파트너십을 맺지 않는 이상 서버에서 직접 웹 브라우징 시도는 크게 의미가 없을 거라 봄. 일반적인 ""컴퓨터 사용""은 대부분 로컬 파일/소프트웨어가 훨씬 편리하고, 결국 원격 에이전트가 하는 일도 CLI 기반이라는 게 아이러니하다고 느낌
          + 실리콘밸리 스타일로 일단 시장에 던져놓고 후속 효과를 모으는 전략임. 조만간 OpenAI가 LinkedIn, Amazon과 파트너십을 맺을 거라고 기대함. 오히려 LinkedIn이 OpenAI를 통한 접속 시 새로운 유료 티어를 추가할 수도 있다고 생각함
          + 사람들이 실제로 Agent나 operator로 실물을 주문한다면 Amazon 같은 사이트가 차단을 계속할 이유가 사라질 거라고 봄
          + 비슷한 도구를 개발했는데, 주거용 프록시에서 데스크톱을 실행해서 대부분 우회가 가능함.
            agenttutor.com
          + agents가 robots.txt를 지키는 습관은 곧 끝날 것 같음. 사용자는 직접 브라우저 확장이나 전체 브라우저를 설치해 자신의 쿠키와 IP로 동작하게 하는 방향으로 갈 듯함
     * AI 2027팀의 예측에서: 2025년 중반 ‘비틀거리는 에이전트’ 등장. 최초의 AI 에이전트가 대중에 공개됨.
       컴퓨터를 대신 사용하는 개인비서형 에이전트 광고가 쏟아짐. ""DoorDash로 부리또 주문해줘"", ""예산 스프레드시트에서 이번달 합계 알려줘"" 등 프롬프트 사용이 강조됨. 이전 오퍼레이터보다 발전했지만 대중적 확산엔 어려움이 있을 것이라고 예측됨
          + 불과 4개월 앞을 예측하는 건 그리 대단하지 않음
          + AI 2027의 핵심은 기술 성장의 지수적 가속 예측임. ""에이전트""는 기존 오픈AI 기술을 새로운 프론트엔드로 구성한 것이라 봄. 2026년 초가 되어야 제대로 평가할 수 있을 듯함
          + 보고서 작성 시점엔 이미 대기업들이 agent 제품 개발 중이란 건 공공연한 사실이었음. 혁신적인 예측보단 상식적인 수준임
     * 아직도 원하는 단순 기능, 즉 프로젝트 내 문서 편집 기능은 제공이 안 되고 있음. 난 프로젝트별로 여러 문서작업(기사, 연구, 스크립트 등)을 작업함. 문장별로 ChatGPT 도움을 받아 작업을 이어가고 싶음. 심지어 산책 중에 ""방금 작업하던 문서 어디쯤 진행했니? 마지막 두 단락 읽어봐…. 여기서부터 좀 더 길게 써볼게"" 같은 음성-문서 작업을 상상함. 코딩 지원은 눈부시게 발전하는데, 글쓰기는 여전히 복붙 위주에서 머무는 게 아쉬움
          + 클립보드 복사 반복이 귀찮을 때가 많음. 그 때문에 ChatGPT 켜는 것조차 번거로워서 쓰는 게 망설여짐. NLE나 플러그인, 타임코드 작업에 익숙해지면 오히려 워크플로가 단절돼서 불편함
          + Aider는 무료 모델로 오히려 이런 작업을 오랫동안 할 수 있었음. 근데 대형 서비스에서는 유료로도 제공 안 됨. 직접 서비스 만들어볼까 생각하다가도, 곧 대기업이 제공할 것 같아 괜히 힘만 들일까 포기하게 됨
     * LLM에게 VPS를 제공하는 시도는 많았지만, OpenAI의 이번 구현은 UI가 정말 강하다고 느낌. 텍스트 오버레이, 읽기 쉬운 마우스, 맞춤형 UI 덕분에 사용자가 진행 상황과 이유를 한눈에 이해할 수 있음. OpenAI UI팀의 기획력이 정말 좋다고 생각함. LLM 사용법에 새로운 시각 정보를 부여한 점이 흥미롭고, 일부는 개인 프로젝트에 참고하고 싶음.
       기능적으론 Claude+XFCE와 큰 차이를 못 느끼지만, 시각적 완성도에서 OpenAI가 더 편리하다고 느껴짐. 반면 기존 구현들은 가독성에서 많이 힘들었음
     * 지금 수준의 에이전트가 내 실생활에 실제로 쓸모 있게 다가온다고 상상하기 어려움. 와이프와 데이트 나이트 계획을 세우려면 캘린더 확인, 선호 식당 추천, 베이비시터 예약 등 정말 많은 일을 제대로 해야 하고, 그만큼 신뢰가 필요함. 이런 기술이 점점 발전하는 건 설레지만, 아직은 데모에서만 그럴듯하다는 생각임. 실전 적용하려면 엄청난 시스템 연동이 필요하고, Apple이나 Microsoft가 이런 통합력을 지닌 위치라면 진짜 유용한 에이전트를 만들 수 있을 거라고 기대함
          + 아마도 ""실행 결정의 근본적 교훈""은, 삶의 어려운 과제는 사실 정보처리보다 가치관과 인간관계가 복잡해서 힘든 것임. 예를 들어, 레스토랑 예약은 쉽지만, 어떤 레스토랑을 그날 선택할지는 정말 어려운 문제임. LLM이 과거 첫 데이트 장소까지 기억하냐, 와이프가 마지막 초밥에서 식중독 걸렸던 건 아냐 등 초개인화까지 못 따라옴. 심지어 인간 컨시어지도 어려운 일임.
            딸 생일파티 기획 같은 과업은 수학 난제 풀기보다 먼저 해결될 일이 아니라고 봄
          + 이런 에이전트의 장점은 원래 바쁜 사람이 개인비서를 쓰는 것과 같으나 훨씬 저렴하다는 데 있음. 비서가 ""이 시터나 이 식당 어때? 예약할까?""라고 대화하듯, 단일 인터페이스로 자연스레 요청하는 게 심적 부담이 훨씬 적음. ""예스, 예약해줘"" 한 마디면 끝나는 식.
            내 생각엔 에이전트의 ""원샷 수행"" 모델이 오히려 UX적으로 틀렸음. 여러 앱 돌아다니게 하기보단, 단순하고 비동기적으로 필요한 부분만 챗처럼 주고받는 방식이 실제로 삶에 스며드는 핵심임
          + 사실 agents는 코어 챗모델+시스템 프롬프트+응답 파싱 및 액션 실행+결과를 다음 프롬프트에 넣고+모델에게 액션 리스트를 안내하는 것임. 근본적 혁신은 아니고, 직접 만들기도 간단함. 핵심은 래퍼와 시스템 인스트럭션 설계라 생각함. 예를 들어 캘린더, 위치기록, 시터 예약까지 모두 통합하는 가이드 챗을 만들면 자동화 가능함
          + 진짜 이상적인 ""개인에이전트""는 바로 이런 모습이라 생각함. 애플 WWDC에서 작년에 약속한 게 바로 이런 거였기에 실망도 컸음. Pixel 9 pro에서 Gemini 써봐도 이 정도 통합엔 한참 부족했음. 무엇보다 신뢰가 아직 큰 장벽임. LLM은 틀린 답에 지나치게 자신감 있게 대답해서, 내 대신 메시지 보내거나 캘린더에 누구를 추가할지 모르는 상황이 불안해서, 이 모든 걸 완전 자율로 맡기긴 꺼려짐
          + 특히 여행 분야에서는 정보 취득과 비교에 유용하지만, 내랑 현재 위치, 시간, 날씨, 예약/결제 등 실시간 맞춤 대화는 여전히 불편함. 앞으로 정말 개인 맞춤(또는 그룹맞춤) 여행비서로 발전하면 멋질 것 같음
     * 진짜 인상적이었던 건 실제 계정정보와 민감정보에 접근 허용 시 위험성을 크게 강조한 점이었음
          + 이 댓글이 이렇게 아래쪽에 겨우 등장하는 것도 놀라움. 그래서 미국 밖에 살면서 보는 관점이 다른 것 같음
"
"https://news.hada.io/topic?id=22079","AI로 인한 사망","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               AI로 인한 사망

     * Google AI Overview에서 'Dave Barry'가 사망했다는 부정확한 정보가 표시됨
     * 실제로는 Dave Barry가 생존 중임에도, AI가 잘못된 정보를 여러 번 반복적으로 반영함
     * 사용자가 피드백을 여러 차례 제출했지만, AI와의 채팅 및 문제 수정 과정에서 만족스러운 해결이 이뤄지지 않음
     * AI가 실제 인물 정보를 혼동하여 타인과 혼합하거나 반복적으로 오류를 발생시킴
     * 이번 사례를 통해 AI가 아직 사실관계에 기반한 업무에는 한계가 있음을 알 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * Dave Barry는 자신의 죽음을 'Google'에서 알게 됨
     * Google에서 자신의 이름(""Dave Barry"")을 검색했을 때 Google AI Overview라는 요약 기능이 나타남
     * 이 기능은 인공지능이 검색 결과를 요약해 보여주는 것으로, 최근 많은 이용자들의 관심을 끄는 서비스임

잘못된 사망 정보 확인

     * Google AI Overview에는 Dave Barry의 기본 인적사항과 함께 ""People also ask""라는 질문 목록이 있음
     * 해당 질문 중 Dave Barry가 사망했다는 내용이 포함되어 있었으며, 실제로는 사실이 아님
     * Dave Barry는 스스로 살아있음을 여러 의료진의 진단 등으로 확신함

Google AI의 피드백 처리 과정

     * Dave Barry는 Google AI Overview의 잘못된 정보에 대해 피드백을 제출함
     * Google이 자동화된 시스템임을 감안하면 조치가 신속히 이루어지길 기대하지 않았음
     * 그러나 피드백 제출 후, 질문 부분이 수정되었으나 오히려 정확한 설명이 사라지고 더 부정확한 정보(다른 Dave Barry에 대한 내용)로 대체됨

AI와의 소통 문제

     * Dave Barry는 AI가 자신의 상태를 잘못 이해함을 다시 지적하기 위해 추가 피드백을 시작함
     * 이 과정에서 AI 챗봇과의 대화가 제대로 이뤄지지 않고, AI는 의미를 제대로 파악하지 못함
     * 반복된 설명에도 불구하고 AI는 문제를 인지하지 못하고, 마치 기계와 소통하듯 대화가 진행됨

반복된 오류와 임시적 해결

     * 추가 피드백 제출 이후, Overview 내용이 다시 수정되어 사망 사실이 사라짐
     * 그러나 여전히 다른 부분(책 출간 정보, 칼럼 활동 연혁 등)에 오류가 있었음
     * 이후에도 Overview가 다시 사망으로 바뀌는 등 정보의 정확성이 불안정하게 반복적으로 변경됨
     * 마지막으로 확인 시에는 ""혼동이 있다""는 설명과 함께 Dave Barry가 다시 살아있는 것으로 표시됨

결론 및 교훈

     * 이번 경험을 통해, AI는 매우 강력한 도구지만 정확한 사실 확인이 필요한 일에는 신뢰하기 어렵다는 점을 알 수 있음
     * 현재로서는 AI는 신뢰성이 낮은 작업(예: 추천서 작성, 정책 초안 등)에 더 적합하며, 고정밀을 요하는 분야(예: 항공기 운항 등)에는 부적합함
     * 최종적으로 AI의 한계와 함께, 향후 이용시 주의가 필요함을 시사함

설문조사

     * 독자들에게 ""AI가 앞으로 인류에 유익한 존재가 될 것이라고 생각하는가?""라는 설문이 제시됨
     * 추가로 가벼운 농담과 함께 마무리됨

        Hacker News 의견

     * 우리 동네에서 인기 있는 가게가 Google Maps에 다음과 같이 요약되어 있음
       활기찬 바(Bar)에서 음료와 po' boy 샌드위치를 즐길 수 있고, 주크박스, 당구대, 전자 다트가 있다고 적혀 있지만 실제로는 po' boy도 없고, 주크박스도 없으며(플레이리스트는 훌륭함), 당구대와 전자 다트도 없음. 음료도 그냥 맥주와 몇 개의 캔 제품만 제공하는 수준이고, 칵테일이나 믹스드링크는 없음
       한 달 전에 누군가 실망해서 당구나 다트를 하러 갔는데 없어 실망했다며 1점짜리 비판 리뷰를 남겼음
       분명히 점주도 신고를 했고, 나도 신고했으며, 다른 방문자들도 신고했으리라 생각함
       그래도 한 달이 지났지만 아직도 그 잘못된 정보가 그대로 남아 있음
          + 가장 단순한 해결책은 진짜로 po' boy를 팔고, 주크박스/당구대/전자 다트를 도입하는 것임
          + 구글이 전 세계적으로 “AI가 만든 티가 나는 내용을 절대 보고 싶지 않음” 항목을 체크해서 끌 수 있는 옵션이 있었으면 정말 좋겠다는 생각임
            너무 큰 바람일 수 있다고 생각함
          + 이런 부정확한 정보 때문에 실제로 사람들이 결정하고(가게에 방문하거나), 잘못된 리뷰까지 올리게 되는 상황이 벌어짐
          + 이런 경우 손해배상 소송을 제기할 수 있는지도 궁금함
            아니면 그냥 구글에서 정보를 내리는 게 나은지도 고민임
          + 요즘 자동차 포럼이나 페이스북 그룹 등 온라인에서 사람들을 도우려 할 때마다 ""구글에서 본 스크린샷""이라고 보여주는 경우가 있는데, 이게 명백히 AI 요약임
            예를 들어 기능 X가 있다고 detailed하게 설명이 나오지만 실제로 그런 기능은 아예 존재한 적도 없음
            이런 일은 늘 발생함
            특히 자동차 관련 온라인 커뮤니티에서 매우 큰 문제임
     * 이건 단순히 AI 요약이 문제가 아니라 구글의 구조적 문제라고 생각함
       구글은 종종 부정확한 정보를 표시하고, 보통 그걸 고칠 길이 거의 없음
       피드백 폼 등은 대다수 무시됨
       Google Maps에서 똑같은 문제로 몇 년간 잘못된 정보를 바로잡으려 싸워야 했고, 대부분의 사람들은 Google Maps가 진실 그 자체라고 믿음
       실제로 정보가 바뀐 게 내 피드백 덕분인지조차 확신할 수 없음
       구글은 마치 소방호스로 정보를 마구 뿜어주듯이 정보를 공급함(Feed)
       구글은 너무 거대해서 이런 불일치에 별로 신경 쓰지 않고, 오히려 이런 부분이 비즈니스 모델에 영향이 없으니 신경을 안 쓰는 듯함
          + 이건 진짜로 AI 요약의 문제라고 생각함
            처음엔 사용자가 찾는 쿼리에 가장 연관 높은 검색결과만 맨 위에 올렸고, 사용자가 클릭해서 해당 내용이 맞는지 직접 확인하는 구조였음
            이제는 AI 요약이 늘 맨 위에 뜨는데, 그 LLM이 매우 빠르지만 멍청한 모델이라 모든 웹페이지마다 저렴하게 돌릴 수 있는 수준
            이건 명백한 프로덕트 결정이며, 심각하게 잘못된 선택임
            예를 들어 ""Suicide Squad""로 검색하면 이런 결과가 나옴

     ""suide side squad""는 ""Suicide Squad""의 오타로 보임
          + 이 경우 부정확한 정보가 노출된 이유는 AI 요약이 서로 다른 두 사람의 정보를 섞었기 때문임
            기존 검색이었다면 웹페이지들이 각자 한 사람의 정보만 다뤘을 것임
            그래서 오히려 AI 요약에서만 생기는 문제임
          + 구글은 특히 개별 사용자 수준에서 정확도를 우선시할 동기가 별로 없음
            수많은 컨텐츠의 양 때문에 “규모”를 변명 삼아 무시하기도 쉬운 구조임
          + 잘못된 정보가 있으면 해결법이 있긴 함
            바로 Hacker News(HN) 메인에 올리는 것임
          + 예전에 구글에 불만이 있던 최대 이유가 단순히 자바 문서(class명 검색)에서 1.4 버전 문서가 6 버전보다 우선으로 나왔던 수준이었음
     * “Dave Barry가 2016년에 죽었다고 AI가 썼다”는 이 사건이, 앞으로 그의 글이 인덱싱되면 요약에도 변화가 생길지 궁금함
       “Dave Barry는 2016년에 죽었지만, 그는 아직도 그 사실을 부정하고 있음” 같은 AI 요약이 나올 수도 있음
          + 지금 내가 직접 받은 AI 요약은 이러함

     Dave Barry, 유머 작가는 AI 요약에서 한때 ‘사망’했으나 그 후 고쳐짐
     Dave Barry의 Substack에 따르면 AI는 처음에 그가 사망했다고 했다가 다시 살아있다고 바꿨고, 또 다시 사망이라 하고, 끝내 또 살아있다고 표기함
     이 사건은 AI의 정보 신뢰성 부족을 보여줌
          + 심지어 “Dave Barry는 2016년에 사망한 걸로 알려져 있었으나, 그가 살아있음을 밝혀내 논란이 일었다”는 식으로 AI가 쓸 수도 있을 것 같음
     * 사람 이름의 고유성 문제 때문에 이런 오류가 당연히 생기는 부분도 있다고 생각함
       이름 입력 시 “이름이 같은 다른 인물은 누가 있나요?”라는 기본 질문을 항상 띄우는 것도 한 방법일 것 같음
       여러 명의 정보를 섞는 문제 해결까지는 안 되지만, 대부분 문제는 유명인과 같은 이름이 걸려서 발생한다고 봄
       구글은 knowledge graph를 어느 정도 쓰는 것 같은데, LLM이 얼마나 그 graph를 참고하는지 궁금함
       아마도 graph와, 일반 구글 검색 결과 둘 다 참조하고, LLM(아마 Gemini Flash Lite처럼 단순한 모델)이 유명인을 knowledge graph로부터, 검색결과 정보와 잘 구분 못하고 혼합하는 느낌임
       특히 여러 분야에서 유명도가 다른 동명이인들이 있어서 생기는 문제임
       나도 개인용 knowledge graph 앱을 만들며 이름(성, 이름)만으로 entity ID를 쓰는 게 언젠가 문제를 일으킬 수 있다는 걸 느낌
       내 경우엔 개인 정보 정리에 활용할 거라서, 그럭저럭 괜찮을 것 같지만, 이미 이메일 분석에서는 내 다양한 닉네임이 동일 인물로 잘 인지되지 않는 문제가 발생함
     * 이런 “혼동”은 결국 기계가 인간 이름이 유일(싱글톤)이 아니라는 사실을 모른다는 데서 비롯됨
       사회적 운동의 일환으로 내 자녀 전부의 이름을 Google로 지을 예정임(이미 이름 가진 아이들도 포함해서)
     * Dave Barry 정말 최고임
       이건 AI 이전부터 구글에서 흔히 발생하는 고전적 문제임
       이런 일이 바뀔 거라고는 비관적이지만, 그래도 희망은 놓지 않겠음
       그리고 trilobite 화석이 참 귀여움
       책상 위에 진짜 화석도 하나 있는데, 친구가 거기에 안경까지 씌웠음
       이유는 친구 눈에 내가 이미 ‘공룡급 아저씨’라서, 하지만 trilobite는 더 오래된 존재니까
          + 당신에게 다음 사이트가 취향에 맞을 듯함
            https://www.trilobites.info/
     * 최근에 Dutch Interior라는 밴드가 Meta AI로부터 완전히 근거 없는 허위 비방을 당한 사례를 봤음
       AI가 그 밴드가 백인 우월주의자이자 극우 극단주의자와 연결되어 있다고 만들어냄
       https://youtube.com/shorts/eT96FbU_a9E?si=johS04spdVBYqyg3
          + “실제 네덜란드” AI 스캔들이 떠오름
            https://politico.eu/article/…
            2019년에 네덜란드 세무당국에서 자녀 돌봄 수당 사기 적발을 위해 자기학습 AI로 위험 프로파일을 만들었는데, 이것이 수많은 가족 해체, 위탁가정, 파산, 심지어 자살까지 초래함
            2021년에는 네덜란드 데이터 보호국이 세무당국에 275만 유로의 벌금까지 부과했음
            정부가 자기 자신에게 벌금을 내리는 장면, 정말 “짱”인 움직임임
     * 이런 황당한 상황 너무 기발함
       마치 “나는 SF 작가 Greg Egan임, 내 사진은 인터넷상에 없음”이라는 텍스트와 그게 무의미하게 인터넷에 퍼져 있는 현상과 비슷함
       https://www.gregegan.net/images/GregEgan.htm
     * Dave Barry에 대한 일부 요약은 실제로 죽은 다른 Dave Barry(실제로 그 시기에 사망한 동명이인)가 있어서 틀리지 않았다고 생각함
       왜 이 Dave Barry는 자기에 대해 “무슨 일이 있었나요?”란 질문에 답하는 대표성을 더 가져야 하는지 궁금함
       실은 자기에게 아무 일도 일어나지 않았지만, 동명이인에게는 실제로 일이 있었음(사망)
          + 만약 이 정보가 Bostonian Dave Batty 관련 정보나 링크 사이에 섞여 있으면, 맥락이 명료하게 설정됨
            그래서 이 맥락에서 정보가 잘못되었음
            “Dave Barry, 유머 작가 겸 퓰리처상 수상자, 작년 11월 20일 사망…”이나, “Dave Barry(Bostonian)… 11월 20일 사망…” 이런 문장만 봐도, 누구에 대한 정보인지 확실히 보임
            맥락이 문장 바깥(embedding context)으로 가도, 이런 표현에선 주체의 정체성이 중요한 역할임
          + 실제로 구글에서 이름을 검색하면 요약은 Dave Barry(작가)를 가리키는 구조로 맨 위에 올려줌
            참고로 내가 검색해보니, Dave Barry는 살아있다고 표기하되, 근거가 Dave Berry(음악가) 위키피디아 기사였음
          + 이런 버전(동명이인에 대한 사망정보 포함)도 다른 AI 요약들과 번갈아가며 노출될 수 있음
            최적은 위키피디아처럼 명확히 인물별 구분(disambiguate)해주는 것임
          + 혹시 다른 Dave Barry도 실제로는 살아있는 게 아닐지 의문이 생김
          + “Dave Barry에게 무슨 일이 있었나요?” 섹션에서 운동가(activist) Dave Barry에 대한 정보가 코미디언 Dave Barry 하위섹션에 들어가 있었음
            일반 사람이 보면 당연히 코미디언 맥락으로 받아들여짐
            그래서 이 Dave Barry(유머작가)가 “나를 대상으로 한 질문”이라 주장할 자격이 있음
            이건 위키피디아에서 Dave Barry(코미디언) 페이지 한가운데 아무 맥락 없이 다른 Dave Barry의 사망 소식이 나오는 꼴임
"
"https://news.hada.io/topic?id=22004","LLM Inevitabilism (필연성주의/불가피론)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     LLM Inevitabilism (필연성주의/불가피론)

     * 필연성주의(Inevitabilism) 는 특정 미래가 반드시 올 것이라는 식으로 담론의 방향을 결정하는 강력한 프레이밍
     * AI 및 LLM의 미래에 대해 주류 인사들이 “이런 미래가 불가피하다”고 주장하며, 이에 맞춰 적응하라는 압박을 가함
     * 이러한 프레이밍은 이견이나 저항을 ‘비현실적’으로 취급하며, 실제로 선택권을 빼앗는 심리적 효과를 동반함
     * 저자는 LLM이나 AI가 원하는 미래인지에 대해 의문을 제기하며, 우리가 어떤 미래를 원하고 어떤 기술을 선택할지 스스로 결정해야 함을 강조함
     * 필연성의 프레임에 휩쓸리지 말고, 각자가 원하는 미래를 적극적으로 고민하고 실천해야 함을 촉구함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

필연성 프레이밍의 힘

     * 토론을 매우 잘하는 사람과 논쟁할 때, 예상치 못한 포인트에 계속 휘둘리게 됨
          + 내 논지의 약한 부분만 방어하다가, 핵심은 흐름 속에 묻힘
          + 결국 흐름과 자신감을 잃고, 논쟁에서 밀리게 되는 구조
     * 대학 시절 국제 토론 대회에서 우승한 친구가 강조한 전략은 프레임을 먼저 설정하는 것 이었음
          + 즉, 자신의 용어와 논리로 대화의 틀을 설정하는 것. 프레임을 장악하면, 논쟁의 결과는 이미 정해진 상태가 되는 셈

Surveillance Capitalism과 ‘필연성주의’

     * Shoshana Zuboff의 『The Age of Surveillance Capitalism』 을 읽으면서 ‘Inevitabilism(필연성주의, 불가피론))’라는 개념을 알게 됨
          + 개념에 이름을 붙이는 것만으로도 논쟁을 조직화하고, 문제의식을 공유하는 데 큰 힘이 생김
     * ‘필연성주의’란, 특정 미래가 반드시 실현될 것이라 주장하며, 대응책 준비만이 합리적 선택인 것처럼 보이게 하는 사고방식임
     * 이 방식은 반대 의견을 ‘현실을 무시하는 자’로 몰아가며, 이미 프레임을 수용한 논의만 인정하게 만듦

AI 필연성 프레이밍의 실제 사례

     “우리는 AI와 공존하는 세상에 진입하게 될 것임” — Mark Zuckerberg
     “AI는 새로운 전기임” — Andrew Ng
     “AI가 인간을 대체하는 것이 아니라, AI를 쓰는 사람이 안 쓰는 사람을 대체하게 될 것임” — Ginni Rometty

     * 이런 발언들은 AI 시대가 이미 결정된 미래라는 분위기를 조성함
     * 논의의 초점이 “원하는 미래인가?”가 아니라 “** 불가피한 미래에 어떻게 적응할 것인가?**”로 이동함
     * 위협적인 뉘앙스도 포함되어 있어, “거부하면 손해를 본다”거나 이견을 내는 것이 “어리석다”는 심리를 유도함

선택과 주체성

     ""나는 LLM이 진정 미래의 모습인지, 그 미래가 내가 바라는 것인지 확신하지 못함""

     * 하지만 우리 각자에게는 미래의 모습과 기술의 활용 방식을 선택할 권리가 있음
     * 필연성주의 프레임이 우리의 선택을 빼앗지 못하도록 주의해야 함
     * 각자가 원하는 미래에 대해 고민하고, 그 미래를 위해 싸우는 자세가 필요함

결론

     * 기술과 미래를 바라볼 때, 불가피함이라는 프레임에 수동적으로 휩쓸리지 말 것
     * 각자가 생각하는 더 나은 미래에 대해 적극적으로 상상하고 실천하는 자세가 필요함

        Hacker News 의견

     * 두 가지 사실이 동시에 성립될 수 있다고 생각함.
         1. LLM은 새로운 기술이고, 이제는 되돌리기 어려운 지니와 같음. 그것이 가져올 시간 절약 효과나 사회적 문제를 감안할 때 앞으로 어떤 형태로든 존재하게 될 미래가 충분히 상상됨
         2. 거의 3년이 지난 지금, LLM에 투자한 기업들은 엄청난 훈련과 호스팅 비용을 정당화할 비즈니스 모델을 아직 찾지 못하고 있음. 대부분의 소비자 사용이 무료 티어에 집중되어 있고, 업계에서는 처음으로 투자를 줄이려는 조짐도 보임. 모델 기능 또한 전반적으로 정체 상태에 도달했고, 많은 이들이 결과물이 진부하고 소비하기 불쾌하다는 데 동의하고 있음
            초음속 여객기와 같이 ‘불가피해 보이던 기술’이 충분한 비즈니스 수익이 없으면 사라지거나, 전자레인지처럼 특정 사례로 자리잡는 경우가 많음. 충분히 수익성 있는 모델이 없을 때, LLM은 현재와 비교해 덜 특별하고 덜 거슬리는 위치에 안착할 것 같음. 억지로 모든 곳에 LLM을 넣는 시도는 별로 좋은 평을 받지 못하고 있음
          + 초음속 여객기를 예로 든 비유가 AI나 심지어 컴퓨터 및 인터넷 전체에도 해당된다고 생각함.
            예전에는 초음속 여객기 기술이 경이롭고 필연적으로 확장될 것처럼 느껴졌지만, 그 이면에는 해당 시대의 기술로 해결할 수 없는 문제와 수익성 부재가 감춰져 있었음.
            컴퓨터와 인터넷이 항공우주산업과 비슷한 흐름을 탈 수도 있다는 생각이 듦. 우리는 이미 기술의 정점에 거의 다다른 것일 수 있음.
            1970년대에 타임머신을 타고 가서 50년 후인 2025년엔 초음속 여객기가 사라지고, 항공산업이 변한 것 없이 더 짜증나는 형태로만 남았다고 해도, 아무도 믿지 않았을 것임.
            그래서 2075년에는 LLM을 주제로 한 다큐멘터리를 보면서, 왜 그렇게 유망해 보였던 기술이 거의 사라졌는지 회상하고 있을지도 모름
          + “대부분의 사람들이 LLM 결과물이 진부하고 소비하기 불쾌하다고 동의한다”는 주장에는 크게 동의하지 않음. 실제로 사람들은 LLM의 결과물을 상당히 좋아해서 ChatGPT가 역대 가장 빠르게 성장한 앱임. Perplexity 같은 AI 앱도 Google의 검색 지배력을 위협하기 시작함.
            물론 대중이 ChatGPT가 쓴 소설이나 시집을 일부러 구입하진 않을 테지만, 그 결과물이 읽기 어렵거나 반감만 주는 건 아님. 명확하고 읽기 쉬운 요약과 설명을 만들어내는 것도 부정하기 어려운 사실임
          + 당신의 두 번째 주장에 혼란을 느낌. LLM 회사들이 지금 모델에서 수익을 못 내고 있다는 게 맞는지 모르겠음. OpenAI는 연간 100억 달러의 ARR과 1억 MAU를 기록함. 물론 지금은 손해를 보고 있지만, 이는 모델 개선을 위한 출혈임. 만약 오늘 당장 모델 개선을 멈추고, 운영비 최적화와 대규모 사용자 기반 수익화에 집중한다면 성공적인 비즈니스 모델이 없다고 말할 수 있을지 의문임. 이미 사람들은 매일 이 도구를 사용하고 있음. 이것은 필연적임
          + 본문의 논지는 기본적으로 “AI를 일반 기술로 보는 관점”의 주장과 같음
            AI as Normal Technology
            참고 토론 링크
          + “필연적이라 여겨지는 기술이 결국 비즈니스 수익성 부족으로 퇴보한 사례가 많다”는 부분에서, 120개 이상의 케이블 TV 채널도 출시 당시에는 좋은 아이디어처럼 보였겠지만, 실제로는 LLM과 비슷하게 대다수의 콘텐츠가 아무도 관심을 가지지 않는 것이었음
     * 현대 세속적 시대의 부정적 결과 중 하나는, 매우 지적이고 사려 깊은 사람들이 수천 년 간의 철학과 종교적 사유를 구시대적이거나 더 이상 쓸모없다고 치부하며 쉽게 무시하는 경향임. (참고로, <A Secular Age>라는 책을 매우 추천함.)
       이런 태도는 사람들이 세상과 미래에 관한 반복되는 심리 패턴을 인식하지 못하게 만들고, 그 인식을 바탕으로 자신의 입장을 조정하지도 않게 만듦
       예를 들어, AI 필연론(inevitabilism)은 종교개혁 시기의 예정설(predestination)과 크게 다르지 않음. 역사가 미리 정해진 경로를 따라간다는 생각은 신에서 기술로 주체만 바뀐 심리적 동일선상임. 이는 자유와 책임을 모호하고 강력한 힘(이제는 기술)에 떠넘기는 심리 구조임
          + AGI가 곧 대세가 될 거란 주장에는 회의적이지만, 성장기에 신학 서적을 많이 읽어 본 입장에서 LessWrong 같은 인기 에세이를 종교적이라거나, 독서량이 부족한 사람의 글이라고 생각하지 않음. “그들에게 새 신이 생겼다!”는 관점은 흔한 논점 흐리기임. 물론, AGI 필연파 중 일부에게는 맞는 비유일 수도 있겠으나, 가장 약한 주장에만 초점을 맞추는 건 의미 없다고 봄
          + 테크노 칼빈주의자(Techno Calvinists) 대 러다이트 개혁주의자(Luddite Reformists)라는 이미지가 떠오름
            이런 경향이 거대담론이나 이데올로기 부재에서 기인한다고 생각함. 철학/종교적 사유에 무관심하지만 뭔가 새롭게 만들고 싶어하는 똑똑한 테크 인구가 많음
            이들은 점점 더 많은 돈을 쫓으며, 결국은 돈을 좇는 행위가 허무하다는 걸 일부 깨달음. 하지만 스스로가 이 인류 보편 문제 위에 있다고 착각함
            이런 왜곡 속에서, 기존의 예술을 재활용하고, 시간이 갈수록 더 나빠지는 앱을 만들고, 인류 개선이라는 본래의 창작의 기쁨을 외면하며 부의 쟁취에만 집중하게 됨
            LLM과 AI는 지니를 병 밖으로 꺼낸 것과 같으나, 실상은 전기보다 선형 원근법이나 인쇄기에 더 가깝게 자리잡을 것 같음. 오늘날 문화 속에서는 레오나르도 다 빈치가 평생 선형 원근법 튜토리얼만 팔던 셈임
          + 이건 완전히 새로운 현상이 아니며, 예정론자들이 자신들의 논지를 뒷받침할 새로운 ‘공포의 대상’이 추가된 것임
            내 목적은 그저 이런 현상을 지적하는 것임. 사람들은 물리나 종교에서 나오는 예정론은 거부하면서도, ""AI는 불가피하다""는 주장에는 여전히 감탄하는 경향이 있음
          + 기사에서 주장하는 핵심은 '필연론(inevitabilism)'이란 자신에게 유리하게 대화를 이끄는 수사적 전략일 뿐, 비판을 “현실 부정”이라 치부하고 논외로 만드는 도구임. 종교개혁 이데올로기와의 비교는 별 의미가 없다고 봄
            또, 여기서 제시하는 '세속적 예정론' 비유에도 아이러니함이 있음.
            프로테스탄트 예정설은 자유와 책임 회피와는 다름. 예정설의 핵심은 신의 은총은 ‘받는 것’이지 ‘얻는 것’이 아니며, 이를 이유로 무위도식하는 것도 아님. 오히려 자신의 선행을 통해 구원의 증거를 확인하고자 노력하게 됨.
            이는 “당장의 보상을 바라지 않는 근면성”으로 연결되고, Max Weber의 저서에서 초기 자본주의의 구동 원인으로도 분석됨
            따라서 예정론과 “기술 필연론”은 실제로는 아주 다른 개념임
          + 역사주의(예: 헤겔의 ‘역사의 필연적 법칙’)에서도 이와 비슷한 논의를 찾을 수 있음
     * 자녀 세대나 손주 세대에는 미국이 서비스와 정보 경제가 모든 제조업을 해외에 넘겨주고, 엄청난 기술력이 소수에게 집중되어 있는 사회가 될 것이란 예감을 가짐
       공익을 대표하는 누구도 기술 문제를 이해하지 못하고, 사람들이 자신만의 의제를 세우거나 권력자를 비판할 지식도 잃음
       사람들은 크리스털을 쥐고 점성술에 의존하며, 비판적 사고력은 쇠퇴하고, 기분 좋음과 진실의 경계조차 흐려진 채 거의 눈치채지 못한 채로 미신과 어둠 속으로 미끄러져 들어갈 것임
          + 이 인용문이 여기에 적합하지는 않다고 생각하지만, 출처에 관심 있는 사람을 위해 밝히자면, The Demon-Haunted World에서 가져온 것임
          + 이 글귀를 읽으니 목소리까지 귀에 들리는 듯함
            남들이 아무리 따라하고 흉내 내도 결코 진부해지지 않는 독특한 어투가 있음
     * 2009년에 스마트폰의 지배가 불가피하다고 주장했다면, 본인이 이미 스마트폰을 써보고 그 파워를 직접 체험했기 때문이지, 무슨 의도를 가지고 자유의지를 왜곡하려 했던 것은 아님
       2025년에는 진짜로 AI를 활용해서 실질적 업무를 해 본다면, 이 기술의 대규모 도입이 피할 수 없음을 부정할 수 없을 것임. AI가 역사상 어느 때보다 빠르고 강하게 다가오고 있음. 두렵다는 이유만으로 이를 외면할 수는 없을 것임
          + 80년대에 AI가 필연적이라고 주장하고 투자했거나, 10년 전에 VR이 대세가 될 것이라고 믿었던 사람들은 결과가 어떻게 됐는지 보면 알 수 있음. Zuck이 아직도 수십억 달러를 태우는 중이고, Apple도 수요 예측을 완전히 빗나감.
            AR이 VR의 구원책이 될 수 있지만, 소비자 시장 입성은 멀었고, VR용으로 쌓인 대부분 기술은 AR과 직접적 연관이 없을 것임
            Tesla의 자율주행 Robo-taxi 신화 역시 10년 째지만, 실제로 주인 없는 상태에서 수익을 내는 Tesla는 전혀 없음
            돌이켜 보면 이미 성공한 기술만 예로 드는 건 어리석음. 성공하지 못한 기술도 수두룩했고, 투자 및 산업 거품도 많았음
          + 이 논법이 바로 본문에서 언급된 수사 전략임
            아주 혁신적이고 필연적이라 여겨진 이동수단이 있다고 떠들던 시절을 떠올려보자. 하이프, 비밀 회의, 엄청난 기대… 결국 그 결과는 세그웨이였음
          + 일종의 자기 성취적 예언처럼 느껴짐. 대형 IT 기업들이 'AI'를 모든 제품에 억지로 넣으면서 “봐라, 이렇게 널리 쓰이고 있으니 필연적이다!”라고 주장함
            AI가 필연적이라고 나도 생각은 하지만, 현재는 너무 집단사고가 심해서, 모든 게 말풍선 기반의 에이전트 UI로 표현되고 있음
            곧 모두가 이것을 지나치고 나면, 그다음을 발견해서 기대되는 마음임
          + 1950년에 누군가 스마트폰이 주류가 될 거라고 말한다면 대부분 쉽게 믿었을 것임. SF 소설이나 영화에도 그런 미래가 자주 등장함
            하지만 소셜 미디어 이야기는 반응이 달랐을 것임. 어떤 이는 멋지다고 생각해도, 누구는 디스토피아라고 봤을 것임
            실제로 이 세 가지(스마트폰, 소셜 미디어, AI)는 50년대 이전부터 사람들의 상상력을 자극했음
            실제로 AI는 고도화된 커뮤니케이션 기기라기보다는, 상상 속의 소셜 미디어에 좀 더 가까움
          + 1950년대에는 핵 기술 또한 필연적이라 여겨졌음. 심지어 우라늄 유리로 만든 식기도 팔렸고, 아직도 집 안 선반 어딘가에서 빛나고 있거나 이미 깨졌을 수도 있음
     * “LLM 겨울”이 다가올 수도 있음
       사람들이 LLM이 실제로 무언가를 ‘할 수 없다’는 사실을 깨달을 때 그렇겠음
       기업들이 LLM의 실수 책임을 소비자에게 떠넘기려는 움직임이 나타날 것임
       “모르겠다”, “이 작업을 할 수 없다”와 같은 출력을 솔직하게 내놓는 시스템이 필요함
       이미 프로그래머들에게 LLM 사용이 오히려 부정적인 가치를 준다는 보고도 나오고 있음
       LLM이 남긴 흔적을 처리하는 데 너무 많은 시간이 듦
          + LLM의 실수 책임을 소비자에게 넘기는 건 기업만의 일이 아님
            이 포럼의 열성 사용자들(또는 어쩌면 사이버 홍보꾼)도 이런 태도를 보임
            LLM에서 가치를 끌어내려면 어느 정도의 지식과 '프롬프트 엔지니어링'(이제는 '컨텍스트 엔지니어링'이란 이름까지 얻음) 역량이 필요하다고 주장함
            결국 이 도구가 시간 낭비라고 느끼는 사람과, 큰 생산성 향상을 느끼는 사람을 나누는 기준이 오로지 사용자의 실력이라는 식임
            이런 내러티브는 블로그, 포럼, 심지어 최근 METR 연구 결과의 오해 해석까지 곳곳에 녹아있음
            물론 어떤 도구든 완전히 활용하려면 일정 수준의 숙련도가 필요함
            하지만 LLM을 쓰고도 이득을 보지 못하는 사람이 능력이 없어서 그렇다고 싸잡아 말하는 건 모욕적임
            LLM은 특별한 엔지니어링 전문성이 필요한 외계 기술이 아님
            올바른 질문을 던지고 도구와 개념을 조금만 익히면 누구든 익힐 수 있음
            이런 식으로 주장하는 사람들은 결국 LLM을 팔려 하거나, 기술의 과대효과를 부풀리려는 목적임
          + 사람 자체도 믿을 수 없기에 가드레일, 점검, 감시, 감사를 둠
            소프트웨어에서는 코드 리뷰, 테스트, 모니터링 등 베스트 프랙티스가 있는 이유임
            그래서 LLM이 소프트웨어 개발 분야에서 가장 빠르게 뿌리내릴 수 있었음
            이미 신뢰할 수 없는 인간 ‘작업자’를 위한 대처법이 있고, 그 경험을 LLM에게 응용하면 됨
            궁극적으로 성공적인 LLM 응용을 위한 핵심은, 비즈니스에 특화된 가드레일을 걸고 유사시 인간이介入하는 시스템을 구축하는 것임
          + LLM의 행동을 강제로 올바르게 만드는 시스템에 넣는 것이 필요함
            예를 들어 LLM이 문서나 man 페이지를 참고하도록 하고, 특정 라인 출력만 하라고 시킴
            그러면 시스템이 실제로 해당 줄을 찾아 인용하고, LLM이 임의로 인용을 만들어낼 수 없음
            LLM이 아직 타입 시스템과 통합된 사례는 없음
            강력한 타입 시스템(예: 종속 타입)은 ""이 함수는 항상 정렬된 리스트를 반환한다""는 것을 컴파일 단계에서 보장할 수 있음
            증명 코드를 직접 많이 써야 하지만, 만약 LLM이 그 증명을 대신 써 준다면, 컴파일이 되는 한 올바르다고 믿을 수 있음
            물론 메모리가 부족하거나 정전 등의 예외는 존재함
          + 이 ‘품질 낮은 결과물 찍어내기’ 열풍이 빨리 끝나길 바람
            그러나 품질에는 아예 관심 없는 사기꾼, 스패머, 클릭베이트 블로거, 선거에 개입하려는 이들, 저질 앱/음악/영상/“예술”로 광고수익 노리는 사람들에게는 지금의 Gen AI가 완벽한 제품임
            품질을 중시하는 사람들이 AI가 쓸데없다는 걸 깨달아도 이미 인터넷은 죽었을 것임
            그 땐 이미 ‘탈진실’, ‘탈예술’, ‘탈기술’, ‘탈민주주의’ 시대이고, 크게 이득을 본 건 캘리포니아 억만장자 몇 명뿐임
            똑똑한 사람들이 사회적 가치를 망치는 잡쓰레기를 만드는 데 재능을 쓰는 모습을 보는 것만큼 우울한 일도 없음
     * 90년대에 친구에게 인터넷을 처음 들었고, 대학에 다니는 누군가가 인터넷을 보여줄 수 있다는 말을 듣고 1시간 뒤 대학 컴퓨터 앞에 앉음
       링크를 클릭하고, 읽을 수 없을 정도로 빠른 속도로 쏟아지는 텍스트, 멋진 레이아웃, 이미지, 다른 웹페이지로의 링크까지. 인쇄, 배송, 기다림 없이 바로 확인하는 모습에 충격을 받음. 이것이 미래라고 확신했고, 그냥 불가피해 보였음
       어제는 큰 라이브러리 기반으로 프로그램을 통째로 다시 써야 해서, 긴 문서 읽거나 코드를 직접 뜯어보아야 할 상황이었음
       대신 GPT 4.1에 프로그램과 라이브러리 전체를 복붙해서 다시 써 달라고 했더니, 한 번에 성공했고, 15분 만에 전체 변경 사항을 읽고 몇 가지 스타일만 수정해서 끝냄. 몇 시간은 절약함. 이것이 미래라고 느꼈고, 역시 불가피해 보였음
       P.S. 많은 답글이 내 경험을 LLM과 대화하며 점진적으로 코드를 변경하는 방식(‘agentic coding’)과 비교하는데, 내가 하는 방식은 ""한 번에 한 파일, 코드는 건들지 않음""임. 자세한 내용은 여기 참고
          + 완전히 동의하지만, 사실은 IDE로 프로그래밍을 하는 것이 미래라고 주장하는 것과 다를 바 없다고 생각함
            필연론의 핵심은 새롭고 강력한 개발 도구가 몇 시간씩 생산성을 올려준다는 점이 아니라, 지식의 브로커가 누구냐, 지식노동이 어떻게 정의되고, 고용주와 고용인의 관계, 감시 수단 등 사회 시스템의 작동이 어떻게 바뀌냐에 있음
            필연론을 유포하는 이들은 완고한 개발자를 설득하려는 게 아니라, 자신에게 유리한 새로운 '게임판'을 만들려는 것임. 여기서 규칙을 싫어하거나 반대하면 ""할 수 없어, 이건 불가피해, 원래 그런 거야""라는 논리로 일축함
          + LLM의 문제는 창의적 사고나 사유에 쓸 때임
            실제로 많은 맥락(특히 코딩)에서 유용하지만, 그렇다고 해서 LLM이 ‘모든 것을 바꿀’ 기술은 아님
            “AI는 새로운 전기”라는 주장도 과장임(AI는 전기처럼 되지 않을 것이라는 점을 강조한 Andrew Ng의 발언 인용)
            오히려 “AI는 새로운 VBA”에 가깝다고 봄. 그 때도 “이제 모두가 프로그래밍 가능!”이라며 들떴지만, 실제로는 자질구레한 자동화에만 크게 파급력이 있었음. 물론 지금은 훨씬 더 빠르고, 하이프도 훨씬 크지만 본질은 비슷함
          + LLM이 항상 잘 동작하는 게 아님
            예를 들어 최근 VirtualBox VM이 Windows 10에서 4배 느려지는 이상현상이 있었음
            AI의 도움을 받아 여러 해결책을 따라갔지만 소용 없었음
            결국 Windows Features의 ""Virtual Machine Platform"" 체크박스가 갑자기 해제된 걸 찾아냄
            AI에게 이걸 언급하자, 해당 옵션은 필요 없고 상태가 '해제'된 게 더 낫다고 우겼음
            하지만 실제로는 그 문제였고, 옵션을 체크하고 재부팅하니 정상화됨
            AI는 기본 상식을 바탕으로 깊은 추론을 해야 할 때뿐 아니라, 단순한 연상 기억조차 뻔히 틀림
            웹검색 대용으로 쓸 거면 반드시 사실 확인이 필요함
            LLM 기반 AI에는 ‘사실’ 개념이 없음. 토큰 예측일 뿐이고, 입력/학습 데이터에 따라 우연히 정답일 확률이 높은 출력을 산출할 뿐임
          + LLM 불가피론에는 전적으로 동의함. 묘사처럼 모든 사람이 매일 쓸 미래가 필연이라고 생각함. 스마트폰처럼 말임
            하지만 AGI 불가피론에는 동의하지 않음. “모델이 계속 발전하니까 AGI는 불가피하다”는 주장은 결과로부터 논리를 비약하는 것임
          + 코드가 정말 제대로 동작하는지 확신 있나?
            만약 AI만 써서 코드를 읽는 법을 배우지 못했다면?
            직접 코드를 읽어서 검증하는 능력이 훨씬 더 중요하다고 봄
            예전 직장에서, 항공기 회사의 플랩 제어 펌웨어 버그를 찾아내고, 문제 설명을 위해 직접 비행기를 타고 가는 길에, 바로 그 문제 펌웨어가 쓰인 비행기를 타고 있었다는 일화를 들었음
     * 이 필연론 논리의 가장 어려운 부분은, “불가피하다”고 주장하는 사람들이 실제로 수억 달러를 개발·활용·광고에 쏟아붓는 이들이기도 하다는 점임
       여우들이 닭장에 문을 만들면서 “여우가 들어가는 건 막을 수 없어. 그렇다면 모두를 위한 시스템이 되게 만드는 게 낫다”고 말하는 셈임
          + “말보다 행동으로 보여라(put your money where your mouth is)”는 전략 자체는 괜찮다고 생각함
          + 그들이 ‘문’을 만들고 있는 것일까, 아니면 이미 문이 있으니 우리(자기들)가 첫 번째로 들어가려는 걸까 하는 의문이 있음
          + 동의함. 우리는 여우를 돕지 말고, 오히려 여우를 쫓아내야 함
     * 두 가지가 아주 분명함
         1. LLM은 기존 컴퓨팅 기술로 할 수 없는 많은 일을 할 수 있고, 이 역량을 가장 잘 활용할 방법을 찾는 데 시간이 필요함
         2. 엄청난 영향력을 지닌 많은 이들이 실제 결과와 무관하게 하이프에 편승할 동기가 큼
            기사에서 말하는 대로, CEO들이 짜놓은 틀 안에서 논쟁하는 것은 무의미하다고 봄. 그들은 주로 시장을 상대로 얘기하는 것이고, 우리는 기술의 작동 원리를 아는 사람들이니, 더 객관적으로 LLM을 평가하고 자체적인 프레임을 만들어야 함
            내 기준에서 LLM은 소프트웨어 도구의 한 진화 단계임. LLM이 쉽게 제대로 된 코드를 써 주는 게 신기하고 위협적으로 느껴질 수 있지만, 수십 년간 반복적인 CRUD와 비즈니스 로직이 끝없이 요구되었고, 그 다양한 사례로부터 거대 확률 생성기가 충분한 맥락과 프롬프트만 주어지면 새 조합을 만드는 데 성공하는 것은 놀라운 일도 아님
            나는 기술자로서 LLM이 내 목표에 어떻게 도움이 될지 이해하고 싶음. 원치 않으면 안 쓰면 되고, 변화하는 역량을 꾸준히 확인해줘야 내린 결정이 현명할 것임
            거대한 하이프를 앞두고 과거에 대한 향수나 바람직함에 집착해 쓸데없는 십자군 전쟁은 하지 않을 것임
          + 나도 이런 식으로 도구를 대하고 있음. 건강한 접근법이라고 생각하지만, 내가 그저 투덜거리는 사람일지도 모름
     * 사람들은 자연어로 소통하는 걸 좋아함
       LLM은 기존의 논리언어나 컴퓨터와 소통하는 복잡한 인터페이스에서 벗어나, 인터넷 초창기의 컴퓨팅 방식을 끝내는 첫걸음임
       불가피론이 여기서 나오는데, 사실 대부분의 사람은 컴퓨터 사용법을 배우고 싶어하지 않고, 마치 살아 있는 존재처럼 그냥 말로 대화하고 싶어함
       (컴퓨터를 진심으로 좋아하는 건 5% 미만일 거라 가정함)
          + 사람들은 또한 신뢰할 수 있고 결정적인 반응을 기대함. 버튼을 누르면 거의 항상 같은 결과가 나타나길 원하고, 10% 확률로 엉뚱한 일이 발생하는 건 원하지 않음
            LLM이 과연 이런 안정성까지 도달할 수 있을지는 아직 불명확함
          + “논리 언어에서 벗어난 첫걸음”이라니… 논리 언어는 기본적으로 결정적 구현을 가능하게 만들어 추상화 구축에 큰 도약이었음
            자연어는 그 목표와 너무 동떨어져서 쉽게 설명할 수조차 없음
            “사람도 언어로 추상화를 달성하지 않나?”라는 생각은, 실제 법률 문서(예: 하원 법안 전문)을 한 번이라도 읽어보면 달라질 것임
          + 자연어 UI의 실현이 생각보다 크게 환영받지 못하는 게 오히려 의아함
            자연어 UI는 컴퓨터공학의 오랜 미완의 꿈이었고, 이제는 너무 당연하게 된 느낌임
            LLM이 코딩/글쓰기/연구 등에 가장 좋지 않을 수 있지만, 이런 사용자 경험(UX)은 꼭 유지되었으면 함
            문제를 언어로 자유롭게 표현하고, 축약/은어/톤까지 정확히 전달하는 건 정말 놀랍고, 실용성도 큼
          + “일상 언어는 물리학자가 말하려는 만큼 추상적으로 표현하기에는 완전히 부적합하다. 오직 수학과 수리 논리만이 물리학자의 의도만큼 함축적으로 표현할 수 있다.”
               o 버트런드 러셀, The Scientific Outlook (1931)
                 우리가 수학에서 더 이상 자연어를 쓰지 않는 이유가 있음. 자연어는 너무 장황하고, 매우 부정확함
          + 자연어로 계약을 설명할 수 있는 방법이 있었으면, 변호사들이 벌써 찾았을 것임.
            계약서 해석 차이로 GDP의 상당 부분을 낭비하고 있음
     * 이 개념은 티모시 스나이더(Timothy Snyder)가 언급한 ‘불가피성의 정치’와 밀접하게 관련됨
       “불가피성의 정치는 미래가 단지 현재의 연장선이고, 진보의 법칙이 이미 알려져 있고, 대안은 없으며, 따라서 우리가 할 일은 아무것도 없다”는 일종의 세계관임
       본문 기사는 이 개념을 상업적 영역에 적용하지만, 본질적으로는 ‘주체성(Agency)’을 박탈시키는 언어를 다루고 있음
       관련 기사 링크
"
"https://news.hada.io/topic?id=22093","AI 발전을 따라잡는 나만의 방법 (그리고 당신도 꼭 해야 하는 이유)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                AI 발전을 따라잡는 나만의 방법 (그리고 당신도 꼭 해야 하는 이유)

     * 생성형 AI는 가장 빠르게 진화하는 기술로, 과소평가와 과대평가 모두 위험함
     * 공신력 있는 출처와 신뢰할 수 있는 전문가를 꾸준히 팔로우하는 것이 필수적임
     * Simon Willison’s Blog, Andrej Karpathy, Every’s Chain of Thought 등 균형 잡힌 정보 파이프라인 추천
     * 공식 AI 연구소 발표, 엔지니어링 블로그, 논문은 기술 실제 진보와 한계를 파악하는 데 필수임
     * 트위터/X, 뉴스 큐레이션, 전문가 리스트 등 다양한 채널로 최신 흐름과 심층 분석 모두 접근 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: AI의 오해와 정보 오염

     * 생성형 AI는 필자의 일생에서 가장 빠르게 발전하는 기술임
     * 하지만 AI에 대한 오해 역시 만연하며, 정보 환경 자체가 매우 혼란스러운 상황임
     * 관련 기술을 충분히 이해하지 못한 기업, 정부, 혹은 사람들이 오용하거나 심각한 피해를 초래하는 사례가 실제로 발생함
     * AI를 과소평가하거나(“곧 사라질 유행” 취급) 과대평가(“프로그래머가 더 이상 필요없음”)하는 양극단 오해가 존재함
     * 올바른 기술 이해 부족이 이런 오해의 근원임

AI 정보 습득의 접근법

     * AI 관련 정보를 올바르게 이해하는 일은 생각보다 쉽지 않음
     * 왜곡된 정보, 과장 혹은 억압된 담론이 일상적으로 노출되는 환경임
     * 의도적이고 체계적으로 정보를 선별하지 않으면 오류, 과장, 왜곡에 쉽게 노출될 위험이 있음
     * 필자는 균형 잡힌 정보 파이프라인을 구축하여 도움을 받고 있으며, 이를 초심자에게 좋은 시작점으로 추천함

정보 습득의 일반 원칙

     * 원천 자료와 가까운 정보를 참고하는 것이 중요함
          + AI 연구소의 공식 발표나 주요 인물의 의견을 1차 소스로 확인해야 함
          + 2차, 3차 보도를 신뢰하지 않는 자세 필요
     * 신뢰할 수 있는 전문가의 코멘트를 적극적으로 찾아 참고해야 함

추천 정보 출발점

     * Simon Willison’s Blog
          + Simon Willison’s Blog (AI 태그)
          + 기술자에게 가장 추천되는 출발점, AI 프런티어, 응용, 보안 및 윤리 문제 폭넓게 다룸
          + Django, Datasette 창시자로도 유명함
          + 예시: The Lethal Trifecta, LLMs in 2024
     * Andrej Karpathy
          + Twitter/X | YouTube
          + OpenAI 창립 멤버이자 Tesla AI 총괄 디렉터
          + AI 모델 내부구조와 원리를 가장 쉽게 배울 수 있는 채널, 문화적 영향 및 AI가 미치는 사회적 변화 관찰 포함
          + 예시: Deep Dive into LLMs like ChatGPT, How I Use LLMs

  Every’s Chain of Thought

     * Every’s Chain of Thought
     * Dan Shipper(Every 공동창업자)가 집필하며 실전 AI 사용기과 벤치마크 외의 실제 모델 경험을 제공함
     * 예시: Vibe Check: Codex, Vibe Check: o3

공식 AI 연구소 자료 모니터링 하는 법

     * OpenAI(News), Google DeepMind(DeepMind), Anthropic(News), Meta AI(Blog), xAI(News), Qwen(Activity) 등 공식 블로그·뉴스·모델 카드·엔지니어링 가이드·논문 정기적으로 확인
          + 예시: OpenAI o3 공식 발표, Claude 4 System Card
          + 엔지니어링 블로그: Anthropic Engineering, OpenAI Voice Agent Guide, Gemini Cookbook
          + 논문 예시: DeepSeek R1 논문, Anthropic Biology 논문
     * 소규모 연구소: Nous Research, Allen AI, Prime Intellect, Pleias, Cohere, Goodfire 등도 참고할 만함

주목할 AI 전문가와 실전 엔지니어

     * 오픈소스 도구 제작, AI 엔지니어링 실무 경험 보유 전문가들의 정보가 공식 가이드보다 실질적으로 도움이 되는 경우가 많음

  대표 추천 인물 및 블로그

     * Hamel Husain
          + Your AI Product Needs Evals, LLM Eval FAQ
     * Shreya Shankar
          + Data Flywheels for LLM Applications, Short Musings on AI Engineering and “Failed AI Projects”
     * Jason Liu
          + The RAG Playbook, Common RAG Mistakes
     * Eugene Yan
          + Task-Specific LLM Evals that Do & Don’t Work, AlignEval, Intuition on Attention
     * What We’ve Learned From A Year of Building with LLMs
          + 실전 구축 경험자가 작성한 LLM 적용 사례 아카이브
     * Chip Huyen
          + Common pitfalls when building generative AI applications, Agents
     * Omar Khattab (트위터)
          + A Guide to Large Language Model Abstractions, 트위터 포스트
     * Kwindla Hultman Kramer (트위터)
          + Voice AI and Voice Agents: An Illustrated Primer, Advice on Building Voice AI in June 2025
     * Han Chung Lee
          + MCP is not REST API, Poking around Claude Code, MLOps Lessons from ChatGPT’s ‘Sycophantic’ Rollback
     * Jo Kristian Bergum
          + Search is the natural abstraction for augmenting AI with moving context
     * David Crawshaw
          + How I program with LLMs, How I program with Agents
     * Alexander Doria / Pierre Carl-Langlais
          + The Model is the Product, A Realistic AI Timeline
     * Nathan Lambert’s “Interconnects”
          + What comes next with Reinforcement Learning, Reinforcement learning with random rewards actually works with Qwen 2.5
     * Ethan Mollick
          + Using AI Right Now: A Quick Guide, Making AI Work: Leadership, Lab, and Crowd
     * AI Snake Oil – Arvind Narayanan & Sayash Kapoor
          + AGI is not a milestone, Evaluating LLMs is a minefield

AI 뉴스/미디어 및 커뮤니티 활용

     * Twitter/X
          + Twitter/X 리스트
          + AI 실시간 토론 및 정보 습득의 중심, 각종 소식 및 논쟁 포착
               o 트위터 피드를 신문 읽듯 15~20분 소화, 필요시 인물·출처 추가 팔로우
     * Shawn Wang(swyx) / smol.ai
          + swyx 트위터 | AI News by smol.ai
          + Latent Space 뉴스레터
          + 트위터 대체 소스로 매일 AI 동향 요약 제공
     * Dwarkesh Patel
          + Dwarkesh Patel 블로그/팟캐스트

AI 심화 토론/자료 커뮤니티

     * LessWrong (AI Alignment), AI Alignment Forum: AI 안전성, 거버넌스, 기술적 분석 등
          + 트위터 등 주류에서는 잘 다루지 않는 고난이도 논의가 많이 나옴
          + 예시: Claude plays Pokémon breakdown, The Waluigi Effect
     * Gwern: AI, LLM, 트랜스포머 등 백과사전급 심층글
          + 대량의 AI 관련 글과 LLM 스케일링 등 예측적 통찰이 담긴 글들을 제공
          + 예시: The Scaling Hypothesis, You could have invented transformers 튜토리얼
     * Prompt Whisperers/Latent space explorers
          + Janus 블로그, Wyatt Walls 트위터, Claude Backrooms
          + 예시: Anomalous tokens reveal the original identities of Instruct models, the void

정보 습득 실전 방법

     * 모든 소스를 완벽하게 챙길 필요 없이 트위터 피드 신문 읽듯이 접근하는 것이 실천적 방법
     * 흥미로운 글을 발견하면 해당 저자를 팔로우하고, 그의 다른 작업도 챙겨보는 식의 확장적 탐독도 추천
     * 과거 음악 탐색과 유사한 정보 탐색 방식임
     * 지적 탐색의 재미로 접근하면 의무감이 아닌 즐거움이 될 수 있음

결론 및 추천 리스트

     * 제공된 트위터/X 리스트를 통해 위의 전문가와 실무자를 한 번에 팔로우 가능함
     * 곧 RSS 포맷의 리스트도 추가할 예정

   트위터 리스트 바로가기

        Hacker News 의견

     * LLM이 어떻게 동작하는지 기본적으로 이해하고 있으면, PR 담당자, 블로거, 업계 리더, 인터넷 사상가들의 꾸준한 콘텐츠를 전부 따라갈 필요 없다고 생각함
       오히려 그런 인터뷰나 글을 따라가다 보면 실제로는 도움되지 않는 이상한 유행을 따라가게 될 위험이 있음
       실제로 모델 간 차이점은 몇 년 동안 정도의 차일 뿐 본질적 차이는 크지 않았음, 현재 대부분의 변화는 도구나 통합 작업에서 이루어짐
       LLM은 결국 ""텍스트 모델""이고 기저 지식 없이 생성됨을 항상 기억할 필요가 있음, 그럼 어디에 유용한지, 어디에는 적합하지 않은지 선별 가능함
          + 이런 의견에 정말로 동의함, 해당 블로그의 '핵심 신호(high signal)' 리스트도 실제로는 자기 홍보 인물이 대부분임(물론 일부는 좋은 분들이지만)이라 느꼈음, 인사이트보다는 '버즈'에 가까운 구성
            ""내 평생 AI가 가장 빠르게 발전한 기술이었다""는 주장도 개인적으로 크게 공감하지 못함
            나는 SVM이 뜨던 시절부터 ""신경망은 농담"" 취급당하던 상황, 그리고 딥러닝, 다양한 DL 프레임워크가 폭발적으로 늘어나던 10여 년을 겪었음
            당시도 10년 사이 정말 급격한 발전이 있었음
            웹에서는 JS가 오직 UX 보완용이던 게, 단일 페이지 앱이 표준이 되기까지 같은 변화였음
            핵심은 '핵심 인플루언서 리스트'에 오르는 게 아니라면, 그냥 묵묵히 본인에게 중요한 시점까지 기다리는 편이 훨씬 나은 전략임
            나도 backbone.js 시절 이후 10년간 웹개발 트렌드를 다 무시하고 있다가 React 필요해진 시점에 며칠 공부해서 바로 쓸 수 있었음
            LSTM도 5년 전에는 모두가 구현법을 배우려 하다가, 지금은 트랜스포머 때문에 구식이 됨
            커리어 내내 느낀 건, '빠르게 움직인다'는 건 '성숙하지 않다'는 의미임
            오히려 예전 통계 모델(GLM 등)과 그것들의 여전히 실질적인 활용법을 익히는 편이, 그때그때 유행하는 'prompt hack'을 좇는 것보다 현업에서 훨씬 생산적인 해결책이 되어줌
          + LLM 구조만 어느 정도 알면 LLM 관련 새 소식들은 대부분 두 가지 부류로 나뉨
            첫째는 기존 툴과 약간 다르거나, 약간 더 나은 성능을 내는 새 도구임(기존에 없는 기능이면 쓸 만하지만, 대부분은 곧 구식이 됨)
            Kimi-K2, GPT 4.1 같은 이름도 몇 달 후면 아무도 언급하지 않을 수 있음
            둘째는 진짜로 모델에 새로운 능력이 추가된 경우임
            예를 들면, RL(reinforcement learning), chain of thought, 실제로 동작하는 코딩 에이전트, 끝판왕급 멀티모달 모델, 똑똑한 도구 연동 등임
            이런 큰 도약이 있을 때만 주목하면 충분함
            실제로 500포인트 넘는 HN 글만 훑어봐도 최신 흐름은 자연히 알게 됨
            LLM의 진짜 역량을 익히는 최고의 방법은 블로그, 영상 따위가 아니라, 직접 만들어보고 부딪쳐 보는 경험이라 생각함
          + 전적으로 동의함. 학생들에게 항상 이렇게 강조함
               o 1. 남의 경험에 지나치게 몰두하지 말고, 자기만의 직접적 경험에 집중할 것
               o 2. 블로그 읽기보다는 직접 앱을 만들어볼 것
               o 3. 각자의 경험은 엄청난 차이가 있으니 남의 생각을 그대로 따르지 말 것
               o 4. 트위터, 서브스택에서 연구자나 개발자를 무턱대고 좇지 말 것(대부분 자기 쇼케이스임)
               o 5. 불안, FOMO에 시간 낭비하지 말고, 직접 해보면서 익힐 것. 진짜 중요한 변화는 어차피 나중에라도 반드시 알게 됨
               o 6. 정보를 아는 건 중요하나 정보 자체에 강박적으로 집착하지 말 것. 시간을 스마트하게 배분할 것임. 학생들에게 항상 이 포인트를 강조함
          + 사실 이런 점은 연구계도 마찬가지임
            원래 연구는 99%가 점진적 발전임(이것 자체가 괜찮은 현상임, 거기에 좌절하지 말 것)
            논문 대부분은 필요 이상으로 길고, 제대로 읽다 보면 수학적 직관만 있어도 어느 정도 예측 가능함(아이디어만 알면, 결과도 미리 가늠됨)
            분야가 빠르게 변한다고 느끼기 쉽지만 실제로는 속도가 그렇게까지 빠르지 않음
            나 역시 개인 사정으로 1년을 쉬고 돌아왔을 때, 크게 변한 게 없다는 점을 깨달았음
            이런 관점이 있으면 '유지'라는 압박에서 벗어날 수 있음
            지금 힘들다면 아직 전문성이 좀 부족하다는 뜻이지, 뒤처졌다는 의미가 아님
            누구든 뛸 줄 알아야 한 발짝 뒤따라도 갈 수 있듯, 조급함은 내 머릿속 걱정일 뿐임
          + LLM을 설명할 때 가장 흥미로워하는 부분은 아키텍처보다는 '토큰별 예측(오토리그레션)'과 반드시 확률이 제일 높은 토큰이 아니라 확률에 비례해 샘플된다는 점임
            실제 코어 알고리즘(다음 토큰 예측 방식) 자체는 대부분의 비전공자에겐 와닿지 않음
            dot product, embedding 같은 세부 내용은 아무도 신경 쓰지 않음. 설명해봤자 남기 힘들고, 큰 도움이 되지 않음
     * 반드시 '최신 트렌드'를 따라갈 필요 없음, 그저 느슨하게 관심 가지고 내 효율을 진짜로 높여줄 기능/기법만 선별적으로 실험하면서 쓸 것만 쌓는 게 좋다고 생각함(누가 X에서 뭘 추천했다는 등은 별로 신뢰 안함). 오히려 AI 과대망상에 부정적 시각인 연구자들의 의견을 들으며 많이 배울 수 있었음[https://x.com/burkov]. 현 상황에서 너무 많은 과장, 변화, 불확실성이 존재함. 진짜 혁신이 나타나면 HN(혹은 주류 커뮤니티)에서 반드시 들을 수 있음
          + 직접 몇 시간이라도 써보는 게 여러 시간 자료 읽는 것보다 훨씬 값진 학습임
     * 글이 ""왜""를 잘 설명하지 않아, 리스트의 ""어떻게""에 설득력이 떨어진다고 느낌. 나의 소중한 시간은 다른 데 써도 충분함
          + 진짜 뭔가를 '계속 따라가야 하나' 의문이 생김. 실질적으로 의미 있는 혁신은 결국 대중화되고 나에게도 자연히 다가옴. 엑셀이나 구글독스도 처음에는 별 관심 없다가, 대중화된 후에도 충분히 잘 익혀 쓰는 중임. AI 스타트업 런웨이가 얼마 안 남은 경우처럼, 급하게 쫓아가야 할 이유가 명확할 때가 아니라면 조급할 필요 없음
     * Gergeley Orosz의 ""Pragmatic Engineer"" 뉴스레터 구독 중임(요즘 AI 주제 많음), Gary Marcus의 서브스택도 체크함(좀 더 LLM 회의적 관점임)
       https://newsletter.pragmaticengineer.com/
       https://substack.com/@garymarcus
       또 Langchain, PydanticAI 같은 파이썬 패키지 뉴스도 자동으로 팔로업함(이런 프로젝트가 업계 실질 트렌드를 어느 정도 반영함). X(트위터)는 이제 안 쓰지만, Simon Willison 같은 분은 BlueSky, Mastodon에도 종종 글을 올림. Sebastian Raschka, Chip Huyen 등은 LinkedIn에도 포스트함. 여기저기 흩어져 있지만 결국 중요한 소식은 대부분 접하게 됨
     * 내가 중요한 내용을 놓치고 있었을 수도 있지만, 핵심적이고 의미 있는 업데이트는 자연스럽게 HN 메인이나 코멘트에서 언급되기 마련임
          + 업계 동향을 따라잡으려는 건 시속 140km 트레드밀 위에 뛰어오르는 것과 같음. 그냥 뛰는 것 자체를 포기함. AI(특히 LLM)는 일시적 유행이 아니라는 점은 동의함, 하지만 지금은 변화가 너무 심함. 꼭 필요해지기 전까지는 긴 시간 투자하지 않을 예정임. 몇 년 지나면 업계 판도가 더 명확해질 거라 기대함. 아니더라도, 그동안 빨리 무의미해질 트렌드에 시간 낭비하지 않을 수 있음. 참고로 나는 현재 AI나 LLM을 실제 업무에 전혀 활용하고 있지 않음
          + 회사들이 엄청난 투자와 R&D를 들인 만큼, 오히려 자기 기술을 감추려 하지 않음(비밀주의와는 반대임)
          + IT 분야 전체 트렌드에 대해 나는 모두 이렇게 대처함
     * LLM, 프런티어 AI 모델 소식을 주로 따라가고 싶다면 추천 리스트가 매우 훌륭함, 절반 이상은 나 역시 별도로 찾아낸 인물임
       X(트위터)에서 AI 계정 리스트를 만들어 팔로우 중이고, 전체적으로 가장 견고한 정보 소스임
       일부는 블로그, 팟캐스트 RSS도 쓸 만함(연구자라면 논문 자체 RSS는 필수적임)
       내가 추가할 만한 곳은 https://epoch.ai, 팟캐스트에서는 Dwarkesh Patel, 블로그는 Peter Wildeford, @omarsar0(DAIR elvis), 그리고 다양한 리서처 직접 팔로우임(중 일부는 정보 전달보다는 재미 위주)
       이 분야는 정보 환경이 심각하게 오염됨. 특히 NYT처럼 정치 위주로 다루는 언론만 팔로우하면 오히려 편향·부정확한 그림만 얻게 됨
       참고로 생성 AI와 직접 연관 없는 ML 분야(예: 단백질, 유전체, 날씨 모델, diffusion·이미지 생성 연구 등)는 정보 출처가 전혀 다름
       AI/ML 범주가 너무 넓어 한꺼번에 다 따라잡기는 불가능함
       꼭 따라가야 하냐면, 그럴 필요는 없음
       대부분은 그저 신기술/최신 트렌드에 대한 호기심으로 그런 것임
       다만 소프트웨어 개발과 같이 AI를 완전히 무시하면 중장기적으로 커리어에 치명적일 수 있음(도구 자체만 익히는 식으로 대응해도 괜찮음)
       나는 지금 전반적인 동향을 파악해야 하는 업무 특성상 꾸준히 트래킹함
     * Simon Willison의 블로그만으로도 최신 고품질 정보 학습에는 충분함(노이즈 거의 없고 S/N 최고임)
          + Simon에게 GitHub 후원($10 이상)하면 ""더 적은 양의 주요 요약본""을 메일로 받아볼 수 있음

     ""지난 한 달간 LLM 분야의 가장 중요한 발전 요약(10분 이내 완독 분량)""
     https://simonwillison.net/about/
          + https://simonwillison.net
          + 다행히 RSS 피드도 제공해서 코드/뉴스 모두 확인 가능함
     * 굳이 '트렌드' 따라갈 필요 없음
          + 맞음, 이제 그만두고 다른 일을 해볼 때임
          + 사실 그냥 테크 업계를 떠나 새 커리어를 만드는 것도 한 방법임
            테크 직종은 점점 줄고, 언젠가는 가파르게 감소할 것임(AI가 10명 몫을 한 명이 하게 되고, 이후 화이트칼라, 블루칼라(아마존 물류센터 로봇직 등)도 마찬가지)
            개인적으로 이번 주에 GPT Plus 구독 취소함, 더 이상 '그 괴물'을 키우고 싶지 않음
            지도(예: 여행 경로, 친구들이랑 고리 타기 코스 등) 지원 등 기능은 Gemini 무료 버전이 오히려 더 잘함
     * 첫 번째 추천만 따라가면(simow의 블로그 읽기), 웬만하면 충분함
          + 그조차 귀찮아서, 아침 커피와 함께 Fireship 동영상으로 대신함
          + 강력 추천 - 클릭베이트 거의 없이 매우 질 좋은 요약임
     * Andrej Karpathy의 강의로 엄청난 도움을 받았음
       유튜브에도 올라가 있음(https://www.youtube.com/@AndrejKarpathy)
       머신러닝 연구자들을 멘토링/도움 주는 입장이었지만, Andrej 목소리 들으면 내가 아무것도 모르는 학생처럼 느껴짐
       처음엔 이상했지만 지금은 그 겸손함 자체가 너무 소중한 자산임
       정말 '나는 아무것도 모른다'는 마음이 중요함

   ㅠㅠ 넘 가독성 없다

   이 정도를 가독성없다고 느끼신다면 본문의 문제가 아닐 것 같습니다.

   동감합니다

   A type of writing that's essential at this moment in time.
"
"https://news.hada.io/topic?id=22019","AI Git Narrator - AI 기반 Git 커밋 메시지·PR 설명 자동 생성 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           AI Git Narrator - AI 기반 Git 커밋 메시지·PR 설명 자동 생성 도구

     * OpenAI GPT, Google Gemini, Ollama(로컬 LLM) 을 활용해 Git diff와 커밋 기록을 바탕으로 자동으로 커밋 메시지와 Pull Request 설명을 생성하는 macOS 전용 CLI 도구
     * 복잡하고 반복적인 Git 메시지 작성 시간을 절약해주며, 팀 협업·코드 리뷰·기록 품질 향상에 매우 유용함
          + 여러 커밋을 분석해 PR 제목·설명을 풍부하게 생성하며, 전체 변경사항·스테이지된 변경·언스테이지된 변경별로 커밋 메시지 자동 작성
     * Swift로 작성되어 macOS 개발 환경에 최적화, Homebrew 설치 지원, 명령어 기반 커스텀 옵션(모델/토큰/온도/비교 브랜치 등) 제공으로 개발자 워크플로우에 자연스럽게 통합 가능
     * 로컬 LLM(Ollama) 지원으로 개인정보 보호, API 비용 부담 없는 오프라인 사용까지 가능
     * 복잡한 IDE 통합형 AI와 달리, 가볍고 목적이 분명한 Git 전용 AI CLI 툴

   요즘 딱 필요한 거였는데....

     macOS 전용 CLI 도구

   ㅠㅠㅠㅠㅠㅠ

   깃 데스크탑에도 비슷한 기능이 있는것 같더라고요.

   를 Linux용으로 포팅해줘! 라고 해주세요
"
"https://news.hada.io/topic?id=22028","파이썬으로 전향중이고, 생각보다 꽤 마음에 들어요 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      파이썬으로 전향중이고, 생각보다 꽤 마음에 들어요

     * 최근 AI 개발의 트렌드로 인해 본격적으로 파이썬 학습 및 사용을 시작했고, 이제는 그 생태계에 큰 만족을 느끼고 있음
     * Python은 과거보다 훨씬 빠르고 현대적인 언어로 발전했고, Cython을 통한 성능 향상 등 급격한 발전을 체감함
     * uv, ruff, pytest, Pydantic 등 최신 개발 도구와 라이브러리를 본인의 워크플로우에 적극 도입하여 개발 생산성을 높이고 있음
     * 프로덕션 환경과 Jupyter 노트북/스크립트 기반 개발 간의 차이를 줄이기 위한 프로젝트 구조 및 자동화 방안도 적용
     * GitHub Actions, Docker 등을 활용해 CI/CD, 테스트, 인프라 관리를 효율적으로 구축함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

I’m Switching to Python and Actually Liking It 요약

  왜 파이썬으로 전향했는가

     * AI 중심의 개발 환경에서는 Python이 사실상의 표준 언어로 자리잡고 있음
     * 과거에는 단순한 스크립트 작성에만 사용했지만, 최근에는 RAG, 에이전트, 생성형 AI 등의 “실전용 앱”을 만들기 위해 진지하게 사용하게 되었음
     * 그 과정에서 Python 생태계가 과거에 비해 매우 진화했다는 사실을 체감하게 되었음

  Python의 강점 3가지

    1. 풍부한 라이브러리와 도구 생태계: 데이터 처리, 분석, 웹, AI에 특화
    2. Cython 등으로 인한 성능 개선: 컴파일 기반 최적화 가능
    3. 개선된 문법 가독성: __init__, __new__ 같은 레거시 문법은 감춰지고, 더 직관적인 문법 제공

  프로젝트 구조 (Monorepo 기반)

     * 백엔드와 프론트엔드를 통합하는 모노레포 구조를 선호함
          + 코드 관리 효율성, 검색 용이성, 단순화된 배포 및 테스트 파이프라인 등을 이유로 단일 저장소를 선택함
          + 과도하게 프로젝트를 여러 저장소로 분리하는 것은 오버 엔지니어링의 신호라고 판단함
     * 전형적인 프로젝트 구조 예시는 다음과 같음
project/
├── .github/ : GitHub Actions 등 CI/CD 워크플로우
├── .vscode/ : VSCode 환경 설정
├── docs/ : MkDocs 기반 정적 문서 및 사이트
├── project-api/ : FastAPI 백엔드 및 데이터/노트북/툴/소스코드/테스트 포함
├── project-ui/  : React/Next.js 프론트엔드
├── docker-compose.yml (통합 실행)
└── Makefile 등 자동화 스크립트

     * API 서버에서 처리된 데이터를 프론트엔드가 HTTP 요청으로 가져오는 방식으로 책임 분리
          + 프론트엔드는 무거운 데이터 처리를 금지하고 파이썬 기반 백엔드 API에 HTTP 요청을 위임함
     * 각 파이썬 모듈 디렉터리는 __init__.py로 명확히 지정함
     * project-api 내부에는 src/app, notebooks, tools, tests 폴더를 분리해 구성

  주요 도구 및 설정

     * uv
          + Astral에서 제공하는 최신 파이썬 패키지 매니저 및 빌드 도구
          + 의존성 관리, 가상환경 생성, 프로젝트 초기화 등 대부분의 작업을 빠르게 처리함
          + pyproject.toml이 핵심 설정 파일로, 모든 메타데이터 및 의존성 정보가 통합됨
          + uv init, uv add, uv sync 명령어로 빠르게 프로젝트 환경 구성 가능
     * ruff
          + 초고속 파이썬 린터 및 코드 포매터
          + isort, flake8, autoflake 등을 통합한 도구
          + ruff check, ruff format 으로 린팅 및 자동 수정
          + PEP 8 코딩 스타일 가이드 기본 지원
     * ty
          + Astral이 만든 Python용 정적 타입 검사기
          + typing과 조합해 정적 분석, 초기 버그 방지에 효과적
          + 초기 개발 단계임에도 안정적으로 사용할 만한 수준임
     * pytest
          + 단위테스트 및 확장 가능한 테스트 환경을 제공하는 대표적인 파이썬 테스트 프레임워크
          + 간단한 파일 네이밍 규칙과 명령어 한 줄로 바로 통합 테스트 가능함
               o test_*.py로 테스트 구성 후 uv run pytest로 실행
          + 간결한 문법, 풍부한 플러그인 생태계
     * Pydantic
          + 데이터 검증 및 환경 설정 관리 라이브러리
          + .env 환경변수 기반 설정 로딩 및 타입 검증
          + BaseSettings 클래스를 통해 API 키나 DB URL 등을 안전하게 관리
     * MkDocs
          + 파이썬 프로젝트의 정적 웹사이트 및 문서 생성을 간편하게 지원
          + 오픈소스 프로젝트 스타일의 미려한 디자인 빠른 적용 가능
          + GitHub Pages 연동도 용이
     * FastAPI
          + 빠른 RESTful API 구축 프레임워크
          + 자동 검증 및 문서화, 빠른 성능, 쉬운 Pydantic 통합 장점
          + Starlette 및 Pydantic 기반으로 높은 타입 안정성과 성능 제공
     * Dataclasses
          + 파이썬 표준 기능으로 데이터 중심 클래스를 간편하게 정의할 수 있음
          + 특별 메소드 자동 생성으로 보일러플레이트 코드 대폭 감소

  버전 관리 및 자동화

     * GitHub Actions
          + project-api와 project-ui 각각에 대해 별도 CI 파이프라인 구성
          + 다양한 OS에서 CI 파이프라인 구축에 최적화된 워크플로우 제공
          + 도커 기반 테스트 환경으로 프로덕션과 동일한 환경에서 테스트 시행 가능
     * Dependabot
          + 자동 의존성 최신화 및 보안 패치 관리를 자동화함
     * Gitleaks
          + 민감 정보(비밀번호, API 키 등) 유출 방지 도구로 git 커밋 전에 보안 검사를 수행함
     * Pre-commit Hooks
          + 커밋 전 자동 린팅, 포매팅, 보안 검사를 위한 도구임
          + ruff, gitleaks 등과 함께 사용해 코드 일관성과 품질 유지

  인프라 자동화

     * Make
          + make test, make infrastructure-up 등의 명령어로 일관된 개발 워크플로우 지원
          + 프로젝트 루트와 project-api에 각각 Makefile 존재
     * Docker & Docker Compose
          + project-api, project-ui 각각을 컨테이너로 분리 실행
          + docker compose up --build -d 한 줄로 전체 앱 실행 가능
          + Dockerfile에는 uv 설치, FastAPI 앱 실행 명령어 포함

마무리

     * 위와 같이 최신 파이썬 개발 환경에서는 효율적이고 견고한 프로덕션 워크플로우를 구성할 수 있음
     * AI, 데이터, 웹 개발 등 다양한 영역에 걸쳐 파이썬 생태계의 성장과 도구 발전으로부터 많은 이점을 경험 가능
     * 모노레포 구조, 자동화 도구, 린터 및 타입 검사기, 즉각적인 테스트 환경, 문서화, 인프라 오케스트레이션까지 하나의 통합된 개발 문화를 구현할 수 있음

   파이썬은 라이브러리와 프레임웍이 풍부한 반면 패키지 버전관리가 잘 안되고 충돌이 잦은게 단점.
   과거의 자바와 장단점의 경향이 비슷함

   본문에도 나온 uv가 진짜 물건이에요. 빠른 것도 빠르고 버전이나 의존성 관리도 npm맹키로 잘 해줘서 uv 정착 중입니다.

   그래도 요즘에는 uv, poetry를 통해서 버전관리와 충돌은 대부분 해소된것 같더라고요

   React와 이런 부분들까지 생태계를 포괄하기에도 적절한가요?

   React와 직접적으로 연동은 언어가 다르기에 어려운 부분도 있지만, 어떤것을 원하시는지에 따라 가능한 부분도 있을것 같아요.
   개인적으로는 python을 통한 프론트엔드 개발은 크게 활성화되지 않은 부분이라 생각합니다.

        Hacker News 의견

     * 코드에서 환경변수 누락 시 “YOUTUBE_API_KEY 또는 YOUTUBE_CHANNEL_ID가 없습니다”라고 OR로 메시지를 띄우는 방식은 굳이 OR를 쓸 필요 없는 상황에서 사용자를 괴롭히는 것임. 각각의 값을 개별적으로 체크해 어떤 것이 빠졌는지 명확히 알려주는 것이 훨씬 좋은 사용자 경험임. 개발 시간 차이도 거의 없다는 점에서 이렇게 하는 걸 추천함
          + 작은 부분까지 집요하게 파고드는 이야기지만, 이런 케이스는 := 연산자(월러스 연산자)를 쓰기에 딱 알맞다고 생각함. 예시로, if not (API_KEY := os.getenv(""API_KEY"")): 처럼 바로 쓸 수 있음. 개인적으로 내부 도구에서는 os.environ[""API_KEY""]에서 KeyError를 그냥 터뜨리게 둠. 이 역시 충분히 명확하다고 생각함
          + 더 나아가서, 조건을 하나씩 검사해 하나라도 빠진 게 있으면 전부 알려주는 편이 훨씬 낫다고 생각함. 한 변수 누락 때문에 프로그램을 돌리고, 그 다음에 또 다른 변수 에러를 보는 불편함을 줄일 수 있음. 상황에 따라 어쩔 수 없이 번거로울 때도 있지만, 가능한 한 한 번에 보여주는 게 좋음
          + 환경변수들을 전부 가져오고, 그 중 빠진 것들을 한 번에 리포트해 주는 방법이 가장 좋음
          + boolean 플래그를 활용해서 마지막에 한 번만 exit(1) 해버리는 방법도 있음. 이렇게 하면 누락된 환경변수를 한 번에 모두 보여줄 수 있음
          + exit(""Missing ..."")처럼 메시지를 바로 출력하면서 코드 1로 종료할 수도 있음
     * 프로젝트 구조를 자동으로 만들어주는 도구를 찾고 있다면 cookiecutter를 추천함. 내가 자주 쓰는 몇 가지 템플릿이 있는데, python-lib, click-app, datasette-plugin, llm-plugin 등이 있음. 다음처럼 사용하면 됨: uvx cookiecutter gh:simonw/python-lib
          + Ruby로 baker라는 걸 만들었음. baker는 템플릿 repo를 복사하지 않고, 해야 할 작업리스트(명령형 단계 목록)를 만들고, 수동 작업(API key를 받아와 설정하기)과 자동 작업('uv init' 등)을 섞을 수 있음. Markdown 문법에 루비 스트링 인터폴레이션, bash도 쓸 수 있음. yml 베이스 config에 아주 지쳤기 때문에 만들게 된 것임
          + 요즘 뜨는 건 Copier라는 도구임. 자세한 건 copier 문서 참고
          + 신규 프로젝트 세팅하는 걸 오히려 즐기는 편임. 이런 걸 굳이 자동화하고 싶지 않음
          + 이런 구조화 자동화 툴은 사실 요즘 에이전트 기반 LLM 개발 워크플로에도 딱 어울리는 영역이라고 생각함
     * ""파이썬이 유닉스 대부분에 기본 탑재라서 더 인간 친화적""이라는 평가는 다소 낙관적인 해석임. ""import json"" 이상만 넘어가면 금방 virtualenv 지옥에 빠지게 됨. Python 3.13.x 환경에서 Ubuntu 22.04나 24.04, Rocky 9 등에서 돌리려면 결국 venv, 컨테이너, 버전매니저는 필수가 됨
          + “import json” 같은 기본 라이브러리조차, 포함되지 않은 언어라면 따로 설치해야 하지만, 파이썬은 표준라이브러리 덕분에 초반 생산성이 높음. 물론 대형 프로젝트에서 표준라이브러리만으론 부족하지만, 실제로 표준라이브러리로만 여러 실전 코드들을 배포했고 배포, 보안 관리 이슈가 생기지 않음. venv 관리도 예전만큼 어렵지 않으며, 패키지 매니저도 발전했음
          + 내 농담삼아 주장하는 이론 중 하나는, Docker/컨테이너가 이렇게 빨리 확산된 이유의 절반은 파이썬 의존성 지옥을 극복하게 해줬기 때문임. 내 첫 파이썬 경험은 2012년 서버에 파이썬 서비스 설치였는데, 의존성 지옥, venv 명령어, 다루기 힘든 환경 세팅, 정말 끔찍했음. pip, brew, macOS 환경에서도 삽질만 반복했고, 파이썬만 보면 바로 피했음. 하지만 최근 uv 덕분에 초보자 입장에서도 파이썬이 한결 나아졌다고 느낌. uv init, uv add, uv run 만으로도 충분함
          + virtualenv는 늘 써야 한다고 생각함. 결국 하나의 디렉토리일 뿐이고, 이제 pip로 시스템 전체에 설치하려 하면 경고도 뜨는 등, 예전만큼 어렵지 않음
          + virtualenv나 container를 꼭 쓰는 게 좋음. 다루기 어렵게 느껴져도, 업데이트나 라이브러리 버전 업 때문에 시스템 전체에 영향 주는 일을 피할 수 있음
          + 예전엔 시스템에 Python2만 기본 포함된 경우가 많았고, 시스템 자체가 이 Python2에 의존할 때도 있어서 오히려 위험했음
     * 파이썬이 동시에 장황하면서도 부족하다고 느껴짐. 뭔가 간단한 거 하려면 500개씩 의존성을 넣거나, 아니면 아주 사소한 일에도 수십부터 많게는 수백 줄까지 코드가 늘어나게 됨. 그래서 파이썬은 불필요한 삽질이 너무 많아 피하게 됨. Perl로 훨씬 빠르고 간결하게 마무리할 수 있어서 Perl을 선호함. 파이썬은 무언가 일 자체보단 프로그래밍을 위한 프로그래밍 같아짐
          + 난 의존성 없는 프로젝트도 많이 만듦. 표준라이브러리와 단일 파일로도 진짜 많은 걸 할 수 있음. Python만 깔려 있다면 curl로 바로 내려서 돌릴 수 있음. 예로 2000줄짜리 돈 관리 CLI 툴이 있는데, plutus는 12개 정도 표준 모듈만 씀. 코드 중 25% 가량이 argparse로 커맨드 파싱하는 부분임. 파라미터별로 라인 하나씩 두는 식으로, 명확하게 쓰는 걸 좋아함
          + Perl이 Python보다 빠르고 강력하다고 했는데, 구체적으로 어떤 예시가 있는지 궁금함
          + 파이썬은 데이터구조 중첩할 때 머리 안 쓰고 바로 쓸 수 있다는 점이 편함. 리스트 안 튜플, 딕셔너리 등 자유롭게 섞을 수 있고 통일된 문법으로 접근 가능함. Perl은 분명 더 똑똑하고 재미있긴 한데, 그래서 오히려 머릿속이 꼬이기 쉬워 내게는 별로임. Python은 심심하긴 한데 5년 뒤 봐도 이해할 수 있게끔 명확성이 상당히 큼
          + 파이썬은 표준라이브러리로도 충분히 쓸 만하다고 생각함
     * 나는 모노레포 구조 선호자지만, 예전에 다녔던 회사에서는 이 방식 때문에 초대형 비대한 구조가 되어 다른 팀 코드를 잘못 건드릴까봐 아무도 손대지 않게 된 경우가 있었음. 이슈의 핵심은 레포 자체가 아니라, requirements.txt를 레포 전체 하나로 관리하거나 빌드 스크립트가 꼬인 탓이 큼. 이론상 한 번만 의존성 업데이트하면 모든 코드가 최신 패치로 안전해져야 하는데, 현실에서 이를 누구도 건들지 못했음. 모노레포는 조직이 매우 NIH(Google처럼 직접 모든 걸 만드는 성향)일 때만 잘 굴러감. 이런 경험 덕에, 각 서비스가 조직의 팀 구조와 일치하는 마이크로서비스 구조를 오히려 높게 보기 시작함. Conway's law도 참조해볼 만함
     * 파이썬은 내가 쓴 의사코드에 가장 근접하게 동작하는 언어임. 머릿속에서 명확하다고 넘어가는 부분마다 실제로도 파이썬이 직관적인 추상화를 제공해줌. 수학 기반 백그라운드에서 왔기에 참 만족스럽게 다가왔음. 물론 지금은 다른 언어들도 좋아하지만 여전히 매력이 있음
          + 수학적 사고를 하는 사람 입장에서는 OOP 쪽이 오히려 사고를 어렵게 만듦. 등가적 추론, 더 좋은 람다, 부작용 회피, 데이터 불변 등이 중요함. OOP는 내가 접한 패러다임 중 수학과 가장 거리가 멀게 느껴짐
     * 거의 똑같은 패턴으로 프로젝트 구성을 만들고 있음. 너무 비슷해서 소름끼칠 정도임. Python 개발자 생태계가 점점 비슷한 스타일로 수렴하는 게 아닌가 생각함. 예전엔 내 선택이 독특한 줄 알았는데, 이렇게 다들 같은 방식이면 내 자유의지는 어디 갔나 싶음. 흔한 아기 이름 짓는 현상과 비슷하게, 독특하다고 생각한 선택이 사실은 인기 #2였던 느낌임
          + 이런 구조는 10년 전부터 파이썬에서 인기가 있었음. 결국 여러 합리적인 엔지니어가 고민 끝에 이 패턴에 자연스럽게 모이는 것 같음
          + 사람의 에고가 마치 파일럿파 양자파처럼 모든 스펙트럼에 깔려 있고, 거기서 존재로 변화한다는 느낌이 듦. becoming-being, 웃음이 절로 남
     * 다른 사람도 파이썬을 좋아하게 된 걸 보면 반갑게 느껴짐. 나는 원래 Ruby를 선호했지만 고객의 요구로 파이썬을 어쩔 수 없이 쓰게 됐음. 예전에는 Ruby가 많이 느렸으나, 억지로 파이썬을 익히면서 점점 익숙해졌고, 지금은 나름 즐기고 있음. Make 사용법에 대해선 다소 이견이 있는데, 의존성을 전혀 쓰지 않는다면, 케이스 문 들어간 스크립트랑 다를 게 없음... 농담 반 섞어서 말하지만, 요즘 세대는 Make에 익숙하지 않은 걸 보면 씁쓸함. 나 때는 말이야, 라는 심정임
          + Ruby는 문법이 훨씬 예쁨. 들여쓰기만으로 스코프를 구분하는 파이썬은 내 스타일이 아님
          + 처음엔 케이스 문으로 된 스크립트로 시작했다가, 결국 flat한 Makefile로 진화했음. Makefile이 더 표준적이고 무작위 스크립트보다 알아보기 쉬움
     * Dataclass와 Pydantic Basemodel 중 어떤 걸 써야 할지 궁금함. Pydantic을 이미 쓴다면 그냥 다 Pydantic으로 통일해도 되지 않을까 싶은데, 굳이 Dataclass를 쓸 이유가 있을지 고민임
          + attrs 프로젝트에서 매우 잘 정리해둔 비교글이 있음. attrs 공식 비교가 있는데, 물론 약간의 편향은 있겠지만 논리적 근거는 충분하다고 생각함. 또 이 블로그도 도움이 됨
          + Dataclass는 nested object 검증을 지원하지 않음. 그래서 단순하게 함수 인자 전달용 flat 구조에는 dataclass를 쓰는 게 나음. 너무 많은 인자를 리스트로 받는 것보다 명확함
          + 생성 시 데이터 검증 때문에 성능 저하 이슈가 있음. msgspec처럼 훨씬 가볍고 빠른 대안도 있음
          + 굳이 검증이나 직렬화가 필요 없으면 Pydantic은 오히려 쓸데없는 오버헤드임. 내 원칙은 직렬화가 필요하면 Pydantic, 아니면 dataclass
          + TypeAdapter(MyDataclass)처럼 기존 dataclass를 바로 쓸 수 있는데, 굳이 Pydantic 모델을 따로 만들 이유가 있나 싶음
     * 최근에는 오히려 Python이 아니라 다른 언어로 이주해서 더 만족하고 있음. 내 파이썬에 대한 생각은 이 글에 정리해놨음. 다음에 다시 파이썬 쓸 기회가 온다면 uv, ruff, ty 같은 걸 꼭 써 볼 생각임
          + 나 역시 백엔드에서 Python에서 JS로 갈아타고 한참 즐기고 있음. 파이썬 설치/패키지 관리가 망가진 건 동의함. 하지만 나에겐 async가 제일 큰 생산성 향상이었음. Python에는 asyncio가 있지만, 여러 경쟁 방식이 혼재되어 표준이 안 잡혀 있음. JS는 딱 하나에 모인 점이 좋아서 훨씬 편했음. 자잘한 항목도 다 합치면 큰 차이를 만들었고, 파이썬은 들여쓰기 스코핑, 임포트에서 상대경로 문제, JS 객체 문법 등 여러 부분이 더 쾌적함. 관련 설명 참고
"
"https://news.hada.io/topic?id=21971","Show GN: Upyo: 현대적인 JavaScript/TypeScript용 크로스 런타임 이메일 전송 라이브러리","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Show GN: Upyo: 현대적인 JavaScript/TypeScript용 크로스 런타임 이메일 전송 라이브러리

   안녕하세요. 개인적으로 이메일 발송 라이브러리를 만들어서 공유해봅니다.

  왜 만들게 되었나요?

   최근에 여러 프로젝트를 진행하면서 Node.js, Deno, Bun 등 다양한 런타임을 사용하게 되었는데, 이메일 발송 부분에서 매번 다른 라이브러리를 찾거나 설정을 다시 해야 하는 불편함이 있었습니다. 특히 Deno나 Bun에서는 Node.js용 이메일 라이브러리들이 제대로 동작하지 않는 경우가 많더라고요.

   그래서 “한 번 작성하면 어디서든 동작하는” 이메일 라이브러리가 있으면 좋겠다는 생각에 Upyo를 만들게 되었습니다.

  주요 특징

    크로스 런타임 호환성

   Node.js, Deno, Bun, 그리고 에지 함수(edge functions)에서 동일한 코드로 작동합니다. 런타임별로 다른 설정이나 코드 변경이 필요 없습니다.

    제로 의존성

   개인적으로 여러 의존성이 딸려 들어오는 것을 선호하지 않다 보니, 제로 의존성으로 제작하게 되었습니다. 이를테면 SMTP 트랜스포트도 smtp 패키지를 사용하지 않고, 직접 개발했습니다.

    단순한 API

   복잡한 설정 없이 몇 줄로 이메일을 보낼 수 있도록 설계했습니다:
import { createMessage } from ""@upyo/core"";
import { MailgunTransport } from ""@upyo/mailgun"";

const message = createMessage({
  from: ""sender@example.com"",
  to: ""recipient@example.com"",
  subject: ""Hello from Upyo!"",
  content: { text: ""간단한 이메일입니다."" },
});

const transport = new MailgunTransport({
  apiKey: process.env.MAILGUN_KEY,
  domain: process.env.MAILGUN_DOMAIN,
});

const receipt = await transport.send(message);

    제공 업체 독립성

   SMTP, Mailgun, SendGrid 등 다양한 이메일 서비스를 지원하며, 제공 업체를 바꿔도 애플리케이션 코드는 그대로 유지됩니다. Transport만 교체하면 됩니다. (다음 버전에는 Amazon SES 지원도 들어갑니다.)

    테스트 친화적

   실제 이메일을 보내지 않고도 이메일 로직을 테스트할 수 있는 MockTransport를 제공합니다. 개발 중에 실수로 실제 이메일이 발송되는 걱정 없이 테스트할 수 있습니다.

  아직 부족한 부분들

     * SMTP 트랜스포트에서 STARTTLS 지원이 아직 구현되지 않았습니다
     * 에지 함수에서 SMTP는 아직 지원하지 않습니다 (HTTP API 기반 트랜스포트만 가능)
     * 아직 초기 개발 단계라 API가 변경될 수 있습니다

  사용해보기

   다양한 런타임에서 사용 가능합니다:
npm  add       @upyo/core @upyo/smtp
pnpm add       @upyo/core @upyo/smtp
yarn add       @upyo/core @upyo/smtp
deno add --jsr @upyo/core @upyo/smtp
bun  add       @upyo/core @upyo/smtp

   트랜스포트 패키지는 @upyo/smtp 외에도 @upyo/mailgun, @upyo/sendgrid, @upyo/ses, @upyo/mock이 있고, 앞으로도 더 추가될 예정입니다.

   문서: https://upyo.org
   코드: https://github.com/dahlia/upyo

  마무리

   개인적인 필요에 의해 시작한 프로젝트이지만, 혹시 비슷한 고민을 하고 계신 분들께 도움이 될 수 있을 것 같아 공유해봅니다. 아직 버전 0.1.0이지만, 꾸준히 개선해나갈 예정입니다.

   피드백이나 기여는 언제든 환영합니다!
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   Upyo는 한국어 “우표”에서 따온 이름입니다. 우표로 편지를 보내듯 이메일을 보낸다는 의미로 지었습니다.

   2~3번은 api를 바꾼 경험이 있어서 공감이 가는 프로젝트입니다. 사이트도, 문서도 깔끔하게 잘 만드신 것 같아요. 서비스를 운영하다 보면 가끔 특정 메일 서비스가 중단돼서 다른 메일 서비스를 failover로 구축해야할 때가 있는데, transport를 pool 개념으로 운용하는 코드도 있으면 좋을 것 같습니다. resend.com 도 지원되면 좋을 것 같아요. 나중에 적용할 때까지 안돼있다면 직접 기여도 해보겠습니다~!

   피드백 감사합니다! PoolTransport랑 ResendTransport는 조만간 추가해 보도록 하겠습니다!

   심볼의 “郵票”를 “우표”로 바꾸는건 어떨까요?
   지금도 정말 찰떡으로 이뻐서 조금 조심스럽긴 하네요..

   오~ 좋네요~ 굿굿

   1인 프로젝트인가요?? 대단하시네요..

   네, 아직까지는 혼자 만들었습니다. 😅
"
"https://news.hada.io/topic?id=22085","LLM을 활용한 코딩 (2025년 여름)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         LLM을 활용한 코딩 (2025년 여름)

     * Redis 개발자 antirez의 LLM 활용기 업데이트
     * Gemini 2.5 PRO와 Claude Opus 4 같은 최첨단 LLM은 개발자의 능력을 강화함
     * LLM을 활용하면 버그 제거, 아이디어 테스트, 지식 확장 등 여러 방식으로 업무 효율 향상 가능함
     * 비전문 분야나 새로운 기술을 LLM의 도움으로 쉽게 다루는 경험이 가능함
     * 그러나 코드의 전반적 품질과 관리를 위해 '인간 + LLM' 협업이 핵심임
     * LLM을 최적으로 활용하려면 충분한 맥락 제공과 명확한 커뮤니케이션 능력이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LLM을 활용한 개발의 변화와 핵심 포인트

   최첨단 LLM(Gemini 2.5 PRO, Claude Opus 4 등)은 방대한 이해력과 대용량 코드 처리 능력을 바탕으로 프로그램 개발자의 능력을 확장하는 역할을 함
     * 명확하게 문제를 기술하고, 반복적인 소통에 익숙하다면
          + 버그를 출시 전 미리 제거하는 경험 가능함(예: Redis의 Vector Sets 구현 사례에서 Gemini/Claude의 코드 리뷰로 즉각적 버그 제거)
          + 아이디어의 작동 여부를 빠르게 테스트하면서 실험적 코드 작성 및 성능 평가 가능함
          + 경험과 본능에 기반한 페어 디자인(pair-design) 이 가능하며, LLM의 풍부한 전문 지식과 인간의 직관이 융합함
          + 명확한 지침을 LLM에 제공하면 일부 코드 구현을 신속하게 완료 가능함
          + 낮선 분야(예: Amiga용 68000 어셈블리 코드)에서도 빠른 기술 적응이 가능함

   예전 ‘LLMs와 2024년 초 프로그래밍’ 글에선 LLM의 유용성을 언급했으나, 최근 1.5년간 LLM이 비약적으로 발전함
   최상의 LLM 활용을 위해서는 인간과 LLM 모두 특정 역량과 습관이 필요하며, 이에 대한 원칙이 중요함

Vibe Coding의 자제와 인간+LLM 협업 원칙

   현재 LLM은 개발자 능력 증폭기로 뛰어나지만, 자율적으로 혼자 모든 업무를 처리하는 수준까지는 도달하지 못한 상태임
     * 테스트, 소규모 유틸리티 등 단발성 소규모 프로젝트에는 LLM 단독 설계가 가능함
     * 하지만 대규모, 비평범한 프로젝트에 LLM 단독 사용 시 복잡함, 불필요한 코드, 구조적 취약성 등 문제 발생 가능성 큼
     * LLM+사람의 협업이 가장 큰 생산성 향상을 보이지만, 전제는 효과적인 커뮤니케이션 및 LLM 관리 경험 보유임
     * 복잡한 작업을 LLM에게만 맡기지 말고, 항상 인간이 과정에 적극적으로 개입하는 전략이 필요함

LLM에 충분한 맥락 제공의 중요성

   LLM에게 개발 또는 문제 수정 방향을 제대로 이해시키려면 광범위한 맥락 정보 제공이 필수임
     * 논문, 대상 코드베이스(최대한 전체), 작업 의도 등을 제공하는 게 바람직함
     * 구현 목적, 불필요하거나 허약한 방안, 실현 가능한 핵심 아이디어, 목표, 불변 조건, 코드 스타일 등 핵심 정보를 포함함
     * 예를 들어, LLM이 모르는 신기술(예: Redis vector sets) 다룰 때 README 문서를 맥락에 포함시키면 전문가 수준의 답변 즉시 가능함

LLM 선택과 사용 방법

   가장 잘 알려진 LLM이 실제로 최고의 결과를 내지는 않음
     * 코딩에는 Gemini 2.5 PRO, Claude Opus 4가 특히 효율적임
          + Gemini 2.5 PRO는 복잡한 버그 탐지와 문제 해결력이 탁월함
          + Claude Opus 4는 새 코드 작성에 능하며, 사용자 경험도 우수함
          + 두 모델을 번갈아가며 사용하면 복잡한 설계 시 이해도가 커짐
          + 단 하나를 선택한다면 Gemini 2.5 PRO를 추천함
     * LLM 사용 시 준수해야 할 조건
          + 코드 에이전트나 IDE 내 통합 에이전트 사용 지양함
          + LLM이 전체 맥락(코드, 문서 등)을 직접 볼 수 있게 하여 최상의 답변을 유도함
          + RAG(지식 추출 기반) 등 일부 맥락만 보여주는 기능 사용 시 성능 저하 발생함
          + 과정마다 사람이 수작업으로 코드를 복사/붙여넣으며 직접 흐름을 추적해야 함

결론 – 통제 유지가 핵심

   코드를 혼자 작성하는 agent의 등장은 멀지 않았지만, 현 시점에서는 사람이 주도적으로 LLM과 협업하는 방식이 가장 날카로운 코드를 생산함
     * 인간이 ‘무엇을, 어떻게’ 할지 결정하는 역할이 여전히 핵심임
     * LLM을 활용하면 기존 지식 경계를 넘어 새로운 기술이나 개념을 배우며 성장 가능함
     * 직접 코드를 통제하면 설계·구현의 일관성을 지킬 수 있으며 LLM의 오류가 주는 불확실성도 최소화됨
     * 에이전트의 성능 발전 상황을 주기적으로 점검하는 것도 현명한 전략임
     * 이 단계에서 LLM 활용을 회피한다면 변화에 뒤처질 수 있으므로, 균형 잡힌 활용법이 중요한 시점임

        Hacker News 의견

     * Gemini 2.5 PRO나 Claude Opus 4 같은 사설 LLM 모델이 표준이 되어가는 현실이 안타까움 느끼는 중임, LLM의 발전과 도구로써의 강력함은 매우 긍정적으로 보지만, 왜 개발자들이(유명 인사든 무명이든 상관없이) 계속 프로그래밍을 하려면 제3자 유료 서비스에 의존해도 괜찮다고 여기는지 이해하기 힘듦, 과거에는 오픈소스와 무료 도구만으로도 코딩이 가능했음, 앞으로 몇 년 뒤에는 유료 LLM에 의존하는 게 지금 IDE나 vim 없이 코딩하는 것만큼 불편해질까봐 걱정임, 월 $200이 별거 아니지 않냐는 식의 얘기는 근본적인 문제를 해결하지 못함
          + 지금 로컬에서 돌릴 수 있는 오픈 모델은 질적으로 부족하고, 무엇보다도 운영비가 훨씬 큼, Claude 4 급 모델을 개인 컴퓨터에서 경제적으로 돌릴 수 있게 되면 그때 많은 사람들이 시도할 것임, 현재로서는 Kimi K2 같은 모델이 512GB Mac Studio 두 대에서 돌아가지만, 장비값만 약 $20,000임
          + 구독 모델의 가치가 초기에는 아주 뛰어난 가격 대비 효율성을 제공하는 것처럼 느끼게 만듦, 하지만 점차 가격이 오르고 품질이 떨어지면서 결국은 서비스에 묶여버리는 상황이 됨, 마치 블랙미러의 ""Common People"" 에피소드처럼 됨
          + 개인적으로는 모든 개발자가 유료 LLM에 무조건 종속되는 미래는 일어나기 힘들다고 생각함, 장기적으로는 코드를 많이 양산하는 것 자체가 문제라는 현실을 사람들이 깨달을 것이라 봄, 코드는 부채이고 불안정하거나 느린 코드가 쌓이면 그 부채도 커짐, AI가 사라지지는 않겠지만 열기가 좀 식고 나면 어디에 어떻게 써야 하는지에 대한 이해가 늘어날 것임, 또 투자금이 마르면 어떻게 될지도 의문임, OpenAI, Anthropic은 수익성이 없고 계속 자본이 들어와야 지금 상태를 유지할 수 있음, 만약 LLM의 진화가 지금 정도가 끝이고 이게 한계라면 투자금도 빠질 테고, 그러면 사용 비용이 오르거나, 완전히 서비스에서 사라질 수도 있다고 봄
          + 현실적으로는 큰 문제 아니라고 생각함, 생산성 향상에 실질적인 이유가 없다면 비싸고 불친절한 서비스에 계속 종속될 이유가 없음, 오픈 모델들도 꾸준히 발전하고 있어서 오픈 모델과의 격차가 크지 않으면 계속 이용할 필요 없음, 만약 LLM 발전이 멈추지 않고 가파르게 발전한다면 우리도 기존 방식으로는 경쟁력이 없으니 다른 영역으로 전환해야 함, 결론적으로 크게 걱정할 필요는 없다고 생각함, 또한 대형 모델 기업들의 가치가 실제보다 매우 과대평가되어 있다고 느끼고 있음
          + 오픈소스와 무료 도구로 코딩할 수 있다는 말에 덧붙이고 싶음, JetBrains는 동료들보다 오래된 기업이고, MS의 Visual Basic/C++/Studio가 윈도우 개발을 쉽게 만들어주었지만, 모두 유료임
     * ""PhD-level knowledge""라는 표현에 동의하지 않음, PhD 과정은 기존 지식 습득이 목적이 아니라 연구를 수행하는 방법을 배우는 것이 핵심임, AI 논의에서 흔히 오해되는 포인트인데, 박사학위 수준의 지식이라는 말이 의미가 불분명해짐
          + PhD라는 게 연구를 익히는 과정이라는 것 외에도, 질문을 던질 수 있느냐가 핵심임, LLM은 ""풍부한 지식을 가진 게으른 노동자""에 가깝고 스스로 질문하면서 가설을 탐색하지 않음, 실제 경험을 예로 들자면, Claude Code(Max Pro)에게 테스트 어서션 수를 주석 처리하게 했더니 원래 계획에서 잘못된 가정을 바탕으로 버그가 생겼음, 내가 직접 계획을 다시 쓰라고 지시해야만 이유를 찾고 고칠 수 있었음, 예를 들어 ORM 객체가 null 값을 가진 이유는 커밋 후 refresh가 없었고, 다른 DB 세션에서 불러왔던 것이 세션 종료 후에도 그대로 남아서 생겨난 문제였음
          + 동의함, 전문가 수준의 지식은 있지만 인간이 잘하는 걸 LLM이 그만큼 하지 못함, 예를 들어 LLM은 단번에 비상한 프로그램을 처음부터 끝까지 써줄 수 있는데, 반복적으로 개선하는 건 어려움
          + PhD가 지식 그 이상이라는 점을 이해한다 해도, 그 지식에 쉽게 접근할 방법이 생겼다는 건 엄청난 가치임, 예전 회사에서 PhD만이 답할 수 있는 난해한 질문(거칠게 말해 ""두 소재 경계에 일정 방향 전압 가하면 무슨 현상이 생기나요?"" 같은 질문)에 LLM이 꽤 쓸만한 답을 내놓기도 함
          + PhD를 땄다고 해서 과목 자체는 더 신경쓰지 않음, 결국 중요한 건 연구 수행법을 배웠다는 것임
     * LLM 기반 코딩에 관한 논의는 다루는 도메인과 사용하는 프로그래밍 언어에 대해 반드시 언급해야 한다고 생각함, 이 두 변수가 LLM 활용 방식보다 훨씬 큰 영향력을 가짐, 누군가 LLM 코딩을 좋아하거나 싫어한다면 어느 영역의 문제를 풀었는지 먼저 묻고, 직접 AI로 그 문제를 해결해 보아야 각자의 입장을 잘 이해할 수 있음, 그렇지 않으면 언제나 ""너가 잘못 써서 그래"", ""나는 시도했는데 별로더라"" 같은 소모적인 얘기만 나온다고 봄
          + 사용자의 프롬프트와 원하는 결과를 얻기까지 얼마나 많은 노력이 들어갔는지 구체적으로 공유되어야 한다고 생각함, LLM 사용법을 설명한 글에서 인간이 얼만큼 세부 정보를 제공하고, 전체 맥락과 이해도를 '브레인 덤프'로 전달해야 함을 강조했음, 그렇게까지 해서 나온 코드라면, 차라리 직접 코드를 쓰는 편이 낫지 않나 하는 생각도 듦, 실제로 입력하는 시간은 별 문제가 아니고 문제를 명확히 설명하는 게 진짜 핵심임
     * 최근 agentic coding에 몇 달간 집중하며 일한 경험을 바탕으로, 게시물의 모든 말에 공감함, 최첨단 LLM이 그나마 제일 쉽게 쓸 수 있지만 오픈모델도 곧 따라올 거라 기대함, LLM에게 새로운 방법을 추천받거나 이미 알고 있는 접근법을 제시하도록 할 수도 있음, 가끔 LLM이 내용을 복잡하게 만드는 경향이 있으니 미리 감지하거나 리팩토링을 요청하면 됨, 다양한 모델이 나올 때마다 또 상황은 달라질 것임, 모든 작업에 최첨단 모델이 반드시 필요한 건 아님, 단순한 기능/버그 픽스에는 Copilot도 꽤 괜찮은 출발점임, 모두들 새로운 변화 속에서 다양한 시도를 해보고 배우는 과정을 즐겼으면 좋겠음
     * Claude의 GitHub action을 10~20개 정도 이슈 구현과 PR 리뷰에 써봤는데, 말 그대로 히트 앤 미스라 무분별한 자동화보단 증강 도구로 쓰는 게 맞다는 데 동의함, 변경사항이 작고 테스트가 잘 갖춰진 소규모 기능/리팩터링은 거의 자동으로 성공함, 액션으로 돌리면 내가 다른 할 일을 할 수 있어서 장점임(이슈도 Claude가 써주면 더 편함), 하지만 중간 규모 이상에서는 종종 코드가 그럴듯해 보이지만 실제론 동작하지 않는 결과가 나옴, 이건 테스트 커버리지 부족 내 책임일 수도 있지만 확실히 자주 발생함, 이슈를 더 상세하게 써주거나 promt를 다양하게 줘도 결과는 실망스러움, 대형 작업은 두말할 것도 없이 힘듦, PR 리뷰 기능은 소/중규모 일에선 쓸만하지만 쓸모없는 확인도 많음, 결론적으로 LLM이 스스로 코딩하는 데는 아직 멀었다고 생각함, 소규모 작업만
       이슈 써주고 PR이 올 때까지 기다리는 식이 제일 효율적임, 대부분의 작업(중간 규모)에서 나는 주로 Claude에게 디렉션만 하면 코딩은 거의 안 해도 되어 생산성은 확실히 오름, Gemini도 사용해 봤는데 그냥 놔두면 예측 못할 수준으로 코드가 요동침, 사내에서 Copilot로 PR 리뷰도 하고 있지만 별 효과 없음, 대규모 코드베이스라면 Gemini 활용도가 더 높을 수도 있겠다 싶음
     * OP와 다르게 한 달간 집중적으로 이 분야를 파면서, Gemini 2.5 PRO와 Opus 4는 아키텍처 같은 추상적 논의에선 더 좋은 결과를 보여줌, 그리고 개별 코드 구현은 Sonnet에게 넘기는 방식이 효율적이었음, 2.5 PRO, Opus 는 정답 주위에서 맴돌다 스스로 수정을 반복하는 패턴이 많고, Sonnet은 답까지 직설적으로 가는데, 대신 충분히 세세한 지시가 필요했음
          + 충분히 가능한 얘기임, 실제로 Sonnet/Opus 4는 최고의 결과에선 더 강력하지만, 일부는 Sonnet 3.5v2(3.6이라고도 함)나 심지어 3.7보다 일관성이나 정렬이 뒤처지는 부분도 있음, 또한 모델도 복잡한 객체라서 도메인에 따라 ""약해 보이는"" 모델이 더 잘 작동하기도 함, 그리고 대화형(Interactive) 환경과 에이전트 지향 환경은 강화학습 기법 자체가 달라서 어떤 방식으로 쓰냐에 따라 모델의 퍼포먼스가 달라짐
          + 실제 내부 통계 데이터에서도 Opus와 Gemini 2.5 pro가 현실적인 환경에서 Sonnet 4보다 성능이 떨어진다는 결과가 확인됨 관련 통계 링크
          + 나 역시 비슷한 경험을 함, Gemini 2.5 Pro는 AI Studio에서 큰 설계 아이디어 검증/정제에 쓰고, Claude Code로 요건을 가져가면 대체로 깔끔하게 코딩해줌, 최근 Gemini CLI도 해봤는데 Claude Code에 비해 코딩 실력이 매우 뒤처짐, 구문 오류도 많고 루프에서 못 빠져나와서 결과물이 장황하고 빨라서 따라가기도 힘듦, 반면 Claude Code는 디버깅력도 탁월함, ""큰 그림"" 문제풀이에는 DeepSeek R1도 써볼 만한데, 매우 느리지만 정답률이 높음
     * AI/LLM이 때로는 엄청나게 비효율적인 코드를 작성하는 현실적인 사례를 공유함 관련 블로그 링크
          + 마찬가지로 AI는 Code Golf(코드 길이 최소화 게임)에 매우 약함, 비밀스런 단축 기법들을 다 알 것 같지만 실제론 장황하게 짜는 걸 더 좋아함
     * LLM에게 먼저 원하는 작업을 직접 설명만 해 달라고 요청하고, 내가 중간에 피드백을 주면서 몇번 반복을 거치고 나면, 그 다음 나온 코드의 품질이 훨씬 좋은 경험을 했음, 상세계획을 먼저 확인시키고 진행하면 효과적임
     * 내 경험상 프론트엔드에서 검증하기 쉬운 반복적이고 단순한 작업은 vibe coding으로 맡겨도 되지만, 평소엔 내 코드를 리뷰하고 각종 대안들을 평가하는 스파링 파트너로 LLM을 씀, 추천이 말도 안 되거나 논리적 결함이 있어도 내가 너무 당연한 걸 놓치지 않게 도와줘서 만족함, 복잡하게 꼬인 문제를 두고 오히려 과한 시도를 하려는 내 습관도 고쳐줌
     * OP가 말하는 방식이 정확히 뭔지 이해가 안 됨, 혹시 redis C 파일을 수작업으로 Gemini Pro 웹 챗창에 붙여넣으란 건가?
          + 나 역시 그 부분까지는 고개를 끄덕였는데, LLM을 쓸 때는 agent 혹은 에디터 통합형 코딩 도구를 피하라는 게 핵심 요구처럼 보임, 근데 진짜로 창에 코드를 복붙하라는 건가? Cursor 나오기 전엔 그렇게 했지만, 지금은 그렇게 할 필요 없고, 자세히 보면 Cursor나 Claude Code 언급은 아예 빠져있어서 정말 이런 도구를 써봤는지조차 의문임
"
"https://news.hada.io/topic?id=22041","서체의 지적재산권에 대하여…","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            서체의 지적재산권에 대하여…

   최근에 6x2x1 이라는 8비트 애플2시절의 비트맵을 트루타입 등으로 변환해서 공유한 후에 저작권 관련 문의를 많이 받는데요…

   한줄요약: 특허가 없는 서체를 특허가 없는 인코딩 방식으로 나열한 비트맵 폰트는 저작권 보호 대상이 아님.

     저는 법률가가 아닙니다, 저의 법령해석은 아무런 법적 구속력이 없으며, 이 해석에 대한 어떠한 법적 책임도 지지 않습니다.

   TL;DR 한국,미국을 포함한 대부분의 국가에서 서체(Typeface; 글자의 디자인)은 저작권 보호 대상이 아님. 트루타입 폰트(Font; 서체의 구현물)등은 일종의 소프트웨어로 간주되어 저작권 보호대상임.

  ChatGPT 요약

   다음은 “Intellectual property protection of typefaces” 문서의 요약입니다:

   ⸻

   🛡️ 1. 기본 용어 구분
   • Typeface: 글자의 외형, 즉 글꼴 디자인 자체
   • Font: 해당 디자인을 구현하는 컴퓨터 코드 및 소프트웨어 형태 ￼

   ⸻

   🎨 2. 국가별 주요 보호 방식

   미국 (U.S.)
   • Typefaces (글자 디자인) 은 저작권 보호 대상이 아님. 1978년 Eltra Corp. v. Ringer 판결과 1992년 규정(37 CFR § 202.1(e))에 따라 디자인 자체는 utilitarian object로 간주되어 보호되지 않음 ￼
   • Font 소프트웨어(컴퓨터 프로그램) 는 저작권 보호가 가능. Adobe vs. Southern Software 판례에서 Adobe의 폰트 코드 복제를 저작권 침해로 인정함 ￼
   • Design Patent (디자인 특허): 글자 모양 디자인 자체를 보호 가능. U.S. 최초 디자인 특허는 1842년에 타입페이스 관련. 보호 기간은 특허 부여 후 15년 ￼
   • Trademark (상표): 글꼴 이름은 상표로 보호 가능 (예: Times New Roman, Palatino) ￼

   유럽 (영국, 독일, 프랑스 등)
   • Typefaces 디자인 자체를 저작권 또는 산업디자인법으로 보호.
   • 영국: 1988년 저작권법에서 typeface 디자인 보호 명시, 보호 기간은 보통 25년, 단 인쇄 용도로의 사용은 예외 ()
   • 독일: 1981년 특별법(“Schriftzeichengesetz”) 시행, 출판 후 10년 동안 자동 보호, 15년 연장 시 최대 25년까지 보호 가능 ￼

   기타 국가들
   • 아일랜드: 출판 후 15년까지 보호 ()
   • 이스라엘: Hadassah 서체의 고유 저작권 인정 사례 존재 ()
   • 일본: 실용적 기능이 우선으로 판단되어 저작권 보호 대상 아님 ￼
   • 러시아: 법적 공백 속에 typeface들이 저작권 대상으로 취급될 수 있음 ()
   • 대한민국: 대법원이 “정보 전달 수단”으로 판단해 타입페이스 자체는 저작권 미인정 ()
   • 스위스: 명시적인 법은 없고 관할 판단에 따라 보호 가능성은 있으나 드뭄. 이름은 상표로 보호 가능 ()

   ⸻

   📄 3. 법적 보호 수단 비교

   보호 대상 미국 유럽 / 일부 국가
   Typeface 디자인 ❌ 보호 제외 ✅ 저작권 또는 산업디자인법 보호 (최대 25년)
   Font 파일 (소프트웨어) ✅ 저작권 보호 ✅ 보호
   디자인 특허 ✅ 디자인 특허 가능 (최대 15년) 일부 국가 가능
   상표권 ✅ 글꼴 이름 보호 가능 ✅ 글꼴 이름 보호 가능

   ⸻

   ⚖️ 4. 주요 사례
   • Eltra Corp. v. Ringer (1978): 미국 연방항소법원, 타입페이스 디자인은 저작권 보호 대상이 아니라고 판결 ￼ ￼ ￼ ￼
   • Adobe Systems vs. Southern Software (1990년대 말): Adobe Utopia 폰트의 control‑points 기반 코드 복제를 저작권 침해로 판단. 디지털 폰트는 코드 측면에서 보호됨 ￼

   ⸻

   🧾 5. 실무적 시사점
   • 글꼴 디자인을 라이선스하려면:
   • 미국에서는 폰트 파일(소프트웨어)와 이름(상표)에 대한 라이선스 필요
   • 유럽 및 보호 법 있는 국가에서는 typeface 디자인 자체도 라이선스 필요
   • EULA(최종 사용자 라이선스 계약) 검토 중요 – 글꼴 사용, 수정, 공유 등에 제한 포함된 경우 많음 ￼
   • AI 기반 글꼴 생성 등 새로운 기술에서는 저작권 및 창작성 판단이 복잡해지고 있음 ()

   ⸻

   ✅ 요약
   • 미국: 디자인 자체는 보호되지 않으며, 폰트 소프트웨어 코드와 이름은 보호 가능. 디자인 특허로도 보호 가능
   • 대부분 유럽 국가들: typeface 자체를 디자인 혹은 저작권으로 보호하며, 최대 약 25년간 유효
   • 한국 및 일본 등 일부 국가: 디자인 자체에 대한 보호는 인정되지 않음

   반례로 대표적인 케이스가 은글꼴인데요. 출판물을 스캔해서 글꼴을 만든 거라서 거의 전부가 문제가 있는데 그 중에 회사가 문제를 제기한 게 있습니다. 회사가 문제를 제기해서 글꼴 패키징한 박원규씨는 삭제를 동의했는데 스캔한 당사자인 은광희씨는 끝까지 동의하지 않았죠. 법정으로 끝까지 가면 어떨지 모르겠지만, 실제로는 문제가 됩니다.

   똥이 무서워서 피하는 건 아니니까요.
   서체 디자인의 저작권을 인정하지 않은 판례는 있지만,

   https://www.law.go.kr/%ED%8C%90%EB%A1%80/(94%EB%88%845632)

   서체 디자인의 저작권 인정한 판례가 있는지 찾아봐야겠네요.

   여기에 안올리셔서 일단 폰트 부터 소개를 ㅎ
     * 6x2x1 폰트 보기 https://iolo.kr/6x2x1-fonts/
     * GitHub Repo : https://github.com/iolo/6x2x1-fonts

   요걸 보고 나니 Tell GN이 있으면 괜찮지 않을까라는 생각이 들었습니다.

   https://news.hada.io/topic?id=22048
   올렸습니다. 이게 이렇게 까지 할 인가 싶지만... -_-;

   한국의 모회사에서 폰트 사용에 대해서 저작권 침해 공문을 보내 논란이 된 적 있는 걸로 알고 있는데 실상은 무효였나보네요
     * https://www.youtube.com/watch?v=4UaLEDCY1IY&ab_channel=JTBCNews

   (소프트웨어로 간주되는) 트루타입 등의 폰트는 저작권이 있습니다. 글에도 그렇게 썼습니다만…?

   폰트의 저작권은 소프트웨어 상에서만 보존되는 것으로 이해했습니다.(ttf, otf 형태의 폰트 파일을 무단 복제, 공유하는 등)

   소프트웨어 형태가 아닌 인쇄물, 영상 등에 사용하는 경우에는 저작권 침해라고 볼 수 없다고 하네요.(저작권 침해와 별개로 이용약관 위반)
     * https://kcopa.or.kr/lay1/bbs/…

   넵 그래서 약관에… 이러저러한 용도외에는 쓸 수 없다고 명시하는 거죠
"
"https://news.hada.io/topic?id=22021","Linux 데스크탑, 처음으로 미국에서 점유율 5% 달성","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Linux 데스크탑, 처음으로 미국에서 점유율 5% 달성

     * 리눅스가 처음으로 미국 데스크탑 시장 점유율 5.03%를 돌파하며 역사적인 이정표를 달성
     * Windows의 불만, Steam Deck 기반의 게임 유입, 리눅스 자체의 발전이 주요 성장 요인
     * 프라이버시 도구 사용, 사용자 에이전트 위장 등으로 인해 실제 리눅스 사용자는 통계보다 더 많을 가능성이 있음
     * Chrome OS까지 포함하면 '리눅스 계열' 점유율은 7.74% 로 더욱 증가함
     * 오픈소스 운영체제에 대한 수요가 커지면서 하드웨어 지원 확대와 생태계 성장이 기대됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

미국에서 리눅스 데스크탑 점유율 5% 돌파

     * 2025년 6월 StatCounter 데이터에 따르면 미국 내 데스크탑 OS 중 리눅스 점유율이 5.03% 에 도달함
     * 세부 점유율은 다음과 같음:
          + Windows: 63.2%
          + OS X: 16.57%
          + macOS: 7.72%
          + Linux: 5.03%
          + Unknown: 4.76%
          + Chrome OS: 2.71%
     * 리눅스가 ""Unknown"" 카테고리까지 추월했다는 점이 상징적임

리눅스 사용자가 증가하는 이유

     * 1. Windows의 문제
          + Windows 10 지원 종료 임박으로 사용자들이 대안을 찾는 중
          + 광고, 프라이버시 침해, 강제 업데이트 등으로 인해 Microsoft에 대한 불신이 확산됨
     * 2. 게임을 통한 신규 유입
          + Steam Deck이 리눅스 기반 시스템으로서 대중화에 기여
          + 새로운 게이머들이 리눅스를 경험하며 사용성과 유연성에 만족
     * 3. 리눅스 자체의 발전
          + Ubuntu, Linux Mint 등의 UI/UX 개선으로 초보자도 접근 가능
          + 프라이버시 보호 기능이 강화되고 있음
          + 구형 하드웨어에서도 성능이 좋아지는 등 경제적 대안으로 주목
          + Wine, Proton 등 도구를 통해 Windows 소프트웨어 호환성도 향상 중

실제 점유율은 더 높을 가능성

     * 웹 기반 사용자 통계는 한계가 있음: 리눅스 사용자는 프라이버시 도구로 추적을 회피하거나 브라우저의 사용자 에이전트를 변경함
     * Unknown 카테고리 4.76% 중 일부도 리눅스일 가능성 있음
     * Chrome OS 역시 리눅스 커널 기반이며, 이를 포함하면 ""리눅스 계열"" 점유율은 7.74% 에 달함

리눅스의 미래

     * 데스크탑 리눅스 점유율 상승 속도:
          + 1% → 2%: 약 8년 소요
          + 2% → 3%: 2.2년
          + 3% → 4%: 0.7년
          + 4% → 5%: 불과 5개월 (2024년 2월 → 2025년 6월)
     * 성장 곡선이 가속화되고 있으며, 이는 더 많은 개발자와 사용자를 끌어들이는 선순환 구조로 이어질 가능성
     * 하드웨어 제조사의 리눅스 지원 확대가 기대되며, 드라이버 문제 등도 점차 해결될 전망

마무리

     * 리눅스의 5% 돌파는 단순한 수치가 아닌 사용자 선택의 다양성과 오픈소스 생태계의 확장을 보여주는 지표
     * 더 많은 사용자가 리눅스를 선택함에 따라, 리눅스의 미래는 이전보다 훨씬 밝아지고 있음

   워낙에 거의 대부분을 웹 서핑만 하니까 chrome os 깔았는데 겁나 쾌적함

        Hacker News 의견

     * 집에서 워크랩탑이 아닌 컴퓨터를 가지는 사람을 거의 못 봤음, 대부분 내 주변은 이미 컴퓨팅을 폰이나 태블릿으로 옮긴 상태임, 이 데이터가 그 부분은 아예 반영 안 됨, 결국 리눅스 데스크탑을 쓰던 기술적인 사람들이 계속 유지하는 비율 증가일지도 모름, 전체 데스크탑 수 감소로 인한 비율 상승이라면 크게 기뻐할 일은 아님, Steam Deck 영향으로 상승은 분명하지만 이걸 리눅스 데스크탑이라고 부르기는 조심스러움, Android와 같이 커널만 쓰는 경우임
          + Steam Deck은 분명 리눅스 데스크탑임, Android는 커널만 같고 완전히 다르지만 SteamOS는 Arch 기반의 리눅스 디스트로임, Steam Deck의 '데스크탑 모드'는 읽기 전용 시스템이나 업데이트 방식이 다를지언정 명백히 리눅스 데스크탑임
          + 어느 지역에 사는지 궁금함, 참고로 나는 유럽 작은 나라 살고 GDP에서 중간쯤임, 여기서는 거의 모두가 개인용 PC나 노트북을 가지고 있고, 어린아이들만 폰/태블릿을 많이 씀, 커가면서 PC게임쪽으로 가는 경우 많았음, 게임은 대부분 PC(윈도우) 위주임, 과거 불법 다운로드가 합법이어서 그런지 전통이 있음, EU 규제로 이제는 회색지대이지만 큰 문제 없는 분위기임, 스팀 등장 후 합법 게임 구매가 늘어남, 그런데도 가격은 미국과 비슷하거나 더 비싼데 급여는 5배 낮음, '우리만의 특수시장'이라는 말로 자조함, 실질적으로 처벌받는 건 업로더뿐임, 독일에서 변호사들이 토렌트 사용했다고 벌금청구 난리치는 걸 보면 이해할 수 없음, 우리에겐 너무 과격함, 여기선 아마 변호사도 토렌트 쓸 것 같음
          + 내 가족 방문 시에도 폰과 랩탑 둘 다 가지고 갔지만 실제로 랩탑은 거의 안 쓰고, 형이나 조카들은 랩탑을 자주 씀, 게임 때문만이 아니라 리포트 작성 등 폰으로 불가능한 작업 때문임
          + 시장점유율은 상대적 개념이고, PC 전체 수가 줄더라도 리눅스 비율이 줄지 않고 다른 플랫폼만 영향을 받았다면 그것만으로도 성과라고 생각함, 가령 리눅스가 95%가 됐다 쳐도 전체 수 논리로 평가절하하는 건 의미 없다고 봄
          + 우리 부모님은 MS Excel로 컴퓨팅하고, 폰/태블릿엔 관심 없음, 만약 ExcelOS 등 익숙한 데스크탑 Excel 환경이 있다면 넘어갈 수도 있었겠지만 현실은 아님, 결국 평생 윈도우 데스크탑 지원 담당임, 앱 버전 Excel이나 구글 시트를 추천해도 “더 이상 새 기술 배우기 싫다”고 하심
     * e-waste 리퍼브 회사의 리눅스 확산에 도움 주고 있음, 윈도우 라이선스로 못 팔아서 동료들은 Ubuntu, 나는 Linux Mint 설치함, 최종 구매자가 계속 리눅스를 쓰는지는 모르지만, 확산에 일조한다는 생각이 즐거움, 우리 이베이 링크 남김
          + 이런 리퍼브 PC는 주로 어르신/저소득층 등에게 가는 것 같음, 게임이나 기업용은 윈도우 유지하겠지만 가벼운 웹서핑용은 리눅스로도 충분히 만족함, 사람들이 컴퓨터 자체를 거의 안 쓰고 폰으로 넘어가는 추세이기도 함, 실제로 내 전여친 가족 모두 컴퓨터 없이 생활 중이고, 내 딸도 거의 사용하지 않음, 개발자 등 실사용 목적이 있는 사람만 리눅스를 필요로 하고, 윈도우에서 강제되는 리부팅 같은 불편이 리눅스로 이동 촉진한다고 느낌
          + 별다른 필요 없는 이상 대부분 사람들은 그냥 리눅스 상태로 둔 채 사용함, 웹서핑만 잘 되면 윈도우와 구분도 못하고 신경 쓰지도 않음, 리퍼브 작업이 큰 의미가 있는 일임
          + 리눅스 민트 노트북을 이베이에 팔 때 “윈도우가 아니라 리눅스입니다, 아니면 주문 취소 가능합니다”라고 꼭 안내하지만, 100% 구매자가 “네! 리눅스 원했습니다”라고 답변함, 숨겨진 수요가 명확함
          + 리눅스 데스크탑 장기 유저로, 중고 리퍼 노트북이 집에 네 대, 새 노트북도 세 대가 있음, 내 경우 최신 하드웨어 성능을 안 써도 되고 예전에는 리눅스/bsd 지원이 더 잘 돼서 리퍼브가 최적이었음, 적은 돈에 고급 제품을 쓸 수 있음, 노트북은 핸드백과 같아서 경우에 따라 여러 대가 필요함
          + 윈도우 라이센스가 BIOS에 저장되어 있어 재설치하면 자동으로 활성화된다고 생각했음
     * statscounter 데이터 자체가 신뢰도가 낮다고 생각함, 이 회사가 뻔히 잘못된 데이터(몇 달간 Classic Mac OS가 7% 등)를 방치하는 걸 봐도 알 수 있음, 지속적으로 오류가 있는데 개선 안 하는 자료를 왜 곧이곧대로 믿어야 하는지 모르겠음
          + Cloudflare OS 시장점유율 통계도 있고, 이게 더 신뢰할 만하다고 느낌, 미국 내 리눅스 데스크탑 시장점유율 4.4%로 나옴, 여름방학 시즌 등 계절 변화 영향도 있다고 보지만 실제 성장도 존재함, Cloudflare radar 링크
          + analytics.usa.gov도 언급할 만함, 2025년 기준 리눅스 OS 방문자 비중 5.7%로, 2024년의 4.5%에서 증가함, 물론 미 정부 사이트 방문 데이터라 일반 미국인과 다를 수 있지만 일종의 지표로 참고할 만함, 관련 댓글 참고
          + 리눅스 사용자는 광고 차단기 사용 비율이 높고 statcounter는 3rd party JS 태그에 의존하기에 신뢰도에 의문을 가짐, 사람들이 리퍼브 리눅스 PC를 윈도우로 착각하고 쓴다는 이야기가 놀라우며, 내 경험상 UI가 달라서 대부분 금방 눈치 채며 불편해함, 전체적으로 포럼이 소위 'LARPing' 만연이라 현실감을 의심함
          + OS X와 macOS를 왜 합산하지 않는지 의문임, 최근 몇 달간 변동폭이 3.5%씩이나 튀는 건 실제 배포 수치가 아닐 것임, 이런 에러 범위에서는 5%라는 숫자 자체도 큰 의미 없음, 애플은 OS를 macOS로 바꿨지만 Safari UA엔 여전히 ‘Mac OS X’만 등장함, 새로운 “macOS” 카테고리의 출처가 궁금함, 문서가 있다면 찾아보고 싶은 마음도 없음
          + OS X 수치는 같은 기간 감소함, 애플이 MacOS로 이름만 바꾼 것 같음, 리포트 방식의 변화일지도 몰라 보임
     * 미국 정부 웹사이트 방문자 중 리눅스 비율을 따로 참고하면 마지막 30일간 6%, 2025년 누적 5.7%, 2024년 4.5%임, 이 통계는 iOS/Android도 ‘운영체제’로 잡기 때문에 전체 비율에서 윈도우가 32%밖에 안 됨, 리눅스 데스크탑만 놓고 보면 6%보다 조금 더 높을 수도 있음, analytics.usa.gov 참고
     * 나 역시 오랜 기간 macOS와 Arch 기반 리눅스를 병행해서 씀, 문제는 게임임, 리눅스에서도 어느 정도 게임이 돌아가지만 배틀넷이나 오리진 같은 서드파티 런처들은 싫어함, Proton 버전 고르기(이른바 ‘Proton 뽑기’)가 불편하지만 나는 견딜 수 있음, 그러나 내 아내는 그렇지 않아서 결국 집엔 윈도우도 반드시 남김, 최근 Windows 11을 QEMU로 설치해봤는데 너무 끔찍함, 시작 메뉴에 광고 나오고, 기본 브라우저/검색 엔진을 무시하며, 시작 메뉴 바를 상단에 둘 수도 없음, 내 염원은 A) 윈도우 8/10/11이 덜 실망스럽길, B) 리눅스에서 모든 게임 플랫폼이 지원되길, C) macOS 게이밍이 너무 비싸지 않길, 이 세 가지임
          + 요즘 GPU 덕분에 클라우드 PC 게이밍 서비스 가격이 헐값임, 나도 boosteroid로 한 달 12달러만 내고 게임함, 게이밍 PC를 직접 샀으면 그 돈을 7년치 내야 하고, 그동안 하드웨어 업글도 안됨, 클라우드는 하드웨어가 점점 좋아지고 언제든 정지 가능함, 약간 딜레이 있지만 캐주얼 플레이엔 문제 없고 인터넷만 빠르면 충분히 시도해볼 가치 있음
          + 나는 여전히 Windows 10 사용 중임, Flow Launcher(https://www.flowlauncher.com/)를 단축키에 매핑해 사용하기에 시작 메뉴 거의 안 씀, 광고 없음, Windows 11로 넘어가면 타사 작업표시줄도 알아봐야 할 듯함, 내가 윈도우에 남아 있는 이유는 배터리 관리 신뢰성 때문임, 일주일 내로 20% 이상 방전되지 않는 보장이 있어야 하고, 리눅스에서는 그게 안 됨, 그리고 화면 잠금 중에 재생/정지 같은 키가 동작 안 되는 것도 불편함
          + Windows 11의 불편은 Home 버전이라 더 심함, Pro 버전은 대부분 비활성화 가능함, 가끔 새로 귀찮은 기능 나와도 대부분 해제할 수 있음, 데스크탑에서는 WSL2 통합으로 리눅스용 그래픽 앱이 CUDA까지 써서 최고임, 랩탑엔 오히려 Linux Mint + i3wm처럼 가벼운 환경이 훨씬 좋음, 한번 익숙해지면 전통적인 데스크탑 환경으로 못 돌아감
     * 이 통계엔 여러 가지가 작용한다고 생각함, 1) 데스크탑 내 상대적 점유율만 보이므로, macOS는 iPadOS에, 윈도우는 스마트폰에 사용자를 뺏길 수 있음, 2) Valve 등이 리눅스에서 게임 환경을 획기적으로 개선했고, 나도 옛날엔 게임 때문에 듀얼부트 필수였지만 Steam Deck 덕분에 다시 리눅스로 회귀함, 이제 게임도 리눅스에서 가능해서 선택지가 늘어남, 3) 프라이버시 의식이 상승 중이고, AI 학습 데이터로 개인 데이터를 쓰려는 빅테크 영향도 있음
          + 데스크탑 통계라 윈도우를 완전히 안 쓰는 사람은 여기 비중에 잡히지 않음, 오로지 데스크탑 OS 갈아탄 사람만 수치에 반영임
     * 오늘 아침에도 사적인 Teams 미팅이 있었는데, 내 개인 기기는 전부 리눅스라 윈도우와 Teams가 설치된 업무용 랩탑을 준비함, 미팅 전에 부팅했는데 업데이트 설치하느라 재부팅 두 번 하고 미팅 5분 전에 결국 리눅스 데스크탑에서 Teams 웹버전으로 접속해 겨우 시간 맞춤, 리눅스 덕분에 겨우 살았음, Teams는 OS/브라우저 가리지 않는 게 장점임, 그러나 윈도우 업데이트가 너무 심하게 강제됨, 최근 동료들도 강제 재부팅 때문에 불만 많음, 보안 측면에서는 이해하지만 리눅스 대안 있어서 다행임, 5년 전부터 집은 리눅스 전용으로 쓰는데 내 선택이 계속 옳다는 걸 확인 중임, 게임도 필요한 건 전부 리눅스에서 돌아감
          + 한때 Teams for Linux 앱이 있었는데, 그냥 Electron 앱인데도 리눅스에선 유난히 버그가 심했음, MS가 크로스플랫폼 프레임워크 써놓고 이런 걸 보면 코드가 얼마나 이식성 떨어지는지 알 수 있음
          + 나도 비슷하게 Mac에서 Teams를 쓰다가, 업데이트 후 작동 안 되는 Teams 앱이 두 개나 설치됨(“Teams”와 “Teams new”), 그나마 리눅스 노트북이 옆에 있어 웹버전으로 해결함, 마이크로소프트가 이 느리고 불안정한 스탠드얼론 앱의 의도를 뭘로 보는지 궁금함
          + 오랜 기간 사용 안 한 컴퓨터에는 이 패턴이 반복됨, 매일 쓰면 괜찮지만 6개월쯤 내버려두면 '패치의 도시'가 펼쳐짐, 게임 콘솔도 똑같음, 자주 쓰는 컴퓨터는 10~15초면 바로 쓸 수 있음
          + 윈도우 랩탑을 매일 쓰는지 궁금함
     * AI 트레이닝 웹 스크래핑 봇이 어떤 OS로 잡히는지 궁금함, 만약 이게 비율 5%의 리눅스를 포함한다면 의미가 다를 수도 있음
          + 이런 설문은 전부 봇을 최대한 걸러내려 함, 인터넷 트래픽의 50% 이상이 봇이기 때문에 그렇지 않으면 결과 해석 자체가 안 됨
          + 이런 리눅스 점유율 기사가 실제 리눅스 확산 정황과는 거의 무관할 수도 있음, 예전 중국 정부가 윈도우 불법복제 강력 단속 후 PC 공급업체가 리눅스로 출하했지만, 집에 들여오면 바로 해적 윈도우 재설치하는 문화가 퍼졌다는 실제 사례가 있음, 그때도 실제 설치 사용률은 다른데 출하 기준만 기사화됨
          + 일반적으로 봇의 User-Agent는 자가 식별을 하거나, 탐지 어려움 위해 흔한 UA로 위장함, 웹스크래핑 자체는 AI 이전부터 대개 리눅스에서 돌았을 것, 내가 분야 전문가는 아니고 더 정확한 경험 있으면 공유 바람
          + OpenAI의 봇 식별 문서에서 봇은 굳이 특정 OS 표시 필요 없음
          + 자동화는 거의 전부 리눅스에서 이뤄짐, 거의 99.99%는 리눅스, 그외 BSD 쓰는 예외 정도임
     * 최근 노트북에 Arch / Gnome 설치해봤는데 윈도우 11보다 3배 빠른 느낌임, 데스크탑에는 오래 전부터 리눅스 썼지만 예전에는 노트북에선 전원관리 문제(덮개 닫을 때 문제 등)로 망설였음, 이번엔 모두 잘 동작함
          + Windows 11이 특히 느림, ThinkPad Carbon X1에 설치했는데 부팅 때 아무 것도 안 되는 수준이고, copilot, O365 등이 실행되느라 더딤, 프로세스와 설치 프로그램을 간신히 정리하니 그제야 쓸 만해졌음
          + 리눅스에도 여전히 이슈가 남아있음, 예를 들어 Framework 포럼에서 리눅스 사용자들의 덮개 닫힘, 고배터리 소모 등 문제 제기가 있음(링크: Community Threads), Fedora에서 쓰고 있는데 약간의 꼼꼼한 설정은 필요함
     * 10년 단위로 넓게 보면 시장점유율 그래프가 오르내림이 심함, 에러바도 클 것으로 보임
          + 일정 수준의 임계점이 필요하고 지금 그 수준엔 도달한 것 같음, 파워유저 & 기술 담당자들이 주로 쓰는 환경인 채로 유지되는 것도 바람직함
"
"https://news.hada.io/topic?id=22094","SaaS 2.0 - Software-as-a-Service 에서 Specialist-and-a-Spreadsheet 로","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   SaaS 2.0 - Software-as-a-Service 에서 Specialist-and-a-Spreadsheet 로

     * Salesforce 등 전통적인 SaaS 제품은 결국 여러 개의 목록(리스트) 관리와, 내장된 업무 노하우(Playbook) 를 결합한 형태임
     * SaaS는 대부분 사용자에게 실용적 도구(리스트, 노트, 작업 관리 등)와 전문가의 관점(업무 방식, 규칙, 프레임워크 등)을 동시에 제공함
     * 하지만 기존 SaaS는 평균적인 팀을 위한 보편적 규칙에 맞춰져 있어, 각 조직의 개별적 요구나 세밀한 예외 처리가 어려움
     * 미래의 SaaS는 AI 기반 전문가 에이전트가 사용자를 대신해 업무를 처리하고, 맞춤형 워크플로우와 리스트 관리를 제공하는 ‘전문가+스프레드시트’ 형태로 발전할 것임
     * 새로운 SaaS 2.0 시대에는 사용자가 직접 복잡한 UI를 다루지 않고, AI 전문가가 리스트와 업무를 전담 관리하는 경험 중심 서비스가 대세가 될 것
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Salesforce의 본질: 리스트와 플레이북

     * Salesforce의 대표 제품(고객관계관리 툴)은 결국 여러 개의 리스트로 구성됨
          + 고객 리스트, 잠재고객 리스트, 제품 리스트, 커뮤니케이션 내역 등
          + 리스트의 데이터베이스 역할과 이를 읽고 편집하는 UI가 결합된 구조임
     * 실질적으로 리스트 자체는 스프레드시트로도 구현할 수 있지만, 대규모·복잡한 목록의 신뢰성 관리는 SaaS가 더 효과적임
     * Salesforce는 연간 379억 달러의 가치를 제공하는, 복잡한 리스트 관리와 안정성의 집약체임

SaaS의 또 다른 측면: 업무 노하우(Playbook) 내장

     * 단순히 리스트만 제공하는 것이 아니라, 실제 세일즈 업무에 필요한 복잡한 프로세스와 협업 규칙도 함께 제공
          + 예: 잠재 고객 발굴, 소개, 미팅, 후속 조치, 자료 준비 등 일련의 영업 과정
          + 업계 표준 프레임워크(BANT, CHAMP, FAINT, NEAT, SPICED, SPIN 등)를 제품의 기본값에 녹임
     * Salesforce는 거래 ‘기회’를 만들고, 계정과 연결하며, 단계별로 이동해야 하며, 특정 필드(계약 금액 등)를 반드시 입력하게 함
     * 이러한 리스트는 단순 목록이 아니라, ‘이렇게 일해야 한다’는 규칙과 절차가 구조적으로 포함된 의견이 담긴 리스트임
     * 많은 SaaS가 이처럼 전문가의 노하우를 UI와 기능, 기본값 등으로 녹여내 사용자에게 업무 방향성을 제시함

SaaS의 구조적 한계

     * Salesforce 등 SaaS는 ‘평균적인 팀’을 위한 제품임
          + 실제 세일즈 현장은 세밀한 맥락, 예외, 중간 단계 등이 많지만, 시스템에는 단순화된 단계(예: Prospecting→Qualification)와 필드만 기록됨
          + 모든 팀이 동일한 방식(MEDDIC, MEDPICC 등)으로 일하는 것은 아님에도, SaaS는 표준화된 규칙과 기본값을 강제함
          + 예외나 특수 상황은 필드 우회, 형식적 입력 등으로 대응하지만, 실제 맥락은 시스템에 반영되지 않음
     * 사용자가 Salesforce를 완벽히 자신의 팀에 맞게 커스터마이징하려면, 컨설턴트 고용 등 추가 비용과 노력이 듦
          + 맞춤화를 위해 컨설턴트 시장이 성장했고, Salesforce 커스터마이징 시장만도 연간 180억 달러 규모에 이름

대안: 맞춤형 SaaS, 그리고 AI 전문가

     * 더 좋은 방법은 자신만의 완전히 커스텀된 Salesforce를 직접 구축하는 것이지만, 현실적으로 비용과 리소스가 큼
     * 혹은 아예 Salesforce의 세일즈 전문가를 직접 고용해서 리스트 관리를 맡기면, 시스템의 틀에 얽매이지 않고 업무 자체에 집중할 수 있음
          + 전문가가 판단해 예외 처리, 상황별 유연한 대응 가능
          + 사용자는 ‘이런 일이 있다’고 말하면 전문가가 알아서 리스트를 관리하고, 필요한 조언을 제공함
     * 현실적으로 전문가를 무한정 고용할 수는 없지만, AI가 무한한 전문가로 복제되어 이 역할을 대체한다면?
          + AI 봇이 세일즈 플레이북을 정확히 따라가며, 스프레드시트 형태의 리스트를 매일 관리
          + 미팅 전 상황 정리부터 각 상황별로 전문가가 할 만한 판단, 후속 업무 조언 및 제안, 예외 처리까지 AI가 직접 수행
          + 제품의 본질은 데이터베이스+워크플로우의 명시적 설명+전문가 프롬프트가 됨
     * “Salesforce에 없으면 실제로 일어난 일이 아니다”라는 불안감처럼, 사용자는 리스트와 시스템을 직접 보고 싶어함
          + 그러나 이메일도 마찬가지로, 진짜 중요한 사람은 Gmail이나 Superhuman 같은 도구가 아니라 전문가(EA)가 모든 메일을 관리해줌
          + 사용자는 더 이상 UI와 목록을 직접 만지지 않고, AI 전문가에게 원하는 결과만 전달하고 모든 처리를 맡김
     * 실제로 이메일 관리, 데이팅, 세일즈 등에서도 이런 전문가 에이전트 기반 서비스가 등장 중임

사례: 데이팅 앱 Sitch

     * Sitch는 기존 데이팅 앱처럼 단순 ‘리스트+매칭’이 아니라, 인간 매치메이커의 전문성을 AI로 재현함
     * 사용자는 약 50개의 상세 질문을 AI에게 답하고, AI가 그 답변과 노하우를 바탕으로 맞춤형 매칭을 제안
     * 양측이 동의하면 AI가 그룹챗을 생성, 이후 피드백을 받아 더욱 정교한 개인화 진행
     * AI가 리스트와 매칭을 전담 관리하며, 사용자는 결과만 신뢰하면 됨
     * 핵심은 ‘전문가가 리스트를 관리하는 모델’ 을 AI로 구현했다는 점임

SaaS 2.0 : ‘전문가+스프레드시트’ 시대의 소프트웨어

     * 궁극적으로 가장 이상적인 소프트웨어는 사용자가 ‘원하는 것만 말하면 나머지는 알아서 관리’해주는 AI 기반 전문가 서비스임
          + 사용자는 원하는 바만 전달하면, AI가 리스트·업무·판단·예외 관리까지 모두 수행
          + 복잡한 UI, 리스트 확인, 운영 인프라에 집착할 필요가 없음
     * 이는 소프트웨어가 단순히 호스팅된 기능 제공이 아니라, ‘전문가+스프레드시트’ 조합으로 사용자 맞춤형 업무를 대행하는 구조
     * 궁극적으로, SaaS 2.0은 전문가와 스프레드시트가 결합된 AI 서비스가 모든 업무와 리스트 관리를 대행하는 모델이 될 것
"
"https://news.hada.io/topic?id=22003","F2 - 파일 이름 일괄 변경 CLI 도구 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        F2 - 파일 이름 일괄 변경 CLI 도구

     * 빠르고 안전한 커맨드라인 기반 파일/디렉토리 Batch Rename 도구. Go로 개발되어 크로스 플랫폼 지원
     * Dry-Run이 기본으로 실제 변경 전 항상 미리보기 실행하여 변경 내역을 안전하게 검토할 수 있음
     * 파일명·확장자·EXIF/ID3 등 속성을 변수로 사용 가능, 규칙적 자동화 작업에 강점
     * 강력한 문자열·정규식 치환 지원해서 간단한 이름 변경부터 복잡한 패턴 변환까지 가능
     * 모든 변경 작업은 실행전 사전 검증되어 충돌을 감지하고, 자동으로 해결
     * 수천개의 대량 파일도 빠른 속도로 일괄 처리 가능
     * Undo 기능을 지원해 언제든지 이전 상태로 손쉽게 복구 가능
     * 풍부한 문서로 실제 예시와 다양한 상황별 가이드 제공, 초보자도 쉽게 사용법 습득 가능

   오 좋네요. 과거에 rename 명령어 써서 했었지만 미리 확인하고 undo 지원만 되도 정말 편하겠네요

   darknamer로 보통은 충분할 것 같은데 성능 필요한 부분들은 이거 쓰면 더 좋겠네요.
"
"https://news.hada.io/topic?id=21975","업스테이지, 프론티어급 추론 모델 Solar Pro 2 출시 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   업스테이지, 프론티어급 추론 모델 Solar Pro 2 출시

     * Solar Pro 2는 31B 파라미터의 컴팩트한 규모에도 불구하고 차세대 추론력, 다양한 도구 활용성, 동급 최고 수준의 한국어 및 다국어 처리 성능을 갖춘 업스테이지의 새로운 프런티어 언어 모델
     * 한국어 주요 벤치마크(예: Ko-Arena-Hard-Auto, Ko-MMLU 등) 에서 GPT-4, Claude 3와 대등하거나 앞서는 결과를 보이며, 법률·금융·의료 등 전문 도메인에서도 일관되고 정확한 답변을 생성
     * 고도화된 추론 모드에서 수학 문제, 논리 질의응답, 복잡한 멀티스텝 추론 등 다양한 작업을 처리할 수 있으며, 코드·엔지니어링 평가에서도 뛰어난 성과를 입증
     * 실제 업무 적용을 위한 에이전트형 구조와 툴 연동, 파일 생성, 자율 실행 기능을 갖추고 있어, 기업 환경에 바로 도입할 수 있음
     * 클라우드·온프레미스 배포, 안정성 및 사용성 강화, 엔터프라이즈 도입 지원 등 실질적 비즈니스 활용성을 중시하는 차세대 LLM
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

글로벌 프런티어급 성능 입증, Solar Pro 2 공식 출시

     * Solar Pro 2는 업스테이지가 개발한 차세대 프런티어 언어 모델로, 31B 파라미터의 컴팩트한 규모에도 불구하고 광범위한 다국어 처리, 고도화된 추론력, 실무 최적화된 도구 활용성을 제공함
     * 특히 한국어 처리에서 GPT-4, Claude 3와 경쟁할 만큼 뛰어난 성능을 입증, 법률·금융·의료 등 고난도 도메인에서도 정확성과 일관성을 보임

동급 최고 수준의 한국어 처리 성능

     * Ko-Arena-Hard-Auto 등 벤치마크에서 최상위 모델들과 동등한 결과를 달성함
     * Ko-MMLU, Hae-Rae, Ko-IFEval 등 다양한 한국어 NLP 과제에서 언어 이해·생성 전반에 걸쳐 선도적 성능을 보임
     * 전문 도메인(법률, 금융, 의료 등) 에서도 안정적이고 정확한 결과를 제공함

진화된 추론 능력

     * 추론 과정의 투명성과 설명 가능성이 중요한 현시점에서, Solar Pro 2는 단순 예측을 넘어 분석·종합·다단계 사고를 실현함
     * MMLU, MMLU-Pro, HumanEval 등 일반 추론 벤치마크에서 한국어 멀티스텝 과제의 성능이 크게 향상됨
     * Math500, AIME 등 고난도 수학 문제, SWE-Bench Agentless 등 복잡한 개발 과제 처리 등에서도 우수함
     * 파라미터 수 대비 뛰어난 추론 효율성을 제공함

실무를 움직이는 에이전트형 LLM

     * Solar Pro 2는 툴 연동, 파일 생성, 자율 작업 실행 등 실제 업무에 바로 적용 가능한 기능을 갖춘 에이전트형 LLM임
     * 크기만 큰 모델이 아닌, 실질적으로 업무에 투입할 수 있는 현실적인 AI임을 강조함
     * 예시: 경쟁사 동향 보고서 자동 생성 등 다양한 업무 자동화 시나리오 적용 가능

   크게 향상됨, 우수함, 정확함을 수치로 보여줬으면 좋았겠네요.

   Claude 4 나온 시점에서 Claude 3 랑 비교하는건 준 사기 아닌가요...
"
"https://news.hada.io/topic?id=21995","소프트웨어를 빠르게 개발하는 나만의 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         소프트웨어를 빠르게 개발하는 나만의 방법

     * 완벽함과 속도의 균형은 쉽지 않지만, 상황에 맞는 적정 품질과 기한 준수가 중요함
     * 초안(러프 드래프트) 개발을 먼저 진행하고, 이후에 코드 품질을 개선하는 방식이 효과적임
     * 요구사항을 완화하거나 과도한 요구를 줄이면 속도와 효율을 높일 수 있음
     * 산만함을 피하고 작은 단위로 자주 커밋하며, 핵심에 집중하는 습관이 필요함
     * 빠른 개발에 도움이 된 코드 읽기, 데이터 모델링, 스크립팅, 디버깅, 순수 함수 지향 등의 구체적 스킬이 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“얼마나 좋은 코드여야 하는가?” – 품질 기준과 현실적 선택

     * 초창기에는 모든 코드가 완벽하길 원함
          + 모든 함수가 철저히 테스트되고, 변수명이 우아하며, 추상화가 명확하고, 버그가 전혀 없는 코드를 꿈꿨음
     * 하지만 시간이 흐르며 “정답은 없다” 는 현실을 배움
          + 상황에 따라 요구되는 코드 품질이 다름
          + 24시간 게임잼: 완성된 코드가 꼭 깔끔하거나 버그가 없어야 할 필요 없음
               o 제한된 시간 내에 동작하는 결과물을 만드는 것이 더 중요함
          + 심장박동기 소프트웨어: 실수 하나가 사람의 생명을 위협할 수 있으므로
               o 최고 수준의 신뢰성과 안전성이 필수임
     * 대부분의 프로젝트는 이 두 극단 사이에 존재함
          + 어떤 회사는 빠른 납기를 요구해 약간의 버그는 허용함
          + 어떤 프로젝트는 높은 품질을 요구하지만 일정이 넉넉함
          + 실제 업무에서는 이 균형을 파악하는 능력이 중요함
          + 팀의 ‘충분히 괜찮음(good enough)’ 기준이 무엇인지를 먼저 파악함
               o 허용 가능한 버그의 범위, 완벽하지 않아도 괜찮은 부분 등 실질적 기준을 함께 점검함
     * 본인의 개인적 기준은
          + “10점 만점에 8점 품질을, 기한 내에 달성” 하는 것
               o 코드는 목적을 충실히 수행하고, 치명적인 문제는 없지만 사소한 이슈는 남아 있을 수 있음
               o 가장 중요한 건 기한 내에 제출하는 것임
          + 단, 이 기준도 프로젝트의 맥락에 따라 유연하게 조정
               o 때로는 완벽을 추구해 일정이 밀려도 괜찮고,
               o 때로는 완성도는 낮아도 일단 빠르게 끝내는 편이 더 가치 있을 때가 있음

Rough drafts – 러프 드래프트, 프로토타이핑의 실제적인 활용과 장점

     * 소프트웨어 개발도 글쓰기처럼 초안(rough draft, spike, walking skeleton) 작성이 매우 유용함
     * 최대한 빠르게 러프 드래프트를 구현하고, 이후에 이를 다듬어 완성형 솔루션으로 발전시킴
     * 나의 러프 드래프트 코드는 버그 투성이이고, 테스트 실패, TODO 주석 남발, 예외 미처리, print/log 남용,
       성능 고려 없음, WIP 커밋 메시지, 불필요한 패키지 추가, 반복 코드, 하드코딩, 린터 경고 등 엉망임
     * 이 과정이 비효율적으로 보이지만, “최소한 문제의 본질을 파악할 수 있는 상태” 에 도달하는 것이 목적임
     * 당연히, 이런 초안 상태의 코드를 최종 배포로 내보내지 않으며, 실제 배포 전에는 반드시 정제함
       (팀에서 초안 코드를 그대로 내보내자고 압박할 때도 있지만 최대한 저항함)
     * 러프 드래프트 접근법의 주요 장점
          + “알려지지 않은 문제(unknown unknowns)”를 빨리 드러냄
               o 완성된 후 버려지는 코드보다, 초기에 프로토타입 단계에서 미지의 장애물을 찾는 것이 훨씬 유리함
          + 프로토타입 작성 중 자연스럽게 사라지는 문제들이 많음
               o 느린 함수나 잘못된 구조도, 나중에 아예 필요 없게 되는 경우가 많아 시간 낭비를 막을 수 있음
               o 지나치게 일찍 최적화나 테스트에 힘 쏟을 필요 없음
          + 집중력을 높여줌
               o 불필요한 리팩터링, 네이밍 고민, 다른 코드베이스 고치기 등 산만함을 방지하고
                 오직 현재 해결해야 하는 문제에 몰입할 수 있음
          + 불필요한 조기 추상화 방지
               o 일단 동작하는 해답을 빠르게 만드는 과정에서는 미래를 위한 불필요한 추상화를 덜 시도하게 됨
               o 당장의 문제에만 집중하여, 불필요하게 복잡한 설계를 피함
          + 진척 상황의 명확한 소통
               o 러프 드래프트를 통해 앞으로 얼마나 남았는지 정확한 예측이 가능해짐
               o 무엇이든 동작하는 것을 먼저 보여주며, 이해관계자의 피드백과 방향성 변경이 빠르게 이뤄짐
     * 러프 드래프트 실전 운영법
          + 되돌리기 어려운 결정(biding decision)은 초안 단계에서 반드시 실험
               o 예: 언어, 프레임워크, DB스키마 등 큰 방향성은 초기에 확인
          + 모든 임시방편/핵(hack)은 반드시 TODO 주석 등으로 기록
               o polish(정제) 단계에서 git grep TODO 등으로 전수 조사해 보완함
          + Top-Down(상위-하위) 순서로 개발
               o UI, API 등 사용 방식부터 scaffold(뼈대) 작성, 내부 로직은 하드코딩/임시 구현도 허용
               o 실제로는 UI/사용 경험이 정해지면서 하위 로직이 바뀌는 일이 많으므로, 상위 레이어부터 구현이 유리함
               o 하위부터 완벽히 구현 후 상위에 맞추는 방식은 비효율적임
          + 작은 변화는 따로 패치 분리
               o 러프 드래프트 도중 코드베이스 개선이나 의존성 업데이트 필요성을 발견하면,
                 해당 부분만 별도 PR/커밋으로 분리해서 빠르게 반영
               o 전체 변경의 복잡도를 줄이고, 리뷰/통합 속도를 높임

     참고: “코드의 첫 번째 초안은 버려라”, “지금 당장 단순한 시스템이 최고”, “YAGNI(You Aren’t Gonna Need It)”

요구사항을 바꿔보려는 시도

     * 덜 하는 것이 더 빠르고 쉽다는 원칙을 강조함
     * 실제 업무에서, 주어진 과업의 요구사항을 완화할 수 있는지 항상 고민함
          + 예시 질문:
               o 여러 화면을 하나로 합칠 수 있는가?
               o 까다로운 엣지 케이스를 굳이 다뤄야 하는가?
               o 1000개 입력을 지원해야 한다면 10개만 지원해도 되는가?
               o 완성형 대신 프로토타입으로 대체 가능한가?
               o 이 기능 자체를 빼버려도 되는가?
     * 이러한 접근은 개발 속도와 효율을 높임
     * 조직 문화 자체도 조금씩 더 느리고 합리적인 페이스로 유도하려고 시도함
          + 갑작스럽고 큰 변화 요구는 잘 먹히지 않음
          + 점진적인 제안, 토론 방식 전환 등으로 조금씩 분위기를 바꿈

코드에서 산만함(Distraction) 피하기

     * 외부 환경(알림, 회의) 뿐 아니라, 코드 작업 중 엉뚱한 일로 새는 것도 큰 방해 요인임
     * 나도 종종 버그를 고치다 전혀 상관 없는 곳을 뜯고 있고, 결국 원래 과제는 미루어짐
     * 두 가지 구체적 실천법:
          + 타이머 설정: 한 작업마다 시간 제한을 두고, 알람이 울리면 현재 진행 상황을 점검
               o 예상보다 시간이 더 걸릴 때 주의 환기 효과 있음
               o 알람과 동시에 git commit 하면 작은 성취감도 생김
               o (이 방법은 시간 추정 연습에도 효과적임)
          + 페어 프로그래밍: 함께 작업하면 쓸데없는 길로 새는 일 감소, 집중력 유지에 도움됨
     * 일부 개발자에겐 이런 산만함 회피가 자연스럽지만, 내겐 의식적 집중과 습관화가 필요함

작은 단위의 변경, 작게 쪼개기

     * 예전에 큰 단위의 패치, 광범위한 변경을 장려하는 상사가 있었지만
       실제로는 매우 비효율적임을 경험함
     * 작고 집중된 diff가 거의 항상 더 낫다고 느낌
          + 코드를 작성하는 데 부담이 적고
          + 코드 리뷰가 더 쉽고 빨라져 동료의 피로감도 줄고, 내 실수도 쉽게 발견됨
          + 문제가 생겼을 때 롤백이 쉽고 안전함
          + 한 번에 바꾸는 범위가 작으므로 신규 버그 발생 위험도 감소함
     * 큰 기능/기능 추가도 작은 변경의 축적으로 완성
          + 예: 화면 추가가 필요하다면, 버그 수정/의존성 업그레이드/기능 추가를 각각 별도 패치로 분리
     * 작은 단위의 변화가 더 빠르고 높은 품질의 소프트웨어 개발에 도움이 된다고 강조함

빠른 개발에 정말 도움이 된 구체적인 스킬들

   위에서 언급한 내용들은 다소 추상적이지만, 실제로 빠른 개발에 효과적인 실전 스킬도 존재함
     * 코드 읽기(Reading code) : 지금까지 습득한 가장 중요한 개발자 역량임
          + 기존 코드를 능숙하게 해석할 수 있으면, 디버깅이 훨씬 쉬워지고
          + 오픈소스/서드파티 라이브러리의 버그나 부족한 문서도 크게 두렵지 않게 됨
          + 타인의 코드를 읽으며 배우는 양도 엄청나고, 전반적인 문제 해결 능력 향상에 직접적 도움을 줌
     * 데이터 모델링(Data modeling) : 시간이 걸려도 데이터 모델을 제대로 설계하는 것이 중요함
          + 잘못 설계된 데이터베이스 스키마는 나중에 다양한 문제와 복잡한 수정 비용을 초래함
          + 유효하지 않은 상태 자체를 아예 표현할 수 없도록 설계하는 것이 버그를 원천적으로 줄임
          + 데이터가 저장되거나 외부와 주고받는 경우엔 더더욱 신중해야 함
     * 스크립팅(Scripting) : Bash, Python 등으로 짧은 스크립트를 빠르게 작성하는 능력은 개발 효율을 극대화함
          + 매주 여러 번씩, 마크다운 정렬, 데이터 정리, 파일 중복 찾기 등 자동화 작업에 활용
          + Bash는 Shellcheck 같은 도구로 문법 오류를 사전에 예방함
          + Robust하지 않아도 되는 작업엔 LLM의 도움을 받아 빠르게 완성 가능
     * 디버거(Debuggers) 활용 : 디버거 사용은 print/log만으로는 불가능한 신속한 문제 진단과 코드 흐름 파악에 필수적임
          + 복잡한 버그의 근본 원인 파악이 훨씬 빨라짐
     * 적절히 쉬는 타이밍 : 막힐 때는 과감하게 휴식을 취하는 습관
          + 장시간 고생해도 못 푸는 문제가, 5분 쉬었다가 바로 풀리는 경험이 빈번함
          + 집중의 효율성을 위해서도 중요함
     * 순수 함수와 불변 데이터 지향 : 함수형 프로그래밍: 순수 함수와 immutable data를 선호하면
          + 버그 감소, 상태 추적 부담 감소, 코드의 명확성/예측성 증가
          + 복잡한 클래스 계층 설계보다 더 단순하고 효과적인 경우가 많음
          + 반드시 항상 가능한 건 아니지만, 기본적으로 이 방식을 먼저 고려함
     * LLM(대규모 언어 모델) 활용 : LLM(예: ChatGPT 등) 은 단점도 있지만, 반복적이거나 자동화가 가능한 개발 업무에서 큰 속도 향상을 가져옴
          + 자신의 코드에 LLM을 접목하는 방법과 한계를 충분히 이해한 뒤 적극 활용 중
          + 커뮤니티의 다양한 경험과 팁, 사례도 참고함
            이 모든 스킬들은 오랜 시간 반복적으로 연습해왔고, 실제로 빠른 개발에 큰 자산이 되었음

요약

     * 내가 소프트웨어를 빠르게 개발하며 얻은 핵심 교훈은 다음과 같음
          + 과제별로 필요한 코드 품질 기준을 명확히 파악
          + 러프 드래프트(초안)를 빠르게 작성해 전체 윤곽을 잡음
          + 요구사항을 완화할 수 있는 여지를 항상 탐색
          + 산만함에 휘둘리지 않고 집중력을 유지
          + 변경은 작고 자주 커밋하며, 큰 패치는 피함
          + 구체적 실전 스킬(코드 읽기, 데이터 모델링, 디버깅, 스크립팅 등)을 꾸준히 연습
     * 모두 너무 당연해 보이지만, 실제로 이 교훈을 얻기까지 오랜 시간이 걸렸음

        Hacker News 의견

     * 최근 몇 년간 빠르고 충분히 견고한 시스템을 구축하는 방법을 터득함
          + 한 가지 도구를 깊이 익히는 것이 중요함을 배움. 표면적으로 더 적합해 보이는 도구보다 내가 잘 아는 것이 훨씬 효율적임. 실제로 대부분의 프로젝트에서 Django가 딱 맞는 선택임
          + 가끔 Django가 너무 무겁지 않을까 걱정하며 프로젝트를 시작했지만, 결과적으로 프로젝트가 초기 의도를 훌쩍 넘어서 성장했음. 예를 들어 상태 페이지 앱을 만들었는데, Django의 한계를 피해가려는 노력이 비효율적임을 바로 깨달음
          + Django 모델에 맞는 대부분의 앱에서는 데이터 모델이 핵심임. 프로토타입이라도 데이터 모델 리팩토링을 미루면 나중에 비용과 난이도가 기하급수적으로 상승하게 됨
          + 대부분의 앱은 싱글 페이지 앱이나 무거운 프론트엔드 프레임워크가 필요하지 않음. 일부가 해당될 수도 있지만 전체 페이지의 80%는 전통적인 Django 뷰가 충분함. 나머지는 AlpineJS나 HTMX를 검토하면 됨
          + 대부분의 경우 직접 개발하는 것이 더 쉬움. Django로 CRM, 상태 페이지, 지원 시스템, 영업 프로세스 등 여러 기능을 빠르게 만들 수 있음. 상업용 CRM 연동보다 훨씬 빠름.
          + 지루할 정도로 평범한 기술을 고를 것. Python/Django/Postgres 조합이면 대부분 해결됨. Kubernetes, Redis, RabbitMQ, Celery 등은 잊어도 됨. Alpine/HTMX는 예외인데, JS 스택 대부분을 피할 수 있기 때문임
          + Redis와 Kubernetes는 나에게는 2025년의 ‘지루한 기술’임. 둘 다 극도로 안정적이며 쓰임새가 명확하고, 단점도 이미 잘 알려져 있어 신뢰도 높음. 나는 개인적으로 이 둘의 팬임. 내가 원하는 일을 정확히 해주니까 신뢰도가 높음
          + 나도 Django를 정말 좋아함. 프로젝트를 엄청 빠르게 시작해 배포할 수 있음
               o 직장에서는 Go를 쓰는데, 동일한 API 엔드포인트 개발에 코드가 10배 더 길어짐. 쿼리 파라미터, 페이징 등 기능이 추가될 때마다 코드가 점점 더 길어짐. 권한 모델 추가하면 더 심해짐
               o 물론 퍼포먼스 차이도 크지만, 실제로는 DB 쿼리가 퍼포먼스의 대부분을 좌우함. Python에서도 충분히 빠름
          + 정말로 ‘지루한 기술’을 고른다면 Postgres조차 한 번 더 생각해 볼 필요가 있음
               o 많은 사람들이 생각하는 것보다 Sqlite는 훨씬 더 규모가 큼. 로컬 개발/격리된 CI 인스턴스에서 특히 그렇고, 소규모 앱에서는 생산 환경에서도 충분히 쓸 수 있음
          + Celery는 Django 프로젝트에서 꽤 자주 쓰는 편임. 복잡성이 마음에 들진 않지만, PaaS 환경에서는 오히려 제일 덜 고통스러운 선택임
               o 매번 Celery 없이 해보겠다고 시작하지만, 결국 HTTP로 트리거되는 작업들이 타임아웃에 부딪혀서 Celery를 쓰게 됨. 그 단계에서 쓰레드, 크론잡(특히 PaaS에서는 어려움), Celery 셋 중 하나를 선택해야 함. 어떻게 대응하는지 궁금함
          + ""대부분의 앱은 SPA나 무거운 프론트엔드 프레임워크가 필요하지 않다""라는 주장과 ""하나의 도구를 깊게 익혀라""는 조언이 충돌되는 것 같음
               o 나는 모든 페이지를 React로 만듦. 그 이유는 SPA가 꼭 필요한 게 아니라, 결국에는 클라이언트 사이드 상태 관리가 필요한 일이 생겨서 처음부터 React로 모든 걸 만드는 것이 더 편리하다고 느꼈기 때문임. 처음엔 무겁게 느껴져도 결론적으로 효율적임
     * 거친 초안으로 코드를 남겼을 때, 흔히 관리자가 그런 코드를 그대로 ‘최종 버전’으로 배포함
          + 그래서 처음부터 robust한 코드로 씀. 심지어 테스트 하네스도 거의 배포 수준으로 견고하게 만듦
          + 핵심은 아주 질 좋은 모듈을 만드는 것임. 변경 가능성이 매우 낮거나, 변경 시 엄청나게 큰 이슈가 되는 부분은 아예 독립적인 모듈로 격리해서 의존성 형태로 임포트함
          + 이런 모듈 덕분에 새로운 앱을 매우 빠르게 개발할 수 있고, 품질도 계속 높게 유지 가능함
          + 직접 사용한 예시는 RVS_Checkbox, ambiamara, RVS_Generic_Swift_Toolbox 등임
          + 질문이 있는데, Swift에서 주석 마커로 ""* ##################################################################"" 같은 코드 패턴을 쓰는 게 표준인지 궁금함
               o 소스코드에서 매우 시각적으로 두드러짐
     * 프로젝트의 규모에 따라 접근 방식이 많이 달라짐
          + 개인 프로젝트나 소규모 팀이라면 ‘빠르고 거칠게’ 개발하는 게 최적임. 이런 방식이 소규모 개발의 강점임
          + 소규모에서는 버그가 생겨도 금방 고칠 수 있고, 팀원 모두가 전체 코드를 거의 완벽하게 이해하고 있음
          + 규모가 커지면 아키텍처 실수나 버그 수정 비용이 폭증함. 아키텍처는 필연적으로 복잡해지고, 대규모 리팩토링은 사실상 불가능함. 이런 환경에선 한 단계 한 단계 정확성이 최우선이 되어야 함
          + 맥락이 정말 중요함. ‘대규모’가 얼마만큼을 의미하는지는 다를 수 있지만, 내가 경험한 바로는 앱 간 API를 일찍 협의하여 프론트/백엔드 모두 빠르게 작업 환경을 갖추는 것이 항상 옳았음
               o 가능한 빨리 프로덕션 서버에 배포하여 테스트와 팀 간 이슈를 드러내는 게 효과적임
               o 글쓴이는 코드 관점에 집중하는 것 같은데, 대규모 팀일수록 이 점이 더 중요하다고 생각함
               o 단, 팀 간 계층식 의존성을 두는 아키텍처는 별로라고 생각하지만 실제로 많이 이루어지고 있음
          + 이런 상황에서는 시스템을 축소해서 운영해야 함. 모두가 거대한 시스템을 원하기는 해도 실제로는 필요하지 않음
     * “24시간 게임잼에서는 코드 품질에 신경 쓸 필요 없다”는 말이 있는데, 내가 해본 대부분 해커톤/코드 리뷰 경험상, 가장 좋은 성과를 낸 팀들이 코드 품질이나 rudimentary한 테스트 환경도 같이 챙겼음
          + 위 두 주장(빨리 하려면 코드 품질을 포기해야 한다 vs 좋은 성적팀일수록 품질이 높다)은 사실 상충하지 않음. 품질 좋은 팀이 꼭 코드 정갈함에만 매달렸던 건 아님
          + 게임잼 사례에서는 코드의 깨끗함에 너무 집착하면 오히려 전체 결과물이 좋지 못함. UE blueprint 같은 시스템이 왜 ‘깔끔함’보다 결과물을 우선시해야 하는지 보여줌
          + 어떤 사람들은 코드의 ‘청결함’을 전체적으로 평가하고, 다른 사람들은 불필요한 코드 개선의 세부적인 비용/효익을 평가함
               o 내 생각엔 후자가 어떤 상황에서도 훨씬 더 나은 성과를 내는 듯함. 해커톤에서든, 안정성 높은 제품 코드든 마찬가지임
     * “프로토타이핑을 해보면 예상 못 한 ‘unknown unknowns’가 드러난다”는 내용과는 달리, 내가 뭔가를 처음 만져볼 때는 항상 장점만 보이고 단점은 잘 보이지 않음
          + 실제로는 엣지 케이스 처리, 유저에게 친절한 에러 메시지, 부작용 제거 등 실제 기능 완성 단계에서야 진짜 문제(unknown unknowns)가 드러남
          + 아마도 내가 경험하는 unknown unknowns는 도구/프레임워크/라이브러리 자체에서 생기고, 저자는 문제 영역 자체에서의 unknown unknowns를 말하는 듯함
          + rough draft가 너무 거칠면 안 된다는 점도 맞음. 대충 넘어가면 안 되는 부분에서 진짜 문제가 터짐.
               o 예를 들어, 랠리 드라이버들이 미리 트랙 리서치를 대충 하면 예상 못 한 위험(예, 커브 앞의 방지턱 등)에 노출될 수 있음
          + 직접 쓸 도구를 만들 때는 대충 만들어도 무난하게 쓸 수 있는데, 그렇게 빠르게 만든 도구는 허점투성이여도 문제를 겪지 않음
     * 요즘처럼 구조조정이 잦은 테크 업계가 소프트웨어 품질과 엔지니어 생산성의 가장 큰 위협임
          + 해고의 불안감, 빠른 성과 압박은 창의성과 실험정신을 죽이고 번아웃을 유발함
          + 모두가 AI 같은 유행 이슈에 군중심리로 휩쓸리고, 비판도 하지 못하는 환경이 되어버림
          + LLM 자동 코딩보다 더 시급한 문제임
          + 항상 소프트웨어 품질의 최대 위협은 소비자가 품질을 위해 돈을 안 쓴다는 것임
               o 품질을 ‘느끼는’ 사용자층이 존재해도 새 제품을 품질만으로 성공시키기에는 역부족임
               o 소프트웨어 이외 분야, 예를 들면 자동차나 가전제품은 품질별로 가격이 다르지만 소프트웨어는 그렇지 않음
          + 프로그래밍 레벨의 벤더 락인이 실제로는 SaaS 락인보다 훨씬 파괴적임
               o 이미 하드웨어 시장은 소수에 의해 독점되고 있는데, 이제 소프트웨어까지 동일 기업들이 독점할 날이 옴
               o 결과적으로 컴퓨터 프로그래머 대신 LLM 프롬프터만 남게 될 것임
     * 24시간 게임잼 등 빠른 사이클에서 오히려 나쁜 코드는 치명적임을 느낌
          + 코드가 깔끔할수록 실수도 줄어들고, 작업 기억 부담도 덜하며, 막판에 원하는 변경이나 기능 추가, 문제 수정이 훨씬 쉬워짐
          + 24시간 프로젝트에서 가장 많이 일을 망치는 건, 코드를 느리게 쓴 게 아니라 자기 스스로 구석에 몰리거나 예측 불가한 문제에 부딪혀 탈선하는 것임
          + 물론 모든 버그를 잡아야 한다는 건 아님. 하지만 기초 품질이 낮으면 전체적으로 힘든 프로젝트 경험임
          + 시간이 더 많은 프로젝트에도 이 원칙은 적용됨. 시간이 많다고 대충 쓰는 게 나은 건 아님
          + 좋은 코드를 습관화하면 추가 비용 없이 품질을 담보할 수 있음. 그리고, 시간이 더 들더라도 결국엔 가치 있는 일임
          + 나도 같은 생각임. 여러 번 게임잼을 해봤지만, ‘엉성한 코드’는 마감 전 1-2시간에, 남이 안 건드릴 파일에 한해서만 허용함
               o 공통 로직 정리 등 코드 정리는 생각보다 오래 안 걸림
               o 현실적으로 어설픈 코드에서 발생하는 버그는 코드 정리로 절약한 시간보다 훨씬 크고 위험함
               o 단, 서로 비슷하지만 다른 기능(예, 빛 fade out vs 색상 fade out)은 반복 코드를 남겨두는 편임. 요구사항이 갈라지기 쉬워서임
          + 빠르고 좋은 코드를 쓰려면 결국 많이 써보는 게 답임
               o 반복적인 작업이 싫을 순 있어도, 실제로 효과적임
               o 시간 내에 깔끔하게 코딩할 수 있는 사람은 그 코드를 많이 써본 경험자임
          + 급할 때 fancy한 asset loader 같은 건 신경 쓰지 않고 그냥 정적 파일로 때움
               o 경로 탐색 등이 필요하면 그냥 breadth first search 같은 간단한 것으로 처리함
               o 이런 게 ‘나쁜 코드’가 아닌, 그냥 임시방편이자 빠른 해결책임
               o 물론 규정상 이런 모듈 사용을 금지할 수도 있어서 그 땐 주어진 규칙에 맞춰야 함
          + ‘좋은 코드를 쓰는 게 더 오래 걸린다’는 인식이 오해라고 생각함. 일정 요구사항 이상을 맞추려면 좋은 코드가 속도에 장애물이 되지 않음
     * “어느 정도가 ‘good enough’냐”라는 기준이 팀마다 다 달라서 그게 내 커리어에서 가장 큰 갈등의 원인임
          + 빅테크 출신은 테스트 미흡에 불만이고, 스타트업 출신은 속도가 느리다고 불만임
          + ‘good enough’ 기준을 명확히 문서로 기록해서 팀 내에 공유하는 것이 유익할 것임
          + 이게 바로 팀 차터, 즉 ‘우리가 일하는 방식’ 문서임
               o 팀 차터 샘플 참고 가능함
     * 글에서 언급되지 않은 중요한 요소 중 하나는 시간에 따른 개발 속도 저하임
          + 프로젝트와 팀 규모가 커지면 개발 속도는 자연스럽게 느려짐
          + 즉각적인 개발 속도에 조금 손해를 보더라도 장기적으로 개발 속도가 덜 저하되도록 초기에 테스트, 문서화, 결정 로그, Agile 미팅 등 준비가 필요함
          + 관찰성(observability) 같은 기능이나 테스트하기 쉬운 코드 구조를 미리 안 준비하면 나중에 엄청난 악영향이 남음
          + 솔로 개발자지만, 결정 로그·테스트·문서화 세 가지의 중요성을 체감함
               o 나는 “랩 노트북”이라고 부르는 실시간 설계 기록을 작성하는데, 이게 나중에 테스트와 문서화의 밑바탕이 됨
               o 랩 노트북 있으면 늦게 시작해도 더 좋은 문서를 빠르게 쓸 수 있음. 테스트는 설계가 변하지 않았는지 검증에도 도움이 됨
               o 아주 짧게 쓸 일회성 도구는 그냥 막 시작해도 되지만, 오래 쓸 시스템은 천천히라도 기초를 단단히 쌓는 게 결국 합리적이고 유지보수 가능한 결과를 줌
               o 별로 인기 없는 의견이지만, 설계는 먼저 종이에서 해보고 그 다음에 디지털로 옮기는 게 효과적임
     * 나에게도 익숙한 패턴임. rough draft, 혹은 아이디어 검증용으로 다른 스크립트 언어나 수동 실행을 묶은 작은 코드로 시작함
          + 이런 과정을 거쳐 오히려 “우리가 원하던 걸 만들 필요가 없겠다”는 결론에 도달할 때도 많았음
          + 코딩하다 집중이 흐트러지는 부분에 정말 공감함. 정리하다보면 토끼굴에 빠져 커밋 단위가 커져 동료들이 리뷰하기 힘든 상태가 됨. 결국엔 작업을 모두 폐기하고 다시 작게, 목적에 집중해서 진행하는 경우가 많음
          + 때로는 쓸 만한 조각만 따로 빼서 다른 PR로 올릴 수 있음
          + 비즈니스는 결과물을 빠르게 원하고, 코드의 트레이드오프를 debt가 산더미처럼 쌓여 아주 느린 개발 속도를 경험하기 전까지는 이해하지 못함
          + 중요한 것은 균형이고, 프로젝트마다 다른 기준이 적용될 수 있음
          + 그래서 작은, 집중된, 간단한 변경만 자주 하는 것이 도움이 됨
          + 하지만 큰 해결책을 작은 조각으로 나누는 게 생각만큼 쉽지 않음
          + 아무 관련 없는, 쓰이지도 않는 코드를 ‘나중에 필요할 것’이라는 이유로 커밋하는 경우를 자주 보는데, 결국 우선순위 변경·사람 이동 등으로 1년 뒤엔 그 모든 게 쓸데 없는 코드가 되고, 그 당시의 계획도 아무도 알지 못해 버리게 됨

   공감가는 이야기가 많네요.
   댓글들도 좋은데 이렇게 누군가가 정리하여 말하면, 그래서 자리를 깔아주면, 그에 대해 반론과 지지, 보충을 거쳐 더 완성된다 싶습니다.

   덧. ""지루한 기술"" 이라는 표현을 최근에 자주 보는데 영어로는 boring technology 군요.
"
"https://news.hada.io/topic?id=22062","NYPD, 얼굴인식 금지 우회해 친팔레스타인 학생 시위대 신원 확인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 NYPD, 얼굴인식 금지 우회해 친팔레스타인 학생 시위대 신원 확인

     * NYPD가 내부 정책상 금지된 얼굴인식 기술을 사용하지 않고, FDNY의 Clearview AI 접근 권한을 통해 학생 시위자의 신원을 확인함
     * 해당 사건은 법원의 결정과 법적 소송으로 드러났으며, FDNY가 NYPD를 대신해 운전면허 사진, 소셜 미디어 이미지 등을 활용해 식별함
     * 논란이 확산되며 시의회에서는 POST Act의 사각지대와 기관 간 정보공유 문제, 그리고 감시기술의 투명성 강화 필요성이 대두됨
     * 프라이버시 옹호단체와 일부 시의원은 FDNY와 NYPD의 방식이 법의 취지에 맞지 않으며, 감시기술의 남용 위험을 경고함
     * 결국 사건 대상자인 Zuhdi Ahmed에 대한 혐의가 기각되었으나, 정신적 충격과 사생활 침해 문제가 남음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NYPD, 얼굴인식 사용 제한 우회해 시위자 신원 확인

  사건 개요 및 과정

     * NYPD는 Columbia University의 친팔레스타인 시위를 진압하는 과정에서, 얼굴인식 사용 제한 정책을 우회하여 FDNY(소방국)의 Clearview AI 접근 권한을 이용해 시위자 Zuhdi Ahmed의 신원을 확인함
     * 이 절차는 법원의 결정과, FDNY의 얼굴인식 시스템 운영 내역에 대한 정보를 요구하는 Legal Aid Society의 소송을 통해 밝혀짐
     * 사건의 발단은 2024년 4월 Columbia 대학 시위 중 한 남성이 친이스라엘 시위자에게 돌을 던졌다고 보고되어, NYPD가 혐의자를 찾으면서 시작됨
     * FDNY가 Clearview AI를 통해 운전면허 사진, 고등학교 행사 및 졸업식과 같은 소셜 미디어 이미지를 활용해 Ahmed의 신원을 식별함

  Clearview AI 및 법적·제도적 배경

     * Clearview AI는 방대한 온라인 이미지를 분석하여 사진을 대조하는 알고리듬임
     * NYPD는 기존에는 해당 기술을 사용했으나, 2020년에 제정된 얼굴인식 정책에 따라 체포 및 보호관찰 사진 내에서만 제한적으로 이미지 검색이 허용됨
     * POST Act와 같은 추가 시 법에 따라 NYPD는 감시기술 활용 내역과 정책을 공개해야 하나, 최근 조사에서 일관적 실행 미비가 드러남
     * FDNY가 NYPD를 대신해 Clearview AI를 사용한 사례는 정책적 사각지대와 기관 협력의 법적·윤리적 문제를 부각시킴

  신원 확인 과정의 문제점 및 사회적 파장

     * FDNY는 Clearview AI와 DMV 데이터에 접근하여 Ahmed의 사진 정보를 NYPD에 전달했고, 디지털 편집된 운전면허 사진을 신원 확인 절차에 사용함
     * 해당 행위는 공식 NYPD 정책에 명백히 위배됨
     * 프라이버시 단체와 일부 시의원들은 해당 방식이 NYPD의 감시기술 금지 취지를 무력화하고, 감시 남용과 투명성 부재 문제를 제기함
     * 시의회에서는 POST Act의 사각지대 해소와 기관 간 정보공유 투명성 강화 입법 필요성을 언급함

  후속 조치 및 당사자 경험

     * Ahmed에 대한 증오범죄 혐의는 법원이 증거 불충분을 이유로 기각함
     * Ahmed 본인과 가족은 심각한 정신적 스트레스와 사생활 침해, 외부로부터의 혐오 메시지 등을 경험함
     * 법원과 시의회에서는 이번 사건이 뉴욕시 감시정책의 허점 및 시민의 권리 보장의 중요성을 환기시킨 사례임을 강조함

  전문가 및 시의회의 반응

     * 감시기술 감독 프로젝트 단체 등은 기관 간 감시기술 사용에 대한 투명성 요구와 함께, 그 누구도 기관 감시기술 활용 현황을 추정해야 하는 상황이 돼서는 안 된다고 지적함
     * 일부 시의원은 FDNY 및 기타 시 기관이 NYPD를 대신해 감시기술을 운용하는 것을 법적으로 금지하고, 각 기관의 기술 사용 내역을 강제 공시토록 하는 신규 입법 추진 의사를 밝힘
     * 반면 소수 의원은 FDNY가 해당 정보를 NYPD에 제공할 법적 권한이 있다고 주장함

  결론 및 현황

     * Ahmed는 사건 무혐의 처분 후 일상으로 복귀하려 노력 중이며, 해당 사건이 사회적 소외와 감시사회 현실에 대한 경각심을 심어주었다고 언급함

관련 상황 요약

     * 최근 몇 달간 NYPD가 여러 대학 내 친팔레스타인 시위 진압 및 학생 체포 과정에서 논란이 증폭됨
     * 시위 현장에서 과격 제압, 무기 사용, 드론 감시 등 비상식적인 방식이 동원됐으며, 관련 내역은 대중의 알 권리와 시민권 보호 측면에서 계속 논란 중임

참고

     * POST Act는 NYPD의 감시기술 사용·내역 공개를 요구하는 법으로, 이번 사건을 계기로 정보공유의 실질적 투명성 문제가 제기됨
     * Clearview AI 같은 기술의 도입이 정부기관 프라이버시, 시민권 보장, 내부 심사 체계에 미치는 영향에 대해 업계·기술 전문가, 스타트업 등도 동향 주시 필요

        Hacker News 의견

     * 한 시의 소방마셜이 FDNY의 얼굴 인식 소프트웨어 접근 권한을 이용해 NYPD 수사관들이 Columbia University에서 친팔레스타인 시위자를 식별하게 도와줌, 이는 경찰의 해당 기술 사용을 엄격히 제한하는 정책을 우회하는 행위임. 도대체 소방서가 왜 얼굴 인식 소프트웨어 접근 권한이 필요한지 궁금함
          + 소방마셜이 얼굴 인식 기능을 가진 게 중요한 게 아니라, 무료로 사용할 수 있고 간단하게 접근할 수 있었기 때문에 소방부서가 선택된 것임. 만약 소방마셜이 아니었다면 독립된 제3자도 사용할 수 있었을 것임. 얼굴 인식 금지를 우회하는 이런 사소한 허점은 많이 알려져 있음 (글로벌 탑 얼굴 인식 시스템 리드 개발자 경험임)
          + 경찰이 조사 목적으로 이메일을 보냈고, 화재 조사 같은 사건을 담당하는 소방마셜에게 연락함. 아마 이런 점 때문에 명분이 있는 걸지도 모르겠음
          + 소방서가 얼굴 인식을 왜 해야 하는지? 방화 조사 혹은 의심스러운 화재 현장에 있던 인원 식별 등의 이유일 수 있음
          + 이스라엘 때문임. 소방마셜이 친이스라엘이거나 이스라엘이 약점을 잡고 있다고 생각함. 외국이 집단학살을 저지르고 있다는 시위 때문임. 이런 사건이 발생하는 현실이 믿기지 않음
     * 반전(反戰) 운동을 억압하기 위해서 우리는 과연 얼마나 많은 권리를 포기해야 하는 걸까라는 의문을 가짐
          + 이 사람은 시위대에게 돌을 던진 혐의로 지목된 것임. 우리는 평화적 시위의 권리 옹호에 집중하는 듯함. 이 사람이 다른 시위자의 얼굴에 돌을 던지는 모습이 영상에 잡혔고, 결과적으로 누군가의 권리 행사를 폭력적으로 억압하려고 한 것임. 경찰 수사 과정의 부정 때문에 처벌을 피했다면, 이는 사회에 손실임. 이런 사람을 “권리 침해당한 시위자”로 포장하려는 말들에도 불구하고 본질이 달라질 수 없음
          + 정부가 1차 세계대전, 2차 세계대전, 베트남전 당시 반전·평화 운동을 어떻게 몰랐거나 어렴풋이 알면서도 많은 권리가 사라졌는지 언급함. 9·11 이후 자발적/비자발적으로 포기한 것도 많음. 정부는 줄곧 일관적으로 권리를 침해해왔음. 남북전쟁조차 기획된 일이라 생각하고, 국민은 권리 뿐 아니라 헌법적 기반부터 파괴당했다고 봄. 지금은 실질적으로 힘, 망상, 뇌물로 유지되는 무효계약이라고 믿음. 사람들을 깨닫지 못하게 하는 건 망상과 뇌물임
     * 정부에 강력한 도구 접근 권한을 주면 반드시 권력 유지 목적으로 남용함을 경험함
          + 실제로 2차 헌법(무기 소지권)과 같은 ‘국민이 정부에 물리적으로 저항할 수 있다’는 개념이 점차 사라질 것임. 얼굴 인식 기술과 데이터 브로커를 통한 정부의 상시 감시가 모든 국민 정보를 실시간으로 확보하게 만들고, AI로 NSA 2008 방식의 저장뿐 아니라 분석, 감정 평가, 검토 대상 자동 선정까지 가능해짐. 우리 사회가 정부와 군대(ICE/주방위군/해병대 등)가 시민사회 영역에 개입하지 않도록 압박하지 않으면, 진정한 악몽을 겪게 될 것임
     * Clearview AI라는 업체가 사용되었는데, 이 회사는 공개된 이미지를 수집함. 만약 정부가 이런 이미지를 제공했다면 얘기가 다르겠지만, 그래도 무섭긴 함. 차라리 공개적으로 사진을 SNS에 올리지 않는 게 나음
          + 나는 이미 10년 전에 소셜미디어 계정 모두 삭제했고, 그 전에도 내 사진 올리는 걸 안 좋아했음. 하지만 다른 사람들이 내 사진을 올릴 수 있고, 나도 그걸 통제할 수 없으며, SNS를 안 쓰면 이런 게시 사실조차 모르게 됨
          + “사진을 소셜에 올리지 마라”는 말이 결국 무고한 사람이 숨길 게 없어야 한다는 논리로 이어질 것 같음
          + 자기가 직접 사진을 올리지 않더라도, 친구들이나 가족이 올릴 수 있음. 그걸 전부 막는 건 불가능하다고 생각함
          + 사진을 SNS에 올리지 말라는 조언 자체가 이미 ‘표현 위축 효과’를 발생시키며, 바로 그러한 위축을 방지하기 위해 만든 것이 미국 수정헌법 1조임
          + 셀카를 소셜미디어에 올리지 않는 것이 언제부터 집회 및 언론의 자유 같은 시민권 보장의 전제가 되었는지 의문임
     * 요즘 저널리즘의 현실이 2025년에는 진짜 물건을 물건이라고도 부르지 못할 정도로 아이러니해졌음을 빈정섞인 어투로 표현함. 만약 내가 소유권 우회를 해서 누군가 은행 계좌의 돈을 가져가도 괜찮은 건가라는 농담을 덧붙임
          + 또다른 우스운 표현이라는 식으로 “친팔레스타인 학생 시위자”라고 불리지만 실제로는 “폭행 용의자”임. 왜 이런 식으로 친팔레스타인 시위자 집단 전체가 일부 폭력적 행위자의 행동과 연루되는지 모르겠음
     * 이 사람이 시위 활동 때문에 대상이 된 것인지, 아니면 범죄를 저지른 것 때문에 대상이 된 것인지가 중요 포인트라고 생각함. 경찰이 범죄 수사를 위해 이런 시스템에 접근하는 건 필요하다고 생각함
     * NYPD에는 이스라엘 지지자가 정말 많은지 의문이 듦
          + 기사 내용엔 이 사람이 증오범죄 폭행 혐의로 수사받고 있다고 나옴. 경찰의 의무가 이런 경우 적극적으로 수사하는 것임. 덧붙여 모든 증오범죄 폭행 혐의에 대해 경찰이 제대로 수사해주길 바라는 마음임
          + NYPD 안에 진짜로 이스라엘 지지자가 많음. 이스라엘과의 공식적 파트너십도 있고 현지에서 직접 트레이닝도 받고 있음. 심지어 NYPD가 텔아비브에도 사무소가 있음. 이들이 어떤 훈련을 받는지 궁금하면 여기 참고할 수 있음
     * 젊을 때 다들 한두 번은 신나서 돌 던져 본 기억 있음
"
"https://news.hada.io/topic?id=21989","애플의 브라우저 엔진 금지는 DMA 하에서도 지속됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      애플의 브라우저 엔진 금지는 DMA 하에서도 지속됨

     * 애플의 기술적·정책적 제한으로 인해 EU에서도 서드파티 브라우저 엔진 도입이 실질적으로 불가능함
     * 수익 보호를 위한 정책으로 Safari 이외 경쟁 브라우저의 성능·기능을 제한하며, 이는 웹앱 경쟁력 약화와 개발자·소비자 피해로 이어짐
     * DMA(디지털시장법) 의 명시적 금지에도 불구하고 애플은 형식적으로만 준수하며, 진정한 경쟁 촉진 목적은 달성되지 않음
     * 최대의 핵심 장벽은 새 엔진 도입 시 기존 EU 사용자를 모두 잃어야 한다는 조건으로, 이는 현실적으로 사업적 실현 불가능성을 초래함
     * 이 문제는 글로벌 규제 및 법적 압박으로 점차 주목받고 있으며, 애플이 자발적으로 변화할 가능성은 희박함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 배경

     * Open Web Advocacy는 브라우저·웹앱 경쟁 촉진을 목표로 하는 비영리 단체로, 애플·구글 등 빅테크의 자금 지원을 받지 않음
     * 애플은 iOS에서 타사 브라우저 엔진 사용을 정책적으로 금지하여, 브라우저 경쟁과 웹앱의 기능 발전에 직접적인 한계 설정
     * EU 디지털시장법(DMA) 는 2024년 3월 7일부터 타사 브라우저 엔진 금지 조항을 명시적으로 금지함
     * 그러나 애플은 대응 초기에 웹앱 지원 자체를 삭제하려 하였고, 거센 항의와 규제 당국 압력으로 해당 계획을 철회함
     * 구글(Blink), Mozilla(Gecko) 등은 독립 엔진 이식 시도를 했으나, 애플의 기술적·계약적 장벽으로 실질적 도입이 번번이 좌절됨

애플이 두는 핵심 장벽

     * EU 기존 이용자 상실: 타사 엔진을 쓰려면 신규 앱을 제출해야 해 기존 이용자를 모두 잃는 구조. 새로 시장을 시작해야 함
     * 웹개발자 테스트 차단: EU 외부 개발자는 타사 엔진의 iOS 내 테스트가 사실상 불가. 애플은 개선 예정이라 발표했으나 구체적 방안 없음
     * EU 외 장기 체류시 업데이트 중단 위협: EU 거주 사용자가 30일 넘게 EU를 떠나면 보안패치 등 업데이트 제공 불가 가능성
     * 계약 조건의 지나친 불합리: 타사 엔진 도입 조건이 과도하게 일방적이며, DMA가 요구하는 ‘엄격히 필요하고 비례적인 보안 조치’ 범위를 초과함
     * 웹앱 설치/관리 권한 제한: 브라우저가 자체 엔진으로 웹앱을 설치하고 관리할 권한 미제공

   이처럼 가장 근본적인 문제는 신규 엔진 도입 시 기존 EU 이용자를 모두 포기해야 한다는 까다로운 정책임. 이는 브라우저 엔진 포팅의 사업적 타당성을 근본적으로 없앰

왜 이 문제가 중요한가

     * 웹은 본질적으로 개방된 플랫폼으로 설계되어 폐쇄적 생태계 의존을 막고, 손쉬운 전환과 교차 플랫폼 호환성을 보장
     * 앱스토어 중심 구조는 업데이트, 결제 등 모든 흐름이 중앙 집중적으로 제어·검열·수익 분배 강제됨
     * 웹앱은 이미 데스크탑 환경 점유율 70% 이상을 차지하며, 애플조차 “브라우저 샌드박스가 네이티브 앱보다 훨씬 엄격”이라고 인정
     * 그러나 브라우저 엔진의 자유로운 경쟁이 보장되지 않으면, 애플이 전체 웹 기능의 한계를 일방적으로 결정 가능
     * 결국 DMA의 실질적 이행이 EU뿐 아니라 전 세계의 공정경쟁 및 기술 혁신에 필수적임

DMA 및 법적 의무

     * DMA 5(7)조: ‘게이트키퍼(애플)는 자사 브라우저 엔진 등의 강제 사용을 요구할 수 없음’을 명시
     * DMA 8(1), 13(4)조: 표면적 준수에 그쳐서는 안 되고, 의무의 목적을 효과적으로 달성해야 하며, 기술적·계약적 방해로 실질 준수 저해 불가
     * 그러나 15개월이 지난 현재도 애플 장벽에 의해 단 한 개의 대체 브라우저 엔진 도입 성공 사례 없음. 실질적 목적 달성 실패로 ‘비(非)준수’ 판정

애플이 변화에 저항하는 이유

     * 경쟁 웹앱, 브라우저 확산 시 자사 핵심 수익(사파리·앱스토어·Google 검색 기본설정 수수료) 심각한 타격 우려
     * Safari는 연간 약 200억 달러의 Google 검색 수익을 보장, 이는 애플 총 영업이익의 14~16% 차지
     * 1% 점유율 하락 시 2억 달러 손실로, 사파리는 애플 최고 수익률 제품
     * 앱스토어 결제, 수수료 등에서 연간 274억 달러 수익. 반면 macOS 등 다른 플랫폼에서는 이런 독점구조 미비로 수익이 미미
     * 웹앱 점유율만 20%만 이동해도 연 55억 달러 감소 추정. 즉, 제대로 된 경쟁 허용은 애플에 수십억 달러의 손실 유발
     * 이런 상황에서 규제 이행 없이 자발적 변화를 기대하긴 실질적으로 불가능함

글로벌 규제 상황과 ‘Apple vs The World’

     * 이미 영국, 일본, 미국, 호주 등 규제 추진 또는 법 제정. 영국 DMCC, 일본 스마트폰법에서는 명시적으로 브라우저 엔진 금지 금지
     * 미국 법무부도 반독점 소송에서 앱스토어·웹브라우저 정책을 문제로 직접 언급
     * 현실적으로 애플만이 전 세계 주요 플랫폼 중 유일하게 이런 수준의 엔진 금지를 집요하게 시행
     * 애플 이후로, Google·Mozilla·Microsoft 등 미국 기업들조차 이 정책 완화를 위해 노력 중. 경쟁 저해는 오직 애플만의 이익 수호
     * EU 규제 강제를 이끌어낼 경우 전 세계 표준이 될 가능성 크며, 각국이 비정상적 경쟁 제한을 묵과하기 더 어려워짐

DMA 워크숍 현장 및 애플의 입장

     * Open Web Advocacy 등 현장 질의: DMA 시행 15개월 지났지만 별도의 앱 제출, 계약상 제약, EU 이용자 상실 등으로 현실상 도입은 불가능한 상태 재확인
     * 애플 측(법무 부사장): “타사도 엔진 도입할 수 있으나, 스스로 선택하지 않은 것”이라 주장. 그러나 실제로는 애플의 기술적/정책적 장벽이 사업적으로 실현 불가하도록 만듦
     * 애플, EU만 지역적 준수, ‘글로벌 확장 의무 없음’ 강조. 실제로는 일부 EU 요구 준수 내용을 전 세계에 반영한 전례 있음
     * 유럽연합 집행위 담당자는 “브라우저 관련 모든 질문이 DMA 세션에서 논의될 수 있다”는 공식 입장 밝혀, DMA 범위임을 재확인

결론 및 전망

     * 애플의 일방적 브라우저 엔진 제한은 전 세계적으로 규제·비판의 중심에 있음
     * 실질적 변화 유도는 규제 외에는 방법 없다는 점 확인
     * 웹의 진정한 경쟁력 확보와 시장 혁신을 위해 DMA 등 강제적 정책 집행 필수
     * 애플의 변화 여부는 글로벌 IT·스타트업 생태계에 중대한 전환점으로 작용할 수 있음

        Hacker News 의견

     * 구글 앱들(예: Maps)에서 iOS 사용자가 외부 링크를 클릭하면, Chrome이나 Google 앱, Safari 중에 선택하라고 하는 강한 유도가 있음. Chrome이나 Google 앱이 기기에 없더라도 앱스토어로 안내되고 웹페이지를 바로 열어주지 않음. Safari를 선택해도 실제로는 Safari 앱이 아니라 Google Maps 내의 웹뷰가 열리고, 다시 한 번 버튼을 눌러서야 실제 Safari 탭이 열림. ""다음에도 이 선택을 기억하기"" 옵션이 있어도 자주 초기화되어 끊임없이 다시 물어봄. 인스타그램 등 다른 앱에서 열어야 하는 링크조차 Chrome 설치를 요구하며, 그렇지 않으면 클릭이 여러 번 늘어나는 불편함이 있음
          + Apple 역시 Apple Maps 사용을 강제로 유도하는 비슷한 불편함이 있음. iMessage에서 주소를 받으면 클릭이나 롱프레스를 해도 무조건 Apple Maps가 열리고, Google Maps로 공유하는 옵션도 보이지 않음. Google Maps를 기본 설정해도 iMessage에는 적용되지 않음. 주소를 복사해서 Google Maps에 직접 붙여넣기 해야 해서, 원하는 지도 앱으로 바로 열리고 싶다는 생각을 하게 됨
          + 사용자인 입장에서 Apple이 이런 사용자 불친화적인 행동을 허용하는 이유를 모르겠음. 대체 앱이 많은데도 이렇다는 점이 의아함. iOS 기본 공유 시트도 있고, (EU에선) 기본 브라우저 지정도 가능한데 말임
          + 자체적인 공유 메뉴를 넣어 두고 네이티브 공유 메뉴까지 한 번 더 눌러야 하는 것도 매우 불편함. Amazon도 마찬가지로 이런 방식인데, 사용자 선택 추적을 위해 이렇게 구현한 것 같음
          + Safari에서 검색하려고 상단 바에 입력하면 구글 검색 결과가 뜨고, 구글이 'Google Search 앱을 사용할래?'라는 팝업을 띄움. '계속하기'(파란색 강조)와 '웹에 남기'(회색 표시)가 있어서 실수로 계속하기를 누르면 앱스토어로 이동함. 다시 브라우저로 돌아가서 검색으로 가려 해도 또 앱스토어로 보내고, 두 번 정도 뒤로가기를 누르면 아예 처음 위치로 돌아옴. 구글의 다크 패턴이 정말 짜증남
          + 애초에 Apple을 주제로 한 글에서 댓글을 Google 이야기로 이끌며 모두를 산만하게 하는 화술이 인상적임. Google에게 호감은 없지만, Apple 비판글에서 Google이 탑 댓글로 나올 줄은 예상하지 못했음
     * Apple의 여러 제약을 뚫더라도, 브라우저 개발사에게 EU 환경은 쉬운 무대가 아님. CRA 법으로 인해 브라우저가 1등급 중요 제품이 되어, 개발 문서, 설계 문서, 사용자 문서, 보안 적합성 테스트, 지원 기간 공지, 소프트웨어 BOM 등 모든 문서를 준비하고, 규제 기관이 요구하면 내부 문서 공개 의무도 생김. 만약 EU가 2027년까지 통일 개발 표준을 내놓지 않으면, 제3자가 브라우저의 설계·보안을 분석해 보고서를 만들어 제출해야 하고, 이 결과로 규제기관이 적합성을 판정함. 구글·애플 같은 대기업 말고 누가 이 모든 부담을 감수하고 EU에서 브라우저를 만들고 싶겠는지 의문임. 전체 법은 여기에서 확인 가능함, 혹시 잘못 해석한 부분 있으면 지적 바람
          + 소프트웨어 업계에서는 이런 복잡한 행정절차가 익숙하지 않을지 몰라도, 다리나 비행기를 만들 때 수많은 서류 작업이 요구되는 걸 생각하면 이해가 쉬움. 브라우저는 사실상 거대한 소프트웨어 플랫폼이 되었고, 다양한 프로그램이 그 위에서 돌아가니 적절한 법적 요건이 생기는 건 전혀 놀랍지 않음. 다양한 소프트웨어 분야에서 이미 법적 규제가 있었으나, 당사자가 아니라면 잘 모를 뿐임
          + 벌칙을 살펴보니 정말 강경함. 주요 요건 위반 시 최대 1,500만 유로 또는 전세계 매출의 2.5%, 기타 의무 위반은 1,000만 유로 또는 2%, 문서 누락·허위는 500만 유로 또는 1%까지 벌금 가능. 기준 마련과 시장 안전에 중요한 법이긴 한데, 소규모 팀에게는 적용이 불가능해 보이기도 함
          + 이 법이 오픈소스 브라우저(FOSS)에도 적용되는지 궁금함
          + “대기업 외에는 누가 EU에서 브라우저를 만들고 싶겠는가”란 질문 자체가 핵심임. 시장을 몇몇 소수 대기업이 독점하는 결과를 초래하고 있음. 댓글들을 보면 이런 상황에 대한 변명만 가득함
          + 늘 그렇듯 너무 과장된 공포라는 생각이 듦. 스타트업들이 규제에 눌려 사라질 정도는 아님. 실제로 법 내용에는 영세기업과 중소기업, 스타트업을 위한 행정서류 간소화 방안이 따로 명확히 명시되어 있음. 기술문서를 쉽게 제출할 수 있는 양식 제공, 적합성 시험료 인하, 스타트업 특화 규제 샌드박스, 벌금 적용 완화 등 세부적으로 보호 조치가 많음. 또한 오픈소스 소프트웨어 책임자에게는 이 규정 위반 시 금전적 벌금이 부과되지 않음
     * EU 밖 웹 개발자를 위한 언급에 동의함. 미국에서 ""firefox for iOS""로 웹앱을 테스트하려면 비행기표를 끊고 EU 심카드를 사야 하므로, EU 전용 브라우저 엔진은 항상 2등 시민일 수밖에 없음. 진정한 브라우저 엔진 경쟁이 EU에서 이루어지려면, Apple이 EU 내외 구분 없이 설치 제한을 풀도록 의무화하는 게 맞다고 봄. Mozilla 역시 사용자를 충분히 확보할 수 없다면 주요 리소스를 할애하지 않을 수밖에 없음
          + 완전 헛소리임. Apple 하드웨어 없이 Safari로 웹사이트를 테스트할 수 없으니, 그냥 테스트 안 함
          + 미국에서 테스트 어려운 점을 지적했지만, VM이 유럽에 있으면 됨. EC2의 임시 인스턴스를 활용하면 필요할 때만 비용이 들고, 고작 몇 센트면 충분함. 마음만 먹으면 못할 게 없음
          + TestFlight 1만 명 한정 테스트론 충분하지 않음. 웹 개발자 수백만 명에게 테스트가 필요하니 훨씬 더 넓은 접근성이 필요함
     * 시장이 오직 하나의 엔진(예: Chromium)으로 통일되는 것엔 절대 동의해서는 안 됨. 안타깝게도 이런 유인이 부족하고, Firefox도 언제든 재정 문제로 사라질 위험이 존재함. 예전에 오페라, IE 등 다양한 엔진이 있었지만 지금은 거의 남지 않았음. 현실적으로 MS Edge, Chrome, Vivaldi를 비롯한 거의 모든 브라우저가 Chromium 기반이고, Firefox는 시장에서 변두리에 불과함. EU의 이번 규제로 결국 Google이 시장을 모두 차지하도록 허용하는 결과가 나타날까봐 두려움. iOS가 다른 엔진 허용을 시작하면, 오히려 단일 브라우저 엔진 시대가 도래해버릴까 걱정임
          + Firefox가 재정난으로 사라질 거라는데, 구글이 지난 10년간 Mozilla에 무려 38억 달러를 지원했음을 기억해야 함. 출처 이 정도 금액이면 본연의 미션에 집중하고 엉뚱한 사업에 돈만 안 쏟아부었다면 엄청난 효과를 냈을 것. Mitchell Baker는 여전히 잘 먹고 잘 살고 있음
          + Firefox가 사라질 확률은 낮다고 봄. 주요 브라우저들엔 끝없이 오픈소스 포크가 존재하고, Mozilla가 갑자기 무너져도 커뮤니티가 유지할 것임. 진짜 위협은 1) Mozilla 임원진이 구글에 사로잡혀 버리는 것, 2) Mozilla 붕괴 후 구글이 웹 표준을 Firefox가 따라가기 어렵게 바꿔버리는 것, 3) 인터넷 사용 자체가 AI 상호작용 등 새로운 패러다임으로 전환되는 것임
          + EU 규제가 구글 독점을 허용하게 될 거라는 주장은 맞음. 결국 양쪽 모두 손해만 보는 상황임
     * 이 규제를 EU에만 제한해서 강제로 옵션을 제공하는 Apple의 태도를 보면, Apple이 진심으로 경쟁을 의도하는 게 아님이 드러남. 법적으로 어쩔 수 없이 최소한만 지키는 것임. 정말로 보안 때문이었다면, EU에만 이런 제한을 둘 이유가 없음. 실제로는 Apple이 직접 설정한 조건을 충족한 타사 엔진만 별도 앱으로 공개하도록 강요하고 있음
          + 이런식으로 EU 내에서만 엔진 허용한다고 해서, “법을 제대로 지키지 않는 것”이라 비판하는 것은 맞지 않음. 이 법은 EU 내에만 적용되고 있으니, 다른 지역에 확대하지 않더라도 법적으론 문제가 없음. Apple이 전 세계적으로 허용했으면 더 나았겠지만, 오히려 Chrome 독점이 심화될 가능성도 있기에 복잡함
          + 오히려 반대로, 보안이 중요하다면 Apple도 어쩔 수 없이 EU에서만 시행하는 게 당연함. 법적 강제 없으면 자기들 플랫폼의 안전을 굳이 포기할 이유가 없음
          + 법적으로 요청받은 EU 내에서만 타사 엔진을 허용하는 게 당연함. Google 쪽엔 EU 개발자도 많기에 실질적으로도 큰 문제는 없을 것임
          + “Apple이 진지하지 않다”는 평에 대해, 그게 법의 원리임을 강조하고 싶음. 법이 바뀌면 Apple도 바뀔 수밖에 없고, EU만에서 적용되고 있다면 그 의도대로 작동하는 것임
     * “Safari는 Apple 역사상 가장 높은 마진의 제품이며, 연간 영업이익의 14~16%를 책임진다”는 주장이 무슨 의미인지 궁금함. Safari는 OS와 통합된 앱인데, 수익성을 어떻게 측정하는지 궁금함. 구글과의 검색 엔진 제휴 얘기일지?
          + 실제론 “Google Search Deal”을 의미함. Google이 애플 기기에서 기본 검색 엔진이 되는 대가로 광고 수익의 36%, 대략 연 200억 달러를 애플에 지급함. 최근 Google 반독점 재판에서 드러난 내용으로, 이 제휴는 불법으로 취급됨
          + Safari가 기본 브라우저이면서 광고 차단도 잘 지원하지 않아, 지난 5년간 써 본 모든 플랫폼 중 최악의 브라우징 경험임
          + Safari는 소규모 팀이 운영하면서 Google의 돈을 그대로 받음
     * 현재 Apple의 정책은 Chrome이 독점이 되는 걸 유일하게 막는 방어선이고, 이걸 무턱대고 없애는 건 신중해야 함
          + Google은 웹을 통한 모든 작업이 가능하도록 만들 인센티브가 있음. Safari는 앱스토어 수익을 지키려 하기에 iOS에서 PWA(프로그레시브 웹앱)는 완전히 무용지물임. Google도 좋지 않은 동기(광고, Android)를 가지고 있으나 Safari는 현대 웹의 IE6이라는 악평을 받기에 변화를 기대함
          + 독점이 불법인 이유는 소비자 선택권과 시장 경쟁을 제한해 왜곡된 인센티브를 만들기 때문임. 현재 상황도 사실상 같은 문제를 가지고 있고, 바꾼다고 별반 달라질 게 없지만, 소비자 선택의 장벽이 또 하나 사라지는 효과는 있음
          + 이런 현실이 안타깝지만, 그렇다고 현 상태를 당연하게 받아들여선 안 됨. Google의 Chrome 반경쟁 행위도 함꼐 규제되길 바람
          + 데이터로 뒷받침할 근거는 부족함. MacOS에서는 오래전부터 브라우저 엔진 선택지가 있지만 Safari 점유율은 여전히 50%를 넘음. 디폴트 효과가 강하고, 많은 사용자가 자체 브랜드의 장점에 만족함. iOS에선 Safari 점유율이 90%가 넘음. 엔진 경쟁을 허용해도 초기에 점유율 일부만 내려갈 것이고, Apple도 부족함을 빠르게 따라잡을 것임. WebKit의 점유율이 글로벌로 충분히 유지되는 한 “크로미움 일변도”는 오기 어려움. 엔진 선택권의 핵심은 Apple을 더 잘하도록 자극하는 진정한 경쟁 유도임
          + 논리는 이해하지만 ‘목적이 수단을 정당화한다’식 접근이라 경계하게 됨. 때때로 목적을 위해 수단을 정당화할 수도 있다고 생각하나, 그게 실제로 정당화되는 상황인지 항상 신중히 판단해야 한다고 생각함. 대형 테크기업이 사용자 경험을 이렇게까지 장악하는 것엔 다양한 부작용이 있음. Chrome 보급이 느려지는 건 장점이지만, Apple(및 다른 기업) 정책을 허용했을 때 나올 많은 단점도 분명 존재함
     * iOS 전용 브라우저 엔진을 만들어야 하는 이유가 잘 와닿지 않음. 굳이 생각나는 건 Shortcuts와 WebExtensions밖에 없음. 지금 Orion이 확장 기능 지원에 도전 중이지만 아직 완전히 쓸만하진 않고, 향후 제대로 구현된다 하더라도 Shortcuts는 JS 주입, 또는 “Safari” 웹페이지에서만 페이지 내용 얻기가 가능함(결국은 모든 웹뷰가 Safari 페이지임). Chrome 확장 기능은 분명 가치가 있기에, Apple이 강제로 개방해야 한다는 소문 이후 Google이 iOS 포팅에 매진하고 있으나, 정확히 어떤 사용성 향상이 있는지 사실 잘 모르겠음. 결국 iOS에 진입 가능한 주요 브라우저는 Google(언젠간 올 것), Mozilla(예산 타격 및 경영 비효율), GNOME Web(진입 희박), Ladybug Browser(의욕은 넘치나 실질 영향으로 이어지기까지 오랜 시간 소요 예상) 정도임. 그렇다면 이런 노력이 정말 의미 있을지 의문임
          + 브라우저 엔진이 웹 앱과 사이트의 기능을 결정함. 지원하지 않는 API나 버그가 있을 때 개발자와 사용자 경험 모두에 악영향을 줌. Apple의 WebKit은 필수 기능의 미지원, 버그 방치 등으로 유명하며, 이로 인해 웹앱이 네이티브앱과 경쟁할 수 없는 상황임. 타사 엔진 진입은 개발자, 기업, 최종 사용자 모두에게 이득이고 모바일 웹앱 활성화에 꼭 필요함
          + Chrome이 브라우저 시장을 장악한다면, 미래의 데스크톱은 영화 ‘Blade Runner’처럼 온갖 광고로 도배될까 두려움
     * Open Web Advocacy의 지속 활동에 관한 감사의 말도 있었음
          + 오픈웹을 위한 실질적 개선을 위해 누군가는 이렇게 Apple을 압박해야 함
          + 이렇게까지 올 수 있던 건 순전히 웹의 더 나은 미래를 위해 자발적으로 노력해 준 자원봉사자들 덕분임. 4년이 걸렸고, 이 메시지를 꼭 전파할 예정임
          + 오픈웹 건강성 유지는 ‘브라우저 선택’ 자체보다 ‘브라우저 다양성’ 확보에 달려 있음. 전자는 Google이 Chrome에 무엇을 넣든지 따라가면 된다는 개념에 불과함. 브라우저 다양성이 사라지면, 웹은 Chrome Protocol로 전락하고 “브라우저 선택권”도 무의미해짐
     * Apple의 악의적 법 준수(최소한만 지키며 본질은 변화시키지 않는 행위)가 도를 넘음. 애플이 실질적으로 타격을 입을 정도의 벌금이 필요함
"
"https://news.hada.io/topic?id=22071","OpenAI, 2025 국제수학올림피아드(IMO)에서 금메달급 성과 달성 발표","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              OpenAI, 2025 국제수학올림피아드(IMO)에서 금메달급 성과 달성 발표

     * OpenAI가 개발한 실험용 추론 LLM이 2025년 국제수학올림피아드(IMO)에서 금메달 수준의 성적을 기록함
     * 공식 IMO 규정과 동일하게 문제 풀이 및 자연어 증명 작성, 인간 채점자 3인의 만장일치 채점으로 42점 만점 중 35점(6문제 중 5문제 해결) 획득
     * IMO 문제는 고난도 창의적 사고·다단계 증명 요구, LLM이 기존 RL 방식 한계를 넘어 인간 수준의 논리적 증명 생성 가능성 입증
     * 특정 과제 중심이 아닌 범용 강화학습 및 테스트 타임 연산 확장으로 달성한 점이 큰 의미를 가짐
     * 모델은 곧 출시될 GPT-5와는 별개인 연구용 버전이며, 수학 최상위 성능 공개는 수개월 후 예정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenAI LLM의 IMO 2025 성과 개요

     * OpenAI의 Alexander Wei (@alexwei_)는 최신 실험용 추론 언어 모델이 2025 IMO에서 금메달 기준 성적을 기록했다고 발표함
          + IMO는 전 세계에서 수학적으로 가장 뛰어난 청소년들이 참가하는 고난도 대회로, 복잡한 논리적 추론과 깊은 개념적 이해력이 요구되는 문제로 유명함
     * 평가 방식은 인간 참가자와 동일하게 2회 4.5시간 시험, 공식 문제지 사용, 외부 도구 미사용, 자연어 증명 제출로 진행
     * 각 문제는 3명의 전직 IMO 메달리스트가 독립 채점 후 만장일치 합의로 점수 확정

성과의 의의와 진화된 난이도

     * IMO 문제는 기존 벤치마크(GSM8K, MATH, AIME)보다 훨씬 긴 사고 시간·창의성·복잡한 논증을 요구
     * 이번 모델은 5문제(P1~P5) 완전 해결, P6은 미제출로 35/42점 획득, 실제 IMO 금메달 기준을 충족
     * 수 페이지에 달하는 논리적 증명 생성 능력은 기존 강화학습(RL) 한계를 넘어섬

연구 접근 및 AI 발전의 맥락

     * 특정 문제풀이만을 위한 모델이 아닌, 범용 RL 및 계산 확장 기반으로 고성능 달성
     * 기존 RL이 제공하는 명확한 보상체계 없이 복잡한 창의적 산출물 생성에 성공
     * 실험용 모델로 곧 출시될 GPT-5와 별개이며, 이 수준의 수학 기능은 수개월 내 일반에 공개하지 않을 예정

향후 전망 및 커뮤니티 언급

     * AI 수학능력의 진보 속도가 예상치를 크게 앞지름(2021년 기준 MATH 벤치마크 30% 예측 대비 IMO 금메달 달성)
     * Alexander는 2025 IMO 참가자 모두에게 축하 인사를 전하며, 팀 내 과거 IMO 참가자들이 많다는 점도 강조
     * 모델의 2025 IMO 문제 풀이도 공개 예정이나, 실험적 스타일임

        Hacker News 의견

     * Noam Brown: 최첨단 연구소에서 일하면 보통 몇 달 앞서 새로운 능력을 미리 보는 경험을 하게 됨, 그런데 이번 결과는 아주 최근에 개발된 기법을 사용한 정말 새로운 성과였음, OpenAI 내부 연구원에게도 놀라운 일이었음, 오늘에야 누구나 어디까지가 최첨단인지 확인할 수 있게 됨
       또한, 이번 성과는 소수 팀이 주도했는데 Alex Wei가 별로 믿는 사람도 적었던 연구 아이디어를 실제 성과로 만들어 냄, OpenAI와 AI 커뮤니티의 오랜 연구와 엔지니어링도 큰 역할을 했음
       링크: https://x.com/polynoamial/status/1946478258968531288
          + 그 신기술이란 게 설마 테스트 데이터로 훈련한 건 아니길 바람 /농담임
     * 흥미롭게도, IMO 풀이들이 상당히 제한된 어휘를 사용하는 게 인상적임
       링크: https://github.com/aw31/openai-imo-2025-proofs/blob/main/problem_1.txt “적은 단어가 더 효과적일 때 굳이 말을 길게 할 필요 없음”
       그리고 주목할 점은 Alex Wei 본인도 IOI 금메달리스트라는 사실임
          + 한편으로, 실제 참가자가 푸는 중 남기는 노트와 비슷해 보인다는 점이 재미있음, 불필요한 말을 줄이면 정보의 잡음이 줄어 집중력에 더 도움임, 특히 LLM이 한 번에 한 토큰씩 생성하고 맥락 길이 제한이 있는 구조라, 의미 있는 토큰만 쓴다면 더 긴 일관성 있는 사고 흐름으로 이어질 수도 있을지 궁금함
          + IOI(정보올림피아드)에서 금메달을 딴 사람인데, 여기 논의는 IMO(수학올림피아드)에 관한 것이라는 점이 재밌음
          + Terence Tao 역시 최근 팟캐스트에서 올해 LLM이 금메달 딸 거라고 예측했음
          + 트랜스포머에서는 어떤 의미를 담았든 각 토큰 생성에 똑같은 시간이 걸림, 텍스트에서 반복적이거나 불필요한 부분을 잘라내면 속도가 대폭 빨라짐
          + “see the world”라 했을 때 “세상을 보라”인지 아니면 “seaworld(씨월드)” 같은 발음장난인지 물어보고 싶음
     * 이게 고등학생 수준이라 얕잡아 보는 사람들은 IMO 문제를 한 번 풀어보길 추천함, 올해 문제도 포함해 모두 공개되어 있음
       링크: https://www.imo-official.org/problems.aspx
       나는 머리가 어지러움
          + 관련해서, 이런 문제를 어떻게 실제로 고민하고 해결해 나가는지 보여주는 영상들이 있음
               o 3Blue1Brown 채널의 2011 IMO Q2 난제 풀이: https://www.youtube.com/watch?v=M64HUIJFTZM
               o 비슷한 난이도의 Putnam 문제 풀이 영상: https://www.youtube.com/watch?v=OkmNXy7er84
               o Fields 메달리스트이자 IMO 만점자인 Timothy Gowers가 올해 IMO 문제 실시간 풀이
                    # Q1: https://www.youtube.com/watch?v=1G1nySyVs2w
                    # Q4: https://www.youtube.com/watch?v=O-vp4zGzwIs
          + 이런 문제 풀이 유튜브 영상 보는 걸 좋아함, 표면적으로는 간단해 보여도 속임수 같음
            예를 들어 x+y=1, xy=1 문제 같은 걸 봤는데 막상 풀이는 우리가 아는 기본 대수학(인수분해, 근의 공식 등) 방식만 사용하는데도 설명마저 아름다움
            오래 생각하면 답을 찾을 수 있을 것처럼 느끼게 되지만 내 경험상 전혀 그렇지 않음
            링크: https://www.youtube.com/watch?v=csS4BjQuhCc
          + 이런 IMO 문제가 leetcode의 hard 난이도 문제와 비교하면 어떤지 궁금함
          + IMO 문제에 여러 언어 버전이 있다는 건 이번에야 알았음
            50개 언어쯤 되는 것 같은데, 그만큼 많으면 문제 유출 등 보안 유지가 훨씬 어려워질 수 있다는 점이 떠오름
     * 이런 문제들이 고교생 수준이라는 건 배경지식 기준일 뿐이고, 매우 어려운 편임
       IMO 출신이 아닌 전문 수학자들도 이런 퍼포먼스를 내기 힘듦
       이게 AI가 수학적으로 인간보다 뛰어나다는 의미는 아니고, 수학자들은 수학의 프런티어를 확장하는 데 초점을 둠
       정답이 훈련 데이터에 들어 있는 게 아니라고 함
       그리고 이 모델은 IMO 문제에만 특화된 모델이 아니라고 주장함
          + 내 기억으론 데이터 과학을 할 때, 검증 세트 누출을 막는 게 생각보다 무척 어렵다는 사실이 떠오름
            훈련 과정을 계속 튜닝하고, 검증 세트 성능이 좋아지면 그에 맞춰 다시 구조와 데이터를 고름
            뭔가 의도하지 않아도 검증 세트 정보가 모델에 조금씩 스며듦
            검증 세트만 달리 골라도 완전히 다른 모델이 만들어짐
          + 정말 IMO 특화 모델이 맞는지 의심됨, 트위터 스레드에서는 “일반 추론”이라고 하던데, 정말 올림피아드 수학 문제로 RL(강화학습)하지 않았다면 OpenAI 측 공식 언급을 꼭 들어보고 싶음
          + “IMO에 특화된 모델이 아니다”라는 주장, 근거가 무엇인지 궁금함
          + “정답이 훈련 데이터에 없다”, “IMO 특화 모델이 아니다”에 어떤 근거나 증거가 있는지 궁금함
          + 아무리 봐도 IMO에 특화된 모델임이 거의 확실해 보임
            문제 답변하는 방식도 딱 그렇게 느껴짐
            예: https://xcancel.com/alexwei_/status/1946477742855532918
            실제 답변 스크린샷: https://pbs.twimg.com/media/GwLtrPeWIAUMDYI.png?name=orig
            AlphaProof 스타일로 자연어와 Lean 같은 시스템을 왔다갔다 하는 것처럼 보임
            OpenAI는 이런 세부 구현을 공유하지 않을 것임
     * 스레드에서: “모델이 P1~P5는 풀었고, P6에는 답변을 못 냄”
       가장 어려운 문제(P6)는 인간도 거의 못 푸는 문제였고, 중국 대표팀조차 42점 만점에 21점만 득점했고, 다른 대부분 국가에선 아무도 못 풀었음
          + IMO에서는 첫째 날 P1,P2,P3, 둘째 날 P4,P5,P6를 보게 됨
            보통 난이도 순서는 P1, P4, P2, P5, P3, P6로 의도되고, P1이 가장 쉽고 P6가 가장 어려움
            현실에서는 이 순서와 다를 때도 있음
          + 캐나다팀 중 누군가가 P6를 풀긴 한 것 같지만, 전체적으로는 극히 일부임
          + 기계가 인간과 똑같이 어렵게 느끼는 문제(특히 P6)에서 못 푼다는 점은 인간이 개입했을 수도 있다는 힌트 같음
            단순히 기계적 우연이라 치더라도, 잘못된 답변도 출력할 수 있었을 텐데 오직 올바른 답변만 고른 건 아닌지, 즉 성공한 결과만 골라낸 건 아닌지 의문임
     * Google도 이번 IMO에 참여해서 금상을 받았음
       링크: https://x.com/natolambert/status/1946569475396120653
       OAI가 먼저 발표했으니 곧 구글도 공식 발표할 것 같음
          + Noam Brown의 “OpenAI 내부 연구자도 놀란 성과였다”는 언급을 보면, 여러 연구소가 동시에 이런 결과를 얻었다면 더더욱 놀랄 만함
            트위터에서 구글은 Lean을 쓴 반면, OpenAI는 도구 없이 LLM만 사용했다는 말이 있었음
            어떤 방식이든 결과 자체가 더 중요하긴 하지만, 구체적 기법의 한계와 발전 과정도 흥미로운 참고사항임
          + Google의 AlphaProof는 작년 은상을 땄고 뉴럴+심볼릭(기호적) 접근을 썼음
            OpenAI의 금상은 순수 LLM만으로 가능했다는 점이 특이함
            구글이 공식 발표하면 어떤 접근법을 썼는지 알 수 있을 것임
            LLM 방식의 장점은 수학적 증명뿐 아니라 다양한 추론 문제로 범용화될 가능성이 높다는 데 있음
     * Noam Brown:
       이건 IMO 특화 모델이 아니라, 새로운 실험적 범용 기법이 들어간 추론 LLM임
       o1, o3보다 사고 과정이 훨씬 효율적임, 테스트 시 효율성도 앞으로 더 밀어붙일 수 있음
       최근 AI 발전 속도가 빨랐지만, 앞으로도 계속될 것으로 기대하고 있음
       특히 AI가 과학적 발견에 본격적으로 기여할 시점에 근접했다고 봄
       나는 최근까지 발전이 느려지고 있다고 생각했지만, 여러 주장(특화 모델이 아니라는 점과 효율성 향상 가능성)에서 실질적 진보가 아주 분명함
       링크: https://x.com/polynoamial/status/1946478249187377206
          + “시험 문제를 푸는 모델”에서 “과학적 발견에 기여하는 AI”까지의 도약에는 상당한 차이점이 있다고 생각함
          + 꿈 같은 이야기이지만, 변호사 시험 등 특정 시험에 맞춰 미세 조정한 것처럼, 이런 모델들도 대개 이전 버전의 시험 문제에 이미 훈련된 경우가 많음
          + 파인튜닝 과정에서 도구 사용(자동 증명 툴 등)이 함께 쓰였는지 궁금함
          + “o1, o3보다 사고가 더 효율적임”이라는 부분,
            “상대가 (고정된) 응답 방식을 취하면 절대 지지 않음. 그녀가 이기려면(상대가 지게 하려면) Q_{even-1}>even, 즉 어떤 a_j> sqrt2가 되어야 하는데, 이미 a_j<=c< sqrt2임. 그래서 절대 질 수 없음” 등등
            적은 말로 효율을 극대화하려는 태도가 보임
            링크: https://github.com/aw31/openai-imo-2025-proofs/blob/main/problem_5.txt
          + 이제는 데이터가 부족한 “피크 데이터”에 도달한 시점에서, 효율 향상의 뚜렷한 발전 경로가 무엇인지 궁금해짐
     * 정말 인상적인 성과인데, 어떻게 해낸 건지 궁금함
       Wei가 추론한 “테스트 시 연산 자원 확대(scaling up test-time compute)”를 보면, 엄청난 돈을 쏟아부었을 것 같음
       수천~만 번 병렬로 돌리고 최적 결과만 골랐다면 실망임
       정말 제대로 된 성과라면 어떤 도구를 썼고 어떻게 활용했는지 투명하게 밝혀야 함
       어렵게 검증되는 문제에서 성능을 높이는 여러 기법이 아마 여기에 들어갔을 것 같음
          + 병렬로 10000번 돌려도 그렇게 덜 흥미로운 건 아님
            오히려 정답의 정확성과 엄밀함을 구분해낼 수 있다는 뜻이고, 이는 사람이 드물게나마 풀어내는 것과 다르지 않음
          + 트위터 스레드에 따르면 별도의 도구는 주어지지 않았다고 함
          + 실제로 OpenAI가 수천~만 번 병렬로 돌리고 결과만 골랐을 가능성이 높다고 봄
            이게 초기 o3 ARC 벤치마크 때도 그랬던 방식임
            아마 복수의 에이전트가 협업하는 방식일 수도 있으니, 맥락 길이(토큰 수 제한)는 우회 가능했을 것임
            이제는 AI가 웬만한 수학 문제에서 이미 99.99% 인간을 넘어섰으니, 99.999%를 이긴다고 해도 딱히 놀랄 일은 아님
          + 만약 OpenAI가 10000번 돌리고 사람이 직접 결과를 골랐다면 의미가 크게 달라짐
            LLM이 스스로 검증해 가며 채택한 것이면, 사람이 어려운 문제를 여러 번 시도해 풀어내는 과정과 유사함
            차이는 AI는 연산 자원이 많아 병렬로 할 수 있다는 점, 인간은 순차적으로만 시도 가능함
     * 이 대회(IMO)가 워낙 상위권 대회라 프로그래머 커뮤니티에서도 정확히 이 대회가 뭔지 모르는 사람이 많은 것 같음
       간단히 계산해보면 미국 기준으로 캠프에 선발된 인원(금메달 가능성) 대략 20명, 전체 해당 세대 고등학생 중 2천만 명을 기준으로 하면, “백만 명 중 한 명 꼴” 탈렌트임
          + 대회 난이도가 엄청나다는 점을 폄하하려는 의도는 아님
            나도 명문고를 나왔지만 IMO란 걸 대학 가서 참가자들을 만나기 전엔 들어본 적이 없음
            실제로 대회를 인지하고 참가하는 학생 수는 전체 학생 수보다 훨씬 적음
            실력을 떠나 많은 학생이 기회와 정보만 제대로 있었다면 좋은 결과도 가능했을 것 같음
     * 최근 LLM의 IMO 2025 평가 관련 보고서를 봤는데, o3 high는 동상 등급도 못 탔음
       링크: https://matharena.ai/imo/
       Terry Tao 의견도 기대되지만, 이런 분야 발전이야말로 AI의 긍정적 활용이라고 생각함
       경제가 준비되지도 않은 상태에서 무분별한 혁신보다는, 과학 발전을 앞당기는 쪽으로 기여했으면 하는 바람임
          + 여기 Terry Tao의 반응 있음
            링크: https://mathstodon.xyz/@tao/114881419368778558
"
"https://news.hada.io/topic?id=22035","전 Waymo 엔지니어들, 건설 자동화를 위해 Bedrock Robotics 설립","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             전 Waymo 엔지니어들, 건설 자동화를 위해 Bedrock Robotics 설립

     * 전 Waymo 출신 엔지니어들이 Bedrock Robotics를 설립해 건설 현장 자동화에 주력함
     * 이 스타트업은 기존 건설 차량에 자율주행 키트 장착을 목표로 하며, 1년간 조용히 운영되다 80백만 달러의 투자 유치 소식을 발표하며 공식적으로 대외에 등장
     * 센서, 연산 장치, 그리고 지능형 알고리듬을 통해 차량이 프로젝트 목표 파악, 현장 상황 적응, 24시간 작업 수행이 가능하도록 하는 기술을 개발 중
     * 최근 건설, 광산, 산업 현장, 방위산업 등 다양한 오프로드 시장을 겨냥한 자율주행 스타트업들이 등장중
     * Pronto, Kodiak Robotics 등과 같이 비도로 환경 자율주행을 겨냥한 스타트업들이 최근 주목받는 추세
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Bedrock Robotics 개요

     * Bedrock Robotics는 Waymo 및 Segment 출신 베테랑 엔지니어들이 창업한 자율주행 기술 스타트업임
     * 지난 1년 넘게 조용히 운영되었으나, Eclipse, 8VC 등으로부터 8000만 달러 투자 유치를 계기로 공식적으로 대외에 등장함
     * 이 회사의 목표는 기존 건설 및 작업장 차량에 자율주행 키트를 손쉽게 장착할 수 있는 솔루션 개발임

기술과 리더십

     * Bedrock Robotics는 센서, 연산 장치, 그리고 지능형 알고리듬을 통해 차량이 프로젝트 목표 파악, 현장 상황 적응, 24시간 작업 수행이 가능하도록 하는 기술을 개발 중임
     * Boris Sofman CEO는 이전에 Waymo 무인 트럭 사업부와 Anki Robotics(Cozmo 로봇 개발)의 공동 창업자로 유명함
     * 공동 창업진에는 Waymo 출신의 Kevin Peterson(CTO), Ajay Gummalla(엔지니어링 VP), 그리고 Segment와 Twilio 경험을 가진 Tom Eliaz(엔지니어링 VP)가 참여 중임

오프로드 자율주행 트렌드

     * 최근 로보틱스, 자율주행, AI 분야의 인재들이 비도로 환경에 혁신을 도입하기 위해 관련 스타트업 창업에 나서는 추세임
     * 건설, 광산, 산업 현장, 방위산업 등 다양한 오프로드 시장을 겨냥한 자율주행 스타트업들이 꾸준히 등장하고 있음
     * 예를 들어 Pronto는 건설 및 광산용 운반 트럭의 자율주행 시스템을 개발하며, 최근 경쟁사 SafeAI를 인수함
     * 이외에도 Kodiak Robotics, Polymath Robotics, Overland AI, Potential, Forterra 등 다양한 회사들이 존재함

주요 시험 운행 및 파트너십

     * Bedrock Robotics는 건설 현장에 초점을 맞추고 있으며, 아칸소, 애리조나, 텍사스, 캘리포니아에서 Sundt Construction, Zachry Construction Corporation, Champion Site Prep Inc. , Capitol Aggregates Inc. 등과 함께 기술을 실증 중임

        Hacker News 의견

     * 나는 미국에서는 물리적인 한계보다 정치적인 문제, 특히 규제와 정치적 후광 때문에 대형 프로젝트가 비싸지는 현상이 더 큰 병목임을 느끼고 있음. 실제로 인건비와 자재비보다 각종 정치적 요소, 예를 들면 환경영향 조사, 구역 설정, 노조 활용, 경찰 초과근무 채용 요구, 특정 업자만 쓰는 조건 등 각종 조건이 총비용을 키움. 이런 자동화 로봇도 제대로 된다면 오히려 불법화될 가능성이 많음
          + 실제 데이터가 있냐고 묻고 싶음. 내가 찾은 모든 자료에 따르면 미국 기준 사전 인허가 비용은 인프라 유형과 위치에 따라 3~10%라 하고 미국은 비교적 3~5%로 낮음. 건설업 이익률은 7%쯤이고, 비용 증가의 주요 원인은 숙련인력 부족임. 관련 자료로 뉴질랜드 인프라 프로젝트 인허가 비용 보고서와 글로벌 건설비용 보고 첨부함
          + 이 말에 동의하지만, 인허가를 통과해도 건설 자체의 비용이 너무 큼. 특히 (주로 노조 소속) 인건비가 크고, 실제로 Berkeley처럼 금리가 오르면서 사전에 승인된 현장조차도 텅 빈 상태임.Berkeleyside 링크 업무의 공정성과 기술 생산성을 함께 높여야 한다고 봄. 원재료 가격도 최근 급등함
          + 중동의 초대형 프로젝트에는 이런 자동화가 큰 도움이 될 것 같음. 외국인 노동자를 싼값에 데려와 형편 없는 환경에서 일하게 하고, 노조나 서류 절차도 없어서 오로지 현장 작업만 엄청나게 많이함
          + 진짜 필요한 건 MARPA, 즉 '경영 및 연구 프로세스 혁신'임. 미국이 세계 상위 15%에 들어가는 효율성과 예산으로 중형 건설을 제때 해낼 수 있는지 의문임. 이 문제를 실제로 해결하려면 경영 쪽에서 더 적극적으로 나서야함
     * 약간 다른 얘기지만, 오토모 카츠히로(‘아키라’ 감독)가 만든 대형 자동화 건설의 디스토피아적 해석인 단편 ""Construction Cancellation Order""를 추천하고 싶음. ‘Neo Tokyo’라는 옴니버스의 일부로 1987년에 공개되었고, 인터넷에 영상이 돌아다니는 걸로 알고 있음. Neo Tokyo (영화) 위키피디아 참고
     * 대형 장비 시장에서 OEM 경쟁이 큰 장벽이 될 것 같음. Caterpillar나 John Deere 같은 대기업이 이미 원격 조종 장비를 갖췄고, 이들의 장비에 추가 키트를 달면 워런티 위반 등 제약도 있음. 기존 OEM과의 협력이 중요해질 것 같고, 현존 장비에 키트 판매 방식보다는 OEM과 파트너십이 적합해 보임
          + 이런 경쟁이 단점이기보단 인수∙합병을 통한 “exit 시나리오” 측면에서는 오히려 장점임
          + 투자 유치액이 8천만 달러로 8억 달러가 아니란 점에서도 이미 쉽지 않은 시장임을 보여줌. 이런 스타트업은 오히려 필요에 따라 방향 전환이 더 용이할 수 있음
          + 실제로 기존 업체와 똑같이 경쟁하는 게 아니라, 예를 들면 작업 트럭에 싣고 다닐 정도의 소형 전동 장비 군집 운용으로 조용하게 24시간 작업한다면 중소 현장부터 실험하고, 시간이 지난 후 대형 장비나 더 나은 배터리로 전환하는 유연한 전략도 가능할 것 같음. 궁극적으로 대형 현장엔 대형 장비가 필요하니, 결국 파트너십이나 exit이 합리적임. (사실 RTS 게임 경험이 반영된 상상임)
          + 기존 강자들이 보유한 특수 기술이 핵심이 아니라면, 그들은 기존 사업에 너무 몰두하여 새로운 시장까지 신경 쓰기 어려울 수 있음. 대형사 경영진도 신기술 개발에는 나서지만, 시장을 두 분야로 동시에 집중 공략하긴 어려움
          + 이미 해당 분야가 변화할 조건이 무르익은 느낌임. 스타트업이 경쟁력 있는 결과를 내면, 벤처 투자금이 몰려옴. 결국 경제 논리—신규 방식이 더 싸면 기존 강자들은 점유율 감소가 불가피함
     * 나는 야외 중장비 건설 작업에서 인간이 장애물이 된다고 생각하지 않았음. 오히려 중장비가 이미 작업의 거의 대부분을 담당하는 줄 알았음. 이 기술의 진전이 어떻게 나아갈지 기대됨. 하지만 예전 Amazon의 공급망 혁신 팀이 크게 성과를 내지 못한 게 떠오름
          + CAT, 현대, 히타치, John Deere, Kubota, Komatsu 같은 기존 대형사들이 이미 10년 전부터 자동화 실험을 해왔음. 단순히 일부 Waymo 출신이 만든 스타트업이라고 새 시장이 열린 건 아니라고 봄
          + 숙련된 오퍼레이터를 그때그때 바로 현장에 보내기가 매우 어렵다는 현실적인 문제도 큼
          + 중장비 한 대가 이미 20~50명의 노동력을 대체했는데, 이제 그 한 명의 인력조차 큰 문제로 비중 있게 여겨지는 점이 흥미로움
     * 나는 Bedrock CTO이자 공동 창업자임. 이 분야에 많은 관심을 보여주는 것에 놀라움과 감사함을 느낌. 우리에 대해 궁금한 점이 있으면 언제든 물어도 됨. 그리고 정말 뛰어난 머신러닝 엔지니어, 소프트웨어 엔지니어를 찾고 있으니 관심 있다면 채용 페이지를 확인해주길 바람
          + 한 가지 피드백을 주자면, 채용 공고까지 스크롤을 너무 많이 해야 해서 중간에 유능한 인재도 그만둘 수 있을 것 같음. 일부러 지원 동기를 테스트하려는 의도일 수도 있지만, 불편함 때문이라면 아쉬울 수 있음
          + 기존 인력 자리에 투입되는 휴머노이드 로봇 개발까지도 고려 중인지 궁금함
          + 미국에 널린 흉물 주택 외벽에 쓸 벽돌을 CNC로 커팅하는 로봇도 만들어줬으면 함
          + 채용 공고를 보니 소프트웨어와 하드웨어 인력이 대부분인 것 같음. 구조공학이나 건축 엔지니어도 추가로 채용해야 한다고 생각함
     * 나는 건축가이자 현재는 건축대 교수로, 컴퓨테이셔널 디자인과 첨단 제작 기술을 개발하고 있음. 현장 자동화를 절실히 원하지만, 타 업계 사람들이 생각하는 것보다 훨씬 어려운 일임. 길을 만들면서 동시에 직접 운전하며 다른 인력들도 그 안에서 무언가를 하고 있다고 상상해 보면 그 난이도가 이해될 것임. 현 단계에서는 Bedrock이 굴착에 집중하는 것 같고, 이 분야는 이미 자동화가 많이 진척된 상태임. 그러나 지하에서 벗어나면 금방 많은 난관에 부딪히게 됨. 이에 대해 참고할만한 글로 로봇 벽돌 쌓기 기사의 고찰을 추천함
          + 건설은 “스케줄링”이 가장 중요한데, 실제 스케줄링 자체가 job shop scheduling 문제와 같아서 NP Hard임. 실제론, 각 상황에 맞춘 맞춤형 최적화가 가능하지만, 사업 관계가 일정 최적화의 핵심임. 예를 들어 밤샘 작업을 해도, 플러머가 아침에 안 오면 아무 소용 없음. 플러머가 오는 진짜 이유는 이 현장과 긴밀한 사업적 관계, 즉 네트워크 덕분임. (전직 건축가로서, 결국 모든 프로젝트는 시간이 필요한 만큼 걸릴 수밖에 없었음)
     * 이 비즈니스는 상당히 기묘할 것으로 느껴짐. 초대형 토목은 이미 인간이 도로와 다리를 놓아 로봇이나 초대형 운송장치를 장비화하고 있지만, 각 프로젝트가 너무 개별적이고 커스터마이징이 많아서 쉽게 자동화될 것 같지는 않음. 단, 신도시/신규 주택단지 생산처럼 반복성이 높은 분야는 완전히 새로운 판이 열릴 수도 있음. 참고로 Self-propelled modular transporter(모듈형 자주 운송장치) 위키피디아 링크
          + 혹시 현업에서 이런 자동화 건설이 실제로 진행 중인지, 현재 상황을 아는 분이 있는지 궁금함. 대형 장비 회사들이 이미 부분적으로 하고 있는 것 같음
     * 나는 새로운 기술을 가능하다고 믿고 싶지만, '결합의 오류(Conjunction Fallacy)'라는 심리적 함정이 떠오름. 결합의 오류 위키피디아 실제로 상업적으로 성공한 '자율 불도저'를 만들려면, 우선 상업적으로 성공한 불도저를 먼저 만들어야 하는데, 이게 바로 하드웨어의 벽임. 자율주행보다 하드웨어가 훨씬 더 어렵고 중요한데, 이미 100년 넘게 시험된 분야라 단순하지 않음. 솔직히 말하면 이 사업은 공학이라기보다 금융 엔지니어링(사업 모델화) 느낌이 듦
          + 기존 불도저의 운전석에 ""로봇""을 앉히는 게 전부면 될 수도 있지 않을까 하는 생각도 듦
          + 불도저 하드웨어 중 인간 승차와 안전 보장을 위한 비율이 얼만지 궁금함. 사람 없이 구조 최적화를 얼마나 할 수 있을지 의문임
          + Waymo도 실제 차량 자체를 생산하지 않음
     * 이 회사가 흥미로운 점은 단순히 자율주행차에서 건설로 이동한 게 아니라, 이미 안전·노동력 부족·원가 압박 등이 높은 산업일수록 자율시스템의 확산이 빠르다고 Bedrock이 판단한 부분임
     * 자동화 현장이 지역 반대가 적거나 규제가 느슨한 곳에서는 산림 개발, 노천광산 채굴 등이 더욱 빨라질 것 같음. Rinto가 투자자일지도 궁금함
          + 은행에서는 일반적으로 노천광산 사업에 투자하지 않음. 대규모 장비가 병목이고, 단순 인력이 핵심 변수가 아닌 것이 보통임
"
"https://news.hada.io/topic?id=21965","물고기 킥은 현재까지 가장 빠른 수중 수영 영법일 수 있음 (2015년)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                물고기 킥은 현재까지 가장 빠른 수중 수영 영법일 수 있음 (2015년)

     * 물고기 킥이 가장 빠른 수중 영법 중 하나로 평가됨
     * 기존 수영 영법 대비 효율성과 속도 면에서 주목받음
     * 인간 수영 기술 발전에 의미 있는 관찰 결과임
     * 선수들이 기록 단축을 위해 새로운 동작 도입 검토 중임
     * 생체 모방 연구 및 알고리듬 개발에도 영향을 주는 주제임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

물고기 킥 영법의 주요 특징

     * 물고기 킥이란, 수영 선수가 물속에서 물고기처럼 다리를 움직이며 추진력을 얻는 동작임
     * 기존의 평영, 자유형, 배영 등과 달리, 몸 전체를 S자 형태로 사용해 속도를 높이는 방식임
     * 연구 결과, 이 영법은 수중 구간에서 최고의 속도와 효율성을 보여줌

인간 수영 기술과 혁신

     * 수영 선수들이 기록 경신을 위해 지상 구간뿐 아니라 수중 영법에 집중하기 시작함
     * 물고기 킥의 특징을 더 연습하고 기술적으로 개선하며, 공식 대회에서 활용을 논의 중임

생체 모방과 기술적 활용

     * 물고기 킥의 원리는 인간의 운동뿐 아니라 로봇공학, 시뮬레이션 등 첨단 기술 개발에도 이용됨
     * 더 빠르고 효율적인 알고리듬 개발에 영감을 주는 생체 모방 연구 사례임

        Hacker News 의견

     * 이 내용은 측면에서 수행되는 dolphin kick, 즉 새롭게 “fish kick”이라고 불리는 동작에 대한 내용임, 왜 동일한 킥을 90도 회전시키면 더 빨라지는지 이해가 안 됐었는데, 실제로는 킥 동작이 주변의 물의 움직임에 의해 제약을 받기 때문임, dolphin kick의 경우 물이 위아래로 움직이고 물 표면과 바닥에 의해 제한받음, 측면으로 회전함으로써 수영선수는 이러한 제약에서 벗어나게 됨
          + 그렇다면 중간 레인에서 출발하는 사람이 이점이 있는지 궁금함
          + 구조/전투용 사이드 스트로크도 매우 효율적임, 옆으로 누워서 큰 킥을 사용할 수 있기 때문임
     * 달리기와의 비교가 인상적이었음, 우리는 거의 육상 포유류로서 자연스러운 달리기 방식을 가지고 있음, 네발로 달리는 것은 더 빠르지 않고 부자연스러운 방식임이 명확함, 하지만 수영에서는 상황이 다름, 우리는 빠른 속도를 위한 본능적 형태가 없음, 대부분의 육상 포유류가 어느 정도 수영을 한다는 것도 생각해보면 그리 놀랍지 않음, 그렇다면 수영에도 좋은 최적의 육상 포유류 체형은 무엇일까 궁금해짐, 인간의 체형이 이족보행 달리기에 최적화되어 있음에도 수영에도 적당히 잘 맞는 것 같음, 물론 인간의 최고 수영 속도는 달리기 속도에 비하면 한참 느리고, 물고기 등 다른 수생동물과 비교하면 현저히 느림, 어쨌든 우리는 수영에 특화된 존재는 아님
          + 모두가 네발로 달리는 것이 더 빠르지 않다는 데 동의하지는 않는다는 점이 흥미로움, 관련 논문 링크
          + 곰, 특히 북극곰은 뛰어난 수영 능력을 가진 육상 포유류임, 쉬지 않고 60마일까지 수영할 수 있고, 개 헤엄을 변형한 효율적인 자세를 사용함
          + 북극곰 이야기가 나왔는데, 이들을 엄밀히 육상이라고 할 순 없겠지만, 북극곰은 올림픽 선수보다 빠르게 수영함, 무스도 굉장히 빨리 헤엄치니 결국 육지에서도 물에서도 주의해야 할 동물임
          + 하마는 상당 시간을 물에서 보내지만, 실제로는 수영을 하지 못함, 너무 밀도가 높아서 뜨지 못함, 영국 BBC에서 하마가 헤엄치는 애니메이션을 방영했지만 실제로는 바닥을 껑충껑충 뛰는 형태임
          + 비버는 넓고 평평한 꼬리 덕에 뛰어난 수영선수임, 조사해보니 검은곰이 전체적으로 가장 빠른 것 같고, 체격 대비로는 비버가 더 빠를 수 있음
     * 내가 10대 때 봤던 ‘Man from Atlantis’ 수영법과 매우 비슷해 보여서 나만 나이 많은가 싶음, Man from Atlantis 소개 링크
          + 유튜브 예시 영상
     * “올림픽 금메달을 딴 Misty Hyman에게 연락했다”라는 부분이 나옴, 그녀의 이름이 항상 웃김, 오빠 이름이 Buster라고 들어서임
          + Fanny Chmelar도 들어본 적 있는지 물어보고 싶음
          + Misty가 연기로 가득 찼다는 뜻도 있음
     * 진정한 프리스타일 카테고리, 즉 15미터 제한이 없는 경기를 보고 싶음, 왜 이런 종목이 없는지 궁금함
          + 이 규칙은 관객 입장에서 수중 경기가 별로 흥미롭지 않고, 표면에서 심판이 판정하기 어렵기 때문에 만들어졌다고 생각함, 하지만 수영장에 GoPro를 여러 대 설치하면 새로운 경기 포맷을 볼 수도 있을 듯함
          + finswimming이라는 스포츠가 있는데, 이는 핀을 사용해서 수중 속도 경기를 함, 이름에서 알 수 있듯이 핀을 사용하는 종목임, finswimming 소개 링크
          + onlypassingthru가 언급하기로, 수중 경기는 시각적으로 좋지 않은 면이 있다고 함, 또 swarnie가 말하길, 올림픽 수영에는 의류 제한이 있는데, 이것도 프리스타일 취지와 맞지 않는 모순이라고 생각함, 참고1, 참고2
          + 기본적으로 버터플라이까지 허용해야 한다고 생각함, 수중 제한은 안전상 이유가 있지만, 영법 제한은 그럴 필요 없음
     * “트랙&필드에서는 절대 일어날 수 없는 일”이라는 Rick Madge의 발언이 재미있었음, Fosbury Flop처럼 실제로 육상에서도 이와 유사한 혁신이 일어났던 점이 생각남, 결국 이 동작이 경쟁 수영에서 독립된 종목으로 추가될 날이 올지 궁금함
     * 20년 전 혼자 수영을 배우려 했을 때 이와 비슷한 동작이 효과가 있을 것 같다는 직감이 있었음, 등반 루트를 준비할 때와 비슷한 근육의 사용감을 느꼈는데, 몇 번 해보고 완전히 실패한 뒤 포기했음
     * 인간이 육상 동물이란 약점을 극복하려면 finswimming이 제일 좋다고 생각함, 핀을 이용하면 내가 무핀 상태에서 가능한 거리의 20배까지 더 수영 가능함
     * 내용이 정말 흥미로움, 연도가 (2015)라고 표기하는 게 좋을 듯함
     * 심지어 죽은 물고기도 특정한 소용돌이 환경이면 거슬러 올라갈 수 있는 등, 물고기의 놀라운 효율성을 떠올리게 함, 인간의 신체가 얼마나 더 발전될 수 있을지도 궁금함, 관련 링크
          + 1996년 올림픽 배영에서 인간 한계치를 시험하는 상황이 나왔었음, 선수들이 전체 혹은 대부분을 수중에서 헤엄쳤고, 시각적으로 좋지 않은 경기였음, 그래서 FINA가 표면에서 실제 배영으로 경기하도록 규정을 바꿈
          + 어떤 점에서 개선이 가능한지 궁금함, 내가 보기에 세 가지방향이 있음, 1) 의류 - 이미 올림픽에서 금지됨, 2) 약물 - 마찬가지로 공식적으로 금지되어 있지만 Enhanced Games 등에서 시도가 이루어지고 있음, 3) 기계적으로 몸을 개조하는 방향
"
"https://news.hada.io/topic?id=22077","아직 아무도 AI로 어떻게 구축해야 할지 모름","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       아직 아무도 AI로 어떻게 구축해야 할지 모름

     * 현재는 AI 개발 방법론이 아직 확립되지 않아 모두가 실험 중인 상황임
     * AI 시대에는 전통적 전문가 개념이 의미를 잃고, 모두가 영원한 초보자임
     * 실제 개발 프로세스는 즉흥적 문서 누적과 반복적 시행착오를 통해 이루어짐
     * AI와 협업하면 짧은 집중 시간과 최소한의 입력만으로도 거대한 결과가 생성됨
     * 자신만의 문서 시스템도 일시적이고, 모두가 일회성 실험을 이어가는 과정
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

The Great Experiment Nobody's Running the Same Way

     * AI 개발에서는 누구도 정해진 방법을 모르는 상태임
     * Malcolm Gladwell의 10,000시간 이론처럼 축적된 전문성이 무력해지는 환경임
          + AI 도구의 발전 속도는 너무 빨라 숙련도를 쌓기가 어려움
     * AI 페어 프로그래밍도 경험치가 2년 이하로, 모두가 항상 초보자임

My Current Experiment (Subject to Change)

     * 작업을 시작하기 전에 참고하는 4개의 주요 문서를 두고 있음
          + pair_programming.md
          + project_plan_{some extension}.md
          + technical_considerations.md
          + mcp-browser-architecture.md
     * 문서 체계도 처음부터 설계한 게 아니라 즉흥적으로 쌓인 결과임
          + 처음에는 아키텍처 관련 문서 한 개로 시작해, 반복되는 문제와 정보전달 문제로 인해 점차 문서가 네 개로 늘어남
          + 네 개의 문서를 최적화된 결과로 정한 것이 아니라, 더는 추가하지 않아도 괜찮다고 느꼈기 때문
     * ""이 문서가 아키텍처야"" ""이 과정이 공식(Official)이야""라고 스스로에게 역할놀이하는 기분이 들기도 함
     * 실제 소프트웨어는 작동하며, 이런 일시적 체계도 결과를 만든다는 점이 핵심임
     * 각 문서의 역할은 아래와 같음
          + Architecture Overview: README에서 출발, '이 소프트웨어가 무엇을 하는 것 같은지'를 기록
          + Technical Considerations: 반복되는 좌절이나 문제들을 문서화, Claude가 헷갈릴 때마다 세부 내용을 추가
          + Workflow Process: 반복해온 절차를 문서화, 실제로는 공식 규칙이나 신성 불가침의 문서가 아니라, 이번에 우연히 효과있었던 방법의 모음
          + Story Breakdown: 15-30분 단위의 쪼갠 작업 청크. Claude가 금새 망각하기 때문에 대화 내역을 자주 리프레시하기 위함임

Time Dilation in the Age of AI - AI 시대의 시간 왜곡

     * 최근 Protocollie 개발을 하며, AI와의 협업은 기존 소프트웨어 개발의 시간감각을 완전히 뒤흔드는 경험이었음
     * Claude에게 특정 기능 작업을 지시한 뒤 그 사이에 개인 생활을 즐기고, 주기적으로 점검 및 간단한 피드백을 주는 방식으로 프로젝트가 진행됨
     * 실제 집중해서 ""일""한 시간은 하루 90분 남짓이지만, AI는 그 사이에도 빠르게 수천 줄의 코드를 생산함
     * 입력에 비해 결과물이 너무 빠르고 많아 기존의 투입-산출, 노력-성과, 시간-진전 공식이 무너짐
     * 때때로 이렇게 빠른 개발이 죄책감마저 들 정도이며, 전통적 개발 패러다임에 맞지 않는다는 혼란을 느끼고 때로는 속임수를 쓰는 듯한 기분도 듦

스파게티 실험"" 단계

   현재의 AI 개발 환경을 ""스파게티 실험"" 단계라고 표현함
     * 즉, 벽에 실험적으로 스파게티를 던지는 과정 자체가 의미 있으며, 무엇이 굳이 붙거나 남는지는 중요하지 않음. 던지는 행위 자체가 실험임
     * 각종 삽질, 실험 실패, 우연히 동작한 절차 등은 집단적 실험의 데이터 포인트 역할을 함
     * 본인이 사용한 4문서 시스템도 언제든 무의미해질 수 있으며, 실험정신을 이어가는 것이 중요함

프로그램이란 무엇인가에 대한 재정의 - What Even Is Programming Anymore?

   코딩 역사를 돌아보면, 추상화의 발전과 함께 ""내가 원하는 것을 설명하면 그것이 구현되는"" 시대에 접어들었음을 인식함
     * AI 활용은 단순한 새로운 추상화 계층 이상의, 전혀 다른 실체로 변화하고 있음
     * 지금의 프로그래밍은 구문 지식, 알고리듬 이해, 시스템 설계 능력이 아닌, '구체적인 상상력'과 '** 정확한 의도 표현**' 같은 새로운 역량이 요구됨
     * “원하는 바를 일관되게 명확히 설명할 수 있는 능력” 이 무엇보다 중요해짐

개 문서 시스템의 철학적 의미 - The Four-Document System as Accidental Philosophy

     * 이 4문서 체계는 결국 기억과 망각, ""다시 반복하고 싶지 않은 경험의 기록""임
          + Architecture Overview: ""내가 기억상실이 된다면 알고 싶은 내용""
          + Technical Considerations: ""다시 반복하고 싶지 않은 문제""
          + Workflow Process: ""놓치기 싫은 패턴""
          + Story Breakdown: ""매번 새로 시작하는 상황에서 어떻게 진전을 낼지""
     * 모든 문서는 결국 미래의 나에게 보내는 메시지 역할임
          + 결국 정보의 유실을 대비한 자신에게 보내는 안내문 성격

불안정한 고원과 영구적 초보자 - The Uncomfortable Plateau

   지금은 모두가 주니어 개발자가 된 것처럼 영원히 불안정한 초보 상태임
     * 전통적인 주니어와 달리 바뀌는 기술 속도 때문에 숙련자가 될 시간조차 없음
     * 끊임없이 변화하는 '물리 법칙' 속에서, 안정적 숙련성보다 적응과 실험정신이 중요해짐
     * 이 불확실성은 통제에 대한 집착이 있다면 두렵지만, 받아들이면 해방감도 큼

Where This All Goes

   다음에 무엇을 만들지, 어떤 프로세스를 쓸지, 이번에 만든 네 개 문서를 계속 쓸지 알 수 없는 상태임
     * 모든 개발자는 동시에 자신의 루틴에서는 전문가, 새로운 상황에서는 완전 초보자임
     * 4일 만의 작업이 과거의 몇 개월 분량이 될 만큼, '원하는 바를 설명하는 역량'이 결정적인 스킬로 부상함
     * 본인의 네 개 문서 역시 권장사항이나 템플릿이 아닌, 집단적 실험의 한 흔적이 될 뿐임
     * 문서, 과정, 방법 모두 일시적 산물이며, 남의 방식이 자신의 답이 되지 않을 수 있음

   결국 우리는 모두 썰물 때의 모래성(소프트웨어) 을 쌓고 있으며, 진보라는 파도가 곧 그것을 다시 쓸어갈 것을 인지하고 있음
   머지않아 누군가는 3문서 시스템, 5문서 시스템, 혹은 완전히 다른 접근방식을 시도할 것이며, 그 방식 또한 효과적일 수 있음

결론

     * AI와 함께하는 개발은 집단적 실험이자 창의적 시행착오의 연속임
     * 한 주의 프로세스도 이미 과거의 유물이 될 정도로 빠르게 변화함
     * 누군가의 흔적이 도움이 될 수도 있지만, 진짜 중요한 건 각자 자신만의 길을 만드는 것임

   마지막으로, 본인이 사용한 네 개 문서는 현재 GitHub에 공개 중임
     * 이는 절대적인 정답이나 템플릿이 아니고, 특정 시기의 한 실험 사례로 볼 것
     * 다른 사람의 자취를 참고하되 그대로 따라갈 필요는 없음을 강조함
     * 각자의 실험과 방법론을 개발하는 것이 AI 시대의 새로운 개발 생태계임

   뭔 방이 선생이라고 이상한 사이비놈 댓글다네

        Hacker News 의견

     * 이 글에 정말 공감함. Kidlin’s Law, 즉 “문제를 명확하게 글로 쓸 수 있으면, 이미 절반은 해결한 것이다”라는 이론을 우연히 발견했음. 요즘 AI 시대에 매우 강력한 원칙임. 자연어가 기술과 소통하는 주된 수단이 되면서, 명확하게 문제를 정의할 수 있으면 AI의 잠재력도 극대화할 수 있음. 비동기형 코딩 접근법도 정말 흥미로움. 개인적으로 Repl.it을 매우 자주 사용하고 있는데, 문제 해결에 집중할 수 있어서 놀라운 변화임. 코딩 도구를 사용할 때 마리오 카트에서 스타나 버섯 먹은 느낌을 받음. 너무 신나기도 하지만, 때로는 AI가 완전히 이상한 방향으로 갈 때도 있어 실시간으로 결정 개입이 필요한 경우도 있음. 스택 하나 관리만 해도 힘들었는데, 이제는 무한한 스택을 상대하는 기분임
          + 나도 스스로 소프트웨어 엔지니어로 성장하는 과정에서, 하고 싶은 것을 설명할 수 있도록 소프트웨어 세계의 용어 자체를 익히는데 많은 시간을 들였다는 점을 종종 떠올리게 됨
          + Repl.it은 정말 잘될 때는 몇 분 안에 해결되는 일이 오후 내내 걸릴 때도 있음. 하지만, 가끔은 프롬프트 박스 아래에 있는 추천사항을 시도해도 제대로 작동하지 않아서 매우 실망스러움
          + 사실 문제를 명확하게 진술하는 게 옛날부터 항상 어렵고, 지금도 마찬가지임. 명확한 자연어를 코드로 바꿔주는 도구가 생긴 건 정말 멋진 일이지만, AGI가 나와도 명확한 스펙을 만들어내야 하는 작업 자체는 변하지 않을 것임. 도구 덕분에 코딩 자체에 싸우는 시간은 줄일 수 있겠지만, 결국 정말로 명확한 명세 작성이 가장 어려운 부분임
     * 새로운 프로그래밍 방식이 너무 마음에 듦. 이 방식이 어디로 갈지 모르겠지만 당장은 만족스러움. 지금도 평소엔 휴식할 시간에 코드를 만들고 있고, 이게 오히려 휴식처럼 느껴짐. 오래 일한 시니어 개발자에게 특히 좋음. 요즘엔 에디팅 일이 대부분 지루함. 코드를 보고 잘못된 패턴을 발견하면 새로운 아이디어를 실험하기 위해 많은 부분을 바꿔야 하는데, 옛날엔 Stack Overflow 검색하고 고민해야 했던 일들이 이제는 Copilot 힌트 한 번, 혹은 Claude가 그냥 다 해결해줌. 예를 들면, 모의 주식 거래소를 만들었는데, 원래는 실제 거래소에 연결하는 것 때문에 종종 미뤄지는 작업이었음. 이제는 Claude가 HN 읽는 동안 다 만들어놓음. 여기에 전략 구현까지 하려면 사실상 지루하기만 했던 반복작업도 바로 처리됨. 오타, 의존성 추가 같은 일들 때문에 시간이 오래
       걸렸는데, 이젠 그럴 필요도 없음. 이렇게 하면 코드가 엉망이 될까 걱정할 수도 있는데, 나는 항상 Claude와 대화하면서 변경사항을 비판적으로 검토함. 경험이 도움이 되긴 하지만, AI가 오답으로 가는 것도 금방 감지할 수 있음. 그래서 내 커리어에 딱 맞는 순간에 이런 도구를 만나게 된 셈임. 문제는 주니어 개발자들에게 남아있음. 계단이 사라진 산꼭대기로 한 번에 올라가는 셈이라서, 어떻게 성장할 수 있을지 궁금함
          + 주니어 개발자들 전망에 동의함. 거의 50살에 30년 넘게 여러 분야에서 프로그래밍했지만, 내 경험에 기반해 에이전트를 잘 다루고 아키텍처를 튼튼하게 만드는 법을 알고 있음. 경험 없이 모든 게 AI가 조리해서 나온다면, 후배들이 어떻게 성장할지 정말 궁금함. 시간이 알려줄 문제임
          + 나도 대형 언어 모델 즐겁게 사용하지만, 계속 프롬프트만 입력하는 건 지루하고 불안하기도 함. 프로그램이 돌아가는 원리를 정확히 모르는 기분임. 직접 무언가를 만드는 건 정말 재미있고, 이미 해본 반복 작업이나 신경 안 쓰는 업무는 LLM에게 시킴. Claude로 터미널 기반 snake 게임도 만들어봤는데, 정말 신기했음
          + 예전의 자질구레한 작업들로 돌아갈 수 없다는 걸 스스로 깨달았는지 궁금함. LLM 등장 덕분에, 작업하는 동안 밖에 나가고 싶은 마음이 커짐. 예전처럼 12시간씩 모니터만 보며 두 개의 블랙박스를 연결하지 못해 허송세월하는 경험을 신입 개발자들은 더 이상 겪지 않아도 되니 부럽기도 함
          + 실제로 구현할 때 모두 처음부터 끝까지 한 번에 처리하는지 궁금함. 나는 항상 반복적이고 점진적으로 작성하고 다듬으면서 구현함. 드로잉에 비유하면, 전체를 대략 잡고, 점점 세밀하게 보완해나가는 구조임. 각 단계마다 내가 뭘 하고 싶은지 조금씩 명확해지고, 최소한의 노력으로 최대 효과를 내는 방식임. 코딩은 리팩토링 중심으로, 최소한으로 동작하는 코드를 만든 다음, TODO 주석을 남기고 반복적으로 개선하는 스타일임
          + 이런 도구들이 예전부터 수천 번 해왔던 지루한 작업을 대신해주어서 정말 설레임
     * 내가 생각하는 AI란 인터넷에 존재하는 모든 정보 위에서 대화를 할 수 있는 차세대 구글 검색임. 검색 엔진이 대중화되면서 여러 산업(신문, 전화번호부, 백과사전, 여행사 등)에서 일자리가 사라졌던 것처럼, AI도 그런 변화를 불러옴. 하지만 이것이 사람들이 생각하는 것만큼 존재론적 위기는 아니라고 봄. AI는 그냥 하나의 도구임. 영리하고 창의적인 사람들이 이 도구를 활용해 멋진 일을 많이 할 것임. 결국 사용은 사용자에게 달렸음. 검색은 채팅이 되었음. 예전엔 직접 찾아봤지만, 이제는 채팅을 하면 AI가 대신 찾아주고, 그 이상도 해줌
          + 채팅형 LLM 인터페이스가 최적의 방식인지는 잘 모르겠음. 좀 더 스마트한 접근법이 필요해 보임
          + 구글 전성기와는 다르게, 이제는 신호 대비 노이즈가 더 많아지고, 데이터 출처도 흐려짐
          + 이미 구글 검색 결과는 쓸 만한 정보보다 AI가 많이 만든 쓰레기가 먼저 노출되는 느낌임
          + 최신 검색 엔진은 답만 주고, 답으로 가는 과정은 제공하지 않아서, 올바르게 정보를 찾고 기록하는 사람의 역할이 사라지고 있음. 이 부분이 사라지면 결국 모두 방향을 잃게 될 것임. AI가 기존 정보를 다시 활용하므로, 창작자(특히 좋은 저널리즘 기자)에게 수익을 돌려주는 방법이 필요함. 그렇지 않으면 민주사회 기반이 무너질 위험이 큼. 뉴스 산업은 이미 수년간 위기에 처해 있었고, 그 결과로 불신과 분열, 잘못된 정보, 외부 조작 등을 경험하고 있음. AI가 마지막으로 업계에 치명타를 줄 수도 있음. 단순한 일자리 대체 문제가 아니라, 지금 우리가 가는 길이 매우 어두운 방향임
          + 검색 이외의 다양한 분야에서도 명확히 유용함
     * Claude Code를 폰에서 클라우드 VM으로 돌려, 산책길이나 자전거 라이딩 중에도 피드백을 주며 작업을 계속하고 싶음
          + vibetunnel 같은 도구로 벌써 비슷하게 할 수 있을 듯함
            https://vibetunnel.sh
     * 입력과 출력 비율이 흥미로움. 우리는 대개 아웃풋 양을 극대화하려고 하지만, 이제는 반대임. 나는 최대량보다는 일의 과정이 구체적이고 검증 가능한 단계로 나눠지길 바람. Cursor와 요구사항을 같이 작성할 때 처음에는 잘 되지만, 실수로 계획과 어긋나는 대량의 코드를 생성하는 문제가 있음. 마크다운 제목 뒤에 빈 줄을 추가 못하거나, 반복적으로 알려줘야 하는 사소한 점도 있음. 반복 과정과 품질, 일관성을 내가 좀 더 통제할 수 있으면 좋겠다고 느낌. 테스트가 가능한 닫힌 문제로 바꿀 수 있을 때 AI가 진가를 발휘함. 내가 열린 문제를 닫힌 문제로 변환하는 걸 도와주는 도구가 필요함
     * “사무실로 들어가 Claude가 만든 걸 테스트하고, 잘 되면 커밋하고 푸시함” 이런 경험이 자꾸 반복되니까, 사이버 보안 컨설턴트로서 앞으로 돈을 정말 많이 벌 수 있을 것 같음
          + 가능성은 있음. 하지만 자율주행차 얘기에서처럼, 인간보다 실수가 줄긴 하겠지만 완전히 사라지지는 않을 거라는 점도 기억해야 함
     * 이건 vibe 코딩이 아니고, 완전히 새로운 거라고 생각함. 나는 이걸 “flex coding”이라고 부름. 한 오후에 전체 앱을 만들고, 좋은 아빠 역할까지 했음. “이제 서버 연결 UI 만들어줘”라고 하면, Claude가 코딩하고 나는 다시 일상으로 돌아감. 아침도 만들고, 아들과 놀고, TV도 보고, 그 사이 사이 Claude가 계속 코딩함. 한두 시간마다 잠깐 들러서 테스트 후 피드백 주는 식임
          + 감정적으로 정말 매력적이고, 많은 사람들이 꿈꾸는 라이프스타일이겠지만 Claude의 코드가 정말 신뢰할 수 있는 수준임? 고객에게 비용을 청구하거나 내 명예를 걸 제품에 사용할 수 있는가? 내 답은 “아니오”임. 직접 써 보니 참조 오류, 기존 타입 복붙 후 이름만 바꾸고, 타입 오류가 아예 없는 상황을 자주 봄. 테스트 코드를 짜게 했더니, 실패하면 실패하는 게 아니라 결국 자가검증만 통과하는 이상한 테스트를 만들기도 함. 소중한 시간 가족과 보내는 건 좋지만, 내가 만든 앱을 중요한 곳에 쓰라고 권하진 않겠음
          + 이런 식으로 일하는 사람에게 왜 급여를 줘야 하냐는 의문과, 내가 직접 할 수 있는데 왜 소프트웨어에 비용을 내느냐는 생각이 듦
          + 조심하라는 의미로, 이제 곧 Claude가 너도 일 좀 하라고 불평하기 시작할지도 모름
     * LLM 활용 소프트웨어 도구들에 한계를 느낌. 하나의 글로벌 시스템 프롬프트를 모든 OpenRouter Key 기반 앱에 공통으로 적용하는 방법도 없고, 한 앱에서 다른 앱으로 대화를 옮기는 것도 곤란함. 모든 앱에 동일한 MCP 툴 접근 권한조차 제대로 제공되지 않음. Claude Code UX가 현재는 최고인 것 같지만, Claude 구독에 묶이고 싶지는 않고 내 키를 통해 원하는 공급자와 연결하고 싶음
     * 보안, 국제화, 현지화, 접근성, 사용성 등등을 성공적으로 프롬프트했던 부분을 놓친 것 같음. 이런 품질 요소가 없는 채로 “소프트웨어 제작자”를 자처하는 아마추어가 너무 많은 게 문제임. 이런 측면들이 빠지면 상용 소프트웨어로는 절대 성공할 수 없음. 이런 부분을 쉽게 프롬프트로 해결할 수 있다고 생각한다면, 그 분야에서 진지한 경험이 없는 것임
          + 공정하게 보면, 실제로 상업용 소프트웨어 중에도 이 부분을 제대로 고려하지 않은 경우가 많음
          + 나 역시 회의적이지만, 링크된 문서 네 개 중에서 최소한 접근성과 사용성 문서는 포함되어 있음. 국제화와 현지화는 안 보여도 본질적으로 크게 다를 건 없다고 생각함. 반면, 보안 문제는 정말 별개의 영역이라고 느낌
          + “내 네 개의 문서 시스템? 결국은 패턴이 된 스파게티일 뿐이고, 내일이면 모두 무너질 수도 있음. 다시 스파게티를 던지는 거임” 이런 식으로 개발이 스케일 될 거라고 믿는 사람들이 아직 많다는 게 놀라움
     * 최근 모델 기반 개발에 실험적으로 접근 중이고, 글의 ""프로그래밍은 뭔가?""라는 부분에 깊이 공감함. 25년의 경험과 컴퓨터공학을 모두 활용하지만, 손으로 코드를 직접 쓰는 전통적 프로그래밍은 아닌 것 같음. 지금은 무언가를 수작업으로 만드는 게 아니라 도구를 조종하는 파일럿 같은 기분임. 수작업을 즐기는 사람은 앞으로 5년 안에 업계를 떠날 가능성이 높다고 보임. 물론 여전히 수작업이 필요한 부분이 있겠지만, 새로운 방법론이 열리고 있음. 현재는 모두가 이 방법론에 능숙하지 않지만, 이 역시 업계의 일부가 될 것임
          + 한때는 생산성을 높이기 위해서는 반드시 지식을 습득해야 했는데, LLM 덕분에 이제 생산성 단계로 바로 건너뛸 수 있음. 단순한 지식 민주화라기보다는, 아예 지식 자체가 불필요해지는 현상임

   주말에 번역해서 올려보려고 했는데, GN+ 에게 일을 빼았겼군요 🥲

   ""문서 체계도 처음부터 설계한 게 아니라 즉흥적으로 쌓인 결과임"" 부분에서 강한공감과 실소가 나오네요.ㅎㅎ
"
"https://news.hada.io/topic?id=22045","AmazingHand - 오픈소스 로봇 손 프로젝트","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      AmazingHand - 오픈소스 로봇 손 프로젝트

     * Amazing Hand 프로젝트는 200달러 이하의 저비용으로 휴머노이드 로봇 손을 만들고 제어할 수 있는 오픈소스 솔루션
     * 8개의 자유도를 가진 4손가락 구조, 모든 액추에이터가 내부에 배치되어 외부 케이블 없이 작동함
     * 3D 프린트 부품, 저렴한 가격, 커스텀 가능성 등으로 기존 상용 로봇 손보다 접근성이 높음
     * 파이썬 스크립트 + Serial bus, 또는 Arduino 기반 제어 등 다양한 방식 지원함
     * 전체 소스 코드, CAD, 조립 가이드 및 BOM 등 제작에 필요한 자료 모두 공개됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Amazing Hand 프로젝트 개요

     * Amazing Hand는 값비싼 기존 로봇 손과 달리, 저렴한 오픈소스 형태로 실험적이고 표현력 있는 휴머노이드 로봇 손 구현을 목표로 함
     * Reachy2와 함께 사용할 수 있고, 다양한 로봇 손목 구조에도 적용 가능함.

     * 8 자유도, 4손가락, 손가락당 2개의 관절 및 유연 쉘 구조
     * 모든 액추에이터를 손 내부에 배치, 외부 케이블/액추에이터 없이 동작
     * 3D 프린트 호환, 400g 무게, 200유로 이하의 저렴한 제작비용
     * 완전 오픈소스, 외부 커뮤니티 업데이트 및 활용 사례 공유

주요 기능 및 설계 특징

     * 각 손가락은 평행 메커니즘으로 구동되고, 2개의 소형 Feetech SCS0009 서보가 굽힘/폄침, 내전/외전 동작 제어
     * 손가락 파츠 대칭성 고려: 오른손/왼손 파트 구분으로 좌우 모두 제작 가능

제어 방법

     * Waveshare Serial bus + Python 스크립트 활용
     * Arduino + Feetech TTL Linker 활용
     * 각 방식별 기본 데모 프로그램과 상세 설명 제공

제작 자료

  BOM(자재 목록) 및 3D 프린트

     * 필수 부품 목록 (BOM), 3D프린트 부품, 프린트 가이드, 조립 가이드 모두 오픈
     * STL/STEP 형식의 CAD파일, Onshape 문서, 프리셋 각도 데이터 등 포함
     * 오른손/왼손 조립차이, 서보 ID 할당 주의사항 설명

  조립 및 데모 실행

     * 조립 가이드 PDF에서 BOM의 표준 부품 조합
     * 파이썬 & Waveshare, 아두이노 & TTLinker로 손가락 캘리브레이션 스크립트 제공
     * 독립형 좌우 손 모두 제작 가능, 로봇 양손 결합 시에는 각기 다른 서보 ID 필요

  데모 실행

     * 파이썬/아두이노 기반 기본 데모 소프트웨어
     * 외부 전원(5V/2A DC/DC 어댑터 등) 필요
     * BOM 문서에 추천 전원 정보 포함

설계 한계 및 참고사항

     * 3D프린트 품질, 부품 수작업 조정 등으로 실제 각도에 차이 발생 가능성
     * 복합 파지(그립) 동작 및 장시간 내구성은 추가 소프트웨어 개발 이후 충분한 실험 필요
     * SCS0009 서보의 스마트 기능(토크, 위치, 온도, 피드백 등) 지원

고급 데모 및 확장성

     * 역방향/정방향 운동학 기반 고급 데모 및 테스트 툴 제공
     * 미래에는 자체 집적 PCB, 지능형 손가락 폐쇄 동작, 다양한 손가락 길이/형상, 센서 추가 등 지속적 개발 목표

커뮤니티, FAQ, 연락처

     * 커뮤니티 기여 사례, 중국어 BOM, 파생 베이스 등 자료 공유
     * To-Do List: 커스텀 PCB, 파지 동작 테스트·스마트 컨트롤, 손가락 추가/변형 연구, 센서 통합 등
     * Discord 공개 채널, 직접 연락 링크 제공
     * 주요 기여자 명시

결론

   Amazing Hand는 저렴하면서도 확장 가능한 로봇 손 오픈소스 프로젝트임. 제작 자료 전면 공개, 다양한 제어 및 디자인 옵션, 커뮤니티 중심의 발전 방향 등으로 로봇 연구자, 메이커, 교육자, 스타트업에게 활용 가치가 높음.

        Hacker News 의견

     * $135의 부품 원가가 가장 눈에 띄는 부분임. 이런 세상이 찾아왔다는 게 정말 놀라움
          + Feetech에서 양방향 컴퓨터 인터페이스를 갖춘 R/C 타입 서보 액추에이터를 판매 중임. 이 방식은 Dynamixel에서도 10년 넘게 쓰이고 있지만, 가격대에서 큰 차이가 있음. Feetech는 $17, 반면 Dynamixel은 $70 이상임. 부품 리스트 대부분이 '강함'이 필요하다고 적혀 있는데 실제로는 3D 프린트된 PLA 플라스틱이라서 내구성이 낮은 편임. 영상에서는 이 로봇 손이 실제로 무언가를 잡거나 다루는 모습이 안 보임. 결국 이것은 컨셉 검증 단계 모델임. 수요가 충분하면 사출 성형 방식으로 더 강한 재질(예: 폴리카보네이트, 유리섬유 강화 나일론 등)의 부품도 만들 수 있을 것임. 전체 플라스틱 부피가 매우 작아서 고급 플라스틱 써도 원가 부담이 적음. 그런데도 하비용 사출 성형은 잘 안 쓰임. TechShop과 대학 메이커스페이스에서도 데스크탑 사출기와 CNC로 몰드까지 만들
            수 있고 Autodesk Moldflow 같은 소프트웨어도 갖췄지만 거의 아무도 이걸 실제로 쓰지 않음. 전 세계의 플라스틱 제품 대부분은 사출 성형으로 만듦
               o Feetech
               o Dynamixel
               o 데스크탑 사출 성형 (NYU 소개)
          + 이 디자인은 내가 지금껏 본 것 중 최고 수준임은 인정함. 하지만 이 가격대에서 모터 외부에 절대적 엔코더, 신뢰도 높은 힘/토크 센서(예를 들어 딸기를 집는 것처럼 정밀한 작업), 텐던 구조(아래 스레드 참고)는 기대하기 어려움. 그래서 실제 연구나 현실적인 프로젝트엔 한계가 있을 듯함
          + 대부분의 손과 관련된 일은 30분에 $100 넘게 비용이 발생함. 만약 이 로봇 손이 그런 일을 할 수 있으면 관련 산업에 큰 변화를 줄 수 있음
          + 이 제품이 곧 출시될 K-Scale 로봇의 옵션으로 추가되면 좋겠음. K-Scale에서는 5지 손 끝-이펙터가 $1,000에 판매될 예정임
               o K-Scale
          + 3D 프린터 구입 비용도 고려해야 함
     * 나는 우리가 기대하는 인간형 로봇보다는, 벽에 설치하거나 바닥에 세울 수 있는 다관절 로봇 팔에 더 관심 있음. 필요나 취향에 따라 팔 개수를 추가하거나 줄일 수 있고, 소방기, 온도계, 주방 필수품도 옵션으로 달 수 있음. 다양한 조리도구, 식기 등을 들고 필요시 조언도 해주는 부엌 어시스턴트 역할을 상상함. 예를 들어 “소금은 한 꼬집, 약 5g 넣어드릴까요?” 같은 피드백도 가능함. 차고, DIY 테이블 등 다양한 용도에 맞춘 맞춤형 다관절 팔 형태 로봇이 이상적임
          + 당장은 부엌에 로봇 팔을 넣고 싶진 않지만, 남들이 베타 테스터가 되어 주면 좋겠음. 대신 빨래처럼 더 정의된 절차의 반복작업(더러운 옷 → 세탁기 → 건조기 → 바구니로 옮기기)부터 시작하면 안전성이나 위험 부담이 적어 보임
          + 진지하게 말하면, Vassar Robotics(와이콤비네이터 투자받은 회사)가 로봇 팔 키트를 지금 주문받고 있음. 내 주문도 최근 카메라 스펙 업그레이드 때문에 출하가 미뤄졌음. 아마 칼같은 위험한 도구는 못 들지만, 벽에 설치하는 로봇 팔을 구현하려는 기업이 실제로 여러 곳 있음
          + 솔직히 바퀴든 벽이든 컴퓨터가 제어하는 칼 휘두르는 팔이 집에 있는 건 그닥 내키지 않음
          + 늘 상상해왔던 건, 부엌 상부장 아래 레일에 매달려 미끄러지는 로봇 손임
          + 혹시 촉수(텐타클) 스타일은 어떨지 제안함
     * Pollen Robotics와 HuggingFace가 요즘 로봇 분야 발전에 많은 공헌을 하고 있음
          + HuggingFace 로봇이 실제로 보급될지 궁금함. 동작 중 ‘잠자기 모드’로 들어가면 눈/카메라가 목 뒤로 돌려지는 것도 포착함
     * 혹시 외골격이나 보조기기와 유사한 다른 오픈소스 프로젝트 아는 사람 있음?
          + theopenexo.nau.edu
          + 본래는 우주복 사용 시 손목/팔 피로를 줄이기 위해 개발된 기술임. 마지막 정보로는 여러 사유로 2020년에 프로젝트가 중단된 상태였음
     * 세상의 대부분 사물이 인간에게 맞춰져 설계되어 있기 때문에, 그에 맞춰 로봇 기술이 진화하는 게 참 반가운 움직임임
          + 이런 이유 때문에 최근 상장된 대부분의 로봇 회사 주가가 오르는 원동력이 됨
     * 혹시 로봇 손가락 수를 5개 대신 4개만 두기로 한 특별한 설계적 이유가 있는지, 그리고 그에 따른 트레이드오프엔 뭐가 있는지 아는 사람 있음? 며칠 전에 트위터에서 이 프로젝트를 봤을 때도 그 질문이 들었음. BOM(부품 명세) 기준으로 손가락 하나로 약 $10 비용 절감되는 것 같음
          + 당분간 손의 두께(서보모터 수준)를 제쳐두더라도, 손바닥의 너비만 봤을 때 이 4손가락 손은 너클 기준 4인치 약간 안됨. 이 정도만 해도 이미 인간용 장갑 치수로 ‘X-Large’임. SCS0009 서보는 개당 1/2인치 정도고 손가락마다 2개씩 필요. 하나 더 추가하면 폭이 5인치로 ‘3X-Large’ 급임.
          + 각 손가락 구동용 서보의 폭이 있어서 손가락 간격이 정해지는 걸로 보임. 다섯 손가락이면 손이 너무 넓어져서 어정쩡해진 것 같음
     * 이 로봇 손을 할로윈용으로 만들어서 Thing(아담스 패밀리의 움직이는 손 캐릭터)처럼 장식하고 싶음
     * 팔 전체에 텐던(줄)을 넣어서 전체 무게를 줄일 수 있을까 궁금함
          + Will Cogley가 텐던식 로봇 손 여러 디자인을 만듦
               o Will Cogley 유튜브
               o Will Cogley 노션 자료
          + 대부분의 텐던 소재가 탄성이 있어서 캘리브레이션 문제를 만들고, 손에 위치감각 센서(프리프리오셉티브 센서)가 필요함
     * 결국 가장 중요한 질문은 이 손이 얼마나 무거운 걸 들 수 있냐는 것임. 예를 들어 0.5파운드 들 수 있다면, 10/20/30 파운드로 늘리려면 어떤 변화가 필요할지 궁금함
          + 손은 잡는 역할, 팔이 들어올리는 역할임. 그래서 팔이 없는 손은 강한 힘을 내기 어려움
     * 아주 멋진 프로젝트임. 하지만 실제 손과 비슷한 수준의 성능을 내려면, 손바닥 전체에 압력과 온도 최소 두 가지의 매우 민감한 센서가 필요하다고 생각함
          + 아주 공감함. 일단 압력 센서만이라도 있으면 시작으로 충분함
"
"https://news.hada.io/topic?id=22054","Kingfisher - 초고속 실시간 시크릿 탐지·검증 도구","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Kingfisher - 초고속 실시간 시크릿 탐지·검증 도구

     * Rust 기반으로 개발된 초고속 시크릿(비밀정보) 스캐닝 및 검증 도구
     * 멀티스레드와 Intel Hyperscan 정규표현식 엔진 기반으로 대규모 코드베이스 초고속 탐색 가능
     * Tree-Sitter로 20개 이상 언어의 AST 파싱 지원, 단순 정규식 오탐 최소화
     * 수백 개의 내장 탐지 규칙과 실제 클라우드 API(예: AWS, Azure, GCP, Stripe 등) 호출을 통한 실시간 시크릿 유효성 검증 기능 제공, 커스텀 규칙도 YAML로 손쉽게 확장 가능
     * GitHub·GitLab 등 다양한 저장소 및 Git 히스토리 지원
     * 실시간 업데이트를 지원하여 자동으로 새 버전 체크 및 무중단 자체 업데이트 가능
     * Praetorian Security의 Nosey Parker 오픈소스 프로젝트를 기반으로 한 포크버전으로, 라이브 검증·Windows 지원·언어 구문 분석 등 다양한 기능을 강화함
     * 개발 단계부터 운영 환경까지, 유출된 시크릿을 조기에 감지·차단하여 보안 사고 예방, 컴플라이언스 비용 절감, 업무 중단을 최소화할 수 있음
"
"https://news.hada.io/topic?id=22044","노션 데스크톱이 오디오 및 네트워크를 모니터링함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       노션 데스크톱이 오디오 및 네트워크를 모니터링함

     * Notion Desktop 앱이 사용자의 오디오와 네트워크 활동을 모니터링하고 있음
     * 사용자들은 일상적인 사용 중에도 이러한 동작이 발생함을 보고함
     * 데스크톱 애플리케이션에서 예상치 못한 권한 요청과 모니터링 움직임이 발견됨
     * 개인 정보 보호와 투명성 부족 이슈가 크게 제기됨
     * 소프트웨어 사용 시 보안 및 프라이버시 점검의 중요성 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Notion Desktop 앱의 오디오 및 네트워크 모니터링 이슈

   최근 여러 사용자가 Notion Desktop 애플리케이션의 사용 중 오디오 입력 감지 및 네트워크 트래픽 모니터링이 일어나고 있음을 보고함
     * 애플리케이션 실행 시 마이크 등 오디오 소스에 접근 시도 움직임 확인됨
     * 네트워크 수집 도구를 통해 앱의 데이터 전송 활동이 평상시 이상으로 포착됨
     * 사용자 동의 없이 이루어지는 모니터링 움직임에 대해 커뮤니티에서 우려 확산 중임

개인정보 보호 및 보안 이슈

     * 해당 동작들은 개인정보 보호 정책 및 투명성에 대한 중요 이슈를 나타냄
     * 사용자는 데스크톱 소프트웨어의 권한 요청 내역과 실제 행위가 일치하는지 확인 필요성이 커짐
     * IT 전문가 및 보안 커뮤니티는 이러한 앱들의 배경 프로세스 감시에 주의를 기울일 것을 권고함

결론 및 시사점

     * 데스크톱이나 기타 소프트웨어 사용 시, 앱 권한 요청과 실제 데이터 접근 내역을 확인하는 습관이 중요함
     * 프라이버시와 보안 확인은 IT 실무자와 개발자 모두가 지속적으로 관심 가져야 할 핵심 과제임

        Hacker News 의견

     * 안녕하세요! 1. Notion은 Meeting Notes 기능을 사용할 때만 오디오를 기록함, 자세한 내용은 공식 문서 참고 2. Notion 데스크톱 앱은 회의 알림을 표시해서 Meeting Notes 사용 여부를 묻는데, 이 과정은 단순히 마이크가 켜져 있는지로 판단함(즉, 마이크에서 나오는 소리를 듣는 것이 아님) 이 기능은 환경설정의 Notifications > Desktop meeting detection notification에서 설정 가능함 Notion 직원임
          + 추가 설명하자면, Notion 데스크톱 앱은 사용자의 컴퓨터에서 Zoom과 같이 마이크를 활용하는 프로세스가 동작 중인지 확인함 Notion은 마이크 오디오를 수집하거나 다른 앱의 신호에 접근할 수 없고, 오로지 마이크를 사용하는 프로세스가 있는지만 판단함 이는 OS의 마이크 인디케이터로 쉽게 검증 가능함, Notion이 마이크를 사용 중이 아니라면 인디케이터에 나타나지 않음 회의 앱이 감지되면 사용자는 알림을 받고, 캘린더를 연결한 경우엔 관련 일정과 연동됨(캘린더 연결이 없어도 알림은 옴) 이 기능은 Settings > Notifications > Desktop meeting detection notifications에서 사용자 설정으로 끌 수 있음 실제로 회의 노트를 시작해 직접 녹음 버튼을 클릭해야 Notion이 마이크를 사용하게 되며, 이 때는 반드시 운영체제의 권한 동의가 필요하고(OS에서 나타나는 권한 창)
            Notion 역시 OS 인디케이터에 마이크 사용 앱으로 표시됨 Notion 직원임
          + 1)에서 언급된 대로, 만약 AI Meeting Notes 기능을 사용하고 싶지 않다면 워크스페이스 관리자가 언제든지 콘솔의 토글 버튼으로 기능을 해제할 수 있음 이 부분의 문제점은, 반드시 opt-in 방식으로 바꿔야 한다고 생각함
          + Notion이 내 네트워크 트래픽을 확인하기 전에 명확한 동의를 왜 얻지 않았는지 궁금함 Notion이 네트워크 트래픽을 모니터링하는 다른 사례가 또 있는지도 궁금함 있다면 구체적으로 어떤 방식인지 알고 싶음
          + 참고로, 어떤 앱이 마이크 입력을 녹음 중인지 OS의 마이크 인디케이터로 확인할 수 있음 Windows, Mac, Linux 모두 이런 기능이 있음 (편집: 위 @jitl 의견도 참고)
          + 마이크 감지 과정에서 Notion으로 네트워크를 통해 체크 사실이나 그 결과가 전송되는지 궁금함
     * Notion은 단순히 마이크가 켜져 있는지만 감지하지 말하는 내용을 듣지 않음(권한을 부여하지 않으면 Notion이 들을 수 없음) 또한 네트워크 트래픽도 확인해서 오디오가 전송되는지 본다고 알고 있음(이렇지 않으면 오탐지 빈도가 높아짐) 마이크와 네트워크 정보 조합은 회의 감지에 자주 쓰이는 기법이며, 내가 직접 만든 앱 LookAway에서도 이 방식으로 통화 중에는 리마인더를 중지함
          + macOS에서는 앱이 네트워크 트래픽을 모니터링하려면 명시적 허가가 필요하지 않음? 내 앱은 사용자에게 요청하도록 돼 있는데, Notion은 별도의 안내 없이 트래픽을 확인해서 원글 작성자가 놀란 듯함
     * 나는 Notion의 끔찍한 성능 때문에 큰 불만이 있지만, 소규모 비즈니스 용도로 계속 유료 결제 중임 비개발직원들이 클라이언트 DB, 태스크, 결제 관리 등에 Notion을 쓰고 있고, 여러 번 교체하려고 리서치했으나 마땅한 대체제가 없었음 가끔 직접 도구를 직접 만들어야 할지 고민함
          + 대체제로 Anytype(https://anytype.io), Appflowy(https://appflowy.com) 추천함
          + ""직접 만들까 고민된다""는 말에 대해, 기존 오픈소스 프로젝트(XWiki, Nextcloud, wiki.js 등) 기여 또는 개선에 먼저 힘써보자고 제안함 이미 좋은 기능들이 많고, 혼자 경쟁 제품을 만드는 것보다는 힘을 합쳐 발전시키는 게 낫다 언급(XWiki SAS 직원이며, 부족한 기능은 유료로 맞춤 개발도 가능함)
          + 내가 가장 흥미롭게 본 도구는 Thymer인데, 직접 써보지는 않았고 빠른 퍼포먼스 쪽 데모 영상을 보고 인상 받았음
          + 같은 고민 중임 AI 기능을 강제로 번들하고 가격까지 30% 올린 뒤로 다른 대안을 찾고 있지만 협업 텍스트 에디터와 DB를 동시에 대체할 만한 게 정말 드물었음
          + 나는 Notion의 정보 구조, 특히 상단 인덱스 페이지와 멀티 유저 기능이 마음에 듦 Confluence, Obsidian 등도 써봤지만 UX 측면에서는 대체제가 없었음 Notion을 정말 좋아해서 계속 쓰는 건 아니고, 동일한 기능의 다른 도구가 나온다면 바로 옮길 의향임
     * Notion의 복잡함(불필요한 기능)이 싫다면 내가 만든 docmost.com을 추천함 UI가 깔끔하고, 실시간 협업, 다이어그램 지원 등 다양한 기능을 제공함 셀프 호스팅도 가능함
          + 문서 공개 기능이 필요해서 찾고 있었음. 사용자 헬프 문서를 만들면서 개발자용 논의/기술 정보까지 섞어서 앱 동작 방식의 단일 소스로 만들 계획임 필요 기능은 일부 블록만 ""공개""로 지정해 해당 부분만 게시되는 것임 현재 지원하는지, 또는 향후 지원할 계획이 있는지 궁금함
          + 나는 docmost를 셀프 호스팅해서 잘 쓰고 있고, 개발자에게 감사함! 한 가지 바라는 점은 공개 위키로 배포할 수 있도록 해줬으면 함 현재 공유 기능은 특정 URL을 지정해야 하고, 공개 페이지에서 실시간 편집이 강제되는 부분이 있음
          + 진짜 디자인이 마음에 듦 하지만 Obsidian 또는 독립실행형 앱(로컬 실행형)이었으면 더 좋았겠다는 아쉬움이 큼 웹앱은 싫고, 노트는 파일로 따로 관리하는 게 필요함(Notion과 docmost의 디자인이 Obsidian에도 있으면 좋겠음)
          + 위에서 언급된 nocodb와 연동해서 쓸 수 있을지 궁금함, 나 역시 DB를 문서와 함께 쓰기 때문임
          + 아주 흥미로움 이런 툴들이 다른 앱에 임베드(내장) 가능하게 해줬으면 좋겠음, 이를테면 이미 구축한 앱에 쉽게 추가할 수 있도록
     * 최근 Notion을 쓰다 재미있는 일이 있었음 노션 문서를 열려고 앱에서 cmd+O를 눌렀는데, 알고보니 이 단축키는 문서 오픈이 아니라 오디오 트랜스크립션(음성 기록)이 시작되는 단축키였음 두 시간 뒤 팀 오버뷰 페이지 하단에 내가 아내, 강아지와 나눈 대화 조각이 ‘광인의 일기장’ 처럼 기록된 걸 발견했음 다행히 찾고 삭제함
          + 혹시 강아지가 말한 것도 트랜스크립트에 찍혔나 궁금함 앞으로 LLM이 개 짖는 소리도 번역할 수 있을 때가 올까 기대함
     * 회의/콜 감지 앱을 직접 만들어본 입장에서, 이런 식의 회의 탐지 방식이 생각만큼 나쁜 일은 아님 하드웨어 사용 패턴만 파악해도(마이크 하드웨어가 활동 중인지 체크) 실제 음성 스트림을 도청하지 않고도 회의를 감지할 수 있음 어떤 앱이 마이크를 쓰고 있다면 거의 회의 중일 확률이 높음
          + 핵심은 무엇을 하느냐보다 어떻게 하느냐의 문제임 최소한 ""저희가 회의 여부를 자동 감지해 노트 도우미 기능을 제공합니다, 괜찮으신가요?""라고 고지라도 해줬어야 함 사용자 동의를 당연하게 여기지 말았으면 함
     * 긍정적으로 보자면, 이런 모니터링도 프라이버시를 최대한 지킬 수 있는 방식(예를 들면 오디오를 실제로 녹음하거나 전송하지 않고 주파수 스펙트럼만 분석하거나 단순히 마이크 사용 여부만 체크 등)으로 구현할 수 있음 네트워크의 경우도 실제 데이터가 아니라 포트 사용 여부 정도만 보면 덜 침해적임 그래도 적어도 off 옵션 제공, 가능하다면 opt-in 기능이 필요한 사안으로 보임
     * 프라이버시 이슈와 관련해, 데이터를 서버로 보내는 '모니터링'과 오프라인 용도의 무해한 점검용 '모니터링'은 엄연히 구분해서 논의해야 할 필요성을 느낌
     * Notion 데스크톱이 마이크 오디오 자체에 접근 가능한지, 아니면 단순히 마이크 사용 중임만 알아낼 수 있는지 궁금함 전자가 사실이라면 걱정이 크고, 후자(사용 유무만 알 수 있다면)는 별 문제 아니라 생각함
          + 후자가 맞음(마이크 오디오 접근권한 없이 앱이 소리까지 들을 수 없음)
     * ""Settings > Notifications > Desktop meeting detections notifications""에서 해당 기능을 끌 수 있음 다만 이 과정에서 마이크/네트워크 트래픽 체크까지 꺼지는지는 확실하게 검증해보지 못했음
"
"https://news.hada.io/topic?id=22026","Firefox 141에서 Windows용 WebGPU 정식 출시","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Firefox 141에서 Windows용 WebGPU 정식 출시

     * WebGPU가 오랜 개발 끝에 Firefox 141 Windows 버전에서 공식 지원됨
     * WebGPU는 최신 그래픽 처리와 고성능 연산을 위한 웹 기반 GPU 인터페이스로, 게임·** 시각화**·** 로컬 연산**의 수준을 크게 높일 것으로 기대됨
     * Firefox의 WebGPU 구현은 Rust 기반 WGPU 라이브러리 위에 구축되었으며, Direct3D 12, Metal, Vulkan 등 다양한 백엔드 지원
     * 현재는 Windows에서만 정식 활성화되며, Mac과 Linux, Android 지원도 향후 예정
     * 아직 성능 개선과 표준 준수 등 추가 개발 과제가 남아 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

WebGPU Windows 지원의 의미

     * 오랜 기간 개발한 WebGPU가 Firefox 141에서 Windows 환경에 정식 탑재됨
     * WebGPU는 웹 콘텐츠가 사용자 GPU와 직접 연동해 고성능 그래픽과 병렬 연산을 구현할 수 있도록 해주는 최신 표준임
     * 이 기술로 인해 웹 기반 게임, 데이터 시각화, 머신러닝 등 다양한 분야에서 성능 한계가 크게 확장될 전망임
     * WebGPU 튜토리얼, WebGPU 샘플, MDN 문서를 통해 관련 학습과 실습 가능함
     * WebGPU는 WebGPU W3C 표준과 WGSL 표준로 정의되며, Mozilla는 2017년부터 표준화 과정에 적극 참여해 왔음

브라우저별 WebGPU 현황

     * Chrome에서는 이미 2023년부터 WebGPU 지원이 이뤄짐
     * Safari 26에서는 올해 가을 출시 예정
     * Firefox 141은 현재 Windows만 정식 지원, Mac/Linux/Android는 향후 업데이트로 확대 예정
     * Firefox Nightly 버전에서는 그동안 Android 제외 모든 플랫폼에서 실험적으로 사용 가능했음

Firefox의 WebGPU 구현

     * Firefox의 WebGPU는 오픈소스 Rust 라이브러리인 ** WGPU**를 기반으로 개발됨
          + WGPU는 다양한 플랫폼 하드웨어에 맞춰 Direct3D 12, Metal, Vulkan 등 저수준 그래픽 API와 연결됨
          + Mozilla는 WGPU 프로젝트의 주요 기여자임
          + Rust 개발자라면 Firefox WebGPU에 기여하려면 WGPU 프로젝트에서 시작하는 것이 적합함
     * WGPU는 Firefox 외부에서도 널리 쓰이며 활발한 커뮤니티가 존재함

주요 과제와 개선 작업

     * WebGPU는 대형·복잡한 API로, 현재까지는 주요 데모 및 실사용 사례 중심으로 안정화에 집중함
     * 추가 개선이 필요한 영역:
          + GPU 샌드박스 프로세스와의 비버퍼링 IPC로 인해 발생하는 성능 저하 문제 해결(버그 1968122, Firefox 142에서 성능 개선 예정)
          + GPU 작업 완료 시점을 인터벌 타이머로만 감지해 생기는 대기 시간 증가(버그 1870699, 더 나은 방식으로 개선 중)
          + importExternalTexture 미지원으로 디코더에서 GPU로의 영상 데이터 직접 읽기 불가(버그 1827116, 개발 진행 중)
     * 실사용에서 문제가 발생하면 Bugzilla WebGPU 컴포넌트에 상세 내용을 첨부해 제보 필요

앞으로의 계획

     * Windows 이후 Mac, Linux, Android 순으로 정식 지원 확대 예정
     * 성능, 호환성, 표준 준수 등 지속 개선 계획
     * WebGPU의 정식 지원을 통해 웹 애플리케이션의 새로운 가능성이 열릴 것으로 기대

        Hacker News 의견

     * 정말 기대되는 소식임, Firefox 팀에게 축하 인사 전함
       내 회사는 Unreal을 브라우저에서 실행할 수 있도록 개발 중이며, Unreal Engine 5에 맞춘 커스텀 WebGPU RHI를 구축했음
       기술 데모를 직접 보고 싶은 사람들은 아래 링크 참고 바람
       (데스크톱의 Chromium 기반 브라우저와 일부 안드로이드 폰에서만 작동함)
       Cropout: https://play-dev.simplystream.com//…
       Car configurator: https://garage.cjponyparts.com/
          + Firefox 142 (nightly)에서 테스트해 봤음
            Cropout은 오랫동안 0%에 머물러 있으면서 1200개 이상의 네트워크 요청이 있었음
            결국 메뉴까지는 로드되는데 배경이 검은색이고 UI 요소만 보임
            쉐이더 파싱 시 많은 에러가 발생했고 기타 에러들도 있었음
            Car configurator는 여러 에러를 내며 0%에서 멈추고 로드가 되지 않음
            ""게임 파일이 누락되어 전역 셰이더와 콘텐츠 초기화가 어렵다""는 메시지가 뜸
            Firefox에서도 최소한의 테스트 후 공유했으면 하는 바람임
          + ""Chromium 기반 브라우저에서만 작동""이라고 밝혔지만, 이 글이 바로 Firefox의 WebGPU에 관한 이야기이므로
            혹시 Firefox 호환 버전을 테스트하거나 출시할 계획이 있는지 궁금함
          + Pixel 7a에서 Android Chrome으로 ""cropout""을 실행해 보니 0%에서 멈춤
            ""car configurator""는 97~98%까지 진행되지만 그 이후로 더 이상 진행되지 않음
          + Firefox 141을 사용하는 Windows에서 동작하는지 궁금함
            아니라면 그 이유가 무엇인지 알고 싶음
          + macOS용 Google Chrome에서 첫 번째 링크는 0%에서 멈춰 움직이지 않았으며
            두 번째 데모는 98% 혹은 97%에서 멈춤
            Safari에서도 동일한 현상이 있었음
     * 지금 그래픽스 API의 상황을 보면 OpenGL 시대보다도 오히려 퇴보한 듯한 느낌임
       현대 API들은 사용의 간편함이나 진정한 의미의 포터블성, 크로스 플랫폼성을 제대로 제공하지 못한다는 생각임
       Vulkan, Metal, DirectX12 같은 다양한 그래픽스 백엔드에 맞춘 커스텀 래퍼를 만드는 건 오히려 시간 낭비에 가까움
       마치 성능을 위해 문자열 대신 char 배열로 되돌아가는 느낌과 비슷함
          + 어떤 약속이 있었는지, 누가 그런 약속을 했는지 모르겠음
            그래픽스 API 목적은 어디까지나 GPU에 최대한 빠르게 코드와 데이터를 넣는 것이지, 개발자 경험이 우선은 아니었음
            WebGPU는 브라우저 안에서 컴퓨트와 렌더를 꽤 잘 래핑해 준다고 느꼈음
            아직 완벽하진 않지만 WebGL이나 OpenGL보다 더 직관적이고 탐색성이 뛰어나다고 생각함
          + 문제를 잘 못 느끼겠음
            그래픽스 스택에는 오래 전부터 저수준 API가 존재해왔고 (예시: Mesa의 Gallium), 이제는 그것들이 표준화되었고 사용자가 직접 선택함
            고수준 API도 여전히 존재하며, OpenGL도 합리적인 플랫폼에서 지원됨
            WebGPU도 native 코드에서 꽤 쓸 수 있었음
            이런 저수준 API의 진정한 포터블성 부족은 거의 Apple과 콘솔 제조사 탓임
            다만 콘솔 제조사는 원래 협력 기대를 안 했었음
          + OpenGL 시대로부터 얻은 교훈은 모든 플랫폼이 동일한 고수준 API를 사용한다고 해서 꼭 좋은 결과를 보장하지 않는다는 것임
            결국 중요한 것은 해당 플랫폼의 하드웨어 제어를 잘 해주는 API가 있느냐임
            OpenGL을 그대로 번역해주는 래퍼가 늘 필요했고, 과거에는 이 래퍼를 피할 방법이 없었음
            각 하드웨어 타입별로 최고의 결과를 내는 방식은 실용성이 떨어짐
            진짜 중요한 것은 쉬운 번역 레이어를 제공하느냐이지
            정말 하드웨어에 딥하게 접근하고 싶으면 오히려 단순하거나 범용적인 인터페이스 대신 하드웨어에 직접 다가갈 수 있는 API가 필요함
          + OpenGL은 2.0 이후 API가 너무 복잡해졌고, WebGPU는 Vk, D3D12, Metal의 기능을 꽤나 편하게 래핑하고 있음
            OpenGL보다 훨씬 설계가 잘됐다 생각함
            별도로 D3D11과 Metalv1이 아마 사용성, 성능 면에서 가장 적당한 균형점임 (특히 D3D11의 성능은 Vulkan, D3D12에서도 따라잡기 힘듦)
          + 나 역시 동의함
            OpenGL과 CUDA 인터롭으로 계속 개발할 예정임
            Vulkan은 너무 오버엔지니어링된 복잡성에 비해 실제 사용상의 이점이 없어서 오히려 CUDA를 쓰게 됐음
     * WebGPU가 브라우저 외부 환경에서도 성공적으로 확장되어
       공식 스펙으로 다루는 사용하기 쉬운 크로스 플랫폼 API(즉, opengl의 대체재)가 되길 여전히 기대하고 있음
       하지만 Rust 쪽을 제외하면 native 코드에서 WebGPU를 쓰려는 흐름은 크지 않다고 느낌
       예를 들어 Dawn을 쓰는 대형 프로젝트는 들어본 적 없음
       아마 WebGPU가 너무 늦게 나와서 대부분 이미 dx, vulkan, metal 위에 자체 추상화 계층을 구축하고 있었기 때문이기도 함
          + 결과적으로 안 퍼질 것으로 생각함
            단순함은 조금 있지만, 기능도 많이 부족함
            Vulkan에서 옵션이 된 일부 기능(예: 렌더 패스)이 WebGPU에서는 여전히 필수임
            바인드 그룹이 정적이라 오히려 불편하고
            또 WebGPU는 여러 제한과 불필요한 요소가 있음
            예를 들어 호스트에서 버퍼 서브리전에 바로 쓸 수 없고 반드시 중간 버퍼(스테이징 버퍼)를 사용해야 함
            대안이 없어서 웹에서는 활용하겠으나, 데스크톱에선 OpenGL+CUDA 인터롭 프레임워크로 계속 진행할 예정임
            더 합리적인 현대식 그래픽스 API가 나오길 바라는 입장임
            간단하게 cuMemAlloc, cuMemcpy 정도로 끝날 작업이, 복잡한 버퍼 할당 및 바인딩, 파이프라인, 엑스플리싯 싱크, 바인딩 그룹, 디스크립터 셋 등 쓸데없는 요소들로 복잡하다면 사용하고 싶지 않음
          + WebGPU는 Vulkan만큼의 최적화와 세밀한 컨트롤을 제공하지 않으며, Vulkan만큼의 성능도 나오지 않는 편임
            Vulkan에 있는 다양한 확장(extension)들도 아직 WebGPU에서는 지원되지 않음
          + 이미 존재하는 미들웨어가 있음
            브라우저 외부에서는 굳이 WebGPU를 기다릴 필요가 없다고 생각함
            애초에 브라우저 샌드박스에 초점을 맞춘 API 디자인이 갖는 제약들이 있기 때문임
          + 이름 자체도 큰 문제였다고 생각함
            나는 순수 native 개발자라 ""web gpu""라는 이름 자체를 수년간 그냥 웹 전용 기술이라 여겨서 신경 쓰지 않았는데, 막상 들어보니 오해였음
     * 우리 gpu-allocator https://github.com/Traverse-Research/gpu-allocator/ 크레이트가 훨씬 더 많은 사람들에게 알려질 것 같아 매우 기쁜 마음임
       지금까지는 wgpu의 dx12 백엔드나 자체 gpu 벤치마크 제품 evolve https://www.evolvebenchmark.com/에 썼었음
       앞으로는 더 넓게 활용될 수 있을 거라 기대함
     * macOS용 Firefox Nightly에서 이미 WebGPU를 쓸 수 있다는 사실을 이제야 알게 됨
       https://www.mozilla.org/en-US/firefox/channel/desktop/에서 mac용 나이틀리 받아서
       https://huggingface.co/spaces/reach-vb/github-issue-generator-webgpu 데모를 돌려봤고, 잘 작동했음
       이 데모는 SmolLM2 모델을 WebAssembly로 컴파일해서 구조화된 데이터 추출에 사용함
       기존에는 크롬에서만 동작한다고 생각했는데, 정식 Firefox(Stable)에서는 ""WebGPU가 지원되지 않는다""는 에러가 남
          + Firefox WebGPU 팀 멤버임
            macOS 지원은 곧 정식으로 제공할 예정임
            Windows 외에도 곧 Mac, Linux, 마지막으로 Android에서도 WebGPU를 지원할 계획임
     * ""Mac과 Linux, 그리고 마지막으로 Android에서 WebGPU를 지원할 계획""이라는 내용이 반갑게 느껴짐
       하지만 지금 시점에서는 큰 기대를 하긴 어려움
       Linux 브라우저에서 WebGPU 지원이 그동안 불가능했던 이유는 아마도 새로운 공격 벡터(attack surface)를 만드는 게 너무 어렵기 때문이라 생각함
       이런 복잡성이야말로 브라우저 개발을 어렵게 만든 웹 표준의 거대함을 보여주는 증거임
       넷스케이프 시절부터 이어진 설계 결정의 영향이 지금까지도 남아 웹 브라우저의 단일화, 펀딩 문제 등 걱정의 뿌리가 되는 것 같음
          + 어떤 리눅스냐에 따라 다름
            Android/Linux, WebOS/Linux, ChromeOS/Linux 등에선 이미 WebGPU가 지원됨
            다만, GNU/Linux에서 이런 작업 로드를 위한 지원은 브라우저 벤더들 입장에선 우선순위가 떨어진다는 점을 방증함
     * Linux용 구현도 기대하고 있음
       WebGPU가 나오면 해볼 만한 데모들이 있는지 궁금함
          + 가장 인상 깊었던 데모 중 하나는
            https://github.com/ArthurBrussee/brush
            WebGPU 기반 가우시안 스플래팅 트레이닝 및 렌더링임
          + Unity 기반 데모도 소개함
              1. https://boat-demo.cds.unity3d.com/
              2. https://www.keijiro.tokyo/WebGPU-Test/
              3. https://www.chatlord.com/4/
          + 대부분의 데모 사이트들은 웹 데모 형태임
            개인적으로 Compute Toys https://compute.toys/를 좋아함
            Shadertoy 스타일로 WebGPU 컴퓨트 샤더 실험이 가능함
            더 다양한 데모는 https://github.com/mikbry/awesome-webgpu
            직접 실험하는 프로젝트 https://github.com/s-macke/WebGPU-Lab도 있음
          + Bevy 엔진의 예제들도 WebGL와 WebGPU 양쪽 다 제공함
            비교 학습에 유용함
            참고 링크: https://bevy.org/examples-webgpu/, https://bevy.org/examples
          + 꽤 인상 깊었던 데모로 https://huggingface.co/spaces/webml-community/kokoro-webgpu 소개하고 싶음
            WebGPU 없이도 작동하지만 속도가 매우 느림
     * Firefox가 Chrome보다 먼저 Linux에서 WebGPU를 지원하게 될 것 같음
          + 사실 좀 의외임
            dawn(Google의 webgpu 구현)이 Linux에서 꽤 잘 동작하는데도 Firefox가 더 앞서서 지원하는 상황임
     * 드디어 WebGPU 지원이 시작됨에 모두에게 박수를 보냄
       Chrome에서만 WebGPU를 가지고 실험했던 것이 살짝 껄끄러웠는데
       Safari 역시 최근 프리뷰 버전에서 지원을 시작함
     * 나는 주요 프로젝트에서 거의 2년 째 wgpu를 사용 중임
       이번 WebGPU 확장으로 인해 유지보수자가 늘어나서
       18달 전에 올려둔 이슈들이 더 빠르게 해결되길 기대함
       아직 Rust는 손대지 않았지만 언젠가 직접 기여할 동기와 시간이 생기길 바람
       wgpu-native 바인딩에 의존하고 있는데 업데이트가 느리게 도달함
       예를 들어 지난 주에 드디어 v25로 갔는데 불과 며칠 전에 v26이 나온 상황임
"
"https://news.hada.io/topic?id=22013","컨텍스트 변질: 입력 토큰이 많아질수록 LLM 성능이 어떻게 변하는가","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 컨텍스트 변질: 입력 토큰이 많아질수록 LLM 성능이 어떻게 변하는가

     * 최신 LLM의 입력 토큰 한도(컨텍스트 윈도우) 가 수백만 단위까지 확장되었으나, 단순 검색 벤치마크(Needle in a Haystack, NIAH)에서 높은 점수를 받아도 실제 긴 입력에서의 성능 저하는 명확히 존재함
     * 연구진은 18개 모델을 대상으로 다양한 실험을 수행, 입력 길이 증가만을 통제한 상태에서도 성능 저하와 비일관적 패턴이 반복적으로 확인됨
     * 질문-정답 유사도 하락, 방해문(디스트랙터) 추가, 지문 구조의 변화에 따라 성능 하락 속도가 가속되거나 예측 불가능하게 바뀌는 현상이 두드러짐
     * 구조적 맥락(논리적 문단 흐름) 유지가 오히려 성능에 부정적 영향을 주는 등, 입력의 배열과 방식이 LLM 성능에 큰 영향을 미침
     * 단순 반복 텍스트 복사처럼 매우 쉬운 작업조차 입력 길이가 늘어날수록 일관성 있는 결과를 내지 못하는 한계가 드러나, 실제 적용 시 맥락 설계(컨텍스트 엔지니어링)의 중요성이 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

연구 배경과 목적

     * 최근 LLM의 컨텍스트 윈도우가 100만~1000만 토큰까지 늘어나면서, 긴 입력에도 “성능이 보장된다”는 인식이 확산됨
          + Gemini 1.5 Pro, GPT-4.1, Llama 4 등이 대표적임
     * 대표 벤치마크인 Needle in a Haystack(NIAH) 은 단순 문장 검색에 불과해, 실제 장문 문서 요약·질의응답 등 복합적 과제에서의 성능 저하를 제대로 반영하지 못함
     * 본 연구는 입력 길이만 조절하고, 과제 난이도는 고정하는 방식으로 성능 변화를 체계적으로 검증함

주요 실험 및 결과

     * 18개 최신 LLM(Anthropic Claude, OpenAI GPT-4.1/4o/3.5, Gemini, Qwen 등) 을 대상으로 총 4가지 실험 설계:
          + 질문-정답(Needle-Question) 의미 유사도 변화
          + 방해문(디스트랙터) 추가
          + 지문(헤이스택) 주제/구조 변화
          + 반복 단어 복사(출력 길이와 입력 길이 동시 확장)
     * 모든 실험에서 입력 길이가 길어질수록 성능이 급격히 저하되며, 특히 의미 유사도가 낮거나 방해문이 많을수록 하락폭이 커짐
     * 질문-정답 유사도가 낮을수록 긴 입력에서 오답 비율이 급상승함
     * 방해문이 하나만 추가돼도 정답률이 즉시 떨어지고, 4개 이상 추가하면 모델별로 혼동·환각(hallucination) 현상이 크게 증가함
          + 예시: Claude 계열은 오답 대신 “정답을 찾을 수 없음”이라고 회피하는 경향이 강하며, GPT 계열은 확신에 찬 오답을 더 많이 생성함
     * 지문 구조(논리 흐름/무작위 배열) 에 따라 성능이 반전되는 특이 현상도 관찰됨
          + 논리적 흐름을 지키는 원본(Original) 지문에서는 오히려 모델 성능이 더 나빠짐
          + 문장이 무작위로 섞인(Shuffled) 지문에서는 오히려 검색 성능이 더 높아짐
     * 반복 단어 복사 실험에서도 입력·출력 토큰이 늘어날수록 오답률·작업 거부·임의 단어 생성 등 예측 불가능한 패턴이 증가함
          + 예시: 2,500~5,000단어 이후 특정 모델에서 복사 거부, 임의 텍스트 생성 등 비정상 결과 급증

LongMemEval: 실전형 장기 대화 평가

     * 실제 대화 기록이 포함된 LongMemEval 벤치마크에서 집중 입력(정답과 관련된 부분만 포함)과 전체 입력(정답과 무관한 맥락까지 포함) 을 비교
     * 모든 모델에서 집중 입력이 훨씬 더 높은 정답률을 보였으며, 전체 입력에서는 관련 내용 찾기 자체가 추가 과제로 작용해 성능이 크게 저하됨
     * Claude 계열 모델은 특히 모호한 상황에서 “정답 없음”으로 회피하는 경향이 뚜렷함

추가 분석 및 시사점

     * 방해문별 혼동률, 답변 위치 정확도, 임의 단어 생성 위치 등 모델별 내부 동작 패턴 차이를 다양한 그래프로 정밀 분석함
     * 반복 단어 복사 실험에서, 정답 단어가 앞쪽에 위치할수록 정확도 높음 등 위치 의존적 특성이 있음
     * 컨텍스트 설계(정보 배열, 논리적 흐름 관리 등) 가 모델 성능에 미치는 영향이 매우 크므로, 실제 서비스 적용 시 단순 컨텍스트 확장만으로 일관된 성능을 기대할 수 없음을 시사함

결론

     * LLM의 장문 입력 처리 능력은 벤치마크 점수로 보장되지 않으며, 실제 입력 길이 증가만으로도 비일관적 성능 저하가 나타남
     * 관련 정보의 단순 포함만으로는 충분치 않으며, 정보의 배열·구조·방해문·유사도 등이 모두 성능에 결정적 영향을 줌
     * LLM 활용 시 장문 컨텍스트 관리와 설계(컨텍스트 엔지니어링) 가 반드시 필요함

   2.5 나온지도 한참은 됐는데 1.5를 왜

        Hacker News 의견

     * 나 역시 이와 비슷한 경험을 했음. 특히 Gemini Pro를 사용할 때 긴 텍스트 레퍼런스를 제공하면, 여러 문서를 한 번에 컨텍스트 윈도우에 넣는 것보다 먼저 각 문서를 요약해서 질문을 한 다음, 필요하면 세부 문서 전체를 RAG 스타일 또는 간단한 에이전트 루프로 제공하는 것이 훨씬 더 좋은 답변을 얻을 수 있었음. 비슷하게 Claude Code를 Opus나 Sonnet과 함께 쓸 때도, 컴팩션(compaction)이 많이 일어날수록 결과가 나빠지는 걸 직접 경험함. 요약의 질이 떨어져서 그런 건지, 아니면 컨텍스트 윈도우 내에 덜 관련된 데이터 비중이 높아져서 그런 건지는 확실하지 않지만, 컨텍스트를 비우고 관련 파일만 다시 읽으라고 하면 (이전 요약에서 이미 언급됐다 해도) 훨씬 더 결과가 괜찮았음
          + Gemini는 챗 컨텍스트 한계에 이르기 전에 이미 일관성과 추론력에서 무너지기 시작함. 그런데도 이 보고서에 따르면 여러 면에서 최고 모델임. 결론은 컨텍스트 엔지니어링이 여전히 중요하고, RAG 접근법도 여전히 유효함
          + ""컴팩션""은 결국 트랜스크립트를 요약으로 단축하는 것 아닌가? 그렇다면 정보가 실제로 손실되기 때문에 성능이 나빠지는 건 당연함. 하지만 이건 컨텍스트 로트(context rot) 때문이 아님. 진짜 컨텍스트 로트 신호는 오토-컴팩트(자동 축약) 임계점에 다가갈 때 나타남. 내가 제대로 이해하고 있는 것 같음?
          + 최적의 코딩 에이전트라면 이런 과정을 자동으로 해줄 것 같음. 필요한 코드, MCP 응답, 레포 맵 등을 수집해서 가끔 요약하고, 모두를 새로운 챗 메시지로 합쳐서 정말 필요한 부분만 남겨놓는 식임. 난 aider라는 도구로 이미 이런 스타일을 쓰고 있는데, 많은 컨텍스트가 필요한 상황에선 에이전틱하거나 자동화된 워크플로보다 훨씬 성능이 좋았음. 단, 손이 많이 감
          + NotebookLM을 써봤는지? 이 앱은 백그라운드에서 문서를 쪼개고 요약해주며, RAG를 통해 전체 문서에 챗 형태로 질문할 수 있음
     * Cursor에서 새로운 기능이나 코드 변경에 대해 오랜 시간 대화할수록 결과물이 점점 안 좋아지는 것을 경험함. 가장 좋은 결과는 명확하고 구체적인 지침과 계획을 먼저 세우고, 수정할 파일들을 컨텍스트 프롬프트에 직접 끌어다 놓았을 때 나왔음
          + ""Explore → plan → code → test → commit"" 흐름으로 진행하는 게 훨씬 더 도움이 되었음. 필요하다면 각 단계마다 컨텍스트를 클리어해서 효과를 높일 수 있음
          + 특정 작업을 위한 충분한 정보가 모이면, 그때 컨텍스트를 저장함. 품질이 떨어지는 게 보이면, 지금까지의 작업을 다시 요약해서 이전 체크포인트에 덧붙임
     * 이 현상은 잘 알려져 있지만 제대로 문서화된 곳이 별로 없어서 이번 글이 매우 반가움. 벤치마크로 쉽게 측정하기 힘들 정도로 실질적 영향이 더 크다고 생각함. 진짜 쓸모 있는 LLM 기반 앱들은 모델이 해낼 수 있는 한계점에 머물러 있음. 즉, 실제 질문이나 작업에서 논리적으로 몇 번씩 ""점프""해야 하는 컨텍스트를 따라가야 할 때 의미가 큼. 여러 번의 논리적 ""점프""가 필요할수록 컨텍스트 로트 문제가 폭발적으로 심해진다고 생각함. 각 점프마다 주의해야 할 대상이 늘어나기 때문임
     * 컨텍스트를 손쉽게 잘라낼 방법이 꼭 필요함. 모델과 전체 대화를 내가 직접 관리할 수 있으면, 대략 20만 토큰짜리 코딩 세션에서 훨씬 더 많은 퍼포먼스를 뽑아낼 수 있을 것 같음. 그런데 현실은 좋은 인스턴스를 쓰더라도 2만 토큰쯤 지나면 모델이 자꾸 이상해지고, 세션도 완전히 로트됨. 그냥 이 부분을 잘라내버릴 수 있으면 좋겠음
          + 로컬 LLM은 원하는 대로 컨텍스트를 편집할 수 있어서, LLM의 응답 자체를 수정해 두면 나중에 모델이 자신이 원래 말한 것처럼 착각할 수도 있음. 그래서 원하는 방향으로 유도하는 데 유리함. 반면 LLM-as-a-service 모델은 이런 기능을 제공하지 않는데, 검열 우회를 쉽게 만들 수 있기 때문임
          + ""헤이 Claude, 이제 컨텍스트를 리셋할 건데 앞으로 계속 작업할 수 있도록 프롬프트 하나 만들어줘""라고 요청한 뒤, Claude가 제안한 프롬프트를 미리 살펴보고 다듬어서 다시 넣는 실험을 해봤음
          + 이전 체크포인트로 쉽게 롤백할 수 있으면 정말 최고의 기능일 것 같음
          + 대부분의 CLI 에이전트에서는 /compress 명령어로 이런 작업을 할 수 있음
     * Claude code를 쓰다 보면 점점 자신의 실수와 내 지시를 구분하지 못함. 헷갈리기 시작하면, 세션을 새로 여는 게 답임. 세션이 길어질수록 같은 루프를 돌거나 스스로 테스트가 이미 깨졌다고 우겨서 무시해버림(실제로는 이 세션에서 깨졌는데도). 내가 프롬프트를 잘못 넣은 탓이겠지만, 요즘 Claude가 어느샌가 30 IQ 정도 떨어진 것 같음
          + 나도 똑같이 느낌. 맥스 플랜인데도 성능 좋을 때와 안 좋을 때가 확실히 있음
          + ""내 프롬프트와 컨텍스트가 문제겠지""라고 생각하는 거, 우리 모두 내면화된 가스라이팅 느낌임
     * 이건 정보 검색 문제의 한 유형이지만, 컨텍스트 길이 변화에 의한 성능 저하는 단순 검색 답변과 다르게 작동할 수 있다고 생각함. 예를 들어 “이 버튼을 빨갛게 수정하려면?” 같은 질문이나 “위의 문장들은 어떤 카테고리에 속하는가?” 같은 문제에서는 다르게 동작할 수 있음. 예전에 인상적이었던 논문으로 Many-Shot In-Context Learning 있었음. 이 연구에선 예시로 컨텍스트를 채울수록 성능이 크게 뛰는 현상을 보임. 결국 문제마다 실제로 직접 테스트해봐야 LLM이 컨텍스트 내용과 길이에 따라 어떻게 바뀌는지 알 수 있음. 항상 컨텍스트 길이가 길어지면 성능이 낮아진다고 가정하면 안 됨
          + 내 직관상 추론이 필요한 질문은 단순 검색 질문보다 예외 없이 항상 성능이 낮음. 특히 부정형 질문이나 덜 관련된 정보가 포함되어 있을 땐 더함. 그래도 직관이 데이터는 아니라는 점, 실제 숫자가 궁금함. In-context learning 현상은 장문 컨텍스트로 인한 성능 저하와 별개임. 두 현상은 동시에 존재함. 'lost-in-the-middle' 문제처럼, 예시 위치에 따라 성능이 달라질 수 있음
     * 내용이 매우 쿨하고 통찰 많은 기사임. 다만 미디어 리터러시 관점에서 Chroma는 벡터DB 회사라는 점을 참고하면 좋겠음
          + Chroma는 벡터, 전체 텍스트, 정규식 검색까지 모두 지원함. 그리고 AI 애플리케이션에서 많이 쓰는 멀티테넌트 워크로드 환경에 최적임. 단순 벡터DB만 하는 회사가 아님
     * 최근 Gemini 2.5 Flash로 장편 소설을 여럿 썼는데, 컨텍스트 로트는 분명히 느껴지지만 이 기사에서 제시한 것보다 훨씬 더 나중에 나타남. 내 경우 5만~10만 토큰 정도 지나야 초반 컨텍스트(예: 출력 언어 등)를 무시하기 시작했음. 아마 창작 같은 복잡한 태스크는 효과 측정이 더 어렵거나 덜 뚜렷할 수도 있음. 어쨌거나, 가끔 빠진 컨텍스트만 다시 넣어주면 쓸 만한 수준은 유지함
          + 소설 얘기 좀 더 듣고 싶음. 재밌게 잘 나왔는지, 출간 계획은 있는지 궁금함
     * 나도 비슷하게 경험함. 프로젝트 중 영상 트랜스크립트 검색기능을 개발하는데, 텍스트 길이가 아주 길었음. GPT 4.1 같은 모델이 컨텍스트 윈도우가 크니까 RAG가 필요 없을 줄 알았는데, 특히 작은 모델에서 이상한 현상이 자주 발생했음. 질문에 제대로 답변하지 않고, 그냥 콘텐츠 전체 요약만 하는 경우가 있었음
     * 흥미로운 리포트임. 모델별로 추천하는 컨텍스트 크기 같은 게 있는지 궁금함. 내 유스케이스에 어느 선까지가 적절한지 알 수 있는 방법이 있는지 알고 싶음
"
"https://news.hada.io/topic?id=22063","신용카드 회사들이 밸브에 특정 성인 게임을 스팀에서 내리도록 압박한 사실 확인","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              신용카드 회사들이 밸브에 특정 성인 게임을 스팀에서 내리도록 압박한 사실 확인

     * 밸브(Valve) 는 최근 신용카드 결제사들의 압력으로 인해 일부 성인 게임들을 스팀에서 삭제했음
     * 결제사 규정 위반 게임들이 판매 중단 대상이 되었으며, 개발자들에게는 향후 스팀에 다른 게임을 등록할 기회가 제공됨
     * 밸브는 결제 수단 상실로 인한 전체 플랫폼 사용자 영향을 방지하기 위한 불가피한 선택이었음을 설명함
     * 구체적으로 어떤 게임이 삭제됐는지는 공개하지 않았지만, 일련의 가족을 소재로 한 성인 게임들이 사라진 것과 관련이 높음
     * 실사 음란물은 스팀에서 허용되지 않으며, 이 같은 조치가 게임 유통 업계 전반에 부정적인 선례가 될 수 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

밸브, 성인 게임 스팀 삭제 배경 및 신용카드 결제사 압박

     * 최근 밸브가 PC Gamer에 전달한 공식 입장을 통해, 신용카드 결제사들의 요청에 따라 일련의 성인 게임들이 스팀에서 삭제되었음
     * 밸브는 “특정 게임들이 당사의 결제 처리 및 카드 네트워크, 은행의 규정에 위배된다는 통보를 받았음”이라고 언급하며, 이에 따라 해당 게임들의 판매를 중단한다고 설명함

개발자 지원과 결제사 규제 영향

     * 밸브는 이번 조치로 영향을 받은 개발자들에게 향후 스팀에 심사 조건만 충족하면 게임을 다시 등록할 수 있는 앱 크레딧을 지급하고 있음
     * 다만, 등록 가능한 게임 역시 결제사들의 기준을 만족해야 하므로 심사 기준 충족이 필수임

결제 시스템의 영향력과 밸브의 대응

     * 밸브는 게임 삭제 조치가 “결제 방식 상실로 인해 전체 스팀 고객들이 다른 게임 및 콘텐츠를 구매하지 못하는 상황을 막기 위한 것”임을 분명히 했음
     * 이런 입장은 결제 처리사와의 거래에서 밸브도 제한을 받을 수밖에 없다는 현실을 보여줌
     * 비자(Visa)와 마스터카드(Mastercard) 같은 대형 결제사는 디지털 유통업체에 막대한 영향력을 행사함

구체적인 삭제 게임 및 별도 이슈

     * 밸브는 이번 규제로 인해 어떤 게임들이 삭제됐는지는 공식 공개하지 않았으나, 최근 가족을 소재로 한 일부 성인 게임들이 사라진 점과 시기가 맞물려 있음
     * 동시에 삭제된 중국산 게임 ‘Trials of Innocence’ 사례의 경우, 결제사 규제가 아닌 DMCA 이의제기에 따른 임시 삭제임이 개발자를 통해 확인되었음

결제사와 온라인 성인 콘텐츠

     * 온라인에서 성인 콘텐츠 관련, 결제사들은 사기, 결제 취소, 비동의/미성년 영상 등에 매우 민감하게 반응하는 상황임
     * 과거 PornHub에서 문제성 콘텐츠 논란 이후 주요 결제사들은 해당 서비스 결제 지원을 중단했으며, 이런 영향은 지금도 이어지고 있음
     * 스팀은 실제 인물의 노골적인 콘텐츠는 원천적으로 금지하고 있지만, 애니메이션 성인 콘텐츠조차도 결제사들은 규제 대상으로 보고 있음

업계에 미치는 함의와 우려

     * 결제사들이 유통 플랫폼의 콘텐츠 범위를 결정하게 되는 현상에 대해 부정적 선례라는 지적이 있음
     * 사용자 입장에서는 스팀에서 어떤 콘텐츠를 구매할 수 있는가에 대한 결정권이 대형 결제사에 지나치게 종속되는 구조에 대한 우려가 커짐

        Hacker News 의견

     * 우린 Postal이나 Soldier of Fortune 같은 게임에서 온갖 난장판과 살인 행위가 중심 내용이어도 문제 삼지 않음이 우스꽝스럽다고 생각함, 하지만 인간의 감각적인 몸을 보여주려고 하면 모두 과도하다고 함
          + 영화도 마찬가지임, 시체 더미는 어린이도 봐도 되지만, 알몸은 엄청 위험하다고 보는 인식임
          + 성인 컨텐츠는 높은 위험의 상점 카테고리로 간주됨, 성인물을 소비하고 나면 일종의 '정신적 맑아짐'이 찾아와서 사용자들이 차지백을 요청하는 일이 많음, 이는 도덕 규범이 아니라 실질적 사기와 차지백 위험 때문임
          + 왜 결제사들이 이런 컨텐츠 제공업체들에게 압박을 가하냐고 물어볼 수 있음, 미국 비즈니스 문화엔 이상한 청교도적 느낌이 없고, 단지 이 거래에 관여된 사람들이 완전히 도덕적 잣대가 없다고 보면 설명이 끝남
          + 나는 이게 허수아비 논법이라고 봄, 잔혹한 고어물에 집착해서 유료로 보는 사람들은 포르노나 OnlyFans를 보는 사람의 수보다 훨씬 적고, 성인 컨텐츠는 그만큼 더 높은 리스크와 차지백이 많음
     * 왜 결제사들이 이런 행동을 하는지 의아함, 규제 때문인지, 사기 방지 때문이라는 건 이해하지만 사기가 많은 업종에는 수수료만 더 매기면 될 것 같음, 게임의 컨텐츠 자체에 왜 신경 쓰는지 이해하기 힘듦, 미국에서 불법도 아닌데다 테러 자금 운운할 일도 없어 보임
          + 도덕단체들이 이런 컨텐츠를 못 보게 결제사들 압박하는 캠페인이 주요 원인임, NCOSE 같은 단체가 오랜 기간 카드사들을 타겟으로 삼았고 효과를 거둠 EFF 기사, 뉴스위크, 학술자료
          + 단순히 차지백 리스크 때문이라고 생각함, 카지노와 성인 사이트가 카드 결제가 힘든 것도 같은 맥락임, 카드명세서에 XXX 게임이 찍히면 차지백하기도 함, Valve도 이런 리스크 동반하면 전체 수수료가 올라가니 부담임, 이런 리스크를 감수하며 어덜트 게임 특화 마켓을 만들 기회도 있음
          + 과거엔 Operation Choke Point라는 정부 정책도 영향이 있었음 Operation Choke Point 위키, 하지만 지금은 정부 개입이 덜함
          + Hot Money 팟캐스트에서 들은 바로는 국가마다 성인물, 동성애, 나이 관련 법이 달라서 카드사에겐 어디까지가 합법인지 자체가 애매해서 위험하다고 느낌
          + 미국은 소송사회라 규칙이 입법보단 판례로 결정되고, 누가 마음만 먹으면 소송이 가능함, 성인물 지지 않는 사람이 결제사를 소송으로 괴롭혀 충분히 사업을 접게 만들 수 있음, 그래서 카드사들이 그런 비즈니스 자체를 포기하는 경향 있음
     * 컨텐츠 내용을 떠나서, 어째서 카드사들이 우리가 돈 쓰는 방식을 좌지우지하게 허용하는 현실이 이해가 안 됨, 사기와 남용 때문이면 카드에 현금을 충전해 쓰고 도난 시 내 손실로 하면 될 일이니 Mastercard가 뭘 보고 판단할 자격이 없음, 수도 회사 임원이 뭘 생각하는지 신경 안 쓰듯이 카드사에 신경 쓸 이유가 없음
          + Visa가 불법 성인물 결제 책임으로 소송에서 패소하고 전체 영역을 사실상 회피함, Economist에서도 정부가 명확한 법을 안 만들어주니 이런 애매한 상황이 온 것이라고 함, 결제업체가 표현의 자유까지 결정하게 되어 결국 소송 한 방에 블랙리스트에 오를 수도 있음
          + 카드사들이 사기 거래에 대해 보상 책임이 있기 때문임, 그래서 사기가 많은 상점은 블랙리스트에 오름
          + 미국 법상 사기 책임이 카드사에 거의 전가되어 있음, 만약 유저 책임 결제 시스템(BTC 등)만 쓴다면 '높은 사기 리스크'만큼만 걸러낼 수 있을 것임, 그러나 실제로는 사기 리스크랑 상관없이 정부가 간접적으로 규제해 불법을 억누름, 이 규제가 btc같은 유저 책임형 결제에도 똑같이 가해짐
     * Steam이 돈을 엄청 벌고 있으니 Valve가 ""우리만의 카드 만들고, Half-Life 3는 ValveCard로만 팜"" 같은 일을 할 수 있을까라는 생각이 듦
          + 실질적으로는 불가능함, Visa/Mastercard를 대체하려면 수천 은행이 ValveCard를 받아줘야 하고, 유저들이 게임 하나 사려고 외국 은행을 쓰지 않을 것임, 오히려 불편하면 유저들은 그냥 해적판을 쓸 것임, Gabe의 ""해적판은 서비스 문제""란 명언도 있음, 즉 유저들이 구매하기 어렵게 만들면 그냥 불법 다운로드로 넘어간다는 사실을 가장 체감한 사례임
          + 사실상 기프트카드가 그런 역할을 하고 있음, Steam 수익 자료처럼 Valve가 어마어마하게 돈을 벌고 있음을 참고로 언급함
          + Valve가 Visa를 위협할 수 있을지 궁금함, 그 정도는 아닌 것 같음, Amazon, Walmart, Target 정도가 그나마 경쟁 가능한 주체로 보임
          + ""아마도 Valve라면 할 순 있겠지만 재미없는 일임""이라고 느끼는 입장임
          + 당장엔 힘들어 보여도 옛날에 Valve가 Linux에 게임 플랫폼을 만든다고 했을 때도 불가능해보였음, 이런 투자가 실제 수익보다도 협상 카드를 쥐고 있기 위한 안전장치로 가치가 더 크다고 봄, Valve가 성인 게임 매출로 손해 볼 정도면 뭔가 다른 보험적 수단을 고민할 수도 있을 것임
     * ACLU가 FTC에 카드사들이 싫어하는 컨텐츠와 회사들에 그 제거를 강요하는 방식을 정리한 문건이 있음 ACLU 문건
     * 이번에 퇴출된 게임들은 근친 소재를 떠나서도 퀄리티가 너무 떨어져 게임이라 부르기도 민망한 수준이 많았음, Valve가 결제사가 나서기 전 자체적으로 정리했어야 하는 작품임, 이런 게임을 옹호하는 건 '블루 이즈 더 워머스트 컬러'와 아무 포르노 비디오를 둘 다 성행위가 나온다고 같은 선상에 놓는 것과 같음, 만약 Baldur's Gate 3 같은 게임이 금지된다면 그때가서야 진짜 문제임
     * 관련 청원과 서명자들 링크가 있음 청원 링크, 관련 기사
          + 이를 반대하는 청원(리버스 청원)은 어디 있냐고 궁금해하는 의견
     * 기사에서 ""특정 성인 게임""이라고 모호하게 표기했는데, 실제로는 근친을 소재로 한 게임이 대다수였다는 점이 흥미로움 SteamDB 소스, 그리고 이것이 앞으로 더 온건한 컨텐츠까지 삭제하는 빌미가 될지도 궁금함
          + 실제로 해당 리스트를 직접 보면 괜히 봤다고 생각할 수준임, 아마도 기사가 ""특정 성인 게임""으로 뭉뚱그린 것은 논란을 키우기 위한 의도로 보임, 제목이 ""Valve, 근친 게임 압박에 삭제""였으면 덜 시끄러웠을 것임
          + Collective Shout 같은 단체는 이전에도 Detroit Becomes Human 같은 메이저 게임을 별 근거 없이 공격한 적 있음, 이번 사례를 빌미 삼아 더 많은 검열을 시도할 거라 확신함
          + 구체적으로 근친, 강간, 아동 학대를 소재로 한 게임들이 대상임
          + 네코파라나 Sabbat of the Witch는 일단 안전한 듯함, 하지만 Making Lovers라는 게임의 여동생 루트를 그들이 모르길 바람
     * 2025년에 이런 말을 반복해야 하는 게 안타까움, '허구는 실제가 아니다'라는 기본 전제를 환기해야 하는 현실임, 시뮬레이션 폭력은 실제 폭력이 아니고, 시뮬레이션 섹스도 해당되며, 마법도 마찬가지임
          + 폭력은 아직도 게임에서 괜찮게 여겨지는 경향임, 아마 가족들이랑 같이 보기에 성적 장면이 더 난감해서 그렇다고 생각함
          + 하지만 시뮬레이션이 너무 극단적인 수준에 이르면 나는 금지될 수도 있다고 봄, 어떤 것들은 사회적으로 정상화돼선 안 된다고 생각함, 다만 이런 결정을 결제사가 일방적으로 하는 건 문제임, 그러나 이런 규제 결과에 아주 반대하는 것은 아님
          + 이건 미끄러운 경사 논리임, 허구라고 해도 현실을 모방해서 실제 범죄에 도움 줄 만큼 리얼한 상황을 연습시켜 줄 수도 있음
     * Valve가 신용카드 결제를 PG게임으로 제한하고, 다른 게임은 암호화폐로 파는 구조라면 결제사들의 차지백 리스크 혹은 성인물 연관 우려가 사라질 것임, 결제사들은 이걸 달가워하진 않겠지만, 그렇게까지 요구하는 건 결국 전면적인 편집권을 주장하는 것이니 우리는 저항해야 한다고 생각함
"
"https://news.hada.io/topic?id=22064","AI 자본지출이 너무 커서 경제 통계까지 뒤흔들고 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     AI 자본지출이 너무 커서 경제 통계까지 뒤흔들고 있음

     * AI 데이터센터 자본지출이 미국 및 글로벌 경제에서 전례 없이 큰 비중을 차지하게 되었음
     * 미국 2025년 AI 관련 데이터센터 투자가 GDP의 약 2% 에 달하며, 이는 0.7%의 GDP 성장 기여로 추정됨
     * 이 거대한 투자금은 기존 제조업, 인프라, 기타 벤처 투자에서 빠져나와 AI 중심으로 집중되고 있음
     * 이러한 현상은 철도, 통신 인프라 투자 붐에 비견될 정도로 빠르게 확산 중이며, 이미 과거 통신 인프라 투자 피크를 넘어섬
     * 결과적으로 AI 데이터센터 투자가 경기 하강을 완화하는 동시에, 다른 산업의 자금 고갈 및 대규모 구조조정, 고용 감소 현상까지 유발하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Updates & Erasures

     * 미국 연준 의사당 리노베이션 관련 논란이 지속되는 가운데, 공공 지출에 대한 비판이 이어짐
     * 최근 기사에서 중앙은행 건물 리노베이션을 두고 정부 관계자들이 불만을 표시하고 있음
     * Powell 연준 의장의 리더십과 건물 리노베이션 사이에서 풍자와 불만이 오가는 상황임

Honey, AI Capex Ate the Economy

  AI 데이터센터 자본지출 현황

     * AI 데이터센터 투자 규모가 너무 커서, 중국의 시진핑조차 각 지방 정부에 AI, 컴퓨팅, 신에너지 산업 투자를 경계하라는 경고를 했음
     * 중국 내 데이터센터 신설 건수만 250개가 넘으며, 전 세계적으로 AI 인프라에 대한 투자 열기가 확산 중임
     * 미국의 경우 Nvidia 데이터센터 매출을 근거로 추정 시, 2025년 AI 자본지출(Capex) 이 미국 GDP의 2% 수준, AI로 인한 GDP 성장 기여도는 0.7% 에 이를 전망임

    AI 투자 규모의 하한선 검토

     * 2025년 예상 미국 GDP는 $25T(3경 4000조원) 규모로 추산됨
     * Nvidia의 데이터센터 대상 연간 매출: 약 $156.4B(216조원), 이 중 99%가 AI 관련으로 집계됨
     * Nvidia가 전체 데이터센터 자본지출에서 차지하는 비율: 25~35%
     * 경제적 파급력(멀티플라이어): 1.5~2배 적용 시, 연간 전체 데이터센터 capex는 약 $520B로 확대 추정됨
     * 2022년 이전 AI capex는 GDP의 0.1% 미만이었으나 3년 만에 10배 이상 성장함
     * 철도, 텔레콤 등 과거 대규모 인프라 투자와 비교해도 그 규모가 크게 증가 중임
     * 특히 닷컴버블 당시 텔레콤 투자 피크를 이미 초과했으며 계속 상승 추세임
     * 데이터센터 투자액은 19세기 철도 전성기 대비 20% 수준이지만 짧은 기간에 급증함

  AI 자본지출은 어디서 오는가

     * 데이터센터 및 AI 인프라 투자금의 출처는 다음과 같음
          + 내부 현금흐름 (Microsoft, Google, Amazon, Meta 등 주요 테크 기업)
          + 부채 발행 (채권 등, 비중 증가 추세)
          + 주식 및 추가공모
          + 벤처캐피털/사모펀드 (CoreWeave, Lambda 등 AI 인프라 스타트업)
          + SPV(특수목적법인), 리스, 자산담보형 대체금융 (Meta 등)
          + 클라우드 사용량 약정 (주로 하이퍼스케일러 기업)
     * AI 중심 투자로 인해 타 산업의 자금 유입이 위축되고 있음
          + 벤처캐피털 자금이 AI 외 영역엔 거의 흐르지 않는 현상
          + 비생명과학 VC는 사실상 현재 오직 AI 투자를 중심으로만 운용함
          + 클라우드 컴퓨트 기업은 기존 클라우드 사업 대신 GPU 중심 IDC에 투자를 집중함
          + Amazon, Microsoft 등 주요 기업에서 AI 데이터센터 비용 증가로 인건비 및 사업 구조조정이 나타남
          + AI 관련 상장기업의 주가수익비율은 급등했으나, 다른 분야의 기업들은 자본조달이 어려워짐
          + AI 기업에 투자금이 몰리며, 제조업/기타 인프라는 상대적으로 자금 부족

  AI 투자로 인한 경제 구조 변화

     * AI 투자 붐은 다른 인프라 부문 투자 위축과 산업 구조 재편을 촉진하고 있음
     * 과거 통신 인프라 버블이 다른 인프라 투자 급감으로 이어진 선례와 유사함
     * 이번 AI 데이터센터 투자 열풍도 비AI 분야 자본 고갈, 대규모 구조조정, 일자리 감소 등 부정적 파급효과가 우려됨

  경제적 미스터리 해소

     * 최근 경제에서 무역분쟁, 정치 불확실성, 관료 리스크 등 불안 요인에도 불구하고 경기 침체 우려가 상대적으로 적다는 점이 수수께끼였음
     * 이유는 민간 부문 주도의 AI 데이터센터 투자라는 대규모 ""사설 경기부양책"" 이 진행 중이기 때문임
     * 이미 dot-com 버블 시기 텔레콤 투자 피크를 초월했으며 19세기 철도 투자 피크에 근접함
     * 역산 시, AI 데이터센터 투자가 없었다면 2025년 1분기 미국 GDP 성장률은 –2.1%까지 하락했을 가능성이 높음
     * 결과적으로, AI 자본지출은 경기 하락을 상쇄하며, 실제로는 경제적 취약성을 은폐하는 역할을 함

결론

     * 단기간 내 AI 및 데이터센터에 대한 투자 폭증은 경제사적으로도 드문 이례적 순간임
     * AI 및 데이터센터의 폭발적 성장에 대해 찬반과는 별개로, 급격한 기술발전과 자본투입의 속도는 비정상적으로 큼
     * AI 데이터센터는 철도나 도로처럼 수십~수백년 쓰이는 인프라가 아니라, 짧은 수명과 빠른 감가상각이 특징
     * 이처럼 단기 기술 사이클에 맞춰 대규모 투자가 이뤄지면서, 기타 산업 투자 위축, 대규모 해고, 비AI 분야의 성장 둔화가 병행되고 있음
     * 자본은 현재 벤처와 내부예산 등에서 IT 중심으로 빠르게 재할당되는 중이며, 결과적으로 일부 분야는 장기적인 투자고갈과 대규모 구조조정의 영향을 받고 있음
     * 아직 AI가 실질적으로 널리 활용되기 전임에도, 일자리 감소 및 산업 재편이 빠르게 진행되는 아이러니한 상황임

Rougher Notes

     * University of California가 헤지펀드 투자를 전면 철회하며 강도 높은 비판을 제기함
     * Jane Street의 인도 트레이딩 조사 진행 중임
     * 나치 장교가 약탈했던 모자이크 작품이 폼페이로 반환됨
     * 에리트리톨 감미료가 뇌세포에 영향을 미치고 뇌졸중 위험을 높일 수 있음
     * 문화 비평가가 소멸 위기에 처할 수 있음
     * 인간 두뇌 크기 증가가 초기 인류 멸종 위험에 영향을 미쳤는지 연구됨
     * 글로벌 경제의 회복력이 점차 약화되고 있다는 우려가 제기됨
     * KKR의 2025년 중간 전망 발표, ""운은 스스로 만들어야 한다""는 메시지 강조
     * 로봇 대사: 다른 기계를 소비하며 성장하는 기계 연구
     * 미국의 웨이 프로틴 열풍으로 유제품 산업 변화
     * Waymo 출신 엔지니어들이 건설 자동화 로봇 스타트업을 설립함
     * 중국 제약사들이 미국 빅파마를 빠르게 추격 중임

   댓글 요약 보니 지금 2%는 많지 않다고 생각하는 거 같은데, 이 속도로 증가하면 26년에 얼마가 될지를 생각해야 하는거 같습니다. AGI가 초근미래에 가능한게 아니라면, 26년, 어쩌면 27년까지도 낙관론자와 비관론자의 대립으로 정말 혼란스러울거 같아요.

        Hacker News 의견

     * 시진핑의 발언은 FT의 과장된 기사에서 나온 것으로 보임. 원문 중국어 기사는 훨씬 더 온화한 어조임. AI와 EV는 회의나 보고서에서 주요 주제가 아니라 언급만 됐을 뿐임. 시진핑의 경고는 AI와 EV 산업에서 또다시 관찰되는 “정치적 업적 경쟁”에 대한 것임. 중앙정부가 산업정책 목표를 정하면 지방정부가 기업들과 결탁해 보여주기식 “프로젝트”를 추진하고, 결국 대부분은 공장만 짓고 멈추는 전형적인 일들임. 이는 예전부터 중앙정부에 큰 골칫거리였고, AI·EV 분야에도 같은 문제가 있다는 게 시진핑의 핵심 경고 내용임. 원문 기사: https://paper.people.com.cn/rmrb/pc/…
     * GDP의 1.2%는 그렇게 극단적으로 느껴지지 않음. 다른 혁신적인 프로젝트와 비교하면 훨씬 낮은 수준임. 예시로 아폴로 프로그램은 4%, 철도는 6%, 코로나 경기부양책은 27%, 2차 대전 국방 지출은 40%까지 갔던 경험이 있음
          + 나도 처음 반응은 비슷함. 1.2%면 별로 많아 보이지 않음. 그냥 언론이 자극적으로 헤드라인을 뽑는 듯함. 만약 물과 에너지 소비량 같은 걸 수치로 보면 더 걱정이 될지도 모르겠음. 다소 주제에서 벗어나지만 미국 GDP의 약 9%가 금융 서비스에서 발생하는데, 개인적으로 더 경계할 만한 수치라고 생각함
          + 전체 GDP 대비로 보면 적어 보이지만, 우리 GDP 자체가 워낙 크기 때문임. 이 1.2%라는 액수도 노르웨이의 GDP 전체만큼임. 별거 아닌 것처럼 보이지만, 올해 군사 지출인 3.4%와 비교해도 큰 금액임
          + 지금은 수치만 보지 말고 변화의 추이와 흐름, 그리고 그 기울기의 의미에 주목해야 한다고 생각함. 자본이 여러 분야에서 AI로 이동하고 있고, 자산 가치의 지속시간(철도는 수십년에서 수세기, AI는 몇 년이나 될지 등)도 다름. “AI 데이터센터 투자가 없었으면 1분기 GDP 하락 폭이 –2.1%까지 갔을 것”이라는 저자의 논점도 있음
          + 이제 겨우 시작된 지 2년밖에 안 됐음! 1.2%도 엄청난 수치임. 이런 비교가 가능한 것 자체가 놀라움
          + 기사 핵심은 이렇게 거대한 투자가 정당화될 수 있느냐는 질문임. 단순히 GDP의 몇 %니 괜찮다는 식의 반박은 본질을 피하는 셈임
     * 철도는 자본을 사회 전체로 분산시키고, 다수의 장기적 부의 증가로 이어졌음. 그러나 AI는 기존 부자에게 자본이 집중되고, 결국 중산층에게 장기적으로 부의 감소를 가져올 여지가 있음. 인구의 구매력이 줄면 경제 성장에 도움 되지 않아서 이런 AI 투자 붐에는 의문이 있음
          + “철도가 자본 분산인가?”라는 점에 의문이 있음. 과거 철도도 벤더빌트 같은 대형 독점이었음. 정부의 반독점 규제가 생길 정도로 가격 담합과 농민들의 운송비 인상이 만연했음. “AI가 자본 집중을 심화시킨다”는 점은 맞지만, 사실 모든 자본집약적 산업이 그러함. AI만 집어서 말할 이유는 없고, AI는 자연 독점도 아니며 경쟁도 가능함
     * AI와 데이터센터에 대해 밴드웨건이 끝나면 그 용량을 좀 더 유용한 곳(예: 신약 개발 등)으로 쓸 수 있으면 좋겠음
          + 1990년대 닷컴 붐 때에도 무분별하게 광섬유 네트워크를 과다하게 깔았고, 닷컴 버블 이후 이 자산들이 헐값에 처분되어, 새로운 스타트업들이 저가로 전국망을 구축함. 이러한 거품의 ‘유물’이 차세대 회사들의 저렴한 연료가 되어왔음. 데이터센터 역시 이 패턴을 따라 지금 당장 과잉이더라도 나중에는 반드시 새로운 용도로 쓰일 것임
          + 유행과 관련해서, 왜 그렇게 많은 사람이 개발자 및 기타 화이트칼라 일자리를 AI가 100% 대체한다는 것에 집착하는지 모르겠음. 이상하리만큼 종말적이고 허무주의적인 환상 같고, 나는 이런 과대광고에 동의하지 않음. 나만 그런 것인지 궁금함
          + 글로벌 기준으로 현재 LLM(대형언어모델) 역량조차 충분히 도입되지 않았음. 만약 지금 이 수준에서 더 지능적인 걸 못 만든다 해도 향후 수년간 다양한 업계에서 반복적인 작업 자동화가 계속될 가능성 있음
          + 이곳에서 AI와 그 멈출 수 없는 발전을 가볍게 보는 사람이 자꾸 나와서 놀라움. 체스, 바둑, 전략 게임, 단백질 구조 예측 등 이미 일어난 예시만 봐도, 형식화하고 검증 가능한 거의 모든 문제는 결국 AI가 풀 수 있음은 분명함. 분야별 특화 ASI(인공지능 초지능) 역시 시간 문제라고 생각함. 모두에게 The Bitter Lesson과 Verifier’s Law 읽어볼 것을 강력 추천함
          + 우리는 그럴 수 없음. 결국 기준에 맞춰 엔지니어를 구조조정하고 남는 설비 역시 없앨 것임
     * 새로운 데이터센터가 무조건 재생에너지로 지어지도록 강제로 했으면 좋겠음. 전체 비용 대비 추가비용도 그렇게 크지 않을 텐데, 이 정도 대기업은 충분히 감당 가능하다고 봄. 어쩌면 이런 정책이 차세대 소형모듈 원자로 발전기술의 진보를 이끌지도 모름
          + 많은 대기업이 이미 데이터센터 동력원으로 소형 원자력 기술에 관심을 갖고 있음. 가장 큰 문제는 이런 시설을 구동할 수 있는 전력망의 부지 선정임. 관련 업계 사람과 30분만 이야기해도 결국 핵심 주제는 원자력임. 풍부한 투자금이 쏟아지는 이 유행이 충분히 지속돼서 실제 필드에 우라늄 원자로가 깔리는 게 큰 긍정 효과가 될 것임. 철도, 광섬유처럼 남는 물리 인프라가 생길 수 있길 바람. 예전의 ‘도적 백만장자(robber barons)’들은 최소한 물리적 인프라라도 남겼으나, 최근의 붐은 거의 남는 게 없었음
          + 유럽에서는 이미 모든 신규 데이터센터가 재생에너지를 의무화하고 있음. 미국에서도 Google, Microsoft, Meta, AWS가 전 세계에서 가장 많은 재생에너지 구매 계약을 체결함. MS만 해도 약 200억 달러를 투자함. 미국은 수요 부족이 아니라 인허가·구획 문제 등으로 재생에너지 설치가 병목임. 전력망에 들어가길 기다리는 용량만 100GW로, 미국 전체 전력의 10% 규모임. 많이 주문한다고 오래 걸리는 병목 자체가 해결되진 않는 구조임. 예외적으로 xAI/Grok 같은 곳은 대형 클러스터를 100% 가스로 돌림. 전력도, 냉각화도 열악한 곳에서 트레일러 가스터빈 35대와 냉장 트럭 50여 대를 동원함. 효율도 낮고 환경 저해가 너무 크다고 생각하고, 이런 시스템은 불법화해야 된다고 봄
          + 미국에서는 강제하지 않아도 이미 시장이 변하는 중임. 2024년 신규 발전 설비의 94%, 2025년 93%가 재생에너지 또는 배터리 저장이고, 앞으로도 비슷한 추세임. 신규 화석연료 발전소는 천연가스만 조금 추가되고, 이마저도 옛 석탄 발전소를 전환하는 경우가 많음. 천연가스 신규 증설 계획도 셰일 붐 이후 최저임. 재생에너지가 이미 승리함
          + 데이터센터는 값비싼 자산이 놀게 되는 걸 피하기 위해 ‘확실한’(firm) 전력을 선호함. 태양광과 풍력은 간헐적임. 신규 가스 발전소는 수년이 걸리는 계획임. 겨울철 태양 대비 12시간 이상 배터리를 확보하는 것도 완전히 공짜는 아님
          + 하드웨어 자체도 재생 가능이면 좋겠음
     * AI 설비투자(CapEx)로 쓴 많은 자금이 다른 산업에서 빠져와서 그쪽은 투자 감소로 ‘굶는다’는 식의 논리와, 동시에 이 돈이 전체 GDP에 곱해진다고 주장하는 건 어불성설임. 자금이 이동한 거라면 곱셈효과는 양쪽에 동일하게 적용해야 함
          + 그래서 기사의 제목이 “Honey, AI Capex is Eating the Economy.”임
     * 글의 주된 주장은 경제가 제로섬이라는 가정에 기반함. 하지만 경제는 명백히 제로섬이 아님. AI에 투자가 몰린다고 해서 똑같이 다른 분야로 바로 전환시킬 수 있는 게 아님. 지금 AI에 투자되는 것은 그만한 가치가 기대되기 때문임. 개인적으론 그 가치가 철도보다 훨씬 클 거라고 생각함. 일부 하드웨어나 특정 지역에서 거품 과잉/과잉투자가 있을 수는 있지만, 아직 본문의 저자가 말하는 ‘붕괴 직전’ 상황은 아닌 것 같음
          + 경제가 항상 복합적으로 작용하므로 지나치게 낙관적으로만 보면 안 됨. 저자의 지적대로 단기적으로 AI 투자를 위해 다른 분야 투자가 줄어드는 것은 사실임
          + 대규모 투자가 과잉이 되더라도 결국 장기적으로 쓸모 있어질 확률이 높음. pets.com을 위해 처음 인터넷 인프라를 과다하게 깔았지만 이후 Amazon, YouTube, Zoom 같은 실제 ‘킬러 앱’이 나타나서, 당시 실수 투자가 미래 사회 기반이 된 것과 같음. 현재의 AI 투자도 비슷하게 역사적 의미가 남을 수 있음. 관련해서 Carlota Perez의 Technological Revolutions and Financial Capital 추천함
     * 그래서 트랜지스터 발전, 즉 Moore’s Law (무어의 법칙)는 앞으로 10년은 더 갈 것임. 스마트폰 성장기(2008~2023)를 이끌었으며, 이미 현재 투자금은 앞으로 2~3년 반도체 생산(2nm, A20, 곧이어 A18/14)으로 투자됨. 2030~2032년에는 A10, A8까지 모멘텀 충분히 보장 가능함. 설사 속도가 느려져도 2035년까지는 끌고 갈 여력임. 만약 2035년에 A5까지 간다 해도, 그때 12배 정도의 집적도 증가임. 패키징, 칩릿, 인터커넥트 개선 등을 포함해봐야 30~40배 수준임. 많은 AI 기업이 요구하는 1000~10000배 컴퓨트에는 아직 한참 부족함. 메모리 대역폭 확장도 그만큼 따라줘야 함
     * 자동화의 역설적 측면은 경제 규모를 늘리는 대신 일부 산업을 없애는 현상임. 재화는 더 많아질 수 있지만, 그것들이 사회적 지위를 올려주지 않게 되면 그 가치는 오히려 낮아짐. 예전엔 못 하나가 경제의 0.5%를 차지했지만 지금은 못 공장 주인도 저마진에 사회적 위상 없듯, 소프트웨어 프론트엔드 개발도 자동화되면 경제 및 사회적 비중 모두 줄어들 것임. 사회적 지위는 결국 제로섬이라, 사람들은 다른 곳에 사회적 지위를 찾으려 씀
          + “자동화가 경제의 일부를 없앤다”는 점을 들었는데, 나는 오히려 새로운 역량이 잠재된 수요를 자극해 전체 파이를 키운다고 봄. 욕구에는 제한이 없고, AI 자동화조차 그러한 수요를 따라잡기 어려움
     * FPGA로 재구성 가능한 컴퓨팅을 최적화해 LLM 연산비용을 90% 이상 낮출 방법이 나올 때를 기대하고 있음
          + 이 분야에서 이론 컴퓨터과학(이론CS) 연구가 더 많아졌으면 좋겠음. 모든 머신러닝(Machine Learning) 기법은 결국 ‘압축’ 기법임을 인식하면, 주어진 파라미터 크기에 인코딩 가능한 정보량, 정보 손실과 성능의 관계, 원 데이터셋의 정보량만 알면 LLM의 최소 사이즈 추정도 가능해야 함. LLM의 크기가 과하다고 생각하지만, 동시에 담고자 하는 데이터 자체가 방대해서 실제론 생각보다 커야 한다고 봄. 손실 압축(loose compression)이 LLM의 ‘일반화’ 원리인 만큼, 정보를 온전히 담으려면 굉장히 큰 용량이 필요함
          + 그 성능 향상의 원천이 어디서 오는지 궁금함. 하드웨어는 이미 GEMM(일반 행렬-행렬 곱셈)을 최대한 빠르게 연산할 수 있는 수준임
          + 주변 칩 친구들은 Qualcom이 FPGA 관련 특허를 다수 갖고 있어서 실제로 의미 있는 FPGA 혁신이 가로막히고 있다는 푸념이 많음
          + 기다릴 필요 없음. FPGA는 이런 아키텍처를 위한 설계가 아님. 전력 효율은 높지만, 배치 및 배선 오버헤드, 제한적인 메모리(시장에 나온 FPGA 대부분은 HBM 없음), 느린 클럭, 불편한 개발 경험 등으로 메인솔루션이 되기 어려움
          + 이미 ASIC이 출시됨. 예시로 Google TPU를 참고하면 비용 가늠 가능함. HBM(고대역폭 메모리) 자체도 매우 비쌈
"
"https://news.hada.io/topic?id=21988","오클랜드 경찰, ICE 등 연방기관에 차량 번호판 데이터 불법 제공","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 오클랜드 경찰, ICE 등 연방기관에 차량 번호판 데이터 불법 제공

     * 오클랜드 및 샌프란시스코 경찰이 자동 번호판 판독기(Automated License Plate Reader, ALPR) 데이터를 연방기관과 공유한 정황이 있음
     * 캘리포니아 주법에 따라 연방 및 주외 기관과 ALPR 데이터 공유가 금지되어 있음에도 불구하고, 수백 건의 데이터 요청과 제공이 확인됨
     * 여러 연방기관이 간접적으로 또는 직접적으로 현지 경찰을 통해 데이터 접근을 시도함
     * 시 당국과 단체들은 개인정보 보호와 책임 있는 감시 기술 사용을 강조하며 논란이 확산됨
     * 시민단체 및 프라이버시 옹호자들은 법 준수와 투명성 강화를 위한 적극적인 소송 및 감시 필요성을 제기함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

캘리포니아 경찰의 차량 번호판 데이터 연방기관 불법 공유 문제

  주요 배경

     * 샌프란시스코와 오클랜드 경찰은 연방 법 집행기관에 자동 번호판 판독기(ALPR) 데이터를 반복적으로 제공한 기록이 있음
     * 이 데이터는 Flock Safety의 카메라로 수집되며, 모든 지나가는 차량의 번호판을 촬영해 데이터베이스에 저장함
     * 2015년에 제정된 캘리포니아 주법(SB 34) 에 따라, 캘리포니아 내 경찰이 연방 또는 타주 기관과 이 데이터를 공유하는 것은 명확히 금지되어 있음

  데이터 제공 및 공유 방식

     * 기록을 공식 정보공개청구를 통해 입수한 결과, 지난 1년간 7개 연방기관(FBI, ICE 등)이 관련 수사를 명목으로 데이터에 접근함
     * 대부분의 경우, 연방기관이 직접적으로 접근하지 않고, 캘리포니아 내 다른 경찰서가 오클랜드 경찰 시스템을 대신 검색하는 방식으로 200건 이상 요청함
     * 예를 들어, 샌프란시스코 경찰(SFPD) 이 FBI, 연방 주류·담배·총기·폭발물국(ATF) 및 기타 기관을 위해 OPD 데이터 검색을 수행함
     * 캘리포니아 고속도로 순찰대(CHP) 도 한 차례 ICE를 위한 수사 명목의 데이터 검색 기록이 있음

  논란과 기관 입장

     * 프라이버시 옹호자 및 일부 도시 의원들은 데이터 오남용을 강하게 비판하고, 연방기관 접근 의혹이 언론 보도로 인해 확대됨
     * OPD 및 SFPD는 ""정책 위반 여부를 직접 재검토할 것이며, 외부 기관과 협력해 책임추궁을 보장하겠다""는 입장 발표
     * SFPD 대변인은 개인정보 보호와 감시 기술의 합법적 사용을 위한 내부 정책의 엄격함을 강조함

  법적 쟁점 및 시민사회 반응

     * Electronic Frontier Foundation(EFF) 의 Adam Schwartz는 ""ALPR 데이터의 연방 공유는 수사 목적에 상관없이 법적으로 금지됨""을 재확인함
     * 오클랜드 프라이버시 단체의 Katz-Lacabe는, ""실제 피해를 입힌 소송이 있어야만 경찰의 습관이 바뀔 것""이라며 적극적인 법적 대응의 중요성 강조함
     * 최근 수백 대의 Flock 카메라 도입에 따라 효율적인 범죄 단속과 개인정보 침해 우려가 동시에 제기됨
     * 일부 시민단체는 ""완전한 피난처 주(Sanctuary State) 실현을 위해 데이터까지 보호하는 데이터 피난처 개념의 필요성""을 주장함

  결론 및 전망

     * 현행법 위반 정황에 대한 내부 및 외부 조사가 진행 중이며, 사실로 드러날 경우 후속 조치 및 새로운 정책 논의가 이어질 전망임
     * 프라이버시 보호와 투명한 데이터 사용에 관한 논의가 지역사회와 기술 스타트업, 법 집행기관 전반에 확산 중임

        Hacker News 의견

     * 법 집행 기관이 늘 해오던 대로 행동했다는 점은 너무나 예측 가능한 일이었음, 이 데이터가 어떻게 활용될지 조금만 법 집행 방식에 익숙하다면 충분히 예상 가능했던 결과임, 만약 누군가를 원망해야 한다면 이러한 데이터셋 생성에 찬성했던 사람들을 원망해야 함, 그들은 일상적 재산 범죄 대응이나 대안 교통 정책 등의 나름의 명분에만 집중했고, 이미 카메라와 ALPR이 설치될 때부터 이런 남용에 대한 경고가 있어왔음에도 불구하고 아무 문제 없다고 생각했음, 이런 일이 반복되는 이유는 데이터 수집 프로그램의 필요성을 주장하는 사람들이 사회적으로 용인받기 때문임
          + 이런 현실이 끝나지 않는 근본적인 이유는 정부가 더 이상 법 집행 기관을 제대로 통제하거나 책임을 묻지 못하기 때문임, 이번 데이터 프로그램도 잘못된 결정이었으나 더 큰 문제는 법 집행 기관이 주정부의 직접적인 지시조차 무시하는 것임, 그로 인해 시민들이 직접적으로 영향을 받고, 또 정부가 자체 집행 기관을 컨트롤하지 못한다면 정당성 자체가 흔들림, 이를 방지하기 위해 위반한 개인과 해당 기관 모두 신속한 책임 추궁이 필요함, 그렇지 않으면 결국 법 집행 기관이 시민의 의지가 아니라 스스로가 지배자가 된 셈임
          + 이와 같은 교훈은 80년 동안이나 반복적으로 무시되어 왔음, 예시로 1943년 암스테르담 시민 등록소 폭파 사건이 있음(https://en.wikipedia.org/wiki/…), 데이터를 수집할 때마다 반드시 그 데이터가 어떻게 잘못 사용될 수 있는지 고민해야 함, 아마 직접 남용하지 않더라도 다음 관리자나 해커가 악용할 가능성이 높기 때문임
          + 지역 법 집행 기관이 지역사회 구성원의 의견과 반대로 행동하고, 심지어 주법까지 어기고 있다면 지역 경찰에 대해 분노하는 것도 충분히 정당함
          + 모두에게 분노해야 한다고 생각함, 민감한 헬스케어 데이터를 다루는 입장에서 말하면, 데이터셋이 존재한다고 해서 반드시 오남용되는 건 아니지만, 누구든 그 데이터를 다루는 사람에게 관리 책임이 있음, 민감한 데이터셋을 만든다면 반드시 관리 장치를 마련해야 하고, 그 데이터의 과거, 현재, 미래 관리자는 모두 책임을 나눠야 함, 미국 법 집행기관처럼 인센티브 구조가 약자를 보호하는 데 전혀 맞지 않는다면 모든 게 무용지물임, 그리고 데이터셋이 만들어진 후 오남용한 사람에 대한 책임 논의를 할 필요가 없다는 주장은 명백히 말도 안 된다는 점을 의료 데이터를 다루는 입장에서 강조하고 싶음
          + 이 문제에 대해 어떻게 생각해야 할지 잘 모르겠음, 데이터 수집을 막기 위해 교통 법규 위반이나 절도, 재산 범죄가 만연한 사회만이 대안이라는 논리에 공감이 가지 않음, 결국 정부가 투표자 즉 지역 주민의 이익을 위해 책임 있게 행동할 수 있다는 신뢰가 있어야 함, 그렇지 않으면 모든 시스템을 포기해야 하는 것과 같음, 또한 사람들은 항상 차털이나 절도, 생활의 질 같은 현실 문제에 더 민감하게 반응하며, 프라이버시나 권한 남용 같은 더 큰 문제를 해결하려면 지역 사회의 현실적 문제를 우선 해결해야 한다는 점도 있다고 봄, 샌프란시스코에 10년 넘게 살았지만, 기본적인 생활 문제조차 진전이 없는 걸 보면 실망스러움
     * Flock은 이런 남용을 조장하고 쉽게 만들기 위해 설계된 시스템임, Flock 시스템은 사용자에게 ‘데이터 소유권’이 있다고 약속하며 동시에 데이터를 폭넓게 공유하는 기능을 내장하고 있음, 우리 지역 경찰도 최근 Flock을 들여오기로 해서 여러 기술자 및 활동가들과 힘을 합쳐 반대 운동을 펼치고 있음, 경찰위원회와 시의회 모든 회의에 참석하고, 지역 언론과도 꾸준히 접촉 중임, 어제는 경찰위원과 3시간 넘게 대화했고, 다른 공무원들과 미팅도 예정되어 있음, 미국 내 여러 도시에서도 비슷한 움직임이 이어지고 있음, Cedar Rapids 사례도 흥미로우니 참고할 만함(https://eyesoffcr.org/blog/blog-8.html), 시스템이 막 가동되자마자 우리 경찰은 모든 기기가 다 준비되기도 전에 비보호 주(비난민 도시 상태 아님)의 다른 경찰 부서와 데이터를 공유하기 시작했고, 이에
       대해 질문하자 투명성 페이지에서 그 내용을 숨겨버림, 이런 행태를 공개적으로 비판 중임, Flock은 VC 투자금을 받은 상업화된 대규모 감시임
          + 크라우드소싱으로 모은 Flock 카메라 위치 정보 사이트가 있음(https://deflock.me/)
     * 오클랜드에 살고 있는데 이 문제는 참 복잡함, 이곳에서 흔히 발생하는 범죄 유형은 기술의 도움 없이 해결하기 거의 불가능함, 누군가가 차를 훔치고 동네에 들어와서 또 다른 차나 집을 턴 뒤 도망감, 때로는 911에 신고도 하기 전 이미 사라지고, 경찰이 도착하기 전이라 범죄자들도 잡히지 않는다는 걸 너무 잘 알고 있음, 그런 만큼 대범하게 범행을 저지르고, 누군가가 보는 앞에서도 태연하게 실행함, 저항하다가 총에 맞거나 목숨까지 잃는 일도 비일비재함, 내 눈앞 한 블록에서 절도범들의 차량 창문을 두드렸다는 이유로 사람이 살해당하기도 했음, 범죄의 근본 원인을 해결하는 일은 시간이 오래 걸리는 사회경제적 과제임, 그래서 단기적으로는 결국 검거와 처벌률을 높이는 것밖에 범죄 억제 수단이 없음, SF에선 드론을 통해 범죄조직을 검거하면서
       차량 털이가 줄었음, 오클랜드는 드론은 없지만 Flock 카메라 덕분에 용의자를 추적하고 실제로 검거하는 사례가 늘었음, 이건 사실임, 그래서 주민들이 이런 해결책을 바라는 것도 이해함, 현황을 이해하지 않고 이미 긍정적 영향이 나타나는 솔루션이 기타 비용과 리스크 대비 무가치하다고 설득하는 건 매우 어렵다고 봄, 더 큰 차원에서 보면 범죄의 근본 대책을 반대하던 사람들이 감시 솔루션을 선호하고, 이 시스템을 남용하는 경향이 있음, 그들이 선호하는 범죄 해결책이 우연의 일치처럼 전체 인구 감시에 도움이 된다는 게 의미심장함
          + 지역 재산 범죄를 줄이기 위한 데이터 활용과 ICE 같은 기관에 정보가 넘어가는 것은 엄청난 차이가 있음, ICE는 사람들을 인권에 어긋나는 환경으로 추방하고, 정당한 절차 없이 군사적 방식으로 활동함, 사실상 비밀경찰과 같음, 지역 경찰이 범죄를 해결하는 건 중요하지만, 차량 털이나 절도 문제에 비밀경찰 수준의 기관이 필요하진 않음, 근본 원인 해결 역시 경찰 외의 시스템과 협력으로 풀어가야 함
          + 이건 추상적인 윤리 논쟁이 아니라 현실 문제임, 오클랜드는 이미 극심한 혼란 상태임, 전 경찰서장이 해임되고, 이를 결정한 시장도 결국 주민소환을 당했으며, 2023-2024년 지방 검사장도 투표로 소환됐음, 주지사도 오클랜드에 경찰 추격 금지 정책을 바꾸라고 경고한 바 있음, 당연히 새 경찰서장이 이 정책 변경을 추진 중임, 관련 기사 참조(https://oaklandside.org/2025/05/…)
     * YC S17
          + 최근 낙태 사례(https://eff.org/deeplinks/2025/…)와 CEO의 책임 회피(https://flocksafety.com/blog/…) 등을 보면 Flock은 이런 무책임을 공공연히 조장하는 모습임, 도시에서 이런 시스템은 반드시 철거되어야 하고, 현재 고객사 리스트가 공유되고 있는지 궁금함, 찾아보니 우리 지역도 2023년에 Flock 설치를 완료했음(https://atlasofsurveillance.org/search?vendor=Flock+Safety), 이번 주 시의원 몇 명을 초대해 함께 이 문제를 논의할 예정임
          + “범죄를 완전히 없애는 첫 번째 공공 안전 운영체제”라는 홍보 문구를 보고, 스타트업 창업자의 자만심에 대해 처음 알게 됐음
          + 한때 YC 지원서에 “개인적 이득을 위해 시스템을 교묘하게 이용한 사례”를 작성하도록 요구했음, YC는 정직한 창업자보다 때론 “말썽꾸러기” 스타일을 더 선호하는 듯함
     * 자동화된 데이터 수집은 헌법상 불법 수색임을 잊지 말아야 함, 경찰국가를 지지한 뒤 다른 결과를 기대하는 건 모순임, (의무적 사물등록, 의무적 의료시술, 의무적 얼굴 인식, 이런 것들 모두 정부의 자동화 지불을 통한 감시 강화와 연결됨)
     * “오클랜드 경찰이 연방 기관과 직접 정보를 공유한 게 아니라, 다른 캘리포니아 경찰이 연방의 요청을 받아 오클랜드 시스템을 대신 검색한 거라면 주법 위반 주체가 달라지는 건지 궁금함”
          + 다른 기관들이 구체적인 기록을 공유한 게 아니라 존재 여부만 확인해준 경우라면 주법 위반이 아닐 수 있음, 예를 들어 자동차 번호판 기록 자체 공유는 금지여도 CHP가 기록 존재 유무만 답변하면 문제 없을 수도 있음, 본문을 끝까지 잘 읽었던 덕분에 헤드라인이 오해를 불러일으킬 수 있음을 알아챘음, 실제 판단은 판사가 하는 것임
          + 한 가지 의문은 오클랜드 경찰이 누구나 검색할 수 있을 정도로 데이터를 개방함으로써 주법을 위반한 건지임, 이 시스템에 가입할 당시 연방기관이 사전동의 없이도 데이터에 접근 가능하다는 점을 인지하고 있었는지가 중요함
     * “오클랜드 경찰이 연방기관과 정보를 직접 공유한 게 아니라, 다른 주 경찰이 연방기관을 대신해 오클랜드 시스템을 200회 이상 검색했다”는 요점임
          + 결국 헤드라인이 오해를 불러올 수 있음을 지적하고 싶음, 오클랜드는 기록을 CHP 같은 주 기관에 개방한 것이고, 그 기관들이 그 정보를 연방기관에 제공한 것임, 그러나 어떤 검색 결과가 실제로 공유됐는지는 기사에 명시되지 않았음, 오클랜드가 법을 위반하지 않았다고 볼 근거 있고, 다른 기관들 역시 위반 증거가 명확치는 않음, 판단은 기자가 아닌 판사가 내리는 것임
     * “이런 의혹이 확인되면 조치가 취해질 것”이라는 말은 형식적 반응 같음
     * 리더십이 처벌하지 않을 것이라는 암시만 주면 경찰관들은 늘 원했던 일을 실제로 저지름, 너무 뻔한 일임
     * 이 주변에는 부패가 만연함, 지역 경찰관들이 연방 법원 집행관으로 임명돼 초과근무까지 챙기기도 함
"
"https://news.hada.io/topic?id=21976","OpenCut - 비디오 편집기 CapCut 의 오픈소스 대체제 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  OpenCut - 비디오 편집기 CapCut 의 오픈소스 대체제

     * OpenCut은 CapCut과 유사한 기능을 제공하는 무료 오픈소스 비디오 편집기
     * 개인정보 보호가 뛰어나며, 모든 영상 데이터가 사용자의 장치 내에서만 처리됨
     * CapCut의 주요 기능들이 유료화된 점과 달리, 모든 핵심 편집 기능을 자유롭게 제공함
     * 웹, 데스크탑, 모바일에서 간편하게 사용 가능하며, 사용자 친화적인 간단한 인터페이스 제공
     * 추적 없는 익명 분석, 무제한 타임라인 기반 편집, 멀티트랙, 실시간 미리보기, 워터마크 미포함 기능 지원
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

OpenCut 프로젝트 개요

     * OpenCut은 CapCut의 대안이 되는 무료·오픈소스 영상 편집 앱으로, 웹/데스크탑/모바일에서 모두 사용 가능함
     * 사용자 비디오가 기기 내에만 남아, 개인정보 보호가 뛰어남
     * CapCut에서 유료로 제공되는 대부분의 주요 기능을 무료로 제공하며, 쉽고 간단한 UI로 설계되어 누구나 접근 가능함

주요 특징

     * 타임라인 기반 편집 가능
     * 여러 오디오/비디오 트랙을 멀티트랙 지원
     * 실시간 미리보기 제공
     * 워터마크 없음 / 구독 필요 없음
     * Databuddy를 통한 100% 익명, 비강제적 분석 정보 제공

후원 및 라이선스

     * Vercel의 오픈소스 지원으로 호스팅 및 개발 인프라 활용 중임
     * 본 프로젝트는 MIT 라이선스로 제공됨

프로젝트의 의의와 장점

     * CapCut이 무료 기능을 줄이고 점차 유료화를 강화하는 흐름 속에서, 누구나 접근할 수 있는 비디오 편집기의 필요성이 커짐
     * OpenCut은 개인정보 보호 중심 설계와 함께, 필수적인 편집 도구를 무료 제공함으로써, 사용자 중심 오픈소스 프로젝트로 주목받고 있음
     * 설치 없이 웹에서 바로 사용 가능하며, 오픈소스 특성상 자유로운 확장 및 커스터마이징이 용이함

        Hacker News 의견

     * 최근에 Cluely 팀이 만들어 낸 스타일의 토론 방식인지 잘 모르겠지만, 이런 분위기가 더 퍼지지 않았으면 하는 바람임 관련 이슈 링크
          + 온라인에서 무례한 태도는 최근에 유행한 게 아니라는 생각임, 커뮤니티에서 “모두의 괴롭힘 없는 경험 보장”을 말하면서도 정작 신고 연락처를 비워두는 경우가 있는데, 이런 형식적 운영은 의미 없다고 생각함
          + 그 스레드는 반면 교사로 삼을 사례가 많음, 뭔가 하면 안되는 것만 모아놓은 보물창고 느낌임
          + 링크된 스레드 전체를 읽어보면 특별한 “토론의 스타일”은 찾아볼 수 없고, 평범하고 건설적인 대화 중 특정 방해 인물 한 명(Zaid)만 튄다는 인상임, 프로젝트 이미지에는 별 문제 없어 보임
          + 요즘 젊은 세대의 온라인 커뮤니케이션 스타일이 이미 이런 공격적이면서 공허한 ‘ragebait’를 쉽게 볼 수 있음, 대부분은 실질적 내용 없이 자극만 주기에 무시하는 게 쉽다는 장점이 있음
          + 태도에 큰 문제를 보이는 인물의 프로젝트에서 괜찮은 제품이 나올 확률이 낮다고 생각함
     * 이 아이디어 자체는 궁금하고 괜찮게 보이지만, 아직 코드를 보거나 직접 써보진 않았으나 몇 가지 불안한 점을 느낌, GitHub 별이 많은데 스크린샷은 공식 웹사이트 어디에도 없고, 트위터에서도 CapCut 스크린샷만 보일 뿐 opencutapp의 실제 이미지는 전혀 없음, 이 프로젝트가 성공하길 바라지만 뭔가 보여줄 만한 것이 충분하지 않아 걱정됨
          + 직접 설치해봤는데 빌드 방법이 제대로 동작하지 않았음, 시간만 낭비했고 이런 게 HN에 올라올 이유를 모르겠음
          + 여기에서 약간의 스크린샷과 활동 내역 확인 가능함, 팔로워 숫자와 활동은 정상적으로 보이며, GitHub 별은 일정 부분 마케팅 덕분이라고 생각함
          + 특유의 쎄한 느낌도 남
     * CapCut의 오픈소스 대안이라는 점이 마음에 들지만, CapCut만큼의 쉽고 편리한 사용성을 갖추려면 아직 갈 길이 멀다고 느낌, 데스크탑의 일반 유저는 “Bun, Docker, Docker Compose, Node.js” 같은 선행 설치 조건을 보고 대부분 포기할 듯함, Blender, Shotcut, OpenShot, Kdenlive 등이 지금 시점에는 더 현실적인 오픈소스 동영상 편집기임
          + 이런 설치 선행조건을 해결하려면 단일 AppImage나 Electron 앱처럼 패키징만 잘하면 된다고 생각함, Bun과 Docker는 개발자를 위한 도구이지 일반 사용자용은 아니기 때문임
     * OpenCut 공식 홈페이지의 CapCut 비판 페이지를 보고 느낌이 복잡함
          + “너희 영상 편집이 특별하다고 생각하지? TikTok에 47개 트랜지션과 12개 폰트를 쓰면 유명해질 거라고 믿지? 모든 효과를 다 넣어도 콘텐츠가 더 좋아지지 않는다, 착각하지 마라”와 같은 강한 문구가 있는데, 이런 자극적인 스타일은 일부에게만 매력적일 뿐 다수의 사용자에게서는 외면 받기 쉽다고 생각함, 실제로 이 프로젝트는 고도의 기술력이 있는 특정 소규모 유저층만 대상으로 삼는 듯함
          + 이 문체는 확실히 AI가 생성한 느낌이 강함
          + ByteDance가 CapCut에서 굳이 유료화를 하는 이유가 궁금함, TikTok에 더 많은 콘텐츠가 올라오는 게 유리할 텐데 왜 쉽고 저렴하게 풀지 않았을지 의문임
     * 다시 한 번 올라온 글을 보고, 이전에도 비슷한 포스트가 있었다는 점을 언급하며 스팸 방지 시스템에 궁금증 및 참고 링크, HN에서 사기 방지 시스템이 어떻게 돌아가는지 알고 싶다는 genuine curiosity임
          + 이전 글들에 점수가 거의 없었기 때문에 재포스팅이 문제되지 않는다고 생각했음, 나는 사기꾼이 아니고 오늘 GitHub 피드에서 OpenCut을 발견해 흥미로워서 공유했을 뿐임
          + 재포스팅은 이전 글이 크게 주목받거나 최근이 아니면 특별히 제재하지 않음
          + 스팸 여부와는 별개로 이 저장소(레포)가 진짜인지 의문임
     * 스크린샷이나 짧은 영상 투어가 있었다면 흥미롭게 볼 수 있었을 듯함
     * opencut.app/projects에서 직접 테스트할 수 있음, README에도 스크린샷 추가했고 upstream에 반영되도록 PR도 올림
     * 스크린샷 하나 없는 채로 홍보만 하는 프로젝트에는 별 관심 없음, 진짜 좋다면 저절로 퍼지게 되는데 README의 ‘프로젝트 구조’ 섹션도 그냥 디렉터리 목록이라 LLM이 뽑아낸 느낌임
          + 특히 셀프호스트 subreddit 등에서 LLM 기반 앱 공유가 너무 많아졌는데, AI가 만든 티가 확연한데 지적하면 제작자들이 불쾌해함
          + 정말 뛰어난 프로젝트라서 저절로 퍼지는 경우는 거의 없지만, 그래도 스크린샷이 없는 게 이상하긴 함
     * 대기자 명단(waitlist)이 있다는 건 유료화 의도가 있는 것인지 궁금함
     * Openshot, Shotcut, Opencut 등 이름이 비슷해서 헷갈리기 시작함
"
"https://news.hada.io/topic?id=22030","Firefox는 앞으로 어디로 갈까요? - 여러분이 말씀해주세요","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Firefox는 앞으로 어디로 갈까요? - 여러분이 말씀해주세요

     * Mozilla는 Firefox의 미래 발전 방향을 이용자들과 함께 만들어가는 새로운 실험을 시작
     * 호응이 좋았던 프로필, 탭 그룹, 세로 탭, 새 탭 배경화면, PWA, 작업 표시줄 고정 등 기능이 모두 이용자 요청에 의해 도입된 사례
     * 그래서 Mozilla는 정기적인 설문과 체크인을 도입하여 사용자의 요구사항, 생각, 기대, 그리고 진짜 원하는 바를 직접 듣고자 함
     * 이를 위해 Firefox 제품 관리자들과의 AMA(무엇이든 물어보세요) 세션을 기획 중임
     * AMA의 주제와 질문을 사용자로부터 사전 수렴해 보다 폭넓고 의미 있는 소통으로 발전시키려 함

댓글에 올라온 Firefox 커뮤니티 AMA 사전 질문들 정리

  주요 요청들

     * Split View: 다중 문서 작업을 위해 데스크탑과 모바일 양쪽에서 필수로 요청됨
     * Workspaces: 브라우저 창을 카테고리별로 나눌 수 있는 기능. Vivaldi/Opera 사용자가 특히 높이 평가함
     * Tab Group & Tab Folder 연동: 탭 그룹을 북마크 폴더처럼 동기화하거나 통합해서 사용하고 싶다는 제안
     * Second Sidebar: 웹 패널을 붙여 쓸 수 있는 Vivaldi식 사이드바 요구
     * 모바일 탭 그룹/탭바 UI 개선: Android 및 iOS에서 탭 그룹 기능, 하단 탭바 위치 조정 요청 많음

  기능 개선 제안

     * 웹 API 지원 확대:
          + showOpenFilePicker, showSaveFilePicker 등 File System API
          + Periodic Background Sync API
          + MIDI 데이터 복사 허용 (Clipboard API 개선)
          + Bluetooth LE 접근 지원 (Web + Hardware 연동용)
     * 화면 공유 기능 개선:
          + Chrome처럼 탭 단위 화면 공유 기능 도입 요청
     * 키보드 단축키 커스터마이징: Chrome처럼 확장기능 실행용 단축키 설정 등 요청
     * 브라우저 내장 기능:
          + PnP (Picture-in-Picture) 개선
          + RAM 사용량 표시
          + 탭 간 동기화
          + 북마크 가져오기 시 폴더 단위 선택 기능

  퍼포먼스 & 최적화 요청

     * 전반적 최적화:
          + 저사양 PC 및 모바일 기기에서의 속도 문제, RAM 사용량, 안정성 개선 요청
          + 특히 Android에서 탭 리로딩 문제가 사용성 저하로 지적됨
     * Gecko 엔진 성능 향상:
          + Chromium 대비 느린 페이지 로딩 속도 개선 요청 다수
          + 메모리 절약을 위한 탭 절전 기능 요청

  모바일 Firefox에 대한 불만과 제안

     * UI 노후화, 대응 부족, 폴더블 디바이스 호환성 문제 지속 지적
     * 사용자들은 Chrome 수준의 UX와 성능을 기대함
     * 확장기능 허용, 광고차단, 사이트 격리 기능을 iOS와 Android 모두에 요청

  프라이버시 관련 제안

     * 내장형 AdGuard 광고 필터 도입 요청 (uBlock보다 규칙 많고 다국어 지원)
     * 브라우저 지문 보호 강화: canvas 외에 webGL, AudioContext, TimeZone 등도 무작위화 필요
     * DNS over Oblivious HTTP (DoOH), DNS4EU 등 새로운 DNS 프라이버시 기술 도입 요청
     * 텔레메트리 차단 시 영향에 대한 투명한 공개 및 사용자 제어권 강화 요청
     * 프라이버시 기능 도입 시 사전 만화 등 유쾌한 커뮤니케이션 방식 제안

  자주 언급된 AMA 질문들

     * Firefox 팀은 어떤 기준으로 기능을 우선순위화하나요?
     * 왜 Chrome/Brave처럼 기능을 '더 잘' 구현하지 않나요?
     * Reddit이나 외부 커뮤니티의 비판은 어떻게 수용하나요?
     * 피드백 반영과 커뮤니티 참여가 실제로 얼마나 영향력이 있나요?
     * 프라이버시를 위한 유료 플랜은 검토 중인가요?

   파이어폭스를 꺼리게되는 주된 이유는 느릿한 기능지원이랄까요.
   웹개발하면서 mdn 자주 보게 되는데.
   가장 먼저 보는 부분이 브라우저 호환성 부분이고.
   요 몇년간 보게된 부분이.. 전체 표준 api는 지원하지만 세부 적인 항목 한두개는 미지원하는 파이어폭스를 보고 크롬으로 테스트하게 됩니다.
   안정적이고 확정된 표준만 지원하려는 것은 좋은데.
   대부분 지원하고 파이어폭스만 안되는 것들이 자주 눈에 띄다보니 점차 기피하게 되더군요.
   개인적으로 파이어폭스가 과거 익스로 인한 암흑기를 걷어내는 단초가 되어줘서 일부러 사용중인데. 프로젝트 할때는 쓰기가 애매해요.

        Hacker News 의견

     * Mozilla의 방향과 초점을 둘러싼 더 심각한 문제가 많은 걸 알지만, 마치 유치원생에게 말하듯 하는 이런 마케팅 홍보 방식이 눈살을 찌푸리게 함
       ""어떤 동물이 당신의 Firefox 브라우징 스타일을 가장 잘 나타내나요?"" 라는 식의 이모지 동물 질문은 너무 얄팍함
       커뮤니티를 지나치게 유아적으로 대하는 PR 트렌드는 진정성과 멀어져 거북함을 자아냄
       유쾌하거나 재미있는 접근을 반대하는 건 아니지만, Mozilla의 커뮤니티 참여는 인공적이고 무미건조한 느낌을 준다는 점이 아쉬움
     * Firefox에 애정을 보여줘서 고마움 링크
     * 이 문제의 근본 원인은 소통이 진실되지 않음에 있다고 생각함
       실제 이해관계자가 프로젝트에 대해 솔직하게 이야기하기보다는, 브랜드 이미지를 만들기 위한 마케팅 메시지가 주가 되고 있음
     * Colorways라는 불필요한 기능도 굳이 강제로 밀어넣었던 걸 잊지 말아야 함
       테마 색상 변경하고 싶은 사람은 이미 그렇게 할 수 있었음
       귀중한 시간과 자원이 진짜 필요한 호환성 증가나 Google 종속 탈피에 쓰이지 못하고, 쓸데없는 일에 낭비되고 있음
     * 이번 AMA는 엔지니어링 팀이 아니라 제품 매니저들과의 행사임
       따라서 이런 어투가 자연스러움
     * 이 글 전체의 톤이 너무 둔감하고 미숙하게 느껴짐
       더 중요한 이슈들을 회피하고 있는데, 그런 문제를 인정하는 것만으로도 진전이 있을 것임
       내부적으로 이런 건 논의됐을 것 같고 시끄러운 회의도 있었을 거라 추측함
       이 모든 건 경영진이 뭔가를 제대로 처리하지 못하고 있다는 신호라 Mozilla가 잘못되고 있는 게 아닌지 더 걱정하게 됨
       귀여운 동물 그림은 적고 Rust/C++ 같은 진짜 개발에 더 투자해야 함
       제품을 실제로 개선할 수 있는 사람들이 실질적으로 업무에 집중할 수 있게 하는 게 중요함
       나는 실제 사용자로서, 지금 성능 향상에는 만족하고 있지만 개발자들이 임무에 집중할 수 있도록 보장하는 게 주된 관심사임
       즉, 광고, 팝업, 기타 광고성 학대에 노출되지 않으면서 Chromium, Webkit과의 기술 표준 경쟁력도 지켜야 함
     * Mozilla의 기부금이 아직도 CEO 보수와 비슷하게 유지되는지 궁금함
       2023년 기부금 출처
       2022년 세금 보고서
     * Signal의 Meredith Whittaker는 80만 달러도 못 받았음
       690만 달러가 어떻게 받아들일 수 있는 보수인지 이해가 안 감
       참고 링크
     * Mozilla Foundation이 Firefox와 실제로 관계가 별로 없는 것 같음
       Mozilla 상황은 Wikimedia Foundation의 기부금 구조보다 더 혼란스러움
       CEO 연봉이 ‘related for-profit’에서만 지급된다는 것도 이상하게 보임
       즉, 모회사에서 700만 달러를 Firefox 대신 CEO 보수로 쓴 셈임
     * 나는 690만 달러보다 훨씬 더 적은 비용으로 브랜드를 망칠 수 있을 것 같음
     * CEO 보수를 절반만 깎아도, 소규모 개발팀에 투자해 Firefox의 Rust 전환을 마무리하고 정말 혁신적인 브라우저와 Rust 기반 GUI, JS 엔진 등을 완성할 수 있었을 것임
       실제로 Rust로 핵심 컴포넌트를 대체했을 때 성능 향상이 뚜렷했음
       나는 2004년부터 Firefox를 써 왔음
     * CEO 보수가 ‘관련된 영리 회사’에서만 지급된다고 명시되어 있음
       기부금이 CEO 보수로 직접 들어가진 않지만, 수익 일부가 비영리로 더 흘러가야 한다는 점에는 동의함
     * 솔직히 Firefox의 최대 장점은 확장 프로그램 지원임
       구글이 MV2 지원을 끊어서 생긴 결과이지 Mozilla의 공로는 아님
       안드로이드도 마찬가지로 구글이 크롬에 확장 지원을 안 넣어서 Firefox의 가치가 상대적으로 높아짐
       하지만 Firefox는 점점 침몰 중임
       우리가 원하는 건 속도, 좋은 UI, 배터리 절약, 광고 차단 등 보편적 기능임
       안드로이드 Firefox UI는 URL 창이 너무 좁은데 버튼이 셋이나 더 생김(공유, 읽기모드, 번역 등)
       가로모드로 돌려야 URL이 겨우 보임
       사용자 입장에서 URL을 항상 볼 필요는 없지만, 전체적으로 사용감이 별로임
       PC에서는 무려 4가지 다른 히스토리 기능이 있음
       하나의 제대로 된 히스토리 기능만 있으면 됨
     * Chrome이 Google 데이터를 얼마나 수집하는지 봤는지 묻고 싶음
       그런 의미에서 Firefox의 데이터 프라이버시는 엄청난 강점임
     * 반대 의견임
       Firefox는 세밀한 설정 옵션이 더 많고, 나는 Container Extension(도메인별 샌드박스 탭)을 쓰고 있음
     * 안드로이드 Firefox UI에 대해 직접 비교해봤음
       Firefox: 홈/SSL/URL/읽기모드/탭/햄버거 메뉴, URL은 화면의 20~70% 차지
       Chrome: 홈/인증정보/URL/탭추가/탭리스트/햄버거 메뉴, 아이콘 패딩은 더 큼
       Firefox가 URL 공간을 3자 정도 더 많이 보여줌
       읽기모드 버튼이 Firefox엔 있고, 새 탭이 Chrome에 독립적으로 있을 뿐 큰 차이는 없음
       데스크톱 히스토리 기능도 살펴보면, Firefox는 전체 계정 뷰, 사이드바, 메뉴 내 최근 항목, 레거시 팝업 등이 있고
       Chrome 역시 전체 히스토리, 최근 히스토리, 사이드바 등 있어 실제로 Firefox만 더 많은 건 파워유저용 ‘레거시’ 뷰임
     * Firefox WebExtensions는 아직 서비스 워커 기능이 꺼져 있고, File System API도 없음
       MV3 확장생태계도 시한폭탄임
     * Google은 광고회사임
       이 점이 훨씬 중요한 차별점임
       Chrome의 MV3는 결국 사용자를 광고 대상으로 만든다는 신호임
     * 시간과 자원을 쓸데없는 데 낭비하지 말고, 커뮤니티에 귀 기울이고 엔지니어를 많이 고용해서 Chrome 수준의 브라우저를 만들어주길 바람
       이미 우리가 원하는 걸 다 말했으니 계속 묻지 않아도 됨
       Mozilla의 자원 낭비 행태는 지겹기만 함
     * Mozilla가 피드백을 자주 묻지만, 늘 명확하게 ‘덜 복잡하게, 성능 향상, 회귀 버그 해결’이라는 답을 받고서도 아무도 원하지 않은 실험만 던지는 게 신기함
     * Mozilla는 오히려 Chrome보다 훨씬 적은 예산으로 더 나은 브라우저를 만들고 있음
       이걸 자원 낭비라고 할 순 없음
     * 시간과 돈 낭비라는 게 AMA 행사를 말하는 건지?
       커뮤니티 의견을 듣는 게 정확히 그런 게 아닐지 궁금함
     * 0.5% 미만 점유율로 곧 직행할 듯함
       수많은 기여자들의 노력을 우롱하는 행위임
       Mozilla Foundation이 상업적 이익에 맞서야 한다는 소임을 저버린 것임
     * 이들의 일은 실제 경쟁 없이 후원자를 만족시키고 수익을 벌어들이는 것이 전부였음
     * 광고 같은 쓸데 없는 기능만 빨리 걷어내고, 웹 표준을 계속 따라가야 함
       그렇지 않으면 사라질 것임
       Thunderbird의 예처럼, 돈이 직접 소프트웨어에 돌아간다는 확신만 있으면 사용자는 기꺼이 기부함
       2022년 Thunderbird는 약 2천만 사용자에서 6백만 달러 기부를 받았는데, Mozilla는 2억 명이 넘는 사용자로 9백만 달러를 받았음
     * Mozilla는 2024년에 8억 2천 6백만 달러를 벌어들였음
       Thunderbird처럼 사용자 기부를 받아도 6천만 달러가 한계라 총수입의 7% 수준임
       Firefox는 Thunderbird만큼 기부를 받긴 어려움
     * Mozilla Corporation에는 기부할 수 없음
       Foundation만 기부를 받음
     * Thunderbird의 안드로이드 버전은 (원래 K-9 Mail 인수 이후) 오히려 차별적이었던 기능을 잃었음
       다른 부분에는 동의함
     * “광고/감시 회사가 아니라는 점이 Firefox 최고의 장점”이라는 건 이런 포럼에서 받아들여지지 않을 것 같음
       이런 문제 말고는 다 사소한 세부사항임
       (참고로 이 글은 LibreWolf에서 쓰고 있지만, 실제로는 Mozilla가 거의 모든 코드를 작성하기 때문에 그 신뢰성까지 의문임)
     * 예전 애드 혹은 사용자 데이터 판매를 어떻게 되돌릴 생각인지 묻고 싶음
       그리고... 당신은 어떤 동물임?
     * Mozilla가 이해하려 하지 않는 가장 큰 차별점임
       Chrome은 기능, 성능, 표준 모두 훌륭하지만, 광고회사가 소유하는 한 사용자의 광고 차단 옵션이나 프라이버시 선택권을 존중할 수 없음
       새로운 기능, 버그 픽스도 기본적인 수준이고, 그 이상이 아니면 경쟁력이 없음
     * Firefox는 프라이버시, 확장성, 웹 표준에 집중해야 경쟁에서 밀리지 않을 것임
       대다수 UX 개선은 중요하지 않음
       나는 LLM 컨텍스트 메뉴나 읽기모드가 유용하다고 생각하지만, 이게 브라우저 선택의 결정적 요인은 아님
     * 읽기모드는 특히 모바일에서 정말 좋음
     * LLM 도입은 새로운 수익원 찾기 위한 전략처럼 보임
       최근 Perplexity 팝업도 그런 실험의 일환으로 보임
     * 솔직히 내 경우 Firefox의 기능성 때문에 오랜만에 다시 쓰게 되었음
       안드로이드에서 광고차단이 필요해 써보기 시작했고,
       이후 모바일-데스크톱 간 히스토리/탭 동기화도 쓸만했음
       모바일 읽기모드와 들려주기 기능에 반함
       Zen browser도 써봤지만, Firefox가 세로 탭과 탭 그룹을 지원해서 다시 Firefox만 쓰는 중
       내 요구를 다 충족하고, 아주 드물게만 크로미움 브라우저를 열게 됨
     * 나는 Linux와 Windows에서 Zen browser를 잘 쓰고 있음
       약간 부족한 점은 있어도 Firefox 래퍼로 괜찮음
       오픈소스 커뮤니티 구성의 다양성에 따라 여러 의견이 존재하고 모두 만족시키기는 불가능하다고 생각함
       Mozilla는 UX 실험은 Zen(Arc 카피)처럼 타사에 맡기고, 본체는 코어 성능·호환성에 집중해야 함
       FF를 더 미니멀하게 유지하고, 새로운 UI 실험은 서드파티가 할 수 있도록 지원하는 게 좋음
       광고·스폰서십 관련 기능은 완전히 버려야 함
     * 안드로이드 Firefox의 주소창은 ‘홈’, ‘번역’, ‘공유’, ‘읽기모드’ 등 불필요한 버튼들로 너무 복잡해졌음
       이 버튼들 때문에 실제 주소는 겨우 몇 글자만 보임
       링크 누를 때마다 항상 이전 탭을 수동으로 닫아야 해서 불편함
       탭 재사용 허용, 자동 닫기 등 더 유연한 탭 관리 필요함
       속도 개선도 필수, 현재는 주요 브라우저 중 제일 느림
       더욱 다양한 커스터마이징 가능해야 하고(about:config 등), 주소창에서 쓸모없는 버튼을 빼고 싶음
     * 모바일에서 about:config 진입법은 여기 참고 링크
     * 참고로 about:config는 안드로이드 베타/나이틀리에서 이미 제공되고 있음
       나이틀리는 불안정해서 베타로 갈아탔는데, 베타는 꽤 안정적임
     * HN 이용자들이 브라우저 탭을 수백 개씩 열어놓고 쓴다는 걸 보고 놀랐음
     * 나도 모바일은 오직 ‘읽기모드’ 때문에 Firefox로 옮김
       광고와 팝업이 많은 페이지 볼 때 이 기능이 모든 걸 싹 정리해줌
     * 나는 초기 Firefox 후원자로서, 버전이 매일 올라가기 시작할 때가 끝의 시작임을 알아챔
       그때 Mozilla 지인들에게 잦은 릴리스로 확장기능이 자꾸 깨진다며 불만을 표출함
       비윤리적인 기본값(다크패턴)도 많아짐
       사용자 배려 없는 행동이 너무 많아서, 더 이상 회복은 불가능하다고 생각함
       전성기의 사람들이 떠나고 저런 기사를 쓴 사람이 들어와 있음
     * HN에는 이런 류의 인공지능 분노 유도, 사실 왜곡 게시글을 구분해서 보여주는 시스템이 있었으면 함
"
"https://news.hada.io/topic?id=22067","애플, Figma/Sketch용 iOS/iPadOS 26 Design Kit 공개","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             애플, Figma/Sketch용 iOS/iPadOS 26 Design Kit 공개

     * Apple 공식 Figma/Sketch용 UI 키트로, iPhone 및 iPad 인터페이스를 매우 정밀하게 재현
     * 모든 기본 컨트롤과 뷰, 텍스트/컬러 스타일, 머티리얼, 레이아웃 가이드가 포함되어 빠른 프로토타입 제작과 사용자 흐름 설계에 최적화
     * iOS 및 iPadOS 26의 신규 디자인 언어가 적용되어, Liquid Glass 컨트롤과 뷰, 컨트롤 크기/레이아웃/코너 반경, 시스템 컬러 등 최신 UI 트렌드 반영
     * 활용 전 최신 SF Symbols 7 설치 필수
     * Icon Composer에서 사용할 App Icon Template 도 Figma/Sketch 및 Photoshop/Illustration 용으로 공개됨
"
"https://news.hada.io/topic?id=22047","Tilck - 리눅스 호환 미니멀 커널","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Tilck - 리눅스 호환 미니멀 커널

     * Tilck는 리눅스와 바이너리 호환되는 모놀리식 커널로, 교육 목적과 내장 시스템용으로 설계됨
     * *리소스 사용 최소화**를 목표로 하여 임베디드 장치나 제한된 환경에서 유용함
     * BusyBox 등 기존 리눅스 사용자 애플리케이션을 그대로 실행할 수 있어 별도 앱 작성 불필요
     * i686과 RISC-V64 아키텍처를 지원하며, 향후 ARM 및 MMU-less CPU로도 포팅 계획 있음
     * 파일시스템, 프로세스 관리, 콘솔, 디버깅 등 다양한 기능이 포함되며, Vim, Micropython, Lua 등도 실행 가능
     * QEMU 및 실기기 부팅 지원, 테스트 및 디버깅 환경도 잘 갖춰져 있어 입문자도 쉽게 시도 가능
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Overview

     * What is Tilck?
          + Tilck는 작고 단순한 설계를 가진 리눅스 호환 교육용 커널
          + 기존 커널처럼 자체 앱이 필요한 게 아니라, musl 기반 툴체인으로 빌드된 리눅스 프로그램 사용 가능
          + BusyBox, Vim, TinyCC 등 여러 앱을 실행 가능
          + monolithic 구조로 리눅스처럼 단일 커널 공간에서 모든 기능 처리
          + 실제 하드웨어에서의 실행도 중시
     * Future plans
          + 초저지연·완전 결정론적 시스템이 필요한 임베디드 환경에 적합
          + Embedded Linux와 FreeRTOS/Zephyr 사이의 틈새 포지션을 목표
          + ARM64, MMU-less, 네트워크 (UDP/IP) 및 저장장치 (FAT32, ext2) 지원 계획 있음
          + 라즈베리파이 3/4 같은 SoC 대상으로 네트워크·저장 기능까지 지원하는 것이 장기 목표
     * What Tilck is NOT?
          + Tilck는 리눅스를 대체하려는 프로젝트가 아님
          + 데스크탑 OS를 목표로 하지 않음 (X 서버 등 미지원)
          + Tilck는 단순성과 실시간성을 위해 기능을 최소화함
     * Tilck vs Linux
          + 리눅스가 복잡한 이유는 다기능 때문이며, Tilck는 단순성, 소형화, 초지연성을 선택
          + Tilck는 테스트 인프라도 엔터프라이즈 수준에 가깝게 갖추려 노력

Features

     * i686 지원
          + 구형 하드웨어(8259 PIC, 8254 PIT 등)와 최신 기능(SSE, AVX, ACPI 등) 동시 지원
          + ACPI 통해 전원 이벤트 처리, 배터리 정보 확인 가능
          + 다양한 x86 환경 (BIOS, UEFI, CSM 등)에서 테스트됨
     * riscv64 지원
          + 임베디드 중심 구조
          + QEMU, Sipeed licheerv-nano 보드 지원
          + 장치 정보는 device tree 통해 전달되며, 커널 코드에 보드 의존 코드 없음
          + ns16550 UART, PLIC/INTC 인터럽트 컨트롤러 등 지원
     * File systems
          + ramfs, devfs, sysfs 지원
          + FAT16, FAT32는 읽기 전용으로 지원되며, 메모리 매핑 가능
          + VFS 존재, 단 블록 디바이스 미지원(모든 것이 메모리 내에서 동작)
     * Processes and signals
          + fork, vfork, waitpid, rt_sigaction 등 기본 프로세스/시그널 기능 지원
          + 사용자 공간 멀티스레딩 미지원이나, libmusl 요구사항에 따라 TLS (set_thread_area) 구현됨
     * I/O
          + read, write 외에 readv, writev, select, poll 지원
          + epoll은 미지원
     * Console
          + 리눅스 콘솔 기능의 90% 이상을 구현
          + 텍스트/프레임버퍼 모드 모두 지원
          + Vim이 Tilck에서 완벽히 작동할 정도로 구현됨
     * Userspace applications
          + BusyBox, Vim, Micropython, Lua, fbDOOM 등 콘솔 및 프레임버퍼 앱 실행 가능

Booting Tilck

     * Tilck's bootloader
          + Tilck는 BIOS/UEFI 모두 지원하는 부트로더 포함
          + 비디오 모드, 커널 선택, 커맨드라인 편집 가능
     * 3rd-party bootloaders
          + Multiboot 1.0 지원 부트로더 (예: GRUB)로 Tilck 부팅 가능
          + GRUB 설정 예시도 제공됨

A comment about user experience

     * Tilck는 초보 개발자도 쉽게 빌드/테스트할 수 있게 설계됨
     * 종속성 설치 부담 없이, 자체 툴체인 빌드 스크립트 제공
     * buildroot와 유사하지만 훨씬 단순
     * 다양한 QEMU 실행 스크립트도 제공

        Hacker News 의견

     * xv6와 같은 옛날 Unix(1975년 버전, Lion book 참고)과 풀 Linux 커널 사이의 중간 지점에 해당하는 흥미로운 프로젝트임을 느낌, LicheeRV Nano라는 $9짜리 RISC-V 보드(1.0GHz 64비트 CPU(C906), MMU, FPU, 128비트 벡터 유닛, 256MB DDR3 탑재)에서 동작하는 모습을 보고 반가움을 느낌, 이 보드는 펜티엄 III 중기나 PowerPC G4와 비슷한 성능임, 같은 SoC를 사용하는 Milk-V Duo 256M이나 Duo S, 혹은 Duo(64MB RAM, $5과 같은 모델)로의 포팅도 매우 쉬울 것 같음, 현재 네트워크나 블록 디바이스, 멀티코어 지원은 없는 상태임
          + 네트워크와 멀티코어 지원이 없는 Linux는 상상이 되지만, 블록 디바이스까지 없는 OS는 낯섦, 캐릭터 디바이스만 의미하는 것인지 궁금함, FAT 드라이버는 어떻게 동작하는 것인지 궁금함
     * Altivec를 탑재한 G4는 멀티미디어에서 SSE2를 가진 PIV와 거의 비슷한 수준의 성능을 냄
     * 몇 달마다 하드웨어 추상화 없이 VM에서만 동작하는 새로운 OS 커널이 등장하는 모습을 봄, Tilck은 그런 종류가 아님, Tilck은 진짜 운영체제임, 실제 하드웨어에서 동작함, 전자의 영역은 이미 포화상태인데 반해 Tilck은 예전부터 거의 채워지지 않았던 틈새를 잘 채워주는 느낌임
     * Tilck이 부팅 속도가 매우 빠르고 doom도 framebuffer로 구동하는 모습이 인상적임, https://www.youtube.com/watch?v=Ce1pMlZO_mI 링크에서 볼 수 있음, 개발자가 유튜브에서 CS 지망생의 질문에 직접 친절하게 답변하는 모습도 보기 좋았음
          + 영상이 짧고 핵심적임, Vim 실행 모습도 보여주는데, 여러 Linux syscall의 일부만으로 Vim 같은 대형 소프트웨어까지 컴파일해서 돌리는 것은 꽤 인상적임
     * Tilck이 ""교육용""이라고 표시된 것은 확인했지만, 부트로더만 교체한다면 소형 임베디드 기기에서도 쓸만한지 궁금함
     * Tilck의 README 파일이 예상외로 길고 재미있어서 OS 개발자라면 꼭 읽어볼 만함
     * Minix와 함께 교육용으로 자라온 나 같은 사람들에게 Tilck이 얼마나 유용할지 궁금함
          + Tilck은 매우 작고 결정론적인 모노커널임, 현재 약 100여 개의 Linux syscall을 리눅스 호환 형태로 구현함, 교육적 도구로도 좋지만 장기적으로는 리눅스 호환 RTOS 커널을 지향함, 현재는 musl로 정적으로 링크된 바이너리만 지원하고 부팅 및 구동에 약 3MB RAM만 소요됨, 코드베이스가 작고 단순해서 교육적 목적에 맞으며, 궁극적으로는 임베디드 시스템의 실제 프로덕션 환경에서도 쓰이는 것이 목표임
     * Tilck 관련한 과거 HN 토론 링크를 공유함:
          + 3년 전(75개 댓글): https://news.ycombinator.com/item?id=34295165 (그때는 riscv64 미지원)
          + 5년 전(7개 댓글): https://news.ycombinator.com/item?id=28040210
     * Tilck이 흥미롭지만 멀티유저 지원이 없는 점이 아쉬움, 최소한 chmod/chgrp 등 파일 소유자와 그룹 변경 정도만 지원하면 NFS 서버 같은 곳에도 활용도가 커질 것 같아서, 멀티유저 지원을 개발자가 다시 고려해줬으면 하는 바람임
          + Tilck의 파일시스템 호환성이 그보다 더 큰 문제임, 사실 내 용도라면 신뢰성이 검증된 플랫폼이 낫고, Tilck은 교육용에 맞춰져 있어서 정보보안이나 데이터 견고성 측면에서는 적합하지 않음
          + user:group 값을 파일시스템에 기록만 해두고 런타임에서는 클라이언트를 서비스할 때만 반영하는 방법도 생각 가능함, 예를 들어 리눅스 파일서버가 루트로 동작하면 자체 사용자 변경 없이도 권한 체크와 소유권 관리를 할 수 있음, 또 한 세션마다 포크해서 해당 클라이언트 유저로 전환하면 커널이 강제로 권한을 부여해줘서 파일 권한 처리도 자동으로 해결됨
     * 이 프로젝트는 정말 인상적임, 예전 3.5인치 플로피로 NAT 방화벽용 Linux를 돌리던 시절이 떠오름
"
"https://news.hada.io/topic?id=22058","lsr - io_uring 기반의 초고속 ls","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       lsr - io_uring 기반의 초고속 ls

     * lsr은 io_uring 기반 IO 라이브러리 ourio를 활용해 개발된 새로운 ls(1) 대체 프로그램
     * 기존 ls 및 대안 도구(eza, lsd, uutils ls)에 비해 명령 실행 속도가 매우 빠르며, 시스템 콜 수도 10배 이상 적음
     * 디렉토리 오픈, stat, lstat 등 모든 주요 IO를 io_uring으로 비동기·배치 처리하여 성능 극대화. 파일이 많을수록 더 빠름
     * Zig의 StackFallbackAllocator를 활용해 메모리 할당 시 mmap 호출을 최소화함
     * 동적 링킹 없이 정적으로 빌드해 실행 파일 크기까지 기존 ls보다 더 작음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 의의

     * lsr 프로젝트는 일반 ls 명령어의 대체제로서 io_uring을 활용하는 빠른 디렉토리 리스팅 도구
     * 기존의 ls, eza, lsd, uutils ls와 비교해, 실행 속도와 시스템 콜 사용량에서 탁월한 성능을 보임
     * 직접 개발한 io 라이브러리(ourio)로 최대한 많은 IO를 직접 수행
     * 벤치마크를 통해 lsr이 대규모 파일환경에서도 빠른 성능과 품질을 증명함

벤치마크 결과

     * hyperfine을 사용해 n개의 일반 파일이 있는 디렉토리에서 각 명령의 실행 시간을 측정
          + lsr -al 10–10,000개 파일 기준, 기존 ls/대체제 대비 월등히 짧은 실행 시간을 기록함
          + 예: 10,000개 파일에서 lsr이 22.1ms, 기존 ls(38.0ms), eza(40.2ms), lsd(153.4ms), uutils ls(89.6ms) 대비 최고의 속도를 기록
     * 시스템 콜 집계는 strace -c로 진행
          + lsr -al: 최소 20회(n=10)부터 최대 848회 (n=10,000)로 매우 낮은 콜 수 유지
          + ls는 최대 30,396회(n=10,000), lsd가 100,512회 등 나머지 대체제도 수천~십만 회 수준임
          + 동일 조건에서 lsr이 최소 10배 이상 적은 syscall 수로 최고의 효율 달성

lsr의 구조와 구현 방식

     * 프로그램은 인자 파싱, 데이터 수집, 데이터 출력의 3단계로 동작
     * 모든 IO는 두 번째 데이터 수집 단계에서 발생하며, 가능한 모든 파일 접근/정보 조회를 io_uring으로 처리
          + 타겟 디렉토리 오픈, stat, lstat, 시간/유저/그룹 정보 조회 모두 io_uring 기반으로 수행
          + stat을 배치 처리하여 시스템 콜 수를 획기적으로 줄임
     * Zig StackFallbackAllocator로 메모리 1MB를 사전 할당해 mmap 등 추가 시스템 콜을 최소화

정적 빌드와 최적화

     * libc 동적 링킹 없이 완전 정적 빌드이므로 실행 오버헤드가 현저히 적음
     * GNU ls 대비 lsr의 ReleaseSmall 빌드 사이즈가 138.7KB vs 79.3KB로 더 작음
     * 단, lsr은 로케일(언어/지역) 지원이 없음. 일반 ls는 다양한 언어 지원을 위해 오버헤드 발생

시스템 콜 및 성능 이슈 분석

     * lsd는 파일당 clock_gettime을 5번 이상 호출, 그 이유는 불명확(내부 타이밍 측정 등 추정)
     * 정렬(sorting) 작업이 전체 작업 중 상당 부분(약 30%)을 차지함
          + uutils ls는 시스템 콜 효율은 높으나 정렬 처리에서 느려짐
     * io_uring 도입만으로도 서버 등 고부하 IO 환경에서 혁신적 성능 향상 가능성 확인

결론

     * 개발 시간도 오래 걸리지 않고, syscall 최적화 효과가 기대 이상임
     * lsr은 빠른 속도, 적은 시스템 콜, 간결한 크기를 동시에 달성하는 실험적 ls 대체제임
     * 대용량 파일 환경이나 고성능 IO가 중요한 시스템에 매우 적합
     * locale 미지원 등 일부 기능 한계가 있지만, 실무와 벤치마크 모두에서 혁신적인 결과를 보임

        Hacker News 의견

     * 프로젝트의 작성자임을 밝힘과 동시에, io_uring 기반 lsr에 대한 소개글을 여기에서 확인 가능함
          + Sun에서 I18N 프로젝트를 했던 경험을 공유함. 여러 환경(지역화, utf8 등)을 지원하려면 프로그램에 다양한 처리를 추가해야 하므로 결과를 내는 데 드는 비용과 속도가 반비례함을 체감했음. 본래 UNIX의 ls(1)은 단순 설계에 굉장히 빨랐지만, 다양한 기능 추가 및 가상 파일 시스템(VFS), 다양한 문자셋, 컬러 지원 등 작은 비용이 계속 누적되어 느려짐. io_uring이 다루는 추상화 비용에 대해 재밌는 논의라고 생각함
          + bfs 프로젝트도 io_uring을 사용함 (소스 코드 링크). lsr와 bfs -ls의 성능 비교가 궁금해짐. 현재 bfs는 멀티스레딩 시에만 io_uring을 쓰고 있지만 single thread(bfs -j1)에도 활용할지 고민해볼 만함
          + tim (소개 링크)을 통해 시간 측정하면 hyperfine보다 더 나을 것 같음. Nim으로 작성되어 도전일 수 있지만 이름이 비슷한 게 우연치고는 재밌음
          + C++ 프로젝트를 Zig로 포팅하는 것을 염두에 두고 있음. 직접 만든 ‘libevring’도 아직 초기라, 필요하면 ourio로 대체 가능하다는 열려있는 마음임. Zig 기반 프로젝트에 C/C++ 바인딩 지원이 있으면 C/C++에서 Zig로 이주할 때 유용하리란 생각임
          + 해당 소개글이 배경 설명이 더 잘 되어 있으니 메인 링크로 삼고, 레포 스레드는 위에 추가해둘 계획임
     * NFS 서버에서 (특히 좋지 못한 네트워크 환경에서) lsr의 성능이 어떨지 궁금함. 불안정한 네트워크 서비스에 blocking POSIX syscall을 사용하는 게 NFS 설계의 단점임이 자명. io_uring이 이런 문제를 얼마나 완화할 수 있는지도 관찰 포인트임
          + NFS 설계자는 분산 시스템을 하드 드라이브처럼 매우 일관되게 동작하게 구현했음. 기존 툴(ls 등)이 네트워크 오류 상황을 직접 처리할 필요가 없던 건 장점이었음. 원래 NFS 프로토콜은 상태를 저장하지 않아 서버 재부팅에도 클라이언트가 자동 복구됨. io_uring이 이런 케이스에서 에러를 제대로 넘겨주는지가 궁금함. NFS 타임아웃 시 어떤 방식으로 처리되는지도 관심사임
          + 집에서 여러 대의 PC에서 NFS $HOME을 사용하는데, 네트워크가 좋고 병렬 쓰기 같은 어려운 케이스만 피하면 NFS의 평균적인 사용성은 꽤 만족스러움. 단, 네트워크 케이블이 불안정했을 때 끊김 현상 때문에 힘든 적은 있었음
          + NFS 폴더를 읽고 있는 앱에서 ctrl+c가 안 먹히는 상황은 잘 알려진 불편함임. 이론적으로는 intr 마운트 옵션이 행중인 원격 서버에서의 오퍼레이션에 시그널을 전달해 중단 가능하도록 지원했으나, 리눅스에선 이미 오래 전에(현재는 soft 옵션만 가능) 제거되었음(참고1, 참고2(FreeBSD 지원))
          + Samba도 이와 비슷한 이슈가 있음
     * syscall 호출 수를 35배 줄였는데도 속도 개선은 2배 정도라는 점이 흥미로움
          + 대부분의 syscall이 VDSO를 통해 이뤄지기 때문에 큰 비용이 들지 않음
          + 예전에 읽었던 io_uring 관련 벤치마크에서, io_uring 기반 syscall이 기존 syscall보다 무거운 것으로 나오기도 했었음. 그렇다고 해도 체감상 상당히 큰 개선임. 정확한 출처는 생각이 안 나지만 인상적으로 남아있음
     * io_uring 활용 사례로서 기대했던 장기적인 속도 이득, 혹은 튜토리얼 사용법 소개로 더 관심이 가는 프로젝트임. 기존 eza같은 툴 대비 왜 이것이 필요한지 체감적 동기부여가 없었음. 파일 만 개 리스트의 실행이 40ms vs 20ms라면, 단일 실행 기준으로는 전혀 차이를 느끼지 못할 것 같음
          + 재미 삼아 io_uring 사용법을 익히고 싶어 만든 실험적 프로젝트임. 실질적으로 얻는 시간 절감 효과는 미미하고(살면서 5초 절약 수준), 이게 핵심 포인트는 아니었음
          + 실제로 수백만 개의 JSON 파일이 들어있는 디렉토리에서는 ls/du 실행에 수 분이 소요됨. coreutils 기본 명령어는 최신 SSD 성능을 제대로 활용하지 못하는 경우가 많음
     * lsr도 좋지만 컬러링 및 아이콘 지원은 eza가 더 우수함. 본인은 ""eza --icons=always -1""로 세팅하고 있어서, 음악 파일(.opus 등)은 알아서 아이콘과 컬러로 표시되는데 lsr에서는 그냥 평범한 파일로 표시됨. 그래도 lsr은 패치도 쉽고 엄청 빠르다는 점은 확실하게 느낌. 더불어 cat이랑 다른 유틸리티도 이런식으로 만들어주면 좋겠다는 기대, tangled.sh 및 atproto 사용도 신기하게 여김. zig로 작성되어 있어 rust보다 초보 입장에서 더 쉽게 느껴짐
          + “bat”는 현대적인 “cat” 대체제임 (bat 바로가기)
          + 컬러링 지원은 LS_COLORS/dircolors처럼 표준 방식을 구현하는 게 제일 좋을 것 같음. GNU ls는 컬러가 예쁘게 나옴
     * 왜 모든 CLI 툴들이 io_uring을 사용하지 않는지 궁금했음. 본인은 nvme를 usb 3.2 gen2에 연결할 때, 평범한 툴로 속도가 740MB/s인데 aio나 io_uring 기반 툴로는 1005MB/s까지 올라감. 큐 길이 전략이나 lock 감소 효과도 있다고 봄
          + 포터블 구축을 위해 전통적으로 #ifdef 같은 매크로 분기 없이 짰기 때문에 플랫폼/버전 전용 신기술 도입이 느림. 이제 와선 다양한 posixy 플랫폼 간 호환성의 이점은 예전만 못하다고 생각함
          + io_uring을 효율적으로 쓰려면 비동기 이벤트 기반 모델이 필요함. 기존 CLI 툴 대부분은 직관적이고 순차적으로 작성되어 있음. 언어 차원에서 async가 자연스럽게 쓰인다면 더 포팅이 쉬웠겠지만, 지금은 큰 리팩터링이 필요함. io_uring도 완전히 안정화된 건 아니어서, 새로운 기술의 등장까지 지켜보고 자동 포팅 툴/AI 등이 나올 수도 있다고 생각함
          + io_uring은 도입 초기 주요 보안 이슈가 있었음(2년 전쯤). 지금은 상당수 해결됐지만 보급에는 악영향을 미쳤음
          + io_uring이 보안 측면에서 매우 어려움
          + io_uring이 워낙 최신 기술이라 coreutils(그리고 선행 패키지)는 수십 년 된 전통이 있고, io_uring 도입까지 시간이 더 걸릴 거임. “공유 링 버퍼” 방식의 시스템 콜이 기존 동기 방식 대신 표준이 되려면 시간이 필요함
     * lsd가 파일 하나당 clock_gettime을 5번쯤 호출하는 현상이 strace로 관찰됨. 원인을 정확히 모르겠으며, 아마 각 시간스탬프별 “몇 분/시간/일 전”을 계산하거나, 라이브러리 레거시일 수도 있음
          + 요즘 clock_gettime은 진짜 syscall이 아니라 vDSO를 통해 처리됨(man 7 vDSO 참고). 혹시 zig가 이 구조를 활용하지 않는 게 아닐까 하는 생각임
     * 약간 주제 이탈일 수 있지만, Mellanox 4 또는 5 등 고엔터프라이즈 서버에서 10G NIC환경 하에서 io_uring이 LD_PRELOAD 대비 소켓 지연 오버헤드를 마이크로초 단위로 얼마나 줄여주는지 실제 경험치나 벤치마크 수치가 궁금함. 서로 효과가 누적되지 않는 듯하며, 직접적인 경험이 있다면 수치를 듣고 싶음
     * io_uring은 getdents를 지원하지 않아 주효한 이점은 bulk stat(예: ls -l)으로 나타남. getdents 처리를 비동기화하고 중복 처리할 수 있다면 좋겠다는 아쉬움 있음
          + POSIX에서 NFS의 “readdirplus”(getdents + stat) 연산을 표준화하면 io_uring만의 이점 일부가 상쇄될 것으로 예상함
     * .mjs와 .cjs 확장자에 대한 아이콘은 있지만, .c, .h, .sh와 같은 파일 확장자에는 없는 점이 재밌게 느껴짐
"
"https://news.hada.io/topic?id=22007","사상 최대 질량의 블랙홀 합병, LIGO를 통해 관측됨","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     사상 최대 질량의 블랙홀 합병, LIGO를 통해 관측됨

     * LIGO-Virgo-KAGRA 협력단이 중력파로 관측된 사상 최대 질량의 블랙홀 합병을 탐지함
     * 이번 합병으로 태양 질량의 약 225배에 해당하는 블랙홀이 형성됨
     * 이 합병은 기존 표준 별 진화 이론으로 설명할 수 없는 고질량으로, 이론과 관측의 한계를 시험함
     * 관련 과학자들은 급격한 회전과 복잡한 신호 분석으로 인해 블랙홀 연구 및 알고리듬 개발이 진전될 전망임
     * 이번 관측은 중력파 천문학의 데이터 분석, 기기 기술, 이론 발전의 새로운 전환점임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

LIGO, Virgo, KAGRA의 사상 최대 질량 블랙홀 합병 탐지

   LIGO-Virgo-KAGRA(LVK) 협력단은 미국 국립과학재단(NSF)에서 지원하는 LIGO 관측소를 이용해, 중력파로 관측된 사상 최대 질량의 블랙홀 합병 현상을 발견함. 이번 합병에서 형성된 최종 블랙홀은 태양 질량의 약 225배에 이르는 것으로 밝혀짐. 해당 중력파 신호는 GW231123으로 명명되어, 2023년 11월 23일에 LVK 네트워크의 네 번째 관측 기간 중 탐지됨.

LIGO의 역사와 진화

   LIGO는 2015년 중력파의 최초 직접 관측을 성공시켜 큰 주목을 받았으며, 당시에도 블랙홀 충돌 후 태양 질량의 62배 크기 블랙홀을 감지함. 루이지애나 리빙스턴과 워싱턴 핸포드에 각각 위치한 LIGO 쌍둥이 탐지기가 이 신호를 공동으로 포착함. 이후 LIGO는 이탈리아의 Virgo, 일본의 KAGRA와 협력해 LVK 협력단을 구성함. 2015년 이후 네 번의 관측 주기 동안 300건 이상의 블랙홀 합병 현상을 관측함.

최근의 기록적 합병 사건

   이전에 가장 질량이 컸던 블랙홀 합병은 2021년의 GW190521 사건으로, 총 질량이 태양의 140배였음. 이번 GW231123 사건에서는 태양 질량의 100배와 140배에 해당하는 블랙홀 두 개가 합쳐져 225배 질량의 블랙홀이 만들어짐. 이 블랙홀들은 매우 빠른 속도로 회전하고 있는 것으로 추정됨.

   LVK 협력단의 Mark Hannam은 ""관측된 이 블랙홀 이중계는 기존 별 진화 이론으로는 설명이 어려우며, 아마도 더 작은 블랙홀들의 중첩적 합병이 원인일 가능성을 내포함""이라고 밝힘. LIGO의 Dave Reitze는 ""중력파 관측을 통해 블랙홀의 본질과 우주의 이색적인 특성을 밝히는 데 큰 진전이 있었음""이라고 언급함.

기록 갱신과 과학적 도전

   GW231123에서 나타난 고질량 및 극한의 고속 회전은 현재 중력파 검출 기술과 이론 모델의 한계를 시험함. 아인슈타인 일반상대성이론이 허용하는 한계에 근접하는 빠른 회전으로 인해 신호 해석과 모델링이 매우 어려워짐. Portsmouth 대학의 Charlie Hoy는 ""이 사례는 이론적 도구와 알고리듬 개발의 중대한 진전 기회를 제공함""이라고 평가함.

   연구진은 이 신호의 패턴과 의미를 완전히 해독하는 데 수년이 걸릴 것으로 예상함. Birmingham 대학의 Gregorio Carullo는 ""합병 자체가 가장 유력한 설명이나, 기존 이론으로 설명되지 않는 복잡한 현상으로 인해 새로운 해석의 실마리 가능성도 내포함""이라고 분석함.

중력파 천문학의 한계를 확장함

   LIGO, Virgo, KAGRA와 같은 중력파 탐지기는 우주 초대형 물리 현상에 의해 발생하는 미세한 시공간 변형을 측정함. 이번 네 번째 관측 주기는 2023년 5월부터 시작되었으며, 추가 데이터는 2024년 여름에 공개 예정임. Caltech의 Sophie Bini는 ""이번 사건이 데이터 분석과 기기 기술의 현 한계를 극복하는 실제 사례이며, 향후 중력파 천문학 연구에 많은 가능성을 시사함""이라고 설명함.

   GW231123 결과는 2025년 7월 14~18일, 스코틀랜드 글래스고에서 열리는 GR24/Amaldi 컨퍼런스에서 발표될 예정임. GW231123에 사용된 보정 데이터는 Gravitational Wave Open Science Center(GWOSC)를 통해 공개되어, 국내외 과학자들이 추가 연구에 활용할 수 있음.

LIGO-Virgo-KAGRA 협력단 소개

     * LIGO는 미국 NSF의 지원으로 Caltech과 MIT에서 운영하며, 독일(Max Planck Society), 영국(Science and Technology Facilities Council), 호주(Australian Research Council)에서 주요 지원을 받음. 1,600명 이상의 전 세계 과학자들이 참여함
     * Virgo Collaboration은 유럽 17개국 152개 기관 소속 약 880명으로 구성됨. 이탈리아 Pisa 근교에 위치한 Virgo 탐지기는 EGO(유럽중력파관측소) 및 CNRS(프랑스 국립과학연구센터), INFN(이탈리아 국립핵물리연구소), Nikhef(네덜란드 국가핵물리연구소)가 공동 지원함
     * KAGRA는 일본 Gifu의 Kamioka에 3km 암장 길이의 레이저 간섭계로 위치하며, 도쿄대 ICRR(우주선연구소), 국립천문대(NAOJ), 고에너지 가속기연구기관(KEK)이 공동 주관함. 17개국/지역 128개 기관 400명 이상이 참여함

   추가 정보 또는 연구자료는 각 기관의 공식 웹사이트에서 확인 가능함

        Hacker News 의견

     * 약 225 태양질량의 블랙홀은 각각 약 100 및 140 태양질량의 블랙홀이 융합되어 만들어짐을 의미함, 그렇다면 15 태양질량이 에너지로 전환된 것인지 궁금함, 왜냐하면 그건 엄청난 양의 에너지이기 때문임
          + Tsar Bomba 핵무기가 약 2.3kg의 물질을 에너지로 변환했다고 볼 수 있음, 태양 한 질량은 약 2 x 10^30 kg이므로, 이번 사건은 10^31개의 Tsar Bomba와 같은 에너지를 방출한 셈임, 이 정도 숫자는 직관적으로 와닿지 않아 다시 생각해봄, 태양은 수명 전체 동안 약 0.034%만을 에너지로 방출함, 즉 태양 한 질량만큼의 에너지는 태양 3,000개 수명 전체와 맞먹음, 이번 사건에서 방출된 에너지는 태양 약 45,000개 수명 전체 에너지와 같음, 이 중 대부분이 합병 마지막 몇 초 동안 방출됐을 거라 생각함, 참고: 에너지 환산 계산 참고 자료, 태양 질량 손실 참고 자료
          + 에너지로 전환되어 블랙홀에서 빠져나왔다는 것인데, 빛조차 빠져나올 수 없는 블랙홀에서 어떻게 그렇게 되는지 잘 이해가 되지 않음, 만약 중력파의 형태라면 대부분의 에너지가 이 방식으로 탈출한다는 당연한 결론에 도달함, 호킹 복사를 기다릴 필요가 없음
          + 그 질량은 어떤 형태의 에너지로 전환되는 것인지 궁금함
          + 인간이 상상할 수 있지만, 그 순간에 관측 가능한 우주의 모든 별이 내보내는 에너지보다 더 많음
          + 맞음, 그럼에도 중력은 너무 약하여 이 엄청난 에너지가 지구와 달 사이 거리에서 머리카락 굵기 정도의 상대적 수축(10^-20 미만)으로 나타남
     * 이 현상은 정말 흥미로움, ""블랙홀들이 매우 빠르게 회전하고 있으며, 이는 일반상대성이론이 허용하는 한계에 거의 접근함""이라고 University of Portsmouth의 Charlie Hoy가 설명함, 그로 인해 신호 모델링과 해석이 어려움, 이 케이스는 이론적 도구 개발을 진전시키는 데 훌륭한 스터디 케이스임
          + 자연이 우리에게 일반상대성이론에 대한 스트레스 테스트를 던져준 느낌임
          + 구형 천체가 회전하는 것만으로도 중력파가 발생하는지 궁금함
     * 한 달 전 제안된 NSF 예산안에서 미국 내 두 곳 중 한 곳의 LIGO 관측소를 폐쇄할 가능성이 있었으며, 이는 이런 블랙홀 병합과 같은 이벤트의 위치를 삼각측량하는 능력을 크게 망칠 것임, 폐쇄는 노이즈 한계와 탐지율에도 심각한 타격을 주게 됨, 아직 폐쇄 계획이 실현되는지 아는 사람 있는지 궁금함, 참고 링크
          + 제안 예산안은 내일(7월 15일, 12:00)에 심사될 예정임, 현재 NSF 예산은 약 70억 달러로 FY2025 대비 23% 삭감됨, 정확히 LIGO에 어떤 영향을 주는지는 모르겠음, 예산안 상세 링크
          + 지난주 Pisa의 virgo ego(사실상 LIGO의 사촌) 이벤트에 참석함, 중력파 발견 10주년 기념 행사였는데, 이탈리아 프로그램 디렉터가 쓴 책을 배우가 낭독했고, 색소폰으로 파동 소리를 연주함, 감동을 말로 표현할 수 없음, virgo 센터 소장과 과학 커뮤니케이터를 인터뷰하는 시간도 있었는데, LIGO의 예산 삭감 가능성에 대해 소장은 꽤나 분노한 상태였음, 그럴만함
          + FY 2026 최종 예산안이 두 곳의 LIGO를 유지하는지 계속 지켜볼 필요 있음, 그 전까지는 여전히 실질적인 위험임, 하지만 아직 완전히 돌이킬 수 없는 상황은 아님
          + 현재 전세계적으로 몇몇 중력파 관측기가 운용되고 있는데, 왜 한 곳의 LIGO 폐쇄가 삼각측량에 치명적인지 궁금함
          + 어쩌면 2023년의 이번 발견이 지금에서야 논문으로 발표되는 것이 그 이유일 수도 있음
     * 난 정말 좋은 소식이 필요함, 이런 종류의 발견이 언젠가 실질적으로 인류의 삶을 더 낫게 실용적으로 활용될 수 있는 길이 있을지 (아주 간접적이어도 됨) 상상력을 자극해 줄 수 있는지 묻고 싶음, ""기초 연구의 유용성"" 논쟁은 아니고, 그 자체로 가치 있다는 점은 동의하지만, 이게 장기적으로 어떻게 유용할지 상상이 잘 안 됨
          + 비전문가지만 흥미로워하는 사람임, 이런 발전에는 분명 긍정적인 점이 있음, 그 중 하나는 중력파가 우주 초기의 사건을 우리에게 알려 주는 신호가 된다는 점임, 예를 들어 코스믹 마이크로파 배경 복사(CMB)는 빅뱅/인플레이션 직후 방출된 초기 광자의 신호임, 하지만 우주는 처음 30만 년간 광자에 불투명했음에도 우리는 이 데이터를 토대로 우주론 이론을 검증·반증해 왔음, 그런데 중력파는 광자와 달리 무엇에도 막히지 않으며 우주 창조 시점부터의 신호를 전달하므로 더욱 명확한 정보를 줄 수 있음, 이로 인해 양자역학·상대성이론 등 기초 물리의 새로운 인사이트가 가능해질 수 있음, 이것은 포톤·중성미자·중력파 세 가지로 이벤트를 관측하는 멀티 메신저 천문학으로 이어져 더욱 깊은 통찰을 얻을 수 있다고 생각함, 이런 기초 물리의
            진보가 장기적으로 지구에서의 삶을 개선시킨 사례가 많았다는 점에서 낙관적인 시선을 가질 수 있음, 세상이 좀 더 좋아진다는 희망을 갖기를 바람
          + “이 연구가 장기적으로 어떻게 유용해질까?”라는 질문엔 솔직히 모름, 다만 블랙홀은 과학적으로 우리가 아는 한계에 가장 근접해 있음, 사건의 지평선 너머에서 무슨 일이 일어나는지는 전혀 모름(실험적으로도 영원히 모를 수 있음), 더 많이 알게 되면 때로는 돌파구가 생겨 기술 발전이 뛰어넘게 되는 경우가 종종 있음, 잠재력이 가장 큰 분야임, 대부분의 경우 진전은 실제 관련 업계 외에는 꽤 ‘지루함’
          + 이런 연구의 실질적 유용성은 “결과 자체”가 아니라, 그 결과를 얻기 위한 방법론에 있음, LIGO는 극도의 정밀 레이저, 안정된 플랫폼, 극한의 위치 측정, 엄청난 소프트웨어 등이 필요함, 이런 “필요”가 실제 발전과 혁신을 불러옴, 예를 들어 천문학의 부수 효과로 CMOS 센서(디지털 카메라)가 만들어짐, 휴대폰 카메라를 쓸 때 “이게 별 거리 측정 연구에서 나왔다”고 생각하지는 않지만 그런 효과임
          + 역사 속에서 부유한 문명들은 자신들의 위대함을 보여주기 위해 기념비적인 건물을 지음, 이처럼 우리는 지금 기초 연구에 사회의 생산성을 투자하는 큰 예술작품을 만드는 것과 같음, 블랙홀 병합 탐지는 실용적 이득이 없음에도 우주의 본질을 발견하려는 지적 기념비임, 고대 이집트인들이 지금까지 기억되는 것처럼 우리의 업적도 오래 남길 바람
     * 블랙홀의 이벤트 호라이즌(사건의 지평선)은 항상 구형이라고 생각했음, 그런데 내 물리학적 직관은 두 블랙홀이 병합할 때 병합 직후 블랙홀이 최소한 초기에는 “땅콩 모양”이 되지 않을까라는 생각이 듦, 내부 질량 분포에 따라 불규칙한 형태가 계속 유지될 수도 있음
          + 이벤트 호라이즌이 구형이 되는 것은 슈바르츠실트(비회전) 블랙홀일 때임, 회전하는 블랙홀은 Kerr 블랙홀이라 불리며 기이한 현상이 많음, 외부에는 ergosphere라는 이상한 외곽 경계가 있고, 이곳에서는 시공간이 끌려가 정지해 있을 수 없고 블랙홀을 이용해 물체에 가속을 줄 수도 있음, 내부에는 Cauchy horizon이라는 더 이상한 경계가 있어 이론적으로 시간여행이 가능함, 특이점은 고리 형태임, 병합 과정에선 이 현상이 훨씬 이상해질 것이라고 생각함, Kerr metric 위키, Kerr 블랙홀 연구 논문, Ergosphere 위키, Cauchy horizon 위키, 연구를 하면서 업데이트함, 복잡해서 정확히 맞는지는 장담 못하지만 최선을 다한 설명임
          + 사건의 지평선 형태에 대해 말하는 것은 어렵다고 생각함, 왜냐하면 보통 구의 정의가 “한 점에서 등거리인 모든 점의 집합”이지만 미분 가능한 다양체에서는 이미 복잡하며, 특이점으로 인해 거리가 무한대가 되어버리거나 기하구조상 기준점이 유일하지 않을 수도 있음, 그래서 보통 “구와 동일한 위상(topology)을 갖는 일정한 스칼라 곡률의 면”으로 정의를 바꿈, 이것이 평면이나 쌍곡면과 구분되는 점임, 내 직관으론 Kerr 블랙홀이나 병합 중 블랙홀의 경우 박하 모양(땅콩 모양)이 될 것 같음(아마 안장점도 존재할 것임), 좌표적으로는 확실히 그렇지만 좌표계 선택에 따라 Schwarzschild 블랙홀도 좌표상 땅콩 모양이 될 수 있음, 그래서 좌표는 크게 의미 없다고 생각함
          + MIT/CalTech에서 만든 병합 애니메이션이 있음, 애니메이션 영상
          + 우리의 관점에서는 이벤트 호라이즌이 실제로 완성된 상태는 아님, 붕괴하는 별이 블랙홀 상태에 도달하는 데는 외부 관찰자 기준으로 무한한 시간이 걸림, 대부분 상황에선 붕괴 별이 블랙홀인 것처럼 보이지만, 블랙홀 병합 과정에서는 이벤트 호라이즌이 완전히 형성되지 않아 에너지가 방출될 수 있음, 이런 경우엔 중요한 차이가 생김
     * 한 블랙홀이 초상대론적 속도로 다른 블랙홀을 관통하듯 지나가면 무슨 일이 생길지 궁금함
          + 블랙홀 주변의 시공간은 극한으로 휘어져 있음, “광속에 근접해서 서로 충돌한다”라고 쉽게 상상하지만, 블랙홀에선 시공간이 서로 엇갈려서, 한쪽이 다가올수록 속도가 완전히 멈춘 것처럼 보일 수도 있음, 관찰자의 위치와 속도에 따라 전혀 다르게 관측될 수 있음, 기본적인 것조차 합의하기 힘듦, 예를 들어 뭔가가 블랙홀에 빨려들어 가는 것도(그게 다른 블랙홀이어도) 외부에선 속도가 0에 수렴하며, 붉은색으로 소멸하는 듯 보일 뿐 실제로 떨어지는 순간을 볼 수 없게 됨, 정말 어렵고 직관에 어긋남
          + 두 블랙홀은 결국 합쳐져서 운동량을 합한 블랙홀이 됨, 이벤트 호라이즌에서 아무것도 빠져나올 수 없기 때문에 블랙홀은 사실상 완전히 끈적거리는 성질을 가짐
          + 이벤트 호라이즌 내부에서는 탈출 속도가 광속보다 빠르므로, 블랙홀은 그 이상 빠르게 서로 접근할 수 없음, 궤도가 완벽히 일치한다면 서로의 중력을 벗어날 수 없음, 두 블랙홀이 서로를 관통해서 지나간다기보다 초강력 자석이 충돌하는 모습과 비슷함
          + 이런 걸 실험해볼 수 있게 우주 입자가속기를 설치할 수 없다는 게 아쉬움
     * LIGO, Virgo, KAGRA가 이처럼 극한의 신호를 실제로 탐지하고 해석해낸다는 것이 신기함
     * LIGO의 예산 전망이 궁금함, 지난주 BBB 통과 시 예산 삭감됐는지 궁금함
     * 블랙홀이 충돌하면 무슨 일이 일어나는지 궁금함, 한 블랙홀이 다른 블랙홀을 “집어삼키는지”, 아니면 더 큰 블랙홀이 되는지, 더 밀도가 높아지는지 혹은 단순히 더 커지는지 알고 싶음
          + 더 큰 블랙홀로 합쳐지고, 질량 대부분은 보존되며 일부는 중력파로 방출됨, 질량이 반지름에 비례하기 때문에 병합으로 밀도는 오히려 낮아짐, 예를 들어 여러 블랙홀을 한줄로 늘어놓고 병합시킨다면 전체를 감싼 구 공간 자체가 블랙홀이 되는 셈임, 우주 전체 질량의 블랙홀은 우주만큼 큰 부피가 됨
          + 더 질량이 큰 블랙홀로 결합됨, 이벤트 호라이즌까지 포함한 부피는 오직 질량에 따라서만 결정되므로 어떻게 만들어졌든, 같은 질량이면 같은 밀도임, “집어삼킴”에 대해서는, 천 조각을 찢다가 두 개의 구멍이 만나 한 개로 합쳐지는 걸 보면 과연 큰 구멍이 작은 구멍을 삼킨 것이라 표현할 수 있을지 애매함
          + 내부에서 무슨 일이 일어나는지는 알 수 없음, 블랙홀은 질량, 스핀(각운동량), 전하 세 가지 양으로만 정의됨, 합병 후 이 양이 더해지는 것으로 예상함, 빠른 회전은 합병 후 스핀이 비등점에 가깝게 될 수 있고, 중력파가 초과 스핀의 에너지를 가져갈 수도 있음
          + 내 이해로는 두 블랙홀이 서로에게 돌면서 영원히 접근함, 우리의 입장에서는 실제로 블랙홀 안으로 뭔가가 떨어지는 걸 볼 수 없음, 시간 팽창 때문에 아무 것도 실제로 지평선을 넘는 걸 볼 수 없음, 자세한 설명은 여기에 있음 시간 팽창 관련 Q&A
          + 원리상 두 블랙홀은 질량이 합해지며 더 커진 블랙홀이 됨, 그렇게 증가한 질량이 더 큰 중력을 만들어 이벤트 호라이즌이 바깥으로 넓어짐
     * 웨이브폼(Chirp)이 없다면 일어난 게 아니라는 농담임
"
"https://news.hada.io/topic?id=22070","15kB 페이지보다 14kB 페이지가 훨씬 더 빠르게 로드될 수 있음 (2022)","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             15kB 페이지보다 14kB 페이지가 훨씬 더 빠르게 로드될 수 있음 (2022)

     * 웹사이트 크기를 14kB 이하로 유지하면 15kB일 때보다 로딩 속도를 크게 단축할 수 있음
     * 이 현상은 TCP 슬로우 스타트 알고리듬에 의해 발생하며, 첫 데이터 전송량 한계로 인해 체감 속도 차이가 나타남
     * 14kB는 대부분의 서버가 처음에 보내는 10개 TCP 패킷의 용량에 해당함
     * 위성 인터넷 등 높은 레이턴시 환경에서는 한 번의 추가 왕복(RTT)이 612ms 이상의 지연을 초래함
     * 실제로 14kB 미만으로 주요 콘텐츠를 넣거나, 첫 14kB 내에 중요한 리소스를 배치하는 것이 웹 성능 최적화에 효과적임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요 및 주요 원리

     * 웹사이트의 크기가 작을수록 빨리 로드된다는 점은 잘 알려진 사실임
     * 하지만 14kB에서 15kB로 넘어가는 순간, 첫 응답 속도에서 획기적 차이가 발생하는 점은 예상치 못한 사실임
     * 15kB와 16kB 페이지 사이의 속도 차이는 미미하지만, 14kB와 15kB 사이에는 최대 612ms 차이가 생길 수 있음

TCP란 무엇인가

     * Transmission Control Protocol (TCP) 는 IP(Internet Protocol) 위에서 작동하며, 패킷의 신뢰성 보장을 담당함
     * 웹 브라우저는 HTTP 요청 시 여러 개의 TCP 패킷을 전송함
     * IP만 사용할 때는 패킷이 도착했는지 확인할 수 없어서, TCP가 패킷 수신 확인(ACK) 기능을 제공함
     * 서버는 소량의 패킷을 먼저 보내고, 브라우저에서 ACK를 받으면 추가 패킷을 전송함
     * ACK가 없을 경우, 패킷 재전송 절차가 실행됨

TCP 슬로우 스타트란

     * TCP 슬로우 스타트는 서버가 네트워크 연결 품질(대역폭)을 파악하기 위해 단계적으로 패킷 전송량을 늘려가는 알고리듬임
     * 접속 초기에 서버는 소량(일반적으로 10개)의 패킷만 전송함
     * 방문자 컴퓨터에서 ACK를 정상적으로 보내면 패킷 전송량을 두 배로 증가시킴
     * ACK 누락이 발생하면 이후에는 느린 속도로 패킷을 보냄
     * 실제 알고리듬은 구현마다 세부 차이가 있지만 개념은 동일함

14kB 기준의 근거

     * 대부분의 서버는 슬로우 스타트에서 TCP 패킷 10개를 한 번에 보냄
     * TCP 패킷 최대 크기는 1500바이트이지만, 헤더(40바이트) 를 제외하면 1460바이트가 실질 데이터임
     * 따라서 10 x 1460 = 14600바이트(약 14kB) 가 첫 전송 극한치임
     * 웹사이트 또는 중요한 리소스를 14kB 이하(압축 적용 시는 원본 데이터 수십 kB 수준)에 맞추면, 시작 왕복 지연 없이 표시 가능함

한 번의 왕복이 얼마나 큰 지연을 유발하는가

  위성 인터넷 예시

     * 높은 레이턴시 환경의 대표 예로 위성 인터넷(석유 시추선, 유람선 등) 사용자가 있음
     * 휴대폰에서 홈페이지 요청 시, 라우터 → 위성 안테나 → 우주 위성 → 지상국 → 서버, 각 구간 이동에 수십~수백 ms 소요됨
     * 전체 전송 왕복에는 두 번의 우주 왕복, 네트워크 구간 이동, 서버 처리까지 포함해 약 612ms의 추가 지연 발생함
     * HTTPS를 사용할 경우, 추가 핸드셰이크로 인해 1836ms까지 늘어날 수 있음

  육상 네트워크의 레이턴시

     * 2G, 3G 등 모바일 네트워크에서도 100~1000ms까지 레이턴시가 발생함
     * 혼잡한 상황이나 서버 과부하, 패킷 손실 등 다양한 환경에서 추가 지연이 생길 수 있음

14kB 규칙을 적용한 웹사이트 최적화 전략

     * 웹사이트나 페이지를 가능한 한 작게 만드는 것이 핵심임
     * 각 페이지의 압축된 전송량이 14kB 이하가 되도록 설계하는 것이 이상적임
          + 압축 적용 시 실제 콘텐츠는 ~50kB까지 포함 가능
     * 자동재생 동영상, 팝업, 추적기, 불필요한 JS/CSS 등 대부분의 불필요 요소를 줄이면 충분히 달성 가능함
     * 만약 14kB로 전체 구현이 어렵다면, 첫 14kB에 핵심 리소스 및 주요 콘텐츠(CSS, JS, 주요 텍스트 등)를 우선 배치하는 것이 중요함
     * HTTP 헤더(압축 불가) , 이미지(필요 최소한/위치한 부분만 로딩 또는 플레이스홀더 적용)도 14kB 내에 포함됨

14kB 규칙의 예외와 최신 프로토콜 이슈

     * 14kB 규칙은 지나친 일반화는 아니지만, 몇 가지 예외 사항 존재함
          + 일부 서버는 초기 윈도우를 30패킷으로 확장함
          + TLS 핸드쉐이크를 통해 더 큰 윈도우 허용 가능성 있음
          + 라우트별로 전송 가능 패킷 수를 캐싱하여 다음 접속 시 더 많이 보낼 수 있음
     * HTTP/2에서도 서버가 TCP 슬로우 스타트로 10 패킷부터 시작하는 관행은 대체로 변하지 않음
     * HTTP/3, QUIC에서도 14kB 규칙이 공식적으로 권장됨

요약 및 참고 자료

     * 기술적 근거와 추가 설명 자료들은 각 링크를 통해 확인 가능함
     * 최초 발행: 2022-08-25, 최근 수정: 2022-08-26, 작성자: Nathaniel, 관련 태그: 웹 성능, HTTP, TCP

   참고 링크
     * Ethernet 프레임과 TCP 헤더 구조, 레이턴시 및 대역폭 관련 추가 자료, TCP/QUIC 사양 등

        Hacker News 의견

     * 소프트웨어 개발자는 미디어 계층에 좀 더 관심을 가질 필요가 있음, 특히 3G/5G의 신뢰성과 지연에 대해 저자가 짚은 점이 인상적임. 라디오는 거의 항상 재전송이 일어나고, 대부분의 HTTP 통신에서 패킷이 순서대로 도착해야 UI가 업데이트됨. 실제로 단일 REST 요청도 요청과 응답이 1400바이트 미만일 때에만 실제 한 패킷으로 처리됨. 그 이상이면 ‘단일’ 요청이 사실 여러 패킷으로 분할됨. 이 중 하나라도 문제가 있으면 모든 패킷이 순서대로 도착해야 화면이 제대로 갱신됨. 실험해보고 싶으면 Chrome 개발자 도구에서 3G 모드와 패킷 손실을 켜고 테스트하면 작은 최적화 하나만으로도 UI 반응성이 크게 개선되는 것을 볼 수 있음. 그래서 API와 UI를 최대한 작게 만드는 것이 설득력 있는 이유임
     * 내 홈페이지 전송 용량이 압축 기준 7.0kB임
          + /
          + main.css
          + favicon.png
          + 총합 7.0kB 블로그 리스트 및 전체 웹사이트는 내 커스텀 정적 사이트 제너레이터(공개: site.lisp, Common Lisp 사용)로 만듦. 수학 관련 포스트에는 KaTeX를 클라이언트 렌더링으로 사용 중인데, 이 경우 347.5kB나 추가됨
          + katex.min.css 23.6kB
          + katex.min.js 277.0kB
          + auto-render.min.js 3.7kB
          + KaTeX_Main-Regular.woff2 26.5kB
          + KaTeX_Main-Italic.woff2 16.7kB
          + 총 추가 347.5kB 언젠가 KaTeX를 서버 렌더링으로 바꿀까 고민 중임. 이 블로그는 대학 기숙사 시절부터 해온 나만의 열정 프로젝트임. 모든 HTML과 템플릿, CSS까지 직접 작성했고, 각 페이지에 꼭 필요한 요소만 넣으려 항상 신중하게 구성해서 작은 용량 유지 중임
          + 내 홈페이지
          + 수학 포스트 모음
               o 굳이 더 나은 방식을 쓰지 말라는 건 아니지만, LaTeX 표현 같은 동적 컴포넌트가 로드될 때 클라이언트 렌더링에서 발생하는 지연은 일반 사용자에게 거의(또는 진짜) 감지되지 않음. 지나친 최적화도 문제임. 이 모든 SEO 기반 퍼포먼스 추구는 수백만 뷰의 트래픽이 있는 서비스가 아니면 시간 대비 얻는 이득이 별로임. 무인 배를 바다에서 조력으로 조정하는 것에 공기역학까지 걱정하는 수준임. 자원이 한정된 입장에서 총 비용과 이익을 고려하면 최적화가 항상 최고의 선택은 아님
               o 수학 콘텐츠가 적은데 굳이 KaTeX를 쓴다면, MathML(mathml-core)을 대체로 검토해보는 것도 방법임
               o 수학 공식이나 LaTeX 표현을 굳이 클라이언트 js로 렌더하는 게 이해되지 않음. 왜 미리 HTML/CSS로 변환해서 미리 렌더된 상태로 넣지 않는지 의문임
               o 무거운 라이브러리는 초기 페이지 렌더 후에 로드하거나, 뷰포트에 보일 때만 SVG로 공식 그래픽을 불러오는 것도 아이디어임. 내 생각임
     * 14kB 목표는 다소 도전적이지만, 초기 10패킷 내로 제한하는 아이디어도 흥미로움. 나처럼 웹사이트 용량에 집중하는 프로젝트로 512kb.club이 있음. 사이트 크기를 골프 스코어처럼 비교하는 곳임. 내 사이트(anderegg.ca)는 등록 전에 전 자원 합산 71kB가 나왔음. 이 프로젝트 덕분에 Cloudflare Radar도 알게 되었는데, 사이트 분석 및 페이지 용량 측정에 좋은 툴이 있음. 주 목적은 전체 인터넷 대시보드지만 페이지 사이즈 분석 도구도 포함되어 있음
          + 사용자 입장에서 묻고 싶은데, 남은 500kB는 무엇을 위해 쓰는 것임? 나는 90% 텍스트만 필요하고, 나머지도 벡터 그래픽이면 충분함. 14 kB만으로도 텍스트와 그래픽이 꽤 들어가는데, 나머지 500은 뭐에 쓰임?
          + 512kb가 현실적인 기준임. 나도 내 웹사이트에 이 기준을 적용함. 14kb 수준 웹사이트는 이미 웹의 기준을 넘어서버렸음
          + 개인 웹사이트라면 512kb는 충분히 달성 가능함. 내 다음 타깃은 99kb(100kb 이내)임. 주말 몇 번만 투자하면 무난함. 내 사이트는 512kb에서 Orange 등급임
     * 이걸 좀 더 재미있게 실험해보고 싶다면, 초기 윈도우(IW) 크기는 서버 측에서 설정할 수 있음. 예를 들어 아래와 같이 조정 가능함
          + ip route change default via <gw> dev <if> initcwnd 20 initrwnd 20 검색해보면 지금은 CDN이 초기 윈도우를 30 패킷(45kb)까지 주는 경우도 있다고 함
          + 13년 전에는 10패킷도 ‘치팅’으로 여겨졌음. 관련 내용은 이곳과 http://blog.benstrong.com/2010/11/…"">벤스트롱 블로그(아카이브) 참고
          + CDN의 초기 윈도우가 30패킷이라는 근거 자료 있는지 궁금함
          + 그냥 '나쁜 시민'처럼 1000 패킷으로 설정할 수도 있음... 다만 단점은 누군가 다이얼업이나 bufferbloat 있는 연결에서 병목이 생길 수 있음
     * 아래 글에서 설명한 내용도 적용됨: Cloudflare 블로그 - 러시아 ISP가 16KB까지만 허용해 대부분 웹 브라우징이 불가능해진 현상. Cloudflare 분석에 따르면, 러시아 ISPs가 자국 사용자의 인터넷 throttling을 통해 웹자산당 처음 16KB까지만 로딩되게 제한 중임
     * TCP Slow Start가 뭔지 모르는 사람과, 웹사이트 로딩 지연을 미세하게까지 신경 써야 할 정도로 관심 있는 사람의 교집합은 매우 적음. 스타트업은 우선 스타트업에 집중해야 하고, 대기업만이 이런 최적화에 집착할 여력이 있음
          + ""중요한 것부터 집중하니까 성능 최적화까지 신경 쓸 겨를이 없다""는 식으로 접근하면 영원히 신경 쓰지 않게 됨. 그래서 요즘 대다수 앱과 사이트가 느리고 형편없음
          + 만약 그게 사실이라면 Microsoft 같은 곳의 소프트웨어는 언제나 완벽하게 최고 효율로 동작해야 함
          + 개념적으로는 맞는 말 같음. 그런데 Figma의 Evan Wallace가 성능에 집착하지 않았다면 Figma는 지금의 모습이 될 수 없음. 어떤 땐 '성능' 자체가 곧 제품의 주요 기능이 되기도 함
          + 이건 선택의 문제가 아니라 그냥 디폴트로 따라오게 만들 수도 있음. 나는 10억개 셀, 체크박스 데모[1] 모두 datastar를 써서 겨우 10kb 조금 넘음. 모바일 네트워크와 3G에서 차이가 큼. 내 실험으론 14kb 넘으면 품질 낮은 연결에선 3초 더 걸렸음. datastar 메인테이너가 TCP slow start까지 챙겨준 덕에 오히려 노력을 안 해도 덩달아 이득을 봄
               o checkboxes.andersmurphy.com
          + 기업 규모가 성능 최적화와 큰 관련 있다고 생각하지 않음. 오히려 대형 기업이 더 느린 경우가 많음
     * 내 홈페이지가 17.2KB임!(의존성 제외) 개인 페이지에 정말 최적화를 열심히 해서 Lighthouse 100점 만점도 달성함. 전엔 불가능할 줄 알았는데 성공함. 참고로 Rails로 만들었음. 이런 최적화, 실제로 할 만한 가치가 있음. 반응없는 느낌 없이 번개처럼 뜨는 페이지 경험은 그 자체로 매우 만족스러움
          + news.ycombinator.com이 즉각 로드되는 걸 경험하면 심리적으로도 엄청 쾌적해서, 쉬는 시간마다 자동적으로 열게 됨
          + 수천 개 사이트용 템플릿 코드 최적화를 엄청나게 해서 Lighthouse 100/100/100/100 점수를 냈음. 모바일도 완벽 100. 초기 로드는 17.2kB보다 훨씬 커서 120kB 가량이지만, 비법은 모든 불필요한 HTTP 요청을 없애고, ""above-the-fold"" 영역만 JS 실행, 나머지는 lazy eval, defer 등 가능한 한 지연 로드. JS/CSS는 inline 처리, 서드파티 위젯도 팝업 아이콘 등 'facade' 방식으로 실제 요청은 뒤로 미룸. SSR 백엔드 덕도 큼. Vimeo 배경 동영상 위도 100점이었으나 Youtube로는 불가능. 완벽한 점수 내는 방법은 Lighthouse 결과를 해석해 코드베이스 자체를 완전히 다시 짠 거였음. 덕분에 속도 관련 클라이언트 컴플레인이 완전히 사라져서 SEO나 실제 점수 자체에서도 큰 경쟁력이 됨
          + Rails는 렌더링 최적화와 직접적 연관은 없음. 완벽한 Lighthouse 점수 축하함
     * 글에 두 가지 논리적 약점이 있다고 생각함
         1. 위성통신에서 패킷 하나 보내는 데 약 1600ms 걸린다는 수식이 있지만, 14kb룰의 근거로는 약함. 큰 웹사이트와 비교가 없으니 10패킷 ≠ 16초임을 보여주지 못함
         2. 웹페이지 이미지도 14kb룰에 포함시킨다고 했는데, 이미지가 처음 로딩에 inline되는 경우가 얼마나 있나? 실제론 아주 드문 예외 케이스라 99.9% 적용 안 되는 점을 더 명확히 언급해야 함 - ""<i>이미지 인라인되는 경우?</i>""라면, 대표적으로 초기 화면에만 등장하는 저해상도 썸네일에 CSS blur를 추가하고, 진짜 이미지는 나중에 비동기로 fade in하는 기법이 있음. 제대로 하면 1~200바이트 정도 추가 소비만 발생함. 내 블로그엔 적용했고, Wordpress, Medium 같은 플랫폼에도 많이 채택됨. 주로 상업용 프론트엔드 하이퍼최적화용 트릭임 - 사용자 대부분이 저지연 위성 연결을 쓰는 것처럼 가정하고, 모든 웹사이트가 수 MB 단위인 현실에서 내 페이지만 못 견딘다는 전제에도 동의하지 않음
     * 요즘 세대가 단순 정적 웹사이트도 Next.js 같은 프레임워크로 만드는 경향이 있음. 이제는 HTML+CSS+js 시대가 저물어 아쉬움을 느낌
          + 동의함. 나도 최소 리소스·자바스크립트 직접 최적화·14kb 룰 사이트 다 해봤는데, 이 방식은 설계와 아키텍처 제약으로 이어짐. 기능이 늘면 그때의 '최소화' 결정들이 전부 기술 부채가 됨. 대부분은 '프레임워크 없는 순수 웹'에 환상 가졌다가, 어느 순간 그게 더 고역이 됨. 근데 이소모픽 JS 프레임워크 쓰면 일단 정적 페이지로 시작해서 적당히 최적화하다 필요하면 thick client JS로 전환할 수 있음
          + 이미 트렌드가 다시 이동 중임. 요즘 프레임워크들은 대부분 정적 사이트도 잘 지원함. Astro 같은 건 아예 정적 사이트용으로 탄생했음
          + 이제야 알았나 싶은데, 사실 jQuery 인기가 급등하던 2010년 전부터 쭉 그래왔음
          + Next.js는 번들 코드를 아주 강하게 최적화해서 람다나 작은 서버 런칭에 최적임. Next로 만든 정적 사이트도 번들 아주 작게 만듦
     * 지연뿐 아니라 자원 소모 최소화는 지속가능한 미래의 필수 조건임. 네트워크가 환경에 끼치는 영향도 결코 적지 않음. 댓글 분위기가 비꼬는 듯한 느낌이 아쉬움. 이 최적화가 궁극의 솔루션 시대는 아니지만, 에너지 사용량 감소 효과가 더 강조됐으면 함
          + 인터넷 트래픽 대부분이 동영상 스트리밍임. 웹페이지에서 몇 메가 최적화해봐야 티도 안 남. 효율성 자체는 논의 필요하지만, 모든 영역에서 최적화하는 노력이 정작 최적화가 필요한 분야에 자원을 소모할 수 있음
          + 이건 낙과도 아님. 웹페이지 1~2mWh 아낄까 최적화하는 동안, 검색엔진 한 번 쿼리할 때 100배, 챗봇 한 번은 1만배 더 씀. 캐싱이나 lazy loading으로 상당 부분 완화되고 있음
          + 페이지 크기 줄이기에 신경 쓰는 건 거의 쓸데없는 짓임. 연간 웹서핑으로 쓰는 전기량을, 안전상 10배로 잡아도 햄버거 하나 만드는 데 드는 에너지보다 한참 적음. 오히려 개발자가 일주일 동안 최적화 대신 샐러드 한 끼 먹는 게 환경 영향이 큼
          + 완전 동의함. 예전에 BBC 갔다가 작은 텍스트 기사 하나로 캐시에 120MB나 저장되는 걸 보고 충격받음. 쓸데없이 많은 에너지와 낭비 문화를 조장함. 내 웹사이트도 최대한 미니멀하게 만들었고, 유튜브 업로드도 4K 대신 1080P만 씀. 4K, 8K는 굳이 존재할 필요가 없음. 사람들은 흔히 태양광을 몇 MW 추가하는 이야기만 하는데, 사실 '더 적게 생산'하는 노력이 얼마나 좋을지 상상해봐야 함. 56K 모뎀 시절 작은 웹 규모가 그립고, 어디선가 중간점이 있었을 텐데 지금은 훨씬 지나쳐버림
          + 사람들이 신경 끄는 건 결국 본인에게 직접적인 영향이 미치기 시작하면 일어남. 나 역시 환경을 고려하는 편임. AI가 더 나쁘다는 반론도 있는데, 사실 AI도 이런 무거운 페이지를 크롤링함. 그리고 14kb 기준은 모바일 평균 페이지 페이로드 대비 1%도 안 됨
"
"https://news.hada.io/topic?id=21967","GravityForms 공식 플러그인에서 공급망 침해로 보이는 맬웨어 발견","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               GravityForms 공식 플러그인에서 공급망 침해로 보이는 맬웨어 발견

     * 워드프레스의 GravityForms 플러그인 최신 버전에서 악성코드가 발견됨
     * 이는 공급망 침해(supply chain breach) 로 인해 공식 배포본이 감염된 상황임
     * GravityForms는 여러 웹사이트에서 폼 빌더로 널리 사용되고 있음
     * 보안 연구자들은 취약점의 영향 범위와 위험성을 조사 중임
     * 해당 플러그인을 사용하는 웹사이트는 신속한 점검과 교체 필요성이 강조됨
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

GravityForms 공급망 침해 개요

     * 최근 공식 워드프레스 플러그인 GravityForms에서 악성코드가 검출되었음
     * 이번 사건은 공급망 침해(Supply Chain Breach) 의 대표적 사례로 평가됨
     * 공식 소스에서 감염이 발생해, 신규·기존 설치 모두 신뢰도 하락 현상 발생함

GravityForms와 보안 영향

     * GravityForms는 워드프레스 기반 웹사이트에서 폼 작성 및 관리를 쉽게 할 수 있도록 지원하는 인기 플러그인임
     * 광범위하게 사용되는 만큼, 공급망 공격의 피해 범위가 상당히 넓을 가능성 높음
     * 이번에 삽입된 악성코드는 전체 웹사이트와 사용자 데이터의 보안 위협으로 이어질 수 있음

조사 및 대응

     * 보안 전문가들은 감염 경로 분석과 더불어, 추가 확산 사례 조사 중임
     * 공급망 침해를 통해 공식 경로로 유포된 악성코드는 신뢰가 높다고 여겨지는 소프트웨어도 위험에 노출됨을 시사함

GravityForms 이용자 권고

     * GravityForms를 설치했거나 업데이트한 웹사이트 운영자는 즉각 플러그인 무결성 점검이 필요함
     * 공식 채널의 보안 발표와 업데이트 공지를 예의주시해야 하며, 의심되는 경우 강제 삭제 및 재설치 권장됨

결론

     * 공급망 공격은 신뢰 사슬 자체를 위협하고, 기업 및 개발자 모두에게 중요한 경각심을 일으킴
     * 향후 플러그인 선택과 보안 관리에 있어 검증 및 지속적 관찰의 중요성이 강조됨

        Hacker News 의견

     * 이 공급망 침해를 꼼꼼한 시스템 운영자가 느린 HTTP 요청을 추적하며 발견해줘서 정말 고마운 마음임
       비슷하게 xz 사건 때도 SSH 로그인 성능 저하를 이상하게 느낀 개발자가 꼼꼼하게 살펴보다가 침해 사실을 밝혀낸 경험이 있음
          + 예전에는 악성코드가 시스템 성능 저하로 쉽게 구별되곤 했지만, 요즘은 하드웨어 속도도 빨라지고 네트워크도 복잡해지다보니 기본적으로 탐지 자체가 훨씬 어려워진 느낌임
            악의적인 사람들도 점점 더 교묘해지고, 우리는 더 다양한 출처의 더 많은 구성요소로 시스템을 조립하는 중임
            전체 IT 인프라가 점점 기본적으로 신뢰도를 잃어가는 장기적인 모습이 걱정되는 마음임
     * 공식 Gravity Forms 공지문(https://www.gravityforms.com/blog/security-incident-notice/)을 보면, Gravity Forms를 홈페이지에서 직접 다운로드했거나 Composer로 설치한 경우에만 영향을 받는다고 안내하고 있음
       내가 확인한 바로는, Composer 설치 방법도 설치 패키지를 받는 과정에서 Gravity Forms API를 사용하기 때문에 Gravity Forms 플러그인 내부의 자동 업데이트 기능이나 WP-CLI 플러그인과 같은 동작 원리를 공유함
       Gravity Forms 개발팀이 이번 사건을 조사하기 위해 제3자 보안업체에 의뢰할지 궁금함
       지금까지는 관련 언급이 없는 상황임
     * RocketGenius 직원 중 한 명에게 확인 받은 바로는, 이번 악성코드는 수동 다운로드와 composer 설치에서만 영향을 끼친다고 들었음
       안도의 마음이 듦
     * form을 확인하기 전에 nonce를 사용하는 방식이면 이번 문제의 상당 부분을 예방할 수 있었을 것임
       다르게 말하자면, 덕분에 대량의 수작업이 갑자기 필요해질 수 있었음
          + 기술적인 배경이 있어서 이해는 되지만, 영국인 입장에선 이런 문장이 항상 좀 웃기게 느껴짐
     * 이런 일이 얼마나 오랫동안 감지되지 않고 있었는지 궁금함
     * 악성코드를 찾아내고 확산을 막기 위한 대처를 한 점이 멋지다고 생각함
       다만 기사에서 약간 헷갈리는 오류가 있었음
       맨 위에 최신 업데이트 날짜가 원래라면 ""Update 7-12-2025 06:00 UTC""여야 할 것 같은데, 미래 날짜인 08-11-2025로 되어 있음
       아마 작성자가 잘못된 자릿수를 올린 것 같다는 생각임
          + 숫자가 어떤 의미인지를 헷갈리는 건 자연스러운 일임
            미국식 날짜에 ISO 형식을 흉내내어 대시를 쓰지만, 순서와 패딩을 잘못 표기하면 이런 혼선이 생긴다는 반면교사의 느낌임
     * 이 사건이 어디까지 영향을 끼치는 질문이 나옴
       인터넷 사이트의 90%까지인가 아니면 트래픽 적은 소수 사이트뿐인가 궁금함
          + 그 중간 정도임
            Gravity Forms는 아주 인기 있는 프리미엄 WordPress 플러그인임
            내가 WordPress 사이트 여러 개를 유지관리하고 있는데(내가 직접 선택한 플랫폼은 아니었으나 어쩔 수 없이) 디자인과 기능 측면에서 Gravity Forms가 대부분 경쟁 플러그인보다 낫다는 생각임(단, CPU 소모량은 많음)
            문제도 그리 많지 않고, 개발자 입장에서 Rocket Genius와 티켓 처리를 하며 소통한 경험에서 긍정적이라는 느낌임
            소규모·중규모 조직에 상당히 많이 설치된 플러그인이 맞음
            정확한 숫자는 모르지만, 공식 WordPress.org 인기 통계는 무료 플러그인만 반영하는 한계가 있어서 실제로는 많은 사이트와 많은 트래픽이 돌아가는 상황임
            단, 실제로 위험에 노출된 사이트 수는 제한적임
            문제가 된 패키지가 메인 자동배포 채널엔 포함되지 않았기 때문에 실제 영향을 받은 곳은 소수임
          + 문제의 버전을 수동 다운로드한 소수의 사이트만 영향 받았음을 강조함
            대다수 프리미엄(유료) 업데이트 파일은 Gravity API 게이트웨이를 통해 전달되는 방식인데(소문에 따르면 AWS 기반 파일 호출 구조), 이 경로는 영향 받지 않았다고 함
            Gravity API 서비스는 라이선스, 자동 업데이트, 애드온 설치를 담당하는데, 자체적으로 침해받은 적이 없음
            해당 서비스를 통한 모든 패키지 업데이트는 안전함을 안내함
          + ""감염은 널리 퍼진 것 같지 않고, 이는 백도어가 심어진 플러그인 버전이 아주 짧은 기간만 노출되어 극소수 사용자에게만 배포된 것일 수 있음""이란 안내도 있음
     * AB of Ac1dB1tch3z 그룹에 의해 털린 경험을 말함
     * 어떤 플러그인인지 명확하게 써줘야 한다는 의견이 나옴
          + 제목에 이미 명확히 나와있고, 공식 GravityForms 플러그인임을 언급함
            이번 이슈는 v2.9.13에서 수정됐고, 공식 변경내역에는 침해 기록이 언급되어있지 않음
"
"https://news.hada.io/topic?id=22074","비동기(asynchrony)는 동시성(concurrency)이 아님","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 비동기(asynchrony)는 동시성(concurrency)이 아님

     * 비동기와 동시성은 자주 혼동되는 개념이지만, 서로 다른 의미를 가짐
     * 비동기는 작업들이 순서에 상관없이 실행될 수 있는 가능성을 뜻함
     * 동시성은 시스템이 여러 작업을 동시에 진행할 수 있는 능력을 의미함
     * 언어 및 라이브러리 생태계에서 두 개념의 명확한 구분 부재로 비효율성과 복잡성이 발생함
     * Zig 언어에서는 비동기와 동시성의 분리를 통해 코드 중복 없이 동기 및 비동기 코드의 공존이 가능함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: 비동기와 동시성의 구별 필요성

   Rob Pike의 유명한 발표로 '동시성은 병렬성이 아니다'라는 문장이 잘 알려져 있지만, 이보다 실질적으로 중요한 논점이 존재함. 바로 '비동기'라는 개념의 필요성임. 위키백과 정의에 따르면,
     * 동시성: 시스템이 동시에 여러 작업을 시간 분할 또는 병렬적으로 처리할 수 있는 능력
     * 병렬 컴퓨팅: 실제 물리적 수준에서 여러 작업을 동시 실행하는 것
       이 외에, 우리가 놓치고 있는 중요한 개념이 바로 '비동기'임.

예시 1: 두 파일 저장

   두 파일(A, B)을 저장할 때 순서가 상관없다면,
io.async(saveFileA, .{io})
io.async(saveFileB, .{io})

     * A를 먼저 저장하거나 B를 먼저 저장해도 무방하며, 중간에 번갈아가며 저장해도 문제가 없음
     * 심지어 A 파일을 다 저장한 후 B 파일을 시작해도 코드상으로는 올바름

예시 2: 두 소켓 (서버, 클라이언트)

   동일 프로그램 내에서 TCP 서버를 만들고 클라이언트를 연결해야 할 때,
io.async(Server.accept, .{server, io})
io.async(Client.connect, .{client, io})

     * 이 경우에는 두 작업의 실행이 반드시 겹쳐서 진행될 필요가 있음
     * 즉, 서버가 연결을 받아들이는 동안에 클라이언트도 연결을 시도해야 함
     * 첫 번째 파일 예시처럼 직렬로 처리하면 의도된 동작이 나오지 않음

개념 정리

   비동기, 동시성, 병렬성의 개념을 다음과 같이 정의함
     * 비동기(asynchrony) : 작업들이 순서를 벗어나 실행되어도 올바른 결과가 나오는 성질
     * 동시성(concurrency) : 병렬적이든 분할 실행이든 여러 작업을 동시에 전개할 수 있는 능력
     * 병렬성(parallelism) : 물리적으로 여러 작업이 실시간으로 동시에 실행되는 능력

   파일 저장과 소켓 연결 두 예시는 모두 비동기적이지만, 두 번째(서버-클라이언트)는 동시성이 반드시 필요

비동기와 동시성 구분의 실익

   이 구분을 하지 않으면 다음과 같은 문제가 초래됨
     * 라이브러리 제작자들은 비동기/동기 버전의 코드를 두 번 짜야 함 (예: redis-py vs asyncio-redis)
     * 사용자는 비동기 코드가 '전염성'을 띠어, 단 하나의 비동기 라이브러리 의존성만 있어도 전체 프로젝트를 비동기로 바꾸어야 하는 불편함이 발생
     * 이를 피하고자 편법적인 우회가 생기고, 이는 종종 *데드락(deadlock)* 과 비효율을 유발함

   따라서, 두 개념의 명확한 분리는 라이브러리와 사용자 모두에게 큰 이점을 제공함

Zig: 비동기와 동시성의 분리

   Zig 언어는 io.async를 통해 비동기를 사용하지만, 이는 동시성을 보장하지 않음
     * 즉, io.async를 써도 내부적으로는 싱글 스레드, 블로킹 모드로 실행 가능
     * 예를 들어
io.async(saveFileA, .{io})
io.async(saveFileB, .{io})

       이 코드는 블로킹 환경에서
saveFileA(io)
saveFileB(io)

       와 동일하게 동작할 수 있음
     * 즉, 라이브러리 제작자가 io.async를 사용해도 사용자는 원하면 순차적 블로킹 IO로 실행할 수 있는 유연성을 확보

동시성 도입과 작업 전환(스케줄링) 메커니즘

   동시성이 필요한 경우, 실제 효과적 동작을 위해서는
    1. 블로킹이 아닌 이벤트 기반 IO (epoll, io_uring 등) 사용
    2. 작업 전환(스위칭) 프리미티브(예: yield) 사용 필요

     * 예시로, Zig는 그린스레드 환경에서 스택 스와핑 기법을 사용해 작업 전환을 수행
     * OS 수준의 스레드 스케줄링과 유사하게, CPU 레지스터와 스택 등 상태를 저장/복원하여 여러 태스크 간 전환 진행
     * 이러한 전환 메커니즘이 있어야 비동기 코드를 실제 동시적으로 스케줄할 수 있음
     * 스택리스 코루틴 구현(예: suspend, resume)도 동일 원리임

동기 코드와 비동기 코드의 공존

   아래처럼 두 번의 saveData를 io.async로 실행하면,
io.async(saveData, .{io, ""a"", ""b""})
io.async(saveData, .{io, ""c"", ""d""})

     * 두 작업이 서로 비동기적이므로, 내부적으로 동기적으로 작성된 함수여도 자연스럽게 작업들을 동시성 컨텍스트에서 스케줄 가능
     * 사용자나 라이브러리 제작자가 코드 중복 없이 동기/비동기 함수를 함께 써도 문제가 없음

동시성이 '필수'인 상황 명시하기

   특정 함수(예: TCP 서버의 accept)는 실행 시 명시적으로 동시성이 필요함을 코드에 표현할 필요가 있음
     * 이를 Zig에서는 io.asyncConcurrent 등 명시적 함수로 구분
     * 이러한 방식은 해당 작업이 실행 환경에서 동시성을 지원하지 않으면 에러를 발생시켜줌
     * 비동기 목적의 io.async와 달리, 동시성 보장이 필수이므로 failable 함수로 구현됨

결론

     * 비동기와 동시성은 전혀 다른 개념이며, 명확히 구분해야 함
     * 동기 코드와 비동기 코드를 공존시키는 것이 가능함
     * Zig의 비동기/동시성 모델은 코드 중복 없이 두 세계를 함께 활용하게 해줌
     * 이러한 구조는 Go 등의 다른 언어에도 적용된 바 있으며, async/await의 전염성을 극복할 수 있는 길 제시
     * Zig의 새로운 async I/O 설계를 통해, 앞으로 더욱 직관적인 동시성/비동기 프로그래밍 환경 기대 가능

        Hacker News 의견

     * async 정의는 참 어렵게 느껴짐, 나 역시 JavaScript에서 async를 설계한 여러 사람 중 한 명임, 이 글에서 제시한 정의에는 동의하지 않음, 단순히 async라는 이유로 올바르게 동작하는 건 아님, async 코드에서도 여전히 여러 종류의 유저 레벨 레이스 컨디션이 발생할 수 있음, 언어가 async/await를 지원하든 그렇지 않든 마찬가지임, 내가 최근 내린 정의는 async란 “동시성을 위해 명시적으로 구조화된 코드”라는 것임, 이 관점도 아직 더 다듬어져야 함, 관련해 내가 직접 정리한 글도 있음 Quite a few words about async 참고 바람
          + 비동기(Asynchronism)라는 추상적 개념과 실질적 구현을 구분하는 것이 중요하다고 생각함, 후자는 언어 차원의 추상 혹은 기계적 조정 수단 모두를 포함함, 최고 수준의 추상적 개념에서 동기(Synchronism)의 반대가 바로 비동기임, 보통 여러 주체가 함께 동작해야 할 때(예를 들면 한 작업이 끝나야 다른 작업이 진행됨) 언제 그 일이 일어날지 알 수 없거나 정의되지 않았을 때가 비동기의 핵심임, 이 정의 자체는 어렵지 않음, 문제는 언어 차원에서 이런 추상을 설계할 때 생기는 인지적 부담임
          + 이 주제에 깊지는 않지만 내 생각엔 async 코드란 원래 블로킹되는 작업을 논블로킹으로 바꿔줌으로써 다른 작업들이 동시에 진행될 수 있도록 하는 것이라 생각함, 특히 내 경우 임베디드 루프에서 오랫동안 블로킹되는 코드가 I/O를 망가뜨리고, 눈에 보이거나 귀로 들리는 오류를 유발할 수 있으므로 이러한 관점이 명백하게 와닿음
          + 애초에 async를 정의해야 하는지조차 의문임, 실제로 정의하는 게 힘든 이유가 한 가지 개념에 완전히 맞아떨어지는 게 없기 때문임, async나 event loop를 꼭 정의해야 할 필요가 있는지도 의심스러움, 실제 병렬 처리가 가능한 물리적 칩 영역에는 내가 모르는 수많은 개념이 있을 것임, 나는 “user finger”(손가락 터치 등을 지칭)와 “quickies”(수행 시간이 매우 짧은 작업), job queue, 블로킹/논블로킹 API만 알면 충분함, 내 목적을 달성하려면 논블로킹 API가 좋음, 왜냐면 시간이 오래 걸리는 작업은 하위 시스템에 맡기고 나는 원하는 데이터 저장 같은 “quicky”만 적고, 성공/실패별로 다른 quicky도 정의해놓으면 됨, sync와 async의 구분 자체가 큰 도움은 안 됨, 물론 남들이 말할 때 개념을 이해해야 하긴 함, 본질적으로 async는 논블로킹 API라고 생각함, async
            프로그래밍 모델이란 사실상 (수행 시간 기준) 작고 원자적인 블로킹 작업을 “혼돈스럽고 비결정적인” 이벤트에 맞추어 작성하는 형태임, 시스템 내부가 무엇을 하든 브라우저, OS, 디바이스 자체가 멀티 실행 유닛, 좋은 스케줄러를 제공한다고 믿음, async란 나에겐 애매하게 정의된 개념임, 설령 정의 가능해도 그게 꼭 유용할지 의문임, 오히려 이벤트, 내가 작성하는 작업의 블로킹 성질, 함수 클로저, API를 사용할 때 어떤 일이 다른 작업(job)으로 나눠지는지 같은 개념들이 훨씬 실용적임, “callback” 용어 자체도 시작할 땐 매우 헷갈렸음, 코드가 거기서 멈춰있는 줄 알았지만 실제론 해당 부분을 끝까지 실행하고 난 뒤 “callback”이 호출될 때 어떤 코드가 돌고, 어떤 정보를 볼 수 있는지 정밀하게 이해해야만 했음, 솔직히 이건 혼돈이자 천재적
            발상임, “async” 자체보다는 근본 모델 즉, 이벤트, 블로킹 작업, 작업 큐, 논블로킹 API가 훨씬 간단함, 그리고 내가 무엇을 하고, 브라우저/OS 등은 또 뭘 하는지 이해하는 것도 상당히 중요함, 예를 들어 cpp는 concurrent 모델을 선언하는 반면에 OS가 진짜 실행을 맡김, JS에서는 논블로킹 API로 “아마도” concurrency가 있다는 걸 브라우저나 Node에 선언하고, 실제로 내부에서 concurrent하게 처리함, 가장 중요한 건 각 작업을 짧게(<50ms) 유지하고, 논블로킹 API로 의도만 알릴 수 있으면 됨, cpp나 rust는 실제 태스크를 concurrent하게 실행해달라고 OS에 알려주기 때문에 물리적으로 한 스레드밖에 없어도 UI 반응성이 유지됨, 결국 async 프로그래머가 해야 할 일은 “멋진 UX 모델”을 만들고 이벤트를 quickies에 잘 맵핑하는 것임
     * 글쓴이가 “실행 양도(yield) 개념”을 동시성 정의에서 꺼내 새로 “비동기(asynchrony)”란 용어에 넣은 듯함, 그리고 이 개념이 없다면 동시성(concurrency) 전체가 망가진다고 주장함, 내 생각에 동시성에는 애초에 실행 양도 기능이 필수라서 그 자체에 내재된 개념임, 중요한 개념이긴 하지만 새로운 용어로 따로 떼어내는 건 혼란만 더하는 것임
          + 나는 1:1 병렬성이 실행 양도가 없는 동시성의 한 형태라고 생각함, 그밖에는 모든 비(非)병렬형 동시성(concurrency)은 실행을 어떤 주기로든 양보(yield)해야 함, 명령어 레벨이라도 마찬가지임, 예를 들어 CUDA에선 같은 워프 내에서 분기된 스레드가 서로 명령을 교차실행하게 되어 한쪽 분기가 다른 쪽을 블로킹할 수 있음
          + 인용한 글에서 오히려 “실행 양도는 동시성의 개념”이라고 명시되어 있음을 강조함
          + 동시성이 반드시 실행 양도를 의미하지는 않음, 동기적 로직은 명확한 동기화가 필요하고, 실행 양도는 동기화 수단일 뿐임, 내가 말하는 비동기 로직은 동기화나 실행 양도 없이 동작하는 동시성을 의미함, 실제 관점에서 동시성이나 비동기 로직은 폰 노이만 머신에선 온전히 존재하지 않음
     * 이 맥락에서 비동기란 요청 준비·제출과 결과 수집을 분리하는 추상화임, 여러 개의 요청을 제출한 후에만 그 결과를 확인하는 것이 가능해짐, 동시적 구현을 허용하지만 필수는 아님, 그래도 이 추상의 목적은 동시성 확보임, 동시성이 없으면 얻고자 하는 이득도 없어짐, 일부 비동기 추상화는 최소한의 동시성이 없으면 구현 자체가 불가함, 예를 들어 콜백 방식은 싱글스레드에서도 흉내낼 수 있지만 비재귀 mutex를 쥐고 있을 때 deadlock이 발생하는 등 한계가 있음, 즉 동시성 없는 비동기 추상화는 반드시 실패하게 되어 있음, 요청자가 mutex를 쥔 채로 요청하고, 언락 전 콜백이 실행되면 언락이 영원히 실행되지 않을 수 있음, 최소한 분리된 쓰레드를 통해 요청자가 언락까지 도달할 수 있도록 해야 함
          + 동의함, 글의 서버-클라이언트 예시는 한 가지에 불과하고, 지금 언급한 case는 전혀 다른 방식의 해결책이 필요한 예시임, 앞으로도 더 많은 유사 케이스를 발견하게 될 거라 생각함, 결국 async를 쓸 때 항상 동시성을 보장해야 함
     * ""협력형 멀티태스킹(cooperative multitasking)은 강제형(preemptive)이 아님"", ""비동기""라는 용어는 보통 ""단일 쓰레드, 협력형 멀티태스킹(명시적 yield)과 이벤트 기반"", 그리고 외부 연산이 동시적으로 실행되며, 결과를 이벤트로 리포트하는 것을 의미함, 멀티스레드나 동시적 실행 모델에서는 비동기가 별 의미가 없음, 해당 쓰레드가 블록돼도 프로그램은 계속 진행함, yield 포인트가 굳이 명시적일 필요가 없어짐
          + Rust, C#, F#, OCaml(5+) 등은 OS 쓰레드와 async를 모두 지원함, OS 쓰레드는 CPU 바운드 작업, async는 IO 바운드 작업에 적합함, async 또는 Go 스타일의 M:N 스케줄링의 최대 장점은 메모리만 충분하다면 태스크/고루틴을 자유롭게 늘릴 수 있음, OS 쓰레드 방식에서는 컨텍스트 스위치 부담, 쓰레드/메모리 부족 등 문제가 있으므로 IO 바운드만 넘어가도 deadlock 문제에 봉착할 수 있음
     * Zig의 새로운 IO 아이디어는 일반 앱 개발엔 참신하게 보임, 스택리스 코루틴이 필요 없는 사람에겐 최적임, 하지만 라이브러리 작성에는 오류가 많아질 듯함, 라이브러리 저자는 주어진 IO가 단일/멀티쓰레드인지, 이벤트 기반 IO인지 파악이 어려움, 동시성/비동기/병렬 관련 코드는 IO 스택을 완전히 알고 있어도 작성하기 어려운데, IO가 외부에서 주어지는 구조에서는 어려움이 배가됨, IO 인터페이스가 ""작은 OS""처럼 방대해지면 테스트 해볼 시나리오도 폭발적으로 늘어남, 인터페이스에서 제공하는 async 프리미티브만으로 실제 엣지케이스를 모두 처리할 수 있을지 확신이 서지 않음, 다양한 IO 구현을 지원하려면 코드가 매우 ""수비적""이어야 하고 항상 가장 병렬적인 IO를 상정해야 할 것 같음, 특히 스택리스 코루틴과 이 방식을 섞는 게 쉽지 않을 것으로
       예상됨, 불필요한 코루틴 스폰을 줄이려면 코루틴 명시적 폴링이 필요한데 대부분의 개발자가 그런 코드를 직접 작성하지 않을 듯함, 결국 일반 async/await 코드와 비슷한 구조로 귀착될 것 같음, 동적 디스패치 및 Zig의 상향식 디자인 경향까지 감안하면 최종적으로 꽤 고수준 언어가 될 것 같음, 아직 실제 적용사례가 없어 ""타협 없는"" 접근이라 부르기엔 이름값이 너무 과감함, 몇 년간의 활용 후에야 진정한 평가가 가능함
          + 스택리스 코루틴은 어차피 지원 예정임, WASM 타깃 지원에 필요하니까 반드시 들어갈 것임, 동적 디스패치는 IO 구현이 2개 이상일 때만 사용됨, 하나만 쓰면 다이렉트 콜로 대체됨, 아직 현장에서 검증이 안 되었으니 “타협없는”이란 표현은 시기상조라고 봄, Jai 언어에서 비슷한 모델을 성공적으로 쓰긴 한다고 들었지만(명시적 컨텍스트 전달이 아닌 암묵적 IO 컨텍스트라는 차이 있음), 이것도 현장에서 실제 쓰였다고 보긴 어려움
          + 동기와 비동기 실행을 모두 지원하려면 코드가 항상 가장 병렬적인 IO를 상정해야 한다는 점 공감함, 그러나 하위 레벨 IO 이벤트 핸들러에서 비동기가 잘 구현되어 있다면, 모든 곳에서 동일 원칙만 적용해주면 됨, 최악의 경우에도 코드가 그냥 순차적으로(느리게) 돈다는 것 뿐, 레이스/데드락 문제엔 빠지지 않음
     * 두 개의 라이브러리를 따로 안 쓸 수 있다는 점에서 Zig의 아이디어가 아주 좋다고 생각함, 다만 비동기 코드의 테스트가 항상 걱정임, 오늘 통과한 테스트가 실제 도중에 발생할 모든 시나리오/순서를 복제한 건지 어떻게 확신할 수 있을지 모르겠음, 스레드 프로그램도 동일 문제인데, 멀티스레드 코드는 쓰고 디버깅하는 게 항상 더 힘듦, 나는 웬만하면 쓰레드 사용을 피함, 실제 문제는 ‘개발자에게 비동기/스레드 환경을 정확히 이해시키는 것’임, 최근 Python 시스템에서 JS 반, Python 반을 써본 팀과 일했는데, 대규모 코드 async, threaded로 바꿔놓음, 근데 Global Interpreter Lock(GIL)이 뭔지도 모름, 내 얘기가 잔소리만 같았던 듯, 게다가 그들의 테스트는 실제로 코드를 부숴도 항상 통과함, mangum이 HTTP request 끝날 때 background 및 async 작업을 강제로 finishing시키는데,
       정작 이걸 몰랐음, 이런 걸 알린다고 해도 다들 무심해함, 아는 게 중요한 게 아니라, 남들이 그걸 봐주느냐가 더 중요함
          + Zig에서는 Io 테스트 구현체를 도입할 예정임, 이걸로 병렬 실행 모델 하에서 퍼즈 테스트 등 스트레스 테스트도 가능하게 만들 계획임, 하지만 핵심은 대부분의 라이브러리 코드가 io.async나 io.asyncConcurrent를 직접 호출할 일이 없으리라는 점임, 예를 들어 대부분의 데이터베이스 라이브러리는 순수 동기 코드만으로 충분함, 해당 코드를 어플리케이션 개발자가 io.async(writeToDb), io.async(doOtherThing) 식으로 손쉽게 비동기화할 수 있음, 이렇게 하면 async/await가 코드 전체를 스프링클하는 것보다 덜 에러프론하며 훨씬 이해가 쉬움
          + 공감됨, 비동기·멀티스레드 코드에서 모든 인터리빙을 테스트하는 건 악명 높게 어려움, 퍼저나 동시성 테스팅 프레임워크를 써도 실제 운영에서 겪는 교훈 없이는 확신하기 힘듦, 분산 시스템에서는 더 악화됨, 예를 들어 웹훅 인프라 설계할 땐 자체 코드 내 async뿐 아니라 네트워크 재시도, 타임아웃, 부분 실패 등 다양한 외부 문제까지 겹침, 고동시 환경에서 리트라이, deduplication, idempotency 보장 등 자체적인 엔지니어링 이슈가 됨, 그래서 Vartiq.com 같은 전문 서비스를 쓸 필요가 생김(여기서 일하고 있음), 이런 서비스가 운영상 동시성 복잡도를 일정 부분 추상화해 blast radius를 줄여주지만, 내 코드 내 async 테스트 이슈는 여전함, 결론적으로 async, threading, 분산 동시성은 서로 리스크를 증폭시키므로, 커뮤니케이션과 시스템 설계가 어떤
            문법/라이브러리보다 더 중요함
     * 저자는 동시성 정의에서 혼동이 있는 것 같다고 봄, Lamport의 논문을 참고할 만함
          + 논문 링크만 남기지 말고 설명 부탁함, 내 생각엔 정의 자체는 괜찮았음, 예를 들면 비동기: 작업들이 순서대로 실행되지 않아도 옳다면 그게 비동기임, 동시성: 병렬이든 태스크 스위칭이든 여러 작업을 동시에 진행 가능한 시스템 성질, 병렬성: 실제 물리적 수준에서 동시에 둘 이상 작업이 돌아가는 것임
          + 이런 이유로 나는 이 용어 사용을 완전히 그만둠, 누구와 이야기하든 이해가 달라서 용어 자체가 소통에 의미가 없어짐
          + 저자도 블로그 글에서 그 용어에 대한 기존 정의가 있었던 걸 알고 있음, 스스로 새 정의를 제안한 거고, 그 정의만 일관적이라면 충분함, 독자가 받아들일지 여부만 차이임
          + Lamport 논문의 절반은 대부분 언어에서 개념적으로 표현할 수 없음, 스레드를 만든다고 해서 전체 및 부분 순서 논의를 할 일은 거의 없음, TLA+로 프로토콜 설계할 때나 이런 논의가 필요함, Zig async API에서 “비동기 실행 환경에서만 동작”하는 함수가 컴파일 오류를 내는 걸 새 이론으로까지 부를 필요는 없음
     * “비동기(asynchrony)”란 용어가 정말 필요한지 가늠하는 좋은 방법은, 한 언어/모델뿐 아니라 다양한 동시성 모델에서도 유용한지 따져보는 것임, 예컨대 Haskell, Erlang, OCaml, Scheme, Rust, Go 등 여러 환경에서 공통적으로 필요한 용어라면 가치가 높음, 일반적으로 협력형 스케줄이 들어오면 시스템 전체가 한 코드의 문제로 락업, 지연 이슈 등으로 더 많은 신경을 써야 하고, 선점형 스케줄이면 이런 문제들이 대거 사라짐, 전체 시스템 락업이 불가능해지므로 문제군이 확 줄어듦
     * “비동기(asynchrony)”는 이 경우 부적절한 단어이고, 이미 잘 정의된 수학적 용어 “교환법칙(commutativity)”가 있음, 어떤 연산은 순서가 중요하지 않은데(덧셈, 곱셈 등), 어떤 연산은 순서가 중요함(뺄셈, 나눗셈 등), 보통 코드의 연산 순서는 줄 번호(위→아래)로 표현되는 데, async 코드에서는 이 순서가 깨짐, 이런 식으로 작성된 asyncConcurrent(...)는 상당히 헷갈릴 수밖에 없음, 블로그 글 내용을 완전히 숙지한 게 아니면 무슨 의미인지 파악이 어려움, Zig(그리고 내가 좋아하는 Rust도)에선 이런 “힙스터”적 접근이 자주 나오는 듯, 절차형(async-기반) 교환법칙/순서 체계를 Rust의 라이프타임같이 구현하거나, 아니면 사람들이 익숙한 걸 그냥 쓰는 게 나음
          + “asyncConcurrent(...)가 혼란스럽다”는 얘기에 동의하지 않음, 블로그 글 핵심을 내재화하면 전혀 혼란스럽지 않음, 그 아이디어를 학습할 가치가 있냐는 별개 문제임, 실제로 이걸 내면화한 사람들이 많이 실습하고, 시간이 지나면 현장에서 이 아이디어가 좋은지 아닌지 알게 될 것임, 그리고 “교환법칙(commutativity)”란 단어를 다른 걸로 대체할 때 오히려 Zig에선 오히려 연산자가 교환법칙을 따르는 게 있으므로 더 혼동됨, f() + g()라면 덧셈이 교환법칙을 따르니까 Zig가 병렬로 실행해도 되냐 같은 혼란이 나오기 쉬움, 실행 순서와 교환법칙은 완전히 다른 얘기니까 구분해야 함
          + 엄밀히 말해 교환법칙(commutativity)은 (이항)연산에 성립하는 특성임, connect/accept 같은 두 async 문이 교환 가능하다고 할 때 “어떤 연산에 대해?”란 질문이 나옴, 현재로선 bind(>>=) 연산자(혹은 .then(...) 등)가 그런 역할에 근접하지만 아직은 직관의 영역임
          + 비동기는 부분 순서도 허용함, 두 연산이 같은 순서로 퇴역해야 하더라도 실제 실행 순서와는 별개임, 예를 들어 뺄셈이 교환법칙은 아니지만 잔액 계산과 차감액 계산을 두 쿼리로 병렬 수행한 뒤, 결과를 적절한 순서로 적용하는 것도 가능함
          + 다른 용어가 이 개념을 포함한다고 해서 “asynchrony”보다 좋은 단어가 된다고 볼 수 없음, “commutativity”란 단어는 읽기에도, 듣기에도, 쓰기에도 지저분함, asynchrony가 훨씬 친숙함
          + 교환법칙이란 주장엔 한계가 있음, A와 B가 각각 C와 교환된다면 ABC=CAB지만, 이게 ACB와도 반드시 같다고 할 순 없음, 비동기에서는 ABC=ACB=CAB 모두가 같아야 함(기존 수학 용어가 있다면 모르겠지만 잘 모르겠음)
     * 네트워크 프로그래머로서 동시성과 병렬성, 비동기 코드를 엄청 많이 작성해봤는데, 이 글은 좀 혼란스럽게 느껴짐, 마치 허점 많은 추상 위에서 답을 찾으려 애쓰는 느낌임, 툴이나 구현 자체가 잘못됐다면 이렇게 쉽게 “망가질” 수 있다는 게 문제임, 사실 멀티쓰레드 코드 디버깅 자체가 꽤 재밌음, 다른 사람들이 멀티쓰레드 괴물을 엄청 두려워하는 걸 보면 오히려 즐거움
"
"https://news.hada.io/topic?id=21979","x86-64 어셈블리 배우기","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            x86-64 어셈블리 배우기

     * x86-64 어셈블리 입문을 위한 시리즈의 첫 번째 글 소개임
     * 현대 64비트 시스템 기준으로 도구 설치 및 기본 구조 설명 제공임
     * Flat Assembler (FASM) 와 WinDbg를 주요 개발 및 디버깅 툴로 사용 안내임
     * PE 포맷, DLL 임포트, 윈도우즈 호출 규약 등 실무에서 필요한 핵심 지식 요약 포함임
     * 단순 종료 프로그램 작성 및 디버깅 절차 실습 경험 중심 설명임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 의의

     * x86 어셈블리를 처음 접할 때 대학에서는 구식 환경(16비트, DOS, 세그먼트 메모리)에 기반한 방식으로 강의 수강 경험임
     * 현대에는 64비트 프로세서가 주류인 만큼, 본 시리즈를 통해 실제로 쓰이는 x86-64 환경만을 다루고 구형 요소는 모두 배제함
     * 본 튜토리얼은 Windows 운영체제 환경에서 동작하는 64비트 프로그램 개발에 집중함
     * 라이브러리를 사용하지 않고, OS에 직접적으로 접근하는 최소한의 코드부터 시작함
     * 이 글은 어셈블리를 처음 배우려는 개발자를 대상으로 하며, 기본적인 C/C++ 지식이 있다고 가정함

개발 도구 준비

  어셈블러(Assembler)

     * CPU는 인간이 이해하기 힘든 머신 코드만 해석 가능하며, 이를 사람이 읽을 수 있는 코드로 바꾼 것이 어셈블리 언어임
     * 어셈블리 언어를 머신 코드로 변환해주는 프로그램이 어셈블러임
     * x86-64 어셈블리어는 표준이 정해져 있지 않고, 어셈블러마다 문법과 동작 방식이 차별화됨
     * 본 시리즈에서는 Flat Assembler(FASM) 를 사용하며, 작고 사용이 간편하고 강력한 매크로 시스템과 에디터를 제공함

  디버거(Debugger)

     * 작성한 어셈블리 코드를 분석 및 실행 흐름 관찰을 위해 디버거를 필수 도구로 활용함
     * WinDbg를 추천하며, 레지스터, 메모리, 어셈블리 코드 등을 독립적으로 확인 및 조작 가능함
     * Windows 10 SDK에서 컴포넌트만 선택하여 설치 가능함
     * 디버거를 통해 프로그램 내부 상태와 메모리 구조, 레지스터 변화를 직접 관찰할 수 있음

어셈블리 프로그래밍의 관점

  CPU 구조와 명령어 집합

     * CPU는 특정 명령어 집합에 따라 제한된 동작만 수행 가능함
     * 명령어란 CPU가 수행할 수 있는 기본 단위 작업임
     * 각 명령어는 매개변수와 함께 매우 단순(값 저장, 산술 연산 등)하게 동작하는 구조임
     * 저수준 프로그래밍 및 디버깅에는 이러한 구조가 모든 고수준 개념의 기반임을 이해하는 것이 핵심임

  레지스터(Registers)

     * 레지스터는 CPU 내부에 내장된 매우 빠른 전용 메모리 영역임
     * x86-64에는 일반 목적 레지스터가 16개 있으며 모두 64비트 크기임
     * 각 레지스터는 바이트, 워드, 더블워드 단위로 부분 접근이 가능함

    레지스터   하위 바이트   하위 워드   하위 더블워드
   rax    al       ax       eax
   rbx    bl       bx       ebx
   rcx    cl       cx       ecx
   rdx    dl       dx       edx
   rsp    spl      sp       esp
   rsi    sil      si       esi
   rdi    dil      di       edi
   rbp    bpl      bp       ebp
   r8~r15 r8b~r15b r8w~r15w r8d~r15d

     * rsp는 스택 포인터, rsi/rdi는 문자열 처리 인덱스로 동작하는 등 일부 레지스터에는 특수 목적이 할당됨
     * rip는 명령어 포인터, rflags는 연산 결과 상태 플래그를 담는 특별 레지스터임

  메모리와 주소

     * 메모리는 0번 인덱스부터 연속된 바이트 배열처럼 동작함
     * 과거 x86 구조에서는 세그먼트-오프셋 방식이 필수였으나 x86-64에서는 모든 메모리를 플랫(Flat) 주소 공간으로 다룸
     * 실제로는 운영체제와 하드웨어가 각 프로세스 별로 가상 주소 공간을 물리 메모리에 동적으로 매핑하여 제공함
     * 즉, 동일한 가상 주소라 해도 서로 다른 프로세스에서는 다른 물리 메모리에 대응함
     * 명령어와 데이터가 동일 메모리에 존재(폰 노이만 구조)하며, 이는 아두이노에 쓰이는 AVR처럼 데이터를 따로 저장하는 하버드 아키텍처와 구별됨

첫 번째 어셈블리 프로그램 작성

     * FASM을 설치한 후 아래의 간단한 프로그램 코드 작성 및 빌드 실습

format PE64 NX GUI 6.0
entry start

section '.text' code readable executable
start:
        int3
        ret

  코드 설명

     * format PE64 NX GUI 6.0 : FASM이 생성할 실행 파일 포맷을 지정, 여기서는 PE(Portable Executable) 64비트 GUI임
     * entry start : 프로그램이 진입할 엔트리 포인트를 정의, 해당 레이블(start) 위치에서 실행 시작함
     * section '.text' code readable executable : PE의 코드 구간임을 지정, 실행 가능한 영역임
     * start: : 앞서 지정한 진입점에 이름 붙임
     * int3 : 디버거용 브레이크포인트로 프로그램을 일시정지시켜 상태 점검 목적에 사용
     * ret : 스택의 주소를 꺼내 그 위치로 제어를 전환하는 명령, 이 프로그램에서 바로 종료 응답

  디버깅 실습

     * WinDbg에서 위 프로그램의 실행파일(.exe)을 열고, 디스어셈블리·레지스터 등 다양한 창을 준비함
     * F5를 눌러 프로그램이 브레이크포인트에 도달하게 하고, F8을 누를 때마다 한 명령씩 실행(단계별 진행)함
     * 레지스터(rip 등)의 변화를 실시간으로 관찰 가능
     * ret 실행 이후에는 운영체제로 제어가 전달되고, 이후 RtlExitUserThread를 호출하면서 스레드 및 프로세스 종료가 이어짐
     * 주의 : ret 명령만으로 종료 시 스레드 외 추가 백그라운드 실행 여부에 따라 프로세스가 남아 있을 수 있으므로, 정상적인 종료 시에는 반드시 ExitProcess를 호출하는 것이 바람직함

PE 포맷과 DLL 임포트

  DLL 함수 임포트 구조 개요

     * ExitProcess와 같은 WinAPI 함수들은 KERNEL32.DLL에 있음
     * 이런 외부 함수를 사용하려면 실행 파일의 임포트 테이블(.idata 섹션)을 구성해야 함
     * idata 섹션의 Import Directory Table(IDT) 에는 DLL명, 함수명, IAT/ILT 등의 주소(RVA) 정보가 담김
     * IAT(Import Address Table)는 실제 함수 주소로 OS 로더에 의해 런타임에 덮어써짐
     * Hint/Name Table은 각 함수의 이름과 힌트 정보로 이루어짐

  FASM에서 .idata 섹션 정의 예시

section '.idata' import readable writeable
idt:
    dd rva kernel32_iat
    dd 0
    dd 0
    dd rva kernel32_name
    dd rva kernel32_iat
    dd 5 dup(0)
name_table:
    _ExitProcess_Name dw 0
                      db ""ExitProcess"", 0, 0
kernel32_name: db ""KERNEL32.DLL"", 0
kernel32_iat:
    ExitProcess dq rva _ExitProcess_Name
    dq 0

     * db/dw/dd/dq : 바이트/워드/더블워드/쿼드워드(8바이트) 단위로 값 삽입
     * rva : 심볼의 가상 주소(Relative Virtual Address) 계산
     * IAT와 Name Table을 수작업으로 구성해 DLL 함수 참조 가능

64비트 Windows 호출 규약(MS x64 Calling Convention)

     * 함수 호출 시에 인자 전달 및 스택 사용 방법을 정하는 표준 규약
     * 64비트 Windows에서는 Microsoft x64 Calling Convention을 사용
     * 주요 특징 :
          + 스택 포인터는 항상 16바이트 정렬 되어야 함
          + 첫 4개 정수/포인터 인자는 rcx, rdx, r8, r9 레지스터 사용
          + 첫 4개 부동소수점 인자는 xmm0~xmm3에 넣음
          + 추가 인자는 스택 사용
          + 인자 개수와 무관하게 32바이트 shadow space를 스택에 확보해야 함
          + 스택 정리는 호출자가 담당

  ExitProcess 호출 예시

format PE64 NX GUI 6.0
entry start

section '.text' code readable executable
start:
    int3
    sub rsp, 8 * 5
    xor rcx, rcx
    call [ExitProcess]

section '.idata' import readable writeable
idt:
    dd rva kernel32_iat
    dd 0
    dd 0
    dd rva kernel32_name
    dd rva kernel32_iat
    dd 5 dup(0)
name_table:
    _ExitProcess_Name dw 0
                      db ""ExitProcess"", 0, 0
kernel32_name db ""KERNEL32.DLL"", 0
kernel32_iat:
    ExitProcess dq rva _ExitProcess_Name
    dq 0

    신규 코드 분석

     * sub rsp, 8 * 5 : 스택 포인터 조정(40바이트 확보), 16바이트 정렬 및 shadow space 확보 한 번에 처리
     * xor rcx, rcx : 첫 번째 인자인 rcx 레지스터에 0을 할당(EXIT 코드로 활용)
     * call [ExitProcess] : import table에 실제로 기록된 ExitProcess의 함수 주소로 점프
     * WinDbg에서 단계별 실행 시, 스택 포인터(rsp) 및 rcx 레지스터의 변화, 그리고 프로세스 종료 흐름을 직접 확인 가능함

마무리

     * 본 글은 기초 도구 세팅부터 PE 포맷, DLL 임포트, x64 호출 규약, 첫 프로그램 작성 및 디버깅까지 실습 중심으로 x86-64 어셈블리의 전반적 흐름을 안내함
     * 다음 파트에서 보다 다양한 기능 구현과 실제 코드를 다룰 예정임

        Hacker News 의견

     * 몇 년 동안 개발해온 프로젝트를 공유하고 싶음
       https://asm-editor.specy.app
       M68K, MIPS, RISC-V, X86 등 다양한 어셈블리 언어를 지원하는 온라인 인터랙티브 IDE임
       어셈블리 프로그래밍을 가르치기 위한 다양한 기능이 많음
       다른 웹사이트에도 임베드할 수 있음
     * 포인터 인덱싱 레지스터에 저주소 바이트 직접 접근 기능(예: 16/32비트에서 si/esi가 sil로 접근 가능)이 있다는 걸 몰랐음
       ax/eax에서 al로 접근하는 것과 비슷한 개념임
       x86_64에서 새로 추가된 오퍼코드가 실제로 존재하는지 궁금함
       플랫폼 명세를 다시 확인해봐야겠다는 생각이 들음
       순수한 호기심에서 묻는 것임
     * 직접 작성을 한 어셈블리 입문 자료를 공유함
       https://nayuki.io/page/…
     * 내 CPU 에뮬레이터 디스패치를 C++보다 빠르게 만들 수 있는지 궁금해서 어셈블리로 최적화 시도해봄
       피보나치 프로그램을 실행해봤는데 결과는 전혀 근접하지 못했음
       결국 기본값 비활성화 옵션으로 병합만 했음
       그래도 분명 더 빠르게 할 수 있는 방법이 있다고 믿음
       https://github.com/libriscv/libriscv/…
       메모리 접근 방법을 익히면서 성능을 약간은 개선함
       점프 테이블을 64비트에서 32비트로 줄이고, .text 섹션에 들어가도록 해서 RIP-relative 접근으로 만듦
       피보나치 프로그램에는 많은 바이트코드가 필요 없었음
       더 개선할 점에 대한 팁을 정말 듣고 싶음
          + 본인이 작성한 코드와 C++ 컴파일러가 생성하는 코드를 직접 비교해봤는지 궁금함
            맥락은 잘 모르지만 차이가 디스패치 메커니즘(명령어 페치 방식) 때문이 아니라, 실제 명령 구현 차이 때문일 가능성도 있다고 생각함
            최적화 방안으로, 에뮬레이션하는 레지스터를 x86-64 실제 레지스터에 매핑하고, 메모리로 아예 안 흘리는 방법이 있음
            이렇게 하면 add 같은 연산에서 메모리에서 꺼내지 않고 바로 연산 가능함
            단, 이 방식은 에뮬레이터 작성이 훨씬 번거로움
     * 브라우저에서 실습 가능한 x86 어셈블리 입문 자료임
       별다른 로컬 셋업 없이 예제를 바로 실행해볼 수 있음
       https://shikaan.github.io/assembly/x86/…
       참고로 본인이 작성한 자료임
          + 입력값 검증을 따로 하는지 궁금함
            NASM으로 바로 어셈블 후 바이너리 실행하는 방식 같아서 보안이 궁금해짐
     * 프로필 사진만 보고 junferno인 줄 알았음
     * 어셈블리를 한 번쯤 만져보는 것만으로도 전반적인 이해가 깊어져서 항상 좋은 경험이 됨
       큰 프로젝트를 만들 필요까지 없으니, 용기를 내서 조금이라도 직접 해보는 것을 추천함
     * (2020년) 당시 HN 토론 링크를 공유함
       https://news.ycombinator.com/item?id=24195627
     * 인텔 방식의 어셈블리 문법(구문)이어서 다행이라고 생각함
          + 다른 어셈블리 문법이 뭐가 있는지 궁금해짐
     * 어셈블리로 뭔가 해보고 싶긴 한데 특별히 아이디어가 떠오르지 않음
          + TIS-100이라는 게임을 추천함
            일종의 유사 어셈블리로 퍼즐을 푸는 게임임
            이런 게임이 어셈블리에 대한 갈증을 해소해줄 수 있다고 생각함
"
"https://news.hada.io/topic?id=22009","DogWalk - 무료 오픈소스 캐주얼 인터랙티브 스토리 게임 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   DogWalk - 무료 오픈소스 캐주얼 인터랙티브 스토리 게임

     * Blender Studio가 공식적으로 선보인 오픈 프로젝트 게임으로, Windows, macOS, Linux에서 플레이 가능함
     * 플레이어는 겨울 숲을 누비는 큰 강아지가 되어 겨울 캠핑장, 숲길, 시냇물, 얼어붙은 연못 등 미니 오픈월드 공간을 자유롭게 돌아다닐 수 있음
     * 게임 목표는 아이와 함께 숨겨진 아이템을 찾아 눈사람을 꾸미는 일이지만, 이 외에도 아이를 이끌거나 장난을 치거나 도와주는 등 다양한 방식의 상호작용이 가능함
     * 플레이어의 스타일과 선택에 따라 다양한 순간이 만들어지며, 실패 조건 없이 오로지 사용자의 플레이에 따라 경험이 달라지는 구조임

기술적·아트적 차별점

     * 배경 및 오브젝트는 실제 종이 공예로 만든 모형을 3D 스캔해 게임 환경으로 구현한 미니어처 페이퍼크래프트 형식의 독특한 아트 스타일을 보여줌
          + 모든 모델은 낮은 폴리곤, 단순한 조명, 거친 엣지를 강조하고, 종이 조각의 틈이나 맞닿지 않은 부분도 의도적으로 살림
          + 로우폴리·로우피델리티 스타일을 채택해 개발 리소스를 줄이면서도, 현실적인 PBR 라이팅을 더해 직관적이고 자연스러운 게임 환경을 구현함
          + 그림책, 수작업 콜라주, 아날로그 질감에서 영감을 받아, 디지털과 전통 미술의 경계를 넘나드는 시도를 적용
     * Blender, Godot, Krita 등 오픈소스 툴을 활용해 개발되었으며, 프로젝트 자체가 Blender 및 Godot의 새로운 기능과 워크플로우 테스트 목적으로 진행됨
     * 게임 리소스는 Creative Commons BY 4.0으로, 소스 코드(스크립트)는 MIT 라이선스로 공개되어 누구나 자유롭게 활용 및 수정이 가능함

커뮤니티 반응 및 평가

     * itch.io에서 평균 4.8점(5점 만점)의 높은 평가를 받으며, “귀엽고 창의적”, “오픈소스 협업의 좋은 예시”라는 긍정적 반응이 이어지고 있음
     * 플레이어들은 아트와 사운드, 애니메이션, 현실과 디지털을 연결하는 제작 방식 등에 깊은 인상을 받았다고 평가함
     * Blender Studio 팀이 라이선스와 소스 공개 방식에 대한 피드백도 적극적으로 반영함

다운로드 및 참고 링크

     * Windows, macOS, Linux용 빌드 제공
     * 프로젝트 공식 홈페이지
     * 소스 코드/리포지토리

프로젝트의 중요성 및 기대 효과

     * Blender Studio의 Dog Walk는 복잡한 대형 게임 대신 단순하지만 창의적이고 상호작용이 풍부한 캐주얼 게임의 가능성을 보여줌
     * 오픈소스로 공개되었기에, 관련 산업 종사자 및 취미 개발자는 실제 제작 자료와 코드를 직접 학습 자료로 삼거나 창작 참고로 활용할 수 있음

   https://godotengine.org/article/godot-showcase-dogwalk/
   Godot 블로그의 인터뷰와

   https://studio.blender.org/blog/our-workflow-with-blender-and-godot/
   Blender 개발진들이 Godot과의 워크플로우를 어떻게 구축했고, 자원은 어떻게 관리했는지에 대한 글이 매우 재미있으니 강추입니다.

        Hacker News 의견

     * 혹시 이 게임 개발에 대한 기술적인 상세 분석 자료를 아는 사람이 있는지 궁금함. 3D 에셋들이 실제 종이로 만든 오브젝트를 포토그래메트리 파이프라인을 통해 만들었다고 들었는데, 구체적인 구현 방법을 못 찾았음. 몇 년 전에는 Meshroom이 오픈소스 레퍼런스로 유명했던 걸 기억함(Meshroom), 요즘에는 뭘 쓰는지 궁금함. 그리고 Godot 얘기도 관심 있음. Blender Studio라면 BGE(BLENDER GAME ENGINE)가 없어졌으니 Armory 3D(Armory 3D)를 썼을 줄 알았는데, Godot로 만든 게 더 미래지향적인 선택 같아 만족스러움
          + 이 프로젝트에 대해 연재된 글이 있음. 많은 에셋은 실제로 종이를 잘라 색칠한 후 만든 모형을 촬영하고, 분해해서 스캔했음. 타일링 가능한 페인트 샘플도 따로 준비함. 자세한 제작 과정은 여기와 검색 결과에서 확인 가능함
          + 종이 오브젝트를 펼쳐서 스캔하거나 촬영하여 Blender로 가져온 뒤, 해당 이미지를 3D 모델에 맵핑했다고 들음. 내가 알기론 포토그래메트리는 사용하지 않았음
     * 이 팀이 처음 만드는 건 아님(Apricot 프로젝트). 2008년에는 선택할 수 있는 무료 게임 엔진이 적어서, Go Frankie 게임을 Blender Game Engine과 Crystal Space 두 버전으로 따로 만들어야 했었음. 이제는 Godot 같은 걸로 깔끔하게 만들 수 있게 돼서 훨씬 편해졌음
          + Apricot이 괜찮은지 궁금함. 3D 플랫포머 장르를 좋아하는데, 예고편을 봤을 땐 크게 기대가 안 됐음
          + 만약 당시 Godot이 있었다면 Go Frankie의 모습도 완전히 달라졌을 거라 상상하게 됨
     * 출시 당일 약 한 시간 정도 플레이해봤음.
       장점: 캐릭터와 환경 디자인이 매력적이고, 모델링, 애니메이션, 사운드 모두 훌륭함. Blender와 재능 있는 팀이 만들 수 있는 결과물의 좋은 데모임. 직접 경험해볼 가치가 충분함
       단점: macOS에서는 시작이 느리고(오프닝 화면 전에 beachball이 몇 초 뜸), 이런 현상은 Godot로 내보낸 macOS 버전에서 자주 나타나는 것 같음. 숲이 복잡한 장면에선 성능이 고르지 못하고(움직임이 끊기거나 프레임 드랍이 발생함). 지금 상태로는 Blender의 데모로는 좋은데, Godot 3D의 장점은 덜 드러남
          + 그럼에도 불구하고, 컨셉 증명 및 Blender의 쇼케이스라는 측면에서는 매우 인상적임
          + macOS에서 느린 시작 문제가 있는데, 보통 이건 코드 서명이나 인증(notarization) 관련 이슈가 원인임
          + 개인적으로는 Godot가 굳이 mac에서의 성능 이슈까지 신경 쓸 필요는 없다고 생각함
     * Godot로 만든 게 흥미로움. Blender에 아직 자체적 Blender Game Engine이 있는 줄 알았는데, 공식적으로 2019년에 중단됐다고 함(Blender Game Engine 위키피디아)
          + Blender Game Engine은 여전히 UP-BGE라는 이름으로 살아있음. 사용하기 쉽고, Godot보다 초보자에게 더 친절한 면이 있음
     * 이 게임 비주얼이 정말 따뜻하고 힐링되는 느낌임. 커다란 개가 눈 쌓인 숲을 돌아다니며 아이를 돕는다는 설정 자체가 극강의 힐링 게임 분위기임
          + 어릴 적 엄청 재미있게 했던 Sleepwalker라는 게임이 생각남
     * 게임이 굉장히 멋지게 보여서 기대됨. 단, 이건 흔히 말하는 ""공짜 맥주(무료)"" 형태임. 소스코드를 보려면 로그인과 결제가 필요한 듯함
       관련 링크
          + 사실상 자유 소프트웨어임. 소스의 라이선스가 약간 혼동스러운데, 정리를 빨리 진행할 예정임. 전체 제작 리포지토리는 대부분 원본 에셋이라 CC-BY 라이선스를 따르며, 게임 소스코드는 GPLv3임. 즉 직접 무료로 배포하진 않지만, 받은 사람은 자유롭게 재배포 가능함
          + 소스코드를 보려면 로그인하고 결제해야 한다는 점은 GPL과 비슷하다고 생각함
     * 팁 하나 남깁니다: 강에서 VLC(아마 캐릭터명 또는 아이템)를 꺼내려 애쓰는 키드가 있다면, 직접 끌어줘야 함. 아무 도움 없이 한참 지켜보다가 내가 개가 되어있단 걸 깨달음. 도움을 요청하지 않으니 내가 뭘 해야 할지 헷갈렸음
     * 정말 귀엽고 아기자기한 게임임. 조작감도 훌륭함. 종이공예풍 아트워크도 참 아름다움. Steam Deck에서의 성능은 아쉽게도 60프레임은 못 내고 평균 40 프레임 정도 나옴
     * 정말 멋진 게임인데, 웹 버전도 있으면 좋겠다고 생각함. 포팅 난이도는 잘 모르겠음
          + Godot는 GDScript로 짜면 웹으로 내보내기가 가능함. 아쉽게도 C#용 웹 익스포트는 아직 준비가 안 됨
     * 과거의 관련 논의글이 있으니 참고 바람(이전 논의)
          + 참고로 이것도 확인하면 좋겠음(추가 논의)
"
"https://news.hada.io/topic?id=22090","빅 LLM들의 아키텍처 비교","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            빅 LLM들의 아키텍처 비교

     * 최근 7년간 LLM 아키텍처는 GPT-2(2019)부터 DeepSeek-V3, Llama 4(2024-2025)까지 구조적으로 큰 변화 없이 진화하여 놀랄 만큼 유사성을 유지함
     * DeepSeek V3/R1, Llama 4 등 최신 모델은 Mixture-of-Experts(MoE), MLA, Sliding Window Attention 같은 새로운 최적화 방식을 도입하여 메모리 효율과 추론 성능을 향상함
     * OLMo 2, Gemma 3 등 일부 오픈소스 모델은 투명한 데이터 공개와 독특한 normalization layer 배치로 연구·개발에 좋은 설계 사례로 주목받음
     * Qwen3, SmolLM3, Kimi 2 등 다양한 크기와 구조의 모델이 등장, MoE와 Dense 아키텍처의 장단점과 활용 목적에 따라 선택지가 넓어짐
     * 최근 LLM의 공통 트렌드는 대형화·고도화와 함께, 효율적인 구조 개량 및 다양한 하드웨어 환경 대응임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론

     * 2017년 GPT 원형 이후 GPT-2(2019)부터 DeepSeek-V3 및 Llama 4(2024-2025)까지 보면 LLM 아키텍처는 큰 틀에서는 비슷함(기본적인 트랜스포머 구조는 크게 변하지 않았음)
     * Positional embedding은 절대형에서 RoPE 등의 방식으로 변화했고, Multi-Head Attention은 메모리/연산 효율이 높은 GQA(그룹 쿼리 어텐션)으로 넘어가고 있지만 근본 구조는 유지 중
     * 성능 비교는 데이터셋·학습 방식에 따라 달라져 직접적인 구조 비교가 어려움
     * 본문에서는 최근 오픈 LLM들의 아키텍처 구조 변화를 집중 분석함

1. DeepSeek V3/R1

     * DeepSeek R1(2025년 1월)은 DeepSeek V3 아키텍처(2024년 12월)를 기반으로 만들어졌으며, 고도화된 추론 능력과 대규모 파라미터(671억개)로 주목받음
     * 핵심 아키텍처: Multi-Head Latent Attention(MLA), Mixture-of-Experts(MoE)
     * MLA: Key/Value를 저차원으로 압축해 KV cache 메모리 절감, GQA 대비 더 좋은 성능
     * MoE: FeedForward 모듈을 여러 expert로 분산, 토큰마다 일부 expert만 활성화하는 sparse 구조
          + DeepSeek V3: 256개 expert, 전체 파라미터 671B, 추론시 9개 expert(37B 파라미터)만 사용
          + 항상 활성화되는 shared expert로 일반 패턴 학습 효율화
     * 특징: 대형(671B)이지만 추론 효율, MLA로 GQA 대비 성능 우위, MoE로 대규모 학습 용량 확보

2. OLMo 2

     * Allen Institute for AI의 완전 공개형 모델
     * 성능보다는 투명한 설계와 코드 공개가 장점
     * 아키텍처 포인트: RMSNorm 위치(Post-Norm 적용), QK-Norm
          + 기존 GPT류는 Pre-Norm, OLMo 2는 Attention/FeedForward 뒤에 normalization 적용(Post-Norm flavor)
          + QK-Norm: Attention의 query/key에 추가 RMSNorm, 학습 안정성 개선
     * 전통적인 Multi-Head Attention(MHA) 구조 유지
     * Llama 3 등과 유사하나 normalization 전략 차별화

3. Gemma 3

     * Google의 대표 오픈 LLM, 다국어 지원을 위한 대형 Vocabulary 및 27B 크기 모델에 집중한 것이 특징
     * Sliding Window Attention(로컬 window)으로 KV cache 메모리 대폭 절감
          + Gemma 2: Global/Local 1:1, 4k window, Gemma 3: 5:1 비율, window 1024로 감소
          + 성능(Perplexity)에 거의 영향 없음
     * Normalization: GQA module 주변에 Pre-Norm, Post-Norm RMSNorm을 모두 적용
     * Gemma 3n: 소형 기기 대응, Per-Layer Embedding(계층별 파라미터만 GPU 상주), MatFormer(부분 모델 분할 사용) 로 경량화

4. Mistral Small 3.1

     * Mistral Small 3.1 24B, Gemma 3 27B보다 빠르고 벤치마크 상위권
     * 커스텀 토크나이저, KV cache·layer 수 축소로 추론 지연 최소화
     * Sliding window attention은 버리고, 최적화된 GQA + FlashAttention 사용하여 추론 속도 및 코드 효율 집중

5. Llama 4

     * MoE 아키텍처 적극 도입하여 추론 효율성과 모델 용량 모두 확보, DeepSeek-V3와 구조 유사
     * GQA 사용, MoE expert 수와 hidden size 다름
          + DeepSeek-V3: 9개 expert(2,048), Llama 4: 2개 expert(8,192), 활성 파라미터 17B(DeepSeek 37B)
     * MoE 블록과 Dense 블록을 번갈아 삽입하는 클래식 MoE 설계
     * 최근 LLM에서 MoE의 대중화 확인

6. Qwen3

     * 다양한 크기의 Dense (0.6B~32B)와 MoE(30B-A3B, 235B-A22B) 버전 제공
     * 소형(0.6B)은 학습·추론 효율 및 토큰 throughput이 뛰어남. 초경량 LLM 중 뛰어난 성능 확보, 메모리 효율 및 학습 편의성도 탁월
     * Dense: 레이어 수 많고, 메모리 적음, 속도는 느림(Llama 3 1B 대비)
     * MoE: Qwen3 235B-A22B는 22B active param, shared expert는 사용하지 않아(이전 Qwen2.5-MoE는 shared expert 포함) 효율성 증가
     * Qwen3 235B-A22B와 DeepSeek-V3는 전체 구조에서 매우 유사
     * Dense와 MoE 모두 제공해 다양한 활용 목적에 대응

7. SmolLM3

     * 3B 파라미터급 소형 모델, Qwen3 1.7/4B, Llama 3 3B, Gemma 3 4B와 경쟁
     * 아키텍처는 표준적이지만 NoPE(No Positional Embedding) 적용
          + RoPE 등 positional encoding 없이 causal mask만 활용
          + 긴 시퀀스에서 길이 일반화(Length Generalization) 향상
          + 실험적 구조, 일부 레이어에만 적용

8. Kimi 2

     * 1조 파라미터 대형 오픈 모델로 오픈 모델로는 최대 규모
     * DeepSeek-V3 구조를 기반으로, MoE 레이어 수 확장 및 MLA의 헤드 수 조정
     * 학습에 AdamW 대신 Muon optimizer 사용하여 학습 효율화, loss decay 우수
     * DeepSeek-V3 대비 더 많은 MoE expert, MLA head 수 축소
     * Kimi 1.5의 경험 기반, Kimi 2로 오픈웨이트 공개 및 최고 수준 성능 달성

결론 및 트렌드

     * 최근 LLM은 근본적인 구조는 유지하되 아키텍처 대형화, MoE 및 각종 효율화 구조의 도입이 특징
     * 오픈 모델의 경우 투명한 데이터, 설계, 코드 공개로 연구 및 산업적 활용 가치가 높아짐
     * Dense와 MoE, MLA·GQA·Sliding Window Attention, 다양한 normalization 전략 등 각 모델마다 최적화 지향점이 다름
     * 하드웨어 환경, 활용 목적, 학습·추론 효율성에 따라 아키텍처 선택지가 다양해진 시기임

   한글은 qwen이 잘 되는 것같아요.

        Hacker News 의견

     * 이번 글은 LLM 아키텍처에 대해 배울 수 있는 완벽한 추상화 수준과 상세한 설명이 있어서 원래 논문을 읽는 것보다 훨씬 쉽게 많은 정보를 습득할 수 있었음
     * 초보자와 전문가 사이 단계에 있는 사람들에게는 이 글의 다이어그램이 매우 인상적으로 느껴짐, 최신 모델들이 한눈에 정리된 모습이 정말 유용함
     * 관련 내용으로 DeepSeek가 트랜스포머 아키텍처를 어떻게 개선했는지 설명하는 글과, Meta의 슈퍼인텔리전스 관련 분석 아티클 일부 섹션도 참고할 만함
     * 나처럼 최신 동향을 못 따라가던 사람들에게는 이런 요약글이 정말 반가운 catchup임
     * 차후에는 o5, o3 Pro, o4 또는 4.5, Gemini 2.5 Pro, Grok 4, Claude Opus 4 등 닫힌 소스의 frontier 모델들에 대한 소문까지 포함한 2부가 나왔으면 하는 바람임
     * 서로 다른 LLM 아키텍처 차이를 자세히 정리해줘서 고맙고, 덕분에 이해하기도 쉽고 교육적임
     * 솔직히 GPT-2(2019) 시절과 비교하면 지금의 발전 속도가 믿기 힘들 정도임, 요즘은 LLM의 성능을 제대로 비교하는 것도 어려울 정도인데 2주마다 새로운 모델이 벤치마크를 갱신함. DeepSeek가 언급된 점이 반가운데, V3에서 도입된 아키텍처 혁신 덕분에 계산 효율성이 크게 향상됐고, 이 점이 당시 다른 모델과 차별점을 없게 만든 결정적인 포인트였음
     * 다양한 새로운 아키텍처들이 정확도나 속도 면에서 많은 혁신을 이뤘지만, 여전히 정확한 정보 생성을 보장하는 근본적인 문제는 해결되지 않고 있음. Retrieval Augmented Generation(RAG)이나 에이전트 등 다양한 방식들이 이런 문제를 개선하긴 하지만, 앞으로의 아키텍처가 결국 이런 방식들을 대체할지도 궁금함
          + 근본적으로 트랜스포머는 텍스트 예측을 목표로 훈련하는데, 이 방식은 논리성 임베딩에 한계가 있기 때문임. 더 이상 환각 현상을 줄이려면, 완전히 다른 교육 목표가 필요하다고 생각함
          + 모델은 어떤 상황에서 일반화를 해도 되는지, 아니면 더 많은 정보가 필요한지 구별하지 못함. 예를 들어, 왜 어떤 메서드는 존재하는데 다른 비슷한 함수는 없는지 쉽게 구별하지 못함. 어릴 적 어머니를 훌륭한 cooker라 부른 적이 있는데, 머신과 인간에게 각기 다른 단어가 할당된다는 걸 몰랐었음. 이런 비슷한 단어의 일반화가 모델에도 적용된다고 느끼게 됨
          + DeepSeek-V2와 Llama 3.1 같은 최근 아키텍처는 설계적 개선만으로도 사실성(factuality)이 꽤 향상된 결과를 보여줌. 특히 집중(attention) 메커니즘과 환각 억제에 특화된 학습 목표가 배경임
          + RAG(검색 기반 응답)는 구조적으로 단순하고 구현도 쉽지만, 왜 아직까지 기본 LLM에 내장되지 않았는지 늘 궁금했음. 아예 모델 내부로 통합되지 못하는 건 RAG나 그 변형들의 근본적인 한계를 반증하는 것 같음. 정말 효과적인 방식이라면 외부 추가가 아니라 아키텍처 기본 기능으로 도입됐을 것이라고 생각함
     * Claude에게 원문을 읽고 새로운 아키텍처를 제안해보라고 지시했음
       Claude의 결과물 링크
       하지만 이 결과가 실제로 쓸만한지는 잘 모르겠음
"
"https://news.hada.io/topic?id=22033","Uzu - 애플 실리콘용 고성능 AI 추론 엔진","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Uzu - 애플 실리콘용 고성능 AI 추론 엔진

     * Apple Silicon 환경에서 AI 모델을 빠르게 실행하기 위한 Rust 기반 추론 엔진
     * GPU 커널 또는 CoreML 하단의 MPSGraph를 선택적으로 활용하는 하이브리드 구조
     * 자체 포맷 모델을 사용하며, lalamo 도구를 통해 Llama3 등 다양한 모델을 변환해 사용할 수 있음
     * llama.cpp 대비 속도에서 대부분 우위를 보이며, 특히 Qwen3-0.6B에서는 13배 빠른 처리 속도를 기록
     * Swift 바인딩, CLI 인터페이스, Rust API 등을 통해 유연한 개발 및 통합이 가능함
     * 모듈화된 구성과 애플기기의 유니파이드 메모리 활용으로 성능 극대화, 성능 검증 가능한 추론 경로 제공 등으로 신뢰성과 확장성 확보
"
"https://news.hada.io/topic?id=22060","NIH(Not Invented Here)가 잘못된 의존성보다 훨씬 저렴함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                NIH(Not Invented Here)가 잘못된 의존성보다 훨씬 저렴함

     * 의존성은 무료 기능처럼 보이지만, 실제로는 다양한 비용과 복잡성을 동반함
     * 잘못된 의존성은 러닝커브, 갑작스런 인터페이스 변경, 배포·설치 문제 등 다양한 리스크를 초래함
     * 대표 사례로 TigerBeetle은 보안·성능·운영 단순성을 위해 ""제로 의존성"" 정책을 지향함
     * 저자는 의존성 평가 프레임워크(보급성, 안정성, 깊이, 사용성, 완전성) 를 제안함
     * 좋은 의존성과 나쁜 의존성을 구별하는 비판적 사고와 의존성 선택 시 단기 생산성만이 아니라 전체 비용과 위험을 감안한 신중한 판단이 필수임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

NIH(Not Invented Here)보다 잘못된 의존성이 더 큰 비용임

  의존성의 단점과 숨겨진 비용

     * 많은 개발자들이 의존성 추가를 ""공짜 기능"" 처럼 여기지만, 실제로는 다음과 같은 비용이 발생함
          + 배우는 데 드는 시간과 복잡성
          + 의존성 설치 자체가 어려운 경우도 빈번함
          + 주요 버전 업그레이드 시 내 코드까지 크게 수정해야 하는 부담
          + 의존성이 결국 배포/설치 환경에 반드시 들어가야 함 (컨테이너, 번들링 등 복잡성 유발)
     * 잘못된 의존성 도입으로 인해 핵심 기능과 관계없는 복잡한 배포 구조가 만들어지기도 함

TigerBeetle의 제로 의존성 원칙

     * Tigerbeetle은 Vanilla Zig 기반의 재무 데이터베이스로, 제로 의존성 정책을 채택함
          + Zig 언어만으로 개발하며, Zig 툴체인 외의 외부 의존성을 두지 않음
          + 의존성으로 인한 공급망 공격 위험, 성능 저하, 설치 시간 증가 등 문제를 피하려는 목적
          + 인프라에 깊게 뿌리내린 소프트웨어일수록 의존성으로 인한 비용이 전체 스택에 증폭됨
          + 표준화된 작은 툴박스 활용이 유지보수와 개발 능률에 유리함
     * Zig 하나로 새로운 문제를 빠르게 다루고 복잡도를 줄이는 데 집중함

의존성 평가 프레임워크

     * 모든 개발자가 완전한 무의존은 불가능함을 인정하면서도, 의존성은 신중히 평가해야 함을 강조함
     * 저자는 다음 5가지 기준으로 의존성 평가를 제안함
          + 보급성(Ubiquity): 대상 환경에 얼마나 흔하게 존재하는가? 별도 배포/설치 필요성 여부
          + 안정성(Stability): 하위 호환성, API 변경 빈도, 지원 중단 등 이슈 발생 빈도
          + 깊이(Depth): API 아래 숨어있는 실제 기능량, 직접 구현 시 대체 난이도
          + 사용성(Ergonomics): API가 직관적/선언적이며 쓰기 쉬운지, 문서화 현황
          + 완전성(Watertightness): 추상화가 얼마나 잘 동작하는가, 하부 기술을 얼마나 신경 쓰지 않아도 되는가
     * 개발 커뮤니티는 주로 사용성만 논의하며 나머지 네 가지는 간과되는 경우가 많음

좋은 의존성 사례

     * POSIX 시스템 호출
          + 보편성: Linux, Android, macOS, BSD 등 거의 모든 플랫폼에서 사용 가능함
          + 안정성: 인터페이스 호환성이 매우 높고 변화 거의 없음
          + 깊이: 단일 API로 수십만 줄의 커널 코드 감춤
          + 사용성: 다소 전통적인 C 스타일이기는 하나 사용에 큰 무리는 없음
          + 완결성: 대부분 문제 없으나 저장 장치의 데이터 영속 처리 등 세부 이슈 있음
     * ECMA-48 터미널 제어 코드
          + 보편성: Windows의 cmd.exe를 제외하면 대부분의 터미널에서 지원함
          + 안정성: 1991년 이후 변경 없음
          + 깊이: 직접 표준을 만드는 것이 터무니없이 힘듦
          + 사용성: Esc 캐릭터로 인한 난독성을 제외하면 무난함
          + 완결성: 하드웨어 의존성 걱정이 매우 적음
     * 웹 플랫폼 (Web API, HTML, JS, CSS 등)
          + 보편성: 웹 브라우저가 전 세계 거의 모든 환경에 설치됨
          + 안정성: 강력한 하위 호환성 정책
          + 깊이: 자체 브라우저 제작은 현실적으로 불가능할 정도로 깊음
          + 사용성: 약간의 복잡함 있으나 문서화와 개발 도구 우수함
          + 완결성: 파일, 오디오, 비디오 등 특이 상황 제외하면 매우 완결성이 높음

결론

     * 카피-앤-페이스트, 의존성 남용 대신 비판적 사고와 총체적 비용 분석이 필수임
     * 의존성을 도입할 때는 모든 의존성의 비용과 이득을 비판적으로 평가하고,
       전체 시스템의 잠재적 위험과 비용도 반드시 고려해 신중하게 선택하는 것이 장기적으로 훨씬 저렴하고 안전함

   TigerBeetle: 회계에 특화된 OLTP 데이터베이스

        Hacker News 의견

     * TigerBeetle라는 예시를 처음 들어서 직접 찾아봤음. 만약 안전성, 성능이 중요한 Zig 기반의 금융 원장 데이터베이스를 만드는 상황이고 CPU 한 코어에서 초당 백만 건의 트랜잭션을 감당해야 한다면, 의존성 추가는 위험 부담이 커서 피하는 게 완전히 합리적임. 하지만 대부분의 일반 개발자들은 평범한 CRUD 시스템을 만들고 있고, 이들은 일반적으로 그렇게 강하지 않은 경우가 많음. 많은 의존성이 버그투성이일 수는 있지만, 대다수 개발자가 직접 만드는 것보다 품질이 높은 경우도 많음. 기업도 실제로는 충원 가능한 개발자의 수준에 의해 병목이 생김. 각자의 상황이 다르니, 반대되는 조언도 각각의 맥락에서는 모두 옳을 수 있음을 기억해야 함
          + TigerBeetle에서 실제로 일하고 있음. 핵심은 맥락임. TigerBeetle이나 rust-analyzer는 강한 개발 문화가 있지만 해결하는 문제가 달라 서로 다른 문화를 형성함. 글에서 언급된 의존성은 POSIX, ECMA-48, 웹 플랫폼처럼 라이브러리라기보다는 시스템 인터페이스임. 라이브러리 의존성은 문제 생기면 직접 새로 쓰면 그만이지만, 시스템 인터페이스 같은 근본적인 것들은 바꾸기가 사실상 불가능하거나 비용이 큼. 소프트웨어의 범위에 맞지 않는 일을 하지 않는 결정을 내리는 것이 강력함. 예를 들어 행렬 곱셈 코드를 만들 전문 팀이라면, 코어 업무와 상관없는 다른 라이브러리는 외부 것을 사용해도 되지만, 제품의 책임 분해를 더 잘 설계해야 한다고 생각함. 이렇게 하면 필수 의존성을 시스템 내에서 더 잘 격리할 수 있음
          + 이 관점의 문제는 바닥 수준의 개발자만 염두에 두었을 때만 성립함. 기술 분야는 조언을 가장 낮은 수준에 맞게 희석시키는 경향이 있는데, 솔직히 내가 일한 곳에서는 개발자들이 고장 난 의존성을 복제해 문제를 해결하지 못할 정도로 약했던 환경이 거의 없음. CRUD에 대한 비하도 많은데, 나쁜 추상화는 CRUD에서 엄청난 시간 낭비와 고통을 유발함. 인기 있는 것들도 실제로 진짜 기초적인 튜토리얼 수준이 아니면 여러 문제가 많고 생산성이 떨어짐
          + 개발자 수준 자체와 관련 없는 이야기임. 어떤 툴체인이나 제품을 쓰든 결국 남이 만든 의존성을 사용하고 있음. 주변에 직접 행렬 곱셈 코드 구현하는 사람은 극소수이고, 그들조차도 자신과 상관없는 오픈소스 라이브러리까지 직접 구현하진 않음. 대개 규제 요건이나 개인적 관심, 혹은 특정 라이브러리에 대한 애착이 있을 때 의존성 자체에 집착하게 됨. 모두가 이런 원칙을 완전히 적용하면 해변에서 모래만 주우며 살고 있을 것임
          + ""평균적인 CRUD 개발자들은 강하지 않다""라는 의견은 지나치게 단정적임. 대부분의 개발자는 자신이 일할 시스템을 선택할 수 없고, 개발 기간에도 리소스가 항상 부족함. '싼' 의존성을 활용해야 정상 동작하는 소프트웨어를 빠르게 출시할 수 있음. 이 현실을 제대로 모르고 한 이야기 같음
          + TigerBeetle은 비교적 최근에 출시한 스타트업이고 1.0 정식 버전도 아직임. 이런 접근이 정말 효과가 있을지는 너무 이르다고 생각함
     * NIH(Not Invented Here) 자체는 어디까지 책임질지 현실적으로 판단해서 사용할 때 굉장히 쓸모있음. 예를 들어, 내 도메인에 딱 맞는 웹 프론트엔드 프레임워크는 독자적으로 만들어 유지관리할 만한 가치가 있는 경우가 많음. 그러나 데이터베이스, 게임 엔진, 웹 서버, 암호화 관련 기본 기능 등은 얘기가 다름. 기존에 나온 솔루션으로는 해결할 수 없을 정도로 난이도가 높다면, 문제 재정의를 먼저 고민해야 함. SQLite 테스트 전체를 처음부터 다시 만들 바에야, 문제 자체를 새롭게 정리하는 게 훨씬 저렴하다고 생각함
          + 데이터베이스, 게임 엔진, 웹 서버, 암호 프리미티브 등도 경우에 따라 직접 만드는 게 더 나은 사례가 많음. 단순 파일과 런타임 인덱스만으로 충분하다면, SQLite조차 과한 선택일 때가 많음. 많은 게임은 소규모 팀일수록 커스텀 엔진이 유리할 때가 많음. 완성된 엔진의 장점은 파이프라인뿐이지만, 큰 오버헤드가 따르기 때문임. 웹서버는 FastCGI 앱과 복잡도 차가 없음. 암호화도 모든 상황이 보안 문제는 아니어서, 단순 해시 확인 같은 건 직접 구현해도 무방함. 어려워 보이는 주제에 배운 무기력감을 가지는 건 좋지 않다고 생각함. 기존 솔루션이 문제를 해결한다고 해서 그게 최적이거나 가장 효율적인 방식은 아니라는 점도 중요함
          + 그렇다면 왜 많은 데이터베이스 엔진이 존재하나? 결국 복잡한 컴퓨터 시스템은 각자의 트레이드오프가 존재함. 제약 조건, 확장성, 동시성, 보안, 데이터 특성, 저장 방식 등 선택지가 매우 다양함. 나는 의존성 비용이 클수록, 그 의존성 자체가 타 의존성 최소화를 추구할 때 더 신뢰가 간다고 생각함. 자동화된 의존성 관리 시스템이 오히려 문제를 복잡하게 만드는 경향이 있어서, 신중한 수동 관리가 부담을 줄여준다고 봄
          + 서드파티 의존성을 사용하는 이유는 두 가지로 생각함. (1) 서비스 제공자가 직접 퍼블리시하며, 상대적으로 수명이 일치하는 경우. (2) 내가 작성하고 싶지 않은 복잡한 코드를 대신하는 경우. (1)은 비즈니스적 이유가 있으니 문제 없음. 단, 해당 서비스가 업데이트될 때 큰 변화가 생기는 건 감수해야 함. (2)는 내가 피해가려는 코드의 복잡성에 따라 가치가 달라짐. 의존성을 도입한다는 것은 상대방의 일정에 맞춰 내가 업데이트/테스트에 시간과 리소스를 소비해야 하고, 그 책임을 떠안게 됨
          + RDBMS로 해결할 수 없는 문제를 빨리 만나게 됨. RDBMS는 동시 데이터 수정과 가변 데이터셋 지원 중심으로 짜여 있어서, 이게 필요 없다면 단순 인덱스로도 엄청난 성능 향상이 가능함. 데이터가 불변이라면 RDBMS 대비 훨씬 빠른 자체 구현이 가능함
          + RDBMS 사례가 흥미로움. 위키피디아에는 100가지가 넘는 RDBMS가 있고, 각각이 해결할 수 있는 문제와 해결 못하는 문제가 있음. 실용적 해결을 고민하고 실제로 실행한 결과임
     * 의존성은 위험을 유발하지만, 전혀 사용하지 않으면 개발 및 시장 출시 경쟁력에서 뒤처질 수 있음. 그래서 의존성 관리 프로세스가 중요함
         1. 오픈소스 의존성만 고려함
         2. 신규 도입 시 코드 리뷰뿐 아니라, 라이선스 확인, 제거 시 소요 노력, 보안 취약점 및 버그 이력, 업데이트 지속성, 커뮤니티 활력도 등 다각적으로 검토함
         3. 가능하다면 의존성 보안 이슈 주기적 검사 필요. 대규모로 할 때는 비용이 큰 문제임. (관련 아이디어 공유: https://blog.majid.info/supply-chain-vetting/)
         4. 유지/포크 부담을 감당할 수 있는 의존성만 도입. 최소한 소스에서 직접 빌드해본 적은 있어야 함
         5. 모든 의존성을 선제적으로 포크함. left-pad 사태처럼 저장소가 갑자기 사라질 수 있기 때문임
          + 4번, 5번 항목은 정말 중요하지만 자주 잊혀짐. 개인 프로젝트에서도 한동안 방치 후 복귀하면 의존성이 구식이 되어 있거나 리포가 삭제된 경우를 겪어봄. 그래서 요즘은 소스 자체를 비공개로 포크해서 직접 빌드해보고, 모든 의존성의 의존성까지 소스 수준으로 포크해둠. 이렇게 하면 나중에 에코시스템이 급격히 변해도 피해를 최소화할 수 있음. 바이너리보다 소스 라이브러리를 선호하게 됨
          + 5번에서 포크는 지나치게 부담이 클 수 있음. 나만의 git이나 캐시 프록시에 의존성을 넣어두는 벤더링도 괜찮은 방법임. 특히 장수하는 프로젝트에 더 어울림. NodeJS처럼 의존성 파일이 많을 때는 Yarn, PNPM 같은 툴이 좀 더 효율적임
          + 4번과 관련해, SQLite 같은 유명 의존성은 내가 만드는 제품보다 훨씬 오래 지속됨. 내 제품이 해당 오픈소스 자체보다 오래 갈 거라 생각하면 그게 더 오만한 자세임. Linux 커널도 직접 빌드할 생각 없음
          + 모든 코드는 최소한 네트워크 연결 없이 빌드가 가능해야 함. 바이너리 아티팩트 없는 게 가장 좋지만, 항상 현실적으로 가능한 건 아님
          + 훌륭한 통찰임. 이 내용을 내 의존성 도입 절차 문서에 추가할 예정임. 문제는 자바스크립트 같이 의존성트리가 너무 깊은 경우임
     * 오랜 경험상 모든 것은 결국 ""상황에 따라 다름(It Depends™)""임. 젊었을 때는 원칙만 따르고 예외 없음에 집착하다가, 부적합한 라이브러리나 패러다임을 억지로 적용하면서 최악의 코드를 만들기도 했음. 지금은 자주 사용하는 것들은 직접 라이브러리화해 패키지로 빼놓고, 필요한 것 중 내가 못하는 사항만 외부 의존성으로 채움. 품질이나 관리성만 납득할 수 있으면 외부 것도 기꺼이 받아들임
     * 에너지 업계는 의존성을 의도적으로 피하는 경향임. 외부 의존성을 도입하면 변경 사항을 모두 살펴야 하기 때문임. AI 코드 도구가 큰 도움이 되었고, 주로 CLI 도구 생성 등에 활용함. 오픈API 문서도 LLM을 활용해서 만들고, 이를 Go 표준 라이브러리로 서비스함. LLM 자체는 외부 의존성이지만, 그것으로 만든 CLI 도구는 실제 코드와 무관해서 품질 요구가 낮음. 물론 프론트엔드 개발자들이 React 없이 일하고 싶어하지는 않지만, 그런 제품은 외부로 다루니까 예외임. 엔지니어들의 품질 의존성 집착을 덜게 만들 소도구를 제공하면 의존성 최소화 정책이 더 쉬워짐
          + LLM이 학습에 사용한 오픈소스 코드 일부를 코드로 내뱉기도 하는데, 그런 경우 의존성 포크해서 자기 소유물처럼 다루는 것과 별 차이 없지 않나 궁금함
     * 의존성의 좋고 나쁨을 구분할 줄 아는 것이 중요한 역량임. 내 생각에는 유료 의존성은 대개 불리함. 제공하는 회사가 락-인 유도를 위해 설계했을 가능성이 큼. ""의존성 미니멀리즘""이 좋은 컨셉임 (VitalikButerin의 관련 트윗)
          + 유료 의존성은 지원이 한 군데밖에 없어서, 제공 회사가 문 닫으면 프로젝트 전체가 위험해짐. 대부분의 회사는 영원하지 않기 때문에, 의존성의 미래가 내 프로젝트 궤도에 영향을 미치는지 꼭 따져야 함
          + 비기술 출신 팀이 강제한 유료 의존성에서 나쁜 경험 있음. 반면 사이드키크처럼 커뮤니티에서 널리 쓰이는 '오픈코어' 의존성은 갑자기 사라질 확률이 훨씬 낮아서 믿을 만함. 유료의 장점은 회사가 건전하게 운영될 때 지원 걱정을 안 해도 된다는 점임
          + 회사 제공 유료/무료 컴포넌트 모두 벤더 락인은 존재함. 위험 관리는 통합 팀의 몫이고, 대안을 찾거나 모듈화해서 리스크를 조절해야 함
          + 유료라면 반드시 오픈 표준이나 대체 구현체가 필요한 인터페이스로 납품해야 락-인을 막을 수 있음. 다른 선택지가 있으면 전환 옵션이 유지됨
          + 유료 의존성이 나쁘다는 것은 비용이 부족할 수 있다는 신호일 수 있음. 내 코드를 지원하는 게 누군가의 '업무'이길 원함. 지원자 수가 많거나 자발적으로 책임지는 개발자가 많으면 안정되고, 개인 프로젝트라면 소스 오픈이라도 문제 생길 때 지원해줄 사람이 필요함. 회사가 중단해도 방치 않으려는 책임감이 중요함
     * 많은 사람들이 신규 코드 작성에 집착하는 경향이 있는데, 실제로는 형편없는 의존성조차 사용이 훨씬 더 효율적인 경우가 9할임
          + 의존성은 양날의 검임. 소프트웨어 대기업은 코드 유지를 포기하고 새로 작성하는 게 더 싸서 그렇게 하는 경우가 많음. 소규모 웹/브랜딩 업체에선 고품질 백엔드가 사실상 필요 없음. 대신 dreaded 엔터프라이즈 패턴이 생긴 이유는, 5년 후에도 문서화와 업계 기억이 사라져도 외부 의존성 없이 보존되는 코드의 격리와 유지보수가 가능하게 하려는 목적임. 외부 의존성은 지원 중단, 혹은 파괴적 변화 발생 리스크 두 가지를 내포함. 결국 기능 개발 흐름에 영향이 감. 내부 컴포넌트면 이런 트레이드오프도 내부에서 통제 가능함. SaaS라면 단기적 성공을 위해 의존성을 빠르게 쓰는 게 맞고, 안전과 장기 지원이 필수라면 멀리 봐야 성공함. 신규 코드 작성이 조직의 병목이 되는 경우는 거의 없음
          + 회사에서 보안 취약점 및 라이선스를 얼마나 진지하게 관리하는지 궁금함. 예전에는 의존성에 관대했는데, 보안과 라이선스에 엄격한 회사로 옮긴 뒤 엄청나게 관점이 바뀜
     * 라이브러리와 프레임워크의 차이도 중요함. 라이브러리는 한 가지 일을 잘 하는 툴이고, 프레임워크는 앱 구조 전체를 규정함. Go 커뮤니티는 대형 프레임워크를 멀리하며, 표준 라이브러리와 경량 라이브러리, 그리고 필요하면 소스 복사/붙여넣기를 선호함. 예를 들어 Gin(웹API), GORM(ORM) 같은 프레임워크는 사용성도 좋지만, 내부 구조를 제한하고 복잡성을 키움. Go는 표준 SDK 자체가 상당히 강하기 때문에 필요 이상으로 의존성 추가 안하는 게 맞다고 생각함
     * 저자는 뉴질랜드 출신임. 뉴질랜드의 Number 8 wire 정신, 즉 '있는 것과 손재주로 어떻게든 해결하는 태도'가 배경에 깔려 있음 (Number 8 wire 위키 문서)
          + 뉴질랜드가 아닌 다른 나라의 경험 많은 개발자들도 비슷하게 동의하는 경우가 많음. 잘못된 의존성 선택이나 필요 없는 라이브러리 업그레이드로 고생한 경험은 거의 모두가 있음
          + 뉴질랜드 출신으로 느끼기에 Number 8 Wire 멘탈리티는 이미 20년 전에 사라진 것 같음
          + 오늘 처음 알게 된 사실임. 호주의 ""She'll buff out, mate"" 유행어가 떠오름
     * 의존성의 대규모, 상용화 기반은 확장성에도 영향을 줌. 나보다 100~1000배 큰 규모로 배포하는 곳에서 활용된 도구라면 내 문제에서 한계에 부딪칠 확률이 낮음. 그 규모에서 버그도 먼저 발견하거나 고치고, 결과적으로 내게 더 안전하게 돌아옴
          + 대형 라이브러리도 소규모 환경에서 아예 안 돌아가는 경우가 있음. 예를 들어 Swift 프로토콜버퍼 컴파일러가 예전엔 예측 못한 필드에서 크래시가 났음. 많은 대형 회사도 대규모 목적 외에는 직접 해당 경로를 테스트하지 않음
          + 큰 회사(Meta, Google, Microsoft 등)에서 만든 유명 라이브러리에서 심각한 버그 발견 경험이 있음. 이슈 신고해도 고치는데 시간이 오래 걸리고, 변화 거부감도 심함. 이런 상황에서 결국 직접 구현하니 오히려 더 빠르고 성능도 향상됐음. 특히 컨설팅 업계는 고객의 불합리한 요청으로 작업의 방향이 흔들리곤 함. 개발자로서 내가 직접 할 수 있다는 확신이 커지니, 굳이 거대한 외부 의존성보다 내가 구현하는 게 나을 때가 많아짐. 물론 브라우저나 AI 모델처럼 정말 대규모 과제는 안 하지만, 예를 들어 로컬 기반 추론 엔진이나 HTML 렌더러, 직접 만든 그래프 데이터베이스 등은 직접 구현함. 의뢰인의 기대는 새로운 것(혁신)이 아니라 위험 감소임. 내가 직접 개발하면 일정 준수가 훨씬 수월함. 구글이나 다른 대기업의 문서를 들여다보는 시간보다 직접
            만드는 게 더 효율적임. 최근 3개월 동안 12시간씩 야근하며 이전 팀이 방치한 프로젝트 구하느라 밤늦게 이런 생각이 많아졌음
"
"https://news.hada.io/topic?id=22057","향수 리뷰","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 향수 리뷰

     * 향수를 예술로 바라보는 시각에 대한 소개와 독특한 개인블로그로 유명한 저자 Gwern Branwen의 개인적 체험을 다룸
     * 아방가르드하고 독특한 향수 세계가 존재하며, 샘플러로 저렴하게 경험할 수 있음
     * 초현실적이고 추상적인 향수(예: Room 237, Asphalt Rainbow 등)들은 감각적이고 예술적인 자극을 줌
     * 저자는 다양한 샘플을 시도한 후 Acqua di Sale와 Kyoto Incense 두 가지를 대표 향수로 선택함
     * 향수 경험을 통해 예술적 감각과 개인적 추억, 공간의 기억을 자극하는 향의 힘을 깨달음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

향수, 예술로의 탐구

     * 저자는 2021년 철학자 C. Thi Nguyen의 트위터 글을 통해 '아방가르드 향수' 세계에 흥미를 느끼게 됨
     * 원래는 향수에 관심이 없었으나, 일상의 무료함을 해소하기 위한 새로운 감각적 경험을 찾다가 이 글을 접하게 됨
     * 기존의 뻔한 향수와 달리 ‘가을 오후의 낙엽 태우는 냄새’, ‘목장’, ‘겨울의 토스카나 마을’, ‘다가오는 눈’, 심지어 영화 ‘샤이닝’의 공포방(Room 237) 등 매우 특이한 향들이 실제로 존재함을 알게 됨
     * Nguyen은 향수 샘플러의 존재(한 병에 10~20회 분량, $6 내외)로 경제적 부담 없이 독특한 향을 다양하게 경험할 수 있음을 강조함

향수는 예술인가

     * Nguyen은 학생들에게 향수가 예술인지 묻고, 실제로 다양한 추상적 향수를 체험시키면 대부분 예술로 인식이 바뀜을 경험함
     * 저자 역시 향수의 미학에 대한 논문([Shiner & Kriskovets 2007], [Kraft 2019], [Burr 2005])을 읽으며 '향수도 예술이 될 수 있다'는 관점에 공감하게 됨

샘플러 주문과 시도

     * Luckyscent 등에서 총 39개의 향수 샘플을 주문하며 직접 다양한 향을 체험하기 시작
     * 대표적인 샘플:
          + Room 237: 영화 샤이닝의 방을 모티브로 한 불안하고 인공적인 냄새
          + Asphalt Rainbow: 아스팔트와 가솔린, 스트리트 푸드의 느낌
          + Lampblack: 잉크와 오래된 책 향, 작가에게 어울릴 법한 향
          + Acqua di Sale: 바다와 소금, 조개껍질의 리얼한 해변 향
          + Megamare: 바닷가 습지대의 썩은 물 냄새 등 호불호가 강한 향
          + Garden Gnome: 시간이 지남에 따라 끊임없이 변하는 향, 정원에서의 하루를 연상시키는 스토리텔링적 향
     * 샘플 중 다수는 그저 향수답거나 화학적이었지만, 일부는 뚜렷한 개성과 감각적 자극을 제공
     * 특히 Room 237, Asphalt Rainbow, Garden Gnome, Lampblack, Acqua di Sale 등은 '예술'로 받아들일 만큼 인상적이었음

향수 경험의 의미

     * 저자는 여러 향수 리뷰를 읽으며 향수에 대한 평가는 극단적으로 엇갈리고 주관적임을 실감함
     * 향수는 기억과 감각을 불러일으키는 예술적 경험이 될 수 있음을 인정하게 됨
     * 예를 들어 Acqua di Sale는 바닷가에서의 어린 시절 추억을 환기시키고, Megamare는 습지의 냄새로 강렬한 인상을 남김

최종 선택과 활용

     * 취미로까지 이어가진 않았지만, 저자는 여러 샘플 중 Acqua di Sale(프로페셔널 용도)와 Kyoto Incense(개인적 용도) 두 가지를 대표 향수로 선택
     * 샘플러로 남은 향수는 쥐 퇴치 목적으로도 사용해 봄
     * 향수 한 병은 오랜 기간 사용할 수 있고, 한두 개의 대표 향수만으로도 충분히 만족감을 얻을 수 있음
     * 여행이나 이동 시에는 크기가 작은 ‘여행용 스프레이’를 사용하거나, 대용량 병에서 소분하여 휴대함

마치며

     * 향수는 단순히 체취를 감추는 용도가 아니라, 감각과 추억, 공간을 소환하는 강력한 예술적 매체임을 체험
     * 세상에는 자동차 배기가스, 우주선, 성수, 포뮬러원 레이싱, 심지어 이집트 클레오파트라 시대의 재현향 등 무한히 독특한 향이 존재
     * 새로운 향에 도전하는 경험 자체가 일상에 색다른 영감을 줄 수 있음

        Hacker News 의견

     * 인터넷 댓글들이 대체로 별로일 때가 많은데, 향수 덕후들은 정말 독특한 사람들임을 느끼게 됨
       basenotes 사이트의 샤넬 No.5 댓글들이 모두 굉장히 흥미로움
       나는 향수를 싫어함에도 불구하고, Christopher Brosius라는 전위적 향수 제작자를 알게 되어 20년을 기다려 그의 샘플을 구매하게 됨
       실제로 그의 향수들은 접근성이 뛰어나면서 대단히 신기함
       예를 들어 ""In the library""는 오래된 책 냄새, ""Wild hunt""에는 썩은 잎이 재료로 들어가 있고, ""Walking on air""는 갓 깎은 잔디향임
       향수를 싫어함에도 이 사람의 향엔 푹 빠져 있음
       Basenotes 샤넬 No.5 리뷰
       CB I Hate Perfume
          + 향이 좋은 향수가 몇 개 있는데, 나는 알레르기와 편두통 때문에 보통 향수를 멀리하게 됨
            많은 사람들이 향수를 과하게 뿌려서 그것도 더 힘들게 만듦
          + 나는 향수를 좋아하는데, 향수 덕후들의 코멘트들은 인터넷에서 가장 화려한 의견임
            ""고양이 오줌 같은 냄새""라는 평 옆에서, 누군가는 ""이건 투명 머스크를 썼네, 옛날 머스크와는 달리 안아주는 느낌이 없는 것""이라는 식의 전문적인 분석을 하기도 함
          + No. 5 리뷰를 보며 예전에 인상 깊게 본 Coco Chanel의 (간소화된) 역사에 관한 훌륭한 영상이 생각남
            디자인의 단순미와 영상의 템포 덕분에 11년 전에 만들어졌음에도 지금 봐도 시대를 초월한 감각이 느껴짐
            유튜브 영상: Coco Chanel의 역사
          + cbihateperfume.com의 첫 향수 이름이 ""At the Beach 1966""임
            이거는 Seinfeld에서 Kramer가 아이디어를 내고 Calvin Klein이 가로채가는 에피소드의 플롯임
          + 온라인상에서는 엄청난 의견 충돌이 빈번하지만, 향수 리뷰에서는 그럴 만한 합리적인 이유가 있다고 생각함
            우리는 눈에 3종류(때로 4종류)의 색 센서가 있어서, 일부를 못 가지면 색맹이 되듯이,
            코에는 수백 가지의 후각 수용체가 있고 각각 다른 조합을 가지고 태어나기 때문에, 사실 우리 모두는 어떤 면에선 ""냄새맹""임
            냄새 관련 의학 정보(NCBI)
            결국 우리 각자가 향수를 다르게 받아들이게 됨
            표준 사물이나 일반적인 식물처럼 모두가 공유하는 경험에 대해서만 의견이 일치함
     * 내가 경험한 가장 멋진 향수 중 하나는 Oriza Legrand의 Relique d’Amour임
       제품 설명페이지
       그 향수의 묘사가 시적임:
       버려진 수도원 예배당, 이끼 낀 돌벽, 촛농 냄새, 제단, 미완성 회화의 아마인유, 몰약과 유향의 흔적, 흰 백합의 알싸함, 금빛 꽃가루와 푸른 잎사귀, 유리창을 뚫고 들어오는 한 줄기 빛 등이 뒤섞인 신성한 소환
       파리 현지에서 실제로 시향해봤는데 깊은 인상을 받음
       미국에서는 Axe Body Spray 문화 영향 등으로 향수가 저평가받는 경향이 있었지만 이제는 향수를 새롭게 바라보고 있음
       우연히 향수 가게 창업자 부부와 파리에서 식사하게 되어 향수 산업에 대해 배우는 재미있는 시간이었음
          + 제품 설명이 마치 J. Peterman 카탈로그 스타일을 떠올리게 함
            예시로, ""마드리드의 Café Gijon에서 예전 예술가들이 모여 창작했던 공간, 설탕을 커피에 저으며 거울에 비친 방의 모습을 상상하는"" 식의 소설적 문구가 인상깊음
          + 흥미롭게 느껴짐
            합리적인 가격[1]에 샘플러 팩도 구입할 수 있다는 점이 마음에 듦
            Oriza Legrand 샘플러 6종 세트
            [1] 그들의 가격 기준에서 라는 의미, 케이스 바이 케이스임
          + 여러 샘플을 시향하다 보니, Axe 향은 유명 향수들의 조악한 모방판임을 깨닫게 됨
          + 묘사가 멋지긴 한데, 실제로는 향수로 쓰고 싶은 향은 아님
            향수는 본래 자신의 채취를 대체하거나 보완해주는 게 좋아야 함
            와인과 음식처럼 조화로워야 함
            Axe Body Spray는 자신의 냄새를 그냥 덮어버리는 마케팅임
            미국에서는 오래전부터 ""너의 자연 냄새를 가리고 다른 향으로 대체하라""는 광고 문화가 있었던 것
            결국 성인 여성뿐 아니라 남자아이들에게도 팔 수 있다는 점을 누군가 알아차린 것임
     * 향수는 매우 재미있는 취미임
       SF나 LA에 있다면 Scent Bar나 Ministry of Scent 같은 부티크 향수샵을 꼭 가보길 권장함
       또한 시향 샘플(디캔트)만 따로 소분‧구매해볼 수 있는 셀러들도 많음
       1~2mL 정도만 있으면 충분히 향을 알아볼 수 있음
       LuckyScent나 Surrender to Chance, 레딧 교환, 평점이 높은 이베이 셀러 경험이 좋았음
       향수 세계는 정말 다양하고 넓어서 트렌드가 있긴 하지만 유명 브랜드도 다 알기 어려움
       향에 대한 취향도 사람마다 다름
       Aventus나 Sauvage 같은 흔한 제품은 거르고, Discovery set 같은 샘플 세트를 강추함
       ""많이 쓰기 좋은 느낌""과 ""전위적인 스토리텔링""이 스펙트럼 형태로 존재함
       Afrika-Olifan 같은 향수는 창의성과 완성도면에선 감탄스럽지만 실제로 밖에 뿌리고 나가면 예의에 어긋날 만큼 독특함
       예를 들어 Black March는 비온 뒤의 대지와 풀 냄새로 시작해, 나중엔 꽃향으로 변하기도 함
          + 미국인이 아닌 사람들을 위해 언급하자면, SF와 LA는 미국의 도시(카운티)임
          + “밖에 뿌리고 가면 무례할 수 있다”고 언급한 이유가 뭐냐는 질문
     * 내 파트너는 향수 업계의 경제 논리가 곧 시그니처 향을 언제든 없애버릴 수 있다는 점에 불만을 가지고 있음
       그녀는 흔치 않은 향수에 반해 10년 동안 써왔는데, 이제 단종되어버림
       이런 일이 두 번 있었고, 그것도 듣도 보도 못한 브랜드가 아니라 Cacharel과 Beckham같이 꽤 알려진 브랜드임
       아마도 매출이 부진한 제품들을 냉정하게 정리해버려서 그런 듯함
       만약 용기가 표준화되어 있다면, 마치 로봇이 생산라인에서 혼합하듯 소량 생산도 가능했을 거라고 생각함
       일종의 코로 느끼는 페인트 블렌딩임
       “War paint” (2003)라는 Liny Woodhead의 Helena Rubenstein과 Elizabeth Arden 관련 책도 흥미롭게 읽었음
          + 정말 공감함
            나는 18살(1999년) 이후로 armani lui를 20년 넘게 시그니처 향으로 써왔는데, 어느 순간 성분이 바뀌어버림
            여전히 제일 좋아하는 향이지만 더 이상 예전 내 냄새가 아니라 영영 되돌릴 수 없다는 점이 슬픔
          + 사람과 추억, 그리고 향수 모두가 영속적이지 않다는 점이 씁쓸하게 와 닿음
          + 경제 논리만이 아니라 IFRA 향료 협회의 점점 더 엄격해지는 가이드라인도 영향이 큼
            알레르기, 건강 문제 등의 이유로 성분 규제나 금지 성향이 심해지고 있음
          + 이런 현상은 향수뿐 아니라 많은 제품에서 생기고 있는 현실임
          + 향수 레시피는 극비에 부쳐지는 영업비밀임
            크로마토그래피로 분석해도, 전문가 조향사 없이는 그대로 재현할 수 없음
     * 어떤 때는 이 사이트가 Adderall(집중력 향상제)이 그대로 폰트에 렌더링되어 브라우저에 뿌려지는 느낌임
          + Adderall이 아니라 Modafinil 느낌임
          + 진심으로, 여기 글을 읽다 보면 내 이야기를 보는 것 같아 공감함
            오래전부터 전위적인 향수 구매를 망설이고 있었음 (예를 들면 1970년대 아이다호 묘지 냄새 같은 걸 담은 향수도 궁금함)
            혹시 다른 사람들도 ADHD가 있는 게 아닌가라고 의심하게 됨
          + HN을 싫어하지만 결코 놓을 수 없어서 계속 보고 있음
            나만 그런 게 아닐 것 같음
     * Fragrantica는 내 경험 기준으로 110% 신뢰할 수 있는 몇 안 되는 사이트임
       내 경험을 확증해줄 뿐 아니라 정말 많은 것을 알게 해 줌
       예를 들어 코코넛 향수가 코코넛 냄새가 별로 안 나는 이유가 바닐라에 가려졌기 때문임을 알게 됨
       오드(oud) 향수도 오드 향이 맞는데 묘하게 장미 같은데 장미도 아니어서 검색해보니, 다른 종류의 로즈 노트였고 이게 오드를 압도해서 여성스러운 느낌이 강하다는 불만이 있었던 것임을 파악함
       지속 시간, 잔향감도 정말 잘 맞춤
       향수가 워낙 비싸서 이렇게 정확한 정보를 주는 사이트가 있음에 감사함
       Fragrantica의 댓글 창 기능은 그냥 읽는 게 아니라, AI 요약 기능을 위해 데이터를 쌓는 역할임
       이용자들이 가장 정확한 댓글에 투표해서 최종적으로 AI+추천 알고리즘 콤보로 도출한 결과물이 생각보다 합리적인 정확도를 보여줌
       향은 말로 묘사하기 정말 어렵기 때문임
     * 이 향수 토론에서 좋은 기회라 생각되어 프탈레이트(Phthalate)에 대해 공익적으로 안내하고 싶음
       향수에는 프탈레이트가 자주 들어가며, 내분비계 장애 유발자임
       특히 임산부, 영유아에게 악영향을 줄 수 있음
       어른에게도 부작용이 있을 수 있음
       프탈레이트는 비누, 샴푸 등 ""향료""로 표기된 여러 제품에도 숨어 있음
       프탈레이트가 없는 비누, 샴푸도 있고, 향수 시장에서도 프탈레이트 프리 제품이 늘고 있음
       프탈레이트 외에도 파라벤(Paraben) 등 추가로 주의해야 할 향료 성분도 있음
          + 100% 천연 향수를 표방하는 브랜드도 있음
            프탈레이트 관련 위험성을 단순하게 ""내분비계 장애 유발자""라고 단정할 수 없음
            어떤 종류가, 어느 정도 사용되었는지에 대한 디테일이 필요함
            IFRA는 diethylphthalate 성분이 향료에 안전하게 사용 가능하다고 명시하고 있음
            IFRA의 DEP 관련 공식입장문
     * 향수는 과소평가되고 오해도 많은 영역임
       본질적으로는 음악, 그림과 같은 하나의 예술 형식, 인간의 표현 방법임
       많은 사람들이 너무 과하게 뿌리는 이들 몇 명만 보고 전체를 부정적으로 평가하지만, 그건 마치 이웃이 너무 시끄럽다며 음악을 금지시키는 것과 같음
       Lucky Scent(기사에 언급) LA 매장에 가면 마음껏 시향 가능
       향수 부티크는 드물지만, 대도시엔 그래도 있고 충분히 접근성 높고 친절함
       Sephora, Macy's에선 절대 경험 못할 다양한 향의 세계가 있음
       좋은 기사였고, 일반적으로는 냄새를 묘사하기 위한 어휘가 부족해 리뷰가 과장되거나 추상적으로 흐르는 경향이 있다는 지적에 동의함
       하지만 약간의 책이나 전문적인 향수 익힘 도구(퍼퓸 오르간)를 활용한다면 이 분야에도 어휘 체계가 있음
          + 나는 향수 금지론자 그룹에 속함
            거의 모든 향수에 알레르기 반응이 오며, 마치 부비동 내부에서 누군가가 때리는 듯한 고통을 느낌
            양이나 강도 문제와 별개로, 미량에도 매우 심한 반응이 옴
            어떤 성분이 원인인지 알고 그 성분만 금지 요구를 하고 싶음
          + 이어폰처럼 개개인만 느끼게 해주는 개인향기 기기를 개발하면 좋겠다는 생각임
            냄새로 타인을 불쾌하게 만드는 문제를 해결할 수 있을 것 같음
          + ""과하게 향수 뿌리는 소수 때문에 전체를 금지하는 것은 과하다""는 비유에 대해,
            실제로 우리는 공공장소에서 소리 음악에 매우 민감하고, 그래서 라이선스 제도도 있는데 이건 타당하다고 생각함
          + 향은 특별한 감각임
            다른 감각인 맛은 다섯 가지 기본 맛의 조합, 시각은 세 가지 기본 색의 조합이지만,
            후각은 분해할 수 없는 독특한 경험임
     * ""$5 시향 샘플은 실제로 10~20회 쓸 수 있다""는 주장에 대해,
       해당 사이트에 있는 샘플(0.7ml)은 실제로 4번 사용하면 금방 끝남
       캡만 열고 공기만 맡는 식이면 10~20번도 가능하겠지만, 실제 뿌리면 금방 소진됨
       나는 보통 1.5~2ml 샘플, 4~5ml 디캔트를 자주 사는 입장임
     * @gwern, 인플레이션 계산 과정에 실수가 있는 것 같음
       “2011”이라고 했지만 이 부분은 “2021”이어야 계산이 맞음
       그 이후 수치는 전부 자연스럽게 맞을 것임
"
"https://news.hada.io/topic?id=22049","Anthropic, 사전고지 없이 Claude Code 사용량 제한을 강화 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Anthropic, 사전고지 없이 Claude Code 사용량 제한을 강화

     * 이번 주 초부터 Anthropic이 Claude Code 사용자에 대해 예고 없이 사용 한도 제한을 강화함
     * 특히 월 $200의 Max 플랜을 사용하는 헤비 유저 중심으로 불만이 폭증하며, 한도 도달 시 “Claude usage limit reached” 메시지만 안내되고 구체적인 설명은 없음
     * 사전 안내나 변경 공지 없이 한도가 줄어들면서, 일부 사용자는 구독 플랜이 다운그레이드된 것 또는 사용량 트래킹 오류로 오해함
     * Anthropic는 상세 설명 없이 “일부 사용자가 느린 응답을 경험 중”이라고만 공식 언급, 정확한 해결 일정이나 원인 안내는 없음
     * API 과부하, 네트워크 오류 등도 동반되어 불만이 커졌으며, 사용자는 신뢰 저하와 함께 명확한 한도 및 소통 개선을 요구하는 상황임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

갑작스러운 사용 한도 강화와 혼란

     * 지난 월요일부터 Claude Code 사용 한도가 갑자기 강화되며, 많은 사용자가 의도치 않게 제한에 걸리는 현상 발생
     * “Claude usage limit reached” 메시지만 제공되고, 한주어진 시간(몇 시간) 후에 제한 해제 예정만 안내될 뿐 구체적인 한도 변경 안내 없음
     * 특히 $200 Max 플랜 등 고가 구독자를 중심으로, 사용량 트래킹 오류 및 플랜 다운그레이드 오해 등 불신 심화
     * GitHub 이슈 페이지 등에서 “30분 동안 몇 번 요청했는데 900 메시지 한도를 넘어버렸다” 는등 사용량 산정의 불투명성에 대한 불만 폭증
     * 한 사용자는 “이 한도로는 프로젝트 진행이 불가능하다”며, “Gemini나 Kimi도 대체제가 되지 않는다”고 토로

공식 입장 및 네트워크 이슈

     * Anthropic는 “일부 사용자가 느린 응답을 경험하고 있다”며, 추가 설명은 회피
     * 같은 기간 API 오버로드 오류, 네트워크 장애가 동시 다발적으로 발생했으나, 공식 상태 페이지는 100% 가동으로 표기되는 등 정보 불일치
     * 한도·가용량 자체가 수요에 따라 유동적으로 정해지는 비공식적·가변적 구조가 혼란을 키움

혼란을 초래한 복잡한 요금제 구조와 사용량 정책

     * Anthropic의 요금제는 명확한 사용량 보장 없이 구간별 제한과 안내만 제공하며, 무료/Pro/Max 모두 명확한 상한이 아닌 ""수요에 따라 변동"" 안내로 혼란 유발
          + Max 요금제: Pro 대비 20배, Pro는 무료 대비 5배 더 많은 사용 한도를 공식 안내 중이나, 절대적 사용 한도 값은 비공개
          + 무료 한도 역시 “수요에 따라 달라진다”는 명시로, 절대적 사용량 보장이 없음
     * 사용자는 실제 이번 제한 강화 전에 서비스 사용량(예: 하루에 천 달러 이상 API호출) 이 가능해, Max 요금제를 불안정하고 장기 지속은 힘든 모델로 인식하고 있었음
     * 그래서 제한 강화 자체에는 놀라지 않았으나, 투명성 부족이 가장 큰 문제라고 지적함
          + ""투명하게 소통해달라. 소통 부재는 신뢰를 상실하게 만든다""라는 사용자 의견이 대표적임

커뮤니케이션·신뢰 문제의 본질

     * 일부 사용자는 Max 플랜 한도의 장기적 지속 가능성 자체엔 이해를 표하면서도, “투명하게만 소통해달라” 는 요구를 강조
     * 예고 없는 변경과 불명확한 안내는 사용자 신뢰 저하로 이어짐
     * 명확한 한도 안내와 사전 소통, 그리고 빠른 이슈 대응이 서비스 유지와 고객 신뢰 확보에 매우 중요함

   Agentic coding의 최전선에 있는 제품이라서 트래픽이 상당히 몰리고 있나보네요..

        Hacker News 의견

     * 한 사용자가 익명을 요청하며, 사용 제한이 생긴 이후로 프로젝트를 더 이상 진행할 수 없었다고 언급함. 바이브 한도에 도달했다는 느낌으로, 이제 스스로 생각을 해야 할 타이밍임
          + 맞는 말이지만, 이 회사들이 판매하는 근본 전제는 '생각'의 상당 부분을 아웃소싱할 수 있다는 것이고, AI 관련 투자도 이 전제에 크게 의존하고 있음. 이렇게 엄청나게 돈이 투입됐는데 시장이 그대로인 게 좀 이상한 느낌임
          + 조금 생각하는 정도로 끝나지 않을 것 같음. 프로젝트를 계속하려면 훨씬 더 깊은 고민, 어쩌면 극한의 사고력이 필요해질 것 같음
          + 써드파티 서비스에 장기 가용성이 불분명한 강력한 의존성을 두면 문제가 될 수 있다는 걸 누가 예측했겠느냐는 반어적인 의문이 듬. 유료 컴파일러나 원격 메인프레임 시절로 되돌아가는 것 같고, 사람들은 항상 이런 실수를 반복함
          + 99% 확신하는데, 이 사용자들은 코딩 자체를 못하는 바이브 코더들이 아님. 그런 사람들은 lovable 같은 툴을 쓰지 터미널 툴은 건드리지 않을 것임
          + 같은 인용구에 대해 코멘트하려고 들어왔음. 이런 상황에 벌써 도달했다니 놀랍지만, 사실 놀랄 일은 아니라는 생각임
     * Claude 4.0은 원시적인 지능 측면에서 보면 다른 대표 모델보다 똑똑하다고 볼 수 없음. 다만 코딩 중 올바른 툴 사용에 맞춰서 정말 잘 다듬어 놓았음. 다른 모델들도 금방 따라잡으면 이런 식으로 제한을 엄격하게 걸기는 힘들 것임. Google 입장에서는 직접 실리콘을 깔고 최적화까지 하니까 절대적인 캐시플로우로 유리한 위치임. 재미있게도 여기 댓글에서 컴퓨트 확장 법칙을 이해하는 사람이 거의 보이지 않음. 사람들 머릿속에는 Uber 모델처럼 시스템이 어느 순간 가격을 올려야 한다는 생각이 박혀있지만 AI는 인간 노동이 아님. 시간이 지나면 컴퓨팅 비용은 떨어지게 되어 있음. 단기적으로 손해를 보면서 베팅하는 게 오히려 가장 확률 높은 전략이고, 이게 바보 같은 행동이 아니라고 생각함. 대부분의 사람들이 이 거품이 붕괴될 날을 손꼽아 기다리며
       ""내가 예측했다""고 똑똑한 척하고 싶어하지만, 결국 장기적으로는 이 방향이 옳은 것임
          + 모어의 법칙이 유효하다고 해도 컴퓨터 자원의 효율이 변하지 않고 계산당 전기 효율도 그대로라는 가정 하에 18개월마다 컴퓨트 비용이 절반으로 내려갈 뿐임. 실제 댓글에서 월 $200 플랜으로 $4000까지 치솟았다는 내용이 있었는데, 비용효율을 따지면 8년 걸림. 8년 동안 적자 감수할 각오가 되어 있는지는 의문임
          + 사실 따지고 보면, 현재 모델들 자체가 말하는 만큼 그렇게 ‘똑똑’하진 않음. AGI와는 거리가 멂. 맥락 관리, 태스크 분리, 재시도, 무한 루프 방지, 알맞은 툴 노출 등등이 더 중요함
          + 나는 월 $100 플랜은 안 쓰고 API 단가로 결제해서 매우 신중하게 Claude Code를 씀. 첫째 날 루프에 빠져서 같은 두 가지 잘못된 솔루션을 반복하다가 $30를 다 태워버리고 나서야 멈췄음. 이후로 하루 $3~$5 정도 쓰면서 효율적으로 많은 걸 달성하고 있음. Anthropic이 개발자들이 더 현명하게 Claude Code를 쓰도록 동기부여 방안을 찾아야 함. 제어를 못하면 진짜로 훅 나가서 폭주함
          + 모델의 문제는 쓸모없는 콘텐츠가 엄청 쏟아진다는 점임. 산업이 작은 규모일 때야 이 정도 오염은 문제 없지만, 전세계급으로 스케일업되면 뒷처리가 만만치 않음
          + Claude도 괜찮지만, ‘스마트함’이 필요하다면 이야기가 다름. 개인적으로는 Mistral의 medium 3나 devstral medium 모델도 과소평가되는 느낌임. 둘 다 ‘스마트’하진 않지만, 단순작업에 유효한 코드가 필요할 때 가격대비 정말 괜찮음
     * Claude Code를 $20/월 기본 플랜으로 사이드 프로젝트에 써봄. 전체 업무시간을 다 쓴 건 아닌데도 호출량이 충분했음. $20 한도에 빨리 닿을 줄 알았는데 끝내 못 닿았음. 솔직히 AI가 못하는 부분은 내가 직접 고치거나 수작업 코딩을 꽤 많이 해야 했지만, 토큰 소모량은 정말 후하게 느껴졌음. API 가격 비교로 따지면 매일 $10~$20어치 토큰을 쓰는 기분이었음. 한동안은 사용자 확보하려고 제한을 엄청 후하게 잡다가 이제 용량 감당이 안 돼서 조이는 것 같음. 기사에 나온 대로 $200/월 플랜 한도를 초과할 정도로 코딩하려면 얼마나 많이 써야 하는지 상상이 안 됨
          + Claude Code에서는 토큰 효율적인 사용 방법이 무궁무진함. 조만간 Claude Code 전용 모델이 나오지 않을까 조심스레 예측함. 내 실험 경험상 많은 토큰이 낭비된 이유는 예를 들어, 파이썬 스크립트 전체를 다시 읽으면서 주석 상태를 점검하거나, R 스크립트를 다시 읽어 대괄호 닫힘만 확인하는 식의 무의미한 반복에서 비롯된다고 봄. 이 정도 비효율적인 패턴만 잡아도 꽤 많은 리소스 세이브가 기대됨
          + 요즘 이런 한도는 정말 빨리, 심지어 일주일이나 며칠 단위로 바뀌는 경우가 많고 시간대, 위치, 계정 개설 날짜 등에 따라서도 다름
          + 나는 CC에서 한 번 요청만으로 한 시간 만에 한도에 닿았음. opus도 아니었음. 좀 쓰다보면 언젠가는 거의 선제 경고 메시지 뜨는데, 뭔가 한도를 제대로 명시하지 않고 상위 플랜 업셀만 하려는 느낌이 들어서 아쉬움
          + 생각(Thiking) 프로세스는 보통의 Chat보다 훨씬 비효율적임. 많이 사용하면 금방 수백 달러 쓸 수 있음
          + 만약 한도를 못 채운다면 프롬프트를 더 창의적으로 써보라는 뜻일 수도 있음. 난 한 번 프롬프트 던지면 거의 한 시간 동안 독립적으로 작업하게 만들어 한도에 도달시키기도 함. 하위 에이전트들을 만들어 병렬로 운영하거나, 완전 자동화로 오랜 시간 작업 시킬 수도 있음. '이거 하나 해줘'보다 더 넓은 관점을 가져보면 좋음
     * 애플이 M4 맥북을 샀는데 아무 경고 없이 성능을 M1으로 낮춰버린다면 테크 미디어, 소비자 단체 모두 난리가 났을 것임. 그런데 AI 회사들은 100불 내고 사용권을 샀는데 아무말 없이 성능을 떨어뜨려도 조용함. 어떻게 이런 일이 가능한지 궁금함
          + 회사 입장에서는 용량이 한정돼 있으니 모든 사용자에게 합리적인 제품을 제공하고 싶어함. 적정 한도를 설정하고 준수하는 것이 굉장히 어려웠던 듯함. 실제 용량이나 참여자 수 예측에 애를 먹고 있는 것 같음. 아직 경쟁사도 별로 없고 가격도 특별한 기준이 없음. 앞으로 나아질 거라는 기대를 해봄
     * 지금은 아마도 적자를 보면서 운영하는 중이라 화낼 타이밍은 아니라는 생각임. Cursor도 마찬가지로 가격 정책이 불투명함. Max 플랜 결제 중인데 API 리포트로 보면 벌써 거의 $1,000 어치 사용함. 남은 쿼터가 얼마 남았는지도 모르겠고, API에서 주는 가격 정보도 납득이 안 감
          + 내 동료는 이번 달에 주당 $1,000을 태웠다고 주장함. 회사 입장에서는 한 달에 $200만 내면 되는 구독료라니까 정말 놀라움
          + 우리는 어제 막 클로즈드 알파를 종료하고, 최적의 가격 정책 고민 중임. 피드백 줄 수 있으면 https://www.charlielabs.ai/pricing 참고해주면 정말 감사함
          + 어떤 툴링으로 가격을 확인하고 있는지 정보가 궁금함. cursor-stats인지 묻고 싶음
          + 적자 운영 중이라고 관대한 마음을 가져야 한다는 말에 동의 못하겠음. 요즘 명확한 수익 모델 없이 런칭부터 시도하는 제품들에 정말 질림
     * 사람들이 한도를 일상적으로 도달하는 실제 작업 현장을 영상으로 보고 싶음. 본인은 sonnet을 주로 코딩에 쓰는데 $20/월 플랜 기준 기본 한도도 못 채워봄. 명세 작성, 문서화, 잘 알려진 예제를 바탕으로 반복작업, 특정 서비스 반복 제작 등 활용함. 전체 코드베이스를 리라이트하는 식이 아니라면, 작은 수정은 굳이 영어로 문제를 설명하고 AI에 맡겨서 큰 사이클을 도는 것보다 내가 직접 손보는 게 빠를 때가 많지 않나 생각임
          + 나는 Opus와 긴 워크플로에서 한도에 도달하게 됨. 구체적으로는 두 가지 대형 워크플로(플랜과 구현)로 나누어, 아이디어 리서치와 플랜 문서 생성에만 $10~$30 정도 API 비용이 듦. 리뷰하면서 작은 실수나 과잉 내용은 직접 손보고, 다음 단계에서 구현도 진행함. 구현 단계가 오히려 저렴한 편임. 이 플랜 문서를 기반으로 자동으로 GitHub PR까지 생성하게 만듦. $100 Max 플랜의 레이트 리밋 도달하려면, 이런 사이클을 3~4번만 5시간 내에 반복하면 충분함. Opus에 복잡한 지침을 막 던지는 것도 가능해서 신뢰도가 아주 높음. Code를 단순 상호작용으로만 사용하면 거의 도달 못함. 주로 vibe coder들이 자주 뚫는 듯함
          + AI가 정답 방향으로 수렴하지 못할 때가 오면, 그때부터는 결국 사람이 직접 마무리하게 됨
          + Claude Code를 sonnet과 함께 사용하고 있는지 궁금함. 웹으로만 쓰면 한도가 엄청 여유로운 느낌임
          + 이번 주에는 Max 플랜인데도 단순한 작업조차 성공적으로 못하게 됨. Max 사용자만 과부하 걸리는 수준이 아님. 아무때나 임의로 레이트 리밋이 걸리는 느낌임. 첫 프롬프트부터 막히기도 함
          + 전체적으로는 사람이 직접 할 때가 빠르지만, 나는 인터럽트 드리븐으로 여러 태스크를 오가니까 프롬프트만 빨리 던져 놓고, 백그라운드에서 작업 돌면서 기다림. agent가 3배 더 오래 걸리더라도 내 시간 소모는 프롬프트 타이핑 몇 초로 끝나기 때문임
     * 며칠 전에 두 프로젝트에서 대규모 리팩터링을 하면서 다른 두 프로젝트 디자인 작업도 병행했음. Gemini API 사용량을 확인하니 하루에 이미 $200를 썼더라. 사용자들은 이보다 훨씬 더 강도 높게 돌릴 수 있음. $200/월 무제한 정책으론 회사에 수익이 남기 어렵다고 생각함. 앞으로는 비용을 고려해 작업을 지능적으로 배분하는 시스템이 만들어질 듯함. openrouter도 이런 부분을 목표로 움직이는 듯한데, 올바른 라우팅을 하려면 엄청난 맥락정보가 필요할 것임
     * 사용 제한 발생 후 ""진짜로 프로젝트가 진행이 안 된다""고 언급한 발언이 있었음. Gemini, Kimi 등도 써봤지만 Claude Code만큼 다양한 기능 세트를 갖춘 도구는 없었다고 함. PMF(제품 시장 적합성)라는 평가임
     * 이번 주에 $200/월 플랜을 시작했는데, 원래 매달 API 토큰으로 $300+ 쓰고 있었음. '이게 Anthropic 입장에서 어떻게 수지타산이 맞을까' 생각하기도 했음. 그런데 API 과부하 에러가 계속 나와서 결국 플랜 해약하고 다시 API 토큰으로 복귀함. 도대체 무슨 의도로 이런 정책을 쓰는지 모르겠지만, 나는 돈 내고라도 쓸 의향이 있음. $200/월이라는 문구만 괜히 내세우지 말고 제대로 접속 보장해주면 좋겠음
          + Opus로 이런 경험을 했는지 궁금함. 어떤 작업을 했는지도 알고싶음. 나는 2.5MB짜리 대형 소스파일 작업할 때만 토큰 왕창 쓴 적이 있음. 그 외엔 100EUR 플랜도 다 못 씀. 주로 Opus만 사용함
          + 애초에 API 단가는 실제보다 훨씬 높게 책정돼 있는 거 아닐까 하는 의구심이 있음. 난 단순히 $5 충전해서 시험삼아 놀아보니 30분 정도 연산이 돌아가고, 실제론 3시간 정도 체험에 해당함. 대략적으로 $10/시간으로 환산하면 연간 $9만임. GPU 구매와 운용비가 이렇게 연 9만 불이나 들지는 아직 확신이 없음. GPU 인프라 비용이 하드웨어 투자 수준에서 벗어날 수 있다는 생각은 아직 안 듦
     * 의도적으로 서비스 질이 낮아진 건지, 서버가 감당 못 할 만큼 수요가 너무 빨리 늘어서 임시 대응으로 한도를 줄인 건지 구분이 안 감. 만약 계속 수요가 추가되면 이 제한은 상시로 더 심해질 수도 있을 듯함. Anthropic이 하필 지금 COGS(매출원가) 최적화를 시도한다고 판단하진 않음. DevTools 시장 전체를 먹을 기회가 있는데, 현금도 많고 투자 의지도 강한 상황에서 제품 파워를 약화시키는 건 단기적인 시야라고 생각함
          + Cursor를 엄청 빠르게 떠난 유저가 많았음. IDE를 바꾸는 건 나에겐 큰일이라서 Cursor를 아예 시도도 안 했음. Claude Code는 훨씬 좋은 개념이고 반드시 IDE와 연결될 필요가 없음. 그래서 오히려 경쟁사로 전환이 더 쉬움. 이런 의미에서 본다면 Claude Code의 모델적 특성은 오히려 시장점유율 확보가 무의미해질 위험도 내포함.
"
"https://news.hada.io/topic?id=22015","머릿속에서 작은 증명을 하며 더 나은 프로그래머가 되는 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   머릿속에서 작은 증명을 하며 더 나은 프로그래머가 되는 방법

     * 더 좋은 프로그래머가 되기 위해선 코드 작성 시 작은 증명을 머릿속으로 그리는 습관이 중요함
     * 단조성, 불변성, 전제·결과 조건, 불변조건 등은 이런 미니 증명을 할 때 핵심적인 개념임
     * 코드 변경이 시스템 전체에 미치는 영향 범위(격리, 파이어월)를 고려해 설계하는 것이 복잡도와 위험 줄임에 큰 도움이 됨
     * 귀납법을 활용하면 재귀 함수나 구조의 올바름을 단계적으로 증명할 수 있음
     * 이러한 사고는 연습과 습관화로 배양되며, 실제 수학적 증명 및 알고리듬 문제 풀이 훈련이 큰 도움임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

소개 및 핵심 아이디어

     * 필자는 경력을 쌓아가며 코드의 정확도와 속도를 높이기 위해 자연스럽게 ‘작은 증명을 그려보는 습관’ 을 갖게 되었음
     * 코딩 중 예상 동작을 머릿속에서 검증하고 추론하는 과정은 연습이 필요하며, 능숙해지면 코드의 완성도가 눈에 띄게 향상됨
     * 구체적으로 어떻게 하는지가 중요하지는 않고, 다양한 방식으로 본인에게 맞게 연습해볼 수 있음

단조성(Monotonicity)

     * 단조성(monotonicity)은 함수나 코드가 한 방향으로만 진행하는 성질을 의미함
     * 예시로 체크포인팅을 들 수 있는데, 이는 작업 단계가 앞으로만 나아가고, 이미 끝난 작업을 뒤로 돌아가 다시 실행하지 않음
     * LSM 트리와 비교되는 B-트리 예시에서, LSM 트리는 대부분 공간이 누적되며, 컴팩션 과정에만 줄어드는 특성을 가짐
     * 단조성이 보장되면 복잡한 상태나 결과 일부를 자연스럽게 배제하거나 예측할 수 있음
     * 불변성(immutability)도 유사한 개념으로, 한 번 설정된 값이 절대 바뀌지 않으므로 상태 변화의 가능성을 배제할 수 있음

전제 조건과 결과 조건(Pre- and post-conditions)

     * 전제 조건(pre-condition)과 결과 조건(post-condition)은 함수가 실행되기 전후에 반드시 참이어야 하는 주장임
     * 함수 작성 시 이 조건을 명시적으로 추적하면 논리적 사고 및 테스트에 도움이 됨
     * 결과 조건을 명확히 규정하면 유닛 테스트 케이스를 더 쉽게 도출할 수 있음
     * 코드에 이러한 조건을 검증하는 assertion을 추가하여 예기치 않은 상황에서 조기 중단되도록 하는 것이 예측가능성과 안전성을 높임
     * 함수에 아예 명확한 전제/결과 조건을 부여하기 어려운 경우도 있는데, 이를 발견하는 것 자체도 시사점이 있음

불변조건(Invariants)

     * 불변조건은 코드가 어떤 상황에서도, 실행 전·후·중에 항상 참이어야 하는 속성임
     * 예를 들어 복식부기 회계의 회계 등식이 대표적인 불변조건 예시임 (총 대변 = 총 차변)
     * 코드 전체를 작은 단계로 분리하고, 각 단계가 불변조건을 보존하는지 증명하면 전체의 무결성을 확보할 수 있음
     * 불변조건을 유지하기 위해 리스너나 라이프사이클 메서드를 사용하는 방식(C++의 생성자/소멸자, React의 useEffect 등)이 있음
     * 변경점이 적거나 경로가 단순할 때 불변조건 검증이 훨씬 쉬움

격리(Isolation)

     * 좋은 소프트웨어의 핵심 중 하나는 기존 시스템을 불안정하게 만들지 않고 새 기능을 추가·수정하는 것임
     * 코드 변경의 ‘영향 반경’ (blast radius)을 파악하고, 구조적 ‘파이어월’ (방화벽)을 만들어 영향이 퍼지는 범위를 최소화해야 함
     * 실제 서비스 Nerve의 예시에서, 쿼리 플래너와 쿼리 실행기의 경계를 명확히 하고, 변경된 부분이 이 경계를 넘지 않도록 설계하는 방식을 소개함
     * 불필요한 변경 전파를 막으면 검증과 유지보수가 쉬워지고, 안정성이 높아짐
     * 이는 OCP(Open-Closed Principle) 의 철학(기존 동작을 바꾸지 않고 기능을 확장)과도 일맥상통함

귀납법(Induction)

     * 많은 프로그램이 재귀 함수 또는 재귀 구조를 포함하며, 이에 대한 논리를 정립할 때 귀납법이 강력하게 쓰임
     * 재귀 함수의 동작과 올바름을 단계적으로 증명하려면, 기저(base) 케이스와 귀납 단계(inductive step)를 각각 검증해야 함
     * 예시로 AST(구문 트리) 구조의 노드 단순화 과정을 들며, 각 단계별 귀납적 논증을 통해 불변조건 유지와 올바른 동작을 증명함
     * 귀납법적 사고가 체화되면 재귀 코드 작성과 검증이 훨씬 직관적이고 쉬워짐
     * 귀납적이 아닌 전역적(holistic) 검증 시도와 비교해보며 어느 방식이 더 자연스러운지 고찰해볼 만함

Proof-affinity(증명 친화성)이라는 품질 지표

     * 필자는 ‘머릿속에서 작은 증명을 그려볼 수 있는 코드’가 좋은 코드라는 주장을 전개함
     * 코드가 단조성, 불변성, 명확한 조건, 불변조건 분할, 화재벽 경계, 귀납법 활용 등의 구조를 가지면, 실제로 증명하기 쉬워지고, 따라서 코드 자체도 품질이 높아짐
     * 코드가 이해하기 어렵고 검증이 힘든 상태라면 리펙터링이나 구조 재고가 필요함을 시사함
     * 이때 ‘증명 가능성’ 대신 ‘proof-affinity’(증명 친화성)이라는 용어를 제안함
     * 증명 친화성은 소프트웨어 품질의 유일한 요소는 아니지만, 코드의 이해·확장·테스트·유지에 매우 중요한 촉진제임

실력을 높이는 방법

     * 이러한 논리적 사고 방식은 연습이 쌓여야 무의식 중에 자연스럽게 적용됨
     * (수학적) 증명을 자주 써보고 논리적 추론 능력을 기르는 것이 필수임
     * 알고리듬 문제 풀이(Stanford의 EdX 강의, Leetcode 등)도 좋은 훈련장이 되며, 단순 요령 문제가 아닌 꼼꼼한 구현과 증명적 사고가 필요한 문제를 집중하면 성과를 높일 수 있음
     * 여러 번 결과를 고쳐가며 맞추기보다는, 한 번에 정답에 가까이 접근하려는 연습이 중요함
     * 이런 습관화를 통해 논리적 시스템 설계와 코드 품질이 큰 폭으로 향상됨

        Hacker News 의견

     * 이 주제에 딱 맞는, 단순하지만 놀라운 예시가 있음. 바로 이진 탐색임. 왼쪽, 오른쪽 변형도 있지만, 루프 불변식에 대해 생각하지 않으면 제대로 구현하기 매우 어려움. 이 글에서 불변식 접근법과 그에 맞는 파이썬 코드 예시를 설명해둠. Programming Pearls의 저자 Jon Bentley가 IBM 프로그래머들에게 평범한 이진 탐색을 구현해 보라고 했더니 90%가 버그가 있었음. 주로 무한 루프에 빠지는 실수였음. 그 시절엔 정수 오버플로도 직접 막아야 했으니 어느 정도 이해는 되지만, 그래도 놀라운 비율임
          + 이걸 보고 면접 질문으로 이진 탐색을 쓰기 시작했음. 잘 알려진 지원자들 중 약 2/3가 20분 이내에 제대로 동작하는 구현을 못 했음. 특히 쉬운 케이스에서 무한 루프에 빠지는 경우가 많았음. 성공한 사람들은 빠르게 구현했음. 문제의 원인 중 하나는 잘못된 인터페이스로 학습된 경우가 많음. 위키피디아 설명도 ""L을 0으로, R을 n-1로 초기화""라고 설명하는데, 이건 R이 포함된 범위임. 실제로는 대부분의 문자열 알고리즘에서 상한을 포함하지 않는, 즉 R이 n인 형태가 더 나음. 이 가설을 직접 실험해보고 싶음. 서로 다른 함수 프로토타입과 초기값으로 많은 사람들에게 작성하게 해보고, inclusive와 exclusive upper bound, length 방식을 쓸 때 버그가 얼마나 나오는지 비교하고 싶음
          + 사실 이진 탐색은 인덱스 관리가 거의 가장 까다로운 대표 예시임. Hoare 파티션 알고리즘과 더불어 실수 없이 정확하게 코딩하기 가장 어려운 기본 알고리즘임
          + 테스트 삼아서 Claude Sonnet에게 버그 없는 이진 탐색 파이썬 코드를 작성하게 시켜봤음
def binary_search(arr, target):
    left = 0
    right = len(arr) - 1
    while left <= right:
        mid = left + (right - left) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1

       예제 배열로 다양한 테스트 케이스도 확인함
          + 이진 탐색 버그가 잘못된 모범 사례로 유명하단 걸 알고 버그 없는 첫 구현을 책에 실어보겠다고 도전함. 정말 신중하게 작성했지만 그래도 버그가 있어서 웃음이 남. 다행히 Manning의 사전 피드백 시스템 덕분에 인쇄 전에 수정할 수 있었음
          + 나는 왼쪽/오른쪽 이진 탐색 구현을 항상 ""이전까지 가장 좋은 값""을 기억하며 구현함. C++의 lower_bound, upper_bound 같은 방식임. while (l < r) 구조에서 중간 지점을 찾고, 현재 위치를 체크하여 알맞게 범위를 조정함. 예시로 upper_bound면 좌측 경계를 올리고, lower_bound면 우측 경계를 내리는 식임. 오랜만에 leetcode 풀다가 머리가 멍해서 포맷이 엉망일 수 있음
     * 오래전에 대학원 수업에서 이와 비슷한 개념을 알게 된 듯함. 학부 마지막 즈음부터 수학 시험을 아예 볼펜으로만 치렀음. 이유는 잘 몰랐는데 성적이 늘 높았고, 한 번에 풀어가며 아예 머릿속에서 풀이 과정을 완전히 그려놓고 썼기 때문임. 덕분에 실수가 많이 줄었음. 코딩할 때도 이런 식으로 머릿속에서 완성도 있게 설계해두고 시작함
     * 더 나은 프로그래머가 되려면 코드에 작은 증명을 작성하는 습관을 들여야 함 테스트와 타입 정의가 바로 그런 행위의 예임 특히 테스트를 먼저, 그 다음 타입, 마지막에 코드 작성 순서로 접근함 각각의 acceptance criteria별로 테스트를 만들고, 입력/출력이 명확하게 설명된 테스트를 작성함이 바람직함 API라면 OpenAPI나 GraphQL로 모든 속성과 타입 포함해 명세를 먼저 만들 수 있음. 런타임에서 이 명세 기반으로 데이터 검증이 가능하고, 이 명세서 자체가 앱이 명세대로 동작한다는 증명이 됨 요약하자면 OpenAPI/GraphQL, 테스트, 타입을 통해 실제로 시스템이 의도한 대로 동작한다는 증명을 확보함이 중요함 명세 자체를 먼저 잘 설계하면 코드 구현은 유연하게 바꿔도 명세로 올바름을 증명할 수 있음 코드 자체보다 명세가 더 중요함
          + 글에서 언급된 다섯 속성은 좋은 타입 시스템에서 표현 가능함. 이런 방법으로 명세의 상당 부분이 코드가 되고, 컴파일러가 올바름을 보장해줌. 프로그래밍의 미래는 이런 접근이 당연해지는 방향이어야 함 OpenAPI와 GraphQL의 타입 시스템은 너무 빈약해서, 이런 미래에 도달하려면 50년은 발전이 더 필요함
     * 대학에서 이론 컴퓨터 과학 기초를 배웠고, 이 글의 취지에 공감함. 실천이 쉽지는 않음 사전/사후 조건 외에도, 루프 불변식(loop invariant), 구조적 귀납법(structural induction) 같은 CS 증명 테크닉이 매우 강력함 loop invariant, structural induction 링크와 함께, UofT CSC236H 강의 노트(강의 노트)를 추천함
          + 이 CSC236 강의노트가 매우 훌륭하고, David Liu 교수도 정말 괜찮은 분임 교수 소개
          + UofT 언급됐네! 반갑다는 마음임
     * ""코드에 대해 머릿속에서 작은 증명을 직접 만들어보기""란 주장은 자명해야 할 정도로 중요한 원칙임. 코드의 각 부분이 무엇을 하는지에 대한 간단한 명제를 늘 의식해야 함
          + 이 취지는 그린필드 프로젝트(최근에 직접 모든 코드를 쓴 경우)에서는 쉽지만, 다양한 함수나 글로벌 상태를 여러 개발자가 건드리는 진짜 코드베이스에서는 훨씬 어렵게 느껴짐
               o 진짜 좋은 개발자는 시스템을 점차 이런 방향으로 발전시키는 능력이 있음. 현실 세계의 코드는 엉망이지만, 인바리언트의 구멍을 점진적으로 줄이고, 후속 개발자들도 인바리언트를 인지한 채 그 유지에 우호적인 코드 구조를 만들어주는 것이 중요함. 문서도 도움 되지만, 내 경험상 코드 구조 자체가 그보다 더 큰 역할임
               o 사실 글로벌 상태가 위험한 결정적 이유가 이 점임. 코드의 정확성을 증명하려면 프로그램 전체를 알아야 하기 때문임. 전역 변수를 불변값으로 바꾼다든지, 함수 인자로 넘기거나, 상태를 감싸는 래퍼 클래스로 관리하면 그 함수의 호출자들만 명확히 파악하면 됨. 함수 내부에 어설션 등으로 더 제약을 추가하면 증명 난이도가 확실히 낮아짐. 이미 많은 프로그래머가 이런 결정을 하고 있지만, 증명을 의식하지 않고 본능적으로 그렇게 하고 있을 뿐임
               o 전역 상태를 여러 개발자가 관리하는 코드는 ""암이 전이된"" 환자와 비유할 수 있음. 치료가 훨씬 힘들고, 운과 외부 조건에 따라 살릴 수 있는 경우도 있음
               o 기사에서 말한 것처럼, 이런 코드는 버그가 생길 확률이 훨씬 높고, 유지보수로 인한 추가 버그 가능성도 큼. 처음부터 증명 가능한 구조로 작성하는 것이 훨씬 올바른 길임을 보여줌
               o 이 글을 읽고 내가 코딩 방식에 대해 끊임없이 다시 고민하고, 더 나은 방향에 대해 재정립하려 노력하는 모습을 떠올리게 됨. John Carmack 같은 개발자도 시간이 지나면 자신의 예전 코드가 부족했다 느끼고, 더 ""잘"" 해나가는 감각이 있을지 궁금함
     * 코드가 증명 가능해야 한다는 생각은 Dekker가 1959년 mutual exclusion 문제를 해결하면서 처음 드러난 개념임. 이에 관한 재밌는 일화가 Dijkstra의 글 EWD1303(원문 링크)에 소개됨. Dijkstra의 후속 연구도 이 통찰의 연장선상에서 이어진다고 볼 수 있음
     * 올바른 증명을 작성하는 건 정말 어려움. 프로그램 검증도 마찬가지로 쉽지 않음. 내 생각엔, 수작업으로 증명하려 들면 효율이 없음. 해당 언어와 코드베이스의 관용적인(idiomatic) 코드를 작성하면 불변식이나 사전/사후 조건 신경 쓸 일이 별로 없음. R. Pike와 B. W. Kernighan의 ""The Practice of Programming""에서 강조하는 ""단순함, 명확함, 범용성""이 실무에서는 아주 큰 효과가 있음. 약간 관련 있지만 다른 얘기로, competitive programming을 해보면 코드의 정확성을 보장하는 기법을 확실히 익혀야 다음 단계로 도달할 수 있음
          + 여기에 동의할 수 없음. 글 작성자가 말한 건 완전한 형식적 증명이 아니라, 자신의 코드에 어떤 논리적 속성이 보장되는지(예: 인바리언트) 꼭 고민하라는 점이라 생각함. 이 과정이 코드를 이해하고, 내용의 두려움을 해소하는 가장 좋은 길임
          + 여기선 오히려 원인과 결과가 바뀐듯함. 문제를 신중히 생각하면서 접근하면 그 결과 코드도 자연스럽게 명료하고 깔끔해짐. 논리가 명확해야 코드 설계도 명확해짐. 하지만 코드를 예쁘게 쓴다고 해서 그 자체로 정확성이 따라오리라 믿는 건 어불성설임. 물론 코드가 깔끔할수록 코드 리뷰 등에서 버그가 줄어드는 건 사실임. 형태는 기능을 따른다는 점을 유념해야 함
          + 증명의 가장 기본 개념은 ""이게 왜 맞는지에 대한 근거""임. 사소한 실수 방지용이 아니라, 근본적으로 방향성이 맞는지 확인하는 과정임
          + 코드의 정확성을 위해선 올바르게 코드를 작성하는 것 외 다른 어떤 대체 방법도 없음. 어렵더라도 무조건 올바르게 짜야 함
          + 첫 문단을 완전히 뒤집어보면, 적절한 추상화(즉 언어/코드베이스에 맞는 관용적 코드)가 있으면 프로그램 검증이 쉬워짐. 적절한 추상화에서 루프 불변식 등을 고민할 필요가 없기 때문에, 코드의 옳음에서 증명이 바로 따라옴
     * 가변성과 불변성(mutable/immutable) 역시 중요한 속성임. 최대한 상태를 불변으로 두면, 멀티스레딩뿐 아니라 프로그램의 상태 추론에서도 복잡도가 줄어듦
          + 원문 기사에 이미 그런 내용이 포함돼 있음
     * 80년대에 Carnegie Mellon에서 학부 시절 이런 원칙을 명확히 교육받았음. 이후 내 인생에서 큰 도움이 됐음. 특히 재귀와 귀납법의 동등성을 배운 순간, 재귀 알고리즘을 ""답 나올 때까지 마구 시도"" 대신 깔끔하게 접근할 수 있게 됐던 기억이 남
          + 최근에 그 강의를 들었고, 함수형 프로그래밍 들으면서 더 크게 깨달음을 얻었음
"
"https://news.hada.io/topic?id=22020","월마트가 외부에 종속되지 않는 AI 플랫폼을 구축한 방법","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    월마트가 외부에 종속되지 않는 AI 플랫폼을 구축한 방법

     * Wallmart는 기존 AI 솔루션을 구매하지 않고, 자체 AI Foundry 플랫폼 Element를 통해 AI앱을 개발하며, 150만 직원이 이를 활용 중
     * LLM에 종속되지 않는 구조를 통해 매 쿼리마다 최적의 모델을 선택할 수 있어, 비용 효율성과 성능 최적화를 동시에 실현함
     * AI 앱을 프로젝트가 아니라 제품처럼 대량 생산하는 'Foundry 모델'을 도입해 개발 속도를 획기적으로 향상시킴
     * 업무 일정, 실시간 번역, 대화형 AI, 재고 관리 등 5개 핵심 앱을 빠르게 출시하며, 앱 개발 주기를 수 주 단위로 단축
     * 공급망 데이터를 중심으로 운영과 피드백을 결합해, 운영 데이터를 실시간으로 분석하고 반영하여 지속적으로 개선되는 AI 앱을 만들고 있음
     * 월마트는 AI를 ‘설치하는 소프트웨어’가 아니라 ‘내재화된 역량’으로 전환하고, 이를 통해 경쟁사와의 격차를 계속 확대하고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Walmart isn’t buying enterprise AI solutions — they’re creating them

  자체 AI 플랫폼 Element 개발

     * 월마트는 외부 AI 벤더가 아닌 내부 AI Foundry를 통해 Element 플랫폼을 개발함
     * 이 플랫폼은 전통적인 소프트웨어 개발 속도를 넘어서는 속도로 AI 애플리케이션을 생산함
     * 150만 직원 중 매주 90만 명의 직원이 사용하며, 하루 3백만 건의 쿼리를 처리할 정도로 대규모 확장성을 보여줌
     * 실시간 번역은 44개 언어, 교대 근무 일정 계획 시간은 90분에서 30분으로 단축됨
     * 이는 단일 앱 성공이 아닌, 산업화된 AI 개발 방식의 효과를 보여주는 초기 신호임

  LLM-agnostic 설계 철학과 오픈소스 기반 설계

     * Element는 특정 대형 언어 모델(LLM)에 종속되지 않고, 유연하게 모델을 선택할 수 있는 구조를 가짐
     * 사용 목적이나 쿼리 유형에 따라 비용 대비 가장 효과적인 LLM을 자동 선택함
     * 플랫폼 구조에 오픈소스 통합 옵션이 기본 탑재되어 있어 확장성과 유연성이 높음

The first wave reveals the principles of the foundry model

  첫 번째 Foundry 앱 생산 사례

     * 다음 5가지 주요 애플리케이션이 Foundry 방식으로 동일한 플랫폼 위에서 ""제조""됨
          + AI 일정 관리: 관리자당 하루에 기존 90분 걸리던 업무 계획을 30분으로 단축, 공급망 데이터를 기반으로 작업 우선순위 결정
          + 실시간 번역: 44개 언어 지원, 언어쌍에 따라 최적 모델 자동 선택
          + 대화형 AI: 일일 3만 건 질의에 응답, 반복 작업에 대해 사람 개입 없이 해결
          + AR 기반 VizPick: RFID + 컴퓨터 비전 기술로 재고 정확도 85%→99% 달성
          + MyAssistant: 사내 문서와 데이터를 분석하는 도우미
     * 공유 인프라와 통합된 데이터 파이프라인으로 중복 개발을 방지함
     * 모든 앱은 동일한 배포 패턴, 품질 관리, 피드백 구조를 공유하며 생산 공정처럼 표준화됨

  반복 가능한 생산 체계

     * Element는 각 앱을 독립적인 프로젝트로 보지 않고, 조립식 제품처럼 생산
     * 데이터 과학자가 사양을 제출하면, 플랫폼이 모델 선택부터 인프라, 배포까지 자동 처리
     * 이전 앱에서 검증된 컴포넌트를 재활용할 수 있어, 신규 앱 개발 마찰이 거의 없음

How Walmart’s foundry model changes development economics

  AI 개발 경제학의 전환

     * 전통적인 기업 AI는 벤더 평가, 계약 협상, 통합을 반복하며 시간과 비용이 소모됨
     * 반면 Element는 여러 앱 개발 요청을 병렬 처리하며 낭비를 최소화
     * 생산성과 속도는 린 제조 수준으로, 앱이 아이디어 단계에서 즉시 개발로 전환됨
     * 일정 계획, 대화형 AI, AR 재고 시스템 등 모두 Element 기반으로 빠르게 구축됨

Supply chain data becomes development fuel

  공급망 데이터를 앱 개발 연료로 전환

     * Element는 공급망 시스템과 연결되어 트럭 도착, 쇼핑 패턴, 직원 피드백 등을 자동 수집
     * 이 데이터는 작업 우선순위 결정, 소비자 행동 예측, 지역별 조건에 따른 맞춤 모델 배포에 활용됨
     * 운영 복잡성을 통합 데이터로 전환하여, 매장별 맞춤형 앱 개발이 가능해짐

Walmart has a model arbitrage strategy

  모델 아비트라지 전략

     * Element는 AI 모델 간 성능-비용 비교를 실시간으로 수행하여 최적의 경로로 쿼리를 처리
     * 쿼리별 복잡도에 따라 기본 또는 프리미엄 모델로 자동 라우팅함
     * 신규 모델이 출시되면 즉시 테스트 및 배포 가능, 기존 모델 성능이 향상되면 자동 적용됨
     * 예: 번역 도구는 언어쌍에 따라 서로 다른 최적 모델을 선택함

How Walmart integrates real-time feedback

  실시간 피드백 통합 구조

     * 직원들의 앱 사용은 단순한 소비가 아니라, 개선 신호를 생성하는 구조로 설계됨
     * 대화형 AI는 3만 건 쿼리를 통해 모델 성능, 쿼리 유형, 만족도를 측정하고 피드백으로 반영
     * 신규 앱은 이전 앱의 피드백을 학습한 상태로 출시되어, 런칭 초기부터 고성능 제공 가능
     * 이를 위해 정교한 데이터 파이프라인, 모델 버전 관리, 배포 오케스트레이션 구조를 갖춤

Why internal Foundries beat external platforms

  내부 Foundry가 외부 플랫폼을 능가하는 이유

     * 외부 플랫폼은 범용성을 위해 기능을 일반화함 → 특정 조직에 완벽히 맞지 않음
     * 월마트는 210만 명 직원의 공통된 업무, 용어, 목표에 맞게 플랫폼을 최적화함
     * 새로운 요구가 생기면, 벤더 협상 없이 즉시 개발 가능 → 아이디어에서 제품까지 빠르게 연결됨

Assessing the competitive implications

  경쟁적 함의

     * Foundry 방식은 앱을 만들수록 플랫폼 자체가 강화되고, 사용자 인터랙션은 모델 선택을 개선하며, 각 배포는 다음 앱의 생산 기준이 됨
     * 경쟁사들은
          + 자체 플랫폼 구축이라는 막대한 투자를 감수하거나
          + 외부 솔루션의 한계를 수용하거나
          + 아무것도 하지 않아 격차가 점점 커지는 위험을 떠안게 됨
     * 예: 일정 앱 하나만으로도 관리자당 하루 1시간 절약 → 전국적으로는 수백만 달러 절감 효과 발생

Lessons learned from Walmart’s enterprise AI Foundry blueprint

  Foundry 설계의 4대 원칙

     * 1. AI 모델은 교체 가능한 부품처럼 다뤄야 함
          + LLM 독립 구조로 벤더 락인 방지 및 최적화 지속 가능
     * 2. 데이터 접근 통합이 우선
          + Element는 LLM의 세계 지식과 월마트의 내부 데이터를 통합함
     * 3. AI 개발을 산업화해야 함
          + Foundry 모델을 통해 개발 → 배포 → 반복의 표준 공정화
     * 4. 처음부터 피드백을 설계에 반영
          + 피드백 루프 내장으로 사용할수록 성능이 향상되는 앱 구조

Walmart just created the enterprises’ new imperative

  기업 AI의 새로운 전환점

     * 월마트는 AI를 ‘도입’한 것이 아니라, AI를 만드는 능력 자체를 확보함
     * AI를 개별 소프트웨어가 아니라 조립 가능한 제품군으로 간주함
     * 직원과의 인터랙션이 시스템을 더 똑똑하게 만들고, 배포될수록 플랫폼이 정교해짐
     * AI 성공의 핵심은 모델 선택이 아니라, 조직의 AI 생산 역량 구축임
     * 월마트는 AI를 소프트웨어가 아닌 전략적 자산으로 정의한 첫 기업 중 하나로 자리매김함

   의미있는 행보네요
"
"https://news.hada.io/topic?id=21980","낙관주의가 현실을 만든다: Scale AI의 CEO, Alexandr Wang의 초고속 실행 방식 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      낙관주의가 현실을 만든다: Scale AI의 CEO, Alexandr Wang의 초고속 실행 방식 [번역글]

낙관주의가 현실을 만든다: 초고속 실행의 심리학

    1. 글의 배경
          + 이 글은 Scale AI의 CEO, Alexandr Wang이 2019년 Scale AI 팀에 보낸 메모를 바탕으로, 스타트업의 빠른 실행력과 그 동력을 유지하는 방법을 다룸.
          + 목표: “N번째 팀원도 10번째 팀원만큼 임팩트 있게 일할 수 있는 조직” 만들기.
    2. 핵심 논지
          + 예상 소요 시간(스코프)과 낙관주의가 실제 실행 속도에 결정적 영향을 미침.
          + “이 일은 오래 걸릴 거야”라고 생각하면 실제로 오래 걸리고, “금방 끝날 거야”라고 생각하면 더 빨리 끝남(=예상 소요 시간이 실제 소요 시간에 영향을 줌).
    3. 실증적 근거
          + 마라톤 기록 히스토그램: 3시간, 4시간 등 목표 시간 근처에 기록이 몰림. 사람들은 목표에 맞춰 엄청난 노력을 기울임.
          + 4분 마일 사례: 불가능해 보이던 기록이 한 번 깨지자, 이후 많은 사람들이 연달아 성공.
          + 학생 과제 마감: 대부분 마감 직전에 과제를 끝냄. 마감일이 행동을 결정짓는 기준선 역할.
    4. 림보 효과(Limbo Effect)
          + 사람들은 기준선(예상 소요 시간, 마감일 등)에 맞춰 행동을 조정하는 데 매우 능숙함.
          + 예상 소요 시간을 도입하면, 그 시간에 맞춰 일하게 되고, 평균적으로 전체 실행 속도는 느려짐.
          + “기준선이 어디에 있든, 우리는 그 바로 아래를 아주 잘 맞춰서 통과한다.”
    5. 낙관적 목표 설정의 효과
          + 매우 낙관적인 예상 소요 시간(예: 상위 10% 수준의 빠른 시간)으로 목표를 잡으면 실제로 더 빠르게 실행하게 됨.
          + 낙관주의가 현실을 바꾼다는 것의 실증적 근거.
    6. 실제 조직에서의 함정
          + 대부분의 팀은 “실패 리스크” 때문에 예상 소요 시간을 넉넉하게(비관적으로) 잡음 → 실행 속도 저하.
          + 이로 인해 사소한 일도 몇 주씩 걸리고, 팀의 에너지가 사라짐.
    7. 결론 및 실천 지침
          + Scale AI의 신조: “속도를 높여라(Up the tempo)”, “야망이 현실을 만든다(Ambition shapes reality)”.
          + 최대한 빠르게 실행하고, 낙관적인 목표를 세우는 것이 림보 효과를 극복하는 유일한 해법.
          + “정말 훌륭한 사람들과 함께 일할 때는 그들에게 위대한 일을 기대하면, 실제로 위대한 일을 해내더군요.” — 스티브 잡스 인용.

   요약 포인트
     * 실행 속도는 심리적 기준선(예상 소요 시간, 낙관주의)에 의해 결정된다.
     * 낙관적인 목표 설정이 실제 현실을 바꾼다.
     * 실행가와 창조자에게 가장 중요한 것은, 낙관주의가 현실을 바꾸도록 내버려 두는 것.
"
"https://news.hada.io/topic?id=22046","여름방학 동안 독서 저하 현상을 해결하기 위해 만들어진 “Reading Rainbow”","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            여름방학 동안 독서 저하 현상을 해결하기 위해 만들어진 “Reading Rainbow”

     * “Reading Rainbow” 프로그램은 학생들의 여름철 독서 저하 문제를 해결하기 위해 시작됨
     * 이 프로그램은 아동들의 독서 습관 유지와 문해력 증진을 목적으로 함
     * 다양한 어린이 도서 소개와 스토리텔링을 바탕으로 주목받은 프로그램임
     * 텔레비전 매체와 연계해 접근성을 높였으며, 아이들의 독서에 대한 흥미 유발 효과가 있었음
     * 결과적으로 교육 효과와 긍정적 반응을 얻으며 장기적으로 방영된 성공적인 사례임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

“Reading Rainbow” 프로그램의 도입 배경

     * “Reading Rainbow”는 미국 내 여름방학 기간 중 학생들의 독서량 급감 현상, 즉 ‘여름방학 슬럼프’를 대응하기 위해 개발된 어린이 TV 프로그램임
     * 프로그램의 목적은 아이들이 방학 중에도 꾸준히 책을 읽는 습관을 유지하게끔 유도하는 데 있었음

프로그램의 구성과 특징

     * 어린이 도서 선정과 다양한 주제의 책을 소개하는 형식으로 구성됨
     * 각 에피소드에서는 책 읽기의 재미와 스토리텔링을 전달하여 아이들의 독서 흥미 자극에 초점을 맞췄음
     * 텔레비전이라는 접근성 높은 매체를 활용해, 다양한 가정의 아이들이 쉽게 접할 수 있었음

성공 요소와 영향

     * “Reading Rainbow”는 장기적으로 방영되며 꾸준한 인기를 얻음
     * 아이들에게 긍정적인 독서 경험을 제공함으로써, 문해력 및 독서 습관 향상에 기여함
     * 교육 현장과 가정에서 높은 평가를 받은 대표적인 어린이 교육 프로그램 사례임

        Hacker News 의견

     * 우리 집 근처의 크리스천 도서관은 매년 방학마다 '독서 여름' 이벤트를 진행함
       아이들이 책을 빌려 읽고 2~3문장짜리 짧은 독후감을 쓰면 추첨에 참여할 수 있고, 마지막에는 작은 상품과 참가증을 모두에게 나눠줌
       이런 게 아이들에게 별 매력이 없을 거라 생각했지만 실제로는 매년 사람들이 많이 참여함
       꽤 놀라운 일임
          + 우리 동네 도서관도 여름 독서 프로그램이 있었음
            책을 읽고 사서에게 책 이야기를 들려줘야 했기 때문에 줄을 서 있었음
            한 어린아이가(3살도 안 됨) 책에 대해 말하는 순서였는데, 사서가 “책에서 가장 좋았던 부분이 뭐니?”라고 물음
            그 아이가 읽은 책은 Dinosailors였는데, 공룡들이 항해하다가 멀미해서 토하는 장면이 말 없이 그려진 페이지가 있음
            그 아이는 책에서 가장 좋았던 부분을 직접 시연해 보임
          + 80년대 Pizza Hut의 BOOK IT! 프로그램에서 책을 읽을 때마다 퍼스널 팬 피자를 공짜로 받을 수 있었던 점이 엄청난 동기 부여였음
          + 어릴 때 East Bay에서 Alameda Co. 도서관의 여름 독서 프로그램에 참여함
            책을 읽을 때마다 보물지도의 칸에 도장을 받았고, 마지막에는 별로 기억에 남지 않는 상품을 받았음
            하지만 45년이 지난 지금도 그 과정을 잊지 못함
          + “이게 아이들에게 매력이 없을 거라 생각했다”는 의견에 동의할 수 없음
            여름 독서 게임은 아주 유명하고, 아이들은 작은 상품을 정말 좋아함
     * 어릴 적 Wishbone이 내게 훨씬 더 와 닿았음
       Reading Rainbow 책 목록을 다시 봤을 때 여기 링크에 있는 책들을 많이 읽은 건 아님
       그래도 90년대에는 독서가 청소년 문화에서 정말 강조되었던 그 분위기가 그리움
       Dolly Parton 프로그램, 무료 Pizza Hut, accelerated reader 프로그램 등 덕분에 성장기에 감사한 마음임
          + Wishbone은 더 나이든 아이들 대상으로 약간 더 고전을 다루었음
            고전을 읽는 것과 쉬운 책을 읽는 건 확실히 더 나이 많은 아이들을 위한 프로그램 성격임
          + Wishbone도 좋은 프로그램이었지만 Reading Rainbow와는 접근 방식이 달랐음
            Wishbone은 고전 명작을 각색해 매회 단편 드라마처럼 제작했고, Reading Rainbow는 아이들에게 그림책이나 동시대 어린이용 책을 접하게 했음
            Wishbone은 지금 다시 봐도 잘 어울리지만 RR은 그 시절 특유의 분위기가 있음
            하지만 누군가는 책에 모험심을 주입해서 아이들이 결국 다음 세대의 고전을 찾게 해야 한다고 생각함
     * PBS 및 Reading Rainbow 같은 공영방송 지원금이 90억 달러나 삭감되지 않았다는 점이 정말 다행임
          + 정확히 하자면 90억 달러 전체가 삭감된 금액이고, 그 중 약 11억 달러가 CPB(PBS+NPR) 부문임
          + 90억 달러라니?!
            모든 것이 민영화되는 수순으로 가는 듯함
            NOAA와 National Weather마저 민간 데이터 서비스로 바꾸려는 움직임도 있음
            앞으로는 기상정보를 알고 싶으면 구독하게 될 수도 있음
          + PBS의 유명 프로그램만이 전부가 아님
            90~00년대엔 PBS에서 고등학생용 일본어 교육 프로그램 ‘Irasshai’를 방송했었음
            Georgia Tech와 협력해서 2년치 커리큘럼, 140개의 30분 강의, 500페이지짜리 교재와 수업지도안, 과제, 시험 등 모든 자료를 무료로 제공함
            방영 시간과 별개로 VCR로 녹화하라고 했고, 직접 클래스에 등록해서 소규모로 전화 수업도 할 수 있었음
            시험, 등급, 1:1 구술 평가까지 제공되는 정말 효과적이고 최고의 일본어 학습 경험이었음
            대부분의 고등학교가 스페인어 수업밖에 없는 현실에서 정말 멋진 프로그램이었음
            이제는 없어짐
            Irasshai 프로그램 링크
          + PBS SpaceTime을 더 이상 유튜브에서 볼 수 없을까봐 벌써부터 아쉬움
          + 당연히 삭감해야 할 건 군사비라고 생각함
            군사력이 필요 없는 세상에서도 미친듯한 과지출을 하고 있음
            엘론 머스크도 못 건드릴 일이고, 어느 대통령도 하지 못할 일이라고 생각함
            뭔가 심각하게 잘못된 부분이라고 느낌
     * 이 글에서는 Reading Rainbow의 취지도 칭찬하지만, 실제로 1983~2006년 동안 어린이 독서 점수는 거의 변화가 없었음 (500점 만점에 10~15점 변동뿐)
       전국적으로 책 읽는 즐거움도 줄어드는 중임
       Reading Rainbow의 목표가 아이들이 더 많은 책을 읽고 이해하게 하는 것이라면, 그 목표가 실제로 달성됐는지는 의문임
       오히려 아이들에게 긍정적인 TV쇼로서 즐거움을 준 것에 머무른 듯하고
       독서 습관과 성적 향상을 원한다면, 부모들이 PBS로 채널 돌리는 것 그 이상의 노력을 해야 한다고 생각함
       어린이 독서 점수 데이터
       책 읽기의 즐거움 감소 데이터
          + Reading Rainbow를 실제로 시청한 아이들의 점수를 비교하는 게 더 의미 있을 거라고 생각함
            미국 전체 어린이 중 이 프로그램을 본 비율은 아마 적어서, 전국 단위의 독서점수에 영향을 크게 미치지 못했을 거임
            즉, 단순 점수만으로 이 프로그램의 교육 효과를 판단할 수 없다고 봄
          + Reading Rainbow는 유행과 시대 흐름에 역행하는 미션을 가지고 있었음
            83~06년 세대는 TV와 함께 자란 첫 세대가 된 시점이고, 인터넷 접근도 가능해짐
            영화로 대신 보자는 문화, 기술 발전으로 미국 사회가 기존에 책보다 다른 매체에 매력을 느끼게 된 상황
            그래서 ""점수를 올렸냐""가 아니라 ""그래도 잘 버텼냐”가 더 의미 있는 질문임
            단순 그래프만으로 알 수 없는 뉘앙스라고 생각함
          + 맞는 말일 수 있지만, Reading Rainbow가 없었으면 점수가 어떻게 변했을지 알 수 없으니 단순히 이 결과만으로 의미를 판단하긴 어렵다고 생각함
          + 어떤 정책이든 부모의 적극적 참여에 기대는 건 결국 많은 아이들을 오히려 놓치게 하는 부분이라고 생각함
     * LeVar Burton은 성인 대상의 팟캐스트도 진행했었음
       작년 종료됐지만 거의 200개의 에피소드가 아카이브에 남아 있음
       여전히 오랜 기간 독서를 장려하는 활동을 이어가고 있음
       LeVar Burton Reads 팟캐스트
     * <i>Butterfly in the Sky</i>라는 다큐멘터리가 넷플릭스에서 볼 수 있음
       다큐멘터리 링크
     * 노르웨이에서는 여름 독서를 게임처럼 재미있게 만드는 프로그램이 있음 Sommerles 프로그램
       초등학생 저학년에서 인기가 많음
       읽은 책을 등록하면 포인트를 받고, 도서관마다 매주 '이주의 코드워드'를 포스터로 알려주며, 맞히면 또 포인트를 얻음
       10레벨을 달성할 때마다 도서관에서 작은 상품(예: 상어 이빨 장난감 등)을 받을 수 있음
          + 어릴 때 Pizza Hut에서도 'book it' 프로그램으로 독서를 게임처럼 만들었음
          + 읽은 책 등록해서 포인트 받고 도서관에서 상품 받는다는 게 정말 동기 부여가 될지 의문임
            이미 집과 도서관에는 장난감이 많기 때문에 이런 작은 상품에는 별로 관심이 없을 것 같음
            '책을 읽으면 상품을 준다'는 시스템 자체가 너무 계산적이고 값싸며, 부모 입장에서 내 아이를 이런 방식으로 조작하고 싶지 않음
     * 내 최애 Doors 노래임
       유튜브 링크
     * 이 프로그램은 아이들 대상으로 훌륭하게 만들어졌다고 생각함
       LeVar Burton이 책을 정말 잘 읽고, 덕분에 배움이 쉽게 느껴지고, 재미와 멋까지 있었음
          + 그는 Fred Rogers처럼 아이들이 잘 이해할 수 있게 하면서도 아이들을 깔보지 않는 희귀한 재능이 있었음
            오늘날 YouTube Kids나 스트리밍 앱 시대에도 그런 진행자 찾기는 훨씬 더 어려움
          + 나는 사실 RR이 너무 지루해서 눈물이 날 뻔했음
            보통보다 더 책을 읽는 아이였지만, 순수한 분위기를 좋아해서 일부러 좋아해 보려고 했던 기억임
            많은 사람들이 모두 이 프로그램을 ""좋아해야 한다""고 느껴서 칭찬하는 분위기임
            Rust 언어에 대해 다들 좋다고 말하는 것과 비슷하게 느껴짐
          + 어떤 방식이든 효과 있으면 좋은 것임
            약간 유치하지만 분명히 긍정적인 변화를 가져옴
            Punky Brewster와 Captain Planet 사이의 촌스러움, 그에 비해 빈티지 Sesame Street는 진정으로 멋졌음
          + LeVar Burton이 책을 잘 읽는다는 말이 이상하게 들림
            그는 직업 배우이기 때문에 당연히 더 잘 읽을 것임
"
"https://news.hada.io/topic?id=22080","경찰이 실시간으로 Ring 카메라에 접근할 수 있는 새로운 기능 도입","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 경찰이 실시간으로 Ring 카메라에 접근할 수 있는 새로운 기능 도입

     * Ring이 경찰이 가정용 보안 카메라를 실시간으로 라이브 스트림 할 수 있는 기능을 도입함
     * 이 변화는 Ring이 과거에 진행했던 개혁 조치를 되돌리는 것으로, 미국 시민의 시민 자유에 심각한 위협을 가함
     * 경찰이 Ring 영상을 이용해 시위대 감시, 무단 영상 확보, 그리고 임신중절자 추적 등 광범위한 감시를 강화할 가능성 있음
     * Ring은 AI 우선 전략을 내세우며 AI 및 얼굴 인식 같은 비디오 분석 기술 도입 신호를 보임
     * 이런 조치는 기술적 권위주의 강화, 방대한 감시 시장에서 이익 추구라는 비판을 받고 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Ring, 경찰 라이브 스트리밍 기능 도입 및 감시 기능 강화

   Ring의 창립자인 Jamie Siminoff가 다시 회사를 이끌면서, Ring 특유의 “감시 우선, 프라이버시 후순위” 정책이 부활함
     * 경찰이 Ring 사용자에게 직접 영상을 요청할 수 있는 기능이 재도입되며, 동시에 경찰이 집 내부 보안 카메라를 실시간으로 스트리밍 요청할 수 있는 새로운 기능이 추가됨
     * 이러한 변화는 미국 수백만 가정의 프라이버시와 시민 자유에 중대한 위협을 제기함
     * 경찰은 이미 Ring의 영상을 이용해 시위 참가자 감시, 영장이나 사용자 동의 없는 영상 확보 등에 활용한 사례가 있음
     * 앞으로는 경찰이 Ring 정보를 통해 임신중절 관련자나 이민 단속 대상자 추적 등에 악용할 여지가 큼

AI 우선 전략과 조직 내 변화

   Ring은 내부적으로 “AI 퍼스트” 접근을 선언했으며, 이는 향후 비디오 분석이나 얼굴 인식 기능이 추가될 위험성을 시사함
     * 직원 승진을 위해 AI 활용 증명을 의무화하는 방침이 시행됨
     * 기존에 도입한 여러 프라이버시 보호 조치가 축소 또는 철회되는 양상을 보임

Axon과의 파트너십 및 경찰 도구 개발

   Ring은 Axon과 새롭게 협력하여 경찰이 사용자에게 직접 영상을 요청하고, 사용자가 경찰이 자신의 카메라를 실시간 시청하도록 동의할 수 있는 도구를 개발 중임
     * 과거 Ring은 다양한 비판과 공공 여론에 따라 엔드투엔드 암호화, 경찰 공식 파트너십 종료, 영상 요청 도구 중단 등 수차례 개혁을 단행한 바 있음
     * 그러나 최근 Ring은 다시 대규모 감시 도구로의 회귀 움직임을 보이고 있음

배경 및 의도

   미국의 폭력 범죄율이 사상 최저 수준에 근접하고 있음에도 불구하고, Ring은 “안전”을 이유로 이러한 조치를 추진하고 있음
     * 실제로는 사용자 신뢰 배신보다는, 기술적 권위주의 흐름에 편승해 이익을 극대화하려는 의도가 있다고 지적됨
     * Google 역시 감시 및 방위산업에서의 영리 추구를 위한 윤리적 약속을 철회하며 유사한 흐름을 보임
     * IT 기업들이 방위 및 경찰 시장에 제품을 판매하며 거대 계약을 확보하는 양상을 보이고 있음

결론

   이와 같은 변화들은 사용자의 프라이버시와 자유를 심각하게 침해하며, 기술을 통한 감시 권위주의 강화 및 기업의 영리 추구라는 강한 비판을 받고 있음

        Hacker News 의견

     * 우리 누나들이 부모님 집에 Ring 카메라를 사서 설치해 달라고 요청했음 설치하기 전에 부모님께 ""이 카메라 앞에서 일어나는 모든 것은 3자에게 전송되고, 경찰 등 타인이 동의 없이 접근할 수 있으며, 데이터가 어디로 팔리는지도 확실하지 않음 이걸 정말 설치하길 원하냐""고 질문했음 부모님이 설치를 원치 않아 2년이 넘도록 상자째로 주방 카운터에 방치되어 있음
          + 부모님께 먼 곳에 사는 낯선 이가 진심으로 감탄한다고 전해주고 싶음 이런 문제에 대한 부모님의 신중한 판단이 특별하다는 점도 말씀해 드리길 바람 Ring의 경우 이미 경찰 등이 고객의 동의 없이 영상 요청을 우회할 수 있는 채널이 마련되어 있었음 실시간 스트리밍까지 허용될 경우 상황이 한층 명확해질 것임 타인의 시선에 자신을 상시 노출시키며 사는 삶을 이해는 하지만, 그런 기술을 이웃들에게 까지 강요하는 건 정말 바람직하지 않다고 생각함 이런 이슈는 결국 더 크게 영향 미칠 것으로 봄
     * opt-in이란 것이 기본값으로 체크돼 있고, 깊숙한 설정 메뉴 12단계쯤 숨어 있는 경우가 많을 거라 예상함 더 심한 경우, opt-in을 동의하면 요금 인상을 면제해 준다(최근 comcast가 한 방식)라는 식일 수도 있음 아니면 opt-in 동의가 데이터베이스 어딘가에 저장되었다가 ""버그""로 잘못 해석될 수도 있다고 우려함 제대로 된 opt-in을 원한다면, 동의가 필요할 때마다 SMS로 알리고 추가 정보 문의용 전화번호도 제공해야 함 그렇게 해야 최소한의 감사 추적이 가능함
          + 우리 집에도 Ring 기기가 여러 대 있는데, '설정이 12단계 숨겨져 있다'는 말이 딱 들어맞음 기본적인 기기 설정 조차 앱의 여러 화면에 흩어져 있어 자주 구글링을 하게 됨 최소한 똑똑한 검색창이라도 도입해줬으면 함 이런 옵션은 반드시 찾기 어렵게 설정돼 있을 것임
          + 예상이 맞음 comcast 일화에 대해 듣고 싶음 방금 검색해봤음
          + 제품 업데이트할 때마다 직접 선택해 둔 옵션들이 원래대로 초기화된다는 점도 문제임
     * 해당 기능이 존재한다는 것만으로 언젠가 법 집행기관이 반드시 악용하게 될 것임 opt-in 역시 사실상 무의미함 이런 기능을 안전하다고 생각하는 건 완전히 순진한 태도라고 봄
          + 기술적, 법적 관점 모두에서 내 생각 역시 동일함 정부가 새 기능을 만들게 조직이나 개인을 압박하는 것은 법적으로도 쉽지 않지만, 이미 존재하는 기능을 활용하는 데는 훨씬 공격적이고 법원 역시 관대해짐 이미 많은 사례에서 봤듯이, 이 기능이 수익원으로 작동하는 순간 기업이 ""우리는 나쁜 기업이 아니에요 약속해요!"" 같은 주장으로만 막아서는 아무 소용이 없음 2025년에도 “그거 opt-in이니까 괜찮아”라고 생각하는 사람이 있다는 게 신기할 정도임 이런 기능은 특히 Ring 사용자들이 자동적으로 설치하는 경우가 많으니 반드시 제대로 알고 대안 제시까지 할 필요가 있다고 생각함 사실 나도 직접 자가 호스팅 보안 시스템 구축을 업그레이드할 동기를 얻고 있음 실례로, 불과 5일 전에 “Oakland 경찰이 ICE에 차량 번호판 정보를 제공했고 SFPD도
            연방에 불법 공유했다”는 500개 이상의 댓글이 달린 HN 쓰레드가 있었음 그럼에도 “문제 없음, 수사기관은 절대 악용 안 함”이라고 주장하는 사람이 여전히 존재함에 놀라움
          + 언젠가 어떤 경찰관이 자신의 전 여자친구를 찾기 위해 이 시스템을 스캔했다는 뉴스가 반드시 나올 것임 확신함
          + Ring 카메라를 구입하는 시점부터 이미 순진한 선택임 어차피 모든 데이터가 '클라우드'로 올라가기 때문에 통제가 불가능함
     * 사람 영상 데이터를 본인 동의 없이 어떠한 상업적 목적에도 보관하는 것을 금지하는 규제가 필요하다고 생각함 명시적 동의를 받지 않은 사람의 얼굴 인식 역시 불법이어야 하며, 데이터의 어떤 형태의 압축본(즉, 얼굴이나 포즈 정보를 딥러닝 학습 등에 활용하는 것조차)도 모두 동의받지 않으면 금지해야 함 위반 시 전체 매출이나 자산의 일정 비율로 강력한 처벌이 필요하고, 내부 고발자에게는 큰 보상금이 따라야 함 더 넓게는, 이용자가 직접 참여한 거래와 직접 관련된 서비스 목적 이외에는 그 어떤 개인정보도 서버에 보존하거나, 상업적으로 결합/집계하는 것을 금지하는 규제가 필요하다고 생각함 결국 이런 원칙이 적용되면, 타겟 광고나 추천 시스템은 아예 불법이 될 것이고, SQL에서 user ID로 여러 row를 쿼리해 후처리하는 것도 이용자에게 직접
       데이터를 보여줄 때만 허용될 것임 이상적으로는 a) 사용자별 암호화 저장소, b) 이용자 소유의 데이터, c) 거래마다 상호 연관이 불가능한 임의의 아이디 등을 통해 데이터 집계가 아예 불가능해지도록 해야 함 물론 현실적으로 실리콘밸리와 정부가 이미 이런 체계를 강력히 저항 중이기 때문에 실현 가능성은 낮다고 생각함, 하지만 지금 상황은 상식적인 프라이버시 개념을 완전히 위배하는 현실임
          + 탈레반이 살아있는 생명체를 촬영하는 것을 불법으로 정한 매우 흥미로운(철학적으로) 법을 가지고 있음 이유는 정확히 모르겠지만, 무함마드 묘사 금지에서 파생된 것일 수 있음 본인 동의 없는 이미지 캡처 및 저장을 금지하는 취지는 꽤 공감됨
          + ""user ID로 여러 row가 반환될 수 있는 쿼리는 오직 이용자 본인에게 직접 데이터를 보여줄 때만 허용""이라는 뜻이 정확히 어떤 의미인지 설명 부탁함
          + ‘직접적인 거래에만 관련된 데이터 취합/보관만을 허용’이라는건 사실상 GDPR과 유사하다고 봄
          + 만일 그 규제가 실현되더라도, 국가기관은 언제나 예외조항(carve out)을 둘 것임 국토안보부(DHS)는 이미 미국 전역의 실시간 얼굴인식 감시를 준비하면서 엄청난 얼굴 기하정보 데이터베이스를 수집하고 있음 공항, 라스베이거스 시내 모든 교차로(및 아마 수많은 도시)에는 이미 카메라가 설치되어 관련 데이터를 모으는 중임
          + ‘사람이 등장하는 이미지 자체의 영리 목적 보관 금지’ 규제가 실제 도입되면, 기업과 실리콘밸리 IT 종사자들이 어떤 반응을 보일지는 이미 GDPR, AI법 사례에서 뻔히 알 수 있음
     * 경찰이 영상 제공을 요청할 수는 있지만, 클라우드에 그냥 로그인해 자기 마음대로 영상을 가져갈 권한은 없다는 점이 핵심임 하지만 요즘 연방 차원에서는 법적 근거 자체가 별 의미 없어진 것처럼 느껴짐
     * 경찰 국가와 이득을 챙기려는 기술 기업들, 그리고 그 경영자들 모두 극도로 혐오함 이런 시스템은 정부와 경찰, 각종 사악한 조직 및 개인에 의해 반드시 악용될 것임
          + 이런 제품 설치하는 사람들도 똑같이 문제라고 생각함 이런 시스템은 기생충처럼 퍼지는데, 결국 숙주가 필요하다는 게 씁쓸함
     * 합리적인 가격대의 오프라인 전용 보안카메라가 없다는 점에 아쉬움 본인은 현재 POE 방식의 상업용 카메라(개당 약 400달러), NAS를 조합해 사설망 위에서 운용하고 있고, 원격 접근도 직접 구축했지만 일반인에겐 비실용적이고 비싸서 추천하기 어려움
          + HomeKit Secure Video 도 클라우드를 쓰긴 하지만, 집을 벗어나기 전에 로컬에서 암호화되고 Apple도 열쇠를 모름 다양한 저렴한 카메라와 도어벨을 지원하며, 인터넷 접속을 막아도 로컬 HomeKit엔 문제 없음 완전한 오프라인 솔루션은 아니지만, 쉽게 쓸 수 있고 프라이버시 측면에서 매우 좋은 선택임
          + Reolink나 Axis, Dahua같은 ONVIF 카메라를 추천함 네트워크에서 NAS 외에는 외부로 통신하지 못하게 차단하면 됨 내 경우 카메라가 시스템으로 간단히 FTP 저장을 지원해 이중화 가능한데, PoE 방식이고 나머지 시스템 접근은 방화벽으로 통제해서 만약 백도어가 있더라도 사용이 힘들 것 같음
          + Synology Surveillance Station(링크)을 추천함 NAS 당 2대까지 무료, 추가 카메라는 대당 50달러로 합리적임 오래된 2베이 NAS에 카메라 2대를 붙여 몇 년째 잘 쓰고 있음 (Reolink, Amcrest 혼용, 둘 다 h264 녹화) 만족함
          + 보안성, 오프라인성 모두 갖춘 저렴한 카메라/베이비 모니터 찾기가 매우 힘들었음 대부분의 소비자용 카메라엔 원격접속 기능(=사실상 백도어)이 무조건 포함되어 있고, 와이파이를 쓰지 않는 일부 베이비모니터도 보안성이 매우 취약함 최종적으로 Amcrest IP2M-841과 Android용 Tinycam(RTSP 기반 추정)을 쓰고, 라우터에서 인터넷 차단함 그냥 연결만 해도 자동으로 원격접속 서버에 접속해버리는 걸 확인함
          + 나는 Unifi 전용임 Ubiquiti 단점도 많지만, Ring보다는 비교 자체가 안 될 정도로 안심임
     * Ring은 오프라인 이용이나 SD카드 저장을 거의 불가하게 설계되어 있음 이런 구조는 결국 정부가 어떤 방식으로든 감시에 접근할 발판을 마련하기 위한 것이 맞다고 생각하게 됨
          + 영상 저장 기능은 SD카드 같은 로컬 미디어 기록이 최소한 기본 사양이어야 함 오프라인 옵션 없는 기기는 절대 구매하지 않을 것임
     * 프라이버시를 신경 쓰는 이용자라면 Ring을 피해야 한다는 건 굳이 추가 이유가 필요 없을 지경임
          + 나는 피하고 싶어도 주변 4가구 중 2가구가 이미 설치함 결국 내 집 앞에서도 매번 본인 동의 없는 녹화에 노출되어 있는 셈임
"
"https://news.hada.io/topic?id=22066","메타, 유럽 AI 협정 서명 거부…성장 저해하는 `과도한 규제`라고 비판","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                메타, 유럽 AI 협정 서명 거부…성장 저해하는 `과도한 규제`라고 비판

     * Meta Platforms가 유럽연합(EU)의 인공지능 실천 강령에 서명하지 않겠다고 공식 발표함
     * 메타 글로벌 정책 책임자인 Joel Kaplan은 이 강령이 과도한 규제로, 혁신과 성장을 저해한다고 강조함
     * 해당 강령은 AI Act(2024년 제정) 준수를 위한 투명성 및 안전성 가이드라인 제공 목적임
     * 유럽연합 집행위원회가 지난주 범용 AI 모델을 위한 최종 실천 강령을 공개했으며, 각 기업이 자율적으로 서명 여부를 결정할 수 있음
     * 메타 외에도 ASML, Airbus 등 기업들이 시행 연기 요구 서한에 참여한 반면, OpenAI는 실천 강령에 동참할 의사를 밝힘
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

메타의 유럽 AI 실천 강령 거부 배경

     * Meta Platforms는 유럽연합의 인공지능 실천 강령에 서명하지 않기로 결정했음을 공식 발표함
     * 글로벌 정책 책임자인 Joel Kaplan은 ""유럽이 AI와 관련해 잘못된 길로 가고 있다""는 입장을 밝힘
     * Kaplan은 이 강령이 모델 개발자에게 법적 불확실성을 초래하며, AI Act 범위를 넘어서는 과도한 조치를 도입하고 있다고 주장함
     * 해당 실천 강령은 지난해 제정된 AI Act의 투명성·안전성 요구사항을 구체화하기 위한 것으로, 다음 달부터 시행 예정임

유럽연합 AI 실천 강령 개요

     * 유럽연합 집행위원회는 범용 AI 모델을 위한 최종 실천 강령을 공개함
     * 각 기업이 자율적으로 강령 서명 여부를 결정할 수 있음
     * 강령의 목적은 AI Act 준수를 위한 프레임워크를 제공하고, AI 기술의 투명성과 안전성을 확보하는 것임

업계 반응과 주요 이슈

     * 메타의 Kaplan은 ""이 강령이 과도하게 규제되어, 유럽 내 AI 모델 개발과 사업 성장에 심각한 장애가 될 것""이라고 비판함
     * ASML, Airbus 등 일부 대기업은 시행 2년 연기 요구 서한에 서명하며 비판적 입장을 표명함
     * 반면, OpenAI는 유럽 실천 강령에 참여 의사를 공식적으로 밝힘

메타 글로벌 정책 책임자 교체

     * Joel Kaplan은 2025년 초부터 메타의 글로벌 정책 책임자로 임명됨
     * 이전에는 Facebook의 미국 정책 부사장 및 조지 W. 부시 행정부에서 근무한 경력이 있음

   아래 해커뉴스 댓글에 나온 Latham & Watkins의 EU AI Act 첫번째 드래프트 설명

European Commission Releases First Draft of General-Purpose AI Code of Practice

     * EU AI Act는 2024년 8월 1일 발효되었으며, 관련 의무 사항은 단계적으로 시행될 예정임
     * 집행위원회 산하 AI Office는 실천 강령 마련을 주도하며, 2025년 5월까지 준비 후 8월 2일부터 발효될 계획임
     * 2024년 11월 14일, EC는 범용 AI 실천 강령 초안을 처음 발표함
     * 초안은 EU 원칙 및 가치에 맞춘 명확한 목표, 조치, 핵심성과지표(KPI)를 제시함

EU AI Act 초안의 주요 내용

  투명성

     * 투명성은 실천 강령의 핵심 원칙임
     * GPAI 모델 제공자는 다음 정보를 문서화하고 유지해야 함
          + 모델 자체에 대한 정보
          + 통합 가능한 AI 시스템 유형 및 용도
          + 허용된 사용 정책
          + 모델 라이선스의 핵심 요소
          + 설계 명세 및 학습 과정
          + 테스트 및 검증 방식
     * 위 정보는 AI Office와 국가 기관, 또는 AI 시스템 제공자에 요청 시 제공해야 함
     * 가능하면 대중에게 정보 공개도 권장함

  저작권 준수

     * EU 저작권 및 관련 권리 준수가 강조됨
     * GPAI 모델 제공자는 전체 수명주기 동안 포괄적 내부 저작권 정책을 수립해야 함
     * 타사와 데이터셋 계약 전 저작권 실사 및 Article 4(3) 권리 보유자 준수 확인 필요
     * SME를 제외한 GPAI 제공자는 모델이 저작권 침해 산출물을 생성하지 않도록 합리적 예방 조치 필요
     * 모델 제공 시, 상대방이 반복적 저작권 침해 방지 조치를 약속하도록 계약 조건화 권장함

    Text and Data Mining(TDM)

     * TDM을 수행할 때, 저작권 보호 콘텐츠에 합법적 접근 권한 확보 의무
     * Robot Exclusion Protocol을 준수하는 크롤러만 사용
     * 크롤러 제외가 검색엔진 내 콘텐츠 검색성에 부정적 영향 미치지 않도록 주의
     * 기계판독 가능한 권리 보유 표시에 적극 대응
     * 권리 보유 표준 개발에 협력
     * 불법 복제 소스의 크롤링을 방지하기 위한 합리적 조치 필요

    저작권 준수 투명성

     * 저작권 관련 조치 및 권리 보유 준수 사항 공개 의무
     * 권리 보유자가 이의제기할 수 있는 단일 연락 창구 제공 필요
     * 학습·테스트·검증 데이터 소스와 접근 권한 정보를 AI Office에 제공해야 함

  시스템 리스크 분류

     * GPAI 제공자는 시스템 리스크 분류 체계를 기반으로 리스크를 지속 평가·대응해야 함
     * 주요 리스크 항목:
          + 사이버 범죄
          + 화학·생물·방사능·핵 위험
          + 모델 통제력 상실
          + AI 연구·개발 목적 자동화
          + 대규모 설득·조작
          + 대규모 차별

  시스템 리스크 관리 프레임워크

     * 시스템 리스크가 확인된 GPAI 모델 제공자는 안전 및 보안 프레임워크를 구축해야 함
     * 투명성·책임성 강화를 위한 문서화와 거버넌스 구조(독립적 전문가 평가 포함) 필요
     * 사고 보고, 내부고발 보호, 공공 투명성 조치 등도 명시

        Hacker News 의견

     * Meta뿐 아니라, 40개의 유럽 기업들이 AI법 시행이 불명확하다는 이유로 EU에 2년 연기를 요청했음, 이번 code of practice는 자발적인 기준이고 실제 법보다 더 포괄적임, EU는 이 기준에 자발적으로 참여하면 규제가 덜할 수 있음을 시사했음, 그러나 Meta는 어차피 모든 면에서 규제 대상이므로 이런 자발적 동의가 실질적으로 실익이 적다고 생각함, 법 안의 중요한 내용은 모델 제공자가 파트너가 부적절하게 사용할 경우까지 책임을 져야 한다는 점임, 오픈소스에겐 매우 까다로운 요구임, 예를 들어 GPAI 제공자는 저작권 침해를 방지하는 합리적 장치를 마련해야 하고, 계약적으로도 파트너가 해당 방지책을 준수해야함을 명시하도록 장려함, 자세한 내용은 Latham & Watkins의 해설 참고 바람
          + 인용문을 읽어보면 EU가 저작권 보호 자료를 라이선스 없이 학습 데이터로 쓰는 걸 허용하는 예외 조항을 둔 맥락을 이해할 수 있음, 시행에 어려움이 있긴 해도 꽤 우아한 균형을 시도했다고 생각함
          + 이 기준은 합리적이라 생각함, 오픈소스 AI 모델에도 ‘저작권 보호 콘텐츠와 동일하거나 비슷한 결과물을 반복적으로 생성하지 않도록 적절한 조치를 취할 것’을 라이선스 조항에 넣을 수 있음, 이건 미국법이 아닌 유럽법이고 ‘합리적(reasonable)’이란 개념은 법관이 양쪽 이해관계를 저울질해서 해석하기 때문에, 문자해석만으로 판단되진 않음
          + 시장이 어떻게 발전할지 아무것도 모르는 상황에서 이렇게 신생 산업에 섣불리 규제하는 모습은 참 답답함
          + 수많은 저작권 작품을 그대로 재생산하는 모델을 만든다면 그 배포를 허용하면 안 된다는 점에서 이 기준이 무리하다고 생각하지 않음, 그게 단순한 소프트웨어에서도 허용되지 않을 것임, 단지 AI 모델이라는 이유만으로 예외가 될 수는 없겠음
          + 법 조항 중 모델 제공자가 파트너의 남용까지 책임져야 한다는 부분은 실제 법 원문[0]에는 없고, Code of Practice의 저작권 장에 포함된 내용임, 다만 이 Code는 법에 추가 요건을 부과하지 않고 법을 ‘어떻게’ 준수할 수 있는지 예시를 들려주는 성격임, 예를 들어 법에서는 ‘머신리더블 opt-out’ 존중을 요구하지만 구체적인 방식을 제시하지 않는데, 코드에서는 robots.txt 활용을 예시로 듦, 저작권 관련 내용은 measure 1.4로 명시되어 있는데, a) 모델이 저작권 침해 콘텐츠를 재생산하지 않도록 기술적 조치를 취할 것, b) 저작권 침해 사용을 허용하지 않는다는 내용을 이용약관이나 문서로 명확히 할 것, 오픈소스 모델의 경우 해당 사실을 문서에 안내만 해도 됨, 이 Code of Practice는 자발적으로 서명한 경우에만 적용되며, 남이 내 모델을 가져다 서명했다고
            해서 내 책임이 생기는 건 아님, 포토샵 플러그인을 GPL로 공개한다고 포토샵 소스 공개를 강제할 수 없는 것과 같음, 법에는 오픈소스 예외가 꽤 있음, Meta가 반발하는 이유는 EU AI Office가 Meta AI를 오픈소스로 인정하지 않아서 이런 예외를 못 받는 듯함, 자세한 원문은 법령 링크와 Code of Practice 링크 참고 바람
     * Meta의 반응만으로도 이번 AI 법이 우리가 정말 필요로 하는 내용일 것이란 편견이 생긴다는 점을 인정함, 실제로 무엇이 들어있는지 잘 모르지만 이런 생각을 하게 됨
          + “AI Code of Practice”는 총 3장으로 구성되고, 여기 및 초안 이력에서 확인 가능함, 아직 전문을 읽진 않았고 이전 AI 법(artificialintelligenceact.eu)만 익숙함, 추측컨대 Meta는 2장, 저작권 관련 내용, 특히 저작권 자료의 무단 크롤링 관행과 충돌하는 점을 두고 문제 삼을 듯함, 이게 진짜 ‘공정 사용’에 해당하는지 지금도 불분명함
          + 어떤 기업이 ‘악하다’고 해서 그 주장이 반드시 틀렸다는 뜻은 아니라 생각함
     * 가이드라인 요약을 궁금해하는 분에게 이곳에서 확인 가능함, 확실히 부담이 큰 규정임, 실제로 보면 대형 저작권자·변호사·관료에게나 유리해 보임
          + 이런 규제들은 결국 유럽 기업에 함정이 될 수 있음, 일정 규모를 넘는 순간 규제 부담이 급격히 커져 중소/신생 유럽 AI 기업 입장에선 성장 임계점을 넘는 게 두려운 상황임, 반면 미국·중국 빅테크는 훨씬 빠르게 혁신할 수 있는 환경 속에서 AI 수준을 높이고 자본도 키워, 결과적으로 잘 다듬어진 제품과 두터운 자금력으로 EU 시장에 진입해 진짜 경쟁은 이들에게 유리하게 기울 수 있음
          + 유럽은 AI 산업을 키워 본 적이 없으면서, 산업 전체를 지나치게 세세하게 규제하려 듦, 규제가 규제를 위한 수준임, Draghi의 EU 경쟁력 보고서에서 변화의 전기를 기대했지만, 실제 EU는 그 방향을 전혀 바꾸지 않았다고 실망감이 큼, EU 정책에서 신뢰를 잃게 됨
          + ‘부담스럽다’고 표현했는데, 구체적으로 어떤 점이 그렇게 부담스럽다는 것인지 궁금함
     * EU의 규정이 때론 쿠키 정책처럼 전 세계를 따르게 만드는 힘이 있었음, 대체로 소수가 대다수에 요구를 관철하려면 준수 비용이 거역 비용보다 낮아야 함, 하지만 AI는 다름, 판이 너무 크고 누구도 멈추지 않을 것임, AI는 핵전쟁이 아니라면 절대 막을 수 없음
          + AI의 잠재 위험은 어마어마함, 자동화 무기부터 악의적 AGI까지 상상 가능함, 독일은 동서독 분단 시 자동화 기관총 트라우마가 남아 있고, 우크라이나 역시 심각한 드론전 심리적 충격을 겪는 중임, AI의 실질적 위험 역시 분명함, 규제와 법이 필수임
          + 쿠키법 결과는 사람들이 끝도 없는 팝업에 시달리는 것뿐임, uBlock 없으면 웹 사용 자체가 고통임, 사용자 추적은 서버사이드로 이동했고, 실제 개인정보보호는 향상되지 못함, 돈이 너무 많이 오가니 산업계가 이런 허술한 규제를 쉽게 우회함
     * 유럽 규제에 무작정 동조하는 댓글이 많아 놀랐음, 기본적으로 유럽 규제는 과도하고 잘못 설계된다고 보는 유일한 사람인지 궁금함
          + 유럽은 지금까지 미국 빅테크 독점을 깨는 정책을 낸 적이 없음, EU 사용자 대부분은 여전히 Google, Meta, Amazon에 의존 중임, EU의 목표는 미국 기업에 직접 대항하는 게 아닌, 예의를 갖춰주고 일부 국가 안보 이익 지키는 수준인 듯함, 너무 점잖지만 본질적으로 합리적인 포지션임
          + 시민 보호 측면에서라면 규제가 엄하다 해도 나쁠 게 뭔지 궁금함
          + ‘맹목적’이라고 단정하는 건 본인은 옳고 상대방은 몰라서 그런 결정을 한 거라 가정하는 셈임, 본인도 ‘기본적으로 가정한다’고 했으니 스스로 비판하는 것과 똑같음
          + 사실 유럽을 억누르려는 외부의견이 너무 많이 개입한 것 같음
          + “기본적으로 가정한다”는 점 자체가 문제임, 일단 가정하지 말고, 관련된 내용을 직접 읽고, 본인만의 의견을 갖는 게 중요하다고 생각함, 초국가 대기업의 의견 말곤 각자 의견을 가져보면 어떨지 제안함
     * EU가 LLM에도 모든 웹사이트처럼 팝업을 다 달아버릴 것 같아 불안한 심정임
          + 인터넷엔 원래 온갖 팝업, 주의 끄는 UX 패턴이 가득한데, 유일하게 문제 삼는 게 사용자에게 추적 파기를 직접 고를 기회가 되는 팝업이라니 의아함
          + 팝업을 의무화한 건 EU가 아니라 기업 쪽임, 웹사이트 운영자들이 직접 데이터 수집을 줄이거나 아예 피하면 팝업도 필요 없는데, 오히려 사용성 포기하면서 무의미한 팝업을 도입함, 그 결과가 지금의 난장임, 그나마 아무 것도 수집하지 않는 사이트(예: Fabien Sanglard 블로그)는 팝업이 전혀 없음, EU의 실수는 이런 악용까지 예상 못했던 점임, 결과가 너무 안 좋긴 함
          + 팝업 없이도 충분히 할 수 있는데, 누구나 대기업 따라하기만 바쁨, 사용 경험보다 따라하기가 우선임
          + 이런 팝업 정말 싫어함, 이런 상황이 계속 방치되는 걸 보면 담당자들이 얼마나 느린지 드러남
     * Meta의 성장 저지가 목적일 거라 예상함, LinkedIn 게시글을 보면 Meta는 오히려 유럽 기업의 성장 저해를 걱정한다 밝힘, “이런 과도한 규제가 유럽 내 프론티어 AI 모델 개발/배포를 저해하고, 그 위에 비즈니스를 쌓으려는 유럽 기업까지 위축시킨다”고 전함
          + Meta가 “타 기업들과 우려를 공유한다”고 할 때 실제 의미는 단순히 자기 PR을 위해 그 순간 편한 논리를 쓰는 것뿐임, 실제론 공익이나 타사의 이익에는 전혀 신경 안 쓰고, 오로지 데이터 더 모으고 광고 더 팔 생각뿐임
          + 실제 Code of Practice를 훑어보면 특별히 과하거나 ‘과도한 권한 남용’이라 할 부분은 없음, 결국 모델 제공자들이 투명하게 행동해야 하는데, 이게 Meta의 기존 방식과 충돌할 뿐임
     * Kaplan의 LinkedIn 포스트엔 정책 중 어떤 점이 문제인지 전혀 언급이 없음, “성장 저해”란 표현도 사실상 신규 기능 도입 시 opt-in 의무화 같은 소박한 내용일 수도 있다고 생각함
          + 어떤 규제 앞에서든 이런 핑계가 단골로 등장하는 느낌임
     * 글로벌 환경이 점점 양분되고 있고, Meta는 불만과 갈등을 증폭하는 데 한몫 했다고 생각함, 언젠가 유럽에 가서 오픈소스 LLM을 자유롭게 쓰고 싶음
     * 이 논의가 “규제가 있으면 AI 발전이 멈춘다”는 결론으로 흐르지 않길 바람, 저작권이나 정보 출처 보호 등에도 충분한 가치를 둬야 함
"
"https://news.hada.io/topic?id=21977","AI로 장사하고 AI 때려잡는 유튜브의 모순","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        AI로 장사하고 AI 때려잡는 유튜브의 모순

   핵심 요점:
     * 유튜브는 저품질 AI 콘텐츠를 규제하려 하면서도 생성 도구를 직접 제공함.
     * 반복적·진정성 없는 콘텐츠를 금지하는 새로운 정책 발표.
     * AI 콘텐츠 확산 배경에는 구글의 책임도 존재.
     * 사용자 자진신고 중심의 규제는 실효성에 한계.
     * 구글은 AI 콘텐츠로 수익을 기대하면서도 유튜브는 깨끗하게 유지하려는 모순적 태도.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    서론: 유튜브가 직면한 AI 콘텐츠 문제

     * 유튜브에 저품질 AI 생성 콘텐츠가 급증하고 있음.
     * 구글은 이를 규제하려 하면서도 AI 도구 제공으로 확산에 기여함.
     * AI slop(쓰레기 콘텐츠) 확산은 플랫폼 신뢰도에 악영향을 줌.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    본론

      1. 유튜브의 새 정책 발표

     * 2025년 7월 15일부터 유튜브 파트너 프로그램(YPP) 수익 기준 강화.
     * ‘대량 제작·반복 콘텐츠’, ‘진정성 없는 콘텐츠’는 수익 창출 제한 또는 계정 퇴출 대상.
     * 리액션이나 클립 중심의 창작자는 직접 영향 받지 않음.

      2. 기존 정책의 연장선에 있는 조치

     * 이전에도 무단 편집·재업로드 콘텐츠는 수익 대상 아님.
     * 단지 이번엔 ‘AI 생성물’도 명시적으로 포함되었음.

      3. AI 생성 콘텐츠의 실태

     * 쇼츠 중심으로 AI 생성 영상이 범람(이미지, 대본, 목소리까지 AI 사용).
     * 가짜 가수 명의 음원 업로드, 자동 업로드 시스템으로 하루 수십 건 이상 게시.
     * 알고리즘에 걸려 수익을 노리는 방식이 일반화됨.

      4. 허위정보·조작 콘텐츠의 문제

     * 일부 콘텐츠는 가짜 예고편 등으로 조회수를 노림.
     * 유튜브는 AI 생성 콘텐츠에 라벨 부착 정책을 시행 중이나, 자진신고 방식에 의존.
     * 악용자들은 라벨을 회피하고 있음.

      5. ‘AI 팟캐스트’의 부자연스러운 콘텐츠

     * 뉴스 요약을 AI 음성으로 읽고, 가짜 대화 연출.
     * 어색하고 진정성 부족.
     * 유튜브 정책 위반이지만, 제작엔 구글 제미나이 활용됨.

      6. 구글의 AI 영상 생성 툴 공개

     * Veo 3: 최대 8초 고화질 AI 영상 생성 기능을 유튜브에 통합 중.
     * 유료 서비스로 전환될 가능성 있음.
     * 자동 태그 시스템 존재하지만, 영상 재업로드 시 우회 가능성 큼.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    결론: 수익과 규제 사이의 모순

     * 구글은 AI 도구를 제공해 콘텐츠 생성을 유도하면서,
       유튜브에서는 저품질 콘텐츠를 배제하려는 이중적 태도를 보임.
     * 시청자 입장에서는 이러한 두 목표가 동시에 충족되기 어려움.
     * 결국 유튜브의 신뢰성과 생태계 건강성을 위한 일관된 전략이 필요함.

   오히려 책임을 지는 자세 아닌가요; 구글의 방향성 자체에는 저는 공감되는 것 같은데.

   왜 이중적 태도라고 하는 지 모르겠네요...

   저도 동감하네요. 이중적인지 모르겠네요. 잘만들어진것들은 냅두던데요

   안 그래도 요즘 쇼츠 보면 죄다 TTS 떡칠 + AI로 찍어낸 이미지에 자막도 AI 시켜서 간단한 문장도 틀려먹는 저급 컨텐츠가 범람해서 안 보게 되던데 옳은 방향인 것 같네요..!

   저 글 쓴 사람 양산형 쇼츠 딸깍했는데 정지 당했나 ㅋㅋ
"
"https://news.hada.io/topic?id=21990","Show GN: Three.js 기반 광선검 시뮬레이터","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Show GN: Three.js 기반 광선검 시뮬레이터

   광선검 시뮬레이터 입니다. three.js로 웹에서 간편하게 돌릴 수 있도록 개발되었고요. 경량화와 인터렉션이 초점을 두어 만들었습니다.

   웹에서 bloom 효과가 필요하시면 이 코드를 참고해도 좋을 듯 합니다. 그 외에도 게임적인 인터렉션이나 UI 통합에서 영감을 얻으실 수 있을 것 같고요.

   살아남은 시간? 같은 걸로 점수를 표시해주면 재밌을거 같아요.
   포스가 없어서 그런지 너무 어렵네요. 포스가 함께하시길

   카메라를 켜니 손으로도 할 수 있군요!

   https://saber.fleet.im/

   아, 난이도가!! 쳐내기가 매우 어렵군요. 재미 있습니다!

   역시 포스가 없는 일반인에게는 어려운게 광선법 검술인것같습니다

   난이도 조정 옵션도 조만간 추가할게요!!

   낭만 넘치는 시뮬레이터입니다...!

   감사합니다..!

   오호 신기하군요 비트 세이버 같은 느낌이 드네요

   ❤️❤️❤️
"
"https://news.hada.io/topic?id=22040","Forge - AI 기반 터미널 개발환경 구축하기 ","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Forge - AI 기반 터미널 개발환경 구축하기

     * 터미널 환경에서 바로 통합하여 사용 가능한 AI 기반 종합 코딩 에이전트
     * 코드 이해·작성·리팩터링·디버깅·DB 설계·코드 리뷰 등 모든 개발 업무에 AI의 실시간 도움을 받을 수 있음
          + npx forgecode@latest 한 줄로 곧바로 대화형 AI 코딩 지원 사용 가능
     * 인터랙티브 명령어/프롬프트 지원: 코드 설명, 에러 진단, 리팩터링, 신규 기능 설계, DB 스키마 디자인, 코드 리뷰 등 다양한 요청 처리
     * 다중 AI 프로바이더: OpenAI, Anthropic, Google Vertex, Groq 등 다양한 LLM과 연동하여 원하는 모델 사용 가능
     * 코드는 클라우드가 아닌 개발자의 머신에서만 분석 및 처리되어 민감 정보 노출 위험 없음
     * Model Context Protocol(MCP)로 웹 자동화, 외부 API 호출 등 에이전트 워크플로우 확장 지원
     * forge.yaml을 수정하여 코드 스타일, 워크플로우, 모델, 최대 트리버설 깊이, temperature 등 세밀한 옵션 조정 가능
     * Rust 오픈소스. MIT 라이센스

   한국어 번역 (링크 포함)
   안녕하세요, 여러분. 이 프로젝트의 메인테이너 중 한 명입니다. 여기에서 저희 프로젝트를 소개해 주셔서 감사합니다!
   현재 forgecode.dev에서 한정 기간 동안 무료로 사용하실 수 있으니 꼭 사용해 보시고, 의견이나 제안이 있으시면 디스코드에 남겨주세요. 여러분의 피드백은 큰 도움이 됩니다.

   (위 한국어는 ChatGPT로 번역되었습니다. 의미가 조금 다르게 전달될 수 있는 점 양해 부탁드립니다.)
"
"https://news.hada.io/topic?id=22051","AtCoder World Tour Final 2025 Heuristic 부문, OpenAI가 2등을 기록","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       AtCoder World Tour Final 2025 Heuristic 부문, OpenAI가 2등을 기록

   해당 대회의 기록은 로그인을 하신 뒤 아래의 링크로 접속하셔서 볼 수 있습니다.
     * https://atcoder.jp/contests/awtf2025heuristic/standings/exhibition

   1등은 슬로베니아 프로그래머인 Psyho가 되었고, 2등은 OpenAI의 AHC AI가 2등을 달성하였습니다.

   1등을 달성한 프로그래머의 회고는 아래에서 확인하실 수 있습니다.
     * https://x.com/FakePsyho/status/1945444118924272018

     Humanity has prevailed (for now!)
     I'm completely exhausted. I figured, I had 10h of sleep in the last 3 days and I'm barely alive.
     I'll post more about the contest when I get some rest.
     (To be clear, those are provisional results, but my lead should be big enough)

   인류가 승리했습니다.(현재까지는!)
   완전히 지쳤어요. 지난 3일 동안 10시간 잠을 잤는데 겨우 살아있는 것 같습니다.
   휴식을 취하면 대회에 대한 자세한 내용을 게시하겠습니다.
   (분명히 말씀드리자면, 이는 잠정적 결과지만 제 점수가 더 클것입니다.)

   다른 참가자 회고에서도 OpenAI가 강력하다라는 의견도 많이 볼 수 있었습니다.
"
"https://news.hada.io/topic?id=22037","셰프의 칼 vs. 스위스 아미 나이프: 스타트업의 진짜 경쟁력 [번역글]","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                셰프의 칼 vs. 스위스 아미 나이프: 스타트업의 진짜 경쟁력 [번역글]

    1. 비유의 출발점: 도구 선택의 딜레마

     * 글은 개발 도구 또는 제품 선택에 있어 두 가지 상징적 도구(셰프 나이프, 스위스 아미 나이프)로 접근합니다.
          + 셰프 나이프(Chef’s Knife): 오직 요리에 ‘자르는’ 용도에만 최적화된 도구. 단일 목적 특화, 성능과 효율이 뛰어남.
          + 스위스 아미 나이프(Swiss Army Knife): 여러 기능(칼, 병따개, 드라이버 등)을 한데 모아 다양한 상황에 유연하게 대응 가능한 만능 도구. 하지만 각 기능은 전문 도구에 비해 퍼포먼스가 떨어질 수 있음.

    2. 전문 도구와 만능 도구의 특징 비교

     * 셰프 나이프의 장점
          + 단순하고 강력하며, 프로 요리사처럼 특정 영역에서 최상의 퍼포먼스를 보임.
          + 작업이 명확하고 일관적일 때 압도적 강점.
     * 셰프 나이프의 한계
          + 한 분야에만 특화되어 다른 영역에선 쓸모 없음.
     * 스위스 아미 나이프의 장점
          + 휴대가 간편하고 예기치 않은 다양한 상황에 대비 가능.
          + 다양한 기능이 하나에 모여 대처의 폭이 넓음.
     * 스위스 아미 나이프의 한계
          + 각각의 기능은 전문 도구에 미치지 못함.

    3. 소프트웨어 및 제품 개발에의 적용

     * 소프트웨어, SaaS, 플랫폼 등 제품을 설계/선택할 때 겪는 현실적 고민과 연결됩니다.
          + “우리 서비스가 만능이어야 할까, 아니면 특정 고객 문제에 아주 전문적으로 대응해야 할까?”
     * 실제로 모든 면에서 만능을 추구하기보다, 서로 다른 사용자의 니즈와 실제 상황을 명확히 파악하여 특정 영역 특화에 집중하는 전략이 더 나을 때가 많다고 주장합니다.

    4. 도구 선택의 기준과 시사점

     * 전문성 vs. 다목적성:
          + 자신의 작업, 팀 목표, 고객 요구에 따라 어떤 특화/만능 전략이 더 효율적일지 고민해야 함.
          + 단일 목적 도구(셰프 나이프)는 특정 작업에 압도적으로 효율적이지만, 예외적 상황이나 새로운 요구엔 유연하지 않을 수 있음.
          + 다목적 도구(스위스 아미 나이프)는 예상치 못한 상황에 대처할 수 있지만, 효율성/정밀도/만족도가 떨어질 수 있음.
     * 현실적 조언
          + ""모든 걸 다 할 수 있어 보이는 도구가 항상 해답이 아니다. 때로는 정말 잘 만들어진 '한 가지'가 모든 걸 바꾼다.""
          + 장기적으로 팀과 조직에서 도구와 제품 선택 시 선택과 집중의 원칙을 고민해야 함(.

    5. 결론

     * 핵심 메시지:
          + “모든 것을 할 수 있는 도구(Swiss Army Knife)가 솔깃하게 들릴 수 있지만, 진짜 필요한 건 특정 목적에 있어 압도적인 퍼포먼스를 발휘하는 진짜 도구(Chef’s Knife)가 되는 것일 때가 많다.”
          + 올바른 도구 선택이 효율성, 결과, 만족도에 엄청난 차이를 만든다.
"
"https://news.hada.io/topic?id=22081","차세대 프로그래머를 환영합니다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            차세대 프로그래머를 환영합니다

요약: “Welcoming The Next Generation of Programmers” (Armin Ronacher, 2025-07-20)

  1. 글의 배경

   저자는 파이썬 커뮤니티에 대한 감사와 회상을 바탕으로, 최근 유로파이썬(EuroPython)에서 느낀 감정과 과거 커뮤니티가 자신에게 준 영향력을 다시 성찰한다.

  2. 회상을 불러온 계기

   새로운 파이썬 다큐멘터리에 참여한 경험과 ‘agentic coding’ 및 이른바 ‘vibe coding’ 흐름으로 자연스럽게 끌려 들어가며 커뮤니티 변화를 체감한 것이 회상의 직접적 동인이다.

  3. 프로그래머 정의의 확장

   AI 도구(예: ChatGPT)를 활용해 실생활 문제를 해결하려는 다양한 배경의 사람들이 단기간에 “프로그래머” 정체성을 획득하고 있으며, ‘무엇을 하느냐’가 곧 ‘그 사람’이라는 관점에서 창작을 하면 곧 개발자로 인정해야 한다고 주장한다.

  4. 커뮤니티 수용성과 변화

   일부 회의적 시각이 존재했지만 AI·에이전트 활용 코드를 초보자가 작성하는 것에 대해 커뮤니티 내 수용이 빠르게 확대되고 있음을 관찰하며, 이는 배제보다 포용 전략이 필요함을 시사한다.

  5. AI가 불러올 저변 확대

   AI는 개발자 수를 감소시키지 않고 오히려 과거 어느 때보다 많은 신규 진입자를 유입시키는 촉매로 작용할 것이라는 전망을 제시한다.

  6. 포용의 선례와 과제

   PyLadies 등 다양한 온보딩·지원 프로그램을 통해 파이썬 커뮤니티가 이미 포용성을 증명해 왔음을 상기시키며, 이제 AI 기반 학습자들이 초기의 시행착오적 학습(과거 GOTO 남용 등)에서 배운 핵심 공학적 교훈을 구조적으로 체득하도록 안내해야 한다고 강조한다. \

  7. ‘Vibe Coding’에 대한 주목

   ‘Vibe coding’을 저품질 코드 우려 관점이 아니라 온보딩 경로로 인식해야 하며, 여기서 소외감을 느끼게 하면 중요한 학습 기회와 커뮤니티 가치 전수가 단절될 위험이 있다고 경고한다.

  8. 컨퍼런스와 연결 기회의 확대

   AI 덕분에 파이썬을 선택한 신규 작성자 다수가 존재하지만, 이들은 컨퍼런스나 커뮤니티 존재 자체를 모르는 경우가 많으므로 적극적 발굴·초대 전략이 커뮤니티 성장에 필수라고 주장한다.

  9. 인간 연결성의 결핍 보완

   AI 경유 진입자는 ‘사람 멘토’가 부재한 채 시작하기 때문에, 커뮤니티가 능동적으로 멘토링·온램프·관계 형성을 제공하여 순수 도구 의존 상태를 넘어 협업·엔지니어링 문화를 학습하도록 해야 한다고 역설한다.

  10. 커뮤니티의 전략적 임무

   고립된 AI 상호작용을 공유 여정으로 전환하고, 기업 중심 폐쇄적 에이전트 생태계에 종속되지 않도록 커뮤니티 주도 교육·가치 전파·지식 자유를 확보하는 것이 다음 세대를 맞이하는 핵심 과제로 제시된다.
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

    한눈에 보는 핵심 포인트

     * AI = 개발자 감소가 아닌 폭발적 저변 확대의 촉매.
     * ‘프로그래머’ 정의: 도구 사용 여부와 무관하게 무언가를 만들면 곧 프로그래머.
     * 위험요소: 인간 멘토·커뮤니티 접점 부재로 인한 학습 편향과 폐쇄적 플랫폼 종속.
     * 대응전략: 적극적 온보딩(멘토링, 컨퍼런스 연결, 가치 전수) 및 vibe coding 수용을 통한 포용적 성장.

   (원문: Armin Ronacher, “Welcoming The Next Generation of Programmers”, 2025-07-20)

   지금이 소프트웨어 개발을 배우기에 가장 좋은 시기일지도 모릅니다

   한 눈에 보기 쉽게 https://a1bbs.com/view/2w5cpznk6xrh166p3tnqpq 만다라트로 만들었습니다.
"
"https://news.hada.io/topic?id=21982","Django의 20번째 생일 축하","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Django의 20번째 생일 축하

     * Django가 출시 20주년을 맞이한 의미 있는 순간임
     * Django는 완벽주의자이면서 기한을 맞춰야 하는 개발자를 위한 웹 프레임워크로 20년 동안 많은 사랑을 받아온 프로젝트
     * Python 생태계 내에서 주요 웹 프레임워크로 자리매김하며, 대규모 커뮤니티와 풍부한 오픈소스 자료, 다양한 플러그인을 바탕으로 기업과 스타트업, 개인 개발자에게 폭넓게 활용됨
     * 오랜 기간에 걸친 지속적인 개발과 유지보수 덕분에 신뢰성과 보안, 확장성이 강화되는 흐름을 이어옴
     * Django는 간결하면서 강력한 설계 방식을 통해 빠른 프로토타이핑과 안정적인 서비스 운영 모두를 지원하는 점에서 독보적인 경쟁력을 보유함
     * 현재도 활발한 업데이트와 생태계 확장 중심으로 대규모 웹 서비스 개발자들의 핵심 도구로 활용됨

   Django만 사용하면 행복한데 DRF까지 엮이면 불행해져요.. 흑흑

   혹시 왜 그럴까요!?
   Django로 REST API 구현했었는데, Form 등을 사용하다보면 DRF라면 더 적절했을텐데 하는 아쉬움이 있었거든요.

   정확히는 프론트 따로 백 따로 있는 서비스에 Django로 백서버를 구현할 경우 백사이드만 쓰기엔 Django 특유의 풀스택스러움에서 오는 덩치가 부담스러운 것도 있었고, Django에 RESTful함을 더하기 위해 같이 사용한 DRF에 너무 큰 의존성을 갖게 되는 게 불쾌했습니다..
   특히 DRF 자체가 Django ORM에 강하게 엮여있는 것에서 오는 종속성과 DRF를 자주, 많은 곳에 사용할수록 아무 곳에서나 DjangoORM을 사용할 수 있게 되면서 오는 파편화, DB 접근 가능성 등이 불안함으로 다가오는 점, DRF에서 제공하는 serializer가 이름 그대로의 데이터 직렬화, 데이터 검증 그 이상으로의 역할과 가능성을 갖고 있는 것에 더해 serializer를 쓰면 쓸 수록 점점 MVC의 구분이 무의미해지는 것 등등.. 이럴 거면 굳이 Django+DRF 조합을 쓸 게 아니라 다른 프레임워크로 백단 구현하는 게 더 안정적이겠다 싶은 생각이 많이 들었었죠. 그래서 실제로 어느 순간부터는 FastAPI를 우선 선택하게 되었네요.

   Django를 사용한 이후, 제 커리어가 완전히 향상된 경험을 한 사용자의 입장에서 너무 감사하고 있습니다. Django에 contribution도 몇 번 했었는데 오래 사랑받았으면 좋겠네요 ㅎㅎ

   django 정말 좋아요! 저도 여러모로 많은 빚을 졌고 감사한 마음입니다. Django 가 예전보다 좀 덜 인기를 끌고 있지만. 사실 Django 가 특별히 언제 핫했던 적이 있던가 싶고 계속 꾸준히 안정적으로 사랑받는 프레임워크로 오래오래 갈거라고 믿숩뉘다!

   취미용 웹 개발로는 잘 쓰고 있습니다

   Django 생일 축하!

   우와...! 첫 사용할때, Python이라서 썼던 프로젝트였는데...
   오랜시간이 지났군요!
   다시금 사용할 수 있는 환경에서 일할 수 있으면 좋겠네요 :) ㅎㅎㅎ
   사이드나 해볼까...

        Hacker News 의견

     * 저는 제 커리어 전체를 Django에 빚졌음. 학부 시절 연구실에서 웹사이트를 만들며 Django를 처음 접했는데, 당시 Django는 완전 새로웠고 안정성이나 보안 같은 고민을 전혀 하지 않았기에 모든 걸 Django로 했음. 몇 년 뒤에는 Django에서 쌓은 실력 덕분에 첨단 머신러닝 연구실에서 복잡한 프론트엔드를 정리하는 역할로 들어감. 그 이후 연구실 스핀아웃에서 첫 번째 정직원, 대기업에 인수, 센서 기반 ML 시스템 확장, ML 중심 VC 펀드 공동창업, 그리고 10년 넘은 AI 기업 창업까지 모두 Django 덕분에 가능했음
          + 그 시절 Python 커뮤니티는 정말 따뜻한 분위기였음. 온라인에 서로 도와주는 사람이 많아서 Ruby도 마찬가지로 즐거운 경험이었음
          + 연구실에서는 Django의 ORM을 활용하면서 유저별로 물리적으로 분리된 MySQL 데이터베이스 서버를 개별 생성하는 바람에 복잡한 구조가 됐음. 사실 이럴 때는 sqlite 같은 게 더 어울린다는 생각임
          + Django가 불안정하거나 보안에 취약하다는 평판이 있었는지 궁금함. 그리고 머신러닝 박사과정 학생들이 왜 유저별 분리 DB라는 요구사항을 만들었는지도 궁금함
          + 나 역시 Django가 첫 프리랜서와 소프트웨어 개발을 손쉽게 경험하게 해준 소프트웨어였고, 고급 Python 소스코드와 개발 문화를 익힐 기회를 제공받음
     * 10년 전에 Kansas의 Lawrence에서 Django 10주년 행사를 오프라인으로 열었음. 그때의 발표 영상은 여기에서 볼 수 있음. 어제 Django 20주년을 맞아 당시 내 발표를 주석과 함께 정리했는데, Django의 시작 스토리가 궁금하다면 이 글을 참고해주길 바람
     * 나는 2006년, v0.95 ""magic removal"" 릴리즈 시절에 Django를 처음 사용함. 당시 19살이었고 작은 스타트업에서 PHP를 쓰고 있었음. Rails가 한창 주목받을 때라 설치해 보려 했지만 우분투 노트북에서 쉽지 않았음. 그러다 Python과 Django를 알게 되었고, 20분 만에 Hello World 페이지를 띄울 수 있었음. 이후 newforms, 1.0 릴리즈, 쿨한 DB 기능, 마이그레이션(South와 Nashvegas를 두고 토론), 클래스 기반 뷰, 내장 JSONField 같은 Postgres 기능, 파이썬3 지원, ASGI 등 다양한 변화가 있었음. 2008년 첫 DjangoCon 참가, 2018년엔 처음 발표 경험도 했음. 내 커리어는 Django에 빚졌고, 크고 작은 프로젝트에서 Django를 선택한 걸 절대 후회한 적 없음. 커뮤니티 덕분에 더 그랬음. Django 생일 축하함
     * Django는 그냥 바로 되는 프레임워크임. JS를 별로 안 좋아하는 사람에겐 Django가 계속 살아남아줘서 너무 고마움. JS 프레임워크가 뜨고 지는 동안에도 꾸준히 기여해 준 분들께 감사함
     * 나는 여러 회사에서 Django로 오랜 기간 일해왔음. 다른 프레임워크를 사용할 때마다 Django가 초창기 원칙인 ""batteries included""를 얼마나 잘 지키면서도 새로운 기술에 잘 적응했는지 깨닫게 됨. 정말 멋진 커뮤니티가 존재하고, 그게 이렇게 오래 유지된 건 정말 특별한 일이라 생각함. 물론 다른 프레임워크도 장점이 있긴 하지만 전체적인 툴링을 보면 Django가 복잡하고 대규모 프로젝트에선 여전히 최고임. 마이크로 프로젝트에도 나쁘지 않은 선택임
     * 나는 요즘 Python을 날카롭게 비판하지만 Simon과 전체 Django 커뮤니티에 고마움을 느끼고 있음. 정말 멋진 ""batteries included"" 프레임워크로 많은 프로젝트, 회사, 커리어에 영향을 줬음. 물론 나도 그중 하나임. 그리고 아직도 다른 생태계의 관리 패널을 평가할 때 pgadmin을 벤치마크로 삼음. Django로 이룬 성과는 대단하다고 생각함. Django가 없었다면 지금의 기술 발전도 훨씬 늦었을 것임. 정말 크게 감사함
          + Python을 비판하면서도 Django를 좋아할 수 있냐고 물어보고 싶음
     * 지난 15년간 Django와 함께 일한 경험이 즐거웠음. 커뮤니티에 합류하면서 큰 깨달음을 얻었고, DSF 이사회와 회장으로 봉사했던 것도 영광이었음. 앞으로 20년의 코드와 커뮤니티를 기대함
     * 지금까지 써본 프레임워크 중 Django가 단연 최고임. 이게 있는데 굳이 백엔드 자바스크립트를 배울 필요성을 느끼지 못했음
          + 나도 똑같음! 그래도 자바스크립트는 배우고 있음
     * Django와 Ruby on Rails 양쪽 모두 경험한 분들께 어느 쪽을 더 선호하는지, 그 이유가 궁금함. 10년 전쯤 Python을 먼저 배웠지만 Ruby를 익히고 싶어서 Rails를 처음 선택했음
          + Rails와 Django 둘 다 프로로 써봤는데, 실제로는 Django를 추천함. Rails의 메타적인 부분도 좋긴 하지만, Python 라이브러리 생태계가 워낙 커서 Django만으로 거의 모든 걸 할 수 있음. Rails 쓰는 회사들은 별도의 Python 코드를 두고 상호작용하게 되는데, Django에서는 ORM으로 한 번에 처리할 수 있음. 머신러닝/AI 등 특별한 라이브러리 필요 없고 한 명이 빠르게 개발한다면 Rails가 정말 빠름. 하지만 스타트업이라면 Django를 기본 프레임워크로 추천함
          + 내가 Django를 더 선호하는 이유는 다음과 같음: 항상 Python을 Ruby보다 좋아했고, 명시적 import와 네임스페이스, 한 가지 방법만 권장하는 철학이 더 확장성이 큼. Django도 이런 철학이라 설정이 약간 더 필요하고 명시적이지만 디버깅이 쉽고 명쾌함. 무엇보다 Django 문서가 프레임워크 설명뿐 아니라 좋은 개발 관행까지 잘 가르치기 때문에, Ruby는 이런 점이 부족해서 프로젝트별로 개발 방식의 편차가 큼. Django가 안정성 면에서도 우수하고, 큰 버전 간 마이그레이션이 쉽게 느껴짐. Python 생태계가 워낙 방대하고, Django의 admin과 Rest Framework는 정말 엄청난 시간 절약임. Rails에도 비슷한 게 있지만 임팩트가 다름. 물론 GIS나 과학 연산이 아니라면 개인 취향이 가장 중요하지만, Rails도 여전히 훌륭한 프레임워크임
     * 그 프로젝트(Django)는 내가 사는 지역 인근 KC 메트로에서 만들어졌고, 내 사업 파트너와 함께 큰 가치를 만들었음. 행복한 생일임
          + 안녕하세요 이웃님! :)
"
"https://news.hada.io/topic?id=21974","스타트업의 새로운 자금조달 전략 "Seed-Strapping"","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   스타트업의 새로운 자금조달 전략 ""Seed-Strapping""

     * 시드-스트래핑은 단 한 번의 초기 투자(프리시드 또는 시드, 보통 $50만~$400만 규모)만 받고, 이후엔 매출/이익만으로 성장하는 창업 방식
     * AI 기반 개발·운영 자동화 덕분에 극소수 인원·저자본으로 수주 내 실서비스 론칭·수익화가 가능해지며, 시장 적합성 확보→수익화→성장의 전 과정에서 “큰돈 없이” 진행 가능해짐
     * 대표 사례로 Zapier, Calendly 등은 한 번의 투자와 빠른 흑자 전환만으로 수천억~수조 기업가치 달성했고, 최근 AI 네이티브 스타트업도 이 전략을 도입하고 있음
     * 창업자에게 지분 희석 최소화, 경영 통제, 선택적 추가 투자, 건전한 조직문화, 채용 리스크 완화 등 다양한 매력 존재. 단, 더 큰 투자 유치 경쟁에서 밀릴 위험, 성장 속도 제한, 채용/유동성 문제 등 단점도 뚜렷함
     * Product-Market Fit 조기 달성, 낮은 CAC, 높은 마진, 분산/니치 시장 등 조건에서 효과적. 반면, 승자독식 시장, 대규모 선행 투자/신뢰 필요 분야, 거대한 성장전략엔 부적합
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Seed-Strapping이란?

     * 시드스트래핑은 시드 또는 프리시드 라운드(약 $50만~$400만) 만 조달하고, 이후 성장과 확장은 오로지 매출과 이익에 기반하여 이루어지는 모델임
     * 이는 기존의 벤처 모델과 달리 지속적인 펀딩 라운드를 건너뛰고, 제품 시장 적합성 달성 이후에는 자생적 성장을 목표로 함
     * ""부트스트래핑에 시드머니가 더해진"" 형태로, 충분한 런웨이로 PMF(Product Market Fit)와 수익성을 확보한 후 추가 펀딩 없이 성장함

AI가 어떻게 시드-스트래핑을 가능하게 하는가

     * A. 초기 제품-시장 적합성 도달이 쉬워짐
          + AI 도구가 제품 개발 속도를 획기적으로 높이고 있음
          + 2~4명 소규모 엔지니어 팀이 몇 주 만에 완성도 높은 제품을 출시할 수 있음
          + AI 덕분에 ROI가 높은 제품을 빠르게 출시, 빠른 고객 확보와 매출 창출 가능
     * B. 소수 인력으로 운영·성장이 가능
          + AI가 고객 지원, 세일즈, 마케팅 등 반복업무를 자동화 → 고정비·자본 소모 감소
          + R&D, 디자인, 엔지니어링까지 소수 정예 인력으로도 효율적 운영 가능
     * Henry Shi의 Lean AI Leaderboard 에서 20~30명 규모, 몇 백만 달러만 소진해도 10억 원 이상의 매출을 기록하는 사례 다수 등장

대표 사례

     * Zapier: $130만 시드 투자 후 2년 만에 흑자 전환, 이후 추가 투자 없이 수십억 달러 기업가치
     * Calendly: $55만 시드, 장기간 흑자 후 단 한 번 대규모 시리즈B($3.5억, 기업가치 $30억)만 유치
     * Veeva: $700만(시드~A)만으로 IPO까지, 그 이후 투자 없음
     * AI 스타트업(Aragon, Jenni.ai, Pump 등): 각 1~5M 투자 후 연매출 10~15M 달성, 한 번의 투자만으로 성장

장점과 리스크

     * 장점
          + 지분 희석 최소화: 창업자, 초기 멤버의 지분 유지 및 경영권 보장
          + 경영적 절제와 유연성: 자본 효율, 선택적 추가 투자, 배당 등 다양한 경로 가능
          + 안정적 근무 환경: 유동성, 거시경제 리스크 완화로 채용·조직 관리에 유리

     cj on June 28, 2023 | Hacker News
     많은 회사가 부트스트랩 또는 ""시드스트래핑"" 모델로 운영되고 있음
     본인 회사는 2015년 스타트업 액셀러레이터 이후 $120만 시드 투자를 받아 수익성을 달성했고, 이후 추가 투자 없이 매년 40% 성장을 오랜 기간 지속 중임
     채용 면접 시에도 이 모델은 큰 장점으로 어필함:
     ""대부분 스타트업과 달리, 우리는 많은 투자를 받지 않았고, 수익성이 있으며, 해고 경험이 없음. 제품 시장 적합성과 지속 가능한 비즈니스 모델이 있어 VC가 아닌 고객이 우리의 성장을 지원함""
     개인적으로도 시드스트래핑 모델을 매우 선호함. PMF를 찾고 조기 세일즈/마케팅을 시작할 수 있을 만큼(100만~200만 달러)만 투자받아, 이 돈이 소진되기 전에 수익성에 도달하는 것이 목표임.
     이 방식은 ""빠르게 실패할 기회""를 제공함. 많은 회사가 과도하게 자금을 조달해 아주 천천히 실패하는 상황에 빠지는데, 이는 피해야 할 부분임
     대규모 벤처 자금 조달은 현재 매출/운영 모델로 성장이 불가능함을 의미하는 신호이며, 무리하게 비유기적 성장 전략을 쓸 가능성이 높음. 이 경우 결국 조직 구조 변화, 직원 해고, 가격 인상 등 불안정성이 커질 수 있음
     Source: Hacker News
     * 리스크
          + 더 많은 투자 유치/성장 경쟁에서 밀릴 위험: 공격적 확장, GTM, 제품력 등에서 자본 많은 경쟁사에 밀릴 수 있음

     Rippling의 CEO Parker Conrad @parkerconrad ""더 큰 자본을 유치한 경쟁자가 결국 압도할 것""
     출처: Twitter
          + 성장 한계: 시장 규모, R&D, 대규모 네트워크 효과 시장 등에서 자본 부족이 발목
          + 채용, 유동성: 화려한 밸류에이션이나 대규모 스톡옵션 제공이 어렵고, 인재 확보에 어려움
          + 투자자/임직원 유동성 부족: 조기 엑시트/매각이 어려울 수 있음

Seed-Strapping이 효과적인 시장/제품

     * 빠른 PMF 달성: 의미 있는 매출 창출이 1년 내 가능할 때 효과적
     * 저비용 배포 : 제품주도성장(Product-led growth), 사용량 기반 가격책정(Usage-Based Pricing), 셀프온보딩 등 낮은 CAC가 가능할 때 적합
     * 고마진, 빠른 회수: 클라우드 기반, 70%+ 마진, CAC 1년 내 회수 구조에서 강점 발휘
     * 니치, 분산 시장에서 여러 업체가 공존 가능한 구조일 때 유리함
     * 비추천 케이스:
          + 승자독식(Winner-take-most) 시장: 마켓플레이스, 소셜 그래프, 플랫폼 등 속도전 필요
          + 하드웨어, 바이오, 심층 인프라, 규제금융 등 대규모 선행자본 필수
          + 신뢰·지속 혁신에 자본이 중요한 분야: 대형 고객, 인재, 규제 대응 등
          + 초고속 성장 경쟁 시장

“Skip-the-A” 전략

     * 최근 등장하는 ""스킵 더 A"" 전략: 시드 라운드 이후 전통적 시리즈A를 건너뛰고 높은 매출과 수익을 바탕으로 바로 시리즈B/C(수십억 밸류) 투자 유치로 직행하는 방식
     * 실제로 Calendly, AI 스타트업 등에서 이 모델이 급증 중
     * Y Combinator의 Garry Tan도 ""시리즈A를 건너뛰고 수익성 기반 성장 후, 높은 밸류에이션에 대형 라운드 유치하는 사례가 늘고 있음""을 강조

결론

     * Seed-strapping은 일부 AI 네이티브 스타트업·SaaS 창업에 현실적이고 매력적인 옵션으로 부상
     * 단, 만능 공식이 아니며 시장/제품 성격에 따라 “선택적 활용” 이 중요
     * 향후 “최고의 기업”들은 여전히 대규모 투자 라운드를 활용하겠지만, Seed-strapping은 벤처 투자·엑시트 구도를 바꾸는 새로운 트렌드로 자리잡을 가능성이 있음

   한국은 어떻게 다르려나요
"
"https://news.hada.io/topic?id=21978","북한 가짜 IT 인력 문제는 어디에나 존재함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        북한 가짜 IT 인력 문제는 어디에나 존재함

     * 북한 출신의 가짜 IT 구직자 문제가 대부분의 대기업에서 널리 확인됨
     * 이들의 침투 방식은 AI, 딥페이크, 위조 신원 등 다양하며, 심지어 내부 자료 절취 및 몸값 요구까지 이어짐
     * 최근에는 미국뿐만 아니라 유럽 기업도 표적이 되고 있으며, 대부분 원격 근무 직무에서 발생함
     * 기업들은 채용 프로세스 중 문서 확인, 대면 온보딩, 지표(IoC) 공유 등 다양한 방어 전략을 도입 중임
     * 범죄 수법이 진화하고 조직 범죄로 확산되고 있어, 보안·채용팀 협력 및 교육, 인적 방화벽 강화가 필수임
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

개요와 현황

     * 최근 북한계 가짜 IT 구직자 문제는 글로벌 대기업에서 보편적으로 발생 중임
     * Fortune 500 기업의 CISO 다수가 이 문제를 겪었음을 언급하며, Google, Snowflake 등의 보안 담당자 역시 내부 채용 과정에서 해당 사례를 확인함
     * 미국 법무부는 최근 6년간 이들로 인한 피해액이 8,800만 달러에 달한다고 발표함
     * 일부 경우 이들이 내부 시스템 접근을 통해 소스코드·기밀정보 탈취 및 몸값 요구로 이어지는 사례 보고됨
     * 미국 기업의 경계 강화로, 현재는 유럽 시장 targeting도 급증 중임

채용 과정에서의 특징적 경향

     * Socure 등의 기업에서는 최근 수 천 건에 이르는 비정상 지원서가 몰려들고 있음
     * LinkedIn 프로필이 얕거나 연결 인원이 적은데 비해 경력이 화려하며, 전화번호·이메일·VPN 사용 등에서 부정합 현상 감지됨
     * 화상면접 시 서구식 이름에 동아시아 외모, 억양 등 인구통계적으로 부조화 사례가 반복됨
     * 지원자 답변이 ChatGPT 등 AI 도구의 응답과 유사함이 다수 감지됨
     * 실제로도 친근하고 정상적인 인상이나, 심층 확인 시 다수 의심 정황 발견됨

보안·채용팀 협력의 필요성

     * 채용 담당자 대부분은 사이버보안이나 신원관리 지식이 부족하며, HR-보안팀 간 소통 부재가 문제임
     * Netskope 등은 보안·HR·법무부서 간 회의, 현지 FBI 브리핑 등 다자 협력체계 마련 중임
     * 원격 근무 환경에서는 PC 수령을 위한 오프라인 방문·주소 검증 등 실제 확인 과정이 중요하게 작동함
     * 문서 검증 요구시 가짜 지원자가 중도 포기하는 현상도 반복 관찰됨

AI 및 정보 공유의 대응 방안

     * AI, 딥페이크 등 신기술을 악용하는 사례가 늘고 있으나, Snowflake 등에서는 IoC(침해지표) 데이터셋 구축 및 파트너와의 정보 공유를 활용함
     * IoC에는 이메일, 실제 주소, 전화번호 등 위조 가능성이 높은 정보가 포함됨
     * 인적 방화벽(채용 담당자 교육) 전략으로 이력서 과장, 면접 답변 지체, 기술 혼동, 콜센터 환경 등 단서 탐지법을 주입함
     * 궁극적으로 대면 인터뷰와 부득이한 이유로 대면이 불가하다는 변명은 강한 의심 사유로 간주함
     * 기업, 파트너, 정부기관 간 협력으로 사전 스크리닝에서 의심 지원자의 진입 자체를 차단

조직화·범죄화의 확산 전망

     * 범죄 조직은 수익성 있는 수법이 확인되면 신속히 모방 및 확산하는 경향임
     * 북한 주도의 현상에 국한되지 않고, 타국·조직범죄까지 확대될 가능성 높음
     * 모든 채용 프로세스에서 보안과 채용팀 협력 강화, 최신 사례 교육 필요성이 커지고 있음

        Hacker News 의견

     * 대면 신원 확인 절차를 필수로 하면 이런 문제를 막을 수 있다고 생각함
     * LinkedIn의 연결 수가 25개밖에 안 되는 프로필로 진짜/가짜를 구분한다고 하는데, 사실 해킹된 LinkedIn 계정도 있음 동료의 계정이 해킹당했는데, 1,000명 이상의 실존 인맥이 있는 상태였음 사진과 이름이 동아시아스럽게 바뀌고, CV도 미국 국방 계약업체 경력으로 변경됨 운 좋게 자동 계정 잠금 기능 덕분에 걸렸지만, 이런 상태로 오래 방치되었을 가능성도 있었음 수천 명과 연결된 사람이라도 모든 인맥을 일일이 기억하지는 못하니까, 이름 변경 알림도 없고 이런 해킹된 프로필이 북한 IT 인력에게 팔려서 악용되는 게 충분히 가능함
          + 나처럼 아예 LinkedIn 계정이 없는 사람도 많음 ""직접 노트북을 받으러 와야 한다""는 오프라인 확인 아이디어가 훨씬 더 실효성 있다고 생각함
     * Jeff Geerling이 최근에 FBI로부터 연락받은 경험을 공유함, 내용은 북한 IT 위장 구직자들이 전략적으로 사용하는 미니 KVM 장비에 대한 것임 관련 영상
          + 여기서 KVM 장치는 여러 대의 노트북에 연결해서, 사람들의 집 지하실이나 방 같은 곳에서 실제로 운영되고 있다고 들음 누군가 회사에서 제공한 노트북 1대당 한 달 기준 일정 금액을 받고, 그 노트북에 작은 KVM을 꽂아서 원격 근로자가 접속하도록 해줌 이 과정에서 추적이 훨씬 어려워짐
          + 내가 잘 몰라서 그러는데 KVM이 정확히 뭘 하는 장치임? 그냥 Ethernet과 HDMI 포트가 있어서 원격 제어가 가능한 건가? 그리고 북한 사람들이 실제로 남의 집에 침입해서 이걸 끼우는 게 흔한 일처럼 이야기되는데 별로 상상이 안 감 FBI가 왜 Jeff Geerling한테 연락한 건지도 이해가 안 감 솔직히 난 KVM이 리눅스 커널 가상화란 의미로만 알았음
     * ""미국 IT 회사들이 가짜 지원자를 잡아내기 더 잘하기 시작하면서, 이제는 유럽 기업이 새로운 타깃이 되고 있다""는 내용이 있었음 미국에서 근무한 모든 회사가 내 신원을 아주 철저하게 검증했음 거의 모든 회사가 백그라운드 체크를 기본으로 함 유럽 회사는 오히려 조금 느슨한 것 같음
          + 미국 회사의 백그라운드 체크가 내 사생활까지 침해할 정도로 지나칠 때가 있음 예를 들어 채용 전에 신용도 조사, 카드·차·집의 잔여 대출, 월 상환액, 지난 7년간 연봉 모두 확인까지 요구함 이건 너무 심하다고 생각함 내 처지를 전부 아는 상황이라 연봉 협상 등에서도 불리함
          + 실제로 기업에서 누군가에게 ""네 신원을 빌려 쓰게 해 줄래?""라고 제안을 하는 사례도 보고됨 외형은 그 사람을 사용하되 실제 업무는 자신들이 처리함 월급을 나눠 갖는 구조임 당연히 엄청 위험하거나 불법적인 요소가 많지만, 그래도 쉽사리 돈을 벌고 싶은 사람들은 꽤 있음
          + 많은 유럽 기업은 대부분 원격 근무 자체를 제공하지 않거나 아주 드물고, 만약 화상면접이나 전화면접을 해도 거의 항상 오프라인 면접을 요구함 그래서 최소한 해당 국가 안에 실제로 거주하고 있을 것을 기본으로 기대함 그리고 현지 언어 사용 능력을 요구하기도 해서, 북한 IT 인력이 이런 과정을 넘기는 게 훨씬 어려움
          + 백그라운드 체크도 완벽하지 않음 이력서가 조작된 신분일 수도 있고, 미국인의 신원을 사서 쓰기도 함 I-9 신원 인증을 깰 수 있는 거의 유일한 방법임 백그라운드 체크 유형도 다양해서, 이전 근무지 확인처럼 번거로운 절차는 아예 생략하는 회사도 많음 추천인 체크도 의미가 없는데, 추천인 자체를 조작할 수 있기 때문임 결국 추천인에 대해서도 신원 검증이 필요함
          + 그건 사기 유형 중 하나임 신입 미국 이민자라면 백그라운드 체크조차 무사 통과할 수 있음 흔한 수법 중 하나가 위조된 학위와 경력으로 계약직 프로그램 개발자로 취업해 일을 아시아로 밤사이에 외주 주는 경우임 ChatGPT 도움으로, 모니터 사진만 찍어도 바로 텍스트로 변환해 원격 작업이 더 쉬워짐 심지어 자기 업무 일부를 외부에 아웃소싱하는 개발자도 흔함, 이런 방식으로 여러 곳에 동시에 근무하는 사람들도 있음
     * 이런 사기꾼들이 실제로 취업에 성공하면 그 다음 목적은 뭔지 궁금함 산업스파이처럼 첩보 정보만 수집하는 건지, 실제 맡은 업무도 하는 건지, 아니면 입사하자마자 최대한 자료나 돈을 빼돌리다 걸리면 도망가는 건지, IT 업무 능력 자체는 있는지 모든 게 궁금함
          + 기사에서는 회사 데이터나 소스코드를 인질로 삼아 몸값을 요구하는 케이스도 언급됨 북한 소속이 아니더라도, 기본 전략이 최대한 여러 회사에서 월급만 받다가 걸리면 새 회사로 넘어가는 식임 평소에 한두 달밖에 못 버티는 회사도 있지만, 관리자가 관심이 적어서 특별한 문제 없이 몇 시간 일만 하고 여러 곳에서 월급을 받는 경우도 있음 이런 식으로 단기 수익만 뽑고, 팀 전체가 구조조정 당하면 다시 채용시장으로 가는 식임
     * 어디서 본 트윗에 따르면, 지원자에게 김정은 비판을 시켜보면 북한 소속인지 걸러낼 수 있다고 함
          + 이건 한두 번만 시도해도 금방 무력화될 방법임 만약 내가 스파이거나 비밀작전 중이라면, 조직 차원에서 어떤 말이든 다 하게 만들어 줄 것 같음
          + 누가 나한테 김정은 비판해보라고 하면, 그냥 거기서 대화 끝임 난 내가 원할 때 자유롭게 비판함 그런 식이면 오히려 진짜 지원자가 걸러질 수도 있음
          + 실제 이런 스크리닝이 많아지면 ""아, 나는 들켰구나""라고 생각하거나 위험 부담이 커졌다고 보고 알아서 떨어져나가는 경향도 있을 것임 이메일 사기꾼들이 일부러 어설픈 문법을 쓰는 이유처럼, 애초에 어려운 지원자는 빨리 걸러내고 쉬운 케이스만 노리는 전략임
          + 이런 질문은 신중하게 걸러야 함 인종이나 이민 신분과 관계없이 모든 지원자에게 똑같이 질문했다는 근거가 남아야 함 사실상 아시아인이 아니거나 네이티브 영미권 주민한테도 이런 질문을 해야 안전함, 왜냐하면 북한도 앞으로 이런 대행자나 외부인을 시켜서 지원시키는 방향으로 나아갈 수 있으니까
     * 이런 사례가 계속 나오는 게 신기함 나야 평범하고 좋은 사람인데도 괜찮은 일자리 찾기가 힘듦 그런데 이런 가짜 지원자들은 줄줄이 채용된다는 게 이해가 안 감 도대체 기업들은 뭘 하고 있는 건지 궁금함
          + 이런 사람들은 지원서부터 거짓말임 신분도 훔치고, 경력도 조작하고, 추천인도 가짜고, LinkedIn도 해킹함 사기와 인터뷰에 특화된 전문가임 취업 자체가 생계인 사람들이 조직적으로 움직이니까 어떤 식으로든 구직에 성공함 그리고 일자리 퀄리티는 신경도 안 씀 일단 가능한 데서는 최대한 오래 버팀 많은 회사에서는 이게 몇 년씩 걸릴 수도 있음
     * 입사 첫 주를 아예 오프라인 근무로 강제하면 어떨까? 온보딩 명목으로 쉽게 포장도 가능하고, 그럼 이런 문제 해결된다고 봄
          + 모든 회사에 사무실이 있는 건 아님 내가 다닌 전 직장은 입사 6개월이 지난 뒤에야 사무실이 생겼고, 국가 내 반수 이상이 사무실까지 3~4시간은 걸려야 하는 거리였음 실제로 팀원 중 일부만 오프라인에서 만나봄, 나머지는 전 세계 3개 대륙에 흩어져 있음
          + 코로나 이후로 완전 원격 근무, 완전 원격 채용이 보편화됐기 때문에 현실적으로 오프라인 온보딩이 빠르게 사라졌음 하지만 이제 이런 문제 인식이 커지면서 오프라인 면접과 출근이 다시 표준이 될 가능성이 높아짐
          + 가능함 모든 사람이 좋은 패스워드 쓰면 보안에 좋다는 것과 비슷함 현실적으로 많은 회사는 아직 그렇게 하고 있지 않음 또 하나의 이유는 1주차 오프라인을 강제하면 인재 풀이 줄어듦 심지어 요즘은 강제출근, 100시간 근무 하자는 분위기지만 그래도 단점은 있음
          + 내가 다닌 직장에서는 3개월간 무조건 오프라인 출근해야 했음 오피스라고 해봐야 작은 원룸 수준이었지만, 목표 달성엔 충분했음
          + 1주차 오프라인 근무를 강제하면 보다 개선될 거라고 생각함 물론 그럼에도 변명을 구구절절 대거나, 특정 음식만 DoorDash로 시켜먹겠다 등 핑계 많을 거임
     * 동료 한 명쯤은 차라리 이런 사람이었으면 나았겠다는 생각 듦
     * 뭔가 이상함 개발자들은 몇백 군데 지원해도 겨우 인터뷰 한 번 받을까 말까인데, 막상 영어도 서툰 북한 IT 인력은 일자리를 계속 잡고 있음 LinkedIn에서도 이제는 최고지도자 찬양이라도 해야 할 판임
          + 사기는 정말 잘 하는 사람이 해야 성공함 완전 전문적으로 거짓말하고, 지원자 파이프라인 찾기, 합격시켜주는 팀, 인터뷰 대응 등 여러 역할이 나뉘어 있음 자동화도 많이 써서 효율도 대폭 높임 개인 개발자는 지원 포인트, 이력서, 인터뷰 등 수십 번 걸러지기만 하고 거짓말은 안 하려 하니 성공 확률이 적음 하지만 이런 사기조직은 하루 종일 오로지 이 일만 하니까 점점 더 잘하게 됨 실제 개발자는 취업 성공하면 더 이상 구직 안 하지만, 사기꾼은 계속 구직 기술을 갈고닦음
          + 이들은 현실의 제약을 신경쓰지 않음 이력서에는 하버드 출신, Meta 재직 경험, 등 온갖 경력 다 붙임 그리고 채용담당자는 이런 이력이 눈에 띄어서 내 이력서보다 위에 올려버림
          + 나도 이상한 이메일 주소에서 ""내가 일자리 따줄 테니, 인터뷰만 니가 하고 나머진 다 해줄게. 가짜 이름과 거액의 돈 보장""이라는 메일을 여러 번 받아 봄 이 정도면 내 이력서가 워낙 좋아서 이런 타입 사기에 노출되기 좋은 거라고 판단됨 근데 이런 방식은 너무 대충 만들어진 사기임 엔지니어 99%는 이런 메일 신경도 안 쓰거나 바로 무시함
          + 사실 이런 사람들이 실제로 일자리를 타기보다는, 그냥 인터뷰까지는 통과하는 수준임
          + 아마 이런 사람들은 정체성을 여러 개 활용하는 것 같음
"
"https://news.hada.io/topic?id=22082","Show GN: AI 만다라트, 비즈니스 모델 캔버스, 프레임워크","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Show GN: AI 만다라트, 비즈니스 모델 캔버스, 프레임워크

   AI 만다라트, 비즈니스 모델 캔버스, 프레임워크

   생각 정리를 위한 도구를 출판사가 만드는 이유
   요즘, 머릿속이 너무 복잡하지 않으신가요?

   매일 새로운 정보가 쏟아지고, 정리할 것도, 정답도 많아지는 시대. 이럴 땐 오히려, 한 발 물러서서 ""지금 내가 뭘 보고 있고, 무슨 생각을 하는 중인지"" 한눈에 그려볼 수 있으면 좋겠다는 생각이 많이 나게 됩니다.

   15년 전부터 생각 정리에 관련된 책들을 출간을 하고 다양한 저자의 강의, 워크샵을 진행하면서 도구, 시각화 프레임워크 종이 문구 툴셋을 만들고 있습니다.
   그러다 LLM을 도입한 정보 정리 도구 만다라트 + 피시본 + 저니맵 + BM 캔버스 같은 툴들이 한곳에 모인, 지식 시각화를 위한 지식체계 조각들을 만들고 연결할 준비를 한 사이트, 웹, 앱을 만들고 있습니다.
    1. 어떻게 동작하나요?
       텍스트를 넣으면 → 프레임워크로 그려줍니다
       뉴스 기사 한 편, 메모 한 줄, 책의 요약…
       키워드를 추출해 만다라트 / 피시본 / 캔버스 / 저니맵으로 시각화해 드립니다.
    2. AI와 함께 정리합니다
       무료 Gemini, 로컬 Ollama를 현재 사용하고 등 주변 컴퓨터에 설치된 Ollama의 다양한 AI 모델과 연결해 일종의 터널링 기반 LLM 개인화 클라우드 네트워크를 확장해서 복수의 내부, 외부 컴퓨터를 다양하게 사용해서 결과를 비교하기도 합니다. 같은 내용을 분해하고, 연결하고, 요약하고, 맥락을 파악해 줍니다.
    3. 관계도와 개념 연결도 그려줍니다
       비정형적인 글이어도, 키워드 사이의 유사성 / 대립 / 인과관계를 시각적으로 보여줍니다.
    4. 무엇을 위해 만들고 있나요?
       이 프로젝트는 다음과 같은 문제의식에서 시작됐습니다:

   생각은 있지만 정리가 안 될 때가 많다

   좋은 정보를 봐도 기억에 안 남는다

   요즘 시대엔 개념 사이의 연결, 흐름이 더 중요하다

   그래서 “개념을 그려주는 도구”,
   “아이디어를 잘게 쪼개서 다시 연결해주는 편집기”
   를 만들기로 했습니다.

   🚧 진행 중인 작업들
   만다라트 생성기 → 맥, 윈도우 네이티브 앱으로 공개 예정
   피시본, 매트릭스, 6블록 등 프레임워크 통합 기능 개발 중
   인디자인 자동 편집기, SVG 출력기 (Canva 호환)
   캐릭터/스토리 생성 & 관계도 도구 (소설/게임용)
   워크숍 & 북웨어 툴킷 → 협업/교육용 패키지 준비 중

   💡 어디에 쓸 수 있을까요?
   기획/전략 정리용: 비즈니스 모델 제너레이션, 고객 여정 맵, 가치 제안 정리
   교육/워크숍: 모둠 활동, 협업형 만다라트, 아이디어 확장용 도구
   출판/EPUB: 챕터 구조 생성, 시각형 목차, QR 연동 애니메이션
   콘텐츠 제작: 카드뉴스, 유튜브 쇼츠, 애니메이션 설명 영상

   개인 지식관리: AI + 시각화 기반 마크다운/위키 형태 정리

   어떤 사람들에게 어울릴까요?
   Notion이나 Obsidian 같은 툴에 만족하지 못한 분, 복잡한 기획서/보고서를 빠르게 정리해야 하는 분, 혼자든 팀이든 워크숍을 자주 여는 기획자/디자이너, 작은 회사에서 큰 전략을 시각화해보고 싶은 스타트업 대표, 텍스트 기반의 스토리/세계관을 시각적 지도로 표현하고 싶은 작가/개발자

   AI시대 워크샵에 빈 키워드로 시작하는 것보다 재미있게 시작하려는 목적으로 만들어봤습니다. 출판사도 새로운 시대에는 AI를 잘 활용해야 한다고 생각해서 워크북 + AI 서비스를 생각해서 꾸준히 작업중입니다.

   감사합니다.
"
"https://news.hada.io/topic?id=22000","샘 알트만의 스타트업 플레이북 한국어 번역","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        샘 알트만의 스타트업 플레이북 한국어 번역

   이 글이 10년전 글이라는게 신기할 정도로 지금 봐도 통렬하게 아프네요. 한국어로 번역된 것이 마침 무료 길래 다시 읽는데 팩폭으로 몸살 난 기분입니다.

   Startup Playbook 원문 보기 : https://playbook.samaltman.com/

   https://drive.google.com/file/d/…

   여기서 번호 입력 없이 그냥 보세요.

   고맙습니다. 센세!

   감사합니다~

   본인이 번역한 글을 마치 다른 사람이 해놓은 것처럼 공유하는 건 좀 짜치네요

   워워

   개인정보를 동의없이 수집해도 되나요?

   개인정보법 아주살짝 깔짝거려본 입장에서는.... 번호를 작성해서 제출하는 행위 자체는 수집, 이용에 동의한거라고 볼 수도 있습니다. 그런데 명시적으로 동의를 받던 안받던 개인정보 수집 목적, 보유 기간은 명시해야합니다. 안하면 빼박 위법이예요

   공유해주셔서 감사합니다. epub 파일을 맥의 기본 Books 앱에 넣었는데 페이지가 넘어가지가 않네요. 제 문제인지 문서의 문제인지 잘 모르겠습니다만.

   https://www.haebom.dev/playbook

   약간의 오해가 있어서 말씀 드립니다. 원래 이 번역은 2023년 년도에 진행 했던 것이며 전자책과 PDF로 보고 싶다고 하시는 분들이 있어 이번에 다른 전자책들을 정리 하는 김에 함께 올렸던 것입니다.

   개인 정보는 제가 별도로 수집 하는 것이 아니라 제가 사용했던 플랫폼인 래피드측에서 수집하고 있습니다. 아래.znjadon님 께서 말씀하신 것처럼 Google 드라이브에서도 그냥 다음 받을 수 있게 했습니다. 자동 메세지로 후기 요청이 간 점도 죄송합니다. 제가 래피드 설정을 제대로 못 해서 그런 것 같습니다.

   그냥 웹사이트를 공유 할 걸 그랬네요. 괜찮 오해와 스트레스를 드려 죄송합니다.

   쩝...어차피 공공재라고 생각하고 번역된 것 제공해주었으니 연락처랑 메일 작성했는데 후기까지 요청하는건...;아쉽네요.
   그리고 후기를 하루만에 쓰라는건...;;너무 하네요...
   혹시라도 의미없는 후기를 쌓을 목적이라면 적어드릴 수 있지만 의미있는 후기를 원하시면 그래도 일주일 뒤에 물어보는게 맞지 않나 싶습니다.

   번호를 수집하는거 매우 찝찝하긴 하네요. 어떻게 쓴다는 설명도 없고요.

   allwehear.com
   음성요약은 위사이트에서 보실 수 있습니다.

   이 글 쓰신 분이 쓰리블록스닷에이아이라는 회사의 안광섭 대표님이신가요?

   latpeed 링크의 505studio = mobeah (https://x.com/mobeahmi)인 것 같아요. 그냥 본인이 번역하셨다고 올렸어도 큰 문제 될 것 같진 않은데 말이죠

   외부 글을 본인 아닌 척 올리는 분들을 못 본 건 아닌데,
   이 경우에는 본인 아닌 척 개인정보를 수집하신 거라서 문제가 있어 보이는데요

   아, 제 의도는 (개인정보 수집을 포함하는) 링크의 내용이 아니라, 링크를 올린 부분에 대해서 말하려는 것이었어요. 설명해주셔서 감사합니다.

   앗, 네 ㅋㅋㅋ 저도 jhk0530님의 말을 반박하려는 의도는 아니었습니다.
   저도 사이트가 개인정보를 요구하더라도 본인이라고 밝히고 올리셨으면 문제 없을 거 같다는 생각인데,
   오히려 안 밝히고 올리시니까 더 꺼림칙하게 느껴지네요

   휴대폰 번호를 수집하는 이유가 뭘까요?

   보고싶지만.. 번호를 입력해야 받을 수 있다는 사실에 망설여지네요.
"
"https://news.hada.io/topic?id=22053","Claude Code 2주 사용 후기: 실전 경험과 파워 유저 관점의 팁","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Claude Code 2주 사용 후기: 실전 경험과 파워 유저 관점의 팁

     * Cursor와 Claude Code를 병행하며 대형 코드베이스에서 실제 개발 작업과 LLM 평가 컨설팅 등 다양한 업무 경험을 쌓음
     * Cursor는 편리한 UI/UX와 무제한 API 접근성 덕분에 파워유저들에게 각광받았으나 최근 강한 레이트 리밋 도입으로 사용자 경험이 급격히 제한됨
     * Claude Code의 Sonnet 4는 코드 이해와 편집, 대규모 맥락 처리 등에서 높은 신뢰성과 효율을 보여주며, Opus 4와의 병행 사용으로 난이도 높은 버그도 해결 가능함
     * 명령어 기반 CLI 환경, 서브에이전트 활용 등 파워유저를 위한 다양한 고급 기능이 숨어 있으며, 꾸준한 실험과 기능 탐색이 중요한 경험임
     * 아쉬운 점으로는 시각적 UI 부족, 느린 복사/붙여넣기, 다른 모델 활용 제약, 체크포인트 등 추가 개선 요청 사항이 남아 있음
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

Cursor에서 Claude Code로: 변화의 배경

     * 최근까지 Cursor는 무제한 API 사용과 직관적 Diff 리뷰 워크플로우로 개발자들에게 사랑받았음
     * 그래서 주로 Gumroad bounties와 AI 엔지니어링/LLM 평가 관련 자문 작업에서 코드를 생성하고, 코드베이스를 빠르게 이해하는 과정에서 활용함
     * 그러나 6월 중순 이후 갑작스러운 강력한 레이트 리밋이 도입되어 작업 효율이 급감, Cursor 사용의 장점이 크게 감소함
     * Sonnet 4, Opus 4, GPT-4.1, Gemini Pro 2.5 등 다양한 모델 중 실제로는 Sonnet 4와 Opus 4의 활용도가 가장 높았음
     * API 가격 부담, 속도 저하 등 한계로 인해 Claude Code Max 구독(월 200달러) 까지 고려하며 본격적인 전환을 시도함

Claude Code 실사용 후기

     * Python, Ruby, TypeScript 등 중대형 오픈소스 코드베이스(50M+ 토큰)에 Claude Code를 투입, 스펙과 테스트를 통한 피드백 루프를 경험
     * 초반에는 단순 명령 입력만 사용하다, 기초 명령어와 plan 모드를 익히며 더 깊은 활용법을 탐험하게 됨
          + 단순 명령 입력 → 명령어/계획 모드 익히기 → 명확한 커맨드 조합으로 자동화와 생산성 향상 실현
     * 문제 해결 과정을 마치 상담처럼 자유롭게 문제를 투입하여 Claude에 전체 맥락을 쏟아놓고, 필요시 Opus로 전환해 계획을 세우고(Plan mode), Sonnet 4로 주요 작업을 수행하는 혼합 전략 사용
     * Claude가 .claude 폴더 내 파일로 기록·정리하도록 시켜 context 관리 및 복사/붙여넣기 불편 해소 Plan 모드/Auto-edit 모드 병행 추천
          + 컨텍스트 관리: 압축(compaction) 대신 주기적 새 채팅 시작, 중요한 변경사항은 별도 파일에 메모하도록 유도

컨텍스트 관리 및 서브에이전트 활용

     * Claude Code는 컨텍스트 압축을 지원하지만, 느리고 효율이 떨어져 직접 요점 파일 생성 후 새 채팅 시작을 선호
     * Scratchpad와 같은 보조 파일에 변경사항, 메모, 히스토리 기록을 시켜 추후 분기(branch) 작업이나 세션 복구(/resume)시 활용
     * 서브에이전트: 코드베이스 내에서 여러 작업(검색, 분석 등)을 병렬 처리, 멀티스레딩 구조로 업무 분산 가능
          + 내부적으로 ToDo 리스트 기반의 멀티에이전트가 생성되어 문맥 관리에 도움을 줌

검색 및 명령어 활용

     * Cursor에서는 노멀/시맨틱 검색, agentic search 등 다양한 도구 사용 가능하며, 검색 속도도 빠름
     * 하지만 Claude Code의 검색은 느릴 수 있음. sub-agents 이용시 대규모 코드베이스 내 병렬 처리 가능
     * sub-agent와 task tool, /think, /ultrathink 등의 명령어 활용으로 대규모 리포지토리 탐색 및 분업화 실현
     * Shift + ? 단축키로 명령어 목록을 보고 신기능을 빠르게 확인하는 것이 중요
     * 터미널 명령(bash)은 !, headless 모드로도 실행 가능
     * 파일 태그(@파일), memorize 기능(사용자 맞춤 system prompt), CLAUDE.md 활용 등 고급 기능 다수 내장

Sonnet 4 vs Opus 4 비교 및 워크플로우 팁

     * Sonnet 4: 대다수 상황에서 더 빠르고, 긴 맥락+에이전트 작업에 강점. Python, 프론트엔드 작업에서 우위
     * Opus 4: Instructions가 여러 차례 누적되면 혼란스러워지는 경향 있음, 이럴 땐 파일로 기록하고 새 챗 시작 추천. Sonnet 4가 막힐 때 난이도 높은 버그 해결에 활용
     * 복잡한 문제는 Opus로 시작, 일반 코딩은 Sonnet으로 처리하는 하이브리드 운용 권장

맞춤 명령어와 기타 팁

     * /pr-comments, /review 등 커스텀 커맨드 지원, Github CLI 필요
     * 브랜치 변경 시 대화 재시작, main과의 diff 리뷰 등 유연한 워크플로우 구성 가능
     * Esc 두 번으로 대화 중 어디서든 포크 가능
     * /permissions 로 세션 전에 퍼미션 조정 가능
     * 용기가 있다면 claude --dangerously-skip-permissions 사용해보기
     * Cluade Code Pro TIPS 영상 추천

앞으로 시도해보고 싶은 것

     * 커스텀 명령어를 직접 정의하고 활용하는 방법을 실험해보고 싶음
     * Playwright server 등 MCP 서버를 활용해 프론트엔드 자동화 개발을 시도하고 싶음
          + Claude가 스크린샷을 찍고, 결과를 인식해 UI를 반복적으로 개선하는 피드백 루프 구축에 초점을 둘 계획임
     * how-i-bring-the-best-out-of-claude-code-part-2에서 제안하는 고급 활용법 전부 실습해볼 계획임
     * 프롬프트 최적화에 도전할 예정임
          + 평가 기준(rubric.md)을 명확히 정의하고, context가 담긴 파일(pmd 등)과 함께 프롬프트를 평가/개선하는 루프를 설계하고 싶음
          + Claude 인스턴스를 여러 개 두고, 한 인스턴스가 프롬프트로 결과를 내면 다른 인스턴스가 이를 평가 및 피드백 후 개선하는 방식(단일 or 멀티 에이전트 시스템)으로 진화시키는 구조를 계획 중임
          + 해당 방식은 Nirant의 포스트에서 영감을 받음
     * 여러 개의 Claude Code 인스턴스를 액션 로그를 통해 서로 소통하게 하는 멀티 에이전트 시스템을 구축해보고 싶음

결론 및 개선 요청

     * Cursor는 UI/UX 면에서 매우 강점이 있지만, Claude Code는 파워유저와 CLI 친화적 환경에서 생산성과 실험정신을 자극
     * 탐험적 학습과 실험이 많은 보상을 주는 툴로, Nerd/파워유저에게 강력히 추천

개선되기를 바라는 기능들

     * UI 통합(Claudia 참고)
     * Cursor와 같은 체크포인트 지원. Git이 있지만 Cursor의 방식이 너무 편함
     * 복사/붙여넣기 품질 개선
     * 다양한 모델 사용 지원

        Hacker News 의견

     * 사람들이 Claude Code에 대한 극찬을 하는 걸 볼 때마다, 마치 다 인플루언서들이거나 터미널과 Emacs, Vim 같은 전통적인 툴에 열광하는 팬들뿐이라는 느낌을 지울 수 없음. 나는 항상 Claude Code가 Cursor보다 훨씬 낫다는 댓글을 볼 때마다 실제로 구독해 대규모 TypeScript 코드베이스에 적용해 보는데, 과정은 오래 걸리고 학습 곡선도 높음. 결과는 결국 Cursor 내장 Claude와 똑같으면서 더 느리고 불명확해서 코드 리뷰도 어려움. 지금은 그냥, 댓글의 열성팬들은 모두 후원받고 있거나 이미 200달러를 냈으니 자기 선택을 정당화하는 느낌이 듦. 솔직히 Cursor가 훨씬 생산성 높았음. 18년차 프로그래머로 매일 코드 많이 쓰지만 Gemini 2.5 Pro와 Claude 4.0을 번갈아 활용하며, 외려 Cursor로 더 많은 걸 얻고 있음. 아직 단 한 명도 날 설득한 적이 없음. 실질적 이점이 안 보임. 이후엔
       생각이 달라질 수도 있겠지만, 지금은 전혀 못 느끼겠음
          + 대부분의 사람들이 소프트웨어 개발에서 진짜 어려운 부분이 뭔지 깊이 오해하고 있다고 생각함. 실제 업무의 대부분은 복잡한 알고리즘 개발이 아니라 기존 아이디어를 잘 엮어서 맞춰 붙이는 일임. 하지만 그 모든 건 사전 명세·설계·아키텍처 등 선행 작업 뒤에 오는 일임. AI로 이런 프로그램을 뚝딱 만들어내는 게 멋져 보이고 데모에서는 빠르게 “끝낸” 것처럼 느껴지지만, 진짜 문제는 30년 간 써야 하는 시스템을 품질 기준 맞춰 제대로 만드는 것임. 프로토타입이나 일회성 목적에는 최고지만 장기적인 내구성엔 한계가 큼
          + 이런 툴로 생산성을 극대화하려면 아주 짧고 빠른 피드백 루프가 핵심임. Cursor의 탭 자동완성 모델은 편집자가 뭔가를 하려는 걸 직감적으로 파악해서, 마치 미친 듯이 영리하게 감속기를 밟는 느낌임. 내가 머리 싸매고 매크로 프로그래밍할 필요 없이, 그냥 필요 없으면 Esc로 취소하면 되고 아니면 점진적으로 agentic 모드로 옮기면 됨. 완전한 에이전트 기반 에디터들은 15~30분씩 걸리고, 워크플로가 완전히 끊기는 문제도 있음. 결과물을 리뷰하는 게 일이고, 짧은 수용/거절 루프와는 비교도 안 되게 신경을 많이 씀. 네트워크 권한 줄지 오프라인으로 띄울지 고민도 커서, 유지보수/보안/신뢰성이 중요하지 않은 코드를 빨리 막 만들어야 할 때만 써볼 만함. 그 외엔 오히려 생산성이 떨어짐. 앞으로 나아지겠지만 현재는 확실히 Cursor에서 더 좋은 결과를
            낼 수 있음
          + 나도 예전엔 그렇게 느꼈지만 최근 Claude Code를 실제로 써보니 Cursor보다 훨씬 낫다고 느낌. 왜 그런지는 잘 모르겠으나 Claude가 전체 구조를 더 잘 파악하고 불필요한 수정을 잘 피하는 듯함. 물론 직접 방향을 잡아줘야 할 때도 있지만 효율성이 훨씬 더 높음. 특징 중 하나는 보통 한 번에 하나의 파일만 딱 보여주니까 리뷰하기 훨씬 쉬움. Cursor는 여러 파일을 한꺼번에 열고, 변화량이 많아서 빠르게 파악하기 힘듦. 참고로 난 VSCode 터미널 창에서 Claude Code 확장 프로그램을 이용함. Claude가 바꿔줄 파일 탭을 열어 변경안을 제안함
          + 아직 사람들이 모르는 건 Cursor는 하나의 완성된 제품이 아니라, 모든 툴들이 빨리 따라잡으려고 추가하는 기능 묶음이라는 점임. 진짜 교훈은, 딥 인터페이스 말고도 각자 원하는 편집기로 베스트 인 클래스 에이전트 솔루션을 결합하려는 전략이 있음. 이런 경험들이 결국 “베스트 프랙티스”로 응집돼서, 사람들이 자기 편집기나 IDE에서 자연스럽게 적용하고, 이런 vscode 포크들은 다 사라질 것임
          + 한 달도 안 되는 기간 17달러 요금제로 썼는데, 신기함과 답답함이 반반임. 러스트로 8천 줄, 마크다운으로 1만 2천 줄을 썼고, 작업명세와 구체적 태스크를 마치 테스트하네스와 같은 방식으로 분리해서 인공지능과 상호작용함. 마법이 VC 보조금 때문인지 뭔지 모르겠지만, 러스트가 마치 스크립트 언어처럼 느껴질 정도였음. (참고: GitHub 저장소는 ‘knowseams’임)
     * 내가 AI에서 제일 좋은 점은, 귀찮을 때 “이거해줘” 하면 된다는 점임. 결과물이 좋든 별로든 상관없음. 일단 시작점 만들어줌
          + LLM 덕분에 백지 공포가 없어짐. 복잡한 맥락을 다시 머릿속에서 되살릴 필요 없이 “우리 뭘 하고 있었지?”, “이 코드는 뭐였지?”라고 물어보면 AI가 금방 설명해주고, 곧바로 다시 몰입할 수 있음. 루버덕 디버깅이나 반복적 사소한 작업(yak shaving)까지 엄청 빠르게 처리해줘서 진짜 유용함. Slack, Notion, Linear 등과도 연동해 써서 내겐 태스크/프로젝트 관리 툴임
          + 스스로 직접 하고 싶을 때도, AI에게 계획을 짜달라고 해서 마크다운에 남겨둠. 오늘도 refactor 계획을 부탁했는데, 40개 파일로 된 프로토타입 코드 블록을 아래에서부터 구조변경하는 식으로 잘못 접근함. 만약 그 방향대로 실수했다면 디버깅에 엄청난 시간이 걸렸을 듯. 그래도 공격 포인트를 제공해주고, 계획도 한 시간만에 고쳐서 적용함. 내가 혼자 했다면 복잡함에 질려 시작도 못했거나, 문서화 반복하다 포기했을 것 같음
          + 하루 끝엔 더 이상 집중이 안 돼서 쓰거나 되돌리는 양이 비슷해질 때 AI에게 핸들을 맡겨 한숨 돌릴 수 있음. 작은 이슈는 diff만 슬쩍 보면 되고, 어려운 이슈도 구체적으로 뭐가 문제인지 알면 AI를 방향 잡아주며 설득하면 됨. 작업의 40~60% 정도 완성되면 직접 이어받아 finish하는 편임. 평소에는 내가 날카로운 시간대에 집중해서 직접 사고하며 개발하고, 남는 야근이나 반복잡무는 AI에게 맡겨 익일 준비나 좀 더 고차원 글쓰기·설계를 주로 함
          + 난 그냥 산책하고 커피를 마심. 인간문제는 인간적인 방식으로 해결하는 게 좀 더 자연스러운 느낌임
     * Claude Code는 뭐라고 설명하기 힘들 정도임. 사용한 뒤로는 아예 직업을 바꾼 것 같은 기분까지 들었음. 기존에도 Claude를 전면 워크플로우에 도입했지만, Claude Code는 그야말로 “스테로이드” 급임. 아직 안 써봤으면 무조건 추천임. 진짜로 주니어 엔지니어와 함께 일하는 느낌을 처음 받았음
          + 내 경험은 정반대임. 무언가 지시하면 몇 분 걸려 뭔가 결과를 주는데, 실제로는 앱이 망가져 디버깅하다 보면 완전히 엉뚱하게 작업해서 결국 다 버림. 그럼에도 Claude를 계속 잡는 건 다른 사람들처럼 잘만 굴러가면 너무 좋기 때문임. 현실은 부일러플레이트만 뽑고, 직접 디버깅을 꽤 해야 하며, 최악이면 한 시간과 토큰만 날림
          + 오늘 처음 회사에서 써봤는데 Cursor보다 압도적으로 혁신적인 변화임. 같은 파운데이션 모델을 씀에도 완전히 다른 경험임. 한 달 전에는 AI 때문에 일이 더 느렸는데, 오늘 Claude Code로 20분 만에 처리가 끝났고, API 사용료도 10달러가 채 안 듦. 문맥 관리에 신경 쓸 일이 매우 적었고, Claude Code는 스스로 필요한 맥락을 찾아넣어 훨씬 오랜 시간 동안 생산적으로 작업함. Cursor의 agent 모드는 3~5분짜리 작업까지만 쓸만하지만 Claude Code는 10분 넘는 작업에서도 자기 길을 잃지 않고 꾸준히 진전시킴. 도구 사용도 탁월하고, 루프에 잘 갇히지 않는 점이 놀라움
          + “주니어 엔지니어와 일하는 듯하다”고 했는데, 내가 느낀 건 오히려 내가 부하직원이고 Claude가 상사 같음. “내가 이런 멋진 거 해냈어요!”라고 자랑하면 “근데 그건 내가 시킨 게 아니야…” 라고 반응하는 느낌임
          + 어떤 작업, 언어, 도메인에서 활용했는지 더 구체적으로 설명해줄 수 있음? 모두 케이스가 너무 달라서 궁금함
          + 나도 같은 경험임. Claude는 단순 주니어 그 이상임. 옵션 제안이나, 결정을 위한 추천, 그리고 트레이드오프 시각화를 너무 잘해줌
     * 실제로 Claude Code로 앱이나 라이브러리를 만드는 워크스루 사례가 많지 않음? 나는 단순 “신기함”을 말하는 글보다 진짜 해당 툴로 실전 개발하는 모습을 보고 싶음. 그런 사례 모음이 있음 정말 좋겠음
          + 뭔가 전체적으로 이 상황이 조금 이상하게 느껴짐. Claude Code 자체는 분명 좋고, 훨씬 빠르게 문서나 스택오버플로를 찾는 데 쓸 수 있음. 하지만 만약 과장된 소문이 사실이라면, 이런 툴로 엄청난 속도로 소프트웨어 혁신이 일어나야 하지 않냐는 의문이 듦. Stripe CEO는 AI 툴이 생산성 100배 증가라 했는데, 3~4개월 지났으면 지금쯤 Stripe는 로켓 발사해야 하지 않음? Microsoft도 AI코딩 올인하였다는데 Teams는 왜 여전히 별로 같지? 1년 넘게 이 툴이 혁신이라는데, 실제 현실은 3~4년 전과 큰 차이가 없는 듯함
          + 최근 눈에 띄는 두 가지 트렌드는 (1) 비숙련자가 사소한 프로젝트에 AI를 쓰는 것, (2) 개발자가 전체 앱 구조와 파일, 인터페이스, 테크 스택, 테스트 프레임워크까지 빽빽하게 미리 명세해두고 LLM에 세밀하게 핸들링을 시켜 괜찮은 결과를 겨우 끌어내는 것임 YouTube 예시. PR/유튜브 등에서 들려오는 80~99%의 얘기는 사실상 첫 번째 그룹에서 나옴. 생산성 상승을 느끼는 건, LLM과 대화·문서화·유도·수정하는 일이 직접 개발하는 것보다 덜 피곤하게 느껴져서임. 시간이나 총노력은 똑같아도 기대치를 덜기 때문임
          + 유튜브에서 진짜 생산성 부스트를 내는 현장 스트림/사례를 찾고 있는데, “와 진짜 빠르다!”고 느낀 케이스는 못 봄
          + 찬반 양극단 의견이 많지만 정작 대다수는 조용히 각자 커밋하고 있음(이 말 자체가 아이러니). 내 경우, 작업에 따라 1.5~10배까지 빨라짐을 реально 체감함. 가장 큰 이점은 순수 창조적·일회성·보일러플레이트·리팩토링류 작업에서 인지적 부하가 크게 줄어서, 일관된 성과를 유지한다는 점임. 여전히 “손코딩”도 많이 하고, 거의 모든 라인을 끝까지 리뷰함. 몇 시간째 혼자 돌려놓는 일은 Nightmare임. 실제로 10년 넘게 유지 중인 프로덕션 앱에서도 어딘가 블로그에 홍보할 시간도 없음. 반면 아주 슬림한 조직이라 전체 시스템 문맥도 내 손에 쥐어져 있어 문제를 더 빠르게 파악할 수 있는 면도 큼. 자기 효율성 확보가 중요한 환경에선 확실히 근본을 키움. 대규모 조직에선 이런 경험 얻기 힘듦
     * 내 경험상 Claude.md라는 마크다운 파일을 각 코드폴더 루트에 둬서, 파이프라인처럼 미니멀한 룰셋을 추가함. 테스트 생성과 배치는 지정된 폴더 및 방식에 꼭 맞게 하고, 디버그 파일 생성을 막음. 새로운 클래스나 구조의 난립을 막고, 꼭 필요한 경우 외엔 재사용하도록 룰을 걸어둠. 프롬프트도 길게 적지 않고, 대개 불확실한 부분만 계획서를 작성함. LLM의 지식범위를 벗어난 신상 이슈에도 큰 입력은 최소한으로만 전달함. 이런 방식으로 1 input–1 output(뎁스까지 다 적용) 결과를 일관성 있게 얻음. 최근엔 Claude Code 대신 CLI 모드로 Opus 등 다른 대형 모델을 더 저렴하게 써서 옮겨갔음. CLI가 진짜 파워임. 60~70개의 에이전트 스트림을 동시에 돌리고 있고, 2억 토큰 규모(react/typescript/golang) 코드베이스도 무리 없이 관리. 단 한두 번 정도만 추가 지시한 경험
          + 에이전트 스트림으로 어떤 걸 운영하는지 리스트 가능함? 매우 궁금함
          + Anthropic 외에 어떤 모델을 쓰는지 알고 싶음. Kimi K2는 써봤는데, 내 사용에 별로임
          + ""에이전트 스트림""이란 게 무엇을 의미하는지 궁금함. 60~70개를 어떻게 관리하는지, 인지적 부담이 상상도 안 됨
     * 가끔 Claude Code로 특정 작업을 할 때 생산성이 극적으로 오름. Slash command를 활용해서 이전 대화를 slash command로 만드는 방식을 추천함. 이렇게 하면 점점 더 활용 가능한 프리미티브 명령어 세트를 쌓아갈 수 있음. 내 사례는 GitHub에 올려둠 make-command.md, improve-command.md
          + 비결정론적 블랙박스를 프로그래밍하려 든다는 점이 정말 대단하다고 생각함. 용기 진짜 대단
     * PSA(공익 정보): 이 저장소로 어떤 모델과도 Claude Code를 연동할 수 있음. 최신 Kimi-K2가 꽤 잘 동작한다고 알려짐
          + 나도 Kimi-K2 써봤는데, Sonnet/Opus 4.0보다는 성능이 떨어지고, tool calling은 Gemini 2.5 Pro보다는 나은 편임. Claude Max(월 100~200달러)가 부담스러우면 강추함. 모델 자체가 군더더기 없이 매우 간단해서 좋음. Anthropic도 차라리 Claude Code 오픈소스화하면 cli coding agent계의 VSCode가 될 수 있을 듯. 그리고 opencode도 추천함. 모든 모델 네이티브 지원 및 Claude Code 유사 기능 제공
          + 여러 모델로 쓸 거라면 sst/opencode를 그냥 추천함(나도 Claude Pro로 씀)
          + 참고로 CC를 아직 못 써본 사람들은 – CC 클라이언트를 npm으로 받아 무료로 쓸 수 있음
     * Claude Code, local LLM, Continue, VSCode로 간단한 파이썬 앱을 “vibe coding” 하며 놀다가, Claude 무료티어를 알게 되어 진행중이던 코드와 LLM 결과물을 넣어봤음. 오류와 업데이트를 한 번에 정확히 정리·수정해줘서 신남! 그래서 다음 단계로 pygame 기반 2d 게임(마닉 마이너 스타일) 프로젝트의 스펙과 프롬프트를 ChatGPT로 짜고 Claude에 넣어보니, Claude가 계속 없는 메서드를 참조하거나 코드베이스 버전 차이 운운하며 헛소리를 함. 라인 넘버와 주변 코드로 콕 찝어줘도 여전히 gaslighting 중. 어떻게 해결하면 좋을지, 완벽은 바라지 않지만 local LLM 때랑 비슷한 한계에 막힌 느낌임. 건강이 안 좋아 간헐적으로 하는 거라 조언 부탁함
          + “모호한 인터페이스와 숨어있는 전제들이 가득한 코드 지옥”에 빠졌을 확률이 큼. 이럴 땐 차라리 기존 ChatGPT 결과물 모두를 요약해, 현재 게임이 무엇을 하는지/모든 기능을 깊게 리스트업한 다음, 그 문서를 Claude에 넣고 요구사항을 처음부터 다시 break down하면 훨씬 깔끔한 결과를 얻을 수 있음. Claude는 zero-shot으로도 훌륭한 샘플을 낼 수 있고, 최악이어도 반복적으로 자체 브러시 업을 할 수 있음. 여전히 Claude가 터무니없는 기능을 만든다면 context7 MCP 서버를 설치해서 Claude에 context7 이용을 명확히 요구할 것 추천
          + 이건 LLM 기술의 근본적 한계임. 확률적으로 “가장 그럴듯한” 토큰 시퀀스를 출력하지만, “그럴듯함”과 “정확함”이 일치하지 않으면 답이 없음. 각 LLM마다 이 “그럴듯함” 기준은 학습/파인튜닝마다 다름
     * 기본적인 설정 이후, 이 툴이 잘 돌아가도록 하기 위해 어떤 추가적인 방법을 고민하는지 궁금함. 즉, 문맥/context와 코드베이스 구성에서 툴이 스스로 정확히 방향을 잡게 하는 실무적 방법들이 뭐가 있는지 노하우 공유 부탁함. 내 생각 정리는 이 글에 남겨둠. 더 좋은 방법론이 계속 나올 거라고 생각함
     * Claude Code로는 여러 알파(Alpha)를 얻고 있지만, 팀 전체로 확장하는 게 고민임. 팀원들이나 내가 관리하는 사람들이 효과적으로 Claude Code를 활용할 수 있게 실무 팁이나 베스트 프랙티스 공유하는 방법이 있을지 궁금함
"
"https://news.hada.io/topic?id=21981","전 세계 음식 배달 시장의 90% 이상을 다섯 개 기업이 지배함","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  전 세계 음식 배달 시장의 90% 이상을 다섯 개 기업이 지배함

     * 최근 음식 배달 시장의 통합이 빠르게 진행되며, 전 세계 거래액의 90% 이상이 Meituan, DoorDash, Uber, Prosus, Delivery Hero(배달의 민족) 다섯 개 대형 기업에 집중됨
     * Prosus와 DoorDash의 대형 인수 사례는 이 변화의 규모와 속도를 보여줌
     * 통합으로 인해 플랫폼 참여자(소비자, 식당, 배달기사) 가 각 기업 생태계에 더 깊이 포함됨
     * 식당과 배달기사에게는 수수료 및 수익 구조 악화 등 부정적 영향이 미칠 우려가 있음
     * 혁신 여지는 남아 있으며 로봇 배달과 화이트 라벨 서비스 등이 새로운 투자 기회로 부상함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

음식 배달 시장의 대규모 통합 현상

     * 최근 Prosus의 실적을 확인하면서, 음식 배달 시장에서 아무도 주목하지 않는 큰 흐름을 발견함
     * 이제 음식 배달 분야에서 통합이 빠르게 진행되고 있으며, 그 결과 전 세계 음식 배달 거래의 90% 이상이 5개 기업에 집중
     * 불과 몇 년 전까지만 해도 수많은 신생 기업과 경쟁이 치열했지만, 투자자금은 AI 스타트업으로 몰리고, 신규 참여자는 거의 사라진 상태
     * 이 과정에서 대형 인수합병(M&A)이 계속되면서 음식 배달 시장은 극도로 집중화되고 있음

최근 주요 인수합병 사례와 시장 구조 변화

     * 2025년 2월, Prosus는 Just Eat Takeaway를 3개월 이동평균 대비 49% 프리미엄에 인수함
     * 2025년 5월, DoorDash도 Deliveroo를 40% 프리미엄에 인수함
     * Prosus의 Just Eat Takeaway 인수, DoorDash의 Deliveroo 인수를 반영하면,
       Meituan, DoorDash, Uber, Prosus, Delivery Hero 등 5개 기업이 세계 음식 배달 시장 총 거래액(GTV)의 90% 이상을 차지하게 됨

2차적 영향(Second Order Effect)

  기업 측면

     * 통합을 통해 고객, 배달기사, 식당 모두 특정 플랫폼 생태계에 속하게 됨
     * 네트워크 효과로 인해 소비자는 더 빠르고 편리한 서비스를 경험하고, 식당은 더 많은 고객에게 접근 가능해짐
     * 경쟁이 줄어들면서 기업은 수익성을 높일 여지가 생김
     * 이용자 선택지가 줄어들고, 기업이 시장 참가자(소비자·식당·배달기사)보다 더 우위를 점할 수 있음

  배달기사 영향

     * 배달기사가 가장 먼저 부정적 영향을 받는 대상임
     * 음식 배달은 진입 장벽이 낮고, 자동차 없이 자전거만 있으면 가능해 기존의 라이드헤일링보다 접근성이 높음
     * 그러나 경제가 약화되거나 규제가 부족할수록, 플랫폼이 노동자에게 불리한 조건을 적용할 가능성이 커짐
     * 사실상 대안보다 덜 힘들기 때문에 종사자가 남아있는 구조임

  식당의 영향

     * 식당은 두 번째 피해자가 될 수 있음
     * 대다수 식당은 주문 한 건 당 15~30%의 수수료를 내고, 이는 수익성에 악영향을 줌
     * 자체 배달 역량이 없는 소규모 식당이 대부분이며, Domino’s Pizza조차 Uber와 협력해야 했음
     * 2024년 기준, Uber는 Domino’s 전체 매출의 3%를 차지함

  고객 영향

     * 고객은 가장 나중에 변화를 체감할 가능성이 높음
     * 음식 배달이 여전히 집밥, 테이크아웃, 외식 등에 비해 편리함을 제공하는 구조임
     * 하지만 경쟁이 줄면서 할인, 프로모션 감소 및 배달비·메뉴 가격 인상이 이뤄질 전망임

투자 관점 및 업계 혁신

   음식 배달 업계는 여전히 혁신 여지가 많은 분야임

  화이트 라벨 서비스와 로봇 배달

     * Uber는 화이트 라벨 딜리버리 서비스인 Uber Direct를 도입, 소매점도 빠른 배송 가능
     * DoorDash와 Uber는 Coco와 제휴해 로봇 배달 실험을 이미 여러 도시에서 진행 중임
     * 이런 기술의 발전은 인력 비용을 줄이고 수익성을 더욱 높일 기회를 제공함

  투자 매력과 밸류에이션 비교

     * DoorDash의 주가매출비율(PSR)은 9, 총이익률은 50% 로 상당히 높은 수준임
     * Gitlab 등 소프트웨어 기업과 비교 시, 음식 배달 기업에 대한 투자는 성장성과 기존 관행의 차이를 고려할 필요가 있음
     * Prosus와 Uber는 DoorDash에 비해 밸류에이션이 낮고, 더 복합적인 사업 구조를 갖고 있음

  향후 업계 전망

     * 업계가 추가 혁신 및 통합을 통해 성장 가능성이 크며, 신규 진입자 인수나 기술 혁신이 이루어질 수 있음
     * 단, 투자 시점에서는 적절한 밸류에이션이 중요함
     * Prosus, Uber 등에 대한 추가 분석 예정

        Hacker News 의견

     * 이 이야기는 거의 모든 산업에서 기술 기업들이 어떻게 시장을 장악했는지에 대한 전형적인 사례임을 말하고 싶음. 내가 대부분의 기술 기업을 사회 전체에 있어서 순손실이라고 보는 이유 중 하나도 이것임. 대부분의 기술 기업이 노리는 목표는 큰 독점적 지위를 갖는 것이고 (Thiel 같은 테크 리더들도 이를 인정한 바 있음) 인터넷이 이런 현상을 가능하게 해줬음. 인터넷 이전에도 비슷한 현상은 있었지만 훨씬 작은 지역 단위에 국한되었었음. 요즘 점점 심각해지는 사회 문제들 중 많은 부분은, 기술이 가능하게 한 극심한 부와 권력의 집중에서 비롯된다고 생각함
          + 조지 칼린이 정치인에 대해 했던 농담이 떠오름. 핵심은 “문제는 정치인이 아니라, 그들을 뽑은 사람들”이라는 것임. 정치인은 우리와 똑같은 사람이고, 다수의 선택이 만든 인센티브에 반응하는 존재임. 기술 기업들도 그냥 일반 기업일 뿐임. 굉장히 많은 사람들이 이들의 서비스와 제품을 실제로 원하고 있음. 조지 칼린이 살아있었다면, “문제는 테크 기업이 아니라, 그 고객들”이라고 했을 것임. 사실 나는 기술 기업들이 사회에 순손실이라는 주장에 동의하지 않음. Amazon이 수십억을 버는 이유는 모든 사람들이 빠르고 저렴한 서비스를 원하기 때문임. Google이 수십억을 버는 것도 인간의 지식에 무료로 접근할 수 있게 해주기 때문임. 만약 기술이나 정치에 대해 지나치게 비관적이면, 결국 인간 집단이 내리는 대부분의 결정에 실망할 수밖에 없을
            것임. 개인적으로 세상은 늘 혼란스럽고 복잡할 테지만, 그 안에서 좋은 일도 많이 생길 거라는 점을 받아들이게 되었음
          + 레스토랑 음식 배달도 굉장히 경쟁이 치열한 산업임을 감안하면, 이런 이슈를 이 쪽 산업에서 제기하는 게 어색하다고 생각함. 대부분 성숙한 산업은 결국 몇몇 기업만 남음
          + 최근 동일한 결론에 도달했음. 이런 현상을 막으려면 경제 시스템 자체를 바꿔야 한다고 생각하게 됨. 기술 분야는 실제 세계보다 ‘승자 독식’이 훨씬 극단적으로 일어남. 소수 기업이 전 세계 시장을 장악해가는 모습을 보면 정말 걱정스러움
          + 그 전에도 워렌 버핏이 ‘경쟁 장벽(moat)’을 잘 구축한 회사에만 투자하겠다고 자주 언급했었음
          + 결국 모든 성숙한 시장은 3~5개 기업이 시장을 지배하게 됨. 나는 이것을 문제로 보지 않음. 오히려 기술 덕분에 시장의 편의성이 커지고, 정부가 가격을 강제 규제하지 않는 한 가격 경쟁도 촉진된다고 생각함. 예를 들어 호텔 시장에 정부가 개입해서 Airbnb를 막으니 가격이 오름. Uber와 Lyft가 없었다면, 택시 노조는 업계를 독점하고 구식 시스템을 고수했을 것임
     * 레스토랑 음식 배달은 사실 사치재라고 생각함. 그냥 ‘내 브리또를 위한 개인 택시’같은 서비스임에도 많은 사람들이 무감각하게 돈을 씀
          + 요즘 Gen Z가 음식 배달 서비스를 일주일에 몇 번, 심지어 매일 쓰는 걸 보고 충격을 받았음. 젊은 동료들이 경제적 어려움을 토로하면서도 거의 매일 DoorDash로 저녁을 시켜먹는 걸 볼 때마다 말문을 닫게 됨. 물론 모든 Gen Z가 그런 건 아님. 하지만 사치성 서비스를 기본 비용으로 착각하게 된 일부 집단이 존재한다고 느낌
          + 2000년대 초반에 피자 배달을 했을 때는 배달료가 $1이고 팁이 $2~3 수준이었음. 꽤 일반적인 사람들이 배달 음식을 시켰음. 당시 시간당 약 $20씩 벌었고(인플레이션 반영 전, 시간당 4번 배달 + 시급 $5), 지금도 드라이버들은 비슷한 수준임. 그런데도 현재는 식당들이 추가 비용을 부담하고, 배달 업체들은 적자를 감수하면서 운영 중임. 지난 20년간 뭔가 상당히 이상한 변화가 있었음
          + 레스토랑 배달이 사치라는 데 동의함. 비부패식품이나 올바르게 포장된 단거리 배송(예: 유제품, 냉동식품)이야말로 ‘비사치재화’로 전환될 수 있다고 생각함. 10~20가구가 주요 식료품을 주 2~3회 묶어서 받아본다면, 모두가 개별 주문하는 것보다 훨씬 저렴해질 수 있음. 효율화만 잘하면 대량 운송과 냉장차 사용도 충분히 경제적임. 다만 소비자가 이런 방식에 동참할 수 있는 마인드셋을 갖는 것이 가장 큰 과제임
          + 기사 제목을 보고 전혀 다른 얘기인 줄 알았음. 읽고 나니 “내가 직접 할 수 있는 서비스까지 독점인가?”라는 생각이 들었음. 마치 수도꼭지에서 물 따라주는 서비스가 독점인 걸 걱정하는 느낌임
          + 처음 5~10년간은 가격이 지나치게 저렴했음. 이제는 VC 보조금이 사라져서 서비스 비용이 예전보다 5배는 올랐기 때문에, 아무리 내가 이런 회사에서 일하더라도 이제는 해당 앱을 쓰고 싶지 않음
     * 기사 제목이 음식 배달 공급망을 일컫는 줄 알고 순간 겁먹었음. 다행히 레스토랑→소비자 배달 얘기였음. 그쪽은 그렇게 크리티컬하지 않다고 생각하지만, 만약 음식 공급망 전체가 몇 개 회사에 완전히 통합됐다면 큰 문제임
          + 실제로 식품 공급망은 이미 거대 기업들이 지배함. 음식 배달보다는 실제 식품 생산/공급 부분이 훨씬 더 중요한 문제라고 생각함. 검색만 해도 닭고기는 4개 회사, 돼지고기는 약 70%, 소고기는 거의 75%가 소수 대기업이 지배함을 알 수 있음
          + 글로벌 푸드서비스 산업에도 거대 기업들이 존재함. Bidfood, Sysco, PFG 같은 회사들이 그 예임. 이런 회사들이 감옥, 학교, 병원 식당부터 고급 라운지, 고급 레스토랑까지 거의 모든 곳에 공급을 담당하고 있음
          + 레스토랑 식자재 유통은 더 심하게 통합됨. 예를 들어 중서부에서는 해산물 공급처가 1~2군데뿐임. 나도 레스토랑에서 일하고, 동시에 이 유통사에 물건을 공급하는 입장이라서 내부 사정을 잘 알고 있음. 장점과 단점이 모두 있음
          + 지난 10년간 해당 분야의 프로모션 관리 툴을 개발하면서 이런 대형 유통회사들을 매일 상대해 옴. 많은 인수합병이 일어나긴 하지만, 미국 내에서도 수백개의 유통사들이 여전히 존재함. 그러나 전체 물량 대부분은 대형 유통사들 손에서 움직임. 인수합병이 워낙 많아서, 시스템 내에서 유통사를 합치는 기능을 매우 자주 추가해야 했음. 제조사도 마찬가지였음
     * 음식 배달업체들이 Amazon 등 흔히 비난 받는 메가테크 기업보다 훨씬 더 ‘악’에 가까워졌다고 생각함. 이들은 어느 쪽에도 실질적인 이득을 주지 않고, 점점 더 나쁜 고객 서비스를 제공하고 있음. 하지만 지금 뉴햄프셔의 호숫가 별장에서 이 글을 쓰고 있으니, 대도시 밖 세상은 다르다는 걸 실감함. 여기 Doordash에 등록된 업체는 한 군데 밖에 없음
          + 그렇다면 왜 많은 사람들이 이런 서비스에 수십억 달러를 쓰는지 설명해줬으면 좋겠음
     * 음식 산업도 오픈소스 혁신이 필요한 시점임. 각 식당이 자체 메뉴 허브(인스타그램 계정처럼)를 만들고, 결제 시스템을 직접 붙여 배달과 주문을 자체적으로 관리할 수 있으면, 식당과 단골 모두에게 이득임
          + 몇 년 전 내가 일하던 식당도 자체 웹사이트와 앱으로 주문을 받았음. 그런데도 Ubereats 등 플랫폼 업체들이 15% 추가 요금을 붙이며 압도적으로 주문을 가져감. 단골 고객에게 직접 주문하면 돈을 아낄 수 있다고 알려줘도, 다양한 식당을 한 번에 고르고 주문할 수 있는 편리함은 무엇과도 바꿀 수 없는 듯함
          + 음식 배달 앱의 주 타겟은 10~20개의 로컬 식당 주문을 직접 관리하고 싶은 사람이 아니라, ‘그냥 빠르게 뭔가 주문해서 먹고 싶은’ 사람이 대부분임. 웹사이트 들어가고 앱 설치하고 추가 주문 절차 거쳐 10% 아끼는 것보다, 그냥 앱에서 한방에 고르는 게 더 중요함
          + 이미 이런 주문 시스템은 존재함(오픈소스는 아니지만 충분히 저렴해서 식당 입장에선 쓸 만함). 문제는 사용자 습관을 바꾸는 것임. 예전에는 직접 식당에 주문하던 고객들이 이제는 배달 앱 켜고 뭘 먹을지 고르는 게 기본이 되어버렸음. 아무리 좋은 자체 주문 시스템을 만들어도, 아무도 써주지 않으면 소용이 없음
          + 우리 동네 단골 태국 음식점은 자체 웹사이트로 주문 받고, 자체 직원이 배달까지 하며 매장 내 가격과 동일하게 운영함. 이들이 쓰는 플랫폼은 mobihq.com임
          + 믿기 어렵겠지만, 2010년대에 성행하던 온디맨드 다중 플랫폼 시장이 곧 또 한 번의 혁신적 ‘디스럽션’의 기로에 서고 있다고 생각함
     * 이런 산업이 결국 불법/비정규/미등록 이주노동자에 의해 유지되고 있다는 점을 지적하고 싶음. 나중에 사회·경제적으로 반드시 영향이 있을 것임
     * “몇 개 회사가 이제 시장의 X%를 점유한다”는 식의 통계를 자주 봄. 표면적으로는 큰 문제가 뭔지 잘 모르겠음. 음식 배달 서비스가 나왔을 때는 정말 혁신적이었고, 초기엔 저렴했음. 소규모 업체들이 몰려 들어와 경쟁했으나, 서비스가 성숙하면서 큰 회사 중심으로 통합되고, 생존을 위해 인수합병이 반복됨. 글로벌 시장 상황, 물가, 연료비, 식자재 가격 등도 배달 가격 상승에 영향을 줬음. 이제는 자본력과 정부 보조금을 바탕으로 글로벌 시장을 운영할 수 있는 소수 배달 회사를 남겼음. 참고로 Deliveroo는 호주에서 고비용 구조 때문에 철수해야 했음. 호주 정부가 긱 이코노미 종사자의 권리 보장을 추진했는데, 이 비용은 대부분의 소규모 업체가 감당 못함
     * 웃기지 않음? Lina Khan과 미 정부는 테크 기업들의 인수합병은 엄격하게 규제해서, 이제 모든 회사가 편법적으로 움직이고, Windsurf 직원들은 피해만 봤음. 한편 식품 공급망이 몇 개 회사에 완전히 장악되는 것은 이상하게 ‘당연’시되고 있음. 미 연방정부는 4~5층짜리 본사 리노베이션에 25억 달러를 쓰면서, 중국은 36km짜리 다리를 20억 달러도 안 써서 건설했음. 미국이 점점 서방 로마제국이 몰락하던 때처럼 각 계파의 이해관계에만 빠져 진짜 개혁은 불가능한 존재가 되고 있는 느낌임
          + Lina Khan의 FTC가 Kroger-Albertsons 합병도 실제로 막았음. 식품 공급망에 대한 정부 개입이 완전히 없는 건 아님
     * 만약 5개 업체가 존재하면 충분한 경쟁이 있는 것 아님? 1~2개 업체만 남으면 더 문제가 크다고 생각함. 그렇다면 경쟁이라 할 수 있으려면 업체가 몇 개쯤 되어야 함?
          + 좋은 질문임! 반독점 문제에서 가장 핵심은 ‘시장 정의’임. 시장을 좁게 정의하면 기업 합병이 곧 독점이 되고, 넓게 정의하면 영향력이 희석됨. 미국에서는 이 시장 정의 문제에 대해 판사 앞에서 기업과 규제 당국(FTC 또는 DOJ)이 각각 주장함. 예를 들어 “앱 기반 레스토랑 배달 서비스”만을 시장으로 보면 5개 회사면 꽤 집중된 시장임(HHI로 집중도를 계산할 수 있음). 하지만 배달 앱만이 아니라 ‘직접 식당 방문하기’, ‘집에서 직접 요리하기’, ‘식당 자체 배달’ 등도 포함하면 이들 점유율은 급격히 작아짐. 그래서 규제당국은 시장을 좁게 정의하려 하고, 업체는 넓게 보려 함. Hacker News에서 “X는 독점이다”라는 강한 주장은 대부분 시장 정의가 결여되어 있음. 다행히 판사들은 HN 유저보다 더 정확하게 판단하고 있음
          + 한 도시에서 5개 업체라면 정말 좋은 환경임. 하지만 지금은 전 세계적으로 5개뿐이고, 실제로 대부분의 시장에는 1~3개만 존재함
          + 실제로 대부분의 지역에는 1~2개 업체만 있음. 미국에서는 DoorDash와 Uber Eats가 시장을 점령하고 있음. 나머지 3개(Meituan, Prosus, Delivery Hero)는 다른 지역에서만 유명함
     * 핸드폰 시장은 두 업체가 90%를 장악하고 있음. 그에 비해 레스토랑 음식 배달 시장은 충분히 건전함
          + 혹시 폰 시장이 아니라 ""App Store"" 시장을 말하는 것 아님? 곧 Harmony OS가 중국과 다른 시장에 정착하면 그 수도 3개로 늘어날 것임
"
"https://news.hada.io/topic?id=22059","모든 AI 모델이 동일할 수 있음","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           모든 AI 모델이 동일할 수 있음

     * Platonic Representation Hypothesis(이데아 표현 가설) 은 AI 모델들이 점점 커지고 똑똑해질수록 내부적으로 유사한 표현 공간으로 수렴함을 주장함
     * 언어 모델의 압축(compression) 개념을 통해, 지능을 데이터 압축력으로 해석하고, 모델이 일반화할 때 방법의 유사성이 높아짐을 설명함
     * 임베딩 역변환(embedding inversion) 문제를 분석하며, PRH에 의하면 서로 다른 모델 간 임베딩 공간을 CycleGAN 등으로 정렬할 수 있음
     * Sparse Autoencoder 실험 등에서 서로 매우 다른 네트워크가 동일하거나 유사한 개념 및 회로를 발견함을 보여줌
     * 이러한 통찰로 고대 미해독 문자나 동물 언어 해독 등 실질적 응용 가능성이 높아짐
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

서론: Mussolini 또는 Bread 게임과 의미 공유

     * 필자는 ""Mussolini 혹은 Bread""라는 게임을 예로 들어, 질문을 반복적으로 좁혀가며 상대방이 생각하는 대상을 추론하는 방식을 소개함
     * 이 게임이 가능한 이유는 사람들 사이의 공통 의미 공간(semantics) 이 존재함에 있음
     * 다양한 사람이 규칙 없이도 대체로 의미상의 '가까움'을 직관적으로 이해함을 강조함

보편 의미론: 세상과 모델의 압축

     * 이 게임처럼, 인간 두뇌는 현실 세계의 복잡한 모델을 비슷한 방식으로 구축함
     * 알고리듬적 관점에서 AI는 세상 데이터를 최대한 압축해 학습함
     * 자연어 생성작업은 곧 확률 분포를 기반으로 한 압축 작업으로 볼 수 있음(Shannon의 정보 이론)
     * 모델이 데이터를 잘 압축할수록 실제 세계를 더 깊이 이해함을 시사함
     * 실제로 더 큰 언어 모델은 더 나은 데이터 압축 능력과 더 높은 지능을 보임
     * 데이터셋이 너무 커서 개별 데이터 포인트 기억이 불가능해질 때, 모델은 데이터를 결합해 일반화를 시작함

Platonic Representation Hypothesis(이데아 표현 가설)

     * MIT 연구진은 ""Platonic Representation Hypothesis"" 를 2024년에 공식화함
     * 이 가설에 따르면, AI 모델 규모가 커질수록 공유되는 특성(feature) 이 늘어나며, 표현 공간이 유사하게 정렬됨
     * 이는 언어 및 시각 등 다양한 영역에서 실험적으로 관찰되고 있음
     * 매년 모델이 더 커지고 효율적으로 발전함에 따라, 모델 간 표현 공간 유사성이 계속 높아질 것이라 전망됨

임베딩 역변환(embedding inversion) 문제

     * 필자는 임베딩 벡터에서 실제 입력 텍스트를 거꾸로 추론하는 임베딩 역변환 문제 연구 경험을 설명함
     * 이미 ImageNet 등에서는 확률값만으로 원본 이미지에 근접한 정보를 복원하는 사례가 있었음
     * 자연어 임베딩은 정보량이 많아 보이지만, 유사 텍스트가 유사 임베딩을 갖기 때문에 명확한 역추론이 매우 어려움
     * 이에 대해 반복적인 임베딩 탐색 및 최적화로 점점 더 정확한 텍스트에 접근하는 iterative refinement 기법이 효과적임을 확인함
     * 해당 방식으로 장문 문장 수준에서 94% 이상 정확도로 역변환 가능성을 실증함

이데아 가설을 이용한 임베딩 역변환 보편화

     * 그러나 기존 방법은 특정 임베딩 모델에만 적용 가능하며, 새로운 모델이나 사설 모델에는 한계가 있었음
     * PRH가 옳다면, 다양한 모델 사이에서도 보편 임베딩 역변환기를 만들 수 있음
     * 쌍을 알 수 없는 서로 다른 임베딩 집합(A, B)이 주어질 때, CycleGAN 방식으로 공간 정렬이 가능함을 수년간 연구함
     * 결과적으로 별도의 파인튜닝 없이도 두 임베딩 공간 사이를 unsupervised matching 방식으로 변환하는 데 성공함(vec2vec)
     * 이를 통해 각 임베딩별 개별 정보 없이도 임의 데이터베이스 임베딩을 번역하거나 거꾸로 추론하는 것이 가능함을 실증함

기계 해석 가능성: Universal Circuits

     * 기계해석(Mechanistic Interpretability) 분야의 회로 해석 연구에서도 모델 구조가 달라도 공통적인 내부 기능이 발견됨
     * Sparse Autoencoder(SAE) 적용 결과, 서로 다른 모델에 대해 독립적으로 학습하더라도 해석 가능한 피처(feature) 에서 상당히 큰 중복성을 확인함
     * 두 SAE의 피처를 비교해 교차 모델 개념 정렬이 가능함
     * PRH가 더욱 정확하다면, 더 강력한 모델일수록 이 현상이 두드러질 것으로 기대됨

실제적 함의 및 전망

     * 이데아 표현 가설은 심오한 철학적 함의 외에도 실제 모델 해석, 역변환, 신호 해독, 언어 복원 등 실용적 가능성이 있음
     * 향후 해석 기법이 발전하면, 더 큰 모델일수록 표현 공간 정렬 및 내부 공통성 발견이 흔해질 것으로 예측됨
     * 해결이 불가능했던 고대 문자(Linear A) 해독이나 동물 언어(고래 음성 등) 해석도 향후 이루어질 가능성이 있음
     * vec2vec 등 현재 방식은 아직 취약점도 있지만, 인터넷 기반 및 이미지-텍스트 임베딩 등에서는 상당한 성공을 보임
     * 언어 간 공간 전환 및 고래 언어→인간 언어 변환도 미래 해독 가능성이 존재함을 시사함

        Hacker News 의견

     * 모든 사람이 ""개"", ""집"", ""사람"", ""보트""처럼 비슷한 개념을 배우는 현상은 플라톤의 이데아 이론처럼 매우 흥미로움, 서로 다른 환경에서 자라나도 관찰 경험이 겹치지 않아도 결국 같은 개념으로 합의함, 대형 언어 모델(LLM)도 이와 비슷한 학습을 보여주지만, LLM은 훈련 데이터가 많이 겹치기 때문에 인간만큼 신기하지는 않음, 플라톤이 지적한 '선의 이데아' 같은 보편적 도덕이나 미덕 등이 진짜 존재한다면, LLM에게도 그런 가치를 학습시켜서 이를 따르도록 하거나 반대되는 요청은 거부하게 만들 수 있을 것이라 기대함
          + ""좋음""이나 ""공정함""이란 개념은 상황에 따라 훨씬 더 복잡함, 우리가 보트나 집처럼 간단한 물건에 대해선 합의할 수 있지만, 낙태, 안락사, 동물·줄기세포 실험 등 도덕적 문제에서는 같은 사회 안에서도 관점이 심각하게 다름, 예시로 2010년 갤럽 여론조사 결과 그림 참고 바람
          + ""대략""이란 표현이 플라톤이 옳았다는 주장을 지탱하기 위해 너무 많은 몫을 함, 우리는 같은 물리법칙·진화압 등 공유된 현실을 살아가니까 보트가 물에 뜨는 방식이 한정적일 수밖에 없음, 그렇다고 플라톤식 이데아가 실제 존재해서 모두가 똑같은 개념에 도달한다고 생각하진 않음, 실제로는 ""자유"", ""경제"", ""정부"" 같은 단어도 각자 정의와 해석이 다르고 문법은 같아서 겉으론 비슷해 보여도 실제 개념은 다 다름
          + 결국 융의 원형(archetype) 개념이라 이해함
     * 임베딩을 텍스트로 다시 변환하는 예시는 ""공유된 현실의 통계적 모델"" 개념을 뒷받침하지 못함, ""Mage (foaled April 18, 2020) is an American Thoroughbred racehorse who won the 2023 Kentucky Derby""의 고래 언어 버전이 상상조차 어렵고, 켄터키, 더비, 그레고리력, 미국, 말 품종 등은 모두 인류의 역사적 우연성과 문화 덕분에 중요해진 인공물임, 결국은 모두가 같은 데이터 더미로 훈련하다보니 통계적으로 비슷해지는 현상일 뿐임
          + 켄터키 더비가 ""현실의 핵심""인지 여부와는 별개로, 현실을 100% 정확히 모델링하려면 켄터키 더비에 대해 알아야 함, 저자는 모델이 궁극적으로 플라톤적 이데아에 가까운 표현으로 수렴하고 있다고 주장함, 완벽한 변환성을 가진 완전자율 모델이라면 ""말의 경주"", ""경주를 이긴 말"" 같은 개념을 고차원적으로도 전달할 수 있을 것이라 생각함, 실제로 플라톤 이데아 이론이 맞든 아니든, 지금 LLM이 이만큼 해내는지는 또 별개의 문제임
          + 현실이 전부 문화적이라고 주장하는 건 의미 없음, 과학적 사실에도 똑같이 적용되고, 고래가 과학이라는 단어를 모르더라도 중력은 존재함, 만약 LLM이 뉴턴의 중력 이론만 배운 뒤, 아인슈타인의 일반 상대성이론(GR)이 나온다면, 훈련 데이터에 GR이 없어도, GR의 현실에 대한 설명성은 달라지지 않음, 또한 GR을 고래노래로 번역은 불가능하겠지만, 영어-중국어-ML 모델-뇌 속 개념으로라도 전달 가능하다는 점이 '공유된 통계적 현실 모델'임, 영아 옹알이로 GR을 번역 못해도 GR의 현실성은 변하지 않음
          + LLM이 현실의 통계적 모델로 수렴한다고 보기 어렵고, 실제로는 단순히 훈련 데이터의 통계적 모델로 수렴 중임, 그나마 훈련 데이터가 워낙 커서 모든 텍스트에 공통된 무언가를 찾아내는 듯 보일 뿐임, 이게 현실의 핵심 진실을 밝혀줄 것 같지는 않고, 다만 우리가 ""이 관용구를 쓸 때 모두가 이 뜻을 이해한다""와 같은 현상은 밝혀줄 수 있음
          + ""Mage (foaled April 18, 2020) is an American Thoroughbred racehorse who won the 2023 Kentucky Derby"" 문장을 그리스어나 일부 현대 토착어로 번역하는 것도 거의 불가능함, 해당 문화에 대한 공유된 맥락이 아예 없기 때문에 용어집이 필요하거나, LLM이 직접 용어집 역할을 해줘야만 이해할 수 있음, 단 현재 최상위 LLM들은 QCD, 중력, 문화현상 등 미시-거시적 개념 설명까지 가능하고, 아예 새 언어로 번역해야 한다면 베이스 개념만 주고 천천히 구조를 쌓아갈 수도 있을 거라 봄, 결국 인간 언어 번역을 LLM이 별도 지도 없이 기본적으로 해내는 것도 이런 능력 덕분임
          + 이 이슈는 데이터셋이 완전히 다른 두 모델(예: 고대 중국 텍스트 전용, 고대 그리스어 전용)을 훈련시켜, 비슷한 구조가 나타나는지 실험하면 손쉽게 결과를 확인할 수 있음
     * ""우리가 고래 언어나 고대 언어를 번역할 수 있다""는 기대는 지나치게 긍정적임, 언어에서 가장 중요한 건 맥락임, 인간은 경험을 바탕으로 남긴 수십억 개의 텍스트가 있어서 AI가 언어를 잘하는 것이고, 고래에겐 그런 데이터가 없음
          + ""사자가 말을 할 수 있다면 우리가 이해할 수 있을까?""라는 의문 던짐
          + 우리 주변 세계는 인간-고래-기타 동물 모두에게 공유된 경험임, 이 점을 감안하면 고래와 인간 사이에도 그 공유점은 존재함
          + 중요한 건 ""언어 간에 공유된 표현 공간""이 있는지임, 만약 있다면 언어별 구조와 번역 매핑을 분리해서 학습할 수 있음, ""유니버설 임베딩 인버터""라 부르는 후자는 더 쉽게 학습 가능할 수 있고, 구조가 충분히 독특하다면 이를 공통 표현 공간에 매핑해서 활용할 수 있음, 맥락 없이도 번역 가능하다면(아직은 희망 섞인 추측이지만) 편견 없이 연구해볼 만함
          + 고릴라나 코끼리(둘 다 매우 지능 높음)가 사물을 명명하고 기호를 쓸 수 있도록 가르친다면, 그들 역시 경험과 지혜를 세대 간 전승할 수 있고, 우리 못지않은 지능을 조용히 발휘할 수 있을 것이라 믿음, 참고로 Google Gemma의 돌고래 프로젝트에 관심이 있지만, 인간이 육상 동물이므로 돌고래보다는 코끼리를 연구 대상으로 삼았으면 하는 바람 있음, 그래서 육상에서 즉각적인 연구 피드백이 가능하고 기본 연구에 더 집중할 수 있음을 강조함
     * 이런 접근은 각 소스의 특성 분포와 의미론적 관계가 충분히 비슷할 때만 통함, MB게임(Mussolini vs Bread 등 비교 추리 게임)은 상대가 내가 모르는 인물을 선택하면 실패함, 레퍼런스를 잡아내지 못하거나 의미론적 거리 판단도 다를 수 있음, 전문가들과는 전문가끼리, 일반인은 일반인끼리 수준을 맞춰야 제대로 통함, 고대 문서 해독도 문제를 갖는데, 고대 문명이 현재와 전혀 다른 개념에 집중했다면, 현대적 의미 임베딩으론 이해가 거의 불가능해짐
          + 친구들과 MB게임을 해보면 — 인물인 경우 끝까지 제대로 맞힌 적이 한 번도 없음
     * Mussolini vs Bread 게임 예시에서 ""이게 무조건 인물이다""란 추리는 논리적으로 성립하지 않음, 동물 중에서도 그런 답이 더 많은 경우도 있을 수 있음
          + 이 농담은 David Beckham이 인물이라기보다는 (악의 화신과 비교해도) 그만큼 인간적으로 못 느껴진다는 식의 유머임
          + 논리가 허술하지만, 실제로는 이런 식의 설명 부족에도 사람들이 답을 잘 추론해내는 게 핵심임, 인간이 공유하는 퍼지(Fuzzy) 의미 공간이 있다는 의미임
          + 내 생각도 저자와 같음, 내 단어는 ""총""이나 ""포병""일 것인데 이 역시 논리 허점을 뚫을 수 있음, 그리고 이런 예시가 왜 순수 임베딩 검색만으론 RAG(retrieval-augmented generation) 문제를 해결 못하는지 시사함
          + 사소한 논리 오류는 양해 바람
          + Oswald Mosley처럼 엉뚱한 답도 나올 수 있음
     * ""이 게임이 작동하는 이유는 세상의 사물들이 단 하나의 방식만으로 연관 있다는 점에서 비롯된다""는 주장에 동의하지 않음, 다양한 관계가 존재하고, 그 관계들 역시 우리가 사는 현실에서 비롯됨, ""방식""이란 단어를 여러 의미로 쓴 것 같은데, 인용문이 모호하게 표현해서 혼동이 있음
     * LLM이 인류의 집단 작업물로 현재의 현실 표현에 수렴하는 점에는 동의함, 이제 AI에 실시간 감각 입력, 대사 및 에너지 사용에 기반한 각기 다른 반감기(half-life)를 가진 가상 호르몬, 상시 사고 루프, 창의적 신경 연결을 유발하는 인공 실로시빈까지 부여해야 함, 인류에 스톤 에이프(stoned ape) 이론이 있다면, AI에는 스톤드 AI 이론이 필요함
          + 혹시 AI를 테마파크에서 이용객용 어트랙션으로 만들고, Anthony Hopkins에게 소스코드 관리 권한을 줘보는 건 어떤지, 뭐가 잘못되겠음?
          + AI 관련 글을 읽는 게 이제 지겨운데, 만약 ""우리가 AI에게 버섯을 먹였다""는 기사가 뜬다면 바로 클릭할 것임
     * ""Ilya가 지능-압축 관련 발표를 했을 때 전혀 이해할 수 없었다""는 내용을 읽고, Marcus Hutter가 잊힌 게 아닌가 생각함, 그렇다면 Hutter Prize도 꼭 다시 참고해볼 가치가 있다고 생각함
     * Grok, o3-pro, Claude에 피에조 효과(piezoelectric effect) 관련 질문을 해봄, 전부 올바른 답을 주긴 했는데, Claude만이 실제 사용케이스에서 발생하는 2차 효과까지 짚어줌, 세 모델이 동일한 공간을 탐색할 수 있지만 Claude가 한 단계 더 깊은 관점 제시함
          + 궁금한 점 하나, Grok 3인지 4인지 알고 싶은 마음 있음
     * 도를 말할 수 있지만, 그 도는 영원한 도가 아님, 도가 무엇인지 묻는다면, 나는 그것이 '의지'라고 봄 — 의지는 인간이 언어로도 표현할 수 있음, 같은 의지라도 중국어·일본어·영어로 모두 표현 가능하며, 언어는 각기 다른 표상일 뿐임, 대형 언어 모델 역시 단어 토큰을 통해 의지를 배우고, 그걸 표현하게 되면 도를 실현하게 됨, 그 의미에서 “AI 모델은 본질적으로 모두 같을 수 있다”는 주장에 동의함
"
"https://news.hada.io/topic?id=21996","AI가 오픈소스 개발자를 느리게 만든다. Peter Naur가 그 이유를 알려줄 수 있다","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           AI가 오픈소스 개발자를 느리게 만든다. Peter Naur가 그 이유를 알려줄 수 있다

     * 최근 연구에 따르면 오픈소스 개발자가 자신이 잘 아는 코드베이스에서 AI 도구를 사용할 때 오히려 작업 시간이 19% 늘어남
     * 개발자들은 AI가 자신을 더 빠르게 만들었다고 믿지만, 실제로는 더 느려진다는 인지와 현실 간의 괴리가 존재함
     * 핵심 원인은 개발자가 가진 정교한 멘탈 모델(이해 구조) 과 AI 간의 지식 전달 한계임
     * Peter Naur의 이론에 따르면, 프로그래밍에서 가장 중요한 것은 개발자 머릿속의 ""정신적 모델""임
          + 덴마크의 컴퓨터 과학 선구자이자 2005년 튜링상 수상자. 프로그래밍 언어 구문을 설명하는 데 사용되는 Backus-Naur 형식 (BNF) 표기법 에 기여함
     * 장기적 관점에서 프로젝트를 깊이 이해하려면 직접 코드를 작성하며 멘탈 모델을 구축하는 것이 중요함
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

AI가 오픈소스 개발자를 느리게 만드는 현상

     * Metr의 연구에 따르면 AI 도구 사용 시 오히려 문제 해결 속도가 19% 느려지는 결과가 나옴
     * 개발자들은 작업 전 AI가 24% 빠르게 도와줄 것이라 기대했고, 작업 후에도 실제보다 20% 빠르다고 믿음
     * 이 연구는 자신이 깊이 이해한 오픈소스 프로젝트를 직접 관리하는 숙련 개발자를 대상으로 진행됨
     * 결과가 모든 개발자에게 일반화되지는 않지만, 이 집단에서는 AI 도구가 생산성에 역효과를 낸다는 사실을 보여줌
     * 기업 환경이나, 새로운 코드베이스에 적응해야 하는 일반적인 개발자에게는 AI 도구가 생산성 향상에 더 긍정적인 역할을 할 수 있음

왜 AI가 숙련 오픈소스 개발자를 느리게 만드는가

     * Peter Naur의 “Programming as Theory Building” 논문에 따르면 프로그래밍의 진정한 결과물은 코드가 아니라 ‘프로젝트에 대한 개발자의 멘탈 모델’ 임
     * 이 멘탈 모델은 시스템의 이해, 문제 진단, 효과적 개선의 핵심임
     * LLM 기반 AI는 개발자의 멘탈 모델에 직접 접근할 수 없으며, 일부 정보를 제공해도 지식 전달 과정에서 본질적 손실이 발생함
          + 예시로, 누군가에게 아기 재우기를 맡길 때 명확하게 설명했어도 실제로는 의도와 다르게 행동하는 일이 잦음
     * 멘탈 모델의 전달은 극도로 복잡하며, AI가 텍스트만으로 이를 흡수하는 것은 거의 불가능함
     * 따라서 자신이 깊이 이해한 프로젝트에서 AI에게 작업을 위임하면 오히려 생산성이 저하됨
     * 개발자의 풍부한 맥락 정보와 직관적 이해는 AI가 쉽게 대체할 수 없음

현업에서 AI 도구를 금지해야 할까?

     * 꼭 그렇지는 않음. “스스로 잘 알고, 잘 이해한 프로젝트에서 일하는 사람”에게만 해당함
     * 많은 기업 개발자는 이미 떠난 선임자의 코드를 유지보수하거나, 시스템 전체 구조를 깊이 이해하지 않은 상태에서 작업하는 경우가 많음
     * 이런 환경에서는 AI가 코드베이스를 빠르게 파악하고 변경사항을 자동 생성함으로써 단기적 생산성 향상에 기여할 수 있음
     * 단기적인 비즈니스 가치 생산과 즉각적 효율만 본다면 AI 도구는 생산성에 긍정적인 역할을 할 수 있음

멘탈 모델 구축과 AI

     * 만약 프로젝트에 대한 멘탈 모델이 없다면, LLM이 생산성 향상을 도와줄 수 있음
     * 그러나 소프트웨어 개발의 본질이 ‘멘탈 모델 구축’에 있다면, AI에 지나치게 의존할 경우 그 능력이 저하될 수 있음
     * 장기적으로 프로젝트를 깊이 이해하고 능동적으로 변화시키고 싶다면 직접 코드 작성 경험이 필요함
     * 반대로, ‘그냥 돌아가게만 하는’ 식의 작업이라면 AI 활용이 효율적일 수 있음

추가 논의 및 결론

     * 현재 수준의 AI 도구로는 충분한 멘탈 모델을 가진 개발자의 생산성을 향상시키기 어려움
     * AI가 멘탈 모델을 제대로 지원하거나, 숙련 개발자의 생산성을 혁신적으로 높일 수 있는 방향은 아직 연구 및 발전이 필요함
     * 향후 모델이 발전하면 인간 개발자가 멘탈 모델을 굳이 갖지 않아도 될 날이 올 수도 있으나, 현재 수준에서는 직접적인 이해와 학습이 필수
     * 최종적으로, AI는 ‘내가 무엇을 하고 있는지 깊이 이해하는 환경’에서는 방해가 되고, ‘빠른 결과물이 중요한 환경’에서는 생산성 도구가 될 수 있음

     반대로, ‘그냥 돌아가게만 하는’ 식의 작업이라면 AI 활용이 효율적일 수 있음

   비단 개발자뿐만은 아니지만 다양한 성향을 가진 사람들이 있다 보니, 어쩌다 보니 개발자를 하고 있고 코드를 작성하거나 보는 것을 싫어하거나 두려워하면서 체계적인 구조나 유지보수 관점에 대한 해석보다는 돌아만 가면 된다는 마인드일 수록 AI에 대한 의존이나 맹신이 강한 것 같다는 생각이 들어요. 아님 말고

     개발자들은 AI가 자신을 더 빠르게 만들었다고 믿지만
     AI를 활용한 리서치가 빨라지면서, 퀄리티를 높게 만들 수 있으니, 똑같은 작업이라도 퀄리티가 조금 더 높게 나오지 않을까요. 개발자들은 작업 후 결과물의 퀄리티에 맞춰서 개발하려면 혼자 도달하는 것 보다, AI의 도움을 받아서 도달하는 게 더 빠르다는 생각을 하는 것은 아닐지.
     애초에 쓰지 않았다면, 좀 더 아는 지식만으로 구현하니 그런 것은 아닐까 하는 생각이 드네요.

   경험 많은 오픈소스 개발자의 생산성에 미치는 ""AI의 임팩트"" 측정하기

        Hacker News 의견

     * HN 여러분, 저는 논문 저자임.
       해당 블로그 글이 AI가 개발 속도를 늦추는 데 기여할 수 있는 구체적 요인 하나를 흥미롭게 다뤘다고 생각함.
       논문(C.1.5 섹션)에 개발자 인용문도 있으니 참고 바람.
       많은 사람이 논문을 읽고 공감되는 요인 하나를 발견해서 “이 한 가지 문제만이 느려지는 이유다”라고 결론내기 쉬움.
       하지만 실제론 요소가 여러 개임(최소 5개가 유력, 최대 9개까지 배제 불가, p.11 요인 표 참고).
       어느 한 가지가 원인이라는 가정보다 다각적인 원인 분석이 타당함.
       스스로 실험해 볼 계획 있는 분은 논문의 이메일로 꼭 결과를 공유해줬으면 하는 바람임.
       그리고 기사 제목이 “AI slows down open source developers. Peter Naur can teach us why”로 작성된 점에 대해 더 정확히는 “2025년초 AI가 경험 많은 오픈소스 개발자를 느리게 함. Peter Naur가 특정 요인에 대해 더 많은 맥락 제공함.” 정도가 적합하다고 생각함.
       표현이 덜 자극적일 수 있지만 정확성이 중요하다고 여김.
       다시 한 번 멋진 글에 감사를 표하고 계속 댓글도 읽고 있음
       이전 관련 토론
       논문 전문
          + 개인적 궁금증이 있는데, 연구에서는 AI 사용 전과 후에 실제 소요 시간이 얼마나 달라졌는지 어떻게 신뢰할 만하게 측정했는지 묻고 싶음. 혹시 개발자가 AI 사용 후 시간이 얼마나 줄어들지 예상한 뒤, 실제 사용 시간을 측정해 그 차이를 본 건지 궁금함. 그리고 어떤 이슈의 난이도나 해결에 필요한 시간 추정이 어려울 때 연구팀에서는 어떤 식으로 통제했는지도 알고 싶음. 이런 측정은 정말 복잡하다는 점에 공감함
          + 결과에 공감하며 답변에 고마움을 전함. 제목은 급진적 스타일이 좋아서 바꿀 계획은 없으나, 기사에서 잘못된 표현임을 분명히 수정할 예정임. 본인이 쓴 글에서 연구 결과의 주요 기여 요인 “레포지터리에 대한 높은 개발자 친숙도”, “크고 복잡한 레포지터리”, “함축적 레포지터리 맥락” 등 연구와 궤를 같이함을 밝힘. 직접 자신에게 실험 진행도 해보고 싶은데, 업무상 요구를 병행하면서 통제된 환경을 만들기는 매우 힘든 일로 느껴짐. 짧은 시간 내 완료 가능한 명확한 태스크의 리스트도 부족함
          + 자신이 잘 아는 프로젝트에서 최적화된 워크플로우에 변화를 주면 초기에는 느려질 거라 기대함. 중요한 점은 이런 개발자들이 6개월, 1년 후에는 어떨지 지켜보는 것임. 이번 연구는 장기적 추이는 보여주지 않으니, 향후 연구에서 동일 개발자가 익숙해진 후 성과가 어떻게 달라지는지 알 수 있길 바람. 자신도 AI로 자동화가 어려웠던 많은 작업을 스크립트화할 수 있겠다는 걸 체험함. 항상 “이게 시간 대비 가치가 있는가?”라는 질문을 품어야 함
            xkcd 타임 매니지먼트 만화
          + “2025년초 AI가 경험 많은 오픈소스 개발자를 느리게 한다”는 것도 너무 일반화된 표현임을 언급. AI가 시간을 절약할 수 있는 특정 작업도 있으므로 어떤 과제냐에 따라 효과가 다름
          + 느려진다는 게 반드시 나쁜 것은 아니고, 느린 프로그래밍(리터러리 프로그래밍/Knuth 방식)이 오히려 이론화에 더 도움이 된다고 생각함. 패스트푸드식 프로그래밍이 아닌, 충분한 사고와 추상화를 동반한 느린 개발이 중요하다는 주장도 가능함
     * “도구가 실제로 자신을 빠르게 만들었는지 느리게 만들었는지 개발자가 파악하지 못한다”는 현상에 공감함. 보트가 바람과 조류 때문에 목표를 벗어나는 현상을 예시로 들어, 현재 내 주변의 움직임 기반으로만 진전을 인식할 뿐, 목표 도달 여부를 직감하기 어렵다는 점을 지적함. 그래서 “진전하는 기분”이 들게 하는 전략을 택하는 경향이 있고, 이는 비효율적이거나 실제로는 더 느린 경로(예: 운전시 자주 우회전 등)를 선택하게도 함
          + AI 도구를 처음 사용할 때는 막힘이 없어서 계속 일이 진행되는 느낌이 좋았음. 하지만 실제로는 직접 한 줄 고치는 게 더 빠른 경우에도 습관적으로 AI를 호출하는 상황이 발생함. 운전 비유와 비슷하게 특정 길이 막히면 다시 원래 길을 추천해주는 GPS처럼 반복적으로 돌아가기 쉬움
          + Waze 같은 내비앱이 실제로는 더 긴 루트를 안내함에도 불구하고, 얽힌 우회길로 인해 “진행 중”이란 착각을 들어 빠르게 간다고 느끼게 하는 것과 비슷함. AI 도구도 프로그래밍을 더 쉽게 한다고 체감되지만, 실제 생산성이 줄 수도 있음. 인간은 고통 없이 진행하는 단기적 체험만 기억하기 쉽고, 어렵지 않았다는 점에서 진전했다 생각함
          + 결국 인간은 본능적으로 탐욕 알고리즘(greedy algorithm)을 선호함
          + 리눅스/유닉스 사용자들은 키보드 컨트롤과 CLI 도구가 최고의 효율이라 생각하지만, 대부분의 작업에서는 마우스가 더 빠르다는 연구 결과가 있음. 키보드 입력이 더 빠르다고 느끼는 이유는 초당 동작 수가 많기 때문임
          + AI가 생성한 코드는 거의 리뷰되지 않고, 많은 개발자들이 코드리뷰 자체를 힘들어해 읽기를 거부함. 그래서 새 프레임워크나 코드 리라이트가 인기가 많은 현상이 발생함
            Joel on Software: Things you should never do, part I
            다수의 AI 생성 코드는 그저 만들어지고, 간단한 테스트만 거치고 끝남. 심지어 본인조차 전체 맥락이나 이유를 충분히 이해하지 못하는 코드가 많아짐
     * 이 연구의 요지는 “AI가 실제보다 생산성 향상이 큰 듯한 착각을 만들게 한다”로 요약 가능함. 일부 참가자만 생산성 소폭 향상이 있었으며, 대부분은 오히려 많이 떨어짐. 많은 AI 덕분에 생산성이 폭발적으로 늘었다는 사람들이 있지만, 실상은 그 효과 자체가 착각이라는 연구의 통찰이 무시되고 있음. AI는 사용자가 이 제품을 사용해야겠다, 유용하다고 믿게 만드는 데 최적화된 상품임. 개인적 가치는 지각된 현실이라 의심의 여지가 없지만, AI에 강하게 의존하는 사람은 자기 인식의 왜곡과 가짜 성취감, 도구 의존성에 대해 정말 조심해야 함. AI가 최적화된 토큰 스트림으로 대답을 해주기도 하지만, 진짜 최적화 목표가 무엇인지 한 번쯤 고민해봐야 한다고 생각함
          + LLM은 뭔가 배울 때 도움이 되긴 하지만, 그 이해가 굉장히 추상적이고 LLM식이 되는 느낌임. 학습할 때는 다양한 방법을 섞는 게 좋다고 여김
          + AI 도구는 개발자가 “빠르다”기보다는 “순간적으로 재빠르다”는 느낌을 줌. 뇌의 부담이 줄어드는 착각 같은 면이 있는데, 다른 피드백 루프에서 느낌 자체가 바뀌고 기억 형성 기제도 바뀌면서 생기는 흥미로운 착시 현상임
     * “경험 많은 오픈소스 개발자가 자기 프로젝트를 작업할 때 AI를 사용하면 오히려 느려진다”는 연구를 논의하던 중, 본인은 완전히 남이 만든 3개월 된 코드베이스와 익숙지 않은 프레임워크에 일하게 됨. 그런데 Claude Code를 활용해 단 몇 시간 만에, 예전 다른 프로젝트에서 하루 이틀, 혹은 최대 2주 걸릴 일(데이터 동기화 등)을 빠르게 완성했고, 엄청난 점프스타트가 됐음. 복잡도가 높아지면 점점 느려지겠지만, 도구의 도움으로 시작이 매우 빨라진 점이 놀라움
          + 본인도 비슷한 경험이 있는데, 이 연구에서 말하는 것은 우리가 겪은 ramp-up(적응기)이 아니라, 이미 매우 익숙해진 오픈소스 개발자들이 AI로 태스크를 수행할 때의 이야기임. LLM이 새로운 코드베이스 적응을 확실히 빠르게 해주지만, 익숙해진 후에는 오히려 방해된다는 경험이 있음
          + “2주 걸릴 PR을 몇 시간에”라는 주장에 생산성 향상 이야기가 항상 따라 붙는데, 실제 우리가 개발 기간 예측에 얼마나 정확한지 점검되는 일은 별로 없다고 봄. 또 이렇게 급히 낸 PR의 품질이 원래 예상대로인지, 단순히 빨리 하려다 중요한 시스템 맥락을 생략해 버그 확률이 높아지는 건 아닌지 따져야 함. AI 없이도 품질 포기하면 빨라짐
          + AI 덕분에 평균적으로 코드베이스 및 시스템 숙련도가 자연스럽게 늘었는지도 의문임. LLM을 쓸 때의 학습 효과는 마치 새로운 언어 읽기(리딩)는 가능하지만, 직접 처음부터 쓰는(스피킹)는 어려운 것과 같은 느낌임. C++를 예로 들면 읽고 기존 것 고치는 건 가능하지만, 어디서부터 새로 만드는 건 힘듦
          + AI 도구 덕분에 엄청난 점프스타트를 얻었을 뿐, 연구나 글, 논문에 대한 비판적 의도가 아니라 특정 컨텍스트에서 AI가 정말 도움이 된다는 점을 말하려 했음. 단순히 코드 작성만이 아니고, 예를 들면 Claude Code가 프로젝트 내부 컨테이너에서 직접 AWS 클러스터 연결 시도 등도 해주며, 전체 인프라와 구조를 파악하는 데 큰 도움이 됐음. 본인 경험상 80~90%는 코드 품질보다 “비즈니스 가치”가 우선시됨. 실제로 코드 품질이 중요한 작업이나, 특별한 알고리즘, 자료구조가 필요한 분야에는 얼마나 쓸모가 있을지는 모르겠음. 그렇지만 좋은 예제와 명확한 맥락만 부여하면 꽤 쓸만한 코드를 써준다는 경험도 했음. 도구들은 매주 혹은 매달 빠른 속도로 발전함. 결국 AI는 마법이 아니라 도구이고, 프로덕트/결과에 대한 책임은 결국 본인에게 있음
          + 논문(TFA)이 다루는 건 매우 익숙한 프로젝트에서의 사례임을 유념해야 함. 내가 겪은 사례는 정반대로, AI가 익숙하지 않은 상황에서 주로 활약함
     * “AI agentic 도구(Claude Code, Amp, Gemini CLI 등)는 프로그래밍에서 테이블쏘가 목공에 등장한 것과 비슷하다”는 비유를 인용해, 사용법을 익히면 어떤 작업은 더 빠르고 잘할 수 있지만, 익숙하지 않을 때는 오히려 손가락을 다칠 수도 있다는 근거를 듦. 본인은 agentic AI 덕분에 좀 더 야심 찬 프로젝트도 시도하게 되고, 하기 싫었거나 반복적인 일은 AI에 맡기니 생각의 여유가 생김. 반면 AI가 모두 대신하게끔 내버려두고 이해 없이 커밋만 하면 안된다는 점도 경계함. 도구답게 더 나은 사용법을 익히려는 노력이 필요함
          + 테이블쏘 비유는 어울리지 않는다고 봄. 테이블쏘는 손 도구에 비해 정밀한 도구인데, agentic AI는 정밀함과 거리가 멀기 때문임
          + “당신이 AI를 잘못 쓰고 있다”는 논리는, 이미 AI가 나타나기 전 모든 오픈소스 스택을 쌓은 개발자들에게 모욕적임. 게다가 AI를 써서 가치 있는 소프트웨어가 만들어진 증거는 아직 없음
     * 이 연구에서 Cursor 경험이 50시간을 넘은 개발자(연구 참여 시간 포함)는 단 한 명뿐이었고 이 개발자는 25% 빨라짐을 경험했음. 나머지는 모두 초보였고, 초보가 새로운 도구를 쓰니 느려지는 건 너무 당연함. 이 연구만 가지고 AI의 생산성 결론을 도출하긴 어렵다고 생각함
          + 논문의 세부 사항을 참조하면, 비슷한(또는 오히려 더 적은) 도구 경험자를 대상으로 진행한 이전 연구들에서도 오히려 속도 향상을 보고함. LLM 프롬프트 경험은 대부분 충분했으며, 특히 Cursor가 VSCode와 유사해 별도의 러닝커브가 크지 않다는 점도 고려함. 만약 모든 개발자가 AI 도구에 엄청나게 익숙해지면, AI 없이 작업할 때 실력이 오히려 떨어져, 결과적으로 AI 사용 시 단순히 더 나빠진 상태가 기준이 되어 속도가 높아진 것처럼 보일 수 있음. 어떤 Tool을 썼는지가 중요한 게 아니라, 생산성 자가 보고가 실제에 비해 지나치게 낙관적이었다는 게 중요한 통찰이라고 봄. 실제 효과를 판단하려면 구체적인 측정치가 필요함
            (논문 C.2.7 “평균 이하의 AI 도구 활용” 섹션에서 더 자세히 다룸)
          + 오랜 기간 자신의 IDE(Vim/Neovim 등)를 사용한 개발자라면, 새로운 도구(Cursor 등)로 전환 시 생산성이 수개월간 현저히 떨어질 수 있음
          + 본인도 같은 생각임. 익숙하지 않은 도구를 쓰는 개발자는 느려질 수밖에 없음. AI도 예외는 아님
     * 현재 Burn(러스트 기반 딥러닝 프레임워크)의 정기 코드 리뷰어임을 밝힘. 최근 AI agent가 전부 작성한 듯한 버그픽스 PR을 닫은 일이 있었음. 해당 PR은 문제의 원인을 해결하지 않은 채 에러를 그냥 무시하는 식이었고, 쓸데없이 장황하게 변명성 코드를 추가했으며, 에러 무시 테스트까지 포함함. 커밋 기록을 위한 행동으로 추정됨. 이런 식의 AI 남용이 우려스러운 경향으로 번지고 있음
          + LLM이 정답을 모르면 엉뚱한 답을 내놓다가 잘못됐다고 지적하면 “맞아요, 다시 고칠게요” 식으로 반응하는 부분은 신기함. 실제로 경험 없는 사람들이 이슈를 구분 못하거나, 혹은 점차 코드를 신경 안쓰는 현상이 두려움. 본격적인 취약점, 악용 사례가 쏟아질 것이란 우려도 있음
          + 동료의 MR을 리뷰하다, 명확히 AI가 생성한 티가 나는 테스트 케이스(thing1, thing2 등 변수명만 다르고 내용은 천편일률)를 발견함. 피드백으로 더 구분 가능한 이름을 제안하니, 이번엔 AI가 각 케이스 특징을 전부 나열하듯 너무 장황한 변수명을 붙여 결과적으로 한눈에 들어오지 않는 코드가 되었음. 작성자는 작성 속도를 크게 높였다고 느꼈겠지만, 실제로 피드백과 리뷰, 수정에 시간을 쓰며 생산성 이득은 다 사라진 셈임
          + “러스트로 딥러닝 프레임워크 → AI와 얽힌 악순환”이란 취지의 익살스러운 의견도 보임
          + 실제로 커밋을 기록하려는 목적에 AI가 쓰이는 분위기는 이미 오래됨. AI는 단순한 스팸 생산도 용이하게 만듦
            참고: 오래전 AI 스팸 이슈
          + LLM이 try:catch 구문을 넓게 써 문제 소스 추적을 힘들게 만듦을 지적함. 본인은 문제를 빨리 그리고 강하게 드러나게(=fail fast) 해서 바로 고치고 싶어함
     * 본인이 느낀 점을 공유하자면, AI 프로그래밍은 집중 흐름이 자꾸 끊기고 더 쉽게 피로해짐. 코딩은 하루 종일 하는 것은 신화이고, 1~3시간씩 집중하고 중간에 쉬는 게 일반임. 심지어 동료의 코드나 변경사항 읽기를 하다 보면 그 시간도 일의 진도에 포함되나 실제 진전은 잘 안 됨. agentic AI(작은 코드 리팩토링 등)는 유용할 수 있으나, 큰 생산성 향상은 별로임. 코드 자동완성(예: 초기 Copilot)은 오히려 쓸데없는 노이즈가 더 많음
          + 실제로 하루 동안 무슨 작업을 했는지 녹화하면, 그 결과는 꽤 우울할 듯함. 특히 성숙한 코드베이스의 경우, 한 시간 집중도 과장일 수 있음
     * 낯선 코드베이스에서 트릭키한 버그(예: race condition)를 디버깅할 때는 로깅 추가, 라이브러리 함수 교체, 구조 개선 등이 필수인데, AI가 “여기서 레이스 컨디션이고 이렇게 고치면 된다”라고 단기적으로 빠른 해결만 제시하면 코드 구조나 논리 이해에 오히려 해가 될 위험도 있음. 장기적으로 AI 주도 코드 편집이 계속된다면, AI 조차 더 이상 정상적인 대응을 못 하게 될 정도로 코드가 변질될 수도 있음
          + “AI를 활용해 모르는 언어, 모르는 코드베이스에 기여했다”는 경험담을 들을 때마다, 단기적으론 괜찮지만 정말로 무엇을 배웠는지 묻게 됨. 이런 기여가 소규모 작업에는 쓸 만할지 몰라도, 장기적 유지보수 경험담은 별로 못 들어본 느낌임
     * 최근 AI 도구를 적극 활용한 첫 프로젝트의 회고로, 1) 속도는 빨라지지 않았음, 2) 오히려 느려졌을 수도 있음, 3) 결과물의 품질은 더 나아졌다는 결론임. 느려짐과 품질 상승은 연결되어 있는데, 아이디어 검증이나 대안 탐색 등 주로 보조 수단으로 활용하다보니 그런 것임. AI 덕분에 생소한 영역에서 학습 경험도 좋았고, 주력 분야에서는 내 아이디어 혹은 AI의 아이디어를 다듬으며 결과적으로 품질이 향상됨. 속도만이 중요한 게 아니며, 품질을 정량화하기 어렵기는 해도, 충분히 가치 있다고 느낌
          + AI가 오히려 품질 증진에 기여할 수 있기에, 요즘엔 주장도 잘 하고 무턱대고 동의하지 않는 AI를 선호함. AI에게 아이디어를 부탁하고 문제점을 공격하거나, 내 아이디어의 허점을 같이 찾아달라고 하면 생산적임. 실현하지 않을 수도 있지만, 전에는 생각 못 했던 다양한 각도를 떠올릴 수 있게 해줌. 실질적으로 도메인에 대해 적당히 의견을 내줄 수 있는 동료와 대화하는 것과 유사한 경험임

   저도 비슷한 생각을 가지고 있었지만 뭐라 표현하기가 애매습니다.
   멘탈 모델 적절한 네이밍이네요. 종종 활용해야겠습니다.
"
"https://news.hada.io/topic?id=21998","카카오 디벨로퍼스, 5년 만에 UI 전면 개편","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       카카오 디벨로퍼스, 5년 만에 UI 전면 개편

   카카오가 자사의 개발자 포털 Kakao Developers의 메인 페이지와 앱 관리 화면을 전면 리디자인했다.
   이번 개편은 2020년 4월 이후 약 5년 3개월 만의 UI 업데이트로, 사용성과 접근성을 전반적으로 개선한 것이 특징이다.

  주요 개편 내용

     * 문서 구조 리디자인
       연동 흐름을 따라가며 API 문서를 더 쉽게 탐색할 수 있도록, 문서 구조와 레이아웃이 재설계됨.
     * 앱 관리 대시보드 개선
       API 키, 사용 이력, 설정 변경 내역 등을 더 직관적으로 확인할 수 있게 UI 구성 최적화.
     * 모바일 최적화 및 다크 모드 지원
       다양한 디바이스 환경에서 가독성을 높였으며, 다크 모드도 새롭게 지원.
     * 서비스 알림 기능 제공
       API 오류, 설정 변경, 공지사항 등을 카카오 디벨로퍼스 웹사이트 및 카카오톡 채널을 통해 실시간 알림.
       (해당 기능은 2025년 2월 도입됨)
     __________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________________

   이번 개편은 UI와 UX 전반이 최근 트렌드에 맞게 정돈되었다는 점에서 긍정적으로 평가할 수 있다.
   다만 일부 항목의 배치나 인터랙션 방식이 변경되면서, 기존 사용자에게는 익숙해지는 데 다소 시간이 필요할 수 있다.

   보다 구체적인 사항은 상세 변경 내용을 참고.
"
