```sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```

```sh
#!/bin/bash
# htmlq로 content만 추출
htmlfile='html/3172.html'
selector='span#topic_contents, span.comment_contents'

echo "[pup]"
time pup "$selector" < "$htmlfile" > /dev/null

echo "[htmlq]"
time htmlq "$selector" < "$htmlfile" > /dev/null
```



```sh
#!/bin/bash
# profile convert text
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "pandoc" ]]; then
        measure_time_ms "pandoc -f html -t markdown < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "html2text" ]]; then
        measure_time_ms "html2text < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "lynx" ]]; then
        measure_time_ms "LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 \"$htmlfile\" > /dev/null"
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath" "$selector")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "pandoc"
run_profiling "html2text"
run_profiling "lynx"
```

```sh
#!/bin/bash
# profile html parse
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"
    local selector="$3"

    if [[ "$tool" == "pup" ]]; then
        measure_time_ms "pup \"$selector\" < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "htmlq" ]]; then
        measure_time_ms "htmlq \"$selector\" < \"$htmlfile\" > /dev/null"
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath" "$selector")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "pup"
run_profiling "htmlq"
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !bash
bash profile_html_parse.sh

[pup]

[pup STATS]
Count: 128
Min:    22 ms
Q1:     25 ms
Median: 25 ms
Q3:     27 ms
P95:    29 ms
P99:    32 ms
Max:    32 ms
Mean:   25 ms
Total:  3285 ms

[htmlq]

[htmlq STATS]
Count: 128
Min:    2 ms
Q1:     3 ms
Median: 3 ms
Q3:     4 ms
P95:    5 ms
P99:    6 ms
Max:    21 ms
Mean:   3 ms
Total:  463 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash profile_convert_text.sh

[pandoc]

[pandoc STATS]
Count: 128
Min:    64 ms
Q1:     68 ms
Median: 70 ms
Q3:     73 ms
P95:    80 ms
P99:    146 ms
Max:    160 ms
Mean:   72 ms
Total:  9277 ms

[html2text]

[html2text STATS]
Count: 128
Min:    43 ms
Q1:     46 ms
Median: 48 ms
Q3:     50 ms
P95:    58 ms
P99:    92 ms
Max:    99 ms
Mean:   49 ms
Total:  6293 ms

[lynx]

[lynx STATS]
Count: 128
Min:    15 ms
Q1:     17 ms
Median: 18 ms
Q3:     19 ms
P95:    21 ms
P99:    24 ms
Max:    24 ms
Mean:   18 ms
Total:  2308 ms
```


```sh
#!/bin/bash
htmlfile='html/3712.html'
selector='span#topic_contents, span.comment_contents'

htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" > temp.html
echo "[pandoc]"
pandoc -f html -t markdown < temp.html

echo "[html2text]"
html2text < temp.html

echo "[lynx]"
LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 temp.html
```


```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash test.sh
[pandoc]
[](https://github.com/JonnyBurger/remotion){.bold .ud}

# Remotion - React로 programmatically하게 동영상 만들기

[]{#topic_contents}

\- React(DOM)를 이용해 영상에 쓰일 컴포넌트를 만들고 ffmpeg로 export
가능하게 해주는 프로젝트\
- React의 장점을 영상 제작에 활용할 수 있는 것이 특징 (Reusable
components, Powerful composition, Fast Refresh, Package ecosystem)

[]{#contents4433 .comment_contents}

와, 한번 써보고 싶네요ㅎㅎ
[html2text]
# [Remotion - React로 programmatically하게 동영상
만들기](https://github.com/JonnyBurger/remotion)

\- React(DOM)를 이용해 영상에 쓰일 컴포넌트를 만들고 ffmpeg로 export 가능하게 해주는 프로젝트
\- React의 장점을 영상 제작에 활용할 수 있는 것이 특징 (Reusable components, Powerful
composition, Fast Refresh, Package ecosystem)

와, 한번 써보고 싶네요ㅎㅎ

[lynx]
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Remotion - React로 programmatically하게 동영상 만들기

   - React(DOM)를 이용해 영상에 쓰일 컴포넌트를 만들고 ffmpeg로 export 가능하게 해주는 프로젝트
   - React의 장점을 영상 제작에 활용할 수 있는 것이 특징 (Reusable components, Powerful composition, Fast Refresh, Package ecosystem)

   와, 한번 써보고 싶네요ㅎㅎ
```


---

```sh
!~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 128
Min:    37 ms
Q1:     43 ms
Median: 50 ms
Q3:     69 ms
P95:    75 ms
P99:    82 ms
Max:    137 ms
Mean:   56 ms
Total:  7206 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 128
Min:    49 ms
Q1:     57 ms
Median: 60 ms
Q3:     64 ms
P95:    81 ms
P99:    138 ms
Max:    142 ms
Mean:   62 ms
Total:  7982 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat compare_parse_twice.sh
#!/bin/bash
# profile html parse
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "htmpq_twice" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      title=$(htmlq "div.topictitle.link h1" -t -f "$htmlfile")
      md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
        '
    elif [[ "$tool" == "htmlq_once" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
      '
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "htmlq_once"
run_profiling "htmpq_twice"
```



```sh
profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "htmpq_twice" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      title=$(htmlq "div.topictitle.link h1" -t -f "$htmlfile")
      md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
        '
    elif [[ "$tool" == "htmlq_once" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
      '
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}
```

```
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 1022
Min:    37 ms
Q1:     65 ms
Median: 68 ms
Q3:     73 ms
P95:    85 ms
P99:    127 ms
Max:    189 ms
Mean:   70 ms
Total:  71567 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 1022
Min:    47 ms
Q1:     55 ms
Median: 59 ms
Q3:     62 ms
P95:    72 ms
P99:    123 ms
Max:    181 ms
Mean:   60 ms
Total:  61821 ms
```


```
[htmlq_once]

[htmlq_once STATS]
Count: 128
Min:    37 ms
Q1:     43 ms
Median: 50 ms
Q3:     69 ms
P95:    75 ms
P99:    82 ms
Max:    137 ms
Mean:   56 ms
Total:  7206 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 128
Min:    49 ms
Q1:     57 ms
Median: 60 ms
Q3:     64 ms
P95:    81 ms
P99:    138 ms
Max:    142 ms
Mean:   62 ms
Total:  7982 ms
```


/home/widehyo/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace/sp_1024/dir_html_00


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 1022
Min:    37 ms
Q1:     65 ms
Median: 68 ms
Q3:     73 ms
P95:    85 ms
P99:    127 ms
Max:    189 ms
Mean:   70 ms
Total:  71567 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 1022
Min:    47 ms
Q1:     55 ms
Median: 59 ms
Q3:     62 ms
P95:    72 ms
P99:    123 ms
Max:    181 ms
Mean:   60 ms
Total:  61821 ms
```


```sh
test_file="path/to/sample.html"  # 테스트할 파일 하나만 선택
basename_id=$(basename "${test_file%%.*}")

echo "▶ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse 완료"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title 추출 완료"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq 완료"

echo ""
echo "▶ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title 추출 완료"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: 본문 추출 완료"
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat test.sh
#!/bin/bash
test_file="html/3712.html"  # 테스트할 파일 하나만 선택
basename_id=$(basename "${test_file%%.*}")

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

echo "▶ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse 완료"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title 추출 완료"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq 완료"

echo ""
echo "▶ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title 추출 완료"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: 본문 추출 완료"
```


---


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash test.sh

[htmlq]

▶ Measuring htmlq_once pipeline...
4
htmlq_once: htmlq parse 완료
5
htmlq_once: title 추출 완료
38
htmlq_once: lynx + jq 완료

▶ Measuring htmpq_twice pipeline...
3
htmpq_twice: title 추출 완료
34
htmpq_twice: 본문 추출 완료

[pup]

▶ Measuring pup_once pipeline...
26
pup_once: pup parse 완료
4
pup_once: title 추출 완료
34
pup_once: lynx + jq 완료

▶ Measuring pup_twice pipeline...
40
pup_twice: title 추출 완료
51
pup_twice: 본문 추출 완료
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat test.sh
#!/bin/bash
test_file="html/3712.html"  # 테스트할 파일 하나만 선택
basename_id=$(basename "${test_file%%.*}")

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

echo ""
echo "[htmlq]"
echo ""

echo "▶ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse 완료"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title 추출 완료"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq 완료"

echo ""
echo "▶ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title 추출 완료"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: 본문 추출 완료"

#################
echo ""
echo "[pup]"
echo ""

echo "▶ Measuring pup_once pipeline..."
measure_time_ms '
parsed=$(pup "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "pup_once: pup parse 완료"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "pup_once: title 추출 완료"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "pup_once: lynx + jq 완료"

echo ""
echo "▶ Measuring pup_twice pipeline..."

measure_time_ms '
title=$(pup "div.topictitle.link h1 text{}" -f "'"$test_file"'")
' && echo "pup_twice: title 추출 완료"

measure_time_ms '
md=$(pup "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "pup_twice: 본문 추출 완료"
```


pup는 느리기 때문에 2번 호출이 더 느림
htmlq는 빠르기 때문에 2번 호출이 더 빠름

---

병렬실행 vs 단일실행 비교 + xargs vs parallel

현재 최종은 htmlq 두번 parsing + lynx

현재 상태에서 비교



---

bash compare_parallel_xargs.sh
flag1
single execution elapsed: 48096
flag2

thread 'main' panicked at /home/widehyo/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/htmlq-0.4.0/src/main.rs:198:37:
called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: "No such file or directory" }







```bash
#!/bin/bash
set -euo pipefail

# 설정
# target_dir='split_workspace/sp_128/dir_sp_128_00'
target_dir='split_workspace/sp_1024/dir_html_00'
outfile="bulk_data.csv"
out_xargs="bulk_xargs.csv"
out_parallel="bulk_parallel.csv"
selector='span#topic_contents, span.comment_contents'

# URL 추출 함수
url_from_filename() {
    local f="$1"
    echo "https://news.hada.io/topic?id=$(basename "${f%%.*}")"
}

# 실제 파일 처리 로직
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url=$(url_from_filename "$f")
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# 방식별 실행
run_sequential() {
    echo "[Sequential execution]"
    local start end elapsed
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$target_dir/$f"
    done > "$outfile"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "Sequential elapsed: ${elapsed} ms"
}

run_xargs() {
    echo "[xargs execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; target_dir="$2"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$target_dir" > "$out_xargs"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "xargs elapsed: ${elapsed} ms"
}

run_parallel() {
    echo "[GNU parallel execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    export target_dir
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    parallel --env target_dir -j"$nproc" '
        f="{}"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' > "$out_parallel"

    end=$(( $(date +%s%3N) ))
    elapsed=$((end - start))
    echo "parallel elapsed: ${elapsed} ms"
}

# 실행
run_sequential
run_xargs 4
run_parallel 4
```

(128, 4)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !bash
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 5225 ms
[xargs execution]
xargs elapsed: 5068 ms
[GNU parallel execution]
parallel elapsed: 6408 ms
```

(1024, 4)
```sh
!~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 57059 ms
[xargs execution]
xargs elapsed: 36924 ms
[GNU parallel execution]
parallel elapsed: 49289 ms
```


nproc = 8

(128, 8)

```sh
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 4927 ms
[xargs execution]
xargs elapsed: 2979 ms
[GNU parallel execution]
parallel elapsed: 5245 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !! 
```

(1024, 8)
```sh
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 53318 ms
[xargs execution]
xargs elapsed: 39294 ms
[GNU parallel execution]
parallel elapsed: 50280 ms
```

(128, 2)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 4963 ms
[xargs execution]
xargs elapsed: 3527 ms
[GNU parallel execution]
parallel elapsed: 6852 ms
```

(1024, 2)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 57885 ms
[xargs execution]
xargs elapsed: 40765 ms
[GNU parallel execution]
parallel elapsed: 65159 ms
```


```sh
#!/bin/bash
set -euo pipefail

# 설정
target_dir='split_workspace/sp_128/dir_sp_128_00'
# target_dir='split_workspace/sp_1024/dir_html_00'
outfile="bulk_data.csv"
out_xargs="bulk_xargs.csv"
out_parallel="bulk_parallel.csv"
selector='span#topic_contents, span.comment_contents'

# URL 추출 함수
url_from_filename() {
    local f="$1"
    echo "https://news.hada.io/topic?id=$(basename "${f%%.*}")"
}

# 실제 파일 처리 로직
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url=$(url_from_filename "$f")
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# 방식별 실행
run_sequential() {
    echo "[Sequential execution]"
    local start end elapsed
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$target_dir/$f"
    done > "$outfile"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "Sequential elapsed: ${elapsed} ms"
}

run_xargs() {
    echo "[xargs execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; target_dir="$2"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$target_dir" > "$out_xargs"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "xargs elapsed: ${elapsed} ms"
}

run_parallel() {
    echo "[GNU parallel execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    export target_dir
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    parallel --env target_dir -j"$nproc" '
        f="{}"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' > "$out_parallel"

    end=$(( $(date +%s%3N) ))
    elapsed=$((end - start))
    echo "parallel elapsed: ${elapsed} ms"
}

# 실행
run_sequential
run_xargs 2 # 4 8
run_parallel 2 # 4 8
```
```
sp_1024  sp_128  split_dir.sh  split_dir_draft.sh
~/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace $ ls sp_1024/
dir_html_00  dir_html_04  dir_html_08  dir_html_12  dir_html_16  dir_html_20  html_02.tgz  html_06.tgz  html_10.tgz  html_14.tgz  html_18.tgz
dir_html_01  dir_html_05  dir_html_09  dir_html_13  dir_html_17  dir_html_21  html_03.tgz  html_07.tgz  html_11.tgz  html_15.tgz  html_19.tgz
dir_html_02  dir_html_06  dir_html_10  dir_html_14  dir_html_18  html_00.tgz  html_04.tgz  html_08.tgz  html_12.tgz  html_16.tgz  html_20.tgz
dir_html_03  dir_html_07  dir_html_11  dir_html_15  dir_html_19  html_01.tgz  html_05.tgz  html_09.tgz  html_13.tgz  html_17.tgz  html_21.tgz
~/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace $ ls sp_128/
dir_sp_128_00  dir_sp_128_32  dir_sp_128_64    dir_sp_128_9006  dir_sp_128_9038  dir_sp_128_9070  sp_128_20.tgz  sp_128_52.tgz  sp_128_84.tgz    sp_128_9026.tgz  sp_128_9058.tgz
dir_sp_128_01  dir_sp_128_33  dir_sp_128_65    dir_sp_128_9007  dir_sp_128_9039  dir_sp_128_9071  sp_128_21.tgz  sp_128_53.tgz  sp_128_85.tgz    sp_128_9027.tgz  sp_128_9059.tgz
dir_sp_128_02  dir_sp_128_34  dir_sp_128_66    dir_sp_128_9008  dir_sp_128_9040  dir_sp_128_9072  sp_128_22.tgz  sp_128_54.tgz  sp_128_86.tgz    sp_128_9028.tgz  sp_128_9060.tgz
...
```

```sh
#!/bin/bash
set -euo pipefail

# split 디렉토리 루트
base_dir="split_workspace/sp_128"

# 대상이 되는 모든 서브디렉토리만 가져옴 (파일 제외)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_sp_128_*" | sort)

# 공통 파싱 로직
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# 각 디렉토리에 대해 반복
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$dir/output.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$dir/$f"
    done > "$outfile"
done
```

```sh
export -f process_file
export base_dir

find "$base_dir" -maxdepth 1 -type d -name "dir_sp_128_*" |
parallel --env process_file '
    dir={}
    outfile="$dir/output.csv"
    find "$dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$dir/$f"
    done > "$outfile"
'
```


TIL: HTML 파싱 성능 실험 및 Bash 병렬 처리 분석

2025-08-31

📌 목표

GeekNews HTML 파일들을 파싱해 CSV로 변환하는 파이프라인을 만들면서, 다음을 실험했다:

pup, htmlq, lynx, jq 등 도구의 성능 비교

htmlq를 한 번 호출 vs 두 번 호출할 때 성능 차이

bash에서 sequential vs xargs vs parallel 병렬 처리 성능

디렉토리 단위로 분할된 파일들을 자동으로 처리하는 스크립트 구조

⚙️ 도구별 HTML 파싱 성능 비교
도구	Mean (ms)	P95 (ms)	Max (ms)
pandoc	77	96	125
html2text	50	62	130
lynx	18	21	46

lynx가 가장 빠름. 단, HTML 구조 파악이나 CSS 선택자는 불가능.

html2text는 균형 잡힌 성능.

pandoc은 느리지만 출력 품질이 가장 안정적.

🔄 htmlq 한 번 vs 두 번 호출 비교
방식	Mean (ms)	Total (ms)
htmlq_once	70	71,567
htmlq_twice	60 ✅	61,821 ✅

한 번에 전체 DOM 파싱 후 awk, head 등으로 title/content 분리 (htmlq_once)

htmlq를 두 번 호출해 selector 별로 직접 추출 (htmlq_twice)

결과: 두 번 호출한 쪽이 오히려 빠름

이유: 전체 노드를 불러온 후 Bash에서 후처리하는 비용(head/awk/jq 등)이 더 큼

⚙️ 병렬 처리 비교: Sequential vs xargs vs GNU parallel
파일 수	방식	Time (ms)
128	Sequential	4,927
	xargs	2,979 ✅
	parallel	5,245
1024	Sequential	53,318
	xargs	39,294 ✅
	parallel	50,280

xargs -P $(nproc) 방식이 가장 효율적

parallel은 쉘 초기화 비용 + 순서 보존 로직이 있어 오히려 느림

병렬 실행이 항상 빠르지는 않으며, I/O 병목, 작업 크기, 부가 오버헤드를 고려해야 함

📁 디렉토리 단위 CSV 생성 자동화
폴더 구조
split_workspace/
├── sp_128/
│   ├── dir_sp_128_00/
│   ├── dir_sp_128_01/
│   ├── ...

자동 처리 스크립트 요약
for dir in split_workspace/sp_128/dir_sp_128_*; do
    outfile="$dir/output.csv"
    find "$dir" -name "*.html" | sort | while read file; do
        # htmlq + lynx + jq 조합으로 파싱
        echo "\"$url\",\"$title\",$content"
    done > "$outfile"
done


디렉토리별로 파일을 읽고, 각 디렉토리마다 개별 CSV 출력

구조화된 분할 처리로 재사용성과 병렬화 확장성 확보

🧠 배운 점 요약

병렬화는 비용이 적은 작업일수록 오히려 더 느려질 수 있다.

xargs -P는 성능과 간결함 면에서 우수하다.

파이프라인 중 lynx, jq와 같은 후처리 도구들이 성능의 주요 병목이 될 수 있다.

Bash에서도 충분히 구조화된 데이터 파이프라인을 짤 수 있으며, 디렉토리 단위 반복 처리가 유용하다.

selector를 잘게 나누어 htmlq를 여러 번 호출하는 것이 더 효율적일 수 있다.

---


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat parse_htmlq_xargs.sh
#!/bin/bash
# bench_xargs.sh
set -euo pipefail
# 설정
base_dir='split_workspace/sp_1024'
out_xargs="bulk_xargs.csv"
selector='span#topic_contents, span.comment_contents'

nproc=${1:-$(nproc)}

# 대상이 되는 모든 서브디렉토리만 가져옴 (파일 제외)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_*" | sort)

echo "nproc: $nproc"
# 각 디렉토리에 대해 반복
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$dir/output.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; dir="$2"
        fullpath="$dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$dir" > "$outfile"
done
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash parse_htmlq_xargs.sh 4
nproc: 4
[Processing: split_workspace/sp_1024/dir_html_00]
[Processing: split_workspace/sp_1024/dir_html_01]
[Processing: split_workspace/sp_1024/dir_html_02]
[Processing: split_workspace/sp_1024/dir_html_03]
[Processing: split_workspace/sp_1024/dir_html_04]
[Processing: split_workspace/sp_1024/dir_html_05]
[Processing: split_workspace/sp_1024/dir_html_06]
[Processing: split_workspace/sp_1024/dir_html_07]
[Processing: split_workspace/sp_1024/dir_html_08]
[Processing: split_workspace/sp_1024/dir_html_09]
[Processing: split_workspace/sp_1024/dir_html_10]
[Processing: split_workspace/sp_1024/dir_html_11]
[Processing: split_workspace/sp_1024/dir_html_12]
[Processing: split_workspace/sp_1024/dir_html_13]
[Processing: split_workspace/sp_1024/dir_html_14]
[Processing: split_workspace/sp_1024/dir_html_15]
[Processing: split_workspace/sp_1024/dir_html_16]
[Processing: split_workspace/sp_1024/dir_html_17]
[Processing: split_workspace/sp_1024/dir_html_18]
[Processing: split_workspace/sp_1024/dir_html_19]
[Processing: split_workspace/sp_1024/dir_html_20]
[Processing: split_workspace/sp_1024/dir_html_21]
```

```awk
{
  printf "ls %s/output.csv\n", $0
}
```
```awk
{
  printf "\\COPY geeknews_xargs (url, title, content) FROM '%s/output.csv' WITH (FORMAT csv);\n", $0
}
```


```sql
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_00/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_01/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_02/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_03/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_04/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_05/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_06/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_07/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_08/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_09/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_10/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_11/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_12/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_13/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_14/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_15/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_16/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_17/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_18/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_19/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_20/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_21/output.csv' WITH (FORMAT csv);
```



```bash
#!/bin/bash
# bench_xargs.sh
set -euo pipefail
# 설정
# base_dir='split_workspace/sp_1024'
base_dir='split_workspace/sp_128'
selector='span#topic_contents, span.comment_contents'

nproc=${1:-$(nproc)}

# 대상이 되는 모든 서브디렉토리만 가져옴 (파일 제외)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_*" | sort)

echo "nproc: $nproc"
# 각 디렉토리에 대해 반복
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$base_dir/output_${dir##*_}.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; dir="$2"
        fullpath="$dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$dir" > "$outfile"
done
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_htmlq_xargs.sh 4
nproc: 4
[Processing: split_workspace/sp_128/dir_sp_128_00]
[Processing: split_workspace/sp_128/dir_sp_128_01]
[Processing: split_workspace/sp_128/dir_sp_128_02]
[Processing: split_workspace/sp_128/dir_sp_128_03]
[Processing: split_workspace/sp_128/dir_sp_128_04]
[Processing: split_workspace/sp_128/dir_sp_128_05]
[Processing: split_workspace/sp_128/dir_sp_128_06]
[Processing: split_workspace/sp_128/dir_sp_128_07]
[Processing: split_workspace/sp_128/dir_sp_128_08]
[Processing: split_workspace/sp_128/dir_sp_128_09]
[Processing: split_workspace/sp_128/dir_sp_128_10]
[Processing: split_workspace/sp_128/dir_sp_128_11]
[Processing: split_workspace/sp_128/dir_sp_128_12]
[Processing: split_workspace/sp_128/dir_sp_128_13]
[Processing: split_workspace/sp_128/dir_sp_128_14]
[Processing: split_workspace/sp_128/dir_sp_128_15]
[Processing: split_workspace/sp_128/dir_sp_128_16]
[Processing: split_workspace/sp_128/dir_sp_128_17]
[Processing: split_workspace/sp_128/dir_sp_128_18]
[Processing: split_workspace/sp_128/dir_sp_128_19]
[Processing: split_workspace/sp_128/dir_sp_128_20]
[Processing: split_workspace/sp_128/dir_sp_128_21]
[Processing: split_workspace/sp_128/dir_sp_128_22]
[Processing: split_workspace/sp_128/dir_sp_128_23]
[Processing: split_workspace/sp_128/dir_sp_128_24]
[Processing: split_workspace/sp_128/dir_sp_128_25]
[Processing: split_workspace/sp_128/dir_sp_128_26]
[Processing: split_workspace/sp_128/dir_sp_128_27]
[Processing: split_workspace/sp_128/dir_sp_128_28]
[Processing: split_workspace/sp_128/dir_sp_128_29]
[Processing: split_workspace/sp_128/dir_sp_128_30]
[Processing: split_workspace/sp_128/dir_sp_128_31]
[Processing: split_workspace/sp_128/dir_sp_128_32]
[Processing: split_workspace/sp_128/dir_sp_128_33]
[Processing: split_workspace/sp_128/dir_sp_128_34]
[Processing: split_workspace/sp_128/dir_sp_128_35]
[Processing: split_workspace/sp_128/dir_sp_128_36]
[Processing: split_workspace/sp_128/dir_sp_128_37]
[Processing: split_workspace/sp_128/dir_sp_128_38]
[Processing: split_workspace/sp_128/dir_sp_128_39]
[Processing: split_workspace/sp_128/dir_sp_128_40]
[Processing: split_workspace/sp_128/dir_sp_128_41]
[Processing: split_workspace/sp_128/dir_sp_128_42]
[Processing: split_workspace/sp_128/dir_sp_128_43]
[Processing: split_workspace/sp_128/dir_sp_128_44]
[Processing: split_workspace/sp_128/dir_sp_128_45]
[Processing: split_workspace/sp_128/dir_sp_128_46]
[Processing: split_workspace/sp_128/dir_sp_128_47]
[Processing: split_workspace/sp_128/dir_sp_128_48]
[Processing: split_workspace/sp_128/dir_sp_128_49]
[Processing: split_workspace/sp_128/dir_sp_128_50]
[Processing: split_workspace/sp_128/dir_sp_128_51]
[Processing: split_workspace/sp_128/dir_sp_128_52]
[Processing: split_workspace/sp_128/dir_sp_128_53]
[Processing: split_workspace/sp_128/dir_sp_128_54]
[Processing: split_workspace/sp_128/dir_sp_128_55]
[Processing: split_workspace/sp_128/dir_sp_128_56]
[Processing: split_workspace/sp_128/dir_sp_128_57]
[Processing: split_workspace/sp_128/dir_sp_128_58]
[Processing: split_workspace/sp_128/dir_sp_128_59]
[Processing: split_workspace/sp_128/dir_sp_128_60]
[Processing: split_workspace/sp_128/dir_sp_128_61]
[Processing: split_workspace/sp_128/dir_sp_128_62]
[Processing: split_workspace/sp_128/dir_sp_128_63]
[Processing: split_workspace/sp_128/dir_sp_128_64]
[Processing: split_workspace/sp_128/dir_sp_128_65]
[Processing: split_workspace/sp_128/dir_sp_128_66]
[Processing: split_workspace/sp_128/dir_sp_128_67]
[Processing: split_workspace/sp_128/dir_sp_128_68]
[Processing: split_workspace/sp_128/dir_sp_128_69]
[Processing: split_workspace/sp_128/dir_sp_128_70]
[Processing: split_workspace/sp_128/dir_sp_128_71]
[Processing: split_workspace/sp_128/dir_sp_128_72]
[Processing: split_workspace/sp_128/dir_sp_128_73]
[Processing: split_workspace/sp_128/dir_sp_128_74]
[Processing: split_workspace/sp_128/dir_sp_128_75]
[Processing: split_workspace/sp_128/dir_sp_128_76]
[Processing: split_workspace/sp_128/dir_sp_128_77]
[Processing: split_workspace/sp_128/dir_sp_128_78]
[Processing: split_workspace/sp_128/dir_sp_128_79]
[Processing: split_workspace/sp_128/dir_sp_128_80]
[Processing: split_workspace/sp_128/dir_sp_128_81]
[Processing: split_workspace/sp_128/dir_sp_128_82]
[Processing: split_workspace/sp_128/dir_sp_128_83]
[Processing: split_workspace/sp_128/dir_sp_128_84]
[Processing: split_workspace/sp_128/dir_sp_128_85]
[Processing: split_workspace/sp_128/dir_sp_128_86]
[Processing: split_workspace/sp_128/dir_sp_128_87]
[Processing: split_workspace/sp_128/dir_sp_128_88]
[Processing: split_workspace/sp_128/dir_sp_128_89]
[Processing: split_workspace/sp_128/dir_sp_128_9000]
[Processing: split_workspace/sp_128/dir_sp_128_9001]
[Processing: split_workspace/sp_128/dir_sp_128_9002]
[Processing: split_workspace/sp_128/dir_sp_128_9003]
[Processing: split_workspace/sp_128/dir_sp_128_9004]
[Processing: split_workspace/sp_128/dir_sp_128_9005]
[Processing: split_workspace/sp_128/dir_sp_128_9006]
[Processing: split_workspace/sp_128/dir_sp_128_9007]
[Processing: split_workspace/sp_128/dir_sp_128_9008]
[Processing: split_workspace/sp_128/dir_sp_128_9009]
[Processing: split_workspace/sp_128/dir_sp_128_9010]
[Processing: split_workspace/sp_128/dir_sp_128_9011]
[Processing: split_workspace/sp_128/dir_sp_128_9012]
[Processing: split_workspace/sp_128/dir_sp_128_9013]
[Processing: split_workspace/sp_128/dir_sp_128_9014]
[Processing: split_workspace/sp_128/dir_sp_128_9015]
[Processing: split_workspace/sp_128/dir_sp_128_9016]
[Processing: split_workspace/sp_128/dir_sp_128_9017]
[Processing: split_workspace/sp_128/dir_sp_128_9018]
[Processing: split_workspace/sp_128/dir_sp_128_9019]
[Processing: split_workspace/sp_128/dir_sp_128_9020]
[Processing: split_workspace/sp_128/dir_sp_128_9021]
[Processing: split_workspace/sp_128/dir_sp_128_9022]
[Processing: split_workspace/sp_128/dir_sp_128_9023]
[Processing: split_workspace/sp_128/dir_sp_128_9024]
[Processing: split_workspace/sp_128/dir_sp_128_9025]
[Processing: split_workspace/sp_128/dir_sp_128_9026]
[Processing: split_workspace/sp_128/dir_sp_128_9027]
[Processing: split_workspace/sp_128/dir_sp_128_9028]
[Processing: split_workspace/sp_128/dir_sp_128_9029]
[Processing: split_workspace/sp_128/dir_sp_128_9030]
[Processing: split_workspace/sp_128/dir_sp_128_9031]
[Processing: split_workspace/sp_128/dir_sp_128_9032]
[Processing: split_workspace/sp_128/dir_sp_128_9033]
[Processing: split_workspace/sp_128/dir_sp_128_9034]
[Processing: split_workspace/sp_128/dir_sp_128_9035]
[Processing: split_workspace/sp_128/dir_sp_128_9036]
[Processing: split_workspace/sp_128/dir_sp_128_9037]
[Processing: split_workspace/sp_128/dir_sp_128_9038]
[Processing: split_workspace/sp_128/dir_sp_128_9039]
[Processing: split_workspace/sp_128/dir_sp_128_9040]
[Processing: split_workspace/sp_128/dir_sp_128_9041]
[Processing: split_workspace/sp_128/dir_sp_128_9042]
[Processing: split_workspace/sp_128/dir_sp_128_9043]
[Processing: split_workspace/sp_128/dir_sp_128_9044]
[Processing: split_workspace/sp_128/dir_sp_128_9045]
[Processing: split_workspace/sp_128/dir_sp_128_9046]
[Processing: split_workspace/sp_128/dir_sp_128_9047]
[Processing: split_workspace/sp_128/dir_sp_128_9048]
[Processing: split_workspace/sp_128/dir_sp_128_9049]
[Processing: split_workspace/sp_128/dir_sp_128_9050]
[Processing: split_workspace/sp_128/dir_sp_128_9051]
[Processing: split_workspace/sp_128/dir_sp_128_9052]
[Processing: split_workspace/sp_128/dir_sp_128_9053]
[Processing: split_workspace/sp_128/dir_sp_128_9054]
[Processing: split_workspace/sp_128/dir_sp_128_9055]
[Processing: split_workspace/sp_128/dir_sp_128_9056]
[Processing: split_workspace/sp_128/dir_sp_128_9057]
[Processing: split_workspace/sp_128/dir_sp_128_9058]
[Processing: split_workspace/sp_128/dir_sp_128_9059]
[Processing: split_workspace/sp_128/dir_sp_128_9060]
[Processing: split_workspace/sp_128/dir_sp_128_9061]
[Processing: split_workspace/sp_128/dir_sp_128_9062]
[Processing: split_workspace/sp_128/dir_sp_128_9063]
[Processing: split_workspace/sp_128/dir_sp_128_9064]
[Processing: split_workspace/sp_128/dir_sp_128_9065]
[Processing: split_workspace/sp_128/dir_sp_128_9066]
[Processing: split_workspace/sp_128/dir_sp_128_9067]
[Processing: split_workspace/sp_128/dir_sp_128_9068]
[Processing: split_workspace/sp_128/dir_sp_128_9069]
[Processing: split_workspace/sp_128/dir_sp_128_9070]
[Processing: split_workspace/sp_128/dir_sp_128_9071]
[Processing: split_workspace/sp_128/dir_sp_128_9072]
[Processing: split_workspace/sp_128/dir_sp_128_9073]
[Processing: split_workspace/sp_128/dir_sp_128_9074]
[Processing: split_workspace/sp_128/dir_sp_128_9075]
[Processing: split_workspace/sp_128/dir_sp_128_9076]
[Processing: split_workspace/sp_128/dir_sp_128_9077]
[Processing: split_workspace/sp_128/dir_sp_128_9078]
[Processing: split_workspace/sp_128/dir_sp_128_9079]
[Processing: split_workspace/sp_128/dir_sp_128_9080]
[Processing: split_workspace/sp_128/dir_sp_128_9081]

real    50m40.390s
user    324m48.741s
sys     52m42.911s
```


```awk
/csv/ {
  printf "\\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_128/%s' WITH (FORMAT csv);\n", $0
}

```

```
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash bulk_insert.sh
Password for user postgres:
COPY 128
COPY 127
psql:bulk_insert.sql:28: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 160: ""
psql:bulk_insert.sql:29: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 79: "   아마존 CTO가 직접 쓴 ""AWS가 현대적인 어플리케이션을 개발 하는 법"""
psql:bulk_insert.sql:30: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 4: ""
psql:bulk_insert.sql:31: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 887: ""https://news.hada.io/topic?id=744","리브라 협회 "G7 보고서, 리브라 가능성 인정""," ..."
COPY 128
psql:bulk_insert.sql:33: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 720: ""https://news.hada.io/topic?id=938","Google Stadia "스트리밍 기술은 훌륭, 게임이 얼마..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:38: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1205: ""https://news.hada.io/topic?id=1649",""실시간 협업 애플리케이션을 위한 프레임워크..."
COPY 128
COPY 128
psql:bulk_insert.sql:41: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 29: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:45: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 433: ""
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:55: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1087: ""https://news.hada.io/topic?id=3802",""그런데, 어떻게 시작하나요?" 엔지니어로써의 ..."
COPY 127
COPY 128
psql:bulk_insert.sql:58: ERROR:  value too long for type character varying(200)
CONTEXT:  COPY geeknews_xargs, line 85, column url: "

   - child_process 에 대한 편리한 래퍼(Wrapper)를 제공
   - $`명령` 으로 쉘커맨..."
COPY 128
COPY 128
psql:bulk_insert.sql:61: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 1537: ""
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:68: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 811: ""https://news.hada.io/topic?id=5579","Kinesis Advantage 360 Pro 인체공학 키보드","           ..."
COPY 128
psql:bulk_insert.sql:70: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 122: "   https://apps.apple.com/us/app/ago-얼마나-지났지/id1602942232"
psql:bulk_insert.sql:71: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 232, column title: "라인 12년차 PM 왈 — “인성이 실력이다 - 좌충우돌 스따뜨업 이야기,         ..."
COPY 128
psql:bulk_insert.sql:73: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 453: ""https://news.hada.io/topic?id=6202","Vimeo: "우리는 B2B 솔루션이지, 인디 버전의 유튜..."
COPY 128
COPY 128
psql:bulk_insert.sql:76: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1192: ""https://news.hada.io/topic?id=6585","Ask GN: 이번 주말에 뭐 하시나요?","                 ..."
psql:bulk_insert.sql:77: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 6: "     * Frontier는 휴렛 팩커드 엔터프라이즈의 HPE Cray EX 슈퍼컴퓨터 시스템으..."
COPY 128
psql:bulk_insert.sql:79: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 136: ""
COPY 127
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:87: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1488: ""https://news.hada.io/topic?id=8015","ChatGPT로 주차 과태료 이의제기 메일을 작성해..."
psql:bulk_insert.sql:88: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2437: "   maclloc은 malloc의 오타겠죠???"
COPY 128
COPY 128
psql:bulk_insert.sql:91: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1294: ""https://news.hada.io/topic?id=8492","Bing AI: "당신이 나를 해치지 않는 한, 나는 당신..."
COPY 128
psql:bulk_insert.sql:93: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 1199, column title: "The Original 5¼ Disk Sleeve Archive,                                                               ..."
COPY 128
psql:bulk_insert.sql:95: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2258: ""https://news.hada.io/topic?id=9129","구글, "(AI에 대해서) 우린 해자가 없음, OpenAI도 ..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:99: ERROR:  value too long for type character varying(200)
CONTEXT:  COPY geeknews_xargs, line 719, column url: "

     * 트위터는 이제 트윗을 보려면 계정이 필요합니다.
     * 이 변경 사항..."
psql:bulk_insert.sql:100: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1081: ""https://news.hada.io/topic?id=9817",""우리의 유료 고객들은 X가 필요합니다, 언제 ..."
COPY 128
COPY 128
COPY 128
COPY 127
COPY 128
psql:bulk_insert.sql:106: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2571: "     * 고급 데이터 분석(이전의 코드 해석기 ) 에 대한 무제한 액세스"
COPY 128
psql:bulk_insert.sql:108: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 1778: "   exa 가 윈도우를 지원 안해서 lsd 를 썼어야 했는데, eza는 윈도우 바이너리..."
psql:bulk_insert.sql:109: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 1514: ""
COPY 128
psql:bulk_insert.sql:111: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 1012: "     * 해당 소송은 HP가 이 사실을 소비자에게 공개하지 않았으며, 이는 비싼..."
COPY 128
psql:bulk_insert.sql:113: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 600: "     * SiFive는 Jim Keller가 이끄는 Tenstorrent를 포함한 RISC-V의 고급 분야에서 다..."
psql:bulk_insert.sql:114: ERROR:  invalid byte sequence for encoding "UTF8": 0xec 0xa0 0x22
CONTEXT:  COPY geeknews_xargs, line 472
COPY 128
psql:bulk_insert.sql:116: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 79: ""
psql:bulk_insert.sql:117: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 85: ""https://news.hada.io/topic?id=12002",""테스트, [, 그리고 [[ (2020)" - 기술명칭 유지"," ..."
COPY 128
COPY 128
psql:bulk_insert.sql:120: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 4585: ""https://news.hada.io/topic?id=12450",""돈 스타브, 디아블로" 기술을 탑재한 "패럴랙..."
COPY 128
COPY 128
psql:bulk_insert.sql:123: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3879: ""https://news.hada.io/topic?id=12750",""LibreOffice, 구형 워드 파일 읽기에서 워드보다 ..."
COPY 128
psql:bulk_insert.sql:125: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2949: ""
COPY 128
psql:bulk_insert.sql:127: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 338: ""https://news.hada.io/topic?id=13286",""제미니, 비윤리적이라 C#에서 메모리를 가장 ..."
psql:bulk_insert.sql:128: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3012: ""https://news.hada.io/topic?id=13444","Groq, Mixtral 8x7B-32k를 500 T/s로 구동","               ..."
psql:bulk_insert.sql:129: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2093: ""https://news.hada.io/topic?id=13582",""엄마, 행렬이 없어도 돼"","                         ..."
psql:bulk_insert.sql:130: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 575, column title: "Eloquent JavaScript(자바스크립트 스킬업) 4판 (2024년)",                                  ..."
psql:bulk_insert.sql:131: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 606: ""https://news.hada.io/topic?id=13897",""어머나, 내 앱이 성공했는데 접근성을 고려하..."
psql:bulk_insert.sql:132: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 765: ""https://news.hada.io/topic?id=13998",""LLMs의 '급발진' 능력, 사실은 점진적이고 예측..."
COPY 128
COPY 128
psql:bulk_insert.sql:135: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 5143: ""
COPY 127
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 127
psql:bulk_insert.sql:143: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 4002: ""
psql:bulk_insert.sql:144: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 3296: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:148: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2917: ""https://news.hada.io/topic?id=16263","Urchin Software Corp: Google Analytics의 예상치 못한 ..."
COPY 127
psql:bulk_insert.sql:150: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3598: ""https://news.hada.io/topic?id=16478","NSA, Grace Hopper 제독의 "미래의 가능성: 데이터, ..."
psql:bulk_insert.sql:151: ERROR:  invalid byte sequence for encoding "UTF8": 0xea 0xb8 0x22
CONTEXT:  COPY geeknews_xargs, line 7566
COPY 128
psql:bulk_insert.sql:153: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 6590: ""
COPY 128
psql:bulk_insert.sql:155: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2977: ""https://news.hada.io/topic?id=17200","Tesla의 Robotaxi 발표 이벤트 "We, Robot"","            ..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:160: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 136: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:164: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 5713: ""
psql:bulk_insert.sql:165: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 654: "     * 링크 및 감사"
COPY 128
COPY 128
psql:bulk_insert.sql:168: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2781: "     * Power Mode를 떠올리게 한다며 Visual Studio Code의 Power Mode 링크를 공유함"
COPY 128
psql:bulk_insert.sql:170: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 3704: "          + 명령어는 구글링으로 쉽게 찾을 수 있으며, 올바른 정신 모델이 중..."
psql:bulk_insert.sql:171: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 219: "     * £320로 4개의 Pi5를 구매할 수 있지만, 중고 12GB 3080을 찾으면 아마도 10..."
psql:bulk_insert.sql:172: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 3123: ""
COPY 128
COPY 128
psql:bulk_insert.sql:175: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 627, column title: "It’s Not As Simple As “Use A Memory Safe Language",                                             ..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:179: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 5543: ""https://news.hada.io/topic?id=20305","SVG로 만드는 멋진 것들","                            ..."
COPY 127
COPY 128
COPY 128
psql:bulk_insert.sql:183: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 5273: ""https://news.hada.io/topic?id=20786","스티브 잡스에게서 온 "좋은 아이디어, 고마워..."
psql:bulk_insert.sql:184: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 6669: ""https://news.hada.io/topic?id=20959","MIT, arXiv에 "AI와 과학적 발견, 그리고 제품 혁..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:188: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 4062: ""https://news.hada.io/topic?id=21518","오픈AI와 마이크로소프트의 갈등이 임계점에 ..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:193: ERROR:  invalid byte sequence for encoding "UTF8": 0xeb 0xb3 0x22
CONTEXT:  COPY geeknews_xargs, line 3226
psql:bulk_insert.sql:194: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 7848: ""
psql:bulk_insert.sql:195: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1495: ""https://news.hada.io/topic?id=22455","Paypal CTO "더 나은 리더가 되기 위해, 코딩을 포..."
COPY 128
COPY 120
```


csvclean -a split_workspace/sp_128/output_01.csv 1>/dev/null



