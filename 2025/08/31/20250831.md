```sh
#!/bin/bash
# bench_xargs.sh

out="bulk_data_xargs_v2.csv"

date
# find sample/ -name '*.html' \
find html/ -name '*.html' \
  | xargs -P "$(nproc)" -I {} bash -c '
      f="{}"
      url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$f")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | pandoc -f html -t markdown | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md"
  ' > "$out"
date
```

```sh
#!/bin/bash
# htmlqÎ°ú contentÎßå Ï∂îÏ∂ú
htmlfile='html/3172.html'
selector='span#topic_contents, span.comment_contents'

echo "[pup]"
time pup "$selector" < "$htmlfile" > /dev/null

echo "[htmlq]"
time htmlq "$selector" < "$htmlfile" > /dev/null
```



```sh
#!/bin/bash
# profile convert text
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "pandoc" ]]; then
        measure_time_ms "pandoc -f html -t markdown < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "html2text" ]]; then
        measure_time_ms "html2text < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "lynx" ]]; then
        measure_time_ms "LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 \"$htmlfile\" > /dev/null"
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath" "$selector")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "pandoc"
run_profiling "html2text"
run_profiling "lynx"
```

```sh
#!/bin/bash
# profile html parse
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"
    local selector="$3"

    if [[ "$tool" == "pup" ]]; then
        measure_time_ms "pup \"$selector\" < \"$htmlfile\" > /dev/null"
    elif [[ "$tool" == "htmlq" ]]; then
        measure_time_ms "htmlq \"$selector\" < \"$htmlfile\" > /dev/null"
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath" "$selector")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "pup"
run_profiling "htmlq"
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !bash
bash profile_html_parse.sh

[pup]

[pup STATS]
Count: 128
Min:    22 ms
Q1:     25 ms
Median: 25 ms
Q3:     27 ms
P95:    29 ms
P99:    32 ms
Max:    32 ms
Mean:   25 ms
Total:  3285 ms

[htmlq]

[htmlq STATS]
Count: 128
Min:    2 ms
Q1:     3 ms
Median: 3 ms
Q3:     4 ms
P95:    5 ms
P99:    6 ms
Max:    21 ms
Mean:   3 ms
Total:  463 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash profile_convert_text.sh

[pandoc]

[pandoc STATS]
Count: 128
Min:    64 ms
Q1:     68 ms
Median: 70 ms
Q3:     73 ms
P95:    80 ms
P99:    146 ms
Max:    160 ms
Mean:   72 ms
Total:  9277 ms

[html2text]

[html2text STATS]
Count: 128
Min:    43 ms
Q1:     46 ms
Median: 48 ms
Q3:     50 ms
P95:    58 ms
P99:    92 ms
Max:    99 ms
Mean:   49 ms
Total:  6293 ms

[lynx]

[lynx STATS]
Count: 128
Min:    15 ms
Q1:     17 ms
Median: 18 ms
Q3:     19 ms
P95:    21 ms
P99:    24 ms
Max:    24 ms
Mean:   18 ms
Total:  2308 ms
```


```sh
#!/bin/bash
htmlfile='html/3712.html'
selector='span#topic_contents, span.comment_contents'

htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" > temp.html
echo "[pandoc]"
pandoc -f html -t markdown < temp.html

echo "[html2text]"
html2text < temp.html

echo "[lynx]"
LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 temp.html
```


```bash
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash test.sh
[pandoc]
[](https://github.com/JonnyBurger/remotion){.bold .ud}

# Remotion - ReactÎ°ú programmaticallyÌïòÍ≤å ÎèôÏòÅÏÉÅ ÎßåÎì§Í∏∞

[]{#topic_contents}

\- React(DOM)Î•º Ïù¥Ïö©Ìï¥ ÏòÅÏÉÅÏóê Ïì∞Ïùº Ïª¥Ìè¨ÎÑåÌä∏Î•º ÎßåÎì§Í≥† ffmpegÎ°ú export
Í∞ÄÎä•ÌïòÍ≤å Ìï¥Ï£ºÎäî ÌîÑÎ°úÏ†ùÌä∏\
- ReactÏùò Ïû•Ï†êÏùÑ ÏòÅÏÉÅ Ï†úÏûëÏóê ÌôúÏö©Ìï† Ïàò ÏûàÎäî Í≤ÉÏù¥ ÌäπÏßï (Reusable
components, Powerful composition, Fast Refresh, Package ecosystem)

[]{#contents4433 .comment_contents}

ÏôÄ, ÌïúÎ≤à Ïç®Î≥¥Í≥† Ïã∂ÎÑ§Ïöî„Öé„Öé
[html2text]
# [Remotion - ReactÎ°ú programmaticallyÌïòÍ≤å ÎèôÏòÅÏÉÅ
ÎßåÎì§Í∏∞](https://github.com/JonnyBurger/remotion)

\- React(DOM)Î•º Ïù¥Ïö©Ìï¥ ÏòÅÏÉÅÏóê Ïì∞Ïùº Ïª¥Ìè¨ÎÑåÌä∏Î•º ÎßåÎì§Í≥† ffmpegÎ°ú export Í∞ÄÎä•ÌïòÍ≤å Ìï¥Ï£ºÎäî ÌîÑÎ°úÏ†ùÌä∏
\- ReactÏùò Ïû•Ï†êÏùÑ ÏòÅÏÉÅ Ï†úÏûëÏóê ÌôúÏö©Ìï† Ïàò ÏûàÎäî Í≤ÉÏù¥ ÌäπÏßï (Reusable components, Powerful
composition, Fast Refresh, Package ecosystem)

ÏôÄ, ÌïúÎ≤à Ïç®Î≥¥Í≥† Ïã∂ÎÑ§Ïöî„Öé„Öé

[lynx]
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Remotion - ReactÎ°ú programmaticallyÌïòÍ≤å ÎèôÏòÅÏÉÅ ÎßåÎì§Í∏∞

   - React(DOM)Î•º Ïù¥Ïö©Ìï¥ ÏòÅÏÉÅÏóê Ïì∞Ïùº Ïª¥Ìè¨ÎÑåÌä∏Î•º ÎßåÎì§Í≥† ffmpegÎ°ú export Í∞ÄÎä•ÌïòÍ≤å Ìï¥Ï£ºÎäî ÌîÑÎ°úÏ†ùÌä∏
   - ReactÏùò Ïû•Ï†êÏùÑ ÏòÅÏÉÅ Ï†úÏûëÏóê ÌôúÏö©Ìï† Ïàò ÏûàÎäî Í≤ÉÏù¥ ÌäπÏßï (Reusable components, Powerful composition, Fast Refresh, Package ecosystem)

   ÏôÄ, ÌïúÎ≤à Ïç®Î≥¥Í≥† Ïã∂ÎÑ§Ïöî„Öé„Öé
```


---

```sh
!~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 128
Min:    37 ms
Q1:     43 ms
Median: 50 ms
Q3:     69 ms
P95:    75 ms
P99:    82 ms
Max:    137 ms
Mean:   56 ms
Total:  7206 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 128
Min:    49 ms
Q1:     57 ms
Median: 60 ms
Q3:     64 ms
P95:    81 ms
P99:    138 ms
Max:    142 ms
Mean:   62 ms
Total:  7982 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat compare_parse_twice.sh
#!/bin/bash
# profile html parse
set -euo pipefail
target_dir='split_workspace/sp_128/dir_sp_128_00'
selector='span#topic_contents, span.comment_contents'

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "htmpq_twice" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      title=$(htmlq "div.topictitle.link h1" -t -f "$htmlfile")
      md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
        '
    elif [[ "$tool" == "htmlq_once" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
      '
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}

summarize_times() {
    local label="$1"
    shift
    local times=("$@")

    printf "\n[%s STATS]\n" "$label"
    printf "Count: %d\n" "${#times[@]}"

    local sorted_times
    sorted_times=$(printf '%s\n' "${times[@]}" | sort -n)
    local count=${#times[@]}

    # min, max
    local min=$(echo "$sorted_times" | head -n1)
    local max=$(echo "$sorted_times" | tail -n1)

    # mean
    local sum=0
    for t in "${times[@]}"; do
        sum=$((sum + t))
    done
    local mean=$((sum / count))

    # helper: percentile index with rounding up
    percentile_idx() {
        local pct=$1
        echo $(( (count * pct + 99) / 100 ))
    }

    local p25_idx=$(percentile_idx 25)
    local p50_idx=$(percentile_idx 50)
    local p75_idx=$(percentile_idx 75)
    local p95_idx=$(percentile_idx 95)
    local p99_idx=$(percentile_idx 99)

    local q1=$(echo "$sorted_times" | sed -n "${p25_idx}p")
    local median=$(echo "$sorted_times" | sed -n "${p50_idx}p")
    local q3=$(echo "$sorted_times" | sed -n "${p75_idx}p")
    local p95=$(echo "$sorted_times" | sed -n "${p95_idx}p")
    local p99=$(echo "$sorted_times" | sed -n "${p99_idx}p")

    echo "Min:    $min ms"
    echo "Q1:     $q1 ms"
    echo "Median: $median ms"
    echo "Q3:     $q3 ms"
    echo "P95:    $p95 ms"
    echo "P99:    $p99 ms"
    echo "Max:    $max ms"
    echo "Mean:   $mean ms"
    echo "Total:  $sum ms"
}

run_profiling() {
    local tool="$1"
    local -a results=()

    echo ""
    echo "[$tool]"
    while IFS= read -r f; do
        local fullpath="$target_dir/$f"
        # echo "$f"
        local t
        t=$(profile_tool "$tool" "$fullpath")
        results+=("$t")
    done < <(find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n)

    summarize_times "$tool" "${results[@]}"
}

# Run both tools
run_profiling "htmlq_once"
run_profiling "htmpq_twice"
```



```sh
profile_tool() {
    local tool="$1"
    local htmlfile="$2"

    if [[ "$tool" == "htmpq_twice" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      title=$(htmlq "div.topictitle.link h1" -t -f "$htmlfile")
      md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
        '
    elif [[ "$tool" == "htmlq_once" ]]; then
        measure_time_ms '
      url="https://news.hada.io/topic?id=$(basename "${htmlfile%%.*}")"
      parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$htmlfile")
      title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
      md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
      echo "\"$url\",\"$title\",$md" > /dev/null
      '
    else
        echo "Unknown tool: $tool" >&2
        return 1
    fi
}
```

```
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 1022
Min:    37 ms
Q1:     65 ms
Median: 68 ms
Q3:     73 ms
P95:    85 ms
P99:    127 ms
Max:    189 ms
Mean:   70 ms
Total:  71567 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 1022
Min:    47 ms
Q1:     55 ms
Median: 59 ms
Q3:     62 ms
P95:    72 ms
P99:    123 ms
Max:    181 ms
Mean:   60 ms
Total:  61821 ms
```


```
[htmlq_once]

[htmlq_once STATS]
Count: 128
Min:    37 ms
Q1:     43 ms
Median: 50 ms
Q3:     69 ms
P95:    75 ms
P99:    82 ms
Max:    137 ms
Mean:   56 ms
Total:  7206 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 128
Min:    49 ms
Q1:     57 ms
Median: 60 ms
Q3:     64 ms
P95:    81 ms
P99:    138 ms
Max:    142 ms
Mean:   62 ms
Total:  7982 ms
```


/home/widehyo/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace/sp_1024/dir_html_00


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash compare_parse_twice.sh

[htmlq_once]

[htmlq_once STATS]
Count: 1022
Min:    37 ms
Q1:     65 ms
Median: 68 ms
Q3:     73 ms
P95:    85 ms
P99:    127 ms
Max:    189 ms
Mean:   70 ms
Total:  71567 ms

[htmpq_twice]

[htmpq_twice STATS]
Count: 1022
Min:    47 ms
Q1:     55 ms
Median: 59 ms
Q3:     62 ms
P95:    72 ms
P99:    123 ms
Max:    181 ms
Mean:   60 ms
Total:  61821 ms
```


```sh
test_file="path/to/sample.html"  # ÌÖåÏä§Ìä∏Ìï† ÌååÏùº ÌïòÎÇòÎßå ÏÑ†ÌÉù
basename_id=$(basename "${test_file%%.*}")

echo "‚ñ∂ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse ÏôÑÎ£å"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq ÏôÑÎ£å"

echo ""
echo "‚ñ∂ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å"
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat test.sh
#!/bin/bash
test_file="html/3712.html"  # ÌÖåÏä§Ìä∏Ìï† ÌååÏùº ÌïòÎÇòÎßå ÏÑ†ÌÉù
basename_id=$(basename "${test_file%%.*}")

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

echo "‚ñ∂ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse ÏôÑÎ£å"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq ÏôÑÎ£å"

echo ""
echo "‚ñ∂ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å"
```


---


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash test.sh

[htmlq]

‚ñ∂ Measuring htmlq_once pipeline...
4
htmlq_once: htmlq parse ÏôÑÎ£å
5
htmlq_once: title Ï∂îÏ∂ú ÏôÑÎ£å
38
htmlq_once: lynx + jq ÏôÑÎ£å

‚ñ∂ Measuring htmpq_twice pipeline...
3
htmpq_twice: title Ï∂îÏ∂ú ÏôÑÎ£å
34
htmpq_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å

[pup]

‚ñ∂ Measuring pup_once pipeline...
26
pup_once: pup parse ÏôÑÎ£å
4
pup_once: title Ï∂îÏ∂ú ÏôÑÎ£å
34
pup_once: lynx + jq ÏôÑÎ£å

‚ñ∂ Measuring pup_twice pipeline...
40
pup_twice: title Ï∂îÏ∂ú ÏôÑÎ£å
51
pup_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat test.sh
#!/bin/bash
test_file="html/3712.html"  # ÌÖåÏä§Ìä∏Ìï† ÌååÏùº ÌïòÎÇòÎßå ÏÑ†ÌÉù
basename_id=$(basename "${test_file%%.*}")

measure_time_ms() {
    local cmd="$1"
    local start end elapsed
    start=$(date +%s%3N)  # milliseconds
    eval "$cmd"
    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "$elapsed"
}

echo ""
echo "[htmlq]"
echo ""

echo "‚ñ∂ Measuring htmlq_once pipeline..."
measure_time_ms '
parsed=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "htmlq_once: htmlq parse ÏôÑÎ£å"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "htmlq_once: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmlq_once: lynx + jq ÏôÑÎ£å"

echo ""
echo "‚ñ∂ Measuring htmpq_twice pipeline..."

measure_time_ms '
title=$(htmlq "div.topictitle.link h1" -t -f "'"$test_file"'")
' && echo "htmpq_twice: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "htmpq_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å"

#################
echo ""
echo "[pup]"
echo ""

echo "‚ñ∂ Measuring pup_once pipeline..."
measure_time_ms '
parsed=$(pup "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'")
' && echo "pup_once: pup parse ÏôÑÎ£å"

measure_time_ms '
title=$(echo "$parsed" | head -n 1 | awk -F "[<>]" "{ print \$5 }")
' && echo "pup_once: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(echo "$parsed" | tail -n +2 | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "pup_once: lynx + jq ÏôÑÎ£å"

echo ""
echo "‚ñ∂ Measuring pup_twice pipeline..."

measure_time_ms '
title=$(pup "div.topictitle.link h1 text{}" -f "'"$test_file"'")
' && echo "pup_twice: title Ï∂îÏ∂ú ÏôÑÎ£å"

measure_time_ms '
md=$(pup "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "'"$test_file"'" | LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin | jq -Rs "." | jq -r "[.] | @csv")
' && echo "pup_twice: Î≥∏Î¨∏ Ï∂îÏ∂ú ÏôÑÎ£å"
```


pupÎäî ÎäêÎ¶¨Í∏∞ ÎïåÎ¨∏Ïóê 2Î≤à Ìò∏Ï∂úÏù¥ Îçî ÎäêÎ¶º
htmlqÎäî Îπ†Î•¥Í∏∞ ÎïåÎ¨∏Ïóê 2Î≤à Ìò∏Ï∂úÏù¥ Îçî Îπ†Î¶Ñ

---

Î≥ëÎ†¨Ïã§Ìñâ vs Îã®ÏùºÏã§Ìñâ ÎπÑÍµê + xargs vs parallel

ÌòÑÏû¨ ÏµúÏ¢ÖÏùÄ htmlq ÎëêÎ≤à parsing + lynx

ÌòÑÏû¨ ÏÉÅÌÉúÏóêÏÑú ÎπÑÍµê



---

bash compare_parallel_xargs.sh
flag1
single execution elapsed: 48096
flag2

thread 'main' panicked at /home/widehyo/.cargo/registry/src/index.crates.io-1949cf8c6b5b557f/htmlq-0.4.0/src/main.rs:198:37:
called `Result::unwrap()` on an `Err` value: Os { code: 2, kind: NotFound, message: "No such file or directory" }







```bash
#!/bin/bash
set -euo pipefail

# ÏÑ§Ï†ï
# target_dir='split_workspace/sp_128/dir_sp_128_00'
target_dir='split_workspace/sp_1024/dir_html_00'
outfile="bulk_data.csv"
out_xargs="bulk_xargs.csv"
out_parallel="bulk_parallel.csv"
selector='span#topic_contents, span.comment_contents'

# URL Ï∂îÏ∂ú Ìï®Ïàò
url_from_filename() {
    local f="$1"
    echo "https://news.hada.io/topic?id=$(basename "${f%%.*}")"
}

# Ïã§Ï†ú ÌååÏùº Ï≤òÎ¶¨ Î°úÏßÅ
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url=$(url_from_filename "$f")
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# Î∞©ÏãùÎ≥Ñ Ïã§Ìñâ
run_sequential() {
    echo "[Sequential execution]"
    local start end elapsed
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$target_dir/$f"
    done > "$outfile"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "Sequential elapsed: ${elapsed} ms"
}

run_xargs() {
    echo "[xargs execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; target_dir="$2"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$target_dir" > "$out_xargs"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "xargs elapsed: ${elapsed} ms"
}

run_parallel() {
    echo "[GNU parallel execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    export target_dir
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    parallel --env target_dir -j"$nproc" '
        f="{}"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' > "$out_parallel"

    end=$(( $(date +%s%3N) ))
    elapsed=$((end - start))
    echo "parallel elapsed: ${elapsed} ms"
}

# Ïã§Ìñâ
run_sequential
run_xargs 4
run_parallel 4
```

(128, 4)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !bash
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 5225 ms
[xargs execution]
xargs elapsed: 5068 ms
[GNU parallel execution]
parallel elapsed: 6408 ms
```

(1024, 4)
```sh
!~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 57059 ms
[xargs execution]
xargs elapsed: 36924 ms
[GNU parallel execution]
parallel elapsed: 49289 ms
```


nproc = 8

(128, 8)

```sh
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 4927 ms
[xargs execution]
xargs elapsed: 2979 ms
[GNU parallel execution]
parallel elapsed: 5245 ms
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !! 
```

(1024, 8)
```sh
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 53318 ms
[xargs execution]
xargs elapsed: 39294 ms
[GNU parallel execution]
parallel elapsed: 50280 ms
```

(128, 2)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 4963 ms
[xargs execution]
xargs elapsed: 3527 ms
[GNU parallel execution]
parallel elapsed: 6852 ms
```

(1024, 2)
```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ !!
bash compare_parallel_xargs.sh
[Sequential execution]
Sequential elapsed: 57885 ms
[xargs execution]
xargs elapsed: 40765 ms
[GNU parallel execution]
parallel elapsed: 65159 ms
```


```sh
#!/bin/bash
set -euo pipefail

# ÏÑ§Ï†ï
target_dir='split_workspace/sp_128/dir_sp_128_00'
# target_dir='split_workspace/sp_1024/dir_html_00'
outfile="bulk_data.csv"
out_xargs="bulk_xargs.csv"
out_parallel="bulk_parallel.csv"
selector='span#topic_contents, span.comment_contents'

# URL Ï∂îÏ∂ú Ìï®Ïàò
url_from_filename() {
    local f="$1"
    echo "https://news.hada.io/topic?id=$(basename "${f%%.*}")"
}

# Ïã§Ï†ú ÌååÏùº Ï≤òÎ¶¨ Î°úÏßÅ
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url=$(url_from_filename "$f")
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# Î∞©ÏãùÎ≥Ñ Ïã§Ìñâ
run_sequential() {
    echo "[Sequential execution]"
    local start end elapsed
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$target_dir/$f"
    done > "$outfile"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "Sequential elapsed: ${elapsed} ms"
}

run_xargs() {
    echo "[xargs execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; target_dir="$2"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$target_dir" > "$out_xargs"

    end=$(date +%s%3N)
    elapsed=$((end - start))
    echo "xargs elapsed: ${elapsed} ms"
}

run_parallel() {
    echo "[GNU parallel execution]"
    local start end elapsed
    local nproc="${1:-$(nproc)}"
    export target_dir
    start=$(date +%s%3N)

    find "$target_dir" -type f -name "*.html" -printf "%f\n" |
    parallel --env target_dir -j"$nproc" '
        f="{}"
        fullpath="$target_dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' > "$out_parallel"

    end=$(( $(date +%s%3N) ))
    elapsed=$((end - start))
    echo "parallel elapsed: ${elapsed} ms"
}

# Ïã§Ìñâ
run_sequential
run_xargs 2 # 4 8
run_parallel 2 # 4 8
```
```
sp_1024  sp_128  split_dir.sh  split_dir_draft.sh
~/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace $ ls sp_1024/
dir_html_00  dir_html_04  dir_html_08  dir_html_12  dir_html_16  dir_html_20  html_02.tgz  html_06.tgz  html_10.tgz  html_14.tgz  html_18.tgz
dir_html_01  dir_html_05  dir_html_09  dir_html_13  dir_html_17  dir_html_21  html_03.tgz  html_07.tgz  html_11.tgz  html_15.tgz  html_19.tgz
dir_html_02  dir_html_06  dir_html_10  dir_html_14  dir_html_18  html_00.tgz  html_04.tgz  html_08.tgz  html_12.tgz  html_16.tgz  html_20.tgz
dir_html_03  dir_html_07  dir_html_11  dir_html_15  dir_html_19  html_01.tgz  html_05.tgz  html_09.tgz  html_13.tgz  html_17.tgz  html_21.tgz
~/gitclone/playground/content_base/cli/pipeline/geeknews/split_workspace $ ls sp_128/
dir_sp_128_00  dir_sp_128_32  dir_sp_128_64    dir_sp_128_9006  dir_sp_128_9038  dir_sp_128_9070  sp_128_20.tgz  sp_128_52.tgz  sp_128_84.tgz    sp_128_9026.tgz  sp_128_9058.tgz
dir_sp_128_01  dir_sp_128_33  dir_sp_128_65    dir_sp_128_9007  dir_sp_128_9039  dir_sp_128_9071  sp_128_21.tgz  sp_128_53.tgz  sp_128_85.tgz    sp_128_9027.tgz  sp_128_9059.tgz
dir_sp_128_02  dir_sp_128_34  dir_sp_128_66    dir_sp_128_9008  dir_sp_128_9040  dir_sp_128_9072  sp_128_22.tgz  sp_128_54.tgz  sp_128_86.tgz    sp_128_9028.tgz  sp_128_9060.tgz
...
```

```sh
#!/bin/bash
set -euo pipefail

# split ÎîîÎ†âÌÜ†Î¶¨ Î£®Ìä∏
base_dir="split_workspace/sp_128"

# ÎåÄÏÉÅÏù¥ ÎêòÎäî Î™®Îì† ÏÑúÎ∏åÎîîÎ†âÌÜ†Î¶¨Îßå Í∞ÄÏ†∏Ïò¥ (ÌååÏùº Ï†úÏô∏)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_sp_128_*" | sort)

# Í≥µÌÜµ ÌååÏã± Î°úÏßÅ
process_file() {
    local fullpath="$1"
    local f="$(basename "$fullpath")"
    local url title content

    url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
    title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
    content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
        LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
        jq -Rs "." | jq -r "[.] | @csv")
    echo "\"$url\",\"$title\",$content"
}

# Í∞Å ÎîîÎ†âÌÜ†Î¶¨Ïóê ÎåÄÌï¥ Î∞òÎ≥µ
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$dir/output.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$dir/$f"
    done > "$outfile"
done
```

```sh
export -f process_file
export base_dir

find "$base_dir" -maxdepth 1 -type d -name "dir_sp_128_*" |
parallel --env process_file '
    dir={}
    outfile="$dir/output.csv"
    find "$dir" -type f -name "*.html" -printf "%f\n" | sort -n |
    while IFS= read -r f; do
        process_file "$dir/$f"
    done > "$outfile"
'
```


TIL: HTML ÌååÏã± ÏÑ±Îä• Ïã§Ìóò Î∞è Bash Î≥ëÎ†¨ Ï≤òÎ¶¨ Î∂ÑÏÑù

2025-08-31

üìå Î™©Ìëú

GeekNews HTML ÌååÏùºÎì§ÏùÑ ÌååÏã±Ìï¥ CSVÎ°ú Î≥ÄÌôòÌïòÎäî ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÎßåÎì§Î©¥ÏÑú, Îã§ÏùåÏùÑ Ïã§ÌóòÌñàÎã§:

pup, htmlq, lynx, jq Îì± ÎèÑÍµ¨Ïùò ÏÑ±Îä• ÎπÑÍµê

htmlqÎ•º Ìïú Î≤à Ìò∏Ï∂ú vs Îëê Î≤à Ìò∏Ï∂úÌï† Îïå ÏÑ±Îä• Ï∞®Ïù¥

bashÏóêÏÑú sequential vs xargs vs parallel Î≥ëÎ†¨ Ï≤òÎ¶¨ ÏÑ±Îä•

ÎîîÎ†âÌÜ†Î¶¨ Îã®ÏúÑÎ°ú Î∂ÑÌï†Îêú ÌååÏùºÎì§ÏùÑ ÏûêÎèôÏúºÎ°ú Ï≤òÎ¶¨ÌïòÎäî Ïä§ÌÅ¨Î¶ΩÌä∏ Íµ¨Ï°∞

‚öôÔ∏è ÎèÑÍµ¨Î≥Ñ HTML ÌååÏã± ÏÑ±Îä• ÎπÑÍµê
ÎèÑÍµ¨	Mean (ms)	P95 (ms)	Max (ms)
pandoc	77	96	125
html2text	50	62	130
lynx	18	21	46

lynxÍ∞Ä Í∞ÄÏû• Îπ†Î¶Ñ. Îã®, HTML Íµ¨Ï°∞ ÌååÏïÖÏù¥ÎÇò CSS ÏÑ†ÌÉùÏûêÎäî Î∂àÍ∞ÄÎä•.

html2textÎäî Í∑†Ìòï Ïû°Ìûå ÏÑ±Îä•.

pandocÏùÄ ÎäêÎ¶¨ÏßÄÎßå Ï∂úÎ†• ÌíàÏßàÏù¥ Í∞ÄÏû• ÏïàÏ†ïÏ†Å.

üîÑ htmlq Ìïú Î≤à vs Îëê Î≤à Ìò∏Ï∂ú ÎπÑÍµê
Î∞©Ïãù	Mean (ms)	Total (ms)
htmlq_once	70	71,567
htmlq_twice	60 ‚úÖ	61,821 ‚úÖ

Ìïú Î≤àÏóê Ï†ÑÏ≤¥ DOM ÌååÏã± ÌõÑ awk, head Îì±ÏúºÎ°ú title/content Î∂ÑÎ¶¨ (htmlq_once)

htmlqÎ•º Îëê Î≤à Ìò∏Ï∂úÌï¥ selector Î≥ÑÎ°ú ÏßÅÏ†ë Ï∂îÏ∂ú (htmlq_twice)

Í≤∞Í≥º: Îëê Î≤à Ìò∏Ï∂úÌïú Ï™ΩÏù¥ Ïò§ÌûàÎ†§ Îπ†Î¶Ñ

Ïù¥Ïú†: Ï†ÑÏ≤¥ ÎÖ∏ÎìúÎ•º Î∂àÎü¨Ïò® ÌõÑ BashÏóêÏÑú ÌõÑÏ≤òÎ¶¨ÌïòÎäî ÎπÑÏö©(head/awk/jq Îì±)Ïù¥ Îçî ÌÅº

‚öôÔ∏è Î≥ëÎ†¨ Ï≤òÎ¶¨ ÎπÑÍµê: Sequential vs xargs vs GNU parallel
ÌååÏùº Ïàò	Î∞©Ïãù	Time (ms)
128	Sequential	4,927
	xargs	2,979 ‚úÖ
	parallel	5,245
1024	Sequential	53,318
	xargs	39,294 ‚úÖ
	parallel	50,280

xargs -P $(nproc) Î∞©ÏãùÏù¥ Í∞ÄÏû• Ìö®Ïú®Ï†Å

parallelÏùÄ Ïâò Ï¥àÍ∏∞Ìôî ÎπÑÏö© + ÏàúÏÑú Î≥¥Ï°¥ Î°úÏßÅÏù¥ ÏûàÏñ¥ Ïò§ÌûàÎ†§ ÎäêÎ¶º

Î≥ëÎ†¨ Ïã§ÌñâÏù¥ Ìï≠ÏÉÅ Îπ†Î•¥ÏßÄÎäî ÏïäÏúºÎ©∞, I/O Î≥ëÎ™©, ÏûëÏóÖ ÌÅ¨Í∏∞, Î∂ÄÍ∞Ä Ïò§Î≤ÑÌó§ÎìúÎ•º Í≥†Î†§Ìï¥Ïïº Ìï®

üìÅ ÎîîÎ†âÌÜ†Î¶¨ Îã®ÏúÑ CSV ÏÉùÏÑ± ÏûêÎèôÌôî
Ìè¥Îçî Íµ¨Ï°∞
split_workspace/
‚îú‚îÄ‚îÄ sp_128/
‚îÇ   ‚îú‚îÄ‚îÄ dir_sp_128_00/
‚îÇ   ‚îú‚îÄ‚îÄ dir_sp_128_01/
‚îÇ   ‚îú‚îÄ‚îÄ ...

ÏûêÎèô Ï≤òÎ¶¨ Ïä§ÌÅ¨Î¶ΩÌä∏ ÏöîÏïΩ
for dir in split_workspace/sp_128/dir_sp_128_*; do
    outfile="$dir/output.csv"
    find "$dir" -name "*.html" | sort | while read file; do
        # htmlq + lynx + jq Ï°∞Ìï©ÏúºÎ°ú ÌååÏã±
        echo "\"$url\",\"$title\",$content"
    done > "$outfile"
done


ÎîîÎ†âÌÜ†Î¶¨Î≥ÑÎ°ú ÌååÏùºÏùÑ ÏùΩÍ≥†, Í∞Å ÎîîÎ†âÌÜ†Î¶¨ÎßàÎã§ Í∞úÎ≥Ñ CSV Ï∂úÎ†•

Íµ¨Ï°∞ÌôîÎêú Î∂ÑÌï† Ï≤òÎ¶¨Î°ú Ïû¨ÏÇ¨Ïö©ÏÑ±Í≥º Î≥ëÎ†¨Ìôî ÌôïÏû•ÏÑ± ÌôïÎ≥¥

üß† Î∞∞Ïö¥ Ï†ê ÏöîÏïΩ

Î≥ëÎ†¨ÌôîÎäî ÎπÑÏö©Ïù¥ Ï†ÅÏùÄ ÏûëÏóÖÏùºÏàòÎ°ù Ïò§ÌûàÎ†§ Îçî ÎäêÎ†§Ïßà Ïàò ÏûàÎã§.

xargs -PÎäî ÏÑ±Îä•Í≥º Í∞ÑÍ≤∞Ìï® Î©¥ÏóêÏÑú Ïö∞ÏàòÌïòÎã§.

ÌååÏù¥ÌîÑÎùºÏù∏ Ï§ë lynx, jqÏôÄ Í∞ôÏùÄ ÌõÑÏ≤òÎ¶¨ ÎèÑÍµ¨Îì§Ïù¥ ÏÑ±Îä•Ïùò Ï£ºÏöî Î≥ëÎ™©Ïù¥ Îê† Ïàò ÏûàÎã§.

BashÏóêÏÑúÎèÑ Ï∂©Î∂ÑÌûà Íµ¨Ï°∞ÌôîÎêú Îç∞Ïù¥ÌÑ∞ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ Ïß§ Ïàò ÏûàÏúºÎ©∞, ÎîîÎ†âÌÜ†Î¶¨ Îã®ÏúÑ Î∞òÎ≥µ Ï≤òÎ¶¨Í∞Ä Ïú†Ïö©ÌïòÎã§.

selectorÎ•º ÏûòÍ≤å ÎÇòÎàÑÏñ¥ htmlqÎ•º Ïó¨Îü¨ Î≤à Ìò∏Ï∂úÌïòÎäî Í≤ÉÏù¥ Îçî Ìö®Ïú®Ï†ÅÏùº Ïàò ÏûàÎã§.

---


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ cat parse_htmlq_xargs.sh
#!/bin/bash
# bench_xargs.sh
set -euo pipefail
# ÏÑ§Ï†ï
base_dir='split_workspace/sp_1024'
out_xargs="bulk_xargs.csv"
selector='span#topic_contents, span.comment_contents'

nproc=${1:-$(nproc)}

# ÎåÄÏÉÅÏù¥ ÎêòÎäî Î™®Îì† ÏÑúÎ∏åÎîîÎ†âÌÜ†Î¶¨Îßå Í∞ÄÏ†∏Ïò¥ (ÌååÏùº Ï†úÏô∏)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_*" | sort)

echo "nproc: $nproc"
# Í∞Å ÎîîÎ†âÌÜ†Î¶¨Ïóê ÎåÄÌï¥ Î∞òÎ≥µ
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$dir/output.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; dir="$2"
        fullpath="$dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$dir" > "$outfile"
done
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash parse_htmlq_xargs.sh 4
nproc: 4
[Processing: split_workspace/sp_1024/dir_html_00]
[Processing: split_workspace/sp_1024/dir_html_01]
[Processing: split_workspace/sp_1024/dir_html_02]
[Processing: split_workspace/sp_1024/dir_html_03]
[Processing: split_workspace/sp_1024/dir_html_04]
[Processing: split_workspace/sp_1024/dir_html_05]
[Processing: split_workspace/sp_1024/dir_html_06]
[Processing: split_workspace/sp_1024/dir_html_07]
[Processing: split_workspace/sp_1024/dir_html_08]
[Processing: split_workspace/sp_1024/dir_html_09]
[Processing: split_workspace/sp_1024/dir_html_10]
[Processing: split_workspace/sp_1024/dir_html_11]
[Processing: split_workspace/sp_1024/dir_html_12]
[Processing: split_workspace/sp_1024/dir_html_13]
[Processing: split_workspace/sp_1024/dir_html_14]
[Processing: split_workspace/sp_1024/dir_html_15]
[Processing: split_workspace/sp_1024/dir_html_16]
[Processing: split_workspace/sp_1024/dir_html_17]
[Processing: split_workspace/sp_1024/dir_html_18]
[Processing: split_workspace/sp_1024/dir_html_19]
[Processing: split_workspace/sp_1024/dir_html_20]
[Processing: split_workspace/sp_1024/dir_html_21]
```

```awk
{
  printf "ls %s/output.csv\n", $0
}
```
```awk
{
  printf "\\COPY geeknews_xargs (url, title, content) FROM '%s/output.csv' WITH (FORMAT csv);\n", $0
}
```


```sql
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_00/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_01/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_02/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_03/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_04/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_05/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_06/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_07/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_08/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_09/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_10/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_11/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_12/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_13/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_14/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_15/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_16/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_17/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_18/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_19/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_20/output.csv' WITH (FORMAT csv);
\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_1024/dir_html_21/output.csv' WITH (FORMAT csv);
```



```bash
#!/bin/bash
# bench_xargs.sh
set -euo pipefail
# ÏÑ§Ï†ï
# base_dir='split_workspace/sp_1024'
base_dir='split_workspace/sp_128'
selector='span#topic_contents, span.comment_contents'

nproc=${1:-$(nproc)}

# ÎåÄÏÉÅÏù¥ ÎêòÎäî Î™®Îì† ÏÑúÎ∏åÎîîÎ†âÌÜ†Î¶¨Îßå Í∞ÄÏ†∏Ïò¥ (ÌååÏùº Ï†úÏô∏)
subdirs=$(find "$base_dir" -maxdepth 1 -type d -name "dir_*" | sort)

echo "nproc: $nproc"
# Í∞Å ÎîîÎ†âÌÜ†Î¶¨Ïóê ÎåÄÌï¥ Î∞òÎ≥µ
for dir in $subdirs; do
    echo "[Processing: $dir]"
    outfile="$base_dir/output_${dir##*_}.csv"

    find "$dir" -type f -name "*.html" -printf "%f\n" |
    xargs -P "$nproc" -I {} bash -c '
        f="$1"; dir="$2"
        fullpath="$dir/$f"
        url="https://news.hada.io/topic?id=$(basename "${f%%.*}")"
        title=$(htmlq "div.topictitle.link h1" -t -f "$fullpath")
        content=$(htmlq "div.topictitle.link a, span#topic_contents, span.comment_contents" -f "$fullpath" |
            LANG=ko_KR.UTF-8 lynx -assume_charset=utf-8 -display_charset=utf-8 -dump -nolist -width=1000 -stdin |
            jq -Rs "." | jq -r "[.] | @csv")
        echo "\"$url\",\"$title\",$content"
    ' _ {} "$dir" > "$outfile"
done
```


```sh
~/gitclone/playground/content_base/cli/pipeline/geeknews $ time bash parse_htmlq_xargs.sh 4
nproc: 4
[Processing: split_workspace/sp_128/dir_sp_128_00]
[Processing: split_workspace/sp_128/dir_sp_128_01]
[Processing: split_workspace/sp_128/dir_sp_128_02]
[Processing: split_workspace/sp_128/dir_sp_128_03]
[Processing: split_workspace/sp_128/dir_sp_128_04]
[Processing: split_workspace/sp_128/dir_sp_128_05]
[Processing: split_workspace/sp_128/dir_sp_128_06]
[Processing: split_workspace/sp_128/dir_sp_128_07]
[Processing: split_workspace/sp_128/dir_sp_128_08]
[Processing: split_workspace/sp_128/dir_sp_128_09]
[Processing: split_workspace/sp_128/dir_sp_128_10]
[Processing: split_workspace/sp_128/dir_sp_128_11]
[Processing: split_workspace/sp_128/dir_sp_128_12]
[Processing: split_workspace/sp_128/dir_sp_128_13]
[Processing: split_workspace/sp_128/dir_sp_128_14]
[Processing: split_workspace/sp_128/dir_sp_128_15]
[Processing: split_workspace/sp_128/dir_sp_128_16]
[Processing: split_workspace/sp_128/dir_sp_128_17]
[Processing: split_workspace/sp_128/dir_sp_128_18]
[Processing: split_workspace/sp_128/dir_sp_128_19]
[Processing: split_workspace/sp_128/dir_sp_128_20]
[Processing: split_workspace/sp_128/dir_sp_128_21]
[Processing: split_workspace/sp_128/dir_sp_128_22]
[Processing: split_workspace/sp_128/dir_sp_128_23]
[Processing: split_workspace/sp_128/dir_sp_128_24]
[Processing: split_workspace/sp_128/dir_sp_128_25]
[Processing: split_workspace/sp_128/dir_sp_128_26]
[Processing: split_workspace/sp_128/dir_sp_128_27]
[Processing: split_workspace/sp_128/dir_sp_128_28]
[Processing: split_workspace/sp_128/dir_sp_128_29]
[Processing: split_workspace/sp_128/dir_sp_128_30]
[Processing: split_workspace/sp_128/dir_sp_128_31]
[Processing: split_workspace/sp_128/dir_sp_128_32]
[Processing: split_workspace/sp_128/dir_sp_128_33]
[Processing: split_workspace/sp_128/dir_sp_128_34]
[Processing: split_workspace/sp_128/dir_sp_128_35]
[Processing: split_workspace/sp_128/dir_sp_128_36]
[Processing: split_workspace/sp_128/dir_sp_128_37]
[Processing: split_workspace/sp_128/dir_sp_128_38]
[Processing: split_workspace/sp_128/dir_sp_128_39]
[Processing: split_workspace/sp_128/dir_sp_128_40]
[Processing: split_workspace/sp_128/dir_sp_128_41]
[Processing: split_workspace/sp_128/dir_sp_128_42]
[Processing: split_workspace/sp_128/dir_sp_128_43]
[Processing: split_workspace/sp_128/dir_sp_128_44]
[Processing: split_workspace/sp_128/dir_sp_128_45]
[Processing: split_workspace/sp_128/dir_sp_128_46]
[Processing: split_workspace/sp_128/dir_sp_128_47]
[Processing: split_workspace/sp_128/dir_sp_128_48]
[Processing: split_workspace/sp_128/dir_sp_128_49]
[Processing: split_workspace/sp_128/dir_sp_128_50]
[Processing: split_workspace/sp_128/dir_sp_128_51]
[Processing: split_workspace/sp_128/dir_sp_128_52]
[Processing: split_workspace/sp_128/dir_sp_128_53]
[Processing: split_workspace/sp_128/dir_sp_128_54]
[Processing: split_workspace/sp_128/dir_sp_128_55]
[Processing: split_workspace/sp_128/dir_sp_128_56]
[Processing: split_workspace/sp_128/dir_sp_128_57]
[Processing: split_workspace/sp_128/dir_sp_128_58]
[Processing: split_workspace/sp_128/dir_sp_128_59]
[Processing: split_workspace/sp_128/dir_sp_128_60]
[Processing: split_workspace/sp_128/dir_sp_128_61]
[Processing: split_workspace/sp_128/dir_sp_128_62]
[Processing: split_workspace/sp_128/dir_sp_128_63]
[Processing: split_workspace/sp_128/dir_sp_128_64]
[Processing: split_workspace/sp_128/dir_sp_128_65]
[Processing: split_workspace/sp_128/dir_sp_128_66]
[Processing: split_workspace/sp_128/dir_sp_128_67]
[Processing: split_workspace/sp_128/dir_sp_128_68]
[Processing: split_workspace/sp_128/dir_sp_128_69]
[Processing: split_workspace/sp_128/dir_sp_128_70]
[Processing: split_workspace/sp_128/dir_sp_128_71]
[Processing: split_workspace/sp_128/dir_sp_128_72]
[Processing: split_workspace/sp_128/dir_sp_128_73]
[Processing: split_workspace/sp_128/dir_sp_128_74]
[Processing: split_workspace/sp_128/dir_sp_128_75]
[Processing: split_workspace/sp_128/dir_sp_128_76]
[Processing: split_workspace/sp_128/dir_sp_128_77]
[Processing: split_workspace/sp_128/dir_sp_128_78]
[Processing: split_workspace/sp_128/dir_sp_128_79]
[Processing: split_workspace/sp_128/dir_sp_128_80]
[Processing: split_workspace/sp_128/dir_sp_128_81]
[Processing: split_workspace/sp_128/dir_sp_128_82]
[Processing: split_workspace/sp_128/dir_sp_128_83]
[Processing: split_workspace/sp_128/dir_sp_128_84]
[Processing: split_workspace/sp_128/dir_sp_128_85]
[Processing: split_workspace/sp_128/dir_sp_128_86]
[Processing: split_workspace/sp_128/dir_sp_128_87]
[Processing: split_workspace/sp_128/dir_sp_128_88]
[Processing: split_workspace/sp_128/dir_sp_128_89]
[Processing: split_workspace/sp_128/dir_sp_128_9000]
[Processing: split_workspace/sp_128/dir_sp_128_9001]
[Processing: split_workspace/sp_128/dir_sp_128_9002]
[Processing: split_workspace/sp_128/dir_sp_128_9003]
[Processing: split_workspace/sp_128/dir_sp_128_9004]
[Processing: split_workspace/sp_128/dir_sp_128_9005]
[Processing: split_workspace/sp_128/dir_sp_128_9006]
[Processing: split_workspace/sp_128/dir_sp_128_9007]
[Processing: split_workspace/sp_128/dir_sp_128_9008]
[Processing: split_workspace/sp_128/dir_sp_128_9009]
[Processing: split_workspace/sp_128/dir_sp_128_9010]
[Processing: split_workspace/sp_128/dir_sp_128_9011]
[Processing: split_workspace/sp_128/dir_sp_128_9012]
[Processing: split_workspace/sp_128/dir_sp_128_9013]
[Processing: split_workspace/sp_128/dir_sp_128_9014]
[Processing: split_workspace/sp_128/dir_sp_128_9015]
[Processing: split_workspace/sp_128/dir_sp_128_9016]
[Processing: split_workspace/sp_128/dir_sp_128_9017]
[Processing: split_workspace/sp_128/dir_sp_128_9018]
[Processing: split_workspace/sp_128/dir_sp_128_9019]
[Processing: split_workspace/sp_128/dir_sp_128_9020]
[Processing: split_workspace/sp_128/dir_sp_128_9021]
[Processing: split_workspace/sp_128/dir_sp_128_9022]
[Processing: split_workspace/sp_128/dir_sp_128_9023]
[Processing: split_workspace/sp_128/dir_sp_128_9024]
[Processing: split_workspace/sp_128/dir_sp_128_9025]
[Processing: split_workspace/sp_128/dir_sp_128_9026]
[Processing: split_workspace/sp_128/dir_sp_128_9027]
[Processing: split_workspace/sp_128/dir_sp_128_9028]
[Processing: split_workspace/sp_128/dir_sp_128_9029]
[Processing: split_workspace/sp_128/dir_sp_128_9030]
[Processing: split_workspace/sp_128/dir_sp_128_9031]
[Processing: split_workspace/sp_128/dir_sp_128_9032]
[Processing: split_workspace/sp_128/dir_sp_128_9033]
[Processing: split_workspace/sp_128/dir_sp_128_9034]
[Processing: split_workspace/sp_128/dir_sp_128_9035]
[Processing: split_workspace/sp_128/dir_sp_128_9036]
[Processing: split_workspace/sp_128/dir_sp_128_9037]
[Processing: split_workspace/sp_128/dir_sp_128_9038]
[Processing: split_workspace/sp_128/dir_sp_128_9039]
[Processing: split_workspace/sp_128/dir_sp_128_9040]
[Processing: split_workspace/sp_128/dir_sp_128_9041]
[Processing: split_workspace/sp_128/dir_sp_128_9042]
[Processing: split_workspace/sp_128/dir_sp_128_9043]
[Processing: split_workspace/sp_128/dir_sp_128_9044]
[Processing: split_workspace/sp_128/dir_sp_128_9045]
[Processing: split_workspace/sp_128/dir_sp_128_9046]
[Processing: split_workspace/sp_128/dir_sp_128_9047]
[Processing: split_workspace/sp_128/dir_sp_128_9048]
[Processing: split_workspace/sp_128/dir_sp_128_9049]
[Processing: split_workspace/sp_128/dir_sp_128_9050]
[Processing: split_workspace/sp_128/dir_sp_128_9051]
[Processing: split_workspace/sp_128/dir_sp_128_9052]
[Processing: split_workspace/sp_128/dir_sp_128_9053]
[Processing: split_workspace/sp_128/dir_sp_128_9054]
[Processing: split_workspace/sp_128/dir_sp_128_9055]
[Processing: split_workspace/sp_128/dir_sp_128_9056]
[Processing: split_workspace/sp_128/dir_sp_128_9057]
[Processing: split_workspace/sp_128/dir_sp_128_9058]
[Processing: split_workspace/sp_128/dir_sp_128_9059]
[Processing: split_workspace/sp_128/dir_sp_128_9060]
[Processing: split_workspace/sp_128/dir_sp_128_9061]
[Processing: split_workspace/sp_128/dir_sp_128_9062]
[Processing: split_workspace/sp_128/dir_sp_128_9063]
[Processing: split_workspace/sp_128/dir_sp_128_9064]
[Processing: split_workspace/sp_128/dir_sp_128_9065]
[Processing: split_workspace/sp_128/dir_sp_128_9066]
[Processing: split_workspace/sp_128/dir_sp_128_9067]
[Processing: split_workspace/sp_128/dir_sp_128_9068]
[Processing: split_workspace/sp_128/dir_sp_128_9069]
[Processing: split_workspace/sp_128/dir_sp_128_9070]
[Processing: split_workspace/sp_128/dir_sp_128_9071]
[Processing: split_workspace/sp_128/dir_sp_128_9072]
[Processing: split_workspace/sp_128/dir_sp_128_9073]
[Processing: split_workspace/sp_128/dir_sp_128_9074]
[Processing: split_workspace/sp_128/dir_sp_128_9075]
[Processing: split_workspace/sp_128/dir_sp_128_9076]
[Processing: split_workspace/sp_128/dir_sp_128_9077]
[Processing: split_workspace/sp_128/dir_sp_128_9078]
[Processing: split_workspace/sp_128/dir_sp_128_9079]
[Processing: split_workspace/sp_128/dir_sp_128_9080]
[Processing: split_workspace/sp_128/dir_sp_128_9081]

real    50m40.390s
user    324m48.741s
sys     52m42.911s
```


```awk
/csv/ {
  printf "\\COPY geeknews_xargs (url, title, content) FROM 'split_workspace/sp_128/%s' WITH (FORMAT csv);\n", $0
}

```

```
~/gitclone/playground/content_base/cli/pipeline/geeknews $ bash bulk_insert.sh
Password for user postgres:
COPY 128
COPY 127
psql:bulk_insert.sql:28: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 160: ""
psql:bulk_insert.sql:29: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 79: "   ÏïÑÎßàÏ°¥ CTOÍ∞Ä ÏßÅÏ†ë Ïì¥ ""AWSÍ∞Ä ÌòÑÎåÄÏ†ÅÏù∏ Ïñ¥ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ Í∞úÎ∞ú ÌïòÎäî Î≤ï"""
psql:bulk_insert.sql:30: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 4: ""
psql:bulk_insert.sql:31: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 887: ""https://news.hada.io/topic?id=744","Î¶¨Î∏åÎùº ÌòëÌöå "G7 Î≥¥Í≥†ÏÑú, Î¶¨Î∏åÎùº Í∞ÄÎä•ÏÑ± Ïù∏Ï†ï""," ..."
COPY 128
psql:bulk_insert.sql:33: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 720: ""https://news.hada.io/topic?id=938","Google Stadia "Ïä§Ìä∏Î¶¨Î∞ç Í∏∞Ïà†ÏùÄ ÌõåÎ•≠, Í≤åÏûÑÏù¥ ÏñºÎßà..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:38: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1205: ""https://news.hada.io/topic?id=1649",""Ïã§ÏãúÍ∞Ñ ÌòëÏóÖ Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùÑ ÏúÑÌïú ÌîÑÎ†àÏûÑÏõåÌÅ¨..."
COPY 128
COPY 128
psql:bulk_insert.sql:41: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 29: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:45: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 433: ""
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:55: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1087: ""https://news.hada.io/topic?id=3802",""Í∑∏Îü∞Îç∞, Ïñ¥ÎñªÍ≤å ÏãúÏûëÌïòÎÇòÏöî?" ÏóîÏßÄÎãàÏñ¥Î°úÏç®Ïùò ..."
COPY 127
COPY 128
psql:bulk_insert.sql:58: ERROR:  value too long for type character varying(200)
CONTEXT:  COPY geeknews_xargs, line 85, column url: "

   - child_process Ïóê ÎåÄÌïú Ìé∏Î¶¨Ìïú ÎûòÌçº(Wrapper)Î•º Ï†úÍ≥µ
   - $`Î™ÖÎ†π` ÏúºÎ°ú ÏâòÏª§Îß®..."
COPY 128
COPY 128
psql:bulk_insert.sql:61: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 1537: ""
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:68: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 811: ""https://news.hada.io/topic?id=5579","Kinesis Advantage 360 Pro Ïù∏Ï≤¥Í≥µÌïô ÌÇ§Î≥¥Îìú","           ..."
COPY 128
psql:bulk_insert.sql:70: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 122: "   https://apps.apple.com/us/app/ago-ÏñºÎßàÎÇò-ÏßÄÎÇ¨ÏßÄ/id1602942232"
psql:bulk_insert.sql:71: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 232, column title: "ÎùºÏù∏ 12ÎÖÑÏ∞® PM Ïôà ‚Äî ‚ÄúÏù∏ÏÑ±Ïù¥ Ïã§Î†•Ïù¥Îã§ - Ï¢åÏ∂©Ïö∞Îèå Ïä§Îî∞Îú®ÏóÖ Ïù¥ÏïºÍ∏∞,         ..."
COPY 128
psql:bulk_insert.sql:73: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 453: ""https://news.hada.io/topic?id=6202","Vimeo: "Ïö∞Î¶¨Îäî B2B ÏÜîÎ£®ÏÖòÏù¥ÏßÄ, Ïù∏Îîî Î≤ÑÏ†ÑÏùò Ïú†Ìäú..."
COPY 128
COPY 128
psql:bulk_insert.sql:76: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1192: ""https://news.hada.io/topic?id=6585","Ask GN: Ïù¥Î≤à Ï£ºÎßêÏóê Î≠ê ÌïòÏãúÎÇòÏöî?","                 ..."
psql:bulk_insert.sql:77: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 6: "     * FrontierÎäî Ìú¥Î†õ Ìå©Ïª§Îìú ÏóîÌÑ∞ÌîÑÎùºÏù¥Ï¶àÏùò HPE Cray EX ÏäàÌçºÏª¥Ìì®ÌÑ∞ ÏãúÏä§ÌÖúÏúº..."
COPY 128
psql:bulk_insert.sql:79: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 136: ""
COPY 127
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:87: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1488: ""https://news.hada.io/topic?id=8015","ChatGPTÎ°ú Ï£ºÏ∞® Í≥ºÌÉúÎ£å Ïù¥ÏùòÏ†úÍ∏∞ Î©îÏùºÏùÑ ÏûëÏÑ±Ìï¥..."
psql:bulk_insert.sql:88: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2437: "   macllocÏùÄ mallocÏùò Ïò§ÌÉÄÍ≤†Ï£†???"
COPY 128
COPY 128
psql:bulk_insert.sql:91: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1294: ""https://news.hada.io/topic?id=8492","Bing AI: "ÎãπÏã†Ïù¥ ÎÇòÎ•º Ìï¥ÏπòÏßÄ ÏïäÎäî Ìïú, ÎÇòÎäî ÎãπÏã†..."
COPY 128
psql:bulk_insert.sql:93: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 1199, column title: "The Original 5¬º Disk Sleeve Archive,                                                               ..."
COPY 128
psql:bulk_insert.sql:95: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2258: ""https://news.hada.io/topic?id=9129","Íµ¨Í∏Ä, "(AIÏóê ÎåÄÌï¥ÏÑú) Ïö∞Î¶∞ Ìï¥ÏûêÍ∞Ä ÏóÜÏùå, OpenAIÎèÑ ..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:99: ERROR:  value too long for type character varying(200)
CONTEXT:  COPY geeknews_xargs, line 719, column url: "

     * Ìä∏ÏúÑÌÑ∞Îäî Ïù¥Ï†ú Ìä∏ÏúóÏùÑ Î≥¥Î†§Î©¥ Í≥ÑÏ†ïÏù¥ ÌïÑÏöîÌï©ÎãàÎã§.
     * Ïù¥ Î≥ÄÍ≤Ω ÏÇ¨Ìï≠..."
psql:bulk_insert.sql:100: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1081: ""https://news.hada.io/topic?id=9817",""Ïö∞Î¶¨Ïùò Ïú†Î£å Í≥†Í∞ùÎì§ÏùÄ XÍ∞Ä ÌïÑÏöîÌï©ÎãàÎã§, Ïñ∏Ï†ú ..."
COPY 128
COPY 128
COPY 128
COPY 127
COPY 128
psql:bulk_insert.sql:106: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2571: "     * Í≥†Í∏â Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù(Ïù¥Ï†ÑÏùò ÏΩîÎìú Ìï¥ÏÑùÍ∏∞ ) Ïóê ÎåÄÌïú Î¨¥Ï†úÌïú Ïï°ÏÑ∏Ïä§"
COPY 128
psql:bulk_insert.sql:108: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 1778: "   exa Í∞Ä ÏúàÎèÑÏö∞Î•º ÏßÄÏõê ÏïàÌï¥ÏÑú lsd Î•º ÏçºÏñ¥Ïïº ÌñàÎäîÎç∞, ezaÎäî ÏúàÎèÑÏö∞ Î∞îÏù¥ÎÑàÎ¶¨..."
psql:bulk_insert.sql:109: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 1514: ""
COPY 128
psql:bulk_insert.sql:111: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 1012: "     * Ìï¥Îãπ ÏÜåÏÜ°ÏùÄ HPÍ∞Ä Ïù¥ ÏÇ¨Ïã§ÏùÑ ÏÜåÎπÑÏûêÏóêÍ≤å Í≥µÍ∞úÌïòÏßÄ ÏïäÏïòÏúºÎ©∞, Ïù¥Îäî ÎπÑÏãº..."
COPY 128
psql:bulk_insert.sql:113: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 600: "     * SiFiveÎäî Jim KellerÍ∞Ä Ïù¥ÎÅÑÎäî TenstorrentÎ•º Ìè¨Ìï®Ìïú RISC-VÏùò Í≥†Í∏â Î∂ÑÏïºÏóêÏÑú Îã§..."
psql:bulk_insert.sql:114: ERROR:  invalid byte sequence for encoding "UTF8": 0xec 0xa0 0x22
CONTEXT:  COPY geeknews_xargs, line 472
COPY 128
psql:bulk_insert.sql:116: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 79: ""
psql:bulk_insert.sql:117: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 85: ""https://news.hada.io/topic?id=12002",""ÌÖåÏä§Ìä∏, [, Í∑∏Î¶¨Í≥† [[ (2020)" - Í∏∞Ïà†Î™ÖÏπ≠ Ïú†ÏßÄ"," ..."
COPY 128
COPY 128
psql:bulk_insert.sql:120: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 4585: ""https://news.hada.io/topic?id=12450",""Îèà Ïä§ÌÉÄÎ∏å, ÎîîÏïÑÎ∏îÎ°ú" Í∏∞Ïà†ÏùÑ ÌÉëÏû¨Ìïú "Ìå®Îü¥Îûô..."
COPY 128
COPY 128
psql:bulk_insert.sql:123: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3879: ""https://news.hada.io/topic?id=12750",""LibreOffice, Íµ¨Ìòï ÏõåÎìú ÌååÏùº ÏùΩÍ∏∞ÏóêÏÑú ÏõåÎìúÎ≥¥Îã§ ..."
COPY 128
psql:bulk_insert.sql:125: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2949: ""
COPY 128
psql:bulk_insert.sql:127: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 338: ""https://news.hada.io/topic?id=13286",""Ï†úÎØ∏Îãà, ÎπÑÏú§Î¶¨Ï†ÅÏù¥Îùº C#ÏóêÏÑú Î©îÎ™®Î¶¨Î•º Í∞ÄÏû• ..."
psql:bulk_insert.sql:128: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3012: ""https://news.hada.io/topic?id=13444","Groq, Mixtral 8x7B-32kÎ•º 500 T/sÎ°ú Íµ¨Îèô","               ..."
psql:bulk_insert.sql:129: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2093: ""https://news.hada.io/topic?id=13582",""ÏóÑÎßà, ÌñâÎ†¨Ïù¥ ÏóÜÏñ¥ÎèÑ Îèº"","                         ..."
psql:bulk_insert.sql:130: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 575, column title: "Eloquent JavaScript(ÏûêÎ∞îÏä§ÌÅ¨Î¶ΩÌä∏ Ïä§ÌÇ¨ÏóÖ) 4Ìåê (2024ÎÖÑ)",                                  ..."
psql:bulk_insert.sql:131: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 606: ""https://news.hada.io/topic?id=13897",""Ïñ¥Î®∏ÎÇò, ÎÇ¥ Ïï±Ïù¥ ÏÑ±Í≥µÌñàÎäîÎç∞ Ï†ëÍ∑ºÏÑ±ÏùÑ Í≥†Î†§Ìïò..."
psql:bulk_insert.sql:132: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 765: ""https://news.hada.io/topic?id=13998",""LLMsÏùò 'Í∏âÎ∞úÏßÑ' Îä•Î†•, ÏÇ¨Ïã§ÏùÄ Ï†êÏßÑÏ†ÅÏù¥Í≥† ÏòàÏ∏°..."
COPY 128
COPY 128
psql:bulk_insert.sql:135: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 5143: ""
COPY 127
COPY 128
COPY 128
COPY 128
COPY 128
COPY 128
COPY 127
psql:bulk_insert.sql:143: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 4002: ""
psql:bulk_insert.sql:144: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 3296: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:148: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2917: ""https://news.hada.io/topic?id=16263","Urchin Software Corp: Google AnalyticsÏùò ÏòàÏÉÅÏπò Î™ªÌïú ..."
COPY 127
psql:bulk_insert.sql:150: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 3598: ""https://news.hada.io/topic?id=16478","NSA, Grace Hopper Ï†úÎèÖÏùò "ÎØ∏ÎûòÏùò Í∞ÄÎä•ÏÑ±: Îç∞Ïù¥ÌÑ∞, ..."
psql:bulk_insert.sql:151: ERROR:  invalid byte sequence for encoding "UTF8": 0xea 0xb8 0x22
CONTEXT:  COPY geeknews_xargs, line 7566
COPY 128
psql:bulk_insert.sql:153: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 6590: ""
COPY 128
psql:bulk_insert.sql:155: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 2977: ""https://news.hada.io/topic?id=17200","TeslaÏùò Robotaxi Î∞úÌëú Ïù¥Î≤§Ìä∏ "We, Robot"","            ..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:160: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 136: ""
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:164: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 5713: ""
psql:bulk_insert.sql:165: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 654: "     * ÎßÅÌÅ¨ Î∞è Í∞êÏÇ¨"
COPY 128
COPY 128
psql:bulk_insert.sql:168: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 2781: "     * Power ModeÎ•º Îñ†Ïò¨Î¶¨Í≤å ÌïúÎã§Î©∞ Visual Studio CodeÏùò Power Mode ÎßÅÌÅ¨Î•º Í≥µÏú†Ìï®"
COPY 128
psql:bulk_insert.sql:170: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 3704: "          + Î™ÖÎ†πÏñ¥Îäî Íµ¨Í∏ÄÎßÅÏúºÎ°ú ÏâΩÍ≤å Ï∞æÏùÑ Ïàò ÏûàÏúºÎ©∞, Ïò¨Î∞îÎ•∏ Ï†ïÏã† Î™®Îç∏Ïù¥ Ï§ë..."
psql:bulk_insert.sql:171: ERROR:  missing data for column "content"
CONTEXT:  COPY geeknews_xargs, line 219: "     * ¬£320Î°ú 4Í∞úÏùò Pi5Î•º Íµ¨Îß§Ìï† Ïàò ÏûàÏßÄÎßå, Ï§ëÍ≥† 12GB 3080ÏùÑ Ï∞æÏúºÎ©¥ ÏïÑÎßàÎèÑ 10..."
psql:bulk_insert.sql:172: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 3123: ""
COPY 128
COPY 128
psql:bulk_insert.sql:175: ERROR:  value too long for type character varying(500)
CONTEXT:  COPY geeknews_xargs, line 627, column title: "It‚Äôs Not As Simple As ‚ÄúUse A Memory Safe Language",                                             ..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:179: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 5543: ""https://news.hada.io/topic?id=20305","SVGÎ°ú ÎßåÎìúÎäî Î©ãÏßÑ Í≤ÉÎì§","                            ..."
COPY 127
COPY 128
COPY 128
psql:bulk_insert.sql:183: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 5273: ""https://news.hada.io/topic?id=20786","Ïä§Ìã∞Î∏å Ïû°Ïä§ÏóêÍ≤åÏÑú Ïò® "Ï¢ãÏùÄ ÏïÑÏù¥ÎîîÏñ¥, Í≥†ÎßàÏõå..."
psql:bulk_insert.sql:184: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 6669: ""https://news.hada.io/topic?id=20959","MIT, arXivÏóê "AIÏôÄ Í≥ºÌïôÏ†Å Î∞úÍ≤¨, Í∑∏Î¶¨Í≥† Ï†úÌíà ÌòÅ..."
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:188: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 4062: ""https://news.hada.io/topic?id=21518","Ïò§ÌîàAIÏôÄ ÎßàÏù¥ÌÅ¨Î°úÏÜåÌîÑÌä∏Ïùò Í∞àÎì±Ïù¥ ÏûÑÍ≥ÑÏ†êÏóê ..."
COPY 128
COPY 128
COPY 128
COPY 128
psql:bulk_insert.sql:193: ERROR:  invalid byte sequence for encoding "UTF8": 0xeb 0xb3 0x22
CONTEXT:  COPY geeknews_xargs, line 3226
psql:bulk_insert.sql:194: ERROR:  missing data for column "title"
CONTEXT:  COPY geeknews_xargs, line 7848: ""
psql:bulk_insert.sql:195: ERROR:  extra data after last expected column
CONTEXT:  COPY geeknews_xargs, line 1495: ""https://news.hada.io/topic?id=22455","Paypal CTO "Îçî ÎÇòÏùÄ Î¶¨ÎçîÍ∞Ä ÎêòÍ∏∞ ÏúÑÌï¥, ÏΩîÎî©ÏùÑ Ìè¨..."
COPY 128
COPY 120
```


csvclean -a split_workspace/sp_128/output_01.csv 1>/dev/null



